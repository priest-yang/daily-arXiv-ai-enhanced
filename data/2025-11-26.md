<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 192]
- [cs.CL](#cs.CL) [Total: 34]
- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PuzzlePoles: Cylindrical Fiducial Markers Based on the PuzzleBoard Pattern](https://arxiv.org/abs/2511.19448)
*Juri Zach,Peer Stelldinger*

Main category: cs.CV

TL;DR: 本文提出了一种新型的视觉基准标记PuzzlePole，可实现360°可见性和高精度定位，可广泛应用于自主系统。


<details>
  <summary>Details</summary>
Motivation: 传统视觉标记在自主系统中用于校准和定位，但常存在视角受限、易受遮挡等问题。因此需要更可靠且适应性更强的新型标记。

Method: 作者基于PuzzleBoard模式，设计了一种柱状（圆柱体）标记PuzzlePole。通过其组合结构，实现了任意方向下的稳定识别与位姿估计，并提升了对遮挡的鲁棒性。

Result: PuzzlePole能在360°范围内被准确识别，具备高定位和方向估计精度，且对遮挡干扰有很强的适应能力。

Conclusion: PuzzlePole为自主系统中的导航、SLAM及交互应用提供了灵活、鲁棒和高精度的视觉基准解决方案。

Abstract: Reliable perception of the environment is a key enabler for autonomous systems, where calibration and localization tasks often rely on robust visual markers. We introduce the PuzzlePole, a new type of fiducial markers derived from the recently proposed PuzzleBoard calibration pattern. The PuzzlePole is a cylindrical marker, enabling reliable recognition and pose estimation from 360° viewing direction. By leveraging the unique combinatorial structure of the PuzzleBoard pattern, PuzzlePoles provide a high accuracy in localization and orientation while being robust to occlusions. The design offers flexibility for deployment in diverse autonomous systems scenarios, ranging from robot navigation and SLAM to tangible interfaces.

</details>


### [2] [Personalized Reward Modeling for Text-to-Image Generation](https://arxiv.org/abs/2511.19458)
*Jeongeun Lee,Ryang Heo,Dongha Lee*

Main category: cs.CV

TL;DR: 本文提出了一种个性化的文本生成图像（T2I）模型评估方法PIGReward，能更好地反映用户个人偏好，并引入PIGBench作为基准。


<details>
  <summary>Details</summary>
Motivation: 现有的T2I模型评估方法无法有效捕捉用户个人多样化的视觉偏好，对用户指令与生成图像的个性化契合度评估存在不足。

Method: 提出了PIGReward模型，通过自举推理获取用户语境，无需大量用户特定训练，可以个性化地生成评估维度并利用链式推理对图像进行多方面评价。同时，系统还能根据评估给出个性化反馈，优化用户的提示语。并构建PIGBench数据集，反映多个用户对于相同提示的不同视觉偏好。

Result: 大量实验表明，PIGReward在准确性和可解释性上均超越了现有方法，有效提升了T2I图像与用户个人意图的匹配程度。

Conclusion: PIGReward为T2I生成任务提供了可扩展、基于推理的个性化评估和优化工具，是朝实现个体化T2I生成迈出的重要一步。

Abstract: Recent text-to-image (T2I) models generate semantically coherent images from textual prompts, yet evaluating how well they align with individual user preferences remains an open challenge. Conventional evaluation methods, general reward functions or similarity-based metrics, fail to capture the diversity and complexity of personal visual tastes. In this work, we present PIGReward, a personalized reward model that dynamically generates user-conditioned evaluation dimensions and assesses images through CoT reasoning. To address the scarcity of user data, PIGReward adopt a self-bootstrapping strategy that reasons over limited reference data to construct rich user contexts, enabling personalization without user-specific training. Beyond evaluation, PIGReward provides personalized feedback that drives user-specific prompt optimization, improving alignment between generated images and individual intent. We further introduce PIGBench, a per-user preference benchmark capturing diverse visual interpretations of shared prompts. Extensive experiments demonstrate that PIGReward surpasses existing methods in both accuracy and interpretability, establishing a scalable and reasoning-based foundation for personalized T2I evaluation and optimization. Taken together, our findings highlight PIGReward as a robust steptoward individually aligned T2I generation.

</details>


### [3] [SG-OIF: A Stability-Guided Online Influence Framework for Reliable Vision Data](https://arxiv.org/abs/2511.19466)
*Penghao Rao,Runmin Jiang,Min Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种称为SG-OIF（Stability-Guided Online Influence Framework）的新方法，可以高效、实时地衡量训练样本对深度学习视觉模型预测的影响。此方法相比传统影响函数计算更加高效且适合动态训练过程。


<details>
  <summary>Details</summary>
Motivation: 在深度学习视觉模型中，了解每个训练样本对测试预测的具体影响至关重要，尤其是在识别和定位噪声数据时。然而，传统影响函数计算成本高昂且难以适应训练过程中的动态变化。因此，亟需一种效率高且能实时应用的影响估算方法。

Method: 论文提出SG-OIF方法，以算法稳定性作为实时控制器，采用轻量级的锚点IHVP计算（如随机Richardson和预处理Neumann方法），并引入模块化曲率后端，通过稳定性引导的残差阈值、异常门控和置信度校准，动态调整每个样本的影响分数。

Result: 在多个数据集的标签噪声和分布外样本检测任务中，SG-OIF取得了SOTA（最先进）性能。例如，在CIFAR-10（有20%不对称噪声）数据集的Top 1%预测样本中达到91.1%的准确率，在MNIST数据集的AUPR评分达到99.8%。

Conclusion: SG-OIF方法显著提升了训练样本影响度估算的效率和准确性，为深度学习模型的运行提供了可实时调控的影响估算工具，有力支持了异常和噪声检测等关键应用。

Abstract: Approximating training-point influence on test predictions is critical for deploying deep-learning vision models, essential for locating noisy data. Though the influence function was proposed for attributing how infinitesimal up-weighting or removal of individual training examples affects model outputs, its implementation is still challenging in deep-learning vision models: inverse-curvature computations are expensive, and training non-stationarity invalidates static approximations. Prior works use iterative solvers and low-rank surrogates to reduce cost, but offline computation lags behind training dynamics, and missing confidence calibration yields fragile rankings that misidentify critical examples. To address these challenges, we introduce a Stability-Guided Online Influence Framework (SG-OIF), the first framework that treats algorithmic stability as a real-time controller, which (i) maintains lightweight anchor IHVPs via stochastic Richardson and preconditioned Neumann; (ii) proposes modular curvature backends to modulate per-example influence scores using stability-guided residual thresholds, anomaly gating, and confidence. Experimental results show that SG-OIF achieves SOTA (State-Of-The-Art) on noise-label and out-of-distribution detection tasks across multiple datasets with various corruption. Notably, our approach achieves 91.1\% accuracy in the top 1\% prediction samples on the CIFAR-10 (20\% asym), and gets 99.8\% AUPR score on MNIST, effectively demonstrating that this framework is a practical controller for online influence estimation.

</details>


### [4] [Pistachio: Towards Synthetic, Balanced, and Long-Form Video Anomaly Benchmarks](https://arxiv.org/abs/2511.19474)
*Jie Li,Hongyi Cai,Mingkang Dong,Muxin Pu,Shan You,Fei Wang,Tao Huang*

Main category: cs.CV

TL;DR: 本文提出了Pistachio，一个通过生成模型自动构建的视频异常检测与理解（VAD/VAU）基准，克服了现有数据集在场景多样性、异常均衡和时序复杂性上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前自动化系统对视频异常检测非常依赖，但现有基准数据集存在场景不丰富、异常覆盖不均、时序复杂度低等问题。此外，更高层次的视频异常理解（VAU）需要大量标注成本，导致研究进展受限。

Method: 作者提出通过受控的视频生成管线构建数据集，结合最新的视频生成模型，实现对场景、异常、时间叙事的精确控制。具体方法包括基于场景的异常分配、多步故事线生成和时序一致的长视频合成，全流程基本无需人工干预。

Result: 通过实验验证，Pistachio基准在规模、场景多样性、异常种类和时序复杂性方面大幅优于传统Internet数据集。现有方法在该基准上面临新的挑战，暴露了它们在动态和多事件异常理解能力上的不足。

Conclusion: Pistachio突破了传统数据集的局限，为视频异常检测与理解任务的研究和评测提供了新的基础，激励未来在动态多事件异常理解方面的研究。

Abstract: Automatically detecting abnormal events in videos is crucial for modern autonomous systems, yet existing Video Anomaly Detection (VAD) benchmarks lack the scene diversity, balanced anomaly coverage, and temporal complexity needed to reliably assess real-world performance. Meanwhile, the community is increasingly moving toward Video Anomaly Understanding (VAU), which requires deeper semantic and causal reasoning but remains difficult to benchmark due to the heavy manual annotation effort it demands. In this paper, we introduce Pistachio, a new VAD/VAU benchmark constructed entirely through a controlled, generation-based pipeline. By leveraging recent advances in video generation models, Pistachio provides precise control over scenes, anomaly types, and temporal narratives, effectively eliminating the biases and limitations of Internet-collected datasets. Our pipeline integrates scene-conditioned anomaly assignment, multi-step storyline generation, and a temporally consistent long-form synthesis strategy that produces coherent 41-second videos with minimal human intervention. Extensive experiments demonstrate the scale, diversity, and complexity of Pistachio, revealing new challenges for existing methods and motivating future research on dynamic and multi-event anomaly understanding.

</details>


### [5] [Tracking and Segmenting Anything in Any Modality](https://arxiv.org/abs/2511.19475)
*Tianlu Zhang,Qiang Zhang,Guiguang Ding,Jungong Han*

Main category: cs.CV

TL;DR: 本文提出了SATA通用跟踪与分割框架，实现了多模态输入下多任务统一处理，在18个基准上达到了最佳表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视频跟踪和分割方法多采用专用架构和参数，造成泛化与可扩展性差。虽然已有工作尝试多模态和多任务融合，但经常忽视模态间分布差异和任务间特征差异，制约通用模型的发展。

Method: 提出了SATA框架，包括解耦专家混合机制（DeMoE）和任务感知多目标跟踪（TaMOT）管线。DeMoE将统一表征任务解耦为跨模态共享知识与特定信息的建模，提升灵活性和泛化能力。TaMOT将所有任务输出规范为带ID实例集合，缓解多任务训练的知识退化。

Result: SATA方法在18个具有挑战性的跟踪与分割基准数据集上表现优异，优于现有方法。

Conclusion: SATA为更通用的视频理解提供了新视角和有效解决方案，能够统一并高效应对多模态、多任务的跟踪和分割需求。

Abstract: Tracking and segmentation play essential roles in video understanding, providing basic positional information and temporal association of objects within video sequences. Despite their shared objective, existing approaches often tackle these tasks using specialized architectures or modality-specific parameters, limiting their generalization and scalability. Recent efforts have attempted to unify multiple tracking and segmentation subtasks from the perspectives of any modality input or multi-task inference. However, these approaches tend to overlook two critical challenges: the distributional gap across different modalities and the feature representation gap across tasks. These issues hinder effective cross-task and cross-modal knowledge sharing, ultimately constraining the development of a true generalist model. To address these limitations, we propose a universal tracking and segmentation framework named SATA, which unifies a broad spectrum of tracking and segmentation subtasks with any modality input. Specifically, a Decoupled Mixture-of-Expert (DeMoE) mechanism is presented to decouple the unified representation learning task into the modeling process of cross-modal shared knowledge and specific information, thus enabling the model to maintain flexibility while enhancing generalization. Additionally, we introduce a Task-aware Multi-object Tracking (TaMOT) pipeline to unify all the task outputs as a unified set of instances with calibrated ID information, thereby alleviating the degradation of task-specific knowledge during multi-task training. SATA demonstrates superior performance on 18 challenging tracking and segmentation benchmarks, offering a novel perspective for more generalizable video understanding.

</details>


### [6] [The Determinant Ratio Matrix Approach to Solving 3D Matching and 2D Orthographic Projection Alignment Tasks](https://arxiv.org/abs/2511.19511)
*Andrew J. Hanson,Sonya M. Hanson*

Main category: cs.CV

TL;DR: 本文关注使用行列式比率矩阵（DRaM）方法解决三维（3D）和二维（2D）正交投影位姿估计问题，并提出了该类问题的新解析解法，扩展了过去基于QR分解和Moore-Penrose伪逆的思路。


<details>
  <summary>Details</summary>
Motivation: 三维参考物体的空间姿态估计（位姿估计）在计算机视觉应用广泛，但针对带噪声数据的二维-三维正交投影问题（OnP）缺乏简洁且普适的闭式解，因此有必要提出新的数学工具和理论框架以提升求解能力。

Method: 该文提出基于行列式比率矩阵（DRaM）的方法，有效求解无误差和有噪声情形下的三维-三维（EnP）和三维-二维正交（OnP）姿态估计问题，并与以往的QR分解、伪逆等方法比较，系统化相关理论。

Result: DRaM方法给出了EnP和OnP问题的全新数学解法，显式推导了其运作机制，同时指出这种方法具有历史可追溯性，并且可推广到N维欧氏空间的通用位姿估计。

Conclusion: DRaM族解法不仅完善了二维、三维空间正交投影位姿估计算法体系，还为更高维度、更多样本噪声下的应用奠定了理论基础，并对相关方法作了统一和推广。

Abstract: Pose estimation is a general problem in computer vision with wide applications. The relative orientation of a 3D reference object can be determined from a 3D rotated version of that object, or from a projection of the rotated object to a 2D planar image. This projection can be a perspective projection (the PnP problem) or an orthographic projection (the OnP problem). We restrict our attention here to the OnP problem and the full 3D pose estimation task (the EnP problem). Here we solve the least squares systems for both the error-free EnP and OnP problems in terms of the determinant ratio matrix (DRaM) approach. The noisy-data case can be addressed with a straightforward rotation correction scheme. While the SVD and optimal quaternion eigensystem methods solve the noisy EnP 3D-3D alignment exactly, the noisy 3D-2D orthographic (OnP) task has no known comparable closed form, and can be solved by DRaM-class methods. We note that while previous similar work has been presented in the literature exploiting both the QR decomposition and the Moore-Penrose pseudoinverse transformations, here we place these methods in a larger context that has not previously been fully recognized in the absence of the corresponding DRaM solution. We term this class of solutions as the DRaM family, and conduct comparisons of the behavior of the families of solutions for the EnP and OnP rotation estimation problems. Overall, this work presents both a new solution to the 3D and 2D orthographic pose estimation problems and provides valuable insight into these classes of problems. With hindsight, we are able to show that our DRaM solutions to the exact EnP and OnP problems possess derivations that could have been discovered in the time of Gauss, and in fact generalize to all analogous N-dimensional Euclidean pose estimation problems.

</details>


### [7] [Single Image to High-Quality 3D Object via Latent Features](https://arxiv.org/abs/2511.19512)
*Huanning Dong,Yinuo Huang,Fan Li,Ping Kuang*

Main category: cs.CV

TL;DR: LatentDreamer是一种新颖的从单张图片生成高质量3D模型的框架，在保持高保真度的同时实现了快速生成。


<details>
  <summary>Details</summary>
Motivation: 现有自动3D生成方法（如image-to-3d）在速度、细节和保真度三者间难以兼顾。作者为此探索能兼顾高效率与高质量的单图3D生成新方法。

Method: 提出LatentDreamer框架，核心是利用预训练变分自编码器(VAE)将3D几何体映射到潜在空间特征，从潜在特征顺序生成粗几何体、精细几何体和逼真纹理。整个流程大大简化3D生成难度，提高效率与质量。

Result: LatentDreamer生成的3D对象与输入图片高度保真，完整流程仅需约70秒。实验结果显示，经过少量训练，系统在性能上与同期方法具备竞争力。

Conclusion: LatentDreamer能够高效、准确地根据单张图片生成具高保真和细节的3D资产，有助于推动数字内容的高效生成。

Abstract: 3D assets are essential in the digital age. While automatic 3D generation, such as image-to-3d, has made significant strides in recent years, it often struggles to achieve fast, detailed, and high-fidelity generation simultaneously. In this work, we introduce LatentDreamer, a novel framework for generating 3D objects from single images. The key to our approach is a pre-trained variational autoencoder that maps 3D geometries to latent features, which greatly reducing the difficulty of 3D generation. Starting from latent features, the pipeline of LatentDreamer generates coarse geometries, refined geometries, and realistic textures sequentially. The 3D objects generated by LatentDreamer exhibit high fidelity to the input images, and the entire generation process can be completed within a short time (typically in 70 seconds). Extensive experiments show that with only a small amount of training, LatentDreamer demonstrates competitive performance compared to contemporary approachs.

</details>


### [8] [Fewer Tokens, Greater Scaling: Self-Adaptive Visual Bases for Efficient and Expansive Representation Learning](https://arxiv.org/abs/2511.19515)
*Shawn Young,Xingyu Zeng,Lijian Xu*

Main category: cs.CV

TL;DR: 本文研究了模型容量与保持图像语义所需最小视觉token数之间的关系，提出了Orthogonal Filtering方法，并发现大模型实际上需要更少的token。


<details>
  <summary>Details</summary>
Motivation: 理解并量化保持图像语义时token数的下限，可有助于高效视觉表征和模型压缩。

Method: 提出将图像token视为视觉语义空间向量，通过最小描述长度原则定义图像的本征语义复杂度，并提出Orthogonal Filtering模块自适应聚合冗余token为一组正交基向量。

Result: 在多种ViT（视觉Transformer）模型上的实验表明：大模型需要显著更少的token即可覆盖图像的语义空间。同时，公开了一个面向视觉长上下文的新数据集。

Conclusion: 提升模型规模反而能减少表达图像语义所需token，为高效视觉表示和token使用策略提供了理论依据。

Abstract: This paper investigates the fundamental relationship between model capacity and the minimal number of visual tokens required to preserve image semantics. Inspired by the Minimum Description Length principle, we reinterpret image tokens as vectors in a visual semantic space and define the intrinsic semantic complexity of an image as the smallest set of basis vectors needed to span this space. Building on this perspective, we propose Orthogonal Filtering, a lightweight module that adaptively clusters redundant tokens into a compact set of orthogonal bases. Through extensive experiments across a range of ViT models, we reveal a consistent token, model scaling law: larger models require significantly fewer tokens to span visual semantic space. Besides, we also contribute a visual long-context dataset.

</details>


### [9] [Connecting the Dots: Training-Free Visual Grounding via Agentic Reasoning](https://arxiv.org/abs/2511.19516)
*Liqin Luo,Guangyao Chen,Xiawu Zheng,Yongxing Dai,Yixiong Zou,Yonghong Tian*

Main category: cs.CV

TL;DR: 该论文提出了GroundingAgent，一种无需任务特定微调即可实现视觉定位的新框架，结合了多种预训练模型，以实现更强的泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉定位方法依赖海量特定标注和微调，导致其泛化能力不足，难以适应新场景或分布外任务。为解决这一问题，作者希望开发无需微调、可直接泛化的视觉定位方法。

Method: GroundingAgent结合了预训练开放词汇目标检测器、多模态大语言模型（MLLM）和大语言模型（LLM），采用结构化、迭代的推理机制，对图像中的候选区域进行逐步语义和空间分析，从而实现视觉定位。整个过程中不依赖任务特定的微调。

Result: GroundingAgent在RefCOCO、RefCOCO+和RefCOCOg等主流基准上，无需微调即可实现65.1%的零样本定位准确率。当在选区阶段使用原始查询文本代替MLLM生成的描述时，准确率可达约90%，接近有监督方法。

Conclusion: GroundingAgent展现了无需微调即可获得高性能的视觉定位能力，并且具有高度可解释性，证明了LLM推理能力对视觉-语言任务的重要作用。该方法促进了视觉与语言模型的进一步集成及实际应用。

Abstract: Visual grounding, the task of linking textual queries to specific regions within images, plays a pivotal role in vision-language integration. Existing methods typically rely on extensive task-specific annotations and fine-tuning, limiting their ability to generalize effectively to novel or out-of-distribution scenarios. To address these limitations, we introduce GroundingAgent, a novel agentic visual grounding framework that operates without any task-specific fine-tuning. GroundingAgent employs a structured, iterative reasoning mechanism that integrates pretrained open-vocabulary object detectors, multimodal large language models (MLLMs), and large language models (LLMs) to progressively refine candidate regions through joint semantic and spatial analyses. Remarkably, GroundingAgent achieves an average zero-shot grounding accuracy of 65.1 % on widely-used benchmarks (RefCOCO, RefCOCO+, RefCOCOg), entirely without fine-tuning. Furthermore, by substituting MLLM-generated captions with the original query texts, the accuracy at the selection stage alone reaches approximately 90 %, closely matching supervised performance and underscoring the critical role of LLM reasoning capabilities. GroundingAgent also offers strong interpretability, transparently illustrating each reasoning step and providing clear insights into its decision-making process.

</details>


### [10] [Towards Efficient VLMs: Information-Theoretic Driven Compression via Adaptive Structural Pruning](https://arxiv.org/abs/2511.19518)
*Zhaoqi Xu,Yingying Zhang,Jian Li,Jianwei Guo,Qiannan Zhu,Hua Huang*

Main category: cs.CV

TL;DR: 本文提出InfoPrune，一种基于信息论的新型视觉-语言大模型（VLMs）结构压缩方法，能在保证性能的前提下显著减少模型计算量并加快推理速度。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言模型规模不断扩大，部署与效率面临严峻挑战。现有压缩策略多为经验法则，缺乏理论信息保真性保障。该论文旨在探索具备信息保真性理论支撑的结构压缩方法，既提升部署可行性，又保证模型效果。

Method: 基于信息瓶颈理论，作者将剪枝建模为任务语义保留与冗余依赖舍弃之间的信息权衡。提出基于熵的有效秩（eRank）衡量注意力头贡献，并基于Kolmogorov–Smirnov距离对比原始与压缩结构间的信息分布差异。实现了统一的稀疏度与信息效率综合准则，并进一步设计了两个子方法：1）以信息损失目标引导的训练式注意力头剪枝，2）无需训练的FFN自适应低秩近似压缩。

Result: 在VQAv2、TextVQA、GQA多模态任务上，InfoPrune可实现3.2倍FLOPS降低和1.8倍加速，性能下降极小。

Conclusion: InfoPrune理论基础扎实、实验效果显著，可有效提升大规模视觉-语言模型在实际部署中的效率和适用性，是高效多模态模型的重要进展。

Abstract: Recent advances in vision-language models (VLMs) have shown remarkable performance across multimodal tasks, yet their ever-growing scale poses severe challenges for deployment and efficiency. Existing compression methods often rely on heuristic importance metrics or empirical pruning rules, lacking theoretical guarantees about information preservation. In this work, we propose InfoPrune, an information-theoretic framework for adaptive structural compression of VLMs. Grounded in the Information Bottleneck principle, we formulate pruning as a trade-off between retaining task-relevant semantics and discarding redundant dependencies. To quantify the contribution of each attention head, we introduce an entropy-based effective rank (eRank) and employ the Kolmogorov--Smirnov (KS) distance to measure the divergence between original and compressed structures. This yields a unified criterion that jointly considers structural sparsity and informational efficiency. Building on this foundation, we further design two complementary schemes: (1) a training-based head pruning guided by the proposed information loss objective, and (2) a training-free FFN compression via adaptive low-rank approximation. Extensive experiments on VQAv2, TextVQA, and GQA demonstrate that InfoPrune achieves up to 3.2x FLOP reduction and 1.8x acceleration with negligible performance degradation, establishing a theoretically grounded and practically effective step toward efficient multimodal large models.

</details>


### [11] [Blinking Beyond EAR: A Stable Eyelid Angle Metric for Driver Drowsiness Detection and Data Augmentation](https://arxiv.org/abs/2511.19519)
*Mathis Wolter,Julie Stephany Berrio Perez,Mao Shan*

Main category: cs.CV

TL;DR: 该论文提出了一种新的眼睑开合度指标（Eyelid Angle, ELA），用于提升驾驶员困倦检测的准确性和鲁棒性，并结合3D建模技术生成合成数据，改善模型训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶员疲劳检测方法受限于二值化的眼状态和二维度量方法（如EAR），对摄像头视角变化不够鲁棒，且自然困倦数据采集困难、高风险。

Method: 提出基于3D人脸关键点计算的眼睑开合角（ELA）指标，能够稳定描述眼睑运动；基于ELA设计眨眼检测框架，提取闭眼时长等时序特征，增强困倦关联；利用Blender 3D动画生成可控噪声和动态的合成眨眼数据，扩展困倦检测训练集。

Result: 实验表明ELA在视角变化下比传统EAR指标方差更小，能准确检测眨眼行为；通过合成数据增强提升了困倦状态识别的多样性和泛化能力。

Conclusion: ELA是稳定可靠的生物特征度量，同时也是生成大规模驾驶状态监测数据的有力工具，有助于提升ADAS等驾驶安全系统的性能。

Abstract: Detecting driver drowsiness reliably is crucial for enhancing road safety and supporting advanced driver assistance systems (ADAS). We introduce the Eyelid Angle (ELA), a novel, reproducible metric of eye openness derived from 3D facial landmarks. Unlike conventional binary eye state estimators or 2D measures, such as the Eye Aspect Ratio (EAR), the ELA provides a stable geometric description of eyelid motion that is robust to variations in camera angle. Using the ELA, we design a blink detection framework that extracts temporal characteristics, including the closing, closed, and reopening durations, which are shown to correlate with drowsiness levels. To address the scarcity and risk of collecting natural drowsiness data, we further leverage ELA signals to animate rigged avatars in Blender 3D, enabling the creation of realistic synthetic datasets with controllable noise, camera viewpoints, and blink dynamics. Experimental results in public driver monitoring datasets demonstrate that the ELA offers lower variance under viewpoint changes compared to EAR and achieves accurate blink detection. At the same time, synthetic augmentation expands the diversity of training data for drowsiness recognition. Our findings highlight the ELA as both a reliable biometric measure and a powerful tool for generating scalable datasets in driver state monitoring.

</details>


### [12] [VideoChat-M1: Collaborative Policy Planning for Video Understanding via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.19524)
*Boyu Chen,Zikang Wang,Zhengrong Yue,Kainan Yan,Chenyun Yu,Yi Huang,Zijun Liu,Yafei Wen,Xiaoxin Chen,Yang Liu,Peng Li,Yali Wang*

Main category: cs.CV

TL;DR: 论文提出了VideoChat-M1系统，通过多智能体协作和动态策略规划，显著提升了视频理解任务的能力，在多个基准测试中取得了SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型的多智能体视频理解系统普遍采用静态、不可学习的工具调用机制，限制了模型对时空复杂视频的感知和推理能力。为克服这一缺陷，需要更灵活且能自我优化的协作机制。

Method: VideoChat-M1引入了协作策略规划（CPP）范式，由多个策略代理组成，通过策略生成、策略执行和策略通信三个步骤协同完成视频理解。策略代理基于用户指令自适应生成和调整各自的工具调用策略，并通过多智能体强化学习联合优化性能。

Result: 在8个基准、4项任务上，VideoChat-M1取得了最优表现。其中在LongVideoBench上，相较Gemini 2.5 pro提升3.6%，相较GPT-4o提升15.6%。

Conclusion: 协作多智能体和动态、可学习的工具调用机制，能极大提升多模态视频理解的效果，VideoChat-M1证明了该协作范式的优越性。

Abstract: By leveraging tool-augmented Multimodal Large Language Models (MLLMs), multi-agent frameworks are driving progress in video understanding. However, most of them adopt static and non-learnable tool invocation mechanisms, which limit the discovery of diverse clues essential for robust perception and reasoning regarding temporally or spatially complex videos. To address this challenge, we propose a novel Multi-agent system for video understanding, namely VideoChat-M1. Instead of using a single or fixed policy, VideoChat-M1 adopts a distinct Collaborative Policy Planning (CPP) paradigm with multiple policy agents, which comprises three key processes. (1) Policy Generation: Each agent generates its unique tool invocation policy tailored to the user's query; (2) Policy Execution: Each agent sequentially invokes relevant tools to execute its policy and explore the video content; (3) Policy Communication: During the intermediate stages of policy execution, agents interact with one another to update their respective policies. Through this collaborative framework, all agents work in tandem, dynamically refining their preferred policies based on contextual insights from peers to effectively respond to the user's query. Moreover, we equip our CPP paradigm with a concise Multi-Agent Reinforcement Learning (MARL) method. Consequently, the team of policy agents can be jointly optimized to enhance VideoChat-M1's performance, guided by both the final answer reward and intermediate collaborative process feedback. Extensive experiments demonstrate that VideoChat-M1 achieves SOTA performance across eight benchmarks spanning four tasks. Notably, on LongVideoBench, our method outperforms the SOTA model Gemini 2.5 pro by 3.6% and GPT-4o by 15.6%.

</details>


### [13] [Perceptual Taxonomy: Evaluating and Guiding Hierarchical Scene Reasoning in Vision-Language Models](https://arxiv.org/abs/2511.19526)
*Jonathan Lee,Xingrui Wang,Jiawei Peng,Luoxin Ye,Zehan Zheng,Tiezheng Zhang,Tao Wang,Wufei Ma,Siyi Chen,Yu-Cheng Chou,Prakhar Kaushik,Alan Yuille*

Main category: cs.CV

TL;DR: 本文提出了Perceptual Taxonomy基准，旨在评价视觉-语言模型在物理和属性推理方面的能力，揭示主流模型在结构化理解任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言基准主要集中于基本识别任务或图文对齐，缺乏对模型进行深入、结构化视觉推理能力的全面评估。而人类对场景的理解包括对物体、空间配置，以及与任务相关属性的推理，这一能力在人类认知中非常关键。为弥补这一评测空白，作者提出设立新的评测基准。

Method: 作者标注了3173个物体，覆盖材料、可供性、功能、物理属性四大类84种细致属性。基于这些标注，构建了一个包含5802张图片的多选题基准（涵盖合成和真实域），设计了28033个基于模板的问题（对象描述、空间推理、属性匹配、分类推理四类），并人工设计了50个专家级问题以全面测试推理能力。

Result: 实验表明，主流视觉-语言模型在识别任务中表现良好，但在属性推理、尤其是需要多步结构化推理的问题上，准确率下降10%到20%。进一步地，通过为模型提供来源于模拟场景的推理示例，可以提升其在真实以及专家设计问题上的表现。

Conclusion: 现有模型更多依赖模式匹配，进行结构化视觉推理时能力明显不足。Perceptual Taxonomy为相关能力的评估和提升提供了新的测试基准和方法。

Abstract: We propose Perceptual Taxonomy, a structured process of scene understanding that first recognizes objects and their spatial configurations, then infers task-relevant properties such as material, affordance, function, and physical attributes to support goal-directed reasoning. While this form of reasoning is fundamental to human cognition, current vision-language benchmarks lack comprehensive evaluation of this ability and instead focus on surface-level recognition or image-text alignment.
  To address this gap, we introduce Perceptual Taxonomy, a benchmark for physically grounded visual reasoning. We annotate 3173 objects with four property families covering 84 fine-grained attributes. Using these annotations, we construct a multiple-choice question benchmark with 5802 images across both synthetic and real domains. The benchmark contains 28033 template-based questions spanning four types (object description, spatial reasoning, property matching, and taxonomy reasoning), along with 50 expert-crafted questions designed to evaluate models across the full spectrum of perceptual taxonomy reasoning.
  Experimental results show that leading vision-language models perform well on recognition tasks but degrade by 10 to 20 percent on property-driven questions, especially those requiring multi-step reasoning over structured attributes. These findings highlight a persistent gap in structured visual understanding and the limitations of current models that rely heavily on pattern matching. We also show that providing in-context reasoning examples from simulated scenes improves performance on real-world and expert-curated questions, demonstrating the effectiveness of perceptual-taxonomy-guided prompting.

</details>


### [14] [MapRF: Weakly Supervised Online HD Map Construction via NeRF-Guided Self-Training](https://arxiv.org/abs/2511.19527)
*Hongyu Lyu,Thomas Monninger,Julie Stephany Berrio Perez,Mao Shan,Zhenxing Ming,Stewart Worrall*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MapRF的弱监督高精地图构建框架，仅利用2D图像标签实现3D地图构建，减少对昂贵3D标注的依赖，显著降低成本并提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶高精地图的在线构建通常依赖昂贵且耗时的3D标注，制约了其大范围推广和适应复杂多变环境的能力。作者希望开发一种无需或极少依赖3D标注、支持大规模推广的自动化地图构建方法。

Method: 提出MapRF框架，主要创新包括：1）通过基于地图预测条件的Neural Radiance Fields (NeRF)模块重建一致的3D结构和语义，生成高质量伪标签；2）伪标签用于自训练迭代优化地图网络，使其逐步提升；3）为抑制自训练误差积累，设计Map-to-Ray Matching策略，将地图预测与从2D标签推导的相机射线对齐。

Result: 在Argoverse 2和nuScenes数据集上，MapRF在仅用2D标签的情况下，性能达到全监督方法的约75%，且优于其它单纯2D标签方法，显示出很强的竞争力。

Conclusion: MapRF能大幅减少对高成本3D标注的依赖，有效提升高精地图的在线自动化生成能力，具备推广至实际自动驾驶系统的潜力。

Abstract: Autonomous driving systems benefit from high-definition (HD) maps that provide critical information about road infrastructure. The online construction of HD maps offers a scalable approach to generate local maps from on-board sensors. However, existing methods typically rely on costly 3D map annotations for training, which limits their generalization and scalability across diverse driving environments. In this work, we propose MapRF, a weakly supervised framework that learns to construct 3D maps using only 2D image labels. To generate high-quality pseudo labels, we introduce a novel Neural Radiance Fields (NeRF) module conditioned on map predictions, which reconstructs view-consistent 3D geometry and semantics. These pseudo labels are then iteratively used to refine the map network in a self-training manner, enabling progressive improvement without additional supervision. Furthermore, to mitigate error accumulation during self-training, we propose a Map-to-Ray Matching strategy that aligns map predictions with camera rays derived from 2D labels. Extensive experiments on the Argoverse 2 and nuScenes datasets demonstrate that MapRF achieves performance comparable to fully supervised methods, attaining around 75% of the baseline while surpassing several approaches using only 2D labels. This highlights the potential of MapRF to enable scalable and cost-effective online HD map construction for autonomous driving.

</details>


### [15] [Vidi2: Large Multimodal Models for Video Understanding and Creation](https://arxiv.org/abs/2511.19529)
*Vidi Team,Celong Liu,Chia-Wen Kuo,Chuang Huang,Dawei Du,Fan Chen,Guang Chen,Haoji Zhang,Haojun Zhao,Lingxi Zhang,Lu Guo,Lusha Li,Longyin Wen,Qihang Fan,Qingyu Chen,Rachel Deng,Sijie Zhu,Stuart Siew,Tong Jin,Weiyan Tao,Wen Zhong,Xiaohui Shen,Xin Gu,Zhenfang Chen,Zuhua Lin*

Main category: cs.CV

TL;DR: Vidi2模型在视频的时空定位与理解、视频问答等多模态任务上取得了领先性能，并提出了新的基准数据集和评价指标。


<details>
  <summary>Details</summary>
Motivation: 网络视频需求大增，推动高质量、可扩展的视频生成技术发展。现有模型在多模态时序检索上虽有突破，但在细粒度时空理解和复杂视频编辑应用中仍有限。

Method: Vidi2模型提升了时空定位（STG）能力，能在给定文本查询下精确标定视频的对应时间段及目标物体的位置（边界框）；还扩展至视频问答，支持多模态综合推理。引入新的数据集VUE-STG，涵盖更长视频、更自然的名词短语查询，高精度人工标注和精炼的评测指标。此外，升级了原有的VUE-TR基准集。

Result: Vidi2在VUE-TR-V2和VUE-STG数据集上的表现显著优于Gemini 3 Pro (Preview)、GPT-5等顶尖商用系统，在视频问答任务上也达到了开源同规模模型的领先或并列水平。

Conclusion: Vidi2推动了视频多模态理解、时空定位、复杂编辑等应用的发展，为未来智能视频内容处理和多模态推理奠定了坚实基础。

Abstract: Video has emerged as the primary medium for communication and creativity on the Internet, driving strong demand for scalable, high-quality video production. Vidi models continue to evolve toward next-generation video creation and have achieved state-of-the-art performance in multimodal temporal retrieval (TR). In its second release, Vidi2 advances video understanding with fine-grained spatio-temporal grounding (STG) and extends its capability to video question answering (Video QA), enabling comprehensive multimodal reasoning. Given a text query, Vidi2 can identify not only the corresponding timestamps but also the bounding boxes of target objects within the output time ranges. This end-to-end spatio-temporal grounding capability enables potential applications in complex editing scenarios, such as plot or character understanding, automatic multi-view switching, and intelligent, composition-aware reframing and cropping. To enable comprehensive evaluation of STG in practical settings, we introduce a new benchmark, VUE-STG, which offers four key improvements over existing STG datasets: 1) Video duration: spans from roughly 10s to 30 mins, enabling long-context reasoning; 2) Query format: queries are mostly converted into noun phrases while preserving sentence-level expressiveness; 3) Annotation quality: all ground-truth time ranges and bounding boxes are manually annotated with high accuracy; 4) Evaluation metric: a refined vIoU/tIoU/vIoU-Intersection scheme. In addition, we upgrade the previous VUE-TR benchmark to VUE-TR-V2, achieving a more balanced video-length distribution and more user-style queries. Remarkably, the Vidi2 model substantially outperforms leading proprietary systems, such as Gemini 3 Pro (Preview) and GPT-5, on both VUE-TR-V2 and VUE-STG, while achieving competitive results with popular open-source models with similar scale on video QA benchmarks.

</details>


### [16] [Cross-Domain Generalization of Multimodal LLMs for Global Photovoltaic Assessment](https://arxiv.org/abs/2511.19537)
*Muhao Guo,Yang Weng*

Main category: cs.CV

TL;DR: 该论文提出用多模态大语言模型（LLM）进行全球分布式光伏（PV）系统的自动识别及量化，克服传统模型对大量标注数据和泛化性差的问题。实验结果表明，所提模型在跨域任务中表现优异，性能衰减最小。


<details>
  <summary>Details</summary>
Motivation: 随着分布式光伏系统迅速扩张，许多装置未被记录，给电网管理带来挑战。现有基于卫星图像的检测方法依赖大量人工标注数据，且模型很难泛化到不同地区。作者希望找到更鲁棒、可迁移的方法，高效、自动地实现全球大规模光伏检测和量化。

Method: 研究通过结构化提示和微调技巧，让多模态大语言模型能够同时进行光伏系统的检测、定位和定量，将任务统一在一个框架之下。方法在多个不同区域数据集上进行跨域泛化评估，主要采用$Δ$F1指标来衡量模型在未见过地区的性能表现变化。

Result: 实验结果显示，所提出的多模态LLM模型，相较于传统CNN、U-Net和Transformer等方法，在不同区域的泛化性最强，跨域任务中性能衰减最小，整体性能最优。

Conclusion: 多模态大语言模型在遭遇领域转移时展现出强大的鲁棒性和可扩展性，有望用于全球范围的光伏系统识别、量化与解释，对实现高效、自动化的电力资产管理具有重要意义。

Abstract: The rapid expansion of distributed photovoltaic (PV) systems poses challenges for power grid management, as many installations remain undocumented. While satellite imagery provides global coverage, traditional computer vision (CV) models such as CNNs and U-Nets require extensive labeled data and fail to generalize across regions. This study investigates the cross-domain generalization of a multimodal large language model (LLM) for global PV assessment. By leveraging structured prompts and fine-tuning, the model integrates detection, localization, and quantification within a unified schema. Cross-regional evaluation using the $Δ$F1 metric demonstrates that the proposed model achieves the smallest performance degradation across unseen regions, outperforming conventional CV and transformer baselines. These results highlight the robustness of multimodal LLMs under domain shift and their potential for scalable, transferable, and interpretable global PV mapping.

</details>


### [17] [Studying Maps at Scale: A Digital Investigation of Cartography and the Evolution of Figuration](https://arxiv.org/abs/2511.19538)
*Remi Petitpierre*

Main category: cs.CV

TL;DR: 本论文利用大规模地图数据集，结合自动化识别技术，系统分析了地图作为文化遗产和语义符号系统的演变规律，揭示了地图与政治、经济、文化变迁的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 尽管地图数字化和自动内容提取技术发展迅速，但现有研究很少关注地图的历史背景及其作为文化符号系统的特性。作者希望填补地图自动分析与历史制图学、文化视角之间的空白，挖掘地图背后的社会和政治意义。

Method: 本研究整合了38个数字目录的77万余条地图记录与近10万张数字化地图图像，跨越1492-1948年。方法包括数据归一化、地理和时间结构分析，结合历史政治背景解读，并引入语义分割和目标检测模型识别土地类型及制图符号。此外，研究还分析了地图合作传播机制及符号系统的空间扩散。

Result: 结果显示：地图的地理重点与历史政治动态（如大西洋三角贸易、殖民扩张）密切相关；军事冲突对地图出版量有显著影响。语义分割显示地图设计具有居中和语义对称性。符号分析发现63M个符号和25M片段组成局部一致的系统，并揭示了如地形描绘方式的历史演变。合作与扩散分析突出主要城市、大型机构及权威对制图标准传播的作用。

Conclusion: 大规模量化分析证明，地图不仅是地理工具，更深刻反映了历史、政治、文化预期和社会权力结构。自动化与历史方法结合为理解地图遗产和符号演变提供了新范式。

Abstract: This thesis presents methods and datasets to investigate cartographic heritage on a large scale and from a cultural perspective. Heritage institutions worldwide have digitized more than one million maps, and automated techniques now enable large-scale recognition and extraction of map content. Yet these methods have engaged little with the history of cartography, or the view that maps are semantic-symbolic systems, and cultural objects reflecting political and epistemic expectations. This work leverages a diverse corpus of 771,561 map records and 99,715 digitized images aggregated from 38 digital catalogs. After normalization, the dataset includes 236,925 contributors and spans six centuries, from 1492 to 1948. These data make it possible to chart geographic structures and the global chronology of map publication. The spatial focus of cartography is analyzed in relation to political dynamics, evidencing links between Atlantic maritime charting, the triangular trade, and colonial expansion. Further results document the progression of national, domestic focus and the impact of military conflicts on publication volumes. The research introduces semantic segmentation techniques and object detection models for the generic recognition of land classes and cartographic signs, trained on annotated data and synthetic images. The analysis of land classes shows that maps are designed images whose framing and composition emphasize features through centering and semantic symmetries. The study of cartographic figuration encodes 63 M signs and 25 M fragments into a latent visual space, revealing figurative shifts such as the replacement of relief hachures by terrain contours and showing that signs tend to form locally consistent systems. Analyses of collaboration and diffusion highlight the role of legitimacy, larger actors, and major cities in the spread of figurative norms and semiotic cultures.

</details>


### [18] [Proxy-Free Gaussian Splats Deformation with Splat-Based Surface Estimation](https://arxiv.org/abs/2511.19542)
*Jaeyeong Kim,Seungwoo Yoo,Minhyuk Sung*

Main category: cs.CV

TL;DR: 本文提出了一种新的无需代理物体的高斯溅射（Gaussian splats, GS）形变方法——SpLap，基于创新的表面感知溅射图计算Laplacian算子，实现更高质量、保持细节与拓扑的形变。


<details>
  <summary>Details</summary>
Motivation: 现有GS形变方法多依赖于代理物体（如笼子、网格），容易受到代理质量影响并带来额外计算开销。直接将Splats当作点云进行Laplacian形变又难以有效捕捉表面信息。为解决上述问题，作者提出表面感知的splats图，提升形变自然性与细节保持能力。

Method: 作者通过定义相邻splats（不仅基于中心距离，也基于相交关系）构建表面感知splats图。由此计算得到Laplacian算子，用于形变。创新性地还引入高斯核自适应技术，在形变过程中增强表面结构保持，提升形变后渲染质量。

Result: 在ShapeNet、Objaverse、Sketchfab及NeRF-Synthetic等数据集上，对50个复杂物体进行实验，显示本方法在形变自然性、细节保持和渲染质量上均优于现有基于代理和无代理的基线方法。

Conclusion: SpLap无需任何代理物体，通过新颖的表面感知splats图和高斯核自适应，从根本上提升了GS形变效果，实验验证了其稳定性和优越性能。

Abstract: We introduce SpLap, a proxy-free deformation method for Gaussian splats (GS) based on a Laplacian operator computed from our novel surface-aware splat graph. Existing approaches to GS deformation typically rely on deformation proxies such as cages or meshes, but they suffer from dependency on proxy quality and additional computational overhead. An alternative is to directly apply Laplacian-based deformation techniques by treating splats as point clouds. However, this often fail to properly capture surface information due to lack of explicit structure. To address this, we propose a novel method that constructs a surface-aware splat graph, enabling the Laplacian operator derived from it to support more plausible deformations that preserve details and topology. Our key idea is to leverage the spatial arrangement encoded in splats, defining neighboring splats not merely by the distance between their centers, but by their intersections. Furthermore, we introduce a Gaussian kernel adaptation technique that preserves surface structure under deformation, thereby improving rendering quality after deformation. In our experiments, we demonstrate the superior performance of our method compared to both proxy-based and proxy-free baselines, evaluated on 50 challenging objects from the ShapeNet, Objaverse, and Sketchfab datasets, as well as the NeRF-Synthetic dataset. Code is available at https://github.com/kjae0/SpLap.

</details>


### [19] [Think First, Assign Next (ThiFAN-VQA): A Two-stage Chain-of-Thought Framework for Post-Disaster Damage Assessment](https://arxiv.org/abs/2511.19557)
*Ehsan Karimi,Nhut Le,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: 本论文提出了一种名为ThiFAN-VQA的视觉问答框架，有效提升了灾后损失评估的准确性、可解释性和适应性。


<details>
  <summary>Details</summary>
Motivation: 灾后损失评估需要快速、准确且大规模的数据分析。现有AI方法受限于标注数据昂贵、样本有限且回答空间固定，难以灵活应对新场景和需求。

Method: 提出了ThiFAN-VQA，两阶段推理框架：第一阶段通过ICL和CoT生成可解释的推理轨迹，第二阶段答案选择模块筛选最具上下文相关性和准确性的答案。同时结合定制检索系统及领域特定提示，实现零样本与有监督方法的优势融合。

Result: 在FloodNet和RescueNet-VQA两大灾害数据集上实验，ThiFAN-VQA在准确率、可解释性与适应性方面明显优于现有方法。

Conclusion: ThiFAN-VQA为灾害后视觉问答和损失评估提供了更灵活、一致与可解释的解决方案，有望实际应用于应急响应和恢复工作中。

Abstract: Timely and accurate assessment of damages following natural disasters is essential for effective emergency response and recovery. Recent AI-based frameworks have been developed to analyze large volumes of aerial imagery collected by Unmanned Aerial Vehicles, providing actionable insights rapidly. However, creating and annotating data for training these models is costly and time-consuming, resulting in datasets that are limited in size and diversity. Furthermore, most existing approaches rely on traditional classification-based frameworks with fixed answer spaces, restricting their ability to provide new information without additional data collection or model retraining. Using pre-trained generative models built on in-context learning (ICL) allows for flexible and open-ended answer spaces. However, these models often generate hallucinated outputs or produce generic responses that lack domain-specific relevance. To address these limitations, we propose ThiFAN-VQA, a two-stage reasoning-based framework for visual question answering (VQA) in disaster scenarios. ThiFAN-VQA first generates structured reasoning traces using chain-of-thought (CoT) prompting and ICL to enable interpretable reasoning under limited supervision. A subsequent answer selection module evaluates the generated responses and assigns the most coherent and contextually accurate answer, effectively improve the model performance. By integrating a custom information retrieval system, domain-specific prompting, and reasoning-guided answer selection, ThiFAN-VQA bridges the gap between zero-shot and supervised methods, combining flexibility with consistency. Experiments on FloodNet and RescueNet-VQA, UAV-based datasets from flood- and hurricane-affected regions, demonstrate that ThiFAN-VQA achieves superior accuracy, interpretability, and adaptability for real-world post-disaster damage assessment tasks.

</details>


### [20] [HunyuanOCR Technical Report](https://arxiv.org/abs/2511.19575)
*Hunyuan Vision Team,Pengyuan Lyu,Xingyu Wan,Gengluo Li,Shangpin Peng,Weinong Wang,Liang Wu,Huawen Shen,Yu Zhou,Canhui Tang,Qi Yang,Qiming Peng,Bin Luo,Hower Yang,Houwen Peng,Hongming Yang,Senhao Xie,Binghong Wu,Mana Yang,Sergey Wang,Raccoon Liu,Dick Zhu,Jie Jiang,Linus,Han Hu,Chengquan Zhang*

Main category: cs.CV

TL;DR: HunyuanOCR是一款商用级、开源且轻量级（1B参数）的视觉-语言模型，专注于OCR任务，整体性能超过同类产品和更大模型（如Qwen3-VL-4B），在多个主流任务和挑战中取得领先。


<details>
  <summary>Details</summary>
Motivation: 目前OCR模型多数受限于功能单一或计算效率低，难以兼顾多任务和高性能。论文旨在开发一种统一、高效、轻量、端到端的OCR解决方案，以简化部署并提升准确性。

Method: 采用原生Vision Transformer与轻量级LLM，通过MLP适配器连接。实现端到端架构，移除传统管线的预处理模块，利用高质量数据和强化学习（RL）优化性能。模型支持检测、解析、信息抽取、视觉问答和翻译等多任务。

Result: HunyuanOCR在文本检测、解析、信息抽取、图文翻译等感知与语义任务均优于主流商用API、传统管线及更大参数模型，在多个挑战（如ICDAR 2025 DIMT小模型赛道、OCRBench <3B参数）取得SOTA表现。

Conclusion: HunyuanOCR以全面、统一、高效的端到端OCR能力及领先性能，为学术前沿和工业应用提供坚实基础，支持开源和高效部署，推动OCR技术进步。

Abstract: This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.
  HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow "OCR expert models" and inefficient "General VLMs". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.
  HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.

</details>


### [21] [Leveraging Unlabeled Scans for NCCT Image Segmentation in Early Stroke Diagnosis: A Semi-Supervised GAN Approach](https://arxiv.org/abs/2511.19576)
*Maria Thoma,Michalis A. Savelonas,Dimitris K. Iakovidis*

Main category: cs.CV

TL;DR: 本论文提出了一种基于生成对抗网络(GAN)的半监督分割方法，用于在非对比CT影像上精准分割早期缺血性脑卒中区域。该方法利用少量有标注和大量无标注的数据，有效提升了早期卒中识别性能。


<details>
  <summary>Details</summary>
Motivation: 缺血性脑卒中需要迅速诊断，而临床常用的无对比CT难以在早期发现微小的缺血变化，影响了及时干预。此研究旨在利用先进自动分割方法弥补NCCT影像在早期病灶检测上的不足。

Method: 研究提出了一种半监督GAN框架，能够利用有限的有标签NCCT样本和大量无标签样本进行训练。方法综合使用Dice损失、交叉熵损失、特征对齐损失和自训练损失，以提高对早期卒中微小病灶的分割能力。

Result: 在公开的急性缺血性脑卒中数据集（AISD）上实验，所提方法在识别和勾画早期卒中灶方面表现优异，并显示出减少人工标注负担和提升诊断效率的潜力。

Conclusion: 该方法有望提高卒中早期病灶的检出率，支持临床高效决策，推动卒中救治流程优化。

Abstract: Ischemic stroke is a time-critical medical emergency where rapid diagnosis is essential for improving patient outcomes. Non-contrast computed tomography (NCCT) serves as the frontline imaging tool, yet it often fails to reveal the subtle ischemic changes present in the early, hyperacute phase. This limitation can delay crucial interventions. To address this diagnostic challenge, we introduce a semi-supervised segmentation method using generative adversarial networks (GANs) to accurately delineate early ischemic stroke regions. The proposed method employs an adversarial framework to effectively learn from a limited number of annotated NCCT scans, while simultaneously leveraging a larger pool of unlabeled scans. By employing Dice loss, cross-entropy loss, a feature matching loss and a self-training loss, the model learns to identify and delineate early infarcts, even when they are faint or their size is small. Experiments on the publicly available Acute Ischemic Stroke Dataset (AISD) demonstrate the potential of the proposed method to enhance diagnostic capabilities, reduce the burden of manual annotation, and support more efficient clinical decision-making in stroke care.

</details>


### [22] [Multiscale Vector-Quantized Variational Autoencoder for Endoscopic Image Synthesis](https://arxiv.org/abs/2511.19578)
*Dimitrios E. Diamantis,Dimitris K. Iakovidis*

Main category: cs.CV

TL;DR: 本文提出了一种新型的多尺度向量量化变分自编码器（MSVQ-VAE）方法，用于合成高质量的无线胶囊内镜（WCE）图像，尤其是带有异常表现的图像，以解决医学影像数据稀缺问题，并提升临床决策支持系统的辅助能力。


<details>
  <summary>Details</summary>
Motivation: 无线胶囊内镜检查会产生大量图像，人工筛查耗时且依赖经验。深度学习临床决策支持系统可助力筛查，但因隐私及标注成本导致医学影像大数据稀缺，严重限制了辅助系统的开发与应用。合成影像生成方法有望缓解该问题。

Method: 论文提出了多尺度向量量化VAE（MSVQ-VAE）模型，该方法在VAE基础上加入多尺度机制用于更好地生成视觉多样性高的WCE合成图像，并实现条件生成，可向正常图像中可控引入多种异常表现（如息肉、血管、炎症）。同时用于扩充训练数据，提升辅助诊断模型的能力。

Result: 通过各种异常类型的WCE图像合成实验，并以合成数据辅助训练CDS分类器，与仅用真实数据训练的分类器性能接近，表明合成数据具有很好的实用价值。同时方法在合成异常表现方面表现优异，且具备通用性。

Conclusion: MSVQ-VAE不仅能高质量生成含异常的医学图像，且对辅助诊断模型提升效果显著，可缓解医学数据获取难题，并有望推广至其他医学多媒体领域。

Abstract: Gastrointestinal (GI) imaging via Wireless Capsule Endoscopy (WCE) generates a large number of images requiring manual screening. Deep learning-based Clinical Decision Support (CDS) systems can assist screening, yet their performance relies on the existence of large, diverse, training medical datasets. However, the scarcity of such data, due to privacy constraints and annotation costs, hinders CDS development. Generative machine learning offers a viable solution to combat this limitation. While current Synthetic Data Generation (SDG) methods, such as Generative Adversarial Networks and Variational Autoencoders have been explored, they often face challenges with training stability and capturing sufficient visual diversity, especially when synthesizing abnormal findings. This work introduces a novel VAE-based methodology for medical image synthesis and presents its application for the generation of WCE images. The novel contributions of this work include a) multiscale extension of the Vector Quantized VAE model, named as Multiscale Vector Quantized Variational Autoencoder (MSVQ-VAE); b) unlike other VAE-based SDG models for WCE image generation, MSVQ-VAE is used to seamlessly introduce abnormalities into normal WCE images; c) it enables conditional generation of synthetic images, enabling the introduction of different types of abnormalities into the normal WCE images; d) it performs experiments with a variety of abnormality types, including polyps, vascular and inflammatory conditions. The utility of the generated images for CDS is assessed via image classification. Comparative experiments demonstrate that training a CDS classifier using the abnormal images generated by the proposed methodology yield comparable results with a classifier trained with only real data. The generality of the proposed methodology promises its applicability to various domains related to medical multimedia.

</details>


### [23] [SkillSight: Efficient First-Person Skill Assessment with Gaze](https://arxiv.org/abs/2511.19629)
*Chi Hsuan Wu,Kumar Ashutosh,Kristen Grauman*

Main category: cs.CV

TL;DR: 本文提出了一种便于穿戴设备实施的技能评估方法SkillSight，仅通过注视点（gaze）信息高效评估技能水平，实现极低能耗，且精度接近最优。


<details>
  <summary>Details</summary>
Motivation: 随着智能眼镜等第一人称感知设备发展，自动化评估现实技能成为提升技能学习体验的关键瓶颈，但现有方法依赖持续视频处理，功耗高、资源消耗大。作者提出能否通过更低成本的方式实现高准确率的自动技能评估。

Method: 作者提出SkillSight的两阶段框架。第一阶段，使用视频和注视点数据联合建模，训练“教师模型”。第二阶段，将知识蒸馏到仅需注视点输入的“学生模型”，推理时仅需少量传感器数据，不用实时视频，大大降低功耗。

Result: 在烹饪、音乐、运动三类现实数据集上，SkillSight教师模型达到现有最优性能；蒸馏后的学生模型在仅用注视点数据情况下，表现依然优良，能耗相比传统方法降至1/73。实验首次证实注视点对多场景技能评估极有价值。

Conclusion: SkillSight证明，无需视频、只利用注视点即可高效评估技能，有望推动AI辅助的实地技能学习普及，为便携低功耗设备上的技能评估带来新方向。

Abstract: Egocentric perception on smart glasses could transform how we learn new skills in the physical world, but automatic skill assessment remains a fundamental technical challenge. We introduce SkillSight for power-efficient skill assessment from first-person data. Central to our approach is the hypothesis that skill level is evident not only in how a person performs an activity (video), but also in how they direct their attention when doing so (gaze). Our two-stage framework first learns to jointly model gaze and egocentric video when predicting skill level, then distills a gaze-only student model. At inference, the student model requires only gaze input, drastically reducing power consumption by eliminating continuous video processing. Experiments on three datasets spanning cooking, music, and sports establish, for the first time, the valuable role of gaze in skill understanding across diverse real-world settings. Our SkillSight teacher model achieves state-of-the-art performance, while our gaze-only student variant maintains high accuracy using 73x less power than competing methods. These results pave the way for in-the-wild AI-supported skill learning.

</details>


### [24] [On the Utility of Foundation Models for Fast MRI: Vision-Language-Guided Image Reconstruction](https://arxiv.org/abs/2511.19641)
*Ruimin Feng,Xingxin He,Ronald Mercer,Zachary Stewart,Fang Liu*

Main category: cs.CV

TL;DR: 作者提出利用视觉-语言基础模型引入高层语义信息，提升欠采样MRI重建的质量，并通过对比实验验证了新框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统MRI重建方法主要依赖低层图像特征或者简单先验，难以充分利用高层语义信息，限制了欠采样下重建图像的结构和感知质量。作者希望通过视觉-语言模型引入新的高层语义先验，促进更真实、更有结构感的MRI图像重建。

Method: 作者实现了一个语义分布引导的重建框架，借助预训练的视觉-语言基础模型，将重建图像和辅助信息编码为高层语义特征，并通过对比损失函数对齐目标语义分布。这种方法可以结合多模态语义先验（图像或图像-文字信息），与现有深度学习重建方法兼容。

Result: 在膝盖和脑部MRI数据集上，语义先验指导下的重建能更好保留解剖结构，感知质量优于传统正则化方法（如更低LPIPS，更高Tenengrad分数）。同时，图像-语言辅助信息进一步增强了对重建属性的高级控制能力。各项指标均表明所提对比损失目标有效地优化了重建特征的语义分布。

Conclusion: 利用视觉-语言基础模型进行语义空间优化，能够显著提升欠采样MRI的重建质量，实现更优的结构保真和可感知性。

Abstract: Purpose: To investigate whether a vision-language foundation model can enhance undersampled MRI reconstruction by providing high-level contextual information beyond conventional priors. Methods: We proposed a semantic distribution-guided reconstruction framework that uses a pre-trained vision-language foundation model to encode both the reconstructed image and auxiliary information into high-level semantic features. A contrastive objective aligns the reconstructed representation with the target semantic distribution, ensuring consistency with high-level perceptual cues. The proposed objective works with various deep learning-based reconstruction methods and can flexibly incorporate semantic priors from multimodal sources. To test the effectiveness of these semantic priors, we evaluated reconstruction results guided by priors derived from either image-only or image-language auxiliary information. Results: Experiments on knee and brain datasets demonstrate that semantic priors from images preserve fine anatomical structures and achieve superior perceptual quality, as reflected in lower LPIPS values, higher Tenengrad scores, and improved scores in the reader study, compared with conventional regularization. The image-language information further expands the semantic distribution and enables high-level control over reconstruction attributes. Across all evaluations, the contrastive objective consistently guided the reconstructed features toward the desired semantic distributions while maintaining data fidelity, demonstrating the effectiveness of the proposed optimization framework. Conclusion: The study highlights that vision-language foundation models can improve undersampled MRI reconstruction through semantic-space optimization.

</details>


### [25] [Navigating Gigapixel Pathology Images with Large Multimodal Models](https://arxiv.org/abs/2511.19652)
*Thomas A. Buckley,Kian R. Weihrauch,Katherine Latham,Andrew Z. Zhou,Padmini A. Manrai,Arjun K. Manrai*

Main category: cs.CV

TL;DR: 本论文提出了一种新框架GIANT，使得通用多模态大模型可迭代浏览和理解超高分辨率的病理全切片图像，并在全新基准MultiPathQA上显著优于传统方法和一些专业模型。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大模型被广泛用于临床辅助，但在医学图像（特别是病理切片）解读方面表现不佳，主要因以往方法仅利用低分辨率缩略图或随机小块，低估了模型潜力。作者希望探索大模型能否通过更拟人化方式高效理解全切片图像。

Method: 提出GIANT框架，允许大模型像病理医生一样，迭代式地在超高分辨率全切片图像（WSI）上进行浏览和推理。并提出了包含934个实际临床相关任务问题的新基准数据集MultiPathQA，尤其包括128道专业病理医生出题的主观解读题。

Result: 基于GIANT系统，GPT-5在MultiPathQA基准上的表现大幅超越传统缩略图或随机小块方式的基线方法，甚至超过专业病理模型，如在医生出题下的任务准确率达62.5%，优于TITAN（43.8%），SlideChat（37.5%）。

Conclusion: GIANT框架可大幅提升大模型在高分辨率病理图像解读中的表现，显示其在医学专业推理任务中的潜力，也为未来多模态大模型在病理学应用提供了发展方向与参考。

Abstract: Despite being widely used to support clinical care, general-purpose large multimodal models (LMMs) have generally shown poor or inconclusive performance in medical image interpretation, particularly in pathology, where gigapixel images are used. However, prior studies have used either low-resolution thumbnails or random patches, which likely underestimated model performance. Here, we ask whether LMMs can be adapted to reason coherently and accurately in the evaluation of such images. In this study, we introduce Gigapixel Image Agent for Navigating Tissue (GIANT), the first framework that allows LMMs to iteratively navigate whole-slide images (WSIs) like a pathologist. Accompanying GIANT, we release MultiPathQA, a new benchmark, which comprises 934 WSI-level questions, encompassing five clinically-relevant tasks ranging from cancer diagnosis to open-ended reasoning. MultiPathQA also includes 128 questions, authored by two professional pathologists, requiring direct slide interpretation. Using MultiPathQA, we show that our simple agentic system substantially outperforms conventional patch- and thumbnail-based baselines, approaching or surpassing the performance of specialized models trained on millions of images. For example, on pathologist-authored questions, GPT-5 with GIANT achieves 62.5% accuracy, outperforming specialist pathology models such as TITAN (43.8%) and SlideChat (37.5%). Our findings reveal the strengths and limitations of current foundation models and ground future development of LMMs for expert reasoning in pathology.

</details>


### [26] [IndEgo: A Dataset of Industrial Scenarios and Collaborative Work for Egocentric Assistants](https://arxiv.org/abs/2511.19684)
*Vivek Chavan,Yasmina Imgrund,Tung Dao,Sanwantri Bai,Bosong Wang,Ze Lu,Oliver Heimann,Jörg Krüger*

Main category: cs.CV

TL;DR: 本文推出了IndEgo，一个聚焦于工业场景下多模态、包含主观（egocentric）和客观（exocentric）视角的大型数据集，用于任务理解、错误检测和推理等研究，数据量丰富，面向协作任务，对多模态AI系统提出挑战。


<details>
  <summary>Details</summary>
Motivation: 当前关于工业协作任务（如装配、物流、检修等）的多模态数据集稀缺，且很少有覆盖主客观点、合作场景和细致注释，限制了AI系统对复杂工业操作的认知与推理能力提升。因此，作者希望通过新数据集推动该领域进展。

Method: 作者采集并整理了3460段主观视角（egocentric）和1092段客观视角（exocentric）的视频（涵盖约197小时和97小时），同步录制了眼动、音频、叙述、动作等多模态信号，提供详细注释和多种元数据，并设计了一系列评测基准（如任务理解、错误检测、推理问答等）。

Result: 实验显示，当前主流的多模态模型在本数据集上的协作任务识别、错误检测和推理表现均未达到理想水平，说明该数据集难度较高且具有挑战性。

Conclusion: IndEgo数据集为工业多模态场景和协作任务的AI研究提供了高价值资源，有望推动错检、推理等难题的突破，对现有多模态模型能力提出了更高要求。

Abstract: We introduce IndEgo, a multimodal egocentric and exocentric dataset addressing common industrial tasks, including assembly/disassembly, logistics and organisation, inspection and repair, woodworking, and others. The dataset contains 3,460 egocentric recordings (approximately 197 hours), along with 1,092 exocentric recordings (approximately 97 hours). A key focus of the dataset is collaborative work, where two workers jointly perform cognitively and physically intensive tasks. The egocentric recordings include rich multimodal data and added context via eye gaze, narration, sound, motion, and others. We provide detailed annotations (actions, summaries, mistake annotations, narrations), metadata, processed outputs (eye gaze, hand pose, semi-dense point cloud), and benchmarks on procedural and non-procedural task understanding, Mistake Detection, and reasoning-based Question Answering. Baseline evaluations for Mistake Detection, Question Answering and collaborative task understanding show that the dataset presents a challenge for the state-of-the-art multimodal models. Our dataset is available at: https://huggingface.co/datasets/FraunhoferIPK/IndEgo

</details>


### [27] [CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization](https://arxiv.org/abs/2511.19661)
*Xinhai Hou,Shaoyuan Xu,Manan Biyani,Mayan Li,Jia Liu,Todd C. Hollon,Bryan Wang*

Main category: cs.CV

TL;DR: 当前的具备代理能力的视觉语言模型虽然答案准确率高，但中间的视觉推理步骤不一定真实可信。作者提出了新的评估协议，并设计了一种通过显式监督工具使用过程的新方法CodeV，有效提升了工具使用的可信度和推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在视觉推理任务上的最终答案准确率高，但这种准确率可能掩盖了其推理过程不真实（如错误地使用视觉工具或无视工具输出）。因此，需要衡量和提升模型在中间推理步骤上的可信工具使用能力，以构建更加可靠的多模态推理系统。

Method: 提出了一种信度评估协议，用来检测模型调用视觉工具时是否真正覆盖了问题相关证据。引入了CodeV系统，利用基于代码的视觉工具，并提出了一种叫做Tool-Aware Policy Optimization (TAPO)的RL训练框架，通过给中间工具输入输出分配密集回报，引导模型必要且依证地使用工具。同时采用了监督微调+强化学习两阶段训练流程。

Result: 与以往方法相比，CodeV在视觉搜索任务中不仅保持甚至提升了最终回答准确率，还显著提高了可信工具使用率。此外，在多种多模态推理和数学任务中，CodeV同样表现出强劲性能。

Conclusion: 单纯关注最终答案准确率无法确保多模态代理系统的推理可信度。显式监督中间工具行为对于构建可信、多模态视觉推理系统至关重要，CodeV为此提供了有效路径。

Abstract: Agentic vision-language models are increasingly trained to "think with images" by calling image operations. However, we show that high final-answer accuracy often hides unfaithful visual reasoning: models may invoke tools on irrelevant regions or ignore tool outputs entirely, yet still guess the correct answer. In this work, we first propose a faithfulness evaluation protocol that measures whether intermediate visual tool outputs (e.g., crops) actually contain the queried evidence. This reveals that recent visual agents achieve high final-answer accuracy but exhibit low rates of faithful tool-use on visual search benchmarks. We then introduce CodeV, a code-based visual agent trained with Tool-Aware Policy Optimization (TAPO). TAPO is a process-level RL framework that augments GRPO with dense rewards defined directly on visual tool inputs and outputs, rather than on chain-of-thought tokens, making supervision easier to verify and less susceptible to reward hacking. CodeV represents visual tools as executable Python code, and TAPO assigns step-wise rewards based solely on the question and tool output, encouraging both necessary and evidence-consistent tool use. In a two-stage SFT+RL pipeline, CodeV achieves competitive or superior accuracy while substantially increasing faithful tool-use rates on related visual search benchmarks. Beyond visual search, CodeV attains strong performance on a range of multimodal reasoning and math benchmarks, suggesting that explicitly supervising intermediate tool behavior is crucial for building trustworthy, agentic visual reasoning systems.

</details>


### [28] [Maritime Small Object Detection from UAVs using Deep Learning with Altitude-Aware Dynamic Tiling](https://arxiv.org/abs/2511.19728)
*Sakib Ahmed,Oscar Pizarro*

Main category: cs.CV

TL;DR: 本文提出了一种基于高度自适应的动态切片方法，显著提升了无人机在海上搜救任务中对小目标的检测性能，同时加速推理速度。


<details>
  <summary>Details</summary>
Motivation: 无人机在海上搜救任务中承担重要角色，但高空检测小目标时，由于目标像素比例低，检测效果不佳。现有方法面临检测精度与速度之间的权衡，亟需更高效的小目标检测方法。

Method: 提出了一个高度感知的动态图像切片方案，根据无人机高度自适应调整切片大小与密度（tiles），将YOLOv5和SAHI框架结合实现自适应的小目标检测，同时降低不必要的计算量。

Result: 在SeaDronesSee数据集上测试，采用该方法后小目标的mAP提升了38%，同时推理速度比固定切片方式提升了两倍以上。

Conclusion: 本方法在保证检测精度的同时，显著提升了推理效率，适用于复杂环境下无人机搜救任务的小目标检测，具有实际应用价值。

Abstract: Unmanned Aerial Vehicles (UAVs) are crucial in Search and Rescue (SAR) missions due to their ability to monitor vast maritime areas. However, small objects often remain difficult to detect from high altitudes due to low object-to-background pixel ratios. We propose an altitude-aware dynamic tiling method that scales and adaptively subdivides the image into tiles for enhanced small object detection. By integrating altitude-dependent scaling with an adaptive tiling factor, we reduce unnecessary computation while maintaining detection performance. Tested on the SeaDronesSee dataset [1] with YOLOv5 [2] and Slicing Aided Hyper Inference (SAHI) framework [3], our approach improves Mean Average Precision (mAP) for small objects by 38% compared to a baseline and achieves more than double the inference speed compared to static tiling. This approach enables more efficient and accurate UAV-based SAR operations under diverse conditions.

</details>


### [29] [OncoVision: Integrating Mammography and Clinical Data through Attention-Driven Multimodal AI for Enhanced Breast Cancer Diagnosis](https://arxiv.org/abs/2511.19667)
*Istiak Ahmed,Galib Ahmed,K. Shahriar Sanjid,Md. Tanzim Hossain,Md. Nishan Khan,Md. Misbah Khan,Md. Arifur Rahman,Sheikh Anisul Haque,Sharmin Akhtar Rupa,Mohammed Mejbahuddin Mia,Mahmud Hasan Mostofa Kamal,Md. Mostafa Kamal Sarker,M. Monir Uddin*

Main category: cs.CV

TL;DR: OncoVision是一款融合乳腺X光影像和临床数据的多模态AI系统，提升乳腺癌诊断的准确性及临床可用性。


<details>
  <summary>Details</summary>
Motivation: 当前乳腺癌筛查存在影像解读主观性强、单一数据源限制诊断准确率的问题，尤其在医疗资源匮乏地区亟需高效、准确且易用的辅助工具。

Method: 本研究提出了基于注意力机制的编码-解码骨干网络，同时分割影像中四个区域（肿块、钙化、腋窝异常及乳腺组织）并预测十项结构化临床特征。采用两种后期融合策略结合影像及临床数据，并开发了可视化和置信度评分功能的网络应用。

Result: OncoVision在四类ROI分割任务上取得最新最佳性能，并通过多模态融合有效提升了诊断精度与一致性。实现了结构化报告输出和实时决策辅助。

Conclusion: OncoVision显著提升了基于AI的乳腺癌诊断能力，为提升全球、特别是欠发达地区的筛查水平与早期干预提供了可扩展与公平的技术解决方案。

Abstract: OncoVision is a multimodal AI pipeline that combines mammography images and clinical data for better breast cancer diagnosis. Employing an attention-based encoder-decoder backbone, it jointly segments four ROIs - masses, calcifications, axillary findings, and breast tissues - with state-of-the-art accuracy and robustly predicts ten structured clinical features: mass morphology, calcification type, ACR breast density, and BI-RADS categories. To fuse imaging and clinical insights, we developed two late-fusion strategies. By utilizing complementary multimodal data, late fusion strategies improve diagnostic precision and reduce inter-observer variability. Operationalized as a secure, user-friendly web application, OncoVision produces structured reports with dual-confidence scoring and attention-weighted visualizations for real-time diagnostic support to improve clinician trust and facilitate medical teaching. It can be easily incorporated into the clinic, making screening available in underprivileged areas around the world, such as rural South Asia. Combining accurate segmentation with clinical intuition, OncoVision raises the bar for AI-based mammography, offering a scalable and equitable solution to detect breast cancer at an earlier stage and enhancing treatment through timely interventions.

</details>


### [30] [Prune-Then-Plan: Step-Level Calibration for Stable Frontier Exploration in Embodied Question Answering](https://arxiv.org/abs/2511.19768)
*Noah Frahm,Prakrut Patel,Yue Zhang,Shoubin Yu,Mohit Bansal,Roni Sengupta*

Main category: cs.CV

TL;DR: 本文提出了一种名为 Prune-Then-Plan 的新框架，通过稳定大模型在探索任务中的行为，提升 embodied QA 任务表现，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉-语言模型（VLMs）虽然提升了 embodied question answering（EQA）表现，但直接用于逐步探索时易因过度自信和分数失调导致探索不稳定，效率低、答案质量差，因此需要新的方法提高其探索的可靠性和效率。

Method: 提出 Prune-Then-Plan 框架：先基于 Holm-Bonferroni 方法裁剪掉不合理的候选行为（以校正VLM模型分数），然后通过基于覆盖率的规划器选择实际行动，从而将 VLM 的过度自信预测转化为更保守、可解释的探索决策。该方法集成至 3D-Mem EQA 框架。

Result: 在 OpenEQA 和 EXPRESS-Bench 数据集上，所提方法在视觉导航 SPL 指标提升49%，在 LLM-Match 指标提升33%；在相同探索预算下，场景覆盖度更高。

Conclusion: 通过将分数校准和规划解耦，Prune-Then-Plan 显著提升了大模型在 embodied QA 任务中的导航效率与答案质量，显示出强大的通用性和应用潜力。

Abstract: Large vision-language models (VLMs) have improved embodied question answering (EQA) agents by providing strong semantic priors for open-vocabulary reasoning. However, when used directly for step-level exploration, VLMs often exhibit frontier oscillations, unstable back-and-forth movements caused by overconfidence and miscalibration, leading to inefficient navigation and degraded answer quality. We propose Prune-Then-Plan, a simple and effective framework that stabilizes exploration through step-level calibration. Instead of trusting raw VLM scores, our method prunes implausible frontier choices using a Holm-Bonferroni inspired pruning procedure and then delegates final decisions to a coverage-based planner. This separation converts overconfident predictions into conservative, interpretable actions by relying on human-level judgments to calibrate the step-level behavior of VLMs. Integrated into the 3D-Mem EQA framework, our approach achieves relative improvements of up to 49% and 33% in visually grounded SPL and LLM-Match metrics respectively over baselines. Overall, our method achieves better scene coverage under equal exploration budgets on both OpenEQA and EXPRESS-Bench datasets.

</details>


### [31] [INTERLACE: Interleaved Layer Pruning and Efficient Adaptation in Large Vision-Language Models](https://arxiv.org/abs/2511.19676)
*Parsa Madinei,Ryan Solgi,Ziqi Wen,Jonathan Skaza,Miguel Eckstein,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: 作者提出了INTERLACE框架，可在减少VLMs（视觉语言模型）层数的同时，通过高效微调维持其性能。方法针对现有剪枝易导致性能损失的问题，采用三层组的局部冗余分析与交错微调冻结机制，显著提升了模型精简后的性能保留率。实验证明，在仅用1%数据和一次epoch训练的情况下，去除1/4层数还能保留88.9%性能，达到目前最优水平。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）剪枝方法常导致性能大幅下降，无法兼顾轻量化和高性能，亟需更有效的剪枝策略来支持边缘或受限场景下的部署。

Method: 作者提出了基于三层组合分析的剪枝方法：每组连续三层中，识别前两层的局部冗余，删去更冗余的一层，对保留的一层进行微调（finetune）以弥补损失容量，同时冻结第三层作为训练的“锚”。这种交错的微调-冻结（interleaved finetune-freeze）策略避免了全模型微调带来的过拟合并加快收敛。

Result: 通过在FineVision数据集上实证，作者仅用1%训练数据、单epoch下，模型在移除25%网络层数后仍能保留88.9%的平均性能表现，在同类剪枝研究中达到SOTA（最优）成绩。

Conclusion: INTERLACE框架可大幅提升VLMs网络精简后的性能保留，在数据和算力有限场景下尤具实际价值，为模型高效剪枝提供了新思路。

Abstract: We introduce INTERLACE, a novel framework that prunes redundant layers in VLMs while maintaining performance through sample-efficient finetuning. Existing layer pruning methods lead to significant performance drop when applied to VLMs. Instead, we analyze triplets of consecutive layers to identify local redundancy, removing the most redundant of the first two layers, finetune the remaining layer to compensate for the lost capacity, and freeze the third layer to serve as a stable anchor during finetuning. We found that this interleaved finetune-freeze design enables rapid convergence with minimal data after pruning. By finetuning only a subset of layers on just 1% of the FineVision dataset for one epoch, Interlace achieves 88.9% average performance retention after dropping 25% of the network, achieving SOTA performance. Our code is available at: https://github.com/pmadinei/Interlace.git

</details>


### [32] [GigaWorld-0: World Models as Data Engine to Empower Embodied AI](https://arxiv.org/abs/2511.19861)
*GigaWorld Team,Angen Ye,Boyuan Wang,Chaojun Ni,Guan Huang,Guosheng Zhao,Haoyun Li,Jiagang Zhu,Kerui Li,Mengyuan Xu,Qiuping Deng,Siting Wang,Wenkang Qin,Xinze Chen,Xiaofeng Wang,Yankai Wang,Yu Cao,Yifan Chang,Yuan Xu,Yun Ye,Yang Wang,Yukun Zhou,Zhengyuan Zhang,Zhehao Dong,Zheng Zhu*

Main category: cs.CV

TL;DR: GigaWorld-0是一种专为视觉-语言-动作（VLA）学习设计的统一世界模型框架，可以大规模高效地产生多样且真实的交互数据，提升AI在物理世界任务中的泛化和成功率。


<details>
  <summary>Details</summary>
Motivation: 当前的具身智能AI（即融合视觉、语言和动作的AI）需要大量高质量、多样化的交互数据进行训练，但现实中收集这些数据成本高、效率低，因此需要合成高质量训练数据的新方法。

Method: GigaWorld-0由GigaWorld-0-Video和GigaWorld-0-3D两大部分组成。Video部分能在可控的外观、视角和动作语义下生成丰富、有时序连贯的视频；3D部分结合了3D生成建模、3D高斯斑点重建、物理可微系统识别和可执行运动规划，实现几何与物理一致。整个系统通过联合优化，使用高效的GigaTrain框架，以FP8精度和稀疏注意力显著降低计算与内存消耗。

Result: GigaWorld-0能够生成在视觉、空间、物理和任务对齐各方面均高质量、多样、可控的数据。实验证明，用GigaWorld-0生成数据训练的VLA模型（如GigaBrain-0）在实际机器人任务上表现极佳，实现了零现实数据下的泛化与任务成功率大幅提升。

Conclusion: GigaWorld-0为具身AI提供了强有力的合成数据引擎，大大提升了训练效率和现实通用性，为无需真实交互就能发展强泛化能力的AI铺平了道路。

Abstract: World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.

</details>


### [33] [MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization](https://arxiv.org/abs/2511.19878)
*Chengyue Huang,Mellon M. Zhang,Robert Azarcon,Glen Chou,Zsolt Kira*

Main category: cs.CV

TL;DR: 本文提出了MAPS（模块化接近度调度），一种有效的Vision-Language-Action模型微调框架，能在不增加参数或数据的情况下，大幅提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 标准的微调方法会破坏预训练视觉-语言模型(VLM)中的先验知识，从而降低在新任务中的泛化能力。现有的解决办法如冻结模块或统一正则化，要么限制了模型适应新任务的能力，要么忽视了各模块功能的差异。

Method: MAPS 通过系统分析，确定了不同模块在微调时适合逐步放宽与预训练模型接近度的顺序，并采用线性调度方案。具体来说，视觉编码器更紧密保持预训练参数，而语言到动作的层则更加灵活地适应新任务。这一过程无需新增参数或数据，并能直接应用于已有VLA模型。

Result: 在多个VLA模型和基准任务（包括仿真和真实机器人平台）上，MAPS在分布内和分布外任务的表现均显著提升，最高提升达30%。

Conclusion: 对预训练视觉-语言模型的模块化、有指导地保持接近性，是迁移到VLA模型时，既能保持泛化又不牺牲适应性的简单但有效的策略。

Abstract: Vision-Language-Action (VLA) models inherit strong priors from pretrained Vision-Language Models (VLMs), but naive fine-tuning often disrupts these representations and harms generalization. Existing fixes -- freezing modules or applying uniform regularization -- either overconstrain adaptation or ignore the differing roles of VLA components. We present MAPS (Module-Wise Proximity Scheduling), the first robust fine-tuning framework for VLAs. Through systematic analysis, we uncover an empirical order in which proximity constraints should be relaxed to balance stability and flexibility. MAPS linearly schedules this relaxation, enabling visual encoders to stay close to their pretrained priors while action-oriented language layers adapt more freely. MAPS introduces no additional parameters or data, and can be seamlessly integrated into existing VLAs. Across MiniVLA-VQ, MiniVLA-OFT, OpenVLA-OFT, and challenging benchmarks such as SimplerEnv, CALVIN, LIBERO, as well as real-world evaluations on the Franka Emika Panda platform, MAPS consistently boosts both in-distribution and out-of-distribution performance (up to +30%). Our findings highlight empirically guided proximity to pretrained VLMs as a simple yet powerful principle for preserving broad generalization in VLM-to-VLA transfer.

</details>


### [34] [CountXplain: Interpretable Cell Counting with Prototype-Based Density Map Estimation](https://arxiv.org/abs/2511.19686)
*Abdurahman Ali Mohammed,Wallapak Tavanapong,Catherine Fonder,Donald S. Sakaguchi*

Main category: cs.CV

TL;DR: 该论文提出了一种基于原型的细胞计数方法，兼具可解释性和高精度，在公开数据集上取得了良好效果，并得到了生物学家的实际认可。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习在生物医学图像细胞计数领域表现优异，但现有方法缺乏可解释性，影响了临床领域对这些工具的信任和应用。因此，提升模型可解释性成为该领域的关键需求。

Method: 作者在密度图估计网络中引入了原型层，使模型能够自动学习并识别细胞以及背景伪影的典型视觉模式。通过与生物学家的调研验证了这些原型的生物学相关性。同时，模型对输入图像中与不同原型相似的区域进行高亮，帮助理解模型做出的判断。

Result: 在两个公开数据集上的实验表明，该方法在保证细胞计数准确率的同时，实现了较强的解释能力。生物学家的调查进一步证实了所提方法的科学性和实用性。

Conclusion: 该方法为细胞计数任务提供了一种兼具透明度与可靠性的方案，有助于深度学习工具在生物医学领域的推广与应用。

Abstract: Cell counting in biomedical imaging is pivotal for various clinical applications, yet the interpretability of deep learning models in this domain remains a significant challenge. We propose a novel prototype-based method for interpretable cell counting via density map estimation. Our approach integrates a prototype layer into the density estimation network, enabling the model to learn representative visual patterns for both cells and background artifacts. The learned prototypes were evaluated through a survey of biologists, who confirmed the relevance of the visual patterns identified, further validating the interpretability of the model. By generating interpretations that highlight regions in the input image most similar to each prototype, our method offers a clear understanding of how the model identifies and counts cells. Extensive experiments on two public datasets demonstrate that our method achieves interpretability without compromising counting effectiveness. This work provides researchers and clinicians with a transparent and reliable tool for cell counting, potentially increasing trust and accelerating the adoption of deep learning in critical biomedical applications. Code is available at https://github.com/NRT-D4/CountXplain.

</details>


### [35] [Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving](https://arxiv.org/abs/2511.19912)
*Dapeng Zhang,Zhenlong Yuan,Zhangquan Chen,Chih-Ting Liao,Yinda Chen,Fei Shen,Qingguo Zhou,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出了Reasoning-VLA模型，通过引入可学习的动作查询和增强推理能力，实现了高效、泛化性强的视觉-语言-动作决策，达到了优异的性能与推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在自动驾驶决策中能力突出，但在高效推理及泛化到新车辆配置和驾驶场景方面仍有不足。作者旨在解决VLA模型泛化能力和推理速度的问题。

Method: 提出Reasoning-VLA模型：使用一组可学习的动作查询，这些查询由高斯采样的真实轨迹初始化，与通过推理增强的视觉-语言特征交互，生成连续动作轨迹。同时，融合八个公开自动驾驶数据集为统一、基于Chain-of-Thought推理的数据格式，并结合有监督学习和强化学习微调。

Result: 模型在多个基准测试中进行了广泛评测，实现了最新的性能，具有更强的泛化能力和迄今为止最优的推理速度。

Conclusion: Reasoning-VLA在效率、泛化能力和性能方面优于现有VLA模型，为视觉-语言-动作决策提供了新的解决方案。

Abstract: Vision-Language-Action (VLA) models have recently shown strong decision-making capabilities in autonomous driving. However, existing VLAs often struggle with achieving efficient inference and generalizing to novel autonomous vehicle configurations and driving scenarios. In this paper, we propose Reasoning-VLA, a general and fast action-generation VLA framework. The proposed model employs a set of learnable action queries, initialized via Gaussian sampling from ground-truth trajectories within the training corpus. These learnable queries interact with reasoning-enhanced vision-language features to generate continuous action trajectories in parallel. To promote robust generalization, we consolidate eight publicly available autonomous driving datasets into a standardized, Chain-of-Thought reasoning-based, and easy-to-use data format for model training. Leveraging both supervised learning and reinforcement learning fine-tuning, extensive empirical evaluations across multiple benchmarks demonstrate that Reasoning-VLA achieves state-of-the-art performance, superior generalization capability, and the excellent inference speed reported to date.

</details>


### [36] [RADSeg: Unleashing Parameter and Compute Efficient Zero-Shot Open-Vocabulary Segmentation Using Agglomerative Models](https://arxiv.org/abs/2511.19704)
*Omar Alama,Darshil Jariwala,Avigyan Bhattacharya,Seungchan Kim,Wenshan Wang,Sebastian Scherer*

Main category: cs.CV

TL;DR: 本文提出了一种基于RADIO视觉基础模型的新型开放词汇语义分割方法RADSeg，显著提升了分割精度的同时大幅降低了计算与参数量。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇语义分割方法要么依赖有限的分割训练数据导致泛化能力弱，要么需要多个大型模型组合以获高精度，但计算和内存消耗大，限制实际应用。缺乏既高效又高性能的单模型方案。

Method: 作者首次系统性研究了RADIO基础模型在零样本开放词汇语义分割的应用，并提出三项改进：自相关递归注意力机制、自相关全局聚合和高效掩模优化。这些创新提升了模型的分割能力和效率。

Result: 在ViT-base级别，RADSeg的mIoU提升了6-30%，推理速度提升近4倍，参数数量减少至原来的2.5分之一。RADSeg-base（参数量105M）在准确率上超越了以往超大组合模型（850-1350M参数）。

Conclusion: RADSeg在保持或提升精度的同时，显著降低了计算和存储资源需求，为开放词汇语义分割任务提供了高效、易部署的解决方案。

Abstract: Open-vocabulary semantic segmentation (OVSS) underpins many vision and robotics tasks that require generalizable semantic understanding. Existing approaches either rely on limited segmentation training data, which hinders generalization, or apply zero-shot heuristics to vision-language models (e.g CLIP), while the most competitive approaches combine multiple models to improve performance at the cost of high computational and memory demands. In this work, we leverage an overlooked agglomerative vision foundation model, RADIO, to improve zero-shot OVSS along three key axes simultaneously: mIoU, latency, and parameter efficiency. We present the first comprehensive study of RADIO for zero-shot OVSS and enhance its performance through self-correlating recursive attention, self-correlating global aggregation, and computationally efficient mask refinement. Our approach, RADSeg, achieves 6-30% mIoU improvement in the base ViT class while being 3.95x faster and using 2.5x fewer parameters. Surprisingly, RADSeg-base (105M) outperforms previous combinations of huge vision models (850-1350M) in mIoU, achieving state-of-the-art accuracy with substantially lower computational and memory cost.

</details>


### [37] [Map-World: Masked Action planning and Path-Integral World Model for Autonomous Driving](https://arxiv.org/abs/2511.20156)
*Bin Hu,Zijian Lu,Haicheng Liao,Chengran Yuan,Bin Rao,Yongkang Li,Guofa Li,Zhiyong Cui,Cheng-zhong Xu,Zhenning Li*

Main category: cs.CV

TL;DR: 该论文提出了一种无需先验的多模态路径规划框架MAP-World，用于自动驾驶中的高效多未来预测与决策，从而提升规划多样性和效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的运动规划需预测多个可能未来状态，但现有方法多依赖手工锚点或强化学习，仅选取单一轨迹，丢失了其他有价值的信息，且加大了优化难度。

Method: 提出Masked Action Planning（MAP）模块，将未来运动建模为带掩码的序列补全任务，结合粗略驾驶意图，利用噪声扩展获得多样一致的轨迹分布。使用轻量级世界模型，根据每条候选轨迹生成未来语义，并基于路径概率对损失进行加权训练，能够从所有可能未来学习而非仅选中一条。

Result: 在NAVSIM测试中，MAP-World达到了与锚点方法接近或更优的性能，并在世界模型方法中取得了最新最好结果，同时避免了强化学习，保持了实时推理速度。

Conclusion: MAP-World无需先验锚点或强化学习，即可实现多模态高效路径规划，提升了预测与决策的广度与性能，有望为自动驾驶提供更高效、更充分利用多未来信息的新范式。

Abstract: Motion planning for autonomous driving must handle multiple plausible futures while remaining computationally efficient. Recent end-to-end systems and world-model-based planners predict rich multi-modal trajectories, but typically rely on handcrafted anchors or reinforcement learning to select a single best mode for training and control. This selection discards information about alternative futures and complicates optimization. We propose MAP-World, a prior-free multi-modal planning framework that couples masked action planning with a path-weighted world model. The Masked Action Planning (MAP) module treats future ego motion as masked sequence completion: past waypoints are encoded as visible tokens, future waypoints are represented as mask tokens, and a driving-intent path provides a coarse scaffold. A compact latent planning state is expanded into multiple trajectory queries with injected noise, yielding diverse, temporally consistent modes without anchor libraries or teacher policies. A lightweight world model then rolls out future BEV semantics conditioned on each candidate trajectory. During training, semantic losses are computed as an expectation over modes, using trajectory probabilities as discrete path weights, so the planner learns from the full distribution of plausible futures instead of a single selected path. On NAVSIM, our method matches anchor-based approaches and achieves state-of-the-art performance among world-model-based methods, while avoiding reinforcement learning and maintaining real-time inference latency.

</details>


### [38] [Rethinking Vision Transformer Depth via Structural Reparameterization](https://arxiv.org/abs/2511.19718)
*Chengwei Zhou,Vipin Chaudhary,Gourav Datta*

Main category: cs.CV

TL;DR: 本文提出了一种基于分支结构重参数化的训练方法，可将ViT网络深度大幅减半甚至更少，同时基本保持分类准确率，并大大提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉Transformer（ViT）推理耗时主要来自模型极深的层叠结构，现有加速方法大多只针对算法层如token剪枝、注意力加速，鲜有关注如何减少Transformer层数却不损失表达能力。

Method: 提出一种在训练阶段引入并行分支的重参数化技术，让Transformer块内部引入多分支，并通过系统性合并将多分支模型在推理时转化为单路径结构，同时保证前馈网络（FFN）和多头自注意力（MHSA）模块合并时无精度损失。

Result: 在ViT-Tiny模型上，可以将12层降到6、4甚至3层，ImageNet-1K分类准确率几乎不变。压缩模型在移动端CPU上推理加速最高可达37%。

Conclusion: 非常深的Transformer堆叠结构可能并非必须，通过结构重参数化可显著提高视觉Transformer的效率，为未来高效ViT设计提供了新方向。

Abstract: The computational overhead of Vision Transformers in practice stems fundamentally from their deep architectures, yet existing acceleration strategies have primarily targeted algorithmic-level optimizations such as token pruning and attention speedup. This leaves an underexplored research question: can we reduce the number of stacked transformer layers while maintaining comparable representational capacity? To answer this, we propose a branch-based structural reparameterization technique that operates during the training phase. Our approach leverages parallel branches within transformer blocks that can be systematically consolidated into streamlined single-path models suitable for inference deployment. The consolidation mechanism works by gradually merging branches at the entry points of nonlinear components, enabling both feed-forward networks (FFN) and multi-head self-attention (MHSA) modules to undergo exact mathematical reparameterization without inducing approximation errors at test time. When applied to ViT-Tiny, the framework successfully reduces the original 12-layer architecture to 6, 4, or as few as 3 layers while maintaining classification accuracy on ImageNet-1K. The resulting compressed models achieve inference speedups of up to 37% on mobile CPU platforms. Our findings suggest that the conventional wisdom favoring extremely deep transformer stacks may be unnecessarily restrictive, and point toward new opportunities for constructing efficient vision transformers.

</details>


### [39] [Material-informed Gaussian Splatting for 3D World Reconstruction in a Digital Twin](https://arxiv.org/abs/2511.20348)
*João Malheiro Silva,Andy Huynh,Tong Duy Son,Holger Caesar*

Main category: cs.CV

TL;DR: 本文提出了一种仅用摄像头重建3D场景的新方法，既能获得高质量外观，又能准确赋予物理材质属性，用于数字孪生与仿真，无需激光雷达和复杂校准，效果可与融合方法媲美。


<details>
  <summary>Details</summary>
Motivation: 现有数字孪生的3D重建多依赖激光雷达，但激光雷达缺乏图像的纹理和语义信息，且对玻璃等材料表现不佳。传统摄像头-激光雷达融合需要繁琐的硬件和校准流程，增加了系统复杂度。

Method: 提出完全基于摄像头的新流程：利用多视角图像进行3D Gaussian Splatting重建，结合视觉模型提取材料语义掩码；再将高斯表示转为网格并赋予投影材料标签，最终根据材料标签分配物理属性，以便于物理仿真。

Result: 基于拥有真实传感器数据的车辆数据集进行验证，并以激光雷达数据作为反射率验证的基准，辅以图像相似度指标，证明方法在外观和物理属性还原上可达业界先进水平。

Conclusion: 这种纯摄像头的重建管线在不增加硬件和校准复杂度的前提下，实现了高保真的数字孪生三维重建和材料属性还原，为仿真与感知算法测试提供了新选择，具备实际应用的潜力。

Abstract: 3D reconstruction for Digital Twins often relies on LiDAR-based methods, which provide accurate geometry but lack the semantics and textures naturally captured by cameras. Traditional LiDAR-camera fusion approaches require complex calibration and still struggle with certain materials like glass, which are visible in images but poorly represented in point clouds. We propose a camera-only pipeline that reconstructs scenes using 3D Gaussian Splatting from multi-view images, extracts semantic material masks via vision models, converts Gaussian representations to mesh surfaces with projected material labels, and assigns physics-based material properties for accurate sensor simulation in modern graphics engines and simulators. This approach combines photorealistic reconstruction with physics-based material assignment, providing sensor simulation fidelity comparable to LiDAR-camera fusion while eliminating hardware complexity and calibration requirements. We validate our camera-only method using an internal dataset from an instrumented test vehicle, leveraging LiDAR as ground truth for reflectivity validation alongside image similarity metrics.

</details>


### [40] [BRIC: Bridging Kinematic Plans and Physical Control at Test Time](https://arxiv.org/abs/2511.20431)
*Dohun Lim,Minji Kim,Jaewoon Lim,Sungchan Kim*

Main category: cs.CV

TL;DR: 本文提出了BRIC框架，通过在测试时自适应调整物理控制器与引导扩散模型，有效解决了扩散模型与物理控制不匹配导致人类运动生成不可持续或物理不合理的问题，实现了稳定且多样化的人体长时运动生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的人体运动生成方法虽然能生成多样且具有表现力的动作，但常因与物理控制冲突，在仿真中产生漂移或不现实的结果，阻碍了长时运动生成与复杂场景部署。如何在不改变已训练扩散模型参数的前提下，解决扩散模型与强化学习物理控制器之间执行不一致的问题，是本文关注的核心。

Method: BRIC在测试时动态适应物理控制器，根据扩散生成的带噪动作序列调整控制输出来弥补执行差异。同时设计特殊损失函数，防止控制器遗忘原有技能，还通过高效的测试时引导机制在信号空间引导扩散模型，无需更新其参数。两种机制结合，从动作规划与动作控制两端共同提升长时运动生成的物理一致性和多样性。

Result: BRIC在多种长时任务（如运动组合、避障、人-场景交互）上进行了验证，取得了所有任务的最优表现，生成效果在物理可达性和多任务一致性方面均超过已有方法。

Conclusion: BRIC通过测试时自适应控制器与轻量引导扩散模型，在不改变扩散模型参数的情况下，实现了扩散模型与物理控制的协同，大幅提升了人体长时运动生成在复杂环境中的稳定性和灵活性，对运动生成领域具有重要推动作用。

Abstract: We propose BRIC, a novel test-time adaptation (TTA) framework that enables long-term human motion generation by resolving execution discrepancies between diffusion-based kinematic motion planners and reinforcement learning-based physics controllers. While diffusion models can generate diverse and expressive motions conditioned on text and scene context, they often produce physically implausible outputs, leading to execution drift during simulation. To address this, BRIC dynamically adapts the physics controller to noisy motion plans at test time, while preserving pre-trained skills via a loss function that mitigates catastrophic forgetting. In addition, BRIC introduces a lightweight test-time guidance mechanism that steers the diffusion model in the signal space without updating its parameters. By combining both adaptation strategies, BRIC ensures consistent and physically plausible long-term executions across diverse environments in an effective and efficient manner. We validate the effectiveness of BRIC on a variety of long-term tasks, including motion composition, obstacle avoidance, and human-scene interaction, achieving state-of-the-art performance across all tasks.

</details>


### [41] [Efficient Transferable Optimal Transport via Min-Sliced Transport Plans](https://arxiv.org/abs/2511.19741)
*Xinran Liu,Elaheh Akbari,Rocio Diaz Martin,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.CV

TL;DR: 本文提出了一种基于切片（slice-based）的最小化切片传输计划（min-STP）方法，用于高效求解最优传输问题，重点分析了切片器的可迁移性，并展示其在新分布对下的有效性。


<details>
  <summary>Details</summary>
Motivation: 最优传输（OT）在计算机视觉等领域的匹配与对齐问题中很重要，但高计算成本限制了其应用规模。基于一维切片的OT方案降低了成本，但其'切片器'在新分布对上的泛化能力尚不明确。提高OT方法的迁移能力对处理变化数据或多次OT计算至关重要。

Method: 作者研究了min-STP框架，并首次理论分析了优化后的切片器在分布轻微扰动下的稳定性和转移能力。同时，提出了mini-batch版本的min-STP，提升了方法的扩展性，并给出准确性保证。

Result: 理论上证明了切片器的迁移能力。实验证明，min-STP不仅在一次匹配领域表现优秀，还支持点云对齐、基于流的生成建模等任务中的高效迁移和端到端训练。

Conclusion: min-STP及其迁移能力大幅提升了最优传输技术的实用性与可扩展性，适用于数据分布经常变化或需要多次高效计算的实际场景。

Abstract: Optimal Transport (OT) offers a powerful framework for finding correspondences between distributions and addressing matching and alignment problems in various areas of computer vision, including shape analysis, image generation, and multimodal tasks. The computation cost of OT, however, hinders its scalability. Slice-based transport plans have recently shown promise for reducing the computational cost by leveraging the closed-form solutions of 1D OT problems. These methods optimize a one-dimensional projection (slice) to obtain a conditional transport plan that minimizes the transport cost in the ambient space. While efficient, these methods leave open the question of whether learned optimal slicers can transfer to new distribution pairs under distributional shift. Understanding this transferability is crucial in settings with evolving data or repeated OT computations across closely related distributions. In this paper, we study the min-Sliced Transport Plan (min-STP) framework and investigate the transferability of optimized slicers: can a slicer trained on one distribution pair yield effective transport plans for new, unseen pairs? Theoretically, we show that optimized slicers remain close under slight perturbations of the data distributions, enabling efficient transfer across related tasks. To further improve scalability, we introduce a minibatch formulation of min-STP and provide statistical guarantees on its accuracy. Empirically, we demonstrate that the transferable min-STP achieves strong one-shot matching performance and facilitates amortized training for point cloud alignment and flow-based generative modeling.

</details>


### [42] [Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI](https://arxiv.org/abs/2511.20620)
*Xinhao Liu,Jiaqi Li,Youming Deng,Ruxin Chen,Yingjia Zhang,Yifei Ma,Li Guo,Yiming Li,Jing Zhang,Chen Feng*

Main category: cs.CV

TL;DR: 本文提出了Wanderland，一个能高保真模拟现实世界场景、支持重复闭环评测的Embodied AI基准系统，解决了现有方法在视觉与几何上和真实环境差距大、评测不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: Embodied AI（具身智能）任务如视觉导航需要可靠且可重复的闭环评测，但现有的仿真环境往往在视觉真实感和几何准确性上与真实世界差距较大，导致评测难以信任。这限制了算法的开发和比较。

Method: 作者提出了Wanderland，一个real-to-sim的框架，具备多传感器采集、可靠重建、精确几何以及高质量的视角合成能力。利用该管线，作者构建了多样化的室内外城市场景数据集，并进行系统性分析，比较了图像-only与几何增强方案在新视角生成、导航策略学习与评估可靠性上的表现。

Result: Wanderland数据集和框架展示，单纯依赖图像的方案表现难以扩展，几何质量直接影响新视角生成和导航策略的学习评估效果。Wanderland同时支持3D重建与新视角生成算法的基准测试。

Conclusion: Wanderland为开放世界Embodied AI提供了可复现的可靠评测基准，促进了具身智能领域的技术发展，并将成为3D建模与视角合成的研究平台。

Abstract: Reproducible closed-loop evaluation remains a major bottleneck in Embodied AI such as visual navigation. A promising path forward is high-fidelity simulation that combines photorealistic sensor rendering with geometrically grounded interaction in complex, open-world urban environments. Although recent video-3DGS methods ease open-world scene capturing, they are still unsuitable for benchmarking due to large visual and geometric sim-to-real gaps. To address these challenges, we introduce Wanderland, a real-to-sim framework that features multi-sensor capture, reliable reconstruction, accurate geometry, and robust view synthesis. Using this pipeline, we curate a diverse dataset of indoor-outdoor urban scenes and systematically demonstrate how image-only pipelines scale poorly, how geometry quality impacts novel view synthesis, and how all of these adversely affect navigation policy learning and evaluation reliability. Beyond serving as a trusted testbed for embodied navigation, Wanderland's rich raw sensor data further allows benchmarking of 3D reconstruction and novel view synthesis models. Our work establishes a new foundation for reproducible research in open-world embodied AI. Project website is at https://ai4ce.github.io/wanderland/.

</details>


### [43] [Leveraging Foundation Models for Histological Grading in Cutaneous Squamous Cell Carcinoma using PathFMTools](https://arxiv.org/abs/2511.19751)
*Abdul Rahman Diab,Emily E. Karn,Renchin Wu,Emily S. Ruiz,William Lotter*

Main category: cs.CV

TL;DR: 本论文提出了PathFMTools工具包，用于高效地执行、分析和可视化病理基础模型，并在皮肤鳞状细胞癌的组织学分级任务中评估了两种主流视觉-语言基础模型。


<details>
  <summary>Details</summary>
Motivation: 尽管计算病理基础模型有潜力，但由于全切片图像处理复杂、特征难以理解以及适应方法多样，其在具体临床任务上的应用存在挑战。

Method: 作者开发了PathFMTools——一个轻量且可扩展的Python包，可以高效执行、分析及可视化病理基础模型。利用该工具，作者测试和分析了CONCH和MUSK两款基础模型在皮肤鳞状细胞癌组织学分级任务中的表现，并基于440例WSI数据集，对多种模型适应策略进行基准测试。

Result: 实验对多种模型适应策略进行了基准对比，揭示了不同预测方法间的取舍关系，并验证了利用基础模型嵌入训练小型专业模型的潜力。

Conclusion: 病理基础模型在临床实际应用中具有较大潜力，PathFMTools能为其分析和验证提供高效支持。

Abstract: Despite the promise of computational pathology foundation models, adapting them to specific clinical tasks remains challenging due to the complexity of whole-slide image (WSI) processing, the opacity of learned features, and the wide range of potential adaptation strategies. To address these challenges, we introduce PathFMTools, a lightweight, extensible Python package that enables efficient execution, analysis, and visualization of pathology foundation models. We use this tool to interface with and evaluate two state-of-the-art vision-language foundation models, CONCH and MUSK, on the task of histological grading in cutaneous squamous cell carcinoma (cSCC), a critical criterion that informs cSCC staging and patient management. Using a cohort of 440 cSCC H&E WSIs, we benchmark multiple adaptation strategies, demonstrating trade-offs across prediction approaches and validating the potential of using foundation model embeddings to train small specialist models. These findings underscore the promise of pathology foundation models for real-world clinical applications, with PathFMTools enabling efficient analysis and validation.

</details>


### [44] [Training-Free Generation of Diverse and High-Fidelity Images via Prompt Semantic Space Optimization](https://arxiv.org/abs/2511.19811)
*Debin Meng,Chen Jin,Zheng Gao,Yanran Li,Ioannis Patras,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: 本文提出了一种名为Token-Prompt embedding Space Optimization（TPSO）的新方法，有效提升了文生图扩散模型的图像多样性，同时保证生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前文生图扩散模型在生成多样性方面存在局限，常出现生成坍缩（mode collapse），导致输出重复，限制创新和应用。现有方法提升多样性往往会牺牲图像质量或仍未能摆脱主模态。

Method: 提出TPSO模块，将可学习参数注入token embedding空间的低代表性区域，避免生成坍缩。TPSO作为训练无关和模型无关的方法，在prompt级别引入全局语义约束，有效规避分布漂移带来的质量下降。

Result: 在MS-COCO及三种扩散模型上，TPSO无损提升了生成多样性，相关指标由1.10大幅提升至4.18，同时保持了高保真图像质量。

Conclusion: TPSO能显著提升文生图扩散模型的生成多样性，无需重新训练且适用于多种架构，具有实际应用前景。代码将在论文接收后开源。

Abstract: Image diversity remains a fundamental challenge for text-to-image diffusion models. Low-diversity models tend to generate repetitive outputs, increasing sampling redundancy and hindering both creative exploration and downstream applications. A primary cause is that generation often collapses toward a strong mode in the learned distribution. Existing attempts to improve diversity, such as noise resampling, prompt rewriting, or steering-based guidance, often still collapse to dominant modes or introduce distortions that degrade image quality. In light of this, we propose Token-Prompt embedding Space Optimization (TPSO), a training-free and model-agnostic module. TPSO introduces learnable parameters to explore underrepresented regions of the token embedding space, reducing the tendency of the model to repeatedly generate samples from strong modes of the learned distribution. At the same time, the prompt-level space provides a global semantic constraint that regulates distribution shifts, preventing quality degradation while maintaining high fidelity. Extensive experiments on MS-COCO and three diffusion backbones show that TPSO significantly enhances generative diversity, improving baseline performance from 1.10 to 4.18 points, without sacrificing image quality. Code will be released upon acceptance.

</details>


### [45] [What You See is (Usually) What You Get: Multimodal Prototype Networks that Abstain from Expensive Modalities](https://arxiv.org/abs/2511.19752)
*Muchang Bahng,Charlie Berens,Jon Donnelly,Eric Chen,Chaofan Chen,Cynthia Rudin*

Main category: cs.CV

TL;DR: 本论文提出了一种结合原型网络的多模态、成本感知物种识别方法，可智能分配遗传信息采集，提升解释性并降低实验成本，同时保持较高识别准确率。


<details>
  <summary>Details</summary>
Motivation: 当前多模态神经网络在物种识别中应用广泛，但存在决策过程不透明（黑箱）且遗传数据采集昂贵、具侵入性等问题，亟需开发解释性更强且成本更低的自动化监测方法。

Method: 作者将可解释的原型网络（ProtoPNets）扩展到多模态和成本考量的设定，通过集成不同模态下的原型，并给每种模态的判断赋予权重，智能决定依赖视觉或基因模态；并提出区分哪些样本仅依靠图像就足以准确分类，无需获取昂贵的遗传信息。

Result: 实验表明，提出的方法能在区分细微种群时智能调配遗传素材采集，同时针对明显物种更多依赖视觉图像，从而在保持整体准确率的情况下有效节约成本。

Conclusion: 该方法实现了物种自动识别中的多模态合理调配，提升模型决策解释性，降低了实验和采样成本，对于生态监测和保护具有现实意义。

Abstract: Species detection is important for monitoring the health of ecosystems and identifying invasive species, serving a crucial role in guiding conservation efforts. Multimodal neural networks have seen increasing use for identifying species to help automate this task, but they have two major drawbacks. First, their black-box nature prevents the interpretability of their decision making process. Second, collecting genetic data is often expensive and requires invasive procedures, often necessitating researchers to capture or kill the target specimen. We address both of these problems by extending prototype networks (ProtoPNets), which are a popular and interpretable alternative to traditional neural networks, to the multimodal, cost-aware setting. We ensemble prototypes from each modality, using an associated weight to determine how much a given prediction relies on each modality. We further introduce methods to identify cases for which we do not need the expensive genetic information to make confident predictions. We demonstrate that our approach can intelligently allocate expensive genetic data for fine-grained distinctions while using abundant image data for clearer visual classifications and achieving comparable accuracy to models that consistently use both modalities.

</details>


### [46] [CropVLM: Learning to Zoom for Fine-Grained Vision-Language Perception](https://arxiv.org/abs/2511.19820)
*Miguel Carvalho,Helder Dias,Bruno Martins*

Main category: cs.CV

TL;DR: CropVLM是一种通过强化学习训练的外部低成本方法，可以让现有视觉-语言模型（VLM）自适应地“放大”关注图像细节区域，显著提升其在细粒度图像任务中的表现。无需修改、微调原VLM或标注框，人力和计算成本低。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在需要细粒度视觉理解的任务（如场景文字识别、文档分析）上表现有限，主要受感知精度不足和视觉碎片化影响。需要一种能提升VLM细致感知能力的方案，同时不增加标注负担和复杂的模型改造。

Method: 提出了CropVLM方法。它作为VLM的外部模块，通过强化学习训练模型判断何时“放大”并关注图像重要局部。训练过程中无需使用人工标注的框，也不依赖昂贵的合成数据。训练完成后，CropVLM可以兼容不同的VLM（开源或闭源），作为插件提升其细粒度感知能力。

Result: 通过不修改不微调VLM，仅加上CropVLM模块，模型在需要高分辨率理解的任务（包括VLM未见过的领域）上表现有显著提升。

Conclusion: CropVLM为提升VLM对复杂图像细节的感知提供了一种简单有效的通用方法，不仅免去了高昂的标注和算力成本，也避免了直接微调VLM导致的遗忘等问题，是一种具有实际应用价值的增强方案。

Abstract: Vision-Language Models (VLMs) often struggle with tasks that require fine-grained image understanding, such as scene-text recognition or document analysis, due to perception limitations and visual fragmentation. To address these challenges, we introduce CropVLM as an external low-cost method for boosting performance, enabling VLMs to dynamically ''zoom in'' on relevant image regions, enhancing their ability to capture fine details. CropVLM is trained using reinforcement learning, without using human-labeled bounding boxes as a supervision signal, and without expensive synthetic evaluations. The model is trained once and can be paired with both open-source and proprietary VLMs to improve their performance. Our approach delivers significant improvements on tasks that require high-resolution image understanding, notably for benchmarks that are out-of-domain for the target VLM, without modifying or fine-tuning the VLM, thus avoiding catastrophic forgetting.

</details>


### [47] [Vision--Language Enhanced Foundation Model for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2511.19759)
*Jiaqi Guo,Mingzhen Li,Hanyu Su,Santiago López,Lexiaozi Fan,Daniel Kim,Aggelos Katsaggelos*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉语言模型（VLM）与半监督学习（SSL）的医学图像分割方法VESSA，通过整合基础级视觉-语义理解显著提升了极少标注下的分割准确率。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割依赖大量专家标注，耗费高昂。近年来SSL能减少标注依赖，但仍受限于特征表达。VLM展现了跨领域泛化与少样本能力，将其引入SSL有望显著改善医用分割任务的表现。

Method: 方法分两阶段：第一阶段，VESSA作为基于VLM的分割助手，借助gold-standard模板库，通过视觉特征匹配从模板提取结构化语义空间提示，随后用类似SAM2的mask解码器生成分割掩码。第二阶段，将VESSA集成入主流SSL流程，学生模型预测不断反馈给VESSA，VESSA动态精化伪标签，实现自适应指导。

Result: 在多组医学分割数据集上实验，VESSA增强的半监督流程在极少标注场景下分割精度显著优于当前最新基线方法。

Conclusion: VESSA有效融合视觉语言模型和半监督框架，大幅提升医学图像极少标注分割效果，具备较强的泛化和实际应用潜力。

Abstract: Semi-supervised learning (SSL) has emerged as an effective paradigm for medical image segmentation, reducing the reliance on extensive expert annotations. Meanwhile, vision-language models (VLMs) have demonstrated strong generalization and few-shot capabilities across diverse visual domains. In this work, we integrate VLM-based segmentation into semi-supervised medical image segmentation by introducing a Vision-Language Enhanced Semi-supervised Segmentation Assistant (VESSA) that incorporates foundation-level visual-semantic understanding into SSL frameworks. Our approach consists of two stages. In Stage 1, the VLM-enhanced segmentation foundation model VESSA is trained as a reference-guided segmentation assistant using a template bank containing gold-standard exemplars, simulating learning from limited labeled data. Given an input-template pair, VESSA performs visual feature matching to extract representative semantic and spatial cues from exemplar segmentations, generating structured prompts for a SAM2-inspired mask decoder to produce segmentation masks. In Stage 2, VESSA is integrated into a state-of-the-art SSL framework, enabling dynamic interaction with the student model: as student predictions become more refined, they are fed back to VESSA as prompts, allowing it to generate higher-quality pseudo-labels and stronger guidance. Extensive experiments across multiple segmentation datasets and domains show that VESSA-augmented SSL significantly enhances segmentation accuracy, outperforming state-of-the-art baselines under extremely limited annotation conditions.

</details>


### [48] [A Storage-Efficient Feature for 3D Concrete Defect Segmentation to Replace Normal Vector](https://arxiv.org/abs/2511.19760)
*Linxin Hua,Jianghua Deng,Ye Lu*

Main category: cs.CV

TL;DR: 该论文提出了利用相对角度特征提升点云损伤重建效率，大幅降低了数据存储和计算需求。


<details>
  <summary>Details</summary>
Motivation: 传统基于图像的方法在背景噪声下效果不佳，而点云重建受限于3D数据量大，存储和运算开销高，需要新的高效特征表达方法。

Method: 提出了一种新的单一特征“相对角度”，即点的法向量与点云整体平均法向量的夹角。通过熵为基础的特征评估筛选冗余信息；利用PointNet++模型进行训练与测试评估性能。

Result: 使用相对角度特征的模型在保留缺陷相关特征的同时，有效滤除了无损区信息，模型性能与使用完整法向量特征相当但存储需求减少27.6%，输入通道压缩83%。

Conclusion: 提出的相对角度特征无需模型结构改变即可实现高效点云缺陷检测，可为资源受限硬件上的大批量点云处理提供可能。

Abstract: Point cloud reconstruction of damage offers an effective solution to image-based methods vulnerable to background noise, yet its application is constrained by the high volume of 3D data. This study proposes a new feature, relative angle, computed as the angle between the normal vector of a point and the average normal vector of its parent point cloud. This single-dimensional feature provides directionality information equivalent to normal vectors for concrete surface defect characteristics. Through entropy-based feature evaluation, this study demonstrates the ability of relative angle to filter out redundant information in undamaged sections while retaining effective information in damaged sections. By training and testing with PointNet++, models based on the relative angles achieved similar performance to that of models based on normal vectors while delivering 27.6% storage reduction and 83% input channel compression. This novel feature has the potential to enable larger-batch execution on resource-constrained hardware without the necessity of architectural modifications to models.

</details>


### [49] [CounterVQA: Evaluating and Improving Counterfactual Reasoning in Vision-Language Models for Video Understanding](https://arxiv.org/abs/2511.19923)
*Yuefei Chen,Jiang Liu,Xiaodong Lin,Ruixiang Tang*

Main category: cs.CV

TL;DR: 本文引入了CounterVQA基准数据集，系统性评估视觉语言模型（VLMs）的视频反事实推理能力，并提出CFGPT方法提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在视频理解方面表现突出，但其推理假设条件下不同结果（即反事实推理）的能力尚未得到充分研究，这种能力对深入理解视频因果结构和未观测可能性至关重要。

Method: 提出CounterVQA数据集，包含三级难度，专门用于评测VLMs的反事实推理能力。对当前主流开源和闭源模型进行大规模基准评测，并提出CFGPT后训练方法，通过语言模态蒸馏方式提升模型的视觉反事实推理能力。

Result: 实验表明，当前VLMs在简单反事实问题上表现良好，但在复杂多跳因果链推理上表现显著下降。CFGPT方法能在所有难度水平上带来一致改进。

Conclusion: 现有VLMs在复杂视频反事实推理上仍存在较大提升空间。通过CFGPT等方法和CounterVQA基准，有望推动该方向模型推理能力的发展。

Abstract: Vision Language Models (VLMs) have recently shown significant advancements in video understanding, especially in feature alignment, event reasoning, and instruction-following tasks. However, their capability for counterfactual reasoning, inferring alternative outcomes under hypothetical conditions, remains underexplored. This capability is essential for robust video understanding, as it requires identifying underlying causal structures and reasoning about unobserved possibilities, rather than merely recognizing observed patterns. To systematically evaluate this capability, we introduce CounterVQA, a video-based benchmark featuring three progressive difficulty levels that assess different aspects of counterfactual reasoning. Through comprehensive evaluation of both state-of-the-art open-source and closed-source models, we uncover a substantial performance gap: while these models achieve reasonable accuracy on simple counterfactual questions, performance degrades significantly on complex multi-hop causal chains. To address these limitations, we develop a post-training method, CFGPT, that enhances a model's visual counterfactual reasoning ability by distilling its counterfactual reasoning capability from the language modality, yielding consistent improvements across all CounterVQA difficulty levels. Dataset and code will be further released.

</details>


### [50] [Lightweight Transformer Framework for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2511.19765)
*Ali Torabi,Sanjog Gaihre,Yaqoob Majeed*

Main category: cs.CV

TL;DR: 提出了CrispFormer，通过对SegFormer解码器的三项改进，实现了更有效的弱监督语义分割，提升了分割边界和小目标的性能，且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督语义分割方法难以从低质量、稀疏标注中获得高质量、细致的分割掩码，尤其在边界和小物体上表现不佳。该工作旨在解决这些挑战。

Method: 在SegFormer解码器基础上，提出三项关键改进：（1）边界分支，利用轻量级边缘头和边界感知损失监督精细边界；（2）不确定性引导精细器，通过预测每像素的不确定性来加权损失并残差校正分割输出；（3）动态多尺度融合，用空间softmax门控动态融合多尺度特征，并可结合不确定性调制。

Result: 在标准WSSS流程下，CrispFormer对比原始SegFormer在边界F分数、小目标召回率和mIoU等指标上均有显著提升，且增加的计算负担极小。

Conclusion: CrispFormer只需简单的解码器改动即可广泛兼容SegFormer体系，有效提升弱监督语义分割精度，尤其改善了边界和细粒度目标的表现，适合推广应用。

Abstract: Weakly supervised semantic segmentation (WSSS) must learn dense masks from noisy, under-specified cues. We revisit the SegFormer decoder and show that three small, synergistic changes make weak supervision markedly more effective-without altering the MiT backbone or relying on heavy post-processing. Our method, CrispFormer, augments the decoder with: (1) a boundary branch that supervises thin object contours using a lightweight edge head and a boundary-aware loss; (2) an uncertainty-guided refiner that predicts per-pixel aleatoric uncertainty and uses it to weight losses and gate a residual correction of the segmentation logits; and (3) a dynamic multi-scale fusion layer that replaces static concatenation with spatial softmax gating over multi-resolution features, optionally modulated by uncertainty. The result is a single-pass model that preserves crisp boundaries, selects appropriate scales per location, and resists label noise from weak cues. Integrated into a standard WSSS pipeline (seed, student, and EMA relabeling), CrispFormer consistently improves boundary F-score, small-object recall, and mIoU over SegFormer baselines trained on the same seeds, while adding minimal compute. Our decoder-centric formulation is simple to implement, broadly compatible with existing SegFormer variants, and offers a reproducible path to higher-fidelity masks from image-level supervision.

</details>


### [51] [DesignPref: Capturing Personal Preferences in Visual Design Generation](https://arxiv.org/abs/2511.20513)
*Yi-Hao Peng,Jeffrey P. Bigham,Jason Wu*

Main category: cs.CV

TL;DR: 本文提出了DesignPref数据集，聚焦生成模型中的个性化视觉设计偏好，并通过实验证明个性化策略优于传统聚合方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型在视觉设计中的应用增加，现有的模型多依赖人类标注的偏好数据，但视觉设计偏好主观化强且个体差异大。现有基于大多数投票的模型无法准确反映个人偏好，因此需要研究个性化建模策略。

Method: 作者构建了DesignPref数据集，包括12,000对UI设计的成对比较，由20位专业设计师按多级偏好评分。测量设计师间一致性，并收集自然语言理由分析分歧原因。比较了采用多数投票与个性化策略（如微调、引入设计师特定标注至RAG管道）预测个人偏好的效果。

Result: 研究发现，设计师间一致性较低（Krippendorff's alpha=0.25），存在显著分歧。个性化模型在预测个人偏好上明显优于基于多数投票的模型，即使使用远少于后者的数据样本。

Conclusion: 传统的主流集合方法难以捕捉个体设计偏好，个性化建模策略更有效。本工作首次构建了用于个性化视觉设计偏好研究的数据集，为后续个性化设计品味建模奠定了基础。

Abstract: Generative models, such as large language models and text-to-image diffusion models, are increasingly used to create visual designs like user interfaces (UIs) and presentation slides. Finetuning and benchmarking these generative models have often relied on datasets of human-annotated design preferences. Yet, due to the subjective and highly personalized nature of visual design, preference varies widely among individuals. In this paper, we study this problem by introducing DesignPref, a dataset of 12k pairwise comparisons of UI design generation annotated by 20 professional designers with multi-level preference ratings. We found that among trained designers, substantial levels of disagreement exist (Krippendorff's alpha = 0.25 for binary preferences). Natural language rationales provided by these designers indicate that disagreements stem from differing perceptions of various design aspect importance and individual preferences. With DesignPref, we demonstrate that traditional majority-voting methods for training aggregated judge models often do not accurately reflect individual preferences. To address this challenge, we investigate multiple personalization strategies, particularly fine-tuning or incorporating designer-specific annotations into RAG pipelines. Our results show that personalized models consistently outperform aggregated baseline models in predicting individual designers' preferences, even when using 20 times fewer examples. Our work provides the first dataset to study personalized visual design evaluation and support future research into modeling individual design taste.

</details>


### [52] [Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward](https://arxiv.org/abs/2511.20561)
*Yuwei Niu,Weiyang Jin,Jiaqi Liao,Chaoran Feng,Peng Jin,Bin Lin,Zongjian Li,Bin Zhu,Weihao Yu,Li Yuan*

Main category: cs.CV

TL;DR: 本文提出了一个名为UniSandbox的评测框架，系统性分析了多模态统一模型中“理解”与“生成”之间的能力差距，针对性地在推理生成和知识迁移任务中发现并量化了该差距，并探索了缓解手段。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态统一模型取得了很大进展，但“理解”能力是否真正提升了“生成”能力仍未有定论。为此，作者设计了受控的数据集和框架，精细分析两者关联及差距。

Method: 1）提出UniSandbox评测框架，构建受控合成数据集以规避数据泄漏。2）设定理解和生成解耦的任务，分任务测试。3）在推理生成任务中，分析Chain-of-Thought（CoT）提示对结果的影响，并探究自训练方案。4）在知识迁移任务中，研究CoT和架构设计对知识迁移效果的作用。

Result: 发现理解与生成之间存在显著能力差距，主要体现在推理生成和知识迁移两个方面。显式CoT提示可有效缩小推理生成差距，自训练能让模型内化推理能力；知识迁移中CoT和基于query的架构均会影响迁移效果。

Conclusion: UniSandbox框架为后续设计真正打通理解与生成的统一多模态模型提供了实验分析基础与参考，并指明了训练与架构设计的新思路。

Abstract: Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox

</details>


### [53] [One Attention, One Scale: Phase-Aligned Rotary Positional Embeddings for Mixed-Resolution Diffusion Transformer](https://arxiv.org/abs/2511.19778)
*Haoyu Wu,Jingyi Xu,Qiaomu Miao,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: 论文指出，在使用扩散Transformer进行混合分辨率去噪时，常用的RoPE线性插值会导致注意力机制失效。为此，作者提出一种新的Attention机制CRPA，有效解决了这一问题。CRPA无需重新训练，适用于现有模型，并实现了更高保真度和效率的生成效果。


<details>
  <summary>Details</summary>
Motivation: 混合分辨率输入在图像/视频生成任务中很常见，但采用标准线性插值的RoPE会导致模型对不同分辨率Token处理时注意力机制结构性崩溃，特别是预训练DiT模型非常脆弱，会引发模糊、伪影甚至完全失效。为解决此结构性缺陷，作者开展研究。

Method: 提出Cross-Resolution Phase-Aligned Attention（CRPA），在每次注意力计算时，调整RoPE索引映射，使所有Q/K位置都映射到Query的步长上，从而物理距离变化总是导致一致的相位增量，使不同分辨率Token的相位对齐，消除了相位混叠，且无需重新训练，作为简单的drop-in替换即可。

Result: CRPA可直接应用于预训练的DiT模型，全面稳定所有注意力头和层，在高保真、高效率的混合分辨率生成（图像和视频）任务中，CRPA都超过了先前的最新方法。

Conclusion: CRPA是一种训练无关、易于集成的结构性解决方案，能彻底消除混合分辨率扩散Transformer中RoPE相关的失效问题，对于实际生成任务效果显著提升。

Abstract: We identify a core failure mode that occurs when using the usual linear interpolation on rotary positional embeddings (RoPE) for mixed-resolution denoising with Diffusion Transformers. When tokens from different spatial grids are mixed, the attention mechanism collapses. The issue is structural. Linear coordinate remapping forces a single attention head to compare RoPE phases sampled at incompatible rates, creating phase aliasing that destabilizes the score landscape. Pretrained DiTs are especially brittle-many heads exhibit extremely sharp, periodic phase selectivity-so even tiny cross-rate inconsistencies reliably cause blur, artifacts, or full collapse.
  To this end, our main contribution is Cross-Resolution Phase-Aligned Attention (CRPA), a training-free drop-in fix that eliminates this failure at its source. CRPA modifies only the RoPE index map for each attention call: all Q/K positions are expressed on the query's stride so that equal physical distances always induce identical phase increments. This restores the precise phase patterns that DiTs rely on. CRPA is fully compatible with pretrained DiTs, stabilizes all heads and layers uniformly. We demonstrate that CRPA enables high-fidelity and efficient mixed-resolution generation, outperforming previous state-of-the-art methods on image and video generation.

</details>


### [54] [Reading Between the Lines: Abstaining from VLM-Generated OCR Errors via Latent Representation Probes](https://arxiv.org/abs/2511.19806)
*Jihan Yao,Achin Kulshrestha,Nathalie Rauschmayr,Reed Roberts,Banghua Zhu,Yulia Tsvetkov,Federico Tombari*

Main category: cs.CV

TL;DR: 本文提出了一种新方法Latent Representation Probing（LRP），通过挖掘视觉语言模型（VLM）内部状态来提升其在不确定时选择不作答（abstention）的能力，并在多个基准测试上实现了较大提升。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLM）应用于安全关键场景，其在面对不确定性时选择不作答的能力变得至关重要，尤其是在场景文字问答（STVQA）任务中。现有abstention方法存在效果不佳或需求不适配的问题，因此需要新的解决思路。

Method: 本文提出Latent Representation Probing（LRP）方法，针对VLM的隐层状态或注意力模式训练轻量级探针。探针设计包括跨层拼接表示、对视觉token的注意力聚合、以及多层投票集合三种方式。

Result: 在涵盖图像和视频模式的四个基准上，LRP方法的abstention准确率比最优基线提升了7.6%。进一步分析发现，探针对多样的不确定性源和数据集都有效，最优信号常出现在中间层。

Conclusion: 本文工作为部署可靠、能自我检测置信度的AI系统提供了新框架；相较依赖输出结果，直接利用模型内部信号更加可靠和泛化，可显著提升安全性与实用性。

Abstract: As VLMs are deployed in safety-critical applications, their ability to abstain from answering when uncertain becomes crucial for reliability, especially in Scene Text Visual Question Answering (STVQA) tasks. For example, OCR errors like misreading "50 mph" as "60 mph" could cause severe traffic accidents. This leads us to ask: Can VLMs know when they can't see? Existing abstention methods suggest pessimistic answers: they either rely on miscalibrated output probabilities or require semantic agreement unsuitable for OCR tasks. However, this failure may indicate we are looking in the wrong place: uncertainty signals could be hidden in VLMs' internal representations.
  Building on this insight, we propose Latent Representation Probing (LRP): training lightweight probes on hidden states or attention patterns. We explore three probe designs: concatenating representations across all layers, aggregating attention over visual tokens, and ensembling single layer probes by majority vote. Experiments on four benchmarks across image and video modalities show LRP improves abstention accuracy by 7.6\% over best baselines. Our analysis reveals: probes generalize across various uncertainty sources and datasets, and optimal signals emerge from intermediate rather than final layers. This establishes a principled framework for building deployment-ready AI systems by detecting confidence signals from internal states rather than unreliable outputs.

</details>


### [55] [ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding](https://arxiv.org/abs/2511.19827)
*Byeongjun Park,Byung-Hoon Kim,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频重拍生成方法ReDirector，能更好地控制摄像机视角，实现不同长度、动态拍摄的视频的重拍。通过改进RoPE和引入RoCE编码，有效提升了摄像机控制性和生成视频的一致性与质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频重拍方法在处理动态摄像和变长视频时，常因为RoPE（旋转位置编码）的空间时间位置对齐问题，导致生成视频效果有限。作者希望改进这一问题，使方法适应不同摄像机轨迹和视频长度。

Method: 提出ReDirector方法，首先修正RoPE在空间时间对齐的问题，然后创新性地引入摄像机条件的RoPE相位移（Rotary Camera Encoding, RoCE），以捕捉输入和目标视频内外的多视角关系。该方法整合了摄像机轨迹信息，使重拍生成的表现更广泛适用，并提升了对视频内容的控制能力。

Result: 实验结果表明，ReDirector在摄像机控制、几何一致性以及视频质量上均获得了显著提升，能适应多种摄像机轨迹与视频长度，在动态物体定位及背景保持等方面表现优异。

Conclusion: 通过修正RoPE和引入RoCE，本文提出的方法在许多评价指标上都超越了现有方法，为变长度、动态视频的重拍生成提供了一种更为强大和通用的解决方案。

Abstract: We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding (RoCE), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE, our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation. Extensive experiments further demonstrate significant improvements in camera controllability, geometric consistency, and video quality across various trajectories and lengths.

</details>


### [56] [Large Language Model Aided Birt-Hogg-Dube Syndrome Diagnosis with Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2511.19834)
*Haoqing Li,Jun Shi,Xianmeng Chen,Qiwei Jia,Rui Wang,Wei Wei,Hong An,Xiaowen Hu*

Main category: cs.CV

TL;DR: 本文提出了一种名为BHD-RAG的多模态检索增强生成框架，有效提升了通过CT影像诊断罕见的Birt-Hogg-Dube综合征的准确性，并缓解了大模型在专科知识缺乏场景下的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: Birt-Hogg-Dube综合征等弥漫性囊性肺病的CT影像诊断难点在于临床样本稀少和不同疾病间特征差异小，传统深度学习方法和通用大模型易产生幻觉，缺乏可信的专业佐证。

Method: BHD-RAG框架包括：1）由专业代理生成影像特征描述，构建多模态病例库；2）基于余弦相似度的检索器为查询图像找出相关病历对；3）多模态大模型融合检索结果与影像信息进行诊断。

Result: 在包含四类弥漫性囊性肺病的真实数据集上，BHD-RAG显著提高了诊断准确率，并能生成与专家认知一致的循证描述。

Conclusion: BHD-RAG通过结合领域知识与检索增强的生成方式，缓解了大语言模型幻觉问题，有望推动罕见肺病的精准诊断，为相关医学AI模型提供借鉴。

Abstract: Deep learning methods face dual challenges of limited clinical samples and low inter-class differentiation among Diffuse Cystic Lung Diseases (DCLDs) in advancing Birt-Hogg-Dube syndrome (BHD) diagnosis via Computed Tomography (CT) imaging. While Multimodal Large Language Models (MLLMs) demonstrate diagnostic potential fo such rare diseases, the absence of domain-specific knowledge and referable radiological features intensify hallucination risks. To address this problem, we propose BHD-RAG, a multimodal retrieval-augmented generation framework that integrates DCLD-specific expertise and clinical precedents with MLLMs to improve BHD diagnostic accuracy. BHDRAG employs: (1) a specialized agent generating imaging manifestation descriptions of CT images to construct a multimodal corpus of DCLDs cases. (2) a cosine similarity-based retriever pinpointing relevant imagedescription pairs for query images, and (3) an MLLM synthesizing retrieved evidence with imaging data for diagnosis. BHD-RAG is validated on the dataset involving four types of DCLDs, achieving superior accuracy and generating evidence-based descriptions closely aligned with expert insights.

</details>


### [57] [Rectified SpaAttn: Revisiting Attention Sparsity for Efficient Video Generation](https://arxiv.org/abs/2511.19835)
*Xuewen Liu,Zhikai Li,Jing Zhang,Mengjuan Chen,Qingyi Gu*

Main category: cs.CV

TL;DR: 本文提出了Rectified SpaAttn，一种提升扩散Transformer视频生成模型注意力稀疏性的方法，在显著加快推理速度的同时，保持高生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散Transformer在视频生成任务中因注意力计算的高复杂度导致处理延迟。虽然注意力稀疏性可减少计算，但现有方法在加速的同时显著损害模型性能。

Method: 提出Rectified SpaAttn，通过引入隐式全注意力参考，校正稀疏注意力分配。具体方法包括对关键token采用隔离池化重新分配注意力，对非关键token用增益感知池化校正，在Triton中定制融合了该方法。

Result: 在HunyuanVideo和Wan 2.1等模型中集成后，推理速度分别提升3.33倍和2.08倍，同时保持较高的生成质量。

Conclusion: Rectified SpaAttn在加速Transformer视频生成的同时，解决了稀疏注意力导致的系统性偏差，可广泛应用于视频生成领域，且已开源。

Abstract: Diffusion Transformers dominate video generation, but the quadratic complexity of attention computation introduces substantial latency. Attention sparsity reduces computational costs by focusing on critical tokens while ignoring non-critical tokens. However, existing methods suffer from severe performance degradation. In this paper, we revisit attention sparsity and reveal that existing methods induce systematic biases in attention allocation: (1) excessive focus on critical tokens amplifies their attention weights; (2) complete neglect of non-critical tokens causes the loss of relevant attention weights. To address these issues, we propose Rectified SpaAttn, which rectifies attention allocation with implicit full attention reference, thereby enhancing the alignment between sparse and full attention maps. Specifically: (1) for critical tokens, we show that their bias is proportional to the sparse attention weights, with the ratio governed by the amplified weights. Accordingly, we propose Isolated-Pooling Attention Reallocation, which calculates accurate rectification factors by reallocating multimodal pooled weights. (2) for non-critical tokens, recovering attention weights from the pooled query-key yields attention gains but also introduces pooling errors. Therefore, we propose Gain-Aware Pooling Rectification, which ensures that the rectified gain consistently surpasses the induced error. Moreover, we customize and integrate the Rectified SpaAttn kernel using Triton, achieving up to 3.33 and 2.08 times speedups on HunyuanVideo and Wan 2.1, respectively, while maintaining high generation quality. We release Rectified SpaAttn as open-source at https://github.com/BienLuky/Rectified-SpaAttn .

</details>


### [58] [4DWorldBench: A Comprehensive Evaluation Framework for 3D/4D World Generation Models](https://arxiv.org/abs/2511.19836)
*Yiting Lu,Wei Luo,Peiyan Tu,Haoran Li,Hanxin Zhu,Zihao Yu,Xingrui Wang,Xinyi Chen,Xinge Peng,Xin Li,Zhibo Chen*

Main category: cs.CV

TL;DR: 论文提出了4DWorldBench基准，用于系统性评估新一代多模态3D/4D“世界生成模型”的四大能力：感知质量、条件4D对齐、物理真实感和4D一致性。该基准支持多种输入方式（图像、视频、文本）并创新性引入多模态自适应条件与评判体系，提升主观一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉生成模型多集中于2D图像或视频，缺乏对3D/4D世界真实性与一致性的系统评估。且以往基准评价维度分散、难以统一和客观比较不同模型，难以满足虚拟现实、自动驾驶等应用需求。因此亟需一个统一、全面的评测框架。

Method: 作者提出4DWorldBench基准，涵盖Image-to-3D/4D、Video-to-4D、Text-to-3D/4D等任务，设置利于评估的四大维度。创新点包括：所有多模态输入映射到统一文本空间；引入LLM、MLLM与传统网络综合评判机制；支持自适应选择评测工具，提高与人类主观判断的一致性。

Result: 经过初步的人体实验，采用自适应工具选择后，模型评判结果与真实人的主观判断更为一致。基准有效地评估了世界生成模型在多模态对齐、物理一致性等关键能力上的表现。

Conclusion: 4DWorldBench实现了对复杂3D/4D世界生成模型的客观、全面评估，加强了主观一致性，为未来“视觉生成”向“世界生成”转型提供了基础工具和标准。

Abstract: World Generation Models are emerging as a cornerstone of next-generation multimodal intelligence systems. Unlike traditional 2D visual generation, World Models aim to construct realistic, dynamic, and physically consistent 3D/4D worlds from images, videos, or text. These models not only need to produce high-fidelity visual content but also maintain coherence across space, time, physics, and instruction control, enabling applications in virtual reality, autonomous driving, embodied intelligence, and content creation. However, prior benchmarks emphasize different evaluation dimensions and lack a unified assessment of world-realism capability. To systematically evaluate World Models, we introduce the 4DWorldBench, which measures models across four key dimensions: Perceptual Quality, Condition-4D Alignment, Physical Realism, and 4D Consistency. The benchmark covers tasks such as Image-to-3D/4D, Video-to-4D, Text-to-3D/4D. Beyond these, we innovatively introduce adaptive conditioning across multiple modalities, which not only integrates but also extends traditional evaluation paradigms. To accommodate different modality-conditioned inputs, we map all modality conditions into a unified textual space during evaluation, and further integrate LLM-as-judge, MLLM-as-judge, and traditional network-based methods. This unified and adaptive design enables more comprehensive and consistent evaluation of alignment, physical realism, and cross-modal coherence. Preliminary human studies further demonstrate that our adaptive tool selection achieves closer agreement with subjective human judgments. We hope this benchmark will serve as a foundation for objective comparisons and improvements, accelerating the transition from "visual generation" to "world generation." Our project can be found at https://yeppp27.github.io/4DWorldBench.github.io/.

</details>


### [59] [Face, Whole-Person, and Object Classification in a Unified Space Via The Interleaved Multi-Domain Identity Curriculum](https://arxiv.org/abs/2511.19846)
*Thomas M Metz,Matthew Q Hill,Alice J O'Toole*

Main category: cs.CV

TL;DR: 本文提出了一种新的训练方法，使得视觉基础模型能在同一嵌入空间中兼顾物体识别、人脸识别（高/低质量）和全身人物识别，同时大幅缓解灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 传统视觉基础模型虽可通过微调实现不同识别任务，但容易出现灾难性遗忘，导致新任务表现提升时，旧任务性能下降。研究动机在于实现多任务并行学习，既扩展模型能力，又不牺牲每个任务的表现。

Method: 本文提出了两种Interleaved Multi-Domain Identity Curriculum（IMIC）训练策略，在同一基础模型（如DINOv3、CLIP、EVA-02）上，采用梯度耦合交错学习四项任务。该方法通过统一的训练安排，使得模型在不遗忘旧知识的前提下，习得新任务。

Result: IMIC方法在EVA-02和CLIP等基础模型上表现优异，能在所有四个任务上达到与领域专家乐观相当的水平，且多任务能力优于人类。分析表明，模型的嵌入空间对四任务表现出良好的线性可分性，并且特征高度共享。此外，方案对模型的泛化能力影响极小。

Conclusion: 提出的IMIC方法可有效缓解视觉基础模型在多任务微调中的灾难性遗忘，实现四种不同视觉任务的高效统一表征，同时维持模型的广泛泛化能力。

Abstract: Vision foundation models can perform generalized object classification in zero-shot mode, and face/person recognition when they are fine-tuned. However, fine-tuned models suffer from catastrophic forgetting. We create models that perform four tasks (object recognition, face recognition from high- and low-quality images, and person recognition from whole-body images) in a single embedding space -- without incurring substantial catastrophic forgetting. To accomplish this, we introduce two variants of the Interleaved Multi-Domain Identity Curriculum (IMIC): a gradient-coupled, interleaving training schedule that fine-tunes a foundation backbone simultaneously on all four tasks. The IMIC method proved effective with three foundation model bases: DINOv3, CLIP, and EVA-02. Two of these (EVA-02 and CLIP) performed comparably with domain experts on all four tasks concurrently and were more accurate than humans at multi-tasking across face, body, and object datasets. Further, we demonstrate that our approach does not substantially harm out-of-distribution generalization, thus maintaining a key property of foundation models. Analysis of the most accurate model variants (EVA-02 + IMIC A and B) showed linearly separable representations of the four tasks in the unified embedding space, but with substantial sharing of features across tasks. Fewer than 100 PCs calculated from any one task could perform all other tasks with nearly zero performance degradation.

</details>


### [60] [DOGE: Differentiable Bezier Graph Optimization for Road Network Extraction](https://arxiv.org/abs/2511.19850)
*Jiahui Sun,Junran Lu,Jinhui Yin,Yishuo Xu,Yuanqi Li,Yanwen Guo*

Main category: cs.CV

TL;DR: 本文提出了一种基于Bezier曲线的道路网络自动提取方法DOGE，无需曲线向量标注，直接从分割掩码中优化参数曲线，刷新了主流道路提取数据集的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于多段线的道路网络提取方法难以表达复杂曲线几何，而道路本质上呈曲线状，因此需要更自然的曲线建模表述方式。最大障碍在于难以获得精确的曲线向量标注。

Method: 方法创新地用Bezier Graph作为道路表征，并提出DOGE框架，通过全局优化直接从分割掩码中学习Bezier Graph参数。其包含DiffAlign模块（用可微渲染连续优化几何）和TopoAdapt模块（用离散算子优化拓扑），交替进行整体提升网络表达能力，无需曲线真值数据。

Result: 在大规模SpaceNet和CityScale两个主流公开数据集上，DOGE方法性能达到新SOTA，高保真自动生成矢量道路网络。

Conclusion: DOGE方法摆脱了曲线向量标注依赖，以参数曲线新范式提升了自动道路网络提取精度，有望推动高质量矢量地图生成发展。

Abstract: Automatic extraction of road networks from aerial imagery is a fundamental task, yet prevailing methods rely on polylines that struggle to model curvilinear geometry. We maintain that road geometry is inherently curve-based and introduce the Bézier Graph, a differentiable parametric curve-based representation. The primary obstacle to this representation is to obtain the difficult-to-construct vector ground-truth (GT). We sidestep this bottleneck by reframing the task as a global optimization problem over the Bézier Graph. Our framework, DOGE, operationalizes this paradigm by learning a parametric Bézier Graph directly from segmentation masks, eliminating the need for curve GT. DOGE holistically optimizes the graph by alternating between two complementary modules: DiffAlign continuously optimizes geometry via differentiable rendering, while TopoAdapt uses discrete operators to refine its topology. Our method sets a new state-of-the-art on the large-scale SpaceNet and CityScale benchmarks, presenting a new paradigm for generating high-fidelity vector maps of road networks. We will release our code and related data.

</details>


### [61] [STAvatar: Soft Binding and Temporal Density Control for Monocular 3D Head Avatars Reconstruction](https://arxiv.org/abs/2511.19854)
*Jiankuo Zhao,Xiangyu Zhu,Zidu Wang,Zhen Lei*

Main category: cs.CV

TL;DR: STAvatar是一种新型方法，能从单目视频中重建高保真、可动画的3D头像，尤其在处理易被遮挡区域和精细细节上超越了以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有通过3D Gaussian Splatting进行3D头像重建的方法受限于刚性运动和表达力不足，且在处理嘴内部、眼睑等高频遮挡区域时效果不好，有必要提出更灵活、细致的重建方法。

Method: 提出STAvatar，包含两个核心技术：(1) UV-Adaptive Soft Binding 框架，结合图像和几何先验，在UV空间内为每个高斯点学习特征偏移，实现动态重采样，兼容自适应密度控制并适应不同形状和材质变化；(2) Temporal ADC策略，先聚类结构相似帧，再通过新颖的感知误差准则引导密度增加于细节区域。

Result: 在四个主流数据集上进行了大量实验，STAvatar在重建精细细节及高频遮挡区域方面，表现出业界领先的高重建精度。

Conclusion: STAvatar有效提升了3D头像动画化重建的表达力和细节还原，尤其在难处理的高频遮挡区表现优异，达到SOTA水平，相关代码即将开源。

Abstract: Reconstructing high-fidelity and animatable 3D head avatars from monocular videos remains a challenging yet essential task. Existing methods based on 3D Gaussian Splatting typically bind Gaussians to mesh triangles and model deformations solely via Linear Blend Skinning, which results in rigid motion and limited expressiveness. Moreover, they lack specialized strategies to handle frequently occluded regions (e.g., mouth interiors, eyelids). To address these limitations, we propose STAvatar, which consists of two key components: (1) a UV-Adaptive Soft Binding framework that leverages both image-based and geometric priors to learn per-Gaussian feature offsets within the UV space. This UV representation supports dynamic resampling, ensuring full compatibility with Adaptive Density Control (ADC) and enhanced adaptability to shape and textural variations. (2) a Temporal ADC strategy, which first clusters structurally similar frames to facilitate more targeted computation of the densification criterion. It further introduces a novel fused perceptual error as clone criterion to jointly capture geometric and textural discrepancies, encouraging densification in regions requiring finer details. Extensive experiments on four benchmark datasets demonstrate that STAvatar achieves state-of-the-art reconstruction performance, especially in capturing fine-grained details and reconstructing frequently occluded regions. The code will be publicly available.

</details>


### [62] [Temporal-Visual Semantic Alignment: A Unified Architecture for Transferring Spatial Priors from Vision Models to Zero-Shot Temporal Tasks](https://arxiv.org/abs/2511.19856)
*Xiangkai Ma,Han Zhang,Wenzhong Li,Sanglu Lu*

Main category: cs.CV

TL;DR: 本文提出了TimeArtist框架，实现了时间序列到高质量图像的直接生成，并在跨模态语义对齐方面取得突破。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型虽在文本和图像跨模态生成上表现优异，但利用非视觉的连续序列（如时间序列）作为条件信号进行图像生成的潜力尚未得到充分挖掘。传统方法将序列转为伪图像进行预测，无法实现语义级的有效对齐。

Method: 提出TimeArtist框架，首创“warmup-align”范式：首步，使用双自编码器和共享量化器在大规模数据集上自监督训练，学习模态共享表征；随后冻结编码器和量化器，引入投影模块，在表征层面对齐时间序列和视觉样本。

Result: TimeArtist支持从时间序列直接生成高质量、多样化的图像，捕捉时间变化并以此渲染图像风格；实验结果显示在各类图像生成指标和零样本时间相关任务上表现优越。

Conclusion: 本工作为跨模态生成提供新范式，有效弥合了时间动态与视觉语义间的差距。

Abstract: Large Multimodal Models (LMMs) have achieved remarkable progress in aligning and generating content across text and image modalities. However, the potential of using non-visual, continuous sequential, as a conditioning signal for high-fidelity image generation remains largely unexplored. Furthermore, existing methods that convert series into "pseudo-images" for temporal forecasting fail to establish semantic-level alignment. In this paper, we propose TimeArtist, a temporal-visual conversion framework that pioneers semantic-level alignment between time series fluctuations and visual concepts. It pioneers a "warmup-align" paradigm: first, a dual-autoencoder and shared quantizer are self-supervised trained on large-scale datasets to learn modality-shared representations. Then, the encoders and quantizer are frozen, and a projection is introduced to align temporal and visual samples at the representation level. TimeArtist establishes a versatile cross-modal framework, enabling high-quality, diverse image generation directly from time series, while capturing temporal fluctuation patterns to render images as styles transfer. Extensive experiments show that TimeArtist achieves satisfactory performance in image generation metrics, while also attaining superior results in zero-shot temporal tasks. Our work establishes a new paradigm for cross-modal generation, bridging the gap between temporal dynamics and visual semantics.

</details>


### [63] [ChessMamba: Structure-Aware Interleaving of State Spaces for Change Detection in Remote Sensing Images](https://arxiv.org/abs/2511.19882)
*Lei Ding,Tong Liu,Xuanguang Liu,Xiangyun Liu,Haitao Guo,Jun Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为ChessMamba的结构感知型时空序列建模框架，用于提升多时相遥感影像变化检测的精度，在多项任务上优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 多时相遥感图像的变化检测因异质性和时空错位问题而难以实现细粒度识别。现有基于视觉Transformer或状态空间模型的方法，在序列化时往往破坏局部结构一致性，难以在错位条件下准确定位变化。

Method: 作者提出ChessMamba框架。其核心方法包括：(1) 棋盘式交错与蛇形扫描，将多时相特征统一为单一序列，通过缩短特征交互路径提升定位准确性；(2) 结构感知的多扩张卷积融合，有选择地采集每个时相的中心和角落邻域以增强特征表达。此外还包括轻量级的跨源交互模块和SpatialMamba编码器。

Result: 在三类变化检测任务——二元变化检测、语义级变化检测和多模态建筑损毁评估——上进行综合实验，结果显示ChessMamba能有效融合异质特征，较当前主流方法有显著精度提升。

Conclusion: ChessMamba框架能在复杂场景下实现更精准的变化检测和定位，在多项标准任务中的实验结果证明了其优越性，对相关领域有较强的实用价值。

Abstract: Change detection (CD) in multitemporal remote sensing imagery presents significant challenges for fine-grained recognition, owing to heterogeneity and spatiotemporal misalignment. However, existing methodologies based on vision transformers or state-space models typically disrupt local structural consistency during temporal serialization, obscuring discriminative cues under misalignment and hindering reliable change localization. To address this, we introduce ChessMamba, a structure-aware framework leveraging interleaved state-space modeling for robust CD with multi-temporal inputs. ChessMamba integrates a SpatialMamba encoder with a lightweight cross-source interaction module, featuring two key innovations: (i) Chessboard interleaving with snake scanning order, which serializes multi-temporal features into a unified sequence within a single forward pass, thereby shortening interaction paths and enabling direct comparison for accurate change localization; and (ii) Structure-aware fusion via multi-dilated convolutions, selectively capturing center-and-corner neighborhood contexts within each mono-temporal. Comprehensive evaluations on three CD tasks, including binary CD, semantic CD and multimodal building damage assessment, demonstrate that ChessMamba effectively fuses heterogeneous features and achieves substantial accuracy improvements over state-of-the-art methods.The relevant code will be available at: github.com/DingLei14/ChessMamba.

</details>


### [64] [Distilling Cross-Modal Knowledge via Feature Disentanglement](https://arxiv.org/abs/2511.19887)
*Junhong Liu,Yuan Zhang,Tao Huang,Wenchao Xu,Renyu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种频域解耦的跨模态知识蒸馏方法（FD-CMKD），显著提升了视觉-语言等跨模态任务的知识迁移效果，实验结果优于传统和主流跨模态KD方法。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏在单一模态（如图像或文本内部）效果显著，但在跨模态（如视觉到语言）中表现不佳，原因在于不同模态的表示存在差异，导致知识难以有效迁移。作者希望解决这种跨模态知识迁移低效的问题。

Method: 方法核心是“频域解耦”，通过分析跨模态特征在频域上的表现，发现低频特征（如整体结构）在模态间一致性高，高频特征（如细节）不一致。于是分别对低频和高频施加差异化的蒸馏损失：对低频强对齐，对高频弱对齐。此外加入尺度一致性损失处理模态分布差异，并使用共享分类器统一特征空间。

Result: 在多个基准数据集上，所提方法在跨模态知识蒸馏任务中，性能显著优于传统知识蒸馏和现有最优跨模态KD方法。

Conclusion: 频域解耦及尺度一致性的策略能有效提升跨模态KD效率，为未来多模态知识迁移提供了新思路。

Abstract: Knowledge distillation (KD) has proven highly effective for compressing large models and enhancing the performance of smaller ones. However, its effectiveness diminishes in cross-modal scenarios, such as vision-to-language distillation, where inconsistencies in representation across modalities lead to difficult knowledge transfer. To address this challenge, we propose frequency-decoupled cross-modal knowledge distillation, a method designed to decouple and balance knowledge transfer across modalities by leveraging frequency-domain features. We observed that low-frequency features exhibit high consistency across different modalities, whereas high-frequency features demonstrate extremely low cross-modal similarity. Accordingly, we apply distinct losses to these features: enforcing strong alignment in the low-frequency domain and introducing relaxed alignment for high-frequency features. We also propose a scale consistency loss to address distributional shifts between modalities, and employ a shared classifier to unify feature spaces. Extensive experiments across multiple benchmark datasets show our method substantially outperforms traditional KD and state-of-the-art cross-modal KD approaches. Code is available at https://github.com/Johumliu/FD-CMKD.

</details>


### [65] [LiMT: A Multi-task Liver Image Benchmark Dataset](https://arxiv.org/abs/2511.19889)
*Zhe Liu,Kai Han,Siqi Ma,Yan Zhu,Jun Chen,Chongwen Lyu,Xinyi Qiu,Chengxuan Qian,Yuqing Song,Yi Liu,Liyuan Tian,Yang Ji,Yuefeng Li*

Main category: cs.CV

TL;DR: 本文提出并发布了一个多任务肝脏影像数据集（LiMT），支持肝脏及肿瘤分割、多标签病变分类和病变检测三大任务，旨在推动相关计算机辅助诊断技术的研究。


<details>
  <summary>Details</summary>
Motivation: 现有的肝脏影像数据集普遍仅支持单一任务，限制了计算机辅助诊断技术的发展，亟需能够同时支持多个临床任务、提升研究普适性和实用性的高质量多任务数据集。

Method: 作者基于动脉期增强CT图像，收集了150例不同类型肝脏疾病及正常病例，对每例影像由经验丰富的临床医师进行精细标注和校准，构建了一个支持分割、分类、检测等任务的公开多任务肝脏数据集。文中还回顾了现有相关的数据集和方法，并给出基线实验结果。

Result: 该数据集包含四类肝脏疾病及正常病例，已在多个典型肝脏影像任务上进行了基线测试，展示了可用于多任务学习，任务间不受异质性影响的潜力。

Conclusion: 公开的LiMT多任务数据集有望成为医学影像科研社区的重要资源，能够促进多任务学习在肝脏影像分析中的发展和应用。

Abstract: Computer-aided diagnosis (CAD) technology can assist clinicians in evaluating liver lesions and intervening with treatment in time. Although CAD technology has advanced in recent years, the application scope of existing datasets remains relatively limited, typically supporting only single tasks, which has somewhat constrained the development of CAD technology. To address the above limitation, in this paper, we construct a multi-task liver dataset (LiMT) used for liver and tumor segmentation, multi-label lesion classification, and lesion detection based on arterial phase-enhanced computed tomography (CT), potentially providing an exploratory solution that is able to explore the correlation between tasks and does not need to worry about the heterogeneity between task-specific datasets during training. The dataset includes CT volumes from 150 different cases, comprising four types of liver diseases as well as normal cases. Each volume has been carefully annotated and calibrated by experienced clinicians. This public multi-task dataset may become a valuable resource for the medical imaging research community in the future. In addition, this paper not only provides relevant baseline experimental results but also reviews existing datasets and methods related to liver-related tasks. Our dataset is available at https://drive.google.com/drive/folders/1l9HRK13uaOQTNShf5pwgSz3OTanWjkag?usp=sharing.

</details>


### [66] [VeriSciQA: An Auto-Verified Dataset for Scientific Visual Question Answering](https://arxiv.org/abs/2511.19899)
*Yuyi Li,Daoyuan Chen,Zhen Wang,Yutong Lu,Yaliang Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于验证的QA生成框架，用于构建高质量大规模科学视觉问答数据集VeriSciQA，有效提升了开源模型在科学图表视觉问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 开源的大型视觉-语言模型（LVLMs）在科学视觉问答任务（SVQA）上表现不佳，主要由于缺乏大规模、高质量的公共SVQA数据集。现有利用LVLM自动生成数据的方法却容易带来系统性错误，需要一种更高质量的数据构建方式。

Method: 提出了一个“生成-验证”框架：首先结合图表相关文本生成QA对，然后通过跨模态一致性检查和辅助筛选机制，剔除不正确的问题对。基于该方法，构建了跨20个科学领域、12种图表类型、包含20,351个QA对的数据集VeriSciQA。

Result: VeriSciQA成为SVQA领域新的挑战性基准数据集。领先的开源模型在该数据集上的准确率为64%，远低于私有模型82%。在VeriSciQA上微调的模型在多个SVQA基准上表现持续提升，且随数据量增加表现越好，优于用已有数据集训练的模型。人工评测也证实VeriSciQA数据对的正确性高于以往数据。

Conclusion: 通过以验证为核心的数据生成框架，可大规模扩展高质量SVQA数据集，显著推动开源社区在科学视觉问答上的能力提升。

Abstract: Large Vision-Language Models (LVLMs) show promise for scientific applications, yet open-source models still struggle with Scientific Visual Question Answering (SVQA), namely answering questions about figures from scientific papers. A key bottleneck lies in the lack of public, large-scale, high-quality SVQA datasets. Although recent work uses LVLMs to synthesize data at scale, we identify systematic errors in their resulting QA pairs, stemming from LVLMs' inherent limitations and information asymmetry between figures and text. To address these challenges, we propose a verification-centric Generate-then-Verify framework that first generates QA pairs with figure-associated textual context, then applies cross-modal consistency checks against figures along with auxiliary filters to eliminate erroneous pairs. We instantiate this framework to curate VeriSciQA, a dataset of 20,351 QA pairs spanning 20 scientific domains and 12 figure types. VeriSciQA poses a challenging benchmark for open-source models, with a substantial accuracy gap between the leading open-source models (64%) and a proprietary model (82%). Moreover, models fine-tuned on VeriSciQA achieve consistent improvements on SVQA benchmarks, with performance gains that scale with data size and surpass models trained on existing datasets. Human evaluation further validates the superior correctness of VeriSciQA. Together, these evidences demonstrate that continued data expansion by our scalable framework can further advance SVQA capability in the open-source community.

</details>


### [67] [Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning](https://arxiv.org/abs/2511.19900)
*Jiaqi Liu,Kaiwen Xiong,Peng Xia,Yiyang Zhou,Haonian Ji,Lu Feng,Siwei Han,Mingyu Ding,Huaxiu Yao*

Main category: cs.CV

TL;DR: 本论文提出了一种新型视觉-语言智能体Agent0-VL，通过工具集成推理实现持续自我进化和无人工标注的自我改进，并在多模态推理任务中取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言智能体在多模态推理任务上虽有进展，但受制于人类标注的监督，纯文本自评方法难以有效验证复杂的视觉推理步骤，常出现评价幻觉。

Method: 受工具集成推理的启发，作者提出Agent0-VL，将工具使用融合到推理、自我评估和自我修复中。该模型统一了'求解器'和'验证者'两个角色，通过工具支撑的多轮推理、结构化反馈和自我奖励，形成自我进化推理循环，并利用工具辅助验证与强化学习促进稳定自我改进，全流程不依赖外部奖励或人工标注。

Result: 在几何问题求解和视觉科学分析实验中，Agent0-VL在无需人工注释和外部奖励的前提下，性能相较基础模型提升12.5%。

Conclusion: Agent0-VL通过工具集成的自我推理和验证，实现了无需外部监督的连续自我改进，为多模态智能体的自我进化提供了新范式。

Abstract: Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at \href{https://github.com/aiming-lab/Agent0/Agent0-VL}{this https URL}.

</details>


### [68] [MHB: Multimodal Handshape-aware Boundary Detection for Continuous Sign Language Recognition](https://arxiv.org/abs/2511.19907)
*Mingyu Zhao,Zhanfu Yang,Yang Zhou,Zhaoyang Xia,Can Jin,Xiaoxiao He,Carol Neidle,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: 提出了一种连续手语识别的多模态方法，结合3D骨架特征与手型识别实现手语分割和识别，相较于以往方法取得显著进展。


<details>
  <summary>Details</summary>
Motivation: 现有的连续手语自动识别面临分割精度与识别准确率的问题，尤其在手语词边界检测和动态特征建模方面存在挑战。如何精确定位手语词的开始与结束帧并有效融合多模态信息，是提升识别系统鲁棒性的关键。

Method: 该方法采用机器学习先检测句子中手语词的起止帧，再识别分割出的手语词。利用从手语视频中提取的3D骨架特征捕捉手语动态特征，并预训练一个手型分类器，检测手语词典型边界手型（基于87类标准手型），通过多模态融合模块综合分割与手型信息。边界信息用于后续识别模型训练，训练数据包含孤立词和连续语句中分割获得的词。

Result: 在ASLLRP手语语料库上评估表明，该方法在手语分割与识别任务上，相较以往方法取得了显著提升。

Conclusion: 多模态融合3D骨架和手型特征，以及更精确的边界检测，可以显著提升连续手语视频中的分割和识别效果。

Abstract: This paper presents a multimodal approach for continuous sign recognition that first uses machine learning to detect the start and end frames of signs in videos of American Sign Language (ASL) sentences, and then recognizes the segmented signs. For improved robustness, we use 3D skeletal features extracted from sign language videos to capture the convergence of sign properties and their dynamics, which tend to cluster at sign boundaries. Another focus of this work is the incorporation of information from 3D handshape for boundary detection. To detect handshapes normally expected at the beginning and end of signs, we pretrain a handshape classifier for 87 linguistically defined canonical handshape categories using a dataset that we created by integrating and normalizing several existing datasets. A multimodal fusion module is then used to unify the pretrained sign video segmentation framework and the handshape classification models. Finally, the estimated boundaries are used for sign recognition, where the recognition model is trained on a large database containing both citation-form isolated signs and signs pre-segmented (based on manual annotations) from continuous signing, as such signs often differ in certain respects. We evaluate our method on the ASLLRP corpus and demonstrate significant improvements over previous work.

</details>


### [69] [Motion Marionette: Rethinking Rigid Motion Transfer via Prior Guidance](https://arxiv.org/abs/2511.19909)
*Haoxuan Wang,Jiachen Tao,Junyi Wu,Gaowen Liu,Ramana Rao Kompella,Yan Yan*

Main category: cs.CV

TL;DR: 本论文提出了Motion Marionette框架，实现了从单目源视频到单视角目标图像的刚体运动零样本迁移，无需依赖外部先验，能够高效生成一致且可控的视频。


<details>
  <summary>Details</summary>
Motivation: 现有运动迁移方法依赖几何、生成或仿真先验，这些外部约束往往导致泛化性与时间一致性之间的权衡，限制了运动迁移方法在多样目标上的适用性。

Method: Motion Marionette采用内部的空间-时间先验，首先将源视频与目标图像统一提升到三维表达空间，再从源视频提取运动轨迹，构建反映空间变化的SpaT先验（不依赖几何与语义），该先验与目标物体整合，生成可控速度场，最后通过基于位置的动力学方法优化视觉效果。

Result: 实验结果显示，Motion Marionette方法可适用于多样对象，生成高时间一致性、运动与源视频一致且可控的视频画面。

Conclusion: 与传统方法相比，本方法无需外部先验，更易泛化，可用于灵活高效的视频制作，提升刚体运动迁移的质量及应用范围。

Abstract: We present Motion Marionette, a zero-shot framework for rigid motion transfer from monocular source videos to single-view target images. Previous works typically employ geometric, generative, or simulation priors to guide the transfer process, but these external priors introduce auxiliary constraints that lead to trade-offs between generalizability and temporal consistency. To address these limitations, we propose guiding the motion transfer process through an internal prior that exclusively captures the spatial-temporal transformations and is shared between the source video and any transferred target video. Specifically, we first lift both the source video and the target image into a unified 3D representation space. Motion trajectories are then extracted from the source video to construct a spatial-temporal (SpaT) prior that is independent of object geometry and semantics, encoding relative spatial variations over time. This prior is further integrated with the target object to synthesize a controllable velocity field, which is subsequently refined using Position-Based Dynamics to mitigate artifacts and enhance visual coherence. The resulting velocity field can be flexibly employed for efficient video production. Empirical results demonstrate that Motion Marionette generalizes across diverse objects, produces temporally consistent videos that align well with the source motion, and supports controllable video generation.

</details>


### [70] [Coupled Physics-Gated Adaptation: Spatially Decoding Volumetric Photochemical Conversion in Complex 3D-Printed Objects](https://arxiv.org/abs/2511.19913)
*Maryam Eftekharifar,Churun Zhang,Jialiang Wei,Xudong Cao,Hossein Heidari*

Main category: cs.CV

TL;DR: 本文提出了一个新的计算机视觉任务，即从3D视觉数据预测复杂三维打印物体的化学转化状态，并设计了Coupled Physics-Gated Adaptation (C-PGA)架构，以克服传统方法在处理物理耦合特性时的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉模型难以预测和理解三维复杂结构中的光化学转化，因为它们无法建模光学物理与材料物理间复杂的非线性耦合行为。传统测量手段效率低且成本高，需要一种通过视觉数据预测物理属性的新方法。

Method: 提出一种C-PGA多模态融合网络，通过稀疏的几何和工艺参数作为Query，对由双3D-CNN提取的视觉特征通过特征线性调制（FiLM）进行动态门控，从而显式地建模光学和材料物理的耦合效应。模型在经过扩散-衍射校正的3D原始视觉流上进行空间自适应地调整特征感知。

Result: 模型依托最大规模的3D打印样本数据集，有效实现了从3D视觉数据精准预测物体终端化学状态，显著超越传统视觉模型。

Conclusion: 该研究突破了虚拟化学表征技术，无需传统的化学检测流程，实现了对3D打印物体化学转化的高精度前馈预测，为后续制造过程的智能控制提供了新工具。

Abstract: We present a framework that pioneers the prediction of photochemical conversion in complex three-dimensionally printed objects, introducing a challenging new computer vision task: predicting dense, non-visual volumetric physical properties from 3D visual data. This approach leverages the largest-ever optically printed 3D specimen dataset, comprising a large family of parametrically designed complex minimal surface structures that have undergone terminal chemical characterisation. Conventional vision models are ill-equipped for this task, as they lack an inductive bias for the coupled, non-linear interactions of optical physics (diffraction, absorption) and material physics (diffusion, convection) that govern the final chemical state. To address this, we propose Coupled Physics-Gated Adaptation (C-PGA), a novel multimodal fusion architecture. Unlike standard concatenation, C-PGA explicitly models physical coupling by using sparse geometrical and process parameters (e.g., surface transport, print layer height) as a Query to dynamically gate and adapt the dense visual features via feature-wise linear modulation (FiLM). This mechanism spatially modulates dual 3D visual streams-extracted by parallel 3D-CNNs processing raw projection stacks and their diffusion-diffraction corrected counterparts allowing the model to recalibrate its visual perception based on the physical context. This approach offers a breakthrough in virtual chemical characterisation, eliminating the need for traditional post-print measurements and enabling precise control over the chemical conversion state.

</details>


### [71] [Scale Where It Matters: Training-Free Localized Scaling for Diffusion Models](https://arxiv.org/abs/2511.19917)
*Qin Ren,Yufei Wang,Lanqing Guo,Wen Zhang,Zhiwen Fan,Chenyu You*

Main category: cs.CV

TL;DR: 提出了一种局部自适应的推理计算分配方法（LoTTS），显著提升扩散模型生成图像的局部和整体质量，并大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型推理加速方法（如TTS）在全图范围均匀分配计算资源，导致对已高质量区域的计算浪费，局部缺陷区域又修复不足。如何在保证整体一致性的同时，聚焦于局部缺陷进行修复，提升推理效率和生成质量，是当前的挑战。

Method: 提出Localized TTS（LoTTS）框架，无需额外训练。其主要包括：1）对比交叉和自注意力信号，通过品质感知的prompt，定位图像中存在缺陷的区域，并生成连贯的掩码；2）局部扰动和去噪修复缺陷，仅对有瑕疵区域进行修正，其余区域保持不变，以保证全局一致性。

Result: 在Stable Diffusion 2.1、SDXL和FLUX等多个主流扩散模型上进行了充分实验。结果显示，LoTTS在提升局部质量与整体视觉保真度方面优于现有方法，并显著降低了2-4倍的GPU推理开销。

Conclusion: LoTTS不仅实现了训练自由、性能优异的局部推理调整，为进一步扩展扩散模型的实际推理能力提供了新方向，也证明了局部TTS的有效性和前景。

Abstract: Diffusion models have become the dominant paradigm in text-to-image generation, and test-time scaling (TTS) further improves quality by allocating more computation during inference. However, existing TTS methods operate at the full-image level, overlooking the fact that image quality is often spatially heterogeneous. This leads to unnecessary computation on already satisfactory regions and insufficient correction of localized defects. In this paper, we explore a new direction - Localized TTS - that adaptively resamples defective regions while preserving high-quality regions, thereby substantially reducing the search space. This paradigm poses two central challenges: accurately localizing defects and maintaining global consistency. We propose LoTTS, the first fully training-free framework for localized TTS. For defect localization, LoTTS contrasts cross- and self-attention signals under quality-aware prompts (e.g., high-quality vs. low-quality) to identify defective regions, and then refines them into coherent masks. For consistency, LoTTS perturbs only defective regions and denoises them locally, ensuring that corrections remain confined while the rest of the image remains undisturbed. Extensive experiments on SD2.1, SDXL, and FLUX demonstrate that LoTTS achieves state-of-the-art performance: it consistently improves both local quality and global fidelity, while reducing GPU cost by 2-4x compared to Best-of-N sampling. These findings establish localized TTS as a promising new direction for scaling diffusion models at inference time.

</details>


### [72] [HybriDLA: Hybrid Generation for Document Layout Analysis](https://arxiv.org/abs/2511.19919)
*Yufan Chen,Omar Moured,Ruiping Liu,Junwei Zheng,Kunyu Peng,Jiaming Zhang,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的文档布局分析框架HybriDLA，通过结合扩散和自回归解码方法，有效提升了复杂文档的检测精度，并在主流基准上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 传统文档布局分析主要依赖经验先验或固定查询点，这对于区域数较少且结构简单的文档尚可，但无法适应现代多样化、复杂的文档结构。因此，需要一种更灵活且鲁棒性更强的分析方法。

Method: HybriDLA框架创新性地在同一层内集成了扩散与自回归解码：其中扩散部分负责迭代地优化定位假设，自回归部分则增强了对于语义和上下文的理解。此外，还设计了多尺度特征融合编码器，以更全面地捕捉细粒度和全局特征。

Result: 该方法在DocLayNet和M$^6$Doc两个主流文档布局基准上进行了大量实验，获得了83.5%的平均精度mAP，超过了既有最佳方法。

Conclusion: HybriDLA能够有效应对各类复杂文档布局，显著提升了文档区域检测的准确率，为文档分析领域设立了新标杆。

Abstract: Conventional document layout analysis (DLA) traditionally depends on empirical priors or a fixed set of learnable queries executed in a single forward pass. While sufficient for early-generation documents with a small, predetermined number of regions, this paradigm struggles with contemporary documents, which exhibit diverse element counts and increasingly complex layouts. To address challenges posed by modern documents, we present HybriDLA, a novel generative framework that unifies diffusion and autoregressive decoding within a single layer. The diffusion component iteratively refines bounding-box hypotheses, whereas the autoregressive component injects semantic and contextual awareness, enabling precise region prediction even in highly varied layouts. To further enhance detection quality, we design a multi-scale feature-fusion encoder that captures both fine-grained and high-level visual cues. This architecture elevates performance to 83.5% mean Average Precision (mAP). Extensive experiments on the DocLayNet and M$^6$Doc benchmarks demonstrate that HybriDLA sets a state-of-the-art performance, outperforming previous approaches. All data and models will be made publicly available at https://yufanchen96.github.io/projects/HybriDLA.

</details>


### [73] [Intelligent Image Search Algorithms Fusing Visual Large Models](https://arxiv.org/abs/2511.19920)
*Kehan Wang,Tingqiong Cui,Yang Zhang,Yu Chen,Shifeng Wu,Zhenzhang Li*

Main category: cs.CV

TL;DR: 本文提出了DetVLM框架，将YOLO目标检测器与视觉大模型（VLM）结合，实现高效且精细的图像检索，主要针对细粒度目标状态和零样本检索问题。在车辆部件数据集上的实验显示该方法大幅超越传统方法，检索准确率达94.82%。


<details>
  <summary>Details</summary>
Motivation: 现有手工特征不可靠，深度学习检测器无法实现状态特定检索及零样本搜索，而VLM虽然具备语义和零样本能力，但空间定位差且计算成本高，难以直接应用于检索。因此急需架构将各方法优势互补。

Method: 提出两阶段DetVLM框架：第一阶段采用YOLO快速筛选疑似目标部件，保证高召回；第二阶段用VLM针对检测器漏检部分进行验证和状态判断，实现“状态检索”和“零样本检索”。全流程无需专门训练即可支持扩展属性和状态。

Result: DetVLM在车辆部件数据集上取得94.82%的总体检索准确率，显著高于仅用检测的基线方法。零样本检索（如司机戴口罩）准确率达94.95%，状态检索任务准确率均超过90%。

Conclusion: DetVLM有效融合目标检测和视觉大模型，实现细粒度组件和状态检索，并显著提升了零样本能力和检索准确率，现有方法的局限得到有效弥补。

Abstract: Fine-grained image retrieval, which aims to find images containing specific object components and assess their detailed states, is critical in fields like security and industrial inspection. However, conventional methods face significant limitations: manual features (e.g., SIFT) lack robustness; deep learning-based detectors (e.g., YOLO) can identify component presence but cannot perform state-specific retrieval or zero-shot search; Visual Large Models (VLMs) offer semantic and zero-shot capabilities but suffer from poor spatial grounding and high computational cost, making them inefficient for direct retrieval. To bridge these gaps, this paper proposes DetVLM, a novel intelligent image search framework that synergistically fuses object detection with VLMs. The framework pioneers a search-enhancement paradigm via a two-stage pipeline: a YOLO detector first conducts efficient, high-recall component-level screening to determine component presence; then, a VLM acts as a recall-enhancement unit, performing secondary verification for components missed by the detector. This architecture directly enables two advanced capabilities: 1) State Search: Guided by task-specific prompts, the VLM refines results by verifying component existence and executing sophisticated state judgments (e.g., "sun visor lowered"), allowing retrieval based on component state. 2) Zero-shot Search: The framework leverages the VLM's inherent zero-shot capability to recognize and retrieve images containing unseen components or attributes (e.g., "driver wearing a mask") without any task-specific training. Experiments on a vehicle component dataset show DetVLM achieves a state-of-the-art overall retrieval accuracy of 94.82\%, significantly outperforming detection-only baselines. It also attains 94.95\% accuracy in zero-shot search for driver mask-wearing and over 90\% average accuracy in state search tasks.

</details>


### [74] [Context-Aware Token Pruning and Discriminative Selective Attention for Transformer Tracking](https://arxiv.org/abs/2511.19928)
*Janani Kugarajeevan,Thanikasalam Kokul,Amirthalingam Ramanan,Subha Fernando*

Main category: cs.CV

TL;DR: 本文提出了CPDATrack，一种新颖的视觉目标跟踪框架，有效抑制背景和干扰令牌的影响并提升计算效率，获得了多项基准测试的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的一体式目标跟踪器通过融合模板和搜索区的序列实现联合注意力，虽然取得了优异表现，但过多的背景信息参与可能削弱对目标的判别能力。现有的令牌裁剪方法虽能减少背景干扰，却常误删靠近目标的重要上下文信息，同时遇到干扰物时识别能力下降，因此亟需更精准高效的策略。

Method: CPDATrack在两层编码器之间引入可学习模块，估算每个搜索令牌与目标相关的概率，据此剪除不重要的背景令牌，保留目标周围上下文；采用判别性选择性注意力机制，早期层完全阻止搜索对模板的注意力，中后期则仅让概率高的目标区域令牌与模板交互，从而压制背景与干扰的影响并加速计算。

Result: CPDATrack在多个数据集上取得了最优或次优的跟踪性能，特别是在GOT-10k评测集上，平均交并比达到75.1%，显示其强大的判别与泛化能力。

Conclusion: CPDATrack采用概率引导的令牌剪枝与分阶段选择性注意力机制，有效提升了Transformer跟踪器的判别性与效率，为背景复杂或有干扰物场景下的目标跟踪提供了新思路。

Abstract: One-stream Transformer-based trackers have demonstrated remarkable performance by concatenating template and search region tokens, thereby enabling joint attention across all tokens. However, enabling an excessive proportion of background search tokens to attend to the target template tokens weakens the tracker's discriminative capability. Several token pruning methods have been proposed to mitigate background interference; however, they often remove tokens near the target, leading to the loss of essential contextual information and degraded tracking performance. Moreover, the presence of distractors within the search tokens further reduces the tracker's ability to accurately identify the target. To address these limitations, we propose CPDATrack, a novel tracking framework designed to suppress interference from background and distractor tokens while enhancing computational efficiency. First, a learnable module is integrated between two designated encoder layers to estimate the probability of each search token being associated with the target. Based on these estimates, less-informative background tokens are pruned from the search region while preserving the contextual cues surrounding the target. To further suppress background interference, a discriminative selective attention mechanism is employed that fully blocks search-to-template attention in the early layers. In the subsequent encoder layers, high-probability target tokens are selectively extracted from a localized region to attend to the template tokens, thereby reducing the influence of background and distractor tokens. The proposed CPDATrack achieves state-of-the-art performance across multiple benchmarks, particularly on GOT-10k, where it attains an average overlap of 75.1 percent.

</details>


### [75] [Image Diffusion Models Exhibit Emergent Temporal Propagation in Videos](https://arxiv.org/abs/2511.19936)
*Youngseo Kim,Dohyun Kim,Geohee Han,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: 本论文提出利用预训练图像扩散模型，通过其自注意力机制实现视频中的零样本物体跟踪（分割），并首次达到了新一代最优表现。


<details>
  <summary>Details</summary>
Motivation: 图像扩散模型虽然用于生成，但其内部蕴含丰富语义结构，具备对图像区域认知与定位的能力。如何有效挖掘和利用这些结构以用于实际视觉任务（如视频物体跟踪），具有很大应用和研究价值。

Method: 本文将扩散模型的自注意力图解释为语义标签传播核，获得像素级对应关系。在视频帧间扩展该机制，实现跨帧的语义传播。结合测试时优化策略（如DDIM inversion、textual inversion、自适应head加权），进一步提高了分割与跟踪的鲁棒性。最终提出DRIFT框架，结合了SAM引导的mask优化，实现了端到端的视频物体跟踪。

Result: DRIFT框架在多个主流视频目标分割基准（VOS）上实现了零样本条件下的最新最优性能，展示了扩散模型在实际视觉任务中的巨大潜力。

Conclusion: 预训练图像扩散模型不仅能用于生成，还能通过合适机制实现高效的视频物体跟踪等视觉任务。本文提出的方法为无监督、零样本分割/跟踪提供了新思路，并推动了扩散模型在理解类任务上的应用。

Abstract: Image diffusion models, though originally developed for image generation, implicitly capture rich semantic structures that enable various recognition and localization tasks beyond synthesis. In this work, we investigate their self-attention maps can be reinterpreted as semantic label propagation kernels, providing robust pixel-level correspondences between relevant image regions. Extending this mechanism across frames yields a temporal propagation kernel that enables zero-shot object tracking via segmentation in videos. We further demonstrate the effectiveness of test-time optimization strategies-DDIM inversion, textual inversion, and adaptive head weighting-in adapting diffusion features for robust and consistent label propagation. Building on these findings, we introduce DRIFT, a framework for object tracking in videos leveraging a pretrained image diffusion model with SAM-guided mask refinement, achieving state-of-the-art zero-shot performance on standard video object segmentation benchmarks.

</details>


### [76] [Low-Resolution Editing is All You Need for High-Resolution Editing](https://arxiv.org/abs/2511.19945)
*Junsung Lee,Hyunsoo Lee,Yong Jae Lee,Bohyung Han*

Main category: cs.CV

TL;DR: 本文提出了一种新的高分辨率图像编辑方法，通过对高分辨率原图的分块优化、细致的细节传递和一致性同步策略，实现高质量、高分辨率的图像内容生成。


<details>
  <summary>Details</summary>
Motivation: 当前高分辨率内容生成需求日益增长，但现有方法多数仅支持低分辨率，1K以上的高分辨率编辑依然挑战重重，难以实现细致且用户可控的图像操作。因而需要新的高分辨率图像编辑机制。

Method: 该论文提出一种测试时优化（test-time optimization）框架，具体包括：1）对高分辨率源图像进行分块优化处理；2）利用细粒度细节传递模块增强局部编辑效果；3）创新同步策略以保持各分块之间的全局一致性。

Result: 大量实验表明，该方法能产生高质量的编辑结果，相比现有技术实现了更优异的高分辨率图像编辑和内容生成效果。

Conclusion: 提出的方法有效推进了高分辨率内容创作的发展，为未来图像编辑和视觉、图形学领域带来了新的可能性。

Abstract: High-resolution content creation is rapidly emerging as a central challenge in both the vision and graphics communities. While images serve as the most fundamental modality for visual expression, content generation that aligns with the user intent requires effective, controllable high-resolution image manipulation mechanisms. However, existing approaches remain limited to low-resolution settings, typically supporting only up to 1K resolution. In this work, we introduce the task of high-resolution image editing and propose a test-time optimization framework to address it. Our method performs patch-wise optimization on high-resolution source images, followed by a fine-grained detail transfer module and a novel synchronization strategy to maintain consistency across patches. Extensive experiments show that our method produces high-quality edits, facilitating the way toward high-resolution content creation.

</details>


### [77] [Supervise Less, See More: Training-free Nuclear Instance Segmentation with Prototype-Guided Prompting](https://arxiv.org/abs/2511.19953)
*Wen Zhang,Qin Ren,Wenjing Liu,Haibin Ling,Chenyu You*

Main category: cs.CV

TL;DR: 本文提出了一种完全不需要训练和注释的“零监督”核实例分割新方法SPROUT，并在多个人类组织病理数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的核实例分割方法大多依赖高昂的人力标注和模型微调，难以大规模推广。训练和注释需求极高，因此设计一种免训练、免注释、可泛化的方法成为重大需求。

Method: 提出SPROUT框架，利用组织学知识构建特定切片的参考原型，通过部分最优传输算法逐步引导特征对齐，将前景与背景特征转换为正负点提示，借助SAM(Segment Anything Model)实现高效分割，全流程无需模型参数更新。

Result: 在多个病理学基准测试中，SPROUT无需监督、无需再训练即可达到有竞争力的核实例分割性能。

Conclusion: SPROUT展示了无需训练和注释即可实现高质量核实例分割的潜力，为计算病理学大规模应用开辟新范式。

Abstract: Accurate nuclear instance segmentation is a pivotal task in computational pathology, supporting data-driven clinical insights and facilitating downstream translational applications. While large vision foundation models have shown promise for zero-shot biomedical segmentation, most existing approaches still depend on dense supervision and computationally expensive fine-tuning. Consequently, training-free methods present a compelling research direction, yet remain largely unexplored. In this work, we introduce SPROUT, a fully training- and annotation-free prompting framework for nuclear instance segmentation. SPROUT leverages histology-informed priors to construct slide-specific reference prototypes that mitigate domain gaps. These prototypes progressively guide feature alignment through a partial optimal transport scheme. The resulting foreground and background features are transformed into positive and negative point prompts, enabling the Segment Anything Model (SAM) to produce precise nuclear delineations without any parameter updates. Extensive experiments across multiple histopathology benchmarks demonstrate that SPROUT achieves competitive performance without supervision or retraining, establishing a novel paradigm for scalable, training-free nuclear instance segmentation in pathology.

</details>


### [78] [GFT-GCN: Privacy-Preserving 3D Face Mesh Recognition with Spectral Diffusion](https://arxiv.org/abs/2511.19958)
*Hichem Felouat,Hanrui Wang,Isao Echizen*

Main category: cs.CV

TL;DR: 本文提出了一种名为GFT-GCN的3D人脸识别框架，兼顾高识别准确率和隐私保护，能够有效抵抗重建攻击。


<details>
  <summary>Details</summary>
Motivation: 3D人脸识别因其抗光照、姿态和欺骗攻击能力强，适用于高安全性场景，但生物特征模板的隐私保护是亟需解决的问题。

Method: 该方法将图傅里叶变换（GFT）与图卷积网络（GCN）结合，用于从3D人脸网格中提取判别性的谱特征，并通过谱扩散机制生成不可逆、可更新且不可关联的模板，同时采用轻量级客户端-服务器结构，保证原始生物特征仅存储在本地设备。

Result: 在BU-3DFE和FaceScape两个公开3D人脸数据集上进行实验，结果显示该框架在识别准确率和抵御重建攻击方面表现优异。

Conclusion: GFT-GCN能够在隐私和性能之间取得有效平衡，是安全3D人脸认证的实用解决方案。

Abstract: 3D face recognition offers a robust biometric solution by capturing facial geometry, providing resilience to variations in illumination, pose changes, and presentation attacks. Its strong spoof resistance makes it suitable for high-security applications, but protecting stored biometric templates remains critical. We present GFT-GCN, a privacy-preserving 3D face recognition framework that combines spectral graph learning with diffusion-based template protection. Our approach integrates the Graph Fourier Transform (GFT) and Graph Convolutional Networks (GCN) to extract compact, discriminative spectral features from 3D face meshes. To secure these features, we introduce a spectral diffusion mechanism that produces irreversible, renewable, and unlinkable templates. A lightweight client-server architecture ensures that raw biometric data never leaves the client device. Experiments on the BU-3DFE and FaceScape datasets demonstrate high recognition accuracy and strong resistance to reconstruction attacks. Results show that GFT-GCN effectively balances privacy and performance, offering a practical solution for secure 3D face authentication.

</details>


### [79] [MambaEye: A Size-Agnostic Visual Encoder with Causal Sequential Processing](https://arxiv.org/abs/2511.19963)
*Changho Choi,Minho Kim,Jinkyu Kim*

Main category: cs.CV

TL;DR: 本文提出了一种全新的视觉编码器MambaEye，实现了真正意义上的输入尺寸无关特性，在高分辨率条件下表现优异，并保持高效的时间与内存复杂度。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉编码器已经取得很多进展，但目前仍缺乏能像人类视觉一样对输入尺寸不敏感的模型。研究者希望解决视觉模型无法适应任意尺寸输入的局限性。

Method: 作者提出了基于Mamba2骨干的单向因果状态空间模型MambaEye，引入了相对移动嵌入来编码相邻视觉补丁间的空间移位，提高了平移不变性。此外，采用了受扩散模型启发的损失函数，对模型的每一步推理进行密集监督。

Result: MambaEye在不同图像分辨率下都展现了稳健的性能，尤其是在ImageNet-1K等高分辨率任务上（如1536^2），同时保持了线性时空复杂度。

Conclusion: MambaEye不仅具备输入尺寸无关性，还在高分辨率视觉任务中实现了高效与高性能，为视觉编码器的设计开辟了新方向。

Abstract: Despite decades of progress, a truly input-size agnostic visual encoder-a fundamental characteristic of human vision-has remained elusive. We address this limitation by proposing \textbf{MambaEye}, a novel, causal sequential encoder that leverages the low complexity and causal-process based pure Mamba2 backbone. Unlike previous Mamba-based vision encoders that often employ bidirectional processing, our strictly unidirectional approach preserves the inherent causality of State Space Models, enabling the model to generate a prediction at any point in its input sequence. A core innovation is our use of relative move embedding, which encodes the spatial shift between consecutive patches, providing a strong inductive bias for translation invariance and making the model inherently adaptable to arbitrary image resolutions and scanning patterns. To achieve this, we introduce a novel diffusion-inspired loss function that provides dense, step-wise supervision, training the model to build confidence as it gathers more visual evidence. We demonstrate that MambaEye exhibits robust performance across a wide range of image resolutions, especially at higher resolutions such as $1536^2$ on the ImageNet-1K classification task. This feat is achieved while maintaining linear time and memory complexity relative to the number of patches.

</details>


### [80] [HiCoGen: Hierarchical Compositional Text-to-Image Generation in Diffusion Models via Reinforcement Learning](https://arxiv.org/abs/2511.19965)
*Hongji Yang,Yucheng Zhou,Wencheng Han,Runzhou Tao,Zhongying Qiu,Jianfei Yang,Jianbing Shen*

Main category: cs.CV

TL;DR: 提出一种新颖的分层组合生成框架HiCoGen，通过分解复杂文本提示并分步合成，显著提升扩散模型在复杂场景下的生成表现。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽然能生成高质量图像，但在多对象和层级结构等复杂提示下表现不佳，容易出现遗漏、混淆和组合性差的问题。作者旨在解决这些在复杂合成任务中的瓶颈。

Method: 1. 利用大型语言模型（LLM）将复杂提示分解为最小语义单元；2. 采用链式合成（CoS）范式，分步生成图像，每步图片作为下一步上下文，实现分层组合；3. 引入强化学习（RL）进一步优化生成过程，并提出“递减随机性调度”以在早期阶段增强采样多样性，同时设计分层奖励机制评估全局、主体及关系层的图像表现。4. 构建了新数据集HiCoPrompt进行层级化文本-图像生成评测。

Result: 实验显示，HiCoGen在概念包含度和组合准确率上明显优于现有方法，能够更高效地还原复杂提示中的各层级语义与对象关系。

Conclusion: HiCoGen框架及其创新方法能有效推动扩散模型在复杂构图场景下的表现，改善了多层次信息还原和文本执行力问题，为文本-图像生成领域提供了新方向和评测基准。

Abstract: Recent advances in diffusion models have demonstrated impressive capability in generating high-quality images for simple prompts. However, when confronted with complex prompts involving multiple objects and hierarchical structures, existing models struggle to accurately follow instructions, leading to issues such as concept omission, confusion, and poor compositionality. To address these limitations, we propose a Hierarchical Compositional Generative framework (HiCoGen) built upon a novel Chain of Synthesis (CoS) paradigm. Instead of monolithic generation, HiCoGen first leverages a Large Language Model (LLM) to decompose complex prompts into minimal semantic units. It then synthesizes these units iteratively, where the image generated in each step provides crucial visual context for the next, ensuring all textual concepts are faithfully constructed into the final scene. To further optimize this process, we introduce a reinforcement learning (RL) framework. Crucially, we identify that the limited exploration of standard diffusion samplers hinders effective RL. We theoretically prove that sample diversity is maximized by concentrating stochasticity in the early generation stages and, based on this insight, propose a novel Decaying Stochasticity Schedule to enhance exploration. Our RL algorithm is then guided by a hierarchical reward mechanism that jointly evaluates the image at the global, subject, and relationship levels. We also construct HiCoPrompt, a new text-to-image benchmark with hierarchical prompts for rigorous evaluation. Experiments show our approach significantly outperforms existing methods in both concept coverage and compositional accuracy.

</details>


### [81] [VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction](https://arxiv.org/abs/2511.19971)
*Yu Hu,Chong Cheng,Sicheng Yu,Xiaoyang Guo,Hao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种无训练的4D场景重建框架VGGT4D，通过增强和利用3D基础模型VGGT的动态线索，实现对动态场景中物体与静态背景的有效分离和精准重建。


<details>
  <summary>Details</summary>
Motivation: 当前4D场景重建需要依赖外部先验、繁琐的后处理或在4D数据集上精细调优，特别是对于动态物体主导的场景，3D基础模型表现下降。该研究希望在不训练的前提下，提升动态场景（包括动态物体和静态背景）重建的鲁棒性和准确性。

Method: 作者提出VGGT4D框架。首先，挖掘并放大VGGT全球注意力层中隐含的动态线索，通过gram相似性和时序窗口聚合获得分离静动态元素的掩码，然后利用投影梯度精细化掩码边界，最后将高质量掩码融入VGGT的前期推理流程，显著降低运动对姿态估计和几何重建的影响。

Result: 在6个数据集上，VGGT4D在动态目标分割、相机姿态估计和稠密重建等任务上均取得了优异成绩，同时支持500帧以上长序列的单次推理。

Conclusion: VGGT4D无需训练或复杂后处理即可实现高精度、鲁棒的动态4D场景重建，在多项任务和大规模场景中优于现有方法。

Abstract: Reconstructing dynamic 4D scenes is challenging, as it requires robust disentanglement of dynamic objects from the static background. While 3D foundation models like VGGT provide accurate 3D geometry, their performance drops markedly when moving objects dominate. Existing 4D approaches often rely on external priors, heavy post-optimization, or require fine-tuning on 4D datasets. In this paper, we propose VGGT4D, a training-free framework that extends the 3D foundation model VGGT for robust 4D scene reconstruction. Our approach is motivated by the key finding that VGGT's global attention layers already implicitly encode rich, layer-wise dynamic cues. To obtain masks that decouple static and dynamic elements, we mine and amplify global dynamic cues via gram similarity and aggregate them across a temporal window. To further sharpen mask boundaries, we introduce a refinement strategy driven by projection gradient. We then integrate these precise masks into VGGT's early-stage inference, effectively mitigating motion interference in both pose estimation and geometric reconstruction. Across six datasets, our method achieves superior performance in dynamic object segmentation, camera pose estimation, and dense reconstruction. It also supports single-pass inference on sequences longer than 500 frames.

</details>


### [82] [Boosting Reasoning in Large Multimodal Models via Activation Replay](https://arxiv.org/abs/2511.19972)
*Yun Xing,Xiaobin Hu,Qingdong He,Jiangning Zhang,Shuicheng Yan,Shijian Lu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文研究了奖励可验证的强化学习（RLVR）对大型多模态模型（LMMs）推理能力的影响，发现RLVR主要影响低熵激活，并提出了一种无需重新训练、通过激活回放（Activation Replay）提升LMM推理表现的新方法。


<details>
  <summary>Details</summary>
Motivation: RLVR作为提升多模态大模型推理能力的重要后训练方法，但其内部机制尚不明确。理解RLVR的激活变化规律，有助于开发更高效、低成本的方法，提升模型的推理能力。

Method: 作者利用logit lens分析RLVR对模型不同输入激活（按熵分类）的作用机制，并通过受控实验关联模型推理与激活变化。随后提出Activation Replay方法：在测试阶段对视觉token进行操作，将基础LMM低熵激活回放到RLVR模型以调控推理。

Result: Activation Replay方法在数学、视觉代理和视频推理等场景下都显著提升了模型推理表现（如提高Pass@K、改善推理覆盖率），优于高熵激活回放和直接跨模型干预等对照做法。

Conclusion: 调节RLVR中低熵激活对推理有积极影响。Activation Replay是一种简单高效的训练外方法，能有效提升多模态模型推理能力，并在多个任务中表现优越。

Abstract: Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach to incentivizing reasoning capability in Large Multimodal Models (LMMs), while the underlying mechanisms behind this post-training paradigm are poorly understood. We begin by exploring how input activations are affected by RLVR through the perspective of logit lens. Our systematic investigations across multiple post-trained LMMs suggest that RLVR shifts low-entropy activations unexpectedly, while high-entropy ones are less affected. We further demonstrate that such phenomena are associated with LMM reasoning by controlled experiments, suggesting a potentially beneficial role of modulating low-entropy activations. To this end, we propose Activation Replay, a novel simple yet effective training-free approach that boosts multimodal reasoning of post-trained LMMs without requiring expensive policy optimization. Our design involves manipulation of visual tokens at test time, replaying low-entropy activations from the input context of base LMMs to regulating the RLVR counterparts. Activation Replay triggers better reasoning across diverse scenarios, including mathematics, o3-like visual agents, and video reasoning. We further show that Activation Replay boosts Pass@K and mitigates narrower reasoning coverage of RLVR. Our design is compared against alternative choices, such as replaying high-entropy activations instead of low-entropy ones, or direct cross-model intervention instead of manipulating input tokens, demonstrating the superiority of our implementation. Codes will be made publicly available.

</details>


### [83] [EmoFeedback2: Reinforcement of Continuous Emotional Image Generation via LVLM-based Reward and Textual Feedback](https://arxiv.org/abs/2511.19982)
*Jingyang Jia,Kai Shu,Gang Yang,Long Xing,Xun Chen,Aiping Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种用于连续情绪图像生成的新范式EmoFeedback2，通过引入视觉-语言大模型的反馈机制，提升了生成图像的情绪连续性和情感真实度。


<details>
  <summary>Details</summary>
Motivation: 现有的连续情绪图像生成方法无法利用生成图像的情感反馈来调控连续情绪，且在情感与文本提示的对应方面存在适应性不足，导致生成图像的情感表达不够真实和细腻。

Method: 作者提出了“生成-理解-反馈”强化学习新范式EmoFeedback2。具体方法包括：1）引入情感感知奖励反馈策略，利用经过微调的视觉-语言大模型(LVLM)自动评价生成图像的情感值，根据与目标情感的距离生成奖励信号，指导生成模型强化学习；2）设计自我提升文本反馈机制，LVLM分析生成图像的情感内容，及时生成更优的情感文本提示，从而细化后续生成。

Result: 大量实验表明，该方法在自建数据集上生成的图像不仅质量较高，且情感表达更连续、更真实，性能指标优于当前主流方法。

Conclusion: EmoFeedback2有效提升了情绪控制能力和图像情感表达的真实性，对连续情绪图像生成领域具有实际价值。代码和数据集将公开。

Abstract: Continuous emotional image generation (C-EICG) is emerging rapidly due to its ability to produce images aligned with both user descriptions and continuous emotional values. However, existing approaches lack emotional feedback from generated images, limiting the control of emotional continuity. Additionally, their simple alignment between emotions and naively generated texts fails to adaptively adjust emotional prompts according to image content, leading to insufficient emotional fidelity. To address these concerns, we propose a novel generation-understanding-feedback reinforcement paradigm (EmoFeedback2) for C-EICG, which exploits the reasoning capability of the fine-tuned large vision-language model (LVLM) to provide reward and textual feedback for generating high-quality images with continuous emotions. Specifically, we introduce an emotion-aware reward feedback strategy, where the LVLM evaluates the emotional values of generated images and computes the reward against target emotions, guiding the reinforcement fine-tuning of the generative model and enhancing the emotional continuity of images. Furthermore, we design a self-promotion textual feedback framework, in which the LVLM iteratively analyzes the emotional content of generated images and adaptively produces refinement suggestions for the next-round prompt, improving the emotional fidelity with fine-grained content. Extensive experimental results demonstrate that our approach effectively generates high-quality images with the desired emotions, outperforming existing state-of-the-art methods in our custom dataset. The code and dataset will be released soon.

</details>


### [84] [SONIC: Spectral Optimization of Noise for Inpainting with Consistency](https://arxiv.org/abs/2511.19985)
*Seungyeon Baek,Erqun Dong,Shadan Namazifard,Mark J. Matthews,Kwang Moo Yi*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练即可在文本到图像生成模型中进行修复的新方法，通过优化初始随机噪声，实现更优质的修复效果，并超越了现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前通常需要专门的修复模型，否则基于通用文本到图像模型的训练自由（training-free）方法在修复任务上表现较差，限制了其实际应用。因此，作者探索如何提升无需训练方法的修复效果。

Method: 作者提出通过优化初始种子噪声，使其与未被遮挡的部分数据更匹配，仅需数十步优化，然后用优化后的噪声配合传统方法进行修复。为高效实现此目标，作者提出：（1）采用线性近似，避免高昂的反向传播运算；（2）在谱域中优化噪声以提高稳定性。

Result: 作者在多种修复任务上验证了所提方法的有效性，取得了优于现有方法的性能。

Conclusion: 无需专门训练，通过优化初始噪声即可极大提升通用文本到图像模型的修复能力，有望推动训练自由修复方法的发展。

Abstract: We propose a novel training-free method for inpainting with off-the-shelf text-to-image models. While guidance-based methods in theory allow generic models to be used for inverse problems such as inpainting, in practice, their effectiveness is limited, leading to the necessity of specialized inpainting-specific models. In this work, we argue that the missing ingredient for training-free inpainting is the optimization (guidance) of the initial seed noise. We propose to optimize the initial seed noise to approximately match the unmasked parts of the data - with as few as a few tens of optimization steps. We then apply conventional training-free inpainting methods on top of our optimized initial seed noise. Critically, we propose two core ideas to effectively implement this idea: (i) to avoid the costly unrolling required to relate the initial noise and the generated outcome, we perform linear approximation; and (ii) to stabilize the optimization, we optimize the initial seed noise in the spectral domain. We demonstrate the effectiveness of our method on various inpainting tasks, outperforming the state of the art. Project page: https://ubc-vision.github.io/sonic/

</details>


### [85] [GazeProphetV2: Head-Movement-Based Gaze Prediction Enabling Efficient Foveated Rendering on Mobile VR](https://arxiv.org/abs/2511.19988)
*Farhaan Ebadulla,Chiraag Mudlpaur,Shreya Chaurasia,Gaurav BV*

Main category: cs.CV

TL;DR: 本论文提出了一种结合多模态数据（注视历史、头部运动和场景内容）的VR注视点预测方法，实现了显著提高的预测准确性，并展示了在多个VR场景中的良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在虚拟现实环境中，准确预测用户的注视行为对于渲染优化和交互设计至关重要。然而，传统方法的预测精度有限，且通常依赖昂贵的眼动追踪设备，因此需要更高效、鲁棒的注视预测技术。

Method: 作者设计了一种多模态融合模型，将用户历史注视轨迹、头部动作数据及视觉场景信息输入，通过门控融合机制和跨模态注意力，实现不同信息流的自适应权重调整。

Result: 在包含22个VR场景、530万条注视样本的数据集上实验，融合多模态数据的模型预测准确率明显高于单一模态；未来1-3帧预测效果尤为突出，跨场景验证准确率达93.1%，且注视轨迹时间一致性良好。

Conclusion: 多模态融合方法能够提升VR环境中用户注视点预测的准确性，为理解虚拟环境中的注意机制提供新见解，并推动无需昂贵硬件的高效VR系统应用于渲染优化和界面设计等领域。

Abstract: Predicting gaze behavior in virtual reality environments remains a significant challenge with implications for rendering optimization and interface design. This paper introduces a multimodal approach to VR gaze prediction that combines temporal gaze patterns, head movement data, and visual scene information. By leveraging a gated fusion mechanism with cross-modal attention, the approach learns to adaptively weight gaze history, head movement, and scene content based on contextual relevance. Evaluations using a dataset spanning 22 VR scenes with 5.3M gaze samples demonstrate improvements in predictive accuracy when combining modalities compared to using individual data streams alone. The results indicate that integrating past gaze trajectories with head orientation and scene content enhances prediction accuracy across 1-3 future frames. Cross-scene generalization testing shows consistent performance with 93.1% validation accuracy and temporal consistency in predicted gaze trajectories. These findings contribute to understanding attention mechanisms in virtual environments while suggesting potential applications in rendering optimization, interaction design, and user experience evaluation. The approach represents a step toward more efficient virtual reality systems that can anticipate user attention patterns without requiring expensive eye tracking hardware.

</details>


### [86] [OmniRefiner: Reinforcement-Guided Local Diffusion Refinement](https://arxiv.org/abs/2511.19990)
*Yaoli Liu,Ziheng Ouyang,Shengtao Lou,Yiren Song*

Main category: cs.CV

TL;DR: 本文提出了一种细节感知的参考引导图像生成细化方法，能有效提升生成图像的细节和与参考图像的一致性。通过两阶段的优化，显著优于现有开源和商业方法。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在参考引导生成图像时，细节易丢失，尤其在使用VAE等潜在压缩时，导致图像特征信息的缺失。同时，目前的局部细化方法在光照、纹理或形状上容易与原图不一致。需要一种能够有效保持细节并与参考图对齐的生成方法。

Method: 提出了一个细节感知的两阶段参考驱动细化框架。第一阶段，通过调整单图扩散编辑器，使其能同时处理草图和参考图，保证全局一致性。第二阶段采用强化学习，优化局部编辑能力，提升细节准确性和语义一致性。

Result: 实验结果表明，所提方法在参考一致性和细粒度细节保留上有明显提升，在多个难度较高的参考引导修复基准数据集上超过开源及商用模型。

Conclusion: 本文方法通过细致地全局和局部编辑，成功解决了现有模型细节缺失及一致性弱的问题，为高保真参考引导图像生成提供了有效方案。

Abstract: Reference-guided image generation has progressed rapidly, yet current diffusion models still struggle to preserve fine-grained visual details when refining a generated image using a reference. This limitation arises because VAE-based latent compression inherently discards subtle texture information, causing identity- and attribute-specific cues to vanish. Moreover, post-editing approaches that amplify local details based on existing methods often produce results inconsistent with the original image in terms of lighting, texture, or shape. To address this, we introduce \ourMthd{}, a detail-aware refinement framework that performs two consecutive stages of reference-driven correction to enhance pixel-level consistency. We first adapt a single-image diffusion editor by fine-tuning it to jointly ingest the draft image and the reference image, enabling globally coherent refinement while maintaining structural fidelity. We then apply reinforcement learning to further strengthen localized editing capability, explicitly optimizing for detail accuracy and semantic consistency. Extensive experiments demonstrate that \ourMthd{} significantly improves reference alignment and fine-grained detail preservation, producing faithful and visually coherent edits that surpass both open-source and commercial models on challenging reference-guided restoration benchmarks.

</details>


### [87] [CREward: A Type-Specific Creativity Reward Model](https://arxiv.org/abs/2511.19995)
*Jiyeon Han,Ali Mahdavi-Amiri,Hao Zhang,Haedong Jeong*

Main category: cs.CV

TL;DR: 本文提出了首个类型特定的创造性奖励模型CREward，针对几何、材质和纹理三个维度对图片的创造性进行评价和指导。该模型基于大规模视觉-语言模型与人类判断的高度一致性开发，用于创造性图片的评测、解释和生成。


<details>
  <summary>Details</summary>
Motivation: 以往对创造性的研究常将其视为单一维度，难以全面描述实际的复杂性，因此有必要建立更细致的创造性评估模型，并提升其在创意生成和评判上的功能。

Method: 作者首先设计了涉及几何、材质和纹理三个维度的人类评测实验，收集人类在各类创意图片上的创造性评价数据。随后，分析了大规模视觉-语言模型（LVLMs）与人类判断的一致性。基于两者的一致性，利用LVLM生成的标签，训练了CREward模型，实现分类型的创造性评估与奖励。

Result: CREward模型能够有效对图片的创造性进行多维度评估和解释，并可用于发现具有创意性的样本和为创意生成模型提供指导，表现出良好的泛化和实际应用潜力。

Conclusion: 多维度的创造性奖励模型比传统单一维度评价更能捕捉图片创造性的复杂性，并可促进创意生成、解释和实用设计中的创新。

Abstract: Creativity is a complex phenomenon. When it comes to representing and assessing creativity, treating it as a single undifferentiated quantity would appear naive and underwhelming. In this work, we learn the \emph{first type-specific creativity reward model}, coined CREward, which spans three creativity ``axes," geometry, material, and texture, to allow us to view creativity through the lens of the image formation pipeline. To build our reward model, we first conduct a human benchmark evaluation to capture human perception of creativity for each type across various creative images. We then analyze the correlation between human judgments and predictions by large vision-language models (LVLMs), confirming that LVLMs exhibit strong alignment with human perception. Building on this observation, we collect LVLM-generated labels to train our CREward model that is applicable to both evaluation and generation of creative images. We explore three applications of CREward: creativity assessment, explainable creativity, and creative sample acquisition for both human design inspiration and guiding creative generation through low-rank adaptation.

</details>


### [88] [On the Feasibility of Hijacking MLLMs' Decision Chain via One Perturbation](https://arxiv.org/abs/2511.20002)
*Changyue Li,Jiaying Li,Youliang Yuan,Jiaming He,Zhicong Huang,Pinjia He*

Main category: cs.CV

TL;DR: 传统对抗攻击主要针对神经网络单一决策，但现实场景中模型往往需要连续决策。本文提出一种新威胁：单次扰动即可劫持整个决策链。作者提出了语义感知型通用扰动（SAUPs），实现多目标误导，并设计了有效的优化算法。实验证明，在实景数据集上，对三种多模态大模型五个目标同时发动攻击，成功率高达70%。


<details>
  <summary>Details</summary>
Motivation: 现实中的AI系统不是只做一次判断，而是多决策连锁进行。以往对抗攻击聚焦于让模型在某一判断上出错，容易被后续流程修正，但连续出错风险巨大。作者希望发现、揭秘和实证更严重的攻击——能一举影响整条决策链的对抗样本。

Method: 作者提出了Semantic-Aware Universal Perturbations（语义感知型通用扰动，SAUPs），能够根据不同输入的语义，诱导模型输出多种目标设定的错误结果。为此，发明了一种在归一化空间结合语义分离策略的优化算法。同时，搭建了带有详细语义标注的新现实世界图像数据集（RIST），便于实际效果评估。

Result: 在三种主流多模态大语言模型上进行实验，利用对抗帧针对五个不同目标同时发动攻击，取得高达70%的攻击成功率。

Conclusion: 现实世界AI系统面临的对抗攻击风险远超想象。单一扰动可控制多重决策，多模态大模型对此高度脆弱。需警惕和防范基于语义通用扰动的新型攻击。

Abstract: Conventional adversarial attacks focus on manipulating a single decision of neural networks. However, real-world models often operate in a sequence of decisions, where an isolated mistake can be easily corrected, but cascading errors can lead to severe risks.
  This paper reveals a novel threat: a single perturbation can hijack the whole decision chain. We demonstrate the feasibility of manipulating a model's outputs toward multiple, predefined outcomes, such as simultaneously misclassifying "non-motorized lane" signs as "motorized lane" and "pedestrian" as "plastic bag".
  To expose this threat, we introduce Semantic-Aware Universal Perturbations (SAUPs), which induce varied outcomes based on the semantics of the inputs. We overcome optimization challenges by developing an effective algorithm, which searches for perturbations in normalized space with a semantic separation strategy. To evaluate the practical threat of SAUPs, we present RIST, a new real-world image dataset with fine-grained semantic annotations. Extensive experiments on three multimodal large language models demonstrate their vulnerability, achieving a 70% attack success rate when controlling five distinct targets using just an adversarial frame.

</details>


### [89] [Pedestrian Crossing Intention Prediction Using Multimodal Fusion Network](https://arxiv.org/abs/2511.20008)
*Yuanzhe Li,Steffen Müller*

Main category: cs.CV

TL;DR: 本论文提出了一种多模态融合网络，利用视觉与运动的7种特征，实现对行人过街意图的高效预测，并在JAAD数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车在城市环境部署时亟需准确预测行人过街意图，以降低碰撞风险。然而，行人行为复杂且受多种情境因素影响，预测任务面临巨大挑战。

Method: 提出多模态融合网络，分别通过多个基于Transformer的模块从原始输入中提取视觉与运动特征。采用深度引导注意力模块利用深度信息指导对另一模态显著区域的关注，并引入模态注意力与时序注意力机制以融合多模态信息和捕获时序依赖。

Result: 在JAAD数据集上进行了大量实验，所提方法在预测准确率等指标上优于现有基线方法，表现出更强的预测能力。

Conclusion: 提出的多模态融合网络能够更好地挖掘和整合多种模态信息，有效提升了行人过街意图预测的精度，为自动驾驶安全提供了保障。

Abstract: Pedestrian crossing intention prediction is essential for the deployment of autonomous vehicles (AVs) in urban environments. Ideal prediction provides AVs with critical environmental cues, thereby reducing the risk of pedestrian-related collisions. However, the prediction task is challenging due to the diverse nature of pedestrian behavior and its dependence on multiple contextual factors. This paper proposes a multimodal fusion network that leverages seven modality features from both visual and motion branches, aiming to effectively extract and integrate complementary cues across different modalities. Specifically, motion and visual features are extracted from the raw inputs using multiple Transformer-based extraction modules. Depth-guided attention module leverages depth information to guide attention towards salient regions in another modality through comprehensive spatial feature interactions. To account for the varying importance of different modalities and frames, modality attention and temporal attention are designed to selectively emphasize informative modalities and effectively capture temporal dependencies. Extensive experiments on the JAAD dataset validate the effectiveness of the proposed network, achieving superior performance compared to the baseline methods.

</details>


### [90] [Multi-Context Fusion Transformer for Pedestrian Crossing Intention Prediction in Urban Environments](https://arxiv.org/abs/2511.20011)
*Yuanzhe Li,Hang Zhong,Steffen Müller*

Main category: cs.CV

TL;DR: 提出了一种多上下文融合Transformer模型（MFT），在四个关键维度（行为、环境、定位、车辆运动）下融合行人意图预测相关特征，提升了复杂城市环境中自动驾驶安全性。该方法在多个公开数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 准确预测城市环境中行人过街意图对提升自动驾驶安全、减少交通事故至关重要，但由于影响行人行为的因素复杂，现有方法难以高效准确建模。作者旨在通过整合多种上下文信息，提高行人意图预测的准确率。

Method: 作者提出多上下文融合Transformer（MFT），利用进阶的融合策略，首先在各自上下文内进行特征交互生成上下文特征，再跨上下文进行全局融合，最后通过指导注意力机制进一步细化与整合信息，获得更丰富有效的全局表征。

Result: 在JAADbeh、JAADall和PIE等主流数据集上，MFT分别取得了73%、93%、90%的准确率，优于目前的主流方法。并通过消融实验验证了各部分网络结构和上下文信息的贡献。

Conclusion: 所提MFT模型有效提升了行人过街意图预测的性能，通过多维上下文融合方式，展示出较强的泛化能力和实际应用前景。

Abstract: Pedestrian crossing intention prediction is essential for autonomous vehicles to improve pedestrian safety and reduce traffic accidents. However, accurate pedestrian intention prediction in urban environments remains challenging due to the multitude of factors affecting pedestrian behavior. In this paper, we propose a multi-context fusion Transformer (MFT) that leverages diverse numerical contextual attributes across four key dimensions, encompassing pedestrian behavior context, environmental context, pedestrian localization context and vehicle motion context, to enable accurate pedestrian intention prediction. MFT employs a progressive fusion strategy, where mutual intra-context attention enables reciprocal interactions within each context, thereby facilitating feature sequence fusion and yielding a context token as a context-specific representation. This is followed by mutual cross-context attention, which integrates features across contexts with a global CLS token serving as a compact multi-context representation. Finally, guided intra-context attention refines context tokens within each context through directed interactions, while guided cross-context attention strengthens the global CLS token to promote multi-context fusion via guided information propagation, yielding deeper and more efficient integration. Experimental results validate the superiority of MFT over state-of-the-art methods, achieving accuracy rates of 73%, 93%, and 90% on the JAADbeh, JAADall, and PIE datasets, respectively. Extensive ablation studies are further conducted to investigate the effectiveness of the network architecture and contribution of different input context. Our code is open-source: https://github.com/ZhongHang0307/Multi-Context-Fusion-Transformer.

</details>


### [91] [ACIT: Attention-Guided Cross-Modal Interaction Transformer for Pedestrian Crossing Intention Prediction](https://arxiv.org/abs/2511.20020)
*Yuanzhe Li,Steffen Müller*

Main category: cs.CV

TL;DR: 本文提出了一种名为ACIT（Attention-guided Cross-modal Interaction Transformer）的新方法，用于预测行人过街意图，通过融合多模态视觉和运动信息，有效提升了预测准确率，优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，准确预测行人是否有过街意图对于避免碰撞至关重要。然而，不同类型数据中的互补信息难以高效提取和融合，成为制约意图预测效果的主要挑战。为此，作者提出了一种新颖的多模态交互机制来提升预测性能。

Method: ACIT方法包含六种模态信息（全局语义地图、全局光流、局部RGB图像、局部光流、车辆速度、行人框），两两配对后分别采取：视觉对之间通过双路径关注机制进行显著区域增强和深度交互，运动对之间通过跨模态注意力建模动态特征，整体上利用多模态特征融合和Transformer模块捕捉时序依赖。

Result: 在JAADbeh和JAADall数据集上，ACIT方法分别实现了70%和89%的准确率，显著优于主流方法。消融实验也验证了各模块对模型性能的贡献。

Conclusion: ACIT通过创新的跨模态交互和时序建模，有效融合了多源信息，在行人过街意图预测任务上展现了优越性能，为自动驾驶安全提供了重要的技术支持。

Abstract: Predicting pedestrian crossing intention is crucial for autonomous vehicles to prevent pedestrian-related collisions. However, effectively extracting and integrating complementary cues from different types of data remains one of the major challenges. This paper proposes an attention-guided cross-modal interaction Transformer (ACIT) for pedestrian crossing intention prediction. ACIT leverages six visual and motion modalities, which are grouped into three interaction pairs: (1) Global semantic map and global optical flow, (2) Local RGB image and local optical flow, and (3) Ego-vehicle speed and pedestrian's bounding box. Within each visual interaction pair, a dual-path attention mechanism enhances salient regions within the primary modality through intra-modal self-attention and facilitates deep interactions with the auxiliary modality (i.e., optical flow) via optical flow-guided attention. Within the motion interaction pair, cross-modal attention is employed to model the cross-modal dynamics, enabling the effective extraction of complementary motion features. Beyond pairwise interactions, a multi-modal feature fusion module further facilitates cross-modal interactions at each time step. Furthermore, a Transformer-based temporal feature aggregation module is introduced to capture sequential dependencies. Experimental results demonstrate that ACIT outperforms state-of-the-art methods, achieving accuracy rates of 70% and 89% on the JAADbeh and JAADall datasets, respectively. Extensive ablation studies are further conducted to investigate the contribution of different modules of ACIT.

</details>


### [92] [WaymoQA: A Multi-View Visual Question Answering Dataset for Safety-Critical Reasoning in Autonomous Driving](https://arxiv.org/abs/2511.20022)
*Seungjun Yu,Seonho Lee,Namho Kim,Jaeyo Shin,Junsung Park,Wonjeong Ryu,Raehyuk Jung,Hyunjung Shim*

Main category: cs.CV

TL;DR: 本文提出了一项新任务——安全关键推理（Safety-Critical Reasoning），利用多视角输入提升自动驾驶场景中对复杂高风险情境的理解与推理能力，并发布了WaymoQA数据集以促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型（MLLM）虽然能理解驾驶场景，但在面对涉及风险权衡的安全关键情境时仍然表现不足，尤其是在需多角度环境信息支撑的决策推理上。为提升模型的高阶推理能力，该工作提出了新的任务和数据集。

Method: 作者将安全关键推理细化为两个阶段：先解决直接风险，再缓解由决策可能引发的下游风险。为训练和评估，高质量地标注了 WaymoQA 数据集，包含了3.5万个涵盖多视角、高风险场景的问答对，支持图片、视频、选择题与开放问答等多种形式。

Result: 实验表明，现有MLLM在安全关键情境下的推理能力远逊于在普通场景下的表现；通过WaymoQA数据集微调后，模型的安全关键推理能力显著提升。

Conclusion: WaymoQA数据集和安全关键推理任务可有效促进自动驾驶模型高阶推理与安全决策能力的发展，为研发更安全、更智能的自动驾驶系统提供了有力支撑。

Abstract: Recent advancements in multimodal large language models (MLLMs) have shown strong understanding of driving scenes, drawing interest in their application to autonomous driving. However, high-level reasoning in safety-critical scenarios, where avoiding one traffic risk can create another, remains a major challenge. Such reasoning is often infeasible with only a single front view and requires a comprehensive view of the environment, which we achieve through multi-view inputs. We define Safety-Critical Reasoning as a new task that leverages multi-view inputs to address this challenge. Then, we distill Safety-Critical Reasoning into two stages: first resolve the immediate risk, then mitigate the decision-induced downstream risks. To support this, we introduce WaymoQA, a dataset of 35,000 human-annotated question-answer pairs covering complex, high-risk driving scenarios. The dataset includes multiple-choice and open-ended formats across both image and video modalities. Experiments reveal that existing MLLMs underperform in safety-critical scenarios compared to normal scenes, but fine-tuning with WaymoQA significantly improves their reasoning ability, highlighting the effectiveness of our dataset in developing safer and more reasoning-capable driving agents.

</details>


### [93] [SAM-MI: A Mask-Injected Framework for Enhancing Open-Vocabulary Semantic Segmentation with SAM](https://arxiv.org/abs/2511.20027)
*Lin Chen,Yingjian Zhu,Qi Yang,Xin Niu,Kun Ding,Shiming Xiang*

Main category: cs.CV

TL;DR: 本文提出了一种新的Open-vocabulary语义分割框架SAM-MI，通过创新性技术有效集成了Segment Anything Model（SAM）与现有分割模型，显著提升了分割精度与速度。


<details>
  <summary>Details</summary>
Motivation: SAM虽在语义分割领域表现出色，但过度分割与掩膜与标签结合不灵活的问题，限制了其在Open-vocabulary任务中的应用。本研究旨在解决这些关键挑战，提高OVSS模型泛化能力与效率。

Method: 提出SAM-MI（Mask-injected Framework），包括：1）用Text-guided Sparse Point Prompter对SAM进行稀疏提示，提升掩膜生成速度；2）Shallow Mask Aggregation（SMAgg）聚合部分掩膜，缓解过度分割；3）Decoupled Mask Injection（DMI）将SAM掩膜分别用于低频及高频引导，避免与标签直接硬结合。

Result: 在多个主流基准上，SAM-MI方法优于Grounded-SAM，在MESS基准集上mIoU提升16.7%，掩膜生成速度提高1.6倍。

Conclusion: SAM-MI为结合SAM与开放词汇分割任务提供了新的解决方案，显著提高了性能与效率，有望为后续研究或实际应用提供借鉴。

Abstract: Open-vocabulary semantic segmentation (OVSS) aims to segment and recognize objects universally. Trained on extensive high-quality segmentation data, the segment anything model (SAM) has demonstrated remarkable universal segmentation capabilities, offering valuable support for OVSS. Although previous methods have made progress in leveraging SAM for OVSS, there are still some challenges: (1) SAM's tendency to over-segment and (2) hard combinations between fixed masks and labels. This paper introduces a novel mask-injected framework, SAM-MI, which effectively integrates SAM with OVSS models to address these challenges. Initially, SAM-MI employs a Text-guided Sparse Point Prompter to sample sparse prompts for SAM instead of previous dense grid-like prompts, thus significantly accelerating the mask generation process. The framework then introduces Shallow Mask Aggregation (SMAgg) to merge partial masks to mitigate the SAM's over-segmentation issue. Finally, Decoupled Mask Injection (DMI) incorporates SAM-generated masks for guidance at low-frequency and high-frequency separately, rather than directly combining them with labels. Extensive experiments on multiple benchmarks validate the superiority of SAM-MI. Notably, the proposed method achieves a 16.7% relative improvement in mIoU over Grounded-SAM on the MESS benchmark, along with a 1.6$\times$ speedup. We hope SAM-MI can serve as an alternative methodology to effectively equip the OVSS model with SAM.

</details>


### [94] [Tell Model Where to Look: Mitigating Hallucinations in MLLMs by Vision-Guided Attention](https://arxiv.org/abs/2511.20032)
*Jianfei Zhao,Feng Zhang,Xin Sun,Chong Feng,Zhixing Tan*

Main category: cs.CV

TL;DR: 本文提出了一种名为Vision-Guided Attention (VGA)的方法，通过利用视觉token的语义内容实现更精确的视觉引导，从而显著减少多模态大语言模型（MLLMs）在视觉理解过程中的幻觉现象，并在多个基准上取得领先的去幻觉效果。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在解释视觉信息时依赖于视觉注意力机制，但其定位能力有限，导致模型容易产生视觉幻觉。作者发现虽然MLLMs能较好地提取视觉token的语义信息，但未能充分利用这些语义进行推理。为了解决这一问题，作者希望提升视觉注意力的定位精度，减少模型幻觉。

Method: 提出了VGA（Vision-Guided Attention）方法，无需训练，首先充分挖掘视觉token的语义内容构建精确的视觉锚定（grounding），然后根据该锚定引导模型关注相关视觉区域。在图像描述任务中，VGA还能动态抑制已描述区域，进一步优化生成过程。该方法兼容高效注意力机制（如FlashAttention），仅带来4.36%的延迟开销。

Result: 通过广泛的实验，VGA在多种主流MLLMs和多项去幻觉评测基准上都取得了最先进的去幻觉效果。分析表明引入显式视觉引导后，模型的视觉理解能力有了明显提升。

Conclusion: VGA方法通过显式视觉引导，有效解决了MLLMs视觉注意力局限和幻觉问题，增强了模型视觉理解能力，且高效实用；结果表明VGA可作为提升MLLMs可靠性的关键手段。

Abstract: Visual attention serves as the primary mechanism through which MLLMs interpret visual information; however, its limited localization capability often leads to hallucinations. We observe that although MLLMs can accurately extract visual semantics from visual tokens, they fail to fully leverage this advantage during subsequent inference. To address this limitation, we propose Vision-Guided Attention (VGA), a training-free method that first constructs precise visual grounding by exploiting the semantic content of visual tokens, and then uses this grounding to guide the model's focus toward relevant visual regions. In image captioning, VGA further refines this guidance dynamically during generation by suppressing regions that have already been described. In VGA, each token undergoes only a single forward pass, introducing a negligible latency overhead of just 4.36\%. In addition, VGA is fully compatible with efficient attention implementations such as FlashAttention. Extensive experiments across diverse MLLMs and multiple hallucination benchmarks demonstrate that VGA achieves state-of-the-art dehallucination performance. Further analysis confirms that explicit visual guidance plays a crucial role in enhancing the visual understanding capabilities of MLLMs.

</details>


### [95] [Clair Obscur: an Illumination-Aware Method for Real-World Image Vectorization](https://arxiv.org/abs/2511.20034)
*Xingyue Lin,Shuai Peng,Xiangyu Xie,Jianhua Zhu,Yuxuan Zhou,Liangcai Gao*

Main category: cs.CV

TL;DR: COVec提出了一种新的照明感知矢量化框架，通过在矢量域引入固有图像分解，实现了更高视觉保真度和可编辑性。


<details>
  <summary>Details</summary>
Motivation: 现有图像矢量化方法难以有效表现复杂真实世界图像，经常生成破碎形状，影响语义表达，难以满足高质量编辑需求。

Method: COVec借鉴Clair-Obscur明暗对比原理，首次在矢量域中采用固有图像分解，将图像分成反照率、阴影和光照三个层次，以统一的矢量表现。方法包括语义引导初始化和两阶段优化，并采用可微渲染精细调整层次。

Result: 在多个数据集上实验显示，COVec相较已有方法在视觉保真性和内容可编辑性上都有显著提升。

Conclusion: COVec为矢量化提供了创新的照明感知框架，显著提升了复杂图像的表示能力和后续可编辑性，为图形编辑和生成带来了新思路。

Abstract: Image vectorization aims to convert raster images into editable, scalable vector representations while preserving visual fidelity. Existing vectorization methods struggle to represent complex real-world images, often producing fragmented shapes at the cost of semantic conciseness. In this paper, we propose COVec, an illumination-aware vectorization framework inspired by the Clair-Obscur principle of light-shade contrast. COVec is the first to introduce intrinsic image decomposition in the vector domain, separating an image into albedo, shade, and light layers in a unified vector representation. A semantic-guided initialization and two-stage optimization refine these layers with differentiable rendering. Experiments on various datasets demonstrate that COVec achieves higher visual fidelity and significantly improved editability compared to existing methods.

</details>


### [96] [MFM-point: Multi-scale Flow Matching for Point Cloud Generation](https://arxiv.org/abs/2511.20041)
*Petr Molodyk,Jaemoo Choi,David W. Romero,Ming-Yu Liu,Yongxin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为MFM-Point的多尺度Flow Matching架构，大幅提升了点云直接生成方法的性能和可扩展性，在无需增加训练/推理成本的前提下，性能媲美甚至挑战了基于其它三维表示的方法。


<details>
  <summary>Details</summary>
Motivation: 当前点云生成的点基方法虽然简单高效，但在精度和生成质量上普遍弱于基于voxel、mesh等表示的方法。作者希望改进点基方法，既维持其简单和高效，又弥补性能不足，使其在复杂、高分辨率生成任务中有竞争力。

Method: 提出MFM-Point多尺度Flow Matching架构，采用由粗到细(coarse-to-fine)的多尺度生成方式。在每个尺度，利用结构化降采样和升采样策略保证几何结构的保持和不同分辨率间分布的平滑衔接，并确保点云无序性的结构一致性。此方法在训练和推理上无额外开销。

Result: 实验显示，MFM-Point无论在多类别还是高分辨率的点云生成任务上，都达到了现有点基方法中的最高水平，并在某些任务上挑战了基于表示的方法。

Conclusion: MFM-Point证明了点基方法通过合理的多尺度设计和结构处理，可以大幅提升点云生成质量，在保持效率和简单性的同时，具备与更复杂方法相抗衡的能力。

Abstract: In recent years, point cloud generation has gained significant attention in 3D generative modeling. Among existing approaches, point-based methods directly generate point clouds without relying on other representations such as latent features, meshes, or voxels. These methods offer low training cost and algorithmic simplicity, but often underperform compared to representation-based approaches. In this paper, we propose MFM-Point, a multi-scale Flow Matching framework for point cloud generation that substantially improves the scalability and performance of point-based methods while preserving their simplicity and efficiency. Our multi-scale generation algorithm adopts a coarse-to-fine generation paradigm, enhancing generation quality and scalability without incurring additional training or inference overhead. A key challenge in developing such a multi-scale framework lies in preserving the geometric structure of unordered point clouds while ensuring smooth and consistent distributional transitions across resolutions. To address this, we introduce a structured downsampling and upsampling strategy that preserves geometry and maintains alignment between coarse and fine resolutions. Our experimental results demonstrate that MFM-Point achieves best-in-class performance among point-based methods and challenges the best representation-based methods. In particular, MFM-point demonstrates strong results in multi-category and high-resolution generation tasks.

</details>


### [97] [History-Augmented Contrastive Meta-Learning for Unsupervised Blind Super-Resolution of Planetary Remote Sensing Images](https://arxiv.org/abs/2511.20045)
*Huijia Zhao,Jie Lu,Yunqing Jiang,Xiao-Ping Lu,Kaichang Di*

Main category: cs.CV

TL;DR: 本文提出了一种名为HACBSR的新型无监督行星遥感图像盲超分框架，无需高质量真实图像和先验知识，效果优于现有方法，并公布了新数据集Ceres-50。


<details>
  <summary>Details</summary>
Motivation: 行星遥感受限于成像环境和硬件，图像存在未知退化，且缺乏高分辨率真实标签图像，阻碍了有监督超分方法的应用。

Method: HACBSR包含两部分：1）对比式核采样机制，结合核相似性控制，减轻高斯采样带来的分布偏差；2）历史增强对比学习，使用旧模型生成负样本，从而降低模型优化的贪心性、提升收敛性，且无需真值图像。此外，提出了Ceres-50数据集用于评测。

Result: 在多个放大倍数下，HACBSR方法在多个基准上无监督超分表现优异，媲美甚至优于最新无监督方法。

Conclusion: HACBSR有效解决了盲超分中的分布偏差和真值缺失问题，在行星遥感图像处理中具有潜力，为无监督超分提供了新思路，相关代码与数据集已开放。

Abstract: Planetary remote sensing images are affected by diverse and unknown degradations caused by imaging environments and hardware constraints. These factors limit image quality and hinder supervised blind super-resolution due to the lack of ground-truth images. This work presents History-Augmented Contrastive Blind Super-Resolution (HACBSR), an unsupervised framework for blind super-resolution that operates without ground-truth images and external kernel priors. HACBSR comprises two components: (1) a contrastive kernel sampling mechanism with kernel similarity control to mitigate distribution bias from Gaussian sampling, and (2) a history-augmented contrastive learning that uses historical models to generate negative samples to enable less greedy optimization and to induce strong convexity without ground-truth. A convergence analysis of the history-augmented contrastive learning is given in the Appendix. To support evaluation in planetary applications, we introduce Ceres-50, a dataset with diverse geological features simulated degradation patterns. Experiments show that HACBSR achieves competitive performance compared with state-of-the-art unsupervised methods across multiple upscaling factors. The code is available at https://github.com/2333repeat/HACBSR, and the dataset is available at https://github.com/2333repeat/Ceres-50.

</details>


### [98] [DeLightMono: Enhancing Self-Supervised Monocular Depth Estimation in Endoscopy by Decoupling Uneven Illumination](https://arxiv.org/abs/2511.20058)
*Mingyang Ou,Haojin Li,Yifeng Zhang,Ke Niu,Zhongxi Qiu,Heng Li,Jiang Liu*

Main category: cs.CV

TL;DR: 本文提出DeLight-Mono，一种具有照明解耦能力的自监督单目深度估计算法，专为内窥镜导航系统中不均匀照明问题设计，并在公开数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像常因照明不均而导致自监督单目深度估计性能下降，现有增强方法难以直接提高深度网络表现，而其他领域的方法又不适用于低光环境，导致数据采集负担加重。

Method: 提出DeLight-Mono框架，通过设计的照明-反射-深度（illumination-reflectance-depth）模型，将内窥镜图像解耦，并借助辅助网络进行分解。同时，提出基于解耦分量的新型自监督联合优化损失，以减弱不均匀照明对深度估计的影响。

Result: 通过在两个公开数据集上的大量对比实验和消融研究，验证了所提出方法在不均匀照明条件下的有效性。

Conclusion: DeLight-Mono有效提升了内窥镜低照度和不均匀照明条件下的单目深度估计性能，为相关导航系统提供了更可靠的技术支撑。

Abstract: Self-supervised monocular depth estimation serves as a key task in the development of endoscopic navigation systems. However, performance degradation persists due to uneven illumination inherent in endoscopic images, particularly in low-intensity regions. Existing low-light enhancement techniques fail to effectively guide the depth network. Furthermore, solutions from other fields, like autonomous driving, require well-lit images, making them unsuitable and increasing data collection burdens. To this end, we present DeLight-Mono - a novel self-supervised monocular depth estimation framework with illumination decoupling. Specifically, endoscopic images are represented by a designed illumination-reflectance-depth model, and are decomposed with auxiliary networks. Moreover, a self-supervised joint-optimizing framework with novel losses leveraging the decoupled components is proposed to mitigate the effects of uneven illumination on depth estimation. The effectiveness of the proposed methods was rigorously verified through extensive comparisons and an ablation study performed on two public datasets.

</details>


### [99] [FLaTEC: Frequency-Disentangled Latent Triplanes for Efficient Compression of LiDAR Point Clouds](https://arxiv.org/abs/2511.20065)
*Xiaoge Zhang,Zijie Wu,Mingtao Feng,Zichen Geng,Mehwish Nasim,Saeed Anwar,Ajmal Mian*

Main category: cs.CV

TL;DR: 本文提出了一种频率感知的点云压缩方法FLaTEC，实现了高压缩率和高重建质量的平衡，显著优于现有标准编解码器。


<details>
  <summary>Details</summary>
Motivation: 现有点云压缩方法难以兼顾高压缩率与高重建质量，主要因为不同频率的点云成分对结果的影响不同。低频结构与高频细节的处理和权衡是主要挑战。

Method: 提出了FLaTEC模型：1）引入频率感知机制，将低频结构与高频纹理解耦；2）采用latent triplanes的混合表示法，将体素嵌入转化为三平面表示，减少稀疏性、计算与存储成本；3）设计频率解耦技术，分别提取低频内容和采集跨尺度高频细节；4）解码时通过调制模块渐进恢复全频信号；5）引入基于频率的注意力机制弥补3D相关性损失，实现任意分辨率输出。

Result: 在SemanticKITTI和Ford等数据集上，本方法在BD-rate指标上相较于标准编解码器分别提升了78%和94%，达到了目前最佳率失真性能。

Conclusion: 所提FLaTEC方法有效地解决了点云压缩中率失真的平衡问题，通过频率解耦和高效的特征表示，在多个权威数据集上超越了现有主流方法，具有广阔应用前景。

Abstract: Point cloud compression methods jointly optimize bitrates and reconstruction distortion. However, balancing compression ratio and reconstruction quality is difficult because low-frequency and high-frequency components contribute differently at the same resolution. To address this, we propose FLaTEC, a frequency-aware compression model that enables the compression of a full scan with high compression ratios. Our approach introduces a frequency-aware mechanism that decouples low-frequency structures and high-frequency textures, while hybridizing latent triplanes as a compact proxy for point cloud. Specifically, we convert voxelized embeddings into triplane representations to reduce sparsity, computational cost, and storage requirements. We then devise a frequency-disentangling technique that extracts compact low-frequency content while collecting high-frequency details across scales. The decoupled low-frequency and high-frequency components are stored in binary format. During decoding, full-spectrum signals are progressively recovered via a modulation block. Additionally, to compensate for the loss of 3D correlation, we introduce an efficient frequency-based attention mechanism that fosters local connectivity and outputs arbitrary resolution points. Our method achieves state-of-the-art rate-distortion performance and outperforms the standard codecs by 78\% and 94\% in BD-rate on both SemanticKITTI and Ford datasets.

</details>


### [100] [PRADA: Probability-Ratio-Based Attribution and Detection of Autoregressive-Generated Images](https://arxiv.org/abs/2511.20068)
*Simon Damm,Jonas Ricker,Henning Petzka,Asja Fischer*

Main category: cs.CV

TL;DR: 本文提出了一种名为PRADA的方法，用于检测和归因自回归生成模型（如AR模型）产生的图像。该方法通过分析模型在生成特定图像时条件概率与无条件概率的比值，实现对生成图像的可靠识别和归属。实验表明，PRADA在多种自回归模型上均表现出高效的检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着自回归图像生成技术的发展，生成的图像越来越逼真，这对图像真伪检测提出了更高要求。目前缺乏专门针对AR生成图像的检测与归因方法，因此亟需研发高效的检测工具以保证内容可信度。

Method: PRADA方法核心在于计算某模型生成某图像的条件概率与无条件概率的比值。不同模型对生成自身图像在概率比值上具有独特性。通过设计阈值与模型特定的分数函数，实现了自回归生成图像的检测和归因。

Result: 在八个类别到图像（class-to-image）和四个文本到图像（text-to-image）的AR模型上，PRADA均能高效区分实拍和生成图像，并准确归因于来源模型。

Conclusion: PRADA是一种简单且可解释性强的方法，在各种自回归生成模型下均能实现高可靠的图像检测和归因，有助于维护图像内容的真实性和安全性。

Abstract: Autoregressive (AR) image generation has recently emerged as a powerful paradigm for image synthesis. Leveraging the generation principle of large language models, they allow for efficiently generating deceptively real-looking images, further increasing the need for reliable detection methods. However, to date there is a lack of work specifically targeting the detection of images generated by AR image generators. In this work, we present PRADA (Probability-Ratio-Based Attribution and Detection of Autoregressive-Generated Images), a simple and interpretable approach that can reliably detect AR-generated images and attribute them to their respective source model. The key idea is to inspect the ratio of a model's conditional and unconditional probability for the autoregressive token sequence representing a given image. Whenever an image is generated by a particular model, its probability ratio shows unique characteristics which are not present for images generated by other models or real images. We exploit these characteristics for threshold-based attribution and detection by calibrating a simple, model-specific score function. Our experimental evaluation shows that PRADA is highly effective against eight class-to-image and four text-to-image models.

</details>


### [101] [Learning Procedural-aware Video Representations through State-Grounded Hierarchy Unfolding](https://arxiv.org/abs/2511.20073)
*Jinghan Zhao,Yifei Huang,Feng Lu*

Main category: cs.CV

TL;DR: 该论文提出了Task-Step-State (TSS) 框架，通过将任务按步骤分解，并引入“state（状态）”概念，增强视频表征的可解释性与对具体视觉内容的对齐，在多个下游任务中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有视频表征方法通常仅将视频内容与高度抽象的“任务”和“步骤”文本描述对齐，难以与视觉中具体细节建立强关联，影响模型理解和推断复杂程序性任务的能力。因此，作者希望找到一种更好地连接抽象语义和视觉细节的方式。

Method: 作者引入“state（状态）”这一文本化的物体配置快照，作为更贴近视觉输入的中间语义层，并提出Task-Step-State (TSS) 框架，将一个任务分解为一系列步骤，每个步骤引起状态的转变。训练上，采用递进式预训练策略，逐步学习状态、步骤到任务的层级关联，强化了表示的语义层次与可解释性。

Result: 在COIN和CrossTask数据集上，所提方法在任务识别、步骤识别、下一步预测等多项下游任务中优于主流基线；消融实验表明“状态”监督是性能提升的关键因素；递进式预训练有助于更好地建立模型的层次结构，也优于常规联合训练。

Conclusion: 通过引入“状态”层与递进式预训练，该方法能更有效地将抽象程序逻辑与具体视觉细节结合，提高了程序性视频理解各项指标，对后续相关任务和模型设计具有重要意义。

Abstract: Learning procedural-aware video representations is a key step towards building agents that can reason about and execute complex tasks. Existing methods typically address this problem by aligning visual content with textual descriptions at the task and step levels to inject procedural semantics into video representations. However, due to their high level of abstraction, 'task' and 'step' descriptions fail to form a robust alignment with the concrete, observable details in visual data. To address this, we introduce 'states', i.e., textual snapshots of object configurations, as a visually-grounded semantic layer that anchors abstract procedures to what a model can actually see. We formalize this insight in a novel Task-Step-State (TSS) framework, where tasks are achieved via steps that drive transitions between observable states. To enforce this structure, we propose a progressive pre-training strategy that unfolds the TSS hierarchy, forcing the model to ground representations in states while associating them with steps and high-level tasks. Extensive experiments on the COIN and CrossTask datasets show that our method outperforms baseline models on multiple downstream tasks, including task recognition, step recognition, and next step prediction. Ablation studies show that introducing state supervision is a key driver of performance gains across all tasks. Additionally, our progressive pretraining strategy proves more effective than standard joint training, as it better enforces the intended hierarchical structure.

</details>


### [102] [Blind Adaptive Local Denoising for CEST Imaging](https://arxiv.org/abs/2511.20081)
*Chu Chen,Aitor Artola,Yang Liu,Se Weon Park,Raymond H. Chan,Jean-Michel Morel,Kannie W. Y. Chan*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的CEST MRI去噪方法BALD，能更有效处理复杂噪声，提高分子影像定量分析准确性。


<details>
  <summary>Details</summary>
Motivation: 现有CEST MRI成像易受硬件和成像协议影响，噪声复杂且空间分布不均，现有去噪手段难以应对，且可能影响关键信息，亟需创新型方法提升影像质量和定量分析能力。

Method: 作者提出Blind Adaptive Local Denoising (BALD)方法，利用CEST数据的自相似特性，无需先验噪声信息即可通过自适应方差稳定变换统一噪声分布；之后，针对线性变换后的数据实施两阶段去噪，并采用局部SVD分解，减小空间与光谱伪影影响。

Result: 通过多组体模和活体实验，BALD在去噪指标及分子浓度图、肿瘤识别等下游任务中均优于现有主流方法，效果显著。

Conclusion: BALD方法能够有效应对CEST MRI噪声的异方差性，无损保留分子信号，极大提升了肿瘤检测及分子成像定量的准确性，为临床应用奠定基础。

Abstract: Chemical Exchange Saturation Transfer (CEST) MRI enables molecular-level visualization of low-concentration metabolites by leveraging proton exchange dynamics. However, its clinical translation is hindered by inherent challenges: spatially varying noise arising from hardware limitations, and complex imaging protocols introduce heteroscedasticity in CEST data, perturbing the accuracy of quantitative contrast mapping such as amide proton transfer (APT) imaging. Traditional denoising methods are not designed for this complex noise and often alter the underlying information that is critical for biomedical analysis. To overcome these limitations, we propose a new Blind Adaptive Local Denoising (BALD) method. BALD exploits the self-similar nature of CEST data to derive an adaptive variance-stabilizing transform that equalizes the noise distributions across CEST pixels without prior knowledge of noise characteristics. Then, BALD performs two-stage denoising on a linear transformation of data to disentangle molecular signals from noise. A local SVD decomposition is used as a linear transform to prevent spatial and spectral denoising artifacts. We conducted extensive validation experiments on multiple phantoms and \textit{in vivo} CEST scans. In these experiments, BALD consistently outperformed state-of-the-art CEST denoisers in both denoising metrics and downstream tasks such as molecular concentration maps estimation and cancer detection.

</details>


### [103] [Explainable Visual Anomaly Detection via Concept Bottleneck Models](https://arxiv.org/abs/2511.20088)
*Arianna Stropeni,Valentina Zaccaria,Francesco Borsatti,Davide Dalle Pezze,Manuel Barusco,Gian Antonio Susto*

Main category: cs.CV

TL;DR: 本文提出了一种基于概念瓶颈模型（CBM）的视觉异常检测方法（CONVAD），不仅检测异常，还能给出基于语义概念的可解释性解释，并在数据集、模型改进及异常合成方面做出创新。


<details>
  <summary>Details</summary>
Motivation: 现有视觉异常检测方法虽然能够检测并定位异常区域，但缺乏对异常的直接、语义化的解释，这限制了其可用性与信任度。为提升异常检测解释的可理解性，作者尝试引入语义概念。

Method: 作者将概念瓶颈模型（CBM）引入视觉异常检测任务，开发了对应的概念数据集，并改进CBM架构，使其既能进行概念级、又能进行视觉级解释。此外，还提出了人工合成异常的流程以避免对真实异常样本的依赖。

Result: 提出的方法（CONVAD）在异常检测性能上与传统方法相当，但在可解释性上明显提升，能给出更加丰富的、基于概念的说明。

Conclusion: CBM扩展到视觉异常检测不仅保持了检测准确性，还显著增强了异常检测结果的可解释性和用户信任，为相关系统的实际应用带来新价值。

Abstract: In recent years, Visual Anomaly Detection (VAD) has gained significant attention due to its ability to identify anomalous images using only normal images during training. Many VAD models work without supervision but are still able to provide visual explanations by highlighting the anomalous regions within an image. However, although these visual explanations can be helpful, they lack a direct and semantically meaningful interpretation for users. To address this limitation, we propose extending Concept Bottleneck Models (CBMs) to the VAD setting. By learning meaningful concepts, the network can provide human-interpretable descriptions of anomalies, offering a novel and more insightful way to explain them. Our contributions are threefold: (i) we develop a Concept Dataset to support research on CBMs for VAD; (ii) we improve the CBM architecture to generate both concept-based and visual explanations, bridging semantic and localization interpretability; and (iii) we introduce a pipeline for synthesizing artificial anomalies, preserving the VAD paradigm of minimizing dependence on rare anomalous samples. Our approach, Concept-Aware Visual Anomaly Detection (CONVAD), achieves performance comparable to classic VAD methods while providing richer, concept-driven explanations that enhance interpretability and trust in VAD systems.

</details>


### [104] [WPT: World-to-Policy Transfer via Online World Model Distillation](https://arxiv.org/abs/2511.20095)
*Guangfeng Jiang,Yueru Luo,Jun Liu,Yi Huang,Yiyao Zhu,Zhan Qu,Dave Zhenyu Chen,Bingbing Liu,Xu Yan*

Main category: cs.CV

TL;DR: 本文提出了一种WPT（World-to-Policy Transfer）训练范式，实现了端到端世界模型指导下的在线蒸馏，有效提升推理效率并提高规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在推理时容易耦合过紧或依赖离线奖励信号，导致推理开销大及难以实现端到端优化，限制了其实时部署与性能。

Method: 作者提出在线世界模型蒸馏范式WPT，通过可训练奖励模型，将世界知识注入教师策略，并通过与世界模型预测的未来动态对齐候选轨迹。之后，通过策略蒸馏及世界奖励蒸馏，将教师的推理能力转移到轻量学生策略上，从而提升规划效果、降低推理延迟。

Result: 在开环和闭环基准实验中，WPT展示出先进性能：开环碰撞率为0.11，闭环驾驶分为79.23，均优于其它世界模型和模仿学习方法，同时学生策略推理速度提升至4.9倍。

Conclusion: WPT能够实现简单网络结构下的高效、准确、安全的实时决策，为世界模型与策略学习结合提供了新范式。

Abstract: Recent years have witnessed remarkable progress in world models, which primarily aim to capture the spatio-temporal correlations between an agent's actions and the evolving environment. However, existing approaches often suffer from tight runtime coupling or depend on offline reward signals, resulting in substantial inference overhead or hindering end-to-end optimization. To overcome these limitations, we introduce WPT, a World-to-Policy Transfer training paradigm that enables online distillation under the guidance of an end-to-end world model. Specifically, we develop a trainable reward model that infuses world knowledge into a teacher policy by aligning candidate trajectories with the future dynamics predicted by the world model. Subsequently, we propose policy distillation and world reward distillation to transfer the teacher's reasoning ability into a lightweight student policy, enhancing planning performance while preserving real-time deployability. Extensive experiments on both open-loop and closed-loop benchmarks show that our WPT achieves state-of-the-art performance with a simple policy architecture: it attains a 0.11 collision rate (open-loop) and achieves a 79.23 driving score (closed-loop) surpassing both world-model-based and imitation-learning methods in accuracy and safety. Moreover, the student sustains up to 4.9x faster inference, while retaining most of the gains.

</details>


### [105] [Exploring State-of-the-art models for Early Detection of Forest Fires](https://arxiv.org/abs/2511.20096)
*Sharjeel Ahmed,Daim Armaghan,Fatima Naweed,Umair Yousaf,Ahmad Zubair,Murtaza Taj*

Main category: cs.CV

TL;DR: 本文提出了一个新的早期森林火灾检测数据集，采用深度学习方法进行评估，尝试提升早期火灾识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有早期森林火灾检测依赖的图像数据集和模型有限，容易出现漏检。作者希望通过构建更适合早期火灾场景的数据集，改进检测效果。

Method: 作者合成了包含烟雾和初期火苗的新数据集，数据部分来自Red Dead Redemption 2等游戏仿真，部分为公开图像。随后，利用YOLOv7和不同的检测Transformer模型对数据集进行图像分类和定位实验。

Result: 构建了更全面的早期火灾识别数据集，并对主流目标检测算法在该数据集上的表现进行了对比。

Conclusion: 通过新的数据集和方法，可以更有效地进行森林火灾的早期检测，为实际场景部署提供了基础。

Abstract: There have been many recent developments in the use of Deep Learning Neural Networks for fire detection. In this paper, we explore an early warning system for detection of forest fires. Due to the lack of sizeable datasets and models tuned for this task, existing methods suffer from missed detection. In this work, we first propose a dataset for early identification of forest fires through visual analysis. Unlike existing image corpuses that contain images of wide-spread fire, our dataset consists of multiple instances of smoke plumes and fire that indicates the initiation of fire. We obtained this dataset synthetically by utilising game simulators such as Red Dead Redemption 2. We also combined our dataset with already published images to obtain a more comprehensive set. Finally, we compared image classification and localisation methods on the proposed dataset. More specifically we used YOLOv7 (You Only Look Once) and different models of detection transformer.

</details>


### [106] [Multi Head Attention Enhanced Inception v3 for Cardiomegaly Detection](https://arxiv.org/abs/2511.20101)
*Abishek Karthik,Pandiyaraju V*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度学习和注意力机制的X光图像心脏肥大（cardiomegaly）自动检测方法，取得了优异的检测效果。


<details>
  <summary>Details</summary>
Motivation: 心脏肥大是常见心血管疾病表现，传统X光诊断存在主观性和误诊风险。利用深度学习提升自动化与诊断准确性有急迫需求。

Method: 收集标注X光图像数据，经过高质量预处理。模型采用基于Inception V3结构的卷积神经网络，并引入多头注意力机制，使模型自动聚焦于关键信息区域。通过严格评价指标完整测试模型性能。

Result: 所提模型在心脏肥大检测上表现优异，准确率为95.6%，精度为95.2%，召回率96.2%，敏感性95.7%，特异性96.1%，AUC达到96.0。

Conclusion: 融合深度学习与多头注意力机制的系统能高效、精准地自动检测X光图像中的心脏肥大，具有显著的临床应用潜力。

Abstract: The healthcare industry has been revolutionized significantly by novel imaging technologies, not just in the diagnosis of cardiovascular diseases but also by the visualization of structural abnormalities like cardiomegaly. This article explains an integrated approach to the use of deep learning tools and attention mechanisms for automatic detection of cardiomegaly using X-ray images. The initiation of the project is grounded on a strong Data Collection phase and gathering the data of annotated X-ray images of various types. Then, while the Preprocessing module fine-tunes image quality, it is feasible to utilize the best out of the data quality in the proposed system. In our proposed system, the process is a CNN configuration leveraging the inception V3 model as one of the key blocks. Besides, we also employ a multilayer attention mechanism to enhance the strength. The most important feature of the method is the multi-head attention mechanism that can learn features automatically. By exact selective focusing on only some regions of input, the model can thus identify cardiomegaly in a sensitive manner. Attention rating is calculated, duplicated, and applied to enhance representation of main data, and therefore there is a successful diagnosis. The Evaluation stage will be extremely strict and it will thoroughly evaluate the model based on such measures as accuracy and precision. This will validate that the model can identify cardiomegaly and will also show the clinical significance of this method. The model has accuracy of 95.6, precision of 95.2, recall of 96.2, sensitivity of 95.7, specificity of 96.1 and an Area Under Curve(AUC) of 96.0 and their respective graphs are plotted for visualisation.

</details>


### [107] [LungEvaty: A Scalable, Open-Source Transformer-based Deep Learning Model for Lung Cancer Risk Prediction in LDCT Screening](https://arxiv.org/abs/2511.20116)
*Johannes Brandt,Maulik Chevli,Rickmer Braren,Georgios Kaissis,Philip Müller,Daniel Rueckert*

Main category: cs.CV

TL;DR: 本文提出了LungEvaty，一个全基于Transformer的肺癌风险预测框架，能够用单次低剂量CT（LDCT）扫描实现1-6年肺癌风险预测，无需像素级标注或区域监督，并在大规模数据集上达到了现有最优水平。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多国家实施基于LDCT的大规模肺癌筛查，如何高效处理和分析大规模CT数据以实现肺癌风险预测成为亟需解决的问题。当前方法要么过度依赖详细标注，影响可扩展性，要么只分析肺部局部，影响效果，因此需要更高效、全面的新方法。

Method: LungEvaty采用端到端的Transformer模型，输入为整个肺部的CT数据，直接从大规模筛查数据中学习肺部解剖和病理特征，无需像素级标注。在训练过程中可以选择加入解剖注意力引导（AIAG）来提升模型对关键解剖区域关注。

Result: LungEvaty在超过9万例CT数据上进行训练和评估，在无监督图像区域的前提下达到与当前最优方法相当的风险预测性能。

Conclusion: LungEvaty框架无需依赖繁琐标注，具有高度可扩展性和简单高效的特点，为未来纵向和多模态肺癌风险预测研究提供了开源和可拓展的基础。

Abstract: Lung cancer risk estimation is gaining increasing importance as more countries introduce population-wide screening programs using low-dose CT (LDCT). As imaging volumes grow, scalable methods that can process entire lung volumes efficiently are essential to tap into the full potential of these large screening datasets. Existing approaches either over-rely on pixel-level annotations, limiting scalability, or analyze the lung in fragments, weakening performance. We present LungEvaty, a fully transformer-based framework for predicting 1-6 year lung cancer risk from a single LDCT scan. The model operates on whole-lung inputs, learning directly from large-scale screening data to capture comprehensive anatomical and pathological cues relevant for malignancy risk. Using only imaging data and no region supervision, LungEvaty matches state-of-the-art performance, refinable by an optional Anatomically Informed Attention Guidance (AIAG) loss that encourages anatomically focused attention. In total, LungEvaty was trained on more than 90,000 CT scans, including over 28,000 for fine-tuning and 6,000 for evaluation. The framework offers a simple, data-efficient, and fully open-source solution that provides an extensible foundation for future research in longitudinal and multimodal lung cancer risk prediction.

</details>


### [108] [UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers](https://arxiv.org/abs/2511.20123)
*Min Zhao,Hongzhou Zhu,Yingze Wang,Bokai Yan,Jintao Zhang,Guande He,Ling Yang,Chongxuan Li,Jun Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为UltraViCo的新方法，无需重新训练即可显著改善视频扩散模型的推理长度泛化能力，显著减少内容重复和画质劣化问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散Transformer在推理时难以超越训练时的长度限制，表现出周期性内容重复与视频质量降低的问题。此前主要关注内容重复，而忽略了画质劣化。作者希望从更基本的注意力机制出发，系统解决这两个问题，提升模型长度扩展能力。

Method: 作者分析发现两个失败模式均由注意力分散（attention dispersion）引起——训练长度外的token稀释了学习到的注意力模式。提出UltraViCo方法，在推理时通过常数衰减因子抑制超出训练窗口的token注意力，无需额外训练，直接提升输出质量。

Result: UltraViCo方法在不同模型和长度扩展（Extrapolation）比例下均显著优于现有方法，将高质量推理长度从2倍提升到4倍。具体在Dynamic Degree和Imaging Quality 4倍扩展下，比分别提升233%和40.5%。此外，该方法还能推广到视频合成、编辑等下游任务。

Conclusion: UltraViCo作为一种简单、免训练的插件式方案，有效抑制注意力分散，统一解决视频长度扩展中的重复与质量下降问题，显著推进了现有方法的泛化极限，并具备良好的扩展和适应能力。

Abstract: Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.

</details>


### [109] [Vision-Language Models for Automated 3D PET/CT Report Generation](https://arxiv.org/abs/2511.20145)
*Wenpei Jiao,Kun Shang,Hui Li,Ke Yan,Jiajin Zhang,Guangjie Yang,Lijuan Guo,Yan Wan,Xing Yang,Dakai Jin,Zhaoheng Xie*

Main category: cs.CV

TL;DR: 本论文提出了一种专门用于PET/CT检查自动报告生成的新方法PETRG-3D，并构建了相关的数据集和评估方法，显著提升了自动报告的准确性和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 随着PET/CT扫描仪数量迅速增加，专业人才供不应求，手动生成临床报告的工作量大，因此需要能够自动生成高质量PET/CT报告的工具，减轻临床负担。同时，相比其它影像，PET具有代谢模式差异大和需要整体三维上下文信息的独特挑战。

Method: 提出PETRG-3D框架，采用三维双分支架构分别编码PET和CT数据，并引入风格自适应提示词以应对不同医院间报告风格的差异。构建多中心淋巴瘤影像报告数据集PETRG-Lym，并制作了AutoPET-RG-Lym公共基准数据集。此外，提出PETRG-Score这一淋巴瘤专用评估协议，综合考察代谢与结构双重临床指标。

Result: 实验显示，PETRG-3D方法在语言指标上（如ROUGE-L提升31.49%）、临床有效性指标上（如PET-All提升8.18%）均大幅优于现有方法，验证了三维双模态建模和风格自适应的优势。

Conclusion: 本工作搭建了PET/CT自动报告生成的新技术框架和公开资源，为后续针对具体疾病、兼顾推理能力和临床可靠性的AI报告系统研究提供了坚实基础。

Abstract: Positron emission tomography/computed tomography (PET/CT) is essential in oncology, yet the rapid expansion of scanners has outpaced the availability of trained specialists, making automated PET/CT report generation (PETRG) increasingly important for reducing clinical workload. Compared with structural imaging (e.g., X-ray, CT, and MRI), functional PET poses distinct challenges: metabolic patterns vary with tracer physiology, and whole-body 3D contextual information is required rather than local-region interpretation. To advance PETRG, we propose PETRG-3D, an end-to-end 3D dual-branch framework that separately encodes PET and CT volumes and incorporates style-adaptive prompts to mitigate inter-hospital variability in reporting practices. We construct PETRG-Lym, a multi-center lymphoma dataset collected from four hospitals (824 reports w/ 245,509 paired PET/CT slices), and construct AutoPET-RG-Lym, a publicly accessible PETRG benchmark derived from open imaging data but equipped with new expert-written, clinically validated reports (135 cases). To assess clinical utility, we introduce PETRG-Score, a lymphoma-specific evaluation protocol that jointly measures metabolic and structural findings across curated anatomical regions. Experiments show that PETRG-3D substantially outperforms existing methods on both natural language metrics (e.g., +31.49\% ROUGE-L) and clinical efficacy metrics (e.g., +8.18\% PET-All), highlighting the benefits of volumetric dual-modality modeling and style-aware prompting. Overall, this work establishes a foundation for future PET/CT-specific models emphasizing disease-aware reasoning and clinically reliable evaluation. Codes, models, and AutoPET-RG-Lym will be released.

</details>


### [110] [Hybrid Convolution and Frequency State Space Network for Image Compression](https://arxiv.org/abs/2511.20151)
*Haodong Pan,Hao Wei,Yusong Wang,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: 提出了一种结合CNN与频域状态空间模块（VFSS）的新型混合网络HCFSSNet，有效提升了深度学习图像压缩性能，并大幅减少参数量。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer和状态空间模型（SSM）的图像压缩方法擅长捕捉长距离特征，但会损失结构信息或忽视关键的频率特征。卷积网络虽能提取高频细节，但全局建模能力有限。亟需一种兼顾局部和全局、频域特性的高效压缩结构。

Method: HCFSSNet以CNN提取高频局部结构，VFSS模块建模低频长距离信息。VFSS包含VONSS（全向邻域状态空间，水平/垂直/对角扫描特征）和AFMM（自适应DCT频率加权，提升比特分配效率）。为增强熵模型，提出与Swin Transformer结合的FSTAM，实现频率感知的辅助信息建模。

Result: 在Kodak、Tecnick及CLIC Professional Validation三大数据集上，HCFSSNet在保证压缩性能的同时，参数量显著低于最新SSM编码器（如MambaIC）。BD率相对VTM基准分别下降18.06%、24.56%和22.44%。

Conclusion: HCFSSNet兼具解释性和效率，在学术及实际深度学习图像压缩领域展现出优异竞争力，有望推动混合架构在未来图像压缩系统中的应用。

Abstract: Learned image compression (LIC) has recently benefited from Transformer based and state space model (SSM) based architectures. Convolutional neural networks (CNNs) effectively capture local high frequency details, whereas Transformers and SSMs provide strong long range modeling capabilities but may cause structural information loss or ignore frequency characteristics that are crucial for compression. In this work we propose HCFSSNet, a Hybrid Convolution and Frequency State Space Network for LIC. HCFSSNet uses CNNs to extract local high frequency structures and introduces a Vision Frequency State Space (VFSS) block that models long range low frequency information. The VFSS block combines an Omni directional Neighborhood State Space (VONSS) module, which scans features horizontally, vertically and diagonally, with an Adaptive Frequency Modulation Module (AFMM) that applies content adaptive weighting of discrete cosine transform frequency components for more efficient bit allocation. To further reduce redundancy in the entropy model, we integrate AFMM with a Swin Transformer to form a Frequency Swin Transformer Attention Module (FSTAM) for frequency aware side information modeling. Experiments on the Kodak, Tecnick and CLIC Professional Validation datasets show that HCFSSNet achieves competitive rate distortion performance compared with recent SSM based codecs such as MambaIC, while using significantly fewer parameters. On Kodak, Tecnick and CLIC, HCFSSNet reduces BD rate over the VTM anchor by 18.06, 24.56 and 22.44 percent, respectively, providing an efficient and interpretable hybrid architecture for future learned image compression systems.

</details>


### [111] [Restora-Flow: Mask-Guided Image Restoration with Flow Matching](https://arxiv.org/abs/2511.20152)
*Arnela Hadzic,Franz Thaler,Lea Bogensperger,Simon Johannes Joham,Martin Urschler*

Main category: cs.CV

TL;DR: 现有基于flow matching的生成模型虽然提升了采样速度，但在图像恢复任务中仍存在处理时间较长或结果过度平滑的问题。本文提出Restora-Flow算法，通过退化掩码引导采样和轨迹校正机制，有效提升了质量和效率，在自然与医学图像的修复、超分辨率、去噪等任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 主流扩散模型生成质量高但采样速度慢, flow matching方法加快了采样, 使其能良好地作为图像恢复先验, 但目前相关方法仍有处理慢或恢复效果过度平滑的问题, 需要进一步提升效率和感知质量。

Method: 提出Restora-Flow, 一种无需训练的方法, 通过利用退化掩码引导flow匹配采样过程, 并引入轨迹校正机制, 保证采样结果与退化输入的一致性。该方法可直接用于图像修复、超分辨率、去噪等掩码退化任务。

Result: 在自然图像和医学图像的多种图像恢复任务中(如修复、超分辨率、去噪), Restora-Flow在感知质量与处理速度方面, 相比于基于扩散模型和flow matching的现有方法, 均表现出更优的效果。

Conclusion: Restora-Flow有效解决了现有flow模型在图像恢复任务中的速度和过度平滑问题, 展现出更高的恢复质量和更快的推理速度, 具有实际应用价值。

Abstract: Flow matching has emerged as a promising generative approach that addresses the lengthy sampling times associated with state-of-the-art diffusion models and enables a more flexible trajectory design, while maintaining high-quality image generation. This capability makes it suitable as a generative prior for image restoration tasks. Although current methods leveraging flow models have shown promising results in restoration, some still suffer from long processing times or produce over-smoothed results. To address these challenges, we introduce Restora-Flow, a training-free method that guides flow matching sampling by a degradation mask and incorporates a trajectory correction mechanism to enforce consistency with degraded inputs. We evaluate our approach on both natural and medical datasets across several image restoration tasks involving a mask-based degradation, i.e., inpainting, super-resolution and denoising. We show superior perceptual quality and processing time compared to diffusion and flow matching-based reference methods.

</details>


### [112] [Alzheimers Disease Progression Prediction Based on Manifold Mapping of Irregularly Sampled Longitudinal Data](https://arxiv.org/abs/2511.20154)
*Xin Hong,Ying Shi,Yinhao Li,Yen-Wei Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种结合流形学习和神经微分方程的新框架，用于解决不规则间隔纵向MRI数据中阿尔茨海默症进展建模难题，并显著提升了预测性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有成像预测模型大多基于欧氏空间，无法处理纵向医学影像采样间隔不规则带来的非线性几何特性和连续性问题，影响对疾病进展的准确建模。急需开发能捕捉复杂时序和空间结构的方法，以提高阿尔茨海默症等疾病的建模效果。

Method: 论文提出R-TNAG框架，包括：1）将高维MRI特征投影到黎曼流形空间，保留疾病进展的内在几何结构；2）在此空间上用时间感知神经常微分方程（TNODE）连续建模潜变量的演化；3）引入基于注意力机制的黎曼门控循环单元（ARGRU）自适应整合历史与当前信息，解决间隔不等长问题。

Result: 在疾病状态判别和认知评分回归任务上，该方法超越了当前主流模型。消融实验证明各模块的有效性和互补作用。方法在不同序列长度和缺失率下表现稳定，跨数据集验证显示出较强的稳健性和适应性。

Conclusion: 提出的R-TNAG方法能有效捕捉纵向医学影像中复杂的时空结构，在不规则采样情景下表现出色，为疾病进展建模提供了更强的工具，具有良好临床推广前景。

Abstract: The uncertainty of clinical examinations frequently leads to irregular observation intervals in longitudinal imaging data, posing challenges for modeling disease progression.Most existing imaging-based disease prediction models operate in Euclidean space, which assumes a flat representation of data and fails to fully capture the intrinsic continuity and nonlinear geometric structure of irregularly sampled longitudinal images. To address the challenge of modeling Alzheimers disease (AD) progression from irregularly sampled longitudinal structural Magnetic Resonance Imaging (sMRI) data, we propose a Riemannian manifold mapping, a Time-aware manifold Neural ordinary differential equation, and an Attention-based riemannian Gated recurrent unit (R-TNAG) framework. Our approach first projects features extracted from high-dimensional sMRI into a manifold space to preserve the intrinsic geometry of disease progression. On this representation, a time-aware Neural Ordinary Differential Equation (TNODE) models the continuous evolution of latent states between observations, while an Attention-based Riemannian Gated Recurrent Unit (ARGRU) adaptively integrates historical and current information to handle irregular intervals. This joint design improves temporal consistency and yields robust AD trajectory prediction under irregular sampling.Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art models in both disease status prediction and cognitive score regression. Ablation studies verify the contributions of each module, highlighting their complementary roles in enhancing predictive accuracy. Moreover, the model exhibits stable performance across varying sequence lengths and missing data rates, indicating strong temporal generalizability. Cross-dataset validation further confirms its robustness and applicability in diverse clinical settings.

</details>


### [113] [SKEL-CF: Coarse-to-Fine Biomechanical Skeleton and Surface Mesh Recovery](https://arxiv.org/abs/2511.20157)
*Da Li,Ji-Ping Jin,Xuanlong Yu,Wei Liu,Xiaodong Cun,Kai Chen,Rui Fan,Jiangang Kong,Shen Xi*

Main category: cs.CV

TL;DR: 本文提出SKEL-CF算法，通过粗到细的方式估计高拟真3D人体骨架参数，显著提升生物力学真实性和参数预测精度，并在多个数据集上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有参数化3D人体模型（如SMPL）虽推动了人体姿态和形状估计的进展，但其动力学结构简化，生物力学真实性不足。新近的SKEL模型通过解剖学精准骨架重组解决这一问题，但直接估计SKEL参数困难重重，主要由于训练数据有限、视角歧义以及人体结构复杂性。为此，需提出新的高效估计算法及高质量数据集。

Method: 作者提出SKEL-CF，一种基于transformer的粗到细框架，用于SKEL参数估计。该方法采用编码器-解码器结构，编码器预测粗略相机及SKEL参数，解码器逐层精细化参数。此外，作者将SMPL标注数据集4DHuman转化为与SKEL对齐的新数据集4DHuman-SKEL，提供高质量训练数据。为减少深度与尺度的歧义，算法中特别整合了相机建模，提升了多视角估计的鲁棒性。

Result: 在MOYO等复杂数据集上，SKEL-CF在平均关节点位置误差（MPJPE）和姿态对齐误差（PA-MPJPE）指标上均显著超过此前SOTA方法HSMR（85.0/51.4 vs 104.5/79.6），验证了其在多场景、多视角下的准确性和稳定性。

Conclusion: SKEL-CF是首个结合生物力学真实骨架和高效参数估计的新一代人体建模框架，有力推动了人体运动分析领域的发展，促进了计算机视觉与生物力学的结合。

Abstract: Parametric 3D human models such as SMPL have driven significant advances in human pose and shape estimation, yet their simplified kinematics limit biomechanical realism. The recently proposed SKEL model addresses this limitation by re-rigging SMPL with an anatomically accurate skeleton. However, estimating SKEL parameters directly remains challenging due to limited training data, perspective ambiguities, and the inherent complexity of human articulation. We introduce SKEL-CF, a coarse-to-fine framework for SKEL parameter estimation. SKEL-CF employs a transformer-based encoder-decoder architecture, where the encoder predicts coarse camera and SKEL parameters, and the decoder progressively refines them in successive layers. To ensure anatomically consistent supervision, we convert the existing SMPL-based dataset 4DHuman into a SKEL-aligned version, 4DHuman-SKEL, providing high-quality training data for SKEL estimation. In addition, to mitigate depth and scale ambiguities, we explicitly incorporate camera modeling into the SKEL-CF pipeline and demonstrate its importance across diverse viewpoints. Extensive experiments validate the effectiveness of the proposed design. On the challenging MOYO dataset, SKEL-CF achieves 85.0 MPJPE / 51.4 PA-MPJPE, significantly outperforming the previous SKEL-based state-of-the-art HSMR (104.5 / 79.6). These results establish SKEL-CF as a scalable and anatomically faithful framework for human motion analysis, bridging the gap between computer vision and biomechanics. Our implementation is available on the project page: https://pokerman8.github.io/SKEL-CF/.

</details>


### [114] [Harmonious Parameter Adaptation in Continual Visual Instruction Tuning for Safety-Aligned MLLMs](https://arxiv.org/abs/2511.20158)
*Ziqi Wang,Chang Che,Qi Wang,Hui Ma,Zenglin Shi,Cees G. M. Snoek,Meng Wang*

Main category: cs.CV

TL;DR: 本文关注于多模态大语言模型（MLLMs）在持续视觉指令微调（CVIT）过程中，如何平衡安全性与任务表现，并提出了一种新的参数调整方法。


<details>
  <summary>Details</summary>
Motivation: 虽然CVIT已用于适应MLLMs，但现有研究大多忽略了安全对齐。现实应用中，对安全的需求不容忽视，因此需要研究如何在持续适应时保证模型的安全性。

Method: 本文提出和实现了Harmonious Parameter Adaptation（HPA）框架，包括三步：1）基于关注点对参数进行分区（安全/任务），2）从平衡角度选择需保留的参数，3）对参数更新施加正交性约束，以减少遗忘。

Result: 大量实验表明，HPA在持续视觉指令微调和安全评测数据集上，相较于主流方法，能更好地保持模型安全性并减缓遗忘。

Conclusion: HPA为在CVIT过程中平衡安全与任务表现提出了有效方案，有望推广到实际MLLMs应用中，提升安全性和长期表现。

Abstract: While continual visual instruction tuning (CVIT) has shown promise in adapting multimodal large language models (MLLMs), existing studies predominantly focus on models without safety alignment. This critical oversight ignores the fact that real-world MLLMs inherently require such mechanisms to mitigate potential risks. In this work, we shift our focus to CVIT for safety-aligned MLLMs and observe that during continual adaptation, the model not only suffers from task forgetting but also exhibits degradation in its safety. Achieving a harmonious balance between safety and task performance remains a crucial challenge. To address this, we propose Harmonious Parameter Adaptation (HPA), a post-training framework composed of focusing-based parameter partition, harmoniously balanced parameter selection, and orthogonal parameter adjustment. Specifically, HPA partitions parameters into two types based on their focus on safety or task performance, and selects the focused ones to preserve from a balanced perspective. In addition, HPA imposes orthogonality constraints on parameter updates to further alleviate catastrophic forgetting. Extensive experiments on the CVIT benchmark and safety evaluation datasets demonstrate that HPA better maintains high safety and mitigates forgetting than existing baselines.

</details>


### [115] [While recognizing actions, LMMs struggle to detect core interaction events](https://arxiv.org/abs/2511.20162)
*Daniel Harari,Michael Sidorov,Liel David,Chen Shterental,Abrham Kahsay Gebreselasie,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 本文提出并标注了一个大规模手部与物体交互事件数据集，并测试了主流大模型（Qwen-2.5VL和GPT-4o）在视频中定位接触事件的能力，发现模型尽管具备对象识别和动作描述能力，但无法定位交互事件的起止帧和准确位置。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大模型（LMMs）在图像及视频的理解任务上不断进步，但它们是否能基于视觉输入进行语义理解、并准确定位交互细节，尚未有深入分析。特别是在物体交互过程中，模型是否能知道具体的接触或分离时刻，是评估其视觉语义基础能力的关键。

Method: 作者首先基于Something-Something-V2视频集，人工标注了2万多个手与物体的交互事件，主要关注‘接触（contact）’和‘释放（release）’的时间与空间节点。随后，将这些视频片段输入两个先进LMM（Qwen-2.5VL和GPT-4o），让模型定位交互的开始或结束帧，并分析其在理解和推理、时空定位等能力上的表现。

Result: 实验显示，这些模型可以准确说出目标对象和动作内容，也能给出连贯合理的推理解释。但在具体定位‘开始接触’或‘结束接触’的帧与场景内具体位置时，表现极差，几乎无法做到准确定位。

Conclusion: 当前主流LMMs尚不具备通过视觉感知实现动态场景深层次、基于物理交互事件的理解能力。这说明他们的语义能力尚未扎根于对真实视觉场景的感知，未来模型需提升其感知与理解动态变化细节的能力。

Abstract: Large multi-modal models (LMMs) show increasing performance in realistic visual tasks for images and, more recently, for videos. For example, given a video sequence, such models are able to describe in detail objects, the surroundings and dynamic actions. In this study, we explored the extent to which these models ground their semantic understanding in the actual visual input. Specifically, given sequences of hands interacting with objects, we asked models when and where the interaction begins or ends. For this purpose, we introduce a first of its kind, large-scale dataset with more than 20K annotated interactions on videos from the Something-Something-V2 dataset. 250 AMTurk human annotators labeled core interaction events, particularly when and where objects and agents become attached ('contact') or detached ('release'). We asked two LMMs (Qwen-2.5VL and GPT-4o) to locate these events in short videos, each with a single event. The results show that although the models can reliably name the target objects, identify the action and provide coherent reasoning, they consistently fail to identify the frame where the interaction begins or ends and cannot localize the event within the scene. Our findings suggest that in struggling to pinpoint the moment and location of physical contact that defines the interaction, the models lack the perceptual grounding required for deeper understanding of dynamic scenes.

</details>


### [116] [ADNet: A Large-Scale and Extensible Multi-Domain Benchmark for Anomaly Detection Across 380 Real-World Categories](https://arxiv.org/abs/2511.20169)
*Hai Ling,Jia Guo,Zhulin Tao,Yunkang Cao,Donglin Di,Hongyan Xu,Xiu Su,Yang Song,Lei Fan*

Main category: cs.CV

TL;DR: 本文提出了ADNet，这是一个跨多领域、大规模的异常检测(AD)基准，涵盖380个类别，共19万余张RGB图像，支持多模态异常检测任务，同时提出了新的方法Dinomaly-m显著提升了大规模场景下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测基准如MVTec-AD类别有限，难以评估跨场景泛化和可扩展性。因此需要更大规模、更多领域的数据集来推动AD方法的发展与公平比较。

Method: 作者构建了ADNet数据集，聚合49个开源数据集、包含五大领域与380个类别，并统一了标注和文本描述。并提出Dinomaly-m模型，通过Mixture-of-Experts机制扩展解码能力，同时保持推理成本不变，以提升多类别大规模异常检测性能。

Result: 大规模测试发现：当前主流方法(如SOTA)在单类别任务中可达90.6% I-AUROC，但扩展到380类别时降至78.5%。新方法Dinomaly-m性能提升至83.2% I-AUROC和93.1% P-AUROC，超越现有方法。

Conclusion: ADNet数据集为异常检测研究提供了更通用、可扩展的基准平台，有助于推动多领域大规模异常检测方法发展，Dinomaly-m证明了扩展解码能力有助于提升大规模AD性能。

Abstract: Anomaly detection (AD) aims to identify defects using normal-only training data. Existing anomaly detection benchmarks (e.g., MVTec-AD with 15 categories) cover only a narrow range of categories, limiting the evaluation of cross-context generalization and scalability. We introduce ADNet, a large-scale, multi-domain benchmark comprising 380 categories aggregated from 49 publicly available datasets across Electronics, Industry, Agrifood, Infrastructure, and Medical domains. The benchmark includes a total of 196,294 RGB images, consisting of 116,192 normal samples for training and 80,102 test images, of which 60,311 are anomalous. All images are standardized with MVTec-style pixel-level annotations and structured text descriptions spanning both spatial and visual attributes, enabling multimodal anomaly detection tasks. Extensive experiments reveal a clear scalability challenge: existing state-of-the-art methods achieve 90.6% I-AUROC in one-for-one settings but drop to 78.5% when scaling to all 380 categories in a multi-class setting. To address this, we propose Dinomaly-m, a context-guided Mixture-of-Experts extension of Dinomaly that expands decoder capacity without increasing inference cost. It achieves 83.2% I-AUROC and 93.1% P-AUROC, demonstrating superior performance over existing approaches. ADNet is designed as a standardized and extensible benchmark, supporting the community in expanding anomaly detection datasets across diverse domains and providing a scalable foundation for future anomaly detection foundation models. Dataset: https://grainnet.github.io/ADNet

</details>


### [117] [Realizing Fully-Integrated, Low-Power, Event-Based Pupil Tracking with Neuromorphic Hardware](https://arxiv.org/abs/2511.20175)
*Federico Paredes-Valles,Yoshitaka Miyatani,Kirk Y. W. Scheper*

Main category: cs.CV

TL;DR: 本文提出了一种电池驱动、可穿戴的眼球中心追踪系统，融合了事件视觉传感和神经形态计算，实现在极低功耗下的高频眼动追踪。


<details>
  <summary>Details</summary>
Motivation: 眼动追踪在诸多应用中至关重要，但在可穿戴设备上实现高频、低功耗的稳健追踪依旧是难题。传统事件视觉传感器虽有高时间分辨率，但缺少实时、低功耗处理方案。本文旨在解决这一技术瓶颈。

Method: 作者开发了首个在Speck2f系统芯片上全端集成的方案，融合了事件传感与神经形态处理，在低功耗微控制器上采用轻量的坐标解码。提出了可量化不确定性的脉冲神经网络及时序门控解码结构，并设计了适合实际部署的机制以缩小现实差距。

Result: 在新的多用户数据集上验证系统，并通过戴在身上的原型系统展示双眼神经形态设备，可实现100Hz频率、每只眼平均功耗低于5mW的稳健双目追踪。

Conclusion: 端到端神经形态计算方案能够实现实际可用、持续运行的低功耗眼动追踪，有助于下一代高能效可穿戴系统的推广。

Abstract: Eye tracking is fundamental to numerous applications, yet achieving robust, high-frequency tracking with ultra-low power consumption remains challenging for wearable platforms. While event-based vision sensors offer microsecond resolution and sparse data streams, they have lacked fully integrated, low-power processing solutions capable of real-time inference. In this work, we present the first battery-powered, wearable pupil-center-tracking system with complete on-device integration, combining event-based sensing and neuromorphic processing on the commercially available Speck2f system-on-chip with lightweight coordinate decoding on a low-power microcontroller. Our solution features a novel uncertainty-quantifying spiking neural network with gated temporal decoding, optimized for strict memory and bandwidth constraints, complemented by systematic deployment mechanisms that bridge the reality gap. We validate our system on a new multi-user dataset and demonstrate a wearable prototype with dual neuromorphic devices achieving robust binocular pupil tracking at 100 Hz with an average power consumption below 5 mW per eye. Our work demonstrates that end-to-end neuromorphic computing enables practical, always-on eye tracking for next-generation energy-efficient wearable systems.

</details>


### [118] [Exo2EgoSyn: Unlocking Foundation Video Generation Models for Exocentric-to-Egocentric Video Synthesis](https://arxiv.org/abs/2511.20186)
*Mohammad Mahdi,Yuqian Fu,Nedko Savov,Jiancheng Pan,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: 本文提出了Exo2EgoSyn方法，使基础视频生成模型（如WAN 2.2）能够由第三人称（外观视角）生成第一人称（自我视角）的视频，从而突破原有只能单一视角生成的局限。


<details>
  <summary>Details</summary>
Motivation: 现有基础视频生成模型（如WAN 2.2）在合成高质量文本或图像条件下的视频时表现优异，但只限于同一视角生成，无法实现不同视角间（如第三人称到第一人称）的跨视角视频合成。实际需求中，跨视角（如外观到自我）的视频合成有重要应用价值。

Method: 该方法基于WAN 2.2，并设计了三个关键模块：（1）Ego-Exo视角对齐模块（EgoExo-Align）在潜空间对齐外观与自我视角的首帧表示；（2）多视角外观条件模块（MultiExoCon）将多个外观视角视频聚合为统一的条件信号，提升对原始WAN2.2的拓展性；（3）姿态感知潜变量注入模块（PoseInj）向潜变量中注入相对视角位姿信息，引导不同视角间的几何感知合成。这样，无需重新训练即可实现第三人称到第一人称的高保真度视频生成。

Result: 在ExoEgo4D数据集上的实验表明，Exo2EgoSyn方法显著提升了第一人称-第三人称的跨视角视频合成质量。

Conclusion: Exo2EgoSyn有效突破了基础视频生成模型同一视角的限制，实现了高质量、高扩展性的跨视角视频合成，为大模型的视频生成能力拓展至更复杂应用领域奠定基础。

Abstract: Foundation video generation models such as WAN 2.2 exhibit strong text- and image-conditioned synthesis abilities but remain constrained to the same-view generation setting. In this work, we introduce Exo2EgoSyn, an adaptation of WAN 2.2 that unlocks Exocentric-to-Egocentric(Exo2Ego) cross-view video synthesis. Our framework consists of three key modules. Ego-Exo View Alignment(EgoExo-Align) enforces latent-space alignment between exocentric and egocentric first-frame representations, reorienting the generative space from the given exo view toward the ego view. Multi-view Exocentric Video Conditioning (MultiExoCon) aggregates multi-view exocentric videos into a unified conditioning signal, extending WAN2.2 beyond its vanilla single-image or text conditioning. Furthermore, Pose-Aware Latent Injection (PoseInj) injects relative exo-to-ego camera pose information into the latent state, guiding geometry-aware synthesis across viewpoints. Together, these modules enable high-fidelity ego view video generation from third-person observations without retraining from scratch. Experiments on ExoEgo4D validate that Exo2EgoSyn significantly improves Ego2Exo synthesis, paving the way for scalable cross-view video generation with foundation models. Source code and models will be released publicly.

</details>


### [119] [SFA: Scan, Focus, and Amplify toward Guidance-aware Answering for Video TextVQA](https://arxiv.org/abs/2511.20190)
*Haibin He,Qihuang Zhong,Juhua Liu,Bo Du,Peng Wang,Jing Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频文本视觉问答（Video TextVQA）方法SFA，能更有效地利用视频中的文本线索指导大模型回答问题，在多个数据集上取得了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 视频TextVQA任务极具挑战性，需要模型准确感知文本信息，并整合时间和语义信息，同时还需过滤无关冗余信息。现有方法在这些方面存在不足，因此需要更有效的机制引导模型聚焦于关键文本。

Method: 提出了SFA框架，无需训练，通过仿照人类作答流程，自适应扫描视频帧，聚焦于关键信息区并放大这些区域，从而引导大规模视频视觉语言模型关注必要线索，提高回答准确性。

Result: SFA在多个公开的Video TextVQA数据集上均取得了新的最优结果，并且相较此前的方法有明显提升，验证了方法的有效性和泛化能力。

Conclusion: SFA是一种有效且无需训练的新方法，可大幅提升视频文本视觉问答的性能，为该领域提供了新思路，具有较好的实用性和泛化性。

Abstract: Video text-based visual question answering (Video TextVQA) task aims to answer questions about videos by leveraging the visual text appearing within the videos. This task poses significant challenges, requiring models to accurately perceive and comprehend scene text that varies in scale, orientation, and clarity across frames, while effectively integrating temporal and semantic context to generate precise answers. Moreover, the model must identify question-relevant textual cues and filter out redundant or irrelevant information to ensure answering is guided by the most relevant and informative cues. To address these challenges, we propose SFA, a training-free framework and the first Video-LLM-based method tailored for Video TextVQA, motivated by the human process of answering questions. By adaptively scanning video frames, selectively focusing on key regions, and directly amplifying them, SFA effectively guides the Video-LLM's attention toward essential cues, enabling it to generate more accurate answers. SFA achieves new state-of-the-art results across several public Video TextVQA datasets and surpasses previous methods by a substantial margin, demonstrating its effectiveness and generalizability.

</details>


### [120] [GHR-VQA: Graph-guided Hierarchical Relational Reasoning for Video Question Answering](https://arxiv.org/abs/2511.20201)
*Dionysia Danai Brilli,Dimitrios Mallis,Vassilis Pitsikalis,Petros Maragos*

Main category: cs.CV

TL;DR: 提出了一种新的人物中心视频问答方法GHR-VQA，利用场景图描述人-物交互，通过图神经网络实现跨帧推理，在AGQA上达到领先表现。


<details>
  <summary>Details</summary>
Motivation: 传统视频问答方法多依赖像素特征，难以深入表达人物与物体间复杂关系，缺乏对时空动态的精细理解。因此需要一种能够显式建模人-物交互、支持跨帧推理并提升可解释性和表现力的框架。

Method: 1. 每帧视频生成场景图节点（人物和物体）；2. 将视频内所有帧中的人物节点连接到一个全局root节点，构建以人物为中心的视频级图结构；3. 采用图神经网络提取场景图表示，使其具备丰富上下文信息；4. 使用分层网络融合视频表达与问题特征，逐层提升对局部和整体内容的理解能力。

Result: 实验证明GHR-VQA在AGQA数据集上取得显著提升，尤其是在物体关系推理任务上性能领先，超过已有方法7.3%。

Conclusion: GHR-VQA通过显式建模人物主导的场景图结构，有效提升了视频问答任务中的人-物交互理解能力和推理表现，为时空语义建模和可解释性带来新进展。

Abstract: We propose GHR-VQA, Graph-guided Hierarchical Relational Reasoning for Video Question Answering (Video QA), a novel human-centric framework that incorporates scene graphs to capture intricate human-object interactions within video sequences. Unlike traditional pixel-based methods, each frame is represented as a scene graph and human nodes across frames are linked to a global root, forming the video-level graph and enabling cross-frame reasoning centered on human actors. The video-level graphs are then processed by Graph Neural Networks (GNNs), transforming them into rich, context-aware embeddings for efficient processing. Finally, these embeddings are integrated with question features in a hierarchical network operating across different abstraction levels, enhancing both local and global understanding of video content. This explicit human-rooted structure enhances interpretability by decomposing actions into human-object interactions and enables a more profound understanding of spatiotemporal dynamics. We validate our approach on the Action Genome Question Answering (AGQA) dataset, achieving significant performance improvements, including a 7.3% improvement in object-relation reasoning over the state of the art.

</details>


### [121] [Robust 3D Brain MRI Inpainting with Random Masking Augmentation](https://arxiv.org/abs/2511.20202)
*Juexin Zhang,Ying Weng,Ke Chen*

Main category: cs.CV

TL;DR: 本文提出了一种创新的MRI脑肿瘤图像健康组织重建深度学习方法，在2025年BraTS-Inpainting挑战赛中取得第一名，效果优于往年冠军。


<details>
  <summary>Details</summary>
Motivation: 目前MRI脑肿瘤数据集存在偏差，限制了深度学习模型的定量分析能力，因此需要更好地还原健康组织以提升模型泛化与判别能力。

Method: 采用了U-Net网络结构来对带有合成缺损的三维脑部MRI进行修补（inpainting），并引入了随机掩膜数据增强策略，以提升模型泛化能力。

Result: 在验证集上取得 SSIM 0.873、PSNR 24.996、MSE 0.005 的优异表现；在最终测试集上，SSIM 0.919、PSNR 26.932、RMSE 0.052，均领先于2023和2024年冠军方案。

Conclusion: 该方法有效提高了MRI健康组织重建质量，明显提升了深度学习模型对有偏差数据的适应性，在同类挑战赛中取得领先，并为医学影像重建提供了更优解决方案。

Abstract: The ASNR-MICCAI BraTS-Inpainting Challenge was established to mitigate dataset biases that limit deep learning models in the quantitative analysis of brain tumor MRI. This paper details our submission to the 2025 challenge, a novel deep learning framework for synthesizing healthy tissue in 3D scans. The core of our method is a U-Net architecture trained to inpaint synthetically corrupted regions, enhanced with a random masking augmentation strategy to improve generalization. Quantitative evaluation confirmed the efficacy of our approach, yielding an SSIM of 0.873$\pm$0.004, a PSNR of 24.996$\pm$4.694, and an MSE of 0.005$\pm$0.087 on the validation set. On the final online test set, our method achieved an SSIM of 0.919$\pm$0.088, a PSNR of 26.932$\pm$5.057, and an RMSE of 0.052$\pm$0.026. This performance secured first place in the BraTS-Inpainting 2025 challenge and surpassed the winning solutions from the 2023 and 2024 competitions on the official leaderboard.

</details>


### [122] [OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation](https://arxiv.org/abs/2511.20211)
*Hao Yu,Jiabo Zhan,Zile Wang,Jinglin Wang,Huaisong Zhang,Hongyu Li,Xinrui Chen,Yongxian Wei,Chun Yuan*

Main category: cs.CV

TL;DR: 本论文提出OmniAlpha，这是首个统一、多任务的序列到序列RGBA图像生成与编辑框架，并在多个任务上超越了现有单一专用模型。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在RGB合成方面表现突出，但实际应用中往往需要对RGBA（含alpha透明度通道）进行操作。目前相关研究要么针对RGBA单任务开发专用模型，缺乏通用性，要么统一框架却只能处理RGB，无法满足实际需求。因此，填补多任务RGBA生成和编辑的统一模型空白非常重要。

Method: 提出了OmniAlpha框架，支持多输入和输出RGBA层的序列到序列生成和编辑。核心方法为MSRoPE-BiL，一种具有双向扩展能力的RoPE变换，用于Diffusion Transformer骨干结构。作者还自制了AlphaLayers数据集（1000组三层高质量图像），并通过新的自动合成和过滤流程生成。模型在21类任务上联合训练。

Result: OmniAlpha在21项多样化任务上的表现均优于强劲的专用基线，尤其在无掩码抠图（mask-free matting）任务上，SAD降低84.8%，且在图层条件补全任务中获得90%以上的人类偏好。

Conclusion: OmniAlpha首次证明统一的多任务模型可以在RGBA图像生成与编辑任务中学到优越的共享特征，显著提升性能，为开发更强大、图层感知的生成系统提供了新方向。

Abstract: Generative models have excelled in RGB synthesis, but real-world applications require RGBA manipulation. This has led to a fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OmniAlpha, the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL, a novel RoPE method with a bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers, a new dataset of 1,000 high-quality, multi-layer triplets, built via a novel automated synthesis and filter pipeline. Jointly training OmniAlpha on this dataset across a comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OmniAlpha achieves a dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion. Our work proves that a unified, multi-task model can learn a superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems.

</details>


### [123] [Text-guided Controllable Diffusion for Realistic Camouflage Images Generation](https://arxiv.org/abs/2511.20218)
*Yuhang Qian,Haiyan Chen,Wentong Li,Ningzhong Liu,Jie Qin*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CT-CIG的可控文本引导式伪装图像生成方法，通过视觉语言大模型与创新模块提升生成图像的自然性和逻辑合理性。


<details>
  <summary>Details</summary>
Motivation: 现有伪装图像生成方法容易忽略目标与环境之间的逻辑关系，导致生成结果不够自然、真实。因此，作者希望通过新方法提升伪装效果的真实性与合理性。

Method: 1. 利用大视觉语言模型（VLM），设计伪装揭示对话机制（CRDM），为伪装数据集生成高质量文本描述。
2. 将图像-文本对用于微调Stable Diffusion，并加入轻量级控制器，实现对伪装目标的形状与位置的精细控制，提高伪装场景的协调性。
3. 设计频率交互精炼模块（FIRM），捕捉高频纹理特征，增强复杂伪装图案的表达能力。

Result: 通过CLIPScore评价和伪装有效性测试，结果显示生成的文本提示具备良好的语义一致性，CT-CIG能生成高真实性的伪装图像。

Conclusion: 作者的方法能生成外观和逻辑上更自然、更合理的伪装图像，优于以往方法，对相关领域有积极意义。

Abstract: Camouflage Images Generation (CIG) is an emerging research area that focuses on synthesizing images in which objects are harmoniously blended and exhibit high visual consistency with their surroundings. Existing methods perform CIG by either fusing objects into specific backgrounds or outpainting the surroundings via foreground object-guided diffusion. However, they often fail to obtain natural results because they overlook the logical relationship between camouflaged objects and background environments. To address this issue, we propose CT-CIG, a Controllable Text-guided Camouflage Images Generation method that produces realistic and logically plausible camouflage images. Leveraging Large Visual Language Models (VLM), we design a Camouflage-Revealing Dialogue Mechanism (CRDM) to annotate existing camouflage datasets with high-quality text prompts. Subsequently, the constructed image-prompt pairs are utilized to finetune Stable Diffusion, incorporating a lightweight controller to guide the location and shape of camouflaged objects for enhanced camouflage scene fitness. Moreover, we design a Frequency Interaction Refinement Module (FIRM) to capture high-frequency texture features, facilitating the learning of complex camouflage patterns. Extensive experiments, including CLIPScore evaluation and camouflage effectiveness assessment, demonstrate the semantic alignment of our generated text prompts and CT-CIG's ability to produce photorealistic camouflage images.

</details>


### [124] [Patch-Level Glioblastoma Subregion Classification with a Contrastive Learning-Based Encoder](https://arxiv.org/abs/2511.20221)
*Juexin Zhang,Qifeng Zhong,Ying Weng,Ke Chen*

Main category: cs.CV

TL;DR: 本论文提出并验证了一种基于Vision Transformer（ViT）的病理切片图像自动分析方法，在BraTS-Path 2025竞赛中取得了第二名，有望推动脑胶质母细胞瘤的客观诊断。


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤具有极高的分子和病理异质性，传统的病理评估方法主观性强，自动化、客观化的分析方法需求迫切。深度学习在计算病理领域展现出前景，尤其针对全切片图像分析。

Method: 作者利用预训练的ViT编码器，并在官方训练集上进行微调，加入专门的分类头。模型性能在Synapse平台的验证集和测试集上评估，以MCC和F1分数为主要指标。

Result: 在验证集上，模型MCC为0.7064，F1分数为0.7676；最终测试集上MCC为0.6509，F1分数为0.5330，团队获得BraTS-Pathology 2025挑战赛第二名。

Conclusion: ViT在病理切片图像分析中表现优秀，为后续相关自动化诊断研究提供了基线。未来工作将着重提高模型在未见过数据上的泛化能力。

Abstract: The significant molecular and pathological heterogeneity of glioblastoma, an aggressive brain tumor, complicates diagnosis and patient stratification. While traditional histopathological assessment remains the standard, deep learning offers a promising path toward objective and automated analysis of whole slide images. For the BraTS-Path 2025 Challenge, we developed a method that fine-tunes a pre-trained Vision Transformer (ViT) encoder with a dedicated classification head on the official training dataset. Our model's performance on the online validation set, evaluated via the Synapse platform, yielded a Matthews Correlation Coefficient (MCC) of 0.7064 and an F1-score of 0.7676. On the final test set, the model achieved an MCC of 0.6509 and an F1-score of 0.5330, which secured our team second place in the BraTS-Pathology 2025 Challenge. Our results establish a solid baseline for ViT-based histopathological analysis, and future efforts will focus on bridging the performance gap observed on the unseen validation data.

</details>


### [125] [V-Attack: Targeting Disentangled Value Features for Controllable Adversarial Attacks on LVLMs](https://arxiv.org/abs/2511.20223)
*Sen Nie,Jie Zhang,Jianxin Yan,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种新型对大规模视觉语言模型（LVLMs）进行更精准语义攻击的方法——V-Attack，通过直接操作Transformer中的Value特征，实现对图像中具体语义概念的有效控制和攻击。


<details>
  <summary>Details</summary>
Motivation: 现有对LVLMs的对抗攻击方法难以精确且可控地操纵图像中特定概念的语义，主要由于自注意力机制聚合导致patch特征语义纠缠，局部语义操控困难。因此，亟需一种能突破这一限制、实现精细控制的新方法。

Method: 作者深入分析Transformer注意力模块，发现其Value特征相比patch-token更能保留解耦的高熵本地语义信息。据此提出V-Attack方法，包括：1）自值增强模块（Self-Value Enhancement）用于提升V的语义丰富性；2）文本引导的值操控模块（Text-Guided Value Manipulation）通过文本提示精确定位与优化概念方向，实现针对性语义攻击。

Result: V-Attack在多个主流LVLMs（如LLaVA、InternVL、DeepseekVL和GPT-4o）上验证，攻击成功率比现有最优方法平均提升36%，展现了卓越的精细语义操控能力，有效暴露了视觉语言模型的语义脆弱性。

Conclusion: 通过定位Transformer注意力中的Value特征为更优攻击切入点，V-Attack实现了前所未有的语义精准攻击，为LVLMs鲁棒性研究提出了新框架和思路，并凸显当前多模态模型面临的关键安全挑战。

Abstract: Adversarial attacks have evolved from simply disrupting predictions on conventional task-specific models to the more complex goal of manipulating image semantics on Large Vision-Language Models (LVLMs). However, existing methods struggle with controllability and fail to precisely manipulate the semantics of specific concepts in the image. We attribute this limitation to semantic entanglement in the patch-token representations on which adversarial attacks typically operate: global context aggregated by self-attention in the vision encoder dominates individual patch features, making them unreliable handles for precise local semantic manipulation. Our systematic investigation reveals a key insight: value features (V) computed within the transformer attention block serve as much more precise handles for manipulation. We show that V suppresses global-context channels, allowing it to retain high-entropy, disentangled local semantic information. Building on this discovery, we propose V-Attack, a novel method designed for precise local semantic attacks. V-Attack targets the value features and introduces two core components: (1) a Self-Value Enhancement module to refine V's intrinsic semantic richness, and (2) a Text-Guided Value Manipulation module that leverages text prompts to locate source concept and optimize it toward a target concept. By bypassing the entangled patch features, V-Attack achieves highly effective semantic control. Extensive experiments across diverse LVLMs, including LLaVA, InternVL, DeepseekVL and GPT-4o, show that V-Attack improves the attack success rate by an average of 36% over state-of-the-art methods, exposing critical vulnerabilities in modern visual-language understanding. Our code and data are available https://github.com/Summu77/V-Attack.

</details>


### [126] [HistoSpeckle-Net: Mutual Information-Guided Deep Learning for high-fidelity reconstruction of complex OrganAMNIST images via perturbed Multimode Fibers](https://arxiv.org/abs/2511.20245)
*Jawaria Maqbool,M. Imran Cheema*

Main category: cs.CV

TL;DR: 该论文提出了一种新型深度学习架构HistoSpeckle-Net，用于在多模光纤（MMF）成像中从散斑重建结构丰富的医学图像，解决了复杂现实任务和小样本情况下的成像难题。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在多模光纤成像领域通常只适用于简单数据集，难以处理真实世界复杂的医学图像。此外，这些方法需要大量数据，受限于临床环境下数据获取的难度。作者希望开发一种能在小样本和复杂数据集上表现优秀的成像模型。

Method: 作者设计了一套光学实验系统，将激光经过空间光调制器（SLM）输入多模光纤，并采集与输入OrganAMNIST图像相关的输出散斑图案。另一方面，提出了HistoSpeckle-Net深度学习架构，引入了基于直方图的互信息损失，用于增强鲁棒性、减少数据需求。模型内置直方图计算单元和三尺度特征细化模块，从而综合采用互信息损失和多尺度结构相似度损失，提升重建图像的结构与统计一致性。

Result: 在复杂的OrganAMNIST数据集上，HistoSpeckle-Net在图像结构保真度上优于U-Net和Pix2Pix等主流方法，即使在样本量很少或光纤发生扰动时也表现出色。

Conclusion: HistoSpeckle-Net能有效用更少的数据和具备更强鲁棒性地重建复杂解剖结构图像，推动了多模光纤医学成像向实际临床环境应用的进展。

Abstract: Existing deep learning methods in multimode fiber (MMF) imaging often focus on simpler datasets, limiting their applicability to complex, real-world imaging tasks. These models are typically data-intensive, a challenge that becomes more pronounced when dealing with diverse and complex images. In this work, we propose HistoSpeckle-Net, a deep learning architecture designed to reconstruct structurally rich medical images from MMF speckles. To build a clinically relevant dataset, we develop an optical setup that couples laser light through a spatial light modulator (SLM) into an MMF, capturing output speckle patterns corresponding to input OrganAMNIST images. Unlike previous MMF imaging approaches, which have not considered the underlying statistics of speckles and reconstructed images, we introduce a distribution-aware learning strategy. We employ a histogram-based mutual information loss to enhance model robustness and reduce reliance on large datasets. Our model includes a histogram computation unit that estimates smooth marginal and joint histograms for calculating mutual information loss. It also incorporates a unique Three-Scale Feature Refinement Module, which leads to multiscale Structural Similarity Index Measure (SSIM) loss computation. Together, these two loss functions enhance both the structural fidelity and statistical alignment of the reconstructed images. Our experiments on the complex OrganAMNIST dataset demonstrate that HistoSpeckle-Net achieves higher fidelity than baseline models such as U-Net and Pix2Pix. It gives superior performance even with limited training samples and across varying fiber bending conditions. By effectively reconstructing complex anatomical features with reduced data and under fiber perturbations, HistoSpeckle-Net brings MMF imaging closer to practical deployment in real-world clinical environments.

</details>


### [127] [Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation](https://arxiv.org/abs/2511.20250)
*Daniel Kienzle,Katja Ludwig,Julian Lorenz,Shin'ichi Satoh,Rainer Lienhart*

Main category: cs.CV

TL;DR: 本文提出了一种新的两阶段管线方法，使用单目视频实现了乒乓球三维运动的高精度估计。该方法在现实世界中比现有方法表现更出色。


<details>
  <summary>Details</summary>
Motivation: 以往方法依赖合成数据，难以适应现实场景下的噪声和检测缺失，其根本原因是缺乏真实世界中的三维轨迹和旋转标注。

Method: 方法采用两阶段架构：前端感知网络利用自建TTHQ数据集的丰富2D监督，训练球和桌面关键点检测器；后端“升维”网络仅用物理合理的合成数据训练，专门设计以增强对漏检和帧率变化等常见现实问题的鲁棒性。

Result: 实验表明，该方法比以往单目三维重建技术更强健、更实用，能准确估计乒乓球的3D轨迹和旋转。

Conclusion: 新的两阶段方案显著提升了在真实单目视频中的3D乒乓球轨迹与旋转的检测精度，为运动分析及相关应用提供了高性能技术基础。

Abstract: Obtaining the precise 3D motion of a table tennis ball from standard monocular videos is a challenging problem, as existing methods trained on synthetic data struggle to generalize to the noisy, imperfect ball and table detections of the real world. This is primarily due to the inherent lack of 3D ground truth trajectories and spin annotations for real-world video. To overcome this, we propose a novel two-stage pipeline that divides the problem into a front-end perception task and a back-end 2D-to-3D uplifting task. This separation allows us to train the front-end components with abundant 2D supervision from our newly created TTHQ dataset, while the back-end uplifting network is trained exclusively on physically-correct synthetic data. We specifically re-engineer the uplifting model to be robust to common real-world artifacts, such as missing detections and varying frame rates. By integrating a ball detector and a table keypoint detector, our approach transforms a proof-of-concept uplifting method into a practical, robust, and high-performing end-to-end application for 3D table tennis trajectory and spin analysis.

</details>


### [128] [PromptMoG: Enhancing Diversity in Long-Prompt Image Generation via Prompt Embedding Mixture-of-Gaussian Sampling](https://arxiv.org/abs/2511.20251)
*Bo-Kai Ruan,Teng-Fang Hsiao,Ling Lo,Yi-Lun Wu,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 本文关注于文本生成图像（T2I）模型在处理长文本提示时，如何平衡生成结果的精确性与多样性。作者发现目前主流模型随着提示变长，多样性明显降低。为此，提出了评测基准LPD-Bench和无训练的新方法PromptMoG，有效提升多样性且不损失语义。


<details>
  <summary>Details</summary>
Motivation: 当前大规模T2I模型能够生成高保真图像，长文本提示可增加内容丰富性和精度，但却让生成结果趋于重复，缺乏多样性。该问题尚缺深入分析和解决方案，因而需系统研究和针对性提升。

Method: 1）提出LPD-Bench基准体系，用于长提示文本生成的保真度与多样性评估。2）理论分析后，设计PromptMoG方法，通过在嵌入空间用高斯混合采样提示嵌入，提升生成多样性。此法无需重新训练模型。

Result: 对SD3.5-Large、Flux.1-Krea-Dev、CogView4与Qwen-Image等四个SOTA模型的实验显示，PromptMoG可持续提升长提示文本下的生成多样性，同时保持语义一致性。

Conclusion: 文章系统揭示了现有T2I模型在长文本提示下的多样性下滑问题，并通过LPD-Bench基准与PromptMoG无训练方法，显著提升了生成多样性，促进了高保真多样化图像生成。

Abstract: Recent advances in text-to-image (T2I) generation have achieved remarkable visual outcomes through large-scale rectified flow models. However, how these models behave under long prompts remains underexplored. Long prompts encode rich content, spatial, and stylistic information that enhances fidelity but often suppresses diversity, leading to repetitive and less creative outputs. In this work, we systematically study this fidelity-diversity dilemma and reveal that state-of-the-art models exhibit a clear drop in diversity as prompt length increases. To enable consistent evaluation, we introduce LPD-Bench, a benchmark designed for assessing both fidelity and diversity in long-prompt generation. Building on our analysis, we develop a theoretical framework that increases sampling entropy through prompt reformulation and propose a training-free method, PromptMoG, which samples prompt embeddings from a Mixture-of-Gaussians in the embedding space to enhance diversity while preserving semantics. Extensive experiments on four state-of-the-art models, SD3.5-Large, Flux.1-Krea-Dev, CogView4, and Qwen-Image, demonstrate that PromptMoG consistently improves long-prompt generation diversity without semantic drifting.

</details>


### [129] [Zoo3D: Zero-Shot 3D Object Detection at Scene Level](https://arxiv.org/abs/2511.20253)
*Andrey Lemeshko,Bulat Gabdullin,Nikita Drozdov,Anton Konushin,Danila Rukhovich,Maksim Kolodiazhnyi*

Main category: cs.CV

TL;DR: Zoo3D提出了首个无需训练的3D目标检测框架，实现了无需场景训练即可进行开放词汇3D目标检测，并在ScanNet200和ARKitScenes两个基准上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 当前3D目标检测方法在识别未见过的新类别或场景时受限，且多依赖大量带标签的训练数据（点云或图像）。为了提升实际环境中的鲁棒性，迫切需要无需训练的开放词汇3D目标检测方法。

Method: 提出Zoo3D，通过对2D实例掩码进行图聚类生成3D包围盒，并提出新的开放词汇模块，结合最佳视角选择和多视角一致掩码生成实现类别分配。框架有零样本（Zoo3D_0，无需训练）和自监督（Zoo3D_1，使用Zoo3D_0生成的伪标签训练类别无关检测器）两种模式，并可扩展至直接从带或不带位姿的图像进行推理。

Result: Zoo3D_0和Zoo3D_1在主流开放词汇3D目标检测基准ScanNet200、ARKitScenes上均实现了最优性能。尤其是无需训练的零样本Zoo3D_0已经超越了所有现有自监督方法。

Conclusion: Zoo3D验证了无需训练的新型3D检测范式的有效性和适用性，为实际开放环境3D理解提供了强有力的新解决方案。

Abstract: 3D object detection is fundamental for spatial understanding. Real-world environments demand models capable of recognizing diverse, previously unseen objects, which remains a major limitation of closed-set methods. Existing open-vocabulary 3D detectors relax annotation requirements but still depend on training scenes, either as point clouds or images. We take this a step further by introducing Zoo3D, the first training-free 3D object detection framework. Our method constructs 3D bounding boxes via graph clustering of 2D instance masks, then assigns semantic labels using a novel open-vocabulary module with best-view selection and view-consensus mask generation. Zoo3D operates in two modes: the zero-shot Zoo3D$_0$, which requires no training at all, and the self-supervised Zoo3D$_1$, which refines 3D box prediction by training a class-agnostic detector on Zoo3D$_0$-generated pseudo labels. Furthermore, we extend Zoo3D beyond point clouds to work directly with posed and even unposed images. Across ScanNet200 and ARKitScenes benchmarks, both Zoo3D$_0$ and Zoo3D$_1$ achieve state-of-the-art results in open-vocabulary 3D object detection. Remarkably, our zero-shot Zoo3D$_0$ outperforms all existing self-supervised methods, hence demonstrating the power and adaptability of training-free, off-the-shelf approaches for real-world 3D understanding. Code is available at https://github.com/col14m/zoo3d .

</details>


### [130] [XiCAD: Camera Activation Detection in the Da Vinci Xi User Interface](https://arxiv.org/abs/2511.20254)
*Alexander C. Jenke,Gregor Just,Claas de Boer,Martin Wagner,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: 本论文提出了一种基于ResNet18的轻量级神经网络管道，能够从达芬奇Xi手术系统的内窥镜视频中自动检测并定位内镜臂的激活状态，为下游的外科数据科学任务提供自动化的元数据提取。


<details>
  <summary>Details</summary>
Motivation: 现有达芬奇Xi机器人辅助手术仅通过内窥镜视频提供视觉反馈，其中UI界面包含内镜臂激活等关键信息。自动检测内镜臂的激活状态，对于实现工具追踪、技能评估和摄像头自动控制等数据科学应用具有重要意义。

Method: 作者开发了基于ResNet18卷积神经网络的轻量级管道，同时在SurgToolLoc数据集的手工标注数据上微调模型，并在三个公共数据集（共7万多帧）上进行评估，用于自动识别UI中摄像头激活状态及其在界面中的位置。

Result: 模型对摄像头激活的二分类检测F1分数达0.993~1.000，并能在所有案例中正确定位摄像头区域，且无误报多个摄像头的情况。

Conclusion: 该管道可稳定、实时地从外科手术视频中提取摄像头激活元数据，为后续的自动化分析和多种手术数据科学应用提供关键支撑，代码与数据均已公开。

Abstract: Purpose: Robot-assisted minimally invasive surgery relies on endoscopic video as the sole intraoperative visual feedback. The DaVinci Xi system overlays a graphical user interface (UI) that indicates the state of each robotic arm, including the activation of the endoscope arm. Detecting this activation provides valuable metadata such as camera movement information, which can support downstream surgical data science tasks including tool tracking, skill assessment, or camera control automation.
  Methods: We developed a lightweight pipeline based on a ResNet18 convolutional neural network to automatically identify the position of the camera tile and its activation state within the DaVinci Xi UI. The model was fine-tuned on manually annotated data from the SurgToolLoc dataset and evaluated across three public datasets comprising over 70,000 frames.
  Results: The model achieved F1-scores between 0.993 and 1.000 for the binary detection of active cameras and correctly localized the camera tile in all cases without false multiple-camera detections.
  Conclusion: The proposed pipeline enables reliable, real-time extraction of camera activation metadata from surgical videos, facilitating automated preprocessing and analysis for diverse downstream applications. All code, trained models, and annotations are publicly available.

</details>


### [131] [The Image as Its Own Reward: Reinforcement Learning with Adversarial Reward for Image Generation](https://arxiv.org/abs/2511.20256)
*Weijia Mao,Hao Chen,Zhenheng Yang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出了一种用于图像生成的RL框架Adv-GRPO，通过对抗奖励模型和视觉基础模型实现更接近人类感知的奖励信号，使得生成图像质量和美学表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的图像生成方法，多依赖预训练偏好模型输出标量奖励来近似人类偏好，但难以真正反映人类视觉感知，且容易被“奖励攻击”利用，导致高分却低质的图像输出。

Method: Adv-GRPO通过对抗性方法实时更新生成器和奖励模型。奖励模型以参考图像作正样本进行有监督训练，结合视觉基础模型（如DINO）输出丰富的视觉奖励信号，而非单一标量。生成器直接由此奖励信号指导优化。对比KL正则化等传统约束方法，该方法避免绑定参数更新，奖励直接关联实际视觉输出，同时支持风格定制和分布转移。

Result: 与主流方法Flow-GRPO和SD3比较，在图像质量和美学方面分别取得了70.0%和72.4%的人类评价胜率，并能有效抵抗奖励攻击。

Conclusion: 所提出的Adv-GRPO能显著提升图像生成质量和美学体验，提升了奖励的鲁棒性与表达力。方法拥有分布适应和风格定制能力，为未来图像生成RL探索提供了新的解决途径。

Abstract: A reliable reward function is essential for reinforcement learning (RL) in image generation. Most current RL approaches depend on pre-trained preference models that output scalar rewards to approximate human preferences. However, these rewards often fail to capture human perception and are vulnerable to reward hacking, where higher scores do not correspond to better images. To address this, we introduce Adv-GRPO, an RL framework with an adversarial reward that iteratively updates both the reward model and the generator. The reward model is supervised using reference images as positive samples and can largely avoid being hacked. Unlike KL regularization that constrains parameter updates, our learned reward directly guides the generator through its visual outputs, leading to higher-quality images. Moreover, while optimizing existing reward functions can alleviate reward hacking, their inherent biases remain. For instance, PickScore may degrade image quality, whereas OCR-based rewards often reduce aesthetic fidelity. To address this, we take the image itself as a reward, using reference images and vision foundation models (e.g., DINO) to provide rich visual rewards. These dense visual signals, instead of a single scalar, lead to consistent gains across image quality, aesthetics, and task-specific metrics. Finally, we show that combining reference samples with foundation-model rewards enables distribution transfer and flexible style customization. In human evaluation, our method outperforms Flow-GRPO and SD3, achieving 70.0% and 72.4% win rates in image quality and aesthetics, respectively. Code and models have been released.

</details>


### [132] [Modality-Balanced Collaborative Distillation for Multi-Modal Domain Generalization](https://arxiv.org/abs/2511.20258)
*Xiaohan Wang,Zhangtao Cheng,Ting Zhong,Leiting Chen,Fan Zhou*

Main category: cs.CV

TL;DR: 提出了MBCD框架，通过协同蒸馏和动态调整手段解决多模态权重平均（WA）易受快收敛模态主导的问题，在多模态域泛化任务中显著提升泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统权重平均方法在多模态域泛化任务中容易被收敛较快的模态主导，抑制了其他模态的作用，导致融合效果不佳，最终影响模型的泛化能力和鲁棒性。

Method: MBCD框架包含三个关键步骤：1）学生模型采用自适应模态Dropout，抑制初期对主导模态的偏向；2）引入梯度一致性约束，使单模态和融合分支的学习信号协调平滑；3）基于WA的教师模型进行跨模态蒸馏，将融合知识传递到各单模态分支，提升模态交互并引导收敛到更平坦的损失面。

Result: 大量实验表明，MBCD在多个多模态域泛化基准任务中优于传统方法，在准确率和未见域的鲁棒性等方面都取得了更优表现。

Conclusion: MBCD能够克服权重平均在多模态场景下的核心缺陷，兼具性能和泛化能力，适合各类多模态域泛化应用。

Abstract: Weight Averaging (WA) has emerged as a powerful technique for enhancing generalization by promoting convergence to a flat loss landscape, which correlates with stronger out-of-distribution performance. However, applying WA directly to multi-modal domain generalization (MMDG) is challenging: differences in optimization speed across modalities lead WA to overfit to faster-converging ones in early stages, suppressing the contribution of slower yet complementary modalities, thereby hindering effective modality fusion and skewing the loss surface toward sharper, less generalizable minima. To address this issue, we propose MBCD, a unified collaborative distillation framework that retains WA's flatness-inducing advantages while overcoming its shortcomings in multi-modal contexts. MBCD begins with adaptive modality dropout in the student model to curb early-stage bias toward dominant modalities. A gradient consistency constraint then aligns learning signals between uni-modal branches and the fused representation, encouraging coordinated and smoother optimization. Finally, a WA-based teacher conducts cross-modal distillation by transferring fused knowledge to each uni-modal branch, which strengthens cross-modal interactions and steer convergence toward flatter solutions. Extensive experiments on MMDG benchmarks show that MBCD consistently outperforms existing methods, achieving superior accuracy and robustness across diverse unseen domains.

</details>


### [133] [Advancing Image Classification with Discrete Diffusion Classification Modeling](https://arxiv.org/abs/2511.20263)
*Omer Belhasin,Shelly Golan,Ran El-Yaniv,Michael Elad*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的基于扩散模型的图像分类方法DiDiCM，能够在不确定性高的场景（如图像被扰动或训练数据有限时）取得更优性能。


<details>
  <summary>Details</summary>
Motivation: 在高不确定性场景下，传统图像分类方法性能下降明显，因此需要新的方法提升在困难任务中的分类准确率。

Method: 作者提出了离散扩散分类建模（DiDiCM），通过扩散过程建模输入图像条件下的类别后验分布。该方法支持在类别概率或离散标签上进行扩散预测，能在计算和内存间灵活权衡。

Result: 大量实验证明，在ImageNet等数据集以及更具挑战性的任务中，DiDiCM比标准分类器表现更优，仅需少量扩散迭代便能显著提升准确率，尤其在任务更加困难时优势更明显。

Conclusion: DiDiCM能够有效提升分类系统在高不确定性环境下的鲁棒性和准确性，是一种有前景的分类新框架。

Abstract: Image classification is a well-studied task in computer vision, and yet it remains challenging under high-uncertainty conditions, such as when input images are corrupted or training data are limited. Conventional classification approaches typically train models to directly predict class labels from input images, but this might lead to suboptimal performance in such scenarios. To address this issue, we propose Discrete Diffusion Classification Modeling (DiDiCM), a novel framework that leverages a diffusion-based procedure to model the posterior distribution of class labels conditioned on the input image. DiDiCM supports diffusion-based predictions either on class probabilities or on discrete class labels, providing flexibility in computation and memory trade-offs. We conduct a comprehensive empirical study demonstrating the superior performance of DiDiCM over standard classifiers, showing that a few diffusion iterations achieve higher classification accuracy on the ImageNet dataset compared to baselines, with accuracy gains increasing as the task becomes more challenging. We release our code at https://github.com/omerb01/didicm .

</details>


### [134] [DRL-Guided Neural Batch Sampling for Semi-Supervised Pixel-Level Anomaly Detection](https://arxiv.org/abs/2511.20270)
*Amirhossein Khadivi Noghredeh,Abdollah Safari,Fatemeh Ziaeetabar,Firoozeh Haghighi*

Main category: cs.CV

TL;DR: 本文提出一种半监督深度强化学习框架，用于工业视觉检测中的异常检测，在有限标注数据下取得了比现有方法更高的检测和定位准确率。


<details>
  <summary>Details</summary>
Motivation: 工业视觉检测中异常样本稀缺，现有方法多为无监督重建，难以检测细微缺陷且易过拟合，亟需有效利用有限标注数据提升检测性能。

Method: 方法融合了神经batch采样器、自编码器和预测器。通过强化学习的采样器自适应选取信息量大的patch，并结合自编码器产生的异常区域loss profile和基于loss的分割预测，提升少样本学习能力。

Result: 在MVTec AD数据集上，方法比最新SOTA提升了F1_max（平均0.15，最高0.37）和AUC（0.06），在定位细微异常和整体准确率上均有明显优势，同时保持了低复杂度。

Conclusion: 该半监督强化学习框架，能在异常样本稀缺情况下，有效提升工业视觉异常检测的准确性和异常区域定位能力。

Abstract: Anomaly detection in industrial visual inspection is challenging due to the scarcity of defective samples. Most existing methods rely on unsupervised reconstruction using only normal data, often resulting in overfitting and poor detection of subtle defects. We propose a semi-supervised deep reinforcement learning framework that integrates a neural batch sampler, an autoencoder, and a predictor. The RL-based sampler adaptively selects informative patches by balancing exploration and exploitation through a composite reward. The autoencoder generates loss profiles highlighting abnormal regions, while the predictor performs segmentation in the loss-profile space. This interaction enables the system to effectively learn both normal and defective patterns with limited labeled data. Experiments on the MVTec AD dataset demonstrate that our method achieves higher accuracy and better localization of subtle anomalies than recent state-of-the-art approaches while maintaining low complexity, yielding an average improvement of 0.15 in F1_max and 0.06 in AUC, with a maximum gain of 0.37 in F1_max in the best case.

</details>


### [135] [VKnowU: Evaluating Visual Knowledge Understanding in Multimodal LLMs](https://arxiv.org/abs/2511.20272)
*Tianxiang Jiang,Sheng Xia,Yicheng Xu,Linquan Wu,Xiangyu Zeng,Limin Wang,Yu Qiao,Yi Wang*

Main category: cs.CV

TL;DR: 本文介绍了VKnowU基准，对多模态大模型（MLLMs）的视觉知识理解能力进行评测，并提出了一种结合视觉知识的新方法（VideoKnow+），显著提升了模型在多个基准测试上的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然当前的多模态大模型能识别对象，但缺乏对世界物理和社会原则的直观、人类式理解（视觉知识），这是感知与推理之间的重要桥梁，相关能力尚未被系统评估或增强。

Method: 1）提出VKnowU基准，涵盖8类视觉知识共1680个问题，系统性评估MLLMs。2）评测23个SOTA模型，发现模型与人类在世界常识理解上仍存在显著差距。3）构建新数据集VKnowQA，并提出VideoKnow+模型，通过“看-思-答”结构和视觉知识奖励的强化学习，增强模型视觉知识能力。

Result: VideoKnow+在VKnowU上较现有模型提升3.7%，并在MVBench、Video-MME、MMVU等多项基准测试中实现一致性增益，表现优于传统多模态大模型。

Conclusion: 视觉知识是实现通用多模态大模型不可或缺的核心。系统性地引入与评测视觉知识，推动了模型从‘能看’向‘理解世界’的转变，为多模态智能发展指明新方向。

Abstract: While Multimodal Large Language Models (MLLMs) have become adept at recognizing objects, they often lack the intuitive, human-like understanding of the world's underlying physical and social principles. This high-level vision-grounded semantics, which we term visual knowledge, forms a bridge between perception and reasoning, yet remains an underexplored area in current MLLMs. To systematically evaluate this capability, we present VKnowU, a comprehensive benchmark featuring 1,680 questions in 1,249 videos, covering 8 core types of visual knowledge spanning both world-centric (e.g., intuitive physics) and human-centric (e.g., subjective intentions). Evaluation of 23 SOTA MLLMs reveals that leading models still fall short of human performance, with particularly notable gaps in the world-centric. To bridge this gap, we introduce a new dataset, VKnowQA, and VideoKnow+, a baseline model that explicitly incorporates visual knowledge into MLLMs. VideoKnow+ follows a structured See-Think-Answer paradigm and adopts reinforcement learning with visual knowledge reward, achieving a +3.7% improvement on VKnowU and consistent gains on MVBench, Video-MME, and MMVU. Our work highlights visual knowledge as a missing cornerstone for developing more generalizable MLLMs that can not only see but also truly understand our physical and social worlds.

</details>


### [136] [ScenarioCLIP: Pretrained Transferable Visual Language Models and Action-Genome Dataset for Natural Scene Analysis](https://arxiv.org/abs/2511.20274)
*Advik Sinha,Saurabh Atreya,Aashutosh A,Sk Aziz Ali,Abhijit Das*

Main category: cs.CV

TL;DR: 本文提出了ScenarioCLIP模型，用于更好地建模复杂场景中的多对象及其关系，实现细粒度视觉理解和跨模态检索，并构建了新数据集和基准，取得了优异的零样本与微调性能。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP类型模型主要针对单对象分类或短文本检索任务，缺乏对现实复杂场景中多对象及其关系结构的显式建模，不能很好地支持细粒度视觉理解。

Method: 提出ScenarioCLIP模型，输入包括文本、关系、图像和关注区域，并在经过精心整理情景数据上预训练。为弥补数据集不足，作者利用现有模型和人工结合的方法，扩展现有公开数据集，生成动作、对象、关系标注的新数据集。模型在多种领域特定任务上进行微调。

Result: ScenarioCLIP在包括跨模态检索和细粒度视觉理解等多项场景任务上建立了基准，零样本和微调表现均超过许多基线方法。

Conclusion: ScenarioCLIP有效提升了多对象及其关系建模能力，在复杂场景分析和跨模态任务中展现出强大适应性和性能。模型与数据集已开源，可供后续研究参考和使用。

Abstract: Until recently, the general corpus of CLIP-type fundamental models has widely explored either the retrieval of short descriptions or the classification of objects in the scene as SINGLE-object image classification task. The same holds for retrieving the image embedding (image retrieval task) given a text prompt. However, real-world scene images exhibit rich compositional structure involving multiple objects and actions. The latest methods in the CLIP-based literature improve class-level discrimination by mining harder negative image-text pairs and by refining permanent text prompts, often using LLMs. However, these improvements remain confined to predefined class lists and do not explicitly model relational or compositional structure. PyramidCLIP partially addresses this gap by aligning global and local visual features, yet it still lacks explicit modeling of inter-object relations. Hence, to further leverage this aspect for scene analysis, the proposed ScenarioCLIP model accepts input texts, grounded relations, and input images, along with focused regions highlighting relations. The proposed model is pretrained on curated scenario data, and finetuned for specialized downstream tasks, such as cross-modal retrieval and fine-grained visual understanding tasks. To address the lack of domain-specific datasets, we generate a novel dataset by extending image-text pairs from existing diverse indoor and outdoor scenario datasets that are publicly available. We used a pipeline of existing language models to ground action, object, and relations, filled by manual and automatic curation. We established a comprehensive benchmark for several scenario-based tasks and compared it with many baseline methods. ScenarioCLIP demonstrates robust zero-shot and finetune performance on various domain-specific tasks. Our code and dataset are available at https://github.com/scenario-clip/ScenarioCLIP

</details>


### [137] [DAPointMamba: Domain Adaptive Point Mamba for Point Cloud Completion](https://arxiv.org/abs/2511.20278)
*Yinghui Li,Qianyu Zhou,Di Shao,Hao Yang,Ye Zhu,Richard Dazeley,Xuequan Lu*

Main category: cs.CV

TL;DR: 本论文提出了DAPointMamba框架，首次研究了状态空间模型(SSM)在跨域点云补全任务中的适应性，并通过创新设计解决了传统方法的计算复杂度高及空间结构信息丢失的问题，实现了跨域高效、低复杂度、效果优异的点云补全。


<details>
  <summary>Details</summary>
Motivation: 点云补全在自动驾驶、机器人等领域极其重要，现实场景中常常面临点云数据分布以及标注状态的巨大跨域差异。现有的卷积或视觉Transformer方法要么感受野有限、要么计算复杂度高，因此需要新方法实现高效且强适应性的跨域点云补全。

Method: 作者提出DAPointMamba框架，首次将状态空间模型(SSM)应用于跨域点云补全。主要包括：(1)跨域Patch级别扫描模块，引入几何patch对应进行局部对齐；(2)跨域空间SSM对齐模块，根据跨域相似性调控patch特征，增强空间一致性；(3)跨域Channel SSM对齐模块，通过特征通道对齐缓解全局语义差异。此设计既保证了全局建模能力，又具有效率和泛化性。

Result: 在公开的合成点云和真实点云数据集上，大量实验表明DAPointMamba在准确性、计算复杂度及推理速度等方面均优于现有主流方法。

Conclusion: 本工作证明了SSM框架在跨域点云补全任务的适应性和高效性，为该领域提供了新的思路和更优的性能表现，对实际应用具有重要价值。

Abstract: Domain adaptive point cloud completion (DA PCC) aims to narrow the geometric and semantic discrepancies between the labeled source and unlabeled target domains. Existing methods either suffer from limited receptive fields or quadratic complexity due to using CNNs or vision Transformers. In this paper, we present the first work that studies the adaptability of State Space Models (SSMs) in DA PCC and find that directly applying SSMs to DA PCC will encounter several challenges: directly serializing 3D point clouds into 1D sequences often disrupts the spatial topology and local geometric features of the target domain. Besides, the overlook of designs in the learning domain-agnostic representations hinders the adaptation performance. To address these issues, we propose a novel framework, DAPointMamba for DA PCC, that exhibits strong adaptability across domains and has the advantages of global receptive fields and efficient linear complexity. It has three novel modules. In particular, Cross-Domain Patch-Level Scanning introduces patch-level geometric correspondences, enabling effective local alignment. Cross-Domain Spatial SSM Alignment further strengthens spatial consistency by modulating patch features based on cross-domain similarity, effectively mitigating fine-grained structural discrepancies. Cross-Domain Channel SSM Alignment actively addresses global semantic gaps by interleaving and aligning feature channels. Extensive experiments on both synthetic and real-world benchmarks demonstrate that our DAPointMamba outperforms state-of-the-art methods with less computational complexity and inference latency.

</details>


### [138] [SelfMOTR: Revisiting MOTR with Self-Generating Detection Priors](https://arxiv.org/abs/2511.20279)
*Fabian Gülhan,Emil Mededovic,Yuli Wu,Johannes Stegmaier*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的跟踪Transformer架构SelfMOTR，通过自身生成检测先验信息，有效提升了联合检测与关联任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的端到端目标跟踪方法存在检测性能不足以及在检测与关联联合架构下的任务冲突问题。虽然有研究通过添加去噪、标签分配或融入外部检测先验来缓解，但依赖外部信息增加了复杂性。

Method: 作者提出SelfMOTR，即无需外部检测器，直接利用模型自身的能力生成检测先验。通过大量分析和消融实验，验证了MOTR类模型具备强大的隐式检测能力，并给出了相应的使用工具。

Result: 在DanceTrack数据集上，SelfMOTR表现优异，性能媲美最新的端到端跟踪方法，展现了其方法的有效性。

Conclusion: SelfMOTR能够自生成检测先验，充分发挥MOTR类模型在检测方面的潜力，为提升端到端目标跟踪的性能提供了新方案。

Abstract: Despite progress toward end-to-end tracking with transformer architectures, poor detection performance and the conflict between detection and association in a joint architecture remain critical concerns. Recent approaches aim to mitigate these issues by (i) employing advanced denoising or label assignment strategies, or (ii) incorporating detection priors from external object detectors via distillation or anchor proposal techniques. Inspired by the success of integrating detection priors and by the key insight that MOTR-like models are secretly strong detection models, we introduce SelfMOTR, a novel tracking transformer that relies on self-generated detection priors. Through extensive analysis and ablation studies, we uncover and demonstrate the hidden detection capabilities of MOTR-like models, and present a practical set of tools for leveraging them effectively. On DanceTrack, SelfMOTR achieves strong performance, competing with recent state-of-the-art end-to-end tracking methods.

</details>


### [139] [Bootstrapping Physics-Grounded Video Generation through VLM-Guided Iterative Self-Refinement](https://arxiv.org/abs/2511.20280)
*Yang Liu,Xilin Zhao,Peisong Wen,Siran Dai,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一种物理认知的视频生成方法，通过引入迭代自我优化框架，使生成视频更符合现实物理规律，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型虽然在视觉质量上有进步，但常常生成不符合真实物理规律的视频。为了解决这一问题，作者希望提升视频在物理一致性方面的表现。

Method: 提出了一个结合大语言模型与视觉-语言模型的迭代自我优化（self-refinement）框架，利用多模态chain-of-thought过程，对生成结果中的物理不一致进行反馈并改进提示词，实现多轮优化，且无需额外训练，属于即插即用方法。

Result: 在PhyIQ基准测试上，提出方法将Physics-IQ分数从56.31提升到62.38，表明对物理一致性和视频质量有显著提升。

Conclusion: 本方法证明了通过多模态自我优化流程，可以有效提升视频生成的物理合理性，为未来物理一致性视频生成提供了新思路和方法基础。

Abstract: Recent progress in video generation has led to impressive visual quality, yet current models still struggle to produce results that align with real-world physical principles. To this end, we propose an iterative self-refinement framework that leverages large language models and vision-language models to provide physics-aware guidance for video generation. Specifically, we introduce a multimodal chain-of-thought (MM-CoT) process that refines prompts based on feedback from physical inconsistencies, progressively enhancing generation quality. This method is training-free and plug-and-play, making it readily applicable to a wide range of video generation models. Experiments on the PhyIQ benchmark show that our method improves the Physics-IQ score from 56.31 to 62.38. We hope this work serves as a preliminary exploration of physics-consistent video generation and may offer insights for future research.

</details>


### [140] [Back to the Feature: Explaining Video Classifiers with Video Counterfactual Explanations](https://arxiv.org/abs/2511.20295)
*Chao Wang,Chengan Che,Xinyue Chen,Sophia Tsoka,Luis C. Garcia-Peraza-Herrera*

Main category: cs.CV

TL;DR: 本文提出了一种针对视频分类器的反事实解释方法BTTF，能够生成符合物理规律、时序连贯的视频反事实样本，用于揭示分类器决策依据。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉反事实生成方法主要针对图像分类器，缺乏对视频分类器的支持。由于视频具有时间连贯性和物理可实现性等独特需求，现有方法在生成时序连贯且真实的视频反事实解释方面存在不足。

Method: 提出BTTF优化框架，核心包括：（1）基于输入视频首帧的条件初始噪声优化方案；（2）两阶段优化策略，使搜索空间聚焦于输入视频邻域；（3）渐进式优化策略，逐步增加去噪步骤以提升收敛效率。优化过程完全由目标分类器指导，确保解释的忠实性。

Result: 在Shape-Moving、MEAD和NTU RGB+D数据集上，BTTF方法可有效生成有效、外观相似且物理真实的视频反事实样本，帮助理解分类器的判别机制。

Conclusion: BTTF能够满足视频反事实样本的连贯性和可实现性要求，显著提升了对视频分类器决策过程的解释能力，为实际模型可解释性应用提供了新途径。

Abstract: Counterfactual explanations (CFEs) are minimal and semantically meaningful modifications of the input of a model that alter the model predictions. They highlight the decisive features the model relies on, providing contrastive interpretations for classifiers. State-of-the-art visual counterfactual explanation methods are designed to explain image classifiers. The generation of CFEs for video classifiers remains largely underexplored. For the counterfactual videos to be useful, they have to be physically plausible, temporally coherent, and exhibit smooth motion trajectories. Existing CFE image-based methods, designed to explain image classifiers, lack the capacity to generate temporally coherent, smooth and physically plausible video CFEs. To address this, we propose Back To The Feature (BTTF), an optimization framework that generates video CFEs. Our method introduces two novel features, 1) an optimization scheme to retrieve the initial latent noise conditioned by the first frame of the input video, 2) a two-stage optimization strategy to enable the search for counterfactual videos in the vicinity of the input video. Both optimization processes are guided solely by the target classifier, ensuring the explanation is faithful. To accelerate convergence, we also introduce a progressive optimization strategy that incrementally increases the number of denoising steps. Extensive experiments on video datasets such as Shape-Moving (motion classification), MEAD (emotion classification), and NTU RGB+D (action classification) show that our BTTF effectively generates valid, visually similar and realistic counterfactual videos that provide concrete insights into the classifier's decision-making mechanism.

</details>


### [141] [Prompting Lipschitz-constrained network for multiple-in-one sparse-view CT reconstruction](https://arxiv.org/abs/2511.20296)
*Baoshun Shi,Ke Jiang,Qiusheng Lian,Xinran Yu,Huazhu Fu*

Main category: cs.CV

TL;DR: 提出了一种能够同时适用于多种视角稀疏CT（SVCT）重建场景的深度展开网络PromptCT，具有显式可证的Lipschitz约束，并大幅降低模型存储成本。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习SVCT重建方法面临两大难题：一是先验网络的Lipschitz约束性难以明确证明；二是在多视角配置下每种情况都需训练独立模型，存储成本高且不利于临床应用。

Method: 设计了具备明显Lipschitz连续性的先验网络LipNet，并在网络中加入Prompt模块以区分不同稀疏采样方案，使一个模型可处理多种配置。将LipNet嵌入深度展开框架，开发出PromptCT，实现多任务一体化SVCT重建。

Result: 在模拟和真实数据的实验中，PromptCT对比其他重建算法，在多种稀疏CT场景下获得更佳的重建质量，同时大大降低了存储消耗。

Conclusion: PromptCT框架兼具显式Lipschitz约束和多任务能力，理论上证明了模块的连续性与算法收敛性，实际应用具有优越性，有望促进SVCT算法的临床推广。

Abstract: Despite significant advancements in deep learning-based sparse-view computed tomography (SVCT) reconstruction algorithms, these methods still encounter two primary limitations: (i) It is challenging to explicitly prove that the prior networks of deep unfolding algorithms satisfy Lipschitz constraints due to their empirically designed nature. (ii) The substantial storage costs of training a separate model for each setting in the case of multiple views hinder practical clinical applications. To address these issues, we elaborate an explicitly provable Lipschitz-constrained network, dubbed LipNet, and integrate an explicit prompt module to provide discriminative knowledge of different sparse sampling settings, enabling the treatment of multiple sparse view configurations within a single model. Furthermore, we develop a storage-saving deep unfolding framework for multiple-in-one SVCT reconstruction, termed PromptCT, which embeds LipNet as its prior network to ensure the convergence of its corresponding iterative algorithm. In simulated and real data experiments, PromptCT outperforms benchmark reconstruction algorithms in multiple-in-one SVCT reconstruction, achieving higher-quality reconstructions with lower storage costs. On the theoretical side, we explicitly demonstrate that LipNet satisfies boundary property, further proving its Lipschitz continuity and subsequently analyzing the convergence of the proposed iterative algorithms. The data and code are publicly available at https://github.com/shibaoshun/PromptCT.

</details>


### [142] [CrossEarth-Gate: Fisher-Guided Adaptive Tuning Engine for Efficient Adaptation of Cross-Domain Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2511.20302)
*Shilei Cao,Ziyang Gong,Hehai Lin,Yang Liu,Jiashun Cheng,Xiaoxing Hu,Haoyuan Liang,Guowen Li,Chengwei Qin,Hong Cheng,Xue Yang,Juepeng Zheng,Haohuan Fu*

Main category: cs.CV

TL;DR: 本文提出了一种新方法CrossEarth-Gate, 用于提升遥感领域大模型在下游任务上的适应性，有效解决了遥感数据中的多方面域差异问题，在16个跨域遥感语义分割基准上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法在遥感领域尤其是大规模地球观测任务中效果有限，无法有效应对遥感数据中多维度且不可预测的域差（如空间、语义、频率等），因此亟需新的方法提升大模型的泛化与适应能力。

Method: 提出了CrossEarth-Gate框架，其核心包括两个创新：（1）构建了涵盖空间、语义、频率三类模块的遥感差异适应工具箱；（2）设计了基于Fisher信息指导的自适应模块选择机制，通过量化各模块对任务梯度流的贡献，动态激活最关键的模块，并在合适的网络层次进行引导优化。

Result: 在16个跨域遥感语义分割基准上，CrossEarth-Gate均取得了领先的性能表现，验证了方法的有效性和泛化能力。

Conclusion: CrossEarth-Gate为遥感领域参数高效微调提供了新的思路和工具，有效提升了大模型应对域差的能力，实现了更高效和泛化的下游任务适应能力。代码即将开源。

Abstract: In Remote Sensing (RS), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a key approach to activate the generalizable representation ability of foundation models for downstream tasks. However, existing specialized PEFT methods often fail when applied to large-scale Earth observation tasks, as they are unable to fully handle the multifaceted and unpredictable domain gaps (\eg, spatial, semantic, and frequency shifts) inherent in RS data. To overcome this, we propose CrossEarth-Gate, which introduces two primary contributions. First, we establish a comprehensive RS module toolbox to address multifaceted domain gaps, comprising spatial, semantic, and frequency modules. Second, we develop a Fisher-guided adaptive selection mechanism that operates on this toolbox. This selection is guided by Fisher Information to quantify each module's importance by measuring its contribution to the task-specific gradient flow. It dynamically activates only the most critical modules at the appropriate layers, guiding the gradient flow to maximize adaptation effectiveness and efficiency. Comprehensive experiments validate the efficacy and generalizability of our method, where CrossEarth-Gate achieves state-of-the-art performance across 16 cross-domain benchmarks for RS semantic segmentation. The code of the work will be released.

</details>


### [143] [TaCo: Capturing Spatio-Temporal Semantic Consistency in Remote Sensing Change Detection](https://arxiv.org/abs/2511.20306)
*Han Guo,Chenyang Liu,Haotian Zhang,Bowen Chen,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: 提出了一种名为TaCo的新方法，通过引入时空语义一致性约束，显著提升了遥感变化检测精度，并在多个公开数据集上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化检测方法主要依靠掩码监督，仅能有效定位空间变化，但对时序语义变化约束有限，导致结果在空间上一致但语义上不连贯。为解决该问题，需要增强模型对时空语义转变的建模能力。

Method: TaCo方法在传统掩码监督框架基础上，新增时空语义联合约束。具体通过Text-guided Transition Generator结合文本语义与时序图像特征，生成跨时序的转移特征。同时，设计了由时序重构约束和转移约束组成的时空语义联合约束，前者对齐重构与原始特征，后者增强变化区域的判别性。此策略在推理阶段无额外计算开销。

Result: 在六个公开数据集（二类与多类变化检测任务）上，TaCo方法均获得了最新最优（SOTA）性能表现。

Conclusion: 通过引入时空语义一致性约束，TaCo有效缓解了变化检测中的语义不连贯问题，提升了模型整体性能，并具备实用价值。

Abstract: Remote sensing change detection (RSCD) aims to identify surface changes across bi-temporal satellite images. Most previous methods rely solely on mask supervision, which effectively guides spatial localization but provides limited constraints on the temporal semantic transitions. Consequently, they often produce spatially coherent predictions while still suffering from unresolved semantic inconsistencies. To address this limitation, we propose TaCo, a spatio-temporal semantic consistent network, which enriches the existing mask-supervised framework with a spatio-temporal semantic joint constraint. TaCo conceptualizes change as a semantic transition between bi-temporal states, in which one temporal feature representation can be derived from the other via dedicated transition features. To realize this, we introduce a Text-guided Transition Generator that integrates textual semantics with bi-temporal visual features to construct the cross-temporal transition features. In addition, we propose a spatio-temporal semantic joint constraint consisting of bi-temporal reconstruct constraints and a transition constraint: the former enforces alignment between reconstructed and original features, while the latter enhances discrimination for changes. This design can yield substantial performance gains without introducing any additional computational overhead during inference. Extensive experiments on six public datasets, spanning both binary and semantic change detection tasks, demonstrate that TaCo consistently achieves SOTA performance.

</details>


### [144] [TReFT: Taming Rectified Flow Models For One-Step Image Translation](https://arxiv.org/abs/2511.20307)
*Shengqian Li,Ming Gao,Yi Liu,Zuzeng Lin,Feng Wang,Feng Dai*

Main category: cs.CV

TL;DR: 本文提出了TReFT方法，用于提升Rectified Flow（RF）模型在图像到图像转换中的效率，使其达到与最先进方法相当的表现并能实现实时推断。


<details>
  <summary>Details</summary>
Motivation: 现有的RF模型在高质量合成任务上表现出色，但在图像到图像转换中仍需耗时的多步去噪，难以实现实时应用。CycleGAN-Turbo虽基于扩散模型实现一步推断，但直接用于RF模型会导致收敛性问题，因此亟需一种高效且稳定的新方法。

Method: 作者提出TReFT方法，通过直接使用预训练DiT或UNet预测的velocity输出，有效解决了一步推断下对抗训练中的收敛性问题。此外，借助理论分析，发现去噪末期的velocity与起点到目标图像的向量趋于一致。训练时引入内存高效的循环一致性和身份损失以及模型简化，加快推断速度。

Result: 使用TReFT微调大规模RF模型（如SD3.5和FLUX），在多个图像转换数据集上取得了与最先进方法相当的性能，并实现了实时推断能力。

Conclusion: TReFT方法为RF模型在一步图像转换和对抗训练中提供了高效、稳定的解决方案，在保证生成质量的同时也大幅提升了速度，推动了相关应用的落地。

Abstract: Rectified Flow (RF) models have advanced high-quality image and video synthesis via optimal transport theory. However, when applied to image-to-image translation, they still depend on costly multi-step denoising, hindering real-time applications. Although the recent adversarial training paradigm, CycleGAN-Turbo, works in pretrained diffusion models for one-step image translation, we find that directly applying it to RF models leads to severe convergence issues. In this paper, we analyze these challenges and propose TReFT, a novel method to Tame Rectified Flow models for one-step image Translation. Unlike previous works, TReFT directly uses the velocity predicted by pretrained DiT or UNet as output-a simple yet effective design that tackles the convergence issues under adversarial training with one-step inference. This design is mainly motivated by a novel observation that, near the end of the denoising process, the velocity predicted by pretrained RF models converges to the vector from origin to the final clean image, a property we further justify through theoretical analysis. When applying TReFT to large pretrained RF models such as SD3.5 and FLUX, we introduce memory-efficient latent cycle-consistency and identity losses during training, as well as lightweight architectural simplifications for faster inference. Pretrained RF models finetuned with TReFT achieve performance comparable to sota methods across multiple image translation datasets while enabling real-time inference.

</details>


### [145] [IrisNet: Infrared Image Status Awareness Meta Decoder for Infrared Small Targets Detection](https://arxiv.org/abs/2511.20319)
*Xuelin Qian,Jiaming Lu,Zixuan Wang,Wenxuan Wang,Zhongling Huang,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: 提出了IrisNet，一种动态适应红外小目标检测的元学习框架，利用transformer连接图像特征与解码器参数，通过实验在多个数据集上取得了最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的红外小目标检测方法在面对不同场景（如昼/夜、天空/海洋/地面）时，容易出现模式漂移，缺乏鲁棒性，无法动态适应复杂背景下的检测任务，因此需要更具适应性的方法。

Method: 提出IrisNet，采用image-to-decoder transformer，将红外图像特征动态映射为整个解码器参数。具体做法是将参数化的解码器表示为结构化二维张量，保持层次相关性，用自注意力建模解码器层间依赖，用交叉注意力生成自适应解码模式，并融合高频信息以加强目标和场景感知能力。

Result: 在NUDT-SIRST、NUAA-SIRST和IRSTD-1K 三个主流红外小目标检测数据集上，IrisNet均取得了最先进的性能，优于现有方法。

Conclusion: IrisNet能够有效应对红外小目标检测中的多样场景和复杂背景，提高了模型的适应性和检测准确率，具有明显优势。

Abstract: Infrared Small Target Detection (IRSTD) faces significant challenges due to low signal-to-noise ratios, complex backgrounds, and the absence of discernible target features. While deep learning-based encoder-decoder frameworks have advanced the field, their static pattern learning suffers from pattern drift across diverse scenarios (\emph{e.g.}, day/night variations, sky/maritime/ground domains), limiting robustness. To address this, we propose IrisNet, a novel meta-learned framework that dynamically adapts detection strategies to the input infrared image status. Our approach establishes a dynamic mapping between infrared image features and entire decoder parameters via an image-to-decoder transformer. More concretely, we represent the parameterized decoder as a structured 2D tensor preserving hierarchical layer correlations and enable the transformer to model inter-layer dependencies through self-attention while generating adaptive decoding patterns via cross-attention. To further enhance the perception ability of infrared images, we integrate high-frequency components to supplement target-position and scene-edge information. Experiments on NUDT-SIRST, NUAA-SIRST, and IRSTD-1K datasets demonstrate the superiority of our IrisNet, achieving state-of-the-art performance.

</details>


### [146] [AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models](https://arxiv.org/abs/2511.20325)
*Tianyi Yan,Tao Tang,Xingtai Gui,Yongkang Li,Jiasen Zhesng,Weiyao Huang,Lingdong Kong,Wencheng Han,Xia Zhou,Xueyang Zhang,Yifei Zhan,Kun Zhan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

TL;DR: 该论文提出了一种基于公正世界模型和反事实合成的数据生成方法，用于提高自动驾驶端到端模型在遇到风险和长尾事件时的安全性和表现。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶端到端模型虽然能够直接从传感器数据学习复杂行为，但在安全性和长尾事件处理上存在严重挑战。强化学习（RL）有望解决这些问题，但在自动驾驶领域应用上收效甚微。作者认为，根本原因在于现有世界模型普遍存在过度乐观的偏见，难以准确预见危险。

Method: 作者提出了公正世界模型（Impartial World Model）与反事实合成（Counterfactual Synthesis）框架。反事实合成系统地生成多样化的碰撞和偏离道路等危险事件丰富模型训练，使其能准确反映动作与结果之间的因果关系。该模型作为内在批判者融入闭环强化学习流程中，能够“梦想”不同动作可能导致的后果，对策略做事后优化。

Result: 在新设立的Risk Foreseeing Benchmark等多项实验中，所提方法在预测失败（如碰撞、偏离）上显著优于现有方法。作为内在评价者时，可显著减少模拟环境中的安全违规事件。

Conclusion: 使世界模型能够“梦想”危险是建设安全且智能自动驾驶体的关键一步，本文提出的策略优化框架有效增强了自动驾驶RL模型的安全性和对极端事件的应对能力。

Abstract: End-to-end models for autonomous driving hold the promise of learning complex behaviors directly from sensor data, but face critical challenges in safety and handling long-tail events. Reinforcement Learning (RL) offers a promising path to overcome these limitations, yet its success in autonomous driving has been elusive. We identify a fundamental flaw hindering this progress: a deep seated optimistic bias in the world models used for RL. To address this, we introduce a framework for post-training policy refinement built around an Impartial World Model. Our primary contribution is to teach this model to be honest about danger. We achieve this with a novel data synthesis pipeline, Counterfactual Synthesis, which systematically generates a rich curriculum of plausible collisions and off-road events. This transforms the model from a passive scene completer into a veridical forecaster that remains faithful to the causal link between actions and outcomes. We then integrate this Impartial World Model into our closed-loop RL framework, where it serves as an internal critic. During refinement, the agent queries the critic to ``dream" of the outcomes for candidate actions. We demonstrate through extensive experiments, including on a new Risk Foreseeing Benchmark, that our model significantly outperforms baselines in predicting failures. Consequently, when used as a critic, it enables a substantial reduction in safety violations in challenging simulations, proving that teaching a model to dream of danger is a critical step towards building truly safe and intelligent autonomous agents.

</details>


### [147] [3D Motion Perception of Binocular Vision Target with PID-CNN](https://arxiv.org/abs/2511.20332)
*Shi Jiazhao,Pan Pan,Shi Haotian*

Main category: cs.CV

TL;DR: 本文提出并训练了一种能够感知双目视觉目标三维运动信息的神经网络。网络可实时输出三维坐标、速度和加速度，具备基础的时空感知能力，且体积较小，在模拟数据集上获得接近输入分辨率极限的预测精度。


<details>
  <summary>Details</summary>
Motivation: 提升神经网络对双目视觉所得三维运动目标的空间与时序特征的感知能力，并探索如何将PID思想以及非线性系统拟合能力结合到网络设计与分析中，使其既高效又精度高。

Method: 从PID理论视角理解神经网络处理非线性问题能力，将单层网络等效为二阶差分方程加非线性表述；多层网络则多次变换特征。设计17层、413K参数的轻量级PID卷积网络，通过拼接和池化实用性特征复用，并在仿真数据集（随机运动小球）上训练测试。

Result: 网络在模拟数据集上的预测精度接近输入图像分辨率所能达到的上限。详尽分析了实验结果中的误差、局限与潜在改进方向。

Conclusion: 高维卷积有提升计算效率和特征利用率的优势，引入PID信息有助于实现记忆与注意力机制。网络设计理念为后续相关任务网络结构提供了有益参考。

Abstract: This article trained a network for perceiving three-dimensional motion information of binocular vision target, which can provide real-time three-dimensional coordinate, velocity, and acceleration, and has a basic spatiotemporal perception capability. Understood the ability of neural networks to fit nonlinear problems from the perspective of PID. Considered a single-layer neural network as using a second-order difference equation and a nonlinearity to describe a local problem. Multilayer networks gradually transform the raw representation to the desired representation through multiple such combinations. Analysed some reference principles for designing neural networks. Designed a relatively small PID convolutional neural network, with a total of 17 layers and 413 thousand parameters. Implemented a simple but practical feature reuse method by concatenation and pooling. The network was trained and tested using the simulated randomly moving ball datasets, and the experimental results showed that the prediction accuracy was close to the upper limit that the input image resolution can represent. Analysed the experimental results and errors, as well as the existing shortcomings and possible directions for improvement. Finally, discussed the advantages of high-dimensional convolution in improving computational efficiency and feature space utilization. As well as the potential advantages of using PID information to implement memory and attention mechanisms.

</details>


### [148] [ShelfRectNet: Single View Shelf Image Rectification with Homography Estimation](https://arxiv.org/abs/2511.20335)
*Onur Berk Tore,Ibrahim Samil Yalciner,Server Calap*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的单张图像单视角单应性估计算法，适用于如零售货架校正等实际场景，并发布了相关数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 在零售等实际应用中，往往只能获得单一视角的货架图像，因此如何仅凭一张图像实现货架校正（单应性估计）成为了实际需求。

Method: 作者提出了一种以ConvNeXt为骨干网络的深度学习框架，采用归一化坐标回归增强模型稳定性，并通过建模和采样生成合成单应变换进行新颖的数据增强，以缓解样本稀缺和提升泛化能力。模型直接预测4点参数化的单应性矩阵。

Result: 方法在测试集上实现了平均角点误差1.298像素，在精度和推理速度方面均优于传统视觉和现有深度学习方法。

Conclusion: 该方法为实际单视角校正场景提供了高效稳健的解决方案，并促进领域进一步研究，相关数据集与代码将公开。

Abstract: Estimating homography from a single image remains a challenging yet practically valuable task, particularly in domains like retail, where only one viewpoint is typically available for shelf monitoring and product alignment. In this paper, we present a deep learning framework that predicts a 4-point parameterized homography matrix to rectify shelf images captured from arbitrary angles. Our model leverages a ConvNeXt-based backbone for enhanced feature representation and adopts normalized coordinate regression for improved stability. To address data scarcity and promote generalization, we introduce a novel augmentation strategy by modeling and sampling synthetic homographies. Our method achieves a mean corner error of 1.298 pixels on the test set. When compared with both classical computer vision and deep learning-based approaches, our method demonstrates competitive performance in both accuracy and inference speed. Together, these results establish our approach as a robust and efficient solution for realworld single-view rectification. To encourage further research in this domain, we will make our dataset, ShelfRectSet, and code publicly available

</details>


### [149] [AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend](https://arxiv.org/abs/2511.20343)
*Hengyi Wang,Lourdes Agapito*

Main category: cs.CV

TL;DR: 该论文提出了一种新型多视图前馈模型AMB3R，在用于稠密三维重建任务时表现优异，不仅能高效提升3D重建质量，还可无缝扩展至视觉里程计和大规模结构光流任务，无需特定微调或额外优化，整体性能优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前3D重建、视觉里程计和结构光流任务通常依赖于点云地图或强依赖于任务专有优化过程，存在空间利用低和泛化能力较弱的问题，因此需要一种更加紧凑、高效的场景表示和通用的模型框架。

Method: 提出使用稀疏且紧凑的体素化场景表示作为模型的后端，通过多视图前馈神经网络实现稠密三维重建。该模型仅需进行多视图重建训练，可直接扩展至视觉里程计(VIO)和大规模结构光流(SfM)等任务，无需针对性微调或测试时优化。

Result: 相比以点云为基础的方法，AMB3R在摄像头位姿、深度及尺度估计、三维重建等方面均取得了最新最优性能，且在常用基准上优于以优化为主的SLAM和SfM方法，尤其在稠密重建任务中表现突出。

Conclusion: AMB3R通过稀疏体素场景表示和多视图前馈网络，突破了三维重建及相关任务的空间利用率和精度瓶颈，具备优异的通用扩展性和领先性能，为三维视觉领域的各类任务提供了统一且高效的解决方案。

Abstract: We present AMB3R, a multi-view feed-forward model for dense 3D reconstruction on a metric-scale that addresses diverse 3D vision tasks. The key idea is to leverage a sparse, yet compact, volumetric scene representation as our backend, enabling geometric reasoning with spatial compactness. Although trained solely for multi-view reconstruction, we demonstrate that AMB3R can be seamlessly extended to uncalibrated visual odometry (online) or large-scale structure from motion without the need for task-specific fine-tuning or test-time optimization. Compared to prior pointmap-based models, our approach achieves state-of-the-art performance in camera pose, depth, and metric-scale estimation, 3D reconstruction, and even surpasses optimization-based SLAM and SfM methods with dense reconstruction priors on common benchmarks.

</details>


### [150] [Thinking in 360°: Humanoid Visual Search in the Wild](https://arxiv.org/abs/2511.20351)
*Heyang Yu,Yinan Han,Xiangyu Zhang,Baiqiao Yin,Bowen Chang,Xiangyu Han,Xinhao Liu,Jing Zhang,Marco Pavone,Chen Feng,Saining Xie,Yiming Li*

Main category: cs.CV

TL;DR: 本文提出了全新的人形体视觉搜索任务和基准，显著提升了开源模型在360°复杂场景下的搜索能力，但仍面临空间推理等巨大挑战。


<details>
  <summary>Details</summary>
Motivation: 以往视觉搜索方法多局限于静态图片，未能考虑人类在真实三维环境中的头部和眼睛协同运动。这限制了智能体在真实世界的表现。因此，迫切需要开发具身的视觉搜索智能体，并在更具挑战性的现实场景中评估其能力。

Method: 提出humanoid visual search任务，让虚拟人形体主动旋转头部搜寻全景图中的目标或路径。为此，构建了H* Bench基准，涵盖车站、超市、街道等复杂真实场景。评测了现有主流模型，并通过后训练技术增强Qwen2.5-VL性能。

Result: 即使是最顶尖的专有模型在新基准上目标和路径搜索成功率仅约30%。开源Qwen2.5-VL在后训练后，目标搜索从14.83%提升到47.38%，路径搜索从6.44%提升到24.94%。路径搜索整体成功率较低，展示该任务的难度和空间常识诉求。

Conclusion: 实验推动了多模态大模型在三维具身视觉语义理解方向的进步，但结果表明对于真实复杂环境的无缝集成还有巨大距离。工作提出了未来更具挑战性方向，并量化了当前主要瓶颈。

Abstract: Humans rely on the synergistic control of head (cephalomotor) and eye (oculomotor) to efficiently search for visual information in 360°. However, prior approaches to visual search are limited to a static image, neglecting the physical embodiment and its interaction with the 3D world. How can we develop embodied visual search agents as efficient as humans while bypassing the constraints imposed by real-world hardware? To this end, we propose humanoid visual search where a humanoid agent actively rotates its head to search for objects or paths in an immersive world represented by a 360° panoramic image. To study visual search in visually-crowded real-world scenarios, we build H* Bench, a new benchmark that moves beyond household scenes to challenging in-the-wild scenes that necessitate advanced visual-spatial reasoning capabilities, such as transportation hubs, large-scale retail spaces, urban streets, and public institutions. Our experiments first reveal that even top-tier proprietary models falter, achieving only ~30% success in object and path search. We then use post-training techniques to enhance the open-source Qwen2.5-VL, increasing its success rate by over threefold for both object search (14.83% to 47.38%) and path search (6.44% to 24.94%). Notably, the lower ceiling of path search reveals its inherent difficulty, which we attribute to the demand for sophisticated spatial commonsense. Our results not only show a promising path forward but also quantify the immense challenge that remains in building MLLM agents that can be seamlessly integrated into everyday human life.

</details>


### [151] [GS-Checker: Tampering Localization for 3D Gaussian Splatting](https://arxiv.org/abs/2511.20354)
*Haoliang Han,Ziyuan Luo,Jun Qi,Anderson Rocha,Renjie Wan*

Main category: cs.CV

TL;DR: 论文提出了一种无监督检测并定位3D Gaussian Splatting（3DGS）模型篡改区域的新方法。


<details>
  <summary>Details</summary>
Motivation: 随着3DGS编辑技术的发展，3D内容被恶意篡改的风险增加，对篡改区域的检测成为亟需解决的问题。

Method: 方法主要包括三部分：1）向3D Gaussian参数中融入了3D篡改属性，用以表示高斯球体是否被篡改；2）设计3D对比机制，通过比较高斯球体关键属性的相似性寻找3D层面的篡改线索；3）引入循环优化策略，进一步精细化篡改属性，无需昂贵的3D标注数据监督。

Result: 实验结果表明，该方法能够有效定位3DGS模型中的篡改区域，在多个数据集上表现出良好的检测性能。

Conclusion: GS-Checker为3DGS模型的篡改检测提供了新颖且有效的解决方案，无需依赖昂贵标注，有助于保障3D内容安全。

Abstract: Recent advances in editing technologies for 3D Gaussian Splatting (3DGS) have made it simple to manipulate 3D scenes. However, these technologies raise concerns about potential malicious manipulation of 3D content. To avoid such malicious applications, localizing tampered regions becomes crucial. In this paper, we propose GS-Checker, a novel method for locating tampered areas in 3DGS models. Our approach integrates a 3D tampering attribute into the 3D Gaussian parameters to indicate whether the Gaussian has been tampered. Additionally, we design a 3D contrastive mechanism by comparing the similarity of key attributes between 3D Gaussians to seek tampering cues at 3D level. Furthermore, we introduce a cyclic optimization strategy to refine the 3D tampering attribute, enabling more accurate tampering localization. Notably, our approach does not require expensive 3D labels for supervision. Extensive experimental results demonstrate the effectiveness of our proposed method to locate the tampered 3DGS area.

</details>


### [152] [From Passive Perception to Active Memory: A Weakly Supervised Image Manipulation Localization Framework Driven by Coarse-Grained Annotations](https://arxiv.org/abs/2511.20359)
*Zhiqing Guo,Dongdong Xi,Songlin Li,Gaobo Yang*

Main category: cs.CV

TL;DR: 本文提出了一种结合弱监督和高精度定位的新框架BoxPromptIML，兼顾了低标注成本和细粒度篡改定位表现。


<details>
  <summary>Details</summary>
Motivation: 现有的图像篡改定位方法在高精度（全监督，需像素级标注）与低成本（弱监督，标签级）之间难以兼得，迫切需要同时兼顾标注成本和定位精度的新方法。

Method: 提出粗区域标注策略，以及基于BoxPrompt的弱监督框架。通过知识蒸馏，从基于SAM的固定教师模型向学生模型传递能力。同时，设计了具有人类记忆灵感的特征融合模块，使模型在当前图像上下文下动态地“回忆”知识，从而提升定位准确性和鲁棒性。

Result: 在多种数据集（包括分布内和分布外）上，BoxPromptIML表现优越，定位效果超越或媲美全监督模型，并兼具强泛化性、低成本和高效部署的优势。

Conclusion: BoxPromptIML实现了低标注成本与高精度定位的平衡，为大规模实际应用带来价值，可高效推广至现实场景。

Abstract: Image manipulation localization (IML) faces a fundamental trade-off between minimizing annotation cost and achieving fine-grained localization accuracy. Existing fully-supervised IML methods depend heavily on dense pixel-level mask annotations, which limits scalability to large datasets or real-world deployment.In contrast, the majority of existing weakly-supervised IML approaches are based on image-level labels, which greatly reduce annotation effort but typically lack precise spatial localization. To address this dilemma, we propose BoxPromptIML, a novel weakly-supervised IML framework that effectively balances annotation cost and localization performance. Specifically, we propose a coarse region annotation strategy, which can generate relatively accurate manipulation masks at lower cost. To improve model efficiency and facilitate deployment, we further design an efficient lightweight student model, which learns to perform fine-grained localization through knowledge distillation from a fixed teacher model based on the Segment Anything Model (SAM). Moreover, inspired by the human subconscious memory mechanism, our feature fusion module employs a dual-guidance strategy that actively contextualizes recalled prototypical patterns with real-time observational cues derived from the input. Instead of passive feature extraction, this strategy enables a dynamic process of knowledge recollection, where long-term memory is adapted to the specific context of the current image, significantly enhancing localization accuracy and robustness. Extensive experiments across both in-distribution and out-of-distribution datasets show that BoxPromptIML outperforms or rivals fully-supervised models, while maintaining strong generalization, low annotation cost, and efficient deployment characteristics.

</details>


### [153] [VGGTFace: Topologically Consistent Facial Geometry Reconstruction in the Wild](https://arxiv.org/abs/2511.20366)
*Xin Ming,Yuxuan Han,Tianyu Huang,Feng Xu*

Main category: cs.CV

TL;DR: VGGTFace提出了一种利用3D基础模型VGGT与Pixel3DMM相结合的方法，从多视角图片自动重建脸部几何，并实现了高质量、强泛化、拓扑一致的人脸重建。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸几何重建方法存在手工工作繁琐、对真实场景数据泛化能力弱、或3D形变模型表达能力有限等问题，急需一种自动化程度高、能处理真实拍摄场景、且保证拓扑一致性的新方法。

Method: VGGTFace创新性地将VGGT 3D基础模型应用于多视角图片，通过引入Pixel3DMM以像素对齐的UV值注入拓扑信息，将VGGT的点云转换为具有已知拓扑的点云。为更好融合这些数据，提出了一种基于Laplacian 能量的新型拓扑感知束调（Bundle Adjustment）策略，实现点云的高效优化和拼合。

Result: 本方法可在单张NVIDIA RTX 4090显卡上，16视角下10秒钟完成高质量人脸重建，在公开基准测试和真实场景测试中取得了业界领先的表现。

Conclusion: VGGTFace打破了以往方法的局限，兼具高质量重建、强泛化能力和拓扑一致性，为数字人构建流程提供了新的自动化工具，具有推广和实际应用价值。

Abstract: Reconstructing topologically consistent facial geometry is crucial for the digital avatar creation pipelines. Existing methods either require tedious manual efforts, lack generalization to in-the-wild data, or are constrained by the limited expressiveness of 3D Morphable Models. To address these limitations, we propose VGGTFace, an automatic approach that innovatively applies the 3D foundation model, \emph{i.e.} VGGT, for topologically consistent facial geometry reconstruction from in-the-wild multi-view images captured by everyday users. Our key insight is that, by leveraging VGGT, our method naturally inherits strong generalization ability and expressive power from its large-scale training and point map representation. However, it is unclear how to reconstruct a topologically consistent mesh from VGGT, as the topology information is missing in its prediction. To this end, we augment VGGT with Pixel3DMM for injecting topology information via pixel-aligned UV values. In this manner, we convert the pixel-aligned point map of VGGT to a point cloud with topology. Tailored to this point cloud with known topology, we propose a novel Topology-Aware Bundle Adjustment strategy to fuse them, where we construct a Laplacian energy for the Bundle Adjustment objective. Our method achieves high-quality reconstruction in 10 seconds for 16 views on a single NVIDIA RTX 4090. Experiments demonstrate state-of-the-art results on benchmarks and impressive generalization to in-the-wild data. Code is available at https://github.com/grignarder/vggtface.

</details>


### [154] [FREE: Uncertainty-Aware Autoregression for Parallel Diffusion Transformers](https://arxiv.org/abs/2511.20390)
*Xinwan Wen,Bowen Li,Jiajun Luo,Ye Li,Zhi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为FREE的Diffusion Transformer加速方法，通过在特征层面实现并行采样，大幅降低采样延迟并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers虽然生成质量优秀，但高采样延迟严重影响实际应用。现有推理加速方法在DiTs上效果有限，急需新方法提升推理效率。

Method: 作者分析了Diffusion Transformer的特征动态，发现其顶层Transformer的特征具有高度时序一致性。基于此，提出FREE框架：用轻量级drafter在特征层做自回归预测，并通过并行verifier校验，实现无损加速。此外，针对后期步预测不确定度增加导致的验收率下降问题，作者又提出FREE (relax)的不确定性引导自适应策略，动态调整采纳概率。

Result: 在ImageNet-512实验中，FREE方法最高可实现1.86倍采样加速，FREE (relax)则能达到2.25倍加速，且生成图像在主观和客观指标上均能保持高质量。

Conclusion: 该工作突破了DiTs推理加速的性能瓶颈，显著提升了采样效率，为高质量扩散模型的实用化应用奠定了基础。

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art generation quality but require long sequential denoising trajectories, leading to high inference latency. Recent speculative inference methods enable lossless parallel sampling in U-Net-based diffusion models via a drafter-verifier scheme, but their acceleration is limited on DiTs due to insufficient draft accuracy during verification. To address this limitation, we analyze the DiTs' feature dynamics and find the features of the final transformer layer (top-block) exhibit strong temporal consistency and rich semantic abstraction. Based on this insight, we propose FREE, a novel framework that employs a lightweight drafter to perform feature-level autoregression with parallel verification, guaranteeing lossless acceleration with theoretical and empirical support. Meanwhile, prediction variance (uncertainty) of DiTs naturally increases in later denoising steps, reducing acceptance rates under speculative sampling. To mitigate this effect, we further introduce an uncertainty-guided relaxation strategy, forming FREE (relax), which dynamically adjusts the acceptance probability in response to uncertainty levels. Experiments on ImageNet-$512^2$ show that FREE achieves up to $1.86 \times$ acceleration, and FREE (relax) further reaches $2.25 \times$ speedup while maintaining high perceptual and quantitative fidelity in generation quality.

</details>


### [155] [A Training-Free Approach for Multi-ID Customization via Attention Adjustment and Spatial Control](https://arxiv.org/abs/2511.20401)
*Jiawei Lin,Guanlong Jiao,Jianjin Xu*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练即可实现多身份定制图像生成的新方法MultiID，显著提升了图像质量和文本可控性，并建立了相关评测基准。


<details>
  <summary>Details</summary>
Motivation: 在计算机视觉中，如何将多个人物的身份特征无缝融合到一张图像中，同时保持各自身份的独特性，是个难题。现有多身份个性化模型存在质量差和文本不可控两大问题，亟需创新方法解决。

Method: 作者提出MultiID方法，核心思路是基于已有单身份生成模型，通过ID解耦交叉注意力机制（ID-decoupled cross-attention）为不同区域注入各自的身份表征。为提高文本可控性，还设计了局部提示（local prompt）、深度引导空间控制（depth-guided spatial control）与拓展自注意力（extended self-attention）三种策略。该方法无需额外训练。

Result: 实验建立了新的评测基准IDBench，实证表明MultiID在生成质量和文本一致性两个方面效果显著优于或媲美现有基于训练的方法，且克服了以往模型的“复制粘贴”问题和文本弱可控性。

Conclusion: MultiID为多身份定制提供了高效、灵活的解决方案，在无需训练的前提下大大改善结果质量和文本可控性，对实际应用具备较高价值，为领域发展带来新思路。

Abstract: Multi-ID customization is an interesting topic in computer vision and attracts considerable attention recently. Given the ID images of multiple individuals, its purpose is to generate a customized image that seamlessly integrates them while preserving their respective identities. Compared to single-ID customization, multi-ID customization is much more difficult and poses two major challenges. First, since the multi-ID customization model is trained to reconstruct an image from the cropped person regions, it often encounters the copy-paste issue during inference, leading to lower quality. Second, the model also suffers from inferior text controllability. The generated result simply combines multiple persons into one image, regardless of whether it is aligned with the input text. In this work, we propose MultiID to tackle this challenging task in a training-free manner. Since the existing single-ID customization models have less copy-paste issue, our key idea is to adapt these models to achieve multi-ID customization. To this end, we present an ID-decoupled cross-attention mechanism, injecting distinct ID embeddings into the corresponding image regions and thus generating multi-ID outputs. To enhance the generation controllability, we introduce three critical strategies, namely the local prompt, depth-guided spatial control, and extended self-attention, making the results more consistent with the text prompts and ID images. We also carefully build a benchmark, called IDBench, for evaluation. The extensive qualitative and quantitative results demonstrate the effectiveness of MultiID in solving the aforementioned two challenges. Its performance is comparable or even better than the training-based multi-ID customization methods.

</details>


### [156] [Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs](https://arxiv.org/abs/2511.20410)
*Bao Tang,Shuai Zhang,Yueting Zhu,Jijun Xiang,Xin Yang,Li Yu,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种无需外部数据、提高效率的扩散模型蒸馏方法（TBCM），实现了高质量生成且显著节省资源。


<details>
  <summary>Details</summary>
Motivation: 现有连续时间的一致性蒸馏方法依赖大量训练数据和算力，难以在资源受限环境及多领域扩展，因此需寻求无需外部数据的新型高效方案。

Method: 提出Trajectory-Backward Consistency Model（TBCM），不依赖VAE编码和大规模数据集，而是直接从教师模型的生成轨迹中提取潜在表示，实现自洽蒸馏，加速训练并简化流程。

Result: TBCM在MJHQ-30k数据集上一步生成下取得6.52 FID和28.08 CLIP分数，训练时间较Sana-Sprint减少约40%，显著节省GPU显存，效率和质量兼具。

Conclusion: TBCM减少了对外部数据与算力依赖，提升了扩散模型蒸馏的效率与适用性，并通过理论分析为未来一致性蒸馏研究提供了新视角和思路。

Abstract: Timestep distillation is an effective approach for improving the generation efficiency of diffusion models. The Consistency Model (CM), as a trajectory-based framework, demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher model's generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectory-extracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 FID and 28.08 CLIP scores on MJHQ-30k under one-step generation, while reducing training time by approximately 40% compared to Sana-Sprint and saving a substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusion-generation space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/hustvl/TBCM.

</details>


### [157] [MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts](https://arxiv.org/abs/2511.20415)
*Zilong Huang,Jun He,Xiaobin Huang,Ziyi Xiong,Yang Luo,Junyan Ye,Weijia Li,Yiping Chen,Ting Han*

Main category: cs.CV

TL;DR: 该论文提出了MajutsuCity，一种基于自然语言、兼具美学自适应的3D城市生成框架，能够实现结构一致性、风格多样性与高度可控性的城市建模。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时满足文本驱动的创意灵活性与对象级别可编辑性，导致3D城市生成在可控性和多样性上受限。因此需要一种新方法实现高可控性、风格多样化和结构合理的三维城市生成。

Method: MajutsuCity将城市表示为可控布局、资产和材质的组合，通过四阶段流水线生成。此外，集成了MajutsuAgent编辑代理，支持五种对象级别操作，并构建了高质量多模态数据集MajutsuDataset（含详细标注的2D布局、3D建筑资产和PBR材质等），并开发了多维度评测指标。

Result: 实验显示，MajutsuCity的布局FID比CityDreamer降低83.7%，比CityCraft降低20.1%；在AQS和RDR等全部分数上排名第一，显著超越现有方法。

Conclusion: MajutsuCity在几何保真度、风格适应性和语义可控性方面成为3D城市生成领域的新SOTA，有望推动相关研究进展。源码及数据集将公开发布。

Abstract: Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.

</details>


### [158] [StableTrack: Stabilizing Multi-Object Tracking on Low-Frequency Detections](https://arxiv.org/abs/2511.20418)
*Matvei Shelukhan,Timur Mamedov,Karina Kvanchiani*

Main category: cs.CV

TL;DR: 本文提出StableTrack方法，通过新颖的两阶段匹配策略与Bbox-Based Distance，有效提升低频检测下的多目标跟踪表现，实验证明在低频场景下显著优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 多目标跟踪在计算资源受限（如低频检测）时表现严重下降，而当前主流方法主要聚焦于高频场景，难以兼顾低算力应用。作者希望解决有限算力下MOT的精度问题。

Method: 1. 提出StableTrack，采用两阶段匹配策略增强跨帧目标关联能力。2. 将传统的Mahalanobis距离替换为更适合Re-ID的Bbox-Based Distance。3. 将视觉跟踪整合进Kalman Filter与整体跟踪流程，提升在低频检测下的跟踪稳定性。

Result: 在MOT17-val数据集低频率检测(1Hz)下，HOTA指标提升11.6%，显著超越当前先进方法；在标准频率下（MOT17、MOT20、DanceTrack）表现亦与最佳方法相当。

Conclusion: StableTrack在低频检测情况下大幅提升多目标跟踪精度，同时能在标准检测频率下维持最优水平，适用于算力受限等实际应用场景。

Abstract: Multi-object tracking (MOT) is one of the most challenging tasks in computer vision, where it is important to correctly detect objects and associate these detections across frames. Current approaches mainly focus on tracking objects in each frame of a video stream, making it almost impossible to run the model under conditions of limited computing resources. To address this issue, we propose StableTrack, a novel approach that stabilizes the quality of tracking on low-frequency detections. Our method introduces a new two-stage matching strategy to improve the cross-frame association between low-frequency detections. We propose a novel Bbox-Based Distance instead of the conventional Mahalanobis distance, which allows us to effectively match objects using the Re-ID model. Furthermore, we integrate visual tracking into the Kalman Filter and the overall tracking pipeline. Our method outperforms current state-of-the-art trackers in the case of low-frequency detections, achieving $\textit{11.6%}$ HOTA improvement at $\textit{1}$ Hz on MOT17-val, while keeping up with the best approaches on the standard MOT17, MOT20, and DanceTrack benchmarks with full-frequency detections.

</details>


### [159] [Block Cascading: Training Free Acceleration of Block-Causal Video Models](https://arxiv.org/abs/2511.20426)
*Hmrishav Bandyopadhyay,Nikhil Pinnaparaju,Rahim Entezari,Jim Scott,Yi-Zhe Song,Varun Jampani*

Main category: cs.CV

TL;DR: 提出Block Cascading方法，通过训练外的并行化显著提升区块因果（block-causal）视频生成速度，最高速度提升约2倍，而几乎不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有区块因果视频生成存在速度与质量的权衡：小模型快但画质一般，大模型画质好但速度极慢，无法兼顾实时性和高质量。

Method: 提出无需额外训练的Block Cascading新方法，在生成时允许后续视频块在前序块未完全去噪时即可启动，通过这种并行方式，多个块可同时去噪。此法可在多块并行时大幅提高推理速度，且消除Key-Value重缓存带来的交互延迟。

Result: 实验证明，使用Block Cascading后，1.3B模型速度由16提升至30 FPS，14B模型由4.5提升到12.5 FPS，在所有模型规模上均近2倍加速。生成质量与原有方法基本持平。

Conclusion: Block Cascading极大缓解了区块因果视频生成中的速度质量权衡，兼顾响应速度和效果，适用于实际交互式生成场景。

Abstract: Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/

</details>


### [160] [Object-Centric Vision Token Pruning for Vision Language Models](https://arxiv.org/abs/2511.20439)
*Guangyuan Li,Rongzhen Zhao,Jinhong Deng,Yanbo Wang,Joni Pajarinen*

Main category: cs.CV

TL;DR: 本文提出了一种用于视觉语言模型（VLM）推理加速的直观且有保障的视觉令牌剪枝方法OC-VTP，在保证高推理效率的同时，最大程度保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有VLM中，视觉令牌数量多但信息分散，导致推理时计算量大且效率低。虽然已有许多视觉令牌剪枝方法，但它们多采用间接方式且缺乏结果保障。因此，亟需一种高效、直接并能保障精度的令牌选择机制。

Method: 提出OC-VTP方法，通过轻量级预训练一个小型以目标为中心的视觉令牌剪枝器，无需对主模型或数据集进行微调。该剪枝器根据重构未剪枝令牌的误差直接筛选信息量最大的视觉令牌，从而实现令牌数量减少但信息尽量保留。

Result: 在各类视觉剪枝率下，OC-VTP均能帮助主流VLM显著提升推理效率且最大程度保持精度。同时，剪枝选择过程具有较好的可解释性。

Conclusion: OC-VTP是一种高效、简单且有理论保障的VLM视觉令牌剪枝方法，为提升VLM推理效率提供了优选方案，对实际应用具有重要意义。

Abstract: In Vision Language Models (VLMs), vision tokens are quantity-heavy yet information-dispersed compared with language tokens, thus consume too much unnecessary computation. Pruning redundant vision tokens for high VLM inference efficiency has been continuously studied but all existing methods resort to indirect and non-guaranteed ways. We propose OC-VTP, a direct and guaranteed approach to select the most representative vision tokens for high-efficiency yet accuracy-preserving VLM inference. Our OC-VTP requires merely light-weight pre-training of a small object-centric vision token pruner, which can then be inserted into existing VLMs, without fine-tuning of any models on any datasets. It is gauranteed that the most representative vision tokens are kept by minimizing the error in reconstructing the original unpruned tokens from the selected ones. Across any vision pruning ratios, i.e., inference efficiency, our OC-VTP consistently helps mainstream VLMs to preserve the highest inference accuracy. Our pruning also demonstrates interesting interpretability. Our codes are available at https://github.com/GarryLarry010131/OC-VTP.

</details>


### [161] [Learning to Generate Human-Human-Object Interactions from Textual Descriptions](https://arxiv.org/abs/2511.20446)
*Jeonghyeon Na,Sangwon Baik,Inhee Lee,Junyoung Lee,Hanbyul Joo*

Main category: cs.CV

TL;DR: 本文提出并研究了人与人之间结合物体的互动（HHOI）建模问题，建立了数据集，并提出了基于生成模型的合成方法，实现了多人人物-物体-人物复杂交互的文本生成。


<details>
  <summary>Details</summary>
Motivation: 现有机器对人类复杂情景下的多人物交互理解能力不足，缺乏能描述多人-多人-物体相关行为的专用数据集和生成模型。作者希望推动机器对多人及其与环境关联互动场景的理解、建模、生成。

Method: 1）提出HHOI（Human-Human-Object Interaction）新问题和相应数据集；2）利用图像生成模型合成HHOI数据；3）解耦出个体HOI（人-物体）和HHI（人-人）数据，分别训练基于score-based diffusion的text-to-HOI、text-to-HHI生成模型；4）提出统一生成框架，将两模型结合，通过高级采样实现完整HHOI的生成，支持多于两人的互动。

Result: 实验表明，该方法可根据文本描述生成更真实的多人-人与人-物体交互场面，生成质量优于只建模单人物-物体交互的以往方法。此外，该框架还可扩展应用到多人物动作生成。

Conclusion: 本文方法提高了对多人-物体复杂互动的建模与生成能力，完善了数据基础与生成流程，对提升机器对于真实复杂场景交互理解与合成具有推动作用。

Abstract: The way humans interact with each other, including interpersonal distances, spatial configuration, and motion, varies significantly across different situations. To enable machines to understand such complex, context-dependent behaviors, it is essential to model multiple people in relation to the surrounding scene context. In this paper, we present a novel research problem to model the correlations between two people engaged in a shared interaction involving an object. We refer to this formulation as Human-Human-Object Interactions (HHOIs). To overcome the lack of dedicated datasets for HHOIs, we present a newly captured HHOIs dataset and a method to synthesize HHOI data by leveraging image generative models. As an intermediary, we obtain individual human-object interaction (HOIs) and human-human interaction (HHIs) from the HHOIs, and with these data, we train an text-to-HOI and text-to-HHI model using score-based diffusion model. Finally, we present a unified generative framework that integrates the two individual model, capable of synthesizing complete HHOIs in a single advanced sampling process. Our method extends HHOI generation to multi-human settings, enabling interactions involving more than two individuals. Experimental results show that our method generates realistic HHOIs conditioned on textual descriptions, outperforming previous approaches that focus only on single-human HOIs. Furthermore, we introduce multi-human motion generation involving objects as an application of our framework.

</details>


### [162] [Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search](https://arxiv.org/abs/2511.20460)
*Yunqi Zhou,Chengjie Jiang,Chun Yuan,Jing Li*

Main category: cs.CV

TL;DR: 提出了一种适用于超高分辨率遥感视觉问答（RS-VQA）的新方法ZoomSearch，能有效兼顾精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感基础模型难以直接处理超高分辨率影像，整体编码消耗资源，缩放处理丢失关键细节，亟需高效且保留细节的方法。

Method: ZoomSearch是一种免训练、可即插即用的流程，分离了“关注何处”与“如何回答”。该方法包括自适应多分支缩放搜索（自顶向下定位与问题相关的图像区域）、以及布局感知的图像块重组（将选中的区域以布局真实的方式整合）。

Result: 在超高分辨率RS-VQA基准测试集MME-RealWorld-RS和LRS-VQA上，ZoomSearch与LLAVA-ov结合达到最优精度，比LLaVA-ov基线任务准确率提升26.3%（LRS-VQA）和114.8%（MME-RealWorld-RS），推理速度超越现有方法20%-44%。

Conclusion: ZoomSearch显著提升了超高分辨率遥感问答的性能和效率，为该领域的应用提供了新的解决思路。

Abstract: With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread. However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details. In this context, guiding the model look where it matters before prediction becomes crucial. Therefore, we present ZoomSearch, a training-free, plug-and-play pipeline that decouples 'where to look' from 'how to answer' for Ultra-HR Remote Sensing Visual Question Answering (RS-VQA). ZoomSearch combines Adaptive Multi-Branch Zoom Search, which performs a hierarchical search over image patches to localize query-relevant regions, with Layout-Aware Patch Reassembly, which reorganizes the selected patches into a compact, layout-faithful canvas. We conduct comprehensive experiments on Ultra-HR RS-VQA benchmarks MME-RealWorld-RS and LRS-VQA, comparing against (i) strong general foundation models, (ii) remote sensing foundation models, (iii) Ultra-HR RS-VQA methods, and (iv) plug-and-play search-based VQA methods. When integrated with LLaVA-ov, ZoomSearch attains state-of-the-art accuracy across diverse tasks, improving the LLaVA-ov baseline by 26.3% on LRS-VQA and 114.8\% on MME-RealWorld-RS. Meanwhile, it achieves much higher inference efficiency, outperforming prior search-based methods by 20%~44% in speed.

</details>


### [163] [STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow](https://arxiv.org/abs/2511.20462)
*Jiatao Gu,Ying Shen,Tianrong Chen,Laurent Dinh,Yuyang Wang,Miguel Angel Bautista,David Berthelot,Josh Susskind,Shuangfei Zhai*

Main category: cs.CV

TL;DR: 本文提出了基于归一化流（Normalizing Flows, NF）的高质量视频生成模型STARFlow-V，首次展示了NFs在视频自回归生成任务中的有效性，具有可解释性强、采样高效等优点。


<details>
  <summary>Details</summary>
Motivation: 近期视频生成主要依赖扩散模型，但其自回归机制存在误差累积等问题，且采样效率不高。作者旨在探索NFs这一概率生成模型，在高复杂度时空结构的视频生成中是否具备优势。

Method: 作者基于现有的STARFlow，提出了STARFlow-V模型。该模型利用全局-局部架构，将因果依赖限制在全局潜变量中，同时保留局部帧内丰富信息，以缓解误差累积问题。设计了flow-score matching机制，引入轻量化的因果去噪器提升连贯性。为提升采样效率，提出基于Jacobi迭代的视频感知采样策略。

Result: STARFlow-V可无缝支持文本-视频、图像-视频和视频-视频生成任务。在视觉质量、时间一致性及采样速度等多项指标上优于扩散基线模型，实验结果首次证明NF模型在视频自回归生成任务中的潜力。

Conclusion: 工作表明NFs不仅理论上可行，而且在实际视频生成中表现优异，为世界模型等复杂生成任务提供了新方向。

Abstract: Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.

</details>


### [164] [Dance Style Classification using Laban-Inspired and Frequency-Domain Motion Features](https://arxiv.org/abs/2511.20469)
*Ben Hamscher,Arnold Brosch,Nicolas Binninger,Maksymilian Jan Dejna,Kira Maag*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的舞蹈风格分类方法，通过视频中提取人体姿态信息并基于时空特征与频域分析进行分类，不依赖复杂模型实现高效且准确的风格辨识。


<details>
  <summary>Details</summary>
Motivation: 舞蹈动作识别因不同风格之间存在许多相似姿势与动态模式而变得复杂，当前方法通常需要高计算量，且解释性不强。为实现高效、易解释的舞蹈风格自动识别，作者提出了新方法。

Method: 作者利用视频中的动作姿态估计，设计基于Laban动作分析灵感的时空特征提取器，关注上半身各关节的速度、加速度和角速度等动态指标；并结合快速傅立叶变换（FFT）特征，从频率域提取舞蹈动作的节奏和周期性特征，从而实现风格分类。

Result: 该方法在无需复杂深度模型的情况下，能够鲁棒且高效地分辨不同舞蹈风格。时空和频域特征的融合有助于捕捉动作风格细微差异。

Conclusion: 可解释性强、轻计算负担的动作时空-频域特征可以有效描述和区分多样的舞蹈风格，为舞蹈识别和相关人类活动分析开辟了新思路。

Abstract: Dance is an essential component of human culture and serves as a tool for conveying emotions and telling stories. Identifying and distinguishing dance genres based on motion data is a complex problem in human activity recognition, as many styles share similar poses, gestures, and temporal motion patterns. This work presents a lightweight framework for classifying dance styles that determines motion characteristics based on pose estimates extracted from videos. We propose temporal-spatial descriptors inspired by Laban Movement Analysis. These features capture local joint dynamics such as velocity, acceleration, and angular movement of the upper body, enabling a structured representation of spatial coordination. To further encode rhythmic and periodic aspects of movement, we integrate Fast Fourier Transform features that characterize movement patterns in the frequency domain. The proposed approach achieves robust classification of different dance styles with low computational effort, as complex model architectures are not required, and shows that interpretable motion representations can effectively capture stylistic nuances.

</details>


### [165] [Modular Deep Learning Framework for Assistive Perception: Gaze, Affect, and Speaker Identification](https://arxiv.org/abs/2511.20474)
*Akshit Pramod Anchan,Jewelith Thomas,Sritama Roy*

Main category: cs.CV

TL;DR: 本研究提出并验证了三种独立的感知模块（眼部状态检测、面部表情识别、语音识别），为多模态辅助技术的集成提供了高效方案。


<details>
  <summary>Details</summary>
Motivation: 辅助技术需整合视觉和听觉感知，以提升用户体验和设备功能；但在资源受限条件下实现高效集成具挑战。

Method: 设计三种独立的感知模块：基于卷积神经网络（CNN）的眼部状态检测，深度CNN的面部表情识别，以及基于长短时记忆网络（LSTM）的语音说话人识别，分别使用不同公开或自定义数据集进行训练和测试。

Result: 眼部状态检测准确率93.0%，面部表情识别准确率97.8%，语音说话人识别准确率96.89%。三模块在各自任务上都取得了较高成绩。

Conclusion: 轻量级、领域专用的模型可在特定任务上实现高精度，适合未来集成到资源有限的实时多模态辅助设备中。

Abstract: Developing comprehensive assistive technologies requires the seamless integration of visual and auditory perception. This research evaluates the feasibility of a modular architecture inspired by core functionalities of perceptive systems like 'Smart Eye.' We propose and benchmark three independent sensing modules: a Convolutional Neural Network (CNN) for eye state detection (drowsiness/attention), a deep CNN for facial expression recognition, and a Long Short-Term Memory (LSTM) network for voice-based speaker identification. Utilizing the Eyes Image, FER2013, and customized audio datasets, our models achieved accuracies of 93.0%, 97.8%, and 96.89%, respectively. This study demonstrates that lightweight, domain-specific models can achieve high fidelity on discrete tasks, establishing a validated foundation for future real-time, multimodal integration in resource-constrained assistive devices.

</details>


### [166] [A Physics-Informed Loss Function for Boundary-Consistent and Robust Artery Segmentation in DSA Sequences](https://arxiv.org/abs/2511.20501)
*Muhammad Irfan,Nasir Rahim,Khalid Mahmood Malik*

Main category: cs.CV

TL;DR: 该论文提出了一种基于物理学的损失函数（Physics-Informed Loss, PIL），用于提高DSA序列中脑动脉分割的准确性。该方法通过模拟预测和真实边界间的弹性相互作用，促使网络捕捉更平滑和一致的血管结构，取得了优于传统损失函数的效果。


<details>
  <summary>Details</summary>
Motivation: 传统的分割损失函数主要关注像素重叠，对于血管边界的几何和物理一致性考虑不足，导致血管分割结果可能出现断裂和不稳定。因此亟需一种能够结合血管物理属性的新损失函数。

Method: 提出了一种受材料物理中位错理论启发的PIL损失函数，通过将预测与真实血管边界间的相互作用建模为弹性过程，引入物理正则化，从而提升网络对结构一致性和几何细节的学习能力。该损失函数被集成到多种主流分割网络，并在公开数据集上进行了评测。

Result: 在DIAS和DSCA两个公开数据集上，PIL损失函数在敏感性、F1分数和边界一致性方面均优于Cross-Entropy、Dice、Active Contour和Surface等传统损失函数。

Conclusion: 基于物理学的PIL损失函数显著提升了动态血管成像中的分割准确性和鲁棒性，为临床脑血管疾病管理模型提供了更可靠的数据支撑。

Abstract: Accurate extraction and segmentation of the cerebral arteries from digital subtraction angiography (DSA) sequences is essential for developing reliable clinical management models of complex cerebrovascular diseases. Conventional loss functions often rely solely on pixel-wise overlap, overlooking the geometric and physical consistency of vascular boundaries, which can lead to fragmented or unstable vessel predictions. To overcome this limitation, we propose a novel \textit{Physics-Informed Loss} (PIL) that models the interaction between the predicted and ground-truth boundaries as an elastic process inspired by dislocation theory in materials physics. This formulation introduces a physics-based regularization term that enforces smooth contour evolution and structural consistency, allowing the network to better capture fine vascular geometry. The proposed loss is integrated into several segmentation architectures, including U-Net, U-Net++, SegFormer, and MedFormer, and evaluated on two public benchmarks: DIAS and DSCA. Experimental results demonstrate that PIL consistently outperforms conventional loss functions such as Cross-Entropy, Dice, Active Contour, and Surface losses, achieving superior sensitivity, F1 score, and boundary coherence. These findings confirm that the incorporation of physics-based boundary interactions into deep neural networks improves both the precision and robustness of vascular segmentation in dynamic angiographic imaging. The implementation of the proposed method is publicly available at https://github.com/irfantahir301/Physicsis_loss.

</details>


### [167] [AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs](https://arxiv.org/abs/2511.20515)
*Kuniaki Saito,Risa Shinoda,Shohei Tanaka,Tosho Hirasawa,Fumio Okura,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: 本文提出AlignBench基准，通过细粒度的图像-文本对评估VLM（视觉语言模型）的对齐能力，发现CLIP相关模型存在明显短板。


<details>
  <summary>Details</summary>
Motivation: 现有图像-文本对齐评测主要依赖规则扰动或简短描述，难以检测模型的精细对齐能力，需要更高分辨率的评测工具。

Method: 作者构建AlignBench基准，包含由多种图像到文本和文本到图像模型生成的详细图文配对，每句都由人工标注正误。用这些数据，直接测量VLM评估对齐的可靠性，并针对广泛的解码式VLM进行基准测试。

Result: 三大主要发现：(i) CLIP相关模型（包括针对组合推理专门设计的）仍然存在严重的感知盲区；(ii) 检测器模型对前几句话评分系统性偏高；(iii) 模型表现出强烈的自偏好，偏向自身生成的输出，损害整体检测性能。

Conclusion: AlignBench能更真实地评估和揭示主流VLM的对齐短板，现有CLIP及相关模型需针对细粒度对齐能力加以改进。

Abstract: Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.

</details>


### [168] [HBridge: H-Shape Bridging of Heterogeneous Experts for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2511.20520)
*Xiang Wang,Zhifei Zhang,He Zhang,Zhe Lin,Yuqian Zhou,Qing Liu,Shiwei Zhang,Yijun Li,Shaoteng Liu,Haitian Zheng,Jason Kuen,Yuehuan Wang,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的异构H形架构模型HBridge，通过选择性中间层衔接并引入语义重构机制，在多模态任务中大幅提升效率与生成质量，优于现有对称融合方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态统一模型多采用对称结构（如MoT、BAGEL、LMFusion），为方便初始化和融合，将各专家模型结构互相镜像。然而不同模态间存在本质差异，这种做法无法发挥各自预训练领域知识的最大优势。因此，作者希望设计一种能更好利用各自领域先验、提升多模态生成表现的新方法。

Method: 作者提出HBridge，一种H形异构结构，仅在专家中间层进行选择性衔接，浅层与深层保持模态特异性、减少信息直接共享，同时通过引入语义重构token，显式引导生成专家重建图像的视觉语义token。这不仅减少了40%以上的共享注意力（提升计算效率），也促进了中层语义对齐与跨模态一致性。

Result: 在多个多模态基准数据集上，HBridge展示出了显著优于BAGEL、LMFusion等先进方法的生成效果与效率，实验充分验证了设计理念的有效性。

Conclusion: HBridge为多模态生成提供了一种高效且表现卓越的新范式，证明异构结构与选择性信息衔接在解决模态鸿沟、提升生成质量方面具有重要优势，为后续相关研究奠定了基础。

Abstract: Recent unified models integrate understanding experts (e.g., LLMs) with generative experts (e.g., diffusion models), achieving strong multimodal performance. However, recent advanced methods such as BAGEL and LMFusion follow the Mixture-of-Transformers (MoT) paradigm, adopting a symmetric design that mirrors one expert to another for convenient initialization and fusion, which remains suboptimal due to inherent modality discrepancies. In this work, we propose HBridge, an asymmetric H-shaped architecture that enables heterogeneous experts to optimally leverage pretrained priors from their respective modality domains. Unlike prior dense fusion strategies that straightforwardly connect all layers between experts via shared attention, HBridge selectively bridges intermediate layers, reducing over 40% attention sharing, which improves efficiency and enhances generation quality. Shallow and deep layers, which capture modality-specific representations, are decoupled, while mid-layer bridging promotes semantic alignment. To further strengthen cross-modal coherence, we introduce semantic reconstruction tokens that explicitly guide the generative expert to reconstruct visual semantic tokens of the target image. Extensive experiments across multiple benchmarks demonstrate the effectiveness and superior performance of HBridge, establishing a new paradigm for unified multimodal generation.

</details>


### [169] [Mistake Attribution: Fine-Grained Mistake Understanding in Egocentric Videos](https://arxiv.org/abs/2511.20525)
*Yayuan Li,Aadit Jain,Filippos Bellos,Jason J. Corso*

Main category: cs.CV

TL;DR: 本文提出了一种新任务Mistake Attribution (MATT)，能对第一视角视频中的人类错误进行细粒度归因，并开发了自动构建大规模高质量错误数据集的引擎和多维错误归因模型，显著提升了相关基准的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的人类错误分析方法缺乏细粒度的归因能力，无法明确指出错误具体涉及到指令的哪一部分、时间节点及空间位置，因此需要开发更精细的错误理解和归因机制。

Method: 1. 提出MATT任务，要求模型识别出指令中被违反的部分（语义维）、错误变得不可逆转的时间点（PNR）、以及该帧中错误的具体位置（空间维）。2. 开发MisEngine数据引擎，从现有大规模第一视角视频数据中自动构建包含丰富归因信息的错误样本集。3. 基于构建的数据集，提出MisFormer模型，应用多维统一注意力机制进行错误归因，并实现端到端训练。

Result: MisEngine引擎分别构建了EPIC-KITCHENS-M和Ego4D-M两个涵盖大规模丰富错误归因的数据集。MisFormer模型在语义、时间、空间三个维度的错误归因实验中，均超越了现有的视频-语言理解、时序定位、手物交互和错误检测等多种基线方法。

Conclusion: 细粒度的人类错误归因不仅可行，且大规模构建相关数据集和统一的多维坏归因模型能显著提升对人类操作失误的理解，并为相关领域提供强有力的技术支撑。

Abstract: We introduce Mistake Attribution (MATT), a task for fine-grained understanding of human mistakes in egocentric video. Unlike prior mistake understanding work, which lacks fine-grained output, MATT concretely attributes mistakes to the input instruction text or the attempt video. MATT determines what part of the instruction is violated (semantic role), when the deviation becomes irreversible (the Point-of-No-Return, PNR), and where the mistake appears in the PNR frame. We develop MisEngine, a data engine that automatically constructs attribution-rich mistake samples from existing datasets and inherits their annotations. Applied to large egocentric corpora, MisEngine yields EPIC-KITCHENS-M and Ego4D-M, two datasets that are up to two orders of magnitude larger than prior mistake datasets. We then present MisFormer, a unified attention-based model for mistake attribution across semantic (what), temporal (when), and spatial (where) dimensions, trained using MisEngine supervision. Experiments on our new datasets and prior benchmarks show that MisFormer outperforms strong video-language, temporal localization, hand-object interaction, and mistake-detection baselines.

</details>


### [170] [Automated Monitoring of Cultural Heritage Artifacts Using Semantic Segmentation](https://arxiv.org/abs/2511.20541)
*Andrea Ranieri,Giorgio Palmieri,Silvia Biasotti*

Main category: cs.CV

TL;DR: 本文比较了多种U-Net架构下不同CNN编码器用于文物裂缝检测的表现，并在OmniCrack30k数据集以及实际未标注裂缝样本上进行了定量与定性评估。结果显示模型能较好泛化到新场景，对文化遗产保护有重要意义。


<details>
  <summary>Details</summary>
Motivation: 在文化遗产保护过程中，自动化裂缝检测有助于提高效率、降低人工成本和防止人为主观误差。因此，需要寻找高效且能泛化的自动化图像分割方法用于裂缝识别。

Method: 采用U-Net结构，分别结合多种CNN编码器，对裂缝图像进行像素级语义分割。通过OmniCrack30k测试集以主流分割指标（mIoU、Dice、Jaccard）量化比较，并用实际未标注样本进行模型泛化能力的定性评估。

Result: 不同CNN编码器在OmniCrack30k数据集上表现各异，部分模型在主流指标上取得了较高分数。同时，模型在从未训练过的真实裂缝场景下表现出较好的泛化能力。

Conclusion: 基于U-Net且不同CNN编码器的架构可有效用于文化遗产裂缝检测，具备良好的泛化能力，能辅助实际文物保护工作。

Abstract: This paper addresses the critical need for automated crack detection in the preservation of cultural heritage through semantic segmentation. We present a comparative study of U-Net architectures, using various convolutional neural network (CNN) encoders, for pixel-level crack identification on statues and monuments. A comparative quantitative evaluation is performed on the test set of the OmniCrack30k dataset [1] using popular segmentation metrics including Mean Intersection over Union (mIoU), Dice coefficient, and Jaccard index. This is complemented by an out-of-distribution qualitative evaluation on an unlabeled test set of real-world cracked statues and monuments. Our findings provide valuable insights into the capabilities of different CNN- based encoders for fine-grained crack segmentation. We show that the models exhibit promising generalization capabilities to unseen cultural heritage contexts, despite never having been explicitly trained on images of statues or monuments.

</details>


### [171] [New York Smells: A Large Multimodal Dataset for Olfaction](https://arxiv.org/abs/2511.20544)
*Ege Ozguroglu,Junbang Liang,Ruoshi Liu,Mia Chiquier,Michael DeTienne,Wesley Wei Qian,Alexandra Horowitz,Andrew Owens,Carl Vondrick*

Main category: cs.CV

TL;DR: 该论文提出了一个大规模的嗅觉-图像配对数据集New York Smells，并展示了基于该数据集进行多模态学习的实验，显著推动了机器嗅觉研究。


<details>
  <summary>Details</summary>
Motivation: 动物的嗅觉对感知世界至关重要，但目前机器尚难以模拟，主要原因是缺乏自然环境下多样、丰富的嗅觉训练数据。

Method: 作者采集并构建了New York Smells数据集，包含了3,500种不同物体的7,000组嗅觉与图像配对样本，涵盖室内外各种场景，并设置了三项基准任务，包括跨模态检索、依据嗅觉识别场景/物体/材质，以及细粒度区分草类物种。

Result: 实验表明：1）视觉数据有助于跨模态嗅觉表征学习，2）所学习的嗅觉特征超过了已有的人工手工特征，在各基准任务上表现更优。

Conclusion: 该工作首次大规模提供了自然场景下的嗅觉-视觉配对数据，并验证了利用视觉辅助嗅觉表征学习的有效性，为机器嗅觉领域提供了重要资源和方法基础。

Abstract: While olfaction is central to how animals perceive the world, this rich chemical sensory modality remains largely inaccessible to machines. One key bottleneck is the lack of diverse, multimodal olfactory training data collected in natural settings. We present New York Smells, a large dataset of paired image and olfactory signals captured ``in the wild.'' Our dataset contains 7,000 smell-image pairs from 3,500 distinct objects across indoor and outdoor environments, with approximately 70$\times$ more objects than existing olfactory datasets. Our benchmark has three tasks: cross-modal smell-to-image retrieval, recognizing scenes, objects, and materials from smell alone, and fine-grained discrimination between grass species. Through experiments on our dataset, we find that visual data enables cross-modal olfactory representation learning, and that our learned olfactory representations outperform widely-used hand-crafted features.

</details>


### [172] [Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning](https://arxiv.org/abs/2511.20549)
*Guanjie Chen,Shirui Huang,Kai Liu,Jianchen Zhu,Xiaoye Qu,Peng Chen,Yu Cheng,Yifu Sun*

Main category: cs.CV

TL;DR: 本文提出了一种新的生成式扩散模型训练框架Flash-DMD，能大幅加速推理速度，同时保持图像质量，并改善基于强化学习的微调过程的稳定性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成效果出色，但采样效率低，且加速方法（如timestep distillation）常造成图像质量下降。同时，利用强化学习进行个性化微调极不稳定。论文旨在解决这两个关键难题。

Method: 提出了两项创新：（1）高效的timestep-aware蒸馏策略，极大降低训练开销并提升图像真实度。（2）联合训练机制：在继续蒸馏训练过程中，同时用RL目标微调，利用蒸馏损失作为正则，提升RL的稳定性，防止策略坍塌。

Result: Flash-DMD以仅2.1%的训练成本超越了DMD2。实验表明，对于score-based和flow matching模型，Flash-DMD可实现更快收敛，并在视觉质量、人类偏好和文本-图像一致性等方面达到最先进水平。

Conclusion: Flash-DMD为训练高效、高保真、稳定的生成模型提供了新范式，不仅大幅加速，还具备优异的生成性能和训练稳定性。

Abstract: Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only $2.1\%$ its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon.

</details>


### [173] [PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding](https://arxiv.org/abs/2511.20562)
*Haoze Zhang,Tianyu Huang,Zichen Wan,Xiaowei Jin,Hongzhi Zhang,Hui Li,Wangmeng Zuo*

Main category: cs.CV

TL;DR: PhysChoreo提出了一种新的视频生成框架，可以从单张图片生成具有物理现实感和可控性的高质量视频，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成方法虽然在视觉效果上取得很大进步，但在物理可控性和物理逼真性方面仍然不足。基于物理渲染的引导方法虽然有助于改善物理可控性，但是在建模复杂物理属性和长时间控制生成过程中仍然存在挑战。

Method: PhysChoreo方法分为两个阶段：第一阶段利用分部感知的物理属性重建，估计图片中所有物体的静态初始物理属性；第二阶段通过有时间指示和可编辑的物理模拟，合成出具有丰富动态行为和物理真实性的高质量视频。

Result: 实验结果表明，PhysChoreo生成的视频在动态行为丰富性和物理逼真性方面都优于现有顶尖方法，在多个评估指标上取得了更好成绩。

Conclusion: PhysChoreo突破了复杂物理属性建模和长期物理行为控制的难题，实现了从单张图片生成高质量、可控且物理真实的视频，对物理可控性视频生成研究具有重要意义。

Abstract: While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility. To address this, some recent studies attempted to guide the video generation with physics-based rendering. However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction. Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism. Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism, outperforming state-of-the-art methods on multiple evaluation metrics.

</details>


### [174] [A Reason-then-Describe Instruction Interpreter for Controllable Video Generation](https://arxiv.org/abs/2511.20563)
*Shengqiong Wu,Weicai Ye,Yuanxing Zhang,Jiahao Wang,Quande Liu,Xintao Wang,Pengfei Wan,Kun Gai,Hao Fei,Tat-Seng Chua*

Main category: cs.CV

TL;DR: ReaDe是一种可通用于各种视频生成模型的解释器，可以将用户模糊、简短的输入转化为精确、详细的可控描述，从而提升了视频生成的一致性、准确性和与用户意图的契合度。


<details>
  <summary>Details</summary>
Motivation: 当前Diffusion Transformers虽提升了视频质量和时序一致性，但在实际应用中，如何将用户含糊或复杂的输入和系统的控制能力精准对接仍是难题。因为用户输入往往与训练时用的详细提示不一致，导致生成结果不符合预期。

Method: 提出了ReaDe，这个模型无关的解释器，采用“先推理，再描述”的范式，先分析用户需求、识别核心要求，解决歧义，再给出详细、可操作的指令。训练分两阶段：一是用推理增强监督，生成分析痕迹和密集描述；二是用多维反馈奖励机制，驱动自然语言风格描述的稳定优化。

Result: 在单条件和多条件场景下，ReaDe明显提升了指令对齐、描述准确度，以及下游视频生成的质量。对推理要求高或未见过的输入也表现出良好泛化能力。

Conclusion: ReaDe能有效将用户意图解释为下游可用的详细指令，有助于实现用户友好且精确可控的视频生成。

Abstract: Diffusion Transformers have significantly improved video fidelity and temporal coherence, however, practical controllability remains limited. Concise, ambiguous, and compositionally complex user inputs contrast with the detailed prompts used in training, yielding an intent-output mismatch. We propose ReaDe, a universal, model-agnostic interpreter that converts raw instructions into precise, actionable specifications for downstream video generators. ReaDe follows a reason-then-describe paradigm: it first analyzes the user request to identify core requirements and resolve ambiguities, then produces detailed guidance that enables faithful, controllable generation. We train ReaDe via a two-stage optimization: (i) reasoning-augmented supervision imparts analytic parsing with stepwise traces and dense captions, and (ii) a multi-dimensional reward assigner enables stable, feedback-driven refinement for natural-style captions. Experiments across single- and multi-condition scenarios show consistent gains in instruction fidelity, caption accuracy, and downstream video quality, with strong generalization to reasoning-intensive and unseen inputs. ReaDe offers a practical route to aligning controllable video generation with accurately interpreted user intent. Project Page: https://sqwu.top/ReaDe/.

</details>


### [175] [DINO-Tok: Adapting DINO for Visual Tokenizers](https://arxiv.org/abs/2511.20565)
*Mingkai Jia,Mingxiao Li,Liaoyuan Fan,Tianxing Shi,Jiaxin Guo,Zeming Li,Xiaoyang Guo,Xiao-Xiao Long,Qian Zhang,Ping Tan,Wei Yin*

Main category: cs.CV

TL;DR: 本文提出了一种基于DINO的视觉分词器DINO-Tok，有效平衡了语义表达和重建质量，在高维潜空间内实现了信息完整的表征，并大幅提升了重建性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉分词器在潜空间表达中无法很好兼顾语义和重建质量，且常常丢失关键信息或出现码本坍缩，限制了生成模型性能。

Method: 作者提出DINO-Tok，将DINO预训练模型的浅层和深层特征融合，形成信息完备的分层潜空间。为解决高维向量量化中的失真和码本坍缩问题，引入了全局PCA重加权策略，提升信息保真。

Result: 在ImageNet 256x256实验中，DINO-Tok的重建性能（PSNR）明显优于其它分词器，并达到或接近于用大量数据训练的主流模型水平。

Conclusion: 将强大的视觉预训练表示用于分词，有助于提升生成模型的潜空间质量和重建能力，对下一代视觉生成模型具有重要意义。

Abstract: Recent advances in visual generation have highlighted the rise of Latent Generative Models (LGMs), which rely on effective visual tokenizers to bridge pixels and semantics. However, existing tokenizers are typically trained from scratch and struggle to balance semantic representation and reconstruction fidelity, particularly in high-dimensional latent spaces. In this work, we introduce DINO-Tok, a DINO-based visual tokenizer that unifies hierarchical representations into an information-complete latent space. By integrating shallow features that retain fine-grained details with deep features encoding global semantics, DINO-Tok effectively bridges pretrained representations and visual generation. We further analyze the challenges of vector quantization (VQ) in this high-dimensional space, where key information is often lost and codebook collapse occurs. We thus propose a global PCA reweighting mechanism to stabilize VQ and preserve essential information across dimensions. On ImageNet 256$\times$256, DINO-Tok achieves state-of-the-art reconstruction performance, reaching 28.54 PSNR for autoencoding and 23.98 PSNR for VQ-based modeling, significantly outperforming prior tokenizers and comparable to billion-level data trained models (such as Hunyuan and Wan). These results demonstrate that adapting powerful pretrained vision models like DINO for tokenization enables semantically aligned and high-fidelity latent representations, enabling next-generation visual generative models. Code will be publicly available at https://github.com/MKJia/DINO-Tok.

</details>


### [176] [VQ-VA World: Towards High-Quality Visual Question-Visual Answering](https://arxiv.org/abs/2511.20573)
*Chenhui Gou,Zilong Chen,Zeyu Wang,Feng Li,Deyao Zhu,Zicheng Duan,Kunchang Li,Chaorui Deng,Hongyi Yuan,Haoqi Fan,Cihang Xie,Jianfei Cai,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的视觉问题-视觉回答（VQ-VA）任务，即面对视觉问题时，用图像生成来作答，而非文本，并首次将这一能力引入开源模型。作者构建了大规模数据集和评价基准，并取得领先的实验表现。


<details>
  <summary>Details</summary>
Motivation: 近期已有专有系统能够对视觉问题生成图片回答，但开源社区尚无此类能力。为了推动相关研究，并缩小开源与专有系统间的差距，作者希望通过开源化工具链带来突破。

Method: 作者提出了VQ-VA World数据框架：通过基于代理的大规模数据采集管道，抓取约180万高质量图文交错样本用于模型训练；并推出了IntelligentBench基准用于从世界知识、设计知识、推理等方面系统性评估VQ-VA。

Result: 以VQ-VA World数据训练后，LightFusion模型在IntelligentBench上的分数显著提升至53.06，远超此前开源最佳（7.78、1.94），并明显缩小了与专有系统间的表现差距。

Conclusion: 作者开放了模型、数据集及管道，期望促进学界在视觉问题-视觉回答上的进一步研究与发展。

Abstract: This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.

</details>


### [177] [The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment](https://arxiv.org/abs/2511.20614)
*Ziheng Ouyang,Yiren Song,Yaoli Liu,Shihao Zhu,Qibin Hou,Ming-Ming Cheng,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文提出了ImageCritic方法，用于通过参考引导的后编辑方式，解决图像生成中细节不一致的问题。该方法通过新数据集、注意力对齐损失和细节编码器，实现了对生成图像细节的不一致纠正，实验效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于参考图像的定制生成任务在细粒度细节一致性上表现有限，生成图像常出现细节不准确或风格不统一的问题。作者希望通过新方法解决这些一致性缺陷。

Method: 作者构建了包含参考-退化-目标三元组的数据集，利用VLM选择和显式退化模拟真实生成失误。设计了注意力对齐损失和细节编码器，以实现模型对生成细节的准确修正。还将ImageCritic集成到agent框架，支持多轮、局部自动编辑纠正。

Result: 大量实验结果表明，ImageCritic可在不同个性化生成任务中有效修复细节问题，在细粒度一致性和图像质量上显著优于同类方法。

Conclusion: ImageCritic为生成任务带来显著的细节一致性提升，解决了定制图像生成中的细节不一致难题，并具有良好的通用性和自动纠正能力。

Abstract: Previous works have explored various customized generation tasks given a reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying a reference-guided post-editing approach and present our ImageCritic. We first construct a dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on a thorough examination of the model's attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and a detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods.

</details>


### [178] [Evaluating the Performance of Deep Learning Models in Whole-body Dynamic 3D Posture Prediction During Load-reaching Activities](https://arxiv.org/abs/2511.20615)
*Seyede Niloofar Hosseini,Ali Mojibi,Mahdi Mohseni,Navid Arjmand,Alireza Taheri*

Main category: cs.CV

TL;DR: 本文研究深度神经网络在动态搬运负重过程中，全身姿态预测软件的应用。通过BLSTM和Transformer模型对人体动作的剩余时序进行预测，并提出了新的损失函数优化预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确预测搬运等动态负重任务中的人体姿态，对提升作业安全、动作分析与机器辅助等场景意义重大。传统方法准确性有限，需探索更高效的深度学习模型。

Method: 采集20位健康男性在多种搬运/举升方式下的3D全身运动序列，用BLSTM和Transformer两种时序模型，利用动作初段25%的骨骼坐标及相关参数预测后续动作姿态。并通过对身体各段恒定长度的优化新损失函数提升预测效能。

Result: 新损失函数使手臂和腿部模型预测误差分别降低约8%与21%；Transformer模型长时预测精度显著优于BLSTM（RMSE 47.0 mm，提升58%）。

Conclusion: 捕捉3D动作时序依赖的神经网络模型在搬运动作等复杂运动预测中表现优异，并且新损失函数显著提升了预测精度。该研究为手工物料搬运等领域提供了新的动作分析和预测方法。

Abstract: This study aimed to explore the application of deep neural networks for whole-body human posture prediction during dynamic load-reaching activities. Two time-series models were trained using bidirectional long short-term memory (BLSTM) and transformer architectures. The dataset consisted of 3D full-body plug-in gait dynamic coordinates from 20 normal-weight healthy male individuals each performing 204 load-reaching tasks from different load positions while adapting various lifting and handling techniques. The model inputs consisted of the 3D position of the hand-load position, lifting (stoop, full-squat and semi-squat) and handling (one- and two-handed) techniques, body weight and height, and the 3D coordinate data of the body posture from the first 25% of the task duration. These inputs were used by the models to predict body coordinates during the remaining 75% of the task period. Moreover, a novel method was proposed to improve the accuracy of the previous and present posture prediction networks by enforcing constant body segment lengths through the optimization of a new cost function. The results indicated that the new cost function decreased the prediction error of the models by approximately 8% and 21% for the arm and leg models, respectively. We indicated that utilizing the transformer architecture, with a root-mean-square-error of 47.0 mm, exhibited ~58% more accurate long-term performance than the BLSTM-based model. This study merits the use of neural networks that capture time series dependencies in 3D motion frames, providing a unique approach for understanding and predict motion dynamics during manual material handling activities.

</details>


### [179] [ShapeGen: Towards High-Quality 3D Shape Synthesis](https://arxiv.org/abs/2511.20624)
*Yangguang Li,Xianglong He,Zi-Xin Zou,Zexiang Liu,Wanli Ouyang,Ding Liang,Yan-Pei Cao*

Main category: cs.CV

TL;DR: 本文提出了ShapeGen方法，通过改进3D表示和监督、提升分辨率，以及应用线性Transformer，实现了高质量的图像到3D形状生成，在各项指标上取得了新一代的表现。


<details>
  <summary>Details</summary>
Motivation: 目前的图像到3D生成方法仍存在细节不够丰富、表面过于平滑、薄壳结构易碎等问题，难以满足艺术家对3D资产的高要求。

Method: 本研究提出ShapeGen方法，主要通过以下三项改进：1）优化3D表示和监督方式；2）提升生成分辨率；3）利用线性Transformer提升建模能力。

Result: 通过大量实验，验证了上述改进对整体性能的显著提升。

Conclusion: ShapeGen方法有效提升了图像到3D生成的质量，使生成资产能够顺利集成进3D流程，广泛适用于各类应用，达到了当前最优水平。

Abstract: Inspired by generative paradigms in image and video, 3D shape generation has made notable progress, enabling the rapid synthesis of high-fidelity 3D assets from a single image. However, current methods still face challenges, including the lack of intricate details, overly smoothed surfaces, and fragmented thin-shell structures. These limitations leave the generated 3D assets still one step short of meeting the standards favored by artists. In this paper, we present ShapeGen, which achieves high-quality image-to-3D shape generation through 3D representation and supervision improvements, resolution scaling up, and the advantages of linear transformers. These advancements allow the generated assets to be seamlessly integrated into 3D pipelines, facilitating their widespread adoption across various applications. Through extensive experiments, we validate the impact of these improvements on overall performance. Ultimately, thanks to the synergistic effects of these enhancements, ShapeGen achieves a significant leap in image-to-3D generation, establishing a new state-of-the-art performance.

</details>


### [180] [MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models](https://arxiv.org/abs/2511.20629)
*Chieh-Yun Chen,Zhonghao Wang,Qi Chen,Zhifan Ye,Min Shi,Yue Zhao,Yinan Zhao,Hui Qu,Wei-An Lin,Yiru Shen,Ajinkya Kale,Irfan Essa,Humphrey Shi*

Main category: cs.CV

TL;DR: 本论文提出了MapReduce LoRA和Reward-aware Token Embedding (RaTE)两种新方法，有效解决了多重奖励优化时的对齐损失问题，在文本到图像、文本到视频生成及语言任务中均取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 在人类反馈强化学习（RLHF）中，生成模型的多偏好对齐通常遭遇‘对齐损失’，即优化某一评价维度会导致其他维度下降，这极大限制了生成模型的实际效果。本文旨在解决多个奖励联合优化时的冲突问题，实现更灵活和高效的多偏好对齐。

Method: 提出了MapReduce LoRA方法，通过并行训练不同偏好的LoRA专家并迭代合并，提升共享模型基础能力；同时提出RaTE方法，为不同奖励学习特定的Token Embedding，在推理阶段灵活组合实现偏好控制。

Result: 在文本到图像生成（Stable Diffusion 3.5 Medium，FLUX.1-dev）任务上，GenEval、PickScore、OCR等指标分别提升最多达55.7%和67.1%；在文本到视频生成（HunyuanVideo）任务上，视觉和运动质量提升48.1%和90.0%；在语言助手任务（Llama-2 7B）上，helpful和harmless指标提升43.4%和136.7%。

Conclusion: 所提方法有效缓解了多奖励联合优化的对齐损失问题，在多个模态和任务中取得了新SOTA，具有广泛的实用前景。

Abstract: Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.

</details>


### [181] [iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation](https://arxiv.org/abs/2511.20635)
*Zhoujie Fu,Xianfang Zeng,Jinghong Lan,Xinyao Liao,Cheng Chen,Junyi Chen,Jiacheng Wei,Wei Cheng,Shiyu Liu,Yunuo Chen,Gang Yu,Guosheng Lin*

Main category: cs.CV

TL;DR: iMontage提出了将强大的预训练视频模型转化为多功能图像生成器的新方法，从而实现了具备丰富动态和强上下文一致性的多图像生成与编辑。


<details>
  <summary>Details</summary>
Motivation: 当前预训练视频模型在时序连贯性上表现优异，但因训练数据的连续性，其动态变化受限。另一方面，图像数据具备丰富的内容多样性。作者希望结合视频模型的连贯性和图像数据的多样性，产生兼具自然过渡和广阔动态范围的图像集合。

Method: 提出iMontage框架，采用轻量且最小干预的适配策略，并结合定制化的数据筛选流程与训练范式，将先进的视频模型转化为可处理变长图像集的一体化生成编辑器，实现多种复杂生成与编辑任务。

Result: iMontage在多个主流多输入多输出任务中表现卓越，能够在保持跨图像上下文一致性的基础上，生成出超过传统方法动态范围的场景。

Conclusion: iMontage有效结合了视频模型的运动先验和丰富的图像内容操控能力，提供了一种统一、强大的多图像生成与编辑解决方案，扩展了现有模型的应用范围。

Abstract: Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.

</details>


### [182] [MotionV2V: Editing Motion in a Video](https://arxiv.org/abs/2511.20640)
*Ryan Burgert,Charles Herrmann,Forrester Cole,Michael S Ryoo,Neal Wadhwa,Andrey Voynov,Nataniel Ruiz*

Main category: cs.CV

TL;DR: 本文提出了一种基于运动轨迹编辑的视频内容修改方法，可实现对现有视频运动进行精确、可控的修改，显著提升了视频编辑的能力。


<details>
  <summary>Details</summary>
Motivation: 虽然生成式视频模型已在清晰度和一致性方面取得进展，但将其应用于视频编辑仍面临困难。此前多关注于运动可控性以提升文本转视频或图像动画，但精确运动控制用于已存在视频的编辑尚未充分研究。

Method: 提出直接编辑输入视频提取出的稀疏运动轨迹，将输入输出轨迹的偏差定义为“运动编辑”。提出以此为基础的视频生成流程，能够生成内容不变但运动不同的视频对，并用这些数据微调条件化扩散视频架构。

Result: 通过用户对比实验（四选一），该方法在用户喜好度上对比前人方法获得超过65%的偏好率，显示出了显著的编辑效果提升。

Conclusion: 稀疏运动轨迹编辑结合生成模型可以实现更强大的视频编辑能力，为精确、自然的视频运动修改提供了新思路。

Abstract: While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a "motion edit" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating "motion counterfactuals", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V

</details>


### [183] [Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition](https://arxiv.org/abs/2511.20641)
*Wei Tang,Zuo-Zheng Wang,Kun Zhang,Tong Wei,Min-Ling Zhang*

Main category: cs.CV

TL;DR: 提出了一种针对长尾多标签视觉识别问题的新方法（CAPNET），显著提升了尾部类别的识别性能，在多个基准数据集上超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 多标签长尾分布导致现有视觉识别模型在尾部类别表现不佳，且现有方法对类别间关系建模存在局限。

Method: 设计了CAPNET，利用CLIP文本编码建模标签关系，通过图卷积网络传播标签信息，引入可学习soft prompt优化嵌入，采用带类别重加权的distribution-balanced Focal loss训练模型，并结合测试时集成和高效参数微调以增强泛化能力。

Result: 在VOC-LT、COCO-LT和NUS-WIDE等多个数据集上，CAPNET在各项指标上优于现有先进方法，特别是在尾部类别，效果提升明显。

Conclusion: CAPNET有效解决了长尾多标签识别中的类别不平衡与关系建模问题，为实际应用提供了更优的方法。

Abstract: Long-tailed multi-label visual recognition poses a significant challenge, as images typically contain multiple labels with highly imbalanced class distributions, leading to biased models that favor head classes while underperforming on tail classes. Recent efforts have leveraged pre-trained vision-language models, such as CLIP, alongside long-tailed learning techniques to exploit rich visual-textual priors for improved performance. However, existing methods often derive semantic inter-class relationships directly from imbalanced datasets, resulting in unreliable correlations for tail classes due to data scarcity. Moreover, CLIP's zero-shot paradigm is optimized for single-label image-text matching, making it suboptimal for multi-label tasks. To address these issues, we propose the correlation adaptation prompt network (CAPNET), a novel end-to-end framework that explicitly models label correlations from CLIP's textual encoder. The framework incorporates a graph convolutional network for label-aware propagation and learnable soft prompts for refined embeddings. It utilizes a distribution-balanced Focal loss with class-aware re-weighting for optimized training under imbalance. Moreover, it improves generalization through test-time ensembling and realigns visual-textual modalities using parameter-efficient fine-tuning to avert overfitting on tail classes without compromising head class performance. Extensive experiments and ablation studies on benchmarks including VOC-LT, COCO-LT, and NUS-WIDE demonstrate that CAPNET achieves substantial improvements over state-of-the-art methods, validating its effectiveness for real-world long-tailed multi-label visual recognition.

</details>


### [184] [Concept-Aware Batch Sampling Improves Language-Image Pretraining](https://arxiv.org/abs/2511.20643)
*Adhiraj Ghosh,Vishaal Udandarao,Thao Nguyen,Matteo Farina,Mehdi Cherti,Jenia Jitsev,Sewoong Oh,Elisa Ricci,Ludwig Schmidt,Matthias Bethge*

Main category: cs.CV

TL;DR: 论文提出了更灵活、任务自适应的在线概念驱动数据筛选方法，用以改进视觉-语言模型训练的数据集选择。核心贡献包括大量精细概念标注的数据集DataConcept，以及基于概念的在线批采样框架CABS，显著提升了多项基准任务的模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型的数据集筛选多为离线、基于静态过滤标准且无概念区分，容易引入偏差，且缺乏对任务特定需求的适应性。为更好适配下游任务，需要一种灵活、能够动态适应目标概念分布的数据筛选方法。

Method: 首先构建了DataConcept数据集，包含1.28亿对网络爬取的图像-文本数据，并进行细粒度概念标注。在此基础上，提出了概念感知批采样方法CABS，能根据设定的概念分布实时组合训练批次。CABS有两种采样方式：CABS-DM（最大化概念多样性）、CABS-FM（最大化对象频次），以满足不同任务需求。

Result: 在28项基准任务上对CLIP、SigLIP等模型做了大量实验，表明通过CABS采样训练得到的模型表现有显著提升。

Conclusion: CABS方法为数据集筛选和批构建提供了一种开放、灵活且效果强劲的方案，不仅优于现有的静态/黑箱数据筛选流程，还允许研究者根据实际任务定制概念分布，实现更强下游表现。

Abstract: What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.

</details>


### [185] [Vision-Language Memory for Spatial Reasoning](https://arxiv.org/abs/2511.20644)
*Zuntao Liu,Yi Du,Taimeng Fu,Shaoshu Su,Cherie Ho,Chen Wang*

Main category: cs.CV

TL;DR: 本文提出了VLM$^2$模型，在仅用2D视频的情况下，实现了对空间推理任务中的一致性三维表征和持续记忆，显著提升了视觉-空间智能，达到视频模型的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在视频空间推理任务上的表现距离人类水平尚有差距，主要由于三维感知不一致（语义-几何失配），以及缺乏对三维表示和空间理解的持久性记忆。

Method: 作者提出了VLM$^2$，将持续性记忆机制引入视觉-语言模型，构建了一个双记忆模块：工作记忆（滑动窗口关注即时上下文）与情节记忆（整合、保存关键长期信息），实现了视角一致、对3D感知敏感的空间表征。所有表征和推理仅基于2D视频。

Result: 在多个空间推理基准上，VLM$^2$超越了此前所有仅基于视频的模型，达到了最新的性能水平。

Conclusion: VLM$^2$有效解决了VLM在空间推理中三维表示不一致和记忆缺失的问题，为视频空间推理任务中的视觉-空间智能带来了新进展。

Abstract: Spatial reasoning is a critical capability for intelligent robots, yet current vision-language models (VLMs) still fall short of human-level performance in video-based spatial reasoning. This gap mainly stems from two challenges: a semantic-geometric misalignment that prevents consistent 3D understanding, and the absence of persistent memory to retain 3D representation and understanding over time. To address these limitations, we present VLM$^2$, a Vision-Language Model with persistent Memory for spatial reasoning with a view-consistent, 3D-aware representation purely from 2D video. Specifically, to enhance long-horizon reasoning, we incorporate a dual-memory module, consisting of a working memory that operates as a sliding window to focus on immediate context, and an episodic memory that consolidates and stores critical long-term information. This design enables efficient and long-horizon spatial reasoning with a fixed computational cost. Extensive experiments on multiple benchmarks show that VLM$^2$ achieves state-of-the-art performance among video-only models, significantly advancing the frontier of visual-spatial intelligence.

</details>


### [186] [PixelDiT: Pixel Diffusion Transformers for Image Generation](https://arxiv.org/abs/2511.20645)
*Yongsheng Yu,Wei Xiong,Weili Nie,Yichen Sheng,Shiqiu Liu,Jiebo Luo*

Main category: cs.CV

TL;DR: PixelDiT提出在像素空间直接进行扩散建模，摒弃自编码器，实现端到端的单阶段高质量生成。


<details>
  <summary>Details</summary>
Motivation: 现有Diffusion Transformers（DiTs）通常依赖自编码器在潜空间建模，但自编码器会带来损失重建和误差累积，且难以端到端优化。作者希望消除这些不足，实现更高质量的图像生成。

Method: 提出单阶段架构PixelDiT，直接在像素空间建模扩散过程，采用全transformer结构。具体采用双层设计：patch-level捕捉全局语义，pixel-level细化纹理，从而兼顾效率和细节。

Result: PixelDiT在ImageNet 256x256上取得1.61 FID，显著优于现有像素生成模型。还可扩展到文本到图像生成，并在1024x1024预训练下获得优异评测分数，接近最优潜空间扩散模型。

Conclusion: PixelDiT证明了像素空间直接建模可行且高效，杜绝二阶段带来的损失和误差，有望推动高分辨率、高质量生成模型发展。

Abstract: Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.

</details>


### [187] [3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding](https://arxiv.org/abs/2511.20646)
*Xiaoye Wang,Chen Tang,Xiangyu Yue,Wei-Hong Li*

Main category: cs.CV

TL;DR: 该论文提出通过引入跨视角相关性（即cost volume）在多任务学习（MTL）网络中增强3D空间感知，提升如分割和深度估计等多密度预测任务的联合性能。


<details>
  <summary>Details</summary>
Motivation: 当前多任务学习方法主要在2D图像空间捕捉任务间关系，导致特征缺乏3D空间结构，对于需要全面场景理解的任务（如分割、深度估计等）存在局限。本论文认为3D空间感知对建模任务间相关性至关重要。

Method: 作者提出了一种轻量级的跨视图模块（Cross-view Module，CvM），可在不同任务间共享，通过跨视图的信息交换和相关性建模，将几何一致性融入多任务预测过程中。该模块可与现有多任务编码器集成，适用于单视图和多视图数据。

Result: 在NYUv2和PASCAL-Context数据集上，大量实验结果表明，该方法能有效提升现有多任务学习方法的性能，尤其在增强几何一致性方面表现突出。

Conclusion: 将跨视图几何一致性注入多任务学习框架，有助于提升多任务预测的精度和场景理解能力，所提模块简单、高效，并具有良好的兼容性。

Abstract: This paper addresses the challenge of training a single network to jointly perform multiple dense prediction tasks, such as segmentation and depth estimation, i.e., multi-task learning (MTL). Current approaches mainly capture cross-task relations in the 2D image space, often leading to unstructured features lacking 3D-awareness. We argue that 3D-awareness is vital for modeling cross-task correlations essential for comprehensive scene understanding. We propose to address this problem by integrating correlations across views, i.e., cost volume, as geometric consistency in the MTL network. Specifically, we introduce a lightweight Cross-view Module (CvM), shared across tasks, to exchange information across views and capture cross-view correlations, integrated with a feature from MTL encoder for multi-task predictions. This module is architecture-agnostic and can be applied to both single and multi-view data. Extensive results on NYUv2 and PASCAL-Context demonstrate that our method effectively injects geometric consistency into existing MTL methods to improve performance.

</details>


### [188] [Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization](https://arxiv.org/abs/2511.20647)
*Tahira Kazimi,Connor Dunlop,Pinar Yanardag*

Main category: cs.CV

TL;DR: 本文提出DPP-GRPO框架，有效提高从单一文本生成多视频时的内容多样性，兼顾生成质量和文本契合度。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成视频（T2V）模型虽然生成质量高，但在多次采样时，输出视频多样性较低，无法充分展现同一文本提示下的多种可能。这限制了T2V模型在创意表达和实际应用中的广度。

Method: 将多样性提升问题建模为集合级策略优化任务，并创新性地结合了Determinanatal Point Processes（DPPs）和Group Relative Policy Optimization（GRPO），通过集合反馈和对冗余样本施加递减回报信号，显式鼓励生成多样化视频。该框架可以无缝集成到不同的T2V模型（例如WAN、CogVideoX）当中。

Result: 在VBench、VideoScore等主流基准及人工偏好测试中，DPP-GRPO能显著提升视频生成的多样性，同时维持高质量与文本一致性。此外，作者还开放了代码和一个包含3万个多样化提示的新基准数据集。

Conclusion: DPP-GRPO框架为提升文本生成视频的输出多样性提供了有效且通用的解决方案，为未来相关研究奠定了数据和方法基础。

Abstract: While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.

</details>


### [189] [LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight](https://arxiv.org/abs/2511.20648)
*Yunze Man,Shihao Wang,Guowen Zhang,Johan Bjorck,Zhiqi Li,Liang-Yan Gui,Jim Fan,Jan Kautz,Yu-Xiong Wang,Zhiding Yu*

Main category: cs.CV

TL;DR: 该论文提出了一个名为LocateAnything3D的方法，将3D检测任务转化为VLM原生的下一个token预测问题，并在Omni3D数据集上取得了显著领先的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLM）在2D描述和定位表现优异，但多目标3D检测能力不足。该研究旨在弥补VLM在3D检测方面的短板，使模型不仅能识别物体，还能准确理解其3D空间位置。

Method: 作者提出LocateAnything3D方法，将3D检测表述为下一token预测。该方法借鉴人类从图像推理物体位置的思路，首先通过视觉链式思维输出2D检测，再分阶段推理物体距离、尺寸和姿态信息。流程遵循易到难原则，先识别近处再到远处物体，并按信息稳定性与可学习性依次预测物体中心、尺寸和旋转。该方法无需专用网络头，维持了开放词表和视觉提示能力。

Result: 该方法在Omni3D基准测试中取得了49.89的AP_3D成绩，比之前最优结果提升了15.51个百分点，即使在baseline具有真实2D框的情况下仍有显著优势。另外，在未见类别上也表现出优秀的零样本泛化能力和强鲁棒性。

Conclusion: LocateAnything3D成功将3D检测转化为结构化的下一个token预测问题，并在VLM框架下实现了性能突破，为3D感知提供了实用基础。

Abstract: To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 49.89 AP_3D, surpassing the previous best by +15.51 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.

</details>


### [190] [Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout](https://arxiv.org/abs/2511.20649)
*Hidir Yesiltepe,Tuna Han Salih Meral,Adil Kaan Akan,Kaan Oktay,Pinar Yanardag*

Main category: cs.CV

TL;DR: 本文提出了一个新的视频生成推理框架∞-RoPE，突破了自回归视频扩散模型的三大瓶颈，支持无限时域、精细动作控制和不连续镜头切换。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视频扩散模型在时间长度、动作响应速度、镜头切换等方面存在局限，不能满足长视频与复杂场景生成的需求，需要新的技术突破。

Method: 提出了∞-RoPE框架，包括三部分：1）Block-Relativistic RoPE 采用相对时序参考帧，消除固定的时间位置限制，实现超长视频生成；2）KV Flush 只保留最新和全局关键信息，实现高效、快速的动作响应；3）RoPE Cut 则允许在同一次生成中进行受控的不连续转场，支持多镜头无缝生成。这一框架无需重新训练，在推理阶段即可应用。

Result: 实验表明，∞-RoPE架构在VBench等评测评分上，整体性能优于以往自回归模型，在时长、控制、镜头切换等方面的能力均有所提升。

Conclusion: ∞-RoPE为无限时域、可精细控制且支持电影式多场景切换的视频生成打开了新的方向，是无需重训的强大通用基础，展现了很高的实际应用潜力。

Abstract: Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.

</details>


### [191] [MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities](https://arxiv.org/abs/2511.20650)
*Tooba Tehreem Sheikh,Jean Lahoud,Rao Muhammad Anwer,Fahad Shahbaz Khan,Salman Khan,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 该论文提出了MedROV，这是首个面向医学影像的实时开放词汇目标检测模型。通过构建大规模多模态数据集Omnis，以及伪标注和对比学习等技术，MedROV可检测已知及新颖结构，在准确率与速度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统医学影像目标检测受限于封闭类别，难以识别新型或未知标签的目标。现有开放词汇目标检测方法在医学领域应用受限于数据集匮乏和文本-图像对齐弱，因此急需突破。

Method: 1. 构建拥有60万个检测样本、涵盖九种成像模态的大规模医学数据集Omnis；2. 针对多源数据中标注缺失，提出伪标注策略；3. 利用大预训练基础模型进行知识迁移；4. 结合对比学习和跨模态表示以提升检测能力。

Result: MedROV在医学开放词汇检测中，在mAP50上比此前的基础模型平均提升40分，优于封闭集检测器超过3分，并且保持70 FPS的实时检测速度，创造了新基准。

Conclusion: MedROV实现了医学影像开放词汇目标检测的新突破，可识别更多新型结构，具有优异性能，是该领域的创新方案，相关数据与代码已开源。

Abstract: Traditional object detection models in medical imaging operate within a closed-set paradigm, limiting their ability to detect objects of novel labels. Open-vocabulary object detection (OVOD) addresses this limitation but remains underexplored in medical imaging due to dataset scarcity and weak text-image alignment. To bridge this gap, we introduce MedROV, the first Real-time Open Vocabulary detection model for medical imaging. To enable open-vocabulary learning, we curate a large-scale dataset, Omnis, with 600K detection samples across nine imaging modalities and introduce a pseudo-labeling strategy to handle missing annotations from multi-source datasets. Additionally, we enhance generalization by incorporating knowledge from a large pre-trained foundation model. By leveraging contrastive learning and cross-modal representations, MedROV effectively detects both known and novel structures. Experimental results demonstrate that MedROV outperforms the previous state-of-the-art foundation model for medical image detection with an average absolute improvement of 40 mAP50, and surpasses closed-set detectors by more than 3 mAP50, while running at 70 FPS, setting a new benchmark in medical detection. Our source code, dataset, and trained model are available at https://github.com/toobatehreem/MedROV.

</details>


### [192] [RubricRL: Simple Generalizable Rewards for Text-to-Image Generation](https://arxiv.org/abs/2511.20651)
*Xuelu Feng,Yunsheng Li,Ziyu Wan,Zixuan Gao,Junsong Yuan,Dongdong Chen,Chunming Qiao*

Main category: cs.CV

TL;DR: 本文提出了RubricRL，一种以量表为基础的奖赏设计框架，使文本生成图像模型的奖励更具可解释性和灵活性。通过细分视觉标准，并利用多模态评判模型对每个标准独立打分，从而提升生成图像与用户需求的对齐程度。实验证明该方法有效提升了模型出图的忠实度、细节和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有RL奖励多为黑盒式的单一分数或固定权重的复合指标，难以解释且不便调整，用户无法根据自身需求灵活设定关注重点。因此需要一种更可解释、可拆解且支持用户直接控制奖励策略的方法。

Method: RubricRL为每个输入文本动态生成可分解的量表（rubric），包括诸如物体正确性、属性准确性、OCR清晰度和真实感等细致标准。每一个细节都由多模态判别器独立评估，并通过与输入相关的权重机制突出主要维度。用户可以灵活调整哪些方面应重点奖励或惩罚。

Result: RubricRL在自回归文本到图像生成模型上的实验显示，能显著提升模型对提示的忠实度、视觉细节和泛化能力，并允许高自由度的用户干预和解释。

Conclusion: RubricRL为RL对齐任务提供了结构化、可解释的奖励信号，比传统黑盒方式更透明灵活，对未来文本生成图像模型的可控对齐具有广泛应用潜力。

Abstract: Reinforcement learning (RL) has recently emerged as a promising approach for aligning text-to-image generative models with human preferences. A key challenge, however, lies in designing effective and interpretable rewards. Existing methods often rely on either composite metrics (e.g., CLIP, OCR, and realism scores) with fixed weights or a single scalar reward distilled from human preference models, which can limit interpretability and flexibility. We propose RubricRL, a simple and general framework for rubric-based reward design that offers greater interpretability, composability, and user control. Instead of using a black-box scalar signal, RubricRL dynamically constructs a structured rubric for each prompt--a decomposable checklist of fine-grained visual criteria such as object correctness, attribute accuracy, OCR fidelity, and realism--tailored to the input text. Each criterion is independently evaluated by a multimodal judge (e.g., o4-mini), and a prompt-adaptive weighting mechanism emphasizes the most relevant dimensions. This design not only produces interpretable and modular supervision signals for policy optimization (e.g., GRPO or PPO), but also enables users to directly adjust which aspects to reward or penalize. Experiments with an autoregressive text-to-image model demonstrate that RubricRL improves prompt faithfulness, visual detail, and generalizability, while offering a flexible and extensible foundation for interpretable RL alignment across text-to-image architectures.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [193] [Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search](https://arxiv.org/abs/2511.19648)
*Manil Shrestha,Edward Kim*

Main category: cs.CL

TL;DR: 本文提出了两种高效且可验证的结合大模型与嵌入搜索的方法，实现了知识图谱上的多跳问答，兼顾了推理效率和答案可验证性。


<details>
  <summary>Details</summary>
Motivation: 当前多跳问答面临路径组合爆炸与大模型推理代价高昂的问题，同时生成的答案往往缺乏对结构化知识的可验证性。

Method: 1）LLM引导规划：使用一次大模型推理指导关系序列预测，并通过广度优先遍历知识图谱实现可验证推理。2）嵌入引导神经搜索：完全不用大模型推理，将文本和图谱嵌入融合，用轻量级神经网络进行高效边评分。并通过知识蒸馏将复杂推理能力压缩到更小的模型中。

Result: LLM引导的方法实现了micro-F1超过0.90且全部答案均可溯源，纯嵌入方法在保持高准确率的同时推理速度提升百倍。小模型经蒸馏后表现与大模型一致且无API成本。在MetaQA数据集上，结构化推理方法始终优于直接生成。

Conclusion: 多跳问答无需依赖大规模模型，只要有合适的结构归纳偏置与表征结合，即可实现高效且可验证的知识推理。

Abstract: Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deployment. Additionally, LLM-generated answers often lack verifiable grounding in structured knowledge. We present two complementary hybrid algorithms that address both efficiency and verifiability: (1) LLM-Guided Planning that uses a single LLM call to predict relation sequences executed via breadth-first search, achieving near-perfect accuracy (micro-F1 > 0.90) while ensuring all answers are grounded in the knowledge graph, and (2) Embedding-Guided Neural Search that eliminates LLM calls entirely by fusing text and graph embeddings through a lightweight 6.7M-parameter edge scorer, achieving over 100 times speedup with competitive accuracy. Through knowledge distillation, we compress planning capability into a 4B-parameter model that matches large-model performance at zero API cost. Evaluation on MetaQA demonstrates that grounded reasoning consistently outperforms ungrounded generation, with structured planning proving more transferable than direct answer generation. Our results show that verifiable multi-hop reasoning does not require massive models at inference time, but rather the right architectural inductive biases combining symbolic structure with learned representations.

</details>


### [194] [Can LLMs Faithfully Explain Themselves in Low-Resource Languages? A Case Study on Emotion Detection in Persian](https://arxiv.org/abs/2511.19719)
*Mobina Mehrazar,Mohammad Amin Yousefi,Parisa Abolfath Beygi,Behnam Bahrak*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLM）在波斯语情感分类任务中生成自解释的可信度，发现目前的解释方法存在偏离人类判断的问题。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM能够生成预测及其相应解释，但尤其在低资源语言中，这些解释的可信度存疑。本研究旨在评估LLM在波斯语情感分类任务中生成解释的可信度。

Method: 通过对比LLM判定的关键影响词与人类注释者标注的词，采用基于token级log概率的置信度分数来评估解释可信度。同时设计了两种不同的提问顺序（先预测后解释与先解释后预测）以考察对解释可信度的影响。

Result: LLM在情感分类表现良好，但其解释与人类判断常有较大偏离，并且不同提示方案下模型解释之间的一致性高于与人类的一致性。

Conclusion: 现有解释方法和度量在多语言和低资源语境下有明显局限性，需要发展更健壮的解释方法以提升LLM的可靠性。

Abstract: Large language models (LLMs) are increasingly used to generate self-explanations alongside their predictions, a practice that raises concerns about the faithfulness of these explanations, especially in low-resource languages. This study evaluates the faithfulness of LLM-generated explanations in the context of emotion classification in Persian, a low-resource language, by comparing the influential words identified by the model against those identified by human annotators. We assess faithfulness using confidence scores derived from token-level log-probabilities. Two prompting strategies, differing in the order of explanation and prediction (Predict-then-Explain and Explain-then-Predict), are tested for their impact on explanation faithfulness. Our results reveal that while LLMs achieve strong classification performance, their generated explanations often diverge from faithful reasoning, showing greater agreement with each other than with human judgments. These results highlight the limitations of current explanation methods and metrics, emphasizing the need for more robust approaches to ensure LLM reliability in multilingual and low-resource contexts.

</details>


### [195] [Comparative Analysis of LoRA-Adapted Embedding Models for Clinical Cardiology Text Representation](https://arxiv.org/abs/2511.19739)
*Richard J. Young,Alice M. Matthews*

Main category: cs.CL

TL;DR: 本文系统评估了十种基于transformer的心脏病领域文本嵌入模型，发现较小的encoder-only架构（如BioLinkBERT）在性能和效率上都优于更大的decoder-based模型。


<details>
  <summary>Details</summary>
Motivation: 临床NLP中，领域专属文本嵌入至关重要，但当前不同模型架构的系统性对比较少，因此作者希望评估并对比主流架构在心脏病领域的表现。

Method: 研究通过Low-Rank Adaptation (LoRA) 微调了十种transformer嵌入模型（包括encoder-only和decoder-based架构），使用106535对已标注的权威心脏病学教科书文本片段进行训练和评估，并公开了模型及数据。

Result: BioLinkBERT等encoder-only架构在分离度（separation score 0.510）等指标上表现最优，且所需计算资源明显低于大型decoder-based模型。

Conclusion: 较小的encoder-only模型同样可以实现甚至超越大型模型的领域特定表现，打破了“大模型必优”假设，为临床NLP选型和系统开发提供了重要参考。

Abstract: Domain-specific text embeddings are critical for clinical natural language processing, yet systematic comparisons across model architectures remain limited. This study evaluates ten transformer-based embedding models adapted for cardiology through Low-Rank Adaptation (LoRA) fine-tuning on 106,535 cardiology text pairs derived from authoritative medical textbooks. Results demonstrate that encoder-only architectures, particularly BioLinkBERT, achieve superior domain-specific performance (separation score: 0.510) compared to larger decoder-based models, while requiring significantly fewer computational resources. The findings challenge the assumption that larger language models necessarily produce better domain-specific embeddings and provide practical guidance for clinical NLP system development. All models, training code, and evaluation datasets are publicly available to support reproducible research in medical informatics.

</details>


### [196] [What does it mean to understand language?](https://arxiv.org/abs/2511.19757)
*Colton Casto,Anna Ivanova,Evelina Fedorenko,Nancy Kanwisher*

Main category: cs.CL

TL;DR: 本文提出理解语言不仅仅是获取表层意义，还需要大脑语言系统与其它脑区协作，整合感知、运动、知识等信息，以形成丰富的心理模型。文章回顾了相关证据，并指出新的认知神经科学方法可以直接验证该假说。


<details>
  <summary>Details</summary>
Motivation: 当前对语言理解的研究主要聚焦于语言系统本身，但如何实现深度理解、生成全面心理模型仍不清楚。该论文旨在探究语言系统与大脑其他区域的协同机制，进一步阐释语言理解的本质。

Method: 作者综合回顾了认知神经科学的最新研究与实验证据，分析语言系统与其它脑区（如感知、运动及记忆系统）之间的信息流动与协作作用。

Result: 已有证据支持语言系统功能有限，深度理解需借助其它脑区。新的脑成像与认知实验方法为阐释这种协作提供了工具。

Conclusion: 深刻理解语言依赖于跨脑区的信息整合，未来可用新的实验范式和脑成像技术进一步揭示语言理解的神经机制。

Abstract: Language understanding entails not just extracting the surface-level meaning of the linguistic input, but constructing rich mental models of the situation it describes. Here we propose that because processing within the brain's core language system is fundamentally limited, deeply understanding language requires exporting information from the language system to other brain regions that compute perceptual and motor representations, construct mental models, and store our world knowledge and autobiographical memories. We review the existing evidence for this hypothesis, and argue that recent progress in cognitive neuroscience provides both the conceptual foundation and the methods to directly test it, thus opening up a new strategy to reveal what it means, cognitively and neurally, to understand language.

</details>


### [197] [Gender Bias in Emotion Recognition by Large Language Models](https://arxiv.org/abs/2511.19785)
*Maureen Herbert,Katie Sun,Angelica Lim,Yasaman Etesam*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型在情绪心智理论任务中是否表现出性别偏见，并评估了多种去偏方法。结果表明，仅依赖于推理阶段的提示工程难以有效降低偏见，需要通过训练阶段的干预来实现显著的去偏效果。


<details>
  <summary>Details</summary>
Motivation: 伴随大语言模型的发展和广泛应用，其公平性问题日益突出。作者关注于LLM在模拟情绪理解（情绪心智理论）时的性别偏见，旨在评估并缓解这类模型的偏见风险。

Method: 作者设计了情境任务，即描述人物及其环境，要求LLM回答该人物的情感感受，从而检测模型对性别的偏见。同时，作者提出和测试了多种去偏方法，包括提示工程和训练干预。

Result: 实验发现，常见的推理阶段去偏方法（如prompt engineering）效果有限，只有通过训练阶段的干预，才能实现对模型偏见的显著降低。

Conclusion: 单纯的推理阶段去偏策略难以解决LLM的性别偏见问题。要实现模型在情绪心智理论中的公平性改进，需要在模型训练阶段施加去偏措施。

Abstract: The rapid advancement of large language models (LLMs) and their growing integration into daily life underscore the importance of evaluating and ensuring their fairness. In this work, we examine fairness within the domain of emotional theory of mind, investigating whether LLMs exhibit gender biases when presented with a description of a person and their environment and asked, "How does this person feel?". Furthermore, we propose and evaluate several debiasing strategies, demonstrating that achieving meaningful reductions in bias requires training based interventions rather than relying solely on inference-time prompt-based approaches such as prompt engineering.

</details>


### [198] [Breaking Bad: Norms for Valence, Arousal, and Dominance for over 10k English Multiword Expressions](https://arxiv.org/abs/2511.19816)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: 本论文发布了NRC VAD Lexicon v2，扩展情感词典，包含大量多词表达及新词的人类情感评级。


<details>
  <summary>Details</summary>
Motivation: 当前主流情感词典覆盖面有限，尤其是新兴词汇和多词短语。丰富和更新词典可促进NLP等多领域研究。

Method: 扩展NRC VAD词典，涵盖1万个多词表达和2.5万个单词，均有人类对愉悦度、唤醒度和支配度的主观标注，并检验其可靠性。

Result: 新词典极大提升了对多词表达和新单词的覆盖，其情感相关性可靠。通过词典分析多词表达的情感强度和情感合成性。

Conclusion: NRC VAD Lexicon v2工具丰富，促进NLP、心理学、公共健康等多个学科的情感计算和研究。词典可免费获取。

Abstract: Factor analysis studies have shown that the primary dimensions of word meaning are Valence (V), Arousal (A), and Dominance (D). Existing lexicons such as the NRC VAD Lexicon, published in 2018, include VAD association ratings for words. Here, we present a complement to it, which has human ratings of valence, arousal, and dominance for 10k English Multiword Expressions (MWEs) and their constituent words. We also increase the coverage of unigrams, especially words that have become more common since 2018. In all, the new NRC VAD Lexicon v2 now has entries for 10k MWEs and 25k words, in addition to the entries in v1. We show that the associations are highly reliable. We use the lexicon to examine emotional characteristics of MWEs, including: 1. The degree to which MWEs (idioms, noun compounds, and verb particle constructions) exhibit strong emotionality; 2. The degree of emotional compositionality in MWEs. The lexicon enables a wide variety of research in NLP, Psychology, Public Health, Digital Humanities, and Social Sciences. The NRC VAD Lexicon v2 is freely available through the project webpage: http://saifmohammad.com/WebPages/nrc-vad.html

</details>


### [199] [Language-Independent Sentiment Labelling with Distant Supervision: A Case Study for English, Sepedi and Setswana](https://arxiv.org/abs/2511.19818)
*Koena Ronny Mabokela,Tim Schlippe,Mpho Raborife,Turgay Celik*

Main category: cs.CL

TL;DR: 本文提出了一种无需人工、可应用于多语言（尤其是低资源语言）的自动情感标注方法，并在南非多语言语料上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前情感分析多聚焦于高资源语言（如英语），而大量非洲语言因缺乏情感标注语料而被归为低资源语言，手动标注数据耗时且昂贵，因此亟需自动化标注以提高效率。

Method: 提出了一种基于带有情感意义的emoji和单词的自动、多语言（语言无关）情感标签方法。该方法无需人工，利用社交媒体文本（如推文）中的emoji与情感词匹配进行标注。

Result: 在SAfriSenti多语言语料（包括英语、Sepedi、Setswana推文）上测试，自动标注准确率：英语66%，Sepedi 69%，Setswana 63%，平均仅有34%的标签需人工修正。

Conclusion: 该方法能有效减少人工标注负担，对于低资源语言情感分析的数据集建设具有较高的应用价值。

Abstract: Sentiment analysis is a helpful task to automatically analyse opinions and emotions on various topics in areas such as AI for Social Good, AI in Education or marketing. While many of the sentiment analysis systems are developed for English, many African languages are classified as low-resource languages due to the lack of digital language resources like text labelled with corresponding sentiment classes. One reason for that is that manually labelling text data is time-consuming and expensive. Consequently, automatic and rapid processes are needed to reduce the manual effort as much as possible making the labelling process as efficient as possible. In this paper, we present and analyze an automatic language-independent sentiment labelling method that leverages information from sentiment-bearing emojis and words. Our experiments are conducted with tweets in the languages English, Sepedi and Setswana from SAfriSenti, a multilingual sentiment corpus for South African languages. We show that our sentiment labelling approach is able to label the English tweets with an accuracy of 66%, the Sepedi tweets with 69%, and the Setswana tweets with 63%, so that on average only 34% of the automatically generated labels remain to be corrected.

</details>


### [200] [Profile-LLM: Dynamic Profile Optimization for Realistic Personality Expression in LLMs](https://arxiv.org/abs/2511.19852)
*Shi-Wei Dai,Yan-Wei Shie,Tsung-Huan Yang,Lun-Wei Ku,Yung-Hui Li*

Main category: cs.CL

TL;DR: 本论文提出PersonaPulse框架，通过优化提示词增强大模型的个性化表达，使生成的AI更具真实和情境感，效果优于以往基于心理学描述的提示，并探究了模型规模与人格建模之间的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管以往通过提示词引导大模型表现个性，但这些提示词未针对个性表达进行优化，导致AI个性呈现不够真实或丰富。因此，作者试图提升个性化交互体验，提出更加有效的个性提示词优化方法。

Method: 提出PersonaPulse框架，利用大模型自身对人格特质的知识，迭代优化角色扮演提示词；引入情境反应基准作为评分工具，实时评估提示词效果并指导优化过程。通过量化实验对比优化前后提示词能力，并分析不同模型规模与个性表达效果的关系。

Result: 优化后生成的提示词在个性表达的真实性和情境适应性方面显著优于以心理学描述为基础的传统提示词。实验还发现，模型规模与个性化表现有关，并且可以通过暂停优化过程来部分控制个性化表达的强度。

Conclusion: 提示词优化在塑造大模型个性表达中起到关键作用，PersonaPulse框架为未来适应性AI交互研究提供了新的参考和重要启示。

Abstract: Personalized Large Language Models (LLMs) have been shown to be an effective way to create more engaging and enjoyable user-AI interactions. While previous studies have explored using prompts to elicit specific personality traits in LLMs, they have not optimized these prompts to maximize personality expression. To address this limitation, we propose PersonaPulse: Dynamic Profile Optimization for Realistic Personality Expression in LLMs, a framework that leverages LLMs' inherent knowledge of personality traits to iteratively enhance role-play prompts while integrating a situational response benchmark as a scoring tool, ensuring a more realistic and contextually grounded evaluation to guide the optimization process. Quantitative evaluations demonstrate that the prompts generated by PersonaPulse outperform those of prior work, which were designed based on personality descriptions from psychological studies. Additionally, we explore the relationship between model size and personality modeling through extensive experiments. Finally, we find that, for certain personality traits, the extent of personality evocation can be partially controlled by pausing the optimization process. These findings underscore the importance of prompt optimization in shaping personality expression within LLMs, offering valuable insights for future research on adaptive AI interactions.

</details>


### [201] [A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction](https://arxiv.org/abs/2511.19858)
*Farzad Ahmed,Joniel Augustine Jerome,Meliha Yetisgen,Özlem Uzuner*

Main category: cs.CL

TL;DR: 本研究评估了三种大语言模型（LLM）提示策略在医学文档错误检测与更正中的效果，发现基于检索的动态提示法（RDP）在多个模型和任务上表现最好。


<details>
  <summary>Details</summary>
Motivation: 医疗文档中常见事实、诊断和管理错误，影响患者安全。当前大语言模型虽然有潜力辅助检测和更正这些错误，但其在不同提示策略下的行为和有效性尚不明确，有必要系统评估。

Method: 研究基于MEDEC数据集，测试了GPT、Claude、Gemini和OpenAI o系列9个指令微调LLM，比较零样本、静态随机示例（SPR）和检索增强动态提示（RDP）三种提示策略在错误标记、句子检测和纠正三个任务中的表现，采用准确率、召回率、假阳性率以及ROUGE-1、BLEURT、BERTScore等综合指标评估。

Result: 零样本提示在检测任务中召回率较低，对缩写或不典型错误识别不佳；SPR虽提高召回率但假阳性上升。所有模型上，RDP将假阳性率降低约15%，句子检测召回率提升5-10%，且纠正结果更具上下文相关性。

Conclusion: RDP动态提示优于零样本和SPR提示，能提升医学文档错误检测准确性、降低假阳性，并增强错误纠正的可靠性，适用于多种主流LLM。

Abstract: Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction.
  Methods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning.
  Results: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections.
  Conclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.

</details>


### [202] [AppSelectBench: Application-Level Tool Selection Benchmark](https://arxiv.org/abs/2511.19957)
*Tianyi Chen,Michael Solodko,Sen Wang,Jongwoo Ko,Junheng Hao,Colby Banbury,Sara Abdali,Saeed Amizadeh,Qing Xiao,Yinheng Li,Tianyu Ding,Kamran Ghasedi Dizaji,Suzhen Zheng,Hao Fan,Justin Wagle,Pashmina Cameron,Kazuhito Koishida*

Main category: cs.CL

TL;DR: 本文提出了AppSelectBench，这是一个专为评估计算代理（CUAs）在应用程序选择方面能力而设计的新基准。该基准包含大规模、多样化且具备真实语义的用户任务，覆盖100个常用桌面应用和超过10万个任务，可系统性评估大模型在应用层级自动推理的表现。


<details>
  <summary>Details</summary>
Motivation: 当前CUAs越来越依赖调用外部工具和应用程序，但现有评测主要关注细粒度API选择，忽视了应用程序层面的选择和跨应用推理能力。鉴于此，作者希望通过新基准推动应用层级推理能力的发展。

Method: 作者提出了AppSelectBench基准，包括一套可大规模生成多样化、语义扎实的用户任务的生成流程，以及统一的评估协议（如随机、启发式、零样本、少样本、检索增强等设置），覆盖100个桌面应用和10万多个任务。

Result: 通过在多种主流开源和闭源大语言模型上的实验，作者系统性揭示了模型间在应用间推理上的优劣。结果表明，即便是最强的模型，应用选择依然存在不一致和困难。

Conclusion: AppSelectBench为研究CUA在应用层级推理和选择的能力提供了重要测试平台，有助于推动智能代理更好完成复杂现实任务，也是该领域亟需关注的能力。

Abstract: Computer Using Agents (CUAs) are increasingly equipped with external tools, enabling them to perform complex and realistic tasks. For CUAs to operate effectively, application selection, which refers to deciding which application to use before invoking fine-grained tools such as APIs, is a fundamental capability. It determines whether the agent initializes the correct environment, avoids orchestration confusion, and efficiently focuses on relevant context. However, existing benchmarks primarily assess fine-grained API selection, offering limited insight into whether models can reason across and choose between different applications. To fill this gap, we introduce AppSelectBench, a comprehensive benchmark for evaluating application selection in CUAs. AppSelectBench contains a novel user task generation pipeline that produces realistic, diverse, and semantically grounded user intents at scale, together with unified evaluation protocols covering random, heuristic, zero-shot, few-shot, and retrieval-augmented-settings. AppSelectBench covers one hundred widely used desktop applications and includes more than one hundred thousand realistic, diverse, and semantically grounded user tasks. Extensive experiments across both closed-source and open-source large language models reveal systematic strengths and weaknesses in inter-application reasoning, showing that even the most capable models still struggle to make consistent application choices. Together, these results establish AppSelectBench as a foundation for studying and advancing application level reasoning, an essential yet underexplored capability of intelligent CUAs. The source is available at https://github.com/microsoft/appselectbench.

</details>


### [203] [$\text{R}^2\text{R}$: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers](https://arxiv.org/abs/2511.19987)
*Xinyu Wang,Hanwei Wu,Qingchen Hu,Zhenghan Tai,Jingrui Tian,Lei Ding,Jijun Chi,Hailin He,Tung Sum Thomas Kwok,Yufei Cui,Sicheng Lyu,Muzhi Li,Mingze Li,Xinyue Yu,Ling Zhou,Peng Lu*

Main category: cs.CL

TL;DR: 该论文提出了一种名为R2R的新型多领域适应重排序框架,能提升涉及法务、金融等高风险场景下的检索增强生成质量,实现跨领域鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 当前的decoder-only重排序器在高风险领域（如金融和法律）难以捕捉领域细粒度特征，而通用模型难以做到有效迁移。直接对其微调容易导致对表面特征的过拟合和“灾难性遗忘”问题，影响模型泛化能力。

Method: 提出R2R框架，结合了动态专家路由和两阶段训练策略（EAG）。EAG通过掩盖最具预测性的表面实体信息，促使模型学习领域无关的相关性模式。R2R利用轻量级的潜在语义路由器，从冻结的decoder主干中提取表征，为每个查询动态选择最优LoRA专家模型。

Result: 在不同重排序器主干及法务、医疗、金融等多种领域的大量实验中，R2R在结果上优于通用基线和单领域微调模型，表现出更好的跨领域鲁棒性。

Conclusion: R2R作为一种不依赖于特定模型架构、模块化可插拔的多领域适应重排序框架，能有效实现领域专精，同时保持出色的跨领域效果。

Abstract: Decoder-only rerankers are central to Retrieval-Augmented Generation (RAG). However, generalist models miss domain-specific nuances in high-stakes fields like finance and law, and naive fine-tuning causes surface-form overfitting and catastrophic forgetting. To address this challenge, we introduce R2R, a domain-aware framework that combines dynamic expert routing with a two-stage training strategy, Entity Abstraction for Generalization (EAG). EAG introduces a counter-shortcut mechanism by masking the most predictive surface cues, forcing the reranker to learn domain-invariant relevance patterns rather than memorizing dataset-specific entities. To efficiently activate domain experts, R2R employs a lightweight Latent Semantic Router that probes internal representations from the frozen backbone decoder to select the optimal LoRA expert per query. Extensive experiments across different reranker backbones and diverse domains (legal, medical, and financial) demonstrate that R2R consistently surpasses generalist and single-domain fine-tuned baselines. Our results confirm that R2R is a model-agnostic and modular approach to domain specialization with strong cross-domain robustness.

</details>


### [204] [Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test](https://arxiv.org/abs/2511.19997)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 本文提出了一个纯合成、熵可控的基准测试，专门用于分析Transformer模型在序列正反向建模中的表现差异。实验证明，即使在完全去除语言和统计偏见的情况下，Transformer训练仍存在“方向性摩擦”。


<details>
  <summary>Details</summary>
Motivation: 在自然语言任务中，Transformer通常表现出“反转诅咒”，即正向和反向任务表现不同，人们难以分辨这是由自身结构还是数据统计特性导致。为了澄清问题来源，作者设计了无语义、无偏见的合成任务，以更本质地研究方向性问题。

Method: 作者设计了基于随机字符串映射的合成任务，调节分支因子K并严格控制条件熵，分别构造出零条件熵的正向任务和可解析熵下限的反向任务，对比Transformer（如GPT-2）和MLP模型的表现，以及不同初始化（如预训练、LoRA适应）的影响。

Result: 结果显示，即便完全消除语言统计偏见，Transformer模型（如GPT-2）在逆向任务中比正向任务损失高出许多（例如K=5时差距为1.16 nats），而同等数据下MLP差距远小。预训练和LoRA的方法虽有一定优化，但无法消除这一本质性优化差距。

Conclusion: 方向性摩擦（Directional Friction）是Transformer因果训练固有的问题，不仅仅由语料统计或时序结构引起。本文提出的基准为后续深入研究Transformer为何本质上难以逆向拟合提供了工具和理论基础。

Abstract: Transformers are theoretically reversal-invariant: their function class does not prefer left-to-right over right-to-left mappings. Yet empirical studies on natural language repeatedly report a "reversal curse," and recent work on temporal asymmetry in LLMs suggests that real-world corpora carry their own arrow of time. This leaves an unresolved question: do directional failures stem from linguistic statistics, or from the architecture itself? We cut through this ambiguity with a fully synthetic, entropy-controlled benchmark designed as a clean-room stress test for directional learning. Using random string mappings with tunable branching factor K, we construct forward tasks with zero conditional entropy and inverse tasks with analytically determined entropy floors. Excess loss above these floors reveals that even scratch-trained GPT-2 models exhibit a strong, reproducible directional optimization gap (e.g., 1.16 nats at K=5), far larger than that of an MLP trained on the same data. Pre-trained initializations shift optimization behavior but do not eliminate this gap, while LoRA encounters a sharp capacity wall on high-entropy inverse mappings. Together, these results isolate a minimal, semantics-free signature of directional friction intrinsic to causal Transformer training-one that persists even when linguistic priors, token frequencies, and corpus-level temporal asymmetries are removed. Our benchmark provides a controlled instrument for dissecting directional biases in modern sequence models and motivates deeper mechanistic study of why inversion remains fundamentally harder for Transformers.

</details>


### [205] [A Machine Learning Approach for Detection of Mental Health Conditions and Cyberbullying from Social Media](https://arxiv.org/abs/2511.20001)
*Edward Ajayi,Martha Kachweka,Mawuli Deku,Emily Aiken*

Main category: cs.CL

TL;DR: 该论文提出了一种统一的多分类框架，可以从社交媒体数据中检测10种不同的心理健康和网络欺凌类别。通过Twitter和Reddit数据测试，采用多种模型检验，发现端到端调优的MentalBERT模型表现最好，并结合可解释性方法和可视化工具，提升了实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 数字空间中心理健康问题和网络欺凌现象频发，亟需可扩展且具备可解释性的自动检测系统来辅助管理和干预，但此前缺乏统一、高效且易用的多分类工具。

Method: 作者整理了社交媒体推文与评论数据，并采用“切分-再平衡”流程获得训练集和测试集，覆盖真实场景的样本失衡问题。评估了传统词汇模型、混合模型和多种端到端微调的transformer模型。同时开发了SHAPLLM解释性框架和“Social Media Screener”仪表盘，实现预测结果与解释的可视化集成。

Result: 端到端微调的MentalBERT（领域适应版本）在10分类检测任务中表现最佳，准确率达0.92，Macro F1分数0.76，优于通用BERT和零样本LLM基线。

Conclusion: 该研究为网络心理健康与网络欺凌多分类检测提供了强有力的基线和工具。系统定位为人机协作的筛查辅助，强调道德合规和使用边界，并指出未来需开发多标签、临床验证的数据集以进一步提升应用。

Abstract: Mental health challenges and cyberbullying are increasingly prevalent in digital spaces, necessitating scalable and interpretable detection systems. This paper introduces a unified multiclass classification framework for detecting ten distinct mental health and cyberbullying categories from social media data. We curate datasets from Twitter and Reddit, implementing a rigorous "split-then-balance" pipeline to train on balanced data while evaluating on a realistic, held-out imbalanced test set. We conducted a comprehensive evaluation comparing traditional lexical models, hybrid approaches, and several end-to-end fine-tuned transformers. Our results demonstrate that end-to-end fine-tuning is critical for performance, with the domain-adapted MentalBERT emerging as the top model, achieving an accuracy of 0.92 and a Macro F1 score of 0.76, surpassing both its generic counterpart and a zero-shot LLM baseline. Grounded in a comprehensive ethical analysis, we frame the system as a human-in-the-loop screening aid, not a diagnostic tool. To support this, we introduce a hybrid SHAPLLM explainability framework and present a prototype dashboard ("Social Media Screener") designed to integrate model predictions and their explanations into a practical workflow for moderators. Our work provides a robust baseline, highlighting future needs for multi-label, clinically-validated datasets at the critical intersection of online safety and computational mental health.

</details>


### [206] [Online-PVLM: Advancing Personalized VLMs with Online Concept Learning](https://arxiv.org/abs/2511.20056)
*Huiyu Bai,Runze Wang,Zhuoyun Du,Yiyang Zhao,Fengji Zhang,Haoyu Chen,Xiaoyong Zhu,Bo Zheng,Xuejiao Zhao*

Main category: cs.CL

TL;DR: 提出了一种名为Online-PVLM的新框架，实现了个性化视觉语言模型（VLM）的在线概念学习，并开发了大规模评测基准OP-Eval，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有个性化VLM方法需要为每个新概念单独训练嵌入，导致测试时无法实时适应新概念，且在大规模场景下检索低效。急需一种高效、可扩展的在线学习方法。

Method: 提出Online-PVLM框架，利用双曲空间表示，在测试阶段无需训练即可生成概念嵌入，大幅提升可扩展性和效率。并开发了OP-Eval评测集，包含1,292个概念和3万多个实例，用于评估在线概念学习性能。

Result: 框架在OP-Eval等多个场景下进行大量实验，展现出优于现有方法的最先进性能。

Conclusion: Online-PVLM为个性化VLM的在线高效扩展提供了新方案，具备良好性能和实用价值，并提供了未来研究的基准和资源。

Abstract: Personalized Visual Language Models (VLMs) are gaining increasing attention for their formidable ability in user-specific concepts aligned interactions (e.g., identifying a user's bike). Existing methods typically require the learning of separate embeddings for each new concept, which fails to support real-time adaptation during testing. This limitation becomes particularly pronounced in large-scale scenarios, where efficient retrieval of concept embeddings is not achievable. To alleviate this gap, we propose Online-PVLM, a framework for online concept learning by leveraging hyperbolic representations. Our approach makes a train-free paradigm for concept embeddings generation at test time, making the use of personalized VLMs both scalable and efficient. In addition, we develop OP-Eval, a comprehensive and large-scale benchmark comprising 1,292 concepts and over 30K high-quality instances with diverse question types, designed to rigorously assess online concept learning in realistic scenarios. Extensive experiments demonstrate the state-of-the-art performance of our proposed framework. Our source code and dataset will be made available.

</details>


### [207] [MTA: A Merge-then-Adapt Framework for Personalized Large Language Model](https://arxiv.org/abs/2511.20072)
*Xiaopeng Li,Yuanjin Zheng,Wanyu Wang,wenlin zhang,Pengyue Jia,Yiqi Wang,Maolin Wang,Xuetao Wei,Xiangyu Zhao*

Main category: cs.CL

TL;DR: 个性化大语言模型（PLLMs）能更好地贴合用户偏好，但传统为每个用户微调模块的方法在存储和数据稀疏性上存在瓶颈。本文提出了MTA框架，通过共享参数、动态融合和轻量级微调，实现了更高效、伸缩性强的个性化建模。实验结果显示该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前PLLM常通过给每个用户微调单独模块实现个性化，但这种做法存储开销大且对数据稀缺用户表现差，亟需更高效可扩展的个性化方式。

Method: 提出MTA框架，分三阶段：1）Meta-LoRA银行，用锚点用户预训练元个性化模块；2）自适应LoRA融合，对用户检索并动态合并相关元LoRA以合成用户特定LoRA，无需为每用户单独存储；3）LoRA堆叠，为小样本个性化引入轻量级额外LoRA，再做微调提升表现。

Result: 在LaMP基准测试上，MTA方法在多个任务上表现均优于现有SOTA方法。

Conclusion: MTA框架有效解决了PLLM在存储和小样本下的个性化难题，兼顾效率、拓展性和个性化表现，对用户中心应用极具价值。

Abstract: Personalized Large Language Models (PLLMs) aim to align model outputs with individual user preferences, a crucial capability for user-centric applications. However, the prevalent approach of fine-tuning a separate module for each user faces two major limitations: (1) storage costs scale linearly with the number of users, rendering the method unscalable; and (2) fine-tuning a static model from scratch often yields suboptimal performance for users with sparse data. To address these challenges, we propose MTA, a Merge-then-Adapt framework for PLLMs. MTA comprises three key stages. First, we construct a shared Meta-LoRA Bank by selecting anchor users and pre-training meta-personalization traits within meta-LoRA modules. Second, to ensure scalability and enable dynamic personalization combination beyond static models, we introduce an Adaptive LoRA Fusion stage. This stage retrieves and dynamically merges the most relevant anchor meta-LoRAs to synthesize a user-specific one, thereby eliminating the need for user-specific storage and supporting more flexible personalization. Third, we propose a LoRA Stacking for Few-Shot Personalization stage, which applies an additional ultra-low-rank, lightweight LoRA module on top of the merged LoRA. Fine-tuning this module enables effective personalization under few-shot settings. Extensive experiments on the LaMP benchmark demonstrate that our approach outperforms existing SOTA methods across multiple tasks.

</details>


### [208] [More Bias, Less Bias: BiasPrompting for Enhanced Multiple-Choice Question Answering](https://arxiv.org/abs/2511.20086)
*Duc Anh Vu,Thong Nguyen,Cong-Duy Nguyen,Viet Anh Nguyen,Anh Tuan Luu*

Main category: cs.CL

TL;DR: 提出了一种新型推理框架——BiasPrompting，通过引导大模型为每个选项生成推理并综合评估，有效提升了多项选择题上的推理与答案准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在选择题任务上虽表现提升，但由于答案选项缺乏上下文和推理支撑，经常导致对所有可能答案的探索不足，影响模型推理能力亟需改进。

Method: BiasPrompting 模型分为两个步骤：首先，要求模型为每个答案选项生成支持性推理过程；其次，再利用所有生成的理由来综合判断并选出最具可信度的答案。

Result: 在五个常用选择题基准测试上，BiasPrompting 方法显著提升了模型性能，增强了大模型推理能力，尤其在原有方法表现不佳的情况下效果突出。

Conclusion: BiasPrompting 为提升大语言模型解答复杂选择题能力提供了有效手段，为后续相关推理任务研究与应用打下了坚实基础。

Abstract: With the advancement of large language models (LLMs), their performance on multiple-choice question (MCQ) tasks has improved significantly. However, existing approaches face key limitations: answer choices are typically presented to LLMs without contextual grounding or explanation. This absence of context can lead to incomplete exploration of all possible answers, ultimately degrading the models' reasoning capabilities. To address these challenges, we introduce BiasPrompting, a novel inference framework that guides LLMs to generate and critically evaluate reasoning across all plausible answer options before reaching a final prediction. It consists of two components: first, a reasoning generation stage, where the model is prompted to produce supportive reasonings for each answer option, and then, a reasoning-guided agreement stage, where the generated reasonings are synthesized to select the most plausible answer. Through comprehensive evaluations, BiasPrompting demonstrates significant improvements in five widely used multiple-choice question answering benchmarks. Our experiments showcase that BiasPrompting enhances the reasoning capabilities of LLMs and provides a strong foundation for tackling complex and challenging questions, particularly in settings where existing methods underperform.

</details>


### [209] [SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space](https://arxiv.org/abs/2511.20102)
*Zhenyi Shen,Junru Lu,Lin Gui,Jiazheng Li,Yulan He,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: 本文提出了一种名为SSA（Sparse Sparse Attention）的训练框架，有效提升大型语言模型在长文本处理中的稀疏注意力效率和表现，兼顾训练过程中的梯度流动，取得了领先的实验效果。


<details>
  <summary>Details</summary>
Motivation: 传统的全注意力机制复杂度高，限制了LLM对长上下文的高效处理。稀疏注意力虽然降低了计算负担，但训练时易导致性能损失，并且本地稀疏注意力方法（如NSA、MoBA）往往稀疏性不如全注意力，影响效果。主要瓶颈在于：未被选中的key-value对未能参与前向和反向传播，无法有效抑制无用信息，造成‘梯度更新缺失’。

Method: SSA框架在每一层同时考虑稀疏和全注意力，采用双向对齐机制，确保所有token都能获得梯度更新，促进稀疏注意力输出与全注意力输出的一致性，同时显式增强稀疏性。同时，模型能够适应不同稀疏预算需求，支持推理时灵活调整计算量和性能间的平衡。

Result: SSA在多个常识类基准测试中，在稀疏和全注意力推理下均取得了最优表现。模型能够适应稀疏预算的变化，允许更多token参与时性能持续提升。此外，SSA提升了长文本外推能力，显著缓解注意力在‘汇聚区’的过度分配问题。

Conclusion: SSA为稀疏注意力训练提供了统一高效的框架，不仅解决了梯度更新缺失问题，也兼顾了训练和推理的灵活性能选择，显著提升模型在长文本任务上的泛化与外推表现。

Abstract: The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.

</details>


### [210] [EM2LDL: A Multilingual Speech Corpus for Mixed Emotion Recognition through Label Distribution Learning](https://arxiv.org/abs/2511.20106)
*Xingfeng Li,Xiaohan Shi,Junjie Li,Yongwei Li,Masashi Unoki,Tomoki Toda,Masato Akagi*

Main category: cs.CL

TL;DR: 本文提出了EM2LDL多语种语音情感数据库，支持混合情感识别，并公开了数据和基线代码。


<details>
  <summary>Details</summary>
Motivation: 现有情感语料库主要是单语种、单标签，缺乏对多语种、混合情感和真实生态语境的支持，限制了相关研究的发展。

Method: 构建包含英语、普通话和粤语的多语种情感语料库，采集自真实网络平台的带有复杂情感（32类分布式标签）的表达，并支持语内切换代码。使用自监督学习模型进行基线实验，兼顾性别、年龄和人格多样性评测。

Result: 基线实验显示，在说话人无关的三种任务（性别、年龄、人格）中，HuBERT-large-EN模型表现最佳，展现了数据集的实用性和挑战性。

Conclusion: EM2LDL极大丰富了多语种混合情感识别的研究基础，有助于促进跨文化情感计算和智能系统的相关应用。

Abstract: This study introduces EM2LDL, a novel multilingual speech corpus designed to advance mixed emotion recognition through label distribution learning. Addressing the limitations of predominantly monolingual and single-label emotion corpora \textcolor{black}{that restrict linguistic diversity, are unable to model mixed emotions, and lack ecological validity}, EM2LDL comprises expressive utterances in English, Mandarin, and Cantonese, capturing the intra-utterance code-switching prevalent in multilingual regions like Hong Kong and Macao. The corpus integrates spontaneous emotional expressions from online platforms, annotated with fine-grained emotion distributions across 32 categories. Experimental baselines using self-supervised learning models demonstrate robust performance in speaker-independent gender-, age-, and personality-based evaluations, with HuBERT-large-EN achieving optimal results. By incorporating linguistic diversity and ecological validity, EM2LDL enables the exploration of complex emotional dynamics in multilingual settings. This work provides a versatile testbed for developing adaptive, empathetic systems for applications in affective computing, including mental health monitoring and cross-cultural communication. The dataset, annotations, and baseline codes are publicly available at https://github.com/xingfengli/EM2LDL.

</details>


### [211] [Mispronunciation Detection and Diagnosis Without Model Training: A Retrieval-Based Approach](https://arxiv.org/abs/2511.20107)
*Huu Tuong Tu,Ha Viet Khanh,Tran Tien Dat,Vu Huan,Thien Van Luong,Nguyen Tien Cuong,Nguyen Thi Thu Trang*

Main category: cs.CL

TL;DR: 本文提出了一种无需额外训练的新型发音错误检测与诊断方法，通过检索技术结合预训练ASR模型，解决了繁琐的声韵母专用建模和训练问题，取得了较高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的发音错误检测和诊断方法往往需要复杂的打分模型或需进行音素级的特殊训练，导致系统实现复杂、门槛高。作者旨在提出一种更简便、高效的方法以推动该领域的发展。

Method: 采用检索机制，将学习者的音频输入与预训练的自动语音识别(ASR)模型输出进行比对，无需音素专用建模和任务特定训练，实现了训练自由的发音错误检测与诊断。

Result: 在L2-ARCTIC数据集上的实验结果显示，该方法取得了69.60%的F1分数，性能优于多数现有方法，同时具有极高的简便性。

Conclusion: 作者所提方法不仅在检测准确率上表现优异，同时规避了繁杂的模型训练，展现出实际应用推广的巨大潜力。

Abstract: Mispronunciation Detection and Diagnosis (MDD) is crucial for language learning and speech therapy. Unlike conventional methods that require scoring models or training phoneme-level models, we propose a novel training-free framework that leverages retrieval techniques with a pretrained Automatic Speech Recognition model. Our method avoids phoneme-specific modeling or additional task-specific training, while still achieving accurate detection and diagnosis of pronunciation errors. Experiments on the L2-ARCTIC dataset show that our method achieves a superior F1 score of 69.60% while avoiding the complexity of model training.

</details>


### [212] ["When Data is Scarce, Prompt Smarter"... Approaches to Grammatical Error Correction in Low-Resource Settings](https://arxiv.org/abs/2511.20120)
*Somsubhra De,Harsh Kumar,Arun Prakash A*

Main category: cs.CL

TL;DR: 本文利用先进的大语言模型（如GPT-4.1、Gemini-2.5和LLaMA-4）结合prompt策略，有效提升了多种低资源印度语言的语法纠错性能，取得了多项任务第一名。


<details>
  <summary>Details</summary>
Motivation: 虽然高资源语言（如英语）的语法纠错得到很大进展，但大多数印度语言因资源稀缺、语言多样性及复杂形态学，进展有限。作者希望通过利用最新的大语言模型和prompt方法弥补这些不足。

Method: 本研究采用prompt-based方法，结合zero-shot和few-shot策略，将GPT-4.1、Gemini-2.5和LLaMA-4等大语言模型应用于低资源的印度语言语法纠错任务。对比了这些LLM与专门微调的Indic模型（如Sarvam-22B），并通过精心设计的prompt和轻量级适配增强纠错质量。

Result: 实验结果表明，在多种印度语言的语法纠错任务上，这些LLM配合prompt策略显著优于专门微调的模型。团队在Tamil、Hindi、Telugu、Bangla和Malayalam语上分别取得了多个领先成绩。

Conclusion: prompt驱动的NLP方法和大语言模型在多语言语法纠错问题上极具潜力，可有效缓解低资源语言的资源瓶颈，并实现跨语种迁移和泛化。

Abstract: Grammatical error correction (GEC) is an important task in Natural Language Processing that aims to automatically detect and correct grammatical mistakes in text. While recent advances in transformer-based models and large annotated datasets have greatly improved GEC performance for high-resource languages such as English, the progress has not extended equally. For most Indic languages, GEC remains a challenging task due to limited resources, linguistic diversity and complex morphology. In this work, we explore prompting-based approaches using state-of-the-art large language models (LLMs), such as GPT-4.1, Gemini-2.5 and LLaMA-4, combined with few-shot strategy to adapt them to low-resource settings. We observe that even basic prompting strategies, such as zero-shot and few-shot approaches, enable these LLMs to substantially outperform fine-tuned Indic-language models like Sarvam-22B, thereby illustrating the exceptional multilingual generalization capabilities of contemporary LLMs for GEC. Our experiments show that carefully designed prompts and lightweight adaptation significantly enhance correction quality across multiple Indic languages. We achieved leading results in the shared task--ranking 1st in Tamil (GLEU: 91.57) and Hindi (GLEU: 85.69), 2nd in Telugu (GLEU: 85.22), 4th in Bangla (GLEU: 92.86), and 5th in Malayalam (GLEU: 92.97). These findings highlight the effectiveness of prompt-driven NLP techniques and underscore the potential of large-scale LLMs to bridge resource gaps in multilingual GEC.

</details>


### [213] [SEDA: A Self-Adapted Entity-Centric Data Augmentation for Boosting Gird-based Discontinuous NER Models](https://arxiv.org/abs/2511.20143)
*Wen-Fang Su,Hsiao-Wei Chou,Wen-Yang Lin*

Main category: cs.CL

TL;DR: 该论文提出了一种结合图像增强技术的网格标注方法，以提升对不连续命名实体的识别能力，实验在多个数据集上取得了优异的提升。


<details>
  <summary>Details</summary>
Motivation: 传统命名实体识别方法对跨句不连续实体的切分和识别存在困难，导致识别准确率降低，需要新的方法解决分割和遗漏问题。

Method: 作者提出在网格标注模型中引入图像增强技术（如裁剪、缩放、填充），提升模型在处理不连续实体与文本分段时的鲁棒性。

Result: 在CADEC、ShARe13和ShARe14数据集上，增强型网格模型的整体F1值提升了1-2.5%，针对不连续实体提升达3.7-8.4%。

Conclusion: 结合图像增强的网格标注模型有效提升了不连续命名实体的识别性能，优于传统分段方法。

Abstract: Named Entity Recognition (NER) is a critical task in natural language processing, yet it remains particularly challenging for discontinuous entities. The primary difficulty lies in text segmentation, as traditional methods often missegment or entirely miss cross-sentence discontinuous entities, significantly affecting recognition accuracy. Therefore, we aim to address the segmentation and omission issues associated with such entities. Recent studies have shown that grid-tagging methods are effective for information extraction due to their flexible tagging schemes and robust architectures. Building on this, we integrate image data augmentation techniques, such as cropping, scaling, and padding, into grid-based models to enhance their ability to recognize discontinuous entities and handle segmentation challenges. Experimental results demonstrate that traditional segmentation methods often fail to capture cross-sentence discontinuous entities, leading to decreased performance. In contrast, our augmented grid models achieve notable improvements. Evaluations on the CADEC, ShARe13, and ShARe14 datasets show F1 score gains of 1-2.5% overall and 3.7-8.4% for discontinuous entities, confirming the effectiveness of our approach.

</details>


### [214] [KyrgyzBERT: A Compact, Efficient Language Model for Kyrgyz NLP](https://arxiv.org/abs/2511.20182)
*Adilet Metinov,Gulida M. Kudakeeva,Gulnara D. Kabaeva*

Main category: cs.CL

TL;DR: 本文提出了KyrgyzBERT，这是首个公开的吉尔吉斯语单语BERT语言模型，并构建了情感分析评测基准kyrgyz-sst2。实验结果显示KyrgyzBERT表现优秀，有力推动了吉尔吉斯语NLP研究发展。


<details>
  <summary>Details</summary>
Motivation: 吉尔吉斯语属于低资源语言，缺乏基础的自然语言处理工具，对相关研究和应用造成限制。现有通用多语言模型对吉尔吉斯语支持有限，因此亟需专门为该语言设计高质量的基础模型和评测集，以促进该语NLP的发展。

Method: 1. 构建并训练了35.9M参数量的吉尔吉斯语BERT模型KyrgyzBERT，采用针对吉尔吉斯语词形结构特点的定制分词器。
2. 创建kyrgyz-sst2数据集，通过翻译Stanford Sentiment Treebank并对测试集进行人工标注，作为情感分析基准。
3. 在该数据集上对KyrgyzBERT进行微调，并与体量更大的多语mBERT进行性能对比。

Result: 在kyrgyz-sst2情感分析基准上，微调后的KyrgyzBERT获得了F1分数0.8280，达到与参数量大五倍的mBERT相当的性能。

Conclusion: KyrgyzBERT为吉尔吉斯语NLP提供了关键基础，表现优异。同时开源了模型、数据和代码，为后续该语种NLP研究提供了重要资源。

Abstract: Kyrgyz remains a low-resource language with limited foundational NLP tools. To address this gap, we introduce KyrgyzBERT, the first publicly available monolingual BERT-based language model for Kyrgyz. The model has 35.9M parameters and uses a custom tokenizer designed for the language's morphological structure. To evaluate performance, we create kyrgyz-sst2, a sentiment analysis benchmark built by translating the Stanford Sentiment Treebank and manually annotating the full test set. KyrgyzBERT fine-tuned on this dataset achieves an F1-score of 0.8280, competitive with a fine-tuned mBERT model five times larger. All models, data, and code are released to support future research in Kyrgyz NLP.

</details>


### [215] [REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance](https://arxiv.org/abs/2511.20233)
*Chuyi Kong,Gao Wei,Jing Ma,Hongzhan Lin,Zhiyuan Fan*

Main category: cs.CL

TL;DR: 本文提出了一种名为REFLEX的自动事实核查新范式，不依赖外部知识，更高效、可靠，并能提升判决准确性与解释质量。


<details>
  <summary>Details</summary>
Motivation: 社交媒体虚假信息泛滥，需要可解释、快速且高性能的自动事实核查系统。现有大模型方法重度依赖外部知识，存在响应慢、易幻觉及解释不可靠等问题。

Method: REFLEX以角色扮演式对话方式，将事实核查建模为联合的判决与解释生成任务，通过对比激活信号提取，构造操控向量引导判决，利用主模型自身的隐藏知识进行推理，抑制噪音解释，并在极少样本下进行自我精炼训练。

Result: REFLEX在真实数据集上超越现有方法，实现了更高的准确率，仅用465个样本即达SOTA。解释任务还能帮助无解释模型推理，准确率提升最高7.57%。

Conclusion: REFLEX无需外部知识即可兼顾事实核查的准确性和可解释性，证明了内部解释信号对提升模型判别和解释双重能力的重要作用。

Abstract: The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.

</details>


### [216] [Scaling LLM Speculative Decoding: Non-Autoregressive Forecasting in Large-Batch Scenarios](https://arxiv.org/abs/2511.20340)
*Luohe Shi,Zuchao Li,Lefei Zhang,Baoyuan Qi,Guoming Liu,Hai Zhao*

Main category: cs.CL

TL;DR: SpecFormer是一种结合单向和双向注意力机制的新架构，能够在低资源和大批量推理场景下高效进行推理加速，降低训练和计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前主流的推理加速方案（如批处理）压缩了闲置算力，使得现有的speculative decoding方法在资源有限场景下效果有限，因此亟需一种能充分利用有限资源、且易于扩展的推理加速新方法。

Method: 提出SpecFormer架构，将自回归模型的整个输入序列信息提取能力与非自回归模型的并行生成优点结合，同时整合单向和双向注意力机制，无需依赖复杂的大规模前缀树，适用于各种大批量和低算力场景。

Result: SpecFormer在不同规模模型的无损speculative decoding实验中展现了优于现有方法的推理加速效果，且训练需求和计算成本更低。

Conclusion: SpecFormer为大模型推理在低资源和高并发场景下提供了有效的加速新范式，有望成为未来推理系统的标准架构。

Abstract: Speculative decoding accelerates LLM inference by utilizing otherwise idle computational resources during memory-to-chip data transfer. Current speculative decoding methods typically assume a considerable amount of available computing power, then generate a complex and massive draft tree using a small autoregressive language model to improve overall prediction accuracy. However, methods like batching have been widely applied in mainstream model inference systems as a superior alternative to speculative decoding, as they compress the available idle computing power. Therefore, performing speculative decoding with low verification resources and low scheduling costs has become an important research problem. We believe that more capable models that allow for parallel generation on draft sequences are what we truly need. Recognizing the fundamental nature of draft models to only generate sequences of limited length, we propose SpecFormer, a novel architecture that integrates unidirectional and bidirectional attention mechanisms. SpecFormer combines the autoregressive model's ability to extract information from the entire input sequence with the parallel generation benefits of non-autoregressive models. This design eliminates the reliance on large prefix trees and achieves consistent acceleration, even in large-batch scenarios. Through lossless speculative decoding experiments across models of various scales, we demonstrate that SpecFormer sets a new standard for scaling LLM inference with lower training demands and reduced computational costs.

</details>


### [217] [The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models](https://arxiv.org/abs/2511.20344)
*Taewhoo Lee,Minju Song,Chanwoong Yoon,Jungwoo Park,Jaewoo Kang*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在类比推理方面的能力，发现其在捕捉高层次关系和应用到新情境中的能力有限但正在显现。


<details>
  <summary>Details</summary>
Motivation: 类比推理是人类认知的核心，众多高级智能活动的基础。尽管已有研究显示LLMs能表示任务模式和表层概念，但尚不清楚它们是否能编码高层关系概念，并通过结构化对比迁移到新情境中。

Method: 通过比例类比与故事类比实验，系统分析LLMs中隐藏层的信息传播和结构对齐表现，并借助关键位置修补措施考察信息迁移能力。

Result: 1）LLMs能正确编码类比实体间的关系信息，尤其在模型中层到高层传播时明显；2）相比人类，LLMs不仅在缺乏关系信息时表现欠佳，还难以将已知关系迁移到新实体中，策略性调整隐藏表示能一定程度改善表现；3）推理成功时，模型展现出强结构对齐，失败时则反映对齐减弱或错位。

Conclusion: LLMs在编码及应用高阶关系概念上表现出初步但有限的能力，揭示其与人类认知的相似与差距。

Abstract: Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.

</details>


### [218] [BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali](https://arxiv.org/abs/2511.20399)
*Abdullah Al Sefat*

Main category: cs.CL

TL;DR: 本文提出了BengaliFig数据集，专门用于评估大型语言模型在孟加拉语（低资源语言）下的隐喻与文化性推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多语言大型语言模型（LLM）表现出色，但在具有文化性或隐喻性推理任务上，尤其是在低资源语言如孟加拉语中，评测和数据集仍然稀缺。作者希望填补LLM在这些语境下评估的空白。

Method: 构建了BengaliFig数据集，收集了435个源自孟加拉口头和文学传统的谜语，并在五个维度（推理类型、陷阱类型、文化深度、答案类别、难度）上进行注释，然后通过自动多选题生成流程整理。利用零样本和少样本链式思考提示，评估了八个主流LLM在该数据集上的表现。

Result: 主流LLM在涉及隐喻和高度文化依赖的推理题上表现出明显短板，鲁棒性不足。

Conclusion: BengaliFig既为低资源文化语境下的LLM评估提供了有力工具，也推动了包容性和文化遗产敏感的NLP研究。

Abstract: Large language models excel on broad multilingual benchmarks but remain to be evaluated extensively in figurative and culturally grounded reasoning, especially in low-resource contexts. We present BengaliFig, a compact yet richly annotated challenge set that targets this gap in Bengali, a widely spoken low-resourced language. The dataset contains 435 unique riddles drawn from Bengali oral and literary traditions. Each item is annotated along five orthogonal dimensions capturing reasoning type, trap type, cultural depth, answer category, and difficulty, and is automatically converted to multiple-choice format through a constraint-aware, AI-assisted pipeline. We evaluate eight frontier LLMs from major providers under zero-shot and few-shot chain-of-thought prompting, revealing consistent weaknesses in metaphorical and culturally specific reasoning. BengaliFig thus contributes both a diagnostic probe for evaluating LLM robustness in low-resource cultural contexts and a step toward inclusive and heritage-aware NLP evaluation.

</details>


### [219] [A Task-Oriented Evaluation Framework for Text Normalization in Modern NLP Pipelines](https://arxiv.org/abs/2511.20409)
*Md Abdullah Al Kafi,Raka Moni,Sumit Kumar Banshal*

Main category: cs.CL

TL;DR: 提出了一种新的针对具体任务的词干提取（stemming）方法评价框架，综合考虑效用、下游任务表现和语义相似度，发现目前的一些词干方法在高效的同时可能过度损害语义。


<details>
  <summary>Details</summary>
Motivation: 词干化作为文本归一化的重要步骤，现有的评估方法不足，无法全面反映因过度词干化可能带来的语义损害，因此需要一种能兼顾效用与语义安全的新评估框架。

Method: 提出了三维评价体系：（1）利用Stemming Effectiveness Score (SES)来衡量词干化效用，（2）通过Model Performance Delta (MPD)评估下游任务表现的影响，（3）采用平均归一化Levenshtein距离（ANLD）度量词干化前后词语的语义相似性。并将该框架应用于孟加拉语和英语的两种stemmer对比实验。

Result: 孟加拉语stemmer在SES指标下表现最好，但ANLD显示过度词干化严重，导致下游任务表现下降；而英语stemmer在词干效用与语义安全之间取得平衡，对下游任务更友好。

Conclusion: 仅关注词干效用指标（SES）易忽视因过度词干化带来的语义损失，所提出评价框架可用于全面区分词干效率提升和意义保留，为词干化方法选择与优化提供更科学的依据。

Abstract: Text normalization is an essential preprocessing step in many natural language processing (NLP) tasks, and stemming is one such normalization technique that reduces words to their base or root form. However, evaluating stemming methods is challenging because current evaluation approaches are limited and do not capture the potential harm caused by excessive stemming; therefore, it is essential to develop new approaches to evaluate stemming methods. To address this issue, this study propose a novel, task-oriented approach to evaluate stemming methods, which considers three aspects: (1) the utility of stemming using Stemming Effectiveness Score (SES), (2) the impact of stemming on downstream tasks using Model Performance Delta (MPD), and (3) the semantic similarity between stemmed and original words using Average Normalized Levenshtein Distance (ANLD), thus providing a comprehensive evaluation framework. We apply our evaluation framework to compare two stemmers for Bangla (BNLTK) and English (Snowball), and our results reveal a significant issue, prompting us to analyze their performance in detail. While the Bangla stemmer achieves the highest SES (1.67) due to effective word reduction (CR = 1.90), SES alone is insufficient because our proposed safety measure, ANLD, reveals that this high SES is due to harmful over-stemming (ANLD = 0.26), which correlates with the observed decrease in downstream performance.In contrast, the English stemmer achieves a moderate SES (1.31) with a safe meaning distance (ANLD = 0.14), allowing its word reduction to contribute positively to downstream performance; therefore, it is a more reliable stemmer. Our study provides a valuable tool for distinguishing between potential efficiency gains (high SES) and meaning preservation (low ANLD).

</details>


### [220] [Generation, Evaluation, and Explanation of Novelists' Styles with Single-Token Prompts](https://arxiv.org/abs/2511.20459)
*Mosab Rezaei,Mina Rajaei Moghadam,Abdul Rahman Shaikh,Hamed Alhoori,Reva Freedman*

Main category: cs.CL

TL;DR: 本文提出了一种结合生成与评估19世纪小说家风格文本的新方法，通过微调大语言模型与AI自动评估，实现高效且可解释的文体仿写。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来大语言模型促进了文体学研究，但在缺乏配对训练数据和评估机制过度依赖人工判断的情况下，难以有效完成风格仿写任务。因此亟需新的生成与评估框架。

Method: 1) 利用极简单词 prompt 对大型预训练语言模型微调，生成狄更斯、奥斯汀、马克·吐温等19世纪小说家风格文本；2) 借助transformer检测器，对生成文本进行分类和风格解释；3) 结合句法对比与可解释性AI（注意力和梯度分析）找出关键文体特征。

Result: 微调模型可生成高度仿真的作家风格文本；基于AI的检测与解释方法能有效区分和解释这些风格仿写；自动评估结果与人工判断高度一致。

Conclusion: 所提框架使AI可高效、可靠地生成并评估作家文体仿写，减少人工干预，推动文体学和自动写作研究发展，并公开发布全部相关资源。

Abstract: Recent advances in large language models have created new opportunities for stylometry, the study of writing styles and authorship. Two challenges, however, remain central: training generative models when no paired data exist, and evaluating stylistic text without relying only on human judgment. In this work, we present a framework for both generating and evaluating sentences in the style of 19th-century novelists. Large language models are fine-tuned with minimal, single-token prompts to produce text in the voices of authors such as Dickens, Austen, Twain, Alcott, and Melville. To assess these generative models, we employ a transformer-based detector trained on authentic sentences, using it both as a classifier and as a tool for stylistic explanation. We complement this with syntactic comparisons and explainable AI methods, including attention-based and gradient-based analyses, to identify the linguistic cues that drive stylistic imitation. Our findings show that the generated text reflects the authors' distinctive patterns and that AI-based evaluation offers a reliable alternative to human assessment. All artifacts of this work are published online.

</details>


### [221] [Adversarial Confusion Attack: Disrupting Multimodal Large Language Models](https://arxiv.org/abs/2511.20494)
*Jakub Hoscilowicz,Artur Janicki*

Main category: cs.CL

TL;DR: 提出了一种针对多模态大模型的新型混淆攻击方式，可以让模型输出混乱或错误的内容，并且该攻击具有很强的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型在现实世界得到广泛应用，传统的攻击方式（如越狱、特定目标误分类）已被充分研究，但缺乏系统性扰乱模型可信度的新方法。该研究旨在弥补这个空白，干扰基于MLLM的自动系统的正常运作。

Method: 作者提出了Adversarial Confusion Attack，利用最大化下一个token的不确定性（熵）为目标，在小规模开源MLLM集群上生成对抗图片，并测试该方法在白盒环境、完整图片和验证码场景下的有效性。主要采用了PGD（投影梯度下降）方法生成扰动。

Result: 实验证明，仅凭单张对抗图片可在白盒场景下扰乱整个集群中的所有模型，无论是在完整图片还是对抗验证码场景。生成的扰动还可以迁移到未见过的开源模型（如Qwen3-VL）和闭源模型（如GPT-5.1），表现出很强的通用性和威胁性。

Conclusion: 该方法为多模态大模型带来了新的安全威胁，现有的对抗防御措施亟需改进，建议未来工作关注模型对系统性混淆攻击的鲁棒性提升。

Abstract: We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Applications include embedding adversarial images into websites to prevent MLLM-powered agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models.

</details>


### [222] [The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models](https://arxiv.org/abs/2511.20507)
*Nathan Roll,Jill Kries,Flora Jin,Catherine Wang,Ann Marie Finley,Meghan Sumner,Cory Shain,Laura Gwilliams*

Main category: cs.CL

TL;DR: 该论文提出了专为大语言模型（LLMs）设计的失语症评估基准Text Aphasia Battery (TAB)，用以分析人工智能语言系统中的语言障碍。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM可作为“语言模型生物体”，帮助我们探索人类语言障碍（如失语症）的计算基础，但现有的临床评估工具是为人类设计的，不适用于模型。该研究旨在填补这一空白。

Method: 论文将Quick Aphasia Battery (QAB) 改编为TAB，包含四个子测验：连贯文本、词语理解、句子理解与复述，并设定了相应的评分标准。同时，使用Gemini 2.5 Flash自动化评分协议进行验证，并与专家人工评分一致性进行比较。

Result: 自动化评分协议的可靠性与人工专家评级相当，模型与专家共识间Cohen's kappa为0.255，而人工之间为0.286。

Conclusion: TAB作为一个临床依据的可扩展框架，能够系统分析人工语言系统中的语言障碍，为评估和研究LLM的类失语表现提供了标准化工具。

Abstract: Large language models (LLMs) have emerged as a candidate "model organism" for human language, offering an unprecedented opportunity to study the computational basis of linguistic disorders like aphasia. However, traditional clinical assessments are ill-suited for LLMs, as they presuppose human-like pragmatic pressures and probe cognitive processes not inherent to artificial architectures. We introduce the Text Aphasia Battery (TAB), a text-only benchmark adapted from the Quick Aphasia Battery (QAB) to assess aphasic-like deficits in LLMs. The TAB comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition. This paper details the TAB's design, subtests, and scoring criteria. To facilitate large-scale use, we validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen's kappa = 0.255 for model--consensus agreement vs. 0.286 for human--human agreement). We release TAB as a clinically-grounded, scalable framework for analyzing language deficits in artificial systems.

</details>


### [223] [Bridging the Language Gap: Synthetic Voice Diversity via Latent Mixup for Equitable Speech Recognition](https://arxiv.org/abs/2511.20534)
*Wesley Bian,Xiaofeng Lin,Guang Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种新的语音数据增强技术，有效提升了低资源语言的自动语音识别系统的性能，并超过了已有增强方法。


<details>
  <summary>Details</summary>
Motivation: 现代音频任务的机器学习模型在高资源语言上效果优异，主要受益于数据丰富，但低资源语言由于数据稀缺，性能表现较差，因此亟需方法弥补这一差距。

Method: 提出并应用了一种新颖的语音语料库数据增强技术，通过为低资源语言生成或扩充训练数据，以提升模型表现。

Result: 实验结果表明该方法能显著提升低资源语言的自动语音识别性能，并优于现有的数据增强策略。

Conclusion: 所提数据增强方法为低资源语言提供了实用的语音技术提升方案，有助于缩小多语言模型性能差距，促进语言公平。

Abstract: Modern machine learning models for audio tasks often exhibit superior performance on English and other well-resourced languages, primarily due to the abundance of available training data. This disparity leads to an unfair performance gap for low-resource languages, where data collection is both challenging and costly. In this work, we introduce a novel data augmentation technique for speech corpora designed to mitigate this gap. Through comprehensive experiments, we demonstrate that our method significantly improves the performance of automatic speech recognition systems on low-resource languages. Furthermore, we show that our approach outperforms existing augmentation strategies, offering a practical solution for enhancing speech technology in underrepresented linguistic communities.

</details>


### [224] [From Words to Wisdom: Discourse Annotation and Baseline Models for Student Dialogue Understanding](https://arxiv.org/abs/2511.20547)
*Farjana Sultana Mim,Shuchin Aeron,Eric Miller,Kristen Wendell*

Main category: cs.CL

TL;DR: 本文提出一个教育对话数据集，用于自动识别学生对话中的知识建构与任务产出话语特征，并利用大语言模型进行基线实验，发现模型表现有限，未来有提升空间。


<details>
  <summary>Details</summary>
Motivation: 人工分析学生对话以识别话语特征耗时且成本高，限制了研究规模。现有NLP的对话话语研究很少涉足教育领域，缺乏自动化识别教育语境中话语特征的方法，有必要弥补这个空白。

Method: 作者构建了一个包含知识建构与任务产出话语特征标注的教育对话数据集，并用GPT-3.5和Llama-3.1等预训练大语言模型对每个对话轮次的话语属性进行自动预测实验，建立了基线模型。

Result: 实验结果显示，当前先进的大语言模型在该任务上的表现不佳，尚未达到令人满意的自动预测效果。

Conclusion: 现有模型识别教育对话话语特征的能力有限，研究表明该领域尚有很大提升空间，为今后的自动化教育对话分析提供了数据和基线参考。

Abstract: Identifying discourse features in student conversations is quite important for educational researchers to recognize the curricular and pedagogical variables that cause students to engage in constructing knowledge rather than merely completing tasks. The manual analysis of student conversations to identify these discourse features is time-consuming and labor-intensive, which limits the scale and scope of studies. Leveraging natural language processing (NLP) techniques can facilitate the automatic detection of these discourse features, offering educational researchers scalable and data-driven insights. However, existing studies in NLP that focus on discourse in dialogue rarely address educational data. In this work, we address this gap by introducing an annotated educational dialogue dataset of student conversations featuring knowledge construction and task production discourse. We also establish baseline models for automatically predicting these discourse properties for each turn of talk within conversations, using pre-trained large language models GPT-3.5 and Llama-3.1. Experimental results indicate that these state-of-the-art models perform suboptimally on this task, indicating the potential for future research.

</details>


### [225] [On Evaluating LLM Alignment by Evaluating LLMs as Judges](https://arxiv.org/abs/2511.20604)
*Yixin Liu,Pengfei Liu,Arman Cohan*

Main category: cs.CL

TL;DR: 该论文提出了一种新的大模型对齐评测范式：通过评估大模型作为“评判者”的能力，间接衡量其对人类偏好的对齐程度，无需直接分析其生成的内容。实验发现这种方法有效捕捉了模型与人类偏好的契合度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）的对齐评估通常要人工或强模型直接对生成内容打分，这样成本高、效率低。已有探索让模型自评，其合理性、与生成能力的关系值得深入研究。

Method: 分析多种LLM的生成-评判一致性（GE-consistency），通过强模型作“裁判”，统计其生成内容和自评能力间的联系。基于此提出AlignEval基准，以模型作为评判者能力替代直接生成内容评分，实现间接对齐评测。

Result: 实验证明，不用直接评测生成内容，仅评测模型的评判能力，AlignEval基准在衡量对齐水平时与或超越主流评测基准，如AlpacaEval和Arena-Hard，且能很好捕捉人类偏好排序。

Conclusion: LLM的生成与评判能力高度相关；基于评判能力的新基准AlignEval能有效、经济地评估模型对齐程度，为对齐评估提供了新的方向。

Abstract: Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.

</details>


### [226] [Latent Collaboration in Multi-Agent Systems](https://arxiv.org/abs/2511.20639)
*Jiaru Zou,Xiyuan Yang,Ruizhong Qiu,Gaotang Li,Katherine Tieu,Pan Lu,Ke Shen,Hanghang Tong,Yejin Choi,Jingrui He,James Zou,Mengdi Wang,Ling Yang*

Main category: cs.CL

TL;DR: 提出了一种无需训练的多智能体语言模型协作框架LatentMAS，通过在潜空间（而非文本）中实现智能体之间的信息交流，提升了系统级推理表现和效率。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体大语言模型通常依赖基于文本的信息交换，存在信息损耗和推理效率低的问题。该论文希望通过消除“文本瓶颈”，提升MAS的表达力和协作效率。

Method: LatentMAS提出让每个LLM代理基于最后一层隐藏向量生成自己的潜在思维，通过一个共享的潜在工作记忆实现智能体间内部表示的无损保存与交换。整个流程无需额外训练，通过端到端推理实现。

Result: 理论分析表明LatentMAS具有更高表达能力与更低复杂度。9项基准实验证明，该方法实现了最高14.6%的准确率提升，输出token数降低70.8%-83.7%，推理速度加快4-4.3倍，全面优于文本协作型与单模模型基线。

Conclusion: LatentMAS在保证系统推理质量提升的同时显著提升效率，且无需额外训练，表明基于潜空间的协作是推进MAS系统智能的重要方向。

Abstract: Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [227] [Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories](https://arxiv.org/abs/2511.19528)
*Rushuai Yang,Zhiyuan Feng,Tianxiang Zhang,Kaixin Wang,Chuheng Zhang,Li Zhao,Xiu Su,Yi Chen,Jiang Bian*

Main category: cs.RO

TL;DR: 本论文提出了一种新方法DLR，能用于生成多样化、高质量的操作轨迹，以扩展视觉-语言-动作（VLA）模型的大规模预训练数据，突破了传统强化学习数据单一的局限。


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型预训练数据主要靠人工采集，成本高且难以扩展；自动化强化学习虽能自主探索，但只会学到单一模式，难以生成多样化数据，限制了VLA模型预训练的广度。

Method: 提出了一种基于信息论的模式发现方法——Discover, Learn and Reinforce（DLR），能够在任务中发现并学习多种不同但同样成功的行为策略，系统性地扩展操作轨迹的多样性。该方法在LIBERO任务集上对比了标准RL和DLR生成的数据多样性和覆盖范围。

Result: DLR大幅度提升了数据样本的多样性和状态-动作空间的覆盖度，在同等数据量下，使用DLR多样数据预训练的模型在新任务上的表现超过了用传统RL方法预训练的模型。此外，DLR生成的数据表现出更好的规模扩展性。

Conclusion: 多模式强化学习（如DLR）是一种高效且可扩展的数据引擎，未来有望成为支持大规模具身基础模型（如VLA模型）预训练的重要技术。

Abstract: Scaling vision-language-action (VLA) model pre-training requires large volumes of diverse, high-quality manipulation trajectories. Most current data is obtained via human teleoperation, which is expensive and difficult to scale. Reinforcement learning (RL) methods learn useful skills through autonomous exploration, making them a viable approach for generating data. However, standard RL training collapses to a narrow execution pattern, limiting its utility for large-scale pre-training. We propose Discover, Lea rn and Reinforce (DLR), an information-theoretic pattern discovery framework that generates multiple distinct, high-success behavioral patterns for VLA pretraining. Empirically, DLR generates a markedly more diverse trajectory corpus on LIBERO. Specifically, it learns multiple distinct, high-success strategies for the same task where standard RL discovers only one, and hence it covers substantially broader regions of the state-action space. When adapted to unseen downstream task suites, VLA models pretrained on our diverse RL data surpass counterparts trained on equal-sized standard RL datasets. Moreover, DLR exhibits positive data-scaling behavior that single-pattern RL lacks. These results position multi-pattern RL as a practical, scalable data engine for embodied foundation models.

</details>


### [228] [A Virtual Mechanical Interaction Layer Enables Resilient Human-to-Robot Object Handovers](https://arxiv.org/abs/2511.19543)
*Omar Faris,Sławomir Tadeja,Fulvio Forni*

Main category: cs.RO

TL;DR: 本文提出了一种结合虚拟模型控制与增强现实的机器人协作物体交接方法，以应对交接过程中物体姿态复杂变化带来的挑战，并且通过实验与用户研究验证了方案的有效性与用户偏好。


<details>
  <summary>Details</summary>
Motivation: 在人机协作任务中，物体交接是常见却难以高效实现的任务，尤其是在物体姿态发生复杂变化时，保障机器人的鲁棒性和适应性尤为关键。

Method: 作者提出利用虚拟模型控制（Virtual Model Control）为机器人创建交互层，使其能实时适应物体在交接过程中的动态变化，并引入增强现实作为人机双向交互辅助工具。

Result: 实验结果表明，该控制器对物体姿态变化及不确定因素具备良好鲁棒性。16名参与者的用户研究还显示，被试普遍偏好所提出的控制与增强现实交互方式；

Conclusion: 所提方法能有效提升机器人物体交接的适应性与人机交互体验，对今后相关应用和交互方式的优化提供了有价值的见解。

Abstract: Object handover is a common form of interaction that is widely present in collaborative tasks. However, achieving it efficiently remains a challenge. We address the problem of ensuring resilient robotic actions that can adapt to complex changes in object pose during human-to-robot object handovers. We propose the use of Virtual Model Control to create an interaction layer that controls the robot and adapts to the dynamic changes in the handover process. Additionally, we propose the use of augmented reality to facilitate bidirectional communication between humans and robots during handovers. We assess the performance of our controller in a set of experiments that demonstrate its resilience to various sources of uncertainties, including complex changes to the object's pose during the handover. Finally, we performed a user study with 16 participants to understand human preferences for different robot control profiles and augmented reality visuals in object handovers. Our results showed a general preference for the proposed approach and revealed insights that can guide further development in adapting the interaction with the user.

</details>


### [229] [Robot-Powered Data Flywheels: Deploying Robots in the Wild for Continual Data Collection and Foundation Model Adaptation](https://arxiv.org/abs/2511.19647)
*Jennifer Grannen,Michelle Pan,Kenneth Llontop,Cherie Ho,Mark Zolotas,Jeannette Bohg,Dorsa Sadigh*

Main category: cs.RO

TL;DR: 本文提出了“机器人驱动数据飞轮”框架，让机器人不仅仅是基础模型（FM）的使用者，而是动态数据的生产者，从而提升FM在真实、复杂环境下的适应性。通过在真实图书馆布署机器人Scanford，收集并自动标注大规模真实场景数据，显著提升了视觉-语言模型在细分和相邻领域任务上的表现，并减少了人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型多依赖网络数据进行预训练，导致其在复杂、无结构的真实世界环境表现不稳定，而这类数据在现有语料中严重稀缺。机器人作为具身智能体，能够主动采集真实复杂环境下的数据，有潜力弥补这一缺口。作者希望通过机器人收集的数据提升模型在实际应用中的适应力和泛化能力。

Method: 提出并实践了“机器人驱动数据飞轮”框架。在该系统中，机器人配备视觉-语言基础模型（VLM），自主在图书馆中巡查、识别书本，并利用馆藏目录自动完成图像标注，整个过程无需人工干预。收集到的大规模数据随后用于微调底层的VLM。具体实例是Scanford机器人，在东亚图书馆连续运行2周，采集了2103个书架的数据。

Result: Scanford自动采集并标注的数据显著提升了VLM的效果：图书识别准确率从32.0%提升到71.8%，英文多语言OCR从24.8%提升到46.6%，中文从30.8%提升到38.0%。同时这一流程节省了约18.7小时人工标注时间。

Conclusion: 机器人驱动的数据飞轮方法使基础模型能够适应真实世界的复杂性和多样性，同时减少了人工成本，为基础模型的持续适应和演化开辟了新方向。

Abstract: Foundation models (FM) have unlocked powerful zero-shot capabilities in vision and language, yet their reliance on internet pretraining data leaves them brittle in unstructured, real-world settings. The messy, real-world data encountered during deployment (e.g. occluded or multilingual text) remains massively underrepresented in existing corpora. Robots, as embodied agents, are uniquely positioned to close this gap: they can act in physical environments to collect large-scale, real-world data that enriches FM training with precisely the examples current models lack. We introduce the Robot-Powered Data Flywheel, a framework that transforms robots from FM consumers into data generators. By deploying robots equipped with FMs in the wild, we enable a virtuous cycle: robots perform useful tasks while collecting real-world data that improves both domain-specific adaptation and domain-adjacent generalization. We instantiate this framework with Scanford, a mobile manipulator deployed in the East Asia Library for 2 weeks. Scanford autonomously scans shelves, identifies books using a vision-language model (VLM), and leverages the library catalog to label images without human annotation. This deployment both aids librarians and produces a dataset to finetune the underlying VLM, improving performance on the domain-specific in-the-wild library setting and on domain-adjacent multilingual OCR benchmarks. Using data collected from 2103 shelves, Scanford improves VLM performance on book identification from 32.0% to 71.8% and boosts domain-adjacent multilingual OCR from 24.8% to 46.6% (English) and 30.8% to 38.0% (Chinese), while saving an ~18.7 hrs of human time. These results highlight how robot-powered data flywheels can both reduce human effort in real deployments and unlock new pathways for continually adapting FMs to the messiness of reality. More details are at: https://scanford-robot.github.io

</details>


### [230] [Online Learning-Enhanced High Order Adaptive Safety Control](https://arxiv.org/abs/2511.19651)
*Lishuo Pan,Mattia Catellani,Thales C. Silva,Lorenzo Sabattini,Nora Ayanian*

Main category: cs.RO

TL;DR: 本论文提出了一种结合神经ODE的高阶自适应控制屏障函数（CBF）方法，提高了在模型失配和动态扰动（如风、载重等）条件下的系统安全性，并通过实验证明其在纳米级四旋翼上的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统CBF虽然可为系统安全性提供理论保证，但其依赖精确模型，面对现实中的扰动（比如风、载荷变化）时难以保持安全性。为实现CBF的真实可用，亟需能够适应动态变化、提高鲁棒性的安全控制方法。

Method: 提出了一种利用神经ODE增强的高阶自适应CBF方法，将在线学习引入CBF安全滤波器中，实现对模型时变扰动的适应。通过将该方法部署于38g的纳米级四旋翼，进行避障和抗风实验。

Result: 实验证明，该混合自适应CBF控制器能够让纳米四旋翼在18km/h风的干扰下，持续与障碍物保持安全距离，表现出优异的自适应和安全性能。

Conclusion: 神经ODE增强的高阶自适应CBF可实现对复杂扰动环境下的实时安全认证，在不完美模型下提升了实际系统的安全性，对安全控制领域有重要意义。

Abstract: Control barrier functions (CBFs) are an effective model-based tool to formally certify the safety of a system. With the growing complexity of modern control problems, CBFs have received increasing attention in both optimization-based and learning-based control communities as a safety filter, owing to their provable guarantees. However, success in transferring these guarantees to real-world systems is critically tied to model accuracy. For example, payloads or wind disturbances can significantly influence the dynamics of an aerial vehicle and invalidate the safety guarantee. In this work, we propose an efficient yet flexible online learning-enhanced high-order adaptive control barrier function using Neural ODEs. Our approach improves the safety of a CBF-certified system on the fly, even under complex time-varying model perturbations. In particular, we deploy our hybrid adaptive CBF controller on a 38g nano quadrotor, keeping a safe distance from the obstacle, against 18km/h wind.

</details>


### [231] [Flow-Based Path Planning for Multiple Homogenous UAVs for Outdoor Formation-Flying](https://arxiv.org/abs/2511.19653)
*Mahmud Suhaimi Ibrahim,Shantanu Rahman,Muhammad Samin Hasan,Minhaj Uddin Ahmad,Abdullah Abrar*

Main category: cs.RO

TL;DR: 本文提出了一种基于流网络的多无人机编队飞行的无碰撞路径规划方法，经过仿真和实际验证，表现出良好的避障效果。


<details>
  <summary>Details</summary>
Motivation: 多无人机编队飞行广泛应用于安防、巡检和救援等领域，如何保证大量无人机在变化队形时不发生碰撞是实际部署的核心难题。

Method: 将物理GPS坐标映射为流网络图，通过图算法找到最小代价（最短路径）的无碰撞路径，并用Ford-Fulkerson算法确保最大流（即同时最多无人机的无碰撞路径）。方法在多种队形和数量无人机情况下进行仿真，并用3架四旋翼进行实验验证。

Result: 仿真支持最多64架无人机在不同队形下安全变换。实际三机实验进一步证明该算法具备物理可行性和有效性，能实现无碰撞路径规划。

Conclusion: 文中方法能有效为大规模无人机编队生成无碰撞路径，经过仿真和实物实验验证，具有良好的安全性和实用性。

Abstract: Collision-free path planning is the most crucial component in multi-UAV formation-flying (MFF). We use unlabeled homogenous quadcopters (UAVs) to demonstrate the use of a flow network to create complete (inter-UAV) collision-free paths. This procedure has three main parts: 1) Creating a flow network graph from physical GPS coordinates, 2) Finding a path of minimum cost (least distance) using any graph-based path-finding algorithm, and 3) Implementing the Ford-Fulkerson Method to find the paths with the maximum flow (no collision). Simulations of up to 64 UAVs were conducted for various formations, followed by a practical experiment with 3 quadcopters for testing physical plausibility and feasibility. The results of these tests show the efficacy of this method's ability to produce safe, collision-free paths.

</details>


### [232] [Development of a Testbed for Autonomous Vehicles: Integrating MPC Control with Monocular Camera Lane Detection](https://arxiv.org/abs/2511.19655)
*Shantanu Rahman,Nayeb Hasin,Mainul Islam*

Main category: cs.RO

TL;DR: 本文提出了一种结合车道线识别与模型预测控制（MPC）的自主车辆轨迹控制方法，并通过仿真和现实环境实验验证其在轨迹跟踪精度与稳定性上的提升。


<details>
  <summary>Details</summary>
Motivation: 现有的自主车辆广泛采用Ackermann转向机制，且在复杂道路和场景下，提高低层级控制的轨迹跟踪精度和稳定性仍然是一个挑战，因此需要改进现有控制方法以适应实际应用。

Method: 采用摄像头进行车道线识别，通过边缘检测、滑动窗口等方法提取车道线与动态ROI；然后利用基于自行车模型建立的MPC进行车辆运动控制；在ROS Gazebo环境下完成仿真实验，并进行了现实车辆测试。

Result: 新方法在轨迹跟踪任务中，将最优轨迹与目标轨迹的均方根误差降低了27.65%，验证了控制器的高鲁棒性和灵活性。

Conclusion: 结合车道线识别与MPC的控制方法能够有效提升自主车辆的轨迹跟踪能力，并具备良好的泛化性及实际应用潜力。

Abstract: Autonomous vehicles are becoming popular day by day not only for autonomous road traversal but also for industrial automation, farming and military. Most of the standard vehicles follow the Ackermann style steering mechanism. This has become to de facto standard for large and long faring vehicles. The local planner of an autonomous vehicle controls the low-level vehicle movement upon which the vehicle will perform its motor actuation. In our work, we focus on autonomous vehicles in road and perform experiments to analyze the effect of low-level controllers in the simulation and a real environment. To increase the precision and stability of trajectory tracking in autonomous cars, a novel method that combines lane identification with Model Predictive Control (MPC) is presented. The research focuses on camera-equipped autonomous vehicles and uses methods like edge recognition, sliding window-based straight-line identification for lane line extraction, and dynamic region of interest (ROI) extraction. Next, to follow the identified lane line, an MPC built on a bicycle vehicle dynamics model is created. A single-lane road simulation model is built using ROS Gazebo and tested in order to verify the controller's performance. The root mean square error between the optimal tracking trajectory and the target trajectory was reduced by 27.65% in the simulation results, demonstrating the high robustness and flexibility of the developed controller.

</details>


### [233] [Multi-Agent gatekeeper: Safe Flight Planning and Formation Control for Urban Air Mobility](https://arxiv.org/abs/2511.19691)
*Thomas Marshall Vielmetti,Devansh R Agrawal,Dimitra Panagou*

Main category: cs.RO

TL;DR: 提出了一种多智能体门卫（Multi-Agent gatekeeper）框架，能够为三维复杂环境下的领航-跟随编队控制提供可证明的安全保障。该方法通过结合预计算安全轨迹和在线控制，实现了在安全性和适应性之间的平衡，并在模拟和实物四旋翼队伍中验证了其实用性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体编队控制方法在安全性与灵活性间存在权衡：在线规划控制方法往往缺乏严格的安全保证，而离线规划方法则难以适应智能体数量或编队形状的变化。需要一种既能适应编队变化又可提供安全保证的解决方案。

Method: 提出一种混合架构：由一名领航者跟踪预先计算好的安全轨迹，该轨迹作为所有跟随者的备选轨迹集。跟随者通过编队跟踪控制器运动，遇危险时可随时沿领航者轨迹执行已知安全的备选动作。通过形式化证明保障系统对静态障碍物与其他智能体都能避免碰撞。

Result: 在三维城市环境模拟中进行100组随机测试，实现100%避碰成功率，大幅优于CBF和NMPC基线方法；在实际四旋翼编队平台上展示了轨迹的物理可实现性。

Conclusion: 所提多智能体门卫算法有效解决了多智能体编队在三维复杂环境中的安全性和适应性难题，为实际多智能体应用（如无人机编队）提供了新型、可行且性能优越的解决方案。

Abstract: We present Multi-Agent gatekeeper, a framework that provides provable safety guarantees for leader-follower formation control in cluttered 3D environments. Existing methods face a trad-off: online planners and controllers lack formal safety guarantees, while offline planners lack adaptability to changes in the number of agents or desired formation. To address this gap, we propose a hybrid architecture where a single leader tracks a pre-computed, safe trajectory, which serves as a shared trajectory backup set for all follower agents. Followers execute a nominal formation-keeping tracking controller, and are guaranteed to remain safe by always possessing a known-safe backup maneuver along the leader's path. We formally prove this method ensures collision avoidance with both static obstacles and other agents. The primary contributions are: (1) the multi-agent gatekeeper algorithm, which extends our single-agent gatekeeper framework to multi-agent systems; (2) the trajectory backup set for provably safe inter-agent coordination for leader-follower formation control; and (3) the first application of the gatekeeper framework in a 3D environment. We demonstrate our approach in a simulated 3D urban environment, where it achieved a 100% collision-avoidance success rate across 100 randomized trials, significantly outperforming baseline CBF and NMPC methods. Finally, we demonstrate the physical feasibility of the resulting trajectories on a team of quadcopters.

</details>


### [234] [Whole-Body Inverse Dynamics MPC for Legged Loco-Manipulation](https://arxiv.org/abs/2511.19709)
*Lukas Molnar,Jin Cheng,Gabriele Fadini,Dongho Kang,Fatemeh Zargarbashi,Stelian Coros*

Main category: cs.RO

TL;DR: 本文提出了一种全身模型预测控制（MPC）框架，实现了四足机器人带机械臂的统一动作和受力规划与控制，能在真实环境下高效完成复杂操作任务。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人行走和操作任务通常是分开处理的，很难同时兼顾复杂的动力学约束和实际物理限制，因此亟需一种能够协调全身运动和操作任务的统一方法。

Method: 作者提出了基于全阶逆动力学的MPC方法，直接以关节力矩为优化对象，在单一层次内同时处理机器人运动和操作受力规划。算法基于Pinocchio和CasADi软件，并采用高效内点法求解器Fatrop实现。

Result: 在Unitree B2四足机器人加装Z1机械臂的实验中，该MPC方法实现在80Hz频率下的实时控制，顺利完成了拉重物、推箱子和擦白板等多种需要精细末端力与位置控制的物理交互任务。

Conclusion: 提出的统一MPC框架能够在保持行走稳定的前提下实现高度协调的全身操作，为多任务四足机器人的实际应用提供了高效可行的解决方案。

Abstract: Loco-manipulation demands coordinated whole-body motion to manipulate objects effectively while maintaining locomotion stability, presenting significant challenges for both planning and control. In this work, we propose a whole-body model predictive control (MPC) framework that directly optimizes joint torques through full-order inverse dynamics, enabling unified motion and force planning and execution within a single predictive layer. This approach allows emergent, physically consistent whole-body behaviors that account for the system's dynamics and physical constraints. We implement our MPC formulation using open software frameworks (Pinocchio and CasADi), along with the state-of-the-art interior-point solver Fatrop. In real-world experiments on a Unitree B2 quadruped equipped with a Unitree Z1 manipulator arm, our MPC formulation achieves real-time performance at 80 Hz. We demonstrate loco-manipulation tasks that demand fine control over the end-effector's position and force to perform real-world interactions like pulling heavy loads, pushing boxes, and wiping whiteboards.

</details>


### [235] [Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation](https://arxiv.org/abs/2511.19859)
*Xiangkai Ma,Lekai Xing,Han Zhang,Wenzhong Li,Sanglu Lu*

Main category: cs.RO

TL;DR: 本文提出了VITA框架，将视觉信息与动作生成对齐，在多个机器人任务环境上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型虽然表现优异，但仅依赖文本的CoT方法无法捕捉复杂环境下的场景细节，且现有基于视觉先验的方法存在视觉和动作间的模态鸿沟与训练不稳定等问题。

Method: 作者提出Vision-Integrated Trajectory Alignment (VITA) 框架，构建了视觉和动作共享的离散潜在空间，实现感知与运动控制的联合建模。VITA通过隐式视觉CoT机制，将自回归生成的token同时解码为未来帧预测与机器人动作，把视觉动态内化为运动规划的归纳偏置。

Result: 在CALVIN、LIBERO和SimplerEnv基准环境中，VITA分别比现有方法提升14.5%、9.6%、12.1%；在六个现实世界任务中平均成功率达80.5%。

Conclusion: VITA有效弥合视觉观察与动作生成间的鸿沟，并提升模态对齐与训练稳定性，展示了其在通用机器人操作模型中的潜力。

Abstract: Vision-Language-Action (VLA) models built upon Chain-of-Thought (CoT) have achieved remarkable success in advancing general-purpose robotic agents, owing to its significant perceptual comprehension. Recently, since text-only CoT struggles to adequately capture scene details in complex spatial environments, a highly promising strategy involves leveraging visual priors to guide robotic action generation. Nevertheless, these strategies face two inherent challenges: (i) a modality gap between visual observations and low-level actions, and (ii) unstable training due to competing objectives between visual prediction and action generation. To address these challenges, we propose a Vision-Integrated Trajectory Alignment (VITA) framework that learns a shared discrete latent space for vision and action, enabling joint modeling of perception and motor control. VITA introduces a implicit visual CoT: autoregressively generated tokens is simultaneously decoded into future frames predictions and robot actions, thereby internalizing visual dynamics as an inductive bias for motion planning. Extensive experiments on simulated and real-world environments demonstrate state-of-the-art performance. VITA improves 14.5\%, 9.6\% and 12.1\% over existing baselines on CALVIN, LIBERO and SimplerEnv. Furthermore, VITA attains an average success rate of 80.5\% across six real-world tasks, demonstrating its potential as a generalist robotic manipulation model.

</details>


### [236] [Human-Centered Cooperative Control Coupling Autonomous and Haptic Shared Control via Control Barrier Function](https://arxiv.org/abs/2511.19869)
*Eito Sato,Takahiro Wada*

Main category: cs.RO

TL;DR: 本文提出了一种在遥操作中结合独立自主控制与触觉共享控制（HSC）的协作框架，并通过实验验证其提升了控制精度和效率。


<details>
  <summary>Details</summary>
Motivation: 在遥操作中，完全自主存在不确定或感知受限等局限性。HSC能改善遥操作效果，但仅依靠增强HSC无法进一步提高自主控制性能，因为操纵杆和人臂动态会影响机器人表现。因此，亟需新的框架突破该瓶颈。

Method: 提出了一种协作框架，将不依赖操纵杆的自主控制器与HSC结合。具体方法是：通过控制障碍函数，在操作员判定安全的区域内忽略操纵杆输入，由自主控制器接管；其余情况下启用HSC。

Result: 在虚拟环境中用水下机器人进行遥操作仿真实验，结果显示新方法在精度提升和所需操作时间减少方面均优于传统HSC。

Conclusion: 该协作框架能有效结合自主与共享控制优点，提高遥操作系统的性能，验证了其实用性和潜力。

Abstract: Haptic shared control (HSC) is effective in teleoperation when full autonomy is limited by uncertainty or sensing constraints. However, autonomous control performance achieved by maximizing HSC strength is limited because the dynamics of the joystick and human arm affect the robot's behavior. We propose a cooperative framework coupling a joystick-independent autonomous controller with HSC. A control barrier function ignores joystick inputs within a safe region determined by the human operator in real-time, while HSC is engaged otherwise. A pilot experiment on simulated tasks with tele-operated underwater robot in virtual environment demonstrated improved accuracy and reduced required time over conventional HSC.

</details>


### [237] [CoC-VLA: Delving into Adversarial Domain Transfer for Explainable Autonomous Driving via Chain-of-Causality Visual-Language-Action Model](https://arxiv.org/abs/2511.19914)
*Dapeng Zhang,Fei Shen,Rui Zhao,Yinda Chen,Peng Zhi,Chenyang Li,Rui Zhou,Qingguo Zhou*

Main category: cs.RO

TL;DR: 本文提出了一种新的视觉-语言模型（VLM）指导的端到端对抗迁移框架（CoC-VLA），能将仿真环境下对复杂、长尾驾驶场景的处理能力迁移到真实世界的自动驾驶系统中。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶研究正从常规场景向包含微妙人类行为、交通事故和违法驾驶等复杂长尾场景拓展。虽然大语言模型（LLM）在多模态理解和推理等方面表现突出，但现有方法通常只利用了真实数据或仿真数据，很少能有效结合两者的优势。

Method: 提出一个包含教师、学生两个视觉-语言模型和一个判别器的端到端对抗迁移架构。基础架构为CoC VLM，能通过文本适配器集成时序信息，并进行链式推理。教师模型在仿真数据上训练，学生模型在真实数据上训练。通过判别器和新颖的反向传播策略，实现仿真到真实场景的长尾能力迁移。

Result: 该框架能够有效地将应对复杂、罕见驾驶场景的能力从仿真环境转移到真实世界，从而提升了自动驾驶系统在各种驾驶场景下的表现。

Conclusion: 结合仿真与真实数据，通过创新的VLM对抗迁移方法，提升了自动驾驶面对长尾复杂场景的应对能力，为进一步推进自动驾驶落地提供了新思路。

Abstract: Autonomous driving represents a prominent application of artificial intelligence. Recent approaches have shifted from focusing solely on common scenarios to addressing complex, long-tail situations such as subtle human behaviors, traffic accidents, and non-compliant driving patterns. Given the demonstrated capabilities of large language models (LLMs) in understanding visual and natural language inputs and following instructions, recent methods have integrated LLMs into autonomous driving systems to enhance reasoning, interpretability, and performance across diverse scenarios. However, existing methods typically rely either on real-world data, which is suitable for industrial deployment, or on simulation data tailored to rare or hard case scenarios. Few approaches effectively integrate the complementary advantages of both data sources. To address this limitation, we propose a novel VLM-guided, end-to-end adversarial transfer framework for autonomous driving that transfers long-tail handling capabilities from simulation to real-world deployment, named CoC-VLA. The framework comprises a teacher VLM model, a student VLM model, and a discriminator. Both the teacher and student VLM models utilize a shared base architecture, termed the Chain-of-Causality Visual-Language Model (CoC VLM), which integrates temporal information via an end-to-end text adapter. This architecture supports chain-of-thought reasoning to infer complex driving logic. The teacher and student VLM models are pre-trained separately on simulated and real-world datasets. The discriminator is trained adversarially to facilitate the transfer of long-tail handling capabilities from simulated to real-world environments by the student VLM model, using a novel backpropagation strategy.

</details>


### [238] [Collaborate sim and real: Robot Bin Packing Learning in Real-world and Physical Engine](https://arxiv.org/abs/2511.19932)
*Lidi Zhang,Han Wu,Liyu Zhang,Ruofeng Liu,Haotian Wang,Chao Li,Desheng Zhang,Yunhuai Liu,Tian He*

Main category: cs.RO

TL;DR: 本论文提出了一种针对3D装箱问题的混合强化学习框架，通过结合物理仿真与真实世界数据反馈，大幅提升了装箱稳定性，并在实际物流系统中取得了明显成效。


<details>
  <summary>Details</summary>
Motivation: 现有3D装箱问题模型多为离散、静态建模，忽略了实际过程中重力等连续物理效应，导致应用不稳定等问题。为解决仿真假设与现实物理动态参数（如摩擦、弹性等）差异带来的部署难题，需引入更贴近实际的方法。

Method: 首先在物理仿真中对物理参数进行域随机化，多方面锻炼强化学习代理的泛化能力；其次利用现实部署反馈，对学习代理进行微调，进一步降低塌陷率。

Result: 实验结果显示该方法在仿真和实际环境中均有效降低了装箱塌陷率，在大规模物流部署中对比基线方法减少了35%的塌陷发生。

Conclusion: 所提混合RL框架能够有效弥合仿真与现实的差距，显著提升3D装箱的工程应用稳定性，具有良好的实际推广价值。

Abstract: The 3D bin packing problem, with its diverse industrial applications, has garnered significant research attention in recent years. Existing approaches typically model it as a discrete and static process, while real-world applications involve continuous gravity-driven interactions. This idealized simplification leads to infeasible deployments (e.g., unstable packing) in practice. Simulations with physical engine offer an opportunity to emulate continuous gravity effects, enabling the training of reinforcement learning (RL) agents to address such limitations and improve packing stability. However, a simulation-to-reality gap persists due to dynamic variations in physical properties of real-world objects, such as various friction coefficients, elasticity, and non-uniform weight distributions. To bridge this gap, we propose a hybrid RL framework that collaborates with physical simulation with real-world data feedback. Firstly, domain randomization is applied during simulation to expose agents to a spectrum of physical parameters, enhancing their generalization capability. Secondly, the RL agent is fine-tuned with real-world deployment feedback, further reducing collapse rates. Extensive experiments demonstrate that our method achieves lower collapse rates in both simulated and real-world scenarios. Large-scale deployments in logistics systems validate the practical effectiveness, with a 35\% reduction in packing collapse compared to baseline methods.

</details>


### [239] [ShapeForce: Low-Cost Soft Robotic Wrist for Contact-Rich Manipulation](https://arxiv.org/abs/2511.19955)
*Jinxuan Zhu,Zihao Yan,Yangyu Xiao,Jingxiang Guo,Chenrui Tie,Xinyi Cao,Yuhang Zheng,Lin Shao*

Main category: cs.RO

TL;DR: 本论文提出了一种低成本且易于使用的软体机器人手腕传感器（ShapeForce），用于在接触丰富的机器人操作中替代价格昂贵、易损坏的六轴力–力矩传感器。


<details>
  <summary>Details</summary>
Motivation: 六轴力–力矩传感器虽然能提供精确的接触反馈，但价格高昂且易碎，限制了其在实际接触丰富机器人任务中的普及。因此，研究动机在于开发一种更经济、实用的替代方案，使更多机器人研究者和开发者能够便捷获取接触反馈。

Method: 设计了一种软体手腕传感器ShapeForce，其灵感来自人类通过感知接触力变化而非绝对数值进行操作。该装置通过其柔性结构将外力和力矩转化为形变，并通过标记检测追踪形变，生成类似于力的信号，无需精密校准和专用电子设备。

Result: 在多种接触丰富的操作任务和策略中，ShapeForce取得了与传统六轴力–力矩传感器相媲美的性能表现，同时成本极低。

Conclusion: ShapeForce为机器人感知接触变化提供了一种低价、即插即用的新选择，有望促进接触反馈应用的普及和相关领域的发展。

Abstract: Contact feedback is essential for contact-rich robotic manipulation, as it allows the robot to detect subtle interaction changes and adjust its actions accordingly. Six- axis force-torque sensors are commonly used to obtain contact feedback, but their high cost and fragility have discouraged many researchers from adopting them in contact-rich tasks. To offer a more cost-efficient and easy-accessible source of contact feedback, we present ShapeForce, a low-cost, plug-and-play soft wrist that provides force-like signals for contact-rich robotic manipulation. Inspired by how humans rely on relative force changes in contact rather than precise force magnitudes, ShapeForce converts external force and torque into measurable deformations of its compliant core, which are then estimated via marker-based pose tracking and converted into force-like signals. Our design eliminates the need for calibration or specialized electronics to obtain exact values, and instead focuses on capturing force and torque changes sufficient for enabling contact-rich manipulation. Extensive experiments across diverse contact-rich tasks and manipulation policies demonstrate that ShapeForce delivers performance comparable to six-axis force-torque sensors at an extremely low cost.

</details>


### [240] [Active3D: Active High-Fidelity 3D Reconstruction via Hierarchical Uncertainty Quantification](https://arxiv.org/abs/2511.20050)
*Yan Li,Yingzhao Li,Gim Hee Lee*

Main category: cs.RO

TL;DR: 本论文提出了一种用于高保真3D重建的主动探索框架，通过不确定性驱动的运动规划选择最佳下一个视角，实现高效精确的重建。


<details>
  <summary>Details</summary>
Motivation: 当前3D重建方法在处理大规模环境时，存在全局与局部信息表达不足，以及如何高效选择采集视角的问题。为了解决这些在真实场景和机器人感知中的挑战，作者希望提升3D重建的准确性和探索效率。

Method: 作者引入混合隐式-显式表示，将神经隐式场与高斯基元融合，既捕捉全局结构先验又精细表达局部细节。在此基础上建立分层不确定性体积，联合量化全局和局部的重建信心。进一步提出了基于不确定性驱动的关键帧选择和滑动窗口策略，关注信息量最大的区域。规划模块则将视角选择建模为信息增益最大化问题，并引入风险敏感路线规划。

Result: 在多个具有挑战性的基准数据集上实验，提出的方法在精度、完整性和渲染质量等方面均取得了最新最优表现。

Conclusion: 该方法有效提升了主动3D重建和机器人感知任务的高质量重建能力，尤其适用于真实场景下高效、动态的探索。

Abstract: In this paper, we present an active exploration framework for high-fidelity 3D reconstruction that incrementally builds a multi-level uncertainty space and selects next-best-views through an uncertainty-driven motion planner. We introduce a hybrid implicit-explicit representation that fuses neural fields with Gaussian primitives to jointly capture global structural priors and locally observed details. Based on this hybrid state, we derive a hierarchical uncertainty volume that quantifies both implicit global structure quality and explicit local surface confidence. To focus optimization on the most informative regions, we propose an uncertainty-driven keyframe selection strategy that anchors high-entropy viewpoints as sparse attention nodes, coupled with a viewpoint-space sliding window for uncertainty-aware local refinement. The planning module formulates next-best-view selection as an Expected Hybrid Information Gain problem and incorporates a risk-sensitive path planner to ensure efficient and safe exploration. Extensive experiments on challenging benchmarks demonstrate that our approach consistently achieves state-of-the-art accuracy, completeness, and rendering quality, highlighting its effectiveness for real-world active reconstruction and robotic perception tasks.

</details>


### [241] [Hibikino-Musashi@Home 2025 Team Description Paper](https://arxiv.org/abs/2511.20180)
*Ryohei Kobayashi,Kosei Isomoto,Kosei Yamao,Soma Fumoto,Koshun Arimura,Naoki Yamaguchi,Akinobu Mizutani,Tomoya Shiba,Kouki Kimizuka,Yuta Ohno,Ryo Terashima,Hiromasa Yamaguchi,Tomoaki Fujino,Ryoga Maruno,Wataru Yoshimura,Kazuhito Mine,Tang Phu Thien Nhan,Yuga Yano,Yuichiro Tanaka,Takeshi Nishida,Takashi Morie,Hakaru Tamukoh*

Main category: cs.RO

TL;DR: 本文介绍了Hibikino-Musashi@Home团队为参加国内标准平台联赛所采用的机器人技术，包括用于视觉系统训练的数据集生成器、基于大语言模型的任务规划器、仿脑记忆模型及导航系统复用等。


<details>
  <summary>Details</summary>
Motivation: 团队致力于开发能够在家庭环境中为人类提供直观、个性化帮助的服务机器人，并通过持续参赛以验证和提升系统性能。

Method: 主要方法包括：1）开发数据集生成器以提升机器人视觉；2）利用大语言模型驱动的任务规划器，从原始技能库中选择以完成用户请求的任务；3）研究仿脑记忆模型以适应不同家庭环境；4）采用开源的仿真开发环境及复用了RoboCup2024中Pumas研发的导航系统。

Result: 团队已研发出可训练机器人视觉系统的数据集生成器、任务规划器及仿脑记忆适应模型，并推动了导航系统的可复用性。

Conclusion: 通过系列技术研发，团队为家庭服务机器人系统构建了坚实基础，并计划通过参赛不断优化系统，推进行业发展。

Abstract: This paper provides an overview of the techniques employed by Hibikino-Musashi@Home, which intends to participate in the domestic standard platform league. The team developed a dataset generator for training a robot vision system and an open-source development environment running on a Human Support Robot simulator. The large-language-model-powered task planner selects appropriate primitive skills to perform the task requested by the user. Moreover, the team has focused on research involving brain-inspired memory models for adaptation to individual home environments. This approach aims to provide intuitive and personalized assistance. Additionally, the team contributed to the reusability of the navigation system developed by Pumas in RoboCup2024. The team aimed to design a home service robot to assist humans in their homes and continuously attend competitions to evaluate and improve the developed system.

</details>


### [242] [Toward generic control for soft robotic systems](https://arxiv.org/abs/2511.20226)
*Yu Sun,Yaosheng Deng,Wenjie Mei,Xiaogang Xiong,Yang Bai,Masaki Ogura,Zeyu Zhou,Mir Feroskhan,Michael Yu Wang,Qiyang Zuo,Yao Li,Yunjiang Lou*

Main category: cs.RO

TL;DR: 本文提出了一种通用的软体机器人控制框架，通过转变传统刚体控制思路，主动利用控制顺应性，实现了跨平台、稳健和安全的机器人行为。


<details>
  <summary>Details</summary>
Motivation: 当前软体机器人控制方法碎片化，不同形态和驱动机制需要专用控制器，缺乏理论统一，阻碍了大规模应用。现有刚体控制逻辑依赖精确模型，无法适应软体机器人顺应性的本质。需要新的控制框架来克服这些挑战。

Method: 作者借鉴人类运动控制原理，提出以控制顺应性为核心的通用软体机器人控制架构，突出高层次运动意图表达，底层由顺应性机制自主处理局部细节，并在多类型、多驱动的软体机器人上进行了验证。

Result: 实验证明，该框架能够保证不同软体机器人平台上的行为稳定、安全及转移性强，有效适应不同的结构和驱动方式。

Conclusion: 主动利用控制顺应性，而不是试图消除它，能够为实现统一的软体机器人控制提供坚实基础，有助于实现软体机器人领域的理论整合和广泛应用。

Abstract: Soft robotics has advanced rapidly, yet its control methods remain fragmented: different morphologies and actuation schemes still require task-specific controllers, hindering theoretical integration and large-scale deployment. A generic control framework is therefore essential, and a key obstacle lies in the persistent use of rigid-body control logic, which relies on precise models and strict low-level execution. Such a paradigm is effective for rigid robots but fails for soft robots, where the ability to tolerate and exploit approximate action representations, i.e., control compliance, is the basis of robustness and adaptability rather than a disturbance to be eliminated. Control should thus shift from suppressing compliance to explicitly exploiting it. Human motor control exemplifies this principle: instead of computing exact dynamics or issuing detailed muscle-level commands, it expresses intention through high-level movement tendencies, while reflexes and biomechanical mechanisms autonomously resolve local details. This architecture enables robustness, flexibility, and cross-task generalization. Motivated by this insight, we propose a generic soft-robot control framework grounded in control compliance and validate it across robots with diverse morphologies and actuation mechanisms. The results demonstrate stable, safe, and cross-platform transferable behavior, indicating that embracing control compliance, rather than resisting it, may provide a widely applicable foundation for unified soft-robot control.

</details>


### [243] [HAFO: Humanoid Force-Adaptive Control for Intense External Force Interaction Environments](https://arxiv.org/abs/2511.20275)
*Chenhui Dong,Haozhe Xu,Wenhao Feng,Zhipeng Wang,Yanmin Zhou,Yifei Zhao,Bin He*

Main category: cs.RO

TL;DR: 本论文提出了HAFO，一个双智能体强化学习控制框架，实现了仿人机器人在外力强干扰下稳定行走和精确上肢操作，显著提升了其负载与抗扰动能力。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习已在机器人的行走与轻负载操作方面取得进展，但面对强外力干扰时，依然难以实现既鲁棒又精确的运动控制。本研究旨在解决这一实际应用中的关键难题。

Method: 方法上，提出了HAFO——一个将地面行走与上肢操作分别由两个智能体协同优化的框架，并在训练中引入弹簧-阻尼系统建模外部拉力，实现精细力控制。通过强化学习，策略能够自动产生抗扰反应。此外，采用不对称Actor-Critic结构，Critic可访问特权力信息以指导Actor学习更强的泛化鲁棒策略。

Result: 实验表明，HAFO能够使仿人机器人在多种强外力干扰任务中保持稳定，包括承受绳索张力等负载，对比现有方法有显著性能提升。

Conclusion: HAFO框架显著提升了机器人在高强度外力交互下的稳定性与精确性，为实际强力操作和负载任务中的仿人机器人控制提供了新路径和理论基础。

Abstract: Reinforcement learning controllers have made impressive progress in humanoid locomotion and light load manipulation. However, achieving robust and precise motion with strong force interaction remains a significant challenge. Based on the above limitations, this paper proposes HAFO, a dual-agent reinforcement learning control framework that simultaneously optimizes both a robust locomotion strategy and a precise upper-body manipulation strategy through coupled training under external force interaction environments. Simultaneously, we explicitly model the external pulling disturbances through a spring-damper system and achieve fine-grained force control by manipulating the virtual spring. During this process, the reinforcement-learning policy spontaneously generates disturbance-rejection response by exploiting environmental feedback. Moreover, HAFO employs an asymmetric Actor-Critic framework in which the Critic-network access to privileged spring-damping forces guides the actor-network to learn a generalizable, robust policy for resisting external disturbances. The experimental results demonstrate that HAFO achieves stable control of humanoid robot under various strong force interactions, showing remarkable performance in load tasks and ensuring stable robot operation under rope tension disturbances. Project website: hafo-robot.github.io.

</details>


### [244] [Dynamic-ICP: Doppler-Aware Iterative Closest Point Registration for Dynamic Scenes](https://arxiv.org/abs/2511.20292)
*Dong Wang,Daniel Casado Herraez,Stefan May,Andreas Nüchter*

Main category: cs.RO

TL;DR: 本文提出了一种面向多普勒信息的点云配准框架Dynamic-ICP，专为高动态环境下提升激光雷达定位的可靠性而设计。


<details>
  <summary>Details</summary>
Motivation: 现有基于ICP的点云配准方法在动态、重复或低纹理场景中表现较差，因为它们假设场景近乎静态，且难以处理包含运动物体的复杂环境。

Method: Dynamic-ICP方法包括：（1）通过鲁棒回归从每个点的多普勒速度估计自车运动并构建速度滤波器；（2）对动态目标进行聚类，并根据去除自车运动影响后的径向速度重建各目标的平移速度；（3）利用恒速模型预测动态点的位置；（4）通过融合点到平面的几何残差及仅依赖旋转的多普勒残差，实现紧凑的目标函数优化配准。该方法完全基于FMCW激光雷达的距离和多普勒速度数据，无需外部传感器或额外校准。

Result: 在HeRCULES、HeLiPR和AevaScenes等三个包含高动态场景的数据集上，Dynamic-ICP在旋转稳定性和位置精度方面均优于当前的先进方法。

Conclusion: Dynamic-ICP不仅能实时运行，易于集成到现有系统，还为动态环境下的鲁棒点云配准提供了一个高效且轻量级的方案。研究成果代码已开源，便利后续研究。

Abstract: Reliable odometry in highly dynamic environments remains challenging when it relies on ICP-based registration: ICP assumes near-static scenes and degrades in repetitive or low-texture geometry. We introduce Dynamic-ICP, a Doppler-aware registration framework. The method (i) estimates ego motion from per-point Doppler velocity via robust regression and builds a velocity filter, (ii) clusters dynamic objects and reconstructs object-wise translational velocities from ego-compensated radial measurements, (iii) predicts dynamic points with a constant-velocity model, and (iv) aligns scans using a compact objective that combines point-to-plane geometry residual with a translation-invariant, rotation-only Doppler residual. The approach requires no external sensors or sensor-vehicle calibration and operates directly on FMCW LiDAR range and Doppler velocities. We evaluate Dynamic-ICP on three datasets-HeRCULES, HeLiPR, AevaScenes-focusing on highly dynamic scenes. Dynamic-ICP consistently improves rotational stability and translation accuracy over the state-of-the-art methods. Our approach is also simple to integrate into existing pipelines, runs in real time, and provides a lightweight solution for robust registration in dynamic environments. To encourage further research, the code is available at: https://github.com/JMUWRobotics/Dynamic-ICP.

</details>


### [245] [How Robot Kinematics Influence Human Performance in Virtual Robot-to-Human Handover Tasks](https://arxiv.org/abs/2511.20299)
*Róisín Keenan,Joost C. Dessing*

Main category: cs.RO

TL;DR: 本研究利用VR仿真平台，系统考察了在人机协作交接任务中，影响人类运动表现的多种机器人动态因素。结果显示，机器人提供显著而早期的视觉信息及采用类人顺滑运动轨迹，能提升人与机器的协作同步与预测准确性。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在与人类协同工作的场景中日益普及，人机协调的问题变得尤为重要。本文旨在探索如何优化人机交互，特别是交接任务中，机器人运动特性的不同设定对人类表现的影响，从而提升人机协作效率与体验。

Method: 作者通过VR平台，设定了安全、可控的人机交互交接实验。分别考察四种因素：(1) 任务启动控制权和机器人的时空同步性；(2) 机器人外观（类人或机器人）；(3) 机器人速度轨迹（最小跃度、恒速、恒加速度、双相速度）；(4) 旋转物体运动的时序。通过多组对照实验量化各因素对人类运动表现的影响。

Result: 实验证明，与人协调的机器人如能早早提供明确的视觉提示、并采用顺滑（类人）的运动轨迹，可帮助人类更好地预测与同步机器人动作。这些设定在不同实验中均有正面效果，显著提升了交互表现。

Conclusion: 人机交互系统应优先考虑让机器人向人清晰传递与任务相关的物体动态信息，并采用类人的平滑运动方式，借助人类擅长生物运动感知的天赋，从而优化协作，而不必复杂化机器人的算法或对人类提出额外适应要求。

Abstract: Recent advancements in robotics have increased the possibilities for integrating robotic systems into human-involved workplaces, highlighting the need to examine and optimize human-robot coordination in collaborative settings. This study explores human-robot interactions during handover tasks using Virtual Reality (VR) to investigate differences in human motor performance across various task dynamics and robot kinematics. A VR-based robot handover simulation afforded safe and controlled assessments of human-robot interactions. In separate experiments, four potential influences on human performance were examined (1) control over task initiation and robot movement synchrony (temporal and spatiotemporal); (2) partner appearance (human versus robotic); (3) robot velocity profiles (minimum jerk, constant velocity, constant acceleration, and biphasic); and (4) the timing of rotational object motion. Findings across experiments emphasize humans benefit from robots providing early and salient visual information about task-relevant object motion, and advantages of human-like smooth robot trajectories. To varying degrees, these manipulations improved predictive accuracy and synchronization during interaction. This suggests that human-robot interactions should be designed to allow humans to leverage their natural capabilities for detecting biological motion, which conversely may reduce the need for costly robotic computations or added cognitive adaptation on the human side.

</details>


### [246] [ArtiBench and ArtiBrain: Benchmarking Generalizable Vision-Language Articulated Object Manipulation](https://arxiv.org/abs/2511.20330)
*Yuhan Wu,Tiantian Wei,Shuo Wang,ZhiChao Wang,Yanyong Zhang,Daniel Cremers,Yan Xia*

Main category: cs.RO

TL;DR: 本文提出了ArtiBench基准和ArtiBrain框架，旨在解决多步、多类别交互式关节操控中的泛化难题，并在新基准上取得了显著优异的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言和扩散模型在处理不同零件、实例及类别的交互时泛化能力有限，难以实现长时序、物理一致的复杂操控任务。

Method: 1）构建了包含厨房、储物、办公和工具环境的五级新基准ArtiBench，系统评测操控任务的泛化能力。2）提出了ArtiBrain模块化框架：高层用基于VLM（如GPT-4.1）的任务推理器分解和校验子目标，低层采用融合几何关键帧与可供性扩散的混合控制器精细操控。3）设计了‘可供性记忆库’，积累成功经验，实现对未见零件和配置的泛化。

Result: 在ArtiBench基准上的大量实验表明，ArtiBrain在泛化和鲁棒性上显著优于当前主流多模态及扩散控制方法。

Conclusion: 综合的高/低层次推理与控制、可供性记忆机制，能更好地泛化至新对象和任务，为交互式关节操控提供了更强大通用的解决方案。

Abstract: Interactive articulated manipulation requires long-horizon, multi-step interactions with appliances while maintaining physical consistency. Existing vision-language and diffusion-based policies struggle to generalize across parts, instances, and categories. We first introduce ArtiBench, a five-level benchmark covering kitchen, storage, office, and tool environments. ArtiBench enables structured evaluation from cross-part and cross-instance variation to long-horizon multi-object tasks, revealing the core generalization challenges of articulated object manipulation. Building on this benchmark, we propose ArtiBrain, a modular framework that unifies high-level reasoning with adaptive low-level control. ArtiBrain uses a VLM-based Task Reasoner (GPT-4.1) to decompose and validate subgoals, and employs a Hybrid Controller that combines geometry-aware keyframe execution with affordance-guided diffusion for precise and interpretable manipulation. An Affordance Memory Bank continually accumulates successful execution episodes and propagates part-level actionable affordances to unseen articulated parts and configurations. Extensive experiments on ArtiBench show that our ArtiBrain significantly outperforms state-of-the-art multimodal and diffusion-based methods in robustness and generalization. Code and dataset will be released upon acceptance.

</details>


### [247] [Quality-guided UAV Surface Exploration for 3D Reconstruction](https://arxiv.org/abs/2511.20353)
*Benjamin Sportich,Kenza Boubakri,Olivier Simonin,Alessandro Renzaglia*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的模块化NBV（Next-Best-View）规划框架，用于无人机自主探索和环境三维重建，通过显式质量目标指导规划，实现高效、适应性强的探测与建图，并在仿真中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在实际无人机自主探索和环境重建中，用户的具体需求（如快速信息采集或高质量结构分析）往往被忽视，导致规划策略不适应目标差异。因此，亟需能够根据不同重建质量需求灵活调整探索行为的方法。

Method: 提出了一种基于TSDF（截断有符号距离场）表征环境不确定性的NBV规划框架。该方法包括面向用户定义的重建质量要求的新型视点生成与候选点选择机制，模块化架构可灵活适配不同任务目标。

Result: 仿真实验表明，所提方法能根据用户目标调整探索行为，并在覆盖率、三维地图质量和路径效率等指标上，始终优于传统NBV策略。

Conclusion: 本研究提出的NBV规划框架提升了无人机探索与建图的适应性和效率，能够满足不同场景下的多样化重建需求，具有实际应用价值。

Abstract: Reasons for mapping an unknown environment with autonomous robots are wide-ranging, but in practice, they are often overlooked when developing planning strategies. Rapid information gathering and comprehensive structural assessment of buildings have different requirements and therefore necessitate distinct methodologies. In this paper, we propose a novel modular Next-Best-View (NBV) planning framework for aerial robots that explicitly uses a reconstruction quality objective to guide the exploration planning. In particular, our approach introduces new and efficient methods for view generation and selection of viewpoint candidates that are adaptive to the user-defined quality requirements, fully exploiting the uncertainty encoded in a Truncated Signed Distance field (TSDF) representation of the environment. This results in informed and efficient exploration decisions tailored towards the predetermined objective. Finally, we validate our method via extensive simulations in realistic environments. We demonstrate that it successfully adjusts its behavior to the user goal while consistently outperforming conventional NBV strategies in terms of coverage, quality of the final 3D map and path efficiency.

</details>


### [248] [Improved adaptive wind driven optimization algorithm for real-time path planning](https://arxiv.org/abs/2511.20394)
*Shiqian Liu,Azlan Mohd Zain,Le-le Mao*

Main category: cs.RO

TL;DR: 本文提出了一种多层次自适应风驱动优化算法（MAWDO），提高了动态环境下路径规划的自适应性、鲁棒性以及收敛能力。经过大量基准测试及实际路径规划实验，其性能优于现有主流算法。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中实现机器人实时、无碰撞且高效的路径规划依然很困难。已有方法虽然提升了全局搜索能力和收敛精度，但在适应快速变化环境和复杂约束条件时表现不佳。

Method: 基于风驱动优化（WDO）原理，提出多层次自适应风驱动优化（MAWDO）算法。该算法采用分层引导机制，将种群划分为多个由个体、区域和全局领导者引导的子群体，平衡探索与开发，避免早熟收敛和不稳定。

Result: 在16个标准函数上的评估表明，MAWDO在优化精度、收敛稳定性和适应性方面均优于多种主流元启发式算法。在动态路径规划中，MAWDO生成的路径最短（469.28像素），分别比MEWDO、AWDO和WDO短3.51%、11.63%和14.93%，并实现了最小最优性差距和最高平滑度。

Conclusion: MAWDO算法在动态和复杂环境下实现了更短、更平滑和无碰撞的路径，展现了在实时路径规划中的有效性和优越性。

Abstract: Recently, path planning has achieved remarkable progress in enhancing global search capability and convergence accuracy through heuristic and learning-inspired optimization frameworks. However, real-time adaptability in dynamic environments remains a critical challenge for autonomous navigation, particularly when robots must generate collision-free, smooth, and efficient trajectories under complex constraints. By analyzing the difficulties in dynamic path planning, the Wind Driven Optimization (WDO) algorithm emerges as a promising framework owing to its physically interpretable search dynamics. Motivated by these observations, this work revisits the WDO principle and proposes a variant formulation, Multi-hierarchical adaptive wind driven optimization(MAWDO), that improves adaptability and robustness in time-varying environments. To mitigate instability and premature convergence, a hierarchical-guidance mechanism divides the population into multiple groups guided by individual, regional, and global leaders to balance exploration and exploitation. Extensive evaluations on sixteen benchmark functions show that MAWDO achieves superior optimization accuracy, convergence stability, and adaptability over state-of-the art metaheuristics. In dynamic path planning, MAWDO shortens the path length to 469.28 pixels, improving over Multi-strategy ensemble wind driven optimization(MEWDO), Adaptive wind driven optimization(AWDO) and WDO by 3.51\%, 11.63\% and 14.93\%, and achieves the smallest optimality gap (1.01) with smoothness 0.71 versus 13.50 and 15.67 for AWDO and WDO, leading to smoother, shorter, and collision-free trajectories that confirm its effectiveness for real-time path planning in complex environments.

</details>


### [249] [Power-Efficient Autonomous Mobile Robots](https://arxiv.org/abs/2511.20467)
*Liangkai Liu,Weisong Shi,Kang G. Shin*

Main category: cs.RO

TL;DR: 本文提出了一种新型的自主移动机器人（AMR）电源管理系统pNav，通过联合优化机器人的物理/机械和网络子系统，显著提高了AMR的电源和能效。


<details>
  <summary>Details</summary>
Motivation: 目前AMR的能效管理存在难题：系统功耗分布波动巨大、导航环境变化多端，以及信息/硬件子系统协同不足，影响了整体的能效优化。

Method: pNav系统结合了毫秒级的功耗预测、空间与时间导航位置的实时建模与监测，以及软硬件配置的动态协同，实现了对能效的全面优化。

Result: 在真实机器人和仿真环境中的测试显示，pNav功耗预测准确率超过96%，能耗降低38.1%，且未影响导航精度与安全性。

Conclusion: pNav证明了通过多维联合优化AMR的物理和网络子系统，可以大幅提升其能效且不牺牲性能，在智能机器人管理领域具有广泛应用前景。

Abstract: This paper presents pNav, a novel power-management system that significantly enhances the power/energy-efficiency of Autonomous Mobile Robots (AMRs) by jointly optimizing their physical/mechanical and cyber subsystems. By profiling AMRs' power consumption, we identify three challenges in achieving CPS (cyber-physical system) power-efficiency that involve both cyber (C) and physical (P) subsystems: (1) variabilities of system power consumption breakdown, (2) environment-aware navigation locality, and (3) coordination of C and P subsystems. pNav takes a multi-faceted approach to achieve power-efficiency of AMRs. First, it integrates millisecond-level power consumption prediction for both C and P subsystems. Second, it includes novel real-time modeling and monitoring of spatial and temporal navigation localities for AMRs. Third, it supports dynamic coordination of AMR software (navigation, detection) and hardware (motors, DVFS driver) configurations. pNav is prototyped using the Robot Operating System (ROS) Navigation Stack, 2D LiDAR, and camera. Our in-depth evaluation with a real robot and Gazebo environments demonstrates a >96% accuracy in predicting power consumption and a 38.1% reduction in power consumption without compromising navigation accuracy and safety.

</details>


### [250] [Safe and Stable Neural Network Dynamical Systems for Robot Motion Planning](https://arxiv.org/abs/2511.20593)
*Allen Emmanuel Binny,Mahathi Anand,Hugo T. M. Kussaba,Lingyun Chen,Shreenabh Agrawal,Fares J. Abu-Dakka,Abdalla Swikir*

Main category: cs.RO

TL;DR: 本文提出S$^2$-NNDS框架，利用神经网络在从演示中学习机器人安全稳定动作的同时，保证动力系统的稳定与安全。


<details>
  <summary>Details</summary>
Motivation: 针对复杂、非线性且动态障碍环境中从演示学习安全稳定机器人运动的难题，现有方法多采用限制性强的多项式模型，难以处理高复杂度动作。

Method: 提出S$^2$-NNDS，通过神经网络联合学习表达能力强的动力学系统，同时学习神经Lyapunov稳定性证书和障碍安全（Barrier）证书。并结合概率性保证（split conformal prediction）提升安全性与稳定性。

Result: 在多个2D和3D数据集上（包括LASA手写数据集和Franka Emika Panda机器人演示），S$^2$-NNDS能够从有潜在风险的演示数据中，成功学习出鲁棒、安全且稳定的机器人运动。

Conclusion: S$^2$-NNDS能够突破多项式建模的限制，实现复杂任务下安全、稳健的运动轨迹学习。方法在实际任务中具有良好效果，拓展了基于演示学习安全动力学系统的应用潜力。

Abstract: Learning safe and stable robot motions from demonstrations remains a challenge, especially in complex, nonlinear tasks involving dynamic, obstacle-rich environments. In this paper, we propose Safe and Stable Neural Network Dynamical Systems S$^2$-NNDS, a learning-from-demonstration framework that simultaneously learns expressive neural dynamical systems alongside neural Lyapunov stability and barrier safety certificates. Unlike traditional approaches with restrictive polynomial parameterizations, S$^2$-NNDS leverages neural networks to capture complex robot motions providing probabilistic guarantees through split conformal prediction in learned certificates. Experimental results on various 2D and 3D datasets -- including LASA handwriting and demonstrations recorded kinesthetically from the Franka Emika Panda robot -- validate S$^2$-NNDS effectiveness in learning robust, safe, and stable motions from potentially unsafe demonstrations.

</details>
