<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 112]
- [cs.CL](#cs.CL) [Total: 31]
- [cs.RO](#cs.RO) [Total: 22]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real](https://arxiv.org/abs/2512.15774)
*Yan Yang,George Bebis,Mircea Nicolescu*

Main category: cs.CV

TL;DR: 论文提出了一种结合规则变换和无配对GAN图像翻译的数据增强框架，用于生成更真实的戴口罩人脸样本，以应对人脸检测和识别中的数据稀缺与分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 由于现实中戴口罩的人脸数据稀缺且分布与非口罩人脸差异大，现有方法生成的数据过于合成化，影响模型泛化和效果，因此需要更真实的戴口罩数据增强方法。

Method: 框架分两步：先基于规则对人脸进行口罩变换，再通过无配对的GAN图像到图像转换精细化生成戴口罩样本。同时引入非口罩保留损失和随机噪声注入，提升训练稳定性和生成数据多样性。

Result: 与仅用规则变换方法相比，该方法在生成样本质量上有一致性提升；与现有GAN方法（如IAMGAN）互补，提升了戴口罩人脸检测和识别的训练数据质量。

Conclusion: 本文框架有效提升了戴口罩人脸数据的真实性和多样性，有助于改善相关人脸识别任务，未来可根据实验结果进一步完善数据驱动的增强方式。

Abstract: Data scarcity and distribution shift pose major challenges for masked face detection and recognition. We propose a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using GANs, enabling the generation of realistic masked-face samples beyond purely synthetic transformations. Compared to rule-based warping alone, the proposed approach yields consistent qualitative improvements and complements existing GAN-based masked face generation methods such as IAMGAN. We introduce a non-mask preservation loss and stochastic noise injection to stabilize training and enhance sample diversity. Experimental observations highlight the effectiveness of the proposed components and suggest directions for future improvements in data-centric augmentation for face recognition tasks.

</details>


### [2] [Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2512.15885)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Pier Luigi Dovesi,Shaghayegh Roohi,Mark Granroth-Wilding,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出JARVIS框架，借助自监督方式提升多模态大语言模型（MLLMs）的视觉理解能力，显著改善模型在视觉相关任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM主要依赖语言描述进行视觉学习，导致模型视觉推理能力不足，且多模态微调数据规模有限，使模型易过拟合语言信息忽视视觉细节。

Method: 作者提出JARVIS框架，将I-JEPA自监督学习范式融入MLLM训练流程，通过冻结视觉基础模型作为上下文和目标编码器，仅训练大语言模型的前几层（预测器），实现无需完全依赖语言监督就能捕捉图像的结构和语义信息。

Result: 在标准MLLM基准测试上，JARVIS在多个LLM体系下均显著提升了模型的视觉任务表现，同时未损害其多模态推理能力。

Conclusion: JARVIS框架有效增强了MLLM的视觉理解能力，在视觉为核心的多模态任务中表现优异，具备良好的通用性和实用价值。

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.

</details>


### [3] [City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs](https://arxiv.org/abs/2512.15933)
*Dwip Dalal,Utkarsh Mishra,Narendra Ahuja,Nebojsa Jojic*

Main category: cs.CV

TL;DR: 本文提出了Sparsely Grounded Visual Navigation任务，并通过CityNav基准测试MLLMs在真实城市导航场景中的表现，发现现有MLLMs在该任务下表现不佳，提出了Verbalization of Path (VoP)方法提升导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型（MLLMs）为具身智能体提供了巨大潜力，但评价基准以语言或模拟环境为主，缺乏对真实世界中知识密集型推理与顺序决策能力的考察，因此需要更贴近现实且具挑战性的评测方法。

Method: 设计了Sparsely Grounded Visual Navigation任务，并创建了CityNav数据集，涵盖全球四座城市。要求智能体仅凭视觉输入与内在多模态推理能力，在没有额外环境标注和模型结构修改的前提下自主完成50+步的城市导航（包括定位、地标识别、空间推理和路线规划）。

Result: 大量实验表明，当前主流MLLMs及推理策略（如链式思维、反思等）在此任务上表现远低于预期。

Conclusion: 提出了Verbalization of Path (VoP)方法，通过抽取和明确认知地图（地标和路线）来外显化智能体推理过程，显著提升了模型导航成功率，证明其在知识密集型导航任务中的有效性。

Abstract: Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environments. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs and standard reasoning techniques (e.g., Chain-of-Thought, Reflection) significantly underperform in this challenging setting. To address this, we propose Verbalization of Path (VoP), which explicitly grounds the agent's internal reasoning by probing an explicit cognitive map (key landmarks and directions toward the destination) from the MLLMs, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/

</details>


### [4] [R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space](https://arxiv.org/abs/2512.15940)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: 本文提出了R4框架，它为视觉-语言模型（VLM）提供结构化的4D（时空）记忆，无需训练即可提升检索与推理能力。


<details>
  <summary>Details</summary>
Motivation: 人类能构建结构化的时空知识，并基于过去经验进行推理和决策，但现有VLM缺乏类似的持久、多模态记忆结构，限制了其对动态环境的理解与推理能力。

Method: R4是一种无训练框架，通过将物体级语义及其在时空的锚定，持续构建4D知识数据库。推理时，将自然语言查询解析为语义、空间与时间三个关键词，从数据库检索相关观测信息，并融合进VLM进行推理，实现基于4D时空的检索增强推理。

Result: 在具身问答与导航基准测试中，R4在检索与时空推理能力上均显著优于现有方法，有效提升了VLM在动态环境中的表现。

Conclusion: R4为动态环境下的4D时空推理开创了新范式，使VLM具备类人连续、结构化记忆与推理能力，为多智能体协作和具身智能研究带来有意义的进步。

Abstract: Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.

</details>


### [5] [The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs](https://arxiv.org/abs/2512.15949)
*Tejas Anvekar,Fenil Bardoliya,Pavan K. Turaga,Chitta Baral,Vivek Gupta*

Main category: cs.CV

TL;DR: 本文提出了一个新的评价框架“The Perceptual Observatory”，用于系统性和细致地评估多模态大语言模型（MLLM）的视觉感知能力，关注模型在干扰下的表现与视觉归因能力。


<details>
  <summary>Details</summary>
Motivation: 目前MLLM模型大多注重语言能力扩展，而视觉模块常被复用，导致难以判断模型对视觉信息的真正理解能力。现有评测多以端到端准确率为主，忽视了模型感知鲁棒性、归因准确性以及在扰动环境下的推理能力。

Method: 设计了覆盖不同视觉任务（如人脸匹配、图像理解、属性定位等）的评测体系，并用人脸和单词真实数据集，通过像素扰动和风格化扩散系统性生成干扰样本，对MLLM认知能力进行测试。

Result: 评测不仅关注任务结果准确率，还揭示了MLLM模型在面对视觉扰动时的感知基础、归因能力及其对视觉结构的信息保留情况。

Conclusion: 该框架为未来MLLM模型视觉评估提供了严谨基础，有助于深入解析现有及后续模型的优势与不足，推动对视觉感知本质的研究。

Abstract: Recent advances in multimodal large language models (MLLMs) have yielded increasingly powerful models, yet their perceptual capacities remain poorly characterized. In practice, most model families scale language component while reusing nearly identical vision encoders (e.g., Qwen2.5-VL 3B/7B/72B), which raises pivotal concerns about whether progress reflects genuine visual grounding or reliance on internet-scale textual world knowledge. Existing evaluation methods emphasize end-task accuracy, overlooking robustness, attribution fidelity, and reasoning under controlled perturbations. We present The Perceptual Observatory, a framework that characterizes MLLMs across verticals like: (i) simple vision tasks, such as face matching and text-in-vision comprehension capabilities; (ii) local-to-global understanding, encompassing image matching, grid pointing game, and attribute localization, which tests general visual grounding. Each vertical is instantiated with ground-truth datasets of faces and words, systematically perturbed through pixel-based augmentations and diffusion-based stylized illusions. The Perceptual Observatory moves beyond leaderboard accuracy to yield insights into how MLLMs preserve perceptual grounding and relational structure under perturbations, providing a principled foundation for analyzing strengths and weaknesses of current and future models.

</details>


### [6] [Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models](https://arxiv.org/abs/2512.15957)
*Utsav Panchal,Yuchen Liu,Luigi Palmieri,Ilche Georgievski,Marco Aiello*

Main category: cs.CV

TL;DR: 本文提出了CAMP-VLM框架，采用视觉语言模型结合场景信息，有效提升了多人的行为预测能力，尤其适用于第三人称视角下的机器人应用，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器人在多人环境中需要准确预测多个人的行为，而现有方法多集中于单人、第一人称的场景，缺乏对多人、第三人称及场景理解的深入研究。

Method: 提出CAMP-VLM框架，融合视觉输入的上下文特征和场景图的空间感知能力。由于缺乏合适的数据集，作者用写实模拟器合成多人人类行为数据，对模型进行微调，并在合成与真实场景下进行评估。模型训练采用监督微调（SFT）和直接偏好优化（DPO）两种方法。

Result: 在预测准确率上，CAMP-VLM相较于表现最好的基线方法有最多66.9%的提升，表现出了优越的泛化能力。

Conclusion: CAMP-VLM框架在多人人类行为预测问题上，尤其是在第三人称视角下，显著提高了准确性和泛化能力，为机器人在复杂人群环境中的应用提供了更好的技术支持。

Abstract: Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.

</details>


### [7] [From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection](https://arxiv.org/abs/2512.15971)
*Manuel Nkegoum,Minh-Tan Pham,Élisa Fromont,Bruno Avignon,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 本文提出利用视觉-语言模型（VLMs）提升少样本多光谱目标检测能力，并在两个主流数据集上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 多光谱目标检测对于自动驾驶、安防等安全关键场景至关重要，但标注数据稀缺限制了深度学习模型的性能。近期VLMs表现出良好的语义迁移能力，因此利用其语义先验辅助少样本多光谱检测成为关注点。

Method: 将两种典型基于VLM的检测器（Grounding DINO和YOLO-World）适配为支持多光谱输入，并提出融合文本、可见光和热成像特征的整合机制；在FLIR和M3FD两个数据集上进行了广泛实验。

Result: VLM-based检测器在少样本（few-shot）条件下，显著优于基于类似数据量训练的多光谱专用模型；在全监督条件下也达到甚至超过主流方法。

Conclusion: VLMs自带的大规模语义知识能够跨光谱迁移，实现高效数据利用，是提升多光谱感知能力的有力途径。

Abstract: Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.

</details>


### [8] [Are vision-language models ready to zero-shot replace supervised classification models in agriculture?](https://arxiv.org/abs/2512.15977)
*Earl Ranario,Mason J. Earles*

Main category: cs.CV

TL;DR: 本文对已有视觉-语言模型（VLMs）在农业多分类任务中的表现进行了系统评测。结果显示，当前VLMs，特别是零样本推理的表现，整体远低于以监督学习为基础的专业模型（如YOLOv11），在农业领域实用性有限。


<details>
  <summary>Details</summary>
Motivation: 随着VLMs在视觉识别任务中的广泛应用，评估其在农业领域（如作物病害、虫害、杂草识别等）中的可靠性和适用性显得尤为重要，但相关研究和基准评测较少。

Method: 作者基于AgML数据集，涵盖162类的27个农业分类任务，系统评测了多种开源和闭源VLM。采用零样本和多选提示等方式，并尝试结合大语言模型进行语义判断，以提升和分析模型表现。

Result: （1）VLM（如Gemini-3 Pro等）在多选任务下平均准确率约62%，开放式提示准确率普遍低于25%；（2）通过引入大模型辅助的语义判断，准确率有所提升，模型排名也会改变；（3）开源模型中Qwen-VL-72B最优，但与顶级闭源模型相比仍有差距；（4）作物及杂草品种分类难度低于虫害和损伤等任务。

Conclusion: 目前即用型视觉-语言模型尚不能胜任农业独立诊断工具，但在结合有限提示、明确标签本体和领域适配评测方法后，可作为辅助组件应用于农业决策支持。

Abstract: Vision-language models (VLMs) are increasingly proposed as general-purpose solutions for visual recognition tasks, yet their reliability for agricultural decision support remains poorly understood. We benchmark a diverse set of open-source and closed-source VLMs on 27 agricultural classification datasets from the AgML collection, spanning 162 classes across plant disease, pest and damage, and plant and weed species identification. Across all tasks, zero-shot VLMs substantially underperform a supervised task-specific baseline (YOLO11), which consistently achieves markedly higher accuracy than any foundation model. Under multiple-choice prompting, the best-performing VLM (Gemini-3 Pro) reaches approximately 62% average accuracy, while open-ended prompting yields much lower performance, with raw accuracies typically below 25%. Applying LLM-based semantic judging increases open-ended accuracy (for example, from 21% to 30% for top models) and alters model rankings, demonstrating that evaluation methodology meaningfully affects reported conclusions. Among open-source models, Qwen-VL-72B performs best, approaching closed-source performance under constrained prompting but still trailing top proprietary systems. Task-level analysis shows that plant and weed species classification is consistently easier than pest and damage identification, which remains the most challenging category across models. Overall, these results indicate that current off-the-shelf VLMs are not yet suitable as standalone agricultural diagnostic systems, but can function as assistive components when paired with constrained interfaces, explicit label ontologies, and domain-aware evaluation strategies.

</details>


### [9] [Eyes on the Grass: Biodiversity-Increasing Robotic Mowing Using Deep Visual Embeddings](https://arxiv.org/abs/2512.15993)
*Lars Beckers,Arno Waes,Aaron Van Campenhout,Toon Goedemé*

Main category: cs.CV

TL;DR: 该论文提出了一种利用视觉感知和自适应决策的机器人割草框架，能主动提升花园生物多样性。基于深度学习，机器人能够识别并保护具有视觉多样性的植被区域，通过有选择地停止割草，实现草坪的生态保护。实验表明该方法能够有效提升花园的物种丰富度。


<details>
  <summary>Details</summary>
Motivation: 传统自动割草存在被动、不可控地影响生物多样性的问题，很难兼顾割草与生态保护。作者希望提出方法，让机器人在割草时能主动识别和保护生态多样性区域，从而提升城市甚至家庭花园的生物多样性价值。

Method: 方法基于ResNet50网络，在PlantNet300K数据集上预训练，提取植被图片的特征嵌入，通过嵌入的空间分散度衡量生物多样性（无需物种级监督）。割草机器人根据该指标动态选择割草或保护植物，实现局部区域的选择性割草。系统在改装商用割草机器人上实现，并结合仿真草坪和真实花园数据进行验证。

Result: 实验结果显示，特征嵌入空间分散度与专家评价的生物多样性高度相关，说明该视觉多样性指标可以作为生态丰富度的有效代理。提出的选择性割草算法能够有效提升多样植被区域的保护效果。

Conclusion: 该研究证明了基于视觉多样性的深度特征可用于割草决策，实现主动生态保护。广泛部署此类系统有望显著提升城市草坪等地的生态价值，支持生物多样性恢复。

Abstract: This paper presents a robotic mowing framework that actively enhances garden biodiversity through visual perception and adaptive decision-making. Unlike passive rewilding approaches, the proposed system uses deep feature-space analysis to identify and preserve visually diverse vegetation patches in camera images by selectively deactivating the mower blades. A ResNet50 network pretrained on PlantNet300K provides ecologically meaningful embeddings, from which a global deviation metric estimates biodiversity without species-level supervision. These estimates drive a selective mowing algorithm that dynamically alternates between mowing and conservation behavior. The system was implemented on a modified commercial robotic mower and validated both in a controlled mock-up lawn and on real garden datasets. Results demonstrate a strong correlation between embedding-space dispersion and expert biodiversity assessment, confirming the feasibility of deep visual diversity as a proxy for ecological richness and the effectiveness of the proposed mowing decision approach. Widespread adoption of such systems will turn ecologically worthless, monocultural lawns into vibrant, valuable biotopes that boost urban biodiversity.

</details>


### [10] [CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion](https://arxiv.org/abs/2512.16023)
*Liudi Yang,Yang Bai,George Eskandar,Fengyi Shen,Mohammad Altillawi,Dong Chen,Ziyuan Liu,Abhinav Valada*

Main category: cs.CV

TL;DR: 本论文提出了一种基于文本指令、初始视觉观测和机器人关节状态自动生成视频-动作对的方法，便于机器人策略学习。方法通过创新的架构和机制，实现高质量视频生成和准确动作标注，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频扩散模型的机器人学习方法通常缺乏准确的动作标签，且模型架构上存在跨模态信息交流受限或无法充分利用预训练知识等问题，极大地限制了大规模数据的应用和策略学习的效果。

Method: 作者提出扩展预训练视频扩散模型，引入并行的动作扩散分支，并通过Bridge Attention（桥接注意力机制）实现跨模态高效交互，同时设计动作精炼模块，将粗糙动作转化为精确控制，适用于低分辨率数据集。

Result: 在多个公开基准测试和真实世界数据集上，方法能够生成更高质量的视频和更精确的动作控制，显著优于现有基线方法。

Conclusion: 本文方法为机器人利用大规模视频数据进行策略学习提供了可扩展的高效框架，通过创新性的架构，有效解决了动作标签匮乏和跨模态耦合的问题，推进了视频驱动机器人学习的发展。

Abstract: We present a method to generate video-action pairs that follow text instructions, starting from an initial image observation and the robot's joint states. Our approach automatically provides action labels for video diffusion models, overcoming the common lack of action annotations and enabling their full use for robotic policy learning. Existing methods either adopt two-stage pipelines, which limit tightly coupled cross-modal information sharing, or rely on adapting a single-modal diffusion model for a joint distribution that cannot fully leverage pretrained video knowledge. To overcome these limitations, we (1) extend a pretrained video diffusion model with a parallel, dedicated action diffusion model that preserves pretrained knowledge, (2) introduce a Bridge Attention mechanism to enable effective cross-modal interaction, and (3) design an action refinement module to convert coarse actions into precise controls for low-resolution datasets. Extensive evaluations on multiple public benchmarks and real-world datasets demonstrate that our method generates higher-quality videos, more accurate actions, and significantly outperforms existing baselines, offering a scalable framework for leveraging large-scale video data for robotic learning.

</details>


### [11] [Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving](https://arxiv.org/abs/2512.16055)
*Jiaheng Geng,Jiatong Du,Xinyu Zhang,Ye Li,Panqu Wang,Yanjun Huang*

Main category: cs.CV

TL;DR: 该论文提出了一个闭环评测平台，能在真实场景下为端到端自动驾驶生成安全关键角案例，并评估模型的性能退化，从而提升自动驾驶系统的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 真实世界中的关键危险场景难以收集，但对评估自动驾驶系统至关重要。现有的对抗性评测方法普遍依赖简化的仿真环境，缺乏在真实场景下的对抗性评估手段。

Method: 作者设计了一个闭环自动驾驶评估平台，结合基于流匹配的真实图像生成器与高效对抗性交通策略。图像生成器可根据交通环境信息稳定生成高效、真实的驾驶图像；对抗性车辆策略则用于模拟具有挑战性的车辆互动，生成极端场景来测试端到端自动驾驶模型。

Result: 实验表明，该平台可高效生成逼真的驾驶图像，并通过对UniAD和VAD等端到端模型的评测，验证了平台能有效揭示模型在关键场景下的性能退化。

Conclusion: 该平台可以有效发现自动驾驶模型潜在的问题，有助于提升端到端自动驾驶系统在现实环境下的安全性与鲁棒性。

Abstract: Safety-critical corner cases, difficult to collect in the real world, are crucial for evaluating end-to-end autonomous driving. Adversarial interaction is an effective method to generate such safety-critical corner cases. While existing adversarial evaluation methods are built for models operating in simplified simulation environments, adversarial evaluation for real-world end-to-end autonomous driving has been little explored. To address this challenge, we propose a closed-loop evaluation platform for end-to-end autonomous driving, which can generate adversarial interactions in real-world scenes. In our platform, the real-world image generator cooperates with an adversarial traffic policy to evaluate various end-to-end models trained on real-world data. The generator, based on flow matching, efficiently and stably generates real-world images according to the traffic environment information. The efficient adversarial surrounding vehicle policy is designed to model challenging interactions and create corner cases that current autonomous driving systems struggle to handle. Experimental results demonstrate that the platform can generate realistic driving images efficiently. Through evaluating the end-to-end models such as UniAD and VAD, we demonstrate that based on the adversarial policy, our platform evaluates the performance degradation of the tested model in corner cases. This result indicates that this platform can effectively detect the model's potential issues, which will facilitate the safety and robustness of end-to-end autonomous driving.

</details>


### [12] [FOD-Diff: 3D Multi-Channel Patch Diffusion Model for Fiber Orientation Distribution](https://arxiv.org/abs/2512.16075)
*Hao Tang,Hanyu Liu,Alessandro Perelli,Xi Chen,Chao Li*

Main category: cs.CV

TL;DR: 该论文提出了一种基于3D多通道patch扩散模型的方法，用于从低角分辨率单壳dMRI数据预测高角分辨率多壳FOD，实现更高效、准确的脑白质纤维分布估计，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 低角分辨率dMRI（LAR-FOD）推断精度不足，而高角分辨率dMRI（HAR-FOD）采集时间过长，不适用于临床实际。如何精准高效地从LAR-FOD重建HAR-FOD，是提升扩散MRI应用能力的关键。

Method: 提出3D多通道patch扩散模型，结合FOD-patch adapter，利用脑解剖先验提升patch学习效率；引入体素级条件协调模块，加强模型全局理解能力；通过SH注意力机制，学习球谐系数之间复杂关系。

Result: 所提方法在高角分辨率FOD预测任务中表现最佳，优于当前主流方法。

Conclusion: 该工作有效提升了HAR-FOD的生成效率和准确性，扩展了低分辨率dMRI临床应用的可能性。

Abstract: Diffusion MRI (dMRI) is a critical non-invasive technique to estimate fiber orientation distribution (FOD) for characterizing white matter integrity. Estimating FOD from single-shell low angular resolution dMRI (LAR-FOD) is limited by accuracy, whereas estimating FOD from multi-shell high angular resolution dMRI (HAR-FOD) requires a long scanning time, which limits its applicability. Diffusion models have shown promise in estimating HAR-FOD based on LAR-FOD. However, using diffusion models to efficiently generate HAR-FOD is challenging due to the large number of spherical harmonic (SH) coefficients in FOD. Here, we propose a 3D multi-channel patch diffusion model to predict HAR-FOD from LAR-FOD. We design the FOD-patch adapter by introducing the prior brain anatomy for more efficient patch-based learning. Furthermore, we introduce a voxel-level conditional coordinating module to enhance the global understanding of the model. We design the SH attention module to effectively learn the complex correlations of the SH coefficients. Our experimental results show that our method achieves the best performance in HAR-FOD prediction and outperforms other state-of-the-art methods.

</details>


### [13] [Auto-Vocabulary 3D Object Detection](https://arxiv.org/abs/2512.16077)
*Haomeng Zhang,Kuan-Chuan Peng,Suhas Lohit,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 本文提出了一种自动生成类别名称的3D目标检测方法AV3DOD，无需用户输入即可检测和命名新类别，并在主流数据集上实现了优于当前最先进方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇3D目标检测方法虽然号称“开放”，但在训练和推理时仍需用户指定类别，局限了实际开放程度。本研究旨在消除用户输入瓶颈，实现真正意义上的自动类别发现与命名。

Method: 作者提出了AV3DOD框架，利用2D视觉-语言模型（VLMs），通过图像描述生成、伪3D框生成及特征语义扩展，自动为检测到的3D对象生成类别名称。文中还引入了Semantic Score (SS)新指标来评估自动生成类别名称的语义质量。

Result: 在ScanNetV2和SUNRGB-D两个主流数据集上，AV3DOD在目标定位（mAP）和语义质量（SS）上均优于现有方法。特别是在ScanNetV2数据集上，较SOTA方法CoDA整体mAP提升3.48，同时SS指标相对提升24.5%。

Conclusion: AV3DOD实现了自动3D目标检测和命名，且性能大幅优于现有方法，推动了开放词汇3D目标检测的真正自动化，有望应用于更广泛的实际场景。

Abstract: Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.

</details>


### [14] [LAPX: Lightweight Hourglass Network with Global Context](https://arxiv.org/abs/2512.16089)
*Haopeng Zhao,Marsha Mariya Kappan,Mahdi Bamdad,Francisco Cruz*

Main category: cs.CV

TL;DR: 本文提出了一种适用于边缘设备的人体姿态估计算法LAPX，在仅使用2.3M参数的前提下，在MPII和COCO数据集上取得了与SOTA模型相当的效果，并具备实时推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有高精度人体姿态估计方法参数量大、计算成本高，不适合边缘设备部署。尽管已有轻量化方法问世，但仍存在效率不足或精度下降的问题。

Method: 基于已有LAP结构，本文提出LAPX，将自注意力模块与小时玻璃网络结合，并改进了网络的阶段设计及轻量注意力模块，实现了对全局上下文的更好建模。

Result: 在MPII和COCO两个标准数据集上，LAPX用2.3M参数达到了可与主流模型媲美的精度，并具备实时推理能力。

Conclusion: LAPX是一种高效、轻量且适用于边缘设备的人体姿态估计方法，兼顾了准确性和推理速度。

Abstract: Human pose estimation is a crucial task in computer vision. Methods that have SOTA (State-of-the-Art) accuracy, often involve a large number of parameters and incur substantial computational cost. Many lightweight variants have been proposed to reduce the model size and computational cost of them. However, several of these methods still contain components that are not well suited for efficient deployment on edge devices. Moreover, models that primarily emphasize inference speed on edge devices often suffer from limited accuracy due to their overly simplified designs. To address these limitations, we propose LAPX, an Hourglass network with self-attention that captures global contextual information, based on previous work, LAP. In addition to adopting the self-attention module, LAPX advances the stage design and refine the lightweight attention modules. It achieves competitive results on two benchmark datasets, MPII and COCO, with only 2.3M parameters, and demonstrates real-time performance, confirming its edge-device suitability.

</details>


### [15] [Collimator-assisted high-precision calibration method for event cameras](https://arxiv.org/abs/2512.16092)
*Zibin Liu,Shunkun Liang,Banglei Guan,Dongcai Tan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种利用带有闪烁星形图案的准直仪进行事件相机几何标定的新方法。该方法在长距离和高精度测量需求下表现优异，实验结果优于现有标定方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机因其高动态范围和高时间分辨率等优点正逐步应用于复杂场景，但其几何标定（内参和外参确定），特别是在长距离测量下，仍面临精度难以保障的挑战。

Method: 作者设计了一种新的标定方法，结合带有闪烁星形图案的准直仪。先使用准直仪的球面运动模型线性求解相机参数，再通过非线性优化进一步高精度优化参数。

Result: 在多种真实场景下进行了实验，结果表明，该方法在标定精度和可靠性上都优于现有的事件相机标定方法。

Conclusion: 该文提出的基于准直仪和闪烁星形图案的事件相机标定方法，在长距离和高精度标定需求下具备领先性能，为事件相机的实际应用提供了有力支持。

Abstract: Event cameras are a new type of brain-inspired visual sensor with advantages such as high dynamic range and high temporal resolution. The geometric calibration of event cameras, which involves determining their intrinsic and extrinsic parameters, particularly in long-range measurement scenarios, remains a significant challenge. To address the dual requirements of long-distance and high-precision measurement, we propose an event camera calibration method utilizing a collimator with flickering star-based patterns. The proposed method first linearly solves camera parameters using the sphere motion model of the collimator, followed by nonlinear optimization to refine these parameters with high precision. Through comprehensive real-world experiments across varying conditions, we demonstrate that the proposed method consistently outperforms existing event camera calibration methods in terms of accuracy and reliability.

</details>


### [16] [TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times](https://arxiv.org/abs/2512.16093)
*Jintao Zhang,Kaiwen Zheng,Kai Jiang,Haoxu Wang,Ion Stoica,Joseph E. Gonzalez,Jianfei Chen,Jun Zhu*

Main category: cs.CV

TL;DR: TurboDiffusion是一种显著加速视频扩散生成的新框架，速度提升100-200倍且保持视频质量。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型生成视频的速度较慢，难以满足实际应用中的效率需求，因此亟需加速扩散模型生成过程，提升推理速度，同时保证视频输出质量不降低。

Method: TurboDiffusion框架主要通过三方面实现加速：(1) 融合低比特SageAttention和可训练稀疏SLA以提升注意力计算效率；(2) 利用rCM进行高效的步数蒸馏；(3) 采用W8A8量化技术将参数及激活量化为8位，从而加速线性层和减小模型体积。此外还包括多项工程优化。

Result: 在多个主流视频生成模型和不同分辨率、参数规模下，TurboDiffusion在单张RTX 5090 GPU上实现了100-200倍的视频生成加速，且生成的视频质量与原模型相当。

Conclusion: TurboDiffusion能大幅提升视频扩散模型的生成速度且不降低质量，具备实际应用价值，并已开源相关代码和模型。

Abstract: We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.
  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.

</details>


### [17] [Flexible Camera Calibration using a Collimator System](https://arxiv.org/abs/2512.16113)
*Shunkun Liang,Banglei Guan,Zhenbao Yu,Dongcai Tan,Pengju Sun,Zibin Liu,Qifeng Yu,Yang Shang*

Main category: cs.CV

TL;DR: 本文提出一种基于独特准直仪系统的新型相机标定方法，通过引入角度不变性约束，将标定问题简化，提高了灵活性和精度，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的相机标定方法依赖于复杂的标定环境和高自由度的目标-相机相对运动，效率低且灵活性不足。本研究旨在通过设计新型准直仪系统，简化标定过程，提高标定可靠性和精度。

Method: 利用专门设计的准直仪系统，提出角度不变性约束，并证明目标与相机之间的相对运动满足球面运动模型，将原本6自由度的运动约束为3自由度纯旋转。基于此，分别提出了多图像的线性闭式求解器和两图像的极小解求解器，并开发了无需相机运动的单图像标定算法。

Result: 在合成和真实实验中测试了所提方法，结果显示利用准直仪系统标定的可行性，并且新方法的性能优于当前主流基线方法。

Conclusion: 本文提出的准直仪系统和新标定方法有效简化了相机标定流程，并提升了准确性和灵活性，适用于灵活和快速的实际应用场景。

Abstract: Camera calibration is a crucial step in photogrammetry and 3D vision applications. This paper introduces a novel camera calibration method using a designed collimator system. Our collimator system provides a reliable and controllable calibration environment for the camera. Exploiting the unique optical geometry property of our collimator system, we introduce an angle invariance constraint and further prove that the relative motion between the calibration target and camera conforms to a spherical motion model. This constraint reduces the original 6DOF relative motion between target and camera to a 3DOF pure rotation motion. Using spherical motion constraint, a closed-form linear solver for multiple images and a minimal solver for two images are proposed for camera calibration. Furthermore, we propose a single collimator image calibration algorithm based on the angle invariance constraint. This algorithm eliminates the requirement for camera motion, providing a novel solution for flexible and fast calibration. The performance of our method is evaluated in both synthetic and real-world experiments, which verify the feasibility of calibration using the collimator system and demonstrate that our method is superior to existing baseline methods. Demo code is available at https://github.com/LiangSK98/CollimatorCalibration

</details>


### [18] [Interaction-via-Actions: Cattle Interaction Detection with Joint Learning of Action-Interaction Latent Space](https://arxiv.org/abs/2512.16133)
*Ren Nakagawa,Yang Yang,Risa Shinoda,Hiroaki Santo,Kenji Oyama,Fumio Okura,Takenao Ohkawa*

Main category: cs.CV

TL;DR: 该论文提出了一种仅利用单张图像即可自动检测放牧牛行为交互的方法，并开发了实际可用的监测系统。


<details>
  <summary>Details</summary>
Motivation: 人类行为交互检测已较为成熟，但针对放牧牛的行为交互检测仍面临数据集稀缺和交互事件少的挑战。牛只行为分析对智慧牧场管理（如发情检测等）非常关键。

Method: 提出CattleAct方法，将牛的行为交互分解为个体牛行为的组合。首先利用大规模牛动作数据集学习动作潜在空间，然后通过对先前训练好的潜在空间进行对比学习微调，将罕见交互嵌入，实现动作与交互的统一表征。此外，整合视频和GPS输入，开发了实际可用的监测系统。

Result: 在商用规模牧场上的实验证明，所提方法在交互检测的准确率优于现有基线方法。

Conclusion: CattleAct方法能够在数据有限且交互稀少的场景下，高效准确地检测牛只行为交互，为智慧牧场的实际应用提供了有效技术支持。

Abstract: This paper introduces a method and application for automatically detecting behavioral interactions between grazing cattle from a single image, which is essential for smart livestock management in the cattle industry, such as for detecting estrus. Although interaction detection for humans has been actively studied, a non-trivial challenge lies in cattle interaction detection, specifically the lack of a comprehensive behavioral dataset that includes interactions, as the interactions of grazing cattle are rare events. We, therefore, propose CattleAct, a data-efficient method for interaction detection by decomposing interactions into the combinations of actions by individual cattle. Specifically, we first learn an action latent space from a large-scale cattle action dataset. Then, we embed rare interactions via the fine-tuning of the pre-trained latent space using contrastive learning, thereby constructing a unified latent space of actions and interactions. On top of the proposed method, we develop a practical working system integrating video and GPS inputs. Experiments on a commercial-scale pasture demonstrate the accurate interaction detection achieved by our method compared to the baselines. Our implementation is available at https://github.com/rakawanegan/CattleAct.

</details>


### [19] [ResDynUNet++: A nested U-Net with residual dynamic convolution blocks for dual-spectral CT](https://arxiv.org/abs/2512.16140)
*Ze Yuan,Wenbin Li,Shusen Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种结合迭代方法与深度学习模型的双光谱CT（DSCT）混合重建框架，有效提升了材料分解的准确性和图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统DSCT重建方法要么依赖先验物理知识但易受噪声影响，要么完全基于数据驱动却缺乏解释性和稳定性。本文旨在结合两者优势，提高重建结果的质量，并解决通道不平衡和界面伪影等问题。

Method: 整合了两个模块：首先在知识驱动阶段采用OPMT（斜投影修正技术）对投影数据进行快速迭代求解，获取中间的基材料图像；然后在数据驱动阶段引入基于UNet++的ResDynUNet++神经网络，利用残差动态卷积进一步优化中间图像，提升图像的清晰度和准确性。

Result: 在合成仿体和真实临床数据集上的大量实验表明，所提出方法在材料分解能力、图像干净度和准确性等方面优于现有方法。

Conclusion: 本方法通过融合物理驱动和深度学习，实现了更加高效且精确的DSCT重建，对基础材料分解和临床应用具有重要意义。

Abstract: We propose a hybrid reconstruction framework for dual-spectral CT (DSCT) that integrates iterative methods with deep learning models. The reconstruction process consists of two complementary components: a knowledge-driven module and a data-driven module. In the knowledge-driven phase, we employ the oblique projection modification technique (OPMT) to reconstruct an intermediate solution of the basis material images from the projection data. We select OPMT for this role because of its fast convergence, which allows it to rapidly generate an intermediate solution that successfully achieves basis material decomposition. Subsequently, in the data-driven phase, we introduce a novel neural network, ResDynUNet++, to refine this intermediate solution. The ResDynUNet++ is built upon a UNet++ backbone by replacing standard convolutions with residual dynamic convolution blocks, which combine the adaptive, input-specific feature extraction of dynamic convolution with the stable training of residual connections. This architecture is designed to address challenges like channel imbalance and near-interface large artifacts in DSCT, producing clean and accurate final solutions. Extensive experiments on both synthetic phantoms and real clinical datasets validate the efficacy and superior performance of the proposed method.

</details>


### [20] [SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation](https://arxiv.org/abs/2512.16143)
*Yueyang Hu,Haiyong Jiang,Haoxuan Song,Jun Xiao,Hao Pan*

Main category: cs.CV

TL;DR: 本论文提出了一种用于小样本三维部件分割的新框架SegGraph，通过基于SAM分割图的传播方法，有效融合2D基础模型到3D任务，在PartNet-E上大幅提升分割表现。


<details>
  <summary>Details</summary>
Motivation: 当前三维小样本部件分割领域亟需能高效利用二维基础模型（如SAM）知识的方法，但现有方法要么忽视三维几何结构，要么未能充分利用高质量分组信息，导致分割欠佳与标签不一致。解决如何有效结合2D与3D信息，是提升性能的关键动力。

Method: 作者提出SegGraph方法：首先以SAM分割掩码为基础，构建“分割图”，节点为分割区域，边反映区域间的空间关系（重叠/邻接）。各节点自适应融合二维基础模型特征，并通过图神经网络传播学习全局结构。为保证语义一致性，还设计了视角加权融合机制，弱化低质量分割对最终结果的影响。

Result: 在PartNet-E数据集上的大量实验显示，SegGraph相对所有主流基线提升mIoU至少6.9%，且对小部件和边界分割表现尤为优异，显示出对几何结构有更强的理解能力。

Conclusion: SegGraph突破了二维知识迁移到三维分割的瓶颈，通过分割图建模与特征传播，显著提升了三维局部分割质量，特别适合复杂部件和边界清晰度高的场景。

Abstract: This work presents a novel framework for few-shot 3D part segmentation. Recent advances have demonstrated the significant potential of 2D foundation models for low-shot 3D part segmentation. However, it is still an open problem that how to effectively aggregate 2D knowledge from foundation models to 3D. Existing methods either ignore geometric structures for 3D feature learning or neglects the high-quality grouping clues from SAM, leading to under-segmentation and inconsistent part labels. We devise a novel SAM segment graph-based propagation method, named SegGraph, to explicitly learn geometric features encoded within SAM's segmentation masks. Our method encodes geometric features by modeling mutual overlap and adjacency between segments while preserving intra-segment semantic consistency. We construct a segment graph, conceptually similar to an atlas, where nodes represent segments and edges capture their spatial relationships (overlap/adjacency). Each node adaptively modulates 2D foundation model features, which are then propagated via a graph neural network to learn global geometric structures. To enforce intra-segment semantic consistency, we map segment features to 3D points with a novel view-direction-weighted fusion attenuating contributions from low-quality segments. Extensive experiments on PartNet-E demonstrate that our method outperforms all competing baselines by at least 6.9 percent mIoU. Further analysis reveals that SegGraph achieves particularly strong performance on small components and part boundaries, demonstrating its superior geometric understanding. The code is available at: https://github.com/YueyangHu2000/SegGraph.

</details>


### [21] [C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation](https://arxiv.org/abs/2512.16164)
*Chao Li,Dasha Hu,Chengyang Li,Yuming Jiang,Yuncheng Shen*

Main category: cs.CV

TL;DR: 本文针对视觉-语言模型（VLMs）在无监督领域自适应（UDA）下的prompt tuning存在的领域差异难题，提出C-DGPA方法，通过双路结构同时优化边缘分布与条件分布对齐，有效提升跨域表现，并在多个主流数据集上取得SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的基于prompt-tuning的UDA方法在应用于视觉-语言模型时，仅关注边缘分布对齐，忽视了条件分布差异，导致类别原型错位和语义区分能力下降。该工作动机是解决此类方法在领域适应时的分布对齐不全面问题。

Method: 提出C-DGPA方法，采用双分支结构：边缘分布分支通过动态对抗训练对齐源域与目标域的边缘分布，条件分布分支引入类别映射机制（CMM），通过标准化语义prompt理解并防止对源域过度依赖来对齐条件分布，从而协同优化整体的分布对齐。

Result: 在OfficeHome、Office31和VisDA-2017三个数据集上广泛实验，C-DGPA在所有基准上均获得了新的SOTA（最先进）结果，验证了其方法的优越性。

Conclusion: C-DGPA实现了边缘分布和条件分布的协同对齐，有效提升了视觉-语言模型在UDA任务中的迁移能力，为prompt学习领域带来了新的突破。

Abstract: Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.

</details>


### [22] [Towards Closing the Domain Gap with Event Cameras](https://arxiv.org/abs/2512.16178)
*M. Oltan Sevinc,Liao Wu,Francisco Cruz*

Main category: cs.CV

TL;DR: 本文提出使用事件相机替代传统相机，以提升自动驾驶在昼夜光照差异等域间差异下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统相机在部署环境与训练环境存在差异时（如昼夜光照变化），在端到端自动驾驶任务中的表现会显著下降，即存在'域间差异'问题。本文试图解决传统相机对昼夜域差变化的不鲁棒问题。

Method: 作者提出采用事件相机作为传感器替代传统相机。事件相机能在不同光照条件下保持高效的感知能力，无需复杂的调整与补偿。

Result: 实验表明，事件相机在不同光照条件下的性能更为稳定，域变换带来的性能损失比灰度帧等方案小，且在跨域（如昼夜）测试中表现优于传统方法。

Conclusion: 事件相机可作为自动驾驶感知系统的新型传感器，有效缓解光照条件造成的域间差异，提升系统的跨域鲁棒性。

Abstract: Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.

</details>


### [23] [Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation](https://arxiv.org/abs/2512.16199)
*Jerrin Bright,Zhibo Wang,Dmytro Klepachevskyi,Yuhao Chen,Sirisha Rambhatla,David Clausi,John Zelek*

Main category: cs.CV

TL;DR: 提出了Avatar4D，一个可定制的人体动作数据集生成工具，能够为特定领域（如体育）生成高拟真、可控的合成人体运动数据，并验证了其在当前姿态估计任务中的有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的人体动作合成工具主要针对日常动作，灵活性有限，无法满足特定领域，特别是体育等高复杂动作场景的数据需求。细粒度控制和无人工标注的数据生成成为亟需解决的问题。

Method: 开发了Avatar4D管线，可以精细控制身体姿态、外观、相机视角和环境，不需人工标注。同时，创建了包括棒球和冰球等运动Syn2Sport大规模模拟数据集。利用这一平台生成高质量三维时序动作序列，并以多样化外观及环境渲染数据。通过在Syn2Sport上评测多种主流姿态估计算法，比较它们的集监督学习、零样本迁移和跨运动泛化能力。还分析了合成数据与真实数据的特征空间相似度。

Result: Avatar4D生成的合成人体数据在监督学习、零样本转移到真实世界数据、以及不同运动类型间泛化方面表现良好。此外，合成数据与真实数据在特征空间中的接近程度也得到了验证。

Conclusion: Avatar4D及Syn2Sport展示了无需依赖领域真实数据，也能为特定领域任务生成高度可控、可迁移的人体运动数据的能力，为体育等复杂动作领域的人体理解模型提供了新选择。

Abstract: We present Avatar4D, a real-world transferable pipeline for generating customizable synthetic human motion datasets tailored to domain-specific applications. Unlike prior works, which focus on general, everyday motions and offer limited flexibility, our approach provides fine-grained control over body pose, appearance, camera viewpoint, and environmental context, without requiring any manual annotations. To validate the impact of Avatar4D, we focus on sports, where domain-specific human actions and movement patterns pose unique challenges for motion understanding. In this setting, we introduce Syn2Sport, a large-scale synthetic dataset spanning sports, including baseball and ice hockey. Avatar4D features high-fidelity 4D (3D geometry over time) human motion sequences with varying player appearances rendered in diverse environments. We benchmark several state-of-the-art pose estimation models on Syn2Sport and demonstrate their effectiveness for supervised learning, zero-shot transfer to real-world data, and generalization across sports. Furthermore, we evaluate how closely the generated synthetic data aligns with real-world datasets in feature space. Our results highlight the potential of such systems to generate scalable, controllable, and transferable human datasets for diverse domain-specific tasks without relying on domain-specific real data.

</details>


### [24] [Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation](https://arxiv.org/abs/2512.16201)
*Sarosij Bose,Ravi K. Rajendran,Biplob Debnath,Konstantinos Karydis,Amit K. Roy-Chowdhury,Srimat Chakradhar*

Main category: cs.CV

TL;DR: 本文提出VALOR方法，通过视觉和语言的对齐，提升医学影像大模型生成放射报告的准确性与事实性。


<details>
  <summary>Details</summary>
Motivation: 目前医学影像报告生成仍面临视觉-语言不一致导致幻觉（hallucination）等问题，现有方法依赖大量有标注数据或成本高昂的偏好数据，实际效果有限，急需提升模型事实性和关注关键影像区域。

Method: 提出基于强化学习的后对齐方法，包含两个阶段：一是利用文本奖励优化医学影像-语言大模型的临床用词准确性；二是将视觉投影模块对齐至疾病特征，指导模型更加关注诊断相关的关键图像区域。核心技术是Group-Relative Proximal Optimization (GRPO)。

Result: 在多个基准数据集上，VALOR方法显著提升了模型在事实准确性和视觉对齐能力上的表现，优于当前最新放射报告生成方法。

Conclusion: VALOR能有效缓解视觉-语言模型在医学报告生成中的幻觉问题，提升诊断相关信息的可靠提取与表述，对自动化医疗有重要推动作用。

Abstract: Radiology Report Generation (RRG) is a critical step toward automating healthcare workflows, facilitating accurate patient assessments, and reducing the workload of medical professionals. Despite recent progress in Large Medical Vision-Language Models (Med-VLMs), generating radiology reports that are both visually grounded and clinically accurate remains a significant challenge. Existing approaches often rely on large labeled corpora for pre-training, costly task-specific preference data, or retrieval-based methods. However, these strategies do not adequately mitigate hallucinations arising from poor cross-modal alignment between visual and linguistic representations. To address these limitations, we propose VALOR:Visual Alignment of Medical Vision-Language Models for GrOunded Radiology Report Generation. Our method introduces a reinforcement learning-based post-alignment framework utilizing Group-Relative Proximal Optimization (GRPO). The training proceeds in two stages: (1) improving the Med-VLM with textual rewards to encourage clinically precise terminology, and (2) aligning the vision projection module of the textually grounded model with disease findings, thereby guiding attention toward image re gions most relevant to the diagnostic task. Extensive experiments on multiple benchmarks demonstrate that VALOR substantially improves factual accuracy and visual grounding, achieving significant performance gains over state-of-the-art report generation methods.

</details>


### [25] [Open Ad-hoc Categorization with Contextualized Feature Learning](https://arxiv.org/abs/2512.16202)
*Zilin Wang,Sangwoo Mo,Stella X. Yu,Sima Behpour,Liu Ren*

Main category: cs.CV

TL;DR: 该论文提出了OAK模型，通过引入可学习的上下文tokens，在冻结的CLIP模型输入端实现针对开放式ad-hoc场景类别的自适应归类任务，在Stanford和Clevr-4等数据集上取得了显著领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉归类任务主要面向常见固定类别，对于AI需要灵活应对的动态任务，临时构建（ad-hoc）类别能力十分重要。传统类别和ad-hoc类别在感知机制上具有共性，因此亟需一个可泛化、易适应的开集ad-hoc分类模型。

Method: 在模型设计上，OAK将一组可学习上下文token加入固定参数的CLIP模型输入，利用CLIP的图文对齐损失和GCD的视觉聚类损失联合优化，兼顾了上下文语义扩展和视觉聚类。

Result: OAK在Stanford和Clevr-4多个分类任务上，准确率和新概念发现能力达到SOTA，包括Stanford Mood数据集上实现87.4%的新类准确率，超过CLIP和GCD 50%以上。

Conclusion: OAK不仅效果优于现有方法，还可生成可解释的显著性图，对动作、情绪和场所等类别聚焦于不同区域，提升了透明度和信任度，为ad-hoc类别的自适应与泛化归类提供了新思路。

Abstract: Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories are created dynamically to serve specific goals. We study open ad-hoc categorization: Given a few labeled exemplars and abundant unlabeled data, the goal is to discover the underlying context and to expand ad-hoc categories through semantic extension and visual clustering around it.
  Building on the insight that ad-hoc and common categories rely on similar perceptual mechanisms, we propose OAK, a simple model that introduces a small set of learnable context tokens at the input of a frozen CLIP and optimizes with both CLIP's image-text alignment objective and GCD's visual clustering objective.
  On Stanford and Clevr-4 datasets, OAK achieves state-of-the-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. Moreover, OAK produces interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling adaptive and generalizable categorization.

</details>


### [26] [SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning](https://arxiv.org/abs/2512.16461)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: SNOW提出了一种结合视觉语言模型（VLM）和点云几何信息的新方法，实现了训练无关、主干无关的4D场景理解系统，大幅提升了机器人系统对动态环境的时空理解能力。


<details>
  <summary>Details</summary>
Motivation: 自动机器人系统在动态环境下需要对时空信息有深刻理解，单一的视觉或几何感知方法各有局限。VLM有丰富语义但缺少3D和时序能力，几何感知有结构但语义稀缺。因此，需要将二者优势结合，解决开放世界、多模态和时序一致性等挑战。

Method: SNOW框架同时处理同步RGB图像和3D点云，运用HDBSCAN聚类产生物体级别的区域建议，然后用SAM2进行分割。新提出的STEP编码法，将每个分割区域表达为兼含语义、几何和时序属性的多模态token，并逐步整合为4D场景图（4DSG）。通过轻量级SLAM实现token的全局空间对齐，所有信息空间锚定，形成可查询的统一4D模型。

Result: 在多个基准测试集上，SNOW实现了4D场景理解和时空推理的新SOTA，多项指标显著优于现有方法，尤其体现在空间定位和多模态理解的精度上。

Conclusion: 将VLM与点云几何及时序一致性结合，能显著提升自动机器人对动态环境的深度理解。SNOW为下游推理和自主机器人提供了强大、结构化的4D先验，推动了具身智能的发展。

Abstract: Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.

</details>


### [27] [Enhanced 3D Shape Analysis via Information Geometry](https://arxiv.org/abs/2512.16213)
*Amit Vishwakarma,K. S. Subrahamanian Moosath*

Main category: cs.CV

TL;DR: 本论文提出了一种新的3D点云比较方法，通过将点云表示为高斯混合模型（GMM），并提出了修正的对称KL散度（MSKL），有效解决了传统度量方法的数值不稳定及结构敏感性差的问题，在不同数据集上实验表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统点云对比方法（如Hausdorff、Chamfer距离）难以刻画全局结构且对异常值敏感，KL散度在GMM下数值不稳定，亟需一种既能稳健反映几何变化又数值稳定的点云度量方法。

Method: 论文将点云作为GMM统计流形上的点，证明了GMM空间形成统计流形，提出了具有理论保证上界和下界的新型修正对称KL散度（MSKL），确保所有GMM间比较的数值稳定性。

Result: 在人体姿态区分（MPI-FAUST数据集）和动物形状比较（G-PCD数据集）实验中，MSKL持续提供反映几何变化的稳定结果，优于传统距离与现有KL近似方法。

Conclusion: MSKL能够稳定且直观地度量点云间的几何变化，是一种优于传统方法的稳健点云度量工具。

Abstract: Three-dimensional point clouds provide highly accurate digital representations of objects, essential for applications in computer graphics, photogrammetry, computer vision, and robotics. However, comparing point clouds faces significant challenges due to their unstructured nature and the complex geometry of the surfaces they represent. Traditional geometric metrics such as Hausdorff and Chamfer distances often fail to capture global statistical structure and exhibit sensitivity to outliers, while existing Kullback-Leibler (KL) divergence approximations for Gaussian Mixture Models can produce unbounded or numerically unstable values. This paper introduces an information geometric framework for 3D point cloud shape analysis by representing point clouds as Gaussian Mixture Models (GMMs) on a statistical manifold. We prove that the space of GMMs forms a statistical manifold and propose the Modified Symmetric Kullback-Leibler (MSKL) divergence with theoretically guaranteed upper and lower bounds, ensuring numerical stability for all GMM comparisons. Through comprehensive experiments on human pose discrimination (MPI-FAUST dataset) and animal shape comparison (G-PCD dataset), we demonstrate that MSKL provides stable and monotonically varying values that directly reflect geometric variation, outperforming traditional distances and existing KL approximations.

</details>


### [28] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian,Boyao Han,Chen Shi,Lei Xiao,Long Yang,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种新型几何感知视觉-语言-动作（VLA）框架GeoPredict，通过在训练阶段引入轨迹级与几何预测模块，显著提升机器人在需要精确3D推理的任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型对机器人操作泛化良好，但主要依赖2D感知和反应性策略，在需要高精度3D几何推理的任务上表现不佳，限制了其实用性。

Method: GeoPredict框架包括两个核心模块：1）轨迹级预测模块，编码运动历史并多步预测机器人臂的3D关键点轨迹；2）3D高斯几何预测模块，伴随关键点轨迹预测未来工位几何，并通过深度渲染提供训练时监督。推理阶段仅需轻量级查询，无需3D解码过程。

Result: 在RoboCasa Human-50、LIBERO等基准和真实机器人操作任务上，GeoPredict相较于强大的VLA基线方法，在涉及复杂空间推理和几何判断的任务中表现出更高的性能和稳定性。

Conclusion: GeoPredict通过训练时引入可预测的几何和运动先验，有效提升了VLA模型在3D空间推理任务中的泛化和可靠性，对实际机器人操作具有较强的实用价值。

Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.

</details>


### [29] [Learning High-Quality Initial Noise for Single-View Synthesis with Diffusion Models](https://arxiv.org/abs/2512.16219)
*Zhihao Zhang,Xuejun Yang,Weihua Liu,Mouquan Shen*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的单视图新视角合成新方法，通过优化初始噪声提升生成效果，实验表明能大幅提升多种主流方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的单视图新视角合成方法存在初始噪声质量影响生成效果的问题，缺乏专门的学习框架去生成高质量噪声。

Method: 1) 设计离散Euler逆过程，将图像语义注入随机噪声，构建了高质量噪声与随机噪声的数据对；2) 提出基于编码器-解码器（EDN）的框架，将随机噪声直接变换为高质量噪声，并可嵌入主流NVS模型。

Result: 在SV3D、MV-Adapter等主流NVS模型和多个公开数据集上，加入EDN后，模型性能显著提升。

Conclusion: EDN框架能有效提升扩散模型生成新视图的效果，并具有良好的通用性和拓展性，为扩散模型的应用带来新的提升空间。

Abstract: Single-view novel view synthesis (NVS) models based on diffusion models have recently attracted increasing attention, as they can generate a series of novel view images from a single image prompt and camera pose information as conditions. It has been observed that in diffusion models, certain high-quality initial noise patterns lead to better generation results than others. However, there remains a lack of dedicated learning frameworks that enable NVS models to learn such high-quality noise. To obtain high-quality initial noise from random Gaussian noise, we make the following contributions. First, we design a discretized Euler inversion method to inject image semantic information into random noise, thereby constructing paired datasets of random and high-quality noise. Second, we propose a learning framework based on an encoder-decoder network (EDN) that directly transforms random noise into high-quality noise. Experiments demonstrate that the proposed EDN can be seamlessly plugged into various NVS models, such as SV3D and MV-Adapter, achieving significant performance improvements across multiple datasets. Code is available at: https://github.com/zhihao0512/EDN.

</details>


### [30] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: 本文推出了OpenTouch数据集，这是第一个包含“第一人称（egocentric）视角”和完整手部触觉的野外数据集，并证明触觉信号可显著提升抓取理解和跨模态感知。


<details>
  <summary>Details</summary>
Motivation: 目前人类与物理世界接触的主要方式是手部，但现有研究很少能知道手何时、何地以及如何与物体接触。缺乏坚固可穿戴的全手触觉传感器，以及缺乏将第一人称视频与全手触觉对齐的大规模数据集，限制了视觉感知与物理交互的联动研究。

Method: 作者提出并采集了OpenTouch数据集，含5.1小时同步的视频、手部触摸和姿态数据，并人工筛选2900个带详细文字注释的片段。在此基础上，提出了检索和分类基准，用于评估触觉在感知和动作中的作用。

Result: 实验证明，触觉信号不仅能紧凑、有力地辅助手部抓取动作的理解，还能增强视频与触觉信号的跨模态对齐，并且可以通过视频检索准确还原所涉及的触觉信号。

Conclusion: 通过发布OpenTouch数据集和基准，作者希望推动多模态第一视角感知、具身学习和丰富接触的机器人操作等方向的发展。

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [31] [Image Compression Using Singular Value Decomposition](https://arxiv.org/abs/2512.16226)
*Justin Jiang*

Main category: cs.CV

TL;DR: 本文研究了用奇异值分解（SVD）及低秩矩阵近似方法进行图像压缩，并对其效果进行了评估。结果显示，该方法在实际压缩效果方面不如JPEG等主流格式。


<details>
  <summary>Details</summary>
Motivation: 网络上的图像数量庞大，高效的图像压缩对于降低存储和带宽需求非常重要。作者希望探索基于线性代数的SVD和低秩近似方法是否能成为一种有效的图像压缩手段。

Method: 作者利用奇异值分解和低秩矩阵近似技术对灰度和多通道图像进行了压缩实验，并通过相对Frobenius误差和压缩比来评估其表现。

Result: 低秩近似通常能生成与原图视觉上相似的图像，但在相同比例误差下，其压缩效率普遍低于JPEG、JPEG2000和WEBP；更低误差容忍度下，甚至压缩后文件比原始图像更大。

Conclusion: SVD和低秩近似在实际图像压缩上的表现远不及当前业界主流的压缩格式，因此不适合作为实际应用的图像压缩方案。

Abstract: Images are a substantial portion of the internet, making efficient compression important for reducing storage and bandwidth demands. This study investigates the use of Singular Value Decomposition and low-rank matrix approximations for image compression, evaluating performance using relative Frobenius error and compression ratio. The approach is applied to both grayscale and multichannel images to assess its generality. Results show that the low-rank approximations often produce images that appear visually similar to the originals, but the compression efficiency remains consistently worse than established formats such as JPEG, JPEG2000, and WEBP at comparable error levels. At low tolerated error levels, the compressed representation produced by Singular Value Decomposition can even exceed the size of the original image, indicating that this method is not competitive with industry-standard codecs for practical image compression.

</details>


### [32] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: 提出了一个新型的3D手部轨迹预测数据集EgoMAN，并基于此设计了EgoMAN模型，实现了从推理到运动的端到端预测，取得了更准确和泛化性更强的效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D手部轨迹预测方法存在数据集缺乏语义和运动结合监督、模型推理与动作生成脱节等问题，导致预测效果有限。作者希望解决这两个关键问题。

Method: 1）构建了大规模、具互动阶段标注的EgoMAN数据集，包含219K条6DoF手部轨迹和300万对结构化QA，覆盖语义、空间和运动推理；2）提出EgoMAN模型，将视觉-语言推理与运动生成通过轨迹token接口相连，实现推理驱动的运动预测；3）采用渐进式训练方式提升推理与运动动态的一致性。

Result: EgoMAN模型能生成阶段感知（stage-aware）的精准3D手轨迹，对真实场景具有良好泛化能力，显著优于此前方法。

Conclusion: 通过构建新型数据集和模型，打通了推理与运动生成的通道，实现了更具语义理解和泛化能力的3D手轨迹预测，对实际应用有重要意义。

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [33] [ARMFlow: AutoRegressive MeanFlow for Online 3D Human Reaction Generation](https://arxiv.org/abs/2512.16234)
*Zichen Geng,Zeeshan Hayder,Wei Liu,Hesheng Wang,Ajmal Mian*

Main category: cs.CV

TL;DR: 本文提出了ARMFlow方法，解决了3D人物反应生成中的运动保真、实时性、自回归适应性等三大难题，实现了高精度、低延迟的实时生成，性能超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D人物反应生成面临高运动保真、实时推理、自回归生成误差累积等挑战，目前方法无法三者兼得，因此需要新的方法突破自回归适应性和实时性瓶颈。

Method: 提出基于MeanFlow的自回归框架ARMFlow，包括因果上下文编码器和多层感知机速度预测器，并创新性地提出Bootstrap Contextual Encoding（BSCE）训练策略，以生成历史替代真实历史缓解误差累积；同时提出了离线变体ReMFlow。

Result: ARMFlow在线生成在InterHuman和InterX数据集上FID指标超过现有方法40%；离线变体ReMFlow达到最快推理速度与SOTA性能；单步推理即可实现高精度、低延迟。

Conclusion: ARMFlow有效同时提升了运动保真度、推理速度与自回归适应性，在实际在线3D人物交互生成中具有显著优势，并缩小了离线与在线方法的性能差距。

Abstract: 3D human reaction generation faces three main challenges:(1) high motion fidelity, (2) real-time inference, and (3) autoregressive adaptability for online scenarios. Existing methods fail to meet all three simultaneously. We propose ARMFlow, a MeanFlow-based autoregressive framework that models temporal dependencies between actor and reactor motions. It consists of a causal context encoder and an MLP-based velocity predictor. We introduce Bootstrap Contextual Encoding (BSCE) in training, encoding generated history instead of the ground-truth ones, to alleviate error accumulation in autoregressive generation. We further introduce the offline variant ReMFlow, achieving state-of-the-art performance with the fastest inference among offline methods. Our ARMFlow addresses key limitations of online settings by: (1) enhancing semantic alignment via a global contextual encoder; (2) achieving high accuracy and low latency in a single-step inference; and (3) reducing accumulated errors through BSCE. Our single-step online generation surpasses existing online methods on InterHuman and InterX by over 40% in FID, while matching offline state-of-the-art performance despite using only partial sequence conditions.

</details>


### [34] [MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning](https://arxiv.org/abs/2512.16909)
*Yuanchen Ju,Yongyuan Liang,Yen-Jen Wang,Nandiraju Gireesh,Yuanliang Ju,Seungjae Lee,Qiao Gu,Elvis Hsieh,Furong Huang,Koushil Sreenath*

Main category: cs.CV

TL;DR: 本文提出MomaGraph体系，实现了面向家庭移动操作机器人的统一场景图表示，并发布了相应的数据集与基准，提出了新的多模态模型，并在多项任务上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有用于家庭移动操作机器人的场景表示方法存在空间和功能关系割裂、静态快照限制、任务相关信息不足等问题，限制了机器人综合感知与任务执行能力，需要一种更适用于实际任务的统一、动态、高语义场景表示方法。

Method: 作者提出MomaGraph，一种统一空间与功能关系、支持部件级交互的场景图表示；发布MomaGraph-Scenes——大规模、任务驱动的场景图注释数据集，以及MomaGraph-Bench基准（涵盖六项推理能力）；进一步开发MomaGraph-R1模型（7B参数的视觉语言模型），通过强化学习在MomaGraph-Scenes上训练，支持任务导向场景图预测及零样本任务规划。

Result: MomaGraph-R1模型在MomaGraph-Bench基准上取得71.6%准确率，比最佳现有开源模型提升11.4%，并在公开数据集与真实机器人应用中展现出良好的泛化与迁移能力。

Conclusion: MomaGraph体系为具身智能体任务规划和环境理解提供了统一、高效的场景表示和系统化评测工具，推动了家庭场景机器人自主能力发展。

Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.

</details>


### [35] [AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection](https://arxiv.org/abs/2512.16235)
*Satya Narayana Panda,Vaishnavi Kukkala,Spandana Iyer*

Main category: cs.CV

TL;DR: 本文开发了一种结合家族史和临床图像的多模态AI系统，用于提升皮肤病诊断准确率，并计划通过临床试验进行验证和实施。


<details>
  <summary>Details</summary>
Motivation: 皮肤病全球患病率高，诊断困难，尤其受限于专家稀缺和临床表现复杂。家族史对疾病易感性和治疗反应影响显著，但临床流程中常被忽视。如何让AI有效整合家族史与影像数据，提升诊断效率与临床应用价值，是亟需解决的问题。

Method: 研究开发了一个多模态AI框架，将基于深度学习的图像分析与结构化临床数据（包括家族史）结合。具体实现是用可解释的卷积神经网络（CNN）结合包含遗传风险因素的临床决策树，对皮肤病进行辅助诊断，并以传统诊断为基准，在真实医疗环境中设计前瞻性临床试验进行比较和验证。

Result: 集成家族史信息的AI系统在皮肤病诊断（尤其是遗传性疾病如黑色素瘤、牛皮癣、特应性皮炎）中表现出更高的准确率。与专家评价对照，反馈认为系统有助于疾病早期发现和个性化推荐，展现较大应用潜力。正式的临床试验正在计划中。

Conclusion: 结合家族史和影像数据的AI系统有望提升皮肤病诊断准确性，具备良好的落地性和可解释性，未来可进一步通过临床试验检验实际效果，推动其在临床中的广泛应用。

Abstract: Dermatological conditions affect 1.9 billion people globally, yet accurate diagnosis remains challenging due to limited specialist availability and complex clinical presentations. Family history significantly influences skin disease susceptibility and treatment responses, but is often underutilized in diagnostic processes. This research addresses the critical question: How can AI-powered systems integrate family history data with clinical imaging to enhance dermatological diagnosis while supporting clinical trial validation and real-world implementation?
  We developed a comprehensive multi-modal AI framework that combines deep learning-based image analysis with structured clinical data, including detailed family history patterns. Our approach employs interpretable convolutional neural networks integrated with clinical decision trees that incorporate hereditary risk factors. The methodology includes prospective clinical trials across diverse healthcare settings to validate AI-assisted diagnosis against traditional clinical assessment.
  In this work, validation was conducted with healthcare professionals to assess AI-assisted outputs against clinical expectations; prospective clinical trials across diverse healthcare settings are proposed as future work. The integrated AI system demonstrates enhanced diagnostic accuracy when family history data is incorporated, particularly for hereditary skin conditions such as melanoma, psoriasis, and atopic dermatitis. Expert feedback indicates potential for improved early detection and more personalized recommendations; formal clinical trials are planned. The framework is designed for integration into clinical workflows while maintaining interpretability through explainable AI mechanisms.

</details>


### [36] [DVGT: Driving Visual Geometry Transformer](https://arxiv.org/abs/2512.16919)
*Sicheng Zuo,Zixun Xie,Wenzhao Zheng,Shaoqing Xu,Fang Li,Shengyin Jiang,Long Chen,Zhi-Xin Yang,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新的驾驶场景三维重建方法，能够自适应不同场景和摄像头配置，直接用多视角无位姿图像序列重建高精度稠密三维点云。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶三维场景感知方法普遍依赖精确的摄像头外参或其他几何先验，缺乏能适应不同配置和场景的通用化解决方案。针对目前通用性和灵活性不足的问题，作者希望提出一种无需显式3D先验、可处理任意视角和设备的模型。

Method: 提出Driving Visual Geometry Transformer（DVGT），使用DINO特征提取基础上，设计了交替的图像内/跨视角/跨时间注意力机制推理几何关系。用多头输出端，一次性预测全局稠密点云和各帧的ego位姿，无需外部参数和对齐后处理。

Result: 在nuScenes、OpenScene、Waymo、KITTI、DDAD等多种主流自动驾驶公开数据集上进行训练和验证，DVGT在各类场景下都显著优于现有同类模型。

Conclusion: DVGT实现了不依赖于精确摄像头参数且能自适应多种场景与配置的高效3D稠密场景重建，极大提升了三维感知的灵活性和实用性。

Abstract: Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.

</details>


### [37] [Semi-Supervised Multi-View Crowd Counting by Ranking Multi-View Fusion Models](https://arxiv.org/abs/2512.16243)
*Qi Zhang,Yunfei Gong,Zhidan Xie,Zhizi Wang,Antoni B. Chan,Hui Huang*

Main category: cs.CV

TL;DR: 本文针对多视角拥挤人数统计任务，在数据有限的情况下，提出了基于模型预测或不确定性排序的半监督多视角融合框架，提高统计准确率。


<details>
  <summary>Details</summary>
Motivation: 目前多视角人数统计受数据采集和标注难题限制，现有多视角数据集样本数量和场景有限，制约模型效果。

Method: 提出两种半监督多视角人数统计方法：一是基于模型预测结果排序（更少视角预测人数不大于更多视角）；二是基于模型不确定性排序（更多视角的不确定性不大于更少视角），并将此约束引入训练以利用有限标注数据。

Result: 实验显示，两种多视角模型排序方法在有限标签数据条件下的表现优于其他半监督人数统计方法。

Conclusion: 所提出的半监督多视角融合方法能有效利用有限标注数据，实现更准确的大场景人群计数，在多视角计数领域展示出明显优势。

Abstract: Multi-view crowd counting has been proposed to deal with the severe occlusion issue of crowd counting in large and wide scenes. However, due to the difficulty of collecting and annotating multi-view images, the datasets for multi-view counting have a limited number of multi-view frames and scenes. To solve the problem of limited data, one approach is to collect synthetic data to bypass the annotating step, while another is to propose semi- or weakly-supervised or unsupervised methods that demand less multi-view data. In this paper, we propose two semi-supervised multi-view crowd counting frameworks by ranking the multi-view fusion models of different numbers of input views, in terms of the model predictions or the model uncertainties. Specifically, for the first method (vanilla model), we rank the multi-view fusion models' prediction results of different numbers of camera-view inputs, namely, the model's predictions with fewer camera views shall not be larger than the predictions with more camera views. For the second method, we rank the estimated model uncertainties of the multi-view fusion models with a variable number of view inputs, guided by the multi-view fusion models' prediction errors, namely, the model uncertainties with more camera views shall not be larger than those with fewer camera views. These constraints are introduced into the model training in a semi-supervised fashion for multi-view counting with limited labeled data. The experiments demonstrate the advantages of the proposed multi-view model ranking methods compared with other semi-supervised counting methods.

</details>


### [38] [Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning](https://arxiv.org/abs/2512.16266)
*Paloma Casteleiro Costa,Parnian Ghapandar Kashani,Xuhui Liu,Alexander Chen,Ary Portes,Julien Bec,Laura Marcu,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的多通道像素超分辨率框架（FLIM_PSR_k），可大幅提升荧光寿命成像显微镜（FLIM）的成像速度与分辨率，兼顾信噪比和实际应用需求。


<details>
  <summary>Details</summary>
Motivation: FLIM由于像素驻留时间长、信噪比较低等局限，影响了其在临床和实时诊断中的推广应用。亟需提升成像分辨率和速度的同时，克服信噪比等技术难题。

Method: 提出FLIM_PSR_k深度学习框架，通过条件生成对抗网络（cGAN）训练，可对最大5倍像素尺寸采集数据实现高分辨率重建。与扩散模型方法相比，cGAN推理速度更快、重建更稳定。

Result: 模型可实现5倍超分辨率重建，在患者肿瘤组织样本的盲测中，输出图像的空间-带宽积提升25倍，显现更多细微结构，各项图像质量指标均有显著提升。

Conclusion: FLIM_PSR_k提升了FLIM的空间分辨率和采集速度，增强其兼容性和灵活性，有望推动FLIM向更快、更高分辨率、低数值孔径和微型化方向发展，促进其临床转化应用。

Abstract: Fluorescence lifetime imaging microscopy (FLIM) is a powerful quantitative technique that provides metabolic and molecular contrast, offering strong translational potential for label-free, real-time diagnostics. However, its clinical adoption remains limited by long pixel dwell times and low signal-to-noise ratio (SNR), which impose a stricter resolution-speed trade-off than conventional optical imaging approaches. Here, we introduce FLIM_PSR_k, a deep learning-based multi-channel pixel super-resolution (PSR) framework that reconstructs high-resolution FLIM images from data acquired with up to a 5-fold increased pixel size. The model is trained using the conditional generative adversarial network (cGAN) framework, which, compared to diffusion model-based alternatives, delivers a more robust PSR reconstruction with substantially shorter inference times, a crucial advantage for practical deployment. FLIM_PSR_k not only enables faster image acquisition but can also alleviate SNR limitations in autofluorescence-based FLIM. Blind testing on held-out patient-derived tumor tissue samples demonstrates that FLIM_PSR_k reliably achieves a super-resolution factor of k = 5, resulting in a 25-fold increase in the space-bandwidth product of the output images and revealing fine architectural features lost in lower-resolution inputs, with statistically significant improvements across various image quality metrics. By increasing FLIM's effective spatial resolution, FLIM_PSR_k advances lifetime imaging toward faster, higher-resolution, and hardware-flexible implementations compatible with low-numerical-aperture and miniaturized platforms, better positioning FLIM for translational applications.

</details>


### [39] [TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering](https://arxiv.org/abs/2512.16270)
*Rui Gui,Yang Wan,Haochen Han,Dongxing Mao,Fangming Liu,Min Li,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一个名为TextEditBench的新基准，用于专注评估图像中文字区域的编辑能力，并引入新的评测维度，对主流编辑系统进行了实验分析。


<details>
  <summary>Details</summary>
Motivation: 随着视觉生成领域的发展，特别是在大模型和多模态领域，如何在保持上下文连贯和语义一致的前提下编辑图像中的文本成为难题。目前缺少专门针对图像文本编辑的系统性评测工具。

Method: 作者设计了TextEditBench基准，涵盖了注重推理能力、物理可行性、语义理解和跨模态依赖的多项编辑场景，并提出了“语义期望（SE）”这一新的评测指标，用于评价模型的推理与一致性能力。

Result: 通过对现有主流图像编辑系统的大量实验发现，这些模型虽能处理基础文字编辑指令，但在语境推理、物理一致性和布局集成等方面存在明显不足。

Conclusion: TextEditBench为文本引导下的图像编辑和多模态推理提供了全新评测标准，有助于相关能力的深入研究和突破。

Abstract: Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.

</details>


### [40] [GFLAN: Generative Functional Layouts](https://arxiv.org/abs/2512.16275)
*Mohamed Abouagour,Eleftherios Garyfallidis*

Main category: cs.CV

TL;DR: 本文提出了一种名为GFLAN的生成式框架，将自动化平面图生成任务分为拓扑规划和几何实现两个阶段，从而更好地捕捉建筑设计中的拓扑与功能需求。


<details>
  <summary>Details</summary>
Motivation: 当前的自动化平面图生成方法（尤其是深度学习方法）难以捕捉建筑设计中的拓扑关系优先性、功能约束传播及交通流线等重要设计要素。

Method: GFLAN框架将平面图生成分为两阶段：第一阶段利用具有双编码器的卷积架构，分别提取空间上下文与布局状态，通过概率图逐步分配房间中心点；第二阶段构建房间与边界顶点的异质图，采用带有Transformer的图神经网络联合回归房间边界。

Result: GFLAN显式分解了拓扑与几何生成过程，提升了平面图生成对于建筑内部关系和功能布局的表达能力。

Conclusion: GFLAN方法有助于自动化平面图生成更好地满足建筑设计的拓扑与功能需求，推进了该领域统一建模的进展。

Abstract: Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.

</details>


### [41] [MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval](https://arxiv.org/abs/2512.16294)
*Amna Amir,Erchan Aptoula*

Main category: cs.CV

TL;DR: 本文提出了多标签自适应对比学习（MACL）方法，有效提升了多标签遥感图像检索的性能，尤其解决了类别重叠、标签分布不平衡和类别共现难题，在多个数据集上均超过了传统对比损失方法。


<details>
  <summary>Details</summary>
Motivation: 现有多标签遥感图像检索中存在类别语义重叠、标签分布极度不均衡、类别间复杂共现等难题，导致检索性能下降。

Method: MACL方法扩展了传统对比学习，结合了标签感知采样、频率敏感加权、动态温标调节三大策略，实现对常见和少见类别的平衡表征。

Result: 在DLRSD、ML-AID和WHDLD三个基准数据集上的大量实验显示，MACL在检索效果上持续优于基于传统对比损失的基线模型，显著缓解了语义不均衡问题。

Conclusion: MACL能够有效提升大规模多标签遥感图像的检索可靠性，为相关领域提供了更优的解决方案，代码和模型也将在论文接收后开源。

Abstract: Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.

</details>


### [42] [PixelArena: A benchmark for Pixel-Precision Visual Intelligence](https://arxiv.org/abs/2512.16303)
*Feng Liang,Sizhe Cheng,Chenqi Yi*

Main category: cs.CV

TL;DR: 本文提出通过语义分割任务（PixelArena）评估多模态大模型在高精度像素级图像生成上的能力，发现Gemini 3 Pro Image在零样本条件下表现出色，优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型的图像生成评测多关注美学，而缺乏对其细粒度生成能力的客观评价方法。作者希望通过更具精确性的方法，公平比较模型间的生成智能。

Method: 提出PixelArena基准，利用像素级的语义分割任务，在零样本设定下评测不同多模态模型的精细图像生成能力，并与其他模型进行定性和定量比较。

Result: Gemini 3 Pro Image模型展现了出色的创新生成能力，能在未见过的任务中生成高保真的语义掩模，显示了前所未有的视觉智能和泛化能力。并发现该模型在多项评测指标上超越其他同类模型，同时也分析了其失败的案例。

Conclusion: 该工作不仅展示了多模态视觉智能领域的重大进展，也为未来关于多模态推理、可解释性和基准测试的研究提供了新的方法和见解。

Abstract: Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.

</details>


### [43] [LaverNet: Lightweight All-in-one Video Restoration via Selective Propagation](https://arxiv.org/abs/2512.16313)
*Haiyu Zhao,Yiwen Shan,Yuanbiao Gou,Xi Peng*

Main category: cs.CV

TL;DR: 本文提出了一个轻量级的视频多任务复原网络LaverNet，仅有36.2万参数，通过创新机制仅传播退化无关特征，实现了体积极小却性能优越的视频通用复原。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务视频复原方法在应对时变退化时，模型容易被退化主导，忽视视频本身内容，且大多依赖超大模型掩盖方法难点，带来算力和部署压力。作者希望实现高效、轻量化且泛化性强的多任务视频复原。

Method: 作者提出LaverNet，一种仅有36.2万参数的轻量级全能型视频复原网络。核心方法是新颖的帧间特征传播机制，仅在帧间选择性地传递退化无关特征，有效缓解退化对时序建模的干扰，实现精简情况下的高效复原。

Result: 实验表明，LaverNet网络尽管参数量不到现有主流方法的1%，但在多个基准测试上都获得了可比甚至更优的性能。

Conclusion: 小模型（轻量化）通过合理设计机制也能实现强大的多任务视频复原能力，LaverNet为视频复原提供了易于部署且高性能的新选择。

Abstract: Recent studies have explored all-in-one video restoration, which handles multiple degradations with a unified model. However, these approaches still face two challenges when dealing with time-varying degradations. First, the degradation can dominate temporal modeling, confusing the model to focus on artifacts rather than the video content. Second, current methods typically rely on large models to handle all-in-one restoration, concealing those underlying difficulties. To address these challenges, we propose a lightweight all-in-one video restoration network, LaverNet, with only 362K parameters. To mitigate the impact of degradations on temporal modeling, we introduce a novel propagation mechanism that selectively transmits only degradation-agnostic features across frames. Through LaverNet, we demonstrate that strong all-in-one restoration can be achieved with a compact network. Despite its small size, less than 1\% of the parameters of existing models, LaverNet achieves comparable, even superior performance across benchmarks.

</details>


### [44] [Ridge Estimation-Based Vision and Laser Ranging Fusion Localization Method for UAVs](https://arxiv.org/abs/2512.16314)
*Huayu Huang,Chen Chen,Banglei Guan,Ze Tan,Yang Shang,Zhang Li,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于岭估计的融合定位方法，通过结合无人机序列图像和激光测距数据，提高目标定位的精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 单一传感信息下的目标定位受限于距离远、交会角小及倾角大等条件，导致最小二乘法存在多重共线性问题，影响算法稳定性和鲁棒性。需要一种方法提升定位精度并增强在有限观测条件下的鲁棒性。

Method: 将无人机多传感数据融合，通过引入岭估计方法，抑制最小二乘估计中因设计矩阵多重共线性导致的病态问题，提升定位系统的稳定性。

Result: 实验表明，所提方法在有限观测条件下，相较于仅用单信息的地面定位算法，具有更高的定位精度与鲁棒性。

Conclusion: 基于岭估计的多信息融合定位方法能够显著提升目标定位的精准度和鲁棒性，尤其适合低可观测性场景，优于传统单信息定位算法。

Abstract: Tracking and measuring targets using a variety of sensors mounted on UAVs is an effective means to quickly and accurately locate the target. This paper proposes a fusion localization method based on ridge estimation, combining the advantages of rich scene information from sequential imagery with the high precision of laser ranging to enhance localization accuracy. Under limited conditions such as long distances, small intersection angles, and large inclination angles, the column vectors of the design matrix have serious multicollinearity when using the least squares estimation algorithm. The multicollinearity will lead to ill-conditioned problems, resulting in significant instability and low robustness. Ridge estimation is introduced to mitigate the serious multicollinearity under the condition of limited observation. Experimental results demonstrate that our method achieves higher localization accuracy compared to ground localization algorithms based on single information. Moreover, the introduction of ridge estimation effectively enhances the robustness, particularly under limited observation conditions.

</details>


### [45] [QUIDS: Quality-informed Incentive-driven Multi-agent Dispatching System for Mobile Crowdsensing](https://arxiv.org/abs/2512.16325)
*Nan Zhou,Zuxin Li,Fanhang Man,Xuecheng Chen,Susu Xu,Fan Dang,Chaopeng Hong,Yunhao Liu,Xiao-Ping Zhang,Xinlei Chen*

Main category: cs.CV

TL;DR: 提出QUIDS系统，通过多因素协同优化，实现非专用车辆移动众包感知系统的高质量信息收集。新指标ASQ量化感知质量，并利用智能派遣和激励机制大幅提升性能。


<details>
  <summary>Details</summary>
Motivation: NVMCS系统存在感知覆盖不足、感知可靠性差以及车辆动态性带来挑战，难以在预算有限下保障信息质量，亟需协同优化方案。

Method: 提出QUIDS系统，定义ASQ作为覆盖与可靠性的联合度量；开发信念感知的车辆派遣算法，在不确定性下优化激励与车辆选择，提升整体感知质量。

Result: 实证结果表明，QUIDS在真实城市数据中将ASQ提升38%（对比无派遣方案），较现有最佳提升10%；地图重建误差降低39-74%。

Conclusion: QUIDS实现了无需专用基础设施的低成本高质量城市感知，兼顾覆盖与可靠性，适用于交通、环境等智慧城市场景。

Abstract: This paper addresses the challenge of achieving optimal Quality of Information (QoI) in non-dedicated vehicular mobile crowdsensing (NVMCS) systems. The key obstacles are the interrelated issues of sensing coverage, sensing reliability, and the dynamic participation of vehicles. To tackle these, we propose QUIDS, a QUality-informed Incentive-driven multi-agent Dispatching System, which ensures high sensing coverage and reliability under budget constraints. QUIDS introduces a novel metric, Aggregated Sensing Quality (ASQ), to quantitatively capture QoI by integrating both coverage and reliability. We also develop a Mutually Assisted Belief-aware Vehicle Dispatching algorithm that estimates sensing reliability and allocates incentives under uncertainty, further improving ASQ. Evaluation using real-world data from a metropolitan NVMCS deployment shows QUIDS improves ASQ by 38% over non-dispatching scenarios and by 10% over state-of-the-art methods. It also reduces reconstruction map errors by 39-74% across algorithms. By jointly optimizing coverage and reliability via a quality-informed incentive mechanism, QUIDS enables low-cost, high-quality urban monitoring without dedicated infrastructure, applicable to smart-city scenarios like traffic and environmental sensing.

</details>


### [46] [Collaborative Edge-to-Server Inference for Vision-Language Models](https://arxiv.org/abs/2512.16349)
*Soochang Song,Yongjune Kim*

Main category: cs.CV

TL;DR: 本文提出了一种用于视觉-语言模型（VLMs）的协同边缘-服务器推理框架，可在降低通信成本的同时保持推理精度。


<details>
  <summary>Details</summary>
Motivation: VLM推理通常需将边缘设备拍摄的图像传到服务器处理，但因图像分辨率调整，易忽略细节，导致精度下降。大规模数据上传也加重了网络负担。亟需在减少通信量的同时保障推理准确。

Method: 设计了两阶段框架：第一阶段，服务器用全局图像推理，通过VLM关注机制识别兴趣区域（RoI），并基于输出token的最小熵评估置信度；若最小熵高于阈值，则仅让边缘设备上传高细节的RoI局部图像。第二阶段，服务器融合全局与局部图像联合推理，减少不必要的数据传输。

Result: 实验证明，该方法在多种VLM架构下能大幅降低通信量，同时保持与原方案相当的推理精度。

Conclusion: 该协同推理框架通过选择性回传和细粒度信息保留，实现了高效且准确的边缘-服务器视觉-语言推理，为相关应用提供了有效解决方案。

Abstract: We propose a collaborative edge-to-server inference framework for vision-language models (VLMs) that reduces the communication cost while maintaining inference accuracy. In typical deployments, visual data captured at edge devices (clients) is transmitted to the server for VLM inference. However, resizing the original image (global image) to match the vision encoder's input resolution often discards fine-grained details, leading to accuracy degradation. To overcome this limitation, we design a two-stage framework. In the first stage, the server performs inference on the global image and identifies a region of interest (RoI) using the VLM's internal attention. The min-entropy of the output tokens is then computed as a confidence measure to determine whether retransmission is required. If the min-entropy exceeds a predefined threshold, the server requests the edge device to send a detail-preserved local image of the RoI. The server then refines its inference by jointly leveraging the global and local images. This selective retransmission strategy ensures that only essential visual content is transmitted. Experiments across multiple VLM architectures show that the proposed framework significantly reduces communication cost while maintaining inference accuracy.

</details>


### [47] [GMODiff: One-Step Gain Map Refinement with Diffusion Priors for HDR Reconstruction](https://arxiv.org/abs/2512.16357)
*Tao Hu,Weiyu Zhou,Yanjie Tu,Peng Wu,Wei Dong,Qingsen Yan,Yanning Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的单步扩散框架GMODiff，用于多曝光HDR重建，显著提升了速度和质量。


<details>
  <summary>Details</summary>
Motivation: 当前预训练潜在扩散模型（LDMs）在低层视觉任务表现优秀，但用于HDR重建时面临动态范围受限、推理成本高及生成内容失真等问题，迫切需要更高效且结构保真的重建方法。

Method: 作者提出了一种以增益图（Gain Map, GM）为核心驱动的HDR重建新方法，将HDR重建任务转化为条件引导的GM估计任务，并用回归先验指导扩散模型，实现单步生成高质量增益图，有效融合了内容保真和感知质量。

Result: 大量实验证明，GMODiff在多曝光HDR重建任务上，效果优于多种最新方法，且推理速度比基于LDM的方法快约100倍。

Conclusion: GMODiff成功解决了LDM在HDR重建任务中的关键问题，实现了高质量、高速度和结构保真的HDR图像生成，在实际应用和学术研究中具有很高的价值。

Abstract: Pre-trained Latent Diffusion Models (LDMs) have recently shown strong perceptual priors for low-level vision tasks, making them a promising direction for multi-exposure High Dynamic Range (HDR) reconstruction. However, directly applying LDMs to HDR remains challenging due to: (1) limited dynamic-range representation caused by 8-bit latent compression, (2) high inference cost from multi-step denoising, and (3) content hallucination inherent to generative nature. To address these challenges, we introduce GMODiff, a gain map-driven one-step diffusion framework for multi-exposure HDR reconstruction. Instead of reconstructing full HDR content, we reformulate HDR reconstruction as a conditionally guided Gain Map (GM) estimation task, where the GM encodes the extended dynamic range while retaining the same bit depth as LDR images. We initialize the denoising process from an informative regression-based estimate rather than pure noise, enabling the model to generate high-quality GMs in a single denoising step. Furthermore, recognizing that regression-based models excel in content fidelity while LDMs favor perceptual quality, we leverage regression priors to guide both the denoising process and latent decoding of the LDM, suppressing hallucinations while preserving structural accuracy. Extensive experiments demonstrate that our GMODiff performs favorably against several state-of-the-art methods and is 100 faster than previous LDM-based methods.

</details>


### [48] [EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation](https://arxiv.org/abs/2512.16360)
*Haotian Ling,Zequn Chen,Qiuying Chen,Donglin Di,Yongjia Ma,Hao Li,Chen Wei,Zhulin Tao,Xun Yang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为EverybodyDance的新方法，以确保多角色动画中的身份对应准确性（IC correctness），在以往单角色动画取得进展的基础上，解决了多角色、位置交换场景下的关键难题。该方法引入了身份匹配图（IMG）与掩码查询注意力机制（MQA），并且通过评测基准及实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管单角色的动作迁移与动画生成技术进展显著，但多角色环境下角色身份混淆，尤其是角色位置交换时，常常导致身份错配。现有方法难以在生成动画时严格地保证参照帧和生成帧之间的身份一致性。针对这个核心问题，作者提出EverybodyDance方法，提升多角色动画中的身份对应和视觉效果。

Method: 作者提出了身份匹配图（IMG），将参照帧和生成帧中的人物分别作为完整二分图的节点集合，并引入掩码查询注意力机制（MQA）计算节点之间的身份亲和度。身份匹配结构被正式建模为图结构度量，并在训练阶段进行优化。此外，提出了身份嵌入式引导、多尺度匹配策略和预分类采样等辅助机制促进身份对应准确性。

Result: 论文建立了多角色动画身份对应评测基准，并通过大量实验比较，在身份对应准确性和视觉保真度方面，EverybodyDance方法均显著优于现有最先进的基线方法。

Conclusion: EverybodyDance方法通过创新的图结构建模及多机制协同优化，切实解决了多角色动画身份错配的关键痛点，为多角色动画生成提供了可靠且有效的解决方案。

Abstract: Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.

</details>


### [49] [Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models](https://arxiv.org/abs/2512.16371)
*Mariam Hassan,Bastien Van Delft,Wuyang Li,Alexandre Alahi*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频生成方法FVG，通过将文本生成视频任务分解为推理、合成和时序三个阶段，大幅提升了复杂场景和逻辑动作的视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成视频（T2V）扩散模型在场景复杂性和逻辑时序上表现不足，许多错误源自模型无法准确生成语义正确的初始帧，影响后续动画效果。

Method: 提出Factorized Video Generation（FVG）流程：1. 推理阶段-用大语言模型（LLM）重写视频提示词，仅描述初始场景，消除时序歧义；2. 合成阶段-利用文本到图像模型生成高质量、逻辑正确的锚点帧；3. 时序合成-用视频模型专注于根据锚帧生成后续动画。

Result: FVG在T2V CompBench基准上取得了新SOTA，并在VBench2上显著提升了多种模型表现。锚帧方法使采样步数减少70%，显著提升生成速度。

Conclusion: FVG能有效提升文本生成视频的质量、效率和可控性，为视频合成提供了更实用的方法。

Abstract: State-of-the-art Text-to-Video (T2V) diffusion models can generate visually impressive results, yet they still frequently fail to compose complex scenes or follow logical temporal instructions. In this paper, we argue that many errors, including apparent motion failures, originate from the model's inability to construct a semantically correct or logically consistent initial frame. We introduce Factorized Video Generation (FVG), a pipeline that decouples these tasks by decomposing the Text-to-Video generation into three specialized stages: (1) Reasoning, where a Large Language Model (LLM) rewrites the video prompt to describe only the initial scene, resolving temporal ambiguities; (2) Composition, where a Text-to-Image (T2I) model synthesizes a high-quality, compositionally-correct anchor frame from this new prompt; and (3) Temporal Synthesis, where a video model, finetuned to understand this anchor, focuses its entire capacity on animating the scene and following the prompt. Our decomposed approach sets a new state-of-the-art on the T2V CompBench benchmark and significantly improves all tested models on VBench2. Furthermore, we show that visual anchoring allows us to cut the number of sampling steps by 70% without any loss in performance, leading to a substantial speed-up in sampling. Factorized Video Generation offers a simple yet practical path toward more efficient, robust, and controllable video synthesis

</details>


### [50] [Adaptive Frequency Domain Alignment Network for Medical image segmentation](https://arxiv.org/abs/2512.16393)
*Zhanwei Li,Liang Li,Jiawan Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的自适应频域对齐网络（AFDAN）来解决医学图像分割中高质量标注数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割高度依赖高质量的标注数据，但人工标注耗时且费力，因此数据稀缺成为限制分割准确性的重要因素。

Method: 提出了一种适用于跨域知识迁移的深度学习框架——自适应频域对齐网络（AFDAN），该网络包含三个核心模块：对抗域学习模块、源-目标频率融合模块以及时空频率融合模块，通过在频域和空间域对齐不同域的特征，实现了精确分割。

Result: 在VITILIGO2025白癜风分割数据集上，AFDAN的IoU达到了90.9%；在DRIVE视网膜血管分割基准上，AFDAN的IoU为82.6%，均优于当前最新方法。

Conclusion: AFDAN有效解决了医学图像分割领域的数据稀缺和跨域泛化问题，能够在多个数据集上实现更优异的分割表现。

Abstract: High-quality annotated data plays a crucial role in achieving accurate segmentation. However, such data for medical image segmentation are often scarce due to the time-consuming and labor-intensive nature of manual annotation. To address this challenge, we propose the Adaptive Frequency Domain Alignment Network (AFDAN)--a novel domain adaptation framework designed to align features in the frequency domain and alleviate data scarcity. AFDAN integrates three core components to enable robust cross-domain knowledge transfer: an Adversarial Domain Learning Module that transfers features from the source to the target domain; a Source-Target Frequency Fusion Module that blends frequency representations across domains; and a Spatial-Frequency Integration Module that combines both frequency and spatial features to further enhance segmentation accuracy across domains. Extensive experiments demonstrate the effectiveness of AFDAN: it achieves an Intersection over Union (IoU) of 90.9% for vitiligo segmentation in the newly constructed VITILIGO2025 dataset and a competitive IoU of 82.6% on the retinal vessel segmentation benchmark DRIVE, surpassing existing state-of-the-art approaches.

</details>


### [51] [Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture](https://arxiv.org/abs/2512.16397)
*Haodi He,Jihun Yu,Ronald Fedkiw*

Main category: cs.CV

TL;DR: 本论文提出了一种利用高斯Splatting进行高质量人脸三维重建的方法，仅需少量未校准人脸图片即可重建，并能高效融入标准图形管线。


<details>
  <summary>Details</summary>
Motivation: 三维神经表示（如NeRF）虽流行但难以被约束，导致在需要高一致性的几何结构时存在局限；而传统方法对输入多、约束多。该论文目的是寻找一个更显式、易融合的三维人脸重建方式，支持更少样本、更高灵活性和可靠性。

Method: 作者结合高斯Splatting和人脸分割语义，将分割标注用于语义区域对齐。采用高斯点软约束到三角面片表面的方法，提升结构化重建；解析准确几何后，将高斯点映射到纹理空间，作为依赖视点的神经纹理使用。不依赖严格的输入条件，并使用可重新光照的高斯模型分离纹理和光照，从影像中获得高分辨率去光照的反照率纹理。最终可无缝整合进标准图形流程，也能适用于资产自动生成。

Result: 实验表明，本方法能用极少的照片（11张）高质量地重建中性姿态人脸表面和反照率纹理，并且这些结果可直接用于标准渲染管线以及结合文本驱动的资产创建。不同光照、设备数据下也表现鲁棒。

Conclusion: 论文提出的方法提高了三维人脸重建的效率和质量，实现了小样本、跨数据源的高质量重建，并显著提升了与传统图形流程和自动资产创建的兼容性和实用性。

Abstract: We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.

</details>


### [52] [BrepLLM: Native Boundary Representation Understanding with Large Language Models](https://arxiv.org/abs/2512.16413)
*Liyuan Deng,Hao Guo,Yunpeng Bai,Yongkang Dai,Huaxi Huang,Yilei Shi*

Main category: cs.CV

TL;DR: 本文提出了BrepLLM，这是第一个使大语言模型（LLM）可以解析和理解复杂3D边界表示（Brep）数据的框架，显著提升了3D几何与自然语言之间的交互能力，在3D对象分类和描述任务上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于token序列的大模型难以直接处理包含复杂几何与拓扑信息的3D Brep模型，因此亟需一种能桥接3D结构化几何数据与自然语言的有效方法。

Method: BrepLLM采取双阶段训练流程：第一阶段利用自适应UV采样将Brep转换为包含几何和拓扑信息的图结构，并通过层次化Brep编码器提取全局与节点特征，再与CLIP文本编码对齐；第二阶段将预训练的BrepEncoder接入LLM，通过三步渐进方式训练：MLP语义映射、LLM微调及引入Mixture-of-Query Experts增强几何多样性建模。还新构建了Brep2Text大规模问答数据集。

Result: BrepLLM在3D对象分类与文本生成任务上实现了当前最优（SOTA）结果，优于传统方法。

Conclusion: 该工作成功实现了3D Brep数据到自然语言的映射，显著提升了LLM对结构化3D数据的理解和推理能力，为多模态融合和3D-AI应用开辟了新路径。

Abstract: Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.

</details>


### [53] [CountZES: Counting via Zero-Shot Exemplar Selection](https://arxiv.org/abs/2512.16415)
*Muhammad Ibraheem Siddiqui,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 本文提出了CountZES，无需训练即可在零样本设定下对复杂场景中的未知类别目标进行计数，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 零样本目标计数(ZOC)极具挑战性，因为只能根据类别名计数未见类别。现有方法在实例分割和代表性选择上均存在局限，因此需要更有效的解决方案。

Method: CountZES为一个免训练的三阶段计数框架，包括检测锚定（DAE）、密度引导（DGE）和特征一致性（FCE）：DAE精炼检测结果得到精准单实例模板，DGE基于密度自监督获取统计一致且语义紧凑的模板，FCE在特征空间聚类以增强视觉一致性。三者协同生成代表性强的多样化样本集用于计数。

Result: 在自然、遥感及医学等多种数据集上，CountZES表现优于现有ZOC方法，且具备良好的泛化能力。

Conclusion: CountZES为无需训练、泛化性强且表现卓越的零样本目标计数方法，适用于多种应用场景，对文本、计数一致性和特征多样性三方面取得良好平衡。

Abstract: Object counting in complex scenes remains challenging, particularly in the zero-shot setting, where the goal is to count instances of unseen categories specified only by a class name. Existing zero-shot object counting (ZOC) methods that infer exemplars from text either rely on open-vocabulary detectors, which often yield multi-instance candidates, or on random patch sampling, which fails to accurately delineate object instances. To address this, we propose CountZES, a training-free framework for object counting via zero-shot exemplar selection. CountZES progressively discovers diverse exemplars through three synergistic stages: Detection-Anchored Exemplar (DAE), Density-Guided Exemplar (DGE), and Feature-Consensus Exemplar (FCE). DAE refines open-vocabulary detections to isolate precise single-instance exemplars. DGE introduces a density-driven, self-supervised paradigm to identify statistically consistent and semantically compact exemplars, while FCE reinforces visual coherence through feature-space clustering. Together, these stages yield a diverse, complementary exemplar set that balances textual grounding, count consistency, and feature representativeness. Experiments on diverse datasets demonstrate CountZES superior performance among ZOC methods while generalizing effectively across natural, aerial and medical domains.

</details>


### [54] [Geometric Disentanglement of Text Embeddings for Subject-Consistent Text-to-Image Generation using A Single Prompt](https://arxiv.org/abs/2512.16443)
*Shangxun Li,Youngjung Uh*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的新方法，通过优化文本嵌入，显著提升了扩散生成模型在多图像生成任务中的主体一致性和文本对齐度。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像的扩散模型在多图故事生成中，难以保持同一主体的一致性，已有方法代价高昂且效率低下，亟需轻量级的解决方案。

Method: 提出了一种基于几何视角的文本嵌入优化方法，通过抑制不需要的语义，实现语义解耦，避免1Prompt1Story方法中的语义泄漏和文本错位。该方法完全无需训练。

Result: 大量实验对比显示，该方法在主体一致性和文本对齐方面均优于现有基线方法。

Conclusion: 本文方案为多图像生成中的主体一致性问题提供了高效有效的新途径，对视觉故事生成等任务具有重要意义。

Abstract: Text-to-image diffusion models excel at generating high-quality images from natural language descriptions but often fail to preserve subject consistency across multiple outputs, limiting their use in visual storytelling. Existing approaches rely on model fine-tuning or image conditioning, which are computationally expensive and require per-subject optimization. 1Prompt1Story, a training-free approach, concatenates all scene descriptions into a single prompt and rescales token embeddings, but it suffers from semantic leakage, where embeddings across frames become entangled, causing text misalignment. In this paper, we propose a simple yet effective training-free approach that addresses semantic entanglement from a geometric perspective by refining text embeddings to suppress unwanted semantics. Extensive experiments prove that our approach significantly improves both subject consistency and text alignment over existing baselines.

</details>


### [55] [Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach](https://arxiv.org/abs/2512.16456)
*Masashi Hatano,Saptarshi Sinha,Jacob Chalk,Wei-Hong Li,Hideo Saito,Dima Damen*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的人体运动生成方法，专注于实现眼动预瞄（gaze priming）和目标接触的自然动作生成，并构建了大规模的带有眼动标注的动作数据集，实验评估体现出较高的生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前的人体动作生成方法主要关注自然性和目标指向，但很少专门模拟人在进行目标操作前的“注视-预瞄”行为。本文针对这一关键但被忽视的行为，提出高质量数据集和评价指标，以推动该方向进步。

Method: 作者首先从五个公开数据集中筛选并整合了2.37万条含“注视-预瞄”过程的人体动作序列，然后用文本条件的扩散模型进行预训练，并在构建的数据上进行目标姿态或位置的条件微调。提出并采用了“Prime Success”新指标，用于评估动作是否生动且贴近人类真实表现。

Result: 在最大的数据集HD-EPIC上，模型在目标位置条件下达到了60%的“眼动预瞄成功率”（Prime Success）和89%的“到达成功率”（Reach Success），显示出较强的目标指向和自然性。

Conclusion: 所提方法和数据集有效提升了目标相关的人体动作生成性能，尤其是模拟人类复杂的盯视-预瞄行为。构建的新评价指标和丰富数据对未来相关研究具有推动作用。

Abstract: Human motion generation is a challenging task that aims to create realistic motion imitating natural human behaviour. We focus on the well-studied behaviour of priming an object/location for pick up or put down -- that is, the spotting of an object/location from a distance, known as gaze priming, followed by the motion of approaching and reaching the target location. To that end, we curate, for the first time, 23.7K gaze-primed human motion sequences for reaching target object locations from five publicly available datasets, i.e., HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. We pre-train a text-conditioned diffusion-based motion generation model, then fine-tune it conditioned on goal pose or location, on our curated sequences. Importantly, we evaluate the ability of the generated motion to imitate natural human movement through several metrics, including the 'Reach Success' and a newly introduced 'Prime Success' metric. On the largest dataset, HD-EPIC, our model achieves 60% prime success and 89% reach success when conditioned on the goal object location.

</details>


### [56] [StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models](https://arxiv.org/abs/2512.16483)
*Senmao Li,Kai Wang,Salman Khan,Fahad Shahbaz Khan,Jian Yang,Yaxing Wang*

Main category: cs.CV

TL;DR: 本文提出了一种面向视觉自回归（VAR）模型的阶段感知加速框架StageVAR，在保证生成质量的前提下，显著提升大规模图像生成的效率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉自回归（VAR）模型虽然有效提升了高质量图像生成能力，但大规模步骤下的计算复杂度和运行时间大幅增加，且现有加速方法需要手动选择步骤，未充分利用生成过程各阶段的重要性差异。

Method: 作者系统分析了VAR生成过程中各阶段的作用，发现早期步骤对语义和结构一致性至关重要，应保持不变；而后期步骤主要负责细节修饰，可采用低秩近似或裁剪进行加速。基于此，提出了一种插拔式StageVAR加速框架，无需额外训练，通过阶段感知操作有选择性地加速后期计算。

Result: StageVAR在不显著损失生成质量（GenEval指标仅下降0.01，DPG下降0.26）的前提下，实现了最高3.4倍的加速效果，且在各项基准加速任务上均优于已有方法。

Conclusion: 论文表明，阶段感知的加速策略能高效提升视觉自回归模型的推理效率，是未来大规模高效图像生成的重要设计原则。

Abstract: Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation.

</details>


### [57] [Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment](https://arxiv.org/abs/2512.16484)
*Yuan Li,Yahan Yu,Youyuan Lin,Yong-Hao Yang,Chenhui Chu,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 本研究提出了一种结合人类感知和推理过程的无参考图像质量评估方法，通过强化学习和人类评价数据指导模型，提升模型在人类解释及一贯性推理上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有无参考图像质量评估（BIQA）系统难以模拟人类在评价图像质量时的感知与推理融合过程，也缺乏具有人类自洽性推理的能力与解释性。因此，需要一种方法让模型能够学习到人类感知与推理的联合过程，并具备一定的自我解释和推理能力。

Method: 1）收集涵盖人类感知-推理链条的图像质量评价数据；2）以强化学习为基础，采用人类标注为奖励信号，引导模型学习人类类似的感知与推理方式；3）设计新的奖励机制，促使模型能够依据自身生成的描述内容推断图像质量，从而实现自洽性推理能力；4）采用通用指标（如Pearson和Spearman相关系数）评估分数预测表现，并通过ROUGE-1比较模型与人类的推理链相似度。

Result: 在分数预测上，该方法在Pearson和Spearman相关系数等主流指标上与SOTA BIQA系统表现相当。模型在ROUGE-1指标上达0.512，显著优于基线模型（0.443），表明其生成的推理链与人类高度一致。

Conclusion: 本工作证明，通过结合人类标注和强化学习可以提升BIQA系统的人类类推理和自洽性推理能力，实现更具解释性的图像质量评估。这为构建更人性化的无参考图像质量评价模型铺平了道路。

Abstract: Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.

</details>


### [58] [Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors](https://arxiv.org/abs/2512.16485)
*Kejun Liu,Yuanyuan Liu,Lin Wei,Chang Tang,Yibing Zhan,Zijing Chen,Zhe Chen*

Main category: cs.CV

TL;DR: 该论文提出将眼动行为作为情感识别（ER）中的重要线索，构建了包含眼动数据的多模态情感识别数据集（EMER），并设计了结合眼动行为与面部表情的Transformer模型（EMERT），该方法在多项评估中优于现有方法，突出眼动行为对情感识别的价值。


<details>
  <summary>Details</summary>
Motivation: 目前情感识别领域过度依赖面部表情识别（FER），而面部表情往往是社交工具，并不总是真实情感的反映。为弥补FER和真实ER之间的差距，作者尝试引入更加真实可靠的情感线索。

Method: 构建基于自然情感诱发范式收集的数据集（EMER），数据同步记录眼动行为（如眼动序列、注视图）及面部表情。同时，分别对多模态ER和FER标签进行多视角标注。基于新数据集，提出结合眼动和表情的EMERT模型，采用模态对抗特征解耦和多任务Transformer结构。

Result: 在七项多模态基准评估协议下，EMERT显著超越当前多模态方法，显示结合眼动行为显著提升情感识别准确性。

Conclusion: 本研究系统性证明了眼动行为在情感识别中的重要作用，并通过发布数据集和模型推动FER与ER差距的弥合，为更加鲁棒的情感识别系统提供基础资源和方法。

Abstract: Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.

</details>


### [59] [YOLO11-4K: An Efficient Architecture for Real-Time Small Object Detection in 4K Panoramic Images](https://arxiv.org/abs/2512.16493)
*Huma Hafeez,Matthew Garratt,Jo Plested,Sankaran Iyer,Arcot Sowmya*

Main category: cs.CV

TL;DR: 提出了YOLO11-4K，一个专为4K全景（360度）图像优化的高效实时目标检测框架，在极高分辨率下兼顾了检测速度和精度。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测器（如YOLO）主要针对常规低分辨率图片，在4K及更高分辨率下的全景图中，检测速度慢且小目标易遗漏，难以满足实际应用需求。

Method: 设计了YOLO11-4K架构，包括新增的多尺度检测头（引入P2层，提高对小目标的敏感性）和GhostConv主干网络（降低计算量），手动标注了CVIP360数据集以评测模型。

Result: YOLO11-4K在CVIP360 4K全景图像数据集上，以0.50 IoU下达到0.95 mAP，单帧推理时间28.3ms，比原YOLO11快75%，同时精度也提升。

Conclusion: YOLO11-4K兼具高效率与高精度，适合4K全景图等高分辨率目标检测实际应用，该方法也可推广到自动驾驶、安防和增强现实等领域的高分辨率检测任务中。

Abstract: The processing of omnidirectional 360-degree images poses significant challenges for object detection due to inherent spatial distortions, wide fields of view, and ultra-high-resolution inputs. Conventional detectors such as YOLO are optimised for standard image sizes (for example, 640x640 pixels) and often struggle with the computational demands of 4K or higher-resolution imagery typical of 360-degree vision. To address these limitations, we introduce YOLO11-4K, an efficient real-time detection framework tailored for 4K panoramic images. The architecture incorporates a novel multi-scale detection head with a P2 layer to improve sensitivity to small objects often missed at coarser scales, and a GhostConv-based backbone to reduce computational complexity without sacrificing representational power. To enable evaluation, we manually annotated the CVIP360 dataset, generating 6,876 frame-level bounding boxes and producing a publicly available, detection-ready benchmark for 4K panoramic scenes. YOLO11-4K achieves 0.95 mAP at 0.50 IoU with 28.3 milliseconds inference per frame, representing a 75 percent latency reduction compared to YOLO11 (112.3 milliseconds), while also improving accuracy (mAP at 0.50 of 0.95 versus 0.908). This balance of efficiency and precision enables robust object detection in expansive 360-degree environments, making the framework suitable for real-world high-resolution panoramic applications. While this work focuses on 4K omnidirectional images, the approach is broadly applicable to high-resolution detection tasks in autonomous navigation, surveillance, and augmented reality.

</details>


### [60] [PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2512.16494)
*Mengyuan Liu,Jiajie Liu,Jinyan Zhang,Wenhao Li,Junsong Yuan*

Main category: cs.CV

TL;DR: 提出了一种基于专家混合（Mixture-of-Experts, MoE）网络的新方法PoseMoE，有效提升了单目3D人体姿态估计精度，特别是在2D和depth解耦编码方面优于传统lifting-based方法。


<details>
  <summary>Details</summary>
Motivation: Lifting-based方法常将2D姿态检测作为中间表示用于3D姿态估计，但2D和深度depth共用特征空间导致不确定的depth影响2D姿态精度，限制了整体性能。

Method: 提出PoseMoE方法：1）采用专家混合网络，将2D姿态和depth特征分开编码，分别由专业子模块处理，其中2D特征由已检测2D姿态精炼，depth特征由网络估计学习；2）引入跨专家知识聚合模块，实现2D与depth的双向特征增强和时空上下文信息融合。

Result: 在Human3.6M、MPI-INF-3DHP和3DPW三大主流数据集上，PoseMoE在3D人体姿态估计算法中取得了优于传统lifting-based方法的实验结果。

Conclusion: 解耦2D姿态与depth特征编码及其专门精炼机制，有效提升了单目3D姿态估计精度。PoseMoE方法证实特征分离与交互的策略更为高效，可为相关任务提供新思路。

Abstract: The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.

</details>


### [61] [VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks](https://arxiv.org/abs/2512.16501)
*Beitong Zhou,Zhexiao Huang,Yuan Guo,Zhangxuan Gu,Tianyu Xia,Zichen Luo,Fei Tang,Dehan Kong,Yanyi Shang,Suling Ou,Zhenlin Guo,Changhua Meng,Shuheng Shen*

Main category: cs.CV

TL;DR: 该论文提出了VenusBench-GD，一个大规模、跨平台、双语的GUI定位基准数据集，并通过系统性的分层评测揭示了多模态和特定GUI模型在不同任务下的优劣。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI定位基准数据集要么数据量太小、覆盖面窄，要么过度聚焦于单一平台并依赖高度专业的领域知识，难以满足实际需求，因此需要一个更全面、多样化的评测基准。

Method: 作者构建了VenusBench-GD数据集，涵盖多平台、多应用、多种UI元素，提供高质量、丰富的标注数据。建立了高精度的数据处理与标注流程，并提出了分层的GUI定位任务分类体系，将任务划分为基础与高级两类共六个子任务，从多角度评测模型能力。

Result: 实验结果表明，通用多模态模型在基础GUI定位任务上已达到甚至超越专用GUI模型的水平，但在高级任务上仍需专用模型发挥优势，而这些模型存在明显的过拟合与鲁棒性不足问题。

Conclusion: 论文强调了建立综合性、多层次GUI定位评测体系的重要性，为模型开发和实际应用提供了有价值的方向和参考。

Abstract: GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.

</details>


### [62] [Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization](https://arxiv.org/abs/2512.16504)
*Qiushuo Cheng,Jingjing Liu,Catherine Morgan,Alan Whone,Majid Mirmehdi*

Main category: cs.CV

TL;DR: 本文提出了一种自监督预训练方法，通过片段判别任务和特征融合，有效提升了骨架动作定位的表现，并在多个数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在骨架动作识别方面已有成功，但在骨架动作的时序定位领域仍面临挑战，因其需要对标签变化点的细微时序特征更为敏感，现有方法不足以胜任高精度的动作边界检测。

Method: 作者提出了一种片段判别的自监督预训练任务，将骨架序列划分为不重叠片段，通过对比学习鼓励区分不同片段特征，并采用U型结构融合主干网络的中间特征，提升帧级定位分辨率。

Result: 该方法在BABEL数据集的多个评测协议和子集上，实现了相比于现有骨架对比学习方法的显著提升；此外，在NTU RGB+D和BABEL预训练后，迁移到PKUMMD数据集取得了当前最优性能。

Conclusion: 所提方法能够有效提升骨架动作的时序定位能力，并具备良好的迁移表现，对复杂动作边界检测提供了有力手段。

Abstract: The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.

</details>


### [63] [Multi-scale Attention-Guided Intrinsic Decomposition and Rendering Pass Prediction for Facial Images](https://arxiv.org/abs/2512.16511)
*Hossein Javidnia*

Main category: cs.CV

TL;DR: 本文提出了MAGINet，一种多尺度注意力引导的网络，可从单张RGB人脸照片中精确预测光照标准化后的漫反射反照率图和其他面部物理渲染通道，推进行业在真实人脸重照明和材质编辑方面的进步。


<details>
  <summary>Details</summary>
Motivation: 人脸图像在自然光照下的内在属性分解是数字克隆、增强现实和真实再光照等应用的基础。然而，光照变化、细节保留和更高的渲染保真度是当前方法的难题。因此，开发高性能的内在属性分解方法，有助于推动相关应用。

Method: 作者提出MAGINet网络，采用分层残差编码、空间与通道注意力机制及自适应多尺度特征融合提升分解精度，随后用RefinementNet进一步细化，通过Pix2PixHD架构预测法线、环境闭塞、镜面反射等五个物理渲染通道。训练中结合多种损失函数，在高质量数据集上优化模型。

Result: 该方法在漫反射反照率估计达到最新最优水平，且完整渲染通道在重照明和材质编辑场景下优于以往方法。实验表明，新模型边界更清晰，对光照变化更鲁棒。

Conclusion: MAGINet及其全流程实现了更高的物理分解保真度，为高质量人脸重照明和材质编辑奠定了基础，推动实际应用性能和视觉效果的提升。

Abstract: Accurate intrinsic decomposition of face images under unconstrained lighting is a prerequisite for photorealistic relighting, high-fidelity digital doubles, and augmented-reality effects. This paper introduces MAGINet, a Multi-scale Attention-Guided Intrinsics Network that predicts a $512\times512$ light-normalized diffuse albedo map from a single RGB portrait. MAGINet employs hierarchical residual encoding, spatial-and-channel attention in a bottleneck, and adaptive multi-scale feature fusion in the decoder, yielding sharper albedo boundaries and stronger lighting invariance than prior U-Net variants. The initial albedo prediction is upsampled to $1024\times1024$ and refined by a lightweight three-layer CNN (RefinementNet). Conditioned on this refined albedo, a Pix2PixHD-based translator then predicts a comprehensive set of five additional physically based rendering passes: ambient occlusion, surface normal, specular reflectance, translucency, and raw diffuse colour (with residual lighting). Together with the refined albedo, these six passes form the complete intrinsic decomposition. Trained with a combination of masked-MSE, VGG, edge, and patch-LPIPS losses on the FFHQ-UV-Intrinsics dataset, the full pipeline achieves state-of-the-art performance for diffuse albedo estimation and demonstrates significantly improved fidelity for the complete rendering stack compared to prior methods. The resulting passes enable high-quality relighting and material editing of real faces.

</details>


### [64] [TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models](https://arxiv.org/abs/2512.16523)
*Zhiwei Li,Yitian Pang,Weining Wang,Zhenan Sun,Qi Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为Test-Time Padding (TTP)的新型轻量化防御框架，显著提升了CLIP等视觉-语言模型在对抗攻击下的鲁棒性，并且不会影响正常样本的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在零样本识别表现优异，但极易受到对抗扰动影响，尤其在安全关键场景下存在风险。现有防御方法需重新训练或识别不可靠，阻碍其实用化。

Method: TTP在推理阶段通过检测特征嵌入前后（空间填充）的余弦相似度变化识别对抗样本，利用普适阈值进行判别。检测到对抗输入后，通过可训练填充和基于相似性的集成策略进行有针对性的自适应修正。对正常样本则保持不变，或可选用其他测试时自适应方案提升准确率。

Result: 在多种CLIP架构和细粒度基准实验中，TTP在对抗鲁棒性和正常准确率方面均超越了当前最先进的测试时防御方法。

Conclusion: TTP为视觉-语言模型提供了一种无需重新训练、简单高效且通用的对抗防御方案，有效兼顾了鲁棒性与实用性。

Abstract: Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.

</details>


### [65] [N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.16561)
*Yuxin Wang,Lei Ke,Boqiang Zhang,Tianyuan Qu,Hanxun Yu,Zhenpeng Huang,Meng Yu,Dan Xu,Dong Yu*

Main category: cs.CV

TL;DR: 本文提出了N3D-VLM，一种能进行本地3D目标感知和三维空间推理的新型多模态视觉-语言模型。该方法通过创新性地提升二维图像标注为三维信息，实现更精准的三维定位与结构化空间理解，显著提升了三维推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型主要基于2D图像进行理解，缺乏对三维空间和物体深度关系的原生感知能力，导致空间理解和推理能力受限，因此需要开发原生具备3D感知和推理能力的统一模型框架。

Method: 提出N3D-VLM框架，通过引入原生的三维物体感知模块，使模型可以依据文本描述直接在三维空间中定位物体，并基于准确三维定位，执行显式的三维空间推理。同时，构建了可扩展的数据流水线，将大量2D标注转换为3D训练数据，并生成支持三维推理的问答数据集，实现了3D定位与推理的联合训练。

Result: 该框架在三维物体定位和三维空间推理任务上实现了新的最优性能，在同类视觉-语言模型中，三维推理能力显著优于现有方法。

Conclusion: N3D-VLM为多模态模型带来了原生的三维物体感知和推理能力，提升了模型对空间结构和关系的解释性和表现力，对未来三维场景理解和推理任务具有重要意义。

Abstract: While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.

</details>


### [66] [4D Primitive-Mâché: Glueing Primitives for Persistent 4D Scene Reconstruction](https://arxiv.org/abs/2512.16564)
*Kirill Mazur,Marwan Taher,Andrew J. Davison*

Main category: cs.CV

TL;DR: 本文提出了一个动态重建系统，可输入单目RGB视频，并输出完整且可持续的场景重建，包括当前和先前观察到的区域，实现4D（时-空）重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以用单目RGB视频实现场景全局及时变重建，尤其对对象消失时的运动连续性和场景的跨时重现能力有限。本文致力于解决这些难题，提升场景感知和重建质量。

Method: 方法将场景分解为多个刚性3D基元，并假定这些基元在场景中运动。利用致密2D对应关系，联合优化推断这些基元的刚体运动，实现4D动态重建。针对消失物体，通过运动分组实现运动外推，保持运动连续性。

Result: 系统在物体扫描和多物体数据集上，定量和定性均显著优于现有方法，实现了可回放的3D动态场景重建、多物体扫描和对象持续性感知等功能。

Conclusion: 提出的系统能持久、完整地重建时变场景，兼顾运动连续性和全局场景感知，具备优异的4D时空重建能力，显著优于此前方法。

Abstract: We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps.
  Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity.
  The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.

</details>


### [67] [Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2512.16567)
*Yin Zhang,Yongqiang Zhang,Yaoyue Zheng,Bogdan Raducanu,Dan Liu*

Main category: cs.CV

TL;DR: 本文提出了Causal-Tune方法，通过频谱分析和因果机制分离视觉基础模型特征中的因果与非因果成分，从而提升跨域语义分割性能，尤其在恶劣天气条件下效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有微调视觉基础模型（VFM）的方法在跨域语义分割任务中忽视了长期预训练过程中产生的伪影（artifacts），这些伪影通常与非因果因素相关，影响模型泛化。现有方法未能有效识别和处理这些非因果因素。

Method: 作者提出Causal-Tune方法：1）利用离散余弦变换（DCT）提取VFM每一层特征的频谱；2）通过高斯带通滤波分离因果与非因果成分；3）引入一组在频域作用的可学习因果感知token进一步优化因果成分，并丢弃非因果部分；4）精炼后的特征通过逆DCT还原到空间域输入下一层。

Result: 在多项跨域任务中，Causal-Tune方法表现优秀。特别是在恶劣天气（如雪天）场景下，mIoU指标相比基线提升了4.8%。

Conclusion: 针对VFM特征中的因果与非因果因素，结合频谱分析与因果机制的Causal-Tune能够更有效地实现跨域泛化，在DGSS领域取得了领先性能。

Abstract: Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.

</details>


### [68] [CRONOS: Continuous Time Reconstruction for 4D Medical Longitudinal Series](https://arxiv.org/abs/2512.16577)
*Nico Albert Disch,Saikat Roy,Constantin Ulrich,Yannick Kirchhoff,Maximilian Rokuss,Robin Peretzke,David Zimmerer,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: CRONOS是一种新颖的3D医学扫描时序预测框架，可以处理多个历史扫描和任意时间点的预测任务，支持连续时间预测并取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有的3D医学扫描预测方法往往局限于只能使用单次历史扫描、固定的时间间隔或仅预测整体标签信息，难以适应实际中患者扫描数据采样不规则、需求多样化等问题，特别是在需要精细到体素级别并支持灵活时间点预测的场景下。

Method: 提出了CRONOS框架，能够从多个历史3D扫描（context volumes）预测任意时间点的目标扫描（many-to-one预测），并首次实现了3D医学数据的连续时间序列到图像的预测。其核心方法是在3D体素空间中直接学习时空速度场，将历史扫描推演到目标时间点的扫描。模型单体即可支持固定时间网格（离散）和连续实数时间的预测。

Result: 在Cine-MRI、灌注CT和纵向MRI三个公开数据集上，CRONOS均优于现有方法（baseline），同时保持了较好的计算效率。

Conclusion: CRONOS为3D医学扫描时序预测提供了统一、灵活且高效的新方案，不仅提升了精度，还增强了时间预测的灵活性。作者计划开源代码和评测协议，促进该领域的可复现性和基准测试。

Abstract: Forecasting how 3D medical scans evolve over time is important for disease progression, treatment planning, and developmental assessment. Yet existing models either rely on a single prior scan, fixed grid times, or target global labels, which limits voxel-level forecasting under irregular sampling. We present CRONOS, a unified framework for many-to-one prediction from multiple past scans that supports both discrete (grid-based) and continuous (real-valued) timestamps in one model, to the best of our knowledge the first to achieve continuous sequence-to-image forecasting for 3D medical data. CRONOS learns a spatio-temporal velocity field that transports context volumes toward a target volume at an arbitrary time, while operating directly in 3D voxel space. Across three public datasets spanning Cine-MRI, perfusion CT, and longitudinal MRI, CRONOS outperforms other baselines, while remaining computationally competitive. We will release code and evaluation protocols to enable reproducible, multi-dataset benchmarking of multi-context, continuous-time forecasting.

</details>


### [69] [Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs](https://arxiv.org/abs/2512.16584)
*Jintao Tong,Jiaqi Gu,Yujing Lou,Lubin Fan,Yixiong Zou,Yue Wu,Jieping Ye,Ruixuan Li*

Main category: cs.CV

TL;DR: SkiLa提出了一种创新的方法，使多模态大模型能够在推理过程中动态地结合视觉与文本信息，实现更接近人类的灵活想象力与多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型虽然善于通过文本推理视觉任务，但面对需要‘视觉想象’的场景时表现较弱。而人类能在大脑中无缝融合视觉与文本思维，无需预定义工具，这激发了作者探索让模型模拟人类这种灵活多模态推理的机制。

Method: 本文提出Sketch-in-Latents（SkiLa）方法，通过在多模态大模型推理过程中直接生成视觉潜在特征（latent sketch tokens），并引入视觉语义重建机制，保证这些视觉特征具备语义对应性。模型可在文本思维模式和视觉草图模式间动态切换，实现“想象”般的多步推理。

Result: 在大量实验中，SkiLa在以视觉为核心的任务上表现出色，并在多种通用多模态基准测试中展现了优异的广泛泛化能力。

Conclusion: SkiLa拓展了多模态大模型联合视觉与文本自主生成与推理的能力，实现了无需外部工具即可灵活视-文想象，提升了模型在复杂多模态场景下的表现与泛化能力。

Abstract: While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.

</details>


### [70] [Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks](https://arxiv.org/abs/2512.16586)
*Shaohua Wu,Tong Yu,Shenling Wang,Xudong Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种基于Swin-Transformer结构的文本条件扩散模型Yuan-TecSwin，大幅提升了图像生成质量和模型对长距离语义信息的理解能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于卷积神经网络（CNN）的扩散模型在捕捉图像中长距离的语义关系时存在局限，无法充分利用全局上下文，导致生成的图像在细节或整体语义一致性上还有待提升。

Method: 将扩散模型中的CNN结构替换为Swin-Transformer块，以增强非局部建模能力；优化文本编码器的选取和文本嵌入的利用方式，并精心设计文本条件的融合方法；通过调整不同扩散阶段的时间步长，进一步提升了推理性能。

Result: 在ImageNet图像生成基准上，方法获得了1.37的最优FID分数，同时不需要在去噪过程中引入额外的模型模块。在用户对比实验中，受试者难以分辨模型生成的图片与真实人画的图片的差异。

Conclusion: Swin-Transformer增强了扩散模型的全局和非局部特征建模能力，使图像生成质量达到业界领先水平。结合文本条件的改进设计，Yuan-TecSwin表现出色，为高质量文本生成图像任务带来了新的方法选择。

Abstract: Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.

</details>


### [71] [Hazedefy: A Lightweight Real-Time Image and Video Dehazing Pipeline for Practical Deployment](https://arxiv.org/abs/2512.16609)
*Ayush Bhavsar*

Main category: cs.CV

TL;DR: 本文提出Hazedefy，一种轻量级、面向实时应用的视频和摄像头图像去雾管道，能够在无需GPU的情况下提升图像质量，适用于移动和嵌入式设备。


<details>
  <summary>Details</summary>
Motivation: 当前去雾算法多依赖高计算资源，难以在移动、嵌入式等普通消费级硬件上实时部署。作者旨在开发一个简单、高效、实用、可在这些平台应用的去雾方法。

Method: Hazedefy基于暗通道先验(DCP)和大气散射模型，采用了伽马自适应重建、快速传输估算并引入下界保证数值稳定性，还提出了基于分数顶部像素平均的稳健大气光估计器，并可选用色彩平衡步骤。整个流程注重计算效率和硬件兼容性。

Result: 在真实世界图像和视频的多项实验中，Hazedefy无需GPU加速即可显著提升画面可见度与对比度。

Conclusion: Hazedefy能够以低计算成本实现高质量去雾，适合移动和嵌入式实时视频增强应用，有很强的实际部署前景。

Abstract: This paper introduces Hazedefy, a lightweight and application-focused dehazing pipeline intended for real-time video and live camera feed enhancement. Hazedefy prioritizes computational simplicity and practical deployability on consumer-grade hardware, building upon the Dark Channel Prior (DCP) concept and the atmospheric scattering model. Key elements include gamma-adaptive reconstruction, a fast transmission approximation with lower bounds for numerical stability, a stabilized atmospheric light estimator based on fractional top-pixel averaging, and an optional color balance stage. The pipeline is suitable for mobile and embedded applications, as experimental demonstrations on real-world images and videos show improved visibility and contrast without requiring GPU acceleration.

</details>


### [72] [Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers](https://arxiv.org/abs/2512.16615)
*Yifan Zhou,Zeqi Xiao,Tianyi Wei,Shuai Yang,Xingang Pan*

Main category: cs.CV

TL;DR: 本文提出了一种名为Log-linear Sparse Attention (LLSA)的新稀疏注意力机制，将扩散Transformer(即DiT)中注意力与选择计算的复杂度从二次降低到对数-线性级别，极大提升了对超长token序列的效率。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer（DiT）在图像生成任务表现出色，但其自注意力的二次复杂度导致难以处理超长序列。现有稀疏注意力方法虽可压缩token降低部分计算，但仍然存在选择及计算上的高复杂度，且质量不佳。因此，亟需更高效方法支持更长序列的展开。

Method: 提出LLSA方法，通过分层结构、多级Top-K选择策略逐步精简参与注意力计算的token，并引入Hierarchical KV Enrichment机制合并不同粒度信息，使选择与注意力实现整体log-linear复杂度。同时，开发基于稀疏索引的高效GPU实现，避免了稠密掩码带来的性能瓶颈。

Result: LLSA在高分辨率原始像素空间上验证，面向256x256像素token序列时，注意力推理提速28.27倍，训练加速6.09倍，同时有效保持了生成质量。

Conclusion: LLSA大幅提升了处理超长序列的效率，为高效训练和推理扩散Transformer提供了新思路，拓宽了其在高分辨率生成等场景的应用前景。

Abstract: Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA

</details>


### [73] [Plug to Place: Indoor Multimedia Geolocation from Electrical Sockets for Digital Investigation](https://arxiv.org/abs/2512.16620)
*Kanwal Aftab,Graham Adams,Mark Scanlon*

Main category: cs.CV

TL;DR: 该论文提出了一种利用电源插座作为室内定位标记物的计算机视觉管道，通过深度学习方法实现室内多媒体定位，为数字取证和打击犯罪提供新工具。


<details>
  <summary>Details</summary>
Motivation: 目前室外多媒体定位已较为成熟，但室内定位因房间布局相似、装修多变、视觉变化大、GPS信号差及数据集受限等难题，研究进展缓慢。数字取证亟需突破室内定位能力，辅助执法部门打击人口贩卖及其他犯罪。

Method: 设计了一个三阶段深度学习流程：（1）用YOLOv11检测插座（mAP@0.5=0.843）；（2）用Xception将插座归类为12种类型（准确率0.912）；（3）插座类型映射到国家（准确率0.96，置信度>90%）。新建了插座检测和分类专用数据集，并用真实环境下的Hotels-50K和TraffickCam子集进行验证。

Result: 该管道在插座检测、分类与地理映射上获得较高准确率和置信度，在真实室内拍摄条件下（如弱光、业余视角）表现良好，比采集自旅游网站的专业照片更贴近应有场景。

Conclusion: 利用标准化插座作为室内定位特征，计算机视觉可为数字取证提供切实手段。提出的数据集、代码和模型已开放，便利后续研究和实际应用。

Abstract: Computer vision is a rapidly evolving field, giving rise to powerful new tools and techniques in digital forensic investigation, and shows great promise for novel digital forensic applications. One such application, indoor multimedia geolocation, has the potential to become a crucial aid for law enforcement in the fight against human trafficking, child exploitation, and other serious crimes. While outdoor multimedia geolocation has been widely explored, its indoor counterpart remains underdeveloped due to challenges such as similar room layouts, frequent renovations, visual ambiguity, indoor lighting variability, unreliable GPS signals, and limited datasets in sensitive domains. This paper introduces a pipeline that uses electric sockets as consistent indoor markers for geolocation, since plug socket types are standardised by country or region. The three-stage deep learning pipeline detects plug sockets (YOLOv11, mAP@0.5 = 0.843), classifies them into one of 12 plug socket types (Xception, accuracy = 0.912), and maps the detected socket types to countries (accuracy = 0.96 at >90% threshold confidence). To address data scarcity, two dedicated datasets were created: socket detection dataset of 2,328 annotated images expanded to 4,072 through augmentation, and a classification dataset of 3,187 images across 12 plug socket classes. The pipeline was evaluated on the Hotels-50K dataset, focusing on the TraffickCam subset of crowd-sourced hotel images, which capture real-world conditions such as poor lighting and amateur angles. This dataset provides a more realistic evaluation than using professional, well-lit, often wide-angle images from travel websites. This framework demonstrates a practical step toward real-world digital forensic applications. The code, trained models, and the data for this paper are available open source.

</details>


### [74] [DeContext as Defense: Safe Image Editing in Diffusion Transformers](https://arxiv.org/abs/2512.16625)
*Linghui Shen,Mingyue Cui,Xingyi Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为 DeContext 的方法，通过对输入图片添加有针对性的微扰，有效防止大型扩散模型在上下文编辑任务中对图片进行未授权的篡改，同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型具有极强的图像编辑能力，但也带来了隐私泄露、身份冒用等风险。已有工作着眼于个性化文本到图像生成的保护方法，但对最新主流的大型基于DiT的上下文编辑模型的防护手段尚未充分研究。

Method: 作者分析发现，源图片的信息主要通过多模态注意力（multimodal attention）层传递到输出，因此通过注入小幅度、定向的扰动到这些跨注意力通道，削弱信息传递，从而打断输入与输出间的联系。进一步实验证明，在早期降噪步骤和特定的transformer模块施加扰动最有效。

Result: 在Flux Kontext 和 Step1X-Edit 上的实验证明，DeContext 能有效阻止非授权图像编辑，且基本不影响图片视觉质量，表现出高效且稳健的防护能力。

Conclusion: 通过基于注意力机制的小扰动方法可以成为扩散模型图像篡改防护的有效途径，为未来防御深度伪造和隐私泄露提供了新思路。

Abstract: In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.

</details>


### [75] [SARMAE: Masked Autoencoder for SAR Representation Learning](https://arxiv.org/abs/2512.16635)
*Danxu Liu,Di Wang,Hebaixu Wang,Haoyang Chen,Wentao Jiang,Yilin Cheng,Haonan Guo,Wei Cui,Jing Zhang*

Main category: cs.CV

TL;DR: 提出了一个面向合成孔径雷达（SAR）图像的自监督学习方法——SARMAE，用于提升SAR图像表征能力，并构建了首个百万级别SAR-光学配对数据集。


<details>
  <summary>Details</summary>
Motivation: 由于SAR图像受散斑噪声影响且高质量标注数据稀缺，现有深度学习方法难以获得细粒度的语义特征，限制了SAR在遥感中的应用。

Method: 1. 构建了包含百万级 SAR 图像及其配对光学图像的数据集 SAR-1M。
2. 设计了噪声感知的掩码自编码器（SARMAE），在自监督预训练中注入SAR专有的散斑噪声（SARE）。
3. 提出“语义锚表征约束（SARC）”，利用配对的光学图像对齐SAR特征，增强语义一致性。

Result: SARMAE 在多个SAR数据集上分别在分类、检测、分割等任务上取得了最优表现。

Conclusion: SARMAE 能有效提升SAR自监督表征的鲁棒性和语义表达能力，为系统性SAR遥感任务奠定了基础。数据集和代码将开源，推动领域发展。

Abstract: Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.

</details>


### [76] [REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion](https://arxiv.org/abs/2512.16636)
*Giorgos Petsangourakis,Christos Sgouropoulos,Bill Psomas,Theodoros Giannakopoulos,Giorgos Sfikas,Ioannis Kakogeorgiou*

Main category: cs.CV

TL;DR: 本文提出了REGLUE，一种将局部和全局视觉语义信息与潜在扩散模型深度融合的新框架，在ImageNet 256x256上实现了更优性能和更快训练收敛。


<details>
  <summary>Details</summary>
Motivation: 当前潜在扩散模型(LDMs)仅通过重建式去噪目标进行间接语义监督，语义学习慢，需更长训练时间且样本质量受限。而基于视觉基础模型(VFM)引入语义的方式，往往未充分利用VFM丰富多层空间语义信息。

Method: REGLUE框架联合建模了(i) VAE图像潜变量、(ii) 局部（patch级）压缩VFM语义、(iii) 全局图像级[CLS] token，在单一SiT骨干网络内使用轻量卷积语义压缩器将VFM多层特征压缩为结构化低维表征，并与VAE潜变量融合于扩散过程；引入外部对齐损失以正则化内部表征靠近冻结的VFM目标。

Result: 在ImageNet 256x256数据集上，REGLUE在FID和收敛速度上均优于SiT-B/2、SiT-XL/2基线及REPA、ReDi、REG方法。实验结果表明：空间VFM语义至关重要，非线性压缩充分释放其作用，全局token与外部对齐作为补充增强效果。

Conclusion: REGLUE提供了一种能更深层次融合局部与全局VFM语义于扩散过程的有效框架，在提升生成质量和训练效率方面表现显著优越。

Abstract: Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .

</details>


### [77] [FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering](https://arxiv.org/abs/2512.16670)
*Ole Beisswenger,Jan-Niklas Dihlmann,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: FrameDiffuser是一种为交互式应用设计的自回归神经渲染框架，能高效生成时序一致且照片级真实感的动态画面，克服了以往单帧和视频模型时序不一致或推理缓慢的限制。


<details>
  <summary>Details</summary>
Motivation: 神经渲染要求能够将物体的几何和材质属性转换为真实感图像，特别在互动场景下，既要保证速度又需帧间一致。然而现有单图像扩散模型无法保证时序一致性，而视频模型计算资源消耗高，不适用于实时互动。为实现实时、稳定的高质量渲染，亟需新方法。

Method: FrameDiffuser引入自回归生成机制，利用上一帧输出作为当前帧的时序指导，结合实时G-buffer（场景几何、材质和表面信息）。架构融合ControlNet（结构指导）与ControlLoRA（时序一致性），并采用三阶段训练策略强化模型鲁棒性。模型在特定环境上专门训练，提升了推理速度和画面一致性。

Result: 在特定环境数据上训练的FrameDiffuser相较广义模型，在光照、阴影和反射等真实感效果上表现更好，并显著保证了长序列的时序稳定性。

Conclusion: FrameDiffuser兼顾渲染质量、速度和时序一致性，解决了扩散模型在互动神经渲染中的关键难题，有望应用于VR、游戏等需要实时、交互图像的场景。

Abstract: Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.

</details>


### [78] [Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray](https://arxiv.org/abs/2512.16685)
*Gonçalo Gaspar Alves,Shekoufeh Gorgi Zadeh,Andreas Husch,Ben Bausch*

Main category: cs.CV

TL;DR: 本文探讨了如何通过主体指纹技术识别医学影像数据中的同一受试者，从而避免开源数据集整合时的数据泄漏问题。作者采用ResNet-50结合三元组损失，并在不同影像数据和难度下验证了方法的高效性。


<details>
  <summary>Details</summary>
Motivation: 目前在整合多个开源医学影像数据集时，由于同一受试者可能出现在不同数据集中，容易造成数据泄漏，使结果评价不准确。作者希望找到一种自动化、系统化方法检测同一主体以提升数据整合的科学性和模型评价的公平性。

Method: 作者提出主体指纹（subject fingerprinting）方案：用ResNet-50神经网络结合三元组间隔损失，将每个受试者所有图像映射到潜空间中的特定区域，以便用相似度匹配的方式实现主体再识别。实验分别在3D MRI和2D X-ray数据上，考察了标准及更高难度的few-shot设置。

Result: 该方法在ChestXray-14数据集（标准模式20-way 1-shot和扩展模式500-way 5-shot）上分别达到99.10%、90.06%的高召回率；在BraTS-2021数据集（20-way 1-shot和100-way 3-shot）上分别达到99.20%、98.86%。

Conclusion: 主体指纹技术能够有效识别同一受试者，防止数据泄漏，是整合开源医学影像数据集、准确评价模型性能的有效方法。

Abstract: Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.

</details>


### [79] [Detecting Localized Deepfakes: How Well Do Synthetic Image Detectors Handle Inpainting?](https://arxiv.org/abs/2512.16688)
*Serafino Pandolfini,Lorenzo Pellegrini,Matteo Ferrara,Davide Maltoni*

Main category: cs.CV

TL;DR: 本文系统评估了现有的深度合成图像检测器在局部修复检测任务中的表现，发现虽然检测器对较大区域的修复有效，但对更局部的篡改仍有限。


<details>
  <summary>Details</summary>
Motivation: 生成式AI推动了图像局部编辑（如修复、局部涂抹）技术的发展，这些技术被潜在威胁场景广泛利用，目前主流的检测器主要针对全合成图像，对于局部编辑的检测效果尚不明确，因此需要系统检验其能力。

Method: 对多个数据集、不同生成模型、遮罩尺寸和修复方法下的局部修复图像，系统测试了最先进的深度伪造检测模型（这些模型原本只训练于全合成图像）对局部修复的检测能力。

Result: 训练于多种生成器的模型能部分迁移至局部修复检测任务，较为有效地识别中到大面积的修复或重建型修复，整体表现优于许多已有针对性的检测方法。

Conclusion: 主流伪造检测模型在检测特定类型的局部修复上表现良好，但对于更隐蔽的小范围修复仍然存在局限，需针对性提升检测的广泛适应性。

Abstract: The rapid progress of generative AI has enabled highly realistic image manipulations, including inpainting and region-level editing. These approaches preserve most of the original visual context and are increasingly exploited in cybersecurity-relevant threat scenarios. While numerous detectors have been proposed for identifying fully synthetic images, their ability to generalize to localized manipulations remains insufficiently characterized. This work presents a systematic evaluation of state-of-the-art detectors, originally trained for the deepfake detection on fully synthetic images, when applied to a distinct challenge: localized inpainting detection. The study leverages multiple datasets spanning diverse generators, mask sizes, and inpainting techniques. Our experiments show that models trained on a large set of generators exhibit partial transferability to inpainting-based edits and can reliably detect medium- and large-area manipulations or regeneration-style inpainting, outperforming many existing ad hoc detection approaches.

</details>


### [80] [SDFoam: Signed-Distance Foam for explicit surface reconstruction](https://arxiv.org/abs/2512.16706)
*Antonella Rech,Nicola Conci,Nicola Garau*

Main category: cs.CV

TL;DR: 本文提出SDFoam，一种结合显式Voronoi图与隐式有符号距离场(SDF)的3D重建方法，实现了更精准的网格重建，并保持了高效的渲染及外观质量。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF, 3D Gaussian Splatting等方法在新视图合成上取得了进展，但在精确的3D网格重建方面表现不足。RadiantFoam虽改善了效率问题，但网格重建精度仍有限。作者希望解决高效渲染与高质量网格重建难以兼得的问题。

Method: 作者提出SDFoam，结合了显式Voronoi图与隐式Signed Distance Field (SDF)，通过射线追踪优化场景，并用Eikonal正则项约束。SDF提供度量一致的等值面，促使Voronoi单元面与物体真实表面（零等值面）更好对齐，从而提升网格质量。整体方法保证了训练效率，并维持较高的光度质量。

Result: SDFoam在各种场景中显著提升了网格重建精度（如Chamfer距离显著降低），外观指标（PSNR, SSIM）与主流方法相当，效率也未下降。所得到的表面更清晰连贯，浮点/杂散物少，拓扑结构更合理。

Conclusion: SDFoam通过混合显式与隐式建模方式，有效兼顾了网格重建精度与渲染性能，为高质量表面重建提供了新方法。

Abstract: Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.

</details>


### [81] [A multi-centre, multi-device benchmark dataset for landmark-based comprehensive fetal biometry](https://arxiv.org/abs/2512.16710)
*Chiara Di Vece,Zhehua Mao,Netanell Avisdris,Brian Dromey,Raffaele Napolitano,Dafna Ben Bashat,Francisco Vasconcelos,Danail Stoyanov,Leo Joskowicz,Sophia Bano*

Main category: cs.CV

TL;DR: 本论文发布了首个公开的多中心、多设备带有标注的胎儿超声基准数据集，涵盖所有主要胎儿生物测量参数，并为领域自适应和多中心泛化研究提供了重要资源。


<details>
  <summary>Details</summary>
Motivation: 目前胎儿超声生物测量严重依赖人工标记，耗时、易受操作者影响，且各设备、机构间可重复性差，限制了AI方法的泛化和发展。缺乏多源、高质量标注数据集是发展的瓶颈。

Method: 作者收集了来自三家临床中心、七种超声设备、1,904名受检者的4,513幅超声图像，由专家手动标记关键解剖点，包括头围、腹围及股骨等主要测量。这些数据被标准化处理，提供训练/测试切分、评价代码和基线结果。作者还构建自动测量模型，专门量化了单中心与多中心情况的泛化差异。

Result: 数据和模型实验揭示：只用单中心数据训练会大幅高估模型在多中心、跨设备场景下的表现。多中心、多设备数据集更真实反映了AI模型的泛化能力和实际应用前景。

Conclusion: 此公开数据集首次覆盖了全部主要胎儿生物测量任务，是推动领域自适应与多中心泛化研究的关键资源，也为AI辅助胎儿生长评估的标准化和可靠性打下基础。所有数据、代码与标注均已开放。

Abstract: Accurate fetal growth assessment from ultrasound (US) relies on precise biometry measured by manually identifying anatomical landmarks in standard planes. Manual landmarking is time-consuming, operator-dependent, and sensitive to variability across scanners and sites, limiting the reproducibility of automated approaches. There is a need for multi-source annotated datasets to develop artificial intelligence-assisted fetal growth assessment methods. To address this bottleneck, we present an open, multi-centre, multi-device benchmark dataset of fetal US images with expert anatomical landmark annotations for clinically used fetal biometric measurements. These measurements include head bi-parietal and occipito-frontal diameters, abdominal transverse and antero-posterior diameters, and femoral length. The dataset contains 4,513 de-identified US images from 1,904 subjects acquired at three clinical sites using seven different US devices. We provide standardised, subject-disjoint train/test splits, evaluation code, and baseline results to enable fair and reproducible comparison of methods. Using an automatic biometry model, we quantify domain shift and demonstrate that training and evaluation confined to a single centre substantially overestimate performance relative to multi-centre testing. To the best of our knowledge, this is the first publicly available multi-centre, multi-device, landmark-annotated dataset that covers all primary fetal biometry measures, providing a robust benchmark for domain adaptation and multi-centre generalisation in fetal biometry and enabling more reliable AI-assisted fetal growth assessment across centres. All data, annotations, training code, and evaluation pipelines are made publicly available.

</details>


### [82] [OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition](https://arxiv.org/abs/2512.16727)
*Haochen Chang,Pengfei Ren,Buyuan Zhang,Da Li,Tianhao Han,Haoyang Zhang,Liang Xie,Hongbo Chen,Erwei Yin*

Main category: cs.CV

TL;DR: 本文提出了微手势识别数据采集与新基准OMG-Bench，并设计出基于层次化记忆的Transformer方法HMATr，实现了更高性能的微手势在线检测与分类。


<details>
  <summary>Details</summary>
Motivation: 微手势对于VR/AR交互至关重要，但因公开数据集缺乏和现有算法局限于特定任务，导致发展受限。特别是细微动作的数据收集和精确标注难度大，极大阻碍了相关研究。

Method: 作者设计了一个多视角自监督数据生成流程，结合启发式规则和专家复审半自动标注，建立了大规模公开数据集OMG-Bench，涵盖40类微手势、13,948个样本和1,272个序列。针对识别难点，提出了层次记忆增强Transformer（HMATr）架构，使用分层记忆库存储帧级细节和时窗级语义，解码中结合可学习位置信息实现动作检测和分类统一。

Result: 所提HMATr方法在OMG-Bench数据集上检测率比主流方法高出7.6%，并在各项评测指标上建立了更强的新基线。

Conclusion: 本文为微手势识别领域提供了首个大规模公开基准和高效新方法，为后续研究和应用开发打下了坚实基础。

Abstract: Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce OMG-Bench, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\% in detection rate, establishing a strong baseline for online micro gesture recognition. Project page: https://omg-bench.github.io/

</details>


### [83] [Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2512.16740)
*Yunkai Yang,Yudong Zhang,Kunquan Zhang,Jinxiao Zhang,Xinying Chen,Haohuan Fu,Runmin Dong*

Main category: cs.CV

TL;DR: 本文提出了一种面向遥感语义分割的数据合成框架（TODSynth），通过多模态扩散Transformer和任务反馈采样策略，生成高质量且任务相关的合成训练数据，有效提升下游分割性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像语义分割需要大量标注数据，但人工标注成本高。可控生成方法虽然可以合成数据，但在控制语义掩码和采样质量上存在困难，影响数据对分割任务的实际提升效果。

Method: 提出了TODSynth框架，包括多模态扩散Transformer（MM-DiT）实现文本-图像-掩码三重注意力融合，并用任务反馈指导的易插拔采样策略。此外，提出控制-校正流匹配（CRFM）方法，在模型高可塑阶段通过语义损失动态调整采样，有效提升数据质量和分割适用性。

Result: 系统评估了不同控制方式，并在少样本和复杂场景下验证该方法，发现联合三重注意力及全量微调可显著提升数据合成效果，所提方法在稳定性和下游分割任务上都优于现有可控生成方法。

Conclusion: TODSynth框架能够稳定生成与下游任务高度相关的遥感语义分割合成数据，提升了实际分割性能，为遥感领域高效数据合成提供了有效思路。

Abstract: With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.

</details>


### [84] [TreeNet: A Light Weight Model for Low Bitrate Image Compression](https://arxiv.org/abs/2512.16743)
*Mahadev Prasad Panda,Purnachandra Rao Makkena,Srivatsa Prativadibhayankaram,Siegfried Fößel,André Kaup*

Main category: cs.CV

TL;DR: 本文提出了一种新型低复杂度的图像压缩模型TreeNet，利用二叉树结构和注意力机制实现高效表示与重建，在低比特率下超越了JPEG AI，并大幅降低了模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的图像压缩方法计算复杂度高，影响实际应用。该文旨在设计一种简单高效的模型，降低复杂度同时保持甚至提升压缩性能。

Method: 提出了TreeNet模型，采用二叉树结构的编码器-解码器架构，用注意力机制融合多分支特征，并在多个标准数据集上与JPEG AI等先进模型对比。还通过消融实验分析模型内部潜表示的作用。

Result: 在低比特率下，TreeNet相比JPEG AI平均BD-rate提升了4.83%，模型复杂度降低了87.82%。

Conclusion: TreeNet大幅降低了模型复杂度并保持甚至提升了压缩性能，为高效学习型图像压缩提供了新的可行方案。

Abstract: Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.

</details>


### [85] [Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation](https://arxiv.org/abs/2512.16767)
*Zhiyang Guo,Ori Zhang,Jax Xiang,Alan Zhao,Wengang Zhou,Houqiang Li*

Main category: cs.CV

TL;DR: 提出了一种名为Make-It-Poseable的新方法，通过在潜在空间中转换，实现3D角色更高质量的姿态生成和编辑。


<details>
  <summary>Details</summary>
Motivation: 现有3D角色自动绑定和基于姿态的生成方法在蒙皮权重预测、模型拓扑以及适应不同姿态等方面表现不佳，难以获得高鲁棒性和泛化能力。为了解决这些问题，作者重新思考角色摆姿问题，并提出了新的潜在空间方法。

Method: 与传统直接变形网格点的方法不同，这项工作将角色的姿态变化建模为潜在空间的变换。核心是一个基于骨骼运动的latent posing transformer，利用形状token和稠密姿态表示实现精准控制，并配合潜在空间监督和自适应补全模块提高几何质量和支持拓扑变化。

Result: 该方法在姿态生成质量上明显优于现有方案，并且可以自然拓展到部件替换、细化等3D编辑应用中。

Conclusion: Make-It-Poseable显著提升了3D角色摆姿的质量和灵活性，拓宽了3D角色编辑的应用场景。

Abstract: Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.

</details>


### [86] [FlowDet: Unifying Object Detection and Generative Transport Flows](https://arxiv.org/abs/2512.16771)
*Enis Baty,C. P. Bridges,Simon Hadfield*

Main category: cs.CV

TL;DR: 提出FlowDet方法，用条件流匹配（Conditional Flow Matching）技术进行目标检测，实现性能优于此前的DiffusionDet和非生成式基线。


<details>
  <summary>Details</summary>
Motivation: 目前主流生成式目标检测方法如DiffusionDet基于扩散过程，但推理路径复杂、推理效率有限。作者希望通过更高效的生成运输策略提升性能和灵活性。

Method: 将目标检测任务视为一种生成运输问题，采用条件流匹配（Conditional Flow Matching）学习更简单、直线的推理路径，允许不重新训练的情况下调整候选框数和推理步数。

Result: FlowDet在COCO和LVIS等多个数据集和不同设置下，检测性能均超越DiffusionDet等扩散方法和标准非生成方法，在严格召回要求场景下效果尤为显著。提升最大达+3.6% AP和+4.2% AP_rare。

Conclusion: FlowDet通过更高效的生成运输方式，实现了更优的目标检测性能和更好的推理可扩展性，为生成式检测方法带来新进展。

Abstract: We present FlowDet, the first formulation of object detection using modern Conditional Flow Matching techniques. This work follows from DiffusionDet, which originally framed detection as a generative denoising problem in the bounding box space via diffusion. We revisit and generalise this formulation to a broader class of generative transport problems, while maintaining the ability to vary the number of boxes and inference steps without re-training. In contrast to the curved stochastic transport paths induced by diffusion, FlowDet learns simpler and straighter paths resulting in faster scaling of detection performance as the number of inference steps grows. We find that this reformulation enables us to outperform diffusion based detection systems (as well as non-generative baselines) across a wide range of experiments, including various precision/recall operating points using multiple feature backbones and datasets. In particular, when evaluating under recall-constrained settings, we can highlight the effects of the generative transport without over-compensating with large numbers of proposals. This provides gains of up to +3.6% AP and +4.2% AP$_{rare}$ over DiffusionDet on the COCO and LVIS datasets, respectively.

</details>


### [87] [Kling-Omni Technical Report](https://arxiv.org/abs/2512.16776)
*Kling Team,Jialu Chen,Yuanzheng Ci,Xiangyu Du,Zipeng Feng,Kun Gai,Sainan Guo,Feng Han,Jingbin He,Kang He,Xiao Hu,Xiaohua Hu,Boyuan Jiang,Fangyuan Kong,Hang Li,Jie Li,Qingyu Li,Shen Li,Xiaohan Li,Yan Li,Jiajun Liang,Borui Liao,Yiqiao Liao,Weihong Lin,Quande Liu,Xiaokun Liu,Yilun Liu,Yuliang Liu,Shun Lu,Hangyu Mao,Yunyao Mao,Haodong Ouyang,Wenyu Qin,Wanqi Shi,Xiaoyu Shi,Lianghao Su,Haozhi Sun,Peiqin Sun,Pengfei Wan,Chao Wang,Chenyu Wang,Meng Wang,Qiulin Wang,Runqi Wang,Xintao Wang,Xuebo Wang,Zekun Wang,Min Wei,Tiancheng Wen,Guohao Wu,Xiaoshi Wu,Zhenhua Wu,Da Xie,Yingtong Xiong,Yulong Xu,Sile Yang,Zikang Yang,Weicai Ye,Ziyang Yuan,Shenglong Zhang,Shuaiyu Zhang,Yuanxing Zhang,Yufan Zhang,Wenzheng Zhao,Ruiliang Zhou,Yan Zhou,Guosheng Zhu,Yongjie Zhu*

Main category: cs.CV

TL;DR: Kling-Omni 是一个通用生成框架，能根据多模态视觉语言输入直接生成高质量视频，可实现视频生成、编辑和推理等复杂任务。


<details>
  <summary>Details</summary>
Motivation: 目前多模态视频生成和编辑的主流方法存在任务割裂、流程分散的问题，难以统一处理多种输入并实现智能化的视频内容生成。作者希望设计一个端到端的系统，统一处理多种输入并具备复杂推理能力。

Method: 提出 Kling-Omni 框架，能够接受文本指令、参考图像、视频上下文等多种输入，并将其融合为统一的多模态表示。构建了全面的数据系统，用于多模态视频训练，并通过大规模预训练和推理优化提升系统效能。

Result: Kling-Omni 在上下文生成、基于推理的视频编辑以及多模态指令遵循方面表现优异，经过全面评估，展示了卓越性能。

Conclusion: Kling-Omni 不仅是一个内容创作工具，更是朝向能够理解、推理、生成并与复杂动态世界交互的多模态世界模拟器的重要进展。

Abstract: We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.

</details>


### [88] [R3ST: A Synthetic 3D Dataset With Realistic Trajectories](https://arxiv.org/abs/2512.16784)
*Simone Teglia,Claudia Melis Tonti,Francesco Pro,Leonardo Russo,Andrea Alfarano,Leonardo Pentassuglia,Irene Amerini*

Main category: cs.CV

TL;DR: 作者提出了R3ST合成数据集，将真实交通轨迹融入3D合成环境，兼具真实行为和高质量标注，用于提高交通分析模型的训练与评估。


<details>
  <summary>Details</summary>
Motivation: 现有真实交通数据集虽具真实性，但标注不精确；合成数据集虽标注精确，但车辆运动不真实。需创建兼具真实运动与精确标注的数据集。

Method: 将真实无人机拍摄的交通轨迹(SinD数据集)整合进高质量3D合成环境，构建既有真实人车运动、又可多模标注的大规模合成数据集（R3ST）。

Result: R3ST数据集实现了高真实度交通轨迹与精确多模态标注的结合，有效填补了传统合成和真实数据集的鸿沟。

Conclusion: 提出的数据集不仅提升了合成交通数据的真实性，还为道路车辆轨迹预测等研究提供了更优质的数据支持，有助于交通安全与分析领域的发展。

Abstract: Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird's-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.

</details>


### [89] [KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals](https://arxiv.org/abs/2512.16791)
*Shuting Zhao,Zeyu Xiao,Xinrong Chen*

Main category: cs.CV

TL;DR: 该论文提出了KineST，一种结合运动学先验的新型状态空间模型，实现基于头戴式设备的高效、精准、平滑的全身动作重建。


<details>
  <summary>Details</summary>
Motivation: 目前，AR/VR中的主要设备是头戴显示器（HMD），但仅能获取到稀疏信号，使得真实、多样的全身动作重建十分困难。现有方法要么计算量大，效率低，要么时空建模分离，效果难以兼顾准确性、流畅性和高效性，因此亟需新的方法来解决这一难题。

Method: 作者提出KineST模型，核心创新包括：1）在状态空间二元性框架下，引入运动学引导的双向扫描方法，更好地捕捉关节间复杂关系，并结合运动学先验；2）采用混合时空表示学习，将空间与时间上下文紧密耦合，提升平滑性与准确性；3）引入几何角速度损失，从物理角度约束旋转变化，提高动作的稳定性。

Result: 大量实验表明，KineST能在轻量化框架下实现高准确度和良好的时序一致性，在动作重建任务中性能优于现有方法。

Conclusion: KineST模型能够有效整合局部与全局姿态感知，兼顾准确性、流畅性和效率，在基于头戴设备的动作重建场景中表现卓越。

Abstract: Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/

</details>


### [90] [DenseBEV: Transforming BEV Grid Cells into 3D Objects](https://arxiv.org/abs/2512.16818)
*Marius Dähling,Sebastian Krebs,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 本文提出DenseBEV，一种基于BEV特征单元作为锚点的多摄像头3D目标检测新方法，有效提升了精度和小目标检测能力，在nuScenes和Waymo数据集上达到了新的SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有BEV-based transformer模型多采用随机查询或辅助网络生成锚点，但这些方式效率不高且直观性不足，且对于大量查询时注意力计算负担大。因此需要一种更高效、直观的锚点生成与训练方法，提升BEV 3D目标检测性能。

Method: 提出直接利用BEV feature cell作为检测锚点（object queries），以密集BEV网格中的每一个cell视为潜在目标，引入两阶段锚点生成方法。为减少大量查询带来的注意力计算压力，提出BEV版非极大值抑制（NMS），只对未被抑制的目标进行梯度传播，用于高效训练。进一步结合嵌入的时序BEV信息以及先前检测结果，实现混合时序建模，增强检测性能。

Result: 在nuScenes数据集上，DenseBEV在NDS和mAP指标上相比基线有显著提升，小目标（如行人）检测mAP提升3.8%。在Waymo数据集LET-mAP提升8%，在Waymo Open数据集上达到60.7%的LET-mAP，超越前SOTA 5.4%。

Conclusion: DenseBEV通过BEV特征单元直接作为anchor并配合BEV-NMS、时序建模，简化了anchor生成流程、提升了检测效率和小目标检测能力，并在public benchmark上刷新了性能纪录，验证了其实用价值和前景。

Abstract: In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.

</details>


### [91] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: 本文研究了YOLOv8不同变体在车牌检测与字符识别任务中的表现，提出基于YOLOv8优化的高效管线，并在两个数据集上验证了其高精度和实时性。


<details>
  <summary>Details</summary>
Motivation: 随着交通管理和车辆监控需求提升，传统车牌检测与识别方法难以同时实现高精度与实时性，特别是在复杂环境下。推动智能交通系统发展，需要更高效可靠的检测识别方案。

Method: 本文探索了YOLOv8 Nano和YOLOv8 Small两种变体在车牌检测（LPR）和字符识别任务中的表现。引入基于x轴位置的自定义字符排序方法，并分别在两个公开数据集上训练与评估。最后提出将Nano用于LPR，Small用于字符识别的优化管线。

Result: YOLOv8 Nano在LPR任务上取得0.964准确率和0.918的mAP50；YOLOv8 Small在字符识别任务上达到0.92准确率和0.91的mAP50。提出的字符排序方法实现了有效排序，整体管线兼顾高效性与性能。

Conclusion: 所提基于YOLOv8优化的车牌检测与识别方案，在保持实时性与较低硬件消耗的同时，实现高检测准确率。为智能交通系统在边缘设备的现实部署提供了坚实基础，助力城市基础设施智能化升级。

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [92] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. Muñiz-De-León,Jorge A. Rosales-de-Golferichs,Ana S. Muñoz-Rodríguez,Alejandro I. Trejo-Castro,Eduardo de Avila-Armenta,Antonio Martínez-Torteya*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级端到端结构，仅需单张胸部X光正位影像，即可生成临床放射学报告的Findings部分。在不用大规模多模态数据和临床元数据的前提下，实现了表现优于现有大型系统的性能。


<details>
  <summary>Details</summary>
Motivation: 当前SOTA自动放射学报告生成系统高度依赖多模态大规模训练和多视角影像，资源消耗大，难以在普通环境中应用。该研究旨在设计一种干预门槛低、资源需求低的高效模型，扩展自动报告生成实际应用场景。

Method: 模型由冻结的DINOv3 ViT编码器和带有解剖结构引导的GPT-2解码器组成，利用层级高斯平滑的肺部和心脏分割掩膜引入解剖注意，无需增加可训练参数，仅输入单张正位X光图像即可实现端到端生成。

Result: 在MIMIC-CXR数据集测试中，实现了五种关键病理CheXpert Macro-F1提升168%、Micro-F1提升146%，14类观察整体提升86%，RadGraph结构一致性提升9.7%。说明空间定位及结构一致性均大幅提升。

Conclusion: 小体量、纯影像输入的模型可通过解读器级解剖引导极大增强空间识别和内容一致性。无需复杂多模态输入即能达优异临床表现，便于推广。源码已开源。

Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.

</details>


### [93] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 本文指出当前文本生成图像（T2I）模型评测基准存在随时间漂移（benchmark drift）的问题，并提出了新的评测基准GenEval 2及其评测方法Soft-TIFA，以更好贴合人类判断。


<details>
  <summary>Details</summary>
Motivation: 随着T2I模型的进步，现有基准如GenEval难以反映最新模型的真实水平，出现基准漂移，导致评测结果与人类判断不符，影响模型公平可靠的评估。

Method: 作者通过实证分析，量化了GenEval评测结果与人类判断的漂移程度，并开展大规模人类实验验证。随后设计了GenEval 2基准，覆盖更广泛的视觉原语和更高的合成复杂性。同时提出Soft-TIFA评测方法，将原语判断组合，更好与人类评判一致。

Result: 实验表明，GenEval存在高达17.7%的绝对误差，已被当前模型“攻破”而失去区分力。GenEval 2更具挑战性，Soft-TIFA的评测与人类更一致，漂移风险更低。

Conclusion: 本文不仅提供了比现有基准更可靠的T2I模型评测方案，同时强调持续审查和迭代更新基准对于自动化模型评测领域的重要性。

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [94] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu,Lei Ke,Xiaohang Zhan,Longxiang Tang,Yuqi Liu,Bohao Peng,Bei Yu,Dong Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的图像编辑方法RePlan，可更好地应对复杂指令与视觉场景，通过区域规划与高效编辑显著提升精度和效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑模型在应对复杂场景与复杂指令（IV-Complexity）时，往往表现不佳，主要难点在于难以精确执行多区域复杂编辑指令，因此需要更强的规划与区域定位能力。

Method: 提出了RePlan框架，包括视觉-语言规划器和扩散式编辑器。规划器先分步推理并将指令明确对应到目标图像区域，编辑器通过免训练的注意力区域注入机制高效并行修改多个区域，无需多次修补。为增强规划能力，利用基于GRPO的强化学习在1K指令样本上训练规划器，提高了推理准确率和指令格式可靠度。还构建了IV-Edit基准数据集，专注于复杂需求下的精细区域与知识型编辑。

Result: RePlan在IV-Complexity场景下显著优于当前SOTA方法，包括那些在大规模数据上训练的主流模型。改进体现在区域定位精度更高、编辑保真度更好，尤其在多区域复杂任务中提升明显。

Conclusion: RePlan通过创新的计划-执行架构和区域注入编辑，极大增强了复杂自然语言指令下的图像编辑能力，在精细化区域分配和复杂任务完成上展现了强大优势。

Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io

</details>


### [95] [Pixel Seal: Adversarial-only training for invisible image and video watermarking](https://arxiv.org/abs/2512.16874)
*Tomáš Souček,Pierre Fernandez,Hady Elsahar,Sylvestre-Alvise Rebuffi,Valeriu Lacatusu,Tuan Tran,Tom Sander,Alexandre Mourachko*

Main category: cs.CV

TL;DR: 本文提出了一种名为Pixel Seal的新型图像与视频隐形水印方法，在不可见性和鲁棒性之间实现了更优的平衡，并显著优于当前先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前隐形水印技术难以兼顾鲁棒性与真正的不可见性，主流方法存在感知损失对人类视觉不匹配、训练不稳定以及在高分辨率场景下效果下降等问题。

Method: （1）提出仅用对抗训练的方法，摒弃像素级不可见性损失；（2）采用三阶段训练日程，将鲁棒性和不可见性训练解耦，提升收敛稳定性；（3）通过基于JND的人眼可见差异衰减和训练时推断模拟，实现高分辨率自适应，消除放大伪影。

Result: 在多种图像类型和多种变换下实验，Pixel Seal在鲁棒性与不可见性指标上均明显超过现有技术，同时通过时序水印融合方式可高效扩展到视频应用。

Conclusion: Pixel Seal方法为真实场景下图像和视频内容溯源的隐形水印提供了实用且可扩展的解决方案，兼具优良的鲁棒性和不可见性，可广泛应用于数字内容安全领域。

Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.

</details>


### [96] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: 本文提出了ReMeDI-SAM3，一种无训练、内存增强版SAM3，针对腔镜外科视频中的手术器械分割问题大幅提升了准确度，尤其是在遮挡、快速运动等复杂场景下，在公开数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手术器械分割对计算机辅助干预至关重要，但腔镜视频由于遮挡、运动、反光等问题，分割精度难以保证。现有方法如SAM3存在记忆机制粗糙、容量有限、遮挡恢复能力弱等问题，亟需改进。

Method: ReMeDI-SAM3包含三大创新模块：1）相关性过滤和遮挡感知的记忆模块以保存遮挡前关键信息；2）分段插值方法扩充有效记忆容量；3）基于特征的重识别模块和时序投票机制实现遮挡后身份可靠恢复。该方法无需额外训练即可直接提升原框架表现。

Result: 在EndoVis17与EndoVis18公开数据集的zero-shot实验下，ReMeDI-SAM3的mcIoU比分别比SAM3提升7%、16%，也优于部分需训练的方法，显示了出色的泛化与鲁棒性。

Conclusion: ReMeDI-SAM3通过创新的内存和身份恢复机制，有效应对了手术场景中的遮挡与错分问题，大幅提升无监督分割性能，为手术器械分割等医学图像分析任务提供了可靠方案。

Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.

</details>


### [97] [M-PhyGs: Multi-Material Object Dynamics from Video](https://arxiv.org/abs/2512.16885)
*Norika Wada,Kohei Yamashita,Ryo Kawahara,Ko Nishino*

Main category: cs.CV

TL;DR: 本文提出M-PhyGs方法，可从视频自动估计多材料复杂自然物体（如花）的物理材料组成及其参数，并引入了Phlowers数据集进行评估。实验结果表明该方法有效且准确。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的物体往往由多种材料组成且结构复杂，现有方法往往假设物体为单一材料或结构简单，难以适用于真实场景。为了实现对真实复杂物体物理响应的准确预测，需要发展能同时估计多种材料及其物理参数的新方法。

Method: 作者提出了Multi-material Physical Gaussians（M-PhyGs）框架，可从自然场景下的视频中，联合分割物体材料并恢复每种材料的连续介质力学参数（考虑重力影响）。该方法通过新的级联3D与2D损失，以及时序小批量处理方案，实现了高效估计。此外，作者创建了Phlowers数据集，包含人与花互动的视频，用于评估方法性能。

Result: 在Phlowers数据集上的实验显示，M-PhyGs及其组成部分在多材料物理参数估计任务中均取得了准确且有效的表现。

Conclusion: 该方法能够突破单材料、简单结构等假设，有效应用于天然多材料复杂物体，并为后续相关研究提供了兼具挑战性与实用价值的开放数据平台。

Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.

</details>


### [98] [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891)
*Haichao Zhang,Yao Lu,Lichen Wang,Yunzhe Li,Daiwei Chen,Yunpeng Xu,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出了一种名为LinkedOut的新型视频大语言模型（VLLM）表示方法，实现了高效、可解释的视频推荐，并在标准基准上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有VLLM在视频推荐等实际任务中面临诸多局限，包括高延迟、无法处理多视频输入、以及丢失细粒度视觉细节等，这主要源于缺乏一种既能保留像素级细节又能利用世界知识的有效表示。

Method: 提出LinkedOut表示，直接从VLLM在原始视频帧中提取语义知识感知token，并可借助prompt和辅助模态进行引导；引入跨层知识融合的MoE结构，从丰富的VLLM特征中自动选择合适的抽象层，实现个性化、可解释且低延迟的推荐。

Result: LinkedOut是首个不依赖人工标签、基于VLLM、直接在原始帧上运行的视频推荐方法，在多个标准数据集上取得了最新最优的推荐效果。

Conclusion: LinkedOut展示了跨层特征融合、丰富世界知识和视觉细节表现在视频推荐等下游视觉任务中的重要性，提供了一条充分发挥VLLM潜力的实践路径。

Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.

</details>


### [99] [Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation](https://arxiv.org/abs/2512.16893)
*Kaiwen Jiang,Xueting Li,Seonwook Park,Ravi Ramamoorthi,Shalini De Mello,Koki Nagano*

Main category: cs.CV

TL;DR: 本论文提出了一种结合2D扩散模型与3D显式表示的高效率、高质量头像动画方法。通过知识蒸馏，将2D扩散模型的细腻动画能力迁移到基于编码器的3D可动画表示，兼具速度与一致性，并能有效表达表情细节，克服了传统方法速度慢或细节差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于2D扩散的视频模型动画效果细腻但缺乏3D一致性和实时性，而3D显式方法速度快、一致但表情不细腻。现实应用（如数字孪生、远程呈现）既需高质量动画，又要求3D一致及高效率。现有方法很难兼得两者优点，存在质量与速度的权衡问题。该论文动机是联合两类方法优点，得到兼具速度、3D一致性与动画细节的新方案。

Method: 作者提出将2D扩散模型的动画知识以知识蒸馏方式迁移到前馈式编码器，输入野外单张图像，即可输出3D一致、可动画且富有细节的表示。动画表示与3D结构分离，动画能力直接由数据隐式学习，无需依靠受限的人脸参数化模型。动画与结构信息的融合采用高效的轻量级局部融合策略，不使用计算复杂的多层注意力机制。

Result: 本方法动画与姿态控制速度达107.31 FPS，动画质量与当前最佳水平相当，并超越了那些不得不在速度和质量间权衡的设计。

Conclusion: 该方法实现了即时、3D一致且高度表达力的人脸动画，在保证极高推理效率的同时不牺牲动画质量，有望拓展于数字孪生、虚拟人等真实场景应用。

Abstract: Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d

</details>


### [100] [FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction](https://arxiv.org/abs/2512.16900)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Kai Qiu,Chong Luo,Zuxuan Wu*

Main category: cs.CV

TL;DR: 本文提出了FlashPortrait，一种能够保留身份一致性并大幅加速推理的视频扩散模型，主要解决现有方法在长人像动画中的身份稳定性与效率难题。


<details>
  <summary>Details</summary>
Motivation: 目前面向长人像动画的扩散模型在加速的同时，难以有效保证角色身份的一致性，容易在生成过程中出现身份漂移或表情失真。本文旨在解决加速和身份保真的平衡问题。

Method: FlashPortrait采用端到端的视频扩散Transformer结构，首先利用现有工具提取与身份无关的面部表情特征，通过Normalized Facial Expression Block进行归一化，使表情特征与扩散潜变量对齐，加强身份稳定性。推理阶段，采用动态滑窗与加权融合策略实现视频片段的平滑衔接与身份一致。同时依据潜变量变化率和各扩散层导数，利用高阶导数预测未来潜变量，跳过部分去噪步骤以实现6倍加速。

Result: 在多个基准数据集上的实验证明，FlashPortrait生成的视频在视觉效果、身份一致性方面显著优于现有方法，并且推理速度最高提升6倍。

Conclusion: FlashPortrait有效兼顾了身份一致性和推理效率，为长人像动画生成任务提供了一种更优的解决方案。

Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.

</details>


### [101] [Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection](https://arxiv.org/abs/2512.16905)
*Kaixin Ding,Yang Zhou,Xi Chen,Miao Yang,Jiarong Ou,Rui Chen,Xin Tao,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Alchemist是一种面向文本到图像生成模型的数据选择框架，通过自动化元梯度方法从大规模数据集中挑选出高质量的子集，用以提升模型视觉效果和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型受限于数据集的质量，网络采集和合成图片常包含低质量或冗余样本，严重影响模型的表现。目前的数据筛选方法成本高或仅基于单维特征，且缺乏专为图像模态设计的自动化高效方案。

Method: 提出了Alchemist：首先用基于梯度信息的轻量级rater对每个样本的影响力进行评估，结合多粒度感知增强评分；随后采用Shift-Gsampling策略，自动挑选出对任务最有益的高质量数据子集，为后续模型训练提供高效数据。整个流程无需人工干预，实现了大规模、自动化的数据筛选。

Result: 在合成和网络采集的数据集上进行实验，使用Alchemist筛选出的50%数据训练下，模型的视觉质量和下游性能均超过用全部数据训练的基线。

Conclusion: Alchemist作为首个基于元梯度的自动文本到图像训练数据选择框架，在提升模型性能、减少冗余计算乃至规模化落地方面具有显著价值与应用前景。

Abstract: Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.

</details>


### [102] [VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization](https://arxiv.org/abs/2512.16906)
*Xiaoyan Cong,Haotian Yang,Angtian Wang,Yizhi Wang,Yiding Yang,Canyu Zhang,Chongyang Ma*

Main category: cs.CV

TL;DR: VIVA是一种旨在推进基于自然语言指令的视频编辑能力的新框架，使用VLM引导的编码和奖励优化策略，有效提升了模型在复杂指令下的泛化和编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的视频编辑方法普遍依赖于简单编辑操作的配对数据训练，导致难以应对多样化且复杂的真实世界指令，泛化能力有限。

Method: 1）提出VLM-based instructor，将文本指令、视频首帧与参考图像编码为具备视觉信息的指令表示，提供细粒度空间和语义上下文给扩散Transformer骨干；2）引入Edit-GRPO作为后训练阶段，将Group Relative Policy Optimization（组相对策略优化）用于视频编辑领域，以相对奖励直接优化模型的指令遵循性、内容保持和美学品质；3）设计合成数据管道，自动生成多样且高保真的基础编辑操作视频-指令配对数据。

Result: VIVA在大量实验中展现出在指令遵循性、泛化性和编辑质量上，均优于当前最先进的方法。

Conclusion: VIVA通过创新的指令编码和奖励优化机制，有效弥补了现有方法在复杂指令泛化上的不足，为基于自然语言指令的视频编辑带来了性能和效果的显著提升。

Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

</details>


### [103] [SceneDiff: A Benchmark and Method for Multiview Object Change Detection](https://arxiv.org/abs/2512.16908)
*Yuqun Wu,Chih-hao Lin,Henry Che,Aditi Tiwari,Chuhang Zou,Shenlong Wang,Derek Hoiem*

Main category: cs.CV

TL;DR: 本文提出了一种无训练的新方法SceneDiff，用于跨视角检测场景中物体的增加、删除或移动，并发布了第一个带有实例标注的多视角变化检测基准SceneDiff Benchmark。该方法在多个数据集上显著优于现有方法，且其数据集和代码将公开。


<details>
  <summary>Details</summary>
Motivation: 在现实应用（如机器人整理、建筑进程和安全监控等）中，检测场景中物体的变化非常重要，但视角不同会导致错误检测。现有方法缺乏大规模、真实、多视角的变化检测数据集，也难以处理复杂场景的对齐和物体级检测。为此，作者提出新基准和新方法推动该方向发展。

Method: （1）提出SceneDiff Benchmark，包含350对多样化视频、物体级变化注释，用于评估多视角物体变化检测；（2）提出SceneDiff方法，无需训练，利用预训练3D、分割和图像编码模型，将多视角图像在3D中对齐，提取物体区域，对比空间和语义特征以检测变化。

Result: 在多视角和双视角检测基准上，SceneDiff方法相比已有方法的AP指标分别提升94%和37.4%，结果显著优于现有方案。

Conclusion: SceneDiff数据集和新方法为多视角物体变动检测设立了新标准，显著提升了检测精度，有助于推进相关应用和研究领域。数据集和代码即将公开，有望带动更多成果。

Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.

</details>


### [104] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新型离散图像分词器SFTok，利用多步迭代机制和创新训练策略，在高压缩率下实现了领先的图像重建与生成表现。


<details>
  <summary>Details</summary>
Motivation: 现有的离散分词器在高效图像生成与多模态任务中的应用受限于重建质量不如连续分词器，影响实际应用。本研究动机是提升离散分词器重建质量，促进其在大规模多模态模型中的应用。

Method: 本文提出SFTok，一种支持多步迭代重建的离散分词器，并引入自逼式引导重建（self-forcing guided visual reconstruction）和消偏-拟合训练策略（debias-and-fitting training strategy），解决了多步过程中的训练-推理不一致问题。

Result: SFTok在每图像仅64 token的高压缩率下，于ImageNet数据集上取得了最优重建质量（rFID=1.21），并在类别到图像生成任务中表现优异（gFID=2.29）。

Conclusion: SFTok显著提升了离散分词器在高压缩下的图像重建与生成能力，为多模态生成模型带来更高效与高质量图像表示，推动离散分词器在实际多模态任务中的应用。

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [105] [Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation](https://arxiv.org/abs/2512.16913)
*Xin Lin,Meixi Song,Dizhe Zhang,Wenxuan Lu,Haodong Li,Bo Du,Ming-Hsuan Yang,Truong Nguyen,Lu Qi*

Main category: cs.CV

TL;DR: 本文提出了一种全景度量深度基础模型，能够在各种场景距离下泛化，并在多个数据集和实际测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前深度估计模型往往依赖特定场景或距离，难以在多样化的室内外场景及真实与合成数据之间泛化，因此需要一种泛用性强且适应多场景的深度估计模型。

Method: 作者通过数据整合（公开数据集、高质量合成数据、网页实拍全景图像）和三阶段的伪标签管道减少数据域差异，并采用DINOv3-Large作为主干网络，结合可插拔的距离掩码头、以锐度和几何一致性为中心的优化方式以增强模型鲁棒性和几何一致性。

Result: 在Stanford2D3D、Matterport3D和Deep360等多个基准数据集上，模型展现出很强的性能和零样本泛化能力，在多样真实场景下度量深度预测稳定、鲁棒。

Conclusion: 该工作实现了一种高度泛化和稳健的全景深度基础模型，为真实世界多场景、远近距全景深度估计奠定了基础。

Abstract: In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\_website/}

</details>


### [106] [StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors](https://arxiv.org/abs/2512.16915)
*Guibao Shen,Yihua Du,Wenhang Ge,Jing He,Chirui Chang,Donghao Zhou,Zhen Yang,Luozhou Wang,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 该论文提出了UniStereo大规模数据集和高效的StereoPilot单目转立体视频模型，显著提升了立体视频转换的效果和效率。


<details>
  <summary>Details</summary>
Motivation: 立体显示设备如VR头显和3D影院快速普及，用户对高质量立体视频的需求增加。但由于成本和复杂度高，现有3D视频生产受限，自动单目转立体（Monocular-to-Stereo）存在一系列难题，尤其是多阶段流程DWI造成的误差传递、深度模糊和格式不统一等问题。

Method: 作者提出了UniStereo，这是一个涵盖平行和汇聚两种主流立体格式的大规模标准数据集。基于该数据集，提出了StereoPilot模型，该模型是直接将单目视频合成另一视角，无需显式深度图或扩散迭代采样。该方法引入可学习的领域切换器和循环一致性损失，实现格式自适应和更高一致性。

Result: 实验结果显示，StereoPilot在视觉效果和计算效率方面均显著优于当前最优方法。

Conclusion: 制订了统一的数据基准和新模型，为单目转立体视频领域带来更高质量和更高效率的解决方案，推动了立体内容生产的实际应用。

Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.

</details>


### [107] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang,Kaituo Feng,Dongyang Chen,Zhongyu Wang,Zhixun Li,Sicheng Gao,Meng Meng,Xu Zhou,Manyuan Zhang,Yuzhang Shang,Xiangyu Yue*

Main category: cs.CV

TL;DR: 提出了一种自适应多模态大模型AdaTooler-V，通过智能判断视觉任务是否真正需要使用工具，提升推理效率和准确性，优于现有多模态模型，并开源所有资源。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型在推理时往往无差别地调用视觉工具，导致效率下降和性能损失。因此，亟需一种能判断何时需要工具的新方法，减少不必要工具调用。

Method: 提出AdaTooler-V模型，核心为新型强化学习算法AT-GRPO，依据每个样本的“工具收益分”自适应调整奖励，引导模型仅在有明显益处时才选择用工具，并构建了两个配套数据集进行训练和验证。

Result: 在12项视觉推理基准测试中，AdaTooler-V展现出更强的推理能力，其中7B版本在V*高分辨率基准上准确率达到89.8%，超过GPT-4o和Gemini 1.5 Pro等商业闭源大模型。

Conclusion: 通过自适应工具使用和奖励机制，有效实现了高效且精准的多模态推理，推动开源社区相关模型的发展。

Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.

</details>


### [108] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: EasyV2V 是一种简单高效的指令驱动视频编辑框架，采用灵活多样的数据组合和轻量级模型微调，实现了领先的视频编辑效果。


<details>
  <summary>Details</summary>
Motivation: 当前视频编辑在一致性、可控性和泛化性方面面临挑战，远不及图像编辑成熟。作者为了解决这些难题，推动视频编辑技术的发展，提出系统性优化和创新。

Method: 在数据层面，融合现有专家模型、将图像编辑提升为视频编辑、利用伪视频对和密集字幕视频片段进行数据构建，并引入过渡监督。在模型层面，基于预训练的文本到视频模型，通过简单序列拼接进行条件输入，以及采用轻量化LoRA微调，降低训练难度。控制层面，统一时空掩膜机制并支持参考图像，实现更灵活的输入控制。

Result: EasyV2V 在多种输入配置下均可实现灵活、精确的视频编辑，实验结果全面超过同期和商用系统，展现最先进的性能。

Conclusion: 论文表明，通过数据设计与模型简化，可以使视频编辑高效且可控，有望加速视频编辑技术的实际落地应用。

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [109] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: 该论文提出了AuditDM框架，通过自动发现和修正多模态大模型（MLLMs）的失败模式，实现更具解释性和高效的模型评估和提升。


<details>
  <summary>Details</summary>
Motivation: 传统多模态大模型的评估方法解释性差，且难以揭示模型之间的重要能力差距。因此，亟需一种能主动挖掘和分析模型弱点的评估方法。

Method: 论文提出并实现了AuditDM：通过强化学习微调一个MLLM作为审核员，生成能最大化目标模型间分歧的具有挑战性问题和反事实图片。审核员据此发现多样且可解释的失效样例，无需人工标注即可用于模型改进。

Result: 在Gemma-3和PaliGemma-2等SOTA模型上测试，AuditDM发掘了20余种不同失败类型。基于这些发现进行微调后，16项基准上所有模型性能均有提升，甚至使一个3B参数模型优于其28B参数版本。

Conclusion: 数据规模扩展效益递减时，面向特定弱点的审核和修正成为模型诊断与提升的有效手段。

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


### [110] [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/abs/2512.16922)
*Sihan Xu,Ziqiao Ma,Wenhao Chai,Xuweiyi Chen,Weiyang Jin,Joyce Chai,Saining Xie,Stella X. Yu*

Main category: cs.CV

TL;DR: 论文提出了一种新的自监督视觉学习方法NEPA，直接预测图像patch嵌入，取得了接近有监督方法的效果。


<details>
  <summary>Details</summary>
Motivation: 受自然语言生成式预训练成功启发，作者探索类似思想在视觉自监督中的可行性，寻求更简单、灵活且强大的视觉预训练方案。

Method: 提出Next-Embedding Predictive Autoregression（NEPA），核心做法是用自回归方式预测图像patch的下一个embedding，只用Transformer结构，通过因果遮罩和停止梯度技巧，无需像素重建、离散化、对比损失或特殊头部。

Result: 在ImageNet-1k上，ViT-B和ViT-L骨干网络fine-tune后top-1准确率分别达到83.8%和85.3%；在ADE20K语义分割迁移上同样表现优异。

Conclusion: NEPA表明仅基于embedding的生成式自监督学习可以实现架构简单、可扩展且高效的视觉表征学习，为视觉自监督提供了一个有前景的、可能跨模态的全新方向。

Abstract: Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.

</details>


### [111] [Generative Refocusing: Flexible Defocus Control from a Single Image](https://arxiv.org/abs/2512.16923)
*Chun-Wei Tuan Mu,Jia-Bin Huang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 本论文提出了单幅图像再聚焦的新方法，通过两步流程实现自动化聚焦与虚化效果，能够灵活控制景深和光圈形状，且表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前单幅图像再聚焦存在难题，如依赖全焦输入、合成数据、以及对光圈控制能力有限，难以得到高质量的虚化与真实光学特性。解决这些问题，是进一步提升摄影图像质量和创意表达的需求。

Method: 方法包括两步：首先用DeblurNet从不同输入恢复全焦图像，再用BokehNet生成可控虚化效果。创新点在于采用半监督训练，结合合成配对数据和未配对真实虚化照片，并借助EXIF元数据实现对真实光学特性的建模。

Result: 实验显示该方法在图像去散焦、虚化合成和再聚焦等任务的评测中均达到领先水平。此外，还能支持文本引导的调整和自定义光圈形状。

Conclusion: Generative Refocusing方法在技术上为单图像再聚焦提供了更真实、更灵活的解决方式，将助力摄影后期和相关视觉应用。

Abstract: Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.

</details>


### [112] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas是一个融合文本、轨迹和参考图像的多模态世界事件生成框架，可以进行细致、可控的视频事件模拟。


<details>
  <summary>Details</summary>
Motivation: 当前的世界事件生成模型大多依赖文本或简单的控制参数，缺乏对多主体交互、物体身份维持、复杂运动轨迹及视觉一致性的综合建模，导致生成内容难以真正模拟丰富、复杂的现实世界变化。本文旨在解决生成事件的表达能力不足和交互性受限的问题。

Method: WorldCanvas将包含运动、时序和可见性的物体轨迹、表达语义意图的自然语言以及用于物体外观锚定的参考图像融合输入，通过多模态融合机制对物体身份、外观、行为和事件逻辑进行一致性建模，实现多主体互动、复杂事件过程等视频生成。相比文本或轨迹单一输入方法，本框架能细粒度控制和丰富生成内容。

Result: 生成的视频不仅在时序上连贯，且展现出物体身份、场景一致性的涌现特性，即使在物体暂时消失后也能保持身份连贯，包括对复杂、多主体交互及反直觉事件的表达效果优于现有方法。

Conclusion: WorldCanvas推动了世界模型从被动的事件预测提升为主动、可控和互动的用户导向模拟器，有效拓展了世界事件生成的表达性和应用范围。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [113] [TabReX : Tabular Referenceless eXplainable Evaluation](https://arxiv.org/abs/2512.15907)
*Tejas Anvekar,Juhna Park,Aparna Garimella,Vivek Gupta*

Main category: cs.CL

TL;DR: 该论文提出了一种无需参考的表格质量评估框架TabReX，通过将源文本和生成表格转换为知识图谱，并利用大模型辅助对齐，实现对结构和事实的细粒度评价。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生成表格的质量评估仍然存在挑战，现有方法要么忽视表格结构，要么依赖固定参考，难以泛化。

Method: TabReX框架将源文本和生成的表格都转换为规范化的知识图谱，再通过大模型辅助的匹配过程进行对齐，并计算可解释的评价分数，这些分数关注结构和事实的准确性，同时可以平衡敏感度与特异性。并引入TabReX-Bench基准集，覆盖多个领域和难度层次，通过不同的扰动方式系统性地评测指标的稳健性。

Result: TabReX在多个域和难度下，与专家排名的相关性最高，在更高难度的扰动下表现依然稳定，并支持细粒度的模型与提示词分析。

Conclusion: TabReX为结构化生成系统的可解释、可信评价提供了新的范式，推进了表格生成质量测评方法的发展。

Abstract: Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.

</details>


### [114] [Social Story Frames: Contextual Reasoning about Narrative Intent and Reception](https://arxiv.org/abs/2512.15925)
*Joel Mire,Maria Antoniak,Steven R. Wilson,Zexin Ma,Achyutarama R. Ganti,Andrew Piper,Maarten Sap*

Main category: cs.CL

TL;DR: 本文提出了SocialStoryFrames，一个用于系统化分析读者对故事性文本反应的形式框架，并基于该框架开发了两个模型，能够大规模地刻画和对比在线社区中的叙事实践。


<details>
  <summary>Details</summary>
Motivation: 尽管人类阅读故事时会产生丰富的推理、情感和评价反应，但当前的计算机模型很难细致进行类似分析。这一限制阻碍了对故事和读者反应更深入的研究。因此，作者希望建立一种工具，帮助系统化分析和理解读者在阅读故事时的多维度反应。

Method: 作者提出了SocialStoryFrames框架，用于形式化整理读者对故事的推理、情感及价值判断，并结合对话语境、叙事理论、语言学语用学和心理学建构了相关分类法。此外，作者开发了SSF-Generator和SSF-Classifier两个模型，分别通过人类问卷和专家标注进行验证，并在包含6,140篇社交媒体故事的数据集SSF-Corpus上进行了应用分析。

Result: 使用提出的模型对SSF-Corpus进行了分析，展示了不同叙事意图的频率及其相互依赖关系，并对不同社区的叙事实践及其多样性进行了比较。结果表明，该方法能够细致地揭示不同社区在故事讲述上的共性与变异。

Conclusion: SocialStoryFrames框架及相关模型为大规模、多维度分析在线社区的叙事与读者反应提供了有力工具，为未来故事讲述和读者反应研究打开了新方向。

Abstract: Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.

</details>


### [115] [BRAID: Bounded Reasoning for Autonomous Inference and Decisions](https://arxiv.org/abs/2512.15959)
*Armağan Amcalar,Eyup Cinar*

Main category: cs.CL

TL;DR: 本文提出了一种新的结构化提示方法BRAID，通过有界推理提升LLM在推理任务上的效率与准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在推理性能、成本和token使用之间存在非线性关系，实际应用中亟需提升其推理准确性及成本效率。研究结构化提示对优化模型推理有现实意义。

Method: 设计BRAID方法，采用Mermaid为基础的指令图引导模型进行有界推理。分别在不同性能等级的GPT模型上，使用AdvancedIF、GSM-Hard和SCALE MultiChallenge等基准集进行定量实验。对比分析结构化、可机读提示与传统自然语言提示在推理效果和成本上的差异。

Result: 基于BRAID结构化提示，模型推理准确率和成本效率明显提升，尤其适用于生产系统中的自主代理任务。相关数据和结果公开。

Conclusion: BRAID是一种高效、可扩展的结构化推理提示技术，有望成为提升自主智能体系统中推理效率和性价比的有效手段。

Abstract: Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai.

</details>


### [116] [Examining the Utility of Self-disclosure Types for Modeling Annotators of Social Norms](https://arxiv.org/abs/2512.16034)
*Kieran Henderson,Kian Omoomi,Vasudha Varadarajan,Allison Lahnala,Charles Welch*

Main category: cs.CL

TL;DR: 本研究分析了不同类型的自我披露信息（如人口统计信息、态度、关系、经历）在预测标注者对于社会规范判断中的作用，发现人口统计信息影响最大，且理论驱动方法优于自动聚类方法。


<details>
  <summary>Details</summary>
Motivation: 过去的研究使用有限的个人信息来提升对主观任务中个体特征和标注者标签的建模，但很少关注哪些类型的信息对预测最有帮助。本文旨在系统分析不同类型自我披露信息对预测标注者标签的影响。

Method: 将自我披露句子进行分类，并基于不同类别的信息构建标注者预测模型，通过消融实验和多种分析方法考察不同自我披露类型对标注行为预测的影响。

Result: 人口统计信息对预测效果最显著，优于态度、关系、经历等信息。理论驱动方法优于自动聚类方法。仅需少量相关评论即可获得良好效果。多样化的标注者自我披露信息样本能获得最佳表现。

Conclusion: 本文系统分析了自我披露信息在建模和预测标注者判断中的作用，结果表明重点采集人口信息和多样化样本能最有效提升模型表现，对相关主观任务个性化建模具有实用价值。

Abstract: Recent work has explored the use of personal information in the form of persona sentences or self-disclosures to improve modeling of individual characteristics and prediction of annotator labels for subjective tasks. The volume of personal information has historically been restricted and thus little exploration has gone into understanding what kind of information is most informative for predicting annotator labels. In this work, we categorize self-disclosure sentences and use them to build annotator models for predicting judgments of social norms. We perform several ablations and analyses to examine the impact of the type of information on our ability to predict annotation patterns. We find that demographics are more impactful than attitudes, relationships, and experiences. Generally, theory-based approaches worked better than automatic clusters. Contrary to previous work, only a small number of related comments are needed. Lastly, having a more diverse sample of annotator self-disclosures leads to the best performance.

</details>


### [117] [Are We on the Right Way to Assessing LLM-as-a-Judge?](https://arxiv.org/abs/2512.16041)
*Yuanning Feng,Sinan Wang,Zhengxiang Cheng,Yao Wan,Dongping Chen*

Main category: cs.CL

TL;DR: 本文提出了Sage评测套件，用于在无需人工标注的情况下评估大语言模型（LLM）作为评判者的可靠性和一致性。实验结果表明，现有最先进的LLM在裁决任务中依然存在明显的不一致性。


<details>
  <summary>Details</summary>
Motivation: 目前LLM-as-a-Judge评测方式普遍依赖人工标注的“真实标签”，这不仅引入了主观偏见，还限制了评估的可扩展性和客观性。为解决这些问题，需要一种无需人工标注的、公正可扩展的模型评测方案。

Method: 作者借鉴理性选择理论的公理，引入了局部自洽性（即成对偏好稳定性）和全局逻辑一致性（即偏好传递性）两种新的评价指标。通过结合结构化基准任务与现实用户提问，构建了包含650个问题的数据集，用以评估不同LLM在担任“评判者”时的表现。并与现有依赖人工标注的主流评测集进行了相关性验证。

Result: Sage评测指标稳定且与主流人工监督评测高度相关，能有效评估LLM-as-a-Judge的鲁棒性与准确性。实验发现，现有最先进的模型在四分之一的复杂案例中难以保持一致性偏好，并发现了“情境偏好”现象。精调与面板式评判、深入推理可提升一致性。此外，发现人类标注之间本身也存在很大不一致性。

Conclusion: Sage无需人工标注即可评估LLM裁判模型，为相关研究提供了更客观可靠的新评测方式。当前LLM在评判任务中仍存在可靠性短板，未来应关注模型的一致性及对评判标准的强化；同时传统人工标注可能不适合作为理想金标准。

Abstract: LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.

</details>


### [118] [Convolutional Lie Operator for Sentence Classification](https://arxiv.org/abs/2512.16125)
*Daniela N. Rim,Heeyoul Choi*

Main category: cs.CL

TL;DR: 本文提出在卷积神经网络（CNN）基础上引入 Lie 群变换（Lie Convolutions），提升文本句子分类模型捕获语言复杂变换的能力，并实验证明新模型优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络虽然能捕捉文本的局部与位移不变特征，但对语言中更复杂变换的建模能力有限。作者希望通过引入能够表达更复杂、非欧式对称变换的 Lie 群，提升模型对语言深层结构的理解能力。

Method: 作者设计并实现了结合 Lie 群卷积结构的句子分类模型（SCLie 和 DPCLie），将 Lie 群操作融入到基于卷积的分类框架，通过对比实验研究模型表现。

Result: 实验表明 SCLie 和 DPCLie 在句子分类任务中能够优于传统基于卷积的模型，证明了 Lie 群卷积对于提升语言模型能力的有效性。

Conclusion: 通过实验证明 Lie 群卷积可以捕捉传统语言模型难以建模的语言变换，提示了对基于对称性和变换的新型语言建模范式的进一步探索价值。

Abstract: Traditional Convolutional Neural Networks have been successful in capturing local, position-invariant features in text, but their capacity to model complex transformation within language can be further explored. In this work, we explore a novel approach by integrating Lie Convolutions into Convolutional-based sentence classifiers, inspired by the ability of Lie group operations to capture complex, non-Euclidean symmetries. Our proposed models SCLie and DPCLie empirically outperform traditional Convolutional-based sentence classifiers, suggesting that Lie-based models relatively improve the accuracy by capturing transformations not commonly associated with language. Our findings motivate more exploration of new paradigms in language modeling.

</details>


### [119] [MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation](https://arxiv.org/abs/2512.16145)
*Pengyu Wang,Shuchang Ye,Usman Naseem,Jinman Kim*

Main category: cs.CL

TL;DR: 提出一种基于语义驱动强化学习(SRL)的新方法，优化医学报告生成的临床正确性，并在两个主流数据集上取得了业界领先表现。


<details>
  <summary>Details</summary>
Motivation: 目前医学报告生成模型多侧重模仿放射科医生的语言风格，缺乏对临床正确性的保证，因为训练目标主要是字词和句法选择，而非医学准确性。因此需要一种能更好保证医学语义精准的训练方法。

Method: 提出语义驱动强化学习(SRL)方法，在大规模视觉-语言模型(LVLM)上应用，采用组相对策略优化(GRPO)方式，以报告级别的奖励——基于边界的余弦相似度(MCCS)——对生成报告的关键放射学发现进行对比。还通过结构化格式约束，引导模型生成推理性更强的'thinking report'。

Result: 在IU X-Ray和MIMIC-CXR两个数据集上用临床有效性(CE)指标评估，MRG-R1模型分别取得了CE-F1分数51.88和40.39，优于以往方法。实验证明，基于语义标签的强化训练优于传统的Token级别监督。

Conclusion: 使用以临床标签一致性为核心的报告级别奖励目标，比仅优化词重叠显著提升了医学报告生成的临床正确性，为后续基于大模型的医学视觉-语言报告生成监督方式提供了新思路。

Abstract: Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured "thinking report" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training.

</details>


### [120] [Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning](https://arxiv.org/abs/2512.16147)
*Yash Bhaskar,Sankalp Bahad,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 本论文针对社交媒体上的混合印地语-英语文本，提出了检测由虚假叙述推动的仇恨言论（Faux-Hate）的系统，并在Faux-Hate共享任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上传播虚假信息和仇恨言论的问题日益突出，尤其是在多语言环境下（如印地语与英语混合）。传统检测方法难以处理由虚假叙述引发的仇恨言论，因此有必要研发更有效的检测模型。

Method: 作者构建了一个多任务学习系统，结合了先进的自然语言处理技术和领域内预训练方法，解决如下两个子任务：1）Faux-Hate二分类检测（区分假新闻和仇恨言论）；2）仇恨内容的目标与严重程度预测。

Result: 系统在Faux-Hate共享任务中取得了有竞争力的成绩，证明了其方法的有效性。

Conclusion: 采用多任务学习并结合领域预训练可有效提升在复杂社交媒体环境中的假新闻与仇恨言论检测能力。

Abstract: Social media platforms, while enabling global connectivity, have become hubs for the rapid spread of harmful content, including hate speech and fake narratives \cite{davidson2017automated, shu2017fake}. The Faux-Hate shared task focuses on detecting a specific phenomenon: the generation of hate speech driven by fake narratives, termed Faux-Hate. Participants are challenged to identify such instances in code-mixed Hindi-English social media text. This paper describes our system developed for the shared task, addressing two primary sub-tasks: (a) Binary Faux-Hate detection, involving fake and hate speech classification, and (b) Target and Severity prediction, categorizing the intended target and severity of hateful content. Our approach combines advanced natural language processing techniques with domain-specific pretraining to enhance performance across both tasks. The system achieved competitive results, demonstrating the efficacy of leveraging multi-task learning for this complex problem.

</details>


### [121] [A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media](https://arxiv.org/abs/2512.16183)
*Mengfan Shen,Kangqi Song,Xindi Wang,Wei Jia,Tao Wang,Ziqiang Han*

Main category: cs.CL

TL;DR: 本文开发了一套针对警方通报（如微博发布信息）进行结构化信息抽取的管道，通过微调Qwen2.5-7B模型并结合低秩适应（LoRA），在典型领域噪声较大的文本中自动并高效提取15个关键字段，准确率极高。


<details>
  <summary>Details</summary>
Motivation: 警方通报常通过非正式文本（如社交媒体）发布，内容格式多变、信息噪声大，阻碍了高效、准确的数据结构化和社会科学分析。需要一种适应性强、结构化信息抽取效果好的自动化工具。

Method: 作者构建了一个4,933条高质量人工标注样本的数据集，并对Qwen2.5-7B大模型基于LoRA进行领域微调，同时通过prompt engineering提升模型在15个关键字段抽取上的能力。

Result: 与基础模型及指导微调模型相比，所提出方法在死亡检测、死亡人数和省级地点等任务上表现优异：死亡检测准确率超98.36%，死亡人数和省级地点提取的Exact Match Rate分别达到95.31%和95.54%。

Conclusion: 这种基于LoRA微调和领域自适应的方法可高效、准确地将警情通知类的非结构化文本转化为结构化数据，为社会科学等领域提供了可行的自动化解决方案。

Abstract: Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.

</details>


### [122] [Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation](https://arxiv.org/abs/2512.16189)
*Musarrat Zeba,Abdullah Al Mamun,Kishoar Jahan Tithee,Debopom Sutradhar,Mohaimenul Azam Khan Raiaan,Saddam Mukta,Reem E. Mohamed,Md Rafiqul Islam,Yakub Sebastian,Mukhtar Hussain,Sami Azam*

Main category: cs.CL

TL;DR: 本文提出了一种面向医疗领域的LLM事实核查模块和领域特定摘要模型，可减少幻觉输出，提高可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大模型（LLM）在医疗等关键领域的输出可能存在“幻觉”，影响决策和患者安全。因此需要提升输出的真实性。

Method: 作者提出独立于LLM的事实核查模块，结合基于LoRa在MIMIC-III数据集微调的领域特定摘要模型。事实核查模块用数字测试和逻辑表达对EHRs中的事实进行验证。摘要模型和事实核查模块配合使用，提高输出可靠性。

Result: 事实核查模块在104份摘要（3,786个命题事实）上，precision为0.8904，recall为0.8234，F1为0.8556。摘要质量得分ROUGE-1=0.5797，BERTScore=0.9120。

Conclusion: 结合事实核查和定制LLM摘要模型可明显提升医疗领域自动化文本生成的可靠性与真实性，为实际应用提供有力支撑。

Abstract: In healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated outputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM, along with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using Low-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical tests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to validate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For evaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these as facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally, the LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality.

</details>


### [123] [An Information-Theoretic Framework for Robust Large Language Model Editing](https://arxiv.org/abs/2512.16227)
*Qizhou Chen,Chengyu Wang,Taolin Zhang,Xiaofeng He*

Main category: cs.CL

TL;DR: 作者提出了一种基于信息瓶颈理论的新型知识编辑框架，显著提升了大语言模型知识更新的准确性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在各领域应用广泛，但其知识过时或错误时，更新效率低下，全面再训练成本高，现有编辑方法易产生副作用、泛化能力弱。

Method: 作者提出以信息瓶颈理论为基础，通过信息压缩与隔离，开发了信息瓶颈知识编辑器（IBKE）。IBKE利用紧凑的潜在表征，引导梯度更新，实现对模型知识的精确、稳健且广泛适用的编辑，而非仅在狭窄领域内有效。

Result: 实验证明IBKE在多种LLM架构和标准任务上的编辑准确率达到了最先进水平，同时在泛化性和特异性方面优于现有技术。

Conclusion: IBKE为开放领域的知识编辑提供了理论支持和实用范例，提升LLM在实际应用中的可靠性和价值。

Abstract: Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.

</details>


### [124] [LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding](https://arxiv.org/abs/2512.16229)
*Chenkai Xu,Yijie Jin,Jiajun Li,Yi Tu,Guoping Long,Dandan Tu,Tianqi Hou,Junchi Yan,Zhijie Deng*

Main category: cs.CL

TL;DR: 本文提出了LoPA算法，大幅提升了扩散式大语言模型(dLLMs)的解码并行度和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有dLLM解码策略下，推理时的并行度受限，大多数情况下每步仅生成1-3个token，限制了解码速度。提升推理并行度，尤其是在不影响输出质量的情况下，是重要且有挑战的课题。

Method: 作者分析发现Token Filling Order（TFO）深刻影响dLLM的推理并行度，提出训练无关、即插即用的LoPA算法，通过并行分支探索多个TFO候选，并依置信度选择最有利于后续并行的顺序。此外，配合开发了适用于LoPA的多设备推理系统，以支持分支并行。

Result: 在D2F模型上应用LoPA，GSM8K任务TPF由基线1-3提升至10.1，性能保持优于基线；多GPU单样本吞吐达到1073.9 token/s，大幅突破推理效率。

Conclusion: LoPA算法无需重新训练即可极大加速dLLM推理，为大模型实际部署带来新路径，并为未来并行解码方法奠定基础。

Abstract: Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.

</details>


### [125] [Sigma-Moe-Tiny Technical Report](https://arxiv.org/abs/2512.16248)
*Qingguo Hu,Zhenghao Lin,Ziyue Yang,Yucheng Ding,Xiao Liu,Yuting Jiang,Ruizhe Wang,Tianyu Chen,Zhongxin Guo,Yifan Xiong,Rui Gao,Lei Qu,Jinsong Su,Peng Cheng,Yeyun Gong*

Main category: cs.CL

TL;DR: Sigma-MoE-Tiny 是一种极度稀疏的混合专家（MoE）语言模型，具备极高的参数稀疏度，通过创新训练策略实现优秀的平衡和性能。


<details>
  <summary>Details</summary>
Motivation: 当前MoE模型在扩展效率上的优势明显，但在极端稀疏条件下会面临专家负载均衡难题。此前的负载均衡损失在稀疏化严重时易失效，阻碍模型进一步做大做稀疏化。本文旨在解决负载均衡的稳定性和有效性问题，从而推动MoE模型向更极端的稀疏度发展。

Method: 提出Sigma-MoE-Tiny模型，每层多达96个专家，每个token只激活1个专家，总参数数约20B，但实际激活仅0.5B。为解决极端稀疏下的负载均衡问题，引入了渐进式稀疏化（progressive sparsification）训练策略，提升专家利用和训练稳定性。此外，采用多样高质量语料预训练和后训练。

Result: Sigma-MoE-Tiny 训练过程高度稳定，无无法恢复的训练损失波动。实验显示，尽管仅激活0.5B参数，模型在同规模或大规模模型中表现突出。在极稀疏MoE负载均衡方面，作者还进行了细致分析。

Conclusion: Sigma-MoE-Tiny 在极端稀疏MoE的负载均衡和性能层面达到了业界领先水平，并为未来推进极稀疏MoE架构提供了有价值的实践和理论经验。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.
  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny
  Code: https://github.com/microsoft/ltp-megatron-lm

</details>


### [126] [Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures](https://arxiv.org/abs/2512.16287)
*Yehor Tereshchenko,Mika Hämäläinen,Svitlana Myroniuk*

Main category: cs.CL

TL;DR: 本研究比较了OpenAI GPT模型（带推理能力与否）在芬兰语与四种低资源乌拉尔语（科米-兹良、莫克沙、厄尔兹亚、乌德穆尔特）互译任务中的表现，发现推理型模型的拒绝率显著更低。


<details>
  <summary>Details</summary>
Motivation: 以往针对大型语言模型（LLM）翻译性能的研究多集中于高资源语言，导致低资源和濒危语言的测试与了解严重不足。本文旨在填补该领域空白，探究LLM在乌拉尔四种低资源语言上的能力表现。

Method: 利用文学文本的平行语料，对OpenAI GPT推理型与非推理型模型进行翻译表现测试，并统计各模型对翻译请求的拒绝率，分析其差异。

Result: 推理型模型拒绝率比非推理型低16个百分点；不同模型在翻译低资源语言时表现存在显著差异。

Conclusion: 推理型大语言模型在低资源和濒危语言翻译中更具优势，对相关语言保护及研究具有参考意义，丰富了对推理能力模型的理解。

Abstract: The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI's GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.

</details>


### [127] [Hacking Neural Evaluation Metrics with Single Hub Text](https://arxiv.org/abs/2512.16323)
*Hiroyuki Deguchi,Katsuki Chousa,Yusuke Sakai*

Main category: cs.CL

TL;DR: 该论文揭示了当前主流的神经网络文本评估指标（如COMET）存在安全性与可靠性隐患，通过寻找到一种可以在多项测试中都被高评分的对抗性文本，展示这些指标的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 神经网络型文本自动评价指标已被广泛应用，但由于其黑箱特性，用户无法保证其输出的评价一定可靠。因此，发现和验证这些评估方法的潜在脆弱性和安全隐患，对于改进生成模型和指标本身至关重要。

Method: 作者提出了一种方法，在离散文本空间中搜索能够在多种测试用例中都被评估为高质量的对抗样本（hub text），用以揭示评估指标的漏洞。通过实验证明，在WMT'24 En-Ja和En-De翻译评测中，该hub text的COMET分数超越了针对每个源句单独翻译文本的分数。该方法也被用于其它语对以检验其通用性。

Result: 单一hub text在WMT'24 En-Ja任务中获得79.1 COMET%、在En-De任务中获得67.8 COMET%，分数均超过了M2M100逐句翻译的结果。进一步实验显示，该hub text可跨语言对（如Ja-En、De-En）泛化。

Conclusion: 当前主流的神经指标（如COMET）容易被某些特定对抗文本欺骗，存在可靠性隐患。此研究提醒学界未来需关注和提升评估指标的安全性与鲁棒性。

Abstract: Strongly human-correlated evaluation metrics serve as an essential compass for the development and improvement of generation models and must be highly reliable and robust. Recent embedding-based neural text evaluation metrics, such as COMET for translation tasks, are widely used in both research and development fields. However, there is no guarantee that they yield reliable evaluation results due to the black-box nature of neural networks. To raise concerns about the reliability and safety of such metrics, we propose a method for finding a single adversarial text in the discrete space that is consistently evaluated as high-quality, regardless of the test cases, to identify the vulnerabilities in evaluation metrics. The single hub text found with our method achieved 79.1 COMET% and 67.8 COMET% in the WMT'24 English-to-Japanese (En--Ja) and English-to-German (En--De) translation tasks, respectively, outperforming translations generated individually for each source sentence by using M2M100, a general translation model. Furthermore, we also confirmed that the hub text found with our method generalizes across multiple language pairs such as Ja--En and De--En.

</details>


### [128] [Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs](https://arxiv.org/abs/2512.16378)
*Sara Papi,Javier Garcia Gilabert,Zachary Hopton,Vilém Zouhar,Carlos Escolano,Gerard I. Gállego,Jorge Iranzo-Sánchez,Ahrii Kim,Dominik Macháček,Patricia Schmidtova,Maike Züfle*

Main category: cs.CL

TL;DR: 本文系统性评测了最新的SpeechLLM（端到端语音到文本大模型）与传统级联语音翻译架构的性能，发现传统级联系统在大多数场景下仍然更可靠，而SpeechLLMs 只在特定环境下能与级联系统媲美。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）逐渐扩展支持语音等多模态输入，研究者关心将语音作为原生模态整合进LLM后的直接语音翻译（SpeechLLM）是否优于依赖转录的传统级联系统。

Method: 作者搭建了“听译测试集”，对5个最前沿的SpeechLLM，16个强大的直接与级联系统进行系统对比。这些系统结合了先进的语音基础模型（SFM）与多语种LLM。实验涵盖了16个基准、13种语言对，以及9种挑战性条件（比如口吃、不清、噪声、长句等）。

Result: 结果显示，在全面的对比下，传统级联系统整体上最为可靠。当前的SpeechLLM只在部分设置下达到级联系统水平，而单纯的SFM表现最差。此外，无论是在端到端模型，还是级联流程中，LLM的集成对于高质量语音翻译至关重要。

Conclusion: 尽管SpeechLLM在部分场景已逐步逼近级联系统，但传统级联系统在多数情况下依然拥有优势。集成LLM（无论何种方式）是提升语音翻译质量的核心关键。

Abstract: As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.

</details>


### [129] [Bridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains](https://arxiv.org/abs/2512.16401)
*Darshil Chauhan,Adityasinh Solanki,Vansh Patel,Kanav Kapoor,Ritvik Jain,Aditya Bansal,Dhruv Kumar,Prateek Narang*

Main category: cs.CL

TL;DR: 本文提出了一种高效且保护隐私的ASR（自动语音识别）模型自适应框架，有效改进了模型在真实临床语音领域的表现，并缓解了连续学习中的遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有ASR模型在医疗文档自动化有巨大潜力，但实际部署受限于隐私保护、算力有限和语音域偏移等难题，导致模型在真实场景下效果大打折扣。

Method: 提出基于LoRA（Low-Rank Adaptation）的持续学习方案，直接在边缘设备上从流式新临床数据中自适应更新模型，并引入多领域经验回放技术以缓解灾难性遗忘。

Result: 在实际临床语音数据上，所提策略能将目标领域的WER提升17.1%，同时遗忘现象减少了47%。

Conclusion: 该方法验证了在高安全需求的现实场景中，可构建出具备自完善能力、且可靠的ASR系统。

Abstract: Automatic Speech Recognition (ASR) holds immense potential to streamline clinical documentation, such as digitizing handwritten prescriptions and reports, thereby increasing patient throughput and reducing costs in resource-constrained sectors like rural healthcare. However, realizing this utility is currently obstructed by significant technical barriers: strict data privacy constraints, limited computational resources, and severe acoustic domain shifts. We quantify this gap by showing that a robust multilingual model (IndicWav2Vec) degrades to a stark 40.94% Word Error Rate (WER) when deployed on real-world clinical audio (Gram Vaani), rendering it unusable for practical applications. To address these challenges and bring ASR closer to deployment, we propose an efficient, privacy-preserving adaptation framework. We employ Low-Rank Adaptation (LoRA) to enable continual learning from incoming data streams directly on edge devices, ensuring patient data confidentiality. Our strategy yields a 17.1% relative improvement in WER on the target domain. Furthermore, by integrating multi-domain experience replay, we reduce catastrophic forgetting by 47% compared to naive adaptation. These results demonstrate a viable pathway for building reliable, self-improving ASR systems that can operate effectively within the constraints of high-impact real-world environments.

</details>


### [130] [Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics](https://arxiv.org/abs/2512.16530)
*Primoz Kocbek,Leon Kopitar,Gregor Stiglic*

Main category: cs.CL

TL;DR: 本研究评估了大语言模型（LLM）在简化生物医学文本以提升健康素养方面的应用。重点比较了不同方法在自动和人工评价下的表现。


<details>
  <summary>Details</summary>
Motivation: 复杂的生物医学文本让许多人难以理解，阻碍健康信息传递。提升医疗文本可读性，有助于全民健康素养的提升。LLM被认为具备简化自然语言的潜力。

Method: 研究使用包含简化版生物医学文摘的公开数据集，设计三种模型方案：1）基于提示词的基线法，2）双AI Agent 协同法，3）微调法。主要采用OpenAI gpt-4o/4o-mini作为基础模型，并采用Flesch-Kincaid、SMOG Index、SARI、BERTScore、G-Eval等定量指标，以及5分Likert量表做定性评价。

Result: gpt-4o-mini表现优异，微调法效果不佳。G-Eval指标给出的排名趋势与人工Likert评价较为一致，表现出较强的区分能力。

Conclusion: gpt-4o-mini 适用于生物医学文本简化任务，自动化LLM评价工具（如G-Eval）可辅助评价模型表现，简化流程。微调方法未带来性能提升。

Abstract: This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric.

</details>


### [131] [UM_FHS at the CLEF 2025 SimpleText Track: Comparing No-Context and Fine-Tune Approaches for GPT-4.1 Models in Sentence and Document-Level Text Simplification](https://arxiv.org/abs/2512.16541)
*Primoz Kocbek,Gregor Stiglic*

Main category: cs.CL

TL;DR: 本文介绍了作者在CLEF 2025 SimpleText任务1中的科学文本简化研究，采用了多种GPT-4.1系列模型，并比较了无上下文提示与微调两种方法，发现简单模型在无上下文下表现优异，而微调方法效果不一。


<details>
  <summary>Details</summary>
Motivation: 科学文本复杂，普通读者难以理解，推动科学文本的自动简化成为重要研究方向。作者旨在通过对比主流大模型及不同简化技术，提升文本易读性。

Method: 作者分别采用GPT-4.1、GPT-4.1mini和GPT-4.1-nano三种模型，比较了无需上下文的提示工程方法与对模型微调（FT）的方法，在句子级和文档级两种粒度下进行实验。

Result: GPT-4.1-mini在无上下文方法下，无论是句子级还是文档级，均表现出强劲性能。微调模型效果表现不一，文档级简化时gpt-4.1-nano-ft偶有突出表现。

Conclusion: 无上下文、简单的模型在两级别文本简化中非常有效，而微调模型在处理不同粒度文本简化时表现复杂，无统一优越性，显示出进一步优化空间。

Abstract: This work describes our submission to the CLEF 2025 SimpleText track Task 1, addressing both sentenceand document-level simplification of scientific texts. The methodology centered on using the gpt-4.1, gpt-4.1mini, and gpt-4.1-nano models from OpenAI. Two distinct approaches were compared: a no-context method relying on prompt engineering and a fine-tuned (FT) method across models. The gpt-4.1-mini model with no-context demonstrated robust performance at both levels of simplification, while the fine-tuned models showed mixed results, highlighting the complexities of simplifying text at different granularities, where gpt-4.1-nano-ft performance stands out at document-level simplification in one case.

</details>


### [132] [Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics](https://arxiv.org/abs/2512.16602)
*Iker García-Ferrero,David Montero,Roman Orus*

Main category: cs.CL

TL;DR: 提出了一种在不需重新训练大模型的情况下，精细调控其针对政治敏感话题拒答行为的方法，既能移除“拒绝回答”也能维持内容安全。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理政治敏感话题时，往往通过脆弱的模板化规则进行拒绝，灵活性和透明度不足，难以兼顾内容安全和用户需求。因此需要一种无需重新训练、更加细粒度且通用的拒绝控制机制。

Method: 提出了Refusal Steering方法，通过LLM担任“裁判员”对拒答概率进行打分，并设计了带岭正则的方向提取算法，以在激活空间中分离拒绝-服从方向。在推断时通过向量操作灵活调控拒答行为，方法适用于不同规模模型，并可根据需求诱导特定拒答。

Result: 在Qwen3-Next-80B-A3B-Thinking模型和其它4B至80B的大模型上，方法能移除针对政治敏感话题的拒绝回答行为，同时在JailbreakBench测试下保持安全性，对一般任务的表现也接近原始水平。通过分析激活向量，发现拒绝信号主要集中在变换器后层且分布于多维空间。

Conclusion: Refusal Steering实现在不损失安全性的前提下，有效去除大语言模型对政治敏感话题的拒答，是一种切实可行且可解释的推断阶段拒答调控方案，为模型内容审核与灵活应用提供新路径。

Abstract: We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.

</details>


### [133] [JustRL: Scaling a 1.5B LLM with a Simple RL Recipe](https://arxiv.org/abs/2512.16649)
*Bingxiang He,Zekai Qu,Zeyuan Liu,Yinghao Chen,Yuxin Zuo,Cheng Qian,Kaiyan Zhang,Weize Chen,Chaojun Xiao,Ganqu Cui,Ning Ding,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 该论文提出了一种名为JustRL的简化强化学习方法，通过单阶段、固定超参数实现与复杂方法相当甚至更好的性能，大大降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的强化学习训练方法日益复杂，存在多阶段流程、动态超参和课程学习等，但是这种复杂性是否真的有必要尚未被系统检验。

Method: JustRL采用单阶段训练和固定超参数，完全去除了动态调整和复杂结构，直接在两个1.5B参数的推理模型上实验，并用消融实验考察常见训练技巧的影响。

Result: JustRL在两个模型上用一半计算量，实现了9个数学基准测试中54.9%和64.3%的平均准确率，达到或超过当前复杂方法。所有超参数可直接迁移，无需调参，训练表现出稳定单调提升，未出现激励复杂手段的崩溃或停滞。某些常规技巧反而导致训练效果下降。

Conclusion: 大语言模型强化学习中的许多复杂方法或许是为了解决本可以通过稳定基线消除的问题。JustRL为社区提供了一个高效、可复现的简化基线。

Abstract: Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \textbf{Is this complexity necessary?} We present \textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\% and 64.3\% average accuracy across nine mathematical benchmarks) while using 2$\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.

</details>


### [134] [GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation](https://arxiv.org/abs/2512.16770)
*William English,Chase Walker,Dominic Simon,Rickard Ewetz*

Main category: cs.CL

TL;DR: 该论文提出了GinSign框架，实现了将自然语言高效地映射到时序逻辑中的原子元素，极大提高了自动化系统规范的自动翻译与验证性能。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言到时序逻辑的翻译方法，要么依赖人工提供精确的原子映射，要么准确率较低。这极大限制了自动化系统中非专家用户参与系统设计与验证的便利性。

Method: 作者提出GinSign框架，用分层的方式将自然语言规格中的高层原子命题映射到具体系统签名。方法包括先预测谓词标签，再选择具体类型的参数。该过程转化为结构化分类任务，使得可以利用轻量化的语言模型（而非庞大的LLM）实现高效映射。

Result: 在跨多个领域的实验中，GinSign框架相比未做原子映射的现有方法，能翻译出与目标表达语义等价的时序逻辑公式。其逻辑等价分数达到95.5%，是当前SOTA方法的1.4倍。

Conclusion: GinSign显著提升了自然语言到时序逻辑翻译的可用性和准确率，可广泛应用于自动化系统规范制定、验证，减少人工干预，助力可信自动系统的开发。

Abstract: Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\mathcal{P}$. We decompose the grounding task hierarchically -- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\%$, a $1.4\times$ improvement over SOTA.

</details>


### [135] [From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs](https://arxiv.org/abs/2512.16795)
*Shubham Mishra,Samyek Jain,Gorang Mehrishi,Shiv Tiwari,Harsh Sharma,Pratik Narang,Dhruv Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种结合结构化推理的新型RAG（检索增强生成）框架，用以提升大语言模型面对冲突、过时或主观信息时的表现，并能生成可解释、有依据的回答。实验结果显示该方法大幅提升了正确率和行为一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统在碰到冲突、过时或主观检索信息时表现不佳，且过去的研究只针对单一问题，缺乏统一的推理监督机制。本文的动机就是解决这些实际应用中的核心缺陷，提升RAG系统的准确性与可解释性。

Method: 作者设计了三阶段推理链：(1) 文档层次的裁判，(2) 冲突分析，(3) 有依据的综合，最终支持引用来源或合理拒答。此外，提出CATS冲突感知信任分数评估体系，利用LLM进行推理监督，并搭建了包含539个推理问题的数据集与评测流程。

Result: 实验用Qwen等模型对照验证，使用有监督微调后，整体回答正确率从0.069提升到0.883，行为一致性从0.074提升到0.722，远超其他基线方法。

Conclusion: 引入结构化推理和冲突感知监督机制能显著提升RAG系统在复杂场景下的表现，为未来具可解释性和可信检索增强生成模型奠定了基础。

Abstract: Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.

</details>


### [136] [Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology](https://arxiv.org/abs/2512.16802)
*Primož Kocbek,Azra Frkatović-Hodžić,Dora Lalić,Vivian Hui,Gordan Lauc,Gregor Štiglic*

Main category: cs.CL

TL;DR: 本文探讨了多模态检索增强生成（MM-RAG）在生物医学问答中的最佳视觉信息处理策略，并在糖生物学领域构建基准进行实证对比。


<details>
  <summary>Details</summary>
Motivation: 当前MM-RAG问答系统在处理包含图、表等视觉信息的生物医学文献时，尚不清楚应优先将视觉内容转为文本，还是采用无需OCR的视觉检索返回页面图像，由生成器解释。迫切需要量化不同策略在视觉密集领域（如糖生物学）的效果差异。

Method: 作者基于糖生物学25篇文献，构建了120道多项选择题，按检索难度细分。实现了四种数据增强管道（无增强、文本增强、多模态转换、无OCR视觉检索），并在多个开源/专有大模型（Gemma、GPT-4o、GPT-5家族等）及数种视觉检索器（ColPali、ColQwen、ColFlor）上进行多次准确率测试，带置信区间统计。

Result: Gemma-3-27B-IT下，文本及多模态增强准确率远高于视觉检索（0.722-0.740 vs 0.510）；GPT-4o下，多模态效果最好（0.808），文本和视觉检索次之，差距变小。GPT-5家族中，视觉检索器之间（ColPali/ColQwen/ColFlor）差异不显著，ColFlor以更小资源消耗提供同步表现。模型越大、OCR-free方案竞争力越强；小模型仍建议文本转换。

Conclusion: 视觉密集型文献中，转换视觉为文本可显著提升中等规模模型的检索增强问答性能。超大模型下，OCR-free检索方案（如ColFlor）表现同样优秀，可优先采用。总体，管道选择需视模型规模与负载权衡。

Abstract: Multi-modal retrieval-augmented generation (MM-RAG) promises grounded biomedical QA, but it is unclear when to (i) convert figures/tables into text versus (ii) use optical character recognition (OCR)-free visual retrieval that returns page images and leaves interpretation to the generator. We study this trade-off in glycobiology, a visually dense domain. We built a benchmark of 120 multiple-choice questions (MCQs) from 25 papers, stratified by retrieval difficulty (easy text, medium figures/tables, hard cross-evidence). We implemented four augmentations-None, Text RAG, Multi-modal conversion, and late-interaction visual retrieval (ColPali)-using Docling parsing and Qdrant indexing. We evaluated mid-size open-source and frontier proprietary models (e.g., Gemma-3-27B-IT, GPT-4o family). Additional testing used the GPT-5 family and multiple visual retrievers (ColPali/ColQwen/ColFlor). Accuracy with Agresti-Coull 95% confidence intervals (CIs) was computed over 5 runs per configuration. With Gemma-3-27B-IT, Text and Multi-modal augmentation outperformed OCR-free retrieval (0.722-0.740 vs. 0.510 average accuracy). With GPT-4o, Multi-modal achieved 0.808, with Text 0.782 and ColPali 0.745 close behind; within-model differences were small. In follow-on experiments with the GPT-5 family, the best results with ColPali and ColFlor improved by ~2% to 0.828 in both cases. In general, across the GPT-5 family, ColPali, ColQwen, and ColFlor were statistically indistinguishable. GPT-5-nano trailed larger GPT-5 variants by roughly 8-10%. Pipeline choice is capacity-dependent: converting visuals to text lowers the reader burden and is more reliable for mid-size models, whereas OCR-free visual retrieval becomes competitive under frontier models. Among retrievers, ColFlor offers parity with heavier options at a smaller footprint, making it an efficient default when strong generators are available.

</details>


### [137] [Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs](https://arxiv.org/abs/2512.16814)
*William English,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: cs.CL

TL;DR: 该论文提出了一种新的自然语言到时序逻辑的翻译框架GraFT，有效提升了翻译准确率，尤其是在端到端和跨领域任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言到时序逻辑的翻译方法，在符号抽取与翻译阶段面临准确性、共指处理以及小样本学习等挑战。作者希望通过精简生成空间，提升模型的效率和泛化能力。

Method: 论文提出了Grammar Forced Translation（GraFT）框架。其核心思想是在每一步只允许模型从有限集合中选取输出token，而非从整个词表预测，从而大幅减少搜索空间。该方法利用任务特点对输出空间进行约束，并给出理论解释为何这样做能提升学习效率。

Result: 作者在CW、GLTL和Navi等基准数据集上评估了GraFT。结果显示，GraFT在端到端翻译准确率上提升了5.49%，在跨领域翻译准确率上提升了14.06%，均优于现有方法。

Conclusion: GraFT通过限制每步输出集合，降低了任务难度，提升了模型数据效率和泛化能力。实验结果验证了其相较主流方法的优势。

Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.

</details>


### [138] [What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels](https://arxiv.org/abs/2512.16832)
*Aditya Yadavalli,Tiago Pimentel,Tamar I Regev,Ethan Wilcox,Alex Warstadt*

Main category: cs.CL

TL;DR: 本文提出了一种信息论方法，量化语音韵律单独传达的信息量及其内容，发现韵律在传递讽刺和情感上比文本更有效。


<details>
  <summary>Details</summary>
Motivation: 语言交流中，韵律（语音的旋律）携带了文本无法表达的重要信息。本研究旨在定量测量韵律独立于文本所传达的信息量，并明确这些信息主要表现在哪些语义维度。

Method: 采用大规模语音与语言模型，通过估计话语某一语义维度（如情感）与某个传播通道（如音频或文本）之间的互信息，来量化各种维度的信息传递量。实验以影视和播客语音为数据，比较音频和文本在传递讽刺、情感和疑问时的信息量。

Result: 结果显示，对于讽刺和情感，音频通道，即韵律，传递的信息量是文本通道的一个数量级以上，尤其在缺乏长程上下文情况下更为明显。而在传递“是否为疑问”这类特征时，韵律带来的信息增量则相对较小。

Conclusion: 韵律在传递语义信息（特别是讽刺和情感）方面拥有显著优势，未来可将该定量方法推广至更多语义维度、通道与语言，以系统探索人类语音交流的多样性和复杂性。

Abstract: Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.

</details>


### [139] [LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference](https://arxiv.org/abs/2512.16843)
*Harsh Vardhan Bansal*

Main category: cs.CL

TL;DR: 本论文提出了一种新的层级缓存框架LLMCache，用于加速Transformer推理，通过复用语义相似输入的中间激活，实现最高3.1倍推理加速且准确率几乎不变。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽然性能优秀，但推理延迟高，难以满足实时和大规模部署需求。现有缓存机制如Token级key-value cache虽可加速，但适用性和效果有限。

Method: 作者提出LLMCache，是一种模型无关、适用于Encoder和Decoder架构、支持任意Transformer层缓存的新方法。核心包括基于输入语义相似度的激活复用、轻量级指纹匹配和自适应缓存淘汰策略。

Result: 在BERT和GPT-2模型上，针对SQuAD、WikiText-103和OpenBookQA等任务，LLMCache带来了最高3.1倍推理加速，且准确率下降不超过0.5%。

Conclusion: LLMCache为Transformer模型推理提供了实用、通用、高效的加速方案，有助于大规模、实时应用场景下的落地。

Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

</details>


### [140] [AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2512.16883)
*Tzu-Han Lin,Wei-Lin Chen,Chen-An Li,Hung-yi Lee,Yun-Nung Chen,Yu Meng*

Main category: cs.CL

TL;DR: 本论文提出AdaSearch，一个提升大语言模型自适应搜索能力、减少不必要外部检索、增强决策透明性的强化学习框架。该方法有效提升了知识边界感知、减少了成本和噪声风险，并在多个模型上验证了其效能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型结合搜索引擎能提升任务能力，但过度依赖外部搜索会增加成本和风险，仅依赖模型自身则易产生幻觉。如何智能平衡内外知识来源，且只在必要时调用搜索，是构建高效、可靠智能体的核心难题。现有方法在限制搜索调用次数时存在奖励设计繁杂、效用模糊、以及评价不科学等诸多缺陷。

Method: 作者首先基于F1决策指标，定量分析了现有模型对自有知识边界的感知能力。随后提出AdaSearch，将问题求解与是否调用外部搜索分为两个阶段，使调用决策过程显式化、可解释。该框架基于强化学习优化，强调决策透明性和解释性。

Result: 实验表明，AdaSearch在多个模型及规模下，显著提升了知识边界自知能力，有效减少了不必要的搜索调用，并保持了强大的任务性能。同时，其决策过程更为透明和可解释。

Conclusion: AdaSearch有效解决了当前搜索智能体过度或不足依赖外部搜索、奖励设计复杂以及决策不透明等问题。框架适用于高风险领域如医疗和金融，既提升了模型效能，又兼顾了决策的安全性与可解释性。

Abstract: Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.

</details>


### [141] [Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image](https://arxiv.org/abs/2512.16899)
*Yushi Hu,Reyhane Askari-Hemmat,Melissa Hall,Emily Dinan,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CL

TL;DR: 该论文提出了用于多模态奖励模型评测的全新基准MMRB2，并对主流模型进行了系统评估和分析。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型（RMs）主要用于单一模态的语言模型训练，而对能够处理交错图像和文本序列的'全能'模型的奖励模型研究仍较为不足。为推动多模态理解与生成领域的进展，需要建立覆盖多任务的高质量评测基准以促进模型优化。

Method: 作者提出MMRB2基准，涵盖文本生成图像、图像编辑、交错生成及多模态推理四类任务，并基于23个模型/智能体、21个源任务，人工标注偏好对1,000组/任务。评测集经过多重过滤，确保引用实际/高难度的提示及专家共识的高质量偏好标注。同时，作者用MMRB2对主流多模态LLM及基于人类偏好训练的评测模型进行分析比较。

Result: 各种主流模型在MMRB2的表现差异明显。最新Gemini 3 Pro准确率75-80%，GPT-5和Gemini 2.5 Pro为66-75%，人类标注者准确率超过90%，均高于常用的GPT-4o（59%）。开源Qwen3-VL-32B和Gemini 2.5 Flash表现相当（64%）。作者还证明，在MMRB2上的表现与下游任务的成功率高度相关。

Conclusion: MMRB2作为首个针对多模态奖励模型的系统性基准，揭示了当前模型与人类水平的显著差距，并为今后的多模态奖励模型改进提供了方向。

Abstract: Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.

</details>


### [142] [In-Context Algebra](https://arxiv.org/abs/2512.16902)
*Eric Todd,Jannik Brinkmann,Rohit Gandikota,David Bau*

Main category: cs.CL

TL;DR: 本文研究了transformer在符号意义每次变化的算术序列任务中的推理机制，发现即使符号和其含义不固定，transformer依然能学习并泛化出高准确度的推理规则，并揭示了其常学会的三种推理机制。


<details>
  <summary>Details</summary>
Motivation: 此前的研究主要探讨了在符号意义固定的情况下，transformer模型如何形成反映代数结构的嵌入表示。但实际任务中，符号的含义往往并不固定，因此迫切需要了解在符号于上下文中赋意的条件下，模型的推理机制及泛化能力。

Method: 作者设计了一种新的任务，让每条序列中的符号和代数群元素的对应关系各不相同。通过针对性的数据分布，进行因果测试，分析并隔离出transformer学习到的推理机制。

Result: transformer不仅在这种任务上能取得近乎完美的准确率，还可以泛化到未见过的代数群。模型普遍学会三种机制：1)交换复制——用专门的attention head直接复制答案；2)单位元识别——辨认带有群单位元的事实；3)基于群闭包的抵消——追踪群成员关系以约束答案有效性。

Conclusion: 与以往固定符号设定下主要靠几何嵌入不同，若符号含义变化，transformer会发展出多样的符号推理机制。这拓展了我们对transformer在更灵活、变量意义不固定情况下推理能力的理解。

Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.

</details>


### [143] [Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates](https://arxiv.org/abs/2512.16914)
*Nikhil Prakash,Donghao Ren,Dominik Moritz,Yannick Assogba*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，通过仅针对负责特定任务的稀疏子网络（称为circuits）进行干预，显著提升了大模型在数学推理任务上的性能，同时对模型整体能力影响极小。


<details>
  <summary>Details</summary>
Motivation: 以往研究发现，大模型内部存在负责特定任务的稀疏子网络，并且微调提升性能主要源于这些子网络的强化。这提示可以直接针对这些‘circuits’做精准、针对性更新，从而更高效提升模型某些能力。

Method: 提出‘Constructive Circuit Amplification’方法，通过分析模型推理轨迹，确定核心关键token和相关模型组件，并仅对这些组件进行更新。

Result: 在数学推理任务上，多种大模型的准确率最高提升了+11.4%，且仅需修改1.59%的模型组件。通过MMLU、TriviaQA和TruthfulQA评测，其他能力影响极小。

Conclusion: 通过对模型内部稀疏关键子网络的选择性更新，可以高效、可靠地增强特定能力，同时对整体性能损伤很低。

Abstract: Prior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called Constructive Circuit Amplification which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [144] [Large Video Planner Enables Generalizable Robot Control](https://arxiv.org/abs/2512.15840)
*Boyuan Chen,Tianyuan Zhang,Haoran Geng,Kiwhan Song,Caiyi Zhang,Peihao Li,William T. Freeman,Jitendra Malik,Pieter Abbeel,Russ Tedrake,Vincent Sitzmann,Yilun Du*

Main category: cs.RO

TL;DR: 本论文提出以大规模视频预训练为主的新范式，训练生成式机器人规划基础模型，实现了零样本任务与场景的可视规划，并在真实机器人上验证了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前机器人基础模型多依赖多模态大模型（如语言+视觉），通过语言和图像的预训练迁移到生成动作，但静态图像和文本难以完全反映物理世界的动态行为。视频则能自然捕捉到空间和时间上动作与状态的序列，更贴合机器人行为学习的需求，因此提出以视频为主的预训练方式。

Method: 作者收集了大规模互联网人类活动和任务演示视频数据集，并首次在基础模型尺度（foundation-model scale）上训练生成式视频模型用于机器人任务规划。模型以视频形式输出对新场景/任务的规划，再后处理转换为具体的可执行机器人动作。

Result: 实验在野外第三方选定任务和真实机器人上测试，展现了模型在新任务和新场景中的强泛化能力和指令跟随能力，并成功执行了物理任务。

Conclusion: 以大规模视频数据为基础预训练的生成式机器人规划模型展现出强泛化、鲁棒性和现实可行性。模型与数据集均已开源，支持视频基础的机器人学习研究。

Abstract: General-purpose robots require decision-making models that generalize across diverse tasks and environments. Recent works build robot foundation models by extending multimodal large language models (MLLMs) with action outputs, creating vision-language-action (VLA) systems. These efforts are motivated by the intuition that MLLMs' large-scale language and image pretraining can be effectively transferred to the action output modality. In this work, we explore an alternative paradigm of using large-scale video pretraining as a primary modality for building robot foundation models. Unlike static images and language, videos capture spatio-temporal sequences of states and actions in the physical world that are naturally aligned with robotic behavior. We curate an internet-scale video dataset of human activities and task demonstrations, and train, for the first time at a foundation-model scale, an open video model for generative robotics planning. The model produces zero-shot video plans for novel scenes and tasks, which we post-process to extract executable robot actions. We evaluate task-level generalization through third-party selected tasks in the wild and real-robot experiments, demonstrating successful physical execution. Together, these results show robust instruction following, strong generalization, and real-world feasibility. We release both the model and dataset to support open, reproducible video-based robot learning. Our website is available at https://www.boyuan.space/large-video-planner/.

</details>


### [145] [SORS: A Modular, High-Fidelity Simulator for Soft Robots](https://arxiv.org/abs/2512.15994)
*Manuel Mekkattu,Mike Y. Michelis,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 本文提出了一种全新的高保真软体机器人模拟器SORS，专为处理多物理场环境下软体机器人复杂动力学而设计，能够精确预测现实世界中的机器人表现。


<details>
  <summary>Details</summary>
Motivation: 软体机器人由于变形大、材料不可压缩和复杂接触等特点，给建模和数值稳定性带来巨大挑战，现有仿真器在可扩展性和应用相关性方面表现有限。因此急需一种能够准确、高效且可扩展的高保真仿真平台。

Method: 提出基于有限元法的能量驱动模拟框架，采用可模块化扩展方法支持自定义材料和驱动模型。接触处理上融入基于序列二次规划的约束非线性优化算法，实现了高效稳定的接触建模，通过多种现实实验验证准确性。

Result: 在悬臂梁变形、软体机械臂驱动、数据集中的接触互动等多项实验证明该模拟器能高保真捕捉材料特性与复杂驱动行为。此外，展示其在软体机器人腿部优化控制中的应用潜力。

Conclusion: SORS填补了软体机器人领域可扩展性、高保真和易用性的空白，为原型开发和应用优化提供了经过验证的有力工具，有效推进了仿真与现实之间的契合。

Abstract: The deployment of complex soft robots in multiphysics environments requires advanced simulation frameworks that not only capture interactions between different types of material, but also translate accurately to real-world performance. Soft robots pose unique modeling challenges due to their large nonlinear deformations, material incompressibility, and contact interactions, which complicate both numerical stability and physical accuracy. Despite recent progress, robotic simulators often struggle with modeling such phenomena in a scalable and application-relevant manner. We present SORS (Soft Over Rigid Simulator), a versatile, high-fidelity simulator designed to handle these complexities for soft robot applications. Our energy-based framework, built on the finite element method, allows modular extensions, enabling the inclusion of custom-designed material and actuation models. To ensure physically consistent contact handling, we integrate a constrained nonlinear optimization based on sequential quadratic programming, allowing for stable and accurate modeling of contact phenomena. We validate our simulator through a diverse set of real-world experiments, which include cantilever deflection, pressure-actuation of a soft robotic arm, and contact interactions from the PokeFlex dataset. In addition, we showcase the potential of our framework for control optimization of a soft robotic leg. These tests confirm that our simulator can capture both fundamental material behavior and complex actuation dynamics with high physical fidelity. By bridging the sim-to-real gap in these challenging domains, our approach provides a validated tool for prototyping next-generation soft robots, filling the gap of extensibility, fidelity, and usability in the soft robotic ecosystem.

</details>


### [146] [dLITE: Differentiable Lighting-Informed Trajectory Evaluation for On-Orbit Inspection](https://arxiv.org/abs/2512.16011)
*Jack Naylor,Raghav Mishra,Nicholas H. Barbara,Donald G. Dansereau*

Main category: cs.RO

TL;DR: 本文提出了一种基于端到端可微分仿真流程库（∂LITE），专门用于提升在轨资产（如卫星）视觉检查操作的数据质量和效果。


<details>
  <summary>Details</summary>
Motivation: 轨道资产（如高价值卫星）的在轨视觉检查对于维护、损伤评估和延寿具有重要意义。然而，低地球轨道（LEO）的特殊环境带来了诸多挑战，比如太阳镜面反射、自我遮挡、动态光照，以及航天器间的相对运动等都会严重影响成像质量。此前的检查路径规划主要依赖于仿真，但很少有工作专门针对提高图像质量做端到端优化。

Method: 作者开发了∂LITE，一个结合可微渲染工具与自定义轨道传播器的端到端仿真流程。通过这一仿真系统，可以基于视觉传感器获得的数据对轨道参数进行端到端优化，自动设计出提升成像质量的非显性轨迹。

Result: 通过∂LITE，能自动规划在轨检查任务的轨迹，大幅提升所采集数据的质量和信息价值。实验表明，该系统能发现以往基于经验难以发现的最优轨迹。

Conclusion: ∂LITE为航天器任务规划提供了新思路，首次实现了将图像质量优化贯穿于在轨检查轨迹规划的全过程。该方法展示了现代计算方法在航天任务规划领域的创新潜力。

Abstract: Visual inspection of space-borne assets is of increasing interest to spacecraft operators looking to plan maintenance, characterise damage, and extend the life of high-value satellites in orbit. The environment of Low Earth Orbit (LEO) presents unique challenges when planning inspection operations that maximise visibility, information, and data quality. Specular reflection of sunlight from spacecraft bodies, self-shadowing, and dynamic lighting in LEO significantly impact the quality of data captured throughout an orbit. This is exacerbated by the relative motion between spacecraft, which introduces variable imaging distances and attitudes during inspection. Planning inspection trajectories with the aide of simulation is a common approach. However, the ability to design and optimise an inspection trajectory specifically to improve the resulting image quality in proximity operations remains largely unexplored. In this work, we present $\partial$LITE, an end-to-end differentiable simulation pipeline for on-orbit inspection operations. We leverage state-of-the-art differentiable rendering tools and a custom orbit propagator to enable end-to-end optimisation of orbital parameters based on visual sensor data. $\partial$LITE enables us to automatically design non-obvious trajectories, vastly improving the quality and usefulness of attained data. To our knowledge, our differentiable inspection-planning pipeline is the first of its kind and provides new insights into modern computational approaches to spacecraft mission planning. Project page: https://appearance-aware.github.io/dlite/

</details>


### [147] [Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios](https://arxiv.org/abs/2512.16019)
*Qiping Zhang,Nathan Tsoi,Mofeed Nagib,Hao-Tien Lewis Chiang,Marynel Vázquez*

Main category: cs.RO

TL;DR: 本研究提出利用大语言模型（LLM）的少样本学习能力，通过小样本自动预测人类对机器人行为的评价，特别聚焦于社会导航场景，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统评估机器人行为时需要大量标注数据，执行用户研究成本高且不具备可扩展性。现有机器学习方法也依赖大量数据，限制了实际应用。作者旨在解决样本需求大的现实难题，提高预测用户评价的效率与普适性。

Method: 作者扩展了SEAN TOGETHER数据集，融入更多真实人机导航场景及用户反馈。采用多种大语言模型，通过少量上下文示例，让模型预判用户对机器人表现的感知，并通过消融实验分析输入特征的重要性，同时探索个性化示例对提升模型效果的价值。

Result: 实验发现，大语言模型仅需极少标注样本就能达到甚至超过传统监督学习方法的表现，且提供更多上下文示例能进一步提升预测能力。个性化示例显著增强了模型的准确性。消融实验揭示了不同传感器信息对判定结果的影响。

Conclusion: 利用大语言模型进行少样本学习，可高效、可扩展地预测人机交互过程中的人类评价，有助于实现基于用户反馈的自适应机器人行为优化，为社交型机器人发展提供了新路径。

Abstract: Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.

</details>


### [148] [Maintaining the Level of a Payload carried by Multi-Robot System on Irregular Surface](https://arxiv.org/abs/2512.16024)
*Rishabh Dev Yadav,Shrey Agrawal,Kamalakar Karlapalem*

Main category: cs.RO

TL;DR: 本文提出了一种多机器人协作运输系统，可在未知且不平整的地形中运输物体，并保持其特定姿态。通过在机器人上安装线性执行器，结合开环与PID闭环控制，系统能够适应各种复杂地形，有效维持负载姿态。


<details>
  <summary>Details</summary>
Motivation: 在未知或地形不平的环境中运输大型或重要物品时，常出现姿态难以保持、运输安全性差或过程不稳定等问题，此研究旨在解决传统方法在复杂地形下的局限性。

Method: 定制机器人配备线性执行器作为机械支撑，搭配持续监测负载姿态的传感与控制系统。采用开环控制计算全局动作，再用闭环PID控制精修单台机器人动作，实现整体协作。所有控制无需对地形作先验假设。

Result: 提出的控制系统在多种仿真环境（含复杂地形）中进行了测试，实验表明新方法能有效使多机器人协作保持负载预期的姿态，并能适应各种地形。

Conclusion: 该系统能够实现在各类未知与不平地面上的稳定负载运输，对于实际多机器人协作运输任务具有较高实用价值和通用性。

Abstract: In this paper, we introduce a multi robot payload transport system to carry payloads through an environment of unknown and uneven inclinations while maintaining the desired orientation of the payload. For this task, we used custom built robots with a linear actuator (pistons) mounted on top of each robot. The system continuously monitors the payload's orientation and computes the required piston height of each robot to maintain the desired orientation of the payload. In this work, we propose an open loop controller coupled with a closed loop PID controller to achieve the goal. As our modelling makes no assumptions on the type of terrain, the system can work on any unknown and uneven terrains and inclinations. We showcase the efficacy of our proposed controller by testing it on various simulated environments with varied and complex terrains.

</details>


### [149] [SWIFT-Nav: Stability-Aware Waypoint-Level TD3 with Fuzzy Arbitration for UAV Navigation in Cluttered Environments](https://arxiv.org/abs/2512.16027)
*Shuaidong Ji,Mahdi Bamdad,Francisco Cruz*

Main category: cs.RO

TL;DR: 提出了一种结合TD3强化学习和模糊逻辑的无人机导航新方法SWIFT-Nav，实现了高效且在复杂动态环境下具有良好鲁棒性的路径规划。


<details>
  <summary>Details</summary>
Motivation: 当前无人机在复杂、动态环境中高效、可靠导航仍面临挑战。现有方法在收敛速度、轨迹平滑性和应对新环境的泛化能力上存在不足。

Method: 提出SWIFT-Nav框架，将基于传感器的感知前端与TD3导航策略结合。感知模块将激光雷达数据转为加权安全地图，并生成目标指引。TD3策略采用优先经验回放，聚焦于高误差样本，同时使用逐渐减少的epsilon-greedy探索机制。引入轻量级模糊逻辑层计算安全分数，控制模式切换并抑制不安全动作。奖励函数设计兼顾目标进度、安全距离和切换经济性，提供密集反馈以加速训练。

Result: 方法在Webots仿真测试环境下表现优异，无人机在轨迹平滑性、对未见环境的泛化和实时响应性方面均优于现有基线方法。

Conclusion: 将TD3与经验回放优先级、校准探索和基于模糊逻辑的安全规则结合，可实现鲁棒且可实际部署的复杂场景无人机导航方案。

Abstract: Efficient and reliable UAV navigation in cluttered and dynamic environments remains challenging. We propose SWIFT-Nav: Stability-aware Waypoint-level Integration of Fuzzy arbitration and TD3 for Navigation, a TD3-based navigation framework that achieves fast, stable convergence to obstacle-aware paths. The system couples a sensor-driven perception front end with a TD3 waypoint policy: the perception module converts LiDAR ranges into a confidence-weighted safety map and goal cues, while the TD3 policy is trained with Prioritised Experience Replay to focus on high-error transitions and a decaying epsilon-greedy exploration schedule that gradually shifts from exploration to exploitation. A lightweight fuzzy-logic layer computes a safety score from radial measurements and near obstacles, gates mode switching and clamps unsafe actions; in parallel, task-aligned reward shaping combining goal progress, clearance, and switch-economy terms provides dense, well-scaled feedback that accelerates learning. Implemented in Webots with proximity-based collision checking, our approach consistently outperforms baselines in trajectory smoothness and generalization to unseen layouts, while preserving real-time responsiveness. These results show that combining TD3 with replay prioritisation, calibrated exploration, and fuzzy-safety rules yields a robust and deployable solution for UAV navigation in cluttered scenes.

</details>


### [150] [A Task-Driven, Planner-in-the-Loop Computational Design Framework for Modular Manipulators](https://arxiv.org/abs/2512.16069)
*Maolin Lei,Edoardo Romiti,Arturo Laurenzi,Rui Dai,Matteo Dalle Vedove,Jiatao Ding,Daniele Fontanelli,Nikos Tsagarakis*

Main category: cs.RO

TL;DR: 本文提出了一种统一的、任务驱动的计算框架，用于模块化机械臂在多形态下的轨迹规划与形态-安装位姿的协同优化，实现了高效的运动生成和硬件实验验证。


<details>
  <summary>Details</summary>
Motivation: 模块化机械臂通过可替换的预制模块实现了适应多任务的高灵活性，但在运动生成时，需要同时满足运动学、动力学和物理约束，且传统单分支结构加长时容易导致基座关节扭矩超限，因此急需统一的设计与运动规划优化方法。

Method: 提出了一个统一的任务驱动计算框架，融合了形态与安装位姿的协同优化。核心方法包括层次化模型预测控制策略（HMPC）实现不同类型机械臂的运动规划；设计优化中采用CMA-ES高效搜索离散形态与连续安装位姿的混合空间；通过虚拟模块抽象实现了双分支形态，让辅助手臂分担主分支扭矩、拓展工作空间。

Result: 方案在打磨、钻孔与搬运等多任务中进行了大量仿真与实物实验，结果表明：1）可以为不同任务生成符合运动学、动力学及环境约束的多种有效设计；2）可通过调整代价函数，灵活实现不同设计目标（如最大可操作性、最小关节负载或模块数）；3）无需更大功率基础模块即可实现大工作空间的双分支机械臂。

Conclusion: 所提出的框架在多类型任务中能够有效生成、优化机械臂结构和运动方案，拓展了模块化机械臂的工作空间与应用场景，对实际机器人系统的设计和部署具有重要意义。

Abstract: Modular manipulators composed of pre-manufactured and interchangeable modules offer high adaptability across diverse tasks. However, their deployment requires generating feasible motions while jointly optimizing morphology and mounted pose under kinematic, dynamic, and physical constraints. Moreover, traditional single-branch designs often extend reach by increasing link length, which can easily violate torque limits at the base joint. To address these challenges, we propose a unified task-driven computational framework that integrates trajectory planning across varying morphologies with the co-optimization of morphology and mounted pose. Within this framework, a hierarchical model predictive control (HMPC) strategy is developed to enable motion planning for both redundant and non-redundant manipulators. For design optimization, the CMA-ES is employed to efficiently explore a hybrid search space consisting of discrete morphology configurations and continuous mounted poses. Meanwhile, a virtual module abstraction is introduced to enable bi-branch morphologies, allowing an auxiliary branch to offload torque from the primary branch and extend the achievable workspace without increasing the capacity of individual joint modules. Extensive simulations and hardware experiments on polishing, drilling, and pick-and-place tasks demonstrate the effectiveness of the proposed framework. The results show that: 1) the framework can generate multiple feasible designs that satisfy kinematic and dynamic constraints while avoiding environmental collisions for given tasks; 2) flexible design objectives, such as maximizing manipulability, minimizing joint effort, or reducing the number of modules, can be achieved by customizing the cost functions; and 3) a bi-branch morphology capable of operating in a large workspace can be realized without requiring more powerful basic modules.

</details>


### [151] [A simulation platform calibration method for automated vehicle evaluation: accurate on both vehicle level and traffic flow level](https://arxiv.org/abs/2512.16076)
*Jia Hu,Junqi Li,Xuerun Yan,Jintao Lai,Lianhua An*

Main category: cs.RO

TL;DR: 本文提出了一种高精度、全自动的仿真平台校准方法，以提升自动驾驶车辆仿真测试中与环境交通的交互复现能力。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶仿真测试中的交通交互校准方法存在准确度不足的问题，影响仿真测试的可靠性。为了解决这一难题，需要开发新的高效、准确的校准方法。

Method: 本文提出了一种新的校准方法，具备车辆间交互校准、结果准确性保障、高效自动化，以及流水线式校准等特性。该方法与无校准和最新校准方法进行了对比验证。

Result: 新方法在车辆交互复现精度提升83.53%，校准效率提升76.75%；同时在车辆级和流量级指标上提升了51.9%，并且全过程无需人工参与。

Conclusion: 所提方法显著提升了自动驾驶仿真测试的交互复现精度和校准效率，具备全面自动化优势，未来可广泛应用于自动驾驶仿真验证领域。

Abstract: Simulation testing is a fundamental approach for evaluating automated vehicles (AVs). To ensure its reliability, it is crucial to accurately replicate interactions between AVs and background traffic, which necessitates effective calibration. However, existing calibration methods often fall short in achieving this goal. To address this gap, this study introduces a simulation platform calibration method that ensures high accuracy at both the vehicle and traffic flow levels. The method offers several key features:(1) with the capability of calibration for vehicle-to-vehicle interaction; (2) with accuracy assurance; (3) with enhanced efficiency; (4) with pipeline calibration capability. The proposed method is benchmarked against a baseline with no calibration and a state-of-the-art calibration method. Results show that it enhances the accuracy of interaction replication by 83.53% and boosts calibration efficiency by 76.75%. Furthermore, it maintains accuracy across both vehicle-level and traffic flow-level metrics, with an improvement of 51.9%. Notably, the entire calibration process is fully automated, requiring no human intervention.

</details>


### [152] [ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation](https://arxiv.org/abs/2512.16302)
*Zixuan Chen,Chongkai Gao,Lin Shao,Jieqi Shi,Jing Huo,Yang Gao*

Main category: cs.RO

TL;DR: 本文提出了一种名为ManiLong-Shot的新框架，使机器人在只需单次示范的情况下，能够完成复杂的长时序操控任务。


<details>
  <summary>Details</summary>
Motivation: 当前一拍模仿学习（OSIL）方法大多仅适用于短时序任务，难以推广到复杂的长时序操控，限制了其实际应用。作者意在解决现有OSIL方法对长时序操纵任务能力不足的问题。

Method: ManiLong-Shot框架将长时序任务分解为物理交互事件，转换为序列化操作单元（primitives）的分阶段决策，而不是直接模仿连续轨迹。这一分解可通过视觉-语言模型（VLM）的高层推理，或基于机器人状态变化的规则驱动。每个primitives阶段预测对应的不变交互区域、建立演示与当前观测之间的联系、计算目标末端执行器位姿，从而高效执行任务。

Result: 大量仿真实验表明，ManiLong-Shot只需基于10个短时序任务训练，即可泛化到20个不同难度等级的未见长时序任务，单次模仿下性能比SOTA方法提升22.8%。此外，真实机器人实验证实框架可稳健完成3个长时序操作任务。

Conclusion: ManiLong-Shot显著提升了一拍模仿学习在长时序操控任务中的适用性，具备实际可用性，扩展了机器人模仿学习方法的应用范围。

Abstract: One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.

</details>


### [153] [A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion](https://arxiv.org/abs/2512.16367)
*Sijia Chen,Wei Dong*

Main category: cs.RO

TL;DR: 本文提出了一种结合地面与空中协作的定位系统，通过多传感器融合，提升飞行机器人在复杂环境下的鲁棒性，特别是在视觉传感器受损时仍能保证定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统的飞行机器人定位方法依赖于固定摄像头与标记点，存在距离受限、捕捉失败等问题，难以应对传感器视觉退化和环境复杂变化。因此，需要一种更为鲁棒和综合性的方案来提升定位可靠性。

Method: 提出了一种综合融合视觉主动观测、单点测距、惯性测程和光流的地空协作定位框架。地面载体搭载的主动视觉系统可旋转并跟踪空中机器人上的红外标记，单点测距提升了远距离和视觉丢失下的重新捕捉能力。定位估计阶段采用基于多项式近似和扩展滑动窗口的降维融合算法，兼顾计算效率和冗余性。针对不同传感器信噪比，利用自适应置信度评估算法动态调整测量权重，提高整体定位质量。

Result: 系统在烟雾干扰、光照变化、遮挡、长时间视觉丢失和大范围作业等苛刻条件下进行了大量实验，取得了平均均方根误差约0.09米的鲁棒在线定位结果，并能有效应对传感器捕捉失败等情况。

Conclusion: 本方法通过多源数据融合和自适应加权，显著提升了飞行机器人在复杂环境下的定位鲁棒性和可靠性，解决了传统方法在视觉退化及传感器失效时鲁棒性不足的问题。

Abstract: It's a practical approach using the ground-aerial collaborative system to enhance the localization robustness of flying robots in cluttered environments, especially when visual sensors degrade. Conventional approaches estimate the flying robot's position using fixed cameras observing pre-attached markers, which could be constrained by limited distance and susceptible to capture failure. To address this issue, we improve the ground-aerial localization framework in a more comprehensive manner, which integrates active vision, single-ranging, inertial odometry, and optical flow. First, the designed active vision subsystem mounted on the ground vehicle can be dynamically rotated to detect and track infrared markers on the aerial robot, improving the field of view and the target recognition with a single camera. Meanwhile, the incorporation of single-ranging extends the feasible distance and enhances re-capture capability under visual degradation. During estimation, a dimension-reduced estimator fuses multi-source measurements based on polynomial approximation with an extended sliding window, balancing computational efficiency and redundancy. Considering different sensor fidelities, an adaptive sliding confidence evaluation algorithm is implemented to assess measurement quality and dynamically adjust the weighting parameters based on moving variance. Finally, extensive experiments under conditions such as smoke interference, illumination variation, obstacle occlusion, prolonged visual loss, and extended operating range demonstrate that the proposed approach achieves robust online localization, with an average root mean square error of approximately 0.09 m, while maintaining resilience to capture loss and sensor failures.

</details>


### [154] [E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion](https://arxiv.org/abs/2512.16446)
*Enis Yalcin,Joshua O'Hara,Maria Stamatopoulou,Chengxu Zhou,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: 该论文提出了E-SDS框架，通过结合视觉-语言模型（VLM）和实时地形传感器，自动生成奖励函数以训练更具感知能力的仿人机器人运动策略。在多种地形测试下，E-SDS显著优于传统方法，并大幅降低了人工奖励设计时间。


<details>
  <summary>Details</summary>
Motivation: 目前利用VLM进行仿人机器人运动奖励自动化设计时，因缺乏环境感知能力，导致在复杂地形上表现受限。为突破VLM“盲目”的问题，作者提出集成环境感知，使奖励设计既自动又能适应多样的实际环境。

Method: 提出E-SDS框架，将VLM与实时地形传感器分析相结合，利用视频示例指导奖励函数的自动生成，并应用于仿人机器人在不同地形上的运动策略训练。通过与手工设计奖励和不含感知的自动化方案对比实验，验证方法效果。

Result: 在四类地形测试（简单、间隙、障碍、楼梯）中，E-SDS训练的策略唯一成功实现下楼梯任务，并在所有地形中将速度跟踪误差降低了51.9%-82.6%。此外，将奖励设计所需人力由数天缩短至两小时以内。

Conclusion: E-SDS大幅提升了仿人机器人在复杂环境下的运动能力和鲁棒性，显著减少了人工奖励工程的投入，展示了环境感知与VLM集成的巨大应用前景。

Abstract: Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially "blind", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.

</details>


### [155] [Single-View Shape Completion for Robotic Grasping in Clutter](https://arxiv.org/abs/2512.16449)
*Abhishek Kashyap,Yuxuan Yang,Henrik Andreasson,Todor Stoyanov*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩散模型的3D形状补全方法，用于提升视觉抓取任务中单视角、遮挡及杂乱场景下的抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 单个摄像头只能获取物体一侧信息，且杂乱场景下遮挡严重，导致所得物体几何信息不完整，使得抓取算法效果不佳。需要一种能够从不完整观测中推断完整3D形状的方法，提升下游抓取任务表现。

Method: 作者利用扩散模型对单视角的部分深度观测数据进行类别级3D形状补全，复原具有多样几何形状的常见家居物品的完整3D模型，将补全结果作为后续抓取推理网络的输入。不同于以往只关注分离物体或低杂乱度场景的方法，本文在真实的杂乱家居场景中进行了评估。

Result: 在初步实验中，该方法相比于不进行形状补全的基线方法，抓取成功率提升了23%；相比当前最新的形状补全方法提升了19%。

Conclusion: 扩散模型进行3D形状补全显著提升了复杂杂乱场景下的目标抓取效果，为实际机器人感知与操作系统提供了更丰富的上下文信息。

Abstract: In vision-based robot manipulation, a single camera view can only capture one side of objects of interest, with additional occlusions in cluttered scenes further restricting visibility. As a result, the observed geometry is incomplete, and grasp estimation algorithms perform suboptimally. To address this limitation, we leverage diffusion models to perform category-level 3D shape completion from partial depth observations obtained from a single view, reconstructing complete object geometries to provide richer context for grasp planning. Our method focuses on common household items with diverse geometries, generating full 3D shapes that serve as input to downstream grasp inference networks. Unlike prior work, which primarily considers isolated objects or minimal clutter, we evaluate shape completion and grasping in realistic clutter scenarios with household objects. In preliminary evaluations on a cluttered scene, our approach consistently results in better grasp success rates than a naive baseline without shape completion by 23% and over a recent state of the art shape completion approach by 19%. Our code is available at https://amm.aass.oru.se/shape-completion-grasping/.

</details>


### [156] [AG-MPBS: a Mobility-Aware Prediction and Behavior-Based Scheduling Framework for Air-Ground Unmanned Systems](https://arxiv.org/abs/2512.16454)
*Tianhao Shao,Kaixing Zhao,Feng Liu,Lixin Yang,Bin Guo*

Main category: cs.RO

TL;DR: 提出了一种名为MPBS的调度框架，通过行为识别、移动预测和动态优先级，实现无人系统中高效的任务招聘与分配，实验证明提升了效率和资源利用。


<details>
  <summary>Details</summary>
Motivation: 随着无人机、无人车等无人系统在城市感知和应急响应中的应用日益广泛，如何高效调度这些设备执行时效性任务成为实际挑战。

Method: MPBS框架包含三个关键模块：行为感知的KNN分类器、用于设备移动预测的时变马尔可夫模型以及结合任务紧急性和基站性能的动态优先级调度机制；系统结合行为分类和时空预测，实现动态、适应性任务分配。

Result: 基于实际GeoLife数据集的实验表明，MPBS在任务完成效率和资源利用率方面显著优于现有方法。

Conclusion: MPBS框架为无人系统的任务调度提供了预测性、行为感知的智能协同方法，对相关应用具有推广价值。

Abstract: As unmanned systems such as Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) become increasingly important to applications like urban sensing and emergency response, efficiently recruiting these autonomous devices to perform time-sensitive tasks has become a critical challenge. This paper presents MPBS (Mobility-aware Prediction and Behavior-based Scheduling), a scalable task recruitment framework that treats each device as a recruitable "user". MPBS integrates three key modules: a behavior-aware KNN classifier, a time-varying Markov prediction model for forecasting device mobility, and a dynamic priority scheduling mechanism that considers task urgency and base station performance. By combining behavioral classification with spatiotemporal prediction, MPBS adaptively assigns tasks to the most suitable devices in real time. Experimental evaluations on the real-world GeoLife dataset show that MPBS significantly improves task completion efficiency and resource utilization. The proposed framework offers a predictive, behavior-aware solution for intelligent and collaborative scheduling in unmanned systems.

</details>


### [157] [Tri-Select: A Multi-Stage Visual Data Selection Framework for Mobile Visual Crowdsensing](https://arxiv.org/abs/2512.16469)
*Jiayu Zhang,Kaixing Zhao,Tianhao Shao,Bin Guo,Liang He*

Main category: cs.RO

TL;DR: 该论文提出了Tri-Select，一个高效筛选移动视觉众包采集数据中过多、低质量图像的多阶段方法。


<details>
  <summary>Details</summary>
Motivation: 移动视觉众包采集得到的图像数据常因用户行为多样、拍摄角度重叠、分辨率不同而存在大量冗余和异质性，影响后续环境监测应用的数据效率与质量。

Method: 论文提出的Tri-Select方法包含三个阶段：（1）基于元数据过滤无关样本；（2）用空间相似性谱聚类组织候选图像；（3）通过最大独立集搜索结合视觉特征挑选高质量、具代表性的图像。

Result: 在真实世界和公开数据集上的实验表明，Tri-Select提升了图像选择的效率与数据集整体质量。

Conclusion: Tri-Select有效解决了移动视觉众包中的图像冗余与异质性问题，适用于大规模数据场景，提升了数据处理的可扩展性。

Abstract: Mobile visual crowdsensing enables large-scale, fine-grained environmental monitoring through the collection of images from distributed mobile devices. However, the resulting data is often redundant and heterogeneous due to overlapping acquisition perspectives, varying resolutions, and diverse user behaviors. To address these challenges, this paper proposes Tri-Select, a multi-stage visual data selection framework that efficiently filters redundant and low-quality images. Tri-Select operates in three stages: (1) metadata-based filtering to discard irrelevant samples; (2) spatial similarity-based spectral clustering to organize candidate images; and (3) a visual-feature-guided selection based on maximum independent set search to retain high-quality, representative images. Experiments on real-world and public datasets demonstrate that Tri-Select improves both selection efficiency and dataset quality, making it well-suited for scalable crowdsensing applications.

</details>


### [158] [A Formal Modular Synthesis Approach for the Coordination of 3-D Robotic Construction with Multi-robots](https://arxiv.org/abs/2512.16555)
*Marcelo Rosa,José E. R. Cury,Fabio L. Baldissera*

Main category: cs.RO

TL;DR: 本文提出了一种基于监督控制理论的方法，实现多机器人协作自主建造三维结构。通过为单个机器人和目标结构建模，合成出可复制的反应式控制器（监督器），从而保证所有机器人可共同完成目标结构。


<details>
  <summary>Details</summary>
Motivation: 多机器人协作建造三维结构面临自动化与协调难题。实现自主、可靠、无人干预的搭建过程，对于提高建筑、制造等领域效率意义重大。

Method: 该方法基于监督控制理论，将单机器人与目标结构建模，通过合成，自动生成可确保系统正确行为的监督器控制策略。然后复制这一监督器到多机器人系统，实现分布式协作。

Result: 实验或理论分析结果表明，所提出的监督控制器能够有效地指导多机器人自主协作，成功搭建预设的三维结构。

Conclusion: 基于监督控制理论的方法可实现多机器人高效、可靠地协作搭建三维结构。该方案具有可扩展性及自动生成控制策略的优点。

Abstract: In this paper, we deal with the problem of coordinating multiple robots to build 3-D structures. This problem consists of a set of mobile robots that interact with each other in order to autonomously build a predefined 3-D structure. Our approach is based on Supervisory Control Theory, and it allows us to synthesize from models that represent a single robot and the target structure a correct-by-construction reactive controller, called supervisor. When this supervisor is replicated for the other robots, then the target structure can be completed by all robots

</details>


### [159] [Olaf: Bringing an Animated Character to Life in the Physical World](https://arxiv.org/abs/2512.16705)
*David Müller,Espen Knoop,Dario Mylonopoulos,Agon Serifi,Michael A. Hopkins,Ruben Grandia,Moritz Bächer*

Main category: cs.RO

TL;DR: 本文将动画角色Olaf实体化，采用强化学习与机械创新控制其动作，实现了高度逼真的动态表现。


<details>
  <summary>Details</summary>
Motivation: 动画角色常常采用非物理性的动作和不寻常的身体比例，这为机器人机械设计和动作控制提供了创新空间。作者希望将动画角色Olaf以高度仿真的方式带入物理世界。

Method: 结合动画参考和强化学习算法对机器人的运动进行控制。采用两条隐藏于泡沫裙下的不对称腿模拟Olaf移动，并在手臂、嘴巴和眼睛使用球面与平面连杆以适应狭小的空间。在降低步态冲击噪声和防止驱动器过热方面引入新型奖励机制，并将温度反馈作为策略输入。

Result: 通过仿真和实体硬件实验证明，提出的控制方法和机械结构能有效生成接近原动画的自然动作，实现噪音及温度控制，展现了高度真实感。

Conclusion: 本研究实现了机器人角色在仿生动作和机械设计上的突破，为动画角色机器人实体化提供了新的方法论，并在动作真实度与物理可控性之间取得出色平衡。

Abstract: Animated characters often move in non-physical ways and have proportions that are far from a typical walking robot. This provides an ideal platform for innovation in both mechanical design and stylized motion control. In this paper, we bring Olaf to life in the physical world, relying on reinforcement learning guided by animation references for control. To create the illusion of Olaf's feet moving along his body, we hide two asymmetric legs under a soft foam skirt. To fit actuators inside the character, we use spherical and planar linkages in the arms, mouth, and eyes. Because the walk cycle results in harsh contact sounds, we introduce additional rewards that noticeably reduce impact noise. The large head, driven by small actuators in the character's slim neck, creates a risk of overheating, amplified by the costume. To keep actuators from overheating, we feed temperature values as additional inputs to policies, introducing new rewards to keep them within bounds. We validate the efficacy of our modeling in simulation and on hardware, demonstrating an unmatched level of believability for a costumed robotic character.

</details>


### [160] [VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation](https://arxiv.org/abs/2512.16724)
*Yixiang Chen,Yan Huang,Keji He,Peiyan Li,Liang Wang*

Main category: cs.RO

TL;DR: 本文提出了一种名为VERM（Virtual Eye for Robotic Manipulation）的新方法，通过基金会模型的知识，从多摄像头构建的3D点云中生成虚拟、任务自适应视角，大幅提取关键特征、过滤冗余信息，实现机器人高效3D操作任务。实验表明，VERM能显著提升性能和速度。


<details>
  <summary>Details</summary>
Motivation: 多摄像头带来大量冗余和无关数据，导致机器人3D操作中的计算和训练负担加重，影响提取关键任务特征的效率，亟需有效方法过滤信息冗余，精准提取与任务相关内容。

Method: VERM方法结合基金会模型知识，根据3D点云生成聚焦任务的虚拟视角，有效应对多摄像头冗余和遮挡问题。方法还包含深度感知模块和动态粗到细的操作流程，实现更细致的3D动作规划和精细操作。

Result: 在仿真RLBench基准和真实机器人测试中，VERM表现优于以往SOTA方法，训练速度提升1.89倍、推理速度提升1.54倍，验证了方法的有效性。

Conclusion: VERM方法可高效过滤冗余信息，提取关键特征，提升机器人3D操作表现和效率，为多摄像头感知下的机器人任务提供了新的解决方案。

Abstract: When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .

</details>


### [161] [Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future](https://arxiv.org/abs/2512.16760)
*Tianshuai Hu,Xiaolu Liu,Song Wang,Yiyao Zhu,Ao Liang,Lingdong Kong,Guoyang Zhao,Zeying Gong,Jun Cen,Zhiyu Huang,Xiaoshuai Hao,Linfeng Li,Hang Song,Xiangtai Li,Jun Ma,Shaojie Shen,Jianke Zhu,Dacheng Tao,Ziwei Liu,Junwei Liang*

Main category: cs.RO

TL;DR: 本文系统梳理了视觉-语言-动作（VLA）在自动驾驶领域的最新进展，并对主流的技术范式进行了分类和分析，指出VLA有望提升自动驾驶系统的可解释性、泛化能力和与人类驾驶员的兼容性。


<details>
  <summary>Details</summary>
Motivation: 传统的“感知-决策-行动”模块化自动驾驶体系在复杂或长尾场景下容易出错，规则驱动和级联结构导致误差传播，并影响后续控制表现。视觉-动作（VA）模型虽然简化流程，但缺乏透明性和人机交互能力，面临泛化与安全性挑战。随着大模型和多模态技术的发展，融合视觉与语言推理的VLA框架有望解决这些瓶颈。

Method: 本文对相关技术发展进行了梳理，将现有VLA方法分为端到端VLA（感知、推理与规划一体化）和双系统VLA（慢速语言模型推理+快速规划器执行）两类。进一步区分了文本/数值动作生成和显式/隐式引导等子类，并梳理了相关数据集和评测基准。

Result: 本文系统归纳并组织了VLA领域的主要研究工作，对不同方法的结构、优势和局限进行总结，并罗列了公开数据集和评测指标，为该领域的后续研究提供了参考框架。

Conclusion: 作者认为VLA技术为实现更可解释、通用、贴近人类驾驶行为的自动驾驶系统提供了新思路，但在鲁棒性、可解释性及指令遵循等方面仍存在挑战，有待未来进一步探索和改进。

Abstract: Autonomous driving has long relied on modular "Perception-Decision-Action" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.

</details>


### [162] [PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence](https://arxiv.org/abs/2512.16793)
*Xiaopeng Lin,Shijie Lian,Bin Yu,Ruoqi Yang,Changti Wu,Yuzhuo Miao,Yurun Jin,Yukun Shi,Cong Huang,Bojun Cheng,Kai Chen*

Main category: cs.RO

TL;DR: 本文提出了一种将大规模人类第一视角视频转化为身体智能训练数据的新方法，并基于此构建了E2E-3M数据集，训练出适用于机器人泛化任务的模型PhysBrain，显著提升了机器人在第一视角任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 机器人泛化任务需要具备物理智能，能够理解自身的感知与动作。但现有视觉语言模型主要基于第三人称数据，这与机器人的第一视角存在根本差异。同时，大规模收集机器人第一视角数据难度高、成本大。针对这一挑战，作者寻求利用大规模人类第一视角视频来替代真实机器人采集，从而为机器人训练提供丰富多样的数据。

Method: 作者提出了一条Egocentric2Embodiment自动翻译管线，把原始的人类第一视角视频转化为结构化、多层级的VQA（视觉问答）标注数据，强化证据溯源和时序一致性。基于此管线，构建了大规模数据集E2E-3M（300万样本），用于训练具有第一视角理解能力的模型PhysBrain。

Result: 在EgoThink等任务和环境中，PhysBrain的第一视角理解能力、规划性能有大幅提升。在SimperEnv任务中表现为成功率高达53.9%，展现了从人类第一视角监督到机器人控制的有效迁移。同时，PhysBrain的参数对下游任务的微调显著提升了样本效率。

Conclusion: 本工作验证了大规模人类第一视角视频辅助机器人训练的可行性和有效性。提出的Egocentric2Embodiment数据管线可扩展性强，为机器人泛化与智能理解提供了新方案，PhysBrain模型取得了领先的性能，预示未来通过这种方式能够进一步缩小机器人与人类在智能和理解上的差距。

Abstract: Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.

</details>


### [163] [ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning](https://arxiv.org/abs/2512.16861)
*Zihan Zhou,Animesh Garg,Ajay Mandlekar,Caelan Garrett*

Main category: cs.RO

TL;DR: 本文提出了一种针对长时序操作任务的机器人系统ReinforceGen，通过多种技术结合并使用强化学习精调，大幅提高了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 长时序操作任务复杂、难以将任务整体高效学习和执行，是机器人领域的难点。机器人需要在实际复杂操作中拥有更强的泛化和适应能力。

Method: ReinforceGen先将复杂任务分解为多个局部技能，通过运动规划连接各步骤。系统利用10次人类演示数据进行模仿学习训练，并随后通过在线自适应和强化学习微调各组件。

Result: 在Robosuite数据集上，ReinforceGen在最高重置间隔设置下，以视觉-运动控制达到了80%的所有任务成功率。消融实验显示，强化学习微调提升了平均性能89%。

Conclusion: ReinforceGen有效提升了机器人长时序操作能力，验证了任务分解、模仿学习与强化学习结合的有效性。在实际复杂场景中具有广泛应用潜力。

Abstract: Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/

</details>


### [164] [PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies](https://arxiv.org/abs/2512.16881)
*Arhan Jain,Mingtong Zhang,Kanav Arora,William Chen,Marcel Torne,Muhammad Zubair Irshad,Sergey Zakharov,Yue Wang,Sergey Levine,Chelsea Finn,Wei-Chiu Ma,Dhruv Shah,Abhishek Gupta,Karl Pertsch*

Main category: cs.RO

TL;DR: 本文提出了PolaRiS框架，通过神经重建技术将现实场景快速转化为高保真的可交互模拟环境，实现了机器人策略在模拟与现实之间更可信的评估。


<details>
  <summary>Details</summary>
Motivation: 机器人学习领域在于能否准确评测与比较各类机器人策略。现有真实环境中的评测耗时且难以复现，模拟环境又存在视觉物理域差异，导致评测结果不可靠，尤其对通用型策略更难以全面评测。

Method: 提出了一套面向高保真机器人物理与视觉交互的评测与场景重建方法（PolaRiS）。该方法利用神经重建技术，将短视频扫描的真实场景自动生成可交互的仿真环境。同时，提出了一种简单的仿真数据联合训练方法，缩小真实-仿真间的差距，实现未见仿真环境下的零样本评测。

Result: 大量的仿真-真实配对评测结果证明了PolaRiS评测结果与现实世界通用机器人策略表现的相关性明显优于现有模拟基准。此外，该方法大大简化了多样化仿真环境的快速创建流程。

Conclusion: PolaRiS框架使得机器人策略的评测更高效、相关性更强，推动了分布式、民主化的机器人基础大模型评测进程。

Abstract: A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.

</details>


### [165] [Sceniris: A Fast Procedural Scene Generation Framework](https://arxiv.org/abs/2512.16896)
*Jinghuan Shang,Harsh Patel,Ran Gong,Karl Schmeckpeper*

Main category: cs.RO

TL;DR: 本文提出了Sceniris，一个高效的程序化三维场景生成框架，大幅提高了合成场景的生成速度，适用于大规模数据集的构建，且支持机器人工作可达性检测。


<details>
  <summary>Details</summary>
Motivation: 合成三维场景对于物理智能和生成模型的发展至关重要。然而，现有的程序化生成方法输出速率低，极大限制了数据集规模的扩展。因此需要更高效的场景生成工具。

Method: 提出了Sceniris框架，重点优化了先前Scene Synthesizer方法的主要性能瓶颈，并利用cuRobo进行批量采样和更快的碰撞检测。同时，Sceniris增加了对象间空间关系配置的多样性，并可选性地为机器人任务提供可达性检测。

Result: Sceniris在场景生成速度上相较于Scene Synthesizer至少有234倍的提升，并支持更多元化的对象空间关系，能快速生成多样、无碰撞、支持机器人操作的三维场景。

Conclusion: Sceniris解决了合成三维场景低效的问题，为大规模、可用于机器人研究的场景数据集构建提供了新的高效工具。

Abstract: Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris

</details>
