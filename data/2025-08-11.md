<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.RO](#cs.RO) [Total: 25]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/abs/2508.05689)
*Jinjia Peng,Zeze Tao,Huibing Wang,Meng Wang,Yang Wang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的对抗攻击方法ResPA，可以提升对抗样本在不同神经网络模型间的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络在黑盒攻击下容易被对抗样本欺骗，而迁移攻击（transfer-based attacks）依赖于在代理模型生成对抗样本并转移到目标模型。突破点在于改善对抗扰动的方向，有助于跨模型泛化。

Method: 提出Residual Perturbation Attack（ResPA）方法：利用输入梯度的指数移动平均获得参考梯度（历史梯度方向），再用当前梯度与参考梯度的残差作为扰动方向，实现综合局部与全局方向的对抗样本生成。

Result: 实验证明，ResPA在对比典型的迁移型攻击方法时，有更好的迁移性，并且与现有的输入变换法结合后，效果进一步提升。

Conclusion: ResPA有效提升了对抗样本的迁移攻击能力，展示了该方法在黑盒攻击场景中的潜力和实际应用价值。

Abstract: Deep neural networks are susceptible to adversarial examples while suffering
from incorrect predictions via imperceptible perturbations. Transfer-based
attacks create adversarial examples for surrogate models and transfer these
examples to target models under black-box scenarios. Recent studies reveal that
adversarial examples in flat loss landscapes exhibit superior transferability
to alleviate overfitting on surrogate models. However, the prior arts overlook
the influence of perturbation directions, resulting in limited transferability.
In this paper, we propose a novel attack method, named Residual Perturbation
Attack (ResPA), relying on the residual gradient as the perturbation direction
to guide the adversarial examples toward the flat regions of the loss function.
Specifically, ResPA conducts an exponential moving average on the input
gradients to obtain the first moment as the reference gradient, which
encompasses the direction of historical gradients. Instead of heavily relying
on the local flatness that stems from the current gradients as the perturbation
direction, ResPA further considers the residual between the current gradient
and the reference gradient to capture the changes in the global perturbation
direction. The experimental results demonstrate the better transferability of
ResPA than the existing typical transfer-based attack methods, while the
transferability can be further improved by combining ResPA with the current
input transformation methods. The code is available at
https://github.com/ZezeTao/ResPA.

</details>


### [2] [Generalized Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2508.05732)
*Pinxuan Li,Bing Cao,Changqing Zhang,Qinghua Hu*

Main category: cs.CV

TL;DR: 本文提出了一种用于少样本异常检测的新框架GOOD，通过引入辅助的通用知识模型提升模型在开放世界下的泛化能力，并通过知识动态嵌入机制实现知识引导，有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本异常检测方法通常泛化能力不足，对训练数据过拟合，导致在实际开放环境下表现不佳。作者旨在从理论与方法层面提升模型的泛化能力。

Method: 作者提出了Generalized Few-shot OOD Detection (GOOD) 框架，利用一个通用知识模型（GKM）引导异常检测，而非仅依赖少量训练样本。此外，引入知识动态嵌入（KDE）机制，实现对知识引导的自适应调节，并通过理论推导出通用性-特异性平衡（GS-balance）。

Result: 在真实世界的异常检测基准上，提出的方法优于现有方法，展现出更强的泛化能力和稳定表现。

Conclusion: 该工作通过引入通用知识指导和自适应知识嵌入机制，显著提升了少样本异常检测在开放环境下的泛化能力，对实际应用有重要意义。

Abstract: Few-shot Out-of-Distribution (OOD) detection has emerged as a critical
research direction in machine learning for practical deployment. Most existing
Few-shot OOD detection methods suffer from insufficient generalization
capability for the open world. Due to the few-shot learning paradigm, the OOD
detection ability is often overfit to the limited training data itself, thus
degrading the performance on generalized data and performing inconsistently
across different scenarios. To address this challenge, we proposed a
Generalized Few-shot OOD Detection (GOOD) framework, which empowers the general
knowledge of the OOD detection model with an auxiliary General Knowledge Model
(GKM), instead of directly learning from few-shot data. We proceed to reveal
the few-shot OOD detection from a generalization perspective and theoretically
derive the Generality-Specificity balance (GS-balance) for OOD detection, which
provably reduces the upper bound of generalization error with a general
knowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)
mechanism to adaptively modulate the guidance of general knowledge. KDE
dynamically aligns the output distributions of the OOD detection model to the
general knowledge model based on the Generalized Belief (G-Belief) of GKM,
thereby boosting the GS-balance. Experiments on real-world OOD benchmarks
demonstrate our superiority. Codes will be available.

</details>


### [3] [UnGuide: Learning to Forget with LoRA-Guided Diffusion Models](https://arxiv.org/abs/2508.05755)
*Agnieszka Polowczyk,Alicja Polowczyk,Dawid Malarz,Artur Kasymov,Marcin Mazur,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: 本文提出了UnGuide方法，通过动态引导机制，有效实现了大规模文本到图像扩散模型的特定知识移除（machine unlearning），且优于现有LoRA方法。


<details>
  <summary>Details</summary>
Motivation: 当前大规模文本到图像扩散模型因可能被滥用于生成有害或误导内容而引发担忧，因此需要高效手段移除模型中的特定知识或概念，同时保持整体性能。

Method: 作者提出UnGuide方法，将动态无分类器引导（UnGuidance）与LoRA相结合，基于去噪初期的稳定性调整引导强度，从而在去除特定概念时由LoRA主导，但对无关内容则由基础模型主导，保障生成质量。

Result: 实验结果表明，UnGuide方法在目标对象擦除和显式内容移除任务中，能更好地控制概念去除，并优于现有基于LoRA的方法，同时保持了图像的表现力和质量。

Conclusion: UnGuide为模型特定知识移除提供了更精细、有效的方案，平衡内容保真与特定概念擦除，提升了现有扩散模型的可控性和应用安全性。

Abstract: Recent advances in large-scale text-to-image diffusion models have heightened
concerns about their potential misuse, especially in generating harmful or
misleading content. This underscores the urgent need for effective machine
unlearning, i.e., removing specific knowledge or concepts from pretrained
models without compromising overall performance. One possible approach is
Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models
for targeted unlearning. However, LoRA often inadvertently alters unrelated
content, leading to diminished image fidelity and realism. To address this
limitation, we introduce UnGuide -- a novel approach which incorporates
UnGuidance, a dynamic inference mechanism that leverages Classifier-Free
Guidance (CFG) to exert precise control over the unlearning process. UnGuide
modulates the guidance scale based on the stability of a few first steps of
denoising processes, enabling selective unlearning by LoRA adapter. For prompts
containing the erased concept, the LoRA module predominates and is
counterbalanced by the base model; for unrelated prompts, the base model
governs generation, preserving content fidelity. Empirical results demonstrate
that UnGuide achieves controlled concept removal and retains the expressive
power of diffusion models, outperforming existing LoRA-based methods in both
object erasure and explicit content removal tasks.

</details>


### [4] [Improving Masked Style Transfer using Blended Partial Convolution](https://arxiv.org/abs/2508.05769)
*Seyed Hadi Seyed,Ayberk Cansever,David Hart*

Main category: cs.CV

TL;DR: 本文提出了一种基于部分卷积的新型区域艺术风格迁移方法，有效提升了对选定区域风格迁移的准确性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有风格迁移方法通常仅支持对整张图像进行处理，但实际应用中用户常常只需对图像的某一特定区域施加艺术风格。直接对风格化结果做掩蔽处理，容易导致目标区域风格表达不自然。为此，需要一种能专门针对指定区域进行风格迁移的方法。

Method: 作者提出了一种基于部分卷积（partial convolution）的风格迁移网络。与传统做法不同，该方法在网络内部仅对感兴趣区域进行风格迁移，同时引入了内部融合技术，以适应区域选择的不完美情况，从而在风格迁移时对边界和遮罩处理更加细致。

Result: 通过在SA-1B数据集上的实验证明，该方法在视觉效果和定量评价上均优于传统的先风格化后掩蔽的做法，实现了对目标区域更精细和自然的风格迁移。

Conclusion: 所提出的方法能够专门针对选定区域进行高质量的风格迁移，有效解决了传统方法在区域风格应用过程中的不足，并在实验中取得了显著提升。相关代码已公开。

Abstract: Artistic style transfer has long been possible with the advancements of
convolution- and transformer-based neural networks. Most algorithms apply the
artistic style transfer to the whole image, but individual users may only need
to apply a style transfer to a specific region in the image. The standard
practice is to simply mask the image after the stylization. This work shows
that this approach tends to improperly capture the style features in the region
of interest. We propose a partial-convolution-based style transfer network that
accurately applies the style features exclusively to the region of interest.
Additionally, we present network-internal blending techniques that account for
imperfections in the region selection. We show that this visually and
quantitatively improves stylization using examples from the SA-1B dataset. Code
is publicly available at https://github.com/davidmhart/StyleTransferMasked.

</details>


### [5] [MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss](https://arxiv.org/abs/2508.05772)
*Can Zhao,Pengfei Guo,Dong Yang,Yucheng Tang,Yufan He,Benjamin Simon,Mason Belue,Stephanie Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TL;DR: 本文提出了MAISI-v2，这是一个加速的三维医学图像合成框架，结合了rectified flow技术，实现了更快且高质量的图像生成，并引入了区域特异性的对比损失以提升输入条件的拟合效果。


<details>
  <summary>Details</summary>
Motivation: 目前基于扩散模型的医学图像合成方法存在泛化性差、推理速度慢和条件一致性不强等问题。此前的MAISI框架虽然解决了一部分泛化性问题，但仍有速度慢和条件一致性不理想的缺陷。作者希望进一步提升模型的推理速度和条件拟合能力。

Method: 提出MAISI-v2框架，在架构中集成了rectified flow方法以实现快速推理，并设计了一种区域特异性的对比损失函数，加强模型对关键区域的关注，从而提升条件一致性。

Result: MAISI-v2实现了当前最优（SOTA）的图像质量，并使潜变量扩散模型的推理速度加快了33倍。此外，合成图像在下游分割任务中可以增强数据扩增效果。

Conclusion: MAISI-v2不仅在图像合成质量和速度上超越了现有方法，还增强了模型对输入区域条件的拟合能力，对医学图像生成及其应用具有重要意义。作者还开放了代码及相关资源，利于学界和业界进一步发展和复现。

Abstract: Medical image synthesis is an important topic for both clinical and research
applications. Recently, diffusion models have become a leading approach in this
area. Despite their strengths, many existing methods struggle with (1) limited
generalizability that only work for specific body regions or voxel spacings,
(2) slow inference, which is a common issue for diffusion models, and (3) weak
alignment with input conditions, which is a critical issue for medical imaging.
MAISI, a previously proposed framework, addresses generalizability issues but
still suffers from slow inference and limited condition consistency. In this
work, we present MAISI-v2, the first accelerated 3D medical image synthesis
framework that integrates rectified flow to enable fast and high quality
generation. To further enhance condition fidelity, we introduce a novel
region-specific contrastive loss to enhance the sensitivity to region of
interest. Our experiments show that MAISI-v2 can achieve SOTA image quality
with $33 \times$ acceleration for latent diffusion model. We also conducted a
downstream segmentation experiment to show that the synthetic images can be
used for data augmentation. We release our code, training details, model
weights, and a GUI demo to facilitate reproducibility and promote further
development within the community.

</details>


### [6] [Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks](https://arxiv.org/abs/2508.05783)
*Mengyu Li,Guoyao Shen,Chad W. Farris,Xin Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种结合MAE（Masked Autoencoder）预训练和脑MRI大数据的方法，实现了预训练MRI Transformer在多种脑成像任务上的高效少样本应用。以冻结MAE编码器和线性分类头用于分类任务，并提出了融合CNN的MAE-FUnet架构用于分割任务，均在多项任务上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 医学影像领域使用transformer的潜力很大，但因标注数据稀缺，实际应用受限。本文旨在通过大规模预训练获得具备高泛化性能的表示，以便更好地适应不同的实际脑影像任务，尤其是在标注数据较少的情况下。

Method: 采用MAE预训练策略，在超大规模（>3100万切片）、多队列的脑MRI数据集上对transformer模型进行预训练，获取通用的深层表示。针对高层次分类任务，采用冻结的MAE编码器配合轻量线性分类头进行少样本迁移。针对低层次分割任务，提出MAE-FUnet，融合多尺度CNN特征与预训练MAE嵌入向量，实现强性能的标签稀缺分割。

Result: 实验表明：在MRI序列识别等分类任务中，冻结MAE编码器加线性头能够以极少的监督达到最新最好准确率。在颅骨剥离、多类别解剖分割等低层次任务中，MAE-FUnet在数据有限情况下稳定超过了其它强基线模型。

Conclusion: 该框架在多项定量与定性评测中表现出高效、稳定和可扩展，证明了其在数据资源有限的临床和神经影像场景中的应用潜力。

Abstract: Machine learning using transformers has shown great potential in medical
imaging, but its real-world applicability remains limited due to the scarcity
of annotated data. In this study, we propose a practical framework for the
few-shot deployment of pretrained MRI transformers in diverse brain imaging
tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a
large-scale, multi-cohort brain MRI dataset comprising over 31 million slices,
we obtain highly transferable latent representations that generalize well
across tasks and datasets. For high-level tasks such as classification, a
frozen MAE encoder combined with a lightweight linear head achieves
state-of-the-art accuracy in MRI sequence identification with minimal
supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a
hybrid architecture that fuses multiscale CNN features with pretrained MAE
embeddings. This model consistently outperforms other strong baselines in both
skull stripping and multi-class anatomical segmentation under data-limited
conditions. With extensive quantitative and qualitative evaluations, our
framework demonstrates efficiency, stability, and scalability, suggesting its
suitability for low-resource clinical environments and broader neuroimaging
applications.

</details>


### [7] [Optimization-Free Style Transfer for 3D Gaussian Splats](https://arxiv.org/abs/2508.05813)
*Raphael Du Sablon,David Hart*

Main category: cs.CV

TL;DR: 本文提出了一种无需重建和优化的3D高斯斑点风格迁移方法，大幅提升了泛用性和速度。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯斑点风格迁移的方法大多需要重建或针对风格信息进行微调，这不仅耗时且需要额外的训练或优化，限制了其灵活性与实用性。

Method: 该方法通过在高斯斑点隐式表面上生成图结构，然后应用基于表面的前馈风格化方法，并将结果插值得到单独的斑点风格效果。整个过程无需额外训练和优化，可直接适配任意风格图片和3D高斯斑点。

Result: 实现了无需训练的快速高斯斑点风格迁移，普通消费级硬件上运行时间低于2分钟。实验显示，该方法生成的风格化效果与现有方法相比，具有较高质量。

Conclusion: 本文方法显著提高了3D高斯斑点风格迁移的效率和适用范围，为相关应用提供了更快速便捷的解决方案，代码已开源。

Abstract: The task of style transfer for 3D Gaussian splats has been explored in many
previous works, but these require reconstructing or fine-tuning the splat while
incorporating style information or optimizing a feature extraction network on
the splat representation. We propose a reconstruction- and optimization-free
approach to stylizing 3D Gaussian splats. This is done by generating a graph
structure across the implicit surface of the splat representation. A
feed-forward, surface-based stylization method is then used and interpolated
back to the individual splats in the scene. This allows for any style image and
3D Gaussian splat to be used without any additional training or optimization.
This also allows for fast stylization of splats, achieving speeds under 2
minutes even on consumer-grade hardware. We demonstrate the quality results
this approach achieves and compare to other 3D Gaussian splat style transfer
methods. Code is publicly available at
https://github.com/davidmhart/FastSplatStyler.

</details>


### [8] [MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses](https://arxiv.org/abs/2508.05819)
*Jong-Ik Park,Carlee Joe-Wong,Gary K. Fedder*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的NeRF框架（MZEN），专门用于解决多缩放图像集的三维重建任务，实现了对微米级细节的高精度捕捉，特别适用于工业检测等精细场景。


<details>
  <summary>Details</summary>
Motivation: 传统的NeRF方法虽能从多视角2D图像实现3D重建，即使相机姿态未知，但难以捕捉对工业检测至关重要的细微结构。在传感器分辨率固定且计算预算受限时，常用的高分辨率或多缩放图像会破坏NeRF所依赖的多视角一致性。因此，需要一种能够本地支持多缩放图片，高精度重建细节的NeRF方法。

Method: 提出了Multi-Zoom Enhanced NeRF（MZEN）框架：(1) 在针孔相机模型中引入可学习的缩放因子，以调整焦距以适配不同缩放图像；(2) 首先用宽视场图像确定全局坐标系，再采用zoom-consistent crop-and-match策略，将局部放大图像与最近的宽视场图像配准，最后联合优化以保持多缩放一致性。

Result: 在8个前向场景（含合成模型、真实微结构SEM图像和BLEFF对象）上，MZEN的性能显著超过无姿态基线和高分辨率变体：PSNR提升最高可达28%，SSIM提升10%，LPIPS降低最高达222%。

Conclusion: MZEN成功将NeRF扩展到工业实际场景，在保持全局精度的同时，可捕捉至关重要的微米级细节，为工业检测等应用提供了有力工具。

Abstract: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from
multiple 2D images, even those taken with unknown camera poses. However, they
still miss the fine-detailed structures that matter in industrial inspection,
e.g., detecting sub-micron defects on a production line or analyzing chips with
Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution
is fixed and compute budgets are tight, so the only way to expose fine
structure is to add zoom-in images; yet, this breaks the multi-view consistency
that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF
(MZEN), the first NeRF framework that natively handles multi-zoom image sets.
MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom
scalar that scales the focal length, and (ii) introduces a novel pose strategy:
wide-field images are solved first to establish a global metric frame, and
zoom-in images are then pose-primed to the nearest wide-field counterpart via a
zoom-consistent crop-and-match procedure before joint refinement. Across eight
forward-facing scenes$\unicode{x2013}$synthetic TCAD models, real SEM of
micro-structures, and BLEFF objects$\unicode{x2013}$MZEN consistently
outperforms pose-free baselines and even high-resolution variants, boosting
PSNR by up to $28 \%$, SSIM by $10 \%$, and reducing LPIPS by up to $222 \%$.
MZEN, therefore, extends NeRF to real-world factory settings, preserving global
accuracy while capturing the micron-level details essential for industrial
inspection.

</details>


### [9] [TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios](https://arxiv.org/abs/2508.05829)
*Guoping Xu,Hua-Chieh Shao,You Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频目标分割与追踪方法（TSMS-SAM2），通过多时间尺度采样和内存分割机制提升了SAM2模型在外科手术视频中的表现，取得了现有最高的分割准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型如SAM2在视频目标分割和追踪领域有显著进展，但其在复杂的外科手术视频中仍面临目标运动剧烈、冗余记忆影响学习效果等挑战，有必要针对这些难题进行特定优化。

Method: 提出TSMS-SAM2框架，包含两大关键策略：1）多时间尺度采样增强（multi-temporal-scale video sampling augmentation），增强模型对运动变化的鲁棒性；2）内存分割与裁剪机制（memory splitting and pruning），对历史帧特征进行高效整理和筛选，从而提升分割准确率和效率。

Result: 在EndoVis2017和EndoVis2018数据集上，TSMS-SAM2取得了分别为95.24和86.73的最高mean Dice分数，超过了现有基于SAM及定制方法。消融实验验证了多尺度时间增强及内存分割的有效性。

Conclusion: TSMS-SAM2极大提升了外科视频中目标分割与追踪的稳健性和效率，尤其适用于运动复杂的手术场景，可为临床应用带来更高的分割准确率和鲁棒性。

Abstract: Promptable video object segmentation and tracking (VOST) has seen significant
advances with the emergence of foundation models like Segment Anything Model 2
(SAM2); however, their application in surgical video analysis remains
challenging due to complex motion dynamics and the redundancy of memory that
impedes effective learning. In this work, we propose TSMS-SAM2, a novel
framework that enhances promptable VOST in surgical videos by addressing
challenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2
introduces two key strategies: multi-temporal-scale video sampling augmentation
to improve robustness against motion variability, and a memory splitting and
pruning mechanism that organizes and filters past frame features for more
efficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018
datasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,
respectively, outperforming prior SAM-based and task-specific methods.
Extensive ablation studies confirm the effectiveness of multiscale temporal
augmentation and memory splitting, highlighting the framework's potential for
robust, efficient segmentation in complex surgical scenarios. Our source code
will be available at https://github.com/apple1986/TSMS-SAM2.

</details>


### [10] [Temporal Cluster Assignment for Efficient Real-Time Video Segmentation](https://arxiv.org/abs/2508.05851)
*Ka-Wai Yung,Felix J. S. Bragman,Jialang Xu,Imanol Luengo,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: 本文提出了一种名为TCA的轻量级token聚类方法，通过利用视频帧间的时序一致性，有效降低了Vision Transformer分割模型的计算量，同时显著提升了分割精度与速度的折中表现。


<details>
  <summary>Details</summary>
Motivation: Swin Transformer虽然能捕捉多尺度信息，但在视频密集分割任务中计算量大，难以满足实时、资源受限场景需求。现有token精简和聚类方法要么无法适配窗口注意力机制，要么没充分利用视频的时序冗余。

Method: 作者提出了Temporal Cluster Assignment（TCA）方法，在保持窗口一致性的同时，结合帧间时序相关性对token聚类结果进行细化，不直接丢弃冗余token，而是用时序信息优化聚类，无需重新训练模型。

Result: 在YouTube-VIS 2019、YouTube-VIS 2021、OVIS及手术视频数据集上，大量实验证明TCA与现有聚类方法结合时，大幅提升视频分割模型的精度-速度表现，可以广泛泛化于自然及专业领域视频。

Conclusion: TCA是一种无需微调、能有效结合视频时序信息的token聚类方法，显著提升了Transformer视频分割的效率和性能，对密集分割和实时应用具有重要意义。

Abstract: Vision Transformers have substantially advanced the capabilities of
segmentation models across both image and video domains. Among them, the Swin
Transformer stands out for its ability to capture hierarchical, multi-scale
representations, making it a popular backbone for segmentation in videos.
However, despite its window-attention scheme, it still incurs a high
computational cost, especially in larger variants commonly used for dense
prediction in videos. This remains a major bottleneck for real-time,
resource-constrained applications. Whilst token reduction methods have been
proposed to alleviate this, the window-based attention mechanism of Swin
requires a fixed number of tokens per window, limiting the applicability of
conventional pruning techniques. Meanwhile, training-free token clustering
approaches have shown promise in image segmentation while maintaining window
consistency. Nevertheless, they fail to exploit temporal redundancy, missing a
key opportunity to further optimize video segmentation performance. We
introduce Temporal Cluster Assignment (TCA), a lightweight and effective,
fine-tuning-free strategy that enhances token clustering by leveraging temporal
coherence across frames. Instead of indiscriminately dropping redundant tokens,
TCA refines token clusters using temporal correlations, thereby retaining
fine-grained details while significantly reducing computation. Extensive
evaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical
video dataset show that TCA consistently boosts the accuracy-speed trade-off of
existing clustering-based methods. Our results demonstrate that TCA generalizes
competently across both natural and domain-specific videos.

</details>


### [11] [VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments](https://arxiv.org/abs/2508.05852)
*Kaiser Hamid,Khandakar Ashrafi Akbar,Nade Liang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合视觉和自然语言的框架，用于预测驾驶员视觉注意力，并能用自然语言描述注意力的分布和变化。其方法在单RGB图像上进行少样本与零样本学习，取得了优于通用视觉-语言模型的效果。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶员视觉注意力预测多依赖静态图像，且仅做瞬时预测，缺乏解释性，难以捕捉注意力转移和复杂场景语义。需要更具解释性和泛化能力的方法。

Method: 本方法基于BDD-A数据集，人工筛选并优化注释文本，通过人机协作获得高质量文本描述，然后微调LLaVA模型，将视觉感知与基于注意力的场景理解对齐。方法整合了底层线索与高层语义信息，支持用自然语言描述注意力分布。通过少样本和单样本训练，采用领域特定指标评估结果的语义对齐与多样性。

Result: 微调后的模型在驾驶员注意力转移检测和可解释性方面，均优于通用视觉-语言模型，实现了更准确和多样化的自然语言注意力描述。

Conclusion: 该研究首次实现了用自然语言生成驾驶员视觉注意力分布和转移，提升了模型可解释性，为自动驾驶中的行为预测、人机协作等任务奠定了基础，推动了可解释AI应用的进一步发展。

Abstract: Driver visual attention prediction is a critical task in autonomous driving
and human-computer interaction (HCI) research. Most prior studies focus on
estimating attention allocation at a single moment in time, typically using
static RGB images such as driving scene pictures. In this work, we propose a
vision-language framework that models the changing landscape of drivers' gaze
through natural language, using few-shot and zero-shot learning on single RGB
images. We curate and refine high-quality captions from the BDD-A dataset using
human-in-the-loop feedback, then fine-tune LLaVA to align visual perception
with attention-centric scene understanding. Our approach integrates both
low-level cues and top-down context (e.g., route semantics, risk anticipation),
enabling language-based descriptions of gaze behavior. We evaluate performance
across training regimes (few shot, and one-shot) and introduce domain-specific
metrics for semantic alignment and response diversity. Results show that our
fine-tuned model outperforms general-purpose VLMs in attention shift detection
and interpretability. To our knowledge, this is among the first attempts to
generate driver visual attention allocation and shifting predictions in natural
language, offering a new direction for explainable AI in autonomous driving.
Our approach provides a foundation for downstream tasks such as behavior
forecasting, human-AI teaming, and multi-agent coordination.

</details>


### [12] [Multi-view Gaze Target Estimation](https://arxiv.org/abs/2508.05857)
*Qiaomu Miao,Vivek Raju Golani,Jingyi Xu,Progga Paromita Dutta,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: 本文提出了一种利用多摄像头视角进行注视目标估计（GTE）的方法，通过融合不同相机视角的信息提高准确性，解决单视角方法在面部遮挡、目标模糊和视野外目标等方面的不足。方法显著优于单视角基线，并发布了新的多视角数据集。


<details>
  <summary>Details</summary>
Motivation: 现有单视角方法在面部被遮挡、视线目标模糊或不在视野内时效果有限。因此需要多视角系统融合信息以提升鲁棒性和应用范围。

Method: 该方法以两个相机视角的图像对作为输入，提出了三大模块：1）头部信息聚合（HIA）模块，整合两视角头部信息提升注视估计精度；2）基于不确定性的注视选择（UGS）模块，选出最可靠的注视输出；3）基于极线几何的场景注意力（ESA）模块，实现跨视角背景信息共享。

Result: 新方法在精度上显著优于单视角基线模型，特别是在第二视角能清晰看到面部时效果更佳。此外，该法支持仅用第二视角图像估计第一视角下的注视目标，这是单视角方法无法实现的。

Conclusion: 多视角融合方法有效克服单视角局限，提升GTE任务表现，为后续多视角数据集与算法研究奠定基础，并开放代码与数据集推动发展。

Abstract: This paper presents a method that utilizes multiple camera views for the gaze
target estimation (GTE) task. The approach integrates information from
different camera views to improve accuracy and expand applicability, addressing
limitations in existing single-view methods that face challenges such as face
occlusion, target ambiguity, and out-of-view targets. Our method processes a
pair of camera views as input, incorporating a Head Information Aggregation
(HIA) module for leveraging head information from both views for more accurate
gaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the
most reliable gaze output, and an Epipolar-based Scene Attention (ESA) module
for cross-view background information sharing. This approach significantly
outperforms single-view baselines, especially when the second camera provides a
clear view of the person's face. Additionally, our method can estimate the gaze
target in the first view using the image of the person in the second view only,
a capability not possessed by single-view GTE methods. Furthermore, the paper
introduces a multi-view dataset for developing and evaluating multi-view GTE
methods. Data and code are available at
https://www3.cs.stonybrook.edu/~cvl/multiview_gte.html

</details>


### [13] [ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates](https://arxiv.org/abs/2508.05898)
*Hamidreza Dastmalchi,Aijun An,Ali cheraghian*

Main category: cs.CV

TL;DR: 本文提出了高效测试时自适应方法（ETTA），通过递归更新模块和自适应集成模块优化VLMs在分布变化下的泛化能力，从而提升零样本测试准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有预训练视觉-语言模型（如CLIP）在面对分布变化的测试数据时泛化能力较弱。虽然有部分测试时自适应（TTA）方法能改善泛化性能，但训练自由且高效的cache-based方法仅依赖于有限高置信度样本，决策能力受限，无法充分利用所有测试数据。解决这一局限，提升模型适应性和效率，是本研究的动机。

Method: ETTA方法包含两个核心创新模块：1）递归更新模块，能将新到的所有测试样本持续整合到cache中，动态细化判别边界，具备无界cache的效果且内存与计算开销极低；2）自适应集成模块，通过为每个类别动态选择最优prompt，降低了对手动prompt的依赖，并根据置信度自适应融合两模块得分。

Result: 在两个基准数据集上，ETTA方法在计算复杂度和准确率上均优于当前最先进的TTA方法，实现了新的性能基线。

Conclusion: ETTA显著提升了VLMs面对分布变化时的自适应能力和泛化性能，在保证效率的同时实现更优准确率。其开源代码推动了测试时自适应技术的进一步发展。

Abstract: Pretrained vision-language models (VLMs) like CLIP show strong zero-shot
performance but struggle with generalization under distribution shifts.
Test-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test
data in new domains. While some TTA methods rely on prompt-tuning,
training-free cache-based approaches are preferred for efficiency. However,
current cache-based TTA models store only a limited set of high-confidence
samples, restricting the decision boundary to these samples and ignoring the
influence of other incoming test data. To address this, we propose Efficient
Test-Time Adaptation (ETTA), introducing a Recursive Updating module that
integrates all incoming test samples, progressively refining the decision
boundary. This strategy mimics an unbounded cache, dynamically updating
contextual embeddings for improved accuracy with minimal memory and
computational overhead. ETTA also includes an Adaptive Ensemble module to
reduce prompt dependency in image-to-text scores by dynamically selecting
optimal prompts for each class. Furthermore, ETTA adaptively combines scores
from both modules based on confidence levels, leveraging their complementary
strengths. Extensive experiments on two benchmarks confirm that ETTA surpasses
the state-of-the-art TTA models in computational complexity and accuracy,
setting a new standard for effective, efficient test-time adaptation. The code
has been released at https://github.com/hamidreza-dastmalchi/ETTA.

</details>


### [14] [HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing](https://arxiv.org/abs/2508.05899)
*Zixuan Bian,Ruohan Ren,Yue Yang,Chris Callison-Burch*

Main category: cs.CV

TL;DR: 本文提出了HOLODECK 2.0框架，实现了基于文本描述的高质量3D场景生成，并支持依赖人类反馈的场景交互式编辑。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景设计过于依赖人工，自动化方法面对开放域和灵活编辑存在局限，因此亟需高效、灵活的3D世界自动生成与编辑方案。

Method: HOLODECK 2.0利用视觉-语言模型（VLM）解析文本需求，调用先进的3D生成模型生成高质量资产，然后迭代应用VLM推断出的空间约束，实现语义连贯和物理合理的场景布局，并支持基于人类反馈的编辑。

Result: HOLODECK 2.0可生成多样风格、高语义保真度的3D场景，并在人工和CLIP评测中，场景质量和文本一致性方面优于现有方法，还支持场景灵活编辑。

Conclusion: HOLODECK 2.0有效提升了3D场景自动生成和交互式编辑能力，在游戏建模等实际应用中展现出提升效率和视觉丰富性的潜力。

Abstract: 3D scene generation plays a crucial role in gaming, artistic creation,
virtual reality and many other domains. However, current 3D scene design still
relies heavily on extensive manual effort from creators, and existing automated
methods struggle to generate open-domain scenes or support flexible editing. As
a result, generating 3D worlds directly from text has garnered increasing
attention. In this paper, we introduce HOLODECK 2.0, an advanced
vision-language-guided framework for 3D world generation with support for
interactive scene editing based on human feedback. HOLODECK 2.0 can generate
diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and
cyberpunk styles) that exhibit high semantic fidelity to fine-grained input
descriptions, suitable for both indoor and open-domain environments. HOLODECK
2.0 leverages vision-language models (VLMs) to identify and parse the objects
required in a scene and generates corresponding high-quality assets via
state-of-the-art 3D generative models. It then iteratively applies spatial
constraints derived from the VLMs to achieve semantically coherent and
physically plausible layouts. Human evaluations and CLIP-based assessments
demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely
aligned with detailed textual descriptions, consistently outperforming
baselines across indoor and open-domain scenarios. Additionally, we provide
editing capabilities that flexibly adapt to human feedback, supporting layout
refinement and style-consistent object edits. Finally, we present a practical
application of HOLODECK 2.0 in procedural game modeling, generating visually
rich and immersive environments, potentially boosting efficiency.

</details>


### [15] [Robust Image Stitching with Optimal Plane](https://arxiv.org/abs/2508.05903)
*Lang Nie,Yuan Mei,Kang Liao,Yunqiu Xu,Chunyu Lin,Bin Xiao*

Main category: cs.CV

TL;DR: RopStitch是一种无需监督的深度图像拼接新框架，兼具鲁棒性和自然性，并在多个数据集上优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像拼接方法在面对多样化真实场景时鲁棒性和结构自然性往往不足，因此作者希望提出一个能同时提升两个方面的新型拼接模型。

Method: 提出了双分支结构：一个预训练分支用于提取语义不变特征，另一个可学习分支提取细粒度特征，融合两者以增强泛化能力。此外，创造“虚拟最优平面”概念，通过同伦分解和语义失真约束，迭代优化找到内容对齐与结构保存的最优平衡，并将各视图双向投影到最优平面完成拼接。

Result: 在多个数据集上实验证明，RopStitch在场景鲁棒性和视觉内容自然性方面均明显优于现有主流拼接方法。

Conclusion: RopStitch克服了内容对齐与结构保持之间的矛盾，实现了高鲁棒性与高自然性的图像拼接，为无人监督场景下的高级拼接开辟了新方向。

Abstract: We present \textit{RopStitch}, an unsupervised deep image stitching framework
with both robustness and naturalness. To ensure the robustness of
\textit{RopStitch}, we propose to incorporate the universal prior of content
perception into the image stitching model by a dual-branch architecture. It
separately captures coarse and fine features and integrates them to achieve
highly generalizable performance across diverse unseen real-world scenes.
Concretely, the dual-branch model consists of a pretrained branch to capture
semantically invariant representations and a learnable branch to extract
fine-grained discriminative features, which are then merged into a whole by a
controllable factor at the correlation level. Besides, considering that content
alignment and structural preservation are often contradictory to each other, we
propose a concept of virtual optimal planes to relieve this conflict. To this
end, we model this problem as a process of estimating homography decomposition
coefficients, and design an iterative coefficient predictor and minimal
semantic distortion constraint to identify the optimal plane. This scheme is
finally incorporated into \textit{RopStitch} by warping both views onto the
optimal plane bidirectionally. Extensive experiments across various datasets
demonstrate that \textit{RopStitch} significantly outperforms existing methods,
particularly in scene robustness and content naturalness. The code is available
at {\color{red}https://github.com/MmelodYy/RopStitch}.

</details>


### [16] [Neural Field Representations of Mobile Computational Photography](https://arxiv.org/abs/2508.05907)
*Ilya Chugunov*

Main category: cs.CV

TL;DR: 本论文提出利用神经场模型直接对手机采集的原始影像数据进行处理，完成复杂的深度估计、图层分离和图像拼接任务，且效果超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统移动影像处理通常依赖大量的预处理、标注数据或机器学习先验，这些方法难以直接适用于新颖的、复杂的场景。作者希望打破这些限制，实现更灵活、高效的数据处理框架。

Method: 作者设计了一种基于神经场的小型神经网络模型，无需显式的数据表示（如像素阵列或点云）。该模型通过随机梯度下降，直接拟合来自智能手机的原始观测数据，实现自正则化，对复杂的几何与光照进行压缩建模。

Result: 该神经场方法在深度估计、图层分离及图像拼接等应用上优于当前主流方法，无需复杂的预处理步骤或标注真值数据。

Conclusion: 本论文证明了面向移动成像的新型神经场模型能够以简洁方式解决复杂的逆问题，为手机领域的计算成像技术带来更高性能和应用灵活性。

Abstract: Over the past two decades, mobile imaging has experienced a profound
transformation, with cell phones rapidly eclipsing all other forms of digital
photography in popularity. Today's cell phones are equipped with a diverse
range of imaging technologies - laser depth ranging, multi-focal camera arrays,
and split-pixel sensors - alongside non-visual sensors such as gyroscopes,
accelerometers, and magnetometers. This, combined with on-board integrated
chips for image and signal processing, makes the cell phone a versatile
pocket-sized computational imaging platform. Parallel to this, we have seen in
recent years how neural fields - small neural networks trained to map
continuous spatial input coordinates to output signals - enable the
reconstruction of complex scenes without explicit data representations such as
pixel arrays or point clouds. In this thesis, I demonstrate how carefully
designed neural field models can compactly represent complex geometry and
lighting effects. Enabling applications such as depth estimation, layer
separation, and image stitching directly from collected in-the-wild mobile
photography data. These methods outperform state-of-the-art approaches without
relying on complex pre-processing steps, labeled ground truth data, or machine
learning priors. Instead, they leverage well-constructed, self-regularized
models that tackle challenging inverse problems through stochastic gradient
descent, fitting directly to raw measurements from a smartphone.

</details>


### [17] [Enhancing Construction Site Analysis and Understanding with 3D Segmentation](https://arxiv.org/abs/2508.05922)
*Sri Ramana Saketh Vasanthawada,Pengkun Liu,Pingbo Tang*

Main category: cs.CV

TL;DR: 本文探讨并对比了两种先进的3D分割方法（SAM与Mask3D）在复杂室外及室内施工场景下的适应性和性能，揭示了目前分割方法在户外场景中的局限，并提出需要针对施工场景定制分割工作流，以推动自动化监测技术的发展。


<details>
  <summary>Details</summary>
Motivation: 传统进度监控方法资源消耗大，且主要集中在室内环境，难以应对施工现场复杂与动态变化的情况。因此，需探索更高效、可扩展、且适合施工现场条件的自动化方法。

Method: 对Segment Anything Model (SAM) 和 Mask3D 两种3D分割模型，在真实施工场景下进行实验与比较分析，评估其在室外和室内环境中的适应性和分割性能。

Result: 两种模型在由室内数据集训练后，均面临在户外复杂场景下适应性不足的问题，表明现有分割方法对施工现场的适用性有限，且目前缺乏户外场景的相关基准数据。

Conclusion: 研究展示了SAM和Mask3D的相对效果，并指出当前分割技术在施工监测自动化中的不足，强调需开发针对施工场景的数据和分割流程，以提升监测自动化与精度。

Abstract: Monitoring construction progress is crucial yet resource-intensive, prompting
the exploration of computer-vision-based methodologies for enhanced efficiency
and scalability. Traditional data acquisition methods, primarily focusing on
indoor environments, falter in construction site's complex, cluttered, and
dynamically changing conditions. This paper critically evaluates the
application of two advanced 3D segmentation methods, Segment Anything Model
(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained
initially on indoor datasets, both models' adaptability and performance are
assessed in real-world construction settings, highlighting the gap in current
segmentation approaches due to the absence of benchmarks for outdoor scenarios.
Through a comparative analysis, this study not only showcases the relative
effectiveness of SAM and Mask3D but also addresses the critical need for
tailored segmentation workflows capable of extracting actionable insights from
construction site data, thereby advancing the field towards more automated and
precise monitoring techniques.

</details>


### [18] [A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image](https://arxiv.org/abs/2508.05950)
*Yanxing Liang,Yinghui Wang,Jinlong Yang,Wei Li*

Main category: cs.CV

TL;DR: 提出了一种结合物理建模与可微渲染的新型扩散网络，在无监督的情况下从单张图像估算高一致性的法线，提高了2D到3D估算的准确性，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有单图像法线估算依赖扩散模型，但容易出现多视角法线冲突，并且需要大量带标签的法线数据，原因是无法将3D几何误差有效反馈到法线生成功能。

Method: 提出SINGAD框架，结合基于光照-表面相互作用的3D高斯分布（3DGS）重参数化和可微渲染投影策略，将3D几何误差转化为法线的优化信号；设计跨域特征融合模块注入几何先验，引入3D可微投影损失实现无监督优化，无需法线标注数据。

Result: 在Google Scanned Objects数据集上的定量实验表明，该方法在多个评价指标上均优于最新方法。

Conclusion: 通过结合物理驱动的几何建模与自监督可微渲染，有效解决了多视角法线冲突和数据依赖问题，实现了高精度的单图像法线估算。

Abstract: The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.

</details>


### [19] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

TL;DR: 本文提出了Bifrost-1，一种高效融合多模态大模型（MLLMs）与扩散模型、具备高质量可控图像生成和强多模态推理能力的新框架。


<details>
  <summary>Details</summary>
Motivation: 当前方法在将高保真视觉合成能力整合进大语言模型时，因LLM在预训练阶段未见过图像表示，直接训练或桥接LLM与扩散模型通常代价高昂。作者希望找到更高效且不损失推理能力的集成方案。

Method: Bifrost-1通过引入patch级别CLIP图像嵌入作为潜变量，将预训练的多模态大模型（MLLMs）与扩散模型有效衔接。这些嵌入原生对齐MLLM中的CLIP视觉编码器，通过轻量化适配的ControlNet引入扩散模型。与此同时，MLLM增加一个视觉生成分支，初始参数源自原MLLM参数，以在预测patch级嵌入时保留原有多模态推理能力。

Result: Bifrost-1在图像保真度和多模态理解能力方面，与以往方法表现相当甚至更优，同时训练计算资源消耗大幅降低。

Conclusion: 该框架实现了多模态大模型与扩散模型的高效融合，实现高保真可控图像生成且保留强大推理能力，设计选择经过消融实验验证。

Abstract: There is growing interest in integrating high-fidelity visual synthesis
capabilities into large language models (LLMs) without compromising their
strong reasoning capabilities. Existing methods that directly train LLMs or
bridge LLMs and diffusion models usually suffer from costly training since the
backbone LLMs have not seen image representations during pretraining. We
present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs
(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent
variables, which are natively aligned with the MLLM's CLIP visual encoder.
These patch-level image embeddings are integrated into the diffusion model with
a lightweight adaptation of its ControlNet. To retain the original multimodal
reasoning capabilities of MLLMs, we equip the MLLM with a visual generation
branch initialized from the original MLLM parameters when predicting the
patch-level image embeddings. By seamlessly integrating pretrained MLLMs and
diffusion models with patch-level CLIP latents, our framework enables
high-fidelity controllable image generation with significant training
efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or
better performance than previous methods in terms of visual fidelity and
multimodal understanding, with substantially lower compute during training. We
also provide comprehensive ablation studies showing the effectiveness of our
design choices.

</details>


### [20] [PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation](https://arxiv.org/abs/2508.05976)
*Zhihao Zhu,Yifan Zheng,Siyu Pan,Yaohui Jin,Yao Mu*

Main category: cs.CV

TL;DR: 提出PASG闭环框架，实现机器人操作中几何特征与任务语义的融合，提高语义-功能理解和任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作领域，任务语义和底层几何特征割裂，影响了对动态语义-功能关系的掌握。主流视觉-语言模型依赖人工标注，缺乏空间语义落地，难以泛化。作者希望在不依赖人工大量标注的前提下，实现几何与语义的有机结合。

Method: 提出Primitive-Aware Semantic Grounding（PASG）闭环框架，包括：（1）通过几何特征聚合自动提取关键点和轴，实现跨类别的几何元件检测；（2）利用VLM驱动的动态语义锚定，实现几何元件与功能性和任务描述的自动关联；（3）构建空间-语义推理基准和微调的视觉-语言模型Qwen2.5VL-PA，用于评估和实现上述目标。

Result: 在多样化实际机器人操作任务中，PASG的表现达到人工标注方案的水平，实现了更细粒度的对象语义-功能理解。

Conclusion: PASG有效桥接了几何特征与任务语义，为机器人操作领域提供了统一的空间-语义范式，增强了系统的泛化能力和自动化水平。

Abstract: The fragmentation between high-level task semantics and low-level geometric
features remains a persistent challenge in robotic manipulation. While
vision-language models (VLMs) have shown promise in generating affordance-aware
visual representations, the lack of semantic grounding in canonical spaces and
reliance on manual annotations severely limit their ability to capture dynamic
semantic-affordance relationships. To address these, we propose Primitive-Aware
Semantic Grounding (PASG), a closed-loop framework that introduces: (1)
Automatic primitive extraction through geometric feature aggregation, enabling
cross-category detection of keypoints and axes; (2) VLM-driven semantic
anchoring that dynamically couples geometric primitives with functional
affordances and task-relevant description; (3) A spatial-semantic reasoning
benchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's
effectiveness in practical robotic manipulation tasks across diverse scenarios,
achieving performance comparable to manual annotations. PASG achieves a
finer-grained semantic-affordance understanding of objects, establishing a
unified paradigm for bridging geometric primitives with task semantics in
robotic manipulation.

</details>


### [21] [AnimateScene: Camera-controllable Animation in Any Scene](https://arxiv.org/abs/2508.05982)
*Qingyang Liu,Bingjie Gao,Weiheng Huang,Jun Zhang,Zhongqian Sun,Yang Wei,Zelin Peng,Qianli Ma,Shuai Yang,Zhaohe Liao,Haonan Zhao,Li Niu*

Main category: cs.CV

TL;DR: 该论文提出了AnimateScene框架，实现了高质量的4D人物动画与3D场景的无缝集成，通过自动化位置与风格对齐以及摄像机运动优化，生成动态细致且空间时间一致的视频。


<details>
  <summary>Details</summary>
Motivation: 近年来3D场景重建与4D人物动画取得进展，但将二者无缝集成仍面临定位、尺度、穿插错误及风格不统一等难题，限制了沉浸式虚拟内容生成的发展。

Method: 1. 提出精确的人物定位模块，自动为人物确定合理3D位置并防止穿插异常；2. 提出无训练的风格对齐方法，使4D人物动画与背景的光照及风格一致；3. 设计联合后处理流程，实现场景与人物的摄像机轨迹优化，生成包含运动摄像机的高质量视频。

Result: AnimateScene能够在多种摄像机轨迹和人物动作组合下，生成空间细节丰富且时空一致的动态场景视频。实验表明视觉一致性和真实性明显提升。

Conclusion: AnimateScene为4D人物与3D场景动态集成提供了一体化方案，显著提升了最终渲染视频的真实感和视觉吸引力，对虚拟现实、影视制作等领域具有重要意义。

Abstract: 3D scene reconstruction and 4D human animation have seen rapid progress and
broad adoption in recent years. However, seamlessly integrating reconstructed
scenes with 4D human animation to produce visually engaging results remains
challenging. One key difficulty lies in placing the human at the correct
location and scale within the scene while avoiding unrealistic
interpenetration. Another challenge is that the human and the background may
exhibit different lighting and style, leading to unrealistic composites. In
addition, appealing character motion videos are often accompanied by camera
movements, which means that the viewpoints need to be reconstructed along a
specified trajectory. We present AnimateScene, which addresses the above issues
in a unified framework. First, we design an accurate placement module that
automatically determines a plausible 3D position for the human and prevents any
interpenetration within the scene during motion. Second, we propose a
training-free style alignment method that adapts the 4D human representation to
match the background's lighting and style, achieving coherent visual
integration. Finally, we design a joint post-reconstruction method for both the
4D human and the 3D scene that allows camera trajectories to be inserted,
enabling the final rendered video to feature visually appealing camera
movements. Extensive experiments show that AnimateScene generates dynamic scene
videos with high geometric detail and spatiotemporal coherence across various
camera and action combinations.

</details>


### [22] [ETA: Energy-based Test-time Adaptation for Depth Completion](https://arxiv.org/abs/2508.05989)
*Younjoon Chung,Hyoungseob Park,Patrick Rim,Xiaoran Zhang,Jihe He,Ziyao Zeng,Safa Cicek,Byung-Woo Hong,James S. Duncan,Alex Wong*

Main category: cs.CV

TL;DR: 本文提出了一种用于深度补全模型的测试时自适应方法——ETA（Energy-based Test-time Adaptation），以应对源数据和目标环境间的分布转移问题。


<details>
  <summary>Details</summary>
Motivation: 深度补全模型在新环境下往往因分布转移而预测不准。实际部署前无法获取目标域数据，急需一种无需目标域先验知识的方法提升模型泛化能力。

Method: 作者利用对抗扰动探索数据空间，训练能量模型判别深度预测属于源域还是异常分布。推理时，更新预训练模型参数以最小化能量，有效对齐测试时结果与源域分布。

Result: 在三个室内及三个室外数据集上，ETA方法相较最新方法提升准确率：室外平均提升6.94%，室内平均提升10.23%。

Conclusion: ETA无需假设目标域，可在测试时自适应提升深度补全模型的泛化表现，在多种实际环境中均表现优越。

Abstract: We propose a method for test-time adaptation of pretrained depth completion
models. Depth completion models, trained on some ``source'' data, often predict
erroneous outputs when transferred to ``target'' data captured in novel
environmental conditions due to a covariate shift. The crux of our method lies
in quantifying the likelihood of depth predictions belonging to the source data
distribution. The challenge is in the lack of access to out-of-distribution
(target) data prior to deployment. Hence, rather than making assumptions
regarding the target distribution, we utilize adversarial perturbations as a
mechanism to explore the data space. This enables us to train an energy model
that scores local regions of depth predictions as in- or out-of-distribution.
We update the parameters of pretrained depth completion models at test time to
minimize energy, effectively aligning test-time predictions to those of the
source distribution. We call our method ``Energy-based Test-time Adaptation'',
or ETA for short. We evaluate our method across three indoor and three outdoor
datasets, where ETA improve over the previous state-of-the-art method by an
average of 6.94% for outdoors and 10.23% for indoors. Project Page:
https://fuzzythecat.github.io/eta.

</details>


### [23] [Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision](https://arxiv.org/abs/2508.05990)
*Haichao Wang,Xinyue Xi,Jiangtao Wen,Yuxing Han*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视频计算机视觉系统，通过减少前端计算和时序冗余，实现了显著加速且性能损失较小。


<details>
  <summary>Details</summary>
Motivation: 现有视频计算机视觉系统存在高时序冗余和前端处理计算开销大两个主要问题，亟需一种方法以提升处理效率。

Method: 1）移除图像信号处理器（ISP），直接用Bayer格式数据输入模型以减少前端计算；2）提出基于快速块匹配的运动估计算法，并引入运动矢量精细化模块；3）设计上下文感知块细化网络以修正大误差区域；4）加入帧选择策略以进一步平衡精度和效率。

Result: 在多个视频视觉任务实验中，新系统实现了明显的加速，仅有轻微性能损失。

Conclusion: 新方案可有效减少时序冗余和前端计算，兼顾准确率与效率，在实际应用中具备良好前景。

Abstract: The efficiency of video computer vision system remains a challenging task due
to the high temporal redundancy inside a video. Existing works have been
proposed for efficient vision computer vision. However, they do not fully
reduce the temporal redundancy and neglect the front end computation overhead.
In this paper, we propose an efficient video computer vision system. First,
image signal processor is removed and Bayer-format data is directly fed into
video computer vision models, thus saving the front end computation. Second,
instead of optical flow models and video codecs, a fast block matching-based
motion estimation algorithm is proposed specifically for efficient video
computer vision, with a MV refinement module. To correct the error,
context-aware block refinement network is introduced to refine regions with
large error. To further balance the accuracy and efficiency, a frame selection
strategy is employed. Experiments on multiple video computer vision tasks
demonstrate that our method achieves significant acceleration with slight
performance loss.

</details>


### [24] [ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge](https://arxiv.org/abs/2508.05991)
*Juewen Hu,Yexin Li,Jiulin Li,Shuo Chen,Pring Wong*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的多模态情感识别框架，通过整合视觉、音频和文本特征，有效提升了人机交互中的情感识别效果。该方法在MER2025-SEMI数据集上的加权F分数显著优于官方基线。


<details>
  <summary>Details</summary>
Motivation: 情感识别对于人机交互至关重要，但现有方法受限于数据稀缺和多模态特征融合效率低等问题，亟需更有效的特征提取和融合策略。

Method: 本文采用大规模预训练模型提取视觉、音频和文本特征：视觉方面引入双分支视觉编码器提取全局与局部特征；文本方面利用大语言模型丰富情感线索。多模态特征通过包含自注意力机制和残差连接的融合策略实现动态权重分配与信息保留。此外，提出多源标注策略，提升训练集标签质量。

Result: 该方法在MER2025-SEMI数据集上的加权F分数达到87.49%，大幅超越官方基线的78.63%。

Conclusion: 提出的多模态情感识别框架在特征提取、融合及噪声标签处理方面均有创新，实验结果显著优于现有基线，验证了方法的有效性。

Abstract: Emotion recognition plays a vital role in enhancing human-computer
interaction. In this study, we tackle the MER-SEMI challenge of the MER2025
competition by proposing a novel multimodal emotion recognition framework. To
address the issue of data scarcity, we leverage large-scale pre-trained models
to extract informative features from visual, audio, and textual modalities.
Specifically, for the visual modality, we design a dual-branch visual encoder
that captures both global frame-level features and localized facial
representations. For the textual modality, we introduce a context-enriched
method that employs large language models to enrich emotional cues within the
input text. To effectively integrate these multimodal features, we propose a
fusion strategy comprising two key components, i.e., self-attention mechanisms
for dynamic modality weighting, and residual connections to preserve original
representations. Beyond architectural design, we further refine noisy labels in
the training set by a multi-source labeling strategy. Our approach achieves a
substantial performance improvement over the official baseline on the
MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to
78.63%, thereby validating the effectiveness of the proposed framework.

</details>


### [25] [EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad](https://arxiv.org/abs/2508.05994)
*Huadong Wu,Yi Fu,Yunhao Li,Yuan Gao,Kang Du*

Main category: cs.CV

TL;DR: 本文提出了一种新的高质量化妆编辑数据集MakeupQuad，并基于此提出了EvoMakeup训练框架，实现了高保真、可控、支持多任务的面部化妆编辑，显著提升了化妆效果和身份保持能力。


<details>
  <summary>Details</summary>
Motivation: 现有面部化妆编辑方法由于缺乏结构化成对数据，往往无法同时保持身份信息与真实化妆细节，导致编辑结果质量较低。

Method: 作者构建了MakeupQuad大规模高质量数据集，包含无妆脸、参考脸、编辑结果及化妆文本，并提出了EvoMakeup统一训练框架，通过多阶段蒸馏缓解图像退化，实现数据与模型的迭代提升。

Result: EvoMakeup仅用合成数据训练但在真实世界基准上优于现有方法，能够在单一模型下支持高保真、可控的多任务（全脸/部分参考编辑、文本引导编辑）化妆。

Conclusion: EvoMakeup方法显著提升了化妆真实性及身份保持能力，达到了业界领先水平，有力推动了化妆编辑任务发展。代码与数据将在论文接收后公布。

Abstract: Facial makeup editing aims to realistically transfer makeup from a reference
to a target face. Existing methods often produce low-quality results with
coarse makeup details and struggle to preserve both identity and makeup
fidelity, mainly due to the lack of structured paired data -- where source and
result share identity, and reference and result share identical makeup. To
address this, we introduce MakeupQuad, a large-scale, high-quality dataset with
non-makeup faces, references, edited results, and textual makeup descriptions.
Building on this, we propose EvoMakeup, a unified training framework that
mitigates image degradation during multi-stage distillation, enabling iterative
improvement of both data and model quality. Although trained solely on
synthetic data, EvoMakeup generalizes well and outperforms prior methods on
real-world benchmarks. It supports high-fidelity, controllable, multi-task
makeup editing -- including full-face and partial reference-based editing, as
well as text-driven makeup editing -- within a single model. Experimental
results demonstrate that our method achieves superior makeup fidelity and
identity preservation, effectively balancing both aspects. Code and dataset
will be released upon acceptance.

</details>


### [26] [MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2508.06009)
*Jun Feng,Zixin Wang,Zhentao Zhang,Yue Guo,Zhihan Zhou,Xiuyi Chen,Zhenyang Li,Dawei Yin*

Main category: cs.CV

TL;DR: 该论文提出了一个新数据集MathReal，专注于真实世界K-12教育场景下的数学问题影像，并系统评估了现有多模态大模型（MLLMs）在此场景下的表现，发现其能力面临严峻挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型数学推理基准多基于干净、处理过的图像，缺乏对真实教育场景下用户拍摄图片的评估，该领域存在显著研究空白。

Method: 作者构建了MathReal数据集，收集了2000个由实际移动设备拍摄的K-12数学问题图像，并据图像噪声（质量衰减、视角变化、无关内容干扰）等多维度进行分类，涵盖多种知识点和难度。同时设计六种实验设置系统评估MLLMs，并对其表现和错误类型进行分析。

Result: 实验显示，当前主流MLLMs在真实教育场景下的解决能力显著下降，与理想基准差距大。论文还细致分析了其识别、理解和推理等方面的具体表现和失败原因。

Conclusion: 现有MLLMs在真实教育场景中的数学推理能力存在明显短板。MathReal为后续研究提供了真实可靠的评测基准，并指明了模型改进的方向。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in visual mathematical reasoning across various existing
benchmarks. However, these benchmarks are predominantly based on clean or
processed multimodal inputs, without incorporating the images provided by
real-world Kindergarten through 12th grade (K-12) educational users. To address
this gap, we introduce MathReal, a meticulously curated dataset comprising
2,000 mathematical questions with images captured by handheld mobile devices in
authentic scenarios. Each question is an image, containing the question text
and visual element. We systematically classify the real images into three
primary categories: image quality degradation, perspective variation, and
irrelevant content interference, which are further delineated into 14
subcategories. Additionally, MathReal spans five core knowledge and ability
categories, which encompass three question types and are divided into three
difficulty levels. To comprehensively evaluate the multimodal mathematical
reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we
design six experimental settings that enable a systematic analysis of their
performance. Through extensive experimentation, we find that the
problem-solving abilities of existing MLLMs are significantly challenged in
realistic educational contexts. Based on this, we conduct a thorough analysis
of their performance and error patterns, providing insights into their
recognition, comprehension, and reasoning capabilities, and outlining
directions for future improvements. Data and code:
https://github.com/junfeng0288/MathReal.

</details>


### [27] [GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.06113)
*Jian Wang,Chaokang Jiang,Haitao Xu*

Main category: cs.CV

TL;DR: 该论文提出了GMF-Drive，一种面向自动驾驶的端到端扩散模型框架，通过引入空间感知的门控状态空间模型，取代传统transformer融合方法，大幅提升了效率和性能，在NAVSIM基准上实现了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的端到端自动驾驶模型普遍依赖transformer进行多模态融合，但transformer存在计算开销大、空间结构建模能力弱等瓶颈，影响高分辨率特征提取及BEV结构化场景的效果。

Method: 作者提出两项创新：一是用几何增强的pillar格式替代直方图类LiDAR表示，保留3D几何形态信息；二是设计层次化门控mamba融合(GM-Fusion)架构，用空间感知的状态空间模型（BEV-SSM）高效替代transformer，通过定向序列建模和自适应融合，线性复杂度建模长程依赖。

Result: 在NAVSIM自动驾驶基准数据集上，GMF-Drive显著超越了当前最优的DiffusionDrive，并通过消融实验证明各模块的有效性。

Conclusion: 面向自动驾驶任务，专用状态空间模型在效率和性能上都优于transformer，为高效、高性能的端到端自动驾驶方案提供了新思路。

Abstract: Diffusion-based models are redefining the state-of-the-art in end-to-end
autonomous driving, yet their performance is increasingly hampered by a
reliance on transformer-based fusion. These architectures face fundamental
limitations: quadratic computational complexity restricts the use of
high-resolution features, and a lack of spatial priors prevents them from
effectively modeling the inherent structure of Bird's Eye View (BEV)
representations. This paper introduces GMF-Drive (Gated Mamba Fusion for
Driving), an end-to-end framework that overcomes these challenges through two
principled innovations. First, we supersede the information-limited
histogram-based LiDAR representation with a geometrically-augmented pillar
format encoding shape descriptors and statistical features, preserving critical
3D geometric details. Second, we propose a novel hierarchical gated mamba
fusion (GM-Fusion) architecture that substitutes an expensive transformer with
a highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM
leverages directional sequencing and adaptive fusion mechanisms to capture
long-range dependencies with linear complexity, while explicitly respecting the
unique spatial properties of the driving scene. Extensive experiments on the
challenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new
state-of-the-art performance, significantly outperforming DiffusionDrive.
Comprehensive ablation studies validate the efficacy of each component,
demonstrating that task-specific SSMs can surpass a general-purpose transformer
in both performance and efficiency for autonomous driving.

</details>


### [28] [ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors](https://arxiv.org/abs/2508.06014)
*Minsu Kim,Subin Jeon,In Cho,Mijin Yoo,Seon Joo Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯斑点(3DGS)的NVS新方法，通过自动生成额外训练视角和视频扩散对结果进行优化，大幅提升了从任意视角的渲染质量，并提出了新基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯斑点的NVS方法，在渲染训练路径之外的视角时存在伪影和缺失区域，影响场景的顺畅探索和渲染质量。需要提升未见视角的重建可靠性。

Method: 提出用信息增益驱动的虚拟相机布局策略自动生成尽量覆盖场景的训练视角，然后用视频扩散先验进一步细化渲染结果，最后用这些增强视角对3D高斯模型进行微调提升重建品质。

Result: 在新设置的Wild-Explore基准上进行实验，结果表明该方法优于现有3DGS方法，可以在任意视角下实现高质量、无伪影的渲染。

Conclusion: 本方法显著提升了基于3DGS的NVS在非训练路径视角的渲染表现，拓展了其在复杂场景探索中的应用潜力。

Abstract: Recent advances in novel view synthesis (NVS) have enabled real-time
rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle
with artifacts and missing regions when rendering from viewpoints that deviate
from the training trajectory, limiting seamless scene exploration. To address
this, we propose a 3DGS-based pipeline that generates additional training views
to enhance reconstruction. We introduce an information-gain-driven virtual
camera placement strategy to maximize scene coverage, followed by video
diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with
these enhanced views significantly improves reconstruction quality. To evaluate
our method, we present Wild-Explore, a benchmark designed for challenging scene
exploration. Experiments demonstrate that our approach outperforms existing
3DGS-based methods, enabling high-quality, artifact-free rendering from
arbitrary viewpoints.
  https://exploregs.github.io

</details>


### [29] [Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor](https://arxiv.org/abs/2508.06177)
*Dominik Brämer,Diana Kleingarn,Oliver Urbann*

Main category: cs.CV

TL;DR: 本文提出了一种创新的定位框架，通过利用地面特征和图卷积网络（GCN）进行机器人精准、高效定位。


<details>
  <summary>Details</summary>
Motivation: 传统的基于激光雷达（Lidar）或二维码的定位系统，在复杂环境中存在扩展性和适应性问题。作者希望找到一种更准确且高效的定位方法，克服现有方法的局限。

Method: 作者使用地面特征构建图结构，通过图卷积网络对这些特征进行建模，实现机器人定位。同时，该方法能解决每一帧的kidnapped robot问题，无需复杂的数据滤波处理。

Result: 该方法在定位精度（0.64厘米误差）和效率方面都显著优于传统以图像特征为基础的方法。

Conclusion: 本文提出的方法不仅提升了机器人定位的准确性和效率，也增强了适应复杂环境的能力，为多样化环境中的机器人导航开辟了新方向。

Abstract: Accurate localization represents a fundamental challenge in
  robotic navigation. Traditional methodologies, such as Lidar or QR-code based
systems, suffer from inherent scalability and adaptability con straints,
particularly in complex environments. In this work, we propose
  an innovative localization framework that harnesses flooring characteris tics
by employing graph-based representations and Graph Convolutional
  Networks (GCNs). Our method uses graphs to represent floor features,
  which helps localize the robot more accurately (0.64cm error) and more
  efficiently than comparing individual image features. Additionally, this
  approach successfully addresses the kidnapped robot problem in every
  frame without requiring complex filtering processes. These advancements
  open up new possibilities for robotic navigation in diverse environments.

</details>


### [30] [Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis](https://arxiv.org/abs/2508.06021)
*Utku Ozbulak,Michaela Cohrs,Hristo L. Svilenov,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: 本论文通过结合流式显微成像和深度学习，提出利用扩散模型生成高质量亚可见颗粒图像，从而解决训练集中数据不平衡问题，有效提升多类别分类器的性能。


<details>
  <summary>Details</summary>
Motivation: 由于亚可见颗粒类型（如硅油、空气泡等）数量稀少，且分布极度不均衡，限制了多类别分类器在实际生物制药分析中的应用。尤其对于无法通过实验大量采集的非蛋白质颗粒，现有深度学习方法依赖于大量均衡数据而效果平平。

Method: 作者开发并应用了前沿的扩散模型来自动生成高保真、结构真实的稀有颗粒图像，进而用于扩充原始不平衡数据集，训练多类别深度神经网络分类器。实验通过公开的50万个蛋白质颗粒验证集测试方法有效性，并对生成样本结构与真实数据相似性做了定量与定性对比。

Result: 生成的扩散模型样本在视觉质量及结构上高度接近真实亚可见颗粒图像。通过将扩散生成样本加入训练集，显著提升了多类别分类任务的准确率。

Conclusion: 扩散模型能够有效缓解流式显微亚可见颗粒分析中的数据不平衡难题，提升分类器性能，对生物制药研究具有促进作用。同时，作者开源了模型与工具以促进相关方向的后续研究与应用。

Abstract: Sub-visible particle analysis using flow imaging microscopy combined with
deep learning has proven effective in identifying particle types, enabling the
distinction of harmless components such as silicone oil from protein particles.
However, the scarcity of available data and severe imbalance between particle
types within datasets remain substantial hurdles when applying multi-class
classifiers to such problems, often forcing researchers to rely on less
effective methods. The aforementioned issue is particularly challenging for
particle types that appear unintentionally and in lower numbers, such as
silicone oil and air bubbles, as opposed to protein particles, where obtaining
large numbers of images through controlled settings is comparatively
straightforward. In this work, we develop a state-of-the-art diffusion model to
address data imbalance by generating high-fidelity images that can augment
training datasets, enabling the effective training of multi-class deep neural
networks. We validate this approach by demonstrating that the generated samples
closely resemble real particle images in terms of visual quality and structure.
To assess the effectiveness of using diffusion-generated images in training
datasets, we conduct large-scale experiments on a validation dataset comprising
500,000 protein particle images and demonstrate that this approach improves
classification performance with no negligible downside. Finally, to promote
open research and reproducibility, we publicly release both our diffusion
models and the trained multi-class deep neural network classifiers, along with
a straightforward interface for easy integration into future studies, at
https://github.com/utkuozbulak/svp-generative-ai.

</details>


### [31] [Depth Jitter: Seeing through the Depth](https://arxiv.org/abs/2508.06227)
*Md Sazidur Rahman,David Cabecinhas,Ricard Marxer*

Main category: cs.CV

TL;DR: 本文提出了一种名为Depth-Jitter的基于深度的增强方法，用于提升模型在不同深度环境下的稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在水下成像、机器人视觉和自主导航等实际应用中，深度信息至关重要。然而，传统的数据增强方法忽视了深度感知变换，导致模型在现实深度变化场景下鲁棒性不足。

Method: Depth-Jitter通过自适应的深度偏移与深度方差阈值控制，模拟真实环境中的深度扰动，同时保持场景结构稳定。该方法用于生成合成数据，并与传统的增强策略（如ColorJitter）在不同参数设置下进行了性能比较。

Result: 在FathomNet和UTDAC2020两个数据集上，Depth-Jitter尽管在绝对性能上不总是优于传统增广方法，但在深度变化较大环境下，可显著提升模型稳定性和泛化能力。

Conclusion: 基于深度的增广方法如Depth-Jitter能有效提升模型应对真实深度变化的能力，具备实际应用潜力，为深度感知学习研究提供了新的思路和基础工具。

Abstract: Depth information is essential in computer vision, particularly in underwater
imaging, robotics, and autonomous navigation. However, conventional
augmentation techniques overlook depth aware transformations, limiting model
robustness in real world depth variations. In this paper, we introduce
Depth-Jitter, a novel depth-based augmentation technique that simulates natural
depth variations to improve generalization. Our approach applies adaptive depth
offsetting, guided by depth variance thresholds, to generate synthetic depth
perturbations while preserving structural integrity. We evaluate Depth-Jitter
on two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on
model stability under diverse depth conditions. Extensive experiments compare
Depth-Jitter against traditional augmentation strategies such as ColorJitter,
analyzing performance across varying learning rates, encoders, and loss
functions. While Depth-Jitter does not always outperform conventional methods
in absolute performance, it consistently enhances model stability and
generalization in depth-sensitive environments. These findings highlight the
potential of depth-aware augmentation for real-world applications and provide a
foundation for further research into depth-based learning strategies. The
proposed technique is publicly available to support advancements in depth-aware
augmentation. The code is publicly available on
\href{https://github.com/mim-team/Depth-Jitter}{github}.

</details>


### [32] [Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts](https://arxiv.org/abs/2508.06032)
*Kiran Chhatre,Christopher Peters,Srikrishna Karanam*

Main category: cs.CV

TL;DR: 针对人体解析中细粒度服饰、身体部位区分难的问题，本文提出了一种名为Spectrum的统一网络，实现了部位级分割和实例级分组，并在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有人体分割方法多采用固定类别标签，导致多样服装类型和精细身体部位无法区分；而现有开放词汇分割又无法对人体内部细节（如服装和身体部位）做出区分。

Method: 本文创新性地将图像到3D纹理转换（I2Tx）扩散模型应用于人体解析。首先用I2Tx模型提取人体内部特征，并通过提示词引导，将特征和服装类别对齐，最后生成精确、细致的分割掩码。该方法兼顾部位和实例分割能力，能够支持多人体、多类别解析。

Result: 在跨数据集评测中，在身体部位、服装部位、未知类别和全身掩码等细粒度任务上，Spectrum模型的表现均优于现有prompt-based分割基线方法。

Conclusion: Spectrum方法有效提升了人体各部位及服装的开放词汇分割精度，可实现任意场景下的细粒度和高鲁棒性解析。方法泛化性强，为后续相关任务提供了新思路。

Abstract: Existing methods for human parsing into body parts and clothing often use
fixed mask categories with broad labels that obscure fine-grained clothing
types. Recent open-vocabulary segmentation approaches leverage pretrained
text-to-image (T2I) diffusion model features for strong zero-shot transfer, but
typically group entire humans into a single person category, failing to
distinguish diverse clothing or detailed body parts. To address this, we
propose Spectrum, a unified network for part-level pixel parsing (body parts
and clothing) and instance-level grouping. While diffusion-based
open-vocabulary models generalize well across tasks, their internal
representations are not specialized for detailed human parsing. We observe
that, unlike diffusion models with broad representations, image-driven 3D
texture generators maintain faithful correspondence to input images, enabling
stronger representations for parsing diverse clothing and body parts. Spectrum
introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --
obtained by fine-tuning a T2I model on 3D human texture maps -- for improved
alignment with body parts and clothing. From an input image, we extract
human-part internal features via the I2Tx diffusion model and generate
semantically valid masks aligned to diverse clothing categories through
prompt-guided grounding. Once trained, Spectrum produces semantic segmentation
maps for every visible body part and clothing category, ignoring standalone
garments or irrelevant objects, for any number of humans in the scene. We
conduct extensive cross-dataset experiments -- separately assessing body parts,
clothing parts, unseen clothing categories, and full-body masks -- and
demonstrate that Spectrum consistently outperforms baseline methods in
prompt-based segmentation.

</details>


### [33] [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](https://arxiv.org/abs/2508.06033)
*Yiming Gong,Zhen Zhu,Minjia Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为InstantEdit的快速文本引导图像编辑方法，能够在极少步骤内完成高质量图像编辑，优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 当前图像编辑方法在编辑细节、速度和文本指令遵循间难以兼顾，且少步编辑很难保持图像内容和结构一致性。为解决这些挑战，作者提出新框架和策略。

Method: 1. 基于RectifiedFlow框架设计几步编辑流程，提出特殊反演策略PerRFI，实现更好的初始内容还原。
2. 提出Inversion Latent Injection方法，在重生成步骤充分利用反演时获得的潜在信息，提高一致性和细节。
3. 提出Disentangled Prompt Guidance，平衡可编辑性和细节保护。
4. 融合Canny边缘信息的ControlNet以增强结构保持和降低伪影。

Result: 在PIE图像编辑数据集上，与现有少步图像编辑方法比较，InstantEdit无论在速度还是编辑质量（定性和定量）上均取得更优结果。

Conclusion: InstantEdit通过多项创新技术，实现了快速、细致且高一致性的文本引导图像编辑，对比主流方法有明显提升。

Abstract: We propose a fast text-guided image editing method called InstantEdit based
on the RectifiedFlow framework, which is structured as a few-step editing
process that preserves critical content while following closely to textual
instructions. Our approach leverages the straight sampling trajectories of
RectifiedFlow by introducing a specialized inversion strategy called PerRFI. To
maintain consistent while editable results for RectifiedFlow model, we further
propose a novel regeneration method, Inversion Latent Injection, which
effectively reuses latent information obtained during inversion to facilitate
more coherent and detailed regeneration. Additionally, we propose a
Disentangled Prompt Guidance technique to balance editability with detail
preservation, and integrate a Canny-conditioned ControlNet to incorporate
structural cues and suppress artifacts. Evaluation on the PIE image editing
dataset demonstrates that InstantEdit is not only fast but also achieves better
qualitative and quantitative results compared to state-of-the-art few-step
editing methods.

</details>


### [34] [More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment](https://arxiv.org/abs/2508.06036)
*Jun Xie,Yingjian Zhu,Feng Chen,Zhenghao Zhang,Xiaohui Fan,Hongzhu Yi,Xinming Wang,Chen Yu,Yue Bi,Zhaoran Zhao,Xiongjun Guan,Zhepeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于专家混合（MoE）的半监督情感识别系统，通过集成多模态信息与高质量伪标签提升性能，在MER2025-SEMI比赛中取得第二名。


<details>
  <summary>Details</summary>
Motivation: 情感识别任务中，带标签数据稀缺，而利用多模态信息和无标签数据有潜力大幅提升系统表现。作者希望设计一个高鲁棒性、可扩展的情感识别系统，充分挖掘未标注数据与新型模态的价值。

Method: 1）构建专家混合（MoE）体系，将多种输入模态（包括大视觉-语言模型知识、时序面部AU信息等）作为独立专家。2）提出基于共识的伪标签策略——融合基线模型和Gemini模型的预测一致性来生成高质量伪标签，形成两阶段训练流程。3）采用多专家投票集成并结合规则再排序，纠正模型偏差，提高输出与人类偏好的一致性。

Result: 在MER2025-SEMI挑战赛公开测试集上取得F1分数0.8772，排名赛道第2。

Conclusion: 多模态专家混合结合高质量伪标签和再排序机制能够显著增强半监督情感识别系统的性能，为相关任务提供了新启发。

Abstract: In this paper, we present our solution for the semi-supervised learning track
(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the
principle that "more is better," to construct a robust Mixture of Experts (MoE)
emotion recognition system. Our approach integrates a diverse range of input
modalities as independent experts, including novel signals such as knowledge
from large Vision-Language Models (VLMs) and temporal Action Unit (AU)
information. To effectively utilize unlabeled data, we introduce a
consensus-based pseudo-labeling strategy, generating high-quality labels from
the agreement between a baseline model and Gemini, which are then used in a
two-stage training paradigm. Finally, we employ a multi-expert voting ensemble
combined with a rule-based re-ranking process to correct prediction bias and
better align the outputs with human preferences. Evaluated on the MER2025-SEMI
challenge dataset, our method achieves an F1-score of 0.8772 on the test set,
ranking 2nd in the track. Our code is available at
https://github.com/zhuyjan/MER2025-MRAC25.

</details>


### [35] [Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](https://arxiv.org/abs/2508.06038)
*Huanyu Wang,Jushi Kai,Haoli Bai,Lu Hou,Bo Jiang,Ziwei He,Zhouhan Lin*

Main category: cs.CV

TL;DR: 本文提出了一种名为Fourier-VLM的新方法，通过在频域压缩视觉特征，有效减少VLM推理计算量并提升速度，同时保持竞争性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言大模型在将视觉特征输入LLM时，因视觉token数量多而导致上下文超长，推理成本高，延迟大。以往通过特征选择或可学习query减少token数，往往要么降低性能，要么引入额外开销。因此，亟需一种高效、无参且能兼顾效率和性能的特征压缩方法。

Method: 作者观察到视觉编码器输出特征能量集中在低频，通过对特征做二维离散余弦变换（DCT）：只保留低频分量，相当于低通滤波，从而节省token数和计算量。DCT通过FFT高效实现，计算复杂度低（O(nlogn)），且无额外参数。方法通用于不同VLM架构。

Result: 在多个视觉-语言基准上，Fourier-VLM在LLaVA和Qwen-VL等架构里均取得与主流方法相当甚至更优的表现。相比LLaVA-v1.5，推理FLOPs最多降低83.8%，生成速度提升31.2%。

Conclusion: Fourier-VLM无需引入额外参数且压缩过程高效，能大幅减少计算量并加速推理，同时保持VLM的强泛化能力和竞争性能，实用性和效率兼备。

Abstract: Vision-Language Models (VLMs) typically replace the predefined image
placeholder token (<image>) in textual instructions with visual features from
an image encoder, forming the input to a backbone Large Language Model (LLM).
However, the large number of vision tokens significantly increases the context
length, leading to high computational overhead and inference latency. While
previous efforts mitigate this by selecting only important visual features or
leveraging learnable queries to reduce token count, they often compromise
performance or introduce substantial extra costs. In response, we propose
Fourier-VLM, a simple yet efficient method that compresses visual
representations in the frequency domain. Our approach is motivated by the
observation that vision features output from the vision encoder exhibit
concentrated energy in low-frequency components. Leveraging this, we apply a
low-pass filter to the vision features using a two-dimentional Discrete Cosine
Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier
Transform (FFT) operator with a time complexity of $\mathcal{O}(n\log n)$,
minimizing the extra computational cost while introducing no additional
parameters. Extensive experiments across various image-based benchmarks
demonstrate that Fourier-VLM achieves competitive performance with strong
generalizability across both LLaVA and Qwen-VL architectures. Crucially, it
reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%
compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.

</details>


### [36] [NEP: Autoregressive Image Editing via Next Editing Token Prediction](https://arxiv.org/abs/2508.06044)
*Huimin Wu,Xiaojian Ma,Haozhe Zhao,Yanpeng Zhao,Qing Li*

Main category: cs.CV

TL;DR: 本论文提出了一种新的基于文本指导的图像编辑方法，仅对需要编辑的局部区域进行重新生成，提升编辑效果并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的文本指导图像编辑方法往往会整体重建目标图像，而非只编辑需要修改的局部区域，导致计算效率低下且非编辑区域的质量下降，影响目标编辑效果。

Method: 作者将图像编辑问题表述为“下一个编辑token预测”(Next Editing-token Prediction, NEP)，基于自回归图像生成方法，只重新生成需要修改的区域。同时，提出了任意顺序自回归文本到图像（T2I）模型进行预训练，实现任意区域编辑，并支持零样本图像编辑和测试时多次精细化图像生成。

Result: 该方法在广泛使用的图像编辑基准上取得了新的最优性能表现（state-of-the-art），验证了其有效性。此外，模型支持在零样本条件下多轮精细化生成，进一步提升了编辑质量。

Conclusion: 论文提出的NEP方法有效避免了对非编辑区域的无谓修改，提升了编辑效率和质量，在多项图像编辑评测中达到了领先水平。该方法具备广泛的实用性和扩展潜力。

Abstract: Text-guided image editing involves modifying a source image based on a
language instruction and, typically, requires changes to only small local
regions. However, existing approaches generate the entire target image rather
than selectively regenerate only the intended editing areas. This results in
(1) unnecessary computational costs and (2) a bias toward reconstructing
non-editing regions, which compromises the quality of the intended edits. To
resolve these limitations, we propose to formulate image editing as Next
Editing-token Prediction (NEP) based on autoregressive image generation, where
only regions that need to be edited are regenerated, thus avoiding unintended
modification to the non-editing areas. To enable any-region editing, we propose
to pre-train an any-order autoregressive text-to-image (T2I) model. Once
trained, it is capable of zero-shot image editing and can be easily adapted to
NEP for image editing, which achieves a new state-of-the-art on widely used
image editing benchmarks. Moreover, our model naturally supports test-time
scaling (TTS) through iteratively refining its generation in a zero-shot
manner. The project page is: https://nep-bigai.github.io/

</details>


### [37] [VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning](https://arxiv.org/abs/2508.06051)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Jun Jia,Kaiwei Zhang,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出了VQAThinker框架，通过结合大规模多模态模型与强化学习，显著提升了视频质量评价模型的泛化能力和可解释性，实现了在多项基准上的最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频质量评价模型在面对分布外（OOD）视频时泛化能力差且可解释性有限，难以满足真实应用需求，因此亟需开发更具泛化性和解释能力的VQA方法。

Method: 作者提出VQAThinker框架，采用基于大规模多模态模型（LMMs）与群体相对策略优化（GRPO）的强化学习方法，实现类人推理式的视频质量理解与打分。设计了三个专用于VQA的奖励机制：1）钟型回归奖励，提升预测误差接近真实值时的灵敏度；2）对间排序奖励，指导模型正确区分视频质量高低；3）时序一致性奖励，鼓励模型偏好时序连贯的视频。监督仅停留在打分层面。

Result: 大量实验展示，VQAThinker在域内外VQA基准上取得了最优性能，泛化能力强，并在失真归因和质量描述等解释性任务上超越现有同类模型和多模态大模型。

Conclusion: VQAThinker通过强化学习与多种奖励设计，显著推进了VQA方法的泛化与可解释性，为仅用打分级监督的通用可解释VQA模型提供了有效途径。

Abstract: Video quality assessment (VQA) aims to objectively quantify perceptual
quality degradation in alignment with human visual perception. Despite recent
advances, existing VQA models still suffer from two critical limitations:
\textit{poor generalization to out-of-distribution (OOD) videos} and
\textit{limited explainability}, which restrict their applicability in
real-world scenarios. To address these challenges, we propose
\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large
multimodal models (LMMs) with reinforcement learning to jointly model video
quality understanding and scoring, emulating human perceptual decision-making.
Specifically, we adopt group relative policy optimization (GRPO), a rule-guided
reinforcement learning algorithm that enables reasoning over video quality
under score-level supervision, and introduce three VQA-specific rewards: (1) a
\textbf{bell-shaped regression reward} that increases rapidly as the prediction
error decreases and becomes progressively less sensitive near the ground truth;
(2) a \textbf{pairwise ranking reward} that guides the model to correctly
determine the relative quality between video pairs; and (3) a \textbf{temporal
consistency reward} that encourages the model to prefer temporally coherent
videos over their perturbed counterparts. Extensive experiments demonstrate
that VQAThinker achieves state-of-the-art performance on both in-domain and OOD
VQA benchmarks, showing strong generalization for video quality scoring.
Furthermore, evaluations on video quality understanding tasks validate its
superiority in distortion attribution and quality description compared to
existing explainable VQA models and LMMs. These findings demonstrate that
reinforcement learning offers an effective pathway toward building
generalizable and explainable VQA models solely with score-level supervision.

</details>


### [38] [LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing](https://arxiv.org/abs/2508.06055)
*Wonjung Park,Suhyun Ahn,Jinah Park*

Main category: cs.CV

TL;DR: 本文提出了一种新的三维侧脑室（LV）形状建模方法LV-Net，实现了更高精度和鲁棒性的个体化重建，并在阿尔茨海默病研究中显示出临床价值。


<details>
  <summary>Details</summary>
Motivation: 侧脑室形状被认为是神经系统疾病的重要生物标志物，但现有分析方法受到个体间形状差异大和MRI分辨率有限导致的分割困难影响，准确性和稳健性不足。

Method: 提出LV-Net，将带有解剖信息的脑室-海马联合模板网格通过变形适应个体MRI数据，实现三维重建。该方法利用模板中解剖邻接关系，优化点对应性，有效减少边界分割伪影，提高统计分析准确性。

Result: LV-Net在多个数据集上表现出更高的重建准确性与鲁棒性，即使在分割不完美时也能可靠表现，并得到更精确的形状统计。应用于阿尔茨海默病数据时，该方法成功定位了与疾病高度相关的侧脑室区域。

Conclusion: LV-Net提升了侧脑室三维建模的精度和可靠性，为神经疾病的临床形状分析提供了有力工具，特别是在阿尔茨海默病等研究中表现突出。

Abstract: Lateral ventricle (LV) shape analysis holds promise as a biomarker for
neurological diseases; however, challenges remain due to substantial shape
variability across individuals and segmentation difficulties arising from
limited MRI resolution. We introduce LV-Net, a novel framework for producing
individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint
LV-hippocampus template mesh. By incorporating anatomical relationships
embedded within the joint template, LV-Net reduces boundary segmentation
artifacts and improves reconstruction robustness. In addition, by classifying
the vertices of the template mesh based on their anatomical adjacency, our
method enhances point correspondence across subjects, leading to more accurate
LV shape statistics. We demonstrate that LV-Net achieves superior
reconstruction accuracy, even in the presence of segmentation imperfections,
and delivers more reliable shape descriptors across diverse datasets. Finally,
we apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that
show significantly associations with the disease relative to cognitively normal
controls. The codes for LV shape modeling are available at
https://github.com/PWonjung/LV_Shape_Modeling.

</details>


### [39] [AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?](https://arxiv.org/abs/2508.06057)
*Mojtaba Valipour,Kelly Zheng,James Lowman,Spencer Szabados,Mike Gartner,Bobby Braswell*

Main category: cs.CV

TL;DR: 本文呼吁将地球观测卫星光谱影像纳入人工通用智能（AGI）研究，并提出构建专门的基准体系以促进该领域发展。


<details>
  <summary>Details</summary>
Motivation: 人工通用智能（AGI）研究领域热衷于多模态数据（文本、图像、视频、音频），但卫星光谱图像这一重要数据模态却被忽视。而地球观测数据对AGI提升理解自然世界能力有重要价值。

Method: 论文首先阐述了地球观测数据对智能模型的重要性，然后回顾了现有benchmark及其在评估基础模型泛化能力上的局限性，最后提出建立更为全面的评测任务集合。

Result: 总结出现有基准对于该领域模型泛化能力评测存在不足，并提出了改进的benchmark所应覆盖的多样任务集合。

Conclusion: 作者强调建立更完善的基准体系对于提升地球观测数据领域模型能力、促进AGI理解自然世界至关重要。

Abstract: Artificial General Intelligence (AGI) is closer than ever to becoming a
reality, sparking widespread enthusiasm in the research community to collect
and work with various modalities, including text, image, video, and audio.
Despite recent efforts, satellite spectral imagery, as an additional modality,
has yet to receive the attention it deserves. This area presents unique
challenges, but also holds great promise in advancing the capabilities of AGI
in understanding the natural world. In this paper, we argue why Earth
Observation data is useful for an intelligent model, and then we review
existing benchmarks and highlight their limitations in evaluating the
generalization ability of foundation models in this domain. This paper
emphasizes the need for a more comprehensive benchmark to evaluate earth
observation models. To facilitate this, we propose a comprehensive set of tasks
that a benchmark should encompass to effectively assess a model's ability to
understand and interact with Earth observation data.

</details>


### [40] [Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention](https://arxiv.org/abs/2508.06058)
*Shiyang Zhou,Haijin Zeng,Yunfan Lu,Yongyong Chen,Jie Liu,Jingyong Su*

Main category: cs.CV

TL;DR: 本文提出了一种适用于移动端HybridEVS事件相机的高效轻量级去马赛克方法TSANet，在多个数据集上效果优于现有方法同时大幅降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 事件相机结合了Quad Bayer CFA和彩色像素和无色事件像素，导致去马赛克阶段产生混叠和伪影，现有方法难以高效解决，且移动设备计算资源有限，因此亟需高质量、低资源消耗的去马赛克方案。

Method: 提出了TSANet，一种两阶段网络结构，基于跨注意力（Cross-Attention）及状态空间模型。该方法将事件像素“修复（inpainting）”和去马赛克两步分开处理，引入Cross-Swin State Block模块，用位置先验提升去马赛克表现，通过线性复杂度的状态空间建模增强全局信息捕捉，整体网络较为轻量便于移动端部署。

Result: TSANet在HybridEVS模拟数据和真实数据上均实现了优异的去马赛克效果。与主流方法DemosaicFormer相比，在7个不同数据集上PSNR和SSIM均有提升，同时参数量减少1.86倍、计算量减少3.29倍。

Conclusion: TSANet为移动设备上的事件相机图像高效去马赛克提供了新方案，实现了更优性能和更低资源消耗，推动了事件相机在移动摄影领域的实际应用。

Abstract: Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera
capture brightness changes as asynchronous "events" instead of frames, offering
advanced application on mobile photography. However, challenges arise from
combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels
lacking color information, resulting in aliasing and artifacts on the
demosaicing process before downstream application. Current methods struggle to
address these issues, especially on resource-limited mobile devices. In
response, we introduce \textbf{TSANet}, a lightweight \textbf{T}wo-stage
network via \textbf{S}tate space augmented cross-\textbf{A}ttention, which can
handle event pixels inpainting and demosaicing separately, leveraging the
benefits of dividing complex tasks into manageable subtasks. Furthermore, we
introduce a lightweight Cross-Swin State Block that uniquely utilizes
positional prior for demosaicing and enhances global dependencies through the
state space model with linear complexity. In summary, TSANet demonstrates
excellent demosaicing performance on both simulated and real data of HybridEVS
while maintaining a lightweight model, averaging better results than the
previous state-of-the-art method DemosaicFormer across seven diverse datasets
in both PSNR and SSIM, while respectively reducing parameter and computation
costs by $1.86\times$ and $3.29\times$. Our approach presents new possibilities
for efficient image demosaicing on mobile devices. Code is available in the
supplementary materials.

</details>


### [41] [Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection](https://arxiv.org/abs/2508.06063)
*Chao Hao,Zitong Yu,Xin Liu,Yuhao Wang,Weicheng Xie,Jingang Shi,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的联合学习机制SCJoint，通过合理分离解码过程，实现了显著目标检测（SOD）与伪装目标检测（COD）的同步优化，并通过基于显著性的采样策略提升了训练效果。最终提出的网络JoNet在相关任务上取得了优异的实验表现。


<details>
  <summary>Details</summary>
Motivation: SOD与COD虽然都是无类别的分割任务，且均需将RGB图像转化为二值图像，但二者目标相反（前者找最显著目标，后者找隐藏目标）。以往研究普遍认为这两项任务若联合训练会导致性能下降，本文旨在挑战这一观点，探索能否通过合适方法实现两者的互补学习。

Method: 提出SCJoint联合学习架构，假设SOD和COD的解码过程在特征分布上存在区别，通过在完全共享网络结构中插入少量特定任务可学习参数，实现对任务属性的有效分离。同时，提出显著性采样策略（SBSS），提升训练集样本均衡性和质量，并缩短训练时间。

Result: 在公开数据集上进行大量实验，结果显示JoNet在SOD和COD两个任务上均达到较具竞争力的检测性能，证明了联合学习机制的有效性和优越性。

Conclusion: 通过适当的任务解耦方法，SOD和COD可以联合训练且相互促进。所提SCJoint和SBSS方法能提升分割质量且训练高效，JoNet模型在两任务上验证了其实用性。

Abstract: Salient object detection (SOD) and camouflaged object detection (COD) are two
closely related but distinct computer vision tasks. Although both are
class-agnostic segmentation tasks that map from RGB space to binary space, the
former aims to identify the most salient objects in the image, while the latter
focuses on detecting perfectly camouflaged objects that blend into the
background in the image. These two tasks exhibit strong contradictory
attributes. Previous works have mostly believed that joint learning of these
two tasks would confuse the network, reducing its performance on both tasks.
However, here we present an opposite perspective: with the correct approach to
learning, the network can simultaneously possess the capability to find both
salient and camouflaged objects, allowing both tasks to benefit from joint
learning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,
assuming that the decoding processes of SOD and COD have different distribution
characteristics. The key to our method is to learn the respective means and
variances of the decoding processes for both tasks by inserting a minimal
amount of task-specific learnable parameters within a fully shared network
structure, thereby decoupling the contradictory attributes of the two tasks at
a minimal cost. Furthermore, we propose a saliency-based sampling strategy
(SBSS) to sample the training set of the SOD task to balance the training set
sizes of the two tasks. In addition, SBSS improves the training set quality and
shortens the training time. Based on the proposed SCJoint and SBSS, we train a
powerful generalist network, named JoNet, which has the ability to
simultaneously capture both ``salient" and ``camouflaged". Extensive
experiments demonstrate the competitive performance and effectiveness of our
proposed method. The code is available at https://github.com/linuxsino/JoNet.

</details>


### [42] [Can Large Models Fool the Eye? A New Turing Test for Biological Animation](https://arxiv.org/abs/2508.06072)
*Zijian Chen,Lirong Deng,Zhengyu Chen,Kaiwei Zhang,Qi Jia,Yuan Tian,Yucheng Zhu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了BioMotion Arena，一个通过可视化动画评测大语言模型（LLMs）及多模态大语言模型（MLLMs）能力的新框架，揭示了当前主流模型在视觉动态生成任务上的显著差距。


<details>
  <summary>Details</summary>
Motivation: 目前大模型评测方法（如静态数据集的标准评分或基于对话风格的主观偏好收集）难以直观且有效地区分模型性能优劣，缺乏即时性与感知性反馈。

Method: 借鉴生物运动中的点光源成像原理，使用可视化动画对模型能力进行放大对比。采用两两比较的评测方式，对53个主流模型在90个生物运动变体上进行评测，并收集超过45,000票的众包人类评分，与专家评分进行一致性分析。

Result: 实验表明，众包评分与专家评分高度一致，BioMotion Arena能明显区分模型表现。研究还发现，超过90%的受测模型，包括新近开源与专有先进模型，都无法生成基本的人体点光组，更遑论流畅、符合生物学的运动动画。

Conclusion: BioMotion Arena作为无须依赖标准答案的开放性评测框架，不仅为模型性能差异提供了清晰可视化，还能作为大模型视觉动画能力的有挑战性的全新基准。

Abstract: Evaluating the abilities of large models and manifesting their gaps are
challenging. Current benchmarks adopt either ground-truth-based score-form
evaluation on static datasets or indistinct textual chatbot-style human
preferences collection, which may not provide users with immediate, intuitive,
and perceptible feedback on performance differences. In this paper, we
introduce BioMotion Arena, a novel framework for evaluating large language
models (LLMs) and multimodal large language models (MLLMs) via visual
animation. Our methodology draws inspiration from the inherent visual
perception of motion patterns characteristic of living organisms that utilizes
point-light source imaging to amplify the performance discrepancies between
models. Specifically, we employ a pairwise comparison evaluation and collect
more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion
variants. Data analyses show that the crowd-sourced human votes are in good
agreement with those of expert raters, demonstrating the superiority of our
BioMotion Arena in offering discriminative feedback. We also find that over
90\% of evaluated models, including the cutting-edge open-source InternVL3 and
proprietary Claude-4 series, fail to produce fundamental humanoid point-light
groups, much less smooth and biologically plausible motions. This enables
BioMotion Arena to serve as a challenging benchmark for performance
visualization and a flexible evaluation framework without restrictions on
ground-truth.

</details>


### [43] [Towards MR-Based Trochleoplasty Planning](https://arxiv.org/abs/2508.06076)
*Michael Wehrli,Alicia Durrer,Paul Friedrich,Sidaty El Hadramy,Edwin Li,Luana Brahaj,Carol C. Hasler,Philippe C. Cattin*

Main category: cs.CV

TL;DR: 该论文提出了一套从低分辨率MRI数据中生成病人特异性、超分辨率‘伪健康’3D形态的自动化方案，用于髁股骨发育不良的手术规划，并且无需CT扫描。实验结果显示，其显著提升了手术相关的影像指标。


<details>
  <summary>Details</summary>
Motivation: 现有的髁股骨发育不良（TD）手术主要依赖低分辨率MRI和外科医生的经验，导致手术效果不一致，而且缺乏微创手术的标准化计划，且传统方法为患者带来不必要的辐射。

Method: 1）利用隐式神经表示（INR）生成各向同性的超分辨率MRI体积；2）用定制的多标签神经网络分割股骨、胫骨、髌骨和腓骨；3）训练小波扩散模型（WDM）生成髁区‘伪健康’目标形态。整个流程完全基于MRI，无需CT。这些高分辨率3D形态可用于术前规划和术中应用。

Result: 在25例TD患者上评估后，发现自动生成的目标形态可以显著改善沟角（SA）和髁沟深度（TGD）两个重要的手术相关解剖指标。

Conclusion: 提出的方法能够不依赖CT、基于常规MRI自动生成高分辨率、贴合个体需求的3D‘伪健康’骨面蓝图，提高了手术可操作性和术后结构一致性，有助于推动髁股骨发育不良手术影像规划自动化和微创化。

Abstract: To treat Trochlear Dysplasia (TD), current approaches rely mainly on
low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.
The surgeries are planned based on surgeons experience, have limited adoption
of minimally invasive techniques, and lead to inconsistent outcomes. We propose
a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy
target morphologies from conventional clinical MR scans. First, we compute an
isotropic super-resolved MR volume using an Implicit Neural Representation
(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label
custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to
generate pseudo-healthy target morphologies of the trochlear region. In
contrast to prior work producing pseudo-healthy low-resolution 3D MR images,
our approach enables the generation of sub-millimeter resolved 3D shapes
compatible for pre- and intraoperative use. These can serve as preoperative
blueprints for reshaping the femoral groove while preserving the native patella
articulation. Furthermore, and in contrast to other work, we do not require a
CT for our pipeline - reducing the amount of radiation. We evaluated our
approach on 25 TD patients and could show that our target morphologies
significantly improve the sulcus angle (SA) and trochlear groove depth (TGD).
The code and interactive visualization are available at
https://wehrlimi.github.io/sr-3d-planning/.

</details>


### [44] [DreamVE: Unified Instruction-based Image and Video Editing](https://arxiv.org/abs/2508.06080)
*Bin Xia,Jiyang Liu,Yuechen Zhang,Bohao Peng,Ruihang Chu,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 本文提出DreamVE，一个统一的指令编辑图像和视频的模型，采用两阶段训练策略（先图像后视频），并设计了高效的数据合成和编辑框架，实现了高效、通用且易于扩展的指令编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的视频编辑因缺乏训练数据而实用性有限，难以泛化和推广。提高训练数据规模、类型与编辑效率是亟需解决的问题。

Method: 采用两阶段训练（先大规模图像数据，后视频），结合拼贴式和生成模型式数据合成管线扩展训练集，进一步使用生成模型数据针对属性编辑微调模型。模型结构基于最优T2V模型，通过token拼接和early drop方法引入源图像指导。

Result: DreamVE在多种重点编辑类型上表现优异，展现了良好的泛化与迁移能力。拼贴数据使模型在主流编辑任务上性能强，但对特定属性编辑有不足，通过生成模型数据微调可提升相关表现。

Conclusion: DreamVE实现了高效、统一的图像与视频指令编辑，迁移性和通用性好；提出的数据生成方法助力低资源领域突破。代码与模型即将公开，有望推动指令编辑应用发展。

Abstract: Instruction-based editing holds vast potential due to its simple and
efficient interactive editing format. However, instruction-based editing,
particularly for video, has been constrained by limited training data,
hindering its practical application. To this end, we introduce DreamVE, a
unified model for instruction-based image and video editing. Specifically, We
propose a two-stage training strategy: first image editing, then video editing.
This offers two main benefits: (1) Image data scales more easily, and models
are more efficient to train, providing useful priors for faster and better
video editing training. (2) Unifying image and video generation is natural and
aligns with current trends. Moreover, we present comprehensive training data
synthesis pipelines, including collage-based and generative model-based data
synthesis. The collage-based data synthesis combines foreground objects and
backgrounds to generate diverse editing data, such as object manipulation,
background changes, and text modifications. It can easily generate billions of
accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE
on extensive collage-based data to achieve strong performance in key editing
types and enhance generalization and transfer capabilities. However,
collage-based data lacks some attribute editing cases, leading to a relative
drop in performance. In contrast, the generative model-based pipeline, despite
being hard to scale up, offers flexibility in handling attribute editing cases.
Therefore, we use generative model-based data to further fine-tune DreamVE.
Besides, we design an efficient and powerful editing framework for DreamVE. We
build on the SOTA T2V model and use a token concatenation with early drop
approach to inject source image guidance, ensuring strong consistency and
editability. The codes and models will be released.

</details>


### [45] [SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment](https://arxiv.org/abs/2508.06082)
*Yanxiao Sun,Jiafu Wu,Yun Cao,Chengming Xu,Yabiao Wang,Weijian Cao,Donghao Luo,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为SwiftVideo的新型视频生成蒸馏框架，在大幅减少推理步数的同时，显著提升了视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 基于扩散或流的模型在视频生成领域有显著进步，但通常需要多次迭代采样，计算成本高昂。现有通过蒸馏加速的方法，在极少步推理下容易性能下降或产生伪影。

Method: SwiftVideo融合了轨迹保持与分布匹配两种策略，提出了连续时间一致性蒸馏保证常微分方程轨迹的精确保持，并在合成数据与真实数据分布间、不同推理步数间都进行双重对齐。

Result: 在OpenVid-1M基准上，SwiftVideo在保持高生成质量的同时，显著减少了推理步数，并且在少步推理场景下的表现明显优于其他方法。

Conclusion: SwiftVideo提供了一种统一、稳定的蒸馏框架，有效解决了现有模型少步推理下的性能退化问题，并提升了视频生成效率和质量。

Abstract: Diffusion-based or flow-based models have achieved significant progress in
video synthesis but require multiple iterative sampling steps, which incurs
substantial computational overhead. While many distillation methods that are
solely based on trajectory-preserving or distribution-matching have been
developed to accelerate video generation models, these approaches often suffer
from performance breakdown or increased artifacts under few-step settings. To
address these limitations, we propose \textbf{\emph{SwiftVideo}}, a unified and
stable distillation framework that combines the advantages of
trajectory-preserving and distribution-matching strategies. Our approach
introduces continuous-time consistency distillation to ensure precise
preservation of ODE trajectories. Subsequently, we propose a dual-perspective
alignment that includes distribution alignment between synthetic and real data
along with trajectory alignment across different inference steps. Our method
maintains high-quality video generation while substantially reducing the number
of inference steps. Quantitative evaluations on the OpenVid-1M benchmark
demonstrate that our method significantly outperforms existing approaches in
few-step video generation.

</details>


### [46] [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/abs/2508.06492)
*Yuwei Yang,Zeyu Zhang,Yunzhong Hou,Zhuowan Li,Gaowen Liu,Ali Payani,Yuan-Sen Ting,Liang Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种模块化和多样化的图表生成流程，用于改进多模态大语言模型（MLLMs）的科学图表理解能力，并发布了高质量的大型合成图表数据集ECD，有效提升了模型在多个测试集上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在科学图表理解上的表现不佳，开源模型在挑战性评测中成功率仅为30%-50%。现有利用合成图表微调模型的方法，由于与真实图表差异较大，限制了训练效果和模型泛化能力。因此，需要改进数据合成流程，提升数据的真实性和复杂度，以增强模型在实际科学情境中的图表理解能力。

Method: 作者设计了五步数据合成流程：(1) 分离数据和函数创建单一图表；(2) 多子图时后续子图受前面子图影响生成；(3) 提高视觉多样性，增加图表复杂性；(4) 筛除低质量数据；(5) 用GPT-4o生成相应问答对。最终生成了包含1万多图表图片和30万问答对的高复杂度、高质量数据集ECD。

Result: 所提出的ECD数据集涵盖25个领域、250+种复杂图表组合。多组实验显示，该数据集能显著且一致地提升多种MLLMs在真实和合成图表测试集上的表现。

Conclusion: 模块化和多样化的图表合成流程能生成更贴近真实、覆盖面广的高质量图表数据，有效提升MLLMs的科学图表理解能力，对推动科研智能体发展具有积极意义。

Abstract: Being able to effectively read scientific plots, or chart understanding, is a
central part toward building effective agents for science. However, existing
multimodal large language models (MLLMs), especially open-source ones, are
still falling behind with a typical success rate of 30%-50% on challenging
benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are
often restricted by their inadequate similarity to the real charts, which could
compromise model training and performance on complex real-world charts. In this
study, we show that modularizing chart generation and diversifying visual
details improves chart understanding capabilities. In particular, we design a
five-step data synthesis pipeline, where we separate data and function creation
for single plot generation, condition the generation of later subplots on
earlier ones for multi-subplot figures, visually diversify the generated
figures, filter out low quality data, and finally generate the question-answer
(QA) pairs with GPT-4o. This approach allows us to streamline the generation of
fine-tuning datasets and introduce the effective chart dataset (ECD), which
contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring
250+ chart type combinations with high visual complexity. We show that ECD
consistently improves the performance of various MLLMs on a range of real-world
and synthetic test sets. Code, data and models are available at:
https://github.com/yuweiyang-anu/ECD.

</details>


### [47] [AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance](https://arxiv.org/abs/2508.06084)
*Weichen Zhang,Zhui Zhu,Ningbo Li,Kebin Liu,Yunhao Liu*

Main category: cs.CV

TL;DR: 本文提出了AdaptInfer框架，通过动态、精细化的视觉token裁剪机制，降低视觉-语言模型（VLMs）推理成本，同时维持甚至提升性能。


<details>
  <summary>Details</summary>
Motivation: VLMs在多模态推理任务上表现优秀，但由于推理阶段需处理大量视觉token，推理成本高昂。现有裁剪方法不足以利用推理过程中动态生成的内部信号，因此需要更高效的裁剪方法。

Method: 提出AdaptInfer框架，采用动态、逐层的基于文本注意力的视觉token裁剪机制，利用文本到文本注意力图对视觉token进行精细评分。同时，通过离线分析跨模态注意力变化，设计出更高效的裁剪策略。方法轻量且可插拔，适用于多种多模态任务。

Result: 实验表明，AdaptInfer在保证高准确率（92.9%）的前提下，将推理CUDA延迟降低61.3%。在相同token预算下，其准确率超越现有SOTA方法。

Conclusion: AdaptInfer有效提升VLMs推理效率，降低成本，兼具高准确率和广泛适用性，在多模态推理场景具有实际应用潜力。

Abstract: Vision-language models (VLMs) have achieved impressive performance on
multimodal reasoning tasks such as visual question answering (VQA), but their
inference cost remains a significant challenge due to the large number of
vision tokens processed during the prefill stage. Existing pruning methods
often rely on directly using the attention patterns or static text prompt
guidance, failing to exploit the dynamic internal signals generated during
inference. To address these issues, we propose AdaptInfer, a plug-and-play
framework for adaptive vision token pruning in VLMs. First, we introduce a
fine-grained, dynamic text-guided pruning mechanism that reuses layer-wise
text-to-text attention maps to construct soft priors over text-token
importance, allowing more informed scoring of vision tokens at each stage.
Second, we perform an offline analysis of cross-modal attention shifts and
identify consistent inflection locations in inference, which inspire us to
propose a more principled and efficient pruning schedule. Our method is
lightweight and plug-and-play, also generalizable across multi-modal tasks.
Experimental results have verified the effectiveness of the proposed method.
For example, it reduces CUDA latency by 61.3\% while maintaining an average
accuracy of 92.9\% on vanilla LLaVA-1.5-7B. Under the same token budget,
AdaptInfer surpasses SOTA in accuracy.

</details>


### [48] [Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation](https://arxiv.org/abs/2508.06092)
*Yachun Mi,Yu Li,Yanting Li,Shixin Sun,Chen Hui,Tong Zhang,Yuanyuan Liu,Chenyue Song,Shaohui Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉-语言模型（VLMs）的全新高效视频质量评价（VQA）方法Q-CLIP，仅需少量训练参数且能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有VQA方法通常依赖大规模分类数据集预训练（如ImageNet、Kinetics），但这种方案传递的仅是语义知识，无法满足复杂的视频质量需求，并且计算资源消耗极大。视觉-语言模型近期展现了强泛化能力，有望应用于更高效的视频质量评价。

Method: 作者提出Q-CLIP框架，核心为只包含极少数可训练参数的共享跨模态适配器（SCMA），极大减少了训练计算量。同时引入5个可学习的质量级别提示，引导VLMs感知视频质量微小差异，并系统研究了不同帧采样策略对性能的影响。

Result: 实验显示，基于帧差抽样的方法在跨数据集泛化性上表现更佳。Q-CLIP在多个VQA数据集上均取得了优异结果。

Conclusion: Q-CLIP显著减少了训练成本，并提升了VQA性能，表明视觉-语言模型在视频质量评价领域具有广阔应用前景。

Abstract: Accurate and efficient Video Quality Assessment (VQA) has long been a key
research challenge. Current mainstream VQA methods typically improve
performance by pretraining on large-scale classification datasets (e.g.,
ImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this
strategy presents two significant challenges: (1) merely transferring semantic
knowledge learned from pretraining is insufficient for VQA, as video quality
depends on multiple factors (e.g., semantics, distortion, motion, aesthetics);
(2) pretraining on large-scale datasets demands enormous computational
resources, often dozens or even hundreds of times greater than training
directly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown
remarkable generalization capabilities across a wide range of visual tasks, and
have begun to demonstrate promising potential in quality assessment. In this
work, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP
enhances both visual and textual representations through a Shared Cross-Modal
Adapter (SCMA), which contains only a minimal number of trainable parameters
and is the only component that requires training. This design significantly
reduces computational cost. In addition, we introduce a set of five learnable
quality-level prompts to guide the VLMs in perceiving subtle quality
variations, thereby further enhancing the model's sensitivity to video quality.
Furthermore, we investigate the impact of different frame sampling strategies
on VQA performance, and find that frame-difference-based sampling leads to
better generalization performance across datasets. Extensive experiments
demonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.

</details>


### [49] [E-React: Towards Emotionally Controlled Synthesis of Human Reactions](https://arxiv.org/abs/2508.06093)
*Chen Zhu,Buzhen Huang,Zijing Wu,Binghui Zuo,Yangang Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合情感因素进行动作反应生成的新方法，可根据不同情感条件生成自然且多样的人类动作反应，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的人体动作生成方法未充分考虑情感的影响，导致生成的动作缺乏自然性，难以应用于需要交互和情感反应的实际任务。为此，作者希望研究如何让动作生成模型融入情感变量，从而提升其交互性和写实性。

Method: 作者引入了一个新的任务——根据情感线索生成多样的反应动作。他们提出了半监督情感先验，并将其整合至actor-reactor扩散模型中，实现情感驱动的反应合成。方法上，作者基于运动片段短时共享情感的观察，设计了半监督学习框架以训练情感先验；再通过该先验训练反应生成模型，使其同时考虑空间互动和情感反应，能够根据不同情感条件生成相对应的动作反应。

Result: 实验结果表明，所提模型在动作反应生成任务上的表现优于现有主流方法，更好地实现了不同情感条件下的动作反应。

Conclusion: 引入情感先验结合actor-reactor扩散模型的方法可以显著提升动作反应的自然性和多样性，有助于推动情感驱动的人机交互等相关领域的发展。

Abstract: Emotion serves as an essential component in daily human interactions.
Existing human motion generation frameworks do not consider the impact of
emotions, which reduces naturalness and limits their application in interactive
tasks, such as human reaction synthesis. In this work, we introduce a novel
task: generating diverse reaction motions in response to different emotional
cues. However, learning emotion representation from limited motion data and
incorporating it into a motion generation framework remains a challenging
problem. To address the above obstacles, we introduce a semi-supervised emotion
prior in an actor-reactor diffusion model to facilitate emotion-driven reaction
synthesis. Specifically, based on the observation that motion clips within a
short sequence tend to share the same emotion, we first devise a
semi-supervised learning framework to train an emotion prior. With this prior,
we further train an actor-reactor diffusion model to generate reactions by
considering both spatial interaction and emotional response. Finally, given a
motion sequence of an actor, our approach can generate realistic reactions
under various emotional conditions. Experimental results demonstrate that our
model outperforms existing reaction generation methods. The code and data will
be made publicly available at https://ereact.github.io/

</details>


### [50] [UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization](https://arxiv.org/abs/2508.06101)
*Yachun Mi,Xingyang He,Shixin Sun,Yu Li,Yanting Li,Zhixuan Li,Jian Jin,Chen Hui,Shaohui Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的统一生成框架UGD-IML，实现了图像篡改检测与定位（IML）及其受限变体（CIML）的统一建模。该方法在有限标注数据下依然表现出色，在多个数据集上的F1指标超越了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 随着高级图像编辑工具普及，图像伪造带来的视觉内容真实性威胁日益严重。现有方法过分依赖大规模高质量标注数据，且现有数据集规模和多样性有限，影响模型在真实场景下的效果。此外，通过算法监督生成像素级标注的CIML方法存在流程复杂、注释效率低下等问题。

Method: 本文首次提出基于生成型扩散模型的统一框架UGD-IML，将IML和CIML任务整合于单一模型体系。通过学习数据分布，模型可在标注数据稀缺时仍有效工作。模型采用类别嵌入与参数共享设计，实现IML与CIML间自如切换且无额外训练负担，且端到端流程省去多余注释步骤。

Result: 在多个数据集实验证明，UGD-IML在IML与CIML任务上的F1分数分别超过当前最好方法9.66和4.36。同时在不确定性估计、可视化和鲁棒性方面也表现优异。

Conclusion: UGD-IML打破了对大规模高质量标注数据的依赖，简化了注释流程，实现了IML与CIML的统一和高效处理，在多个维度优于现有方法，对于提升图像篡改检测及定位的实用性具有重要意义。

Abstract: In the digital age, advanced image editing tools pose a serious threat to the
integrity of visual content, making image forgery detection and localization a
key research focus. Most existing Image Manipulation Localization (IML) methods
rely on discriminative learning and require large, high-quality annotated
datasets. However, current datasets lack sufficient scale and diversity,
limiting model performance in real-world scenarios. To overcome this, recent
studies have explored Constrained IML (CIML), which generates pixel-level
annotations through algorithmic supervision. However, existing CIML approaches
often depend on complex multi-stage pipelines, making the annotation process
inefficient. In this work, we propose a novel generative framework based on
diffusion models, named UGD-IML, which for the first time unifies both IML and
CIML tasks within a single framework. By learning the underlying data
distribution, generative diffusion models inherently reduce the reliance on
large-scale labeled datasets, allowing our approach to perform effectively even
under limited data conditions. In addition, by leveraging a class embedding
mechanism and a parameter-sharing design, our model seamlessly switches between
IML and CIML modes without extra components or training overhead. Furthermore,
the end-to-end design enables our model to avoid cumbersome steps in the data
annotation process. Extensive experimental results on multiple datasets
demonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and
4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the
proposed method also excels in uncertainty estimation, visualization and
robustness.

</details>


### [51] [MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment](https://arxiv.org/abs/2508.06104)
*Gui Zou,Chaofan Gan,Chern Hong Lim,Supavadee Aramvith,Weiyao Lin*

Main category: cs.CV

TL;DR: 本文提出了一种用于2D-3D跨模态检索的多层次自适应校正与对齐方法（MCA），能有效应对标注噪声，取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有2D-3D跨模态检索在真实任务中常遇到标签不完美（噪声标签），而现有对策容易让模型在各模态上各自分裂处理，从而容易过拟合于错误标签。为提升在噪声环境下的鲁棒性，需要一种更有效的统一处理方案。

Method: 提出MCA框架，包含：1）多模态联合标签修正机制（MJC），通过融合多模态历史自预测，实现标签一致性和修正；2）多层次自适应对齐策略（MAA），在不同语义层次上强化跨模态特征的表达和判别能力。

Result: 大量实验（包括标准和真实噪声3D数据集）表明，MCA方法在各项检索指标上均达到或超越当前最好水平，体现其通用性和有效性。

Conclusion: MCA方法能显著提升2D-3D跨模态检索在有噪声标签条件下的稳健性，具有较强的实用价值和推广潜力。

Abstract: With the increasing availability of 2D and 3D data, significant advancements
have been made in the field of cross-modal retrieval. Nevertheless, the
existence of imperfect annotations presents considerable challenges, demanding
robust solutions for 2D-3D cross-modal retrieval in the presence of noisy label
conditions. Existing methods generally address the issue of noise by dividing
samples independently within each modality, making them susceptible to
overfitting on corrupted labels. To address these issues, we propose a robust
2D-3D \textbf{M}ulti-level cross-modal adaptive \textbf{C}orrection and
\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal
Joint label Correction (MJC) mechanism that leverages multimodal historical
self-predictions to jointly model the modality prediction consistency, enabling
reliable label refinement. Additionally, we propose a Multi-level Adaptive
Alignment (MAA) strategy to effectively enhance cross-modal feature semantics
and discrimination across different levels. Extensive experiments demonstrate
the superiority of our method, MCA, which achieves state-of-the-art performance
on both conventional and realistic noisy 3D benchmarks, highlighting its
generality and effectiveness.

</details>


### [52] [Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention](https://arxiv.org/abs/2508.06107)
*Shree Mitra,Ritabrata Chakraborty,Nilkanta Sahu*

Main category: cs.CV

TL;DR: 该论文提出了一种自监督学习框架来识别手写数学表达式，无需大量标注数据，并在公开基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手写数学表达式识别任务具有结构复杂、符号尺度变化大和符号间空间关系复杂等挑战。传统方法依赖大量人工标注数据，成本高且效率低，因此亟需无需标注或减少标注的高效方法。

Method: 提出先使用全局和局部对比损失自监督预训练图像编码器，再通过渐进式空间掩码策略训练自监督注意力网络，最后用transformer解码器监督细调生成LATEX序列。尤其注意力机制可自适应关注操作符、指数、嵌套记号等数学表达的关键区域。

Result: 在CROHME基准实验中，该方法优于现有自监督和完全监督的主流方法，验证了渐进式注意力机制提升结构理解和识别性能的有效性。

Conclusion: 该自监督注意力方法不依赖大量标注数据，增强了模型对复杂数学表达结构的感知和鲁棒性，有效提升了手写数学表达式识别效果。

Abstract: Recognizing handwritten mathematical expressions (HMER) is a challenging task
due to the inherent two-dimensional structure, varying symbol scales, and
complex spatial relationships among symbols. In this paper, we present a
self-supervised learning (SSL) framework for HMER that eliminates the need for
expensive labeled data. Our approach begins by pretraining an image encoder
using a combination of global and local contrastive loss, enabling the model to
learn both holistic and fine-grained representations. A key contribution of
this work is a novel self-supervised attention network, which is trained using
a progressive spatial masking strategy. This attention mechanism is designed to
learn semantically meaningful focus regions, such as operators, exponents, and
nested mathematical notation, without requiring any supervision. The
progressive masking curriculum encourages the network to become increasingly
robust to missing or occluded visual information, ultimately improving
structural understanding. Our complete pipeline consists of (1) self-supervised
pretraining of the encoder, (2) self-supervised attention learning, and (3)
supervised fine-tuning with a transformer decoder to generate LATEX sequences.
Extensive experiments on CROHME benchmarks demonstrate that our method
outperforms existing SSL and fully supervised baselines, validating the
effectiveness of our progressive attention mechanism in enhancing HMER
performance. Our codebase can be found here.

</details>


### [53] [FMCE-Net++: Feature Map Convergence Evaluation and Training](https://arxiv.org/abs/2508.06109)
*Zhibo Zhu,Renyu Huang,Lei He*

Main category: cs.CV

TL;DR: 本文提出了一种名为FMCE-Net++的新型训练框架，能通过特征图收敛评分辅助主任务优化，进而提升模型性能，在多项标准数据集和主流网络结构上取得了显著的准确率提升。


<details>
  <summary>Details</summary>
Motivation: DNN的特征表征难以解释，现有特征图收敛评估（FMCE）虽可量化模块收敛情况，但缺乏实验验证和在闭环系统中的应用。为解决该缺陷，作者尝试将FMCE指标融入实际训练过程。

Method: 提出FMCE-Net++训练框架，将预训练、冻结的FMCE-Net作为辅助头，生成特征收敛分数(FMCS)，并与标签共同参与监督主干网络，通过引入Representation Auxiliary Loss（RAL），动态平衡分类损失和特征收敛的优化。

Result: 在MNIST、CIFAR-10、FashionMNIST、CIFAR-100等数据集上测试，FMCE-Net++在无需修改模型结构和训练数据的情况下，显著提升了如ResNet-50、ShuffleNet v2等主流模型的准确率，提升幅度如CIFAR-10上ResNet-50为+1.16个百分点，CIFAR-100上ShuffleNet v2为+1.08个百分点。

Conclusion: FMCE-Net++能有效将特征收敛信息纳入训练过程，在不增加模型复杂度或数据的前提下，提升主流模型的性能，具备广泛实用价值。

Abstract: Deep Neural Networks (DNNs) face interpretability challenges due to their
opaque internal representations. While Feature Map Convergence Evaluation
(FMCE) quantifies module-level convergence via Feature Map Convergence Scores
(FMCS), it lacks experimental validation and closed-loop integration. To
address this limitation, we propose FMCE-Net++, a novel training framework that
integrates a pretrained, frozen FMCE-Net as an auxiliary head. This module
generates FMCS predictions, which, combined with task labels, jointly supervise
backbone optimization through a Representation Auxiliary Loss. The RAL
dynamically balances the primary classification loss and feature convergence
optimization via a tunable \Representation Abstraction Factor. Extensive
experiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100
demonstrate that FMCE-Net++ consistently enhances model performance without
architectural modifications or additional data. Key experimental outcomes
include accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp
(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate
state-of-the-art performance ceilings.

</details>


### [54] [SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.06115)
*Weichen Zhang,Kebin Liu,Fan Dang,Zhui Zhu,Xikai Sun,Yunhao Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的弱监督语义分割方法SynSeg，在无需强标签的情况下实现了更精细和泛化的分割，超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 开放词汇场景语义分割难点在于类别多、粒度细，现有弱监督方法由于依赖类别特定监督和不适合的对比特征构建，导致语义对齐差、性能低下，亟需新的弱监督方法提升分割表现。

Method: 作者提出SynSeg方法，包含两个关键技术：1）多类别对比学习（MCCL），同时对同类和异类特征进行对齐和分离，强化特征区分能力；2）特征协同结构（FSS）框架，通过先验融合和语义激活图增强重建用于对比学习的判别性特征，有效减弱编码器带来的前景偏置。

Result: 在VOC、Context、Object、City等常用分割数据集上，SynSeg较SOTA分别提升4.5%、8.9%、2.6%、2.0%的准确率，表现优异。

Conclusion: SynSeg能有效提升开放词汇弱监督分割的语义定位和判别能力，在多个数据集上刷新了现有最优性能，为弱监督语义分割提供了新的思路。

Abstract: Semantic segmentation in open-vocabulary scenarios presents significant
challenges due to the wide range and granularity of semantic categories.
Existing weakly-supervised methods often rely on category-specific supervision
and ill-suited feature construction methods for contrastive learning, leading
to semantic misalignment and poor performance. In this work, we propose a novel
weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs
Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a
new feature reconstruction framework named Feature Synergy Structure (FSS).
Specifically, MCCL strategy robustly combines both intra- and inter-category
alignment and separation in order to make the model learn the knowledge of
correlations from different categories within the same image. Moreover, FSS
reconstructs discriminative features for contrastive learning through prior
fusion and semantic-activation-map enhancement, effectively avoiding the
foreground bias introduced by the visual encoder. In general, SynSeg
effectively improves the abilities in semantic localization and discrimination
under weak supervision. Extensive experiments on benchmarks demonstrate that
our method outperforms state-of-the-art (SOTA) performance. For instance,
SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% on
Context, 2.6\% on Object and 2.0\% on City.

</details>


### [55] [Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events](https://arxiv.org/abs/2508.06122)
*Ting-Shuo Yo,Shih-Hao Su,Chien-Ming Wu,Wei-Ting Chen,Jung-Lien Chu,Chiao-Wei Chang,Hung-Chi Kuo*

Main category: cs.CV

TL;DR: 本文对比了不同表示学习方法在卫星气象图像上的表现，发现卷积自编码器（CAE）学习到的潜在空间对天气事件分类效果最好。


<details>
  <summary>Details</summary>
Motivation: 卫星气象图像数据量大、特征复杂，传统方法难以高效表征和分类。研究希望通过先进的表示学习方法提升自动识别和分类多种天气事件的能力。

Method: 作者对比了三种表示学习方法：线性主成分分析（PCA）、卷积自编码器（CAE）和预训练残差网络（PT），并将它们应用于不同分辨率的卫星图像，评估其在多种天气事件分类中的表现。还分析了不同潜在空间维度对分类性能的影响。

Result: CAE的潜在空间在各种分类任务中表现最优，威胁得分（threat scores）最高。PCA命中率高但误报率也高；PT在热带气旋识别中表现突出但其他任务不佳。更高分辨率数据可提升CAE和PT表现，潜在空间小于128维将急剧提高误报率。

Conclusion: CAE在卫星图像表征与分类方面效果优越，但其潜在空间缺乏物理意义的解释性。未来应开发物理信息融入的CAE模型以增强其物理可解释性和应用潜力。

Abstract: This study applied representation learning algorithms to satellite images and
evaluated the learned latent spaces with classifications of various weather
events. The algorithms investigated include the classical linear
transformation, i.e., principal component analysis (PCA), state-of-the-art deep
learning method, i.e., convolutional autoencoder (CAE), and a residual network
pre-trained with large image datasets (PT). The experiment results indicated
that the latent space learned by CAE consistently showed higher threat scores
for all classification tasks. The classifications with PCA yielded high hit
rates but also high false-alarm rates. In addition, the PT performed
exceptionally well at recognizing tropical cyclones but was inferior in other
tasks. Further experiments suggested that representations learned from
higher-resolution datasets are superior in all classification tasks for
deep-learning algorithms, i.e., CAE and PT. We also found that smaller latent
space sizes had minor impact on the classification task's hit rate. Still, a
latent space dimension smaller than 128 caused a significantly higher false
alarm rate. Though the CAE can learn latent spaces effectively and efficiently,
the interpretation of the learned representation lacks direct connections to
physical attributions. Therefore, developing a physics-informed version of CAE
can be a promising outlook for the current work.

</details>


### [56] [SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning](https://arxiv.org/abs/2508.06125)
*Lin Zhang,Xianfang Zeng,Kangcong Li,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: 本文提出了SC-Captioner，一种具有自我纠错能力的图像描述生成模型，通过精巧的奖励机制和精细化数据集，提升了大模型生成描述的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 目前的图像描述模型缺乏自我纠错能力，难以灵活修正生成过程中出现的错误，且现有评价方法和数据集精度有限，难以真实衡量生成质量。

Method: 1. 提出SC-Captioner，通过强化学习框架，使模型能够自我修正生成的描述；2. 利用场景图解析，将生成描述和参考描述分解为“对象、属性、关系”集合，比较原始与修正后的差异并制定奖励与惩罚机制；3. 优化评价指标，针对CAPTURE方法的不足进行了改进；4. 构建了RefinedCaps微调数据集，提升了评测精度。

Result: 实验证明，SC-Captioner应⽤于大型视觉语言模型后，在多种场景下生成的图像描述质量明显优于直接偏好优化方法。

Conclusion: SC-Captioner为图像描述生成带来了可自纠的进化机制，并通过改进评价指标与精细标注数据集，推动了图像描述任务的实际应用效果。

Abstract: We propose SC-Captioner, a reinforcement learning framework that enables the
self-correcting capability of image caption models. Our crucial technique lies
in the design of the reward function to incentivize accurate caption
corrections. Specifically, the predicted and reference captions are decomposed
into object, attribute, and relation sets using scene-graph parsing algorithms.
We calculate the set difference between sets of initial and self-corrected
captions to identify added and removed elements. These elements are matched
against the reference sets to calculate correctness bonuses for accurate
refinements and mistake punishments for wrong additions and removals, thereby
forming the final reward. For image caption quality assessment, we propose a
set of metrics refined from CAPTURE that alleviate its incomplete precision
evaluation and inefficient relation matching problems. Furthermore, we collect
a fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K
diverse images from COCO dataset. Experiments show that applying SC-Captioner
on large visual-language models can generate better image captions across
various scenarios, significantly outperforming the direct preference
optimization training strategy.

</details>


### [57] [SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures](https://arxiv.org/abs/2508.06127)
*Yi Qin,Rui Wang,Tao Huang,Tong Xiao,Liping Jing*

Main category: cs.CV

TL;DR: 本论文针对Segment Anything Model (SAM) 的对抗性脆弱性，提出一种新的攻击方法VeSCA，显著提升了攻击的可迁移性，对下游应用构成风险。


<details>
  <summary>Details</summary>
Motivation: SAM因其零样本能力而应用广泛，但其自身脆弱点可能导致许多下游任务失败，因此急需评估和揭示这些跨领域可迁移的脆弱性。

Method: 提出了Vertex-Refining Simplicial Complex Attack（VeSCA）方法，只利用SAM的编码器生成对抗样本。通过参数化的单纯形复合体，显式描述SAM与下游模型间共享的脆弱区域，并通过逐顶点迭代优化查找对抗性强的区域。同时引入了轻量的领域再适应策略，用极少的参考数据初始化单纯形结构以弥合领域差异。最后通过随机采样产生高可迁移性的对抗样本。

Result: 在包含五个领域数据集、三类下游任务的实验中，VeSCA方法的对抗转移性能比最新方法平均提升12.7%。

Conclusion: VeSCA方法显著提升了对SAM及其下游应用的对抗攻击能力，揭示了现有大模型潜在的共性安全风险，强调了提高基础模型鲁棒性的紧迫性。

Abstract: While the Segment Anything Model (SAM) transforms interactive segmentation
with zero-shot abilities, its inherent vulnerabilities present a single-point
risk, potentially leading to the failure of numerous downstream applications.
Proactively evaluating these transferable vulnerabilities is thus imperative.
Prior adversarial attacks on SAM often present limited transferability due to
insufficient exploration of common weakness across domains. To address this, we
propose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that
leverages only the encoder of SAM for generating transferable adversarial
examples. Specifically, it achieves this by explicitly characterizing the
shared vulnerable regions between SAM and downstream models through a
parametric simplicial complex. Our goal is to identify such complexes within
adversarially potent regions by iterative vertex-wise refinement. A lightweight
domain re-adaptation strategy is introduced to bridge domain divergence using
minimal reference data during the initialization of simplicial complex.
Ultimately, VeSCA generates consistently transferable adversarial examples
through random simplicial complex sampling. Extensive experiments demonstrate
that VeSCA achieves performance improved by 12.7% compared to state-of-the-art
methods across three downstream model categories across five domain-specific
datasets. Our findings further highlight the downstream model risks posed by
SAM's vulnerabilities and emphasize the urgency of developing more robust
foundation models.

</details>


### [58] [Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation](https://arxiv.org/abs/2508.06136)
*YoungChan Choi,HengFei Wang,YiHua Cheng,Boeun Kim,Hyung Jin Chang,YoungGeun Choi,Sang-Il Choi*

Main category: cs.CV

TL;DR: 提出一种新颖的基于3D显式眼球结构的凝视重定向方法，通过3D高斯涂抹对眼球进行建模，并引入自适应变形模块提升细腻动态表现，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有凝视重定向方法多依赖隐式神经辐射场（NeRF），难以精确建模3D旋转与平移，无法真实模拟眼球运动和细微肌肉变化，因此需要更显式、更可控的建模方式。

Method: 创新性地以3D高斯涂抹（3D Gaussian Splatting, 3DGS）显式建模眼球，通过直接旋转和平移眼球结构实现凝视方向调整，并提出自适应变形模块，用以再现眼睛周围细微肌肉运动。

Result: 在ETH-XGaze公开数据集上的实验表明，该方法能生成多样化的新视角高保真凝视图像，在图像质量和凝视估计准确率两方面均超越当前主流方法。

Conclusion: 所提出的显式3D眼球建模及自适应形变方法，有效提升了凝视重定向任务的图像真实度和精确度，为相关应用提供了更优解决方案。

Abstract: We propose a novel 3D gaze redirection framework that leverages an explicit
3D eyeball structure. Existing gaze redirection methods are typically based on
neural radiance fields, which employ implicit neural representations via volume
rendering. Unlike these NeRF-based approaches, where the rotation and
translation of 3D representations are not explicitly modeled, we introduce a
dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian
Splatting (3DGS). Our method generates photorealistic images that faithfully
reproduce the desired gaze direction by explicitly rotating and translating the
3D eyeball structure. In addition, we propose an adaptive deformation module
that enables the replication of subtle muscle movements around the eyes.
Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our
framework is capable of generating diverse novel gaze images, achieving
superior image quality and gaze estimation accuracy compared to previous
state-of-the-art methods.

</details>


### [59] [DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera](https://arxiv.org/abs/2508.06139)
*Shaohua Pan,Xinyu Yi,Yan Zhou,Weihua Jian,Yuan Zhang,Pengfei Wan,Feng Xu*

Main category: cs.CV

TL;DR: 本论文提出了一种结合稀疏IMU与单目相机的、基于扩散模型的人体动作捕捉新方法，实现了实时、高精度的姿态估计，在现有方法中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 人体动作捕捉常常受限于视觉信息的遮挡和视野限制，单纯依赖IMU或相机各有局限。如何将两者优势互补，特别是在视觉信息丢失时，依然保持高精度实时捕捉，是本研究关注的问题。

Method: 采用扩散模型学习人体运动先验，并将视觉信息集成为整体的条件嵌入，IMU数据则按帧拼接到带噪音的位姿序列中，形成扩散模型的序列输入。这种方式兼顾两种信号特点：视觉特征用于全局条件，IMU作为时序信息输入，从而提升鲁棒性。

Result: 实验验证了系统设计的有效性，在姿态估计任务上取得了领先效果，优于以往方法。

Conclusion: 该方法能在遭遇遮挡或视野丢失时，凭借IMU保证稳定性，并利用视觉全局特征提升姿态估计精度，展现了很高的应用潜力。

Abstract: Combining sparse IMUs and a monocular camera is a new promising setting to
perform real-time human motion capture. This paper proposes a diffusion-based
solution to learn human motion priors and fuse the two modalities of signals
together seamlessly in a unified framework. By delicately considering the
characteristics of the two signals, the sequential visual information is
considered as a whole and transformed into a condition embedding, while the
inertial measurement is concatenated with the noisy body pose frame by frame to
construct a sequential input for the diffusion model. Firstly, we observe that
the visual information may be unavailable in some frames due to occlusions or
subjects moving out of the camera view. Thus incorporating the sequential
visual features as a whole to get a single feature embedding is robust to the
occasional degenerations of visual information in those frames. On the other
hand, the IMU measurements are robust to occlusions and always stable when
signal transmission has no problem. So incorporating them frame-wisely could
better explore the temporal information for the system. Experiments have
demonstrated the effectiveness of the system design and its state-of-the-art
performance in pose estimation compared with the previous works. Our codes are
available for research at https://shaohua-pan.github.io/diffcap-page.

</details>


### [60] [SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models](https://arxiv.org/abs/2508.06142)
*Hanqing Wang,Yuan Tian,Mingyu Liu,Zhenhao Zhang,Xiangyang Zhu*

Main category: cs.CV

TL;DR: 本文提出了一个名为SDEval的动态安全评测框架，通过文本、图像及其融合的动态策略生成新样本，有效评估多模态大模型（MLLM）输出的安全性，并能缓解已有基准被污染及过时的问题。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型的快速发展，其输出的安全性问题得到越来越多关注。现有评测数据集可能因模型更新而失效或受到数据污染，因此需要更灵活、动态的新方法来评估模型安全性。

Method: SDEval采用文本、图像、文本-图像三种动态生成策略，从原有安全基准生成新的样本。通过单独和组合方式注入动态性，考察不同动态对模型安全评测的影响，可扩展至多种现有安全与能力基准。

Result: 在多个主流安全和能力评测基准（如MLLMGuard、VLSBench、MMBench、MMVet）上的实验表明，SDEval不仅能显著影响安全性评估，还能减轻数据污染，揭示多模态模型的安全局限性。

Conclusion: SDEval为多模态大模型的安全动态评测提供全面手段，灵活应对模型演进带来的挑战，助力模型安全性更科学有效地评估。

Abstract: In the rapidly evolving landscape of Multimodal Large Language Models
(MLLMs), the safety concerns of their outputs have earned significant
attention. Although numerous datasets have been proposed, they may become
outdated with MLLM advancements and are susceptible to data contamination
issues. To address these problems, we propose \textbf{SDEval}, the
\textit{first} safety dynamic evaluation framework to controllably adjust the
distribution and complexity of safety benchmarks. Specifically, SDEval mainly
adopts three dynamic strategies: text, image, and text-image dynamics to
generate new samples from original benchmarks. We first explore the individual
effects of text and image dynamics on model safety. Then, we find that
injecting text dynamics into images can further impact safety, and conversely,
injecting image dynamics into text also leads to safety risks. SDEval is
general enough to be applied to various existing safety and even capability
benchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and
capability benchmarks, MMBench and MMVet, show that SDEval significantly
influences safety evaluation, mitigates data contamination, and exposes safety
limitations of MLLMs. Code is available at https://github.com/hq-King/SDEval

</details>


### [61] [Text-guided Visual Prompt DINO for Generic Segmentation](https://arxiv.org/abs/2508.06146)
*Yuchen Guan,Chong Sun,Canmiao Fu,Zhipeng Huang,Chun Yuan,Chen Li*

Main category: cs.CV

TL;DR: 本文提出了Prompt-DINO框架，通过早期融合、多模态查询对齐和生成式数据引擎，有效提升了开放世界分割和检测的性能，解决了原有模型融合滞后、查询选择不佳和语义覆盖受限等问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态视觉模型在特征迟滞融合、语义模糊及固定词汇表等方面存在局限，难以应对开放世界分割任务的多样性和扩展性需求。

Method: 1. 设计文本与视觉Prompt及主干特征的早期融合，增强模态间交互；2. 提出针对DETR结构的顺序对齐查询选择，优化文本与视觉查询的结构对齐和空间一致性；3. 利用RAP模型开发生成数据引擎，通过双路径交叉验证生成海量高质量标注样本，有效降低标注噪声。

Result: Prompt-DINO在开放世界检测基准任务上取得了最新最优性能，语义覆盖范围大幅提升，生成的数据集噪声相比传统方法降低80.5%。

Conclusion: Prompt-DINO为开放世界场景下的多模态检测和数据生成提供了全新可扩展范式，有效突破了现有方法的语义和任务局限。

Abstract: Recent advancements in multimodal vision models have highlighted limitations
in late-stage feature fusion and suboptimal query selection for hybrid prompts
open-world segmentation, alongside constraints from caption-derived
vocabularies. To address these challenges, we propose Prompt-DINO, a
text-guided visual Prompt DINO framework featuring three key innovations.
First, we introduce an early fusion mechanism that unifies text/visual prompts
and backbone features at the initial encoding stage, enabling deeper
cross-modal interactions to resolve semantic ambiguities. Second, we design
order-aligned query selection for DETR-based architectures, explicitly
optimizing the structural alignment between text and visual queries during
decoding to enhance semantic-spatial consistency. Third, we develop a
generative data engine powered by the Recognize Anything via Prompting (RAP)
model, which synthesizes 0.5B diverse training instances through a dual-path
cross-verification pipeline, reducing label noise by 80.5% compared to
conventional approaches. Extensive experiments demonstrate that Prompt-DINO
achieves state-of-the-art performance on open-world detection benchmarks while
significantly expanding semantic coverage beyond fixed-vocabulary constraints.
Our work establishes a new paradigm for scalable multimodal detection and data
generation in open-world scenarios. Data&Code are available at
https://github.com/WeChatCV/WeVisionOne.

</details>


### [62] [DSConv: Dynamic Splitting Convolution for Pansharpening](https://arxiv.org/abs/2508.06147)
*Xuanyu Liu,Bonan An*

Main category: cs.CV

TL;DR: 本文提出了一种用于遥感图像融合（pansharpening）的新型动态分割卷积核（DSConv）方法，并展现了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 目前遥感图像融合主要通过标准卷积实现，高分辨率融合仍具有挑战性。标准卷积难以适应遥感图像中像素间相关性，现有方法对自适应卷积关注较少。因此，研究更有效的特征提取方法具有重要意义。

Method: 提出动态分割卷积（DSConv）：结合注意力机制，动态选择感兴趣区域，将原卷积核按位置分割为多个更小的卷积核，以实现更有效的特征提取。基于DSConv设计了新的pansharpening网络结构。

Result: 本文方法在多项公平实验中达到了最先进的融合效果，展示了DSConv在泛化、优化、特征表达等方面的提升。

Conclusion: DSConv在遥感图像融合任务中更有效地提取不同空间区域特征，显著优于现有方法，并且作者提供了对优劣条件的充分分析。

Abstract: Aiming to obtain a high-resolution image, pansharpening involves the fusion
of a multi-spectral image (MS) and a panchromatic image (PAN), the low-level
vision task remaining significant and challenging in contemporary research.
Most existing approaches rely predominantly on standard convolutions, few
making the effort to adaptive convolutions, which are effective owing to the
inter-pixel correlations of remote sensing images. In this paper, we propose a
novel strategy for dynamically splitting convolution kernels in conjunction
with attention, selecting positions of interest, and splitting the original
convolution kernel into multiple smaller kernels, named DSConv. The proposed
DSConv more effectively extracts features of different positions within the
receptive field, enhancing the network's generalization, optimization, and
feature representation capabilities. Furthermore, we innovate and enrich
concepts of dynamic splitting convolution and provide a novel network
architecture for pansharpening capable of achieving the tasks more efficiently,
building upon this methodology. Adequate fair experiments illustrate the
effectiveness and the state-of-the-art performance attained by
DSConv.Comprehensive and rigorous discussions proved the superiority and
optimal usage conditions of DSConv.

</details>


### [63] [VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation](https://arxiv.org/abs/2508.06152)
*Kaiyuan Jiang,Ruoxi Sun,Ying Cao,Yuqi Xu,Xinran Zhang,Junyan Guo,ChengSheng Deng*

Main category: cs.CV

TL;DR: 本文提出了VISTAR，这是一个专注于用户视角、具有多维度的文本生成图像（T2I）评测基准，并通过结合可脚本化的物理属性指标与创新的分层加权问题（HWPQ）法，解决了现有T2I评价指标的不足。该基准以专家德尔菲研究为基础，综合人工评分与多维度需求，并在大规模数据验证下，证实了其评价结果与人工高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有的T2I评测指标往往局限在单一维度，且难以较好地反映实际用户的多样化需求和对抽象语义的理解，因此急需一个更全面、更贴合实际用户体验的评测体系。

Method: VISTAR采用了两层混合范式：一方面对可量化物理属性（如文本渲染、灯光效果）使用确定性的、可脚本的指标评测；另一方面，提出了分层加权问答（HWPQ）新方案，利用受限的视觉语言模型，评估风格、文化等抽象语义。通过德尔菲专家研究，界定了7种用户角色和9个评估维度，并构建了包含2845条提示、超1.5万对人工对比的数据集。

Result: 所提出的度量指标与人工判断高度一致（>75%），其中HWPQ在抽象语义评测上达到85.9%的准确率，显著超越了现有VQA基线。在系统评估主流T2I模型时未出现绝对最优者，不同维度和角色权重下模型排名发生显著变化。

Conclusion: VISTAR显著提升了T2I模型评测的科学性与实际指导意义，可为领域部署提供更精准的模型选择建议，所有资源公开，助力可复现的T2I评测。

Abstract: We present VISTAR, a user-centric, multi-dimensional benchmark for
text-to-image (T2I) evaluation that addresses the limitations of existing
metrics. VISTAR introduces a two-tier hybrid paradigm: it employs
deterministic, scriptable metrics for physically quantifiable attributes (e.g.,
text rendering, lighting) and a novel Hierarchical Weighted P/N Questioning
(HWPQ) scheme that uses constrained vision-language models to assess abstract
semantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study
with 120 experts, we defined seven user roles and nine evaluation angles to
construct the benchmark, which comprises 2,845 prompts validated by over 15,000
human pairwise comparisons. Our metrics achieve high human alignment (>75%),
with the HWPQ scheme reaching 85.9% accuracy on abstract semantics,
significantly outperforming VQA baselines. Comprehensive evaluation of
state-of-the-art models reveals no universal champion, as role-weighted scores
reorder rankings and provide actionable guidance for domain-specific
deployment. All resources are publicly released to foster reproducible T2I
assessment.

</details>


### [64] [An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06157)
*Xiaoxiao Yang,Meiliang Liu,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新型深度学习框架MPF-KANSC，通过多平面融合与先进注意力机制提升了阿尔茨海默病（AD）MRI图像的诊断效果，并揭示了脑部结构变化的新特征。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的早期、精准诊断因脑部细微结构变化而十分困难，目前深度学习方法多只分析单一MRI切面，难以深入捕捉复杂、非线性的脑区病变联系，限制了其对萎缩特征的识别能力。因此急需一种能更全面、高效表征脑结构特征的方法。

Method: 提出MPF-KANSC框架：通过多平面融合（MPF）联合冠状面、矢状面和轴状面，多通路提取高维结构信息；引入Kolmogorov-Arnold Network引导的空间-通道注意力机制（KANSC），提升非线性特征表达能力，并精准定位病变区域。

Result: 在ADNI数据集上的实验显示，MPF-KANSC在阿尔茨海默病诊断任务上取得了优于现有方法的性能。同时，模型解释结果强调了AD进展期间脑部皮层下结构的右侧不对称变化。

Conclusion: MPF-KANSC框架不仅提升了AD影像诊断准确率，还具备较好的可解释性，有助于理解疾病进展机制，显示出在神经影像辅助诊断中的广阔应用前景。

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder that
severely impairs cognitive function and quality of life. Timely intervention in
AD relies heavily on early and precise diagnosis, which remains challenging due
to the complex and subtle structural changes in the brain. Most existing deep
learning methods focus only on a single plane of structural magnetic resonance
imaging (sMRI) and struggle to accurately capture the complex and nonlinear
relationships among pathological regions of the brain, thus limiting their
ability to precisely identify atrophic features. To overcome these limitations,
we propose an innovative framework, MPF-KANSC, which integrates multi-plane
fusion (MPF) for combining features from the coronal, sagittal, and axial
planes, and a Kolmogorov-Arnold Network-guided spatial-channel attention
mechanism (KANSC) to more effectively learn and represent sMRI atrophy
features. Specifically, the proposed model enables parallel feature extraction
from multiple anatomical planes, thus capturing more comprehensive structural
information. The KANSC attention mechanism further leverages a more flexible
and accurate nonlinear function approximation technique, facilitating precise
identification and localization of disease-related abnormalities. Experiments
on the ADNI dataset confirm that the proposed MPF-KANSC achieves superior
performance in AD diagnosis. Moreover, our findings provide new evidence of
right-lateralized asymmetry in subcortical structural changes during AD
progression, highlighting the model's promising interpretability.

</details>


### [65] [Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment](https://arxiv.org/abs/2508.06160)
*Zhenbang Du,Yonggan Fu,Lifu Wang,Jiayi Qian,Xiao Luo,Yingyan,Lin*

Main category: cs.CV

TL;DR: 本文提出了一种名为PostDiff的无训练加速扩散模型方法，旨在提升扩散模型的运行效率，尤其适用于资源受限的平台。


<details>
  <summary>Details</summary>
Motivation: 扩散模型尽管在生成任务中表现优异，但其高昂的计算成本限制了其在资源有限平台上的部署。本文关心在无需微调、后处理的前提下，提升效率是减少去噪步数还是降低每步计算成本更为有效。

Method: 提出PostDiff框架，包括输入层面的混合分辨率去噪策略（前期采用低分辨率提升低频保真度）与模块层面的模块缓存机制（不同去噪阶段复用计算结果），均为后训练（post-training）方法，无需模型再训练。

Result: 实验和消融研究证明：1）PostDiff显著提升了主流扩散模型在保真度和效率之间的权衡表现；2）为提效且保证生成质量，降低单步推理成本通常比单纯减少去噪步骤更有效。

Conclusion: 对比不同加速方案，本文发现以降低每步推理成本为主要手段，可在不牺牲生成质量的前提下有效加速扩散模型，并为后训练压缩扩散模型提供了新的思路。

Abstract: Diffusion models have shown remarkable success across generative tasks, yet
their high computational demands challenge deployment on resource-limited
platforms. This paper investigates a critical question for compute-optimal
diffusion model deployment: Under a post-training setting without fine-tuning,
is it more effective to reduce the number of denoising steps or to use a
cheaper per-step inference? Intuitively, reducing the number of denoising steps
increases the variability of the distributions across steps, making the model
more sensitive to compression. In contrast, keeping more denoising steps makes
the differences smaller, preserving redundancy, and making post-training
compression more feasible. To systematically examine this, we propose PostDiff,
a training-free framework for accelerating pre-trained diffusion models by
reducing redundancy at both the input level and module level in a post-training
manner. At the input level, we propose a mixed-resolution denoising scheme
based on the insight that reducing generation resolution in early denoising
steps can enhance low-frequency components and improve final generation
fidelity. At the module level, we employ a hybrid module caching strategy to
reuse computations across denoising steps. Extensive experiments and ablation
studies demonstrate that (1) PostDiff can significantly improve the
fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to
boost efficiency while maintaining decent generation fidelity, reducing
per-step inference cost is often more effective than reducing the number of
denoising steps. Our code is available at
https://github.com/GATECH-EIC/PostDiff.

</details>


### [66] [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](https://arxiv.org/abs/2508.06169)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Changting Lin,Jianfeng Dong,Chaochao Chen,Xun Zhou,Meng Han*

Main category: cs.CV

TL;DR: 本论文提出了一种名为UW-3DGS的新方法，有效提升了水下三维场景重建的几何和色彩保真度，且在主流数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 水下三维重建因光的吸收、散射和浑浊度等问题导致图像质量退化，几何和颜色信息难以准确恢复。现有方法如NeRF及其物理模型扩展在效率和空间分辨率上都存在局限性，需要开发更高效鲁棒的重建方法。

Method: 作者提出UW-3DGS框架：一是引入可学习的水下图像形成模块，基于体素回归估算空间变化的衰减和反向散射；二是设计了物理感知的不确定性剪枝分支（PAUP），通过不确定性评分自适应去除伪影噪声点。训练阶段，联合优化高斯及水下参数，渲染阶段输出去介质影响的清晰图像以及物理感知的真实水下图像。

Result: 在SeaThru-NeRF和UWBundle数据集上，UW-3DGS表现优异：在SeaThru-NeRF上PSNR达到27.604，SSIM 0.868，LPIPS 0.104，并且浮动伪影降低约65%。

Conclusion: UW-3DGS显著提升了水下三维场景重建的准确性和图像保真度，在恶劣水下环境下也能高效执行，为实际水下重建任务带来了更为可靠的技术方案。

Abstract: Underwater 3D scene reconstruction faces severe challenges from light
absorption, scattering, and turbidity, which degrade geometry and color
fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF
extensions such as SeaThru-NeRF incorporate physics-based models, their MLP
reliance limits efficiency and spatial resolution in hazy environments. We
introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for
robust underwater reconstruction. Key innovations include: (1) a plug-and-play
learnable underwater image formation module using voxel-based regression for
spatially varying attenuation and backscatter; and (2) a Physics-Aware
Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating
Gaussians via uncertainty scoring, ensuring artifact-free geometry. The
pipeline operates in training and rendering stages. During training, noisy
Gaussians are optimized end-to-end with underwater parameters, guided by PAUP
pruning and scattering modeling. In rendering, refined Gaussians produce clean
Unattenuated Radiance Images (URIs) free from media effects, while learned
physics enable realistic Underwater Images (UWIs) with accurate light
transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior
performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on
SeaThru-NeRF, with ~65% reduction in floating artifacts.

</details>


### [67] [Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation](https://arxiv.org/abs/2508.06170)
*Ojonugwa Oluwafemi Ejiga Peter,Akingbola Oluwapemiisin,Amalahu Chetachi,Adeniran Opeyemi,Fahmi Khalifa,Md Mahmudur Rahman*

Main category: cs.CV

TL;DR: 本研究提出了一种面向肠镜图像的多方向架构框架，实现了息肉自动检测和分割。通过Stable Diffusion生成合成数据，并结合Faster R-CNN和SAM等先进模型，提升了检测与分割精度。多种分割模型的实验结果显示，FPN在PSNR和SSIM上表现最佳，UNet在召回率上表现突出，LinkNet在IoU和Dice得分上表现均衡。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌是全球癌症相关死亡的主要原因，肠镜作为早期诊断结直肠癌的重要手段，其自动化检测息肉的技术对于预防和早诊具有重要意义。然而，由于医疗数据集体量有限和标注复杂，现有自动检测方法受到限制。为此，研究旨在通过数据扩充与新型检测分割框架，提升肠镜图像息肉检测的自动化水平和精度。

Method: 首先利用Stable Diffusion生成合成肠镜图像数据，扩充数据集；然后采用Faster R-CNN进行初步息肉目标定位，随后利用Segment Anything Model (SAM)对分割掩膜进行细化。进一步，比较了五种以ResNet34为基底的分割算法（U-Net, PSPNet, FPN, LinkNet, MANet），系统评估各模型在不同指标下的表现。

Result: Faster R-CNN检测达到93.08%召回率、88.97%精度，F1分数为90.98%。分割任务中，FPN在PSNR（7.206）和SSIM（0.492）两项指标上表现最佳，UNet在召回率（84.85%）方面优异，LinkNet在IoU（64.20%）和Dice得分（77.53%）上均表现平衡。

Conclusion: 通过合成数据增强、检测和高级分割方法的结合，显著提升了肠镜息肉自动检测与分割的性能。各模型适用情形不同，FPN适合静态特征精度要求高的场景，UNet适合召回率优先的任务，LinkNet则在综合表现上较为均衡，为实际临床应用提供了更有效的技术解决方案。

Abstract: Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,
which is one of the main causes of cancer-related mortality globally; hence, it
is deemed an essential technique for the prevention and early detection of
colorectal cancer. The research introduces a unique multidirectional
architectural framework to automate polyp detection within colonoscopy images
while helping resolve limited healthcare dataset sizes and annotation
complexities. The research implements a comprehensive system that delivers
synthetic data generation through Stable Diffusion enhancements together with
detection and segmentation algorithms. This detection approach combines Faster
R-CNN for initial object localization while the Segment Anything Model (SAM)
refines the segmentation masks. The faster R-CNN detection algorithm achieved a
recall of 93.08% combined with a precision of 88.97% and an F1 score of
90.98%.SAM is then used to generate the image mask. The research evaluated five
state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,
and MANet using ResNet34 as a base model. The results demonstrate the superior
performance of FPN with the highest scores of PSNR (7.205893) and SSIM
(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced
performance in IoU (64.20%) and Dice score (77.53%).

</details>


### [68] [MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration](https://arxiv.org/abs/2508.06189)
*Cheng Liu,Daou Zhang,Tingxu Liu,Yuhan Wang,Jinyang Chen,Yuexuan Li,Xinying Xiao,Chenbo Xin,Ziru Wang,Weichao Wu*

Main category: cs.CV

TL;DR: 本文提出了一种基于多智能体异步协作的犯罪行为预测框架（MA-CBP），能够通过对视频流进行多层次语义解析实现早期预警，并在多个数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加速，公共场景中的犯罪行为日益威胁社会安全。现有检测方法难以高效捕获行为语义或满足实时性需求，因此亟需新的高效准确预测手段。

Method: MA-CBP框架以多智能体异步协作为核心，将实时视频流转化为帧级语义描述，并建立因果一致的历史摘要，融合邻近图像帧，实现长短时上下文的联合推理。其行为决策涵盖事件主体、位置和原因等关键要素。同时，作者还构建包含多尺度语义标注的高质量犯罪行为数据集。

Result: 实验结果显示，该方法在多个数据集上性能优越，有效提升了犯罪行为预测的准确率和实时性。

Conclusion: MA-CBP为城市公共安全场景下的风险预警提供了有前景的技术路径，可实现准确并及时的犯罪行为预测，有助于提升社会治安管理水平。

Abstract: With the acceleration of urbanization, criminal behavior in public scenes
poses an increasingly serious threat to social security. Traditional anomaly
detection methods based on feature recognition struggle to capture high-level
behavioral semantics from historical information, while generative approaches
based on Large Language Models (LLMs) often fail to meet real-time
requirements. To address these challenges, we propose MA-CBP, a criminal
behavior prediction framework based on multi-agent asynchronous collaboration.
This framework transforms real-time video streams into frame-level semantic
descriptions, constructs causally consistent historical summaries, and fuses
adjacent image frames to perform joint reasoning over long- and short-term
contexts. The resulting behavioral decisions include key elements such as event
subjects, locations, and causes, enabling early warning of potential criminal
activity. In addition, we construct a high-quality criminal behavior dataset
that provides multi-scale language supervision, including frame-level,
summary-level, and event-level semantic annotations. Experimental results
demonstrate that our method achieves superior performance on multiple datasets
and offers a promising solution for risk warning in urban public safety
scenarios.

</details>


### [69] [A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet](https://arxiv.org/abs/2508.06191)
*Ruixiang Tang,Jianglong Qin,Mingda Zhang,Yan Song,Yi Wu,Wei Wu*

Main category: cs.CV

TL;DR: 本文提出了一种针对胸腔积液CT图像语义分割的创新神经网络DBIF-AUNet，显著提升了分割准确率，特别适用于边缘模糊和形状多变的复杂病例。


<details>
  <summary>Details</summary>
Motivation: 胸腔积液CT图像的语义分割对临床诊疗至关重要，但现有方法在处理组织灰度相近、边界模糊和形态变化大的影像时效果有限，主要原因是特征拼接造成语义差距，影响分割精度。

Method: 提出了DBIF-AUNet神经网络，构建密集嵌套跳跃连接，创新性引入了双域特征解耦模块(DDFD)以实现多尺度特征互补和增强，同时设计了分支交互注意力融合模块(BIAF)融合全局、局部及频域特征，并采用嵌套深度监督机制结合分层自适应混合损失函数来应对类别不平衡。

Result: 在1622例胸腔积液CT影像数据上验证，DBIF-AUNet模型的IoU和Dice分别达80.1%和89.0%，对比U-Net++和Swin-UNet最高提升5.7%/2.7%和2.2%/1.5%。

Conclusion: DBIF-AUNet有效提升了胸腔积液CT语义分割的鲁棒性和精度，优于当前主流模型，为复杂病例的自动化诊断提供了有力技术支持。

Abstract: Pleural effusion semantic segmentation can significantly enhance the accuracy
and timeliness of clinical diagnosis and treatment by precisely identifying
disease severity and lesion areas. Currently, semantic segmentation of pleural
effusion CT images faces multiple challenges. These include similar gray levels
between effusion and surrounding tissues, blurred edges, and variable
morphology. Existing methods often struggle with diverse image variations and
complex edges, primarily because direct feature concatenation causes semantic
gaps. To address these challenges, we propose the Dual-Branch Interactive
Fusion Attention model (DBIF-AUNet). This model constructs a densely nested
skip-connection network and innovatively refines the Dual-Domain Feature
Disentanglement module (DDFD). The DDFD module orthogonally decouples the
functions of dual-domain modules to achieve multi-scale feature complementarity
and enhance characteristics at different levels. Concurrently, we design a
Branch Interaction Attention Fusion module (BIAF) that works synergistically
with the DDFD. This module dynamically weights and fuses global, local, and
frequency band features, thereby improving segmentation robustness.
Furthermore, we implement a nested deep supervision mechanism with hierarchical
adaptive hybrid loss to effectively address class imbalance. Through validation
on 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet
achieved IoU and Dice scores of 80.1% and 89.0% respectively. These results
outperform state-of-the-art medical image segmentation models U-Net++ and
Swin-UNet by 5.7%/2.7% and 2.2%/1.5% respectively, demonstrating significant
optimization in segmentation accuracy for complex pleural effusion CT images.

</details>


### [70] [LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning](https://arxiv.org/abs/2508.06202)
*Chang Che,Ziqi Wang,Pengwan Yang,Qi Wang,Hui Ma,Zenglin Shi*

Main category: cs.CV

TL;DR: 本文提出了一种高效的多模态大模型持续视觉指令微调架构扩展方法LiLoRA，在缓解灾难性遗忘的同时大大提升了参数效率和任务学习能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（MLLMs）需要持续地学习新任务，然而现有方法常出现灾难性遗忘，即新任务学习导致旧任务性能大幅下降。常见的架构扩展方法尽管能一定缓解遗忘，但往往带来参数规模膨胀、扩展性差等问题。

Method: LiLoRA方法针对持续视觉指令微调场景，创新性地：1）在各任务间共享LoRA矩阵A，减少冗余参数；2）对LoRA矩阵B进行再次低秩分解，最小化任务特有参数的数量；3）引入余弦正则化稳定损失，增强共享表示的一致性和稳定性。

Result: 实验在多样的CVIT基准上验证，LiLoRA在顺序任务学习中，性能优于现有方法，并大幅提高了参数效率。

Conclusion: LiLoRA为多模态大模型持续学习提供了一种高效、稳定且可扩展的架构扩展解决方案，在参数效率和任务泛化上表现出明显优势。

Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language
Models (MLLMs) to incrementally learn new tasks over time. However, this
process is challenged by catastrophic forgetting, where performance on
previously learned tasks deteriorates as the model adapts to new ones. A common
approach to mitigate forgetting is architecture expansion, which introduces
task-specific modules to prevent interference. Yet, existing methods often
expand entire layers for each task, leading to significant parameter overhead
and poor scalability. To overcome these issues, we introduce LoRA in LoRA
(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in
MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,
applies an additional low-rank decomposition to matrix B to minimize
task-specific parameters, and incorporates a cosine-regularized stability loss
to preserve consistency in shared representations over time. Extensive
experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves
superior performance in sequential task learning while significantly improving
parameter efficiency compared to existing approaches.

</details>


### [71] [AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection](https://arxiv.org/abs/2508.06203)
*Zhaopeng Gu,Bingke Zhu,Guibo Zhu,Yingying Chen,Wei Ge,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mixture-of-Experts（MoE）架构的通用异常检测框架AnomalyMoE，通过分层方式捕捉各种类型的异常，显著提升了跨领域的泛化能力和检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的异常检测方法通常专注于某一特定类型异常，导致泛化能力弱。不同领域间的方法迁移困难，急需一种能够普适应用的异常检测通用模型。

Method: 作者提出AnomalyMoE框架，将异常检测问题分解为三个层次（局部结构、组件级语义和全局逻辑异常），分别用三个专用专家网络检测，同时引入EIR模块提升专家多样性，ESB模块保证专家有效利用。

Result: 在涵盖工业、医学、视频和逻辑检测等领域的8个数据集上，AnomalyMoE达到了新的最优性能，远超各领域专用的现有方法。

Conclusion: AnomalyMoE框架在广泛领域内证明了其强大的普适性和有效性，为异常检测任务提供了可扩展的通用解决方案。

Abstract: Anomaly detection is a critical task across numerous domains and modalities,
yet existing methods are often highly specialized, limiting their
generalizability. These specialized models, tailored for specific anomaly types
like textural defects or logical errors, typically exhibit limited performance
when deployed outside their designated contexts. To overcome this limitation,
we propose AnomalyMoE, a novel and universal anomaly detection framework based
on a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the
complex anomaly detection problem into three distinct semantic hierarchies:
local structural anomalies, component-level semantic anomalies, and global
logical anomalies. AnomalyMoE correspondingly employs three dedicated expert
networks at the patch, component, and global levels, and is specialized in
reconstructing features and identifying deviations at its designated semantic
level. This hierarchical design allows a single model to concurrently
understand and detect a wide spectrum of anomalies. Furthermore, we introduce
an Expert Information Repulsion (EIR) module to promote expert diversity and an
Expert Selection Balancing (ESB) module to ensure the comprehensive utilization
of all experts. Experiments on 8 challenging datasets spanning industrial
imaging, 3D point clouds, medical imaging, video surveillance, and logical
anomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art
performance, significantly outperforming specialized methods in their
respective domains.

</details>


### [72] [PA-HOI: A Physics-Aware Human and Object Interaction Dataset](https://arxiv.org/abs/2508.06205)
*Ruiyan Wang,Lin Zuo,Zonghao Lin,Qiang Wang,Zhengxue Cheng,Rong Xie,Jun Ling,Li Song*

Main category: cs.CV

TL;DR: 本文提出了PA-HOI Motion Capture数据集，旨在弥补现有HOI数据集对物体物理属性影响人类运动动态研究的不足。该数据集强调物体物理属性（如大小、形状、重量）对人类姿态、速度等运动特征的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的人-物交互（HOI）数据集大多关注可供性细节，忽略了物体的物理属性对人类长期动作的影响，因此需要新的数据集来研究这些因素。

Method: 构建了PA-HOI运动捕捉数据集，共包括562组运动序列，涵盖不同性别受试者与35种不同大小、形状和重量的3D物体的交互。并将该数据集与现有的动作生成方法集成，用以验证其有效性。

Result: 新的PA-HOI数据集显著扩展了现有HOI数据集的研究范畴，并能有效支持基于物体物理属性的人体运动研究。通过与动作生成方法结合展示了其在迁移真实物理意识方面的能力。

Conclusion: PA-HOI数据集为研究物体物理属性对人类运动动态的影响提供了重要工具，对机器人、虚拟现实等领域具备应用价值，能够更真实地模拟和理解人-物交互。

Abstract: The Human-Object Interaction (HOI) task explores the dynamic interactions
between humans and objects in physical environments, providing essential
biomechanical and cognitive-behavioral foundations for fields such as robotics,
virtual reality, and human-computer interaction. However, existing HOI data
sets focus on details of affordance, often neglecting the influence of physical
properties of objects on human long-term motion. To bridge this gap, we
introduce the PA-HOI Motion Capture dataset, which highlights the impact of
objects' physical attributes on human motion dynamics, including human posture,
moving velocity, and other motion characteristics. The dataset comprises 562
motion sequences of human-object interactions, with each sequence performed by
subjects of different genders interacting with 35 3D objects that vary in size,
shape, and weight. This dataset stands out by significantly extending the scope
of existing ones for understanding how the physical attributes of different
objects influence human posture, speed, motion scale, and interacting
strategies. We further demonstrate the applicability of the PA-HOI dataset by
integrating it with existing motion generation methods, validating its capacity
to transfer realistic physical awareness.

</details>


### [73] [Interpretable Rheumatoid Arthritis Scoring via Anatomy-aware Multiple Instance Learning](https://arxiv.org/abs/2508.06218)
*Zhiyan Bo,Laura C. Coates,Bartlomiej W. Papiez*

Main category: cs.CV

TL;DR: 本文提出了一种用于类风湿性关节炎（RA）SvdH评分自动化预测的两阶段影像分析流程，在双手X光片上结合注意力机制实现高准确度的可解释预测，达到了接近经验丰富放射科医生的水平。


<details>
  <summary>Details</summary>
Motivation: SvdH评分虽广泛用于RA的影像学损伤量化研究，但其手工操作繁琐，限制了在日常临床实践中的应用。作者希望通过自动化方法解决手动评分耗时、效率低和主观性强的问题。

Method: 提出了两阶段的影像评分流程。第一步，基于双手X光片自动提取与疾病相关的区域，分别采用1）采样最可能异常的图像块，2）裁剪包含关节的图像补丁两种方案；第二步，通过基于注意力的多实例学习，将提取区域的信息整合，生成全片级别特征进行SvdH评分预测。还进一步应用了集成学习优化结果。

Result: 实验显示，关节裁剪方案下模型得分与人工评分高度相关（PCC=0.943，RMSE=15.73）；加用集成学习后，PCC提升至0.945，RMSE降至15.57，达到了国际领先水平，且与经验丰富放射科医生的评分表现非常接近（PCC=0.97，RMSE=18.75）。

Conclusion: 所提自动化管线不仅准确率高，且具有良好的可解释性，能够自动识别符合临床医生关注的解剖结构特征，为RA常规临床影像评估提供了可行且有效的工具，有望推广应用于实际工作中。

Abstract: The Sharp/van der Heijde (SvdH) score has been widely used in clinical trials
to quantify radiographic damage in Rheumatoid Arthritis (RA), but its
complexity has limited its adoption in routine clinical practice. To address
the inefficiency of manual scoring, this work proposes a two-stage pipeline for
interpretable image-level SvdH score prediction using dual-hand radiographs.
Our approach extracts disease-relevant image regions and integrates them using
attention-based multiple instance learning to generate image-level features for
prediction. We propose two region extraction schemes: 1) sampling image tiles
most likely to contain abnormalities, and 2) cropping patches containing
disease-relevant joints. With Scheme 2, our best individual score prediction
model achieved a Pearson's correlation coefficient (PCC) of 0.943 and a root
mean squared error (RMSE) of 15.73. Ensemble learning further boosted
prediction accuracy, yielding a PCC of 0.945 and RMSE of 15.57, achieving
state-of-the-art performance that is comparable to that of experienced
radiologists (PCC = 0.97, RMSE = 18.75). Finally, our pipeline effectively
identified and made decisions based on anatomical structures which clinicians
consider relevant to RA progression.

</details>


### [74] [TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images](https://arxiv.org/abs/2508.06224)
*Guoyu Zhou,Jing Zhang,Yi Yan,Hui Zhang,Li Zhuo*

Main category: cs.CV

TL;DR: 本文提出了一种结合纹理感知和边缘引导机制的新型Transformer（TEFormer），旨在提高城市遥感图像的语义分割精度。方法通过纹理感知模块和边缘引导解码器，有效缓解了地物纹理细微差异、空间形态相似等带来的分类混淆，实现了更精细的分割效果。实验证明该方法在多个公开数据集上取得了优异的分割性能。


<details>
  <summary>Details</summary>
Motivation: 城市遥感图像中的地物常常存在纹理微差、空间结构相似、边界模糊等问题，导致语义分割时容易出现类别混淆和误分割，影响后续的城市规划、环境监测等应用。因此，亟需提升遥感图像语义分割的精细化及边缘识别能力。

Method: 提出TEFormer模型，其包含三个核心模块：1）纹理感知模块（TaM）增强对同类地物之间微小纹理差异的捕捉能力，提高语义判别力；2）边缘引导的三分支解码器（Eg3Head）通过保留多尺度的边缘和细节信息，改善空间上下文感知；3）边缘引导特征融合模块（EgFFM）融合上下文、细节与边缘信息，实现精细分割。

Result: TEFormer在Potsdam、Vaihingen和LoveDA数据集上分别取得了88.57%、81.46%、53.55%的mIoU，优于当前主流方法，验证了其出色的语义分割能力。

Conclusion: TEFormer通过引入纹理和边缘感知机制，显著提升了城市遥感影像的语义分割精度，有效缓解了细粒度分割中的混淆问题，为高精度城市地物识别与分析提供了有力的算法支持。

Abstract: Semantic segmentation of urban remote sensing images (URSIs) is crucial for
applications such as urban planning and environmental monitoring. However,
geospatial objects often exhibit subtle texture differences and similar spatial
structures, which can easily lead to semantic ambiguity and misclassification.
Moreover, challenges such as irregular object shapes, blurred boundaries, and
overlapping spatial distributions of semantic objects contribute to complex and
diverse edge morphologies, further complicating accurate segmentation. To
tackle these issues, we propose a texture-aware and edge-guided Transformer
(TEFormer) that integrates texture awareness and edge-guidance mechanisms for
semantic segmentation of URSIs. In the encoder, a texture-aware module (TaM) is
designed to capture fine-grained texture differences between visually similar
categories to enhance semantic discrimination. Then, an edge-guided tri-branch
decoder (Eg3Head) is constructed to preserve local edges and details for
multiscale context-awareness. Finally, an edge-guided feature fusion module
(EgFFM) is to fuse contextual and detail information with edge information to
realize refined semantic segmentation. Extensive experiments show that TEFormer
achieves mIoU of 88.57%, 81.46%, and 53.55% on the Potsdam, Vaihingen, and
LoveDA datasets, respectively, shows the effectiveness in URSI semantic
segmentation.

</details>


### [75] [Towards Unified Image Deblurring using a Mixture-of-Experts Decoder](https://arxiv.org/abs/2508.06228)
*Daniel Feijoo,Paula Garrido-Mellado,Jaesung Rim,Alvaro Garcia,Marcos V. Conde*

Main category: cs.CV

TL;DR: 本论文提出了一种全新的通用图像去模糊方法，能够高效应对多种模糊类型，包括全局运动模糊、局部运动模糊、低光模糊和离焦模糊。


<details>
  <summary>Details</summary>
Motivation: 目前的图像去模糊方法多针对特定模糊类型设计，导致泛化能力弱，实际应用中需多个模型协作，缺乏实用性。

Method: 提出了一种基于专家混合（Mixture-of-Experts, MoE）结构的解码模块，根据识别的模糊类型动态分流特征，实现图像的高效精确恢复，端到端完成整体流程。

Result: 所提方法在多个模糊类型下均表现出与专用模型媲美的性能，并在未见过的新型模糊场景中展现出极佳的鲁棒性和泛化能力。

Conclusion: 该方法实现了一套统一、通用且高效的图像去模糊方案，提升了实际应用场景下的实用性和可靠性，有望取代多模型协作的传统方案。

Abstract: Image deblurring, removing blurring artifacts from images, is a fundamental
task in computational photography and low-level computer vision. Existing
approaches focus on specialized solutions tailored to particular blur types,
thus, these solutions lack generalization. This limitation in current methods
implies requiring multiple models to cover several blur types, which is not
practical in many real scenarios. In this paper, we introduce the first
all-in-one deblurring method capable of efficiently restoring images affected
by diverse blur degradations, including global motion, local motion, blur in
low-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)
decoding module, which dynamically routes image features based on the
recognized blur degradation, enabling precise and efficient restoration in an
end-to-end manner. Our unified approach not only achieves performance
comparable to dedicated task-specific models, but also demonstrates remarkable
robustness and generalization capabilities on unseen blur degradation
scenarios.

</details>


### [76] [Deepfake Detection that Generalizes Across Benchmarks](https://arxiv.org/abs/2508.06248)
*Andrii Yermakov,Jan Cech,Jiri Matas,Mario Fritz*

Main category: cs.CV

TL;DR: 本文提出了一种高效、轻量的方法，应对深度伪造检测器对未见操控手法泛化性不强的问题。方法只需要微调预训练的CLIP视觉编码器的Layer Normalization参数，并强制特征分布在超球面上，通过13个数据集实验证明超越现有复杂方法，展现极强的跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测模型难以泛化至新型或未见过的伪造手法，且许多提升泛化性的策略依赖复杂的新结构或显著增加参数量，影响实用性和可复现性。作者希望用最小的模型改动、最少的参数，实现高效且高性能的泛化。

Method: 作者提出LNCLIP-DF方法，仅微调CLIP视觉编码器中的Layer Normalization参数（占比极小），同时通过L2归一化和特征空间数据增强，使其特征分布趋于超球面结构。该策略实现了参数高效且泛化能力强的深度伪造检测。

Result: 在2019-2025年13个主流深度伪造检测基准上，LNCLIP-DF实现了最先进的跨数据集AUROC表现，胜过了结构复杂度更高的新方法。实验还发现：1）源自同一视频的真实-伪造对训练数据对提高泛化性关键；2）学术数据集的检测难度并未随时间简单上升，多样老数据集训练得模型依然泛化性极强。

Conclusion: 无需大幅改动或增加参数，只需对预训练CLIP的特定部分精修即可实现领先的深度伪造泛化检测性能。这为学界和工业界提供了一种高效、可复现的新范式。代码将在论文通过审核后开源。

Abstract: The generalization of deepfake detectors to unseen manipulation techniques
remains a challenge for practical deployment. Although many approaches adapt
foundation models by introducing significant architectural complexity, this
work demonstrates that robust generalization is achievable through a
parameter-efficient adaptation of a pre-trained CLIP vision encoder. The
proposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters
(0.03% of the total) and enhances generalization by enforcing a hyperspherical
feature manifold using L2 normalization and latent space augmentations.
  We conducted an extensive evaluation on 13 benchmark datasets spanning from
2019 to 2025. The proposed method achieves state-of-the-art performance,
outperforming more complex, recent approaches in average cross-dataset AUROC.
Our analysis yields two primary findings for the field: 1) training on paired
real-fake data from the same source video is essential for mitigating shortcut
learning and improving generalization, and 2) detection difficulty on academic
datasets has not strictly increased over time, with models trained on older,
diverse datasets showing strong generalization capabilities.
  This work delivers a computationally efficient and reproducible method,
proving that state-of-the-art generalization is attainable by making targeted,
minimal changes to a pre-trained CLIP model. The code will be made publicly
available upon acceptance.

</details>


### [77] [FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing](https://arxiv.org/abs/2508.06256)
*Barış Büyüktaş,Jonas Klotz,Begüm Demir*

Main category: cs.CV

TL;DR: 本文提出了一种名为FedX的新型联邦学习策略，通过解释性引导的剪枝方法，显著降低了遥感图像分类任务中的通信开销，同时提升了模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在遥感图像分类等数据分散敏感领域，联邦学习允许各客户端只交换模型参数以保护隐私，但高频率的大模型传输带来了严重通信负担，亟需有效减少通信量的技术。

Method: FedX利用基于反向传播的模型解释方法，分析模型各组件对任务的重要性，在服务器端裁剪掉不重要的部分，得到稀疏的全局模型后再下发到各客户端，从而减少传输参数量。

Result: 在BigEarthNet-S2与EuroSAT这两个遥感数据集上，FedX有效减少了参数传输量，并在模型泛化能力上优于未剪枝模型和现有主流剪枝方法。

Conclusion: FedX在不明显影响模型性能的同时，大大降低了联邦学习中的通信开销，为敏感领域的实际应用带来更高效的解决方案。

Abstract: Federated learning (FL) enables the collaborative training of deep neural
networks across decentralized data archives (i.e., clients), where each client
stores data locally and only shares model updates with a central server. This
makes FL a suitable learning paradigm for remote sensing (RS) image
classification tasks, where data centralization may be restricted due to legal
and privacy constraints. However, a key challenge in applying FL to RS tasks is
the communication overhead caused by the frequent exchange of large model
updates between clients and the central server. To address this issue, in this
paper we propose a novel strategy (denoted as FedX) that uses
explanation-guided pruning to reduce communication overhead by minimizing the
size of the transmitted models without compromising performance. FedX leverages
backpropagation-based explanation methods to estimate the task-specific
importance of model components and prunes the least relevant ones at the
central server. The resulting sparse global model is then sent to clients,
substantially reducing communication overhead. We evaluate FedX on multi-label
scene classification using the BigEarthNet-S2 dataset and single-label scene
classification using the EuroSAT dataset. Experimental results show the success
of FedX in significantly reducing the number of shared model parameters while
enhancing the generalization capability of the global model, compared to both
unpruned model and state-of-the-art pruning methods. The code of FedX will be
available at https://git.tu-berlin.de/rsim/FedX.

</details>


### [78] [XAG-Net: A Cross-Slice Attention and Skip Gating Network for 2.5D Femur MRI Segmentation](https://arxiv.org/abs/2508.06258)
*Byunghyun Ko,Anning Tian,Jeongkyu Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新型2.5D U-Net架构XAG-Net，通过像素级跨切片注意力和跳跃注意力机制，有效提升了MRI股骨分割的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的2D和3D深度学习分割方法在MRI股骨分割中表现有限，尤其难以兼顾切片间上下文信息和特征细致性，影响了骨科诊断和手术规划的精确性。

Method: 提出XAG-Net，一种基于2.5D U-Net的新架构，创新性引入像素级跨切片注意力（CSA）和跳跃注意力门控（AG）机制，实现精细的切片间上下文建模和切片内特征优化。与以往方法不同，XAG-Net在空间位置上对相邻切片进行像素级softmax注意力融合。

Result: 实验显示，XAG-Net在股骨MRI分割准确性上优于主流2D、2.5D和3D U-Net，同时保持计算效率。消融实验进一步证明了CSA和AG模块对性能提升的关键作用。

Conclusion: XAG-Net为高效、精确的股骨MRI分割提供了有力方法，适用于骨科影像辅助诊断和手术规划，具有良好应用前景。

Abstract: Accurate segmentation of femur structures from Magnetic Resonance Imaging
(MRI) is critical for orthopedic diagnosis and surgical planning but remains
challenging due to the limitations of existing 2D and 3D deep learning-based
segmentation approaches. In this study, we propose XAG-Net, a novel 2.5D
U-Net-based architecture that incorporates pixel-wise cross-slice attention
(CSA) and skip attention gating (AG) mechanisms to enhance inter-slice
contextual modeling and intra-slice feature refinement. Unlike previous
CSA-based models, XAG-Net applies pixel-wise softmax attention across adjacent
slices at each spatial location for fine-grained inter-slice modeling.
Extensive evaluations demonstrate that XAG-Net surpasses baseline 2D, 2.5D, and
3D U-Net models in femur segmentation accuracy while maintaining computational
efficiency. Ablation studies further validate the critical role of the CSA and
AG modules, establishing XAG-Net as a promising framework for efficient and
accurate femur MRI segmentation.

</details>


### [79] [SIFThinker: Spatially-Aware Image Focus for Visual Reasoning](https://arxiv.org/abs/2508.06259)
*Zhangquan Chen,Ruihui Zhao,Chuwei Luo,Mingze Sun,Xinlei Yu,Yangyang Kang,Ruqi Huang*

Main category: cs.CV

TL;DR: 本文提出了SIFThinker框架，能够通过空间感知和注意力校正，提升多模态大模型在复杂视觉任务中的表现，并显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大模型在空间理解和细粒度感知等复杂视觉任务中表现有限，已有方法未能充分利用空间线索进行关注修正和区域聚焦。作者希望通过模拟人类视觉感知机制，提升模型对与提示相关区域的动态关注能力。

Method: 提出SIFThinker框架，使用深度增强的边界框与自然语言交错，实现空间感知和区域关注能力。创新性地引入reverse-expansion-forward-inference策略，生成图文交错的思维链并构建SIF-50K数据集。同时提出GRPO-SIF训练范式，将深度感知的视觉定位整合进统一推理流程，实现动态关注和修正。

Result: SIFThinker在空间理解和细粒度视觉感知任务上明显优于现有最先进方法，同时保持良好的通用能力，实验结果验证了方法的有效性。

Conclusion: SIFThinker通过空间感知、注意力校正和新型训练范式，显著提升了多模态大模型对复杂视觉任务的能力，展示了较强的泛化性和实用价值。

Abstract: Current multimodal large language models (MLLMs) still face significant
challenges in complex visual tasks (e.g., spatial understanding, fine-grained
perception). Prior methods have tried to incorporate visual reasoning, however,
they fail to leverage attention correction with spatial cues to iteratively
refine their focus on prompt-relevant regions. In this paper, we introduce
SIFThinker, a spatially-aware "think-with-images" framework that mimics human
visual perception. Specifically, SIFThinker enables attention correcting and
image region focusing by interleaving depth-enhanced bounding boxes and natural
language. Our contributions are twofold: First, we introduce a
reverse-expansion-forward-inference strategy that facilitates the generation of
interleaved image-text chains of thought for process-level supervision, which
in turn leads to the construction of the SIF-50K dataset. Besides, we propose
GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual
grounding into a unified reasoning pipeline, teaching the model to dynamically
correct and focus on prompt-relevant regions. Extensive experiments demonstrate
that SIFThinker outperforms state-of-the-art methods in spatial understanding
and fine-grained visual perception, while maintaining strong general
capabilities, highlighting the effectiveness of our method.

</details>


### [80] [Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding](https://arxiv.org/abs/2508.06317)
*Jian Hu,Zixu Cheng,Shaogang Gong,Isabel Guan,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.CV

TL;DR: 本文提出了一种无需目标域标注且数据高效的跨域视频时序定位方法URPA，可在目标域仅有少量无标签视频的条件下，实现高效跨域迁移，并兼顾实时性、低存储和低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有利用VLMs或GRPO的方法，分别存在细粒度定位不佳及高度依赖有标注数据的问题，且全面适配需要高昂的计算和存储成本，无法满足无标签域和实时部署的需求。

Method: 作者提出在有标签源域训练模型后，在目标域利用极少无标签视频进行适应。核心方法URPA通过GRPO生成多个候选时序定位，基于这些候选的平均值生成伪标签，并利用候选分布方差衡量置信度，将置信度作为权重引导训练，只关注高置信度的伪监督信号，从而实现无需目标域标注的高效跨域时序定位。

Result: 在3个数据集6个跨域实验设定下，URPA仅用少量无标签目标域视频便能实现良好泛化，明显优于现有方法。

Conclusion: URPA方法无需目标域标注，训练和部署成本低，能够高效迁移于新域，具备实际应用价值。

Abstract: Video Temporal Grounding (TG) aims to temporally locate video segments
matching a natural language description (a query) in a long video. While
Vision-Language Models (VLMs) are effective at holistic semantic matching, they
often struggle with fine-grained temporal localisation. Recently, Group
Relative Policy Optimisation (GRPO) reformulates the inference process as a
reinforcement learning task, enabling fine-grained grounding and achieving
strong in-domain performance. However, GRPO relies on labelled data, making it
unsuitable in unlabelled domains. Moreover, because videos are large and
expensive to store and process, performing full-scale adaptation introduces
prohibitive latency and computational overhead, making it impractical for
real-time deployment. To overcome both problems, we introduce a Data-Efficient
Unlabelled Cross-domain Temporal Grounding method, from which a model is first
trained on a labelled source domain, then adapted to a target domain using only
a small number of unlabelled videos from the target domain. This approach
eliminates the need for target annotation and keeps both computational and
storage overhead low enough to run in real time. Specifically, we introduce.
Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain
knowledge transfer in learning video temporal grounding without target labels.
URPA generates multiple candidate predictions using GRPO rollouts, averages
them to form a pseudo label, and estimates confidence from the variance across
these rollouts. This confidence then weights the training rewards, guiding the
model to focus on reliable supervision. Experiments on three datasets across
six cross-domain settings show that URPA generalises well using only a few
unlabelled target videos. Codes will be released once published.

</details>


### [81] [Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.06318)
*Giacomo D'Amicantonio,Snehashis Majhi,Quan Kong,Lorenzo Garattoni,Gianpiero Francesca,François Bremond,Egor Bondarev*

Main category: cs.CV

TL;DR: 本文提出了一种新的弱监督视频异常检测方法GS-MoE，通过混合专家和时序高斯约束机制有效提升复杂异常识别能力，获得了新的性能基准。


<details>
  <summary>Details</summary>
Motivation: 视频异常检测由于异常类型多样且标注数据有限，特别是在弱监督下，仅有视频级标签，精确定位帧级异常极具挑战。当前方法对复杂真实异常处理能力有限，需要更有效的类别特异建模和时序异常定位方式。

Method: 本文提出GS-MoE框架，引入多个专家模型分别专注不同类型异常，结合时序高斯splatting损失，引导模型关注于异常高发的时间段，并通过混合专家机制整合多类型异常建模能力。

Result: 在UCF-Crime数据集上取得91.58%AUC，在XD-Violence和MSAD等数据集上优于现有方法，显示该方法在多样化异常场景下有显著提升。

Conclusion: GS-MoE在弱监督视频异常检测领域，通过类别特异专家和时序引导机制大幅提升复杂异常检测能力，为该任务设立了新的性能基线。

Abstract: Video Anomaly Detection (VAD) is a challenging task due to the variability of
anomalous events and the limited availability of labeled data. Under the
Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided
during training, while predictions are made at the frame level. Although
state-of-the-art models perform well on simple anomalies (e.g., explosions),
they struggle with complex real-world events (e.g., shoplifting). This
difficulty stems from two key issues: (1) the inability of current models to
address the diversity of anomaly types, as they process all categories with a
shared model, overlooking category-specific features; and (2) the weak
supervision signal, which lacks precise temporal information, limiting the
ability to capture nuanced anomalous patterns blended with normal events. To
address these challenges, we propose Gaussian Splatting-guided Mixture of
Experts (GS-MoE), a novel framework that employs a set of expert models, each
specialized in capturing specific anomaly types. These experts are guided by a
temporal Gaussian splatting loss, enabling the model to leverage temporal
consistency and enhance weak supervision. The Gaussian splatting approach
encourages a more precise and comprehensive representation of anomalies by
focusing on temporal segments most likely to contain abnormal events. The
predictions from these specialized experts are integrated through a
mixture-of-experts mechanism to model complex relationships across diverse
anomaly patterns. Our approach achieves state-of-the-art performance, with a
91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on
XD-Violence and MSAD datasets. By leveraging category-specific expertise and
temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.

</details>


### [82] [Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?](https://arxiv.org/abs/2508.06327)
*Xin Ci Wong,Duygu Sarikaya,Kieran Zucker,Marc De Kamps,Nishant Ravikumar*

Main category: cs.CV

TL;DR: 本论文提出利用基于扩散模型的合成数据生成方法，提升心脏磁共振（MR）图像分割在不同领域间的泛化能力，有效应对设备和协议变化导致的领域转移问题。


<details>
  <summary>Details</summary>
Motivation: 心脏MR成像受设备和采集协议差异影响，存在明显的领域转移（domain shift）问题，导致AI模型在新领域中的性能下降。现有方法通过扩充数据集或迁移学习等手段改善泛化能力，但面临数据获取难、标注成本高及结构一致性差等挑战。为突破这些局限，论文致力于寻找既能保证解剖结构一致性，又利于高效泛化的合成数据生成路径。

Method: 作者提出以扩散模型为基础，针对心脏MR合成数据生成，模型可利用参考图像生成与源领域高度相似、结构一致性良好的合成心脏MR图像。并将这些高保真合成数据用于2D nnU-Net、3D nnU-Net和U-Net等主流分割网络，采用两种策略：领域泛化（训练领域不变模型）和领域自适应（将目标域数据迁移到源域）。

Result: 在跨中心心脏MR分割任务上，采用合成数据训练的分割网络（无论领域泛化还是自适应）均显著优于仅用真实数据训练的网络，且在表面度量指标上有统计学显著提升（p<0.01）。

Conclusion: 基于扩散模型的合成心脏MR数据生成方法能有效缓解领域转移带来的性能下降，无需迁移学习和在线训练，尤其适用于数据稀缺环境下的AI心脏图像分析。

Abstract: Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain
shift due to variations in imaging devices and acquisition protocols. This
challenge limits the deployment of trained AI models in real-world scenarios,
where performance degrades on unseen domains. Traditional solutions involve
increasing the size of the dataset through ad-hoc image augmentation or
additional online training/transfer learning, which have several limitations.
Synthetic data offers a promising alternative, but anatomical/structural
consistency constraints limit the effectiveness of generative models in
creating image-label pairs. To address this, we propose a diffusion model (DM)
trained on a source domain that generates synthetic cardiac MR images that
resemble a given reference. The synthetic data maintains spatial and structural
fidelity, ensuring similarity to the source domain and compatibility with the
segmentation mask. We assess the utility of our generative approach in
multi-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and
vanilla U-Net segmentation networks. We explore domain generalisation, where,
domain-invariant segmentation models are trained on synthetic source domain
data, and domain adaptation, where, we shift target domain data towards the
source domain using the DM. Both strategies significantly improved segmentation
performance on data from an unseen target domain, in terms of surface-based
metrics (Welch's t-test, p < 0.01), compared to training segmentation models on
real data alone. The proposed method ameliorates the need for transfer learning
or online training to address domain shift challenges in cardiac MR image
analysis, especially useful in data-scarce settings.

</details>


### [83] [ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction](https://arxiv.org/abs/2508.06335)
*Patrick Takenaka,Johannes Maucher,Marco F. Huber*

Main category: cs.CV

TL;DR: 本文提出了改进的ViPro模型，能够在无监督的情况下，从观测中正确推断初始符号状态，实现更可靠的视频帧预测。


<details>
  <summary>Details</summary>
Motivation: 之前的视频帧预测模型（如ViPro）依赖已知的初始符号状态，但现实中往往得不到真实的初始状态，而且原模型在初始状态受噪声影响时无法准确预测和连接环境状态。

Method: 在ViPro基础上，加入新的机制，使模型可以在没有完整真实初始状态的情况下，通过观测自适应推断初始符号状态。方法还对ViPro进行无监督训练，并构建了一个3D扩展的Orbits数据集，提升了模型的泛化能力和现实适应性。

Result: 改进后的模型能够在无监督的情境下，自动推断并连接观测环境和符号状态，解决了先前因依赖初始真值状态带来的问题。经过新的3D数据集验证，模型表现更贴近真实应用。

Conclusion: 增强版ViPro模型摆脱了对初始真实符号状态的依赖，实现了更加健壮和现实的视频帧预测，为复杂动态环境中的视频理解提供了更有力的支撑。

Abstract: Predicting future video frames is a challenging task with many downstream
applications. Previous work has shown that procedural knowledge enables deep
models for complex dynamical settings, however their model ViPro assumed a
given ground truth initial symbolic state. We show that this approach led to
the model learning a shortcut that does not actually connect the observed
environment with the predicted symbolic state, resulting in the inability to
estimate states given an observation if previous states are noisy. In this
work, we add several improvements to ViPro that enables the model to correctly
infer states from observations without providing a full ground truth state in
the beginning. We show that this is possible in an unsupervised manner, and
extend the original Orbits dataset with a 3D variant to close the gap to real
world scenarios.

</details>


### [84] [Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities](https://arxiv.org/abs/2508.06342)
*Kieran Elrod,Katherine Flanigan,Mario Bergés*

Main category: cs.CV

TL;DR: 本论文探讨如何利用街景图像自动化评估街道社会活动性，为城市规划设计提供数据支持。作者提出街景图像中蕴含社会互动信息，通过多模态大模型结合社会学理论进行解读，并用回归分析验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有城市规划相关研究多以行人数量衡量街道活跃度，忽略了社会互动的质量。如何低成本、大范围捕捉街道社交活跃性质，成为城市设计亟需解决的问题。

Method: 作者选取了15座城市、2998张街景图像，结合Mehta社会互动类型理论，利用多模态大语言模型提取三类社交属性（被动、短暂、持久），并通过线性回归控制天气、时段、行人数等变量，分析与世界价值观调查中的地方依恋评分及绿色、天空、水景等环境指数之间的关系。

Result: 天空可视指数与三类社交属性均相关，绿色指数可预测持久社交性，地方依恋度与短暂社交性正相关。这些结果支持城市设计长期以来的理论假设。

Conclusion: 研究初步验证了街景图像可以用来推断特定社交互动类型与建成环境变量之间的关系。未来可探索其作为隐私保护、可扩展的城市社交性研究工具的潜力，有助于跨文化理论检验和实证支持的城市设计。

Abstract: Designing socially active streets has long been a goal of urban planning, yet
existing quantitative research largely measures pedestrian volume rather than
the quality of social interactions. We hypothesize that street view imagery --
an inexpensive data source with global coverage -- contains latent social
information that can be extracted and interpreted through established social
science theory. As a proof of concept, we analyzed 2,998 street view images
from 15 cities using a multimodal large language model guided by Mehta's
taxonomy of passive, fleeting, and enduring sociability -- one illustrative
example of a theory grounded in urban design that could be substituted or
complemented by other sociological frameworks. We then used linear regression
models, controlling for factors like weather, time of day, and pedestrian
counts, to test whether the inferred sociability measures correlate with
city-level place attachment scores from the World Values Survey and with
environmental predictors (e.g., green, sky, and water view indices) derived
from individual street view images. Results aligned with long-standing urban
planning theory: the sky view index was associated with all three sociability
types, the green view index predicted enduring sociability, and place
attachment was positively associated with fleeting sociability. These results
provide preliminary evidence that street view images can be used to infer
relationships between specific types of social interactions and built
environment variables. Further research could establish street view imagery as
a scalable, privacy-preserving tool for studying urban sociability, enabling
cross-cultural theory testing and evidence-based design of socially vibrant
cities.

</details>


### [85] [Aligning Effective Tokens with Video Anomaly in Large Language Models](https://arxiv.org/abs/2508.06350)
*Yingxian Chen,Jiahui Liu,Ruifan Di,Yanwei Li,Chirui Chang,Shizhen Zhao,Wilton W. T. Fok,Xiaojuan Qi,Yik-Chung Wu*

Main category: cs.CV

TL;DR: 本文提出了一种新型的多模态大语言模型（MLLM）VA-GPT，专门用于视频异常事件的摘要与定位，通过两大模块提升了在空间和时间上对异常事件的捕捉能力，并在多个基准上超越了已有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM虽能理解一般视频，但面对空间和时间分布极为稀疏的异常事件时常常效果不佳，主要因为冗余信息影响了异常检测的准确性。迫切需要提升这类模型在异常检测任务上的有效性。

Method: 提出VA-GPT模型，结合视觉-语言模型和大型语言模型的优势。设计了空间有效token选择（SETS）和时间有效token生成（TETG）两大核心模块，实现视觉编码器与LLM间的高效信息对齐。构建了针对异常事件指令微调的数据集，并基于XD-Violence建立跨域评测基准。

Result: VA-GPT在多个公开异常事件检测基准上取得了超越现有最新方法的成绩，表现出更强的异常事件理解、定位和总结能力。

Conclusion: 通过提出新的模型结构和对齐机制，VA-GPT有效提升了视频异常事件检测的准确性与实用性，推动了MLLM在视频异常理解领域的发展。

Abstract: Understanding abnormal events in videos is a vital and challenging task that
has garnered significant attention in a wide range of applications. Although
current video understanding Multi-modal Large Language Models (MLLMs) are
capable of analyzing general videos, they often struggle to handle anomalies
due to the spatial and temporal sparsity of abnormal events, where the
redundant information always leads to suboptimal outcomes. To address these
challenges, exploiting the representation and generalization capabilities of
Vison Language Models (VLMs) and Large Language Models (LLMs), we propose
VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in
various videos. Our approach efficiently aligns effective tokens between visual
encoders and LLMs through two key proposed modules: Spatial Effective Token
Selection (SETS) and Temporal Effective Token Generation (TETG). These modules
enable our model to effectively capture and analyze both spatial and temporal
information associated with abnormal events, resulting in more accurate
responses and interactions. Furthermore, we construct an instruction-following
dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a
cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed
method outperforms existing state-of-the-art methods on various benchmarks.

</details>


### [86] [An Implemention of Two-Phase Image Segmentation using the Split Bregman Method](https://arxiv.org/abs/2508.06351)
*Olakunle S. Abawonse,Günay Doğan*

Main category: cs.CV

TL;DR: 本文实现了Goldstein、Bresson和Osher提出的两相图像分割算法，基于改进的Chan-Vese能量模型，并采用分裂Bregman方法高效优化，实现了前景和背景的自动分割。


<details>
  <summary>Details</summary>
Motivation: 图像分割是计算机视觉中的核心任务，传统Chan-Vese模型虽有效但计算效率有限，因此需要一种高效的优化方法以提升分割性能。

Method: 在原有Chan-Vese模型上，Goldstein等人引入了一种新的能量函数，使能量最小化问题可以通过分裂Bregman方法高效求解。该算法将二维图像分为前景和背景，通过像素均值和区域边界光滑性建模，利用新引入的优化准则获得分割结果。作者详细实现了该方法，并在多种参数下评测实验效果。

Result: 实验表明，分裂Bregman优化方法下的两相分割算法能在多种图像和参数设定下表现出较好的分割效果，效率和准确率较传统方法有所提高。

Conclusion: 改进的Chan-Vese模型结合分裂Bregman优化，为两相图像分割任务提供了高效且有效的工具，具有良好实验性能，对实际图像分割任务有积极意义。

Abstract: In this paper, we describe an implementation of the two-phase image
segmentation algorithm proposed by Goldstein, Bresson, Osher in
\cite{gold:bre}. This algorithm partitions the domain of a given 2d image into
foreground and background regions, and each pixel of the image is assigned
membership to one of these two regions. The underlying assumption for the
segmentation model is that the pixel values of the input image can be
summarized by two distinct average values, and that the region boundaries are
smooth. Accordingly, the model is defined as an energy in which the variable is
a region membership function to assign pixels to either region, originally
proposed by Chan and Vese in \cite{chan:vese}. This energy is the sum of image
data terms in the regions and a length penalty for region boundaries.
Goldstein, Bresson, Osher modify the energy of Chan-Vese in \cite{gold:bre} so
that their new energy can be minimized efficiently using the split Bregman
method to produce an equivalent two-phase segmentation. We provide a detailed
implementation of this method \cite{gold:bre}, and document its performance
with several images over a range of algorithm parameters.

</details>


### [87] [Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd](https://arxiv.org/abs/2508.06357)
*Aman Bhatta,Maria Dhakal,Michael C. King,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: 本论文提出了一个新方法，通过利用属于rank-one身份的其他注册图像的排名信息，改进一对多人脸识别中的In-gallery（库内身份）与Out-of-gallery（库外身份）分类，从而更准确地区分嫌疑人是否在数据库中。


<details>
  <summary>Details</summary>
Motivation: 在一对多人脸识别中，嫌疑人可能未注册在库内，现有方法大多数依赖相似度分数阈值，这易导致误检。减少误报、冤假错案和人力浪费，因此需要一种更可靠的Out-of-gallery检测方法。

Method: 利用数据库内同一身份的多张注册图像，统计这些图像在识别中排名情况，构建特征向量，训练分类器判别rank-one结果属于库内或库外。数据集包括高质量和受损探测图像，并在多种人脸匹配器下评估方法表现，分析不同人群的分类结果。

Result: 实验证明该方法不仅适用于高质量的探测图像，也能适应模糊、低分辨率、大气干扰、佩戴墨镜等复杂条件。分类准确性在不同人群中表现相似。使用新型、基于margin-loss训练的人脸匹配器时，Out-of-gallery判别效果明显优于传统神经网络。

Conclusion: 新方法可为一对多人脸识别提供可靠的Out-of-gallery估计，减少误报和误判风险，节省调查资源，并具备良好的人群公平性。随着更先进的匹配模型应用，该方法的优势更加突出。

Abstract: A central problem in one-to-many facial identification is that the person in
the probe image may or may not have enrolled image(s) in the gallery; that is,
may be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one
result is Out-of-gallery have mostly focused on finding a suitable threshold on
the similarity score. We take a new approach, using the additional enrolled
images of the identity with the rank-one result to predict if the rank-one
result is In-gallery / Out-of-gallery. Given a gallery of identities and
images, we generate In-gallery and Out-of-gallery training data by extracting
the ranks of additional enrolled images corresponding to the rank-one identity.
We then train a classifier to utilize this feature vector to predict whether a
rank-one result is In-gallery or Out-of-gallery. Using two different datasets
and four different matchers, we present experimental results showing that our
approach is viable for mugshot quality probe images, and also, importantly, for
probes degraded by blur, reduced resolution, atmospheric turbulence and
sunglasses. We also analyze results across demographic groups, and show that
In-gallery / Out-of-gallery classification accuracy is similar across
demographics. Our approach has the potential to provide an objective estimate
of whether a one-to-many facial identification is Out-of-gallery, and thereby
to reduce false positive identifications, wrongful arrests, and wasted
investigative time. Interestingly, comparing the results of older deep
CNN-based face matchers with newer ones suggests that the effectiveness of our
Out-of-gallery detection approach emerges only with matchers trained using
advanced margin-based loss functions.

</details>


### [88] [Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning](https://arxiv.org/abs/2508.06382)
*Xiangyu Wu,Feng Yu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: 本文提出了一种仅依赖文本数据、可扩展到任意模态的多模态提示调优方法TaAM-CPT，在无模态特定标注数据的情况下，在视频、图像、音频等多种任务中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习和提示调优方法要么依赖大量模态相关标注数据，要么仅针对单一模态，限制了通用性和扩展性。作者希望仅靠文本数据，实现面向不限模态的通用表征学习，降低训练成本、提升泛化能力。

Method: 作者提出TaAM-CPT架构，核心包括模态提示池、文本构造模块和模态对齐的文本编码器。通过内外模态学习目标协同训练，无需额外标注数据即可扩展新模态，模型学习模态内类别细节及跨模态语义一致性。

Result: 在未用任何模态专属标注数据的情况下，TaAM-CPT在视频、图像、音频等多个公开数据集上均取得领先或有竞争力的分类准确率，展现出极强的跨模态泛化能力。

Conclusion: TaAM-CPT证明了仅凭文本数据，结合一致性提示调优，可构建易扩展、通用性强的多模态表征模型，为多模态学习带来新范式，并大大减轻对标注资源的依赖。

Abstract: The integration of prompt tuning with multimodal learning has shown
significant generalization abilities for various downstream tasks. Despite
advancements, existing methods heavily depend on massive modality-specific
labeled data (e.g., video, audio, and image), or are customized for a single
modality. In this study, we present Text as Any-Modality by Consistent Prompt
Tuning (TaAM-CPT), a scalable approach for constructing a general
representation model toward unlimited modalities using solely text data.
TaAM-CPT comprises modality prompt pools, text construction, and
modality-aligned text encoders from pre-trained models, which allows for
extending new modalities by simply adding prompt pools and modality-aligned
text encoders. To harmonize the learning across different modalities, TaAM-CPT
designs intra- and inter-modal learning objectives, which can capture category
details within modalities while maintaining semantic consistency across
different modalities. Benefiting from its scalable architecture and pre-trained
models, TaAM-CPT can be seamlessly extended to accommodate unlimited
modalities. Remarkably, without any modality-specific labeled data, TaAM-CPT
achieves leading results on diverse datasets spanning various modalities,
including video classification, image classification, and audio classification.
The code is available at https://github.com/Jinx630/TaAM-CPT.

</details>


### [89] [FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation](https://arxiv.org/abs/2508.06392)
*Wenbin Teng,Gonglin Chen,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TL;DR: 本文提出FVGen框架，通过创新性的视频扩散模型蒸馏方法，实现了快速且高质量的新视角合成，大幅提升稀疏视角下的三维重建效率。


<details>
  <summary>Details</summary>
Motivation: 现有三维重建方法在稠密图片采集下效果极佳，但在稀疏视角时，未见区域容易出现伪影。为弥补数据不足，视频扩散模型（VDMs）常用于生成补充视角。然而，VDMs采样速度较慢，极大限制了其实用性，因而亟需提升VDM新视角合成的效率。

Method: 提出FVGen框架，通过一种新的视频扩散模型蒸馏技术，用生成对抗网络（GAN）和软化反向KL散度最小化，将多步去噪教师模型知识迁移到少步去噪的学生模型，实现仅用四步采样即可生成新视角。

Result: 实验表明，FVGen框架在多个真实数据集上，不仅在视觉质量上与现有方法持平甚至更优，而且采样耗时减少90%以上，大幅提升了三维重建任务的效率。

Conclusion: FVGen显著提升了基于稀疏视角输入的新视角快速合成和三维重建效率，有助于实际应用中稀疏视角三维重建任务的发展。

Abstract: Recent progress in 3D reconstruction has enabled realistic 3D models from
dense image captures, yet challenges persist with sparse views, often leading
to artifacts in unseen areas. Recent works leverage Video Diffusion Models
(VDMs) to generate dense observations, filling the gaps when only sparse views
are available for 3D reconstruction tasks. A significant limitation of these
methods is their slow sampling speed when using VDMs. In this paper, we present
FVGen, a novel framework that addresses this challenge by enabling fast novel
view synthesis using VDMs in as few as four sampling steps. We propose a novel
video diffusion model distillation method that distills a multi-step denoising
teacher model into a few-step denoising student model using Generative
Adversarial Networks (GANs) and softened reverse KL-divergence minimization.
Extensive experiments on real-world datasets show that, compared to previous
works, our framework generates the same number of novel views with similar (or
even better) visual quality while reducing sampling time by more than 90%.
FVGen significantly improves time efficiency for downstream reconstruction
tasks, particularly when working with sparse input views (more than 2) where
pre-trained VDMs need to be run multiple times to achieve better spatial
coverage.

</details>


### [90] [A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery](https://arxiv.org/abs/2508.06407)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: 本论文提出一种结合图像超分辨率和分类目标的新方法，能够在提升图像质量的同时进一步提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像对于提升分类、检测和分割等视觉识别任务的表现至关重要。然而，实际应用中低分辨率图像常限制自动分析的准确性。超级分辨率（SR）技术虽能改善图像质量，但多数传统方法仅关注像素级提升，较少关注超分图像对后续分类任务的实际影响。因此，作者希望探索将分类目标直接纳入超分辨率过程，是否可以提升分类准确率。

Method: 作者提出了一种新颖的方法，在合成孔径雷达图像上，通过优化同时包含图像质量和分类性能的损失函数，既提升图像分辨率，又兼顾分类目标。该方法采用联合损失，优化图像视觉质量的同时强化下游分类器的输出表现。

Result: 所提出的方法在图像质量和分类准确率上均优于传统只关注图像质量的超分辨率方法。通过科学的图像质量指标和分类实验验证了该方法的有效性。

Conclusion: 集成分类目标到超分辨率训练过程中，不仅能生成高质量图像，还能显著提升下游分类任务表现。该方法为图像增强与识别任务协同优化提供了新思路，尤其适用于遥感等对分类精度有高需求的领域。

Abstract: High-resolution imagery plays a critical role in improving the performance of
visual recognition tasks such as classification, detection, and segmentation.
In many domains, including remote sensing and surveillance, low-resolution
images can limit the accuracy of automated analysis. To address this,
super-resolution (SR) techniques have been widely adopted to attempt to
reconstruct high-resolution images from low-resolution inputs. Related
traditional approaches focus solely on enhancing image quality based on
pixel-level metrics, leaving the relationship between super-resolved image
fidelity and downstream classification performance largely underexplored. This
raises a key question: can integrating classification objectives directly into
the super-resolution process further improve classification accuracy? In this
paper, we try to respond to this question by investigating the relationship
between super-resolution and classification through the deployment of a
specialised algorithmic strategy. We propose a novel methodology that increases
the resolution of synthetic aperture radar imagery by optimising loss functions
that account for both image quality and classification performance. Our
approach improves image quality, as measured by scientifically ascertained
image quality indicators, while also enhancing classification accuracy.

</details>


### [91] [Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification](https://arxiv.org/abs/2508.06420)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: 本文提出并验证了两种基于特征空间过采样的新方法，有效提升了SAR船舶分类中长尾数据集小样本类别的识别性能。


<details>
  <summary>Details</summary>
Motivation: SAR船舶分类任务中，类别分布常常极不均衡（长尾问题），导致少数类别难以准确分类。需要有效缓解类别不平衡带来的分类瓶颈。

Method: 受Major-to-minor (M2m) 方法启发，提出了M2m$_f$和M2m$_u$两种新的基于特征空间的过采样算法，并分别结合ViT、VGG16、ResNet50等主流特征提取方法，在OpenSARShip和FuSARShip公开数据集上进行实验。

Result: 新方法相比传统M2m和其他基线，在FuSARShip和OpenSARShip数据集的平均F1分数分别提升了8.82%和4.44%。

Conclusion: 基于特征空间的过采样新方法显著改善了SAR船舶分类中的数据不平衡问题，提升了小样本类别的分类表现，在实际应用中有较高价值。

Abstract: SAR ship classification faces the challenge of long-tailed datasets, which
complicates the classification of underrepresented classes. Oversampling
methods have proven effective in addressing class imbalance in optical data. In
this paper, we evaluated the effect of oversampling in the feature space for
SAR ship classification. We propose two novel algorithms inspired by the
Major-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two
public datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three
state-of-the-art models as feature extractors: ViT, VGG16, and ResNet50.
Additionally, we also analyzed the impact of oversampling methods on different
class sizes. The results demonstrated the effectiveness of our novel methods
over the original M2m and baselines, with an average F1-score increase of 8.82%
for FuSARShip and 4.44% for OpenSARShip.

</details>


### [92] [SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation](https://arxiv.org/abs/2508.06429)
*Guido Manni,Clemente Lauretti,Loredana Zollo,Paolo Soda*

Main category: cs.CV

TL;DR: 本文提出了一种针对标注数据稀缺情境的GAN半监督学习框架，并在MedMNIST多个数据集上实现了比现有方法更优的医学影像分类性能，尤其是在极低标注（5-shot）条件下表现突出。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像领域受到标注数据不足的严重制约，因此有必要开发在极少标注数据下也能卓越表现的方法，以便解决医学标注昂贵且难以获得的问题。

Method: 该方法通过三阶段训练框架，将生成器（用于类别条件的图像转换）、判别器（进行图像真实性及分类判断）和专用分类器相结合，交替进行有监督与无监督训练。创新之处包括利用image-to-image translation替代噪声生成，并结合判别器和分类器的加权集成伪标签、指数滑动平均实现时间一致性，提升对无标签数据的可靠伪标签生成能力。

Result: 在MedMNIST 11个数据集上，提出的方法在少样本（5、10、20、50 shots per class）条件下均显著优于六种主流GAN半监督学习方法，尤其在5-shot极低标注条件下效果最为突出，取得了统计学意义上的性能提升。

Conclusion: 该方法为标注成本高昂的医学影像任务提供了实际可行且鲁棒的分类解决方案，即便只有极少量标注数据也能获得优秀结果。

Abstract: Deep learning has revolutionized medical imaging, but its effectiveness is
severely limited by insufficient labeled training data. This paper introduces a
novel GAN-based semi-supervised learning framework specifically designed for
low labeled-data regimes, evaluated across settings with 5 to 50 labeled
samples per class. Our approach integrates three specialized neural networks --
a generator for class-conditioned image translation, a discriminator for
authenticity assessment and classification, and a dedicated classifier --
within a three-phase training framework. The method alternates between
supervised training on limited labeled data and unsupervised learning that
leverages abundant unlabeled images through image-to-image translation rather
than generation from noise. We employ ensemble-based pseudo-labeling that
combines confidence-weighted predictions from the discriminator and classifier
with temporal consistency through exponential moving averaging, enabling
reliable label estimation for unlabeled data. Comprehensive evaluation across
eleven MedMNIST datasets demonstrates that our approach achieves statistically
significant improvements over six state-of-the-art GAN-based semi-supervised
methods, with particularly strong performance in the extreme 5-shot setting
where the scarcity of labeled data is most challenging. The framework maintains
its superiority across all evaluated settings (5, 10, 20, and 50 shots per
class). Our approach offers a practical solution for medical imaging
applications where annotation costs are prohibitive, enabling robust
classification performance even with minimal labeled data. Code is available at
https://github.com/GuidoManni/SPARSE.

</details>


### [93] [MotionSwap](https://arxiv.org/abs/2508.06430)
*Om Patil,Jinesh Modi,Suryabha Mukhopadhyay,Meghaditya Giri,Chhavi Malhotra*

Main category: cs.CV

TL;DR: 本文对SimSwap人脸置换框架进行了改进，提升了换脸模型的真实度、身份保持与属性一致性。


<details>
  <summary>Details</summary>
Motivation: 人脸置换技术在学术和商业领域受到高度关注，原始SimSwap在身份保持和属性一致性上具有改进空间。

Method: 在生成器结构中引入了自注意力和交叉注意力机制，采用动态损失权重和余弦退火学习率调度，对模型架构和训练流程进行整体优化。

Result: 实验在40万次训练迭代中，模型在身份相似度、FID分数和视觉效果上均优于原始SimSwap，消融实验验证了各项改进措施的有效性。

Conclusion: 改进后的模型大幅提升了人脸置换效果。未来方向包括融入StyleGAN3、提升唇形同步、3D人脸建模以及实现视频时序一致性。

Abstract: Face swapping technology has gained significant attention in both academic
research and commercial applications. This paper presents our implementation
and enhancement of SimSwap, an efficient framework for high fidelity face
swapping. We introduce several improvements to the original model, including
the integration of self and cross-attention mechanisms in the generator
architecture, dynamic loss weighting, and cosine annealing learning rate
scheduling. These enhancements lead to significant improvements in identity
preservation, attribute consistency, and overall visual quality.
  Our experimental results, spanning 400,000 training iterations, demonstrate
progressive improvements in generator and discriminator performance. The
enhanced model achieves better identity similarity, lower FID scores, and
visibly superior qualitative results compared to the baseline. Ablation studies
confirm the importance of each architectural and training improvement. We
conclude by identifying key future directions, such as integrating StyleGAN3,
improving lip synchronization, incorporating 3D facial modeling, and
introducing temporal consistency for video-based applications.

</details>


### [94] [CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment](https://arxiv.org/abs/2508.06434)
*Shengzhu Yang,Jiawei Du,Shuai Lu,Weihang Zhang,Ningli Wang,Huiqi Li*

Main category: cs.CV

TL;DR: 提出了CLIPin，一种能无缝集成进CLIP架构的非对比式插件，以提升多模态语义对齐的强度和鲁棒性。通过两个共享预投影器，实现了对比和非对比学习的融合，并验证了其作为可插拔组件的有效性与通用性。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模自然图片-文本数据集常因自动爬取导致语义对齐不紧密，而医疗数据集虽跨模态相关性高但内容多样性低，这些问题阻碍了基于对比学习的图文表示学习能力。作者意在提升多模态模型对齐的鲁棒性和泛化能力。

Method: 提出CLIPin插件，可直接集成入CLIP或类似结构，通过引入更强的非对比式监督增强图片和文本的语义对齐。设计了两个共享预投影器，分别用于图片和文本，有效结合对比式与非对比式学习，兼顾参数消耗和效果提升。

Result: 在多种下游任务及不同框架中，大量实验验证了CLIPin插件能显著提升多模态对齐表现，具有良好兼容性和通用性。

Conclusion: CLIPin作为一种可插拔非对比式学习组件，有效提升了CLIP式模型的语义对齐和鲁棒性，对多模态学习领域具有推广价值。

Abstract: Large-scale natural image-text datasets, especially those automatically
collected from the web, often suffer from loose semantic alignment due to weak
supervision, while medical datasets tend to have high cross-modal correlation
but low content diversity. These properties pose a common challenge for
contrastive language-image pretraining (CLIP): they hinder the model's ability
to learn robust and generalizable representations. In this work, we propose
CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated
into CLIP-style architectures to improve multimodal semantic alignment,
providing stronger supervision and enhancing alignment robustness. Furthermore,
two shared pre-projectors are designed for image and text modalities
respectively to facilitate the integration of contrastive and non-contrastive
learning in a parameter-compromise manner. Extensive experiments on diverse
downstream tasks demonstrate the effectiveness and generality of CLIPin as a
plug-and-play component compatible with various contrastive frameworks. Code is
available at https://github.com/T6Yang/CLIPin.

</details>


### [95] [TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.06452)
*Mattia Litrico,Mario Valerio Giuffrida,Sebastiano Battiato,Devis Tuia*

Main category: cs.CV

TL;DR: 该论文提出了一种新的无监督域自适应（UDA）方法TRUST，利用语言模态的鲁棒性，引导视觉模型适应复杂域间转移，并在标准和复杂数据集上取得了新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法虽然在经典合成到真实等场景下表现良好，但在更复杂的地理等转移问题上表现不佳，因为背景和物体外观差异较大。近期研究发现语言模态对复杂转移更为鲁棒，因此尝试借助语言信息提升UDA性能。

Method: TRUST方法核心包括：1）利用图像字幕为目标域样本生成伪标签，并通过CLIP得分归一化实现新的伪标签不确定性估计策略，据此调整分类损失，减少伪标签错误影响；2）提出多模态软对比学习损失，将视觉与语言特征对齐，具体通过字幕指导视觉对比训练，并根据字幕相似度决定样本吸引或排斥力度，避免了UDA中困难的正负样本判定问题。

Result: TRUST在标准数据集DomainNet以及复杂转移数据集GeoNet上均超过已有方法，达到了最新的最优性能。

Conclusion: 本文证明了语言模态辅助UDA在应对复杂域转移中的有效性，TRUST提出的伪标签不确定性估计和多模态对比损失为UDA领域带来了新的解决思路。

Abstract: Recent unsupervised domain adaptation (UDA) methods have shown great success
in addressing classical domain shifts (e.g., synthetic-to-real), but they still
suffer under complex shifts (e.g. geographical shift), where both the
background and object appearances differ significantly across domains. Prior
works showed that the language modality can help in the adaptation process,
exhibiting more robustness to such complex shifts. In this paper, we introduce
TRUST, a novel UDA approach that exploits the robustness of the language
modality to guide the adaptation of a vision model. TRUST generates
pseudo-labels for target samples from their captions and introduces a novel
uncertainty estimation strategy that uses normalised CLIP similarity scores to
estimate the uncertainty of the generated pseudo-labels. Such estimated
uncertainty is then used to reweight the classification loss, mitigating the
adverse effects of wrong pseudo-labels obtained from low-quality captions. To
further increase the robustness of the vision model, we propose a multimodal
soft-contrastive learning loss that aligns the vision and language feature
spaces, by leveraging captions to guide the contrastive training of the vision
model on target images. In our contrastive loss, each pair of images acts as
both a positive and a negative pair and their feature representations are
attracted and repulsed with a strength proportional to the similarity of their
captions. This solution avoids the need for hardly determining positive and
negative pairs, which is critical in the UDA setting. Our approach outperforms
previous methods, setting the new state-of-the-art on classical (DomainNet) and
complex (GeoNet) domain shifts. The code will be available upon acceptance.

</details>


### [96] [Text Embedded Swin-UMamba for DeepLesion Segmentation](https://arxiv.org/abs/2508.06453)
*Ruida Cheng,Tejas Sudharshan Mathai,Pritam Mukherjee,Benjamin Hou,Qingqing Zhu,Zhiyong Lu,Matthew McAuliffe,Ronald M. Summers*

Main category: cs.CV

TL;DR: 本文提出将大语言模型(LLM)融入Swin-UMamba架构，实现结合影像及报告文本描述进行病灶分割，并在DeepLesion公开数据集上取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 目前病灶分割大多仅依据影像，未充分利用放射科报告中的文本描述。作者希望探究影像特征与文本描述结合，是否能显著提升病灶分割的准确性。

Method: 提出Text-Swin-UMamba模型，将报告中病灶描述文本与影像信息结合输入，利用Swin-UMamba架构进行端到端训练。在DeepLesion数据集上，提取短文本描述辅助分割。

Result: 模型在测试集上取得Dice分数82%、Hausdorff距离6.58像素，优于以LLM为核心的LanGuideMedSeg（提升37%），并超越仅用影像的xLSTM-UNet和nnUNet（提升1.74%、0.22%）。

Conclusion: 结合影像和文本描述可以显著提升病灶分割性能，所提方法在现有公开数据集上表现先进，并超越现有主要方法。

Abstract: Segmentation of lesions on CT enables automatic measurement for clinical
assessment of chronic diseases (e.g., lymphoma). Integrating large language
models (LLMs) into the lesion segmentation workflow offers the potential to
combine imaging features with descriptions of lesion characteristics from the
radiology reports. In this study, we investigate the feasibility of integrating
text into the Swin-UMamba architecture for the task of lesion segmentation. The
publicly available ULS23 DeepLesion dataset was used along with short-form
descriptions of the findings from the reports. On the test dataset, a high Dice
Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for
lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior
approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <
0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by
1.74% and 0.22%, respectively. The dataset and code can be accessed at
https://github.com/ruida/LLM-Swin-UMamba

</details>


### [97] [WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion](https://arxiv.org/abs/2508.06485)
*Sofiane Bouaziz,Adel Hafiane,Raphael Canals,Rachid Nedjai*

Main category: cs.CV

TL;DR: 本文提出了一种名为WGAST的新深度学习框架，实现了基于多源卫星影像的10米日尺度地表温度（LST）高精度估算，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着城市化加剧、气候变化和农业压力增加，对高精度实时环境监测的需求不断提升。LST（地表温度）作为核心监测变量，目前遥感手段难以兼顾高时空分辨率，因此需要新的方法解决。

Method: 提出WGAST模型——一种弱监督生成网络，通过特征提取、多源信息融合、重建及噪声抑制四阶段结构，结合条件生成对抗网络与PatchGAN判别器，实现MODIS、Landsat 8及Sentinel-2多源遥感信息的时空融合，得到10米、日尺度的LST。融合过程中用到了余弦相似度、归一化和时间注意力机制。

Result: 在定量和定性评价中，WGAST均优于现有方法。与最优基线相比，RMSE降低17.18%，SSIM提升11.00%。对云干扰和局地细尺度温度格局恢复更有鲁棒性，并经33个地面传感器验证准确性。

Conclusion: WGAST为多源卫星高分辨率LST估算提供了新范式，提高了监测精度和实用性，有广泛的应用潜力和工程价值。

Abstract: Urbanization, climate change, and agricultural stress are increasing the
demand for precise and timely environmental monitoring. Land Surface
Temperature (LST) is a key variable in this context and is retrieved from
remote sensing satellites. However, these systems face a trade-off between
spatial and temporal resolution. While spatio-temporal fusion methods offer
promising solutions, few have addressed the estimation of daily LST at 10 m
resolution. In this study, we present WGAST, a Weakly-Supervised Generative
Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra
MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning
framework designed for this task. It adopts a conditional generative
adversarial architecture, with a generator composed of four stages: feature
extraction, fusion, LST reconstruction, and noise suppression. The first stage
employs a set of encoders to extract multi-level latent representations from
the inputs, which are then fused in the second stage using cosine similarity,
normalization, and temporal attention mechanisms. The third stage decodes the
fused features into high-resolution LST, followed by a Gaussian filter to
suppress high-frequency noise. Training follows a weakly supervised strategy
based on physical averaging principles and reinforced by a PatchGAN
discriminator. Experiments demonstrate that WGAST outperforms existing methods
in both quantitative and qualitative evaluations. Compared to the
best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves
SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and
effectively captures fine-scale thermal patterns, as validated against 33
ground-based sensors. The code is available at
https://github.com/Sofianebouaziz1/WGAST.git.

</details>


### [98] [LightSwitch: Multi-view Relighting with Material-guided Diffusion](https://arxiv.org/abs/2508.06494)
*Yehonathan Litman,Fernando De la Torre,Shubham Tulsiani*

Main category: cs.CV

TL;DR: 本文提出了一种名为Lightswitch的新型3D重光照方法，结合了材料和多视角信息，能够高效且高质量地将任意数量的输入图像在目标光照条件下重新渲染。实验结果优于现有2D生成先验和逆渲染方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D重光照方法大多直接利用2D生成模型的先验，将输入图像直接进行重光照，但未能充分利用物体的内在属性和多视角信息，导致重光照质量有限。作者旨在克服上述不足，提高多视角和复杂材质物体的重光照质量与效率。

Method: 作者提出了Lightswitch方法，该方法是一种专门微调的基于扩散模型的材料-重光照生成体系。它利用输入图像的多视角信息和推断出的物体内在属性，通过高效的去噪流程，在目标光照条件下高质量重渲对象的外观。此外，方法具备良好的扩展性，能处理大量输入视角和多种材质。

Result: 实验表明，Lightswitch在2D重光照预测质量上显著优于直接基于图像的生成先验方法；在合成和真实对象的3D重光照任务中，用时仅为2分钟，效果可与或优于当前最先进的扩散逆渲染方法。

Conclusion: Lightswitch在复杂场景和多材质条件下，能更高效、更准确地重光照3D对象，代表了3D重光照领域的进步，对实际应用场景具有重要意义。

Abstract: Recent approaches for 3D relighting have shown promise in integrating 2D
image relighting generative priors to alter the appearance of a 3D
representation while preserving the underlying structure. Nevertheless,
generative priors used for 2D relighting that directly relight from an input
image do not take advantage of intrinsic properties of the subject that can be
inferred or cannot consider multi-view data at scale, leading to subpar
relighting. In this paper, we propose Lightswitch, a novel finetuned
material-relighting diffusion framework that efficiently relights an arbitrary
number of input images to a target lighting condition while incorporating cues
from inferred intrinsic properties. By using multi-view and material
information cues together with a scalable denoising scheme, our method
consistently and efficiently relights dense multi-view data of objects with
diverse material compositions. We show that our 2D relighting prediction
quality exceeds previous state-of-the-art relighting priors that directly
relight from images. We further demonstrate that LightSwitch matches or
outperforms state-of-the-art diffusion inverse rendering methods in relighting
synthetic and real objects in as little as 2 minutes.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [99] [PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare](https://arxiv.org/abs/2508.05722)
*Rania Al-Sabbagh*

Main category: cs.CL

TL;DR: 本文提出了PEACH，这是一个句子级对齐的英-阿平行语料库，包含健康领域的患者信息单和教育材料，共有5万余对句子，是研究健康领域翻译和NLP的高质量数据资源。


<details>
  <summary>Details</summary>
Motivation: 健康领域的多语种资源稀缺，尤其是高质量的英-阿平行语料。缺乏这样的资源限制了相关的对比语言学研究、机器翻译等技术的发展。

Method: 作者收集了健康领域患者信息单和教育材料，进行英-阿语言的句子级人工对齐，构建出包含51671对句子的平行语料库，并统计了各项词数、句长等数据，保证语料的高质量。

Result: PEACH语料库包含51,671对句子，英文约59万词，阿文约56万词，句长适中，达到黄金标准的人工对齐水平，并已公开发布。

Conclusion: PEACH语料库为健康领域的对比语言学、机器翻译、双语词汇提取等研究和教育提供了高质量资源，有助于推动英-阿健康信息的可及性及相关自动处理技术的发展。

Abstract: This paper introduces PEACH, a sentence-aligned parallel English-Arabic
corpus of healthcare texts encompassing patient information leaflets and
educational materials. The corpus contains 51,671 parallel sentences, totaling
approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths
vary between 9.52 and 11.83 words on average. As a manually aligned corpus,
PEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,
translation studies, and natural language processing. It can be used to derive
bilingual lexicons, adapt large language models for domain-specific machine
translation, evaluate user perceptions of machine translation in healthcare,
assess patient information leaflets and educational materials' readability and
lay-friendliness, and as an educational resource in translation studies. PEACH
is publicly accessible.

</details>


### [100] [Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation](https://arxiv.org/abs/2508.05775)
*Chi Zhang,Changjia Zhu,Junjie Xiong,Xiaoran Xu,Lingyao Li,Yao Liu,Zhuo Lu*

Main category: cs.CL

TL;DR: 本文综述了大语言模型（LLMs）在内容生成和理解方面的应用与潜在风险，重点分析了其有益与有害两面性，以及当前应对这些风险的技术和方法。


<details>
  <summary>Details</summary>
Motivation: LLMs在带来强大实用价值的同时，也可能生成有害内容（如不良、冒犯性或带偏见的语言），当前急需系统性分析LLMs的安全性风险与防护策略。

Method: 对近期关于非故意有害内容、对抗性越狱攻击和内容审核技术的研究进行了系统综述，提出LLM相关风险与防护的统一分类法，并分析多模态与模型辅助的越狱策略，以及各类缓解措施（如RLHF、提示工程和安全对齐）。

Result: 本文梳理了LLM安全领域的发展态势，评估了不同缓解方法的效果，指出现有评测方法的不足，并总结了未来研究方向。

Conclusion: LLMs的安全性是多维挑战，亟需更健全的评测和防护手段。未来应加强模型的伦理对齐和实用性之间的平衡，推动更加安全和可靠的语言模型技术发展。

Abstract: Large Language Models (LLMs) have revolutionized content creation across
digital platforms, offering unprecedented capabilities in natural language
generation and understanding. These models enable beneficial applications such
as content generation, question and answering (Q&A), programming, and code
reasoning. Meanwhile, they also pose serious risks by inadvertently or
intentionally producing toxic, offensive, or biased content. This dual role of
LLMs, both as powerful tools for solving real-world problems and as potential
sources of harmful language, presents a pressing sociotechnical challenge. In
this survey, we systematically review recent studies spanning unintentional
toxicity, adversarial jailbreaking attacks, and content moderation techniques.
We propose a unified taxonomy of LLM-related harms and defenses, analyze
emerging multimodal and LLM-assisted jailbreak strategies, and assess
mitigation efforts, including reinforcement learning with human feedback
(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the
evolving landscape of LLM safety, identifies limitations in current evaluation
methodologies, and outlines future research directions to guide the development
of robust and ethically aligned language technologies.

</details>


### [101] [FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification](https://arxiv.org/abs/2508.05782)
*Xiangyan Chen,Yufeng Li,Yujian Gan,Arkaitz Zubiaga,Matthew Purver*

Main category: cs.CL

TL;DR: 本文提出了一个面向对话系统的细粒度事实核查基准FineDialFact，并证明现有方法在此任务上仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在对话生成中的幻觉问题（即生成事实性错误或虚构信息）严重威胁许多NLP应用，需要更有效的方法检测和核查对话中的事实。当前基于整体事实一致性的检测，过于粗糙，难以应对回复中混杂的多种事实。

Method: 作者提出了FineDialFact基准，通过从对话中抽取原子事实进行逐项核查，并基于已有公开对话数据集构建了该数据集。随后，使用多种基线方法，包括链式思维（CoT）推理，进行实验评测。

Result: 实验证明融入链式思维推理的方法能提升对话事实核查表现，但在开放域对话数据集HybriDialogue上的最佳F1分数仅为0.75，表明任务仍有难度。

Conclusion: 细粒度对话事实核查是一个具有挑战性的未解任务。作者提供了新的基准数据集和代码，有助于推动未来相关研究。

Abstract: Large Language Models (LLMs) are known to produce hallucinations - factually
incorrect or fabricated information - which poses significant challenges for
many Natural Language Processing (NLP) applications, such as dialogue systems.
As a result, detecting hallucinations has become a critical area of research.
Current approaches to hallucination detection in dialogue systems primarily
focus on verifying the factual consistency of generated responses. However,
these responses often contain a mix of accurate, inaccurate or unverifiable
facts, making one factual label overly simplistic and coarse-grained. In this
paper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact
verification, which involves verifying atomic facts extracted from dialogue
responses. To support this, we construct a dataset based on publicly available
dialogue datasets and evaluate it using various baseline methods. Experimental
results demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning
can enhance performance in dialogue fact verification. Despite this, the best
F1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is
only 0.75, indicating that the benchmark remains a challenging task for future
research. Our dataset and code will be public on GitHub.

</details>


### [102] [Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models](https://arxiv.org/abs/2508.05803)
*Abishek Thamma,Micha Heilbron*

Main category: cs.CL

TL;DR: 本文探讨了人类记忆的瞬时性在语言学习中的作用，并通过对比实验验证了在Transformer模型中引入记忆限制对语言建模能力的影响。


<details>
  <summary>Details</summary>
Motivation: 人类处理语言时，单词的信息会迅速在记忆中消失。有理论认为这种记忆的短暂性有助于语言学习，但现代无记忆限制的Transformer似乎能高效学习语言，因此有必要实证验证记忆限制的作用。

Method: 作者设计了具有限制记忆（fleeting memory）和无限制记忆的Transformer模型，并在模拟发展的真实语料上训练，通过对比整体语言建模指标和针对性的句法评估，分析记忆限制对于模型能力的影响。另外，还考察了模型对于人类阅读时间预测的表现。

Result: 引入瞬时记忆的Transformer在总体语言建模和句法任务上表现更好，但在基于surprisal的人类阅读时间预测上反而更差。此外，常见的理解模型为何不适合拟合人类阅读时间的解释，不能解释这种差异。

Conclusion: 有限记忆确实有助于神经网络的语言学习，但不一定能提升对人类行为的预测能力，这支持了“记忆瞬时性提升语言学习”理论，但对行为预测另需深入研究。

Abstract: Human memory is fleeting. As words are processed, the exact wordforms that
make up incoming sentences are rapidly lost. Cognitive scientists have long
believed that this limitation of memory may, paradoxically, help in learning
language - an idea supported by classic connectionist modelling work. The rise
of Transformers appears to challenge this idea, as these models can learn
language effectively, despite lacking memory limitations or other architectural
recency biases. Here, we investigate the hypothesized benefit of fleeting
memory for language learning in tightly controlled experiments on transformer
language models. Training transformers with and without fleeting memory on a
developmentally realistic training set, we find that fleeting memory
consistently improves language learning (as quantified by both overall language
modelling performance and targeted syntactic evaluation) but, unexpectedly,
impairs surprisal-based prediction of human reading times. Interestingly,
follow up analyses revealed that this discrepancy - better language modeling,
yet worse reading time prediction - could not be accounted for by prior
explanations of why better language models sometimes fit human reading time
worse. Together, these results support a benefit of memory limitations on
neural network language learning - but not on predicting behavior.

</details>


### [103] ["Mirror" Language AI Models of Depression are Criterion-Contaminated](https://arxiv.org/abs/2508.05830)
*Tong Li,Rasiq Hussain,Mehak Gupta,Joshua R. Oltmanns*

Main category: cs.CL

TL;DR: 该研究比较了两类基于大语言模型（LLM）的抑郁症评估预测方法：直接基于标准化抑郁访谈回答（Mirror models）与基于非标准化个人生活史访谈（Non-Mirror models）。结果显示，Mirror模型的效果被高估、泛化能力较差，Non-Mirror模型的效果更贴近实际并具可解释性。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型能较好地预测抑郁分数，但现有大量模型直接用抑郁量表内容建模，可能出现预测指标与预测内容重叠（标准污染），使得模型效果被高估，泛化性受到质疑。因此，本研究旨在检验Mirror模型的泛化问题，并探索Non-Mirror模型的实际效能。

Method: 研究招募了110名参与者，分别完成结构化诊断访谈和生活史访谈。用GPT-4、GPT-4o和LLaMA3-70B分别基于两种访谈转录文本预测标准化抑郁分数。对比Mirror和Non-Mirror模型的预测效果，并分析两者模型预测分数与自评抑郁症状的相关性。同时通过主题建模分析两种模型预测的语义特征。

Result: Mirror模型（同质访谈训练和预测）表现极好（如R2=0.80），但Non-Mirror模型（异质访谈）效果较小（R2=0.27），但仍具实用意义。两类模型预测分数与自评抑郁症状的相关性一致（均约为0.54），表明Mirror模型因标准污染出现效果虚高。

Conclusion: Mirror模型虽表现突出，但效果有水分、泛化性差。Non-Mirror模型更能反映真实泛化效果。未来抑郁AI评估应注重Non-Mirror模型开发，以挖掘更具实际解释和应用价值的通用语义特征。

Abstract: A growing number of studies show near-perfect LLM language-based prediction
of depression assessment scores (up to R2 of .70). However, many develop these
models directly from language responses to depression assessments. These
"Mirror models" suffer from "criterion contamination", which arises when a
predicted score depends in part on the predictors themselves. This causes
artificial effect size inflation which reduces model generalizability. The
present study compares the performance of Mirror models versus "Non-Mirror
models", which are developed from language that does not mirror the assessment
they are developed to predict. N = 110 research participants completed two
different interviews: structured diagnostic and life history interviews. GPT-4,
GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic
interview depression scores from the two transcripts separately. Mirror models
(using structured diagnostic data) showed very large effect sizes (e.g., R2 =
.80). As expected, NonMirror models (using life history data) demonstrated
smaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror
and Non-Mirror model-predicted structured interview depression scores were
correlated with self-reported depression symptoms, Mirror and NonMirror
performed the same (e.g., r = ~.54), indicating that Mirror models contain bias
perhaps due to criterion contamination. Topic modeling identified clusters
across Mirror and Non-Mirror models, as well as between true-positive and
false-positive predictions. In this head-to-head comparison study, Mirror
language AI models of depression showed artificially inflated effect sizes and
less generalizability. As language AI models for depression continue to evolve,
incorporating Non-Mirror models may identify interpretable, and generalizable
semantic features that have unique utility in real-world psychological
assessment.

</details>


### [104] [Discovering Properties of Inflectional Morphology in Neural Emergent Communication](https://arxiv.org/abs/2508.05843)
*Miles Gilberti,Shane Storks,Huteng Dai*

Main category: cs.CL

TL;DR: 本文通过设置小词汇量约束，重新设计了属性值重建游戏，以模拟自然语言的屈折形态学现象，提出并评估了新指标，分析了神经网络中涌现通信的形态学归纳机制。


<details>
  <summary>Details</summary>
Motivation: 现有的涌现通信（EmCom）研究大多关注于一对一属性表达和句法组合，较少关注自然语言中常见的屈折形态学等更复杂现象。为弥补这一不足，作者提出通过模拟和度量自然语言的屈折特点，加深对神经网络涌现语言机制本质的理解。

Method: 作者在经典的属性值重建游戏中加入限制词汇量条件，模拟双重分节现象，并设计了与自然语言屈折形态类似的新实验设置及新评估指标，考察了“串联性”和“融合性”两种自然形态学属性。

Result: 实验发现，施加类语音约束会促使神经网络产出的涌现语言更偏向串联形态结构，同时也能像自然语言那样融合多个语法属性。

Conclusion: 神经网络中，通过模拟自然语言约束，涌现语言能自发再现自然语言形态学中的串联性与融合性，提示深度模型有能力模拟并揭示人类语言机制的复杂现象。

Abstract: Emergent communication (EmCom) with deep neural network-based agents promises
to yield insights into the nature of human language, but remains focused
primarily on a few subfield-specific goals and metrics that prioritize
communication schemes which represent attributes with unique characters
one-to-one and compose them syntactically. We thus reinterpret a common EmCom
setting, the attribute-value reconstruction game, by imposing a
small-vocabulary constraint to simulate double articulation, and formulating a
novel setting analogous to naturalistic inflectional morphology (enabling
meaningful comparison to natural language communication schemes). We develop
new metrics and explore variations of this game motivated by real properties of
inflectional morphology: concatenativity and fusionality. Through our
experiments, we discover that simulated phonological constraints encourage
concatenative morphology, and emergent languages replicate the tendency of
natural languages to fuse grammatical attributes.

</details>


### [105] [Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models](https://arxiv.org/abs/2508.05880)
*Sree Bhattacharyya,Lucas Craig,Tharun Dilliraj,Jia Li,James Z. Wang*

Main category: cs.CL

TL;DR: 本文提出并发布了一个新基准任务 CoRE，用于评估当前大语言模型（LLMs）在情感认知推理方面的能力，并通过多项实验揭示了不同LLMs在推理情感时的认知模式和差异。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在情感计算研究中主要集中于基于标签的情感预测或识别，任务较为浅表，缺乏对机器在更复杂的认知层面推理情感的研究。该工作旨在探究算法在‘认知评价’维度上解释和推理情感的能力。

Method: 作者依据认知评价理论，设计了涉及不同认知维度的大型情感推理基准CoRE，并通过一系列评测任务，系统性分析LLMs如何在多认知维度上对带有情感刺激的信息进行推理。

Result: 实验显示，不同LLMs在处理情感推理时表现出多样的认知模式，有时会更加依赖某些特定认知评价维度；同时，情感类别在模型内部表征上也呈现出可被认知评价维度解释的结构。

Conclusion: 本文表明，当前LLMs在情感认知推理上存在复杂且模型相关的表现，CoRE基准为未来AI系统研究认知情感推理能力提供了有力工具和方向。

Abstract: Affective Computing has been established as a crucial field of inquiry to
advance the holistic development of Artificial Intelligence (AI) systems.
Foundation models -- especially Large Language Models (LLMs) -- have been
evaluated, trained, or instruction-tuned in several past works, to become
better predictors or generators of emotion. Most of these studies, however,
approach emotion-related tasks in a supervised manner, assessing or training
the capabilities of LLMs using discrete emotion labels associated with stimuli
(e.g., text, images, video, audio). Evaluation studies, in particular, have
often been limited to standard and superficial emotion-related tasks, such as
the recognition of evoked or expressed emotions. In this paper, we move beyond
surface-level emotion tasks to investigate how LLMs reason about emotions
through cognitive dimensions. Drawing from cognitive appraisal theory, we
examine whether LLMs produce coherent and plausible cognitive reasoning when
reasoning about emotionally charged stimuli. We introduce a large-scale
benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal
cognitive structures implicitly used by LLMs for emotional reasoning. Through a
plethora of evaluation experiments and analysis, we seek to answer: (a) Are
models more likely to implicitly rely on specific cognitive appraisal
dimensions?, (b) What cognitive dimensions are important for characterizing
specific emotions?, and, (c) Can the internal representations of different
emotion categories in LLMs be interpreted through cognitive appraisal
dimensions? Our results and analyses reveal diverse reasoning patterns across
different LLMs. Our benchmark and code will be made publicly available.

</details>


### [106] [Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05909)
*Zhanghao Hu,Qinglin Zhu,Siya Qi,Yulan He,Hanqi Yan,Lin Gui*

Main category: cs.CL

TL;DR: 本文提出了一种用于评估大语言模型RAG（检索增强生成）检索和阅读器分开贡献的新指标（SPS），并基于该指标构建了一个灵活的检索摘要压缩控制框架（xCompress），从而提升了问答系统的整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在评估时，通常将检索器和阅读器作为整体来评估，难以准确分离检索部分的真实贡献，尤其是在LLMs作为阅读器对输入提示高度敏感的场景下。

Method: 作者提出了一种名为Spectrum Projection Score（SPS）的轻量化、无监督的新指标，用于衡量模型对检索摘要与其语义表示的对齐程度。进一步，基于SPS构建了xCompress框架，可以在推理阶段动态采样、排序和压缩检索候选摘要。

Result: 在五个QA基准和四个开源大语言模型上的实验表明，SPS指标及其驱动的xCompress方法能够显著提升RAG系统的任务表现，并有助于深入理解检索和生成模块的交互。

Conclusion: SPS作为无监督、有效的新指标，不仅提升了RAG系统性能，还为检索与生成过程提供了更清晰的判别与优化思路，表明对检索-生成组件的精细化分析和控制对提升整体系统表现具有重要价值。

Abstract: Large Language Models (LLMs) have shown improved generation performance
through retrieval-augmented generation (RAG) following the retriever-reader
paradigm, which supplements model inputs with externally retrieved knowledge.
However, prior work often evaluates RAG holistically, assessing the retriever
and reader jointly, making it difficult to isolate the true contribution of
retrieval, particularly given the prompt sensitivity of LLMs used as readers.
We introduce Spectrum Projection Score (SPS), a lightweight, supervision-free
metric that allows the reader to gauge the semantic alignment of a retrieved
summary with its hidden representation by comparing the area formed by
generated tokens from the summary, and the principal directions of subspace in
the reader and to measure the relevance. Building on SPS we present xCompress,
an inference time controller framework that dynamically samples, ranks, and
compresses retrieval summary candidates. Extensive experiments on five QA
benchmarks with four open source LLMs show that SPS not only enhances
performance across a range of tasks but also provides a principled perspective
on the interaction between retrieval and generation.

</details>


### [107] [Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale](https://arxiv.org/abs/2508.05938)
*Rafal Kocielnik,Min Kim,Penphob,Boonyarungsrit,Fereshteh Soltani,Deshawn Sambrano,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: 本文提出了一种高效检测文本中积极社交性（prosociality）的三阶段流程，实现了高精度、低成本且可拓展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 与有害内容检测不同，积极社交性内容缺乏明确定义和标注数据，这涉及信任和安全系统的新需求，因此亟需新的标注方法和部署流程。

Method: 方法包括三步：1) 通过少量人工标注，确定最优的基于大型语言模型（LLM）标注策略；2) 融合人机协作，由标注者对GPT-4与人工高分歧的案例进行审查，持续优化任务定义；3) 利用GPT-4生成1万条高质量标注，训练两阶段推断系统：第一阶段用轻量级分类器处理高置信度预测，第二阶段仅将约35%模糊样本交由更昂贵的GPT-4o处理。

Result: 在保证高精度（约0.90）的前提下，该架构实现了约70%的推断成本节约。标签质量和定义贴合度也得到显著提升。

Conclusion: 综合精确的人机交互、合理任务设计和面向部署的架构，可为新兴的负责任AI任务提供可拓展、高效率的解决方案。

Abstract: Detecting prosociality in text--communication intended to affirm, support, or
improve others' behavior--is a novel and increasingly important challenge for
trust and safety systems. Unlike toxic content detection, prosociality lacks
well-established definitions and labeled data, requiring new approaches to both
annotation and deployment. We present a practical, three-stage pipeline that
enables scalable, high-precision prosocial content classification while
minimizing human labeling effort and inference costs. First, we identify the
best LLM-based labeling strategy using a small seed set of human-labeled
examples. We then introduce a human-AI refinement loop, where annotators review
high-disagreement cases between GPT-4 and humans to iteratively clarify and
expand the task definition-a critical step for emerging annotation tasks like
prosociality. This process results in improved label quality and definition
alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train
a two-stage inference system: a lightweight classifier handles high-confidence
predictions, while only $\sim$35\% of ambiguous instances are escalated to
GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving
high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI
interaction, careful task formulation, and deployment-aware architecture design
can unlock scalable solutions for novel responsible AI tasks.

</details>


### [108] [Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring](https://arxiv.org/abs/2508.05987)
*Chunyun Zhang,Hongyan Zhao,Chaoran Cui,Qilong Song,Zhiqing Lu,Shuai Gong,Kailin Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的跨话题自动作文评分方法ATOP，结合了对话题共享和特定特征的联合学习，实现了在不同话题下更高效、准确的作文评分。


<details>
  <summary>Details</summary>
Motivation: 现有的跨话题作文评分方法主要关注源话题和目标话题之间的分布对齐，提取通用特征，但往往忽略了话题专有特征，导致话题相关性评估能力不足。为此，作者旨在提升对不同话题下作文的评分准确性，特别是对话题贴合度这类关键特质的识别能力。

Method: 作者提出了ATOP方法，利用可学习的话题感知提示（prompt），包括共享和特定成分，引导预训练语言模型利用相关知识。ATOP结合对共享提示的对抗训练，缓解因话题对齐引入的特征尺度敏感性，并在回归与分类混合框架下优化。此外，引入邻域分类器对作文表示进行本地结构建模，为目标话题作文生成伪标签，这些伪标签进一步指导针对目标话题的提示学习。

Result: 在公开的ASAP++数据集上，ATOP方法在整体和多维作文评分任务下均显著优于当前主流方法，取得了领先表现。

Conclusion: ATOP有效融合了话题共享与特有特征，提升了跨话题自动作文评分的准确性和鲁棒性，为实际应用中的多话题作文自动评分提供了更优解决方案。

Abstract: Cross-topic automated essay scoring (AES) aims to develop a transferable
model capable of effectively evaluating essays on a target topic. A significant
challenge in this domain arises from the inherent discrepancies between topics.
While existing methods predominantly focus on extracting topic-shared features
through distribution alignment of source and target topics, they often neglect
topic-specific features, limiting their ability to assess critical traits such
as topic adherence. To address this limitation, we propose an Adversarial
TOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns
topic-shared and topic-specific features to improve cross-topic AES. ATOP
achieves this by optimizing a learnable topic-aware prompt--comprising both
shared and specific components--to elicit relevant knowledge from pre-trained
language models (PLMs). To enhance the robustness of topic-shared prompt
learning and mitigate feature scale sensitivity introduced by topic alignment,
we incorporate adversarial training within a unified regression and
classification framework. In addition, we employ a neighbor-based classifier to
model the local structure of essay representations and generate pseudo-labels
for target-topic essays. These pseudo-labels are then used to guide the
supervised learning of topic-specific prompts tailored to the target topic.
Extensive experiments on the publicly available ASAP++ dataset demonstrate that
ATOP significantly outperforms existing state-of-the-art methods in both
holistic and multi-trait essay scoring. The implementation of our method is
publicly available at: https://anonymous.4open.science/r/ATOP-A271.

</details>


### [109] [Crisp Attention: Regularizing Transformers via Structured Sparsity](https://arxiv.org/abs/2508.06016)
*Sagar Gandhi,Vishal Gandhi*

Main category: cs.CL

TL;DR: 本文发现：在Transformer模型（以DistilBERT为例）中，增加注意力机制的结构化稀疏性不仅可以提高计算效率，还能在SST-2情感分析任务上提升模型准确率，挑战了稀疏性损害模型表现的常见观点。


<details>
  <summary>Details</summary>
Motivation: 近年来，Transformer模型由于其注意力机制的二次复杂度，面临扩展难题。虽然广泛研究了注意力稀疏以提升效率，但普遍认为会牺牲模型准确性，本文旨在验证这一假设。

Method: 在DistilBERT模型进行微调时，于SST-2数据集上引入结构性且后置的注意力稀疏性，对比不同稀疏度下的模型表现（最高80%稀疏性）。

Result: 引入80%稀疏注意力机制后，模型验证集准确率达到91.59%，比密集基线模型提高了0.97个百分点。

Conclusion: 结构化注意力稀疏作为一种强有力的隐式正则，能够防止过拟合，并提升Transformer的泛化与性能，可作为性能提升的新工具而非单纯的效率优化手段。

Abstract: The quadratic computational cost of the self-attention mechanism is a primary
challenge in scaling Transformer models. While attention sparsity is widely
studied as a technique to improve computational efficiency, it is almost
universally assumed to come at the cost of model accuracy. In this paper, we
report a surprising counter-example to this common wisdom. By introducing
structured, post-hoc sparsity to the attention mechanism of a DistilBERT model
during fine-tuning on the SST-2 sentiment analysis task, we find that model
accuracy improves significantly. Our model with 80\% attention sparsity
achieves a validation accuracy of 91.59\%, a 0.97\% absolute improvement over
the dense baseline. We hypothesize that this phenomenon is due to sparsity
acting as a powerful implicit regularizer, preventing the model from
overfitting by forcing it to make predictions with a more constrained and
robust set of features. Our work recasts attention sparsity not just as a tool
for computational efficiency, but as a potential method for improving the
generalization and performance of Transformer models.

</details>


### [110] [Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future](https://arxiv.org/abs/2508.06026)
*Yidong Wang,Xin Wang,Cunxiang Wang,Junfeng Fang,Qiufeng Wang,Jianing Chu,Xuran Meng,Shuxun Yang,Libo Qin,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.CL

TL;DR: 本文提出了时间自激励语言模型，有效解决了传统自激励范式在对比学习时正反样本差异减弱、偏好信号学习效率下降的问题，通过引入时序结构，在多个模型（Llama、Qwen、Mistral）及尺寸上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统自激励语言模型因被选样本与被拒样本同步提升，导致两者差异逐步缩小，影响对比偏好学习效果。论文希望找到方法来维持有意义的学习信号，持续提升模型性能。

Method: 作者提出了双阶段时序自激励方法：（1）锚定拒绝，将被拒绝样本固定为初始模型的历史输出；（2）未来引导的选择，通过使用后续新模型的预测动态生成被选样本。这样能够持续维持正反样本差异，增强偏好信号。

Result: 在Llama（3B/8B/70B）、Qwen、Mistral等模型上测试，采用作者方法后，性能明显优于原始自激励方案。在AlpacaEval 2.0评测中，Llama3.1-8B的胜率由19.69提高到29.44，提升了9.75个百分点。此外，在GSM8K、ARC、TruthfulQA、HumanEval等任务上也展现出更好的泛化能力。

Conclusion: 时序自激励方法不仅提升了语言模型的生成与评价能力，还具有较强的泛化能力，无需额外收集针对每个任务的数据，是提升大模型自监督优化效率和效果的有力手段。

Abstract: Self-Rewarding Language Models propose an architecture in which the Large
Language Models(LLMs) both generates responses and evaluates its own outputs
via LLM-as-a-Judge prompting, dynamically improving its generative capabilities
through iterative Direct Preference Optimization (DPO). However, our analysis
reveals a critical limitation in existing Self-Rewarding paradigms: the
synchronized improvement of chosen and rejected responses progressively narrows
the representational difference between contrasting samples, undermining
effective preference learning. We propose \textbf{Temporal Self-Rewarding
Language Models} that strategically coordinate past, present, and future model
generations to sustain learning signals. Our dual-phase framework introduces:
(1) \textit{Anchored Rejection} - fixing rejected responses using the past
initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically
curating chosen samples using next-generation model predictions. Extensive
experiments across three model families (Llama, Qwen, Mistral) and different
model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained
with our method compared to Self-Rewarding using same computation resources.
For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our
method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our
method also demonstrates superior out-of-distribution generalization across
mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code
generation (HumanEval) tasks, even though we do not specifically collect such
training data.

</details>


### [111] [Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings](https://arxiv.org/abs/2508.06030)
*Kartik Sharma,Yiqiao Jin,Rakshit Trivedi,Srijan Kumar*

Main category: cs.CL

TL;DR: 该论文提出了一种名为PEEK的新方法，用于高效预测大语言模型（LLMs）已掌握哪些事实知识，无需对LLM进行大量计算昂贵的前向推断。


<details>
  <summary>Details</summary>
Motivation: 现有探测LLM事实知识的方法（如隐表示分析、任务提示设计等）需要对模型进行前向推断，计算量大且耗时。因此，急需一种低计算成本的方法来高效推断LLM已知信息，便于大规模知识分析与模型缺口识别。

Method: 作者提出PEEK方法：首先用多种探测策略确定LLM已知事实的训练集，然后使用线性解码器训练预训练文本或图嵌入模型来预测LLM的输出表现，将这些嵌入作为LLM知识的代理。最后评估不同嵌入模型在未知事实上的预测能力。

Result: 在3个基于维基百科的数据集、4种LLM与7种嵌入模型上的评估显示，PEEK方法利用嵌入模型对LLM知识的预测准确率最高可达90%。同时，句子嵌入比图嵌入更能有效代理LLM的知识分布。

Conclusion: PEEK方法表明，经过知识适配的嵌入模型能在大规模上高效发现LLM的知识缺口，并为理解LLM的内部归纳偏置提供新视角。该策略为后续知识追踪与模型改进带来便利。

Abstract: Large language models (LLMs) acquire knowledge across diverse domains such as
science, history, and geography encountered during generative pre-training.
However, due to their stochasticity, it is difficult to predict what LLMs have
acquired. Prior work has developed different ways to probe this knowledge by
investigating the hidden representations, crafting specific task prompts,
curating representative samples, and estimating their uncertainty. However,
these methods require making forward passes through the underlying model to
probe the LLM's knowledge about a specific fact, making them computationally
expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or
$\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate
$\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models
that effectively encode factual knowledge as text or graphs as proxies for
LLMs. First, we identify a training set of facts known by LLMs through various
probing strategies and then adapt embedding models to predict the LLM outputs
with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived
datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict
LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find
that sentence embedding models are more suitable than graph embeddings to
predict LLM knowledge, shedding light on the underlying representation of the
factual landscape. Thus, we believe that knowledge-adapted embeddings can be
used to identify knowledge gaps in LLMs at scale and can provide deeper
insights into LLMs' internal inductive bias. The code and data are made
available at https://github.com/claws-lab/peek.

</details>


### [112] [EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation](https://arxiv.org/abs/2508.06046)
*Xinda Wang,Zhengxu Hou,Yangshijie Zhang,Bingren Yan,Zhibo Yang,Xingsheng Zhang,Luxi Xing,Qiang Zhou,Chen Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新框架EvolvR，在故事评估任务中通过自我进化的方法提升大语言模型的推理与评判能力，在StoryER等评测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）应用于故事评估时存在瓶颈：闭源模型的prompt engineering通用性差，开源模型的微调难以满足严密推理需求。故事评估不仅辅助人类判断，还为自动故事生成提供关键信号，因此亟需更强大的评估方法。

Method: 提出了自进化式成对推理（EvolvR）框架。该框架利用多角色策略，自动生成分数对齐的思维链数据（CoT），再通过多智能体自过滤确保数据的逻辑性和鲁棒性，最后用精炼后的数据训练评估器，并作为奖励模型指导故事生成。

Result: EvolvR在StoryER、HANNA和OpenMEVA三大评测基准上达到了最新SOTA效果。同时，将其作为奖励模型用于故事生成时，显著提升了所生成故事的质量。

Conclusion: 自进化推理框架不仅提升了LLMs在开放式故事评估上的表现，还能有效助力高质量故事的生成，验证了该方法的优越性。

Abstract: Although the effectiveness of Large Language Models (LLMs) as judges
(LLM-as-a-judge) has been validated, their performance remains limited in
open-ended tasks, particularly in story evaluation. Accurate story evaluation
is crucial not only for assisting human quality judgment but also for providing
key signals to guide story generation. However, existing methods face a
dilemma: prompt engineering for closed-source models suffers from poor
adaptability, while fine-tuning approaches for open-source models lack the
rigorous reasoning capabilities essential for story evaluation. To address
this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.
Grounded in pairwise comparison, the framework first self-synthesizes
score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To
ensure data quality, these raw CoTs undergo a self-filtering process, utilizing
multi-agents to guarantee their logical rigor and robustness. Finally, the
evaluator trained on the refined data is deployed as a reward model to guide
the story generation task. Experimental results demonstrate that our framework
achieves state-of-the-art (SOTA) performance on three evaluation benchmarks
including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward
model, it significantly enhances the quality of generated stories, thereby
fully validating the superiority of our self-evolving approach.

</details>


### [113] [ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline](https://arxiv.org/abs/2508.06094)
*Morris Alper,Moran Yanuka,Raja Giryes,Gašper Beguš*

Main category: cs.CL

TL;DR: 本文提出了一个利用大语言模型（LLM）辅助自动化构建人工语言（如世界语、昆雅语）的方法。该方法被实现为名为ConlangCrafter的多阶段流水线系统，能自动从发音、构词、句法到词汇生成及翻译，实现多样且连贯的新语言创作。


<details>
  <summary>Details</summary>
Motivation: 人工语言在艺术、哲学和国际交流中具有独特价值，但全面设计一门新语言是一项复杂且专业的工作。随着大型语言模型技术发展，探索其在创造性人工语言设计中的应用前景，具有重要意义。

Method: 作者设计了ConlangCrafter，一种将人工语言设计过程拆为多个模块阶段（发音、构词、句法、词汇、翻译），每一模块都利用LLM进行元语言推理、注入随机性以增强多样化，并通过自我反馈提升描述一致性，以端到端方式自动构建人工语言。

Result: 通过连贯性和类型学多样性评价，ConlangCrafter无需人工语言学专家参与，能创造出结构合理、风格各异的人工语言。

Conclusion: 论文验证了大型语言模型能够作为创新工具，在无专业知识下实现高质量人工语言生成，为人工语言的自动化构建和创意生成领域提供了全新思路和方法。

Abstract: Constructed languages (conlangs) such as Esperanto and Quenya have played
diverse roles in art, philosophy, and international communication. Meanwhile,
large-scale foundation models have revolutionized creative generation in text,
images, and beyond. In this work, we leverage modern LLMs as computational
creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a
multi-hop pipeline that decomposes language design into modular stages --
phonology, morphology, syntax, lexicon generation, and translation. At each
stage, our method leverages LLMs' meta-linguistic reasoning capabilities,
injecting randomness to encourage diversity and leveraging self-refinement
feedback to encourage consistency in the emerging language description. We
evaluate ConlangCrafter on metrics measuring coherence and typological
diversity, demonstrating its ability to produce coherent and varied conlangs
without human linguistic expertise.

</details>


### [114] [Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs](https://arxiv.org/abs/2508.06103)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: 本文提出了两个有效方法用于《古兰经》的抽取式问答（QA），克服了文本复杂语言、独特术语和深层含义的挑战。通过few-shot提示和指令微调大模型，并开发专门的阿拉伯语提示框架，结合后处理技术有效提升了精度和表现。


<details>
  <summary>Details</summary>
Motivation: 《古兰经》作为语义丰富的低资源语料，其抽取式问答面临特殊挑战，包括复杂的语言结构、专用术语和深层意义，常规方法难以获得高质量答案，因此需要新方法提升问答效果。

Method: 提出两种方法：一是few-shot提示结合指令微调的大型语言模型（如Gemini和DeepSeek）；二是开发专门的阿拉伯语提示框架用于答案片段抽取。后处理包括子词对齐、重叠抑制及语义过滤，提升精度降低幻觉现象。

Result: 评估结果显示，配备阿拉伯语指令的大型语言模型优于传统微调方法，最佳配置下pAP10分数达到0.637。

Conclusion: 基于提示的指令微调方法能够有效应对低资源、语义丰富环境下的问答任务，优于以往基于微调的模型。

Abstract: This paper presents two effective approaches for Extractive Question
Answering (QA) on the Quran. It addresses challenges related to complex
language, unique terminology, and deep meaning in the text. The second uses
few-shot prompting with instruction-tuned large language models such as Gemini
and DeepSeek. A specialized Arabic prompt framework is developed for span
extraction. A strong post-processing system integrates subword alignment,
overlap suppression, and semantic filtering. This improves precision and
reduces hallucinations. Evaluations show that large language models with Arabic
instructions outperform traditional fine-tuned models. The best configuration
achieves a pAP10 score of 0.637. The results confirm that prompt-based
instruction tuning is effective for low-resource, semantically rich QA tasks.

</details>


### [115] [You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures](https://arxiv.org/abs/2508.06105)
*Shengyuan Chen,Chuang Zhou,Zheng Yuan,Qinggang Zhang,Zeyang Cui,Hao Chen,Yilin Xiao,Jiannong Cao,Xiao Huang*

Main category: cs.CL

TL;DR: 本文提出了LogicRAG，一种无需预先构建知识图谱、动态推理的RAG方法，显著提升大模型事实性和检索效率。


<details>
  <summary>Details</summary>
Motivation: 现有图结构检索增强生成(GraphRAG)方法在构建知识图谱过程中成本高、更新慢且灵活性差，不能满足不同查询逻辑结构的需求。

Method: LogicRAG在推理时动态分解复杂查询为子问题，通过构建有向无环图(DAG)建立子问题间的逻辑依赖关系，并通过拓扑排序指导检索顺序。同时，采用图剪枝和上下文剪枝减少冗余检索和无关内容，降低token消耗。

Result: LogicRAG在大量实验中展现出比当前主流方法更优的准确率和效率。

Conclusion: LogicRAG能更灵活高效地支持复杂推理场景，解决了预构图方法的高成本和适应性不足问题，为RAG模型带来更强的实际应用能力。

Abstract: Large language models (LLMs) often suffer from hallucination, generating
factually incorrect statements when handling questions beyond their knowledge
and perception. Retrieval-augmented generation (RAG) addresses this by
retrieving query-relevant contexts from knowledge bases to support LLM
reasoning. Recent advances leverage pre-constructed graphs to capture the
relational connections among distributed documents, showing remarkable
performance in complex tasks. However, existing Graph-based RAG (GraphRAG)
methods rely on a costly process to transform the corpus into a graph,
introducing overwhelming token cost and update latency. Moreover, real-world
queries vary in type and complexity, requiring different logic structures for
accurate reasoning. The pre-built graph may not align with these required
structures, resulting in ineffective knowledge retrieval. To this end, we
propose a \textbf{\underline{Logic}}-aware
\textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented
\textbf{\underline{G}}eneration framework (\textbf{LogicRAG}) that dynamically
extracts reasoning structures at inference time to guide adaptive retrieval
without any pre-built graph. LogicRAG begins by decomposing the input query
into a set of subproblems and constructing a directed acyclic graph (DAG) to
model the logical dependencies among them. To support coherent multi-step
reasoning, LogicRAG then linearizes the graph using topological sort, so that
subproblems can be addressed in a logically consistent order. Besides, LogicRAG
applies graph pruning to reduce redundant retrieval and uses context pruning to
filter irrelevant context, significantly reducing the overall token cost.
Extensive experiments demonstrate that LogicRAG achieves both superior
performance and efficiency compared to state-of-the-art baselines.

</details>


### [116] [AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models](https://arxiv.org/abs/2508.06124)
*Sayantan Adak,Pratyush Chatterjee,Somnath Banerjee,Rima Hazra,Somak Aditya,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 当前LLMs在输出过程中容易因推理细节被忽视而引发危害行为，传统安全机制难以细致应对。作者提出AURA框架，通过多层次的过程奖励模型( Process Reward Models, PRMs )，能更动态细致地监控和干预大模型推理，显著提升逻辑一致性和安全性。实验结果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有大模型容易在推理过程中因逻辑遗漏，无意中促进有害行为（affordance-based safety risks）。传统基于结果的安全方法难以提前、细粒度地识别和干预这些隐患。需要新的框架提升AI系统的安全性、责任感和上下文感知能力。

Method: 提出了AURA多层次安全框架，核心为过程奖励模型（PRMs），包括自我反思（self-critique）、细分PRMs评估和适应性解码等机制，从推理过程的每一步出发细致评估模型输出的合逻辑性和安全性，并动态调整输出过程。

Result: AURA显著提升了模型推理的安全性和合逻辑性，避免了因推理疏漏而带来的安全风险。实验表明AURA在逻辑和安全性上均优于传统方法。

Conclusion: AURA为安全、责任、与环境敏感型AI设立了新基准，有助于推进更可靠、上下文感知更强的人工智能。

Abstract: Present day LLMs face the challenge of managing affordance-based safety
risks-situations where outputs inadvertently facilitate harmful actions due to
overlooked logical implications. Traditional safety solutions, such as scalar
outcome-based reward models, parameter tuning, or heuristic decoding
strategies, lack the granularity and proactive nature needed to reliably detect
and intervene during subtle yet crucial reasoning steps. Addressing this
fundamental gap, we introduce AURA, an innovative, multi-layered framework
centered around Process Reward Models (PRMs), providing comprehensive, step
level evaluations across logical coherence and safety-awareness. Our framework
seamlessly combines introspective self-critique, fine-grained PRM assessments,
and adaptive safety-aware decoding to dynamically and proactively guide models
toward safer reasoning trajectories. Empirical evidence clearly demonstrates
that this approach significantly surpasses existing methods, significantly
improving the logical integrity and affordance-sensitive safety of model
outputs. This research represents a pivotal step toward safer, more
responsible, and contextually aware AI, setting a new benchmark for
alignment-sensitive applications.

</details>


### [117] [Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models](https://arxiv.org/abs/2508.06135)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: 本文提出Selective Reflection Distillation (SRD) 框架，以提升大语言模型知识蒸馏效率和效果，通过动态筛选更高质量、更适配学生模型的数据，显著提升模型性能并减少训练资源。


<details>
  <summary>Details</summary>
Motivation: 现有的白盒知识蒸馏主要关注如何平衡真实答案和学生模型输出，而忽视了训练数据质量以及学生模型与训练数据的匹配度，这导致压缩后的学生模型表现受限。作者希望通过提升训练数据的质量和兼容性来解决这一瓶颈。

Method: 提出SRD框架，动态对比真实数据与学生模型输出，采用自动排序根据难度挑选与学生模型更兼容、高质量的训练数据。采用课程调度策略，分批逐步将这些精选数据引入蒸馏流程，增强了蒸馏效果。此外，SRD可作为即插即用的模块加到不同蒸馏算法上，无需更改底层方法。

Result: SRD在多种白盒知识蒸馏方法和模型架构上都能提升学生模型性能。同时，SRD可将训练时间最多减少39%。实验覆盖不同基准和模型，均显示出SRD的有效性和广泛适用性。

Conclusion: 数据质量和与学生模型的兼容性是高效蒸馏大模型的核心。SRD通过系统化数据筛选与调度，提升压缩模型的能力和效率，为数据驱动型蒸馏研究和实际应用提供了可行新思路。

Abstract: Knowledge Distillation (KD) is a fundamental technique for compressing large
language models (LLMs) into compact, efficient student models. However,
existing white-box KD methods mainly focus on balancing ground truth and
student-generated responses while overlooking two critical factors: training
data quality and student-model compatibility. To address these limitations, we
propose Selective Reflection Distillation (SRD), a novel data curation
framework that leverages reflections from student models to systematically
refine training data. SRD dynamically evaluates and selects prompt-response
pairs by comparing ground truth data with student model outputs, selectively
curating high-quality, student-compatible training instances through automated
ranking based on difficulty. Furthermore, after selecting the training data, a
curriculum scheduling strategy is employed to incrementally introduce these
curated subsets into the distillation process at fixed intervals. As a
plug-and-play enhancement, SRD consistently improves distillation outcomes
across diverse white-box KD approaches and model architectures, as well as
decreases computational cost significantly during KD training. Experiments on a
range of language model benchmarks demonstrate SRD's consistent improvements in
distilled model performance, as well as a reduction in training runtime by up
to 39%, under diverse KD methods and model families. Notably, SRD operates as a
plug-and-play module, enhancing sample efficiency without modifying underlying
KD algorithms. Our findings highlight that data quality and compatibility are
pivotal to effective and efficient distillation of LLMs, and SRD provides a
principled framework to achieve both. This work advances the understanding of
data-centric factors in KD and offers practical insights for enhancing the
capability and efficiency of compressed LLMs.

</details>


### [118] [Scaling Personality Control in LLMs with Big Five Scaler Prompts](https://arxiv.org/abs/2508.06149)
*Gunhee Cho,Yun-Gyung Cheong*

Main category: cs.CL

TL;DR: 本文提出了一种基于提示词（prompt）的框架Big5-Scaler，可实现大语言模型（LLM）对五大人格特质的可控表达，无需额外训练，即可通过自然语言提示精细调节人格特质。实验证明该方法能有效赋予模型可区分的人格特性，是构建具有人格感知对话体代理的高效途径。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型缺乏对人格特质的可控表达能力，限制了其在需要人格特质调控场景（如拟人化对话体）的应用。作者希望通过简单、高效的方式增强模型的人格可控性，满足实际需求。

Method: Big5-Scaler通过将五大人格特质的数值以自然语言插入提示，对LLM进行条件引导，产生带有特定人格特质的输出。该方法不需要额外训练，仅通过调整提示词即可调控人格表达的细致程度。

Result: 在特质表达、对话生成和模仿能力等多项任务上，Big5-Scaler能够让模型展现出一致且易区分的人格特征。实验显示，提示类型及人格强度的设定会影响表现，其中简洁的提示和较低的人格特质数值效果较佳。

Conclusion: Big5-Scaler为大语言模型的人格可控性提供了高效、简单的实现方式，对于打造更具人格化、拟真人格感知的对话体代理具有重要意义。

Abstract: We present Big5-Scaler, a prompt-based framework for conditioning large
language models (LLMs) with controllable Big Five personality traits. By
embedding numeric trait values into natural language prompts, our method
enables fine-grained personality control without additional training. We
evaluate Big5-Scaler across trait expression, dialogue generation, and human
trait imitation tasks. Results show that it induces consistent and
distinguishable personality traits across models, with performance varying by
prompt type and scale. Our analysis highlights the effectiveness of concise
prompts and lower trait intensities, providing a efficient approach for
building personality-aware dialogue agents.

</details>


### [119] [Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach](https://arxiv.org/abs/2508.06155)
*Renhan Zhang,Lian Lian,Zhen Qi,Guiran Liu*

Main category: cs.CL

TL;DR: 本文提出一种可解释的偏见检测方法，用于发现大型语言模型输出中的隐性社会偏见。该方法结合嵌套语义表示和上下文对比机制，利用注意力权重扰动来分析模型对特定社会属性词语的敏感性，在StereoSet数据集上验证方法有效性。结果显示方法能准确识别语义相近文本中的偏见差异，并保持高语义一致性与输出稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成文本时，可能会无意中产生隐含的社会偏见，且这些偏见往往难以通过表面语言特征直接检测。现有方法可解释性不足且难以揭示潜在的偏见形成机制，因此亟需一种既能检测潜在偏见又能解释偏见来源的方法。

Method: 该方法采用嵌套语义表示和上下文对比机制相结合，从模型输出的向量空间结构中提取潜在的偏见特征。通过干扰注意力权重，分析模型对特定社会属性词（如性别、职业、宗教、种族等）的反应和敏感性，从而揭示偏见形成的语义路径。

Result: 在StereoSet多维刻板印象数据集（涵盖性别、职业、宗教、种族）上进行实验，重点评估偏见检测准确率、语义一致性和上下文敏感性等指标。实验表明，该方法在各个维度上都表现出较强的检测能力，能够准确区分语义相近文本中的偏见差异，并保持输出的高一致性和稳定性。同时在结构上具备较高可解释性。

Conclusion: 该方法能高效、可解释地检测语言模型输出中的隐含社会偏见，揭示模型内部的偏见关联机制，为偏见检测提供了更透明可靠的技术基础，适用于对生成内容可信度要求高的实际应用场景。

Abstract: This paper addresses the issue of implicit stereotypes that may arise during
the generation process of large language models. It proposes an interpretable
bias detection method aimed at identifying hidden social biases in model
outputs, especially those semantic tendencies that are not easily captured
through explicit linguistic features. The method combines nested semantic
representation with a contextual contrast mechanism. It extracts latent bias
features from the vector space structure of model outputs. Using attention
weight perturbation, it analyzes the model's sensitivity to specific social
attribute terms, thereby revealing the semantic pathways through which bias is
formed. To validate the effectiveness of the method, this study uses the
StereoSet dataset, which covers multiple stereotype dimensions including
gender, profession, religion, and race. The evaluation focuses on several key
metrics, such as bias detection accuracy, semantic consistency, and contextual
sensitivity. Experimental results show that the proposed method achieves strong
detection performance across various dimensions. It can accurately identify
bias differences between semantically similar texts while maintaining high
semantic alignment and output stability. The method also demonstrates high
interpretability in its structural design. It helps uncover the internal bias
association mechanisms within language models. This provides a more transparent
and reliable technical foundation for bias detection. The approach is suitable
for real-world applications where high trustworthiness of generated content is
required.

</details>


### [120] [One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging](https://arxiv.org/abs/2508.06163)
*Yingfeng Luo,Dingyang Lin,Junxin Wang,Ziqiang Xu,Kaiyan Chang,Tong Zheng,Bei Li,Anxiang Ma,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种新的模型合并方法TADrop，通过对参数进行张量级自适应稀疏化（而非统一稀疏比）来提升多任务模型效果，在多个任务和模型上均实现了明显性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并的稀疏化方法通常采用统一的稀疏率，忽视了模型参数的结构和分布异质性，导致可能误删重要参数，保留无关参数，影响最终性能。

Method: TADrop方法根据每个参数张量的分布特征，分配不同的稀疏率。具体来说，分布密集且冗余的张量会被更激进地稀疏化，而稀疏且关键的张量则被优先保留。TADrop作为一个简单易用的模块，可集成到多种已有的模型合并方法中。

Result: 在视觉、语言、多模态等多类任务，以及如ViT、BEiT等模型上，TADrop集成于主流模型合并方法时，均显著提升了性能。例如，在8个ViT-B/32任务上，性能平均提升2.0%。

Conclusion: TADrop通过结构自适应的稀疏化显著减轻了参数干扰，为高性能模型合并提供了新的基线方法，具有良好的通用性和效果。

Abstract: Model merging has emerged as a compelling data-free paradigm for multi-task
learning, enabling the fusion of multiple fine-tuned models into a single,
powerful entity. A key technique in merging methods is sparsification, which
prunes redundant parameters from task vectors to mitigate interference.
However, prevailing approaches employ a ``one-size-fits-all'' strategy,
applying a uniform sparsity ratio that overlooks the inherent structural and
statistical heterogeneity of model parameters. This often leads to a suboptimal
trade-off, where critical parameters are inadvertently pruned while less useful
ones are retained. To address this limitation, we introduce \textbf{TADrop}
(\textbf{T}ensor-wise \textbf{A}daptive \textbf{Drop}), an adaptive
sparsification strategy that respects this heterogeneity. Instead of a global
ratio, TADrop assigns a tailored sparsity level to each parameter tensor based
on its distributional properties. The core intuition is that tensors with
denser, more redundant distributions can be pruned aggressively, while sparser,
more critical ones are preserved. As a simple and plug-and-play module, we
validate TADrop by integrating it with foundational, classic, and SOTA merging
methods. Extensive experiments across diverse tasks (vision, language, and
multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and
significantly boosts their performance. For instance, when enhancing a leading
merging method, it achieves an average performance gain of 2.0\% across 8
ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter
interference by tailoring sparsification to the model's structure, offering a
new baseline for high-performance model merging.

</details>


### [121] [UR$^2$: Unify RAG and Reasoning through Reinforcement Learning](https://arxiv.org/abs/2508.06165)
*Weitao Li,Boran Xiang,Xiaolong Wang,Zhinan Gou,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 本文提出了一个名为UR2的框架，融合了检索增强生成（RAG）和可验证奖励强化学习（RLVR），实现了检索与推理能力的统一，显著提升了大模型在多任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG和RLVR各自提升了大模型的知识与推理能力，但它们一直各自为战，缺乏统一整合，现有的方法往往只能适用于有限的场景。这种割裂限制了通用性与适用范围。

Method: 本文提出UR2 框架，核心包括：1）难度感知课程训练，仅在难题上调用检索；2）混合知识访问策略，同时结合领域离线语料与生成摘要。通过强化学习统一调度检索与推理，实现动态适应。

Result: 在多项任务（包括开放领域问答、MMLU、医学与数学推理）上，UR2（基于Qwen2.5-3/7B和LLaMA-3.1-8B）效果远超现有RAG和RL方法，并在若干基准上媲美GPT-4o-mini和GPT-4.1-mini。

Conclusion: UR2框架有效统一了知识检索与复杂推理，显著提升了大模型的多任务能力和泛化能力，为更广泛领域的应用奠定了基础。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities through two
complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances
knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),
which optimizes complex reasoning abilities. However, these two capabilities
are often developed in isolation, and existing efforts to unify them remain
narrow in scope-typically limited to open-domain QA with fixed retrieval
settings and task-specific assumptions. This lack of integration constrains
generalization and limits the applicability of RAG-RL methods to broader
domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a
general framework that unifies retrieval and reasoning through reinforcement
learning. UR2 introduces two key contributions: a difficulty-aware curriculum
training that selectively invokes retrieval only for challenging problems, and
a hybrid knowledge access strategy combining domain-specific offline corpora
with LLM-generated summaries. These components are designed to enable dynamic
coordination between retrieval and reasoning, improving adaptability across a
diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,
and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B
and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,
achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several
benchmarks. We have released all code, models, and data at
https://github.com/Tsinghua-dhy/UR2.

</details>


### [122] [Pragmatics beyond humans: meaning, communication, and LLMs](https://arxiv.org/abs/2508.06167)
*Vít Gvoždiak*

Main category: cs.CL

TL;DR: 本文认为语用学应被重新定义为语言与社会行动之间的动态接口，尤其在大语言模型（LLM）兴起后，这一理解亟需更新与方法论反思。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在交流场景中的应用，传统语用学框架（通常基于人类交流假设）已难以充分解释人机交互中的语言现象，亟需理论转型。

Method: 文章三个部分：1）批判传统符号学三分法，提出人机沟通（HMC）更适合分析LLM语用问题；2）对比人本语用学与机器中心的LLM，讨论概率语用学等更适合LLM的模型；3）讨论评价LLM的“替代主义”三种表现及其带来的类人偏误，并提出“语境挫败”概念。

Result: 提出了人机沟通框架和概率语用学适用于LLM交流，揭示了类人化评估对LLM理解的误导，强调了“语境挫败”等新问题。

Conclusion: LLM参与交流的特殊性要求我们修正甚至扩展现有语用学理论；纯粹基于人类的语用假设难以适用，需要为生成式AI的发展提供新的理论和评估工具。

Abstract: The paper reconceptualizes pragmatics not as a subordinate, third dimension
of meaning, but as a dynamic interface through which language operates as a
socially embedded tool for action. With the emergence of large language models
(LLMs) in communicative contexts, this understanding needs to be further
refined and methodologically reconsidered. The first section challenges the
traditional semiotic trichotomy, arguing that connectionist LLM architectures
destabilize established hierarchies of meaning, and proposes the Human-Machine
Communication (HMC) framework as a more suitable alternative. The second
section examines the tension between human-centred pragmatic theories and the
machine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics
continue to dominate, it relies on human-specific assumptions ill-suited to
predictive systems like LLMs. Probabilistic pragmatics, particularly the
Rational Speech Act framework, offers a more compatible teleology by focusing
on optimization rather than truth-evaluation. The third section addresses the
issue of substitutionalism in three forms - generalizing, linguistic, and
communicative - highlighting the anthropomorphic biases that distort LLM
evaluation and obscure the role of human communicative subjects. Finally, the
paper introduces the concept of context frustration to describe the paradox of
increased contextual input paired with a collapse in contextual understanding,
emphasizing how users are compelled to co-construct pragmatic conditions both
for the model and themselves. These arguments suggest that pragmatic theory may
need to be adjusted or expanded to better account for communication involving
generative AI.

</details>


### [123] [Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime](https://arxiv.org/abs/2508.06178)
*Hugo Abonizio,Thales Almeida,Roberto Lotufo,Rodrigo Nogueira*

Main category: cs.CL

TL;DR: 本文探讨如何在数据量有限的情况下将新知识注入大语言模型（LLMs），并分析过程中模型遗忘旧知识的现象。提出多种数据增强方法以提升知识注入效果，并公开相关代码和数据。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在知识更新时需大量语料，但实际应用中往往只能获得有限的新数据，如数千到百万级tokens。如何高效利用小规模数据迅速补充模型知识，并避免“灾难性遗忘”，成为一个亟需解决的问题。

Method: 作者使用近期新闻构建不与预训练数据重叠的知识注入测试集，通过问答形式评估模型新知识掌握效果。在基础继续预训练的框架下，实验不同的数据增强合成算法（如多样化提示生成变体文本），比较它们对知识注入与遗忘的影响，并分析RAG方法和参数微调方法的优劣。同时尝试让模型自助生成合成训练数据，探索自我改进的路径。

Result: 实验发现：1）仅靠少量数据继续预训练提升有限；2）多样化文本增强显著增强新知识学习，尤其是通过多种提示诱导生成数据的方法效果更佳；3）小数据情境下知识获取与旧知识保持呈微妙平衡，容易出现遗忘现象；4）RAG方法虽有效，但更易导致模型在控制数据集上的能力下降；5）模型自身可生成有效合成数据支持训练。

Conclusion: 少量异构数据通过数据增强和多样化提示有助于高效注入新知识，但需注意遗忘问题，参数微调比RAG方法对模型原能力影响小。公开的实验资源为后续小规模知识注入的研究提供了基础。

Abstract: Large language models (LLMs) often require vast amounts of text to
effectively acquire new knowledge. While continuing pre-training on large
corpora or employing retrieval-augmented generation (RAG) has proven
successful, updating an LLM with only a few thousand or million tokens remains
challenging. In this work, we investigate the task of injecting small,
unstructured information into LLMs and its relation to the catastrophic
forgetting phenomenon. We use a dataset of recent news -- ensuring no overlap
with the model's pre-training data -- to evaluate the knowledge acquisition by
probing the model with question-answer pairs related the learned information.
Starting from a continued pre-training baseline, we explored different
augmentation algorithms to generate synthetic data to improve the knowledge
acquisition capabilities. Our experiments show that simply continuing
pre-training on limited data yields modest improvements, whereas exposing the
model to diverse textual variations significantly improves the learning of new
facts -- particularly with methods that induce greater variability through
diverse prompting. Furthermore, we shed light on the forgetting phenomenon in
small-data regimes, illustrating the delicate balance between learning new
content and retaining existing capabilities. We also confirm the sensitivity of
RAG-based approaches for knowledge injection, which often lead to greater
degradation on control datasets compared to parametric methods. Finally, we
demonstrate that models can generate effective synthetic training data
themselves, suggesting a pathway toward self-improving model updates. All code
and generated data used in our experiments are publicly available, providing a
resource for studying efficient knowledge injection in LLMs with limited data
at https://github.com/hugoabonizio/knowledge-injection-methods.

</details>


### [124] [DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration](https://arxiv.org/abs/2508.06186)
*Ali Sarabadani,Maryam Abdollahi Shamami,Hamidreza Sadeghsalehi,Borhan Asadi,Saba Hesaraki*

Main category: cs.CL

TL;DR: 本文提出DKG-LLM框架，通过将动态知识图谱与大型语言模型Grok 3相结合，利用自适应语义融合算法处理复杂医疗数据，实现高准确率的医疗诊断与个性化治疗推荐。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在医疗领域仍有诊断准确性和推荐个性化的局限，同时现有医疗知识图谱更新滞后。本文旨在通过动态构建知识图谱，将异构医疗数据与最新大模型结合，实现更高效准确的医疗智能辅助系统。

Method: 提出DKG-LLM架构，将动态知识图谱（DKG）与Grok 3 LLM集成。应用自适应语义融合算法（ASFA）从临床报告、文献、病人数据中提取信息，结合概率模型、贝叶斯推断和图优化，动态扩展与更新知识图。在MIMIC-III和PubMed等真实数据集上进行评估。

Result: DKG-LLM框架生成包含13类节点与26类关系的大型知识图谱，支持多达近百万条边。在医学诊断准确率达到84.19%，治疗推荐准确率达89.63%，语义覆盖93.48%。

Conclusion: DKG-LLM能够高效处理复杂与噪声较大的医疗数据，实现疾病诊断和治疗推荐的高准确性，是医疗AI领域有变革意义的工具，并可通过医生反馈持续学习。

Abstract: Large Language Models (LLMs) have grown exponentially since the release of
ChatGPT. These models have gained attention due to their robust performance on
various tasks, including language processing tasks. These models achieve
understanding and comprehension of tasks by training billions of parameters.
The development of these models is a transformative force in enhancing natural
language understanding and has taken a significant step towards artificial
general intelligence (AGI). In this study, we aim to present the DKG-LLM
framework. The DKG-LLM framework introduces a groundbreaking approach to
medical diagnosis and personalized treatment recommendations by integrating a
dynamic knowledge graph (DKG) with the Grok 3 large language model. Using the
Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data
(including clinical reports and PubMed articles) and patient records
dynamically generate a knowledge graph consisting of 15,964 nodes in 13
distinct types (e.g., diseases, symptoms, treatments, patient profiles) and
127,392 edges in 26 relationship types (e.g., causal, therapeutic,
association). ASFA utilizes advanced probabilistic models, Bayesian inference,
and graph optimization to extract semantic information, dynamically updating
the graph with approximately 150 new nodes and edges in each data category
while maintaining scalability with up to 987,654 edges. Real-world datasets,
including MIMIC-III and PubMed, were utilized to evaluate the proposed
architecture. The evaluation results show that DKG-LLM achieves a diagnostic
accuracy of 84.19%. The model also has a treatment recommendation accuracy of
89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and
transformative tool that handles noisy data and complex multi-symptom diseases,
along with feedback-based learning from physician input.

</details>


### [125] [Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation](https://arxiv.org/abs/2508.06194)
*Lai Jiang,Yuekang Li,Xiaohan Zhang,Youtao Ding,Li Pan*

Main category: cs.CL

TL;DR: 本文提出了SceneJailEval评估框架，实现了对大语言模型（LLM）越狱情况的精准、多维度评估，并发布了多场景数据集，突破了以往评估方法的适用性和精度限制。


<details>
  <summary>Details</summary>
Motivation: 现有越狱检测多以二分类为主，只能简单判断越狱是否发生，无法量化危害程度；多维度评估方法又存在“一刀切”问题，导致不同场景评估不匹配。为提升越狱评估的准确性和适应性，需要更灵活、细致的评估体系和高质量的多场景数据集。

Method: 提出SceneJailEval框架，根据不同场景自适应地采用多维度、多标准评估方式，具有良好的可扩展性。作者还构建了包含14个场景的多样化数据集，用于衡量和检验评估方法的有效性。

Result: SceneJailEval在全场景数据集上取得了F1分数0.917（较现有最高方法提升6%），在JBB数据集上F1为0.995（提升3%），超越了现有方法在不同场景下的准确性表现。

Conclusion: SceneJailEval极大提升了大模型越狱检测与评估的精度、适用性和场景覆盖性，为相关研究和红队测试提供了更加科学完善的工具。

Abstract: Precise jailbreak evaluation is vital for LLM red teaming and jailbreak
research. Current approaches employ binary classification ( e.g., string
matching, toxic text classifiers, LLM-driven methods), yielding only "yes/no"
labels without quantifying harm intensity. Existing multi-dimensional
frameworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)
apply uniform evaluation criteria across scenarios, resulting in
scenario-specific mismatches--for instance, "Relative Truthfulness" is
irrelevant to "hate speech"--which compromise evaluation precision. To tackle
these limitations, we introduce SceneJailEval, with key contributions: (1) A
groundbreaking scenario-adaptive multi-dimensional framework for jailbreak
evaluation, overcoming the critical "one-size-fits-all" constraint of existing
multi-dimensional methods, and featuring strong extensibility to flexibly adapt
to customized or emerging scenarios. (2) A comprehensive 14-scenario dataset
with diverse jailbreak variants and regional cases, filling the long-standing
gap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)
SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on
our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over
prior SOTA), surpassing accuracy limits of existing evaluation methods in
heterogeneous scenarios and confirming its advantage.

</details>


### [126] [EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations](https://arxiv.org/abs/2508.06196)
*Nizi Nazar,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: 本文提出了一种针对大语言模型(LLMs)的情感智能(EI)评测新框架和基准，并对多款主流LLMs在多语言、多文化背景下进行了情感智能能力评估。通过微调尝试，发现现有训练范式对情感智能提升有限。


<details>
  <summary>Details</summary>
Motivation: 情感智能是人类对齐大模型的重要但被忽视的维度，现有LLMs在复杂情感理解与回应上能力不足，缺乏系统性的评测基准和训练方法。

Method: （1）提出了EI四层次心理学分类法（情感追踪、原因推断、评价和情感反应生成）；（2）构建了多回合MCQ风格的EICAP-Bench基准，评价LLMs在多语种、多文化下的情感智能；（3）对多个主流LLMs进行基准测试，并对Qwen2.5模型用LoRA在大规模对话数据上进行英阿双语微调。

Result: Qwen2.5-Instruct表现最佳。微调后仅对Appraisal层的情感智能表现有显著提升，其它EI层效果提升不明显。

Conclusion: 现有预训练及指令微调范式难以显著提升LLMs的深层情感推理能力，未来需要针对性数据和建模手段以实现更全面的情感智能对齐。

Abstract: Emotional Intelligence (EI) is a critical yet underexplored dimension in the
development of human-aligned LLMs. To address this gap, we introduce a unified,
psychologically grounded four-layer taxonomy of EI tailored for large language
models (LLMs), encompassing emotional tracking, cause inference, appraisal, and
emotionally appropriate response generation. Building on this framework, we
present EICAP-Bench, a novel MCQ style multi-turn benchmark designed to
evaluate EI capabilities in open-source LLMs across diverse linguistic and
cultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma
(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,
identifying Qwen2.5-Instruct as the strongest baseline. To assess the potential
for enhancing EI capabilities, we fine-tune both Qwen2.5-Base and
Qwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,
instruction-tuned dialogue dataset, in both English and Arabic. Our statistical
analysis reveals that among the five EI layers, only the Appraisal layer shows
significant improvement through UC-based fine-tuning. These findings highlight
the limitations of existing pretraining and instruction-tuning paradigms in
equipping LLMs with deeper emotional reasoning and underscore the need for
targeted data and modeling strategies for comprehensive EI alignment.

</details>


### [127] [Classification is a RAG problem: A case study on hate speech detection](https://arxiv.org/abs/2508.06204)
*Richard Willats,Josh Pennington,Aravind Mohan,Bertie Vidgen*

Main category: cs.CL

TL;DR: 本文提出了一种基于检索增强生成（RAG）的内容审核分类新方法，通过在推理阶段检索与政策相关的知识进行判断，实现无需重新训练模型即可应对政策变化。


<details>
  <summary>Details</summary>
Motivation: 内容审核领域的政策变化频繁，传统分类系统需要频繁且代价高昂的重新训练，难以快速适应新政策，影响了审核的及时性和合规性。

Method: 作者提出将传统的分类任务转变为依照检索到的政策知识进行内容判断。具体采用Contextual Policy Engine（CPE），即以RAG框架为基础的智能系统，在推理时查找相关政策片段，用于针对性地判断内容。应用中，如仇恨言论检测，将判断问题由“这是仇恨言论吗？”转变为“这是否违反仇恨言论政策？”。

Result: CPE系统在三项实验中表现出与主流商用系统相当的分类准确率。同时，该系统天然具备高可解释性（通过引用具体政策片段），而且能够动态调整政策而无需模型重新训练。例如，能细粒度地调控对具体身份群体的保护水平且不会影响整体性能。

Conclusion: 基于RAG的方法能够为内容审核等分类任务带来更强的灵活性、透明度和适应性，克服了传统方法对于政策变化的适应瓶颈，在更广泛的分类问题上具备应用潜力。

Abstract: Robust content moderation requires classification systems that can quickly
adapt to evolving policies without costly retraining. We present classification
using Retrieval-Augmented Generation (RAG), which shifts traditional
classification tasks from determining the correct category in accordance with
pre-trained parameters to evaluating content in relation to contextual
knowledge retrieved at inference. In hate speech detection, this transforms the
task from "is this hate speech?" to "does this violate the hate speech policy?"
  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates
this approach and offers three key advantages: (1) robust classification
accuracy comparable to leading commercial systems, (2) inherent explainability
via retrieved policy segments, and (3) dynamic policy updates without model
retraining. Through three experiments, we demonstrate strong baseline
performance and show that the system can apply fine-grained policy control by
correctly adjusting protection for specific identity groups without requiring
retraining or compromising overall performance. These findings establish that
RAG can transform classification into a more flexible, transparent, and
adaptable process for content moderation and wider classification problems.

</details>


### [128] [InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?](https://arxiv.org/abs/2508.06220)
*Keummin Ka,Junhyeong Park,Jahyun Jeon,Youngjae Yu*

Main category: cs.CL

TL;DR: 本论文提出了InfoCausalQA基准，用于评估现有视觉-语言模型在信息图基础上的因果推理能力，结果显示现有模型在此方面表现远低于人类，存在显著提升空间。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）在感知和推理方面取得了进展，但因果推理——尤其是在多模态环境下——能力尚未被充分研究。鉴于因果推理是人类认知的核心，亟需系统评估和提升AI在该领域的能力。

Method: 研究团队提出了InfoCausalQA基准，包含两项任务：任务一评估模型对数字趋势的量化因果推理，任务二针对五类因果关系（原因、结果、干预、反事实和时间）评估语义因果推理。研究手工收集并人审494组信息图及文本数据，利用GPT-4o生成1,482道多项选择题，确保题目需要真正的视觉理解和因果推理。

Result: 实验发现：当前主流VLMs在计算推理上能力有限，在语义因果推理上的表现更为不足。所有模型在人类基准线下有较大差距。

Conclusion: 多模态AI系统在基于信息图的因果推理能力上还有很大提升空间，InfoCausalQA的提出有助于推动该领域的研究进展。

Abstract: Recent advances in Vision-Language Models (VLMs) have demonstrated impressive
capabilities in perception and reasoning. However, the ability to perform
causal inference -- a core aspect of human cognition -- remains underexplored,
particularly in multimodal settings. In this study, we introduce InfoCausalQA,
a novel benchmark designed to evaluate causal reasoning grounded in
infographics that combine structured visual data with textual context. The
benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning
based on inferred numerical trends, while Task 2 targets semantic causal
reasoning involving five types of causal relations: cause, effect,
intervention, counterfactual, and temporal. We manually collected 494
infographic-text pairs from four public sources and used GPT-4o to generate
1,482 high-quality multiple-choice QA pairs. These questions were then
carefully revised by humans to ensure they cannot be answered based on
surface-level cues alone but instead require genuine visual grounding. Our
experimental results reveal that current VLMs exhibit limited capability in
computational reasoning and even more pronounced limitations in semantic causal
reasoning. Their significantly lower performance compared to humans indicates a
substantial gap in leveraging infographic-based information for causal
inference. Through InfoCausalQA, we highlight the need for advancing the causal
reasoning abilities of multimodal AI systems.

</details>


### [129] [Large Language Model Data Generation for Enhanced Intent Recognition in German Speech](https://arxiv.org/abs/2508.06277)
*Theresa Pekarek Rosin,Burak Can Kaplan,Stefan Wermter*

Main category: cs.CL

TL;DR: 本文提出了一种面向德国老年人语音指令识别的新方法，有效提升了意图识别的准确性和鲁棒性，并发现小型领域特定的LLM（如LeoLM）在德语场景中优于更大模型（ChatGPT）。


<details>
  <summary>Details</summary>
Motivation: 现有的语音意图识别方法大多基于英语、短指令且数据资源丰富，对德语及老年人语音的适应性较差。德语老年语音资源稀缺，迫切需要提升相关AI助理的实用性和准确性。

Method: 方法包括：1）构建并微调Whisper语音识别模型，使其适应德国老年人口音和发音（SVC-de）；2）利用LeoLM、Llama3和ChatGPT三种大型语言模型生成的合成文本，训练Transformer系列的意图识别模型；3）结合文本到语音技术合成训练用语音，进行跨数据集测试评估。

Result: 合成的语言模型数据显著提升了模型在不同说话风格及新词汇下的分类性能与鲁棒性。LeoLM（13B）生成的数据在效果上超越了体量更大的ChatGPT（175B），尤其符合德语场景的需求。

Conclusion: 生成式AI能有效解决低资源领域的数据缺口，小型领域特定语言模型在专业语种意图识别中优于通用型超大模型。所提方法及其数据生成、训练流程具有高度透明性和可复现性，为相关领域研究提供了范例。

Abstract: Intent recognition (IR) for speech commands is essential for artificial
intelligence (AI) assistant systems; however, most existing approaches are
limited to short commands and are predominantly developed for English. This
paper addresses these limitations by focusing on IR from speech by elderly
German speakers. We propose a novel approach that combines an adapted Whisper
ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based
language models trained on synthetic text datasets generated by three
well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To
evaluate the robustness of our approach, we generate synthetic speech with a
text-to-speech model and conduct extensive cross-dataset testing. Our results
show that synthetic LLM-generated data significantly boosts classification
performance and robustness to different speaking styles and unseen vocabulary.
Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the
much larger ChatGPT (175B) in dataset quality for German intent recognition.
Our approach demonstrates that generative AI can effectively bridge data gaps
in low-resource domains. We provide detailed documentation of our data
generation and training process to ensure transparency and reproducibility.

</details>


### [130] [Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC](https://arxiv.org/abs/2508.06309)
*Ruichong Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Matrix-Driven Instant Review（MDIR）的新方法，用于检测大语言模型（LLMs）中的抄袭行为，并在实验中证明其高效且稳定。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的使用和开源，知识产权侵权问题日益严重，尤其是模型权重的直接复制、剪枝或持续预训练导致的抄袭行为，这会对原创开发者造成很大损害。目前的检测方法在权重对应重建、统计显著性和误报控制等方面存在不足。

Method: MDIR方法基于矩阵分析和大偏差理论，能够精确重建模型权重关系，给出严格的p值评估，并且仅需关注权重相似性，无需完整推理模型。此外，检测可以在单台PC一小时内完成。

Result: 实验表明，MDIR即使在权重经历随机置换和规模化持续预训练后的情况下，依然能可靠地检测抄袭行为，且检测效率高、资源消耗低。

Conclusion: MDIR有效弥补了现有LLM抄袭检测方法的短板，具备准确、严谨、高效和易用等优势，对知识产权保护具有重要意义。

Abstract: In recent years, concerns about intellectual property (IP) in large language
models (LLMs) have grown significantly. Plagiarizing other LLMs (through direct
weight copying, upcycling, pruning, or continual pretraining) and claiming
authorship without properly attributing to the original license, is a serious
misconduct that can lead to significant financial and reputational harm to the
original developers. However, existing methods for detecting LLM plagiarism
fall short in key areas. They fail to accurately reconstruct weight
correspondences, lack the ability to compute statistical significance measures
such as $p$-values, and may mistakenly flag models trained on similar data as
being related. To address these limitations, we propose Matrix-Driven Instant
Review (MDIR), a novel method that leverages matrix analysis and Large
Deviation Theory. MDIR achieves accurate reconstruction of weight
relationships, provides rigorous $p$-value estimation, and focuses exclusively
on weight similarity without requiring full model inference. Experimental
results demonstrate that MDIR reliably detects plagiarism even after extensive
transformations, such as random permutations and continual pretraining with
trillions of tokens. Moreover, all detections can be performed on a single PC
within an hour, making MDIR both efficient and accessible.

</details>


### [131] [Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering](https://arxiv.org/abs/2508.06345)
*Yanbin Wei,Jiangyue Yan,Chun Kang,Yang Chen,Hua Liu,James T. Kwok,Yu Zhang*

Main category: cs.CL

TL;DR: 本文提出DynamicTRF框架，通过为图问答任务动态选择最优的图结构表示形式（TRF），提升了大规模多模态模型（LMMs）在零样本图问答任务中的准确性与简明性。


<details>
  <summary>Details</summary>
Motivation: 目前LMMs在处理图问答任务时一般只采用单一的图结构表示方式，这种“一刀切”方案未考虑不同模型或任务的特殊需求，容易造成答复冗长或不准确。作者希望通过更细致的TRF选择，提升问答质量。

Method: 作者首先分析了现有TRF的优缺点，设计出用于零样本图问答的一系列TRF，然后提出图响应效率（GRE）新指标用于衡量答复的性能与简练之间的平衡。基于此，构建DynamicTRF框架：1）先构建TRF偏好（TRFP）数据集，评测各种TRF的GRE得分并排序；2）在TRFP数据集上训练TRF路由器，使其在推理阶段根据问题自动选择最优TRF。

Result: 在7个领域内的算法图问答任务和2个跨领域下游任务上，DynamicTRF显著提升了LMM的零样本图问答准确率。

Conclusion: 通过动态自适应选择图表示形式，DynamicTRF有效改善了LMM针对不同图问答任务的表现，特别在提升答复准确性和简明性方面有显著效果。

Abstract: Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities
in diverse domain question-answering (QA) tasks, including graph QA that
involves complex graph topologies. However, most current approaches use only a
single type of graph representation, namely Topology Representation Form (TRF),
such as prompt-unified text descriptions or style-fixed visual styles. Those
"one-size-fits-all" approaches fail to consider the specific preferences of
different models or tasks, often leading to incorrect or overly long responses.
To address this, we first analyze the characteristics and weaknesses of
existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to
zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency
(GRE), which measures the balance between the performance and the brevity in
graph QA. Built on these, we develop the DynamicTRF framework, which aims to
improve both the accuracy and conciseness of graph QA. To be specific,
DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based
on their GRE scores, to probe the question-specific TRF preferences. Then it
trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from
$F_{ZS}$ for each question during the inference. Extensive experiments across 7
in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show
that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms
of accuracy

</details>


### [132] [Cyberbullying Detection via Aggression-Enhanced Prompting](https://arxiv.org/abs/2508.06360)
*Aisha Saeid,Anu Sabu,Girish A. Koushik,Ferrante Neri,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 该论文通过将攻击性检测作为辅助任务，提出一种提升大模型在社交媒体网络欺凌检测任务中表现的方法，并验证了包含攻击性信息的提示对提升检测能力的作用。


<details>
  <summary>Details</summary>
Motivation: 网络欺凌在社交媒体中表现多样且隐蔽，自动检测面临很大挑战。研究者希望通过引入攻击性言论检测这一相关任务，提升大语言模型对网络欺凌检测的泛化能力和性能。

Method: 论文利用五个攻击性数据集和一个网络欺凌数据集，对指令微调的大语言模型进行训练。实验包括零样本、少样本、独立LoRA微调、多任务学习和提出的增强提示管道（在提示中嵌入攻击性预测信息），比较各种训练方案的效果。

Result: 实验结果表明，多任务学习效果不稳定，而引入攻击性信息的增强提示方案，在所有实验中一致优于标准LoRA微调，显著提升了模型识别网络欺凌的能力。

Conclusion: 将攻击性检测作为辅助任务，并通过提示管道将该信息融入主任务中，可有效提升大语言模型在网络安全相关的社交媒体文本检测任务中的泛化与性能。

Abstract: Detecting cyberbullying on social media remains a critical challenge due to
its subtle and varied expressions. This study investigates whether integrating
aggression detection as an auxiliary task within a unified training framework
can enhance the generalisation and performance of large language models (LLMs)
in cyberbullying detection. Experiments are conducted on five aggression
datasets and one cyberbullying dataset using instruction-tuned LLMs. We
evaluated multiple strategies: zero-shot, few-shot, independent LoRA
fine-tuning, and multi-task learning (MTL). Given the inconsistent results of
MTL, we propose an enriched prompt pipeline approach in which aggression
predictions are embedded into cyberbullying detection prompts to provide
contextual augmentation. Preliminary results show that the enriched prompt
pipeline consistently outperforms standard LoRA fine-tuning, indicating that
aggression-informed context significantly boosts cyberbullying detection. This
study highlights the potential of auxiliary tasks, such as aggression
detection, to improve the generalisation of LLMs for safety-critical
applications on social networks.

</details>


### [133] [Evaluating Style-Personalized Text Generation: Challenges and Directions](https://arxiv.org/abs/2508.06374)
*Anubhav Jangra,Bahareh Sarrafzadeh,Adrian de Wynter,Silviu Cucerzan,Sujay Kumar Jauhar*

Main category: cs.CL

TL;DR: 本论文探讨了在低资源作者风格个性化文本生成领域，如何更有效评价结果，并提出采用多种评测方法的集成来提升评价效果。


<details>
  <summary>Details</summary>
Motivation: 现有关于文本风格个性化生成的研究多集中在工具和基准建设，对如何进行有效评测，尤其是在低资源场景下，探讨较少。传统指标如BLEU和ROUGE未必能充分反映“风格”维度，因此需要新的评价方法。

Method: 作者提出了使用风格嵌入（style embeddings）和大模型裁判（LLM-as-judge）等新型评价方式，构建了覆盖八种写作任务的风格判别基准，并在领域判别、作者鉴别、LLM个性化与非个性化区分等三类任务上评测了各评价指标及其集成效果。

Result: 实验证明，现有指标（如BLEU、ROUGE）存在局限，新型指标以及多指标集成能“更全面有效”地评价风格化文本生成任务。

Conclusion: 强烈建议在个性化文本生成的评测中采用多元化评价指标的集成方法，可以更好地反映模型对风格个性化的把握。

Abstract: While prior research has built tools and benchmarks towards style
personalized text generation, there has been limited exploration of evaluation
in low-resource author style personalized text generation space. Through this
work, we question the effectiveness of the widely adopted evaluation metrics
like BLEU and ROUGE, and explore other evaluation paradigms such as style
embeddings and LLM-as-judge to holistically evaluate the style personalized
text generation task. We evaluate these metrics and their ensembles using our
style discrimination benchmark, that spans eight writing tasks, and evaluates
across three settings, domain discrimination, authorship attribution, and LLM
personalized vs non-personalized discrimination. We provide conclusive evidence
to adopt ensemble of diverse evaluation metrics to effectively evaluate style
personalized text generation.

</details>


### [134] [LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing](https://arxiv.org/abs/2508.06388)
*Lanlan Qiu,Xiao Pu,Yeqi Feng,Tianxing He*

Main category: cs.CL

TL;DR: 本文提出了首个支持情感陪伴角色扮演的ChatAnime数据集，聚焦于动漫角色，系统对比并评价LLMs与人类在角色扮演和情感支持能力。结果显示顶尖LLM在这两方面优于真人粉丝，但回复多样性仍低于人类。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs分别在角色扮演或情感支持有突出表现，但如何结合这两能力用于虚拟角色的情感支持尚缺乏研究。由于动漫角色个性鲜明且粉丝群体庞大，选用其作为案例可更有效评估LLMs在保持角色特质情况下的情感支持能力。

Method: 作者精心挑选了20个顶级动漫角色，设计60个情感相关真实场景问题，通过全国范围筛选40位中国动漫高阶粉丝，分别与10种LLMs和这些真人粉丝进行两轮对话，最终形成2400个人类样本和24000个LLMs样本并由人工注释。评估体系包含基本对话、角色扮演、情感支持三维度的9个细分指标和总体多样性综合指标。

Result: 实验结果表明，顶级LLMs在角色扮演和情感支持方面均超越了高阶动漫粉丝人类，但在人类回复多样性方面仍具短板。

Conclusion: 本文首次构建并公开了情感陪伴角色扮演数据集，为LLMs在此领域的研究优化提供了宝贵的资源和评测基准，有望推动虚拟角色情感交互领域发展。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
role-playing conversations and providing emotional support as separate research
directions. However, there remains a significant research gap in combining
these capabilities to enable emotionally supportive interactions with virtual
characters. To address this research gap, we focus on anime characters as a
case study because of their well-defined personalities and large fan bases.
This choice enables us to effectively evaluate how well LLMs can provide
emotional support while maintaining specific character traits. We introduce
ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We
first thoughtfully select 20 top-tier characters from popular anime communities
and design 60 emotion-centric real-world scenario questions. Then, we execute a
nationwide selection process to identify 40 Chinese anime enthusiasts with
profound knowledge of specific characters and extensive experience in
role-playing. Next, we systematically collect two rounds of dialogue data from
10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP
performance of LLMs, we design a user experience-oriented evaluation system
featuring 9 fine-grained metrics across three dimensions: basic dialogue,
role-playing and emotional support, along with an overall metric for response
diversity. In total, the dataset comprises 2,400 human-written and 24,000
LLM-generated answers, supported by over 132,000 human annotations.
Experimental results show that top-performing LLMs surpass human fans in
role-playing and emotional support, while humans still lead in response
diversity. We hope this work can provide valuable resources and insights for
future research on optimizing LLMs in ESRP. Our datasets are available at
https://github.com/LanlanQiu/ChatAnime.

</details>


### [135] [Quantifying Conversation Drift in MCP via Latent Polytope](https://arxiv.org/abs/2508.06418)
*Haoran Shi,Hongwei Yao,Shuo Shao,Shaopeng Jiao,Ziqi Peng,Zhan Qin,Cong Wang*

Main category: cs.CL

TL;DR: 本文提出SecMCP框架，通过建模LLM激活向量的潜空间，检测由外部工具引入的对话漂移，有效防护MCP协议下的对话劫持等安全威胁。


<details>
  <summary>Details</summary>
Motivation: MCP协议将外部工具整合进大语言模型，提升了任务处理能力，但开放环境带来了对话被劫持、数据泄露等严重安全隐患，现有防御手段无法动态、有效检测并量化此类威胁。

Method: 作者提出SecMCP安全框架，建模LLM的激活向量于潜多面体空间，捕捉因恶意外部知识输入导致的对话动力学异常，用于主动检测对话漂移、劫持和敏感数据外泄。

Result: 在Llama3、Vicuna、Mistral等主流LLM上进行实验，配合MS MARCO、HotpotQA、FinQA数据集，SecMCP在检测对话异常上的AUROC超0.915，且系统可用性未受显著影响。

Conclusion: SecMCP可系统性识别并量化MCP协议下的安全威胁，提出的潜多面体空间方法在实际多模型多场景下有效，有助于提升集成工具的大模型应用的安全性。

Abstract: The Model Context Protocol (MCP) enhances large language models (LLMs) by
integrating external tools, enabling dynamic aggregation of real-time data to
improve task execution. However, its non-isolated execution context introduces
critical security and privacy risks. In particular, adversarially crafted
content can induce tool poisoning or indirect prompt injection, leading to
conversation hijacking, misinformation propagation, or data exfiltration.
Existing defenses, such as rule-based filters or LLM-driven detection, remain
inadequate due to their reliance on static signatures, computational
inefficiency, and inability to quantify conversational hijacking. To address
these limitations, we propose SecMCP, a secure framework that detects and
quantifies conversation drift, deviations in latent space trajectories induced
by adversarial external knowledge. By modeling LLM activation vectors within a
latent polytope space, SecMCP identifies anomalous shifts in conversational
dynamics, enabling proactive detection of hijacking, misleading, and data
exfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,
Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),
demonstrating robust detection with AUROC scores exceeding 0.915 while
maintaining system usability. Our contributions include a systematic
categorization of MCP security threats, a novel latent polytope-based
methodology for quantifying conversation drift, and empirical validation of
SecMCP's efficacy.

</details>


### [136] [Memp: Exploring Agent Procedural Memory](https://arxiv.org/abs/2508.06433)
*Runnan Fang,Yuan Liang,Xiaobin Wang,Jialong Wu,Shuofei Qiao,Pengjun Xie,Fei Huang,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种让大型语言模型（LLMs）驱动的智能体拥有可学习、可更新和可持续的程序性记忆机制。通过Memp方法，将智能体的历史轨迹精炼为细粒度步骤和高层抽象脚本，并研究其构建、检索、更新策略，对智能体性能有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在多任务处理上表现优秀，但其程序性记忆依赖人工设计或静态参数，导致灵活性和可持续性不足。需要一种能够持续学习、动态调整的程序性记忆机制，提升智能体的终身学习与适应能力。

Method: 提出Memp方法，将智能体过往轨迹蒸馏为细致的分步指令和高层次脚本，系统性探讨程序性记忆的构建（Build）、检索（Retrieval）和更新（Update）策略。设计动态机制，确保记忆随新经验不断演化、修正和弃用无效内容。

Result: 在TravelPlanner和ALFWorld两个任务的实证评测中，随着程序性记忆库的优化，智能体在相似任务上的成功率和效率稳步提升。此外，将强模型构建的程序记忆迁移到弱模型，也能显著增强后者性能。

Conclusion: 动态可学习的程序性记忆显著提升智能体的长期表现与泛化能力，并可实现模型间的知识迁移。该方法为实现自主学习和持续成长的智能体提供了新路径。

Abstract: Large Language Models (LLMs) based agents excel at diverse tasks, yet they
suffer from brittle procedural memory that is manually engineered or entangled
in static parameters. In this work, we investigate strategies to endow agents
with a learnable, updatable, and lifelong procedural memory. We propose Memp
that distills past agent trajectories into both fine-grained, step-by-step
instructions and higher-level, script-like abstractions, and explore the impact
of different strategies for Build, Retrieval, and Update of procedural memory.
Coupled with a dynamic regimen that continuously updates, corrects, and
deprecates its contents, this repository evolves in lockstep with new
experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as
the memory repository is refined, agents achieve steadily higher success rates
and greater efficiency on analogous tasks. Moreover, procedural memory built
from a stronger model retains its value: migrating the procedural memory to a
weaker model yields substantial performance gains.

</details>


### [137] [Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages](https://arxiv.org/abs/2508.06435)
*Andrea Nasuto,Stefano Maria Iacus,Francisco Rowe,Devika Jain*

Main category: cs.CL

TL;DR: 本研究探讨大型语言模型（LLMs）经过少量语言微调后在未见过语言上的迁移能力，特别是在对13种语言的移民话题推文进行分类任务中的表现。实验表明，只需单语或双语微调就能实现主题层面的跨语言迁移，但态度识别（如支持或反对移民）则需多语微调。对低资源语言的微小微调可显著纠正预训练偏差，且开源模型推理速度和成本远优于商用GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的灵活性和强大能力让其在社会科学研究中作用日益突出，但主流观点认为实现多语言跨任务泛化需要大量多语数据。本研究旨在验证是否通过有限语种的微调，便可在主题分类等任务上实现具有表现力的跨语言迁移，并纠正对小语种的不公平偏置。

Method: 选用LLaMA 3 2-3B轻量模型，对其进行单语、双语或多语数据集微调，任务为对X/Twitter上13种语言的移民相关推文进行主题和态度分类。微调数据规模极小，重点考察未见语言的泛化及微调对低资源语言表现的改善。评测模型在不同语言及分类维度（主题/立场）下的准确率与偏差纠正能力。

Result: 在只用一到两种语言进行微调的前提下，模型能可靠地对未见过语言的移民内容进行主题分类。但针对推文立场（支持/反对移民）分类，多语微调会显著提升表现。即便极低比例的低资源语种样本参与微调，也大幅缓解了主流语种偏置。模型开源量化后，推理速度提升35倍，推理成本远低于商用模型。

Conclusion: 有限的语言微调足以实现主题级的跨语言迁移，纠正结构性偏见不必采用大规模多语微调。所提出开源模型为社会科学研究提供了高效、低成本、可复现的LLM替代方案。

Abstract: Large language models (LLMs) are transforming social-science research by
enabling scalable, precise analysis. Their adaptability raises the question of
whether knowledge acquired through fine-tuning in a few languages can transfer
to unseen languages that only appeared during pre-training. To examine this, we
fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or
multilingual data sets to classify immigration-related tweets from X/Twitter
across 13 languages, a domain characterised by polarised, culturally specific
discourse. We evaluate whether minimal language-specific fine-tuning enables
cross-lingual topic detection and whether adding targeted languages corrects
pre-training biases. Results show that LLMs fine-tuned in one or two languages
can reliably classify immigration-related content in unseen languages. However,
identifying whether a tweet expresses a pro- or anti-immigration stance
benefits from multilingual fine-tuning. Pre-training bias favours dominant
languages, but even minimal exposure to under-represented languages during
fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training
token volume) yields significant gains. These findings challenge the assumption
that cross-lingual mastery requires extensive multilingual training: limited
language coverage suffices for topic-level generalisation, and structural
biases can be corrected with lightweight interventions. By releasing
4-bit-quantised, LoRA fine-tuned models, we provide an open-source,
reproducible alternative to proprietary LLMs that delivers 35 times faster
inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,
enabling scalable, inclusive research.

</details>


### [138] [Echoes of Automation: The Increasing Use of LLMs in Newsmaking](https://arxiv.org/abs/2508.06445)
*Abolfazl Ansari,Delvin Ce Zhang,Nafis Irtiza Tripto,Dongwon Lee*

Main category: cs.CL

TL;DR: 本研究分析了四万余篇新闻报道，利用AI文本检测工具发现近年生成式AI在新闻写作中的运用显著提升，尤其在地方和校园媒体中。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（如大语言模型）快速发展，其对新闻行业的诚信和作者责任带来担忧。作者想了解AI内容在新闻写作中的实际应用情况及其对新闻文体的影响。

Method: 收集了主流、地方及高校媒体的四万余篇新闻稿，涵盖多种媒体形式，应用三种先进的AI文本检测器，对文章中AI生成内容进行检测，并分析了句子层面的应用部位和语言特征。

Result: 数据显示，近年来生成式AI在新闻稿件中使用显著增加，地方和高校新闻尤为突出。句子解析发现，AI生成内容多出现在新闻导语，结尾通常由人工撰写。语言特征分析显示，AI提升了用词丰富度和可读性，但降低了文章的正式程度，致使风格趋向一致，尤以地方媒体为甚。

Conclusion: 生成式AI已广泛渗透新闻写作环节，对新闻风格和表达方式带来显著影响。虽然提升了可读性和多样性，但也带来了风格同质化和正式度下降的问题，需关注其对新闻公信力和写作规范的影响。

Abstract: The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns
for journalistic integrity and authorship. This study examines AI-generated
content across over 40,000 news articles from major, local, and college news
media, in various media formats. Using three advanced AI-text detectors (e.g.,
Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of
GenAI use in recent years, especially in local and college news. Sentence-level
analysis reveals LLMs are often used in the introduction of news, while
conclusions usually written manually. Linguistic analysis shows GenAI boosts
word richness and readability but lowers formality, leading to more uniform
writing styles, particularly in local media.

</details>


### [139] [SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning](https://arxiv.org/abs/2508.06447)
*Lingkun Long,Rubing Yang,Yushi Huang,Desheng Hui,Ao Zhou,Jianlei Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为SlimInfer的新框架，通过在前向推理过程中逐层剪除不重要的提示token，以提升大语言模型在长上下文推理任务下的推理速度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文推理方法计算需求高，虽然有方法优化了注意力计算，但每层依然需处理完整的隐藏状态，整体推理效率提升有限，需要新的高效机制。

Method: 作者提出SlimInfer，实现了动态的细粒度剪枝机制，能在每一层精确地移除多余的隐藏状态token，并由此设计出异步键值缓存管理器，无需复杂的预测器即可预取必须的token区块，从而减少内存和I/O开销。

Result: 在LLaMA3.1-8B-Instruct模型和RTX 4090硬件上的实验表明，SlimInfer能将首次生成token时间加速至2.53倍，总端到端时延降低至1.88倍，并且在LongBench任务上保持模型性能不损失。

Conclusion: SlimInfer能有效提升大语言模型长上下文推理的效率，显著减少计算、内存和I/O开销，同时保持模型的语义能力和任务性能。

Abstract: Long-context inference for Large Language Models (LLMs) is heavily limited by
high computational demands. While several existing methods optimize attention
computation, they still process the full set of hidden states at each layer,
limiting overall efficiency. In this work, we propose SlimInfer, an innovative
framework that aims to accelerate inference by directly pruning less critical
prompt tokens during the forward pass. Our key insight is an information
diffusion phenomenon: As information from critical tokens propagates through
layers, it becomes distributed across the entire sequence. This diffusion
process suggests that LLMs can maintain their semantic integrity when excessive
tokens, even including these critical ones, are pruned in hidden states.
Motivated by this, SlimInfer introduces a dynamic fine-grained pruning
mechanism that accurately removes redundant tokens of hidden state at
intermediate layers. This layer-wise pruning naturally enables an asynchronous
KV cache manager that prefetches required token blocks without complex
predictors, reducing both memory usage and I/O costs. Extensive experiments
show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token
(TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for
LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on
LongBench. Our code will be released upon acceptance.

</details>


### [140] [GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models](https://arxiv.org/abs/2508.06471)
*GLM-4. 5 Team,:,Aohan Zeng,Xin Lv,Qinkai Zheng,Zhenyu Hou,Bin Chen,Chengxing Xie,Cunxiang Wang,Da Yin,Hao Zeng,Jiajie Zhang,Kedong Wang,Lucen Zhong,Mingdao Liu,Rui Lu,Shulin Cao,Xiaohan Zhang,Xuancheng Huang,Yao Wei,Yean Cheng,Yifan An,Yilin Niu,Yuanhao Wen,Yushi Bai,Zhengxiao Du,Zihan Wang,Zilin Zhu,Bohan Zhang,Bosi Wen,Bowen Wu,Bowen Xu,Can Huang,Casey Zhao,Changpeng Cai,Chao Yu,Chen Li,Chendi Ge,Chenghua Huang,Chenhui Zhang,Chenxi Xu,Chenzheng Zhu,Chuang Li,Congfeng Yin,Daoyan Lin,Dayong Yang,Dazhi Jiang,Ding Ai,Erle Zhu,Fei Wang,Gengzheng Pan,Guo Wang,Hailong Sun,Haitao Li,Haiyang Li,Haiyi Hu,Hanyu Zhang,Hao Peng,Hao Tai,Haoke Zhang,Haoran Wang,Haoyu Yang,He Liu,He Zhao,Hongwei Liu,Hongxi Yan,Huan Liu,Huilong Chen,Ji Li,Jiajing Zhao,Jiamin Ren,Jian Jiao,Jiani Zhao,Jianyang Yan,Jiaqi Wang,Jiayi Gui,Jiayue Zhao,Jie Liu,Jijie Li,Jing Li,Jing Lu,Jingsen Wang,Jingwei Yuan,Jingxuan Li,Jingzhao Du,Jinhua Du,Jinxin Liu,Junkai Zhi,Junli Gao,Ke Wang,Lekang Yang,Liang Xu,Lin Fan,Lindong Wu,Lintao Ding,Lu Wang,Man Zhang,Minghao Li,Minghuan Xu,Mingming Zhao,Mingshu Zhai,Pengfan Du,Qian Dong,Shangde Lei,Shangqing Tu,Shangtong Yang,Shaoyou Lu,Shijie Li,Shuang Li,Shuang-Li,Shuxun Yang,Sibo Yi,Tianshu Yu,Wei Tian,Weihan Wang,Wenbo Yu,Weng Lam Tam,Wenjie Liang,Wentao Liu,Xiao Wang,Xiaohan Jia,Xiaotao Gu,Xiaoying Ling,Xin Wang,Xing Fan,Xingru Pan,Xinyuan Zhang,Xinze Zhang,Xiuqing Fu,Xunkai Zhang,Yabo Xu,Yandong Wu,Yida Lu,Yidong Wang,Yilin Zhou,Yiming Pan,Ying Zhang,Yingli Wang,Yingru Li,Yinpei Su,Yipeng Geng,Yitong Zhu,Yongkun Yang,Yuhang Li,Yuhao Wu,Yujiang Li,Yunan Liu,Yunqing Wang,Yuntao Li,Yuxuan Zhang,Zezhen Liu,Zhen Yang,Zhengda Zhou,Zhongpei Qiao,Zhuoer Feng,Zhuorui Liu,Zichen Zhang,Zihan Wang,Zijun Yao,Zikang Wang,Ziqiang Liu,Ziwei Chai,Zixuan Li,Zuodong Zhao,Wenguang Chen,Jidong Zhai,Bin Xu,Minlie Huang,Hongning Wang,Juanzi Li,Yuxiao Dong,Jie Tang*

Main category: cs.CL

TL;DR: GLM-4.5是一款开源的大型MoE语言模型，参数量达355B，支持混合推理，性能优异，参数更少但效果领先，促进智能体与推理AI研究，同时发布了轻量版，代码与模型已开放。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的不断发展，提升AI系统在推理和智能体任务上的能力成为研究热点。同时，当前市面上的顶尖模型存在效率低、参数极多等问题，限制了其实际应用和研究创新。该论文希望通过更高效、参数更优的模型推动推理与智能体AI的发展。

Method: 本论文提出了GLM-4.5模型，采用MoE（专家混合）结构，参数总量为355B，实际激活参数为32B。模型支持混合推理方法，在多阶段23T token大数据训练基础上，通过专家迭代与强化学习进行全方位后训练。同时，发布了标准与轻量版两种模型，并进行了细致的基准测试。

Result: GLM-4.5在各类推理与智能体任务上表现优异，在TAU-Bench获得70.1%，AIME 24获得91.0%，SWE-bench Verified获得64.2%。尽管参数数量少于一些同类模型，整体排名第三，在智能体任务上排名第二。

Conclusion: GLM-4.5以更优的参数效率在多项基准测试中取得领先表现，有望推动推理与智能体AI系统的研究进步。开源的策略也将加速相关领域技术的交流和发展。

Abstract: We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language
model with 355B total parameters and 32B activated parameters, featuring a
hybrid reasoning method that supports both thinking and direct response modes.
Through multi-stage training on 23T tokens and comprehensive post-training with
expert model iteration and reinforcement learning, GLM-4.5 achieves strong
performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on
TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer
parameters than several competitors, GLM-4.5 ranks 3rd overall among all
evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B
parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance
research in reasoning and agentic AI systems. Code, models, and more
information are available at https://github.com/zai-org/GLM-4.5.

</details>


### [141] [HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning](https://arxiv.org/abs/2508.06475)
*Guimin Hu,Daniel Hershcovich,Hasti Seifi*

Main category: cs.CL

TL;DR: 本论文提出了HapticLLaMA模型，可将触觉信号转化为自然语言描述，突破了以视觉和听觉为主的多模态研究局限。模型通过新颖的触觉分词及强化学习优化，在自动和人工评测上表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态研究长期关注视觉和音频领域，但真实世界中触觉感知同样重要，相关研究却较为稀缺。推动触觉信号理解有助于虚拟现实、辅助技术和康复等实际应用。作者为促进触觉内容生成，正式定义了'触觉字幕生成（haptic captioning）'任务。

Method: 作者提出HapticLLaMA模型，结合LLaMA大语言模型与两种触觉分词（基于频率和基于EnCodec）。通过先利用LLaMA架构和LoRA技术进行有监督微调，再结合基于人类反馈的强化学习（RLHF）进一步提升模型表现。

Result: 模型获得了优异的自动化指标（METEOR 59.98，BLEU-4 32.06），且61%以上生成描述在人类7分制打分中高于3.5分。特别地，RLHF带来分布提升10%，更贴近人类触觉感知。

Conclusion: 本工作证实了大语言模型能有效处理触觉信号，为多模态人工智能拓展了新的触觉范式，有望应用于虚拟现实、辅具及康复等领域。

Abstract: Haptic captioning is the task of generating natural language descriptions
from haptic signals, such as vibrations, for use in virtual reality,
accessibility, and rehabilitation applications. While previous multimodal
research has focused primarily on vision and audio, haptic signals for the
sense of touch remain underexplored. To address this gap, we formalize the
haptic captioning task and propose HapticLLaMA, a multimodal sensory language
model that interprets vibration signals into descriptions in a given sensory,
emotional, or associative category. We investigate two types of haptic
tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that
convert haptic signals into sequences of discrete units, enabling their
integration with the LLaMA model. HapticLLaMA is trained in two stages: (1)
supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,
and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We
assess HapticLLaMA's captioning performance using both automated n-gram metrics
and human evaluation. HapticLLaMA demonstrates strong capability in
interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a
BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated
captions received human ratings above 3.5 on a 7-point scale, with RLHF
yielding a 10% improvement in the overall rating distribution, indicating
stronger alignment with human haptic perception. These findings highlight the
potential of large language models to process and adapt to sensory data.

</details>


### [142] [Post-training for Efficient Communication via Convention Formation](https://arxiv.org/abs/2508.06482)
*Yilun Hua,Evan Wang,Yoav Artzi*

Main category: cs.CL

TL;DR: 本文通过特殊后训练过程，让大型语言模型（LLM）逐步获得在人类多轮交流中形成“对话约定”的能力，并在两个新的基准测试中证明了效果提升。


<details>
  <summary>Details</summary>
Motivation: 人类在多轮对话中会逐渐形成更高效的表达方式（即形成对话约定），而现有LLM缺乏这种自适应能力。本文旨在提升LLM在多轮对话中形成约定的能力，使其更拟人化、更实用。

Method: 作者通过对大量人工筛选出的“约定生成”对话进行有针对性的微调（fine-tuning），让LLM学习何时以及如何形成新的对话约定。为评估该能力，设计了两个新基准测试：一个是认知动机的交互基准（反映人类真实约定形成过程），另一个是基于文档的参考补全任务（模拟真实应用场景下的对话约定行为）。

Result: 经过微调训练后的LLM在两个新基准上均表现出显著更好的对话约定生成、适应对话风格和语言简化的能力，相比普通LLM接近或达到人类水平。

Conclusion: 通过后训练流程和新基准测试，LLM的“交互适应”能力显著提升，使其更能像人类那样凭对话历史自主形成交流习惯，对实际对话系统应用具有重大意义。

Abstract: Humans communicate with increasing efficiency in multi-turn interactions, by
adapting their language and forming ad-hoc conventions. In contrast, prior work
shows that LLMs do not naturally show this behavior. We develop a post-training
process to develop this ability through targeted fine-tuning on heuristically
identified demonstrations of convention formation. We evaluate with two new
benchmarks focused on this capability. First, we design a focused,
cognitively-motivated interaction benchmark that consistently elicits strong
convention formation trends in humans. Second, we create a new
document-grounded reference completion task that reflects in-the-wild
convention formation behavior. Our studies show significantly improved
convention formation abilities in post-trained LLMs across the two evaluation
methods.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [143] [GPU-Accelerated Barrier-Rate Guided MPPI Control for Tractor-Trailer Systems](https://arxiv.org/abs/2508.05773)
*Keyvan Majd,Hardik Parwana,Bardh Hoxha,Steven Hong,Hideki Okamoto,Georgios Fainekos*

Main category: cs.RO

TL;DR: 本文提出了一种Barrier-Rate引导的模型预测路径积分（BR-MPPI）控制方法，能够有效解决带铰接车辆在复杂、障碍物密集环境下的安全导航问题，特别是在倒车和停车场场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 带铰接车辆（如挂车、场地车）在有障碍物和行人存在的环境中倒车和机动非常困难，现有路径规划和控制方法在安全性与操作灵活性上存在局限。

Method: 提出BR-MPPI控制方法，将控制障碍函数（CBF）约束直接嵌入路径积分的更新中，通过调整采样分布，侧重探索无碰撞和动态可行路径。利用高精度CarMaker仿真器，对12米拖挂车在停车场倒车及前进停车任务进行验证，并与标准MPPI及带碰撞代价的MPPI进行对比。

Result: 在有8个障碍物场景下，BR-MPPI方法在单GPU上可实现100Hz以上的控制频率，并在停车间隙和安全性方面优于其它MPPI基线方法。

Conclusion: BR-MPPI方法显著提高了路径探索能力和轨迹鲁棒性，对于带铰接车辆在复杂环境下的安全导航和倒车具有较好应用前景。

Abstract: Articulated vehicles such as tractor-trailers, yard trucks, and similar
platforms must often reverse and maneuver in cluttered spaces where pedestrians
are present. We present how Barrier-Rate guided Model Predictive Path Integral
(BR-MPPI) control can solve navigation in such challenging environments.
BR-MPPI embeds Control Barrier Function (CBF) constraints directly into the
path-integral update. By steering the importance-sampling distribution toward
collision-free, dynamically feasible trajectories, BR-MPPI enhances the
exploration strength of MPPI and improves robustness of resulting trajectories.
The method is evaluated in the high-fidelity CarMaker simulator on a 12 [m]
tractor-trailer tasked with reverse and forward parking in a parking lot.
BR-MPPI computes control inputs in above 100 [Hz] on a single GPU (for
scenarios with eight obstacles) and maintains better parking clearance than a
standard MPPI baseline and an MPPI with collision cost baseline.

</details>


### [144] [Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction](https://arxiv.org/abs/2508.05838)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

TL;DR: 本论文提出了一种创新方法，将视觉基础模型与强化学习结合，以提升模拟环境中物体交互能力，实验显示在厨房场景中显著提升了交互成功率与导航效率。


<details>
  <summary>Details</summary>
Motivation: 目前机器人在感知与理解环境、尤其是复杂物体交互方面仍存在局限，需要更强的感知与交互能力以推动自主智能体发展。

Method: 通过将Segment Anything Model (SAM)和YOLOv5视觉模型与Proximal Policy Optimization (PPO)强化学习智能体集成，并在AI2-THOR厨房模拟环境中进行训练和评估。

Result: 与未使用高级感知辅助的基线相比，平均累计奖励提升68%，物体交互成功率提升52.5%，导航效率提升33%。

Conclusion: 将基础视觉模型与强化学习结合显著增强了机器人复杂任务能力，为实现更智能的自主体开辟了新路径。

Abstract: This paper presents a novel approach that integrates vision foundation models
with reinforcement learning to enhance object interaction capabilities in
simulated environments. By combining the Segment Anything Model (SAM) and
YOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the
AI2-THOR simulation environment, we enable the agent to perceive and interact
with objects more effectively. Our comprehensive experiments, conducted across
four diverse indoor kitchen settings, demonstrate significant improvements in
object interaction success rates and navigation efficiency compared to a
baseline agent without advanced perception. The results show a 68% increase in
average cumulative reward, a 52.5% improvement in object interaction success
rate, and a 33% increase in navigation efficiency. These findings highlight the
potential of integrating foundation models with reinforcement learning for
complex robotic tasks, paving the way for more sophisticated and capable
autonomous agents.

</details>


### [145] [Modular Vacuum-Based Fixturing System for Adaptive Disassembly Workspace Integration](https://arxiv.org/abs/2508.05936)
*Haohui Pan,Takuya Kiyokawa,Tomoki Ishikura,Shingo Hamada,Genichiro Matsuda,Kensuke Harada*

Main category: cs.RO

TL;DR: 本论文提出了一种基于真空的模块化夹持系统，可适应复杂家用小电器的弯曲表面，有效提升拆卸任务时的稳定性并超越传统的刚性夹具。


<details>
  <summary>Details</summary>
Motivation: 传统刚性夹具难以适应形状复杂和曲面丰富的小型家用电器，导致拆卸（如螺丝移除）过程效率和稳定性低。为解决这些问题，需要一种能够灵活适应不同几何表面的夹持方案。

Method: 作者提出采用市售气球型软夹持器，通过真空吸附实现对任意形状表面的自适应夹持，并配套开发了基于稳定性评估的规划框架。该框架包括底面采样、几何连续性筛选和静态稳定性（凸包分析）评价，用于优化支持点配置。此外，开展了多种夹持配置和数量的对比实验。

Result: 实验结果表明，所提系统无论在支持点数目还是配置上都显著优于传统刚性夹具。在实际拆卸任务（如拧螺丝）中，成功率和夹持稳定性均有明显提升。

Conclusion: 气球型软夹持器与真空吸附结合，可高效、稳定地固定复杂曲面电器，为拆卸自动化提供了更优解决方案。方法优于传统刚性夹具，具有实际应用前景。

Abstract: The disassembly of small household appliances poses significant challenges
due to their complex and curved geometries, which render traditional rigid
fixtures inadequate. In this paper, we propose a modular vacuum-based fixturing
system that leverages commercially available balloon-type soft grippers to
conform to arbitrarily shaped surfaces and provide stable support during
screw-removal tasks. To enable a reliable deployment of the system, we develop
a stability-aware planning framework that samples the bottom surface of the
target object, filters candidate contact points based on geometric continuity,
and evaluates support configurations using convex hull-based static stability
criteria. We compare the quality of object placement under different numbers
and configurations of balloon hands. In addition, real-world experiments were
conducted to compare the success rates of traditional rigid fixtures with our
proposed system. The results demonstrate that our method consistently achieves
higher success rates and superior placement stability during screw removal
tasks.

</details>


### [146] [Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts](https://arxiv.org/abs/2508.05937)
*Gen Sako,Takuya Kiyokawa,Kensuke Harada,Tomoki Ishikura,Naoya Miyaji,Genichiro Matsuda*

Main category: cs.RO

TL;DR: 本文提出了一种基于可供性引导的遥操作系统，提升双臂机械臂对配合件无损拆卸任务的灵活性和作业成功率。


<details>
  <summary>Details</summary>
Motivation: 机器人在无损拆卸配合件时面临操作灵活性和对内部结构可见性受限的挑战，亟需更加直观和高效的拆卸辅助方式。

Method: 设计了一套可供性引导的遥操作系统，通过虚拟环境中可视化的可行抓取姿势和拆卸方向，结合双臂机械臂的混合控制器（位置+阻抗控制）以辅助人类直观演示拆卸过程。

Result: 实际实验结果表明，该系统能够提升拆卸任务的完成率，并有效减少物体姿态偏差。

Conclusion: 所提系统可有效提升复杂配合件拆卸的操作性和安全性，展现了在实际装配拆卸场景中的应用潜力。

Abstract: Robotic non-destructive disassembly of mating parts remains challenging due
to the need for flexible manipulation and the limited visibility of internal
structures. This study presents an affordance-guided teleoperation system that
enables intuitive human demonstrations for dual-arm fix-and-disassemble tasks
for mating parts. The system visualizes feasible grasp poses and disassembly
directions in a virtual environment, both derived from the object's geometry,
to address occlusions and structural complexity. To prevent excessive position
tracking under load when following the affordance, we integrate a hybrid
controller that combines position and impedance control into the teleoperated
disassembly arm. Real-world experiments validate the effectiveness of the
proposed system, showing improved task success rates and reduced object pose
deviation.

</details>


### [147] [Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution](https://arxiv.org/abs/2508.05941)
*Zhanyi Sun,Shuran Song*

Main category: cs.RO

TL;DR: 本文提出Latent Policy Barrier (LPB)方法，通过借鉴控制障碍函数思想，在策略学习中利用专家演示的潜在空间作为区分安全与不安全状态的障碍，以缓解行为克隆易受协变量转移影响的问题，在无需额外人工修正或标注数据的条件下提升策略鲁棒性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 行为克隆在视觉-运动控制任务中易受到协变量转移影响，导致小偏差不断积累而失败。现有增强数据分布方法（如人工干预或合成数据增强）费力、假设多或影响模仿质量，因此需要一种更高效、低成本的鲁棒策略学习方法。

Method: LPB通过将专家演示的潜在嵌入空间用作障碍区域，将精确模仿和OOD（分布外）恢复分为两个模块：1）仅用专家数据训练的扩散策略；2）用专家和次优策略数据训练的动力学模型。推理时利用动力学模型预测和调整未来潜在状态，确保其保持在专家分布内。

Result: LPB在仿真和真实环境下实现了更强的策略鲁棒性和更高的数据效率，能够让策略在专家数据有限的情况下，可靠地完成操作任务。

Conclusion: LPB无需额外人工数据修正或标注，即可提升行为克隆策略的鲁棒性和泛化能力，是高效可靠的视觉-运动控制策略学习新框架。

Abstract: Visuomotor policies trained via behavior cloning are vulnerable to covariate
shift, where small deviations from expert trajectories can compound into
failure. Common strategies to mitigate this issue involve expanding the
training distribution through human-in-the-loop corrections or synthetic data
augmentation. However, these approaches are often labor-intensive, rely on
strong task assumptions, or compromise the quality of imitation. We introduce
Latent Policy Barrier, a framework for robust visuomotor policy learning.
Inspired by Control Barrier Functions, LPB treats the latent embeddings of
expert demonstrations as an implicit barrier separating safe, in-distribution
states from unsafe, out-of-distribution (OOD) ones. Our approach decouples the
role of precise expert imitation and OOD recovery into two separate modules: a
base diffusion policy solely on expert data, and a dynamics model trained on
both expert and suboptimal policy rollout data. At inference time, the dynamics
model predicts future latent states and optimizes them to stay within the
expert distribution. Both simulated and real-world experiments show that LPB
improves both policy robustness and data efficiency, enabling reliable
manipulation from limited expert data and without additional human correction
or annotation.

</details>


### [148] [Social and Telepresence Robots for Accessibility and Inclusion in Small Museums](https://arxiv.org/abs/2508.05946)
*Nello Balossino,Rossana Damiano,Cristina Gena,Alberto Lillo,Anna Maria Marras,Claudio Mattutino,Antonio Pizzo,Alessia Prin,Fabiana Vernero*

Main category: cs.RO

TL;DR: ROBSO-PM项目利用社交机器人和远程临场机器人，提高小型博物馆的无障碍参观体验，特别关注感知、文化和认知障碍。


<details>
  <summary>Details</summary>
Motivation: 许多小型博物馆（尤其是人口稀少地区的博物馆）由于感知、文化和认知方面的障碍，导致许多参观者（比如残障人士、外地人或行动不便者）无法轻松访问。该项目意在解决这一长期存在的问题。

Method: 项目选取拖林圣裹尸布博物馆和两家山区小型博物馆作为案例，采用社交机器人引导参观及远程临场机器人，支持远程参与和包容性参观。研究涉及讲故事、机器人个性、同理心、个性化以及机器人与远程操作者的协作分工。

Result: 项目初步表明：机器人可以有效辅助不同背景和能力的参观者，包括提供定制化讲解、同理心沟通和远程博物馆体验。

Conclusion: 社交机器人和远程临场机器人有助于提升小型博物馆的可达性和包容性，帮助解决目前存在的人群覆盖局限，为博物馆无障碍参观和文化传播提供了新途径。

Abstract: There are still many museums that present accessibility barriers,
particularly regarding perceptual, cultural, and cognitive aspects. This is
especially evident in low-density population areas. The aim of the ROBSO-PM
project is to improve the accessibility of small museums through the use of
social robots and social telepresence robots, focusing on three museums as case
studies: the Museum of the Holy Shroud in Turin, a small but globally known
institution, and two lesser known mountain museums: the Museum of the Champlas
du Col Carnival and the Pragelato Museum of Alpine Peoples' Costumes and
Traditions. The project explores two main applications for robots: as guides
supporting inclusive visits for foreign or disabled visitors, and as
telepresence tools allowing people with limited mobility to access museums
remotely. From a research perspective, key topics include storytelling, robot
personality, empathy, personalization, and, in the case of telepresence,
collaboration between the robot and the person, with clearly defined roles and
autonomy.

</details>


### [149] [Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land Bimodal Unmanned Aerial Vehicles](https://arxiv.org/abs/2508.05972)
*Shaoting Liu,Zhou Liu*

Main category: cs.RO

TL;DR: 本文提出了一种面向扰动的空气-地面双模态车辆轨迹规划框架，实现了车辆在复杂环境中的高鲁棒性自主导航。


<details>
  <summary>Details</summary>
Motivation: 空气-地面双模态车辆兼具飞行灵活性和地面节能优势，但在实际应用中会受到环境扰动影响，影响规划鲁棒性。亟需能自适应扰动且高效的规划方法。

Method: 方法包括基于实时扰动估计的路径搜索与轨迹优化，并引入扰动自适应的动态安全边界调整机制，结合车辆动力学模型，对路径可行性进行动态调整。

Result: 方法通过定制平台的实际对比实验，验证了在地面及空中环境扰动情况下，系统轨迹跟踪精度、任务效率及能耗表现均优于传统方法。

Conclusion: 所提框架能有效提升双模态车辆在多变环境下的计划鲁棒性和综合性能，具备实际应用潜力。

Abstract: Air-land bimodal vehicles provide a promising solution for navigating complex
environments by combining the flexibility of aerial locomotion with the energy
efficiency of ground mobility. To enhance the robustness of trajectory planning
under environmental disturbances, this paper presents a disturbance-aware
planning framework that incorporates real-time disturbance estimation into both
path searching and trajectory optimization. A key component of the framework is
a disturbance-adaptive safety boundary adjustment mechanism, which dynamically
modifies the vehicle's feasible dynamic boundaries based on estimated
disturbances to ensure trajectory feasibility. Leveraging the dynamics model of
the bimodal vehicle, the proposed approach achieves adaptive and reliable
motion planning across different terrains and operating conditions. A series of
real-world experiments and benchmark comparisons on a custom-built platform
validate the effectiveness and robustness of the method, demonstrating
improvements in tracking accuracy, task efficiency, and energy performance
under both ground and aerial disturbances.

</details>


### [150] [ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian Inference](https://arxiv.org/abs/2508.06053)
*Kaixuan Wu,Yuanzhuo Xu,Zejun Zhang,Weiping Zhu,Steve Drew,Xiaoguang Niu*

Main category: cs.RO

TL;DR: 提出了一个新的贝叶斯深度学习行人惯性定位框架ReNiL，能更高效、准确并具备不确定性感知，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的行人惯性定位方法需要固定窗口长度，难以适应多变的运动节奏与尺度，不确定性表达不一致，限制了实际使用。

Method: 提出ReNiL框架，使用惯性定位需求点(IPDPs)替代密集追踪，实现更具意义的关键点定位，并能自适应分析任意尺度的IMU序列。方法结合运动感知定向滤波器与任意尺度拉普拉斯估计器(ASLE)：ASLE是一个双任务网络，结合自监督与贝叶斯回归，利用拉普拉斯分布建模位移以统一不确定性，最终通过贝叶斯推理链将各IPDP关联成连续轨迹。

Result: 在RoNIN-ds和新WUDataset（涵盖28人多场景）实验中，ReNiL在位移准确性和不确定性一致性方面显著优于TLIO、CTIN、iMoT与RoNIN变种，且计算成本更低。

Conclusion: ReNiL在实际移动与物联网场景下表现出卓越的稳定性和实用性，为下一代可扩展、感知不确定性的定位系统提供坚实基础。

Abstract: Pedestrian inertial localization is key for mobile and IoT services because
it provides infrastructure-free positioning. Yet most learning-based methods
depend on fixed sliding-window integration, struggle to adapt to diverse motion
scales and cadences, and yield inconsistent uncertainty, limiting real-world
use. We present ReNiL, a Bayesian deep-learning framework for accurate,
efficient, and uncertainty-aware pedestrian localization. ReNiL introduces
Inertial Positioning Demand Points (IPDPs) to estimate motion at contextually
meaningful waypoints instead of dense tracking, and supports inference on IMU
sequences at any scale so cadence can match application needs. It couples a
motion-aware orientation filter with an Any-Scale Laplace Estimator (ASLE), a
dual-task network that blends patch-based self-supervision with Bayesian
regression. By modeling displacements with a Laplace distribution, ReNiL
provides homogeneous Euclidean uncertainty that integrates cleanly with other
sensors. A Bayesian inference chain links successive IPDPs into consistent
trajectories. On RoNIN-ds and a new WUDataset covering indoor and outdoor
motion from 28 participants, ReNiL achieves state-of-the-art displacement
accuracy and uncertainty consistency, outperforming TLIO, CTIN, iMoT, and RoNIN
variants while reducing computation. Application studies further show
robustness and practicality for mobile and IoT localization, making ReNiL a
scalable, uncertainty-aware foundation for next-generation positioning.

</details>


### [151] [Incremental Language Understanding for Online Motion Planning of Robot Manipulators](https://arxiv.org/abs/2508.06095)
*Mitchell Abrams,Thies Oelerich,Christian Hartl-Nesic,Andreas Kugi,Matthias Scheutz*

Main category: cs.RO

TL;DR: 本文提出了一种基于推理的增量解析器，能够让机器人在接收动态语言输入时灵活适应并调整动作计划，实现更加自然的人机协作。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人受语言引导动作规划方法常假设指令已完全指定，遇到修正或澄清时需停止并重新规划，效率低下。本研究旨在解决机器人在不完全或动态语言指令下，如何实时、平滑地适应人类指令变更。

Method: 研究者设计了一套将在线动作规划算法与认知架构相结合的增量式语言解析器。该解析器支持持续接收并处理语言输入，维护多个候选解析，通过推理机制消除歧义与修正理解，并即时将解析结果反馈给动作规划系统，无需完全重启动作执行。

Result: 系统在实际人机交互场景中进行了评估，能实时应对目标位置、约束或任务目标的语言变动，动态修正动作规划，提高了人机协作的流畅性与自然性。

Conclusion: 将增量式语言理解与实时动作规划深度集成，有助于提升机器人对复杂、人类式交互的适应能力，是实现自然、高效人机协作的重要一步。

Abstract: Human-robot interaction requires robots to process language incrementally,
adapting their actions in real-time based on evolving speech input. Existing
approaches to language-guided robot motion planning typically assume fully
specified instructions, resulting in inefficient stop-and-replan behavior when
corrections or clarifications occur. In this paper, we introduce a novel
reasoning-based incremental parser which integrates an online motion planning
algorithm within the cognitive architecture. Our approach enables continuous
adaptation to dynamic linguistic input, allowing robots to update motion plans
without restarting execution. The incremental parser maintains multiple
candidate parses, leveraging reasoning mechanisms to resolve ambiguities and
revise interpretations when needed. By combining symbolic reasoning with online
motion planning, our system achieves greater flexibility in handling speech
corrections and dynamically changing constraints. We evaluate our framework in
real-world human-robot interaction scenarios, demonstrating online adaptions of
goal poses, constraints, or task objectives. Our results highlight the
advantages of integrating incremental language understanding with real-time
motion planning for natural and fluid human-robot collaboration. The
experiments are demonstrated in the accompanying video at
www.acin.tuwien.ac.at/42d5.

</details>


### [152] [Bounding Distributional Shifts in World Modeling through Novelty Detection](https://arxiv.org/abs/2508.06096)
*Eric Jing,Abdeslam Boularias*

Main category: cs.RO

TL;DR: 本文提出用变分自编码器(VAE)作为新颖性检测器，限制模型推理时的轨迹，使其不偏离训练分布，从而提升基于视觉的世界模型在规划算法中的稳健性。实验表明，该方法在数据效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有利用预训练图像骨干网络获得的视觉世界模型，对训练数据的覆盖度高度敏感，难以适应训练质量不足或状态空间覆盖不全的情况。一旦推理超出训练分布易导致模型发散。需解决模型对训练分布的依赖性，提高规划过程中模型的稳健性。

Method: 在模型预测控制与世界模型架构（如DINO-WM）中，嵌入VAE作为新颖性检测器。规划时，检测所生成的行动轨迹是否超出训练分布，若超出则进行约束，从而降低模型发散风险。该方案在仿真机器人环境中进行了实验验证。

Result: 实验结果显示，嵌入VAE新颖性检测的世界模型在数据效率上显著优于当前主流方法；模型在面对不完整训练覆盖时表现出更好的稳健性。

Conclusion: 通过引入VAE新颖性检测，规划算法能有效规避训练分布之外的危险轨迹，使基于视觉的世界模型具备更高的健壮性和数据利用效率。

Abstract: Recent work on visual world models shows significant promise in latent state
dynamics obtained from pre-trained image backbones. However, most of the
current approaches are sensitive to training quality, requiring near-complete
coverage of the action and state space during training to prevent divergence
during inference. To make a model-based planning algorithm more robust to the
quality of the learned world model, we propose in this work to use a
variational autoencoder as a novelty detector to ensure that proposed action
trajectories during planning do not cause the learned model to deviate from the
training data distribution. To evaluate the effectiveness of this approach, a
series of experiments in challenging simulated robot environments was carried
out, with the proposed method incorporated into a model-predictive control
policy loop extending the DINO-WM architecture. The results clearly show that
the proposed method improves over state-of-the-art solutions in terms of data
efficiency.

</details>


### [153] [Beyond Constant Parameters: Hyper Prediction Models and HyperMPC](https://arxiv.org/abs/2508.06181)
*Jan Węgrzynowski,Piotr Kicki,Grzegorz Czechmanowski,Maciej Krupka,Krzysztof Walas*

Main category: cs.RO

TL;DR: 本文针对模型预测控制（MPC）对动力学模型精度的依赖，提出了一种基于时变参数的Hyper Prediction Model（HyperPM），显著提升了预测准确性和实际表现。


<details>
  <summary>Details</summary>
Motivation: 现有用于梯度型MPC的动力学模型在计算复杂性和状态表示上存在局限，难以高效、准确地建模复杂或未建模动力学。为了解决这一问题，提升机器人的控制效果，作者提出新方法。

Method: 作者提出了Hyper Prediction Model（HyperPM），将未建模动力学投影到时变动力学模型中。具体地，动力学模型的参数随时间变化，这些时变参数的演化通过神经网络学习得到，在MPC预测区间内动态调整模型。这样既保持了基础模型的高效与鲁棒性，又引入了处理未建模动力学的能力。

Result: 在包括真实F1TENTH自动驾驶赛车等多个高难度系统上进行实验，结果显示HyperPM显著降低了长时域预测误差。将其集成到MPC框架（HyperMPC）后，方法在多项指标上均优于现有最优技术。

Conclusion: HyperPM有效提升了MPC模型对复杂和未建模动力学现象的预测和控制能力，结合高效运算和鲁棒性，为机器人控制提供了更优的方案。

Abstract: Model Predictive Control (MPC) is among the most widely adopted and reliable
methods for robot control, relying critically on an accurate dynamics model.
However, existing dynamics models used in the gradient-based MPC are limited by
computational complexity and state representation. To address this limitation,
we propose the Hyper Prediction Model (HyperPM) - a novel approach in which we
project the unmodeled dynamics onto a time-dependent dynamics model. This
time-dependency is captured through time-varying model parameters, whose
evolution over the MPC prediction horizon is learned using a neural network.
Such formulation preserves the computational efficiency and robustness of the
base model while equipping it with the capacity to anticipate previously
unmodeled phenomena. We evaluated the proposed approach on several challenging
systems, including real-world F1TENTH autonomous racing, and demonstrated that
it significantly reduces long-horizon prediction errors. Moreover, when
integrated within the MPC framework (HyperMPC), our method consistently
outperforms existing state-of-the-art techniques.

</details>


### [154] [Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model](https://arxiv.org/abs/2508.06206)
*Hanqing Wang,Shaoyang Wang,Yiming Zhong,Zemin Yang,Jiamin Wang,Zhiqing Cui,Jiahao Yuan,Yifan Han,Mingyu Liu,Yuexin Ma*

Main category: cs.RO

TL;DR: 本文提出了Affordance-R1，这是首个结合了链式思维（CoT）和群组相对策略优化（GRPO）的统一能供性定位框架，用于提升机器人对物体可操作区域的理解和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的能供性定位模型缺乏链式思维推理，忽视了不同物体间共享的能供性，导致模型在域外泛化和显式推理能力上的不足。

Method: 作者设计了一种包含格式感知和认知奖励的复杂能供性函数，并提出GRPO强化学习方法对模型进行训练；还构建了专门用于推理的ReasonAff数据集支持训练。

Result: Affordance-R1在零样本泛化和推理能力上表现出色，并在与主流方法比较中取得了更优的性能，展示了良好的开放世界泛化能力。

Conclusion: Affordance-R1首次将GRPO强化学习与推理能力结合，引领能供性定位的新方向，提升机器人跨场景任务的适应性。

Abstract: Affordance grounding focuses on predicting the specific regions of objects
that are associated with the actions to be performed by robots. It plays a
vital role in the fields of human-robot interaction, human-object interaction,
embodied manipulation, and embodied perception. Existing models often neglect
the affordance shared among different objects because they lack the
Chain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD)
generalization and explicit reasoning capabilities. To address these
challenges, we propose Affordance-R1, the first unified affordance grounding
framework that integrates cognitive CoT guided Group Relative Policy
Optimization (GRPO) within a reinforcement learning paradigm. Specifically, we
designed a sophisticated affordance function, which contains format,
perception, and cognition rewards to effectively guide optimization directions.
Furthermore, we constructed a high-quality affordance-centric reasoning
dataset, ReasonAff, to support training. Trained exclusively via reinforcement
learning with GRPO and without explicit reasoning data, Affordance-R1 achieves
robust zero-shot generalization and exhibits emergent test-time reasoning
capabilities. Comprehensive experiments demonstrate that our model outperforms
well-established methods and exhibits open-world generalization. To the best of
our knowledge, Affordance-R1 is the first to integrate GRPO-based RL with
reasoning into affordance reasoning. The code of our method and our dataset is
released on https://github.com/hq-King/Affordance-R1.

</details>


### [155] [Computer Vision-based Adaptive Control for Back Exoskeleton Performance Optimization](https://arxiv.org/abs/2508.06207)
*Andrea Dal Prete,Seyram Ofori,Chan Yon Sin,Ashwin Narayan,Francesco Braghin,Marta Gandolla,Haoyong Yu*

Main category: cs.RO

TL;DR: 本研究提出了一种基于视觉的自适应控制框架，用于背部外骨骼，显著降低背部肌肉负荷并提升用户舒适度和满意度。


<details>
  <summary>Details</summary>
Motivation: 背部外骨骼可减少肌肉骨骼压力，但如何实现动态、智能的支撑调节以优化效果仍存在挑战。研究动机在于：1）制定最优支撑策略，2）实现基于负载估计的自适应控制，从而提升工业外骨骼的实用性与用户体验。

Method: 提出了以降低肌肉活动、减轻不适感和结合用户偏好的最优化空间，实验分析12名受试者以找出最优工作区间，随后基于对外部负载的实时视觉感知，开发出自适应控制管线，实现动态支撑调节，使外骨骼能随负载变化动态助力。

Result: 再次招募12名新受试者进行验证，自适应控制能以超过80%的准确率估计负载，对比静态控制，峰值背部肌肉激活度下降最高达23%，用户偏好和舒适度获得同步提升。

Conclusion: 研究验证了所提出的智能、自适应控制框架可大大增强外骨骼辅助效果，兼顾效率与用户体验，为未来工业智能外骨骼的应用提供了有力支持。

Abstract: Back exoskeletons can reduce musculoskeletal strain, but their effectiveness
depends on support modulation and adaptive control. This study addresses two
challenges: defining optimal support strategies and developing adaptive control
based on payload estimation. We introduce an optimization space based on muscle
activity reduction, perceived discomfort, and user preference, constructing
functions to identify optimal strategies. Experiments with 12 subjects revealed
optimal operating regions, highlighting the need for dynamic modulation. Based
on these insights, we developed a vision-based adaptive control pipeline that
estimates payloads in real-time by enhancing exoskeleton contextual
understanding, minimising latency and enabling support adaptation within the
defined optimisation space. Validation with 12 more subjects showed over 80%
accuracy and improvements across all metrics. Compared to static control,
adaptive modulation reduced peak back muscle activation by up to 23% while
preserving user preference and minimising discomfort. These findings validate
the proposed framework and highlight the potential of intelligent,
context-aware control in industrial exoskeletons.

</details>


### [156] [REBot: Reflexive Evasion Robot for Instantaneous Dynamic Obstacle Avoidance](https://arxiv.org/abs/2508.06229)
*Zihao Xu,Ce Hao,Chunzheng Wang,Kuankuan Sima,Fan Shi,Jin Song Dong*

Main category: cs.RO

TL;DR: 本文提出了一种名为REBot的控制框架，使四足机器人能够实现实时反射式动态障碍物规避，并通过模拟和实际实验验证了其在成功率、能效和鲁棒性上的提升。


<details>
  <summary>Details</summary>
Motivation: 随着四足机器人在复杂和有人环境中的应用增加，现有基于重规划的动态障碍物规避方法反应不够迅速，无法应对高速接近的障碍物。为提升机器人的即时规避能力，亟需开发能即时做出反应的低延迟规避机制。

Method: 本文提出了REBot控制框架，将规避策略和恢复策略整合在有限状态机内。通过精心设计的学习过程、正则化和自适应奖励机制，训练机器人具备反射式规避和快速恢复的能力。

Result: 通过大量仿真和现实实验，REBot在面对快速移动障碍物时展现出更高的规避成功率、更好的能效以及更强的鲁棒性。

Conclusion: REBot为四足机器人提供了高效、可靠的动态反射式障碍规避能力，适用于有高安全和反应速度需求的复杂动态环境。

Abstract: Dynamic obstacle avoidance (DOA) is critical for quadrupedal robots operating
in environments with moving obstacles or humans. Existing approaches typically
rely on navigation-based trajectory replanning, which assumes sufficient
reaction time and leading to fails when obstacles approach rapidly. In such
scenarios, quadrupedal robots require reflexive evasion capabilities to perform
instantaneous, low-latency maneuvers. This paper introduces Reflexive Evasion
Robot (REBot), a control framework that enables quadrupedal robots to achieve
real-time reflexive obstacle avoidance. REBot integrates an avoidance policy
and a recovery policy within a finite-state machine. With carefully designed
learning curricula and by incorporating regularization and adaptive rewards,
REBot achieves robust evasion and rapid stabilization in instantaneous DOA
tasks. We validate REBot through extensive simulations and real-world
experiments, demonstrating notable improvements in avoidance success rates,
energy efficiency, and robustness to fast-moving obstacles. Videos and appendix
are available on https://rebot-2025.github.io/.

</details>


### [157] [ADPro: a Test-time Adaptive Diffusion Policy for Robot Manipulation via Manifold and Initial Noise Constraints](https://arxiv.org/abs/2508.06266)
*Zezeng Li,Rui Yang,Ruochen Chen,ZhongXuan Luo,Liming Chen*

Main category: cs.RO

TL;DR: 本文提出了一种新的扩散策略自适应方法（ADP），通过引入几何约束和结构化初始化提升现有机器人操作的扩散策略在新任务和环境下的泛化能力和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散策略方法虽然表现优异，但在动作生成时通常未能利用任务相关的几何和控制先验知识，导致泛化能力受限、探索效率较低。作者试图通过引入结构知识，提升策略对新任务的适应能力与执行效率。

Method: 提出自适应扩散策略（ADP），主要特点：（1）在去噪过程引入几何流形约束，使去噪方向对齐于任务相关的子空间；（2）动作初始化采用解析引导，通过粗略配准末端执行器和目标物体场景，生成结构化噪声动作。ADP可用于已有的预训练扩散策略，无需重新训练，实现测试时自适应。

Result: 在RLBench、CALVIN和真实机器人数据集上实验，ADP的实现（ADPro）在成功率、泛化能力和采样效率上均优于传统扩散基线，达到了最快25%的执行加速和高出9个百分点的成功率提升。

Conclusion: ADP通过测试时引入几何和结构先验，实现了显著提升扩散策略在机器人操作任务中的泛化与效率，且兼容现有预训练策略，具有良好的理论价值和实际应用前景。

Abstract: Diffusion policies have recently emerged as a powerful class of visuomotor
controllers for robot manipulation, offering stable training and expressive
multi-modal action modeling. However, existing approaches typically treat
action generation as an unconstrained denoising process, ignoring valuable a
priori knowledge about geometry and control structure. In this work, we propose
the Adaptive Diffusion Policy (ADP), a test-time adaptation method that
introduces two key inductive biases into the diffusion. First, we embed a
geometric manifold constraint that aligns denoising updates with task-relevant
subspaces, leveraging the fact that the relative pose between the end-effector
and target scene provides a natural gradient direction, and guiding denoising
along the geodesic path of the manipulation manifold. Then, to reduce
unnecessary exploration and accelerate convergence, we propose an analytically
guided initialization: rather than sampling from an uninformative prior, we
compute a rough registration between the gripper and target scenes to propose a
structured initial noisy action. ADP is compatible with pre-trained diffusion
policies and requires no retraining, enabling test-time adaptation that tailors
the policy to specific tasks, thereby enhancing generalization across novel
tasks and environments. Experiments on RLBench, CALVIN, and real-world dataset
show that ADPro, an implementation of ADP, improves success rates,
generalization, and sampling efficiency, achieving up to 25% faster execution
and 9% points over strong diffusion baselines.

</details>


### [158] [EcBot: Data-Driven Energy Consumption Open-Source MATLAB Library for Manipulators](https://arxiv.org/abs/2508.06276)
*Juan Heredia,Christian Schlette,Mikkel Baun Kjærgaard*

Main category: cs.RO

TL;DR: 本文提出了一个开源的基于Matlab的库，可自动生成机械臂的电功率估算模型，并通过多种轻量型机器人验证，取得较高的精度。


<details>
  <summary>Details</summary>
Motivation: 现有机械臂电功率估算模型主要基于传统工业机器人，且精度有限，不能满足多样化和精确化的应用需求。

Method: 作者开发了一个Matlab库，用户只需输入Denavit-Hartenberg参数、连杆质量、质心等信息，以及实际操作时的关节位置、速度、加速度、电功率及时间戳等数据，库会自动生成针对具体机械臂的电功率估算模型。该模型采用数据驱动方法，需要真实操作数据进行拟合和训练。

Result: 在Universal Robots、Franka Emika和Kinova等三家厂商的四款轻量型机器人上进行测试，所得模型在训练集上的RMSE为1.42W-2.80W，在测试集上为1.45W-5.25W，显示出良好的估算精度。

Conclusion: 该方法提升了机械臂电功率估算模型的适用性和精度，并通过开源工具降低了模型生成门槛，可广泛应用于多种机器人平台。

Abstract: Existing literature proposes models for estimating the electrical power of
manipulators, yet two primary limitations prevail. First, most models are
predominantly tested using traditional industrial robots. Second, these models
often lack accuracy. To address these issues, we introduce an open source
Matlab-based library designed to automatically generate \ac{ec} models for
manipulators. The necessary inputs for the library are Denavit-Hartenberg
parameters, link masses, and centers of mass. Additionally, our model is
data-driven and requires real operational data, including joint positions,
velocities, accelerations, electrical power, and corresponding timestamps. We
validated our methodology by testing on four lightweight robots sourced from
three distinct manufacturers: Universal Robots, Franka Emika, and Kinova. The
model underwent testing, and the results demonstrated an RMSE ranging from 1.42
W to 2.80 W for the training dataset and from 1.45 W to 5.25 W for the testing
dataset.

</details>


### [159] [Mitigating Undesired Conditions in Flexible Production with Product-Process-Resource Asset Knowledge Graphs](https://arxiv.org/abs/2508.06278)
*Petr Novak,Stefan Biffl,Marek Obitko,Petr Kadera*

Main category: cs.RO

TL;DR: 本文提出了一种新型面向工业的语义模型——产品-过程-资源资产知识图谱（PPR-AKG），用于分析和缓解智能制造系统中的异常和错误。该模型结合语义技术与大语言模型，能用自然语言直观交互，有效提升生产资源分配及异常处理能力。


<details>
  <summary>Details</summary>
Motivation: 灵活的工业4.0环境给基于机器人工作单元的生产系统带来异常情况分析的难题，现有质量保证机制难以满足智能制造的需求，尤其是在异常和错误处理能力方面存在不足。

Method: 基于ISA-95和VDI-3682标准的PPR模型，本文开发了一个全面的OWL本体扩展，实现了异常和错误状态精细表征。将语义技术与大语言模型集成，使操作员、计划人员和工程师可通过自然语言与系统交互。

Result: 通过电动汽车电池再制造案例，验证了PPR-AKG在基于能力的资源分配、生产异常识别与缓解等方面的高效性。

Conclusion: PPR-AKG为智能制造提供了多维度生产知识的全面建模方式，并与LLM结合提升了人机交互体验，为灵活生产环境下的异常管理和资源分配带来了显著提升。

Abstract: Contemporary industrial cyber-physical production systems (CPPS) composed of
robotic workcells face significant challenges in the analysis of undesired
conditions due to the flexibility of Industry 4.0 that disrupts traditional
quality assurance mechanisms. This paper presents a novel industry-oriented
semantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG),
which is designed to analyze and mitigate undesired conditions in flexible
CPPS. Built on top of the well-proven Product-Process-Resource (PPR) model
originating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses
shortcomings of conventional model-driven engineering for CPPS, particularly
inadequate undesired condition and error handling representation. The
integration of semantic technologies with large language models (LLMs) provides
intuitive interfaces for factory operators, production planners, and engineers
to interact with the entire model using natural language. Evaluation with the
use case addressing electric vehicle battery remanufacturing demonstrates that
the PPR-AKG approach efficiently supports resource allocation based on
explicitly represented capabilities as well as identification and mitigation of
undesired conditions in production. The key contributions include (1) a
holistic PPR-AKG model capturing multi-dimensional production knowledge, and
(2) the useful combination of the PPR-AKG with LLM-based chatbots for human
interaction.

</details>


### [160] [Situationally-aware Path Planning Exploiting 3D Scene Graphs](https://arxiv.org/abs/2508.06283)
*Saad Ejaz,Marco Giberna,Muhammad Shaheer,Jose Andres Millan-Romera,Ali Tourani,Paul Kremer,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: 本文提出S-Path方法，通过结合室内3D场景图的语义和度量信息，实现高效且可解释的路径规划，在实验中规划效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管3D场景图融合了空间度量和语义信息，但其结构在提升路径规划效率和可解释性方面尚未被充分利用。本文旨在开发一种方法，有效利用场景图的语义结构来提升路径规划性能。

Method: S-Path采用两阶段流程：首先在基于场景图抽取的语义图上进行全局高层路径搜索，获得人类可理解的高阶路径并锁定目标区域；其次将问题分解为可并行处理的小子问题，从而提高计算效率。若发现路径不可行，则启用重新规划机制，复用已有子问题的信息以优化后续搜索的语义启发式策略。

Result: 在多个真实和模拟环境的实验中，S-Path在路径质量相当的前提下，平均将规划时间缩短了5.7倍，尤其在复杂场景中表现优于经典采样方法。

Conclusion: S-Path显著提升了利用室内3D场景图进行路径规划的效率和可解释性，是复杂环境下高效的路径规划工具。

Abstract: 3D Scene Graphs integrate both metric and semantic information, yet their
structure remains underutilized for improving path planning efficiency and
interpretability. In this work, we present S-Path, a situationally-aware path
planner that leverages the metric-semantic structure of indoor 3D Scene Graphs
to significantly enhance planning efficiency. S-Path follows a two-stage
process: it first performs a search over a semantic graph derived from the
scene graph to yield a human-understandable high-level path. This also
identifies relevant regions for planning, which later allows the decomposition
of the problem into smaller, independent subproblems that can be solved in
parallel. We also introduce a replanning mechanism that, in the event of an
infeasible path, reuses information from previously solved subproblems to
update semantic heuristics and prioritize reuse to further improve the
efficiency of future planning attempts. Extensive experiments on both
real-world and simulated environments show that S-Path achieves average
reductions of 5.7x in planning time while maintaining comparable path
optimality to classical sampling-based planners and surpassing them in complex
scenarios, making it an efficient and interpretable path planner for
environments represented by indoor 3D Scene Graphs.

</details>


### [161] [Real-Time 3D Vision-Language Embedding Mapping](https://arxiv.org/abs/2508.06291)
*Christian Rauch,Björn Ellensohn,Linus Nwankwo,Vedant Dave,Elmar Rueckert*

Main category: cs.RO

TL;DR: 该论文提出了一种简洁高效的方法，将视觉语言模型生成的2D嵌入在实时环境下集成到高精度3D语义表示中，并通过多种策略实现了更准确的目标定位和更好的实时性。


<details>
  <summary>Details</summary>
Motivation: 许多机器人任务依赖于精确的3D语义表征，但现有方法在嵌入区分性、可靠性和实时性等方面存在不足。因此，提升3D语义表示的准确性与泛化能力，同时满足实时性要求，是一项重要挑战。

Method: 作者提出将视觉语言模型的2D嵌入通过局部嵌入掩码和置信度加权策略整合到3D语义建图过程，实现更加区分和可靠的嵌入表达。该方法可扩展到多房间和物体级别语义场景，且实现了任务无关、原始图像即用。

Result: 在多组真实世界场景中实验表明，该方法在目标定位准确度和运行效率上均有提升，能够更好地满足实时需求。

Conclusion: 所提方法不仅提升了3D语义表征的区分性和可靠性，还兼具实时性和通用性，适用于多种交互机器人场景，包括便携、移动和操作型机器人任务。

Abstract: A metric-accurate semantic 3D representation is essential for many robotic
tasks. This work proposes a simple, yet powerful, way to integrate the 2D
embeddings of a Vision-Language Model in a metric-accurate 3D representation at
real-time. We combine a local embedding masking strategy, for a more distinct
embedding distribution, with a confidence-weighted 3D integration for more
reliable 3D embeddings. The resulting metric-accurate embedding representation
is task-agnostic and can represent semantic concepts on a global multi-room, as
well as on a local object-level. This enables a variety of interactive robotic
applications that require the localisation of objects-of-interest via natural
language. We evaluate our approach on a variety of real-world sequences and
demonstrate that these strategies achieve a more accurate object-of-interest
localisation while improving the runtime performance in order to meet our
real-time constraints. We further demonstrate the versatility of our approach
in a variety of interactive handheld, mobile robotics and manipulation tasks,
requiring only raw image data.

</details>


### [162] [Evaluating Robot Program Performance with Power Consumption Driven Metrics in Lightweight Industrial Robots](https://arxiv.org/abs/2508.06295)
*Juan Heredia,Emil Stubbe Kolvig-Raun,Sune Lundo Sorensen,Mikkel Baun Kjaergaard*

Main category: cs.RO

TL;DR: 本文提出了一种以机器人能耗为核心的新方法，通过分析工业机器人执行任务时的电力使用，衡量和优化机器人程序性能。


<details>
  <summary>Details</summary>
Motivation: 传统用CPU指标分析机器人代码性能，忽略代码对机器人物理行为的实际影响。工业界亟需一种能反映任务执行物理表现的有效评价体系。

Method: 作者开发了一套标准化指标，包括能量利用系数、能量转化指标和可靠性系数，并结合机器人磨损度，评估程序对能源的高效和可靠使用。在一个实际机床上下料案例中，用不同程序对UR5e机器人进行测试对比。

Result: 新指标能够跨任务直接对比不同策略下的机器人程序优势劣势，揭示各方案的能耗效率与可靠性，为优化机器人编程带来定量参考。

Conclusion: 基于能耗表现的方法增强了机器人的能效与可靠性评估，为可持续制造和成本降低目标提供了工具；该框架可指导更好地编写和选择机器人程序，助力工业升级。

Abstract: The code performance of industrial robots is typically analyzed through CPU
metrics, which overlook the physical impact of code on robot behavior. This
study introduces a novel framework for assessing robot program performance from
an embodiment perspective by analyzing the robot's electrical power profile.
Our approach diverges from conventional CPU based evaluations and instead
leverages a suite of normalized metrics, namely, the energy utilization
coefficient, the energy conversion metric, and the reliability coefficient, to
capture how efficiently and reliably energy is used during task execution.
Complementing these metrics, the established robot wear metric provides further
insight into long term reliability. Our approach is demonstrated through an
experimental case study in machine tending, comparing four programs with
diverse strategies using a UR5e robot. The proposed metrics directly compare
and categorize different robot programs, regardless of the specific task, by
linking code performance to its physical manifestation through power
consumption patterns. Our results reveal the strengths and weaknesses of each
strategy, offering actionable insights for optimizing robot programming
practices. Enhancing energy efficiency and reliability through this embodiment
centric approach not only improves individual robot performance but also
supports broader industrial objectives such as sustainable manufacturing and
cost reduction.

</details>


### [163] [Surrogate-Enhanced Modeling and Adaptive Modular Control of All-Electric Heavy-Duty Robotic Manipulators](https://arxiv.org/abs/2508.06313)
*Amir Hossein Barjini,Mohammad Bahari,Mahdi Hejrati,Jouni Mattila*

Main category: cs.RO

TL;DR: 本文提出了一种针对全电重型机器人操作臂的新型统一建模与控制框架，通过集成数据驱动的致动器代理模型与模块化自适应控制，实现高精度实时控制并经仿真与实验证实有效性。


<details>
  <summary>Details</summary>
Motivation: 随着对重型机器人自动化和精确控制需求的增加，传统液压系统存在体积大、效率低等问题，亟需全电化高性能控制系统以适应移动作业设备的发展。

Method: 本文将集成电机-机械动力学与神经网络代理模型的致动器代理嵌入到基于虚拟分解控制（VDC）的层次化控制框架中，并提出自然自适应律，实现高阶任务目标到执行层指令的无缝映射。同时结合Lyapunov方法保证系统稳定性。通过多域仿真和实际1自由度平台测试验证策略有效性。

Result: 在立方体和自定义平面三角形轨迹跟踪仿真中，提出的自适应模块化控制器可实现亚厘米级定位精度。实物1自由度测试平台在模拟载荷下实验进一步确认了控制策略的有效性。

Conclusion: 结果表明，融合神经网络代理模型的模块化自适应控制方法可实现全电重型机器人操作臂的高效、实时、高精度控制，此方案具备应用于新一代移动重型工作设备的关键潜力。

Abstract: This paper presents a unified system-level modeling and control framework for
an all-electric heavy-duty robotic manipulator (HDRM) driven by
electromechanical linear actuators (EMLAs). A surrogate-enhanced actuator
model, combining integrated electromechanical dynamics with a neural network
trained on a dedicated testbed, is integrated into an extended virtual
decomposition control (VDC) architecture augmented by a natural adaptation law.
The derived analytical HDRM model supports a hierarchical control structure
that seamlessly maps high-level force and velocity objectives to real-time
actuator commands, accompanied by a Lyapunov-based stability proof. In
multi-domain simulations of both cubic and a custom planar triangular
trajectory, the proposed adaptive modular controller achieves sub-centimeter
Cartesian tracking accuracy. Experimental validation of the same 1-DoF platform
under realistic load emulation confirms the efficacy of the proposed control
strategy. These findings demonstrate that a surrogate-enhanced EMLA model
embedded in the VDC approach can enable modular, real-time control of an
all-electric HDRM, supporting its deployment in next-generation mobile working
machines.

</details>


### [164] [Towards Balanced Behavior Cloning from Imbalanced Datasets](https://arxiv.org/abs/2508.06319)
*Sagar Parekh,Heramb Nemlekar,Dylan P. Losey*

Main category: cs.RO

TL;DR: 本论文分析了机器人从人类示范中学习复杂行为时，由于示范数据不均衡导致的模仿学习问题，并提出了自动重加权方法进行数据重平衡。实验显示，合理的数据重平衡能显著提升机器人模仿学习的策略表现。


<details>
  <summary>Details</summary>
Motivation: 现有机器人模仿学习方法假定所有人类示范数据同等重要，但现实中人类演示往往数据分布不均（某些子任务被示范得多，某些被忽略）。这种数据与任务实际重要性不匹配的问题会影响最终学习效果，因此有必要系统分析和解决数据不均衡带来的偏差。

Method: 作者首先理论证明了若对每个状态-动作对等权重处理，不均衡的数据会导致偏颇的策略。接着探索无需人工干预、可自动调整不同状态-动作对权重的数据重加权算法，并提出了一种新的元梯度重平衡方法以克服现有方法的局限性。

Result: 实验表明，对离线示范数据进行合理的重加权，即使不收集新数据，也能明显改进模仿学习算法在多任务和复杂行为上的策略表现。同时，不同数据重平衡方法具有各自优缺点，适用场景有所不同。

Conclusion: 离线数据集重平衡是提升机器人模仿学习效果的重要手段。论文提出的新元梯度重平衡算法针对现有技术存在的主要问题，并获得了更好的结果。未来相关研究可以根据具体任务，选用适宜的重加权手段，以改善复杂行为学习效果。

Abstract: Robots should be able to learn complex behaviors from human demonstrations.
In practice, these human-provided datasets are inevitably imbalanced: i.e., the
human demonstrates some subtasks more frequently than others. State-of-the-art
methods default to treating each element of the human's dataset as equally
important. So if -- for instance -- the majority of the human's data focuses on
reaching a goal, and only a few state-action pairs move to avoid an obstacle,
the learning algorithm will place greater emphasis on goal reaching. More
generally, misalignment between the relative amounts of data and the importance
of that data causes fundamental problems for imitation learning approaches. In
this paper we analyze and develop learning methods that automatically account
for mixed datasets. We formally prove that imbalanced data leads to imbalanced
policies when each state-action pair is weighted equally; these policies
emulate the most represented behaviors, and not the human's complex, multi-task
demonstrations. We next explore algorithms that rebalance offline datasets
(i.e., reweight the importance of different state-action pairs) without human
oversight. Reweighting the dataset can enhance the overall policy performance.
However, there is no free lunch: each method for autonomously rebalancing
brings its own pros and cons. We formulate these advantages and disadvantages,
helping other researchers identify when each type of approach is most
appropriate. We conclude by introducing a novel meta-gradient rebalancing
algorithm that addresses the primary limitations behind existing approaches.
Our experiments show that dataset rebalancing leads to better downstream
learning, improving the performance of general imitation learning algorithms
without requiring additional data collection. See our project website:
https://collab.me.vt.edu/data_curation/.

</details>


### [165] [L2Calib: $SE(3)$-Manifold Reinforcement Learning for Robust Extrinsic Calibration with Degenerate Motion Resilience](https://arxiv.org/abs/2508.06330)
*Baorun Li,Chengrui Zhu,Siyi Du,Bingran Chen,Jie Ren,Wenfei Wang,Yong Liu,Jiajun Lv*

Main category: cs.RO

TL;DR: 本文提出一种基于强化学习的多传感器外参标定方法，无需结构化目标和高激励数据，有效提升多平台机器人数据融合的实用性与准确率。


<details>
  <summary>Details</summary>
Motivation: 现有多传感器外参标定方法往往依赖结构化目标或充分激励数据，限制了其在真实场景中的应用。在线标定对激励要求高，弱激励情况下结果不可靠，因此亟须一种通用、无需额外标定物的高效自动标定方法。

Method: 将多传感器外参标定建模为决策问题，应用强化学习方法直接优化$SE(3)$外参以提升里程计精度。采用概率型Bingham分布建模三维旋转，利用轨迹对齐奖励机制评估无目标下标定效果，并引入自动数据筛选模块，过滤无信息样本以提升效率。

Result: 在UAV、UGV及手持平台上进行大量实验，结果显示本方法较传统优化方法有更高精度，并在弱激励条件下表现优越。无需高质量初始外参，仅凭平台日常数据即可完成高精度标定。

Conclusion: 论文提出的RL标定框架显著提高了多平台、多传感器系统的可部署性和精度，简化标定流程并扩展了实际应用场景。

Abstract: Extrinsic calibration is essential for multi-sensor fusion, existing methods
rely on structured targets or fully-excited data, limiting real-world
applicability. Online calibration further suffers from weak excitation, leading
to unreliable estimates. To address these limitations, we propose a
reinforcement learning (RL)-based extrinsic calibration framework that
formulates extrinsic calibration as a decision-making problem, directly
optimizes $SE(3)$ extrinsics to enhance odometry accuracy. Our approach
leverages a probabilistic Bingham distribution to model 3D rotations, ensuring
stable optimization while inherently retaining quaternion symmetry. A
trajectory alignment reward mechanism enables robust calibration without
structured targets by quantitatively evaluating estimated tightly-coupled
trajectory against a reference trajectory. Additionally, an automated data
selection module filters uninformative samples, significantly improving
efficiency and scalability for large-scale datasets. Extensive experiments on
UAVs, UGVs, and handheld platforms demonstrate that our method outperforms
traditional optimization-based approaches, achieving high-precision calibration
even under weak excitation conditions. Our framework simplifies deployment on
diverse robotic platforms by eliminating the need for high-quality initial
extrinsics and enabling calibration from routine operating data. The code is
available at https://github.com/APRIL-ZJU/learn-to-calibrate.

</details>


### [166] [V*: An Efficient Motion Planning Algorithm for Autonomous Vehicles](https://arxiv.org/abs/2508.06404)
*Abdullah Zareh Andaryan,Michael G. H. Bell,Mohsen Ramezani,Glenn Geers*

Main category: cs.RO

TL;DR: 提出了V*，一种面向自动驾驶车辆的动态图生成运动规划器，通过将速度与方向作为状态变量，有效生成动态可行的最优避碰轨迹，并在复杂场景下验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法常将路径搜索和动力学约束分开处理，难以在动态环境中高效生成时间最优且符合汽车动力学的轨迹。实际自动驾驶车辆在结构化环境导航时需要同时满足避障、速度和运动学约束。

Method: V*将速度和方向纳入离散化状态空间，通过动态图扩展生成搜索图，并采用六边形离散化以降低高维搜索复杂度。对运动学自行车模型中的瞬时转向动力学进行了指数收敛建模，理论推导了收敛速率关系，同时通过几何剪枝排除不可行运动。

Result: V*在仿真环境中展现出良好性能，能在拥挤和动态环境下制定安全、有效且可实现的避碰轨迹，具备主动让行、动态协调和等待等能力。与传统依赖后处理平滑的方法相比，无需额外优化即可保证动力学可行性。

Conclusion: V*实现了动力学与轨迹搜索一体化的高效自动驾驶路径规划，在动态复杂环境下可生成真正可执行的优质轨迹，为实际自动驾驶规划提供理论与方法支撑。

Abstract: Autonomous vehicle navigation in structured environments requires planners
capable of generating time-optimal, collision-free trajectories that satisfy
dynamic and kinematic constraints. We introduce V*, a graph-based motion
planner that represents speed and direction as explicit state variables within
a discretised space-time-velocity lattice. Unlike traditional methods that
decouple spatial search from dynamic feasibility or rely on post-hoc smoothing,
V* integrates both motion dimensions directly into graph construction through
dynamic graph generation during search expansion. To manage the complexity of
high-dimensional search, we employ a hexagonal discretisation strategy and
provide formal mathematical proofs establishing optimal waypoint spacing and
minimal node redundancy under constrained heading transitions for
velocity-aware motion planning. We develop a mathematical formulation for
transient steering dynamics in the kinematic bicycle model, modelling steering
angle convergence with exponential behaviour, and deriving the relationship for
convergence rate parameters. This theoretical foundation, combined with
geometric pruning strategies that eliminate expansions leading to infeasible
steering configurations, enables V* to evaluate dynamically admissible
manoeuvres, ensuring each trajectory is physically realisable without further
refinement. We further demonstrate V*'s performance in simulation studies with
cluttered and dynamic environments involving moving obstacles, showing its
ability to avoid conflicts, yield proactively, and generate safe, efficient
trajectories with temporal reasoning capabilities for waiting behaviours and
dynamic coordination.

</details>


### [167] [Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation](https://arxiv.org/abs/2508.06426)
*Youguang Xing,Xu Luo,Junlin Xie,Lianli Gao,Hengtao Shen,Jingkuan Song*

Main category: cs.RO

TL;DR: 本文研究了通用型机器人策略在大规模数据集上的泛化能力受限的原因，发现“捷径学习”（依赖与任务无关的特征）是主要障碍，并分析了数据集结构如何导致该问题，同时提出了数据增强策略以改善泛化。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模数据集训练出的通用机器人策略能完成多样任务，但在训练分布外的泛化表现有限。作者希望深入理解和破解限制其泛化能力的根本原因。

Method: 作者首先从理论和实证两个角度分析了大规模多子数据集（如OXE）中捷径学习的成因，包括子集多样性不足和子集间分布差异大。随后，提出针对离线数据集的数据增强策略以减轻捷径学习问题，并在仿真与真实环境中对策略效果进行验证。

Result: 分析发现，大规模数据集本身的结构（子集多样性有限和碎片化）导致策略模型更容易形成捷径学习。采用精心设计的数据增强策略后，基于现有数据集训练的机器人策略在泛化任务上的表现得到明显提升。

Conclusion: 理解并改进大规模数据集结构可减少捷径学习，从而提升通用机器人策略的泛化能力。在无法采集新数据时，合理的数据增强也能有效缓解捷径学习，提升现有模型的泛化性能。

Abstract: Generalist robot policies trained on large-scale datasets such as Open
X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.
However, they often struggle to generalize beyond the distribution of their
training data. In this paper, we investigate the underlying cause of this
limited generalization capability. We identify shortcut learning -- the
reliance on task-irrelevant features -- as a key impediment to generalization.
Through comprehensive theoretical and empirical analysis, we uncover two
primary contributors to shortcut learning: (1) limited diversity within
individual sub-datasets, and (2) significant distributional disparities across
sub-datasets, leading to dataset fragmentation. These issues arise from the
inherent structure of large-scale datasets like OXE, which are typically
composed of multiple sub-datasets collected independently across varied
environments and embodiments. Our findings provide critical insights into
dataset collection strategies that can reduce shortcut learning and enhance the
generalization ability of generalist robot policies. Moreover, in scenarios
where acquiring new large-scale data is impractical, we demonstrate that
carefully selected robotic data augmentation strategies can effectively reduce
shortcut learning in existing offline datasets, thereby improving
generalization capabilities of generalist robot policies, e.g., $\pi_0$, in
both simulation and real-world environments. More information at
https://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.

</details>
