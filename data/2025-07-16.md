<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 68]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.RO](#cs.RO) [Total: 34]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.10689)
*Tongshun Zhang,Pingping Liu,Yubing Lu,Mengen Cai,Zijian Zhang,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一种新型低光照图像增强方法CWNet，通过引入因果推理和小波变换，有效提升了增强结果，在多项数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有低光照图像增强通常只做统一亮度调整，忽视了实例级的语义信息和特征的内在属性，导致增强效果不理想。作者希望通过结合因果推理和频率分析，提升增强质量和泛化能力。

Method: CWNet包含两个关键组件：（1）采用因果推理视角，结合因果干预和度量学习，分离非因果干扰因素，仅保留因果因子；在局部级别引入实例级CLIP语义损失，以保持因果因子的一致性。（2）提出基于小波变换的主干网络，优化频率信息恢复，精确增强图像细节。

Result: CWNet在多个数据集上的实验结果明显优于当前主流方法，显示出对多种场景的强鲁棒性。

Conclusion: CWNet通过融合因果推理和小波变换，为低光照图像增强提供了新思路，提升了增强效果和泛化能力，有望引领该领域进一步发展。

Abstract: Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on
uniform brightness adjustment, often neglecting instance-level semantic
information and the inherent characteristics of different features. To address
these limitations, we propose CWNet (Causal Wavelet Network), a novel
architecture that leverages wavelet transforms for causal reasoning.
Specifically, our approach comprises two key components: 1) Inspired by the
concept of intervention in causality, we adopt a causal reasoning perspective
to reveal the underlying causal relationships in low-light enhancement. From a
global perspective, we employ a metric learning strategy to ensure causal
embeddings adhere to causal principles, separating them from non-causal
confounding factors while focusing on the invariance of causal factors. At the
local level, we introduce an instance-level CLIP semantic loss to precisely
maintain causal factor consistency. 2) Based on our causal analysis, we present
a wavelet transform-based backbone network that effectively optimizes the
recovery of frequency information, ensuring precise enhancement tailored to the
specific attributes of wavelet transforms. Extensive experiments demonstrate
that CWNet significantly outperforms current state-of-the-art methods across
multiple datasets, showcasing its robust performance across diverse scenes.
Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.

</details>


### [2] [Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines](https://arxiv.org/abs/2507.10737)
*Jiayuan Chen,Thai-Hoang Pham,Yuanlong Wang,Ping Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合外部生物学知识的新型预训练框架，用于提高显微图像分析模型在新细胞系上的泛化能力，实验表明方法有效提升了新细胞系的表型筛选性能。


<details>
  <summary>Details</summary>
Motivation: 高通量筛选对药物发现和生物医学研究至关重要，但由于细胞系间存在显著异质性，传统方法难以在新出现（de novo）细胞系上表现稳定。因此，亟需设计能提升模型迁移和泛化能力的策略。

Method: 作者提出一种结合外部生物学知识的预训练框架，将蛋白互作知识图谱和单细胞转录组特征加入表型图像建模中，分别引导模型解耦扰动特异与细胞系特异表示，并提升模型在新细胞系上的泛化能力。

Result: 在RxRx数据库上，通过one-shot和few-shot微调实验，所提方法在新细胞系药物扰动表型识别任务上表现优于现有方案。

Conclusion: 该方法显著提升了顯微镜像在新细胞系表型筛选中的通用性，为真实场景下的表型驱动药物发现提供了有力工具。

Abstract: High-throughput screening techniques, such as microscopy imaging of cellular
responses to genetic and chemical perturbations, play a crucial role in drug
discovery and biomedical research. However, robust perturbation screening for
\textit{de novo} cell lines remains challenging due to the significant
morphological and biological heterogeneity across cell lines. To address this,
we propose a novel framework that integrates external biological knowledge into
existing pretraining strategies to enhance microscopy image profiling models.
Our approach explicitly disentangles perturbation-specific and cell
line-specific representations using external biological information.
Specifically, we construct a knowledge graph leveraging protein interaction
data from STRING and Hetionet databases to guide models toward
perturbation-specific features during pretraining. Additionally, we incorporate
transcriptomic features from single-cell foundation models to capture cell
line-specific representations. By learning these disentangled features, our
method improves the generalization of imaging models to \textit{de novo} cell
lines. We evaluate our framework on the RxRx database through one-shot
fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from
the RxRx19a dataset. Experimental results demonstrate that our method enhances
microscopy image profiling for \textit{de novo} cell lines, highlighting its
effectiveness in real-world phenotype-based drug discovery applications.

</details>


### [3] [Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias](https://arxiv.org/abs/2507.10755)
*Rina Khan,Catherine Stinson*

Main category: cs.CV

TL;DR: 该论文审查了面部表情识别（FER）算法在数据集中的真实性（自发/摆拍）和种族肤色偏见，发现现有数据集含有大量摆拍图像且模型对有色人种存在明显负面情感识别偏差。


<details>
  <summary>Details</summary>
Motivation: FER算法在识别自发与摆拍表情以及不同种族肤色时表现不佳，这一问题严重影响其实用性和公正性，因此有必要系统性审查现有FER数据集及算法偏见。

Method: 作者对两个主流FER数据集进行了抽样分析，提出判断图片为自发或摆拍的方法，并检查样本中不同肤色比例及模型对应表情预测差异，测试了三种基于各数据集训练的FER模型在不同肤色和种族上的表现。

Result: 发现被标为“自然环境”的数据集实际包含大量摆拍照片。同时，FER模型在识别深色肤色或非白人时更倾向于预测负面表情（如愤怒、悲伤），即使这些人实际上在微笑。

Conclusion: FER算法和数据集不仅存在真实性误差，还对少数族裔存在严重的情感识别偏见。若不解决这些问题，相关模型在现实应用中可能导致不公甚至伤害。

Abstract: Facial expression recognition (FER) algorithms classify facial expressions
into emotions such as happy, sad, or angry. An evaluative challenge facing FER
algorithms is the fall in performance when detecting spontaneous expressions
compared to posed expressions. An ethical (and evaluative) challenge facing FER
algorithms is that they tend to perform poorly for people of some races and
skin colors. These challenges are linked to the data collection practices
employed in the creation of FER datasets. In this study, we audit two
state-of-the-art FER datasets. We take random samples from each dataset and
examine whether images are spontaneous or posed. In doing so, we propose a
methodology for identifying spontaneous or posed images. We discover a
significant number of images that were posed in the datasets purporting to
consist of in-the-wild images. Since performance of FER models vary between
spontaneous and posed images, the performance of models trained on these
datasets will not represent the true performance if such models were to be
deployed in in-the-wild applications. We also observe the skin color of
individuals in the samples, and test three models trained on each of the
datasets to predict facial expressions of people from various races and skin
tones. We find that the FER models audited were more likely to predict people
labeled as not white or determined to have dark skin as showing a negative
emotion such as anger or sadness even when they were smiling. This bias makes
such models prone to perpetuate harm in real life applications.

</details>


### [4] [FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching](https://arxiv.org/abs/2507.10770)
*Ionuţ Grigore,Călin-Adrian Popa,Claudiu Leoveanu-Condrei*

Main category: cs.CV

TL;DR: 本文提出了一种新的兴趣点提取与匹配方法，无需传统的描述子，从而大幅降低了内存需求，尽管匹配精度略低。


<details>
  <summary>Details</summary>
Motivation: 在计算机视觉任务中，兴趣点的提取和匹配通常依赖于计算和使用描述子，这不仅增加了计算和存储负担，也限制了实时系统和内存敏感型应用的效率。作者希望通过消除描述子，实现更高效的兴趣点匹配。

Method: 本文提出的方法在检测兴趣点的同时，直接将匹配信息融入检测过程，无需再计算、存储、传输或匹配描述子。该方法通过设计，使兴趣点在检测阶段就被“配对”。

Result: 实验结果显示，虽然该方法的匹配准确率略低于传统方法（包括手工和深度学习方法），但在内存消耗和系统复杂性上具有显著优势。

Conclusion: 作者得出结论，所提出的方法在大幅减少内存占用的同时，能够实现无需描述子的兴趣点匹配，特别适合对存储资源敏感的定位等系统应用。

Abstract: The extraction and matching of interest points are fundamental to many
geometric computer vision tasks. Traditionally, matching is performed by
assigning descriptors to interest points and identifying correspondences based
on descriptor similarity. This work introduces a technique where interest
points are inherently associated during detection, eliminating the need for
computing, storing, transmitting, or matching descriptors. Although the
matching accuracy is marginally lower than that of conventional approaches, our
method completely eliminates the need for descriptors, leading to a drastic
reduction in memory usage for localization systems. We assess its effectiveness
by comparing it against both classical handcrafted methods and modern learned
approaches.

</details>


### [5] [A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers](https://arxiv.org/abs/2507.10775)
*Jeffrey Joan Sam,Janhavi Sathe,Nikhil Chigali,Naman Gupta,Radhey Ruparel,Yicheng Jiang,Janmajay Singh,James W. Berck,Arko Barman*

Main category: cs.CV

TL;DR: 本文介绍了一个包含约6.4万张航天器图像分割标注的数据集，并基于该数据集对YOLOv8和YOLOv11分割模型进行了微调和基准测试，显著提升了空间器自主巡检系统的发展。


<details>
  <summary>Details</summary>
Motivation: 航天器在太空中常因恶劣环境遭受损伤，目前的人工或机器人维修成本高、风险大。利用图像分割实现自动化巡检具有巨大意义，但现有公开航天器图像分割数据极其稀缺。

Method: 作者基于真实航天器模型，结合NASA TTALOS管线生成的真实及合成背景，制作了近64,000张分割标注图片，并仿真摄像头失真与噪声。随后对YOLOv8/YOLOv11进行了有针对性的微调，模拟硬件和推理时间受限的真实场景下进行基准测试。

Result: 经过微调的模型在数据集上测试，获得了Dice评分0.92、Hausdorff距离0.69、推理时间约0.5秒，表明其在限定硬件下具备实时分割能力。数据集及基准模型现已公开。

Conclusion: 新数据集和基准模型为太空实时自主巡检系统提供了重要资源，将推动实际应用落地和后续相关研究进展。

Abstract: Spacecraft deployed in outer space are routinely subjected to various forms
of damage due to exposure to hazardous environments. In addition, there are
significant risks to the subsequent process of in-space repairs through human
extravehicular activity or robotic manipulation, incurring substantial
operational costs. Recent developments in image segmentation could enable the
development of reliable and cost-effective autonomous inspection systems. While
these models often require large amounts of training data to achieve
satisfactory results, publicly available annotated spacecraft segmentation data
are very scarce. Here, we present a new dataset of nearly 64k annotated
spacecraft images that was created using real spacecraft models, superimposed
on a mixture of real and synthetic backgrounds generated using NASA's TTALOS
pipeline. To mimic camera distortions and noise in real-world image
acquisition, we also added different types of noise and distortion to the
images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to
generate performance benchmarks for the dataset under well-defined hardware and
inference time constraints to mimic real-world image segmentation challenges
for real-time onboard applications in space on NASA's inspector spacecraft. The
resulting models, when tested under these constraints, achieved a Dice score of
0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.
The dataset and models for performance benchmark are available at
https://github.com/RiceD2KLab/SWiM.

</details>


### [6] [Warehouse Spatial Question Answering with LLM Agent](https://arxiv.org/abs/2507.10778)
*Hsiang-Wei Huang,Jen-Hao Cheng,Kuang-Ming Chen,Cheng-Yen Yang,Bahaa Alattar,Yi-Ru Lin,Pyongkun Kim,Sangwon Kim,Kwangju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的新方法，通过集成多工具的LLM代理系统提升其在仓库场景复杂空间问答任务中的表现，并在公开数据集上验证了其高准确率和高效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）在空间理解任务上表现有限，且依赖大规模微调。为了提升空间理解能力并减少数据需求，亟需更高效的解决方案。

Method: 提出一个集成多种工具的LLM代理系统。该系统通过API工具交互和空间推理能力，针对复杂的室内仓库空间问答任务进行解答，无需大规模微调。

Result: 在2025 AI City Challenge Physical AI Spatial Intelligence Warehouse数据集上，该系统在物体检索、计数和距离估算任务上实现了高准确率和高效率。

Conclusion: 所提方法无需大量数据，通过智能集成工具使LLM能进行有效空间推理，在复杂场景中优于传统MLLM微调方法，具备较强实用价值。

Abstract: Spatial understanding has been a challenging task for existing Multi-modal
Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM
finetuning to enhance MLLM's spatial understanding ability. In this paper, we
present a data-efficient approach. We propose a LLM agent system with strong
and advanced spatial reasoning ability, which can be used to solve the
challenging spatial question answering task in complex indoor warehouse
scenarios. Our system integrates multiple tools that allow the LLM agent to
conduct spatial reasoning and API tools interaction to answer the given
complicated spatial question. Extensive evaluations on the 2025 AI City
Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that
our system achieves high accuracy and efficiency in tasks such as object
retrieval, counting, and distance estimation. The code is available at:
https://github.com/hsiangwei0903/SpatialAgent

</details>


### [7] [ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference](https://arxiv.org/abs/2507.10800)
*Ali Hojjat,Janek Haberer,Soren Pirk,Olaf Landsiedel*

Main category: cs.CV

TL;DR: 本文提出了ThinkingViT，一种具有动态计算能力的嵌套Vision Transformer（ViT）架构。该方法通过渐进式决策阶段，实现根据输入难度动态调整计算量，从而提升了推理效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉Transformer因计算预算固定，难以根据硬件异构和输入复杂度灵活分配算力，导致部署受限和推理低效。部分嵌套Transformer在一定程度上支持推理可扩展性，但仍对所有输入一视同仁，缺乏针对性资源分配。

Method: ThinkingViT采用渐进式推理机制：初始仅激活少量关键注意力头，若预测已足够确定则提前终止，否则逐步激活更多注意力头并重新评估。每个阶段基于前一阶段的token嵌入进行Token Recycling，促进推理逐步精细化。该方法保持了ViT主干结构，能作为普通ViT的插件升级。

Result: 在ImageNet-1K实验中，ThinkingViT在相同吞吐量下准确率较现有嵌套Transformer基线提升最高2.0个百分点，在等GMACs计算量条件下提升最高2.9个百分点。

Conclusion: ThinkingViT通过动态分配计算资源，有效兼顾推理效率和准确率，适用于多样化部署场景，同时具备良好的ViT兼容性和易用性。

Abstract: Vision Transformers deliver state-of-the-art performance, yet their fixed
computational budget prevents scalable deployment across heterogeneous
hardware. Recent nested Transformer architectures mitigate this by embedding
nested subnetworks within a single model to enable scalable inference. However,
these models allocate the same amount of compute to all inputs, regardless of
their complexity, which leads to inefficiencies. To address this, we introduce
ThinkingViT, a nested ViT architecture that employs progressive thinking stages
to dynamically adjust inference computation based on input difficulty.
ThinkingViT initiates inference by activating a small subset of the most
important attention heads and terminates early if predictions reach sufficient
certainty. Otherwise, it activates additional attention heads and re-evaluates
the input. At the core of ThinkingViT is our Token Recycling mechanism, which
conditions each subsequent inference stage on the embeddings from the previous
stage, enabling progressive improvement. Due to its backbone-preserving design,
ThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show
that ThinkingViT surpasses nested baselines by up to 2.0 percentage points
(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs
on ImageNet-1K. The source code is available at
https://github.com/ds-kiel/ThinkingViT.

</details>


### [8] [LLM-Guided Agentic Object Detection for Open-World Understanding](https://arxiv.org/abs/2507.10844)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: 本文提出了一种由大语言模型（LLM）引导的智能体式目标检测（LAOD）框架，无需标签即可实现零样本检测和命名，大大增强了目标检测的自动化和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测方法通常依赖固定类别集合，对于新类别需要高成本的重新训练。现有的开放世界（OWOD）和开放词汇目标检测（OVOD）虽提升了灵活性，但OWOD不能命名未知类别，OVOD又依赖于用户提示，限制了系统的自主性。因此亟需一种具备更高自主性、能适应新目标并自动命名的检测方法。

Method: 提出LAOD框架：由大语言模型根据场景生成对象名称，无需人工标签。这些名称用于引导开放词汇检测器完成定位及命名。为衡量定位和命名能力，还新引入了CAAP和SNAP两个指标。

Result: 在LVIS、COCO和COCO-OOD多个数据集上实验表明，提出的方法在新目标的检测和命名方面表现优异。

Conclusion: 该方法极大提升了目标检测系统的自主性和对开放世界环境的适应能力，为目标检测领域带来了新的解决思路。

Abstract: Object detection traditionally relies on fixed category sets, requiring
costly re-training to handle novel objects. While Open-World and
Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD
lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting
autonomy. We propose an LLM-guided agentic object detection (LAOD) framework
that enables fully label-free, zero-shot detection by prompting a Large
Language Model (LLM) to generate scene-specific object names. These are passed
to an open-vocabulary detector for localization, allowing the system to adapt
its goals dynamically. We introduce two new metrics, Class-Agnostic Average
Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately
evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD
validate our approach, showing strong performance in detecting and naming novel
objects. Our method offers enhanced autonomy and adaptability for open-world
understanding.

</details>


### [9] [Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization](https://arxiv.org/abs/2507.10846)
*Casey Wall,Longwei Wang,Rodrigue Rizk,KC Santosh*

Main category: cs.CV

TL;DR: Winsor-CAM 是一种改进的可解释性方法，通过聚合并去除极端值，对CNN模型各卷积层的显著性图赋予更高可解释性和可调性，效果优于传统Grad-CAM。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型尤其是卷积神经网络（CNN）在高风险领域应用时，模型决策过程的可解释性非常重要。现有主流方法Grad-CAM仅关注最后一层或简单平均各层特征，可能忽视重要语义信息或引入噪声，因此需要新的方法提升解释性和灵活调整能力。

Method: 提出Winsor-CAM，对Grad-CAM进行扩展，融合所有卷积层的特征信息，并使用Winsorization方法（基于百分位数的异常值削弱）减少极端噪声。同时设计了用户可控的阈值参数，实现人类可调的显著性关注层级。

Result: 在主流网络结构（ResNet50、DenseNet121、VGG16、InceptionV3）及PASCAL VOC 2012数据集上进行评估，Winsor-CAM比Grad-CAM和简单层平均法在解释性热力图和定位精度（如IoU和重心对齐）等指标上均表现更优。

Conclusion: Winsor-CAM不仅提升了可解释性和定位准确率，还引入了用户参与调控的能力，为实现可信AI提供了多层次、解释性强的工具。

Abstract: Interpreting the decision-making process of Convolutional Neural Networks
(CNNs) is critical for deploying models in high-stakes domains.
Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method
for visual explanations, yet it typically focuses on the final convolutional
layer or na\"ively averages across layers, strategies that can obscure
important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a
novel, human-tunable extension of Grad-CAM that generates robust and coherent
saliency maps by aggregating information across all convolutional layers. To
mitigate the influence of noisy or extreme attribution values, Winsor-CAM
applies Winsorization, a percentile-based outlier attenuation technique. A
user-controllable threshold allows for semantic-level tuning, enabling flexible
exploration of model behavior across representational hierarchies. Evaluations
on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the
PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable
heatmaps and achieves superior performance in localization metrics, including
intersection-over-union and center-of-mass alignment, when compared to Grad-CAM
and uniform layer-averaging baselines. Winsor-CAM advances the goal of
trustworthy AI by offering interpretable, multi-layer insights with
human-in-the-loop control.

</details>


### [10] [Sparse Fine-Tuning of Transformers for Generative Tasks](https://arxiv.org/abs/2507.10855)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种受稀疏编码启发的微调框架，用于改进大规模预训练Transformer在下游任务中的可解释性和表现。


<details>
  <summary>Details</summary>
Motivation: 现有的微调方法往往让模型参数的修改成为稠密组合，导致难以解释各参数贡献，理解模型如何适应新任务变得复杂。

Method: 作者提出用稀疏编码思想，将微调后的特征用特征原子字典的稀疏线性组合表示。通过选择和调整少量有意义的原子（字典基）实现微调，稀疏系数指标则反映各原子的贡献。

Result: 该方法在图像编辑、文本对齐和文本到图像的定制概念任务中表现优良。例如，通过去除无关特征原子提升了文本对齐能力，并在相关任务上超越了其他微调基线方法。

Conclusion: 稀疏原子字典微调不仅提升了模型微调的可解释性，还在一些关键任务上带来了效果提升，是一种值得推广的新型微调框架。

Abstract: Large pre-trained transformers have revolutionized artificial intelligence
across various domains, and fine-tuning remains the dominant approach for
adapting these models to downstream tasks due to the cost of training from
scratch. However, in existing fine-tuning methods, the updated representations
are formed as a dense combination of modified parameters, making it challenging
to interpret their contributions and understand how the model adapts to new
tasks. In this work, we introduce a fine-tuning framework inspired by sparse
coding, where fine-tuned features are represented as a sparse combination of
basic elements, i.e., feature dictionary atoms. The feature dictionary atoms
function as fundamental building blocks of the representation, and tuning atoms
allows for seamless adaptation to downstream tasks. Sparse coefficients then
serve as indicators of atom importance, identifying the contribution of each
atom to the updated representation. Leveraging the atom selection capability of
sparse coefficients, we first demonstrate that our method enhances image
editing performance by improving text alignment through the removal of
unimportant feature dictionary atoms. Additionally, we validate the
effectiveness of our approach in the text-to-image concept customization task,
where our method efficiently constructs the target concept using a sparse
combination of feature dictionary atoms, outperforming various baseline
fine-tuning methods.

</details>


### [11] [A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n](https://arxiv.org/abs/2507.10864)
*Saadat Behzadi,Danial Sharifrazi,Bita Mesbahzadeh,Javad Hassannataj Joloudarid,Roohallah Alizadehsani*

Main category: cs.CV

TL;DR: 该论文提出了一种结合LOF异常值过滤与YOLO-v11n轻量级深度学习模型的新型结直肠息肉检测框架，在多个公开数据集上验证了方法的高效、准确与实时性。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌发病率高，早期诊断和预防依赖于及时、准确地检测结直肠息肉。目前，深度学习方法虽已用于息肉检测，但普遍存在噪声样本影响和模型计算资源消耗大的问题。本文旨在提出一种既高效又轻量化的新方法，提升实际应用价值。

Method: 方法结合LOF算法去除噪声数据，并采用YOLO-v11n高效目标检测模型。在五个公开结直肠息肉数据集上，首先将分割掩码转为检测标签；接着利用LOF移除5%异常值，剩余数据用于YOLO-v11n训练，应用5折交叉验证和增强技术提升模型鲁棒性。

Result: 提出的方法在五个数据集上取得95.83%的精准率、91.85%的召回率、93.48%的F1分数，以及96.48%的mAP@0.5和77.75%的mAP@0.5:0.95，各项指标均优于以往YOLO类方法，表现出更高准确性和效率。

Conclusion: 该方法适用于临床实时结肠镜检查辅助系统，强调了数据预处理与模型效率在医疗影像AI系统中的重要作用，对实际应用有显著促进意义。

Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a
crucial role in diagnosing and preventing colorectal cancer, a major cause of
mortality worldwide. This study introduces a new, lightweight, and efficient
framework for polyp detection that combines the Local Outlier Factor (LOF)
algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier
removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly
available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.
Since these datasets originally lacked bounding box annotations, we converted
their segmentation masks into suitable detection labels. To enhance the
robustness and generalizability of our model, we apply 5-fold cross-validation
and remove anomalous samples using the LOF method configured with 30 neighbors
and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a
fast and resource-efficient object detection architecture optimized for
real-time applications. We train the model using a combination of modern
augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance,
achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5
of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,
our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited
for real-time colonoscopy support in clinical settings. Overall, the study
underscores how crucial data preprocessing and model efficiency are when
designing effective AI systems for medical imaging.

</details>


### [12] [Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes](https://arxiv.org/abs/2507.10881)
*Roman Naeem,David Hagerman,Jennifer Alvén,Lennart Svensson,Fredrik Kahl*

Main category: cs.CV

TL;DR: 本文提出了Trexplorer Super，一种改进的三维医学图像树状结构中心线追踪模型，并构建了难度递增的三组公开数据集，实验显示新方法在所有数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确追踪医学图像中的血管和气道等树状结构对许多后续任务至关重要，而现有模型在重复预测分支和追踪中断方面存在不足，且缺乏用于评估的公开数据集。

Method: 提出一种增强版Trexplorer Super模型，针对既有Trexplorer模型的不足进行了改进，还构建了一个合成数据集和两个真实数据集，以便对比分析和评测模型性能。

Result: 在新设计的三个数据集上，Trexplorer Super在所有评测指标上均超过现有最先进方法。同时，实验也发现仅在合成数据上的优异表现无法保证在真实数据集上的同等效果。

Conclusion: Trexplorer Super技术提升明显，能够更有效地追踪树状中心线结构，且与公开数据集一同为领域研究带来实用进展。

Abstract: Tubular tree structures, such as blood vessels and airways, are essential in
human anatomy and accurately tracking them while preserving their topology is
crucial for various downstream tasks. Trexplorer is a recurrent model designed
for centerline tracking in 3D medical images but it struggles with predicting
duplicate branches and terminating tracking prematurely. To address these
issues, we present Trexplorer Super, an enhanced version that notably improves
performance through novel advancements. However, evaluating centerline tracking
models is challenging due to the lack of public datasets. To enable thorough
evaluation, we develop three centerline datasets, one synthetic and two real,
each with increasing difficulty. Using these datasets, we conduct a
comprehensive evaluation of existing state-of-the-art (SOTA) models and compare
them with our approach. Trexplorer Super outperforms previous SOTA models on
every dataset. Our results also highlight that strong performance on synthetic
data does not necessarily translate to real datasets. The code and datasets are
available at https://github.com/RomStriker/Trexplorer-Super.

</details>


### [13] [Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency](https://arxiv.org/abs/2507.10893)
*Minjong Cheon,Eunhan Goo,Su-Hyeon Shin,Muhammad Ahmed,Hyungjun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于现代化CNN的新型全球天气预报模型KAI-a，在准确率上可比肩主流AI和传统NWP模型，但参数量和资源占用大幅降低，可在单GPU上快速训练。


<details>
  <summary>Details</summary>
Motivation: 尽管AI天气预报取得巨大进展，但主流Transformer模型普遍参数量巨大，训练和推理成本高。因此，急需一种计算高效、资源消耗低，且预测准确率优秀的替代模型。

Method: KAI-a以现代化CNN为核心，采用多尺度架构升级，包括尺度无关结构、InceptionNeXt-block设计，并融合地球物理数据的结构特性。模型在ERA5数据集（67个气象变量）上训练，参数量约700万，可在单块NVIDIA L40s GPU上12小时训练完成。

Result: KAI-a在中期天气预报准确率与SOTA模型相当，且计算资源需求大幅降低。案例分析显示其对2018年欧洲热浪和东亚夏季风等极端事件有良好捕捉能力，实用性强。

Conclusion: KAI-a实现了更高效、轻量的AI天气预报，兼顾精度与资源消耗，为全球天气预报提供了更具实际应用价值的深度学习解决方案。

Abstract: Recently, AI-based weather forecast models have achieved impressive advances.
These models have reached accuracy levels comparable to traditional NWP
systems, marking a significant milestone in data-driven weather prediction.
However, they mostly leverage Transformer-based architectures, which often
leads to high training complexity and resource demands due to the massive
parameter sizes. In this study, we introduce a modernized CNN-based model for
global weather forecasting that delivers competitive accuracy while
significantly reducing computational requirements. To present a systematic
modernization roadmap, we highlight key architectural enhancements across
multiple design scales from an earlier CNN-based approach. KAI-a incorporates a
scale-invariant architecture and InceptionNeXt-based blocks within a
geophysically-aware design, tailored to the structure of Earth system data.
Trained on the ERA5 daily dataset with 67 atmospheric variables, the model
contains about 7 million parameters and completes training in just 12 hours on
a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the
performance of state-of-the-art models in medium-range weather forecasting,
while offering a significantly lightweight design. Furthermore, case studies on
the 2018 European heatwave and the East Asian summer monsoon demonstrate
KAI-a's robust skill in capturing extreme events, reinforcing its practical
utility.

</details>


### [14] [Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition](https://arxiv.org/abs/2507.10895)
*Xiaocong Zeng,Craig Michoski,Yan Pang,Dongyang Kuang*

Main category: cs.CV

TL;DR: 本论文提出应对脑电（EEG）情感识别中标签不一致问题的新正则化方法，并取得优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: EEG情感识别模型在训练时常遇到随时间变化的标签不一致（TsDLI）问题，这影响了模型泛化能力和可解释性。当前方法鲜有专门为此设计的有效机制，亟需新方法加以改进。

Method: 作者提出了两种新型正则化策略：局部变化损失（LVL）和局部-全局一致性损失（LGCL），基于有界变差函数和图论的通勤时间距离理论嵌入神经网络训练过程中。此外，还引入了一套评价模型局部预测与全局情感标签一致性的新指标。

Result: 在DREAMER和DEAP两个公开EEG情感数据集以及多种主流架构（LSTM和Transformer等）下，综合五种新旧指标进行评测。结果显示，LVL和LGCL均优于现有方法，LVL在全部模型和指标中表现最佳，LGCL次之。

Conclusion: 提出的正则化方法有效缓解了标签随时间变化造成的不一致性，显著提升了情感识别的准确率和一致性表现，并在可解释性和性能之间实现了权衡。

Abstract: In this work, we address the often-overlooked issue of Timescale Dependent
Label Inconsistency (TsDLI) in training neural network models for EEG-based
human emotion recognition. To mitigate TsDLI and enhance model generalization
and explainability, we propose two novel regularization strategies: Local
Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods
incorporate classical mathematical principles--specifically, functions of
bounded variation and commute-time distances--within a graph theoretic
framework. Complementing our regularizers, we introduce a suite of new
evaluation metrics that better capture the alignment between temporally local
predictions and their associated global emotion labels. We validate our
approach through comprehensive experiments on two widely used EEG emotion
datasets, DREAMER and DEAP, across a range of neural architectures including
LSTM and transformer-based models. Performance is assessed using five distinct
metrics encompassing both quantitative accuracy and qualitative consistency.
Results consistently show that our proposed methods outperform state-of-the-art
baselines, delivering superior aggregate performance and offering a principled
trade-off between interpretability and predictive power under label
inconsistency. Notably, LVL achieves the best aggregate rank across all
benchmarked backbones and metrics, while LGCL frequently ranks the second,
highlighting the effectiveness of our framework.

</details>


### [15] [GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization](https://arxiv.org/abs/2507.10935)
*Shaowen Tong,Zimin Xia,Alexandre Alahi,Xuming He,Yujiao Shi*

Main category: cs.CV

TL;DR: 本文提出GeoDistill框架，通过弱监督自蒸馏和FoV掩码，提升跨视角定位的准确性和泛化能力，无需昂贵的姿态标注。


<details>
  <summary>Details</summary>
Motivation: 跨视角定位在自动导航和增强现实等大型户外场景有重要应用，但当前方法依赖大量人工姿态标注，成本高、扩展性差。作者希望减小对标注的依赖，同时提升定位性能。

Method: 提出GeoDistill框架：用教师模型对全景图像定位，学生模型输入FoV掩码后的普通图像，并通过对齐两者预测提升学生模型关注关键特征（如车道线）。创新性地引入基于视场遮罩的自蒸馏机制，并增加了无需精确位置信息的相对朝向估计网络。

Result: GeoDistill在多种跨视角定位框架中显著提升了定位性能，对全景和有限FoV图像都表现良好。同时相对朝向估计效果优异，无需精确标注。

Conclusion: GeoDistill实现了更高效、可扩展的跨视角定位，降低了标注成本，并对实际应用具有较大推动作用。

Abstract: Cross-view localization, the task of estimating a camera's
3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with
satellite images, is crucial for large-scale outdoor applications like
autonomous navigation and augmented reality. Existing methods often rely on
fully supervised learning, which requires costly ground-truth pose annotations.
In this work, we propose GeoDistill, a Geometry guided weakly supervised self
distillation framework that uses teacher-student learning with Field-of-View
(FoV)-based masking to enhance local feature learning for robust cross-view
localization. In GeoDistill, the teacher model localizes a panoramic image,
while the student model predicts locations from a limited FoV counterpart
created by FoV-based masking. By aligning the student's predictions with those
of the teacher, the student focuses on key features like lane lines and ignores
textureless regions, such as roads. This results in more accurate predictions
and reduced uncertainty, regardless of whether the query images are panoramas
or limited FoV images. Our experiments show that GeoDistill significantly
improves localization performance across different frameworks. Additionally, we
introduce a novel orientation estimation network that predicts relative
orientation without requiring precise planar position ground truth. GeoDistill
provides a scalable and efficient solution for real-world cross-view
localization challenges. Code and model can be found at
https://github.com/tongshw/GeoDistill.

</details>


### [16] [Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2507.10938)
*Zhengyi Xu,Haoran Wu,Wen Jiang,Jie Geng*

Main category: cs.CV

TL;DR: 本文提出了一种基于图聚合原型学习的遥感语义变化检测方法（GAPL-SCD），实现了对多时相遥感数据的精细语义变化标注，并在多项任务优化下显著提升了检测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 语义变化检测（SCD）不仅能检测变化位置，还能标注具体的‘由-到’类别，对遥感应用极具价值。然而，多任务并行学习易出现负迁移问题，影响模型性能。

Method: 作者提出GAPL-SCD方法，融合主任务（语义分割和变化检测）与辅助任务（图聚合原型学习）联合优化，并利用自适应权重分配及梯度旋转，缓解多任务冲突。通过高层特征构建交互图，采用原型实现时间点间类别对齐，减少无关变化干扰。自查询多层特征交互和双时相特征融合模块提升了复杂场景下的特征表达。

Result: 在SECOND与Landsat-SCD遥感数据集上的实验结果显示，该方法在检测精度和鲁棒性方面均达到了最新水平，显著优于现有技术。

Conclusion: GAPL-SCD有效缓解了多任务学习中的冲突，实现了精细准确的语义变化检测，并提升了模型的泛化能力和实用价值。

Abstract: Semantic change detection (SCD) extends the binary change detection task to
provide not only the change locations but also the detailed "from-to"
categories in multi-temporal remote sensing data. Such detailed semantic
insights into changes offer considerable advantages for a wide array of
applications. However, since SCD involves the simultaneous optimization of
multiple tasks, the model is prone to negative transfer due to task-specific
learning difficulties and conflicting gradient flows. To address this issue, we
propose Graph Aggregation Prototype Learning for Semantic Change Detection in
remote sensing(GAPL-SCD). In this framework, a multi-task joint optimization
method is designed to optimize the primary task of semantic segmentation and
change detection, along with the auxiliary task of graph aggregation prototype
learning. Adaptive weight allocation and gradient rotation methods are used to
alleviate the conflict between training tasks and improve multi-task learning
capabilities. Specifically, the graph aggregation prototype learning module
constructs an interaction graph using high-level features. Prototypes serve as
class proxies, enabling category-level domain alignment across time points and
reducing interference from irrelevant changes. Additionally, the proposed
self-query multi-level feature interaction and bi-temporal feature fusion
modules further enhance multi-scale feature representation, improving
performance in complex scenes. Experimental results on the SECOND and
Landsat-SCD datasets demonstrate that our method achieves state-of-the-art
performance, with significant improvements in accuracy and robustness for SCD
task.

</details>


### [17] [Robust ID-Specific Face Restoration via Alignment Learning](https://arxiv.org/abs/2507.10943)
*Yushun Fang,Lu Liu,Xiang Gao,Qiang Hu,Ning Cao,Jianghe Cui,Gang Chen,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的强健身份特定人脸修复方法(RIDFR)，通过并行内容与身份注入模块以及多参考对齐学习，实现了高质量、高身份保真的修复效果，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管最新的人脸修复方法能提高视觉质量，但由于输入不清晰和随机生成过程，会导致身份信息丧失，现有方法难以在严重退化情况下恢复出准确的身份，因此亟需一种能增强身份一致性的人脸修复新框架。

Method: RIDFR方法采用预训练的扩散模型，并设计了两个并行调控模块：内容注入模块负责输入严重退化的人脸图像，身份注入模块输入目标身份信息。再通过对多个同身份参考的修复结果执行“对齐学习”，抑制与身份无关的干扰（如表情、姿态、化妆、发型等）。

Result: 实验结果显示，RIDFR方法在重建高质量、身份特定的人脸图像方面超越了最先进的对比方法，同时表现出极高的身份保真度和强健鲁棒性。

Conclusion: RIDFR不仅提升了人脸修复的视觉与身份一致性，还解决了身份信息难以恢复的问题，为高保真身份还原提供了有效的新思路，具有实际应用价值。

Abstract: The latest developments in Face Restoration have yielded significant
advancements in visual quality through the utilization of diverse diffusion
priors. Nevertheless, the uncertainty of face identity introduced by
identity-obscure inputs and stochastic generative processes remains unresolved.
To address this challenge, we present Robust ID-Specific Face Restoration
(RIDFR), a novel ID-specific face restoration framework based on diffusion
models. Specifically, RIDFR leverages a pre-trained diffusion model in
conjunction with two parallel conditioning modules. The Content Injection
Module inputs the severely degraded image, while the Identity Injection Module
integrates the specific identity from a given image. Subsequently, RIDFR
incorporates Alignment Learning, which aligns the restoration results from
multiple references with the same identity in order to suppress the
interference of ID-irrelevant face semantics (e.g. pose, expression, make-up,
hair style). Experiments demonstrate that our framework outperforms the
state-of-the-art methods, reconstructing high-quality ID-specific results with
high identity fidelity and demonstrating strong robustness.

</details>


### [18] [Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data](https://arxiv.org/abs/2507.10969)
*Palash Ray,Mahuya Sasmal,Asish Bera*

Main category: cs.CV

TL;DR: 本文提出了一个专门用于女性体育动作分类的新数据集WomenSports，并设计了一种结合卷积神经网络（CNN）和通道注意力机制的新方法，在多个体育和舞蹈数据集上表现突出，尤其在WomenSports数据集上取得了89.15%的top-1准确率。


<details>
  <summary>Details</summary>
Motivation: 目前用于体育动作分类的图像数据集很少涵盖女性动作，且数据集间类内与类间变化有限，限制了针对女性体育分析及算法的发展。

Method: 作者构建了WomenSports数据集，包含多种女性体育动作，并提出结合CNN和基于局部上下文区域的通道注意力机制的深度特征提取方法。该方法在三个体育数据集和一个舞蹈数据集上进行了广泛实验，采用ResNet-50架构。

Result: 在WomenSports数据集上，基于ResNet-50的深度学习方法取得了89.15%的top-1分类准确率，且在其他数据集也表现优秀。

Conclusion: 新提出的WomenSports数据集为女性体育动作研究提供了宝贵资源，结合通道注意力的CNN方法在小样本体育图像分类中取得了优异效果，对相关领域具有积极推动作用。

Abstract: Sports action classification representing complex body postures and
player-object interactions is an emerging area in image-based sports analysis.
Some works have contributed to automated sports action recognition using
machine learning techniques over the past decades. However, sufficient image
datasets representing women sports actions with enough intra- and inter-class
variations are not available to the researchers. To overcome this limitation,
this work presents a new dataset named WomenSports for women sports
classification using small-scale training data. This dataset includes a variety
of sports activities, covering wide variations in movements, environments, and
interactions among players. In addition, this study proposes a convolutional
neural network (CNN) for deep feature extraction. A channel attention scheme
upon local contextual regions is applied to refine and enhance feature
representation. The experiments are carried out on three different sports
datasets and one dance dataset for generalizing the proposed algorithm, and the
performances on these datasets are noteworthy. The deep learning method
achieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed
WomenSports dataset, which is publicly available for research at Mendeley Data.

</details>


### [19] [Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection](https://arxiv.org/abs/2507.10977)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: 本文提出了一种新的人-物体交互（HOI）检测架构，包括wavelet注意力主干网络和光线编码器，在保证效率的同时提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有HOI检测方法在预测准确性和计算效率之间存在矛盾，通常依赖高资源消耗的训练方法和复杂架构，难以高效满足实际需求。

Method: 作者设计了一种wavelet注意力主干网络，通过多种卷积滤波器提取并聚合低/高阶特征，增强对中阶交互信息的表达能力。同时提出基于光线的编码器，优化解码器关注关键区域，利用可学习的光线原点实现多尺度注意力，降低计算量提升效率。

Result: 在ImageNet和HICO-DET等基准数据集上，所提架构展现出显著性能提升，验证了方法的有效性。

Conclusion: 本文方法在提升HOI检测精度的同时，显著降低了计算资源消耗，有望在实际视觉场景理解任务中发挥重要作用。

Abstract: Human-object interaction (HOI) detection is essential for accurately
localizing and characterizing interactions between humans and objects,
providing a comprehensive understanding of complex visual scenes across various
domains. However, existing HOI detectors often struggle to deliver reliable
predictions efficiently, relying on resource-intensive training methods and
inefficient architectures. To address these challenges, we conceptualize a
wavelet attention-like backbone and a novel ray-based encoder architecture
tailored for HOI detection. Our wavelet backbone addresses the limitations of
expressing middle-order interactions by aggregating discriminative features
from the low- and high-order interactions extracted from diverse convolutional
filters. Concurrently, the ray-based encoder facilitates multi-scale attention
by optimizing the focus of the decoder on relevant regions of interest and
mitigating computational overhead. As a result of harnessing the attenuated
intensity of learnable ray origins, our decoder aligns query embeddings with
emphasized regions of interest for accurate predictions. Experimental results
on benchmark datasets, including ImageNet and HICO-DET, showcase the potential
of our proposed architecture. The code is publicly available at
[https://github.com/henry-pay/RayEncoder].

</details>


### [20] [Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction](https://arxiv.org/abs/2507.10978)
*Ayush Gupta,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: 本文提出了一种名为RG-Gait的新方法，通过残差学习提升在遮挡步态下的识别率，同时保持对完整步态的识别精度。该方法无需成对的遮挡与完整样本，实用性强，在多个公开数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别方法在面对遮挡问题时往往需要成对的完整与遮挡样本，且大多聚焦于遮挡情景而导致完整步态下性能下降，这些都与实际应用存在脱节。

Method: 作者将遮挡步态识别建模为残差学习问题，将遮挡步态特征视为完整特征的残差偏移，通过残差网络自适应地融合所学残差信息。

Result: 在Gait3D、GREW和BRIAR等多个复杂步态数据集上，RG-Gait在遮挡情况下识别准确率显著提升，并且在完整步态下的性能未受影响。

Conclusion: 残差学习是应对遮挡步态识别且不牺牲完整步态识别能力的有效方法，RG-Gait在多数据集验证了其实用性和有效性。

Abstract: Gait is becoming popular as a method of person re-identification because of
its ability to identify people at a distance. However, most current works in
gait recognition do not address the practical problem of occlusions. Among
those which do, some require paired tuples of occluded and holistic sequences,
which are impractical to collect in the real world. Further, these approaches
work on occlusions but fail to retain performance on holistic inputs. To
address these challenges, we propose RG-Gait, a method for residual correction
for occluded gait recognition with holistic retention. We model the problem as
a residual learning task, conceptualizing the occluded gait signature as a
residual deviation from the holistic gait representation. Our proposed network
adaptively integrates the learned residual, significantly improving performance
on occluded gait sequences without compromising the holistic recognition
accuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR
datasets and show that learning the residual can be an effective technique to
tackle occluded gait recognition with holistic retention.

</details>


### [21] [SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition](https://arxiv.org/abs/2507.10999)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: 本文提出SpaRTAN，一种高效卷积神经网络架构，通过增强空间和通道信息处理，实现了参数高效且表现优良的视觉识别。


<details>
  <summary>Details</summary>
Motivation: 现有CNN和transformer架构偏向简单特征，对复杂结构特征处理有限。同时现代CNN集成MLP-like结构存在信息冗余问题，提升性能需高扩展比，效率受限。

Method: SpaRTAN设计采用多感受野卷积核（通过核大小和膨胀因子调控）捕获多阶空间特征，同时引入基于波的通道聚合模块，强化像素间相互作用、减少通道冗余，两者结合提升特征表达能力和建模效率。

Result: 在ImageNet-1k上，使用仅3.8M参数和约1.0 GFLOPs的SpaRTAN取得77.7%的准确率；在COCO上，21.5M参数下AP达到50.0%，比前期最佳提升1.2%。

Conclusion: SpaRTAN在保证模型紧凑与高效的情况下，实现了与更大模型媲美或超越的性能，显示了改进空间与通道处理对于CNN效率和效果的显著提升。

Abstract: The resurgence of convolutional neural networks (CNNs) in visual recognition
tasks, exemplified by ConvNeXt, has demonstrated their capability to rival
transformer-based architectures through advanced training methodologies and
ViT-inspired design principles. However, both CNNs and transformers exhibit a
simplicity bias, favoring straightforward features over complex structural
representations. Furthermore, modern CNNs often integrate MLP-like blocks akin
to those in transformers, but these blocks suffer from significant information
redundancies, necessitating high expansion ratios to sustain competitive
performance. To address these limitations, we propose SpaRTAN, a lightweight
architectural design that enhances spatial and channel-wise information
processing. SpaRTAN employs kernels with varying receptive fields, controlled
by kernel size and dilation factor, to capture discriminative multi-order
spatial features effectively. A wave-based channel aggregation module further
modulates and reinforces pixel interactions, mitigating channel-wise
redundancies. Combining the two modules, the proposed network can efficiently
gather and dynamically contextualize discriminative features. Experimental
results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable
parameter efficiency while maintaining competitive performance. In particular,
on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M
parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver
strong performance through an efficient design. On the COCO benchmark, it
achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M
parameters. The code is publicly available at
[https://github.com/henry-pay/SpaRTAN].

</details>


### [22] [Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection](https://arxiv.org/abs/2507.11003)
*Yuhu Bai,Jiangning Zhang,Yunkang Cao,Guangyuan Lu,Qingdong He,Xiangtai Li,Guanzhong Tian*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的CLIP新方法——FiSeCLIP，用于零样本异常检测，通过批量内部参考和跨模态特征筛选，大幅提升分类与分割效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于CLIP的零样本异常检测因难以获得稀有类而颇受关注，但实际工业场景中批量数据处理更为常见，亟需提升在此设置下的检测精度。

Method: FiSeCLIP结合了特征匹配与跨模态对齐。具体地，利用批次内其他无标签图片作为参考，并通过文本条件筛除嘈杂特征，同时挖掘CLIP本身的局部语义相关性，实现更精细的异常检测。

Result: 在多个异常检测基准（如MVTec-AD）上，FiSeCLIP在分类与分割任务均明显优于SOTA方法，如分割AU-ROC和F1-max分别提升4.6%和5.7%。

Conclusion: FiSeCLIP作为一种高效无训练的CLIP应用，在实际批量检测任务中建立了更强的新基线，并证明了CLIP本身在细粒度异常检测中的巨大潜力。

Abstract: With the advent of vision-language models (e.g., CLIP) in zero- and few-shot
settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in
recent research, where the rare classes are essential and expected in many
applications. This study introduces \textbf{FiSeCLIP} for ZSAD with
training-free \textbf{CLIP}, combining the feature matching with the
cross-modal alignment. Testing with the entire dataset is impractical, while
batch-based testing better aligns with real industrial needs, and images within
a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes
other images in the same batch as reference information for the current image.
However, the lack of labels for these references can introduce ambiguity, we
apply text information to \textbf{fi}lter out noisy features. In addition, we
further explore CLIP's inherent potential to restore its local
\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection
tasks to enable a more accurate filtering process. Our approach exhibits
superior performance for both anomaly classification and segmentation on
anomaly detection benchmarks, building a stronger baseline for the direction,
e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by
+4.6\%$\uparrow$/+5.7\%$\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.

</details>


### [23] [Semantically Informed Salient Regions Guided Radiology Report Generation](https://arxiv.org/abs/2507.11015)
*Zeyi Hou,Zeqiang Wei,Ruixin Yan,Ning Lang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一种新的语义引导的显著区域（SISRNet）方法，解决自动放射报告生成过程中因数据偏差导致的医学不准确问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的胸部X光自动报告生成方法，虽然可以降低放射科医生的工作负担，但由于医学图像固有的数据偏差和异常区域稀疏，导致生成的报告常常流畅但医学上不准确，限制了其临床应用。

Method: 该方法（SISRNet）提出以细粒度跨模态语义明确识别医学关键的显著区域。在图像建模和报告生成阶段聚焦这些高信息区域，从而更好地捕获细微异常，减缓数据偏差带来的负面影响。

Result: 在常用的IU-Xray和MIMIC-CXR数据集上，SISRNet相较于现有方法展现出更优越的性能。

Conclusion: SISRNet通过语义引导显著区域的方法，有效提升了自动放射报告生成的医学准确性和临床适用性。

Abstract: Recent advances in automated radiology report generation from chest X-rays
using deep learning algorithms have the potential to significantly reduce the
arduous workload of radiologists. However, due to the inherent massive data
bias in radiology images, where abnormalities are typically subtle and sparsely
distributed, existing methods often produce fluent yet medically inaccurate
reports, limiting their applicability in clinical practice. To address this
issue effectively, we propose a Semantically Informed Salient Regions-guided
(SISRNet) report generation method. Specifically, our approach explicitly
identifies salient regions with medically critical characteristics using
fine-grained cross-modal semantics. Then, SISRNet systematically focuses on
these high-information regions during both image modeling and report
generation, effectively capturing subtle abnormal findings, mitigating the
negative impact of data bias, and ultimately generating clinically accurate
reports. Compared to its peers, SISRNet demonstrates superior performance on
widely used IU-Xray and MIMIC-CXR datasets.

</details>


### [24] [Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion](https://arxiv.org/abs/2507.11025)
*Sung Ho Kang,Hyun-Cheol Park*

Main category: cs.CV

TL;DR: 本文提出了一种结合薛定谔桥（Schrodinger Bridge）理论的新颖CBCT到MDCT图像转换方法，融合了GAN先验和人类引导的条件扩散，显著提高了医学图像转换的精确性与效率。


<details>
  <summary>Details</summary>
Motivation: CBCT（锥形束CT）和MDCT（多层CT）在医学影像中各有优势与不足，但CBCT图像存在伪影和细节丢失，影响诊断，迫切需要更真实、结构一致的CBCT到MDCT转换方法。

Method: 该方法以薛定谔桥为基础，整合GAN生成先验与人类引导的条件扩散生成模型，显式约束输入CBCT与合成MDCT的边界一致性。采用无分类器引导（CFG）结合二元人类反馈，通过多轮迭代精炼和锦标赛偏好选择让模型习得临床优先的人类偏好，无需训练额外奖励模型。

Result: 在真实临床数据集上，所提方法在10步采样内即可实现转换，相较GAN和微调等方法，在RMSE、SSIM、LPIPS和Dice等多项指标上均大幅领先，减弱了关键解剖区伪影同时保留了细微结构。

Conclusion: 该框架在实现实时、可控且贴合人类偏好的医学影像转换方面表现高效且优越，有潜力应用于临床实际，提升医学影像质量与诊断可靠性。

Abstract: We present a novel framework for CBCT-to-MDCT translation, grounded in the
Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with
human-guided conditional diffusion. Unlike conventional GANs or diffusion
models, our approach explicitly enforces boundary consistency between CBCT
inputs and pseudo targets, ensuring both anatomical fidelity and perceptual
controllability. Binary human feedback is incorporated via classifier-free
guidance (CFG), effectively steering the generative process toward clinically
preferred outcomes. Through iterative refinement and tournament-based
preference selection, the model internalizes human preferences without relying
on a reward model. Subtraction image visualizations reveal that the proposed
method selectively attenuates shade artifacts in key anatomical regions while
preserving fine structural detail. Quantitative evaluations further demonstrate
superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical
datasets -- outperforming prior GAN- and fine-tuning-based feedback methods --
while requiring only 10 sampling steps. These findings underscore the
effectiveness and efficiency of our framework for real-time, preference-aligned
medical image translation.

</details>


### [25] [Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2507.11030)
*Sunghyun Park,Jungsoo Lee,Shubhankar Borse,Munawar Hayat,Sungha Choi,Kyuwoong Hwang,Fatih Porikli*

Main category: cs.CV

TL;DR: 提出了一种个性化开放词汇语义分割任务和方法，能准确分割用户特别关注的对象（如“我的杯子”），并在新建立的基准上取得优越效果。


<details>
  <summary>Details</summary>
Motivation: 现有的开放词汇语义分割方法虽然支持未见类，但无法理解带有个人指代的文本（如“我的杯子”），这限制了用户对特定兴趣目标的分割需求。

Method: 首次提出个性化开放词汇语义分割任务。方法上，通过基于文本提示调优的插件式方案，仅利用少量图像与掩码对，学习用户的个性化目标。创新性地引入“负掩码提议”减少错误预测，并将个性化视觉特征嵌入文本提示以提升表达能力，同时保证原有开放词汇分割性能不受影响。

Result: 在作者新建立的FSS^per、CUB^per和ADE^per三个个性化基准上，该方法性能超越了现有方法，有效分割出用户特别关注的目标。

Conclusion: 提出的方法在支持个性化分割需求的同时，不影响原有系统性能，是开放词汇分割领域的有益扩展。

Abstract: While open-vocabulary semantic segmentation (OVSS) can segment an image into
semantic regions based on arbitrarily given text descriptions even for classes
unseen during training, it fails to understand personal texts (e.g., `my mug
cup') for segmenting regions of specific interest to users. This paper
addresses challenges like recognizing `my mug cup' among `multiple mug cups'.
To overcome this challenge, we introduce a novel task termed
\textit{personalized open-vocabulary semantic segmentation} and propose a text
prompt tuning-based plug-in method designed to recognize personal visual
concepts using a few pairs of images and masks, while maintaining the
performance of the original OVSS. Based on the observation that reducing false
predictions is essential when applying text prompt tuning to this task, our
proposed method employs `negative mask proposal' that captures visual concepts
other than the personalized concept. We further improve the performance by
enriching the representation of text prompts by injecting visual embeddings of
the personal concept into them. This approach enhances personalized OVSS
without compromising the original OVSS performance. We demonstrate the
superiority of our method on our newly established benchmarks for this task,
including FSS$^\text{per}$, CUB$^\text{per}$, and ADE$^\text{per}$.

</details>


### [26] [Efficient Dual-domain Image Dehazing with Haze Prior Perception](https://arxiv.org/abs/2507.11035)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的新型去雾网络（DGFDNet），能在保证实时性的同时，实现优异的单幅图像去雾效果，尤其适应复杂雾霾场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的去雾方法虽有较强的全局建模能力，但计算量大限制了其实时应用；且以往方法多数依赖空间域特征，难以高效处理复杂雾霾。空间-频率域结合虽有进展，但两域联动性弱，整体性能受限。作者旨在设计一个高效、双域联动且物理引导的去雾框架。

Method: 设计了DGFDNet，包括：1）使用暗通道先验生成雾置信图，通过频域调制增强雾相关频率成分，实现全局退化感知调制（HAFM）；2）多尺度特征门控聚合（MGAM），结合多种卷积核和门控机制，恢复细节；3）增加先验校正反馈支路（PCGB），利用中间去雾特征迭代优化先验，提高雾的定位。

Result: 在四个主流基准数据集上，DGFDNet取得了最新最优的去雾效果，表现出极强的鲁棒性和实时性能。

Conclusion: DGFDNet实现了高效、强鲁棒性的单幅图像去雾，尤其适合复杂室外环境，推动了去雾领域的实际应用发展。

Abstract: Transformer-based models exhibit strong global modeling capabilities in
single-image dehazing, but their high computational cost limits real-time
applicability. Existing methods predominantly rely on spatial-domain features
to capture long-range dependencies, which are computationally expensive and
often inadequate under complex haze conditions. While some approaches introduce
frequency-domain cues, the weak coupling between spatial and frequency branches
limits the overall performance. To overcome these limitations, we propose the
Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel
dual-domain framework that performs physically guided degradation alignment
across spatial and frequency domains. At its core, the DGFDBlock comprises two
key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a
pixel-level haze confidence map from dark channel priors to adaptively enhance
haze-relevant frequency components, thereby achieving global degradation-aware
spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which
fuses multi-scale features through diverse convolutional kernels and hybrid
gating mechanisms to recover fine structural details. Additionally, a Prior
Correction Guidance Branch (PCGB) incorporates a closed-loop feedback
mechanism, enabling iterative refinement of the prior by intermediate dehazed
features and significantly improving haze localization accuracy, especially in
challenging outdoor scenes. Extensive experiments on four benchmark haze
datasets demonstrate that DGFDNet achieves state-of-the-art performance with
superior robustness and real-time efficiency. Code is available at:
https://github.com/Dilizlr/DGFDNet.

</details>


### [27] [A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion](https://arxiv.org/abs/2507.11037)
*Jie-Wen Li,Zi-Han Ye,Qingyuan Zhou,Jiayi Song,Ying He,Ben Fei,Wen-Ming Chen*

Main category: cs.CV

TL;DR: 本文提出 FootGait3D 数据集，专注于步态过程中足踝区域的高分辨率三维点云数据采集，旨在帮助三维点云补全方法的研究和临床步态分析。


<details>
  <summary>Details</summary>
Motivation: 现有步态数据集多关注于全身或下肢运动，足踝区域的精细动态三维数据有限；步态检测中，受脚摆动、遮挡和视角限制，很难精确采集足-踝表面数据。

Method: 构建了FootGait3D多视角数据集，含46名受试者、8403帧点云，使用五摄像头深度感知系统同步采集。每帧含完整五视角三维重建（真值）及不完全（2~4视角）点云，模拟遮挡和视角缺失。

Result: 该数据集使不同三维点云补全方法（单模态如PointTr、SnowflakeNet、Anchorformer，多模态如SVDFormer、PointSea、CSDN）在足踝复杂运动下进行基准评测成为可能。

Conclusion: FootGait3D弥补了足-踝三维动态高精度数据的空白，对生物力学、临床步态分析、假肢设计和机器人足部建模等应用具有重要推动作用。

Abstract: The kinematics analysis of foot-ankle complex during gait is essential for
advancing biomechanical research and clinical assessment. Collecting accurate
surface geometry data from the foot and ankle during dynamic gait conditions is
inherently challenging due to swing foot occlusions and viewing limitations.
Thus, this paper introduces FootGait3D, a novel multi-view dataset of
high-resolution ankle-foot surface point clouds captured during natural gait.
Different from existing gait datasets that typically target whole-body or
lower-limb motion, FootGait3D focuses specifically on the detailed modeling of
the ankle-foot region, offering a finer granularity of motion data. To address
this, FootGait3D consists of 8,403 point cloud frames collected from 46
subjects using a custom five-camera depth sensing system. Each frame includes a
complete 5-view reconstruction of the foot and ankle (serving as ground truth)
along with partial point clouds obtained from only four, three, or two views.
This structured variation enables rigorous evaluation of 3D point cloud
completion methods under varying occlusion levels and viewpoints. Our dataset
is designed for shape completion tasks, facilitating the benchmarking of
state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and
multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the
challenge of recovering the full foot geometry from occluded inputs. FootGait3D
has significant potential to advance research in biomechanics and multi-segment
foot modeling, offering a valuable testbed for clinical gait analysis,
prosthetic design, and robotics applications requiring detailed 3D models of
the foot during motion. The dataset is now available at
https://huggingface.co/datasets/ljw285/FootGait3D.

</details>


### [28] [Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery](https://arxiv.org/abs/2507.11040)
*Nicolas Drapier,Aladine Chetouani,Aurélien Chateigner*

Main category: cs.CV

TL;DR: GLOD是一种面向高分辨率卫星图像目标检测的新型transformer架构，通过创新性模块实现了较大精度提升。


<details>
  <summary>Details</summary>
Motivation: 高分辨率卫星图像中的目标检测因尺度多样和复杂背景极具挑战，现有方法（大多基于CNN）存在局限，因此需要新的、更适合复杂场景的特征提取与融合方法。

Method: GLOD用Swin Transformer取代常见的CNN骨干用于端到端特征提取，配合创新的UpConvMixer模块进行上采样和多尺度融合模块（Fusion Blocks）。核心创新包括：带CBAM注意力的非对称融合结构和多路径检测头，以增强多尺度目标的检测能力。

Result: 在xView数据集上，GLOD取得了32.95%的检测精度，较现有最优方法提升了11.46%。

Conclusion: GLOD架构专为卫星遥感场景优化，兼顾空间先验和计算效率，并在多个方面表现出色，有望成为遥感目标检测领域的新技术路线。

Abstract: We present GLOD, a transformer-first architecture for object detection in
high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin
Transformer for end-to-end feature extraction, combined with novel UpConvMixer
blocks for robust upsampling and Fusion Blocks for multi-scale feature
integration. Our approach achieves 32.95\% on xView, outperforming SOTA methods
by 11.46\%. Key innovations include asymmetric fusion with CBAM attention and a
multi-path head design capturing objects across scales. The architecture is
optimized for satellite imagery challenges, leveraging spatial priors while
maintaining computational efficiency.

</details>


### [29] [Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation](https://arxiv.org/abs/2507.11055)
*Shuchang Ye,Usman Naseem,Mingyuan Meng,Jinman Kim*

Main category: cs.CV

TL;DR: 现有医学语言引导分割方法因依赖图像-文本对数据集，在缺乏文本配对时应用受限。ProLearn提出原型驱动的方式，无需每张图像都配文本报告，也可高效完成分割。实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学影像分割利用文本报告作为辅助，可提升分割精度。但大部分真实场景没法获得成对的图像-文本数据，且推理时通常图像在前、报告在后，导致当前方法在训练和推理两方面局限明显。

Method: 提出ProLearn框架，核心为原型驱动语义近似（PSA）模块。PSA首先从有限的文本报告中提取出与分割相关的语义，初始化一个紧凑的原型空间。之后，在没有文本输入的情况下利用查询-响应机制，通过原型空间为图像提供语义引导，减轻对文本的依赖。

Result: 在QaTa-COV19、MosMedData+和Kvasir-SEG等数据集上的实验显示，当可用文本有限时，ProLearn分割效果优于现有最先进的语言引导方法。

Conclusion: ProLearn利用原型驱动的生成语义引导，有效解决了医学分割领域对图像-文本配对的依赖问题，提升了方法的普适性和实际应用价值。

Abstract: Medical language-guided segmentation, integrating textual clinical reports as
auxiliary guidance to enhance image segmentation, has demonstrated significant
improvements over unimodal approaches. However, its inherent reliance on paired
image-text input, which we refer to as ``textual reliance", presents two
fundamental limitations: 1) many medical segmentation datasets lack paired
reports, leaving a substantial portion of image-only data underutilized for
training; and 2) inference is limited to retrospective analysis of cases with
paired reports, limiting its applicability in most clinical scenarios where
segmentation typically precedes reporting. To address these limitations, we
propose ProLearn, the first Prototype-driven Learning framework for
language-guided segmentation that fundamentally alleviates textual reliance. At
its core, in ProLearn, we introduce a novel Prototype-driven Semantic
Approximation (PSA) module to enable approximation of semantic guidance from
textual input. PSA initializes a discrete and compact prototype space by
distilling segmentation-relevant semantics from textual reports. Once
initialized, it supports a query-and-respond mechanism which approximates
semantic guidance for images without textual input, thereby alleviating textual
reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG
demonstrate that ProLearn outperforms state-of-the-art language-guided methods
when limited text is available.

</details>


### [30] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Main category: cs.CV

TL;DR: 本文提出了RoMaP，一个用于局部3D高斯编辑的新框架，通过引入3D掩码生成和正则化SDS损失，实现了对3D内容精确且灵活的局部修改，取得了目前最好的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D神经表达和实例级编辑模型在生成高质量3D内容方面已取得进步，但在实现高斯分布渲染中的精准局部编辑时仍存在视角不一致和损失不明确的问题。

Method: 1）提出了3D-Geometry Aware Label Prediction（3D-GALP）模块，利用球谐系数建模视角相关标签变化，实现跨视角一致的3D分割掩码。2）设计了正则化的SDS损失，并通过SLaMP（Scheduled Latent Mixing and Part editing）方法引入L1锚点损失，仅在目标区域进行修改，同时添加高斯先验去除等正则项提升灵活性，以及鲁棒的3D掩码防止误编辑。

Result: RoMaP在重建和生成的高斯场景与目标物体上，均在局部3D编辑任务中取得了最新的定性和定量最优表现。

Conclusion: RoMaP能更准确、灵活地进行3D高斯分布场景的局部部件级编辑，为3D内容创建提供了更强工具。

Abstract: Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing.

</details>


### [31] [Joint angle model based learning to refine kinematic human pose estimation](https://arxiv.org/abs/2507.11075)
*Chang Peng,Yifei Zhou,Huifeng Xi,Shiqing Huang,Chuangye Chen,Jianming Yang,Bao Yang,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 本论文提出了一种基于关节角度建模的新方法，用于提升人类姿态估计的精度和稳定性，尤其在现有方法易出错和波动较大的场景中效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有的无标记人类姿态估计方法常因手工标注数据集的不准确性导致关键点识别错误和关键点轨迹波动，限制了其实际应用和深度学习模型的性能，因此亟需一种可以提高姿态估计质量的精细化方法。

Method: 提出了基于关节角度的人体姿态模型，并通过高阶傅里叶级数逼近关节角度的时间变化来获得高质量的“真实值”数据。基于此，使用双向循环神经网络作为HRNet的后处理模块，对识别结果进行校正和轨迹平滑。

Result: 实验表明，用该方法构建的高质量数据集训练的网络能有效修正错误关键点，极大提升了姿态估计的时空平滑性。在花样滑冰、霹雳舞等高难度场景下，提出的关节角度去噪（JAR）方法超越了目前最先进的姿态估计优化网络。

Conclusion: 基于关节角度建模及傅里叶级数逼近组合的高质量数据训练和网络后处理策略，为人类姿态估计的精确性和鲁棒性提供了新的解决方案，尤其是在复杂运动场景下效果明显优于现有技术。

Abstract: Marker-free human pose estimation (HPE) has found increasing applications in
various fields. Current HPE suffers from occasional errors in keypoint
recognition and random fluctuation in keypoint trajectories when analyzing
kinematic human poses. The performance of existing deep learning-based models
for HPE refinement is considerably limited by inaccurate training datasets in
which the keypoints are manually annotated. This paper proposed a novel method
to overcome the difficulty through joint angle-based modeling. The key
techniques include: (i) A joint angle-based model of human pose, which is
robust to describe kinematic human poses; (ii) Approximating temporal variation
of joint angles through high order Fourier series to get reliable "ground
truth"; (iii) A bidirectional recurrent network is designed as a
post-processing module to refine the estimation of well-established HRNet.
Trained with the high-quality dataset constructed using our method, the network
demonstrates outstanding performance to correct wrongly recognized joints and
smooth their spatiotemporal trajectories. Tests show that joint angle-based
refinement (JAR) outperforms the state-of-the-art HPE refinement network in
challenging cases like figure skating and breaking.

</details>


### [32] [Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers](https://arxiv.org/abs/2507.11287)
*An-Lun Liu,Yu-Wei Chao,Yi-Ting Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种新的任务导向型人类抓取合成方法，能够结合任务和场景信息，实现更精准的抓取。


<details>
  <summary>Details</summary>
Motivation: 当前的抓取合成方法通常只关注手与物体的关系，缺乏对任务目标及场景上下文的考量，难以获得真正符合实际需求的抓取方式。

Method: 作者提出任务感知接触图（task-aware contact maps），将场景和任务信息融入其中，并设计了两阶段流程：首先基于场景与任务构造接触图，随后利用该图合成任务导向型的人类抓取姿势。此外，作者还构建了新数据集和评测指标。

Result: 实验结果验证了同时建模场景与任务对于提升抓取质量和任务执行力的重要性。该方法在抓取质量和完成任务的表现上，相较现有方法表现出显著改进。

Conclusion: 全面纳入场景和任务信息能够极大提升抓取合成的鲁棒性和有效性，为更智能的手-物交互提供了新思路。

Abstract: In this paper, we study task-oriented human grasp synthesis, a new grasp
synthesis task that demands both task and context awareness. At the core of our
method is the task-aware contact maps. Unlike traditional contact maps that
only reason about the manipulated object and its relation with the hand, our
enhanced maps take into account scene and task information. This comprehensive
map is critical for hand-object interaction, enabling accurate grasping poses
that align with the task. We propose a two-stage pipeline that first constructs
a task-aware contact map informed by the scene and task. In the subsequent
stage, we use this contact map to synthesize task-oriented human grasps. We
introduce a new dataset and a metric for the proposed task to evaluate our
approach. Our experiments validate the importance of modeling both scene and
task, demonstrating significant improvements over existing methods in both
grasp quality and task performance. See our project page for more details:
https://hcis-lab.github.io/TOHGS/

</details>


### [33] [GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft](https://arxiv.org/abs/2507.11077)
*Weizhao Ma,Dong Zhou,Yuhui Hu,Zipeng He*

Main category: cs.CV

TL;DR: 本文提出了一种基于图结构的关键点网络（GKNet），用于单目非合作航天器位姿估计，并同时发布了一个中等规模的数据集SKD。实验显示该方法在关键点检测准确性上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 非合作航天器单目位姿估计对于在轨服务（如卫星维护、空间碎片移除等）具有重要意义。现有主流方法高度依赖关键点检测，但在面对结构对称和部分遮挡时表现不佳，需要新的方法提升检测鲁棒性与精度。

Method: 提出GKNet，这是一种利用关键点图几何约束的检测网络，同时配套发布了包含3个航天器、90,000张仿真图像及精确关键点标注的SKD数据集，并通过广泛实验和消融分析评估方法有效性。

Result: GKNet在关键点检测的准确性和有效性方面，相比主流的航天器关键点检测器有显著提升。实验和消融研究支持了其优越性。

Conclusion: GKNet作为基于图结构的关键点检测方法，对提升非合作航天器的单目位姿估计具有积极作用。SKD数据集的发布也为社区提供了重要资源。

Abstract: Monocular pose estimation of non-cooperative spacecraft is significant for
on-orbit service (OOS) tasks, such as satellite maintenance, space debris
removal, and station assembly. Considering the high demands on pose estimation
accuracy, mainstream monocular pose estimation methods typically consist of
keypoint detectors and PnP solver. However, current keypoint detectors remain
vulnerable to structural symmetry and partial occlusion of non-cooperative
spacecraft. To this end, we propose a graph-based keypoints network for the
monocular pose estimation of non-cooperative spacecraft, GKNet, which leverages
the geometric constraint of keypoints graph. In order to better validate
keypoint detectors, we present a moderate-scale dataset for the spacecraft
keypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000
simulated images, and corresponding high-precise keypoint annotations.
Extensive experiments and an ablation study have demonstrated the high accuracy
and effectiveness of our GKNet, compared to the state-of-the-art spacecraft
keypoint detectors. The code for GKNet and the SKD dataset is available at
https://github.com/Dongzhou-1996/GKNet.

</details>


### [34] [Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification](https://arxiv.org/abs/2507.11081)
*Chang Peng,Bao Yang,Meiqi Li,Ge Zhang,Hui Sun,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种结合3D GPR数据集和新型交叉验证策略的RSD自动识别方法，大幅提升了检测准确率并降低人工成本。


<details>
  <summary>Details</summary>
Motivation: 利用GPR进行道路病害检测虽高效但对人工判读依赖大，深度学习又受限于高质量数据的缺乏和网络区分能力不足，亟需提高自动检测准确性和数据集多样性。

Method: 建立含2134个多样化样本的经实地验证的3D GPR数据集，发现用YOLO模型对不同GPR扫描训练后对特定病害类型敏感性不同，便提出了一套新颖的交叉验证策略并集成到在线检测系统。

Result: 提出的方法在实地测试中召回率超过98.6%，在线检测系统能够减少约90%的人工巡查工作量。

Conclusion: 通过优质数据集和交叉验证策略相结合，成功实现高效高准确率的道路病害自动检测，显著提升行业检修效率和智能化水平。

Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive
solution for road subsurface distress (RSD) detection. However, RSD recognition
from GPR images is labor-intensive and heavily relies on inspectors' expertise.
Deep learning offers the possibility for automatic RSD recognition, but its
current performance is limited by two factors: Scarcity of high-quality dataset
for network training and insufficient capability of network to distinguish RSD.
In this study, a rigorously validated 3D GPR dataset containing 2134 samples of
diverse types was constructed through field scanning. Based on the finding that
the YOLO model trained with one of the three scans of GPR images exhibits
varying sensitivity to specific type of RSD, we proposed a novel
cross-verification strategy with outstanding accuracy in RSD recognition,
achieving recall over 98.6% in field tests. The approach, integrated into an
online RSD detection system, can reduce the labor of inspection by around 90%.

</details>


### [35] [Atmos-Bench: 3D Atmospheric Structures for Climate Insight](https://arxiv.org/abs/2507.11085)
*Tianchi Xu*

Main category: cs.CV

TL;DR: 本文提出了首个3D大气结构基准数据集Atmos-Bench，以及一种创新的基于物理约束的频率增强时空专家混合网络FourCastX，用于更精确地恢复卫星激光雷达体散射数据，显著提升了方法的标准性与精度。


<details>
  <summary>Details</summary>
Motivation: 目前大气结构的恢复方法普遍依赖辅助输入和简化物理近似，存在诸如评估标准不统一、难以捕捉真实辐射及散射吸收等问题，因此亟需标准化的高质量3D基准与更真实的还原方法来提升气候研究及极端天气预报的准确性。

Method: 论文构建了大规模模拟的3D大气散射体积数据集Atmos-Bench，并提出FourCastX模型，模型通过将物理约束嵌入网络架构，配合频率增强和时空混合专家机制，无需依赖额外辅助输入，直接从卫星激光雷达数据恢复高质量三维大气结构。

Result: FourCastX在Atmos-Bench数据集上，在355 nm和532 nm两个波段均取得了较当前最优方法更一致和更优的恢复表现，并有效保持能量一致性。

Conclusion: Atmos-Bench确立了3D大气结构恢复的新基准，FourCastX为卫星反演大气体积结构提供了更高的精度和可靠性，为深入气候理解与预报开启新方向。

Abstract: Atmospheric structure, represented by backscatter coefficients (BC) recovered
from satellite LiDAR attenuated backscatter (ATB), provides a volumetric view
of clouds, aerosols, and molecules, playing a critical role in human
activities, climate understanding, and extreme weather forecasting. Existing
methods often rely on auxiliary inputs and simplified physics-based
approximations, and lack a standardized 3D benchmark for fair evaluation.
However, such approaches may introduce additional uncertainties and
insufficiently capture realistic radiative transfer and atmospheric
scattering-absorption effects. To bridge these gaps, we present Atmos-Bench:
the first 3D atmospheric benchmark, along with a novel FourCastX:
Frequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)
generates 921,600 image slices from 3D scattering volumes simulated at 532 nm
and 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean
time steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC
physical constraints into the model architecture, promoting energy consistency
during restoration; (c) achieves consistent improvements on the Atmos-Bench
dataset across both 355 nm and 532 nm bands, outperforming state-of-the-art
baseline models without relying on auxiliary inputs. Atmos-Bench establishes a
new standard for satellite-based 3D atmospheric structure recovery and paves
the way for deeper climate insight.

</details>


### [36] [A Survey on Interpretability in Visual Recognition](https://arxiv.org/abs/2507.11099)
*Qiyang Wan,Chengzhi Gao,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: 本文系统回顾了视觉识别模型可解释性研究，提出了以人为中心的分类法，对相关方法进行细致梳理。


<details>
  <summary>Details</summary>
Motivation: 视觉识别模型虽然取得显著进展，但其应用于自动驾驶、医疗诊断等关键领域时，模型可解释性问题越来越重要。现有模型“黑箱”特性导致研究者和应用者难以理解和信任其决策过程，因此需要系统梳理和提升其可解释性。

Method: 论文对视觉识别模型的可解释性方法进行了系统性综述，并从以人为本的视角出发提出了新的分类法。该分类法从意图（Intent）、对象（Object）、呈现方式（Presentation）、方法论（Methodology）四个维度对可解释性方法进行分门别类。同时，论文还对当前评价指标需求进行了总结，并探讨了大规模多模态模型等新技术带来的机遇。

Result: 论文梳理了已有的可解释性方法，提出了一套更细致且系统的分类标准，对现有研究进行了有序归纳，并指明了可解释性研究的新机会和挑战。

Conclusion: 本文为视觉识别模型的可解释性研究提供了系统梳理与新分类体系，为今后的研究和实际应用提供了参考，旨在激发更多针对可解释性的新研究方向。

Abstract: In recent years, visual recognition methods have advanced significantly,
finding applications across diverse fields. While researchers seek to
understand the mechanisms behind the success of these models, there is also a
growing impetus to deploy them in critical areas like autonomous driving and
medical diagnostics to better diagnose failures, which promotes the development
of interpretability research. This paper systematically reviews existing
research on the interpretability of visual recognition models and proposes a
taxonomy of methods from a human-centered perspective. The proposed taxonomy
categorizes interpretable recognition methods based on Intent, Object,
Presentation, and Methodology, thereby establishing a systematic and coherent
set of grouping criteria for these XAI methods. Additionally, we summarize the
requirements for evaluation metrics and explore new opportunities enabled by
recent technologies, such as large multimodal models. We aim to organize
existing research in this domain and inspire future investigations into the
interpretability of visual recognition models.

</details>


### [37] [KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model](https://arxiv.org/abs/2507.11102)
*Jie Yang,Wang Zeng,Sheng Jin,Lumin Xu,Wentao Liu,Chen Qian,Zhen Li,Ruimao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新型多模态大语言模型KptLLM++，专为通用关键点理解而设计，在多个关键点检测基准上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型虽在图像理解中表现突出，但在细粒度语义、如对象关键点的精准识别与分析上表现不足。关键点对于细粒度分析、对象检索及行为识别等应用至关重要，因此亟需一种能有效处理关键点理解的新方法。

Method: 作者提出KptLLM++，通过用户指令引导整合多种输入模态，采用“先识别再检测”范式——先解释关键点语义，再通过结构化链式思维推理定位其精确位置。该模型利用包含50万以上样本、涵盖多类别对象、关键点类型、多风格图片和复杂遮挡场景的大规模数据集进行训练。

Result: KptLLM++在多个关键点检测公开基准（datasets）上取得了显著优于现有方法的准确率和泛化能力。

Conclusion: KptLLM++展示了作为细粒度图像理解统一方案的巨大潜力，有望推动人机协作及多模态理解的发展。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has revolutionized
image understanding by bridging textual and visual modalities. However, these
models often struggle with capturing fine-grained semantic information, such as
the precise identification and analysis of object keypoints. Keypoints, as
structure-aware, pixel-level, and compact representations of objects,
particularly articulated ones, play a crucial role in applications such as
fine-grained image analysis, object retrieval, and behavior recognition. In
this paper, we propose KptLLM++, a novel multimodal large language model that
specifically designed for generic keypoint comprehension through the
integration of diverse input modalities guided by user-defined instructions. By
unifying keypoint detection across varied contexts, KptLLM++ establishes itself
as an advanced interface, fostering more effective human-AI collaboration. The
model is built upon a novel identify-then-detect paradigm, which first
interprets keypoint semantics and subsequently localizes their precise
positions through a structured chain-of-thought reasoning mechanism. To push
the boundaries of performance, we have scaled up the training dataset to over
500K samples, encompassing diverse objects, keypoint categories, image styles,
and scenarios with complex occlusions. This extensive scaling enables KptLLM++
to unlock its potential, achieving remarkable accuracy and generalization.
Comprehensive experiments on multiple keypoint detection benchmarks demonstrate
its state-of-the-art performance, underscoring its potential as a unified
solution for fine-grained image understanding and its transformative
implications for human-AI interaction.

</details>


### [38] [Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach](https://arxiv.org/abs/2507.11116)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha,Mostofa Kamal Nasir*

Main category: cs.CV

TL;DR: 本研究提出了一种基于深度学习的水母物种检测与分类方法，在水下图像数据集上取得了98%的高准确率。


<details>
  <summary>Details</summary>
Motivation: 水母在维持海洋生态系统中起着重要作用，但由于其迅速增殖和生态影响，给生物多样性和保护带来了巨大挑战。因此，准确识别水母物种对于生态监测和管理十分重要。

Method: 研究提出了结合MobileNetV3、ResNet50、EfficientNetV2-B0和VGG16等先进特征提取器，与7种传统机器学习分类器和3种前馈神经网络分类器的深度学习框架。同时，在卷积神经网络模型中也直接使用softmax函数进行分类。

Result: 其中，人工神经网络与MobileNetV3的组合表现最佳，准确率高达98%，显著优于其他特征提取器-分类器组合。

Conclusion: 本研究证明了深度学习及混合框架在解决生物多样性挑战和推进海洋环境物种检测方面的高效性。

Abstract: Jellyfish, a diverse group of gelatinous marine organisms, play a crucial
role in maintaining marine ecosystems but pose significant challenges for
biodiversity and conservation due to their rapid proliferation and ecological
impact. Accurate identification of jellyfish species is essential for
ecological monitoring and management. In this study, we proposed a deep
learning framework for jellyfish species detection and classification using an
underwater image dataset. The framework integrates advanced feature extraction
techniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,
combined with seven traditional machine learning classifiers and three
Feedforward Neural Network classifiers for precise species identification.
Additionally, we activated the softmax function to directly classify jellyfish
species using the convolutional neural network models. The combination of the
Artificial Neural Network with MobileNetV3 is our best-performing model,
achieving an exceptional accuracy of 98%, significantly outperforming other
feature extractor-classifier combinations. This study demonstrates the efficacy
of deep learning and hybrid frameworks in addressing biodiversity challenges
and advancing species detection in marine environments.

</details>


### [39] [Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID](https://arxiv.org/abs/2507.11119)
*Hankun Liu,Yujian Zhao,Guanglin Niu*

Main category: cs.CV

TL;DR: 提出一种多模态引导的难样本生成与学习框架，用以提升换装行人重识别任务中的模型表现，显著增强对难样本的鲁棒性和识别能力。


<details>
  <summary>Details</summary>
Motivation: 换装行人重识别任务中，难样本因特征模糊或相似而显著影响模型优化与泛化能力，且缺乏明确的定义及专用处理策略。因此，需要针对性方法来生成和学习各类难样本，从而增强模型在衣物、视角变化下的鲁棒性。

Method: 提出HSGL框架，整合了文本和视觉信息：（1）双粒度难样本生成模块（DGHSG）根据多模态线索合成具有语义一致性的粗粒度、细粒度难正负样本，提升训练集多样性和难度；（2）难样本自适应学习模块（HSAL）基于文本语义标签，引入难度感知优化策略，自适应调整特征距离，优化嵌入空间表征能力。

Result: 在多个换装ReID基准数据集上进行了大量实验证明，所提方法有效提升了模型的识别能力和对难样本的鲁棒性。HSAL显著加快了目标学习的收敛速度，并在PRCC和LTCC两个数据集上获得了SOTA（最优）成绩。

Conclusion: 多模态引导的难样本显式生成与自适应学习推动了换装行人重识别的模型鲁棒性提升，验证了在训练阶段针对性难样本策略的显著价值，为相关领域提供了新思路。

Abstract: Hard samples pose a significant challenge in person re-identification (ReID)
tasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent
ambiguity or similarity, coupled with the lack of explicit definitions, makes
them a fundamental bottleneck. These issues not only limit the design of
targeted learning strategies but also diminish the model's robustness under
clothing or viewpoint changes. In this paper, we propose a novel
multimodal-guided Hard Sample Generation and Learning (HSGL) framework, which
is the first effort to unify textual and visual modalities to explicitly
define, generate, and optimize hard samples within a unified paradigm. HSGL
comprises two core components: (1) Dual-Granularity Hard Sample Generation
(DGHSG), which leverages multimodal cues to synthesize semantically consistent
samples, including both coarse- and fine-grained hard positives and negatives
for effectively increasing the hardness and diversity of the training data. (2)
Hard Sample Adaptive Learning (HSAL), which introduces a hardness-aware
optimization strategy that adjusts feature distances based on textual semantic
labels, encouraging the separation of hard positives and drawing hard negatives
closer in the embedding space to enhance the model's discriminative capability
and robustness to hard samples. Extensive experiments on multiple CC-ReID
benchmarks demonstrate the effectiveness of our approach and highlight the
potential of multimodal-guided hard sample generation and learning for robust
CC-ReID. Notably, HSAL significantly accelerates the convergence of the
targeted learning procedure and achieves state-of-the-art performance on both
PRCC and LTCC datasets. The code is available at
https://github.com/undooo/TryHarder-ACMMM25.

</details>


### [40] [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/abs/2507.11129)
*Zhifeng Gu,Bing Wang*

Main category: cs.CV

TL;DR: 提出了MMOne框架，用于高效统一地表示多模态场景，能处理不同模态间的冲突和扩展到更多模态。


<details>
  <summary>Details</summary>
Motivation: 在实际环境中，人类通过多种感知模态理解世界，但不同模态间的本质差异（如属性和粒度不一致）使得联合表示与理解变得困难。需要有效方法来协调并统一多模态信息，以获得更好的场景理解。

Method: 提出了一般性的多模态场景表示框架MMOne。核心做法包括：1) 设计了带有新颖模态指示器的模态建模模块，来捕捉每种模态的独特属性；2) 构建多模态高斯分解机制，将多模态高斯分布分离成单模态高斯；3) 解耦多模态信息为共享成分和特定成分，使得表示更紧凑高效。

Result: 大量实验证明，MMOne在多模式场景中显著提升了每个模态的表示能力，并且易于扩展到额外的模态。

Conclusion: MMOne能有效处理多模态间的冲突，统一和高效地表示场景，多模态信息表征能力增强，对新模态有良好扩展性。

Abstract: Humans perceive the world through multimodal cues to understand and interact
with the environment. Learning a scene representation for multiple modalities
enhances comprehension of the physical world. However, modality conflicts,
arising from inherent distinctions among different modalities, present two
critical challenges: property disparity and granularity disparity. To address
these challenges, we propose a general framework, MMOne, to represent multiple
modalities in one scene, which can be readily extended to additional
modalities. Specifically, a modality modeling module with a novel modality
indicator is proposed to capture the unique properties of each modality.
Additionally, we design a multimodal decomposition mechanism to separate
multi-modal Gaussians into single-modal Gaussians based on modality
differences. We address the essential distinctions among modalities by
disentangling multimodal information into shared and modality-specific
components, resulting in a more compact and efficient multimodal scene
representation. Extensive experiments demonstrate that our method consistently
enhances the representation capability for each modality and is scalable to
additional modalities. The code is available at
https://github.com/Neal2020GitHub/MMOne.

</details>


### [41] [RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images](https://arxiv.org/abs/2507.11143)
*Lam Pham,Cam Le,Hieu Tang,Khang Truong,Truong Nguyen,Jasmin Lampert,Alexander Schindler,Martin Boyer,Son Phan*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的端到端滑坡自动观测模型，能够利用遥感影像进行滑坡检测和分割任务，并在多个基准数据集上取得了优异的实验结果。


<details>
  <summary>Details</summary>
Motivation: 由于极端天气事件和人类活动等因素，滑坡灾害频发。而滑坡自动观测面临区域广阔、地形复杂等挑战，因此亟需高效的自动观测方法。

Method: 本文提出一种新型神经网络架构，处理遥感影像，实现滑坡检测和滑坡分割两项任务。该模型使用遥感影像作为输入，可以大范围、连续观测复杂地形。模型在LandSlide4Sense、Bijie、Nepal三大数据集上进行评测。

Result: 在LandSlide4Sense、Bijie数据集上的滑坡检测任务中，F1分数分别达到98.23和93.83；在LandSlide4Sense、Nepal数据集上的分割任务中，mIoU分别达到63.74和76.88。

Conclusion: 实验结果证明该模型在实际滑坡观测系统中具备良好应用潜力，有望实现滑坡的自动化和高效监测。

Abstract: In recent years, landslide disasters have reported frequently due to the
extreme weather events of droughts, floods , storms, or the consequence of
human activities such as deforestation, excessive exploitation of natural
resources. However, automatically observing landslide is challenging due to the
extremely large observing area and the rugged topography such as mountain or
highland. This motivates us to propose an end-to-end deep-learning-based model
which explores the remote sensing images for automatically observing landslide
events. By considering remote sensing images as the input data, we can obtain
free resource, observe large and rough terrains by time. To explore the remote
sensing images, we proposed a novel neural network architecture which is for
two tasks of landslide detection and landslide segmentation. We evaluated our
proposed model on three different benchmark datasets of LandSlide4Sense, Bijie,
and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,
93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU
scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,
Nepal datasets. These experimental results prove potential to integrate our
proposed model into real-life landslide observation systems.

</details>


### [42] [Latent Space Consistency for Sparse-View CT Reconstruction](https://arxiv.org/abs/2507.11152)
*Duoyou Chen,Yunqing Chen,Can Zhang,Zhou Wang,Cheng Chen,Ruoxiu Xiao*

Main category: cs.CV

TL;DR: 为了解决CT成像中的高辐射与耗时问题，本文提出了一种基于稀疏视角X射线重建CT的改进扩散模型（CLS-DM），并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: CT成像虽然应用广泛，但因为需要拍摄大量X射线而导致高耗时和高辐射，影响临床应用安全与效率。因此，如何用更少的X射线图像重建高质量的3D CT图像成为一个重要课题。

Method: 本文提出了一种新的扩散模型——Consistent Latent Space Diffusion Model（CLS-DM），通过跨模态特征对比学习，实现稀疏2D X射线图像和3D CT图像的潜在空间对齐，提升了重建效果。

Result: 在LIDC-IDRI和CTSpine1K数据集上，CLS-DM在PSNR、SSIM等体素级评价指标上，均超过了传统与最新的生成模型。

Conclusion: CLS-DM有效提升了稀疏X射线重建CT的质量，兼具经济性和实用性，并能推广到其它跨模态任务领域，如文本到图像生成。

Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical
settings. Using densely acquired rotational X-ray arrays, CT can capture 3D
spatial features. However, it is confronted with challenged such as significant
time consumption and high radiation exposure. CT reconstruction methods based
on sparse-view X-ray images have garnered substantial attention from
researchers as they present a means to mitigate costs and risks. In recent
years, diffusion models, particularly the Latent Diffusion Model (LDM), have
demonstrated promising potential in the domain of 3D CT reconstruction.
Nonetheless, due to the substantial differences between the 2D latent
representation of X-ray modalities and the 3D latent representation of CT
modalities, the vanilla LDM is incapable of achieving effective alignment
within the latent space. To address this issue, we propose the Consistent
Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature
contrastive learning to efficiently extract latent 3D information from 2D X-ray
images and achieve latent space alignment between modalities. Experimental
results indicate that CLS-DM outperforms classical and state-of-the-art
generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the
LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing
the effectiveness and economic viability of sparse X-ray reconstructed CT but
can also be generalized to other cross-modal transformation tasks, such as
text-to-image synthesis. We have made our code publicly available at
https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research
and applications in other domains.

</details>


### [43] [Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/abs/2507.11153)
*Hongfei Ye,Bin Chen,Wenxi Liu,Yu Zhang,Zhao Li,Dandan Ni,Hongyang Chen*

Main category: cs.CV

TL;DR: 本文关注大规模视觉-语言模型在色觉任务上的表现，提出了专门的测试集并分析模型错误，同时提出微调策略以提升其色觉能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模视觉-语言模型被广泛应用，但其色觉能力尚未被系统性研究。为此，作者认为有必要专门测试与提升该类模型的色觉表现。

Method: 1. 定义专门针对视觉-语言模型的色觉测试任务。
2. 构建覆盖多类别、不同难度的问题及任务的数据集。
3. 系统性分析模型在色觉任务中的错误类型。
4. 提出和实验微调策略提升模型色觉能力。

Result: 通过构建数据集及实验，揭示了主流大模型在色觉任务中的具体表现和易犯错误类型；验证了特定微调策略可提升模型的相关能力。

Conclusion: 当前大模型的色觉能力有限，专门设计的数据集和优化策略可以显著提升其在色觉测试中的表现，对视觉-语言模型未来发展具有指导意义。

Abstract: With the widespread adoption of large vision-language models, the capacity
for color vision in these models is crucial. However, the color vision
abilities of large visual-language models have not yet been thoroughly
explored. To address this gap, we define a color vision testing task for large
vision-language models and construct a dataset \footnote{Anonymous Github
Showing some of the data
https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers
multiple categories of test questions and tasks of varying difficulty levels.
Furthermore, we analyze the types of errors made by large vision-language
models and propose fine-tuning strategies to enhance their performance in color
vision tests.

</details>


### [44] [Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification](https://arxiv.org/abs/2507.11171)
*Jun Chen,Yonghua Yu,Weifu Li,Yaohui Chen,Hong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的柑橘病害图像自监督对比学习方法CMCRL，在无需大量标注样本的情况下，提升了病害检测和分类的准确率，并在公开数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 柑橘作为全球重要的经济作物，易受各类病害影响而导致减产。疾病的自动检测和分类对于精准防控非常关键，但现有深度学习方法依赖大量高质量标注样本，存在样本获取难、成本高的问题。急需一种高效处理未标注样本的方法。

Method: 本文提出了聚类引导自监督多层对比表示学习（CMCRL）算法，核心创新包括：与聚类中心对比的机制，以及多层次对比训练范式（MCT）。该方法能够针对未标注样本进行优化，有效学习层次化特征表征，并解决不同疾病间症状相似的问题。

Result: 在公开柑橘病害图像数据集CDD上，CMCRL方法的准确率比现有方法提高了4.5%-30.1%，性能接近全监督方法，并在F1分数、精度、召回率等多个指标上表现出优越性，尤其在类别不平衡条件下表现稳健。

Conclusion: 创新性自监督对比学习方法（CMCRL）可有效利用未标注数据，提高柑橘病害检测和分类的准确性和鲁棒性，对农业害病防控具有广阔应用前景。

Abstract: Citrus, as one of the most economically important fruit crops globally,
suffers severe yield depressions due to various diseases. Accurate disease
detection and classification serve as critical prerequisites for implementing
targeted control measures. Recent advancements in artificial intelligence,
particularly deep learning-based computer vision algorithms, have substantially
decreased time and labor requirements while maintaining the accuracy of
detection and classification. Nevertheless, these methods predominantly rely on
massive, high-quality annotated training examples to attain promising
performance. By introducing two key designs: contrasting with cluster centroids
and a multi-layer contrastive training (MCT) paradigm, this paper proposes a
novel clustering-guided self-supervised multi-layer contrastive representation
learning (CMCRL) algorithm. The proposed method demonstrates several advantages
over existing counterparts: (1) optimizing with massive unannotated samples;
(2) effective adaptation to the symptom similarity across distinct citrus
diseases; (3) hierarchical feature representation learning. The proposed method
achieves state-of-the-art performance on the public citrus image set CDD,
outperforming existing methods by 4.5\%-30.1\% accuracy. Remarkably, our method
narrows the performance gap with fully supervised counterparts (all samples are
labeled). Beyond classification accuracy, our method shows great performance on
other evaluation metrics (F1 score, precision, and recall), highlighting the
robustness against the class imbalance challenge.

</details>


### [45] [How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study](https://arxiv.org/abs/2507.11200)
*Che Liu,Jiazhen Pan,Weixiang Shen,Wenjia Bai,Daniel Rueckert,Rossella Arcucci*

Main category: cs.CV

TL;DR: 本文系统评估了开源通用和医学专用视觉-语言模型（VLMs）在多个医学视觉问答基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在自然图像相关任务上表现出色，并逐渐被尝试用于医疗领域，但它们在医学任务中的有效性尚未被全面检验。

Method: 挑选了参数量从3B到72B的多种开源VLMs，在8个医学相关视觉问答基准（如MedXpert、OmniMedVQA等）上进行评测，并将模型表现拆分为理解和推理两方面进行细致分析。

Result: 发现：（1）大规模通用模型在部分医学基准上已可媲美甚至超越医学专用模型；（2）推理性能普遍低于理解，成为安全医学决策支持的重要瓶颈；（3）不同基准的得分差异大，受任务设计、标注和知识需求影响明显。

Conclusion: 当前无论通用还是医学专用VLMs都未达到临床部署的可靠性门槛，亟需强化模态对齐和更严谨细致的评测标准。

Abstract: Vision-Language Models (VLMs) trained on web-scale corpora excel at natural
image tasks and are increasingly repurposed for healthcare; however, their
competence in medical tasks remains underexplored. We present a comprehensive
evaluation of open-source general-purpose and medically specialised VLMs,
ranging from 3B to 72B parameters, across eight benchmarks: MedXpert,
OmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model
performance across different aspects, we first separate it into understanding
and reasoning components. Three salient findings emerge. First, large
general-purpose models already match or surpass medical-specific counterparts
on several benchmarks, demonstrating strong zero-shot transfer from natural to
medical images. Second, reasoning performance is consistently lower than
understanding, highlighting a critical barrier to safe decision support. Third,
performance varies widely across benchmarks, reflecting differences in task
design, annotation quality, and knowledge demands. No model yet reaches the
reliability threshold for clinical deployment, underscoring the need for
stronger multimodal alignment and more rigorous, fine-grained evaluation
protocols.

</details>


### [46] [A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition](https://arxiv.org/abs/2507.11202)
*Xinkui Zhao,Jinsong Shu,Yangyang Wu,Guanjie Cheng,Zihe Liu,Naibo Wang,Shuiguang Deng,Zhongle Xie,Jianwei Yin*

Main category: cs.CV

TL;DR: 本文提出了MCULoRA框架，有效提升了在多模态信息不完整情景下的情感识别准确性。MCULoRA通过解耦和动态参数调整，克服了现有方法中多模态训练冲突的问题。


<details>
  <summary>Details</summary>
Motivation: 在实际多模态情感识别中，经常会遇到某些模态数据缺失（如传感器失效或隐私限制）。现有方法用附加梯度处理多模态缺失，但不同组合的训练梯度会相互冲突，影响模型最终表现。

Method: 作者提出了MCULoRA方法，包括两大模块：一是MCLA模块，能将多模态组合中的共享信息与各自特性有效解耦；二是DPFT模块，依据各模态特征空间的可分离性动态调整训练比例，从而优化模型在各种模态组合下的学习效率。整个框架实现了参数高效的多模态不完整学习。

Result: 在多个基准数据集上的实验证明，MCULoRA显著优于现有不完整多模态学习方法，提升了下游任务的准确率。

Conclusion: MCULoRA能有效解决多模态信息不完整带来的训练冲突问题，是一种高效的参数训练框架，对于实际场景下的多模态情感识别具有较大应用价值。

Abstract: Multimodal Emotion Recognition (MER) often encounters incomplete
multimodality in practical applications due to sensor failures or privacy
protection requirements. While existing methods attempt to address various
incomplete multimodal scenarios by balancing the training of each modality
combination through additional gradients, these approaches face a critical
limitation: training gradients from different modality combinations conflict
with each other, ultimately degrading the performance of the final prediction
model. In this paper, we propose a unimodal decoupled dynamic low-rank
adaptation method based on modality combinations, named MCULoRA, which is a
novel framework for the parameter-efficient training of incomplete multimodal
learning models. MCULoRA consists of two key modules, modality combination
aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The
MCLA module effectively decouples the shared information from the distinct
characteristics of individual modality combinations. The DPFT module adjusts
the training ratio of modality combinations based on the separability of each
modality's representation space, optimizing the learning efficiency across
different modality combinations. Our extensive experimental evaluation in
multiple benchmark datasets demonstrates that MCULoRA substantially outperforms
previous incomplete multimodal learning approaches in downstream task accuracy.

</details>


### [47] [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](https://arxiv.org/abs/2507.11245)
*X. Feng,H. Yu,M. Wu,S. Hu,J. Chen,C. Zhu,J. Wu,X. Chu,K. Huang*

Main category: cs.CV

TL;DR: 本论文提出了NarrLV，这是首个专门评估长视频生成模型叙事表达能力的基准，通过设计新颖的指标和自动化测试流程，为领域内模型的进一步研究和发展提供了重要工具。


<details>
  <summary>Details</summary>
Motivation: 现有长视频生成模型主要依赖于简单叙事提示的基准进行评估，无法有效反映模型在复杂叙事实境中的实际能力，亟需针对长视频叙事表达的综合评估基准。

Method: 1）受电影叙事理论启发，提出“时序叙事实元（TNA）”量化叙事丰富度，并构建自动化提示生成流程，使评估提示可灵活调整TNA数量。2）结合叙事内容表达的三个递进层级，设计基于多模态大语言模型（MLLM）的问答框架作为评测指标。3）系统性地在主流长视频生成模型上开展评测。

Result: 实验结果表明，所提出的叙事表达评测指标与人工评价结果高度一致，且通过评测揭示了当前视频生成模型在叙事内容表达方面的能力边界。

Conclusion: NarrLV基准和自动化评测流程能够有效、精确地评估长视频生成模型的叙事表达水平，为后续模型改进和研究提供了参考标准与方法。

Abstract: With the rapid development of foundation video generation technologies, long
video generation models have exhibited promising research potential thanks to
expanded content creation space. Recent studies reveal that the goal of long
video generation tasks is not only to extend video duration but also to
accurately express richer narrative content within longer videos. However, due
to the lack of evaluation benchmarks specifically designed for long video
generation models, the current assessment of these models primarily relies on
benchmarks with simple narrative prompts (e.g., VBench). To the best of our
knowledge, our proposed NarrLV is the first benchmark to comprehensively
evaluate the Narrative expression capabilities of Long Video generation models.
Inspired by film narrative theory, (i) we first introduce the basic narrative
unit maintaining continuous visual presentation in videos as Temporal Narrative
Atom (TNA), and use its count to quantitatively measure narrative richness.
Guided by three key film narrative elements influencing TNA changes, we
construct an automatic prompt generation pipeline capable of producing
evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based
on the three progressive levels of narrative content expression, we design an
effective evaluation metric using the MLLM-based question generation and
answering framework. (iii) Finally, we conduct extensive evaluations on
existing long video generation models and the foundation generation models.
Experimental results demonstrate that our metric aligns closely with human
judgments. The derived evaluation outcomes reveal the detailed capability
boundaries of current video generation models in narrative content expression.

</details>


### [48] [Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone](https://arxiv.org/abs/2507.11247)
*Veronika Shilova,Emmanuel Malherbe,Giovanni Palma,Laurent Risser,Jean-Michel Loubes*

Main category: cs.CV

TL;DR: 本文提出一种针对连续敏感属性（如肤色）的公平性分组方法，以发现和衡量隐藏或被掩盖的群体歧视，并通过在多个数据集上的实验证明了其效果和适用性。


<details>
  <summary>Details</summary>
Motivation: 现有公平性评估方法通常将敏感属性（如性别、肤色）划为几个预定义的离散群体，但当敏感属性是连续值时，这种分组方法可能遗漏对某些少数群体的歧视，无法精确捕捉歧视现象。因此，亟需探索新的分组和评估方法，更准确地反映并应对真实世界中复杂的歧视分布。

Method: 作者提出了一种新的基于公平性的分组方法，针对连续甚至是多维的敏感属性数据。该方法根据观测到的歧视程度将数据进行分组，优化一种基于群体之间差异的新判据，从而识别出最关键的、受影响最大的子群体。方法通过大量合成数据和实际人脸数据集（CelebA和FFHQ）验证，并应用工业算法对肤色等敏感特征进行预测。

Result: 实验结果显示，所提方法比现有方法能识别出更复杂、细腻的歧视模式，且在不同的数据集和模型间表现一致，具有稳健性。此外，该方法结合分组后处理，可以有效提升公平性，并对准确性影响很小。

Conclusion: 本文的方法能够在处理连续敏感属性时识别和度量更细致的歧视现象，有望推动公平性工具在工业界部署，实现精细化去偏和公平调整。

Abstract: Within a legal framework, fairness in datasets and models is typically
assessed by dividing observations into predefined groups and then computing
fairness measures (e.g., Disparate Impact or Equality of Odds with respect to
gender). However, when sensitive attributes such as skin color are continuous,
dividing into default groups may overlook or obscure the discrimination
experienced by certain minority subpopulations. To address this limitation, we
propose a fairness-based grouping approach for continuous (possibly
multidimensional) sensitive attributes. By grouping data according to observed
levels of discrimination, our method identifies the partition that maximizes a
novel criterion based on inter-group variance in discrimination, thereby
isolating the most critical subgroups.
  We validate the proposed approach using multiple synthetic datasets and
demonstrate its robustness under changing population distributions - revealing
how discrimination is manifested within the space of sensitive attributes.
Furthermore, we examine a specialized setting of monotonic fairness for the
case of skin color. Our empirical results on both CelebA and FFHQ, leveraging
the skin tone as predicted by an industrial proprietary algorithm, show that
the proposed segmentation uncovers more nuanced patterns of discrimination than
previously reported, and that these findings remain stable across datasets for
a given model. Finally, we leverage our grouping model for debiasing purpose,
aiming at predicting fair scores with group-by-group post-processing. The
results demonstrate that our approach improves fairness while having minimal
impact on accuracy, thus confirming our partition method and opening the door
for industrial deployment.

</details>


### [49] [MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection](https://arxiv.org/abs/2507.11252)
*Guanghao Wu,Chen Xu,Hai Song,Chong Wang,Qixing Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的森林火灾烟雾图像生成框架，能够合成高质量和多样性强的烟雾图像，用以提升烟雾检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 森林火灾早期监测对防灾减灾极为重要，但深度学习烟雾检测方法受限于真实烟雾图像数据稀缺，现有图像修复/生成模型生成的烟雾效果还无法与背景自然融合，导致合成数据质量不高。

Method: 作者提出了一套完整的烟雾图像生成方法，先利用预训练分割模型和多模态模型获得烟雾区域掩码和图像描述，再引入掩码及其特征引导的生成网络，通过设计新的损失函数（掩码随机差分损失）提升掩码边缘与生成结果的一致性，并结合大语言模型筛选优质合成图片，生成高质量的数据集。

Result: 实验表明，所生成的烟雾图像逼真、多样，有效提升了森林火灾烟雾检测的准确性。

Conclusion: 通过上述方法，作者生成了高质量森林火灾烟雾合成数据集，显著增强了后续烟雾检测模型的效果，为森林火灾早期监测提供了有力的数据和技术支持。

Abstract: Smoke is the first visible indicator of a wildfire.With the advancement of
deep learning, image-based smoke detection has become a crucial method for
detecting and preventing forest fires. However, the scarcity of smoke image
data from forest fires is one of the significant factors hindering the
detection of forest fire smoke. Image generation models offer a promising
solution for synthesizing realistic smoke images. However, current inpainting
models exhibit limitations in generating high-quality smoke representations,
particularly manifesting as inconsistencies between synthesized smoke and
background contexts. To solve these problems, we proposed a comprehensive
framework for generating forest fire smoke images. Firstly, we employed the
pre-trained segmentation model and the multimodal model to obtain smoke masks
and image captions.Then, to address the insufficient utilization of masks and
masked images by inpainting models, we introduced a network architecture guided
by mask and masked image features. We also proposed a new loss function, the
mask random difference loss, which enhances the consistency of the generated
effects around the mask by randomly expanding and eroding the mask
edges.Finally, to generate a smoke image dataset using random masks for
subsequent detection tasks, we incorporated smoke characteristics and use a
multimodal large language model as a filtering tool to select diverse and
reasonable smoke images, thereby improving the quality of the synthetic
dataset. Experiments showed that our generated smoke images are realistic and
diverse, and effectively enhance the performance of forest fire smoke detection
models. Code is available at https://github.com/wghr123/MFGDiffusion.

</details>


### [50] [ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition](https://arxiv.org/abs/2507.11261)
*Ronggang Huang,Haoxin Yang,Yan Cai,Xuemiao Xu,Huaidong Zhang,Shengfeng He*

Main category: cs.CV

TL;DR: 提出了一种新的3D视觉定位框架ViewSRD，通过多视角分解与多模态交互，有效提升了复杂场景中基于文本的3D物体定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法在处理复杂的多参照物查询以及因视角差异带来的空间描述不一致时表现不佳。需要一种能够更好分解和理解多锚点关系、适应多视角空间变化的方法。

Method: 提出ViewSRD框架，包含三个主要模块：首先用SRD模块将复杂查询拆分为多个针对单一锚点的陈述，获得结构化、具备视角区分的描述；然后通过Multi-TSI模块利用跨模态、一致性视角Token（CCVTs），在多视角下融合文本与场景特征，保留空间关系；最后Textual-Scene Reasoning模块将多视角预测整合为统一、鲁棒的3D定位结果。

Result: 在3D视觉定位公开数据集上的实验显示，ViewSRD在需要精确空间区分的复杂查询任务上，显著优于现有最新方法。

Conclusion: ViewSRD有效解决了多锚点与视角分歧带来的难题，为基于文本的3D定位提供了新的高效框架。

Abstract: 3D visual grounding aims to identify and localize objects in a 3D space based
on textual descriptions. However, existing methods struggle with disentangling
targets from anchors in complex multi-anchor queries and resolving
inconsistencies in spatial descriptions caused by perspective variations. To
tackle these challenges, we propose ViewSRD, a framework that formulates 3D
visual grounding as a structured multi-view decomposition process. First, the
Simple Relation Decoupling (SRD) module restructures complex multi-anchor
queries into a set of targeted single-anchor statements, generating a
structured set of perspective-aware descriptions that clarify positional
relationships. These decomposed representations serve as the foundation for the
Multi-view Textual-Scene Interaction (Multi-TSI) module, which integrates
textual and scene features across multiple viewpoints using shared, Cross-modal
Consistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a
Textual-Scene Reasoning module synthesizes multi-view predictions into a
unified and robust 3D visual grounding. Experiments on 3D visual grounding
datasets show that ViewSRD significantly outperforms state-of-the-art methods,
particularly in complex queries requiring precise spatial differentiation.

</details>


### [51] [YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery](https://arxiv.org/abs/2507.11267)
*Aon Safdar,Usman Akram,Waseem Anwar,Basit Malik,Mian Ibad Ali*

Main category: cs.CV

TL;DR: 论文提出了一种对热红外图像自动目标检测与识别任务（ATR）专门优化的YOLOv5s改进模型YOLOatr，在DSIAC MWIR数据集上达到了最高99.6%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习技术在用于国防和监控领域热红外图像的目标检测识别时表现不佳，原因在于该领域数据集有限、成像特性特殊、分辨率较低、与商业自动驾驶场景存在较大差异。因此，亟需为此领域设计专门优化的检测架构。

Method: 作者基于YOLOv5s，对检测头、特征融合部分（neck）及数据增强策略进行了针对性的定制和优化，提出了YOLOatr模型。

Result: 在DSIAC中波红外数据集上，通过多种测试协议，YOLOatr模型均取得了高精度检测和识别的结果，最高可达99.6%。

Conclusion: 针对热红外目标检测的特殊挑战，专门优化的YOLOatr结构能够实现当前最先进的性能，证明了定制化方法的有效性。

Abstract: Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared
(TI) imagery in the defense and surveillance domain is a challenging computer
vision (CV) task in comparison to the commercial autonomous vehicle perception
domain. Limited datasets, peculiar domain-specific and TI modality-specific
challenges, i.e., limited hardware, scale invariance issues due to greater
distances, deliberate occlusion by tactical vehicles, lower sensor resolution
and resultant lack of structural information in targets, effects of weather,
temperature, and time of day variations, and varying target to clutter ratios
all result in increased intra-class variability and higher inter-class
similarity, making accurate real-time ATR a challenging CV task. Resultantly,
contemporary state-of-the-art (SOTA) deep learning architectures underperform
in the ATR domain. We propose a modified anchor-based single-stage detector,
called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the
detection heads, feature fusion in the neck, and a custom augmentation profile.
We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR
dataset for real-time ATR over both correlated and decorrelated testing
protocols. The results demonstrate that our proposed model achieves
state-of-the-art ATR performance of up to 99.6%.

</details>


### [52] [Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping](https://arxiv.org/abs/2507.11279)
*Yujie Zhang,Sabine Struckmeyer,Andreas Kolb,Sven Reichardt*

Main category: cs.CV

TL;DR: 论文提出了TomatoMAP，一个基于物联网成像系统的西红柿全新表型数据集，并证明基于该数据集训练的AI模型在植物精细表型分析中与专家表现相当。


<details>
  <summary>Details</summary>
Motivation: 传统植物表型研究存在观察者偏差和不一致，限制了精细化植物分析的准确性和可复现性。

Method: 开发并发布了TomatoMAP数据集，包括64,464张多视角RGB图片，人工标注的7个感兴趣区域（ROI）、50类生长阶段标签，以及3,616张高分辨率像素级分割标注子集。用MobileNetv3、YOLOv11和MaskRCNN构建级联深度学习模型进行分类、检测与分割。通过专家与AI对比验证效果。

Result: AI模型在分类、检测和分割任务上达到与领域专家相当的精度和速度。Cohen’s Kappa和一致性热图表明AI判读的可靠性高。

Conclusion: TomatoMAP数据集和自动化AI表型管道可显著提升植物精细表型分析的准确性和标准化，具有可重复性和实用性优势。

Abstract: Observer bias and inconsistencies in traditional plant phenotyping methods
limit the accuracy and reproducibility of fine-grained plant analysis. To
overcome these challenges, we developed TomatoMAP, a comprehensive dataset for
Solanum lycopersicum using an Internet of Things (IoT) based imaging system
with standardized data acquisition protocols. Our dataset contains 64,464 RGB
images that capture 12 different plant poses from four camera elevation angles.
Each image includes manually annotated bounding boxes for seven regions of
interest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,
axillary shoot, shoot and whole plant area, along with 50 fine-grained growth
stage classifications based on the BBCH scale. Additionally, we provide 3,616
high-resolution image subset with pixel-wise semantic and instance segmentation
annotations for fine-grained phenotyping. We validated our dataset using a
cascading model deep learning framework combining MobileNetv3 for
classification, YOLOv11 for object detection, and MaskRCNN for segmentation.
Through AI vs. Human analysis involving five domain experts, we demonstrate
that the models trained on our dataset achieve accuracy and speed comparable to
the experts. Cohen's Kappa and inter-rater agreement heatmap confirm the
reliability of automated fine-grained phenotyping using our approach.

</details>


### [53] [3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images](https://arxiv.org/abs/2507.11293)
*J. Senthilnath,Chen Hao,F. C. Wellstood*

Main category: cs.CV

TL;DR: 本文提出了一种名为 3D MIR 的新方法，结合深度学习与物理优化，从磁场图像中精确恢复半导体封装内部电路的三维信息，实现无损检测。


<details>
  <summary>Details</summary>
Motivation: 半导体封装中的非破坏性检测需要精确恢复三维电流信息，以定位电路缺陷，现有方法在精度与应用可行性上存在不足。

Method: 3D MIR方法分为三步：首先用CNN从磁场图像预测导线相关参数并分类；其次依据空间-物理约束获得电流段位置、长度、电流值与方向的初始估计；最后用优化方法微调这些参数以匹配真实磁场图像。

Result: 实验表明该方法能高精度恢复半导体封装内三维电流信息，对磁场图像重建精度达到新的基准。

Conclusion: 3D MIR展示了深度学习与物理驱动优化结合的巨大潜力，为半导体封装检测提供了一种更精确、实用的新方案。

Abstract: In semiconductor packaging, accurately recovering 3D information is crucial
for non-destructive testing (NDT) to localize circuit defects. This paper
presents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),
which leverages Magnetic Field Images (MFI) to retrieve the parameters for the
3D current flow of a single-segment. The 3D MIR integrates a deep learning
(DL)-based Convolutional Neural Network (CNN), spatial-physics-based
constraints, and optimization techniques. The method operates in three stages:
i) The CNN model processes the MFI data to predict ($\ell/z_o$), where $\ell$
is the wire length and $z_o$ is the wire's vertical depth beneath the magnetic
sensors and classify segment type ($c$). ii) By leveraging
spatial-physics-based constraints, the routine provides initial estimates for
the position ($x_o$, $y_o$, $z_o$), length ($\ell$), current ($I$), and current
flow direction (positive or negative) of the current segment. iii) An optimizer
then adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\ell$, $I$) to
minimize the difference between the reconstructed MFI and the actual MFI. The
results demonstrate that the 3D MIR method accurately recovers 3D information
with high precision, setting a new benchmark for magnetic image reconstruction
in semiconductor packaging. This method highlights the potential of combining
DL and physics-driven optimization in practical applications.

</details>


### [54] [Detección y Cuantificación de Erosión Fluvial con Visión Artificial](https://arxiv.org/abs/2507.11301)
*Paúl Maji,Marlon Túquerres,Stalin Valencia,Marcela Valenzuela,Christian Mejia-Escobar*

Main category: cs.CV

TL;DR: 本文提出了一种基于人工智能的新方法，可自动检测和量化河流冲蚀区域，显著减少人工操作，并提升效率。研究利用YOLOv11模型结合照片和激光雷达数据进行训练，实现对冲蚀区域的自动分割和面积估算。最终开发出EROSCAN系统，便于用户在线识别和管理冲蚀风险。


<details>
  <summary>Details</summary>
Motivation: 传统的河流冲蚀检测方法依赖摄影测量和地理信息系统分析，需专业知识且人工流程繁琐，效率与精度受限。因此亟需自动化、高效和易用的新工具来监测和管理土壤冲蚀风险。

Method: 本研究采用前沿计算机视觉模型YOLOv11，通过微调训练，集成照片和LiDAR激光雷达影像作为数据源，并基于Roboflow平台完成数据标注和分割。系统自动识别河流冲蚀区域，并精确估算其面积。

Result: 实验结果表明，该方法对冲蚀模式具备约70％的识别准确率，能够精确定位受影响区域及其像素和实际面积（平方米）。开发完成了EROSCAN网络应用，实现用户图像上传后能自动输出冲蚀分割结果及面积估算。

Conclusion: 基于深度学习的EROSCAN系统显著优化了冲蚀检测与定量流程，降低使用门槛，提高监测效率，为风险管理和国土规划提供了有力工具。

Abstract: Fluvial erosion is a natural process that can generate significant impacts on
soil stability and strategic infrastructures. The detection and monitoring of
this phenomenon is traditionally addressed by photogrammetric methods and
analysis in geographic information systems. These tasks require specific
knowledge and intensive manual processing. This study proposes an artificial
intelligence-based approach for automatic identification of eroded zones and
estimation of their area. The state-of-the-art computer vision model YOLOv11,
adjusted by fine-tuning and trained with photographs and LiDAR images, is used.
This combined dataset was segmented and labeled using the Roboflow platform.
Experimental results indicate efficient detection of erosion patterns with an
accuracy of 70%, precise identification of eroded areas and reliable
calculation of their extent in pixels and square meters. As a final product,
the EROSCAN system has been developed, an interactive web application that
allows users to upload images and obtain automatic segmentations of fluvial
erosion, together with the estimated area. This tool optimizes the detection
and quantification of the phenomenon, facilitating decision making in risk
management and territorial planning.

</details>


### [55] [A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction](https://arxiv.org/abs/2507.11321)
*Haoxuan Qu,Yujun Cai,Hossein Rahmani,Ajay Kumar,Junsong Yuan,Jun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种可结合多种几何基元的全新高斯投影（Gaussian Splatting）表面重建框架，相比以往仅用单一基元，能更高质量地表示复杂多变的三维物体表面。


<details>
  <summary>Details</summary>
Motivation: 现有的高斯投影方法通常仅用单一类型的基元（如高斯椭圆或椭球）来重建表面，这在面对现实世界中复杂和多样的三维物体时表现不足，限制了表面的高质量表达。

Method: 作者提出了一个允许在高斯投影中组合多种基元进行表面重建的新框架。方法包括：1）组合型投影（compositional splatting）策略，实现不同基元的投影与渲染；2）基于多基元的初始化策略；3）顶点删减机制。这些设计能协同利用不同类型的基元提升表面表达能力。

Result: 通过大量实验验证，该方法在三维表面重建中表现出更优的准确性，能够高效重建复杂表面，明显优于只用单一基元的传统方法。

Conclusion: 将多种几何基元结合进高斯投影表面重建，有效提升了表面质量和复杂结构的表达能力，为三维重建领域提供了新的高效方案。

Abstract: Recently, Gaussian Splatting (GS) has received a lot of attention in surface
reconstruction. However, while 3D objects can be of complex and diverse shapes
in the real world, existing GS-based methods only limitedly use a single type
of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent
object surfaces during their reconstruction. In this paper, we highlight that
this can be insufficient for object surfaces to be represented in high quality.
Thus, we propose a novel framework that, for the first time, enables Gaussian
Splatting to incorporate multiple types of (geometrical) primitives during its
surface reconstruction process. Specifically, in our framework, we first
propose a compositional splatting strategy, enabling the splatting and
rendering of different types of primitives in the Gaussian Splatting pipeline.
In addition, we also design our framework with a mixed-primitive-based
initialization strategy and a vertex pruning mechanism to further promote its
surface representation learning process to be well executed leveraging
different types of primitives. Extensive experiments show the efficacy of our
framework and its accurate surface reconstruction performance.

</details>


### [56] [HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging](https://arxiv.org/abs/2507.11325)
*Arefin Ittesafun Abian,Ripon Kumar Debnath,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Asif Karim,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: 本文提出HANS-Net网络，结合超球卷积、时序注意力等多项创新方法，实现了肝脏及肿瘤在腹部CT图像中的高精度分割，在不同数据集上均表现出优异的准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 腹部CT中肝脏和肿瘤的自动分割对临床诊断和治疗方案制定至关重要，但受解剖结构复杂、肿瘤形态多变及标注数据稀缺等影响仍然具有挑战性。作者旨在提升分割精度与模型泛化能力，同时量化预测的不确定性。

Method: 提出HANS-Net，包括：1）利用超球卷积获取更具层次的几何表达，2）基于小波的分解模块实现多尺度纹理学习，3）仿生突触可塑性机制自适应增强特征，4）隐式神经表征建模连续细粒度边界，并引入不确定性感知的Monte Carlo dropout及轻量时序注意力提高切片间一致性和推理效率。

Result: 在LiTS数据库上，平均Dice为93.26%、IoU为88.09%、ASSD为0.72 mm、VOE为11.91%；在3D-IRCADb-01数据库跨集验证，平均Dice为87.45%、IoU为80.30%、ASSD为1.525 mm、VOE为19.71%。显示出卓越的分割精度和跨数据集泛化性。

Conclusion: HANS-Net能够有效提升肝脏与肿瘤的分割精度，能，鲁棒性强，在不同数据集下均能提供解剖一致、准确且带不确定性评估的分割结果。

Abstract: Accurate liver and tumor segmentation on abdominal CT images is critical for
reliable diagnosis and treatment planning, but remains challenging due to
complex anatomical structures, variability in tumor appearance, and limited
annotated data. To address these issues, we introduce Hyperbolic-convolutions
Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity
Network (HANS-Net), a novel segmentation framework that synergistically
combines hyperbolic convolutions for hierarchical geometric representation, a
wavelet-inspired decomposition module for multi-scale texture learning, a
biologically motivated synaptic plasticity mechanism for adaptive feature
enhancement, and an implicit neural representation branch to model fine-grained
and continuous anatomical boundaries. Additionally, we incorporate
uncertainty-aware Monte Carlo dropout to quantify prediction confidence and
lightweight temporal attention to improve inter-slice consistency without
sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate
that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an
average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap
error (VOE) of 11.91%. Furthermore, cross-dataset validation on the
3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of
1.525 mm, and VOE of 19.71%, indicating strong generalization across different
datasets. These results confirm the effectiveness and robustness of HANS-Net in
providing anatomically consistent, accurate, and confident liver and tumor
segmentation.

</details>


### [57] [MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network](https://arxiv.org/abs/2507.11333)
*Jianfei Jiang,Qiankun Liu,Haochen Yu,Hongyuan Liu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: 本文提出MonoMVSNet，通过引入单目深度先验和特征到多视图立体网络，有效提升了传统MVS方法在低纹理等困难区域的深度估计表现，实现了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多视图立体重建(MVS)方法在纹理较少或反光表面等区域，由于特征匹配困难，深度估计效果差。单目深度网络不依赖特征匹配，在这些区域表现更稳健。作者希望能够结合单目深度的优势，提升MVS整体鲁棒性和精度。

Method: 提出MonoMVSNet，将单目深度模型作为先验与传统MVS结合。具体方法包括：1）将参考图像的单目特征通过带有新设计的跨视角位置编码的注意力机制，融入源视图特征中；2）对采样过程中边缘区域的深度候选进行动态更新，对齐单目深度结果；3）设计了相对一致性损失，以单目深度监督多视图深度预测。

Result: 在DTU和Tanks-and-Temples主流数据集上，MonoMVSNet在多个评价基准上取得了新SOTA效果，尤其在Tanks-and-Temples Intermediate和Advanced榜单排名第一。

Conclusion: 融合单目深度模型先验的方法有效提升了多视图立体重建的性能，特别是在以往MVS方法难以处理的区域表现尤为突出。该框架具备良好推广性，为复杂场景的3D重建提供了新思路。

Abstract: Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for
a sequence of calibrated images to recover dense point clouds. However,
existing MVS methods often struggle with challenging regions, such as
textureless regions and reflective surfaces, where feature matching fails. In
contrast, monocular depth estimation inherently does not require feature
matching, allowing it to achieve robust relative depth estimation in these
regions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature
and depth guided MVS network that integrates powerful priors from a monocular
foundation model into multi-view geometry. Firstly, the monocular feature of
the reference view is integrated into source view features by the attention
mechanism with a newly designed cross-view position encoding. Then, the
monocular depth of the reference view is aligned to dynamically update the
depth candidates for edge regions during the sampling procedure. Finally, a
relative consistency loss is further designed based on the monocular depth to
supervise the depth prediction. Extensive experiments demonstrate that
MonoMVSNet achieves state-of-the-art performance on the DTU and
Tanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate
and Advanced benchmarks. The source code is available at
https://github.com/JianfeiJ/MonoMVSNet.

</details>


### [58] [UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks](https://arxiv.org/abs/2507.11336)
*Peiran Wu,Yunze Liu,Zhengdong Zhu,Enmin Zhou,Shawn Shen*

Main category: cs.CV

TL;DR: 本文提出了一个新的视频数据集UGC-VideoCap和一个参数量为3B的视频描述生成模型，用于提升对用户生成内容（UGC）短视频中音频与视觉的多模态细粒度理解和字幕生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕生成方法和数据集大都以视觉信息为核心，忽视了音频在场景解读和故事表达中的作用，导致模型无法很好地理解真实世界中复杂的多模态信息。

Method: 1. 构建UGC-VideoCap数据集：包含1000条TikTok用户生成视频，通过三阶段人工流程分别标注音频、视觉和联合音视频语义；并包含4000条QA问答以测评单模态与跨模态理解能力。
2. 提出UGC-VideoCaptioner (3B)模型：用Gemini 2.5 Flash蒸馏获得，通过两阶段训练（有监督微调+Group Relative Policy Optimization, GRPO）提升在少量数据下的适应与性能。

Result: 模型和新数据集在多模态理解及细粒度字幕生成任务中表现出竞争力，展现了数据高效和高质量的基础能力。

Conclusion: 该工作为真实用户生成内容的多模态视频字幕生成提供了新基准和高效能解决方案，推动了该方向的进一步发展。

Abstract: Real-world user-generated videos, especially on platforms like TikTok, often
feature rich and intertwined audio visual content. However, existing video
captioning benchmarks and models remain predominantly visual centric,
overlooking the crucial role of audio in conveying scene dynamics, speaker
intent, and narrative context. This lack of omni datasets and lightweight,
capable models hampers progress in fine grained, multimodal video
understanding. To address these challenges, we introduce UGC-VideoCap, a new
benchmark and model framework specifically designed for detailed omnimodal
captioning of short form user-generated videos. Unlike prior datasets,
UGC-VideoCap emphasizes balanced integration of audio and visual modalities,
featuring 1000 TikTok videos annotated through a structured three stage
human-in-the-loop pipeline covering audio only, visual only, and joint audio
visual semantics. The benchmark also includes 4000 carefully crafted QA pairs
probing both unimodal and cross modal understanding. Alongside the dataset, we
propose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from
Gemini 2.5 Flash. Using a novel two-stage training strategy supervised fine
tuning followed by Group Relative Policy Optimization (GRPO), our approach
enables efficient adaptation from limited data while maintaining competitive
performance. Together, our benchmark and model offer a high-quality foundation
and a data-efficient solution for advancing omnimodal video captioning in
unconstrained real-world UGC settings.

</details>


### [59] [Attributes Shape the Embedding Space of Face Recognition Models](https://arxiv.org/abs/2507.11372)
*Pierrick Leroy,Antonio Mastropietro,Marco Nurisso,Francesco Vaccarino*

Main category: cs.CV

TL;DR: 本文提出了一种新的几何度量方法，用于分析和衡量人脸识别模型在特征嵌入空间中对不同属性的依赖性与不变性，从而提升对深度学习模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于深度神经网络的人脸识别方法有很大进展，但现有方法在特征嵌入时只关注身份标签，忽视如头发颜色、对比度等其他可解释的面部或图像属性对嵌入空间结构的影响，因此亟需研究模型对这些属性的敏感性及其解释性。

Method: 作者提出了一种几何方法，结合物理启发的对齐度量指标，通过在受控模型和经过目标属性增强的合成数据上进行实验，衡量模型对不同人脸属性和图像属性的依赖性与不变性。

Result: 实验结果显示，不同人脸识别模型在不同属性上表现出不同程度的不变性。新引入的度量方法能够有效揭示模型对特定属性的强项和弱点，并提升模型的解释性。

Conclusion: 论文表明，基于几何和对齐度量的方法，能够深入分析和解释深度人脸识别模型的属性依赖性，将有助于未来对模型结构、数据增强和模型鲁棒性的优化。

Abstract: Face Recognition (FR) tasks have made significant progress with the advent of
Deep Neural Networks, particularly through margin-based triplet losses that
embed facial images into high-dimensional feature spaces. During training,
these contrastive losses focus exclusively on identity information as labels.
However, we observe a multiscale geometric structure emerging in the embedding
space, influenced by interpretable facial (e.g., hair color) and image
attributes (e.g., contrast). We propose a geometric approach to describe the
dependence or invariance of FR models to these attributes and introduce a
physics-inspired alignment metric. We evaluate the proposed metric on
controlled, simplified models and widely used FR models fine-tuned with
synthetic data for targeted attribute augmentation. Our findings reveal that
the models exhibit varying degrees of invariance across different attributes,
providing insight into their strengths and weaknesses and enabling deeper
interpretability. Code available here:
https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs

</details>


### [60] [Implementing Adaptations for Vision AutoRegressive Model](https://arxiv.org/abs/2507.11441)
*Kaif Shaikh,Antoni Kowalczuk,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: 本文针对近期提出的视觉自回归模型（VAR）适应性进行了详细评估，尤其是涉及差分隐私（DP）场景下的模型微调，并与主流扩散模型（DMs）方法对比。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在下游任务适应和差分隐私保护方面已有多种方法，VAR尚缺少此类系统的研究。作者希望填补VAR在实际适应和隐私保护领域的空白，验证其在各类微调任务，尤其是医疗数据生成等高敏感性任务中的表现。

Method: 作者实现并对比了多种VAR的适应性微调策略，包括非差分隐私（non-DP）和差分隐私（DP）两类，并将其表现与代表性的扩散模型适应性方法进行了系统性基准评测。

Result: 结果表明，在非差分隐私微调任务中，VAR表现优于扩散模型。但在差分隐私微调任务中，VAR性能下降明显，显示当前的DP方法对VAR适用性有限。

Conclusion: VAR在无隐私约束的下游任务适应性中优于扩散模型，但在差分隐私场景下还需进一步研究以提升其性能，是未来的重要研究方向。

Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative
to Diffusion Models (DMs) in image generation domain. In this work we focus on
its adaptations, which aim to fine-tune pre-trained models to perform specific
downstream tasks, like medical data generation. While for DMs there exist many
techniques, adaptations for VAR remain underexplored. Similarly, differentially
private (DP) adaptations-ones that aim to preserve privacy of the adaptation
data-have been extensively studied for DMs, while VAR lacks such solutions. In
our work, we implement and benchmark many strategies for VAR, and compare them
to state-of-the-art DM adaptation strategies. We observe that VAR outperforms
DMs for non-DP adaptations, however, the performance of DP suffers, which
necessitates further research in private adaptations for VAR. Code is available
at https://github.com/sprintml/finetuning_var_dp.

</details>


### [61] [COLI: A Hierarchical Efficient Compressor for Large Images](https://arxiv.org/abs/2507.11443)
*Haoran Wang,Hanyu Pei,Yang Lyu,Kai Zhang,Li Li,Feng-Lei Fan*

Main category: cs.CV

TL;DR: 本文提出了一种面向大图像高效压缩的新方法COLI，通过改进现有的隐式神经表示（INR）框架提升了压缩速度和比率，在医学影像等数据集上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着高分辨率、大视场图像应用的增多，现有压缩方法要么无法保留关键细节，要么泛化性差，因此亟需寻找既能高效压缩又细节保留能力强且应用范围广的新方法。

Method: 提出COLI方法，结合INR和NeRV，通过预训练-微调、混合精度训练及损失函数并行改进，加速模型收敛。同时提出Hyper-Compression后处理技术，在保证输出失真低的情况下进一步提升压缩比。

Result: 在两个医学成像数据集上，COLI实现了更低比特率下更好的PSNR与SSIM指标，并将NeRV训练速度提升至原来的4倍。

Conclusion: COLI在保持图像质量的前提下显著提升了大图像压缩效率和训练速度，是大规模高分辨率图像压缩方向的重要进展。

Abstract: The escalating adoption of high-resolution, large-field-of-view imagery
amplifies the need for efficient compression methodologies. Conventional
techniques frequently fail to preserve critical image details, while
data-driven approaches exhibit limited generalizability. Implicit Neural
Representations (INRs) present a promising alternative by learning continuous
mappings from spatial coordinates to pixel intensities for individual images,
thereby storing network weights rather than raw pixels and avoiding the
generalization problem. However, INR-based compression of large images faces
challenges including slow compression speed and suboptimal compression ratios.
To address these limitations, we introduce COLI (Compressor for Large Images),
a novel framework leveraging Neural Representations for Videos (NeRV). First,
recognizing that INR-based compression constitutes a training process, we
accelerate its convergence through a pretraining-finetuning paradigm,
mixed-precision training, and reformulation of the sequential loss into a
parallelizable objective. Second, capitalizing on INRs' transformation of image
storage constraints into weight storage, we implement Hyper-Compression, a
novel post-training technique to substantially enhance compression ratios while
maintaining minimal output distortion. Evaluations across two medical imaging
datasets demonstrate that COLI consistently achieves competitive or superior
PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while
accelerating NeRV training by up to 4 times.

</details>


### [62] [HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing](https://arxiv.org/abs/2507.11474)
*Pan Du,Mingqi Xu,Xiaozhi Zhu,Jian-xun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合NURBS曲面参数化与扩散生成模型的新方法（HUG-VAS），能够高精度合成具有复杂分支的主动脉血管几何结构，并优于传统统计形状建模方法。


<details>
  <summary>Details</summary>
Motivation: 传统统计形状建模方法因线性假设限制了对多分支复杂血管形态的表达能力，且难以扩展。亟需一种能生成真实感高、细致入微且适应复杂血管拓扑结构的新型建模方法。

Method: 文中提出HUG-VAS框架，将层次化NURBS参数化与扩散生成模型相结合。在21个病人特异性样本基础上训练，采用层次结构：首先用去噪扩散模型生成血管中心线，再用有条件引导的扩散模型生成径向剖面（取决于中心线），以此刻画血管的多层次解剖变异。此方法还支持利用影像先验进行零样本条件生成。

Result: HUG-VAS可合成具备超主动脉分支的精细血管几何，生成数据在生物标志物分布上与真实数据高度一致，支持即插即用的半自动分割、抗噪声重建及医疗器械设计优化等应用场景。

Conclusion: HUG-VAS是首个将影像先验和生成式形状建模通过统一的NURBS参数化与层次化扩散模型整合的SSM框架，突破了传统模型的局限，推动了医学图像分析与器械设计领域发展。

Abstract: Accurate characterization of vascular geometry is essential for
cardiovascular diagnosis and treatment planning. Traditional statistical shape
modeling (SSM) methods rely on linear assumptions, limiting their expressivity
and scalability to complex topologies such as multi-branch vascular structures.
We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular
geometry Synthesis, which integrates NURBS surface parameterization with
diffusion-based generative modeling to synthesize realistic, fine-grained
aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates
anatomically faithful aortas with supra-aortic branches, yielding biomarker
distributions that closely match those of the original dataset. HUG-VAS adopts
a hierarchical architecture comprising a denoising diffusion model that
generates centerlines and a guided diffusion model that synthesizes radial
profiles conditioned on those centerlines, thereby capturing two layers of
anatomical variability. Critically, the framework supports zero-shot
conditional generation from image-derived priors, enabling practical
applications such as interactive semi-automatic segmentation, robust
reconstruction under degraded imaging conditions, and implantable device
optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge
image-derived priors with generative shape modeling via a unified integration
of NURBS parameterization and hierarchical diffusion processes.

</details>


### [63] [C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images](https://arxiv.org/abs/2507.11476)
*Esteban Román Catafau,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 本文提出了一种名为3C-FBI的新算法，实现了在图像退化（如模糊）情况下的高效且精确的圆检测与拟合，并在多个实际和合成数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在计算机视觉领域，受噪声、模糊等退化影响的图像下，圆的检测与精确拟合始终是难点。现有方法在效率、鲁棒性和精度上无法兼顾。本文旨在提供一种能够在复杂环境下实现高精度、快速、抗干扰圆检测的新方法。

Method: 提出3C-FBI算法，它结合了高效的组合式边缘像素采样和基于卷积的参数空间密度估计。这种方法能够桥接传统检测与参数拟合的性能鸿沟。算法在帕金森病实际医学数据、标准合成基准、不同分辨率和异常值污染下都进行了全面实验评估。

Result: 3C-FBI在多个实验中展现出先进的准确性（如Jaccard指数0.896）、实时性（40.3 fps），且优于经典方法（如RCD为6.8 fps）。高分辨率下几乎完美准确（Jaccard几乎1.0），低分辨率及高异常值下也有较好鲁棒性。同时，在合成数据上与最新方法持平或超越。

Conclusion: 3C-FBI凭借兼具高精度、高速与高鲁棒，特别适用于医学影像、机器人、工业检验等复杂条件下的圆检测与拟合任务。

Abstract: This paper addresses the fundamental computer vision challenge of robust
circle detection and fitting in degraded imaging conditions. We present
Combinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an
algorithm that bridges the gap between circle detection and precise parametric
fitting by combining (1) efficient combinatorial edge pixel (edgel) sampling
and (2) convolution-based density estimation in parameter space.
  We evaluate 3C-FBI across three experimental frameworks: (1) real-world
medical data from Parkinson's disease assessments (144 frames from 36 videos),
(2) controlled synthetic data following established circle-fitting benchmarks,
and (3) systematic analysis across varying spatial resolutions and outlier
contamination levels. Results show that 3C-FBI achieves state-of-the-art
accuracy (Jaccard index 0.896) while maintaining real-time performance (40.3
fps), significantly outperforming classical methods like RCD (6.8 fps) on a
standard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost
1.0) at high resolutions (480x480) and reliable performance (Jaccard higher
than 0.95) down to 160x160 with up to 20% outliers.
  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989
across contamination levels, comparable to modern methods like Qi et al. (2024,
0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and
robustness makes 3C-FBI ideal for medical imaging, robotics, and industrial
inspection under challenging conditions.

</details>


### [64] [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](https://arxiv.org/abs/2507.11488)
*Pakizar Shamoi,Nuray Toganas,Muragul Muratbekova,Elnara Kadyrgali,Adilet Yerkin,Ayan Igali,Malika Ziyada,Ayana Adilova,Aron Karatayev,Yerdauit Torekhan*

Main category: cs.CV

TL;DR: 本文提出了一种基于模糊逻辑的人类感知色彩模型COLIBRI，通过大规模用户实验将计算机色彩表示与人类视觉感知更好地对齐。相较传统模型，该方法在实际人类感知上的表现更好。


<details>
  <summary>Details</summary>
Motivation: 当前计算机色彩模型难以准确模拟人类对颜色的主观感知，特别是在存在感知不确定性和模糊性的情况下。因此，迫切需要建立更符合人类视觉机制的色彩表示系统。

Method: 提出COLIBRI模型，采用模糊集和模糊逻辑描述颜色属性，通过三阶段实验（包括初步刺激测试和大规模人类分类调查，涉及2496名受试者），提取颜色的模糊分区和隶属度函数，并内置基于反馈和语境变化的自适应机制。

Result: 通过用户实验采集到大规模感知数据，基于这些数据建立了反映真实感知模糊性的色彩分类模型。与RGB、HSV、LAB等传统色彩模型相比，COLIBRI模型在与人类主观感知的一致性上表现更优。

Conclusion: COLIBRI模型首次在大样本基础上，实现了高度符合人类感知不确定性的色彩建模。该模型在设计、AI、市场营销和人机交互等对色彩感知有高要求的领域具有重要应用价值。

Abstract: Colors are omnipresent in today's world and play a vital role in how humans
perceive and interact with their surroundings. However, it is challenging for
computers to imitate human color perception. This paper introduces the Human
Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based
Representation and Interpretation), designed to bridge the gap between
computational color representations and human visual perception. The proposed
model uses fuzzy sets and logic to create a framework for color categorization.
Using a three-phase experimental approach, the study first identifies
distinguishable color stimuli for hue, saturation, and intensity through
preliminary experiments, followed by a large-scale human categorization survey
involving more than 1000 human subjects. The resulting data are used to extract
fuzzy partitions and generate membership functions that reflect real-world
perceptual uncertainty. The model incorporates a mechanism for adaptation that
allows refinement based on feedback and contextual changes. Comparative
evaluations demonstrate the model's alignment with human perception compared to
traditional color models, such as RGB, HSV, and LAB. To the best of our
knowledge, no previous research has documented the construction of a model for
color attribute specification based on a sample of this size or a comparable
sample of the human population (n = 2496). Our findings are significant for
fields such as design, artificial intelligence, marketing, and human-computer
interaction, where perceptually relevant color representation is critical.

</details>


### [65] [CATVis: Context-Aware Thought Visualization](https://arxiv.org/abs/2507.11522)
*Tariq Mehmood,Hamza Ahmad,Muhammad Haroon Shakeel,Murtaza Taj*

Main category: cs.CV

TL;DR: 本文提出了一种创新的五阶段框架，实现从脑电（EEG）信号中解码视觉表征并生成高质量的图像，取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: EEG信号含有丰富的大脑活动信息，但由于其本身复杂且噪音大，直接将脑电信号解码为具体视觉内容（如场景或图像）存在很大挑战。目前在脑-机接口应用中，关于视觉内容重建的效果有限，因此亟需新的方法提升其表现。

Method: 提出了五阶段的EEG视觉解码框架：（1）利用EEG编码器进行概念分类；（2）将EEG和文本嵌入对齐到CLIP特征空间，实现跨模态对齐；（3）通过再排序优化生成的文本描述；（4）对概念和文本嵌入加权插值，增强语义信息；（5）采用预训练的Stable Diffusion模型生成图像。核心在于充分利用多模态对齐和语义结合实现上下文感知的EEG到图像生成。

Result: 实验结果表明，该方法生成的图像与视觉刺激高度一致，各项指标显著优于SOTA：分类准确率提升13.43%，生成准确率提升15.21%，并将Fréchet Inception Distance（FID）降低36.61%，体现了更优的语义对齐和图像质量。

Conclusion: 该五阶段框架有效提升了EEG到图像的生成质量和语义对齐度，在脑-机接口视觉重建方向有重要意义，为未来通过脑信号直接“看见”大脑视觉内容提供了可行路径。

Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various
applications, such as motor imagery and cognitive state monitoring. However,
decoding visual representations from EEG signals remains a significant
challenge due to their complex and noisy nature. We thus propose a novel
5-stage framework for decoding visual representations from EEG signals: (1) an
EEG encoder for concept classification, (2) cross-modal alignment of EEG and
text embeddings in CLIP feature space, (3) caption refinement via re-ranking,
(4) weighted interpolation of concept and caption embeddings for richer
semantics, and (5) image generation using a pre-trained Stable Diffusion model.
We enable context-aware EEG-to-image generation through cross-modal alignment
and re-ranking. Experimental results demonstrate that our method generates
high-quality images aligned with visual stimuli, outperforming SOTA approaches
by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and
reducing Fr\'echet Inception Distance by 36.61%, indicating superior semantic
alignment and image quality.

</details>


### [66] [CharaConsist: Fine-Grained Consistent Character Generation](https://arxiv.org/abs/2507.11533)
*Mengyu Wang,Henghui Ding,Jianing Peng,Yao Zhao,Yunpeng Chen,Yunchao Wei*

Main category: cs.CV

TL;DR: 本文提出了一种名为CharaConsist的新方法，显著提升了文本生成图片时前景和背景的一致性，特别适合制作连续画面和多场景角色一致图像。


<details>
  <summary>Details</summary>
Motivation: 现有零训练增强角色一致性的方法往往无法维持背景细节一致，且前景出现大幅动作变化时易出现身份和服装细节不一致，限制了实际应用。

Method: 提出CharaConsist方法，核心技术包括点追踪注意力、动态token合并，并分别对前景和背景进行解耦控制，从而实现更细致的一致性；此外，CharaConsist针对DiT模型（Diffusion Transformer）做了适配。

Result: CharaConsist不仅能确保一个角色在连续镜头或不同场景下维持高一致性细节，还充分发挥了新一代基础模型的大容量优势，实现了高质量、高应用性的图片生成。

Conclusion: CharaConsist首次提供了适用于文本-图像DiT模型的一致性生成方法，有效拓展了文本生成图片技术在现实世界中的应用范围，相关代码已经开源。

Abstract: In text-to-image generation, producing a series of consistent contents that
preserve the same identity is highly valuable for real-world applications.
Although a few works have explored training-free methods to enhance the
consistency of generated subjects, we observe that they suffer from the
following problems. First, they fail to maintain consistent background details,
which limits their applicability. Furthermore, when the foreground character
undergoes large motion variations, inconsistencies in identity and clothing
details become evident. To address these problems, we propose CharaConsist,
which employs point-tracking attention and adaptive token merge along with
decoupled control of the foreground and background. CharaConsist enables
fine-grained consistency for both foreground and background, supporting the
generation of one character in continuous shots within a fixed scene or in
discrete shots across different scenes. Moreover, CharaConsist is the first
consistent generation method tailored for text-to-image DiT model. Its ability
to maintain fine-grained consistency, combined with the larger capacity of
latest base model, enables it to produce high-quality visual outputs,
broadening its applicability to a wider range of real-world scenarios. The
source code has been released at https://github.com/Murray-Wang/CharaConsist

</details>


### [67] [Streaming 4D Visual Geometry Transformer](https://arxiv.org/abs/2507.11539)
*Dong Zhuo,Wenzhao Zheng,Jiahe Guo,Yuqi Wu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出了一种流式4D视觉几何变换器（StreamVGGT），能实时高效地从视频数据中感知和重建4D时空几何，实现可扩展的交互式4D视觉系统。


<details>
  <summary>Details</summary>
Motivation: 传统4D时空几何重建耗时高、难以实时处理，限制了实际交互应用的发展。为此，亟需一种高效、低延迟的方法实现流式4D重建。

Method: 借鉴自回归大语言模型的设计，提出流式因果Transformer结构，对视频序列进行时序因果注意力处理，并缓存历史信息作为隐式记忆。模型以密集双向视觉几何Transformer为教师进行知识蒸馏，并可兼容如FlashAttention等高效注意力算子。

Result: 在多个4D几何感知基准上，所提方法显著提升在线推理速度，并在保持高性能的同时实现实时流式推理。

Conclusion: 流式4D视觉几何Transformer可为大规模、交互式4D视觉系统铺平道路，兼具高效性与竞争性的性能表现，具有实际应用前景。

Abstract: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a
fundamental yet challenging computer vision task. To facilitate interactive and
real-time applications, we propose a streaming 4D visual geometry transformer
that shares a similar philosophy with autoregressive large language models. We
explore a simple and efficient design and employ a causal transformer
architecture to process the input sequence in an online manner. We use temporal
causal attention and cache the historical keys and values as implicit memory to
enable efficient streaming long-term 4D reconstruction. This design can handle
real-time 4D reconstruction by incrementally integrating historical information
while maintaining high-quality spatial consistency. For efficient training, we
propose to distill knowledge from the dense bidirectional visual geometry
grounded transformer (VGGT) to our causal model. For inference, our model
supports the migration of optimized efficient attention operator (e.g.,
FlashAttention) from the field of large language models. Extensive experiments
on various 4D geometry perception benchmarks demonstrate that our model
increases the inference speed in online scenarios while maintaining competitive
performance, paving the way for scalable and interactive 4D vision systems.
Code is available at: https://github.com/wzzheng/StreamVGGT.

</details>


### [68] [Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation](https://arxiv.org/abs/2507.11540)
*Zhen Xu,Hongyu Zhou,Sida Peng,Haotong Lin,Haoyu Guo,Jiahao Shao,Peishan Yang,Qinglin Yang,Sheng Miao,Xingyi He,Yifan Wang,Yue Wang,Ruizhen Hu,Yiyi Liao,Xiaowei Zhou,Hujun Bao*

Main category: cs.CV

TL;DR: 该论文综述了深度学习在多种深度估计算法及其架构上的进展，重点关注如何通过大规模数据集和强泛化能力来解决传统深度估计方法的局限。


<details>
  <summary>Details</summary>
Motivation: 传统深度估计方法主要依赖于如LiDAR等硬件传感器，成本高、分辨率低、受环境影响大，限制了在现实世界中的应用。基于视觉的方法虽然有望替代，但小模型容量和小规模、特定领域数据集导致泛化和稳定性不佳。需要探究能广泛泛化且鲁棒的深度估计算法。

Method: 综述了深度学习架构（如单目、双目、多视角及单目视频）在深度估计中的应用，重点考察了随着大模型和基础模型思路的兴起，如何通过训练在大规模数据集上提升算法的零样本泛化能力。清点和评述了相关的大规模深度数据集，并分析不同架构与训练策略的优劣。

Result: 针对各类方法和数据集进行了系统性梳理，总结了推动深度泛化模型发展的关键架构和训练方式。指出了当前模型的能力、局限及未来方向。

Conclusion: 该综述论文呼吁构建具备强泛化能力与鲁棒性的深度基础模型，同时明确了未来研究的重点路径及实际应用场景。强调大数据与高效模型架构对深度学习在深度估计领域的重要性。

Abstract: Depth estimation is a fundamental task in 3D computer vision, crucial for
applications such as 3D reconstruction, free-viewpoint rendering, robotics,
autonomous driving, and AR/VR technologies. Traditional methods relying on
hardware sensors like LiDAR are often limited by high costs, low resolution,
and environmental sensitivity, limiting their applicability in real-world
scenarios. Recent advances in vision-based methods offer a promising
alternative, yet they face challenges in generalization and stability due to
either the low-capacity model architectures or the reliance on domain-specific
and small-scale datasets. The emergence of scaling laws and foundation models
in other domains has inspired the development of "depth foundation models":
deep neural networks trained on large datasets with strong zero-shot
generalization capabilities. This paper surveys the evolution of deep learning
architectures and paradigms for depth estimation across the monocular, stereo,
multi-view, and monocular video settings. We explore the potential of these
models to address existing challenges and provide a comprehensive overview of
large-scale datasets that can facilitate their development. By identifying key
architectures and training strategies, we aim to highlight the path towards
robust depth foundation models, offering insights into their future research
and applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [69] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
*Logé Cécile,Ghori Rehan*

Main category: cs.CL

TL;DR: 该论文提出了一种AI驱动的系统，用于在YouTube平台检测视频内容中的虚假信息，并在评论区主动与用户互动，挑战误导性叙述。系统由两个智能体组成：一个负责事实核查，另一个在评论区引导讨论，旨在提升网络平台的信息可信度。


<details>
  <summary>Details</summary>
Motivation: 随着虚假信息在数字平台（如YouTube）上的快速传播，传统的人工审核难以应对其规模和速度。因此亟需自动化、智能化方法来有效检测和应对虚假信息，维护信息环境的健康。

Method: 系统包含Truth Sleuth和Trend Bender两个智能体。Truth Sleuth使用基于检索增强生成（RAG）的方法，从YouTube视频中抓取声明并利用Wikipedia、Google Search、Google FactCheck等信息源进行核查，生成详细报告。Trend Bender则结合报告和相关文章，通过精心设计的Prompt，在评论区生成有说服力的回复，引发理性讨论，并通过自我评估机制提升输出质量。

Result: 在标准数据集和YouTube真实环境中测试，系统展现出较高的事实核查准确率，并能够有效影响评论区讨论氛围，引导用户参与有益讨论。

Conclusion: AI驱动的系统能够提升虚假信息检测和应对的效率，促进网络平台的理性讨论和信息透明性。该方法在实际应用中有着广泛潜力，有助于打造更健康的网络信息生态。

Abstract: Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.

</details>


### [70] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Main category: cs.CL

TL;DR: 本文提出了一种完全离线运行于智能手机上的情感支持对话应用EmoSApp，旨在克服现有数字心理健康支持平台在可访问性、网络依赖和隐私等方面的挑战。该系统通过在自定义的心理健康数据集上微调和量化LLaMA系列大模型，实现了在资源受限设备上的本地推理。评估表明其具备高效、同理且相关性强的对话能力。


<details>
  <summary>Details</summary>
Motivation: 目前大量基于互联网的心理健康支持平台存在用户访问受限、网络条件要求高以及数据隐私难以保障等问题。因此，开发一种无需联网，既安全又易于普及的心理健康支持工具成为迫切需求。

Method: 作者开发了EmoSApp，将大语言模型（基于LLaMA-3.2-1B-Instruct）利用Torchtune和Executorch进行量化和本地化部署，并在包含14582条心理健康问答和多轮会话的数据集上进行微调。评估包括定性人工评审和定量的通用常识及推理基准测试。

Result: 应用在学生群体中的质性人工测试显示，EmoSApp能够连贯和同理性地对话，针对心理健康问题给出相关建议；在9个通用推理基准上量化评测显示，该离线模型在低资源环境下依然表现优良。

Conclusion: EmoSApp验证了基于端侧部署和领域专用调优的可行性和有效性，为未来便携、安全且高度定制的人工智能心理健康支持系统提供了技术蓝本。

Abstract: Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [71] [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)
*Anders Ledberg,Anna Thalén*

Main category: cs.CL

TL;DR: 本文提出了一种模块化工具链，能够在保护隐私的前提下，对法律、医疗等领域的非结构化敏感文本进行标准化、匿名化和嵌入表示的处理，从而便于大规模分析和后续研究。该工具链基于开源模型，本地可运行，利用大语言模型消除异构性和隐私泄露风险。验证实验说明其在保持语义的同时有效去标识化，示例应用也展示了自动化内容分析的能力。


<details>
  <summary>Details</summary>
Motivation: 当前公共健康和社会科学等领域拥有大量非结构化文本数据（如法律、医疗、行政文档），蕴藏丰富研究价值，但因包含敏感隐私信息以及结构和语言差异巨大，导致难以规模化分析。现有主流解决方案并不完美，既缺乏隐私保护也不易本地部署，因此亟需能够处理、匿名化并标准化此类数据的新方法。

Method: 作者设计了一套模块化工具链，完全基于开源大语言模型，可在本地GPU运行，分为三个主要流程：1）利用LLM标准化、摘要化、必要时翻译文本至英文，消除结构语言异构性；2）通过LLM、命名实体识别和规则结合方法实现文本匿名化；3）将文本转换为文档级嵌入用于后续分析。作者在一组现实法律判决数据上进行了系统性实验与人工验证。

Result: 实验数据为10,842份瑞典法院裁决（超过56,000页），工具链能有效输出匿名化、标准化的摘要和文档嵌入。多重验证（人工审核、自动扫描、预测评估）显示，该方法在有效保护隐私的同时，较好保留了语义信息。此外，通过利用少量手动标注的摘要和嵌入向量，作者训练了预测模型，展示了其在大规模内容分析中的应用潜力。

Conclusion: 该工具链为隐私敏感的非结构化文本处理与大规模分析开启了新途径，尤其适用于原本因隐私与异构性障碍难以利用的数据。其本地可运行、模块化的特性，兼顾了安全性与操作便利性，将促进公共健康、社会科学等领域的数据分析研究。

Abstract: Unstructured text from legal, medical, and administrative sources offers a
rich but underutilized resource for research in public health and the social
sciences. However, large-scale analysis is hampered by two key challenges: the
presence of sensitive, personally identifiable information, and significant
heterogeneity in structure and language. We present a modular toolchain that
prepares such text data for embedding-based analysis, relying entirely on
open-weight models that run on local hardware, requiring only a
workstation-level GPU and supporting privacy-sensitive research.
  The toolchain employs large language model (LLM) prompting to standardize,
summarize, and, when needed, translate texts to English for greater
comparability. Anonymization is achieved via LLM-based redaction, supplemented
with named entity recognition and rule-based methods to minimize the risk of
disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court
decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.
Each document is processed into an anonymized, standardized summary and
transformed into a document-level embedding. Validation, including manual
review, automated scanning, and predictive evaluation shows the toolchain
effectively removes identifying information while retaining semantic content.
As an illustrative application, we train a predictive model using embedding
vectors derived from a small set of manually labeled summaries, demonstrating
the toolchain's capacity for semi-automated content analysis at scale.
  By enabling structured, privacy-conscious analysis of sensitive documents,
our toolchain opens new possibilities for large-scale research in domains where
textual data was previously inaccessible due to privacy and heterogeneity
constraints.

</details>


### [72] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
*Isar Nejadgholi,Mona Omidyeganeh,Marc-Antoine Drouin,Jonathan Boisvert*

Main category: cs.CL

TL;DR: 该论文提出了一个新的XAI（可解释人工智能）分类法，专门用于分析和改进基于自然语言解释（NLEs）的AI治理。


<details>
  <summary>Details</summary>
Motivation: 当前大模型和自然语言解释能力提升，使得AI系统行为的解释和验证变得重要。但现有方法和理论未能覆盖基于提示的自然语言解释NLEs的特点和治理需求。

Method: 作者借鉴已有的XAI研究，针对NLEs提出了三维度的分类体系：1）上下文（任务、数据、受众和目标），2）生成与展现（生成方法、输入、交互、输出和表现形式），3）评估（内容、表现与用户中心属性，以及评估环境）。

Result: 该分类法为研究者、审计员和政策制定者提供了一个评估和设计透明AI系统NLEs的结构化工具。论文展示了其如何支持对自然语言解释的分类与改进。

Conclusion: 通过本分类法，可以系统地分析并指导透明AI系统中NLEs的开发和治理，加强AI系统的可解释性和可信性。

Abstract: Effective AI governance requires structured approaches for stakeholders to
access and verify AI system behavior. With the rise of large language models,
Natural Language Explanations (NLEs) are now key to articulating model
behavior, which necessitates a focused examination of their characteristics and
governance implications. We draw on Explainable AI (XAI) literature to create
an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:
(1) Context, including task, data, audience, and goals; (2) Generation and
Presentation, covering generation methods, inputs, interactivity, outputs, and
forms; and (3) Evaluation, focusing on content, presentation, and user-centered
properties, as well as the setting of the evaluation. This taxonomy provides a
framework for researchers, auditors, and policymakers to characterize, design,
and enhance NLEs for transparent AI systems.

</details>


### [73] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
*Kaushik Dwivedi,Padmanabh Patanjali Mishra*

Main category: cs.CL

TL;DR: 本文提出了AutoRAG-LoRA框架，通过结合轻量级LoRA适配器与KL正则化训练，集成自动化提示重写、混合检索和低秩微调，有效减少大语言模型中的幻觉问题，同时维持模型效率与模块化。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然语言任务中表现优秀，但其幻觉（生成事实错误信息）问题依然严重，限制了其在实际场景中的可信度和部署。因此亟需开发有效方法来增强其事实准确性。

Method: 提出AutoRAG-LoRA框架，将LoRA适配器与KL正则训练结合，设计自动化的提示重写和混合检索机制，利用外部证据支撑回答。引入包含分类器和自评的幻觉检测组件，根据信心评分触发反馈校正环，通过反差KL损失和适配器微调约束事实一致性。

Result: 实验证明该框架能显著减少大语言模型的事实漂移（幻觉），同时保持了模型的高效率和可模块化特性。

Conclusion: AutoRAG-LoRA为解决大模型幻觉问题提供了一种高效、模块化且易于集成的RAG新范式，为语言模型的实际可靠部署带来新的可能。

Abstract: Large Language Models (LLMs) have demonstrated remarkable fluency across a
range of natural language tasks, yet remain vulnerable to hallucinations -
factual inaccuracies that undermine trust in real world deployment. We present
AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that
tackles hallucination in large language models through lightweight LoRA-based
adapters and KL-regularized training. Our pipeline integrates automated prompt
rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in
retrieved evidence. A hallucination detection module, using both
classifier-based and self-evaluation techniques, assigns confidence scores to
generated outputs, triggering an optional feedback correction loop. This loop
enforces factual alignment via contrastive KL loss and adapter fine tuning. We
demonstrate that AutoRAG-LoRA significantly reduces the factual drift while
preserving the efficiency and modularity of the model.

</details>


### [74] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
*Dennis Ulmer,Alexandra Lorson,Ivan Titov,Christian Hardmeier*

Main category: cs.CL

TL;DR: 本论文讨论了在大型语言模型（LLMs）中通过语言表达不确定性的重要性，强调了模仿人类语言中表达不确定性的方式对于提高人机交互信任度的作用。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在输出内容时经常过于自信，即便结果可能不准确，这降低了用户对模型的信任。因此，需要有效传达模型的置信度，以提升人机协作效果并减少潜在危害。

Method: 作者综述了人类表达不确定性的相关研究，调研了自然语言处理中不确定性表达的话题，并通过额外分析揭示了当前数据在表达不确定性上的偏见。

Result: 论文发现当前NLP领域较少关注人类不确定性表达的细腻差异及其数据偏见，指出了在语言模型中仿人类表达不确定性可提升交互可信度。

Conclusion: 作者提出“类人不确定性”（anthropomimetic uncertainty）概念，主张借鉴真实人类的语言表达方式，通过个性化和真实性提升人机沟通中的不确定性表达，并为未来NLP研究指出了方向。

Abstract: Human users increasingly rely on natural language interactions with large
language models (LLMs) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of LLMs is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty communication and the data
biases that influence machine uncertainty communication. We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
communication requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human communication. We
present a thorough overview over the research in human uncertainty
communication, survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine communication of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.

</details>


### [75] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
*Yogachandran Rahulamathavan,Misbah Farooq,Varuna De Silva*

Main category: cs.CL

TL;DR: 本文提出一种无需扰动的新型本地可解释方法PLEX，通过利用LLM上下文嵌入和Siamese网络，有效地解释文本分类决策，显著加快解释速度且与现有LIME/SHAP一致性高。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然文本分类能力很强，但决策过程难以解释。现有如LIME、SHAP等XAI方法虽然能找出关键特征词，但需要大量生成扰动样本和逐个推理，计算代价极高，特别是在大型模型上。研究动机是如何减少解释LLM预测时的计算复杂度，实现高效、准确的可解释性。

Method: 提出了一种名为PLEX（Perturbation-free Local Explanation）的新方法，核心思想是结合LLM输出的上下文嵌入和Siamese网络，网络训练用于对齐特征重要性评分。经过一次性训练后，无需对新输入进行扰动和多次推理，显著提升解释效率。

Result: 在情感、假新闻、假COVID-19新闻和抑郁检测四项任务上，PLEX与LIME/SHAP的结果一致性超过92%。压力测试显示，PLEX能精准识别关键影响词，移除这些词导致的准确率下降与LIME/SHAP相当，且部分场景下表现更优。解释速度上，PLEX比传统方法快2-4个数量级。

Conclusion: PLEX无需传统扰动，极大提升了解释文本分类模型预测的效率，解释效果与业界主流可解释方法高度一致甚至更优，为LLM模型可解释性应用提供了高效实用的解决方案。

Abstract: Large Language Models (LLMs) excel in text classification, but their
complexity hinders interpretability, making it difficult to understand the
reasoning behind their predictions. Explainable AI (XAI) methods like LIME and
SHAP offer local explanations by identifying influential words, but they rely
on computationally expensive perturbations. These methods typically generate
thousands of perturbed sentences and perform inferences on each, incurring a
substantial computational burden, especially with LLMs. To address this, we
propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation
(PLEX), a novel method that leverages the contextual embeddings extracted from
the LLM and a ``Siamese network" style neural network trained to align with
feature importance scores. This one-off training eliminates the need for
subsequent perturbations, enabling efficient explanations for any new sentence.
We demonstrate PLEX's effectiveness on four different classification tasks
(sentiment, fake news, fake COVID-19 news and depression), showing more than
92\% agreement with LIME and SHAP. Our evaluation using a ``stress test"
reveals that PLEX accurately identifies influential words, leading to a similar
decline in classification accuracy as observed with LIME and SHAP when these
words are removed. Notably, in some cases, PLEX demonstrates superior
performance in capturing the impact of key features. PLEX dramatically
accelerates explanation, reducing time and computational overhead by two and
four orders of magnitude, respectively. This work offers a promising solution
for explainable LLM-based text classification.

</details>


### [76] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
*Bo Zhao,Maya Okawa,Eric J. Bigelow,Rose Yu,Tomer Ullman,Ekdeep Singh Lubana,Hidenori Tanaka*

Main category: cs.CL

TL;DR: 本文分析了大语言模型（LLMs）在处理情感状态时的分层结构，发现其情感推理接近于人类心理模型，但存在群体偏差。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地用于对话系统，理解其如何建模用户情感状态对于伦理部署至关重要，尤其是关注群体偏误和心理模型的内在一致性。

Method: 借鉴心理学的情感轮理论，分析LLMs输出中情感状态的概率依赖性，研究情感状态的层次关系，并对不同社会经济背景角色进行情感识别测试，还对比人类用户的表现。

Result: LLMs输出中自发形成层次化的情感树，与人类心理学模型一致，且规模更大的模型表现更复杂的层次结构。同时发现LLMs在情感识别上存在对部分群体的系统性偏差，这些偏差在人类实验中也有反映。

Conclusion: LLMs具备一定的情感推理能力，并反映人类社会感知的某些特征，但存在对特定边缘化群体的识别误差。心理学理论可用于优化模型评估和减轻偏差。

Abstract: As large language models (LLMs) increasingly power conversational agents,
understanding how they model users' emotional states is critical for ethical
deployment. Inspired by emotion wheels -- a psychological framework that argues
emotions organize hierarchically -- we analyze probabilistic dependencies
between emotional states in model outputs. We find that LLMs naturally form
hierarchical emotion trees that align with human psychological models, and
larger models develop more complex hierarchies. We also uncover systematic
biases in emotion recognition across socioeconomic personas, with compounding
misclassifications for intersectional, underrepresented groups. Human studies
reveal striking parallels, suggesting that LLMs internalize aspects of social
perception. Beyond highlighting emergent emotional reasoning in LLMs, our
results hint at the potential of using cognitively-grounded theories for
developing better model evaluations.

</details>


### [77] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
*Nickolas Freeman,Thanh Nguyen,Gregory Bott,Jason Parton,Collin Francel*

Main category: cs.CL

TL;DR: 本文探讨如何更有效地分析成人服务网站（ASW）中的广告文本，以协助识别和打击性交易行为，重点在于定制变换器模型的开发及其应用。


<details>
  <summary>Details</summary>
Motivation: 由于性交易广告常用模糊语言、表情符号及故意的语法错误，现有方法难以有效识别和分析这些文本。因此，提升ASW文本的自动化分析能力对打击性交易有重大意义。

Method: 作者系统评估了基于ASW数据的多种文本建模方法，包括信息检索、预训练变换器以及定制化变换器模型。最终开发出更高效、更适用于ASW领域特征的定制变换器模型，并与BERT-base、RoBERTa、ModernBERT等主流模型进行了对比。

Result: 定制变换器模型在准确率、召回率、F1分数和ROC AUC等指标上均超过主流变换器模型，同时在较小GPU资源条件下亦能高效训练和推理。

Conclusion: 定制化的ASW文本分析模型代表了此领域的显著进步，不仅优化了文本分析流程，也为后续应用和研究提供了有力工具，有助于更有效地打击性交易行为。

Abstract: Sex trafficking refers to the use of force, fraud, or coercion to compel an
individual to perform in commercial sex acts against their will. Adult service
websites (ASWs) have and continue to be linked to sex trafficking, offering a
platform for traffickers to advertise their victims. Thus, organizations
involved in the fight against sex trafficking often use ASW data when
attempting to identify potential sex trafficking victims. A critical challenge
in transforming ASW data into actionable insight is text analysis. Previous
research using ASW data has shown that ASW ad text is important for linking
ads. However, working with this text is challenging due to its extensive use of
emojis, poor grammar, and deliberate obfuscation to evade law enforcement
scrutiny. We conduct a comprehensive study of language modeling approaches for
this application area, including simple information retrieval methods,
pre-trained transformers, and custom transformer models. We demonstrate that
characteristics of ASW text data allow efficient custom transformer models to
be trained with relatively small GPU resources and used efficiently for
inference on consumer hardware. Our custom models outperform fine-tuned
variants of well-known encoder-only transformer models, including BERT-base,
RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We
demonstrate the use of our best-performing custom configuration on three tasks
related to ASW data analysis: (i) decomposing the giant component in a graph
representation of ASW data, (ii) clustering ASW ad text, and (iii) using the
learned token embeddings to understand the use of emojis in the illicit context
we study. The models we develop represent a significant advancement in ASW text
analysis, which can be leveraged in a variety of downstream applications and
research.

</details>


### [78] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
*Michal Podstawski*

Main category: cs.CL

TL;DR: 本文提出利用预训练文本嵌入模型，将属性图中节点和边的文本属性转化为向量表示，增强语义分析能力，提升节点分类及关系预测表现。


<details>
  <summary>Details</summary>
Motivation: 属性图中包含很多丰富的文本属性，传统分析方法未能充分利用这些文本信息，限制了模型对图结构和语义的理解。

Method: 将预训练语言模型的文本嵌入应用于节点和边的文本属性，将其嵌入图分析流程，同时保持图结构不变。

Result: 实验表明，结合文本语义信息后，节点分类与关系预测的准确率显著提升，并提高了解释性。

Conclusion: 在属性图分析中引入文本嵌入技术可以有效利用文本语义信息，从而提升下游任务的效果和可解释性。

Abstract: Labeled property graphs often contain rich textual attributes that can
enhance analytical tasks when properly leveraged. This work explores the use of
pretrained text embedding models to enable efficient semantic analysis in such
graphs. By embedding textual node and edge properties, we support downstream
tasks including node classification and relation prediction with improved
contextual understanding. Our approach integrates language model embeddings
into the graph pipeline without altering its structure, demonstrating that
textual semantics can significantly enhance the accuracy and interpretability
of property graph analysis.

</details>


### [79] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
*Yilun Zhao,Chengye Wang,Chuhan Li,Arman Cohan*

Main category: cs.CL

TL;DR: 本论文提出了MISS-QA，这是首个专为评估模型解读科学文献中示意图能力而设计的基准数据集，并展示了当前多模态模型与人类专家在此任务上的显著差距。


<details>
  <summary>Details</summary>
Motivation: 随着多模态基础模型的发展，模型在理解视觉与文本信息的能力已有提升，但对科学论文中复杂的示意图解读能力尚缺乏针对性评估工具。因此，需要创建专门的基准数据集，以检测和推动模型在科学文献互补信息解读方面的进步。

Method: 作者构建了MISS-QA数据集，含有1500个来自465篇科学论文、由专家注释的问答样本。任务要求模型对展示研究概要的示意图进行解读，并结合论文上下文回答信息检索类问题，随后评估了18种前沿多模态基础模型（如o4-mini、Gemini-2.5-Flash、Qwen2.5-VL）的表现。

Result: 评估结果显示，所有主流多模态模型在MISS-QA上的表现与人类专家存在显著差距。论文还深入分析了模型在无法回答的问题和具体错误类型上的表现，进一步揭示了模型的优势与不足。

Conclusion: 目前多模态基础模型在理解科学论文中示意图上的能力远低于人类，MISS-QA为模型能力提升提供了重要诊断工具，并为今后多模态科学文献理解模型的优化指明了方向。

Abstract: This paper introduces MISS-QA, the first benchmark specifically designed to
evaluate the ability of models to interpret schematic diagrams within
scientific literature. MISS-QA comprises 1,500 expert-annotated examples over
465 scientific papers. In this benchmark, models are tasked with interpreting
schematic diagrams that illustrate research overviews and answering
corresponding information-seeking questions based on the broader context of the
paper. We assess the performance of 18 frontier multimodal foundation models,
including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant
performance gap between these models and human experts on MISS-QA. Our analysis
of model performance on unanswerable questions and our detailed error analysis
further highlight the strengths and limitations of current models, offering key
insights to enhance models in comprehending multimodal scientific literature.

</details>


### [80] [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)
*David M. Markowitz,Samuel Hardman Taylor*

Main category: cs.CL

TL;DR: 本文调查了社交媒体上仇恨言论是否因获得他人社会认同而增加。通过对Parler网站1.1亿帖子分析，发现获得点赞的仇恨言论不会导致后续更多或更极端的仇恨言论。社交认同强化机制在小众平台可能有不同表现。


<details>
  <summary>Details</summary>
Motivation: 目前关于社交认同（如点赞、支持等）是否推动网络仇恨言论扩散的理论和实证研究较少，尤其在小众社交平台。作者旨在验证Walther的“在线仇恨社会认同理论”，评估社会认同是否会导致更多或更极端的仇恨言论。

Method: 作者收集并分析了2018至2021年Parler平台上1.1亿条帖子数据。采用统计方法考察仇恨帖获点赞数与之后仇恨言论数量和极端程度的相关性，分析了短期和长期（至半年）效果，并区分了个体内和个体间效应。

Result: 分析发现，个人在仇恨言论获得更多点赞后，其后续发帖的仇恨程度和数量没有显著增加。个体间层面，在单个帖子上点赞数与仇恨言论呈负相关，对不同时间尺度下结果不一。

Conclusion: 社会认同并不是在小众平台上激励仇恨言论持续或升级的决定性因素，社交认同强化机制的效果因平台特性可能不同。

Abstract: In this paper, we explored how online hate is motivated by receiving social
approval from others. We specifically examined two central tenets of Walther's
(2024) social approval theory of online hate: (H1a) more signals of social
approval on hate messages predicts more subsequent hate messages, and (H1b) as
social approval increases, hate speech messages become more extreme. Using over
110 million posts from Parler (2018-2021), we observed that the number of
upvotes a person received on a hate speech post was unassociated with the
amount of hate speech in their next post and posts during the next week, month,
three months, and six months. Between-person effects revealed an average
negative relationship between social approval and hate speech production at the
post level, but this relationship was mixed at other time intervals. Social
approval reinforcement mechanisms of online hate may operate differently on
niche social media platforms.

</details>


### [81] [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
*Yiran Hu,Zongyue Xue,Haitao Li,Siyuan Zheng,Qingjing Chen,Shaochun Wang,Xihan Zhang,Ning Zheng,Yun Liu,Qingyao Ai,Yiqun Liu,Charles L. A. Clarke,Weixing Shen*

Main category: cs.CL

TL;DR: 本文从司法公正角度系统性评估了大语言模型（LLMs）在判决任务中的公平性，建立评估框架、数据集与评测指标，并发现目前LLMs在司法应用中表现出普遍的不公正与偏见。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在高风险领域、尤其是会影响权利和公平的司法领域的日益应用，对于其在司法判决中能否保持公正、公平性成为亟需研究的问题。本研究旨在填补大语言模型司法公平性领域的理论和实践空白。

Method: 作者从司法公正理论出发，构建了包括65个标签、161个取值的评估框架，并据此编制了包含177,100个司法案例事实的数据集JudiFair。提出了不一致性、偏见和非均衡不准确性三大评测指标，并研发方法评估多种LLMs在各类标签下整体公平性。

Result: 采用上述框架与指标对16个主流LLMs进行实验，结果发现：1) LLMs在司法任务中普遍呈现不一致、不公平、偏见等问题；2) 在人口统计标签上的偏见尤为严重，实质性标签上略轻，程序性标签较重；3) 不一致性越高，偏见反而降低，而预测准确性提升时偏见加重；4) 通过调整temperature参数可一定程度改善公平性，但模型规模、发布时间和原产国对司法公平性影响不显著。

Conclusion: 当前主流LLMs在司法判决任务中存在显著的不公正和偏见，尚难以成为可信赖的公正判官。作者发布了数据集和工具包，为后续LLM公平性评估与改进研究提供支撑。

Abstract: Large Language Models (LLMs) are increasingly used in high-stakes fields
where their decisions impact rights and equity. However, LLMs' judicial
fairness and implications for social justice remain underexplored. When LLMs
act as judges, the ability to fairly resolve judicial issues is a prerequisite
to ensure their trustworthiness. Based on theories of judicial fairness, we
construct a comprehensive framework to measure LLM fairness, leading to a
selection of 65 labels and 161 corresponding values. Applying this framework to
the judicial system, we compile an extensive dataset, JudiFair, comprising
177,100 unique case facts. To achieve robust statistical inference, we develop
three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and
introduce a method to assess the overall fairness of multiple LLMs across
various labels. Through experiments with 16 LLMs, we uncover pervasive
inconsistency, bias, and imbalanced inaccuracy across models, underscoring
severe LLM judicial unfairness. Particularly, LLMs display notably more
pronounced biases on demographic labels, with slightly less bias on substance
labels compared to procedure ones. Interestingly, increased inconsistency
correlates with reduced biases, but more accurate predictions exacerbate
biases. While we find that adjusting the temperature parameter can influence
LLM fairness, model size, release date, and country of origin do not exhibit
significant effects on judicial fairness. Accordingly, we introduce a publicly
available toolkit containing all datasets and code, designed to support future
research in evaluating and improving LLM fairness.

</details>


### [82] [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)
*Ikumi Numaya,Shoji Moriya,Shiki Sato,Reina Akama,Jun Suzuki*

Main category: cs.CL

TL;DR: 该论文提出并发布了一个包含用户偏好、主观和客观风格相似性的新对话数据集，分析了它们与用户偏好的关系，发现主观风格相似性与用户偏好高度相关，且主观、客观相似性评价存在差异。


<details>
  <summary>Details</summary>
Motivation: 虽然以往研究表明对话风格相似性有助于提升用户印象，但对主观与客观相似性区别关注较少，作者希望深入探索主观和客观风格相似性与用户偏好的具体关系。

Method: 作者构建了一个新颖的数据集，记录用户偏好、用户自己对风格相似性的主观评价，以及第三方标注的客观风格相似性评分，并基于该数据集分析这些因素之间的相关性。

Result: 分析发现，用户主观感知到的风格相似性与其偏好表现出强正相关。同时，主观风格相似性与第三方客观评价结果不完全一致。

Conclusion: 研究强调了主观和客观风格相似性评估的区别及其在用户偏好分析中的重要价值，提示今后相关工作应分别对待两者并关注它们所反映的不同维度。

Abstract: Recent advancements in dialogue generation have broadened the scope of
human-bot interactions, enabling not only contextually appropriate responses
but also the analysis of human affect and sensitivity. While prior work has
suggested that stylistic similarity between user and system may enhance user
impressions, the distinction between subjective and objective similarity is
often overlooked. To investigate this issue, we introduce a novel dataset that
includes users' preferences, subjective stylistic similarity based on users'
own perceptions, and objective stylistic similarity annotated by third party
evaluators in open-domain dialogue settings. Analysis using the constructed
dataset reveals a strong positive correlation between subjective stylistic
similarity and user preference. Furthermore, our analysis suggests an important
finding: users' subjective stylistic similarity differs from third party
objective similarity. This underscores the importance of distinguishing between
subjective and objective evaluations and understanding the distinct aspects
each captures when analyzing the relationship between stylistic similarity and
user preferences. The dataset presented in this paper is available online.

</details>


### [83] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
*Seungho Choi*

Main category: cs.CL

TL;DR: 本文针对韩语等低资源语言在大语言模型（LLM）上的表现不佳的问题，提出了HanjaBridge方法，通过引入多义汉字消岐机制提升韩语理解能力，并实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 韩语因同音多义词（同音汉字词）在韩文表记下难以区分，导致LLM语义理解困难。现有方法不能有效解决同形异义词歧义。

Method: 提出HanjaBridge，在模型持续预训练过程中，对于韩语中的多音字/同形词，把所有可能的汉字候选同时输入模型，让模型通过上下文习得消歧能力，并结合token级知识蒸馏避免遗忘。

Result: 在KoBALT基准测试集上性能提升21%，且韩中两语言语义对齐效果增强，在预测时不需要再用Hanja增强，无额外推理开销。

Conclusion: HanjaBridge大幅提升了韩语LLM的语义理解和跨语言能力，且方法高效实用，可为低资源语言提供类似解决思路。

Abstract: Large language models (LLMs) often show poor performance in low-resource
languages like Korean, partly due to unique linguistic challenges such as
homophonous Sino-Korean words that are indistinguishable in Hangul script. To
address this semantic ambiguity, we propose HanjaBridge, a novel
meaning-injection technique integrated into a continual pre-training (CPT)
framework. Instead of deterministically mapping a word to a single Hanja
(Chinese character), HanjaBridge presents the model with all possible Hanja
candidates for a given homograph, encouraging the model to learn contextual
disambiguation. This process is paired with token-level knowledge distillation
to prevent catastrophic forgetting. Experimental results show that HanjaBridge
significantly improves Korean language understanding, achieving a 21\% relative
improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment
between Korean and Chinese through shared Hanja, we observe a strong positive
cross-lingual transfer. Furthermore, these gains persist even when Hanja
augmentation is omitted at inference time, ensuring practical efficiency with
no additional run-time cost.

</details>


### [84] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
*Kalit Inani,Keshav Kabra,Vijay Marupudi,Sashank Varma*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在类比识别与映射任务中的能力，比较了其与人类在细粒度推理水平上的表现，并分析了模型规模和架构对结果的影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在多种认知任务上日益接近人类表现，研究它们在复杂类比推理任务中的能力和局限性有助于理解其是否能模拟人类推理过程，尤其是在理解故事和隐含关系方面。

Method: 1）使用故事为背景的类比映射任务，分析LLMs在句子嵌入空间中对源-目标文本相似性和源-干扰项文本差异性的捕捉能力；2）引入显式的提示，要求LLMs解释类比；3）在整体准确率之上，细致比较单个类比项目的人机表现；4）测试不同模型规模（8B与70B参数）及不同架构（如GPT-4和LLaMA3）在上述任务上的表现。

Result: LLMs能够在整体和细粒度类比推理任务中表现出一定程度的与人类相似的认知特征，较大规模和先进架构如GPT-4、LLaMA3表现更优。显式解释提示可在一定程度上改善模型推理表现，但与人类仍存在差距，在部分细分推理任务上表现不如人类。

Conclusion: LLMs在类比推理任务上已有较大进步，部分场景下可作为人类推理的模型，但在解释能力和一致性方面仍有提升空间。未来应着重改进其高层推理和解释型能力。

Abstract: Recent advancements in Large Language Models (LLMs) have brought them closer
to matching human cognition across a variety of tasks. How well do these models
align with human performance in detecting and mapping analogies? Prior research
has shown that LLMs can extract similarities from analogy problems but lack
robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the
current study focused on a story-based analogical mapping task and conducted a
fine-grained evaluation of LLM reasoning abilities compared to human
performance. First, it explored the semantic representation of analogies in
LLMs, using sentence embeddings to assess whether they capture the similarity
between the source and target texts of an analogy, and the dissimilarity
between the source and distractor texts. Second, it investigated the
effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we
examine whether LLMs exhibit similar performance profiles to those observed in
humans by evaluating their reasoning at the level of individual analogies, and
not just at the level of overall accuracy (as prior studies have done). Our
experiments include evaluating the impact of model size (8B vs. 70B parameters)
and performance variation across state-of-the-art model architectures such as
GPT-4 and LLaMA3. This work advances our understanding of the analogical
reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [85] [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)
*Anthony Miyaguchi,David Guecha,Yuwen Chiu,Sidharth Gaur*

Main category: cs.CL

TL;DR: 本文总结了DS@GT团队在eRisk 2025两个挑战赛中的参与，重点介绍了基于提示工程的抑郁症对话检测方法及其结果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在心理健康领域应用的兴起，如何利用其进行有效、结构化的心理健康评估成为挑战。特别是在缺乏真实标签的情况下，提升评估准确性和一致性尤为关键。

Method: 采用提示工程策略，设计了一套基于Beck抑郁问卷（BDI-II）标准的问题，引导多个类型的大语言模型进行评估，并将输出结构化为JSON格式。通过多个模型间的输出一致性和内部一致性来评价其性能。

Result: 所提方法在官方排行榜获得第二名，主要指标DCHR为0.50，ADODL为0.89，ASHR为0.27，展示了较高的有效性和稳定性。

Conclusion: 利用提示工程对齐大模型输出到专业抑郁评估标准，并通过结构化输出和一致性评估，能在没有真实标签的情况下，实现较为可靠的对话型抑郁检测。

Abstract: This Working Note summarizes the participation of the DS@GT team in two eRisk
2025 challenges. For the Pilot Task on conversational depression detection with
large language-models (LLMs), we adopted a prompt-engineering strategy in which
diverse LLMs conducted BDI-II-based assessments and produced structured JSON
outputs. Because ground-truth labels were unavailable, we evaluated cross-model
agreement and internal consistency. Our prompt design methodology aligned model
outputs with BDI-II criteria and enabled the analysis of conversational cues
that influenced the prediction of symptoms. Our best submission, second on the
official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

</details>


### [86] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Main category: cs.CL

TL;DR: 本文提出了一种将手语视为自然语言，并通过微调大语言模型（LLM）以促进文本到手语生成的方法。实验结果表明，这种方法能有效对齐手语与口语的分布和语法差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多个AI领域表现出强大能力，但由于手语的复杂性及其独特规则，其在手语生成任务中的影响仍有限。因此，亟需方法发挥LLM优势以提升手语生成质量。

Method: 作者提出了TEAch Me Sign（TEAM-Sign）方法，将手语视为另一种自然语言，通过对LLM微调，使其学习文本与手语之间的对应关系。此外，采用逐步提示策略(stepwise prompting)以抽取模型内在的手语知识并辅助生成过程。

Result: 在How2Sign和Phoenix14T两个手语数据集上的实验表明，TEAM-Sign方法能有效利用LLM的知识和推理能力，对齐手语与口语在分布和语法规则上的差异。

Conclusion: 该方法表明，通过合理微调和提示，大语言模型可以胜任手语生成任务，促进手语与口语间的高效转换，并为未来相关研究提供了新方向。

Abstract: Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


### [87] [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)
*Lin Tian,Johanne R. Trippas,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 本文提出了一种基于层次化LoRA的Llama 3.1 8B模型，用于多语言推文中的性别歧视检测，方法简单高效，参数量极低却取得了竞争性表现。


<details>
  <summary>Details</summary>
Motivation: 当前在多语言社交媒体文本中检测性别歧视的任务复杂且资源消耗大，尤其是在细粒度和多标签场景下。作者希望通过更高效的参数微调和统一建模，实现更少资源消耗下的强大性能。

Method: 采用Llama 3.1 8B大模型，在所有线性变换层施加LoRA微调（非仅关注attention层），提出条件适配器路由，对三级任务（性别歧视二分类、意图检测、多标签归因）分层适配。多语言统一训练，无需区分英西两个模型。各子任务配备独立LoRA适配器（秩为16，QLoRA 4-bit）。标准监督学习，无需复杂数据处理和集合算法。

Result: 方法只需全量参数1.67%即可，训练时间减少75%，存储空间缩减98%。跨语种迁移提升F1分数1.7%~2.4%。具体子任务在ICM-Hard上的F1分别为0.6774（二分类）、0.4991（意图）、0.6519（多标签）。

Conclusion: 层次化LoRA微调结合适配路由，能以极低资源成本显著提升多语种性别歧视检测任务的表现，且不用复杂的数据处理或多模型方案，具有广泛推广价值。

Abstract: This paper presents our approach to EXIST 2025 Task 1, addressing text-based
sexism detection in English and Spanish tweets through hierarchical Low-Rank
Adaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter
routing that explicitly models label dependencies across three hierarchically
structured subtasks: binary sexism identification, source intention detection,
and multilabel sexism categorization. Unlike conventional LoRA applications
that target only attention layers, we apply adaptation to all linear
transformations, enhancing the model's capacity to capture task-specific
patterns. In contrast to complex data processing and ensemble approaches, we
show that straightforward parameter-efficient fine-tuning achieves strong
performance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each
subtask using unified multilingual training that leverages Llama 3.1's native
bilingual capabilities. The method requires minimal preprocessing and uses
standard supervised learning. Our multilingual training strategy eliminates the
need for separate language-specific models, achieving 1.7-2.4\% F1 improvements
through cross-lingual transfer. With only 1.67\% trainable parameters compared
to full fine-tuning, our approach reduces training time by 75\% and model
storage by 98\%, while achieving competitive performance across all subtasks
(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,
0.6519 for multilabel categorization).

</details>


### [88] [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: HerO 2是一个高效、可靠的事实核查系统，在FEVER-25竞赛中获得第二名，并拥有最快的运行速度。


<details>
  <summary>Details</summary>
Motivation: 为了提升事实核查任务中的证据质量和系统效率，基于上一年最优开源模型HerO进行优化。

Method: 通过文档摘要与答案重构提升证据质量；利用后训练量化技术，在计算资源受限下提升真实性预测的性能；整合更新的语言模型骨干提升系统整体表现。

Result: HerO 2在排行榜上排名第二，在前3名系统中运行时间最短，兼具高效率和实际应用潜力。

Conclusion: HerO 2在事实核查任务中展现了强效能和高效率，为实际场景中的快速事实验证提供了有力工具。

Abstract: This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task
at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the
best-performing open-source model from the previous year's challenge. It
improves evidence quality through document summarization and answer
reformulation, optimizes veracity prediction via post-training quantization
under computational constraints, and enhances overall system performance by
integrating updated language model (LM) backbones. HerO 2 ranked second on the
leaderboard while achieving the shortest runtime among the top three systems,
demonstrating both high efficiency and strong potential for real-world fact
verification. The code is available at https://github.com/ssu-humane/HerO2.

</details>


### [89] [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
*Dahyun Lee,Jonghyeon Choi,Jiyoung Han,Kunwoo Park*

Main category: cs.CL

TL;DR: 本文提出了首个面向韩语的长新闻文章立场检测数据集及新方法，有效提升了立场检测的精度，并能提升新闻推荐系统的多样性与揭示媒体偏向。


<details>
  <summary>Details</summary>
Motivation: 个性化新闻推荐容易加剧信息过滤泡沫及政治极化，新闻报道的立场检测有助于增强观点多样性，但现有方法多局限于短文本和高资源语言。该研究旨在突破这些限制，服务低资源语言韩语长新闻的立场分析。

Method: 作者构建了首个韩语新闻立场检测数据集K-News-Stance，包含2,000篇新闻及19,650个段落的立场标注。提出了JoA-ICL框架：利用大语言模型对新闻结构化片段（如导语、引用）独立预测立场，再加权聚合各片段判定全文立场。

Result: 实验证明，JoA-ICL方法优于现有立场检测模型，能够更好地把握长新闻整体立场。

Conclusion: 该方法为多观点新闻推荐与媒体偏向分析提供了有效工具，对促进新闻推荐的观点多样性具有应用价值。

Abstract: As online news consumption grows, personalized recommendation systems have
become integral to digital journalism. However, these systems risk reinforcing
filter bubbles and political polarization by failing to incorporate diverse
perspectives. Stance detection -- identifying a text's position on a target --
can help mitigate this by enabling viewpoint-aware recommendations and
data-driven analyses of media bias. Yet, existing stance detection research
remains largely limited to short texts and high-resource languages. To address
these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for
article-level stance detection, comprising 2,000 news articles with
article-level and 19,650 segment-level stance annotations across 47 societal
issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided
\textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that
employs a language model agent to predict the stances of key structural
segments (e.g., leads, quotes), which are then aggregated to infer the overall
article stance. Experiments show that \textsc{JoA-ICL} outperforms existing
stance detection methods, highlighting the benefits of segment-level agency in
capturing the overall position of long-form news articles. Two case studies
further demonstrate its broader utility in promoting viewpoint diversity in
news recommendations and uncovering patterns of media bias.

</details>


### [90] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
*Haowei Yang,Ziyu Shen,Junli Shao,Luyao Men,Xinyue Han,Jing Dong*

Main category: cs.CL

TL;DR: 该论文利用经过领域适应的大语言模型（LLM）提升了对心血管疾病（CVD）风险的自动化预测，从自由文本的临床记录中提取早期症状，并显著提高了风险评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 以往心血管疾病风险预测主要基于结构化数据，忽视了临床自由文本中富有价值的潜在信息。现有方法对这些信息的利用有限，导致早期预警和分层管理的效果不足。

Method: 本文提出一种基于大语言模型的临床NLP流程。该流程融合了心血管领域特定的微调、基于提示词的推理和实体感知推理，可以从自由文本病例记录中进行症状抽取、上下文推理和相关性分析。同时，通过提示工程和混合规则验证方法解决上下文幻觉和时间序列歧义问题。

Result: 在MIMIC-III和CARDIO-NLP数据集上测试，该方法在精确率、召回率、F1值和AUROC等指标上均取得提升。经心脏科医生评估，临床相关性Kappa系数达0.82，表现优异。

Conclusion: 领域适应的LLM显著提升了临床自由文本中风险信号的感知与利用能力，有助于临床决策支持系统的早期预警和风险分层，为患者叙述数据转化为可操作风险评估提供了新思路和工具。

Abstract: Timely identification and accurate risk stratification of cardiovascular
disease (CVD) remain essential for reducing global mortality. While existing
prediction models primarily leverage structured data, unstructured clinical
notes contain valuable early indicators. This study introduces a novel
LLM-augmented clinical NLP pipeline that employs domain-adapted large language
models for symptom extraction, contextual reasoning, and correlation from
free-text reports. Our approach integrates cardiovascular-specific fine-tuning,
prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III
and CARDIO-NLP datasets demonstrate improved performance in precision, recall,
F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by
cardiologists. Challenges such as contextual hallucination, which occurs when
plausible information contracts with provided source, and temporal ambiguity,
which is related with models struggling with chronological ordering of events
are addressed using prompt engineering and hybrid rule-based verification. This
work underscores the potential of LLMs in clinical decision support systems
(CDSS), advancing early warning systems and enhancing the translation of
patient narratives into actionable risk assessments.

</details>


### [91] [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha*

Main category: cs.CL

TL;DR: 本文提出并验证了一种基于混合型Transformer的情感分析方法，有效解析孟加拉国七月革命期间社交媒体评论中的公众情感。


<details>
  <summary>Details</summary>
Motivation: 七月革命期间，社交媒体成为表达和传播公众情感的重要渠道，亟需针对低资源语言（如孟加拉语）的高效情感分析方法。

Method: 构建包含4200条孟加拉语社交媒体评论的新数据集，采用BanglaBERT、mBERT、XLM-RoBERTa及创新型混合XMB-BERT对文本进行特征抽取，并用主成分分析（PCA）降维，结合11种分类器识别情感。

Result: 混合型XMB-BERT联合投票分类器取得了83.7%的准确率，优于其他模型组合。

Conclusion: 机器学习与Transformer模型可有效分析低资源语言中的社交情感，适用于重大社会事件的数据分析。

Abstract: The July Revolution in Bangladesh marked a significant student-led mass
uprising, uniting people across the nation to demand justice, accountability,
and systemic reform. Social media platforms played a pivotal role in amplifying
public sentiment and shaping discourse during this historic mass uprising. In
this study, we present a hybrid transformer-based sentiment analysis framework
to decode public opinion expressed in social media comments during and after
the revolution. We used a brand new dataset of 4,200 Bangla comments collected
from social media. The framework employs advanced transformer-based feature
extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the
proposed hybrid XMB-BERT, to capture nuanced patterns in textual data.
Principle Component Analysis (PCA) were utilized for dimensionality reduction
to enhance computational efficiency. We explored eleven traditional and
advanced machine learning classifiers for identifying sentiments. The proposed
hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of
83.7% and outperform other model classifier combinations. This study
underscores the potential of machine learning techniques to analyze social
sentiment in low-resource languages like Bangla.

</details>


### [92] [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)
*Andres Azqueta-Gavaldón,Joaquin Ramos Cosgrove*

Main category: cs.CL

TL;DR: 论文分析了金融领域中外国实体识别与匹配的问题，比较了传统算法与大型语言模型（LLMs）在处理实际公司数据时的表现。


<details>
  <summary>Details</summary>
Motivation: 全球跨境金融活动日益频繁，准确识别和分类外国实体对西班牙金融体系的风险管理、监管合规和反金融犯罪至关重要。然而，传统实体匹配方法在面对语言差异、特殊字符、名称变更等复杂情况时效果有限，带来实际操作挑战。

Method: 论文采用了三类方法进行实体匹配：传统算法（Jaccard、余弦、Levenshtein距离）、Hugging Face平台上的LLM、以及基于接口的LLM（如微软Copilot、阿里Qwen 2.5）。实验在65个葡萄牙公司案例的数据集上进行，分析各方法的准确率与误报率。

Result: 传统方法准确率超过92%，但误报率高达20-40%。界面型LLM表现更佳，准确率超过93%，F1值超过96%，且误报率更低。

Conclusion: 与传统匹配算法相比，LLMs特别是接口型LLM，在跨国金融实体识别任务中更能处理上下文语义和复杂业务情况，可提升准确性并降低误报率，显示出广阔的应用前景。

Abstract: The growing prevalence of cross-border financial activities in global markets
has underscored the necessity of accurately identifying and classifying foreign
entities. This practice is essential within the Spanish financial system for
ensuring robust risk management, regulatory adherence, and the prevention of
financial misconduct. This process involves a labor-intensive entity-matching
task, where entities need to be validated against available reference sources.
Challenges arise from linguistic variations, special characters, outdated
names, and changes in legal forms, complicating traditional matching algorithms
like Jaccard, cosine, and Levenshtein distances. These methods struggle with
contextual nuances and semantic relationships, leading to mismatches. To
address these limitations, we explore Large Language Models (LLMs) as a
flexible alternative. LLMs leverage extensive training to interpret context,
handle abbreviations, and adapt to legal transitions. We evaluate traditional
methods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft
Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.
Results show traditional methods achieve accuracies over 92% but suffer high
false positive rates (20-40%). Interface-based LLMs outperform, achieving
accuracies above 93%, F1 scores exceeding 96%, and lower false positives
(40-80%).

</details>


### [93] [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)
*Zichen Wen,Jiashu Qu,Dongrui Liu,Zhiyuan Liu,Ruixi Wu,Yicun Yang,Xiangqi Jin,Haoyun Xu,Xuyang Liu,Weijia Li,Chaochao Lu,Jing Shao,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文揭示了基于扩散的LLM（dLLM）在安全对齐机制上的新型脆弱性，并提出了一种系统性越狱攻击框架DIJA，有效突破了现有对齐防护。


<details>
  <summary>Details</summary>
Motivation: dLLM因其并行解码和双向建模能力带来了更快推理和更好交互，但现有安全对齐机制并不能有效防御对dLLM独有的攻击方法，暴露了潜在的安全风险。

Method: 提出了DIJA攻击框架，通过构造交织mask-文本的对抗性提示，专门利用dLLM的双向建模和并行解码机制，实现突破常规安全机制的越狱。并通过系统性实验评估该攻击方法的有效性。

Result: DIJA比现有最强的越狱攻击（如ReNeLLM）表现更佳，在Dream-Instruct、JailbreakBench等基准上，ASR成功率和StrongREJECT分数均远超现有方法，并且无需伪装或隐藏有害内容。

Conclusion: dLLM架构由于其内在机制暴露了新的安全攻击面，现有对齐防御无法有效防护此类攻击，必须重新思考dLLM的安全对齐策略。

Abstract: Diffusion-based large language models (dLLMs) have recently emerged as a
powerful alternative to autoregressive LLMs, offering faster inference and
greater interactivity via parallel decoding and bidirectional modeling.
However, despite strong performance in code generation and text infilling, we
identify a fundamental safety concern: existing alignment mechanisms fail to
safeguard dLLMs against context-aware, masked-input adversarial prompts,
exposing novel vulnerabilities. To this end, we present DIJA, the first
systematic study and jailbreak attack framework that exploits unique safety
weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial
interleaved mask-text prompts that exploit the text generation mechanisms of
dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional
modeling drives the model to produce contextually consistent outputs for masked
spans, even when harmful, while parallel decoding limits model dynamic
filtering and rejection sampling of unsafe content. This causes standard
alignment mechanisms to fail, enabling harmful completions in alignment-tuned
dLLMs, even when harmful behaviors or unsafe instructions are directly exposed
in the prompt. Through comprehensive experiments, we demonstrate that DIJA
significantly outperforms existing jailbreak methods, exposing a previously
overlooked threat surface in dLLM architectures. Notably, our method achieves
up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior
baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and
by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of
harmful content in the jailbreak prompt. Our findings underscore the urgent
need for rethinking safety alignment in this emerging class of language models.
Code is available at https://github.com/ZichenWen1/DIJA.

</details>


### [94] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
*Sanhanat Sivapiromrat,Caiqi Zhang,Marco Basaldella,Nigel Collier*

Main category: cs.CL

TL;DR: 本文提出了一个研究大语言模型（LLM）中数据投毒攻击的新框架，并发现多个后门触发器可以在同一模型中并存，增强了攻击威胁。作者还提出了一种高效的后处理防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要聚焦于单一触发短语的攻击效果，缺乏对触发机制及多触发器交互的深入理解。作者旨在探索多后门触发器在LLM中的行为与威胁，以及提升防御手段。

Method: 作者提出用多个不同但高嵌入相似性的表现作为后门触发器，并观察在LLM中这些触发器的共存与激活情况。为了防御多触发器攻击，提出基于层级权重差分析，对特定模型组件进行有针对性再训练的恢复方法。

Result: 实验证明，多触发器可共存且不会互相干扰，且即使触发短语被分隔或替换，后门也能被激活。所提后处理方法能有效去除后门行为，而且参数调整很少。

Conclusion: LLM的投毒攻击面比预想更广且更顽固。所提出的防御技术有效且高效，有助于提升多触发器投毒威胁下LLM的安全性。

Abstract: Recent studies have shown that Large Language Models (LLMs) are vulnerable to
data poisoning attacks, where malicious training examples embed hidden
behaviours triggered by specific input patterns. However, most existing works
assume a phrase and focus on the attack's effectiveness, offering limited
understanding of trigger mechanisms and how multiple triggers interact within
the model. In this paper, we present a framework for studying poisoning in
LLMs. We show that multiple distinct backdoor triggers can coexist within a
single model without interfering with each other, enabling adversaries to embed
several triggers concurrently. Using multiple triggers with high embedding
similarity, we demonstrate that poisoned triggers can achieve robust activation
even when tokens are substituted or separated by long token spans. Our findings
expose a broader and more persistent vulnerability surface in LLMs. To mitigate
this threat, we propose a post hoc recovery method that selectively retrains
specific model components based on a layer-wise weight difference analysis. Our
method effectively removes the trigger behaviour with minimal parameter
updates, presenting a practical and efficient defence against multi-trigger
poisoning.

</details>


### [95] [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)
*Seif Ahmed,Mohamed T. Younes,Abdelrahman Moustafa,Abdelrahman Allam,Hamza Moustafa*

Main category: cs.CL

TL;DR: 该论文提出了一种面向多语言多模态推理的稳健集成系统，在ImageCLEF 2025 EXAMS V挑战赛取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 当今多语言多模态推理任务需求增长，尤其在教育等实际场景中，如何兼顾各语言准确性和系统高效性，是当前技术难点。

Method: 系统集成了Gemini 2.5 Flash用于视觉描述、Gemini 1.5 Pro用于描述润色和一致性检查、Gemini 2.5 Pro用于最终推理判定，全部通过巧妙few-shot及zero-shot提示协调。此外，探索了提示设计和跨语言增强对模型性能的影响，并对多种模型做消融实验。

Result: 1）零样本设置下Gemini 2.5 Flash表现优于训练模型；2）标准化提示能将准确率由55.9%提升至61.7%；3）系统在多语言赛道以81.4%准确率夺冠，并在13种语言中11项居首。

Conclusion: 轻量级OCR-多模态大模型集成配合精确提示和跨语种增强策略，在多语言教育场景可超越大型端到端模型，是高效多语多模态推理的可行路径。

Abstract: We present a robust ensemble-based system for multilingual multimodal
reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach
integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption
refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which
handles final answer selection, all coordinated through carefully engineered
few-shot and zero-shot prompts. We conducted an extensive ablation study,
training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,
Mistral) on an English dataset and its multilingual augmented version.
Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for
comparison and found it to substantially outperform the trained models. Prompt
design also proved critical: enforcing concise, language-normalized formats and
prohibiting explanatory text boosted model accuracy on the English validation
set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)
achieved first place overall in the multilingual track with 81.4% accuracy, and
led 11 out of 13 individual language tracks, with top results such as 95.07%
for Croatian and 92.12% for Italian. These findings highlight that lightweight
OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual
augmentation, can outperform heavier end-to-end models in high-stakes,
multilingual educational settings.

</details>


### [96] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
*Dimitri Staufer*

Main category: cs.CL

TL;DR: 本论文提出了一种新方法用于识别大语言模型（LLMs）中被记忆的个人信息，从而助力GDPR中「被遗忘权」的合规处理。作者构建了WikiMem数据集，并提出模型无关的衡量指标，揭示了现有隐私审计和遗忘方法的不足，并分析了不同规模和主题的LLM记忆行为。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs容易存储并泄露个人信息，触发GDPR中关于个人数据的法规，尤其是「被遗忘权」。现有机器遗忘方法主要假设要遗忘的数据已知，但实际无法有效识别模型中究竟存储了哪些个人信息。此外，主流隐私审计方法只针对总体或少数已知标识符，无法满足个人化的数据遗忘需求。因此有必要研发一种可定位并量化LLM中个人信息的方法，以实现更完整的法规合规。

Method: 作者提出并公开了WikiMem数据集，包含5000多条自然语言线索，覆盖来自Wikidata的243种人相关属性。同时，提出一种模型无关的衡量方法，通过对经过改写的提示进行负对数似然（NLL）校准，将真实值与反事实进行排序，对模型中的人-事实关联进行量化。该方法可应用于不同规模（410M-70B参数）和类型的LLM。

Result: 在15种主流LLM和200名个体的测试中，发现大模型及其对象在网络上的可见度与模型记忆个人信息的概率显著相关。提出的框架能够有效识别LLM中被记忆的个人事实，为后续的数据遗忘和模型审计实验提供技术基础。

Conclusion: 论文为以个人为单位识别LLM中被记忆的个人信息提供了新工具。此方法可助力动态构建「遗忘集」，服务于机器遗忘及GDPR「被遗忘权」请求，为大规模机器学习模型的隐私合规开辟新路径。

Abstract: Large Language Models (LLMs) can memorize and reveal personal information,
raising concerns regarding compliance with the EU's GDPR, particularly the
Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the
data to forget is already known but do not address how to identify which
individual-fact associations are stored in the model. Privacy auditing
techniques typically operate at the population level or target a small set of
identifiers, limiting applicability to individual-level data inquiries. We
introduce WikiMem, a dataset of over 5,000 natural language canaries covering
243 human-related properties from Wikidata, and a model-agnostic metric to
quantify human-fact associations in LLMs. Our approach ranks ground-truth
values against counterfactuals using calibrated negative log-likelihood across
paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B
parameters), showing that memorization correlates with subject web presence and
model scale. We provide a foundation for identifying memorized personal data in
LLMs at the individual level, enabling the dynamic construction of forget sets
for machine unlearning and RTBF requests.

</details>


### [97] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
*Conrad Borchers,Bahar Shahrokhian,Francesco Balzan,Elham Tajik,Sreecharan Sankaranarayanan,Sebastian Simon*

Main category: cs.CL

TL;DR: 本文通过对多智能体系统（MAS）与单智能体在大语言模型（LLM）辅助下的定性编码表现进行对比，发现多智能体虽然能模拟人类协作流程，但通常并未提升编码准确率，甚至在多样人格设置下反而延迟共识达成；单智能体多数情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 定性研究中大量依赖人工编码，而LLM的进步使自动化成为可能。然而，采用模拟人类小组协作的多智能体（MAS）方案是否优于单智能体，目前学界尚不清楚。作者希望探究MAS中的人格与参数（如温度设定）对编码决策共识达成与准确率的影响。

Method: 采用6种开源LLM（参数量3B-32B），通过18种实验配置（涵盖单/多智能体、不同人格、温度设定），镜像人工定性编码流程，让代理基于8类编码方案判定对话片段。结果与金标准（人工标注）进行对比，总计分析超77,000条决策。

Result: 温度显著影响了6种LLM在编码讨论达成共识的频率与时机；多种人格设定下，4/6 LLM的MAS显著延缓共识达成。3种LLM中，调高温度弱化了多人格对共识的负面影响。但无论温度还是人格，多智能体一致并未带来准确率提升，大多数条件下单智能体更优。仅在OpenHermesV2:7B及某个编码类型、高共识（温度0.5或更低、含强势人格）条件下，MAS表现超出单体。

Conclusion: MAS虽然可以模拟人类协作流程，但并未带来编码准确率提升，目前LLM的定性研究应用下单智能体同样甚至更为高效。不同人格和参数的MAS也许主要用于揭示模糊标签，辅助完善定性工具书，而不直接提升自动化编码效果。论文同时开源全部MAS及实验配置代码。

Abstract: Large Language Models (LLMs) enable new possibilities for qualitative
research at scale, including coding and data annotation. While multi-agent
systems (MAS) can emulate human coding workflows, their benefits over
single-agent coding remain poorly understood. We conducted an experimental
study of how agent persona and temperature shape consensus-building and coding
accuracy of dialog segments based on a codebook with 8 codes. Our open-source
MAS mirrors deductive human coding through structured agent discussion and
consensus arbitration. Using six open-source LLMs (with 3 to 32 billion
parameters) and 18 experimental configurations, we analyze over 77,000 coding
decisions against a gold-standard dataset of human-annotated transcripts from
online math tutoring sessions. Temperature significantly impacted whether and
when consensus was reached across all six LLMs. MAS with multiple personas
(including neutral, assertive, or empathetic), significantly delayed consensus
in four out of six LLMs compared to uniform personas. In three of those LLMs,
higher temperatures significantly diminished the effects of multiple personas
on consensus. However, neither temperature nor persona pairing lead to robust
improvements in coding accuracy. Single agents matched or outperformed MAS
consensus in most conditions. Only one model (OpenHermesV2:7B) and code
category showed above-chance gains from MAS deliberation when temperature was
0.5 or lower and especially when the agents included at least one assertive
persona. Qualitative analysis of MAS collaboration for these configurations
suggests that MAS may nonetheless aid in narrowing ambiguous code applications
that could improve codebooks and human-AI coding. We contribute new insight
into the limits of LLM-based qualitative methods, challenging the notion that
diverse MAS personas lead to better outcomes. We open-source our MAS and
experimentation code.

</details>


### [98] [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)
*Valle Ruiz-Fernández,Mario Mina,Júlia Falcão,Luis Vasquez-Reina,Anna Sallés,Aitor Gonzalez-Agirre,Olatz Perez-de-Viñaspre*

Main category: cs.CL

TL;DR: 本论文提出了适用于西班牙语和加泰罗尼亚语的社会偏见评估基准数据集，用来检测大语言模型（LLMs）在西班牙社会语境下的偏见表现。研究发现，模型在存在歧义的问题场景下容易选择错误答案，并且高问答准确率往往意味着更依赖社会偏见。


<details>
  <summary>Details</summary>
Motivation: 当前针对大语言模型社会偏见的评估资源主要局限于英语和美国社会语境，针对其他语言及不同文化背景的系统化偏见评估工具明显缺乏，限制了多语言模型的公平性研究。

Method: 基于原有的BBQ问答偏见基准，作者构建了西班牙语（EsBBQ）和加泰罗尼亚语（CaBBQ）两个平行多项选择问答数据集，覆盖10类社会偏见类型，并适配至西班牙社会文化语境。通过不同家族、规模和变体的大语言模型在这些数据集上的表现，对模型偏见进行系统性评测。

Result: 测试结果显示，大多数模型在面对歧义性社会偏见问题时准确率较低，经常无法做出正确选择。同时，模型在问答准确率较高时，往往更依赖于社会偏见线索作答。

Conclusion: 研究结果表明，当前的大型语言模型在跨语言、跨文化背景下仍普遍存在社会偏见，且高准确率未必证明其公正性。所提出的EsBBQ和CaBBQ为西班牙社会背景下多语言公平性研究提供了基础工具，有助于后续模型改进与偏见消除方向的探索。

Abstract: Previous literature has largely shown that Large Language Models (LLMs)
perpetuate social biases learnt from their pre-training data. Given the notable
lack of resources for social bias evaluation in languages other than English,
and for social contexts outside of the United States, this paper introduces the
Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and
CaBBQ). Based on the original BBQ, these two parallel datasets are designed to
assess social bias across 10 categories using a multiple-choice QA setting, now
adapted to the Spanish and Catalan languages and to the social context of
Spain. We report evaluation results on different LLMs, factoring in model
family, size and variant. Our results show that models tend to fail to choose
the correct answer in ambiguous scenarios, and that high QA accuracy often
correlates with greater reliance on social biases.

</details>


### [99] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Main category: cs.CL

TL;DR: 本文提出了FlowFSM，一种结合了大型语言模型（LLM）、提示链和思维链的新框架，用于从RFC文档中高效精准地提取有限状态机（FSM），显著提升了协议分析的自动化水平。


<details>
  <summary>Details</summary>
Motivation: 现有FSM提取方法在可扩展性、覆盖完整性和自然语言规范歧义上存在局限，难以应对复杂的协议分析与安全研究需求。

Method: FlowFSM利用LLM，结合多步提示链和思维链推理，对协议规范（如RFC文档）进行系统性解析，自动识别各种状态转换，并生成结构化规则手册。

Result: 实验在FTP和RTSP协议上显示，FlowFSM拥有高精度FSM提取能力，并能有效减少虚假的状态转换，结果优于现有技术。

Conclusion: 基于智能体的LLM系统在协议安全分析和FSM自动推理领域有巨大潜力，可为网络协议逆向和漏洞检测等应用带来助益。

Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [100] [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
*Lyzander Marciano Andrylie,Inaya Rahmanisa,Mahardika Krisna Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 本论文提出了一种利用稀疏自编码器（SAE）的方法，以识别大语言模型（LLM）中对特定语言有代表性的特征，有助于揭示多语言处理机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理多语言时的内部机制尚不明确，现有研究多聚焦于单一神经元，但神经元的多语义特性使得区分语言相关特征非常困难。理解多语言机制有助于改进模型的表现和可解释性。

Method: 作者提出SAE-LAPE方法，基于特征激活概率，利用稀疏自编码器学习LLM中各语言的具象和抽象单语义特征，并定位模型中与特定语言相关的特征，重点分析前馈网络各层的差异。

Result: 实验发现，许多语言特有特征主要出现在模型的中后层，这些特征可解释性较好，对模型的多语言性能与输出具有显著影响。此外，该方法可用于语言识别，效果与fastText相当但具有更高的可解释性。

Conclusion: 提出的方法能有效检测和解释LLM中语言特有的内部特征，不仅揭示了多语言能力形成机制，还提升了模型的可解释性和实际应用价值。

Abstract: Understanding the multilingual mechanisms of large language models (LLMs)
provides insight into how they process different languages, yet this remains
challenging. Existing studies often focus on individual neurons, but their
polysemantic nature makes it difficult to isolate language-specific units from
cross-lingual representations. To address this, we explore sparse autoencoders
(SAEs) for their ability to learn monosemantic features that represent concrete
and abstract concepts across languages in LLMs. While some of these features
are language-independent, the presence of language-specific features remains
underexplored. In this work, we introduce SAE-LAPE, a method based on feature
activation probability, to identify language-specific features within the
feed-forward network. We find that many such features predominantly appear in
the middle to final layers of the model and are interpretable. These features
influence the model's multilingual performance and language output and can be
used for language identification with performance comparable to fastText along
with more interpretability. Our code is available at
https://github.com/LyzanderAndrylie/language-specific-features .

</details>


### [101] [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)
*Luohe Shi,Zuchao Li,Lefei Zhang,Guoming Liu,Baoyuan Qi,Hai Zhao*

Main category: cs.CL

TL;DR: 本文提出KV-Latent方法，通过将Key-Value缓存下采样到更低维的潜在空间，显著减少LLM推理过程中的KV Cache占用和带宽压力，实现更快推理和更低显存需求。此外，改进了低维度上的旋转位置编码鲁棒性。实验验证了方法有效且训练成本极低。


<details>
  <summary>Details</summary>
Motivation: Transformer Decoder结构的LLM虽然性能优异，但推理时KV缓存会随着序列增长，造成显存和带宽瓶颈，影响大模型的部署和推理效率。有效减少KV缓存开销成为优化重点。

Method: 提出KV-Latent方法，通过对Key和Value向量下采样映射到低维潜在空间，配合小规模再训练实现。进一步改进了用于低维特征的旋转位置编码方式，增强其稳定性。评价了不同下采样配置下的性能影响。

Result: 实验表明，KV-Latent在多种注意力机制LLM上均可显著减小KV缓存占用，加快推理，且模型性能影响微小。比较了Key、Value独立降维的效果。增强后的低维旋转位置编码提升了稳定性。

Conclusion: KV-Latent可大幅提升LLM KV缓存效率，降低推理资源消耗，为更高效的LLM应用提供方向。相关代码已开源，便于社区应用和验证。

Abstract: Large language models (LLMs) based on Transformer Decoders have become the
preferred choice for conversational generative AI. Despite the overall
superiority of the Decoder architecture, the gradually increasing Key-Value
(KV) cache during inference has emerged as a primary efficiency bottleneck,
both in aspects of memory consumption and data transfer bandwidth limitations.
To address these challenges, we propose a paradigm called KV-Latent. By
down-sampling the Key-Value vector dimensions into a latent space, we can
significantly reduce the KV Cache footprint and improve inference speed, only
with a small amount of extra training, less than 1\% of pre-training takes.
Besides, we enhanced the stability of Rotary Positional Embedding applied on
lower-dimensional vectors by modifying its frequency sampling mechanism,
avoiding noise introduced by higher frequencies while retaining position
attenuation. Our experiments, including both models with Grouped Query
Attention and those without, have yielded satisfactory results. Finally, we
conducted comparative experiments to study the impact of separately reducing
Key and Value components on model's performance. Our approach allows for the
construction of more efficient language model systems, and opens the new
possibility on KV Cache saving and efficient LLMs. Our code is available at
https://github.com/ShiLuohe/KV-Latent.

</details>


### [102] [FMC: Formalization of Natural Language Mathematical Competition Problems](https://arxiv.org/abs/2507.11275)
*Jiaxuan Xie,Chengwu Liu,Ye Yuan,Siqi Li,Zhiping Xiao,Ming Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型并结合错误反馈机制的自动化形式化方法，不需要额外训练。通过该流程生成了包含近4000道自然语言数学题目和近10000道Lean形式化题目的高质量数据集，并通过一系列实验验证了其作为自动定理证明基准的价值。


<details>
  <summary>Details</summary>
Motivation: 推动形式化数学推理的发展，需要将大量自然语言数学问题转化为正式语言的数据集，而高效、准确的自动形式化方法正变得越来越关键。现有方法尚存在效率、自动化程度和数据质量等问题。

Method: 作者提出了一个基于大语言模型（LLMs）的自动形式化流程，该流程引入错误反馈机制，通过自动修正输出结果以提升正确性。整个流程无需训练，通过多次采样和反馈机制提升了结果质量。作者利用该流程，将自然语言数学难题自动形式化为Lean语言，并针对不同LLM及参数设置做了能力分析。

Result: 利用提出的方法，作者整理了一个奥赛级别、对齐自然语言与Lean形式化问题的数据集，包含3922道自然语言题与9787道Lean题。其中约64.46%的条目被评为高于平均质量水平。同时，实验证明，通过few-shot、小样本学习、错误反馈和增加采样数量可以显著提升自动形式化表现。此外，三种自动定理证明器测试显示，该数据集难度较高，具有良好的基准价值。

Conclusion: 本文方法实现了自动、高效且无须训练的数学问题形式化流程，产出了高质量的基准数据集，对自动推理和智能定理证明领域具有推动作用。该流程和数据集为后续形式化数学研究提供了重要基础。

Abstract: Efficient and accurate autoformalization methods, which leverage large-scale
datasets of extensive natural language mathematical problems to construct
formal language datasets, are key to advancing formal mathematical reasoning.
In this paper, we propose an autoformalization pipeline based on large language
models with error feedback, achieving a fully automatic and training-free
formalization approach. Using this pipeline, we curate an Olympiad-level
dataset aligning natural language problems with Lean formalizations. The
dataset comprises $3,922$ mathematical problems in natural language and $9,787$
in Lean, of which $64.46\%$ were assessed as at least above-average quality,
making it suitable as a benchmark for automated theorem provers. Additionally,
we investigate the formalization and reasoning capabilities of various LLMs and
empirically demonstrate that few-shot learning, error feedback, and increasing
sampling numbers enhance the autoformalization process. Experiments of three
automated theorem provers on the \dataset\ dataset also highlight its
challenging nature and its value as a benchmark for formal reasoning tasks.

</details>


### [103] [Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks](https://arxiv.org/abs/2507.11292)
*Zewen Bai,Liang Yang,Shengdi Yin,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 本文提出了首个面向中文的span级别仇恨言论数据集，并研究了现有模型对于仇恨言论的理解与解释能力，特别关注难以识别的隐晦仇恨词。通过构建词典并注入模型，显著提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前中文仇恨言论检测研究进展滞后，主要面临精细标注数据稀缺和对隐晦仇恨词语解释能力不足两大挑战，导致模型在真实复杂场景下的可解释性和性能受限。

Method: 1）发布了首个中文仇恨言论span级别数据集（STATE ToxiCN）；2）系统性研究了大模型对编码型（隐晦）仇恨词的理解与解释能力；3）提出将人工注释词典集成到模型中的方法。

Result: 实验表明：a. 利用新的高质量数据集对现有模型的仇恨语义理解做了评价，揭示其局限性；b. 集成词典后，模型在仇恨言论检测任务中的性能得到了显著提升。

Conclusion: 本研究不仅为中文仇恨言论检测提供了新数据和方法，还推动了复杂场景下模型可解释性的发展，对未来相关研究具备重要参考价值。

Abstract: The proliferation of hate speech has inflicted significant societal harm,
with its intensity and directionality closely tied to specific targets and
arguments. In recent years, numerous machine learning-based methods have been
developed to detect hateful comments on online platforms automatically.
However, research on Chinese hate speech detection lags behind, and
interpretability studies face two major challenges: first, the scarcity of
span-level fine-grained annotated datasets limits models' deep semantic
understanding of hate speech; second, insufficient research on identifying and
interpreting coded hate speech restricts model explainability in complex
real-world scenarios. To address these, we make the following contributions:
(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE
ToxiCN), the first span-level Chinese hate speech dataset, and evaluate the
hate semantic understanding of existing models using it. (2) We conduct the
first comprehensive study on Chinese coded hate terms, LLMs' ability to
interpret hate semantics. (3) We propose a method to integrate an annotated
lexicon into models, significantly enhancing hate speech detection performance.
Our work provides valuable resources and insights to advance the
interpretability of Chinese hate speech detection research.

</details>


### [104] [Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian](https://arxiv.org/abs/2507.11299)
*Andrei Niculae,Adrian Cosma,Cosmin Dumitrache,Emilian Rǎdoi*

Main category: cs.CL

TL;DR: 本文介绍了Dr.Copilot系统，用于提升罗马尼亚远程医疗中医生书面回复的表达质量，通过多智能体LLM系统在17个维度上提供反馈，并已在真实环境下取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 尽管文本远程医疗愈发常见，医生建议往往依据表达效果而非临床准确性来评价，为提升回复质量，需要智能系统支持医生改进交流方式。

Method: 设计了Dr.Copilot多智能体LLM系统，包含三种LLM智能体，使用DSPy进行自动化提示词优化，在罗马尼亚语低资源数据下进行训练，并集成到远程医疗平台为医生实时反馈。反馈聚焦于17个易理解的表现维度，而不涉及医学正确性判断。

Result: 在与41位医生的真实部署及实验中，Dr.Copilot系统显著提升了医生回复质量和用户评价。

Conclusion: Dr.Copilot实现了LLM在罗马尼亚医疗实际中的首批部署，验证了其在低资源环境下提升医生交流质量的有效性。

Abstract: Text-based telemedicine has become increasingly common, yet the quality of
medical advice in doctor-patient interactions is often judged more on how
advice is communicated rather than its clinical accuracy. To address this, we
introduce Dr.Copilot , a multi-agent large language model (LLM) system that
supports Romanian-speaking doctors by evaluating and enhancing the presentation
quality of their written responses. Rather than assessing medical correctness,
Dr.Copilot provides feedback along 17 interpretable axes. The system comprises
of three LLM agents with prompts automatically optimized via DSPy. Designed
with low-resource Romanian data and deployed using open-weight models, it
delivers real-time specific feedback to doctors within a telemedicine platform.
Empirical evaluations and live deployment with 41 doctors show measurable
improvements in user reviews and response quality, marking one of the first
real-world deployments of LLMs in Romanian medical settings.

</details>


### [105] [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)
*Haoran Jin,Meng Li,Xiting Wang,Zhihao Xu,Minlie Huang,Yantao Jia,Defu Lian*

Main category: cs.CL

TL;DR: 本文提出了一种名为ConVA的LLM价值观对齐方法，通过直接干预模型内部的激活向量来确保一致的人类价值观，不影响模型性能，且控制效果优异。


<details>
  <summary>Details</summary>
Motivation: 当前LLM需要与人类价值观对齐以提升其透明度、可控性和适应性，但如何高效且不影响模型性能地实现对齐仍是挑战。

Method: 提出受控价值向量激活（ConVA）方法：1）通过解释模型潜在表示中价值的编码方式，直接调整相关激活向量；2）采用上下文受控的价值向量识别，提升解释准确性和无偏性；3）引入门控机制，实现有效且最小度的价值控制。

Result: 在10种基本价值观上，ConVA实现了最高的价值控制成功率，同时保持了LLM的性能和流畅性；在面对具有恶意或相反引导的问题时，也能保证目标价值输出。

Conclusion: ConVA方法为LLM人类价值观对齐提供了可解释、有效且实用的新范式，对LLM实际部署具有重大意义。

Abstract: Aligning Large Language Models (LLMs) with human values has attracted
increasing attention since it provides clarity, transparency, and the ability
to adapt to evolving scenarios. In this paper, we introduce a Controlled Value
Vector Activation (ConVA) method that directly aligns the internal values of
LLMs by interpreting how a value is encoded in their latent representations and
modifies relevant activations to ensure consistent values in LLMs. To ensure an
accurate and unbiased interpretation, we propose a context-controlled value
vector identification method. To consistently control values without
sacrificing model performance, we introduce a gated value vector activation
method for effective and minimum degree of value control. Experiments show that
our method achieves the highest control success rate across 10 basic values
without hurting LLM performance and fluency, and ensures target values even
with opposite and potentially malicious input prompts. Source code and data are
available at~ https://github.com/hr-jin/ConVA.

</details>


### [106] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
*Wenqing Wu,Chengzhi Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种结合人类专家和大语言模型（LLM）知识的新方法，用于辅助预训练语言模型（PLM）自动判别学术论文方法新颖性的任务。通过提取评审报告和论文方法部分的内容，有效推动了新颖性评估的智能化。


<details>
  <summary>Details</summary>
Motivation: 目前学术论文新颖性的评判主要依赖于专家经验或参考文献组合的独特性，但这两者各有局限：专家知识有限，组合法效果不佳且新颖性不确定。需要一种有效整合人类判断与知识型AI能力的新方法来提升新颖性自动判别的效果。

Method: 论文提出融合人类专家和LLM知识，抽取论文同行评审报告中与新颖性相关的句子，并将LLM对论文方法部分的摘要用于微调PLM（如BERT）。此外，设计了文本引导的融合模块（text-guided fusion module）和稀疏注意力机制（Sparse-Attention），以更好地整合人类与LLM知识。

Result: 通过与大量基线方法的对比实验，所提出的方法在方法新颖性判别任务上表现优越，实验结果验证了其有效性和先进性。

Conclusion: 本研究方法能够更好地结合人类判断与大语言模型知识，提升对学术论文方法新颖性的自动化、智能化判别能力，为后续同行评审自动化和学术评价提供了有价值的技术借鉴。

Abstract: Novelty is a crucial criterion in the peer review process for evaluating
academic papers. Traditionally, it's judged by experts or measure by unique
reference combinations. Both methods have limitations: experts have limited
knowledge, and the effectiveness of the combination method is uncertain.
Moreover, it's unclear if unique citations truly measure novelty. The large
language model (LLM) possesses a wealth of knowledge, while human experts
possess judgment abilities that the LLM does not possess. Therefore, our
research integrates the knowledge and abilities of LLM and human experts to
address the limitations of novelty assessment. The most common novelty in
academic papers is the introduction of new methods. In this paper, we propose
leveraging human knowledge and LLM to assist pretrained language models (PLMs,
e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we
extract sentences related to the novelty of the academic paper from peer review
reports and use LLM to summarize the methodology section of the academic paper,
which are then used to fine-tune PLMs. In addition, we have designed a
text-guided fusion module with novel Sparse-Attention to better integrate human
and LLM knowledge. We compared the method we proposed with a large number of
baselines. Extensive experiments demonstrate that our method achieves superior
performance.

</details>


### [107] [What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models](https://arxiv.org/abs/2507.11356)
*Alexis Brissard,Frédéric Cuppens,Amal Zouaq*

Main category: cs.CL

TL;DR: 本文首次系统性地对比了用于大语言模型驱动流程建模任务的多种流程模型表示（PMR），并提出了包含九种PMR的全新数据集。评估结果显示：Mermaid在流程建模整体指标中表现最佳，而BPMN文本在生成流程元素相似度方面最优。


<details>
  <summary>Details</summary>
Motivation: 目前针对流程建模任务，学界引入了多种流程模型表示（PMR），作为模型抽象或生成目标，但这些PMR在结构、复杂度、易用性等方面差异明显，且尚无系统比较。此外，不同的流程建模生成方法评估策略和生成技术各异，缺乏统一比较基准。

Method: 作者构建了PMo数据集，包含55个流程描述及其对应的九种不同PMR所建流程模型。基于这个多PMR数据集，实证评估了这些PMR在大语言模型驱动下的适用性，并分类在流程建模和生成中的表现。

Result: 在六项流程建模评价指标中，Mermaid格式表现最佳。以流程元素相似度衡量，BPMN文本在流程模型生成任务中取得最优结果。

Conclusion: 选用不同的PMR会影响LLM驱动流程建模的效果。Mermaid适合整体流程建模，BPMN文本则利于模型生成准确性。提出的新数据集为今后PMR及生成方法对比提供了基础，也为相关研究提供方向。

Abstract: Large Language Models (LLMs) are increasingly applied for Process Modeling
(PMo) tasks such as Process Model Generation (PMG). To support these tasks,
researchers have introduced a variety of Process Model Representations (PMRs)
that serve as model abstractions or generation targets. However, these PMRs
differ widely in structure, complexity, and usability, and have never been
systematically compared. Moreover, recent PMG approaches rely on distinct
evaluation strategies and generation techniques, making comparison difficult.
This paper presents the first empirical study that evaluates multiple PMRs in
the context of PMo with LLMs. We introduce the PMo Dataset, a new dataset
containing 55 process descriptions paired with models in nine different PMRs.
We evaluate PMRs along two dimensions: suitability for LLM-based PMo and
performance on PMG. \textit{Mermaid} achieves the highest overall score across
six PMo criteria, whereas \textit{BPMN text} delivers the best PMG results in
terms of process element similarity.

</details>


### [108] [Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss](https://arxiv.org/abs/2507.11384)
*Xia Cui*

Main category: cs.CL

TL;DR: 本文提出了在Transformer模型中应用简单加权损失函数以处理多标签情感检测任务的数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 多标签情感识别任务中，由于数据集情感类别分布不均，模型容易忽略少数类情感。为克服这一问题，需要有效的解决方法，同时减少对传统重采样技术带来的计算开销。

Method: 作者针对情感类别不平衡，提出对Transformer模型（BERT、RoBERTa、BART）损失函数进行动态权重调整，提升少数类的学习效果，并在BRIGHTER数据集上进行了相关实验，采用Micro F1、Macro F1、ROC-AUC、准确率和Jaccard相似系数作为评估指标。

Result: 引入加权损失函数后，模型在高频情感类别（占多数类）上的性能有所提升，但对低频情感类别（少数类）的改进有限。

Conclusion: 加权损失函数对于改善多标签情感检测任务中高频类别的识别有效，但在少数类上的提升有限，表明该方法对极度不平衡样本仍存挑战。

Abstract: This paper explores the application of a simple weighted loss function to
Transformer-based models for multi-label emotion detection in SemEval-2025
Shared Task 11. Our approach addresses data imbalance by dynamically adjusting
class weights, thereby enhancing performance on minority emotion classes
without the computational burden of traditional resampling methods. We evaluate
BERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such
as Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.
The results demonstrate that the weighted loss function improves performance on
high-frequency emotion classes but shows limited impact on minority classes.
These findings underscore both the effectiveness and the challenges of applying
this approach to imbalanced multi-label emotion detection.

</details>


### [109] [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)
*Cheng Xu,Nan Yan,Shuhao Guan,Changhong Jin,Yuke Mei,Yibing Guo,M-Tahar Kechadi*

Main category: cs.CL

TL;DR: 本文提出了数据污染风险（DCR）框架，用于检测并量化大语言模型在基准测试中的数据污染问题，通过综合多维度的污染分数调整模型性能评估，更公平地反映真实泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的发展带来了对基准数据污染（BDC）的担忧，即模型在训练过程中可能见过评测数据，导致性能虚高，影响对模型泛化能力的真实评估。

Method: 作者提出DCR框架，从语义、信息、数据、标签四个层面对模型与评测数据的关系进行量化，通过模糊推理系统合成各维度污染分数，形成统一的DCR因子，并用于修正原始准确率。

Result: 在9个不同规模（0.5B-72B）的LLMs上，针对情感分析、假新闻检测、算术推理等任务，DCR框架可有效诊断污染程度，并使调整后的准确率与真实（无污染）基准的误差小于4%。

Conclusion: DCR框架实现了高效且透明的数据污染评估，为大语言模型的公平评测提供实用工具，有助于提升LLM基准测试的可信度。

Abstract: The rapid advancement of large language models (LLMs) has heightened concerns
about benchmark data contamination (BDC), where models inadvertently memorize
evaluation data, inflating performance metrics and undermining genuine
generalization assessment. This paper introduces the Data Contamination Risk
(DCR) framework, a lightweight, interpretable pipeline designed to detect and
quantify BDC across four granular levels: semantic, informational, data, and
label. By synthesizing contamination scores via a fuzzy inference system, DCR
produces a unified DCR Factor that adjusts raw accuracy to reflect
contamination-aware performance. Validated on 9 LLMs (0.5B-72B) across
sentiment analysis, fake news detection, and arithmetic reasoning tasks, the
DCR framework reliably diagnoses contamination severity and with accuracy
adjusted using the DCR Factor to within 4% average error across the three
benchmarks compared to the uncontaminated baseline. Emphasizing computational
efficiency and transparency, DCR provides a practical tool for integrating
contamination assessment into routine evaluations, fostering fairer comparisons
and enhancing the credibility of LLM benchmarking practices.

</details>


### [110] [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)
*LG AI Research,:,Kyunghoon Bae,Eunbi Choi,Kibong Choi,Stanley Jungkyu Choi,Yemuk Choi,Kyubeen Han,Seokhee Hong,Junwon Hwang,Taewan Hwang,Joonwon Jang,Hyojin Jeon,Kijeong Jeon,Gerrard Jeongwon Jo,Hyunjik Jo,Jiyeon Jung,Euisoon Kim,Hyosang Kim,Jihoon Kim,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Youchul Kim,Edward Hwayoung Lee,Gwangho Lee,Haeju Lee,Honglak Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Young Min Paik,Yongmin Park,Youngyong Park,Sanghyun Seo,Sihoon Yang,Heuiyeen Yeen,Sihyuk Yi,Hyeongu Yun*

Main category: cs.CL

TL;DR: EXAONE 4.0是一个新版本的大模型，结合了推理与非推理模式，增强了多语言和工具能力，且提供不同规模模型供下载。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型面临在推理能力和易用性之间权衡，以及迈向Agent时代对模型多样功能（如工具用例、多语言支持等）的需求。

Method: 提出多模式架构（推理/非推理），引入Agent工具使用，扩展多语言能力（新增西班牙语），并推出两种规模（32B和1.2B），覆盖高性能到设备端应用。

Result: EXAONE 4.0在同类开源模型中表现优越，对比前沿模型也具备竞争力。

Conclusion: EXAONE 4.0融合强大推理与实用性，并支持更丰富的应用场景，相关模型开放下载，助力Agent AI发展。

Abstract: This technical report introduces EXAONE 4.0, which integrates a Non-reasoning
mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5
and the advanced reasoning abilities of EXAONE Deep. To pave the way for the
agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool
use, and its multilingual capabilities are extended to support Spanish in
addition to English and Korean. The EXAONE 4.0 model series consists of two
sizes: a mid-size 32B model optimized for high performance, and a small-size
1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates
superior performance compared to open-weight models in its class and remains
competitive even against frontier-class models. The models are publicly
available for research purposes and can be easily downloaded via
https://huggingface.co/LGAI-EXAONE.

</details>


### [111] [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)
*Soumadeep Saha,Akshay Chaturvedi,Saptarshi Saha,Utpal Garain,Nicholas Asher*

Main category: cs.CL

TL;DR: 本文通过引入因果链式思考图（CCG），探索链式思考（CoT）对大语言模型推理任务性能提升的机制，并构建了KisMATH数据集以支持进一步研究。


<details>
  <summary>Details</summary>
Motivation: 目前链式思考已被证明能提升大语言模型的推理能力，但其提升机制尚无定论。作者希望通过更细粒度的因果依赖建模，揭示这一机制。

Method: 作者提出了因果链式思考图（CCG），自动从推理过程生成有向无环图，刻画语言模型输出中的细粒度因果依赖。同时，作者基于MATH500、GSM8K和AIME数学推理题，构建了包含1671个问题及其CCG的KisMATH数据集，并在15个开源大模型上开展实证分析。

Result: 实证分析表明：（1）CCG中的推理节点是最终答案的中介，符合必要的推理条件；（2）大语言模型的推理路径与CCG高度相关，说明模型内部形成了类似的结构。

Conclusion: KisMATH数据集支持基于图结构的可控干预，为深入理解链式思考在大语言模型推理中的作用提供了新途径。

Abstract: Chain-of-thought traces have been shown to improve performance of large
language models in a plethora of reasoning tasks, yet there is no consensus on
the mechanism through which this performance boost is achieved. To shed more
light on this, we introduce Causal CoT Graphs (CCGs), which are directed
acyclic graphs automatically extracted from reasoning traces that model
fine-grained causal dependencies in the language model output. A collection of
$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their
associated CCGs are compiled into our dataset -- \textbf{KisMATH}. Our detailed
empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in
the CCG are mediators for the final answer, a condition necessary for
reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating
that models internally realise structures akin to our graphs. KisMATH enables
controlled, graph-aligned interventions and opens up avenues for further
investigation into the role of chain-of-thought in LLM reasoning.

</details>


### [112] [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)
*Orion Weller,Kathryn Ricci,Marc Marone,Antoine Chaffin,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文提出一套大规模、公开的Ettin模型集，系统对比了编码器（encoder-only）和解码器（decoder-only）架构在不同任务和参数规模下的表现，并开源了全部训练数据和模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大多数LLM社区关注于易于生成文本的解码器架构，但仍有大量用户依赖编码器架构完成分类、检索等任务。此前对比研究由于模型参数、训练方案、数据集不一致，结果缺乏说服力。因此，作者旨在提供公平、系统的架构对比。

Method: 作者构建并训练了一系列参数规模从1700万到10亿的编码器和解码器模型，采用完全相同的训练配方和数据（高达2万亿token），系统评估两种架构在分类、检索和生成任务上的表现，同时测试不同架构适应对方任务的能力。

Result: 相同训练配方下，编码器在分类和检索任务上表现最佳，解码器则在生成类任务中领先。通常，直接用原生架构执行对方擅长的任务要优于通过继续训练适配转化。例如，400M参数的编码器在MNLI任务上超越了1B的解码器，生成能力则相反。

Conclusion: 针对公平对比LLM两种主流架构，该工作系统展示了各自优势，并提供开放数据和模型，便于社区进行进一步研究和扩展，推动LLM生态发展。

Abstract: The large language model (LLM) community focuses almost exclusively on
decoder-only language models, since they are easier to use for text generation.
However, a large subset of the community still uses encoder-only models for
tasks such as classification or retrieval. Previous work has attempted to
compare these architectures, but is forced to make comparisons with models that
have different numbers of parameters, training techniques, and datasets. We
introduce the SOTA open-data Ettin suite of models: paired encoder-only and
decoder-only models ranging from 17 million parameters to 1 billion, trained on
up to 2 trillion tokens. Using the same recipe for both encoder-only and
decoder-only models produces SOTA recipes in both categories for their
respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as
decoders. Like previous work, we find that encoder-only models excel at
classification and retrieval tasks while decoders excel at generative tasks.
However, we show that adapting a decoder model to encoder tasks (and vice
versa) through continued training is subpar compared to using only the reverse
objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa
for generative tasks). We open-source all artifacts of this study including
training data, training order segmented by checkpoint, and 200+ checkpoints to
allow future work to analyze or extend all aspects of training.

</details>


### [113] [Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?](https://arxiv.org/abs/2507.11423)
*Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Thierry Charnois*

Main category: cs.CL

TL;DR: 本论文探讨了大模型推理策略选择的问题，并提出用提示词引导以提升逻辑推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现，LLM倾向于使用单一推理策略，这限制了其在复杂问题上的效果。该研究希望通过控制和引导LLM选择不同推理策略，提升其问题解决能力。

Method: 通过设计不同的提示词，尝试引导LLM采用不同的推理策略，并评估这些策略对逻辑问题求解准确率的影响。同时，提出方法让模型能自适应选择最优推理策略。

Result: 结果显示，没有单一推理策略可以在所有情况下提升准确率，但如果模型能自适应选择最合适的策略，性能有望提升。

Conclusion: 提示词能够引导LLM采用不同推理策略，但要显著提升推理表现，需要开发模型的自适应策略选择能力。

Abstract: Human reasoning involves different strategies, each suited to specific
problems. Prior work shows that large language model (LLMs) tend to favor a
single reasoning strategy, potentially limiting their effectiveness in diverse
reasoning challenges. In this work, we investigate whether prompting can
control LLMs reasoning strategies and assess its impact on logical
problem-solving. While our experiments show that no single strategy
consistently improves accuracy, performance could be enhanced if models could
adaptively choose the optimal strategy. We propose methods to guide LLMs in
strategy selection, highlighting new ways to refine their reasoning abilities.

</details>


### [114] [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)
*Sirui Han,Junqi Zhu,Ruiyuan Zhang,Yike Guo*

Main category: cs.CL

TL;DR: 本文介绍了HKGAI-V1，一款针对香港多语言、独特制度和价值观量身定制的大型语言模型，技术上基于DeepSeek架构，通过全面调优对地区规范进行对齐，并结合检索增强生成功能，实现更准确可靠的信息获取。论文还开发了专有的评测基准，有效评估模型的本地伦理与法律对齐度。结果显示模型在处理与香港相关的敏感问题上优于通用模型，为区域性AI研发树立了范例。


<details>
  <summary>Details</summary>
Motivation: 现有通用大模型无法满足香港在语言、法律、文化等方面的独特需求，尤其在数字主权、公共服务和本地价值观对齐领域存在不足，因此急需建立一套符合本地实际的基础模型和评估体系。

Method: 基于DeepSeek架构，通过全面参数微调，将模型与香港多语言（粤语、普通话、英语）、‘一国两制’下的法律体系、本地文化价值对齐；集成检索增强生成系统提升事实准确性；同时设计专有的Adversarial HK Value Benchmark严格测试模型在本地伦理、法律情境下的表现。

Result: HKGAI-V1模型在涉及香港地方文化、法律等敏感问题的处理上，优于现有的通用大型语言模型。模型通过“治理嵌入”机制提升本地主控性，可服务于港府、法律、教育等关键领域。Adversarial HK Value Benchmark作为评估工具，能有效识别模型对本地标准的适应度。

Conclusion: 本文不仅开发了符合香港本地需求的大语言模型和评测标准，为香港数字主权和本地AI应用提供支撑，还为全球其他地区开发区域型、价值对齐AI模型提供了可参考的技术和方法模板。

Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign
large language model (LLM), developed as part of an initiative to establish
value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing
the region's unique multilingual environment (Cantonese, Mandarin, and
English), its distinct socio-legal context under the "one country, two systems"
framework, and specific local cultural and value considerations, the model is
built upon the DeepSeek architecture and systematically aligned with regional
norms through a multifaceted full parameter fine-tuning process. It is further
integrated with a retrieval-augmented generation (RAG) system to ensure timely
and factually grounded information access. The core contribution lies in the
design and implementation of a comprehensive, region-specific AI alignment and
safety framework, demonstrated through two key achievements: 1) The successful
development of HKGAI-V1 itself - which outper-forms general-purpose models in
handling Hong Kong-specific culturally sensitive queries, and embodies a
"governance-embedded" approach to digital sovereignty - empowers Hong Kong to
exercise control over AI applications in critical sectors including public
services, legal systems, and edu-cation. 2) The development of the proprietary
Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment
with local ethical and legal stand-ards under challenging conditions. By
documenting these achievements, the paper provides not only a technological
artifact but also a replicable blueprint for developing advanced, regionally
focused AI systems deeply rooted in their local identities.

</details>


### [115] [Real-World Summarization: When Evaluation Reaches Its Limits](https://arxiv.org/abs/2507.11508)
*Patrícia Schmidtová,Ondřej Dušek,Saad Mahamood*

Main category: cs.CL

TL;DR: 本文研究了如何评估酒店亮点摘要（由大语言模型生成的住宿特色简介）的真实性，通过人工评估和多种自动化方法比较不同评价指标，指出简单的词重叠指标在实际应用中表现意外优异，并讨论了评估中存在的挑战和业务风险。


<details>
  <summary>Details</summary>
Motivation: 酒店等住宿服务往往依赖高质量、真实的摘要向用户展示自身特色，因此需要有效手段来衡量由大语言模型生成的摘要内容是否真实可信。

Method: 作者设计并执行了人工评测，包括分类错误评估和具体片段标注；随后，将人工结果与传统自动评测指标、可训练模型及“大模型判官”法进行比较，分析各自优劣。

Result: 结果发现，简单的词重叠指标（如ROUGE等）与人工判断有较高相关性（Spearman相关系数0.63），在跨领域应用时较复杂方法表现更佳。同时，虽然大模型可以生成优质摘要，但作为评估工具时，经常过度或不足标注，不够可靠。

Conclusion: 结论认为复杂自动评测方法未必优于简单指标，且大模型本身不适合用作评测。文中还指出，错误和无法校验的信息给实际业务带来较大风险，并强调众包评测本身存在挑战。

Abstract: We examine evaluation of faithfulness to input data in the context of hotel
highlights: brief LLM-generated summaries that capture unique features of
accommodations. Through human evaluation campaigns involving categorical error
assessment and span-level annotation, we compare traditional metrics, trainable
methods, and LLM-as-a-judge approaches. Our findings reveal that simpler
metrics like word overlap correlate surprisingly well with human judgments
(Spearman correlation rank of 0.63), often outperforming more complex methods
when applied to out-of-domain data. We further demonstrate that while LLMs can
generate high-quality highlights, they prove unreliable for evaluation as they
tend to severely under- or over-annotate. Our analysis of real-world business
impacts shows incorrect and non-checkable information pose the greatest risks.
We also highlight challenges in crowdsourced evaluations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [116] [Learning to Move in Rhythm: Task-Conditioned Motion Policies with Orbital Stability Guarantees](https://arxiv.org/abs/2507.10602)
*Maximilian Stölzle,T. Konstantin Rusch,Zach J. Patterson,Rodrigo Pérez-Dattari,Francesco Stella,Josie Hughes,Cosimo Della Santina,Daniela Rus*

Main category: cs.RO

TL;DR: 本文提出轨道稳定运动基元（OSMPs）方法，通过学习的可微编码器与Hopf分岔在潜在空间结合，实现了对复杂周期动作的高效学习与泛化，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的动态运动基元（DMPs）虽具备稳定性和鲁棒性，但在表达复杂周期性运动和任务间插值方面有限，限制了其在实际如步态或节奏性工具操作等任务中的应用。

Method: 作者引入了一种新的轨道稳定运动基元（OSMP）框架，采用学习的双射编码器与潜在空间的超临界Hopf分岔结合，既能确保周期动作的准确学习也能提供严格的轨道稳定性和横向收敛性。另外，编码器可按任务调节，使单一策略能泛化表示多种动作目标，实现零样本泛化。

Result: 在多种仿真和实际机器人平台上，包括协作机械臂、软体机械手以及刚软结合的仿生龟型机器人，实验验证了该方法的强泛化能力与卓越表现，且相较于扩散策略等现有顶尖基线有明显优势。

Conclusion: OSMPs 能稳定高效地学习与泛化复杂周期性机器人动作，显著扩展了基于示教学习在多任务场景中的应用边界。

Abstract: Learning from demonstration provides a sample-efficient approach to acquiring
complex behaviors, enabling robots to move robustly, compliantly, and with
fluidity. In this context, Dynamic Motion Primitives offer built - in stability
and robustness to disturbances but often struggle to capture complex periodic
behaviors. Moreover, they are limited in their ability to interpolate between
different tasks. These shortcomings substantially narrow their applicability,
excluding a wide class of practically meaningful tasks such as locomotion and
rhythmic tool use. In this work, we introduce Orbitally Stable Motion
Primitives (OSMPs) - a framework that combines a learned diffeomorphic encoder
with a supercritical Hopf bifurcation in latent space, enabling the accurate
acquisition of periodic motions from demonstrations while ensuring formal
guarantees of orbital stability and transverse contraction. Furthermore, by
conditioning the bijective encoder on the task, we enable a single learned
policy to represent multiple motion objectives, yielding consistent zero-shot
generalization to unseen motion objectives within the training distribution. We
validate the proposed approach through extensive simulation and real-world
experiments across a diverse range of robotic platforms - from collaborative
arms and soft manipulators to a bio-inspired rigid-soft turtle robot -
demonstrating its versatility and effectiveness in consistently outperforming
state-of-the-art baselines such as diffusion policies, among others.

</details>


### [117] [Vision Language Action Models in Robotic Manipulation: A Systematic Review](https://arxiv.org/abs/2507.10672)
*Muhayy Ud Din,Waseem Akram,Lyes Saad Saoud,Jan Rosell,Irfan Hussain*

Main category: cs.RO

TL;DR: 本文综述了视觉-语言-动作（VLA）模型在机器人领域的发展，系统总结分析了相关模型、数据集与仿真平台，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着机器人应用日益复杂，如何统一视觉感知、自然语言理解和动作控制成为难题。VLA模型试图将三者集成于一个学习框架中，推动机器人向自主、通用任务执行能力发展。

Method: 作者综合梳理了102种VLA模型、26个基础数据集及12个仿真平台，并基于模型结构、数据集任务复杂度与多模态性，提出了分类与评估标准。建立了二维框架对数据集进行语义丰富度与多模态对齐性分析，并对仿真环境的评测能力进行了评价。

Result: 通过系统综述，展示了VLA模型及其所依赖的数据集和平台的全景图，明确各自的特点与不足，并指出了当前数据与模型在多模态对齐、泛化能力与实际部署等方面的挑战。

Conclusion: VLA模型在推动机器人智能化与自主化具有重要价值，未来应关注可扩展的预训练、模块化设计及高效多模态对齐等方向。本文为从数据集到实际部署的机器人研究提供了全面参考与指导。

Abstract: Vision Language Action (VLA) models represent a transformative shift in
robotics, with the aim of unifying visual perception, natural language
understanding, and embodied control within a single learning framework. This
review presents a comprehensive and forward-looking synthesis of the VLA
paradigm, with a particular emphasis on robotic manipulation and
instruction-driven autonomy. We comprehensively analyze 102 VLA models, 26
foundational datasets, and 12 simulation platforms that collectively shape the
development and evaluation of VLAs models. These models are categorized into
key architectural paradigms, each reflecting distinct strategies for
integrating vision, language, and control in robotic systems. Foundational
datasets are evaluated using a novel criterion based on task complexity,
variety of modalities, and dataset scale, allowing a comparative analysis of
their suitability for generalist policy learning. We introduce a
two-dimensional characterization framework that organizes these datasets based
on semantic richness and multimodal alignment, showing underexplored regions in
the current data landscape. Simulation environments are evaluated for their
effectiveness in generating large-scale data, as well as their ability to
facilitate transfer from simulation to real-world settings and the variety of
supported tasks. Using both academic and industrial contributions, we recognize
ongoing challenges and outline strategic directions such as scalable
pretraining protocols, modular architectural design, and robust multimodal
alignment strategies. This review serves as both a technical reference and a
conceptual roadmap for advancing embodiment and robotic control, providing
insights that span from dataset generation to real world deployment of
generalist robotic agents.

</details>


### [118] [Exteroception through Proprioception Sensing through Improved Contact Modeling for Soft Growing Robots](https://arxiv.org/abs/2507.10694)
*Francesco Fuentes,Serigne Diagne,Zachary Kingston,Laura H. Blumenschein*

Main category: cs.RO

TL;DR: 本文提出利用软体增长机器人进行环境探索与地图构建，基于其被动变形能力，通过建模碰撞行为和开发几何仿真器，实现高效环境感知与地图构建。


<details>
  <summary>Details</summary>
Motivation: 软体机器人的被动变形能力让其在复杂非结构环境中表现优异。若能深入理解其碰撞和变形过程，可利用其触觉信息反推环境结构，实现地图绘制与探索功能。

Method: 首先，作者对软体增长机器人在执行离散转弯过程中的碰撞行为进行建模和特征化。然后，基于该模型开发了二维环境下的几何仿真器，用于预测机器人轨迹。最后，结合蒙特卡洛采样方法，估算在当前已知信息下的最优下一步机器人布置，实现环境探索和地图构建。

Result: 通过仿真及实际应用测试，提出的仿真器和蒙特卡洛采样决策方法在不同结构（均匀和非均匀）的环境中均能迅速趋近于最优动作选择结果，有效实现环境的探索与感知。

Conclusion: 软体增长机器人凭借其被动变形的优势，结合合理的仿真与决策机制，可作为非结构化环境下高效地图绘制和探索的有力工具，显示出广阔的应用前景。

Abstract: Passive deformation due to compliance is a commonly used benefit of soft
robots, providing opportunities to achieve robust actuation with few active
degrees of freedom. Soft growing robots in particular have shown promise in
navigation of unstructured environments due to their passive deformation. If
their collisions and subsequent deformations can be better understood, soft
robots could be used to understand the structure of the environment from direct
tactile measurements. In this work, we propose the use of soft growing robots
as mapping and exploration tools. We do this by first characterizing collision
behavior during discrete turns, then leveraging this model to develop a
geometry-based simulator that models robot trajectories in 2D environments.
Finally, we demonstrate the model and simulator validity by mapping unknown
environments using Monte Carlo sampling to estimate the optimal next deployment
given current knowledge. Over both uniform and non-uniform environments, this
selection method rapidly approaches ideal actions, showing the potential for
soft growing robots in unstructured environment exploration and mapping.

</details>


### [119] [RCG: Safety-Critical Scenario Generation for Robust Autonomous Driving via Real-World Crash Grounding](https://arxiv.org/abs/2507.10749)
*Benjamin Stoler,Juliet Yang,Jonathan Francis,Jean Oh*

Main category: cs.RO

TL;DR: 本文提出了一种新的场景生成框架Real-world Crash Grounding（RCG），能够生成更贴近真实高危事故的自动驾驶训练和评估场景，有效提升自动驾驶系统在极端状况下的表现。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶安全评估依赖于大量安全关键场景，但这类高危场景在真实数据中极为稀缺，导致现有系统训练和测试不充分。为了解决真实事故场景稀缺问题，急需能生成高风险且真实的模拟对抗行为场景。

Method: 作者提出RCG框架，结合大规模驾驶日志的对比式预训练与小型含事故数据的微调，获取能表现高危行为的行为表示。这种表示用于选择具备高风险且行为真实的对手轨迹，并将该机制接入现有两种场景生成管线，用嵌入判据替代人工打分目标。

Result: 实验表明：基于RCG框架生成的场景对自动驾驶系统训练有显著提升作用，7种评测环境平均提升9.2%成功率。定量与定性分析都显示，本方法能生成更为细致与真实的对手行为场景。

Conclusion: RCG方法不仅提升了自动驾驶系统在安全关键场景下的鲁棒性，也促进了更有效、现实的极端场景测试，具有实际应用与推广价值。代码和工具将开源，有利于社区进一步研究。

Abstract: Safety-critical scenarios are essential for training and evaluating
autonomous driving (AD) systems, yet remain extremely rare in real-world
driving datasets. To address this, we propose Real-world Crash Grounding (RCG),
a scenario generation framework that integrates crash-informed semantics into
adversarial perturbation pipelines. We construct a safety-aware behavior
representation through contrastive pre-training on large-scale driving logs,
followed by fine-tuning on a small, crash-rich dataset with approximate
trajectory annotations extracted from video. This embedding captures semantic
structure aligned with real-world accident behaviors and supports selection of
adversary trajectories that are both high-risk and behaviorally realistic. We
incorporate the resulting selection mechanism into two prior scenario
generation pipelines, replacing their handcrafted scoring objectives with an
embedding-based criterion. Experimental results show that ego agents trained
against these generated scenarios achieve consistently higher downstream
success rates, with an average improvement of 9.2% across seven evaluation
settings. Qualitative and quantitative analyses further demonstrate that our
approach produces more plausible and nuanced adversary behaviors, enabling more
effective and realistic stress testing of AD systems. Code and tools will be
released publicly.

</details>


### [120] [rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding](https://arxiv.org/abs/2507.10776)
*Howard H. Qian,Yiting Chen,Gaotian Wang,Podshara Chanrungmaneekul,Kaiyu Hang*

Main category: cs.RO

TL;DR: 本文提出了一种新的实时交互感知框架rt-RISeg，用于机器人在与环境交互的过程中无监督地对未见物体进行分割，相比现有方法精度提升27.5%。


<details>
  <summary>Details</summary>
Motivation: 现有未见物体实例分割（UOIS）方法依赖于大量静态数据训练，容易过拟合，泛化到新环境时表现较差。作者认为视觉本质上应与交互紧密结合，因此希望通过机器人与物体的实时交互提升分割泛化能力。

Method: 提出rt-RISeg框架：通过机器人的一系列交互动作，引入新的机体参考帧，并基于旋转和线速度变化提取设计的帧不变特征（BFIF），无需训练模型即可实时、持续地对未见物体进行分割，动态生成和更新分割掩码，打破静态视觉的局限。

Result: 与当前最先进的UOIS方法相比，该方法分割精度平均提升了27.5%。同时，自动生成的分割掩码还能作为提示词，显著提升视觉基础模型的性能。

Conclusion: rt-RISeg实现了真正自包含、实时交互式的物体分割，能在多种机器人交互场景中获得优异泛化能力，为未见物体实例分割及相关视觉任务的实际应用提供了新思路。

Abstract: Successful execution of dexterous robotic manipulation tasks in new
environments, such as grasping, depends on the ability to proficiently segment
unseen objects from the background and other objects. Previous works in unseen
object instance segmentation (UOIS) train models on large-scale datasets, which
often leads to overfitting on static visual features. This dependency results
in poor generalization performance when confronted with out-of-distribution
scenarios. To address this limitation, we rethink the task of UOIS based on the
principle that vision is inherently interactive and occurs over time. We
propose a novel real-time interactive perception framework, rt-RISeg, that
continuously segments unseen objects by robot interactions and analysis of a
designed body frame-invariant feature (BFIF). We demonstrate that the relative
rotational and linear velocities of randomly sampled body frames, resulting
from selected robot interactions, can be used to identify objects without any
learned segmentation model. This fully self-contained segmentation pipeline
generates and updates object segmentation masks throughout each robot
interaction without the need to wait for an action to finish. We showcase the
effectiveness of our proposed interactive perception method by achieving an
average object segmentation accuracy rate 27.5% greater than state-of-the-art
UOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show
that the autonomously generated segmentation masks can be used as prompts to
vision foundation models for significantly improved performance.

</details>


### [121] [Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection](https://arxiv.org/abs/2507.10814)
*Huiyi Wang,Fahim Shahriar,Alireza Azimi,Gautham Vasan,Rupam Mahmood,Colin Bellinger*

Main category: cs.RO

TL;DR: 本文提出了结合大规模预训练模型（如大语言模型和目标检测器）提升机器人感知能力，用于强化学习中的目标条件操作，实现更通用、高效的机械臂抓取任务。通过掩码目标条件方法，机器人能准确识别并操作多种物体，实验结果显示其成功率高、泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人操作任务泛化性弱，对不同对象的学习极其耗时和资源。随着大模型的预训练技术进步，机器人能利用这些模型实现对多类别物体和复杂场景的理解，有望显著提升机器人在实际生活和工作环境中的实用性和适应性。

Method: 本论文将预训练的目标检测模型集成到目标条件强化学习框架中。具体做法是：利用模型根据文本提示识别目标物体，并生成掩码信息用于目标条件。该掩码方式为机器人提供了和具体对象无关的提示，加强了特征共享和任务泛化能力。

Result: 在模拟抓取任务中，该方法对于分布内和分布外的新对象都能够保持约90%的抓取成功率，同时收敛速度更快，获得更高收益。

Conclusion: 将大规模预训练模型与目标条件强化学习相结合能极大提升机器人抓取任务的适应性和泛化性，掩码目标条件方法为未来通用型机器人感知与操作提供了有效路径。

Abstract: General-purpose robotic manipulation, including reach and grasp, is essential
for deployment into households and workspaces involving diverse and evolving
tasks. Recent advances propose using large pre-trained models, such as Large
Language Models and object detectors, to boost robotic perception in
reinforcement learning. These models, trained on large datasets via
self-supervised learning, can process text prompts and identify diverse objects
in scenes, an invaluable skill in RL where learning object interaction is
resource-intensive. This study demonstrates how to integrate such models into
Goal-Conditioned Reinforcement Learning to enable general and versatile robotic
reach and grasp capabilities. We use a pre-trained object detection model to
enable the agent to identify the object from a text prompt and generate a mask
for goal conditioning. Mask-based goal conditioning provides object-agnostic
cues, improving feature sharing and generalization. The effectiveness of the
proposed framework is demonstrated in a simulated reach-and-grasp task, where
the mask-based goal conditioning consistently maintains a $\sim$90\% success
rate in grasping both in and out-of-distribution objects, while also ensuring
faster convergence to higher returns.

</details>


### [122] [Mixed Discrete and Continuous Planning using Shortest Walks in Graphs of Convex Sets](https://arxiv.org/abs/2507.10878)
*Savva Morozov,Tobia Marcucci,Bernhard Paus Graesdal,Alexandre Amice,Pablo A. Parrilo,Russ Tedrake*

Main category: cs.RO

TL;DR: 该论文研究了凸集图上的最短路径问题，并提出了一种基于半正定规划与增量式搜索的高效近似算法，实现了多种机器人规划任务的统一建模与高效计算。


<details>
  <summary>Details</summary>
Motivation: 离散-连续混合规划问题在机器人学中普遍存在，但传统方法通常需对每类问题设计专用解法，难以统一并且效率有限。作者希望通过一种通用的建模与高效求解框架，将这类问题统一于一个理论与算法体系下。

Method: 作者提出将每个节点与一个凸优化程序关联，把约束和代价体现在图的边上。使用半正定规划合成分段二次型的cost-to-go函数下界，并以此为启发引导增量式搜索算法，近似求解最短路径（即累计最优值最小的游走）。

Result: 实验表明，所提方法可高效地应用于机器人避障运动规划、技能链规划以及混合系统最优控制等多种离散-连续混合问题；表现出高效性和统一性。

Conclusion: 利用凸集图建模，把多个传统需要专用算法的问题整合为统一框架，所提方法不仅建模灵活，还能高效近似求解，有望为机器人等领域带来理论和实践突破。

Abstract: We study the Shortest-Walk Problem (SWP) in a Graph of Convex Sets (GCS). A
GCS is a graph where each vertex is paired with a convex program, and each edge
couples adjacent programs via additional costs and constraints. A walk in a GCS
is a sequence of vertices connected by edges, where vertices may be repeated.
The length of a walk is given by the cumulative optimal value of the
corresponding convex programs. To solve the SWP in GCS, we first synthesize a
piecewise-quadratic lower bound on the problem's cost-to-go function using
semidefinite programming. Then we use this lower bound to guide an
incremental-search algorithm that yields an approximate shortest walk. We show
that the SWP in GCS is a natural language for many mixed discrete-continuous
planning problems in robotics, unifying problems that typically require
specialized solutions while delivering high performance and computational
efficiency. We demonstrate this through experiments in collision-free motion
planning, skill chaining, and optimal control of hybrid systems.

</details>


### [123] [Object-Centric Mobile Manipulation through SAM2-Guided Perception and Imitation Learning](https://arxiv.org/abs/2507.10899)
*Wang Zhicheng,Satoshi Yagi,Satoshi Yamamori,Jun Morimoto*

Main category: cs.RO

TL;DR: 现有移动操作系统往往将导航和操作分离，导致操作误差增加。本文提出基于SAM2的物体中心方法，将操作朝向信息纳入模型，提高操作任务在不同朝向下的泛化能力，在自主搭建的移动机械臂上实现了更鲁棒的模仿学习。


<details>
  <summary>Details</summary>
Motivation: 现有移动操作领域通常先导航再操作，导航误差、尤其是接近角度的偏差会导致操作性能下降。只有当机械臂能从不同方向完成同一任务，才有可能真正提升其泛用性。提高系统对操作角度的适应能力成为该领域的重要挑战。

Method: 提出一种基于SAM2模型、加入操作朝向的物体中心方法。SAM2能够在图像中实现提示式分割，结合操作朝向，使模型能对同一任务在不同朝向下有一致理解。方法在定制的移动机械臂平台上实现和部署，并测试了夹取和放置任务，涵盖多种朝向。

Result: 与Action Chunking Transformer方法相比，在训练数据包含多种接近角度示范的情况下，所提模型具有更强的泛化能力。实验表明，模型能在不同朝向角度下维持优越的模仿学习操作表现。

Conclusion: 该方法显著提升了基于模仿学习的移动操作系统的泛化性与鲁棒性，为构建通用型机器人操作模型奠定了基础。

Abstract: Imitation learning for mobile manipulation is a key challenge in the field of
robotic manipulation. However, current mobile manipulation frameworks typically
decouple navigation and manipulation, executing manipulation only after
reaching a certain location. This can lead to performance degradation when
navigation is imprecise, especially due to misalignment in approach angles. To
enable a mobile manipulator to perform the same task from diverse orientations,
an essential capability for building general-purpose robotic models, we propose
an object-centric method based on SAM2, a foundation model towards solving
promptable visual segmentation in images, which incorporates manipulation
orientation information into our model. Our approach enables consistent
understanding of the same task from different orientations. We deploy the model
on a custom-built mobile manipulator and evaluate it on a pick-and-place task
under varied orientation angles. Compared to Action Chunking Transformer, our
model maintains superior generalization when trained with demonstrations from
varied approach angles. This work significantly enhances the generalization and
robustness of imitation learning-based mobile manipulation systems.

</details>


### [124] [Fast Non-Episodic Adaptive Tuning of Robot Controllers with Online Policy Optimization](https://arxiv.org/abs/2507.10914)
*James A. Preiss,Fengze Xie,Yiheng Lin,Adam Wierman,Yisong Yue*

Main category: cs.RO

TL;DR: 本文提出了一种适用于动态变化任务场景下的单轨迹模型驱动在线策略优化算法M-GAPS，并在硬件实验中展现出优良的自适应性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 机器人控制器在实际环境中往往面临动力学、策略空间和目标不断变化，现有方法很难高效、稳定应对这些变化，且大多依赖人工划分任务阶段（episode）。作者希望实现无需重置、能实时适应动态环境的在线优化算法。

Method: 提出M-GAPS算法，用于在没有状态重置、信息未知且持续变化的情况下，基于模型单轨迹优化策略参数；引入对四旋翼状态空间与策略类的重新参数化，以改善优化空间结构。与现有模型驱动和无模型基线方法在硬件（四旋翼、Ackermann小车）上进行对比实验。

Result: M-GAPS算法相较于现有基线方法，能够更快找到接近最优的参数，特别是在不利于人工episode设定时效果更优。同时M-GAPS能迅速应对未知强风和载荷等突发干扰，对不同类型机器人均展现良好泛化性和鲁棒性。

Conclusion: M-GAPS等单轨迹模型驱动在线优化方法在硬件上具有实用性，灵活度高于经典自适应控制，且比深度强化学习算法更稳定、更高效。

Abstract: We study online algorithms to tune the parameters of a robot controller in a
setting where the dynamics, policy class, and optimality objective are all
time-varying. The system follows a single trajectory without episodes or state
resets, and the time-varying information is not known in advance. Focusing on
nonlinear geometric quadrotor controllers as a test case, we propose a
practical implementation of a single-trajectory model-based online policy
optimization algorithm, M-GAPS,along with reparameterizations of the quadrotor
state space and policy class to improve the optimization landscape. In hardware
experiments,we compare to model-based and model-free baselines that impose
artificial episodes. We show that M-GAPS finds near-optimal parameters more
quickly, especially when the episode length is not favorable. We also show that
M-GAPS rapidly adapts to heavy unmodeled wind and payload disturbances, and
achieves similar strong improvement on a 1:6-scale Ackermann-steered car. Our
results demonstrate the hardware practicality of this emerging class of online
policy optimization that offers significantly more flexibility than classic
adaptive control, while being more stable and data-efficient than model-free
reinforcement learning.

</details>


### [125] [Unified Modeling and Structural Optimization of Multi-magnet Embedded Soft Continuum Robots for Enhanced Kinematic Performances](https://arxiv.org/abs/2507.10950)
*Zhiwei Wu,Jiahao Luo,Siyi Wei,Jinhui Zhang*

Main category: cs.RO

TL;DR: 提出用于优化多磁体嵌入式软体连续体机器人（MeSCRs）运动学性能的统一建模与优化方法，通过可微系统建模和结构优化，显著提升机器人可控自由度和运动性能。


<details>
  <summary>Details</summary>
Motivation: 多磁体嵌入式软体连续体机器人具有柔性与可控性强的优点，但其运动学性能和自由度受结构设计限制。论文旨在通过建模与优化提升其性能。

Method: 采用扩展伪刚体模型建立可微分系统公式，分析磁驱动下的机器人平衡及运动配置。提出基于微分几何的结构优化框架，将运动学性能指标与磁体结构配置关联，并通过解析与数值方法给出最优磁体配置。

Result: 证明了MeSCRs最大可控自由度为磁体数量的两倍，提出的优化条件可通过调整配置空间度量的谱实现性能提升。推导出典型条件下的磁体最优配置，并通过仿真验证优化框架的有效性。

Conclusion: 所提统一建模与优化框架能有效提升多磁体嵌入式软体连续体机器人的运动学性能，为其结构优化设计提供了理论与方法支持。

Abstract: This paper presents a unified modeling and optimization framework to enhance
the kinematic performance of multi-magnet embedded soft continuum robots
(MeSCRs). To this end, we establish a differentiable system formulation based
on an extended pseudo-rigid-body model. This formulation enables analysis of
the equilibrium well-posedness and the geometry of the induced configuration
under magnetic actuation. In particular, we show that the maximum controllable
degrees of freedom of a MeSCR equal twice the number of embedded magnets. We
subsequently develop a structural optimization framework based on differential
geometry that links classical kinematic measures (e.g., manipulability and
dexterity) to the configuration of embedded magnets. The resulting optimization
condition reveals that improving local performance requires structurally
modulating the spectrum of the configuration space metric to counteract its
distortion. Closed-form solutions for optimal magnet configurations are derived
under representative conditions, and a gradient-based numerical method is
proposed for general design scenarios. Simulation studies validate the
effectiveness of the proposed framework.

</details>


### [126] [Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction](https://arxiv.org/abs/2507.10960)
*He Zhu,Ryo Miyoshi,Yuki Okafuji*

Main category: cs.RO

TL;DR: 本文提出一种基于Transformer的多任务学习框架，提升社交机器人在多用户环境下的响应决策能力，并通过新损失函数和新数据集取得了更优表现。


<details>
  <summary>Details</summary>
Motivation: 现有大多数人机交互（HRI）研究关注单用户场景，缺乏对多方交互中机器人响应时机和对象决策问题的深入研究。面对真实环境如商场、医院的需求，机器人需理解并判断何时、对谁回应。

Method: 提出基于Transformer的多任务学习框架，并设计两个新损失函数：一个用于约束活跃发言人以提升场景建模，另一个引导机器人优先回应向其定向的发言。同时，构建包含现实复杂因素（如视线未对齐）的多方交互数据集。

Result: 在新的多用户人机交互数据集上，提出的方法在响应决策任务中优于现有启发式和单任务方法，实现了当前最佳性能。

Conclusion: 提出的模型有效提升了社交机器人在多方交互中的自然性和情境感知能力，对实现更聪明的社交机器人具有重要意义。

Abstract: Prior human-robot interaction (HRI) research has primarily focused on
single-user interactions, where robots do not need to consider the timing or
recipient of their responses. However, in multi-party interactions, such as at
malls and hospitals, social robots must understand the context and decide both
when and to whom they should respond. In this paper, we propose a
Transformer-based multi-task learning framework to improve the decision-making
process of social robots, particularly in multi-user environments. Considering
the characteristics of HRI, we propose two novel loss functions: one that
enforces constraints on active speakers to improve scene modeling, and another
that guides response selection towards utterances specifically directed at the
robot. Additionally, we construct a novel multi-party HRI dataset that captures
real-world complexities, such as gaze misalignment. Experimental results
demonstrate that our model achieves state-of-the-art performance in respond
decisions, outperforming existing heuristic-based and single-task approaches.
Our findings contribute to the development of socially intelligent social
robots capable of engaging in natural and context-aware multi-party
interactions.

</details>


### [127] [EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks](https://arxiv.org/abs/2507.10961)
*Joohwan Seo,Arvind Kruthiventy,Soomi Lee,Megan Teng,Xiang Zhang,Seoyeon Choi,Jongeun Choi,Roberto Horowitz*

Main category: cs.RO

TL;DR: 本文提出了EquiContact框架，用于学习能够空间泛化的基于视觉的机器人操作策略，特别针对插销（peg-in-hole）任务，并在实际环境中表现出极强的泛化能力和近乎完美的成功率。


<details>
  <summary>Details</summary>
Motivation: 插销等接触丰富操作任务在机器人领域应用广泛，但基于视觉的策略通常空间泛化能力差、对示范数量敏感，难以适应不同场景。作者旨在解决基于少量示范的策略空间泛化难题。

Method: 提出分层结构EquiContact，高层为视觉规划器Diff-EDF，低层为新颖依从性视觉驱动策略G-CompACT。G-CompACT仅需局部观测（几何一致误差向量、力矩传感器数据和腕部RGB图像），并在末端执行器坐标系中输出动作。整个管道实现SE(3)等变性，包括感知到控制。

Result: 在真实机器人插销任务中，EquiContact展现了几乎完美的成功率，并且能够鲁棒地泛化到未见过的空间配置，远优于此前方法。

Conclusion: 所提出的EquiContact框架和原则（依从性、局部策略、诱导等变性）可实现空间泛化强且稳定的机器人接触操作策略，在实际应用中效果显著。

Abstract: This paper presents a framework for learning vision-based robotic policies
for contact-rich manipulation tasks that generalize spatially across task
configurations. We focus on achieving robust spatial generalization of the
policy for the peg-in-hole (PiH) task trained from a small number of
demonstrations. We propose EquiContact, a hierarchical policy composed of a
high-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF)
and a novel low-level compliant visuomotor policy (Geometric Compliant ACT,
G-CompACT). G-CompACT operates using only localized observations (geometrically
consistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB
images) and produces actions defined in the end-effector frame. Through these
design choices, we show that the entire EquiContact pipeline is
SE(3)-equivariant, from perception to force control. We also outline three key
components for spatially generalizable contact-rich policies: compliance,
localized policies, and induced equivariance. Real-world experiments on PiH
tasks demonstrate a near-perfect success rate and robust generalization to
unseen spatial configurations, validating the proposed framework and
principles. The experimental videos can be found on the project website:
https://sites.google.com/berkeley.edu/equicontact

</details>


### [128] [SMART-Merge Planner: A Safe Merging and Real-Time Motion Planner for Autonomous Highway On-Ramp Merging](https://arxiv.org/abs/2507.10968)
*Toktam Mohammadnejad,Jovin D'sa,Behdad Chalaki,Hossein Nourkhiz Mahjoub,Ehsan Moradi-Pari*

Main category: cs.RO

TL;DR: 本文提出了一种名为SMART-Merge的高速公路自动驾驶安全合流规划器，通过自适应代价项和速度启发式，使自动驾驶车辆能够在安全舒适的前提下实现高效的强制合流，仿真结果显示其成功率和效率优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 高速公路合流是一项复杂且要求极高安全性的任务，现有方法在处理强制合流时往往难以兼顾安全、舒适以及合流效率。因此，需要设计新的方法，提升在复杂高速公路情景下的自动合流能力和可靠性。

Method: 本文设计了基于lattice的运动规划器SMART-Merge。该方法专门针对强制合流的挑战，自适应调整代价项，并引入期望速度启发式，旨在兼顾安全合流和合流时间最小化。其有效性通过CarMaker高保真仿真平台，在数百种合流场景下进行了大规模实验验证。

Result: 仿真结果表明，SMART-Merge方法在所有测试场景下合流成功率为100%，合流用时均优于对比基线方案，极大地提升了合流任务的可靠性与高效性。

Conclusion: SMART-Merge能够在复杂的强制合流场景下实现安全、舒适和高效的自动驾驶合流，具有良好的泛化性、可靠性和实际应用潜力。

Abstract: Merging onto a highway is a complex driving task that requires identifying a
safe gap, adjusting speed, often interactions to create a merging gap, and
completing the merge maneuver within a limited time window while maintaining
safety and driving comfort. In this paper, we introduce a Safe Merging and
Real-Time Merge (SMART-Merge) planner, a lattice-based motion planner designed
to facilitate safe and comfortable forced merging. By deliberately adapting
cost terms to the unique challenges of forced merging and introducing a desired
speed heuristic, SMART-Merge planner enables the ego vehicle to merge
successfully while minimizing the merge time. We verify the efficiency and
effectiveness of the proposed merge planner through high-fidelity CarMaker
simulations on hundreds of highway merge scenarios. Our proposed planner
achieves the success rate of 100% as well as completes the merge maneuver in
the shortest amount of time compared with the baselines, demonstrating our
planner's capability to handle complex forced merge tasks and provide a
reliable and robust solution for autonomous highway merge. The simulation
result videos are available at
https://sites.google.com/view/smart-merge-planner/home.

</details>


### [129] [Uncertainty Aware Mapping for Vision-Based Underwater Robots](https://arxiv.org/abs/2507.10991)
*Abhimanyu Bhowmik,Mohit Singh,Madhushree Sannigrahi,Martin Ludvigsen,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文针对水下机器人在狭小空间中利用视觉进行环境感知时的映射不一致性与深度估计置信度问题进行了研究，提出改进的深度置信度融合映射方法，并在实际场景中进行实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统传感器与预规划路径在水下狭小空间存在局限，且由于传感器噪声与环境变化会引起环境表示的不确定，亟需一种能更好表述与处理环境不确定性的视觉感知方法。

Method: 利用RAFT-Stereo模型对场景深度和置信度进行估计，并将其集成到基于体素的Voxblox映射框架中。同时改进了Voxblox的权重计算与更新机制，提升了对不确定性与置信度的表达能力。

Result: 在受限泳池和特隆赫姆峡湾码头的实际测试中，提出的方法在不确定性可视化上表现出明显改进。实验证明方法能反映环境表征的不确定性变化。

Conclusion: 结合深度估计置信度的视觉映射框架能够提升水下机器人在复杂环境中的感知准确性和可信度，对提高水下机器人实用性具有积极意义。

Abstract: Vision-based underwater robots can be useful in inspecting and exploring
confined spaces where traditional sensors and preplanned paths cannot be
followed. Sensor noise and situational change can cause significant uncertainty
in environmental representation. Thus, this paper explores how to represent
mapping inconsistency in vision-based sensing and incorporate depth estimation
confidence into the mapping framework. The scene depth and the confidence are
estimated using the RAFT-Stereo model and are integrated into a voxel-based
mapping framework, Voxblox. Improvements in the existing Voxblox weight
calculation and update mechanism are also proposed. Finally, a qualitative
analysis of the proposed method is performed in a confined pool and in a pier
in the Trondheim fjord. Experiments using an underwater robot demonstrated the
change in uncertainty in the visualization.

</details>


### [130] [ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations](https://arxiv.org/abs/2507.11000)
*Minwoo Cho,Jaehwi Jang,Daehyung Park*

Main category: cs.RO

TL;DR: 本文提出了一种从示范中学习时间逻辑约束的方法（ILCL），能更好地还原并迁移复杂的时序逻辑约束行为。


<details>
  <summary>Details</summary>
Motivation: 当前基于示范学习时序逻辑约束很困难，主要因为可能约束空间巨大且非马尔可夫性质使得问题更为严峻。现有方法存在表达不灵活、泛化能力差的问题。

Method: 作者提出逆向逻辑约束学习（ILCL）方法，将约束学习建模为一个博弈过程：首先通过基于遗传算法的时间逻辑挖掘（GA-TL-Mining）自动生成参数化线性时序逻辑（TLTL）的语法树，然后用受逻辑约束的强化学习（Logic-CRL）在该约束下寻找最优策略，并采用创新的约束重新分配机制增强效果。

Result: 在四个有时序约束的任务上，与SOTA基线方法相比，ILCL学习和迁移TL约束的表现更优。此外，在实际的“浅孔插销”实验中成功迁移了学习到的约束策略。

Conclusion: ILCL方法不仅有效学习并转移复杂的时序逻辑约束，大幅优于现有方法，还首次实现了向真实任务的应用，验证其实用性和泛化能力。

Abstract: We aim to solve the problem of temporal-constraint learning from
demonstrations to reproduce demonstration-like logic-constrained behaviors.
Learning logic constraints is challenging due to the combinatorially large
space of possible specifications and the ill-posed nature of non-Markovian
constraints. To figure it out, we introduce a novel temporal-constraint
learning method, which we call inverse logic-constraint learning (ILCL). Our
method frames ICL as a two-player zero-sum game between 1) a genetic
algorithm-based temporal-logic mining (GA-TL-Mining) and 2) logic-constrained
reinforcement learning (Logic-CRL). GA-TL-Mining efficiently constructs syntax
trees for parameterized truncated linear temporal logic (TLTL) without
predefined templates. Subsequently, Logic-CRL finds a policy that maximizes
task rewards under the constructed TLTL constraints via a novel constraint
redistribution scheme. Our evaluations show ILCL outperforms state-of-the-art
baselines in learning and transferring TL constraints on four temporally
constrained tasks. We also demonstrate successful transfer to real-world
peg-in-shallow-hole tasks.

</details>


### [131] [Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation](https://arxiv.org/abs/2507.11001)
*Yanbo Wang,Zipeng Fang,Lei Zhao,Weidong Chen*

Main category: cs.RO

TL;DR: 该论文提出LE-Nav框架，通过多模态大语言模型推理和条件变分自编码器，实现服务机器人在复杂动态场景下的自适应导航超参数调优，并在真实世界中展现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统导航系统往往依赖固定参数，难以适应多变的物理和社会场景，导致机器人的导航性能降低并影响社会接受度。现有基于强化学习的方法也因泛化性不足和仿真-现实迁移问题，在真实环境中表现有限，亟需更智能、更适应场景变化的导航方案。

Method: LE-Nav框架结合多模态大型语言模型和条件变分自编码器，用于导航参数自适应调优。该系统通过one-shot例子结合chain-of-thought提示，零次学习场景语义，并用条件VAE建立自然语言与导航参数的映射，实现专家级的自动参数调整。

Result: 实验结果显示，LE-Nav能在多种主流导航规划器和复杂环境中，生成达到人类专家水平的参数调优效果。在真实世界的机器人导航和智能轮椅平台上，其在成功率、效率、安全性和舒适性等量化指标上都优于现有方法，并获得更高的用户主观评价。

Conclusion: LE-Nav框架显著提升了服务机器人在多变复杂环境中的导航能力和社会接受度，展现了大语言模型在机器人控制系统中的强大场景理解和参数调优能力。

Abstract: Service robots are increasingly deployed in diverse and dynamic environments,
where both physical layouts and social contexts change over time and across
locations. In these unstructured settings, conventional navigation systems that
rely on fixed parameters often fail to generalize across scenarios, resulting
in degraded performance and reduced social acceptance. Although recent
approaches have leveraged reinforcement learning to enhance traditional
planners, these methods often fail in real-world deployments due to poor
generalization and limited simulation diversity, which hampers effective
sim-to-real transfer. To tackle these issues, we present LE-Nav, an
interpretable and scene-aware navigation framework that leverages multi-modal
large language model reasoning and conditional variational autoencoders to
adaptively tune planner hyperparameters. To achieve zero-shot scene
understanding, we utilize one-shot exemplars and chain-of-thought prompting
strategies. Additionally, a conditional variational autoencoder captures the
mapping between natural language instructions and navigation hyperparameters,
enabling expert-level tuning. Experiments show that LE-Nav can generate
hyperparameters achieving human-level tuning across diverse planners and
scenarios. Real-world navigation trials and a user study on a smart wheelchair
platform demonstrate that it outperforms state-of-the-art methods on
quantitative metrics such as success rate, efficiency, safety, and comfort,
while receiving higher subjective scores for perceived safety and social
acceptance. Code is available at https://github.com/Cavendish518/LE-Nav.

</details>


### [132] [Enhancing Autonomous Manipulator Control with Human-in-loop for Uncertain Assembly Environments](https://arxiv.org/abs/2507.11006)
*Ashutosh Mishra,Shreya Santra,Hazal Gozbasi,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本研究提出了一种结合人类决策与自主机器人控制的新方法，提高了机器人在不确定和复杂环境（如月球任务）中的操作能力。通过数字孪生仿真和实时反馈，实现了柔性太阳能板的自主精确部署，并在多变环境下验证了系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人在月球等恶劣环境中自主操作时，面对环境不确定性（如地形、负载、光照等）可靠性不足。人工干预虽可提升准确性，但效率受限。因此，亟需一种兼顾自主决策与人工辅助的解决方案，以提升复杂空间任务的操作可靠性和效率。

Method: 方法包括：利用可伸缩梯状结构结合机械臂实现柔性太阳能板的自主部署；机械臂通过实时反馈（位置、力-力矩数据）动态检测误差和自适应控制；针对沉陷、负载变化和弱光等因素设计高效运动规划及人机协作机制，使操作员在模糊场景下可及时介入；引入数字孪生仿真，实现过程持续反馈、任务迭代优化与与实际部署流程无缝集成；在模拟月球环境中全面测试系统性能。

Result: 系统成功在各类极端模拟月球环境下完成太阳能板部署，展现出在多变光照、复杂地形、变载荷及传感器受限等条件下的高可靠性与适应性，综合性能得到有效验证。

Conclusion: 该研究验证了人机协同增强自主机器人技术在太空复杂环境中应用的可行性和优越性，为月球等空间探索任务自动化与智能化操作提供了新思路和技术支撑。

Abstract: This study presents an advanced approach to enhance robotic manipulation in
uncertain and challenging environments, with a focus on autonomous operations
augmented by human-in-the-loop (HITL) control for lunar missions. By
integrating human decision-making with autonomous robotic functions, the
research improves task reliability and efficiency for space applications. The
key task addressed is the autonomous deployment of flexible solar panels using
an extendable ladder-like structure and a robotic manipulator with real-time
feedback for precision. The manipulator relays position and force-torque data,
enabling dynamic error detection and adaptive control during deployment. To
mitigate the effects of sinkage, variable payload, and low-lighting conditions,
efficient motion planning strategies are employed, supplemented by human
control that allows operators to intervene in ambiguous scenarios. Digital twin
simulation enhances system robustness by enabling continuous feedback,
iterative task refinement, and seamless integration with the deployment
pipeline. The system has been tested to validate its performance in simulated
lunar conditions and ensure reliability in extreme lighting, variable terrain,
changing payloads, and sensor limitations.

</details>


### [133] [TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update](https://arxiv.org/abs/2507.11069)
*Jeongyun Kim,Seunghoon Jeong,Giseop Kim,Myung-Hwan Jeon,Eunji Jun,Ayoung Kim*

Main category: cs.RO

TL;DR: 本文提出了一种名为TRAN-D的新方法，用于从有限视角的RGB图像中重建透明物体的3D几何结构。通过分离对象与背景、引入基于物理的仿真优化，提高了透明物体重建的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 透明物体因其反射、折射等物理特性，难以通过常规RGB图像准确恢复3D几何结构，尤其在视角稀疏或动态场景下。因此，亟需创新性方法提升透明物体重建的效果。

Method: 提出TRAN-D方法，基于2D高斯Splatting实现深度重建。首先做对象与背景的分离，将高斯关注于物体本身。同时设计对象感知损失函数以处理遮挡区，降低伪影与过拟合。最后引入物理仿真进一步优化重建，仅需秒级时间，无需重复扫描。

Result: 在合成与真实数据集上实验，TRAN-D显著优于现有GS系方法。对合成数据，平均绝对误差（MAE）降幅超过39%。用单张图片训练时，其重建精度为48.46%，是使用六张图的基线方法的1.5倍以上。

Conclusion: TRAN-D显著提升了透明物体的3D重建表现，特别是在数据稀少和动态场景下，展示了良好的通用性与精度，推动相关领域技术进步。

Abstract: Understanding the 3D geometry of transparent objects from RGB images is
challenging due to their inherent physical properties, such as reflection and
refraction. To address these difficulties, especially in scenarios with sparse
views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian
Splatting-based depth reconstruction method for transparent objects. Our key
insight lies in separating transparent objects from the background, enabling
focused optimization of Gaussians corresponding to the object. We mitigate
artifacts with an object-aware loss that places Gaussians in obscured regions,
ensuring coverage of invisible surfaces while reducing overfitting.
Furthermore, we incorporate a physics-based simulation that refines the
reconstruction in just a few seconds, effectively handling object removal and
chain-reaction movement of remaining objects without the need for rescanning.
TRAN-D is evaluated on both synthetic and real-world sequences, and it
consistently demonstrated robust improvements over existing GS-based
state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean
absolute error by over 39% for the synthetic TRansPose sequences. Furthermore,
despite being updated using only one image, TRAN-D reaches a {\delta} < 2.5 cm
accuracy of 48.46%, over 1.5 times that of baselines, which uses six images.
Code and more results are available at https://jeongyun0609.github.io/TRAN-D/.

</details>


### [134] [Closed Form Time Derivatives of the Equations of Motion of Rigid Body Systems](https://arxiv.org/abs/2507.11076)
*Andreas Mueller,Shivesh Kumar*

Main category: cs.RO

TL;DR: 本文提出了一种刚体动力学方程（二阶时间导数）闭式表达的新方法，简化了多体系统中导数的推导与应用，便于机器人控制和分析。


<details>
  <summary>Details</summary>
Motivation: 机器人系统的设计与控制中，尤其是含有弹性部件的多体系统，需要不只是轨迹的平滑，还要对控制力/力矩及其时间导数精确计算，因此需要更高阶的运动方程导数。现有方法主要依赖递归算法，缺乏结构性直观表达。

Method: 作者基于Lie群理论，对刚体系统的运动方程导数提出了闭式表达，推导出高阶时间导数（至二阶），并与现有递归算法的形式进行了对比。通过Lie群参数化，使得推导结果更为紧凑且便于分析和实现。

Result: 得到了刚体动力学方程一阶和二阶时间导数的简单、紧凑的闭式表达式，并展示其结构优势。相较于传统递归算法，闭式表达更方便分析和参数化。

Conclusion: 闭式表达方法为动力学方程高阶导数的求解提供了更直观、易用的工具，有利于机器人动力学分析和控制设计，特别适用于含弹性元件的复杂多体系统。

Abstract: Derivatives of equations of motion(EOM) describing the dynamics of rigid body
systems are becoming increasingly relevant for the robotics community and find
many applications in design and control of robotic systems. Controlling robots,
and multibody systems comprising elastic components in particular, not only
requires smooth trajectories but also the time derivatives of the control
forces/torques, hence of the EOM. This paper presents the time derivatives of
the EOM in closed form up to second-order as an alternative formulation to the
existing recursive algorithms for this purpose, which provides a direct insight
into the structure of the derivatives. The Lie group formulation for rigid body
systems is used giving rise to very compact and easily parameterized equations.

</details>


### [135] [Force-Based Viscosity and Elasticity Measurements for Material Biomechanical Characterisation with a Collaborative Robotic Arm](https://arxiv.org/abs/2507.11133)
*Luca Beber,Edoardo Lamon,Giacomo Moretti,Matteo Saveriano,Luca Fambri,Luigi Palopoli,Daniele Fontanelli*

Main category: cs.RO

TL;DR: 本论文评估了一种机器人系统在估算不同材料（包括生物样本）黏弹性参数方面的准确性，实验结果显示其表现接近高精度仪器，验证了其应用于临床诊断的可行性。


<details>
  <summary>Details</summary>
Motivation: 超声扫描和触诊虽成本低且对疾病早期发现重要，但依赖高技能医务人员且容易出错，因此开发可以提升诊断可靠性和效率的机器人系统具有实际需求。

Method: 采用机器人系统对各种材料（含部分离体生物组织）进行触诊实验，并与经过高精度仪器测量的硅胶样本作为真值进行对比，评估机器人的参数估算能力。

Result: 机器人系统在黏弹性参数的测量上与真值高度吻合，在多种材料和生物组织初步实验中表现出较高的准确性和一致性。

Conclusion: 实验结果证明了机器人系统具备作为临床诊断辅助工具的潜力，可提高诊断的客观性和效率。

Abstract: Diagnostic activities, such as ultrasound scans and palpation, are relatively
low-cost. They play a crucial role in the early detection of health problems
and in assessing their progression. However, they are also error-prone
activities, which require highly skilled medical staff. The use of robotic
solutions can be key to decreasing the inherent subjectivity of the results and
reducing the waiting list. For a robot to perform palpation or ultrasound
scans, it must effectively manage physical interactions with the human body,
which greatly benefits from precise estimation of the patient's tissue
biomechanical properties. This paper assesses the accuracy and precision of a
robotic system in estimating the viscoelastic parameters of various materials,
including some tests on ex vivo tissues as a preliminary proof-of-concept
demonstration of the method's applicability to biological samples. The
measurements are compared against a ground truth derived from silicone
specimens with different viscoelastic properties, characterised using a
high-precision instrument. Experimental results show that the robotic system's
accuracy closely matches the ground truth, increasing confidence in the
potential use of robots for such clinical applications.

</details>


### [136] [A Robust Controller based on Gaussian Processes for Robotic Manipulators with Unknown Uncertainty](https://arxiv.org/abs/2507.11170)
*Giulio Giacomuzzo,Mohamed Abdelwahab,Marco Calì,Alberto Dalla Libera,Ruggero Carli*

Main category: cs.RO

TL;DR: 该论文提出了一种基于学习的鲁棒反馈线性化方法，以实现拉格朗日系统的精确轨迹跟踪。核心思想是利用高斯过程回归（GPR）对模型失配进行估计，并结合反馈线性化和鲁棒项来提升控制性能。方法在仿真实验中得到了验证。


<details>
  <summary>Details</summary>
Motivation: 拉格朗日系统在实际中往往存在动力学模型不确定性，传统反馈线性化方法依赖于精确建模，难以应对模型部分未知或失配的情况。因此，亟需一种能够在模型不准确甚至无法给出失配上界时，依然能够保证跟踪性能的控制方法。

Method: 方法基于已有的动力学名义模型，对未知模型失配部分采用高斯过程回归（GPR）在线估计。该估计结果作为补偿项添加到反馈线性化的外环控制中。对于GPR无法完全捕捉的不确定性，通过引入基于GPR输出方差的鲁棒修正项进行补偿，从而增强系统鲁棒性。

Result: 理论证明了该方法可以以高概率实现期望轨迹的渐近跟踪。并且在二维平面机器人系统上进行了数值仿真，验证了方法的有效性。

Conclusion: 结合GPR和鲁棒反馈线性化的新方法无需先验失配上界，能较好解决模型不确定性问题，提升了拉格朗日系统轨迹跟踪的精度与鲁棒性。

Abstract: In this paper, we propose a novel learning-based robust feedback
linearization strategy to ensure precise trajectory tracking for an important
family of Lagrangian systems. We assume a nominal knowledge of the dynamics is
given but no a-priori bounds on the model mismatch are available. In our
approach, the key ingredient is the adoption of a regression framework based on
Gaussian Processes (GPR) to estimate the model mismatch. This estimate is added
to the outer loop of a classical feedback linearization scheme based on the
nominal knowledge available. Then, to compensate for the residual uncertainty,
we robustify the controller including an additional term whose size is designed
based on the variance provided by the GPR framework. We proved that, with high
probability, the proposed scheme is able to guarantee asymptotic tracking of a
desired trajectory. We tested numerically our strategy on a 2 degrees of
freedom planar robot.

</details>


### [137] [MPC-based Coarse-to-Fine Motion Planning for Robotic Object Transportation in Cluttered Environments](https://arxiv.org/abs/2507.11211)
*Chen Cai,Ernesto Dickel Saraiva,Ya-jun Pan,Steven Liu*

Main category: cs.RO

TL;DR: 本文提出了一种适用于复杂且未知环境下机器人操作的由粗到细的运动规划框架，通过多相机感知与自适应优化算法，实现了在动态和不确定场景中的高效运动规划。


<details>
  <summary>Details</summary>
Motivation: 在复杂且未建模的环境中，机器人操作面临感知不确定性和动态障碍等难题，现有规划方法无法兼顾全局可行性与动态实时优化，亟需新的方法提升机器人在此类环境下的鲁棒性与适应性。

Method: 该系统融合了双摄像头视觉感知与基于B样条的模型预测控制（MPC）方案。首先利用部分与不确定观测信息生成全局可行轨迹，之后随着视觉信息不断增量融合，对环境模型与运动规划细化。通过基于视觉的代价函数驱动目标探索，利用核感知器（kernel-perceptron）碰撞检测模块实现约束的高效实时更新，支持闭链运动学与动态重新规划。

Result: 在多机械臂平台上进行的实验显示，该方法在环境不确定与场地拥挤情况下表现出较强的鲁棒性与适应能力。

Conclusion: 所提的新框架能够在未建模与动态环境中实现高效、可靠的机器人操作运动规划，具有良好的实际应用潜力。

Abstract: This letter presents a novel coarse-to-fine motion planning framework for
robotic manipulation in cluttered, unmodeled environments. The system
integrates a dual-camera perception setup with a B-spline-based model
predictive control (MPC) scheme. Initially, the planner generates feasible
global trajectories from partial and uncertain observations. As new visual data
are incrementally fused, both the environment model and motion planning are
progressively refined. A vision-based cost function promotes target-driven
exploration, while a refined kernel-perceptron collision detector enables
efficient constraint updates for real-time planning. The framework accommodates
closed-chain kinematics and supports dynamic replanning. Experiments on a
multi-arm platform validate its robustness and adaptability under uncertainties
and clutter.

</details>


### [138] [Comparison of Localization Algorithms between Reduced-Scale and Real-Sized Vehicles Using Visual and Inertial Sensors](https://arxiv.org/abs/2507.11241)
*Tobias Kern,Leon Tolksdorf,Christian Birkner*

Main category: cs.RO

TL;DR: 本文研究了使用相机和IMU的视觉及视觉-惯性算法在实体缩比车上的自定位精度，为推进自动驾驶功能开发提供实验基础。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要大量测试，而实际道路测试成本高、风险大。使用缩比车辆可以加速研发进程，因此需要验证相关定位算法在缩比车辆上的适用性和精度。

Method: 作者选择了三种ROS2兼容的自定位算法（OpenVINS、VINS-Fusion和RTAB-Map），利用公开数据集作基线，并对缩比车和实车分别采集测试数据，进行位置精度对比分析。

Result: 在所有算法中，OpenVINS在实车和缩比车上的定位误差最低。各算法的误差区间有重叠，但OpenVINS整体表现最好。缩比车与实车在平移运动估计上有小的精度差异，旋转运动估计几乎无差别。

Conclusion: 缩比载具可作为自定位算法的测试平台，能较好地反映实车精度，尤其在旋转运动方面表现一致，有助于自动驾驶自定位技术的加速迭代。

Abstract: Physically reduced-scale vehicles are emerging to accelerate the development
of advanced automated driving functions. In this paper, we investigate the
effects of scaling on self-localization accuracy with visual and
visual-inertial algorithms using cameras and an inertial measurement unit
(IMU). For this purpose, ROS2-compatible visual and visual-inertial algorithms
are selected, and datasets are chosen as a baseline for real-sized vehicles. A
test drive is conducted to record data of reduced-scale vehicles. We compare
the selected localization algorithms, OpenVINS, VINS-Fusion, and RTAB-Map, in
terms of their pose accuracy against the ground-truth and against data from
real-sized vehicles. When comparing the implementation of the selected
localization algorithms to real-sized vehicles, OpenVINS has the lowest average
localization error. Although all selected localization algorithms have
overlapping error ranges, OpenVINS also performs best when applied to a
reduced-scale vehicle. When reduced-scale vehicles were compared to real-sized
vehicles, minor differences were found in translational vehicle motion
estimation accuracy. However, no significant differences were found when
comparing the estimation accuracy of rotational vehicle motion, allowing RSVRs
to be used as testing platforms for self-localization algorithms.

</details>


### [139] [Development of an Autonomous Mobile Robotic System for Efficient and Precise Disinfection](https://arxiv.org/abs/2507.11270)
*Ting-Wei Ou,Jia-Hao Jiang,Guan-Lin Huang,Kuu-Young Young*

Main category: cs.RO

TL;DR: 本文提出了一种面向病毒高危热点的移动式紫外线消毒机器人系统，在保持消毒效果的同时大大提升了消毒效率，显著缩短了消毒时间。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情导致医疗系统压力剧增，暴露出医院环境下自动化消毒的迫切需求，现有紫外线消毒多关注覆盖率，忽视了人员活动对病毒分布的影响，难以有效针对高风险区域消杀。

Method: 作者设计了一种机器人自动化消毒方案，能优先识别并消毒病毒高危热点区域，并通过优化算法动态调整紫外线剂量，实现各表面“适度消毒”的目标，从而提升效率并避免低风险区的资源浪费。

Result: 在两种典型医院场景下，所提方法消毒有效性与传统方法持平，但分别将消毒时间缩短了30.7%和31.9%。

Conclusion: 面向病毒高危热点的智能化消毒方案可以大幅提升医院自动化消毒的效率，减少人员暴露风险，有助于疫情期间医疗安全防护。

Abstract: The COVID-19 pandemic has severely affected public health, healthcare
systems, and daily life, especially amid resource shortages and limited
workers. This crisis has underscored the urgent need for automation in hospital
environments, particularly disinfection, which is crucial to controlling virus
transmission and improving the safety of healthcare personnel and patients.
Ultraviolet (UV) light disinfection, known for its high efficiency, has been
widely adopted in hospital settings. However, most existing research focuses on
maximizing UV coverage while paying little attention to the impact of human
activity on virus distribution. To address this issue, we propose a mobile
robotic system for UV disinfection focusing on the virus hotspot. The system
prioritizes disinfection in high-risk areas and employs an approach for
optimized UV dosage to ensure that all surfaces receive an adequate level of UV
exposure while significantly reducing disinfection time. It not only improves
disinfection efficiency but also minimizes unnecessary exposure in low-risk
areas. In two representative hospital scenarios, our method achieves the same
disinfection effectiveness while reducing disinfection time by 30.7% and 31.9%,
respectively. The video of the experiment is available at:
https://youtu.be/wHcWzOcoMPM.

</details>


### [140] [Ocean Diviner: A Diffusion-Augmented Reinforcement Learning for AUV Robust Control in the Underwater Tasks](https://arxiv.org/abs/2507.11283)
*Weiyi Liu,Jingzehua Xu,Guanwen Xie,Yi Li*

Main category: cs.RO

TL;DR: 本文提出了一种结合扩散模型与强化学习的新方法，显著提升了水下自主机器人（AUV）在轨迹规划与动态环境适应中的鲁棒性和性能。实验结果显示该方法优于传统方法。


<details>
  <summary>Details</summary>
Motivation: AUV在复杂水下环境中需面对轨迹动态规划和环境变化，现有控制方法在多步预测和适应性方面存在局限，因此需要更为高效、鲁棒的控制策略。

Method: 方法包含三大创新点：（1）基于扩散模型的多步轨迹生成框架，结合历史状态与当前观测，通过创新的扩散U-Net编码提升长时规划能力；（2）高效混合学习架构，将扩散模型引导的探索与RL策略优化结合，通过RL评价器选出最优动作，提升探索效率与策略稳定性；（3）在动态环境中进行广泛仿真实验验证。

Result: 仿真实验证明，该方法在复杂海洋环境下的鲁棒性和适应性明显优于传统AUV控制方法，展现出更强的灵活性和可靠性。

Conclusion: 基于扩散与强化学习的新方法为水下机器人的自主控制提供了更强的适应性和鲁棒性，对实际AUV任务有重要意义。

Abstract: This paper presents a diffusion-augmented reinforcement learning (RL)
approach for robust autonomous underwater vehicle (AUV) control, addressing key
challenges in underwater trajectory planning and dynamic environment
adaptation. The proposed method integrates three core innovations: (1) A
diffusion-based trajectory generation framework that produces physically
feasible multi-step trajectories, enhanced by a high-dimensional state encoding
mechanism combining current observations with historical states and actions
through a novel diffusion U-Net architecture, significantly improving
long-horizon planning. (2) A sample-efficient hybrid learning architecture that
synergizes diffusion-guided exploration with RL policy optimization, where the
diffusion model generates diverse candidate actions and the RL critic selects
optimal actions, achieving higher exploration efficiency and policy stability
in dynamic underwater environments. Extensive simulation experiments validating
the method's superior robustness and flexibility, outperforms conventional
control methods in challenging marine conditions, offering enhanced
adaptability and reliability for AUV operations in the underwater tasks.

</details>


### [141] [Diffusion-Based Imaginative Coordination for Bimanual Manipulation](https://arxiv.org/abs/2507.11296)
*Huilin Xu,Jian Ding,Jiakun Xu,Ruixiang Wang,Jun Chen,Jinjie Mai,Yanwei Fu,Bernard Ghanem,Feng Xu,Mohamed Elhoseiny*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩散模型的统一框架，通过联合优化视频预测和动作预测，有效提升了机器人双臂操作任务的表现。方法在两个仿真基准和实际环境中均取得了显著超越主流方法ACT的成功率提升。


<details>
  <summary>Details</summary>
Motivation: 机器人双臂协作操作具有高维动作空间和复杂协调要求，当前的视频预测虽有助于特征学习和控制，但对提升双臂协调潜力的探索不足，因此需开发新的方法提升这类任务的实现能力。

Method: 提出基于扩散模型的联合优化框架，利用多帧潜在预测策略在压缩的潜在空间中编码未来状态，保留任务相关特征。引入单向注意力机制，使得视频预测以动作为条件，动作预测则独立于视频预测，从而推理过程中可省略视频预测，大幅提升效率。

Result: 在ALOHA、RoboTwin两个仿真基准以及真实世界场景下，本方法较强基线ACT分别实现了24.9%、11.1%、32.5%的成功率提升。

Conclusion: 提出的方法显著提升了机器人双臂操作的性能，并通过高效的模型设计兼顾了推理效率和任务表现，具有较强实际应用价值。

Abstract: Bimanual manipulation is crucial in robotics, enabling complex tasks in
industrial automation and household services. However, it poses significant
challenges due to the high-dimensional action space and intricate coordination
requirements. While video prediction has been recently studied for
representation learning and control, leveraging its ability to capture rich
dynamic and behavioral information, its potential for enhancing bimanual
coordination remains underexplored. To bridge this gap, we propose a unified
diffusion-based framework for the joint optimization of video and action
prediction. Specifically, we propose a multi-frame latent prediction strategy
that encodes future states in a compressed latent space, preserving
task-relevant features. Furthermore, we introduce a unidirectional attention
mechanism where video prediction is conditioned on the action, while action
prediction remains independent of video prediction. This design allows us to
omit video prediction during inference, significantly enhancing efficiency.
Experiments on two simulated benchmarks and a real-world setting demonstrate a
significant improvement in the success rate over the strong baseline ACT using
our method, achieving a \textbf{24.9\%} increase on ALOHA, an \textbf{11.1\%}
increase on RoboTwin, and a \textbf{32.5\%} increase in real-world experiments.
Our models and code are publicly available at
https://github.com/return-sleep/Diffusion_based_imaginative_Coordination.

</details>


### [142] [All Eyes, no IMU: Learning Flight Attitude from Vision Alone](https://arxiv.org/abs/2507.11302)
*Jesse J. Hagenaars,Stein Stroobants,Sander M. Bohte,Guido C. H. E. De Croon*

Main category: cs.RO

TL;DR: 本文提出了一种仅依赖视觉，不使用惯性传感器的无人机姿态估计和飞行控制方法。通过下视事件相机和神经网络，实现在真实环境下的飞行控制。


<details>
  <summary>Details</summary>
Motivation: 目前飞行机器人在姿态控制方面主要依赖加速度计和陀螺仪等惯性测量单元(IMU)，而许多生物只利用视觉感知控制姿态。受此启发，本文试图研究能否仅凭视觉信息实现飞行控制，推动无人机向更小型、更灵活的方向发展。

Method: 作者在无人机上安装了下视事件相机，利用事件流数据输入到一个小型递归卷积神经网络，通过监督学习训练输出即时姿态和旋转速率，从而实现飞行控制，无需IMU。对不同场景和视野进行了泛化能力分析。

Result: 实验结果表明，该方法能够实现依赖视觉的稳定飞行，神经网络和事件相机的组合可以替代传统飞控回路中的IMU。带有记忆和地平线视觉线索的网络性能最佳，视野较小的网络在泛化性上表现更好。

Conclusion: 本文证明了基于视觉的视频流和神经网络能够实现无人机的姿态估计及飞行控制，展示了视觉飞控在推动昆虫级小型飞行机器人自治方面的应用前景。

Abstract: Vision is an essential part of attitude control for many flying animals, some
of which have no dedicated sense of gravity. Flying robots, on the other hand,
typically depend heavily on accelerometers and gyroscopes for attitude
stabilization. In this work, we present the first vision-only approach to
flight control for use in generic environments. We show that a quadrotor drone
equipped with a downward-facing event camera can estimate its attitude and
rotation rate from just the event stream, enabling flight control without
inertial sensors. Our approach uses a small recurrent convolutional neural
network trained through supervised learning. Real-world flight tests
demonstrate that our combination of event camera and low-latency neural network
is capable of replacing the inertial measurement unit in a traditional flight
control loop. Furthermore, we investigate the network's generalization across
different environments, and the impact of memory and different fields of view.
While networks with memory and access to horizon-like visual cues achieve best
performance, variants with a narrower field of view achieve better relative
generalization. Our work showcases vision-only flight control as a promising
candidate for enabling autonomous, insect-scale flying robots.

</details>


### [143] [Acting and Planning with Hierarchical Operational Models on a Mobile Robot: A Study with RAE+UPOM](https://arxiv.org/abs/2507.11345)
*Oscar Lima,Marc Vinci,Sunandita Patra,Sebastian Stock,Joachim Hertzberg,Martin Atzmueller,Malik Ghallab,Dana Nau,Paolo Traverso*

Main category: cs.RO

TL;DR: 本文提出并首次在真实机器人上部署了整合型actor-planner系统，结合了层次化执行与规划，提升了机器人任务执行的健壮性。


<details>
  <summary>Details</summary>
Motivation: 在机器人任务执行中，符号规划器的建模与机器人底层控制间存在脱节，影响机器人在现实环境下的表现。因此，亟需将两者有效结合，提升任务执行的鲁棒性与灵活性。

Method: 作者提出并实现了RAE+UPOM系统，将反应式执行引擎（RAE）与类UCT蒙特卡洛规划器（UPOM）进行集成，并采用层级化操作模型用于执行与规划的共享。系统在移动操作机器人上被部署执行真实的物体收集任务。

Result: 实验证明，系统在动作失败和传感器噪声条件下表现出较强任务执行鲁棒性。同时，实验也提供了关于“交替执行与规划”决策过程的经验性见解。

Conclusion: 整合式层次操作模型actor-planner系统（RAE+UPOM）能够更好地应对现实环境的不确定性，提升机器人的任务执行能力，为今后实际部署提供思路。

Abstract: Robotic task execution faces challenges due to the inconsistency between
symbolic planner models and the rich control structures actually running on the
robot. In this paper, we present the first physical deployment of an integrated
actor-planner system that shares hierarchical operational models for both
acting and planning, interleaving the Reactive Acting Engine (RAE) with an
anytime UCT-like Monte Carlo planner (UPOM). We implement RAE+UPOM on a mobile
manipulator in a real-world deployment for an object collection task. Our
experiments demonstrate robust task execution under action failures and sensor
noise, and provide empirical insights into the interleaved acting-and-planning
decision making process.

</details>


### [144] [From Production Logistics to Smart Manufacturing: The Vision for a New RoboCup Industrial League](https://arxiv.org/abs/2507.11402)
*Supun Dissanayaka,Alexander Ferrein,Till Hofmann,Kosuke Nakajima,Mario Sanz-Lopez,Jesus Savage,Daniel Swoboda,Matteo Tschesche,Wataru Uemura,Tarik Viehmann,Shohei Yasuda*

Main category: cs.RO

TL;DR: 本文介绍了RoboCup智能制造联盟（Smart Manufacturing League）的设想，旨在加强RoboCup竞赛与现代智能工厂的结合。新联盟将扩展竞赛内容，涵盖组装、人机协作等工业机器人核心挑战，以提升赛事前沿性和吸引力。


<details>
  <summary>Details</summary>
Motivation: 目前RoboCup物流联盟主要关注任务规划、作业调度和多智能体协调等生产物流，但未能紧跟智能制造领域最新的发展，因此其行业相关性逐渐减弱。作者希望通过设立一个涵盖更广泛智能工厂要素的新竞赛形式，提升其现实意义和吸引力。

Method: 作者提出设立RoboCup智能制造联盟，并设计多条独立赛道，包括组装、人机协作、类人机器人等工业机器人挑战。这些赛道初期独立，逐步整合，最终形成一个全面的智能制造场景，同时保留生产物流方面的核心内容。

Result: 通过新联盟的规划，竞赛内容更贴合现代工厂实际，既保留了原有物流优势，又拓展了新方向。这使比赛对新参赛者和老队伍都更具吸引力，技术挑战性和行业前沿性得到增强。

Conclusion: 重新设想的RoboCup智能制造联盟将提升赛事与现实工业发展的关联，吸引更多团队参与，并引导竞赛关注工业机器人当前及未来的核心挑战，有助于推动智能制造领域创新。

Abstract: The RoboCup Logistics League is a RoboCup competition in a smart factory
scenario that has focused on task planning, job scheduling, and multi-agent
coordination. The focus on production logistics allowed teams to develop highly
competitive strategies, but also meant that some recent developments in the
context of smart manufacturing are not reflected in the competition, weakening
its relevance over the years. In this paper, we describe the vision for the
RoboCup Smart Manufacturing League, a new competition designed as a larger
smart manufacturing scenario, reflecting all the major aspects of a modern
factory. It will consist of several tracks that are initially independent but
gradually combined into one smart manufacturing scenario. The new tracks will
cover industrial robotics challenges such as assembly, human-robot
collaboration, and humanoid robotics, but also retain a focus on production
logistics. We expect the reenvisioned competition to be more attractive to
newcomers and well-tried teams, while also shifting the focus to current and
future challenges of industrial robotics.

</details>


### [145] [Multi-IMU Sensor Fusion for Legged Robots](https://arxiv.org/abs/2507.11447)
*Shuo Yang,John Z. Zhang,Ibrahima Sory Sow,Zachary Manchester*

Main category: cs.RO

TL;DR: 本文提出了一种适用于足式机器人、基于多IMU（惯性测量单元）与视觉融合的状态估计算法，能够实现在恶劣运动条件下低漂移的位置和速度估计。


<details>
  <summary>Details</summary>
Motivation: 足式机器人在行走过程中，由于剧烈的地面冲击、滑移和急转弯等情况，导致传统仅依赖单IMU或自身体感器的里程计出现较大累积误差（漂移），限制了其在复杂环境中的应用。因此，研究低成本、轻量级、低漂移的状态估计方法具有重要意义。

Method: 作者在机器人多个部件上布置多个IMU，结合关节编码器信息，通过扩展卡尔曼滤波器融合这些数据，得到准确的速度估计；随后再利用滑动窗口的因子图框架，将相机信息与前述估计结果进一步融合，构建视觉-惯性-腿部融合里程计（visual-inertial-leg odometry, VILO）系统。

Result: 经理论分析和实际机器人平台在多种复杂行走任务下的实验验证，提出的算法能够在极端运动情况下依然保持极小的位置偏差，有效应对撞击、滑动和剧烈旋转等问题。

Conclusion: 该系统为足式机器人在实际复杂环境中提供了低成本、高鲁棒性的状态估计解决方案，有望推动其在真实任务中的广泛部署，相关代码与数据集已开源，便于学界和工业界进一步研究和应用。

Abstract: This paper presents a state-estimation solution for legged robots that uses a
set of low-cost, compact, and lightweight sensors to achieve low-drift pose and
velocity estimation under challenging locomotion conditions. The key idea is to
leverage multiple inertial measurement units on different links of the robot to
correct a major error source in standard proprioceptive odometry. We fuse the
inertial sensor information and joint encoder measurements in an extended
Kalman filter, then combine the velocity estimate from this filter with camera
data in a factor-graph-based sliding-window estimator to form a
visual-inertial-leg odometry method. We validate our state estimator through
comprehensive theoretical analysis and hardware experiments performed using
real-world robot data collected during a variety of challenging locomotion
tasks. Our algorithm consistently achieves minimal position deviation, even in
scenarios involving substantial ground impact, foot slippage, and sudden body
rotations. A C++ implementation, along with a large-scale dataset, is available
at https://github.com/ShuoYangRobotics/Cerberus2.0.

</details>


### [146] [Human-Robot collaboration in surgery: Advances and challenges towards autonomous surgical assistants](https://arxiv.org/abs/2507.11460)
*Jacinto Colan,Ana Davila,Yutaro Yamada,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 本文系统性回顾了自主外科机器人助手（ASARs）在手术人机协作中的研究进展与挑战，涵盖主流协作方式、应用热点和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着自主机器人能力的提升，提升医生在复杂手术中的协作效率和精度，有必要系统梳理ASARs的最新进展与面临的主要障碍。

Method: 遵循PRISMA流程，从IEEE Xplore、Scopus和Web of Science收集文献，筛选出32篇相关论文，聚焦于机器人为外科医生提供实际支持的协作场景，归纳协作形式及其应用。

Result: 主要发现两种协作模式：远程操作辅助和直接协作操作。ASARs应用以内窥镜引导最为成熟，工具自主操作亦有新进展。当前面临诸多挑战，包括机器人行为与外科医生偏好一致性、自主系统对手术过程的理解、信息交互流畅性，以及团队技能协同等。

Conclusion: 本综述总结了ASARs领域的研究趋势与关键瓶颈，指出今后需重点提升系统的可靠性、安全性与协作效果，为外科手术中人机融合的发展提供方向。

Abstract: Human-robot collaboration in surgery represents a significant area of
research, driven by the increasing capability of autonomous robotic systems to
assist surgeons in complex procedures. This systematic review examines the
advancements and persistent challenges in the development of autonomous
surgical robotic assistants (ASARs), focusing specifically on scenarios where
robots provide meaningful and active support to human surgeons. Adhering to the
PRISMA guidelines, a comprehensive literature search was conducted across the
IEEE Xplore, Scopus, and Web of Science databases, resulting in the selection
of 32 studies for detailed analysis. Two primary collaborative setups were
identified: teleoperation-based assistance and direct hands-on interaction. The
findings reveal a growing research emphasis on ASARs, with predominant
applications currently in endoscope guidance, alongside emerging progress in
autonomous tool manipulation. Several key challenges hinder wider adoption,
including the alignment of robotic actions with human surgeon preferences, the
necessity for procedural awareness within autonomous systems, the establishment
of seamless human-robot information exchange, and the complexities of skill
acquisition in shared workspaces. This review synthesizes current trends,
identifies critical limitations, and outlines future research directions
essential to improve the reliability, safety, and effectiveness of human-robot
collaboration in surgical environments.

</details>


### [147] [LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control](https://arxiv.org/abs/2507.11464)
*Ajay Shankar,Keisuke Okumura,Amanda Prorok*

Main category: cs.RO

TL;DR: 本文提出了一种多机器人控制框架，结合了集中式离散路径规划与分布式动态轨迹控制，实现了高效、可扩展的多机器人导航，支持动态、异步目标更新，并有真实机器人实验验证。


<details>
  <summary>Details</summary>
Motivation: 多机器人团队在复杂环境中高效、协同导航面临路径碰撞、动态变化、目标异步更新等挑战。现有方法难以同时兼顾全局规划效率、动态适应性与系统可扩展性。

Method: 该框架异步高频运行两个进程：(1) 利用先进的MAPF（多智能体路径规划）算法进行集中式、离散、全视野的路径规划，确保无碰撞无死锁；(2) 每个机器人独立运行动态感知的最优轨迹控制器，保证路径跟踪鲁棒。通过这种分层解耦，兼具全局优化与局部执行灵活性。实际实现为LF系统，结合LaCAM与Freyja两大模块。

Result: 系统可在多组真实飞行机器人和地面机器人上实现高效协同行为，适应动态环境和目标实时变化。实验包括15架多旋翼无人机动态目标跟踪并与实时人类干扰协作。

Conclusion: 该方法验证了分层解耦多机器人控制在实际环境中的有效性和可扩展性，适用于长期、多目标、动态更新的生产与服务场景，为多机器人协作提供了有力的基础。

Abstract: We propose a multi-robot control paradigm to solve point-to-point navigation
tasks for a team of holonomic robots with access to the full environment
information. The framework invokes two processes asynchronously at high
frequency: (i) a centralized, discrete, and full-horizon planner for computing
collision- and deadlock-free paths rapidly, leveraging recent advances in
multi-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal
trajectory controllers that ensure all robots independently follow their
assigned paths reliably. This hierarchical shift in planning representation
from (i) discrete and coupled to (ii) continuous and decoupled domains enables
the framework to maintain long-term scalable motion synthesis. As an
instantiation of this idea, we present LF, which combines a fast
state-of-the-art MAPF solver (LaCAM), and a robust feedback control stack
(Freyja) for executing agile robot maneuvers. LF provides a robust and
versatile mechanism for lifelong multi-robot navigation even under asynchronous
and partial goal updates, and adapts to dynamic workspaces simply by quick
replanning. We present various multirotor and ground robot demonstrations,
including the deployment of 15 real multirotors with random, consecutive target
updates while a person walks through the operational workspace.

</details>


### [148] [Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming](https://arxiv.org/abs/2507.11498)
*Asad Ali Shahid,Francesco Braghin,Loris Roveda*

Main category: cs.RO

TL;DR: 本文提出了一种可以进行复杂、富有表现力击鼓表演的人形机器人系统，并在多种音乐曲目上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 虽然人形机器人在灵巧性、平衡和运动方面取得了显著进展，但在音乐表演等表达性任务上的应用却很少。这类任务如击鼓，对时序、协调性和反应速度要求极高，因此亟需探索人形机器人在此领域的能力极限与实现方法。

Method: 作者将击鼓任务建模为时序性接触达成问题，将鼓谱转化为“节奏接触链”，并将表演分割为固定长度的片段。通过强化学习，在不同片段上并行训练统一的策略，使机器人能够协调多个四肢，连续完成复杂曲目。

Result: 在三十多首流行的摇滚、金属和爵士乐曲目实验中，该机器人均取得了高F1分数。机器人展现出类似人类的击鼓技巧，包括交叉手击打和自适应鼓棒分配等。

Conclusion: 强化学习能够促进人形机器人在创意和表现性极强的音乐表演任务中的应用，为人形机器人进入音乐等创造性领域展示了巨大潜力。

Abstract: Humanoid robots have seen remarkable advances in dexterity, balance, and
locomotion, yet their role in expressive domains, such as music performance,
remains largely unexplored. Musical tasks, like drumming, present unique
challenges, including split-second timing, rapid contacts, and multi-limb
coordination over pieces lasting minutes. In this paper, we introduce Robot
Drummer, a humanoid system capable of expressive, high-precision drumming
across a diverse repertoire of songs. We formulate humanoid drumming as
sequential fulfillment of timed-contacts and transform drum scores in to a
Rhythmic Contact Chain. To handle the long-horizon nature of musical
performance, we decompose each piece into fixed-length segments and train a
single policy across all segments in parallel using reinforcement learning.
Through extensive experiments on over thirty popular rock, metal, and jazz
tracks, our results demonstrate that Robot Drummer consistently achieves high
F1 scores. The learned behaviors exhibit emergent human-like drumming
strategies, such as cross-arm strikes, and adaptive sticks assignments,
demonstrating the potential of reinforcement learning to bring humanoid robots
into the domain of creative musical performance. Project page:
\href{https://robot-drummer.github.io}{robot-drummer.github.io}

</details>


### [149] [LLM-based ambiguity detection in natural language instructions for collaborative surgical robots](https://arxiv.org/abs/2507.11525)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 本文提出了一种基于大型语言模型（LLMs）的框架，用于检测手术协作场景中自然语言指令的歧义性，提高人机协作手术的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的人机协作（如手术）中，自然语言指令的歧义性可能带来显著风险，因此需要有效检测和规避这些歧义。

Method: 该方法结合了多个采用不同提示工程技术的LLM评估器，能够分别识别语言、情境、操作和关键性歧义，并引入链式思考型评估器系统性分析指令结构。之后用符合性预测整合各评估器结果，通过与标注校准集对比生成非符合性分数，实现最终判断。

Result: 在Llama 3.2 11B与Gemma 3 12B两种LLM上实验，框架对区分模糊和清晰手术指令的分类准确率均超过60%。

Conclusion: 所提出的方法为手术场景中的人机协作提供了在机器人动作前检测歧义指令的机制，有助于提升协作安全性与可靠性。

Abstract: Ambiguity in natural language instructions poses significant risks in
safety-critical human-robot interaction, particularly in domains such as
surgery. To address this, we propose a framework that uses Large Language
Models (LLMs) for ambiguity detection specifically designed for collaborative
surgical scenarios. Our method employs an ensemble of LLM evaluators, each
configured with distinct prompting techniques to identify linguistic,
contextual, procedural, and critical ambiguities. A chain-of-thought evaluator
is included to systematically analyze instruction structure for potential
issues. Individual evaluator assessments are synthesized through conformal
prediction, which yields non-conformity scores based on comparison to a labeled
calibration dataset. Evaluating Llama 3.2 11B and Gemma 3 12B, we observed
classification accuracy exceeding 60% in differentiating ambiguous from
unambiguous surgical instructions. Our approach improves the safety and
reliability of human-robot collaboration in surgery by offering a mechanism to
identify potentially ambiguous instructions before robot action.

</details>
