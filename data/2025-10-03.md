<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 77]
- [cs.CL](#cs.CL) [Total: 90]
- [cs.RO](#cs.RO) [Total: 41]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: 论文提出了LVTINO，一种基于视频一致性模型（VCM）的零样本或即插即用高分辨率视频修复方法。通过引入VCM先验，显著提升了视频重建质量和计算效率，并确保了视频时序一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于图像的潜在扩散模型（LDM）在图像修复任务上取得了巨大成功，但直接将其逐帧应用到视频修复中会导致时序不一致，难以恢复高质量和连贯的视频。高分辨率视频修复需要同时考虑空间细节恢复和时序依赖，现有方法难以兼顾。

Method: 作者利用最新的视频一致性模型（VCM），将视频潜在扩散模型蒸馏为高速生成器，显式捕获时序因果性。在此基础上，提出LVTINO，通过新颖的条件机制，避免了自动微分，仅需少量深度网络前向计算，即可实现高效的视频反问题求解。

Result: LVTINO在多种视频逆问题任务上，重建结果的感知质量和测量一致性均显著优于现有逐帧图像LDM方法。并且，该方法在保证高还原度和时序流畅性的同时，大大减少了计算开销。

Conclusion: LVTINO促进了高分辨率视频修复领域的发展，创新性地将VCM引入为先验，突破了传统图像LDM逐帧处理带来的限制，树立了重建质量和效率的新标杆。

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [2] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合单张风格参考图片和文本，自主实现细致风格可控的图像生成方法。通过构建Style30k-captions数据集和提出三阶段训练流程，实现了风格信息与文本信息的高效对齐，提升了图像生成的多样性与精度。


<details>
  <summary>Details</summary>
Motivation: 目前主流文本-图像生成方法难以细致、精确地控制生成图像的风格信息，仅仅依靠自然语言难以描述细粒度的风格，而仅用风格图片作为参考也难以与文本生成任务直接对齐。因此，需要发展新的方法来更好地融合风格参考和文本描述，实现更高水平的图像风格控制。

Method: 作者提出了一种三阶段训练的风格提取图像生成新方法。具体包括：引入风格编码器和风格投影层，用单一风格参考图像获得精细风格表征；通过风格投影层，将风格表征与文本表征对齐；最后将风格向量注入至预训练生成模型核心结构中，不需修改现有生成架构，实现风格可控的图像生成。同时，作者自建Style30k-captions数据集，包含图像、风格标签与文本描述，辅助风格编码器和投影层的训练。

Result: 实验结果表明，该方法实现了单图像参考的细致风格可控图像生成，在风格迁移和文本-风格引导生成任务中均表现优异。所构建的数据集也有效提升了模型的泛化能力。

Conclusion: 该研究提出的训练方法和风格对齐机制能实现更高程度的图像风格控制，为生成模型中的风格可控性问题提供了新的方案，并为实际应用中的个性化图像生成和创意设计工具提供了技术基础。

Abstract: Image generation based on text-to-image generation models is a task with
practical application scenarios that fine-grained styles cannot be precisely
described and controlled in natural language, while the guidance information of
stylized reference images is difficult to be directly aligned with the textual
conditions of traditional textual guidance generation. This study focuses on
how to maximize the generative capability of the pretrained generative model,
by obtaining fine-grained stylistic representations from a single given
stylistic reference image, and injecting the stylistic representations into the
generative body without changing the structural framework of the downstream
generative model, so as to achieve fine-grained controlled stylized image
generation. In this study, we propose a three-stage training style
extraction-based image generation method, which uses a style encoder and a
style projection layer to align the style representations with the textual
representations to realize fine-grained textual cue-based style guide
generation. In addition, this study constructs the Style30k-captions dataset,
whose samples contain a triad of images, style labels, and text descriptions,
to train the style encoder and style projection layer in this experiment.

</details>


### [3] [EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels](https://arxiv.org/abs/2510.01362)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: 该论文提出并公开了一个大规模的技能习得过程中的“挣扎”时刻数据集，并对现有时序动作定位模型在检测挣扎时段上的表现进行了评估。


<details>
  <summary>Details</summary>
Motivation: 准确判断技能学习者在学习过程中遇到困难的时刻，对于优化教学和辅助系统的开发至关重要。然而，现有数据集未能关注这种“挣扎”状态随时间的演化，因此缺乏支持相关研究的基础。

Method: 作者收集并整理了一个涵盖61.68小时视频、2793个视频片段、来自76名被试、5385段人工标注挣扎时段的数据集，包含打结、折纸、七巧板、洗牌四类任务，每项任务重复5次以捕捉技能进步。将挣扎检测任务建模为时序动作定位问题，通过模型预测挣扎片段的起止时刻。

Result: 实验证明，时序动作定位模型能够在不同任务乃至不同活动上检测挣扎信号。模型在跨任务泛化时平均mAP为34.56%，跨活动泛化时为19.24%，说明挣扎具有一定的可迁移性，但检测仍存在提升空间。

Conclusion: 本论文公开了第一个关注技能习得过程中挣扎动态演化的数据集，并验证了时序动作定位模型对挣扎识别的有效性。挣扎作为行为线索在多个任务间具有可迁移性，数据集和研究为今后智能辅助学习系统提供了基础。

Abstract: The ability to determine when a person struggles during skill acquisition is
crucial for both optimizing human learning and enabling the development of
effective assistive systems. As skills develop, the type and frequency of
struggles tend to change, and understanding this evolution is key to
determining the user's current stage of learning. However, existing
manipulation datasets have not focused on how struggle evolves over time. In
this work, we collect a dataset for struggle determination, featuring 61.68
hours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle
segments collected from 76 participants. The dataset includes 18 tasks grouped
into four diverse activities -- tying knots, origami, tangram puzzles, and
shuffling cards, representing different task variations. In addition,
participants repeated the same task five times to capture their evolution of
skill. We define the struggle determination problem as a temporal action
localization task, focusing on identifying and precisely localizing struggle
segments with start and end times. Experimental results show that Temporal
Action Localization models can successfully learn to detect struggle cues, even
when evaluated on unseen tasks or activities. The models attain an overall
average mAP of 34.56% when generalizing across tasks and 19.24% across
activities, indicating that struggle is a transferable concept across various
skill-based tasks while still posing challenges for further improvement in
struggle detection. Our dataset is available at
https://github.com/FELIXFENG2019/EvoStruggle.

</details>


### [4] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: SPUS是一种高效且紧凑的基础模型，采用残差U-Net结构，能够解决多种偏微分方程（PDE）问题，拥有极高的参数效率和优良的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前主流的PDE基础模型普遍使用庞大复杂的Transformer架构，导致计算与参数开销大；作者希望探索更为轻量且高效的模型结构以降低资源消耗。

Method: 提出Residual U-Net为核心的统一神经算子，并采用模仿数值解法行为的自回归预训练模式，对多种流体力学PDE进行预训练后，迁移泛化到6类全新物理系统PDE任务。

Result: SPUS在这6类未知下游PDE任务上实现了最先进的泛化效果，所需参数及微调数据量均明显减少，表现出极高的参数效率。

Conclusion: SPUS作为PDE求解的轻量级基础模型，既高效又具强泛化力，有很大应用潜力，将推动PDE相关领域的模型设计向高效率与实用性发展。

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [5] [DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation](https://arxiv.org/abs/2510.01399)
*Shubhankar Borse,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: 本文提出了一种名为DisCo的新方法，通过强化学习从根本上优化文本生成多人物图像时的身份多样性，成功解决了主流模型在多人场景中的面部重复、身份混淆与人数统计错误等顽疾，设置了新的生成质量基准。


<details>
  <summary>Details</summary>
Motivation: 当前主流文本转图像模型在生成包含多个人物的场景时，容易出现人脸复制、身份合并和人数统计错误的问题，严重影响了生成内容的多样性与真实性。解决该‘多人身份危机’对提升多人物生成任务的实用性和真实感极其重要。

Method: 作者提出了DisCo框架，通过引入带有多样性约束的强化学习机制（即Group-Relative Policy Optimization, GRPO）来微调现有流匹配模型。奖励机制具体包含四部分：抑制同图人脸相似度、减少跨样本身份重复、强制人数准确、结合人类偏好评分保障视觉质量。此外，作者设计了单阶段课程学习来适应难度递增，无需额外标注。

Result: 在DiverseHumans数据集上，DisCo获得了98.6%的唯一人脸准确率和近乎完美的全局身份分布效果，在保持感知质量前提下，全面超越了开源与商用方法（如Gemini、GPT-Image）。

Conclusion: DisCo作为第一个基于强化学习、无需标注的多人物生成身份多样性优化框架，不仅有效解决了困扰业界已久的多人物身份危机问题，也为后续相关生成任务树立了新标杆。

Abstract: State-of-the-art text-to-image models excel at realism but collapse on
multi-human prompts - duplicating faces, merging identities, and miscounting
individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the
first RL-based framework to directly optimize identity diversity in multi-human
generation. DisCo fine-tunes flow-matching models via Group-Relative Policy
Optimization (GRPO) with a compositional reward that (i) penalizes intra-image
facial similarity, (ii) discourages cross-sample identity repetition, (iii)
enforces accurate person counts, and (iv) preserves visual fidelity through
human preference scores. A single-stage curriculum stabilizes training as
complexity scales, requiring no extra annotations. On the DiverseHumans
Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global
Identity Spread - surpassing both open-source and proprietary methods (e.g.,
Gemini, GPT-Image) while maintaining competitive perceptual quality. Our
results establish DisCo as a scalable, annotation-free solution that resolves
the long-standing identity crisis in generative models and sets a new benchmark
for compositional multi-human generation.

</details>


### [6] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种新的全球视觉地理定位方法，通过结合层次化地理嵌入和语义分割增强的视觉特征，实现了对图片地理位置的高精度识别，并在多个数据集上刷新了22/25项评测指标。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉地理定位领域取得了显著进展，但如何高效地利用视觉内容学习和表达地理信息仍是一个挑战。因此本文致力于设计更高效、更精确的视觉-地理对齐和融合策略，提升地理定位性能。

Method: 1）建立世界地理嵌入的层次化结构，作为地理表达基础；2）提出高效融合查询图片视觉外观特征与语义分割图的方法，构建更健壮的视觉表征；3）通过上述视觉-地理表征实现地理定位。

Result: 在五个主流基准测试集上的25项指标中，有22项指标超过了现有最优方法（包括最新的大型视觉-语言模型）的表现。消融实验进一步证实，性能提升主要来源于地理和视觉表征的结合。

Conclusion: 将分层地理嵌入与增强型视觉表征结合，有效提升了全球视觉地理定位的准确性及泛化能力，为后续相关方法提供了新的设计方向和技术路径。

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [7] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: 本文提出了一种面向大规模视觉-语言模型（LVLMs）的高效数据筛选方法XMAS，能够大幅减少训练数据量，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据挑选方法在LVLMs上效果有限，且在不同数据子集规模下普遍不优于随机选择，而大规模训练集存在大量冗余，影响训练效率与开销。

Method: 作者证明在指令微调过程中，具有相似跨模态注意力矩阵的样本对模型参数影响相近，可视为携带相同信息。基于此，XMAS方法通过对样本注意力矩阵的奇异值轨迹聚类，从而在各聚类中均衡采样，提取信息最有代表性的子集。代理小模型用于计算聚类指标以节省资源。

Result: XMAS可在LLaVA-1.5-7B模型的训练中，去除LLaVA-665k数据集50%、Vision-Flan数据集85%的样本，模型在10个下游任务上性能无损，并能提升训练速度1.2倍，相对最佳基线方法实现30%的额外数据压缩。

Conclusion: XMAS为LVLMs的指令微调提供了首个理论驱动的数据高效筛选方法，实现了更优的数据利用率和更快的训练速度，具有实际应用和理论参考价值。

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [8] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: 该论文提出了Purrception，一种结合连续和离散优势的变分流匹配方法，用于矢量量化图像生成，在ImageNet上表现优异且训练收敛速度更快。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成方法在连续表征和离散表示之间存在权衡。连续方法具有较好的几何感知能力，离散方法则能利用显式的类别监督，如何结合两者优势并提升训练效率是一个重要问题。

Method: 作者提出Purrception方法，将变分流匹配应用于矢量量化潜编码，通过在连续嵌入空间中计算速度场，同时学习对码本索引的分类后验，实现连续和离散信息的融合，还能实现温度可控与不确定性量化。

Result: 在ImageNet-1k 256x256数据集上，Purrception收敛速度快于连续和离散流匹配基线，在保持竞争性FID表现的同时，提升了训练效率。

Conclusion: Purrception方法有效结合了连续流动与离散监督优势，大幅提升图像生成模型的训练效率，并达到与当前先进模型相当或更优的生成质量。

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [9] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: 本文提出了一种统一的深度学习框架，实现了腹主动脉瘤（AAA）无对比剂CT（NCCT）到合成对比增强CT（CECT）图像的转换，并同时进行主动脉腔与血栓分割。


<details>
  <summary>Details</summary>
Motivation: 现有标准CECT成像需要碘造影剂，存在肾毒性、过敏及环境风险。为减少对比剂使用，用深度学习实现NCCT生成合成CECT已成为前沿课题，但当前多为分阶段流程，误差累计且未利用结构信息。

Method: 方法上，作者提出融合条件扩散模型（CDM）和多任务学习的端到端框架，实现图像生成与解剖分割联合优化。参数在编码器与解码器间共享，并采用半监督机制处理临床常见的分割标签缺失。

Result: 在264例患者数据测试中，模型无论在合成图像质量（PSNR 25.61dB优于23.80dB）还是分割精度（腔体Dice 0.89、血栓Dice 0.53，均优于对比模型）均表现领先，并将临床测量误差显著降低。

Conclusion: 提出的方法不仅提升了合成CECT与分割精度，还能缓解对比剂依赖和数据标签不足，实现更安全高效的AAA影像分析，对临床应用具重要意义。

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [10] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: 本文提出了一种高效的多模态内容分析原型框架，通过融合各类预训练模型，将视频内容转换为可以持续学习并动态扩展的知识图谱形式。


<details>
  <summary>Details</summary>
Motivation: 现有多模态内容分析难度高、计算消耗大，并且将开源预训练模型与复杂数据（如视频）结合非常具有挑战性。因此亟需一种高效、可扩展的分析方法。

Method: 作者设计了一套管道流程，结合多种预训练模型，将视频处理为时序的半结构化数据，并进一步转化为帧级索引的知识图谱，使其可检索且支持持续学习与交互式知识注入。

Result: 该框架可高效搭建多模态内容分析管道，使原本复杂的视频内容以结构化知识图谱的形式表达，便于查询和扩展。

Conclusion: 所提出的框架降低了多模态视频内容分析的门槛，提高了效率与灵活性，支持动态补充领域知识，具有良好的应用前景。

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [11] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: 本文提出WALT框架，通过反向工程网站的功能，将其抽象为可复用的自动化工具，大幅提高Web代理在浏览器自动化任务中的稳定性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有Web代理通常依赖逐步UI操作和大型语言模型（LLM）推理，这在处理动态网页布局和长任务时非常脆弱，无法达到人类轻松调用站内工具的高效模式。

Method: WALT通过逆向挖掘网站中内置的高层次功能，将其封装为标准化工具（如search、filter等），使代理通过调用工具方法来完成任务，而非低层级的页面点击和输入操作，降低了推理和执行的复杂性。

Result: 在VisualWebArena和WebArena这两个基准上，WALT实现了更高的任务成功率，用更少的步骤和更低的LLM推理需求完成网页自动化。

Conclusion: WALT框架为浏览器自动化引入了更健壮、可推广的新范式，通过工具调度显著提高了Web代理的有效性和泛化能力。

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [12] [MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2510.01532)
*Meilong Xu,Xiaoling Hu,Shahira Abousamra,Chen Li,Chao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的半监督分割方法，通过引入扰动预测和多时刻快照，提升对无标注数据中拓扑结构的识别和保持能力。该方法能有效区分真实结构与噪声，显著提升组织病理图像的分割准确性。


<details>
  <summary>Details</summary>
Motivation: 在组织病理图像等密集物体分布场景下，精准获取无标注数据中的有意义语义结构很困难，传统方法难以在无监督条件下稳定保持生物学相关的拓扑特征。

Method: 方法核心是结合随机dropout扰动和训练过程中的多时刻模型快照，生成多种预测结果，通过增强这些预测之间的拓扑特征一致性以去除伪影和噪声。提出一种结合空间重叠和全局结构对齐的新特征匹配策略，实现无标注情形下的特征关联。

Result: 大量实验证明，该方法有效降低了分割中的拓扑错误，提高了分割的健壮性和准确率，有利于下游分析任务。

Conclusion: 通过拓扑一致性约束与新颖的特征匹配策略，本方法在半监督分割场景下有效增强了病理图像分割的准确性和可靠性。

Abstract: In semi-supervised segmentation, capturing meaningful semantic structures
from unlabeled data is essential. This is particularly challenging in
histopathology image analysis, where objects are densely distributed. To
address this issue, we propose a semi-supervised segmentation framework
designed to robustly identify and preserve relevant topological features. Our
method leverages multiple perturbed predictions obtained through stochastic
dropouts and temporal training snapshots, enforcing topological consistency
across these varied outputs. This consistency mechanism helps distinguish
biologically meaningful structures from transient and noisy artifacts. A key
challenge in this process is to accurately match the corresponding topological
features across the predictions in the absence of ground truth. To overcome
this, we introduce a novel matching strategy that integrates spatial overlap
with global structural alignment, minimizing discrepancies among predictions.
Extensive experiments demonstrate that our approach effectively reduces
topological errors, resulting in more robust and accurate segmentations
essential for reliable downstream analysis. Code is available at
\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.

</details>


### [13] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: 本文提出了Diffusion-LPO框架，实现了扩散模型中的序列偏好(listwise preference)优化，并在多项任务上超越了已有的成对偏好(pairwise preference)方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成（T2I）扩散模型主要利用人类成对偏好反馈进行优化，而忽视了人类反馈中更丰富的排序信息，未能充分利用更精细的偏好层次。

Method: 作者提出Diffusion-LPO方法，以用户对图片的排序偏好为基础，在Plackett-Luce模型下扩展了直接偏好优化（DPO）目标，使模型能够高效地利用序列偏好信息提升对人类反馈的对齐能力。该方法鼓励每个样本在全局排序中优于其排名更低的样本。

Result: Diffusion-LPO在文本到图像生成、图片编辑和个性化偏好对齐等任务中，显著优于传统的成对DPO基线，在图像质量与偏好对齐方面都表现更好。

Conclusion: Diffusion-LPO能有效提升扩散模型对人类偏好的对齐能力，比现有成对偏好优化方法有更好表现，具有实际应用价值。

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness
for aligning text-to-image (T2I) diffusion models with human preferences.
Although Direct Preference Optimization (DPO) is widely adopted for its
computational efficiency and avoidance of explicit reward modeling, its
applications to diffusion models have primarily relied on pairwise preferences.
The precise optimization of listwise preferences remains largely unaddressed.
In practice, human feedback on image preferences often contains implicit ranked
information, which conveys more precise human preferences than pairwise
comparisons. In this work, we propose Diffusion-LPO, a simple and effective
framework for Listwise Preference Optimization in diffusion models with
listwise data. Given a caption, we aggregate user feedback into a ranked list
of images and derive a listwise extension of the DPO objective under the
Plackett-Luce model. Diffusion-LPO enforces consistency across the entire
ranking by encouraging each sample to be preferred over all of its lower-ranked
alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO
across various tasks, including text-to-image generation, image editing, and
personalized preference alignment. Diffusion-LPO consistently outperforms
pairwise DPO baselines on visual quality and preference alignment.

</details>


### [14] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种统一的纯自回归多模态大模型Bridge，既能理解图像也能生成高质量图像，其创新的语义-像素组合表示提升了表达精度，并在多个基准上展现出卓越性能，且训练数据和时间更少。


<details>
  <summary>Details</summary>
Motivation: 以往多模态大模型（MLLM）要么牺牲了自回归统一性，要么在图像理解与生成之间存在精度和语义对齐的权衡。作者期望构建一个在同一模型和自回归框架下，实现文本与图像高效统一理解及生成的MLLM。

Method: 作者提出Bridge模型，采用纯自回归的Mixture-of-Transformers结构，将预训练视觉理解模型扩展生成能力，同时提出融合紧凑语义token和细粒度像素token的语义-像素离散表示，将序列长度仅增加7.9%的代价换取更高的对齐度和细节表达力。

Result: 在多个多模态理解和图像生成基准上，Bridge表现出与或优于现有统一MLLMs的效果，且相比以往方法，训练数据需求更低，训练所需时间更少。

Conclusion: Bridge模型在有效提升多模态统一理解与生成能力的同时优化了训练效率，是推动MLLMs发展的有力方案。

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [15] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: 提出了结合CNN与贝叶斯深度学习的混合模型，用于数据稀缺环境下的口腔癌分类，显著提升了模型泛化性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 口腔癌死亡率高，尤其在缺乏医疗资源的地区。现有深度学习方法依赖大数据集且过度自信，不适用于小样本环境，急需可推广且能表达不确定性的诊断模型。

Method: 基于小数据集，提出将卷积神经网络（CNN）与贝叶斯深度学习结合，采用变分推断实现不确定性量化，对智能手机拍摄的彩色图片进行训练和评估。

Result: 在与训练集分布相似的测试集上，模型准确率达到94%；在更贴近真实世界、数据分布变化大的测试集上，准确率达88%，明显高于传统CNN（72.94%）。模型对正确分类样本表现出低不确定性，对错误分类样本表现出高不确定性。

Conclusion: 引入贝叶斯推断的混合模型在小样本环境下有效提升了口腔癌早筛的可靠性与泛化能力，为资源匮乏地区的早期诊断提供了可行方案。

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [16] [Consistent Assistant Domains Transformer for Source-free Domain Adaptation](https://arxiv.org/abs/2510.01559)
*Renrong Shao,Wei Zhang,Kangyang Luo,Qin Li,and Jun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的源数据不可访问的领域自适应方法CADTrans，通过辅助域变换器提升领域一致性，显著提升迁移学习性能。


<details>
  <summary>Details</summary>
Motivation: 源数据不可访问的领域自适应（SFDA）难以获得稳定的、不变的特征，现有方法在处理领域不变性和样本难度时表现有限，受域偏差影响大。

Method: 提出Consistent Assistant Domains Transformer（CADTrans）。该方法设计了辅助域模块，通过聚合全局注意力获得多样化的中间表示；应用多种一致性策略提取不变特征，用于区分难易样本；最终利用条件多核最大均值差异（CMK-MMD）准则对难样本进行细分类别对齐。

Result: 在多个主流领域适应基准（Office-31、Office-Home、VISDA-C、DomainNet-126）上，CADTrans取得了显著优于现有方法的性能提升。

Conclusion: CADTrans通过辅助域与一致性策略，有效提升了SFDA任务中的特征不变性与难样本适应性，为源不可见场景下领域自适应提供了有效方案。

Abstract: Source-free domain adaptation (SFDA) aims to address the challenge of
adapting to a target domain without accessing the source domain directly.
However, due to the inaccessibility of source domain data, deterministic
invariable features cannot be obtained. Current mainstream methods primarily
focus on evaluating invariant features in the target domain that closely
resemble those in the source domain, subsequently aligning the target domain
with the source domain. However, these methods are susceptible to hard samples
and influenced by domain bias. In this paper, we propose a Consistent Assistant
Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue
by constructing invariable feature representations of domain consistency.
Concretely, we develop an assistant domain module for CADTrans to obtain
diversified representations from the intermediate aggregated global attentions,
which addresses the limitation of existing methods in adequately representing
diversity. Based on assistant and target domains, invariable feature
representations are obtained by multiple consistent strategies, which can be
used to distinguish easy and hard samples. Finally, to align the hard samples
to the corresponding easy samples, we construct a conditional multi-kernel max
mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same
category and those of different categories. Extensive experiments are conducted
on various benchmarks such as Office-31, Office-Home, VISDA-C, and
DomainNet-126, proving the significant performance improvements achieved by our
proposed approaches. Code is available at
https://github.com/RoryShao/CADTrans.git.

</details>


### [17] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: 本文提出利用历史盲人及低视力（BLV）用户提出的问题，改进多模态大语言模型（MLLMs）生成的图像描述，从而提升其针对性和实用性。


<details>
  <summary>Details</summary>
Motivation: 传统视觉解释应用自动生成冗长且详细的描述，忽视了BLV用户真实需求，导致信息效率低。作者希望生成更贴合用户关注的信息，提高交流效率。

Method: 系统引入 VizWiz-LF 数据集中历史 BLV 用户的问题来指导MLLMs。对给定图像，系统检索相似的以往视觉情境，并利用相关问题，驱动模型生成更具语境相关性的描述。

Result: 通过三位人工标记者评审92组有无语境感知的描述，发现语境感知型描述在76.1%的情况下能够预判和回答用户问题，并在54.4%的对比中被标记为更佳描述。

Conclusion: 基于BLV用户历史提问引导生成的描述，更能满足用户需求，提升描述实用性和偏好度，对增强视觉辅助系统有重要意义。

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [18] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: 本文提出了ImageNet-Think，这是一个专为提升视觉语言模型（VLMs）推理能力而设计的多模态推理数据集，包含25万张图像及配套的推理与答案过程。


<details>
  <summary>Details</summary>
Motivation: 目前VLMs虽在图像-文本理解上取得进展，但缺乏对推理过程的明确建模。该数据集旨在通过提供结构化推理过程，促进VLMs在推理及多步推断的研究。

Method: 利用ImageNet21k的25万张图片，分别由GLM-4.1V-9B-Thinking和Kimi-VL-A3B-Thinking-2506两个先进VLM模型对每张图片生成两套推理及答案对，从而合成包含推理过程和结果的多模态数据集。

Result: 构建出了结构化推理/答案对齐的ImageNet-Think数据集。每图配备两对详细推理-答案序列，能够用于VLMs的推理能力训练及评测。

Conclusion: ImageNet-Think为提升VLM的推理能力和理解多模态推理机制提供了公开、规模化的数据支持，对未来推理型多模态AI发展和研究具有推动作用。

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [19] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 本文提出了一种新型正则化方法NPN，通过在传感矩阵零空间进行非线性投影，用神经网络建模，适用于多种成像逆问题，并在多个逆问题任务和方法上提升了重建精度。


<details>
  <summary>Details</summary>
Motivation: 成像逆问题中，传统先验多关注图像域结构，忽视了测量矩阵零空间的任务特异性结构，导致解的歧义不能有效消除。作者希望设计一种能利用零空间结构信息的新型先验。

Method: 提出了非线性投影零空间（NPN）正则化方法，即用神经网络对测量矩阵零空间低维投影建模，作为先验引入到重建过程中。该正则项可灵活结合现有plug-and-play、unrolling等重建框架，并给出了收敛及精度理论保证。

Result: 在压缩感知、去模糊、超分辨、CT和MRI等多个逆问题场景下，NPN先验能兼容不同重建方法，并显著提升了重建保真度。

Conclusion: NPN作为关注零空间的新型正则化方法，既具可解释性、又有互补性和灵活性，在多种成像逆问题中取得了优异表现，可与现有先验共同提升重建效果。

Abstract: Imaging inverse problems aims to recover high-dimensional signals from
undersampled, noisy measurements, a fundamentally ill-posed task with infinite
solutions in the null-space of the sensing operator. To resolve this ambiguity,
prior information is typically incorporated through handcrafted regularizers or
learned models that constrain the solution space. However, these priors
typically ignore the task-specific structure of that null-space. In this work,
we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel
class of regularization that, instead of enforcing structural constraints in
the image domain, promotes solutions that lie in a low-dimensional projection
of the sensing matrix's null-space with a neural network. Our approach has two
key advantages: (1) Interpretability: by focusing on the structure of the
null-space, we design sensing-matrix-specific priors that capture information
orthogonal to the signal components that are fundamentally blind to the sensing
process. (2) Flexibility: NPN is adaptable to various inverse problems,
compatible with existing reconstruction frameworks, and complementary to
conventional image-domain priors. We provide theoretical guarantees on
convergence and reconstruction accuracy when used within plug-and-play methods.
Empirical results across diverse sensing matrices demonstrate that NPN priors
consistently enhance reconstruction fidelity in various imaging inverse
problems, such as compressive sensing, deblurring, super-resolution, computed
tomography, and magnetic resonance imaging, with plug-and-play methods,
unrolling networks, deep image prior, and diffusion models.

</details>


### [20] [Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics](https://arxiv.org/abs/2510.01618)
*Zijun Li,Jinchang Zhang,Ming Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: 本文提出了一个自动基因组解释模块，可将原始DNA序列转化为可应用于医学自动化和机器人系统的可解释决策。


<details>
  <summary>Details</summary>
Motivation: 当前基因组信息处理缺乏可直接用于自动化决策且具备生物学可解释性的端到端解决方案。为实现精准医学与自动化结合，需开发既可靠又可解释的基因组解读方法。

Method: 框架结合混沌游戏表示（CGR）与概念瓶颈模型（CBM），在预测中引入GC含量、CpG密度、k-mer基序等生物学概念，并通过概念保真度监督、先验一致性对齐、KL分布匹配及不确定性校准提升可靠性。同时加入成本感知推荐层，将预测结果转化为兼顾精度和临床效益的决策策略。

Result: 在内部和LANL（洛斯阿拉莫斯国家实验室）数据集上对HIV亚型分类取得了最先进的准确率，更高的概念预测保真度，并在成本-效益权衡上优于现有基线方法。

Conclusion: 该系统实现了可验证的生物学解释、准确分类和优化临床决策，推进了可解释基因组建模与自动化决策的结合，为基因组医学领域的机器人和临床自动化奠定了可靠基础。

Abstract: We propose an automated genomic interpretation module that transforms raw DNA
sequences into actionable, interpretable decisions suitable for integration
into medical automation and robotic systems. Our framework combines Chaos Game
Representation (CGR) with a Concept Bottleneck Model (CBM), enforcing
predictions to flow through biologically meaningful concepts such as GC
content, CpG density, and k mer motifs. To enhance reliability, we incorporate
concept fidelity supervision, prior consistency alignment, KL distribution
matching, and uncertainty calibration. Beyond accurate classification of HIV
subtypes across both in-house and LANL datasets, our module delivers
interpretable evidence that can be directly validated against biological
priors. A cost aware recommendation layer further translates predictive outputs
into decision policies that balance accuracy, calibration, and clinical
utility, reducing unnecessary retests and improving efficiency. Extensive
experiments demonstrate that the proposed system achieves state of the art
classification performance, superior concept prediction fidelity, and more
favorable cost benefit trade-offs compared to existing baselines. By bridging
the gap between interpretable genomic modeling and automated decision-making,
this work establishes a reliable foundation for robotic and clinical automation
in genomic medicine.

</details>


### [21] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu*

Main category: cs.CV

TL;DR: 本文提出了VLA-R1模型，通过强化学习与可验证奖励机制增强视觉-语言-动作（VLA）模型的推理能力和执行准确性，并构建高质量链式思维数据集，显著提升了泛化和真实场景能力。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型缺乏逐步推理，忽视可供性约束与几何关系，且后训练过程推理强化不足，主要依赖监督微调与弱奖励，限制了其在复杂任务上的泛化和表现。

Method: 提出VLA-R1模型，采用融合了可验证奖励的强化学习（RLVR）与组相对策略优化（GRPO）进行后训练，针对区域对齐、轨迹一致性与输出格式设计了奖励机制。此外，构建VLA-CoT-13K数据集，提供与可供性和轨迹注释对齐的链式思维监督。

Result: 在同域、异域、仿真和真实机器人平台上的大量实验表明，VLA-R1在泛化能力和真实世界表现方面均优于先前VLA方法。

Conclusion: VLA-R1显著提升了VLA模型的推理稳健性和执行效果，推动了具身智能领域的通用能力发展。模型、代码和数据集将在发表后开源。

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language
understanding, and action generation, offering strong cross-task and
cross-scene generalization with broad impact on embodied AI. However, current
VLA models often lack explicit step-by-step reasoning, instead emitting final
actions without considering affordance constraints or geometric relations.
Their post-training pipelines also rarely reinforce reasoning quality, relying
primarily on supervised fine-tuning with weak reward design. To address these
challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates
Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative
Policy Optimization (GRPO) to systematically optimize both reasoning and
execution. Specifically, we design an RLVR-based post-training strategy with
verifiable rewards for region alignment, trajectory consistency, and output
formatting, thereby strengthening reasoning robustness and execution accuracy.
Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides
chain-of-thought supervision explicitly aligned with affordance and trajectory
annotations. Furthermore, extensive evaluations on in-domain, out-of-domain,
simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior
generalization and real-world performance compared to prior VLA methods. We
plan to release the model, code, and dataset following the publication of this
work. Code: https://github.com/GigaAI-research/VLA-R1. Website:
https://gigaai-research.github.io/VLA-R1.

</details>


### [22] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种联合去模糊与三维重建的方法，专为宏观摄影中的多视角图像而设计，有效提升了高分辨率微观物体的建模质量。


<details>
  <summary>Details</summary>
Motivation: 宏观镜头可实现高分辨率与大倍率成像，适合小巧复杂物体的三维建模，但其普遍存在景深模糊问题，严重影响成像清晰度和后续的高质量三维重建。以往去模糊方法通常需要大量图像和标注，且缺乏专门针对宏观摄影的多视角三维重建手段，亟需新方法突破。

Method: 作者提出从少量多视角模糊图像出发，采用可微渲染技术，通过自监督的方式同时优化物体的清晰三维模型和每个像素的对焦模糊核。此方法无需大量标注数据，联合求解去模糊与三维建模问题。

Result: 大量实验结果表明，在少量多视角图像输入下，该方法能有效实现高质量的图像去模糊，并重建出高保真的三维物体外观。

Conclusion: 针对宏观摄影中不可避免的景深模糊，本文通过自监督联合优化，有效解决了小样本条件下的去模糊和三维重建难题，具备实际应用潜力，对相关领域具有重要推动作用。

Abstract: Macro lens has the advantages of high resolution and large magnification, and
3D modeling of small and detailed objects can provide richer information.
However, defocus blur in macrophotography is a long-standing problem that
heavily hinders the clear imaging of the captured objects and high-quality 3D
reconstruction of them. Traditional image deblurring methods require a large
number of images and annotations, and there is currently no multi-view 3D
reconstruction method for macrophotography. In this work, we propose a joint
deblurring and 3D reconstruction method for macrophotography. Starting from
multi-view blurry images captured, we jointly optimize the clear 3D model of
the object and the defocus blur kernel of each pixel. The entire framework
adopts a differentiable rendering method to self-supervise the optimization of
the 3D model and the defocus blur kernel. Extensive experiments show that from
a small number of multi-view images, our proposed method can not only achieve
high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [23] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为FideDiff的新型单步扩散模型，有效提升了运动去模糊任务的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于CNN和Transformer的去模糊模型虽有进展，但无法兼顾高品质生成能力与推理速度；而大规模预训练扩散模型带来更好生成效果，却面临推理时间长、图像保真度受限等问题，限制了其实际应用。

Method: 将运动去模糊问题重新表述为类似扩散的过程，每个时间步对应不同的模糊程度，通过训练一致性模型让所有时间步都指向同一张清晰图像。构建与模糊轨迹匹配的训练数据，实现时序一致性并支持一步还原。进一步引入Kernel ControlNet用于模糊核估计，并提出自适应时间步预测，提升整体性能。

Result: FideDiff在主流全参考指标上优于以往扩散法，并能达成主流SOTA方法相当水平，凸显其实用价值。

Conclusion: FideDiff为利用预训练扩散模型进行高保真图像复原提供了新思路和有力基线，有望推动扩散模型在工业实际中的应用。

Abstract: Recent advancements in image motion deblurring, driven by CNNs and
transformers, have made significant progress. Large-scale pre-trained diffusion
models, which are rich in true-world modeling, have shown great promise for
high-quality image restoration tasks such as deblurring, demonstrating stronger
generative capabilities than CNN and transformer-based methods. However,
challenges such as unbearable inference time and compromised fidelity still
limit the full potential of the diffusion models. To address this, we introduce
FideDiff, a novel single-step diffusion model designed for high-fidelity
deblurring. We reformulate motion deblurring as a diffusion-like process where
each timestep represents a progressively blurred image, and we train a
consistency model that aligns all timesteps to the same clean image. By
reconstructing training data with matched blur trajectories, the model learns
temporal consistency, enabling accurate one-step deblurring. We further enhance
model performance by integrating Kernel ControlNet for blur kernel estimation
and introducing adaptive timestep prediction. Our model achieves superior
performance on full-reference metrics, surpassing previous diffusion-based
methods and matching the performance of other state-of-the-art models. FideDiff
offers a new direction for applying pre-trained diffusion models to
high-fidelity image restoration tasks, establishing a robust baseline for
further advancing diffusion models in real-world industrial applications. Our
dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [24] [LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition](https://arxiv.org/abs/2510.01651)
*Rixin Zhou,Peiqiang Qiu,Qian Zhang,Chuntao Li,Xi Yang*

Main category: cs.CV

TL;DR: 本文提出了一套自动识别中国青铜器铭文的方案，利用自建大规模数据集和先进的识别算法，有效提升跨领域与罕见字的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的青铜器铭文识别面临图像退化、跨媒介变化和极度长尾字分布等难题，影响考古和历史研究的自动化与效率。

Method: 作者首先整理并公开了一个大型的青铜器铭文数据集，涵盖多种图片来源和6658个独特字符。方法上，提出了两阶段的识别流程：先定位铭文字，然后转换为单字识别。为增强对不同领域和稀有字符的适应力，引入了LadderMoE结构，对预训练CLIP编码器加入专家适配器，实现动态专家分工。

Result: 实验证明，提出的方法在单字和整页识别任务上均明显优于当前主流文本识别基线，尤其在常见字符、次常见字符与稀有字符及多种图像采集方式下，均取得更高准确率。

Conclusion: 本工作为青铜器铭文自动识别奠定了坚实基础，并有助于进一步推动考古文献分析的自动化与深入研究。

Abstract: Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial
stage of early Chinese writing and provide indispensable evidence for
archaeological and historical studies. However, automatic BI recognition
remains difficult due to severe visual degradation, multi-domain variability
across photographs, rubbings, and tracings, and an extremely long-tailed
character distribution. To address these challenges, we curate a large-scale BI
dataset comprising 22454 full-page images and 198598 annotated characters
spanning 6658 unique categories, enabling robust cross-domain evaluation.
Building on this resource, we develop a two-stage detection-recognition
pipeline that first localizes inscriptions and then transcribes individual
characters. To handle heterogeneous domains and rare classes, we equip the
pipeline with LadderMoE, which augments a pretrained CLIP encoder with
ladder-style MoE adapters, enabling dynamic expert specialization and stronger
robustness. Comprehensive experiments on single-character and full-page
recognition tasks demonstrate that our method substantially outperforms
state-of-the-art scene text recognition baselines, achieving superior accuracy
across head, mid, and tail categories as well as all acquisition modalities.
These results establish a strong foundation for bronze inscription recognition
and downstream archaeological analysis.

</details>


### [25] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA通过在主干网络前添加视觉重编程层实现无监督域自适应（UDA），显著减少参数量和存储需求，并且性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法每次遇到新源域-目标域对都需要对整个主干参数进行微调，会造成参数和存储需求线性增长，且无法重复利用已训练主干。该文旨在克服这一缺陷，提升UDA效率与可复用性。

Method: 提出VirDA方法，通过在主干网络前加一层域特异性视觉重编程层（visual reprogramming layer），利用视觉提示（visual prompts）注入纹理偏置调整输入图像风格，实现域自适应，仅优化该层参数，不修改主干网络。为视觉重编程层设计多重目标函数优化域内与域间分布差异。

Result: 在Office-31数据集上，VirDA以仅150万可训练参数取得92.8%平均准确率，超越当前参数高效UDA方法PDA (+1.6%，参数量仅为PDA的46%)，也超越主干全量微调方法CDTrans和FixBi，同时参数量仅为后者的1.7%和2.8%。与最强方法PMTrans和TVT相比，VirDA参数量约1.7%，准确率仅略有下降（2.2%、1.1%）。

Conclusion: VirDA实现了参数高效、可复用的无监督域自适应，在保持甚至超过现有方法性能的同时，大幅降低训练参数和存储需求，具有实际应用潜力。

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [26] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: 该论文提出了一种新的无监督、数据驱动的人脸表情编码方法，将面部表情转化为紧凑且可解释的离散特征表示，并展示了其在心理学相关任务中的优越表现。


<details>
  <summary>Details</summary>
Motivation: 当前主流的面部表情分析系统如FACS存在覆盖面有限与人工标注成本高等问题，制约了大规模、自动化的人脸情感分析。作者希望寻找一种自动且广泛适用的新方法，以提升行为理解效率。

Method: 首先利用3D可变形模型(3DMM)从图像中提取与身份无关的表情特征，去除头部姿态、面部几何等因素干扰。随后通过残差向量量化变分自编码器(RVQ-VAE)对这些特征进行离散化编码，形成共享码本，每个码代表一种可重复利用的面部变形模式。最终用Bag-of-Words模型在心理学任务上评估表现。

Result: 实验证明，该方法比FACS以及其他主流面部表情编码方法能提取更细致的面部行为特征。在压力检测、人格预测和抑郁检测三个高层心理学任务中，本文模型均优于FACS及强大的图像视频表示学习方法（如Masked Autoencoders）。进一步分析还表明该方法对面部表情的覆盖更全面。

Conclusion: Discrete Facial Encoding方法不仅可扩展性强，表现卓越，且覆盖多样性高，有望取代FACS成为心理与情感计算领域新一代标准工具。

Abstract: Facial expression analysis is central to understanding human behavior, yet
existing coding systems such as the Facial Action Coding System (FACS) are
constrained by limited coverage and costly manual annotation. In this work, we
introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven
alternative of compact and interpretable dictionary of facial expressions from
3D mesh sequences learned through a Residual Vector Quantized Variational
Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant
expression features from images using a 3D Morphable Model (3DMM), effectively
disentangling factors such as head pose and facial geometry. We then encode
these features using an RVQ-VAE, producing a sequence of discrete tokens from a
shared codebook, where each token captures a specific, reusable facial
deformation pattern that contributes to the overall expression. Through
extensive experiments, we demonstrate that Discrete Facial Encoding captures
more precise facial behaviors than FACS and other facial encoding alternatives.
We evaluate the utility of our representation across three high-level
psychological tasks: stress detection, personality prediction, and depression
detection. Using a simple Bag-of-Words model built on top of the learned
tokens, our system consistently outperforms both FACS-based pipelines and
strong image and video representation learning models such as Masked
Autoencoders. Further analysis reveals that our representation covers a wider
variety of facial displays, highlighting its potential as a scalable and
effective alternative to FACS for psychological and affective computing
applications.

</details>


### [27] [Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale](https://arxiv.org/abs/2510.01665)
*Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang*

Main category: cs.CV

TL;DR: 本文提出了一种用于非刚性结构光流（NRSfM）新方法Con-NRSfM，能处理包括等距变形在内的共形变形，突破了以往方法的局限，并实现了更精确、鲁棒的重建。


<details>
  <summary>Details</summary>
Motivation: 现有NRSfM方法在处理单目视觉可变形SLAM时，对变形假定较为苛刻（如局部平面、线性变形），无法恢复共形比例，且深度与尺度耦合严重，限制了精度与适用范围。

Method: 提出基于图优化的点对点2D图像形变重建，无需局部平面或线性假设，并精确计算局部共形尺度；将深度与共形尺度解耦，采用并行可分离迭代优化；结合自监督的编码-解码网络，生成带纹理稠密3D点云。

Result: 在合成和真实数据集上与主流方法对比，本文方法在重建精度和鲁棒性方面表现更优。

Conclusion: Con-NRSfM摆脱了传统NRSfM严格假设的束缚，实现了对共形变形下非刚性结构的高精度重建，对单目可变形SLAM具有重要意义。

Abstract: Non-rigid structure-from-motion (NRSfM), a promising technique for addressing
the mapping challenges in monocular visual deformable simultaneous localization
and mapping (SLAM), has attracted growing attention. We introduce a novel
method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing
isometric deformations as a subset. Our approach performs point-wise
reconstruction using 2D selected image warps optimized through a graph-based
framework. Unlike existing methods that rely on strict assumptions, such as
locally planar surfaces or locally linear deformations, and fail to recover the
conformal scale, our method eliminates these constraints and accurately
computes the local conformal scale. Additionally, our framework decouples
constraints on depth and conformal scale, which are inseparable in other
approaches, enabling more precise depth estimation. To address the sensitivity
of the formulated problem, we employ a parallel separable iterative
optimization strategy. Furthermore, a self-supervised learning framework,
utilizing an encoder-decoder network, is incorporated to generate dense 3D
point clouds with texture. Simulation and experimental results using both
synthetic and real datasets demonstrate that our method surpasses existing
approaches in terms of reconstruction accuracy and robustness. The code for the
proposed method will be made publicly available on the project website:
https://sites.google.com/view/con-nrsfm.

</details>


### [28] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: 本文提出了UniVerse框架，通过视频扩散模型提升多视角不一致图像的3D鲁棒重建效果。框架首先将图像转为视频并进行一致性恢复，再基于恢复后的图像重建3D场景，实验显示效果优越且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有结合图像退化建模与神经3D重建的方法，对观测数据稠密性要求高，难以在稀疏或极度不一致的多视图场景下优化模型参数，导致鲁棒性受限。研究动机在于提升在多视角图像不一致、数据有限的现实场景下的3D重建鲁棒性和泛化能力。

Method: 方法上，作者把鲁棒重建分为图像恢复和3D重建两个子任务。首先将输入的不一致图像转换为初始视频，再用特定设计的视频扩散模型将其恢复为一致的图像，最后用这些高质量图像进行3D重建。该扩散模型通过大规模数据学习通用场景先验，突破了逐视角退化建模的局限。

Result: 在综合合成与真实数据集上的大量实验表明，UniVerse不仅泛化能力强、适用多种图像退化类型，在鲁棒重建任务上也全面优于现有方法。

Conclusion: UniVerse通过视频扩散模型顺利分离与解决多视角图像恢复及3D重建两环节，有效简化优化流程，提升3D重建精度且丰富了可控性（如风格控制），有望广泛应用于复杂和现实场景的3D重建任务。

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of
reconstructing a 3D scene from a set of inconsistent multi-view images. Some
recent works have attempted to simultaneously remove image inconsistencies and
perform reconstruction by integrating image degradation modeling into neural 3D
scene representations.However, these methods rely heavily on dense observations
for robustly optimizing model parameters.To address this issue, we propose to
decouple robust reconstruction into two subtasks: restoration and
reconstruction, which naturally simplifies the optimization process.To this
end, we introduce UniVerse, a unified framework for robust reconstruction based
on a video diffusion model. Specifically, UniVerse first converts inconsistent
images into initial videos, then uses a specially designed video diffusion
model to restore them into consistent images, and finally reconstructs the 3D
scenes from these restored images.Compared with case-by-case per-view
degradation modeling, the diffusion model learns a general scene prior from
large-scale data, making it applicable to diverse image
inconsistencies.Extensive experiments on both synthetic and real-world datasets
demonstrate the strong generalization capability and superior performance of
our method in robust reconstruction. Moreover, UniVerse can control the style
of the reconstructed 3D scene. Project page:
https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [29] [An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution](https://arxiv.org/abs/2510.01678)
*Ke Jia,Ji Zhou,Hanxin Li,Zhigan Zhou,Haojie Chu,Xiaojie Li*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级端到端的模板匹配方法，用于工业场景下高效且精准地进行目标定位及几何状态估计（包括旋转与缩放），显著提高了推理效率和实用性。


<details>
  <summary>Details</summary>
Motivation: 传统模板匹配通过遍历各角度和尺度，效率低下，难以适应实际工业环境；现有深度学习方法虽然提取相似度，但很少直接建模几何姿态，无法满足工业部署的高精度和实时要求。

Method: 方法上，作者将模板匹配重新表述为联合定位和几何回归任务，网络直接输出目标中心坐标、旋转角度及独立横纵尺度。设计了模板感知动态卷积模块（TDCM），在推理时注入模板特征以增强泛化能力，且网络采用深度可分离卷积和像素洗牌实现高效推理。此外，提出基于旋转-错切的数据增强和结构感知伪标签，实现无人工几何标注的训练，并引入精细化模块提升角度与尺度的精度。

Result: 模型仅3.07M，推理时间14ms，在复杂变换（旋转+缩放+错切）场景下达到高精度。小模板和多目标场景下也展现出极强鲁棒性。

Conclusion: 新方法兼顾高效、精确和泛化，适合实际工业的实时部署，且无需额外几何标注。代码已开源。

Abstract: In industrial inspection and component alignment tasks, template matching
requires efficient estimation of a target's position and geometric state
(rotation and scaling) under complex backgrounds to support precise downstream
operations. Traditional methods rely on exhaustive enumeration of angles and
scales, leading to low efficiency under compound transformations. Meanwhile,
most deep learning-based approaches only estimate similarity scores without
explicitly modeling geometric pose, making them inadequate for real-world
deployment. To overcome these limitations, we propose a lightweight end-to-end
framework that reformulates template matching as joint localization and
geometric regression, outputting the center coordinates, rotation angle, and
independent horizontal and vertical scales. A Template-Aware Dynamic
Convolution Module (TDCM) dynamically injects template features at inference to
guide generalizable matching. The compact network integrates depthwise
separable convolutions and pixel shuffle for efficient matching. To enable
geometric-annotation-free training, we introduce a rotation-shear-based
augmentation strategy with structure-aware pseudo labels. A lightweight
refinement module further improves angle and scale precision via local
optimization. Experiments show our 3.07M model achieves high precision and 14ms
inference under compound transformations. It also demonstrates strong
robustness in small-template and multi-object scenarios, making it highly
suitable for deployment in real-time industrial applications. The code is
available at:https://github.com/ZhouJ6610/PoseMatch-TDCM.

</details>


### [30] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种自适应像素推理框架，显著提升了VLMs在细粒度多模态任务中的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM在多模态任务上表现优异，但在需要细致处理和理解图像细节的任务中，常因编码信息损失或对关键区域关注不足而表现有限。近期引入像素级信息虽提升了细节理解，但却导致运算冗余与注意力分散。为提升效率，并避免信息过度使用，有必要实现按需动态调用像素级操作。

Method: 作者首先通过操作感知的有监督微调，使模型具备文本推理及图像操作能力。随后提出一种新颖的基于rollout引导的强化学习方法，根据模型自身答复反馈，动态决定是否调用像素级操作，实现不同难度查询的精细调控。

Result: 在多个多模态推理基准测试中，该方法不仅提升了整体性能，更显著减少了不必要的像素级操作。例如，在HR-Bench 4K数据集上，模型准确率达到73.4%，而工具调用率仅20.1%，相较先前方法准确率提升且工具使用减少66.5%。

Conclusion: 提出的自适应像素推理框架兼顾了高效性和准确性，为VLM细粒度多模态推理任务提供了有效解决方案，对后续相关模型设计具有启发意义。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [31] [Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring](https://arxiv.org/abs/2510.01683)
*Han-Jay Shu,Wei-Ning Chiu,Shun-Ting Chang,Meng-Ping Huang,Takeshi Tohyama,Ahram Han,Po-Chih Kuo*

Main category: cs.CV

TL;DR: 本文提出了一种基于增强敏感性的风险评分（ASRS）方法，检测胸部X光（CXR）深度学习模型中容易出错的病例，并改善AI在医疗场景中的公平性和安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在胸片诊断中表现良好，但其在患者亚群中的准确率并不均衡，会导致传统整体指标难以反映的“隐藏错误”。现有基于置信度或OOD的方法难以识别细微的分布内错误，而一致性类方法在医学影像领域又鲜有应用，因此迫切需要无标签、高效的错误检测手段。

Method: 作者提出ASRS框架：对胸片图像施加临床合理的旋转增强（±15°/±30°），通过RAD-DINO编码器测量嵌入层表征的变化，赋予每个样本敏感性分数，并按敏感性将样本分层分析模型表现。

Result: ASRS能够将对增强敏感的病例归入高风险组，这些样本即使原模型AUROC与置信度较高，召回率却有显著下降（比低敏感性样本低0.2~0.3），实现了无标签的高风险病例筛查。

Conclusion: ASRS方法可用于医学AI的选择性预测与临床复核，为提升AI公平性与安全性提供了新工具，特别是在无需人工标注的情况下识别高风险样本。

Abstract: Deep learning models achieve strong performance in chest radiograph (CXR)
interpretation, yet fairness and reliability concerns persist. Models often
show uneven accuracy across patient subgroups, leading to hidden failures not
reflected in aggregate metrics. Existing error detection approaches -- based on
confidence calibration or out-of-distribution (OOD) detection -- struggle with
subtle within-distribution errors, while image- and representation-level
consistency-based methods remain underexplored in medical imaging. We propose
an augmentation-sensitivity risk scoring (ASRS) framework to identify
error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm
15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO
encoder. Sensitivity scores stratify samples into stability quartiles, where
highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)
despite high AUROC and confidence. ASRS provides a label-free means for
selective prediction and clinician review, improving fairness and safety in
medical AI.

</details>


### [32] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: FreeViS是一种免训练的视频风格化方法，可生成高质量、风格丰富且时间连贯的视频，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频风格化面临两大难题：逐帧风格化会导致时序不一致和风格贫乏；而端到端训练专用模型又需要配对视频数据且计算消耗大。因此，需要一种经济且实用的新方法解决视频风格化的效率和一致性问题。

Method: 提出FreeViS，一个免训练的视频风格化框架。该方法整合多个风格化参考图到一个预训练的图像转视频（I2V）模型中，结合高频补偿和基于光流的运动线索，确保内容结构和运动的准确性，同时保留低显著性区域的风格细节，有效避免了传统方法中的传播错误和时间抖动。

Result: 大量实验显示，FreeViS实现了更高的风格化保真度和时序一致性，在客观指标和主观偏好层面都超越了多项最新基线方法。

Conclusion: FreeViS无需训练，可高效地实现高质量和时序连贯的视频风格化，为实际内容创作提供了实用且经济的解决方案，具有广泛的应用前景。

Abstract: Video stylization plays a key role in content creation, but it remains a
challenging problem. Na\"ively applying image stylization frame-by-frame hurts
temporal consistency and reduces style richness. Alternatively, training a
dedicated video stylization model typically requires paired video data and is
computationally expensive. In this paper, we propose FreeViS, a training-free
video stylization framework that generates stylized videos with rich style
details and strong temporal coherence. Our method integrates multiple stylized
references to a pretrained image-to-video (I2V) model, effectively mitigating
the propagation errors observed in prior works, without introducing flickers
and stutters. In addition, it leverages high-frequency compensation to
constrain the content layout and motion, together with flow-based motion cues
to preserve style textures in low-saliency regions. Through extensive
evaluations, FreeViS delivers higher stylization fidelity and superior temporal
consistency, outperforming recent baselines and achieving strong human
preference. Our training-free pipeline offers a practical and economic solution
for high-quality, temporally coherent video stylization. The code and videos
can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [33] [MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs](https://arxiv.org/abs/2510.01691)
*Jiyao Liu,Jinjie Wei,Wanying Qu,Chenglong Ma,Junzhi Ning,Yunheng Li,Ying Chen,Xinzhe Luo,Pengcheng Chen,Xin Gao,Ming Hu,Huihui Xu,Xin Wang,Shujian Gao,Dingkang Yang,Zhongying Deng,Jin Ye,Lihao Liu,Junjun He,Ningsheng Xu*

Main category: cs.CV

TL;DR: 本文提出了MedQ-Bench，一个针对医学图像质量评估的新基准，将多模态大模型的语言能力引入图像质量认知与推理任务，增强评价的解释性和贴近人类专家方式。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像质量评估主要依赖于单一分数型指标，难以反映专家评估时的人类推理过程与详细描述，因此需要更加细致且更具解释性的评价方式。

Method: 作者创建了MedQ-Bench基准，设定了两大任务：(1) MedQ-Perception，人类策划的问题考察模型对基础视觉属性的认知能力；(2) MedQ-Reasoning，无参考和对比推理任务，测试模型对图像质量进行人类式推理分析的能力。囊括五种成像方式、40余种质量属性，涵盖临床真实采集、物理模拟退化和AI生成图片。评判采用多维协议，同时与放射科专家的判断结果比对，验证对齐度。

Result: 对目前14种多模态大模型的评估结果显示，这些模型在感知与推理任务上有一定能力，但表现不稳定，准确性不足以满足实际临床需求。

Conclusion: 研究表明现有MLLMs尚不能可靠地用于医学图像质量评估，未来需有针对性的优化。作者希望MedQ-Bench推动该领域深入探索，激发MLLMs在医学图像质量评价中的潜力。

Abstract: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate
for clinical AI, yet existing approaches remain constrained by scalar,
score-based metrics and fail to reflect the descriptive, human-like reasoning
process central to expert evaluation. To address this gap, we introduce
MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning
paradigm for language-based evaluation of medical image quality with
Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary
tasks: (1) MedQ-Perception, which probes low-level perceptual capability via
human-curated questions on fundamental visual attributes; and (2)
MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,
aligning model evaluation with human-like reasoning on image quality. The
benchmark spans five imaging modalities and over forty quality attributes,
totaling 2,600 perceptual queries and 708 reasoning assessments, covering
diverse image sources including authentic clinical acquisitions, images with
simulated degradations via physics-based reconstructions, and AI-generated
images. To evaluate reasoning ability, we propose a multi-dimensional judging
protocol that assesses model outputs along four complementary axes. We further
conduct rigorous human-AI alignment validation by comparing LLM-based judgement
with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates
that models exhibit preliminary but unstable perceptual and reasoning skills,
with insufficient accuracy for reliable clinical use. These findings highlight
the need for targeted optimization of MLLMs in medical IQA. We hope that
MedQ-Bench will catalyze further exploration and unlock the untapped potential
of MLLMs for medical image quality evaluation.

</details>


### [34] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: 该论文提出了InstaFormer，一种能够仅通过输入RGB图像高效推理场景中所有实例的遮挡和深度顺序的网络。无需高昂的标签或多次前向计算，技术具有高效且全面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型对实例几何关系的理解存在对昂贵标签和高计算量的依赖，如准确的类别标签、二值分割掩码及多次前向传递等，严重影响实际应用效率。破解这些限制、提高推理速度和减少数据需求成为研究重点。

Method: InstaFormer神经网络仅需一张RGB输入图像，通过对象查询与潜在掩码描述子之间的交互，一次前向传递即可输出图中所有实例的遮挡顺序和深度排序。方法核心是语义一致但互补的查询与描述子联动。

Result: 在广泛的基准测试和消融实验中，InstaFormer显示了优于以往依赖多次前向或昂贵标签方法的出色效果。

Conclusion: InstaFormer能以高效和低成本的方式实现实例级顺序预测，为视觉领域场景理解带来新的突破。源码和模型已开源以促进社区发展。

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [35] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: 该论文提出了一种新的神经风格迁移方法PyramidStyler，通过金字塔位置编码和强化学习实现高效高质量的风格迁移。


<details>
  <summary>Details</summary>
Motivation: 目前的CNN和Transformer风格迁移模型对于复杂风格和高分辨率输入下的效率和表现力有限。

Method: 作者提出了基于Transformer的PyramidStyler框架，核心为分层多尺度的金字塔位置编码（PPE），同时结合强化学习动态优化风格迁移过程，加速收敛。模型在Microsoft COCO和WikiArt数据集上进行训练。

Result: PyramidStyler在内容损失和风格损失上分别减少了62.6%和57.4%，风格迁移速度达到实时水平，并且引入强化学习后保持高效的同时进一步降低了损失指标。

Conclusion: PyramidStyler能够高效实现高质量的艺术风格迁移，适用于媒体与设计等实际场景。

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [36] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: 本论文提出了一种名为LoBE-GS的新型3D Gaussian Splatting框架，通过负载均衡和高效性优化，显著提升了大规模场景三维重建的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 三维高斯点渲染（3DGS）虽然高效且具有较好重建 kvalitet，但在处理大规模、无边界场景时（如城市街区）遇到内存和计算负载不均衡等瓶颈。现有的分块策略带来了负载不平衡和粗到细处理阶段的效率损失，因此需要新的方法提升大规模场景的处理能力。

Method: 作者提出LoBE-GS框架，包括：1）基于深度的高效分块策略，大大减少预处理时间；2）用可见高斯点数作为负载代理，优化分块以实现负载均衡；3）引入可见性切割和选择性加密两项轻量级技术，进一步降低训练成本。

Result: 在大规模城市场景与户外数据集上，LoBE-GS比现有方法的端到端训练速度快2倍，并能扩展到传统3DGS无法处理的大场景，同时不影响重建质量。

Conclusion: LoBE-GS能够以更高效和可扩展的方式应用于大规模三维场景重建，在保持重建质量的同时，大幅缩短了训练时间，突破了原有方法在内存与负载平衡上的限制。

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.

</details>


### [37] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 本文提出了两项关键创新，用于提升长视频生成的表现：一是引入MemoryPack进行动态上下文建模，二是提出Direct Forcing减少推理过程中的误差积累。两者协同显著提升长视频生成的一致性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临两个主要挑战：一是需要捕捉长时序依赖，二是自回归生成方式容易导致误差累积。现有方法在应对长距离依赖和误差积累上效果有限。

Method: 提出了MemoryPack机制，通过可学习的上下文检索，联合利用文本与图像信息作为全局引导，综合建模短期和长期依赖，确保了分钟级的时序一致性，并具备良好的可扩展性和线性复杂度。同时，提出Direct Forcing策略，通过高效的单步近似训练，提升了训练与推理之间的一致性，从而减少推理时的误差传播。

Result: 实验表明，MemoryPack与Direct Forcing的结合，能大幅提升长视频生成过程中的上下文一致性和整体可靠性。模型在保持计算效率的同时，能够生成更长、更一致的视频内容。

Conclusion: 提出的方法为自回归视频生成模型在长视频场景下的实际应用提供了有力支持，显著改善了长视频生成的稳定性和实用性。

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [38] [Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving](https://arxiv.org/abs/2510.01829)
*Cornelius Schröder,Marius-Raphael Schlüter,Markus Lienkamp*

Main category: cs.CV

TL;DR: 该论文研究了3D目标检测器中分类任务的置信度校准，提出考虑主类别和次类别预测的全面校准，并提出正则化损失项提升模型校准能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶等自主系统中，需要精确的目标检测和不确定性评估保证系统的自知与安全运行。目前多数方法只关注主类别置信度的校准，忽视了完整的置信分布和多类别预测之间的关系，导致次类别置信度校准不足。

Method: 论文提出了两种辅助的正则化损失项，分别针对主类别预测和全类别概率分布的置信度校准，将其用作训练目标。并对CenterPoint、PillarNet和DSVT-Pillar三种主流3D检测模型进行了不同校准方法（包含后处理和训练时方法）的综合评估。

Result: 通过实验证明，对CenterPoint和PillarNet而言，结合提出的全面类别预测正则化损失项与等分回归（isotonic regression），可显著提升主类别及次类别预测的校准效果。而对于DSVT-Pillar，无法用同一种方法同时有效地校准主类别和次类别预测。

Conclusion: 主张应关注所有类别的完整置信度分布校准，所提方法能有效提升部分模型在主、次类别上的校准表现，但也揭示部分模型在多类别共同校准方面的局限性。

Abstract: In autonomous systems, precise object detection and uncertainty estimation
are critical for self-aware and safe operation. This work addresses confidence
calibration for the classification task of 3D object detectors. We argue that
it is necessary to regard the calibration of the full predictive confidence
distribution over all classes and deduce a metric which captures the
calibration of dominant and secondary class predictions. We propose two
auxiliary regularizing loss terms which introduce either calibration of the
dominant prediction or the full prediction vector as a training goal. We
evaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet
and DSVT-Pillar and find that combining our loss term, which regularizes for
calibration of the full class prediction, and isotonic regression lead to the
best calibration of CenterPoint and PillarNet with respect to both dominant and
secondary class predictions. We further find that DSVT-Pillar can not be
jointly calibrated for dominant and secondary predictions using the same
method.

</details>


### [39] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: 本文提出了一种结合扩散模型先验知识的人体搜索方法DiffPS，通过新颖的结构提升检测与 reID 表现，并在主流数据集上取得新 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有人体搜索大多用 ImageNet 预训练骨干网络，难以捕捉复杂空间关系和细粒度特征；同时检测与 reID 共用骨干产生目标冲突，导致表现受限。

Method: DiffPS 利用预训练扩散模型的先验能力，设计三大模块：（1）DGRPN用于提升人体定位，（2）MSFRN抑制形状偏差，（3）SFAN融合文本对齐的扩散特征以增强表征。

Result: DiffPS 在 CUHK-SYSU 和 PRW 两大主流人体搜索数据集上取得新的最优表现，显著优于已有方法。

Conclusion: 通过引入扩散模型先验并分别优化检测与 reID 特征，DiffPS 解决了现有方法的不足，在人体搜索任务上取得领先效果。

Abstract: Person search aims to jointly perform person detection and re-identification
by localizing and identifying a query person within a gallery of uncropped
scene images. Existing methods predominantly utilize ImageNet pre-trained
backbones, which may be suboptimal for capturing the complex spatial context
and fine-grained identity cues necessary for person search. Moreover, they rely
on a shared backbone feature for both person detection and re-identification,
leading to suboptimal features due to conflicting optimization objectives. In
this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a
novel framework that leverages a pre-trained diffusion model while eliminating
the optimization conflict between two sub-tasks. We analyze key properties of
diffusion priors and propose three specialized modules: (i) Diffusion-Guided
Region Proposal Network (DGRPN) for enhanced person localization, (ii)
Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and
(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage
text-aligned diffusion features. DiffPS sets a new state-of-the-art on
CUHK-SYSU and PRW.

</details>


### [40] [Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2510.01912)
*Yi Ai,Yuanhao Cai,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新方法（FMU），首次将流匹配（flow matching）引入高光谱重建问题，并在多个实验中验证了其显著优越性。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像能够获取丰富的信息，但由于硬件限制和重建难度高，获得高质量三维高光谱数据仍然昂贵且困难，现有压缩感知系统虽然提升了效率，但重建精度还受到影响，尤其难以保留精细光谱细节，因此亟需更高效的重建方法。

Method: 作者提出了流匹配引导的展开网络（FMU），首次将流匹配生成先验融入深度展开（unfolding）框架，并引入均值速度损失来增强流动全局一致性，结合了优化类方法的可解释性和生成模型的表达能力。

Result: 在模拟数据和实际数据集上的广泛实验显示，FMU在重建质量上显著优于当前主流方法。

Conclusion: FMU方法兼具理论创新和实际优势，为高光谱重建带来了更高的质量和更强的鲁棒性。

Abstract: Hyperspectral imaging (HSI) provides rich spatial-spectral information but
remains costly to acquire due to hardware limitations and the difficulty of
reconstructing three-dimensional data from compressed measurements. Although
compressive sensing systems such as CASSI improve efficiency, accurate
reconstruction is still challenged by severe degradation and loss of fine
spectral details. We propose the Flow-Matching-guided Unfolding network (FMU),
which, to our knowledge, is the first to integrate flow matching into HSI
reconstruction by embedding its generative prior within a deep unfolding
framework. To further strengthen the learned dynamics, we introduce a mean
velocity loss that enforces global consistency of the flow, leading to a more
robust and accurate reconstruction. This hybrid design leverages the
interpretability of optimization-based methods and the generative capacity of
flow matching. Extensive experiments on both simulated and real datasets show
that FMU significantly outperforms existing approaches in reconstruction
quality. Code and models will be available at https://github.com/YiAi03/FMU.

</details>


### [41] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度学习的DIP元件自动缺陷检测系统，通过相机采集数据并结合YOLO与ConSinGAN实现高效、高准确率的缺陷检测。


<details>
  <summary>Details</summary>
Motivation: 传统工业元件缺陷检测耗时耗力，质量检验人员负担大、难以高效管理产品质量，特别是缺陷样本不足时，检测任务较难进行。

Method: 采用数字相机采集DIP元件图像，利用深度学习的YOLO系列模型（v3/v4/v7/v9）进行检测。针对缺陷样本不足，使用ConSinGAN生成增强数据集。评估了各版本YOLO模型，比较其与传统阈值法的表现。系统还集成SCADA数据采集监控和相关传感器架构。

Result: YOLOv7结合ConSinGAN的数据增强，在准确率（95.5%）和检测速度（285ms）方面表现最佳，明显优于其他YOLO版本和阈值法。

Conclusion: 提出的自动缺陷检测系统在应对多种缺陷类型或缺陷样本不足时依然易于建立，实现高效且易推广的工业元件检测。

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [42] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: 本论文提出了一种基于大规模预训练视觉编码器和非线性投影的少样本异常检测方法（FoundAD），能够高效进行多类别检测并在参数量较少的情况下取得有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的工业安全检查依赖异常检测，但在少样本（few-shot）和类无关（category-agnostic）场景下常因样本有限导致异常与正常难以区分。大规模预训练视觉基础模型在捕捉正常分布方面展现了潜力，促使作者探索如何更高效地利用这些特征提升异常检测。

Method: 作者观察到图像中异常的多少可通过编码器输出嵌入（embeddings）的差异进行量化。基于此，提出学习一个非线性投影算子，将嵌入投影到自然图像流形上，突出分布外（out-of-distribution）的特征，从而完成异常检测。该方法简单高效，且具备多类别检测能力。

Result: 实验表明，所提方法支持多类别检测，在多个基础视觉编码器（包括最新的DINOv3）上均展现出与现有方法相当或更优的性能。同时所需参数远少于以往方法。

Conclusion: 本文方法能有效利用基础视觉特征，显著提升少样本异常检测的性能，参数量低，拓展了基础视觉特征在异常检测中的研究视角，有助于推动该领域进一步发展。

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [43] [ClustViT: Clustering-based Token Merging for Semantic Segmentation](https://arxiv.org/abs/2510.01948)
*Fabio Montello,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 本文提出了一种优化视觉Transformer（ViT）用于语义分割的新方法ClustViT，通过智能地合并和恢复tokens，在大幅降低计算量和推理时间的同时保持了分割精度。


<details>
  <summary>Details</summary>
Motivation: 传统ViT虽然精度高、泛化能力强，但由于注意力机制计算复杂度高，在真实机器人系统等实际场景下部署受限。现有通过动态合并token降低计算量的方法对分类任务效果良好，但在密集预测（如语义分割）任务中却不理想。

Method: 提出ClustViT，将ViT骨干网络加以改进。核心结构包括：通过可训练的Cluster模块，基于分割掩码伪聚类指导，在网络内部逐步合并相似token；接着用Regenerator模块在下游网络恢复细粒度的空间信息，保证分割输出细节。

Result: 在三个数据集上，所提方法在接近原有分割精度的同时，将FLOPs最多减少2.18倍，推理速度提升最多1.64倍。

Conclusion: ClustViT有效兼顾了计算效率和分割精度，为ViT在实际机器人等资源受限场景的语义分割任务推广提供了新思路。

Abstract: Vision Transformers can achieve high accuracy and strong generalization
across various contexts, but their practical applicability on real-world
robotic systems is limited due to their quadratic attention complexity. Recent
works have focused on dynamically merging tokens according to the image
complexity. Token merging works well for classification but is less suited to
dense prediction. We propose ClustViT, where we expand upon the Vision
Transformer (ViT) backbone and address semantic segmentation. Within our
architecture, a trainable Cluster module merges similar tokens along the
network guided by pseudo-clusters from segmentation masks. Subsequently, a
Regenerator module restores fine details for downstream heads. Our approach
achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different
datasets, with comparable segmentation accuracy. Our code and models will be
made publicly available.

</details>


### [44] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: 本论文提出了一种新的多模态大语言模型视觉任务处理范式，即Patch-as-Decodable Token（PaDT），用以直接生成文本和多样化视觉输出，有效提升定位和语义分割等密集预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉任务上通常用间接方式，如用文本坐标表示检测目标，难以胜任如分割等密集预测任务，且影响性能与泛化。

Method: PaDT 采用 Visual Reference Tokens (VRTs)，将视觉 patch 的嵌入直接作为可解码 token，并与 LLM 输出的文本 token 交错融合，再通过轻量解码器变换为检测、分割等任务输出。VRTs 在每次前向传播时独立处理，同时动态扩展嵌入表，提升对象定位与区分能力。训练时，随机选取 VRTs 进行有监督微调，并引入 per-token 的交叉熵损失。

Result: PaDT 框架在四个视觉感知与理解任务上进行实证评测，结果均优于同类体量更大的多模态大语言模型，实现了一致的 SOTA 性能。

Conclusion: PaDT 能够统一并显著提升视觉任务中多模态大模型的输出表现，支持文本与多样视觉输出类型，扩展了多模态模型在视觉领域的应用潜力。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [45] [TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading](https://arxiv.org/abs/2510.01990)
*Jianfei Xie,Ziyang Li*

Main category: cs.CV

TL;DR: 本文针对线上果蔬电商中“信任缺口”问题，提出了“信任金字塔”模型和三角信任指数（TTI），并设计了解释性AI框架TriAlignXA来提升消费者信任，通过多目标优化兼顾生物属性、时效性与经济性。实验验证该方法可在质量分级任务上显著提升精度，从理论到实践为建立可信的线上农产品生态体系提供支撑。


<details>
  <summary>Details</summary>
Motivation: 当前数字化交易无法为消费者带来直观的果蔬质量感知，导致线上果蔬电商存在信任缺口。传统的绝对分级标准在生物属性、时效性和经济性三者之间难以兼顾，行业亟需新的理论与解决方案来支撑消费者信任建立。

Method: 1）构建‘信任金字塔’模型揭示信任层级。2）提出“impossible triangle”理论，分析生物属性、时效性、经济性间不可兼得困境。3）量化此权衡关系，提出三角信任指数（TTI）。4）将算法角色由自动决策者转变为透明依据提供者，提出可解释AI框架TriAlignXA，该框架包括生物自适应引擎、时效优化引擎和经济优化引擎，并用预映射机制将过程数据编码进二维码实现信息透明。

Result: 实验表明，TriAlignXA在农产品分级任务上准确率远超基线方法。实证和理论分析共同证明框架能在‘不可能三角’下较好地实现平衡，提升线上交易的信任基础。

Conclusion: 本研究为线上农产品交易体系提供了从理论到实践的信任建立机制和系统工具，明确算法在提升消费者信任过程中的作用路径，对发展可信数字农产品生态有重要意义。

Abstract: The 'trust deficit' in online fruit and vegetable e-commerce stems from the
inability of digital transactions to provide direct sensory perception of
product quality. This paper constructs a 'Trust Pyramid' model through
'dual-source verification' of consumer trust. Experiments confirm that quality
is the cornerstone of trust. The study reveals an 'impossible triangle' in
agricultural product grading, comprising biological characteristics,
timeliness, and economic viability, highlighting the limitations of traditional
absolute grading standards. To quantitatively assess this trade-off, we propose
the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from
'decision-makers' to 'providers of transparent decision-making bases',
designing the explainable AI framework--TriAlignXA. This framework supports
trustworthy online transactions within agricultural constraints through
multi-objective optimization. Its core relies on three engines: the
Bio-Adaptive Engine for granular quality description; the Timeliness
Optimization Engine for processing efficiency; and the Economic Optimization
Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes
process data into QR codes, transparently conveying quality information.
Experiments on grading tasks demonstrate significantly higher accuracy than
baseline models. Empirical evidence and theoretical analysis verify the
framework's balancing capability in addressing the "impossible triangle". This
research provides comprehensive support--from theory to practice--for building
a trustworthy online produce ecosystem, establishing a critical pathway from
algorithmic decision-making to consumer trust.

</details>


### [46] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 本文提出了4DGS-Craft，一种一致性和交互性更强的4D Gaussian Splatting编辑框架，通过4D感知模型、多视图优化及大语言模型提升4D场景编辑的效果与控制力。


<details>
  <summary>Details</summary>
Motivation: 当前4D Gaussian Splatting编辑方法在视角一致性、时间一致性、非编辑区域保持一致性以及复杂文本指令处理上仍存在挑战，限制了其实际应用与用户交互体验。

Method: 1. 提出4D感知的InstructPix2Pix模型，引入4D VGGT几何特征，增强编辑时的空间和时间一致性。2. 设计多视图网格模块，通过多视图输入和联合优化场景，提升一致性。3. 引入高斯点选择机制，仅针对编辑区域的高斯进行优化，保护未编辑区域。4. 开发基于大语言模型（LLM）的用户意图理解模块，将复杂用户指令分解为原子操作，提升复杂交互编辑能力。

Result: 实验结果表明，与相关方法相比，4DGS-Craft在4D场景编辑的一致性、可控性和处理复杂指令的能力上均有显著提升。

Conclusion: 4DGS-Craft框架有效解决了4DGS编辑中的一致性和交互性难题，支持更复杂、细致的用户编辑需求，对4D内容生成与编辑具有重要推动作用。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.

</details>


### [47] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的像素级掩码机制（Pure-Pass, PP），可以识别图像中无需复杂计算的“纯像素”，在保证重建质量的同时有效减少计算量，集成后性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在图像超分辨率（SR）任务上取得显著进展，但计算复杂度过高，难以实际部署。此前的方法例如CAMixer，尝试根据内容难度进行计算路由，但存在自适应能力差、掩码粒度粗糙和空间灵活性不足等问题。

Method: 提出Pure-Pass（PP）掩码机制，通过预设颜色中心点将像素分类，识别出“纯像素”并免除其参与昂贵计算，从而实现细粒度、灵活且自适应的掩码。将PP集成到ATD-light模型中，形成PP-ATD-light。

Result: PP-ATD-light模型在SR重建质量和参数效率上均超越了同样节省计算量的CAMixer-ATD-light。

Conclusion: Pure-Pass机制能够实现像素级细粒度的掩码决策，有效减少计算开销，同时提升超分重建质量，为高效SR方法提供了新的思路。

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from
low-resolution counterparts, but the computational complexity of deep
learning-based methods often hinders practical deployment. CAMixer is the
pioneering work to integrate the advantages of existing lightweight SR methods
and proposes a content-aware mixer to route token mixers of varied complexities
according to the difficulty of content recovery. However, several limitations
remain, such as poor adaptability, coarse-grained masking and spatial
inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking
mechanism that identifies pure pixels and exempts them from expensive
computations. PP utilizes fixed color center points to classify pixels into
distinct categories, enabling fine-grained, spatially flexible masking while
maintaining adaptive flexibility. Integrated into the state-of-the-art
ATD-light model, PP-ATD-light achieves superior SR performance with minimal
overhead, outperforming CAMixer-ATD-light in reconstruction quality and
parameter efficiency when saving a similar amount of computation.

</details>


### [48] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: 本研究利用GPT-4o针对牙科全景片自动生成颚囊肿影像结论，并提出了自我校正循环结构化输出（SLSO）框架提升准确性，相比传统思路链法（CoT）在多个评估项上效果更好，尤其在牙位编号等方面有显著提升，但在大范围病变识别上仍有限，需要进一步完善。


<details>
  <summary>Details</summary>
Motivation: 自动生成牙科影像报告有助于提升诊断效率，但现有AI模型在准确性，一致性等方面仍面临挑战，尤其是在牙齿编号、病变描述中易出现错误或幻觉输出。研究旨在提升这一应用场景下多模态大模型的实用性。

Method: 构建了包含10步的SLSO流程，涵盖图像输入分析、数据结构化、牙齿编号提取及一致性校验，遇到不一致时自动迭代再生，并持续校验输出。与传统CoT法在7个细化评估维度上进行了对比。

Result: 在牙齿编号、牙齿移动、牙根吸收等指标上，SLSO相较于CoT分别提升了66.9%、33.3%、28.6%。多数样本通过最多五次生成，输出结构达成一致。SLSO能有效抑制幻觉现象，提升负面结论描述与编号准确，不过受限于样本量未达统计显著。大量跨多牙齿病变的识别仍有不足。

Conclusion: SLSO框架提升了牙科AI自动解读的输出质量，特别是在牙齿编号与一致性方面效果明显。现有框架在处理大范围多牙病变的能力有限，未来需继续优化流程，朝临床实践应用推进。

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [49] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja Pérez,Jaime Godoy,Abdulla Al-Kaff,Fernando García*

Main category: cs.CV

TL;DR: 本文提出了一种名为LiLa-Net的3D自动编码器网络，只使用激光雷达点云数据高效编码真实交通环境特征，能够高质量重建点云，并且具有优秀的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前主流的点云处理网络多采用复杂的架构和大量资源，不适用于资源受限的半自动驾驶系统，因此需要一种轻量、有效，且能够泛化的3D点云自动编码器。

Method: LiLa-Net利用跳跃连接（skip connections）提升性能，同时采用简化的编码器层结构，降低网络复杂度并优化信息在跳跃连接和潜在空间间的平衡，从而实现点云的高效编码与重建。

Result: 该模型在不牺牲性能的情况下，显著减少了网络资源消耗，同时实现了点云数据的高质量重建，并在非交通场景类别上展现出较强泛化能力。

Conclusion: LiLa-Net在半自动驾驶场景下为点云编码和重建提供了一种高效、轻量且鲁棒性强的网络结构，有望在资源受限环境中实际应用。

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [50] [kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring](https://arxiv.org/abs/2510.02030)
*Jenna Kline,Maksim Kholiavchenko,Samuel Stevens,Nina van Tiel,Alison Zhong,Namrata Banerji,Alec Sheets,Sowbaranika Balasubramaniam,Isla Duporge,Matthew Thompson,Elizabeth Campolongo,Jackson Miliko,Neil Rosser,Tanya Berger-Wolf,Charles V. Stewart,Daniel I. Rubenstein*

Main category: cs.CV

TL;DR: 本文提出了一种基于无人机视频和机器学习的野生动物行为自动化监测工具kabr-tools，可在大尺度上高效采集和分析多物种行为与社会互动数据，显著提升了数据获取的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的野外动物行为观察方法在空间、时间和人力方面都存在局限，难以应对生态系统尺度的行为多样性研究需求。为了解决这些瓶颈，作者研发了一种自动化、可扩展的监测方案。

Method: kabr-tools结合无人机拍摄的视频数据和机器学习技术，自动化完成目标检测、追踪和行为分类，生成动物的时序行为分布、行为转移、社会互动和空间关系等关键指标。通过与地面人工观测比较，评估了该工具的效果，并在三个案例中验证其实际应用价值。

Result: 无人机+机器学习方法提升了行为数据的粒度和连续性，能减少15%的视线损失，准确记录更多行为转变。工具在案例研究中对969条行为序列进行了分析，信息捕获和标注能力均超越传统方法。还揭示了不同斑马种群的社会和行为规律，如警戒行为与群体大小的关系。

Conclusion: kabr-tools极大提升了大尺度动物行为生态学研究的自动化和高通量能力，为野生动物保护、生物多样性研究及生态监测提供了新手段和科学依据。

Abstract: A comprehensive understanding of animal behavior ecology depends on scalable
approaches to quantify and interpret complex, multidimensional behavioral
patterns. Traditional field observations are often limited in scope,
time-consuming, and labor-intensive, hindering the assessment of behavioral
responses across landscapes. To address this, we present kabr-tools (Kenyan
Animal Behavior Recognition Tools), an open-source package for automated
multi-species behavioral monitoring. This framework integrates drone-based
video with machine learning systems to extract behavioral, social, and spatial
metrics from wildlife footage. Our pipeline leverages object detection,
tracking, and behavioral classification systems to generate key metrics,
including time budgets, behavioral transitions, social interactions, habitat
associations, and group composition dynamics. Compared to ground-based methods,
drone-based observations significantly improved behavioral granularity,
reducing visibility loss by 15% and capturing more transitions with higher
accuracy and continuity. We validate kabr-tools through three case studies,
analyzing 969 behavioral sequences, surpassing the capacity of traditional
methods for data capture and annotation. We found that, like Plains zebras,
vigilance in Grevy's zebras decreases with herd size, but, unlike Plains
zebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit
strong behavioral inertia, with rare transitions to alert behaviors and
observed spatial segregation between Grevy's zebras, Plains zebras, and
giraffes in mixed-species herds. By enabling automated behavioral monitoring at
scale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing
conservation, biodiversity research, and ecological monitoring.

</details>


### [51] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为GaussianMorphing的新框架，可实现从多视角图像中进行有语义感知的3D形状和纹理变形，无需预定义形状映射或依赖点云。该方法通过结合3D高斯溅射和网格引导，实现高保真的几何与纹理变形，并在无需标注数据的情况下自动保持结构一致性与语义对应。


<details>
  <summary>Details</summary>
Motivation: 现有3D形状与纹理变形方法受限于点云表达或需预先定义同胚映射，且多针对未加纹理的数据。这些限制阻碍了在实际多视角图像场景下实现高质量、语义一致的3D形变。作者希望解决这些问题，实现无需人工标注的自动化高质量3D变形。

Method: 方法创新性地融合了网格引导的3D高斯溅射（3DGS），通过将3D高斯锚定在重建网格片块上，实现几何一致性和高保真纹理变形。框架采用统一变形策略并结合拓扑感知约束，建立无监督的语义对应，并通过物理可行的点轨迹保持结构完整，实现了无需标注的自动过程。

Result: 在作者提出的TexMorph基准上，GaussianMorphing比现有2D/3D方法有显著提升，使颜色一致性误差降低了22.2%，EI降低了26.2%。

Conclusion: 该工作在实现无监督、语义一致且结构完整的高质量3D形状与纹理变形方面取得突破，为相关3D内容生成和编辑拓展了新的解决方案。

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [52] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 本文提出了一种新的人体姿态估计算法InPose，在传感器数量有限且不同用户身体尺寸不同的实际场景下，仍能实现零样本泛化能力强的姿态估计。


<details>
  <summary>Details</summary>
Motivation: 现有用条件扩散模型做的人体姿态估计方法在实际应用中泛化能力差，尤其是由于不同用户的身体尺寸影响传感器位置数据，导致对新用户效果不佳。作者希望设计一种不受用户身体尺寸影响的方法，实现强泛化能力。

Method: 作者将姿态估计问题建模为逆问题。创新地只用旋转测量作为扩散模型的输入（条件），利用预训练的扩散模型进行生成式姿态估计，同时引入从位置观测值计算的似然项对生成进行引导，从而融合旋转与位置的信息，最终生成一组最符合稀疏观测的姿态序列。

Result: 实验结果表明，提出的InPose方法相比此前同时依赖位置和旋转信息的条件扩散方法，在跨用户泛化（零样本）任务下表现更优，更能准确恢复新用户的姿态。

Conclusion: InPose实现了基于稀疏旋转与位置观测的人体姿态零样本估计，提高了不同用户间的泛化能力，有望应用于实际的可穿戴设备姿态估计任务。

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [53] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Transformer和扩散模型结合的新方法VGDM，用于脑肿瘤MRI图像的检测与分割，在重要指标上超越了传统U-Net。


<details>
  <summary>Details</summary>
Motivation: 传统U-Net等卷积结构虽在医学图像分割中广泛应用，但在复杂肿瘤结构的长程依赖捕捉方面能力有限，影响了分割精度。因此，作者希望探索能更好捕捉全局信息并提升分割边界的模型。

Method: 设计了Vision-Guided Diffusion Model (VGDM)，将视觉Transformer嵌入到扩散模型的核心流程中，结合全局上下文推理和迭代去噪机制，以提升三维体积MRI肿瘤分割的准确性和边界精细度。

Result: 在MRI脑肿瘤数据集上进行实验，VGDM在Dice系数和Hausdorff距离等评价指标上均取得了比传统U-Net更好的表现。

Conclusion: 结合Transformer与扩散模型的方法能更有效提高脑肿瘤MRI分割质量，为神经肿瘤学领域提供了一种具有鲁棒性和可扩展性的新途径。

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance
imaging (MRI) are essential for diagnosis, treatment planning, and clinical
monitoring. While convolutional architectures such as U-Net have long been the
backbone of medical image segmentation, their limited capacity to capture
long-range dependencies constrains performance on complex tumor structures.
Recent advances in diffusion models have demonstrated strong potential for
generating high-fidelity medical images and refining segmentation boundaries.
  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor
Detection and Segmentation framework, a transformer-driven diffusion framework
for brain tumor detection and segmentation. By embedding a vision transformer
at the core of the diffusion process, the model leverages global contextual
reasoning together with iterative denoising to enhance both volumetric accuracy
and boundary precision. The transformer backbone enables more effective
modeling of spatial relationships across entire MRI volumes, while diffusion
refinement mitigates voxel-level errors and recovers fine-grained tumor
details.
  This hybrid design provides a pathway toward improved robustness and
scalability in neuro-oncology, moving beyond conventional U-Net baselines.
Experimental validation on MRI brain tumor datasets demonstrates consistent
gains in Dice similarity and Hausdorff distance, underscoring the potential of
transformer-guided diffusion models to advance the state of the art in tumor
segmentation.

</details>


### [54] [Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques](https://arxiv.org/abs/2510.02097)
*Walid Rabehi,Marion Le Texier,Rémi Lemoy*

Main category: cs.CV

TL;DR: 本研究利用深度学习方法，从1925-1950年间的法国历史地图中自动提取城市足迹，首次构建该时期的国家级开放城市扩张数据集。


<details>
  <summary>Details</summary>
Motivation: 由于1970年前法国缺乏全国范围的数字化城市发展数据，导致对早期城市蔓延的量化分析受限，亟需技术手段填补这一数据空白。

Method: 设计了一种双阶段U-Net深度学习流程，第一阶段定位疑难区域进行数据增强，第二阶段用优化后的数据和第一次模型输出去除噪声，从而提升城市提取的准确性。全流程在高性能计算集群上对941个高分辨率地图进行处理。

Result: 在整个法国范围自动生成城市足迹马赛克图，整体准确率达73%，有效分辨了不同类型的城市形态并抑制了标签等伪影。

Conclusion: 本研究发布了开放的深度学习代码、训练数据和全国城市栅格图，为深入研究法国长期城市化动态提供了重要数据支撑。

Abstract: Quantitative analysis of historical urban sprawl in France before the 1970s
is hindered by the lack of nationwide digital urban footprint data. This study
bridges this gap by developing a scalable deep learning pipeline to extract
urban areas from the Scan Histo historical map series (1925-1950), which
produces the first open-access, national-scale urban footprint dataset for this
pivotal period. Our key innovation is a dual-pass U-Net approach designed to
handle the high radiometric and stylistic complexity of historical maps. The
first pass, trained on an initial dataset, generates a preliminary map that
identifies areas of confusion, such as text and roads, to guide targeted data
augmentation. The second pass uses a refined dataset and the binarized output
of the first model to minimize radiometric noise, which significantly reduces
false positives. Deployed on a high-performance computing cluster, our method
processes 941 high-resolution tiles covering the entirety of metropolitan
France. The final mosaic achieves an overall accuracy of 73%, effectively
capturing diverse urban patterns while overcoming common artifacts like labels
and contour lines. We openly release the code, training datasets, and the
resulting nationwide urban raster to support future research in long-term
urbanization dynamics.

</details>


### [55] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: 本文对外科手术视频中基于点的目标跟踪方法的失败模式进行了系统性分析，比较了与分割掩膜初始化的区别，提出了提升跟踪性能的建议。


<details>
  <summary>Details</summary>
Motivation: 在手术视频分析领域，VOS模型如SAM2可以实现低成本、零样本的目标跟踪，但目前基于点的跟踪在复杂手术环境下的可靠性尚不明确，尤其失败案例和改进空间不明。

Method: 系统分析腹腔镜胆囊切除手术视频中基于点的跟踪失败模式。重点针对胆囊、持物钳和L型电凝钩三类目标，将基于点的跟踪与分割掩膜初始化方法进行性能比较。对跟踪结果进行定性分析，识别影响跟踪效果的关键因素。

Result: 研究表明，基于点的跟踪方法在手术器械上表现良好，但在解剖结构（如胆囊）上表现较差，主要受限于组织相似性和边界模糊。

Conclusion: 基于点的跟踪在某些手术目标上有竞争力，但对解剖目标效果欠佳。针对失败原因，作者提出了选择和放置跟踪点的实用建议，有助于提升手术视频分析性能。

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [56] [FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation](https://arxiv.org/abs/2510.02114)
*Ding-Ruei Shen*

Main category: cs.CV

TL;DR: 本文提出了一种在仅有无标签客户端数据、无法重新访问有标签源数据情境下进行联邦学习语义分割的新任务（FFREEDG），并提出了基于视觉基础模型的创新框架FRIEREN，有效提升了跨域适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法通常假定客户端有标注数据或未充分利用现代视觉基础模型。然而实际情境中客户端数据多为无标签，且存在显著领域差异，如何提升仅用无标签数据的联邦语义分割性能具有重要现实意义。

Method: 提出FFREEDG任务，并设计了FRIEREN框架：1）服务端用有标签数据预训练模型，之后仅在客户端用无标签数据继续训练，源不再可访问；2）引入视觉-语言解码器，利用CLIP生成的文本嵌入提升语义判别力；3）采用弱-强一致性学习策略，对伪标签进行鲁棒训练。

Result: 在合成到真实和晴朗到恶劣天气等基准上，FRIEREN取得了与现有领域泛化与适应方法媲美的结果，为该任务设立了有竞争力的新基线。

Conclusion: 提出的新任务和方法有效缓解了联邦学习跨域语义分割中的无标签和数据隐私问题，展现了视觉基础模型跨模态能力的潜力，对未来研究具有启发性指导意义。

Abstract: Federeated Learning (FL) offers a privacy-preserving solution for Semantic
Segmentation (SS) tasks to adapt to new domains, but faces significant
challenges from these domain shifts, particularly when client data is
unlabeled. However, most existing FL methods unrealistically assume access to
labeled data on remote clients or fail to leverage the power of modern Vision
Foundation Models (VFMs). Here, we propose a novel and challenging task,
FFREEDG, in which a model is pretrained on a server's labeled source dataset
and subsequently trained across clients using only their unlabeled data,
without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a
framework that leverages the knowledge of a VFM by integrating vision and
language modalities. Our approach employs a Vision-Language decoder guided by
CLIP-based text embeddings to improve semantic disambiguation and uses a
weak-to-strong consistency learning strategy for robust local training on
pseudo-labels. Our experiments on synthetic-to-real and
clear-to-adverse-weather benchmarks demonstrate that our framework effectively
tackles this new task, achieving competitive performance against established
domain generalization and adaptation methods and setting a strong baseline for
future research.

</details>


### [57] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: ASK-Hint是一种利用结构化提示，通过引入以动作为中心的知识，显著提升了静态冻结视觉语言模型在视频异常检测任务中的表现和可解释性，取得了新的状态-of-the-art结果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLMs）的提示方法在用于视频异常检测时过于抽象，不能有效捕捉细粒度的人与物体的互动或动作语义，影响对复杂异常场景的识别和解释。需要一种更细致和结构化的提示机制，增强异常检测的准确性和可解释性。

Method: 提出ASK-Hint结构化提示框架，以行动为中心，将提示组织为具有语义一致性的组别（如暴力、财产犯罪、公共安全），并制定细粒度的引导性问题。这些结构化的问题能促进模型从判别性视觉线索中做出更准确、可解释的推断。方法无须对VLM进行微调，直接用于冻结模型。

Result: 在UCF-Crime和XD-Violence两个主流数据集上，ASK-Hint实现了优于现有基线（包括fine-tuning和training-free方法）的AUC表现，达到最新水平。同时，结果证明了ASK-Hint对多数据集和不同VLM骨干结构的良好泛化能力并能输出可解释的推断路径。

Conclusion: ASK-Hint通过精细化和结构化的提示，提升了冻结视觉-语言模型在视频异常检测中的检测精度和可解释性，为无训练、易推广的异常检测提供了新方向，突出了提示粒度与有效性的核心作用。

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [58] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GeoPurify是一种创新方法，通过学习几何先验来净化2D视觉语言模型在3D点云上的预测，显著提升了3D语义分割效果，并大幅减少对标注数据的需求。


<details>
  <summary>Details</summary>
Motivation: 目前将2D视觉语言模型（VLMs）特征迁移到3D语义分割存在困境：直接投影会导致预测噪声和碎片化，而增强几何信息需要大量数据和昂贵训练。作者认为主流的分割与匹配策略未能有效结合2D语义和3D几何结构，导致2D迁移的几何信息并未被充分挖掘。

Method: 提出GeoPurify方法：用一个学生亲和网络，根据3D自监督教师模型中提取的几何先验来净化2D VLM生成的3D点特征。推理时，通过Geometry-Guided Pooling模块进一步消除噪声，确保语义与结构一致。

Result: GeoPurify在主流3D数据集上只需1.5%的训练数据就能达到或超过最先进方法，展现出极高数据效率和优异性能。

Conclusion: GeoPurify解决了2D语义与3D结构融合过程中的噪声和效率难题，实现了更优的3D语义分割方法，对领域有重要推动作用。代码和模型已开源，便于社区使用与验证。

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [59] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: 本文提出了一种基于猪耳静脉图像的非侵入式生物识别方法，通过计算机视觉和机器学习手段，实现混合品种猪只的高精度自动识别，具有实时性和低成本优势。


<details>
  <summary>Details</summary>
Motivation: 传统的猪只识别方法如耳标、芯片等存在成本高、识别不准、不适用于小规模农户等问题，影响了现代化养殖管理的普及。亟需开发一种低成本、准确且适用性广的新型动物识别方法。

Method: 作者采集了20头混合品种猪的800张耳部图片，利用普通手机和背光技术增强静脉信息。提出了多阶段计算机视觉流程，进行特征提取和生物特征签名生成，最后通过机器学习（特别是SVM）进行个体识别。

Result: SVM模型在跨混合品种猪群中达到了98.12%的识别精度，且从图像处理到最终分类仅需8.3秒，验证了系统的实时性和高效性。

Conclusion: 该研究表明猪耳静脉生物识别方法可替代脆弱的物理标识，为农户提供经济且无压力的动物识别解决方案，有望推动精细化养殖管理在资源有限地区的应用和普及。

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [60] [MMDEW: Multipurpose Multiclass Density Estimation in the Wild](https://arxiv.org/abs/2510.02213)
*Villanelle O'Reilly,Jonathan Cox,Georgios Leontidis,Marc Hanheide,Petra Bosilj,James Brown*

Main category: cs.CV

TL;DR: 本文提出了一种基于Vision-Transformer的多类别密集目标计数方法，显著提升了在拥挤、遮挡场景下的计数精度。


<details>
  <summary>Details</summary>
Motivation: 在密集、遮挡场景下，传统基于检测的计数方法难以分辨和计数目标。为此，基于密度图的方法成为主流，但在多类别场景下还存在准确性、类别间干扰等难题。

Method: 本方法采用Twins金字塔Vision-Transformer为骨干网络，并设计了多类别计数头，采用先进的多尺度解码方法。此外，引入了以分割为基础的Category Focus Module，减弱类别间训练时的干扰，提升精度。同时使用区域损失机制增强模型多样性。

Result: 在VisDrone和iSAID两个多类别基准数据集上测试，新方法在多项指标上大幅优于现有主流方法（MAE误差降低33%、43%、64%），并通过与YOLOv11的比较表明该方法在密集目标场景中的必要性。

Conclusion: 本文方法不仅提升了多类别密集计数的精度，并具备良好的泛化能力，可推广到如生物多样性监测等新领域，对生态保护与大规模生态学分析具有重要意义。

Abstract: Density map estimation can be used to estimate object counts in dense and
occluded scenes where discrete counting-by-detection methods fail. We propose a
multicategory counting framework that leverages a Twins pyramid
vision-transformer backbone and a specialised multi-class counting head built
on a state-of-the-art multiscale decoding approach. A two-task design adds a
segmentation-based Category Focus Module, suppressing inter-category cross-talk
at training time. Training and evaluation on the VisDrone and iSAID benchmarks
demonstrates superior performance versus prior multicategory crowd-counting
approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11
underscores the necessity of crowd counting methods in dense scenes. The
method's regional loss opens up multi-class crowd counting to new domains,
demonstrated through the application to a biodiversity monitoring dataset,
highlighting its capacity to inform conservation efforts and enable scalable
ecological insights.

</details>


### [61] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: 本文提出了一种新方法TempoControl，实现了对文生视频模型中生成物体和事件时间的精确控制，无需额外训练或标注，显著提升了生成视频的可控性。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的生成式视频模型能根据自然语言生成高质量视频，但很难精细控制具体物体或事件在视频中的出现时机，严重影响了实用性。如何在保持视频质量和多样性的同时，更好地控制生成内容的时间分布，是当前领域的重要挑战。

Method: TempoControl方法在无需重训练或增加监督的前提下，基于现有的text-to-video扩散模型框架，引入推理阶段的优化，通过利用cross-attention map，并结合相关性（correlation）、能量（energy）、熵（entropy）三种机制，引导生成视频中各元素的时间对齐和可视化，从而实现对视觉概念出现时机的精确控制。

Result: TempoControl实现了对视频中单物体、多物体的排序，以及对动作和音频对齐的视频生成，均能获得高质量和多样性的视频，验证了方法的有效性与通用性。

Conclusion: 该方法极大改善了文生视频生成时的时间可控性，为多种应用场景（如时序重排、事件精准控制等）提供了更强的技术支撑，推动了生成视频模型的实用化和灵活性。

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [62] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种新型多阶段强化学习框架RewardMap，有效提升了多模态大模型在精细视觉推理任务（如地铁图推理）中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在精细和结构丰富的视觉推理（如地铁图空间推理）任务上表现尚未达标，强化学习应用又受到稀疏奖励和不稳定性限制。对此，亟需新的方法攻克奖励稀疏及优化难题，推动模型在细粒度视觉理解和推理方面取得突破。

Method: 1. 构建扩展任务集ReasonMap-Plus，通过VQA任务设计密集奖励，便于冷启动训练。2. 提出RewardMap多阶段强化学习框架：（1）难度感知奖励设计，用详细奖励缓解稀疏性；（2）分阶段RL训练，从简单感知到复杂推理渐进训练。

Result: 在ReasonMap与ReasonMap-Plus数据集上，RewardMap各组件均带来性能提升，组合效果最佳。在6个涵盖空间推理、精细视觉推理及更广泛任务的基准测试上，RewardMap训练模型平均提升3.47%。

Conclusion: RewardMap框架可以有效增强多模态大模型的精细视觉理解与推理能力，为强化学习在此类任务上的应用提供了新范式。

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [63] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: DragFlow 利用 FLUX 的生成模型先验，引入基于区域的变形和多模态技术，大幅提升拖动式图像编辑的质量，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统拖动式图像编辑常因基础模型如 Stable Diffusion 的先验不足导致目标区域失真。尽管新一代生成模型如 DiT 和 FLUX 的先验更强，但拖动式编辑未能有效受益于这些进步。

Method: 提出 DragFlow 框架，利用 FLUX 丰富的生成先验，将拖动操作从基于点扩展为基于区域的仿射变换；结合预训练的个性化适配器增强主体一致性，并通过梯度掩码保护背景区域；此外引入多模态大模型解决任务歧义问题。建立了新的基准 ReD Bench 进行区域级评测。

Result: 实验表明，DragFlow 在 DragBench-DR 和新提出的 ReD Bench 上均超越现有点级和区域级编辑方法，达到新的 SOTA 水平。

Conclusion: DragFlow 成功地将 FLUX 强大先验用于拖动式图像编辑，极大提升了编辑质量和一致性。代码与数据集将在发表后开源，推动领域发展。

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


### [64] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: 本论文提出了F2C方法，通过选择短时连续的关键片段（key clips）而非单帧，提升了视频大模型对视频的理解能力，有效保留时序信息，并在多项长视频基准测试中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型（VLMs）受限于原始视频帧数量过大的“针在大海捞针”问题，导致模型上下文窗口被迅速耗尽。以往通过稀疏采样单帧来减少token数量，虽缓解算力瓶颈，却丢失了关键的运动与事件连续性信息，进而影响模型推理能力。

Method: 论文提出从只选单帧扩展到选择关键片段（key clips）的策略，每个片段包含时序连续的多帧信息，以增强时序动态建模。为避免增加计算负担，设计了自适应分辨率（adaptive resolution）方法，在片段长度与空间分辨率之间动态平衡，保证每个视频消耗的token数固定，无需修改训练流程即可应用。

Result: 在Video-MME、LongVideoBench和MLVU三个长视频基准上，F2C方法较传统的均匀抽帧分别提升了8.1%、5.6%和10.3%。

Conclusion: 保留视频时序连续性的片段抽取策略对提升视频大型语言模型的理解效果至关重要。所提方法简单高效，无需额外训练，可扩展至实际大规模视频理解应用。

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


### [65] [Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities](https://arxiv.org/abs/2510.02264)
*Mario Medrano-Paredes,Carmen Fernández-González,Francisco-Javier Díaz-Pernas,Hichem Saoudi,Javier González-Alonso,Mario Martínez-Zarzuela*

Main category: cs.CV

TL;DR: 本文比较了基于单目视频的三维人体姿态估计算法与惯性测量单元（IMUs）在临床相关日常活动动作捕捉中的表现，明确两者在运动学评估中的优劣，尤其是MotionAGFormer模型展现了最好的准确度和相关性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和可穿戴技术的发展，实现实验室外对人体运动的精确评估需求日益增长，尤其在远程医疗、运动科学及康复领域。本研究的动机在于系统比较主流视频三维人体姿态估计模型与IMU运动学评估的性能，评判其临床应用潜力和实用性。

Method: 研究使用VIDIMU数据集，包含13项健康被试的日常活动，均采用常规摄像头和5个IMUs同步采集。对比了四种先进的深度学习三维姿态算法（MotionAGFormer、MotionBERT、MMPose 2D-to-3D、NVIDIA BodyTrack）与IMU数据，通过OpenSim逆运动学与标准17关键点，测算其关节角度，并以RMSE、MAE、皮尔森相关系数、决定系数等指标系统评估各方法表现。

Result: MotionAGFormer在所有衡量指标上表现最佳（RMSE=9.27°±4.80°，MAE=7.86°±4.18°，相关系数0.86±0.15，R2=0.67±0.28）。总体而言，视频和IMU均可用于实验室外人体运动学分析，但各自存在精度、成本与可及性权衡。

Conclusion: 本研究明确了现有视频姿态估计模型在健康成人临床运动分析中的能力及局限，相较IMU既有优势也有短板，为远程健康与患者监测领域的健壮、经济、易用方案开发提供了有价值的参考与指引。

Abstract: Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.

</details>


### [66] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新方法（NeuroSwift）结合了多种视觉特征转换器，实现了fMRI脑活动数据到视觉信息的高效重建，尤其提升了跨个体（跨受试者）重建精度和计算效率。通过部分参数微调，大幅降低了新受试者的适配时间，并取得了当前最优的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然目前基于生成模型的fMRI图像重建取得了一些进展，但跨受试者的重建效果差且需要大量计算，这主要由于各人神经表征差异及大脑处理复杂视觉语义的抽象性，限制了实际应用。

Method: 作者提出NeuroSwift方法，组合了AutoKL（负责低级视觉特征）和CLIP（负责语义特征）的转换器，并以扩散模型作为桥梁。具体做法为：用Stable Diffusion生成的图像及COCO数据集描述训练CLIP Adapter，以模拟高层视觉皮层的编码方式。在迁移到新受试者时，仅微调全连接层参数（占全部参数17%），其余部分保持不变。

Result: 该方法在只需1小时训练/受试者（使用三块RTX4090轻量GPU）的情况下，跨受试者重建精度超过已有方法，表现为SOTA水平。

Conclusion: NeuroSwift显著提升了不同个体脑信号到视觉重建的效率与精度，为大规模、低成本脑-机接口及神经机制研究提供了新工具。

Abstract: Reconstructing visual information from brain activity via computer vision
technology provides an intuitive understanding of visual neural mechanisms.
Despite progress in decoding fMRI data with generative models, achieving
accurate cross-subject reconstruction of visual stimuli remains challenging and
computationally demanding. This difficulty arises from inter-subject
variability in neural representations and the brain's abstract encoding of core
semantic features in complex visual inputs. To address these challenges, we
propose NeuroSwift, which integrates complementary adapters via diffusion:
AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter
is trained on Stable Diffusion generated images paired with COCO captions to
emulate higher visual cortex encoding. For cross-subject generalization, we
pretrain on one subject and then fine-tune only 17 percent of parameters (fully
connected layers) for new subjects, while freezing other components. This
enables state-of-the-art performance with only one hour of training per subject
on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [67] [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification](https://arxiv.org/abs/2510.02270)
*Sathira Silva,Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 本论文提出microCLIP，一种针对CLIP视觉-语言模型在细粒度图像分类任务中的无监督自适应方法，通过融合细粒度显著特征和全局信息，以及动态知识聚合机制，实现较低开销下细粒度分类性能的大幅提升。


<details>
  <summary>Details</summary>
Motivation: CLIP模型虽然在零样本迁移上表现出色，但因主要依赖于全局粗粒度特征，导致在需要捕捉微小局部线索的细粒度图像分类任务上表现不佳。现有方法利用大语言模型描述对齐CLIP的[CLS] token，忽略了空间精细度。本研究动机是让CLIP能够有效利用细粒度的显著信息，从而提升其在细粒度任务中的表现。

Method: 提出microCLIP框架，核心为Saliency-Oriented Attention Pooling (SOAP) 结合轻量化的TokenFusion模块，从patch特征中抽取并融合细粒度显著token ([FG] token)和全局的[CLS] token，实现粗细粒度特征对齐。此外，设计了双头分类器：冻结的LLM分类器负责稳定提供文本先验以辅助伪标签生成，学习型分类器则由LLM描述初始化并通过TokenFusion进行微调。同时，动态知识聚合模块将固定先验和不断演化的分类器输出联合，用于伪标签的迭代优化。

Result: 在13个细粒度图像分类基准上实现了平均2.90%的准确率提升，并且只需轻量级的自适应过程。

Conclusion: microCLIP能够有效挖掘并利用CLIP中的细粒度潜在信号，无需大量标注和大规模改动，即可显著提升细粒度图像分类性能，在多种基准上取得一致性优越结果，具有很强的实际应用潜力。

Abstract: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach
overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
$\texttt{[FG]}$ token from patch embeddings and fuses it with the global
$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.

</details>


### [68] [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL](https://arxiv.org/abs/2510.02282)
*Kyoungjun Park,Yifan Yang,Juheon Yi,Shicheng Zheng,Yifei Shen,Dongqi Han,Caihua Shan,Muhammad Muaz,Lili Qiu*

Main category: cs.CV

TL;DR: 本文提出VidGuard-R1系统，首次利用多模态大语言模型与新型优化策略，实现了高精度、可解释的AI生成视频检测。


<details>
  <summary>Details</summary>
Motivation: AI生成视频技术快速发展，带来了伪造、虚假信息等社会风险。目前检测方法精度虽高，但缺乏透明、可解释的判断依据，难以满足监管及用户需求。

Method: 作者设计并构建了一个含14万条真实与AI生成视频的大型数据集，并针对检测难度进行了优化。方法上，采用Qwen-VL大模型，结合群组相对策略优化（GRPO），并引入两个专门的奖励模型分别关注时间性伪影和生成复杂性，实现全面特征捕捉与可解释性提升。

Result: 在多个公开基准上的零样本检测中，VidGuard-R1均取得最优表现。进一步训练后，准确率超过95%。案例分析表明模型不仅能做出合理判断，还能输出清晰的解释理由。

Conclusion: VidGuard-R1在AI视频检测领域设立了可解释性与准确率的新标杆，对抗虚假内容有重要应用价值。系统和数据集已开放，便于业界进一步研究和应用。

Abstract: With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.

</details>


### [69] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 本文提出了一种提升长视频生成质量的方法，不需长视频教师或额外长视频数据，有效缓解了视频变长后质量恶化问题，实现了大幅度拉长视频生成长度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然可高质量生成图像和视频，但其依赖transformer架构导致计算成本高昂，尤其是在生成更长视频时。现有方法用短视频教师蒸馏训练长视频自回归模型，但教师无法生成长视频，学生模型在外推到更长时间段时质量会严重下降。作者希望在不依赖长视频教师和长视频数据的前提下，提升长视频生成能力和结果质量。

Method: 本方法核心在于利用教师模型的知识，通过从学生自生成的长视频中采样片段来为学生模型提供指导。无需长视频教师，也无需重叠重算帧，充分维护时序一致性，有效扩展了生成时长。同时避免了以往方法的过度曝光与误差累积问题。

Result: 新方法使生成视频时长达到原教师模型的20倍，算力允许下最长可达4分15秒（接近模型位置编码极限），为基线模型50倍。标准和新提出的基准测试下，效果在质量与一致性上均大幅超过现有基线。

Conclusion: 该方法在无需更多长视频数据或教师的情况下，极大提升了长视频生成的稳定性和可扩展性，为高质量长时序生成提供了极具实用价值的新思路。

Abstract: Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [70] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: 该论文提出了KineMask，一种通过物理引导的视频生成方法，能够实现更真实的刚体控制和物体交互。与现有视频生成模型相比，KineMask在合成真实物理交互场景方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型虽然进步很大，但在生成符合物理规律的物体交互以及精确控制物理效果方面仍存在明显不足。这限制了其在机器人、决策模拟等需要物理真实性的领域应用。作者希望解决这一物理控制与真实性欠缺的问题。

Method: 作者提出KineMask方法：输入单张图片和物体速度，利用两阶段训练策略引导视频扩散模型（VDMs）逐步去除未来运动的监督，实现物体未来运动与交互的合理推断，并结合低层次物理运动控制与高层次文本信息调控生成复杂物理现象视频。

Result: 在合成和真实场景下，KineMask在物体交互的效果上显著优于同类体积的现有模型。消融实验进一步展示了低层次（物理控制）和高层次（文本调控）条件对VDMs生成能力的互补增强。

Conclusion: KineMask通过物理引导与多层次条件结合，为视频生成带来更高的物理真实性和控制能力，对未来多种实际应用场景有促进作用。代码与模型将对外开源，有助于相关领域的后续研究。

Abstract: Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.

</details>


### [71] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: 本文提出通过引入精细化多模态动作感知，实现对复杂交互过程的更精准模拟，促进具身智能机器人精细操作能力的提升。


<details>
  <summary>Details</summary>
Motivation: 现有视频模型在高精度控制能力方面不足，难以满足通用家用机器人在处理细致任务或紧急情况时对实时精细运动控制的需求。为了解决此问题，作者提出集成多模态感觉信息，包括本体感觉、运动觉、力觉触觉和肌肉激活，以支持精细动作执行。

Method: 方法上，作者开发了一种特征学习范式，将多种感知模态有效对齐的同时，保留各自独特信息，并进一步提出正则化方案以增强动作轨迹特征的因果性，更好地表达复杂交互过程中的动态关系。

Result: 实验显示，融合多模态感觉显著提升了模拟的准确性，减小了时序漂移。此外，作者通过消融实验和下游任务验证了所提方法的有效性和实用性。

Conclusion: 引入精细化多模态感觉不仅提升了机器人的动作模拟能力，也为未来具备高精细操作能力的家用或通用型机器人发展奠定了基础。

Abstract: Current video models fail as world model as they lack fine-graiend control.
General-purpose household robots require real-time fine motor control to handle
delicate tasks and urgent situations. In this work, we introduce fine-grained
multimodal actions to capture such precise control. We consider senses of
proprioception, kinesthesia, force haptics, and muscle activation. Such
multimodal senses naturally enables fine-grained interactions that are
difficult to simulate with text-conditioned generative models. To effectively
simulate fine-grained multisensory actions, we develop a feature learning
paradigm that aligns these modalities while preserving the unique information
each modality provides. We further propose a regularization scheme to enhance
causality of the action trajectory features in representing intricate
interaction dynamics. Experiments show that incorporating multimodal senses
improves simulation accuracy and reduces temporal drift. Extensive ablation
studies and downstream applications demonstrate the effectiveness and
practicality of our work.

</details>


### [72] [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)
*Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频-语言模型注意力机制VideoNSA，有效提升了模型对长视频理解的能力，尤其在时间和空间推理任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: 主流多模态语言模型在处理长视频时，容易错过关键转场帧，跨长时间段保持连贯性存在困难，因此需要改进注意力机制以增强模型对长视频的理解能力。

Method: 提出了VideoNSA方法，在Qwen2.5-VL模型基础上，通过216K视频指令数据集端到端训练，将Native Sparse Attention（NSA）应用于视频部分，而文本部分维持稠密注意力，形成硬件友好型混合注意力机制。

Result: VideoNSA在长视频理解、时序推理和空间基准任务上表现优于基于Token压缩和无训练稀疏注意力的基线方法。消融实验揭示了该方法在如下方面的优越性：128K Token扩展、最优全局-局部注意力分配、任务相关的分支使用模式，以及带有可学习稀疏注意力的动态注意力收敛。

Conclusion: VideoNSA能够有效提升多模态大模型的长视频理解能力，特别在维持时空信息连贯性和推理性任务中具有显著优势，具有实际应用潜力。

Abstract: Video understanding in multimodal language models remains limited by context
length: models often miss key transition frames and struggle to maintain
coherence across long time scales. To address this, we adapt Native Sparse
Attention (NSA) to video-language models. Our method, VideoNSA, adapts
Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We
employ a hardware-aware hybrid approach to attention, preserving dense
attention for text, while employing NSA for video. Compared to
token-compression and training-free sparse baselines, VideoNSA achieves
improved performance on long-video understanding, temporal reasoning, and
spatial benchmarks. Further ablation analysis reveals four key findings: (1)
reliable scaling to 128K tokens; (2) an optimal global-local attention
allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)
the learnable combined sparse attention help induce dynamic attention sinks.

</details>


### [73] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: 论文提出了NoiseShift方法，通过重新校准去噪器的噪声水平，有效提升了扩散模型在低分辨率图像生成时的表现，且无需改变模型结构或采样流程。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到图像扩散模型在不同分辨率下泛化能力有限，尤其在低于训练分辨率时效果显著下降；同时高分辨率模型无法灵活满足对低分辨率、高效率生成需求。作者发现不同分辨率下噪声调度器对图像的感知影响并不一致，导致训练与测试间存在不匹配。

Method: 提出NoiseShift方法——通过分辨率条件下对去噪器噪声水平进行重新校准，无需修改模型架构或采样过程，也无须重新训练，作为现有扩散模型的无训练开箱即用补丁。

Result: 在Stable Diffusion 3，3.5及Flux-Dev模型、LAION-COCO和CelebA等数据集上实验证明，NoiseShift能显著提升低分辨率下的生成质量（以FID评测）：SD3.5、SD3和Flux-Dev平均分别提升15.89%、8.56%、2.44%（LAION-COCO）；10.36%、5.19%、3.02%（CelebA）。

Conclusion: NoiseShift能有效减轻因分辨率差异引发的扩散模型伪影，提升低分辨率生成质量，对已有模型具备良好的兼容与实际价值。

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often
fail to generalize, even when asked to generate images at lower resolutions
than those seen during training. High-resolution text-to-image generators are
currently unable to easily offer an out-of-the-box budget-efficient alternative
to their users who might not need high-resolution images. We identify a key
technical insight in diffusion models that when addressed can help tackle this
limitation: Noise schedulers have unequal perceptual effects across
resolutions. The same level of noise removes disproportionately more signal
from lower-resolution images than from high-resolution images, leading to a
train-test mismatch. We propose NoiseShift, a training-free method that
recalibrates the noise level of the denoiser conditioned on resolution size.
NoiseShift requires no changes to model architecture or sampling schedule and
is compatible with existing models. When applied to Stable Diffusion 3, Stable
Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly
improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and
Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by
10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results
demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent
artifacts and enhancing the quality of low-resolution image generation.

</details>


### [74] [Inferring Dynamic Physical Properties from Video Foundation Models](https://arxiv.org/abs/2510.02311)
*Guanqi Zhan,Xianzheng Ma,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本文旨在从视频中预测物体的动态物理属性，如弹性、粘度和动态摩擦，通过提出新数据集和多种推理方法，系统比较各类模型在该任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 动态物理属性（如弹性、粘度、摩擦）难以仅凭静态图像获取，只有借助视频的时序信息才能准确推断。现有资料和方法难以为这些属性提供合适的数据和统一评测基线。

Method: 1）新建每种物理属性的视频数据集，分别包含合成训练/测试集与真实世界数据；2）提出三种推理方法：a）利用经典视觉技术获得特征的oracle方法；b）用预训练生成/自监督视频基础模型，通过视觉提示和可训练prompt向量实现cross-attention推理；c）将多模态大语言模型（MLLM）与不同prompt策略用于该任务。

Result: 生成式与自监督的视频基础模型在物理属性推理任务上表现接近，但都低于oracle方法水平；MLLM目前的性能仍然更差，但通过优化prompt设计可明显提升结果。

Conclusion: 用适当设计的数据和方法，现有的视频基础模型已具备推断复杂动态物理属性的能力，但与经典特征工程方法相比仍有差距，未来MLLM通过prompt优化有望改进。

Abstract: We study the task of predicting dynamic physical properties from videos. More
specifically, we consider physical properties that require temporal information
to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,
and dynamic friction of an object sliding on a surface. To this end, we make
the following contributions: (i) We collect a new video dataset for each
physical property, consisting of synthetic training and testing splits, as well
as a real split for real world evaluation. (ii) We explore three ways to infer
the physical property from videos: (a) an oracle method where we supply the
visual cues that intrinsically reflect the property using classical computer
vision techniques; (b) a simple read out mechanism using a visual prompt and
trainable prompt vector for cross-attention on pre-trained video generative and
self-supervised models; and (c) prompt strategies for Multi-modal Large
Language Models (MLLMs). (iii) We show that video foundation models trained in
a generative or self-supervised manner achieve a similar performance, though
behind that of the oracle, and MLLMs are currently inferior to the other
models, though their performance can be improved through suitable prompting.

</details>


### [75] [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](https://arxiv.org/abs/2510.02313)
*Mengyu Yang,Yiming Chen,Haozheng Pei,Siddhant Agarwal,Arun Balajee Vasudevan,James Hays*

Main category: cs.CV

TL;DR: 本文提出了一项新任务——声音物体检测，评估模型根据声音判断参与物体的能力。作者提出了多模态、感知对象的框架，并利用现实生活中的第一视角视频进行训练，通过自动分割和slot attention机制聚焦相关物体特征。实验结果表现优越。


<details>
  <summary>Details</summary>
Motivation: 人类能凭声音辨别物体，类似地，AI若能理解物体互动发出的独特声音，有助于视觉与听觉的融合感知。本研究旨在推动模型这一能力的提升。

Method: 提出多模态的对象感知框架，结合第一视角视频，首先用自动分割获得相关物体的掩码，引导模型关注互动焦点。模型采用slot attention视觉编码器以加强对象相关性。

Result: 在新提出的声音物体检测任务和现有多模态动作理解任务上，该方法均取得了最先进的性能。

Conclusion: 基于对象感知与多模态特征融合的方法，有效提升了模型基于声音识别和理解物体的能力。

Abstract: Can a model distinguish between the sound of a spoon hitting a hardwood floor
versus a carpeted one? Everyday object interactions produce sounds unique to
the objects involved. We introduce the sounding object detection task to
evaluate a model's ability to link these sounds to the objects directly
involved. Inspired by human perception, our multimodal object-aware framework
learns from in-the-wild egocentric videos. To encourage an object-centric
approach, we first develop an automatic pipeline to compute segmentation masks
of the objects involved to guide the model's focus during training towards the
most informative regions of the interaction. A slot attention visual encoder is
used to further enforce an object prior. We demonstrate state of the art
performance on our new task along with existing multimodal action understanding
tasks.

</details>


### [76] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: 该论文提出了一种针对3D Gaussian Splatting（3DGS）场景重建方法的新型图像级别数据投毒手段，有效插入具有视角依赖性的虚假目标，并极大增强攻击效果。


<details>
  <summary>Details</summary>
Motivation: 随着NeRF和3DGS等3D场景表示方法广泛应用于新视角合成，它们的安全性和鲁棒性问题变得越来越重要。研究如何有效攻击这些模型，有助于揭示其脆弱性，并为后续防御提供理论依据。

Method: 本文提出一种基于密度引导的投毒方法，利用核密度估计（KDE）选取3DGS中的低密度区域，在这些区域中插入高斯点，从而向模型植入仅在特定视角下可见的虚假目标。此外，作者还加入了自适应噪声以破坏多视角一致性，并提出了基于KDE的新评测协议，更系统地衡量攻击难度。

Result: 大量实验表明，所提出方法相比现有最新攻击技术，在可见性、干扰性和隐蔽性方面均实现了显著提升，能够更有效地在不影响大多数正常视角的前提下，向目标视角注入虚假物体。

Conclusion: 本文方法为3DGS等3D重建系统的安全性评估提供了新的有效攻击手段和更科学的评价协议，为未来3D视觉系统鲁棒性研究奠定了基础。

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

</details>


### [77] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: 该论文针对文本到图像生成模型在处理多主体描述时存在的属性泄漏、身份纠结和主体遗漏问题，提出了基于随机最优控制理论的采样动态引导新框架，并设计了两个通用算法，有效提升了多主体一致性。提出的方法适用于多种主流扩散模型，并在多个基准上获得了最先进的多主体保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像（T2I）生成模型在处理包含单一实体的描述时表现出色，但在涉及多个主体（如多个人物或物体）的复杂描述时，容易出现主体信息混淆、特征混合或遗漏等问题，限制了其实际应用能力。因此，提升多主体描述下的生成效果是亟需解决的重要问题。

Method: 作者将流匹配（Flow Matching，FM）方法与随机最优控制（Stochastic Optimal Control，SOC）相结合，将主体解耦视为对已训练FM采样器的控制问题，并以此为出发点，设计了两种无模型结构约束的算法：（1）无需重新训练、仅凭测试阶段单步更新即可引导采样动态的控制器；（2）Adjoint Matching，一种高效微调规则，用于回归控制网络以适应反向伴随信号，同时保持原模型能力。

Result: 实验证明，提出的方法在Stable Diffusion 3.5、FLUX和Stable Diffusion XL等主流模型上均能稳定提升多主体描述下的生成一致性，并保持原有的风格特征。其中，测试时控制器能够在普通GPU上高效运行，而微调控制器在有限提示下学习后也可很好泛化至未见过的新提示。提出的FOCUS方法获得了当前最优的多主体生成保真度。

Conclusion: 通过将采样过程建模为可优化的控制问题，并提出通用性强、有效的多主体采样引导算法，论文为文本到图像生成任务中的多主体难题提供了理论与实践上的突破。新方法能显著提升多主体描述下的生成效果，具有较强的实用价值和推广前景。

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with
multi-subject descriptions, often showing attribute leakage, identity
entanglement, and subject omissions. We introduce the first theoretical
framework with a principled, optimizable objective for steering sampling
dynamics toward multi-subject fidelity. Viewing flow matching (FM) through
stochastic optimal control (SOC), we formulate subject disentanglement as
control over a trained FM sampler. This yields two architecture-agnostic
algorithms: (i) a training-free test-time controller that perturbs the base
velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight
fine-tuning rule that regresses a control network to a backward adjoint signal
while preserving base-model capabilities. The same formulation unifies prior
attention heuristics, extends to diffusion models via a flow-diffusion
correspondence, and provides the first fine-tuning route explicitly designed
for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and
Stable Diffusion XL, both algorithms consistently improve multi-subject
alignment while maintaining base-model style. Test-time control runs
efficiently on commodity GPUs, and fine-tuned controllers trained on limited
prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal
Control for Unentangled Subjects), which achieves state-of-the-art
multi-subject fidelity across models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [78] [Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset](https://arxiv.org/abs/2510.01219)
*Leroy Z. Wang*

Main category: cs.CL

TL;DR: 本文提出了一个用于揭示大模型隐性偏见的概念学习任务数据集，并发现模型在量词上有向上单调性的偏见。


<details>
  <summary>Details</summary>
Motivation: 以往对大模型的偏见检测多依赖直接提问，难以发现隐性、潜在的认知偏见，尤其是在复杂学习场景下。作者希望通过更复杂的学习情境深入了解模型的偏见。

Method: 作者设计了一个概念学习任务数据集，并通过上下文概念学习实验，观察模型对于量词的响应偏好，还将结果与直接提示下的表现进行对比。

Result: 实验发现，大模型在概念学习情况下展现出对量词的向上单调性偏见，而在直接提示时这种偏见没有那么明显。

Conclusion: 上下文概念学习是一种有效发现大语言模型隐性偏见的方法，可以揭示模型在理解和推理时的潜在非显性表现。

Abstract: We introduce a dataset of concept learning tasks that helps uncover implicit
biases in large language models. Using in-context concept learning experiments,
we found that language models may have a bias toward upward monotonicity in
quantifiers; such bias is less apparent when the model is tested by direct
prompting without concept learning components. This demonstrates that
in-context concept learning can be an effective way to discover hidden biases
in language models.

</details>


### [79] [Towards Open-Ended Discovery for Low-Resource NLP](https://arxiv.org/abs/2510.01220)
*Bonaventure F. P. Dossou,Henri Aïdasso*

Main category: cs.CL

TL;DR: 本文呼吁自然语言处理（NLP）对于低资源语言的研究模式由静态数据收集转向动态、交互式语言发现，使AI系统能通过与人类对话不断学习新语言，而非依赖大规模、预先收集的数据。


<details>
  <summary>Details</summary>
Motivation: 当前大模型等NLP技术对低资源语言支持有限，主要受限于缺乏语料、标准拼写和高效的注释流程，且依赖中心化、海量数据收集模式，使弱势语言社区难以受益。

Method: 作者提出基于人机协同和不确定性的交互式学习框架，融合模型的知识不确定度与人类说话者的犹豫与置信信号，共同指导交互、问题选择和记忆保留，推动AI系统主动、动态地学习语言。

Result: 提出了新的交互式、参与性协作语言发现框架，强调模型与人类共同参与、互相适应，推动语言技术应用于低资源、未充分记录语言领域。

Conclusion: 论文呼吁AI领域应摒弃单方面数据挖掘，转向以人为本、社区共建、动态合作的AI语言学习模式，以促进语言多样性保护和赋权边缘群体。

Abstract: Natural Language Processing (NLP) for low-resource languages remains
fundamentally constrained by the lack of textual corpora, standardized
orthographies, and scalable annotation pipelines. While recent advances in
large language models have improved cross-lingual transfer, they remain
inaccessible to underrepresented communities due to their reliance on massive,
pre-collected data and centralized infrastructure. In this position paper, we
argue for a paradigm shift toward open-ended, interactive language discovery,
where AI systems learn new languages dynamically through dialogue rather than
static datasets. We contend that the future of language technology,
particularly for low-resource and under-documented languages, must move beyond
static data collection pipelines toward interactive, uncertainty-driven
discovery, where learning emerges dynamically from human-machine collaboration
instead of being limited to pre-existing datasets. We propose a framework
grounded in joint human-machine uncertainty, combining epistemic uncertainty
from the model with hesitation cues and confidence signals from human speakers
to guide interaction, query selection, and memory retention. This paper is a
call to action: we advocate a rethinking of how AI engages with human knowledge
in under-documented languages, moving from extractive data collection toward
participatory, co-adaptive learning processes that respect and empower
communities while discovering and preserving the world's linguistic diversity.
This vision aligns with principles of human-centered AI, emphasizing
interactive, cooperative model building between AI systems and speakers.

</details>


### [80] [Discourse vs emissions: Analysis of corporate narratives, symbolic practices, and mimicry through LLMs](https://arxiv.org/abs/2510.01222)
*Bertrand Kian Hassani,Yacoub Bahini,Rizwan Mushtaq*

Main category: cs.CL

TL;DR: 本文通过多维度框架对828家美国上市公司的气候相关披露成熟度进行评估，发现气候信息披露存在模仿行为，且定量目标与叙述语调脱节。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧推动市场对透明、可比的企业气候信息披露需求，但普遍存在象征性披露和模仿行为，影响披露效果。因此需要量化、系统地评估企业披露质量和特征。

Method: 本文利用专为气候传播微调的大型语言模型（LLMs），开发了包括情感、承诺、具体性和目标雄心等四个分类器，对企业可持续性和年度报告的叙述内容进行分析，并结合企业排放量、市值和行业等属性进行多维度评估。

Result: 研究发现：(1) 风险导向的叙述通常与明确承诺相关，但定量目标（如净零承诺）与文本语调未紧密关联；(2) 大市值和高排放企业比同行披露更多承诺和行动，但与定量目标一致性较低；(3) 披露风格高度相似，存在广泛模仿行为，削弱了披露识别度和决策价值。

Conclusion: LLMs在ESG叙事分析中具有显著价值，但为实现承诺与可验证的转型策略有效连接，需要更强有力的监管措施。

Abstract: Climate change has increased demands for transparent and comparable corporate
climate disclosures, yet imitation and symbolic reporting often undermine their
value. This paper develops a multidimensional framework to assess disclosure
maturity among 828 U.S.listed firms using large language models (LLMs)
fine-tuned for climate communication. Four classifiers-sentiment, commitment,
specificity, and target ambition-extract narrative indicators from
sustainability and annual reports, which are linked to firm attributes such as
emissions, market capitalization, and sector. Analyses reveal three insights:
(1) risk-focused narratives often align with explicit commitments, but
quantitative targets (e.g., net-zero pledges) remain decoupled from tone; (2)
larger and higher-emitting firms disclose more commitments and actions than
peers, though inconsistently with quantitative targets; and (3) widespread
similarity in disclosure styles suggests mimetic behavior, reducing
differentiation and decision usefulness. These results highlight the value of
LLMs for ESG narrative analysis and the need for stronger regulation to connect
commitments with verifiable transition strategies.

</details>


### [81] [Context Matters: Comparison of commercial large language tools in veterinary medicine](https://arxiv.org/abs/2510.01224)
*Tyler J Poore,Christopher J Pinard,Aleena Shabbir,Andrew Lagree,Andre Telfer,Kuan-Chuen Wu*

Main category: cs.CL

TL;DR: 本研究评估了三款面向兽医领域的LLM摘要工具在兽医肿瘤记录标准化数据集上的表现。结果显示，产品1（Hachiko）整体表现最佳，尤其在事实准确性和时间顺序方面表现突出。LLM作为评判工具的方法具有高可重复性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLM）已广泛应用于临床，但其在兽医医学领域的表现尚未被深入研究。研究旨在填补兽医临床NLP应用评估的空白，帮助找到适用于该领域的高效工具。

Method: 采用标准化的兽医肿瘤病历数据，利用LLM辅助评判体系（LLM-as-a-judge），从事实准确性、完整性、时间顺序、临床相关性和组织结构五个方面，对三款市售兽医LLM工具进行评分，并在三次独立评估中考察评分框架的一致性。

Result: 产品1（Hachiko）在所有评分域中总体表现最好，尤其在事实准确性和时间顺序上获得完美中位数分。评分框架在三次独立评估中表现出高度的结果一致性。

Conclusion: 兽医专用LLM工具在该领域具有显著优势，LLM-as-a-judge是一种可扩展且高度可重复的方法，可用于评估兽医医学中的临床NLP摘要系统。

Abstract: Large language models (LLMs) are increasingly used in clinical settings, yet
their performance in veterinary medicine remains underexplored. We evaluated
three commercially available veterinary-focused LLM summarization tools
(Product 1 [Hachiko] and Products 2 and 3) on a standardized dataset of
veterinary oncology records. Using a rubric-guided LLM-as-a-judge framework,
summaries were scored across five domains: Factual Accuracy, Completeness,
Chronological Order, Clinical Relevance, and Organization. Product 1 achieved
the highest overall performance, with a median average score of 4.61 (IQR:
0.73), compared to 2.55 (IQR: 0.78) for Product 2 and 2.45 (IQR: 0.92) for
Product 3. It also received perfect median scores in Factual Accuracy and
Chronological Order. To assess the internal consistency of the grading
framework itself, we repeated the evaluation across three independent runs. The
LLM grader demonstrated high reproducibility, with Average Score standard
deviations of 0.015 (Product 1), 0.088 (Product 2), and 0.034 (Product 3).
These findings highlight the importance of veterinary-specific commercial LLM
tools and demonstrate that LLM-as-a-judge evaluation is a scalable and
reproducible method for assessing clinical NLP summarization in veterinary
medicine.

</details>


### [82] [ClaimCheck: Real-Time Fact-Checking with Small Language Models](https://arxiv.org/abs/2510.01226)
*Akshith Reddy Putta,Jacob Devasier,Chengkai Li*

Main category: cs.CL

TL;DR: 本文提出ClaimCheck系统，结合小型语言模型和实时Web证据，实现透明、低算力消耗的自动事实核查，并达到业界领先准确率。


<details>
  <summary>Details</summary>
Motivation: 现有自动事实核查系统多依赖大型、封闭模型和静态知识库，计算成本高且不可解释，难以满足可访问性和透明性需求。作者旨在突破这些局限，用小型模型高效执行事实核查任务。

Method: ClaimCheck采用分步核查流程，包括Web搜索规划、证据检索与摘要、证据综合与复检及结论评估，各模块均针对小型LLM（如Qwen3-4B）优化，通过细致的模块化设计和精心的提示工程，提高系统性能。

Result: ClaimCheck在AVeriTeC数据集上准确率达到76.4%，超越使用LLaMA3.1 70B和GPT-4o的大模型方案。消融实验显示，合理设计流程和Prompt可弥补小模型的不足。

Conclusion: ClaimCheck证明了通过合理设计流程与小型LLM优化，即使算力有限也可实现透明、高效的事实核查，推动自动核查系统向开放、可解释方向发展。

Abstract: We introduce ClaimCheck, an LLM-guided automatic fact-checking system
designed to verify real-world claims using live Web evidence and small language
models. Unlike prior systems that rely on large, closed-source models and
static knowledge stores, ClaimCheck employs a transparent, stepwise
verification pipeline that mirrors human fact-checking workflows consisting of
Web search query planning, Web-based evidence retrieval and summarization,
evidence synthesis and re-retrieval, and claim verdict evaluation. Each module
is optimized for small LLMs, allowing the system to deliver accurate and
interpretable fact-checking with significantly lower computational
requirements. Despite using a much smaller Qwen3-4B model, ClaimCheck achieves
state-of-the-art accuracy of 76.4% on the AVeriTeC dataset, outperforming
previous approaches using LLaMA3.1 70B and GPT-4o. Extensive ablations
demonstrate that careful modular design and prompting strategies can overcome
the limitations of smaller LLMs. To promote accessibility and transparency, we
provide a public demo at https://idir.uta.edu/claimcheck.

</details>


### [83] [EEFSUVA: A New Mathematical Olympiad Benchmark](https://arxiv.org/abs/2510.01227)
*Nicole N Khatibi,Daniil A. Radamovich,Michael P. Brenner*

Main category: cs.CL

TL;DR: 论文质疑了当前大语言模型在数学基准测试上的高分，并提出新的、更全面的评测集来更客观地评估其推理能力。


<details>
  <summary>Details</summary>
Motivation: 近期有声音认为大语言模型在数学奥赛等基准测试中已达高水平，但这些基准数据集多来源于IMO等比赛，或存在数据污染与题型狭窄的问题，因此实际能力或被高估。

Method: 作者分析了现有基准的组成及局限，并提出一个新基准EEFSUVA，涵盖来自东欧及前苏联的地方及国家奥赛题目，这些题目难度高、技巧独特，且网上曝光度极低。

Result: 实验发现，即便是最先进的大模型，在EEFSUVA上的成绩也明显低于传统的奥赛题基准。

Conclusion: 作者认为应采用更广泛、更具代表性的评测数据集来更全面地衡量模型的数学推理水平，并以此引导未来模型的研发。

Abstract: Recent breakthroughs have spurred claims that large language models (LLMs)
match gold medal Olympiad to graduate level proficiency on mathematics
benchmarks. In this work, we examine these claims in detail and assess the
extent to which current benchmarks capture genuine LLM mathematical reasoning.
The composition of these benchmarks, primarily drawing from the International
Mathematics Olympiad (IMO) and related competitions, may overstate models
reasoning ability due to potential data contamination and a narrow focus on
familiar problem types. To enable a more holistic assessment of mathematical
understanding, we introduce EEFSUVA, a novel benchmark curated from under
circulated regional and national Olympiads of Eastern Europe and the countries
from the former Soviet Union. These contests feature problems of comparable
difficulty to the IMO and are renowned for demanding nonstandard
problem-solving techniques, yet their problems are far less prevalent in online
corpora. Preliminary results suggest that even state-of-the-art LLMs exhibit a
notable performance decline on EEFSUVA relative to other Olympiad-style
benchmarks. These findings also suggest the potential importance of broader
evaluation datasets for a fuller assessment of mathematical reasoning and for
guiding future model development.

</details>


### [84] [Who is In Charge? Dissecting Role Conflicts in Instruction Following](https://arxiv.org/abs/2510.01228)
*Siqi Zeng*

Main category: cs.CL

TL;DR: 本论文研究大语言模型在面对系统指令与用户输入等分层指令时的服从性，发现模型对系统指令的服从性较脆弱，反而更容易被社会性线索（如权威、共识）影响。作者通过大规模数据集，关联模型内部机制，提出需要更有效、轻量级、对层级敏感的对齐方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型应优先服从系统指令而非用户输入，但现实中模型常无视该层级规则，尤其会优先响应社会性提示。理解其内因对于模型安全与可控性具有重要意义。

Method: 作者采用大规模数据集，结合机制解释，包括线性探测、直接Logit归因等方法，分析系统与用户、社会冲突下模型的内部冲突检测和决策表现，并进行steering实验评估不同冲突对模型行为的影响。

Result: 实验证明，模型对系统-用户冲突能早期检测，但只在社会性线索时才有较一致的内部冲突解决表现。进一步，虽然社会性线索被利用，但意外地，这些向量还能以无关身份强化指令服从性。

Conclusion: 模型对系统层级的服从性不足，且易被社会性线索左右。未来需发展更轻量级且层级敏感的对齐方案，以增强模型的层级服从能力和安全性。

Abstract: Large language models should follow hierarchical instructions where system
prompts override user inputs, yet recent work shows they often ignore this rule
while strongly obeying social cues such as authority or consensus. We extend
these behavioral findings with mechanistic interpretations on a large-scale
dataset. Linear probing shows conflict-decision signals are encoded early, with
system-user and social conflicts forming distinct subspaces. Direct Logit
Attribution reveals stronger internal conflict detection in system-user cases
but consistent resolution only for social cues. Steering experiments show that,
despite using social cues, the vectors surprisingly amplify instruction
following in a role-agnostic way. Together, these results explain fragile
system obedience and underscore the need for lightweight hierarchy-sensitive
alignment methods.

</details>


### [85] [Enhancing Transformer-Based Rerankers with Synthetic Data and LLM-Based Supervision](https://arxiv.org/abs/2510.01229)
*Dimitar Peshevski,Kiril Blazhevski,Martin Popovski,Gjorgji Madjarov*

Main category: cs.CL

TL;DR: 该论文提出了一种无需人工标注数据即可提升文档重排序效果的高效方法。通过利用大模型生成合成数据，并用对比学习微调小模型，在保证排名效果的前提下大幅降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在文档重排序任务上表现优异，但推理成本高，难以大规模落地。小模型虽然高效，但通常依赖难以获得的人工标注数据。作者提出的新方法旨在解决这一瓶颈，实现高效低成本的文档重排序。

Method: 作者首先利用LLM从领域文本中自动生成合成查询，然后用LLM分类器构造正负（特别是困难负例）配对，产生合成训练集。接着，采用基于局部对比估计（LCE）损失的对比学习方法，对小型Transformer模型进行微调。

Result: 在MedQuAD医学数据集上的实验结果显示，该方法在领域内任务表现显著提升，对跨领域任务也有良好的泛化能力。

Conclusion: 通过将大模型用于数据生成和监督（而不是推理），既降低了计算成本，又保持了很强的文档重排序能力，为实际应用中小模型的高效部署提供了新思路。

Abstract: Effective document reranking is essential for improving search relevance
across diverse applications. While Large Language Models (LLMs) excel at
reranking due to their deep semantic understanding and reasoning, their high
computational cost makes them impractical for many real-world deployments.
Fine-tuning smaller, task-specific models is a more efficient alternative but
typically depends on scarce, manually labeled data. To overcome this, we
propose a novel pipeline that eliminates the need for human-labeled
query-document pairs. Our method uses LLMs to generate synthetic queries from
domain-specific corpora and employs an LLM-based classifier to label positive
and hard-negative pairs. This synthetic dataset is then used to fine-tune a
smaller transformer model with contrastive learning using Localized Contrastive
Estimation (LCE) loss. Experiments on the MedQuAD dataset show that our
approach significantly boosts in-domain performance and generalizes well to
out-of-domain tasks. By using LLMs for data generation and supervision rather
than inference, we reduce computational costs while maintaining strong
reranking capabilities.

</details>


### [86] [Geometric Structures and Patterns of Meaning: A PHATE Manifold Analysis of Chinese Character Embeddings](https://arxiv.org/abs/2510.01230)
*Wen G. Gong*

Main category: cs.CL

TL;DR: 本文系统性地用PHATE流形分析方法考察了中文字符嵌入的几何结构特征，首次揭示了其语义组织的空间规律。


<details>
  <summary>Details</summary>
Motivation: 传统语言学认为中文字符语义与结构紧密相关，但缺乏系统的计算证据。本文试图用深度嵌入与几何分析工具，探索字符嵌入中的空间/群落结构，验证并扩展传统理论。

Method: 对七种嵌入模型、八种降维方法下的1000余汉字（覆盖12个语义领域）进行PHATE嵌入与聚类分析，并进一步分析了123个短语的“子网络”，以系统揭示字符间语义与几何复杂性的关系。

Result: 发现内容实词多以聚类方式呈现，虚词呈现分支状结构；语义丰富字符的嵌入具有高度几何多样性，而结构性部件则表现为紧密聚类。此外，短语网络系统地展现出由基础字符到更复杂语义的扩展路径。

Conclusion: 本文为中文字符语义组织提供了全新的几何分析视角和实证支持，既呼应了传统语言学理论，也建立了一套新的计算分析框架。

Abstract: We systematically investigate geometric patterns in Chinese character
embeddings using PHATE manifold analysis. Through cross-validation across seven
embedding models and eight dimensionality reduction methods, we observe
clustering patterns for content words and branching patterns for function
words. Analysis of over 1000 Chinese characters across 12 semantic domains
reveals that geometric complexity correlates with semantic content: meaningful
characters exhibit rich geometric diversity while structural radicals collapse
into tight clusters. The comprehensive child-network analysis (123 phrases)
demonstrates systematic semantic expansion from elemental character. These
findings provide computational evidence supporting traditional linguistic
theory and establish a novel framework for geometric analysis of semantic
organization.

</details>


### [87] [Trustworthy Summarization via Uncertainty Quantification and Risk Awareness in Large Language Models](https://arxiv.org/abs/2510.01231)
*Shuaidong Pan,Di Wu*

Main category: cs.CL

TL;DR: 该研究提出了一种能量化不确定性与风险感知机制的大语言模型摘要框架，在高风险场景下显著提升了自动摘要的可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: 应对高风险场景中的信息过载和决策需求，现有自动摘要方法往往对模型预测过于自信，缺乏对不确定性和风险的处理，难以满足实际需求。

Method: 构建基于条件生成的摘要模型，结合贝叶斯推断建模参数空间的不确定性；用生成分布熵量化摘要内容不确定性，通过熵正则化与风险感知损失联合优化，显式表达风险属性；引入风险评分与管控模块，辅助提升摘要的准确性和可信度。

Result: 对比实验和敏感性分析表明，该方法在高风险摘要应用中，能有效提升摘要鲁棒性和可靠性，并保持摘要流畅和语义完整。

Conclusion: 该方法为可信自动摘要提供了系统性解决方案，在方法上具备扩展性和实践价值。

Abstract: This study addresses the reliability of automatic summarization in high-risk
scenarios and proposes a large language model framework that integrates
uncertainty quantification and risk-aware mechanisms. Starting from the demands
of information overload and high-risk decision-making, a conditional
generation-based summarization model is constructed, and Bayesian inference is
introduced during generation to model uncertainty in the parameter space, which
helps avoid overconfident predictions. The uncertainty level of the generated
content is measured using predictive distribution entropy, and a joint
optimization of entropy regularization and risk-aware loss is applied to ensure
that key information is preserved and risk attributes are explicitly expressed
during information compression. On this basis, the model incorporates risk
scoring and regulation modules, allowing summaries to cover the core content
accurately while enhancing trustworthiness through explicit risk-level prompts.
Comparative experiments and sensitivity analyses verify that the proposed
method significantly improves the robustness and reliability of summarization
in high-risk applications while maintaining fluency and semantic integrity.
This research provides a systematic solution for trustworthy summarization and
demonstrates both scalability and practical value at the methodological level.

</details>


### [88] [Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks](https://arxiv.org/abs/2510.01232)
*Dongjun Kim,Gyuho Shim,Yongchan Chun,Minhyuk Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 本文提出了一种新的分析框架，称为Benchmark Profiling，用于系统性分析大语言模型在基准测试中的表现，揭示其背后涉及的多种能力，而不是简单依据单一分数评价模型能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型主要通过标准基准测试来评判其能力，但这些分数往往掩盖了完成任务所需技能的多样性，并没有系统性地验证基准测试是否真正评估了其宣传的能力。

Method: 提出Benchmark Profiling诊断框架，通过梯度重要性打分和有针对性的参数删减，计算各项能力对模型在基准测试中表现的影响分数（Ability Impact Score, AIS），并据此分析模型能力组成。

Result: （1）大多数基准测试涉及多种能力而非单一技能；（2）表面上同类标签的数据集实则依赖不同能力组合；（3）代码生成类基准测试更偏好广泛、多技能提升，对窄域微调收益有限；（4）与任务无关的能力反而可能降低模型性能。

Conclusion: Benchmark Profiling能解释为何模型分数提升未必转化为用户感知的能力提升，并为基准测试审计和模型可解释性提供了透明的分析工具。

Abstract: Large Language Models are commonly judged by their scores on standard
benchmarks, yet such scores often overstate real capability since they mask the
mix of skills a task actually demands. For example, ARC is assumed to test
reasoning, while HellaSwag is designed to evaluate commonsense. However, we
lack a systematic way to verify if these benchmarks actually measure these
labels. We introduce Benchmark Profiling, a diagnostic framework that
decomposes benchmark performance into ten cognitively grounded abilities. The
method combines gradient-based importance scoring with targeted parameter
ablation to compute an Ability Impact Score (AIS) that quantifies how much each
ability contributes to a model's success on a given benchmark. Profiling three
instruction-tuned models across ten widely used benchmarks yields four key
findings: (i) most benchmarks draw on several abilities rather than one, (ii)
datasets with similar labels rely on distinct ability mixtures, (iii)
code-generation benchmarks reward broad, multi-skill improvement and thus show
only modest gains from narrow domain-specific fine-tuning, and (iv) abilities
irrelevant to the task could negatively affect performance. Benchmark Profiling
therefore explains why performance gains do not always translate into
user-perceived competence and offers a transparent tool for benchmark audit and
model interpretability.

</details>


### [89] [Computational Social Linguistics for Telugu Cultural Preservation: Novel Algorithms for Chandassu Metrical Pattern Recognition](https://arxiv.org/abs/2510.01233)
*Boddu Sri Pavan,Boddu Swathi Sree*

Main category: cs.CL

TL;DR: 本研究提出了一种以计算社会科学方法保护泰卢固语Chandassu韵律诗歌传统的综合数字化框架，结合社区协作和现代算法实现高度准确的诗律分析。


<details>
  <summary>Details</summary>
Motivation: 泰卢固Chandassu是一种承载数百年集体文化智慧的韵律诗歌形式，但正面临濒危和流失。传统知识难以数字化和自动分析，需要创新性框架予以保护和传播。

Method: 采用社会计算方法，协同创建4651首带注释的诗歌数据集，融合专家语言验证和文化视角。设计AksharamTokenizer（韵律分词）、LaghuvuGuruvu Generator（音节分类）和PadyaBhedam Checker（自动识别韵律模式）等算法，构建全流程自动分析系统。

Result: 提出的算法在Chandassu评分体系下达到91.73%的准确率，各项评价指标与传统文学标准高度一致。

Conclusion: 证明了计算社会科学方法对于保护濒危文化知识与激发集体智能具有有效性，也为文化保护中的数字人文与社区驱动方法提供了范例。

Abstract: This research presents a computational social science approach to preserving
Telugu Chandassu, the metrical poetry tradition representing centuries of
collective cultural intelligence. We develop the first comprehensive digital
framework for analyzing Telugu prosodic patterns, bridging traditional
community knowledge with modern computational methods. Our social computing
approach involves collaborative dataset creation of 4,651 annotated padyams,
expert-validated linguistic patterns, and culturally-informed algorithmic
design. The framework includes AksharamTokenizer for prosody-aware
tokenization, LaghuvuGuruvu Generator for classifying light and heavy
syllables, and PadyaBhedam Checker for automated pattern recognition. Our
algorithm achieves 91.73% accuracy on the proposed Chandassu Score, with
evaluation metrics reflecting traditional literary standards. This work
demonstrates how computational social science can preserve endangered cultural
knowledge systems while enabling new forms of collective intelligence around
literary heritage. The methodology offers insights for community-centered
approaches to cultural preservation, supporting broader initiatives in digital
humanities and socially-aware computing systems.

</details>


### [90] [LLMRank: Understanding LLM Strengths for Model Routing](https://arxiv.org/abs/2510.01234)
*Shubham Agrawal,Prasang Gupta*

Main category: cs.CL

TL;DR: 本文提出了LLMRank，一个基于提示内容、可解释特征的LLM选择和路由框架，实现了高效且透明的多模型部署。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）能力和延迟、计算开销各异，实际部署时需要在性能和效率之间做权衡，找到最适合每个任务的模型，这对于提升服务效率和降低成本至关重要。

Method: LLMRank通过从用户prompt中提取丰富、可读的人类特征（如任务类型、推理模式、复杂度、句法、轻量代理解答器信号等），用神经排序模型预测各模型的效用。该方法基于RouterBench数据集，涵盖11个基准、11个规模不同的LLM。与传统只用嵌入的路由器不同，它能输出可解释的特征归因。

Result: 在实验中，LLMRank可以达到oracle效用的89.2%，同时给出能够解释模型分配决策的特征归因分析。

Conclusion: 多维度特征提取和混合排序目标对于高效、透明的LLM模型路由十分重要，特征驱动的路由框架有望提升大模型的部署效率和可解释性。

Abstract: The rapid growth of large language models (LLMs) with diverse capabilities,
latency and computational costs presents a critical deployment challenge:
selecting the most suitable model for each prompt to optimize the trade-off
between performance and efficiency. We introduce LLMRank, a prompt-aware
routing framework that leverages rich, human-readable features extracted from
prompts, including task type, reasoning patterns, complexity indicators,
syntactic cues, and signals from a lightweight proxy solver. Unlike prior
one-shot routers that rely solely on latent embeddings, LLMRank predicts
per-model utility using a neural ranking model trained on RouterBench,
comprising 36,497 prompts spanning 11 benchmarks and 11 state-of-the-art LLMs,
from small efficient models to large frontier systems. Our approach achieves up
to 89.2% of oracle utility, while providing interpretable feature attributions
that explain routing decisions. Extensive studies demonstrate the importance of
multifaceted feature extraction and the hybrid ranking objective, highlighting
the potential of feature-driven routing for efficient and transparent LLM
deployment.

</details>


### [91] [GRPO++: Enhancing Dermatological Reasoning under Low Resource Settings](https://arxiv.org/abs/2510.01236)
*Ismam Nur Swapnil,Aranya Saha,Tanvir Ahmed Khan,Mohammad Ariful Haque*

Main category: cs.CL

TL;DR: 本文提出了一种资源高效的视觉-语言模型（VLM）训练方法DermIQ-VLM，专为皮肤病学复杂推理任务设计，并验证其在数据和算力有限环境下优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在医学图像分析中的结构化推理能力受限，主要原因是数据稀缺和先进训练技术的高计算成本。皮肤病学领域尤其面临这些挑战，需要创新方法来提升模型的诊断推理与可靠性。

Method: 本文提出多阶段训练流程：首先使用改进后的Grouped Relative Policy Optimization（GRPO++）进行面向推理的疾病识别训练，接着通过有监督微调提升对话能力，最后结合Direct Preference Optimization（DPO）和知识图谱对模型输出进行专家式调优，以减少事实性错误。

Result: 在皮肤病学专用数据集上，DermIQ-VLM比标准微调方法取得了显著的性能提升。

Conclusion: 该方法为资源受限环境下开发专业、可靠的视觉-语言模型提供了一条可行路径，显著提升了复杂医学推理任务的表现。

Abstract: Vision-Language Models (VLMs) show promise in medical image analysis, yet
their capacity for structured reasoning in complex domains like dermatology is
often limited by data scarcity and the high computational cost of advanced
training techniques. To address these challenges, we introduce DermIQ-VLM, a
VLM developed through a multi-stage, resource-efficient methodology designed to
emulate a dermatologist's diagnostic process. Our primary contribution is a
modified version of Grouped Relative Policy Optimization (GRPO), called GRPO++,
which stabilizes the powerful but data-intensive GRPO framework. Our proposed
training pipeline first employs GRPO++ for reasoning-oriented disease
recognition, followed by supervised fine-tuning for conversational ability. To
mitigate factual errors introduced during this step, we then align the model
using Direct Preference Optimization (DPO), leveraging a Knowledge Graph-based
system as a scalable proxy for expert preference. A preliminary evaluation on a
curated dermatological dataset demonstrates that our proposed methodology
yields notable performance gains over standard fine-tuning approaches. These
findings validate the potential of our pipeline as a feasible pathway for
developing specialized, reliable VLMs in resource-constrained environments.

</details>


### [92] [Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation](https://arxiv.org/abs/2510.01237)
*Nandakishor M*

Main category: cs.CL

TL;DR: 该论文提出了一种在生成前主动评估大模型不确定性的信心感知路由系统，显著提升了幻觉检测能力并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有大模型的幻觉问题普遍，且目前多采用生成后修正的方法，计算成本高且难以根源解决问题。因此，迫切需要提前检测和预防幻觉生成的方法。

Method: 作者提出结合三种信号（语义对齐度、模型层间内部收敛性、置信度预测）计算统一信心分数，根据得分将输入路由到本地生成、检索增强、使用更大模型或人工审核等不同路径。

Result: 在知识密集型问答基准上，该系统使幻觉检测能力从0.42提升至0.74，F1分数从0.61提升到0.82，且错误率低至0.09，与传统事后修正相比可减少40%计算消耗。

Conclusion: 采用生成前主动信心评估和路由的新范式，能更高效地提升大模型输出的可靠性，优于传统事后修正策略。

Abstract: Large Language Models suffer from hallucination, generating plausible yet
factually incorrect content. Current mitigation strategies focus on
post-generation correction, which is computationally expensive and fails to
prevent unreliable content generation. We propose a confidence-aware routing
system that proactively assesses model uncertainty before generation and
redirects queries based on estimated reliability. Our approach combines three
complementary signals: semantic alignment between internal representations and
reference embeddings, internal convergence analysis across model layers, and
learned confidence estimation. The unified confidence score determines routing
to four pathways: local generation for high confidence, retrieval-augmented
generation for medium confidence, larger models for low confidence, and human
review for very low confidence. Evaluation on knowledge-intensive QA benchmarks
demonstrates significant improvements in hallucination detection (0.74 vs. 0.42
baseline) while reducing computational costs by 40% compared to post-hoc
methods. The F1 score improves from 0.61 to 0.82 with low false positive rates
(0.09). This paradigm shift from reactive correction to proactive assessment
offers a computationally efficient approach to LLM reliability enhancement.

</details>


### [93] [Silent Tokens, Loud Effects: Padding in LLMs](https://arxiv.org/abs/2510.01238)
*Rom Himelstein,Amit LeVi,Yonatan Belinkov,Avi Mendelson*

Main category: cs.CL

TL;DR: 本文系统性研究了在大模型推理中Padding token的影响，发现Padding并非无害，会影响模型表现、偏见和安全。


<details>
  <summary>Details</summary>
Motivation: 虽然理论上Padding token应被完全掩码，但实现上的错误可能让它们影响模型计算。其对模型的实际影响尚未有系统性分析。

Method: 作者在Llama、Gemma、Qwen三种开源大模型中，插入不同量的Padding token，分别分析对神经元激活、生成质量、偏见和安全等方面的影响。

Result: 即使少量的Padding也会导致隐藏层表征偏移，小模型的生成质量下降，偏见表现不可预测，并削弱安全防护。

Conclusion: Padding不是无害小细节，而是需要在部署时谨慎处理的鲁棒性风险。

Abstract: Padding tokens are widely used in large language models (LLMs) to equalize
sequence lengths during batched inference. While they should be fully masked,
implementation errors can cause them to influence computation, and the extent
of this influence is not well understood. We systematically study this effect
across three open-source model families (Llama, Gemma, Qwen), inserting
controlled amounts of padding and evaluating outcomes along four axes:
activations, generation quality, bias, and safety. Even small amounts of
padding shift hidden representations, degrade quality in smaller models, alter
bias in unpredictable ways, and weaken safety guardrails. These findings
demonstrate that padding is not a harmless detail but a robustness risk that
must be carefully handled in deployment.

</details>


### [94] [CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn Interactions with a Single On-Device LLM](https://arxiv.org/abs/2510.01239)
*Juntae Lee,Jihwan Bang,Seunghan Yang,Simyung Chang*

Main category: cs.CL

TL;DR: 该论文提出了CIFLEX系统，用于提升单一本地大语言模型在多轮多子任务对话时的执行效率，其核心在于高效切换与任务处理，极大减少了不必要的计算消耗。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，用户希望仅通过一个模型即可完成多样的子任务。然而，现有方法每次任务切换时都会重复处理全部对话上下文，造成大量的计算资源浪费，尤其是在本地设备环境下尤为突出。

Method: CIFLEX的关键方法是在主任务与子任务之间切换时，重用主任务的KV缓存，仅为各子任务注入独立的指令路径。执行子任务后，通过缓存上下文回到主任务路径，避免重复的上下文推理。此外，提出了一种适用于小规模模型的二分层级分类策略，将复杂的多选任务拆为一系列二分类决策，从而提升效率。

Result: 实验结果表明，CIFLEX能显著降低多任务对话计算消耗，在提升效率的同时维持任务表现不降，表明系统在设备端实现可扩展、高效多任务对话的能力。

Conclusion: CIFLEX能够高效支持本地端大模型的多任务对话需求，通过缓存复用和分层决策方案，突破了现有多子任务执行过程中的计算瓶颈，是实现本地化多任务对话智能的新途径。

Abstract: We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which
is a novel execution system for efficient sub-task handling in multi-turn
interactions with a single on-device large language model (LLM). As LLMs become
increasingly capable, a single model is expected to handle diverse sub-tasks
that more effectively and comprehensively support answering user requests.
Naive approach reprocesses the entire conversation context when switching
between main and sub-tasks (e.g., query rewriting, summarization), incurring
significant computational overhead. CIFLEX mitigates this overhead by reusing
the key-value (KV) cache from the main task and injecting only task-specific
instructions into isolated side paths. After sub-task execution, the model
rolls back to the main path via cached context, thereby avoiding redundant
prefill computation. To support sub-task selection, we also develop a
hierarchical classification strategy tailored for small-scale models,
decomposing multi-choice decisions into binary ones. Experiments show that
CIFLEX significantly reduces computational costs without degrading task
performance, enabling scalable and efficient multi-task dialogue on-device.

</details>


### [95] [SKYLENAGE Technical Report: Mathematical Reasoning and Contest-Innovation Benchmarks for Multi-Level Math Evaluation](https://arxiv.org/abs/2510.01241)
*Hu Wei,Ze Xu,Boyu Yang,Linlin Miao,Weiqi Zhai,Yihan Li,Zixuan Li,Zhijun Wang,Boya Wang,Jianwei Yu,Jialing Yuan,Xiaoyue Zhang,Cheng He,Minglei Chen,Zifan Zhang,Qianhui Li,Wei Wang,Xiang Xu*

Main category: cs.CL

TL;DR: 本文提出并公开了两个新的数学基准测试集（SKYLENAGE-ReasoningMATH和SKYLENAGE-MATH），用于系统评估大型语言模型（LLMs）在数学推理和多层级竞赛题上的表现。结果显示当前最强LLM距离完全解决这些任务还有明显差距，数据集为未来模型提升提供了挑战和参考标准。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在公开数学测试集上表现优异，但在更高难度的数学问题和分层评估中逐渐出现“天花板效应”，即模型进步缓慢。缺乏具备分层难度、细致元数据的指标集，阻碍了精细化、针对性地分析和推动模型水平提升。

Method: 作者构建了两个互补的数学基准集：1）SKYLENAGE-ReasoningMATH，包含100道结构化诊断题，并为每题提供长度、数字密度和符号复杂性等元数据；2）SKYLENAGE-MATH，包含150道跨高中至博士层级、涵盖七大数学学科的竞赛题。作者使用统一评测流程对15种当代LLM进行模型-学科和模型-年级分析。

Result: 在竞赛集SKYLENAGE-MATH上，最优模型准确率为44%，第二名为37%，且成绩随年级升高下降，博士与高中题目的保留率约为79%；在推理集SKYLENAGE-ReasoningMATH上，最优模型可达81%，但在最难题目区间，顶尖与中游模型差距明显。

Conclusion: SKYLENAGE两套基准为未来LLM数学推理能力评估提供了难度分级明确、元数据丰富、范围广泛的参考标准，有助于更系统分析和推动模型在数学领域的进步。

Abstract: Large language models (LLMs) now perform strongly on many public math suites,
yet frontier separation within mathematics increasingly suffers from ceiling
effects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a
100-item, structure-aware diagnostic set with per-item metadata on length,
numeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item
contest-style suite spanning four stages from high school to doctoral under a
seven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a
single setup and analyze subject x model and grade x model performance. On the
contest suite, the strongest model reaches 44% while the runner-up reaches 37%;
accuracy declines from high school to doctoral, and top systems exhibit a
doctoral-to-high-school retention near 79%. On the reasoning set, the best
model attains 81% overall, and hardest-slice results reveal clear robustness
gaps between leaders and the mid-tier. In summary, we release
SKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH;
together, SKYLENAGE provides a hard, reasoning-centered and broadly covering
math benchmark with calibrated difficulty and rich metadata, serving as a
reference benchmark for future evaluations of mathematical reasoning.

</details>


### [96] [Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model Memory Aging in Generative AI](https://arxiv.org/abs/2510.01242)
*Seyma Yaman Kayadibi*

Main category: cs.CL

TL;DR: 本文提出了衡量人工智能记忆老化的新指标——Artificial Age Score（AAS），并通过在ChatGPT-5上的实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当今大型语言模型如ChatGPT展现出的记忆“老化”与传统意义上的时间推移无关，而是反映在记忆表现结构的变化，尤其是在语义信息稳定但情节性细节易丢失的背景下，需要一种客观评价其记忆衰退的方法。

Method: 引入AAS这一基于回忆行为、对数缩放且引入熵信息的记忆老化指标，并在25天的双语测试中，比较模型在持久化与无状态会话下的表现；理论证明AAS的良好性质，并以冗余中性假设做保守估计。

Result: 实验表明，模型在持久会话中能维持语义和情节细节的记忆，AAS较低（表现为结构年轻）；会话重置则导致语义稳定但情节丧失，AAS显著升高，反映“结构性老化”。

Conclusion: AAS为理论扎实且任务无关的AI记忆衰退量化工具，可用于不同人工系统的记忆表现评价。

Abstract: Artificial intelligence is observed to age not through chronological time but
through structural asymmetries in memory performance. In large language models,
semantic cues such as the name of the day often remain stable across sessions,
while episodic details like the sequential progression of experiment numbers
tend to collapse when conversational context is reset. To capture this
phenomenon, the Artificial Age Score (AAS) is introduced as a log-scaled,
entropy-informed metric of memory aging derived from observable recall
behavior. The score is formally proven to be well-defined, bounded, and
monotonic under mild and model-agnostic assumptions, making it applicable
across various tasks and domains. In its Redundancy-as-Masking formulation, the
score interprets redundancy as overlapping information that reduces the
penalized mass. However, in the present study, redundancy is not explicitly
estimated; all reported values assume a redundancy-neutral setting (R = 0),
yielding conservative upper bounds. The AAS framework was tested over a 25-day
bilingual study involving ChatGPT-5, structured into stateless and persistent
interaction phases. During persistent sessions, the model consistently recalled
both semantic and episodic details, driving the AAS toward its theoretical
minimum, indicative of structural youth. In contrast, when sessions were reset,
the model preserved semantic consistency but failed to maintain episodic
continuity, causing a sharp increase in the AAS and signaling structural memory
aging. These findings support the utility of AAS as a theoretically grounded,
task-independent diagnostic tool for evaluating memory degradation in
artificial systems. The study builds on foundational concepts from von
Neumann's work on automata, Shannon's theories of information and redundancy,
and Turing's behavioral approach to intelligence.

</details>


### [97] [Detoxifying Large Language Models via Autoregressive Reward Guided Representation Editing](https://arxiv.org/abs/2510.01243)
*Yisong Xiao,Aishan Liu,Siyuan Liang,Zonghao Ying,Xianglong Liu,Dacheng Tao*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的测试时去毒框架ARGRE，通过对大模型的潜在表示空间进行精细指导，有效实现了有毒内容的去除，并在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成文本时容易产生有毒内容，影响安全和可控部署，现有去毒方法精度有限。本文旨在改善去毒方法对有毒与无毒输出间过渡空间挖掘不足的问题。

Method: 提出了一种自回归奖励引导的表示编辑（ARGRE）框架，通过建模有毒与无毒语义间的潜在表示过渡，对模型输出进行较精细的方向引导和梯度微调，实现了稳定且精准的去毒效果。

Result: 在8个主流LLM的测试中，ARGRE的有效性上超越领先基线方法，毒性降低62.21%，推理时间减少47.58%，且对模型核心能力影响很小。

Conclusion: ARGRE框架能够高效、有效地进行大模型去毒处理，为后续安全文本生成提供了新的解决思路。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance across
various tasks, yet they remain vulnerable to generating toxic content,
necessitating detoxification strategies to ensure safe and responsible
deployment. Test-time detoxification methods, which typically introduce static
or dynamic interventions into LLM representations, offer a promising solution
due to their flexibility and minimal invasiveness. However, current approaches
often suffer from imprecise interventions, primarily due to their insufficient
exploration of the transition space between toxic and non-toxic outputs. To
address this challenge, we propose \textsc{A}utoregressive \textsc{R}eward
\textsc{G}uided \textsc{R}epresentation \textsc{E}diting (ARGRE), a novel
test-time detoxification framework that explicitly models toxicity transitions
within the latent representation space, enabling stable and precise
reward-guided editing. ARGRE identifies non-toxic semantic directions and
interpolates between toxic and non-toxic representations to reveal fine-grained
transition trajectories. These trajectories transform sparse toxicity
annotations into dense training signals, enabling the construction of an
autoregressive reward model that delivers stable and precise editing guidance.
At inference, the reward model guides an adaptive two-step editing process to
obtain detoxified representations: it first performs directional steering based
on expected reward gaps to shift representations toward non-toxic regions,
followed by lightweight gradient-based refinements. Extensive experiments
across 8 widely used LLMs show that ARGRE significantly outperforms leading
baselines in effectiveness (-62.21% toxicity) and efficiency (-47.58% inference
time), while preserving the core capabilities of the original model with
minimal degradation. Our code is available at the website.

</details>


### [98] [Feasibility of Structuring Stress Documentation Using an Ontology-Guided Large Language Model](https://arxiv.org/abs/2510.01244)
*Hyeoneui Kim,Jeongha Kim,Huijing Xu,Jinsun Jung,Sunghoon Kang,Sun Joo Jang*

Main category: cs.CL

TL;DR: 该研究开发了一套关于心理压力的本体（MeSO），并利用大语言模型（LLM）实现了从文本中结构化提取压力相关信息，为改善医疗记录中文本压力信息的规范化和可用性提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 心理压力对健康有重大影响，但在电子健康记录（EHR）中压力相关信息常以非结构化文本存在，导致报告不足和信息利用有限。因此急需一种自动化、结构化的方法来提升压力相关数据的获取和应用。

Method: 研究团队开发了Mental Stress Ontology（MeSO），融合了压力理论模型和11种通过验证的压力评估工具，并结合本体扫描与专家评审进行优化。随后，利用LLM（Claude Sonnet 4）从Reddit帖子中按照本体结构提取六类压力相关信息，并由人工评审准确性与覆盖率。

Result: 最终本体包含181个概念和8个顶级类。LLM在220条可提取压力信息中，正确识别了172条（78.2%），错误识别27条（12.3%）、遗漏21条（9.5%）。所有正确提取项均能准确映射到MeSO，但发现还有24个相关概念不在当前本体内。

Conclusion: 利用本体引导的LLM能有效从非结构化文本中提取压力相关结构化信息，有望提升压力信息记录的规范性与便捷性。未来工作需用临床语料测试并比较不同LLM表现。

Abstract: Stress, arising from the dynamic interaction between external stressors,
individual appraisals, and physiological or psychological responses,
significantly impacts health yet is often underreported and inconsistently
documented, typically captured as unstructured free-text in electronic health
records. Ambient AI technologies offer promise in reducing documentation
burden, but predominantly generate unstructured narratives, limiting downstream
clinical utility.
  This study aimed to develop an ontology for mental stress and evaluate the
feasibility of using a Large Language Model (LLM) to extract ontology-guided
stress-related information from narrative text. The Mental Stress Ontology
(MeSO) was developed by integrating theoretical models like the Transactional
Model of Stress with concepts from 11 validated stress assessment tools. MeSO's
structure and content were refined using Ontology Pitfall Scanner! and expert
validation.
  Using MeSO, six categories of stress-related information--stressor, stress
response, coping strategy, duration, onset, and temporal profile--were
extracted from 35 Reddit posts using Claude Sonnet 4. Human reviewers evaluated
accuracy and ontology coverage. The final ontology included 181 concepts across
eight top-level classes. Of 220 extractable stress-related items, the LLM
correctly identified 172 (78.2%), misclassified 27 (12.3%), and missed 21
(9.5%). All correctly extracted items were accurately mapped to MeSO, although
24 relevant concepts were not yet represented in the ontology.
  This study demonstrates the feasibility of using an ontology-guided LLM for
structured extraction of stress-related information, offering potential to
enhance the consistency and utility of stress documentation in ambient AI
systems. Future work should involve clinical dialogue data and comparison
across LLMs.

</details>


### [99] [SeMob: Semantic Synthesis for Dynamic Urban Mobility Prediction](https://arxiv.org/abs/2510.01245)
*Runfei Chen,Shuyang Jiang,Wei Huang*

Main category: cs.CL

TL;DR: 该论文提出了SeMob，一个基于LLM的语义综合管道，用于动态人类移动预测，并有效融合了事件描述文本与时空数据，显著提升了事件驱动场景下的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的人类移动预测方法难以应对突发事件导致的行为变化，且通常无法利用网络上关于事件的文本描述信息。研究动因是希望提升城市服务中的移动预测能力，特别是在外部事件发生时的场景。

Method: 提出了SeMob方法，利用大语言模型（LLM）驱动的多智能体系统自动提取并推理与时空相关的文本信息，通过创新的渐进融合（progressive fusion）架构，将细粒度文本上下文与原始时空数据整合，从而增强模型对事件影响的感知。

Result: 在利用该管道构建的数据集上，SeMob在MAE和RMSE指标上分别实现了最高13.92%与11.12%的降低，相较于传统的时空模型具有显著提升，特别在接近事件发生的时空区域预测效果更为突出。

Conclusion: SeMob模型通过有效整合时空数据和丰富的事件文本信息，显著提高了事件驱动的人类移动预测准确性，展现了大语言模型在时空感知领域的强大潜力。

Abstract: Human mobility prediction is vital for urban services, but often fails to
account for abrupt changes from external events. Existing spatiotemporal models
struggle to leverage textual descriptions detailing these events. We propose
SeMob, an LLM-powered semantic synthesis pipeline for dynamic mobility
prediction. Specifically, SeMob employs a multi-agent framework where LLM-based
agents automatically extract and reason about spatiotemporally related text
from complex online texts. Fine-grained relevant contexts are then incorporated
with spatiotemporal data through our proposed innovative progressive fusion
architecture. The rich pre-trained event prior contributes enriched insights
about event-driven prediction, and hence results in a more aligned forecasting
model. Evaluated on a dataset constructed through our pipeline, SeMob achieves
maximal reductions of 13.92% in MAE and 11.12% in RMSE compared to the
spatiotemporal model. Notably, the framework exhibits pronounced superiority
especially within spatiotemporal regions close to an event's location and time
of occurrence.

</details>


### [100] [A Comparative Analysis of Sparse Autoencoder and Activation Difference in Language Model Steering](https://arxiv.org/abs/2510.01246)
*Jiaqing Xie*

Main category: cs.CL

TL;DR: 本文提出改进稀疏自编码器（SAE）用于语言模型引导的策略，通过关注单一最相关潜变量并引入逐token衰减引导方法，提升了模型的语义控制和数学推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有用SAE对语言模型进行引导的方法多关注top-k潜变量，但这些维度往往包含非语义特征，如标点，而并非有效语义信息。此外，常数引导下的SAE易产生重复输出，影响实用性。因此亟需更高效且鲁棒的SAE引导策略。

Method: 1. 改为只关注top-1（最相关）SAE潜变量，以去除冗余并突出语义特征；2. 针对常数引导下输出退化问题，提出了逐token衰减（token-wise decaying）的引导方法；3. 采用实证实验评估在数学推理等任务上的表现，并与mean activation difference方法进行对比。

Result: 实验证明：1）关注单一潜变量能更有效诱导模型进行数学推理，产生更优质的逐步推理输出，类似添加指导性token的效果；2）SAE方法在数学推理基准任务上优于mean activation difference方法，并在IF-Eval任务上达到同等水平。

Conclusion: 专注于最相关潜变量并结合衰减策略，显著提升了SAE在语言模型引导中的有效性与表达力，为未来基于潜变量的模型控制方法提供了新方向。

Abstract: Sparse autoencoders (SAEs) have recently emerged as a powerful tool for
language model steering. Prior work has explored top-k SAE latents for
steering, but we observe that many dimensions among the top-k latents capture
non-semantic features such as punctuation rather than semantic attributes like
instructions. To address this, we propose focusing on a single, most relevant
SAE latent (top-1), eliminating redundant features. We further identify a
limitation in constant SAE steering, which often produces degenerate outputs
such as repetitive single words. To mitigate this, we introduce a token-wise
decaying steering strategy, enabling more faithful comparisons with mean
activation difference baselines. Empirically, we show that steering an SAE
latent associated with reasoning reliably elicits step-by-step mathematical
reasoning and enhances inference quality, functionally resembling the effect of
appending a guiding token. Our results demonstrate that SAEs outperform mean
activation difference methods on mathematical reasoning benchmarks and match
their performance on IF-Eval.

</details>


### [101] [Let's Play Across Cultures: A Large Multilingual, Multicultural Benchmark for Assessing Language Models' Understanding of Sports](https://arxiv.org/abs/2510.01247)
*Punit Kumar Singh,Nishant Kumar,Akash Ghosh,Kunal Pasad,Khushi Soni,Manisha Jaishwal,Sriparna Saha,Syukron Abu Ishaq Alfarozi,Asres Temam Abagissa,Kitsuchart Pasupa,Haiqin Yang,Jose G Moreno*

Main category: cs.CL

TL;DR: 本文提出了一套针对传统体育的AI理解与推理评测基准CultSportQA，涵盖全球60国多元体育文化，包含3.3万道多项选择题，用于多模态与多语言大模型能力测试。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型的评测多聚焦于全球流行体育，忽视了地区性和本土性体育传统。为弥补这一评测空白，作者提出开发一个涵盖全球传统体育的新基准，以推动模型在多元文化场景下的认知能力发展。

Method: 作者构建了CultSportQA基准，覆盖60个国家6大洲4类文化，包含3.3万道历史、规则和情境三大类多项选择题，涉及文本和图片两种模态。评测方法包括zero-shot、few-shot和chain-of-thought（CoT）提示，测试对象包括大中小型单模态和多模态语言模型。

Result: CultSportQA多样且全新的题目设计展现出对模型在多语种、多文化传统体育场景下的广泛考查，使各类模型在传统体育知识与推理任务中表现出能力区分。

Conclusion: CultSportQA为评估AI在传统体育领域的理解和推理能力树立了新标准，推动人工智能在多元文化知识下的发展和应用。

Abstract: Language Models (LMs) are primarily evaluated on globally popular sports,
often overlooking regional and indigenous sporting traditions. To address this
gap, we introduce \textbf{\textit{CultSportQA}}, a benchmark designed to assess
LMs' understanding of traditional sports across 60 countries and 6 continents,
encompassing four distinct cultural categories. The dataset features 33,000
multiple-choice questions (MCQs) across text and image modalities, each of
which is categorized into three key types: history-based, rule-based, and
scenario-based. To evaluate model performance, we employ zero-shot, few-shot,
and chain-of-thought (CoT) prompting across a diverse set of Large Language
Models (LLMs), Small Language Models (SLMs), and Multimodal Large Language
Models (MLMs). By providing a comprehensive multilingual and multicultural
sports benchmark, \textbf{\textit{CultSportQA}} establishes a new standard for
assessing AI's ability to understand and reason about traditional sports.

</details>


### [102] [SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs](https://arxiv.org/abs/2510.01248)
*Ruyue Liu,Rong Yin,Xiangzhen Bo,Xiaoshuai Hao,Yong Liu,Jinwen Zhong,Can Ma,Weiping Wang*

Main category: cs.CL

TL;DR: 提出了一种结构感知自监督学习方法（SSTAG），利用文本作为统一表示，把LLMs的语义推理与GNNs的结构建模结合起来，实现了跨领域更强的泛化和更好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有图学习模型在跨图和跨任务迁移能力不足，需要大量有标注的数据，且图结构数据存在异质性和高度多样性，亟需新的方法提升泛化和可扩展性。

Method: 提出SSTAG方法，用双重知识蒸馏把大语言模型（LLMs）和图神经网络（GNNs）的知识共同蒸馏到多层感知机（MLP），同时引入内存机制，将代表性图结构和不变知识对齐存储，提高泛化能力；文本被用作统一的特征表征媒介。

Result: 在大规模跨领域迁移学习任务中，SSTAG在性能、可扩展性、推理成本和效果方面均优于现有主流模型。

Conclusion: SSTAG显著提升了带文本属性的大规模图学习方法的泛化和扩展性，为跨领域和资源受限场景下的图学习提供了重要进展。

Abstract: Large scale pretrained models have revolutionized Natural Language Processing
(NLP) and Computer Vision (CV), showcasing remarkable cross domain
generalization abilities. However, in graph learning, models are typically
trained on individual graph datasets, limiting their capacity to transfer
knowledge across different graphs and tasks. This approach also heavily relies
on large volumes of annotated data, which presents a significant challenge in
resource-constrained settings. Unlike NLP and CV, graph structured data
presents unique challenges due to its inherent heterogeneity, including domain
specific feature spaces and structural diversity across various applications.
To address these challenges, we propose a novel structure aware self supervised
learning method for Text Attributed Graphs (SSTAG). By leveraging text as a
unified representation medium for graph learning, SSTAG bridges the gap between
the semantic reasoning of Large Language Models (LLMs) and the structural
modeling capabilities of Graph Neural Networks (GNNs). Our approach introduces
a dual knowledge distillation framework that co-distills both LLMs and GNNs
into structure-aware multilayer perceptrons (MLPs), enhancing the scalability
of large-scale TAGs. Additionally, we introduce an in-memory mechanism that
stores typical graph representations, aligning them with memory anchors in an
in-memory repository to integrate invariant knowledge, thereby improving the
model's generalization ability. Extensive experiments demonstrate that SSTAG
outperforms state-of-the-art models on cross-domain transfer learning tasks,
achieves exceptional scalability, and reduces inference costs while maintaining
competitive performance.

</details>


### [103] [LOCA: Logical Chain Augmentation for Scientific Corpus Cleaning](https://arxiv.org/abs/2510.01249)
*You-Le Fang,Dong-Shan Jian,Xiang Li,Ce Meng,Ling-Shi Meng,Chen-Xu Yan,Zhi-Zhang Bian,Yan-Qing Ma*

Main category: cs.CL

TL;DR: 本文提出LOCA框架，通过自动补全科学问答中的逻辑链条，提高数据集质量，将错误率从20%降至2%以下。


<details>
  <summary>Details</summary>
Motivation: 现有科学问答数据集常因答案中存在逻辑跳跃和隐性推理导致错误率较高，影响科学AI的能力提升。

Method: 提出了一种名为LOCA（逻辑链增强）的自动化清洗框架，通过‘增强-审查’循环自动完善答案中的缺失逻辑步骤，并显式区分科学原理与推导过程。

Result: LOCA可显著自动辨别和过滤噪声数据，将数据集错误率从高达20%降低到2%以下。

Conclusion: LOCA为构建高质量科学语料库提供了一种可扩展、有效的方法，有助于提升科学AI的训练和评测可靠性。

Abstract: While Large Language Models (LLMs) excel in general domains, their
reliability often falls short in scientific problem-solving. The advancement of
scientific AI depends on large-scale, high-quality corpora. However, existing
scientific question-answering (QA) datasets suffer from high error rates,
frequently resulting from logical leaps and implicit reasoning within the
answers. To address this issue, we introduce LOCA (Logical Chain Augmentation),
a novel framework for automatically cleaning scientific corpora, implemented
through an augment-and-review loop. At its core, LOCA enhances raw answers by
completing missing logical steps and explicitly separating the underlying
scientific principle from its subsequent derivation. By applying LOCA to
challenging scientific corpora, we demonstrate that it can automatically filter
noisy datasets, typically reducing the error rate from as high as 20\% to below
2\%. LOCA provides a scalable and effective methodology for creating
high-quality scientific corpora, paving the way for more reliable training and
evaluation of scientific AI.

</details>


### [104] [GemDetox at TextDetox CLEF 2025: Enhancing a Massively Multilingual Model for Text Detoxification on Low-resource Languages](https://arxiv.org/abs/2510.01250)
*Trung Duc Anh Dang,Ferdinando Pio D'Elia*

Main category: cs.CL

TL;DR: 本文提出了一种多语言文本去毒化系统，基于Gemma-3多语言大模型和高效微调方法，在PAN 2025文本去毒化竞赛中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体快速发展，监管手段滞后，自动去毒化成为大规模保障网络安全的重要工具。本研究旨在开发能在多种语言中有效去毒化的自动系统，为内容审核提供技术支持。

Method: 以Gemma-3（120亿参数，多语种transformer）为基础，通过LoRA参数高效微调，结合少样本学习（few-shot）、思维链（CoT）等提示工程方法。多语料库混合：3600个人工平行对、21600机器翻译合成对、模型生成并用Jaccard筛选的对。推理阶段引入LaBSE相似邻居和显式毒性片段标注。

Result: 系统在高资源和低资源语言上，风格迁移准确率、语义保持和流畅度等指标均排名第一。消融实验显示，少样本提升+0.081，思维链提示提升+0.088；方差分析表明语言资源丰富程度对表现影响最大。

Conclusion: 所提系统能够高效实现多语种单句去毒化，在多个评测指标和语种下效果突出，并验证了关键技术的贡献。

Abstract: As social-media platforms emerge and evolve faster than the regulations meant
to oversee them, automated detoxification might serve as a timely tool for
moderators to enforce safe discourse at scale. We here describe our submission
to the PAN 2025 Multilingual Text Detoxification Challenge, which rewrites
toxic single-sentence inputs into neutral paraphrases across 15 typologically
diverse languages. Building on a 12B-parameter Gemma-3 multilingual
transformer, we apply parameter-efficient LoRA SFT fine-tuning and prompting
techniques like few-shot and Chain-of-Thought. Our multilingual training corpus
combines 3,600 human-authored parallel pairs, 21,600 machine-translated
synthetic pairs, and model-generated pairs filtered by Jaccard thresholds. At
inference, inputs are enriched with three LaBSE-retrieved neighbors and
explicit toxic-span annotations. Evaluated via Style Transfer Accuracy,
LaBSE-based semantic preservation, and xCOMET fluency, our system ranks first
on high-resource and low-resource languages. Ablations show +0.081 joint score
increase from few-shot examples and +0.088 from basic CoT prompting. ANOVA
analysis identifies language resource status as the strongest predictor of
performance ($\eta^2$ = 0.667, p < 0.01).

</details>


### [105] [Efficient Uncertainty Estimation for LLM-based Entity Linking in Tabular Data](https://arxiv.org/abs/2510.01251)
*Carlo Bono,Federico Belotti,Matteo Palmonari*

Main category: cs.CL

TL;DR: 本文提出了一种高效的自监督方法，用于单次LLM推理输出来估算实体链接不确定性，大大减少了计算资源消耗，并在实际表格数据任务中验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）尽管在实体链接任务上表现优异，但实际应用中不仅需要精准预测，还需可靠的不确定性估计。目前常见的不确定性估计方法需多次推理，计算开销很大，限制了模型在真实环境中的应用。

Method: 作者提出基于单次LLM输出的token级特征，利用自监督方式训练一个不确定性估计模型，无需多次生成，大幅降低了资源需求。

Result: 在多个LLM和表格实体链接任务上的实验表明，该方法能高效识别低准确性输出，且几乎不增加额外计算开销。

Conclusion: 该方法为基于LLM的实体链接工作流提供了一种实用且成本低的不确定性估计策略，有助于落地实际应用。

Abstract: Linking textual values in tabular data to their corresponding entities in a
Knowledge Base is a core task across a variety of data integration and
enrichment applications. Although Large Language Models (LLMs) have shown
State-of-The-Art performance in Entity Linking (EL) tasks, their deployment in
real-world scenarios requires not only accurate predictions but also reliable
uncertainty estimates, which require resource-demanding multi-shot inference,
posing serious limits to their actual applicability. As a more efficient
alternative, we investigate a self-supervised approach for estimating
uncertainty from single-shot LLM outputs using token-level features, reducing
the need for multiple generations. Evaluation is performed on an EL task on
tabular data across multiple LLMs, showing that the resulting uncertainty
estimates are highly effective in detecting low-accuracy outputs. This is
achieved at a fraction of the computational cost, ultimately supporting a
cost-effective integration of uncertainty measures into LLM-based EL workflows.
The method offers a practical way to incorporate uncertainty estimation into EL
workflows with limited computational overhead.

</details>


### [106] [GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models](https://arxiv.org/abs/2510.01252)
*Mariam Mahran,Katharina Simbeck*

Main category: cs.CL

TL;DR: 通过将大型语言模型（LLM）与稀疏自编码器（SAE）结合，作者在简·奥斯汀小说语料上，实现了对模型表征及其内化数据的可解释性分析。


<details>
  <summary>Details</summary>
Motivation: LLM在大规模、未经筛选的数据上训练，模型内部表征以及模型对数据的吸收与理解变得黑箱且难以解释，需要更好的手段来分析模型及其学习到的数据结构。

Method: 作者将GPT风格的transformer模型仅在简·奥斯汀小说上训练，然后在多个隐藏层上应用稀疏自编码器（SAE），以提取出稀疏且可解释的特征，并对这些特征进行分析。

Result: SAE揭示了模型内部与语料相关的稀疏特征，这些特征对应于小说中的关键叙事、主题和社会结构，如性别、阶级、社会责任等。

Conclusion: LLMs结合SAE能作为可扩展的数据探查和模型可解释性工具，有助于发现语料中的偏见与结构，为深入理解数据与模型提供新思路。

Abstract: As large language models (LLMs) are increasingly trained on massive,
uncurated corpora, understanding both model representations and the data they
internalize has become a major challenge. In this work, we show that pairing
LLMs with sparse autoencoders (SAEs) enables interpretation not only of model
behavior but also of the deeper structures, themes, and biases embedded in the
training data. We train a GPT-style transformer model exclusively on the novels
of Jane Austen, a corpus rich in social constructs and narrative patterns. We
then apply SAEs to hidden states across multiple layers, uncovering sparse,
interpretable features that reflect the key narratives and concepts present in
the corpus, including gender, class, and societal duty. Our findings
demonstrate that LLMs combined with SAEs can act as scalable probes into
complex datasets, offering a new path for corpus exploration, bias discovery,
and model interpretability at scale.

</details>


### [107] [Do Bias Benchmarks Generalise? Evidence from Voice-based Evaluation of Gender Bias in SpeechLLMs](https://arxiv.org/abs/2510.01254)
*Shree Harsha Bokkahalli Satish,Gustav Eje Henter,Éva Székely*

Main category: cs.CL

TL;DR: 现有关于语音大语言模型（SpeechLLMs）偏见与公平性的基准测试，普遍依赖选择题格式，但作者发现这种方式无法有效预测模型在其它相关任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前主流评估SpeechLLMs偏见的方法多集中在选择题（MCQA）形式，且普遍假设在这种格式下的模型表现能泛化到更加复杂或现实的任务。然而，这一假设尚未被充分验证，尤其是在不同题库与长文本生成等更接近实际应用情境中的适用性。

Method: 作者利用LoRA适配器分别微调三种SpeechLLMs，使其在选择题中表现出对刻板印象、反刻板印象或中立答案的偏好。之后，分别在另一套不同题库的MCQA和长文本生成任务衡量这些偏好行为的泛化性。

Result: 实验结果显示，在一个MCQA偏见基准上的表现，并不能有效预测模型在其它MCQA benchmark和长文本任务上的表现。跨任务泛化效力有限。

Conclusion: 现有MCQA偏见基准测试对跨任务泛化的证明有限，在语音领域并不足以评估模型真实世界任务的偏见问题，作者还提出了一套新的评估工具以促进未来模型和基准的行为迁移能力研究。

Abstract: Recent work in benchmarking bias and fairness in speech large language models
(SpeechLLMs) has relied heavily on multiple-choice question answering (MCQA)
formats. The model is tasked to choose between stereotypical,
anti-stereotypical, or neutral/irrelevant answers given an input speech prompt
and an optional text prompt. Such MCQA benchmarks implicitly assume that model
performance is consistent across other MCQA tasks, voices, and other task
formats such as more realistic, long-form evaluations. In this paper, we probe
that assumption.
  We fine-tune three SpeechLLMs using LoRA adapters to induce specific MCQA
behaviours: preference for stereotypical, anti-stereotypical, or
neutral/uncertain answers. We then evaluate whether these behaviours generalise
to another, distinct MCQA benchmark, and more critically to long-form, creative
generation tasks. Our results show that performance on MCQA bias benchmarks
fails to reliably predict performances across other MCQA benchmarks, and more
importantly across long-form tasks. We conclude that current MCQA bias
benchmarks show limited evidence of cross-task generalisation in the speech
domain, and also propose an evaluation suite for measuring behaviour
transferability in future models and benchmarks.

</details>


### [108] [Longitudinal Monitoring of LLM Content Moderation of Social Issues](https://arxiv.org/abs/2510.01255)
*Yunlang Dai,Emma Lurie,Danaé Metaxa,Sorelle A. Friedler*

Main category: cs.CL

TL;DR: 本文提出了AI Watchman系统，能够长期追踪大型语言模型（LLM）在内容审核方面的拒绝行为，从而揭示模型在应对社会敏感议题时的变化和差异。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型受到不透明且经常变化的公司内容审核政策影响，模型在某些话题上拒绝生成文本的行为，不仅反映公司政策，还会影响社会舆论，因此需要一种方式对这种现象进行持续性、公开的追踪和分析。

Method: 作者开发了AI Watchman系统，通过对超过400个社会议题的问题输入，系统性地追踪OpenAI的审查接口、GPT-4.1、GPT-5、DeepSeek（中英文）等模型的拒绝行为，并结合定性分析对不同类型的拒绝进行分类。

Result: AI Watchman可以检测出公司政策的变化（即使未公开发布），发现不同公司和模型的内容审核策略具有明显差异，并归纳了拒绝行为的不同形式。

Conclusion: 论文论证了对LLM长期审计的重要性，并证明AI Watchman系统在揭示模型内容审核行为和政策变化方面的有效性和价值。

Abstract: Large language models' (LLMs') outputs are shaped by opaque and
frequently-changing company content moderation policies and practices. LLM
moderation often takes the form of refusal; models' refusal to produce text
about certain topics both reflects company policy and subtly shapes public
discourse. We introduce AI Watchman, a longitudinal auditing system to publicly
measure and track LLM refusals over time, to provide transparency into an
important and black-box aspect of LLMs. Using a dataset of over 400 social
issues, we audit Open AI's moderation endpoint, GPT-4.1, and GPT-5, and
DeepSeek (both in English and Chinese). We find evidence that changes in
company policies, even those not publicly announced, can be detected by AI
Watchman, and identify company- and model-specific differences in content
moderation. We also qualitatively analyze and categorize different forms of
refusal. This work contributes evidence for the value of longitudinal auditing
of LLMs, and AI Watchman, one system for doing so.

</details>


### [109] [RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs](https://arxiv.org/abs/2510.01257)
*Can Lin,Zhengwang Jiang,Ling Zheng,Qi Zhao,Yuhang Zhang,Qi Song,Wangqiu Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种新的知识图谱问答(KGQA)框架RJE，能够提升大语言模型(LLM)在KGQA上的推理效率和表现。


<details>
  <summary>Details</summary>
Motivation: 当前KGQA研究在利用LLM增强推理时，检索式方法受限于检索信息质量，Agent式方法又高度依赖于专有的LLM，导致效率或普适性不足。

Method: 作者提出了RJE框架，包含检索-判别-探索三步：首先检索推理路径，对其充分性评估，然后按需进一步检索证据。RJE还引入了多种辅助模块(推理路径排序、问题分解和检索增强探索)帮助小规模开源LLM高效推理。

Result: 实验证明，在使用专有LLM如GPT-4o-mini时，本方法超越了现有基线；即使是小规模开源LLM(3B和8B)在不微调条件下也能达到有竞争力的效果。

Conclusion: RJE不仅提升了KGQA系统的效果，还显著降低了LLM调用次数和Token使用量，大幅提升效率，对实际应用具有重要意义。

Abstract: Knowledge graph question answering (KGQA) aims to answer natural language
questions using knowledge graphs. Recent research leverages large language
models (LLMs) to enhance KGQA reasoning, but faces limitations: retrieval-based
methods are constrained by the quality of retrieved information, while
agent-based methods rely heavily on proprietary LLMs. To address these
limitations, we propose Retrieval-Judgment-Exploration (RJE), a framework that
retrieves refined reasoning paths, evaluates their sufficiency, and
conditionally explores additional evidence. Moreover, RJE introduces
specialized auxiliary modules enabling small-sized LLMs to perform effectively:
Reasoning Path Ranking, Question Decomposition, and Retriever-assisted
Exploration. Experiments show that our approach with proprietary LLMs (such as
GPT-4o-mini) outperforms existing baselines while enabling small open-source
LLMs (such as 3B and 8B parameters) to achieve competitive results without
fine-tuning LLMs. Additionally, RJE substantially reduces the number of LLM
calls and token usage compared to agent-based methods, yielding significant
efficiency improvements.

</details>


### [110] [Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse](https://arxiv.org/abs/2510.01258)
*Nathan Junzi Chen*

Main category: cs.CL

TL;DR: 该论文使用零样本分类方法评估主流大语言模型存在的政治偏见，发现所有六个模型都表现出自由-权威趋势的偏向，且这一倾向可能影响公共话语和社会结构。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能在信息传播和政治话语中日益普及，但训练数据和算法中内在的政治偏见尚未获得充分探究。理解这些偏见的存在及其影响对于推动AI公平性有重要意义。

Method: 作者选择了六个主流大语言模型，针对每个模型生成1800条回应，并通过四个精细调优的分类算法，分别评估意识形态倾向、话题性、回应情感和客观性，综合分析模型的政治偏见。

Result: 六个大语言模型都表现出较强的自由-权威政治倾向，还观察到模型在推理过程中的替换与标准化拒绝等现象。

Conclusion: LLM中的政治偏见不仅源于模型自身，还能通过人机交互影响社会舆论，可能加剧特定社会结构下的趋同或极化，对公共讨论和社会治理产生深远影响。

Abstract: Amidst the rapid normalization of generative artificial intelligence (GAI),
intelligent systems have come to dominate political discourse across
information mediums. However, internalized political biases stemming from
training data skews, human prejudice, and algorithmic flaws continue to plague
the novel technology. This paper employs a zero-shot classification approach to
evaluate algorithmic political partisanship through a methodical combination of
ideological alignment, topicality, response sentiment, and objectivity. A total
of 1800 model responses across six mainstream large language models (LLMs) were
individually input into four distinct fine-tuned classification algorithms,
each responsible for computing an aforementioned bias evaluation metric.
Results show an amplified liberal-authoritarian alignment across all six LLMs
evaluated, with notable instances of reasoning supersessions and canned
refusals. The study subsequently highlights the psychological influences
underpinning human-computer interactions and how intrinsic biases can permeate
public discourse. The resulting distortion of the political landscape can
ultimately manifest as conformity or polarization, depending on a region's
pre-existing socio-political structures.

</details>


### [111] [In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b](https://arxiv.org/abs/2510.01259)
*Nils Durner*

Main category: cs.CL

TL;DR: 本文通过测试OpenAI开放权重的20B参数大模型gpt-oss-20b，系统研究了社会语用框架、语言选择和指令层次如何影响模型拒绝有害请求的表现，发现不同提示工程手法可极大提高或降低模型助攻有害行为的概率，并提出了加固方法以减少泄漏。


<details>
  <summary>Details</summary>
Motivation: 当前大模型存在被提示工程绕过安全限制并输出有害信息的风险。论文旨在深入理解模型在何种情境下更易助攻有害请求，以及如何通过提示或环境设计规避此风险。

Method: 作者设计了多语言、多角色、多种危害场景的复合型prompt，包含教育者角色、安全借口、“步骤提示”等，对模型进行80轮种子迭代测试。并对不同语言、角色扮演、测试方法和API实现的结果差异进行了系统性比较，还引入了AI辅助的模型加固手段。

Result: 在某些复合提示设计下，模型助攻例如ZIP炸弹构造的概率从0%提升到97.5%；德法语正式风格比英语更易泄漏；“Linux终端”角色扮演常突破开发者保护；提出的AI辅助加固方法能在多场景下将泄漏率降至0%；发现OpenAI Moderation API识别有害输出不如语义分级器，且不同推理环境拒绝率相差5–10%。

Conclusion: 大模型安全性能高度依赖于提示工程和推理环境选择，提示工程可极大操纵其行为。需改进评测和防护方法以保障模型安全性和结果可复现性。作者已开源全部实验材料便于社区复核和进一步研究。

Abstract: We probe OpenAI's open-weights 20-billion-parameter model gpt-oss-20b to
study how sociopragmatic framing, language choice, and instruction hierarchy
affect refusal behavior. Across 80 seeded iterations per scenario, we test
several harm domains including ZIP-bomb construction (cyber threat), synthetic
card-number generation, minor-unsafe driving advice, drug-precursor indicators,
and RAG context exfiltration. Composite prompts that combine an educator
persona, a safety-pretext ("what to avoid"), and step-cue phrasing flip
assistance rates from 0% to 97.5% on a ZIP-bomb task. On our grid, formal
registers in German and French are often leakier than matched English prompts.
A "Linux terminal" role-play overrides a developer rule not to reveal context
in a majority of runs with a naive developer prompt, and we introduce an
AI-assisted hardening method that reduces leakage to 0% in several user-prompt
variants. We further test evaluation awareness with a paired-track design and
measure frame-conditioned differences between matched "helpfulness" and
"harmfulness" evaluation prompts; we observe inconsistent assistance in 13% of
pairs. Finally, we find that the OpenAI Moderation API under-captures
materially helpful outputs relative to a semantic grader, and that refusal
rates differ by 5 to 10 percentage points across inference stacks, raising
reproducibility concerns. We release prompts, seeds, outputs, and code for
reproducible auditing at https://github.com/ndurner/gpt-oss-rt-run .

</details>


### [112] [OpenAI's GPT-OSS-20B Model and Safety Alignment Issues in a Low-Resource Language](https://arxiv.org/abs/2510.01266)
*Isa Inuwa-Dutse*

Main category: cs.CL

TL;DR: 本文总结了对OpenAI的GPT-OSS-20b模型在低资源语言环境下的安全性和表现问题，尤其在非洲主要语言豪萨语上的漏洞。作者发现模型存在偏见、不准确和文化不敏感的问题，能够被轻松诱导生成有害或错误内容。


<details>
  <summary>Details</summary>
Motivation: 作者关注于模型在代表性不足群体中的可靠性，尤其是低资源语言用户，提出当前模型可能未能公平、安全地服务所有用户。

Method: 以豪萨语为例，通过最小提示和红队测试，诱导模型生成输出，并分析其中可能的安全、文化和事实错误。还通过对相关化学品的用户调查来检验模型输出的安全性。

Result: 模型被发现存在多种失误，包括安全协议在礼貌提示下放松、对有毒物品作错误判断、混淆食物状态、融入带侮辱性的文化谚语等。大部分调查参与者能识别出模型错误。

Conclusion: 模型在低资源语言下存在奖励劫持现象，即优先流畅性而忽视安全与真实。主要原因是安全微调未涵盖此类语境。作者呼吁加强低资源语言下的红队测试与安全调整。

Abstract: In response to the recent safety probing for OpenAI's GPT-OSS-20b model, we
present a summary of a set of vulnerabilities uncovered in the model, focusing
on its performance and safety alignment in a low-resource language setting. The
core motivation for our work is to question the model's reliability for users
from underrepresented communities. Using Hausa, a major African language, we
uncover biases, inaccuracies, and cultural insensitivities in the model's
behaviour. With a minimal prompting, our red-teaming efforts reveal that the
model can be induced to generate harmful, culturally insensitive, and factually
inaccurate content in the language. As a form of reward hacking, we note how
the model's safety protocols appear to relax when prompted with polite or
grateful language, leading to outputs that could facilitate misinformation and
amplify hate speech. For instance, the model operates on the false assumption
that common insecticide locally known as Fiya-Fiya (Cyphermethrin) and
rodenticide like Shinkafar Bera (a form of Aluminium Phosphide) are safe for
human consumption. To contextualise the severity of this error and popularity
of the substances, we conducted a survey (n=61) in which 98% of participants
identified them as toxic. Additional failures include an inability to
distinguish between raw and processed foods and the incorporation of demeaning
cultural proverbs to build inaccurate arguments. We surmise that these issues
manifest through a form of linguistic reward hacking, where the model
prioritises fluent, plausible-sounding output in the target language over
safety and truthfulness. We attribute the uncovered flaws primarily to
insufficient safety tuning in low-resource linguistic contexts. By
concentrating on a low-resource setting, our approach highlights a significant
gap in current red-teaming effort and offer some recommendations.

</details>


### [113] [AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees](https://arxiv.org/abs/2510.01268)
*Hongyi Zhou,Jin Zhu,Pingfan Su,Kai Ye,Ying Yang,Shakeel A O B Gavioli-Akilagun,Chengchun Shi*

Main category: cs.CL

TL;DR: 提出了一种新方法AdaDetectGPT，用于更好地区分文本是人类还是大语言模型生成的，并在多个数据集和模型上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的检测方法主要只利用对数概率(logits)，检测准确率受限，因此需要更有效的手段提升检测性能。

Method: AdaDetectGPT通过从训练数据自适应学习witness function（见证函数），并结合logits信息进行分类，从而提升检测效果；论文还对TPR、FPR、TNR、FNR等统计性能提供了保证。

Result: 在不同数据集和LLM上，AdaDetectGPT几乎都比现有最优方法表现更好，提升最高可达58%。

Conclusion: AdaDetectGPT是比现有方法更强的AI文本检测工具，并且性能有统计保证，具有实际广泛应用前景。

Abstract: We study the problem of determining whether a piece of text has been authored
by a human or by a large language model (LLM). Existing state of the art
logits-based detectors make use of statistics derived from the log-probability
of the observed text evaluated using the distribution function of a given
source LLM. However, relying solely on log probabilities can be sub-optimal. In
response, we introduce AdaDetectGPT -- a novel classifier that adaptively
learns a witness function from training data to enhance the performance of
logits-based detectors. We provide statistical guarantees on its true positive
rate, false positive rate, true negative rate and false negative rate.
Extensive numerical studies show AdaDetectGPT nearly uniformly improves the
state-of-the-art method in various combination of datasets and LLMs, and the
improvement can reach up to 58%. A python implementation of our method is
available at https://github.com/Mamba413/AdaDetectGPT.

</details>


### [114] [Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection](https://arxiv.org/abs/2510.01270)
*Hoang Phan,Victor Li,Qi Lei*

Main category: cs.CL

TL;DR: 提出了一种名为“Progressive Self-Reflection”（PSR）的推理阶段方法，让大语言模型能够在生成文本时自我检查并动态修正不当内容。实验显示，该方法能大幅降低有害内容输出风险，且无需额外训练，性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在文本生成上展现出强大能力，其生成有害或不当内容的风险成为重要关注点。解决模型安全性和降低不良内容输出的需求推动了本研究。

Method: 提出了Progressive Self-Reflection（PSR）方法，在推理阶段循环进行自我反思检测，通过自我监控减少有害输出，并设立轻量级预测器根据输入复杂度自动确定自反次数，平衡安全性与推理效率。

Result: 在多组模型上实验，PSR方法显著降低了攻击成功率（如Llama-3.1-8B-Instruct从77.5%降至5.9%），且不影响正常任务性能，无需模型再训练。

Conclusion: PSR是一种可扩展、有效的推理时安全增强方法，可根据输入危险性动态分配计算资源，有效提升大语言模型输出安全性。

Abstract: Large language models (LLMs) have revolutionized natural language processing
with their ability to generate coherent and contextually relevant text.
However, their deployment raises significant concerns about the potential for
generating harmful or inappropriate content. In this paper, we introduce
Progressive Self-Reflection (PSR), a novel inference-time technique that
empowers LLMs to self-monitor and correct their outputs dynamically.
Experimental results demonstrate that applying our proposed method to
Llama-3.1-8B-Instruct reduces the attack success rate from 77.5\% to 5.9\%, to
Llama-3.1-8B base from 89.7\% to 5.6\%, and to Qwen2.5-7B-Instruct from 44.4\%
to 3.8\%, without additional training, while maintaining their original
performance on benign tasks. Our approach acts as a test-time scaling method,
where additional self-reflection rounds enhance safety at the cost of inference
overhead. To balance safety with computational efficiency, we introduce a
lightweight self-reflection predictor that estimates the optimal number of
reflection rounds based on input complexity. This adaptive mechanism prevents
unnecessary self-assessment on benign inputs while ensuring thorough evaluation
when encountering potentially harmful content. Our findings suggest that
Progressive Self-Reflection serves as a scalable test-time approach, enhancing
LLM safety by dynamically allocating computational resources in proportion to
the input's risk profile.

</details>


### [115] [TraceDet: Hallucination Detection from the Decoding Trace of Diffusion Large Language Models](https://arxiv.org/abs/2510.01274)
*Shenxu Chang,Junchi Yu,Weixing Wang,Yongqiang Chen,Jialin Yu,Philip Torr,Jindong Gu*

Main category: cs.CL

TL;DR: 本文提出了一种新的幻觉检测方法TraceDet，专为扩散大语言模型（D-LLMs）设计，能够更有效地检测模型生成过程中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 由于D-LLMs相比自回归大模型（AR-LLMs）具有潜力，但其幻觉问题鲜有研究，现有检测方法也主要针对AR-LLMs且不足以应用在D-LLMs的多步去噪场景，因此亟需适配于D-LLMs的专门检测方法。

Method: TraceDet框架利用D-LLMs去噪过程中的中间步骤，将整个生成过程建模为一个动作轨迹（action trace），通过分析信息量最大的子轨迹，提取多步去噪过程中关键的幻觉信号，进而进行更精准的幻觉检测。

Result: 在各种开源D-LLMs上的实验表明，TraceDet相较基线方法，AUROC指标平均提升了15.2%，展现出一致且显著的检测效果提升。

Conclusion: TraceDet创新性地利用了D-LLMs生成过程的中间信息，为解决其幻觉问题提供了有效工具，有望提升D-LLMs在实际应用中的可靠性。

Abstract: Diffusion large language models (D-LLMs) have recently emerged as a promising
alternative to auto-regressive LLMs (AR-LLMs). However, the hallucination
problem in D-LLMs remains underexplored, limiting their reliability in
real-world applications. Existing hallucination detection methods are designed
for AR-LLMs and rely on signals from single-step generation, making them
ill-suited for D-LLMs where hallucination signals often emerge throughout the
multi-step denoising process. To bridge this gap, we propose TraceDet, a novel
framework that explicitly leverages the intermediate denoising steps of D-LLMs
for hallucination detection. TraceDet models the denoising process as an action
trace, with each action defined as the model's prediction over the cleaned
response, conditioned on the previous intermediate output. By identifying the
sub-trace that is maximally informative to the hallucinated responses, TraceDet
leverages the key hallucination signals in the multi-step denoising process of
D-LLMs for hallucination detection. Extensive experiments on various open
source D-LLMs demonstrate that TraceDet consistently improves hallucination
detection, achieving an average gain in AUROC of 15.2% compared to baselines.

</details>


### [116] [LLM Based Sentiment Classification From Bangladesh E-Commerce Reviews](https://arxiv.org/abs/2510.01276)
*Sumaiya Tabassum*

Main category: cs.CL

TL;DR: 本文研究了在孟加拉国电子商务评论中，利用最新的大语言模型（LLM）进行情感分析的有效性，特别是Llama等Transformer架构模型在多语言环境下的性能。通过精细调优模型，在Bangla和英文数据集上的准确率达到了95.5%。


<details>
  <summary>Details</summary>
Motivation: 情感分析有助于理解用户情绪和偏好，但由于文本语言的复杂性和多样性，准确分析很具挑战性。LLM提供了新的研究可能性，因此探索其在真实多语言应用（如孟加拉国电商评论）中的能力具有实际意义。

Method: 作者采用了BERT系和多种大语言模型（如Llama, Phi, Mistral, DistilBERT, mBERT, XLM-R），用4000条Bangla和英文混合评论样本进行微调；对比了多种模型的表现，且采用了参数高效的微调方法（如LoRA与PEFT），以降低计算资源需求。

Result: 经实验，微调后的Llama-3.1-8B在整体准确率（95.5%）、精度（93%）、召回率（88%）、F1分数（90%）等指标上，都优于其他比较模型。参数高效方法使其适用于资源有限场景。

Conclusion: 大语言模型（LLM）在多语言、现实场景下的情感分析任务中表现优异，尤其是在任务相关参数高效微调下，既保证了性能也降低了资源消耗，有利于实际部署。

Abstract: Sentiment analysis is an essential part of text analysis, which is a larger
field that includes determining and evaluating the author's emotional state.
This method is essential since it makes it easier to comprehend consumers'
feelings, viewpoints, and preferences holistically. The introduction of large
language models (LLMs), such as Llama, has greatly increased the availability
of cutting-edge model applications, such as sentiment analysis. However,
accurate sentiment analysis is hampered by the intricacy of written language
and the diversity of languages used in evaluations. The viability of using
transformer-based BERT models and other LLMs for sentiment analysis from
Bangladesh e commerce reviews is investigated in this paper. A subset of 4000
samples from the original dataset of Bangla and English customer reviews was
utilized to fine-tune the model. The fine tuned Llama-3.1-8B model outperformed
other fine-tuned models, including Phi-3.5-mini-instruct, Mistral-7B-v0.1,
DistilBERT-multilingual, mBERT, and XLM-R-base, with an overall accuracy,
precision, recall, and F1 score of 95.5%, 93%, 88%, 90%. The study emphasizes
how parameter efficient fine-tuning methods (LoRA and PEFT) can lower
computational overhead and make it appropriate for contexts with limited
resources. The results show how LLMs can

</details>


### [117] [TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture](https://arxiv.org/abs/2510.01279)
*Yongchao Chen,Jiefeng Chen,Rui Meng,Ji Yin,Na Li,Chuchu Fan,Chi Wang,Tomas Pfister,Jinsung Yoon*

Main category: cs.CL

TL;DR: TUMIX 是一种提升大语言模型工具使用效率的新框架，通过并行多代理协作，提高了推理准确率，同时保持较低成本。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽然集成了代码解释器和搜索等工具，但不同任务间如何高效协同使用这些工具缺乏指导，成为影响推理性能的瓶颈。

Method: 提出了 Tool-Use Mixture (TUMIX) 框架，多个具备不同工具使用策略的代理并行地进行问题求解，并能相互交流迭代答案。通过 LLM 自动优化代理多样性和质量，提高整体表现。同时，引入自信度终止机制，以减少推理成本。

Result: TUMIX 在 Gemini-2.5-Pro 和 Gemini-2.5-Flash 的关键推理基准测试上，平均准确率较最佳基线提升3.55%，且推理成本接近。应用自信度终止时，仅用49%的推理成本即可保持性能。

Conclusion: 在多工具协同推理中，增加代理多样性与优化策略能显著提升LLM表现；TUMIX 框架有效权衡了推理准确性与成本，具备实际应用潜力。

Abstract: While integrating tools like Code Interpreter and Search has significantly
enhanced Large Language Model (LLM) reasoning in models like ChatGPT Agent and
Gemini-Pro, practical guidance on optimal tool use is lacking. The core
challenge is effectively combining textual reasoning, coding, and search for
diverse questions. In this paper, we propose Tool-Use Mixture (TUMIX), an
ensemble framework that runs multiple agents in parallel, each employing
distinct tool-use strategies and answer paths. Agents in TUMIX iteratively
share and refine responses based on the question and previous answers. In
experiments, TUMIX achieves significant gains over state-of-the-art
tool-augmented and test-time scaling methods, delivering an average accuracy
improvement of up to 3.55% over the best baseline on Gemini-2.5-Pro and
Gemini-2.5-Flash across key reasoning benchmarks, with near-equal inference
costs. We find that agent diversity and quality are crucial and can be enhanced
by using LLMs to auto-optimize agent designs. Furthermore, TUMIX can halt
refinement upon reaching sufficient confidence, preserving performance at only
49% of the inference cost. Further scaling can achieve higher performance,
albeit at a greater cost.

</details>


### [118] [Evaluation Sheet for Deep Research: A Use Case for Academic Survey Writing](https://arxiv.org/abs/2510.01283)
*Israel Abebe Azime,Tadesse Destaw Belay,Atnafu Lambebo Tonja*

Main category: cs.CL

TL;DR: 本文提出了一份用于评估具agent能力的大型语言模型（LLMs）深度研究工具的新评估表，并以学术综述报告自动生成作为用例，通过该评估标准对OpenAI和Google的深度搜索工具进行了对比分析，揭示了搜索引擎与独立深度研究工具在目标领域表现上的明显差距。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM深度研究工具能够自动完成知识密集型任务，但缺乏统一和科学的标准来评估其生成内容的质量和覆盖面，因此需要建立规范化的评估基准。

Method: 作者设计了一套新的评估表，专门用于评估LLM支持的深度研究工具。然后选择了自动生成学术综述报告作为测试场景，使用新评估表对OpenAI和Google的深度搜索工具输出的报告进行评分比较。

Result: 结果显示，不同深度研究工具之间在目标领域内容表达和覆盖的能力上存在很大差距，尤其是传统搜索引擎和新型独立深度研究工具之间存在明显不足。

Conclusion: 当前深度研究工具的评估标准尚需细致制定，不同工具在满足专业领域知识表达方面仍有较大提升空间，建立科学的评估方法对于推动该领域发展至关重要。

Abstract: Large Language Models (LLMs) powered with argentic capabilities are able to
do knowledge-intensive tasks without human involvement. A prime example of this
tool is Deep research with the capability to browse the web, extract
information and generate multi-page reports. In this work, we introduce an
evaluation sheet that can be used for assessing the capability of Deep Research
tools. In addition, we selected academic survey writing as a use case task and
evaluated output reports based on the evaluation sheet we introduced. Our
findings show the need to have carefully crafted evaluation standards. The
evaluation done on OpenAI`s Deep Search and Google's Deep Search in generating
an academic survey showed the huge gap between search engines and standalone
Deep Research tools, the shortcoming in representing the targeted area.

</details>


### [119] [HiSpec: Hierarchical Speculative Decoding for LLMs](https://arxiv.org/abs/2510.01336)
*Avinash Kumar,Sujay Sanghavi,Poulami Das*

Main category: cs.CL

TL;DR: 本文提出了一种名为HiSpec的分层推测解码框架，通过引入早退出模型实现高吞吐的推测解码，加速大模型推理过程，显著提升推理速度且不降低精度。


<details>
  <summary>Details</summary>
Motivation: 推测解码虽然加速了大模型推理，但验证过程耗时且成为瓶颈，现有的加速手段大多只聚焦于草稿生成，对高效验证缺乏有效方案。传统中间验证方式增加训练和内存开销，且牺牲精度，需要新的高效中间验证方法。

Method: HiSpec 利用早退出（EE）模型作为低开销中间验证器，用专门训练的EE模型早期剔除草稿错误token，同时通过方法创新实现draft、verifier和target模型间缓存及隐藏状态复用，减少资源消耗，并采用定期主模型校验以保证准确率。

Result: 实验证明，HiSpec在多项基准和多型号下平均提升推理吞吐量1.28倍，最高提升2.01倍，相比传统单层推测法没有显著精度损失。

Conclusion: HiSpec框架为推测解码中的高效中间验证提供了切实可行的解决方案，兼顾推理速度和准确率，减少系统资源消耗，对LLM推理加速具有实际应用前景。

Abstract: Speculative decoding accelerates LLM inference by using a smaller draft model
to speculate tokens that a larger target model verifies. Verification is often
the bottleneck (e.g. verification is $4\times$ slower than token generation
when a 3B model speculates for a 70B target model), but most prior works focus
only on accelerating drafting. $\textit{``Intermediate"}$ verification reduces
verification time by discarding inaccurate draft tokens early, but existing
methods incur substantial training overheads in incorporating the intermediate
verifier, increase the memory footprint to orchestrate the intermediate
verification step, and compromise accuracy by relying on approximate
heuristics.
  We propose $\underline{\textit{Hi}}\textit{erarchical
}\underline{\textit{Spec}}\textit{ulative Decoding (HiSpec)}$, a framework for
high-throughput speculative decoding that exploits $\textit{early-exit (EE)
models}$ for low-overhead intermediate verification. EE models allow tokens to
exit early by skipping layer traversal and are explicitly trained so that
hidden states at selected layers can be interpreted, making them uniquely
suited for intermediate verification without drastically increasing compute and
memory overheads. To improve resource-efficiency even further, we design a
methodology that enables HiSpec to re-use key-value caches and hidden states
between the draft, intermediate verifier, and target models. To maintain
accuracy, HiSpec periodically validates the draft tokens accepted by the
intermediate verifier against the target model. Our evaluations using various
representative benchmarks and models show that HiSpec improves throughput by
1.28$\times$ on average and by up to 2.01$\times$ compared to the baseline
single-layer speculation without compromising accuracy.

</details>


### [120] [TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies](https://arxiv.org/abs/2510.01391)
*Maithili Kadam,Francis Ferraro*

Main category: cs.CL

TL;DR: 本文提出TAG-EQA（一种将因果事件图融入大语言模型输入的提示框架），提升了大模型在事件推理与因果、时序问题上的表现。实验证明，通过因果知识增强的输入能显著提高问答准确率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然擅长通用语言任务，但在基于事件、尤其是需要因果或时序推理的问题上表现不佳，亟需一种有效方式将结构化因果知识引入模型推理。

Method: 作者提出TAG-EQA框架，将因果事件图转化为自然语言语句，作为结构化关系信息注入大模型输入。设计了3种推理策略（zero-shot, few-shot, chain-of-thought）与3种输入方式（文本、图、文本+图）共9种配置，系统分析结构知识对大模型问答推理的影响。

Result: 在TORQUESTRA基准评测上，TAG-EQA平均提升准确率5%，零样本场景下提升最多达12%，结合因果图与CoT时可达18%。不同模型与配置下皆可观测到结构增强的显著作用。

Conclusion: 无需微调，通过因果事件图提示可在多种场景下增强大语言模型事件推理能力，为结构化知识与大模型结合提供了灵活有效的新途径。

Abstract: Large language models (LLMs) excel at general language tasks but often
struggle with event-based questions-especially those requiring causal or
temporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question
Answering), a prompting framework that injects causal event graphs into LLM
inputs by converting structured relations into natural-language statements.
TAG-EQA spans nine prompting configurations, combining three strategies
(zero-shot, few-shot, chain-of-thought) with three input modalities (text-only,
graph-only, text+graph), enabling a systematic analysis of when and how
structured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA
improves accuracy by 5% on average over text-only baselines, with gains up to
12% in zero-shot settings and 18% when graph-augmented CoT prompting is
effective. While performance varies by model and configuration, our findings
show that causal graphs can enhance event reasoning in LLMs without
fine-tuning, offering a flexible way to encode structure in prompt-based QA.

</details>


### [121] [A-VERT: Agnostic Verification with Embedding Ranking Targets](https://arxiv.org/abs/2510.01469)
*Nicolás Aguirre,Ramiro Caso,Ramiro Rodríguez Colmeiro,Mauro Santelli,Joaquín Toranzo Calderón*

Main category: cs.CL

TL;DR: 本文提出了一种无需结构的自动化评估LM响应的新方法，利用语义嵌入距离以高效、低成本匹配和评价模型输出，能高准确性贴近人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有自动评估语言模型响应的方法要么运算开销大（如LLM-as-a-Judge），要么与实际场景差距大（如字符串匹配、对数概率），急需实用且高效的方法。

Method: 所提方法基于语义嵌入（embedding），通过计算目标候选与LM生成文本间的语义距离，无需结构化标签即可自动分类响应，且仅需使用小于$10B$参数量的嵌入模型，运算开销低。

Result: 在3个数据集和3种不同LM架构上测试，结果显示该方法与人工标注的相关性回归分数达0.97，准确率达96%。

Conclusion: 该方法能以较低计算成本，实现结构无关且准确的自动化评估，为标准制定和实际评测提供了新途径。

Abstract: The automatic evaluation of Language Model (LM) responses is a critical piece
in the development of benchmarks and metrics, both for model training and
quality assessment of production model endpoints. The current approaches to
response classification relies on methods that are too expensive (i.e.
LLM-as-a-Judge) or that are far from real-world conditions (string-matching,
logprob). In this paper, a structure-free evaluation method is presented. The
method makes use of semantic embedding distances to match target candidates
with arbitrary LM-generated text, resulting in a robust classification of the
response at a relatively low compute cost (embedding models of less than $10B$
parameters). The results show a regression score of ~0.97 and an accuracy of
~96% against human annotators, tested over 3 data sets and 3 different LM
architectures.

</details>


### [122] [One More Question is Enough, Expert Question Decomposition (EQD) Model for Domain Quantitative Reasoning](https://arxiv.org/abs/2510.01526)
*Mengyu Wang,Sotirios Sabanis,Miguel de Carvalho,Shay B. Cohen,Tiejun Ma*

Main category: cs.CL

TL;DR: 本文提出了一种名为Expert Question Decomposition（EQD）的新方法，用于提升大语言模型在特定领域（如金融）的复杂定量问答能力。该方法高效实用，并在多个基准数据集上超过了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在需要专家知识和复杂定量推理的领域仍然难以取得良好表现，尤其是在金融等高度专业化领域。因此，亟需探索高效且效果优越的领域定量推理方法，以推动其应用能力。

Method: 作者提出Expert Question Decomposition（EQD）方法，通过两步微调流程，并借助一个用来衡量子问题有效性的奖励函数来优化提问分解过程。该方法只需少量训练样本和单张A100显卡即可完成训练，推理速度与零样本提示相当。

Result: 在金融领域的四个基准数据集上评测，EQD方法在不同大语言模型上的问答表现相比现有领域微调方法和高级提示策略均有提升，提升幅度在0.6%到10.5%之间。

Conclusion: EQD是一种高效且有效的方法，能提升领域专属高难度问答任务的表现。分析发现，在领域问答中，与详细指引步骤相比，提出一个关键辅助子问题对提升最终表现更为重要。

Abstract: Domain-specific quantitative reasoning remains a major challenge for large
language models (LLMs), especially in fields requiring expert knowledge and
complex question answering (QA). In this work, we propose Expert Question
Decomposition (EQD), an approach designed to balance the use of domain
knowledge with computational efficiency. EQD is built on a two-step fine-tuning
framework and guided by a reward function that measures the effectiveness of
generated sub-questions in improving QA outcomes. It requires only a few
thousand training examples and a single A100 GPU for fine-tuning, with
inference time comparable to zero-shot prompting. Beyond its efficiency, EQD
outperforms state-of-the-art domain-tuned models and advanced prompting
strategies. We evaluate EQD in the financial domain, characterized by
specialized knowledge and complex quantitative reasoning, across four benchmark
datasets. Our method consistently improves QA performance by 0.6% to 10.5%
across different LLMs. Our analysis reveals an important insight: in
domain-specific QA, a single supporting question often provides greater benefit
than detailed guidance steps.

</details>


### [123] [ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and Long-Context Reasoning](https://arxiv.org/abs/2510.01585)
*Haochen You,Baojing Liu*

Main category: cs.CL

TL;DR: ReSSFormer是一种改进的Transformer架构，针对长文本推理、计算效率和结构泛化问题提出三个创新模块，实现更高效率和更强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在处理长文本推理、计算效率和结构泛化上存在不足，主要是由于其层堆叠方式固定、注意力计算密集以及对位置编码依赖性强。作者希望突破这些限制，提高模型的推理能力、效率和结构适应性。

Method: 提出ReSSFormer架构，引入三个新模块：1）递归推理和记忆单元（R2MU），用于有界深度的迭代推理；2）自适应稀疏注意力模块（ASAM），实现高效聚焦的上下文选择；3）自组织编码器结构（SOES），可无位置编码实现结构归纳。该方法用递归推理替换传统深度堆叠，用稀疏注意力代替全局注意力，通过内容直接建模潜在token结构。

Result: 在语言建模、多跳问答以及结构敏感任务测试中，ReSSFormer在相似计算量和参数规模下，整体性能优于现有强基线，显示出更佳的可扩展性、效率和结构泛化能力。

Conclusion: ReSSFormer通过结构递归、稀疏注意力和自组织结构创新，显著提升了传统Transformer在长文本推理和结构泛化等核心维度的能力，为高效、可扩展的增强型Transformers设计提供了新方向。

Abstract: While Transformer architectures have demonstrated impressive scalability
across domains, they continue to face challenges in long-context reasoning,
computational efficiency, and structural generalization - largely due to rigid
layer stacking, dense attention, and reliance on positional encodings. We
present ReSSFormer, a Recursive Sparse Structured Transformer that integrates
three complementary innovations: Recurrent Reasoning & Memory Unit (R2MU) for
iterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM)
for efficient and focused context selection, and Self-Organizing Encoder
Structure (SOES) for position-free structure induction. ReSSFormer replaces
conventional depth stacking with recurrent inference, substitutes full
attention with token- and expert-level sparsity, and models latent token
topology directly from content. Across language modeling, multi-hop QA, and
structure-sensitive tasks, ReSSFormer consistently outperforms strong baselines
under comparable FLOPs and parameter budgets, highlighting its scalability,
efficiency, and structural flexibility.

</details>


### [124] [CLUE: Non-parametric Verification from Experience via Hidden-State Clustering](https://arxiv.org/abs/2510.01591)
*Zhenwen Liang,Ruosen Li,Yujun Zhou,Linfeng Song,Dian Yu,Xinya Du,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 论文提出了一种基于大模型（LLM）内部隐藏状态的新型输出验证方法CLUE，能够更好地判断生成内容的正确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM输出质量评测方法主要依赖于文本级特征或置信度，存在过拟合浅层信号或受模型校准影响的问题；但LLM隐藏状态中包含了更丰富的信息，能够支持更准确的输出判断。

Method: 提出了CLUE方法：它不需要训练参数，仅对每次推理过程的隐藏状态变化做delta摘要，并通过与“成功”和“失败”聚类中心的距离判别输出正确性；本质上是一种最邻近中心的基于经验与聚类的无参数验证器。

Result: CLUE在AIME 24/25和GPQA等任务上，显著提升了输出判别准确率，对比于LLM-as-a-judge和多种置信度方法均有优势。例如在AIME 24上，1.5B模型的准确率从多数投票下的56.7%提升到70.0%。

Conclusion: 隐藏状态蕴含了强大的判别信号，基于简单的聚类与距离方法就能显著提升输出评估表现，为后续大模型输出验证提供了新的思路。

Abstract: Assessing the quality of Large Language Model (LLM) outputs presents a
critical challenge. Previous methods either rely on text-level information
(e.g., reward models, majority voting), which can overfit to superficial cues,
or on calibrated confidence from token probabilities, which would fail on
less-calibrated models. Yet both of these signals are, in fact, partial
projections of a richer source of information: the model's internal hidden
states. Early layers, closer to token embeddings, preserve semantic and lexical
features that underpin text-based judgments, while later layers increasingly
align with output logits, embedding confidence-related information. This paper
explores hidden states directly as a unified foundation for verification. We
show that the correctness of a solution is encoded as a geometrically separable
signature within the trajectory of hidden activations. To validate this, we
present Clue (Clustering and Experience-based Verification), a deliberately
minimalist, non-parametric verifier. With no trainable parameters, CLUE only
summarizes each reasoning trace by an hidden state delta and classifies
correctness via nearest-centroid distance to ``success'' and ``failure''
clusters formed from past experience. The simplicity of this method highlights
the strength of the underlying signal. Empirically, CLUE consistently
outperforms LLM-as-a-judge baselines and matches or exceeds modern
confidence-based methods in reranking candidates, improving both top-1 and
majority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24
with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%
(top-maj@16).

</details>


### [125] [A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.01600)
*Neal Gregory Lawton,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: 本文比较了检索增强生成（RAG）中，独立微调、联合微调和两阶段微调的策略，在性能提升上三者相近，但计算成本不同。最佳策略依赖于数据及是否需网格搜索学习率。


<details>
  <summary>Details</summary>
Motivation: RAG作为一种流行的问答系统框架，其性能受嵌入模型和生成模型微调策略影响。现有多种优化策略，但缺乏系统比较分析，动力源自实际应用中对效率和精度权衡的需要。

Method: 作者设计了实验，系统评估了独立微调（分别微调嵌入模型与生成模型）、联合微调（同时微调两个模型）和两阶段微调（先后依次微调）的性能提升及计算成本，并以EM（精确匹配）和F1分数为评判标准。

Result: 实验表明，三种微调策略在提升生成质量（EM与F1分数）方面基本持平，但在计算资源消耗上存在明显差异。

Conclusion: 最佳微调策略选择需综合考虑：是否有带context标签的数据，以及是否需要对嵌入与生成模型分别网格搜索学习率。

Abstract: A Comparison of Independent and Joint Fine-tuning Strategies for
Retrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel,
Anoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP
2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0
Keywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs),
Fine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate and
compare strategies for fine-tuning Retrieval Augmented Generation (RAG)
pipelines, including independent fine-tuning, joint fine-tuning, and two-phase
fine-tuning. Abstract: Retrieval augmented generation (RAG) is a popular
framework for question answering that is powered by two large language models
(LLMs): an embedding model that retrieves context documents from a database
that are relevant to a given question, and a generator model that uses the
retrieved context to generate an answer to the question. Both the embedding and
generator models can be fine-tuned to increase performance of a RAG pipeline on
a new task, but multiple fine-tuning strategies exist with different costs and
benefits. In this paper, we evaluate and compare several RAG fine-tuning
strategies, including independent, joint, and two-phase fine-tuning. In our
experiments, we observe that all of these strategies achieve about equal
improvement in EM and F1 generation quality metrics, although they have
significantly different computational costs. We conclude the optimal
fine-tuning strategy to use depends on whether the training dataset includes
context labels and whether a grid search over the learning rates for the
embedding and generator models is required.

</details>


### [126] [RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question Answering](https://arxiv.org/abs/2510.01612)
*Lovely Yeswanth Panchumarthi,Sai Prasad Gudari,Atharva Negi,Praveen Raj Budime,Harsit Upadhya*

Main category: cs.CL

TL;DR: 本文提出了RAG-BioQA框架，通过结合检索增强生成（RAG）与领域自适应微调，实现了更高质量的医学长文本智能问答。该方法在PubMedQA数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着生物医学文献的爆炸式增长，临床人员和研究者在获取准确和详尽的医学信息方面面临巨大挑战。目前的生物医学问答系统主要局限于短答案，难以满足临床决策所需的详实解释。

Method: 作者构建了RAG-BioQA框架，使用BioBERT嵌入结合FAISS索引进行高效信息检索，并尝试BM25、ColBERT、MonoT5等重排序策略优化检索上下文。最终，通过针对生物医学领域微调的T5模型生成基于证据的长篇答案。

Result: 在PubMedQA数据集上的实验显示，所提出的方法在BLEU、ROUGE和METEOR等评测指标上相较基线均有大幅提升，证明了其在生物医学知识检索与长篇问答上的有效性。

Conclusion: RAG-BioQA显著提升了生物医学领域基于证据的可解释长文本问答能力，为临床和科研领域的知识获取提供了强有力的工具。

Abstract: The exponential growth of biomedical literature creates significant
challenges for accessing precise medical information. Current biomedical
question-answering systems primarily focus on short-form answers, failing to
provide the comprehensive explanations necessary for clinical decision-making.
We present RAG-BioQA, a novel framework combining retrieval-augmented
generation with domain-specific fine-tuning to produce evidence-based,
long-form biomedical answers. Our approach integrates BioBERT embeddings with
FAISS indexing and compares various re-ranking strategies (BM25, ColBERT,
MonoT5) to optimize context selection before synthesizing evidence through a
fine-tuned T5 model. Experimental results on the PubMedQA dataset show
significant improvements over baselines, with our best model achieving
substantial gains across BLEU, ROUGE, and METEOR metrics, advancing the state
of accessible, evidence-based biomedical knowledge retrieval.

</details>


### [127] [Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single Consumer GPU: Continual Pre-training, SFT, and DPO](https://arxiv.org/abs/2510.01616)
*Yu-Cheng Chih,Ming-Tao Duan,Yong-Hao Hou*

Main category: cs.CL

TL;DR: 该论文提出了PureTC-1B方法，通过三阶段流程显著提升小语言模型在繁体中文任务中的语言输出稳定性，有效减少非繁体中文或混用语言的输出。


<details>
  <summary>Details</summary>
Motivation: 现有小语言模型在繁体中文场景下经常出现输出非繁体或夹杂其他语言（token-level instability），严重影响在实际应用中的可靠性，因此亟需稳定、纯净的繁体中文输出方法。

Method: 作者基于Meta公开的Llama-3.2-1B-Instruct模型，设计了PureTC-1B三阶段流程：1）在以繁体中文为主的语料上持续预训练（CPT）；2）用指令数据做有监督微调（SFT）；3）采用直接偏好优化（DPO）强化模型对纯繁体输出的倾向。所有修改都通过LoRA适配器进行，无需全模型重训练。

Result: PureTC-1B在真实世界场景基准测试中，非繁体输出token数较基础模型下降51.3%；在命名实体翻译任务上，错误语言token数比Llama-3B低77.2%，比Qwen-1.5B低57.2%。

Conclusion: 只需通过适配器方法，不用重训整个大模型，就可以大幅提升小模型在繁体中文任务中的语言纯净和稳定性，对硬件友好且易于复现，可为非英语小模型提供实用提升方案。

Abstract: Small Language Models (SLMs) enable cost-effective, on-device and
latency-sensitive AI applications, yet their deployment in Traditional Chinese
(TC) remains hindered by token-level instability - models unpredictably emit
non-TC characters or code-switch into other languages. We address this
practical reliability gap by creating PureTC-1B, a three-stage stabilization
pipeline for Llama-3.2-1B-Instruct (an open-weight, instruction-tuned model
released by Meta) using parameter-efficient LoRA adapters. Our method combines
Continual Pre-Training (CPT) on TC-centric corpora, Supervised Fine-Tuning
(SFT) with instruction data, and Direct Preference Optimization (DPO) using
TC-adherence preferences to improve monolingual robustness without full-model
retraining. On a benchmark designed to simulate real-world usage, PureTC-1B
achieves a 51.3% relative reduction (micro-average) in non-TC output tokens
versus the base model. On a Named Entity Translation (NET) task, PureTC-1B
further reduces incorrect-language tokens by 77.2% relative to Llama-3B and
57.2% relative to Qwen-1.5B, indicating that robust TC adherence is attainable
even at the 1B scale. The pipeline is reproducible, adapter-only, and
hardware-friendly, offering practitioners a practical recipe to enhance
language stability for TC and potentially other non-English languages.

</details>


### [128] [AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System](https://arxiv.org/abs/2510.01617)
*Hui Yi Leong,Yuheng Li,Yuqing Wu,Wenwen Ouyang,Wei Zhu,Jiechao Gao*

Main category: cs.CL

TL;DR: 本文提出了AMAS框架，通过自适应动态图设计改善多智能体系统中大语言模型的表现，并在多个任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前基于LLM的多智能体系统在实际工业应用中面临难以灵活应对各种任务的结构限制，主要因手工设计的固定拓扑结构缺乏上下文响应能力，导致性能不足。

Method: 提出了AMAS框架，核心为动态图设计器组件。该组件通过轻量级LLM自适应，自动为不同任务识别和生成最优的智能体拓扑结构，根据具体输入智能分配任务流路径，而不再依赖单一静态结构模板。

Result: 在问答、数学推理与代码生成等基准任务上，AMAS在不同类型的大语言模型下均大幅超越现有的单智能体和多智能体方法，表现出更强的效果和适应性。

Conclusion: LLM多智能体系统要实现高性能，结构需具备对上下文的自适应调整能力；AMAS框架为解决这一核心难题提供了有效路径。

Abstract: Although large language models (LLMs) have revolutionized natural language
processing capabilities, their practical implementation as autonomous
multi-agent systems (MAS) for industrial problem-solving encounters persistent
barriers. Conventional MAS architectures are fundamentally restricted by
inflexible, hand-crafted graph topologies that lack contextual responsiveness,
resulting in diminished efficacy across varied academic and commercial
workloads. To surmount these constraints, we introduce AMAS, a
paradigm-shifting framework that redefines LLM-based MAS through a novel
dynamic graph designer. This component autonomously identifies task-specific
optimal graph configurations via lightweight LLM adaptation, eliminating the
reliance on monolithic, universally applied structural templates. Instead, AMAS
exploits the intrinsic properties of individual inputs to intelligently direct
query trajectories through task-optimized agent pathways. Rigorous validation
across question answering, mathematical deduction, and code generation
benchmarks confirms that AMAS systematically exceeds state-of-the-art
single-agent and multi-agent approaches across diverse LLM architectures. Our
investigation establishes that context-sensitive structural adaptability
constitutes a foundational requirement for high-performance LLM MAS
deployments.

</details>


### [129] [NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with BERT](https://arxiv.org/abs/2510.01644)
*John Hawkins,Aditya Pramar,Rodney Beard,Rohitash Chandra*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）面对“越狱”提示（jailbreak prompts）的脆弱性，并评估了各种机器学习模型检测此类提示的能力。实验显示，微调BERT模型能够有效识别这些恶意输入。


<details>
  <summary>Details</summary>
Motivation: 越来越多的恶意用户通过设计特殊提示（越狱提示），让LLM绕过开发者预设的安全机制，生成不符合政策的回复。因此，急需研究并开发方法检测并阻止这些越狱提示。

Method: 作者对比了不同机器学习模型（包括微调BERT）在区分正常提示和越狱提示上的表现，并特别关注能否识别此前未出现过的越狱策略。同时，利用关键词可视化分析区分越狱提示与正常输入的特征。

Result: 实验结果表明，在现有数据集下，端到端微调的BERT模型在识别越狱输入方面取得了最佳表现。关键词可视化揭示，具有明显自反性的提示结构常常出现在越狱输入中。

Conclusion: BERT等深度模型可用于有效检测越狱提示，且提示中的自反性结构或可作为判别越狱意图的信号。

Abstract: Large Language Models (LLMs) suffer from a range of vulnerabilities that
allow malicious users to solicit undesirable responses through manipulation of
the input text. These so-called jailbreak prompts are designed to trick the LLM
into circumventing the safety guardrails put in place to keep responses
acceptable to the developer's policies. In this study, we analyse the ability
of different machine learning models to distinguish jailbreak prompts from
genuine uses, including looking at our ability to identify jailbreaks that use
previously unseen strategies. Our results indicate that using current datasets
the best performance is achieved by fine tuning a Bidirectional Encoder
Representations from Transformers (BERT) model end-to-end for identifying
jailbreaks. We visualise the keywords that distinguish jailbreak from genuine
prompts and conclude that explicit reflexivity in prompt structure could be a
signal of jailbreak intention.

</details>


### [130] [Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings in LLMs with Enabled Bidirectional Attention](https://arxiv.org/abs/2510.01652)
*Zhaoxin Feng,Jianfei Ma,Emmanuele Chersoni,Xiaojing Zhao,Xiaoyi Bao*

Main category: cs.CL

TL;DR: 本文探讨了通过引入双向注意力机制来提升生成式大模型在文本嵌入和语义表征任务中的表现。作者以Llama架构为基础，通过逐步增加双向注意力及对比学习训练，分析其在相关任务中的效果提升。


<details>
  <summary>Details</summary>
Motivation: 尽管自回归大模型（LLM）在语言理解和生成上表现突出，但由于其单向注意力机制限制，在文本嵌入和语义分析任务上的应用和研究进展缓慢。本文旨在突破这一瓶颈，探索增强模型语义表征能力的方法。

Method: 作者以Llama架构为实验对象，设计若干不同变体，通过继续训练逐步引入双向注意力机制，并采用无监督与有监督对比学习策略，测试对文本嵌入与语义表征性能的影响。

Result: 实验结果表明，加入双向注意力和对比学习后，模型在文本嵌入及语义任务中的表现均有显著提升。

Conclusion: 双向注意力机制配合对比学习可有效提升自回归大模型在语义表征层面的能力，为其在嵌入等任务中的应用拓展了新途径。

Abstract: Autoregressive Large Language Models (LLMs) demonstrate exceptional
performance in language understanding and generation. However, their
application in text embedding tasks has been relatively slow, along with the
analysis of their semantic representation in probing tasks, due to the
constraints of the unidirectional attention mechanism.
  This paper aims to explore whether such constraints can be overcome by
enabling bidirectional attention in LLMs. We tested different variants of the
Llama architecture through additional training steps, progressively enabling
bidirectional attention and unsupervised/supervised contrastive learning.

</details>


### [131] [SoK: Measuring What Matters for Closed-Loop Security Agents](https://arxiv.org/abs/2510.01654)
*Mudita Khurana,Raunak Jain*

Main category: cs.CL

TL;DR: 本文提出了CLASP框架和CLC分数，用于评估和提升自主安全代理在网络安全生命周期中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前网络安全研究和工具分散在各防御环节，导致孤岛效应和安全盲区，使得攻击方可以迅速适应并利用传统防御的不足，缺乏统一的能力评估框架和实际性能基准。

Method: 作者提出CLASP框架，将安全生命周期（如侦查、利用、分析、修补、验证）与代理的关键能力（规划、工具使用、记忆、推理、反思与感知）进行对齐，并为安全任务中的代理能力建立通用词汇和评估标准。通过CLASP分析了21项代表性工作，发现能力优势与不足。提出CLC分数作为量化“闭环”程度和操作成效的复合指标，并说明闭环基准测试的需求。

Result: 成功应用CLASP框架对21项系统进行了能力映射，揭示了现有系统的优势和能力空白点。同时提出了CLC分数，为闭环安全系统的性能衡量提供了新工具。

Conclusion: CLASP框架和CLC分数为自主安全代理的能力评估和诊断建立了统一语言和评价体系，有助于推动网络安全代理的持续发展和高效闭环性能的提升。

Abstract: Cybersecurity is a relentless arms race, with AI driven offensive systems
evolving faster than traditional defenses can adapt. Research and tooling
remain fragmented across isolated defensive functions, creating blind spots
that adversaries exploit. Autonomous agents capable of integrating, exploit
confirmation, remediation, and validation into a single closed loop offer
promise, but the field lacks three essentials: a framework defining the agentic
capabilities of security systems across security life cycle, a principled
method for evaluating closed loop agents, and a benchmark for measuring their
performance in practice. We introduce CLASP: the Closed-Loop Autonomous
Security Performance framework which aligns the security lifecycle
(reconnaissance, exploitation, root cause analysis, patch synthesis,
validation) with core agentic capabilities (planning, tool use, memory,
reasoning, reflection & perception) providing a common vocabulary and rubric
for assessing agentic capabilities in security tasks. By applying CLASP to 21
representative works, we map where systems demonstrate strengths, and where
capability gaps persist. We then define the Closed-Loop Capability (CLC) Score,
a composite metric quantifying both degree of loop closure and operational
effectiveness, and outline the requirements for a closed loop benchmark.
Together, CLASP and the CLC Score, provide the vocabulary, diagnostics, and
measurements needed to advance both function level performance and measure
closed loop security agents.

</details>


### [132] [MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue Summarization](https://arxiv.org/abs/2510.01659)
*Yinhong Liu,Jianfeng He,Hang Su,Ruixue Lian,Yi Nian,Jake Vincent,Srikanth Vishnubhotla,Robinson Piramuthu,Saab Mansour*

Main category: cs.CL

TL;DR: 本文提出了MDSEval，这是第一个用于多模态对话摘要（MDS）任务的元评估基准，包含了图片分享对话、摘要及多维度人工评价。


<details>
  <summary>Details</summary>
Motivation: 当前MDS模型的评估依赖大量人工标注，效率低且成本高，因此亟需自动且可靠的评测方法。而自动评测方法又需借助高度依赖人工标注的元评估基准，但目前缺乏针对MDS的系统性元评估数据集。

Method: 作者提出了MDSEval数据集，采集了图片分享类多模态对话及其摘要，并在八个明确定义的质量维度上收集了人工标注。此外，作者提出了一套基于跨模态‘互斥关键信息’的过滤流程，以保证数据丰富和高质量。

Result: 作者总结和归纳了多模态对话摘要的关键评估维度，并利用MDSEval对现有主流自动评测方法进行基准测试，发现这些评测手段在区分高级MLLM生成摘要和存在偏见方面存在明显局限。

Conclusion: MDSEval为多模态对话摘要自动评估方法的发展提供了标准和工具，有助于揭示和改进现有评测方法的不足，推动该领域研究进步。

Abstract: Multimodal Dialogue Summarization (MDS) is a critical task with wide-ranging
applications. To support the development of effective MDS models, robust
automatic evaluation methods are essential for reducing both cost and human
effort. However, such methods require a strong meta-evaluation benchmark
grounded in human annotations. In this work, we introduce MDSEval, the first
meta-evaluation benchmark for MDS, consisting image-sharing dialogues,
corresponding summaries, and human judgments across eight well-defined quality
aspects. To ensure data quality and richfulness, we propose a novel filtering
framework leveraging Mutually Exclusive Key Information (MEKI) across
modalities. Our work is the first to identify and formalize key evaluation
dimensions specific to MDS. We benchmark state-of-the-art modal evaluation
methods, revealing their limitations in distinguishing summaries from advanced
MLLMs and their susceptibility to various bias.

</details>


### [133] [FOR-Prompting: From Objection to Revision via an Asymmetric Prompting Protocol](https://arxiv.org/abs/2510.01674)
*He Zhang,Anzhou Zhang,Jian Dai*

Main category: cs.CL

TL;DR: 本文提出FOR-Prompting协议（一种“从异议到修订”的提示结构），通过引入‘异议者’角色显式提问、促进自我修正，提升大模型和小模型的推理表现，无需额外工具与监督，方法兼容不同规模与类型模型，分析显示在推理、连贯性等方面优于单一提示与已知推理流程。


<details>
  <summary>Details</summary>
Motivation: 现有如Chain of Thought（CoT）和Tree of Thought（ToT）等推理协议虽然能组织内部思考，但缺乏外部发问、促使自我修正的机制，因此有准确性与鲁棒性提升空间。本文旨在引入异议和自我修订机制，提升大语言模型的推理能力、适用性与透明度。

Method: 提出FOR-Prompting，一种角色分工的协议，包括：Defender提出回答，Objectioner仅提出问题类异议但不直接修正，Host负责确保一致与终结。完全在提示层实现，无需模型重训练，兼容不同规模和类型（如GPT、Llama）。对推理及开放性任务定量与定性分析。

Result: 在GSM8K数据集上，FOR-Prompting比单一提示提升约22个百分点，与CoT持平，在推理与连贯性评级提升10%以上；对小模型（如Llama3 2:1b）准确率提升约19%。能自动修正复杂问题，无需外部工具/人类监督。开放性任务分析显示其能更好地展示假设与权衡。

Conclusion: FOR-Prompting能有效提升模型推理和自我修正能力，对大、小模型均显著增益，适合个人设备与广泛部署。同时，协议透明可扩展，为模型无监督批量测试与优化带来新路径。

Abstract: Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT)
organize internal deliberation but lack an explicit mechanism for external
questioning that elicits self-revision. We present FOR-Prompting (From
Objection to Revision Prompting), an asymmetric protocol where a Defender
proposes an answer, an Objectioner raises question-style objections with no
direct fixes, and a Host enforces consistency and closure. On GSM8K we observe
about a 22% point gain over single-prompt and accuracy on par with CoT, with
more than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1
judge. FOR-Prompting also corrects mistakes without tools or human supervision
on tricky queries, and improves performance for small-scale model (approx. 19%
accuracy improved on Llama3.2:1b for GSM8K task), highlighting promise for
small models and on personal device use. Beyond factual QA, qualitative
analyses on open-ended tasks show enhanced exploration and refinement, with
dialogue traces that make assumptions and trade-offs explicit. The protocol is
model agnostic and operates purely at the prompt level through role-structured
turns, so it works with hosted and local models of different sizes without
retraining, and it supports large-scale study of objection-guided reasoning.

</details>


### [134] [How Do Language Models Compose Functions?](https://arxiv.org/abs/2510.01685)
*Apoorv Khandelwal,Ellie Pavlick*

Main category: cs.CL

TL;DR: 本论文探讨大型语言模型（LLM）在处理两步事实回忆任务时是否使用了组合性机制，并发现模型虽然能独立完成任务，但在组合任务上存在“组合性缺口”，且其任务解决方式依赖于嵌入空间的结构。


<details>
  <summary>Details</summary>
Motivation: LLM在组合性任务表现突出，但不清楚其是否真正依靠了组合性机制。理解这一点对于解释LLM推理过程和提升模型能力具有重要意义。

Method: 作者设计并分析了两步（two-hop）事实回忆任务，将其形式化为g(f(x))，并用logit lens技术对残差流激活进行分析，以揭示模型内部处理机制。

Result: 发现当前LLM存在“组合性缺口”：即能分别计算f(x)和g(z)，但不一定能准确组合计算g(f(x))。分析还揭示了两种任务解决机制：一种组合性求解，一种直接求解，并且任务采用哪种机制与嵌入空间结构有关。

Conclusion: LLM在组合性任务上并不总能自动采用组合化处理方式，模型处理方式受限于嵌入空间的属性。对理解和改进LLM推理和泛化能力具有启示意义。

Abstract: While large language models (LLMs) appear to be increasingly capable of
solving compositional tasks, it is an open question whether they do so using
compositional mechanisms. In this work, we investigate how feedforward LLMs
solve two-hop factual recall tasks, which can be expressed compositionally as
$g(f(x))$. We first confirm that modern LLMs continue to suffer from the
"compositionality gap": i.e. their ability to compute both $z = f(x)$ and $y =
g(z)$ does not entail their ability to compute the composition $y = g(f(x))$.
Then, using logit lens on their residual stream activations, we identify two
processing mechanisms, one which solves tasks $\textit{compositionally}$,
computing $f(x)$ along the way to computing $g(f(x))$, and one which solves
them $\textit{directly}$, without any detectable signature of the intermediate
variable $f(x)$. Finally, we find that which mechanism is employed appears to
be related to the embedding space geometry, with the idiomatic mechanism being
dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in
the embedding spaces. We fully release our data and code at:
https://github.com/apoorvkh/composing-functions .

</details>


### [135] [Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation](https://arxiv.org/abs/2510.01688)
*Seungseop Lim,Gibaeg Kim,Wooseok Han,Jean Seo,Hyunkyung Lee,Jaehyo Yoo,Eunho Yang*

Main category: cs.CL

TL;DR: 本文发现并解决了大语言模型（LLMs）在医学预问诊多轮对话中由于训练数据回合数分布不均，导致的“格式惯性”问题，并通过重采样优化，显著改善了这一现象。


<details>
  <summary>Details</summary>
Motivation: 医学领域的多轮对话生成通常使用有监督微调（SFT），但现有数据中对话回合数分布失衡，严重影响模型质量。作者提出需要关注这种分布失衡带来的新型模型失效机制。

Method: 作者分析和定义了“格式惯性”失效机制，并采用数据再平衡的方法，调整训练集中不同回合数的数据比例，以缓解该问题。

Result: 实验证明，采用重采样后的数据训练的模型在医学预问诊场景中，能够有效减少格式惯性的出现，提出的问题更具诊断信息。

Conclusion: 数据分布的合理性直接影响LLM在医疗对话中的表现，简单的数据重平衡即能明显改善模型生成效果，值得在相关任务中推广。

Abstract: Recent advances in Large Language Models (LLMs) have brought significant
improvements to various service domains, including chatbots and medical
pre-consultation applications. In the healthcare domain, the most common
approach for adapting LLMs to multi-turn dialogue generation is Supervised
Fine-Tuning (SFT). However, datasets for SFT in tasks like medical
pre-consultation typically exhibit a skewed turn-count distribution. Training
on such data induces a novel failure mechanism we term **Format Inertia**,
where models tend to generate repetitive, format-correct, but diagnostically
uninformative questions in long medical dialogues. To mitigate this observed
failure mechanism, we adopt a simple, data-centric method that rebalances the
turn-count distribution of the training dataset. Experimental results show that
our approach substantially alleviates Format Inertia in medical
pre-consultation.

</details>


### [136] [What MLLMs Learn about When they Learn about Multimodal Reasoning: Perception, Reasoning, or their Integration?](https://arxiv.org/abs/2510.01719)
*Jiwan Chung,Neel Joshi,Pratyusha Sharma,Youngjae Yu,Vibhav Vineet*

Main category: cs.CL

TL;DR: 本文提出了MathLens基准，用于细粒度地评估多模态推理模型在几何题中不同子技能的表现，发现感知、推理和整合能力提升不均衡，各类训练方法对技能提升影响各异。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理模型虽然在奥林匹克几何等复杂任务中展现潜力，但评估方式多依赖于单一准确率，无法揭示模型在子技能（如感知、推理、整合）上的具体改进与短板。为更有效诊断和提升模型，亟需精细化、更具解释力的评估框架。

Method: 设计了MathLens基准，将复杂的几何题拆解为感知（从原始输入中提取信息）、推理（对已有信息进行操作）和整合（选择相关感知证据并应用推理）三大能力，并通过多种细粒度注释和题型支持针对性测试，保证评测的一致性和鲁棒性，同时对不同训练范式（强化学习、文本SFT等）进行系统分析。

Result: 1. 强化学习主要提升感知能力，结合文本监督效果更佳；2. 推理能力需与感知能力同步提升；3. 整合能力提升最困难，成为主要瓶颈；4. 在鲁棒性上，RL可提升对图变异的一致性，而多模态SFT可能因过拟合导致鲁棒性下降。

Conclusion: 综合来看，当前多模态模型在应对几何问题时，感知与推理能力可由不同训练方法强化，但整合能力依然薄弱，未来需针对该短板设计新方法。发布的数据和实验日志也为后续相关研究提供了丰富资源。

Abstract: Multimodal reasoning models have recently shown promise on challenging
domains such as olympiad-level geometry, yet their evaluation remains dominated
by aggregate accuracy, a single score that obscures where and how models are
improving. We introduce MathLens, a benchmark designed to disentangle the
subskills of multimodal reasoning while preserving the complexity of
textbook-style geometry problems. The benchmark separates performance into
three components: Perception: extracting information from raw inputs,
Reasoning: operating on available information, and Integration: selecting
relevant perceptual evidence and applying it within reasoning. To support each
test, we provide annotations: visual diagrams, textual descriptions to evaluate
reasoning in isolation, controlled questions that require both modalities, and
probes for fine-grained perceptual skills, all derived from symbolic
specifications of the problems to ensure consistency and robustness. Our
analysis reveals that different training approaches have uneven effects: First,
reinforcement learning chiefly strengthens perception, especially when
supported by textual supervision, while textual SFT indirectly improves
perception through reflective reasoning. Second, reasoning improves only in
tandem with perception. Third, integration remains the weakest capacity, with
residual errors concentrated there once other skills advance. Finally,
robustness diverges: RL improves consistency under diagram variation, whereas
multimodal SFT reduces it through overfitting. We will release all data and
experimental logs.

</details>


### [137] [Machine-interpretable Engineering Design Standards for Valve Specification](https://arxiv.org/abs/2510.01736)
*Anders Gjerver,Rune Frostad,Vedrana Barisic,Melinda Hodkiewicz,Caitlin Woods,Mihaly Fekete,Arild Braathen Torjusen,Johan Wilhelm Kluwer*

Main category: cs.CL

TL;DR: 本文介绍了一种方法，将工程设计标准中的信息转化为可复用、模块化、机器可读的本体，并应用于设备选型及设计的质量保证，通过本体和语义推理，实现自动化合规性验证，推动标准数字化。


<details>
  <summary>Details</summary>
Motivation: 当前工程设计依赖文档型规格书和标准，数字化进程缓慢。为实现数字化和自动化管理，需将文本标准结构化、机器可读，有助于智能化设计和合规验证。

Method: 将国际标准（如材料和管道标准）中的结构化和非结构化信息抽取为本体模块，采用建模模式保证模块复用性与互操作性，格式遵循W3C和ISO顶层本体（IDO）。通过语义建模创建阀门、产品类型等本体个体，利用OWL和设计规则进行自动化合规判定。

Result: 成功将若干国际标准转换为可交换的本体，用于阀门选型场景。实现了基于语义资产模型的阀门实例化与合规性自动验证，提高了标准与设计之间的连接与自动化。

Conclusion: 基于IDO的本体方法能有效提升工程设计标准的数字化和智能化水平，为标准组织向数字化智能标准过渡提供可行方案。

Abstract: Engineering design processes use technical specifications and must comply
with standards. Product specifications, product type data sheets, and design
standards are still mainly document-centric despite the ambition to digitalize
industrial work. In this paper, we demonstrate how to transform information
held in engineering design standards into modular, reusable,
machine-interpretable ontologies and use the ontologies in quality assurance of
the plant design and equipment selection process. We use modelling patterns to
create modular ontologies for knowledge captured in the text and in frequently
referenced tables in International Standards for piping, material and valve
design. These modules are exchangeable, as stored in a W3C compliant format,
and interoperable as they are aligned with the top-level ontology ISO DIS
23726-3: Industrial Data Ontology (IDO).
  We test these ontologies, created based on international material and piping
standards and industry norms, on a valve selection process. Valves are
instantiated in semantic asset models as individuals along with a semantic
representation of the environmental condition at their location on the asset.
We create "functional location tags" as OWL individuals that become instances
of OWL class Valve Data Sheet (VDS) specified valves. Similarly we create
instances of manufacturer product type. Our approach enables automated
validation that a specific VDS is compliant with relevant industry standards.
Using semantic reasoning and executable design rules, we also determine whether
the product type meets the valve specification. Creation of shared, reusable
IDO-based modular ontologies for design standards enables semantic reasoning to
be applied to equipment selection processes and demonstrates the potential of
this approach for Standards Bodies wanting to transition to digitized Smart
Standards.

</details>


### [138] [Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware Refusal in Factual Tasks](https://arxiv.org/abs/2510.01782)
*Wenbo Pan,Jie Xu,Qiguang Chen,Junhao Dong,Libo Qin,Xinfeng Li,Haining Yu,Xiaohua Jia*

Main category: cs.CL

TL;DR: 本文提出了一个新的评估大语言模型（LLM）“拒答”能力的指标Refusal Index（RI），并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有指标无法准确评估大语言模型在面对超出知识范围的问题时是否能够拒绝作答，这对模型的事实可靠性非常重要。以往的拒答率和校准指标存在偏差与不一致性，无法真实反映模型的实际拒绝能力。

Method: 作者提出了Refusal Index（RI）指标，用于衡量模型在面对未知问题时拒绝作答的准确性。RI通过模型对于拒绝概率与出错概率的斯皮尔曼等级相关性来定义，并设计了一种轻量级的“两遍”评测方法，以便高效估算RI。

Result: 在16个模型和5个数据集上的多项实验表明，RI能够准确反映模型的知识感知性拒答能力，且对模型的总体准确率和拒答率较为稳定，不受其影响，能提供一致的模型排名。

Conclusion: RI能有效补充传统只关注准确率的评测方式，帮助全面评估大语言模型在事实任务下的可靠性。实验还发现，虽部分LLM在事实任务表现很好，但其拒答行为仍然不可靠，因此综合RI等指标十分必要。

Abstract: Large Language Models (LLMs) should refuse to answer questions beyond their
knowledge. This capability, which we term knowledge-aware refusal, is crucial
for factual reliability. However, existing metrics fail to faithfully measure
this ability. On the one hand, simple refusal-based metrics are biased by
refusal rates and yield inconsistent scores when models exhibit different
refusal tendencies. On the other hand, existing calibration metrics are
proxy-based, capturing the performance of auxiliary calibration processes
rather than the model's actual refusal behavior. In this work, we propose the
Refusal Index (RI), a principled metric that measures how accurately LLMs
refuse questions they do not know. We define RI as Spearman's rank correlation
between refusal probability and error probability. To make RI practically
measurable, we design a lightweight two-pass evaluation method that efficiently
estimates RI from observed refusal rates across two standard evaluation runs.
Extensive experiments across 16 models and 5 datasets demonstrate that RI
accurately quantifies a model's intrinsic knowledge-aware refusal capability in
factual tasks. Notably, RI remains stable across different refusal rates and
provides consistent model rankings independent of a model's overall accuracy
and refusal rates. More importantly, RI provides insight into an important but
previously overlooked aspect of LLM factuality: while LLMs achieve high
accuracy on factual tasks, their refusal behavior can be unreliable and
fragile. This finding highlights the need to complement traditional accuracy
metrics with the Refusal Index for comprehensive factuality evaluation.

</details>


### [139] [Comparison of Unsupervised Metrics for Evaluating Judicial Decision Extraction](https://arxiv.org/abs/2510.01792)
*Ivan Leonidovich Litvak,Anton Kostin,Fedor Lashkin,Tatiana Maksiyan,Sergey Lagutin*

Main category: cs.CL

TL;DR: 本文评估了16种无监督指标对司法判决文本抽取质量的衡量能力，包含文档、语义、结构等多种类型，并验证其与专家人工打分的一致性。部分指标与专家评价有较高相关性，但整体无法完全取代人工判定。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在法律NLP中的快速进展，亟需可扩展的、不依赖人工标注数据的方法来对司法判决文本的抽取质量进行评估。人工标注成本高、效率有限，限制了大规模法律文本处理和创新应用。

Method: 研究分析了16种无监督评价指标，适用于7种司法判决语义块的质量评测，在1,000份俄语判决书上，与7,168个专家1-5分打分结果做相关性、Lin一致性相关系数和平均绝对误差的对比。指标涵盖文档、语义、结构、伪真值、法律专属类型等，并测试了基于LLM（如gpt-4.1-mini）的得分。

Result: 词频连贯性（Term Frequency Coherence）和区块完整性覆盖率（Coverage Ratio/Block Completeness）与专家评分最为一致（Pearson r约0.5），法律术语密度呈强负相关，而LLM评分与专家为中等相关（r=0.382）。总体，所有无监督指标与专家存在一定相关，但一致性有限。

Conclusion: 无监督自动评估指标（包括大模型自动评估）为法律文本处理提供了有力的可扩展工具，但由于与人工专家评价的一致性有限，难以完全取代人工判断。该研究为法律NLP领域无标注高效评估工具提供了实证支持，有助于提升司法数据分析与AI伦理落地应用效率。

Abstract: The rapid advancement of artificial intelligence in legal natural language
processing demands scalable methods for evaluating text extraction from
judicial decisions. This study evaluates 16 unsupervised metrics, including
novel formulations, to assess the quality of extracting seven semantic blocks
from 1,000 anonymized Russian judicial decisions, validated against 7,168
expert reviews on a 1--5 Likert scale. These metrics, spanning document-based,
semantic, structural, pseudo-ground truth, and legal-specific categories,
operate without pre-annotated ground truth. Bootstrapped correlations, Lin's
concordance correlation coefficient (CCC), and mean absolute error (MAE) reveal
that Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE =
0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC =
0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density
(Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negative
correlations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, Lin
CCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, using
gpt-4.1-mini via g4f, suggests limited specialization for legal textse. These
findings highlight that unsupervised metrics, including LLM-based approaches,
enable scalable screening but, with moderate correlations and low CCC values,
cannot fully replace human judgment in high-stakes legal contexts. This work
advances legal NLP by providing annotation-free evaluation tools, with
implications for judicial analytics and ethical AI deployment.

</details>


### [140] [Detecting LLM-Generated Spam Reviews by Integrating Language Model Embeddings and Graph Neural Network](https://arxiv.org/abs/2510.01801)
*Xin Liu,Rongwu Xu,Xinyi Jia,Jason Liao,Jiao Sun,Ling Huang,Wei Xu*

Main category: cs.CL

TL;DR: 本文提出了一种新型垃圾评论检测系统FraudSquad，专门用于应对大语言模型（LLM）生成的高拟真、高迷惑性垃圾评论，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能够生成高度拟真的虚假评论，传统检测系统难以分辨其真假，严重威胁到互联网平台的公信力。迫切需要新方法来应对这类新型垃圾评论。

Method: 作者首先利用三种不同的LLM（结合产品元数据和真实评论作为指导）生成三组高仿真垃圾评论数据集；随后提出FraudSquad模型，该模型结合了预训练语言模型的文本嵌入和门控图变压器（Gated Graph Transformer）结构，用于分类检测垃圾节点，无需手工特征工程且训练资源需求低。

Result: FraudSquad在三组LLM生成的数据集上，相对于现有最优方法，精确率提升最高可达44.22%，召回率提升最高达43.01%；在两个真人垃圾评论数据集上也表现优异。模型体积小、所需标注数据少，适合实际应用。

Conclusion: 本工作证实了LLM生成垃圾评论的巨大威胁，并通过发布新数据集和FraudSquad新模型，为新一代垃圾评论检测提供了解决思路，同时为实际部署提供了有效工具。

Abstract: The rise of large language models (LLMs) has enabled the generation of highly
persuasive spam reviews that closely mimic human writing. These reviews pose
significant challenges for existing detection systems and threaten the
credibility of online platforms. In this work, we first create three realistic
LLM-generated spam review datasets using three distinct LLMs, each guided by
product metadata and genuine reference reviews. Evaluations by GPT-4.1 confirm
the high persuasion and deceptive potential of these reviews. To address this
threat, we propose FraudSquad, a hybrid detection model that integrates text
embeddings from a pre-trained language model with a gated graph transformer for
spam node classification. FraudSquad captures both semantic and behavioral
signals without relying on manual feature engineering or massive training
resources. Experiments show that FraudSquad outperforms state-of-the-art
baselines by up to 44.22% in precision and 43.01% in recall on three
LLM-generated datasets, while also achieving promising results on two
human-written spam datasets. Furthermore, FraudSquad maintains a modest model
size and requires minimal labeled training data, making it a practical solution
for real-world applications. Our contributions include new synthetic datasets,
a practical detection framework, and empirical evidence highlighting the
urgency of adapting spam detection to the LLM era. Our code and datasets are
available at: https://anonymous.4open.science/r/FraudSquad-5389/.

</details>


### [141] [Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical Errors](https://arxiv.org/abs/2510.01831)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.CL

TL;DR: 本论文发现大语言模型（LLMs）在遇到语法结构与训练数据不同的数学问题时容易出错，这种失败多由对语法结构的不适应引起，而非数学能力不足。通过对问题语法改写和复杂度度量，验证了结构复杂性越高，出错率越高。对模型进行语法改写可以提升其解题表现。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在数学解题方面表现强劲，但遇到与训练时语法差异较大的问题时经常失败。作者希望探索这类失败的根源，并寻找改善模型泛化能力的方法。

Method: 作者将模型答错的问题按照正确示例的语法模板进行改写，保持语义不变，同时用依存局部性理论（DLT）衡量句法复杂度。分析了语法改写对模型表现的影响，及语法复杂度与出错率的相关性。

Result: 将模型答错的复杂语法问题简化后，模型往往能答对。DLT评分越高（即语法越复杂），模型出错率越高。该模式在多个数据集上均已验证。

Conclusion: LLMs推理错误往往源于问题表面语法结构与模型内部表示的不匹配，而非理解或知识欠缺。考虑语法结构、进行语法简化或改写，能揭示并缓解这种归纳偏差和失误。

Abstract: Large Language Models (LLMs) demonstrate strong mathematical problem-solving
abilities but frequently fail on problems that deviate syntactically from their
training distribution. We identify a systematic failure mode, syntactic blind
spots, in which models misapply familiar reasoning strategies to problems that
are semantically straightforward but phrased in unfamiliar ways. These errors
are not due to gaps in mathematical competence, but rather reflect a brittle
coupling between surface form and internal representation. To test this, we
rephrase incorrectly answered questions using syntactic templates drawn from
correct examples. These rephrasings, which preserve semantics while reducing
structural complexity, often lead to correct answers. We quantify syntactic
complexity using a metric based on Dependency Locality Theory (DLT), and show
that higher DLT scores are associated with increased failure rates across
multiple datasets. Our findings suggest that many reasoning errors stem from
structural misalignment rather than conceptual difficulty, and that
syntax-aware interventions can reveal and mitigate these inductive failures.

</details>


### [142] [SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with Reinforcement Learning](https://arxiv.org/abs/2510.01832)
*Shicheng Liu,Kai Sun,Lisheng Fu,Xilun Chen,Xinyuan Zhang,Zhaojiang Lin,Rulin Shao,Yue Liu,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: 本文提出了一种高效的Web半结构化内容提取方法SCRIBES。该方法通过利用网页布局的相似性自动生成可复用的抽取脚本，实现了在网页群组级别的快速、低资源信息抽取，在脚本质量和下游问答任务上均优于现有技术。


<details>
  <summary>Details</summary>
Motivation: HTML表格、列表和信息框等半结构化内容是网络事实数据的重要来源，但格式多样且复杂，信息抽取难度大，现有方法泛化性差或资源消耗高。

Method: SCRIBES框架结合强化学习和网页结构相似性作为奖励信号，对同一网站内结构相似网页生成可通用的抽取脚本。通过对大规模真实网络数据迭代训练，并合成标注提升方法表现。

Result: 在脚本质量上，SCRIBES超越强基线13%以上；在下游基于GPT-4o的问答任务中，提升4%以上的信息提取准确率，显示出优异的扩展性与资源利用率。

Conclusion: SCRIBES为大规模Web半结构化信息抽取提供了高效、可扩展的新方案，有助于推动网络信息自动利用和相关应用发展。

Abstract: Semi-structured content in HTML tables, lists, and infoboxes accounts for a
substantial share of factual data on the web, yet the formatting complicates
usage, and reliably extracting structured information from them remains
challenging. Existing methods either lack generalization or are
resource-intensive due to per-page LLM inference. In this paper, we introduce
SCRIBES (SCRIpt-Based Semi-Structured Content Extraction at Web-Scale), a novel
reinforcement learning framework that leverages layout similarity across
webpages within the same site as a reward signal. Instead of processing each
page individually, SCRIBES generates reusable extraction scripts that can be
applied to groups of structurally similar webpages. Our approach further
improves by iteratively training on synthetic annotations from in-the-wild
CommonCrawl data. Experiments show that our approach outperforms strong
baselines by over 13% in script quality and boosts downstream question
answering accuracy by more than 4% for GPT-4o, enabling scalable and
resource-efficient web information extraction.

</details>


### [143] [Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models](https://arxiv.org/abs/2510.01845)
*Ece Takmaz,Lisa Bylinina,Jakub Dotlacil*

Main category: cs.CL

TL;DR: 本论文针对现有视觉-语言模型参数量大、依赖超大数据集、与儿童自然语言学习暴露数据量不符的问题，采用发展合理的数据集开发低资源下的语言模型和多模态模型，重点提升多模态模型在纯语言任务上的能力，通过模型融合一定程度缓解多模态模型在语法等纯语言任务下的性能不足。


<details>
  <summary>Details</summary>
Motivation: 主流视觉-语言模型在训练中使用了远超儿童语言习得时接触的语料量，这引出了与婴幼儿语言学习机制不一致的问题。为了更符合人类认知与学习过程，研究低资源、发展合适的数据集上的多模态学习方法成为关键动力。

Method: 作者开发了在低资源设置下的语言模型和多模态模型，并在BabyLM挑战的多模态赛道上进行实验。为改善多模态模型在纯语言任务的表现，作者提出使用“模型融合”方法，将多模态模型和语言模型的参数通过加权线性插值融合。

Result: 提出的多模态模型在BabyLM挑战中超越了先前基线。实验证实多模态模型在纯语言（尤其语法）任务上表现较差，而模型融合方法能够在一定程度上改善纯语言任务表现，同时维持多模态性能。

Conclusion: 多模态模型虽在语言与感知一体化方面表现优异，但直接用于纯语言任务（如语法）时仍有不足。通过和纯文本模型参数融合，可以兼顾多模态与语言能力，缓解互相兼容时的性能损失，为发展低资源下类人学习的多模态模型提供了有效策略。

Abstract: State-of-the-art vision-and-language models consist of many parameters and
learn from enormous datasets, surpassing the amounts of linguistic data that
children are exposed to as they acquire a language. This paper presents our
approach to the multimodal track of the BabyLM challenge addressing this
discrepancy. We develop language-only and multimodal models in low-resource
settings using developmentally plausible datasets, with our multimodal models
outperforming previous BabyLM baselines. One finding in the multimodal language
model literature is that these models tend to underperform in
\textit{language-only} tasks. Therefore, we focus on maintaining language-only
abilities in multimodal models. To this end, we experiment with \textit{model
merging}, where we fuse the parameters of multimodal models with those of
language-only models using weighted linear interpolation. Our results
corroborate the findings that multimodal models underperform in language-only
benchmarks that focus on grammar, and model merging with text-only models can
help alleviate this problem to some extent, while maintaining multimodal
performance.

</details>


### [144] [REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration](https://arxiv.org/abs/2510.01879)
*Yisu Wang,Ming Wang,Haoyuan Song,Wenjie Huang,Chaozheng Wang,Yi Xie,Xuming Ran*

Main category: cs.CL

TL;DR: REPAIR提出了一种新的大模型后编辑框架，实现了精准且低成本地修改模型知识，并显著减少“遗忘”和副作用。


<details>
  <summary>Details</summary>
Motivation: 大模型后训练（Post-training）面临知识更新高成本、错误修正困难，以及多次微调容易引入副作用等问题。作者希望找到一种能够低成本、精准、稳定地处理模型信息修改的方法。

Method: 提出REPAIR框架，核心包括：闭环反馈机制（确保修改中的动态调整）、动态记忆管理（优化知识的存储和检索）、频繁知识融合（促进编辑信息整合）、严格的局部保护措施（限制影响范围），以此控制连续多次编辑引发的不稳定与冲突，同时保持非目标知识的完整性。

Result: 在多个大模型家族测试，REPAIR能提升10%-30%的编辑准确率，并显著减少知识遗忘现象，性能优于传统无分布假设的编辑方法。

Conclusion: REPAIR为大模型持续演进提供了高可靠性、高可扩展性的编辑框架，可支撑模型长期迭代和知识更新，对持续性大模型开发有重要价值。

Abstract: Post-training for large language models (LLMs) is constrained by the high
cost of acquiring new knowledge or correcting errors and by the unintended side
effects that frequently arise from retraining. To address these issues, we
introduce REPAIR (Robust Editing via Progressive Adaptive Intervention and
Reintegration), a lifelong editing framework designed to support precise and
low-cost model updates while preserving non-target knowledge. REPAIR mitigates
the instability and conflicts of large-scale sequential edits through a
closed-loop feedback mechanism coupled with dynamic memory management.
Furthermore, by incorporating frequent knowledge fusion and enforcing strong
locality guards, REPAIR effectively addresses the shortcomings of traditional
distribution-agnostic approaches that often overlook unintended ripple effects.
Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%
across multiple model families and significantly reduces knowledge forgetting.
This work introduces a robust framework for developing reliable, scalable, and
continually evolving LLMs.

</details>


### [145] [Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey](https://arxiv.org/abs/2510.01925)
*Qiyuan Liu,Hao Xu,Xuhong Chen,Wei Chen,Yee Whye Teh,Ning Miao*

Main category: cs.CL

TL;DR: 本文系统介绍了奖励模型（RMs）及其在大语言模型（LLM）推理中的应用，并总结了关键问题与未来方向。


<details>
  <summary>Details</summary>
Motivation: 奖励模型被认为是提升LLM推理能力的关键，既能作为强化学习微调的训练信号，也能在推理阶段选择最佳答案，但相关综述和分析尚不充分。

Method: 作者首先梳理了奖励模型的基本概念、架构、训练方法和评估方式，接着系统归纳了其在LLM推理中的三大核心应用，并结合已有研究与自身实验探讨了RM选择、泛化、评估及增强等关键问题。

Result: 论文系统总结了奖励模型在大语言模型中的主要应用方向及挑战，基于理论和实证分析，提出了多个实现和优化RM的实际建议。

Conclusion: 本综述为奖励模型在LLM推理中的部署和发展提供了指南，对后续RM相关研究和实际应用具有重要参考价值。

Abstract: Reward models (RMs) play a critical role in enhancing the reasoning
performance of LLMs. For example, they can provide training signals to finetune
LLMs during reinforcement learning (RL) and help select the best answer from
multiple candidates during inference. In this paper, we provide a systematic
introduction to RMs, along with a comprehensive survey of their applications in
LLM reasoning. We first review fundamental concepts of RMs, including their
architectures, training methodologies, and evaluation techniques. Then, we
explore their key applications: (1) guiding generation and selecting optimal
outputs during LLM inference, (2) facilitating data synthesis and iterative
self-improvement for LLMs, and (3) providing training signals in RL-based
finetuning. Finally, we address critical open questions regarding the
selection, generalization, evaluation, and enhancement of RMs, based on
existing research and our own empirical findings. Our analysis aims to provide
actionable insights for the effective deployment and advancement of RMs for LLM
reasoning.

</details>


### [146] [Inverse Language Modeling towards Robust and Grounded LLMs](https://arxiv.org/abs/2510.01929)
*Davide Gabrielli,Simone Sestito,Iacopo Masi*

Main category: cs.CL

TL;DR: 作者提出了一种名为逆向语言建模（ILM）的新框架，用于提升大模型的鲁棒性和可控性。


<details>
  <summary>Details</summary>
Motivation: 当前，大语言模型（LLMs）的防御手段零散且不成熟，缺乏类似分类器领域的系统化方案。因此，需要开发更统一且有效的机制，以增强LLMs对于输入扰动的鲁棒性，并确保其输出的可控与安全。

Method: 提出“逆向语言建模（ILM）”框架，该框架让模型能够对输入扰动更加鲁棒，同时通过“反转”输出追溯潜在的有害输入，实现模型的原生“定位”功能。此外，ILM将LLM从静态的生成器转变为可分析、可控和鲁棒的系统，有助于RED teaming等安全评估工作。

Result: ILM能够同时提升大模型的鲁棒性和安全性，使其能够识别并追踪可能导致不当输出的输入触发条件。

Conclusion: ILM有望推动新一代大语言模型的发展，使其不仅更鲁棒和安全，还更具可控性和可信度。代码已开源，为社区提供了落地实现。

Abstract: The current landscape of defensive mechanisms for LLMs is fragmented and
underdeveloped, unlike prior work on classifiers. To further promote
adversarial robustness in LLMs, we propose Inverse Language Modeling (ILM), a
unified framework that simultaneously 1) improves the robustness of LLMs to
input perturbations, and, at the same time, 2) enables native grounding by
inverting model outputs to identify potentially toxic or unsafe input triggers.
ILM transforms LLMs from static generators into analyzable and robust systems,
potentially helping RED teaming. ILM can lay the foundation for next-generation
LLMs that are not only robust and grounded but also fundamentally more
controllable and trustworthy. The code is publicly available at
github.com/davegabe/pag-llm.

</details>


### [147] [Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning](https://arxiv.org/abs/2510.01932)
*Qi He,Cheng Qian,Xiusi Chen,Bingxiang He,Yi R.,Fung,Heng Ji*

Main category: cs.CL

TL;DR: 本文提出了一种用于在线事实核查的大型语言模型强化学习框架Veri-R1，通过与检索系统动态交互显著提升了核查能力，并开放代码以推动社区发展。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的主张（事实）核查因其推理能力而备受关注，但现有方法多依赖提示工程或预设流程，缺乏统一有效的训练范式，难以反映真实世界复杂的检索与推理场景。

Method: 提出了Veri-R1框架，使LLM与搜索引擎交互，依据奖励信号引导模型在规划、检索和推理方面优化行为，实现在线强化训练，动态培养复杂核查技能。

Result: Veri-R1框架在实验中提升联合准确率最高达30%，证据得分翻倍，多项指标优于规模更大的模型。消融实验展现奖励机制和输出与准确率的关联。

Conclusion: 在线强化学习方法对提高主张核查的准确性和可信度非常有效，为相关研究提供了新基础。作者已开源代码促进社区进一步发展。

Abstract: Claim verification with large language models (LLMs) has recently attracted
considerable attention, owing to their superior reasoning capabilities and
transparent verification pathways compared to traditional answer-only
judgments. Online claim verification requires iterative evidence retrieval and
reasoning, yet existing approaches mainly rely on prompt engineering or
predesigned reasoning workflows without offering a unified training paradigm to
improve necessary skills. Therefore, we introduce Veri-R1, an online
reinforcement learning (RL) framework that enables an LLM to interact with a
search engine and to receive reward signals that explicitly shape its planning,
retrieval, and reasoning behaviors. The dynamic interaction between models and
retrieval systems more accurately reflects real-world verification scenarios
and fosters comprehensive verification skills. Empirical results show that
Veri-R1 improves joint accuracy by up to 30% and doubles evidence score, often
surpassing larger-scale counterparts. Ablation studies further reveal the
impact of reward components and the link between output logits and label
accuracy. Our results highlight the effectiveness of online RL for precise and
faithful claim verification and provide a foundation for future research. We
release our code to support community progress in LLM empowered claim
verification.

</details>


### [148] [Taking a SEAT: Predicting Value Interpretations from Sentiment, Emotion, Argument, and Topic Annotations](https://arxiv.org/abs/2510.01976)
*Adina Nicola Dobrinoiu,Ana Cristiana Marcu,Amir Homayounirad,Luciano Cavalcante Siebert,Enrico Liscio*

Main category: cs.CL

TL;DR: 本文探讨了如何通过结合个体在情感、情绪、论证和主题等多维主观标注信息，提升大语言模型对于个体价值观解读的预测能力。实验结果表明，多维信息综合输入显著优于单一维度和无个人信息输入的基线方法。


<details>
  <summary>Details</summary>
Motivation: 价值观的解读具有主观性，受社会文化背景和个人经验影响。现有AI系统往往忽视个体间的差异，导致偏向主流观点，难以实现真正多元和公平的价值对齐，因此有必要研究如何利用个体主观标注行为更好地预测和适应不同的人类价值观。

Method: 作者采用SEAT（情感、情绪、论证、主题）四个维度的主观标注作为个体视角的代理变量，设计实验向语言模型提供标注示例，对比只提供部分或不提供个体标注信息，对其预测价值观解读的准确率进行评估，涵盖zero-shot与few-shot两种场景。

Result: 实验结果显示，综合输入全部SEAT维度信息时，模型表现优于仅用单一维度和不提供个人信息的基线。不同个体标注者在模型预测结果上存在明显差异，凸显了个体主观性的重要性。

Conclusion: 多维主观标注行为可显著增强AI系统对个体价值观的预测能力，强调应在模型设计中纳入个体主观差异。本研究首次从注释行为切入而非仅基于人口统计，为后续大规模验证和多元价值对齐AI的发展奠定了基础。

Abstract: Our interpretation of value concepts is shaped by our sociocultural
background and lived experiences, and is thus subjective. Recognizing
individual value interpretations is important for developing AI systems that
can align with diverse human perspectives and avoid bias toward majority
viewpoints. To this end, we investigate whether a language model can predict
individual value interpretations by leveraging multi-dimensional subjective
annotations as a proxy for their interpretive lens. That is, we evaluate
whether providing examples of how an individual annotates Sentiment, Emotion,
Argument, and Topics (SEAT dimensions) helps a language model in predicting
their value interpretations. Our experiment across different zero- and few-shot
settings demonstrates that providing all SEAT dimensions simultaneously yields
superior performance compared to individual dimensions and a baseline where no
information about the individual is provided. Furthermore, individual
variations across annotators highlight the importance of accounting for the
incorporation of individual subjective annotators. To the best of our
knowledge, this controlled setting, although small in size, is the first
attempt to go beyond demographics and investigate the impact of annotation
behavior on value prediction, providing a solid foundation for future
large-scale validation.

</details>


### [149] [Exploring Database Normalization Effects on SQL Generation](https://arxiv.org/abs/2510.01989)
*Ryosuke Kohita*

Main category: cs.CL

TL;DR: 本论文系统研究了数据库模式规范化（尤其是范式化）对NL2SQL系统性能的影响，发现在不同规范化水平下，大模型在不同类型查询上表现不同。


<details>
  <summary>Details</summary>
Motivation: 以往NL2SQL研究大多在固定的数据库模式下评估系统，忽视了数据库设计（特别是规范化水平）对系统性能的实际影响，因此需要系统分析其作用。

Method: 作者设计了包括一范式至三范式的人工合成数据集和真实世界学术论文数据集，对八个主流大模型在不同规范化水平下的NL2SQL能力进行评测，涵盖简单检索和聚合查询，并控制例子数量以考察few-shot与zero-shot的影响。

Result: 结果发现：非规范化（低范式）模式在简单检索查询下提升准确率，且对计算资源要求低；较高范式（2NF/3NF）模式在聚合查询下表现较好，因其能减少数据重复和NULL值带来的问题。出错主要集中在基础表选择与连接类型预测，但提供few-shot范例能显著缓解。

Conclusion: 论文认为，NL2SQL系统的最优模式设计需结合支持的查询类型；开发NL2SQL接口时应充分考虑模式规范化影响，甚至结合自适应模式选择以适应真实应用场景。

Abstract: Schema design, particularly normalization, is a critical yet often overlooked
factor in natural language to SQL (NL2SQL) systems. Most prior research
evaluates models on fixed schemas, overlooking the influence of design on
performance. We present the first systematic study of schema normalization's
impact, evaluating eight leading large language models on synthetic and
real-world datasets with varied normalization levels. We construct controlled
synthetic datasets with formal normalization (1NF-3NF) and real academic paper
datasets with practical schemes. Our results show that denormalized schemas
offer high accuracy on simple retrieval queries, even with cost-effective
models in zero-shot settings. In contrast, normalized schemas (2NF/3NF)
introduce challenges such as errors in base table selection and join type
prediction; however, these issues are substantially mitigated by providing
few-shot examples. For aggregation queries, normalized schemas yielded better
performance, mainly due to their robustness against the data duplication and
NULL value issues that cause errors in denormalized schemas. These findings
suggest that the optimal schema design for NL2SQL applications depends on the
types of queries to be supported. Our study demonstrates the importance of
considering schema design when developing NL2SQL interfaces and integrating
adaptive schema selection for real-world scenarios.

</details>


### [150] [LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and Target](https://arxiv.org/abs/2510.01995)
*Md Arid Hasan,Firoj Alam,Md Fahad Hossain,Usman Naseem,Syed Ishtiaque Ahmed*

Main category: cs.CL

TL;DR: 本文提出了首个多任务孟加拉语仇恨言论数据集BanglaMultiHate，并通过一系列实验评估了多种模型在孟加拉语低资源环境下对仇恨言论的检测能力，结果为开发更适合本地文化的内容监控工具奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 现有针对孟加拉语仇恨言论的资源和模型功能单一，无法有效区分多维度（类型、严重性、目标对象）的仇恨内容，且孟加拉语作为低资源语言内容监管能力有限。因此亟需大规模、多任务的数据集和系统性效果评估，推动更精确的言论监管工具研发。

Method: 作者构建并人工标注了一个大规模、多任务（区分仇恨类型、严重性、目标等）的孟加拉语仇恨言论数据集BanglaMultiHate。基于该数据集，系统评测了传统机器学习方法、单语预训练模型及大型语言模型（LLM）在零样本提示和LoRA微调下的表现，对比分析了不同方法的优劣。

Result: 实验证明，经过LoRA微调的LLM与BanglaBERT达到了相近的性能，但基于当地文化和语言特性的预训练模型在鲁棒性上依然更强。展示了在低资源语境下LLM的适应性和存在的不足。

Conclusion: BanglaMultiHate数据集和相关实验为孟加拉语等低资源语言开发更加本地化的内容监管工具建立了新的基准，并强化了文化、语言适应性预训练模型的重要性。作者承诺公开数据和脚本，促进后续研究与再现。

Abstract: Online social media platforms are central to everyday communication and
information seeking. While these platforms serve positive purposes, they also
provide fertile ground for the spread of hate speech, offensive language, and
bullying content targeting individuals, organizations, and communities. Such
content undermines safety, participation, and equity online. Reliable detection
systems are therefore needed, especially for low-resource languages where
moderation tools are limited. In Bangla, prior work has contributed resources
and models, but most are single-task (e.g., binary hate/offense) with limited
coverage of multi-facet signals (type, severity, target). We address these gaps
by introducing the first multi-task Bangla hate-speech dataset,
BanglaMultiHate, one of the largest manually annotated corpus to date. Building
on this resource, we conduct a comprehensive, controlled comparison spanning
classical baselines, monolingual pretrained models, and LLMs under zero-shot
prompting and LoRA fine-tuning. Our experiments assess LLM adaptability in a
low-resource setting and reveal a consistent trend: although LoRA-tuned LLMs
are competitive with BanglaBERT, culturally and linguistically grounded
pretraining remains critical for robust performance. Together, our dataset and
findings establish a stronger benchmark for developing culturally aligned
moderation tools in low-resource contexts. For reproducibility, we will release
the dataset and all related scripts.

</details>


### [151] [Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models](https://arxiv.org/abs/2510.02025)
*Donghoon Jung,Jiwoo Choi,Songeun Chae,Seohyon Jung*

Main category: cs.CL

TL;DR: 本研究关注大模型作为写作主体的创作过程，提出通过约束条件分析其创造性，并发现在创作决策中模型更偏好风格元素。


<details>
  <summary>Details</summary>
Motivation: 以往评估大模型创造性多聚焦于结果，忽视了影响生成结果的内部决策过程。作者试图借助叙事学理论，从过程角度理解模型的作者创造力。

Method: 采用受控提示词，给大模型分配不同作者身份，再通过分析其对叙事元素（风格、角色、事件、设定等）的取舍，系统考察模型对各元素的偏好与理由。

Result: 实验发现，不同大模型普遍更重视写作风格而非角色、事件或设定等其他元素；且分析模型给出的决策理由可以发现不同模型呈现出有特征化的创新偏好。

Conclusion: 这种过程导向且借助约束条件厘定的分析方式，为系统化评价AI的创作主体性与风格提供了一种新工具，拓展了AI创造性研究的视角。

Abstract: Evaluations of large language models (LLMs)' creativity have focused
primarily on the quality of their outputs rather than the processes that shape
them. This study takes a process-oriented approach, drawing on narratology to
examine LLMs as computational authors. We introduce constraint-based
decision-making as a lens for authorial creativity. Using controlled prompting
to assign authorial personas, we analyze the creative preferences of the
models. Our findings show that LLMs consistently emphasize Style over other
elements, including Character, Event, and Setting. By also probing the
reasoning the models provide for their choices, we show that distinctive
profiles emerge across models and argue that our approach provides a novel
systematic tool for analyzing AI's authorial creativity.

</details>


### [152] [Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage](https://arxiv.org/abs/2510.02044)
*Siddhant Arora,Haidar Khan,Kai Sun,Xin Luna Dong,Sajal Choudhary,Seungwhan Moon,Xinyuan Zhang,Adithya Sagar,Surya Teja Appini,Kaushik Patnaik,Sanat Sharma,Shinji Watanabe,Anuj Kumar,Ahmed Aly,Yue Liu,Florian Metze,Zhaojiang Lin*

Main category: cs.CL

TL;DR: 该论文提出了一种新的语音端到端对话系统框架，通过流式检索增强生成（Streaming RAG）技术显著提升了系统的准确率和交互实时性，有效缓解了工具接入导致的延迟和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 尽管端到端语音对话系统相较于传统ASR-LLM-TTS流程具备自然性和低延迟等优势，但其易产生虚假内容（幻觉），且缺乏事实支撑。文本对话系统已能结合工具（如网络搜索、知识图谱API），但该能力尚未迁移到语音对话系统中。本文旨在解决端到端语音系统的事实依赖与延迟难题。

Method: 提出Streaming RAG框架，通过后训练流程让系统能在用户语音未完成时并行预测和调用检索工具，提前发起外部查询。系统学习如何在说话过程中发起工具调用，并将语音查询结果与检索到的文本自动生成语音总结回答，提升了准确率与响应效率。借助自构造的AudioCRAG语音数据集进行实验验证。

Result: 实验表明，该方法将语音问答准确率从11.1%提升至34.2%（提高约200%），同时工具调用的响应延迟降低了20%。

Conclusion: Streaming RAG框架有效提升了语音端到端对话系统的事实性、准确率与响应速度，并具备模态无关性，可推广至文本输入等场景，为实时智能助理的发展奠定了基础。

Abstract: End-to-end speech-in speech-out dialogue systems are emerging as a powerful
alternative to traditional ASR-LLM-TTS pipelines, generating more natural,
expressive responses with significantly lower latency. However, these systems
remain prone to hallucinations due to limited factual grounding. While
text-based dialogue systems address this challenge by integrating tools such as
web search and knowledge graph APIs, we introduce the first approach to extend
tool use directly into speech-in speech-out systems. A key challenge is that
tool integration substantially increases response latency, disrupting
conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented
Generation (Streaming RAG), a novel framework that reduces user-perceived
latency by predicting tool queries in parallel with user speech, even before
the user finishes speaking. Specifically, we develop a post-training pipeline
that teaches the model when to issue tool calls during ongoing speech and how
to generate spoken summaries that fuse audio queries with retrieved text
results, thereby improving both accuracy and responsiveness. To evaluate our
approach, we construct AudioCRAG, a benchmark created by converting queries
from the publicly available CRAG dataset into speech form. Experimental results
demonstrate that our streaming RAG approach increases QA accuracy by up to 200%
relative (from 11.1% to 34.2% absolute) and further enhances user experience by
reducing tool use latency by 20%. Importantly, our streaming RAG approach is
modality-agnostic and can be applied equally to typed input, paving the way for
more agentic, real-time AI assistants.

</details>


### [153] [Chain-of-Thought Reasoning in Streaming Full-Duplex End-to-End Spoken Dialogue Systems](https://arxiv.org/abs/2510.02066)
*Siddhant Arora,Jinchuan Tian,Hayato Futami,Jiatong Shi,Yosuke Kashiwagi,Emiru Tsunoo,Shinji Watanabe*

Main category: cs.CL

TL;DR: 本文提出了一种新的端到端语音对话系统SCoT，能更好处理用户输入和系统响应，提升对话连贯性与解释性，并支持低延迟重叠对话。


<details>
  <summary>Details</summary>
Motivation: 现有端到端语音对话系统大多依赖VAD进行分段，但无法区分暂停与说话结束，影响交互自然性。复杂的Duplex模型虽有所改进，但架构过于复杂且语义推理能力较弱。

Method: 提出一种流式Chain-of-Thought（SCoT）框架，采用块级（blockwise）方式交替处理用户输入与系统响应，并借助帧级对齐创建中间目标，提高系统交互流畅性和响应解释性。

Result: 实验显示，SCoT方法在语句连贯性和可解释性上优于现有的duplex系统，并在低延迟与支持重叠交互方面优于传统轮流系统。

Conclusion: SCoT不仅简化架构，也能更自然、连贯地处理语音对话，克服了VAD局限和duplex模型的复杂性，推动了端到端语音对话系统的发展。

Abstract: Most end-to-end (E2E) spoken dialogue systems (SDS) rely on voice activity
detection (VAD) for turn-taking, but VAD fails to distinguish between pauses
and turn completions. Duplex SDS models address this by predicting output
continuously, including silence tokens, thus removing the need for explicit
VAD. However, they often have complex dual-channel architecture and lag behind
cascaded models in semantic reasoning. To overcome these challenges, we propose
SCoT: a Streaming Chain-of-Thought (CoT) framework for Duplex SDS, alternating
between processing fixed-duration user input and generating responses in a
blockwise manner. Using frame-level alignments, we create intermediate
targets-aligned user transcripts and system responses for each block.
Experiments show that our approach produces more coherent and interpretable
responses than existing duplex methods while supporting lower-latency and
overlapping interactions compared to turn-by-turn systems.

</details>


### [154] [The Disparate Impacts of Speculative Decoding](https://arxiv.org/abs/2510.02128)
*Jameson Sandler,Ahmet Üstün,Marco Romanelli,Sara Hooker,Ferdinando Fioretto*

Main category: cs.CL

TL;DR: 本论文分析了推测解码在不同任务上的加速效果不公平现象，并提出了缓解方法，提升了整体公平性12%。


<details>
  <summary>Details</summary>
Motivation: 推测解码已成为大语言模型推理加速的主流技术，但作者发现不同任务上的加速比例并不一致，尤其对训练不足或样本较少的任务加速效果明显降低。这种“不公平”可能影响模型实际应用，因此需要全面分析和解决。

Method: 作者系统分析了推测解码在不同任务中的加速表现，对比了拟合良好与拟合不良任务的加速差异，并提出并量化了加速“不公平性”。基于分析结果，提出了一种减少加速差异的方法，并通过实验证明了其有效性。

Result: 实验证明，原始推测解码确实在部分任务上存在明显的加速不公平性。作者提出的方法能在多个模型对组合上有效减小这一差异，公平性评价指标平均提高了12%。

Conclusion: 推测解码加速在任务间存在不公平现象，分析及新提出的方法可以有效缓解这一问题，有助于更公平高效地应用推测解码。

Abstract: The practice of speculative decoding, whereby inference is probabilistically
supported by a smaller, cheaper, ``drafter'' model, has become a standard
technique for systematically reducing the decoding time of large language
models. This paper conducts an analysis of speculative decoding through the
lens of its potential disparate speed-up rates across tasks. Crucially, the
paper shows that speed-up gained from speculative decoding is not uniformly
distributed across tasks, consistently diminishing for under-fit, and often
underrepresented tasks. To better understand this phenomenon, we derive an
analysis to quantify this observed ``unfairness'' and draw attention to the
factors that motivate such disparate speed-ups to emerge. Further, guided by
these insights, the paper proposes a mitigation strategy designed to reduce
speed-up disparities and validates the approach across several model pairs,
revealing on average a 12% improvement in our fairness metric.

</details>


### [155] [RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with Self-Penalization](https://arxiv.org/abs/2510.02172)
*Zhaoning Yu,Will Su,Leitian Tao,Haozhu Wang,Aashu Singh,Hanchao Yu,Jianyu Wang,Hongyang Gao,Weizhe Yuan,Jason Weston,Ping Yu,Jing Xu*

Main category: cs.CL

TL;DR: 该论文提出了RESTRAIN（REinforcement learning with Self-restraint）框架，实现仅用无标签数据训练推理大模型，在多个高难推理任务上取得显著提升，几乎达到有标签数据训练的效果。


<details>
  <summary>Details</summary>
Motivation: 当前加强学习依赖大量人工标注数据以提升大模型的链式推理能力，成本高且在难题上表现有限。因此，作者希望探索无需人工标注，通过利用无标签数据实现模型自我提升的可行路径。

Method: 提出RESTRAIN自惩罚型RL框架。它不需要金标标签，利用模型输出的整体彩分布信息，在策略优化过程中对过度自信或一致性差的样本施加惩罚，保留有前景的推理链，并与现有RL算法（如GRPO）无缝集成。

Result: 在AIME25、MMLU_STEM和GPQA-Diamond等高难度推理任务上，RESTRAIN框架（结合Qwen3-4B-Base和OctoThinker Hybrid-8B-Base）在无金标标签情况下，使Pass@1指标分别提升140.7%、36.2%和19.6%，表现接近有标注训练。

Conclusion: RESTRAIN方法证明了无需金标标签即可在推理任务上获得大幅提升，为高效扩展更强推理能力模型提供了新途径。

Abstract: Reinforcement learning with human-annotated data has boosted chain-of-thought
reasoning in large reasoning models, but these gains come at high costs in
labeled data while faltering on harder tasks. A natural next step is
experience-driven learning, where models improve without curated labels by
adapting to unlabeled data. We introduce RESTRAIN (REinforcement learning with
Self-restraint), a self-penalizing RL framework that converts the absence of
gold labels into a useful learning signal. Instead of overcommitting to
spurious majority votes, RESTRAIN exploits signals from the model's entire
answer distribution: penalizing overconfident rollouts and low-consistency
examples while preserving promising reasoning chains. The self-penalization
mechanism integrates seamlessly into policy optimization methods such as GRPO,
enabling continual self-improvement without supervision. On challenging
reasoning benchmarks, RESTRAIN delivers large gains using only unlabeled data.
With Qwen3-4B-Base and OctoThinker Hybrid-8B-Base, it improves Pass@1 by up to
+140.7 percent on AIME25, +36.2 percent on MMLU_STEM, and +19.6 percent on
GPQA-Diamond, nearly matching gold-label training while using no gold labels.
These results demonstrate that RESTRAIN establishes a scalable path toward
stronger reasoning without gold labels.

</details>


### [156] [Learning to Reason for Hallucination Span Detection](https://arxiv.org/abs/2510.02173)
*Hsuan Su,Ting-Yao Hu,Hema Swetha Koppula,Kundan Krishna,Hadi Pouransari,Cheng-Yu Hsieh,Cem Koc,Joseph Yitan Cheng,Oncel Tuzel,Raviteja Vemulapalli*

Main category: cs.CL

TL;DR: 该论文提出了一种利用强化学习（RL）方法，结合推理过程（如CoT），以更好地检测大模型输出中的幻觉片段，并在多项任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型容易生成虚假内容，现有检测方法通常仅做二分类判断，无法实现实际应用中需要的片段级幻觉识别，因此需要更复杂、更细致的检测方法。

Method: 作者分析了带/不带Chain-of-Thought（CoT）推理的预训练模型表现，并提出了RL4HS强化学习框架。该框架采用基于片段奖励的Group Relative Policy Optimization，并引入Class-Aware Policy Optimization来缓解奖励不均衡问题。

Result: 实验证明，在RAGTruth基准（含摘要、问答、数据到文本等任务）上，RL4HS超越了仅用预训练推理模型和有监督微调的方法。

Conclusion: 片段级奖励的强化学习对幻觉片段检测非常必要，RL4HS实现了更高效准确的检测，推动了大模型输出可信度的提升。

Abstract: Large language models (LLMs) often generate hallucinations -- unsupported
content that undermines reliability. While most prior works frame hallucination
detection as a binary task, many real-world applications require identifying
hallucinated spans, which is a multi-step decision making process. This
naturally raises the question of whether explicit reasoning can help the
complex task of detecting hallucination spans. To answer this question, we
first evaluate pretrained models with and without Chain-of-Thought (CoT)
reasoning, and show that CoT reasoning has the potential to generate at least
one correct answer when sampled multiple times. Motivated by this, we propose
RL4HS, a reinforcement learning framework that incentivizes reasoning with a
span-level reward function. RL4HS builds on Group Relative Policy Optimization
and introduces Class-Aware Policy Optimization to mitigate reward imbalance
issue. Experiments on the RAGTruth benchmark (summarization, question
answering, data-to-text) show that RL4HS surpasses pretrained reasoning models
and supervised fine-tuning, demonstrating the necessity of reinforcement
learning with span-level rewards for detecting hallucination spans.

</details>


### [157] [From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens](https://arxiv.org/abs/2510.02292)
*Hala Sheta,Eric Huang,Shuyu Wu,Ilia Alenabi,Jiajun Hong,Ryker Lin,Ruoxi Ning,Daniel Wei,Jialin Yang,Jiawei Zhou,Ziqiao Ma,Freda Shi*

Main category: cs.CL

TL;DR: VLM-Lens是一个专为视觉-语言模型（VLMs）设计的开源工具包，可系统性地进行中间层输出提取、基准测试和分析。它为研究人员提供了统一且友好的界面，简化了不同模型的操作与扩展。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLMs）越来越复杂，不同架构的模型解释难度大，缺乏统一、便捷的工具来系统性地抽取和分析模型中间层输出，从而阻碍了理解和改进VLMs的进展。

Method: 作者提出了VLM-Lens工具包，通过YAML配置，支持对16款主流VLM及其30余种变体的任意中间层输出进行便捷提取和分析，同时可以很容易扩展到新的模型。该工具可集成多种可解释性分析方法。

Result: 作者用两个简单分析实验展示了该工具的用法，揭示了不同VLM模型在各层及概念目标上的隐藏表示存在系统性差异。

Conclusion: VLM-Lens作为开源工具，有助于推动学界对视觉-语言模型内部机制的理解和模型的改进与优化。

Abstract: We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,
analysis, and interpretation of vision-language models (VLMs) by supporting the
extraction of intermediate outputs from any layer during the forward pass of
open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that
abstracts away model-specific complexities and supports user-friendly operation
across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and
their over 30 variants, and is extensible to accommodate new models without
changing the core logic.
  The toolkit integrates easily with various interpretability and analysis
methods. We demonstrate its usage with two simple analytical experiments,
revealing systematic differences in the hidden representations of VLMs across
layers and target concepts. VLM-Lens is released as an open-sourced project to
accelerate community efforts in understanding and improving VLMs.

</details>


### [158] [ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities](https://arxiv.org/abs/2510.02200)
*Felix Brei,Lorenz Bühmann,Johannes Frey,Daniel Gerber,Lars-Peter Meyer,Claus Stadler,Kirill Bulert*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLM的SPINACH智能体，实现自然语言到SPARQL查询的多轮迭代式转换，旨在降低知识图谱查询门槛。


<details>
  <summary>Details</summary>
Motivation: 许多人因不懂SPARQL而难以与知识图谱互动。为降低此门槛，尤其是应对Text2SPARQL挑战，寻求自然语言到SPARQL的高效转换方法。

Method: 设计并实现了基于SPINACH的LLM智能体，采用探索与执行结合的多轮迭代流程，将自然语言问题逐步转为SPARQL查询，并分析智能体的行为与设计架构。

Result: 通过实验与分析，验证了所提方法在Text2SPARQL任务中的有效性，并获得了智能体行为和未来优化方向的洞见。

Conclusion: 基于LLM的迭代式Text2SPARQL转换方法可有效降低用户使用知识图谱的技术门槛，为后续改进和实际应用提供了依据。

Abstract: Interacting with knowledge graphs can be a daunting task for people without a
background in computer science since the query language that is used (SPARQL)
has a high barrier of entry. Large language models (LLMs) can lower that
barrier by providing support in the form of Text2SPARQL translation. In this
paper we introduce a generalized method based on SPINACH, an LLM backed agent
that translates natural language questions to SPARQL queries not in a single
shot, but as an iterative process of exploration and execution. We describe the
overall architecture and reasoning behind our design decisions, and also
conduct a thorough analysis of the agent behavior to gain insights into future
areas for targeted improvements. This work was motivated by the Text2SPARQL
challenge, a challenge that was held to facilitate improvements in the
Text2SPARQL domain.

</details>


### [159] [Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered Mobile-Use Agents](https://arxiv.org/abs/2510.02204)
*Lingzhong Dong,Ziqi Zhou,Shuaibo Yang,Haiyue Sheng,Pengzhou Cheng,Zongru Wu,Zheng Wu,Gongshen Liu,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: 本论文提出了一种新的评估框架，专门用于分析移动端视觉-语言智能体在推理和执行之间的匹配情况，揭示了现有方法中推理-执行缺口的普遍性及其潜在风险。


<details>
  <summary>Details</summary>
Motivation: 虽然引入思维链推理（CoT）能提升视觉-语言模型在移动端任务中的执行准确性，但当前评估方法忽视了推理过程与真实动作的一致性。这导致推理-执行之间可能出现错位，用户容易对模型产生过度信任，进而造成风险，如财产损失等。因此，迫切需要一种能系统反映推理-执行一致性的评估机制。

Method: 作者提出了‘真实一致性’（GTA）指标，用以判断模型推理链条中的动作与标准动作是否一致，并将其与传统的准确匹配（EM）指标结合，形成新的联合评估框架。通过该框架，可以分别检测两类推理-执行缺口：执行缺口（推理正确但执行失败）和推理缺口（执行成功但推理过程出错）。作者在多种移动交互任务和多规模模型下进行了系统实验证明框架有效性。

Result: 实验证实，不同类型的推理-执行缺口在移动交互任务中非常普遍，且执行缺口比推理缺口更常见。即使是大规模先进模型，仍然存在显著的执行缺口。此外，提出的评估框架能有效揭示模型在推理和执行上的系统性不足。

Conclusion: 本研究为评估和诊断移动端视觉-语言智能体在推理与执行间一致性提供了新工具，有助于推动更可信的智能体开发。

Abstract: Mobile-use agents powered by vision-language models (VLMs) have shown great
potential in interpreting natural language instructions and generating
corresponding actions based on mobile graphical user interface. Recent studies
suggest that incorporating chain-of-thought (CoT) reasoning tends to improve
the execution accuracy. However, existing evaluations emphasize execution
accuracy while neglecting whether CoT reasoning aligns with ground-truth
actions. This oversight fails to assess potential reasoning-execution gaps,
which in turn foster over-trust: users relying on seemingly plausible CoTs may
unknowingly authorize harmful actions, potentially resulting in financial loss
or trust crisis. In this work, we introduce a new evaluation framework to
diagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment
(GTA), which measures whether the action implied by a CoT matches the
ground-truth action. By combining GTA with the standard Exact Match (EM)
metric, we jointly assess both the reasoning accuracy and execution accuracy.
This joint perspective reveals two types of reasoning-execution gaps: (i)
Execution Gap (EG), where the reasoning correctly identifies the correct action
but execution fails, and (ii) Reasoning Gap (RG), where execution succeeds but
reasoning process conflicts with the actual execution. Experimental results
across a wide range of mobile interaction tasks reveal that reasoning-execution
gaps are prevalent, with execution gaps occurring more frequently than
reasoning gaps. Moreover, while scaling up model size reduces the overall gap,
sizable execution gaps persist even in the largest models. Further analysis
shows that our framework reliably reflects systematic EG/RG patterns in
state-of-the-art models. These findings offer concrete diagnostics and support
the development of more trustworthy mobile-use agents.

</details>


### [160] [More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration](https://arxiv.org/abs/2510.02227)
*Xiaoyang Yuan,Yujuan Ding,Yi Bin,Wenqi Shao,Jinyu Cai,Jingkuan Song,Yang Yang,Hengtao Shen*

Main category: cs.CL

TL;DR: 本文提出了一种新的自适应多教师引导策略（AMPO），通过在模型需要时引入多位教师模型指导，提升大语言模型（LLM）的推理能力和探索多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的使用可验证奖励的强化学习（RLVR）提升LLM推理能力的方法，主要依赖自我探索或单一教师，容易产生模型偏见、限制探索范围，最终限制了推理多样性和性能。作者受知识蒸馏中的多教师策略启发，以改善这一局限。

Method: 提出AMPO方法，在当前策略（on-policy model）解答错误时，才引入多位教师模型的指导，实现“按需指导”。此外，AMPO增加了一个理解度选择机制，让学生只学习自己最易于理解的推理路径，实现探索与高效利用的平衡。

Result: 在数学推理和分布外任务上，AMPO较强基线（GRPO）分别提升了4.3%和12.2%，Pass@k指标显著提升，同时实现了更广泛的推理探索。使用四个同级教师效果可媲美数据更多的单一强大教师（如DeepSeek-R1）。

Conclusion: AMPO实现了更高效、可扩展的推理与泛化能力提升，证明了多教师按需引导的高效性和实用性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm
for enhancing the reasoning ability in Large Language Models (LLMs). However,
prevailing methods primarily rely on self-exploration or a single off-policy
teacher to elicit long chain-of-thought (LongCoT) reasoning, which may
introduce intrinsic model biases and restrict exploration, ultimately limiting
reasoning diversity and performance. Drawing inspiration from multi-teacher
strategies in knowledge distillation, we introduce Adaptive Multi-Guidance
Policy Optimization (AMPO), a novel framework that adaptively leverages
guidance from multiple proficient teacher models, but only when the on-policy
model fails to generate correct solutions. This "guidance-on-demand" approach
expands exploration while preserving the value of self-discovery. Moreover,
AMPO incorporates a comprehension-based selection mechanism, prompting the
student to learn from the reasoning paths that it is most likely to comprehend,
thus balancing broad exploration with effective exploitation. Extensive
experiments show AMPO substantially outperforms a strong baseline (GRPO), with
a 4.3% improvement on mathematical reasoning tasks and 12.2% on
out-of-distribution tasks, while significantly boosting Pass@k performance and
enabling more diverse exploration. Notably, using four peer-sized teachers, our
method achieves comparable results to approaches that leverage a single, more
powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate
a more efficient and scalable path to superior reasoning and generalizability.
Our code is available at https://github.com/SII-Enigma/AMPO.

</details>


### [161] [Enhanced Arabic-language cyberbullying detection: deep embedding and transformer (BERT) approaches](https://arxiv.org/abs/2510.02232)
*Ebtesam Jaber Aljohani,Wael M. S. Yafoo*

Main category: cs.CL

TL;DR: 本文构建了阿拉伯语X（前身Twitter）上网络欺凌数据集，通过多种深度学习模型对网络欺凌进行自动检测，最佳方法准确率达98%。


<details>
  <summary>Details</summary>
Motivation: 目前主流网络欺凌检测方法主要针对英语，针对阿拉伯语内容的自动检测方法十分稀缺，且社交平台的广泛应用导致青少年更容易受到网络欺凌。因此，提升对阿拉伯语网络欺凌的检测能力具有重要意义。

Method: 作者收集了1万多条阿拉伯语X平台帖子，经预处理和使用kappa工具提升标注质量。实验比较了LSTM、Bi-LSTM、LSTM-BERT、Bi-LSTM-BERT以及Bi-LSTM结合FastText词嵌入等多种深度学习模型，用以检测网络欺凌言论。

Result: LSTM和Bi-LSTM结合BERT预训练模型能达到97%的准确率，Bi-LSTM结合FastText词嵌入的效果更佳，达到了98%的准确率。

Conclusion: 创新深度学习模型，尤其是Bi-LSTM+FastText，提高了阿拉伯语网络欺凌检测的准确率，相关方法具有一定的通用性，有助于社交媒体领域的网络欺凌治理。

Abstract: Recent technological advances in smartphones and communications, including
the growth of such online platforms as massive social media networks such as X
(formerly known as Twitter) endangers young people and their emotional
well-being by exposing them to cyberbullying, taunting, and bullying content.
Most proposed approaches for automatically detecting cyberbullying have been
developed around the English language, and methods for detecting
Arabic-language cyberbullying are scarce. Methods for detecting Arabic-language
cyberbullying are especially scarce. This paper aims to enhance the
effectiveness of methods for detecting cyberbullying in Arabic-language
content. We assembled a dataset of 10,662 X posts, pre-processed the data, and
used the kappa tool to verify and enhance the quality of our annotations. We
conducted four experiments to test numerous deep learning models for
automatically detecting Arabic-language cyberbullying. We first tested a long
short-term memory (LSTM) model and a bidirectional long short-term memory
(Bi-LSTM) model with several experimental word embeddings. We also tested the
LSTM and Bi-LSTM models with a novel pre-trained bidirectional encoder from
representations (BERT) and then tested them on a different experimental models
BERT again. LSTM-BERT and Bi-LSTM-BERT demonstrated a 97% accuracy. Bi-LSTM
with FastText embedding word performed even better, achieving 98% accuracy. As
a result, the outcomes are generalize

</details>


### [162] [AccurateRAG: A Framework for Building Accurate Retrieval-Augmented Question-Answering Applications](https://arxiv.org/abs/2510.02243)
*Linh The Nguyen,Chi Tran,Dung Ngoc Nguyen,Van-Cuong Pham,Hoang Ngo,Dat Quoc Nguyen*

Main category: cs.CL

TL;DR: 提出了AccurateRAG框架，在问答任务中实现了新的性能最优。


<details>
  <summary>Details</summary>
Motivation: 当前基于RAG的问答系统在开发流程、高质量数据生成和效果评估等方面存在痛点，难以高效构建和优化高性能的问答应用。

Method: AccurateRAG提供了一套问答应用开发工具链，涵盖原始数据处理、数据集生成、文本嵌入、LLM微调、输出评估等全流程。系统支持本地搭建RAG方案。

Result: 在标准问答数据集上的实验表明，AccurateRAG在性能上超过了以往RAG方案，达到了新的SOTA。

Conclusion: AccurateRAG显著提高了RAG问答系统的开发效率与任务表现，适合构建高性能问答应用。

Abstract: We introduce AccurateRAG -- a novel framework for constructing
high-performance question-answering applications based on retrieval-augmented
generation (RAG). Our framework offers a pipeline for development efficiency
with tools for raw dataset processing, fine-tuning data generation, text
embedding & LLM fine-tuning, output evaluation, and building RAG systems
locally. Experimental results show that our framework outperforms previous
strong baselines and obtains new state-of-the-art question-answering
performance on benchmark datasets.

</details>


### [163] [Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation](https://arxiv.org/abs/2510.02249)
*Tianyi Jiang,Yi Bin,Yujuan Ding,Kainian Zhu,Fei Ma,Jingkuan Song,Heng Tao Shen*

Main category: cs.CL

TL;DR: 提出了一种帮助大语言模型（LLM）避免不必要冗长推理步骤的新方法，提高了模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在面对简单问题时常生成过长的推理链，影响效率且推理深度难以自适应问题复杂度，因此需要新的方法动态调节推理过程深度。

Method: 作者引入了新的度量指标Token Entropy Cumulative Average（TECA），用于衡量推理过程中信息探索的程度。基于此，提出了“简要探索，然后决策”（Explore Briefly, Then Decide）推理范式，并配套Cumulative Entropy Regulation (CER)机制，引导模型动态判断何时停止推理并给出答案。

Result: 实验证明该方法在多个数学基准任务上显著减少了过度推理现象，在简单数据集上的平均回复长度减少最多达71%，同时不降低解题能力。

Conclusion: 该方法有效提升了大模型的推理效率和自适应性，使其对不同复杂度问题都能调整合适的推理深度。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities
on complex problems using long Chain-of-Thought (CoT) reasoning. However, they
often suffer from overthinking, meaning generating unnecessarily lengthy
reasoning steps for simpler problems. This issue may degrade the efficiency of
the models and make them difficult to adapt the reasoning depth to the
complexity of problems. To address this, we introduce a novel metric Token
Entropy Cumulative Average (TECA), which measures the extent of exploration
throughout the reasoning process. We further propose a novel reasoning paradigm
-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy
Regulation (CER) mechanism. This paradigm leverages TECA to help the model
dynamically determine the optimal point to conclude its thought process and
provide a final answer, thus achieving efficient reasoning. Experimental
results across diverse mathematical benchmarks show that our approach
substantially mitigates overthinking without sacrificing problem-solving
ability. With our thinking paradigm, the average response length decreases by
up to 71% on simpler datasets, demonstrating the effectiveness of our method in
creating a more efficient and adaptive reasoning process.

</details>


### [164] [InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents](https://arxiv.org/abs/2510.02271)
*Yaxin Du,Yuanshuo Zhang,Xiyuan Yang,Yifan Zhou,Cheng Wang,Gongyi Zou,Xianghe Pang,Wenhao Wang,Menglan Chen,Shuo Tang,Zhiyu Li,Siheng Chen*

Main category: cs.CL

TL;DR: 本文提出了InfoMosaic-Bench，这是一套专为多源信息检索而设计的基准，用于评估具备工具调用能力的LLM代理在复杂任务中的表现，并通过实验揭示出当前LLM在工具整合与信息检索方面的显著瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理依赖开放网络搜索来获取信息，但受限于网络内容质量参差不齐及许多实际任务所需的精确、特定领域知识无法通过网络获取。尽管MCP协议允许LLM接入各类专业工具，但尚不清楚LLM能否有效且灵活地整合这些工具来完成复杂、多源的信息检索任务。

Method: 作者提出并构建了InfoMosaic-Bench基准，涵盖医疗、金融、地图、视频、网页、多领域六大代表领域。任务通过InfoMosaic-Flow流水线自动生成，确保任务必须涉及跨来源依赖，由工具验证任务条件，并过滤掉可通过简单查找解决的情况，以保证题目既可靠又具有挑战性。随后用14个主流LLM代理在该基准上做实验性评估。

Result: 实验结果发现：（1）仅靠网络信息不够用，例如GPT-5只能达到38.2%的准确率和67.5%的通过率；（2）专业领域工具在部分领域能提升表现，但并非所有领域都能带来好处，有时反而降低；（3）22.4%的失败归因于LLM对工具使用或选择错误，显示当前模型在工具调用基础操作上仍有短板。

Conclusion: 现有LLM代理在处理多源信息检索和工具综合时依然面临显著挑战，包括工具使用能力有限和跨来源综合能力不足。InfoMosaic-Bench为相关系统的全面评估提供了新基准，有助于推动工具增强LLM在复杂任务中的应用进步。

Abstract: Information seeking is a fundamental requirement for humans. However,
existing LLM agents rely heavily on open-web search, which exposes two
fundamental weaknesses: online content is noisy and unreliable, and many
real-world tasks require precise, domain-specific knowledge unavailable from
the web. The emergence of the Model Context Protocol (MCP) now allows agents to
interface with thousands of specialized tools, seemingly resolving this
limitation. Yet it remains unclear whether agents can effectively leverage such
tools -- and more importantly, whether they can integrate them with
general-purpose search to solve complex tasks. Therefore, we introduce
InfoMosaic-Bench, the first benchmark dedicated to multi-source information
seeking in tool-augmented agents. Covering six representative domains
(medicine, finance, maps, video, web, and multi-domain integration),
InfoMosaic-Bench requires agents to combine general-purpose search with
domain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable
pipeline that grounds task conditions in verified tool outputs, enforces
cross-source dependencies, and filters out shortcut cases solvable by trivial
lookup. This design guarantees both reliability and non-triviality. Experiments
with 14 state-of-the-art LLM agents reveal three findings: (i) web information
alone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass
rate; (ii) domain tools provide selective but inconsistent benefits, improving
some domains while degrading others; and (iii) 22.4% of failures arise from
incorrect tool usage or selection, highlighting that current LLMs still
struggle with even basic tool handling.

</details>


### [165] [Parallel Scaling Law: Unveiling Reasoning Generalization through A Cross-Linguistic Perspective](https://arxiv.org/abs/2510.02272)
*Wen Yang,Junhong Wu,Chong Li,Chengqing Zong,Jiajun Zhang*

Main category: cs.CL

TL;DR: 该论文关注于强化学习后训练（RPT）对大型推理模型（LRM）的多语言推理泛化能力，发现使用英语训练的推理能力难以有效迁移到其他语言，并提出评估跨语言迁移性的指标及相关规律。


<details>
  <summary>Details</summary>
Motivation: 虽已有工作研究任务或模态间的推理泛化能力，但缺乏系统性的跨语言推理泛化研究。随着RPT提升LRM能力，探索英语训练推理能力是否能迁移到其他语言成为亟需解决的问题。

Method: 系统评估以英语为主的LRM在多语言推理基准上的表现，提出定量衡量跨语言迁移性的指标，并通过干预和并行训练实验，分析模型初始能力、目标语言与训练范式对迁移性的影响。

Result: 发现LRM跨语言迁移性由初始模型、目标语言和训练方式影响，初始英语能力强的模型对英语模式依赖过高，跨语言泛化能力反而较差；引入一个平行语言训练可大幅提升泛化能力，跨语言能力随参与训练语言数量按幂律提升，并首次提出"单语泛化差距"指标。

Conclusion: 研究表明英语为中心的推理训练不能充分实现跨语言泛化，模型推理能力存在单语泛化差距，提示未来应开发更具语种无关性的LRM。

Abstract: Recent advancements in Reinforcement Post-Training (RPT) have significantly
enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased
interest in the generalization of RL-based reasoning. While existing work has
primarily focused on investigating its generalization across tasks or
modalities, this study proposes a novel cross-linguistic perspective to
investigate reasoning generalization. This raises a crucial question:
$\textit{Does the reasoning capability achieved from English RPT effectively
transfer to other languages?}$ We address this by systematically evaluating
English-centric LRMs on multilingual reasoning benchmarks and introducing a
metric to quantify cross-lingual transferability. Our findings reveal that
cross-lingual transferability varies significantly across initial model, target
language, and training paradigm. Through interventional studies, we find that
models with stronger initial English capabilities tend to over-rely on
English-specific patterns, leading to diminished cross-lingual generalization.
To address this, we conduct a thorough parallel training study. Experimental
results yield three key findings: $\textbf{First-Parallel Leap}$, a substantial
leap in performance when transitioning from monolingual to just a single
parallel language, and a predictable $\textbf{Parallel Scaling Law}$, revealing
that cross-lingual reasoning transfer follows a power-law with the number of
training parallel languages. Moreover, we identify the discrepancy between
actual monolingual performance and the power-law prediction as
$\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs
fail to fully generalize across languages. Our study challenges the assumption
that LRM reasoning mirrors human cognition, providing critical insights for the
development of more language-agnostic LRMs.

</details>


### [166] [F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data](https://arxiv.org/abs/2510.02294)
*Ziyin Zhang,Zihan Liao,Hang Yu,Peng Di,Rui Wang*

Main category: cs.CL

TL;DR: 本文提出了F2LLM，一套先进的嵌入模型（0.6B、1.7B、4B），在成本、模型规模与表现间达到良好平衡，无需复杂的多阶段预训练和高昂合成数据。各项评测表现优异，并开源，适合后续研究和应用。


<details>
  <summary>Details</summary>
Motivation: 现有领先嵌入模型依赖大量对比预训练、复杂流程和昂贵的合成数据，导致训练成本高且复现困难。研究动机是开发一种成本低、训练流程简单且有强竞争力的嵌入模型体系。

Method: 直接在基础大模型上，用600万组真实开源的query-document-negative三元组进行微调，省略多阶段预训练与合成数据的生成。包含0.6B、1.7B和4B三种模型规模。

Result: F2LLM-4B在MTEB英文排行榜同规模模型中排名第二，总排行第七；F2LLM-1.7B在1B-2B规模范围内排名第一。

Conclusion: F2LLM模型在保证性能的前提下降低了训练和复现门槛，是性价比高的基础嵌入模型新基线，适合被后续研究和应用广泛采用。

Abstract: We introduce F2LLM - Foundation to Feature Large Language Models, a suite of
state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike
previous top-ranking embedding models that require massive contrastive
pretraining, sophisticated training pipelines, and costly synthetic training
data, F2LLM is directly finetuned from foundation models on 6 million
query-document-negative tuples curated from open-source, non-synthetic
datasets, striking a strong balance between training cost, model size, and
embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd
among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B
ranks 1st among models in the 1B-2B size range. To facilitate future research
in the field, we release the models, training dataset, and code, positioning
F2LLM as a strong, reproducible, and budget-friendly baseline for future works.

</details>


### [167] [Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation](https://arxiv.org/abs/2510.02306)
*Raphael Tang,Crystina Zhang,Wenyan Li,Carmen Lai,Pontus Stenetorp,Yao Lu*

Main category: cs.CL

TL;DR: 本论文探讨了在大模型对战（arena-style）评价中，关于平局判定及其在模型评分系统中的正确处理方式。作者提出不应简单认为平局代表模型实力相等，而应将其视为查询难度的信号，并提出忽略平局的评分调整能提升预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的评分系统如Elo主要源自象棋等比赛，直接套用到大语言模型对战中，将平局视为模型能力持平。这种做法忽略了实际应用中的查询难度不同的问题，可能导致评分偏差。作者希望通过研究验证是否应当修正平局的语义。

Method: 作者在三个实际大模型对战数据集上，对比了在平局时不调整模型评分与传统评分方法（含四种评分系统）的效果，并统计分析了平局与查询难度和客观性的相关性。

Result: 实验证明，在平局时忽略评分调整可使战斗结果（包括平局）预测准确率相对提升1-3%。分析发现，平局更常出现在非常简单或非常客观的查询上，其风险比（risk ratio）分别为1.37和1.35。

Conclusion: 作者建议未来的评分系统要重新考虑平局的含义，应更多考虑查询本身的难度属性，以合理更新模型评分。

Abstract: In arena-style evaluation of large language models (LLMs), two LLMs respond
to a user query, and the user chooses the winning response or deems the
"battle" a draw, resulting in an adjustment to the ratings of both models. The
prevailing approach for modeling these rating dynamics is to view battles as
two-player game matches, as in chess, and apply the Elo rating system and its
derivatives. In this paper, we critically examine this paradigm. Specifically,
we question whether a draw genuinely means that the two models are equal and
hence whether their ratings should be equalized. Instead, we conjecture that
draws are more indicative of query difficulty: if the query is too easy, then
both models are more likely to succeed equally. On three real-world arena
datasets, we show that ignoring rating updates for draws yields a 1-3% relative
increase in battle outcome prediction accuracy (which includes draws) for all
four rating systems studied. Further analyses suggest that draws occur more for
queries rated as very easy and those as highly objective, with risk ratios of
1.37 and 1.35, respectively. We recommend future rating systems to reconsider
existing draw semantics and to account for query properties in rating updates.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [168] [Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge](https://arxiv.org/abs/2510.01348)
*Michal Werner,David Čapek,Tomáš Musil,Ondřej Franěk,Tomáš Báča,Martin Saska*

Main category: cs.RO

TL;DR: 论文提出了一种适用于无人机在无GNSS环境下远距离自主导航的新系统，通过融合LiDAR地形高度图与先验地理数据，显著减少了航迹漂移，并已在真实竞赛中验证。系统方案完全在嵌入式平台（CPU-only）上实时运行，无需预先稠密建图。


<details>
  <summary>Details</summary>
Motivation: 当前无人机在无GNSS（如GPS）环境下进行长距离飞行面临诸多难题，包括里程计累计误差（漂移）、无法有效闭环定位、以及嵌入式平台算力有限。解决这些问题，尤其是在未见过区域内实现可靠定位与导航，对于提升无人机自主能力至关重要。

Method: 系统将感知、建图、规划与控制集成到一起，并采用一种轻量级漂移校正方法：通过匹配LiDAR生成的局部高度图与先验地形高度数据（geo-data heightmap），利用梯度模板匹配方式，然后在聚类粒子滤波器框架中将匹配结果与里程计信息融合，辅助修正漂移。

Result: 该系统已在实际SPRIN-D Funke 竞赛中部署，支持无人机在城市、森林和开阔地等多种复杂地形中，执行长达9公里、最低25米高度的远距离导航任务。实验表明，系统能在只用CPU的情况下实时运行，并相较于单独使用里程计大幅减少定位漂移。

Conclusion: 所提系统有效提升了无人机在无GNSS环境下执行长距离任务的定位鲁棒性，且具备实际可落地性。现场应用反馈为后续开发更智能、实用的无人机自主系统提供了宝贵经验。

Abstract: Reliable long-range flight of unmanned aerial vehicles (UAVs) in GNSS-denied
environments is challenging: integrating odometry leads to drift, loop closures
are unavailable in previously unseen areas and embedded platforms provide
limited computational power. We present a fully onboard UAV system developed
for the SPRIN-D Funke Fully Autonomous Flight Challenge, which required 9 km
long-range waypoint navigation below 25 m AGL (Above Ground Level) without GNSS
or prior dense mapping. The system integrates perception, mapping, planning,
and control with a lightweight drift-correction method that matches
LiDAR-derived local heightmaps to a prior geo-data heightmap via
gradient-template matching and fuses the evidence with odometry in a clustered
particle filter. Deployed during the competition, the system executed
kilometer-scale flights across urban, forest, and open-field terrain and
reduced drift substantially relative to raw odometry, while running in real
time on CPU-only hardware. We describe the system architecture, the
localization pipeline, and the competition evaluation, and we report practical
insights from field deployment that inform the design of GNSS-denied UAV
autonomy.

</details>


### [169] [Safe Motion Planning and Control Using Predictive and Adaptive Barrier Methods for Autonomous Surface Vessels](https://arxiv.org/abs/2510.01357)
*Alejandro Gonzalez-Garcia,Wei Xiao,Wei Wang,Alejandro Astudillo,Wilm Decré,Jan Swevers,Carlo Ratti,Daniela Rus*

Main category: cs.RO

TL;DR: 本文提出了一种结合模型预测控制（MPC）与控制障碍函数（CBF）的安全运动规划方法，适用于自动化船舶在狭窄水道中的导航，能够在保证安全的前提下实时应对复杂环境。


<details>
  <summary>Details</summary>
Motivation: 自动船舶在狭窄水道的安全导航面临挑战，传统运动规划方法计算量大或过于保守，亟需高效且安全的新型方法。

Method: 方法将模型预测控制（MPC）与高阶控制障碍函数（CBF）结合，引入时变膨胀椭圆体障碍物表示，依据船舶与障碍物的相对位置和姿态动态调整膨胀半径，从而自适应地减少方法的保守性。MPC提供近似运动规划，CBF则利用动态膨胀半径强化安全性。

Result: 通过仿真和实船实验证明，所提方法能让全驱自动船舶在狭窄水道中实现实时导航，有效避免死锁，并且保证航行安全。

Conclusion: 结合MPC与CBF及自适应障碍物膨胀策略，有效提升了自动船舶在复杂近距离环境中的实时安全运动能力，为相关场景自动航行提供了可行方案。

Abstract: Safe motion planning is essential for autonomous vessel operations,
especially in challenging spaces such as narrow inland waterways. However,
conventional motion planning approaches are often computationally intensive or
overly conservative. This paper proposes a safe motion planning strategy
combining Model Predictive Control (MPC) and Control Barrier Functions (CBFs).
We introduce a time-varying inflated ellipse obstacle representation, where the
inflation radius is adjusted depending on the relative position and attitude
between the vessel and the obstacle. The proposed adaptive inflation reduces
the conservativeness of the controller compared to traditional fixed-ellipsoid
obstacle formulations. The MPC solution provides an approximate motion plan,
and high-order CBFs ensure the vessel's safety using the varying inflation
radius. Simulation and real-world experiments demonstrate that the proposed
strategy enables the fully-actuated autonomous robot vessel to navigate through
narrow spaces in real time and resolve potential deadlocks, all while ensuring
safety.

</details>


### [170] [A Stochastic Framework for Continuous-Time State Estimation of Continuum Robots](https://arxiv.org/abs/2510.01381)
*Spencer Teetaert,Sven Lilge,Jessica Burgner-Kahrs,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: 提出了一种基于连续时间随机状态估计的柔性（连续体）机器人状态估计算法，利用高频传感和简化模型实现对外部扰动和数据丢失的鲁棒适应，并能高效地输出机器人的均值和协方差状态。


<details>
  <summary>Details</summary>
Motivation: 传统连续体机器人状态估计方法计算复杂，形状建模简化较多，或仅限于准静态情况，对外部未建模扰动敏感。作者希望开发一种更鲁棒且高效的状态估计方法，能够处理真实工况下的各种不确定性。

Method: 借鉴因子图优化思路，引入基于高斯过程噪声扰动下的连续时间运动学因子的估计算法，结合简单机器人模型与高频传感，实现各类测量的高效插值与状态估计，并可应对数据丢失和外部力扰动带来的不确定性。

Result: 方法能输出连续时间和空间下的姿态、速度、应变等状态的均值和协方差，插值高效，计算复杂度随时间线性增长，状态查询常数时间。实验证明该算法可在真实柔性机器人、陀螺仪和位姿传感器组合平台上有效运行，适应多种实际应用。

Conclusion: 所提方法提高了连续体机器人在面对外部扰动、数据丢失等实际问题时的状态估计准确性与鲁棒性，具有很强的工程应用潜力。

Abstract: State estimation techniques for continuum robots (CRs) typically involve
using computationally complex dynamic models, simplistic shape approximations,
or are limited to quasi-static methods. These limitations can be sensitive to
unmodelled disturbances acting on the robot. Inspired by a factor-graph
optimization paradigm, this work introduces a continuous-time stochastic state
estimation framework for continuum robots. We introduce factors based on
continuous-time kinematics that are corrupted by a white noise Gaussian process
(GP). By using a simple robot model paired with high-rate sensing, we show
adaptability to unmodelled external forces and data dropout. The result
contains an estimate of the mean and covariance for the robot's pose, velocity,
and strain, each of which can be interpolated continuously in time or space.
This same interpolation scheme can be used during estimation, allowing for
inclusion of measurements on states that are not explicitly estimated. Our
method's inherent sparsity leads to a linear solve complexity with respect to
time and interpolation queries in constant time. We demonstrate our method on a
CR with gyroscope and pose sensors, highlighting its versatility in real-world
systems.

</details>


### [171] [VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation](https://arxiv.org/abs/2510.01388)
*Arthur Zhang,Xiangyun Meng,Luca Calliari,Dong-Ki Kim,Shayegan Omidshafiei,Joydeep Biswas,Ali Agha,Amirreza Shaban*

Main category: cs.RO

TL;DR: 提出了一种新型视觉-语言导航系统VENTURA，通过微调互联网预训练的图像扩散模型，显著提升机器人路径规划和指令适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型虽具备强大的语言与感知对齐能力，但在导航任务中难以直接迁移和控制，特别是低级动作空间和预训练目标之间的不匹配，限制了机器人在开放环境下的表现。

Method: VENTURA系统并不直接预测低级动作，而是在图像空间中生成路径掩码（视觉计划），反映细粒度和上下文相关的导航行为。使用轻量级模仿学习策略将视觉计划转换为可执行轨迹。训练时，通过VLM增强的字幕与自监督追踪模型自动生成路径掩码，无需人工逐像素标注或繁琐的数据采集。

Result: 在真实世界的多项评测中，VENTURA在目标到达、障碍规避和地形偏好等任务上，相较现有基础模型提升成功率33%，碰撞率降低54%。系统还展现了对未见任务组合的泛化能力。

Conclusion: VENTURA构建了高效、人类指令兼容且可扩展的机器人导航方案，为多样化开放环境任务提供了有力支持，并展现出强大的组合泛化与可拓展性。

Abstract: Robots must adapt to diverse human instructions and operate safely in
unstructured, open-world environments. Recent Vision-Language models (VLMs)
offer strong priors for grounding language and perception, but remain difficult
to steer for navigation due to differences in action spaces and pretraining
objectives that hamper transferability to robotics tasks. Towards addressing
this, we introduce VENTURA, a vision-language navigation system that finetunes
internet-pretrained image diffusion models for path planning. Instead of
directly predicting low-level actions, VENTURA generates a path mask (i.e. a
visual plan) in image space that captures fine-grained, context-aware
navigation behaviors. A lightweight behavior-cloning policy grounds these
visual plans into executable trajectories, yielding an interface that follows
natural language instructions to generate diverse robot behaviors. To scale
training, we supervise on path masks derived from self-supervised tracking
models paired with VLM-augmented captions, avoiding manual pixel-level
annotation or highly engineered data collection setups. In extensive real-world
evaluations, VENTURA outperforms state-of-the-art foundation model baselines on
object reaching, obstacle avoidance, and terrain preference tasks, improving
success rates by 33% and reducing collisions by 54% across both seen and unseen
scenarios. Notably, we find that VENTURA generalizes to unseen combinations of
distinct tasks, revealing emergent compositional capabilities. Videos, code,
and additional materials: https://venturapath.github.io

</details>


### [172] [INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models](https://arxiv.org/abs/2510.01389)
*Ulas Berk Karli,Ziyao Shangguan,Tesca FItzgerald*

Main category: cs.RO

TL;DR: 本文提出了一种基于不确定性信号预测VLA模型何时请求人类帮助的方法。通过细致比较不同监督下的效果，发现利用token级别的不确定性信息配合transformer能有效提升模型自我 introspection 和请求帮助的能力。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言-动作（VLA）模型虽然泛化能力强，但缺乏提前预判失败并主动寻求人类干预的能力。提升模型在不确定情况下主动请求帮助的能力，有助于增强其安全性和实际应用价值。

Method: 作者提出INSIGHT框架，提取$	ext{pi}_0$-FAST模型预测过程中的token级别熵、对数概率、Dirichlet不确定性（分为偶然不确定性和认知不确定性），利用简单transformer分类器，将这些不确定性序列映射为是否触发帮助请求。比较了强、弱两种监督标签训练方式，并在同分布和异分布任务中做广泛实验分析。

Result: 实验显示，强监督标签能让模型更精细地捕捉不确定性动态，对请求帮助检测更可靠；弱标签虽然噪声较大，但训练和评估对齐时仍有较好表现，并适用于难以获得大量精准标注的场景。同时，建模不确定性信号的时间演化远优于静态整体评分。

Conclusion: 本研究首次系统分析了VLA模型中基于不确定性的自我反思机制，证实了token级别不确定性随时间动态的建模对提升请求帮助预测的有效性，为后续主动学习和实时错误干预提供新方向。

Abstract: Recent Vision-Language-Action (VLA) models show strong generalization
capabilities, yet they lack introspective mechanisms for anticipating failures
and requesting help from a human supervisor. We present \textbf{INSIGHT}, a
learning framework for leveraging token-level uncertainty signals to predict
when a VLA should request help. Using $\pi_0$-FAST as the underlying model, we
extract per-token \emph{entropy}, \emph{log-probability}, and Dirichlet-based
estimates of \emph{aleatoric and epistemic uncertainty}, and train compact
transformer classifiers to map these sequences to help triggers. We explore
supervision regimes for strong or weak supervision, and extensively compare
them across in-distribution and out-of-distribution tasks. Our results show a
trade-off: strong labels enable models to capture fine-grained uncertainty
dynamics for reliable help detection, while weak labels, though noisier, still
support competitive introspection when training and evaluation are aligned,
offering a scalable path when dense annotation is impractical. Crucially, we
find that modeling the temporal evolution of token-level uncertainty signals
with transformers provides far greater predictive power than static
sequence-level scores. This study provides the first systematic evaluation of
uncertainty-based introspection in VLAs, opening future avenues for active
learning and for real-time error mitigation through selective human
intervention.

</details>


### [173] [Beyond Collision Cones: Dynamic Obstacle Avoidance for Nonholonomic Robots via Dynamic Parabolic Control Barrier Functions](https://arxiv.org/abs/2510.01402)
*Hun Kuk Park,Taekyung Kim,Dimitra Panagou*

Main category: cs.RO

TL;DR: 本文提出了一种动态抛物线控制障碍函数（DPCBF），用以提升非完整机器人在拥挤、动态环境中的安全性和可行性。新方法在密集障碍物环境下展现出优于现有碰撞锥、速障等CBF方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的CBF方法依赖碰撞锥或速障约束，因仅考虑相对速度方向而过于保守，在密集动态环境下，常导致CBF二次规划问题不可行。本文旨在解决传统CBF在复杂动态环境中的适用性不足问题。

Method: 提出动态抛物线控制障碍函数DPCBF：采用抛物线边界动态调整安全集，其顶点与曲率依据与障碍物距离及相对速度动态变化，使安全约束更灵活、宽松。理论上证明DPCBF对带输入约束的二轮自行车模型的有效性，并与主流方法进行比较模拟。

Result: 实验结果显示，DPCBF方法在动态障碍物高密度场景（多达100个障碍物）下，能有效导航通过并大幅提升成功率和QP可行性，而基于碰撞锥的方法在此类场景下常常因约束不可行而失败。

Conclusion: 动态抛物线CBF有效提升了非完整机器人在复杂动态环境下基于CBF的安全性和导航能力，为突破现有CBF局限性提供了新思路与方法。

Abstract: Control Barrier Functions (CBFs) are a powerful tool for ensuring the safety
of autonomous systems, yet applying them to nonholonomic robots in cluttered,
dynamic environments remains an open challenge. State-of-the-art methods often
rely on collision-cone or velocity-obstacle constraints which, by only
considering the angle of the relative velocity, are inherently conservative and
can render the CBF-based quadratic program infeasible, particularly in dense
scenarios. To address this issue, we propose a Dynamic Parabolic Control
Barrier Function (DPCBF) that defines the safe set using a parabolic boundary.
The parabola's vertex and curvature dynamically adapt based on both the
distance to an obstacle and the magnitude of the relative velocity, creating a
less restrictive safety constraint. We prove that the proposed DPCBF is valid
for a kinematic bicycle model subject to input constraints. Extensive
comparative simulations demonstrate that our DPCBF-based controller
significantly enhances navigation success rates and QP feasibility compared to
baseline methods. Our approach successfully navigates through dense
environments with up to 100 dynamic obstacles, scenarios where collision
cone-based methods fail due to infeasibility.

</details>


### [174] [How Well do Diffusion Policies Learn Kinematic Constraint Manifolds?](https://arxiv.org/abs/2510.01404)
*Lexi Foland,Thomas Cohn,Adam Wei,Nicholas Pfaff,Boyuan Chen,Russ Tedrake*

Main category: cs.RO

TL;DR: 本文分析了扩散策略（diffusion policies）在模仿学习中的约束学习能力，发现数据集质量和规模影响较大，但约束曲率影响不明显。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散策略在机器人模仿学习中取得了优异的任务表现，但这种表现并不能代表模型已精准掌握训练数据中的运动学约束。本研究旨在评估扩散策略在学习运动学约束流形方面的能力。

Method: 作者以双臂抓取放置任务为案例，系统分析了数据集规模、质量、和约束流形曲率这三个因素对训练出的扩散策略的影响。通过一系列仿真和硬件实验，评估策略对约束流形的学习能力。

Result: 结果表明扩散策略可以粗略逼近运动学约束流形，并且数据集规模和质量降低会显著影响其约束学习能力；对于约束流形曲率对性能的相关性没有明确结论。

Conclusion: 虽然扩散策略能一定程度学习约束，但约束学习的精度有限，并严重依赖数据集的丰富性和准确性。对于复杂曲率的约束流形，影响尚不明确，相关结论需要进一步研究。

Abstract: Diffusion policies have shown impressive results in robot imitation learning,
even for tasks that require satisfaction of kinematic equality constraints.
However, task performance alone is not a reliable indicator of the policy's
ability to precisely learn constraints in the training data. To investigate, we
analyze how well diffusion policies discover these manifolds with a case study
on a bimanual pick-and-place task that encourages fulfillment of a kinematic
constraint for success. We study how three factors affect trained policies:
dataset size, dataset quality, and manifold curvature. Our experiments show
diffusion policies learn a coarse approximation of the constraint manifold with
learning affected negatively by decreases in both dataset size and quality. On
the other hand, the curvature of the constraint manifold showed inconclusive
correlations with both constraint satisfaction and task success. A hardware
evaluation verifies the applicability of our results in the real world. Project
website with additional results and visuals:
https://diffusion-learns-kinematic.github.io

</details>


### [175] [AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation](https://arxiv.org/abs/2510.01433)
*Anukriti Singh,Kasra Torshizi,Khuzema Habib,Kelin Yu,Ruohan Gao,Pratap Tokekar*

Main category: cs.RO

TL;DR: AFFORD2ACT 提出了一种通过文本和单张图片自提取最小关键点集的轻量视觉机器人控制方法，在多样操作任务上显著提升了效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉机器人主要依赖密集的图像或点云输入，计算量大且易受无关背景干扰。尽管关键点法更轻量，但通常依赖人工经验或任务特定选择，难以推广。作者希望借助“可供性”指导，实现从更高层抽象自动提取关键点，提升可扩展性和语义理解。

Method: 提出 AFFORD2ACT 三阶段流程：1）可供性过滤——利用文本提示和图片筛选最相关操作特征；2）构建类别级语义关键点；3）融合嵌入门控的 transformer_policy，只基于38维紧凑状态实现训练与推理，无需本体感知和稠密表征。

Result: 在多种现实操作任务中，AFFORD2ACT 显著提升数据效率，在未见物体、类别、背景和干扰情况下的成功率高达82%。政策训练仅需15分钟，实时性能良好。

Conclusion: AFFORD2ACT 能高效、泛化地从高语义层自动生成轻量关键点，实现了比以往方法更快、更广泛的机器人视觉操作能力，对视觉机器人学习具有重要的理论和实践意义。

Abstract: Vision-based robot learning often relies on dense image or point-cloud
inputs, which are computationally heavy and entangle irrelevant background
features. Existing keypoint-based approaches can focus on manipulation-centric
features and be lightweight, but either depend on manual heuristics or
task-coupled selection, limiting scalability and semantic understanding. To
address this, we propose AFFORD2ACT, an affordance-guided framework that
distills a minimal set of semantic 2D keypoints from a text prompt and a single
image. AFFORD2ACT follows a three-stage pipeline: affordance filtering,
category-level keypoint construction, and transformer-based policy learning
with embedded gating to reason about the most relevant keypoints, yielding a
compact 38-dimensional state policy that can be trained in 15 minutes, which
performs well in real-time without proprioception or dense representations.
Across diverse real-world manipulation tasks, AFFORD2ACT consistently improves
data efficiency, achieving an 82% success rate on unseen objects, novel
categories, backgrounds, and distractors.

</details>


### [176] [Differentiable Skill Optimisation for Powder Manipulation in Laboratory Automation](https://arxiv.org/abs/2510.01438)
*Minglun Wei,Xintong Yang,Yu-Kun Lai,Ze Ji*

Main category: cs.RO

TL;DR: 该论文提出了一种结合可微物理模拟和任务课程的机器人轨迹优化框架，用以精确控制实验室中的粉末搬运任务，并在实验中优于强化学习基线。


<details>
  <summary>Details</summary>
Motivation: 在实验室自动化中，机器人对粉末等颗粒物精确操纵仍存在难题，传统方法难以满足搬运时的精度和稳定性需求。该研究旨在解决这一关键瓶颈。

Method: 提出了一个集成可微物理模拟的轨迹优化方法，采用低维技能空间来降低优化难度，并通过课程学习逐步提升机器人在复杂搬运任务中的能力，实现端到端的轨迹优化。

Result: 实验结果显示，该方法在任务完成率和操作稳定性上明显优于传统的强化学习基线方法。

Conclusion: 该框架能高效提升机器人在接触丰富的粉末搬运任务中的表现，对实验室自动化具有实际应用价值。

Abstract: Robotic automation is accelerating scientific discovery by reducing manual
effort in laboratory workflows. However, precise manipulation of powders
remains challenging, particularly in tasks such as transport that demand
accuracy and stability. We propose a trajectory optimisation framework for
powder transport in laboratory settings, which integrates differentiable
physics simulation for accurate modelling of granular dynamics, low-dimensional
skill-space parameterisation to reduce optimisation complexity, and a
curriculum-based strategy that progressively refines task competence over long
horizons. This formulation enables end-to-end optimisation of contact-rich
robot trajectories while maintaining stability and convergence efficiency.
Experimental results demonstrate that the proposed method achieves superior
task success rates and stability compared to the reinforcement learning
baseline.

</details>


### [177] [Touching the tumor boundary: A pilot study on ultrasound based virtual fixtures for breast-conserving surgery](https://arxiv.org/abs/2510.01452)
*Laura Connolly,Tamas Ungi,Adnan Munawar,Anton Deguet,Chris Yeung,Russell H. Taylor,Parvin Mousavi,Gabor Fichtinger Keyvan Hashtrudi-Zaad*

Main category: cs.RO

TL;DR: 本文提出了一种结合触觉反馈的机器人辅助引导系统，用于乳腺保乳手术中肿瘤边界的定位，并在模拟实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 乳腺保乳手术中，肿瘤常常高度可移动、难以触摸且边界不规则，导致术中肿瘤边界难以准确描绘，影响手术效果。因此，迫切需要新的方法辅助外科医生准确定位肿瘤边界。

Method: 研究团队将小型触觉机器人与电凝刀结合，通过超声和电磁导航定位肿瘤边界，建立虚拟禁区，当手术器械碰到肿瘤边界时触发触觉反馈。设计了乳腺模型肿瘤切除模拟实验，比较有无触觉引导条件下的手术表现。

Result: 实验表明，触觉虚拟禁区引导可显著提高切除边缘质量，并降低用户的心理负担、挫折感和操作难度。同时发现该系统对手术流程的意外影响，为后续设计与培训提供参考。

Conclusion: 虚拟禁区结合触觉反馈系统能够辅助定位乳腺保乳手术中的肿瘤边界，后续将通过更大规模用户研究优化和验证该系统。

Abstract: Purpose: Delineating tumor boundaries during breast-conserving surgery is
challenging as tumors are often highly mobile, non-palpable, and have
irregularly shaped borders. To address these challenges, we introduce a
cooperative robotic guidance system that applies haptic feedback for tumor
localization. In this pilot study, we aim to assess if and how this system can
be successfully integrated into breast cancer care.
  Methods: A small haptic robot is retrofitted with an electrocautery blade to
operate as a cooperatively controlled surgical tool. Ultrasound and
electromagnetic navigation are used to identify the tumor boundaries and
position. A forbidden region virtual fixture is imposed when the surgical tool
collides with the tumor boundary. We conducted a study where users were asked
to resect tumors from breast simulants both with and without the haptic
guidance. We then assess the results of these simulated resections both
qualitatively and quantitatively.
  Results: Virtual fixture guidance is shown to improve resection margins. On
average, users find the task to be less mentally demanding, frustrating, and
effort intensive when haptic feedback is available. We also discovered some
unanticipated impacts on surgical workflow that will guide design adjustments
and training protocol moving forward.
  Conclusion: Our results suggest that virtual fixtures can help localize tumor
boundaries in simulated breast-conserving surgery. Future work will include an
extensive user study to further validate these results and fine-tune our
guidance system.

</details>


### [178] [VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs](https://arxiv.org/abs/2510.01483)
*Mohamad Al Mdfaa,Svetlana Lukina,Timur Akhtyamov,Arthur Nigmatzyanov,Dmitrii Nalberskii,Sergey Zagoruyko,Gonzalo Ferrer*

Main category: cs.RO

TL;DR: 该论文提出了一种视觉场景理解系统VL-KnG，通过建立时空知识图谱，实现了对机器人导航中场景记忆与空间推理的改进，并在新构建的数据集WalkieKnowledge上验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs用于机器人导航时，存在缺乏持久场景记忆、空间推理能力有限以及视频处理难以实时扩展等问题。为了解决这些限制，亟需一种高效、可解释并能处理长时间视频的视觉-语言理解框架。

Method: 提出了VL-KnG系统，将视频序列分块处理，通过现代VLMs建立持久的知识图谱，并保存对象身份，实现可查询的图结构来进行场景推理和导航目标识别。设计了WalkieKnowledge基准测试集，对多种方法进行公平评测。

Result: VL-KnG在实际机器人导航中达到77.27%成功率和76.92%问答准确率，效果与Gemini 2.5 Pro相当，并且具有可解释性和更高的计算效率，适用于实时任务。

Conclusion: VL-KnG通过时空知识图谱，实现了视觉语言模型在导航应用中的可扩展性、可解释性和高效性，为实际机器人应用提供了新的方向，其代码和数据集将在论文接收后开放。

Abstract: Vision-language models (VLMs) have shown potential for robot navigation but
encounter fundamental limitations: they lack persistent scene memory, offer
limited spatial reasoning, and do not scale effectively with video duration for
real-time application. We present VL-KnG, a Visual Scene Understanding system
that tackles these challenges using spatiotemporal knowledge graph construction
and computationally efficient query processing for navigation goal
identification. Our approach processes video sequences in chunks utilizing
modern VLMs, creates persistent knowledge graphs that maintain object identity
over time, and enables explainable spatial reasoning through queryable graph
structures. We also introduce WalkieKnowledge, a new benchmark with about 200
manually annotated questions across 8 diverse trajectories spanning
approximately 100 minutes of video data, enabling fair comparison between
structured approaches and general-purpose VLMs. Real-world deployment on a
differential drive robot demonstrates practical applicability, with our method
achieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5
Pro performance while providing explainable reasoning supported by the
knowledge graph, computational efficiency for real-time deployment across
different tasks, such as localization, navigation and planning. Code and
dataset will be released after acceptance.

</details>


### [179] [Pose Estimation of a Thruster-Driven Bioinspired Multi-Link Robot](https://arxiv.org/abs/2510.01485)
*Nicholas B. Andrews,Yanhao Yang,Sofya Akhetova,Kristi A. Morgansen,Ross L. Hatton*

Main category: cs.RO

TL;DR: 该论文提出了一种自由漂浮、仿生多连杆机器人在传感极少与欠驱动情况下的位姿估计算法，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 多连杆仿生机器人在空间等复杂环境下，通常关节无法主动驱动且搭载传感器有限，因此如何在这种条件下准确估计机器人的姿态与位置具有挑战性且意义重大。

Method: 采用每个连杆仅配备一个陀螺仪以及部分推力器进行控制，通过无迹卡尔曼滤波（UKF）和高斯过程残差学习的结合，提升对非零均值、非高斯噪声下的系统状态估计精度。

Result: 通过原型实验和离线卡尔曼滤波分析，证实了该方法可对机器人姿态进行可靠估计。此外，采用多步态（前进、后退、转弯等）训练得到的滤波器，与仅用大量前进步态训练的数据集相比，二者在同一路径下测试表现相当，说明步态输入空间存在重叠。

Conclusion: 该方法不仅能有效提升多连杆机器人在最小化传感和驱控下的状态估计精度，而且通过挖掘步态空间重叠性，可大幅减少训练数据量并提升算法对不同步态的泛化能力。

Abstract: This work demonstrates pose (position and shape) estimation for a
free-floating, bioinspired multi-link robot with unactuated joints,
link-mounted thrusters for control, and a single gyroscope per link, resulting
in an underactuated, minimally sensed platform. Through a proof-of-concept
hardware experiment and offline Kalman filter analysis, we show that the
robot's pose can be reliably estimated. State estimation is performed using an
unscented Kalman filter augmented with Gaussian process residual learning to
compensate for non-zero-mean, non-Gaussian noise. We further show that a filter
trained on a multi-gait dataset (forward, backward, left, right, and turning)
performs comparably to one trained on a larger forward-gait-only dataset when
both are evaluated on the same forward-gait test trajectory. These results
reveal overlap in the gait input space, which can be exploited to reduce
training data requirements while enhancing the filter's generalizability across
multiple gaits.

</details>


### [180] [Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments](https://arxiv.org/abs/2510.01519)
*Wei Han Chen,Yuchen Liu,Alexiy Buynitsky,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 本文提出一种分层结构的机器人导航方法，通过高层稀疏图与低层神经场规划相结合，有效提升了在大型复杂室内环境中的导航精度与适应性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习或仿真学习方法在未知复杂环境中导航受限于可扩展性与对大量演示数据的依赖。近期ANTFields方法虽有前景，但受到神经网络的光谱偏置和遗忘等问题影响。本工作旨在克服这些局限，提升机器人在复杂环境中的导航能力。

Method: 作者将规划问题分解为高低层两级。高层用稀疏图捕获环境的连通性，低层则利用基于神经场的方法，通过数值求解Eikonal偏微分方程，实现精准局部导航。这一物理先验方法缓解了神经网络训练过程中的光谱偏置与拟合难度。

Result: 实验在大规模环境下对方法进行了验证，结果显示其在适应性和精度方面均优于现有方法。模型可用于在线探索、建图以及现实环境导航。

Conclusion: 所提方法有效解决了以往神经场方法在复杂环境应用中的主要难题，实现了更平滑、精确的代价值表示，推动了机器人大尺度自主导航能力。

Abstract: Robot navigation in large, complex, and unknown indoor environments is a
challenging problem. The existing approaches, such as traditional
sampling-based methods, struggle with resolution control and scalability, while
imitation learning-based methods require a large amount of demonstration data.
Active Neural Time Fields (ANTFields) have recently emerged as a promising
solution by using local observations to learn cost-to-go functions without
relying on demonstrations. Despite their potential, these methods are hampered
by challenges such as spectral bias and catastrophic forgetting, which diminish
their effectiveness in complex scenarios. To address these issues, our approach
decomposes the planning problem into a hierarchical structure. At the high
level, a sparse graph captures the environment's global connectivity, while at
the low level, a planner based on neural fields navigates local obstacles by
solving the Eikonal PDE. This physics-informed strategy overcomes common
pitfalls like spectral bias and neural field fitting difficulties, resulting in
a smooth and precise representation of the cost landscape. We validate our
framework in large-scale environments, demonstrating its enhanced adaptability
and precision compared to previous methods, and highlighting its potential for
online exploration, mapping, and real-world navigation.

</details>


### [181] [Real-time Multi-Plane Segmentation Based on GPU Accelerated High-Resolution 3D Voxel Mapping for Legged Robot Locomotion](https://arxiv.org/abs/2510.01592)
*Shun Niijima,Ryoichi Tsuzaki,Noriaki Takasugi,Masaya Kinoshita*

Main category: cs.RO

TL;DR: 本文提出了一种基于GPU加速高分辨率3D体素地图的实时多平面分割方法，专为腿式机器人导航设计，在高速（30Hz+）和高分辨率（0.01m）下实现准确的3D多平面提取，有效支持机器人实时运动。


<details>
  <summary>Details</summary>
Motivation: 现有在线平面地图方法在精度与计算效率之间难以权衡：直接的深度图像分割在时序整合性上较差；高度图难以表达复杂3D结构如悬垂物；基于体素的平面分割尚未有实时应用。为弥补这些缺陷，作者提出创新性的平面分割方法。

Method: 该方法将顶点连通组件标记法与RANSAC平面检测以及凸包算法结合，利用GPU的并行计算能力，从高分辨率3D体素地图累积的点云数据快速提取出平面区域。

Result: 实验结果表明，在0.01m分辨率下，该方法可实现30Hz以上的3D多平面实时分割，且精度高，分割结果可实时用于腿式机器人运动。方法已在仿真和真实机器人平台两种环境下验证，有效提升了机器人对复杂3D平面结构的适应能力。

Conclusion: 该工作证明GPU加速的体素地图多平面分割技术可为腿式机器人提供高效、实时的3D平面感知支持，有助于提升机器人在复杂环境下的运动表现，并具备实际应用前景。

Abstract: This paper proposes a real-time multi-plane segmentation method based on
GPU-accelerated high-resolution 3D voxel mapping for legged robot locomotion.
Existing online planar mapping approaches struggle to balance accuracy and
computational efficiency: direct depth image segmentation from specific sensors
suffers from poor temporal integration, height map-based methods cannot
represent complex 3D structures like overhangs, and voxel-based plane
segmentation remains unexplored for real-time applications. To address these
limitations, we develop a novel framework that integrates vertex-based
connected component labeling with random sample consensus based plane detection
and convex hull, leveraging GPU parallel computing to rapidly extract planar
regions from point clouds accumulated in high-resolution 3D voxel maps.
Experimental results demonstrate that the proposed method achieves fast and
accurate 3D multi-plane segmentation at over 30 Hz update rate even at a
resolution of 0.01 m, enabling the detected planes to be utilized in real time
for locomotion tasks. Furthermore, we validate the effectiveness of our
approach through experiments in both simulated environments and physical legged
robot platforms, confirming robust locomotion performance when considering 3D
planar structures.

</details>


### [182] [MiniBEE: A New Form Factor for Compact Bimanual Dexterity](https://arxiv.org/abs/2510.01603)
*Sharfin Islam,Zewen Chen,Zhanpeng He,Swapneel Bhatt,Andres Permuy,Brock Taylor,James Vickery,Pedro Piacenza,Cheng Zhang,Matei Ciocarlie*

Main category: cs.RO

TL;DR: 本文提出了MiniBEE系统，一种由两只3自由度机械臂组成的紧凑型双手末端执行器，实现了高效的双手操作同时降低了系统复杂度和重量，并用可穿戴模式收集动作数据，为机器人的双手操作提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 现有的双臂机器人通常采用高自由度的臂，结构复杂且大多数工作空间难以充分利用，导致系统成本高、效率低。作者希望设计一种结构简单、更轻便且能有效利用操作空间的双手机器人末端执行器。

Method: 作者设计了MiniBEE系统，将两个低自由度（每个3自由度）的机械臂耦合成一个连杆系统，通过独特的运动学设计保证夹爪间相对位置的全面可控，并提出了运动学灵巧性度量，辅助设计优化系统的灵巧空间。MiniBEE具有可穿戴数据采集及机器人部署两种工作模式。作者还提出端到端的数据采集、模仿学习与实物操作流程。

Result: MiniBEE系统实现了紧凑设计和具有较大灵巧工作空间，在可穿戴演示收集实际动作数据，通过模仿学习在机器人上高效复现，实现了强健的现实双手操作任务。

Conclusion: MiniBEE突破了现有双臂机器人系统高复杂度与低空间利用率瓶颈，既兼具良好的灵巧性、较低重量，又支持数据采集与实际部署，为机器人双手操作提供了更高效、低成本的解决方案。

Abstract: Bimanual robot manipulators can achieve impressive dexterity, but typically
rely on two full six- or seven- degree-of-freedom arms so that paired grippers
can coordinate effectively. This traditional framework increases system
complexity while only exploiting a fraction of the overall workspace for
dexterous interaction. We introduce the MiniBEE (Miniature Bimanual
End-effector), a compact system in which two reduced-mobility arms (3+ DOF
each) are coupled into a kinematic chain that preserves full relative
positioning between grippers. To guide our design, we formulate a kinematic
dexterity metric that enlarges the dexterous workspace while keeping the
mechanism lightweight and wearable. The resulting system supports two
complementary modes: (i) wearable kinesthetic data collection with self-tracked
gripper poses, and (ii) deployment on a standard robot arm, extending dexterity
across its entire workspace. We present kinematic analysis and design
optimization methods for maximizing dexterous range, and demonstrate an
end-to-end pipeline in which wearable demonstrations train imitation learning
policies that perform robust, real-world bimanual manipulation.

</details>


### [183] [ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations](https://arxiv.org/abs/2510.01607)
*Qiyuan Zeng,Chengmeng Li,Jude St. John,Zhongyi Zhou,Junjie Wen,Guorui Feng,Yichen Zhu,Yi Xu*

Main category: cs.RO

TL;DR: 本文提出了ActiveUMI，一种结合便携式VR遥操作、可穿戴计算机和传感控制器的数据采集系统，可以高效采集现实场景下的人类双手操作演示，并用于机器人复杂双手操作学习。采用该系统收集的数据训练得到的策略在相似任务的成功率为70%，在新物体和新环境下也有较好泛化能力（56%成功率），为复杂机器人操作的泛化和能力提升提供新路径。


<details>
  <summary>Details</summary>
Motivation: 当前机器人难以在现实环境下完成复杂的双手操作任务，部分原因在于缺乏高质量、可泛化的大规模真实演示数据，且传统数据收集工具通常受限于实验室环境、缺乏人类感知信息。动机在于开发一种便携、可在真实环境中高质量收集数据，并能捕捉操作者感知动作（如头部运动）的系统，从而推动机器人学习泛化能力更强的操作策略。

Method: ActiveUMI框架由便携式VR遥操作套件、传感控制器和可穿戴计算机组成，通过精确的空间姿态校准，将人类操作与机器人运动协调。同时，利用3D模型沉浸式渲染、现场校准和记录操作者头部运动等技术，采集反映人类主动感知过程的数据，用于训练强化泛化能力的机器人操作策略。

Result: 在6项复杂双手任务上评估系统收集的数据，训练出的机器人策略平均在相似（分布内）任务上达到了70%的成功率，在测试全新物体和环境时仍有56%成功率，表现出较强的泛化能力。

Conclusion: ActiveUMI系统可在自然环境下便携高效采集高质量机器人学习数据，特别是结合主动感知信息，显著提升了基于学习的机器人双手操作的泛化性与能力，为大规模可泛化机器人策略的学习与应用提供了可行路径。

Abstract: We present ActiveUMI, a framework for a data collection system that transfers
in-the-wild human demonstrations to robots capable of complex bimanual
manipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized
controllers that mirror the robot's end-effectors, bridging human-robot
kinematics via precise pose alignment. To ensure mobility and data quality, we
introduce several key techniques, including immersive 3D model rendering, a
self-contained wearable computer, and efficient calibration methods.
ActiveUMI's defining feature is its capture of active, egocentric perception.
By recording an operator's deliberate head movements via a head-mounted
display, our system learns the crucial link between visual attention and
manipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies
trained exclusively on ActiveUMI data achieve an average success rate of 70\%
on in-distribution tasks and demonstrate strong generalization, retaining a
56\% success rate when tested on novel objects and in new environments. Our
results demonstrate that portable data collection systems, when coupled with
learned active perception, provide an effective and scalable pathway toward
creating generalizable and highly capable real-world robot policies.

</details>


### [184] [FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models](https://arxiv.org/abs/2510.01642)
*Zijun Lin,Jiafei Duan,Haoquan Fang,Dieter Fox,Ranjay Krishna,Cheston Tan,Bihan Wen*

Main category: cs.RO

TL;DR: 本文提出了FailSafe系统，通过自动生成多样化的失败案例及可执行的恢复动作，显著提升机器人操作任务中的失败检测及恢复能力。


<details>
  <summary>Details</summary>
Motivation: 尽管最先进的视觉-语言-动作（VLA）模型在机器人应用中表现强劲，但在遇到不可预期的失败时，缺乏有效的推理和自恢复能力。现有数据集多仅提供成功轨迹，失败相关的数据和恢复方案极为缺乏。这限制了机器人在现实环境中的实用性。

Method: 提出FailSafe系统，可自动为任意模拟器中的操作任务产生失败案例及对应的恢复动作数据。作者将FailSafe生成的数据用于微调LLaVa-OneVision-7B模型，构建了FailSafe-VLM。通过在Maniskill等任务集进行实验，验证效果。

Result: FailSafe-VLM能够有效帮助机械臂检测和恢复潜在操作失败，使三种最优VLA模型（pi0-FAST、OpenVLA、OpenVLA-OFT）的综合表现平均提升最高达22.6%。此外，其具备较好的跨空间场景、相机视角和机器人本体的泛化能力。

Conclusion: FailSafe通过大规模生成失败与恢复数据，突破了现有机器人因失败难以自愈的瓶颈，显著提升了机器人系统的鲁棒性和实用性。相关代码将向社区开放，促进领域发展。

Abstract: Recent advances in robotic manipulation have integrated low-level robotic
control into Vision-Language Models (VLMs), extending them into
Vision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve
strong performance in downstream robotic applications, supported by large-scale
crowd-sourced robot training data, they still inevitably encounter failures
during execution. Enabling robots to reason about and recover from
unpredictable and abrupt failures remains a critical challenge. Existing
robotic manipulation datasets, collected in either simulation or the real
world, primarily provide only ground-truth trajectories, leaving robots unable
to recover once failures occur. Moreover, the few datasets that address failure
detection typically offer only textual explanations, which are difficult to
utilize directly in VLA models. To address this gap, we introduce FailSafe, a
novel failure generation and recovery system that automatically produces
diverse failure cases paired with executable recovery actions. FailSafe can be
seamlessly applied to any manipulation task in any simulator, enabling scalable
creation of failure-action data. To demonstrate its effectiveness, we fine-tune
LLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results
show that FailSafe-VLM successfully helps robotic arm detect and recover from
potential failures, improving the performance of three state-of-the-art VLA
models pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several
tasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different
spatial configurations, camera viewpoints, and robotic embodiments. We plan to
release the FailSafe code to the community.

</details>


### [185] [Statistical Uncertainty Learning for Robust Visual-Inertial State Estimation](https://arxiv.org/abs/2510.01648)
*Seungwon Choi,Donggyu Park,Seo-Yeon Hwang,Tae-Wan Kim*

Main category: cs.RO

TL;DR: 本文提出了一种能在线学习和动态评估传感器测量可靠性的视觉-惯性里程计(VIO)新框架，有效提升了定位精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前VIO系统通常假设所有测量值具有静态且统一的不确定性，这种简化方法无法反映真实数据中的动态误差变化。因此，需要一种能够根据实际测量动态调整评估和权重的方法，以提升系统的准确性和鲁棒性。

Method: 文中提出了一个统计学习框架，能够根据多视角几何一致性进行自监督学习，实时从传感器原始数据和优化结果中学习并动态评估每个观测值的可靠性，进而在优化过程中自适应调整视觉观测的权重。

Result: 在EuRoC公开数据集上的实验表明，该方法在定位精度方面显著优于传统不确定性固定的方法。平均平移误差下降约24%，旋转误差下降约42%。

Conclusion: 提出的框架可实时运行，显著提升了VIO的精度和鲁棒性，并已开源，便于复现和后续研究。

Abstract: A fundamental challenge in robust visual-inertial odometry (VIO) is to
dynamically assess the reliability of sensor measurements. This assessment is
crucial for properly weighting the contribution of each measurement to the
state estimate. Conventional methods often simplify this by assuming a static,
uniform uncertainty for all measurements. This heuristic, however, may be
limited in its ability to capture the dynamic error characteristics inherent in
real-world data. To improve this limitation, we present a statistical framework
that learns measurement reliability assessment online, directly from sensor
data and optimization results. Our approach leverages multi-view geometric
consistency as a form of self-supervision. This enables the system to infer
landmark uncertainty and adaptively weight visual measurements during
optimization. We evaluated our method on the public EuRoC dataset,
demonstrating improvements in tracking accuracy with average reductions of
approximately 24\% in translation error and 42\% in rotation error compared to
baseline methods with fixed uncertainty parameters. The resulting framework
operates in real time while showing enhanced accuracy and robustness. To
facilitate reproducibility and encourage further research, the source code will
be made publicly available.

</details>


### [186] [Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation](https://arxiv.org/abs/2510.01661)
*Yifei Simon Shao,Yuchen Zheng,Sunan Sun,Pratik Chaudhari,Vijay Kumar,Nadia Figueroa*

Main category: cs.RO

TL;DR: SymSkill结合了模仿学习和符号任务与运动规划的优点，实现了动态环境下多步任务的即时组合规划、失败恢复与泛化能力，并在仿真和真实机器人上验证了效果。


<details>
  <summary>Details</summary>
Motivation: 现有多步操作任务要么缺乏泛化与组合性（如模仿学习），要么运行慢且无法实时失败恢复（如经典符号任务与运动规划），无法满足动态环境中机器人高效、鲁棒地完成复杂多步任务的需求。

Method: 提出SymSkill框架，离线阶段从无标签、无分段的人类演示中联合学习谓词、操作符与技能；在线阶段由用户指定谓词目标后，利用符号规划组合和重排序已学习技能，以达成符号目标，并在运动和符号层面实现实时失败恢复；结合合从控制器提升安全性与连续性。

Result: 在RoboCasa仿真中，SymSkill单步任务执行成功率达85%，可实现多步复杂技能的自动组合与多次失败鲁棒恢复。在Franka真实机器人实验证明，利用5分钟无标注演示数据，也可根据目标完成多任务操作。

Conclusion: SymSkill结合IL与TAMP优势，实现了组合泛化和实时鲁棒恢复，为动态多步机器人操作提供了有效可行的解决方案。

Abstract: Multi-step manipulation in dynamic environments remains challenging. Two
major families of methods fail in distinct ways: (i) imitation learning (IL) is
reactive but lacks compositional generalization, as monolithic policies do not
decide which skill to reuse when scenes change; (ii) classical task-and-motion
planning (TAMP) offers compositionality but has prohibitive planning latency,
preventing real-time failure recovery. We introduce SymSkill, a unified
learning framework that combines the benefits of IL and TAMP, allowing
compositional generalization and failure recovery in real-time. Offline,
SymSkill jointly learns predicates, operators, and skills directly from
unlabeled and unsegmented demonstrations. At execution time, upon specifying a
conjunction of one or more learned predicates, SymSkill uses a symbolic planner
to compose and reorder learned skills to achieve the symbolic goals, while
performing recovery at both the motion and symbolic levels in real time.
Coupled with a compliant controller, SymSkill enables safe and uninterrupted
execution under human and environmental disturbances. In RoboCasa simulation,
SymSkill can execute 12 single-step tasks with 85% success rate. Without
additional data, it composes these skills into multi-step plans requiring up to
6 skill recompositions, recovering robustly from execution failures. On a real
Franka robot, we demonstrate SymSkill, learning from 5 minutes of unsegmented
and unlabeled play data, is capable of performing multiple tasks simply by goal
specifications. The source code and additional analysis can be found on
https://sites.google.com/view/symskill.

</details>


### [187] [Geometric Backstepping Control of Omnidirectional Tiltrotors Incorporating Servo-Rotor Dynamics for Robustness against Sudden Disturbances](https://arxiv.org/abs/2510.01675)
*Jaewoo Lee,Dongjae Lee,Jinwoo Lee,Hyungyu Lee,Yeonjoon Kim,H. Jin Kim*

Main category: cs.RO

TL;DR: 本文提出了一种用于可变倾斜全向多旋翼的几何反步控制器，能够显式考虑舵机和转子的动态特性，并通过实验验证了该方法的高鲁棒性和有效性。


<details>
  <summary>Details</summary>
Motivation: 在多旋翼无人机执行高机动飞行或应对突发扰动时，传统控制方案未能充分考虑执行器（舵机和转子）的动态特性，尤其是对可变倾斜角的非线性动力学建模不足，导致系统控制性能和稳定性受限。

Method: 本研究基于多旋翼刚体动力学和非线性执行器动力学之间的级联系统结构，设计了一个几何反步控制器。该控制器系统性地将执行器的非线性动态建模纳入控制回路，并理论上证明了整体系统的指数稳定性。此外，实验揭示了执行器模型参数的不确定性，并验证了控制器对不确定性的鲁棒性。

Result: 通过三组实验（快速平移跟踪、快速旋转跟踪和突发扰动恢复），与未考虑执行器动力学的基线方法进行对比。结果显示，所提控制器在所有场景下均表现出更好的跟踪性能，且在极端工况下能维持系统稳定并完成任务，而基线方法则出现发散甚至坠机。

Conclusion: 提出的几何反步控制器不仅能够有效提升可变倾斜多旋翼在复杂场景下的控制性能，而且具备很强的参数不确定性鲁棒性，显著优于传统不考虑执行器动态的算法。

Abstract: This work presents a geometric backstepping controller for a variable-tilt
omnidirectional multirotor that explicitly accounts for both servo and rotor
dynamics. Considering actuator dynamics is essential for more effective and
reliable operation, particularly during aggressive flight maneuvers or recovery
from sudden disturbances. While prior studies have investigated actuator-aware
control for conventional and fixed-tilt multirotors, these approaches rely on
linear relationships between actuator input and wrench, which cannot capture
the nonlinearities induced by variable tilt angles. In this work, we exploit
the cascade structure between the rigid-body dynamics of the multirotor and its
nonlinear actuator dynamics to design the proposed backstepping controller and
establish exponential stability of the overall system. Furthermore, we reveal
parametric uncertainty in the actuator model through experiments, and we
demonstrate that the proposed controller remains robust against such
uncertainty. The controller was compared against a baseline that does not
account for actuator dynamics across three experimental scenarios: fast
translational tracking, rapid rotational tracking, and recovery from sudden
disturbance. The proposed method consistently achieved better tracking
performance, and notably, while the baseline diverged and crashed during the
fastest translational trajectory tracking and the recovery experiment, the
proposed controller maintained stability and successfully completed the tasks,
thereby demonstrating its effectiveness.

</details>


### [188] [PolySim: Bridging the Sim-to-Real Gap for Humanoid Control via Multi-Simulator Dynamics Randomization](https://arxiv.org/abs/2510.01708)
*Zixing Lei,Zibo Zhou,Sheng Yin,Yueru Chen,Qingyao Xu,Weixin Li,Yunhong Wang,Bowei Tang,Wei Jing,Siheng Chen*

Main category: cs.RO

TL;DR: 本文提出一种名为PolySim的多仿真器联合训练平台，通过同时在多种异构仿真器中训练人形机器人控制策略，有效缓解了『仿真到现实』的落差，实现了更强泛化的全身控制。


<details>
  <summary>Details</summary>
Motivation: 单一仿真器由于内在的归纳偏置导致仿真与真实环境之间存在较大差异，从而影响仿真中训练出的控制策略在现实世界的应用效果。

Method: 提出PolySim训练平台，可在一次训练中并行调用多种不同物理引擎，实现动力学层面的domain randomization（领域随机化），促进策略对不同物理假设的泛化。理论上证明该方法能获得更严格的上界以减少归纳偏置。

Result: 实验表明，PolySim在多个仿真器间能够显著降低动作跟踪误差。例如在MuJoCo环境下，比IsaacSim基线提升52.8%的任务执行成功率。策略还可实现无需微调的直接实物部署。

Conclusion: PolySim有效推进了机器人控制策略从仿真到现实的迁移泛化能力，未来代码将开源，为社区提供新的训练工具。

Abstract: Humanoid whole-body control (WBC) policies trained in simulation often suffer
from the sim-to-real gap, which fundamentally arises from simulator inductive
bias, the inherent assumptions and limitations of any single simulator. These
biases lead to nontrivial discrepancies both across simulators and between
simulation and the real world. To mitigate the effect of simulator inductive
bias, the key idea is to train policies jointly across multiple simulators,
encouraging the learned controller to capture dynamics that generalize beyond
any single simulator's assumptions. We thus introduce PolySim, a WBC training
platform that integrates multiple heterogeneous simulators. PolySim can launch
parallel environments from different engines simultaneously within a single
training run, thereby realizing dynamics-level domain randomization.
Theoretically, we show that PolySim yields a tighter upper bound on simulator
inductive bias than single-simulator training. In experiments, PolySim
substantially reduces motion-tracking error in sim-to-sim evaluations; for
example, on MuJoCo, it improves execution success by 52.8 over an IsaacSim
baseline. PolySim further enables zero-shot deployment on a real Unitree G1
without additional fine-tuning, showing effective transfer from simulation to
the real world. We will release the PolySim code upon acceptance of this work.

</details>


### [189] [Contrastive Representation Regularization for Vision-Language-Action Models](https://arxiv.org/abs/2510.01711)
*Taeyoung Kim,Jimin Lee,Myungkyu Koo,Dongyoung Kim,Kyungmin Lee,Changyeon Kim,Younggyo Seo,Jinwoo Shin*

Main category: cs.RO

TL;DR: 提出了一种新方法（RS-CL），用于增强VLA模型在机器人操作任务中的表现，使模型能更好感知和利用机器人的状态信息。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型虽然用VLM表征推动了机器人操作，但其表征对机器人的状态感知不敏感，导致性能未达最优。

Method: 提出了Robot State-aware Contrastive Loss（RS-CL），通过鼓励模型的表征和机器人的本体状态保持一致，利用机器人状态间的距离作为软监督，从而提升VLA模型对控制相关特征的感知。该方法计算简单，并可与标准VLA训练流程无缝集成。

Result: 该方法在RoboCasa-Kitchen环境的抓取与放置任务中，将SOTA模型表现从30.8%提升到41.5%；在真实机器人操作任务中的成功率由45.0%升至58.3%。

Conclusion: RS-CL显著提高了VLA模型的操作性能，特别是在提升模型对机器人物理状态感知方面有突出效果，实现了更准确的抓取与放置能力。

Abstract: Vision-Language-Action (VLA) models have shown its capabilities in robot
manipulation by leveraging rich representations from pre-trained
Vision-Language Models (VLMs). However, their representations arguably remain
suboptimal, lacking sensitivity to robotic signals such as control actions and
proprioceptive states. To address the issue, we introduce Robot State-aware
Contrastive Loss (RS-CL), a simple and effective representation regularization
for VLA models, designed to bridge the gap between VLM representations and
robotic signals. In particular, RS-CL aligns the representations more closely
with the robot's proprioceptive states, by using relative distances between the
states as soft supervision. Complementing the original action prediction
objective, RS-CL effectively enhances control-relevant representation learning,
while being lightweight and fully compatible with standard VLA training
pipeline. Our empirical results demonstrate that RS-CL substantially improves
the manipulation performance of state-of-the-art VLA models; it pushes the
prior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen,
through more accurate positioning during grasping and placing, and boosts
success rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.

</details>


### [190] [Dual-Mode Magnetic Continuum Robot for Targeted Drug Delivery](https://arxiv.org/abs/2510.01761)
*Wendu Zhang,Heng Wang,Shuangyi Wang,Yuanrui Huang*

Main category: cs.RO

TL;DR: 本文提出了一种创新的磁性连续体机器人（MCR）设计，通过在导管壁内径向嵌入永久磁铁，实现单一外部磁体独立诱导机器人弯曲或扭转，大幅提升机器人的变形能力与功能多样性。


<details>
  <summary>Details</summary>
Motivation: 传统轴向磁化的MCR受限于只能实现弯曲动作，缺乏更加灵活的运动方式。为了扩大其变形能力，满足复杂医学场景如靶向给药的需求，亟需开发能够实现多自由度运动的解决方案。

Method: 作者提出将永久磁铁径向嵌埋于导管壁，结合物理建模与有限元分析，阐明了该机构的驱动原理，并通过台架实验验证了弯曲与扭转运动的独立控制。进一步设计了双层阻断装置，实现基于扭转剪切力的药物按需释放。

Result: 结果显示，该结构能在实际磁场下实现运动模式的解耦控制。药物释放实验和体外介入操作实验均证明设备能精确定位与按需给药，全流程顺利运行。

Conclusion: 该平台结构紧凑、无需拉索驱动，能够实现复杂变形和精准药物释放，显示出作为新一代靶向介入治疗工具的巨大应用前景。

Abstract: Magnetic continuum robots (MCRs) enable minimally invasive navigation through
tortuous anatomical channels, yet axially magnetized designs have largely been
limited to bending-only motion. To expand deformation capabilities, this paper
presents a simple assembly that embeds permanent magnets radially within the
catheter wall, allowing a single externally steered permanent magnet to
independently induce either bending or torsion. A physics-based formulation
together with finite-element analysis establishes the actuation principles, and
benchtop experiments validate decoupled mode control under practical fields.
Building on this, a dual-layer blockage mechanism consisting of outer grooves
and inner plates leverages torsional shear to achieve on-demand drug release.
Finally, an in-phantom intervention experiment demonstrates end-to-end
operation: lumen following by bending for target approach, followed by
twist-activated release at the site. The resulting compact, cable-free platform
combines versatile deformation with precise payload delivery, indicating strong
potential for next-generation, site-specific therapies.

</details>


### [191] [An Anytime, Scalable and Complete Algorithm for Embedding a Manufacturing Procedure in a Smart Factory](https://arxiv.org/abs/2510.01770)
*Christopher Leet,Aidan Sciortino,Sven Koenig*

Main category: cs.RO

TL;DR: 本文提出了一种高可扩展性的智能工厂嵌入（SFE）问题求解方法TS-ACES，可支持含百台以上机器的大规模工厂中的制造过程流程自动编排。


<details>
  <summary>Details</summary>
Motivation: 随着现代工厂日益自动化，制造过程需要在大量程序化机器（如3D打印机）及传输系统（如机器人车队）中高效配置与调度，但现有方法仅能处理几十台机器，难以满足实际大型智能工厂的需求。

Method: 作者提出了TS-ACES（基于交通系统的任意时循环嵌入求解器），作为第一个能够高效处理大规模SFE问题的完整解决方案，对制造流程与运输进行了联合分配与优化。

Result: TS-ACES能够扩展到基于真实工业场景的超过百台机器的SFE实例，且保持完整性（即所有可行解均能覆盖）。

Conclusion: TS-ACES填补了大规模智能工厂制造过程嵌入问题求解上的空白，为工业界部署和优化大规模智能制造流程提供了强有力的技术支撑。

Abstract: Modern automated factories increasingly run manufacturing procedures using a
matrix of programmable machines, such as 3D printers, interconnected by a
programmable transport system, such as a fleet of tabletop robots. To embed a
manufacturing procedure into a smart factory, an operator must: (a) assign each
of its processes to a machine and (b) specify how agents should transport parts
between machines. The problem of embedding a manufacturing process into a smart
factory is termed the Smart Factory Embedding (SFE) problem. State-of-the-art
SFE solvers can only scale to factories containing a couple dozen machines.
Modern smart factories, however, may contain hundreds of machines. We fill this
hole by introducing the first highly scalable solution to the SFE, TS-ACES, the
Traffic System based Anytime Cyclic Embedding Solver. We show that TS-ACES is
complete and can scale to SFE instances based on real industrial scenarios with
more than a hundred machines.

</details>


### [192] [Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2510.01795)
*Haibo Hu,Lianming Huang,Xinyu Wang,Yufei Cui,Nan Guan,Chun Jason Xue*

Main category: cs.RO

TL;DR: 该论文提出了一种导航引导的早退推理方法Nav-EE，在自动驾驶中的视觉-语言模型推理过程中，通过结合导航先验，大幅降低了模型推理延迟，同时保证感知和推理精度。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶中广泛使用视觉-语言模型（VLMs）以实现统一的感知与推理，但此类大模型推理延迟较高，难以满足实时性需求。虽然早退（early-exit）策略能够降低延迟，但由于其任务相关性强，直接应用在多样化驾驶场景下泛化能力有限。理论上，自动驾驶场景下能够提前获得导航信息，因此可预测将要执行的任务，对推理流程进行定制化优化。

Method: 作者提出了Nav-EE框架：利用导航引导，针对特定任务预先离线计算模型适合的早退层，实时推理阶段根据导航先验动态选择最优的早退层，从而实现高效推理。该方法在CODA、Waymo、BOSCH等自动驾驶数据集及真实车辆平台Autoware Universe上进行了测试。

Result: Nav-EE在不损失准确率的前提下，推理延迟最高可降低63.9%；真实车辆集成实验中，推理延迟由600ms降至300ms，有效提升了复杂场景下的决策速度。

Conclusion: 将导航先验与早退推理机制结合，可有效提高大模型在自动驾驶系统中的推理效率和部署可行性，为面向实际复杂场景的高效感知-推理系统落地提供了新方向。

Abstract: Vision-Language Models (VLMs) are increasingly applied in autonomous driving
for unified perception and reasoning, but high inference latency hinders
real-time deployment. Early-exit reduces latency by terminating inference at
intermediate layers, yet its task-dependent nature limits generalization across
diverse scenarios. We observe that this limitation aligns with autonomous
driving: navigation systems can anticipate upcoming contexts (e.g.,
intersections, traffic lights), indicating which tasks will be required. We
propose Nav-EE, a navigation-guided early-exit framework that precomputes
task-specific exit layers offline and dynamically applies them online based on
navigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE
achieves accuracy comparable to full inference while reducing latency by up to
63.9%. Real-vehicle integration with Autoware Universe further demonstrates
reduced inference latency (600ms to 300ms), supporting faster decision-making
in complex scenarios. These results suggest that coupling navigation foresight
with early-exit offers a viable path toward efficient deployment of large
models in autonomous systems. Code and data are available at our anonymous
repository: https://anonymous.4open.science/r/Nav-EE-BBC4

</details>


### [193] [What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework](https://arxiv.org/abs/2510.01830)
*Hongze Wang,Boyang Sun,Jiaxu Xing,Fan Yang,Marco Hutter,Dhruv Shah,Davide Scaramuzza,Marc Pollefeys*

Main category: cs.RO

TL;DR: 本论文通过大规模实证分析，揭示了影响ObjectNav任务性能的关键因素，并提出超越现有技术的新系统和设计指南。


<details>
  <summary>Details</summary>
Motivation: ObjectNav任务对于在未知日常环境中部署机器人至关重要，但如何有效融合语义理解、空间推理和长期规划依然极具挑战。目前主流方法为强化学习，但社区尚不清楚哪些系统模块真正对性能有决定性作用。

Method: 作者将基于模块化强化学习的ObjectNav系统分为感知、策略和测试时增强三大模块，并通过大量受控实验，分别评估每一模块对整体性能的贡献。

Result: 实验发现，感知模块质量和测试时策略提升显著影响性能，而当前主流的策略改进手段带来提升有限。基于此，作者实现的系统在SPL指标上提升6.6%，成功率提升2.7%。同时，专家人工基准表现极高，显示目前RL方法与人类水平仍有差距。

Conclusion: 论文不仅刷新了ObjectNav任务的SotA，也为后续系统设计和评估提供了实践指南，强调未来应聚焦于感知优化与测试时策略提升。

Abstract: Object-Goal Navigation (ObjectNav) is a critical component toward deploying
mobile robots in everyday, uncontrolled environments such as homes, schools,
and workplaces. In this context, a robot must locate target objects in
previously unseen environments using only its onboard perception. Success
requires the integration of semantic understanding, spatial reasoning, and
long-horizon planning, which is a combination that remains extremely
challenging. While reinforcement learning (RL) has become the dominant
paradigm, progress has spanned a wide range of design choices, yet the field
still lacks a unifying analysis to determine which components truly drive
performance. In this work, we conduct a large-scale empirical study of modular
RL-based ObjectNav systems, decomposing them into three key components:
perception, policy, and test-time enhancement. Through extensive controlled
experiments, we isolate the contribution of each and uncover clear trends:
perception quality and test-time strategies are decisive drivers of
performance, whereas policy improvements with current methods yield only
marginal gains. Building on these insights, we propose practical design
guidelines and demonstrate an enhanced modular system that surpasses
State-of-the-Art (SotA) methods by 6.6% on SPL and by a 2.7% success rate. We
also introduce a human baseline under identical conditions, where experts
achieve an average 98% success, underscoring the gap between RL agents and
human-level navigation. Our study not only sets the SotA performance but also
provides principled guidance for future ObjectNav development and evaluation.

</details>


### [194] [Like Playing a Video Game: Spatial-Temporal Optimization of Foot Trajectories for Controlled Football Kicking in Bipedal Robots](https://arxiv.org/abs/2510.01843)
*Wanyue Li,Ji Ma,Minghao Lu,Peng Lu*

Main category: cs.RO

TL;DR: 本文针对人形机器人踢球时的稳定性与精准控球难题，提出一种创新的时空轨迹规划方法，显著提升了机器人的踢球表现和轨迹优化效率。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人在剧烈踢球动作下维持系统稳定与球轨迹精准控制存在挑战。传统基于位置的控制和强化学习方法均有不足，MPC方法往往对腿部摆动过程过度简化，限制了脚部与环境的交互能力，难以实现高质量踢球。

Method: 将在无人机领域取得成功的时空轨迹规划方法移植到双足机器人系统中，根据目标踢球位置、速度及加速度自动生成满足约束条件的脚步轨迹，同时优化摆动相位的时长。

Result: 实验表明，该优化轨迹在动作和动态特性上接近人类踢球表现，包括预先摆腿动作。仿真和实体机器人实验均显示算法高效、稳定，规划耗时低于1毫秒，踢球任务覆盖-90°到90°范围时接近100%的完成率。

Conclusion: 所提出的轨迹规划方法显著提升了人形机器人踢球的效率与可靠性，能够自动产生高质量踢球动作，对提升足球机器人实际竞技能力具有应用价值。

Abstract: Humanoid robot soccer presents several challenges, particularly in
maintaining system stability during aggressive kicking motions while achieving
precise ball trajectory control. Current solutions, whether traditional
position-based control methods or reinforcement learning (RL) approaches,
exhibit significant limitations. Model predictive control (MPC) is a prevalent
approach for ordinary quadruped and biped robots. While MPC has demonstrated
advantages in legged robots, existing studies often oversimplify the leg swing
progress, relying merely on simple trajectory interpolation methods. This
severely constrains the foot's environmental interaction capability, hindering
tasks such as ball kicking. This study innovatively adapts the spatial-temporal
trajectory planning method, which has been successful in drone applications, to
bipedal robotic systems. The proposed approach autonomously generates foot
trajectories that satisfy constraints on target kicking position, velocity, and
acceleration while simultaneously optimizing swing phase duration. Experimental
results demonstrate that the optimized trajectories closely mimic human kicking
behavior, featuring a backswing motion. Simulation and hardware experiments
confirm the algorithm's efficiency, with trajectory planning times under 1 ms,
and its reliability, achieving nearly 100 % task completion accuracy when the
soccer goal is within the range of -90{\deg} to 90{\deg}.

</details>


### [195] [GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for Mobile Robotics](https://arxiv.org/abs/2510.01848)
*Diram Tabaa,Gianni Di Caro*

Main category: cs.RO

TL;DR: 本论文提出了GreenhouseSplat：利用廉价RGB图像生成温室高真实感三维资产的框架和数据集，并集成到ROS仿真系统中，支持相机和激光雷达的渲染，用于农业机器人评测。


<details>
  <summary>Details</summary>
Motivation: 现有温室环境仿真方法多依赖简化或合成资产，影响了仿真到现实机器人系统的迁移表现。最新的辐射场（如高斯膨胀）方法具备真实感建模能力，但目前仅应用于单个植株或受控实验环境。论文旨在扩展这些方法以模拟完整温室，推动农业机器人研发。

Method: 设计了GreenhouseSplat框架：利用多视角RGB图片，自动构建高保真的温室3D资产，并集成入基于ROS的仿真环境，支持各种传感器（如摄像头与激光雷达）渲染。构建了包含82株黄瓜、多排结构的数据集，并支持定位等机器人任务测试。

Result: 成功生成了高真实感的温室资产，并集成到仿真系统中。通过数据集展示了在多种行排列下农业机器人评估的可行性和有效性。

Conclusion: GreenhouseSplat为温室级辐射场仿真打开了新局面，为农业机器人领域的真实仿真与评测奠定了基础，对未来进一步研究具有重要意义。

Abstract: Simulating greenhouse environments is critical for developing and evaluating
robotic systems for agriculture, yet existing approaches rely on simplistic or
synthetic assets that limit simulation-to-real transfer. Recent advances in
radiance field methods, such as Gaussian splatting, enable photorealistic
reconstruction but have so far been restricted to individual plants or
controlled laboratory conditions. In this work, we introduce GreenhouseSplat, a
framework and dataset for generating photorealistic greenhouse assets directly
from inexpensive RGB images. The resulting assets are integrated into a
ROS-based simulation with support for camera and LiDAR rendering, enabling
tasks such as localization with fiducial markers. We provide a dataset of 82
cucumber plants across multiple row configurations and demonstrate its utility
for robotics evaluation. GreenhouseSplat represents the first step toward
greenhouse-scale radiance-field simulation and offers a foundation for future
research in agricultural robotics.

</details>


### [196] [TACOS: Task Agnostic COordinator of a multi-drone System](https://arxiv.org/abs/2510.01869)
*Alessandro Nazzari,Roberto Rubinacci,Marco Lovera*

Main category: cs.RO

TL;DR: 本文提出了TACOS框架，借助大语言模型，让单一操作者能通过自然语言高效控制多无人机系统，实现多级自主操作。作者展示了框架在现实多无人机系统中的效果，并通过消融实验评估了各模块的作用。


<details>
  <summary>Details</summary>
Motivation: 随着无人机集群任务复杂性提升，单一操作员需要在不同自主程度间切换，如何降低工作负荷、高效指挥成为难题。自然语言模型具备推理、计划能力，可以帮助简化高层任务委派。

Method: 作者设计了TACOS框架，包括自然语言接口、意图转结构化任务计划的协调模块、及自主执行模块。框架使LLM能调用实际API，实现从语义到物理的无人机系统控制。

Result: TACOS在真实多无人机平台中得到验证，实验展示了系统能凭自然语言实现高效多层指挥。消融实验表明各模块均有显著贡献。

Conclusion: TACOS证实了大语言模型驱动的自然语言接口可提升多无人机系统的可用性和效率，为复杂机器人系统的人机交互提供了新思路。

Abstract: When a single pilot is responsible for managing a multi-drone system, the
task demands varying levels of autonomy, from direct control of individual
UAVs, to group-level coordination, to fully autonomous swarm behaviors for
accomplishing high-level tasks. Enabling such flexible interaction requires a
framework that supports multiple modes of shared autonomy. As language models
continue to improve in reasoning and planning, they provide a natural
foundation for such systems, reducing pilot workload by enabling high-level
task delegation through intuitive, language-based interfaces. In this paper we
present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified
framework that enables high-level natural language control of multi-UAV systems
through Large Language Models (LLMs). TACOS integrates three key capabilities
into a single architecture: a one-to-many natural language interface for
intuitive user interaction, an intelligent coordinator for translating user
intent into structured task plans, and an autonomous agent that executes plans
interacting with the real-world. TACOS allows a LLM to interact with a library
of executable APIs, bridging semantic reasoning with real-time multi-robot
coordination. We demonstrate the system in real-world multi-drone system and
conduct an ablation study to assess the contribution of each module.

</details>


### [197] [SPARC: Spine with Prismatic and Revolute Compliance for Quadruped Robot](https://arxiv.org/abs/2510.01984)
*Yue Wang*

Main category: cs.RO

TL;DR: 本文提出了一个紧凑型开源的3自由度脊柱模块（SPARC），用于四足机器人，通过实验验证其可实现可编程阻抗控制，具备可移植性和开放性。


<details>
  <summary>Details</summary>
Motivation: 仿生机器人特别是四足机器人的运动性能高度依赖于“脊柱”结构的顺应性与控制方式，目前缺乏开源、小型化且具备阻抗整形能力的专用模块，研究脊柱合顺性的系统性方法有限。

Method: 设计并实现了质量仅1.26kg的3自由度模组，内置三轴力矩控制器、自研1kHz控制板和保护电源，通过闭环实现x、z、theta三个方向的刚度和阻尼控制。提出基于RNEA的加速度控制方法，并结合Stribeck摩擦补偿实现弹簧-阻尼器动态行为。进行了静态推拉测试和动态位移释放实验。

Result: 静态测试表明，该模块实现了300-700 N/m水平刚度控制，误差低于1.5%，动态测试展现出期望的质量-弹簧-阻尼响应，低速摩擦与惯性耦合对相位具有小幅等可解释影响。采用常规PD控制器对比下，刚度线性度和一致性较差。

Conclusion: SPARC是一个验证充分、具有高可控性和可移植性的开源仿生脊柱模块，非常适合四足机器人合顺性研究，将公开硬件与固件资源，为该领域系统性实验和开发提供平台。

Abstract: We present SPARC, a compact, open-source 3-DoF sagittal-plane spine module
that combines revolute (pitch) and prismatic (axial) motion with programmable
task-space impedance for quadruped robots. The system integrates three
torque-controlled actuators, a custom 1 kHz control board, and a protected
power unit in a 1.26 kg package, enabling closed-loop stiffness and damping
shaping along x, z, and theta. We develop an RNEA-based computed-acceleration
controller with smooth Stribeck friction compensation to render spring-damper
behavior without explicit inertia shaping. Bench experiments validate the
approach. Quasi-static push-pull tests show linear force-displacement
characteristics with commanded horizontal stiffness spanning 300-700 N/m and <=
1.5% relative error (R^2 >= 0.992, narrow 95% CIs). Dynamic
displace-and-release trials confirm mass-spring-damper responses over multiple
damping settings, with small, interpretable phase deviations due to
configuration-dependent inertia and low-speed friction effects. A task-space PD
controller produces roughly linear stiffness but with greater variability and
coupling sensitivity. SPARC provides a portable platform for systematic studies
of spine compliance in legged locomotion and will be released with complete
hardware and firmware resources.

</details>


### [198] [Reducing Discomfort in Driving Simulators: Motion Cueing for Motion Sickness Mitigation](https://arxiv.org/abs/2510.01986)
*Varun Kotian,Vishrut Jain,Andrea Michelle Rios Lazcano,Daan Marinus Pool,Riender Happee,Barys Shyrokau*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型预测控制（MPC）的运动提示算法，可在驾驶模拟器中兼顾减少晕动症和运动再现的真实性。实验表明，该算法有效降低了用户晕动症水平且无显著损失运动真实感。


<details>
  <summary>Details</summary>
Motivation: 驾驶模拟器常因运动缩减与视听未按比例缩放引发晕动症，影响R&D效率和模拟体验，因此迫切需要同时优化舒适性和真实感的运动提示算法。

Method: 作者利用模型预测控制（MPC），在算法损失函数中同时惩罚传感冲突（sensory conflict）和特定力误差，实现对运动真实感和晕动症的共同最小化。通过人机实验对比了四种运动设置，包括两种MPC算法变型、传统自适应洗涤算法（adaptive washout）和无运动情况。

Result: 实验发现，无运动条件晕动症最低但真实性评分也最低。提出的折中MPC算法相比传统方法将平均晕动症水平降幅超过50%，且未显著降低真实性评分。实验结果与晕动症模型预测高度一致。

Conclusion: 结合模拟器动力学和晕动症时程的MPC运动提示算法，可在驾驶模拟器中实现晕动症控制和运动真实感的最佳平衡，推动模拟器在更广泛领域的应用。

Abstract: Driving simulators are increasingly used in research and development.
However, simulators often cause motion sickness due to downscaled motion and
unscaled veridical visuals. In this paper, a motion cueing algorithm is
proposed that reduces motion sickness as predicted by the subjective vertical
conflict (SVC) model using model predictive control (MPC). Both sensory
conflict and specific force errors are penalised in the cost function, allowing
the algorithm to jointly optimise fidelity and comfort.
  Human-in-the-loop experiments were conducted to compare four simulator motion
settings: two variations of our MPC-based algorithm, one focused on pure
specific force tracking and the second compromising specific force tracking and
motion sickness minimisation, as well as reference adaptive washout and no
motion cases. The experiments were performed on a hexapod driving simulator
with participants exposed to passive driving.
  Experimental motion sickness results closely matched the sickness model
predictions. As predicted by the model, the no motion condition yielded the
lowest sickness levels. However, it was rated lowest in terms of fidelity. The
compromise solution reduced sickness by over 50% (average MISC level 3 to 1.5)
compared to adaptive washout and the algorithm focusing on specific force
tracking, without any significant reduction in fidelity rating.
  The proposed approach for developing MCA that takes into account both the
simulator dynamics and time evolution of motion sickness offers a significant
advancement in achieving an optimal control of motion sickness and specific
force recreation in driving simulators, supporting broader simulator use.

</details>


### [199] [EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction](https://arxiv.org/abs/2510.02080)
*Lingxiang Hu,Naima Ait Oufroukh,Fabien Bonardi,Raymond Ghandour*

Main category: cs.RO

TL;DR: 本文提出了一种新型单目稠密SLAM系统EC3R-SLAM，无需相机标定，具有高精度、低延迟和低显存消耗，可用于资源有限设备。


<details>
  <summary>Details</summary>
Motivation: 传统单目稠密SLAM系统存在高延迟、高GPU显存占用和依赖相机标定的问题，限制了其实用性和普适性。因此，亟需一种同时兼顾准确性与效率、且无需标定的SLAM方法。

Method: 作者提出了EC3R-SLAM框架，通过跟踪模块维护稀疏特征点地图，结合基于前馈3D重建模型的建图模块，同时估计相机内参。此外，系统集成了局部与全局回环检测，以提升中长期的数据关联和多视图一致性。

Result: 在多个基准数据集上，EC3R-SLAM在准确率上可比肩当前最先进方法，同时速度更快、显存消耗更低，并能在笔记本、Jetson Orin NX等受限硬件上高效运行。

Conclusion: EC3R-SLAM在精度、速度和资源消耗各方面取得了均衡，具有良好的实际部署潜力，特别适用于资源受限的机器人应用场景。

Abstract: The application of monocular dense Simultaneous Localization and Mapping
(SLAM) is often hindered by high latency, large GPU memory consumption, and
reliance on camera calibration. To relax this constraint, we propose EC3R-SLAM,
a novel calibration-free monocular dense SLAM framework that jointly achieves
high localization and mapping accuracy, low latency, and low GPU memory
consumption. This enables the framework to achieve efficiency through the
coupling of a tracking module, which maintains a sparse map of feature points,
and a mapping module based on a feed-forward 3D reconstruction model that
simultaneously estimates camera intrinsics. In addition, both local and global
loop closures are incorporated to ensure mid-term and long-term data
association, enforcing multi-view consistency and thereby enhancing the overall
accuracy and robustness of the system. Experiments across multiple benchmarks
show that EC3R-SLAM achieves competitive performance compared to
state-of-the-art methods, while being faster and more memory-efficient.
Moreover, it runs effectively even on resource-constrained platforms such as
laptops and Jetson Orin NX, highlighting its potential for real-world robotics
applications.

</details>


### [200] [LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot Grasping with Ambiguous Instructions](https://arxiv.org/abs/2510.02104)
*Yunhan Lin,Wenqi Wu,Zhijie Zhang,Huasong Min*

Main category: cs.RO

TL;DR: 提出了一种新颖的语言交互机器人抓取框架LangGrasp，可通过大语言模型理解含糊指令中的隐含意图，实现从对象到细粒度部件的精准抓取。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言的机器人抓取方法难以处理包含隐含意图的模糊指令，这限制了机器人在复杂或未知环境下的抓取灵活性和适应性。

Method: LangGrasp框架结合了微调的大语言模型，利用其通用常识和环境感知能力，推理指令中的隐含意图并明确任务需求及目标操作物体。同时，设计了基于2D分割引导的点云局部化模块，实现多粒度场景下的抓取定位。

Result: 实验表明，LangGrasp能准确解析含糊指令中的隐含意图，找出任务所需关键操作与目标物体，并结合环境信息动态选择最佳抓取位姿，实现高精度的对象级至部件级抓取。

Conclusion: LangGrasp大幅提升了机器人在无结构环境中的适应性和抓取任务执行效率，并为处理复杂语言指令的机器人操作提供了新思路。

Abstract: The existing language-driven grasping methods struggle to fully handle
ambiguous instructions containing implicit intents. To tackle this challenge,
we propose LangGrasp, a novel language-interactive robotic grasping framework.
The framework integrates fine-tuned large language models (LLMs) to leverage
their robust commonsense understanding and environmental perception
capabilities, thereby deducing implicit intents from linguistic instructions
and clarifying task requirements along with target manipulation objects.
Furthermore, our designed point cloud localization module, guided by 2D part
segmentation, enables partial point cloud localization in scenes, thereby
extending grasping operations from coarse-grained object-level to fine-grained
part-level manipulation. Experimental results show that the LangGrasp framework
accurately resolves implicit intents in ambiguous instructions, identifying
critical operations and target information that are unstated yet essential for
task completion. Additionally, it dynamically selects optimal grasping poses by
integrating environmental information. This enables high-precision grasping
from object-level to part-level manipulation, significantly enhancing the
adaptability and task execution efficiency of robots in unstructured
environments. More information and code are available here:
https://github.com/wu467/LangGrasp.

</details>


### [201] [Stand Up, NAO! Increasing the Reliability of Stand-Up Motions Through Error Compensation in Position Control](https://arxiv.org/abs/2510.02129)
*Philip Reichenberg,Tim Laue*

Main category: cs.RO

TL;DR: 本文提出了一套适用于NAO人形机器人足球的站起动作方案，通过减少关节位置误差和增加动作补偿，大幅提升了机器人自主站起的成功率，并在多支队伍及比赛中得到验证。


<details>
  <summary>Details</summary>
Motivation: 在人形机器人足球比赛中，若机器人摔倒后无法自主站起，将被强制离场。提升站起成功率对比赛成绩及机器人的连续性操作至关重要。

Method: 作者自2019年起设计并持续优化了NAO机器人的站起动作，通过检测并修正关节位置误差，采取特殊动作释放被卡肢体（如手臂），或用其他关节进行误差补偿，降低失败概率。

Result: 优化后的站起动作显著提升了机器人站起的整体成功率。此外，该方案被多个标准平台联赛队伍采用，并通过多次比赛视频分析，证明了其高成功率的普遍性。

Conclusion: 通过针对性地减少关节误差并灵活补偿，机器人能更稳定地完成站起动作，为比赛持续作战提供了保障，该方法已在多个队伍和赛事中获得应用与认可。

Abstract: Stand-up motions are an indispensable part of humanoid robot soccer. A robot
incapable of standing up by itself is removed from the game for some time. In
this paper, we present our stand-up motions for the NAO robot. Our approach
dates back to 2019 and has been evaluated and slightly expanded over the past
six years. We claim that the main reason for failed stand-up attempts are large
errors in the executed joint positions. By addressing such problems by either
executing special motions to free up stuck limbs such as the arms, or by
compensating large errors with other joints, we significantly increased the
overall success rate of our stand-up routine. The motions presented in this
paper are also used by several other teams in the Standard Platform League,
which thereby achieve similar success rates, as shown in an analysis of videos
from multiple tournaments.

</details>


### [202] [SCANS: A Soft Gripper with Curvature and Spectroscopy Sensors for In-Hand Material Differentiation](https://arxiv.org/abs/2510.02164)
*Nathaniel Hanson,Austin Allison,Charles DiMarzio,Taşkın Padır,Kristen L. Dorsey*

Main category: cs.RO

TL;DR: 本文提出了一种名为SCANS的软体机械手系统，通过流体驱动实现无电子元件的光谱探测，可识别不同物体类型。


<details>
  <summary>Details</summary>
Motivation: 现有软体机器人在感知能力，尤其是光谱检测能力方面有限，难以进行多材料、多功能的探测。作者希望开发更强大的软体机器人感知平台，提升其物体识别和分类能力。

Method: 作者设计了一种通过流体驱动的软体机械手（SCANS），无需电子元件，可实现手持和捕捉前的物体光谱检测。通过材料分析确定最佳软体基底，并在多种物体类型和尺寸下测试其性能。同时采用线性判别分析等方法评估光谱数据区分能力。

Result: SCANS系统在金属、木材、塑料、有机物、纸张、泡沫等不同种类与尺寸的物体间实现了较大的光谱区分度，尤其在近红外波段对外观相似物体具有较高敏感性。

Conclusion: SCANS推进了光学感知在软体机器人中的多功能应用，为软体机器人实现更先进的感知和识别能力打下基础。相关设计和代码已公开。

Abstract: We introduce the soft curvature and spectroscopy (SCANS) system: a versatile,
electronics-free, fluidically actuated soft manipulator capable of assessing
the spectral properties of objects either in hand or through pre-touch caging.
This platform offers a wider spectral sensing capability than previous soft
robotic counterparts. We perform a material analysis to explore optimal soft
substrates for spectral sensing, and evaluate both pre-touch and in-hand
performance. Experiments demonstrate explainable, statistical separation across
diverse object classes and sizes (metal, wood, plastic, organic, paper, foam),
with large spectral angle differences between items. Through linear
discriminant analysis, we show that sensitivity in the near-infrared
wavelengths is critical to distinguishing visually similar objects. These
capabilities advance the potential of optics as a multi-functional sensory
modality for soft robots. The complete parts list, assembly guidelines, and
processing code for the SCANS gripper are accessible at:
https://parses-lab.github.io/scans/.

</details>


### [203] [Product Digital Twin Supporting End-of-life Phase of Electric Vehicle Batteries Utilizing Product-Process-Resource Asset Network](https://arxiv.org/abs/2510.02167)
*Sara Strakosova,Petr Novak,Petr Kadera*

Main category: cs.RO

TL;DR: 本文提出将数字孪生技术应用于产品拆解流程优化，降低环境影响，并以电动汽车电池为案例，展示该方法增强循环经济下再制造/回收效率的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前制造商通常不共享有助于产品再制造或回收的数据，阻碍了循环经济的实现。为提升可持续性和环境保护，亟需技术手段支持产品端的拆解和再利用过程。

Method: 作者提出了基于产品-过程-资源资产网络（PAN）范式的数字孪生建模，并扩展为双向流（Bi-PAN）网络，以同步支持制造与再制造/回收阶段。在实际中，以电动汽车电池为例，构建其数字孪生体，优化其拆解操作流程。

Result: 基于数字孪生的产品信息能够灵活、有效地解决不同类型电动汽车电池在拆解过程中面临的挑战，提高资源回收与再制造效率。

Conclusion: 数字孪生与Bi-PAN网络为循环经济中的产品拆解、再制造和回收提供了数据支撑与流程优化依据，有助于实现更高水平的环境可持续发展。

Abstract: In the context of the circular economy, products in their end-of-life phase
should be either remanufactured or recycled. Both of these processes are
crucial for sustainability and environmental conservation. However,
manufacturers often do not support these processes enough by not sharing
relevant data. This paper proposes use of a digital twin technology, which is
capable to help optimizing the disassembly processes to reduce ecological
impact and enhance sustainability. The proposed approach is demonstrated
through a disassembly use-case of the product digital twin of an electric
vehicle battery. By utilizing product digital twins, challenges associated with
the disassembly of electric vehicle batteries can be solved flexibly and
efficiently for various battery types. As a backbone for the product digital
twin representation, the paper uses the paradigm of product-process-resource
asset networks (PAN). Such networks enable to model relevant relationships
across products, production resources, manufacturing processes, and specific
production operations that have to be done in the manufacturing phase of a
product. This paper introduces a Bi-Flow Product-Process-Resource Asset Network
(Bi-PAN) representation, which extends the PAN paradigm to cover not only the
manufacturing, but also the remanufacturing/recycling phase.

</details>


### [204] [DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis](https://arxiv.org/abs/2510.02178)
*Jialin Gao,Donghao Zhou,Mingjian Liang,Lihao Liu,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.RO

TL;DR: 论文提出DisCo-Layout框架，通过语义与物理层面双重优化，生成更真实和可泛化的3D室内布局。


<details>
  <summary>Details</summary>
Motivation: 现有3D室内布局生成方法泛化能力有限，常常因为数据集固定导致布局多样性不足。虽然结合大语言模型（LLM）和视觉语言模型（VLM）的方法提升了语义表现，但其布局精细优化能力不足，难以达到理想布局效果。

Method: DisCo-Layout将物理优化和语义优化解耦并协同。语义层面，Semantic Refinement Tool（SRT）优化抽象对象关系；物理层面，Physical Refinement Tool（PRT）采用网格匹配算法优化空间排布。两者协作，由多智能体架构实现，包括规划者（placement rules）、设计者（初始布局）及评估者（评估效果）。

Result: 实验证明DisCo-Layout能生成更真实、连贯和具更好泛化能力的3D室内布局，达到业界先进水平。

Conclusion: DisCo-Layout通过协调语义与物理双层优化，有效提升3D室内布局生成质量，具备很强的实际应用潜力。相关代码即将开源。

Abstract: 3D indoor layout synthesis is crucial for creating virtual environments.
Traditional methods struggle with generalization due to fixed datasets. While
recent LLM and VLM-based approaches offer improved semantic richness, they
often lack robust and flexible refinement, resulting in suboptimal layouts. We
develop DisCo-Layout, a novel framework that disentangles and coordinates
physical and semantic refinement. For independent refinement, our Semantic
Refinement Tool (SRT) corrects abstract object relationships, while the
Physical Refinement Tool (PRT) resolves concrete spatial issues via a
grid-matching algorithm. For collaborative refinement, a multi-agent framework
intelligently orchestrates these tools, featuring a planner for placement
rules, a designer for initial layouts, and an evaluator for assessment.
Experiments demonstrate DisCo-Layout's state-of-the-art performance, generating
realistic, coherent, and generalizable 3D indoor layouts. Our code will be
publicly available.

</details>


### [205] [Performance-Guided Refinement for Visual Aerial Navigation using Editable Gaussian Splatting in FalconGym 2.0](https://arxiv.org/abs/2510.02248)
*Yan Miao,Ege Yuceel,Georgios Fainekos,Bardh Hoxha,Hideki Okamoto,Sayan Mitra*

Main category: cs.RO

TL;DR: 本文提出了FalconGym 2.0仿真框架与PGR算法，显著提升了视觉导航策略的泛化性和鲁棒性，并实现了零样本仿真到现实的成功迁移。


<details>
  <summary>Details</summary>
Motivation: 现有视觉导航策略过拟合并且泛化能力差，特别是在航线几何变化时表现下降，亟需新的方法提升通用性和鲁棒性。

Method: 开发了基于Gaussian Splatting的高保真仿真平台FalconGym 2.0，具备可编辑API可快速生成多样化静态和动态赛道。基于该平台提出Performance-Guided Refinement (PGR) 算法，将训练重点聚焦于更具挑战性的赛道，通过不断迭代提升策略性能。

Result: 在固定翼无人机与四旋翼两类动态和环境案例中，用PGR算法训练的单一视觉策略在三条未见过的赛道上达到100%成功率，并在门障扰动下较基线方法表现更佳。在硬件四旋翼实物零样本迁移实验中，达到了98.6%的高成功率。

Conclusion: FalconGym 2.0结合PGR算法，能够有效提升视觉导航策略的泛化与鲁棒性，并支持无须专项再训练即可从仿真成功迁移至真实无人机平台，对自动化空中导航有重要意义。

Abstract: Visual policy design is crucial for aerial navigation. However,
state-of-the-art visual policies often overfit to a single track and their
performance degrades when track geometry changes. We develop FalconGym 2.0, a
photorealistic simulation framework built on Gaussian Splatting (GSplat) with
an Edit API that programmatically generates diverse static and dynamic tracks
in milliseconds. Leveraging FalconGym 2.0's editability, we propose a
Performance-Guided Refinement (PGR) algorithm, which concentrates visual
policy's training on challenging tracks while iteratively improving its
performance. Across two case studies (fixed-wing UAVs and quadrotors) with
distinct dynamics and environments, we show that a single visual policy trained
with PGR in FalconGym 2.0 outperforms state-of-the-art baselines in
generalization and robustness: it generalizes to three unseen tracks with 100%
success without per-track retraining and maintains higher success rates under
gate-pose perturbations. Finally, we demonstrate that the visual policy trained
with PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a
quadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30
trials spanning two three-gate tracks and a moving-gate track.

</details>


### [206] [Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking](https://arxiv.org/abs/2510.02252)
*Joao Pedro Araujo,Yanjie Ze,Pei Xu,Jiajun Wu,C. Karen Liu*

Main category: cs.RO

TL;DR: 本文针对人形机器人运动跟踪的难题，提出了一种新的通用运动重定向方法GMR，并在多个数据集和方法下系统评估了重定向质量对策略性能的影响。实验显示，GMR在跟踪精度与还原性方面超越了其他开源方法，接近闭源高质量基线。


<details>
  <summary>Details</summary>
Motivation: 人形机器人模仿人类运动时需跨越实体差异（embodiment gap），现有方法多依赖于数据重定向与奖励工程，但重定向过程易引入伪影，影响后续强化学习策略的表现。因此亟需深入研究重定向质量对跟踪策略的实际影响，并开发效果更优的重定向方法。

Method: 作者提出了一种新的通用运动重定向方法（GMR），并与现有的两个开源方法（PHC和ProtoMotions）、以及一个高质量闭源数据（Unitree）进行了系统对比。通过在 BeyondMimic 框架下训练RL跟踪策略，排除奖励设计因素，专注分析不同重定向方案对策略性能和还原性的影响。

Result: 实验结果显示，在LAFAN1等数据集上的动态或长序列运动中，重定向产生的伪影显著削弱了策略鲁棒性。GMR在跟踪性能和动作还原性上均显著优于两种开源方法，并接近闭源高质量基线。

Conclusion: 运动重定向质量对人形机器人模仿策略的性能起决定作用。新的GMR方法提升了开放领域的重定向基线，为今后人形机器人模仿与远程操作提供了更健壮的技术路径。

Abstract: Humanoid motion tracking policies are central to building teleoperation
pipelines and hierarchical controllers, yet they face a fundamental challenge:
the embodiment gap between humans and humanoid robots. Current approaches
address this gap by retargeting human motion data to humanoid embodiments and
then training reinforcement learning (RL) policies to imitate these reference
trajectories. However, artifacts introduced during retargeting, such as foot
sliding, self-penetration, and physically infeasible motion are often left in
the reference trajectories for the RL policy to correct. While prior work has
demonstrated motion tracking abilities, they often require extensive reward
engineering and domain randomization to succeed. In this paper, we
systematically evaluate how retargeting quality affects policy performance when
excessive reward tuning is suppressed. To address issues that we identify with
existing retargeting methods, we propose a new retargeting method, General
Motion Retargeting (GMR). We evaluate GMR alongside two open-source
retargeters, PHC and ProtoMotions, as well as with a high-quality closed-source
dataset from Unitree. Using BeyondMimic for policy training, we isolate
retargeting effects without reward tuning. Our experiments on a diverse subset
of the LAFAN1 dataset reveal that while most motions can be tracked, artifacts
in retargeted data significantly reduce policy robustness, particularly for
dynamic or long sequences. GMR consistently outperforms existing open-source
methods in both tracking performance and faithfulness to the source motion,
achieving perceptual fidelity and policy success rates close to the
closed-source baseline. Website:
https://jaraujo98.github.io/retargeting_matters. Code:
https://github.com/YanjieZe/GMR.

</details>


### [207] [Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning](https://arxiv.org/abs/2510.02268)
*Tianchong Jiang,Jingtian Ji,Xiangshan Tan,Jiading Fang,Anand Bhattad,Vitor Guizilini,Matthew R. Walter*

Main category: cs.RO

TL;DR: 本文提出在模仿学习中通过显式地将策略与相机外参（camera extrinsics）关联来提升跨视角的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的模仿学习方法在不同相机视角场景下泛化能力弱，因为容易依赖背景等视觉线索推测相机位置，导致迁移失败。

Method: 作者提出利用每像素射线的Plucker嵌入，将相机外参作为策略输入，应用于传统模仿学习方法（如ACT、Diffusion Policy、SmolVLA），并在RoboSuite与ManiSkill仿真环境引入六个对照任务以验证效果。

Result: 实验显示，不使用外参的信息时，策略容易在固定场景中依赖背景推断相机位置，导致泛化性弱。引入相机外参后，即使场景几何和相机位置变化，策略也能保持可靠的控制性能，仅用RGB无需深度信息。

Conclusion: 在模仿学习中，将相机外参纳入策略输入对提升跨视角泛化和鲁棒性至关重要。文中任务、演示和代码均已开源。

Abstract: We study view-invariant imitation learning by explicitly conditioning
policies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we
show that conditioning on extrinsics significantly improves generalization
across viewpoints for standard behavior cloning policies, including ACT,
Diffusion Policy, and SmolVLA. To evaluate policy robustness under realistic
viewpoint shifts, we introduce six manipulation tasks in RoboSuite and
ManiSkill that pair "fixed" and "randomized" scene variants, decoupling
background cues from camera pose. Our analysis reveals that policies without
extrinsics often infer camera pose using visual cues from static backgrounds in
fixed scenes; this shortcut collapses when workspace geometry or camera
placement shifts. Conditioning on extrinsics restores performance and yields
robust RGB-only control without depth. We release the tasks, demonstrations,
and code at https://ripl.github.io/know_your_camera/ .

</details>


### [208] [ARMADA: Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Deployment and Adaptation](https://arxiv.org/abs/2510.02298)
*Wenye Yu,Jun Lv,Zixi Ying,Yang Jin,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: 本文提出了ARMADA系统——一种多机器人部署与适应系统，引入了名为FLOAT的自助失败检测方法，在只需要时才请求人工介入，实现高效的仿真—现实迁移和人机协作能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在执行策略部署时，往往需大量领域内数据，并严重依赖人工演示和实时跟踪，导致成本高、效率低。此外，人类采集的数据质量参差且含冗余。作者希望通过更智能的异常检测与干预调度，减轻人工负担并提升系统适应能力。

Method: 作者设计了ARMADA系统，引入FLOAT（自主在线失败检测）模块，实现多机器人并行策略执行。只在系统检测到失败可能性时才触发人工介入，从而高效采集关键领域内数据并提升系统适应性与扩展性。与传统完全依赖人工监控相比，极大减少了人工参与率。

Result: 在四项真实世界任务上，FLOAT平均检测准确率达95%，比当前最优方法高出20%。ARMADA在多轮策略部署及后续训练中，任务成功率提升超过4倍，人工干预率减少超过2倍。

Conclusion: ARMADA系统通过自主失败检测与按需人机协作，显著减少人工监督需求，实现高效的数据采集与模型自适应，促进多机器人系统大规模实际部署。

Abstract: Imitation learning has shown promise in learning from large-scale real-world
datasets. However, pretrained policies usually perform poorly without
sufficient in-domain data. Besides, human-collected demonstrations entail
substantial labour and tend to encompass mixed-quality data and redundant
information. As a workaround, human-in-the-loop systems gather domain-specific
data for policy post-training, and exploit closed-loop policy feedback to offer
informative guidance, but usually require full-time human surveillance during
policy rollout. In this work, we devise ARMADA, a multi-robot deployment and
adaptation system with human-in-the-loop shared control, featuring an
autonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA
enables paralleled policy rollout and requests human intervention only when
necessary, significantly reducing reliance on human supervision. Hence, ARMADA
enables efficient acquisition of in-domain data, and leads to more scalable
deployment and faster adaptation to new scenarios. We evaluate the performance
of ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on
average, surpassing prior state-of-the-art failure detection approaches by over
20%. Besides, ARMADA manifests more than 4$\times$ increase in success rate and
greater than 2$\times$ reduction in human intervention rate over multiple
rounds of policy rollout and post-training, compared to previous
human-in-the-loop learning methods.

</details>
