<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 70]
- [cs.CL](#cs.CL) [Total: 28]
- [cs.RO](#cs.RO) [Total: 11]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility](https://arxiv.org/abs/2512.19711)
*Md Nahid Hasan Shuvo,Moinul Hossain*

Main category: cs.CV

TL;DR: 该论文提出了PHANTOM框架，利用变形艺术生成物理可实现的视角相关对抗实例，有效欺骗CAV自动驾驶系统的主流目标检测器，并引发通信层网络级安全隐患。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆（CAVs）对视觉DNN和低时延V2X通信的依赖增强，其物理对抗攻击威胁日益突出，现有攻击多需模型信息或存在转移性不足，亟需更强且现实的攻击方式以揭示系统潜在安全漏洞。

Method: 提出PHANTOM框架，基于变形艺术设计视角相关的物理对抗样本，在黑盒情况下跨YOLOv5、SSD、Faster R-CNN和RetinaNet等主流检测器，利用CARLA平台在不同速度、天气、光照下系统性测试攻击有效性，并评估对V2X通信和全局交通网络的影响。

Result: 在最优条件下，PHANTOM攻击成功率超过90%，复杂环境下其效果仍有60-80%；攻击范围为6-10米，留给车辆反应时间极短。通过SUMO-OMNeT++仿真，攻击还会引发虚假紧急信息传播，导致信息峰值时延增加68-89%，危害安全通信。

Conclusion: CAV生态的感知和通信层均存在严重安全漏洞，PHANTOM框架通过现实且有效的物理攻击手段暴露这些威胁，提示需加强相关防御与检测机制。

Abstract: Connected autonomous vehicles (CAVs) rely on vision-based deep neural networks (DNNs) and low-latency (Vehicle-to-Everything) V2X communication to navigate safely and efficiently. Despite their advances, these systems remain vulnerable to physical adversarial attacks. In this paper, we introduce PHANTOM (PHysical ANamorphic Threats Obstructing connected vehicle Mobility), a novel framework for crafting and deploying perspective-dependent adversarial examples using \textit{anamorphic art}. PHANTOM exploits geometric distortions that appear natural to humans but are misclassified with high confidence by state-of-the-art object detectors. Unlike conventional attacks, PHANTOM operates in black-box settings without model access and demonstrates strong transferability across four diverse detector architectures (YOLOv5, SSD, Faster R-CNN, and RetinaNet). Comprehensive evaluation in CARLA across varying speeds, weather conditions, and lighting scenarios shows that PHANTOM achieves over 90\% attack success rate under optimal conditions and maintains 60-80\% effectiveness even in degraded environments. The attack activates within 6-10 meters of the target, providing insufficient time for safe maneuvering. Beyond individual vehicle deception, PHANTOM triggers network-wide disruption in CAV systems: SUMO-OMNeT++ co-simulation demonstrates that false emergency messages propagate through V2X links, increasing Peak Age of Information by 68-89\% and degrading safety-critical communication. These findings expose critical vulnerabilities in both perception and communication layers of CAV ecosystems.

</details>


### [2] [Generating the Past, Present and Future from a Motion-Blurred Image](https://arxiv.org/abs/2512.19817)
*SaiKiran Tedla,Kelly Zhu,Trevor Canham,Felix Taubner,Michael S. Brown,Kiriakos N. Kutulakos,David B. Lindell*

Main category: cs.CV

TL;DR: 本文提出了一种新技术，利用预训练的视频扩散模型，从运动模糊图像中恢复复杂场景动态，并预测其捕获前后的场景变化，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 运动模糊图像虽然降低了视觉质量，但包含了场景及相机运动的信息。现有方法难以揭示更复杂的场景动态，且主要局限于恢复单帧清晰图像或模糊瞬间的视频片段，无法预测图像捕获之前或之后的变化。为此，作者希望开发一种更通用且强大的方法，最大限度挖掘运动模糊图像中的时空信息。

Method: 作者提出利用在大规模互联网数据集上预训练的视频扩散模型，实现从单张运动模糊图像生成展现捕获时刻动态的视频，并且能推断图片拍摄前后的可能场景，突破以往只能恢复单帧清晰图像或静态视频的限制。

Result: 实验表明，该方法不仅在恢复场景动态方面优于以往方法，还具备很强的泛化能力，能够处理野外拍摄等复杂情况。同时，其成果还能应用于相机轨迹恢复、目标运动估计、三维动态场景重建等下游任务。

Conclusion: 提出的方法显著提升了通过运动模糊图像理解和重建场景动态的能力，扩展了运动模糊图像的时空信息利用范围，为多种计算机视觉任务带来潜在价值。

Abstract: We seek to answer the question: what can a motion-blurred image reveal about a scene's past, present, and future? Although motion blur obscures image details and degrades visual quality, it also encodes information about scene and camera motion during an exposure. Previous techniques leverage this information to estimate a sharp image from an input blurry one, or to predict a sequence of video frames showing what might have occurred at the moment of image capture. However, they rely on handcrafted priors or network architectures to resolve ambiguities in this inverse problem, and do not incorporate image and video priors on large-scale datasets. As such, existing methods struggle to reproduce complex scene dynamics and do not attempt to recover what occurred before or after an image was taken. Here, we introduce a new technique that repurposes a pre-trained video diffusion model trained on internet-scale datasets to recover videos revealing complex scene dynamics during the moment of capture and what might have occurred immediately into the past or future. Our approach is robust and versatile; it outperforms previous methods for this task, generalizes to challenging in-the-wild images, and supports downstream tasks such as recovering camera trajectories, object motion, and dynamic 3D scene structure. Code and data are available at https://blur2vid.github.io

</details>


### [3] [Learning to Refocus with Video Diffusion Models](https://arxiv.org/abs/2512.19823)
*SaiKiran Tedla,Zhoutong Zhang,Xuaner Zhang,Shumian Xin*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，利用视频扩散模型实现单张模糊图像的真实感后期重聚焦，并发布了一个大规模焦距栈数据集，方法在感知质量和鲁棒性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动对焦系统常常无法准确捕捉目标，用户也时常希望在拍摄后调整焦点。因此需要一种便捷、现实的图像后期重聚焦方法。

Method: 通过视频扩散模型，从单张模糊图像生成感知准确的焦距栈（用作视频序列），实现交互式后期重聚焦。同时，发布了在多样化真实智能手机条件下采集的大规模焦距栈数据集。

Result: 提出的方法在各种具有挑战性的环境下，在感知质量和鲁棒性上均优于现有方法。

Conclusion: 该方法为日常摄影实现更高级的对焦编辑能力奠定基础，有望推动该领域后续研究和应用。

Abstract: Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io

</details>


### [4] [RANSAC Scoring Functions: Analysis and Reality Check](https://arxiv.org/abs/2512.19850)
*A. Shekhovtsov*

Main category: cs.CV

TL;DR: 本文系统性地分析了RANSAC等鲁棒几何拟合中常用的打分函数，指出现有方法如MAGSAC++的理论依据并不充分，并在理论与实验上比较不同打分函数的表现，发现它们在实际中的效果非常接近。


<details>
  <summary>Details</summary>
Motivation: 鲁棒几何拟合广泛应用于计算机视觉等领域，其中，打分函数对模型评价的好坏直接影响鲁棒性和最终拟合质量。目前一些先进方法如MAGSAC++取得了最好成绩，但其理论合理性尚未被充分理解和验证，因而需要系统性地进行重新审视。

Method: 作者首先从概率角度将几何误差的理论基础从高斯噪声推广到球面噪声，并在鲁棒场景中，结合均匀分布的离群点提出统一的阈值参数化视角，将似然法和M-估计方法纳入统一框架。针对MAGSAC++，分析了其建模假设和推导过程，指出其实质是基本的高斯-均匀似然函数。最后，提出了新的实验评估方法，系统比较各类打分函数的实际表现。

Result: 通过理论分析和系统实验，发现各种打分函数（包括MAGSAC++以及基于学习的内点分布方法）实际表现相差无几，MAGSAC++并未体现更优或对阈值超参数更不敏感。

Conclusion: 对鲁棒几何拟合框架下的打分函数进行了全面的理论和实验复查，澄清了现有方法的理论基础和实际效果，对今后算法改进和相关应用研究具有重要参考价值。

Abstract: We revisit the problem of assigning a score (a quality of fit) to candidate geometric models -- one of the key components of RANSAC for robust geometric fitting. In a non-robust setting, the ``gold standard'' scoring function, known as the geometric error, follows from a probabilistic model with Gaussian noises. We extend it to spherical noises. In a robust setting, we consider a mixture with uniformly distributed outliers and show that a threshold-based parameterization leads to a unified view of likelihood-based and robust M-estimators and associated local optimization schemes.
  Next we analyze MAGSAC++ which stands out for two reasons. First, it achieves the best results according to existing benchmarks. Second, it makes quite different modeling assumptions and derivation steps. We discovered, however that the derivation does not correspond to sound principles and the resulting score function is in fact numerically equivalent to a simple Gaussian-uniform likelihood, a basic model within the proposed framework.
  Finally, we propose an experimental methodology for evaluating scoring functions: assuming either a large validation set, or a small random validation set in expectation. We find that all scoring functions, including using a learned inlier distribution, perform identically. In particular, MAGSAC++ score is found to be neither better performing than simple contenders nor less sensitive to the choice of the threshold hyperparameter.
  Our theoretical and experimental analysis thus comprehensively revisit the state-of-the-art, which is critical for any future research seeking to improve the methods or apply them to other robust fitting problems.

</details>


### [5] [HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction](https://arxiv.org/abs/2512.19871)
*Jong Wook Kim,Wonseok Roh,Ha Dam Baek,Pilhyeon Lee,Jonghyun Choi,Sangpil Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种名为HyGE-Occ的新框架，用于提升3D全景占用预测的几何一致性与边界感知能力，并在Occ3D-nuScenes数据集上超越了已有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D全景占用预测方法难以保持精确的几何信息与3D实例的空间范围，影响全景分离的鲁棒性。作者希望通过改进方法克服这些局限。

Method: 提出HyGE-Occ框架，结合了基于3D高斯与深度区间的混合视图变换分支以提升几何一致性，且利用BEV特征提取边缘图作为辅助信息学习边界特征。

Result: 在Occ3D-nuScenes数据集上，HyGE-Occ在3D几何推理和场景重建方面优于现有方法，表现出更好的性能。

Conclusion: HyGE-Occ有效提升了3D全景占用预测的细粒度几何理解能力与边界感知能力，为这一领域带来新的进步。

Abstract: 3D Panoptic Occupancy Prediction aims to reconstruct a dense volumetric scene map by predicting the semantic class and instance identity of every occupied region in 3D space. Achieving such fine-grained 3D understanding requires precise geometric reasoning and spatially consistent scene representation across complex environments. However, existing approaches often struggle to maintain precise geometry and capture the precise spatial range of 3D instances critical for robust panoptic separation. To overcome these limitations, we introduce HyGE-Occ, a novel framework that leverages a hybrid view-transformation branch with 3D Gaussian and edge priors to enhance both geometric consistency and boundary awareness in 3D panoptic occupancy prediction. HyGE-Occ employs a hybrid view-transformation branch that fuses a continuous Gaussian-based depth representation with a discretized depth-bin formulation, producing BEV features with improved geometric consistency and structural coherence. In parallel, we extract edge maps from BEV features and use them as auxiliary information to learn edge cues. In our extensive experiments on the Occ3D-nuScenes dataset, HyGE-Occ outperforms existing work, demonstrating superior 3D geometric reasoning.

</details>


### [6] [Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs](https://arxiv.org/abs/2512.19918)
*Houston H. Zhang,Tao Zhang,Baoze Lin,Yuanqi Xue,Yincheng Zhu,Huan Liu,Li Gu,Linfeng Ye,Ziqiang Wang,Xinxin Zuo,Yang Wang,Yuanhao Yu,Zhixiang Chi*

Main category: cs.CV

TL;DR: 本文提出了一种用于将小部件（widgets）图像转换为可执行代码（Widget2Code）的新基线方法，建立了专门的评测基准，并开发了支持多前端实现的通用基础设施，有效提升了视觉还原度。


<details>
  <summary>Details</summary>
Motivation: 现有的UI2Code研究主要关注网页和移动端界面，对于界面紧凑且上下文独立的小部件研究不足。而这些小部件常见于实际应用中，其界面设计和代码数据难以获取，缺乏有效的生成方法和评测体系。

Method: 1. 正式定义小部件至代码（Widget2Code）任务，并构建全新基于图像的小部件评测基准，包含多维细粒度评价指标。
2. 评测多模态大模型（MLLMs）和UI2Code方法表现，并分析其不足。
3. 提出WidgetFactory方案：以部件组装模拟设计原则，采用图标检索和可视化模块提升感知理解；设计WidgetDSL领域特定语言及编译器，支持React、HTML/CSS等多端代码生成；自适应渲染模块优化空间布局以满足紧凑性需求。

Result: 大模型虽超越传统方法但生成不稳、视觉还原度不足。WidgetFactory能够更准确还原输入小部件图像，显著提升了输出代码的可用性和视觉一致性，并建立了强基线。

Conclusion: Widget2Code作为独立方向值得关注。本文方法为Widget2Code任务提供了统一高效的基础设施与评测工具，为该领域后续研究奠定了重要基础。

Abstract: User interface to code (UI2Code) aims to generate executable code that can faithfully reconstruct a given input UI. Prior work focuses largely on web pages and mobile screens, leaving app widgets underexplored. Unlike web or mobile UIs with rich hierarchical context, widgets are compact, context-free micro-interfaces that summarize key information through dense layouts and iconography under strict spatial constraints. Moreover, while (image, code) pairs are widely available for web or mobile UIs, widget designs are proprietary and lack accessible markup. We formalize this setting as the Widget-to-Code (Widget2Code) and introduce an image-only widget benchmark with fine-grained, multi-dimensional evaluation metrics. Benchmarking shows that although generalized multimodal large language models (MLLMs) outperform specialized UI2Code methods, they still produce unreliable and visually inconsistent code. To address these limitations, we develop a baseline that jointly advances perceptual understanding and structured code generation. At the perceptual level, we follow widget design principles to assemble atomic components into complete layouts, equipped with icon retrieval and reusable visualization modules. At the system level, we design an end-to-end infrastructure, WidgetFactory, which includes a framework-agnostic widget-tailored domain-specific language (WidgetDSL) and a compiler that translates it into multiple front-end implementations (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to satisfy compactness constraints. Together, these contributions substantially enhance visual fidelity, establishing a strong baseline and unified infrastructure for future Widget2Code research.

</details>


### [7] [Unified Brain Surface and Volume Registration](https://arxiv.org/abs/2512.19928)
*S. Mazdak Abulnaga,Andrew Hoopes,Malte Hoffmann,Robin Magnet,Maks Ovsjanikov,Lilla Zöllei,John Guttag,Bruce Fischl,Adrian Dalca*

Main category: cs.CV

TL;DR: 该论文提出了NeurAlign，一个基于深度学习的框架，实现了大脑MRI图像皮层和皮层下区同时一致配准，在准确率和速度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的大脑MRI配准方法通常分别处理体素（体积）和表面配准，导致内外结构不一致，影响后续神经科学分析。

Method: 提出了一种深度学习框架NeurAlign，采用统一的体积—表面表达，同时参考球面坐标空间，将解剖学表面拓扑与体积对应起来。通过在学习过程中融入球面配准，实现了体素与表面的几何一致。

Result: 在多组数据集上，NeurAlign比经典和现有机器学习配准方法的性能更好，Dice系数最高可提升7分，且变形场更为规则，运算速度快许多，并且仅需MRI作为输入，操作更简便。

Conclusion: NeurAlign提供了兼具高准确率、快速推断和易用性的联合配准新标准，能够更好地服务大脑MRI的皮层及皮层下结构分析。

Abstract: Accurate registration of brain MRI scans is fundamental for cross-subject analysis in neuroscientific studies. This involves aligning both the cortical surface of the brain and the interior volume. Traditional methods treat volumetric and surface-based registration separately, which often leads to inconsistencies that limit downstream analyses. We propose a deep learning framework, NeurAlign, that registers $3$D brain MRI images by jointly aligning both cortical and subcortical regions through a unified volume-and-surface-based representation. Our approach leverages an intermediate spherical coordinate space to bridge anatomical surface topology with volumetric anatomy, enabling consistent and anatomically accurate alignment. By integrating spherical registration into the learning, our method ensures geometric coherence between volume and surface domains. In a series of experiments on both in-domain and out-of-domain datasets, our method consistently outperforms both classical and machine learning-based registration methods -- improving the Dice score by up to 7 points while maintaining regular deformation fields. Additionally, it is orders of magnitude faster than the standard method for this task, and is simpler to use because it requires no additional inputs beyond an MRI scan. With its superior accuracy, fast inference, and ease of use, NeurAlign sets a new standard for joint cortical and subcortical registration.

</details>


### [8] [Vehicle-centric Perception via Multimodal Structured Pre-training](https://arxiv.org/abs/2512.19934)
*Wentao Wu,Xiao Wang,Chenglong Li,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: 本文提出了一种新的车辆感知预训练大模型VehicleMAE-V2，通过引入结构化先验信息提升模型通用车辆感知能力，并在五项下游任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有车辆感知方法在预训练阶段缺乏有效车辆相关知识的学习，导致对车辆通用表征能力不足，影响各类车辆智能系统的效果。

Method: 提出VehicleMAE-V2，通过结合车辆对称性、轮廓和语义三类结构化多模态先验信息，设计了对称性引导掩码模块（SMM）、轮廓引导表征模块（CRM）和语义引导表征模块（SRM），优化掩码重建过程。同时，构建了包含约400万车辆图片和12693条文本描述的大规模数据集Autobot4M。

Result: 在五项车辆感知相关下游任务上，VehicleMAE-V2的表现超过现有方法，验证了所提出结构化先验和模型设计的有效性。

Conclusion: 结合车辆结构化多模态先验进行掩码重建可显著提升车辆感知通用表征能力，VehicleMAE-V2为智能交通和自动驾驶等领域提供了更强的基础视觉模型。

Abstract: Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.

</details>


### [9] [Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)](https://arxiv.org/abs/2512.20148)
*Robert van de Ven,Trim Bresilla,Bram Nelissen,Ard Nieuwenhuizen,Eldert J. van Henten,Gert Kootstra*

Main category: cs.CV

TL;DR: 该论文提出了一种结合3D Gaussian Splatting和简化标注流程的新型果园场景苹果姿态估计算法，大大减少了人工标注的工作量，并在不同遮挡情况下进行了性能评估。


<details>
  <summary>Details</summary>
Motivation: 在果园环境中，由于遮挡与环境变化，自动化任务如苹果姿态估计十分困难，尤其是关键点因遮挡而难以准确标注，标注过程耗时且容易出错。现有方法虽降低了对关键点的依赖，但依然需要复杂的标注。因此，作者希望通过3D重建与自动投影标注的方式，大幅提升数据集生成效率、简化标注流程、减少人工工作量。

Method: 论文提出了一整套流水线：首先利用3D Gaussian Splatting技术对果园场景进行3D重建；然后对重建场景中苹果做简化标注；接着自动投影这些标注至多幅图像上，极大扩增训练标签；最后利用这些标签训练并评估姿态估计算法。通过这套管线，仅需105次人工标注即可生成28191个训练标签。

Result: 实验证明，仅用很少的标注实现了大规模扩增，标注工作量减少了99.6%。对于遮挡≤95%的果实，姿态估计获得了最佳表现，其F1分数在原图像上为0.927，在重建图像上为0.970。训练集规模的调整对最终模型效果影响不大。遮挡越少的果实，位置估计越准确，但姿态方法对苹果朝向的学习效果有限。

Conclusion: 该工作显著提升了果园苹果姿态估计的标注效率，同时保持了较高的性能，对大规模果园自动化提供了实际可行的新方法，但在极端遮挡与苹果朝向估计方面仍有改进空间。

Abstract: Automating tasks in orchards is challenging because of the large amount of variation in the environment and occlusions. One of the challenges is apple pose estimation, where key points, such as the calyx, are often occluded. Recently developed pose estimation methods no longer rely on these key points, but still require them for annotations, making annotating challenging and time-consuming. Due to the abovementioned occlusions, there can be conflicting and missing annotations of the same fruit between different images. Novel 3D reconstruction methods can be used to simplify annotating and enlarge datasets. We propose a novel pipeline consisting of 3D Gaussian Splatting to reconstruct an orchard scene, simplified annotations, automated projection of the annotations to images, and the training and evaluation of a pose estimation method. Using our pipeline, 105 manual annotations were required to obtain 28,191 training labels, a reduction of 99.6%. Experimental results indicated that training with labels of fruits that are $\leq95\%$ occluded resulted in the best performance, with a neutral F1 score of 0.927 on the original images and 0.970 on the rendered images. Adjusting the size of the training dataset had small effects on the model performance in terms of F1 score and pose estimation accuracy. It was found that the least occluded fruits had the best position estimation, which worsened as the fruits became more occluded. It was also found that the tested pose estimation method was unable to correctly learn the orientation estimation of apples.

</details>


### [10] [Block-Recurrent Dynamics in Vision Transformers](https://arxiv.org/abs/2512.19941)
*Mozes Jacobs,Thomas Fel,Richard Hakim,Alessandra Brondetta,Demba Ba,T. Andy Keller*

Main category: cs.CV

TL;DR: 本文提出并验证了视觉Transformer（ViT）在深度维度上具有可复用的区块递归结构，即原始网络可用远少于原始深度的区块重复实现相近计算，极大简化了模型复杂度。


<details>
  <summary>Details</summary>
Motivation: ViT在视觉任务中广泛应用，但其深度结构的真实计算机制尚不明确。研究Transformer层深度是否存在统一、可解释的动力学流程，对理解模型工作原理与优化结构有重要意义。

Method: 提出Block-Recurrent Hypothesis（BRH），假设ViT的深层结构可以分解为少数可复用区块。采用表示相似度分析，提出并训练Raptor（可复现block递归结构的ViT代理模型）对比不同ViT，在不增加计算量下用更少的区块重现原始模型性能。

Result: 实验证明，主流ViT之间层的表示分为少数几个阶段，可被重复利用的区块良好地逼近。Raptor模型仅用2个区块即可恢复DINOv2 96%的准确率。分析了不同token动态及模型后期表示结构特征，揭示了动力学低秩收敛现象。

Conclusion: ViT在深度上自发形成紧凑而低复杂度的区块递归结构，未来可用动力学与可解释性工具系统研究Transformer类网络。

Abstract: As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent structure and subsequently correlate with our ability to accurately fit Raptor. We then provide an empirical existence proof for BRH by training a Raptor model to recover $96\%$ of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks at equivalent computational cost. Finally, we leverage our hypothesis to develop a program of Dynamical Interpretability. We find i) directional convergence into class-dependent angular basins with self-correcting trajectories under small perturbations, ii) token-specific dynamics, where cls executes sharp late reorientations while patch tokens exhibit strong late-stage coherence toward their mean direction, and iii) a collapse to low rank updates in late depth, consistent with convergence to low-dimensional attractors. Altogether, we find a compact recurrent program emerges along ViT depth, pointing to a low-complexity normative solution that enables these models to be studied through principled dynamical systems analysis.

</details>


### [11] [LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving](https://arxiv.org/abs/2512.20563)
*Long Nguyen,Micha Fauth,Bernhard Jaeger,Daniel Dauner,Maximilian Igl,Andreas Geiger,Kashyap Chitta*

Main category: cs.CV

TL;DR: 本文分析了模拟环境中模仿学习表现不佳的原因，聚焦于专家和学生之间的信息不对齐，通过方法与调整极大提升了自动驾驶算法在CARLA等基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然仿真器可以生成大量驾驶数据，但基于模仿学习的策略在闭环性能上依旧不够鲁棒。作者发现，原因之一在于专家演示（具有更高“特权”如可见性与低不确定性）与基于传感器感知的学生之间存在信息不对齐，这限制了模仿学习的有效性。

Method: 首先，作者系统性分析了专家和学生之间的可见性、不确定性和导航意图表达方面的差异，然后提出针对性干预措施以缩小这些差距。此外，在模型设计中整合了感知监督，并构建了端到端的sim-to-real pipeline。

Result: 经上述调整后，提出的TransFuser v6策略在CARLA多个主流闭环测试基准（如Bench2Drive、Longest6~v2和Town13）上取得新SOTA表现。同时，在NAVSIM和Waymo Vision-Based End-to-End等基准上也得到持续提升。

Conclusion: 专家和学生观测信息的不一致显著影响模仿学习，在缩小二者的能力差距后，可以显著提升自动驾驶任务的实际性能。所提出的方案具备较强实用价值，并且相关代码与数据集已经开源。

Abstract: Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' actions), making them difficult to imitate reliably. Furthermore, navigational intent (i.e., the route to follow) is under-specified in student models at test time via only a single target point. We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them. After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13. Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks. Our code, data, and models are publicly available at https://github.com/autonomousvision/lead.

</details>


### [12] [SE360: Semantic Edit in 360$^\circ$ Panoramas via Hierarchical Data Construction](https://arxiv.org/abs/2512.19943)
*Haoyi Zhong,Fang-Lue Zhang,Andrew Chalmers,Taehyun Rhee*

Main category: cs.CV

TL;DR: 本文提出了SE360框架，实现了360°全景图像中基于多条件的对象编辑，并在视觉质量与语义准确性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本指令的图像编辑法难以适用于360°全景图，常在全景与投影视图中产生不真实结果，因此有必要设计更适用于全景图的编辑方法。

Method: 提出了一种全新的数据生成流程，从粗到细、自主生成、无需人工干预，结合视觉-语言模型和自适应投影调整，保障了对象及其物理环境的整体分割；还提出双阶段数据优化策略，提升数据真实感并减少编辑伪影，同时基于这些数据集训练了Transformer-扩散模型，可在多条件（文本、掩码、参考图像）下进行360°全景对象编辑。

Result: 实验结果显示，在视觉效果和语义准确性上，所提出方法均优于现有的同类全景编辑方法。

Conclusion: SE360有效提升了360°全景图对象编辑的生成质量和语义一致性，并验证了数据自动生成管线和两阶段优化策略的有效性。

Abstract: While instruction-based image editing is emerging, extending it to 360$^\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.

</details>


### [13] [How Much 3D Do Video Foundation Models Encode?](https://arxiv.org/abs/2512.19949)
*Zixuan Huang,Xiang Li,Zhaoyang Lv,James M. Rehg*

Main category: cs.CV

TL;DR: 本论文提出一种模型无关的框架，评估各类视频大模型对三维世界的理解能力，发现其具备显著的3D感知能力，甚至超过专为3D任务训练的大模型。


<details>
  <summary>Details</summary>
Motivation: 当前视频大模型（VidFMs）在海量视频数据上训练，但其是否具备对三维世界的自然理解能力尚不明确。该问题对于后续三维场景建模和感知有重要意义。

Method: 作者提出了一种首个不依赖于具体模型的评估框架，通过对视频大模型的特征用浅层读取器预测多种3D属性，从而量化其3D理解能力。

Result: 实验证实，先进的视频生成模型即便未在3D数据上训练，仍展现出对三维物体和场景的强理解能力；甚至在部分任务上超过了一些专门为3D训练的专家模型。

Conclusion: 当前主流视频大模型具备良好的3D通用感知能力，这为后续构建可扩展的3D理解和生成模型提供了方法论和实证基础。

Abstract: Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.

</details>


### [14] [HistoWAS: A Pathomics Framework for Large-Scale Feature-Wide Association Studies of Tissue Topology and Patient Outcomes](https://arxiv.org/abs/2512.19954)
*Yuechen Yang,Junlin Guo,Yanfan Zhu,Jialin Yue,Junchao Zhu,Yu Wang,Shilin Zhao,Haichun Yang,Xingyi Guo,Jovan Tanevski,Laura Barisoni,Avi Z. Rosenberg,Yuankai Huo*

Main category: cs.CV

TL;DR: 本文提出了HistoWAS框架，通过引入空间拓扑特征和大规模单变量回归分析，实现了组织切片空间结构与临床结局的关联分析。


<details>
  <summary>Details</summary>
Motivation: 尽管全视野切片图像的高通量分析有助于组织特征和生物标志物的研究，但现有工具难以有效测量微观与宏观组织结构特征的空间交互及其与临床指标的关联性，限制了其临床价值。

Method: 提出HistoWAS框架，包括：1）构建新的特征空间，将传统特征和从GIS借鉴的30种空间拓扑特征结合用于定量组织微结构特征；2）开发关联分析引擎，仿照PheWAS方法，对每个特征进行大规模单变量回归并实施统计校正。并在KPMP项目中分析385张切片的102项特征，实际验证该方法。

Result: HistoWAS成功整合了传统与空间特征，对大规模肾脏切片样本进行分析，验证了该方法能够有效揭示组织空间结构特征与临床结局的关联关系。

Conclusion: HistoWAS为病理全图像分析带来了新的空间组织量化与临床关联研究方法，为未来组织生物标志物筛查和临床研究提供了重要工具，相关代码和数据已开放共享。

Abstract: High-throughput "pathomic" analysis of Whole Slide Images (WSIs) offers new opportunities to study tissue characteristics and for biomarker discovery. However, the clinical relevance of the tissue characteristics at the micro- and macro-environment level is limited by the lack of tools that facilitate the measurement of the spatial interaction of individual structure characteristics and their association with clinical parameters. To address these challenges, we introduce HistoWAS (Histology-Wide Association Study), a computational framework designed to link tissue spatial organization to clinical outcomes. Specifically, HistoWAS implements (1) a feature space that augments conventional metrics with 30 topological and spatial features, adapted from Geographic Information Systems (GIS) point pattern analysis, to quantify tissue micro-architecture; and (2) an association study engine, inspired by Phenome-Wide Association Studies (PheWAS), that performs mass univariate regression for each feature with statistical correction. As a proof of concept, we applied HistoWAS to analyze a total of 102 features (72 conventional object-level features and our 30 spatial features) using 385 PAS-stained WSIs from 206 participants in the Kidney Precision Medicine Project (KPMP). The code and data have been released to https://github.com/hrlblab/histoWAS.

</details>


### [15] [WSD-MIL: Window Scale Decay Multiple Instance Learning for Whole Slide Image Classification](https://arxiv.org/abs/2512.19982)
*Le Feng,Li Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多实例学习（MIL）方法——WSD-MIL，实现更高效、更精确的肿瘤区域分析，并在主流病理数据集上达到最新性能，同时显著降低算力消耗。


<details>
  <summary>Details</summary>
Motivation: 现有MIL方法忽视了WSI中实例间复杂的语义关系，且Transformer-based MIL无法高效处理大规模图像，其固有的固定尺度注意机制难以准确捕获不同病灶区域的关系。

Method: WSD-MIL包含两个核心模块：1）基于窗口尺度衰减的注意力机制，通过聚类采样减少计算量，并以递进式衰减的注意窗口，捕获不同尺度下的局部实例关系；2）基于压缩激励（SE）的区域门控模块，动态调整窗口权重，强化全局信息建模。

Result: WSD-MIL在CAMELYON16和TCGA-BRCA等病理数据集上取得了领先的诊断效果，并在同等条件下减少了62%的计算内存占用。

Conclusion: WSD-MIL有效提升了多尺度肿瘤区域建模能力与计算效率，为计算病理学领域带来了更实用的高性能MIL解决方案。

Abstract: In recent years, the integration of pre-trained foundational models with multiple instance learning (MIL) has improved diagnostic accuracy in computational pathology. However, existing MIL methods focus on optimizing feature extractors and aggregation strategies while overlooking the complex semantic relationships among instances within whole slide image (WSI). Although Transformer-based MIL approaches aiming to model instance dependencies, the quadratic computational complexity limits their scalability to large-scale WSIs. Moreover, due to the pronounced variations in tumor region scales across different WSIs, existing Transformer-based methods employing fixed-scale attention mechanisms face significant challenges in precisely capturing local instance correlations and fail to account for the distance-based decay effect of patch relevance. To address these challenges, we propose window scale decay MIL (WSD-MIL), designed to enhance the capacity to model tumor regions of varying scales while improving computational efficiency. WSD-MIL comprises: 1) a window scale decay based attention module, which employs a cluster-based sampling strategy to reduce computational costs while progressively decaying attention window-scale to capture local instance relationships at varying scales; and 2) a squeeze-and-excitation based region gate module, which dynamically adjusts window weights to enhance global information modeling. Experimental results demonstrate that WSD-MIL achieves state-of-the-art performance on the CAMELYON16 and TCGA-BRCA datasets while reducing 62% of the computational memory. The code will be publicly available.

</details>


### [16] [A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection](https://arxiv.org/abs/2512.19989)
*Tamim Ahasan Rijon,Yeasin Arafath*

Main category: cs.CV

TL;DR: 本研究利用卷积神经网络（CNN）和主流机器学习方法识别孟加拉国常见的番石榴病害，实现了约99.99%的准确率。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国作为重要的农业国家，番石榴产量与品质对经济有重要贡献，但其易受到炭疽病和果蝇感染，影响产量和质量。及时准确地检测病害是提高产量和保障收成的关键。

Method: 收集自孟加拉Rajshahi和Pabna果园的番石榴果实图片，建立包含健康、果蝇和炭疽病三类的公开数据集GFDD24。采用CNN与梯度提升机（GBM）等机器学习方法单独及组合（集成模型）进行分类，并评估其性能。

Result: 所提出的基于CNN与GBM集成的模型在番石榴果实病害分类任务中达到了约99.99%的最高准确率，模型表现优异。

Conclusion: CNN-ML级联框架能够实现高效、准确的番石榴病害识别，适合用于农业实时监控系统，有助于减少因病害带来的经济损失。

Abstract: As a significant agricultural country, Bangladesh utilizes its fertile land for guava cultivation and dedicated labor to boost its economic development. In a nation like Bangladesh, enhancing guava production and agricultural practices plays a crucial role in its economy. Anthracnose and fruit fly infection can lower the quality and productivity of guava, a crucial tropical fruit. Expert systems that detect diseases early can reduce losses and safeguard the harvest. Images of guava fruits classified into the Healthy, Fruit Flies, and Anthracnose classes are included in the Guava Fruit Disease Dataset 2024 (GFDD24), which comes from plantations in Rajshahi and Pabna, Bangladesh. This study aims to create models using CNN alongside traditional machine learning techniques that can effectively identify guava diseases in locally cultivated varieties in Bangladesh. In order to achieve the highest classification accuracy of approximately 99.99% for the guava dataset, we propose utilizing ensemble models that combine CNNML with Gradient Boosting Machine. In general, the CNN-ML cascade framework exhibits strong, high-accuracy guava disease detection that is appropriate for real-time agricultural monitoring systems.

</details>


### [17] [A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping](https://arxiv.org/abs/2512.19990)
*Peng Gao,Ke Li,Di Wang,Yongshan Zhu,Yiming Zhang,Xuemei Luo,Yifeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的弱监督高分辨率土地覆盖制图方法DDTM，通过双分支架构提升了低分辨率监督下的高分辨率语义预测精度，并在公开数据集上大幅超过以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督方法难以解决高低分辨率标签间的空间结构对齐问题，导致监督信号噪声大、制图精度差。针对这种分辨率不匹配带来的挑战，本文寻求显式解耦局部语义与全局上下文推理以提升精度。

Method: 提出双分支架构DDTM：扩散分支在粗监督下逐步细化局部高分辨率语义，Transformer分支保证大范围空间一致性。此外，设计了伪标签置信度评估模块，有效缓解交叉分辨率噪声，增强可靠监督信号的利用。

Result: 在Chesapeake Bay基准数据集上，DDTM取得了66.52%的mIoU，显著优于现有弱监督方法，树立了新的性能标杆。

Conclusion: DDTM通过结构性的创新，有效提升了弱监督下的高分辨率土地覆盖制图能力，为相关场景提供了新技术路径及更高实用价值。

Abstract: Cross-resolution land cover mapping aims to produce high-resolution semantic predictions from coarse or low-resolution supervision, yet the severe resolution mismatch makes effective learning highly challenging. Existing weakly supervised approaches often struggle to align fine-grained spatial structures with coarse labels, leading to noisy supervision and degraded mapping accuracy. To tackle this problem, we propose DDTM, a dual-branch weakly supervised framework that explicitly decouples local semantic refinement from global contextual reasoning. Specifically, DDTM introduces a diffusion-based branch to progressively refine fine-scale local semantics under coarse supervision, while a transformer-based branch enforces long-range contextual consistency across large spatial extents. In addition, we design a pseudo-label confidence evaluation module to mitigate noise induced by cross-resolution inconsistencies and to selectively exploit reliable supervisory signals. Extensive experiments demonstrate that DDTM establishes a new state-of-the-art on the Chesapeake Bay benchmark, achieving 66.52\% mIoU and substantially outperforming prior weakly supervised methods. The code is available at https://github.com/gpgpgp123/DDTM.

</details>


### [18] [Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models](https://arxiv.org/abs/2512.20000)
*Zhenhao Li,Shaohan Yi,Zheng Liu,Leonartinus Gao,Minh Ngoc Le,Ambrose Ling,Zhuoran Wang,Md Amirul Islam,Zhixiang Chi,Yuanhao Yu*

Main category: cs.CV

TL;DR: 本文提出了一种名为MIVA的轻量级模块，用于增强扩散模型在图像动画生成任务中的运动控制和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在图像和视频生成领域表现优异，但应用于图像动画时面临两个主要挑战：1）高维视频信号导致训练样本稀缺，模型容易记忆而难以响应新指令；2）缺乏泛化能力，尤其难以扩展到训练集中未见过的运动模式。针对这些问题，需要既能高效学习新运动、又不增加太多计算开销的方法。

Method: 作者提出了MIVA（Modular Image-to-Video Adapter），这是一种可附加在预训练扩散模型上的小型子网络，每个MIVA负责建模一种运动模式。MIVA能够并行扩展，且训练效率高（仅需大约10个样本、单张消费级GPU即可完成训练）。推理时，用户只需选择一个或多个MIVA组合指定运动，无需复杂的提示词工程。

Result: 实验结果表明，使用MIVA后，模型在精确运动控制方面表现更优，且生成质量可与使用大规模数据集训练的模型媲美甚至超越。

Conclusion: MIVA为扩散模型动画生成带来高效、灵活、易用的新方案，在小样本条件下也能实现高质量、可控的动画生成，极大拓展了扩散模型在该领域的应用潜力。

Abstract: Diffusion models (DMs) have recently achieved impressive photorealism in image and video generation. However, their application to image animation remains limited, even when trained on large-scale datasets. Two primary challenges contribute to this: the high dimensionality of video signals leads to a scarcity of training data, causing DMs to favor memorization over prompt compliance when generating motion; moreover, DMs struggle to generalize to novel motion patterns not present in the training set, and fine-tuning them to learn such patterns, especially using limited training data, is still under-explored. To address these limitations, we propose Modular Image-to-Video Adapter (MIVA), a lightweight sub-network attachable to a pre-trained DM, each designed to capture a single motion pattern and scalable via parallelization. MIVAs can be efficiently trained on approximately ten samples using a single consumer-grade GPU. At inference time, users can specify motion by selecting one or multiple MIVAs, eliminating the need for prompt engineering. Extensive experiments demonstrate that MIVA enables more precise motion control while maintaining, or even surpassing, the generation quality of models trained on significantly larger datasets.

</details>


### [19] [PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification](https://arxiv.org/abs/2512.20011)
*Blessing Agyei Kyem,Joshua Kofi Asamoah,Anthony Dontoh,Andrews Danyo,Eugene Denteh,Armstrong Aboah*

Main category: cs.CV

TL;DR: 本文提出了一个全球标准化的路面缺陷检测数据集，整合了来自七国的52747张图像，覆盖13类缺陷，为自动检测算法的统一训练和评估提供了基准。


<details>
  <summary>Details</summary>
Motivation: 目前路面缺陷自动检测任务中，不同数据集在标注风格、缺陷类型定义和格式上存在较大差异，导致算法难以泛化并公平比较。因此，急需一个统一标准的全球性路面缺陷数据集。

Method: 作者整合并标准化了七个国家的多份公开数据，统一缺陷类别和标注格式，最终形成52747张图像、135277个标注框、覆盖13种路面缺陷类型的多样化数据集，并采用YOLOv8-YOLOv12、Faster R-CNN和DETR等先进目标检测模型进行了基准测试。

Result: 多个SOTA目标检测模型在该数据集上取得了具有竞争力的性能，验证了该数据集的代表性和实用性。此外，还支持模型在新环境的零样本迁移。

Conclusion: 该全球标准化数据集首次为路面缺陷检测任务提供了统一的国际基准，使得不同模型可以在一致条件下公平比较，有助于推动该领域研究的进步。

Abstract: Automated pavement defect detection often struggles to generalize across diverse real-world conditions due to the lack of standardized datasets. Existing datasets differ in annotation styles, distress type definitions, and formats, limiting their integration for unified training. To address this gap, we introduce a comprehensive benchmark dataset that consolidates multiple publicly available sources into a standardized collection of 52747 images from seven countries, with 135277 bounding box annotations covering 13 distinct distress types. The dataset captures broad real-world variation in image quality, resolution, viewing angles, and weather conditions, offering a unique resource for consistent training and evaluation. Its effectiveness was demonstrated through benchmarking with state-of-the-art object detection models including YOLOv8-YOLOv12, Faster R-CNN, and DETR, which achieved competitive performance across diverse scenarios. By standardizing class definitions and annotation formats, this dataset provides the first globally representative benchmark for pavement defect detection and enables fair comparison of models, including zero-shot transfer to new environments.

</details>


### [20] [SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images](https://arxiv.org/abs/2512.20013)
*Zepeng Xin,Kaiyu Li,Luodi Chen,Wanchen Li,Yuchen Xiao,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: 作者提出了LaSeRS这一大规模遥感图像语言引导分割数据集，并开发了SegEarth-R2模型，显著提升了复杂场景下的分割效果。


<details>
  <summary>Details</summary>
Motivation: 当前遥感图像中，语言与像素的复杂对应问题尚未解决，现有模型在应对分层粒度、多目标、隐式意图等复杂任务时表现有限，缺乏合适的数据集和评测基准。

Method: 作者构建了LaSeRS数据集，针对分层粒度、目标多样性、推理需求和语言多样性四个方面设计，填补现有数据集不足。同时，提出了SegEarth-R2模型，采用空间注意力监督机制提升小目标定位，并设计了灵活高效的分割查询机制，兼顾单/多目标场景。

Result: SegEarth-R2在LaSeRS及其他基准数据集上均取得了领先表现，有效推动了复杂遥感场景下语言引导分割的进展。

Conclusion: LaSeRS和SegEarth-R2为遥感图像复杂场景的多层面语言引导分割任务提供了强有力的基准和新方法，对领域内后续研究具有重要推动作用。

Abstract: Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.

</details>


### [21] [A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments](https://arxiv.org/abs/2512.20025)
*Anthony Dontoh,Stephanie Ivey,Armstrong Aboah*

Main category: cs.CV

TL;DR: 本研究比较了只用驾驶员视角和同时用道路与驾驶员双视角的模型，对分心驾驶检测的准确度影响，发现并非所有模型加入环境视角都会提升表现。


<details>
  <summary>Details</summary>
Motivation: 当前大部分分心驾驶检测仅用驾驶员面部视角，忽视环境上下文信息。本研究探究融入道路视角是否能提升真实驾驶场景下的分心检测准确度。

Method: 采用真实驾驶双摄像头同步数据，选用三种时空动作识别模型（SlowFast-R50、X3D-M、SlowOnly-R50），分别在单一驾驶员视角和双视角（叠加）两种输入方式下评估其检测效果。

Result: 在SlowOnly模型中，融合双视角提升准确度9.8%；但在SlowFast模型中，双视角反而使准确率下降7.2%，表明不同模型对多视角信息融合的适应性差异明显。

Conclusion: 简单叠加视觉上下文并不能保证性能提升，甚至可能因融合方式不当产生干扰，应针对多视角信息设计融合感知架构，为多模态驾驶员检测系统开发提供设计依据。

Abstract: Despite increasing interest in computer vision-based distracted driving detection, most existing models rely exclusively on driver-facing views and overlook crucial environmental context that influences driving behavior. This study investigates whether incorporating road-facing views alongside driver-facing footage improves distraction detection accuracy in naturalistic driving conditions. Using synchronized dual-camera recordings from real-world driving, we benchmark three leading spatiotemporal action recognition architectures: SlowFast-R50, X3D-M, and SlowOnly-R50. Each model is evaluated under two input configurations: driver-only and stacked dual-view. Results show that while contextual inputs can improve detection in certain models, performance gains depend strongly on the underlying architecture. The single-pathway SlowOnly model achieved a 9.8 percent improvement with dual-view inputs, while the dual-pathway SlowFast model experienced a 7.2 percent drop in accuracy due to representational conflicts. These findings suggest that simply adding visual context is not sufficient and may lead to interference unless the architecture is specifically designed to support multi-view integration. This study presents one of the first systematic comparisons of single- and dual-view distraction detection models using naturalistic driving data and underscores the importance of fusion-aware design for future multimodal driver monitoring systems.

</details>


### [22] [MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis](https://arxiv.org/abs/2512.20026)
*Ziwei Qin,Xuhui Song,Deqing Huang,Na Qin,Jun Li*

Main category: cs.CV

TL;DR: 该论文提出了MAPI-GNN，通过学习多维语义特征子空间，动态构建多元激活图谱，显著提升了多模态医学诊断的性能。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络对于医学诊断主要依赖一个静态、基于非特定特征的单一图，难以有效建模针对具体患者的病理关系，这限制了诊断准确性。

Method: 作者提出MAPI-GNN，包含多维判别器用于发现潜在的图感知模式，指导动态构建多层激活图，并利用关系融合模块进行多元信息聚合，实现语义解耦和患者个性化。

Result: 在两个包含1300+患者样本的医学诊断任务上进行大量实验，MAPI-GNN性能明显优于现有主流方法。

Conclusion: MAPI-GNN能够更好地捕捉患者个性化的病理特征关系，在多模态医学诊断领域表现出强大的实用价值和推广潜力。

Abstract: Graph neural networks are increasingly applied to multimodal medical diagnosis for their inherent relational modeling capabilities. However, their efficacy is often compromised by the prevailing reliance on a single, static graph built from indiscriminate features, hindering the ability to model patient-specific pathological relationships. To this end, the proposed Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN) reconstructs this single-graph paradigm by learning a multifaceted graph profile from semantically disentangled feature subspaces. The framework first uncovers latent graph-aware patterns via a multi-dimensional discriminator; these patterns then guide the dynamic construction of a stack of activation graphs; and this multifaceted profile is finally aggregated and contextualized by a relational fusion engine for a robust diagnosis. Extensive experiments on two diverse tasks, comprising over 1300 patient samples, demonstrate that MAPI-GNN significantly outperforms state-of-the-art methods.

</details>


### [23] [$\text{H}^2$em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning](https://arxiv.org/abs/2512.20029)
*Lin Li,Jiahui Li,Jiaming Lei,Jun Xiao,Feifei Shao,Long Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的分层超球面嵌入框架H2em，用于提升组合零样本学习（CZSL）对复杂层级结构的建模能力，显著提升了现实场景下的泛化与识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有CZSL方法通常忽视了状态、物体等原语及其组合间丰富的层级结构，并且平坦欧氏空间难以很好表达大规模层级，影响泛化。需要更适合表达树状层级结构的方法以提升性能。

Method: 提出H2em框架，利用超球面几何对状态、物体及其组合进行层次化嵌入。设计了双层级蕴涵损失（Dual-Hierarchical Entailment Loss）来保证层级结构，以及判别性对齐损失（Discriminative Alignment Loss），结合超球面交叉模态注意力模块，提升细粒度区分与语义对齐能力。

Result: 在三个组合零样本学习基准数据集上进行了大量消融实验，H2em在闭集和开放世界两种场景下皆取得了最优性能，超越了现有方法。

Conclusion: 通过引入超球面层次结构嵌入和创新的损失设计，H2em极大提升了大规模组合零样本学习的状态，实现了新的SOTA，验证了更适合层级结构的嵌入空间对泛化性的重要性。

Abstract: Compositional zero-shot learning (CZSL) aims to recognize unseen state-object compositions by generalizing from a training set of their primitives (state and object). Current methods often overlook the rich hierarchical structures, such as the semantic hierarchy of primitives (e.g., apple fruit) and the conceptual hierarchy between primitives and compositions (e.g, sliced apple apple). A few recent efforts have shown effectiveness in modeling these hierarchies through loss regularization within Euclidean space. In this paper, we argue that they fail to scale to the large-scale taxonomies required for real-world CZSL: the space's polynomial volume growth in flat geometry cannot match the exponential structure, impairing generalization capacity. To this end, we propose H2em, a new framework that learns Hierarchical Hyperbolic EMbeddings for CZSL. H2em leverages the unique properties of hyperbolic geometry, a space naturally suited for embedding tree-like structures with low distortion. However, a naive hyperbolic mapping may suffer from hierarchical collapse and poor fine-grained discrimination. We further design two learning objectives to structure this space: a Dual-Hierarchical Entailment Loss that uses hyperbolic entailment cones to enforce the predefined hierarchies, and a Discriminative Alignment Loss with hard negative mining to establish a large geodesic distance between semantically similar compositions. Furthermore, we devise Hyperbolic Cross-Modal Attention to realize instance-aware cross-modal infusion within hyperbolic geometry. Extensive ablations on three benchmarks demonstrate that H2em establishes a new state-of-the-art in both closed-world and open-world scenarios. Our codes will be released.

</details>


### [24] [VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement](https://arxiv.org/abs/2512.20032)
*Chang Sun,Dongliang Xie,Bo Qin,Hong Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为VALLR-Pin的两阶段视觉语音识别框架，结合视觉、拼音和语言模型，提升了普通话无声唇读的准确率。


<details>
  <summary>Details</summary>
Motivation: 普通话视觉语音识别难度大，主要因为唇形信息易混淆与同音字多，传统方法难以准确区分。

Method: 首先，用共享视频编码器提取视觉特征，并通过双解码器预测汉字序列与标准拼音序列，实现字符和语音多任务学习；推理时，利用拼音与候选汉字构建提示词输入大语言模型（LLM）以消除同音干扰，并通过合成噪声样本微调LLM让其适应模型常见错误。

Result: VALLR-Pin结合多模态（视觉、拼音、语言）上下文信息，有效提升普通话唇读识别准确性，尤其在同音字区分上取得明显进步。

Conclusion: VALLR-Pin证明了结合视觉、语音与语言知识可提升复杂语音识别任务性能，为中文唇读和多模态语音理解提供了新范式。

Abstract: Visual Speech Recognition aims to transcribe spoken words from silent lip-motion videos. This task is particularly challenging for Mandarin, as visemes are highly ambiguous and homophones are prevalent. We propose VALLR-Pin, a novel two-stage framework that extends the recent VALLR architecture from English to Mandarin. First, a shared video encoder feeds into dual decoders, which jointly predict both Chinese character sequences and their standard Pinyin romanization. The multi-task learning of character and phonetic outputs fosters robust visual-semantic representations. During inference, the text decoder generates multiple candidate transcripts. We construct a prompt by concatenating the Pinyin output with these candidate Chinese sequences and feed it to a large language model to resolve ambiguities and refine the transcription. This provides the LLM with explicit phonetic context to correct homophone-induced errors. Finally, we fine-tune the LLM on synthetic noisy examples: we generate imperfect Pinyin-text pairs from intermediate VALLR-Pin checkpoints using the training data, creating instruction-response pairs for error correction. This endows the LLM with awareness of our model's specific error patterns. In summary, VALLR-Pin synergizes visual features with phonetic and linguistic context to improve Mandarin lip-reading performance.

</details>


### [25] [FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs](https://arxiv.org/abs/2512.20033)
*Andreas Zinonos,Michał Stypułkowski,Antoni Bigata,Stavros Petridis,Maja Pantic,Nikita Drobyshev*

Main category: cs.CV

TL;DR: 该论文提出了FlashLips系统，实现端到端、无需显式口罩的实时唇动同步，速度超过100FPS，视觉效果媲美主流大型模型。


<details>
  <summary>Details</summary>
Motivation: 现有的唇动同步方法通常依赖口罩（mask）和复杂的训练方式（如GAN、扩散模型），导致推理速度慢、部署成本高，难以达到实时性能，因此亟需一种高效、无需显式口罩且易于部署的方法。

Method: 提出了两阶段系统：第一阶段为潜空间编辑器，输入身份参照图像、带口部变化的目标帧和低维唇部姿态向量，仅用重建损失训练，无需GAN/扩散模型。去口罩化通过自监督伪标注，即伪造带口腔变化的目标图像辅助网络在测试时自主聚焦唇部区域编辑。第二阶段为音频到唇部姿态的transformer，通过flow-matching目标训练，从语音预测唇部姿态。两阶段组合实现鲁棒且确定性的重建及稳健的语音控制。

Result: FlashLips在一块GPU上实现百帧以上实时推理，视频质量与较大的先进模型持平。其简单性和稳定性兼顾高性能和实际部署需求。

Conclusion: FlashLips方法刷新了实时唇动同步的速度和质量门槛，为实际音视频编辑、动画等场景提供了简单易用、高效且高质量的解决方案。

Abstract: We present FlashLips, a two-stage, mask-free lip-sync system that decouples lips control from rendering and achieves real-time performance running at over 100 FPS on a single GPU, while matching the visual quality of larger state-of-the-art models. Stage 1 is a compact, one-step latent-space editor that reconstructs an image using a reference identity, a masked target frame, and a low-dimensional lips-pose vector, trained purely with reconstruction losses - no GANs or diffusion. To remove explicit masks at inference, we use self-supervision: we generate mouth-altered variants of the target image, that serve as pseudo ground truth for fine-tuning, teaching the network to localize edits to the lips while preserving the rest. Stage 2 is an audio-to-pose transformer trained with a flow-matching objective to predict lips-poses vectors from speech. Together, these stages form a simple and stable pipeline that combines deterministic reconstruction with robust audio control, delivering high perceptual quality and faster-than-real-time speed.

</details>


### [26] [Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva](https://arxiv.org/abs/2512.20042)
*Nguyen Lam Phu Quy,Pham Phu Hoa,Tran Chi Nguyen,Dao Sy Duy Minh,Nguyen Hoang Minh Ngoc,Huynh Trung Kiet*

Main category: cs.CV

TL;DR: 本文提出了一个多模态流程，通过外部文本知识增强图像描述，克服了现实世界图像标题缺乏背景和上下文信息的问题，在OpenEvents v1数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实图像标题通常遗漏事件背景、时间线索、结果和不可见的专有名词，这在新闻、教育和档案等领域限制了图像理解的有效性。针对这一限制，亟需更丰富、上下文感知的描述方法。

Method: 作者提出了利用多模态信息和知识检索的流程。具体方法是：用BEIT-3和SigLIP等模型检索语义相似图像，通过ORB和SIFT进行几何重排序，结合相关文献和文章进行知识抽取，然后用微调后的Qwen3（配合QLoRA）融合上下文，最终在Instruct BLIP生成的基础标题上生成上下文丰富的描述。

Result: 在OpenEvents v1数据集上，所提方法比传统方法生成了显著更有信息量的图像标题，提升了视觉与文本的深度理解能力。

Conclusion: 该方法能够有效提升图像标题的上下文丰富性和事件感知能力，具有显著实际应用潜力，特别适合需要更深层次理解的场景，如新闻报道、教育和数字档案等。

Abstract: Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding

</details>


### [27] [Progressive Learned Image Compression for Machine Perception](https://arxiv.org/abs/2512.20070)
*Jungwoo Kim,Jun-Hyuk Kim,Jong-Seok Lee*

Main category: cs.CV

TL;DR: 本文提出了面向机器感知的渐进式图像压缩方法PICM-Net，可以根据任务需求以不同质量级别解码同一比特流，并在下游分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前的学习型图像编解码器多关注于人类感知，而渐进式图像压缩（如FGS）在面向机器感知的场景下仍未被充分探索。实际应用中，针对机器视觉任务，对不同质量和处理优先级的需求亟需更灵活、高效的压缩方案。

Method: 提出了一种名为PICM-Net的面向机器感知的渐进式图像压缩方法，基于trit-plane编码机制。该方法深入分析了人类与机器感知在率失真优先级上的差异，设计了适应机器端推理需求的潜在分层优先策略。同时，提出自适应解码控制器，可在解码时动态调整解码质量级别，以保证下游机器预测的置信度需求。

Result: 实验证明，该方法支持高效、自适应、渐进式图像传输，并在下游分类任务上维持高性能，较现有方案在机器感知场景下具有明显优势。

Conclusion: 研究成果为面向机器的渐进式图像压缩开辟了新方向，实现了在不同下游机器视觉任务需求下的自适应解码和高效数据传输。

Abstract: Recent advances in learned image codecs have been extended from human perception toward machine perception. However, progressive image compression with fine granular scalability (FGS)-which enables decoding a single bitstream at multiple quality levels-remains unexplored for machine-oriented codecs. In this work, we propose a novel progressive learned image compression codec for machine perception, PICM-Net, based on trit-plane coding. By analyzing the difference between human- and machine-oriented rate-distortion priorities, we systematically examine the latent prioritization strategies in terms of machine-oriented codecs. To further enhance real-world adaptability, we design an adaptive decoding controller, which dynamically determines the necessary decoding level during inference time to maintain the desired confidence of downstream machine prediction. Extensive experiments demonstrate that our approach enables efficient and adaptive progressive transmission while maintaining high performance in the downstream classification task, establishing a new paradigm for machine-aware progressive image compression.

</details>


### [28] [Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts](https://arxiv.org/abs/2512.20088)
*Jinyoung Choi,Youngchae Kwon,Injung Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于服装局部区域的时尚风格分类网络（IRSN），能够有效提升风格分类的准确率。


<details>
  <summary>Details</summary>
Motivation: 因为同一风格内部视觉变化大，不同风格间又可能存在视觉相似，导致时尚风格分类非常具有挑战性。风格不仅源于整体外观，还与单项服饰的属性及其组合相关。

Method: 提出IRSN，首先用item region pooling (IRP)提取每个服饰区域特征，分别分析后再用gated feature fusion (GFF)机制融合这些特征与整体特征。此外，网络采用双主干架构，结合了领域特定特征提取器和通用特征提取器（后者经大规模图文数据预训练）。

Result: 在FashionStyle14和ShowniqV3两个数据集上，通过将IRSN应用于多个主流网络（如EfficientNet、ConvNeXt、SwinTransformer）主干，平均准确率分别提升了6.9%/7.6%，最高提升达14.5%/15.1%。可视化分析也显示IRSN模型比基线模型更能区分相似风格。

Conclusion: IRSN通过同时关注整体与局部特征及其组合，显著提升了时尚风格分类的效果，尤其擅长区分视觉上相近的风格类别。

Abstract: Fashion style classification is a challenging task because of the large visual variation within the same style and the existence of visually similar styles.
  Styles are expressed not only by the global appearance, but also by the attributes of individual items and their combinations.
  In this study, we propose an item region-based fashion style classification network (IRSN) to effectively classify fashion styles by analyzing item-specific features and their combinations in addition to global features.
  IRSN extracts features of each item region using item region pooling (IRP), analyzes them separately, and combines them using gated feature fusion (GFF).
  In addition, we improve the feature extractor by applying a dual-backbone architecture that combines a domain-specific feature extractor and a general feature extractor pre-trained with a large-scale image-text dataset.
  In experiments, applying IRSN to six widely-used backbones, including EfficientNet, ConvNeXt, and Swin Transformer, improved style classification accuracy by an average of 6.9% and a maximum of 14.5% on the FashionStyle14 dataset and by an average of 7.6% and a maximum of 15.1% on the ShowniqV3 dataset. Visualization analysis also supports that the IRSN models are better than the baseline models at capturing differences between similar style classes.

</details>


### [29] [Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models](https://arxiv.org/abs/2512.20104)
*Subrata Kumer Paula,Dewan Nafiul Islam Noora,Rakhi Rani Paula,Md. Ekramul Hamidb,Fahmid Al Faridc,Hezerul Abdul Karimd,Md. Maruf Al Hossain Princee,Abu Saleh Musa Miahb*

Main category: cs.CV

TL;DR: 本研究分析了在深度学习人类活动识别（HAR）任务中，不同激活函数（ReLU、Sigmoid、Tanh）与优化器（SGD、Adam、RMSprop、Adagrad）组合对模型性能的影响。通过对BiLSTM和ConvLSTM两种架构在医疗相关数据集上的实验证明：ConvLSTM尤其配合Adam或RMSprop时表现最佳，准确率高达99%。这为医疗等实际环境下的HAR系统优化提供了指导。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习的HAR系统普及，但激活函数和优化器对模型性能的具体影响及其交互作用尚未被系统研究。多数现有工作聚焦于网络结构创新，忽略了AF与MO选择间的耦合影响。本工作旨在揭示AF与MO组合对常见时序模型（BiLSTM、ConvLSTM）在实际医疗相关应用场景中的效果差异。

Method: 选用BiLSTM和ConvLSTM为主干架构，组合三种激活函数（ReLU、Sigmoid、Tanh）与四种优化器（SGD、Adam、RMSprop、Adagrad），在从HMDB51与UCF101中挑选的6类与医疗相关的人体活动数据上进行实验，系统评估各组合方案在不同模型和数据集中的表现。

Result: ConvLSTM在两个数据集的表现均优于BiLSTM。尤其当搭配Adam或RMSprop优化器时，ConvLSTM可取得99%的最高准确率，展现出优良的时空特征学习及稳定性。BiLSTM虽在UCF101上接近98%准确率，但在HMDB51上仅约60%，显示其对不同数据集的鲁棒性和对AF/MO的敏感性不足。

Conclusion: 本文实证了激活函数与优化器组合对HAR系统性能的显著影响，尤其在医疗等场景下，推荐采用ConvLSTM结合Adam或RMSprop以实现高准确率及强泛化能力。该结果为应用中优化HAR模型提供了实践指南。

Abstract: Human Activity Recognition (HAR) plays a vital role in healthcare, surveillance, and innovative environments, where reliable action recognition supports timely decision-making and automation. Although deep learning-based HAR systems are widely adopted, the impact of Activation Functions (AFs) and Model Optimizers (MOs) on performance has not been sufficiently analyzed, particularly regarding how their combinations influence model behavior in practical scenarios. Most existing studies focus on architecture design, while the interaction between AF and MO choices remains relatively unexplored. In this work, we investigate the effect of three commonly used activation functions (ReLU, Sigmoid, and Tanh) combined with four optimization algorithms (SGD, Adam, RMSprop, and Adagrad) using two recurrent deep learning architectures, namely BiLSTM and ConvLSTM. Experiments are conducted on six medically relevant activity classes selected from the HMDB51 and UCF101 datasets, considering their suitability for healthcare-oriented HAR applications. Our experimental results show that ConvLSTM consistently outperforms BiLSTM across both datasets. ConvLSTM, combined with Adam or RMSprop, achieves an accuracy of up to 99.00%, demonstrating strong spatio-temporal learning capabilities and stable performance. While BiLSTM performs reasonably well on UCF101, with accuracy approaching 98.00%, its performance drops to approximately 60.00% on HMDB51, indicating limited robustness across datasets and weaker sensitivity to AF and MO variations. This study provides practical insights for optimizing HAR systems, particularly for real-world healthcare environments where fast and precise activity detection is critical.

</details>


### [30] [LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs](https://arxiv.org/abs/2512.20105)
*Haiyun Wei,Fan Lu,Yunwei Zhu,Zehan Zheng,Weiyi Xue,Lin Shao,Xudong Zhang,Ya Wu,Rong Fu,Guang Chen*

Main category: cs.CV

TL;DR: LiDARDraft方法通过3D布局桥接多种控制信号与LiDAR点云，实现高质量、高多样性的点云模拟，支持多样输入、细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法在保证高质量的基础上，灵活控制生成的LiDAR点云，原因在于点云分布复杂、控制信号简单之间的不平衡。本工作旨在解决高质量与可控性的矛盾。

Method: 提出LiDARDraft框架，将文本、图片等多种用户输入转化为统一的3D布局，再分解为语义和深度控制信号，利用基于rangemap的ControlNet进行点云生成，实现像素级对齐和可控性。

Result: 实验验证该方法支持从文本、图片、草图等输入直接生成高质量点云，展现出更强的灵活性和可控性。

Conclusion: LiDARDraft可实现高质量、可控的LiDAR点云生成，适用于自动驾驶仿真等多场景，“从零起步”构建自定义环境，显著优于先前方法。

Abstract: Generating realistic and diverse LiDAR point clouds is crucial for autonomous driving simulation. Although previous methods achieve LiDAR point cloud generation from user inputs, they struggle to attain high-quality results while enabling versatile controllability, due to the imbalance between the complex distribution of LiDAR point clouds and the simple control signals. To address the limitation, we propose LiDARDraft, which utilizes the 3D layout to build a bridge between versatile conditional signals and LiDAR point clouds. The 3D layout can be trivially generated from various user inputs such as textual descriptions and images. Specifically, we represent text, images, and point clouds as unified 3D layouts, which are further transformed into semantic and depth control signals. Then, we employ a rangemap-based ControlNet to guide LiDAR point cloud generation. This pixel-level alignment approach demonstrates excellent performance in controllable LiDAR point clouds generation, enabling "simulation from scratch", allowing self-driving environments to be created from arbitrary textual descriptions, images and sketches.

</details>


### [31] [Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark](https://arxiv.org/abs/2512.20174)
*Hao Guo,Xugong Qin,Jun Jie Ou Yang,Peng Zhang,Gangyan Zeng,Yubo Li,Hailun Lin*

Main category: cs.CV

TL;DR: 本文提出了一个基于自然语言检索文档图像（NL-DIR）的新基准数据集和评测方法，促进文档图像理解领域的发展。


<details>
  <summary>Details</summary>
Motivation: 现有文档图像检索（DIR）方法主要基于图像检索，且多只区分粗类别，但实际需求中常以带有细粒度语义的文本为查询，现有方法效果有限，亟需新的数据集与评测体系。

Method: 提出了NL-DIR基准，包含4.1万张真实文档图像，每张图片配有5条高质量细粒度自然语言描述（由大模型生成并人工审核）；并对现有主流的对比式视觉-语言模型和无OCR视觉文档理解模型进行了零样本和微调评测；还探索了两阶段检索方法以在效率和效果间优化。

Result: NL-DIR数据集和评测体系丰富了检索类型；实验结果分析了主流模型的表现，并表明两阶段检索方法有助于提升检索效率和效果。

Conclusion: NL-DIR为文档图像检索提供了更贴近真实应用需求的评测基准，有助于推动视觉文档理解领域基于自然语言检索方向的研究。

Abstract: Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.

</details>


### [32] [UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis](https://arxiv.org/abs/2512.20107)
*Thanh-Tung Le,Tuan Pham,Tung Nguyen,Deying Kong,Xiaohui Xie,Stephan Mandt*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的视图合成方法，将确定性网络与生成性扩散模型融合，提高了3D一致性和真实感，并大幅降低了渲染时间。


<details>
  <summary>Details</summary>
Motivation: 现有NVS方法存在取舍困境：确定性网络速度快但对未观测区域表现较差，扩散模型可生成合理内容但训练和推理开销极高，因此亟需一种兼具两者优点的方法。

Method: 提出了一个双分支架构，核心为双向Transformer，结合多视角图像与Plücker射线嵌入生成共享潜在表示。该表示分别被馈送至前馈回归头（渲染已知几何像素）和带掩码自回归扩散头（补全遮挡/不可见区域），模型端到端以联合光度和扩散损失训练，无需手工3D归纳偏置。

Result: 在多个实验中，该方法实现了超越现有生成式模型的图像质量，并且渲染速度提升一个数量级。

Conclusion: 本文提出的混合框架既保证了真实感和3D一致性，又大大提高了效率，展现了强大的通用性和可扩展性。

Abstract: Novel view synthesis (NVS) seeks to render photorealistic, 3D-consistent images of a scene from unseen camera poses given only a sparse set of posed views. Existing deterministic networks render observed regions quickly but blur unobserved areas, whereas stochastic diffusion-based methods hallucinate plausible content yet incur heavy training- and inference-time costs. In this paper, we propose a hybrid framework that unifies the strengths of both paradigms. A bidirectional transformer encodes multi-view image tokens and Plucker-ray embeddings, producing a shared latent representation. Two lightweight heads then act on this representation: (i) a feed-forward regression head that renders pixels where geometry is well constrained, and (ii) a masked autoregressive diffusion head that completes occluded or unseen regions. The entire model is trained end-to-end with joint photometric and diffusion losses, without handcrafted 3D inductive biases, enabling scalability across diverse scenes. Experiments demonstrate that our method attains state-of-the-art image quality while reducing rendering time by an order of magnitude compared with fully generative baselines.

</details>


### [33] [Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection](https://arxiv.org/abs/2512.20113)
*Alireza Moayedikia,Sattar Dorafshan*

Main category: cs.CV

TL;DR: 本文提出了一种多模态注意力网络，将雷达时序模式与热成像空间特征融合，用于桥梁覆层分层缺陷检测。实验结果显示该方法在多数据集上优于传统单一及拼接融合方法，具有不确定性量化能力，能提升安全决策的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基础设施检测方法如雷达和红外热成像各有局限：雷达难以应对潮湿/浅层缺陷，热成像受天气和深度限制。单一传感器不能全面检测桥梁缺陷，因此亟需融合多种传感器的自动化方法。

Method: 设计了一种多模态注意力网络架构，包括：对雷达信号采用时序注意力，对热成像采用空间注意力，并通过可学习嵌入进行跨模态特征融合。引入了基于蒙特卡洛dropout和学习方差的不确定性分解（区分epistemic和aleatoric），用于提升决策安全。

Result: 在五个桥梁数据集上评估，结果表明：在数据平衡或中度失衡时，该方法在准确率和AUC等指标上显著优于单一模态和简单拼接融合；消融实验表明跨模态注意力带来关键性能提升；多头机制有助于校准不确定性。在极端类别失衡下注意力机制易受主类塌缩影响。

Conclusion: 多模态注意力融合方法适用于绝大多数典型检测场景，提升了准确率和决策安全性，对实时部署友好。但在极端类别失衡情形下需引入专业平衡技术。对于工程应用提供了有针对性的改进建议。

Abstract: Deteriorating civil infrastructure requires automated inspection techniques overcoming limitations of visual assessment. While Ground Penetrating Radar and Infrared Thermography enable subsurface defect detection, single modal approaches face complementary constraints radar struggles with moisture and shallow defects, while thermography exhibits weather dependency and limited depth. This paper presents a multi modal attention network fusing radar temporal patterns with thermal spatial signatures for bridge deck delamination detection. Our architecture introduces temporal attention for radar processing, spatial attention for thermal features, and cross modal fusion with learnable embeddings discovering complementary defect patterns invisible to individual sensors. We incorporate uncertainty quantification through Monte Carlo dropout and learned variance estimation, decomposing uncertainty into epistemic and aleatoric components for safety critical decisions. Experiments on five bridge datasets reveal that on balanced to moderately imbalanced data, our approach substantially outperforms baselines in accuracy and AUC representing meaningful improvements over single modal and concatenation based fusion. Ablation studies demonstrate cross modal attention provides critical gains beyond within modality attention, while multi head mechanisms achieve improved calibration. Uncertainty quantification reduces calibration error, enabling selective prediction by rejecting uncertain cases. However, under extreme class imbalance, attention mechanisms show vulnerability to majority class collapse. These findings provide actionable guidance: attention based architecture performs well across typical scenarios, while extreme imbalance requires specialized techniques. Our system maintains deployment efficiency, enabling real time inspection with characterized capabilities and limitations.

</details>


### [34] [DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation](https://arxiv.org/abs/2512.20117)
*Jingqi Tian,Yiheng Du,Haoji Zhang,Yuji Wang,Isaac Ning Lee,Xulong Bai,Tianrui Zhu,Jingxuan Niu,Yansong Tang*

Main category: cs.CV

TL;DR: 提出了一种新颖的音频-视觉分割（AVS）方法——DDAVS，通过解耦音频语义和引入延迟的双向对齐机制，显著提升了复杂场景下的分割精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有AVS方法面临的多音源纠缠和音频视觉对齐不良等问题，如过度关注声音大或物体大的目标，忽略弱小或共现目标。

Method: DDAVS设计了可学习的查询机制来提取音频语义，并利用音频原型记忆库构建有结构的语义空间，结合对比学习提升鲁棒性和判别性；同时，引入延迟的双向交互注意力机制，有效增强音视协同对齐能力。

Result: 在AVS-Objects和VPO等基准上，DDAVS方法在单源、多源及多实例等多种场景下均大幅优于现有方案，表现出了强大的泛化能力和鲁棒性。

Conclusion: DDAVS框架在面对现实复杂AVS任务时，展示了较强的效果和通用性，有效缓解了多源纠缠与视听对齐难题。

Abstract: Audio-Visual Segmentation (AVS) aims to localize sound-producing objects at the pixel level by jointly leveraging auditory and visual information. However, existing methods often suffer from multi-source entanglement and audio-visual misalignment, which lead to biases toward louder or larger objects while overlooking weaker, smaller, or co-occurring sources. To address these challenges, we propose DDAVS, a Disentangled Audio Semantics and Delayed Bidirectional Alignment framework. To mitigate multi-source entanglement, DDAVS employs learnable queries to extract audio semantics and anchor them within a structured semantic space derived from an audio prototype memory bank. This is further optimized through contrastive learning to enhance discriminability and robustness. To alleviate audio-visual misalignment, DDAVS introduces dual cross-attention with delayed modality interaction, improving the robustness of multimodal alignment. Extensive experiments on the AVS-Objects and VPO benchmarks demonstrate that DDAVS consistently outperforms existing approaches, exhibiting strong performance across single-source, multi-source, and multi-instance scenarios. These results validate the effectiveness and generalization ability of our framework under challenging real-world audio-visual segmentation conditions. Project page: https://trilarflagz.github.io/DDAVS-page/

</details>


### [35] [HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer](https://arxiv.org/abs/2512.20120)
*Mohammad Helal Uddin,Liam Seymour,Sabur Baidya*

Main category: cs.CV

TL;DR: HEART-ViT提出了一种基于Hessian的高效Vision Transformer稀疏化方法，能够联合裁剪Token和Attention Head，实现更大幅度的计算加速和准确率保持，并在多种数据集和真实设备上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: ViT模型的高精度伴随着高昂的计算和延迟成本，尤其在资源受限的平台部署受限，现有稀疏化方法效果有限，通常只能处理Token或Head中的一种且牺牲精度，因此需要一种更有效、兼顾两种粒度且具有理论支持的稀疏化方法。

Method: 提出HEART-ViT框架，利用高效的Hessian向量积，估算Token和Head的二阶敏感性，并据此在明确定义的损失约束下联合动态裁剪Token和Head。该方法兼顾了粗粒度（Token）和细粒度（Head）的冗余移除，实现更优权衡。

Result: 在ImageNet-100和ImageNet-1K等数据集上（如ViT-B/16、DeiT-B/16）最多可减少49.4%的FLOPs、36%的延迟和提升46%的吞吐，同时在大幅稀疏情况下通过微调维持甚至提升了准确率。在边缘设备上（如AGX Orin）表现出实际推理速度和能效的提高。

Conclusion: HEART-ViT是首个统一的、基于曲率的动态ViT稀疏化框架，在保持精度的同时大幅提升了边缘部署效率，有效缩小了理论算法与实际应用之间的差距。

Abstract: Vision Transformers (ViTs) deliver state-of-the-art accuracy but their quadratic attention cost and redundant computations severely hinder deployment on latency and resource-constrained platforms. Existing pruning approaches treat either tokens or heads in isolation, relying on heuristics or first-order signals, which often sacrifice accuracy or fail to generalize across inputs. We introduce HEART-ViT, a Hessian-guided efficient dynamic attention and token pruning framework for vision transformers, which to the best of our knowledge is the first unified, second-order, input-adaptive framework for ViT optimization. HEART-ViT estimates curvature-weighted sensitivities of both tokens and attention heads using efficient Hessian-vector products, enabling principled pruning decisions under explicit loss budgets.This dual-view sensitivity reveals an important structural insight: token pruning dominates computational savings, while head pruning provides fine-grained redundancy removal, and their combination achieves a superior trade-off. On ImageNet-100 and ImageNet-1K with ViT-B/16 and DeiT-B/16, HEART-ViT achieves up to 49.4 percent FLOPs reduction, 36 percent lower latency, and 46 percent higher throughput, while consistently matching or even surpassing baseline accuracy after fine-tuning, for example 4.7 percent recovery at 40 percent token pruning. Beyond theoretical benchmarks, we deploy HEART-ViT on different edge devices such as AGX Orin, demonstrating that our reductions in FLOPs and latency translate directly into real-world gains in inference speed and energy efficiency. HEART-ViT bridges the gap between theory and practice, delivering the first unified, curvature-driven pruning framework that is both accuracy-preserving and edge-efficient.

</details>


### [36] [milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion](https://arxiv.org/abs/2512.20128)
*Niraj Prakash Kini,Shiau-Rung Tsai,Guan-Hsun Lin,Wen-Hsiao Peng,Ching-Wen Ma,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 本文提出了一种利用毫米波雷达进行2D人体姿态估计的新方法milliMamba，通过跨视角融合和时空建模，有效提升了稀疏雷达信号下的特征提取与推断能力，在多个公开数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达在隐私保护和光照不变性方面优于RGB传感器，但雷达信号受镜面反射影响通常很稀疏，导致人体姿态估计中的鲁棒特征提取变得非常具有挑战性，亟需创新性的方法来克服此难题。

Method: 提出了milliMamba框架：首先采用跨视角融合Mamba编码器以线性复杂度高效提取雷达输入的长时序时空特征，然后用时空交叉注意力解码器预测多帧各关键点坐标。该管线能够综合利用相邻帧与关节的上下文信息，对因镜面反射缺失的关节点进行推断。同时在训练时引入速度损失，以强化运动平滑性。

Result: 在TransHuPR和HuPR两个公开数据集上的实验表明，本方法分别超过基线11.0和14.6 AP，同时保持了合理的模型复杂度。

Conclusion: milliMamba有效提升了雷达人体姿态估计的准确性与鲁棒性，为隐私友好且适应复杂环境的姿态估计提供了一条有前景的技术路线。

Abstract: Millimeter-wave radar offers a privacy-preserving and lighting-invariant alternative to RGB sensors for Human Pose Estimation (HPE) task. However, the radar signals are often sparse due to specular reflection, making the extraction of robust features from radar signals highly challenging. To address this, we present milliMamba, a radar-based 2D human pose estimation framework that jointly models spatio-temporal dependencies across both the feature extraction and decoding stages. Specifically, given the high dimensionality of radar inputs, we adopt a Cross-View Fusion Mamba encoder to efficiently extract spatio-temporal features from longer sequences with linear complexity. A Spatio-Temporal-Cross Attention decoder then predicts joint coordinates across multiple frames. Together, this spatio-temporal modeling pipeline enables the model to leverage contextual cues from neighboring frames and joints to infer missing joints caused by specular reflections. To reinforce motion smoothness, we incorporate a velocity loss alongside the standard keypoint loss during training. Experiments on the TransHuPR and HuPR datasets demonstrate that our method achieves significant performance improvements, exceeding the baselines by 11.0 AP and 14.6 AP, respectively, while maintaining reasonable complexity. Code: https://github.com/NYCU-MAPL/milliMamba

</details>


### [37] [CoDi -- an exemplar-conditioned diffusion model for low-shot counting](https://arxiv.org/abs/2512.20153)
*Grega Šuštar,Jer Pelhan,Alan Lukežič,Matej Kristan*

Main category: cs.CV

TL;DR: 本文提出了一种新的低样本物体计数方法CoDi，实现了对图片中新类别小目标的精确计数和定位，并在多个公开数据集上刷新了表现。


<details>
  <summary>Details</summary>
Motivation: 现有的低样本计数方法在密集、小目标区域面临挑战。基于密度的计数方法虽然总数表现好，但定位差；基于点检测的计数方法由于预训练查询数目有限，无法处理大量目标，只能依赖不够稳定的采样和拼图等技巧。

Method: 作者提出了一种基于潜在扩散的低样本计数模型CoDi。通过新设计的示例条件模块将物体原型动态嵌入到去噪网络中，实现既能输出高质量密度图，又能通过非极大值抑制得到精确的目标定位。

Result: 在FSC数据集上，CoDi分别在多样本、单样本和无参考场景下将最优方法的MAE分别降低了15%、13%和10%；在MCAC数据集上的MAE提升高达44%。

Conclusion: CoDi显著提升了低样本计数的准确性和定位能力，在多个主流基准上刷新了SOTA纪录，具备较强的实际应用前景。

Abstract: Low-shot object counting addresses estimating the number of previously unobserved objects in an image using only few or no annotated test-time exemplars. A considerable challenge for modern low-shot counters are dense regions with small objects. While total counts in such situations are typically well addressed by density-based counters, their usefulness is limited by poor localization capabilities. This is better addressed by point-detection-based counters, which are based on query-based detectors. However, due to limited number of pre-trained queries, they underperform on images with very large numbers of objects, and resort to ad-hoc techniques like upsampling and tiling. We propose CoDi, the first latent diffusion-based low-shot counter that produces high-quality density maps on which object locations can be determined by non-maxima suppression. Our core contribution is the new exemplar-based conditioning module that extracts and adjusts the object prototypes to the intermediate layers of the denoising network, leading to accurate object location estimation. On FSC benchmark, CoDi outperforms state-of-the-art by 15% MAE, 13% MAE and 10% MAE in the few-shot, one-shot, and reference-less scenarios, respectively, and sets a new state-of-the-art on MCAC benchmark by outperforming the top method by 44% MAE. The code is available at https://github.com/gsustar/CoDi.

</details>


### [38] [AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model](https://arxiv.org/abs/2512.20157)
*Sofian Chaybouti,Sanath Narayan,Yasser Dahou,Phúc H. Lê Khac,Ankit Singh,Ngoc Dung Huynh,Wamiq Reyaz Para,Hilde Kuehne,Hakim Hacid*

Main category: cs.CV

TL;DR: 提出了融合多教师蒸馏的新视觉基础模型AMoE，能以更低计算成本实现有效知识迁移，并通过新方法提升样本和数据利用率。


<details>
  <summary>Details</summary>
Motivation: 多教师蒸馏可以融合不同视觉模型优势，但其学习机制与数据效率尚未被系统研究。本文旨在探索如何在降低计算开销同时提升蒸馏效果。

Method: 提出AMoE模型，采用SigLIP2与DINOv3为教师，通过异构关系知识蒸馏损失函数实现几何知识保留与转换。引入token-balanced batching方法，均匀分配不同分辨率图像token，稳定多分辨率下的表示学习。同时利用层次聚类与采样（通常用于自监督学习）优化训练数据，提升样本效率。

Result: 验证所提方法能在不损失性能前提下降低计算量，层次聚类采样显著优于随机采样。构建并开源高效的OpenLVD200M数据集与蒸馏模型。

Conclusion: 通过创新的知识蒸馏损失、合理的数据打包和高效采样策略，AMoE模型能更高效、低成本地整合多教师知识，推动视觉基础模型统一与数据高效利用。

Abstract: Vision foundation models trained via multi-teacher distillation offer a promising path toward unified visual representations, yet the learning dynamics and data efficiency of such approaches remain underexplored. In this paper, we systematically study multi-teacher distillation for vision foundation models and identify key factors that enable training at lower computational cost. We introduce Agglomerative Mixture-of-Experts Vision Foundation Models (AMoE), which distill knowledge from SigLIP2 and DINOv3 simultaneously into a Mixture-of-Experts student. We show that (1) our Asymmetric Relation-Knowledge Distillation loss preserves the geometric properties of each teacher while enabling effective knowledge transfer, (2) token-balanced batching that packs varying-resolution images into sequences with uniform token budgets stabilizes representation learning across resolutions without sacrificing performance, and (3) hierarchical clustering and sampling of training data--typically reserved for self-supervised learning--substantially improves sample efficiency over random sampling for multi-teacher distillation. By combining these findings, we curate OpenLVD200M, a 200M-image corpus that demonstrates superior efficiency for multi-teacher distillation. Instantiated in a Mixture-of-Experts. We release OpenLVD200M and distilled models.

</details>


### [39] [Generative Latent Coding for Ultra-Low Bitrate Image Compression](https://arxiv.org/abs/2512.20194)
*Zhaoyang Jia,Jiahao Li,Bin Li,Houqiang Li,Yan Lu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的图像压缩架构GLC，通过在生成式VQ-VAE的潜在空间中进行变换编码，实现低码率下的高保真和高真实感压缩，优于传统像素域方法。


<details>
  <summary>Details</summary>
Motivation: 传统的图像压缩在像素空间进行变换编码，但在低码率下难以同时达到高真实感和高保真，因为像素空间的失真度量不符合人类感知。论文希望从更符合人类视觉系统的角度改善压缩质量。

Method: 提出Generative Latent Coding（GLC）架构，不在像素空间，而是在生成式VQ-VAE的稀疏、具语义的潜在空间进行编码。引入了分类超模块（categorical hyper module）降低超信息的比特消耗，同时用基于码预测的监督增强语义一致性。

Result: GLC在自然图像低于0.04 bpp、面部图像低于0.01 bpp时保持高视觉质量。在CLIC2020测试集上，比MS-ILLM用更少45%的比特达到相同FID。

Conclusion: GLC在低码率下有效实现高保真和真实感，且生成式潜在空间还支持图像修复、风格迁移等应用，具有广泛的扩展潜力。

Abstract: Most existing image compression approaches perform transform coding in the pixel space to reduce its spatial redundancy. However, they encounter difficulties in achieving both high-realism and high-fidelity at low bitrate, as the pixel-space distortion may not align with human perception. To address this issue, we introduce a Generative Latent Coding (GLC) architecture, which performs transform coding in the latent space of a generative vector-quantized variational auto-encoder (VQ-VAE), instead of in the pixel space. The generative latent space is characterized by greater sparsity, richer semantic and better alignment with human perception, rendering it advantageous for achieving high-realism and high-fidelity compression. Additionally, we introduce a categorical hyper module to reduce the bit cost of hyper-information, and a code-prediction-based supervision to enhance the semantic consistency. Experiments demonstrate that our GLC maintains high visual quality with less than 0.04 bpp on natural images and less than 0.01 bpp on facial images. On the CLIC2020 test set, we achieve the same FID as MS-ILLM with 45% fewer bits. Furthermore, the powerful generative latent space enables various applications built on our GLC pipeline, such as image restoration and style transfer. The code is available at https://github.com/jzyustc/GLC.

</details>


### [40] [JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement](https://arxiv.org/abs/2512.20213)
*Tao Ye,Hongbin Ren,Chongbing Zhang,Haoran Chen,Xiaosong Li*

Main category: cs.CV

TL;DR: 本文提出了一种应对水下图像多重复杂退化的新方法JDPNet，通过联合特征挖掘和自适应损失平衡，显著提升了水下图像增强效果。


<details>
  <summary>Details</summary>
Motivation: 当前水下图像常见多种退化现象，这些退化以非线性的方式相互耦合，给图像增强带来巨大挑战。现有方法多针对单一退化设计，难以整体处理耦合退化带来的复杂性。

Method: 提出JDPNet联合退化处理网络，引入联合特征挖掘模块和概率引导分布策略，有效统一挖掘和调节多重退化特征。同时设计了AquaBalanceLoss损失函数，平衡色彩、清晰度和对比度，从多退化损失联合指导网络学习。

Result: 在六个公开水下数据集和两个新构建数据集上，JDPNet在效果、模型体积和计算复杂度间取得了最佳平衡，达到了当前最优性能。

Conclusion: JDPNet能有效处理非线性复杂耦合退化，提升水下图像质量，具有广泛的应用前景和推广价值。

Abstract: Given the complexity of underwater environments and the variability of water as a medium, underwater images are inevitably subject to various types of degradation. The degradations present nonlinear coupling rather than simple superposition, which renders the effective processing of such coupled degradations particularly challenging. Most existing methods focus on designing specific branches, modules, or strategies for specific degradations, with little attention paid to the potential information embedded in their coupling. Consequently, they struggle to effectively capture and process the nonlinear interactions of multiple degradations from a bottom-up perspective. To address this issue, we propose JDPNet, a joint degradation processing network, that mines and unifies the potential information inherent in coupled degradations within a unified framework. Specifically, we introduce a joint feature-mining module, along with a probabilistic bootstrap distribution strategy, to facilitate effective mining and unified adjustment of coupled degradation features. Furthermore, to balance color, clarity, and contrast, we design a novel AquaBalanceLoss to guide the network in learning from multiple coupled degradation losses. Experiments on six publicly available underwater datasets, as well as two new datasets constructed in this study, show that JDPNet exhibits state-of-the-art performance while offering a better tradeoff between performance, parameter size, and computational cost.

</details>


### [41] [LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation](https://arxiv.org/abs/2512.20217)
*Xiangxuan Ren,Zhongdao Wang,Pin Tang,Guoqing Wang,Jilai Zheng,Chao Ma*

Main category: cs.CV

TL;DR: LiteFusion提出了一种无需3D骨干网络的多模态3D目标检测方法，有效增强了仅基于相机的检测器，同时大幅提升了部署友好性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态三维检测器高度依赖于LiDAR及复杂的3D骨干网络，导致在未配备LiDAR或特殊硬件平台（如NPU，FPGA）时性能下滑，限制了实际部署价值和安全性。

Method: LiteFusion摒弃独立的3D特征提取网络，将LiDAR点云作为几何辅助信息，通过四元数空间将其集成到图像特征中，强化相机特征表达且保持模态正交约束，实现轻量化的跨模态融合。

Result: 在nuScenes数据集上，LiteFusion在不引入专用LiDAR编码器、参数仅增加1.1%的情况下，实现了mAP提升20.4%，NDS提升19.7%；即使无LiDAR输入时依然可以保持较强性能。

Conclusion: LiteFusion显著提高了基于视觉的检测准确性和多模态鲁棒性，极大简化了部署要求，是一种优异而高效的3D目标检测方案。

Abstract: 3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.

</details>


### [42] [IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing](https://arxiv.org/abs/2512.20236)
*Oikantik Nath,Sahithi Kukkala,Mitesh Khapra,Ravi Kiran Sarvadevabhatla*

Main category: cs.CV

TL;DR: 本文介绍了IndicDLP，多语言、多领域的大规模文档版面分析数据集，专注于填补现有数据集在地区、多样性和精细标注上的空白。


<details>
  <summary>Details</summary>
Motivation: 现有大型文档布局数据集（如PubLayNet、DocBank）缺乏细粒度标签和多语言样本，而人工标注数据集规模过小且语种覆盖有限，特别是印度地区文档显著缺失，阻碍了多语种及多域文档理解和数字化进展。

Method: 提出IndicDLP数据集，涵盖11种印度代表性语种及英语和12类常见文档类型。同时，基于DocLayNet和M6Doc整理UED-mini数据集，以提升预训练基础。通过模型微调实验证明其有效性。

Result: 在IndicDLP数据集上微调现有英文模型后，相关指标大幅提升。此外，基于该数据集训练的模型具备良好泛化性能，适用于多样化文档布局。

Conclusion: IndicDLP在数据集规模、标注颗粒度和多语种多领域覆盖方面实现突破，为文档数字化和理解提供了强有力的多样数据基础，推动文档分析领域向更包容和高效发展。

Abstract: Document layout analysis is essential for downstream tasks such as information retrieval, extraction, OCR, and digitization. However, existing large-scale datasets like PubLayNet and DocBank lack fine-grained region labels and multilingual diversity, making them insufficient for representing complex document layouts. In contrast, human-annotated datasets such as M6Doc and D4LA offer richer labels and greater domain diversity, but are too small to train robust models and lack adequate multilingual coverage. This gap is especially pronounced for Indic documents, which encompass diverse scripts yet remain underrepresented in current datasets, further limiting progress in this space. To address these shortcomings, we introduce IndicDLP, a large-scale foundational document layout dataset spanning 11 representative Indic languages alongside English and 12 common document domains. Additionally, we curate UED-mini, a dataset derived from DocLayNet and M6Doc, to enhance pretraining and provide a solid foundation for Indic layout models. Our experiments demonstrate that fine-tuning existing English models on IndicDLP significantly boosts performance, validating its effectiveness. Moreover, models trained on IndicDLP generalize well beyond Indic layouts, making it a valuable resource for document digitization. This work bridges gaps in scale, diversity, and annotation granularity, driving inclusive and efficient document understanding.

</details>


### [43] [Degradation-Aware Metric Prompting for Hyperspectral Image Restoration](https://arxiv.org/abs/2512.20251)
*Binfeng Wang,Di Wang,Haonan Guo,Ying Fu,Jing Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一个统一的高光谱图像(HSI)复原框架DAMP，无需依赖难以获得的退化标签，通过自设计的劣化度量和专家混合架构，实现对多种复杂退化类型的自适应恢复，效果先进且泛化强。


<details>
  <summary>Details</summary>
Motivation: 现有HSI复原方法往往需要先验的退化标签作为提示，而实际中退化类型复杂且难以获取，限制了实际应用。解决无需显式退化标签、能适应多种退化类型的方法需求迫切。

Method: 作者提出Degradation-Aware Metric Prompting(DAMP)框架，基于自定义的空间-光谱退化度量，不需先验标签，通过度量得到的退化提示作为模型输入并在Mixture-of-Experts理论下，通过SSAM专家模块动态调整特征提取，实现对不同退化的自适应复原。

Result: DAMP在自然和遥感高光谱数据集上的大量实验证明取得了领先的复原表现，并在面对新型或复杂退化时表现出更强的泛化能力。

Conclusion: DAMP框架突破了对显示退化标签的依赖，实现了高效、鲁棒、泛化强的高光谱图像统一复原，对于实际复杂场景具备重要应用价值。

Abstract: Unified hyperspectral image (HSI) restoration aims to recover various degraded HSIs using a single model, offering great practical value. However, existing methods often depend on explicit degradation priors (e.g., degradation labels) as prompts to guide restoration, which are difficult to obtain due to complex and mixed degradations in real-world scenarios. To address this challenge, we propose a Degradation-Aware Metric Prompting (DAMP) framework. Instead of relying on predefined degradation priors, we design spatial-spectral degradation metrics to continuously quantify multi-dimensional degradations, serving as Degradation Prompts (DP). These DP enable the model to capture cross-task similarities in degradation distributions and enhance shared feature learning. Furthermore, we introduce a Spatial-Spectral Adaptive Module (SSAM) that dynamically modulates spatial and spectral feature extraction through learnable parameters. By integrating SSAM as experts within a Mixture-of-Experts architecture, and using DP as the gating router, the framework enables adaptive, efficient, and robust restoration under diverse, mixed, or unseen degradations. Extensive experiments on natural and remote sensing HSI datasets show that DAMP achieves state-of-the-art performance and demonstrates exceptional generalization capability. Code is publicly available at https://github.com/MiliLab/DAMP.

</details>


### [44] [BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation](https://arxiv.org/abs/2512.20255)
*Jinghao Shi,Jianing Song*

Main category: cs.CV

TL;DR: 该论文提出了一种用于高分辨率遥感图像语义分割的新方法BiCoR-Seg，通过引入热力图驱动的双向信息协同模块和分层监督策略，提高了分类分辨能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 高分遥感图像语义分割面临同类间相似度高、类内变化大等难题，现有方法难以将具有强判别力的语义知识有效注入像素级特征学习，导致边界模糊和类别混淆。论文旨在提升分割准确性和模型可解释性。

Method: 提出BiCoR-Seg框架，设计了热力图驱动的双向信息协同模块（HBIS），通过生成类别热力图在特征图与类别嵌入之间建立信息流；引入分层监督策略，利用各层HBIS生成的热力图直接进行低分辨率分割监督；此外，提出跨层类别嵌入Fisher判别损失，提升类内紧致性与类间可分性。

Result: 在LoveDA、Vaihingen和Potsdam等遥感数据集上的大量实验结果显示，BiCoR-Seg在分割性能及可解释性方面均优于现有方法。

Conclusion: BiCoR-Seg通过双向信息协同和分层监督有效提升遥感图像语义分割的精度和可解释性，在公开数据集上达到了先进水平。

Abstract: High-resolution remote sensing image semantic segmentation (HRSS) is a fundamental yet critical task in the field of Earth observation. However, it has long faced the challenges of high inter-class similarity and large intra-class variability. Existing approaches often struggle to effectively inject abstract yet strongly discriminative semantic knowledge into pixel-level feature learning, leading to blurred boundaries and class confusion in complex scenes. To address these challenges, we propose Bidirectional Co-Refinement Framework for HRSS (BiCoR-Seg). Specifically, we design a Heatmap-driven Bidirectional Information Synergy Module (HBIS), which establishes a bidirectional information flow between feature maps and class embeddings by generating class-level heatmaps. Based on HBIS, we further introduce a hierarchical supervision strategy, where the interpretable heatmaps generated by each HBIS module are directly utilized as low-resolution segmentation predictions for supervision, thereby enhancing the discriminative capacity of shallow features. In addition, to further improve the discriminability of the embedding representations, we propose a cross-layer class embedding Fisher Discriminative Loss to enforce intra-class compactness and enlarge inter-class separability. Extensive experiments on the LoveDA, Vaihingen, and Potsdam datasets demonstrate that BiCoR-Seg achieves outstanding segmentation performance while offering stronger interpretability. The released code is available at https://github.com/ShiJinghao566/BiCoR-Seg.

</details>


### [45] [LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation](https://arxiv.org/abs/2512.20257)
*Daniele Cardullo,Simone Teglia,Irene Amerini*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态虚假信息检测方法LADLE-MM，在有限标注和计算资源下也能高效工作，参数更少但准确率与最新方法相当。


<details>
  <summary>Details</summary>
Motivation: 随着多媒体生成和篡改工具的普及，通过多模态（如图像-文本对）传播的虚假信息日益成为威胁，现有检测方法通常需要大量计算资源或标注数据。作者希望解决这一资源消耗高和数据需求量大的问题。

Method: LADLE-MM包含两个单模态分支和一个多模态分支，多模态分支利用BLIP提取的嵌入作为固定参考空间进行特征增强。模型采用模型组合法进行初始化，即所谓“model-soup”方法，以提升性能和泛化能力。此外，LADLE-MM专为有限标注和有限训练资源环境设计，显著减少了可训练参数数量。

Result: LADLE-MM在DGM4基准测试集上实现了与现有最高水平相当的性能，尤其在没有依赖grounding标注数据训练时表现优越。它使用的可训练参数比之前的方法少60.3%。在VERITE数据集上，LADLE-MM还超越了采用大型视觉语言模型的复杂架构方法，显示出良好的泛化能力和对单模态偏差的鲁棒性。

Conclusion: LADLE-MM证明了在有限资源场景下，多模态虚假信息检测可以在减少模型复杂度的同时实现高准确性和强泛化能力，对实际应用具有重要意义。

Abstract: With the rise of easily accessible tools for generating and manipulating multimedia content, realistic synthetic alterations to digital media have become a widespread threat, often involving manipulations across multiple modalities simultaneously. Recently, such techniques have been increasingly employed to distort narratives of important events and to spread misinformation on social media, prompting the development of misinformation detectors. In the context of misinformation conveyed through image-text pairs, several detection methods have been proposed. However, these approaches typically rely on computationally intensive architectures or require large amounts of annotated data. In this work we introduce LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation, a model-soup initialized multimodal misinformation detector designed to operate under a limited annotation setup and constrained training resources. LADLE-MM is composed of two unimodal branches and a third multimodal one that enhances image and text representations with additional multimodal embeddings extracted from BLIP, serving as fixed reference space. Despite using 60.3% fewer trainable parameters than previous state-of-the-art models, LADLE-MM achieves competitive performance on both binary and multi-label classification tasks on the DGM4 benchmark, outperforming existing methods when trained without grounding annotations. Moreover, when evaluated on the VERITE dataset, LADLE-MM outperforms current state-of-the-art approaches that utilize more complex architectures involving Large Vision-Language-Models, demonstrating the effective generalization ability in an open-set setting and strong robustness to unimodal bias.

</details>


### [46] [${D}^{3}${ETOR}: ${D}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive ${D}$ebiasing for Weakly-Supervised Camouflaged Object ${D}$etection with Scribble Annotations](https://arxiv.org/abs/2512.20260)
*Jiawei Ge,Jiuxin Cao,Xinyi Li,Xuelin Zhu,Chang Liu,Bo Liu,Chen Feng,Ioannis Patras*

Main category: cs.CV

TL;DR: 本文提出了一种新的弱监督伪装物体检测（WSCOD）框架${D}^{3}$ETOR，在只用稀疏监督（如涂鸦标注）的情况下大幅提升了伪装物体检测的效果，缩小了与全监督方法之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有的弱监督伪装物体检测方法主要存在两大问题：（1）基于通用分割模型（如SAM）生成的伪标签准确性不高，缺乏针对伪装目标的语义理解；（2）涂鸦类监督存在固有的标注偏差，导致模型难以捕捉目标的全局结构，影响最终准确性。因此，需要新的方法提升伪标签质量并减少监督偏差。

Method: ${D}^{3}$ETOR提出了两阶段流程：第一阶段通过自适应熵驱动的点采样和多代理讨论机制，提升SAM等模型在伪装检测中的伪标签质量和解释性；第二阶段设计了FADeNet网络，通过多层级频域特征融合，实现全局-局部平衡，同时动态调整区域监督强度，减轻涂鸦偏差。全流程联合利用伪标签和涂鸦语义信息。

Result: 在多个基准数据集上，${D}^{3}$ETOR取得了目前弱监督伪装物体检测领域的最新最优成绩（state-of-the-art），显著缩小了与全监督方法之间的性能差距。

Conclusion: ${D}^{3}$ETOR有效提升了弱监督伪装物体检测的性能，证明了增强伪标签机制及频率感知去偏方案的有效性，为更高效低成本的伪装目标检测提供了新方向。

Abstract: Weakly-Supervised Camouflaged Object Detection (WSCOD) aims to locate and segment objects that are visually concealed within their surrounding scenes, relying solely on sparse supervision such as scribble annotations. Despite recent progress, existing WSCOD methods still lag far behind fully supervised ones due to two major limitations: (1) the pseudo masks generated by general-purpose segmentation models (e.g., SAM) and filtered via rules are often unreliable, as these models lack the task-specific semantic understanding required for effective pseudo labeling in COD; and (2) the neglect of inherent annotation bias in scribbles, which hinders the model from capturing the global structure of camouflaged objects. To overcome these challenges, we propose ${D}^{3}$ETOR, a two-stage WSCOD framework consisting of Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing. In the first stage, we introduce an adaptive entropy-driven point sampling method and a multi-agent debate mechanism to enhance the capability of SAM for COD, improving the interpretability and precision of pseudo masks. In the second stage, we design FADeNet, which progressively fuses multi-level frequency-aware features to balance global semantic understanding with local detail modeling, while dynamically reweighting supervision strength across regions to alleviate scribble bias. By jointly exploiting the supervision signals from both the pseudo masks and scribble semantics, ${D}^{3}$ETOR significantly narrows the gap between weakly and fully supervised COD, achieving state-of-the-art performance on multiple benchmarks.

</details>


### [47] [UbiQVision: Quantifying Uncertainty in XAI for Image Recognition](https://arxiv.org/abs/2512.20288)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.CV

TL;DR: 本文针对医用图像深度学习模型的解释性问题，提出利用Dirichlet后验采样和Dempster-Shafer理论，对SHAP可解释性方法的不确定性进行量化，并在三个不同类型的医学影像数据集上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学影像分析领域取得了显著进展，但由于模型复杂度提高，模型的可解释性和可理解性受到挑战。其中，SHAP作为常用可解释工具，在面对不确定性时表现不稳定，影响领域专家对模型决策的信任。本文旨在提升SHAP解释结果的可靠性与可信度。

Method: 该方法采用Dirichlet后验采样和Dempster-Shafer证据理论，通过信念图、似然图与融合图的方式，对SHAP解释中由认知性（epistemic）与随机性（aleatoric）不确定性引入的解释不稳定性进行统计量化分析。

Result: 作者在三个具有不同类别分布、图像质量及成像模态的医学影像数据集（涉及病理学、眼科学、放射学）上评估了该框架，证实其能有效量化并揭示SHAP解释的不确定性。

Conclusion: 研究为深度学习医学影像模型的解释性增强提供了新工具，有助于提升模型输出解释的稳定性与专家信任度，对高风险医学场景具有实际应用意义。

Abstract: Recent advances in deep learning have led to its widespread adoption across diverse domains, including medical imaging. This progress is driven by increasingly sophisticated model architectures, such as ResNets, Vision Transformers, and Hybrid Convolutional Neural Networks, that offer enhanced performance at the cost of greater complexity. This complexity often compromises model explainability and interpretability. SHAP has emerged as a prominent method for providing interpretable visualizations that aid domain experts in understanding model predictions. However, SHAP explanations can be unstable and unreliable in the presence of epistemic and aleatoric uncertainty. In this study, we address this challenge by using Dirichlet posterior sampling and Dempster-Shafer theory to quantify the uncertainty that arises from these unstable explanations in medical imaging applications. The framework uses a belief, plausible, and fusion map approach alongside statistical quantitative analysis to produce quantification of uncertainty in SHAP. Furthermore, we evaluated our framework on three medical imaging datasets with varying class distributions, image qualities, and modality types which introduces noise due to varying image resolutions and modality-specific aspect covering the examples from pathology, ophthalmology, and radiology, introducing significant epistemic uncertainty.

</details>


### [48] [TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation](https://arxiv.org/abs/2512.20296)
*Ji-Hoon Kim,Junseok Ahn,Doyeop Kwak,Joon Son Chung,Shinji Watanabe*

Main category: cs.CV

TL;DR: 本文提出了一种名为TAVID的统一框架，从文本和参考图像联合生成互动视频与会话语音，实现面部与语音的同步生成。


<details>
  <summary>Details</summary>
Motivation: 尽管以往研究分别关注口型生成或听众头像生成与会话语音合成，但这些方法通常孤立研究，没有考虑人类对话中紧密耦合的多模态音视频交互。

Method: TAVID框架通过两个跨模态映射器（动作映射器与说话者映射器），实现面部生成管线和语音生成管线的信息互通，达成音视频的同步与互补。

Result: 在口型真实感、听众头像响应性、双边互动流畅度与语音质量四个维度进行了系统性评估，实验表明TAVID在各方面表现优异。

Conclusion: TAVID能够有效同步生成互动人脸和会话语音，为制造更加拟人的多模态对话系统迈出了重要一步。

Abstract: The objective of this paper is to jointly synthesize interactive videos and conversational speech from text and reference images. With the ultimate goal of building human-like conversational systems, recent studies have explored talking or listening head generation as well as conversational speech generation. However, these works are typically studied in isolation, overlooking the multimodal nature of human conversation, which involves tightly coupled audio-visual interactions. In this paper, we introduce TAVID, a unified framework that generates both interactive faces and conversational speech in a synchronized manner. TAVID integrates face and speech generation pipelines through two cross-modal mappers (i.e., a motion mapper and a speaker mapper), which enable bidirectional exchange of complementary information between the audio and visual modalities. We evaluate our system across four dimensions: talking face realism, listening head responsiveness, dyadic interaction fluency, and speech quality. Extensive experiments demonstrate the effectiveness of our approach across all these aspects.

</details>


### [49] [The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection](https://arxiv.org/abs/2512.20340)
*Qingdong He,Xueqin Chen,Yanjie Pan,Peng Tang,Pengcheng Xu,Zhenye Gan,Chengjie Wang,Xiaobin Hu,Jiangning Zhang,Yabiao Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种用于视频虚拟试穿（VVT）的新方法KeyTailor，并发布了高质量大规模视频数据集ViT-HD。KeyTailor通过关键帧驱动的细节注入机制，有效提升了服装动态捕捉和背景一致性，并且改进了计算效率。实验结果表明新方法在服装逼真度和背景完整性方面均超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的VVT方法难以同时捕捉细致服装动态和保证背景一致性，并因复杂交互模块导致计算开销大。此外，公开数据集规模和质量有限，影响了模型泛化与训练效果。因此，亟需一种高效、细致且背景完整的VVT新方案以及更好的数据支持。

Method: 作者提出KeyTailor框架，核心为关键帧驱动的细节注入策略。首先通过指令引导的关键帧筛选，选出信息量高的帧。随后使用两个模块：服装细节增强模块和协作背景优化模块，分别强化服装动态和优化背景一致性，并将这些关键信息注入到标准DiT模块中。此外，提出高清高质量数据集ViT-HD，包含15,070个810*1080分辨率视频样本，涵盖多样服装。

Result: KeyTailor在实验中，在服装细节还原和背景一致性上效果明显优于主流方法，在动态及静态场景均有突出表现。

Conclusion: KeyTailor在不增加DiT结构复杂度的前提下，有效提升了VVT任务的服装动态和背景一致性表现，同时提升了计算效率，并配套高质量数据集，有助于推动领域发展。

Abstract: Although diffusion transformer (DiT)-based video virtual try-on (VVT) has made significant progress in synthesizing realistic videos, existing methods still struggle to capture fine-grained garment dynamics and preserve background integrity across video frames. They also incur high computational costs due to additional interaction modules introduced into DiTs, while the limited scale and quality of existing public datasets also restrict model generalization and effective training. To address these challenges, we propose a novel framework, KeyTailor, along with a large-scale, high-definition dataset, ViT-HD. The core idea of KeyTailor is a keyframe-driven details injection strategy, motivated by the fact that keyframes inherently contain both foreground dynamics and background consistency. Specifically, KeyTailor adopts an instruction-guided keyframe sampling strategy to filter informative frames from the input video. Subsequently,two tailored keyframe-driven modules, the garment details enhancement module and the collaborative background optimization module, are employed to distill garment dynamics into garment-related latents and to optimize the integrity of background latents, both guided by keyframes.These enriched details are then injected into standard DiT blocks together with pose, mask, and noise latents, enabling efficient and realistic try-on video synthesis. This design ensures consistency without explicitly modifying the DiT architecture, while simultaneously avoiding additional complexity. In addition, our dataset ViT-HD comprises 15, 070 high-quality video samples at a resolution of 810*1080, covering diverse garments. Extensive experiments demonstrate that KeyTailor outperforms state-of-the-art baselines in terms of garment fidelity and background integrity across both dynamic and static scenarios.

</details>


### [50] [CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation](https://arxiv.org/abs/2512.20362)
*V. Kovalev,A. Kuvshinov,A. Buzovkin,D. Pokidov,D. Timonin*

Main category: cs.CV

TL;DR: 提出了一种名为CRAFT的新型推理方法，在不需重新训练模型的前提下，通过结构化的推理和反馈提升文生图准确性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有文生图推理提升方法多为隐式、整体批判或无约束的prompt重写，这导致其不可解释、难以控制且难以稳定中止。而大语言模型中显式、结构化思考（如验证、定向修正和早停）在推理效果上展现了优势，作者希望将类似理念引入多模态生成任务。

Method: 提出CRAFT框架，将待生成内容的prompt分解为带依赖结构的视觉问题，并用视觉-语言模型对生成图片进行判别。仅在约束未满足的地方，通过LLM agent定向优化prompt，反复迭代，直到所有约束都满足为止，整个流程明确且可外部控制。

Result: 在多种生成模型和复杂基准下，CRAFT显著提升组合准确率、文本渲染和偏好度，尤其在轻量级生成器中提升更为明显。且推理时间开销极小，使得小模型生成效果接近高价大型模型。

Conclusion: 结论表明，显式结构化、基于约束的推理流程是提升多模态生成模型可靠性的关键手段。

Abstract: Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.
  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.
  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.

</details>


### [51] [Linking Faces and Voices Across Languages: Insights from the FAME 2026 Challenge](https://arxiv.org/abs/2512.20376)
*Marta Moscati,Ahmed Abdullah,Muhammad Saad Saeed,Shah Nawaz,Rohan Kumar Das,Muhammad Zaigham Zaheer,Junaid Mir,Muhammad Haroon Yousaf,Khalid Mahmood Malik,Markus Schedl*

Main category: cs.CV

TL;DR: FAME 2026 挑战关注于多语言环境下的人脸-声音关联方法，强调测试语言与训练语言不同时的适应性。


<details>
  <summary>Details</summary>
Motivation: 全球超过一半人口为双语或多语种者，实际交流环境常存在多语言切换。目前的人脸-声音关联方法多局限于单一语言，缺乏跨语言泛化能力。FAME 2026 希望推动在多语言背景下的技术发展。

Method: 本挑战鼓励参赛者提出能适应训练语言与测试语言不同的人脸-声音关联技术，虽然摘要中未详细描述具体方法，但暗示主要任务是提升模型跨语言鲁棒性。

Result: 摘要仅为竞赛简要介绍，未包含具体实验结果或方法创新。

Conclusion: FAME 2026 旨在提升多语言环境下人脸与声音关联的研究水平，推动更具泛化能力的方法发展。

Abstract: Over half of the world's population is bilingual and people often communicate under multilingual scenarios. The Face-Voice Association in Multilingual Environments (FAME) 2026 Challenge, held at ICASSP 2026, focuses on developing methods for face-voice association that are effective when the language at test-time is different than the training one. This report provides a brief summary of the challenge.

</details>


### [52] [SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images](https://arxiv.org/abs/2512.20377)
*Linfei Li,Lin Zhang,Zhong Wang,Ying Shen*

Main category: cs.CV

TL;DR: SmartSplat 是一种新颖的、高度自适应且具特征感知的基于高斯溅射的图像压缩框架，能够高效地对超高分辨率图像进行压缩与还原，在强压缩条件下也能保留高还原质量，并优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 生成式 AI 推动了超高分辨率视觉内容的产生，但此类内容对高效压缩与终端实时解码提出了巨大挑战。现有基于高斯模型的图像压缩方法在超高分辨率下，难以平衡压缩率与还原质量。

Method: 提出 SmartSplat 框架，结合图像的梯度和颜色方差信息，设计了梯度-颜色引导的变分采样（Gradient-Color Guided Variational Sampling）与排除式均匀采样（Exclusion-based Uniform Sampling），以提升像素空间内高斯基元的无重叠覆盖率。还提出了尺度自适应的高斯颜色采样，联合优化高斯空间布局、尺度与颜色初始化。

Result: 在 DIV8K 和新构建的 16K 数据集上广泛实验，SmartSplat 在相同压缩比条件下，始终优于最新主流方法，并突破了它们的压缩极限，展现出强适用性和可扩展性。

Conclusion: SmartSplat 实现了超高分辨率图像在高效压缩与高还原质量上的新突破，对终端设备和大规模视觉内容存储与传输具有实际应用价值。

Abstract: Recent advances in generative AI have accelerated the production of ultra-high-resolution visual content, posing significant challenges for efficient compression and real-time decoding on end-user devices. Inspired by 3D Gaussian Splatting, recent 2D Gaussian image models improve representation efficiency, yet existing methods struggle to balance compression ratio and reconstruction fidelity in ultra-high-resolution scenarios. To address this issue, we propose SmartSplat, a highly adaptive and feature-aware GS-based image compression framework that supports arbitrary image resolutions and compression ratios. SmartSplat leverages image-aware features such as gradients and color variances, introducing a Gradient-Color Guided Variational Sampling strategy together with an Exclusion-based Uniform Sampling scheme to improve the non-overlapping coverage of Gaussian primitives in pixel space. In addition, we propose a Scale-Adaptive Gaussian Color Sampling method to enhance color initialization across scales. Through joint optimization of spatial layout, scale, and color initialization, SmartSplat efficiently captures both local structures and global textures using a limited number of Gaussians, achieving high reconstruction quality under strong compression. Extensive experiments on DIV8K and a newly constructed 16K dataset demonstrate that SmartSplat consistently outperforms state-of-the-art methods at comparable compression ratios and exceeds their compression limits, showing strong scalability and practical applicability. The code is publicly available at https://github.com/lif314/SmartSplat.

</details>


### [53] [DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning](https://arxiv.org/abs/2512.20409)
*Junho Yoon,Jaemo Jung,Hyunju Kim,Dongman Lee*

Main category: cs.CV

TL;DR: 本文提出了一种适用于第三视角摄像头与环境传感器的对齐方法，解决了全局对齐在捕捉细节和时空背景区分上的局限，并显著提升了下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有通过全局特征对齐头戴摄像头与可穿戴传感器的方法，在实际应用中容易受用户舒适度、隐私和可扩展性限制。采用第三视角和环境传感器，能避免这类问题，但现有全局对齐方式在这一场景下难以捕获局部细节，且容易混淆具有相似时间模式但空间语义不同的动作。

Method: 提出 DETACH 框架。首先将特征在时空维度上解耦，利用在线聚类获得具有语义嵌入的传感特征。框架分两步实现对齐：1）通过互监督建立空间对应关系；2）采用时空加权对比损失进行时间轴对齐，有效区分难负样本、易负样本和伪负样本。

Result: 在 Opportunity++ 和 HWU-USP 两个数据集上的多个下游任务中，DETACH 明显优于经过改造的头戴-可穿戴对齐基线方法。

Conclusion: DETACH 有效解决了第三视角-环境对齐过程中的细节捕获、上下文混淆问题，提升了实际应用下的人体动作识别效果，具备更好的可扩展性和非侵入性。

Abstract: Aligning egocentric video with wearable sensors have shown promise for human action recognition, but face practical limitations in user discomfort, privacy concerns, and scalability. We explore exocentric video with ambient sensors as a non-intrusive, scalable alternative. While prior egocentric-wearable works predominantly adopt Global Alignment by encoding entire sequences into unified representations, this approach fails in exocentric-ambient settings due to two problems: (P1) inability to capture local details such as subtle motions, and (P2) over-reliance on modality-invariant temporal patterns, causing misalignment between actions sharing similar temporal patterns with different spatio-semantic contexts. To resolve these problems, we propose DETACH, a decomposed spatio-temporal framework. This explicit decomposition preserves local details, while our novel sensor-spatial features discovered via online clustering provide semantic grounding for context-aware alignment. To align the decomposed features, our two-stage approach establishes spatial correspondence through mutual supervision, then performs temporal alignment via a spatial-temporal weighted contrastive loss that adaptively handles easy negatives, hard negatives, and false negatives. Comprehensive experiments with downstream tasks on Opportunity++ and HWU-USP datasets demonstrate substantial improvements over adapted egocentric-wearable baselines.

</details>


### [54] [Chain-of-Anomaly Thoughts with Large Vision-Language Models](https://arxiv.org/abs/2512.20417)
*Pedro Domingos,João Pereira,Vasco Lopes,João Neves,David Semedo*

Main category: cs.CV

TL;DR: 提出了一种名为Chain-of-Anomaly-Thoughts (CoAT) 的多智能体推理框架，通过引入异常（犯罪）偏置显著提升大规模视觉语言模型在视频监控中的异常检测与分类能力。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉语言模型在自动视频监控任务中，容易因偏向“常规”而漏检犯罪异常事件，且现有链式推理方法（CoT）也倾向于常规解释。作者希望解决模型捕捉异常、犯罪行为的能力不足问题。

Method: 提出了Chain-of-Anomaly-Thoughts (CoAT) 框架，在多智能体推理流程中加入了诱导刑事（异常）偏置的分类层，使模型推理过程更关注异常事件。该框架利用异常优先分类层来引导推理方向，从而有效提升异常检测能力。

Result: 在低分辨率视频异常检测任务上，F1分数提升了11.8个百分点；在高分辨率视频的异常分类任务中，F1分数提升了3.78个百分点，均显著优于传统方法。

Conclusion: 通过引入异常/犯罪偏置的多智能体推理框架，CoAT显著提升了大模型在视频异常检测与分类任务上的性能，为自动化视频监控系统提供了更有效的解决方案。

Abstract: Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.

</details>


### [55] [Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks](https://arxiv.org/abs/2512.20431)
*Abdullah Al Shafi,Abdul Muntakim,Pintu Chandra Shill,Rowzatul Zannat,Abdullah Al-Amin*

Main category: cs.CV

TL;DR: 本文提出了一种基于CNN集成的皮肤癌早期分类方法，利用多数据集验证其较高的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌早期检测能显著提高生存率，但传统人工方法存在局限，AI方法有望提升早诊断准确性。

Method: 利用HAM10000、ISIC 2016、ISIC 2019三大公开数据集，通过数据重采样、增强及滤波处理，然后采用迁移学习下的混合双编码器实现分割，之后将结果输入MobileNetV2、VGG19和InceptionV3三种CNN网络组成软投票集成模型进行分类。

Result: 提出的方法在三个数据集上的皮损识别准确率分别达到96.32%、90.86%和93.92%，各项皮损检测指标表现优异。

Conclusion: 本方法能有效提升皮肤癌分类准确性，具有较好的实际部署价值。

Abstract: Skin cancer can be identified by dermoscopic examination and ocular inspection, but early detection significantly increases survival chances. Artificial intelligence (AI), using annotated skin images and Convolutional Neural Networks (CNNs), improves diagnostic accuracy. This paper presents an early skin cancer classification method using a soft voting ensemble of CNNs. In this investigation, three benchmark datasets, namely HAM10000, ISIC 2016, and ISIC 2019, were used. The process involved rebalancing, image augmentation, and filtering techniques, followed by a hybrid dual encoder for segmentation via transfer learning. Accurate segmentation focused classification models on clinically significant features, reducing background artifacts and improving accuracy. Classification was performed through an ensemble of MobileNetV2, VGG19, and InceptionV3, balancing accuracy and speed for real-world deployment. The method achieved lesion recognition accuracies of 96.32\%, 90.86\%, and 93.92\% for the three datasets. The system performance was evaluated using established skin lesion detection metrics, yielding impressive results.

</details>


### [56] [High Dimensional Data Decomposition for Anomaly Detection of Textured Images](https://arxiv.org/abs/2512.20432)
*Ji Song,Xing Wang,Jianguo Wu,Xiaowei Yue*

Main category: cs.CV

TL;DR: 本文提出了一种新方法TBSD，用于高效检测带有平滑背景和稀疏异常的纹理图像缺陷，解决了传统方法误判率高、鲁棒性差以及对大规模结构化数据依赖强的问题，在仿真和真实数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的图像异常检测方法在处理带纹理缺陷的图片时，常出现误识别、鲁棒性低、且对海量结构化数据依赖严重，不适合实际制造业等高维复杂场景。作者希望提出一个既能准确检测异常、又能减少对数据规模依赖的新方法。

Method: 提出纹理基集整合下的平滑分解（TBSD）方法，包括两大流程：首先学习纹理基函数，有效提取准周期性的纹理模式；随后利用学到的纹理基作为先验，进行异常检测，以避免对正常纹理的误判并高效识别异常。提出了准周期性纹理估计的数学建模及理论分析。

Result: TBSD方法在仿真和真实数据集上均表现更好，异常检测精度高、误判率低，并且相较于现有主流方法，训练数据集需求量更小。

Conclusion: TBSD是一种高效且稳健的纹理异常检测方法，在纹理异常检测任务中具备较高的实用价值和推广前景，并明显优于传统方法。

Abstract: In the realm of diverse high-dimensional data, images play a significant role across various processes of manufacturing systems where efficient image anomaly detection has emerged as a core technology of utmost importance. However, when applied to textured defect images, conventional anomaly detection methods have limitations including non-negligible misidentification, low robustness, and excessive reliance on large-scale and structured datasets. This paper proposes a texture basis integrated smooth decomposition (TBSD) approach, which is targeted at efficient anomaly detection in textured images with smooth backgrounds and sparse anomalies. Mathematical formulation of quasi-periodicity and its theoretical properties are investigated for image texture estimation. TBSD method consists of two principal processes: the first process learns the texture basis functions to effectively extract quasi-periodic texture patterns; the subsequent anomaly detection process utilizes that texture basis as prior knowledge to prevent texture misidentification and capture potential anomalies with high accuracy.The proposed method surpasses benchmarks with less misidentification, smaller training dataset requirement, and superior anomaly detection performance on both simulation and real-world datasets.

</details>


### [57] [Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding](https://arxiv.org/abs/2512.20451)
*Anh Dao,Manh Tran,Yufei Zhang,Xiaoming Liu,Zijun Cui*

Main category: cs.CV

TL;DR: 本研究将物理推断的关节力引入现有的人体运动理解任务，实验证明力学线索能提升识别、动作分析与视频描述等多项任务表现。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉方法在人体运动理解如识别、跟踪和描述等任务中取得了进展，但大多数方法忽视了关节驱动力等生物力学基础物理信息。作者希望探究在何时、何种情形下，推断到的“力”特征能增强运动理解能力。

Method: 作者将推断得到的力学信号嵌入到常规的人体运动理解流程中，对步态识别、动作识别和视频描述三大任务、八个公开数据集进行了系统实验。通过比较加入力信息前后的模型表现，评测其提升效果。

Result: 在八个基准数据集上，力信息的一致性引入带来了性能提升。如CASIA-B数据集的Rank-1步态识别精度从89.52%上升到90.39%，穿外套和侧视等困难条件下提升更显著。Gait3D从46%增至47.3%。动作识别里高强度类别提升最大（如打击动作+6.96%）。视频描述的ROUGE-L提升0.029，说明力学信息有助于时序定位与语义信息丰富。

Conclusion: 生物力学中的“力”特征能够有效补充视觉与运动学特征，在动态、遮挡或外观变化等复杂条件下显著增强人体运动理解相关任务的性能。

Abstract: Human motion understanding has advanced rapidly through vision-based progress in recognition, tracking, and captioning. However, most existing methods overlook physical cues such as joint actuation forces that are fundamental in biomechanics. This gap motivates our study: if and when do physically inferred forces enhance motion understanding? By incorporating forces into established motion understanding pipelines, we systematically evaluate their impact across baseline models on 3 major tasks: gait recognition, action recognition, and fine-grained video captioning. Across 8 benchmarks, incorporating forces yields consistent performance gains; for example, on CASIA-B, Rank-1 gait recognition accuracy improved from 89.52% to 90.39% (+0.87), with larger gain observed under challenging conditions: +2.7% when wearing a coat and +3.0% at the side view. On Gait3D, performance also increases from 46.0% to 47.3% (+1.3). In action recognition, CTR-GCN achieved +2.00% on Penn Action, while high-exertion classes like punching/slapping improved by +6.96%. Even in video captioning, Qwen2.5-VL's ROUGE-L score rose from 0.310 to 0.339 (+0.029), indicating that physics-inferred forces enhance temporal grounding and semantic richness. These results demonstrate that force cues can substantially complement visual and kinematic features under dynamic, occluded, or appearance-varying conditions.

</details>


### [58] [UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images](https://arxiv.org/abs/2512.20479)
*Yiming Zhao,Yuanpeng Gao,Yuxuan Luo,Jiwei Duan,Shisong Lin,Longfei Xiong,Zhouhui Lian*

Main category: cs.CV

TL;DR: 本文提出了UTDesign，一种面向图形设计中的高精度风格化文本编辑和条件文本生成的统一框架，支持中英文脚本，实现了文本风格迁移、条件文本生成及全自动文本到设计流程，在开源方法中取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型尽管在视觉内容生成上表现出色，但在小字体排版和非拉丁文字方面的文本渲染还存在显著局限，特别是在设计类应用对于风格一致性和多语种支持的需求下，开发更精准、灵活的AI文本编辑和生成工具成为迫切需求。

Method: 作者提出基于DiT结构的文本风格迁移模型，能生成带RGBA通道的透明文本前景，实现参考字形的风格转移。基于人工合成数据集从零开始训练，并通过多模态条件编码器进一步扩展为条件文本生成框架，可根据背景图像、文本提示和布局规范进行风格一致、精确的文本合成。最后融合预训练文本生成模型和多模态大模型布局规划，实现端到端的全自动文本到设计管线。

Result: 大量实验表明，UTDesign在风格一致性和文本准确性两个维度上均优于现有开源方法；同时与商业闭源方案相比，也展现出自身的独特优势。

Conclusion: UTDesign为AI辅助的设计文本生成与编辑提供了统一且高效的新范式，尤其提升了小尺寸与多语言文本渲染的质量，并具备良好的实际应用前景。

Abstract: AI-assisted graphic design has emerged as a powerful tool for automating the creation and editing of design elements such as posters, banners, and advertisements. While diffusion-based text-to-image models have demonstrated strong capabilities in visual content generation, their text rendering performance, particularly for small-scale typography and non-Latin scripts, remains limited. In this paper, we propose UTDesign, a unified framework for high-precision stylized text editing and conditional text generation in design images, supporting both English and Chinese scripts. Our framework introduces a novel DiT-based text style transfer model trained from scratch on a synthetic dataset, capable of generating transparent RGBA text foregrounds that preserve the style of reference glyphs. We further extend this model into a conditional text generation framework by training a multi-modal condition encoder on a curated dataset with detailed text annotations, enabling accurate, style-consistent text synthesis conditioned on background images, prompts, and layout specifications. Finally, we integrate our approach into a fully automated text-to-design (T2D) pipeline by incorporating pre-trained text-to-image (T2I) models and an MLLM-based layout planner. Extensive experiments demonstrate that UTDesign achieves state-of-the-art performance among open-source methods in terms of stylistic consistency and text accuracy, and also exhibits unique advantages compared to proprietary commercial approaches. Code and data for this paper are available at https://github.com/ZYM-PKU/UTDesign.

</details>


### [59] [Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems](https://arxiv.org/abs/2512.20487)
*James E. Gallagher,Edward J. Oughton,Jana Kosecka*

Main category: cs.CV

TL;DR: 本研究评估了基于无人机系统（UAS）的地表地雷检测，采用自适应RGB与长波红外（LWIR）图像融合，通过提升热对比度提取特征，并用YOLO系列等模型实现检测。


<details>
  <summary>Details</summary>
Motivation: 地雷仍然对人类安全构成巨大威胁，全球有1.1亿颗地雷，每年导致大量人员伤亡。当前检测技术效率低、风险大，因此需开发更高效、自动化的地雷检测方法。

Method: 利用自适应RGB与LWIR热融合技术增强地雷与背景土壤的特征差异，结合YOLO（v8/v10/v11）、RF-DETR、Faster R-CNN与RetinaNet等主流目标检测网络，在收集的多时序、不同温度条件下的图像数据上进行检测性能评估。

Result: YOLOv11在10-30%热融合、5-10米高度参数下mAP最高达86.8%。虽然RF-DETR检测准确率最高（69.2%mAP），但YOLOv11训练17.7倍更快。多时序数据训练提升了1.8-9.6%准确率。反坦克地雷检测准确率显著高于反人员地雷。

Conclusion: 自适应多模态融合结合高效深度网络可显著提升地雷检测性能，实现准确与效率间的平衡。未来需关注不同埋深、异质土壤下的热对比度及检测能力扩展。

Abstract: Landmines remain a persistent humanitarian threat, with 110 million actively deployed mines across 60 countries, claiming 26,000 casualties annually. This research evaluates adaptive Red-Green-Blue (RGB) and Long-Wave Infrared (LWIR) fusion for Unmanned Aerial Systems (UAS)-based detection of surface-laid landmines, leveraging the thermal contrast between the ordnance and the surrounding soil to enhance feature extraction. Using You Only Look Once (YOLO) architectures (v8, v10, v11) across 114 test images, generating 35,640 model-condition evaluations, YOLOv11 achieved optimal performance (86.8% mAP), with 10 to 30% thermal fusion at 5 to 10m altitude identified as the optimal detection parameters. A complementary architectural comparison revealed that while RF-DETR achieved the highest accuracy (69.2% mAP), followed by Faster R-CNN (67.6%), YOLOv11 (64.2%), and RetinaNet (50.2%), YOLOv11 trained 17.7 times faster than the transformer-based RF-DETR (41 minutes versus 12 hours), presenting a critical accuracy-efficiency tradeoff for operational deployment. Aggregated multi-temporal training datasets outperformed season-specific approaches by 1.8 to 9.6%, suggesting that models benefit from exposure to diverse thermal conditions. Anti-Tank (AT) mines achieved 61.9% detection accuracy, compared with 19.2% for Anti-Personnel (AP) mines, reflecting both the size differential and thermal-mass differences between these ordnance classes. As this research examined surface-laid mines where thermal contrast is maximized, future research should quantify thermal contrast effects for mines buried at varying depths across heterogeneous soil types.

</details>


### [60] [Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition](https://arxiv.org/abs/2512.20501)
*Gorjan Radevski*

Main category: cs.CV

TL;DR: 该论文系统性探讨了多模态对齐、翻译、融合与迁移技术，以提升计算机对复杂输入的分析与理解能力。涵盖空间推理、医学文本-空间映射、知识图谱抽取、多模态动作识别及知识蒸馏等任务，多角度推动多模态机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习系统在多模态复杂输入的理解方面存在局限，如空间语言理解、医学文本可视化、文本事实抽取及动作识别等，仍面临精度与解释性不足等难题。本文旨在通过多项创新方法，提升多模态数据的对齐、解码和迁移能力。

Method: 1. 提出Spatial-Reasoning Bert，将文本空间关系映射为2D视觉布局，实现自动场景生成。2. 设计空间共现损失函数，实现医学文本到三维解剖图谱的空间定位。3. 构建面向知识图谱的结构化文本翻译基准，解决实体与谓词歧义链接。4. 融合视频帧与目标检测信息提升动作识别鲁棒性。5. 实现多模态知识蒸馏，无需多模态输入即可复现高性能。

Result: 1. 成功将文本空间表述高效转码为视觉场景。2. 显著提升医学文本导航的空间精确性与可解释性。3. 有效减少文本-知识图谱对齐歧义。4. 融合方法提升动作识别准确率。5. 多模态蒸馏模型在计算量降低的同时性能接近原多模态模型。

Conclusion: 论文在空间理解、医学文本解读、知识抽取和动作识别等多领域实现多模态处理技术突破，推动复杂多模态数据的高效对齐、融合和迁移，为多模态机器学习系统实际应用落地提供了方法论支持。

Abstract: This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.
  Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.
  Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.
  Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.
  Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.
  Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.
  These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.

</details>


### [61] [SirenPose: Dynamic Scene Reconstruction via Geometric Supervision](https://arxiv.org/abs/2512.20531)
*Kaitong Cai,Jensen Zhang,Jing Yang,Keze Wang*

Main category: cs.CV

TL;DR: SirenPose通过结合正弦神经网络的周期激活特性与关键点几何监督，实现了单目视频中动态三维场景的高精度、时序一致重建，在多个 benchmark 上超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前三维重建方法在处理快速运动、多目标交互、遮挡及场景剧烈变化时，常面临运动保真度与时空一致性差的问题。该研究旨在提升动态3D场景重建的时空一致性及几何精度。

Method: 提出SirenPose，将正弦激活网络与关键点几何监督结合，并引入物理约束以增强时空一致性；利用高频信号建模捕获几何细节，并结合图神经网络建模关键点关系，还扩展了UniKPT数据集并在主流数据集上测试。

Result: 在Sintel、Bonn、DAVIS等多个benchmark进行大量实验，SirenPose在DAVIS测试中相较于MoSCA，FVD降低17.8%，FID降低28.7%，LPIPS提升6%，同时在时间一致性、几何精度、用户评分及运动平滑性上全面优于竞品。在姿态估计任务中，对比Monst3R也表现出更低的绝对轨迹误差和相对误差。

Conclusion: SirenPose有效提升了三维动态场景重建与姿态估计算法在高动态、复杂场景中的表现，显示其在运动保真、时空一致性和物理可行性方面的显著优势。

Abstract: We introduce SirenPose, a geometry-aware loss formulation that integrates the periodic activation properties of sinusoidal representation networks with keypoint-based geometric supervision, enabling accurate and temporally consistent reconstruction of dynamic 3D scenes from monocular videos. Existing approaches often struggle with motion fidelity and spatiotemporal coherence in challenging settings involving fast motion, multi-object interaction, occlusion, and rapid scene changes. SirenPose incorporates physics inspired constraints to enforce coherent keypoint predictions across both spatial and temporal dimensions, while leveraging high frequency signal modeling to capture fine grained geometric details. We further expand the UniKPT dataset to 600,000 annotated instances and integrate graph neural networks to model keypoint relationships and structural correlations. Extensive experiments on benchmarks including Sintel, Bonn, and DAVIS demonstrate that SirenPose consistently outperforms state-of-the-art methods. On DAVIS, SirenPose achieves a 17.8 percent reduction in FVD, a 28.7 percent reduction in FID, and a 6.0 percent improvement in LPIPS compared to MoSCA. It also improves temporal consistency, geometric accuracy, user score, and motion smoothness. In pose estimation, SirenPose outperforms Monst3R with lower absolute trajectory error as well as reduced translational and rotational relative pose error, highlighting its effectiveness in handling rapid motion, complex dynamics, and physically plausible reconstruction.

</details>


### [62] [AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment](https://arxiv.org/abs/2512.20538)
*Anna Šárová Mikeštíková,Médéric Fourmy,Martin Cífka,Josef Sivic,Vladimir Petrik*

Main category: cs.CV

TL;DR: 论文提出了一种新的多视角无需对象特定训练的6D姿态估计方法AlignPose，通过特定的特征度量优化方案，在多个数据集上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 单视角RGB对象姿态估计方法虽然泛化能力强，但受限于深度歧义、遮挡和环境杂乱等问题。多视角方法对此有潜在克服能力，但现有方法依赖精确的单视角输出或泛化不足。因此，急需一种无需对象特定训练且能有效融合多视角信息的方法。

Method: 提出AlignPose方法，一种面向6D对象姿态估计的多视角聚合方案。核心是新的多视角特征度量优化，通过最小化渲染特征与多视角观测特征的差异，直接在世界坐标优化一致的姿态估计；该方法不依赖对象特定训练和对称性标注。

Result: 在YCB-V、T-LESS、ITODD-MV、HouseCat6D等四个数据集上的BOP基准测试中，AlignPose在准确性上优于已发表的多视角方法，尤其在工业等多视角易获取场景表现突出。

Conclusion: AlignPose方法能够有效解决现有多视角姿态估计的泛化与依赖问题，为无对象特定训练场景下的多视角6D姿态估计提供了更优解，尤其适用于实际工业应用中。

Abstract: Single-view RGB model-based object pose estimation methods achieve strong generalization but are fundamentally limited by depth ambiguity, clutter, and occlusions. Multi-view pose estimation methods have the potential to solve these issues, but existing works rely on precise single-view pose estimates or lack generalization to unseen objects. We address these challenges via the following three contributions. First, we introduce AlignPose, a 6D object pose estimation method that aggregates information from multiple extrinsically calibrated RGB views and does not require any object-specific training or symmetry annotation. Second, the key component of this approach is a new multi-view feature-metric refinement specifically designed for object pose. It optimizes a single, consistent world-frame object pose minimizing the feature discrepancy between on-the-fly rendered object features and observed image features across all views simultaneously. Third, we report extensive experiments on four datasets (YCB-V, T-LESS, ITODD-MV, HouseCat6D) using the BOP benchmark evaluation and show that AlignPose outperforms other published methods, especially on challenging industrial datasets where multiple views are readily available in practice.

</details>


### [63] [Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios](https://arxiv.org/abs/2512.20556)
*Mingwei Tang,Jiahao Nie,Guang Yang,Ziqing Cui,Jie Li*

Main category: cs.CV

TL;DR: 本论文提出了一种多粒度文本引导的图像融合新方法（MTIF），融合多层次文本信息以提升图像融合效果，并在多种任务上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 传统图像融合方法在处理输入图像动态范围和焦点深度差异时面临挑战，单纯依赖粗粒度文本描述无法有效对齐视觉与文本信息，影响融合质量。因此亟需更细致、对齐更精确的多模态融合手段。

Method: 提出MTIF方法，包括三个关键设计：1）采用多粒度的文本描述（细节、结构、语义），通过分层跨模态调制模块引导图像融合；2）在每一粒度层次引入监督信号，促进视觉与文本特征的对齐和文本信息的利用；3）引入基于显著性的丰富模块，增强训练数据的语义信息，提高跨模态调制和对齐效果。

Result: 大量实验表明，所提出的MTIF方法在多曝光与多焦点图像融合任务中持续优于现有各类方法。

Conclusion: 多粒度的文本描述和分层引导机制能够更好地对齐和调动文本辅助信息，从而显著提升图像融合任务在各类挑战性条件下的表现。

Abstract: Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.

</details>


### [64] [Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models](https://arxiv.org/abs/2512.20557)
*Shengchao Zhou,Yuxin Chen,Yuying Ge,Wei Huang,Jiehong Lin,Ying Shan,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 本文提出了DSR Suite，包括自动生成4D动态空间推理数据集的方法和模型增强模块，有效提升了视觉-语言模型在3D动态空间推理任务上的能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在一般理解任务中表现优异，但在空间中对象几何和关系随时间演变的动态空间推理（DSR）能力欠缺，主要原因是缺乏大规模4D感知训练资源。为此，亟需在数据集、基准和模型架构层面改进以提升模型在DSR任务中的表现。

Method: 1）提出自动化流水线，从真实视频自动生成DSR相关的多项选择题数据，提取数据信息如摄像机姿态、点云、物体蒙版、朝向、三维轨迹等，用于DSR-Train（训练）和人工精炼的DSR-Bench（评测）。2）提出轻量几何选择模块（GSM），从4D重建先验中提取与问题相关的几何信息，以紧凑几何token形式集成到VLM中，避免提供过多无关信息。

Result: 在Qwen2.5-VL-7B模型中集成DSR-Train和GSM后，其动态空间推理能力显著提升，同时在通用视频理解基准上保持准确率不变。

Conclusion: DSR Suite有效填补了视觉-语言模型在4D动态空间推理领域的数据与建模空白，提出的数据集和模块为相关研究提供了重要资源，显著提升了主流VLM的动态空间推理能力。

Abstract: Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.

</details>


### [65] [FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models](https://arxiv.org/abs/2512.20561)
*Kaitong Cai,Jusheng Zhang,Jing Yang,Yijia Fan,Pengtao Xie,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: FlashVLM提出了一种文本引导的视觉token选择框架，能极大压缩视觉token数量，同时略优于现有未裁剪模型，在高压缩率下依然保持高准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视觉-语言模型（VLM）对每帧需要处理大量视觉token，计算开销高且存在冗余；已有的token压缩方法忽视文本查询或依赖不稳定的深层注意力机制，影响语义对齐与性能。

Method: 提出FlashVLM框架，通过计算视觉token与文本嵌入在语言模型空间的跨模态相似度，结合视觉显著性以选择最相关的token，并保留多样性的背景token维护全局信息。避免了仅依赖注意力权重的缺陷，实现了压缩与性能兼顾。

Result: 在相同token预算和评测方案下，FlashVLM能在压缩至仅剩22.2% token时略超未裁剪基线，极端压缩至仅剩5.6% token时仍保持92.8%的准确率。实验覆盖14个图像与视频基准，均表现为优异的效率-性能权衡和强鲁棒性。

Conclusion: FlashVLM能在主流VLM下实现显著token压缩与小幅性能提升，在保证全局语义和多样性的同时，大幅提升计算效率并具备优越的泛化与鲁棒性。

Abstract: Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.
  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.
  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.

</details>


### [66] [Repurposing Video Diffusion Transformers for Robust Point Tracking](https://arxiv.org/abs/2512.20606)
*Soowon Son,Honggyu An,Chaehyun Kim,Hyunah Ko,Jisu Nam,Dahyun Chung,Siyoon Jin,Jung Yi,Jaewon Min,Junhwa Hur,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出了DiTracker，一种基于视频扩散Transformer（DiT）的点追踪方法，实现了在动态运动和频繁遮挡等复杂场景下的高鲁棒性追踪，并在多个权威基准上取得了最优或同等的表现。


<details>
  <summary>Details</summary>
Motivation: 点追踪是4D重建、机器人和视频编辑等诸多应用的基础，现有方法多使用Shallow CNN作为骨干，帧间缺乏时序一致性，容易在复杂情况下失效，有待提升其时空建模能力和鲁棒性。

Method: 本文利用在大规模真实视频上预训练的DiT模型，发挥其时空关注能力，并通过三步优化：1）查询-键值注意力匹配，2）轻量级LoRA微调，3）与ResNet骨干的匹配代价融合。

Result: 在ITTO等高难度基准上取得了最高性能，并在TAP-Vid等权威基准的数据集上达到或超过了目前最优水平，即便只有原先方法1/8的批量训练规模。

Conclusion: 视频DiT特征可作为点追踪任务高效又有效的基础，本方法在点追踪准确性、鲁棒性和效率上表现突出，推动了该任务的发展。

Abstract: Point tracking aims to localize corresponding points across video frames, serving as a fundamental task for 4D reconstruction, robotics, and video editing. Existing methods commonly rely on shallow convolutional backbones such as ResNet that process frames independently, lacking temporal coherence and producing unreliable matching costs under challenging conditions. Through systematic analysis, we find that video Diffusion Transformers (DiTs), pre-trained on large-scale real-world videos with spatio-temporal attention, inherently exhibit strong point tracking capability and robustly handle dynamic motions and frequent occlusions. We propose DiTracker, which adapts video DiTs through: (1) query-key attention matching, (2) lightweight LoRA tuning, and (3) cost fusion with a ResNet backbone. Despite training with 8 times smaller batch size, DiTracker achieves state-of-the-art performance on challenging ITTO benchmark and matches or outperforms state-of-the-art models on TAP-Vid benchmarks. Our work validates video DiT features as an effective and efficient foundation for point tracking.

</details>


### [67] [FedPOD: the deployable units of training for federated learning](https://arxiv.org/abs/2512.20610)
*Daewoon Kim,Si Young Yie,Jae Sung Lee*

Main category: cs.CV

TL;DR: 本文提出了FedPOD算法，在保证通信效率和优化学习效率的基础上提升联邦学习的表现，并具备良好的扩展性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在多客户端环境下面临数据分布不均、通信开销大和参与者动态变化等问题。现有方法如FedPIDAvg虽提升了性能，但存在数据利用受限和参与者要求固定的问题。为解决这些局限，作者提出FedPOD。

Method: FedPOD借鉴控制理论，采用新颖任务划分（round-wise task），在每轮训练中评估验证损失，允许上轮被排除的异常参与者重新加入，并消除对历史轮次信息的依赖。同时，FedPOD设计兼容Kubernetes自动伸缩机制，提升灵活性与可扩展性。

Result: FedPOD在医学分割任务中的Dice分数与FedPIDAvg相当，分别在WT、ET、TC指标上平均达到0.78、0.71和0.72，综合收敛分达到0.74，显示了其竞争力。

Conclusion: FedPOD有效提升了联邦学习的效率、数据利用率和灵活性。其设计易于与Kubernetes集成，适用于动态扩展的云计算场景，对实际大规模联邦学习任务具有重要价值。

Abstract: This paper proposes FedPOD (Proportionally Orchestrated Derivative) for optimizing learning efficiency and communication cost in federated learning among multiple clients. Inspired by FedPIDAvg, we define a round-wise task for FedPOD to enhance training efficiency. FedPIDAvg achieved performance improvement by incorporating the training loss reduction for prediction entropy as weights using differential terms. Furthermore, by modeling data distribution with a Poisson distribution and using a PID controller, it reduced communication costs even in skewed data distribution. However, excluding participants classified as outliers based on the Poisson distribution can limit data utilization. Additionally, PID controller requires the same participants to be maintained throughout the federated learning process as it uses previous rounds' learning information in the current round. In our approach, FedPOD addresses these issues by including participants excluded as outliers, eliminating dependency on previous rounds' learning information, and applying a method for calculating validation loss at each round. In this challenge, FedPOD presents comparable performance to FedPIDAvg in metrics of Dice score, 0.78, 0.71 and 0.72 for WT, ET and TC in average, and projected convergence score, 0.74 in average. Furthermore, the concept of FedPOD draws inspiration from Kubernetes' smallest computing unit, POD, designed to be compatible with Kubernetes auto-scaling. Extending round-wise tasks of FedPOD to POD units allows flexible design by applying scale-out similar to Kubernetes' auto-scaling. This work demonstrated the potentials of FedPOD to enhance federated learning by improving efficiency, flexibility, and performance in metrics.

</details>


### [68] [Active Intelligence in Video Avatars via Closed-loop World Modeling](https://arxiv.org/abs/2512.20615)
*Xuanhua He,Tianyu Yang,Ke Cao,Ruiqi Wu,Cheng Meng,Yong Zhang,Zhuoliang Kang,Xiaoming Wei,Qifeng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频数字人智能框架ORCA，使其不仅能保持身份和动作一致性，还能自主实现多步任务规划和执行，推动视频数字人从被动动画迈向主动智能。


<details>
  <summary>Details</summary>
Motivation: 现有视频数字人虽在身份保留和动作对齐上表现优异，但缺乏主动智能，无法自主追求长期目标和进行自适应环境交互。本文致力于赋予视频数字人主动任务规划与执行能力，突破其被动特性。

Method: 提出了L-IVA任务和评测基准，用于测试数字人在随机生成环境中的目标规划能力。引入ORCA架构，创新点包括：1）闭环的“观-思-行-反”循环，持续验证预测与实际结果，提升状态追踪；2）分层双系统结构，系统2负责战略推理和状态预测，系统1将计划转为具体动作指令。将控制建模为POMDP，并实现连续信念更新和结果验证。

Result: 实验显示，ORCA在多步任务达成率和行为一致性上显著优于开放循环和无反思基线模型，有效实现自主多步任务完成。

Conclusion: ORCA通过内部世界建模与闭环推理设计，显著推动了视频数字人从被动画向主动、目标导向行为的转变，为数字人智能发展提供了新方向。

Abstract: Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.

</details>


### [69] [SpatialTree: How Spatial Abilities Branch Out in MLLMs](https://arxiv.org/abs/2512.20617)
*Yuxi Xiao,Longfei Li,Shen Yan,Xinhang Liu,Sida Peng,Yunchao Wei,Xiaowei Zhou,Bingyi Kang*

Main category: cs.CV

TL;DR: 本文提出了一个受认知科学启发的空间能力分级体系SpatialTree，并基于此构建了一个分层基准，用于全面评估主流多模态大语言模型（MLLMs）的空间推理能力。研究揭示了不同层级能力之间的关系，并探索了提升模型空间能力的训练策略。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型的空间能力评估主要集中于有限子任务，缺乏系统性、分级化的理解和度量。本文借鉴认知科学，人类空间能力是逐步发展的，希望系统化刻画和提升MLLM的空间能力。

Method: 作者提出了SpatialTree分层体系，将空间能力划分为感知（L1）、心智映射（L2）、模拟（L3）、能动能力（L4）四级，并基于此设计分层基准，涵盖27个子能力。同时，通过监督微调和强化学习实验，分析能力之间的迁移和提升空间。

Result: 评测结果显示，L1能力（感知）较为独立，L2-L4（较高层能力）之间联系紧密，表现出依赖关系。同时发现同级能力微调存在负迁移，但低级能力有助于高级能力提升。普通强化学习对复杂推理有益但损害直觉感知，提出的自适应思维策略可兼顾各层能力提升。

Conclusion: SpatialTree为理解和提升MLLM空间能力提供了新框架和基准，对系统化分析与提升多模态模型空间推理能力具有开创性意义。作者提出的提升策略有助于全面提升模型性能。

Abstract: Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive "thinking" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.

</details>


### [70] [SemanticGen: Video Generation in Semantic Space](https://arxiv.org/abs/2512.20619)
*Jianhong Bai,Xiaoshi Wu,Xintao Wang,Fu Xiao,Yuanxing Zhang,Qinghe Wang,Xiaoyu Shi,Menghan Xia,Zuozhu Liu,Haoji Hu,Pengfei Wan,Kun Gai*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视频生成方法SemanticGen，通过先在语义空间生成视频布局，后续再细化高频细节，实现高质量、高效率的视频生成。实验结果显示，其性能优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型普遍在VAE潜空间中建模，但该方式会导致收敛慢、长视频生成计算开销大。因此，亟需更高效的视频生成框架。

Method: SemanticGen采用两阶段流程。首先，扩散模型在紧凑的语义空间中生成视频的全局语义特征，然后第二个扩散模型基于这些特征生成VAE潜变量，最后还原为最终视频帧。这样只需在高层次语义上建模，减轻低级数据的计算负担。

Result: 实验证明，SemanticGen在收敛速度上明显快于传统VAE潜空间建模方式，并在长视频生成任务中兼具高效性和效果。其生成视频的质量及各项指标均优于主流的先进方法。

Conclusion: SemanticGen提供了一条高效生成高质量及长视频的新路径，对视频生成领域有重要推动作用。

Abstract: State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [71] [HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data](https://arxiv.org/abs/2512.19864)
*Shashi Kant Gupta,Arijeet Pramanik,Jerrin John Thomas,Regina Schwind,Lauren Wiener,Avi Raju,Jeremy Kornbluth,Yanshan Wang,Zhaohui Su,Hrituraj Singh*

Main category: cs.CL

TL;DR: 论文提出利用大型语言模型(LLM)为核心的智能体化框架，从海量真实肿瘤患者的非结构化电子健康记录(EHR)中，系统性、全面地自动提取结构化肿瘤学变量，表现优越，显著提升数据抽取效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的非结构化文本包含关键信息，但因复杂性和异构性，自动高效地结构化抽取肿瘤相关数据十分困难，手工方式成本高且不可扩展。以往自动化方法大多局限于小范围、人工合成数据或仅抽取少数变量，缺乏跨多文档、全局整合的解决方案。

Method: 作者提出一种智能体化(agentic)框架，将复杂肿瘤数据抽取任务拆分为可适应、可组合的子任务，核心技术是运用具备推理、检索和迭代综合能力的大语言模型，能从真实患者的各种格式肿瘤病历中，全面提取高质量结构化信息。

Result: 在包含2250名癌症患者、超40万份病例和PDF文档的大型真实数据集上，该方法平均F1分数达到0.93，103个肿瘤学变量中有100个超过0.85，关键变量如生物标志物、用药等均超过0.95。系统集成到真实数据整理流程后，人工直接通过率94%，大幅降低标注成本。

Conclusion: 本研究率先在大规模真实场景下，实现了基于LLM智能体的肿瘤数据结构化自动抽取，验证了其实用性与高效性，为后续临床研究和决策支持奠定了坚实基础。

Abstract: Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale

</details>


### [72] [How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse](https://arxiv.org/abs/2512.19903)
*Kirk Vanacore,Rene F. Kizilcec*

Main category: cs.CL

TL;DR: 本文比较了六个大语言模型（LLMs）在无需大规模定制的情况下，进行真实课堂对话中教学动作分类任务的表现。结果显示，基础模型对该任务有一定能力，但存在明显限制。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育领域的广泛应用，了解其在真实教育场景下的“开箱即用”能力（无需针对性微调）变得尤为重要，有助于建立合理预期和评测基准。

Method: 对六种LLMs使用零样本（zero-shot）、单样本（one-shot）和少样本（few-shot）提示方式，比较它们在对真实课堂记录中教学行为分类任务的基线性能，并与专家标注结果进行对比。

Result: 少样本范例提示下，先进模型表现有显著提升，最高Cohen's Kappa达到0.58（与人工专家一致程度）。但改进并不全面，不同教学动作表现差异较大，提高召回率通常导致假阳性增多。

Conclusion: 大语言模型具备了初步理解课堂教学语言的能力，巧妙的Prompt设计能发掘其潜力，但其解读教学情景的准确性和可靠性始终存在基础性局限。

Abstract: Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.

</details>


### [73] [Counterfactual LLM-based Framework for Measuring Rhetorical Style](https://arxiv.org/abs/2512.19908)
*Jingyi Qiu,Hong Chen,Zongyi Li*

Main category: cs.CL

TL;DR: 本论文提出了一种基于大模型（LLM）的反事实框架，用于量化机器学习论文中的修辞风格，区分华丽表述与实际内容。


<details>
  <summary>Details</summary>
Motivation: 随着AI兴起，学界对机器学习论文“炒作”（hype）现象日益关注，但如何独立于论文实质内容来量化修辞风格仍无公认方法。

Method: 作者提出利用多样LLM修辞风格人格生成相同内容的不同修辞表述，通过LLM裁判进行成对评比，并用Bradley--Terry模型聚合结果。在8485篇ICLR论文上生成25万以上反事实文本，量化修辞风格。

Result: 发现具有远见（visionary framing）的修辞可显著预测论文后续关注度（如引用和媒体报道），即便控制了同行评议评分也如此。自2023年后修辞强度大幅提升，主要归因于LLM写作助手的普及。方法在不同人格及人与LLM评判中表现稳定。

Conclusion: LLM能可靠地量化科学论文中的修辞风格，并可用作改进学术评价工具。

Abstract: The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because bold language can stem from either strong empirical results or mere rhetorical style, it is often difficult to distinguish between the two. To disentangle rhetorical style from substantive content, we introduce a counterfactual, LLM-based framework: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. We find that visionary framing significantly predicts downstream attention, including citations and media attention, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, and provide empirical evidence showing that this increase is largely driven by the adoption of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments to measure and improve scientific evaluation.

</details>


### [74] [PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation](https://arxiv.org/abs/2512.19933)
*Zhixiang Lu,Xueyuan Deng,Yiran Liu,Yulong Li,Qiang Yan,Imran Razzak,Jionglong Su*

Main category: cs.CL

TL;DR: 本文提出了一种新型模拟模型PRISM，更真实地再现了社交媒体在线极化背后的心理异质性与个体差异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Agent的舆论动力学模型，往往假设个体心理趋同，导致无法捕捉网络极化中个体认知偏差与信息传播的复杂作用机制。因此，迫切需要一个能融合个体心理多样性的模拟框架，以便揭示意识形态分化是如何被放大的。

Method: 作者提出了PRISM模型，将连续情感演化的随机微分方程（SDE）与基于人格条件的部分可观测马尔可夫决策过程（PC-POMDP）结合，并采用MBTI人格类型为多模态大语言模型Agent赋予差异化的决策策略，利用社交媒体大数据进行数据先验初始化。

Result: PRISM模型在个性一致性方面更符合人类真实表现，显著优于传统同质模型和Big Five基线。该模型能更好地复现“理性自抑”和“情感共振”等社交媒体复杂现象。

Conclusion: PRISM为分析社交媒体观点极化等复杂现象提供了强有力工具，有助于深入理解个体认知与信息扩散的耦合作用，推动网络社会科学发展。

Abstract: Traditional agent-based models (ABMs) of opinion dynamics often fail to capture the psychological heterogeneity driving online polarization due to simplistic homogeneity assumptions. This limitation obscures the critical interplay between individual cognitive biases and information propagation, thereby hindering a mechanistic understanding of how ideological divides are amplified. To address this challenge, we introduce the Personality-Refracted Intelligent Simulation Model (PRISM), a hybrid framework coupling stochastic differential equations (SDE) for continuous emotional evolution with a personality-conditional partially observable Markov decision process (PC-POMDP) for discrete decision-making. In contrast to continuous trait approaches, PRISM assigns distinct Myers-Briggs Type Indicator (MBTI) based cognitive policies to multimodal large language model (MLLM) agents, initialized via data-driven priors from large-scale social media datasets. PRISM achieves superior personality consistency aligned with human ground truth, significantly outperforming standard homogeneous and Big Five benchmarks. This framework effectively replicates emergent phenomena such as rational suppression and affective resonance, offering a robust tool for analyzing complex social media ecosystems.

</details>


### [75] [Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems](https://arxiv.org/abs/2512.19950)
*Heet Bodara,Md Masum Mushfiq,Isma Farah Siddiqui*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）在对话系统中表现出的语气偏差问题，发现即使在中性对话生成中也会存在系统性的语气偏差。作者通过设计合成数据集和训练分类器，验证了语气偏差的可测量性与重要性。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM生成的对话非常流畅自然，但它们经常无意识地表现出偏向某种语气（如过于礼貌、积极或谨慎），这会影响用户对AI助手的信任、公平和共情感知。因此，研究LLM中的语气偏差对于设计更公平、可靠的对话系统至关重要。

Method: 作者结合可控式对话生成与语气分类模型，首先利用LLM合成了两种对话数据集（中性与带有明显正面/负面语气引导），再用预训练DistilBERT模型对数据进行弱监督标注，接着训练多个语气分类器检测偏差。

Result: 结果显示，即便是以中性为初衷生成的数据也存在系统性的语气偏差。集成模型的macro F1分数最高达到0.92，说明偏差具有可检测性且普遍存在。

Conclusion: 大型语言模型在对话系统中存在着不可忽视的语气偏差，这种偏差易于被检测和量化。为了提升对话AI的公正性与可信赖性，需要正视并管理这种隐藏的行为特征。

Abstract: Large Language Models are increasingly used in conversational systems such as digital personal assistants, shaping how people interact with technology through language. While their responses often sound fluent and natural, they can also carry subtle tone biases such as sounding overly polite, cheerful, or cautious even when neutrality is expected. These tendencies can influence how users perceive trust, empathy, and fairness in dialogue. In this study, we explore tone bias as a hidden behavioral trait of large language models. The novelty of this research lies in the integration of controllable large language model based dialogue synthesis with tone classification models, enabling robust and ethical emotion recognition in personal assistant interactions. We created two synthetic dialogue datasets, one generated from neutral prompts and another explicitly guided to produce positive or negative tones. Surprisingly, even the neutral set showed consistent tonal skew, suggesting that bias may stem from the model's underlying conversational style. Using weak supervision through a pretrained DistilBERT model, we labeled tones and trained several classifiers to detect these patterns. Ensemble models achieved macro F1 scores up to 0.92, showing that tone bias is systematic, measurable, and relevant to designing fair and trustworthy conversational AI.

</details>


### [76] [Schoenfeld's Anatomy of Mathematical Reasoning by Language Models](https://arxiv.org/abs/2512.19995)
*Ming Li,Chenrui Fan,Yize Cheng,Soheil Feizi,Tianyi Zhou*

Main category: cs.CL

TL;DR: 提出了ThinkARM框架，通过中尺度抽象方法对大语言模型的推理过程进行结构化分析，揭示了模型推理中的关键动态和结构差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理问题上生成推理痕迹越来越普遍，但目前只能通过浅层统计方法分析，难以理解推理过程中的真实结构和内部步骤。作者希望通过更细致、中间层级的方法揭示模型推理的具体流程和本质特征。

Method: 借鉴Schoenfeld的Episode Theory理论，提出ThinkARM框架，将模型的推理过程分解为分析、探索、实现、验证等多个功能性推理步骤，并将其应用于不同模型的数学问题求解推理痕迹，进行结构化归纳分析。

Result: 抽象框架揭示了推理模型和非推理模型在推理结构上的显著差异，并发现“探索”步骤对于推理正确性具有关键影响。同时，数据表明追求效率的模型往往会选择性地跳过评价反馈步骤，而不是单纯缩短整体输出。

Conclusion: Episode-level（中尺度）表示方法使推理步骤更加显性化，能够系统地分析现代大模型推理结构、稳定性及其变化方式。

Abstract: Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.

</details>


### [77] [Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents](https://arxiv.org/abs/2512.20092)
*Yiming Du,Baojun Wang,Yifan Xiang,Zhaowei Wang,Wenyu Huang,Boyang Xue,Bin Liang,Xingshan Zeng,Fei Mi,Haoli Bai,Lifeng Shang,Jeff Z. Pan,Yuxin Jiang,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Memory-T1的新方法，用于提升对长、多轮对话的时间推理能力，通过时间感知记忆选择策略显著提升了推理准确性和模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 面对长对话历史，当前模型由于噪声积累和信息过载，时间相关信息难以精确捕捉，导致推理性能显著下降。因此，亟需设计能够有效筛选时序证据的机制，以提升对话智能体的推理能力。

Method: 提出了Memory-T1框架，分为两步：首先使用时间性和相关性过滤器粗筛候选历史片段，然后利用强化学习（RL）智能体精细筛选确切时序证据信息。该RL组件基于答案准确性、证据支撑和时间一致性多重奖励函数优化学习。尤其是时间一致性奖励能细粒度对齐查询时序范围，帮助模型解决时序歧义。

Result: 在Time-Dialog基准上，Memory-T1将7B模型推理性能提升至67.0分，超过14B基线10.2个百分点，并创造了新开源SOTA。消融实验显示时间一致性与证据支撑奖励联合贡献了15个百分点性能提升。此外，该方法在超长对话（128k tokens）场景下依旧稳定，而基线模型已失效。

Conclusion: Memory-T1显著改善了长多轮对话的时间推理能力,在对抗噪声和超长历史情况下也表现出显著的鲁棒性，为对话智能体的时序理解奠定了坚实基础。

Abstract: Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/

</details>


### [78] [A Novel Graph-Sequence Learning Model for Inductive Text Classification](https://arxiv.org/abs/2512.20097)
*Zuo Wang,Ye Yuan*

Main category: cs.CL

TL;DR: 本文提出了一种结合图结构与序列信息的新型文本分类模型TextGSL，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的文本分类方法无法充分利用单词对之间多样的结构信息，并忽视了文本的序列信息，同时难以处理包含新词和新关系的文本。

Method: 作者构建了基于每个文本的单独图结构，针对单词对之间的共现、句法和语义等多种关系设置不同的边类型，并提出自适应多边信息传递机制以融合多样结构信息。同时，通过引入Transformer层捕获文本的序列信息，从而学习更具判别性的文本表示。

Result: 在多个基准数据集上实验，TextGSL在分类准确率指标上优于多个强力对比模型。

Conclusion: 将多关系图结构和序列信息有机结合可有效提升文本分类模型的性能，对应方法在泛化和处理新词新关系方面优势明显。

Abstract: Text classification plays an important role in various downstream text-related tasks, such as sentiment analysis, fake news detection, and public opinion analysis. Recently, text classification based on Graph Neural Networks (GNNs) has made significant progress due to their strong capabilities of structural relationship learning. However, these approaches still face two major limitations. First, these approaches fail to fully consider the diverse structural information across word pairs, e.g., co-occurrence, syntax, and semantics. Furthermore, they neglect sequence information in the text graph structure information learning module and can not classify texts with new words and relations. In this paper, we propose a Novel Graph-Sequence Learning Model for Inductive Text Classification (TextGSL) to address the previously mentioned issues. More specifically, we construct a single text-level graph for all words in each text and establish different edge types based on the diverse relationships between word pairs. Building upon this, we design an adaptive multi-edge message-passing paradigm to aggregate diverse structural information between word pairs. Additionally, sequential information among text data can be captured by the proposed TextGSL through the incorporation of Transformer layers. Therefore, TextGSL can learn more discriminative text representations. TextGSL has been comprehensively compared with several strong baselines. The experimental results on diverse benchmarking datasets demonstrate that TextGSL outperforms these baselines in terms of accuracy.

</details>


### [79] [ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language](https://arxiv.org/abs/2512.20111)
*Aly Lidayan,Jakob Bjorner,Satvik Golechha,Kartik Goyal,Alane Suhr*

Main category: cs.CL

TL;DR: 本文提出了一种用于大型语言模型（LLM）智能体在多步决策任务中保持简洁上下文的方法——ABBEL，通过信念瓶颈压缩历史，并结合强化学习进行优化，能显著降低内存使用同时保持甚至提升表现。


<details>
  <summary>Details</summary>
Motivation: 随着多步决策任务长度增加，完整保留全部历史交互数据会导致计算和存储资源压力大。为解决LLM在长序列任务中的上下文效率问题，需要设计低内存且易解释的新机制。

Method: 提出ABBEL框架，用自然语言信念状态代替冗长的历史，每步通过观测更新信念，并用最新信念选择动作。此外引入强化学习后训练LLM生成高质量且简洁的信念，并奖励压缩和准确性。

Result: 实验涵盖6种多步环境，ABBEL能用近乎恒定的内存生成可解释信念，表现接近于完整历史。同时，通过强化学习对信念产生和动作决策进一步改进，部分场景下性能甚至超过全历史方法，且内存消耗更低。

Conclusion: ABBEL框架为长序列任务中LLM高效决策和推理提供新思路，强化训练可进一步超越现有方法，但瓶颈机制的信念误差传播仍需解决。

Abstract: As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.

</details>


### [80] [M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2512.20136)
*Hyeongcheol Park,Jiyoung Seo,Jaewon Mun,Hogun Park,Wonmin Byeon,Sung June Kim,Hyeonsoo Im,JeungSub Lee,Sangpil Kim*

Main category: cs.CL

TL;DR: 论文提出了一种新方法M$^3$KG-RAG，通过增强多模态知识图谱（MMKG）的多跳推理和检索，提升多模态大语言模型（MLLMs）在音视频等多模态检索和生成任务中的表现，实现更深层次的推理和更高的答案真实性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态RAG方法在音视频领域受到限制，主要原因是现有MMKG模态覆盖有限、多跳关系不足，以及基于简单相似性检索导致离题/冗余知识难以过滤。

Method: 1）提出轻量多智能体管线构建多跳多模态知识图谱（M$^3$KG），内容为上下文丰富的多模态实体三元组，支持按模态按需检索；2）设计GRASP机制，确保实体与查询精准对齐，评估答案相关性，并剪除冗余上下文，只保留关键知识以生成响应。

Result: 在多个多模态基准任务上，M$^3$KG-RAG显著提升了MLLMs在多模态推理和事实对齐方面的表现，优于现有方法。

Conclusion: M$^3$KG-RAG有效缓解了现有多模态RAG在知识覆盖和检索精度上的局限，推动了多模态大语言模型在推理和答案真实性等方面的发展。

Abstract: Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.

</details>


### [81] [Multi-hop Reasoning via Early Knowledge Alignment](https://arxiv.org/abs/2512.20144)
*Yuxin Wang,Shicheng Fang,Bo Wang,Qi Luo,Xuanjing Huang,Yining Zheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了一种改进RAG系统性能的方式——Early Knowledge Alignment (EKA)，能够提升大语言模型在多跳推理下的信息检索准确性和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统在处理复杂多跳问题时，往往忽略了检索语料的信息，导致推理链条冗长且效率低下，因此亟需一种能更好结合检索与推理的信息对齐机制。

Method: 提出EKA模块，将大语言模型与检索集进行早期对齐，在多步推理前提前利用上下文相关的检索知识。整个流程无需额外训练，能直接集成到现有RAG系统中，并通过熵分析解释其有效性。

Result: 在六个标准RAG数据集上的实验显示，EKA显著提升了检索准确率，减少了推理过程中的连锁错误，提高了整体性能和效率，并在各种模型规模与数据集下表现出强泛化能力。

Conclusion: EKA既提升了基于强化学习的RAG系统的性能，也强调了结构化推理和高效探索之间的协同作用，为今后的RAG框架优化提供了新视角。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}.

</details>


### [82] [Retrieval-augmented Prompt Learning for Pre-trained Foundation Models](https://arxiv.org/abs/2512.20145)
*Xiang Chen,Yixin Ou,Quan Feng,Lei Li,Piji Li,Haibo Ye,Sheng-Jun Huang,Shuofei Qiao,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为RetroPrompt的新方法，通过引入检索机制和外部知识库，提升预训练基础模型在零样本和小样本学习中的泛化能力，减少死记硬背式的记忆依赖。


<details>
  <summary>Details</summary>
Motivation: 现有PFMs的prompt learning虽然提升了小样本学习性能，但依然采取参数化学习范式，导致泛化受限，容易记忆训练数据而非真正理解，从而难以处理非典型样本且易过拟合。

Method: RetroPrompt利用从训练数据生成的公共知识库，并在输入、训练和推理阶段引入检索机制，实时检索相关上下文信息，以增强模型可用线索，从而在记忆和泛化之间达到平衡。

Result: 在多个自然语言处理和计算机视觉任务数据集的实验表明，RetroPrompt在零样本和小样本场景下性能优越。

Conclusion: RetroPrompt能够有效减少死记硬背，提升基础模型的泛化能力，对多领域任务具有良好适应性。

Abstract: The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.

</details>


### [83] [Fun-Audio-Chat Technical Report](https://arxiv.org/abs/2512.20156)
*Qian Chen,Luyao Cheng,Chong Deng,Xiangang Li,Jiaqing Liu,Chao-Hong Tan,Wen Wang,Junhao Xu,Jieping Ye,Qinglin Zhang,Qiquan Zhang,Jingren Zhou*

Main category: cs.CL

TL;DR: Fun-Audio-Chat提出创新方法，解决语音-文本模型中语音/文本分辨率不匹配、高计算开销和知识遗忘等问题，实现文本与音频理解、推理和生成的平衡，并开源了模型及代码。


<details>
  <summary>Details</summary>
Motivation: 当前多模态语音-文本大模型在实际应用中存在如语音与文本token分辨率不一致导致语义信息稀释、高计算成本，以及模型容易遗忘LLM文本知识等实际问题，亟需新方法兼顾效率和效果。

Method: 1. 提出Dual-Resolution Speech Representations（DRSR）：主干LLM用5Hz处理音频提升效率，Speech Refined Head用25Hz保证高质量；2. 提出Core-Cocktail Training，两阶段微调并中间合并，有效防止知识遗忘；3. Multi-Task DPO Training提升鲁棒性与多项任务能力。整个流程强调后训练、多任务多阶段微调，并利用现有预训练模型。

Result: Fun-Audio-Chat系列模型（8B和MoE 30B-A3B）在语音转文本、语音QA、音频理解、语音功能调用、指令跟随和语音共情等任务上，取得了同量级模型内的领先或接近最优性能，尤其在Spoken QA等基准有效验证。全双工变体也表现突出。

Conclusion: Fun-Audio-Chat高效结合LLM文本知识和对音频的理解与生成能力，创新方法切实提升多模态任务表现，并兼具推理、生成和交互能力。模型开源促进了社区进一步研究和应用。

Abstract: Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.

</details>


### [84] [AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications](https://arxiv.org/abs/2512.20164)
*Honglin Mu,Jinghao Liu,Kaiyang Wan,Rui Xing,Xiuying Chen,Timothy Baldwin,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文揭示了大语言模型（LLMs）在简历筛选等任务中易受“对抗指令”攻击，提出了检测与防御方法，并验证了防御效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型被广泛应用于自动化流程，如代码审查和简历筛选。然而，尚未深入研究在如简历筛选等领域，LLMs对隐藏输入指令（对抗性注入）的脆弱性及其防御机制。本文希望填补此安全性空白。

Method: 作者构建了针对简历筛选场景的对抗攻击基准，测试了不同类型隐蔽攻击下的成功率。随后评估了两类防御方法：推理时通过提示词调节（prompt-based defense）和训练时通过低秩适配（LoRA）的FIDS方法，以及二者结合的效果。

Result: 对抗攻击成功率最高可达80%。提示词防御可减少10.1%的攻击但带来12.5%误拒率，FIDS方法可减低15.4%攻击且误拒率为10.4%，组合防御能降低26.3%的攻击。训练时防御优于推理时缓解。

Conclusion: 现有LLM在简历筛选等现实应用下存在对抗脆弱性，需开发专门防御。训练时的防御机制在安全性与使用友好度上都优于推理时防御，对实际部署具备指导意义。

Abstract: Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by "adversarial instructions" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.

</details>


### [85] [FaithLens: Detecting and Explaining Faithfulness Hallucination](https://arxiv.org/abs/2512.20182)
*Shuzheng Si,Qingyi Wang,Haozhe Zhao,Yuzhuo Bai,Guanqiao Chen,Kangyang Luo,Gang Chen,Fanchao Qi,Minjia Zhang,Baobao Chang,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出了FaithLens模型，用于高效检测大语言模型输出中的真实性幻觉，并同时给出解释。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，识别大语言模型（LLM）输出内容是否存在真实性幻觉对检索增强生成、摘要等任务至关重要。

Method: 1）利用先进的LLM合成带解释的训练数据，并经过严格筛选以保证标签正确、解释高质量和数据多样性；2）用筛选后的数据对模型进行微调，作为冷启动；3）采用基于规则的强化学习进一步优化模型，同时考虑预测正确性和解释质量。

Result: 在12项任务上，8B参数的FaithLens模型优于当前先进模型（如GPT-4.1和o3），并能生成高质量解释。

Conclusion: FaithLens在可信性、效率和有效性之间展现了独特的平衡，是一种出色的真实性幻觉检测与解释模型。

Abstract: Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.

</details>


### [86] [Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings](https://arxiv.org/abs/2512.20204)
*Marko Čechovič,Natália Komorníková,Dominik Macháček,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本论文介绍了一个多语言无共同语言对话语料库，用于评估自动语音翻译系统，并提出了跨语言误解检测的方法。


<details>
  <summary>Details</summary>
Motivation: 随着高效语音翻译技术的发展，越来越多人希望打破语言隔阂进行交流。然而，相关自动系统的公开、真实评价语料匮乏，且跨语言沟通中的误解检测是亟需攻克的现实问题。本文旨在提供一个高质量语料库，支持跨语言会议和误解检测等研究。

Method: 作者人工收集了5小时无共同语言对话的录音，涵盖12种语言，提供ASR转写、手工转写、自动及校正后的英文翻译，并补充了会议摘要。作者还手动标注了会议中的误解片段，并测试了Gemini等大语言模型对误解检测能力。

Result: 建立了多语言对话语料库，覆盖多语言ASR、翻译与摘要；手动标注出误解片段；实验发现Gemini大模型对误解文本定位召回率为77%，准确率为47%。

Conclusion: 该语料库有助于推动自动语音翻译系统与跨语言会议总结、误解检测等任务研究。当前的大语言模型具备一定的误解自动检测能力，但精度仍有提升空间。

Abstract: Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.
  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.

</details>


### [87] [SlideTailor: Personalized Presentation Slide Generation for Scientific Papers](https://arxiv.org/abs/2512.20292)
*Wenzheng Zeng,Mingyu Ouyang,Langyuan Cui,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 该论文提出了一种结合用户偏好的论文自动生成演示幻灯片的新任务和方法。核心是SlideTailor框架，可根据用户轻松提供的案例和模板，自适应生成符合个人需求的可编辑幻灯片，并提出了创新的语音链机制以提升内容与口头讲述的一致性。实验和数据集验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的论文到幻灯片自动生成方法无法适应各用户的多元化和隐性偏好，导致输出结果无法满足个性需求，当前方法更是要求详细描述偏好，用户负担较重。为此，作者动机是开发一种无需用户详细表达偏好、自然易用且能充分体现用户风格的个性化生成方法。

Method: 提出SlideTailor agentic 框架，通过让用户提供一组论文-幻灯片案例对及视觉模板（而非详细文字偏好），系统自动学习和泛化用户在内容与视觉风格上的隐性偏好，从而引导后续幻灯片生成。此外，创新性地引入了chain-of-speech机制，使内容更好地与可能的口头讲解一致，适用于视频演示等应用。还建立了多样化的数据集并设计了解释性强的指标用于评估。

Result: 实验证明所提方法SlideTailor能够有效地提取和应用用户隐性偏好，泛化能力好，生成的幻灯片在多项指标上优于现有方法，并能显著提升内容与口头输出的一致性。框架支持多样化场景，并适用于多种下游任务。

Conclusion: SlideTailor方法能够无缝结合用户个性化内容与风格偏好，实现高质量、用户对齐的自动幻灯片生成，为智能内容生成领域提供了新思路，并具有广泛应用前景。

Abstract: Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.

</details>


### [88] [AprielGuard](https://arxiv.org/abs/2512.20293)
*Jaykumar Kasundra,Anjaneya Praharaj,Sourabh Surana,Lakshmi Sirisha Chodisetty,Sourav Sharma,Abhigya Verma,Abhishek Bhardwaj,Debasish Kanhar,Aakash Bhagat,Khalil Slimi,Seganrasan Subramanian,Sathwik Tejaswi Madhusudhan,Ranga Prasad Chenna,Srinivas Sunkara*

Main category: cs.CL

TL;DR: AprielGuard是一种用于保护大语言模型（LLM）免受不安全或对抗性行为的新型防护模型，将安全风险和对抗性威胁统一在一个框架下，在多个基准上超越了现有开源解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在对话和智能体场景中的广泛部署，保障其安全性变得极其重要。但现有的防护措施通常将安全风险和对抗性威胁分开处理，导致通用性和鲁棒性有限。

Method: 提出了AprielGuard，一个拥有8B参数的防护模型，将安全与对抗威胁统一建模，利用多样的开放与合成数据（涵盖单步、多轮对话和智能体操作），并引入结构化推理轨迹来提升可解释性。

Result: 在多项公开和私有基准上，AprielGuard在检测有害内容和对抗操控方面表现出色，尤其在多步推理和复杂场景下优于Llama-Guard等现有开源防护模型。

Conclusion: AprielGuard模型的发布推动了LLM安全防护的透明和可复现研究，为提升LLM部署的可靠性和安全性提供了有力支持。

Abstract: Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.

</details>


### [89] [Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives](https://arxiv.org/abs/2512.20298)
*Karolina Drożdż,Kacper Dudzic,Anna Sterna,Marcin Moskalewicz*

Main category: cs.CL

TL;DR: 本文比较了先进的大语言模型（LLMs）和精神科专家对人格障碍诊断的表现，发现LLMs整体准确率高于专家，但对自恋型人格障碍（NPD）诊断严重不足，展现出偏见和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs广泛用于精神健康自我评估，关注其是否能够像专业人士一样解读患者的叙述案例，对其在实际临床中的能力和局限性进行量化分析显得尤为重要。

Method: 作者对比了最先进的Gemini Pro LLM模型与人类精神健康专家在分析波兰语第一人称自述（涉及BPD和NPD）时的诊断表现。评估包括准确率、F1分数，以及诊断解释内容的质性分析。

Result: LLMs在总体诊断准确率上超过专业人员（65.48% vs. 43.57%），BPD诊断表现相当（LLMs F1=83.4，人类F1=80.0），但LLMs对NPD诊断极差（F1=6.7 vs. 50.0），且诊断叙述风格更具自信和形式化。

Conclusion: 虽然LLMs在理解复杂病人自述方面表现突出，其诊断仍受到偏见和可靠性缺陷的制约，需谨慎用于临床场景，特别是在涉及带有负面价值判断的诊断时。

Abstract: Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term "narcissism." Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues.

</details>


### [90] [SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision](https://arxiv.org/abs/2512.20308)
*Maxime Poli,Mahi Luthra,Youssef Benchekroun,Yosuke Higuchi,Martin Gleize,Jiayi Shen,Robin Algayres,Yu-An Chung,Mido Assran,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 论文提出了SpidR，一种自监督语音表示模型，能更高效地从语音中学习语义信息并应用于无文本口语语言建模，表现优于现有方法，并大幅缩短了预训练时间。


<details>
  <summary>Details</summary>
Motivation: 随着语言建模和语音表示学习的进步，研究者希望绕过文本中介，直接从语音中学习语言。要实现这一目标，关键需要从语音中提取高质量的语义表示。

Method: SpidR模型采用自监督学习，直接用原始波形训练，通过masked prediction目标、在线聚类和自蒸馏机制提升语音表征能力。学生模型中间层预测教师模型中间层产生的分配，稳定了在线聚类并提升了码本质量。作者还评估了多种模型和不同层间语音单元质量与语言建模性能的相关性。此外，SpidR通过高效的预训练方法和代码，大幅提升了训练速度。

Result: 实验表明，SpidR在下游语言建模任务（sWUGGY, sBLIMP, tSC）中均优于wav2vec 2.0、HuBERT、WavLM、DinoSR；其预训练时间只需16块GPU一天，比HuBERT一周快了数倍。

Conclusion: SpidR为无文本语音语言建模提供了更高效、更强表现的解决方案，并通过验证相关性实验强化了指标可靠性，提升了预训练效率，同时也开源了代码和模型，有利于领域进一步快速发展。

Abstract: The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr.

</details>


### [91] [Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles](https://arxiv.org/abs/2512.20324)
*Nurul Labib Sayeedi,Md. Faiyaz Abdullah Sayeedi,Khushnur Binte Jahangir,Swakkhar Shatabda,Sarah Masud Preum*

Main category: cs.CL

TL;DR: 本文介绍了BanglaRiddleEval——一个包含1244个孟加拉传统谜语的全新基准测试，用以评估大型语言模型（LLMs）在低资源情景与隐喻、文化性推理方面的能力。测试结果显示现有LLMs在这些任务上与人类表现存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多NLP任务上表现突出，但其处理隐喻、文化依赖、低资源语言（特别是孟加拉语）推理的能力尚未被仔细研究，因此需要新的评测体系来补全这一空白。

Method: 构建了BanglaRiddleEval数据集，涵盖4个任务，总计4976个谜题任务实例。利用LLM自动生成逐步推理（Chain-of-Thought）、高质量的干扰项及精细的歧义标注，并采用多种开放与闭源模型、不同提示方式系统测试LLMs的表现。

Result: LLMs在生成式问答任务上能部分抓住语义要点但正确率较低，多项选择题的最高准确率仅为56%（人类基线为83%），歧义消解能力在26%-68%之间，且高质量解释仅限于最强模型。

Conclusion: 当前LLMs仅能捕捉到部分解谜线索，与人类水平有明显差距。BanglaRiddleEval为低资源语言的隐喻推理研究提供了新的具有挑战性的基准。

Abstract: Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval.

</details>


### [92] [Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation](https://arxiv.org/abs/2512.20352)
*Nilesh Jain,Seyi Adeyinka,Leor Roseman,Aza Allsop*

Main category: cs.CL

TL;DR: 本文提出了一个用于大型语言模型（LLM）辅助主题分析的多视角验证框架，通过集成验证和双可靠性指标（Cohen's Kappa和余弦相似度）提升定性分析的信度。实验证明，该方法可高效、灵活且透明地实现高一致性和共识主题提取。


<details>
  <summary>Details</summary>
Motivation: 当前定性研究中人工标注一致性（inter-rater agreement）方法依赖多位人工编码者，耗时且一致性常常一般。随着AI语言模型的强大能力，如何系统化、可靠地用LLM辅助主题分析成为关键挑战。本研究旨在解决传统方法信度不足和效率低的问题。

Method: 作者提出了一个多视角验证框架，将集成（ensemble）分析与两类可靠性指标结合：Cohen’s Kappa（人工标注一致性）和语义余弦相似度。该框架可灵活设置模型参数（种子数、温度）、自定义prompt结构、适配任意JSON格式。以迷幻艺术治疗访谈文本为案例，评估了Gemini 2.5 Pro、GPT-4o、Claude 3.5 Sonnet三种主流LLM，每种模型独立运行六次。

Result: 实验发现，Gemini取得最高一致性（Kappa=0.907，余弦相似度95.3%），GPT-4o和Claude分别次之（Kappa均大于0.80）。所有模型对提取共识主题的能力表现优秀，Gemini能识别6个共识主题，一致性在50-83%之间，GPT-4o识别5个，Claude识别4个。

Conclusion: 本文开源的框架为AI辅助定性研究提供了高透明度的信度指标、灵活的配置和结构无关的共识主题提取方式，为可靠的AI定性分析方法奠定了重要基础。

Abstract: Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($κ$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($κ= 0.907$, cosine=95.3%), followed by GPT-4o ($κ= 0.853$, cosine=92.6%) and Claude ($κ= 0.842$, cosine=92.1%). All three models achieve a high agreement ($κ> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.

</details>


### [93] [Sentiment-Aware Extractive and Abstractive Summarization for Unstructured Text Mining](https://arxiv.org/abs/2512.20404)
*Junyi Liu,Stanley Kok*

Main category: cs.CL

TL;DR: 本文提出了一种融合情感信息的摘要方法，能有效处理社交媒体等用户生成内容的情感与信息提取难题。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体、评论和论坛中非结构化数据激增，信息系统领域需要从中高效提炼有用信息。然而，现有摘要方法多针对结构化新闻，不适应情感丰富、噪声多的用户内容。此外，情感因素对品牌监控、市场分析等任务至关重要，但摘要时很少融入情感建模。

Method: 提出了一套情感感知的摘要框架，对提取式（TextRank）和生成式（UniLM）方法均进行扩展，在其排序和生成过程中嵌入情感信号，从而兼顾情感认知与主题相关性。

Result: 该方法在摘要中更好地捕捉了用户发帖的情感细节和主旨，提高了摘要的凝练性和情感表达，优于传统方法。

Conclusion: 所提框架生成的摘要更加情感丰富、信息集约，有助于品牌方和企业实现高效、及时决策和干预，适用于动态复杂的在线环境。

Abstract: With the rapid growth of unstructured data from social media, reviews, and forums, text mining has become essential in Information Systems (IS) for extracting actionable insights. Summarization can condense fragmented, emotion-rich posts, but existing methods-optimized for structured news-struggle with noisy, informal content. Emotional cues are critical for IS tasks such as brand monitoring and market analysis, yet few studies integrate sentiment modeling into summarization of short user-generated texts. We propose a sentiment-aware framework extending extractive (TextRank) and abstractive (UniLM) approaches by embedding sentiment signals into ranking and generation processes. This dual design improves the capture of emotional nuances and thematic relevance, producing concise, sentiment-enriched summaries that enhance timely interventions and strategic decision-making in dynamic online environments.

</details>


### [94] [Step-DeepResearch Technical Report](https://arxiv.org/abs/2512.20491)
*Chen Hu,Haikuo Du,Heng Wang,Lin Lin,Mingrui Chen,Peng Liu,Ruihang Miao,Tianchi Yue,Wang You,Wei Ji,Wei Yuan,Wenjin Deng,Xiaojian Yuan,Xiaoyun Zhang,Xiangyu Liu,Xikai Liu,Yanming Xu,Yicheng Cao,Yifei Zhang,Yongyao Wang,Yubo Shu,Yurong Zhang,Yuxiang Zhang,Zheng Gong,Zhichao Chang,Binyan Li,Dan Ma,Furong Jia,Hongyuan Wang,Jiayu Liu,Jing Bai,Junlan Liu,Manjiao Liu,Na Wang,Qiuping Wu,Qinxin Du,Shiwei Li,Wen Sun,Yifeng Gong,Yonglin Chen,Yuling Zhao,Yuxuan Lin,Ziqi Ren,Zixuan Wang,Aihu Zhang,Brian Li,Buyun Ma,Kang An,Li Xie,Mingliang Li,Pan Li,Shidong Yang,Xi Chen,Xiaojia Liu,Yuchu Luo,Yuan Song,YuanHao Ding,Yuanwei Liang,Zexi Li,Zhaoning Zhang,Zixin Zhang,Binxing Jiao,Daxin Jiang,Jiansheng Chen,Jing Li,Xiangyu Zhang,Yibo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种高效、端到端的深度研究智能体Step-DeepResearch，通过创新的数据合成与训练方法，并在中文领域建立评测基准ADR-Bench，提升了中等规模模型的深度研究能力，效果媲美主流大模型。


<details>
  <summary>Details</summary>
Motivation: 当前大模型正向自主智能体方向发展，现实研究往往需要模型具备意图识别、长程决策和多源验证等能力，而现有学术评测（如BrowseComp）难以满足这些实际需求，特别是中文领域缺乏有效评估。

Method: （1）提出基于原子能力的数据合成策略，强化智能体规划与写作能力；（2）结合从智能体中期训练到监督微调与强化学习的递进式训练路径；（3）增加Checklist式判别器，提升系统鲁棒性；（4）建立中文深度研究基准ADR-Bench，支持更真实的评估。

Result: Step-DeepResearch（32B）模型在Scale AI Research Rubrics上取得61.4%得分，在ADR-Bench上显著超越同类开源模型，并可媲美OpenAI与Gemini DeepResearch等顶级闭源模型。

Conclusion: 经过精细训练的中等规模模型可具备专家能力，实现高成本效率，推动深度研究智能体实用化，尤其在中文场景取得突破。

Abstract: As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.

</details>


### [95] [Distilling to Hybrid Attention Models via KL-Guided Layer Selection](https://arxiv.org/abs/2512.20569)
*Yanhong Li,Songlin Yang,Shawn Tan,Mayank Mishra,Rameswar Panda,Jiawei Zhou,Yoon Kim*

Main category: cs.CL

TL;DR: 本文提出了一种高效的层选择方法，将大模型中的部分softmax注意力层替换为线性注意力层，并通过简单的蒸馏流程构建高效混合架构，无需昂贵的重新预训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法将预训练变换器模型转为更高效的混合软/线性注意力架构时，层的替换方式（即层选择）往往依赖固定规则或专用数据集，效率和效果都有限。

Method: 作者提出利用基于少量通用文本微调获得的层重要性得分，指导哪些层应替换为线性注意力。选定后再使用RADLADS流水线依次进行注意力权重转移、隐状态对齐、分布匹配蒸馏和微调。

Result: 该基于层重要性得分的方法在层选择效果上优于按比例均匀插入及依赖诊断集的复杂方法；所得模型推理更高效。

Conclusion: 通过简单且通用的层重要性评估，可显著提升混合注意力架构的蒸馏效率与性能，无需专用数据或复杂规则。

Abstract: Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.

</details>


### [96] [Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits](https://arxiv.org/abs/2512.20578)
*Amirhosein Ghasemabadi,Di Niu*

Main category: cs.CL

TL;DR: 本文提出Gnosis机制，通过解码LLM内部状态信号，实现模型自我正确性预测，无需外部监督且计算开销极低，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型易产生错误和幻觉，但很难自行察觉。现有自检与纠错手段依赖于外部判决器、多样本对比或文本自我批判，这些方法要么计算昂贵，要么与真实正确性弱相关，因此亟需一种高效、准确的自我检测机制。

Method: 作者提出Gnosis机制：允许冻结的大语言模型通过被动观察推理时的隐藏状态与注意力模式，压缩成固定结构的描述符，并据此预测输出是否正确。该方法不需改变原模型权重，仅新增约500万参数，且推理代价极低，独立于序列长度。

Result: 在数学推理、开放领域问答、学术知识等任务以及参数规模从1.7B到20B的冻结模型上，Gnosis自我判断正确性的准确率和置信度明显优于强基线和外部评审器。此外，该机制可零样本泛化到部分生成阶段，实现早期故障预警和资源可控。

Conclusion: 生成过程中确实存在可靠的内部正确性信号，只需极低额外开销便可有效提取，为LLM的自我纠错和智能自治提供了高效实用的新途径。

Abstract: Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.

</details>


### [97] [Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs](https://arxiv.org/abs/2512.20595)
*Dhruv Anand,Ehsan Shareghi*

Main category: cs.CL

TL;DR: 论文提出Cube Bench—一个使用魔方任务的基准，以评估多模态大模型在空间和序列推理方面的能力，发现现有模型随着难度增加表现迅速下降，且闭源模型明显优于开源模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型 (MLLMs) 在空间和顺序推理任务的能力评估不够细致与系统，缺乏标准化基准，因此亟需构建一个能全面衡量其复杂推理和操作能力的测试环境。

Method: 作者设计了Cube Bench，以魔方为核心，细分为五项能力测试（如图文还原魔方、选择最优下步、预测操作结果等），并采用相同魔方状态、标准化提示与评测方式，对七种最新MLLMs进行了分层对比。

Result: 结果显示，所有模型在混乱程度增加时准确率大幅下降，轨迹一旦偏离难以恢复；闭源模型在所有任务上显著优于开源模型，开源模型在高难度设定下接近随机水平；自我反思机制带来有限提升，偶有过度思考副作用。

Conclusion: Cube Bench为MLLMs的时空序列推理能力提供了紧凑且可重现的评价利器，目前主流模型仍有很大提升空间，尤其是开源模型与闭源模型差距明显，提示未来改进重点。

Abstract: We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.

</details>


### [98] [MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts](https://arxiv.org/abs/2512.20604)
*Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: 本文提出了MoE-DiffuSeq，一种基于专家混合(MoE)的扩散模型，用于提升长文档生成任务中的效率与表现。


<details>
  <summary>Details</summary>
Motivation: 现有扩散文本生成模型（如DiffuSeq）在处理长序列时效率低下，计算与显存开销巨大，限制了其实用性。

Method: MoE-DiffuSeq结合了稀疏注意力机制与专家混合结构，针对性地设计了稀疏注意力算法以降低复杂度，并在扩散过程中引入软吸收态以加速序列重建和提升生成精度。

Result: 大量实验显示，MoE-DiffuSeq相比传统扩散模型显著提升了训练效率与采样速度，尤其适合科学论文生成、代码库建模及长对话等长文本场景。

Conclusion: MoE-DiffuSeq有效提升了扩散模型在长文档生成中的效率、速度、准确性与表达能力，推动了高质量长文本生成模型的实际应用。

Abstract: We present MoE-DiffuSeq, a mixture of experts based framework for enhancing diffusion models in long document generation. Existing diffusion based text generation models, such as DiffuSeq, suffer from high computational cost and memory overhead when applied to extended sequences. To address these challenges, MoE-DiffuSeq integrates sparse attention with a mixture of experts architecture, enabling efficient and scalable long sequence modeling. Our approach introduces a customized sparse attention mechanism designed to reduce computational complexity while preserving text quality and coherence. In addition, we incorporate a soft absorbing state within the diffusion process to accelerate sequence reconstruction and improve generation precision. Extensive experiments demonstrate that MoE-DiffuSeq significantly improves training efficiency and sampling speed compared to existing diffusion models. These advantages are particularly effective for long document scenarios, including scientific article generation, code repository modeling, and long form dialogue generation. Benchmark results further show that MoE-DiffuSeq improves efficiency, speed, accuracy, and expressiveness, advancing the practical applicability of diffusion models for high quality long form text generation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [99] [Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study](https://arxiv.org/abs/2512.19855)
*Andrew Stirling,Mykola Lukashchuk,Dmitry Bagaev,Wouter Kouw,James R. Forbes*

Main category: cs.RO

TL;DR: 本文对ESGVI（精确稀疏高斯变分推断）算法进行了两方面的扩展，使其更好地应用于含有方向分量的状态估计及复杂噪声环境。方法在UWB定位中验证，提升了定位精度，并提供了开源实现。


<details>
  <summary>Details</summary>
Motivation: 原ESGVI算法主要针对状态估计问题，但局限于欧式空间，难以直接处理含有方向（如旋转）的状态。此外，实际场景如超宽带（UWB）定位常面临非视距和多路径带来的重尾和偏斜噪声，原算法对此适应性有限。

Method: 1）将ESGVI算法推广至矩阵李群，使其能处理带有方向成分的状态估计，符合对象的内在结构；2）在因子分解中引入适应重尾和偏斜分布的因子，更好地适应现实中的噪声特性。

Result: 扩展后的算法在UWB定位实验中进行了验证，面对丰富的NLOS测量时，提高了定位精度，结果表现出了更优的准确性和与原方法相当的一致性。

Conclusion: 本文提出的两项扩展可以有效提升ESGVI算法应用于带有旋转成分及复杂噪声分布的状态估计任务的能力，并保持了原有的稀疏性和无导数特性，对相关领域具有应用推广前景。此外，提供了开源Python实现，便于研究者使用。

Abstract: This letter extends the exactly sparse Gaussian variational inference (ESGVI) algorithm for state estimation in two complementary directions. First, ESGVI is generalized to operate on matrix Lie groups, enabling the estimation of states with orientation components while respecting the underlying group structure. Second, factors are introduced to accommodate heavy-tailed and skewed noise distributions, as commonly encountered in ultra-wideband (UWB) localization due to non-line-of-sight (NLOS) and multipath effects. Both extensions are shown to integrate naturally within the ESGVI framework while preserving its sparse and derivative-free structure. The proposed approach is validated in a UWB localization experiment with NLOS-rich measurements, demonstrating improved accuracy and comparable consistency. Finally, a Python implementation within a factor-graph-based estimation framework is made open-source (https://github.com/decargroup/gvi_ws) to support broader research use.

</details>


### [100] [A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones](https://arxiv.org/abs/2512.19914)
*Sujan Warnakulasooriya,Andreas Willig,Xiaobing Wu*

Main category: cs.RO

TL;DR: 本文提出了一种高效的优先级调度算法，用于提升无人机群集队形初始形成的效率和安全性，实现多达5000架无人机的无碰撞规划，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前无人机集群在初始队形组建时面临效率低、可扩展性差和碰撞风险高等挑战，现有算法在大规模无人机应用中的表现有限。因此，亟需更高效的队形规划方法。

Method: 算法为每架无人机分配基于潜在碰撞数量和是否会阻碍其他无人机到达目标位置的优先级，然后各无人机根据优先级自适应计算延迟，以获得无碰撞路径。

Result: 仿真结果显示，本算法能为高达5000架无人机的大规模集群生成无碰撞轨迹，且在性能和计算效率上都优于基于耦合度的优先级规划（CDH-PP）方法。

Conclusion: 提出的优先级调度算法有效提升了无人机群初始组队的效率和安全性，在大规模应用场景下更加实用，优于现有主流算法。

Abstract: Drone applications continue to expand across various domains, with flocking offering enhanced cooperative capabilities but introducing significant challenges during initial formation. Existing flocking algorithms often struggle with efficiency and scalability, particularly when potential collisions force drones into suboptimal trajectories. This paper presents a time-efficient prioritised scheduling algorithm that improves the initial formation process of drone flocks. The method assigns each drone a priority based on its number of potential collisions and its likelihood of reaching its target position without permanently obstructing other drones. Using this hierarchy, each drone computes an appropriate delay to ensure a collision-free path. Simulation results show that the proposed algorithm successfully generates collision-free trajectories for flocks of up to 5000 drones and outperforms the coupling-degree-based heuristic prioritised planning method (CDH-PP) in both performance and computational efficiency.

</details>


### [101] [Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting](https://arxiv.org/abs/2512.20014)
*Sangoh Lee,Sangwoo Mo,Wook-Shin Han*

Main category: cs.RO

TL;DR: 本文提出了一种无需训练的新方法Visual Attentive Prompting（VAP），通过视觉记忆和注意力机制使机器人能够识别并操控指定用户的个性化物体，即使这些物体在训练时未见过。实验表明VAP显著提升了系统在真实和仿真环境下针对个性化目标的操控能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型（VLA）虽能理解一般指令，但在如“拿我的杯子”此类个性化指令上表现欠佳，因为需要区分外观相似但属于个人的物体。本文旨在让机器人能够泛化到未见过但用户指定的个性化物体操控场景。

Method: 提出了Visual Attentive Prompting（VAP），这是一种基于视觉引用图像作为非参数视觉记忆，通过开放词汇检测和特征匹配，将个性化物体在场景中定位，并将其作为视觉提示嵌入系统，对原始指令进行重写和目标高亮，无需更改原有VLA参数实现定制化感知和操控。

Result: 在两个仿真基准（Personalized-SIMPLER、Personalized-VLABench）和一个真实桌面基准上进行评估。VAP在多个机器人和任务中，相比通用策略和token-learning基线，在任务成功率及正确操控目标物体方面均有显著提升。

Conclusion: VAP作为简单高效且无需训练的感知适配器，为VLA模型带来了实例级别的操控能力，能有效弥补语义理解和实例区分之间的鸿沟，对机器人个性化物体操作具有实际意义。

Abstract: While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as "bring my cup", where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.

</details>


### [102] [LoLA: Long Horizon Latent Action Learning for General Robot Manipulation](https://arxiv.org/abs/2512.20166)
*Xiaofan Wang,Xingyu Gao,Jianlong Fu,Zuolei Li,Dean Fortier,Galen Mullins,Andrey Kolobov,Baining Guo*

Main category: cs.RO

TL;DR: 本文提出了一种名为LoLA的新方法，提升了机器人在长时间、多步操控任务中的表现，通过整合多视角信息和机器人状态，实现更协调、合理的动作序列生成，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在处理需要长期推理和连续动作生成的机器人任务时，往往没有充分利用历史信息，导致在复杂任务中的表现受限。解决这一点有助于提升机器人完成复杂、多步任务的能力。

Method: LoLA框架将长时间、多视角的观察信息和机器人本体状态结合，利用视觉-语言模型提取历史序列和多视角下的上下文特征，并提出“状态感知潜在再表征”模块，将视觉和语言指令转化为可执行的机器人动作空间。不同于以往简单拼接机器人状态，该模块通过可学习的“具身锚定”潜在空间，更有效地将视觉语言信息与物理动作关联。

Result: 在多个机器人预训练数据集上训练，并在模拟基准（SIMPLER和LIBERO）及实物机器人（Franka和Bi-Manual Aloha）上广泛评估，LoLA在长时序操控任务中效果明显优于其他最新方法（如pi0）。

Conclusion: LoLA通过创新的状态感知潜在再表征方法，使得机器人在复杂长序列操控任务中展现出更高的执行能力和动作连贯性，可望推动机器人领域复杂任务的进展。

Abstract: The capability of performing long-horizon, language-guided robotic manipulation tasks critically relies on leveraging historical information and generating coherent action sequences. However, such capabilities are often overlooked by existing Vision-Language-Action (VLA) models. To solve this challenge, we propose LoLA (Long Horizon Latent Action Learning), a framework designed for robot manipulation that integrates long-term multi-view observations and robot proprioception to enable multi-step reasoning and action generation. We first employ Vision-Language Models to encode rich contextual features from historical sequences and multi-view observations. We further introduces a key module, State-Aware Latent Re-representation, which transforms visual inputs and language commands into actionable robot motion space. Unlike existing VLA approaches that merely concatenate robot proprioception (e.g., joint angles) with VL embeddings, this module leverages such robot states to explicitly ground VL representations in physical scale through a learnable "embodiment-anchored" latent space. We trained LoLA on diverse robotic pre-training datasets and conducted extensive evaluations on simulation benchmarks (SIMPLER and LIBERO), as well as two real-world tasks on Franka and Bi-Manual Aloha robots. Results show that LoLA significantly outperforms prior state-of-the-art methods (e.g., pi0), particularly in long-horizon manipulation tasks.

</details>


### [103] [Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation](https://arxiv.org/abs/2512.20188)
*Teqiang Zou,Hongliang Zeng,Yuxuan Nong,Yifan Li,Kehui Liu,Haotian Yang,Xinyang Ling,Xin Li,Lianyang Ma*

Main category: cs.RO

TL;DR: 本文提出了一个真正异步的视觉-语言-行为（VLA）系统框架DuoCore-FS，在保证大模型推理能力的同时，实现了高频高效的动作输出，提升了机器人全身操作的实时性与成功率。


<details>
  <summary>Details</summary>
Motivation: 当前VLA系统因需同步执行视觉语言模型（VLM）和动作专家，受限于VLM推理速度，导致机器人全身操作中的稳定性和实时性能不足。

Method: DuoCore-FS采用快通道（高频动作生成）与慢通道（丰富VLM推理）异步双通道结构。通过潜在表示缓冲区对接快慢通道，整合场景与指令语义信息引导高频动作生成。此外，引入全身动作编码器，形成统一紧凑动作表达。系统端到端联合训练，兼顾统一策略学习与异步推理能力。

Result: 在3B参数量的VLM下，DuoCore-FS实现了30Hz的全身动作生成，速度约为同类模型的三倍。实验证明其全身操作任务成功率和响应速度均优于同步结构的主流基线。

Conclusion: DuoCore-FS打破VLA系统推理瓶颈，显著提升机器人全身操作的实时性与任务完成效果，并已集成至Astribot商用平台，具有较强实际应用价值。

Abstract: Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.

</details>


### [104] [UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas](https://arxiv.org/abs/2512.20224)
*Qijun Qin,Ziqi Zhang,Yihan Zhong,Feng Huang,Xikun Liu,Runzhi Hu,Hang Chen,Wei Hu,Dongzhe Su,Jun Zhang,Hoi-Fung Ng,Weisong Wen*

Main category: cs.RO

TL;DR: 本文介绍了UrbanV2X数据集，这是一套来自香港C-V2X测试平台的、用于智能出行研究的多传感器协作感知数据集，支持车辆与路侧协作导航的研究。数据集包含车载与路侧多种同步传感器数据，并为算法基准测试提供支持。


<details>
  <summary>Details</summary>
Motivation: 目前单车智能局限性明显，全自动驾驶依赖环境感知与信息共享。针对复杂城市环境下车辆与基础设施协作导航的真实世界数据集稀缺这一问题，作者希望填补这一空白，促进C-V2X和自动驾驶相关研究。

Method: 作者在香港C-V2X测试平台上搭建车载和路侧传感器系统，收集同步多源感知数据，包括多路工业相机、激光雷达、4D雷达、UWB、IMU、高精度GNSS-RTK/INS等，并将路侧激光雷达、GNSS和UWB数据与车载数据通过精确时间协议（PTP）同步和标定。最后基于数据集对多种协同导航算法做了基准测试。

Result: 搭建了完备的车辆-路侧多传感器平台，成功采集并同步了大量城市复杂环境下的真实多源数据，验证了多种协作导航算法的效果，并开放数据集供学术界使用。

Conclusion: UrbanV2X数据集为车路协作与智能出行提供了高质量的多传感器同步感知数据，有助于推动密集城市环境下自动驾驶与C-V2X领域的研究与应用。

Abstract: Due to the limitations of a single autonomous vehicle, Cellular Vehicle-to-Everything (C-V2X) technology opens a new window for achieving fully autonomous driving through sensor information sharing. However, real-world datasets supporting vehicle-infrastructure cooperative navigation in complex urban environments remain rare. To address this gap, we present UrbanV2X, a comprehensive multisensory dataset collected from vehicles and roadside infrastructure in the Hong Kong C-V2X testbed, designed to support research on smart mobility applications in dense urban areas. Our onboard platform provides synchronized data from multiple industrial cameras, LiDARs, 4D radar, ultra-wideband (UWB), IMU, and high-precision GNSS-RTK/INS navigation systems. Meanwhile, our roadside infrastructure provides LiDAR, GNSS, and UWB measurements. The entire vehicle-infrastructure platform is synchronized using the Precision Time Protocol (PTP), with sensor calibration data provided. We also benchmark various navigation algorithms to evaluate the collected cooperative data. The dataset is publicly available at https://polyu-taslab.github.io/UrbanV2X/.

</details>


### [105] [KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System](https://arxiv.org/abs/2512.20299)
*Zhongyu Xia,Wenhao Chen,Yongtao Wang,Ming-Hsuan Yang*

Main category: cs.RO

TL;DR: 本文提出了一种结合视觉-语言推理、知识检索和价值对齐的新型自动驾驶系统KnowVal，有效提升了规划性能，并在权威数据集上取得了最佳或先进的表现。


<details>
  <summary>Details</summary>
Motivation: 当前高级自动驾驶系统难以通过模仿或有限奖励学习复杂的决策逻辑，缺乏对知识和价值驱动逻辑的深层次建模。

Method: 提出KnowVal系统，集成开放世界视觉感知与驾驶知识检索，构建涵盖交通法规、防御性驾驶原则及伦理规范的驾驶知识图谱，利用高效的大模型驱动检索机制；开发人类偏好数据集并训练价值模型引导可解释、价值一致性的轨迹评估。

Result: 在nuScenes数据集上达到最低碰撞率，在Bench2Drive取得SOTA表现，同时系统兼容现有自动驾驶架构。

Conclusion: KnowVal系统能够有效提升自动驾驶决策的安全性和价值遵循性，为未来高级自动驾驶系统的开发提供了新方向。

Abstract: Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.

</details>


### [106] [Pneumatic bladder links with wide range of motion joints for articulated inflatable robots](https://arxiv.org/abs/2512.20322)
*Katsu Uchiyama,Ryuma Niiyama*

Main category: cs.RO

TL;DR: 该论文提出了一种基于多气囊关节的新型充气机器人结构，并通过实验展示了其运动能力和负载能力。


<details>
  <summary>Details</summary>
Motivation: 充气机器人因其轻便和安全性而受到关注，但传统充气关节活动范围有限，应用受限。作者旨在拓展充气机器人在各类运动和承载应用中的潜力。

Method: 设计了一种由多个气囊和滚动接触Hillberry关节组成的充气机器人，每个气囊由防水帆布和聚氨酯双层结构制成，结合创新的Hilberry滚动关节以提升运动范围。通过构建不同自由度的手臂和腿部样机并进行载荷、运动测试。

Result: 采用该结构的3自由度机械臂可以搬运500克负载，2自由度机械臂可举起3.4千克，1自由度则可举起5千克。同时，3自由度充气腿样机实现了基本的腿部运动验证。

Conclusion: 所提出的气囊+Hillberry关节结构大大拓展了充气机器人运动范围和承载能力，验证了其在机器人臂和腿等多种应用场景的可行性和潜力。

Abstract: Exploration of various applications is the frontier of research on inflatable robots. We proposed an articulated robots consisting of multiple pneumatic bladder links connected by rolling contact joints called Hillberry joints. The bladder link is made of a double-layered structure of tarpaulin sheet and polyurethane sheet, which is both airtight and flexible in shape. The integration of the Hilberry joint into an inflatable robot is also a new approach. The rolling contact joint allows wide range of motion of $\pm 150 ^{\circ}$, the largest among the conventional inflatable joints. Using the proposed mechanism for inflatable robots, we demonstrated moving a 500 g payload with a 3-DoF arm and lifting 3.4 kg and 5 kg payloads with 2-DoF and 1-DoF arms, respectively. We also experimented with a single 3-DoF inflatable leg attached to a dolly to show that the proposed structure worked for legged locomotion.

</details>


### [107] [FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration](https://arxiv.org/abs/2512.20355)
*Hao Wei,Peiji Wang,Qianhao Wang,Tong Qin,Fei Gao,Yulin Si*

Main category: cs.RO

TL;DR: 本论文提出了FAR-AVIO，一种基于舒尔补（Schur-Complement）和扩展卡尔曼滤波（EKF）的水下机器人声-视觉-惯性紧耦合里程计系统，兼顾高准确性和实时性能，并通过AWARE模块自适应调整多传感器权重，实验证明比现有方法更优。


<details>
  <summary>Details</summary>
Motivation: 水下环境对视觉-惯性里程计系统造成极大挑战，现有声-视觉-惯性融合方法因计算量大难以实时运行，限制了其在嵌入式低功耗平台上的应用。需要一种既高效又精确的融合方法，解决恶劣水下环境下的定位鲁棒性问题。

Method: 提出了一种将舒尔补嵌入EKF实现紧耦合声-视觉-惯性融合，通过高效地边缘化路标状态以实现常时间（constant-time）更新。引入AWARE模块，在线评估和调整视觉、惯性和DVL各传感器的可靠性，通过动态调整权重来提升系统稳定性。同时，提出在线联合标定DVL与IMU外参的方法，无需专门的标定动作。

Result: 数值仿真和真实水下实验表明，FAR-AVIO在定位精度和计算效率上均优于当前主流的水下SLAM系统，能够在低功耗嵌入式平台上实现鲁棒、长时间稳定运行。

Conclusion: FAR-AVIO为水下机器人在光照差、浑浊和信号不理想环境下提供了鲁棒且高效的状态估计解决方案，有望推广到各种资源受限的水下应用场景。其开源实现为后续研究带来便利。

Abstract: Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation. While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms. Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots. FAR-AVIO embeds a Schur complement formulation into an Extended Kalman Filter(EKF), enabling joint pose-landmark optimization for accuracy while maintaining constant-time updates by efficiently marginalizing landmark states. On top of this backbone, we introduce Adaptive Weight Adjustment and Reliability Evaluation(AWARE), an online sensor health module that continuously assesses the reliability of visual, inertial and DVL measurements and adaptively regulates their sigma weights, and we develop an efficient online calibration scheme that jointly estimates DVL-IMU extrinsics, without dedicated calibration manoeuvres. Numerical simulations and real-world underwater experiments consistently show that FAR-AVIO outperforms state-of-the-art underwater SLAM baselines in both localization accuracy and computational efficiency, enabling robust operation on low-power embedded platforms. Our implementation has been released as open source software at https://far-vido.gitbook.io/far-vido-docs.

</details>


### [108] [Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing](https://arxiv.org/abs/2512.20475)
*Maulana Bisyir Azhari,Donghun Han,Je In You,Sungjun Park,David Hyunchul Shim*

Main category: cs.RO

TL;DR: 本文介绍了一套在A2RL与DCL比赛中表现突出的单目无人机自主竞速系统，采用YOLO门检测结合卡尔曼滤波器修正VIO漂移，并用感知驱动规划器平衡速度与感知需求，多项赛事中获奖。


<details>
  <summary>Details</summary>
Motivation: 该比赛仅允许使用单目摄像头和低质量惯导，模拟人类高手实际竞速感知条件，极大限制无人机感知精度。因此，提升此类极简传感系统下的无人机自主竞速性能成为研究难题。

Method: 系统采用YOLO门检测算法获取全局位置信息，与VIO估计值结合卡尔曼滤波进行漂移修正；同时设计了感知驱动的轨迹规划器，在保证速度的同时尽量让无人机保持对门的可见性。

Result: 系统在多项赛事中名列前茅：AI大挑战赛第三名（43.2km/h）、AI直线竞速第二名（59km/h+）、AI多机竞速第二名。

Conclusion: 本文提出的融合视觉门检测与传统VIO的方法有效提升了极简感知下自主竞速无人机的表现，并提出了感知驱动规划的设计思路，为未来单目自主飞行系统提供了有价值的经验和方法。

Abstract: The Abu Dhabi Autonomous Racing League(A2RL) x Drone Champions League competition(DCL) requires teams to perform high-speed autonomous drone racing using only a single camera and a low-quality inertial measurement unit -- a minimal sensor set that mirrors expert human drone racing pilots. This sensor limitation makes the system susceptible to drift from Visual-Inertial Odometry (VIO), particularly during long and fast flights with aggressive maneuvers. This paper presents the system developed for the championship, which achieved a competitive performance. Our approach corrected VIO drift by fusing its output with global position measurements derived from a YOLO-based gate detector using a Kalman filter. A perception-aware planner generated trajectories that balance speed with the need to keep gates visible for the perception system. The system demonstrated high performance, securing podium finishes across multiple categories: third place in the AI Grand Challenge with top speed of 43.2 km/h, second place in the AI Drag Race with over 59 km/h, and second place in the AI Multi-Drone Race. We detail the complete architecture and present a performance analysis based on experimental data from the competition, contributing our insights on building a successful system for monocular vision-based autonomous drone flight.

</details>


### [109] [LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing](https://arxiv.org/abs/2512.20591)
*Changyi Lin,Boda Huo,Mingyang Yu,Emily Ruppel,Bingqing Chen,Jonathan Francis,Ding Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种名为LightTact的新型视觉-触觉指尖传感器，能够在不依赖表面形变的情况下实现高精度的接触感知，特别适用于软物体、液体等轻触场景。


<details>
  <summary>Details</summary>
Motivation: 大多数现有触觉传感器依赖表面形变来检测接触，但在与液体、半液体或超软材料的轻触交互中，往往不会产生明显变形，导致感知困难。因此亟需一种对表面形变不敏感、能准确检测轻触的传感器。

Method: 提出了一种以光学原理为基础、不依赖形变的视觉-触觉指尖传感器（LightTact）。其创新性的光学结构能屏蔽外界和非接触区域的光，仅让真实接触区域的散射光通过，实现高对比度成像，从而实现准确的像素级接触分割。

Result: LightTact能够准确区分接触与非接触区域，对不同材料、接触力、表面外观以及环境光都具有很强的鲁棒性。此外，LightTact已集成在机器人手臂上，展示了对极轻触操作（如水的铺展、面霜蘸取、薄膜操作）的出色表现，并能与视觉-语言模型结合实现复杂任务（如电阻分拣）。

Conclusion: LightTact为机器人在需要极轻接触的场景中提供了高精度、强鲁棒性的触觉感知能力，将推动机器人与软物体、液体等复杂环境的交互水平，并可与AI模型直接结合，扩展应用场景。

Abstract: Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value < 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.

</details>
