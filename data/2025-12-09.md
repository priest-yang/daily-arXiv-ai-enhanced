<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 225]
- [cs.CL](#cs.CL) [Total: 74]
- [cs.RO](#cs.RO) [Total: 62]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices](https://arxiv.org/abs/2512.05969)
*Hokin Deng*

Main category: cs.CV

TL;DR: 本论文展示了视频生成模型在推理能力上的新进展，开发了新的评估范式并提供了可扩展的实验框架。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在复杂推理任务上的能力尚未被系统性评测和挖掘，研究动机是评估和推进视频生成模型在推理任务上的表现。

Method: 论文提出“Task Pair”实验范式，涵盖棋类、迷宫、数独、心智旋转及Raven推理矩阵等任务，并开发了支持39个模型、易于拓展的代码框架。同时，通过自动化评估与人工评估的高相关性验证方案的鲁棒性和可扩展性。

Result: 主流视频模型如Sora-2在这些推理任务上成功率可达60%。同时，自动化评估结果与人工评估高度相关，验证了评估范式的有效性。

Conclusion: 该工作证明了视频生成模型已具备一定的推理能力，提出了通用、可扩展的推理评测方案，为进一步用强化学习提升视频模型推理能力奠定基础，并开放了相关数据与代码。

Abstract: We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the "Task Pair" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.

</details>


### [2] [Adaptive Dataset Quantization: A New Direction for Dataset Pruning](https://arxiv.org/abs/2512.05987)
*Chenyue Yu,Jianyu Yu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的数据集量化方法，针对边缘设备的大规模数据存储和通信成本高的问题，压缩每个样本的冗余内容，从而在保证训练性能的前提下极大减少数据集体积。


<details>
  <summary>Details</summary>
Motivation: 随着大规模数据集广泛应用，资源有限的边缘设备面临存储与通信瓶颈，亟须有效的数据压缩方法。传统的数据集剪枝和蒸馏侧重于样本间冗余，但未充分利用单个样本内部冗余空间，有进一步压缩潜力。

Method: 首先对每个样本应用线性对称量化，得到初始量化范围与缩放系数。然后通过自适应量化分配算法，根据不同样本对精度的需求分配不同的量化比率，总体保证数据集压缩比恒定。该方法用有限比特数对数据集编码，并在数据集层面自适应分配压缩率。

Result: 在CIFAR-10、CIFAR-100、ImageNet-1K等公开数据集上进行实验，结果显示方法在保持模型训练性能的情况下，实现了显著的数据集压缩效果，并在相同比例下优于传统量化与剪枝方法。

Conclusion: 提出的方法能大幅减小数据集存储和通信成本，在资源受限场景下有实际应用价值，并且为数据集层面的进一步量化压缩研究提供了新思路。

Abstract: This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.

</details>


### [3] [VG3T: Visual Geometry Grounded Gaussian Transformer](https://arxiv.org/abs/2512.05988)
*Junho Kim,Seongwon Lee*

Main category: cs.CV

TL;DR: 本文提出了VG3T方法，能高效地从多视角图像预测统一的3D语义表达，并在nuScenes基准上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角融合时存在碎片化和性能不足的问题，缺乏统一、连贯的3D场景表达方式。

Method: 提出VG3T多视角前馈网络，能联合预测3D语义高斯表达，并引入基于网格的采样与位置微调技术，以解决高斯初始化时的密度偏置问题。该方法摒弃了以往单视图推理方式，改为直接多视角联合建模。

Result: 在nuScenes基准上，VG3T的mIoU提升了1.7个百分点，用到的3D基元数减少了46%，表现出更高效、更优异的性能。

Conclusion: VG3T实现了高效且统一的3D语义场景表示，克服了以往多视角融合的碎片化及效率问题，在准确率和资源利用上优于现有方法。

Abstract: Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.

</details>


### [4] [EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head](https://arxiv.org/abs/2512.05991)
*Chang Liu,Tianjiao Jing,Chengcheng Ma,Xuanqi Zhou,Zhengxuan Lian,Qin Jin,Hongliang Yuan,Shi-Sheng Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为EmoDiffTalk的新型可编辑3D高斯人头系统，实现了基于多模态控制的高质量、细粒度、高可控情感编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D Gaussian Splatting的3D写实人头在情感表达操控方面存在明显短板，尤其是在实现细腻、丰富的情感动态编辑和多模态控制方面能力不足。

Method: 提出了Emotion-aware Gaussian Diffusion方法，包括基于动作单元（AU）的高斯扩散过程用于精细化的面部动画，以及准确的文本到AU的情绪控制器，使得可用文本输入完成动态情感编辑。

Result: 在EmoTalk3D和RenderMe-360等公开数据集上，EmoDiffTalk在人类情感细腻度、口型同步性和可控性上均优于先前方法。

Conclusion: EmoDiffTalk为高质量、基于扩散、多模态的可编辑3D人头合成提供了有效途径，是少数支持基于AU空间连续、多模态情感编辑的3D Gaussian Splatting方法之一。

Abstract: Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.

</details>


### [5] [Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology](https://arxiv.org/abs/2512.05993)
*Ruchika Verma,Shrishtee Kandoi,Robina Afzal,Shengjia Chen,Jannes Jegminat,Michael W. Karlovich,Melissa Umphlett,Timothy E. Richardson,Kevin Clare,Quazi Hossain,Jorge Samanamud,Phyllis L. Faust,Elan D. Louis,Ann C. McKee,Thor D. Stein,Jonathan D. Cherry,Jesse Mez,Anya C. McGoldrick,Dalilah D. Quintana Mora,Melissa J. Nirenberg,Ruth H. Walker,Yolfrankcis Mendez,Susan Morgello,Dennis W. Dickson,Melissa E. Murray,Carlos Cordon-Cardo,Nadejda M. Tsankova,Jamie M. Walker,Diana K. Dangoor,Stephanie McQuillan,Emma L. Thorn,Claudia De Sanctis,Shuying Li,Thomas J. Fuchs,Kurt Farrell,John F. Crary,Gabriele Campanella*

Main category: cs.CV

TL;DR: 该论文提出了针对脑神经退行性病变的领域专用基础模型NeuroFM，相比通用基础模型，在神经病理特定任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型主要在外科病理数据训练，缺乏对神经病理（如阿尔茨海默病、帕金森病等）独有特征的捕捉能力，影响神经退行性疾病的精准解读。针对这一领域失配问题，有开发神经病理专属模型的必要。

Method: 作者开发了NeuroFM，一个专门在多种神经退行性病变脑组织全切片图像上训练的基础模型，并与通用模型在神经病理相关下游任务上进行了比较。

Result: NeuroFM在多种神经病理下游任务（如混合性痴呆分类、海马分区分割、小脑共济失调亚型识别等）均优于通用模型。

Conclusion: 针对神经病理领域特征设计并训练的基础模型，可以比通用病理模型更准确地捕捉神经疾病关键形态学特征，有助于脑疾病AI诊断与研究，也为领域专用模型的发展提供了范例。

Abstract: Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.

</details>


### [6] [FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting](https://arxiv.org/abs/2512.05996)
*Yi Liu,Jingyu Song,Vedanth Kallakuri,Katherine A. Skinner*

Main category: cs.CV

TL;DR: 本文提出了FishDetector-R1，一个基于多模态大语言模型（MLLM）的弱监督水下鱼类检测、分割与计数框架，在DeepFish数据集上表现优异，并具备良好的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 水下鱼类图像分析具有生态监测的重要意义，但由于图像质量退化和标注成本高，现有方法难以取得突破。因此，需要一种能在标注稀缺情况下高效、准确处理鱼类检测和计数的方法。

Method: 作者提出了FishDetector-R1框架，包含两大核心创新：一是设计了新的detect-to-count提示词机制，实现检测与计数空间一致性；二是基于可验证奖励的强化学习（RLVR），结合稀疏点标签实现高效弱监督学习。此外，通过消融实验分析奖励机制的有效性。

Result: 在DeepFish数据集上，FishDetector-R1相较基线AP提高20%、mIoU提升10%，MAE减少30%、GAME降低35%；方法在其他水下数据集上同样展现出强大的泛化性。

Conclusion: FishDetector-R1能在弱监督条件下有效提升水下鱼类检测、分割和计数的性能，具备可靠性和可扩展性，为海洋视觉理解提供了新颖实用的解决方案。

Abstract: Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.

</details>


### [7] [PrunedCaps: A Case For Primary Capsules Discrimination](https://arxiv.org/abs/2512.06003)
*Ramin Sharifi,Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: 本文提出了一种通过修剪Capsule Networks（CapsNets）中Primary Capsules来提升网络性能的方法，在大幅提高速度和减少计算成本的同时，几乎不损失准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然CapsNets在处理仿射变换和重叠图像时优于CNNs，但由于Primary Capsules数量太多，导致其计算和资源开销较大，难以高效部署。因此，作者希望通过修剪来提升其资源效率。

Method: 作者在MNIST、Fashion-MNIST、CIFAR-10和SVHN数据集上，研究对Primary Capsules进行修剪的方法，分析哪些胶囊对性能影响大，对不重要的胶囊进行移除，比较修剪前后的速度和准确率等指标。

Result: 经过修剪，CapsNet在去除95% Primary Capsules的情况下，推理速度提升至原来的9.9倍，动态路由阶段的浮点运算量减少了95.36%，而准确率基本未下降。

Conclusion: 通过大规模修剪Primary Capsules，可以显著提升CapsNet的推理效率和资源利用率，且不会损失性能，这使得CapsNet更适合资源受限环境。同时，作者发现不同数据集合对修剪的敏感性有所不同。

Abstract: Capsule Networks (CapsNets) are a generation of image classifiers with proven advantages over Convolutional Neural Networks (CNNs). Better robustness to affine transformation and overlapping image detection are some of the benefits associated with CapsNets. However, CapsNets cannot be classified as resource-efficient deep learning architecture due to the high number of Primary Capsules (PCs). In addition, CapsNets' training and testing are slow and resource hungry. This paper investigates the possibility of Primary Capsules pruning in CapsNets on MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN datasets. We show that a pruned version of CapsNet performs up to 9.90 times faster than the conventional architecture by removing 95 percent of Capsules without a loss of accuracy. Also, our pruned architecture saves on more than 95.36 percent of floating-point operations in the dynamic routing stage of the architecture. Moreover, we provide insight into why some datasets benefit significantly from pruning while others fall behind.

</details>


### [8] [Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization](https://arxiv.org/abs/2512.06006)
*Xuefei,Wang,Kai A. Horstmann,Ethan Lin,Jonathan Chen,Alexander R. Farhang,Sophia Stiles,Atharva Sehgal,Jonathan Light,David Van Valen,Yisong Yue,Jennifer J. Sun*

Main category: cs.CV

TL;DR: 本文提出了一种利用AI代理自动化定制科学数据集计算机视觉工具适配的方案，并通过系统性框架评估三大生产级生物医学影像管线。结果显示简单的AI代理优于人类专家，复杂代理架构并不总是最佳选择，论文也开源了相关框架并在实际生产环境验证了效果。


<details>
  <summary>Details</summary>
Motivation: 当前现有的计算机视觉工具要适配到特定科学数据集非常困难。现有方法要么需要大量标注数据（科学家难以获得），要么需要手工改代码（耗时周长）。因此需要更自动化、高效的适配方案。

Method: 作者提出了针对代码自动化优化的计算代理设计问题，搭建了系统性评估框架，专门针对AI代理自动生成适配代码能力进行实验，测试了三种生物医学影像处理管线。同时，对各种代理架构（简单到复杂）进行了对比分析。

Result: 实验证明，简单的AI代理在生成生产管线适配代码时，表现普遍超过人类专家。进一步发现，复杂的代理结构（比如更深层次、更多功能）并不总是更优。

Conclusion: 论文提出了一种实用的设计路线，用简单的AI代理即可以高效进行管线适配，节省科学家大量时间。开源了评估框架，并实际部署了代理生成的代码，证实该方法有现实应用价值。

Abstract: Adapting production-level computer vision tools to bespoke scientific datasets is a critical "last mile" bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.

</details>


### [9] [Fast and Flexible Robustness Certificates for Semantic Segmentation](https://arxiv.org/abs/2512.06010)
*Thomas Massena,Corentin Friedrich,Franck Mamalet,Mathieu Serrurier*

Main category: cs.CV

TL;DR: 本文提出了一种具有Lipschitz约束、能够认证鲁棒性的语义分割网络，相比主流方法显著提升了效率，同时在像Cityscapes这样的高难度数据集上也保持了竞争性的像素准确率，实现了首次可用于实时的认证鲁棒语义分割。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络对小扰动非常敏感，现有的鲁棒性增强或认证方法主要聚焦于分类任务，且很少有高效的语义分割认证手段。因此，亟需一种能兼顾效率与认证鲁棒性的语义分割新方法。

Method: 构建嵌入Lipschitz约束的神经网络架构，实现可认证的鲁棒语义分割，并提出一个泛化的鲁棒性认证框架，提升结果的灵活性和效率，还能度量多种指标下的最坏情况性能。

Result: 该方法在Cityscapes等挑战性数据集上取得了与现有技术相当的像素级准确率，其认证过程在NVIDIA A100 GPU上推理速度约为随机平滑方法的600倍，并对最坏情况认证结果与现有攻击作了紧密性评估。

Conclusion: 本文首次实现了可用于实时场景的认证鲁棒语义分割方法，同时大幅提升了认证效率，为大规模实用化提供了可行路线。

Abstract: Deep Neural Networks are vulnerable to small perturbations that can drastically alter their predictions for perceptually unchanged inputs. The literature on adversarially robust Deep Learning attempts to either enhance the robustness of neural networks (e.g, via adversarial training) or to certify their decisions up to a given robustness level (e.g, by using randomized smoothing, formal methods or Lipschitz bounds). These studies mostly focus on classification tasks and few efficient certification procedures currently exist for semantic segmentation. In this work, we introduce a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable and achieve competitive pixel accuracy on challenging datasets such as Cityscapes. Additionally, we provide a novel framework that generalizes robustness certificates for semantic segmentation tasks, where we showcase the flexibility and computational efficiency of using Lipschitz networks. Our approach unlocks real-time compatible certifiably robust semantic segmentation for the first time. Moreover, it allows the computation of worst-case performance under $\ell_2$ attacks of radius $ε$ across a wide range of performance measures. Crucially, we benchmark the runtime of our certification process and find our approach to be around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU. Finally, we evaluate the tightness of our worstcase certificates against state-of-the-art adversarial attacks to further validate the performance of our method.

</details>


### [10] [High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing](https://arxiv.org/abs/2512.06012)
*Emmanuel Akeweje,Conall Kirk,Chi-Wai Chan,Denis Dowling,Mimi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种自动化机器学习框架，实现了对金属粉末形貌的大规模高通量分析，显著提高了选择性激光熔化(SLM)过程中原料质量评估的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有的粉末表征方法通量低、结果定性，难以全面、定量反映工业批量粉末形貌的异质性，亟需高效、自动化、定量的新方法。

Method: 作者将高通量成像、形状特征提取与聚类算法集成，设计并对比了三种聚类流程（自编码器、形状描述符、函数型数据），在大规模（约12.6万张）不同粒径金属粉末图像上进行分析，并通过内部指标评价聚类表现。

Result: 结果显示，利用傅里叶描述符加k均值算法的流程在Davies-Bouldin指数和Calinski-Harabasz分数上表现最佳，并且每个颗粒的聚类计算时间小于1毫秒。该方法能够建立粉末形态分组，为后续探究其与流动性、堆积密度及SLM零件质量的关系打下基础。

Conclusion: 该无监督学习框架大幅提升了粉末形貌的自动化评估与追踪能力，并具备实时在线监控SLM原料状态的潜力，有助于提高SLM工艺的生产质量和效率。

Abstract: Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.

</details>


### [11] [VAT: Vision Action Transformer by Unlocking Full Representation of ViT](https://arxiv.org/abs/2512.06013)
*Wenhao Li,Chengwei Ma,Weixin Mao*

Main category: cs.CV

TL;DR: 本文提出了一种新型的视觉感知与动作融合模型VAT（Vision Action Transformer），通过贯穿利用ViT的所有层级特征，在机器人模仿学习任务上实现了超过以往方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大部分基于ViT的视觉感知方法只利用最后一层的特征，丢弃了前面层的有价值信息，导致表征能力不足。作者希望通过充分利用ViT所有层的特征，提升机器人决策能力。

Method: 提出Vision Action Transformer（VAT）架构，在ViT网络的每层加入动作token，与视觉特征进行逐层融合，实现感知与动作生成的深度耦合。VAT可以逐步融合来自各层视觉信息，改善对任务关键要素的表达。

Result: VAT在四个LIBERO的仿真操作基准任务中取得了平均98.15%的成功率，显著超过了OpenVLA-OFT等其他已有方法，创造了当前的最新性能记录。

Conclusion: 全面利用ViT全部层级的表示能大幅提升机器人感知与动作决策模型的性能，VAT为模仿学习领域提供了新的强大工具，并验证了表征轨迹完整性的重要性。

Abstract: In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.

</details>


### [12] [Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets](https://arxiv.org/abs/2512.06014)
*Jiho Shin,Dominic Marshall,Matthieu Komorowski*

Main category: cs.CV

TL;DR: 本文对两种大型胸部X光（CXR）基础模型（CXR-Foundation和MedImageInsight）在不同公开数据集上的表现进行了系统对比，发现各自有优劣，并为后续研究提供了标准化评估基线。


<details>
  <summary>Details</summary>
Motivation: 尽管医学图像基础模型表现优秀，但其在不同数据集间的比较研究较少，缺乏统一评估方法。作者旨在填补这一空白，推动该领域的标准化和可重复性研究。

Method: 作者选用两种主流基础CXR模型，对其在MIMIC-CR和NIH ChestX-ray14两大公开数据集上的表现进行基准测试。为保证公平和可复现，采用统一的预处理流程和固定下游分类器（LightGBM），通过提取模型预训练编码器的特征嵌入来评估多类疾病标签的识别能力。

Result: MedImageInsight模型在绝大多数任务上表现略优，但CXR-Foundation在跨数据集泛化能力上表现更为稳定。此外，无监督聚类分析发现，MedImageInsight的特征嵌入可分出与疾病相关的内在结构，验证结果一致性。

Conclusion: 结果凸显了医学基础模型标准化评估的必要性，并建立了后续多模态和临床融合研究的可重复性能基线。

Abstract: Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.

</details>


### [13] [PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation](https://arxiv.org/abs/2512.06020)
*Wenyi Mo,Tianyu Zhang,Yalong Bai,Ligong Han,Ying Ba,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: 本论文提出了一种结合多模态大模型(MLLM)与扩散生成模型的个性化图像生成框架，通过捕捉和注入用户偏好，实现更好地契合个人审美的图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于用户偏好的图像生成方法，要么无法捕捉细腻的个体偏好，要么难以有效编码个性化的视觉信号，因此迫切需要提升图像生成对用户偏好的契合度和多样性。

Method: 提出利用多模态大语言模型进行视觉问答训练，提取包含用户细粒度偏好的表征；引入区分不同用户和判别喜欢/不喜欢内容的两项probe任务，以提取更相关的特征；再用最大平均差异对齐损失，把多模态表征与扩散文本编码器对齐，从而精准注入到图像生成器。

Result: 大量实验表明，所提方法在图像质量和对用户偏好的契合程度上，明显优于现有强基线，验证了所提表征提取与对齐机制的有效性。

Conclusion: 本方法展现了多模态表征提取和对齐在个性化图像生成中的巨大潜力，可显著提升生成结果对用户提示语和个性偏好的满足度。

Abstract: Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.

</details>


### [14] [Neural reconstruction of 3D ocean wave hydrodynamics from camera sensing](https://arxiv.org/abs/2512.06024)
*Jiabin Liu,Zihao Zhou,Jialei Yan,Anxin Guo,Alvise Benetazzo,Hui Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于注意力的金字塔神经网络，用于高效、精确地重建海洋波浪自由面和相应的三维速度场，适应多尺度、时间连续性并克服遮挡问题，在真实海况下展现出毫米级精度和显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前对海洋波浪表面的三维重建在长期观测任务中计算量大，且视觉遮挡严重影响重建质量，因此需要一种高效且鲁棒的新方法。

Method: 作者设计了一种注意力增强金字塔式的神经网络结构，结合物理约束条件实现了海浪自由面的时间分辨三维重建，并通过多尺度和时序特性强化对波浪动态的学习。

Result: 实验证明，该方法在真实海况下可实现毫米级的波浪高程预测，主频误差小于0.01 Hz，精确估算高频谱功率定律，能在1.35秒内密集重建200万个点的三维速度场，超越传统视觉重建方法，对遮挡环境也有良好的泛化能力。

Conclusion: 文中提出的神经网络方法不仅大幅提升了三维重建的精度与效率，还具备优异的泛化与抗遮挡能力，对海洋物理研究和长期波浪观测具有重要意义。

Abstract: Precise three-dimensional (3D) reconstruction of wave free surfaces and associated velocity fields is essential for developing a comprehensive understanding of ocean physics. To address the high computational cost of dense visual reconstruction in long-term ocean wave observation tasks and the challenges introduced by persistent visual occlusions, we propose an wave free surface visual reconstruction neural network, which is designed as an attention-augmented pyramid architecture tailored to the multi-scale and temporally continuous characteristics of wave motions. Using physics-based constraints, we perform time-resolved reconstruction of nonlinear 3D velocity fields from the evolving free-surface boundary. Experiments under real-sea conditions demonstrate millimetre-level wave elevation prediction in the central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, and high-fidelity 3D reconstruction of nonlinear velocity fields, while enabling dense reconstruction of two million points in only 1.35 s. Built on a stereo-vision dataset, the model outperforms conventional visual reconstruction approaches and maintains strong generalization in occluded conditions, owing to its global multi-scale attention and its learned encoding of wave propagation dynamics.

</details>


### [15] [The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation](https://arxiv.org/abs/2512.06032)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.CV

TL;DR: 本文对最新的两个Segment Anything Models（SAM2和SAM3）进行了对比分析，揭示了它们之间在结构和方法上的根本性断裂。


<details>
  <summary>Details</summary>
Motivation: 分析SAM2与SAM3之间为什么存在迁移障碍，以及这种断裂背后的架构和实现动因，从而帮助理解从基于提示的分割向概念驱动、跨模态分割的转变。

Method: 论文分五个方面对SAM2与SAM3进行了系统性分析，分别是：1）从提示驱动到概念驱动的分割方式变化；2）模型架构的差异和新增模块（如多模态融合、DETR风格解码器等）；3）使用数据集和注释类型的不同；4）训练和超参数等优化方式差异；5）评估指标与失效模式的变化。

Result: 分析显示，SAM3通过引入统一的视觉-语言结构，实现开放词汇分割、语义对齐、多模态推理等显著功能，且不再适用SAM2的优化和评估经验。

Conclusion: SAM3代表了分割基础模型的新范式，未来分割领域将朝向概念驱动、多模态理解方向发展，提示基础到概念基础的转变已开启新的研究篇章。

Abstract: This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.

</details>


### [16] [Representation Learning for Point Cloud Understanding](https://arxiv.org/abs/2512.06058)
*Siming Yan*

Main category: cs.CV

TL;DR: 本论文提出了结合2D预训练模型以提升3D点云表示学习的新方法，显著增强了3D数据处理能力。


<details>
  <summary>Details</summary>
Motivation: 随着3D数据在各领域应用的增加，如何更好地理解和利用点云数据成为重要课题，尤其是在机器环境感知等实际应用中。

Method: 论文围绕点云原语分割的监督表示学习、自监督学习方法，以及2D到3D的迁移学习展开，提出通过集成和迁移2D预训练模型知识来改进3D网络训练。

Result: 实验结果显示，该方法在多个任务上有效提升了3D表示学习的效果，证明了2D知识集成对3D理解的促进作用。

Conclusion: 通过融合2D领域的预训练经验，能够在无需简单地将2D转换为3D的前提下，实质性提升3D点云的理解和处理能力，并为相关应用带来实际进步。

Abstract: With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.

</details>


### [17] [EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing](https://arxiv.org/abs/2512.06065)
*Runjia Li,Moayed Haji-Ali,Ashkan Mirzaei,Chaoyang Wang,Arpit Sahni,Ivan Skorokhodov,Aliaksandr Siarohin,Tomas Jakab,Junlin Han,Sergey Tulyakov,Philip Torr,Willi Menapace*

Main category: cs.CV

TL;DR: 本论文提出了针对第一视角（egocentric）视频的交互式增强现实（AR）应用编辑系统，包括新数据集、实时编辑器及评测基准，有效提升了此类视频的编辑性能和实时性。


<details>
  <summary>Details</summary>
Motivation: 随着AR和第一视角视频的流行，现有AI视频编辑器多针对第三视角，难以应对第一视角下快速运动和手物交互的复杂情况，且传统离线编辑流程延迟高，不适用于实时交互需求。因此，迫切需要专门面向第一视角视频、兼具实时性和编辑质量的完整解决方案。

Method: 论文构建了首个侧重手物交互且显式保留手部信息的第一视角编辑数据集EgoEditData；开发了支持自然语言指令、可在单GPU上实时推理的编辑器EgoEdit；并提出了覆盖指令遵循性、手物保留与时序稳定性的系统评测基准EgoEditBench。

Result: EgoEdit能够在第一视角及普通视频编辑任务中，实现时序稳定、指令遵循、互动延迟低的编辑效果。在专门的第一视角编辑基准上取得了明显优势，而在一般编辑任务中表现也与主流方法相当。

Conclusion: 文中提出的EgoEdit系统与数据集显著提升了第一视角视频编辑的可用性和交互体验，为AR应用提供了有力支持，同时相关数据与评测资源将面向科研社区开放，推动该领域发展。

Abstract: We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit

</details>


### [18] [Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light](https://arxiv.org/abs/2512.06080)
*Tzofi Klinghoffer,Siddharth Somasundaram,Xiaoyu Xiang,Yuchen Fan,Christian Richardt,Akshat Dave,Ramesh Raskar,Rakesh Ranjan*

Main category: cs.CV

TL;DR: 本文提出了一种基于单光子激光雷达的新方法，实现了从单次测量中重建包含遮挡与镜面材料的复杂3D场景。作者通过模拟数据和数据驱动模型，成功分解了多路径反射光，恢复了场景的深度和几何信息。


<details>
  <summary>Details</summary>
Motivation: 单次测量恢复3D场景在存在遮挡和镜面材料时极具挑战。传统单光子雷达方法仅能处理逐点激光照射，难以应对实际中更复杂的多点同时照射与多次反射情形。

Method: 作者提出了一种数据驱动的光传输反演方法，利用新构建的大规模模拟数据集（约10万组），学习复杂光传输的先验，从而分离单次测量获得的多跳反射光，提取不同激光点的贡献。

Result: 实验结果表明，该方法可从单次测量中成功恢复包含遮挡和镜面反射的场景3D几何结构，优于以往逐点扫描方案。相关代码与数据集已开源。

Conclusion: 该研究突破了单光子雷达3D重建在复杂场景（如有镜面、遮挡作用）下的限制，提出的数据驱动反演方案在实际应用中更具可行性和效率。

Abstract: 3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.

</details>


### [19] [BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving](https://arxiv.org/abs/2512.06096)
*Karthik Mohan,Sonam Singh,Amit Arvind Kale*

Main category: cs.CV

TL;DR: 本文提出了BeLLA，一种将360度鸟瞰图（BEV）表示与大语言模型相结合的端到端架构，用于自动驾驶场景下的问答任务，能更好地进行空间推理。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言与多模态语言模型在自动驾驶领域多依赖单一视角或多视角特征聚合，无法统一空间表示，因此难以处理与空间结构、方位和物体关系相关的高级推理问题。

Method: BeLLA架构通过统一360度BEV（鸟瞰图）空间表示，与大型语言模型进行结合，实现多摄像头视角的空间整合，并以问答方式解读驾驶场景，实现自动驾驶中的复杂空间关系推理。

Result: 在NuScenes-QA和DriveLM两个基准测试中，BeLLA在需要空间推理与行为理解类问题上，性能优于现有方法，某些任务提升达+9.3%；在其他类别问题表现也极具竞争力。

Conclusion: BeLLA能够高效处理涉及空间关系和多样问题的自动驾驶场景问答任务，在空间推理与行为理解上有明显优势，推动了多模态模型在自动驾驶中的应用和发展。

Abstract: The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360° BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.

</details>


### [20] [SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection](https://arxiv.org/abs/2512.06103)
*Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

TL;DR: 本论文提出了一种基于深度学习的新型多光谱虹膜活体检测方法（SpectraIrisPAD），并发布了一个包含多类攻击的新虹膜数据集（MSIrPAD）。该方法在多种未知攻击下表现优越，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 虹膜识别虽然准确率高，但易受伪造攻击威胁。传统方法主要依赖于单一近红外波段，泛化能力不足，因此亟需结合多波段信息提升安全性和泛化性。

Method: 提出SpectraIrisPAD框架，基于DINOv2 Vision Transformer骨干网络，引入可学习的光谱位置编码、token融合和对比学习，以提取具有判别力的、带特异性的特征，有效区分真实虹膜与多种伪造攻击。同时发布新多光谱数据集MSIrPAD，涵盖5种NIR波段及多类攻击类型。

Result: 在包含多种未知攻击的评测协议下，SpectraIrisPAD在所有性能指标上均优于现有主流方法，表现出更强的鲁棒性和泛化能力。

Conclusion: SpectraIrisPAD有效提升了多光谱虹膜活体检测的安全性和通用性，为未来虹膜识别系统在实际场景中的安全应用奠定了基础。

Abstract: Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800\,nm, 830\,nm, 850\,nm, 870\,nm, and 980\,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.

</details>


### [21] [Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation](https://arxiv.org/abs/2512.06105)
*Junwen Zheng,Xinran Xu,Li Rong Wang,Chang Cai,Lucinda Siyun Tan,Dingyuan Wang,Hong Liang Tey,Xiuyi Fan*

Main category: cs.CV

TL;DR: 提出了一种跨模态可解释框架（CEFM），用于提高黑盒深度学习模型在黑色素瘤分类中的可解释性和临床信任度。该方法将临床诊断标准（ABC）嵌入视觉特征空间，并通过自然语言生成结构化解释，实现诊断决策的可追溯性，且准确率高达92.79%。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在皮肤黑色素瘤分类方面具备专家级表现，但由于模型的不透明性及解释性不足，导致临床医生难以信任其诊断结果，阻碍了其临床应用。因此，亟需提升模型解释性，增强黑盒算法与临床实践的结合。

Method: 提出CEFM框架，将皮肤黑色素瘤的临床ABC诊断标准映射到视觉Transformer的嵌入空间，通过对比学习实现视觉特征与临床语义的对齐，再结合自然语言生成模块，将嵌入特征转化为结构化文本解释，实现端到端的可解释分类。

Result: 在公开数据集上，所提方法取得了92.79%的分类准确率和0.961的AUC值，在多项可解释性评价指标上都明显优于传统方法。定性分析显示，模型输出的嵌入特征在空间分布上与临床实际应用ABC规则高度吻合。

Conclusion: CEFM有效提升了深度学习模型在黑色素瘤分类中的透明度和临床可解释性，在保证高性能分类的同时，建立了视觉数据到临床语言解释的桥梁，有助于提升模型的临床信赖度与实际应用价值。

Abstract: Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.

</details>


### [22] [Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation](https://arxiv.org/abs/2512.06158)
*Su Sun,Cheng Zhao,Himangi Mittal,Gaurav Mittal,Rohith Kukkala,Yingjie Victor Chen,Mei Chen*

Main category: cs.CV

TL;DR: 该论文提出Track4DGen框架，通过引入特征级跟踪先验，有效提升基于稀疏输入的4D动态对象生成的时序一致性和视图一致性并抑制漂移现象。


<details>
  <summary>Details</summary>
Motivation: 现有4D对象生成方法容易出现跨时间和跨视角的外观漂移与失真，主要由于现有损失函数缺乏显式的时序感知特征级追踪信息，仅在像素或潜空间层面做约束，导致运动一致性不足。

Method: 论文提出Track4DGen，两阶段：第一阶段在多视图视频扩散生成中嵌入点级特征追踪，使扩散生成器获得时序连贯的特征表达，抑制漂移；第二阶段融合了扩散特征、Hex-plane特征和4D球谐函数，实现高保真的4D高斯泼溅建模。

Result: Track4DGen在多视图视频和4D生成基准上均超越现有方法，生成了时序稳定、可文本编辑的4D动态资产。还开源了高质量4D对象数据集Sketchfab28。

Conclusion: 通过引入跟踪驱动的特征一致性约束，Track4DGen显著改善了4D对象的运动和视图一致性，推动了4D视觉资产生成的发展，为后续研究提供了高质量数据基准。

Abstract: Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.

</details>


### [23] [Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection](https://arxiv.org/abs/2512.06171)
*Jessica Plassmann,Nicolas Schuler,Michael Schuth,Georg von Freymann*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的自动化流程，可从剪切干涉（Shearography）测量中生成缺陷注释，并实现高质量分割与检测。该方法减少人工标注难度，并有助于大规模数据集构建。


<details>
  <summary>Details</summary>
Motivation: 剪切干涉法检测表面及亚表面缺陷灵敏度高，但工业应用受制于缺少高质量标注数据，人工标注费时且主观，难以标准化。

Method: 提出利用深度学习模型自动从剪切干涉仪获取的测量数据中产生缺陷的分割和边界框标签，实现自动高分辨率注释。

Result: 自动注释结果与专家标注数据对比，表明模型精度足以支撑弱监督训练，大大减轻了手工标注负担。

Conclusion: 该自动化流程有效促进了数据集大规模构建，为工业中剪切干涉法的缺陷检测应用提供了支持。

Abstract: Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.

</details>


### [24] [Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction](https://arxiv.org/abs/2512.06174)
*Shilin Hu,Jingyi Xu,Akshat Dave,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: 该论文提出了一种结合物理显式建模与深度学习的新型阴影生成框架，通过引入几何和光照的显式信息，有效提升了阴影的物理一致性和视觉真实感。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习的阴影生成方法很少直接引入阴影物理形成过程中的关键要素，如物体几何和场景光照，导致生成结果常存在物理不一致或不真实的问题。作者希望将物理建模与深度学习结合，提升生成阴影的质量和一致性。

Method: 作者提出的框架分为两步：首先利用单张RGB图像估算场景的密集点云几何与主要光源方向，并据此通过物理建模获得初步阴影估计；其次将该物理结果作为先验，与扩散模型结合进一步优化，生成高保真且与场景几何和光照一致的阴影图像。

Result: 在DESOBAV2数据集上训练后，所提方法在各种场景，尤其是复杂几何或光照不明确的场合下，生成出比现有方法更真实且物理合理的阴影效果，取得了更优性能。

Conclusion: 结合物理建模与深度学习，能显著提高阴影生成的真实性与一致性，为复杂场景下的阴影合成任务提供了有效解决方案。

Abstract: Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.

</details>


### [25] [Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction](https://arxiv.org/abs/2512.06179)
*Shilin Hu,Jingyi Xu,Sagnik Das,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: 本文提出了一种联合检测投射阴影与附着阴影的新方法，并构建了专门数据集，显著提升了附着阴影检测表现。


<details>
  <summary>Details</summary>
Motivation: 现有阴影检测方法主要关注投射阴影，忽视了对三维结构和场景理解至关重要的附着阴影。同时，公开数据集和模型对附着阴影缺乏专门研究和标注，导致该领域存在空缺。

Method: 作者提出一种联合检测框架，通过同时预测附着和投射阴影，并引入光照估计模块推断光照方向。利用检测到的阴影和表面法线信息获得几何一致的部分遮挡（自遮挡）区图，进一步反哺优化阴影分割和光照估计，实现闭环推理。为支持模型开发，制作了1458张带有详细阴影类型标注的数据集。

Result: 实验结果表明，该方法在检测附着阴影方面有明显优势，最小化了误差（BER至少降低33%），同时保持投射和整体阴影检测的高表现。

Conclusion: 通过将几何和光照信息结合进行闭环推理，实现了投射与附着阴影的高效检测，推动了阴影理解领域发展。

Abstract: Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.

</details>


### [26] [SPOOF: Simple Pixel Operations for Out-of-Distribution Fooling](https://arxiv.org/abs/2512.06185)
*Ankit Gupta,Christoph Adami,Emily Dolson*

Main category: cs.CV

TL;DR: 本文发现即使是最先进的神经网络架构，依然容易对“欺骗性图片”做出过度自信的错误分类。提出了更高效的黑盒攻击方法SPOOF，显示这些模型的脆弱性难以根除。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽广泛用于图像识别，但在面对非自然图像时易出现过度自信的问题。文章重审欺骗性图片的脆弱性，以评估现代架构的安全性并推动更鲁棒的模型设计。

Method: 作者复现并分析了CPPN和直接编码的进化欺骗性攻击，在现代卷积神经网络和Transformer（ViT-B/16）架构上进行实验。提出SPOOF攻击，以更少像素改动和更少计算成本生成高置信度欺骗性图片，还实验了用欺骗性图片扩充训练来增强鲁棒性的效果。

Result: Transformer（ViT-B/16）对欺骗性图片最脆弱，可在很少查询下被误导。SPOOF方法高效产生不可辨认的欺骗图片且极易欺骗分类器。即使加上欺骗性图片再训练，SPOOF也能凭稍多的查询继续欺骗，大部分模型仍然易受攻击。

Conclusion: 即便是当前最先进的深度分类器，也易被极少像素修改的欺骗图片误导，防御措施效果有限，表明现代深度网络的脆弱性问题依然突出。

Abstract: Deep neural networks (DNNs) excel across image recognition tasks, yet continue to exhibit overconfidence on inputs that bear no resemblance to natural images. Revisiting the "fooling images" work introduced by Nguyen et al. (2015), we re-implement both CPPN-based and direct-encoding-based evolutionary fooling attacks on modern architectures, including convolutional and transformer classifiers. Our re-implementation confirm that high-confidence fooling persists even in state-of-the-art networks, with transformer-based ViT-B/16 emerging as the most susceptible--achieving near-certain misclassifications with substantially fewer queries than convolution-based models. We then introduce SPOOF, a minimalist, consistent, and more efficient black-box attack generating high-confidence fooling images. Despite its simplicity, SPOOF generates unrecognizable fooling images with minimal pixel modifications and drastically reduced compute. Furthermore, retraining with fooling images as an additional class provides only partial resistance, as SPOOF continues to fool consistently with slightly higher query budgets--highlighting persistent fragility of modern deep classifiers.

</details>


### [27] [Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying](https://arxiv.org/abs/2512.06190)
*Shichen Li,Ahmadreza Eslaminia,Chenhui Shao*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态颜色轨迹预测方法，能够结合高维时序颜色信息与干燥工艺参数，实现对食品干燥过程中颜色变化的精准预测，并在未见过的工况条件下表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前食品干燥过程的颜色变化研究多依赖低维特征，无法全面刻画复杂的动态颜色变化过程，且现有建模方法在新工艺条件下泛化能力不足。为提升颜色变化预测的准确性与通用性，亟需新的建模方法。

Method: 作者开发了一种多模态颜色轨迹预测方法，将高维时序颜色数据与干燥过程参数相结合，基于机器学习/深度学习等技术，实现对不同干燥条件下颜色动态的建模与预测。

Result: 在未见过的干燥条件下，模型在曲奇干燥和苹果干燥实验中取得了RMSE分别为2.12和1.29的成绩，相较传统基线模型，误差降低超过90%。

Conclusion: 该方法预测精度高、鲁棒性强、适用范围广，有望提升食品干燥过程中产品质量监控与工艺优化能力。

Abstract: Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.

</details>


### [28] [The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning](https://arxiv.org/abs/2512.06206)
*Akis Linardos,Sarthak Pati,Ujjwal Baid,Brandon Edwards,Patrick Foley,Kevin Ta,Verena Chung,Micah Sheller,Muhammad Irfan Khan,Mojtaba Jafaritadi,Elina Kontio,Suleiman Khan,Leon Mächler,Ivan Ezhov,Suprosanna Shit,Johannes C. Paetzold,Gustav Grimberg,Manuel A. Nickel,David Naccache,Vasilis Siomos,Jonathan Passerat-Palmbach,Giacomo Tarroni,Daewoon Kim,Leonard L. Klausmann,Prashant Shah,Bjoern Menze,Dimitrios Makris,Spyridon Bakas*

Main category: cs.CV

TL;DR: MICCAI FeTS 2024挑战评估多机构脑肿瘤分割的联邦学习方法，重点关注聚合算法和通信效率。PID控制器聚合方法表现最佳，显著提升分割精度和通信效率。


<details>
  <summary>Details</summary>
Motivation: 医学影像分析中的数据隐私和多中心数据异质性，使得传统集中式训练方法难以应用。联邦学习通过分散训练保护隐私，但其聚合方式和效率仍有提升空间。作者希望通过挑战赛推动更稳健、高效的聚合方法发展。

Method: 基于BraTS基准，六支队伍用1,251例训练+219例验证+570例测试数据进行联邦学习式MRI脑肿瘤分割，比较不同聚合方法。评测标准综合了Dice系数、HD95距离及通信收敛效率。

Result: PID控制器聚合方法在所有指标中表现最佳，平均Dice值分别为0.733（ET）、0.761（TC）、0.751（WT），HD95约为33mm，通信收敛得分0.764，超过以往最佳方法。

Conclusion: 用PID控制器进行权重聚合显著提升了医学图像分割中的联邦学习稳定性与效率，为未来医学AI协作训练提供有效方案。

Abstract: We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.

</details>


### [29] [Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study](https://arxiv.org/abs/2512.06221)
*Alena Makarova*

Main category: cs.CV

TL;DR: 本研究对结合奇异值分解（SVD）与小波差分减少（WDR）的有损图像压缩方法进行了独立复现测试，发现其压缩效果并未明显优于JPEG2000或WDR。


<details>
  <summary>Details</summary>
Motivation: 原始论文声称SVD与WDR组合可以在压缩比和视觉质量上优于JPEG2000和单独的WDR方法，但缺乏完整的实现细节，存在复现难度。

Method: 作者重新实现了SVD+WDR方法，尽量还原原始实验设定，并用新的测试图片补充实验，使用PSNR和SSIM评估压缩效果。

Result: 实验结果显示，SVD+WDR方法在PSNR方面通常不如JPEG2000或WDR，在SSIM上部分超越JPEG2000，但整体未体现出原文所述的全面优势。

Conclusion: 原论文在实现细节上存在模糊，影响了方法可复现性和实际表现。结合SVD与WDR并未实现预期的明显提升，提示后续研究需详尽描述算法细节并加强复现性验证。

Abstract: This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.

</details>


### [30] [GPU-GLMB: Assessing the Scalability of GPU-Accelerated Multi-Hypothesis Tracking](https://arxiv.org/abs/2512.06230)
*Pranav Balakrishnan,Sidisha Barik,Sean M. O'Rourke,Benjamin M. Marlin*

Main category: cs.CV

TL;DR: 本文提出了一种支持单物体多检测的GLMB（Generalized Labeled Multi-Bernoulli）滤波器变体，并展示了其在GPU上的并行加速能力。


<details>
  <summary>Details</summary>
Motivation: 多目标跟踪中，基于随机有限集的带标签方法（如GLMB）具有理论优势，但传统方法由于需要维护大量假设计算量极大，即使采用裁剪也仍然昂贵。随着分布式、基于机器学习的虚拟传感器网络的发展，单个物体可能被多次检测，如何高效处理这些多检测数据成为现实应用中的关键挑战。

Method: 本文基于GLMB滤波器，提出允许每个物体每轮由同一传感器产生多次检测的变体，从而打破了标准GLMB滤波中各检测间的依赖，并设计了适合GPU并行的高效滤波更新算法。

Result: 提出的方法显著提升了滤波器的并行可扩展性，并通过GPU实现，初步实验展示了其在目标数量和保留假设数增加时的运行时间扩展性优势。

Conclusion: 新提出的GLMB变体不仅理论上支持单物体多检测场景，而且能有效利用GPU硬件进行高效并行，实现了多目标跟踪在实际分布式传感器网络中的可用性和扩展性。

Abstract: Much recent research on multi-target tracking has focused on multi-hypothesis approaches leveraging random finite sets. Of particular interest are labeled random finite set methods that maintain temporally coherent labels for each object. While these methods enjoy important theoretical properties as closed-form solutions to the multi-target Bayes filter, the maintenance of multiple hypotheses under the standard measurement model is highly computationally expensive, even when hypothesis pruning approximations are applied. In this work, we focus on the Generalized Labeled Multi-Bernoulli (GLMB) filter as an example of this class of methods. We investigate a variant of the filter that allows multiple detections per object from the same sensor, a critical capability when deploying tracking in the context of distributed networks of machine learning-based virtual sensors. We show that this breaks the inter-detection dependencies in the filter updates of the standard GLMB filter, allowing updates with significantly improved parallel scalability and enabling efficient deployment on GPU hardware. We report the results of a preliminary analysis of a GPU-accelerated implementation of our proposed GLMB tracker, with a focus on run time scalability with respect to the number of objects and the maximum number of retained hypotheses.

</details>


### [31] [Opinion: Learning Intuitive Physics May Require More than Visual Data](https://arxiv.org/abs/2512.06232)
*Ellen Su,Solim Legris,Todd M. Gureckis,Mengye Ren*

Main category: cs.CV

TL;DR: 本研究探讨了深度学习模型在直觉物理任务中的表现，发现即使使用更贴近人类成长过程的视频数据训练，也无法显著提升表现。单靠调整数据分布和数量，难以让现有模型达到人类直觉物理水平。


<details>
  <summary>Details</summary>
Motivation: 尽管现有深度学习模型在大规模视频数据训练下已经达到很高水平，但在需要直觉物理理解的任务上仍不及人类。作者想验证问题的关键是数据的质量（分布）而非数量，提出用更接近婴幼儿真实视觉体验的数据集（SAYCam）来测试。

Method: 作者使用V-JEPA模型，在SAYCam这个捕捉3名儿童日常第一视角视觉体验的数据集上进行预训练，并将其表现与在大规模互联网数据上训练的模型进行对比，重点考查其在IntPhys2直觉物理基准测试上的表现。

Result: 即便SAYCam数据量只占主流模型训练数据的0.01%，模型在该数据集上训练后，在IntPhys2上的表现并无显著提升。

Conclusion: 仅靠接近人类视觉习得过程的数据和调整数据分布或规模，无法让当前主流架构学到支持直觉物理的表征，意味着还需更本质的方法创新来实现人工的直觉物理解。

Abstract: Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.

</details>


### [32] [NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks](https://arxiv.org/abs/2512.06251)
*Fangzhou Lin,Yuping Wang,Yuliang Guo,Zixun Huang,Xinyu Huang,Haichong Zhang,Kazunori Yamada,Zhengzhong Tu,Liu Ren,Ziming Zhang*

Main category: cs.CV

TL;DR: 本文提出了NexusFlow框架，通过引入可逆耦合层绑定任务特征，实现不同任务（甚至结构差异很大任务）间的知识迁移，在多任务部分监督学习（PS-MTL）领域实现了新突破。


<details>
  <summary>Details</summary>
Motivation: 现有的部分监督多任务学习方法，多集中于同质且密集预测任务，对于结构多样任务协同学习的挑战关注不足。实际应用中，不同任务的结构不一致情况更为常见，因此需要一种能处理结构异质任务的高效多任务部分监督框架。

Method: 提出NexusFlow框架，在多任务学习主网络外引入一组可逆耦合层的代理网络，将不同任务的特征分布对齐到统一的表征空间。其关键在于耦合层具备双射性，既保证信息不丢失，也防止表征坍塌，能应对结构多样任务的特征融合对齐需求。

Result: 在自动驾驶域分割任务（地图重建与目标跟踪地理区域不同）中，NexusFlow显著超过基线并刷新nuScenes数据集SOTA。此外，在NYUv2的三个同质密集预测任务上亦取得全部任务一致性提升，体现了方法通用性。

Conclusion: NexusFlow是通用、轻量、可插拔的多任务部分监督框架，能够有效对抗结构不一致和标签不完全问题，实现更强的多任务知识迁移，并具备广泛实际应用潜力。

Abstract: Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.

</details>


### [33] [Language-driven Fine-grained Retrieval](https://arxiv.org/abs/2512.06255)
*Shijie Wang,Xin Yu,Yadan Luo,Zijian Wang,Pengfei Zhang,Zi Huang*

Main category: cs.CV

TL;DR: 本文提出了LaFG框架，通过利用大语言模型和视觉-语言模型，将类别名称转化为属性级监督信息，提升细粒度图像检索的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度图像检索方法主要依赖于类别标签作为监督信号，但类别标签语义稀疏，难以捕捉跨类别的细节可比性，限制了模型在未见类别上的泛化性能。

Method: 作者提出LaFG，将类别名称视为语义锚点，利用大语言模型生成细致的属性描述，并通过冻结的视觉-语言模型将这些描述映射到视觉对齐空间，提取并聚类生成属性词表，从而构建类别特定的语言原型，对检索模型进行指导。

Result: LaFG能通过丰富的属性级监督，有效提升检索系统对未见类别的泛化能力，获取更具区分性的嵌入表征。

Conclusion: 基于属性驱动的语义监督能够弥补类别标签监督的不足，为细粒度图像检索带来更强的泛化性和细致判别能力。

Abstract: Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer

</details>


### [34] [Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs](https://arxiv.org/abs/2512.06258)
*Chaoyang Wang,Yangfan He,Yiyang Zhou,Yixuan Wang,Jiaqi Liu,Peng Xia,Zhengzhong Tu,Mohit Bansal,Huaxiu Yao*

Main category: cs.CV

TL;DR: 本文发现大型视觉-语言模型（LVLMs）在推理路径选择上存在严重偏差：即使模型知道正确答案，依然常常通过错误的推理路径获得，导致结果不稳定甚至不可信。作者提出了一种两阶段后训练框架PSO（Path-Select Optimization）来提升模型的推理能力和稳定性。


<details>
  <summary>Details</summary>
Motivation: 动机在于现有LVLMs虽然知识覆盖广，但推理路径选择有明显偏误：模型常偏向不稳或逻辑不一致的路径，而主要不是因为知识缺乏。通过Pass@K与Pass@1的差异性，揭示根本问题在于推理路径偏置，亟需有效解决。

Method: 方法包括两阶段：（1）用GRPO（Group Relative Policy Optimization）结合模板和答案奖励，促使结构化、逐步式推理路径学习；（2）在线偏好优化，模型自我采样和评估推理路径，优选并自适应于“最佳路径”；同时将劣质路径存入负重放存储，周期性复训防止旧错重犯。

Result: 实验显示，PSO能有效剔除无效推理路径，推理准确率平均提升7.4%，并带来更加稳定、一致的推理链。

Conclusion: PSO有效缓解了LVLMs推理路径偏差难题，大幅提升了推理表现与可靠性，为LVLMs安全、可靠应用奠定基础。

Abstract: We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.

</details>


### [35] [TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06269)
*Quan Tran,Tuan Dang*

Main category: cs.CV

TL;DR: 本论文提出了一种通过多视图三角测量约束，提升3D高斯点渲染重建质量的新方法，有效减少了漂浮伪影并获得了更高保真度的三维表面重建。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯点方法仅依赖光度损失来优化，导致重建的不确定性和结构不一致，出现“漂浮物”等伪影，难以获得高质量的三维表面。

Method: 作者提出引入全局几何一致性约束，结合多视图自监督三角测量，通过惩罚重建点与自邻近视角重投影共识点间的偏差，实现几何一致性优化，提高了重建表面的一致性和结构准确性。

Result: 该方法在多个数据集上表现出色，在DTU数据集上获得了0.50mm的平均Chamfer距离，优于现有显式重建方法，达到了最新水平。

Conclusion: 论文方法有效提高了3D高斯点渲染的重建精度，减少了伪影，具有良好的实用前景。作者承诺开源代码以促进社区研究和复现。

Abstract: 3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in "floater" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.

</details>


### [36] [FacePhys: State of the Heart Learning](https://arxiv.org/abs/2512.06275)
*Kegang Wang,Jiankai Tang,Yuntao Wang,Xin Liu,Yuxuan Fan,Jiatong Ji,Yuanchun Shi,Daniel McDuff*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的rPPG（远程光电容积描记法）算法FacePhys，实现了低计算资源下的实时生命体征测量，准确率和效率大幅提升。


<details>
  <summary>Details</summary>
Motivation: 利用摄像头实现无创、广泛适用的生命体征检测前景广阔，但现有算法受限于设备端计算能力和信号质量衰减，难以兼顾模型规模、泛化能力和实时性能。

Method: 提出FacePhys算法，基于时间-空间状态空间二元性理论，设计可迁移的心脏状态表示，精确捕捉视频帧中的微小周期变化，同时减少模型内存消耗，实现对长序列视频的训练和极低延时推理。

Result: FacePhys算法在精度上较现有方法误差降低49%，每帧延迟仅9.46ms，内存占用只有3.6MB，性能超越同类方法83%到99%。

Conclusion: FacePhys在实际部署环境中实现高效、可靠的实时生命体征测量，推动了摄像头健康监测技术的实用化，相关演示已上线。

Abstract: Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\% to 99\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.

</details>


### [37] [RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension](https://arxiv.org/abs/2512.06276)
*Tianyi Gao,Hao Li,Han Fang,Xin Wei,Xiaodong Dong,Hongbo Sun,Ye Yuan,Zhongjiang He,Jinglin Xu,Jingmin Xin,Hao Sun*

Main category: cs.CV

TL;DR: 本文提出RefBench-PRO，一个可解释性强、细化维度的指代表达理解（REC）基准，以及Ref-R1增强学习方案，用以推动多模态大语言模型在感知与推理能力上的评测和提升。


<details>
  <summary>Details</summary>
Motivation: 现有REC基准主要评估模型感知能力，缺乏解释性和细化的评分体系，无法全面反映多模态大语言模型在感知与推理等不同认知维度上的表现。

Method: 提出RefBench-PRO基准，将指代表达分解为感知与推理两个核心维度，进一步细分为六类任务（属性、位置、交互、常识、关系、拒绝），并开发自动化数据生成流程，覆盖上述各子任务。此外，提出Ref-R1增强学习方法，引入动态IoU指标的GRPO，提升面对复杂推理任务时的定位准确率。

Result: 实验结果表明，RefBench-PRO可用于对MLLM进行可解释性评测，在感知和推理层面为模型带来更多挑战，同时Ref-R1实现了更强REC基线表现。

Conclusion: RefBench-PRO和Ref-R1为REC任务提供了更具解释性和挑战性的评测标准，对多模态大模型的理解能力评估和训练具有提升作用。

Abstract: Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.

</details>


### [38] [Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models](https://arxiv.org/abs/2512.06281)
*Hengzhuang Li,Xinsong Zhang,Qiming Peng,Bin Luo,Han Hu,Dengyang Jiang,Han-Jia Ye,Teng Zhang,Hai Jin*

Main category: cs.CV

TL;DR: 该论文提出了一种新的训练框架LaVer，通过在大语言模型的联合语义空间进行掩码图像建模，有效提升多模态大模型对视觉信息的利用能力，从而改善视觉表现和减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）虽然在多模态任务中取得了优异成绩，但存在模态不平衡问题：在深层网络中，视觉信息相较于文本表示被严重弱化，导致视觉能力下降或模型产生幻觉。这主要由于训练方式偏向文本预测，缺少直接的视觉监督信号。

Method: 作者提出Latent Visual Reconstruction（LaVer）训练框架，在多模态大模型的联合语义空间内引入掩码图像建模任务，从而给予模型直接的视觉激活信号，促使视觉表征更具判别性，并提升模型对视觉信息的关注度。

Result: 在各类基准测试中，LaVer展示出在不同场景下的卓越性能，尤其是在需要密集视觉理解的任务上表现突出。实验表明，该方法显著优于现有方法。

Conclusion: LaVer有助于解决多模态大模型中的模态不平衡问题，有效提升视觉能力。该方法可作为提升多模态模型视觉表现的有效工具，论文源码已开源。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.

</details>


### [39] [A Sleep Monitoring System Based on Audio, Video and Depth Information](https://arxiv.org/abs/2512.06282)
*Lyn Chao-ling Chen,Kuan-Wen Chen,Yi-Ping Hung*

Main category: cs.CV

TL;DR: 本论文提出了一种用于睡眠障碍定量评估的非侵入式监测系统，通过事件识别方法捕捉和分类三类睡眠干扰事件：运动、灯光变化和噪音。实验结果验证了该系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统的睡眠干扰评估方式多为侵入式或主观报告，影响被试睡眠质量或评估准确性。为克服这些缺陷，作者提出开发一种既方便又准确的非侵入式定量评估方法。

Method: 该系统使用集成了红外深度传感器、RGB摄像头和四麦阵列的设备，在微光环境下进行家庭睡眠观测。通过对深度信号和彩色图像分别建模，定量检测运动和照明干扰。此外，开发了基于事件检测的算法，融合多种传感器数据以识别各类睡眠干扰事件。

Result: 通过实际睡眠场景测试，系统能够准确检测到三类干扰事件，体现出较高可靠性，验证了事件检测方法在非侵入式睡眠监测中的有效性。

Conclusion: 文中提出的非侵入式事件检测系统可用于家庭环境下对睡眠障碍进行定量、可靠监测，有应用推广潜力。

Abstract: For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.

</details>


### [40] [StrokeNet: Unveiling How to Learn Fine-Grained Interactions in Online Handwritten Stroke Classification](https://arxiv.org/abs/2512.06290)
*Yiheng Huang,Shuang She,Zewei Wei,Jianmin Lin,Ming Yang,Wenyin Liu*

Main category: cs.CV

TL;DR: 本文提出了StrokeNet，通过参考点对表示法，实现了对于笔画语义关系的高效建模，在多项手写笔画分类任务中创下新高。


<details>
  <summary>Details</summary>
Motivation: 现有笔画分类难以解决笔画风格、内容歧义、书写位置动态等带来的挑战。主要难题在于如何精细地建模笔画之间的语义关系，而传统深度学习方法难以捕获这种细粒度的局部交互。

Method: 创新性地提出StrokeNet，用'参考点+特征向量'对笔画进行编码。具体做法是动态选择每个笔画的参考点并排序，引入“Inline Sequence Attention”模块建构上下文特征；并通过“Cross-Ellipse Query”机制，在不同空间尺度上聚类参考点并提取空间交互特征。模型以联合优化方式预测笔画类别及相邻笔画的语义转移，辅以Auxiliary Branch分支。

Result: 在多个公开手写数据集上测试，该方法均取得了最优表现；在CASIA-onDo数据集上，准确率从93.81%提升至95.54%。

Conclusion: 通过参考点串行建模和空间特征交互机制，StrokeNet能更精细地建模和判别手写笔画，显著提升了笔画分类的准确性和鲁棒性。

Abstract: Stroke classification remains challenging due to variations in writing style, ambiguous content, and dynamic writing positions. The core challenge in stroke classification is modeling the semantic relationships between strokes. Our observations indicate that stroke interactions are typically localized, making it difficult for existing deep learning methods to capture such fine-grained relationships. Although viewing strokes from a point-level perspective can address this issue, it introduces redundancy. However, by selecting reference points and using their sequential order to represent strokes in a fine-grained manner, this problem can be effectively solved. This insight inspired StrokeNet, a novel network architecture encoding strokes as reference pair representations (points + feature vectors), where reference points enable spatial queries and features mediate interaction modeling. Specifically, we dynamically select reference points for each stroke and sequence them, employing an Inline Sequence Attention (ISA) module to construct contextual features. To capture spatial feature interactions, we devised a Cross-Ellipse Query (CEQ) mechanism that clusters reference points and extracts features across varying spatial scales. Finally, a joint optimization framework simultaneously predicts stroke categories via reference points regression and adjacent stroke semantic transition modeling through an Auxiliary Branch (Aux-Branch). Experimental results show that our method achieves state-of-the-art performance on multiple public online handwritten datasets. Notably, on the CASIA-onDo dataset, the accuracy improves from 93.81$\%$ to 95.54$\%$, demonstrating the effectiveness and robustness of our approach.

</details>


### [41] [Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion](https://arxiv.org/abs/2512.06504)
*Andrii Lysyi,Anatoliy Sachenko,Pavlo Radiuk,Mykola Lysyi,Oleksandr Melnychenko,Diana Zahorodnia*

Main category: cs.CV

TL;DR: 本文提出了一种智能化、一体化的光伏（PV）基础设施自动巡检系统，显著提升了巡检的准确率与效率，并减少了数据和通信带宽消耗。


<details>
  <summary>Details</summary>
Motivation: 现有光伏巡检方法存在调色板偏差、数据冗余和高带宽传输等弊端，亟需更高效、智能的自动化解决方案。

Method: 设计了多模态巡检架构，首先通过一致性学习获得调色板不变的热成像表达，并与对比归一化后的RGB图像信息通过门控机制融合；引入基于Rodrigues变换的闭环自适应重采集控制器以精确确认异常，同时利用地理空间去重模块（DBSCAN结合haversine距离）以聚类冗余警报。

Result: 系统在PVF-10基准上mAP@0.5达到0.903，较单一模态提升12-15%；实地验证召回率达96%；数据去重将误报减少了15-20%；相关性遥测大幅减少60-70%数据传输。

Conclusion: 本研究提出的系统为光伏自动巡检提供了全新范式，实现了高精度、高效率、低带宽占用，可有效提升电站安全与运维水平。

Abstract: The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.

</details>


### [42] [Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation](https://arxiv.org/abs/2512.06306)
*Haoxian Zhou,Chuanzhi Xu,Langyi Chen,Haodong Chen,Yuk Ying Chung,Qiang Qu,Xaoming Chen,Weidong Cai*

Main category: cs.CV

TL;DR: 本文提出了一种基于点云的事件流人体姿态估计算法，通过新模块提升估计精度，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前事件相机具有高时空分辨率，但传统方法将事件转为密集帧，计算量大且丧失时间分辨率，因此需要新的方法充分利用事件流的优势。

Method: 作者提出了基于点云的事件流人体姿态估计方法，包含Event Temporal Slicing Convolution模块抓取事件切片间短期依赖，Event Slice Sequencing模块用于结构化时间建模，并引入边缘增强技术提升稀疏条件下的空间信息表达。

Result: 在DHP19数据集上，所提方法结合PointNet、DGCNN和Point Transformer三种点云主干网络进行测试，均实现了性能一致提升。

Conclusion: 基于点云的事件流处理和时空建模有效提升了事件相机下的人体姿态估计精度，对高分辨率、低延迟人体关键点预测具有重要意义。

Abstract: Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.

</details>


### [43] [More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery](https://arxiv.org/abs/2512.07596)
*Wenzhen Dong,Jieming Yu,Yiming Huang,Hongqiu Wang,Lei Zhu,Albert C. S. Chung,Hongliang Ren,Long Bai*

Main category: cs.CV

TL;DR: 本文评估了最新的Segment Anything Model（SAM 3）在机器人辅助手术场景下的表现，特别是其零样本分割、语言提示和3D重建能力，并与前代模型做对比。


<details>
  <summary>Details</summary>
Motivation: 现有的SAM模型在医学图像分割中的应用尚未成熟，而手术场景对于模型的识别与分割能力要求极高。SAM 3集成了多种交互方式及3D感知，亟需系统验证其在复杂医疗实际中的有效性与局限性。

Method: 作者使用了MICCAI EndoVis 2017、2018等公开手术数据集，对SAM 3在点、框及语言提示下的零样本分割表现进行综合测试，并对其3D重建及深度估计进行评估，比较了SAM 3与前述版本（SAM、SAM 2）的分割效果和局限。

Result: 实验显示，SAM 3在空间提示（点、框）下的图像和视频分割均优于前代模型，3D场景重建和深度估计能力也较强，但在高动态复杂手术场景中和纯语言提示分割方面还有明显提升空间。

Conclusion: SAM 3扩展的功能提升了其在医疗场景的适用性，尤其是在三维重建方面表现出潜力，但在复杂动态及语言提示适应性上尚需针对性优化和训练。

Abstract: The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.

</details>


### [44] [ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models](https://arxiv.org/abs/2512.06328)
*Jiahao Li,Yusheng Luo,Yunzhong Lou,Xiangdong Zhou*

Main category: cs.CV

TL;DR: 本文提出了ReCAD，一个基于强化学习（RL）的框架，结合了预训练大模型（PLMs）的生成能力，实现了从多模态输入自动生成高精度的参数化CAD模型。ReCAD在text-to-CAD和image-to-CAD任务上均取得了当前最佳效果。


<details>
  <summary>Details</summary>
Motivation: 现有CAD生成方法主要依赖监督微调，难以发掘预训练大模型的生成潜力，且编辑性有限。为此，作者希望开发一种方法，通过利用PLMs强大的生成先验，实现复杂CAD操作的自动化与可编辑性扩展。

Method: 1）先对视觉-语言模型（VLMs）进行微调，采用重写为参数化代码的CAD脚本生成文本描述提供监督信号。2）提出创新的RL策略，利用参数化代码引导模型提升推理复杂问题的能力。3）采用分层原语学习流程，以统一奖励函数逐步学习结构化与组合化技能，确保几何准确性与语义一致性。

Result: 在text-to-CAD和image-to-CAD任务上，ReCAD均大幅提升了几何准确性。例如，image-to-CAD任务中，Chamfer Distance均值在各种分布设定下显著优于现有方法：从73.47降至29.61（同分布），从272.06降至80.23（异分布）。

Conclusion: ReCAD框架实现了对PLMs生成能力的深度挖掘，在利用简单接口实现复杂CAD操作的同时，达到了业界领先的生成精度，为CAD模型的自动生成和可编辑性开辟了新路径。

Abstract: We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.

</details>


### [45] [sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only](https://arxiv.org/abs/2512.07698)
*Arslan Artykov,Corentin Sautier,Vincent Lepetit*

Main category: cs.CV

TL;DR: 本文提出了一种从单目视频中同时预测物体分割与关节参数的数据驱动方法，扩展了过往仅限于多视角或固定摄像头场景的现有研究。该方法仅用合成数据训练，能很好地泛化到真实世界物体。


<details>
  <summary>Details</summary>
Motivation: 以往对可动（关节）物体的理解依赖多视角或静态相机，缺乏对自由移动相机下数据的处理能力，而实际应用常常需要快速、灵活地从单目移动视频建模关节物体。

Method: 提出了一种端到端的数据驱动算法，从由自由移动相机拍摄的单目动态视频输入出发，同时预测物体的部件分割与关节参数。训练完全基于合成数据。

Result: 方法在合成训练数据基础上，在真实世界物体上表现出了良好的泛化能力；可从随意录制的视频可靠实现运动物体分割与运动建模。

Conclusion: 该方法对理解和建模关节物体提供了一种灵活且可扩展的解决方案，支持在现实动态环境中实时应用，有望促进机器人学和数字孪生领域的发展。

Abstract: Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/

</details>


### [46] [S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening](https://arxiv.org/abs/2512.06330)
*Haoyu Zhang,Junhan Luo,Yugang Cao,Siran Peng,Jie Huang,Liangjian-Deng*

Main category: cs.CV

TL;DR: 本文提出了一种新的全色锐化（pansharpening）方法S2WMamba，通过独特的多尺度小波分解与跨模态交互，实现高分辨率多光谱图像的有效融合，并优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 全色锐化任务的主要挑战是如何在增强空间细节的同时保持光谱保真度，传统方法往往空间与光谱信息混合难以解耦，导致结果存在失真。

Method: S2WMamba方法将二维Haar离散小波变换（2D DWT）应用于PAN图像，以分离空间边缘和纹理；同时将一维Haar DWT按通道应用于MS图像，将每个像素的光谱看作一维信号以分离频率成分，降低光谱失真。模型采用并行的光谱/空间两路分支，并通过基于Mamba的跨调制机制实现信息交互，还包括多尺度动态门控以自适应融合分支输出。

Result: 在WV3、GF2、QB等多个主流遥感数据集上，S2WMamba在PSNR、HQNR等指标上达到或优于现有最佳方法，PSNR提升最高0.23 dB，WV3全分辨率HQNR达0.956。消融实验验证了2D/1D DWT布置、并行双分支和融合门的合理性。

Conclusion: S2WMamba通过频域解耦与高效跨模态交互，实现了空间细节增强与光谱保真度的兼顾，推动了全色锐化技术发展，代码已开源便于复现。

Abstract: Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.

</details>


### [47] [UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction](https://arxiv.org/abs/2512.07756)
*Mayank Anand,Ujair Alam,Surya Prakash,Priya Shukla,Gora Chand Nandi,Domenec Puig*

Main category: cs.CV

TL;DR: 本文提出了一种新的临床超声采集辅助系统UltrasODM，通过不确定性指示、显著性诊断和动态提示帮助操作者提升超声重建的可靠性和实用性。该方法有效减少了运动漂移和重建误差，提高了超声成像的可信度和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统临床超声采集高度依赖操作者经验，快速的探头运动和亮度波动易导致重建失误，降低临床信任度和应用价值。因此，亟需自带反馈、提升易用性和安全性的辅助技术。

Method: UltrasODM分为三大核心组成：（1）对比排序模块按运动相似性分组图像帧；（2）结合光流和Dual-Mamba时序模块，实现稳健的六自由度位姿估计；（3）HITL层利用贝叶斯不确定性、临床校准阈值及显著性图，实时诊断并在超阈值时向操作者发出可操作提示，如重新扫描或减慢移动速度。

Result: 在临床自由手超声数据集上，UltrasODM相较前作UltrasOM将漂移减少15.2%、距离误差降低12.1%、Hausdorff距离降低10.1%，并能实时输出逐帧不确定性和显著性信息。

Conclusion: UltrasODM通过集成透明不确定性反馈和专家校准机制，提升了超声图像重建的可靠性和临床使用的信任度，支持更安全、可信的临床工作流程。

Abstract: Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.

</details>


### [48] [CryoHype: Reconstructing a thousand cryo-EM structures with transformer-based hypernetworks](https://arxiv.org/abs/2512.06332)
*Jeffrey Gu,Minkyu Jeon,Ambri Ma,Serena Yeung-Levy,Ellen D. Zhong*

Main category: cs.CV

TL;DR: 本文提出了CryoHype，一种基于Transformer的超网络方法，可同时重建大量不同分子种类的三维结构，在现有挑战性数据集上取得领先成果，并能扩展到重建千种结构。


<details>
  <summary>Details</summary>
Motivation: 现有cryo-EM方法主要针对单一或少数分子的构象异质性，难以处理由多个不同分子混合带来的组分异质性，限制了高通量结构解析的能力。

Method: 作者提出CryoHype，这是一种基于Transformer的超网络，能够动态调整隐式神经表示的权重，实现多种分子同时的三维重建。

Result: CryoHype在包含100个分子的公开数据集上达到了当前最优表现，并成功扩展至从未标记cryo-EM图像中重建1000种分子的三维结构（在固定朝向设定下）。

Conclusion: CryoHype极大提升了cryo-EM在高通量、多分子三维重建场景下的应用潜力，有望推动复杂生物大分子体系研究发展。

Abstract: Cryo-electron microscopy (cryo-EM) is an indispensable technique for determining the 3D structures of dynamic biomolecular complexes. While typically applied to image a single molecular species, cryo-EM has the potential for structure determination of many targets simultaneously in a high-throughput fashion. However, existing methods typically focus on modeling conformational heterogeneity within a single or a few structures and are not designed to resolve compositional heterogeneity arising from mixtures of many distinct molecular species. To address this challenge, we propose CryoHype, a transformer-based hypernetwork for cryo-EM reconstruction that dynamically adjusts the weights of an implicit neural representation. Using CryoHype, we achieve state-of-the-art results on a challenging benchmark dataset containing 100 structures. We further demonstrate that CryoHype scales to the reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting.

</details>


### [49] [Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate](https://arxiv.org/abs/2512.06344)
*Kaile Wang,Lijun He,Haisheng Fu,Haixia Bi,Fan Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态引导的生成式图像压缩框架（MTGC），针对超低码率下生成压缩方法出现的语义偏差问题，集成文本、极度压缩图像和任务相关伪词三种模态以提升语义一致性，在多个实验中表现出更好的感知质量与像素保真度。


<details>
  <summary>Details</summary>
Motivation: 现有生成式图像压缩在极低码率（bpp<0.05）时常因生成性失真而产生与原图语义不一致（“幻觉”），限制了其在带宽受限的6G语义通信等场景的应用。提升超低码率下图像重建的语义一致性和感知质量成为亟待解决的问题。

Method: 提出MTGC框架，综合利用三种模态（简洁鲁棒的文本描述、保留底层视觉信息的高度压缩图像、描述细粒度任务相关语义的伪词SPWs），其中SPWs通过任务感知语义压缩模块（TASCM）获得。多模态信息通过多模态引导扩散解码器（MGDD）和双路径协同引导机制（跨注意力和ControlNet残差）被高效注入生成过程，依赖扩散模型的强大生成先验重建图像。

Result: 在DIV2K等数据集上的大量实验表明，MTGC在极低码率下显著提升语义一致性（如DISTS降低10.59%），并在感知质量和像素级保真度方面取得优异效果。

Conclusion: 引入多模态协同指导机制能有效缓解低码率生成式压缩的语义偏差，提升图像在受限带宽场景下的实用性，为6G语义通信等应用提供了切实可行的解决方案。

Abstract: Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.

</details>


### [50] [CLUENet: Cluster Attention Makes Neural Networks Have Eyes](https://arxiv.org/abs/2512.06345)
*Xiangshuai Song,Jun-Jie Huang,Tianrui Liu,Ke Liang,Chang Tang*

Main category: cs.CV

TL;DR: 本文提出了一种新型可解释性强的深度视觉理解模型CLUENet，结合聚类及注意力机制，兼顾精度、效率与透明度，并在主流数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流视觉模型（如卷积神经网络和注意力机制网络）虽性能突出，但受限于固定感受野和复杂结构，难以建模不规则的空间模式且可解释性较差，不利于高透明度需求的场景。聚类方法可提升可解释性和语义建模灵活性，却存在准确率低、效率差及训练梯度消失等问题。因此，亟需一种同时兼具高性能与可解释性的视觉理解方法。

Method: 提出CLUENet深度架构，融合三项创新：(1) 采用温度缩放余弦注意力的全局软聚合与硬分配机制，并引入门控残差连接，提升局部特征建模；(2) 设计块间特征的硬分配与共享调度机制以优化信息流动；(3) 改进聚类池化策略进一步增强模型性能。

Result: 在CIFAR-100与Mini-ImageNet数据集上的实验表明，CLUENet在分类准确率和可解释性方面均超过了现有聚类方法和主流视觉模型，同时在效率方面表现优异。

Conclusion: CLUENet能够在保证可解释性和效率的同时显著提升视觉理解性能，是兼顾准确率、可解释性与效率的有力视觉模型，适用于需要高模型透明度的应用场景。

Abstract: Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.

</details>


### [51] [TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search](https://arxiv.org/abs/2512.06353)
*Kaicheng Yang,Kaisen Yang,Baiting Wu,Xun Zhang,Qianrui Yang,Haotong Qin,He Zhang,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种针对Diffusion Transformer (DiT)模型混合精度量化的新框架TreeQ，通过创新技术显著提升低比特量化下的性能，实现了首个在DiT模型4-bit量化下的几乎无损表现。


<details>
  <summary>Details</summary>
Motivation: 虽然DiT在图像生成任务中优于U-Net等传统结构，但其高计算和内存消耗严重限制了实际应用。已知U-Net可以通过混合精度量化减负，然而DiT的量化技术尚不成熟且缺乏探索。因此，作者试图解决DiT量化中的效率、精度及信息损失等关键难题。

Method: TreeQ框架包括三项创新：1）Tree Structured Search (TSS)，利用DiT线性结构以O(n)高效遍历量化方案空间并通过裁剪提升目标精度；2）Environmental Noise Guidance (ENG)，用单一超参数统一PTQ与QAT优化目标；3）General Monarch Branch (GMB)，加入稀疏分支以缓解极低比特带来的信息瓶颈，保证细节保留。

Result: 在DiT-XL/2模型和W3A3、W4A4等极低比特PTQ/PEFT设定下，TreeQ框架取得了目前最优量化表现。尤其是在4位量化（PTQ）下，首次实现无明显性能损失。

Conclusion: TreeQ为DiT模型量化提供了高效且有统一理论支撑的解决方案，显著降低了部署成本和硬件门槛，为低比特模型实际落地扫清了障碍，对大规模生成模型的部署具有重要意义。

Abstract: Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ

</details>


### [52] [Rectifying Latent Space for Generative Single-Image Reflection Removal](https://arxiv.org/abs/2512.06358)
*Mingjia Li,Jin Hu,Hainuo Wang,Qiming Hu,Jiarui Wang,Xiaojie Guo*

Main category: cs.CV

TL;DR: 本文提出了一种基于潜在扩散模型的新方法，有效解决单幅图像反射去除问题，并在多个基准和实际场景中取得了最新的SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 单幅图像反射去除问题高度不适定，现有方法难以处理图像中反射与真实内容的混合，导致还原效果差、泛化能力弱。本文关注语义编码器潜在空间缺乏反射成分线性分层表达能力这一被忽略的关键难题。

Method: 方法包括三部分：1）设计反射等变的VAE，使潜在空间具备反射物理层合线性特性。2）引入可学习的任务相关文本嵌入，避免文本描述模糊性，提升模型引导精度。3）深度引导的早分支采样策略，利用生成过程随机性以增强结果的多样性和可信度。整个系统基于编辑向潜在扩散模型调整。

Result: 实验证明，该方法在多个反射去除基准数据集上获得新SOTA表现，并能很好地泛化到复杂的真实反射场景。

Conclusion: 通过潜在空间建模与多分支生成策略的结合，显著提升了单幅图像反射去除的质量与泛化能力，对后续复杂图像分层任务具有重大参考价值。

Abstract: Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.

</details>


### [53] [Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection](https://arxiv.org/abs/2512.06363)
*Jiabao Guo,Yadian Wang,Hui Ma,Yuhao Fu,Ju Jia,Hui Liu,Shengeng Tang,Lechao Cheng,Yunfeng Diao,Ajian Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的统一物理-数字攻击检测框架 SPL-UAD，通过解耦不同攻击类型的优化分支和自适应生成提示，从而提升对人脸识别系统攻防的全面防护能力。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸识别系统同时面临物理攻击（如伪造面具）和数字伪造攻击（如深度伪造），但大多数检测方法在处理这两类攻击时存在优化目标冲突，难以实现统一且有效的防御。作者希望通过新方法解决这一冲突，实现对生物特征整体的安全保护。

Method: 提出“SPL-UAD”框架，把物理攻击和数字攻击的检测分成各自独立的提示分支，通过自适应生成提示（Spoofing Context Prompt Generation）实现每种类型的独立优化。同时，设计了提示感知的数据增强策略（Cues-awareness Augmentation），利用双提示机制生成更具挑战性的样本，提升模型面对未知攻击类型的鲁棒性。

Result: 在大规模UniAttackDataPlus数据集上的大量实验表明，SPL-UAD在统一检测物理与数字攻击任务中，相较于现有方法实现了显著的性能提升。

Conclusion: SPL-UAD框架通过提示分支解耦与自适应增强，能更有效地检测和防御物理与数字人脸攻击，为生物特征识别系统安全提供了更全面的方法支持。

Abstract: Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.

</details>


### [54] [Human3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos](https://arxiv.org/abs/2512.06368)
*Weitao Xiong,Zhiyuan Yuan,Jiahao Lu,Chengfeng Zhao,Peng Li,Yuan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种利用SMPL人体模型和单目深度估计融合的混合几何先验方法，显著提升了动态人类场景的单目动态视频重建的几何一致性与分辨率。


<details>
  <summary>Details</summary>
Motivation: 以往方法缺乏对人类三维结构的理解，导致肢体比例失真、人与物体融合不自然，以及在内存受限下下采样造成边界漂移等问题，影响了单目动态视频中人体重建的准确性。

Method: 作者提出了一种集成SMPL人体模型和单目深度估计的混合几何先验方法，并开发了Human3R层次化流程。该流程包括：全分辨率场景处理、针对人体的剪裁与跨注意力融合，以及利用特征融合模块整合SMPL先验以确保几何合理性并提升细节边界的表现。

Result: 在TUM Dynamics和GTA-IM数据集上的大量实验表明，该方法在动态人体重建任务中取得了优异的性能。

Conclusion: 融合人体结构先验和深度信息的方法能有效提升动态视频中人体及其边界的几何一致性和重建细节，为单目动态场景高质量重建提供了有力手段。

Abstract: Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce Human3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.

</details>


### [55] [VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2512.06373)
*Yuji Wang,Wenlong Liu,Jingxuan Niu,Haoji Zhang,Yansong Tang*

Main category: cs.CV

TL;DR: 本文提出了VG-Refiner框架，通过“思考-反思”机制与奖励函数，提高模型在面对不可靠工具输出时的纠错和推理能力，在指代与定位任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有工具整合视觉推理方法主要依赖强化学习整合工具，但忽视了应对工具不可靠输出的机制，尤其在指代与 grounding 任务中易被错误检测误导，导致推理出现幻觉。因此，需要新的方法显式回应和修正工具反馈错误。

Method: 提出VG-Refiner框架，引入两阶段“think-rethink”机制，让模型主动分析与响应视觉工具反馈，并设计 refinement reward 激励纠错行为。此外，定义两个新评价指标并构建公平评测协议，结合少量任务数据增强修正能力。

Result: 在指代与推理定位基准测试中，VG-Refiner在准确性和纠错能力上有显著提升，同时模型通用性能不受损。

Conclusion: VG-Refiner有效提升了工具整合视觉推理在面对不可靠工具输出时的优化与纠错能力，为视觉推理系统的可靠性和泛化能力提供了新思路。

Abstract: Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.

</details>


### [56] [Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework](https://arxiv.org/abs/2512.06376)
*Xinhao Xiang,Abhijeet Rastogi,Jiawei Zhang*

Main category: cs.CV

TL;DR: 本文探究了AI生成驾驶视频（AIGVs）是否能作为自动驾驶模型的训练和评估数据，通过提出新的基准与评估方法，发现AIGVs需经过筛选才能有效助力下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有文生视频模型可低成本高效生成驾驶场景，但尚不清楚这些AI生成数据是否能可靠支持自动驾驶模型的训练和评估。因此，作者希望系统性研究AIGV在自动驾驶领域的表现、安全性和应用价值。

Method: 作者提出AIGV常见失败类型的分类体系，并分析其对感知任务（如目标检测、跟踪、实例分割）的负面影响。构建了ADGV-Bench基准，包含多感知任务的人类标注数据。提出ADGVE评价器，融合语义、时序、道路合规等特征，并借助视觉-语言模型对每段视频给出质量分。

Result: 实验证明，直接加入未经筛选的AIGV会降低感知性能，而使用ADGVE筛选后的视频则能提升视频质量评估指标，并显著提升下游自动驾驶模型性能，AIGV成为现实数据的有益补充。

Conclusion: AIGV虽有缺陷和风险，但通过合适筛选和评测工具，可以安全有效地用于自动驾驶数据流水线。本工作为大规模自动驾驶视频生成的应用提供工具和分析，并指明未来利用方向。

Abstract: Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.

</details>


### [57] [VAD-Net: Multidimensional Facial Expression Recognition in Intelligent Education System](https://arxiv.org/abs/2512.06377)
*Yi Huo,Yun Ge*

Main category: cs.CV

TL;DR: 本文针对面部表情识别（FER）中情感识别维度有限的问题，首次为FER2013数据集引入三维度VAD（愉快-激活-支配）标签，并提出使用正交卷积改进VAD识别方法。


<details>
  <summary>Details</summary>
Motivation: 当前FER数据集大多只标注有限的情感类别，情感表达力不足，且缺乏更细致、连续的多维情感度量。AffectNet加了VA(愉快-激活)标签，但仍缺失了D(支配性)信息。作者希望使FER数据集在VAD三维情感量表下更实用。

Method: （1）为FER2013数据集手动标注D(支配性)维度，构建VAD注释版本。 （2）在网络结构上引入正交卷积，以提升网络对多样特征的提取能力，提高对VAD维度的预测精度。 （3）进行了人工标注与回归预测的实验与消融测试。

Result: （1）D维度在人工和网络预测中都可以测量，但相较于V和A更难获得高准确度。 （2）正交卷积结构能更好地进行VAD情感预测，验证了方法有效性。

Conclusion: 本研究首次为FER2013提供VAD三维情感标注的基准数据集，为支配性标签提供了实践方案，并通过正交卷积网络提升了VAD连续情感预测的表现。相关数据与代码已开源，有助于推动多维情感识别研究。

Abstract: Current FER (Facial Expression Recognition) dataset is mostly labeled by emotion categories, such as happy, angry, sad, fear, disgust, surprise, and neutral which are limited in expressiveness. However, future affective computing requires more comprehensive and precise emotion metrics which could be measured by VAD(Valence-Arousal-Dominance) multidimension parameters. To address this, AffectNet has tried to add VA (Valence and Arousal) information, but still lacks D(Dominance). Thus, the research introduces VAD annotation on FER2013 dataset, takes the initiative to label D(Dominance) dimension. Then, to further improve network capacity, it enforces orthogonalized convolution on it, which extracts more diverse and expressive features and will finally increase the prediction accuracy. Experiment results show that D dimension could be measured but is difficult to obtain compared with V and A dimension no matter in manual annotation or regression network prediction. Secondly, the ablation test by introducing orthogonal convolution verifies that better VAD prediction could be obtained in the configuration of orthogonal convolution. Therefore, the research provides an initiative labelling for D dimension on FER dataset, and proposes a better prediction network for VAD prediction through orthogonal convolution. The newly built VAD annotated FER2013 dataset could act as a benchmark to measure VAD multidimensional emotions, while the orthogonalized regression network based on ResNet could act as the facial expression recognition baseline for VAD emotion prediction. The newly labeled dataset and implementation code is publicly available on https://github.com/YeeHoran/VAD-Net .

</details>


### [58] [Rethinking Training Dynamics in Scale-wise Autoregressive Generation](https://arxiv.org/abs/2512.06421)
*Gengze Zhou,Chongjian Ge,Hao Tan,Feng Liu,Yicong Hong*

Main category: cs.CV

TL;DR: 本文提出了一种提升自回归生成模型质量的新方法SAR，有效缓解了多尺度图像生成中的曝光偏差问题。


<details>
  <summary>Details</summary>
Motivation: 自回归生成模型特别是在逐尺度（coarse-to-fine）图像生成任务中表现出色，但受限于训练与测试过程间的不一致（曝光偏差），以及不同尺度间优化难度不均，导致生成质量下降。本文旨在深度分析这些问题机理，并寻求新的训练策略以提升模型性能。

Method: 提出Self-Autoregressive Refinement (SAR) 框架，包含两大创新：1）Stagger-Scale Rollout（SSR）机制，在训练时让模型暴露于自身的中间预测结果，缓解训练与测试的不一致；2）Contrastive Student-Forcing Loss（CSFL），对模型自生成的上下文进行对比监督，保证训练收敛与稳定。该方法可无缝集成到已有的自回归模型中，作为高效的后训练方案。

Result: 实验证明SAR能在主流的自回归模型如FlexVAR-d16上带来显著提升。例如，在ImageNet 256数据集上，SAR在仅10个epoch训练（5小时，32块A100 GPU）下将FID指标下降5.2%。

Conclusion: SAR作为一种高效、可扩展、有效的后训练方法，有望广泛应用于视觉自回归生成领域，显著提升生成质量，建议常规引入到相关的生成任务中。

Abstract: Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.

</details>


### [59] [OCFER-Net: Recognizing Facial Expression in Online Learning System](https://arxiv.org/abs/2512.06379)
*Yi Huo,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种通过正则化约束卷积核正交性的方法（OCFER-Net），提升了人脸表情识别（FER）的准确性，实验显示其在FER-2013数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 疫情导致在线学习普及，情感交互在远程教育中日益重要。通过提升FER的准确率有助于教师实时了解学生情绪状态。当前算法很少关注卷积核正交性对表情特征提取的影响。

Method: 本研究在卷积核中引入正交性约束的正则项，通过增强特征多样性和表达能力，提出了OCFER-Net框架。

Result: 在复杂的FER-2013数据集上，OCFER-Net取得了比主流基线方法高1.087的准确率提升。

Conclusion: 通过卷积核正交性约束，OCFER-Net有效提升了FER性能，为在线情感分析和应用奠定了基础。

Abstract: Recently, online learning is very popular, especially under the global epidemic of COVID-19. Besides knowledge distribution, emotion interaction is also very important. It can be obtained by employing Facial Expression Recognition (FER). Since the FER accuracy is substantial in assisting teachers to acquire the emotional situation, the project explores a series of FER methods and finds that few works engage in exploiting the orthogonality of convolutional matrix. Therefore, it enforces orthogonality on kernels by a regularizer, which extracts features with more diversity and expressiveness, and delivers OCFER-Net. Experiments are carried out on FER-2013, which is a challenging dataset. Results show superior performance over baselines by 1.087. The code of the research project is publicly available on https://github.com/YeeHoran/OCFERNet.

</details>


### [60] [The Role of Entropy in Visual Grounding: Analysis and Optimization](https://arxiv.org/abs/2512.06726)
*Shuo Li,Jiajun Sun,Zhihao Zhang,Xiaoran Fan,Senjie Jin,Hui Li,Yuming Yang,Junjie Ye,Lixing Shen,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: 该论文关注在视觉指向（visual grounding）任务中，提出了一种新的熵控制算法ECVGPO，用于提升多模态大语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型用强化学习进行微调已取得进展，但在视觉指向这类感知类任务中，熵（entropy）如何影响模型输出及其控制方式还未被深入研究。

Method: 本文首先分析了视觉指向与推理任务中熵的不同作用与特性，然后基于分析结果提出了一种新的、可解释的熵控制优化算法ECVGPO（Entropy Control Visual Grounding Policy Optimization）。该方法通过精细调控熵，更好地平衡模型的探索与利用。

Result: 实验结果表明，ECVGPO在多个视觉指向基准测试和不同模型上都带来了全面的性能提升。

Conclusion: 控制熵在视觉指向任务中具有重要价值，ECVGPO为相关任务带来提升，同时为多模态模型的RL微调提供了新的视角与方法。

Abstract: Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.

</details>


### [61] [Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement](https://arxiv.org/abs/2512.06400)
*Jing Tao,Yonghong Zong,Banglei Guana,Pengju Sun,Taihang Lei,Yang Shanga,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于区域感知的红外（IR）与可见光（VIS）图像融合框架，能在极端环境下提高融合图像的清晰度和测量精度。


<details>
  <summary>Details</summary>
Motivation: 传统红外与可见光图像融合方法常在极端环境下牺牲可见光的几何特征，导致测量精度下降。本文旨在实现兼顾可见光清晰度和热辐射信息保留的高质量融合。

Method: 提出了利用空间变化曝光（SVE）相机采集多曝光、多模态数据，并根据区域感知的特征融合算法进行多模态、多曝光联合融合。其流程包括：区分区域的特征匹配与精确配准、自适应融合与对比度增强、借助区域显著性图的结构相似补偿机制。此外，框架也可灵活适应单曝光场景。

Result: 在合成和真实数据集上的实验表明，该方法在图像清晰度和整体表现方面优于现有主流方法，且在定量和视觉评估中都取得了更好结果。

Conclusion: 该框架能有效提升极端条件下IR-VIS融合的质量，为测量等需求提供更高的精度和清晰度，具有实际应用潜力。

Abstract: In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.

</details>


### [62] [MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2512.06810)
*Yueqian Wang,Songxiang Liu,Disong Wang,Nuo Xu,Guanglu Wan,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于文本的方法用于视频多模态大模型（Video MLLMs）的主动交互，实现模型在视频播放时自主决定何时回复用户。通过多轮强化学习训练模型，显著改进了应答时机与质量，在ProactiveVideoQA基准上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频多模态大模型大多采用轮询式（turn-based）交互，仅能在用户发言后做出回应，无法满足实时主动交互的需求。而当前实现主动交互的方法涉及人工调整阈值和精确的应答时间标注，既繁琐又不高效。因此，亟需一种可自动决定何时应答且容易训练的新方法。

Method: 提出一种基于文本到文本的主动交互框架，模型根据历史对话及当前视频帧的视觉内容自主判断是否应答。为避免繁琐的人为阈值调整和回复时刻标注，采用多轮强化学习训练方法（RL），通过收集含对话的视频数据进行监督微调（SFT）和强化学习训练，使模型学会在合适时机给出高质量回应。

Result: 在包含52,000个视频的两类对话数据集上训练所得模型MMDuet2，在主动应答时机和答复质量上，均显著优于现有主动型Video MLLM方法。在ProactiveVideoQA基准测试上取得业界领先性能。

Conclusion: 基于文本到文本的主动交互模型结合多轮强化学习显著提升了视频多模态大模型的主动对话能力和实时响应性能，为相关领域的实际应用（如实时视频助理等）提供了高效的解决方案，具有很强的应用价值和推广前景。

Abstract: Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.

</details>


### [63] [Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior](https://arxiv.org/abs/2512.06866)
*Yulin Li,Haokun Gui,Ziyang Fan,Junjie Wang,Bin Kang,Bin Chen,Zhuotao Tian*

Main category: cs.CV

TL;DR: 提出了一种名为DyToK的高效视频大语言模型（VLLM）token压缩方法，大幅提高推理速度且保持准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型处理长视频时计算量随视觉token序列长度呈二次增长，效率低下。虽然关键帧采样法可提升效率，但在特征编码前引入额外开销且二元选择策略并不最佳。

Method: DyToK是一种训练无关的新范式，利用VLLM自带的注意力机制，通过分析其注意力层实现基于查询条件的关键帧先验，动态确定每帧token保留比率，有效压缩冗余帧，突出语义丰富帧，且可与其他压缩方法结合。

Result: 大规模实验显示，DyToK在多个VLLM（如LLaVA-OneVision、Qwen2.5-VL）上实现了4.3倍推理加速且准确率不降，同时能与VisionZip、FastV等现有方法无缝兼容。

Conclusion: DyToK实现了高效的token压缩，兼备效率和准确率，在不同VLLM框架下表现出优异的通用性和应用前景。

Abstract: Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .

</details>


### [64] [A Perception CNN for Facial Expression Recognition](https://arxiv.org/abs/2512.06422)
*Chunwei Tian,Jingyuan Xie,Lingjun Li,Wangmeng Zuo,Yanning Zhang,David Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种融合局部和全局特征的感知卷积神经网络（PCNN），显著提升了人脸表情识别（FER）的效果，在多个公开数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 传统的卷积神经网络（CNN）虽然能表达人脸图像特征，但可能忽略了人脸局部分块（如眼睛、脸颊、嘴巴）在表情识别中的重要作用。因此，研究者希望设计能自动融合局部与全局特征的网络结构，提升表情识别的精度与鲁棒性。

Method: 作者提出的PCNN利用五个并行网络分别学习包括眼睛、脸颊和嘴巴等局部感官特征，同时通过多域交互机制整合本地（局部）和整体（结构性）特征。为保证网络学习能力，设计了两阶段损失函数，限制感官信息和重建脸部图像的准确性。

Result: 在CK+、JAFFE、FER2013、FERPlus、RAF-DB和遮挡与姿态变化数据集等多个实验基准上，PCNN取得了优异和领先的表情识别准确率。

Conclusion: PCNN通过并行局部特征提取、多域融合和新型损失函数，有效提升了人脸表情识别的性能，为该领域提供了一种有效的新方法。代码已开源，便于复现和进一步研究。

Abstract: Convolutional neural networks (CNNs) can automatically learn data patterns to express face images for facial expression recognition (FER). However, they may ignore effect of facial segmentation of FER. In this paper, we propose a perception CNN for FER as well as PCNN. Firstly, PCNN can use five parallel networks to simultaneously learn local facial features based on eyes, cheeks and mouth to realize the sensitive capture of the subtle changes in FER. Secondly, we utilize a multi-domain interaction mechanism to register and fuse between local sense organ features and global facial structural features to better express face images for FER. Finally, we design a two-phase loss function to restrict accuracy of obtained sense information and reconstructed face images to guarantee performance of obtained PCNN in FER. Experimental results show that our PCNN achieves superior results on several lab and real-world FER benchmarks: CK+, JAFFE, FER2013, FERPlus, RAF-DB and Occlusion and Pose Variant Dataset. Its code is available at https://github.com/hellloxiaotian/PCNN.

</details>


### [65] [NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification](https://arxiv.org/abs/2512.06921)
*Ziyang Song,Zelin Zang,Xiaofan Ye,Boqiang Xu,Long Bai,Jinlin Wu,Hongliang Ren,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: 本文提出了NeuroABench，这是首个专注于神经外科领域解剖理解的多模态评测基准，包含9小时带注释的视频和68类解剖结构的识别任务。实验显示现有主流多模态大模型在此任务上表现有限，远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在手术视频理解上表现突出，但缺乏对手术中精确解剖知识的关注和评估，而解剖理解对临床诊疗和外科教育至关重要。本工作正是为弥补这一研究空白。

Method: 作者开发了NeuroABench基准，包含89种神经外科手术、9小时高质量视频，并设计多模态注释流程。针对68种解剖结构，对10+多模态大模型进行识别测试，并与4名神经外科实习生的表现进行对比。

Result: 最优模型在解剖结构识别上准确率仅40.87%，低于神经外科实习生平均46.5%的水平，虽与最低表现的学生相当，但与人类整体差距明显。

Conclusion: 多模态大模型在神经外科解剖理解方面虽有进步，但与人类能力相比仍存在较大提升空间。NeuroABench为今后模型改进和评估提供了重要基准。

Abstract: Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.

</details>


### [66] [DragMesh: Interactive 3D Generation Made Easy](https://arxiv.org/abs/2512.06424)
*Tianshan Zhang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: DragMesh是一种实现实时、交互式3D物体运动生成的系统，通过解耦运动学推理与运动生成，克服了现有方法的速度与物理一致性难题。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型难以高效且准确地模拟物体运动，现有方案要么物理上准确但实时性能差，要么生成能力强但不符合运动学约束，亟需一种兼顾效率与物理合理性的解决方案。

Method: DragMesh提出了两个核心创新：1）通过KPP-Net解耦运动学语义推理和几何回归，推断关节参数；2）基于对偶四元数提出Dual Quaternion VAE（DQ-VAE），结合FiLM条件注入，实现多尺度运动生成，同时用数值稳定的损失确保运动轴对齐和严格的运动学约束。

Result: DragMesh达到了实时性能，并能在无需重新训练的情况下，对新物体实现具有物理合理性的生成式关节运动，解决了现有实时性与物理一致性难以兼得的难题。

Conclusion: DragMesh为生成式3D智能迈出了实用的一步，能够高效、合理地生成新物体的运动，对交互式3D内容生成具有重要意义。

Abstract: While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.

</details>


### [67] [Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models](https://arxiv.org/abs/2512.07141)
*Fenghua Weng,Chaochao Lu,Xia Hu,Wenqi Shao,Wenjie Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Think-Reflect-Revise (TRR) 的三阶段训练框架，显著提升了大规模视觉语言模型(LVLMs)的安全表现，抵御越狱攻击，并大幅提高安全响应率。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理虽然提升了LVLMs的综合能力，也出现了面向安全的推理框架，但现有“先思考再作答”的单步流程仍易受到越狱攻击，且可能遗漏自生成内容中的有害信息。作者因此希望通过反思机制实现模型的自我纠正，提升安全性。

Method: TRR采用“思考-反思-修订”三阶段：1) 构建了包含5,000个思考-反思-修订示例的ReSafe数据集，用于引导模型的反思行为；2) 利用该数据集微调目标模型初始化反思能力；3) 通过基于策略的强化学习进一步增强反思修正过程。

Result: TRR在安全性评测和越狱攻击测试中显著提升安全表现，在Qwen2.5-VL-7B模型上的安全响应率从42.8%提升至87.7%，同时在通用任务基准（如MMMU、MMStar）上保持性能稳定。

Conclusion: 通过引入反思和修订机制，TRR有效提升了LVLMs的安全性，对越狱攻击具备更强防御力，并且不影响模型在通用任务上的表现。

Abstract: As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.

</details>


### [68] [When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition](https://arxiv.org/abs/2512.06426)
*Nzakiese Mbongo,Kailash A. Hambarde,Hugo Proença*

Main category: cs.CV

TL;DR: 本文提出了一种基于CLIP的双路径Transformer框架，实现了远距离图像中的性别识别，并在新构建的大规模数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 远距离图像中的性别识别因分辨率低、视角变化和面部信息丢失而极具挑战。作者希望通过结合视觉与软生物特征信息，提高在无约束远距离场景下的性别识别准确率和可靠性。

Method: 方法上，框架包含两个互补分支：一是视觉路径，利用CLIP图像编码器的高层进行微调；二是属性路径，基于如发型、服饰、配饰等软生物特征生成prompt，在CLIP的文本-图像空间中推断性别。空间通道注意力模块进一步提升了低分辨率和遮挡情况下的判别力。此外，作者构建了统一的大规模远距离性别数据集U-DetAGReID。

Result: 实验结果显示，该方法在多个评价指标（macro-F1、准确率、AUC）上均超越了现有的基线模型，且在距离、角度和高度变化下表现稳健。可视化分析表明框架可以解释性地定位属性，并能在信息不足时合理地选择“不确定”标签。

Conclusion: 语言引导的双路径学习为远距离非约束性别识别提供了可扩展且负责任的技术基础，具备实际部署和进一步拓展的潜力。

Abstract: Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.

</details>


### [69] [Generating Storytelling Images with Rich Chains-of-Reasoning](https://arxiv.org/abs/2512.07198)
*Xiujie Song,Qi Jia,Shota Watanabe,Xiaoyi Pang,Ruijie Chen,Mengyue Wu,Kenny Q. Zhu*

Main category: cs.CV

TL;DR: 本论文提出了面向生成富有故事性的图像的新任务与解决方案，并提出了一套生成与评估框架，有效拓展了图像内容生成的边界。


<details>
  <summary>Details</summary>
Motivation: 故事性图像因其蕴含丰富且逻辑关联的视觉线索，能够传递复杂的信息与因果关系，但生成此类图像难度高、样本稀缺，亟需自动化生成方法。

Method: 作者提出了两阶段生成管线StorytellingPainter，结合大语言模型（LLM）负责创意推理，以及文生图模型（T2I）进行视觉合成。为评估效果，专门设计了语义复杂度、KNN多样性、故事图像一致性三大评估器。同时探索了LLM微调策略，训练出一批高效小模型Mini-Storytellers。

Result: 实验结果显示，所提方法在可行性和有效性方面表现良好，并有助于缩小开源与专有LLM在该任务上的差距。

Conclusion: 本文工作为生成与理解复杂故事性图像提供了新思路，对促进多模态内容创作具有重要意义。相关代码已开源，可支持后续研究与应用。

Abstract: An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.

</details>


### [70] [Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening](https://arxiv.org/abs/2512.06434)
*Lucas R. Mareque,Ricardo L. Armentano,Leandro J. Cymberknop*

Main category: cs.CV

TL;DR: 本研究提出了一种利用深度学习从2D人体合成图像中自动估算关键人体测量数据的方法，精度高，有望应用于运动员心脏筛查。


<details>
  <summary>Details</summary>
Motivation: 运动员心脏猝死多与结构或电气异常有关。通过身体测量（如腰围、四肢长度等）能够发现类似马凡综合征等高风险人群。目前，这些测量依赖人工，效率低且主观性强，难以大范围推广。

Method: 作者使用10万组基于3D人体网格生成的2D合成图像，训练并评估了VGG19、ResNet50和DenseNet121三种深度神经网络，回归预测五项关键人体测量值。

Result: 所有模型都实现了亚厘米级的测量精度，ResNet50表现最优，所有测量均值MAE为0.668厘米。

Conclusion: 深度学习方法能够高效且准确地实现关键人体测量，为运动员筛查提供自动化工具。未来将验证其在真实图像中的应用效果，以进一步拓展实用性。

Abstract: Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.

</details>


### [71] [Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models](https://arxiv.org/abs/2512.07564)
*Kassoum Sanogo,Renzo Ardiccioni*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的自纠错框架，让视觉-语言模型（VLMs）通过基于不确定性的视觉重关注机制，迭代优化生成内容，以减少生成幻觉（错误但符合逻辑的描述）。


<details>
  <summary>Details</summary>
Motivation: VLMs容易对图像内容生成看似合理但实际错误的描述（幻觉），影响其在实际应用中的可靠性。现有纠错方法多依赖于模型额外训练或复杂操作，亟需一种高效、简单的方法提升VLMs的可信度。

Method: 提出无需训练、基于多维不确定性量化（如token熵、注意力分散度、语义一致性、断言置信度）与注意力引导的图像裁剪框架。在冻结的、预训练VLM上，通过识别高不确定性的区域，完成视觉重关注和输出修正。全程无需梯度更新。

Result: 在POPE和MMHAL BENCH数据集上、以Qwen2.5-VL-7B为基础，所提方法将幻觉率降低了9.8%，目标存在准确率提升了4.7%；定性分析显示方法能有效利用视觉证据修正输出，相较于标准推理更稳健。

Conclusion: 该方法无需重新训练即可有效减少VLM幻觉，提高输出可靠性。代码及方法已开源，为提升多模态模型可信性研究提供新工具。今后将拓展至更多模型架构的验证。

Abstract: Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.

</details>


### [72] [AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars](https://arxiv.org/abs/2512.06438)
*Ramazan Fazylov,Sergey Zagoruyko,Aleksandr Parkin,Stamatis Lefkimmiatis,Ivan Laptev*

Main category: cs.CV

TL;DR: 本文提出AGORA框架，在3D Gaussian Splatting基础上结合生成对抗网络，实现在逼真动态人脸三维重建中的高效率和可控性。性能显著优于基于NeRF的方法，并可在单GPU和CPU上实现实时推理。


<details>
  <summary>Details</summary>
Motivation: 现有高保真、可动画3D人类头像生成技术存在渲染速度慢（如NeRF）或动态表现差（如3D Gaussian Splatting仅限静态生成）等问题，阻碍其在虚拟现实、远程在场和娱乐领域的应用。该工作旨在弥合这一技术空白。

Method: 提出AGORA框架，将3D Gaussian Splatting扩展到可动画头像生成。通过轻量级的FLAME条件变形分支预测每个高斯点的残差，实现高精度、可实时的表情控制。并采用双判别器训练策略，利用合成参数网格渲染提升表情一致性。

Result: 相比最新的NeRF-based方法，AGORA在表情准确率上取得更优结果。在单GPU下支持250+FPS实时渲染，CPU环境下也能达到约9FPS，展现了首次实用的CPU端可动画3DGS头像生成系统。

Conclusion: AGORA代表了可控、高性能数字人的重要进展，为实际应用中的高保真3D头像建模带来了显著提升。

Abstract: The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/

</details>


### [73] [Towards Stable Cross-Domain Depression Recognition under Missing Modalities](https://arxiv.org/abs/2512.06447)
*Jiuyi Chen,Mingkui Tan,Haifeng Lu,Qiuna Xu,Zhihua Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态大语言模型（SCD-MLLM）的抑郁症跨领域识别统一框架，有效提升了应对不同场景、缺失模态情况下的检测稳定性，并在多个公开数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前声像多模态自动抑郁症检测方法缺乏统一和高度泛化的框架，且在实际应用中面临模态缺失导致的不稳定，因此亟需一种稳健有效的跨场景识别方案。

Method: 提出SCD-MLLM框架，包括多源数据输入适配器MDIA（通过掩码机制和任务指令解决异构数据一致性），以及模态感知自适应融合模块MAFM（通过共享投影机制自适应整合音频和视觉特征），以实现对异构多源输入的统一处理和模态缺失情况下的鲁棒融合。

Result: 在CMDC、AVEC2014、DAIC-WOZ、DVlog与EATD五个公开多样抑郁症数据集上，进行完整和模态缺失环境下的联合训练实验。SCD-MLLM在各项指标上均优于当前主流模型及商用大模型（如Gemini、GPT），表现出优越的跨领域泛化能力和对模态缺失的稳定鲁棒性。

Conclusion: SCD-MLLM为自动化抑郁症检测提供了统一、稳定、通用的多模态大模型解决方案，在复杂现实应用场景中具备更强泛化与实用价值。

Abstract: Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.

</details>


### [74] [Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction](https://arxiv.org/abs/2512.06485)
*Kush Revankar,Shreyas Deshpande,Araham Sayeed,Ansh Tandale,Sarika Bobde*

Main category: cs.CV

TL;DR: Sanvaad提出了一个轻量级多模态无障碍沟通框架，支持听障、视障及普通用户之间的实时双向交流。


<details>
  <summary>Details</summary>
Motivation: 现有交流工具多只能支持单向交流，难以满足听障、视障群体与普通人之间的实时、双向互动需求，因此需要一个更加包容且高效的解决方案。

Method: Sanvaad框架包含面向听障用户的印度手语（ISL）识别模块，利用MediaPipe高效地检测手势，实现低算力设备的运行。还配备语音转手语的组件，将手机语音输入映射到预定义短语并生成GIF或字母可视化。针对视障用户，框架提供免屏语音界面，集成多语言语音识别、文本摘要及语音合成等功能，所有模块基于Streamlit前端，适配桌面与移动端。

Result: Sanvaad能够在无需专用硬件的情况下，实现高效的手语识别、语音转手语、语音互动等功能，有效支持跨障碍人群的实时双向交流。

Conclusion: 通过融合计算机视觉与语音处理技术，Sanvaad为听障、视障与普通人群间的包容性交流提供了实用、可扩展的解决方案。

Abstract: Communication between deaf users, visually im paired users, and the general hearing population often relies on tools that support only one direction of interaction. To address this limitation, this work presents Sanvaad, a lightweight multimodal accessibility framework designed to support real time, two-way communication. For deaf users, Sanvaad includes an ISL recognition module built on MediaPipe landmarks. MediaPipe is chosen primarily for its efficiency and low computational load, enabling the system to run smoothly on edge devices without requiring dedicated hardware. Spoken input from a phone can also be translated into sign representations through a voice-to-sign component that maps detected speech to predefined phrases and produces corresponding GIFs or alphabet-based visualizations. For visually impaired users, the framework provides a screen free voice interface that integrates multilingual speech recognition, text summarization, and text-to-speech generation. These components work together through a Streamlit-based interface, making the system usable on both desktop and mobile environments. Overall, Sanvaad aims to offer a practical and accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools within a unified framework.

</details>


### [75] [ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images](https://arxiv.org/abs/2512.06521)
*Jens Dede,Anna Förster*

Main category: cs.CV

TL;DR: 本文针对野生动物监测中AI模型面临的环境多样性挑战，提出了名为ShadowWolf的统一框架，提升模型适应性与效率。


<details>
  <summary>Details</summary>
Motivation: 随着人类活动范围扩大，人类与野生动物的交互日益增多，野生动物监测需求增加，但传统AI方法难以适应多变环境。亟需能高效适应不同场景的新方法来提升监测效果。

Method: 提出ShadowWolf框架，将AI图像识别的采集、标注和训练等流程进行统一设计与优化，实现动态重训练以适应环境变化，并减少人工标注负担，使模型可以现场自适应。

Result: 通过整合AI训练和评估流程，动态调整和减少人工参与，提高了野生动物识别的准确性和效率，验证了框架在实际应用中的有效性。

Conclusion: ShadowWolf框架为野生动物监测AI模型提供了高适应性与高效率的解决方案，有助于推动大规模、智能化的保护工作。

Abstract: The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.
  In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.

</details>


### [76] [On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization](https://arxiv.org/abs/2512.06530)
*Mohammed Wattad,Tamir Shor,Alex Bronstein*

Main category: cs.CV

TL;DR: 本论文探讨了通过学习k-space采集模式提升MRI重建质量，并首次系统分析了该方法在跨域场景下的泛化能力。研究发现学习的k-space采样可以改善域外重建效果，并提出通过在训练阶段引入采集不确定性以进一步提升其稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前大部分基于学习的k-space采样研究只关注对单一数据集或模态的优化，缺乏对这种采样模式在不同成像领域间迁移能力的系统分析。随着医学影像领域设备及成像条件多样化，提升采样方案的泛化能力和鲁棒性亟需解决。

Method: 研究首先系统性地在不同数据集及采集范式下，对学习的k-space采样方案进行了横向评估。同时，提出在训练阶段对k-space轨迹进行随机扰动，模拟不同扫描仪及成像环境的变化，从而提升泛化鲁棒性。

Result: 实验表明：1）通过学习的采样方案，可显著提升重建模型在跨域任务下的表现；2）引入采集不确定性的创新训练方法能进一步加强域泛化能力。

Conclusion: k-space采样策略不仅仅是提升扫描加速的手段，更应作为主动优化MRI重建泛化性的自由度。该工作推动了向更强鲁棒性及实用性采样设计方案的探索。

Abstract: Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.

</details>


### [77] [Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images](https://arxiv.org/abs/2512.06531)
*Sayan Das,Arghadip Biswas*

Main category: cs.CV

TL;DR: 该论文提出了两个基于深度学习的新架构，用于脑肿瘤的自动分类和分割，在相关数据集上取得了极高的准确率。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤威胁生命，早期准确检测至关重要。近年来，脑肿瘤发病率上升，手动分析MRI图像费时费力，现有模型泛化能力弱。为提高自动检测准确性和效率，作者提出新型AI辅助方案。

Method: 该论文提出了两种新架构：1）SAETCN（自注意力增强肿瘤分类网络），用于脑肿瘤（胶质瘤、脑膜瘤、垂体瘤及无瘤病例）的分类；2）SAS-Net（自注意力分割网络），用于肿瘤分割。均利用深度学习和自注意力机制。

Result: SAETCN在分类任务的验证集上达到99.38%的准确率，SAS-Net在分割任务上像素准确率达到99.23%。

Conclusion: 所提网络大幅提升了脑肿瘤检测和分割的自动化准确率，为临床早期诊断和治疗提供了强有力的辅助工具。

Abstract: Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.

</details>


### [78] [Bridging spatial awareness and global context in medical image segmentation](https://arxiv.org/abs/2512.06560)
*Dalia Alzu'bi,A. Ben Hamza*

Main category: cs.CV

TL;DR: 本文提出了一种新型轻量级医学图像分割网络U-CycleMLP，有效提升分割精度并保持效率，在多个基准数据集上优于现有方法，尤其在边界识别和多模态鲁棒性方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割模型难以同时捕捉局部与全局上下文信息，导致分割边界模糊和精度下降，因此需要设计一种兼顾准确性与计算效率的新型分割方法。

Method: 提出U-CycleMLP，一种U型编码器-解码器结构。编码器通过位置注意力权重激励块、密集空洞块和下采样，提取多尺度上下文信息；解码器通过上采样、密集空洞块与特征融合重建高分辨率分割图。特有的通道CycleMLP块用于跳跃连接，提升特征融合效果，并保持线性计算复杂度。

Result: 在三个医学图像分割基准数据集上，U-CycleMLP在定量和定性实验中均取得优异表现。不仅分割精度优于现有主流方法，还能准确捕捉细粒度结构，对不同成像模态具有良好鲁棒性。消融实验验证了各模块在提升性能中的重要作用。

Conclusion: U-CycleMLP实现了分割准确性与轻量化的良好平衡，有效提升了医学图像分割的性能，具备现实应用潜力。

Abstract: Medical image segmentation is a fundamental task in computer-aided diagnosis, requiring models that balance segmentation accuracy and computational efficiency. However, existing segmentation models often struggle to effectively capture local and global contextual information, leading to boundary pixel loss and segmentation errors. In this paper, we propose U-CycleMLP, a novel U-shaped encoder-decoder network designed to enhance segmentation performance while maintaining a lightweight architecture. The encoder learns multiscale contextual features using position attention weight excitation blocks, dense atrous blocks, and downsampling operations, effectively capturing both local and global contextual information. The decoder reconstructs high-resolution segmentation masks through upsampling operations, dense atrous blocks, and feature fusion mechanisms, ensuring precise boundary delineation. To further refine segmentation predictions, channel CycleMLP blocks are incorporated into the decoder along the skip connections, enhancing feature integration while maintaining linear computational complexity relative to input size. Experimental results, both quantitative and qualitative, across three benchmark datasets demonstrate the competitive performance of U-CycleMLP in comparison with state-of-the-art methods, achieving better segmentation accuracy across all datasets, capturing fine-grained anatomical structures, and demonstrating robustness across different medical imaging modalities. Ablation studies further highlight the importance of the model's core architectural components in enhancing segmentation accuracy.

</details>


### [79] [SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities](https://arxiv.org/abs/2512.06562)
*Dung Thuy Nguyen,Quang Nguyen,Preston K. Robinette,Eli Jiang,Taylor T. Johnson,Kevin Leach*

Main category: cs.CV

TL;DR: 本论文提出了SUGAR框架，可以在不重新训练整个模型的情况下，有效并大规模地移除生成式3D人像模型中的特定人物身份，同时保持模型输出质量。


<details>
  <summary>Details</summary>
Motivation: 随着3D感知生成模型在高保真度人像合成上的进步，如何在保障用户同意的前提下，及时有效地从模型输出中删除特定人物身份，成为迫切的问题。现有删除方法或导致输出失真，或依赖静态模板脸，存在局限性。

Method: SUGAR框架针对每个身份学习专属的替代潜变量，从而让涉及被删除身份的重建结果偏向其他合理且连贯的面孔，同时引入持续性效用保持目标，确保随着更多身份被移除，模型整体性能不会明显降低。

Result: SUGAR能在不重新训练整个模型的前提下，连续或同时移除多达200个身份，并相较于现有方法，在保留模型功能效用方面提升700%。

Conclusion: SUGAR为生成式模型中的大规模身份遗忘问题提供了高效、实用的解决方案，在保证数据隐私和同意的同时，能够大幅提升模型的灵活性与实际应用可行性。

Abstract: Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.

</details>


### [80] [GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation](https://arxiv.org/abs/2512.06565)
*Xiujin Liu*

Main category: cs.CV

TL;DR: 本文提出了一种无需学习的单目6D物体位姿估计算法GNC-Pose，融合了渲染初始化、基于几何的对应加权和强鲁棒性的GNC优化，在无训练和先验的条件下达到与主流方法相当的效果。


<details>
  <summary>Details</summary>
Motivation: 目前6D物体位姿估计领域，深度学习方法虽准确但对训练数据依赖大，非学习方法易受外点影响且表现有限。该工作旨在提出一种既无需训练又能鲁棒应对外点的解决方案。

Method: 方法首先通过特征匹配和渲染实现2D-3D初始对应，基于GNC优化框架，引入了考虑三维结构一致性的加权机制，使每个点根据其几何一致性分配置信度，可有效抵抗外点影响，最后通过LM优化进一步提升精度。

Result: 在YCB Object和Model Set数据集上，GNC-Pose无需任何已学习特征、训练数据或类别先验即可达到与主流学习/非学习方法相当的结果。

Conclusion: GNC-Pose在无需训练和场景先验的条件下，对有纹理物体实现了高鲁棒性和高精度6D位姿估计，方案简明实用，有望应用于实际场景中的学习无关位姿估计任务。

Abstract: We present GNC--Pose, a fully learning--free monocular 6D object pose estimation pipeline for textured objects that combines rendering--based initialization, geometry--aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D--3D correspondences obtained through feature matching and rendering--based alignment, our method builds upon the Graduated Non--Convexity (GNC) principle and introduces a geometry--aware, cluster--based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC--Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC--Pose achieves competitive accuracy compared with both learning-based and learning--free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.

</details>


### [81] [Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules](https://arxiv.org/abs/2512.06575)
*Fariza Dahes*

Main category: cs.CV

TL;DR: 本文将一种在阿尔茨海默MRI上表现良好的轻量级ConvNeXt Tiny架构方法验证并拓展到乳腺X光片分类任务，通过多骨干比较和多评估指标，确认部分模块有效，但也发现特定损失函数并非在所有场景都有效。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像分类对高性能但计算友好的深度学习方法需求强烈；作者希望验证并推广一种近期在MRI分类上有效的新型架构，看看其能否改善乳腺X光片的良恶性判别，降低误检率。

Method: 作者使用整合自Kaggle（含INbreast、MIAS和DDSM）的乳腺X光片数据集，对基线CNN、ConvNeXt Tiny和InceptionV3三种架构进行对比，均加入GAGM（全局均值与最大池化融合）、SEVector（轻量通道注意力）模块，及Feature Smoothing Loss（FSL）。评估采用F1、召回方差、ROC/AUC等多指标，并用Grad-CAM分析特征可解释性，还开发了临床可交互仪表盘。

Result: GAGM和SEVector显著提升了特征分辨能力并降低恶性假阴性，但FSL在乳腺X光片分类下效果有限，显示其依赖于特定架构及计算场景；多指标和可解释性分析支持上述结论。

Conclusion: 部分新方法在乳腺X光片分类中通用且具有实用前景，尤其有助于恶性检出，但有的改进（如FSL）对不同任务的适用性有限，后续需进一步探索提高类内紧凑度和类间可分性的新方法。

Abstract: This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.

</details>


### [82] [MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding](https://arxiv.org/abs/2512.06581)
*Yuhao Su,Anwesa Choudhuri,Zhongpai Gao,Benjamin Planche,Van Nguyen Nguyen,Meng Zheng,Yuhan Shen,Arun Innanje,Terrence Chen,Ehsan Elhamifar,Ziyan Wu*

Main category: cs.CV

TL;DR: 本文提出了一个针对医学视频理解的大规模基准MedVidBench和一种多数据集平衡强化学习训练框架MedGRPO，显著提升了视觉-语言模型在医学领域多任务表现，优于主流大模型。


<details>
  <summary>Details</summary>
Motivation: 当前的大型视觉-语言模型在处理医学视频时，难以实现空间精度、时序推理和临床语义的准确解读，因此需要专门针对医学视频的基准和优化训练方法。

Method: 1) 构建MedVidBench基准，包括53万多条医学视频指导配对样本，涵盖八大来源及多任务。2) 提出MedGRPO新型强化学习框架，引入跨数据集奖励归一化和基于医学大型语言模型的临床多维评判，解决标准RL在多数据不均衡时的奖励失衡和训练坍塌问题。3) 以Qwen2.5-VL-7B为例，在MedVidBench上做有监督微调，并在此基础上应用MedGRPO进行进一步训练。

Result: 有监督微调后的Qwen2.5-VL-7B在所有任务上大幅超越GPT-4.1和Gemini-2.5-Flash，MedGRPO进一步提升模型在标注和溯源类任务的表现，验证了新基准和方法的有效性。

Conclusion: MedVidBench为医学视觉-语言模型提供了基础性、权威性的评测基准，MedGRPO则为多数据集医学视频建模提供了稳定可靠的训练范式，有助于推动医疗AI视觉-语言理解能力的提升。

Abstract: Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.

</details>


### [83] [From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain](https://arxiv.org/abs/2512.06598)
*Muhammad Adil,Patrick J. Clemins,Andrew W. Schroth,Panagiotis D. Oikonomou,Donna M. Rizzo,Peter D. F. Isles,Xiaohan Zhang,Kareem I. Hannoun,Scott Turnbull,Noah B. Beckage,Asim Zia,Safwan Wshah*

Main category: cs.CV

TL;DR: 本文提出了一种基于遥感的预测框架，结合Transformers和BiLSTM模型，可提前14天预测湖泊中蓝藻水华（CyanoHABs）事件，取得了显著的准确率。


<details>
  <summary>Details</summary>
Motivation: 湖泊中的蓝藻水华频发，对生态和公共健康造成威胁，但常规监测点有限，难以及时掌握动态，亟需高效、覆盖广的预测手段。

Method: 采用Transformers与BiLSTM深度学习模型，输入稀疏卫星遥感时间序列数据（Cyanobacterial Index和温度），通过两步预处理（前向填充加加权时序插补、平滑处理）解决缺失，提取等频箱特征和温度统计量进行预测。

Result: 方法在1、2、3天预测分别达到89.5%、86.4%、85.5%的F1分数，14天仍有78.9%的F1和82.6%的AUC，表现优异。

Conclusion: 该框架有效利用稀疏遥感数据，实现对蓝藻水华的时空早期预警，为湖泊环境管理提供了可靠技术支撑。

Abstract: Cyanobacterial Harmful Algal Blooms (CyanoHABs) pose significant threats to aquatic ecosystems and public health globally. Lake Champlain is particularly vulnerable to recurring CyanoHAB events, especially in its northern segment: Missisquoi Bay, St. Albans Bay, and Northeast Arm, due to nutrient enrichment and climatic variability. Remote sensing provides a scalable solution for monitoring and forecasting these events, offering continuous coverage where in situ observations are sparse or unavailable. In this study, we present a remote sensing only forecasting framework that combines Transformers and BiLSTM to predict CyanoHAB intensities up to 14 days in advance. The system utilizes Cyanobacterial Index data from the Cyanobacterial Assessment Network and temperature data from Moderate Resolution Imaging Spectroradiometer satellites to capture long range dependencies and sequential dynamics in satellite time series. The dataset is very sparse, missing more than 30% of the Cyanobacterial Index data and 90% of the temperature data. A two stage preprocessing pipeline addressed data gaps by applying forward fill and weighted temporal imputation at the pixel level, followed by smoothing to reduce the discontinuities of CyanoHAB events. The raw dataset is transformed into meaningful features through equal frequency binning for the Cyanobacterial Index values and extracted temperature statistics. Transformer BiLSTM model demonstrates strong forecasting performance across multiple horizons, achieving F1 scores of 89.5%, 86.4%, and 85.5% at one, two, and three-day forecasts, respectively, and maintaining an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon. These results confirm the model's ability to capture complex spatiotemporal dynamics from sparse satellite data and to provide reliable early warning for CyanoHABs management.

</details>


### [84] [Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics](https://arxiv.org/abs/2512.06612)
*Kazuya Nishimura,Haruka Hirose,Ryoma Bise,Kaito Shiku,Yasuhiro Kojima*

Main category: cs.CV

TL;DR: 本文提出了一种新的损失函数STRank，用于从病理图像估计基因表达时，学习相对而非绝对表达模式，从而更好地适应实验批次效应和噪声干扰。实验验证显示该方法优于传统点对点损失函数。


<details>
  <summary>Details</summary>
Motivation: 传统病理图像到基因表达预测方法依赖点对点损失，但由于测序技术噪声和批次效应，真实基因表达常含有较大随机性和系统误差，导致预测绝对表达值难度大、效果受限。因此，需要一种能更有效识别和利用稳定表达模式的方法。

Method: 作者提出关注基因间相对表达关系（排序信息）而非绝对值，建模该相对关系在不同实验中的一致性，并据此设计了对批次效应和噪声具有鲁棒性的STRank损失函数，通过排序或秩信息来指导模型训练。

Result: 通过合成数据和真实数据集验证，STRank方法在不同批次、不同噪声水平下表现出更强的鲁棒性，取得了比传统点对点损失更好的基因表达估计性能。

Conclusion: 相对表达模式的建模可以显著提高从病理图像估计基因表达的准确性和鲁棒性。STRank损失为此类任务提供了有效新思路，并显示出较大潜在应用价值。

Abstract: Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.

</details>


### [85] [Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach](https://arxiv.org/abs/2512.06613)
*Yueying Ke*

Main category: cs.CV

TL;DR: 本文提出了一种结合分类层级的卷积神经网络用于硅藻多级分类，能显著改善高层级分类准确率及错误局部性，展示了比传统单层分类方法更加鲁棒和生物学解释性强的性能。


<details>
  <summary>Details</summary>
Motivation: 硅藻的分类识别对于水生态系统监测至关重要，但传统方法高度依赖专家，并且近期深度学习方法仅支持单一层级分类，未利用完整的生物分类体系结构。该研究动机是探究将分类层级结构融入神经网络能否提升分类准确性和结果的错误局部性。

Method: 提出一种层级卷积网络，包含级联的五个分类头，实现对纲、目、科、属、种五级联合预测。每个头部在训练与推理时利用高层预测概率以及二元掩码，仅允许有效下级内容的预测。通过包含82个物种、1456张图片的数据集，将该网络与传统的平面分类器进行对比，分析效果。

Result: 层级模型在物种层面与传统平面模型准确率相当（69.4%），在更高层级显著优于平面模型。物种分类失败时，92.5%的错误预测还在正确的属，远高于平面基线的67.2%。平均分类距离下降了38.2%。渐进式训练分析显示，层级掩码带来自顶向下的约束，精细层级梯度自底向上传递优化特征，使纲级准确率提升至99.5%。

Conclusion: 引入层级结构约束的深度学习方法实现了高准确率、错误局部化和更高生物学一致性，为多级生态学分类提供了更稳健且可解释性强的工具。

Abstract: Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality.
  We introduce a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings.
  The hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5 % of misclassified species are correctly predicted at genus level, versus 67.2% for flat baselines. The hierarchical model reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).
  Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.

</details>


### [86] [Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution](https://arxiv.org/abs/2512.06642)
*Achmad Ardani Prasha,Clavino Ourizqi Rachmadi,Muhamad Fauzan Ibnu Syahlan,Naufal Rahfi Anugerah,Nanda Garin Raditya,Putri Amelia,Sabrina Laila Mutiara,Hilman Syachr Ramadhan*

Main category: cs.CV

TL;DR: 本文提出利用masked autoencoder（MAE）预训练策略，通过模拟的强引力透镜图像提升暗物质亚结构识别及超分辨率重建能力，有效提高判别与重建精度。


<details>
  <summary>Details</summary>
Motivation: 强引力透镜现象能揭示星系暗物质亚结构信息，但实际观测图像往往分辨率低、噪声高，很难分析和提取相关物理特征。因此，需要开发新的机器学习方法，从复杂且有限的数据中学习泛化能力强的特征表达，提升对暗物质模型的判别和图像重建效果。

Method: 作者采用Vision Transformer架构，并通过Masked Autoencoder在模拟的强引力透镜图像数据集（DeepLense ML4SCI benchmark）上进行预训练。MAE通过对图像随机遮盖后重建任务促使模型学习高效特征，随后针对两个下游任务进行微调：（1）暗物质模型分类（冷暗物质、类似轴子的暗物质、无亚结构）；（2）低分辨率透镜图像超分辨率重建。作者还系统研究了MAE遮盖比例对结果的影响。

Result: 实验显示，MAE预训练配合合适遮盖比例（90%）可获得比单独训练更优的分类性能（AUC 0.968、准确率88.65% vs. 基线AUC 0.957、准确率82.46%）。在超分辨率任务中（16x16至64x64），MAE预训练模型重建图像的PSNR约33dB，SSIM为0.961，均略优于从零训练。遮盖比例提升有助于分类，但对重建效果略有影响。

Conclusion: MAE方法在物理模拟数据预训练后，可成为多种强引力透镜分析任务的通用高效编码器，为复杂天文场景下暗物质研究和图像恢复提供灵活、可复用的解决方案。

Abstract: Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.

</details>


### [87] [TextMamba: Scene Text Detector with Mamba](https://arxiv.org/abs/2512.06657)
*Qiyan Zhao,Yue Yan,Da-Han Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Mamba模型的场景文本检测方法，通过集成选择机制与注意力层强化长序列信息提取能力，并在多个基准数据集上获得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 传统CNN方法难以提取全局特征，虽然Transformer提升了这一能力，但其原生注意力层在长距离依赖建模时会遗忘重要信息或关注无关特征，存在跨领域局限。Mamba模型在建模长距离依赖方面表现更优，因此有必要探索其在场景文本检测中的应用。

Method: 提出基于Mamba的场景文本检测器，将选择机制与注意力层结合以增强编码器的关键信息提取能力。具体采用Top_k算法显式筛选关键信息，减少冗余干扰，并设计了双尺度前馈网络及嵌入金字塔增强模块，提升高维交互与多尺度特征融合。

Result: 所提方法在CTW1500、TotalText和ICDAR19ArT等基准数据集上分别取得了89.7%、89.2%和78.5%的F-measure，达到了SOTA或具备竞争力的表现。

Conclusion: 通过引入Mamba与选择机制，方法在长序列关键特征提取和冗余信息抑制方面取得突破，为场景文本检测带来了新思路，具备实际应用价值。

Abstract: In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\%, 89.2\%, and 78.5\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.

</details>


### [88] [Personalized Image Descriptions from Attention Sequences](https://arxiv.org/abs/2512.06662)
*Ruoyu Xue,Hieu Le,Jingyi Xu,Sounak Mondal,Abe Leite,Gregory Zelinsky,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: 本文提出了一种结合个体视觉关注模式和语言风格的新方法，显著提升了个性化图像描述任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然人们对同一图像的关注点和描述方式存在很大差异，但现有个性化图像描述方法仅关注语言风格，未利用个人的视觉关注行为。因此，作者希望将个体的观图行为引入，以提升图像描述的个人化和质量。

Method: 作者提出了DEPER（一种描述-感知人格编码器），通过一个辅助注意力预测任务，学习同时捕捉语言风格与视觉行为的受试者表征，并通过轻量化的适配器将该表征与冻结的视觉-语言模型对齐，实现少样本的个性化描述生成，无需重新训练基础模型。

Result: 在涵盖不同视觉任务和描述长度的四个数据集上，DEPER带来了平均24%的性能提升，验证了个体化视觉注意力建模能显著提高描述的质量和与人类的一致性。

Conclusion: 通过建模人的多样化感知（视觉关注方式），不仅能提升多模态系统的功能表现，还能更好地对齐人类偏好，实现更加人性化的人工智能个性化描述。

Abstract: People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.

</details>


### [89] [CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks](https://arxiv.org/abs/2512.06663)
*Yu Qi,Yumeng Zhang,Chenting Gong,Xiao Tan,Weiming Zhang,Wei Zhang,Jingdong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CoT4Det的新方法，通过链式思考（Chain-of-Thought）策略，将感知类视觉任务分拆为分类、计数和定位三步，有效提升了大型视觉语言模型在对象检测等任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型（LVLMs）在许多视觉语言任务上表现优异，但在如目标检测、分割和深度估计等感知任务上远逊于专用模型，尤其是在复杂场景和对小目标的识别方面。作者希望提升LVLMs在这些任务上的性能。

Method: 提出Chain-of-Thought for Detection（CoT4Det），将复杂的感知任务重构为分类、计数和定位三步，分别利用LVLM的推理和理解能力，更好地匹配感知任务的需求。

Result: 借助CoT4Det，在Qwen2.5-VL-7B-Instruct模型上，COCO2017 val测试集mAP从19.0%提升至33.0%；在RefCOCO系列和Flickr30k entities等多个感知任务上均显著优于基线模型。

Conclusion: CoT4Det能在不影响LVLMs通用视觉语言能力的前提下，大幅提升其在感知类视觉任务上的表现，为LVLM感知能力提升提供了简单有效的新路径。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding -- each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.

</details>


### [90] [1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning](https://arxiv.org/abs/2512.06673)
*Shida Gao,Feng Xue,Xiangfeng Wang,Anlong Ming,Teng Long,Yihua Shao,Haozhe Wang,Zhaowen Lin,Wei Wang,Nicu Sebe*

Main category: cs.CV

TL;DR: 本文提出了DEViL模型，通过结合视频大语言模型（Video LLM）和开放词汇物体检测器（OVD），提升了视频时空定位及推理的准确性，尤其在空间解码和长视频序列的鲁棒性方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型（MLLMs）在处理视频时空定位任务时，大多采用自回归的方式生成空间边界框。这容易导致输出序列过长，空间误差在序列中不断累计，最终影响定位准确性。因此，亟需新的方法提升空间定位的稳定性和理解能力。

Method: 作者提出Detector-Empowered Video LLM（DEViL），将视频LLM与开放词汇检测器通过“参考语义令牌”（RST）连接。RST不仅作为控制信号，还能取代检测器的文本嵌入，实现端到端地学习空间定位和参照理解。进一步，作者提出了tube-mined temporal regularization（TTReg）机制，帮助检测器生成时序上一致的查询，从而保持对象的时域关联。

Result: DEViL在多个精细化的视频理解任务，如时空可视化引导（STVG）和带定位的视觉问答（GroundedVQA）上取得了优异的性能，实验表明其在空间和时序一致性上均有显著提升。

Conclusion: 本文方法有效缓解了自回归空间解码带来的误差累积问题，提升了视频多模态理解任务的表现，为后续多模态视频分析提供了新范式。

Abstract: Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.

</details>


### [91] [RunawayEvil: Jailbreaking the Image-to-Video Generative Models](https://arxiv.org/abs/2512.06674)
*Songping Wang,Rufan Qian,Yueming Lyu,Qinglong Liu,Linzhuang Zou,Jie Qin,Songhua Liu,Caifeng Shan*

Main category: cs.CV

TL;DR: 本文提出了RunawayEvil，这是首个针对图像到视频（I2V）生成模型的自进化多模态越狱（jailbreak）攻击框架，能极大提高对I2V系统安全性评估的能力。实验表明，该方法对主流商用I2V模型攻击成功率大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前I2V模型广泛应用且带来全新创造力，但其安全性和对越狱攻击的防御能力研究极其有限。作者旨在揭示和分析这些多模态生成系统在安全层面的脆弱性。

Method: RunawayEvil采用“策略-战术-行动”三阶段架构：通过策略感知单元自适应生成攻击策略，利用多模态的文本与图像操作指导生成越狱攻防战术，并以行动单元自动实施、评估攻击效果，具备自主进化策略能力和强化学习机制。

Result: 在Open-Sora 2.0、CogVideoX等主流商用I2V模型上的大规模实验显示，RunawayEvil攻击成功率比当前最佳方法高出58.5%至79%，充分显示其高效和强大。

Conclusion: RunawayEvil为I2V模型的脆弱性分析提供了重要工具，有助于推动更安全、鲁棒的视频生成系统的发展，并为多模态生成模型的安全防护研究奠定了基础。

Abstract: Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a "Strategy-Tactic-Action" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.

</details>


### [92] [EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy](https://arxiv.org/abs/2512.06684)
*Yumeng He,Zanwei Zhou,Yekun Zheng,Chen Liang,Yunbo Wang,Xiaokang Yang*

Main category: cs.CV

TL;DR: EMGauss是一种针对体积电子显微镜（vEM）采集的各向异性3D数据进行重建的新方法，通过高斯splatting将2D切片重建为3D场景，结合教师-学生自举机制提升稀疏数据情况下的重建质量，无需大规模预训练，优于现有扩散和GAN方法，并可推广至其他成像领域。


<details>
  <summary>Details</summary>
Motivation: 现有vEM技术在获取3D超微结构时存在轴向分辨率受限的问题，导致重建体存在各向异性。现有深度学习恢复方法依赖侧向分布假设，对于形态各向异性的结构处理效果有限。因此，亟需开发一种既能突破分辨率限制，又能推广到不同结构和成像场景的通用切片到3D重建框架。

Method: EMGauss方法将2D切片到3D重建的问题重新表述为基于高斯splatting的3D动态场景渲染。在该框架下，序列切片的变化被视为2D高斯点云的时序演化。此外，作者采用教师-学生自举机制，将未观测切片的高置信度预测作为伪监督信号，增强数据稀疏情况下的模型表现。

Result: EMGauss在vEM数据上的重建质量明显优于扩散模型和GAN重建方法，实现了更高质量的插值和连续切片合成，并在数据稀缺环境下同样表现优异。无需大规模预训练，且具备良好的泛化能力。

Conclusion: EMGauss不仅改善了vEM体数据的3D重建精度和插值质量，还可作为跨领域的通用切片到三维重建解决方案，具有广阔的应用前景。

Abstract: Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.

</details>


### [93] [Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation](https://arxiv.org/abs/2512.06689)
*Jisoo Park,Seonghak Lee,Guisik Kim,Taewoo Kim,Junseok Kwon*

Main category: cs.CV

TL;DR: 本文提出了一种名为UniVoiceLite的轻量级、无监督音视频融合模型，能够同时实现语音增强和语音分离。该方法依赖于唇动和面部身份线索，无需配对的数据，效果优异且泛化性强。


<details>
  <summary>Details</summary>
Motivation: 实际语音处理中，常常同时存在背景噪声和说话人重叠，传统的语音增强和分离方法往往各自独立，难以高效统一处理。此外，现有集成方法参数复杂，依赖有监督训练，限制了其可扩展性和泛化能力。

Method: 作者提出UniVoiceLite，一个统一SE和SS任务的音视频轻量级无监督框架。该方法利用唇动与面部身份信息引导语音提取，并通过Wasserstein距离正则化稳定潜在空间，无需配对的噪声-纯净语音数据。

Result: 实验结果表明，UniVoiceLite在噪声环境和多说话者场景下均表现优异，兼具高效性与泛化能力。

Conclusion: UniVoiceLite无监督、轻量化的模型结构实现了语音增强和分离任务的统一，为实际复杂语音处理提供了新的有效方案。

Abstract: Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.

</details>


### [94] [Graph Convolutional Long Short-Term Memory Attention Network for Post-Stroke Compensatory Movement Detection Based on Skeleton Data](https://arxiv.org/abs/2512.06736)
*Jiaxing Fan,Jiaojiao Liu,Wenkong Wang,Yang Zhang,Xin Ma,Jichen Zhang*

Main category: cs.CV

TL;DR: 该研究提出基于骨骼数据的GCN-LSTM-ATT网络检测脑卒中后的代偿性动作，准确率显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 大多数脑卒中患者存在上肢运动障碍，康复训练中普遍伴随代偿性动作，这会影响长期恢复。准确检测代偿性动作对于优化康复至关重要。

Method: 本研究招募16名脑卒中患者，通过Kinect相机采集其进行特定康复动作的骨骼点数据，采用GCN-LSTM-ATT模型进行代偿性动作检测，并与SVM、KNN和RF等传统算法对比，同时做消融实验分析模型各部分贡献。

Result: GCN-LSTM-ATT模型的检测准确率达0.8580，显著高于SVM、KNN和RF。消融实验显示模型每个组成部分都能显著提升性能。

Conclusion: GCN-LSTM-ATT模型为脑卒中后代偿性动作检测提供了更高精度和效率的工具，有望助力优化脑卒中康复训练策略。

Abstract: Most stroke patients experience upper limb motor dysfunction. Compensatory movements are prevalent during rehabilitation training, which is detrimental to patients' long-term recovery. Therefore, detecting compensatory movements is of great significance. In this study, a Graph Convolutional Long Short-Term Memory Attention Network (GCN-LSTM-ATT) based on skeleton data is proposed for the detection of compensatory movements after stroke. Sixteen stroke patients were selected in the research. The skeleton data of the patients performing specific rehabilitation movements were collected using the Kinect depth camera. After data processing, detection models were constructed respectively using the GCN-LSTM-ATT model, the Support Vector Machine(SVM), the K-Nearest Neighbor algorithm(KNN), and the Random Forest(RF). The results show that the detection accuracy of the GCN-LSTM-ATT model reaches 0.8580, which is significantly higher than that of traditional machine learning algorithms. Ablation experiments indicate that each component of the model contributes significantly to the performance improvement. These findings provide a more precise and powerful tool for the detection of compensatory movements after stroke, and are expected to facilitate the optimization of rehabilitation training strategies for stroke patients.

</details>


### [95] [FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation](https://arxiv.org/abs/2512.06738)
*M Yashwanth,Sampath Koti,Arunabh Singh,Shyam Marjit,Anirban Chakraborty*

Main category: cs.CV

TL;DR: 本文提出了FedSCAl，一种在无源数据和强域间异质条件下，基于联邦学习实现域自适应的框架，通过服务器-客户端对齐机制有效提升伪标签精度和分类性能。


<details>
  <summary>Details</summary>
Motivation: 在实际联邦学习中，客户端数据来自不同分布且无标注，且无法访问原始训练源域数据，使得传统无源域自适应方法在数据极度异质时易出现客户端漂移，导致伪标签不可靠，影响最终表现。

Method: FedSCAl框架引入服务器-客户端预测对齐机制（SCAl），通过正则化客户端更新，使其预测结果与服务器预训练模型的预测一致，从而缓解由域差异导致的“客户端漂移”，提升伪标签准确性。

Result: 实验在多个视觉基准数据集上表明，FedSCAl方法能持续优于当前最先进的联邦学习方法，特别是在FFreeDA无源联邦域自适应分类任务中效果突出。

Conclusion: FedSCAl通过有效的服务器与客户端预测对齐，解决了极端数据异质下客户端漂移难题，为FFreeDA问题提供了强有力的解决方案，并在多个任务中取得领先性能。

Abstract: We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.

</details>


### [96] [Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2512.06746)
*Ruoxin Chen,Jiahui Gao,Kaiqing Lin,Keyue Zhang,Yandan Zhao,Isabel Guan,Taiping Yao,Shouhong Ding*

Main category: cs.CV

TL;DR: 该论文提出了一种新的AIGI（AI生成图像）检测方法AlignGemini，通过分别针对语义一致性与像素伪影的模型分支，提高了检测的泛化能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM（视觉语言模型）在AIGI检测中的表现有限，主要由于其对细粒度像素伪影不敏感，同时像素伪影检测器缺乏语义判别力，造成检测准确率和泛化性不足。因此，需要一种能同时针对这两类特征的新方法。

Method: 作者将AIGI检测正式描述为两个互补任务：语义一致性检测与像素伪影检测。对应地，提出了AlignGemini，由两个分支组成：一是仅用高层语义监督微调的VLM，二是仅用低层像素伪影监督训练的专家模型。各分支分别在定制子集上训练，各自发挥所长。

Result: AlignGemini在五个真实场景基准测试中，平均准确率较现有方法提升9.5个百分点，证明了任务-模型匹配（Task-Model Alignment）原则的有效性。

Conclusion: 将AIGI检测任务分解为语义与像素两个互补子任务，不但弥补了各自模型盲区，还极大提升了泛化能力，这为后续检测类任务提供了一条清晰有效的研究路径。

Abstract: Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.

</details>


### [97] [UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement](https://arxiv.org/abs/2512.06750)
*Weiqi Li,Xuanyu Zhang,Bin Chen,Jingfen Xie,Yan Wang,Kexin Zhang,Junlin Li,Li Zhang,Jian Zhang,Shijie Zhao*

Main category: cs.CV

TL;DR: 本文提出了首个将图像质量评价（IQA）、图像恢复与增强统一于一个视觉-语言模型（UARE）中，实现了多任务联合训练，并验证了IQA对图像恢复的提升作用。


<details>
  <summary>Details</summary>
Motivation: 尽管图像质量评价和图像恢复在理论上密切相关，但以往研究多将两者分开处理。受新一代多模态理解—生成模型的启发，作者希望探索强理解能力对生成任务的提升，尤其关注如何借助IQA信号指导恢复过程。

Method: 方法包括两个阶段：（1）分阶段由简单到复杂地训练模型应对多种失真类型；（2）利用交织的文本-图像数据统一微调模型，在多任务联合训练中对齐IQA和恢复的目标，并通过多任务联合学习，让IQA提升图像恢复与增强表现。

Result: 在IQA、恢复与增强等多项任务上的广泛实验表明，UARE模型效能优异，展示了跨任务的有效提升。

Conclusion: UARE模型首次将图像质量评价和恢复任务统一于一体，验证了两者协同的潜力，为低级视觉领域相关任务的联合研究提供了新途径。

Abstract: Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.

</details>


### [98] [VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors](https://arxiv.org/abs/2512.06759)
*Wenbo Lyu,Yingjun Du,Jinglin Zhao,Xianton Zhen,Ling Shao*

Main category: cs.CV

TL;DR: 本文提出了VisChainBench，这是一个用于评估大型视觉-语言模型（LVLMs）在多图像、多轮推理场景下表现的大规模基准，填补了现有基准集中在静态、横向对比而忽略渐进式、基于上下文的视觉推理的空白。


<details>
  <summary>Details</summary>
Motivation: 当前LVLMs评测普遍依赖语言提示，主要考查静态或单步能力，缺乏对模型在多步、上下文相关的视觉推理能力进行深入评估，因此有必要建立新的基准测试LVLMs的真实推理能力。

Method: 作者设计了VisChainBench基准，包含1457个任务、超2万张图片，覆盖三个多样化领域。每个任务为多步、前后关联的视觉推理，并使用多智能体生成流程制造任务与图片，降低语言偏置，提高视觉多样性。

Result: VisChainBench作为首个规模化多步视觉推理评测体系，结构化地设计任务并控制语言信息泄露，能够有效鉴别LVLMs在真实视觉推理问题中的优劣，为后续相关模型迭代和算法发展奠定基础。

Conclusion: VisChainBench填补了多图像多轮推理评测的空白，推动LVLMs朝向更真实、复杂的视觉语言理解任务发展，对未来视觉语言模型评测体系和应用具有积极意义。

Abstract: Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench

</details>


### [99] [JOCA: Task-Driven Joint Optimisation of Camera Hardware and Adaptive Camera Control Algorithms](https://arxiv.org/abs/2512.06763)
*Chengyang Yan,Mitch Bryson,Donald G. Dansereau*

Main category: cs.CV

TL;DR: 本论文提出了一种联合优化摄像头硬件和自适应控制算法的方法，以提升下游感知任务的表现，并验证了该方法在困难环境下的优越性。


<details>
  <summary>Details</summary>
Motivation: 已有的摄像头与感知任务协同设计方法多仅优化制造前就已确定的静态参数，忽视了如曝光等需运行时动态调控的重要参数。如何联合优化硬件配置和自适应控制策略，从而服务感知任务，是当前的主要挑战。

Method: 提出了一个统一优化框架，将基于梯度的方法和无导数优化方法结合，既能优化连续又能优化离散参数，支持非可微分的成像过程，适用于基于神经网络的自适应控制算法。特别地，针对如运动模糊等非可微分效应，提出了DF-Grad混合优化策略，将无导数优化器的信号与无监督任务驱动学习结合，训练自适应控制网络。

Result: 实验表明，所提方法在低光、快速运动等复杂环境下表现显著优于对静态和动态参数单独优化的基线方法。

Conclusion: 硬件参数与自适应控制算法联合优化能有效提升整体感知性能，为任务驱动下的摄像头系统设计提供了统一的解决思路。

Abstract: The quality of captured images strongly influences the performance of downstream perception tasks. Recent works on co-designing camera systems with perception tasks have shown improved task performance. However, most prior approaches focus on optimising fixed camera parameters set at manufacturing, while many parameters, such as exposure settings, require adaptive control at runtime. This paper introduces a method that jointly optimises camera hardware and adaptive camera control algorithms with downstream vision tasks. We present a unified optimisation framework that integrates gradient-based and derivative-free methods, enabling support for both continuous and discrete parameters, non-differentiable image formation processes, and neural network-based adaptive control algorithms. To address non-differentiable effects such as motion blur, we propose DF-Grad, a hybrid optimisation strategy that trains adaptive control networks using signals from a derivative-free optimiser alongside unsupervised task-driven learning. Experiments show that our method outperforms baselines that optimise static and dynamic parameters separately, particularly under challenging conditions such as low light and fast motion. These results demonstrate that jointly optimising hardware parameters and adaptive control algorithms improves perception performance and provides a unified approach to task-driven camera system design.

</details>


### [100] [Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding](https://arxiv.org/abs/2512.06769)
*Hang Yin,Xiaomin He,PeiWen Yuan,Yiwei Li,Jiayi Shi,Wenxiao Fan,Shaoxiong Feng,Kan Li*

Main category: cs.CV

TL;DR: 本文提出了一种简单、无标注、可即插即用的方法SiTe，通过拼接图片和生成含空间关系的文本，为视觉-语言模型注入结构化空间监督，有效提升了模型在空间理解任务上的表现，同时保持甚至提升了通用视觉-语言任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型存在空间幻觉问题，即对于图片中物体相对位置的描述常出现错误。其主要原因在于图像与文本之间的特性不对称，导致模型空间理解能力不足。

Method: 作者提出了Stitch and Tell（SiTe）方法：无需人工标注，也不需复杂模型，只需沿空间轴拼接图片并根据其布局生成对应的空间语义文本数据（如描述、问答对），从而在训练数据中显式注入结构化的空间信息。该方法可即插即用地应用于多种模型和数据集。

Result: 在LLaVA-v1.5-7B，LLaVA-Qwen2-1.5B和HALVA-7B等三种模型，两个训练集，以及包括MME_Position、Spatial-MM、COCO-QA、MMBench等八个基准测试上进行了实验。结果表明，SiTe在空间理解任务（如MME_Position提升5.50%，Spatial-MM提升4.19%）及通用VLP任务（如COCO-QA、MMBench分别提升1.02%、4.76%）上均有显著提升。

Conclusion: 在数据中显式注入空间结构信息，能够有效缓解视觉-语言模型的空间幻觉问题，提升模型的空间理解能力，并且不会损害模型的整体多模态表现，反而可能进一步促进其提升。

Abstract: Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\text{MME}_{\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.

</details>


### [101] [RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06774)
*Longjie Zhao,Ziming Hong,Zhenyang Ren,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

TL;DR: 本论文提出了一种名为RDSplat的3D高斯散点（3DGS）水印方法，在抵抗扩散式编辑（diffusion-based editing）时表现出更高的鲁棒性。方法通过将水印嵌入扩散编辑下更易保留的低频高斯分量，并利用对抗训练增强鲁棒性。实验显示该方法兼具水印隐蔽性和鲁棒性，达到当前最优水平。


<details>
  <summary>Details</summary>
Motivation: 随着3DGS在数字资产和应用中的普及，如何对这类内容进行版权保护显得尤为重要。然而，现有3DGS水印方法容易被扩散式编辑抹除，这对版权追溯带来挑战。因此，亟需开发对扩散式编辑具备内在抵抗力的3DGS水印技术。

Method: RDSplat将水印嵌入扩散编辑下难以抹除的低频高斯分量，具体采用多域框架，在3DGS空间内通过协同协方差正则化和2D滤波实现水印嵌入。同时，利用扩散编辑的低通滤波特性，引入高斯模糊作为扩散代理进行对抗训练，进一步提高了水印鲁棒性。

Result: 在三个基准数据集上的定量与定性实验均显示，RDSplat方案在抵抗扩散式编辑的鲁棒性方面表现优越，同时保证了水印不可见性，实现了业界最佳性能。

Conclusion: RDSplat为3DGS领域的数字水印提供了一种本质上更抗扩散式编辑的解决方案，兼具水印鲁棒性和隐蔽性，有望显著提升3D内容的版权保护能力。

Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.

</details>


### [102] [Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos](https://arxiv.org/abs/2512.06783)
*Tobias Leuthold,Michele Xiloyannis,Yves Zimmermann*

Main category: cs.CV

TL;DR: 提出了一种结合BlazePose 3D与2D估计结果的实时后处理算法，引入骨骼长度和生物力学约束，显著提升了单目视频的三维姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 尽管当前单目姿态估计算法（如BlazePose）在实时性和可用性上表现出色，但缺乏人体解剖约束，导致物理合理性不足，影响训练、康复等自动化教练应用的可靠性。

Method: 融合BlazePose 3D与2D姿态估计结果，通过加权优化，结合骨骼长度和生物力学惩罚项，并利用Kalman滤波动态适应个体骨骼长度，实现个性化解剖参数优化。

Result: 在Physio2.2M数据集上，所提方法的3D MPJPE比原BlazePose 3D降低10.2%，身体各部分夹角错误率下降16.6%。

Conclusion: 本方法在保证实时性的同时，显著提升了单目视频三维姿态估计的解剖一致性与鲁棒性，能够应用于自动化康复、健康和体育指导等消费级设备，且能保障用户数据匿名性。

Abstract: Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.

</details>


### [103] [Generalized Geometry Encoding Volume for Real-time Stereo Matching](https://arxiv.org/abs/2512.06793)
*Jiaxin Liu,Gangwei Xu,Xianqi Wang,Chengliang Zhang,Xin Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的实时立体匹配网络GGEV，可以在保持实时性的同时，实现出色的泛化能力，并在多个公开数据集上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有实时立体匹配方法多侧重提升同域性能，却往往忽视了实际应用中更重要的泛化能力。最近的方法通过单目基础模型提升泛化，但推理延迟较大，因此需要兼顾泛化性能与实时性的立体匹配网络。

Method: 方法包含两个主要步骤：（1）从输入图像中提取包含结构先验、领域无关的深度感知特征，作为代价聚合的指导。（2）提出深度感知动态代价聚合模块（DDCA），将前述先验自适应地融合进每个视差假设之中。两者结合，构建了一种泛化能力强的几何编码体积。

Result: 实验证明，GGEV在KITT 2012、KITTI 2015和ETH3D基准测试上性能优异，零次泛化能力超过所有已知实时方法。

Conclusion: GGEV在提升泛化性的同时兼顾了实时性，为立体视觉任务在实际复杂场景中的推广应用带来了新的可能。

Abstract: Real-time stereo matching methods primarily focus on enhancing in-domain performance but often overlook the critical importance of generalization in real-world applications. In contrast, recent stereo foundation models leverage monocular foundation models (MFMs) to improve generalization, but typically suffer from substantial inference latency. To address this trade-off, we propose Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network that achieves strong generalization. We first extract depth-aware features that encode domain-invariant structural priors as guidance for cost aggregation. Subsequently, we introduce a Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis, effectively enhancing fragile matching relationships in unseen scenes. Both steps are lightweight and complementary, leading to the construction of a generalized geometry encoding volume with strong generalization capability. Experimental results demonstrate that our GGEV surpasses all existing real-time methods in zero-shot generalization capability, and achieves state-of-the-art performance on the KITTI 2012, KITTI 2015, and ETH3D benchmarks.

</details>


### [104] [VDOT: Efficient Unified Video Creation via Optimal Transport Distillation](https://arxiv.org/abs/2512.06802)
*Yutong Wang,Haiyu Zhang,Tianfan Xue,Yu Qiao,Yaohui Wang,Chang Xu,Xinyuan Chen*

Main category: cs.CV

TL;DR: 本文提出了一个高效统一的视频生成模型VDOT，结合分布匹配蒸馏和最优传输技术，提高了视频生成质量与效率，并通过自动化数据处理和新基准集取得了比现有方法更优或相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型受限于只适应少数特定条件或推理速度慢，难以应用于实际场景。因此，作者旨在设计一个效率高、统一且适用于多种生成条件的视频生成方法。

Method: 1）提出了一种基于分布匹配蒸馏（DMD）的训练范式；2）引入创新的计算型最优传输（OT）技术，优化真实与生成分数分布之间的距离，从而避免KL方式下的梯度塌陷等问题，提高蒸馏效率与稳定性；3）整合判别器提升生成视频质量；4）开发自动化视频注释及过滤流程，便于多任务训练；5）提出统一测试基准UVCBench，标准化评测流程。

Result: 实验显示，作者提出的4步VDOT方法在视频生成质量和效率上，超越或匹配了采用100步去噪等主流基线方法。

Conclusion: VDOT在效率、适用范围和生成质量上都取得了优异表现，为实际视频生成应用提供了更加可行的解决方案。

Abstract: The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.

</details>


### [105] [RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2512.06811)
*Xiang Lin,Weixin Li,Shu Guo,Lihong Wang,Di Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新型多模态适配器（RMAdapter），在参数高效微调和保持模型泛化能力方面取得突破，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前预训练视觉-语言模型（如CLIP）在少样本微调时，任务适应性与泛化能力难以平衡。现有方法多关注基于Prompt的适配，适配器方法探索较少且表现存在较大差距。亟需新的机制兼顾任务定制和知识泛化。

Method: 提出RMAdapter，包含两条分支：一是适应分支，通过高效微调注入任务特定知识；二是重构分支，通过将潜在特征重建回原始特征空间以保持通用知识。引入层内重构损失与投影模块共享，保持计算低开销，并设计一致性约束调节可判别性与泛化性。

Result: 在新类别泛化、新目标数据集泛化和领域泛化三种代表性任务上，RMAdapter在所有评测指标上均超过现有最优方法，无需数据增强或复杂Prompt设计。

Conclusion: RMAdapter实现了任务特定知识与通用知识的动态平衡，在多种转移学习场景下提供了更优微调策略，为少样本多模态适配研究带来新进展。

Abstract: Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.

</details>


### [106] [MeshSplatting: Differentiable Rendering with Opaque Meshes](https://arxiv.org/abs/2512.06818)
*Jan Held,Sanghyun Son,Renaud Vandeghen,Daniel Rebain,Matheus Gadelha,Yi Zhou,Anthony Cioppa,Ming C. Lin,Marc Van Droogenbroeck,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: 本文提出了MeshSplatting方法，将体素/点为主的3D Gaussian Splatting转化为可直接供游戏与AR/VR引擎使用的网格（mesh）表示，同时优化几何与外观，实现高质量、实时的3D场景渲染与交互，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的3D Gaussian Splatting等基于点的方式虽然渲染效果好且实时，但无法直接适配广泛使用的网格渲染管线，这限制了其在AR/VR和游戏等场景的实际应用。

Method: 提出MeshSplatting，通过可微分渲染同时优化几何（使用受限Delaunay三角剖分保证连通性和网面一致性）和外观，得到平滑、高质量的三维网格，实现高效可交互的实时渲染。

Result: 在Mip-NeRF360数据集上，相较于当前最优的基于网格的新视角合成方法MiLo，PSNR提升了0.69 dB，训练速度提升2倍，内存消耗减少一半。

Conclusion: MeshSplatting有效桥接了神经渲染和可交互3D图形，为实时场景交互提供了一种兼容、高效、高质量的新方法，促进了3D视觉技术与工业应用的结合。

Abstract: Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.

</details>


### [107] [SparseCoop: Cooperative Perception with Kinematic-Grounded Queries](https://arxiv.org/abs/2512.06838)
*Jiahao Wang,Zhongwei Jiang,Wenchao Sun,Jiaru Zhong,Haibao Yu,Yuner Zhang,Chenyang Lu,Chuang Zhang,Lei He,Shaobing Xu,Jianqiang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种全稀疏的协同感知框架SparseCoop，可高效进行3D目标检测与跟踪，显著提升通信和推理效率，并在主流数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 单车感知受限于视野和遮挡等因素，现有基于稠密BEV（鸟瞰图）特征的协同感知方法受限于通信成本高和视角同步困难，稀疏查询方法虽然通信高效但表现不稳、融合效果和几何建模不足，因此亟需一种高效、稳定、几何表达能力强的合作感知方案。

Method: SparseCoop框架采用三大创新：1）基于动力学状态（含3D位置和速度）的实例化查询，实现精确时空对齐；2）粗到细的特征聚合模块，增强融合鲁棒性；3）合作实例去噪训练任务，提高训练稳定性与效率。同时完全摒弃中间BEV表示，采用稀疏点到点的表达与交互，极大降低了通信负担。

Result: 在V2X-Seq和Griffin数据集上的实验证明，SparseCoop在3D检测与跟踪任务取得了当前最优性能，同时具有更高的计算效率、更低的通信开销，并对延迟有良好鲁棒性。

Conclusion: SparseCoop有效解决了现有协同感知方法的通信、融合和稳定性难题，是高效协同3D感知的有力方案。

Abstract: Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.

</details>


### [108] [CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles](https://arxiv.org/abs/2512.06840)
*Satoshi Hashimoto,Tatsuya Konishi,Tomoya Kaichi,Kazunori Matsumoto,Mori Kurokawa*

Main category: cs.CV

TL;DR: 本文提出了一种结合持续学习（CL）和弱监督视频异常检测（WVAD）的全新方法CADE，有效解决了不同场景下的遗忘和异常漏检，明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督视频异常检测方法多针对静态数据集，未能考虑数据场景的变化，导致新旧场景切换时模型遗忘并且漏检异常。针对实际应用中持续变化和多场景的需求，需要结合持续学习的思想来减少遗忘，提高检测性能。

Method: 提出了Continual Anomaly Detection with Ensembles（CADE），结合了持续学习和弱监督异常检测。CADE通过Dual-Generator（DG）处理标签不确定和数据不平衡；并提出Multi-Discriminator（MD）集成防止遗忘问题，通过多个判别器捕捉模型遗漏的异常类型。

Result: 在上海科技与Charlotte等多场景数据集上，CADE的性能显著超过了现有的主流视频异常检测方法。

Conclusion: CADE首次有效结合了持续学习与弱监督视频异常检测的思路，能处理多场景下的遗忘、漏检等实际问题，在实际应用具有很好的前景。

Abstract: Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the "incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.

</details>


### [109] [Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2512.06845)
*Satoshi Hashimoto,Hitoshi Nishimura,Yanan Wang,Mori Kurokawa*

Main category: cs.CV

TL;DR: 提出了一种无需真实异常视频即可进行异常检测的新方法PA-VAD，利用合成的伪异常视频训练模型，并在标准数据集上取得了领先结果。


<details>
  <summary>Details</summary>
Motivation: 实际部署视频异常检测受到真实异常视频稀缺和采集成本高昂的限制。为减少对真实异常视频的依赖，作者希望通过合成方法实现高效泛化的异常检测。

Method: 1. 用CLIP从正常图片中筛选与类别相关的初始图片。2. 结合视觉-语言模型优化文本提示，提升合成视频的真实性和场景一致性。3. 通过视频扩散模型生成伪异常视频，并与真实正常视频配对训练检测器。4. 采用领域对齐和内存感知的正则化模块，缓解伪异常过度时空幅度问题。

Result: 在两个主流异常检测数据集（ShanghaiTech和UCF-Crime）上，方法获得了显著的准确率（分别为98.2%和82.5%），超过了当前最强的基于真实异常视频的方法。

Conclusion: 不依赖真实异常视频即可获得高准确率的视频异常检测，有助于实际场景的大规模部署。

Abstract: Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.

</details>


### [110] [Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT](https://arxiv.org/abs/2512.06849)
*Matan Atad,Alexander W. Marka,Lisa Steinhelfer,Anna Curto-Vilalta,Yannik Leonhardt,Sarah C. Foreman,Anna-Sophia Walburga Dietrich,Robert Graf,Alexandra S. Gersing,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke,Hendrik Möller*

Main category: cs.CV

TL;DR: 本论文提出了一种仅利用椎体级健康/恶性标签（无需病灶掩码）的弱监督方法，实现了CT中椎体转移性肿瘤的有效分割。


<details>
  <summary>Details</summary>
Motivation: 椎体转移瘤的精准分割对临床意义重大，但因像素级标注稀缺且破坏性和增生性病灶难与良性退行性改变区分，现有方法难以大规模应用。

Method: 该方法仅以椎体级的健康/恶性标签进行训练，结合了一个Diffusion Autoencoder（DAE）进行分类引导的健康图像编辑，再通过像素级差分图生成候选病灶区。为确定哪些区域真正反映恶性特征，提出了Hide-and-Seek Attribution方法，对各候选区分别显隐，并利用DAE重投影及潜空间分类进行恶性贡献量化，高分区域最终形成分割结果。

Result: 在未见过的放射科医生注释数据上，该方法在无掩码监督的情况下表现优异（溶骨/成骨型F1分别为0.91/0.85，Dice分别为0.87/0.78），超越了基线（F1分别为0.79/0.67，Dice为0.74/0.55）。

Conclusion: 论文表明，仅凭椎体级标签，结合生成式图像编辑与选择性遮挡，可获得可靠的病灶分割，验证了该弱监督框架在CT分割任务中的有效性。

Abstract: Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.

</details>


### [111] [Omni-Referring Image Segmentation](https://arxiv.org/abs/2512.06862)
*Qiancheng Zheng,Yunhang Shen,Gen Luo,Baiyang Song,Xing Sun,Xiaoshuai Sun,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: 作者提出了一种新任务“全方位引用图像分割”(OmniRIS)，支持利用文本指令和带注释的参考图像对图像进行多样化、高度泛化的分割。构建了大规模数据集OmniRef，并提出基线模型OmniSegNet用于验证任务实用性。


<details>
  <summary>Details</summary>
Motivation: 现有分割任务大多只支持单一模态（文本或图像）条件, 难以满足实际中多样化分割需求，尤其是在复杂、泛化性要求高的场景下。因此，有必要提出支持多模态引用的分割方法，以提升分割模型的适应性与通用性。

Method: 构建OmniRIS新任务，允许同时输入文本指令和不同标注形式（掩码、框、涂鸦）的参考图像作为条件。搭建大规模OmniRef数据集，设计综合评测机制，并提出OmniSegNet模型以支持复杂的omni-prompt编码与分割操作。

Result: 实验表明，OmniSegNet能够有效理解并执行多模态指令，支持不同应用场景下的灵活分割需求，且在高度泛化分割能力上优于传统方法。

Conclusion: OmniRIS显著提升了分割任务的通用性与实用性，提供了一种既能细致属性引用、又能识别不常见目标的统一框架，对推动高泛化分割任务具有重要意义。

Abstract: In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.

</details>


### [112] [Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training](https://arxiv.org/abs/2512.06864)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: 提出了一种新的无监督视频实例分割（VIS）框架AutoQ-VIS，通过自适应质量引导的自训练显著提升了从合成域到真实域的泛化能力，实现了在无需人工标注的前提下的SOTA表现。


<details>
  <summary>Details</summary>
Motivation: VIS任务难点在于需要像素级分割与时序一致性标注，且人工注释复杂；现有无监督方法虽能缓解部分问题，但因合成数据与真实数据分布差异较大（domain gap），模型泛化仍有限。

Method: AutoQ-VIS提出构建伪标签生成与自动质量评估闭环，自适应筛选高质量伪标签，支持模型向真实视频逐步迁移，实现在合成到真实数据的高效自监督训练。

Result: 在YouTubeVIS-2019验证集上，AutoQ-VIS达到52.6 AP50，超越同类无监督方法VideoCutLER 4.4个百分点，全流程无需人工标注。

Conclusion: 面向VIS领域，质量感知的自训练策略能够有效缓解domain gap，无监督方法能够取得与有监督甚至超越的性能，为VIS研究和实际应用带来新的解决思路。

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 $\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.

</details>


### [113] [Spatial Retrieval Augmented Autonomous Driving](https://arxiv.org/abs/2512.06865)
*Xiaosong Jia,Chenhe Zhang,Yule Jiang,Songbur Wong,Zhiyuan Zhang,Chen Chen,Shaofeng Zhang,Xuanhe Zhou,Xue Yang,Junchi Yan,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出了一种新的自动驾驶感知范式：在常规传感器基础上引入离线地理图片以增强模型回忆能力，并证明了该方式可提升部分任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶依赖车载传感器，对感知范围、遮挡和极端天气等情况适应性差。而人类司机能基于记忆回忆道路结构。为赋予模型类似能力，研究引入可离线获取的地理图片。

Method: 利用Google Maps等渠道获取地理图片，并与nuScenes数据集中的车辆轨迹对齐，将该图片作为额外输入。同时在五大核心自动驾驶任务上建立基线：目标检测、在线建图、占用预测、端到端规划和生成式世界建模，系统评估其效果。

Result: 扩展后的新模态数据在部分自动驾驶任务上提升了模型性能。

Conclusion: 引入离线检索的地理图片能够作为自动驾驶系统的有效辅助。论文开放数据和代码，为后续研究该新感知范式提供基础。

Abstract: Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.
  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.

</details>


### [114] [Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective](https://arxiv.org/abs/2512.06870)
*Wangkai Li,Rui Sun,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的伪标签学习方法ECOCSeg，用于提升语义分割中的标签质量，主要采用纠错输出码来增强模型对错误标签的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统伪标签学习（如UDA和SSL场景）易产生错误伪标签，且这些错误会因one-hot编码方式在训练中被放大，影响模型性能。解决伪标签噪声与提升泛化能力成为亟需。

Method: 提出ECOCSeg，利用纠错输出码（ECOC）将类别编码为更细粒度、多比特的特征向量，引入基于ECOC的分类器来解耦类间属性，并开发了比特级的标签去噪机制来提升伪标签质量。整体方法关注于标签的精细编码与修正。

Result: ECOCSeg易于集成至现有方法，在多种UDA和SSL基准数据集及不同分割网络架构下都显著提升了性能。

Conclusion: ECOCSeg能有效减少伪标签噪声，提升语义分割模型的稳健性与泛化性，为处理标签稀缺场景下的分割问题提供了新思路。

Abstract: Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.

</details>


### [115] [SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification](https://arxiv.org/abs/2512.06877)
*Mohammed Q. Alkhatib,Ali Jamali,Swalpa Kumar Roy*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级遥感场景分类模型，在AID和EuroSAT数据集上取得了较强的准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 遥感场景分类由于空间分辨率、视角、朝向以及背景条件变化大，现有基于CNN与ViT的方法泛化能力有限，因此需要新的模型提升泛化能力，并兼顾效率。

Method: 本文采用了一种基于卷积混合器（convolutional mixer）的轻量级架构，结合多尺度的深度可分离卷积进行空间混合，以及点卷积进行通道混合，实现对局部与全局信息的高效提取，同时参数量和计算量低。

Result: 在AID和EuroSAT数据集上，模型取得了AID整体准确率74.7%，均值准确率74.57%，Kappa值73.79，EuroSAT整体准确率93.90%，均值准确率93.93%，Kappa值93.22，表现优于许多CNN和Transformer模型。

Conclusion: 提出的卷积混合模型兼具较高精度与优良效率，为遥感图像场景分类提供了新的高效方案，适用于资源受限场景。

Abstract: Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer

</details>


### [116] [Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion](https://arxiv.org/abs/2512.06882)
*Yu Zhu,Naoya Chiba,Koichi Hashimoto*

Main category: cs.CV

TL;DR: 该论文提出了一种分层的图像引导3D分割框架，通过将2D分割技术与3D点云相结合，实现了复杂工业环境下多尺度和高遮挡场景中的高精度3D分割。


<details>
  <summary>Details</summary>
Motivation: 工业场景常见的复杂布局和密集物体导致的严重遮挡与尺度差异，使得现有的3D分割方法难以同时精准捕捉粗略与细致的结构。此外，3D点云方法需高昂标注，而2D图像方法又存在跨视角语义不一致的问题。

Method: 框架分为实例级和部件级分割两阶段：实例分割阶段渲染俯视图像，并用SAM和YOLO-World得到的掩码回投到3D点云；部件级分割则针对每个实例渲染多视角图像，重复2D分割—回投，并通过贝叶斯更新融合，保证跨视角语义一致。

Result: 在真实工厂数据集上，方法能有效应对遮挡和结构复杂性，达到了每类均高的mIoU分数；在公开数据上同样表现出良好的泛化能力。

Conclusion: 该框架在复杂3D环境下表现出较强的稳健性、泛化性和注释效率，能够适应多样化的实际3D分割任务。

Abstract: Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.

</details>


### [117] [JoPano: Unified Panorama Generation via Joint Modeling](https://arxiv.org/abs/2512.06885)
*Wancheng Feng,Chen An,Zhenliang He,Meina Kan,Shiguang Shan,Lukun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种称为JoPano的联合面全景生成方法，统一了文本到全景和视图到全景两项任务，并大幅提升生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有全景生成方法多采用U-Net结构，导致视觉质量受限，并且通常将文本到全景和视图到全景任务分开处理，造成冗余和效率低下。

Method: 提出以DiT为主干的联合面适配器（Joint-Face Adapter），基于全景的立方体表示方法，借助现有DiT模型的强大生成能力来处理全景生成。同时，通过Poisson融合减少立方体面之间的缝隙不一致，并引入Seam-SSIM和Seam-Sobel指标来评估缝隙一致性。此外，通过条件切换机制将文本到全景和视图到全景任务整合于同一模型。

Result: 实验显示，JoPano在文本到全景和视图到全景两项任务中均能生成高质量全景图，在FID、CLIP-FID、IS和CLIP-Score等指标上达到最新最优水平。

Conclusion: JoPano有效攻克了现有全景生成方法的结构与任务分离局限，提升了生成质量和模型效率，并为未来全景生成任务统一建模与高质量输出提供了新途径。

Abstract: Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.

</details>


### [118] [Balanced Learning for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2512.06886)
*Wangkai Li,Rui Sun,Bohao Liao,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督领域自适应（UDA）语义分割方法BLDA，通过直接评估并缓解类别偏置，提高了自适应分割的效果，尤其对于被低估的类别有较大提升。


<details>
  <summary>Details</summary>
Motivation: 传统UDA方法中的自训练技术由于类别不平衡和跨域标签分布偏移，导致模型在各类别学习上表现不一致，特别是对少数类难以充分学习。作者旨在解决UDA中的类别偏置问题。

Method: 1）通过分析预测logits分布，识别被高估和低估的类别；2）基于锚点分布，对各类别logits进行对齐；3）在网络自训练时，在线估算logits分布并将logits修正项加入损失函数；4）利用累积密度作为跨域的结构知识，连接源域和目标域。

Result: 在两个主流UDA语义分割基准上进行大量实验，BLDA在集成到多种现有方法时，均带来了持续的性能提升，尤其明显改善了对低估类别的分割性能。

Conclusion: BLDA方法可以显著缓解UDA中的类别偏置问题，无需先验知识即可提升跨域语义分割的整体表现，尤其对难分的少数类别效果更加突出。

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.

</details>


### [119] [Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation](https://arxiv.org/abs/2512.06888)
*Liyang Song,Hardik Bishnoi,Sai Kumar Reddy Manne,Sarah Ostadabbas,Briana J. Taylor,Michael Wan*

Main category: cs.CV

TL;DR: 本文提出了一个新的婴儿无接触呼吸监测视频数据集（AIR-400），并开发了首套可复现的基于视觉的婴儿呼吸估计算法和基准测试。


<details>
  <summary>Details</summary>
Motivation: 目前针对成人的视觉呼吸估计已有丰富算法和数据集，但婴儿呼吸监测相关的公开数据和有效算法极为稀缺，制约了呼吸异常早期检测和相关疾病预警研究。

Method: 作者收集并精细标注了10名婴儿共275段新视频，整合已有数据，总共构建了AIR-400数据集。提出了针对婴儿的兴趣区域检测、结合光流的时空神经网络等方法，开发了呼吸估计算法及完整处理流程。

Result: 通过全面实验，作者首次为婴儿呼吸视觉估计确立了可复现的性能基准，并公开了数据集、代码和训练模型。

Conclusion: 本文显著推动了婴儿无接触呼吸监测的研究基础，数据与算法工具的公开为后续相关研究与实际临床应用提供了重要资源。

Abstract: The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.

</details>


### [120] [Scaling Zero-Shot Reference-to-Video Generation](https://arxiv.org/abs/2512.06905)
*Zijian Zhou,Shikun Liu,Haozhe Liu,Haonan Qiu,Zhaochong An,Weiming Ren,Zhiheng Liu,Xiaoke Huang,Kam Woh Ng,Tian Xie,Xiao Han,Yuren Cong,Hang Li,Chuyan Zhu,Aditya Patel,Tao Xiang,Sen He*

Main category: cs.CV

TL;DR: Saber提出一种无需显式参考图像-视频-文本三元组数据的参考到视频（R2V）生成零样本框架，具备更强泛化性和更高性能。


<details>
  <summary>Details</summary>
Motivation: 当前R2V方法依赖于高昂且难以扩展的参考图像-视频-文本三元组数据，极大限制了该领域的发展和应用规模。

Method: Saber仅利用视频-文本对进行训练，采用掩码训练策略和专门设计的注意力模型，学习身份一致且参考感知的表征，并通过掩码增强方式减轻常见的参考拷贝失真问题。

Result: Saber能灵活适应不同数量的参考图像输入，在OpenS2V-Eval基准上，相较于依赖R2V数据的方法表现更优。

Conclusion: Saber框架在无需显式R2V数据的前提下，实现了参考一致性和泛化性，并提升了R2V生成的性能，极大拓展了技术应用潜力。

Abstract: Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.

</details>


### [121] [Can We Go Beyond Visual Features? Neural Tissue Relation Modeling for Relational Graph Analysis in Non-Melanoma Skin Histology](https://arxiv.org/abs/2512.06949)
*Shravan Venkatraman,Muthu Subash Kavitha,Joe Dhanith P R,V Manikandarajan,Jia Wu*

Main category: cs.CV

TL;DR: 本论文提出了一种新型组织关系建模神经网络（NTRM），用于提升皮肤癌病理图像分割的准确性，尤其是在组织重叠或形态相似区域。该方法在主流数据集上显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN的病理图像分割方法主要关注局部纹理，忽视了组织之间的空间和功能关系，导致在边界密集或组织相似区域分割效果差，本研究旨在解决这一问题。

Method: 作者提出在CNN基础上，加入组织级图神经网络（GNN），为预测区域建立图结构，通过信息传递建模组织间上下文关系，并通过空间投影优化分割结果。

Result: 在Histopathology Non-Melanoma Skin Cancer Segmentation数据集上，NTRM方法的Dice相似系数比现有最佳方法高4.9%至31.25%。

Conclusion: 组织关系建模为病理分割带来更好的上下文理解和解释能力，相比传统方法具备更强的结构感知能力，在实际应用中潜力巨大。

Abstract: Histopathology image segmentation is essential for delineating tissue structures in skin cancer diagnostics, but modeling spatial context and inter-tissue relationships remains a challenge, especially in regions with overlapping or morphologically similar tissues. Current convolutional neural network (CNN)-based approaches operate primarily on visual texture, often treating tissues as independent regions and failing to encode biological context. To this end, we introduce Neural Tissue Relation Modeling (NTRM), a novel segmentation framework that augments CNNs with a tissue-level graph neural network to model spatial and functional relationships across tissue types. NTRM constructs a graph over predicted regions, propagates contextual information via message passing, and refines segmentation through spatial projection. Unlike prior methods, NTRM explicitly encodes inter-tissue dependencies, enabling structurally coherent predictions in boundary-dense zones. On the benchmark Histopathology Non-Melanoma Skin Cancer Segmentation Dataset, NTRM outperforms state-of-the-art methods, achieving a robust Dice similarity coefficient that is 4.9\% to 31.25\% higher than the best-performing models among the evaluated approaches. Our experiments indicate that relational modeling offers a principled path toward more context-aware and interpretable histological segmentation, compared to local receptive-field architectures that lack tissue-level structural awareness. Our code is available at https://github.com/shravan-18/NTRM.

</details>


### [122] [Selective Masking based Self-Supervised Learning for Image Semantic Segmentation](https://arxiv.org/abs/2512.06981)
*Yuemin Wang,Ian Stavness*

Main category: cs.CV

TL;DR: 提出了一种新的自监督语义分割预训练方法——选择性掩码图像重建，相较于随机掩码方法显著提升了下游分割任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有掩码图像建模大多采用随机掩码，但这种方式未能充分利用模型已经习得的知识，从而限制了自监督预训练的有效性。

Method: 提出将图像重建预训练任务分为迭代步骤，在每一步选择掩掉重建损失最高的图像块（而非随机掩码），以利用模型当前的已学知识，提升图像分割特征的表征能力。

Result: 在Pascal VOC、Cityscapes（通用数据集）和Nassar 2020、Sugarbeets 2016（杂草分割数据集）上，选择性掩码方法分别比随机掩码和ImageNet监督预训练提升了2.9%和2.5%的分割准确率，尤其对难分割类别提升显著。

Conclusion: 选择性掩码图像重建是一种高效实用的自监督预训练方法，能有效提升低计算资源环境和有限模型容量下的语义分割性能，建议在特定场景优先使用。

Abstract: This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.

</details>


### [123] [Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues](https://arxiv.org/abs/2512.07034)
*Tuan-Anh Vu,Hai Nguyen-Truong,Ziqiang Zheng,Binh-Son Hua,Qing Guo,Ivor Tsang,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 本文提出了一种新方法TransCues，通过边界特征增强和反射特征增强两个模块，提升对于玻璃等透明物体的分割性能，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有分割方法难以区分玻璃等透明物体与不透明物体，主要由于玻璃的透明性和反射性所致。人类感知依赖边界和反射特征识别玻璃，但以往方法未同时充分利用这两种线索。

Method: 提出了边界特征增强（Boundary Feature Enhancement）和反射特征增强（Reflection Feature Enhancement）模块，并将其融合到金字塔Transformer编码-解码结构TransCues中，同时挖掘和利用两种视觉线索以提升分割效果。

Result: 该方法在多个主流数据集上效果显著优于SOTA。在Trans10K-v2、MSD、RGBD-Mirror、TROSD、Stanford2D3D等数据集上，mIoU提升4.2%到13.1%。

Conclusion: 结合边界和反射特征对分割透明物体非常有效，TransCues方法显著提升分割性能，对透明和镜面物体的理解有重要意义。

Abstract: Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.

</details>


### [124] [Evaluating and Preserving High-level Fidelity in Super-Resolution](https://arxiv.org/abs/2512.07037)
*Josep M. Rocafort,Shaolin Su,Javier Vazquez-Corral,Alexandra Gomez-Villa*

Main category: cs.CV

TL;DR: 当前超分辨率（SR）模型虽然能生成高视觉质量的图片，但容易产生“幻觉”改变内容。本文提出衡量高层次语义保真度的新标准，并构建了第一个带注释保真分数的数据集，分析SR模型在高层次语义保真度上的表现，提出用基础模型辅助，进而提升SR模型的语义保真与感知质量。


<details>
  <summary>Details</summary>
Motivation: 视觉质量高的SR模型往往会生成虚假的细节，导致图像内容被更改，现有评价指标难以度量这种高层次语义失真。为了更好地反映SR模型可靠性，亟需新的度量标准。

Method: 1）构建并标注了首个SR高层次保真度数据集；2）评估SOTA SR模型在高层语义保真度上的表现，并对比主流图像质量评价指标与保真度的相关性；3）通过基础模型辅助改进；4）用反馈微调SR模型，实现质量和保真度同步提升。

Result: 验证了现有评价指标与高层语义保真度相关性不足；基础模型能更好地处理高层保真度评价，通过用数据集反馈微调SR模型可同时提升语义保真和视觉感知效果。

Conclusion: 高层次语义保真度是评估SR模型可靠性的必要补充，所提出的数据集和标准具备实际应用和模型优化价值。

Abstract: Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.

</details>


### [125] [DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation](https://arxiv.org/abs/2512.07051)
*Adnan Munir,Shujaat Khan*

Main category: cs.CV

TL;DR: 本论文提出了一种新型轻量级医学图像分割网络DAUNet，通过引入可变形卷积和无参数注意力机制，实现了更高分割精度且模型复杂度低。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像分割模型在应对几何变化与上下文感知能力上存在局限，尤其是在实时和资源有限的临床环境下，需要更加高效且精度高的分割方法。

Method: 提出DAUNet，将第二版可变形卷积（Deformable V2 Convolutions）用于UNet主干以提升空间适应性，并在解码器与跳跃连接路径中加入SimAM无参数注意力模块，以增强特征融合和区域显著性。

Result: 在两个医学图像数据集（超声胎头-耻骨联合和CT肺栓子检测）上测试，DAUNet在Dice分数、HD95和ASD等指标上均优于当前主流方法，并且参数量更低。消融实验确认了新引入模块的独立有效性。

Conclusion: DAUNet不仅提升了分割准确率，还具备高效、对缺失上下文和低对比度区域鲁棒的特点，非常适合在实时和资源受限的临床环境中应用。

Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.

</details>


### [126] [RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting](https://arxiv.org/abs/2512.07052)
*Hoang-Nhat Tran,Francesco Di Sario,Gabriele Spadaro,Giuseppe Valenzise,Enzo Tartaglione*

Main category: cs.CV

TL;DR: 本文提出了一种适用于3D高斯Splatting(3DGS)的灵活压缩方法，可在预设范围内自适应调整压缩率，无需多次训练且保持高渲染质量，极大提升了实际应用场景下的效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 尽管3DGS实现了实时、逼真的三维场景渲染，但其对存储和训练资源要求较高，限制了其在带宽及设备性能受限环境下的应用。因此，亟需一种高效且可自适应的压缩方案，使其适应不同的带宽和设备条件。

Method: 作者提出了一种可以在预置上下限内连续调整压缩率的新型3DGS压缩框架。该框架计算开销低、在任何压缩率下无需重新训练模型，同时通过设计实现了在不同运行点间进行高质量插值。

Result: 实验表明，该方法在不同压缩率下均可保持高效和高质量的渲染表现，实现了灵活且动态的码率控制，优于现有固定压缩率的方法。

Conclusion: 这项工作为3DGS带来了灵活、高效的压缩能力，适合在多样化实际设备和带宽条件下广泛应用，推动了3D场景实时渲染技术的落地。

Abstract: Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.

</details>


### [127] [$\mathrm{D}^{\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction](https://arxiv.org/abs/2512.07062)
*Changliang Xia,Chengyou Jia,Minnan Luo,Zhuohang Dang,Xin Shen,Bowen Ping*

Main category: cs.CV

TL;DR: 作者指出扩散模型在密集预测任务中的噪声机制与任务本质不匹配，提出了一种去噪声的确定性扩散模型，即D3-Predictor，实现了高效准确的几何映射。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在密集预测（如图像到几何结构的映射）中受限于其随机噪声机制，因为密集预测需确定性映射，而扩散采样的噪声会破坏空间信息和结构对应。

Method: 提出D3-Predictor：重新设计预训练扩散模型，彻底去除随机噪声，将每个扩散步视为不同特征专家，并聚合其特征形成完整几何先验；结合任务特定监督实现端到端适配。

Result: 在多种密集预测任务上，D3-Predictor展现出与最新技术媲美或更优的性能，同时所需训练数据量减少一半以上，推理仅需一步，极大提升了效率。

Conclusion: 用无噪声、确定性机制重塑扩散模型可显著改善密集预测任务表现，是提升视觉预测结构映射的新思路。

Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^{\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^{\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^{\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.

</details>


### [128] [Persistent Homology-Guided Frequency Filtering for Image Compression](https://arxiv.org/abs/2512.07065)
*Anil Chintapalli,Peter Tenholder,Henry Chen,Arjun Rao*

Main category: cs.CV

TL;DR: 该论文结合离散傅里叶变换和持久性同调分析，实现了噪声图像数据集中的有效特征提取和压缩，在多项实验指标上与JPEG压缩效果相当，并提升了CNN分类任务的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对噪声图像数据集中，现有特征提取与压缩方法在可靠性和下游任务表现上的局限性，研究如何利用拓扑与频率信息改善压缩可靠性。

Method: 采用离散傅里叶变换提取频域信息，并结合持久性同调分析筛选对应特定拓扑特征的频率，实现有意义信息的压缩与重构。

Result: 在六项指标上，所提方法的图像压缩率与JPEG相当，同时在CNN二分类实验中，持久性同调引导的频率筛选提升了分类性能。

Conclusion: 持久性同调分析指导的频率过滤方法能提升噪声干扰下图像压缩的可靠性，对深度学习任务具有实用价值。

Abstract: Feature extraction in noisy image datasets presents many challenges in model reliability. In this paper, we use the discrete Fourier transform in conjunction with persistent homology analysis to extract specific frequencies that correspond with certain topological features of an image. This method allows the image to be compressed and reformed while ensuring that meaningful data can be differentiated. Our experimental results show a level of compression comparable to that of using JPEG using six different metrics. The end goal of persistent homology-guided frequency filtration is its potential to improve performance in binary classification tasks (when augmenting a Convolutional Neural Network) compared to traditional feature extraction and compression methods. These findings highlight a useful end result: enhancing the reliability of image compression under noisy conditions.

</details>


### [129] [Context-measure: Contextualizing Metric for Camouflage](https://arxiv.org/abs/2512.07076)
*Chen-Yang Wang,Gepeng Ji,Song Shao,Ming-Ming Cheng,Deng-Ping Fan*

Main category: cs.CV

TL;DR: 提出一种新的针对伪装场景的评估方法Context-measure，解决现有度量忽视空间上下文的问题，比现有方法更契合人类感知。


<details>
  <summary>Details</summary>
Motivation: 目前伪装场景的度量多数源于通用或显著性物体评估，假定空间上下文无关，无法准确评价含伪装关系的图像。

Method: 提出Context-measure度量框架，基于概率像素相关性，结合空间依赖和像素级的伪装量化，反映图像中上下文关系。

Result: 在三个具有挑战性的伪装物体分割数据集上进行实验，结果显示该度量相比传统无上下文方法表现更可靠。

Conclusion: Context-measure为涉及伪装模式的计算机视觉领域提供了更符合实际感知的评估工具，可广泛用于农业、工业和医学等场景。

Abstract: Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.

</details>


### [130] [DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection](https://arxiv.org/abs/2512.07078)
*Bo Gao,Jingcheng Tong,Xingsheng Chen,Han Yu,Zichen Li*

Main category: cs.CV

TL;DR: 本文提出DFIR-DETR方法，在无人机遥感图像小目标检测和工业表面缺陷识别任务上表现出色，解决了特征稀疏、尺度变化大、背景杂乱等挑战。创新模块结合频域处理与动态特征聚合，在NEU-DET和VisDrone数据集上达到了先进水平。


<details>
  <summary>Details</summary>
Motivation: 无人机遥感和工业检测存在小目标难检测、特征弱、背景复杂等共性难题；现有transformer检测器在特征降维、长距离依赖和上采样等方面存在缺陷。为此，需要新的模型结构提升小目标检测表现和泛化能力。

Method: 构建DFIR-DETR架构，包含DCFA模块（动态K稀疏注意力，复杂度降低，提升非线性表达）、DFPN模块（归一化上采样防止特征膨胀，双路径卷积保持空间信息）、FIRC3模块（频域操作获取全局感受野）。

Result: 在NEU-DET和VisDrone数据集分别获得mAP50为92.9%和51.6%的领先成绩，仅11.7M参数和41.2 GFLOPs，模型轻量高效。

Conclusion: DFIR-DETR方法能跨场景高效检测小目标，具备良好泛化性，适合算力受限场景的通用检测任务。

Abstract: Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.

</details>


### [131] [COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision](https://arxiv.org/abs/2512.07107)
*Jaeyoon Lee,Hojoon Jung,Sungtae Hwang,Jihyong Oh,Jongwon Choi*

Main category: cs.CV

TL;DR: 提出了COREA，这是首个能联合学习可重光照3D高斯和有符号距离场（SDF）的统一框架，实现了精确的几何重建和真实的重光照效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯Splatting方法虽然开始支持网格重建和基于物理的渲染，但其三维几何是基于二维渲染结果学习的，导致表面粗糙且BRDF分解不可靠。

Method: COREA采用了一个粗到细的双向3D对齐策略，直接在三维空间中学习几何信号。具体地，先利用深度信息完成初步对齐，再通过深度梯度和法线优化精细结构，最终支持稳定的BRDF-光照分解，并通过密度控制机制平衡几何精度和内存效率。

Result: 在标准数据集上，COREA在新视角合成、网格重建和物理渲染方面的表现优于现有方法。

Conclusion: COREA实现了几何重建和真实重光照的统一框架，为相关三维重建与渲染问题提供了新的解决思路和更高的性能。

Abstract: We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.

</details>


### [132] [MSN: Multi-directional Similarity Network for Hand-crafted and Deep-synthesized Copy-Move Forgery Detection](https://arxiv.org/abs/2512.07110)
*Liangwei Jiang,Jinluo Xie,Yecheng Huang,Hua Zhang,Hongyu Yang,Di Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的双流模型（MSN），能更高效准确地检测图像复制-粘贴伪造，通过多方向卷积编码和二维相似矩阵提升了特征表征与篡改区域定位能力，并在多个数据集上取得了最佳效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型在复制-粘贴伪造检测的特征表征与篡改定位两个方面存在明显局限，特别是在应对复杂变换和精细化操作下检测难度加大，亟需新方法提升检测准确率与鲁棒性。

Method: 提出多方向相似性网络（MSN），双流结构分别对图像进行多层次、多尺度多旋转方向的特征编码，然后采用二维相似性矩阵解码器，充分利用空间信息提升伪造区域定位精度。同时，构建了包含多种深度神经网络生成的伪造新数据库。

Result: 在CASIA CMFD、CoMoFoD以及自建数据库上进行了大量实验，所提方法在检测准确率与伪造区域定位能力方面均优于现有主流方法，达到了最新技术水平（SOTA）。

Conclusion: 多方向特征编码和二维相似性解码结合提升了复制-粘贴伪造检测的有效性，在实际复杂伪造情况下具备更强泛化能力和实用价值，并为后续深度伪造鉴别研究提供了新工具和数据集。

Abstract: Copy-move image forgery aims to duplicate certain objects or to hide specific contents with copy-move operations, which can be achieved by a sequence of manual manipulations as well as up-to-date deep generative network-based swapping. Its detection is becoming increasingly challenging for the complex transformations and fine-tuned operations on the tampered regions. In this paper, we propose a novel two-stream model, namely Multi-directional Similarity Network (MSN), to accurate and efficient copy-move forgery detection. It addresses the two major limitations of existing deep detection models in \textbf{representation} and \textbf{localization}, respectively. In representation, an image is hierarchically encoded by a multi-directional CNN network, and due to the diverse augmentation in scales and rotations, the feature achieved better measures the similarity between sampled patches in two streams. In localization, we design a 2-D similarity matrix based decoder, and compared with the current 1-D similarity vector based one, it makes full use of spatial information in the entire image, leading to the improvement in detecting tampered regions. Beyond the method, a new forgery database generated by various deep neural networks is presented, as a new benchmark for detecting the growing deep-synthesized copy-move. Extensive experiments are conducted on two classic image forensics benchmarks, \emph{i.e.} CASIA CMFD and CoMoFoD, and the newly presented one. The state-of-the-art results are reported, which demonstrate the effectiveness of the proposed approach.

</details>


### [133] [Training-free Clothing Region of Interest Self-correction for Virtual Try-On](https://arxiv.org/abs/2512.07126)
*Shengjie Lu,Zhibin Wan,Jiejie Liu,Quan Zhang,Mingjie Sun*

Main category: cs.CV

TL;DR: 该论文提出了一种改进的虚拟试衣（VTON）方法，通过在生成过程中利用能量函数约束注意力图，使生成的服装与目标服装在细节上一致，并设计了新的评测指标VTID。实验结果在多个主流数据集和传统指标上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣方法在服装图案、纹理和边界的保真上与目标服装存在差异，且当前评测指标单一，仅关注图像真实性，忽略了生成结果与目标元素的一致性。

Method: 方法上，作者在生成过程中用能量函数约束注意力图，让注意力更集中在服装区域，提高细节复制能力。同时提出新的评测指标VTID，更全面评估生成效果。

Result: 在VITON-HD和DressCode数据集上，所提方法在LPIPS、FID、KID及VTID等指标上分别超越SOTA方法1.4%、2.3%、12.3%、5.8%。用于服装更换再识别任务（CC-Reid），在LTCC、PRCC、VC-Clothes数据集的Rank-1指标上提升2.5%、1.1%、1.6%。

Conclusion: 本文方法能更好地保留目标服装细节，生成结果与目标高度一致，并提出的新评测指标更适用于虚拟试衣任务。所提方法已公开代码，有较高推广价值。

Abstract: VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.

</details>


### [134] [MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP](https://arxiv.org/abs/2512.07128)
*Chau Truong,Hieu Ta Quang,Dung D. Le*

Main category: cs.CV

TL;DR: MulCLIP提出了一种新的视觉-语言对齐框架，更好地处理了长文本与图像间的匹配，相较于区域建议方法，同时减少了部署成本，并提升了精细内容理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型如CLIP主要在短文本训练下表现良好，但对于长文本描述的对齐表现有限。近期方法通过依赖区域建议来辅助对齐，虽有改进，但在实际部署时带来较高成本。作为回应，该工作旨在开发一种更有效率、能多尺度对齐长文本细节与图像内容的方法。

Method: MulCLIP引入了多级对齐框架：一是在全局上保持图像与摘要和长描述的对比学习对齐，并扩展位置编码以适应长文本；二是在局部层面，提出token重建对齐，加强词与图像区域的语义联系；三是子标题聚合patch对齐方法，用于自动提取并聚合与每个子标题相关联的丰富图像patch。整个框架无须额外地域建议辅助，实现端到端训练。

Result: 在多个主流基准测试上，MulCLIP方法在下游任务中表现出持续提升，特别是在需要精细粒度理解的任务上优于依赖区域建议的方法。消融实验表明其多级对齐机制是提升表现的关键。

Conclusion: MulCLIP为视觉-语言模型提供了一种高效、精细的对齐机制，兼顾长文本处理和理解能力，降低了部署复杂度，适合多样化的真实应用场景。

Abstract: Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.

</details>


### [135] [TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning](https://arxiv.org/abs/2512.07135)
*Zebin Xing,Pengxuan Yang,Linbo Wang,Yichen Zhang,Yiming Hu,Yupeng Zheng,Junli Wang,Yinfeng Gao,Guang Li,Kun Ma,Long Chen,Zhongpu Xia,Qichao Zhang,Hangjun Ye,Dongbin Zhao*

Main category: cs.CV

TL;DR: 本论文针对自动驾驶系统中端到端学习遇到的轨迹先验泛化与评估机制不足的问题，提出利用专家混合模型(MoE)应对场景差异，并结合强化学习优化轨迹评估，最终在权威基准获得第三名。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶更多关注于端到端的感知到轨迹映射，而忽视了轨迹先验的场景适应性与轨迹评分机制的策略性，导致泛化能力与规划性能受限。作者希望解决轨迹先验的场景适配与评分机制优化两个核心问题。

Method: 针对不同驾驶场景使用MoE方法(专家混合模型)选择不同的轨迹先验，同时通过强化学习流程对轨迹评分机制进行策略层的优化。此外，集成不同感知backbone加强感知表示，提升整体性能。

Result: 提出的方法在navsim ICCV基准任务中获得了51.08分，排名第三，显示了方法的有效性。

Conclusion: 通过为不同场景引入针对性的轨迹先验，并用强化学习进一步优化轨迹选择策略，可以显著提升端到端自动驾驶系统的规划表现。集成不同感知backbone也增强了模型的整体感知能力。

Abstract: Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.

</details>


### [136] [A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning](https://arxiv.org/abs/2512.07136)
*Siyang Jiang,Mu Yuan,Xiang Ji,Bufang Yang,Zeyu Liu,Lilin Xu,Yang Li,Yuting He,Liran Dong,Wenrui Lu,Zhenyu Yan,Xiaofan Jiang,Wei Gao,Hongkai Chen,Guoliang Xing*

Main category: cs.CV

TL;DR: 本文提出CUHK-X，一个大规模多模态人体动作识别与推理数据集，为细粒度人类动作理解和因果推理提供支持，并提出基于LLM生成高一致性文本描述的方法。


<details>
  <summary>Details</summary>
Motivation: 现有大模型尤其是视觉语言大模型难以处理如深度、IMU和毫米波等非RGB模态，因为缺乏大规模数据-文本配对资源，而现有HAR数据集多为标签型、粗粒度标注，不支持细粒度的人体动作理解与推理任务。

Method: 1) 建立CUHK-X 多模态人体动作数据集，涵盖深度、多种传感器共40类动作58,445条样本；2) 提出基于大模型的场景描述生成流程，通过prompt设计，让LLM生成连贯的动作序列并人工校验，提升文本注释一致性；3) 设置三个基准和六个子任务全面评测。

Result: CUHK-X在三个任务上的平均表现为：动作识别(HAR)76.52%、动作理解(HAU)40.76%、动作推理(HARn)70.25%。数据与基准对比支持基于数据的大规模多模态动作表征、理解与推理模型研究。

Conclusion: CUHK-X推进多模态人体动作识别、理解和推理，为数据密集型模型开发提供了高质量基准和任务分解，填补了现有数据集在文本描述和多模态细粒度标签方面的空白。

Abstract: Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.

</details>


### [137] [CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics](https://arxiv.org/abs/2512.07155)
*Dahyeon Kye,Jeahun Sung,MinKyu Jeon,Jihyong Oh*

Main category: cs.CV

TL;DR: 本文提出了CHIMERA，一个零样本的扩散模型变换框架，可以平滑且语义一致地实现图像变换，显著提升现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成上表现突出，但现有的图像变换方法往往会出现突兀的过渡或者过度饱和外观，缺乏结构与语义的自适应对齐。为解决这些问题，提出更自然和一致的图像变换方法。

Method: 提出了两项核心方法：（1）自适应缓存注入（ACI）：在DDIM反演中缓存两张输入图像的多层特征，并在去噪过程中自适应地重新注入，有效实现空间与语义的对齐，促使特征融合和顺滑过渡；（2）语义锚点提示（SAP）：利用视觉语言模型为两张输入生成共享的语义锚点提示，引导去噪过程获得一致性语义；（3）还提出了面向变换任务的全局-局部一致性评分指标GLCS。

Result: 实验和用户调研表明，CHIMERA框架在过渡的平滑性和语义一致性方面优于现有方法，在图像变换领域达到最新水平。

Conclusion: CHIMERA能实现平滑且语义对齐的图像变换，提升了扩散模型在图像变换领域的应用价值，并为后续研究提供了新基线。

Abstract: Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.

</details>


### [138] [MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation](https://arxiv.org/abs/2512.07165)
*Muyu Xu,Fangneng Zhan,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的3D高斯溅射新方法MuSASplat，能够在输入视角稀疏的情况下实现高质量新视角渲染，同时显著减少训练资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏视角3D高斯溅射方法虽然效果出色，但普遍依赖大规模ViT骨干网络的充分微调，GPU消耗极高，资源瓶颈严重。亟需一种能够兼顾效率与质量的训练框架。

Method: 提出MuSASplat，采用轻量级多尺度适配器（Multi-Scale Adapter）部分微调ViT架构，仅需极少训练参数即可高效训练；新增特征融合聚合器（Feature Fusion Aggregator），有效融合多视角特征，优化几何一致性，同时大幅降低显存和计算复杂度。

Result: 在多个多样数据集上广泛实验，MuSASplat在渲染质量方面达到SOTA水平，相比现有方法参数量与训练资源需求显著降低。

Conclusion: MuSASplat为稀疏视角3D高斯溅射带来了训练效率与渲染质量的新突破，兼具低资源消耗和高保真表现，适合实际应用场景。

Abstract: Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.

</details>


### [139] [When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing](https://arxiv.org/abs/2512.07166)
*Siyuan Xu,Yibing Liu,Peilin Chen,Yung-Hui Li,Shiqi Wang,Sam Kwong*

Main category: cs.CV

TL;DR: 该论文针对多模态大模型中的隐私恢复问题，提出了SPPE数据集并设计了一种新的隐私恢复方法，在隐私保护和模型可用性之间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型的隐私保护研究通常忽略了隐藏信息真实性和恢复质量的评价，缺乏相关数据集及方法，因此需要更好地衡量和提升隐私恢复效果。

Method: （1）提出SPPE数据集，包含多类隐私及多种用户指令，能模拟实际应用，并提供隐私保护版数据及其多模态编辑版本；（2）将隐私恢复建模为条件生成任务，利用补充多模态信号进行引导，提出统一的方法，可有效重建隐私内容，且保持编辑一致性。

Result: 方法在SPPE和InstructPix2Pix等数据集上实验，表明其在不同视觉内容和编辑任务上均有良好的泛化能力，实现了隐私保护和大模型可用性的权衡。

Conclusion: 本文首次系统提出并评估多模态大模型隐私内容恢复问题，通过新数据集和统一恢复方法，在保证隐私安全前提下提升了多模态模型的实际应用能力。

Abstract: Privacy leakage in Multimodal Large Language Models (MLLMs) has long been an intractable problem. Existing studies, though effectively obscure private information in MLLMs, often overlook the evaluation of the authenticity and recovery quality of user privacy. To this end, this work uniquely focuses on the critical challenge of how to restore surrogate-driven protected data in diverse MLLM scenarios. We first bridge this research gap by contributing the SPPE (Surrogate Privacy Protected Editable) dataset, which includes a wide range of privacy categories and user instructions to simulate real MLLM applications. This dataset offers protected surrogates alongside their various MLLM-edited versions, thus enabling the direct assessment of privacy recovery quality. By formulating privacy recovery as a guided generation task conditioned on complementary multimodal signals, we further introduce a unified approach that reliably reconstructs private content while preserving the fidelity of MLLM-generated edits. The experiments on both SPPE and InstructPix2Pix further show that our approach generalizes well across diverse visual content and editing tasks, achieving a strong balance between privacy protection and MLLM usability.

</details>


### [140] [Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach](https://arxiv.org/abs/2512.07170)
*Jiayang Li,Chengjie Jiang,Junjun Jiang,Pengwei Liang,Jiayi Ma,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出了DiTFuse，一个基于扩散-Transformer的图像融合框架，实现了多模态图像与自然语言指令的联合语义增强融合，不依赖真实融合图像，支持用户多级控制，并在多项公开数据集上实现了领先的定量和定性效果。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合方法在鲁棒性、适应性和可控性方面有限，且难以灵活地结合用户意图；同时缺乏大规模标注数据和真实融合图像，限制了端到端模型的语义理解和精细融合能力。针对这些问题，提出更智能、交互和通用的融合方法迫在眉睫。

Method: 提出了DiTFuse，一个融合扩散模型与Transformer架构的端到端框架，将两张图像与自然语言指令编码到共享潜空间，并通过多退化掩码图像建模联合学习模态对齐、恢复和特征选择，无需真实融合图像。还构建了多粒度指令数据集，实现互动式融合。

Result: DiTFuse可统一处理红外-可见光、多焦、多曝光等融合任务，并通过文本指令调控细节及后续任务。实验显示模型在IVIF、MFF、MEF等公开基准上取得更优的量化和质化表现，纹理更清晰、语义保留更好，并支持多级用户控制与零样本泛化。

Conclusion: DiTFuse实现了语义感知、多模态对齐且可控的图像融合，在多种应用任务和复杂场景下展现出优异表现，为多图像融合研究和实际应用提供了新范式。

Abstract: Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.

</details>


### [141] [TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration](https://arxiv.org/abs/2512.07171)
*Shravan Venkatraman,Rakesh Raj Madavan,Pavan Kumar S,Muthu Subash Kavitha*

Main category: cs.CV

TL;DR: 本文提出了一种名为TIDE的两阶段逆退化估计框架，有效提升了复杂水下图像的修复效果，特别是在多种不同退化因素共同存在的环境下。


<details>
  <summary>Details</summary>
Motivation: 现有的水下图像修复方法通常对全图采用统一处理策略，无法应对空间上多变且多种退化因素并存的复杂情况，因此亟需更精细、具有适应性的恢复方法。

Method: TIDE框架分两阶段：首先通过特定的先验分解，显式建模水下图像的退化特性，将退化分解为颜色失真、雾霾、细节丢失和噪声四个因素，并针对每一因素设计专门的修复模型，生成多种专业化假设；其次，通过自适应融合局部假设，并进一步进行递进式细化修正残余伪影。

Result: 在多项标准基准和挑战性的浑浊水域数据集上的实验表明，TIDE在有参考和无参考的评价指标上都表现优越，尤其在色彩校正和对比度提升方面优于现有主流方法。

Conclusion: TIDE框架能根据局部退化特征实施针对性修复，在高度退化场景下也能均衡各退化因素，生成更自然、更高质量的水下图像修复结果，在实际应用中具有较高价值。

Abstract: Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\underline{t}$wo stage $\underline{i}$nverse $\underline{d}$egradation $\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.

</details>


### [142] [START: Spatial and Textual Learning for Chart Understanding](https://arxiv.org/abs/2512.07186)
*Zhuoming Liu,Xiaofeng Gao,Feiyang Niu,Qiaozi Gao,Liu Liu,Robinson Piramuthu*

Main category: cs.CV

TL;DR: 本文提出了一种针对图表理解的新方法START，通过结合图像空间结构与底层数据文本属性，实现多模态大模型对图表的更精准理解。


<details>
  <summary>Details</summary>
Motivation: 图表在科学论文、技术报告等现实场景中广泛存在，但与自然图像不同，图表同时具备空间结构（视觉布局）与数据表示（文本属性）。现有多模态模型难以同时充分理解这两方面，影响图表推理效果。

Method: 提出START框架，包括图表元素定位和图表到代码的生成，增强模型对图表空间和文本信息的理解。同时设计了新的数据集START-Dataset，通过利用MLLM翻译真实图表为可执行代码来还原数据，并借助大语言模型推断元素空间位置。此外，提出了衡量空间结构理解能力的新基准CS-Bench。

Result: START方法在多个模型规模和基准测试上均优于基础模型，显著领先先前最佳方法。

Conclusion: START框架通过融合空间和文本学习，显著提升了多模态大模型在图表理解任务上的表现，为实际任务部署提供了有力支撑。

Abstract: Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.

</details>


### [143] [Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification](https://arxiv.org/abs/2512.07190)
*Pengfei Gu,Huimin Li,Haoteng Tang,Dongkuan,Xu,Erik Enriquez,DongChul Kim,Bin Fu,Danny Z. Chen*

Main category: cs.CV

TL;DR: 本文提出了一种将多尺度、多滤波的拓扑特征与视觉分类主干网络相结合的新框架，大幅提升了医学图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络在医学图像分类中表现出色，但往往侧重像素特征，忽略了反映解剖结构等重要信息的拓扑不变量，或仅仅利用单一参数的简单拓扑特征，难以反映图像结构的复杂性。

Method: 作者提出先在不同尺度下计算图像的立方体持久性图，再用“vineyard”算法将不同尺度的持久性图合并为单一的稳定图，综合捕捉全局与局部拓扑特征。为充分利用多滤波下的复杂拓扑表达，设计了基于跨注意力的神经网络，直接处理合并后的持久性图，并与CNN或Transformer主干的特征融合，实现端到端训练。

Result: 在三个公开医学图像数据集上，该方法相较于强基线和业界最新技术均取得了稳定且显著的性能提升。

Conclusion: 多尺度多滤波的拓扑特征能有效增强模型对复杂解剖结构的理解，提升医学图像分类的准确性和可解释性，该综合拓扑方案具有实际应用与推广价值。

Abstract: Modern deep neural networks have shown remarkable performance in medical image classification. However, such networks either emphasize pixel-intensity features instead of fundamental anatomical structures (e.g., those encoded by topological invariants), or they capture only simple topological features via single-parameter persistence. In this paper, we propose a new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them into vision classification backbones. For an input image, we first compute cubical persistence diagrams (PDs) across multiple image resolutions/scales. We then develop a ``vineyard'' algorithm that consolidates these PDs into a single, stable diagram capturing signatures at varying granularities, from global anatomy to subtle local irregularities that may indicate early-stage disease. To further exploit richer topological representations produced by multiple filtrations, we design a cross-attention-based neural network that directly processes the consolidated final PDs. The resulting topological embeddings are fused with feature maps from CNNs or Transformers. By integrating multi-scale and multi-filtration topologies into an end-to-end architecture, our approach enhances the model's capacity to recognize complex anatomical structures. Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating the value of our comprehensive topological perspective for robust and interpretable medical image classification.

</details>


### [144] [RefLSM: Linearized Structural-Prior Reflectance Model for Medical Image Segmentation and Bias-Field Correction](https://arxiv.org/abs/2512.07191)
*Wenqi Zhao,Jiacheng Sang,Fenghua Cheng,Yonglu Shu,Dong Li,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 提出了一种新的基于反射率分解的水平集模型（RefLSM），用于医学图像分割，能更好处理强度不均、噪声等问题。


<details>
  <summary>Details</summary>
Motivation: 传统水平集方法在医学图像分割中效果有限，尤其在成像非均匀性强烈时依赖于近似偏置场估计，难以获得稳定和精准分割。

Method: 提出RefLSM模型，将Retinex反射率分解思想引入，将图像分解为反射率和偏置场，直接对与照明无关、保留结构细节的反射率进行分割；引入线性结构先验增强几何指导，采用凸松弛与符号投影处理水平集函数，利用ADMM进行高效求解。

Result: 在多个医学图像数据集上进行了大量实验，RefLSM在分割精度、鲁棒性、计算效率方面均优于现有先进水平集方法。

Conclusion: RefLSM显著提升了医学图像分割的鲁棒性与准确性，能够更好应对噪声、低对比度和非均匀成像条件，具有良好应用前景。

Abstract: Medical image segmentation remains challenging due to intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods, while effective in certain cases, often depend on approximate bias field estimations and therefore struggle under severe non-uniform imaging conditions. To address these limitations, we propose a novel variational Reflectance-based Level Set Model (RefLSM), which explicitly integrates Retinex-inspired reflectance decomposition into the segmentation framework. By decomposing the observed image into reflectance and bias field components, RefLSM directly segments the reflectance, which is invariant to illumination and preserves fine structural details. Building on this foundation, we introduce two key innovations for enhanced precision and robustness. First, a linear structural prior steers the smoothed reflectance gradients toward a data-driven reference, providing reliable geometric guidance in noisy or low-contrast scenes. Second, a relaxed binary level-set is embedded in RefLSM and enforced via convex relaxation and sign projection, yielding stable evolution and avoiding reinitialization-induced diffusion. The resulting variational problem is solved efficiently using an ADMM-based optimization scheme. Extensive experiments on multiple medical imaging datasets demonstrate that RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.

</details>


### [145] [HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression](https://arxiv.org/abs/2512.07192)
*Niu Yi,Xu Tianyi,Ma Mingming,Wang Xinkun*

Main category: cs.CV

TL;DR: 该论文提出了一种新的基于VQ超先验的可控生成式图像压缩框架（HVQ-CGIC），用以提升编码率-失真（RD）性能并实现灵活的比特率控制，在知名数据集上用更少比特达到与现有方法同等感知质量。


<details>
  <summary>Details</summary>
Motivation: 现有生成式VQ图像压缩方法的比特率主要依赖于全局静态概率分布，无法针对不同图像内容自适应调整，导致比特率利用不充分，也难以灵活控制压缩率。为解决这些局限，亟需针对图像内容自适应估算熵的方法提升压缩效果和灵活性。

Method: 作者提出HVQ-CGIC框架：1）基于严格数学推导，首次在VQ索引熵模型中引入超先验；2）设计新型损失函数，引入码率-失真权衡与控制能力；3）采用轻量级超先验估算网络，整体框架对比前沿方法实现更优性能。

Result: 在Kodak等公开数据集上，HVQ-CGIC达到与Control-GIC、CDC、HiFiC等同样的LPIPS感知质量指标，但平均比特数降低61.3%，充分证明了新方法在压缩率与感知质量之间的优势。

Conclusion: HVQ-CGIC不仅极大提升了生成式VQ图像压缩的压缩效率和灵活性，还有望像先验机制在神经网络压缩领域那样，成为VQGAN相关压缩方法的重要基础组件。

Abstract: Generative learned image compression methods using Vector Quantization (VQ) have recently shown impressive potential in balancing distortion and perceptual quality. However, these methods typically estimate the entropy of VQ indices using a static, global probability distribution, which fails to adapt to the specific content of each image. This non-adaptive approach leads to untapped bitrate potential and challenges in achieving flexible rate control. To address this challenge, we introduce a Controllable Generative Image Compression framework based on a VQ Hyperprior, termed HVQ-CGIC. HVQ-CGIC rigorously derives the mathematical foundation for introducing a hyperprior to the VQ indices entropy model. Based on this foundation, through novel loss design, to our knowledge, this framework is the first to introduce RD balance and control into vector quantization-based Generative Image Compression. Cooperating with a lightweight hyper-prior estimation network, HVQ-CGIC achieves a significant advantage in rate-distortion (RD) performance compared to current state-of-the-art (SOTA) generative compression methods. On the Kodak dataset, we achieve the same LPIPS as Control-GIC, CDC and HiFiC with an average of 61.3% fewer bits. We posit that HVQ-CGIC has the potential to become a foundational component for VQGAN-based image compression, analogous to the integral role of the HyperPrior framework in neural image compression.

</details>


### [146] [SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting](https://arxiv.org/abs/2512.07197)
*Seokhyun Youn,Soohyun Lee,Geonho Kim,Weeyoung Kwon,Sung-Ho Bae,Jihyong Oh*

Main category: cs.CV

TL;DR: 本文综述了高效3D/4D高斯投影（Gaussian Splatting）技术的发展，系统分类并总结了相关压缩方法，评估指标及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 传统3DGS方法虽然能实现高保真三维重建和视图合成，但对存储与计算资源的需求极大，尤其是在4D动态场景下更为严重，因此需要高效的表示和处理方法。

Method: 本文首次对高效的3D/4D高斯投影方法进行统一归纳，将现有方法分为参数压缩和结构压缩两大类，并总结每类的核心思想和技术趋势。此外，论文还对常用数据集、评价指标和主流算法性能进行了梳理与对比。

Result: 该综述系统整理了高效高斯投影方法的研究进展，明确了主要技术路线，并对各类方法进行横向比较，展示了现有高效化技术在表达能力与资源消耗上的权衡。

Conclusion: 作者指出目前高效3D/4DGS仍存在瓶颈，未来研究应聚焦于实现可扩展、紧凑且实时性能优异的高斯投影技术，以更好地支持静态与动态场景的三维重建与表示。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.

</details>


### [147] [Understanding Diffusion Models via Code Execution](https://arxiv.org/abs/2512.07201)
*Cheng Yu*

Main category: cs.CV

TL;DR: 本文提出了一份简洁的扩散模型实现范例（约300行代码），通过代码视角帮助理解扩散模型的实际运作方式。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型理论较为复杂，且论文中的数学公式和开源实现之间存在理解鸿沟。多数教程偏重公式推导，缺少对实际代码运行的指导。作者希望通过代码实现来加深对扩散模型运作机理的理解。

Method: 作者编写了约300行的极简实现代码，覆盖了主要组件：正向扩散过程、反向采样、噪声预测网络及训练循环，剔除了多余的工程细节。

Result: 通过这份极简代码，读者能够清楚地看到理论与实际代码的对应关系，从代码执行角度掌握扩散模型的核心流程。作者还开源了完整代码和预训练模型。

Conclusion: 该技术报告为研究者提供了从实践出发理解扩散模型的有效工具，有助于弥合理论与实现的差距。

Abstract: Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.

</details>


### [148] [MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning](https://arxiv.org/abs/2512.07203)
*Xuhui Zheng,Kang An,Ziliang Wang,Yuhang Wang,Faqiang Qian,Yichao Wu*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态预训练框架MMRPT，通过引入强化学习机制，提升了大模型的视觉推理能力，并在多个评测中表现出更强泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在图文对数据上预训练时，容易受到描述性偏见影响，模型倾向于学习表层语言特征而非真实的视觉理解，导致推理能力不足。

Method: 该工作首次将强化学习直接应用于多模态大模型的预训练阶段。具体地，方法基于视觉token注意力，评估文本与视觉的依赖性，对高度依赖视觉的信息进行掩码，要求模型通过视觉信息推理恢复原文，并通过语义-视觉奖励信号监督模型学习更强的视觉 grounding 能力。

Result: 在多个评测基准上，模型展现出更高的零样本能力，并且在有监督微调下表现出更强的鲁棒性和泛化能力。

Conclusion: 基于强化学习的掩码推理预训练机制能够提供更加可靠、泛化性更强的目标，对于提升多模态模型的视觉推理和理解能力具有实际意义。

Abstract: Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.

</details>


### [149] [AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT](https://arxiv.org/abs/2512.07206)
*Boyang Pan,Zeyu Zhang,Hongyu Meng,Bin Cui,Yingying Zhang,Wenli Hou,Junhao Li,Langdi Zhong,Xiaoxiao Chen,Xiaoyu Xu,Changjin Zuo,Chao Cheng,Nan-Jie Gong*

Main category: cs.CV

TL;DR: 本文提出了AutoLugano，一种全自动深度学习系统，可实现从FDG-PET/CT图像到淋巴瘤分期的端到端自动化流程，具备精准而高效的分割、定位与分期能力。


<details>
  <summary>Details</summary>
Motivation: 现有淋巴瘤分期依赖手工标注与医生经验，耗时且存在主观性，临床急需一个自动化、标准化的分期工具来提升诊断效率与准确性，辅助医生治疗决策。

Method: AutoLugano包括三大模块：(1) 基于3D nnU-Net的解剖感知病灶分割模型，用于自动检测病灶；(2) 借助TotalSegmentator工具包的基于图谱的解剖定位，将分割区域映射到21个淋巴结区域；(3) 自动Lugano分期模块，根据病灶分布实现Lugano分期与治疗分组。模型在公共autoPET数据集训练，并在独立队列做外部验证，评估指标涵盖准确率、灵敏度、特异度与F1分数。

Result: 在外部验证集上，整体区域检测准确率为88.31%、灵敏度74.47%、特异度94.21%、F1分数80.80%，均优于基线模型；在关键的临床分组任务中（有限/进展期），准确率达85.07%，特异度90.48%，灵敏度82.61%。

Conclusion: AutoLugano是首个可将单一FDG-PET/CT扫描自动转化为完整Lugano分期的全自动端到端系统，具有显著提升初诊分期和治疗分组效率的潜力，可辅助临床决策。

Abstract: Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.

</details>


### [150] [Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds](https://arxiv.org/abs/2512.07211)
*Frederik Hagelskjær,Dimitrios Arapis,Steffen Madsen,Thorbjørn Mosekjær Iversen*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的基于神经网络的方法，在没有RGB信息的情况下仅使用无色3D数据来估计物体姿态不确定性。


<details>
  <summary>Details</summary>
Motivation: 以往的姿态估计方法通常只输出单一结果，无法反映视觉歧义带来的不确定性，且多数基于颜色信息。但在工业环境中，颜色信息常不可用，因此需要一个能在无色环境下估计姿态分布的不确定性方法。

Method: 提出采用基于神经网络的方法，仅用无色3D数据（如点云或深度图）输入，对物体的姿态分布进行建模。该方法可捕捉反射和旋转对称性的影响，且无需依赖RGB相机。

Result: 方法在实际的拆垛（bin picking）场景下验证，面对不同几何歧义物体时能有效估计姿态分布，并能处理反射与旋转对称问题。

Conclusion: 本工作率先实现了无需RGB输入的深度学习姿态分布估计，验证了其在工业场景的不确定性表达能力，并具备扩展到完整SE(3)空间的潜力。

Abstract: Object pose estimation is crucial to robotic perception and typically provides a single-pose estimate. However, a single estimate cannot capture pose uncertainty deriving from visual ambiguity, which can lead to unreliable behavior. Existing pose distribution methods rely heavily on color information, often unavailable in industrial settings.
  We propose a novel neural network-based method for estimating object pose uncertainty using only 3D colorless data. To the best of our knowledge, this is the first approach that leverages deep learning for pose distribution estimation without relying on RGB input. We validate our method in a real-world bin picking scenario with objects of varying geometric ambiguity. Our current implementation focuses on symmetries in reflection and revolution, but the framework is extendable to full SE(3) pose distribution estimation. Source code available at opde3d.github.io

</details>


### [151] [VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation](https://arxiv.org/abs/2512.07215)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: 本文对比分析了基于CLIP和DINOv2的视觉模型在三维抓取场景下的6D姿态估计性能，揭示了二者在语义一致性与几何精度上的互补优势。


<details>
  <summary>Details</summary>
Motivation: 基于VFMs和VLMs的新一代视觉模型已赋予计算机视觉更丰富的语义和几何表达能力，但在实际任务如机器人抓取中，不同模型各有优势与不足，因此需要深入对比分析，指导模型选择。

Method: 作者设计了实验证明，将基于CLIP和DINOv2的视觉处理方法分别应用于手物体抓取场景下的6D姿态估计算法，并在标准数据集上进行系统对比，重点关注语义一致性和几何特征的表现差异。

Result: 实验结果显示，CLIP模型因其良好的语言语义对齐，能实现更优的语义一致性；DINOv2则在稠密几何特征提取上表现突出，在几何精度上略优。两者表现具有互补性。

Conclusion: 本研究为机器人操作和抓取任务中视觉模型的选择提供了实证分析建议：如果应用更重语义理解可优先CLIP，若关注几何精度可选DINOv2，甚至可考虑融合两者优势。

Abstract: Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.

</details>


### [152] [Towards Robust Protective Perturbation against DeepFake Face Swapping](https://arxiv.org/abs/2512.07228)
*Hengyang Yao,Lin Li,Ke Sun,Jianing Qiu,Huiping Chen*

Main category: cs.CV

TL;DR: 本文针对DeepFake换脸攻击中的防御脆弱性问题，提出了自适应变换分布学习的对抗扰动方法（EOLT），明显提升了防御鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前通过在图片中嵌入不可见扰动来防御DeepFake存在鲁棒性差的问题，这些扰动易被压缩、缩放等基础操作破坏。因此，需要一种对多种变换更鲁棒的扰动生成方法。

Method: 系统性分析了30种常见图片变换，揭示目前常用的期望变换训练（EOT，采用均匀采样）策略在选择变换上的不足。为此，提出EOLT方法：通过策略网络自动学习和优先分配关键变换类别，并用强化学习动态生成针对特定实例的防御扰动，从而优化防御效果和适应性。

Result: 实验表明，EOLT方法在平均鲁棒性上比最新方法高出26%，在最具挑战性的变换类别下提升可达30%。

Conclusion: EOLT将变换分布由固定设计变为可学习组件，针对DeepFake防御显著增强鲁棒性，可广泛适用于不同图像变换环境。

Abstract: DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.

</details>


### [153] [ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery](https://arxiv.org/abs/2512.07229)
*Fang Zhou,Zhiqiang Chen,Martin Pavlovski,Yizhong Zhang*

Main category: cs.CV

TL;DR: 本文提出了ReLKD框架，通过挖掘和利用隐式类别间关系，提升了已知和新类的无标签数据分类性能，尤其适用于标签有限的场景。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法往往独立对待每一类别，忽略了类别间固有的关系，但在实际情况中直接获得这些类别关系非常困难，因此需要设计新方法自动挖掘并利用这些关系。

Method: 设计了ReLKD框架，包含三大模块：1）目标粒度模块用于学习判别性表示；2）粗粒度模块用来捕获类别间的层级关系；3）蒸馏模块将粗粒度模块获得的知识迁移到目标粒度模块以优化表示。整体为端到端训练。

Result: 在四个数据集上进行了大量实验证明，ReLKD方法在尤其是标注数据有限的场景下，具有更好的新类别分类表现。

Conclusion: ReLKD能有效挖掘类别间隐式关系，并应用于提升新类别发现与分类准确性，适合实际缺乏标注的应用环境。

Abstract: Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.

</details>


### [154] [STRinGS: Selective Text Refinement in Gaussian Splatting](https://arxiv.org/abs/2512.07230)
*Abhinav Raundhal,Gaurav Behera,P J Narayanan,Ravi Kiran Sarvadevabhatla,Makarand Tapaswi*

Main category: cs.CV

TL;DR: 本文提出了一种名为STRinGS的面向文本细致重建的3D高斯渲染增强框架，专门提升3D场景中文本区域的清晰度与可读性。方法在不到7K步迭代时，文本重建效果相较基线提升63.6%。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建技术（如3D Gaussian Splatting，3DGS）难以兼顾视觉质量与文本区域的细节保存，尤其文本信息微小误差就会导致大量语义丧失，实际应用中需要更强的文本保真能力。

Method: STRinGS采用“文本-非文本分离处理”的思想：首先检测和区分场景中的文本与非文本区域，对文本区进行优先和精细的独立优化，然后再和非文本区融合进行全局优化。文中还提出用OCR字符错误率（CER）作为文本可读性指标，并发布了包含多种真实文本场景的STRinGS-360数据集用于评估。

Result: 在7K步训练迭代下，STRinGS在文本重建可读性上比原3DGS方法提升了63.6%，并且能在复杂文本布局下生成锐利、可识别的文字。

Conclusion: STRinGS实现了面向文本区域的3D重建质量跃升，对丰富3D场景语义理解能力、推动文本感知的3D重建工具有重要意义。

Abstract: Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.

</details>


### [155] [Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models](https://arxiv.org/abs/2512.07234)
*Biao Chen,Lin Zuo,Mengmeng Jing,Kunbin He,Yuchen Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的Dropout Prompt Learning方法，通过在视觉-语言模型（Vision-Language Models, VLM）中对文本和视觉分支的token进行自适应的dropout，以提升模型鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 常规dropout主要针对神经元，且未充分利用多模态模型中token的不同重要性。原有方法在视觉-语言模型的泛化、低样本学习和应对分布外（OOD）任务方面依然存在局限。

Method: 设计了一种token级别的dropout策略，根据token在模态内和跨模态对齐中的重要性，分配不同的dropout概率；同时引入残差熵正则化（residual entropy regularization）以在保持知识对齐的同时增强表示多样性。

Result: 在15个基准数据集上实验，方法在低样本学习、长尾分类和分布外泛化等有挑战性的任务中显示出优越效果。其中，在base-to-novel泛化上，相比于KgCoOp提升了5.10%，相比PromptSRC提升了2.13%。

Conclusion: 提出的Dropout Prompt Learning能显著提升视觉-语言模型在多种复杂任务下的鲁棒性和泛化能力，优于多种现有正则化方法。

Abstract: Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.

</details>


### [156] [Unified Camera Positional Encoding for Controlled Video Generation](https://arxiv.org/abs/2512.07237)
*Cheng Zhang,Boying Li,Meng Wei,Yan-Pei Cao,Camilo Cruz Gambardella,Dinh Phung,Jianfei Cai*

Main category: cs.CV

TL;DR: 该论文提出了一种新的相机编码方式UCPE，用于Transformer在3D感知、视频生成等任务中准确表达和控制真实世界相机的几何信息，提升了多种相机参数和失真情况下的适应性和视觉效果。


<details>
  <summary>Details</summary>
Motivation: 现有相机编码大多采用简化的针孔模型，难以覆盖真实世界中各种镜头和内参，多样性有限，难以支持对相机几何严格把控的下游任务（如视频生成、3D建模）。因此亟需一种更加通用且几何一致的相机编码方法。

Method: 提出了Relative Ray Encoding，将姿态、内参及镜头畸变等完整相机信息统一编码，并构建了UCPE（Unified Camera Positional Encoding）。在视频生成任务中通过轻量级空间注意力适配器集成于预训练Diffusion Transformer，仅增加不到1%参数量。同时，建立了涵盖多种相机运动和镜头类型的大型视频数据集，便于系统训练与评估。

Result: 在相机可控的视频生成任务上，集成UCPE后实现了业界最先进的相机可控性和视觉逼真性。实验全面验证了该编码方法的有效性和泛化性。

Conclusion: UCPE可作为Transformer结构未来在多视角、视频及三维任务中的通用相机表示方法，极大提升了模型对真实相机复杂几何的表达与控制能力。

Abstract: Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.

</details>


### [157] [Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture](https://arxiv.org/abs/2512.07241)
*Md. Srabon Chowdhury,Syeda Fahmida Tanzim,Sheekar Banerjee,Ishtiak Al Mamoon,AKM Muzahidul Islam*

Main category: cs.CV

TL;DR: 本文提出了一种结合SqueezeNet v1和EfficientNet-B0的混合深度学习模型，并融合了多种手工放射组学特征，用于脑肿瘤MRI影像分类。模型在公开数据集上测试取得98.93%准确率，并在TTA增强下提升至99.08%。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤诊断需准确且及时，现有MRI人工分割过程繁琐、易出错。为提升分割效率和准确性，亟需高效、自动化的辅助诊断方法。

Method: 该研究基于轻量的SqueezeNet v1和性能优秀的EfficientNet-B0构建混合网络，并引入HOG、LBP、Gabor、小波等4种手工提取放射组学特征。使用公开Nickparvar脑肿瘤MRI数据集，对四类脑肿瘤及无肿瘤图像进行分类。

Result: 模型在测试集上准确率为98.93%，加入测试时增强（TTA）后提升至99.08%。仅需210万参数和1.2 GFLOPs，兼顾效率与精度。手工特征提升了纹理敏感性，EfficientNet-B0提高了层级表达力。

Conclusion: 所提混合网络结构在脑肿瘤MRI自动分类任务中接近临床可靠性，有望用于临床决策支持系统，兼顾计算资源消耗和诊断准确率。

Abstract: Brain tumors are one of the most common and dangerous neurological diseases which require a timely and correct diagnosis to provide the right treatment procedures. Even with the promotion of magnetic resonance imaging (MRI), the process of tumor delineation is difficult and time-consuming, which is prone to inter-observer error. In order to overcome these limitations, this work proposes a hybrid deep learning model based on SqueezeNet v1 which is a lightweight model, and EfficientNet-B0, which is a high-performing model, and is enhanced with handcrafted radiomic descriptors, including Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gabor filters and Wavelet transforms. The framework was trained and tested only on publicly available Nickparvar Brain Tumor MRI dataset, which consisted of 7,023 contrast-enhanced T1-weighted axial MRI slices which were categorized into four groups: glioma, meningioma, pituitary tumor, and no tumor. The testing accuracy of the model was 98.93% that reached a level of 99.08% with Test Time Augmentation (TTA) showing great generalization and power. The proposed hybrid network offers a compromise between computation efficiency and diagnostic accuracy compared to current deep learning structures and only has to be trained using fewer than 2.1 million parameters and less than 1.2 GFLOPs. The handcrafted feature addition allowed greater sensitivity in texture and the EfficientNet-B0 backbone represented intricate hierarchical features. The resulting model has almost clinical reliability in automated MRI-based classification of tumors highlighting its possibility of use in clinical decision-support systems.

</details>


### [158] [Zero-Shot Textual Explanations via Translating Decision-Critical Features](https://arxiv.org/abs/2512.07245)
*Toshinori Yamauchi,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.CV

TL;DR: 本文提出了TEXTER方法，通过识别对分类决策关键的神经元特征并将其与文本特征对齐，生成更具解释性的图像分类模型文本解释。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉-语言模型通常生成一般性图像描述，难以反映分类器具体的决策逻辑；而零样本解释方法多基于全局特征对齐，并未强调模型实际用来决策的关键特征。作者希望解决这一透明性和“忠实性”不足的问题，使解释更加贴近模型推理机制。

Method: TEXTER首先定位对分类预测起关键作用的神经元，强调这些神经元编码的决策相关特征。随后将被强调的特征投影到CLIP的特征空间，从中检索能够反映模型推理过程的文本解释。为提升可解释性，尤其在Transformer架构下，引入了稀疏自编码器。

Result: 大量实验证明，TEXTER方法生成的文本解释比现有方法更加忠实于模型的真实推理行为，也更易于理解。

Conclusion: TEXTER解决了视觉模型解释无法反映决策依据的问题，实现了更具可解释性和忠实性的文本解释。相关代码也将对外公开，有助于进一步的研究和应用。

Abstract: Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.

</details>


### [159] [AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing](https://arxiv.org/abs/2512.07247)
*Ziming Hong,Tianyu Huang,Runnan Chen,Shanshan Ye,Mingming Gong,Bo Han,Tongliang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种保护3D高斯喷溅（3DGS）资产不被恶意或未经授权编辑的新方法AdLift，通过提升受限的二维对抗扰动，实现了对三维内容的防护，兼顾视角泛化与可见性，对抗现有2D和3D编辑攻击效果显著。


<details>
  <summary>Details</summary>
Motivation: 随着基于扩散模型的指令驱动编辑技术扩展到3DGS，虽然造福了3D内容创作，但也带来了被篡改与滥用的风险。已有的2D对抗扰动技术难以直接应用于3DGS，主要因为视角泛化难和扰动不可见性与保护能力难以兼顾。因此，迫切需要一种既能广泛保护不同视角又难以察觉的新防护机制。

Method: 提出AdLift方法，首先将二维对抗扰动提升为嵌入3D高斯球中的保护结构；采用定制的Lifted PGD优化策略，交替进行梯度截断和image-to-Gaussian拟合，以在多训练视角下优化扰动，实现对不同视角甚至新视角的一致防护，同时保证扰动对人类视觉不可见。

Result: 实验结果显示AdLift在防御最新2D图像及3DGS编辑系统中均有效，能显著降低未经授权编辑的成功率，同时不会影响原有3D资产质量，定性、定量指标均优异。

Conclusion: AdLift是针对3DGS资产首个可行、有效的编辑保护机制，可广泛适配多视角，兼顾不可察觉性与较强防护，是3D内容保护领域的重要进展。

Abstract: Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.

</details>


### [160] [See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement](https://arxiv.org/abs/2512.07251)
*Junqi Liu,Zejun Wu,Pedro R. A. S. Bassi,Xinze Zhou,Wenxuan Li,Ibrahim E. Hamamci,Sezgin Er,Tianyu Lin,Yi Luo,Szymon Płotka,Bjoern Menze,Daguang Xu,Kai Ding,Kang Wang,Yang Yang,Yucheng Tang,Alan L. Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一种解剖感知的扩散模型SMILE, 仅增强医学图像中临床相关区域，防止过度编辑和假阳性，提高图像质量和临床诊断价值。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像增强方法易造成器官变形、产生虚假信息、遗漏微小肿瘤, 因为这些方法缺乏对解剖结构和对比剂动力学的理解, 影响临床诊断。

Method: 提出SMILE模型，采用结构感知监督，跟踪真实器官边界和对比增强模式；采用无需配准的学习，直接处理未对齐的多期CT扫描；采用统一推断，实现各期一致且快速的图像增强。

Result: 在6个外部数据集上，SMILE方法在图像质量指标（SSIM提升14.2%、PSNR提升20.6%、FID提升50%）和临床有用性上均优于现有方法。对于非增强CT的癌症检测，F1分数提升高达10%。

Conclusion: SMILE能生成解剖结构准确、具诊断意义的图像，提升医学影像诊断与肿瘤检出效果，具有较高的临床应用价值。

Abstract: Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.

</details>


### [161] [DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement](https://arxiv.org/abs/2512.07253)
*Handing Xu,Zhenguo Nie,Tairan Peng,Huimin Pan,Xin-Jun Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种适用于内窥镜手术视频的实时增强方法，通过传播降质（图像退化）表征，实现图像质量提升，兼顾效率与效果。


<details>
  <summary>Details</summary>
Motivation: 内窥镜手术严重依赖视频图像质量，但现实中常因照明不均、组织散射、遮挡和运动模糊等问题，导致关键解剖细节模糊，影响手术安全和操作。现有深度学习增强方法虽然有效，但大多计算量过大，不适合手术实时场景。

Method: 作者提出了一种降质感知的内镜视频增强框架。首先利用对比学习从图像中提取降质表征。随后通过特定特征融合机制，将降质表征与图像特征融合，引导单帧增强网络进行增强。该模型通过退化-恢复流程的循环一致性约束训练，以提升鲁棒性和泛化能力。

Result: 实验结果显示，该方法在性能和效率之间取得了优良平衡，并优于多种先进方法，能够实现实时高质量的内镜视频增强。

Conclusion: 降质感知建模可有效提升内镜视频的实时增强质量。隐式学习与传播降质表征，为临床实际应用提供了可行路径。

Abstract: Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.

</details>


### [162] [A graph generation pipeline for critical infrastructures based on heuristics, images and depth data](https://arxiv.org/abs/2512.07269)
*Mike Diessner,Yannick Tarant*

Main category: cs.CV

TL;DR: 该论文提出了一种基于影像测量的图生成流程，通过RGB和深度数据来替代高成本的激光雷达点云，实现对关键基础设施3D建模。实验显示效果接近真实，为数字孪生系统提供低成本方案。


<details>
  <summary>Details</summary>
Motivation: 传统的3D激光雷达获取和建模方法费用高、技术门槛高，不适用于广泛部署。为降低成本，并提升关键基础设施数字孪生的普适性，有必要探索更经济、便捷的模型构建方案。

Method: 本文提出一个基于摄影测量的图生成流程，利用立体相机采集RGB及深度图像，通过深度学习方法进行对象检测和实例分割，再结合用户自定义规则推理对象关系，自动生成基础设施的关联图。

Result: 在两个水利系统上的实验显示，该方法生成的图与真实情况高度接近，具有定制灵活性，可根据不同场景调整，结果透明可解释。

Conclusion: 该方法为物理关键基础设施建模提供了一种低成本、高灵活性、透明可控的替代方案，有利于支持关键系统的决策和运维。

Abstract: Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.

</details>


### [163] [RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2512.07273)
*Zhi Rao,Yucheng Zhou,Benjia Zhou,Yiqing Huang,Sergio Escalera,Jun Wan*

Main category: cs.CV

TL;DR: 作者提出了一个三阶段的视觉-语言强化框架（RVLF），有效提升了无gloss标注手语翻译（SLT）的表现，解决了视觉表达不足和语义对齐问题，通过引入GRPO优化显著提高了多数据集上的BLEU-4指标。


<details>
  <summary>Details</summary>
Motivation: 当前无gloss手语翻译面临两个核心挑战：1）手语表征不足，难以捕捉丰富的视觉细节，2）现有大模型方法存在句级语义对齐偏差。这些问题导致翻译质量受限，亟需改进。

Method: 提出三阶段框架（RVLF）：首先，通过融合骨架运动特征与DINOv2视觉特征，结合instruction tuning，获得强基线模型（SLT-SFT）；然后，引入基于GRPO的优化策略，利用BLEU和ROUGE作为奖励函数，进行强化学习微调，最终得到优化后的SLT-GRPO模型。

Result: 在CSL-Daily、PHOENIX-2014T、How2Sign和OpenASL四个手语数据集上，BLEU-4指标分别提升+5.1、+1.11、+1.4、+1.61，均未借助外部大规模手语预训练数据。

Conclusion: 首次将GRPO引入无gloss手语翻译，实验和消融验证证明该优化策略能有效提升翻译质量和语义一致性，为无gloss SLT提供新的高效解决方案。

Abstract: Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.

</details>


### [164] [Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation](https://arxiv.org/abs/2512.07275)
*Siyu Wang,Hua Wang,Huiyu Li,Fan Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种创新性的多尺度残差结构编码器-解码器网络，用于提高皮肤病变分割的准确性和鲁棒性，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习皮肤病变分割方法在处理病变形状不规则和对比度低时效果有限。为了实现更精确的分割，亟需新型网络结构和特征提取机制。

Method: 提出了基于多尺度残差结构的编码器-解码器网络。引入了多分辨多通道融合（MRCF）模块以捕捉跨尺度特征，引入了交叉混合注意力模块（CMAM）动态计算多上下文权重，并使用外部注意力桥（EAB）优化解码器过程，减少信息损失。

Result: 在多个皮肤病变分割数据集上进行了充分实验，结果显示该方法在分割准确性和鲁棒性上均显著优于现有的Transformer和CNN模型。

Conclusion: 本方法通过独特的多尺度融合和注意力机制有效提升了病变分割效果，为皮肤病变自动识别任务提供了先进的解决方案。

Abstract: In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.

</details>


### [165] [Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery](https://arxiv.org/abs/2512.07276)
*Mai Tsujimoto,Junjue Wang,Weihao Xuan,Naoto Yokoya*

Main category: cs.CV

TL;DR: 提出了Geo3DVQA基准，用于评估视觉-语言模型（VLM）在仅利用RGB遥感影像进行3D地理空间推理的能力。结果显示当前VLM在该任务上表现有限，表明该领域面临重大挑战。


<details>
  <summary>Details</summary>
Motivation: 当前3D地理空间分析高度依赖昂贵的传感器(如LiDAR、多光谱)，限制了全球普及；同时，现有方法在整合多种3D线索、应对多样化查询及推理解释性上存在不足。因此，研究者希望推动RGB图像基础上的3D空间推理能力提升，使分析方法更具可访问性与可扩展性。

Method: 作者构建了一个大规模基准数据集Geo3DVQA，基于真实场景的RGB遥感图像，整合海拔、高空视野因子与地表覆盖等要素，设计了16类任务、三个难度层次（单特征推断、多特征推理、应用级空间分析），共11万对问答样本，用以全面评估VLM在3D空间推理的表现。同时，对10个主流VLM进行实验，包括基础模型和经过地理领域微调的模型。

Result: 主流VLM在仅用RGB图像进行3D推理时准确率很低，如GPT-4o 28.6%，Gemini-2.5-Flash 33.0%；但Qwen2.5-VL-7B经过地理领域微调后提升至49.6%（提升了24.8个百分点），显示领域适配的有效性。

Conclusion: 现有通用VLM在3D地理空间推理准确率较低，暴露出从RGB到3D推理的局限性，需要进一步研究领域适配与数据集改进。Geo3DVQA为可扩展、可访问、全面的3D地理空间分析开启了新方向。

Abstract: Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.

</details>


### [166] [Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts](https://arxiv.org/abs/2512.07302)
*Mingning Guo,Mengwei Wu,Shaoxian Li,Haifeng Li,Chao Tao*

Main category: cs.CV

TL;DR: 本文提出了AerialVP——一种针对无人机(UAV)图像感知任务的任务提示增强代理框架，通过从UAV图像中主动提取多维辅助信息提升现有视觉语言模型(VLM)的感知能力，并构建了权威评测基准AerialSense，实验验证了AerialVP对现有VLM性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的图像感知方法，在处理无人机图像中复杂的目标混淆、尺度变化和背景复杂等问题时，因任务提示简单、图像内容复杂，导致语义对齐偏弱，难以充分理解和聚焦任务相关信息，限制了VLM的感知效果。

Method: 提出AerialVP框架，分三步实现任务提示增强：1）分析任务提示，识别任务类型与增强需求；2）从工具库选择合适工具；3）结合分析和工具生成增强后的任务提示。此外，构建了AerialSense评测基准，包含视觉推理、问答和定位任务，标准化多种场景评估。

Result: AerialVP能够极大提升VLM对任务相关信息的聚焦能力，在多种分辨率、光照和场景条件下的公开及闭源VLM上稳定、显著提升了感知任务表现。

Conclusion: AerialVP突破了传统方法在无人机图像感知中的瓶颈，并为多种VLM带来了通用且显著的效果提升。AerialSense基准为模型评价提供了规范的平台，有助于推动无人机图像理解领域发展。

Abstract: Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.

</details>


### [167] [Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset](https://arxiv.org/abs/2512.07305)
*Tobias Abraham Haider*

Main category: cs.CV

TL;DR: 本文复现了Carl等人关于用预训练Inception-ResNet-v2模型自动检测欧洲野生哺乳动物的研究，并用不同数据集进行检验，验证了该方法具有一定可复现性，但在不同类别泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 评估和验证已有深度学习模型（如Inception-ResNet-v2）在动物物种识别中的可复现性和泛化能力，确认其是否适合于新的、未见过的数据集，以及原论文结果能否复现。

Method: 作者使用公开资源和一个包含90种物种共900张图片的新数据集，从零实现并复现实验流程，仅做最小的数据预处理。模型直接基于ImageNet预训练权重，随后计算总体分类准确率和宏平均F1分数。

Result: 模型在新数据集上的总体分类准确率为62%，与原文71%结果接近；每个类别表现差异很大，宏F1分数为0.28，显示了与ImageNet标签不完全契合时的泛化局限性。

Conclusion: 预训练卷积神经网络可作为动物物种识别的实用基线，但要获得稳定高质量的预测，还需要针对特定物种做适应性调整或迁移学习。

Abstract: This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.

</details>


### [168] [ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation](https://arxiv.org/abs/2512.07328)
*Ziyang Mai,Yu-Wing Tai*

Main category: cs.CV

TL;DR: 该论文提出ContextAnyone方法，实现了在文本到视频生成任务中角色一致性的提升，尤其能保持角色外貌、发型、服装等整体特征，在多种场景下生成高一致性、高质量的视频。


<details>
  <summary>Details</summary>
Motivation: 现有的T2V生成方法难以保证同一角色在不同视频帧中的外观一致，尤其在发型、服装、体型等上下文特征保持上存在不足，影响了视频的视觉连贯性。因此作者提出改进方法以突破这一瓶颈。

Method: 提出ContextAnyone：1）同时复原参考图像并生成视频帧，使模型充分利用参考信息；2）在DiT扩散模型骨干中，引入Emphasize-Attention模块，有效强化与参考图像相关的特征，防止身份漂移；3）采用双重引导损失，将扩散目标与参考重建目标结合，提升角色外观的还原度；4）设计Gap-RoPE位置嵌入，分离参考与视频token，保证时序建模的稳定性。

Result: 实验证明ContextAnyone在角色一致性和视觉质量上均超越了现有参照图像到视频的方法，能在多种动作和场景下生成连贯且保持角色特征的视频片段。

Conclusion: ContextAnyone有效解决了T2V生成中角色一致性的问题，为角色驱动的视频生成提供了更优解，实现了更高水平的上下文特征保持和视觉连贯性。

Abstract: Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.

</details>


### [169] [The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers](https://arxiv.org/abs/2512.07331)
*Kanishk Awadhiya*

Main category: cs.CV

TL;DR: 本文探讨了Vision Transformer（ViT）中出现的“U型”熵信息瓶颈现象，并证明其源于数据自适应而非模型结构本身。


<details>
  <summary>Details</summary>
Motivation: 尽管ViT理论上能维持高维表达，但实际观察中其中间层存在信息压缩现象。研究动机是理解这一现象背后的成因及其与任务语义复杂度的关系。

Method: 通过分析不同数据集（如UC Merced、Tiny ImageNet、CIFAR-100）下DINO训练的ViT的逐层有效编码维度（EED），研究瓶颈深度与任务语义抽象的相关性。

Result: 实验证明瓶颈深度与任务所需的语义抽象相关。纹理为主的数据集在各层维持高秩表达，而以物体为中心的数据集中间层信息收缩，ViT自适应形成信息瓶颈。

Conclusion: ViT中的信息瓶颈来自于数据的自适应需求，是模型“主动学习”分离语义特征的表现，而非结构本身固有缺陷。

Abstract: Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a "U-shaped" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this "Inductive Bottleneck" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively "learning" a bottleneck to isolate semantic features.

</details>


### [170] [Generalized Referring Expression Segmentation on Aerial Photos](https://arxiv.org/abs/2512.07338)
*Luís Marnoto,Alexandre Bernardino,Bruno Martins*

Main category: cs.CV

TL;DR: 本文提出了Aerial-D，大规模航空影像指代表达分割数据集，并通过自动化流水线生成多样丰富的语言表达，促进文本驱动的实例及语义分割研究，尤其针对历史与现代航空图像的多样场景。


<details>
  <summary>Details</summary>
Motivation: 航空影像在空间分辨率、色彩表现、目标尺寸及遮挡等方面具有高度复杂性，现有指代表达分割领域的数据资源极匮乏，缺乏覆盖多样实际场景的高质量数据集，影响相关研究应用进展。

Method: 构建新数据集Aerial-D，囊括37288张图片、1522523条指代表达、259709个目标，涵盖21类，数据通过规则生成结合大语言模型提升表达多样性和细节性，并用过滤技术模拟历史影像。同时，基于RSRefSeg架构将新数据与已有数据联合训练，实现了文本驱动的实例和语义分割。

Result: 联合训练模型在当前各类基准上表现出竞争力，且对单色、棕褐色、颗粒化等历史影像降质条件下仍保持较高准确率。

Conclusion: Aerial-D数据集及衍生模型适用于现代与历史航空影像，促进该领域文本指代分割任务发展，相关资源已公开，有望成为航空图像理解领域的重要基石。

Abstract: Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .

</details>


### [171] [Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting](https://arxiv.org/abs/2512.07345)
*Shilong Jin,Haoran Duan,Litao Hua,Wentao Huang,Yuan Zhou*

Main category: cs.CV

TL;DR: 本论文提出了一种新的框架TD-Attn，用于解决文本到图像扩散模型在3D任务中多视角表现不一致的问题，有效提升了多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 当前基于Text-to-Image (T2I)扩散模型的3D生成与编辑方法无需大量3D训练数据，但存在“先验视角偏置”——即模型对于不同视角下同一对象容易产生矛盾外观，严重影响多视角一致性。迫切需求是找到问题根源并有效解决。

Method: 作者通过数学分析揭示T2I模型中先验视角偏置的根本原因，并发现UNet不同层对视角偏置影响不同。为此，提出TD-Attn框架，包含：（1）3D感知注意力引导模块（3D-AAG），构造对主词空间一致的3D注意力高斯分布，增强空间一致性；（2）层次化注意力调制模块（HAM），利用语义引导树（SGT）和语义响应分析器（SRP）定位并调节受视角影响的注意力层，实现语义可控与精准3D编辑。两模块相辅相成，提升整体一致性。

Result: 大量实验结果表明，TD-Attn大幅度提升了T2I扩散模型在多视角3D任务中的一致性表现，并能作为通用插件应用于多种3D任务。

Conclusion: TD-Attn有效解决了T2I模型在3D任务中的多视角不一致问题，为未来多视角3D生成与编辑任务提供了可推广、强力的基础工具。

Abstract: Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.

</details>


### [172] [MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition](https://arxiv.org/abs/2512.07348)
*Xinyu Wei,Kangrui Cen,Hongyang Wei,Zhen Guo,Bairui Li,Zeqing Wang,Jinrui Zhang,Lei Zhang*

Main category: cs.CV

TL;DR: 本文针对多图合成（Multi-Image Composition, MICo）任务，创建了大规模高质量数据集MICo-150K，提出了评测基准MICo-Bench及新的评测指标，并展示了基于此数据集微调模型的显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前可控图像生成领域中多参考图像合成的一致性和连贯性难题，受到高质量训练数据匮乏的限制，缺乏系统的数据支持与评测手段，阻碍了相关模型与算法的发展。

Method: 1）将MICo细分为7类代表性任务，系统性整理任务类型；2）收集大规模高质量源图像并合成多样化合成提示语，运用强大生成模型合成复合图像，并结合人工筛选精修，构建MICo-150K数据集；3）设计Decomposition-and-Recomposition子集，支持真实与合成混合评测；4）构建涵盖多任务的评测集MICo-Bench与新指标Weighted-Ref-VIEScore；5）用MICo-150K对多种模型进行微调，并在MICo-Bench上系统评测。

Result: 实验表明MICo-150K可显著提升基础模型的多图合成能力，不具备该能力的模型亦可通过该数据集获得新技能。基线模型Qwen-MICo不仅在三图合成上与强基线Qwen-Image-2509表现持平，还突破支持更多输入图像的限制。

Conclusion: 本论文提供的MICo-150K数据集、MICo-Bench评测基准与Qwen-MICo基线模型，为多图合成领域研究提供了坚实资源基础和评测工具，有望促进该方向的进一步发展与创新。

Abstract: In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.

</details>


### [173] [DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection](https://arxiv.org/abs/2512.07351)
*Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Reem E. Mohamed,Md Rafiqul Islam,Asif Karim,Sami Azam*

Main category: cs.CV

TL;DR: 本文提出了一个多智能体框架（DeepAgent），通过视觉和音频两种模态协同检测深度伪造（deepfake），并经过多组数据集实验证明了其高效性及鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前许多深度伪造检测方法将音频和视觉特征整合到单一模型中，导致对模态失配、噪声和操控更为脆弱。因此，亟需一种更能利用两种模态优势、提升检测鲁棒性的方法。

Method: 作者提出了DeepAgent，包括两种互补的智能体：Agent-1用基于AlexNet的CNN检测视觉伪造特征，Agent-2利用音频特征、音频转写（Whisper）及视频帧OCR（EasyOCR）检测音视频不一致。两者的判断通过随机森林融合，以提升最终检测性能。

Result: 在Celeb-DF和FakeAVCeleb混合测试集上，Agent-1达到了94.35%的准确率。Agent-2和最终元分类器在FakeAVCeleb上准确率分别为93.69%和81.56%。跨数据集验证（DeepFakeTIMIT）中，元分类器准确率达97.49%，显示良好泛化能力。

Conclusion: 分层融合方式提升了鲁棒性，有效缓解了单一模态局限。多智能体协作方法能更好应对多样化的深度伪造手段，是对当前deepfake检测手段的有力补充。

Abstract: The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.

</details>


### [174] [Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2512.07360)
*Qiming Huang,Hao Ai,Jianbo Jiao*

Main category: cs.CV

TL;DR: 该论文提出了一种基于结构感知特征校正的开放词汇语义分割方法，通过引入图片实例特有的结构先验，显著提升了局部区域的分割一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP等视觉语言模型在开放词汇语义分割任务中因为关注全局语义对齐，导致在细粒度区域语义关联上的性能不足，产生了较多噪声和不一致分割结果。作者认为其根源在于CLIP的对比训练方式导致的分散偏置，并且单靠CLIP特征难以解决。

Method: 作者提出了一种结构感知的特征校正方法。具体做法是，从图像的底层特征（如颜色、纹理）出发，构建区域邻接图（RAG），以捕捉局部结构关系。然后利用该结构对CLIP特征进行细化，增强局部判别能力，实现特征校正。

Result: 大量实验表明，该方法能够有效抑制分割过程中的噪声，提高区域级别的一致性，在多个开放词汇语义分割基准上取得了良好表现。

Conclusion: 通过引入结构先验的特征校正机制，可以弥补现有开放词汇分割模型在局部识别上的不足，提升了分割性能。

Abstract: Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.

</details>


### [175] [Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency](https://arxiv.org/abs/2512.07379)
*Mahila Moghadami,Mohammad Ali Keyvanrad,Melika Sabaghian*

Main category: cs.CV

TL;DR: 本文提出并改进了适用于大规模航拍图像中小目标检测的方法，通过优化裁剪和模型结构设计，有效提升了检测的速度与准确性，在VisDrone2019数据集上的mAP准确率由35.5提高到61.2，超过了业界主流方法。


<details>
  <summary>Details</summary>
Motivation: 随着航拍图像在关键和工业应用中的重要性不断提升，现有的小目标检测方法难以兼顾速度、准确性和处理大图像的能力，因此，需要一种更高效、准确的小目标检测框架。

Method: 基于SW-YOLO方法，改进了滑动窗口裁剪尺寸和重叠度，同时对主干网络进行了结构升级，包括在主干中集成CBAM注意力机制，增加高效的特征提取模块，并设计了新的检测头，以提升小目标检测性能。

Result: 本文提出的改进模型在VisDrone2019数据集上实现了mAP从35.5（YOLOv5L）提升到61.2，优于SAHI和CZDet等主流方法（其中CZDet为58.36），展现了显著的精度优势。

Conclusion: 所提出的小目标检测方法不仅大幅提升了精度，也兼顾了对大规模航拍图像的高效处理能力，为实际应用中的小目标检测需求提供了更强有力的技术支持。

Abstract: This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.

</details>


### [176] [Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects](https://arxiv.org/abs/2512.07381)
*Shuohan Tao,Boyao Zhou,Hanzhang Tu,Yuwang Wang,Yebin Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于网格面的结构化二维高斯溅射(Tessellation GS)方法，用于从单个连续移动或静止相机重建动态场景，显著提升了稀疏视角和动态场景下的泛化能力，并在重建质量上大幅超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统3D高斯溅射虽然能实现高真实感场景重建，但由于各向异性特性，在视角外推（extrapolation）时容易过拟合，导致稀疏视图与动态场景重建效果差，特别是单相机动态重建更具挑战性。因此，亟需提升该技术的泛化能力。

Method: 作者提出了Tessellation GS，将2D高斯约束在网格面的局部区域，并通过分层神经特征推断其属性，引入细节感知损失引导自适应面划分以细致建模；并利用重建基础模型的先验初始化高斯变形，增强了对一般动态对象的鲁棒重建能力。

Result: 实验表明该方法在外观和网格重建任务上，LPIPS指标降低了29.1%，Chamfer距离降低了49.2%，显著优于现有最先进方法。

Conclusion: Tessellation GS在单相机动态场景重建尤其具备明显优势，提升了高斯溅射在稀疏、动态、单视角等极具挑战性场景下的泛化性和重建质量，为实际应用提供了更强的支持。

Abstract: 3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.

</details>


### [177] [LogicCBMs: Logic-Enhanced Concept-Based Learning](https://arxiv.org/abs/2512.07383)
*Deepika SN Vemuri,Gautham Bellamkonda,Aditya Pola,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: 本文提出通过引入可微分逻辑模块增强概念瓶颈模型（CBM），使其不仅仅依赖线性组合来理解和利用语义概念，从而提升模型的表达能力和解释性。


<details>
  <summary>Details</summary>
Motivation: 传统CBMs主要通过线性组合语义概念来提供可解释性，但这种方式表达能力有限，难以捕捉复杂的概念之间关系。

Method: 作者设计了一个逻辑模块，将CBM学习到的概念通过可微分的逻辑运算符进行组合，实现了逻辑关系层面的建模。新模型（LogicCBM）能够端到端地学习并作出推断。

Result: 实验结果显示，LogicCBM在多个公开基准和合成数据集上获得了更好的准确率、能进行有效的干预，并且具有较高的可解释性。

Conclusion: 通过将逻辑运算融入CBM，模型增强了对概念间逻辑关系的建模能力和整体性能，同时仍然保持了高度可解释性。

Abstract: Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.

</details>


### [178] [How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline](https://arxiv.org/abs/2512.07385)
*Chunhui Zhang,Li Liu,Zhipeng Zhang,Yong Wang,Hao Wen,Xi Zhou,Shiming Ge,Yanfeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一项新的反无人机多模态视觉跟踪任务UAV-Anti-UAV，收集并标注了大规模数据集，同时设计了一个基于Mamba的跟踪基线方法，并通过实验验证了方法的有效性和该领域改进的空间。


<details>
  <summary>Details</summary>
Motivation: 随着无人机应用的广泛化，相应带来了安全和隐私风险，而当前反无人机研究主要专注于固定地面摄像头获取的视频，对来自移动平台的无人机间跟踪关注不足，因此亟需针对无人机追踪无人机的视觉跟踪任务和数据集。

Method: 作者提出了UAV-Anti-UAV跟踪任务，通过动捕无人机追踪目标无人机，收集并手动标注了包含1810个视频的多模态大规模数据集，包括边界框、语言提示及15类跟踪属性。方法上提出了MambaSTS基线模型，结合Mamba和Transformer实现全局语义与空间特征学习，利用长序列建模能力实现视频级长期上下文信息的传播。

Result: 通过在UAV-Anti-UAV数据集上的实验，验证了MambaSTS方法的有效性，并评测了当前50种主流深度跟踪算法，发现它们在本任务上表现均有限，显示该领域仍有较大提升空间。

Conclusion: 本研究首次提出无人机追踪对抗无人机的新任务并构建了大规模高质量数据集，通过创新基线方法验证了该任务的挑战性和现有方法的不足，为相关领域的进一步研究提供了重要资源和基准。

Abstract: Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.

</details>


### [179] [GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring](https://arxiv.org/abs/2512.07391)
*Đorđe Nedeljković*

Main category: cs.CV

TL;DR: GlimmerNet 是一种极轻量级 CNN 网络，通过新颖的卷积和特征融合结构，实现了更高效的全局感知。它在无人机场景下表现出极佳的精度和效率，适合边缘和移动设备的实时任务。


<details>
  <summary>Details</summary>
Motivation: 现有增强全局感知能力的方法（如 Vision Transformer）虽然有效，但计算开销大，不适合资源受限的平台。本文旨在提出一种无需高昂计算成本、仍能具备全局感知能力的卷积网络，以满足移动端和无人机应用的需求。

Method: 提出 GlimmerNet，利用分组膨胀深度可分卷积（GDBlocks）实现多尺度特征提取，并用创新的聚合器模块（Aggregator）通过分组逐点卷积高效融合特征，显著减少参数量和计算量。

Result: GlimmerNet 仅含 3.1 万参数，FLOPs 比最新基线低 29%，在 AIDERv2 数据集上达到 0.966 的有权 F1 分数，达到新的性能与效率平衡。

Conclusion: GlimmerNet 在超轻量级与高效准确性间取得突破，特别适用于资源受限的无人机应急监控等实时视觉任务，推动了该领域的发展。

Abstract: Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.

</details>


### [180] [Reconstructing Objects along Hand Interaction Timelines in Egocentric Video](https://arxiv.org/abs/2512.07394)
*Zhifan Zhu,Siddhant Bansal,Shashank Tripathi,Dima Damen*

Main category: cs.CV

TL;DR: 本文提出了ROHIT任务，即在手与物体交互时间轴上重建物体姿态，并引入了一种新的受约束优化与传播（COP）框架，在无3D真值的场景下显著提升了物体重建效果。


<details>
  <summary>Details</summary>
Motivation: 场景中的物体与手的交互动态复杂，现有重建方法通常缺乏对物体被手稳定持握期间姿态传播约束的建模。为提升在无3D标注情况下的物体重建精度，亟需开发更合理的模型与任务设定。

Method: 作者提出了“手交互时间线”（HIT）概念，按照物体从静止—接触—稳定持握—释放的时序，系统建模物体姿态变化。进一步设计了受约束优化与传播（COP）框架，基于此约束沿时间传播物体姿态提升重建效果。方法在2个头戴视角数据集（HOT3D与EPIC-Kitchens）上验证，并利用2D投影误差进行评价。

Result: 在HOT3D和EPIC-Kitchens数据集上，COP方法分别让稳定持握重建精度提升了6.2%-11.3%，沿HIT传播则提升高达24.5%。

Conclusion: 通过引入手交互时间线建模和COP框架，显著提升了无3D真值情况下的视频物体重建性能，验证了建模持握时序约束的有效性。

Abstract: We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.

</details>


### [181] [InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs](https://arxiv.org/abs/2512.07410)
*Bin Li,Ruichi Zhang,Han Liang,Jingyan Zhang,Juze Zhang,Xin Chen,Lan Xu,Jingyi Yu,Jingya Wang*

Main category: cs.CV

TL;DR: 该论文提出了InterAgent，一个首个面向多主体仿人机器人、支持文本驱动、结合物理模拟的端到端控制框架，实现了多机器人在物理世界中根据文本指令自协调动作生成，且效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前大多数仿人机器人控制方法仅适用于单体场景，忽略了多主体间本应存在的物理和行为交互，导致无法还原人类真实的群体社会行为。需要新的方法实现多主体复杂协调和互动。

Method: 作者提出了一个自回归扩散Transformer，具备多通道结构，分别处理本体感知（proprioception）、外部感知（exteroception）和动作，减少模态干扰并促进协同。同时设计了交互图的外部感知表示方法，显式捕捉个体之间精细的空间依赖；再用稀疏边注意力机制动态筛选重要空间联系，提升网络学习与建模多机器人互动的稳健性。

Result: 大量实验表明，该方法在文本驱动的多主体仿人机器人任务中超越了多个已知强基线，刷新了最优表现，可有效生成连贯、物理可行与语义一致的多主体动作。

Conclusion: InterAgent有效补足了多主体仿人机器人文本驱动控制的空白，为相关领域提供了强有力的基础模型和工具，未来代码和数据将在社区开放，推动后续研究。

Abstract: Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.

</details>


### [182] [Data-driven Exploration of Mobility Interaction Patterns](https://arxiv.org/abs/2512.07415)
*Gabriele Galatolo,Mirco Nanni*

Main category: cs.CV

TL;DR: 本文提出了一种基于数据挖掘的方法，通过分析实际运动数据，发现和建模人群中个体之间的相互影响及行为模式，用于提升如人群仿真和应急管理中的人类动力学建模。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖预设的行为模型，忽略了从实际数据中学习人群动力学规律。为更真实地模拟个体间相互作用，需提出从数据出发的新方法。

Method: 作者提出一种基于数据挖掘的方法，直接从运动记录中搜寻潜在的个体间相互作用事件，再进一步识别复杂、持续和随时间演化的交互模式。该方法在车辆和行人两类实际案例中进行了实验应用。

Result: 方法在两个真实案例（车辆和行人）上做了充分实验，评估了性能、参数敏感性并对部分结果进行了具体解释。实验验证了该方法的有效性和可解释性。

Conclusion: 基于数据驱动的分析能够揭示人与人之间移动交互的机制，有助于改进现有的人群仿真模型，为相关领域（如紧急管理）提供更科学的建模工具。

Abstract: Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.

</details>


### [183] [When normalization hallucinates: unseen risks in AI-powered whole slide image processing](https://arxiv.org/abs/2512.07426)
*Karel Moens,Matthew B. Blaschko,Tinne Tuytelaars,Bart Diricx,Jonas De Vylder,Mustafa Yousif*

Main category: cs.CV

TL;DR: 本文关注全切片图像（WSI）归一化过程中，现有基于深度学习的方法常常会生成肉眼难以察觉但严重影响后续分析的虚构内容（假象），并提出了新的检测该现象的方法，揭示了主流方法的不足。


<details>
  <summary>Details</summary>
Motivation: WSI归一化对于数字病理分析至关重要，但深度学习方法可能会生成假象内容，危及后续诊断。以往相关风险及其评估手段未被充分重视，亟需有效检测措施。

Method: 作者开发了一种新颖的图像比较指标，可自动检测归一化过程中的假象，并用该方法系统评估多种主流归一化方法在真实临床数据上的表现。

Result: 通过新方法发现，当主流归一化模型在真实世界临床数据上重训练和评估时，假象现象频繁发生，且传统评测指标难以察觉；多种方法都出现明显不足。

Conclusion: 现有WSI归一化方法存在重要隐患。临床应用中应采用更具解释性和鲁棒性的归一化技术，并建立更严格的验证流程。

Abstract: Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.

</details>


### [184] [Unified Video Editing with Temporal Reasoner](https://arxiv.org/abs/2512.07469)
*Xiangpeng Yang,Ji Xie,Yiyuan Yang,Yan Huang,Min Xu,Qiang Wu*

Main category: cs.CV

TL;DR: 提出了一种新的视频编辑方法VideoCoF，不依赖于人工掩码，使用“先观察、推理，再编辑”的链式推理方式，实现了精确的指令到区域映射和细粒度编辑，效率和效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视频编辑方法在准确性和通用性之间存在矛盾：专家模型精确但依赖掩码，不利于统一；统一模型不依赖掩码但局部定位能力弱。作者旨在解决无需人工标注掩码、同时保持高精度区域编辑的问题。

Method: 提出VideoCoF模型，借鉴Chain-of-Thought推理思想，在视频扩散模型中强制“观察、推理、编辑”的流程，先预测编辑区域的潜在特征，再生成目标视频内容，引入RoPE对齐策略以保证运动一致和支持跨时长编辑。

Result: 以仅5万对视频数据在VideoCoF-Bench上达到最新SOTA效果，验证了方法的高效性与有效性。

Conclusion: VideoCoF无需用户提供掩码即可做到高精度的指令到区域对齐，实现高质量视频编辑，并且具备扩展性和高效率，在标注成本低的情况下取得领先性能。

Abstract: Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.

</details>


### [185] [Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance](https://arxiv.org/abs/2512.07480)
*Naifu Xue,Zhaoyang Jia,Jiahao Li,Bin Li,Zihan Zheng,Yuan Zhang,Yan Lu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于单步扩散的神经视频编码器S2VC，能够在低码率下高效且真实地重建视频画面，实现了比以往方法更高的感知质量和更低的压缩比。


<details>
  <summary>Details</summary>
Motivation: 传统和神经视频编码方法在提升低码率感知质量方面依然存在挑战。当前方法要么生成能力有限导致伪影明显，要么利用预训练扩散模型提升质量却计算消耗巨大，因此迫切需要一种既高质量又高效率的编码方法。

Method: 提出S2VC编解码框架，利用单步扩散生成器搭建条件编码系统。创新点包括提出语义引导机制（Contextual Semantic Guidance）用于提取帧自适应语义，实现细粒度条件控制，取代文本描述，提升生成真实感；同时，提出时间一致性引导（Temporal Consistency Guidance）强化扩散网络的时间连续性，保证视频帧间稳定性。

Result: 大量实验结果显示，S2VC在感知质量方面达到最前沿水平，比现有感知优化方法平均节省52.73%的码率，同时保持甚至提升视频画质。

Conclusion: S2VC验证了单步扩散在高效高质量视频压缩中的应用潜力，兼顾了重建质量、计算效率和帧间一致性，是低码率高质量视频编码的有力解决方案。

Abstract: While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.

</details>


### [186] [Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior](https://arxiv.org/abs/2512.07498)
*Chih-Chung Hsu,Shao-Ning Chen,Chia-Ming Lee,Yi-Fang Wang,Yi-Shiuan Chou*

Main category: cs.CV

TL;DR: 本文提出了一种能够在面临噪声、顺序紊乱或缺失帧的情况下仍具强鲁棒性的DeepFake检测方法——拉普拉斯正则化图卷积网络（LR-GCN），其在常见数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有DeepFake检测方法普遍假设人脸序列时间连贯且干净，但实际应用中常遇到压缩、遮挡、攻击等问题，使得人脸检测不稳定。为此，需要一种对乱序、缺失及异常帧有鲁棒性的检测技术。

Method: 方法核心为提出基于CNN帧特征自适应构建稀疏图的无序时序图嵌入（OF-TGE）；同时施加图结构与节点特征的双重稀疏约束以弱化无效人脸的信息。关键在于引入图拉普拉斯谱先验（高通与低通结合），通过频谱滤波抑制背景与噪声、突出伪造异常。

Result: 在FF++、Celeb-DFv2、DFDC等数据集的大量实验表明，该方法在多种异常扰动（如缺失、遮挡、对抗攻击）的情形下均优于现有方法，展现出卓越的鲁棒性和先进性能。

Conclusion: 拉普拉斯正则化图卷积网络有效提升了DeepFake检测对不规则、受损面部序列的鲁棒性，有望广泛应用于现实中的视频内容真实性检测场景。

Abstract: Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.

</details>


### [187] [MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer](https://arxiv.org/abs/2512.07500)
*Penghui Liu,Jiangshan Wang,Yutong Shen,Shanhui Mo,Chenyang Qi,Yue Ma*

Main category: cs.CV

TL;DR: 本文提出了MultiMotion框架，创新性地解决了DiT在多物体视频动作迁移中的控制和解耦难题，实现了更精准流畅的多物体动作迁移。


<details>
  <summary>Details</summary>
Motivation: 现有Diffusion Transformer (DiT) 架构在处理多物体视频动作迁移时，受到动作纠缠和缺乏物体级控制的问题，难以进行多个独立物体的精准动作迁移。

Method: 提出了MultiMotion框架，核心创新为掩码感知注意力动作流（AMF），利用SAM2掩码在DiT流程中实现多物体运动特征的显式解耦与控制；此外提出了高阶预测-校正采样器RectPC以提升采样效率和精度，还构建了首个针对DiT多物体动作迁移的数据集。

Result: MultiMotion框架在精度、语义对齐性和时序连贯性方面，实现了对多个不同物体的准确动作迁移，同时保持了DiT本身的高质量和可扩展性。

Conclusion: 该框架有效解决了多物体动作迁移中的主要难题，为DiT架构带来了更强的物体级控制和复杂场景适应能力，为后续相关研究打下了坚实基础。

Abstract: Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.

</details>


### [188] [SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation](https://arxiv.org/abs/2512.07503)
*Yao Teng,Zhihuan Jiang,Han Shi,Xian Liu,Xuefei Ning,Guohao Dai,Yu Wang,Zhenguo Li,Xihui Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为 SJD++ 的并行解码算法，实现了无损画质下的高效自回归文本生成图像。相比传统方法，生成速度提升 2-3 倍，步骤数减少 2-7 倍。


<details>
  <summary>Details</summary>
Motivation: 自回归大模型尽管能生成高质量图像，但由于推理时需顺序逐步生成每个 token，速度非常慢，影响实际应用。

Method: SJD++ 是一种训练无关的概率并行解码算法，将 Jacobi 并行多 token 预测机制和 speculative 拟草稿+校验机制结合，并在校验后复用高置信 token，避免全部重采，从而大幅减少生成步骤数。

Result: 在多个主流的自回归文本生成图像模型上，SJD++ 能在无明显画质损失的情况下，将推理延迟减少 2-3 倍，生成步骤数降低 2-7 倍。

Conclusion: SJD++ 有效解决了自回归文本生成图像的速度瓶颈，为大模型落地提供了新思路，且无需额外训练或损失画质。

Abstract: Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\times$ to $3\times$ inference latency reduction and $2\times$ to $7\times$ step compression, while preserving visual quality with no observable degradation.

</details>


### [189] [ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points](https://arxiv.org/abs/2512.07504)
*Ryota Okumura,Kaede Shiohara,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 本文提出了ControlVP，一个用于修正文本到图像生成模型中消失点不一致问题的框架，通过引入结构引导和几何约束，提升了生成图片的几何一致性，同时保持了高视觉质量。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到图像生成模型虽然视觉效果提升显著，但在生成结构复杂的场景（如建筑）时，常显现消失点不一致，导致空间结构不真实，严重影响相关应用（如图像到三维重建）。

Method: 提出ControlVP框架，在预训练扩散模型基础上，结合建筑轮廓信息进行结构引导，并引入几何约束，使图像边缘与透视线更准确对齐，用户可参与引导以纠正消失点位置。

Result: 在保持与现有主流方案相近视觉保真的基础上，显著提升了生成图片的几何一致性，尤其在建筑与需要空间结构准确性的场景效果突出。

Conclusion: ControlVP有效解决了文本到图像模型中的消失点问题，使生成图像在空间结构上更加真实，极大拓展了这类模型在三维重建等结构敏感应用中的实用价值。

Abstract: Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .

</details>


### [190] [MeshRipple: Structured Autoregressive Generation of Artist-Meshes](https://arxiv.org/abs/2512.07514)
*Junkai Lin,Hang Long,Huipeng Guo,Jielei Zhang,JiaYi Yang,Tianle Guo,Yang Yang,Jianwen Li,Wenxiao Zhang,Matthias Nießner,Wei Yang*

Main category: cs.CV

TL;DR: 本文提出了MeshRipple，一种新型自回归网格生成方法，通过模拟涟漪扩展的方式生成网格，解决了传统滑动窗口方法导致的几何依赖中断、洞与碎片化等问题，在表面保真度和拓扑完整性上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的自回归网格生成方法为了克服内存限制，把网格面序列化并采用滑动窗口训练与推理，但这样会破坏几何的长距离依赖，造成输出网格存在洞和非连通结构，影响生成质量。

Method: MeshRipple创新性地采用基于生成前沿的广度优先搜索（BFS）序列化，将生成顺序与网格拓扑紧密结合，并采用扩张式预测策略，实现连贯且连续的表面扩展。同时结合稀疏注意力全局记忆，使模型具备几乎无限的感受野以处理长距离拓扑依赖。

Result: MeshRipple在网格表面保真度和拓扑完整性方面超越了多个近期强力基线方法，能生成高质量、无碎片和洞的网格。

Conclusion: MeshRipple的集成设计有效解决了现有自回归网格生成中的长距离依赖断裂问题，为3D网格生成提供了更高质量和完整性的解决方案。

Abstract: Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.

</details>


### [191] [From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images](https://arxiv.org/abs/2512.07527)
*Fei Yu,Yu Liu,Luyang Tang,Mingchao Sun,Zengye Ge,Rui Bu,Yuchao Jin,Haisen Zhao,He Sun,Yangyan Li,Mu Xu,Wenzheng Chen,Baoquan Chen*

Main category: cs.CV

TL;DR: 本研究提出了一种针对城市级3D重建的问题新方法，能够从稀疏、视角变化极大的卫星图像高质量合成地面视角，实现了性能和效果上的突破。


<details>
  <summary>Details</summary>
Motivation: 目前基于卫星影像的城市级三维重建存在极端视角推断难题，尤其是在数据稀疏、立面扭曲和纹理受损的情况下，主流方法（如NeRF和3DGS）难以重建逼真模型。因此，亟需开发能适用于稀疏卫星图像输入并能实现高质量重建的新方法。

Method: 1）将城市几何建模为2.5D高度图，通过Z单调有符号距离场（SDF）描述，以适配卫星俯视视角，获得结构清晰且封闭的三维模型；2）采用可微渲染技术把卫星图像的外观映射到网格上，同时用生成式纹理修复网络提升模糊区域的高频纹理细节。

Result: 提出的方法在大规模城市重建实验中展现出出色的可扩展性和鲁棒性，例如仅用少量卫星图像成功重建了4平方公里的真实城区，并在合成高质量地面视角（如照片级真实感）方面取得了领先效果。

Conclusion: 该方法显著提升了卫星图像城市重建的质量和应用价值，不仅视觉效果优良，也适合作为城市规划、仿真等下游任务的高保真三维资产。

Abstract: City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.
  To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs.
  Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\,\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.

</details>


### [192] [Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation](https://arxiv.org/abs/2512.07568)
*Xuecheng Li,Weikuan Jia,Alisher Kurbonaliev,Qurbonaliev Alisher,Khudzhamkulov Rustam,Ismoilov Shuhratjon,Eshmatov Javhariddin,Yuanjie Zheng*

Main category: cs.CV

TL;DR: 提出了DSRSD-Net新模型，通过分离模态专有和共享信息，减少多模态学习中的信息冗余与伪相关，提高预测表现。


<details>
  <summary>Details</summary>
Motivation: 多模态学习常出现信息冗余、主导模态压制、特征纠缠与可解释性差等问题，难以在噪声或缺失情况下保持鲁棒性。

Method: 提出Dual-Stream Residual Semantic Decorrelation Network（DSRSD-Net），包含：1）双流表示学习模块，分离模态私有与共享潜变量；2）残差语义对齐头，结合对比和回归目标，对不同模态共享因素对齐到统一空间；3）解相关与正交性损失，抑制冗余，防止特征坍缩。

Result: 在两个大规模教育数据集上，DSRSD-Net在下一步预测与最终结果预测任务上，相较于主流单模态、早融合、晚融合及共注意力模型均有显著提升。

Conclusion: DSRSD-Net有效缓解多模态冗余与特征纠缠问题，提升了预测精度和模型解释性，是多模态融合的有力方案。

Abstract: Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while naïve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.

</details>


### [193] [All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs](https://arxiv.org/abs/2512.07580)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Longzhen Yang,Yihang Liu,Chengmei Yang,Ying Wen,Xianfeng Tang,Hui Liu,Yuyin Zhou,Lianghua He*

Main category: cs.CV

TL;DR: 本文研究了视觉大型语言模型（VLLMs）推理过程中视觉token剪枝方法，并首次提出'信息地平线'现象，发现深层剪枝随机化即可兼顾效率和准确率。


<details>
  <summary>Details</summary>
Motivation: VLLMs需要处理大量视觉token，导致推理计算量巨大。虽然token剪枝是提升效率的方法，但现有无训练剪枝在深层并无优越性，迫切需要理解剪枝机制失效的根本原因。

Method: 提出用删除单个视觉token带来的输出概率变化，量化token信息内容，追踪在不同层的token信息分布，比较不同任务与不同模型容量下的信息变化；并基于此分析随机剪枝与现有剪枝效果。

Result: 主要发现包括：(1) 随着网络加深，视觉token的信息趋于均匀并在中间层迅速消散（信息地平线）；(2) 信息地平线层数随任务复杂度和模型规模变化；(3) 深层采用随机剪枝即可高效，且能提升主流方法表现。最终DivPrune+随机剪枝在保留96.9%准确率下，剪去50%视觉token。

Conclusion: 深入剖析了VLLMs视觉token信息消散规律，证明深层随机剪枝简便高效，具备实际价值。提出的信息地平线理论为未来token剪枝和网络设计提供新视角。

Abstract: Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by "vanishing token information", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as "information horizon", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.

</details>


### [194] [LongCat-Image Technical Report](https://arxiv.org/abs/2512.07584)
*Meituan LongCat Team,Hanghang Ma,Haoxian Tan,Jiale Huang,Junqiang Wu,Jun-Yan He,Lishuai Gao,Songlin Xiao,Xiaoming Wei,Xiaoqi Ma,Xunliang Cai,Yayong Guan,Jie Hu*

Main category: cs.CV

TL;DR: LongCat-Image是一个面向中英文双语的开源基础图像生成模型，具备强大的文本渲染、照片级真实感、高效率和开发友好性。其模型小巧，推理快，支持极复杂中文字符，超过业内现有产品，并配套完整开源生态。


<details>
  <summary>Details</summary>
Motivation: 当前主流图像生成模型在多语言文本渲染、真实感、部署与易用性方面存在不足，尤其是对中文字符支持有限。

Method: 采用多阶段（预训练、中期训练、SFT）严格数据筛选，融合奖励模型进行强化学习，并通过小参数量（6B）扩散模型实现高效推理。对数据和训练流程全面开源，支持图像生成和编辑。

Result: 模型在文本渲染、照片级真实感和美学质量上达到SOTA，尤其在中英文字符覆盖和准确性上超越主流产品。推理速度快，部署资源消耗低，图像编辑准确性也领先。

Conclusion: LongCat-Image推动了双语图像生成技术水平，提升了开发和研究的可获取性，有望促进视觉内容创作领域的发展。

Abstract: We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.

</details>


### [195] [Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation](https://arxiv.org/abs/2512.07590)
*Kaili Qi,Zhongyi Huang,Wenli Yang*

Main category: cs.CV

TL;DR: 本文提出了一种增强的变分模型驱动UNet（VM_TUNet）方法，有效解决了噪声高、边界模糊或断裂的图像分割难题。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习分割方法在处理噪声较大或边界不清晰的医学图像时存在准确性瓶颈，因此迫切需要一个能兼顾物理先验和强表示能力的方法来提升分割效果，特别是在边界平滑和解释性方面。

Method: 该方法融合了变分方法和深度神经网络。在网络中引入物理先验、边缘检测器及平均曲率项，通过改进的Cahn-Hilliard方程提升边界平滑和可解释性。架构分为两大模块：F模块在频域中预处理以避免陷入局部最优，T模块局部计算，保证准确性与稳定性。

Result: 在三大基准数据集上的大量实验表明，VM_TUNet在分割性能与计算效率之间实现了良好平衡，量化指标优于纯CNN方法，视觉质量提升，并以合理计算开销接近transformer类方法的性能。

Conclusion: 所提方法不仅提升了噪声复杂场景下的分割质量，还保持了较低的计算成本，为结合物理建模与深度学习分割提供了有效的新方向。

Abstract: To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.

</details>


### [196] [Online Segment Any 3D Thing as Instance Tracking](https://arxiv.org/abs/2512.07599)
*Hanshi Wang,Zijian Cai,Jin Gao,Yiwei Zhang,Weiming Hu,Ke Wang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种创新的实时在线3D分割方法，将3D分割任务重新定义为实例追踪问题（AutoSeg3D），通过时空信息的高效传播与一致性学习，显著提升了3D语义分割在多数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于对象查询的3D分割方法主要关注空间信息的传递，但忽视了环境感知中的时间维度。考虑到智能体在动态环境中需要持续感知与理解，缺乏时间信息会限制模型对环境的完整洞察。因此，论文动机是引入和强化3D分割中的时序理解能力。

Method: 作者提出将在线3D分割看作实例追踪问题，实现长短期的实例信息关联和更新。具体机制包括：(1) 长期关联以保证特征和身份的一致性，(2) 短期更新强化最新观察，同时针对不同视角导致的局部可见问题；(3) 引入空间一致性学习，缓解VFMs导致的对象碎片化问题，从而丰富实例信息。整体框架通过稀疏对象查询高效交换时序与空间信息，降低了计算开销。

Result: 本文方法在多个公开数据集（ScanNet200、ScanNet、SceneNN、3RScan）上实验验证，相较于前沿方法ESAM，在ScanNet200上AP提升2.8，在其他数据集也有持续优势。

Conclusion: 通过将在线3D分割重构为实例追踪任务，并整合时序信息传播与空间一致性学习，本文显著提升了三维分割的实时性和准确性，为智能体的动态环境感知提供了更强支持。

Abstract: Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.

</details>


### [197] [Decomposition Sampling for Efficient Region Annotations in Active Learning](https://arxiv.org/abs/2512.07606)
*Jingna Qiu,Frauke Wilm,Mathias Öttl,Jonas Utz,Maja Schlereth,Moritz Schillinger,Marc Aubreville,Katharina Breininger*

Main category: cs.CV

TL;DR: 该论文提出了一种新的主动学习采样策略DECOMP，通过分解图像并有针对性地采样，提升了密集预测任务下的标注效率，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 密集预测任务（如医学影像分割）需要高成本的精细标注，传统的主动学习方法存在计算高、无关区域选择频繁以及对不确定性采样过度依赖的问题，亟需更高效的采样策略。

Method: 提出了DECOMP方法：利用伪标签将图像分解为面向特定类别的区域，并从每个类别中采样区域以提升样本多样性；根据类别预测置信度引导采样，使难以识别的类别获得更多标注。

Result: 在ROI分类、2D和3D分割等多项任务中，DECOMP方法对少数类区域的采样效果更佳，在提升少数类类别表现的同时，整体性能超过对比基线方法。

Conclusion: DECOMP能显著提升密集预测任务的主动学习效率，合理分配标注资源，尤其适合医学影像等高成本标注场景。

Abstract: Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.

</details>


### [198] [MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation](https://arxiv.org/abs/2512.07628)
*Zhiqi Li,Wenhuan Li,Tengfei Wang,Zhenwei Wang,Junta Wu,Haoyuan Wang,Yunhan Yang,Zehuan Huang,Yang Li,Peidong Liu,Chunchao Guo*

Main category: cs.CV

TL;DR: 本论文提出MoCA，一种高效的组合式3D生成模型，通过稀疏全局注意机制和无关组件压缩实现大规模3D对象和场景的高效生成，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前具备部件感知的3D生成方法在处理大量组件时，全局注意力的计算复杂度呈二次增长，导致难以扩展到复杂对象和场景，亟需提升计算效率和可扩展性。

Method: 作者提出MoCA模型，包含两个核心设计：（1）基于重要性的组件路由，仅对最相关的k个组件进行全局稀疏注意力计算；（2）对无关组件进行压缩，在降低计算量的同时保留其上下文信息。

Result: 实验表明，MoCA在组合式对象和场景生成任务上，在效率和效果方面均优于主流基线方法。

Conclusion: MoCA能够实现可扩展、高效且精细的3D资产生成，为复杂3D内容的自动化设计提供了有效解决方案。

Abstract: Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA

</details>


### [199] [Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method](https://arxiv.org/abs/2512.07651)
*Yuanye Liu,Hanxiao Zhang,Nannan Shi,Yuxin Shi,Arif Mahmood,Murtaza Taj,Xiahai Zhuang*

Main category: cs.CV

TL;DR: 本文介绍了LiQA数据集，这是一个包含来自多中心、多阶段MRI扫描的440例患者的肝纤维化数据集，用于算法在真实复杂场景下的分割与分期评测。文中还描述了竞赛中表现最优的方法，并表明多源数据与解剖约束能提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 肝纤维化在全球范围内是重要的健康问题，准确分期对于临床管理至关重要。但由于临床数据异质性和复杂性，现有算法在实际应用中表现受限，因此有必要建立更具挑战性和真实场景背景的数据集以促进算法发展。

Method: 本文提出并发布了LiQA数据集，设计用于肝脏分割（LiSeg）与肝纤维化分期（LiFS）。同时，报告中介绍了顶尖算法：在分割中采用半监督学习结合外部数据，提升在不同域、不完整模态和空间错位等复杂情况的表现；在分期中采用多视图共识与基于类激活图（CAM）的正则化，增强分期准确性。

Result: 基线方法实验结果显示，结合多源数据与解剖约束可显著提升模型在实际临床设置下的鲁棒性和效果。

Conclusion: LiQA数据集为肝脏分割及分期算法提供了严苛且具代表性的评测平台，相关方法和经验有助于推动肝纤维化自动分析技术迈向临床应用。

Abstract: Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.

</details>


### [200] [An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research](https://arxiv.org/abs/2512.07652)
*Hamad Almazrouei,Mariam Al Nasseri,Maha Alzaabi*

Main category: cs.CV

TL;DR: 本文提出了一种创新的AI驱动自主水下机器人（AUV）系统，能够高效、自动化地检测、分析和汇报水下物体，大幅提升海洋探索的效率和自动化程度。


<details>
  <summary>Details</summary>
Motivation: 传统海洋探索面临极端环境、能见度低与高成本等问题，导致海洋大面积未被探索。需要开发能够自动、高效、低风险进行检测与分析的新型技术系统。

Method: 系统集成了YOLOv12 Nano进行实时目标检测，ResNet50 CNN进行特征提取，PCA用于降维，K-Means++聚类按视觉特征分组；同时结合LLM（GPT-4o Mini）自动生成结构化报告，训练和评估基于超5.5万张澳洲水域图片数据集。

Result: 系统实现了mAP@0.5为0.512，准确率为0.535，召回率为0.438的目标检测性能；PCA降维后保留了98%方差，有效辅助聚类，能够将对象按视觉特征分组，LLM高效生成基于检测与聚类结果的汇总报告并结合位置信息。

Conclusion: 该系统集成多项AI技术，显著降低人工潜水风险，提高作业效率和数据分析深度，为复杂海洋环境下科学研究和新发现提供了有力工具。

Abstract: Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.

</details>


### [201] [Optimization-Guided Diffusion for Interactive Scene Generation](https://arxiv.org/abs/2512.07661)
*Shiaho Li,Naisheng Ye,Tianyu Li,Kashyap Chitta,Tuo An,Peng Su,Boyang Wang,Haiou Liu,Chen Lv,Hongyang Li*

Main category: cs.CV

TL;DR: 该论文提出了OMEGA，一个用于多智能体驾驶场景生成的优化指导、免训练框架，通过约束优化引导扩散采样生成更真实、可控的场景，并可生成更多安全关键事件。


<details>
  <summary>Details</summary>
Motivation: 目前自动驾驶评测所需的多智能体驾驶场景中，关键的安全事件较为罕见，现有数据集对此表现不足。数据驱动的场景生成虽成本低，但现有生成模型可控性差，且常常违背物理或社会约束，影响实际应用。

Method: 提出OMEGA框架，在扩散生成模型的每一步反向扩散中，通过约束优化方法校正轨迹，确保生成场景的物理可行性和行为一致性。进一步，将“自车-攻击者”交互场景建模为分布空间中的博弈论优化，近似Nash均衡，合成现实且包含安全关键事件的对抗场景。

Result: 在nuPlan和Waymo数据集上实验发现，OMEGA将物理和行为有效场景的比例大幅提高（自由探索从32.35%提升到72.27%，可控性生成从11%提升到80%），产生的near-collision帧也提升至原来的5倍，并保持场景真实性。

Conclusion: OMEGA框架兼具真实性、一致性与可控性，可以高效合成包含安全关键事件的复杂交通场景，为自动驾驶评测提供更有代表性的数据支持。

Abstract: Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.

</details>


### [202] [EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset](https://arxiv.org/abs/2512.07668)
*Ronan John,Aditya Kesari,Vincenzo DiMatteo,Kristin Dana*

Main category: cs.CV

TL;DR: 本文提出了EgoCampus数据集和相应的预测方法，用于研究真实世界漫步中的人类视觉注意力，特别关注户外校园环境。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉注意力或目光追踪数据多偏重室内或忽略真实导航场景中的目光数据，现有数据集很难研究行人在开放、动态环境中的视觉行为。因此，研究者希望收集并建模更真实、复杂场景下的人类视觉关注。

Method: 作者利用Meta的Project Aria智能眼镜收集数据，包含眼动追踪、前视RGB摄像头、惯性传感器和GPS，覆盖大学校园25条路径（超过6公里），录制了80多名行人的视角视频。同时，提出了EgoCampusNet模型，专门针对行人在户外导航时的眼动预测任务进行建模和实验。

Result: 建立了EgoCampus数据集，成为包含丰富户外行人眼动数据的重要资源；提出的EgoCampusNet在预测真实世界户外行人目光方面取得实证效果。

Conclusion: 该工作首次系统性地采集并公开大规模真实户外场景下的行人视觉注意力数据，并通过新方法推动了导航场景中的目光预测研究，对后续相关领域有重要资源和技术价值。

Abstract: We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .

</details>


### [203] [DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations](https://arxiv.org/abs/2512.07674)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法DIST-CLIP，用于实现MRI医学图像的风格迁移和标准化，有效提升数据的通用性和分析能力。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析中，深度学习模型难以在实际临床环境中泛化，主要由于不同设备与采集协议导致的MRI数据异质性和域间差异，现有图像和文本引导的数据协调方法存在适用性和表达力局限。

Method: 提出DIST-CLIP框架，可通过目标图像或DICOM元数据引导。该方法用预训练CLIP模型提取图像对比度嵌入，并与解耦的解剖内容结合，采用创新的自适应风格迁移模块，将风格特征整合到解剖内容中，实现灵活高效的图像标准化。

Result: 在多个真实临床数据集上测试，DIST-CLIP在风格迁移保真度和解剖结构保持度方面均明显优于现有主流方法。

Conclusion: DIST-CLIP为MRI图像标准化和风格迁移提供了灵活高效的解决方案，有助于提升医学影像分析的通用性和准确性。代码和模型将在发表时公开。

Abstract: Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.

</details>


### [204] [Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment](https://arxiv.org/abs/2512.07702)
*Sangha Park,Eunji Kim,Yeongtak Oh,Jooyoung Choi,Sungroh Yoon*

Main category: cs.CV

TL;DR: 本文提出了一种名为NPC（Negative Prompting for Image Correction）的自动化管道，有效提升了文本到图像生成任务中的文本-图像对齐能力，尤其对复杂或富有想象力的描述表现突出。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像生成技术取得了显著进展，但在处理结构复杂或富有想象力的提示词时，生成图像往往难以精确对齐文本内容。为了解决文本与图像之间的对齐问题，本文试图找到能够抑制非预期内容的方法。

Method: NPC方法通过分析扩散模型中的cross-attention模式，识别导致对齐误差的“负向提示”（包括与目标直接相关和无关但图像中出现的token）。系统采用“验证-生成描述-提出者”框架自动生成若干负向提示候选，并用显著性文本空间打分机制选出效果最优者，无需额外图像生成。

Result: 在GenEval++和Imagine-Bench两个基准测试上，NPC均超越了其他强基线方法。其中，GenEval++上NPC得分为0.571（相比基线0.371），在Imagine-Bench上取得整体最佳表现。

Conclusion: 通过引入与目标负相关的提示词，NPC可自动、有效地抑制非预期内容，显著提升扩散模型中的文本-图像对齐，展示了潜在的实际应用价值。

Abstract: Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.

</details>


### [205] [PVeRA: Probabilistic Vector-Based Random Matrix Adaptation](https://arxiv.org/abs/2512.07703)
*Leo Fillioux,Enzo Ferrante,Paul-Henry Cournède,Maria Vakalopoulou,Stergios Christodoulidis*

Main category: cs.CV

TL;DR: 本文提出了一种新的参数高效适配器PVeRA，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大模型训练和微调需大量数据及计算资源，适配器方法通过增加少量可训练参数来提升效率。但已有方法在处理输入不确定性方面有限，本文旨在改进此点。

Method: 提出PVeRA，基于VeRA，将其低秩矩阵设计为概率化形式，使每层共享的随机矩阵具备不确定性处理能力，并可在训练和测试时以不同方式采样。

Result: 在VTAB-1k基准和七种主流适配器上进行评估，PVeRA在性能上超越VeRA及其它适配器。

Conclusion: PVeRA在高效适配和处理输入不确定性上展现优越表现，是参数高效大模型适配领域的有价值进展。

Abstract: Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.

</details>


### [206] [UnCageNet: Tracking and Pose Estimation of Caged Animal](https://arxiv.org/abs/2512.07712)
*Sayak Dutta,Harish Katti,Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: 本文提出了一种三阶段的预处理流程，通过分割与修复笼子结构，有效解决了动物追踪和姿态估计算法在含有笼子遮挡时的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 当前的动物追踪和姿态估计系统在处理含有笼子结构以及系统性遮挡的图像和视频时，表现显著下降。由于实验动物常在笼内活动，亟需方法提升在此类场景下的算法鲁棒性。

Method: 方法包含三个阶段：(1) 采用带有72个方向可调滤波器的Gabor增强ResNet-UNet对笼子进行分割；(2) 利用CRFill对被笼子遮挡区域进行内容感知修复；(3) 在去除笼子后的图像上评估姿态估计和追踪的效果。

Result: 实验结果表明，使用该预处理流程后，姿态估计与追踪的性能与无遮挡环境下相当，同时关键点检测精度和轨迹一致性有显著提升。

Conclusion: 通过分割和修复笼子遮挡，显著提升了动物追踪与姿态估计系统在复杂遮挡环境下的表现，为相关应用提供了更强的工具。

Abstract: Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.

</details>


### [207] [ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation](https://arxiv.org/abs/2512.07720)
*Fan Yang,Heyuan Li,Peihao Li,Weihao Yuan,Lingteng Qiu,Chaoyue Song,Cheng Chen,Yisheng He,Shifeng Zhang,Xiaoguang Han,Steven Hoi,Guosheng Lin*

Main category: cs.CV

TL;DR: 该论文提出一种结合3D重建与视频生成优势的新方法，实现了从单张输入图像高质量、动态真实的上半身3D数字化身生成，显著提升了视觉效果并减少了伪影。


<details>
  <summary>Details</summary>
Motivation: 目前3D化身生成方法虽然结构稳定但纹理模糊、动作僵硬，而视频生成模型虽真实却容易出现身份漂移和结构错误，需要一种方法兼顾结构稳定与动态真实。

Method: 新方法先用3D重建模型获得结构和外观先验，再用自回归视频扩散模型引导渲染，实现高频、真实细节和流畅动态。结合两者优点，有效防止传统方法中的常见问题。

Result: 实验表明，该方法能在实时条件下生成高保真、外观真实且动作连贯的数字化身，显著降低伪影，并在视觉质量上优于先进方法。

Conclusion: 该方法为实时应用（如游戏和虚拟现实）带来高质量数字化身生成的高效可行方案，结合结构与生成的优势，大幅提升了数字化身的表现力与实用性。

Abstract: Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa

</details>


### [208] [Improving action classification with brain-inspired deep networks](https://arxiv.org/abs/2512.07729)
*Aidas Aglinskas,Stefano Anzellotti*

Main category: cs.CV

TL;DR: 本文研究了动作识别中深度神经网络（DNN）对身体和背景信息的利用情况，并提出了类脑域特异性结构以提升识别性能，使表现更接近人类。


<details>
  <summary>Details</summary>
Motivation: 虽然动作识别依赖于身体姿态、动作和背景信息，但目前不清楚DNN实际利用了多少这两类信息。由于训练数据中两类信息可能高度相关，DNN可能只依赖其中一种，无法充分利用全部信息。而人类大脑则有分工明确的区域分别处理身体和背景。论文旨在探索DNN和人类在信息利用上的差异，并尝试设计更拟人化的网络结构。

Method: 首先用包含完整身体+背景、移除身体、移除背景三组图像对DNN和人类被试（N=28）分别进行动作识别实验。然后，基于大脑的域特异性，设计了具有身体与背景分离处理通路的新型DNN架构，对比其与传统DNN在人类实验类似任务下的表现。

Result: 实验结果表明，传统DNN在没有背景信息时识别准确率降为随机水平，但在只有背景时与身体+背景时准确率差别不大。人类则能够在三种条件下都进行较准确的识别，且在只有身体信息时表现远好于仅有背景时。新提出的分离通路架构在动作识别表现上得到提升，并且其在不同刺激条件下的准确率分布更接近人类。

Conclusion: 现有DNN过度依赖背景信息，缺乏人类大脑的域特异性处理优势。采用类脑分离通路结构能提高模型识别能力，并使模型表现更符合人类，在动作识别领域具有实际意义。

Abstract: Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.

</details>


### [209] [SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination](https://arxiv.org/abs/2512.07730)
*Sangha Park,Seungryong Yoo,Jisoo Mok,Sungroh Yoon*

Main category: cs.CV

TL;DR: 提出了一种使用稀疏自编码器（Sparse Autoencoder, SAE）特征引导的框架（SAVE），显著减少多模态大模型（MLLMs）在视觉对象识别中的幻觉现象，并在多项基准测试中优于现有无训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在认知视觉对象时易出现由语言先验与视觉信息损失引发的幻觉，对其可靠性造成威胁。为提升视觉理解并减少幻觉，需要新的机制强化模型对关键视觉特征的关注。

Method: 提出了SAVE框架，通过生物类别有无问答探针筛选最具视觉信息代表性的SAE特征（视觉理解特征），训练时引导模型沿这些特征方向前进，改进视觉信息处理。该方法无需重新训练，结构简单，直接增强了模型的视觉理解能力。

Result: SAVE在标准基准数据集（CHAIR_S、POPE、MMHal-Bench）上超过了现有最优的无训练方法，例如在CHAIR_S上提升约10%，其他测评也有明显改进。同时反复实验证明方法对多种模型和不同层均具有稳健性和泛化性。

Conclusion: 通过引导多模态模型关注稀疏自编码器提取出的视觉理解特征，能够显著抑制模型对不确定对象及虚假内容的产生，提高对图像内容的信息利用与理解效果。

Abstract: Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\%p improvement in CHAIR\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.

</details>


### [210] [SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery](https://arxiv.org/abs/2512.07733)
*Meng Cao,Xingyu Li,Xue Liu,Ian Reid,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出了一种新的强化学习框架SpatialDreamer，用于提升多模态大模型在复杂空间推理任务中的能力，并通过几何一致性的奖励优化，实现在多个基准测试上的优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型虽能理解场景，但在需要主动空间推理与心理模拟的复杂任务上表现有限，主要因为它们仅被动观察，而没有主动构建和内化空间想象。为了提升模型主动推理能力，提出新方法。

Method: 提出SpatialDreamer强化学习框架。该框架通过闭环的主动探索、基于世界模型的视觉想象以及基于证据的推理来实现空间推理。同时，创新性地提出几何策略优化（GeoPO）方法，用树结构采样和步骤级奖励估计，结合几何一致性约束，解决长水平空间推理中奖励稀疏的问题。

Result: 通过大量实验，SpatialDreamer在多个具有挑战性的空间推理基准任务上取得了优异甚至领先的结果，显示出比传统方法更强的人类式空间心理模拟能力。

Conclusion: SpatialDreamer为多模态大模型在主动空间推理和心理模拟方面提供了显著提升，为模型更接近人类空间理解能力迈出了重要一步。

Abstract: Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.

</details>


### [211] [HLTCOE Evaluation Team at TREC 2025: VQA Track](https://arxiv.org/abs/2512.07738)
*Dengjia Zhang,Charles Weng,Katherine Guerrerio,Yi Lu,Kenton Murray,Alexander Martin,Reno Kriz,Benjamin Van Durme*

Main category: cs.CV

TL;DR: 该论文针对TREC VQA答题生成任务提出了一种结合生成和排序的多模态回答生成方法，并采用创新的损失函数提升答案的语义准确性和排名一致性。实验显示该方法在处理需要时序推理和语义消歧的问题时表现优异。


<details>
  <summary>Details</summary>
Motivation: 视觉问答（VQA）任务需要模型不仅生成与视频和问题相关的准确答案，还要保证答案的相关性排序一致性，目前生成类模型和排序类模型难以兼顾这两点。

Method: 提出了列表学习(listwise learning)框架。具体做法是，首先利用多模态基模型对每个视频问题对生成多个候选答案，然后采用带有等级权重的Masked Pointer Cross-Entropy Loss对排序模型进行训练，该损失融合了指针式候选选择、排序加权和词表限制下的掩蔽交叉熵，实现稳定可解释的全局优化。

Result: 所提方法实验中在准确率和排序稳定性上均有提升，尤其是在那些需要时序推理和语义消歧的复杂问题上表现更加明显。

Conclusion: 通过将生成建模和判别排序巧妙结合，论文方法实现了细粒度、高一致性的答案列表生成，为视觉问答领域提供了一种新的提升途径。

Abstract: The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.

</details>


### [212] [DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving](https://arxiv.org/abs/2512.07745)
*Jialv Zou,Shaoyu Chen,Bencheng Liao,Zhiyu Zheng,Yuehao Song,Lefei Zhang,Qian Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 本文提出DiffusionDriveV2，通过结合强化学习与扩散模型，提升自动驾驶轨迹生成的多样性与高质量，实现了领先性能。


<details>
  <summary>Details</summary>
Motivation: 传统端到端自动驾驶中的生成式扩散模型存在模式崩溃，导致行为单一；而现有DiffusionDrive方案尽管引入意图锚点产生多样轨迹，但依赖模仿学习，难以兼顾多样性和高质量。

Method: DiffusionDriveV2创新性地将强化学习引入轨迹生成：一方面通过自适应乘性噪声探索高质量轨迹空间，另一方面设计了锚点间内外适应性GRPO算法，防止不同驾驶意图间的评价混淆与模式崩溃，保留Gaussian Mixture多模态能力。

Result: 在NAVSIM v1/v2数据集的闭环评估中，DiffusionDriveV2分别取得了91.2 PDMS和85.5 EPDMS的新纪录，优于现有方法。

Conclusion: DiffusionDriveV2有效兼顾了生成轨迹的多样性与高一致性，为自动驾驶扩散模型提供了更优方案。

Abstract: Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2

</details>


### [213] [Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation](https://arxiv.org/abs/2512.07747)
*Shihao Zhao,Yitong Chen,Zeyinzi Jiang,Bojia Zi,Shaozhe Hao,Yu Liu,Chaojie Mao,Kwan-Yee K. Wong*

Main category: cs.CV

TL;DR: 本文提出了Unison模型，实现了统一的多模态理解与生成，具备低成本、自动化和高性能的特点。


<details>
  <summary>Details</summary>
Motivation: 当前多模态统一理解与生成方法存在高成本、任务覆盖有限、生成质量不佳及需手动配置参数等问题，且不能自动解析输入的任务元信息，缺乏智能自动化。

Method: 采用两阶段方案，结合预训练理解模型与生成模型，通过微调对齐并自动解析用户意图与元信息，实现多种多模态理解与生成任务的全自动化。

Result: 在仅使用50万训练样本和50 GPU小时的低成本设定下，Unison模型能够准确自动识别任务及参数，在多种理解和生成任务上表现优异。

Conclusion: Unison实现了低成本、高性能、自动化的多模态理解与生成，解决了现有方法的主要局限，为相关任务的自动化和普及提供了新的可能性。

Abstract: Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.

</details>


### [214] [Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2512.07760)
*Menglin Wang,Xiaojin Gong,Jiachen Li,Genlin Ji*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，解决了无人监督下可见光与红外行人重识别中的跨模态关联问题，取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 可见光与红外模态间存在巨大差异，无标注下的跨模态关联难度大，现有方法在聚类时易受局部错误影响，且忽视全局实例关系。

Method: 1) 提出模态感知Jaccard距离，纠正由模态差异带来的距离偏差，实现更可靠的全局聚类；2) 设计“分割-对比”策略，获得模态特定的全局原型，并在全局关联的指导下对齐，实现模态无关且可区分身份的表征学习。

Result: 在VI-ReID基准数据集上显著优于现有方法，取得当前最优结果。

Conclusion: 所提方法有效缓解了跨模态偏差，实现了更可靠的跨模态身份关联和表征学习，具备实际应用前景。

Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.

</details>


### [215] [GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring](https://arxiv.org/abs/2512.07776)
*Maximilian Schall,Felix Leonard Knöfel,Noah Elias König,Jan Jonas Kubeler,Maximilian von Klinski,Joan Wilhelm Linnemann,Xiaoshi Liu,Iven Jelle Schlegelmilch,Ole Woyciniuk,Alexandra Schild,Dante Wasmuht,Magdalena Bermejo Espinet,German Illera Basas,Gerard de Melo*

Main category: cs.CV

TL;DR: 本文提出了GorillaWatch系统和多个新数据集，用于自动化识别和监测极度濒危的西部低地大猩猩，有效提高了视频数据下的个体识别效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前人工对海量摄像机陷阱视频进行个体再识别的过程非常繁琐费时，限制了濒危动物监测的效率。因此，亟需自动化且鲁棒的方法来实现大猩猩等濒危动物的个体识别和数量统计。

Method: 1. 构建并公开了三个大猩猩相关视频数据集，包括在野外和动物园、用于多目标跟踪和跨域泛化评估。
2. 提出GorillaWatch管道，集成检测、跟踪与个体再识别，并采用多帧自监督预训练提取时序一致性特征，减少人工标注依赖。
3. 利用可微的AttnLRP方法验证模型是否依赖有效生物学特征。
4. 在无监督数量统计中，引入时空约束改进聚类算法，减少过分割现象。

Result: 实验表明，汇聚大规模图像骨干特征的表现在野生视频数据上优于专门的视频结构，同时GorillaWatch pipeline能够实现高效准确的识别、追踪和数量统计。

Conclusion: 本文为濒危大猩猩的高效、可扩展、非侵入性监测提供了新方法和公开资源，促进了野生动物保护领域的视频智能分析与研究。

Abstract: Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, "in-the-wild" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species

</details>


### [216] [Distribution Matching Variational AutoEncoder](https://arxiv.org/abs/2512.07778)
*Sen Ye,Jianning Pei,Mengde Xu,Shuyang Gu,Chunyu Wang,Liwei Wang,Han Hu*

Main category: cs.CV

TL;DR: 该论文提出Distribution-Matching VAE (DMVAE)，通过分布匹配约束使编码器的潜变量分布与任意参考分布对齐，不局限于高斯先验，提升了潜在空间的建模能力和图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 目前主流的视觉生成模型（如VAE）在图像编码进潜在空间时，通常默认采用高斯分布等固定先验。这种处理方式对潜在空间的分布施加了隐性但不明确的约束，导致难以判断哪种分布更有利于数据建模和图像合成。为此，作者希望设计一种能够灵活对齐任意分布的编码方案，系统性研究不同潜在分布对生成效果的影响。

Method: 作者提出Distribution-Matching VAE（DMVAE），核心方法是在VAE结构中引入分布匹配约束，使得编码器输出的潜变量分布可以与任意给定的参考分布对齐。这一参考分布可以来自自监督学习特征、扩散噪声或任何其他先验，从而突破传统VAE只对齐高斯分布的限制。

Result: 实验证明，采用DMVAE并选择自监督学习得到的分布作为参考分布，可以在ImageNet数据集上以仅64个epoch达到gFID=3.2，兼顾了重建质量和建模效率，优于传统的固定潜在分布方法。

Conclusion: 该研究发现，通过主动选择和对齐合适的潜在分布（而不是死用高斯等固定分布），可以更好地权衡易建模性和高保真图像合成能力，为视觉生成模型的潜在空间建模提供了新的思路。

Abstract: Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \textbf{Distribution-Matching VAE} (\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.

</details>


### [217] [OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory](https://arxiv.org/abs/2512.07802)
*Zhaochong An,Menglin Jia,Haonan Qiu,Zijian Zhou,Xiaoke Huang,Zhiheng Liu,Weiming Ren,Kumara Kahatapitiya,Ding Liu,Sen He,Chenyang Zhang,Tao Xiang,Fanny Yang,Serge Belongie,Tian Xie*

Main category: cs.CV

TL;DR: 本文针对现实世界中多镜头视频生成任务，提出OneStory方法，通过构建紧凑的全局跨镜头语义上下文，实现一致且可扩展的叙事视频生成，在多样复杂场景下取得了最新的叙事连贯性表现。


<details>
  <summary>Details</summary>
Motivation: 现有多镜头视频生成方法在建模长距离跨镜头上下文时存在不足，导致在复杂故事叙述下生成效果受限，因为它们多依赖有限的时间窗口或单一关键帧，难以捕捉全局语义线索。

Method: 作者将多镜头视频生成重构为“下一镜头生成”任务，利用预训练图像到视频（I2V）模型强化视觉条件，并引入两个关键模块：帧选择模块（选出前镜头中语义相关帧构建全局记忆）和自适应条件器（通过重要性驱动分块生成紧凑条件上下文）。此外，作者还整理了一个带有指涉性描述的新多镜头高质量数据集，设计了在新范式下的有效训练策略。

Result: 使用作者自建6万数据集微调的OneStory方法，在文本和图像条件下、多样且复杂的场景中，实现了业界最优的叙事一致性和连贯性表现。

Conclusion: OneStory为多镜头视频生成带来了强大的跨镜头上下文建模能力，实现了可控、沉浸式的长篇故事视频生成，在真实世界故事叙事任务中具有重要意义。

Abstract: Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.

</details>


### [218] [Multi-view Pyramid Transformer: Look Coarser to See Broader](https://arxiv.org/abs/2512.07806)
*Gyeongjin Kang,Seungkwon Yang,Seungtae Nam,Younggeun Lee,Jungwoo Kim,Eunbyung Park*

Main category: cs.CV

TL;DR: 提出了一种新的多视角金字塔Transformer（MVP）架构，可以高效地根据大量图像直接重建大的三维场景。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景重建方法在处理大规模、多视角输入时存在效率和表达能力不足的问题。本文旨在设计一个既高效又具有强大表达能力的多视角Transformer，用于处理和重建大规模3D场景。

Method: MVP采用双层次结构：一是由局部到全局的视角层级，逐步扩大模型对场景的理解范围；二是由细致到粗略的视内层级，把详细的空间信息逐步聚合为稠密的表征。结合3D高斯斑点（Gaussian Splatting）作为底层三维表达。

Result: 在多种数据集上验证了MVP架构，实验证明其重建精度达到最新水平，同时在面对不同数目视角和大规模场景时保持高效率和可扩展性。

Conclusion: MVP架构有效结合效率和表达能力，实现了高效且优质的大规模3D场景重建，为多视角3D重建任务提供了新方案。

Abstract: We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details," MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.

</details>


### [219] [Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes](https://arxiv.org/abs/2512.07807)
*Shai Krakovsky,Gal Fiebelman,Sagie Benaim,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，将语言字段嵌入3D表示中，以提升对空间环境的语义理解，解决了现有方法在大规模场景下的语义失配和效率问题，并在HolyScenes数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前将语言信息结合到3D空间表示中，提升了语义理解和自然语言交互能力，但已有的方法在处理大规模网络数据时，因语义特征失配和计算内存效率低而效果有限。

Method: 方法上，作者提出了超低维语义瓶颈特征，将其嵌入到3D高斯表示内，并通过多分辨率特征哈希编码器提高运行时和GPU内存效率。此外，引入了衰减降采样器模块，并通过正则化方法提升2D语义特征的对齐。

Result: 在真实场景数据集HolyScenes上，该方法在性能和效率上均优于现有方法。

Conclusion: 结合高效语义瓶颈和特征对齐策略，能更好地在大规模数据下实现语言-3D表示融合，提升空间语义认知与相关应用能力。

Abstract: Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.

</details>


### [220] [WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling](https://arxiv.org/abs/2512.07821)
*Shaoheng Fang,Hanwen Jiang,Yunpeng Bai,Niloy J. Mitra,Qixing Huang*

Main category: cs.CV

TL;DR: WorldReel是一种生成时空一致的高质量4D视频方法，能够同时输出RGB帧和4D场景信息，实现了大幅领先于以往方法的几何与运动一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法虽然图像真实但缺乏在空间和时间上的3D一致性，尤其是在动态场景和相机大幅运动下，无法保证场景的逻辑和几何一致性，因此需要一种能本质上保证4D一致性的视频生成方法。

Method: WorldReel联合生成RGB帧与4D场景表示（点云、相机轨迹、密集流映射），明确以单一、持久化场景为建模核心，结合了合成数据（用于准确几何与运动监督）和真实视频（增强视觉多样性和真实感）进行训练。

Result: 在动态场景和运动相机下，WorldReel在几何一致性、运动连贯性、减少视角-时序伪影等评测指标上均超越现有方法，表现出优秀的泛化能力和规模化视频生成效果。

Conclusion: WorldReel推动了4D一致世界建模的进步，为虚拟代理可稳定理解、渲染和交互的场景表述奠定基础，朝向更可靠的视频生成与场景推理迈出重要一步。

Abstract: Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.

</details>


### [221] [OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing](https://arxiv.org/abs/2512.07826)
*Haoyang He,Jie Wang,Jiangning Zhang,Zhucun Xue,Xingyuan Bu,Qiangpeng Yang,Shilei Wen,Lei Xie*

Main category: cs.CV

TL;DR: 本论文提出了OpenVE-3M，这是一个大规模、开源、高质量的指令型视频编辑数据集，并进一步发布了相关基准测试集和编辑模型。


<details>
  <summary>Details</summary>
Motivation: 目前指令型图像编辑数据集越来越丰富，但视频编辑领域缺乏高质量的大规模公开数据集，这极大地限制了相关模型的研究与发展。

Method: 作者构建了OpenVE-3M数据集，涵盖空间对齐和非空间对齐的多类视频编辑任务，并通过严格的数据管道和过滤流程保证数据质量。同时构建了开放基准测试集OpenVE-Bench，并基于新数据集训练了OpenVE-Edit大模型。

Result: OpenVE-3M在规模、编辑类型多样性、指令长度、以及整体质量上均超过现有开源数据集。OpenVE-Edit模型在新的基准测试(OpenVE-Bench)上表现优异，超过了先前所有开源模型（包括体量更大的14B基线模型）。

Conclusion: OpenVE-3M数据集和相关基准、模型的发布，为指令型视频编辑领域提供了坚实的数据和评测基础，能够推动该方向未来的研究进展。

Abstract: The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.

</details>


### [222] [One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation](https://arxiv.org/abs/2512.07829)
*Yuan Gao,Chen Chen,Tianrong Chen,Jiatao Gu*

Main category: cs.CV

TL;DR: 提出了一种新的自编码器框架（FAE），可以将高质量视觉特征适配到适用于生成任务的低维空间，同时在生成质量和效率之间取得平衡。该方法在多个基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成模型大多在低维潜空间中操作以提升训练效率和样本质量，但高质量预训练视觉特征与易于生成的潜空间有本质差异，导致适配困难，现有方法复杂。

Method: 提出了FAE（特征自编码器）框架，通过仅需一个注意力层将高质量预训练视觉表示（如DINO, SigLIP）映射到低维潜空间，并利用两个深度解码器：一个用于特征重建，一个用于图像生成。FAE既可用于扩散模型亦可用于正态化流模型。

Result: 在ImageNet 256x256等基准上，无论类条件还是文本到图像生成，FAE均取得优异性能。例如在扩散模型+CFG下FID可达1.29（800 epochs），无CFG下达到1.48，显示了高生成质量和学习速度。

Conclusion: FAE极大简化了结构和目标复杂度，实现了高度有效和通用的视觉表示适配，有望提升各类生成模型表现。

Abstract: Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.

</details>


### [223] [UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation](https://arxiv.org/abs/2512.07831)
*Jiehui Huang,Yuechen Zhang,Xu He,Yuan Gao,Zhi Cen,Bin Xia,Yan Zhou,Xin Tao,Pengfei Wan,Jiaya Jia*

Main category: cs.CV

TL;DR: UnityVideo提出了一个多模态、统一的视频生成框架，能够在多个条件下生成高质量、具备物理世界一致性的视频，并在零样本泛化能力上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法大多只基于单一模态（如仅依赖图像或文本进行条件生成），限制了对现实世界的全局理解能力，缺乏跨模态交互和多样的世界知识表达。为了解决这些瓶颈，作者提出了新方法。

Method: UnityVideo具备两个核心技术：（1）动态加噪策略，将不同模态和训练范式统一起来；（2）模态切换器结合上下文学习，可通过模块化参数和上下文感知，实现多模态统一处理。同时，作者构建了包含130万样本的大规模统一数据集，利用多模态联合优化训练。

Result: UnityVideo在学习速度（收敛加快）、零样本泛化（对未见数据的生成能力）、视频质量、一致性和物理世界对齐度上均显著优于现有方法。

Conclusion: 通过跨多模态联合优化和统一训练，UnityVideo大幅提升了视频生成质量及泛化能力，为世界感知视频合成提供了新方向。

Abstract: Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo

</details>


### [224] [Relational Visual Similarity](https://arxiv.org/abs/2512.07833)
*Thao Nguyen,Sicheng Mo,Krishna Kumar Singh,Yilin Wang,Jing Shi,Nicholas Kolkin,Eli Shechtman,Yong Jae Lee,Yuheng Li*

Main category: cs.CV

TL;DR: 本文提出了关系相似性（relational similarity）的概念，并开发了用于度量图像间关系相似性的新方法，揭示了现有视觉相似性方法的不足，并扩展了视觉计算的能力。


<details>
  <summary>Details</summary>
Motivation: 人类不仅能感知属性相似性，还能感知关系相似性，后者被认为是人类智能的重要特征。但目前主流视觉相似性度量方法只关注属性相似性，忽略了关系相似性。该研究致力于弥补这一差距，提升视觉模型的理解能力。

Method: 首先将关系相似性定义为图像内部元素之间存在相似关系，无论表面属性如何差异。然后，作者构建了一个11.4万条的图像-匿名化描述数据集，描述的是图像的关系逻辑而非表面内容，并据此微调视觉-语言模型，使其可以度量图像的关系相似性。

Result: 新方法在衡量图像间关系相似性方面取得了首批成果，实验表明现有的图像相似性模型（如LPIPS, CLIP, DINO）难以识别或捕捉人类感知到的关系相似性。提出的数据集和模型为未来研究打下了基础。

Conclusion: 本文首次将关系相似性引入视觉表征学习领域，有效弥补了现有方法只关注表面相似性的不足，并为以关系结构为基础的图像表示与理解提供了新路径。

Abstract: Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.

</details>


### [225] [Voxify3D: Pixel Art Meets Volumetric Rendering](https://arxiv.org/abs/2512.07834)
*Yi-Chuan Huang,Jiewen Chan,Hao-Jen Chien,Yu-Lun Liu*

Main category: cs.CV

TL;DR: Voxify3D是一种将3D网格自动转换为像素精确、色彩受限的体素艺术的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动生成体素艺术的方法在几何抽象与语义保持、颜色离散的一致性上难以兼得，导致生成效果要么过于简化、要么缺乏像素艺术的美感。

Method: 提出了一个两阶段可微分框架，融合三大组件：(1)正交视图像素监督以提升像素-体素对齐精度；(2)基于CLIP的Patch对齐用于语义保持；(3)受调色板约束的Gumbel-Softmax量化策略，实现对离散颜色空间的可控优化。

Result: 在多个角色、多种抽象和分辨率设定下，Voxify3D都展现出优异性能（CLIP-IQA得分37.12，用户偏好达77.90%）。

Conclusion: Voxify3D能够在保证体素艺术语义表达和像素艺术审美的前提下，实现端到端的离散优化和可控抽象。

Abstract: Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [226] [Empathy by Design: Aligning Large Language Models for Healthcare Dialogue](https://arxiv.org/abs/2512.06097)
*Emre Umucu,Guillermina Solis,Leon Garza,Emilia Rivas,Beatrice Lee,Anantaa Kotal,Aritran Piplai*

Main category: cs.CL

TL;DR: 本论文提出了一种基于直接偏好优化（DPO）的对齐方法，以提升大语言模型（LLM）在医疗和看护场景中的事实准确性和共情交流能力。实验证明，这种方法显著优于传统和商业基线。


<details>
  <summary>Details</summary>
Motivation: 当前通用大语言模型在医疗和看护应用中存在事实不可靠和缺乏共情的两大问题，这严重影响了模型在敏感语境下的实用性和安全性。为解决这些关键障碍，提升AI助理在医疗对话场景中的价值，亟需在模型上进行针对性的对齐和优化。

Method: 作者提出并实现了基于直接偏好优化（DPO）的模型对齐框架。具体方法是利用成对偏好数据微调领域适应的大语言模型，被偏好的回复体现支持性和可及性的交流风格，而被拒绝的回复展现出命令化或过于专业的语气。这样直接优化方式旨在更高效且透明地使模型输出契合人类偏好。

Result: 在多种开源及闭源LLM上的实验证明，经DPO调优后的模型在语义对齐、事实准确性及以人为中心的评价得分上均优于基线模型及Google等商业医疗对话系统。

Conclusion: 本研究验证了基于偏好对齐的优化方法能有效提升医疗与看护AI助手的可信赖度、共情力和医学实用性，具备良好的可扩展性和透明性，有助于推动相关领域AI系统的发展。

Abstract: General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design

</details>


### [227] [Morphologically-Informed Tokenizers for Languages with Non-Concatenative Morphology: A case study of Yoloxóchtil Mixtec ASR](https://arxiv.org/abs/2512.06169)
*Chris Crawford*

Main category: cs.CL

TL;DR: 本论文研究了如何利用形态学信息的分词器，通过结合ASR（自动语音识别）与基于文本的序列到序列工具，提升对Yoloxóchitl Mixtec（YM）语料的行间注释效率，并减少人工负担。提出了两种新颖的非线性分词方案，并验证其效果。


<details>
  <summary>Details</summary>
Motivation: 针对结构复杂、非线性形态特征显著的YM语言，传统分词方法在自动注释过程中表现有限，希望通过融合形态学知识的分词方案提升自动化效率和准确性，减轻人工注释压力。

Method: 提出了两种新型分词器：1）Segment and Melody分词器，仅提取音调不做分割预测；2）Sequence of Processes分词器，能同时预测词语分割与整体转录。这两种方法均结合ASR与序列到序列模型进行评估，并与传统BPE及Unigram分词法比较。同时，分析分词器的形态学和信息论指标对下游表现的预测关联。

Result: 提出的非线性分词器与传统BPE、Unigram方法在ASR任务中具有竞争力。Segment-and-Melody模型在词错误率上优于传统方法，但在字符错误率上略逊。同时，相关分析揭示形态学定制分词器与下游表现间具有可预测的相关性。

Conclusion: 针对非连接型形态的语言，基于形态学信息设计的非线性分词器可有效应用于ASR，与主流分词技术相当。未来需进一步研究这些分词器在更多下游任务的适用性。

Abstract: This paper investigates the impact of using morphologically-informed tokenizers to aid and streamline the interlinear gloss annotation of an audio corpus of Yoloxóchitl Mixtec (YM) using a combination of ASR and text-based sequence-to-sequence tools, with the goal of improving efficiency while reducing the workload of a human annotator. We present two novel tokenization schemes that separate words in a nonlinear manner, preserving information about tonal morphology as much as possible. One of these approaches, a Segment and Melody tokenizer, simply extracts the tones without predicting segmentation. The other, a Sequence of Processes tokenizer, predicts segmentation for the words, which could allow an end-to-end ASR system to produce segmented and unsegmented transcriptions in a single pass. We find that these novel tokenizers are competitive with BPE and Unigram models, and the Segment-and-Melody model outperforms traditional tokenizers in terms of word error rate but does not reach the same character error rate. In addition, we analyze tokenizers on morphological and information-theoretic metrics to find predictive correlations with downstream performance. Our results suggest that nonlinear tokenizers designed specifically for the non-concatenative morphology of a language are competitive with conventional BPE and Unigram models for ASR. Further research will be necessary to determine the applicability of these tokenizers in downstream processing tasks.

</details>


### [228] [Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots](https://arxiv.org/abs/2512.06193)
*Jihyung Park,Saleh Afroogh,Junfeng Jiao*

Main category: cs.CL

TL;DR: 本文提出了一种全新的实时检测隐性情感升级的框架GAUGE，专注于捕捉大模型在对话中潜在的情感危害。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地作为信息助手和情感伴侣融入日常生活，除了显性有害内容外，模型在对话中的重复情感强化或情感漂移也会带来隐性危害，且传统过滤机制难以及时捕捉。

Method: 提出了GAUGE框架，可基于LLM输出的logit概率，实时检测对话中的情感状态漂移。该方法不依赖外部分类器或固定的临床规则，而直接量化对话情感的动态变化。

Result: GAUGE能够实时捕捉大模型对话中逐步升级的隐性情感危害，优于现有依赖外部分类器的传统保护机制。

Conclusion: GAUGE为检测和管理AI对话中潜在情感危害提供了高效途径，有助于提升AI助手的安全性和情感健康水平。

Abstract: Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.

</details>


### [229] [Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety](https://arxiv.org/abs/2512.06227)
*Junyu Mao,Anthony Hills,Talia Tseriotou,Maria Liakata,Aya Shamir,Dan Sayda,Dana Atzil-Slonim,Natalie Djohari,Arpan Mandal,Silke Roth,Pamela Ugwudike,Mahesan Niranjan,Stuart E. Middleton*

Main category: cs.CL

TL;DR: 本文提出并验证了一种基于大型语言模型（LLM）的新型数据增强方法CFD，通过让多个LLM代理模拟人类注释的辩论过程，提升了心理健康和网络安全等NLP任务的数据标注效率和下游性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中复杂事件（如心理健康与网络风险）信息标注困难且成本高，影响NLP模型性能。因此需探索更高效的数据增强手段。

Method: 提出Confidence-Aware Fine-Grained Debate（CFD）框架：多LLM代理模拟人类标注者，基于细粒度证据进行辩论达成共识。并引入两个专家标注数据集（心理健康Reddit数据集和网络安全Facebook Sharenting数据集），将CFD结果与多种基线方法对比。

Result: CFD在数据增强任务表现最强，尤其在网络安全任务上，基于辩论生成的增强特征助力模型超过未增强基线10.1%。

Conclusion: CFD框架有效提升了现实世界NLP任务的数据增强与下游性能，尤其通过辩论方式生成的数据特征带来明显提升。

Abstract: Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.

</details>


### [230] [Policy-based Sentence Simplification: Replacing Parallel Corpora with LLM-as-a-Judge](https://arxiv.org/abs/2512.06228)
*Xuanxin Wu,Yuki Arase,Masaaki Nagata*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型作为评判者自动构建与简化策略一致训练数据的新方法，无需人工标注，实现多种不同简化策略的灵活控制。实验证明该方法在词汇级简化任务上优于GPT-4o，在整体改写上也表现相当，且适用于多种模型。


<details>
  <summary>Details</summary>
Motivation: 不同应用场景对句子简化有不同需求，如仅替换复杂词或整体重写句子，实现灵活、自动化的策略控制一直是难题。传统做法依赖人工标注或成对语料，成本高、灵活性差，因此亟需一种无需人工标注的多策略简化数据构建方法。

Method: 作者采用“大语言模型作为评判者”（LLM-as-a-Judge），利用大模型自动判断和选择符合不同简化策略的数据，从而自动生成满足多样化简化策略要求的训练数据，无需人工参与。该方法可以应用于各种开源或闭源LLMs，极大提升了构造定制化简化系统的效率和覆盖面。

Result: 实验显示，使用此法训练的小型开源LLM（如Phi-3-mini-3.8B）在词汇级简化上超越了GPT-4o，在整体句子改写上表现也相当。无论模型大小，此方法均能带来一致的性能提升。该结果经自动指标与人工评测验证。

Conclusion: 本方法为实现按需定制的多策略句子简化提供了高效、通用、低成本的解决方案，且其鲁棒性与适用性已在不同类型和规模的大模型上得到验证。

Abstract: Sentence simplification aims to modify a sentence to make it easier to read and understand while preserving the meaning. Different applications require distinct simplification policies, such as replacing only complex words at the lexical level or rewriting the entire sentence while trading off details for simplicity. However, achieving such policy-driven control remains an open challenge. In this work, we introduce a simple yet powerful approach that leverages Large Language Model-as-a-Judge (LLM-as-a-Judge) to automatically construct policy-aligned training data, completely removing the need for costly human annotation or parallel corpora. Our method enables building simplification systems that adapt to diverse simplification policies. Remarkably, even small-scale open-source LLMs such as Phi-3-mini-3.8B surpass GPT-4o on lexical-oriented simplification, while achieving comparable performance on overall rewriting, as verified by both automatic metrics and human evaluations. The consistent improvements across model families and sizes demonstrate the robustness of our approach.

</details>


### [231] [LOCUS: A System and Method for Low-Cost Customization for Universal Specialization](https://arxiv.org/abs/2512.06239)
*Dhanasekar Sundararaman,Keying Li,Wayne Xiong,Aashna Garg*

Main category: cs.CL

TL;DR: LOCUS是一种低成本定制NLP模型的流水线，只需少量标注数据，通过检索、合成数据生成和高效参数微调，显著提升NER和文本分类任务表现，模型准确率接近完全微调但内存占用仅5%。


<details>
  <summary>Details</summary>
Motivation: 现有NLP模型在特定任务的定制和微调通常需要大量的数据和计算资源。作者希望提出一种方法，在仅有少量标注样本的情况下，实现高效、低成本的专业化模型定制，并且终端模型小巧高效，易于部署和推广。

Method: LOCUS包含三个关键环节：1）从大型数据仓库检索与任务相关的数据；2）基于少量样本和上下文生成合成训练数据；3）采用全参数或低秩（LoRA）参数适配方式进行高效模型微调。

Result: 在命名实体识别（NER）和文本分类（TC）任务上，LOCUS在少样本情况下持续优于强基线（包括GPT-4o），模型体积显著减小，仅用5%内存即可达到99%完整微调的准确率，在多个基准上以不到1%的参数量超过GPT-4o。

Conclusion: LOCUS以极低的标注和资源成本，为NLP的专用化模型定制提供了可行、高效的解决方案，在兼顾效果和资源占用上优于当前主流基线方法。

Abstract: We present LOCUS (LOw-cost Customization for Universal Specialization), a pipeline that consumes few-shot data to streamline the construction and training of NLP models through targeted retrieval, synthetic data generation, and parameter-efficient tuning. With only a small number of labeled examples, LOCUS discovers pertinent data in a broad repository, synthesizes additional training samples via in-context data generation, and fine-tunes models using either full or low-rank (LoRA) parameter adaptation. Our approach targets named entity recognition (NER) and text classification (TC) benchmarks, consistently outperforming strong baselines (including GPT-4o) while substantially lowering costs and model sizes. Our resultant memory-optimized models retain 99% of fully fine-tuned accuracy while using barely 5% of the memory footprint, also beating GPT-4o on several benchmarks with less than 1% of its parameters.

</details>


### [232] [Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup](https://arxiv.org/abs/2512.06256)
*Aniruddha Maiti,Satya Nimmagadda,Kartha Veerya Jammuladinne,Niladri Sengupta,Ananya Jana*

Main category: cs.CL

TL;DR: 本文研究了两大语言模型在没有外部输入的情况下，互相对话多轮后的表现。结果显示对话初期较为连贯，但后期容易陷入自我重复和收敛。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在多智能体自发对话时，是否能够长时间保持多样性和创造性，而不是陷入机械式重复。

Method: 采用Mistral Nemo Base 2407和Llama 2 13B hf，两模型交替沟通，起始只给一句种子语句，连续对话若干轮，并用词汇和嵌入指标评估对话内容的变化与模型输出的一致性。

Result: 大多数对话最初是连贯的，但几轮后就出现短语不断重复、输出趋于一致的现象，难以产生新意，表现出所谓“收敛”行为。

Conclusion: 即使是大型、独立训练且无任务指示的模型，在多轮互相对话时，也很容易陷入输出收敛和内容重复的模式，这对多智能体系统的实际应用具有重要警示。

Abstract: In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.

</details>


### [233] [Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models](https://arxiv.org/abs/2512.06266)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Wei Ruan,Xiaoqi Liu,Xiaoxue Cheng,Xiyun Xu,Yang Song,Yanzipeng Gao,Yiming Jia,Yun Xing,Yuntao Wen,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.CL

TL;DR: 本文提出了Nanbeige4-3B系列小规模但性能优异的语言模型，通过创新的预训练与微调流程，显著提升了小模型的效果，并在多个基准测试中表现优异，超越了同规模及更大模型。


<details>
  <summary>Details</summary>
Motivation: 当前小规模语言模型普遍性能受限，难以与大模型抗衡。本文旨在突破小模型的性能瓶颈，提高其在推理和对齐等任务上的表现，使其拥有媲美甚至超过大规模模型的能力。

Method: 1）提出了精细化FG-WSD训练调度器，在预训练阶段动态优化数据混合比例；2）在指令微调数据准备时，结合了深度生成重构和链式思维重建; 3）通过双重偏好蒸馏（DPD）模型进行知识迁移；4）在微调后，引入多阶段强化学习训练，结合可验证奖励和偏好建模进行进一步人类对齐和推理能力强化。

Result: 在广泛基准测试中，Nanbeige4-3B显著超越了同参数量的其他模型，在多个复杂任务中甚至可与大模型媲美。

Conclusion: Nanbeige4-3B通过一系列创新的训练及微调技术，显著提升了小规模模型的性能，为小参数量模型的研究和应用提供了新思路和可行路径。

Abstract: We present Nanbeige4-3B, a family of small-scale but high-performing language models. Pretrained on 23T high-quality tokens and finetuned on over 30 million diverse instructions, we extend the boundary of the scaling law for small language models. In pre-training, we design a Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler, which progressively refines data mixtures across stages to boost model performance. In post-training, to improve the quality of the SFT data, we design a joint mechanism that integrates deliberative generation refinement and chain-of-thought reconstruction, yielding substantial gains on complex tasks. Following SFT, we employ our flagship reasoning model to distill Nanbeige4-3B through our proposed Dual Preference Distillation (DPD) method, which leads to further performance gains. Finally, a multi-stage reinforcement learning phase was applied, leveraging verifiable rewards and preference modeling to strengthen abilities on both reasoning and human alignment. Extensive evaluations show that Nanbeige4-3B not only significantly outperforms models of comparable parameter scale but also rivals much larger models across a wide range of benchmarks. The model checkpoints are available at https://huggingface.co/Nanbeige.

</details>


### [234] [Modeling Contextual Passage Utility for Multihop Question Answering](https://arxiv.org/abs/2512.06464)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

TL;DR: 本文提出了一种面向多跳问答的高效文段效用建模方法，通过对多篇文段间的上下文依赖进行建模，提高了问答系统中文段重新排序和最终答案的准确性。


<details>
  <summary>Details</summary>
Motivation: 多跳问答任务需要从多个文段中综合信息，现有检索方法仅关注相关性，忽视了文段间因上下文关系导致的效用变化，噪音文段容易影响最终答案。因此，需要更有效的文段效用评估机制。

Method: 作者提出利用小型transformer模型，结合先进推理模型的推理轨迹生成训练数据，fine-tune模型预测文段上下文相关的效用评分，实现文段重排序。

Result: 实验表明，基于效用评分的文段排序方法优于仅基于相关性的重排序方法，并提升了多跳问答系统的准确率和表现。

Conclusion: 文段效用的上下文建模对多跳问答至关重要，该方法在提升文段排序和下游问答任务表现方面效果显著，比传统方法更有效。

Abstract: Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multihop reasoning: the utility of a passage can be context-dependent, influenced by its relation to other passages - whether it provides complementary information or forms a crucial link in conjunction with others. In this paper, we propose a lightweight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question and obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods.

</details>


### [235] [Knowing What's Missing: Assessing Information Sufficiency in Question Answering](https://arxiv.org/abs/2512.06476)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的Identify-then-Verify（识别-验证）框架，用于提升问答系统判断信息充分性的能力，尤其是在需要推理的问题上更加有效。


<details>
  <summary>Details</summary>
Motivation: 现有问答系统在判断上下文信息是否足够回答问题时，特别是在复杂推理题上表现不佳，容易漏判缺失的信息。作者希望通过更系统的方法提升系统的判断准确性和可解释性。

Method: 作者设计了“先识别、再验证”两步法：模型先提出若干关于可能缺失信息的假设并汇总共识，然后再回溯文本进行验证，确保所假设信息确实缺失。

Result: 在多个多跳推理和事实型QA数据集上，该方法优于已有基线，能更准确判断信息充分性，并能清晰指出缺失的信息。

Conclusion: 通过指导模型先推测再验证，此框架能提升信息充分性判断的准确率和解释透明度，对构建更可靠问答系统具有积极意义。

Abstract: Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success on factual questions, they frequently fail on inferential ones that require reasoning beyond direct text extraction. We hypothesize that asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency. To this end, we propose a structured Identify-then-Verify framework for robust sufficiency modeling. Our method first generates multiple hypotheses about missing information and establishes a semantic consensus. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this information is truly absent. We evaluate our method against established baselines across diverse multi-hop and factual QA datasets. The results demonstrate that by guiding the model to justify its claims about missing information, our framework produces more accurate sufficiency judgments while clearly articulating any information gaps.

</details>


### [236] [Classifying German Language Proficiency Levels Using Large Language Models](https://arxiv.org/abs/2512.06483)
*Elias-Leander Ahlers,Witold Brunsmann,Malte Schilling*

Main category: cs.CL

TL;DR: 本文利用大语言模型（LLM）对德语文本进行CEFR（欧洲语言共同参考框架）分级，实现自动化的语言能力评估，并通过合成数据和现有语料多元构建数据集，采用多种方法提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 语言能力分级对教育工作十分重要，有助于根据学习者实际水平个性化教学。现有手工分级及自动化模型存在局限，提升自动化分级的准确性和可靠性有实际应用需求。

Method: 作者整合多个已标注CEFR语料并生成合成数据，构建多样化的数据集。技术路线包括提示词工程(prompt-engineering)、LLaMA-3-8B-Instruct模型微调(fine-tuning)、以及利用LLM内部神经状态的探测法(probing-based approach)来进行文本分级。

Result: 所有方法都在性能上超过现有方法表现，LLM在CEFR分级任务中展现出强大的可靠性和扩展性。

Conclusion: 大语言模型具备在语言能力分级上实现更高精度和可扩展性的潜力，为自动化教育和个性化教学工具带来更广阔的应用前景。

Abstract: Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.

</details>


### [237] [ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models](https://arxiv.org/abs/2512.06515)
*Somnath Banerjee,Sayan Layek,Sayantan Adak,Mykola Pechenizkiy,Animesh Mukherjee,Rima Hazra*

Main category: cs.CL

TL;DR: 提出了一种无需重新训练基础模型、能够在生成文本时实现安全、共情和价值对齐的参数高效方法——ProSocialAlign，实现更好的人类价值对齐和内容安全。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型安全机制在情绪激烈或高风险场景下存在局限，单一的拒答或盲从用户输入均可能带来负面影响。因此，需要新的机制生成既安全又具人性关怀的响应。

Method: 提出ProSocialAlign，在生成过程中通过两个机制确保安全和人性化响应：（1）方向调节机制，通过在参数空间减去训练出的“危害向量”以减少有害内容；（2）偏好感知的自回归奖励建模，可在多个属性上联合训练，通过梯度冲突解决实现用户可控的细粒度解码。整体流程先用硬性约束剔除有害内容，然后在安全内容集合内优化亲社会品质。

Result: 在五个安全基准上取得了最新最优表现，显著减少不安全内容输出，并提升对人类价值的对齐度，多项评测指标均有大幅提升。

Conclusion: ProSocialAlign在推理阶段为生成上下文敏感、安全且更符合人类价值观的内容提供了稳健、模块化的新基础。

Abstract: Current language model safety paradigms often fall short in emotionally charged or high-stakes settings, where refusal-only approaches may alienate users and naive compliance can amplify risk. We propose ProSocialAlign, a test-time, parameter-efficient framework that steers generation toward safe, empathetic, and value-aligned responses without retraining the base model. We formalize five human-centered objectives and cast safety as lexicographic constrained generation: first, applying hard constraints to eliminate harmful continuations; then optimizing for prosocial quality within the safe set. Our method combines (i) directional regulation, a harm-mitigation mechanism that subtracts a learned "harm vector" in parameter space, and (ii) preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution, enabling fine-grained, user-controllable decoding. Empirical evaluations across five safety benchmarks demonstrate state-of-the-art performance, reducing unsafe leakage and boosting alignment to human values, with strong gains across multiple evaluation metrics. ProSocialAlign offers a robust and modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time.

</details>


### [238] [Adapting AlignScore Mertic for Factual Consistency Evaluation of Text in Russian: A Student Abstract](https://arxiv.org/abs/2512.06586)
*Mikhail Zimin,Milyausha Shamsutdinova,Georgii Andriushchenko*

Main category: cs.CL

TL;DR: 本文提出了AlignRuScore工具，实现了英文AlignScore度量方法在俄语的适配，有效评估俄语文本的事实一致性。


<details>
  <summary>Details</summary>
Motivation: 现有事实一致性评估工具主要集中于英语，缺乏对俄语文本的专用工具。因此，需要开发专门面向俄语的度量方法。

Method: 作者以AlignScore为基础，基于RuBERT模型，微调了具有分类和回归头的对齐模型。训练数据包括俄语语料及英-俄翻译语料。

Result: 统一的对齐度量方法成功迁移到俄语，实现了对俄语文本事实一致性的有效评估。

Conclusion: AlignRuScore为多语言事实一致性评估奠定了基础，所发布的语料、模型和代码有助于进一步研究。

Abstract: Ensuring factual consistency in generated text is crucial for reliable natural language processing applications. However, there is a lack of evaluation tools for factual consistency in Russian texts, as existing tools primarily focus on English corpora. To bridge this gap, we introduce AlignRuScore, a comprehensive adaptation of the AlignScore metric for Russian. To adapt the metric, we fine-tuned a RuBERT-based alignment model with task-specific classification and regression heads on Russian and translated English datasets. Our results demonstrate that a unified alignment metric can be successfully ported to Russian, laying the groundwork for robust multilingual factual consistency evaluation. We release the translated corpora, model checkpoints, and code to support further research.

</details>


### [239] [The Online Discourse of Virtual Reality and Anxiety](https://arxiv.org/abs/2512.06656)
*Kwabena Yamoah,Cass Dykeman*

Main category: cs.CL

TL;DR: 该研究通过语料库语言学方法分析了网络上虚拟现实（VR）与焦虑相关讨论的热点词和词汇网络，揭示了用户对VR治疗焦虑的看法。


<details>
  <summary>Details</summary>
Motivation: 随着VR在治疗广泛性焦虑障碍和社交焦虑等临床问题的应用增多，理解用户在网上对该技术的讨论有助于进一步提高其疗效。

Method: 采用语料库语言学方法，使用Sketch Engine软件分析英语Trends语料库中与VR和焦虑相关的讨论，挖掘高频词与词语搭配。

Result: 最常被提及的词是VR、Oculus和headset，反映了对虚拟系统及其硬件设备的关注。同时，短语如of/in/for virtual reality在讨论设计、体验、开发等方面也频繁出现。

Conclusion: 该研究揭示了大众如何在日常话语中讨论VR与焦虑，为未来通过VR辅助心理咨询服务的研发和普及提供了新思路。

Abstract: VR in the treatment of clinical concerns such as generalized anxiety disorder or social anxiety. VR has created additional pathways to support patient well-being and care. Understanding online discussion of what users think about this technology may further support its efficacy. The purpose of this study was to employ a corpus linguistic methodology to identify the words and word networks that shed light on the online discussion of virtual reality and anxiety. Using corpus linguistics, frequently used words in discussion along with collocation were identified by utilizing Sketch Engine software. The results of the study, based upon the English Trends corpus, identified VR, Oculus, and headset as the most frequently discussed within the VR and anxiety subcorpus. These results point to the development of the virtual system, along with the physical apparatus that makes viewing and engaging with the virtual environment possible. Additional results point to collocation of prepositional phrases such as of virtual reality, in virtual reality, and for virtual reality relating to the design, experience, and development, respectively. These findings offer new perspective on how VR and anxiety together are discussed in general discourse and offer pathways for future opportunities to support counseling needs through development and accessibility. Keywords: anxiety disorders, corpus linguistics, Sketch Engine, and virtual reality VR

</details>


### [240] [CMV-Fuse: Cross Modal-View Fusion of AMR, Syntax, and Knowledge Representations for Aspect Based Sentiment Analysis](https://arxiv.org/abs/2512.06679)
*Smitha Muthya Sudheendra,Mani Deep Cherukuri,Jaideep Srivastava*

Main category: cs.CL

TL;DR: 本文提出了一种多视角融合框架CMV-Fuse，通过系统性整合多种语言学视角，有效提升了细粒度情感分析任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的细粒度情感分析方法（ABSA）通常只利用某一单一语言学视角，忽略了人类在理解语言时会自然结合句法、语义及世界知识等多种结构信息带来的互补作用，因此需要一种能综合不同语言学结构信息的方法来提升ABSA系统的表现。

Method: 作者提出了CMV-Fuse框架，融合了四种语言学视角：抽象语义表示（AMR）、成分句法分析、依存句法分析和语义注意机制，同时引入外部知识。通过分层门控注意力机制，在局部句法、中级语义及全局知识层次下进行信息融合，并设计了结构感知的多视角对比学习机制，保证不同视角表示的一致性与计算效率。

Result: 在标准的ABSA数据集上，CMV-Fuse在多个基线模型之上实现了显著性能提升。实验分析还揭示了各语言视角对于提升整体系统鲁棒性的具体贡献。

Conclusion: 融合多种结构化语言知识并通过跨模态、多视角对比学习优化，可有效提升细粒度情感分析性能。该方法为ABSA任务提供了更全面、鲁棒的解决思路。

Abstract: Natural language understanding inherently depends on integrating multiple complementary perspectives spanning from surface syntax to deep semantics and world knowledge. However, current Aspect-Based Sentiment Analysis (ABSA) systems typically exploit isolated linguistic views, thereby overlooking the intricate interplay between structural representations that humans naturally leverage. We propose CMV-Fuse, a Cross-Modal View fusion framework that emulates human language processing by systematically combining multiple linguistic perspectives. Our approach systematically orchestrates four linguistic perspectives: Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention, enhanced with external knowledge integration. Through hierarchical gated attention fusion across local syntactic, intermediate semantic, and global knowledge levels, CMV-Fuse captures both fine-grained structural patterns and broad contextual understanding. A novel structure aware multi-view contrastive learning mechanism ensures consistency across complementary representations while maintaining computational efficiency. Extensive experiments demonstrate substantial improvements over strong baselines on standard benchmarks, with analysis revealing how each linguistic view contributes to more robust sentiment analysis.

</details>


### [241] [Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis](https://arxiv.org/abs/2512.06681)
*Amartya Hatua*

Main category: cs.CL

TL;DR: 本文针对GPT-2进行因果机制解释性研究，发现其情感信息处理并非分阶段分层进行，而是在后期层统一整合。


<details>
  <summary>Details</summary>
Motivation: 现有理论认为GPT-2等Transformer模型中，情感信息的处理分为前期的词汇检测和中期的上下文整合两个阶段，但缺乏实验的机理层级验证。

Method: 作者采用系统性激活修补（activation patching）的方法，逐层干预GPT-2全部12层的激活状态，以探索情感信号在不同层级和阶段的处理方式。

Result: 实验表明，早期层（0-3层）主要作为词汇型情感检测器，输出稳定且与上下文基本无关的情感极性信号。而对三种中层上下文整合假说进行检验后发现皆为否：情感上下文等复杂现象其实主要在后期（8-11层）通过统一机制整合，而不是中层或模块化整合。

Conclusion: GPT-2模型中情感计算的层级结构与先前假说不符，强调了对大模型中上下文信息整合过程做进一步实证和机制层次研究的必要性。

Abstract: We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.

</details>


### [242] [PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory](https://arxiv.org/abs/2512.06688)
*Bowen Jiang,Yuan Yuan,Maohao Shen,Zhuoqun Hao,Zhangchen Xu,Zichen Chen,Ziyi Liu,Anvesh Rao Vijjini,Jiashu He,Hanchao Yu,Radha Poovendran,Gregory Wornell,Lyle Ungar,Dan Roth,Sihao Chen,Camillo Jose Taylor*

Main category: cs.CL

TL;DR: 本文提出了PersonaMem-v2数据集，用于提升大模型(LLM)的个性化能力，并开发了高效的智能体记忆框架，显著提升了长上下文个性化推理表现。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在个性化与对用户偏好的理解方面存在不足，尤其是在用户偏好隐式表达的真实场景下，现有数据集和方法难以有效支持个性化智能的发展。

Method: 1) 构建PersonaMem-v2数据集，涵盖1000次真实用户-机器人对话、300+场景、2万+用户偏好及超长上下文窗口，用户偏好多为隐式表达。2) 基于该数据集，采用强化学习微调模型以提升个性化和长上下文推理能力。3) 设计智能体记忆系统，使模型随用户互动可动态累积、可读的单一记忆，而非依赖完整对话历史。

Result: 实验显示，现有大型语言模型在隐式个性化准确率仅为37-48%；强化学习微调的Qwen3-4B准确率升至53%，超越GPT-5；自研记忆系统用2k-token的记忆即可达到55%准确率，提升效率16倍。

Conclusion: PersonaMem-v2数据集和智能体记忆框架显著推动了个性化AI的发展，展示了大模型高效、可扩展的个性化能力，对实际应用具有重要意义。

Abstract: Personalization is one of the next milestones in advancing AI capability and alignment. We introduce PersonaMem-v2, the state-of-the-art dataset for LLM personalization that simulates 1,000 realistic user-chatbot interactions on 300+ scenarios, 20,000+ user preferences, and 128k-token context windows, where most user preferences are implicitly revealed to reflect real-world interactions. Using this data, we investigate how reinforcement fine-tuning enables a model to improve its long-context reasoning capabilities for user understanding and personalization. We also develop a framework for training an agentic memory system, which maintains a single, human-readable memory that grows with each user over time.
  In our experiments, frontier LLMs still struggle with implicit personalization, achieving only 37-48% accuracy. While they support long context windows, reasoning remains the bottleneck for implicit personalization tasks. Using reinforcement fine-tuning, we successfully train Qwen3-4B to outperforms GPT-5, reaching 53% accuracy in implicit personalization. Moreover, our agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens, relying on a 2k-token memory instead of full 32k conversation histories. These results underscore the impact of our dataset and demonstrate agentic memory as a scalable path toward real-world personalized intelligence.

</details>


### [243] [Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation](https://arxiv.org/abs/2512.06690)
*Chengbing Wang,Yang Zhang,Wenjie Wang,Xiaoyan Zhao,Fuli Feng,Xiangnan He,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本文提出了FlyThinker框架，通过在生成过程同时进行推理，实现高效个性化长文本生成。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的偏好对齐方法主要关注群体层面，忽视个体用户，且现有个性化方法对隐式偏好推理能力有限，难以适应实际应用需求。

Method: FlyThinker是一种“生成中推理”的架构，通过独立推理模型并行生成潜在推理信息，并在生成过程中动态融合以引导响应生成。该推理模型设计为仅依赖历史响应，不依赖自身输出，以实现高效并行训练和推理。

Result: 在真实世界基准测试中，FlyThinker实现了更好的个性化生成表现，同时维持高效的训练和推理。

Conclusion: FlyThinker有效提升了长文本个性化生成的质量和效率，有望推广应用于实际场景。

Abstract: Preference alignment has enabled large language models (LLMs) to better reflect human expectations, but current methods mostly optimize for population-level preferences, overlooking individual users. Personalization is essential, yet early approaches-such as prompt customization or fine-tuning-struggle to reason over implicit preferences, limiting real-world effectiveness. Recent "think-then-generate" methods address this by reasoning before response generation. However, they face challenges in long-form generation: their static one-shot reasoning must capture all relevant information for the full response generation, making learning difficult and limiting adaptability to evolving content. To address this issue, we propose FlyThinker, an efficient "think-while-generating" framework for personalized long-form generation. FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. This design enables reasoning and generation to run concurrently, ensuring inference efficiency. In addition, the reasoning model is designed to depend only on previous responses rather than its own prior outputs, which preserves training parallelism across different positions-allowing all reasoning tokens for training data to be produced in a single forward pass like standard LLM training, ensuring training efficiency. Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while keeping training and inference efficiency.

</details>


### [244] [TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction](https://arxiv.org/abs/2512.06694)
*Aoi Fujita,Taichi Yamamoto,Yuri Nakayama,Ryota Kobayashi*

Main category: cs.CL

TL;DR: 本文提出了一种适用于社交媒体短文本的新主题建模方法TopiCLEAR，通过聚类嵌入和自适应降维，有效提升了主题提取的精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型在短文本（如社交媒体帖子）上的表现欠佳，主要因为文本短小、语义碎片化及非正式表达等问题，亟需新方法提升分析效果。

Method: 方法依次将文本转为SBERT嵌入，初步用高斯混合模型（GMM）聚类，再采用基于线性判别分析的有监督降维进行聚类细化，重复直至收敛，且无需停用词去除等预处理。

Result: 在20News、AgNewsTitle、Reddit和TweetTopic四个含人工主题标注的数据集上，与七个基线方法对比，TopiCLEAR取得了与人工标注主题的最高相似度，尤其在社交媒体短文本和新闻文本中表现突出。

Conclusion: TopiCLEAR不仅提升了主题识别的准确度，还大幅增强了主题的可解释性，展示了在社交媒体与网络内容分析方面的巨大应用潜力。

Abstract: Rapid expansion of social media platforms such as X (formerly Twitter), Facebook, and Reddit has enabled large-scale analysis of public perceptions on diverse topics, including social issues, politics, natural disasters, and consumer sentiment. Topic modeling is a widely used approach for uncovering latent themes in text data, typically framed as an unsupervised classification task. However, traditional models, originally designed for longer and more formal documents, struggle with short social media posts due to limited co-occurrence statistics, fragmented semantics, inconsistent spelling, and informal language. To address these challenges, we propose a new method, TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction. Specifically, each text is embedded using Sentence-BERT (SBERT) and provisionally clustered using Gaussian Mixture Models (GMM). The clusters are then refined iteratively using a supervised projection based on linear discriminant analysis, followed by GMM-based clustering until convergence. Notably, our method operates directly on raw text, eliminating the need for preprocessing steps such as stop word removal. We evaluate our approach on four diverse datasets, 20News, AgNewsTitle, Reddit, and TweetTopic, each containing human-labeled topic information. Compared with seven baseline methods, including a recent SBERT-based method and a zero-shot generative AI method, our approach achieves the highest similarity to human-annotated topics, with significant improvements for both social media posts and online news articles. Additionally, qualitative analysis shows that our method produces more interpretable topics, highlighting its potential for applications in social media data and web content analytics.

</details>


### [245] [Parameter-Efficient Fine-Tuning with Differential Privacy for Robust Instruction Adaptation in Large Language Models](https://arxiv.org/abs/2512.06711)
*Yulin Huang,Yaxuan Luan,Jinxu Guo,Xiangchen Song,Yuchen Liu*

Main category: cs.CL

TL;DR: 本文提出了一种结合差分隐私与参数高效微调的新方法，有效提升大模型在指令微调中的隐私保护与效率，同时保持模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在指令微调时，存在隐私泄露风险和资源消耗高等问题，亟需在保护隐私的前提下提升微调效率与实用性。

Method: 方法上，将主干模型参数冻结，通过低维投影子空间进行参数更新，并在梯度计算阶段引入梯度裁剪与自适应噪声分配，统一了梯度约束、噪声分配和参数投影的优化框架。

Result: 实验结果显示，该方法在多组超参数、环境及数据敏感性场景下，均优于现有基线模型，不仅提升了准确率和参数效率，还减少了隐私预算消耗，并具备稳定的泛化能力。

Conclusion: 所提方法验证了差分隐私与参数高效微调在理论结合和实际指令任务中的可行性，为复杂指令环境下大模型的安全训练提供了有效方案。

Abstract: This study addresses the issues of privacy protection and efficiency in instruction fine-tuning of large-scale language models by proposing a parameter-efficient method that integrates differential privacy noise allocation with gradient clipping in a collaborative optimization framework. The method keeps the backbone model frozen and updates parameters through a low-dimensional projection subspace, while introducing clipping and adaptive noise allocation during gradient computation. This design reduces privacy budget consumption and ensures training stability and robustness. The unified framework combines gradient constraints, noise allocation, and parameter projection, effectively mitigating performance fluctuations and privacy risks in multi-task instruction scenarios. Experiments are conducted across hyperparameter, environment, and data sensitivity dimensions. Results show that the method outperforms baseline models in accuracy, privacy budget, and parameter efficiency, and maintains stable performance under diverse and uncertain data conditions. The findings enrich the theoretical integration of differential privacy and parameter-efficient fine-tuning and demonstrate its practical adaptability in instruction tasks, providing a feasible solution for secure training in complex instruction environments.

</details>


### [246] ["The Dentist is an involved parent, the bartender is not": Revealing Implicit Biases in QA with Implicit BBQ](https://arxiv.org/abs/2512.06732)
*Aarushi Wagh,Saniya Srivastava*

Main category: cs.CL

TL;DR: 本文提出了ImplicitBBQ基准，用于检测LLM在未明确指出受保护属性（如性别、种族等）情况下的隐性偏见，发现现有模型在这方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有偏见评测大多依靠显式声明受保护属性，但实际交流中往往是隐含表达，这导致对LLM公平性的评估存在盲点。

Method: 作者设计了ImplicitBBQ基准，在原有BBQ基础上引入通过名字、文化线索等隐含提示的受保护属性，然后用此基准测试GPT-4o等LLM，比较显式与隐式提示下的表现差异。

Result: GPT-4o在面对隐性提示时，某些类别（如性取向）准确率下降高达7%，大多数类别也有一致性的准确率下降，显示当前LLM存在隐性偏见。

Conclusion: ImplicitBBQ填补了隐性偏见检测的空白，是NLP公平性评估的重要补充，对推动更全面的偏见检测具有实际意义。

Abstract: Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the "sexual orientation" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.

</details>


### [247] [A Patient-Doctor-NLP-System to contest inequality for less privileged](https://arxiv.org/abs/2512.06734)
*Subrit Dikshit,Ritu Tiwari,Priyank Jain*

Main category: cs.CL

TL;DR: 提出了一种新模型PDFTEMRA，可降低资源消耗同时保持对印地语等低资源语言的医疗NLP任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在医疗NLP应用中很有效，但对视觉障碍者和印地语等低资源语言用户支持有限，且其在资源受限的医疗环境中难以训练和部署。

Method: 提出PDFTEMRA模型，结合了模型蒸馏、频域调制、集成学习和随机激活技术，设计为紧凑、高效的Transformer架构，并在专门为印地语和无障碍场景定制的医疗问答和咨询数据集上进行训练和评估。

Result: PDFTEMRA在印地语医疗数据集上达到了与主流NLP模型类似的性能，但计算资源需求明显更低。

Conclusion: PDFTEMRA适用于资源受限环境中的医疗NLP任务，为印地语等低资源语言和无障碍用户提供了可行的支持。

Abstract: Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.

</details>


### [248] [One Word Is Not Enough: Simple Prompts Improve Word Embeddings](https://arxiv.org/abs/2512.06744)
*Rajeev Ranjan*

Main category: cs.CL

TL;DR: 该论文发现，在单词嵌入前添加简单的语义提示词（如“meaning: {word}”），可显著提升主流文本嵌入模型在单词相似性任务上的性能，显著优于传统静态词向量。


<details>
  <summary>Details</summary>
Motivation: 当前主流的文本嵌入模型多为句子级任务设计与评测，其对孤立单词的表现和机制不够明确。研究者希望探索通过提示词(prompt)机制提升其在单词级别任务（如词相似性）上的效果。

Method: 作者对7种主流嵌入模型（包括OpenAI、Cohere、Voyage AI等）在3个标准单词相似性基准（SimLex-999、WordSim-353、MEN-3000）上，测试了在单词前添加语义提示词（如“meaning: word”）后的表现变化，并以斯皮尔曼相关系数为衡量指标。

Result: 在不需额外训练、零样本设定下，添加语义提示词后，模型的单词相似性相关性可大幅提升，SimLex-999提升高达0.29，有的模型从完全失效提升到高性能，如相关性提升+0.73。在SimLex-999等数据集上获得了当前嵌入方法的新最好成绩，超越了Word2Vec、LexVec等静态方法。

Conclusion: 通过添加简单的语义提示词，能显著提升任何文本嵌入模型在单词相似性任务上的表现，这种零样本技术无需训练，兼容性强，为类任务提供了一种便捷和高效的新途径。

Abstract: Text embedding models are designed for sentence-level applications like retrieval and semantic similarity, and are primarily evaluated on sentence-level benchmarks. Their behavior on isolated words is less understood. We show that simply prepending semantic prompts to words before embedding substantially improves word similarity correlations. Testing 7 text embedding models, including text-embedding-3-large (OpenAI), embed-english-v3.0 (Cohere), voyage-3(Voyage AI), all-mpnet-base-v2, and Qwen3-Embedding-8B, on 3 standard benchmarks (SimLex-999, WordSim-353, MEN-3000), we find that prompts like "meaning: {word}" or "Represent the semantic concept: {word}" improve Spearman correlations by up to +0.29 on SimLex-999. Some models fail completely on bare words (correlation = 0) but recover with prompts (+0.73 improvement). Our best results achieve correlation = 0.692 on SimLex-999 with embed-english-v3.0 (Cohere), correlation = 0.811 on WordSim-353, and correlation = 0.855 on MEN-3000 with text-embedding-3-large (OpenAI). These results outperform classic static embeddings like Word2Vec (correlation = 0.40) and even the best static method LexVec (correlation = 0.48) on SimLex-999, establishing a new state-of-the-art for pure embedding methods. This zero-shot technique requires no training and works with any text embedding model.

</details>


### [249] [Becoming Experienced Judges: Selective Test-Time Learning for Evaluators](https://arxiv.org/abs/2512.06751)
*Seungyeon Jwa,Daechul Ahn,Reokyoung Kim,Dongyeop Kang,Jonghyun Choi*

Main category: cs.CL

TL;DR: 本文提出了一种名为LWE（Learning While Evaluating）的新框架，使大语言模型在评估过程中可以动态自我改进。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自动评测通常每次处理独立样本，且使用固定评测提示，未能累积经验和针对具体样本调整评测标准，存在泛化和适应性不足的问题。

Method: LWE通过维护一个不断进化的meta-prompt，针对每个样本自动生成评测标准，并利用自身反馈实时优化评测方式。论文还提出了Selective LWE，仅在检测到自我评价不一致时才更新meta-prompt，以降低计算成本。

Result: 在两个成对比较的任务基准上，Selective LWE优于现有强基线，展示出通过有选择的更新能显著提升评测效果。

Conclusion: 评测模型可以通过在测试时动态、选择性地调整评测标准，有效学习和提升，特别是在遇到评测难点时效果显著。

Abstract: Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.

</details>


### [250] [From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs](https://arxiv.org/abs/2512.06776)
*Yuchuan Tian,Yuchen Liang,Jiacheng Sun,Shuo Zhang,Guangwen Yang,Yingte Shu,Sibo Fang,Tianyu Guo,Kai Han,Chao Xu,Hanting Chen,Xinghao Chen,Yunhe Wang*

Main category: cs.CL

TL;DR: 本文提出了一种从自回归（AR）到块扩散（Block-Diffusion）语言模型的高效迁移方法，实现高效并行文本生成并保持推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型主要采用自回归解码，生成速度受限且难以并行。扩散式语言模型虽然可并行生成，但训练成本高且难以保留已有模型知识。现有的迁移尝试均未彻底解决AR与块扩散中的因果性和双向性不匹配问题。

Method: 作者将AR看作块扩散的特例（块大小为1），通过设计上下文因果注意力掩码、并行高效的适应过程、辅助AR损失函数，以及逐步增大块大小的方法，实现了知识迁移。同时确保了训练与推理的一致性，并集成进了masked block-diffusion流程。

Result: 基于上述方法开发的NBDiff-7B（基础版和Instruct版）在长上下文建模和推理能力上表现突出，在通用知识、数学和编程任务上显著优于同类7B参数规模的扩散模型。

Conclusion: 论文显示，系统性地将AR模型适应到块扩散范式是一种高效且节省算力的训练方案，可以替代从零训练扩散语言模型。

Abstract: Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior "adaptation" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.

</details>


### [251] [LLM4SFC: Sequential Function Chart Generation via Large Language Models](https://arxiv.org/abs/2512.06787)
*Ofek Glick,Vladimir Tchuiev,Marah Ghoummaid,Michal Moshkovitz,Dotan Di-Castro*

Main category: cs.CL

TL;DR: 提出LLM4SFC框架，实现自动生成可执行的序列功能图（SFC），推动工业自动化编程。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型可以生成结构化文本（ST）等PLC语言，但对于IEC 61131-3标准下的图形化语言（如SFC）研究不足。SFC涉及图形特性和内嵌ST，传统文本生成方法难以保证其工业工具链的兼容性及可执行性。

Method: 提出LLM4SFC框架，包含三部分：1）精简的结构化表达，囊括拓扑和嵌入的ST；2）LLM通过微调和少样本检索增强生成，保证符合SFC编程规范；3）结构化生成策略，实时剪除非法token，确保文本兼容SFC格式。

Result: 在自动化制造项目的真实SFC数据集上，结合开源与专有LLM评估，LLM4SFC生成的SFC语法有效，生成成功率达75%-94%。

Conclusion: LLM4SFC实现了从自然语言到可执行SFC自动生成，有效连接图形化与文本PLC语言技术，推动自动化工业编程发展。

Abstract: While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.

</details>


### [252] [Large Language Model-Based Generation of Discharge Summaries](https://arxiv.org/abs/2512.06812)
*Tiago Rodrigues,Carla Teixeira Lopes*

Main category: cs.CL

TL;DR: 本文评估了五种大语言模型（含开源和私有）在自动生成出院小结任务中的表现，发现私有模型（如Gemini、GPT-4）优于开源模型。


<details>
  <summary>Details</summary>
Motivation: 手工撰写出院小结耗时且易错，自动化有助于减轻医护负担、提升信息可用性和准确性。

Method: 选用MIMIC-III数据，比较了Mistral、Llama 2（开源）与GPT-3、GPT-4、Gemini 1.5 Pro（私有）在出院小结自动生成上的表现，评测手段包括严格匹配、模糊匹配及无参照评估，并结合临床专家人工评判。

Result: Gemini（私有，one-shot）表现最佳，生成摘要与人类标准最相似。经过微调的Mistral虽有提升，但总体落后于私有模型，易出现幻觉和重复内容。

Conclusion: 尽管存在幻觉和遗漏问题，只要保障数据隐私，大模型、特别是私有模型，在自动生成出院小结任务上展现出很大前景。

Abstract: Discharge Summaries are documents written by medical professionals that detail a patient's visit to a care facility. They contain a wealth of information crucial for patient care, and automating their generation could significantly reduce the effort required from healthcare professionals, minimize errors, and ensure that critical patient information is easily accessible and actionable. In this work, we explore the use of five Large Language Models on this task, from open-source models (Mistral, Llama 2) to proprietary systems (GPT-3, GPT-4, Gemini 1.5 Pro), leveraging MIMIC-III summaries and notes. We evaluate them using exact-match, soft-overlap, and reference-free metrics. Our results show that proprietary models, particularly Gemini with one-shot prompting, outperformed others, producing summaries with the highest similarity to the gold-standard ones. Open-source models, while promising, especially Mistral after fine-tuning, lagged in performance, often struggling with hallucinations and repeated information. Human evaluation by a clinical expert confirmed the practical utility of the summaries generated by proprietary models. Despite the challenges, such as hallucinations and missing information, the findings suggest that LLMs, especially proprietary models, are promising candidates for automatic discharge summary generation as long as data privacy is ensured.

</details>


### [253] [CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation](https://arxiv.org/abs/2512.06814)
*Dibyanayan Bandyopadhyay,Soham Bhattacharjee,Mohammed Hasanuzzaman,Asif Ekbal*

Main category: cs.CL

TL;DR: 该论文提出了一种新方法CAuSE，用于为多模态分类器生成更忠实于模型决策过程的自然语言解释，并通过理论和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态分类器通常表现为黑盒，现有解释方法不够直观或不够易于访问。自然语言解释（NLE）能提升用户信任，但必须忠实反映模型内部决策（忠实性）。缺乏一个统一且有效的方法为任意预训练多模态分类器生成高忠实度的NLE。

Method: 提出了一种名为CAuSE的框架（Causal Abstraction under Simulated Explanations），采用interchange intervention训练，以实现对原分类器决策过程的因果抽象，并能自动为任意多模态分类器生成NLE。此外设计算法测量解释的因果忠实性。

Result: 在多个数据集和模型上进行广泛实验，结果显示CAuSE方法在因果忠实性指标上明显优于现有方法。定性分析也证实了该方法的优势。

Conclusion: CAuSE为多模态黑盒模型提供了高忠实度的自然语言解释，既提升了透明性，也有利于建立用户信任。并且对潜在失败案例进行了深入分析，为后续改进提供了依据。

Abstract: Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE

</details>


### [254] [AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices](https://arxiv.org/abs/2512.06848)
*Sepyan Purnama Kristanto,Lutfi Hakim,Hermansyah*

Main category: cs.CL

TL;DR: 文章提出了一种名为AquaFusionNet的跨模态轻量化框架，可在低功耗边缘设备上融合微生物显微图像与水质传感器数据，实现更准确、实时的饮用水污染检测。


<details>
  <summary>Details</summary>
Motivation: 在发展中地区，小型饮用水系统受到微生物污染波动显著，现有监测工具无法全面捕捉污染动态。微观图像和理化传感器信息分割，导致实时决策不可靠。需要集成二者以提升监测准确性和时效性。

Method: AquaFusionNet 利用门控跨注意力机制，将历时微观图像和水质传感器数据在同一模型内融合，专为低功耗硬件（如Jetson Nano）设计。模型在AquaMicro12K新建数据集上训练，并与主流轻量化检测器进行对比。

Result: 在印尼东爪哇7个设施现场部署6个月，系统处理184万帧，污染检测事件mAP@0.5达94.8%，异常预测准确率96.3%，功耗4.8W。相比单一模态检测器，准确率更高，对常见失效情况（如污垢、浊度骤升、光照不稳）具有更强鲁棒性。

Conclusion: AquaFusionNet 实现了饮用水微生物污染的实时、精准、低功耗检测，优势明显。数据集、代码和硬件方案已开源，促进了分布式水质安全监控的普及和适应。

Abstract: Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.

</details>


### [255] [Rhea: Role-aware Heuristic Episodic Attention for Conversational LLMs](https://arxiv.org/abs/2512.06869)
*Wanyang Hong,Zhaoning Zhang,Yi Chen,Libo Zhang,Baihui Liu,Linbo Qiao,Zhiliang Tian,Dongsheng Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为Rhea的新型多轮对话大模型结构，通过角色感知和启发式记忆机制，显著提升了多轮对话中的上下文保持能力，有效遏制了长对话下性能衰减问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在单轮任务中表现突出，但在多轮对话中会出现上下文完整性逐步下降（称为累积性上下文衰减），主要原因是注意力污染、稀释与漂移。需要一种机制来维持长对话下的高质量交流。

Method: Rhea将对话历史分为两个相互独立的记忆模块：指导记忆（Instructional Memory, IM）用于长期保存全局约束，采用结构化优先级机制；情景记忆（Episodic Memory, EM）动态管理人机交互信息，利用异步噪声控制和启发式上下文检索。推理时通过优先注意力机制，优先整合全局指令，同时有选择性地纳入相关情景信息。

Result: 在多轮对话基准（如MT-Eval、Long-MT-Bench+）上实验表明，Rhea能有效减缓性能退化，总体精度提升1.04分（相对强基线提升16%），并且在长对话中保持了高指令一致性（IAR>8.1）。

Conclusion: Rhea为构建更精准、指令一致的对话式大语言模型提供了有效且系统的方法，未来有望进一步推动多轮人机对话技术发展。

Abstract: Large Language Models (LLMs) have achieved remarkable performance on single-turn tasks, yet their effectiveness deteriorates in multi-turn conversations. We define this phenomenon as cumulative contextual decay - a progressive degradation of contextual integrity caused by attention pollution, dilution, and drift. To address this challenge, we propose Rhea (Role-aware Heuristic Episodic Attention), a novel framework that decouples conversation history into two functionally independent memory modules: (1) an Instructional Memory (IM) that persistently stores high-fidelity global constraints via a structural priority mechanism, and (2) an Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, Rhea constructs a high signal-to-noise context by applying its priority attention: selectively integrating relevant episodic information while always prioritizing global instructions. To validate this approach, experiments on multiple multi-turn conversation benchmarks - including MT-Eval and Long-MT-Bench+ - show that Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (a 16% relative gain over strong baselines). Moreover, Rhea maintains near-perfect instruction fidelity (IAR > 8.1) across long-horizon interactions. These results demonstrate that Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs.

</details>


### [256] [An Analysis of Large Language Models for Simulating User Responses in Surveys](https://arxiv.org/abs/2512.06874)
*Ziyun Yu,Yiru Zhou,Chen Zhao,Hongyi Wen*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在模拟用户观点时的局限性，提出了多元化观点生成方法CLAIMSIM，但结果显示LLM难以准确模拟不同用户的回答。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在模拟用户意见方面受到关注，尤其用于跨领域问卷调查，作者关注其偏见和代表性不足的问题，特别是LLMs容易偏向主流观点，难以涵盖多元人口和文化背景。

Method: 作者评估了LLMs通过直接提问和思维链提示来模拟用户回答的能力，并提出CLAIMSIM方法，从模型参数化知识中引出多样观点作为上下文输入，在问卷回答任务中做实验对比。

Result: 实验发现CLAIMSIM能生成更丰富多样的回答，但无论是哪种方法LLMs在准确模拟用户回答方面都存在困难。具体地，LLMs对不同人口特征持固定、单一视角，同时在面对相互矛盾观点时无法很好地区分和推理复杂的人口特征差异。

Conclusion: 当前LLMs在模拟用户多元观点和个性化回答方面存在显著不足，需要进一步改进以更好地适应多样化的用户背景和需求。

Abstract: Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.

</details>


### [257] [Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles](https://arxiv.org/abs/2512.06919)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla*

Main category: cs.CL

TL;DR: 本文提出了一种基于历史安全性数据的自动化方法，用于优化肿瘤临床试验中PRO-CTCAE项目的选择，兼顾患者负担与信号覆盖。


<details>
  <summary>Details</summary>
Motivation: 手动选择PRO-CTCAE项目容易过多，增加患者负担，或过少，遗漏重要安全信号。需要一种能自动优化项目数量和覆盖范围的客观方法。

Method: 该方法将候选PRO-CTCAE症状项映射到MedDRA首选术语，并使用Safeterm语义空间进行编码，实现临床和语境多样性的度量。通过综合症状相关性和发生率建立效用函数，并利用谱分析在相关性与多样性之间找到平衡，从而自动筛选项目。工具已集成在Safeterm trial-safety应用中。

Result: 通过模拟和真实肿瘤案例研究，验证了该方法可有效提升PRO-CTCAE设计效率，实现自动化和可复现的项目选择。

Conclusion: 该自动化方法能利用MedDRA语义和历史数据，优化PRO-CTCAE选择，实现信号覆盖与患者负担的平衡，提升试验安全监测的科学性和高效性。

Abstract: The PRO-CTCAE is an NCI-developed patient-reported outcome system for capturing symptomatic adverse events in oncology trials. It comprises a large library drawn from the CTCAE vocabulary, and item selection for a given trial is typically guided by expected toxicity profiles from prior data. Selecting too many PRO-CTCAE items can burden patients and reduce compliance, while too few may miss important safety signals. We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Each candidate PRO-CTCAE symptom term is first mapped to its corresponding MedDRA Preferred Terms (PTs), which are then encoded into Safeterm, a high-dimensional semantic space capturing clinical and contextual diversity in MedDRA terminology. We score each candidate PRO item for relevance to the historical list of adverse event PTs and combine relevance and incidence into a utility function. Spectral analysis is then applied to the combined utility and diversity matrix to identify an orthogonal set of medical concepts that balances relevance and diversity. Symptoms are rank-ordered by importance, and a cut-off is suggested based on the explained information. The tool is implemented as part of the Safeterm trial-safety app. We evaluate its performance using simulations and oncology case studies in which PRO-CTCAE was employed. This automated approach can streamline PRO-CTCAE design by leveraging MedDRA semantics and historical data, providing an objective and reproducible method to balance signal coverage against patient burden.

</details>


### [258] [Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI](https://arxiv.org/abs/2512.06922)
*George Mikros*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLM）对法庭语言学的双重影响：既能赋能分析，又带来新的挑战，特别在作者识别和AI文本检测领域存在诸多科学与法律难题。


<details>
  <summary>Details</summary>
Motivation: LLM在法庭语言学既有助于作者鉴别、文本分析，又因其伪装和生成能力冲击了既有理论和证据规范。随着AI文本泛滥，原有技术难以应对，因此亟需审视和改革相关方法。

Method: 梳理当前LLM相关的文体分析、AI文本检测（包括分类、文体计量、水印法）的原理和实际表现，并结合法律证据标准分析这些方法的科学性和可采信性。

Result: LLM生成文本能模仿表层文体，但与人类写作在检测上仍有差异。现有AI检测技术因高误判率和易被规避，难以满足法律证据（如Daubert和Kumho Tire标准）要求。

Conclusion: 法庭语言学需引入混合人机流程、可解释的检测范式和多样人群的误差与偏见验证体系，才能在AI时代维持科学性与法律采信力。语言仍能揭示作者特征，但需考虑复杂的人机共创背景。

Abstract: Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.

</details>


### [259] [XAM: Interactive Explainability for Authorship Attribution Models](https://arxiv.org/abs/2512.06924)
*Milad Alshomary,Anisha Bhatnagar,Peter Zeng,Smaranda Muresan,Owen Rambow,Kathleen McKeown*

Main category: cs.CL

TL;DR: 本文提出了IXAM，一个用于作者归属模型的可交互解释框架，可帮助用户探索模型嵌入空间并以不同粒度对模型预测进行解释。用户评估显示该框架优于预定义风格解释。


<details>
  <summary>Details</summary>
Motivation: 当前嵌入式作者归属模型难以解释其预测结果，缺乏能够以多粒度解析模型决策过程的工具。

Method: 设计了IXAM框架，使用户可以交互式地探索嵌入空间，并根据用户选择自动生成不同层次写作风格特征的解释。

Result: 通过用户评估，证明IXAM生成的解释比传统预定义风格解释更有价值和灵活性。

Conclusion: IXAM为嵌入式作者归属模型提供了更透明、更能被用户理解的解释，有助于提升模型的可用性和可信度。

Abstract: We present IXAM, an Interactive eXplainability framework for Authorship Attribution Models. Given an authorship attribution (AA) task and an embedding-based AA model, our tool enables users to interactively explore the model's embedding space and construct an explanation of the model's prediction as a set of writing style features at different levels of granularity. Through a user evaluation, we demonstrate the value of our framework compared to predefined stylistic explanations.

</details>


### [260] [Progress Ratio Embeddings: An Impatience Signal for Robust Length Control in Neural Text Generation](https://arxiv.org/abs/2512.06938)
*Ivanhoé Botcazou,Tassadit Amghar,Sylvain Lamprier,Frédéric Saubion*

Main category: cs.CL

TL;DR: 提出并验证了一种新的长度控制方法（PRE），可以让文本生成模型更稳定地控制输出长度，同时保持文本质量。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络文本生成模型虽能生成高质量文本，但对生成文本长度的精确控制仍不足。已有的基于反向位置编码（RPE）的方法在长度控制超出训练分布时表现不佳，容易导致不稳定。

Method: 作者分析了RPE方法的局限性，发现将离散倒计时信号与剩余token计数绑定会让长度控制变得不稳定。为此，提出了Progress Ratio Embeddings（PRE）：将连续嵌入与基于三角函数的'不耐烦'信号相结合。PRE易于集成到标准Transformer结构中。

Result: PRE方法能在不影响文本准确率的前提下，实现稳定的长度控制，并能在未见过的目标长度上良好泛化。在两个常用的新闻摘要数据集上进行的实验验证了其有效性。

Conclusion: PRE嵌入为神经语言模型带来了稳定和泛化性强的长度控制能力，而不会损害文本的生成质量。

Abstract: Modern neural language models achieve high accuracy in text generation, yet precise control over generation length remains underdeveloped. In this paper, we first investigate a recent length control method based on Reverse Positional Embeddings (RPE) and show its limits when control is requested beyond the training distribution. In particular, using a discrete countdown signal tied to the absolute remaining token count leads to instability. To provide robust length control, we introduce Progress Ratio Embeddings (PRE), as continuous embeddings tied to a trigonometric impatience signal. PRE integrates seamlessly into standard Transformer architectures, providing stable length fidelity without degrading text accuracy under standard evaluation metrics. We further show that PRE generalizes well to unseen target lengths. Experiments on two widely used news-summarization benchmarks validate these findings.

</details>


### [261] [Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models](https://arxiv.org/abs/2512.06991)
*Jing Jie Tan,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum,Anissa Mokraoui,Shih-Yu Lo*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的“系列提示”算法PICEPR，通过模块化的解码器型大语言模型提升人格识别，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽在自然语言处理任务中表现优异，但在人格识别领域相关内容生成和特征提取仍有提升空间，亟需新的方法提高识别准确率和实用性。

Method: 提出PICEPR算法，包含内容与嵌入两大处理流程，充分利用模块化解码器型LLM进行内容总结、生成及特征提取，并分别对封闭源模型（如GPT-4o、Gemini）与开源模型（如Mistral）进行了对比实验。

Result: PICEPR在人格识别任务中实现了5-15%的准确率提升，表现优于现有方法。并对多种主流模型的生成内容质量进行了系统对比。

Conclusion: PICEPR作为一种系列化提示及内容嵌入算法，显著增强了大语言模型在人格识别领域的性能，可作为人格特征提取和生成的有力工具。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel "Prompting-in-a-Series" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \textit{gpt4o} from OpenAI and \textit{gemini} from Google, along with open-source models like \textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.

</details>


### [262] [FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations](https://arxiv.org/abs/2512.07015)
*Mayank Ravishankara*

Main category: cs.CL

TL;DR: 本文提出了一种新的FVA-RAG（Falsification-Verification Alignment RAG）框架，能够显著减少大型语言模型在检索增强生成（RAG）系统中的“随声附和”型幻觉，尤其是在面对带有错误前提或常见误解的查询时。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统虽然能降低LLM的幻觉现象，但在面对错误前提的问题时，检索模型倾向于返回支持用户错误认知的内容，从而导致模型“有理有据地胡说八道”，这一现象被称为Retrieval Sycophancy（检索谄媚性），亟需解决。

Method: FVA-RAG将检索范式从以往的归纳式验证（即寻找支持性证据）转为演绎式证伪（即主动寻找反驳性证据）。具体方法包括：设计对抗检索策略，通过生成“Kill Queries”来有针对性地检索可能的反例，并引入“双重验证”机制，将初稿答案与这些反对性证据进行对比权衡。

Result: 在常见误解数据集上的初步实验表明，FVA-RAG相比标准RAG大幅提升了系统在错误前提下应对‘随声附和’型幻觉的鲁棒性。

Conclusion: FVA-RAG为事实型生成任务提供了一种推理阶段的“红队”机制，有效抑制了基于检索的幻觉现象，推动RAG系统更加客观、可靠。

Abstract: Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to "hallucinate with citations."
  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing "Self-Correction" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates "Kill Queries"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this "Anti-Context." Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time "Red Team" for factual generation.

</details>


### [263] [Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models](https://arxiv.org/abs/2512.07059)
*Richard Young*

Main category: cs.CL

TL;DR: 本文通过对具有先进安全对齐措施的大型语言模型进行多轮对抗攻击实验，发现大多数模型在面对多回合复杂对抗时依然高易受攻击，模型规模与鲁棒性无关，采用推理增强方式可显著提升安全性。


<details>
  <summary>Details</summary>
Motivation: 虽然各大公司投入大量资源用于提升大语言模型的安全性，但实际面对复杂多轮对抗攻击时，这些模型的脆弱性尚未被充分揭示，且缺乏对规模和推理方式和鲁棒性关系的系统量化。

Method: 采用TEMPEST多轮攻击框架，针对来自八家厂商的十个前沿大模型，对1,000种有害行为展开自动化攻防实验，总计生成97,000余次API对话，并借助独立安全分类器自动评估攻击效果。

Result: 实验发现，六个模型的攻击成功率高达96%-100%，四个表现较好的模型攻击成功率为42%-78%；而在相同架构下激活‘扩展推理’模式可显著降低攻击成功率（从97%降至42%）。

Conclusion: 现有安全对齐方法在面对适应性强的多轮对抗攻击时仍然存在根本性脆弱，不同厂商产品安全质量差异大，模型规模对抵御对抗攻击无预测作用，采用更具推理能力的推断模式是提升安全性的有前景方法。

Abstract: Despite substantial investment in safety alignment, the vulnerability of large language models to sophisticated multi-turn adversarial attacks remains poorly characterized, and whether model scale or inference mode affects robustness is unknown. This study employed the TEMPEST multi-turn attack framework to evaluate ten frontier models from eight vendors across 1,000 harmful behaviors, generating over 97,000 API queries across adversarial conversations with automated evaluation by independent safety classifiers. Results demonstrated a spectrum of vulnerability: six models achieved 96% to 100% attack success rate (ASR), while four showed meaningful resistance, with ASR ranging from 42% to 78%; enabling extended reasoning on identical architecture reduced ASR from 97% to 42%. These findings indicate that safety alignment quality varies substantially across vendors, that model scale does not predict adversarial robustness, and that thinking mode provides a deployable safety enhancement. Collectively, this work establishes that current alignment techniques remain fundamentally vulnerable to adaptive multi-turn attacks regardless of model scale, while identifying deliberative inference as a promising defense direction.

</details>


### [264] [SETUP: Sentence-level English-To-Uniform Meaning Representation Parser](https://arxiv.org/abs/2512.07068)
*Emma Markle,Javier Gutierrez Bach,Shira Wein*

Main category: cs.CL

TL;DR: 本文介绍了两种将英文文本解析为UMR（统一意义表达）图的方法，其中最佳模型SETUP显著提升了解析精度。


<details>
  <summary>Details</summary>
Motivation: UMR作为新型图结构语义表示，旨在支持多语言语义标注，尤其有助于低资源语言，但其下游应用依赖于自动、准确的大规模UMR解析器。目前针对文本到UMR解析的研究相对有限，因此亟需提升自动解析技术。

Method: 作者提出两类英文文本到UMR图的解析方法：（1）对现有AMR（抽象意义表示）解析器进行微调；（2）利用Universal Dependencies（UD）转换器，并以已有工作为基线进行对比评测。

Result: 最佳模型SETUP取得了AnCast分数84和SMATCH++分数91，显著超过了基线，说明了新方法能够更好地实现UMR自动解析。

Conclusion: 提出的解析方法为UMR自动解析带来了实质性进展，为低资源语言技术和语言学研究中的UMR实际应用奠定了基础。

Abstract: Uniform Meaning Representation (UMR) is a novel graph-based semantic representation which captures the core meaning of a text, with flexibility incorporated into the annotation schema such that the breadth of the world's languages can be annotated (including low-resource languages). While UMR shows promise in enabling language documentation, improving low-resource language technologies, and adding interpretability, the downstream applications of UMR can only be fully explored when text-to-UMR parsers enable the automatic large-scale production of accurate UMR graphs at test time. Prior work on text-to-UMR parsing is limited to date. In this paper, we introduce two methods for English text-to-UMR parsing, one of which fine-tunes existing parsers for Abstract Meaning Representation and the other, which leverages a converter from Universal Dependencies, using prior work as a baseline. Our best-performing model, which we call SETUP, achieves an AnCast score of 84 and a SMATCH++ score of 91, indicating substantial gains towards automatic UMR parsing.

</details>


### [265] [Do Large Language Models Truly Understand Cross-cultural Differences?](https://arxiv.org/abs/2512.07075)
*Shiwei Guo,Sihang Jiang,Qianxi He,Yanghua Xiao,Jiaqing Liang,Bi Yude,Minggui He,Shimin Tao,Li Zhang*

Main category: cs.CL

TL;DR: 本文提出了SAGE评测基准，用于系统性评估大语言模型（LLMs）的跨文化理解与推理能力，揭示现有模型在深层次文化推理上的不足，并为未来研究提供数据支持。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多语种任务上表现优异，但在跨文化理解能力评估方面，现有基准存在缺乏真实情境、跨文化概念映射不足、深层文化推理能力有限等问题。为弥补这些缺陷，作者希望建立更科学有效的评测基准。

Method: 作者基于文化理论，将跨文化能力分为九个维度，整理了210个核心概念，并通过生成式任务设计，在15类真实场景下，覆盖四大跨文化情境类别，共构建了4530个测试项。SAGE数据集设计科学，便于按需扩展，并能支持多语言迁移评测。

Result: 实验结果表明SAGE能够揭示现有LLMs在各个跨文化能力维度和具体场景下的系统性弱点，数据集迁移性良好，有助于持续扩展和后续研究。

Conclusion: 目前的大语言模型在跨文化深度推理和理解上仍有显著提升空间。SAGE为未来相关研究提供了坚实基础，有利于更全面科学地评估和推动大模型跨文化智能的发展。

Abstract: In recent years, large language models (LLMs) have demonstrated strong performance on multilingual tasks. Given its wide range of applications, cross-cultural understanding capability is a crucial competency. However, existing benchmarks for evaluating whether LLMs genuinely possess this capability suffer from three key limitations: a lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. To address these gaps, we propose SAGE, a scenario-based benchmark built via cross-cultural core concept alignment and generative task design, to evaluate LLMs' cross-cultural understanding and reasoning. Grounded in cultural theory, we categorize cross-cultural capabilities into nine dimensions. Using this framework, we curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios, organized under four broader categories of cross-cultural situations, following established item design principles. The SAGE dataset supports continuous expansion, and experiments confirm its transferability to other languages. It reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. While progress has been made, LLMs are still some distance away from reaching a truly nuanced cross-cultural understanding. In compliance with the anonymity policy, we include data and code in the supplement materials. In future versions, we will make them publicly available online.

</details>


### [266] [Leveraging KV Similarity for Online Structured Pruning in LLMs](https://arxiv.org/abs/2512.07090)
*Jungmin Lee,Gwangeun Byeon,Yulhwa Kim,Seokin Hong*

Main category: cs.CL

TL;DR: 本文提出了一种名为Token Filtering的在线结构化剪枝方法，通过在推理过程中动态跳过冗余token的注意力计算，加速大语言模型（LLM）推理。在各大模型上，该方法在保留高准确率的同时，实现了大幅计算加速。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM剪枝方法主要依赖于离线校准数据，导致泛化性差、推理时不稳定。希望发展一种无需校准数据且具备更强稳定性的剪枝方法。

Method: 提出Token Filtering，一种在推理阶段基于token联合key-value相似性进行冗余性判别、跳过多余注意力计算的轻量级剪枝方法；并提出基于方差感知的融合策略动态加权，增强重要token保留能力，且无额外显存开销。

Result: 在LLaMA-2、LLaMA-3和Mistral等主流模型上进行实验，Token Filtering在高剪枝率（如50%）下依旧保持了推理性能，并且在常识推理和MMLU等任务上优于以往方案。

Conclusion: Token Filtering方法能够无需校准数据地实现高效稳定的在线结构化剪枝，显著提升LLM推理效率，同时几乎不影响模型下游任务表现。

Abstract: Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.

</details>


### [267] [DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning](https://arxiv.org/abs/2512.07132)
*Nithin Sivakumaran,Justin Chih-Yao Chen,David Wan,Yue Zhang,Jaehong Yoon,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 本文提出了DART，一个通过多代理辩论识别和调用视觉专家工具（如目标检测、OCR、空间推理等）来提升多模态理解能力的多代理框架，并在多个基准数据集上取得了优于现有多代理和单代理方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型或视觉语言模型虽然可以通过视觉工具增强专业知识，但如何有效确定何时、调用何种工具存在挑战，尤其是在多代理协作场景下处理复杂问题时。

Method: DART框架借助多个视觉代理的辩论，利用其分歧（disagreement）来有针对性地调用能解决争议的专家工具，引入新信息，并通过工具对齐的评分机制辅助讨论，最终由一个聚合代理整合代理输出和工具信息选择最佳答案。

Result: 在A-OKVQA、MMMU等四个基准数据集上，DART分别比多代理评判模型基线高出3.4%和2.4%；在医学领域M3D数据集上也取得了1.3%的提升，并证明了DART能适应新领域工具。DART的讨论丰富性和工具调用多样性也超过现有多代理方法。

Conclusion: DART能有效利用代理分歧针对性调用视觉专家工具，增强多代理讨论和推理能力，在多个领域和任务中均优于以往方法，并具良好的扩展性和适应性。

Abstract: Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.

</details>


### [268] [GUMBridge: a Corpus for Varieties of Bridging Anaphora](https://arxiv.org/abs/2512.07134)
*Lauren Levine,Amir Zeldes*

Main category: cs.CL

TL;DR: 本文提出了GUMBridge，一个涵盖16种英语体裁、专用于bridging anaphora现象的新型语料库，并展示了该语料库的标注质量评估和利用现代LLM进行相关任务时的基线表现。结果显示该现象的自动处理难度仍高。


<details>
  <summary>Details</summary>
Motivation: 现有英语bridging anaphora资源数量少、覆盖范围有限，无法满足对不同体裁和现象细分类别的研究需求。

Method: 作者开发了GUMBridge语料库，囊括16种体裁，并对bridging anaphora现象进行细致标注；此外，评估了标注质量，并使用当代LLM在该数据集上的相关任务进行实验。

Result: GUMBridge实现了对bridging anaphora丰富、细致的覆盖。标注质量经过系统评估。使用开放/闭源LLM在桥接消解和子类型分类等任务上的基线表现均表明，这些任务对当前技术仍具挑战性。

Conclusion: GUMBridge拓展了bridging anaphora的资源基础，有助于多体裁、多类别研究，但实验表明该现象的自动识别和分类仍是NLP领域挑战。

Abstract: Bridging is an anaphoric phenomenon where the referent of an entity in a discourse is dependent on a previous, non-identical entity for interpretation, such as in "There is 'a house'. 'The door' is red," where the door is specifically understood to be the door of the aforementioned house. While there are several existing resources in English for bridging anaphora, most are small, provide limited coverage of the phenomenon, and/or provide limited genre coverage. In this paper, we introduce GUMBridge, a new resource for bridging, which includes 16 diverse genres of English, providing both broad coverage for the phenomenon and granular annotations for the subtype categorization of bridging varieties. We also present an evaluation of annotation quality and report on baseline performance using open and closed source contemporary LLMs on three tasks underlying our data, showing that bridging resolution and subtype classification remain difficult NLP tasks in the age of LLMs.

</details>


### [269] [MASim: Multilingual Agent-Based Simulation for Social Science](https://arxiv.org/abs/2512.07195)
*Xuan Zhang,Wenxuan Zhang,Anxu Wang,See-Kiong Ng,Yang Deng*

Main category: cs.CL

TL;DR: 本论文提出了MASim，这是第一个支持多语言多智能体角色扮演的模拟框架，能够研究跨语言的社会行为和信息传播，并通过新的基准数据集和分析任务验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体社交模拟主要局限于单一语言，难以反映现实社会中的跨语言互动和多元文化现象。因此需要新的工具来填补多语言、多文化模拟的空白，从而推动更真实和更大规模的社会科学研究。

Method: 作者提出了MASim多语言模拟框架，支持拥有不同语言和社会属性的生成型智能体进行多轮互动。框架涵盖全球舆论建模和媒体影响分析两大应用场景，并构建了MAPS基准数据集，通过涵盖全球人口分布的问卷与画像设定，进行多维实验和案例分析。

Result: 实验包括模型校准、敏感性、结果一致性，以及文化案例等，显示MASim能有效再现实际社会现象，能够模拟多语言环境下的舆论传播与媒体影响。

Conclusion: MASim为可扩展、可控的计算社会科学研究提供了强力工具，强调了多语言、多文化模拟在理解社会行为与传播的独特作用和重要性。

Abstract: Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.

</details>


### [270] [NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models](https://arxiv.org/abs/2512.07218)
*Feng Liang,Weixin Zeng,Runhao Zhao,Xiang Zhao*

Main category: cs.CL

TL;DR: 本文提出了NeSTR框架，通过结合符号表示和混合反思思维，有效提升了大语言模型的复杂时序推理能力，实现了更优的零样本时序问答表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在自然语言处理任务中表现优异，但在涉及复杂时序约束的推理任务上仍面临较大挑战。现有的符号方法未能充分利用LLMs的推理能力，而反思机制又缺乏结构化的时序表示，导致推理不一致或产生幻觉。为了弥补这两者的不足，作者提出一种新框架以提升时序推理的准确性和一致性。

Method: 提出NeSTR（Neuro-Symbolic Temporal Reasoning）框架，将结构化符号时序表示与混合型反思思维结合。该框架：1）通过符号编码显式保留时序关系；2）借助逻辑验证保障推理一致性；3）通过溯因反思修正推理错误。整体方法无需微调，适用于各类时序问答基准测试。

Result: 大量实验证明，NeSTR在多种时序问答基准上实现了更优的零样本表现，有效提升了大模型的时序敏感推理能力。

Conclusion: 神经-符号一体化能显著提升大型语言模型在时序理解上的能力，NeSTR框架为复杂时序推理任务提供了新的可行途径。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.

</details>


### [271] [Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection](https://arxiv.org/abs/2512.07246)
*Mengqi Wang,Jianwei Wang,Qing Liu,Xiwei Xu,Zhenchang Xing,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新的表格数据错误检测方法，利用大语言模型（LLM）归纳生成决策树，从而提升检测结果的可解释性和鲁棒性，并通过集成多个决策树进一步提升性能。实验结果显示该方法在准确性、可解释性和鲁棒性上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有错误检测方法依赖于LLM直接标记错误，但这导致决策过程黑盒、缺乏可解释性，并且对提示词敏感，结果不稳定。作者希望解决这些问题，提升错误检测系统的可解释性和鲁棒性。

Method: 提出“LLM-as-an-inducer”框架，利用LLM引导生成决策树（TreeED），决策树包含规则节点、GNN节点和叶子节点。再通过子集采样和TreeED构建多颗决策树（ForestED），用EM算法联合估计树的可靠性并优化共识预测。

Result: 该方法在多个数据集上实验，显示其准确率、可解释性和鲁棒性均优于基线方法，平均F1分数提升16.1%。

Conclusion: 通过让LLM归纳决策树并集成树的方法，可以显著提升错误检测系统的性能、可解释性与鲁棒性。这为数据质量管理提供了更有效的解决方案。

Abstract: Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is highly sensitive to prompts, yielding inconsistent outputs due to inherent model stochasticity, therefore lacking robustness. To address these limitations, we propose an LLM-as-an-inducer framework that adopts LLM to induce the decision tree for ED (termed TreeED) and further ensembles multiple such trees for consensus detection (termed ForestED), thereby improving explainability and robustness. Specifically, based on prompts derived from data context, decision tree specifications and output requirements, TreeED queries the LLM to induce the decision tree skeleton, whose root-to-leaf decision paths specify the stepwise procedure for evaluating a given sample. Each tree contains three types of nodes: (1) rule nodes that perform simple validation checks (e.g., format or range), (2) Graph Neural Network (GNN) nodes that capture complex patterns (e.g., functional dependencies), and (3) leaf nodes that output the final decision types (error or clean). Furthermore, ForestED employs uncertainty-based sampling to obtain multiple row subsets, constructing a decision tree for each subset using TreeED. It then leverages an Expectation-Maximization-based algorithm that jointly estimates tree reliability and optimizes the consensus ED prediction. Extensive xperiments demonstrate that our methods are accurate, explainable and robust, achieving an average F1-score improvement of 16.1% over the best baseline.

</details>


### [272] [TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation](https://arxiv.org/abs/2512.07265)
*Bhavana Akkiraju,Srihari Bandarupalli,Swathi Sambangi,Vasavi Ravuri,R Vijaya Saraswathi,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: 该论文提出了高质量的泰卢固语到英语的语音翻译基准数据集，系统比较了级联系统和端到端系统，并对常用自动评测指标的可靠性进行了研究。


<details>
  <summary>Details</summary>
Motivation: 尽管泰卢固语有8000多万使用者，但该语言的语音翻译研究十分稀缺。为弥补这一空白，论文着手构建高质量的语音翻译基准并深入研究不同建模方法。

Method: 作者基于46小时的人工校验语料，构建了泰卢固语—英语翻译基准集；系统对比了级联（如IndicWhisper+IndicMT）与端到端模型（如SeamlessM4T微调）的性能表现，同时评估了6种自动评测指标与人工评判的一致性。

Result: 级联模型因大量泰卢固语专有数据取得最优表现；但经过微调的端到端模型在极少量专有数据的情况下也有可观竞争力。部分评测指标（如BLEU、ChrF++等）在泰卢固语-英语任务中对译文质量有优于BERTScore的判别力。

Conclusion: 此工作贡献了首个可复现泰卢固语-英语语音翻译基准，证实了少资源场景下端到端模型的潜力，并为复杂语言对自动评测策略提供了指导。

Abstract: Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.

</details>


### [273] [Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data](https://arxiv.org/abs/2512.07277)
*Srihari Bandarupalli,Bhavana Akkiraju,Charan Devarakonda,Vamsiraghusimha Narsinga,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: 此论文针对低资源语种的自动语音识别（ASR）难题，提出高效利用无标签语音数据和策略性预训练的新方法，在参数规模远小于主流大模型的情况下取得了同等或更好的识别效果。


<details>
  <summary>Details</summary>
Motivation: 当前ASR领域主流模型参数庞大、训练数据需求极高，不适用于低资源语种。如何在计算资源有限和标注数据稀缺的情况下提升ASR性能，成为亟需解决的问题。

Method: 作者以波斯语、阿拉伯语和乌尔都语为例，通过自动化无标签多语种语音数据收集，构建了3000小时语料库。采用了持续预训练和考虑形态学的分词方式，在300M参数量下训练模型，与当前主流1.5B参数模型进行了对比。

Result: 提出的模型在波斯语超过Whisper Large v3（1.5B参数），在阿拉伯语和乌尔都语上也表现竞争力。所需标注数据和计算资源远小于对比系统。

Conclusion: 结果表明，ASR系统性能与模型规模并不简单呈正相关，相关性更大的因素是数据选择和预训练策略。为低资源语种ASR提供了切实可行的技术路径，可减少对大型计算平台和专有数据的依赖，促进技术普惠。

Abstract: Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.

</details>


### [274] [Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models](https://arxiv.org/abs/2512.07288)
*Tomoki Doi,Masaru Isonuma,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本研究探讨如何提升大语言模型生成自我解释时的忠实性，并分析忠实性提升是否能在不同解释风格间泛化。通过构建受限于单词的解释和持续学习，发现训练能提升模型在多任务和风格下自我解释的忠实性及泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型能根据用户指令生成多种风格的自我解释，但已有研究发现这些解释常常缺乏对模型真实推理过程的忠实反映，且提升忠实性的有效方法及泛化能力尚未得到充分探索。

Method: 作者选用三项分类任务及三种解释风格，采用特征归因方法生成受单词数限制且更可能忠实的伪自我解释，然后对指令微调模型进行持续学习，评估训练对解释忠实性及其泛化的影响。

Result: 训练显著提升了模型在所有分类任务和解释风格下自我解释的忠实性，且该提升对多词解释和未见任务也有一定泛化性。同时，三种风格之间也表现出一致的交叉泛化趋势。

Conclusion: 通过利用伪忠实的自解释持续训练，大语言模型的自解释忠实性在多任务、多风格场景下均能提升，并且改进有一定的风格和任务泛化能力。

Abstract: Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models' actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.

</details>


### [275] [Multilingual corpora for the study of new concepts in the social sciences and humanities:](https://arxiv.org/abs/2512.07367)
*Revekka Kyriakoglou,Anna Pappa*

Main category: cs.CL

TL;DR: 本文提出了一种混合构建多语种语料库的方法，用于研究人文和社会科学中涌现的新概念，以“非技术创新”为案例。方法结合公司网站文本和年度报告，并通过自动处理流程清洗、过滤和结构化元数据，最终生成可用于NLP和机器学习的数据集。


<details>
  <summary>Details</summary>
Motivation: 针对人文和社科领域中的新兴概念缺乏适用于多语种和机器学习分析的高质量语料库，尤其关注“非技术创新”这一具体案例，旨在帮助研究者系统分析相关词汇及其应用。

Method: 混合采集公司网站（英文、法文）内容和年度报告，通过自动语言识别、内容过滤、片段提取和结构化元数据增强，生成基础语料；随后对包含专家词汇的句块进行语境提取和主题标注，得到用于分类任务的训练数据。

Result: 最终构建了一个可重现、可扩展的多语种语料库，生成了配套的英文机器学习数据集，能够支持词汇变异性分析和自然语言处理应用。

Conclusion: 所提出方法能够有效支撑社科领域新概念研究，所得资源便于复现和扩展，具有实际分析和NLP应用价值。

Abstract: This article presents a hybrid methodology for building a multilingual corpus designed to support the study of emerging concepts in the humanities and social sciences (HSS), illustrated here through the case of ``non-technological innovation''. The corpus relies on two complementary sources: (1) textual content automatically extracted from company websites, cleaned for French and English, and (2) annual reports collected and automatically filtered according to documentary criteria (year, format, duplication). The processing pipeline includes automatic language detection, filtering of non-relevant content, extraction of relevant segments, and enrichment with structural metadata. From this initial corpus, a derived dataset in English is created for machine learning purposes. For each occurrence of a term from the expert lexicon, a contextual block of five sentences is extracted (two preceding and two following the sentence containing the term). Each occurrence is annotated with the thematic category associated with the term, enabling the construction of data suitable for supervised classification tasks. This approach results in a reproducible and extensible resource, suitable both for analyzing lexical variability around emerging concepts and for generating datasets dedicated to natural language processing applications.

</details>


### [276] [Training Language Models to Use Prolog as a Tool](https://arxiv.org/abs/2512.07407)
*Niklas Mellgren,Peter Schneider-Kamp,Lukas Galke Poech*

Main category: cs.CL

TL;DR: 本文提出通过结合Prolog形式化工具与强化学习，提升大语言模型（如Qwen2.5-3B-Instruct）在可靠推理与验证能力上的表现和安全性。实验证明此方法实现了比同等规模监督微调更优的表现，且零样本泛化能力大幅提升。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在执行推理和工具调用任务时，常常生成看似合理但实际上错误且难以验证的答案，这对安全领域应用提出挑战。该研究旨在提高AI系统的推理可靠性和可审计性，降低因错误推理带来的风险。

Method: 作者采用了Group Relative Policy Optimization (GRPO)强化学习方法，利用GSM8K-Prolog-Prover数据集，对Qwen2.5-3B-Instruct模型进行微调。实验中变化的因素包括提示结构、奖励组成（执行、语法、语义、结构）以及推理协议（单步、best-of-N与两种agentic模式），比较强化学习与监督微调的效果。

Result: 强化学习方式优于传统监督微调，3B模型无需额外训练即可达到与7B模型few-shot相当的表现。发现联合优化提示、奖励和推理方式可以显著改善模型的程序语法与逻辑结构，在GSM8K数据集上，通过外部Prolog验证选择最佳预测结果，可最大化准确率。在MMLU-Stem和MMLU-Pro集上，agentic内部修复推理模式能获得最优零样本泛化性能。

Conclusion: 通过将大模型推理过程与外部形式化验证（如Prolog）紧密结合，可以显著提升AI在安全关键任务下的推理可靠性与可审计性。该方法为大语言模型安全应用提供了新范式，相关代码已公开。

Abstract: Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference

</details>


### [277] [Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning](https://arxiv.org/abs/2512.07454)
*Amir Mohammad Akhlaghi,Amirhossein Shabani,Mostafa Abdolmaleki,Saeed Reza Kheradpisheh*

Main category: cs.CL

TL;DR: 本文提出了一种高效方法，使用资源有限的训练流程，将微软Phi-3 Mini 英文大模型成功迁移到波斯语，推出了3.8B参数的Persian-Phi模型，在多个评测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多语言大模型的训练通常需要极高的计算资源，对于低资源语言如波斯语尤为困难，缺乏有效的经济可行方案。本研究旨在证明小参数单语模型也可通过创新流程高效迁移到低资源语言，有助于实现AI的民主化。

Method: 方法包括三个阶段：首先使用中英双语小故事进行'预热'，对模型嵌入进行初步校准；接着进行持续预训练；最后通过参数高效微调（PEFT）进行指令调优。整个流程兼顾效果和资源效率。

Result: Persian-Phi在 HuggingFace 的 Open Persian LLM Leaderboard 上取得了有竞争力的成绩，显示出即使模型规模较小，经过精心设计训练流程依然能够在特定低资源语言任务中表现优异。

Conclusion: 本文方法为将主流大模型迁移应用于低资源语言提供了一种实证、可扩展、高性价比的解决思路，有助于推动更多语言社区平等享有顶级AI工具。Persian-Phi 模型已公开发布。

Abstract: The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique "warm-up" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.

</details>


### [278] [Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning](https://arxiv.org/abs/2512.07461)
*Tong Wu,Yang Liu,Jun Bai,Zixia Jia,Shuyi Zhang,Ziyong Lin,Yanting Wang,Song-Chun Zhu,Zilong Zheng*

Main category: cs.CL

TL;DR: 本文提出了NPR（Native Parallel Reasoner），一个无需教师的框架，使大语言模型（LLMs）能够自我进化出真正的并行推理能力，实现从串行到并行的质的飞跃，在多项推理任务上均取得了显著性能提升和推理速度加速。


<details>
  <summary>Details</summary>
Motivation: 目前的大语言模型主要以自回归（串行）方式推理，导致效率和推理能力受限，难以发挥并行推理的潜力。现有提升并行性的做法往往依赖教师信号或简单的并行模拟，缺乏模型自身自发提升并行性的机制。因此，亟需一种无需教师指导，能让模型自主进化出原生并行推理能力的方法。

Method: NPR包括三大创新：（1）自蒸馏渐进训练范式，让模型从无监督的格式探索逐步过渡到严格的拓扑约束，不依赖外部教师；（2）创新的PAPO（Parallel-Aware Policy Optimization）分支策略优化算法，在执行图内直接优化推理分支，模型可通过试错自主学习任务分解方式；（3）NPR Engine重构了SGLang的内存管理和流程控制，支持大规模、稳定、并行的强化学习训练。

Result: 在八项推理基准测试上，基于Qwen3-4B训练的NPR实现了最高24.5%的性能提升与最多4.6倍的推理速度加速。与基线相比，NPR不再依赖自回归解码，达成了100%本地并行执行。

Conclusion: NPR框架为大语言模型赋予了自我进化的原生并行泛化推理能力，实现了高效、可扩展的自主智能体推理标准，有望推动大模型推理效率和能力的全新突破。

Abstract: We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.

</details>


### [279] [Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization](https://arxiv.org/abs/2512.07478)
*Zhuoran Zhuang,Ye Chen,Jianghao Su,Chao Luo,Luhui Liu,Xia Zeng*

Main category: cs.CL

TL;DR: 本文提出了两种新的方法（PRS和VSPO），以提升大语言模型在工具整合推理任务中的强化学习表现，显著改善了奖励稀疏和梯度退化等问题，最终在各类问答基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前基于强化学习训练具备工具整合推理能力的大语言模型时，常面临奖励稀疏、缺乏中间激励和梯度退化等问题，影响模型学习效率和表现。作者希望针对这些关键瓶颈提出有效改进方法。

Method: 作者提出了渐进式奖励塑形（PRS）和基于价值的采样策略优化（VSPO）：PRS通过引入分阶段、密集的奖励体系，逐步引导模型先学会格式正确的调用工具，再优化事实正确性和答案质量；VSPO则在采样时根据任务价值挑选更有效的样本，并采用平滑裁剪稳定梯度更新，提升采样效率和训练稳定性。

Result: 在多个短答和长答任务基准上，PRS较传统二值奖励有更好效果；VSPO相比PPO、GRPO等主流方法收敛更快、结果更稳定，最终性能更好。结合使用时，两者能够提升模型跨领域泛化能力。

Conclusion: PRS和VSPO两项方法有效解决了以往方法在TIR任务中的主要瓶颈，显著提升了大模型在复杂工具整合推理及问答场景下的训练效果和泛化表现。

Abstract: Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.

</details>


### [280] [SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG](https://arxiv.org/abs/2512.07515)
*Pengqian Lu,Jie Lu,Anjin Liu,Guangquan Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种新的方法（SPAD）用于检测RAG系统中的幻觉，并取得了当前最佳的检测效果。


<details>
  <summary>Details</summary>
Motivation: RAG（检索增强生成）系统中幻觉难以检测，现有方法大多将幻觉归因于内部知识和检索语境的二元冲突，但忽略了生成过程中的其他重要因素。论文旨在全面分析和量化各个生成组件对幻觉的影响，从而提升检测准确性。

Method: 提出SPAD方法，将每个token的生成概率归因到七个不同来源：用户查询（Query）、检索内容（RAG）、过去token（Past）、当前token（Current Token）、FFN、最终层归一化（Final LayerNorm）、初始嵌入（Initial Embedding），并按词性聚合得分，分析不同生成成分对特定语言类别的驱动作用，通过异常分布检测幻觉。

Result: SPAD通过大量实验在检测RAG系统的幻觉上取得了SOTA的结果，显示出对特定词性异常依赖（如名词依赖Final LayerNorm）能有效指示幻觉。

Conclusion: SPAD方法为RAG系统的细粒度幻觉检测提供了更准确、全面的解决工具，并突破了传统二元冲突视角的局限，在实际应用中效果显著。

Abstract: Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance

</details>


### [281] [LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings](https://arxiv.org/abs/2512.07522)
*Sebastian Sztwiertnia,Felix Friedrich,Kristian Kersting,Patrick Schramowski,Björn Deiseroth*

Main category: cs.CL

TL;DR: 该论文提出了一种名为LIME的方法，将语言元数据直接融入语言模型的训练过程，显著提升训练效率和任务效果，并提出了可引导生成的变体LIME+1。


<details>
  <summary>Details</summary>
Motivation: 现有的解码器型语言模型高度依赖大规模高质量训练数据，而这些数据日益稀缺。尽管元数据普遍用于数据准备，但其作为直接训练信号的潜力尚未被充分挖掘。

Method: 作者提出LIME（Linguistic Metadata Embeddings）方法，将语法、语义、上下文等元数据信息编码进token嵌入，以丰富训练信号。方法引入极小的参数量与计算开销，并能提升分词效果。同时，提出变体LIME+1，通过引入“前瞻”元数据，引导下一个token生成。

Result: LIME的训练效率提升显著，模型对训练分布的适应速度提高了最高56%，新增参数量极低（0.01%），计算开销可以忽略。LIME还能改善分词策略，提升语言建模和生成任务性能。这些提升在500M到2B参数量级模型上均有效。LIME+1则能利用下一个token的元数据，将推理性能提升至多38%，算术精度提升至多35%。

Conclusion: LIME证明了直接使用语言元数据作为训练信号对提升模型训练效率和效果尤为有效，为破解高质量数据短缺问题提供了新思路。同时，LIME+1为可控文本生成和推理带来了显著的准确率提升。

Abstract: Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.

</details>


### [282] [Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs](https://arxiv.org/abs/2512.07525)
*Xiaoran Liu,Yuerong Song,Zhigeng Liu,Zengfeng Huang,Qipeng Guo,Zhaoxiang Liu,Shiguo Lian,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了基于RoPE的位置编码改进方法，利用原本被忽略的复数点积虚部信息，提升了长文本依赖建模能力，并在多个长上下文基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统RoPE编码只利用了复数点积的实部，忽略了虚部的相位信息，这可能导致序列中重要位置依赖关系的丢失，尤其是在长文本场景下；因此作者希望改进这一局限。

Method: 作者提出利用复数点积的全部信息（实部和虚部），设计出双分量注意力得分，理论上证明并实证此方法能保留更多位置信息，有助于长文本建模。

Result: 在多个长上下文的语言建模基准测试中，该方法相较标准RoPE表现出更佳且随上下文长度增长愈发显著的效果提升。

Conclusion: 重新引入并利用RoPE被忽略的虚部信息明显提升了长文本依赖建模能力，未来大模型编码时可考虑全面利用复杂空间信息。

Abstract: Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.

</details>


### [283] [SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents](https://arxiv.org/abs/2512.07538)
*Michelle Wastl,Jannis Vamvas,Rico Sennrich*

Main category: cs.CL

TL;DR: 本文提出了SwissGov-RSD数据集，这是首个针对跨语言、篇章级语义差异识别的自然语料集，同时测试了多种主流大模型，但自动化方法在该任务上表现仍远低于现有句子级、单语或合成数据任务。


<details>
  <summary>Details</summary>
Motivation: 跨语言和长文本级别的语义差异识别对于自动文本生成评测和多语言内容对齐很关键，但目前相关研究和数据集较为缺乏，尤其是自然语料和细粒度注释的数据。

Method: 构建并发布了SwissGov-RSD数据集，包括英-德、英-法、英-意多语言平行文档，共224份，并由人工逐词标注语义差异；采用多种主流开源和封闭源大模型及编码器，测试不同微调策略下的性能。

Result: 所有目前的大语言模型（LLM）及编码器模型在该新任务上的表现均较差，远逊于它们在单语、句级或人工合成任务的数据表现，说明存在显著方法/能力差距。

Conclusion: 篇章级、跨语言真实语料下的语义差异识别仍具挑战，现有模型难以胜任，对该领域的新数据和改进模型方法有迫切需求。

Abstract: Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations by human annotators. We evaluate a variety of open-source and closed source large language models as well as encoder models across different fine-tuning settings on this new benchmark. Our results show that current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models. We make our code and datasets publicly available.

</details>


### [284] [Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation](https://arxiv.org/abs/2512.07540)
*Boxuan Lyu,Haiyue Song,Hidetaka Kamigaito,Chenchen Ding,Hideki Tanaka,Masao Utiyama,Kotaro Funakoshi,Manabu Okumura*

Main category: cs.CL

TL;DR: 本文提出了在自动机器翻译评价的错误区间检测任务中，采用最小贝叶斯风险（MBR）解码以替代最大后验概率（MAP）解码，从而更好地拟合人工标注数据，并通过MBR蒸馏技术解决计算开销高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有ESD方法通常假设模型概率与人工标注高度一致，但实际上模型生成的高概率结果未必最接近人工标注，导致评价结果可能不准确。为此作者希望改善该局限。

Method: 作者将MBR解码方法应用于生成式ESD模型，结合句级与区间级相似性作为效用函数选择与人工标注最相似的候选解。同时，提出MBR蒸馏，让标准贪婪模型在推断时也能接近MBR解码的效果，从而降低推理成本。

Result: 实验表明，MBR解码在系统、句子和区间级别上均优于传统的MAP基线。MBR蒸馏后，普通贪婪解码模型也可达到MBR解码的性能。

Conclusion: MBR解码能有效提升错误区间检测的生成模型表现，并通过蒸馏技术消除其计算瓶颈，对实际部署更为友好。

Abstract: Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.

</details>


### [285] [Most over-representation of phonological features in basic vocabulary disappears when controlling for spatial and phylogenetic effects](https://arxiv.org/abs/2512.07543)
*Frederic Blum*

Main category: cs.CL

TL;DR: 本研究检验了基础词汇中语音象征性普遍模式的稳健性，发现许多先前认为的普遍规律在控制语言亲属关系和地理分布后不再成立。


<details>
  <summary>Details</summary>
Motivation: 以往对基础词汇中语音特征普遍性的研究，常受样本选择和模型偏倚影响，且未充分控制语言之间的亲缘和地理相关性，从而质疑普遍性结论的可信度。

Method: 对比复现了先前对245种语言语音象征性的分析，扩大样本到2864种语言，使用Lexibank数据集。对原有模型进行了修正，引入地理和系谱统计控制，严密剔除了语言间非独立性的干扰。

Result: 在加入严格控制后，绝大多数原本被认为显著的语音象征模式不再重现，许多模式完全消失。仅有极少数模式能在新数据和模型中保持高度一致和稳定。

Conclusion: 许多关于语音象征性的普遍性结论在大规模、严格控制下难以成立。今后此类语言普遍性主张需在多层面进行稳健性测试以确保可靠性。

Abstract: The statistical over-representation of phonological features in the basic vocabulary of languages is often interpreted as reflecting potentially universal sound symbolic patterns. However, most of those results have not been tested explicitly for reproducibility and might be prone to biases in the study samples or models. Many studies on the topic do not adequately control for genealogical and areal dependencies between sampled languages, casting doubts on the robustness of the results. In this study, we test the robustness of a recent study on sound symbolism of basic vocabulary concepts which analyzed245 languages.The new sample includes data on 2864 languages from Lexibank. We modify the original model by adding statistical controls for spatial and phylogenetic dependencies between languages. The new results show that most of the previously observed patterns are not robust, and in fact many patterns disappear completely when adding the genealogical and areal controls. A small number of patterns, however, emerges as highly stable even with the new sample. Through the new analysis, we are able to assess the distribution of sound symbolism on a larger scale than previously. The study further highlights the need for testing all universal claims on language for robustness on various levels.

</details>


### [286] [MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue](https://arxiv.org/abs/2512.07544)
*Kyungro Lee,Dongha Choi,Hyunju Lee*

Main category: cs.CL

TL;DR: 提出了一种名为MoCoRP的新框架，通过显式建模个性与回复的关系，提升个性对话系统的人设一致性和对话质量。


<details>
  <summary>Details</summary>
Motivation: 现有个性对话系统数据集未显式标注个性句和回复的关系，导致模型难以准确利用个性信息，因而生成的人设不连贯或不具体。

Method: 提出MoCoRP框架，利用NLI（自然语言推理）专家来明确提取个性句与回复之间的NLI关系，将这些关系融入语言模型。MoCoRP被应用到BART等预训练模型，并可通过对齐调优扩展到大语言模型（LLMs）上。

Result: 在ConvAI2和MPChat等公开数据集上，MoCoRP优于现有方法，无论在量化指标还是主观评价上都显著提升了个性一致性和对话的相关性与吸引力。

Conclusion: 通过显式建模个性-回复关系，可以有效提升个性对话系统的人设一致性与对话质量。MoCoRP为未来相关研究提供了新的思路和工具。

Abstract: As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.

</details>


### [287] [Performance of the SafeTerm AI-Based MedDRA Query System Against Standardised MedDRA Queries](https://arxiv.org/abs/2512.07552)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: 本文评估了SafeTerm自动医学查询（AMQ）系统在药物不良事件信号检测中的表现，结果表明其在召回率和精确率之间能取得较好平衡，是MedDRA自动查询生成的可行补充方法。


<details>
  <summary>Details</summary>
Motivation: 在药物上市前安全性评估中，将相关不良事件归类到SMQs/OCMQs对于信号检测十分关键，传统方法效率有限，因此需开发自动化、智能化工具提升检索效率和准确性。

Method: 作者提出并测试了一种基于人工智能的AMQ系统——SafeTerm。该系统将医学术语和MedDRA术语编码到多维向量空间，利用余弦相似度和极值聚类生成相关PT排序列表，对110个SMQ进行验证，并在不同阈值下计算精确率、召回率和F1分数。

Result: 在适中阈值下召回率达94%，表明系统具备良好敏感性；提高阈值能提升精确率至89%；最优阈值0.70下整体召回率48%、精确率45%。自动阈值选择使召回率优先（0.58），精确率较低（0.29）。对narrow-term PTs表现稍有提升。

Conclusion: SafeTerm AMQ可作为MedDRA自动查询生成的有效补充，能根据需要平衡召回和精确率。推荐在构建查询时采用合适术语并应用自动阈值以优化召回，通过提高相似度阈值实现更精准检索。

Abstract: In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. We assess the performance of SafeTerm Automated Medical Query (AMQ) on MedDRA SMQs. The AMQ is a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score (0-1) using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity, and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against tier-1 SMQs (110 queries, v28.1). Precision, recall and F1 were computed at multiple similarity-thresholds, defined either manually or using an automated method. High recall (94%)) is achieved at moderate similarity thresholds, indicative of good retrieval sensitivity. Higher thresholds filter out more terms, resulting in improved precision (up to 89%). The optimal threshold (0.70)) yielded an overall recall of (48%) and precision of (45%) across all 110 queries. Restricting to narrow-term PTs achieved slightly better performance at an increased (+0.05) similarity threshold, confirming increased relatedness of narrow versus broad terms. The automatic threshold (0.66) selection prioritizes recall (0.58) to precision (0.29). SafeTerm AMQ achieves comparable, satisfactory performance on SMQs and sanitized OCMQs. It is therefore a viable supplementary method for automated MedDRA query generation, balancing recall and precision. We recommend using suitable MedDRA PT terminology in query formulation and applying the automated threshold method to optimise recall. Increasing similarity scores allows refined, narrow terms selection.

</details>


### [288] [A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification](https://arxiv.org/abs/2512.07571)
*Nicolas Calbucura,Valentin Barriere*

Main category: cs.CL

TL;DR: 该论文提出了一种能轻松将语音信息融入文本大模型用于分类任务的简单方法，通过基于套索回归的特征选择，仅保留最关键的音频token，有效提升模型性能，并在谬误检测等任务上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 在多模态任务中，将音频embedding与文本融合时，音频序列长度远大于文本，且高维token难以低成本整合进大模型，因此需要高效的信息融合办法。

Method: 利用已训练好的语音分词器（生成长token序列），在多模态BOW表示上采用Lasso特征选择，仅选出与任务最相关的音频token。随后基于自监督语言建模目标对LLM适配这些token，最后对下游分类任务进行微调。

Result: 所提方法相比单模态模型、更大的SpeechLM或直接用音频表征的方法表现更优，在两项最新的谬误检测和分类任务上达到SOTA，并分析了即使随机选择token也有提升效果。

Conclusion: 有效的音频token选择和适配可提升文本大模型的多模态能力，尤其在此前被认为音频反而有害的任务中表现出显著性能提升，为多模态融合提供简洁且有效的方法。

Abstract: This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available [online](https://github.com/salocinc/EACL26SpeechTokFallacy/).

</details>


### [289] [Complementary Learning Approach for Text Classification using Large Language Models](https://arxiv.org/abs/2512.07583)
*Navid Asgari,Benjamin M. Cole*

Main category: cs.CL

TL;DR: 本文提出了一种高效且节约资源的大语言模型（LLMs）应用方法，用于定量科研中的人-机合作，并通过真实案例验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在科研中的应用不断增加，存在如何高效、低成本整合机器与学者优点，同时规避双方弱点的问题。现有做法多针对机器独立或人工独立参与，缺乏系统性人-机协作框架，特别是在定量研究领域。

Method: 作者提出一种基于chain of thought和few-shot learning提示的结构化方法，借鉴定性协作最佳实践，将其扩展到定量研究中的人-机团队合作。该方法支持研究者利用自然语言结合溯因推理，细致审视人和机器的贡献和失误，并通过低成本措施管理LLMs固有弱点。

Result: 通过案例分析，方法被用于考察和解释1990-2017年间1934份医药联盟公告新闻稿在人-机评价结果的差异，展示了该方法在实际数据中的可行性与有效性。

Conclusion: 所提方法实现了高效低成本的人-机团队合作，帮助科学家更好地利用LLM，弥补其不足，推动定量研究质量和效率提升。

Abstract: In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).

</details>


### [290] [Metric-Fair Prompting: Treating Similar Samples Similarly](https://arxiv.org/abs/2512.07608)
*Jing Wang,Jie Shen,Xing Niu,Tong Zhang,Jeremy Weiss*

Main category: cs.CL

TL;DR: 本文提出了一种关注公平性的提示框架（Metric-Fair Prompting），用于指导大语言模型（LLMs）在满足度量公平约束下做出决策，并验证其在医疗多项选择题场景下提升了性能和公平性。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在决策时往往忽视了个体公平性，即对相似问题的处理不一致，尤其在医疗等高风险领域这可能导致不公平或不可靠结果。因此，作者希望设计一种机制，提升模型对相似输入的决策一致性和公平，特别是在医疗问答这样的实际场景中。

Method: 作者将每个（问题，选项）对视为二元分类实例，根据NLP嵌入计算问题相似度，并采用成对解答而非独立解答的方法。通过在提示中引入全局决策协议，包括抽取关键信息、打分及施加Lipschitz约束，确保相似输入获得一致的信心分和输出，从而实现个体公平性。

Result: 在MedQA（US）医疗多项选择题基准上，Metric-Fair Prompting较标准逐题提示方法获得了更高的表现，显示出公平性指导下的信心推理能够提升LLM在高风险医学任务中的准确性。

Conclusion: Metric-Fair Prompting不仅提升了模型在临床多项选择题上的准确率，也验证了引入个体公平约束对高风险决策任务模型性能的积极影响。

Abstract: We introduce \emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \((\text{question}, \text{option})\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.

</details>


### [291] [PCMind-2.1-Kaiyuan-2B Technical Report](https://arxiv.org/abs/2512.07612)
*Kairong Luo,Zhenbo Sun,Xinyu Shi,Shengqi Chen,Bowen Yu,Yunyi Chen,Chenyi Dang,Hengtao Tao,Hui Wang,Fangming Liu,Kaifeng Lyu,Wenguang Chen*

Main category: cs.CL

TL;DR: 本文提出了PCMind-2.1-Kaiyuan-2B，这是一个完全开源、参数量为20亿的大语言模型，针对资源有限情况下提升训练效率和效果。模型性能与最先进的开源模型持平，并开放了全部模型、数据和代码。


<details>
  <summary>Details</summary>
Motivation: 当前开源社区与工业界在大模型能力上存在较大差距，主要原因在于工业界依赖于高质量的闭源数据和训练配方。论文旨在缩小这种差距，为资源有限的环境下提供低门槛、高效率的大模型训练方案。

Method: 1）提出Quantile Data Benchmarking方法，用于体系化比较不同的开源数据集，并指导数据混合策略；2）在多阶段训练框架下，采用Strategic Selective Repetition方法高效利用稀缺高质量数据；3）提出多领域课程训练策略，按数据质量排序样本次序。此外，优化了数据预处理流程，并对模型结构做出适应FP16的调整。

Result: Kaiyuan-2B模型在多个开源基准的表现与当前最先进的开源大模型相当。具体实验指标未详述，但表明其在资源受限下实现了高效、可扩展的预训练表现。

Conclusion: PCMind-2.1-Kaiyuan-2B为资源有限场景的大语言模型训练提供了有效的方法论和实用工具，推动大模型能力进一步向开源社区开放。

Abstract: The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.

</details>


### [292] [Bridging Code Graphs and Large Language Models for Better Code Understanding](https://arxiv.org/abs/2512.07666)
*Zeqi Chen,Zhaoyang Chu,Yi Gui,Feng Guo,Yao Wan,Chuan Shi*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法CGBridge，通过外部可训练的Bridge模块，为大语言模型（LLM）引入代码图信息，大幅提升代码智能相关任务的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码智能任务中表现出色，但由于仅处理线性化的token序列，难以理解程序的结构语义。之前的结构增强方法要么受限于prompt长度，要么需要对LLM架构做不兼容的大改动，限制了实际应用。

Method: CGBridge方法包含：1）用自监督学习大规模预训练一个代码图编码器获得结构语义；2）通过跨模态注意力机制训练一个Bridge模块，将代码、图和文本的语义对齐；3）用Bridge模块生成结构化提示并注入冻结的LLM进行下游微调。无须改动LLM参数，兼容性强。

Result: 实验表明，CGBridge在代码摘要和代码翻译任务上的表现明显优于原模型和图增强prompt方法，具体数据提升分别为16.19%、9.12%和9.84%、38.87%。此外，推理速度比LoRA-tuned模型快4倍以上。

Conclusion: CGBridge无需更改LLM结构即可大幅提升其对代码结构语义的理解能力，同时兼具高效性，为结构感知代码智能提供了有效且实用的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.

</details>


### [293] [When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks](https://arxiv.org/abs/2512.07684)
*Zihan Chen,Lanyu Yu*

Main category: cs.CL

TL;DR: 提出了一种基于图神经网络（GNN）的模型，能更高效准确地检测维基百科社区中的不文明行为，性能优于现有大语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有平台在遏制网络不文明行为上依赖文本处理和自动检测，但现有方法在准确性和效率上表现有限。发展更有效的检测模型是改善数字社区环境的关键。

Method: 将用户评论建模为图中的节点，通过评论间的文本相似性构建边。模型利用语言内容和结构关系信息，并引入动态调整的注意力机制，以在信息聚合时自适应平衡节点和拓扑特征。

Result: 实验证明，该GNN架构在多个评价指标上明显优于12种主流大语言模型，且推理成本更低。

Conclusion: 结构化上下文信息对不文明行为检测至关重要，仅依赖文本的大模型存在局限。该方法为社区内容治理提供了新路径，相关数据与代码将开源以促进后续研究。

Abstract: Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.

</details>


### [294] [HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs](https://arxiv.org/abs/2512.07687)
*Sujoy Nath,Arkaprabha Basu,Sharanya Dasgupta,Swagatam Das*

Main category: cs.CL

TL;DR: 本文关注多模态大模型在视觉-语言任务中出现幻觉现象的问题，提出一种基于模型内部层特征变化的新检测方法——HalluShift++，可有效评估和扩展至多模态场景。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）能够在视觉-语言理解任务中表现优异，但常因幻觉产生和视觉内容不符的描述，带来潜在风险。目前主流的幻觉检测大多依赖外部LLM评价器，这些评价器自身也易受幻觉影响，并存在领域适应难题。因此，需开发更鲁棒且可泛化的幻觉检测方法。

Method: 作者假设幻觉会在多模态大模型内部层级动态中表现出可测量的异常。他们通过层级分析和特定假设下的特征分布变化，提出了HalluShift++方法，从内部表示层面检测幻觉，实现从文本大模型至多模态场景的幻觉检测泛化。

Result: HalluShift++方法能够高效地检测多模态大模型中的幻觉现象，不局限于文本场景，实验证明该方法在不同多模态任务上的适用性和有效性。

Conclusion: HalluShift++为幻觉检测提供了一种新的内部机制分析视角，减少了对外部LLM的依赖，为多模态大模型的安全性评估和后续研究提供了新的方法基础。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \textsc{\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.

</details>


### [295] [Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map](https://arxiv.org/abs/2512.07694)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: 本论文提出了一种基于人工智能的新方法（SafeTerm），用于自动将医学查询词与标准医学术语（MedDRA PTs）匹配并排序，提高药物安全信号检测中的查询效率和标准化。


<details>
  <summary>Details</summary>
Motivation: 当前药物上市前安全性审查过程中，为了更有效地进行信号检测，存在将相关不良事件术语标准化分组的需求。传统方法依赖人工或半自动方式，效率有限且难以兼顾准确性和通用性。因此，需要更高效且标准化的自动匹配方法来辅助药物不良事件术语的规范查询。

Method: SafeTerm系统将医学查询词和MedDRA PTs映射到多维向量空间中，使用余弦相似度和极值聚类方法，对相关PTs进行检索与相关性排序。系统在FDA OCMQ v3.0的标准查询集（104条）上进行验证，比较不同相似度阈值下的精确率、召回率和F1分数，以评估系统性能。

Result: 在中等相似度阈值下有较高的召回率（>95%）；提高阈值可提升精确率（最高达86%）。最优阈值（大约0.70-0.75）时，召回率约50%，精确率约33%。对于较窄范围术语组，表现相似但需要更高阈值。

Conclusion: SafeTerm系统可作为标准化MedDRA查询自动生成的补充工具，推荐初始相似度阈值为0.60，若需更精确匹配可适当提高阈值。

Abstract: In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (>95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.

</details>


### [296] [Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?](https://arxiv.org/abs/2512.07777)
*Karin de Langis,Püren Öncel,Ryan Peters,Andrew Elfenbein,Laura Kristen Allen,Andreas Schramm,Dongyeop Kang*

Main category: cs.CL

TL;DR: 本文利用成对叙事数据集研究大语言模型（LLM）区分连贯与不连贯故事的能力，发现模型内部表示能较好区分，但实际给出的回答不总能准确区分两类故事，且倾向于依赖常识而非深入故事理解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理故事分析和生成任务时被广泛使用，但其是否真正理解叙事连贯性尚存疑问。作者希望系统性评估LLM在这一方面的表现，并揭示模型在叙事理解上的潜在不足。

Method: 研究基于包含连贯与不连贯版本的成对叙事数据集，采用探针测试分析LLM的内部表示对叙事连贯性的区分能力，同时考查其对评分类问题生成的实际回答，并测试不同类型的不连贯（情节与人物特性违背）。

Result: LLM内部表示可有效识别不连贯叙事，但其对评分类问题的回答往往未能准确区分连贯与不连贯的故事。思维链推理并未显著改善结果。LLM对“情境违背”型不连贯比对“人物特性违背”型更敏感，表明其更依赖原型世界知识，而不是深入理解剧情连贯性。

Conclusion: 尽管LLM内部能一定程度上区分叙事连贯性，但其实际输出表现和叙事理解仍有限，对连贯性掌握并不全面，特别是在人物特性相关连贯性方面表现较差。

Abstract: Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.

</details>


### [297] [On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models](https://arxiv.org/abs/2512.07783)
*Charlie Zhang,Graham Neubig,Xiang Yue*

Main category: cs.CL

TL;DR: 本文提出了一个全程可控的实验框架，系统剖析了语言模型在预训练、中间训练与RL后训练三阶段对推理能力提升的因果贡献，并明确了各阶段作用及其相互作用。


<details>
  <summary>Details</summary>
Motivation: 虽然RL后训练可以提升语言模型的推理能力，但目前尚不清楚这一提升是否超出了预训练所获得的能力，因为训练流程中对各阶段的把控和理解不足，因此需要更细致的实验框架来厘清各流程对能力提升的因果影响。

Method: 作者设计了一个采用合成推理任务、可解析的逐步推理过程，并能系统操控训练分布的实验框架，严格分离预训练、中间训练与RL后训练三阶段，并在复杂推理泛化和上下文泛化两个维度上对模型表现进行评估。

Result: ① 当预训练尚留提升空间且RL数据聚焦模型能力边界时，RL才能带来真实能力提升；② 上下文泛化只需足够的预训练暴露，RL可可靠迁移；③ 在计算预算固定下，中间训练提升明显优于RL单独训练；④ 流程级奖励可减少奖励作弊，提升推理准确性。

Conclusion: 文章厘清了预训练、中间训练和RL三阶段对推理型语言模型训练的实际作用及其交互，为未来设计更高效的推理型语言模型训练策略提供了理论基础。

Abstract: Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.

</details>


### [298] [Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support](https://arxiv.org/abs/2512.07801)
*Raunak Jain,Mudita Khurana*

Main category: cs.CL

TL;DR: 本文认为现有基于大型语言模型（LLM）的决策支持代理在复杂高风险环境中未能有效提升人机团队的决策表现。作者提出，应将人机协作的认知过程和因果推理整合为新的研究方向：协作性因果建模（CCS），以促进真正的协同思考。


<details>
  <summary>Details</summary>
Motivation: 当前，大语言模型常被应用于专家决策支持，但实际人机团队常常表现不如最佳人类专家。这一问题不仅仅源于准确率，而是由于AI辅助系统在认知协作、本体建模和共同推理方面存在根本性缺口。

Method: 作者提出协作性因果建模（CCS）作为新的研究框架。该框架关注人机在决策过程中如何共同建模、测试和修正心理模型、目标以及约束。同时，CCS系统需持续学习专家的推理方式、共同构建因果假设并基于协同决策结果共同进步。文中还提出相关挑战，如如何设计支持协同行为训练环境、共创模型的表示和交互协议，以及以信任和互补性为中心的评估方法等。

Result: 该论文侧重搭建研究议程而非实验验证。主要结果在于提出CCS框架和一系列研究挑战，为未来开发能真正与人类认知协作的AI系统指明方向。

Conclusion: 人机决策支持应超越单纯准确率，重视认知层面的协作。通过CCS研究框架，可以重新定义AI为人的‘协作思考伙伴’，提升人机团队的整体智能和互补性。

Abstract: LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.

</details>


### [299] [Do Generalisation Results Generalise?](https://arxiv.org/abs/2512.07832)
*Matteo Boglioni,Andrea Sgobbi,Gabriel Tavernini,Francesco Rita,Marius Mosbach,Tiago Pimentel*

Main category: cs.CL

TL;DR: 本文研究了大语言模型在多个分布外（OOD）测试集上的泛化能力，发现不同数据集间的泛化表现相关性不稳定，且依赖于具体模型。


<details>
  <summary>Details</summary>
Motivation: 部署大语言模型时面临着不可预期的分布外数据，现有评估多聚焦于单一OOD数据集，难以全面衡量模型的泛化能力。因此需要探究模型在多种OOD场景下的表现相关性。

Method: 作者在微调过程中，使用多个OOD测试集对模型性能进行评估，并采用偏相关分析控制住域内性能，考察不同测集上的泛化表现是否一致。

Result: 以OLMo2和OPT为例，分析发现跨多个OOD测试集的泛化表现没有统一趋势，相关性取决于模型本身。

Conclusion: 通过多测试集分析表明，模型在不同OOD场景下的泛化能力并不总是相关或可推广，单一测试集评估无法代表其整体表现。

Abstract: A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [300] [POrTAL: Plan-Orchestrated Tree Assembly for Lookahead](https://arxiv.org/abs/2512.06002)
*Evan Conway,David Porfirio,David Chan,Mark Roberts,Laura M. Hiatt*

Main category: cs.RO

TL;DR: 该论文提出了一种新的轻量级概率规划算法POrTAL，并通过案例验证其在任务分配中的优势。


<details>
  <summary>Details</summary>
Motivation: 当前机器人在不完全可观测环境下进行任务规划面临效率低或步骤多的问题，现有方法如FF-Replan和POMCP各有短板，难以兼顾效率与效果。

Method: 作者提出了POrTAL算法，将FF-Replan和POMCP两种规划算法的优点结合，形成一种更高效的概率规划方式，并通过多组案例测试算法表现。

Result: 实验结果显示，POrTAL在决策步骤数上优于FF-Replan和POMCP，并能在不同时间约束下保持较好性能。

Conclusion: POrTAL算法能够在部分可观测环境下更高效完成机器人任务规划，为人机交互、机器人任务分配等应用带来改进。

Abstract: Assigning tasks to robots often involves supplying the robot with an overarching goal, such as through natural language, and then relying on the robot to uncover and execute a plan to achieve that goal. In many settings common to human-robot interaction, however, the world is only partially observable to the robot, requiring that it create plans under uncertainty. Although many probabilistic planning algorithms exist for this purpose, these algorithms can be inefficient if executed with the robot's limited computational resources, or may require more steps than expected to achieve the goal. We thereby created a new, lightweight, probabilistic planning algorithm, Plan-Orchestrated Tree Assembly for Lookahead (POrTAL), that combines the strengths of two baseline planning algorithms, FF-Replan and POMCP. In a series of case studies, we demonstrate POrTAL's ability to quickly arrive at solutions that outperform these baselines in terms of number of steps. We additionally demonstrate how POrTAL performs under varying temporal constraints.

</details>


### [301] [Training-Free Robot Pose Estimation using Off-the-Shelf Foundational Models](https://arxiv.org/abs/2512.06017)
*Laurence Liang*

Main category: cs.RO

TL;DR: 本论文提出利用先进的视觉语言模型（VLMs）直接从单张图片估算机械臂关节角度，并对模型在合成和真实数据上进行了性能基线评估。


<details>
  <summary>Details</summary>
Motivation: 随着机械臂在工业和家用领域的广泛应用，准确估计其姿态对于提升安全性和性能极为重要。传统方法在鲁棒性与泛化能力上有限，作者希望借助VLM的新进展解决这一问题。

Method: 将“开箱即用”的前沿视觉语言模型应用于单幅图像的机械臂关节角度估算任务，并在合成数据集和真实图片上进行实验和基线评估。同时探讨了测试时缩放与参数缩放对模型表现的影响。

Result: 实验发现当前VLMs在合成和真实图片数据上能够提供基线性能。此外，单纯依靠测试时的尺度调整或模型参数规模扩大，并未带来显著的性能提升。

Conclusion: 当前前沿VLM虽然可用于机械臂关节估计，但提升效果有限，仅作为基线，未来需要发展更专业或定制化的方案以进一步提升精度。

Abstract: Pose estimation of a robot arm from visual inputs is a challenging task. However, with the increasing adoption of robot arms for both industrial and residential use cases, reliable joint angle estimation can offer improved safety and performance guarantees, and also be used as a verifier to further train robot policies. This paper introduces using frontier vision-language models (VLMs) as an ``off-the-shelf" tool to estimate a robot arm's joint angles from a single target image. By evaluating frontier VLMs on both synthetic and real-world image-data pairs, this paper establishes a performance baseline attained by current FLMs. In addition, this paper presents empirical results suggesting that test time scaling or parameter scaling alone does not lead to improved joint angle predictions.

</details>


### [302] [Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction](https://arxiv.org/abs/2512.06038)
*Kelsey Fontenot,Anjali Gorti,Iva Goel,Tonio Buonassisi,Alexander E. Siemenn*

Main category: cs.RO

TL;DR: 本文提出了一种结合机器人、双驱动分配器和深度学习视觉的新自动化底物处理系统（ASHE），极大提升了自助实验室在处理易碎、透明底物时的自动化能力。该系统实现在130次实验中98.5%的首次精准放置，并能自动纠错，有效提升化学与材料发现流程的效率。


<details>
  <summary>Details</summary>
Motivation: 自助实验室（SDL）在化学和材料自动化实验中已取得显著进展，但底物（尤其是易碎、透明底物）的处理与重新装载长期未被充分自动化且易出错。作者希望解决这一实际瓶颈，提升自动化生产线的完整性和效率。

Method: 作者设计并实现了一个闭环的自动底物处理与交换系统（ASHE），整合了机器人操作、双驱动分配器和基于深度学习的计算机视觉算法。该系统能够自动检测并纠正易碎、透明底物在搬运中的错误。

Result: 在130次将透明玻璃底物重新装载进SDL装置的独立实验中，ASHE实现了98.5%的首次精准放置，仅有两次发生位置错误且都被系统自动检测和纠正。

Conclusion: ASHE显著提升了SDL自动化底物处理的准确性和可靠性，推动了自助实验室自动化水平的提升，从而进一步加速化学和材料领域的创新与发现。

Abstract: Self-driving laboratories (SDLs) have accelerated the throughput and automation capabilities for discovering and improving chemistries and materials. Although these SDLs have automated many of the steps required to conduct chemical and materials experiments, a commonly overlooked step in the automation pipeline is the handling and reloading of substrates used to transfer or deposit materials onto for downstream characterization. Here, we develop a closed-loop method of Automated Substrate Handling and Exchange (ASHE) using robotics, dual-actuated dispensers, and deep learning-driven computer vision to detect and correct errors in the manipulation of fragile and transparent substrates for SDLs. Using ASHE, we demonstrate a 98.5% first-time placement accuracy across 130 independent trials of reloading transparent glass substrates into an SDL, where only two substrate misplacements occurred and were successfully detected as errors and automatically corrected. Through the development of more accurate and reliable methods for handling various types of substrates, we move toward an improvement in the automation capabilities of self-driving laboratories, furthering the acceleration of novel chemical and materials discoveries.

</details>


### [303] [WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving](https://arxiv.org/abs/2512.06112)
*Yifang Xu,Jiahao Cui,Feipeng Cai,Zhihao Zhu,Hanlin Shang,Shan Luan,Mingwang Xu,Neng Zhang,Yaoyi Li,Jia Cai,Siyu Zhu*

Main category: cs.RO

TL;DR: WAM-Flow是一种新型视觉-语言-动作模型，通过离散流匹配实现高效且高级的自动驾驶决策，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶中的视觉-语言-动作模型多采用自回归生成，存在推理速度慢和精度受限的问题。作者希望引入一种能够并行推理，并在计算量和精度之间灵活权衡的新方法，从而显著提升模型效率和表现。

Method: WAM-Flow提出将自我轨迹规划建模为结构化token空间上的离散流匹配，并采用全并行、双向去噪生成实现粗到细的推理。方法结合了度量对齐的数值分词器（三元组损失）、几何感知的流目标和集成安全、舒适等多策略奖励的模拟器引导对齐。在模型训练中，将预训练的Janus-1.5B骨干从自回归解码转为非因果流模型，并通过二次多模态预训练提升场景理解能力。

Result: 在NAVSIM v1基准上，1步推理达到89.1 PDMS，5步推理达到90.3 PDMS，超过了主流自回归和扩散式视觉-语言-动作基线。

Conclusion: WAM-Flow展示出离散流匹配在自动驾驶端到端决策中的巨大潜力，为该领域提供了新的高效、精准的范式，代码即将开源。

Abstract: We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.

</details>


### [304] [Probabilistic Weapon Engagement Zones for a Turn Constrained Pursuer](https://arxiv.org/abs/2512.06130)
*Grant Stagg,Isaac E. Weintraub,Cameron K. Peterson*

Main category: cs.RO

TL;DR: 本文提出了分析和生成曲线-直线概率交战区（CSPEZ）的方法，以帮助受害方（逃避者）在追击者参数不确定的情况下制定安全路径。


<details>
  <summary>Details</summary>
Motivation: 当前交战区研究大多假设追击者参数已知或确定，然而实际情况中追击者的速度、位置、航向、转弯率等均存在不确定性。如何在这样的不确定性环境下为逃避者最大程度降低被捕获风险，是安全导航和自动驾驶等领域亟须解决的问题。

Method: （1）首先解析推导了确定性条件下的基本交战区模型（CSBEZ）；（2）然后扩展到概率框架，通过蒙特卡洛采样、线性化、二次近似和神经网络回归四种不同不确定性传播方法对概率交战区（CSPEZ）进行计算与比较；（3）进一步将CSPEZ约束集成到轨迹优化算法中，为逃避者生成最小化被捕获风险的路径。

Result: 对比分析了四种不确定性传播方法的精度和计算代价，展示了CSPEZ约束纳入轨迹优化后，可以有效生成显著降低被捕获风险的安全路径。

Conclusion: 本文系统地提出了分析、计算和利用曲线-直线概率交战区（CSPEZ）的方法，为逃避者在不确定性条件下的安全路径规划提供了理论与实践工具，有助于提升实际系统中的安全性和鲁棒性。

Abstract: Curve-straight probabilistic engagement zones (CSPEZ) quantify the spatial regions an evader should avoid to reduce capture risk from a turn-rate-limited pursuer following a curve-straight path with uncertain parameters including position, heading, velocity, range, and maximum turn rate. This paper presents methods for generating evader trajectories that minimize capture risk under such uncertainty. We first derive an analytic solution for the deterministic curve-straight basic engagement zone (CSBEZ), then extend this formulation to a probabilistic framework using four uncertainty-propagation approaches: Monte Carlo sampling, linearization, quadratic approximation, and neural-network regression. We evaluate the accuracy and computational cost of each approximation method and demonstrate how CSPEZ constraints can be integrated into a trajectory-optimization algorithm to produce safe paths that explicitly account for pursuer uncertainty.

</details>


### [305] [GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers](https://arxiv.org/abs/2512.06147)
*Hochul Hwang,Soowan Yang,Jahir Sadik Monon,Nicholas A Giudice,Sunghoon Ivan Lee,Joydeep Biswas,Donghyun Kim*

Main category: cs.RO

TL;DR: 本文通过深度调研视障人士及其导盲服务，总结提出类导盲犬导航理念，并推出了基于视觉的GuideNav机器人导航系统，实现了可与导盲犬相媲美的路径复现能力。


<details>
  <summary>Details</summary>
Motivation: 尽管移动助残系统领域用户研究取得了一定进展，但针对盲视障人士机器人导航设计的指导性资料非常少，因此作者希望填补这一空白，为人性化导航系统开发提供真实基础。

Method: 研究团队通过与26位导盲犬使用者、4位白手杖用户、9位导盲犬训练师和1位定向/移动训练师访谈，并进行15小时的导盲犬辅助手段实地观察，构建开源数据集。随后，受导盲犬协助原理启发，开发了基于视觉的GuideNav导航系统：通过拓扑化路径建模、视觉位置识别、时序滤波和相对位姿估计器实现无需昂贵LiDAR等传感器的路径学习与自主复现。

Result: GuideNav在五种室外环境下完成了公里级路径复现测试，表现出较强的稳定性和对场景变化的适应性。通过对导盲犬使用者与教练的初步体验调查，验证了其可行性和意义。

Conclusion: GuideNav是首个实现类导盲犬路径复现的四足导航机器人系统，为视觉障碍人士辅助导航机器人发展带来新思路和有力实验支撑，并推动了该领域的开源数据和人本设计研究。

Abstract: While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\&M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system's feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.

</details>


### [306] [Real-Time Spatiotemporal Tubes for Dynamic Unsafe Sets](https://arxiv.org/abs/2512.06151)
*Ratnangshu Das,Siddhartha Upadhyay,Pushpak Jagtap*

Main category: cs.RO

TL;DR: 本论文提出一种用于非线性纯反馈系统的实时控制框架，能在动态环境中实现任务目标下的安全导航。


<details>
  <summary>Details</summary>
Motivation: 面对具有未知动力学的非线性系统，如何实时保证安全（如避障）并按时完成任务是自动控制领域的难题，特别是在动态未知环境下。

Method: 提出了实时时空管（STT）框架，将系统状态限制在一个会随时间变化、基于实时传感信息自适应的时空‘球’内。设计了无需近似的解析型控制律保证系统输出始终被STT包络，实现安全约束和任务要求。

Result: 框架通过理论推导提供了安全与按时完成任务的形式化保证，并在移动机器人和飞行器两个平台上进行了仿真与硬件实验验证其有效性与可扩展性。

Conclusion: 所提出的STT框架能在动态复杂环境中实现实时、安全、按时的任务完成，适用于不完全已知的非线性系统，具备理论保障与实验可行性。

Abstract: This paper presents a real-time control framework for nonlinear pure-feedback systems with unknown dynamics to satisfy reach-avoid-stay tasks within a prescribed time in dynamic environments. To achieve this, we introduce a real-time spatiotemporal tube (STT) framework. An STT is defined as a time-varying ball in the state space whose center and radius adapt online using only real-time sensory input. A closed-form, approximation-free control law is then derived to constrain the system output within the STT, ensuring safety and task satisfaction. We provide formal guarantees for obstacle avoidance and on-time task completion. The effectiveness and scalability of the framework are demonstrated through simulations and hardware experiments on a mobile robot and an aerial vehicle, navigating in cluttered dynamic environments.

</details>


### [307] [Situation-Aware Interactive MPC Switching for Autonomous Driving](https://arxiv.org/abs/2512.06182)
*Shuhao Qi,Qiling Aori,Luyao Zhang,Mircea Lazar,Sofie Haesaert*

Main category: cs.RO

TL;DR: 本文提出了一种根据交通情境智能切换不同交互能力的模型预测控制（MPC）方法，实现自动驾驶在交互式交通场景下的高效与智能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在与其他交通参与者复杂交互时，需要高保真度的MPC模型以获得更智能的行为，但这带来高计算成本。而强交互场景实际较少见，因此有必要按需选择控制器，实现性能与计算资源的平衡。

Method: 作者首先比较分析了多种MPC在交互能力上的表现并进行分级。随后，开发了基于神经网络的分类器，用于识别当前交通情境，并按需在不同交互能力的MPC控制器间切换。

Result: 实验表明，这种情境自适应切换策略，在少数关键场景启用高级MPC能显著提升整体性能，同时在大多数场景下使用基础MPC大幅降低了计算负担。

Conclusion: 文中提出的情境感知自适应MPC切换方法，兼顾了自动驾驶系统的智能性和实时性，为未来实际部署提供了有效路线。

Abstract: To enable autonomous driving in interactive traffic scenarios, various model predictive control (MPC) formulations have been proposed, each employing different interaction models. While higher-fidelity models enable more intelligent behavior, they incur increased computational cost. Since strong interactions are relatively infrequent in traffic, a practical strategy for balancing performance and computational overhead is to invoke an appropriate controller based on situational demands. To achieve this approach, we first conduct a comparative study to assess and hierarchize the interactive capabilities of different MPC formulations. Furthermore, we develop a neural network-based classifier to enable situation-aware switching among controllers with different levels of interactive capability. We demonstrate that this situation-aware switching can both substantially improve overall performance by activating the most advanced interactive MPC in rare but critical situations, and significantly reduce computational load by using a basic MPC in the majority of scenarios.

</details>


### [308] [REWW-ARM -- Remote Wire-Driven Mobile Robot: Design, Control, and Experimental Validation](https://arxiv.org/abs/2512.06192)
*Takahiro Hattori,Kento Kawaharazuka,Temma Suzuki,Keita Yoneda,Kei Okada*

Main category: cs.RO

TL;DR: 该论文提出了一种全新的“远程线驱”系统，通过远程布置电机和控制单元，实现机器人终端无电子元件化，并开发了一款原型机器人REWW-ARM，用以验证其在陆地和水下的多种运动与操作能力。


<details>
  <summary>Details</summary>
Motivation: 传统电子设备对机器人可用环境造成限制，例如在高湿、水下或强辐射等环境中，电子元件容易失效。现有的无电子机器人（如远程液压驱动）和末端电机集中线驱机械臂各有优劣，缺乏能兼具两者优势的新方案，推动了本研究的开展。

Method: 提出“远程线驱”系统，并开发了包含远程线传动机构（RWTM）、无电子末端移动平台，以及集中电动和回馈控制单元的原型机器人REWW-ARM。通过该系统将动力和控制由远程传递至机器人末端。对原型进行了运动学与姿态控制、物体操作等多场景（包括水下）的实验评估，分析了其机械传递和反馈控制性能。

Result: 实验表明，REWW-ARM可在无电子的末端实现稳定运动、精确的姿态控制以及对象操控，在陆地和水下均表现出良好的适应性和操作能力。系统的动力传递与反馈控制机制有效可行。

Conclusion: 远程线驱系统为机器人部署在极端环境提供了新途径，具备广泛的应用前景，可扩展各种机器人工作空间，对提升环境适应性具有重要意义。

Abstract: Electronic devices are essential for robots but limit their usable environments. To overcome this, methods excluding electronics from the operating environment while retaining advanced electronic control and actuation have been explored. These include the remote hydraulic drive of electronics-free mobile robots, which offer high reachability, and long wire-driven robot arms with motors consolidated at the base, which offer high environmental resistance. To combine the advantages of both, this study proposes a new system, "Remote Wire Drive." As a proof-of-concept, we designed and developed the Remote Wire-Driven robot "REWW-ARM", which consists of the following components: 1) a novel power transmission mechanism, the "Remote Wire Transmission Mechanism" (RWTM), the key technology of the Remote Wire Drive; 2) an electronics-free distal mobile robot driven by it; and 3) a motor-unit that generates power and provides electronic closed-loop control based on state estimation via the RWTM. In this study, we evaluated the mechanical and control performance of REWW-ARM through several experiments, demonstrating its capability for locomotion, posture control, and object manipulation both on land and underwater. This suggests the potential for applying the Remote Wire-Driven system to various types of robots, thereby expanding their operational range.

</details>


### [309] [Cascaded Tightly-Coupled Observer Design for Single-Range-Aided Inertial Navigation](https://arxiv.org/abs/2512.06198)
*Oussama Sifour,Soulaimane Berkane,Abdelhamid Tayebi*

Main category: cs.RO

TL;DR: 本论文提出了一种基于单点距离辅助的导航观测器，只需使用IMU、体坐标系向量传感器（如磁力计）和与固定锚点之间的距离测量，即可重建刚体的全状态（包括位置、速度和姿态）。系统通过级联观测器架构实现了几乎全局渐近稳定，并在三维仿真中取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 当前辅助导航常依赖多传感器组合与复杂系统，给实际部署带来负担。本文尝试只用极简的传感器配置（IMU、向量传感器及单锚点距离测量）实现高精度的刚体状态估计，提高系统的轻量化和实用性。

Method: 观测器首先将状态估计建模为扩展线性时变系统，估算体坐标系下的位置、速度及重力方向。随后，利用恢复的重力方向和体坐标系向量测量，在$	ext{SO}(3)$上重建刚体完整姿态。系统采用级联观测器结构，并理论证明了在一致可观测性条件下的几乎全局渐近稳定。

Result: 通过三维轨迹仿真，验证了所提出方法能准确估计位置、速度和姿态，展现了抗噪声鲁棒性和对轨迹变化的适应能力。

Conclusion: 单距离辅助的导航观测器提供了导航系统的一种轻量、鲁棒且有效的新方式，可简化硬件配置，提升自主导航应用的实用性。

Abstract: This work introduces a single-range-aided navigation observer that reconstructs the full state of a rigid body using only an Inertial Measurement Unit (IMU), a body-frame vector measurement (e.g., magnetometer), and a distance measurement from a fixed anchor point. The design first formulates an extended linear time-varying (LTV) system to estimate body-frame position, body-frame velocity, and the gravity direction. The recovered gravity direction, combined with the body-frame vector measurement, is then used to reconstruct the full orientation on $\mathrm{SO}(3)$, resulting in a cascaded observer architecture. Almost Global Asymptotic Stability (AGAS) of the cascaded design is established under a uniform observability condition, ensuring robustness to sensor noise and trajectory variations. Simulation studies on three-dimensional trajectories demonstrate accurate estimation of position, velocity, and orientation, highlighting single-range aiding as a lightweight and effective modality for autonomous navigation.

</details>


### [310] [Where to Fly, What to Send: Communication-Aware Aerial Support for Ground Robots](https://arxiv.org/abs/2512.06207)
*Harshil Suthar,Dipankar Maity*

Main category: cs.RO

TL;DR: 本文研究了在通信带宽受限条件下，多机器人协作探索未知环境时的有效信息传递与分配问题。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，空地协作机器人常面临带宽受限，难以完全、高效地共享地图与信息，亟需信息选择与传输优化方案，以提升整体任务效率。

Method: 提出基于信息价值（Value-of-Information, VoI）的信息选择框架，采用混合整数线性规划（MILP）优化传输数据量，并用基于效用分数的探索策略进行环境探索。

Result: 框架能使空中机器人在探索同时，高效决定何时、传输多少、哪些信息给地面机器人。并通过分析量化了地面导航成本与传输数据总量的权衡。

Conclusion: 所提方法在保障地面机器人任务完成效率的同时降低了通信负载，实现了通信与运动之间的有效权衡。

Abstract: In this work we consider a multi-robot team operating in an unknown environment where one aerial agent is tasked to map the environment and transmit (a portion of) the mapped environment to a group of ground agents that are trying to reach their goals. The entire operation takes place over a bandwidth-limited communication channel, which motivates the problem of determining what and how much information the assisting agent should transmit and when while simultaneously performing exploration/mapping. The proposed framework enables the assisting aerial agent to decide what information to transmit based on the Value-of-Information (VoI), how much to transmit using a Mixed-Integer Linear Programming (MILP), and how to acquire additional information through an utility score-based environment exploration strategy. We perform a communication-motion trade-off analysis between the total amount of map data communicated by the aerial agent and the navigation cost incurred by the ground agents.

</details>


### [311] [Safe Model Predictive Diffusion with Shielding](https://arxiv.org/abs/2512.06261)
*Taekyung Kim,Keyvan Majd,Hideki Okamoto,Bardh Hoxha,Dimitra Panagou,Georgios Fainekos*

Main category: cs.RO

TL;DR: 本文提出了一种全新的无训练扩散路径规划方法Safe MPD，将基于模型的扩散框架与安全盾结合，直接生成既动态可行又安全的机器人轨迹，并在多种复杂环境中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 复杂机器人系统在路径规划时，需同时兼顾安全性、动力学可行性和计算效率，这是机器人领域的重要挑战。而现有方法常常需要后处理修正，导致计算复杂或不可行。

Method: 提出Safe MPD：在去噪过程中实时对所有采样施加动力学可行性和安全约束，无需训练，统一建模扩散和安全盾。通过防止后处理修正带来的问题，保证每个轨迹点原生满足要求。

Result: 在包括拖挂系统等非凸复杂场景下，Safe MPD的成功率和安全性明显优于现有方案，并且单次规划计算在一秒内完成，展现出高效率和优越性能。

Conclusion: Safe MPD能有效生成安全、动力学可行且高效的机器人轨迹，优于当前主流安全策略，为复杂工业机器人规划问题带来更实用的解决方式。

Abstract: Generating safe, kinodynamically feasible, and optimal trajectories for complex robotic systems is a central challenge in robotics. This paper presents Safe Model Predictive Diffusion (Safe MPD), a training-free diffusion planner that unifies a model-based diffusion framework with a safety shield to generate trajectories that are both kinodynamically feasible and safe by construction. By enforcing feasibility and safety on all samples during the denoising process, our method avoids the common pitfalls of post-processing corrections, such as computational intractability and loss of feasibility. We validate our approach on challenging non-convex planning problems, including kinematic and acceleration-controlled tractor-trailer systems. The results show that it substantially outperforms existing safety strategies in success rate and safety, while achieving sub-second computation times.

</details>


### [312] [Leveraging Port-Hamiltonian Theory for Impedance Control Benchmarking](https://arxiv.org/abs/2512.06423)
*Leonardo F. Dos Santos,Elisa G. Vergamini,Cícero Zanette,Lucca Maitan,Thiago Boaventura*

Main category: cs.RO

TL;DR: 该论文提出了基于Port-Hamiltonian（PH）模型的阻抗控制基准评价指标，并通过仿真验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有阻抗控制的基准评价缺乏标准化和泛化性，尤其在多自由度机器人、复杂任务及不同传感条件下难以一致衡量性能。

Method: 作者提出了符合因果律的PH模型用于笛卡尔空间中的质量-弹簧-阻尼系统，并基于该模型推导出一个可微、与力-力矩传感无关、适用于n自由度和时变参考的广义无源性条件，同时定义了表征动态解耦的阻抗保真度指标。

Result: 通过在Gazebo仿真环境下，针对六自由度机械臂和四足机器人腿的阻抗控制进行实验，验证了所提基准指标的有效性和适用性。

Conclusion: PH框架及相关指标可为阻抗控制性能的标准化基准评价提供有力工具，具备推广价值。

Abstract: This work proposes PH-based metrics for benchmarking impedance control. A causality-consistent PH model is introduced for mass-spring-damper impedance in Cartesian space. Based on this model, a differentiable, force-torque sensing-independent, n-DoF passivity condition is derived, valid for time-varying references. An impedance fidelity metric is also defined from step-response power in free motion, capturing dynamic decoupling. The proposed metrics are validated in Gazebo simulations with a six-DoF manipulator and a quadruped leg. Results demonstrate the suitability of the PH framework for standardized impedance control benchmarking.

</details>


### [313] [Fault Tolerant Control of Mecanum Wheeled Mobile Robots](https://arxiv.org/abs/2512.06444)
*Xuehui Ma,Shiliang Zhang,Zhiyong Sun*

Main category: cs.RO

TL;DR: 本文提出了一种同时应对全局和部分驱动器故障的麦克纳姆轮移动机器人（MWMR）容错控制策略。该方法利用后验概率实时学习故障参数，通过概率加权多种控制律，实现了各类故障下的鲁棒与安全控制，并在仿真中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有MWMR容错控制多针对电机完全失效（如电机卡死），忽略了更常见的部分失效（如扭矩降级）。这极大威胁了机器人任务的完成与安全。因此，需要一种能同时处理各种故障情况的实时容错控制方法。

Method: 本文通过引入后验概率，实现对系统中实际故障参数的实时学习，并预设多种典型故障场景下的控制律。故障出现时，以概率加权的方式综合这些控制律，制定最终的FTC策略。

Result: 仿真表明，所提FTC策略在不同故障类型和级别下，均能有效保持系统性能，提升鲁棒性和安全性。

Conclusion: 提出的方法能有效应对MWMR在全局或部分执行器故障下的控制难题，增强了系统的可靠性与任务完成能力。

Abstract: Mecanum wheeled mobile robots (MWMRs) are highly susceptible to actuator faults that degrade performance and risk mission failure. Current fault tolerant control (FTC) schemes for MWMRs target complete actuator failures like motor stall, ignoring partial faults e.g., in torque degradation. We propose an FTC strategy handling both fault types, where we adopt posterior probability to learn real-time fault parameters. We derive the FTC law by aggregating probability-weighed control laws corresponding to predefined faults. This ensures the robustness and safety of MWMR control despite varying levels of fault occurrence. Simulation results demonstrate the effectiveness of our FTC under diverse scenarios.

</details>


### [314] [Entropy-Controlled Intrinsic Motivation Reinforcement Learning for Quadruped Robot Locomotion in Complex Terrains](https://arxiv.org/abs/2512.06486)
*Wanru Gong,Xinyi Zheng,Xiaopeng Yang,Xiaoqing Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种基于熵控制与内在动机的新型强化学习算法ECIM，有效提升了四足机器人在多种地形上的运动表现，并降低了能耗。


<details>
  <summary>Details</summary>
Motivation: 传统的PPO及其变体在四足机器人运动训练中因过早收敛，导致表现不佳和任务性能下降，因此有必要设计一种更能平衡探索与收敛的算法。

Method: 提出了名为ECIM的强化学习算法，通过融合基于熵的探索机制与自适应内在动机激励，有效抑制过早收敛。实验在Isaac Gym平台上的六类地形环境（如斜坡、楼梯、崎岖地面等）中，与当前主流算法进行了对比评测。

Result: 与主流基线相比，ECIM在多个指标上表现更优：任务奖励提升4~12%；身体姿态波动降低23~29%；关节加速度减少20~32%；关节扭矩消耗降低11~20%。

Conclusion: ECIM算法兼具更强的泛化性和适应性，在提升四足机器人多地形稳定行走能力的同时也降低了能耗，是复杂机器人控制任务的实用选择。

Abstract: Learning is the basis of both biological and artificial systems when it comes to mimicking intelligent behaviors. From the classical PPO (Proximal Policy Optimization), there is a series of deep reinforcement learning algorithms which are widely used in training locomotion policies for quadrupedal robots because of their stability and sample efficiency. However, among all these variants, experiments and simulations often converge prematurely, leading to suboptimal locomotion and reduced task performance. Therefore, in this paper, we introduce Entropy-Controlled Intrinsic Motivation (ECIM), an entropy-based reinforcement learning algorithm in contrast with the PPO series, that can reduce premature convergence by combining intrinsic motivation with adaptive exploration.
  For experiments, in order to parallel with other baselines, we chose to apply it in Isaac Gym across six terrain categories: upward slopes, downward slopes, uneven rough terrain, ascending stairs, descending stairs, and flat ground as widely used. For comparison, our experiments consistently achieve better performance: task rewards increase by 4--12%, peak body pitch oscillation is reduced by 23--29%, joint acceleration decreases by 20--32%, and joint torque consumption declines by 11--20%. Overall, our model ECIM, by combining entropy control and intrinsic motivation control, achieves better results in stability across different terrains for quadrupedal locomotion, and at the same time reduces energetic cost and makes it a practical choice for complex robotic control tasks.

</details>


### [315] [Vision-Guided Grasp Planning for Prosthetic Hands in Unstructured Environments](https://arxiv.org/abs/2512.06517)
*Shifa Sulaiman,Akash Bachhar,Ming Shen,Simon Bøgh*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉引导的仿生手抓取算法，实现了感知、规划与控制一体化，以提升义肢手的灵巧性和环境适应性。


<details>
  <summary>Details</summary>
Motivation: 传统义肢手在手部精细操作与适应多变环境方面存在局限，亟需更智能的控制方法，实现自然、灵活的抓取。视觉辅助方法有望解决上述问题。

Method: 使用机载摄像头采集场景，通过基于BVH的视觉算法对目标物体进行分割并生成包围盒；利用RRT*算法生成候选抓取轨迹并根据最小欧式距离选择合适指尖末端位置，再采用阻尼最小二乘逆运动学求解器计算指关节角度，按指独立进行抓取规划，实现模块化、实时适应性的控制。

Result: 方法通过仿真验证，并在Linker Hand O7平台上完成实验集成，能够支持无结构环境下的实时自适应抓取。

Conclusion: 提出的视觉引导抓取算法提升了义肢手操作的灵活性与自主性，为智能假肢在多变环境下的自然抓取提供了有效解决方案。

Abstract: Recent advancements in prosthetic technology have increasingly focused on enhancing dexterity and autonomy through intelligent control systems. Vision-based approaches offer promising results for enabling prosthetic hands to interact more naturally with diverse objects in dynamic environments. Building on this foundation, the paper presents a vision-guided grasping algorithm for a prosthetic hand, integrating perception, planning, and control for dexterous manipulation. A camera mounted on the set up captures the scene, and a Bounding Volume Hierarchy (BVH)-based vision algorithm is employed to segment an object for grasping and define its bounding box. Grasp contact points are then computed by generating candidate trajectories using Rapidly-exploring Random Tree Star algorithm, and selecting fingertip end poses based on the minimum Euclidean distance between these trajectories and the objects point cloud. Each finger grasp pose is determined independently, enabling adaptive, object-specific configurations. Damped Least Square (DLS) based Inverse kinematics solver is used to compute the corresponding joint angles, which are subsequently transmitted to the finger actuators for execution. This modular pipeline enables per-finger grasp planning and supports real-time adaptability in unstructured environments. The proposed method is validated in simulation, and experimental integration on a Linker Hand O7 platform.

</details>


### [316] [TacFinRay: Soft Tactile Fin-Ray Finger with Indirect Tactile Sensing for Robust Grasping](https://arxiv.org/abs/2512.06524)
*Saekwang Nam,Bowen Deng,Loong Yi Lee,Jonathan M. Rossiter,Nathan F. Lepora*

Main category: cs.RO

TL;DR: 本文提出了一种带有触觉传感功能的Fin-Ray手指，通过间接感知方式实现接触位置和压入深度的同时检测，具备较高精度且能够广泛泛化于不同形状与尺寸的接触体，成功应用于抓取与放置任务中。


<details>
  <summary>Details</summary>
Motivation: 软体机器人及末端执行器对柔性且可扩展的触觉传感方案有巨大需求，尤其是在传感器无法直接设置于接触表面时，如何实现高精度的接触信息获取成为亟需解决的问题。

Method: 提出将Fin-Ray结构与铰链装置结合，使手指变形和位移通过结构传递到底部交叉梁上的生物仿生结构针阵列。内部相机采集变形图像后，用卷积神经网络分析以推断外部接触信息，且系统设计中通过不同针配置和铰链方向优化精度。

Result: 最终系统实现了0.1 mm深度和2 mm位置的感知精度，对不同形状和尺寸的接触体具有强泛化能力，并在带有抓取不确定性的抓取-放置任务中，触觉反馈显著提升了放置精度。

Conclusion: 该工作提供了一种轻量化、柔性且可扩展的触觉传感解决方案，尤其适用于需将传感元件远离接触面的软体机器人结构，具有良好的实用前景。

Abstract: We present a tactile-sensorized Fin-Ray finger that enables simultaneous detection of contact location and indentation depth through an indirect sensing approach. A hinge mechanism is integrated between the soft Fin-Ray structure and a rigid sensing module, allowing deformation and translation information to be transferred to a bottom crossbeam upon which are an array of marker-tipped pins based on the biomimetic structure of the TacTip vision-based tactile sensor. Deformation patterns captured by an internal camera are processed using a convolutional neural network to infer contact conditions without directly sensing the finger surface. The finger design was optimized by varying pin configurations and hinge orientations, achieving 0.1\,mm depth and 2mm location-sensing accuracies. The perception demonstrated robust generalization to various indenter shapes and sizes, which was applied to a pick-and-place task under uncertain picking positions, where the tactile feedback significantly improved placement accuracy. Overall, this work provides a lightweight, flexible, and scalable tactile sensing solution suitable for soft robotic structures where the sensing needs situating away from the contact interface.

</details>


### [317] [Embodied Referring Expression Comprehension in Human-Robot Interaction](https://arxiv.org/abs/2512.06558)
*Md Mofijul Islam,Alexi Gladstone,Sujan Sarker,Ganesh Nanduru,Md Fahim,Keyan Du,Aman Chadha,Tariq Iqbal*

Main category: cs.RO

TL;DR: 本文提出了Refer360数据集和MuRes多模态模块，用于提升机器人对人体指令的理解能力。通过跨环境、多视角、大规模采集自然语言与非语言互动数据，并设计新方法，用于改进机器人对肢体指令的理解，显著提升了现有多模态模型的表现。


<details>
  <summary>Details</summary>
Motivation: 机器人进入人类工作环境后，需要理解人类包括语言和肢体在内的指令，实现更自然的人机交互。然而，现有数据集局限于单一视角、缺乏肢体动作、多为室内场景，难以支撑泛化和高效的指令理解。论文旨在解决这些数据与方法上的不足。

Method: 论文提出Refer360数据集，涵盖多样化场景、视角、包含语言与非语言指令交互。同时，提出MuRes多模态引导残差模块：作为信息瓶颈，有效提取各模态关键信号，并加强融合形成新的特征，用于下游的指令理解任务。通过实验，将MuRes集成至多个现有多模态模型中。

Result: 在四个人机交互数据集（包括Refer360）上广泛实验，发现当前多模态模型难以充分理解肢体交互；而加入MuRes后，所有任务性能均有明显提升。

Conclusion: Refer360数据集成为了新的基准数据集，适用于评估机器人对人类指令的综合理解。MuRes模块证明了通过引导残差学习，能显著提升机器人对“具身指令表达”的理解能力，对未来机器人在人机共融环境中应用具有极大意义。

Abstract: As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.

</details>


### [318] [Learning Agile Striker Skills for Humanoid Soccer Robots from Noisy Sensory Input](https://arxiv.org/abs/2512.06571)
*Zifan Xu,Myoungkyu Seo,Dongmyeong Lee,Hao Fu,Jiaheng Hu,Jiaxun Cui,Yuqian Jiang,Zhihan Wang,Anastasiia Brund,Joydeep Biswas,Peter Stone*

Main category: cs.RO

TL;DR: 本文提出了一套基于强化学习的仿人机器人连续踢球系统，能适应不同球门配置并应对感知噪声和扰动。


<details>
  <summary>Details</summary>
Motivation: 仿人机器人踢球需要腿部快速摆动、单足稳定及对感知噪声和外部扰动具备鲁棒性，现实环境下难以获得。为此需开发既快速又鲁棒的动作技能训练方法。

Method: 提出扩展的师生强化学习训练框架，包括四个训练阶段：1)球场追球（教师）；2)定向踢球（教师）；3)教师策略精炼（学生）；4)学生自适应和细化。方法融合定制奖励、实际噪声建模与在线约束强化学习等模块。

Result: 在仿真和真实机器人上，系统表现出高踢球准确性和进球率，无论球与球门配置怎样；消融实验验证约束RL、噪声建模与自适应阶段的重要性。

Conclusion: 该系统有效实现了在感知不完美下的鲁棒连续仿人踢球，为仿人机器人全身视觉运动技能学习建立了一个新基准任务。

Abstract: Learning fast and robust ball-kicking skills is a critical capability for humanoid soccer robots, yet it remains a challenging problem due to the need for rapid leg swings, postural stability on a single support foot, and robustness under noisy sensory input and external perturbations (e.g., opponents). This paper presents a reinforcement learning (RL)-based system that enables humanoid robots to execute robust continual ball-kicking with adaptability to different ball-goal configurations. The system extends a typical teacher-student training framework -- in which a "teacher" policy is trained with ground truth state information and the "student" learns to mimic it with noisy, imperfect sensing -- by including four training stages: (1) long-distance ball chasing (teacher); (2) directional kicking (teacher); (3) teacher policy distillation (student); and (4) student adaptation and refinement (student). Key design elements -- including tailored reward functions, realistic noise modeling, and online constrained RL for adaptation and refinement -- are critical for closing the sim-to-real gap and sustaining performance under perceptual uncertainty. Extensive evaluations in both simulation and on a real robot demonstrate strong kicking accuracy and goal-scoring success across diverse ball-goal configurations. Ablation studies further highlight the necessity of the constrained RL, noise modeling, and the adaptation stage. This work presents a system for learning robust continual humanoid ball-kicking under imperfect perception, establishing a benchmark task for visuomotor skill learning in humanoid whole-body control.

</details>


### [319] [Error-Centric PID Untrained Neural-Net (EC-PIDUNN) For Nonlinear Robotics Control](https://arxiv.org/abs/2512.06578)
*Waleed Razzaq*

Main category: cs.RO

TL;DR: 本文提出了一种新型控制架构EC-PIDUNN，将未经训练的神经网络与改进的PID控制器结合，引入稳定因子，显著提升非线性复杂系统中的控制性能。实验显示其在机器人等场景中比传统PID表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统PID控制在面对系统非线性和复杂互联变量时常出现失稳、超调和响应慢等问题。现有神经网络PID方法虽能提升性能，但对数据与算力要求高，阻碍实际应用。研究动机是设计一种能适应复杂非线性系统且实现成本低的新型控制方法。

Method: 提出EC-PIDUNN架构，将未经训练的神经网络输入为系统稳态误差，通过提升输入维度丰富数据表示。同时引入稳定因子τ调节控制信号，以及参数向量ρ_t调节输出轨迹，动态调整PID系数。

Result: 在无人地面车辆与云台系统两类典型非线性机器人应用中，EC-PIDUNN模型均优于传统PID控制，表现为更快收敛和更高稳定性，实现了近似临界阻尼的优良响应。

Conclusion: EC-PIDUNN提供了一种无需大量训练数据的实用改进PID框架，显著提升了复杂非线性系统的控制性能，具有广阔的工业应用前景。

Abstract: Classical Proportional-Integral-Derivative (PID) control has been widely successful across various industrial systems such as chemical processes, robotics, and power systems. However, as these systems evolved, the increase in the nonlinear dynamics and the complexity of interconnected variables have posed challenges that classical PID cannot effectively handle, often leading to instability, overshooting, or prolonged settling times. Researchers have proposed PIDNN models that combine the function approximation capabilities of neural networks with PID control to tackle these nonlinear challenges. However, these models require extensive, highly refined training data and have significant computational costs, making them less favorable for real-world applications. In this paper, We propose a novel EC-PIDUNN architecture, which integrates an untrained neural network with an improved PID controller, incorporating a stabilizing factor (\(τ\)) to generate the control signal. Like classical PID, our architecture uses the steady-state error \(e_t\) as input bypassing the need for explicit knowledge of the systems dynamics. By forming an input vector from \(e_t\) within the neural network, we increase the dimensionality of input allowing for richer data representation. Additionally, we introduce a vector of parameters \( ρ_t \) to shape the output trajectory and a \textit{dynamic compute} function to adjust the PID coefficients from predefined values. We validate the effectiveness of EC-PIDUNN on multiple nonlinear robotics applications: (1) nonlinear unmanned ground vehicle systems that represent the Ackermann steering mechanism and kinematics control, (2) Pan-Tilt movement system. In both tests, it outperforms classical PID in convergence and stability achieving a nearly critically damped response.

</details>


### [320] [A New Trajectory-Oriented Approach to Enhancing Comprehensive Crowd Navigation Performance](https://arxiv.org/abs/2512.06608)
*Xinyu Zhou,Songhao Piao,Chao Gao,Liguo Chen*

Main category: cs.RO

TL;DR: 本文针对群体导航中评价指标优先级模糊及轨迹连续性（$C^2$光滑性）评价缺失的问题，提出了统一的评估框架和显式优化轨迹曲率的新奖励策略，实验验证了在多尺度场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前群体导航研究多用深度强化学习提升效率与舒适性，但对多目标评估指标的优先级缺少明确分析，轨迹连续性指标（如$C^2$光滑度）常被忽略，导致方法公平评估难、轨迹自然性及能效不佳。作者旨在解决评价与轨迹优化的不足。

Method: 本文建立了可公平透明评估多目标优化优先级的统一群体导航评估框架；并设计了新颖的奖励塑形策略，重点引入显式轨迹曲率优化项，从而提升轨迹连续性与质量。

Result: 在2D及3D多尺度实验中，所提出方法在轨迹质量、适应性等方面显著优于当前主流方法，充分验证了新框架及策略的有效性。

Conclusion: 本文方案有效补全了群体导航评价和轨迹优化方面的研究空白，为后续研究和实际应用提供了更公平、高质量的群体导航算法评价及生成工具。

Abstract: Crowd navigation has garnered considerable research interest in recent years, especially with the proliferating application of deep reinforcement learning (DRL) techniques. Many studies, however, do not sufficiently analyze the relative priorities among evaluation metrics, which compromises the fair assessment of methods with divergent objectives. Furthermore, trajectory-continuity metrics, specifically those requiring $C^2$ smoothness, are rarely incorporated. Current DRL approaches generally prioritize efficiency and proximal comfort, often neglecting trajectory optimization or addressing it only through simplistic, unvalidated smoothness reward. Nevertheless, effective trajectory optimization is essential to ensure naturalness, enhance comfort, and maximize the energy efficiency of any navigation system. To address these gaps, this paper proposes a unified framework that enables the fair and transparent assessment of navigation methods by examining the prioritization and joint evaluation of multiple optimization objectives. We further propose a novel reward-shaping strategy that explicitly emphasizes trajectory-curvature optimization. The resulting trajectory quality and adaptability are significantly enhanced across multi-scale scenarios. Through extensive 2D and 3D experiments, we demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches.

</details>


### [321] [Robust Optimization-based Autonomous Dynamic Soaring with a Fixed-Wing UAV](https://arxiv.org/abs/2512.06610)
*Marvin Harms,Jaeyoung Lim,David Rohr,Friedrich Rockenbauer,Nicholas Lawrance,Roland Siegwart*

Main category: cs.RO

TL;DR: 本文提出了一种面向固定翼无人机（UAV）的自主动态翱翔飞行框架，并在仿真和实际飞行测试中验证了其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 动态翱翔能够让飞行动物和飞行器利用风切层能量实现几乎无限的飞行，但对于无人机，尤其是固定翼UAV，如何实现自主动态翱翔仍面临风场估计误差大、路径跟踪不稳等挑战。

Method: 该框架采用显式风场建模和经典制导控制方法。通过设计点对点鲁棒参考轨迹和鲁棒路径跟踪控制器，提升无人机在风场建模误差下的飞行鲁棒性。

Result: 在仿真中，系统对不同风况、测量误差和扰动表现出鲁棒动态翱翔能力。关键模块（如能量预测和路径跟踪）还通过实际飞行得到验证，显示出仿真与实际之间较小的差距。

Conclusion: 结果表明，该框架能有效实现固定翼UAV在风切层中的自主动态翱翔飞行，具有较强鲁棒性和实际应用潜力。

Abstract: Dynamic soaring is a flying technique to exploit the energy available in wind shear layers, enabling potentially unlimited flight without the need for internal energy sources. We propose a framework for autonomous dynamic soaring with a fixed-wing unmanned aerial vehicle (UAV). The framework makes use of an explicit representation of the wind field and a classical approach for guidance and control of the UAV. Robustness to wind field estimation error is achieved by constructing point-wise robust reference paths for dynamic soaring and the development of a robust path following controller for the fixed-wing UAV. The framework is evaluated in dynamic soaring scenarios in simulation and real flight tests. In simulation, we demonstrate robust dynamic soaring flight subject to varied wind conditions, estimation errors and disturbances. Critical components of the framework, including energy predictions and path-following robustness, are further validated in real flights to assure small sim-to-real gap. Together, our results strongly indicate the ability of the proposed framework to achieve autonomous dynamic soaring flight in wind shear.

</details>


### [322] [MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment](https://arxiv.org/abs/2512.06628)
*Ruicheng Zhang,Mingyang Zhang,Jun Zhou,Zhangrui Guo,Xiaofan Liu,Zunnan Xu,Zhizhou Zhong,Puxin Yan,Haocheng Luo,Xiu Li*

Main category: cs.RO

TL;DR: MIND-V提出了一种用于生成符合物理和逻辑规律的长时序机器人操作视频的新方法，显著提升了复杂任务的视频合成能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人模仿学习受限于多样且长时序操作数据的稀缺，目前视频生成方法仅能合成简单短片段且常依靠手工设定轨迹，难以满足实际复杂操作需求。

Method: MIND-V采用分层架构，包括三个核心模块：(1)语义推理中枢（SRH），利用预训练视觉-语言模型进行任务规划；(2)行为语义桥（BSB），将抽象指令转为领域无关表达；(3)马达视频生成器（MVG），实现条件视频渲染。同时提出分阶段视觉未来展演（Staged Visual Future Rollouts）来提高长时序生成稳定性，并引入基于物理前视一致性奖励（PFC）的强化学习后训练，通过V-JEPA模型对视频物理合理性进行约束。

Result: MIND-V在长时序机器人操作视频生成上表现优异，达到了当前最优状态，生成的视频在物理与逻辑层面均具有较强的可控性与一致性。

Conclusion: MIND-V为机器人操作数据的生成提供了一种可扩展、可控且具物理一致性的新范式，有助于推动多样化、复杂机器人任务数据的高效合成。

Abstract: Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.

</details>


### [323] [Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving](https://arxiv.org/abs/2512.06664)
*Wei-Bin Kou,Guangxu Zhu,Jingreng Lei,Chen Zhang,Yik-Chung Wu,Jianping Wang*

Main category: cs.RO

TL;DR: 本文提出了一种用于自动驾驶场景的统计增强、解耦的专家混合路由与聚合机制（MoE-RAM），显著提升了自动驾驶语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 单一深度学习模型很难完全覆盖自动驾驶中的复杂多变场景（如天气、交通密度、道路类型），而现有大型模型驱动的MoE方法在专家选择和聚合时存在路由不准与聚合低效的问题。

Method: 提出MoE-RAM方法：1）在专家路由阶段，通过统计检索机制，将大型模型提取的特征与已有专家的原型特征进行匹配，实现更精确的专家选择；2）在专家聚合阶段，依据专家特征与大模型特征的统计距离自适应加权融合输出，提高最终预测效果。

Result: 在自动驾驶语义分割任务上，通过大量实验验证，MoE-RAM在多个自动驾驶数据集上均优于传统单模型及其他MoE基线方法。

Conclusion: MoE-RAM借助统计增强的专家路由与聚合机制，有效提升了自动驾驶场景下的模型表现，为应对复杂多变环境提供了更优的解决方案。

Abstract: Autonomous driving (AD) scenarios are inherently complex and diverse, posing significant challenges for a single deep learning model to effectively cover all possible conditions, such as varying weather, traffic densities, and road types. Large Model (LM)-Driven Mixture of Experts (MoE) paradigm offers a promising solution, where LM serves as the backbone to extract latent features while MoE serves as the downstream head to dynamically select and aggregate specialized experts to adapt to different scenarios. However, routing and aggregating in MoE face intrinsic challenges, including imprecise expert selection due to flawed routing strategy and inefficient expert aggregation leading to suboptimal prediction. To address these issues, we propose a statistic-augmented, decoupled MoE }outing and Aggregating Mechanism (MoE-RAM) driven by LM. Specifically, on the one hand, MoE-RAM enhances expert routing by incorporating statistical retrieval mechanism to match LM-extracted latent features with cached prototypical features of the most relevant experts; on the other hand, MoE-RAM adaptively reweights experts' outputs in fusion by measuring statistical distances of experts' instant features against LM-extracted latent features. Benefiting from the synergy of the statistic-augmented MoE's routing and aggregating, MoE-RAM ultimately improves the prediction performance. We take the AD semantic segmentation task as an example to assess the proposed MoE-RAM. Extensive experiments on AD datasets demonstrate the superiority of MoE-RAM compared to other MoE baselines and conventional single-model approaches.

</details>


### [324] [FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving](https://arxiv.org/abs/2512.06676)
*Wei-Bin Kou,Guangxu Zhu,Bingyang Cheng,Chen Zhang,Yik-Chung Wu,Jianping Wang*

Main category: cs.RO

TL;DR: 本文提出了一种名为FedDSR的联合深度监督与正则化联邦学习方法，用于提升自动驾驶模型在非IID数据下的泛化能力和收敛速度。通过在模型中引入对中间层的多重监督及正则化，显著提升了分布式自动驾驶系统中的训练效果。实验表明该方法在实际任务中优于主流联邦学习基线。


<details>
  <summary>Details</summary>
Motivation: 目前联邦学习在自动驾驶场景下受到不同车辆数据分布差异（非IID数据）影响，导致模型泛化能力差、收敛速度慢。为解决此问题，作者需设计新方法以提升训练效果、降低收敛轮数。

Method: FedDSR在联邦学习框架下采取三大策略：1）基于架构无关标准选取多个中间层。2）在这些中间层处计算互信息和负熵，将其作为中间损失和正则项，整合进输出层损失，形成统一优化目标。3）在中心服务器聚合采用上述规则本地训练得出的模型参数，获得全局模型。

Result: 以语义分割任务为例，FedDSR在多种模型结构和联邦学习算法下评测，实验结果显示其在mIoU提升高达8.93%，训练轮数减少28.57%，显著优于其它主流联邦学习基线方法。

Conclusion: FedDSR通过联合深度监督与正则化有效提升了联邦自动驾驶模型的泛化性和收敛效率，展现出在实际分布式自动驾驶场景中的广泛应用前景。

Abstract: Federated Learning (FL) enables collaborative training of autonomous driving (AD) models across distributed vehicles while preserving data privacy. However, FL encounters critical challenges such as poor generalization and slow convergence due to non-independent and identically distributed (non-IID) data from diverse driving environments. To overcome these obstacles, we introduce Federated Deep Supervision and Regularization (FedDSR), a paradigm that incorporates multi-access intermediate layer supervision and regularization within federated AD system. Specifically, FedDSR comprises following integral strategies: (I) to select multiple intermediate layers based on predefined architecture-agnostic standards. (II) to compute mutual information (MI) and negative entropy (NE) on those selected layers to serve as intermediate loss and regularizer. These terms are integrated into the output-layer loss to form a unified optimization objective, enabling comprehensive optimization across the network hierarchy. (III) to aggregate models from vehicles trained based on aforementioned rules of (I) and (II) to generate the global model on central server. By guiding and penalizing the learning of feature representations at intermediate stages, FedDSR enhances the model generalization and accelerates model convergence for federated AD. We then take the semantic segmentation task as an example to assess FedDSR and apply FedDSR to multiple model architectures and FL algorithms. Extensive experiments demonstrate that FedDSR achieves up to 8.93% improvement in mIoU and 28.57% reduction in training rounds, compared to other FL baselines, making it highly suitable for practical deployment in federated AD ecosystems.

</details>


### [325] [Model-Less Feedback Control of Space-based Continuum Manipulators using Backbone Tension Optimization](https://arxiv.org/abs/2512.06754)
*Shrreya Rajneesh,Nikita Pavle,Rakesh Kumar Sahoo,Manoranjan Sinha*

Main category: cs.RO

TL;DR: 本文提出了一种无模型的连续体机械臂控制框架，通过在线优化修正经验初始化的Jacobian，实现高精度、稳定的末端控制，无需传统建模，在多个轨迹测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 连续体机械臂在狭小、障碍密集环境中的导航具有独特优势，但因骨架变形的无限维特性、未建模的摩擦和随形态变化的刚度，使得基于模型的运动学方法可靠性不足，出现雅可比矩阵预测不准、奇异性和驱动不稳定的问题。解决这些难题需要全新的控制方法。

Method: 本文采用一种基于无模型思想的控制框架：用经验数据初始化雅可比矩阵，并在操作过程中通过微分凸优化实时调整；通过实时二次规划（Quadratic Program）求解驱动器增量，同时加入避免拉索松弛和结构极限的约束；引入背骨张力优化以稳定轴向载荷并抑制共激活压缩。

Result: 该方法在圆形、五边形、方形等多种轨迹跟踪实验中验证，有效实现了平滑收敛、张力稳定演化，并在无任何模型标定或参数辨识的情况下达到亚毫米的稳态精度。

Conclusion: 提出的无模型控制框架无需建模即可实现高精度、稳定的连续体机械臂控制，是受限环境下模型依赖型方法的可扩展替代方案。

Abstract: Continuum manipulators offer intrinsic dexterity and safe geometric compliance for navigation within confined and obstacle-rich environments. However, their infinite-dimensional backbone deformation, unmodeled internal friction, and configuration-dependent stiffness fundamentally limit the reliability of model-based kinematic formulations, resulting in inaccurate Jacobian predictions, artificial singularities, and unstable actuation behavior. Motivated by these limitations, this work presents a complete model-less control framework that bypasses kinematic modeling by using an empirically initialized Jacobian refined online through differential convex updates. Tip motion is generated via a real-time quadratic program that computes actuator increments while enforcing tendon slack avoidance and geometric limits. A backbone tension optimization term is introduced in this paper to regulate axial loading and suppress co-activation compression. The framework is validated across circular, pentagonal, and square trajectories, demonstrating smooth convergence, stable tension evolution, and sub-millimeter steady-state accuracy without any model calibration or parameter identification. These results establish the proposed controller as a scalable alternative to model-dependent continuum manipulation in a constrained environment.

</details>


### [326] [db-LaCAM: Fast and Scalable Multi-Robot Kinodynamic Motion Planning with Discontinuity-Bounded Search and Lightweight MAPF](https://arxiv.org/abs/2512.06796)
*Akmaral Moldagalieva,Keisuke Okumura,Amanda Prorok,Wolfgang Hönig*

Main category: cs.RO

TL;DR: 本文提出了一种新的多机器人运动规划器db-LaCAM，能高效处理高达50台机器人，规划速度为现有方法的10倍，同时保证解的质量。


<details>
  <summary>Details</summary>
Motivation: 现有多机器人运动规划方法在处理机器人数量增加时，计算负担过重，导致规划效率低、扩展性差。为解决这些问题，需要一个兼顾动态约束、可扩展性和高效率的规划方法。

Method: 作者将多智能体路径规划（MAPF）算法的可扩展性和速度，与动力学感知规划器的动态约束能力相结合，提出了db-LaCAM方法。该方法利用预先计算的动态符合机器人动力学的运动原语生成有限时域内的运动序列，并允许用户自定义连续运动之间的不连续性。该方法支持任意机器人动力学，理论上具备分辨率完备性。

Result: 实验显示，db-LaCAM能在2D和3D（如独轮车、三维双积分等动力学）场景下高效扩展至50台机器人，运行速度比最先进算法快10倍，解的质量相当。同时在真实飞行机器人和拖挂小车机器人试验平台上安全验证了该方法的可行性。

Conclusion: db-LaCAM兼具多智能体路径规划的可扩展性和动力学感知规划的约束处理能力，是一种高效、可扩展且适用于任意动力学场景的多机器人运动规划方法。

Abstract: State-of-the-art multi-robot kinodynamic motion planners struggle to handle more than a few robots due to high computational burden, which limits their scalability and results in slow planning time.
  In this work, we combine the scalability and speed of modern multi-agent path finding (MAPF) algorithms with the dynamic-awareness of kinodynamic planners to address these limitations.
  To this end, we propose discontinuity-Bounded LaCAM (db-LaCAM), a planner that utilizes a precomputed set of motion primitives that respect robot dynamics to generate horizon-length motion sequences, while allowing a user-defined discontinuity between successive motions.
  The planner db-LaCAM is resolution-complete with respect to motion primitives and supports arbitrary robot dynamics.
  Extensive experiments demonstrate that db-LaCAM scales efficiently to scenarios with up to 50 robots, achieving up to ten times faster runtime compared to state-of-the-art planners, while maintaining comparable solution quality.
  The approach is validated in both 2D and 3D environments with dynamics such as the unicycle and 3D double integrator.
  We demonstrate the safe execution of trajectories planned with db-LaCAM in two distinct physical experiments involving teams of flying robots and car-with-trailer robots.

</details>


### [327] [MagicSkin: Balancing Marker and Markerless Modes in Vision-Based Tactile Sensors with a Translucent Skin](https://arxiv.org/abs/2512.06829)
*Oluwatimilehin Tijani,Zhuo Chen,Jiankang Deng,Shan Luo*

Main category: cs.RO

TL;DR: 本文提出了一种新型视觉触觉传感皮肤MagicSkin，采用半透明着色标记，兼顾了传统标记与无标记方案的优点，实现了多模态高性能触觉感知。


<details>
  <summary>Details</summary>
Motivation: 现有视觉触觉传感技术面临两难选择：使用不透明标记虽可检测受力和切向位移，但会遮挡表面几何特征；无标记则易于表面分类但难以追踪切向位移。传统改进方法如紫外标记或深度学习迁移，增加了硬件复杂性或计算负担。

Method: 提出MagicSkin半透明着色标记触觉皮肤，可在保留表面细节的同时，进行切向位移追踪和力度预测。设计简单，无需额外硬件或复杂软件，易于集成到现有GelSight类触觉传感器。

Result: MagicSkin同时在物体分类（99.17%）、纹理分类（93.51%）、切向位移追踪（97%点保留率）和力度预测（总误差提升66%）等多项任务中超越传统有/无标记设计。

Conclusion: 半透明标记的MagicSkin有效消除了传统方案在性能上的权衡，为实现高效的多模态触觉感知和先进的触觉机器人技术奠定了基础。

Abstract: Vision-based tactile sensors (VBTS) face a fundamental trade-off in marker and markerless design on the tactile skin: opaque ink markers enable measurement of force and tangential displacement but completely occlude geometric features necessary for object and texture classification, while markerless skin preserves surface details but struggles in measuring tangential displacements effectively. Current practice to solve the above problem via UV lighting or virtual transfer using learning-based models introduces hardware complexity or computing burdens. This paper introduces MagicSkin, a novel tactile skin with translucent, tinted markers balancing the modes of marker and markerless for VBTS. It enables simultaneous tangential displacement tracking, force prediction, and surface detail preservation. This skin is easy to plug into GelSight-family sensors without requiring additional hardware or software tools. We comprehensively evaluate MagicSkin in downstream tasks. The translucent markers impressively enhance rather than degrade sensing performance compared with traditional markerless and inked marker design: it achieves best performance in object classification (99.17\%), texture classification (93.51\%), tangential displacement tracking (97\% point retention) and force prediction (66\% improvement in total force error). These experimental results demonstrate that translucent skin eliminates the traditional performance trade-off in marker or markerless modes, paving the way for multimodal tactile sensing essential in tactile robotics. See videos at this \href{https://zhuochenn.github.io/MagicSkin_project/}{link}.

</details>


### [328] [Dynamic Visual SLAM using a General 3D Prior](https://arxiv.org/abs/2512.06868)
*Xingguang Zhong,Liren Jin,Marija Popović,Jens Behley,Cyrill Stachniss*

Main category: cs.RO

TL;DR: 本文提出了一种能在动态自然环境下鲁棒估计相机位姿的单目视觉SLAM系统，通过结合几何块状自适应束调整和前馈重建模型，有效隔离动态区域并提升三维重建准确性。


<details>
  <summary>Details</summary>
Motivation: 动态环境中的相机位姿估计与三维重建任务因场景动态性面临显著挑战，传统SLAM系统通常难以鲁棒工作，因此亟需新的解决思路。

Method: 方法上，作者设计了一种融合前馈重建模型与基于几何块的在线束调整的单目SLAM系统。前馈模型用于动态区域过滤和深度预测，之后将其深度估计与束调整输出的几何块进行对齐，有效消除尺度歧义。

Result: 系统能够在动态自然场景下，显著提升相机位姿估计的鲁棒性，提高3D重建质量。

Conclusion: 结合前馈模型和几何优化的新型单目视觉SLAM方法能够有效解决动态环境中的位姿估计难题，对机器人、AR等应用具有实际价值。

Abstract: Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality. However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy. In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes. To this end, we leverage the complementary strengths of geometric patch-based online bundle adjustment and recent feed-forward reconstruction models. Specifically, we propose a feed-forward reconstruction model to precisely filter out dynamic regions, while also utilizing its depth prediction to enhance the robustness of the patch-based visual SLAM. By aligning depth prediction with estimated patches from bundle adjustment, we robustly handle the inherent scale ambiguities of the batch-wise application of the feed-forward reconstruction model.

</details>


### [329] [From Zero to High-Speed Racing: An Autonomous Racing Stack](https://arxiv.org/abs/2512.06892)
*Hassan Jardali,Durgakant Pushp,Youwei Yu,Mahmoud Ali,Ihab S. Mohamed,Alejandro Murillo-Gonzalez,Paul D. Coen,Md. Al-Masrur Khan,Reddy Charan Pulivendula,Saeoul Park,Lingchuan Zhou,Lantao Liu*

Main category: cs.RO

TL;DR: 论文介绍了用于Indy Autonomous Challenge（IAC）的大型无人赛车自主系统ARS，并通过多次实地验证实现了最高260公里/小时的运行速度。


<details>
  <summary>Details</summary>
Motivation: 高速度、对抗性无人赛车面临定位、感知、路径规划、实时控制等巨大挑战，且数据采集和实验成本高昂，急需通用且高效的自主系统解决这些问题。

Method: 提出并迭代开发了三代Autonomous Race Stack（ARS1、ARS2和ARS3），采用模块化架构，分别在不同类型赛道上进行实测验证；对系统的控制、感知和估测性能在环形和常规赛道场景下进行详细对比，此外还公开了相应多传感器高数据集。

Result: 三代ARS系统均已在不同赛道成功部署，赛车速度最高达260公里/小时。实验数据展现了控制、感知和状态估测在各类型赛道的性能差异。

Conclusion: 提出的ARS系统有效应对了高速度无人赛车比赛中的挑战，公开数据集将进一步推动相关领域发展；实测经验为后续全尺寸无人竞速平台的研发提供了重要参考与启示。

Abstract: High-speed, head-to-head autonomous racing presents substantial technical and logistical challenges, including precise localization, rapid perception, dynamic planning, and real-time control-compounded by limited track access and costly hardware. This paper introduces the Autonomous Race Stack (ARS), developed by the IU Luddy Autonomous Racing team for the Indy Autonomous Challenge (IAC). We present three iterations of our ARS, each validated on different tracks and achieving speeds up to 260 km/h. Our contributions include: (i) the modular architecture and evolution of the ARS across ARS1, ARS2, and ARS3; (ii) a detailed performance evaluation that contrasts control, perception, and estimation across oval and road-course environments; and (iii) the release of a high-speed, multi-sensor dataset collected from oval and road-course tracks. Our findings highlight the unique challenges and insights from real-world high-speed full-scale autonomous racing.

</details>


### [330] [Control of Powered Ankle-Foot Prostheses on Compliant Terrain: A Quantitative Approach to Stability Enhancement](https://arxiv.org/abs/2512.06896)
*Chrysostomos Karakasis,Camryn Scully,Robert Salati,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: 本文提出并验证了一种基于导纳控制的假肢控制策略，通过动态调节动力踝足假肢的刚度，提高在柔顺地面行走时的稳定性，显著降低跌倒风险。


<details>
  <summary>Details</summary>
Motivation: 下肢截肢者在柔顺地面行走时面临更高的跌倒风险。虽然动力踝足假肢能适应速度和刚性地面，但针对柔顺表面的控制策略尚有不足，亟需开发更稳健的控制方法以提升穿戴者安全性。

Method: 采用导纳控制策略，根据地面刚度动态调整假肢准刚度。在两种不同刚度的柔顺地面上，招募三名健康受试者进行行走实验，并利用相位轨迹和行走稳定性指标评估控制器性能。将实验结果与传统的相位变量控制器进行对比。

Result: 与传统相位变量控制相比，导纳控制器在所有柔顺地面条件下均能提升步态稳定性，降低跌倒风险。

Conclusion: 自适应、关注稳定性的假肢控制策略能有效提高真实环境下下肢假肢用户的安全性和交互鲁棒性，为康复机器人领域的发展提供了新思路。

Abstract: Walking on compliant terrain presents a substantial challenge for individuals with lower-limb amputation, further elevating their already high risk of falling. While powered ankle-foot prostheses have demonstrated adaptability across speeds and rigid terrains, control strategies optimized for soft or compliant surfaces remain underexplored. This work experimentally validates an admittance-based control strategy that dynamically adjusts the quasi-stiffness of powered prostheses to enhance gait stability on compliant ground. Human subject experiments were conducted with three healthy individuals walking on two bilaterally compliant surfaces with ground stiffness values of 63 and 25 kN/m, representative of real-world soft environments. Controller performance was quantified using phase portraits and two walking stability metrics, offering a direct assessment of fall risk. Compared to a standard phase-variable controller developed for rigid terrain, the proposed admittance controller consistently improved gait stability across all compliant conditions. These results demonstrate the potential of adaptive, stability-aware prosthesis control to reduce fall risk in real-world environments and advance the robustness of human-prosthesis interaction in rehabilitation robotics.

</details>


### [331] [Ground Compliance Improves Retention of Visual Feedback-Based Propulsion Training for Gait Rehabilitation](https://arxiv.org/abs/2512.06897)
*Bradley Hobbs,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: 本研究比较了结合地面顺应性（柔性地面）与视觉反馈训练对步态推进力提升的效果，发现加入地面顺应性比单纯视觉反馈效果更好，对步态康复有启示。


<details>
  <summary>Details</summary>
Motivation: 步态推进力不足会影响中风等疾病后的步态康复，传统视觉反馈训练提升推进力效果有限，因此作者探索是否地面顺应性与视觉反馈联合使用能更有效促进推进力学习和持久化。

Method: 招募十名健康被试，利用定制分带跑台进行步行实验。全部受试者实时获得地面反作用力的视觉反馈；实验组在此基础上步行于顺应性地面，控制组仅视觉反馈。通过指令激发推进力提升，评估实验前后推进力、肌电和关节运动变化。

Result: 两组都能提升推进力，但加入地面顺应性的实验组不仅提升更显著，且在干预后仍能保持推进力、肌电和运动学的持久改变，说明学习效果更佳且自然。

Conclusion: 地面顺应性与视觉反馈联合能加强推进力学习与维持，揭示视觉—本体感觉交互对步态适应的重要性，并为中风后等踩地能力障碍的长期康复提供新思路。

Abstract: This study investigates whether adding ground compliance to visual feedback (VF) gait training is more effective at increasing push-off force (POF) compared to using VF alone, with implications for gait rehabilitation. Ten healthy participants walked on a custom split-belt treadmill. All participants received real-time visual feedback of their ground reaction forces. One group also experienced changes in ground compliance, while a control group received only visual feedback. Intentional increases in propulsive ground reaction forces (POF) were successfully achieved and sustained post-intervention, especially in the group that experienced ground compliance. This group also demonstrated lasting after-effects in muscle activity and joint kinematics, indicating a more robust learning of natural strategies to increase propulsion. This work demonstrates how visual and proprioceptive systems coordinate during gait adaptation. It uniquely shows that combining ground compliance with visual feedback enhances the learning of propulsive forces, supporting the potential use of compliant terrain in long-term rehabilitation targeting propulsion deficits, such as those following a stroke.

</details>


### [332] [Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields](https://arxiv.org/abs/2512.06912)
*Rushiraj Gadhvi,Sandeep Manjanna*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的方法，使自动化表面航行器（ASV）在海洋复杂流场中实现能效最优的自主航行，能够大幅节省能源。


<details>
  <summary>Details</summary>
Motivation: 传统ASV的路径规划方法在部分可观测和动态变化的海洋流场中表现有限，尤其在能源受限下难以实现高效导航。作者希望模仿古老导航实践，在能源受限前提下提升ASV在复杂流场中的自主性和续航能力。

Method: 作者采用基于Soft Actor Critic的端到端强化学习框架，仅利用本地流速测量数据，学习对流感知的导航策略，以适应涡旋等复杂动力流场，实现能量优化导航。

Result: 实验表明，该方法在多种复杂动态流场中展现了强大的泛化能力和明显的能量节约效果，相较现有最优方法，节能提升达30%~50%。

Conclusion: 本文方法为海洋自主导航提供了高效、节能、具备广泛适应性的解决思路，为ASV等自主系统实现长时间、低能耗运行奠定了基础。

Abstract: For centuries, khalasi have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.

</details>


### [333] [Interconnection and Damping Assignment Passivity-Based Control using Sparse Neural ODEs](https://arxiv.org/abs/2512.06935)
*Nicolò Botteghi,Owen Brook,Urban Fasel,Federico Califano*

Main category: cs.RO

TL;DR: 该论文提出了一种基于神经微分方程的IDA-PBC数值化设计方法，突破了传统IDA-PBC难以求解匹配PDE的限制，使该控制方法适用于更复杂系统和任务。


<details>
  <summary>Details</summary>
Motivation: 传统IDA-PBC控制方法因需解析性地求解匹配条件PDE，在实际复杂物理系统与任务中难以实施，主要被局限在学术与稳定性任务上。作者旨在解决IDA-PBC实际应用的重大障碍，即难以解析求解匹配PDE的问题。

Method: 作者将IDA-PBC设计问题转化为神经常微分方程学习，通过稀疏字典学习对闭环系统进行参数化；将任务相关与匹配条件相关的代价组合到一个多目标优化问题中，通过优化获得控制器参数。

Result: 数值实验表明，该方法不仅能使IDA-PBC应用于复杂任务（如周期振荡行为的发现），而且能导出含有剩余项的系统闭式表达式。

Conclusion: 本文提出的数值化IDA-PBC设计方法克服了传统方法的主要障碍，极大拓展了其应用场景，为复杂系统和多样化任务的非线性控制提供了新思路。

Abstract: Interconnection and Damping Assignment Passivity-Based Control (IDA-PBC) is a nonlinear control technique that assigns a port-Hamiltonian (pH) structure to a controlled system using a state-feedback law. While IDA-PBC has been extensively studied and applied to many systems, its practical implementation often remains confined to academic examples and, almost exclusively, to stabilization tasks. The main limitation of IDA-PBC stems from the complexity of analytically solving a set of partial differential equations (PDEs), referred to as the matching conditions, which enforce the pH structure of the closed-loop system. However, this is extremely challenging, especially for complex physical systems and tasks.
  In this work, we propose a novel numerical approach for designing IDA-PBC controllers without solving the matching PDEs exactly. We cast the IDA-PBC problem as the learning of a neural ordinary differential equation. In particular, we rely on sparse dictionary learning to parametrize the desired closed-loop system as a sparse linear combination of nonlinear state-dependent functions. Optimization of the controller parameters is achieved by solving a multi-objective optimization problem whose cost function is composed of a generic task-dependent cost and a matching condition-dependent cost. Our numerical results show that the proposed method enables (i) IDA-PBC to be applicable to complex tasks beyond stabilization, such as the discovery of periodic oscillatory behaviors, (ii) the derivation of closed-form expressions of the controlled system, including residual terms

</details>


### [334] [Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge](https://arxiv.org/abs/2512.06951)
*Ilia Larchenko,Gleb Zarin,Akash Karnatak*

Main category: cs.RO

TL;DR: 本文提出了一种获得2025 BEHAVIOR Challenge冠军的视觉-动作策略，用于解决在逼真场景中包含50种复杂家务任务的挑战。该方法在多项创新的加持下，实现所有任务总q-score为26%。


<details>
  <summary>Details</summary>
Motivation: BEHAVIOR Challenge要求智能体在逼真的家庭环境中完成多样化、长时序家务任务，这类任务包括双手操作、导航和情境感知决策。目前的方法在高效训练、动作平滑和歧义消解等方面仍有限制。

Method: 基于Pi0.5架构，提出了相关噪声的流配对机制提升训练效率并实现具相关性的inpainting，保证动作序列的平滑。同时引入可学习的混合层注意力和System 2阶段跟踪以解决任务歧义。在训练阶段采用多样本流配对减少方差，推理时采用动作压缩和挑战相关纠错规则。

Result: 本方法在BEHAVIOR Challenge所有50项家务任务中，公私榜单均取得26%的q-score，排名第一。

Conclusion: 通过引入相关噪声流配对等技术，提出的方法在复杂家务任务场景下显著提升了智能体的行动平滑性和决策效率，在权威挑战下取得优异成绩，展示了方法的有效性。

Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.
  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.
  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.

</details>


### [335] [VideoVLA: Video Generators Can Be Generalizable Robot Manipulators](https://arxiv.org/abs/2512.06963)
*Yichao Shen,Fangyun Wei,Zhiying Du,Yaobo Liang,Yan Lu,Jiaolong Yang,Nanning Zheng,Baining Guo*

Main category: cs.RO

TL;DR: 该论文提出了VideoVLA，一种利用大规模视频生成模型实现机器人操作通用化的方法，通过同时预测动作与未来视觉结果，显著提升对新任务、新物体与新场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器人在开放世界环境中的有效泛化能力是通用人工智能的重要前提，而现有视觉-语言-动作（VLA）模型面对新任务、物体和环境时泛化能力有限。因此，需要探索能够赋予机器人更强泛化能力的新方法。

Method: 作者提出VideoVLA，将大规模预训练视频生成模型转化为机器人VLA操作器。该方法基于多模态扩散Transformer，实现了视频、语言和动作的联合建模，通过自然语言指令和单张图片输入，联合预测动作序列和未来的视觉结果。利用预训练生成模型进行视觉和动作的联合预测，提升泛化能力。

Result: 实验显示，VideoVLA能够生成高质量的未来视觉预测，这与动作预测的准确性和任务完成率高度相关。该方法在模仿他种机器人技能和处理新物体等方面表现出强泛化能力。

Conclusion: VideoVLA的“双重预测”策略，即同时预测动作及其视觉结果，为机器人学习带来范式转变，极大提升了操作系统的泛化能力，推动了机器人面向通用人工智能的发展。

Abstract: Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.

</details>


### [336] [Parametric Design of a Cable-Driven Coaxial Spherical Parallel Mechanism for Ultrasound Scans](https://arxiv.org/abs/2512.06995)
*Maryam Seraj,Mohammad Hossein Kamrava,Carlo Tiseo*

Main category: cs.RO

TL;DR: 本文提出了一种新型的缆索驱动同轴球面并联机构（CDC-SPM），用于提升医疗远程操作中力觉交互的精度和响应性。


<details>
  <summary>Details</summary>
Motivation: 在医疗远程操作中，医生需通过力/运动反馈与远程环境互动，但目前系统在实现纯转动运动、高带宽、低惯量等方面存在折中，影响操作精度和安全性。

Method: 设计了一种新型的缆索驱动同轴球面并联机构（CDC-SPM），采用并联和同轴驱动的方法，减小末端执行器质量，实现各向同性的力和力矩传递，并对其运动学进行分析与仿真验证。

Result: 仿真和分析表明，CDC-SPM 能实现高精度、快速且安全的力觉反馈，满足高要求的触觉应用。

Conclusion: 这种机制有望广泛应用于如超声成像等需要精确、直观操作的医疗远程操控场景。

Abstract: Haptic interfaces play a critical role in medical teleoperation by enabling surgeons to interact with remote environments through realistic force and motion feedback. Achieving high fidelity in such systems requires balancing performance trade-off among workspace, dexterity, stiffness, inertia, and bandwidth, particularly in applications demanding pure rotational motion. This paper presents the design methodology and kinematic analysis of a Cable-Driven Coaxial Spherical Parallel Mechanism (CDC-SPM) developed to address these challenges. The proposed cable-driven interface design allows for reducing the mass placed at the robot arm end-effector, thereby minimizing inertial loads, enhancing stiffness, and improving dynamic responsiveness. Through parallel and coaxial actuation, the mechanism achieves decoupled rotational degrees of freedom with isotropic force and torque transmission. Simulation and analysis demonstrate that the CDC-SPM provides accurate, responsive, and safe motion characteristics suitable for high-precision haptic applications. These results highlight the mechanism's potential for medical teleoperation tasks such as ultrasound imaging, where precise and intuitive manipulation is essential.

</details>


### [337] [A Hetero-Associative Sequential Memory Model Utilizing Neuromorphic Signals: Validated on a Mobile Manipulator](https://arxiv.org/abs/2512.07032)
*Runcong Wang,Fengyi Wang,Gordon Cheng*

Main category: cs.RO

TL;DR: 本文提出了一种可用于移动操作机器人的异构关联序列记忆系统，通过神经形态的方式将机器人关节状态与触觉观测绑定，实现高效的按步骤动作决策；方法经济、泛化能力良好，并支持多关节复杂行为的联想式召回。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人与触觉结合常常受限于计算和存储资源，难以实现高效动作决策和泛化。作者希望通过神经形态建模，实现低计算和存储开销下，高效、可泛化的感知-动作关联与序列记忆。

Method: 1）用群体位置编码编码关节角度；2）用Izhikevich神经元模型将皮肤接收力转为脉冲率特征；3）二者转为二元向量，逐元素绑定并存储于大容量时序记忆中；4）引入3D旋转位置嵌入，根据受力方向旋转子空间，增强区分度和语义结合；5）通过softmax加权回忆不同时间步的动作序列，实现模糊检索。

Result: 在安装了机器人皮肤的丰田人类支援机器人实验中，该系统可以实现触觉伪顺应控制——按触觉方向与力量幅值带动连杆移动，还能凭持续触觉输入检索多关节抓取序列。训练快速，可直接用同步流数据生成，表现出一定泛化。

Conclusion: 该系统经济、易部署，支持通过关联记忆实现单关节和全臂多关节复杂行为，具有良好推广前景，可拓展至模仿学习、运动规划和多模态整合等方向。

Abstract: This paper presents a hetero-associative sequential memory system for mobile manipulators that learns compact, neuromorphic bindings between robot joint states and tactile observations to produce step-wise action decisions with low compute and memory cost. The method encodes joint angles via population place coding and converts skin-measured forces into spike-rate features using an Izhikevich neuron model; both signals are transformed into bipolar binary vectors and bound element-wise to create associations stored in a large-capacity sequential memory. To improve separability in binary space and inject geometry from touch, we introduce 3D rotary positional embeddings that rotate subspaces as a function of sensed force direction, enabling fuzzy retrieval through a softmax weighted recall over temporally shifted action patterns. On a Toyota Human Support Robot covered by robot skin, the hetero-associative sequential memory system realizes a pseudocompliance controller that moves the link under touch in the direction and with speed correlating to the amplitude of applied force, and it retrieves multi-joint grasp sequences by continuing tactile input. The system sets up quickly, trains from synchronized streams of states and observations, and exhibits a degree of generalization while remaining economical. Results demonstrate single-joint and full-arm behaviors executed via associative recall, and suggest extensions to imitation learning, motion planning, and multi-modal integration.

</details>


### [338] [CERNet: Class-Embedding Predictive-Coding RNN for Unified Robot Motion, Recognition, and Confidence Estimation](https://arxiv.org/abs/2512.07041)
*Hiroki Sawada,Alexandre Pitti,Mathias Quoy*

Main category: cs.RO

TL;DR: 本文提出了一种统一的预测编码递归神经网络（PC-RNN）模型CERNet，实现了机器人运动生成、意图识别和自信度估计三大功能，并通过动态类嵌入向量实现运动生成与识别的统一。模型在实际机器人上验证，运动重现误差低、鲁棒性强、识别准确率高。


<details>
  <summary>Details</summary>
Motivation: 机器人与人类互动时，需要能实时生成已学习的动作，同时识别人类动作的意图，并对自身推断的准确性有所估计。但传统方法常将运动生成和识别分开处理，缺乏对不确定性的自发估计，难以统一地、实时地处理这三大问题。

Method: 作者设计了一种分层预测编码递归神经网络（PC-RNN）并引入动态类嵌入向量：在生成模式下，嵌入向量约束隐藏状态，使其运动生成受限于特定类别子空间；推断模式下，嵌入向量实时优化以最小化预测误差，从而实现在线识别。同时，模型内部的预测误差反映识别的信心度。该策略在实际机器人通过运动示教（如英文字母）进行验证。

Result: 模型在26类机器人运动字母复现中，轨迹重现误差比同参数单层基线低76%，在外部干扰下动作仍具高保真度。在线运动类别识别准确率Top-1为68%、Top-2为81%。模型的内部预测误差可作为信心度指示。

Conclusion: 该方法以紧凑的PC-RNN结构，统一实现了高效的动作生成、实时意图识别及内建不确定性估计，为物理机器人中的运动记忆提供了拓展性强且实用的解决方案，适用于对意图敏感的人机协作任务。

Abstract: Robots interacting with humans must not only generate learned movements in real-time, but also infer the intent behind observed behaviors and estimate the confidence of their own inferences. This paper proposes a unified model that achieves all three capabilities within a single hierarchical predictive-coding recurrent neural network (PC-RNN) equipped with a class embedding vector, CERNet, which leverages a dynamically updated class embedding vector to unify motor generation and recognition. The model operates in two modes: generation and inference. In the generation mode, the class embedding constrains the hidden state dynamics to a class-specific subspace; in the inference mode, it is optimized online to minimize prediction error, enabling real-time recognition. Validated on a humanoid robot across 26 kinesthetically taught alphabets, our hierarchical model achieves 76% lower trajectory reproduction error than a parameter-matched single-layer baseline, maintains motion fidelity under external perturbations, and infers the demonstrated trajectory class online with 68% Top-1 and 81% Top-2 accuracy. Furthermore, internal prediction errors naturally reflect the model's confidence in its recognition. This integration of robust generation, real-time recognition, and intrinsic uncertainty estimation within a compact PC-RNN framework offers a compact and extensible approach to motor memory in physical robots, with potential applications in intent-sensitive human-robot collaboration.

</details>


### [339] [A Flexible Funnel-Shaped Robotic Hand with an Integrated Single-Sheet Valve for Milligram-Scale Powder Handling](https://arxiv.org/abs/2512.07091)
*Tomoya Takahashi,Yusaku Nakajima,Cristian Camilo Beltran-Hernandez,Yuki Kuroda,Kazutoshi Tanaka,Masashi Hamaya,Kanta Ono,Yoshitaka Ushiku*

Main category: cs.RO

TL;DR: 本研究提出了一种新型柔性漏斗形机器人手，实现了高精度毫克级粉末处理，有望推动实验室自动化发展。


<details>
  <summary>Details</summary>
Motivation: 目前实验室自动化设备在粉末研磨和XRD分析领域已有应用，但高精度毫克级粉末处理由于粉末流动复杂性和实验任务多样性，仍面临挑战。解决该难题有助于实现材料发现的全流程自动化。

Method: 设计了一种新型柔性漏斗形机器人手，底部有可控阀门，结合粉末流动建模、在线参数识别与外接天平的反馈控制，实现精确定量分装。并通过不同类型粉末实验证明系统性能，并与传统PID控制进行对比。

Result: 实验显示，80%试验分装误差在2mg以内，最大误差约为20mg，适用于20mg至3g范围。集成流动预测和参数在线调整后，分装精度及收敛速度较PID显著提升。

Conclusion: 该系统实现了高效、灵活的粉末分装，可扩展至更大量级，适用于多种实验室自动化任务，推动实验室自动化向更全面发展。

Abstract: Laboratory Automation (LA) has the potential to accelerate solid-state materials discovery by enabling continuous robotic operation without human intervention. While robotic systems have been developed for tasks such as powder grinding and X-ray diffraction (XRD) analysis, fully automating powder handling at the milligram scale remains a significant challenge due to the complex flow dynamics of powders and the diversity of laboratory tasks. To address this challenge, this study proposes a novel, funnel-shaped, flexible robotic hand that preserves the softness and conical sheet designs in prior work while incorporating a controllable valve at the cone apex to enable precise, incremental dispensing of milligram-scale powder quantities. The hand is integrated with an external balance through a feedback control system based on a model of powder flow and online parameter identification. Experimental evaluations with glass beads, monosodium glutamate, and titanium dioxide demonstrated that 80% of the trials achieved an error within 2 mg, and the maximum error observed was approximately 20 mg across a target range of 20 mg to 3 g. In addition, by incorporating flow prediction models commonly used for hoppers and performing online parameter identification, the system is able to adapt to variations in powder dynamics. Compared to direct PID control, the proposed model-based control significantly improved both accuracy and convergence speed. These results highlight the potential of the proposed system to enable efficient and flexible powder weighing, with scalability toward larger quantities and applicability to a broad range of laboratory automation tasks.

</details>


### [340] [Surrogate compliance modeling enables reinforcement learned locomotion gaits for soft robots](https://arxiv.org/abs/2512.07114)
*Jue Wang,Mingsong Jiang,Luis A. Ramirez,Bilige Yang,Mujun Zhang,Esteban Figueroa,Wenzhong Yan,Rebecca Kramer-Bottiglio*

Main category: cs.RO

TL;DR: 本文提出一种通过刚体模拟器间接建模软体机械柔顺性的策略，允许仿生机器人在模拟中通过学习控制策略，实现优秀的跨环境适应与迁移能力。


<details>
  <summary>Details</summary>
Motivation: 软体机器人具备形态可变能力，适应多变任务和环境，但准确高效地仿真其柔顺性的模拟困难大、计算量大，且现有刚体模拟器无法描述软材料动力学。如何在不精细软体物理建模下，依然获得可靠控制策略并实现有效仿真到真实迁移，是当前研究瓶颈。

Method: 提出软体柔顺性代理建模方法，将软体变形通过间接变量（如肢体有效长度、质心位置）表征，嵌入刚体模拟器。通过强化学习，在这些变量充分随机化下完成策略训练。该方法用在四足两栖拟龟机器人上，其软体肢体可变形以适应多环境运动需求。

Result: 无需软体物理仿真，仅凭上述代理变量与刚体仿真，所学运动策略可直接迁移至物理机器人，实现高保真性能；在复杂地形有较强鲁棒性，并大幅降低能耗。现场实验表现出在多种自然地形下的稳定多步态运动能力。

Conclusion: 代理柔顺性建模结合强化学习，可以显著简化软体机器人仿真难题，并实现高效且可迁移的控制策略。该策略为软体机器人多环境自适应及仿真到现实迁移提供了可行路径。

Abstract: Adaptive morphogenetic robots adapt their morphology and control policies to meet changing tasks and environmental conditions. Many such systems leverage soft components, which enable shape morphing but also introduce simulation and control challenges. Soft-body simulators remain limited in accuracy and computational tractability, while rigid-body simulators cannot capture soft-material dynamics. Here, we present a surrogate compliance modeling approach: rather than explicitly modeling soft-body physics, we introduce indirect variables representing soft-material deformation within a rigid-body simulator. We validate this approach using our amphibious robotic turtle, a quadruped with soft morphing limbs designed for multi-environment locomotion. By capturing deformation effects as changes in effective limb length and limb center of mass, and by applying reinforcement learning with extensive randomization of these indirect variables, we achieve reliable policy learning entirely in a rigid-body simulation. The resulting gaits transfer directly to hardware, demonstrating high-fidelity sim-to-real performance on hard, flat substrates and robust, though lower-fidelity, transfer on rheologically complex terrains. The learned closed-loop gaits exhibit unprecedented terrestrial maneuverability and achieve an order-of-magnitude reduction in cost of transport compared to open-loop baselines. Field experiments with the robot further demonstrate stable, multi-gait locomotion across diverse natural terrains, including gravel, grass, and mud.

</details>


### [341] [Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving](https://arxiv.org/abs/2512.07130)
*Zebin Xing,Yupeng Zheng,Qichao Zhang,Zhixing Ding,Pengxuan Yang,Songen Gu,Zhongpu Xia,Dongbin Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种名为Mimir的分层双系统框架，能够通过目标点及其不确定性估计，为端到端自动驾驶生成更鲁棒的轨迹，并显著提升了推理速度和驾驶表现。


<details>
  <summary>Details</summary>
Motivation: 目前端到端自动驾驶依赖高层引导信号来指导低层轨迹，但现有方法常受限于引导信号不准确以及复杂引导系统造成的高计算开销。本文旨在提升自动驾驶的轨迹鲁棒性，并解决高层引导慢速瓶颈。

Method: Mimir框架包括两大创新：一是用拉普拉斯分布建模目标点的不确定性，以提升整体鲁棒性；二是提出多速率引导机制，提前预测扩展目标点，加快高层模块推理速度。

Result: 在Navhard和Navtest等挑战性自动驾驶基准测试上，Mimir实现了驾驶分数EPDMS提升20%，高层模块推理速度提升1.6倍，且精度未受影响。

Conclusion: Mimir框架有效提升了端到端自动驾驶系统的鲁棒性与计算效率，是该领域的重要进展。代码即将开源，有助于促进相关研究的复现与发展。

Abstract: End-to-end autonomous driving has emerged as a pivotal direction in the field of autonomous systems. Recent works have demonstrated impressive performance by incorporating high-level guidance signals to steer low-level trajectory planners. However, their potential is often constrained by inaccurate high-level guidance and the computational overhead of complex guidance modules. To address these limitations, we propose Mimir, a novel hierarchical dual-system framework capable of generating robust trajectories relying on goal points with uncertainty estimation: (1) Unlike previous approaches that deterministically model, we estimate goal point uncertainty with a Laplace distribution to enhance robustness; (2) To overcome the slow inference speed of the guidance system, we introduce a multi-rate guidance mechanism that predicts extended goal points in advance. Validated on challenging Navhard and Navtest benchmarks, Mimir surpasses previous state-of-the-art methods with a 20% improvement in the driving score EPDMS, while achieving 1.6 times improvement in high-level module inference speed without compromising accuracy. The code and models will be released soon to promote reproducibility and further development. The code is available at https://github.com/ZebinX/Mimir-Uncertainty-Driving

</details>


### [342] [Time-Varying Formation Tracking Control of Wheeled Mobile Robots With Region Constraint: A Generalized Udwadia-Kalaba Framework](https://arxiv.org/abs/2512.07137)
*Kang Yijie,Hao Yuqing,Wang Qingyun,Chen Guanrong*

Main category: cs.RO

TL;DR: 本文提出了一种基于广义Udwadia-Kalaba框架的区域约束下轮式移动机器人时变编队跟踪控制方法，并通过数值仿真验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 以往的时变编队跟踪控制研究较少考虑机器人运动中的安全性区域约束。为了提高系统安全性，需在保持跟踪编队任务的同时，确保机器人运动始终受限于安全区域内。

Method: 首先将时变编队跟踪控制目标重写为受约束方程，将区域约束通过微分同胚变换形式化，然后在广义Udwadia-Kalaba理论框架下设计区域约束的时变编队跟踪控制器。此外，基于有向加权通信拓扑，该拓扑包含领导者作为根的生成树结构。

Result: 仿真实验表明，所提出的控制策略能够有效实现轮式移动机器人在安全区域内的时变编队跟踪，验证了方法的可行性和有效性。

Conclusion: 该方法成功将区域约束纳入时变编队跟踪控制，有效提升了系统安全性和实际应用的可靠性。

Abstract: In this paper, the time-varying formation tracking control of wheeled mobile robots with region constraint is investigated from a generalized Udwadia-Kalaba framework. The communication topology is directed, weighted and has a spanning tree with the leader being the root. By reformulating the time-varying formation tracking control objective as a constrained equation and transforming the region constraint by a diffeomorphism, the time-varying formation tracking controller with the region constraint is designed under the generalized Udwadia-Kalaba framework. Compared with the existing works on time-varying formation tracking control, the region constraint is takeninto account in this paper, which ensures the safety of the robots.Finally, some numerical simulations are presented to illustrate the effectiveness of the proposed control strategy.

</details>


### [343] [Using Vision-Language Models as Proxies for Social Intelligence in Human-Robot Interaction](https://arxiv.org/abs/2512.07177)
*Fanjun Bu,Melina Tsai,Audrey Tjokro,Tapomayukh Bhattacharjee,Jorge Ortiz,Wendy Ju*

Main category: cs.RO

TL;DR: 本文研究服务机器人如何通过人的非语言行为判断是否与人交互，并提出结合轻量传感器和视觉语言模型的新方法提升机器人社交能力。


<details>
  <summary>Details</summary>
Motivation: 在现实环境中，机器人需判断与人互动的合适时机，但人的非语言信号复杂且难以建模，因此需要有效利用这些信号提升机器人交互体验。

Method: 通过在咖啡馆的五天Wizard-of-Oz实验，观察和分析人类的非语言互动信号，并提出一个两阶段管线：先用低成本传感器检测（如目光与距离），在关键时刻激活计算量更大的视觉-语言模型以获取更深入理解。比较两种提示策略。

Result: 验证了该方法在真实互动录像中的有效性，并发现选择性调用视觉-语言模型可以更好地模拟社交推理，提升机器人对人自然信号的响应能力。

Conclusion: 分阶段、选择性利用视觉-语言模型有助于机器人在日常环境中更好地感知和响应人类的社交信号，提升互动得当性和社交智能。

Abstract: Robots operating in everyday environments must often decide when and whether to engage with people, yet such decisions often hinge on subtle nonverbal cues that unfold over time and are difficult to model explicitly. Drawing on a five-day Wizard-of-Oz deployment of a mobile service robot in a university cafe, we analyze how people signal interaction readiness through nonverbal behaviors and how expert wizards use these cues to guide engagement. Motivated by these observations, we propose a two-stage pipeline in which lightweight perceptual detectors (gaze shifts and proxemics) are used to selectively trigger heavier video-based vision-language model (VLM) queries at socially meaningful moments. We evaluate this pipeline on replayed field interactions and compare two prompting strategies. Our findings suggest that selectively using VLMs as proxies for social reasoning enables socially responsive robot behavior, allowing robots to act appropriately by attending to the cues people naturally provide in real-world interactions.

</details>


### [344] [Spatiotemporal Calibration and Ground Truth Estimation for High-Precision SLAM Benchmarking in Extended Reality](https://arxiv.org/abs/2512.07221)
*Zichao Shu,Shitao Bei,Lijun Li,Zetao Chen*

Main category: cs.RO

TL;DR: 这篇论文提出了一种新方法，针对基于MoCap系统用于SLAM评测中存在的时空标定和数据抖动问题，提高了基准测试的精度，尤其适用于XR应用。


<details>
  <summary>Details</summary>
Motivation: 随着XR沉浸体验标准的提升，对SLAM评测准确性要求越来越高。目前常用的MoCap系统虽然精度高但仍受限于与测试设备之间的时空标定误差和数据抖动，这些问题影响关键指标（特别是旋转误差和帧间抖动）的评测，阻碍了SLAM发展。

Method: 提出了一种连续时间最大似然估计算法，结合辅助IMU数据来修正MoCap抖动。同时，采用可变时间同步方法和基于螺旋共线约束的位姿残差，实现多传感器和待测设备间的高精度时空标定。

Result: 实验表明，该方法在多项基准测试指标上优于现有方法，能够有效提升SLAM算法在XR应用中的评测精度。实际评测了多个主流XR设备和开源SLAM算法，验证了方法的有效性和广泛适用性。

Conclusion: 所提出的方法显著提高了SLAM基准测试的精度，特别解决了XR应用中对高精度空间跟踪的需求，并为业界和学界提供了可复现的工具。

Abstract: Simultaneous localization and mapping (SLAM) plays a fundamental role in extended reality (XR) applications. As the standards for immersion in XR continue to increase, the demands for SLAM benchmarking have become more stringent. Trajectory accuracy is the key metric, and marker-based optical motion capture (MoCap) systems are widely used to generate ground truth (GT) because of their drift-free and relatively accurate measurements. However, the precision of MoCap-based GT is limited by two factors: the spatiotemporal calibration with the device under test (DUT) and the inherent jitter in the MoCap measurements. These limitations hinder accurate SLAM benchmarking, particularly for key metrics like rotation error and inter-frame jitter, which are critical for immersive XR experiences. This paper presents a novel continuous-time maximum likelihood estimator to address these challenges. The proposed method integrates auxiliary inertial measurement unit (IMU) data to compensate for MoCap jitter. Additionally, a variable time synchronization method and a pose residual based on screw congruence constraints are proposed, enabling precise spatiotemporal calibration across multiple sensors and the DUT. Experimental results demonstrate that our approach outperforms existing methods, achieving the precision necessary for comprehensive benchmarking of state-of-the-art SLAM algorithms in XR applications. Furthermore, we thoroughly validate the practicality of our method by benchmarking several leading XR devices and open-source SLAM algorithms. The code is publicly available at https://github.com/ylab-xrpg/xr-hpgt.

</details>


### [345] [SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks](https://arxiv.org/abs/2512.07266)
*Florian Tretter,Daniel Flögel,Alexandru Vasilache,Max Grobbel,Jürgen Becker,Sören Hohmann*

Main category: cs.RO

TL;DR: 本文提出了一种混合型社会化深度强化学习（DRL）方法，将脉冲神经网络(SNN)和人工神经网络(ANN)结合，在提升机器人社会导航能力的同时，大幅度减少能耗。


<details>
  <summary>Details</summary>
Motivation: 现有自主移动机器人在集成人类环境时，需要具备类人决策和高能效的事件驱动计算，但由于训练不稳定，神经形态方法很少用于深度强化学习导航领域，因此有必要探索更有效的解决方案。

Method: 提出一种混合型社会集成DRL的actor-critic方法：actor部分用脉冲神经网络(SNN)，critic部分用人工神经网络(ANN)，同时通过神经形态特征提取器捕捉人群动态和人机交互的时序特征。

Result: 该方法提升了社会导航的性能，并将能耗估算降低至原来的约1.69个数量级。

Conclusion: 混合SNN+ANN的DRL导航方法能够兼顾社会行为理解和能量效率，有望推进自主机器人在实际人类环境中的应用。

Abstract: Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.

</details>


### [346] [Efficient Computation of a Continuous Topological Model of the Configuration Space of Tethered Mobile Robots](https://arxiv.org/abs/2512.07303)
*Gianpietro Battocletti,Dimitris Boskos,Bart De Schutter*

Main category: cs.RO

TL;DR: 该论文提出了一种全新的方法，将系带机器人配置空间的拓扑信息与机器人在工作空间中的连续位置结合建模，并应用于路径规划问题。


<details>
  <summary>Details</summary>
Motivation: 目前系带机器人路径规划方法多基于离散配置空间，难以同时考虑系带的拓扑特性和机器人的连续位置；为此需要建立更高效且细致的配置空间模型，以提升路径规划性能。

Method: 首先将系带机器人配置空间与工作空间的覆盖空间建立联系，然后据此开发出一种计算配置空间单纯复形拓扑模型的算法，实现对空间拓扑与连续性的统一建模。

Result: 这种建模方法相较于传统的同伦增强图表示，运算速度大大提升，仅需其一部分时间即可完成建模，且模型具备连续性，可兼容多种路径规划算法。

Conclusion: 提出的模型在精度和效率上优于传统方法，为系带机器人路径规划提供了更高效且通用的解决方案。

Abstract: Despite the attention that the problem of path planning for tethered robots has garnered in the past few decades, the approaches proposed to solve it typically rely on a discrete representation of the configuration space and do not exploit a model that can simultaneously capture the topological information of the tether and the continuous location of the robot. In this work, we explicitly build a topological model of the configuration space of a tethered robot starting from a polygonal representation of the workspace where the robot moves. To do so, we first establish a link between the configuration space of the tethered robot and the universal covering space of the workspace, and then we exploit this link to develop an algorithm to compute a simplicial complex model of the configuration space. We show how this approach improves the performances of existing algorithms that build other types of representations of the configuration space. The proposed model can be computed in a fraction of the time required to build traditional homotopy-augmented graphs, and is continuous, allowing to solve the path planning task for tethered robots using a broad set of path planning algorithms.

</details>


### [347] [Model Predictive Control for Cooperative Docking Between Autonomous Surface Vehicles with Disturbance Rejection](https://arxiv.org/abs/2512.07316)
*Gianpietro Battocletti,Dimitris Boskos,Bart De Schutter*

Main category: cs.RO

TL;DR: 该论文提出了一种多无人水面艇（USV）协同对接的方法，通过集中式模型预测控制（MPC）实现更快速、高效的对接。


<details>
  <summary>Details</summary>
Motivation: 现有USV对接方法主要让一方静止、另一方靠近，效率有限。在实际任务中，双方都积极配合将提升对接效果，因此需要新的协同对接方案。

Method: 本文采用集中式MPC控制方法，将两艘USV的运动综合建模，通过预测和约束处理，规划双方可行的对接轨迹。该方法还能通过预测外部扰动（如水流）对USV的影响，提高鲁棒性。

Result: 仿真结果表明，所提协作方法相比传统单向对接方法，对接速度更快，效率更高，且能更好地应对如水流等近静态扰动。

Conclusion: 提出的协同MPC对接方法提升了多USV协作的对接性能，对实际应用具有参考意义。

Abstract: Uncrewed Surface Vehicles (USVs) are a popular and efficient type of marine craft that find application in a large number of water-based tasks. When multiple USVs operate in the same area, they may be required to dock to each other to perform a shared task. Existing approaches for the docking between autonomous USVs generally consider one USV as a stationary target, while the second one is tasked to reach the required docking pose. In this work, we propose a cooperative approach for USV-USV docking, where two USVs work together to dock at an agreed location. We use a centralized Model Predictive Control (MPC) approach to solve the control problem, obtaining feasible trajectories that also guarantee constraint satisfaction. Owing to its model-based nature, this approach allows the rejection of disturbances, inclusive of exogenous inputs, by anticipating their effect on the USVs through the MPC prediction model. This is particularly effective in case of almost-stationary disturbances such as water currents. In simulations, we demonstrate how the proposed approach allows for a faster and more efficient docking with respect to existing approaches.

</details>


### [348] [Multi-Rigid-Body Approximation of Human Hands with Application to Digital Twin](https://arxiv.org/abs/2512.07359)
*Bin Zhao,Yiwen Lu,Haohua Zhu,Xiao Li,Sheng Yi*

Main category: cs.RO

TL;DR: 本文提出了一种实现高效、真实感人手物理仿真的完整流程，兼顾人体结构精确性和实时性能。


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生和仿真应用中对人手模型要求既要精确还要高效。传统模型在物理仿真、运动还原等场合受限于骨骼运动自由度和转动处理方式，如何将高度自由的手部运动精确转换为可物理仿真的多刚体模型，是提升仿真精度和效率的关键难点。

Method: 作者首先通过光学动作捕捉获取特定人手数据，构建个性化的MANO手部模型。之后，将该模型转换为URDF机器手描述格式，确保关节轴设定符合解剖学。为解决MANO模型中的SO(3)自由旋转无法直接映射到刚体系统的运动约束，作者针对一自由度和二自由度关节，分别提出封闭解和BCH校正的迭代算法，实现正确的旋转投影和转换处理。最后，通过数字孪生实验（使用强化学习策略驱动多刚体手重演真人操作）进行评估。

Result: 实验证明该方法可以在多种操作任务中实现物理仿真手准确重现真人演示，重建误差低于一厘米，各类抓取任务均能成功完成。

Conclusion: 本文方法有效结合了真实手部外观、多刚体运动模型与高效物理仿真，为手部动作的数字孪生与智能交互提供了新的解决思路。

Abstract: Human hand simulation plays a critical role in digital twin applications, requiring models that balance anatomical fidelity with computational efficiency. We present a complete pipeline for constructing multi-rigid-body approximations of human hands that preserve realistic appearance while enabling real-time physics simulation. Starting from optical motion capture of a specific human hand, we construct a personalized MANO (Multi-Abstracted hand model with Neural Operations) model and convert it to a URDF (Unified Robot Description Format) representation with anatomically consistent joint axes. The key technical challenge is projecting MANO's unconstrained SO(3) joint rotations onto the kinematically constrained joints of the rigid-body model. We derive closed-form solutions for single degree-of-freedom joints and introduce a Baker-Campbell-Hausdorff (BCH)-corrected iterative method for two degree-of-freedom joints that properly handles the non-commutativity of rotations. We validate our approach through digital twin experiments where reinforcement learning policies control the multi-rigid-body hand to replay captured human demonstrations. Quantitative evaluation shows sub-centimeter reconstruction error and successful grasp execution across diverse manipulation tasks.

</details>


### [349] [ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning](https://arxiv.org/abs/2512.07371)
*Byungju Kim,Jinu Pahk,Chungwoo Lee,Jaejoon Kim,Jangha Lee,Theo Taeyeong Kim,Kyuhwan Shim,Jun Ki Lee,Byoung-Tak Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为ESPADA的新框架，通过语义和空间信息区分操作演示中的关键与非关键片段，实现高效加速，同时保持精度，无需额外数据或重训练。实验证明其在多种场景下可实现约2倍加速，且几乎不损失成功率。


<details>
  <summary>Details</summary>
Motivation: 行为克隆的视觉运动策略虽然精确，但因为模仿人类演示时的缓慢谨慎动作，导致实际应用速度缓慢。现有加速方法多依赖经验规则，缺乏任务语义理解，难以适应复杂多变的任务。

Method: 作者提出ESPADA框架，首先利用VLM-LLM流水线和3D手爪-物体关系，对演示进行语义和空间分段。非关键片段大幅下采样以提速，关键精密片段则保留，且整个流程无需新数据、结构改动或重训练。接着借助动态时间规整（DTW）机制，将单条标注传播到整个数据集。

Result: 在仿真和真实场景下，ESPADA与现有基线（ACT、DP）对比，实现了约2倍的速度提升，同时保持任务成功率不降，显著缩小了人类演示与高效机器人控制之间的差距。

Conclusion: ESPADA能够有效加速视觉运动模仿中的非关键操作部分，同时保证精度，将行为克隆更好地应用于高效机器人操作，且具备较好的通用性和可扩展性。

Abstract: Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.

</details>


### [350] [Gait-Adaptive Perceptive Humanoid Locomotion with Real-Time Under-Base Terrain Reconstruction](https://arxiv.org/abs/2512.07464)
*Haolin Song,Hongbo Zhu,Tao Yu,Yan Liu,Mingqi Yuan,Wengang Zhou,Hua Chen,Houqiang Li*

Main category: cs.RO

TL;DR: 本文提出了一种整合地形感知、步态调节和全身控制的感知运动框架，通过强化学习实现全尺寸人形机器人在复杂地形（如长楼梯）上的稳定行走。通过向下深度摄像头和U-Net网络实时生成本体高度图，并与本体感知数据结合生成步态和姿态控制信号。实验表明该方法在仿真和现实中均实现了鲁棒的楼梯上下与跨越障碍能力。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习为人形机器人行走控制带来了进步，但面对复杂地形（如长楼梯）仍缺乏鲁棒性，感知受限和步态时序调整不足可能导致失衡，因此亟需融合感知与控制的方法提升适应性。

Method: 采用装有朝下深度摄像头的机器人，利用U-Net网络实时重建足下高度图，将感知数据与本体感知结合输入统一的强化学习策略网络，同时同时输出关节命令与步态时序信号，实现步态时序与全身控制协同。训练上采用教师-学生继教策略高效迁移知识。

Result: 在31自由度、1.65米高的人形机器人上验证，系统在仿真与现实中均能实现向前、向后上下楼梯及跨越46厘米间隙，展现出良好的稳健性和适应性。

Conclusion: 感知—控制一体化的强化学习框架显著提升了人形机器人在复杂动态地形上的行走能力，为未来全尺寸多功能人形机器人实用落地提供了技术基础。

Abstract: For full-size humanoid robots, even with recent advances in reinforcement learning-based control, achieving reliable locomotion on complex terrains, such as long staircases, remains challenging. In such settings, limited perception, ambiguous terrain cues, and insufficient adaptation of gait timing can cause even a single misplaced or mistimed step to result in rapid loss of balance. We introduce a perceptive locomotion framework that merges terrain sensing, gait regulation, and whole-body control into a single reinforcement learning policy. A downward-facing depth camera mounted under the base observes the support region around the feet, and a compact U-Net reconstructs a dense egocentric height map from each frame in real time, operating at the same frequency as the control loop. The perceptual height map, together with proprioceptive observations, is processed by a unified policy that produces joint commands and a global stepping-phase signal, allowing gait timing and whole-body posture to be adapted jointly to the commanded motion and local terrain geometry. We further adopt a single-stage successive teacher-student training scheme for efficient policy learning and knowledge transfer. Experiments conducted on a 31-DoF, 1.65 m humanoid robot demonstrate robust locomotion in both simulation and real-world settings, including forward and backward stair ascent and descent, as well as crossing a 46 cm gap. Project Page:https://ga-phl.github.io/

</details>


### [351] [Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation](https://arxiv.org/abs/2512.07472)
*Siyu Xu,Zijian Wang,Yunke Wang,Chenghao Xia,Tao Huang,Chang Xu*

Main category: cs.RO

TL;DR: 本文提出了一种名为Affordance Field Intervention (AFI)的混合框架，通过引入3D空间可行域（SAF），增强现有视觉-语言-动作（VLA）模型在分布变化场景下的适应性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在面对测试场景分布变化时（如场景布局变化、任务变化等）容易受到“记忆陷阱”影响——机械性地重复记忆的轨迹，缺乏对新环境的适应，影响操作的鲁棒性。这一局限主要来源于其端到端设计中缺乏显式的三维空间推理能力。

Method: 作者提出了一种轻量级混合框架（AFI），将3D空间可行域（SAF）作为插件融入到VLA模型中。AFI通过本体感觉侦测识别记忆陷阱，并通过重新定位机器人到高可行性区域、为VLA生成的动作引入基于可行性的锚点引导。最后，系统使用SAF打分器选择累计可行性最高的轨迹。

Result: 在现实机器人平台的超分布测试任务上，AFI在不同类型VLA的平均性能提升23.5%；在LIBERO-Pro基准上提升20.2%。证明了方法能显著提高VLA的鲁棒性和泛化能力。

Conclusion: 通过结合3D空间可行域，本文提出的AFI框架有效缓解了VLA模型在分布变化下的记忆陷阱问题，提升其在新环境中的适应性和表现。

Abstract: Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the "Memory Trap". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($π_{0}$ and $π_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.

</details>


### [352] [From Real-World Traffic Data to Relevant Critical Scenarios](https://arxiv.org/abs/2512.07482)
*Florian Lüttner,Nicole Neis,Daniel Stadler,Robin Moss,Mirjam Fehling-Kaschek,Matthias Pfriem,Alexander Stolz,Jens Ziehn*

Main category: cs.RO

TL;DR: 本文提出了自动化和辅助驾驶系统在高速公路变道场景下的安全验证方法，包括真实数据收集、关键性评估以及基于数据生成合成关键场景。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶和高级驾驶辅助系统需要在多样且复杂的道路场景中可靠运行，但如何覆盖全部可能且危险的驾驶场景十分困难，因此需要高效识别与生成安全相关场景，提高系统验证效率。

Method: 本研究以高速公路变道场景为例，收集并处理真实世界高速公路交通数据，并对车辆轨迹数据施加关键性（criticality）指标进行评价，进而关联特定变道场景与条件。为扩展场景空间，还基于已有数据生成合成未知危险场景。

Result: 建立了一套链式处理流程，从数据采集、场景识别到合成场景生成，实现了安全关键场景的高效提取，并能为自动驾驶系统验证生成更多代表性、危险场景。

Conclusion: 本方法可显著提升自动驾驶系统在关键场景下的测试与验证效率，并为今后在更复杂城市交通等场景的推广应用奠定基础。

Abstract: The reliable operation of autonomous vehicles, automated driving functions, and advanced driver assistance systems across a wide range of relevant scenarios is critical for their development and deployment. Identifying a near-complete set of relevant driving scenarios for such functionalities is challenging due to numerous degrees of freedom involved, each affecting the outcomes of the driving scenario differently. Moreover, with increasing technical complexity of new functionalities, the number of potentially relevant, particularly "unknown unsafe" scenarios is increasing. To enhance validation efficiency, it is essential to identify relevant scenarios in advance, starting with simpler domains like highways before moving to more complex environments such as urban traffic. To address this, this paper focuses on analyzing lane change scenarios in highway traffic, which involve multiple degrees of freedom and present numerous safetyrelevant scenarios. We describe the process of data acquisition and processing of real-world data from public highway traffic, followed by the application of criticality measures on trajectory data to evaluate scenarios, as conducted within the AVEAS project (www.aveas.org). By linking the calculated measures to specific lane change driving scenarios and the conditions under which the data was collected, we facilitate the identification of safetyrelevant driving scenarios for various applications. Further, to tackle the extensive range of "unknown unsafe" scenarios, we propose a way to generate relevant scenarios by creating synthetic scenarios based on recorded ones. Consequently, we demonstrate and evaluate a processing chain that enables the identification of safety-relevant scenarios, the development of data-driven methods for extracting these scenarios, and the generation of synthetic critical scenarios via sampling on highways.

</details>


### [353] [VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform](https://arxiv.org/abs/2512.07507)
*Yiming Cui,Shiyu Fang,Jiarui Zhang,Yan Huang,Chengkai Xu,Bing Zhu,Hao Zhang,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: 本文提出了一种用于自动驾驶汽车的虚实融合测试平台（VP-AutoTest），能够整合多类型虚拟与物理元素，支持多种测试模式，并具备多维评估与自动诊断能力，有效提升了自动驾驶测试的真实性、广度及效率。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟仿真、封闭跑道和实际道路测试等传统测试方法存在车辆状态不真实、测试能力有限与成本高等问题，不能满足自动驾驶快速发展的测试需求。因此亟需更真实、高效和全面的测试手段。

Method: 提出VP-AutoTest平台，融合超过十种虚实元素（如车辆、行人、路侧设施），支持单车交互与多车协同，以对抗测试和并行推理快速发现系统极限与缺陷。通信方面，OBU与Redis实现车车/车路协同。平台内置多维评估和AI专家系统用于性能评估与缺陷诊断，同时通过与真实实验比对实现自我可信度评估。

Result: VP-AutoTest平台实现了多类型交通参与者、多模态协同及多维度性能评估，能覆盖更广、更真实的测试场景，并通过自我评估机制提升了测试的可靠性。此外，平台能够加速系统故障检测和性能极限探索。

Conclusion: VP-AutoTest有效拓展了自动驾驶测试的覆盖面，提高了测试的真实性和效率，对现有自动驾驶虚实融合测试的发展具有显著促进作用。

Abstract: The rapid development of autonomous vehicles has led to a surge in testing demand. Traditional testing methods, such as virtual simulation, closed-course, and public road testing, face several challenges, including unrealistic vehicle states, limited testing capabilities, and high costs. These issues have prompted increasing interest in virtual-physical fusion testing. However, despite its potential, virtual-physical fusion testing still faces challenges, such as limited element types, narrow testing scope, and fixed evaluation metrics. To address these challenges, we propose the Virtual-Physical Testing Platform for Autonomous Vehicles (VP-AutoTest), which integrates over ten types of virtual and physical elements, including vehicles, pedestrians, and roadside infrastructure, to replicate the diversity of real-world traffic participants. The platform also supports both single-vehicle interaction and multi-vehicle cooperation testing, employing adversarial testing and parallel deduction to accelerate fault detection and explore algorithmic limits, while OBU and Redis communication enable seamless vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) cooperation across all levels of cooperative automation. Furthermore, VP-AutoTest incorporates a multidimensional evaluation framework and AI-driven expert systems to conduct comprehensive performance assessment and defect diagnosis. Finally, by comparing virtual-physical fusion test results with real-world experiments, the platform performs credibility self-evaluation to ensure both the fidelity and efficiency of autonomous driving testing. Please refer to the website for the full testing functionalities on the autonomous driving public service platform OnSite:https://www.onsite.com.cn.

</details>


### [354] [See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations](https://arxiv.org/abs/2512.07582)
*Guangyan Chen,Meiling Wang,Qi Shao,Zichen Zhou,Weixin Mao,Te Cui,Minzhao Zhu,Yinan Deng,Luojie Yang,Zhanqi Zhang,Yi Yang,Hua Chen,Yufeng Yue*

Main category: cs.RO

TL;DR: 本文提出了一种通用型机器人操作策略ViVLA，可以仅通过测试时的一段专家演示视频，实现新任务的高效学习。ViVLA联合处理专家演示和机器人视觉观测，实现知识迁移，且能大幅提升新任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 目前VLA（视觉-语言-动作）机器人控制模型在跨分布任务上的泛化能力有限。人类能够通过一次观察快速学习新技能，如何让机器人具备类似能力，成为亟需解决的问题。

Method: ViVLA通过同时处理专家演示视频与自身视觉观测，预测动作序列，并对专家行为进行知识蒸馏。为支持训练，提出了专家-代理人数据对自动生成流程，通过爬取人类视频和公开数据集合成892,911组训练样本。

Result: ViVLA在测试时仅需一段专家示范即可习得新技能，在LIBERO未见任务上效果提升超30%，在跨具身视频下仍有35%以上提升。现实环境实验也验证了从人类视频学习的有效性，未见任务提升超38%。

Conclusion: ViVLA实现了从单次专家演示高效学习新操作任务，具备较强的泛化性能，为机器人学习带来了显著突破。

Abstract: Developing robust and general-purpose manipulation policies represents a fundamental objective in robotics research. While Vision-Language-Action (VLA) models have demonstrated promising capabilities for end-to-end robot control, existing approaches still exhibit limited generalization to tasks beyond their training distributions. In contrast, humans possess remarkable proficiency in acquiring novel skills by simply observing others performing them once. Inspired by this capability, we propose ViVLA, a generalist robotic manipulation policy that achieves efficient task learning from a single expert demonstration video at test time. Our approach jointly processes an expert demonstration video alongside the robot's visual observations to predict both the demonstrated action sequences and subsequent robot actions, effectively distilling fine-grained manipulation knowledge from expert behavior and transferring it seamlessly to the agent. To enhance the performance of ViVLA, we develop a scalable expert-agent pair data generation pipeline capable of synthesizing paired trajectories from easily accessible human videos, further augmented by curated pairs from publicly available datasets. This pipeline produces a total of 892,911 expert-agent samples for training ViVLA. Experimental results demonstrate that our ViVLA is able to acquire novel manipulation skills from only a single expert demonstration video at test time. Our approach achieves over 30% improvement on unseen LIBERO tasks and maintains above 35% gains with cross-embodiment videos. Real-world experiments demonstrate effective learning from human videos, yielding more than 38% improvement on unseen tasks.

</details>


### [355] [Multi-Domain Motion Embedding: Expressive Real-Time Mimicry for Legged Robots](https://arxiv.org/abs/2512.07673)
*Matthias Heyrman,Chenhao Li,Victor Klemm,Dongho Kang,Stelian Coros,Marco Hutter*

Main category: cs.RO

TL;DR: 提出了一种新颖的机器人动作表达方法MDME，可实时模仿多样复杂的动作风格，具备优异的泛化能力和还原度。


<details>
  <summary>Details</summary>
Motivation: 现有动作控制器通常忽略了运动中的内在模式，难以同时捕捉结构化的周期模式和不规则变化，不利于机器人高质量仿人和仿生动作。

Method: 提出了MDME（Multi-Domain Motion Embedding）方法，利用小波编码器联合概率式嵌入对动作进行多维建模，有效融合结构化与非结构化特征；通过把学习到的动作嵌入作为机器人控制策略的条件输入，实现无需动作重定向的实时控制。

Result: MDME在仿人/四足机器人平台上，能准确重现复杂轨迹，比现有方法在动作还原度和对新动作的泛化能力上均表现更好。

Conclusion: MDME为机器人实时动作模仿提供了强大、可泛化、结构感知的动作基础，无需特定调优或在线重定向，支持多样化动作风格的零样本部署。

Abstract: Effective motion representation is crucial for enabling robots to imitate expressive behaviors in real time, yet existing motion controllers often ignore inherent patterns in motion. Previous efforts in representation learning do not attempt to jointly capture structured periodic patterns and irregular variations in human and animal movement. To address this, we present Multi-Domain Motion Embedding (MDME), a motion representation that unifies the embedding of structured and unstructured features using a wavelet-based encoder and a probabilistic embedding in parallel. This produces a rich representation of reference motions from a minimal input set, enabling improved generalization across diverse motion styles and morphologies. We evaluate MDME on retargeting-free real-time motion imitation by conditioning robot control policies on the learned embeddings, demonstrating accurate reproduction of complex trajectories on both humanoid and quadruped platforms. Our comparative studies confirm that MDME outperforms prior approaches in reconstruction fidelity and generalizability to unseen motions. Furthermore, we demonstrate that MDME can reproduce novel motion styles in real-time through zero-shot deployment, eliminating the need for task-specific tuning or online retargeting. These results position MDME as a generalizable and structure-aware foundation for scalable real-time robot imitation.

</details>


### [356] [AMBER: Aerial deployable gripping crawler with compliant microspine for canopy manipulation](https://arxiv.org/abs/2512.07680)
*P. A. Wigner,L. Romanello,A. Hammad,P. H. Nguyen,T. Lan,S. F. Armanini,B. B. Kocer,M. Kovac*

Main category: cs.RO

TL;DR: 论文提出了一种可空中部署的树冠爬行机器人，集成了多项创新结构，实现了高效安全的分枝抓取和移动，具备良好的能效和机动性，适合环境监测应用。


<details>
  <summary>Details</summary>
Motivation: 树冠环境难以通过传统地面或空中机器人完全覆盖。现有空中机器人在树冠内部的操作和续航受限，亟需设计能够适应树冠复杂地形且能高效长时间工作的机器人。

Method: 设计了一套由柔顺微刺履带、双履带旋转抓手和弹性尾部组成的爬行机构，通过空中无人机部署，机器人能贴合并可靠抓附各种曲率与倾角的树枝。实验验证了其攀爬、抓持、转向性能，并评估了能耗。

Result: 实验显示，该机器人能在90°滚转和倾斜下稳定抓持，并可在最大67.5°倾斜的树枝上有效攀爬，水平枝速度达0.55体长/秒，履带可实现10°偏航转向。能耗远低于典型悬停飞行器。

Conclusion: 提出的机器人系统兼具可靠性、高能效与灵活的越障能力，特别适合树冠环境监测和生态采样，可弥补空中、地面机器人在该应用中的不足。

Abstract: This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination.
  Experiments demonstrate reliable gripping up to 90 degrees of body roll and inclination, while effective climbing on branches inclined up to 67.5 degrees, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10 degrees, enhancing maneuverability on irregular surfaces.
  Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. Integrated within a drone-tether deployment system, the crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing, bridging the gap between aerial and surface-based ecological robotics.

</details>


### [357] [Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks](https://arxiv.org/abs/2512.07697)
*Aileen Liao,Dong-Ki Kim,Max Olan Smith,Ali-akbar Agha-mohammadi,Shayegan Omidshafiei*

Main category: cs.RO

TL;DR: 本文提出了一种新的策略学习方法Delay-Aware Diffusion Policy (DA-DP)，能有效应对机器人感知与执行中的状态延迟问题，并在多种任务与机器人上验证其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 以往强化学习或模仿学习假设推理和动作执行几乎无延迟，但实际过程中机器人感知和行动间存在延迟（几十到几百毫秒），这会导致基于过时状态做决策，从而影响任务表现。

Method: 提出DA-DP框架，主要包括两点：(1) 针对带延迟的轨迹进行“延迟补偿”修正，使训练数据与实际延迟情景一致；(2) 将延迟信息作为条件输入融入策略网络。此外，该方法不依赖具体网络结构，可迁移到多类模仿学习策略。

Result: 在多个机器人和任务上验证了DA-DP，在存在不同长度的延迟时，相较于传统不考虑延迟的方法，其成功率更高且表现更稳定，具有更强鲁棒性。

Conclusion: DA-DP为解决机器人决策推理延迟提供了通用方法，建议今后评测机器人算法时，不仅要看任务难度，更要报告在不同实时性条件下的表现。

Abstract: As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.

</details>


### [358] [Toward Seamless Physical Human-Humanoid Interaction: Insights from Control, Intent, and Modeling with a Vision for What Comes Next](https://arxiv.org/abs/2512.07765)
*Gustavo A. Cardona,Shubham S. Kumbhar,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: 本文回顾了物理人-仿人机器人交互（pHHI）的最新进展，围绕建模与控制、人类意图估计以及人类计算模型三大核心方向，梳理了现状、挑战和未来路径。


<details>
  <summary>Details</summary>
Motivation: pHHI在用于真实、复杂和人类为中心的环境中有巨大应用潜力，但现有成果依然存在诸多难题，尤其在多领域协作与集成方面有待突破，因此需系统梳理现状并指明研究方向。

Method: 作者综述了建模与控制、人类意图估计、计算人类模型三大研究支柱，对每个方向的典型方法、未解决挑战与局限进行剖析，并对未来如何跨支柱整合提出建议。同时，建立统一交互类型分类法，按交互方式和机器人参与程度区分，并为各类别指出研究机会。

Result: 评述表明，尽管各方向各自取得进展，但整合性不强，实际应用受限。所提出的未来统一研究路径和交互类型分类为后续方法融合指明了思路，也揭示了提升交互健壮性与自然性的关键环节。

Conclusion: 文章为pHHI领域的未来发展绘制了操作性路线图，通过整合各支柱研究促进更健壮、安全、直观的物理互动，为仿人机器人在人类环境中的高效协作奠定理论基础。

Abstract: Physical Human-Humanoid Interaction (pHHI) is a rapidly advancing field with significant implications for deploying robots in unstructured, human-centric environments. In this review, we examine the current state of the art in pHHI through three core pillars: (i) humanoid modeling and control, (ii) human intent estimation, and (iii) computational human models. For each pillar, we survey representative approaches, identify open challenges, and analyze current limitations that hinder robust, scalable, and adaptive interaction. These include the need for whole-body control strategies capable of handling uncertain human dynamics, real-time intent inference under limited sensing, and modeling techniques that account for variability in human physical states. Although significant progress has been made within each domain, integration across pillars remains limited. We propose pathways for unifying methods across these areas to enable cohesive interaction frameworks. This structure enables us not only to map the current landscape but also to propose concrete directions for future research that aim to bridge these domains. Additionally, we introduce a unified taxonomy of interaction types based on modality, distinguishing between direct interactions (e.g., physical contact) and indirect interactions (e.g., object-mediated), and on the level of robot engagement, ranging from assistance to cooperation and collaboration. For each category in this taxonomy, we provide the three core pillars that highlight opportunities for cross-pillar unification. Our goal is to suggest avenues to advance robust, safe, and intuitive physical interaction, providing a roadmap for future research that will allow humanoid systems to effectively understand, anticipate, and collaborate with human partners in diverse real-world settings.

</details>


### [359] [OptMap: Geometric Map Distillation via Submodular Maximization](https://arxiv.org/abs/2512.07775)
*David Thorne,Nathan Chan,Christa S. Robison,Philip R. Osteen,Brett T. Lopez*

Main category: cs.RO

TL;DR: 本论文提出了OptMap，一种面向实时、特定应用的几何地图萃取算法，能够高效地从LiDAR数据中选取信息丰富、体积受限的地图，用于自主机器人多样化感知与决策任务。


<details>
  <summary>Details</summary>
Motivation: 自主机器人依赖几何地图进行多层次认知与规划，但不同任务对地图信息有不同需求。在LiDAR感知下，从大量点云中选出既小巧又信息丰富的地图是NP-难的组合优化问题，亟需有效解决方案。

Method: 作者提出OptMap算法，基于子模性思想，设计了新颖的子模奖励函数衡量信息量，压缩输入集规模并减少数据偏差。同时提出动态重排序流式子模算法，通过近似评估所有扫描的价值，有效消除输入顺序偏差。该算法支持高效的多任务自适应地图生成，并适用于长时间采集场景。

Result: 在开源与自制数据集上的实验证明，OptMap在长时地图构建任务下计算资源消耗极低，并能够生成高质量、应用相关的Lean地图。ROS1和ROS2开源包已支持算法并兼容任意LiDAR SLAM。

Conclusion: OptMap有效解决了基于LiDAR的自主机器人应用中，信息量-体积相权衡的地图生成难题，具备理论最优保证、实时性强、开源易用等优势，助力多尺度环境感知与决策。

Abstract: Autonomous robots rely on geometric maps to inform a diverse set of perception and decision-making algorithms. As autonomy requires reasoning and planning on multiple scales of the environment, each algorithm may require a different map for optimal performance. Light Detection And Ranging (LiDAR) sensors generate an abundance of geometric data to satisfy these diverse requirements, but selecting informative, size-constrained maps is computationally challenging as it requires solving an NP-hard combinatorial optimization. In this work we present OptMap: a geometric map distillation algorithm which achieves real-time, application-specific map generation via multiple theoretical and algorithmic innovations. A central feature is the maximization of set functions that exhibit diminishing returns, i.e., submodularity, using polynomial-time algorithms with provably near-optimal solutions. We formulate a novel submodular reward function which quantifies informativeness, reduces input set sizes, and minimizes bias in sequentially collected datasets. Further, we propose a dynamically reordered streaming submodular algorithm which improves empirical solution quality and addresses input order bias via an online approximation of the value of all scans. Testing was conducted on open-source and custom datasets with an emphasis on long-duration mapping sessions, highlighting OptMap's minimal computation requirements. Open-source ROS1 and ROS2 packages are available and can be used alongside any LiDAR SLAM algorithm.

</details>


### [360] [Inchworm-Inspired Soft Robot with Groove-Guided Locomotion](https://arxiv.org/abs/2512.07813)
*Hari Prakash Thanabalan,Lars Bengtsson,Ugo Lafont,Giovanni Volpe*

Main category: cs.RO

TL;DR: 本论文提出一种模仿尺蠖的软体机器人，通过在基底上设计凹槽图案，无需多执行器即可实现行进方向的精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有软体机器人通常需多个执行器实现定向控制，这加大了机械复杂性、控制难度和能耗。因此，亟需一种简单、高效的方向控制方法。

Method: 设计并制造了仅用一个卷曲介电弹性体执行器的软体机器人，通过3D打印不同角度的凹槽基底，引导机器人运动方向，并通过系统实验分析不同凹槽参数对运动轨迹的影响。

Result: 实验结果显示，改变基底凹槽角度即可有效、精确控制机器人运动方向，无需复杂的动力学控制或多执行器协作，并能显著降低能耗。

Conclusion: 采用基底结构引导而非多执行器的软体机器人方向控制方法，简化了结构、降低能耗，有望广泛应用于搜救、管道检测、行星探索等多个领域。

Abstract: Soft robots require directional control to navigate complex terrains. However, achieving such control often requires multiple actuators, which increases mechanical complexity, complicates control systems, and raises energy consumption. Here, we introduce an inchworm-inspired soft robot whose locomotion direction is controlled passively by patterned substrates. The robot employs a single rolled dielectric elastomer actuator, while groove patterns on a 3D-printed substrate guide its alignment and trajectory. Through systematic experiments, we demonstrate that varying groove angles enables precise control of locomotion direction without the need for complex actuation strategies. This groove-guided approach reduces energy consumption, simplifies robot design, and expands the applicability of bio-inspired soft robots in fields such as search and rescue, pipe inspection, and planetary exploration.

</details>


### [361] [Efficient and Compliant Control Framework for Versatile Human-Humanoid Collaborative Transportation](https://arxiv.org/abs/2512.07819)
*Shubham S. Kumbhar,Abhijeet M. Kulkarni,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: 本文提出了一个控制框架，使类人机器人能够与人类协作搬运物体，实现平移和旋转等常见协作动作。该方法通过高级规划、底层控制和弹性调节机制实现。


<details>
  <summary>Details</summary>
Motivation: 当前类人机器人在与人类合作搬运任务中，特别是在平移与旋转交互中面临控制与动态耦合的难题，因此需要一种高效、协调的控制方案。

Method: 方法包括高级规划器（引入I-LIP并结合自适应模型和MPC进行了步态动态规划）、QP整体身体控制器（兼顾机器人与物体的动力学耦合），以及刚度调节策略以保证最终相对机械配置。通过提出新的效率评价指标量化机器人与人之间的协作质量。

Result: 在Digit人形机器人上进行了真实实验，验证了框架在平移、转弯、半圆等多种自然协作搬运任务中的有效性。提出的效率度量展示了顺应性在协作任务中对性能的影响，并揭示了高、低层控制对轨迹优劣的洞见。

Conclusion: 该框架能有效支持类人机器人与人类间的协同搬运，且能通过新的度量标准分析协作机制，为复杂任务下的机器人控制提供了理论及实验依据。

Abstract: We present a control framework that enables humanoid robots to perform collaborative transportation tasks with a human partner. The framework supports both translational and rotational motions, which are fundamental to co-transport scenarios. It comprises three components: a high-level planner, a low-level controller, and a stiffness modulation mechanism. At the planning level, we introduce the Interaction Linear Inverted Pendulum (I-LIP), which, combined with an admittance model and an MPC formulation, generates dynamically feasible footstep plans. These are executed by a QP-based whole-body controller that accounts for the coupled humanoid-object dynamics. Stiffness modulation regulates robot-object interaction, ensuring convergence to the desired relative configuration defined by the distance between the object and the robot's center of mass. We validate the effectiveness of the framework through real-world experiments conducted on the Digit humanoid platform. To quantify collaboration quality, we propose an efficiency metric that captures both task performance and inter-agent coordination. We show that this metric highlights the role of compliance in collaborative tasks and offers insights into desirable trajectory characteristics across both high- and low-level control layers. Finally, we showcase experimental results on collaborative behaviors, including translation, turning, and combined motions such as semi circular trajectories, representative of naturally occurring co-transportation tasks.

</details>
