<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 69]
- [cs.CL](#cs.CL) [Total: 56]
- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants](https://arxiv.org/abs/2511.08609)
*I. Bailo,F. Buonora,G. Ciarfaglia,L. T. Consoli,A. Evangelista,M. Gabusi,M. Ghiani,C. Petracca Ciavarella,F. Picariello,F. Sarcina,F. Tuosto,V. Zullo,L. Airoldi,G. Bruno,D. D. Gobbo,S. Pezzenati,G. A. Tona*

Main category: cs.CV

TL;DR: 本文介绍了利用生成式人工智能模型，自动化采集和数字化SNAM能源基础设施（意大利大型燃气运输公司）工厂结构信息的AI系统方案，实现对工厂P&ID（管道与仪表流程图）文档的自动解析和关键信息提取。


<details>
  <summary>Details</summary>
Motivation: 随着能源转型和可持续发展需求的提升，能源基础设施的数字化变得尤为重要。人工读取和录入工厂设计文件效率低，耗时耗力，且易错，因此亟需通过AI技术实现信息自动化提取，提高管理效率。

Method: 提出了一套结合OCR、视觉大模型、目标检测、关系推理及优化算法的AI解决方案，输入为PDF格式的工厂P&ID文件。针对复杂组件关系，创新性地将Transformers引入场景图生成模型，更深入地建模工厂组件间关系，并整合多种AI算法协同处理。

Result: 文本设计数据的提取准确率达到91%；工厂拓扑组件识别准确率达93%；层级结构抽取准确率约为80%。有效应对了数据高度多样化和缺乏标准化带来的挑战。

Conclusion: 融合多种AI方法的自动化系统方案可大幅提升燃气厂数字化效率，准确识别大量非标准化设计信息，对能源基础设施数字化和自动管理具有实际推动意义。

Abstract: The energy transition is a key theme of the last decades to determine a future of eco-sustainability, and an area of such importance cannot disregard digitization, innovation and the new technological tools available. This is the context in which the Generative Artificial Intelligence models described in this paper are positioned, developed by Engineering Ingegneria Informatica SpA in order to automate the plant structures acquisition of SNAM energy infrastructure, a leading gas transportation company in Italy and Europe. The digitization of a gas plant consists in registering all its relevant information through the interpretation of the related documentation. The aim of this work is therefore to design an effective solution based on Artificial Intelligence techniques to automate the extraction of the information necessary for the digitization of a plant, in order to streamline the daily work of MGM users. The solution received the P&ID of the plant as input, each one in pdf format, and uses OCR, Vision LLM, Object Detection, Relational Reasoning and optimization algorithms to return an output consisting of two sets of information: a structured overview of the relevant design data and the hierarchical framework of the plant. To achieve convincing results, we extend a state-of-the-art model for Scene Graph Generation introducing a brand new Transformer architecture with the aim of deepening the analysis of the complex relations between the plant's components. The synergistic use of the listed AI-based technologies allowed to overcome many obstacles arising from the high variety of data, due to the lack of standardization. An accuracy of 91\% has been achieved in the extraction of textual information relating to design data. Regarding the plants topology, 93\% of components are correctly identified and the hierarchical structure is extracted with an accuracy around 80\%.

</details>


### [2] [Assessing Identity Leakage in Talking Face Generation: Metrics and Evaluation Framework](https://arxiv.org/abs/2511.08613)
*Dogucan Yaman,Fevziye Irem Eyiokur,Hazım Kemal Ekenel,Alexander Waibel*

Main category: cs.CV

TL;DR: 本文关注于‘以修补为基础’的说话人脸生成任务，提出一种系统化方法来量化和分析生成过程中出现的嘴唇泄漏问题，并引入新的评估指标和测试设置，推动该领域基准的改进。


<details>
  <summary>Details</summary>
Motivation: 现有以修补为基础的人脸生成方法，虽然可以保持姿态、光照等细节，但容易出现‘嘴唇泄漏’，即生成嘴唇受到参考图像影响，不完全依赖音频驱动，且该现象难以被传统指标或测试检测。缺乏有效评估标准影响了模型改进与实际应用。

Method: 作者提出了一套系统的评测方法，包括三种互补的测试场景（静音输入生成、音视频错配、音视频匹配合成）。同时引入新指标，如唇同步差异和静音-音频唇同步分数，并分析不同身份参考的选择对泄漏的影响。

Result: 研究表明，新提出的评测方法和指标能够更好地发现和衡量嘴唇泄漏问题。通过不同身份参考选取的对比分析，获得了关于如何设计参考图像以减小泄漏的经验。

Conclusion: 本文提出的方法具有模型无关性，为未来说话人脸生成任务提供了更可靠的基准，助力相关研究更准确地评估和改进生成模型。

Abstract: Inpainting-based talking face generation aims to preserve video details such as pose, lighting, and gestures while modifying only lip motion, often using an identity reference image to maintain speaker consistency. However, this mechanism can introduce lip leaking, where generated lips are influenced by the reference image rather than solely by the driving audio. Such leakage is difficult to detect with standard metrics and conventional test setup. To address this, we propose a systematic evaluation methodology to analyze and quantify lip leakage. Our framework employs three complementary test setups: silent-input generation, mismatched audio-video pairing, and matched audio-video synthesis. We also introduce derived metrics including lip-sync discrepancy and silent-audio-based lip-sync scores. In addition, we study how different identity reference selections affect leakage, providing insights into reference design. The proposed methodology is model-agnostic and establishes a more reliable benchmark for future research in talking face generation.

</details>


### [3] [A Multi-Drone Multi-View Dataset and Deep Learning Framework for Pedestrian Detection and Tracking](https://arxiv.org/abs/2511.08615)
*Kosta Dakic,Kanchana Thilakarathna,Rodrigo N. Calheiros,Teng Joon Lim*

Main category: cs.CV

TL;DR: 本文提出了MATRIX多无人机行人跟踪新数据集及深度学习框架，显著提升了动态复杂环境下的多视角监控能力，并验证了模型的鲁棒性与泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有多无人机监控方法难以应对动态相机位置和复杂遮挡，缺乏高质量同步多视角数据集。为此，作者构建了多无人机、动态场景和遮挡条件下的新基准，以推动该领域发展。

Method: 1. 构建MATRIX数据集，包含8架无人机同步拍摄、动态变化位置、40名行人在带有巨大遮挡的城市环境中活动。2. 提出多任务深度学习框架，包含实时相机标定、特征级图像配准以及鸟瞰图视角的多视角特征融合，实现检测与追踪。3. 进行静态与动态场景广泛实验，兼顾迁移学习与相机掉线的系统鲁棒性分析。

Result: 数据和实验表明：在简化场景下，经典静态方法检测与追踪准确率达90%以上；但在复杂MATRIX场景中，性能大幅下降。作者方法在高难度环境下依然保持90%左右的精度，并能追踪约80%的行进轨迹。此外，迁移学习提升了泛化能力，相机掉线时系统表现出鲁棒特性。

Conclusion: MATRIX数据集与检测跟踪框架为多无人机动态监控领域提供了新基准，显著提升了复杂环境下的多视角追踪效果，对实际部署和后续研究具有重要推动作用。

Abstract: Multi-drone surveillance systems offer enhanced coverage and robustness for pedestrian tracking, yet existing approaches struggle with dynamic camera positions and complex occlusions. This paper introduces MATRIX (Multi-Aerial TRacking In compleX environments), a comprehensive dataset featuring synchronized footage from eight drones with continuously changing positions, and a novel deep learning framework for multi-view detection and tracking. Unlike existing datasets that rely on static cameras or limited drone coverage, MATRIX provides a challenging scenario with 40 pedestrians and a significant architectural obstruction in an urban environment. Our framework addresses the unique challenges of dynamic drone-based surveillance through real-time camera calibration, feature-based image registration, and multi-view feature fusion in bird's-eye-view (BEV) representation. Experimental results demonstrate that while static camera methods maintain over 90\% detection and tracking precision and accuracy metrics in a simplified MATRIX environment without an obstruction, 10 pedestrians and a much smaller observational area, their performance significantly degrades in the complex environment. Our proposed approach maintains robust performance with $\sim$90\% detection and tracking accuracy, as well as successfully tracks $\sim$80\% of trajectories under challenging conditions. Transfer learning experiments reveal strong generalization capabilities, with the pretrained model achieving much higher detection and tracking accuracy performance compared to training the model from scratch. Additionally, systematic camera dropout experiments reveal graceful performance degradation, demonstrating practical robustness for real-world deployments where camera failures may occur. The MATRIX dataset and framework provide essential benchmarks for advancing dynamic multi-view surveillance systems.

</details>


### [4] [Learning Topology-Driven Multi-Subspace Fusion for Grassmannian Deep Network](https://arxiv.org/abs/2511.08628)
*Xuan Yu,Tianyang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种拓扑驱动的多子空间融合框架，通过在Grassmann流形上实现自适应子空间协作，提高了对复杂几何结构的刻画能力，并在多个任务上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Grassmann流形的几何表示学习方法多为静态单子空间建模，难以捕捉多子空间间的动态关系，限制了模型对高维复杂结构的表达能力。

Method: 1）借鉴Kolmogorov-Arnold表示定理，设计了一种自适应多子空间建模机制，通过拓扑收敛性分析动态选择与加权相关子空间；2）提出多子空间交互模块，通过在流形空间上的Fréchet均值优化融合不同几何表征，并通过Riemannian批归一化和互信息正则提升判别性和鲁棒性。

Result: 该方法在3D动作识别（HDM05, FPHA）、脑电分类（MAMEM-SSVEPII）和图任务上表现优异，取得了最新的性能。理论上证明了自适应子空间的收敛性保障。

Conclusion: 框架有效扩展了欧氏网络中多通道交互理念到非欧空间，提升了几何深度学习的判别性与可解释性，并推动了相关领域的发展。

Abstract: Grassmannian manifold offers a powerful carrier for geometric representation learning by modelling high-dimensional data as low-dimensional subspaces. However, existing approaches predominantly rely on static single-subspace representations, neglecting the dynamic interplay between multiple subspaces critical for capturing complex geometric structures. To address this limitation, we propose a topology-driven multi-subspace fusion framework that enables adaptive subspace collaboration on the Grassmannian. Our solution introduces two key innovations: (1) Inspired by the Kolmogorov-Arnold representation theorem, an adaptive multi-subspace modelling mechanism is proposed that dynamically selects and weights task-relevant subspaces via topological convergence analysis, and (2) a multi-subspace interaction block that fuses heterogeneous geometric representations through Fréchet mean optimisation on the manifold. Theoretically, we establish the convergence guarantees of adaptive subspaces under a projection metric topology, ensuring stable gradient-based optimisation. Practically, we integrate Riemannian batch normalisation and mutual information regularisation to enhance discriminability and robustness. Extensive experiments on 3D action recognition (HDM05, FPHA), EEG classification (MAMEM-SSVEPII), and graph tasks demonstrate state-of-the-art performance. Our work not only advances geometric deep learning but also successfully adapts the proven multi-channel interaction philosophy of Euclidean networks to non-Euclidean domains, achieving superior discriminability and interpretability.

</details>


### [5] [Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising](https://arxiv.org/abs/2511.08633)
*Assaf Singer,Noam Rotstein,Amir Mann,Ron Kimmel,Or Litany*

Main category: cs.CV

TL;DR: 提出了Time-to-Move（TTM）框架，实现了无需训练即可通过简单参考动画精准控制视频运动和外观。TTM适配于任何I2V扩散模型，兼具易用性和高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图像或文本扩散的视频生成方法难以精确控制运动，且以运动为条件的生成方法通常需为每个模型进行昂贵的微调，限制泛用性和效率。

Method: 提出TTM框架，利用用户友好操作（如剪切-拖拽或深度重投影）得到粗糙参考动画，作为运动粗线索。引入dual-clock denoising策略，对指定运动区域实施区域依赖式降噪，兼顾对运动和外观的精细控制。整个流程无需额外训练或推理成本，可兼容任意扩散模型骨干。

Result: 在物体与相机运动的基准测试中，TTM在真实性和运动控制上与训练型方法持平甚至更优。同时，TTM可实现像素级外观控制，超越仅靠文本提示的精度。

Conclusion: TTM是一种轻量级、无需训练即可精确控制运动和外观的视频生成方法，具有广泛适应性和优越性能，为扩散模型视频生成带来精细化、简便化的运动控制能力。

Abstract: Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: https://time-to-move.github.io/.

</details>


### [6] [CADIC: Continual Anomaly Detection Based on Incremental Coreset](https://arxiv.org/abs/2511.08634)
*Gen Yang,Zhipeng Deng,Junfeng Man*

Main category: cs.CV

TL;DR: 本文提出了一种共享统一记忆库的连续异常检测方法，有效适应新任务且提升了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有连续异常检测方法需为每个任务构建独立的类别记忆库，这造成了方法的灵活性和可扩展性受限。为解决这一问题，作者希望设计一种无需为每个任务分别维护记忆库的方法。

Method: 使用统一的固定大小的记忆库（coreset），所有任务共同更新和使用这一记忆库。在训练阶段，持续增量式地更新嵌入向量；在推理阶段，通过最近邻匹配机制计算异常分数。

Result: 在MVTec AD和Visa数据集上，方法取得了平均图像级AUROC分别为0.972和0.891，超过了现有方法。在真实电子纸数据集上对异常样本检测达到100%准确率。

Conclusion: 该方法在连续异常检测任务下，显著提升了准确率和实际应用鲁棒性，并克服了以往方法的可扩展性与灵活性问题。代码将开源，利于社区使用和发展。

Abstract: The primary objective of Continual Anomaly Detection (CAD) is to learn the normal patterns of new tasks under dynamic data distribution assumptions while mitigating catastrophic forgetting. Existing embedding-based CAD approaches continuously update a memory bank with new embeddings to adapt to sequential tasks. However, these methods require constructing class-specific sub-memory banks for each task, which restricts their flexibility and scalability. To address this limitation, we propose a novel CAD framework where all tasks share a unified memory bank. During training, the method incrementally updates embeddings within a fixed-size coreset, enabling continuous knowledge acquisition from sequential tasks without task-specific memory fragmentation. In the inference phase, anomaly scores are computed via a nearest-neighbor matching mechanism, achieving state-of-the-art detection accuracy. We validate the method through comprehensive experiments on MVTec AD and Visa datasets. Results show that our approach outperforms existing baselines, achieving average image-level AUROC scores of 0.972 (MVTec AD) and 0.891 (Visa). Notably, on a real-world electronic paper dataset, it demonstrates 100% accuracy in anomaly sample detection, confirming its robustness in practical scenarios. The implementation will be open-sourced on GitHub.

</details>


### [7] [Predict and Resist: Long-Term Accident Anticipation under Sensor Noise](https://arxiv.org/abs/2511.08640)
*Xingcheng Liu,Bin Rao,Yanchen Guan,Chengyue Wang,Haicheng Liao,Jiaxun Zhang,Chengyu Lin,Meixin Zhu,Zhenning Li*

Main category: cs.CV

TL;DR: 本文提出一种结合扩散去噪和时间感知行为者-评论家模型的新方法，显著提升了在真实世界噪声条件下的事故预判能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶领域需要提前预判事故，实现主动安全。现实部署面临两个难题：传感器输入噪声干扰和预警及时性与准确性的平衡。当前方法无法很好应对这些挑战。

Method: 作者设计了一个统一框架，将扩散去噪模块与时间感知的actor-critic架构结合。扩散模块通过多步迭代提升图像与对象特征的抗噪能力，行为者-评论家结构则利用长时序推理与时间加权奖励决定何时预警，兼顾提前性和可靠性。

Result: 在DAD、CCD、A3D三个数据集上，该方法达到当前最优准确率，并在平均事故预警时间上取得显著提升。即使在高斯噪声及脉冲噪声下也有稳健表现。

Conclusion: 所提模型不仅实现了更早、更稳定且符合人类直觉的事故预测，而且具有较强实际应用潜力，适用于复杂交通场景的安全关键部署。

Abstract: Accident anticipation is essential for proactive and safe autonomous driving, where even a brief advance warning can enable critical evasive actions. However, two key challenges hinder real-world deployment: (1) noisy or degraded sensory inputs from weather, motion blur, or hardware limitations, and (2) the need to issue timely yet reliable predictions that balance early alerts with false-alarm suppression. We propose a unified framework that integrates diffusion-based denoising with a time-aware actor-critic model to address these challenges. The diffusion module reconstructs noise-resilient image and object features through iterative refinement, preserving critical motion and interaction cues under sensor degradation. In parallel, the actor-critic architecture leverages long-horizon temporal reasoning and time-weighted rewards to determine the optimal moment to raise an alert, aligning early detection with reliability. Experiments on three benchmark datasets (DAD, CCD, A3D) demonstrate state-of-the-art accuracy and significant gains in mean time-to-accident, while maintaining robust performance under Gaussian and impulse noise. Qualitative analyses further show that our model produces earlier, more stable, and human-aligned predictions in both routine and highly complex traffic scenarios, highlighting its potential for real-world, safety-critical deployment.

</details>


### [8] [RS-Net: Context-Aware Relation Scoring for Dynamic Scene Graph Generation](https://arxiv.org/abs/2511.08651)
*Hae-Won Jo,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: 本文提出了RS-Net，一种用于视频动态图场景图生成（DSGG）的关系评分网络，通过空间与时序上下文共同学习增长关系识别能力，显著提升了现有方法的召回率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有DSGG方法只针对带注释的对象对进行训练，缺乏对无关对象对的有效指导，导致推理时难以识别有意义的关系。

Method: 提出了关系评分网络（RS-Net），包括可学习上下文token的空间上下文编码器和聚合视频级信息的时序编码器，二者结合对对象对的重要性进行评分，并无缝集成到DSGG模型中，提升了关系预测能力。

Result: 在Action Genome数据集上，RS-Net在多个基线下均显著提升了召回率（Recall）和精度（Precision），尤其在均值召回率（mean Recall）上取得了显著提升。尽管参数数量有所增加，但效率依然具有竞争力，并超越了现有SOTA方法。

Conclusion: RS-Net有效缓解了关系分布长尾问题，可无缝集成到现有DSGG框架中，提升了动态图场景图关系预测的表现，具有较强的实际应用价值。

Abstract: Dynamic Scene Graph Generation (DSGG) models how object relations evolve over time in videos. However, existing methods are trained only on annotated object pairs and lack guidance for non-related pairs, making it difficult to identify meaningful relations during inference. In this paper, we propose Relation Scoring Network (RS-Net), a modular framework that scores the contextual importance of object pairs using both spatial interactions and long-range temporal context. RS-Net consists of a spatial context encoder with learnable context tokens and a temporal encoder that aggregates video-level information. The resulting relation scores are integrated into a unified triplet scoring mechanism to enhance relation prediction. RS-Net can be easily integrated into existing DSGG models without architectural changes. Experiments on the Action Genome dataset show that RS-Net consistently improves both Recall and Precision across diverse baselines, with notable gains in mean Recall, highlighting its ability to address the long-tailed distribution of relations. Despite the increased number of parameters, RS-Net maintains competitive efficiency, achieving superior performance over state-of-the-art methods.

</details>


### [9] [Privacy Beyond Pixels: Latent Anonymization for Privacy-Preserving Video Understanding](https://arxiv.org/abs/2511.08666)
*Joseph Fioresi,Ishan Rajendrakumar Dave,Mubarak Shah*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉隐私保护方法，专注于视频基础模型的特征空间，通过自动化插件模块在不影响下游任务性能的情况下，有效去除视频中的个人敏感信息。


<details>
  <summary>Details</summary>
Motivation: 随着视频基础模型越来越普及，人们将其提取的视觉特征用于各种下游任务，但这些特征中常常包含可识别个人隐私的信息，如肤色、性别、服饰等。现有的隐私保护大多在像素层面进行，既需要对整个模型重新训练，也容易导致只适用于特定任务，因此有必要提出高效且通用的隐私保护方法。

Method: 作者设计了可插拔的匿名化适配器模块（AAM），能在冻结视频编码器的前提下，对中间特征中的私人信息进行去除。该模块采用三种训练目标：自监督隐私目标降低静态片段间的互信息、共训练目标保留任务效用、特征一致性损失提升通用性。整个模块轻量级且易于集成。

Result: 在动作识别、时序动作检测和异常检测等下游任务中，该方法在不明显损失模型效用的情况下，将隐私泄露风险减少了35%。此外，分析表明该方法能有效缓解性别偏差，提高视频理解的公平性。

Conclusion: AAM模块为视频基础模型提供了便捷、高效的隐私保护方案，在不牺牲性能的前提下显著减少敏感信息泄露，并在公平性和泛化能力上表现突出。

Abstract: We introduce a novel formulation of visual privacy preservation for video foundation models that operates entirely in the latent space. While spatio-temporal features learned by foundation models have deepened general understanding of video content, sharing or storing these extracted visual features for downstream tasks inadvertently reveals sensitive personal information like skin color, gender, or clothing. Current privacy preservation methods focus on input-pixel-level anonymization, which requires retraining the entire utility video model and results in task-specific anonymization, making them unsuitable for recent video foundational models. To address these challenges, we introduce a lightweight Anonymizing Adapter Module (AAM) that removes private information from video features while retaining general task utility. AAM can be applied in a plug-and-play fashion to frozen video encoders, minimizing the computational burden of finetuning and re-extracting features. Our framework employs three newly designed training objectives: (1) a clip-level self-supervised privacy objective to reduce mutual information between static clips, (2) a co-training objective to retain utility across seen tasks, and (3) a latent consistency loss for generalization on unseen tasks. Our extensive evaluations demonstrate a significant 35% reduction in privacy leakage while maintaining near-baseline utility performance across various downstream tasks: Action Recognition (Kinetics400, UCF101, HMDB51), Temporal Action Detection (THUMOS14), and Anomaly Detection (UCF-Crime). We also provide an analysis on anonymization for sensitive temporal attribute recognition. Additionally, we propose new protocols for assessing gender bias in action recognition models, showing that our method effectively mitigates such biases and promotes more equitable video understanding.

</details>


### [10] [Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction?](https://arxiv.org/abs/2511.08704)
*Xinchen Yan,Chen Liang,Lijun Yu,Adams Wei Yu,Yifeng Lu,Quoc V. Le*

Main category: cs.CV

TL;DR: 本文研究了用于视觉统一模型的自回归像素预测的可扩展性，发现最佳扩展策略高度依赖任务类型，计算资源依然是主要瓶颈，并预测未来五年高分辨率逐像素建模将可实现。


<details>
  <summary>Details</summary>
Motivation: 虽然自回归像素预测框架简单、端到端，但在统一视觉模型中的可扩展性尚未被充分探索。作者希望系统性分析其在不同任务和资源下的表现，为未来视觉模型的设计提供理论指导。

Method: 从低分辨率（32x32）图像起，采用IsoFlops配置，在不同计算资源预算下训练Transformer家族，评估自回归预测、分类准确率及生成图像质量（Fr'echet Distance）。系统分析最佳扩展策略随任务和分辨率变化的依赖性。

Result: 实验发现：在同一分辨率内，分类和生成任务的最佳扩展方式明显不同；高分辨率下，模型规模需增长远快于数据规模；理论分析与实验证明计算资源是主要限制因素。

Conclusion: 不同视觉任务对扩展策略需求差异巨大，随着计算资源持续快速增长，像素级生成模型将在五年内具备实际可行性，为统一视觉建模提供方向。

Abstract: This paper investigates the scaling properties of autoregressive next-pixel prediction, a simple, end-to-end yet under-explored framework for unified vision models. Starting with images at resolutions of 32x32, we train a family of Transformers using IsoFlops profiles across compute budgets up to 7e19 FLOPs and evaluate three distinct target metrics: next-pixel prediction objective, ImageNet classification accuracy, and generation quality measured by Fr'echet Distance. First, optimal scaling strategy is critically task-dependent. At a fixed 32x32 resolution alone, the optimal scaling properties for image classification and image generation diverge, where generation optimal setup requires the data size grow three to five times faster than for the classification optimal setup. Second, as image resolution increases, the optimal scaling strategy indicates that the model size must grow much faster than data size. Surprisingly, by projecting our findings, we discover that the primary bottleneck is compute rather than the amount of training data. As compute continues to grow four to five times annually, we forecast the feasibility of pixel-by-pixel modeling of images within the next five years.

</details>


### [11] [Harnessing Diffusion-Generated Synthetic Images for Fair Image Classification](https://arxiv.org/abs/2511.08711)
*Abhipsa Basu,Aviral Gupta,Abhijnya Bhat,R. Venkatesh Babu*

Main category: cs.CV

TL;DR: 本论文提出采用多种扩散模型微调技术（如LoRA、DreamBooth）生成更均衡的训练数据，有效改善图像分类中的数据偏置问题，在多个基准测试上优于普通Stable Diffusion，并在数据偏差加重时超过现有去偏方法。


<details>
  <summary>Details</summary>
Motivation: 图像分类训练数据常有群体分布不均，导致模型强化刻板印象（如金发与女性高度相关），需有更平衡的数据生成方法解决模型刻板偏差。

Method: 利用Stable Diffusion为基础，分别采用LoRA、DreamBooth等微调技术，使模型能从各群体样本中学习并生成更真实、均衡的训练数据。为应对群体内部样本差异过大，进一步将群体内图片聚类，并按簇训练专属DreamBooth生成器。预训练用合成均衡数据，随后在真实数据上微调。

Result: 在多个评测基准上，所用微调扩散方法整体优于基础Stable Diffusion，并能达到与SOTA去偏工具（如Group-DRO）接近甚至更优的效果，尤其在数据偏差较大时优势明显。

Conclusion: 多种扩散模型微调方法结合聚类可有效生成去偏、均衡的训练数据，是缓解视觉分类模型偏差的有力工具，实际效果超越传统SOTA方法，潜力显著。

Abstract: Image classification systems often inherit biases from uneven group representation in training data. For example, in face datasets for hair color classification, blond hair may be disproportionately associated with females, reinforcing stereotypes. A recent approach leverages the Stable Diffusion model to generate balanced training data, but these models often struggle to preserve the original data distribution. In this work, we explore multiple diffusion-finetuning techniques, e.g., LoRA and DreamBooth, to generate images that more accurately represent each training group by learning directly from their samples. Additionally, in order to prevent a single DreamBooth model from being overwhelmed by excessive intra-group variations, we explore a technique of clustering images within each group and train a DreamBooth model per cluster. These models are then used to generate group-balanced data for pretraining, followed by fine-tuning on real data. Experiments on multiple benchmarks demonstrate that the studied finetuning approaches outperform vanilla Stable Diffusion on average and achieve results comparable to SOTA debiasing techniques like Group-DRO, while surpassing them as the dataset bias severity increases.

</details>


### [12] [WiCV at CVPR 2025: The Women in Computer Vision Workshop](https://arxiv.org/abs/2511.08748)
*Estefania Talavera,Deblina Bhattacharjee,Himangi Mittal,Mengwei Ren,Karen Sanchez,Carla Muntean,JungEun Kim,Mona Jalal*

Main category: cs.CV

TL;DR: 本文总结了2025年计算机视觉女性研讨会（WiCV@CVPR 2025）的内容、参与情况与影响，对比历年发展，为未来多样性和包容性活动提供参考。


<details>
  <summary>Details</summary>
Motivation: 旨在提高女性和少数群体在计算机视觉领域的能见度、包容性和职业发展，总结和记录会议的演变和影响，为相似多样性倡议提供经验。

Method: 全面回顾研讨会的议程、投稿与录用情况、导师项目配对、参与人数以及赞助与奖学金支持，并与历届数据进行对比分析。

Result: 2025年会议共收到32篇长文投稿并录用14篇（其中5篇口头报告），还有36份扩展摘要。举办了导师项目（80名学员与37位导师），吸引逾100名现场参与者，获得10家赞助，总计发放约4.4万美元奖助金。

Conclusion: WiCV@CVPR 2025继续支持女性和多元背景研究者，促进计算机视觉领域多样性与包容性发展，总结为未来同类活动积累了宝贵经验。

Abstract: The Women in Computer Vision Workshop (WiCV@CVPR 2025) was held in conjunction with the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2025) in Nashville, Tennessee, United States. This report presents an overview of the workshop program, participation statistics, mentorship outcomes, and historical trends from previous WiCV editions. The goal is to document the impact and evolution of WiCV as a reference for future editions and for other initiatives aimed at advancing diversity, equity, and inclusion within the AI and computer vision communities. WiCV@CVPR 2025 marked the 16th edition of this long-standing event dedicated to increasing the visibility, inclusion, and professional growth of women and underrepresented minorities in the computer vision community. This year's workshop featured 14 accepted papers in the CVPR Workshop Proceedings out of 32 full-paper submissions. Five of these were selected for oral presentations, while all 14 were also presented as posters, along with 36 extended abstract posters accepted from 62 short-paper submissions, which are not included in the proceedings. The mentoring program matched 80 mentees with 37 mentors from both academia and industry. The 2025 edition attracted over 100 onsite participants, fostering rich technical and networking interactions across all career stages. Supported by 10 sponsors and approximately $44,000 USD in travel grants and diversity awards, WiCV continued its mission to empower emerging researchers and amplify diverse voices in computer vision.

</details>


### [13] [Adaptive graph Kolmogorov-Arnold network for 3D human pose estimation](https://arxiv.org/abs/2511.08809)
*Abu Taib Mohammed Shahjahan,A. Ben Hamza*

Main category: cs.CV

TL;DR: 提出了一种新的基于自适应图Kolmogorov-Arnold网络（PoseKAN）的2D到3D人体姿态估计算法，克服了传统GCN局部感受野和频谱偏置的不足，实现了更优精准的姿态重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于GCN的方法虽然利用了人体骨架的图结构，在3D姿态估计任务中表现良好，但受限于局部感受野，难以建模远距离依赖，对遮挡和深度模糊场景表现不佳，并存在频谱偏置，无法精细刻画高频细节。

Method: 提出PoseKAN框架，将可学习激活函数的KAN方法拓展到图学习，允许边特征自适应变化。采用多跳特征聚合，结合残差结构和全局归一化增强空间感知和特征表达能力，实现了端到端的2D到3D姿态估计。

Result: 在多个基准数据集上，PoseKAN与最新方法相比显示出竞争力甚至更优的性能，能更好处理复杂姿态变化和高维特征。

Conclusion: PoseKAN有效克服了GCN的感受野和频谱问题，提升了三维姿态重建的准确性及适应性，为复杂场景下的人体姿态估计提供了新思路。

Abstract: Graph convolutional network (GCN)-based methods have shown strong performance in 3D human pose estimation by leveraging the natural graph structure of the human skeleton. However, their local receptive field limits their ability to capture long-range dependencies essential for handling occlusions and depth ambiguities. They also exhibit spectral bias, which prioritizes low-frequency components while struggling to model high-frequency details. In this paper, we introduce PoseKAN, an adaptive graph Kolmogorov-Arnold Network (KAN), framework that extends KANs to graph-based learning for 2D-to-3D pose lifting from a single image. Unlike GCNs that use fixed activation functions, KANs employ learnable functions on graph edges, allowing data-driven, adaptive feature transformations. This enhances the model's adaptability and expressiveness, making it more expressive in learning complex pose variations. Our model employs multi-hop feature aggregation, ensuring the body joints can leverage information from both local and distant neighbors, leading to improved spatial awareness. It also incorporates residual PoseKAN blocks for deeper feature refinement, and a global response normalization for improved feature selectivity and contrast. Extensive experiments on benchmark datasets demonstrate the competitive performance of our model against state-of-the-art methods.

</details>


### [14] [SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph](https://arxiv.org/abs/2511.08810)
*Jingjie He,Weijie Liang,Zihan Shan,Matthew Caesar*

Main category: cs.CV

TL;DR: 提出了一种结合手工与学习特征的多模态防御框架SIFT-Graph，通过聚合结构化特征显著提升深度视觉模型对对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度视觉模型对像素级扰动极其敏感，现有防御方法往往局限在脆弱的像素空间，缺乏引入本质鲁棒视觉特征的机制，因此亟需更坚固的防御方法。

Method: 将SIFT关键点与图注意力网络（Graph Attention Network, GAT）结合，用于提取具备尺度和旋转不变性的局部特征，将其嵌入与经典视觉模型（如ViT和CNN）融合，形成结构感知且对扰动有防御力的模型。

Result: 实验初步结果表明，该方法在面对基于梯度的白盒对抗攻击时，能显著提升模型鲁棒性，仅带来极小的干净样本准确率损失。

Conclusion: SIFT-Graph框架能够有效增强现代深度视觉模型对对抗攻击的防御能力，为结构化特征与像素特征的融合提供了新颖思路。

Abstract: Adversarial attacks expose a fundamental vulnerability in modern deep vision models by exploiting their dependence on dense, pixel-level representations that are highly sensitive to imperceptible perturbations. Traditional defense strategies typically operate within this fragile pixel domain, lacking mechanisms to incorporate inherently robust visual features. In this work, we introduce SIFT-Graph, a multimodal defense framework that enhances the robustness of traditional vision models by aggregating structurally meaningful features extracted from raw images using both handcrafted and learned modalities. Specifically, we integrate Scale-Invariant Feature Transform keypoints with a Graph Attention Network to capture scale and rotation invariant local structures that are resilient to perturbations. These robust feature embeddings are then fused with traditional vision model, such as Vision Transformer and Convolutional Neural Network, to form a unified, structure-aware and perturbation defensive model. Preliminary results demonstrate that our method effectively improves the visual model robustness against gradient-based white box adversarial attacks, while incurring only a marginal drop in clean accuracy.

</details>


### [15] [DT-NVS: Diffusion Transformers for Novel View Synthesis](https://arxiv.org/abs/2511.08823)
*Wonbong Jang,Jonathan Tremblay,Lourdes Agapito*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D扩散模型的新方法DT-NVS，实现了从单张自然场景图片生成多视角新视图，适用于现实生活中各类复杂场景。该方法在Unaligned的、真实多类别大规模数据集上训练，并在准确性和多样性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的生成方法主要适用于小范围相机移动或非自然、以物体为中心的场景，因此难以应用于现实中复杂、无对齐、日常的自然场景，需要一种通用的、更强泛化能力的新视角合成方法。

Method: 提出DT-NVS，一种融合Transformer和自注意力机制的3D感知扩散模型，利用仅基于图像的损失，在大规模未对齐的、包含多类别日常场景的视频数据集上训练。创新点包括改进Transformer结构、制定新的三维图像转换方法、实现新的对齐方式和训练范式，即互换条件和输入图像的参考帧角色。

Result: 在通用新视角合成任务上，DT-NVS方法在单张输入图像下生成的多视角结果在准确性和多样性上均优于当前已知3D扩散模型以及确定性方法。

Conclusion: 该工作扩展了新视角合成的应用范围，使其能在单图像输入下应对不对齐的、复杂现实场景，并推动了3D感知扩散模型和Transformer结构在实际场景生成任务中的进一步应用。

Abstract: Generating novel views of a natural scene, e.g., every-day scenes both indoors and outdoors, from a single view is an under-explored problem, even though it is an organic extension to the object-centric novel view synthesis. Existing diffusion-based approaches focus rather on small camera movements in real scenes or only consider unnatural object-centric scenes, limiting their potential applications in real-world settings. In this paper we move away from these constrained regimes and propose a 3D diffusion model trained with image-only losses on a large-scale dataset of real-world, multi-category, unaligned, and casually acquired videos of everyday scenes. We propose DT-NVS, a 3D-aware diffusion model for generalized novel view synthesis that exploits a transformer-based architecture backbone. We make significant contributions to transformer and self-attention architectures to translate images to 3d representations, and novel camera conditioning strategies to allow training on real-world unaligned datasets. In addition, we introduce a novel training paradigm swapping the role of reference frame between the conditioning image and the sampled noisy input. We evaluate our approach on the 3D task of generalized novel view synthesis from a single input image and show improvements over state-of-the-art 3D aware diffusion models and deterministic approaches, while generating diverse outputs.

</details>


### [16] [Enhancing Rotation-Invariant 3D Learning with Global Pose Awareness and Attention Mechanisms](https://arxiv.org/abs/2511.08833)
*Jiaxun Guo,Manar Amayri,Nizar Bouguila,Xin Liu,Wentao Fan*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法来提升3D点云在任意旋转下的区分能力，有效解决了现有RI学习方法无法区分空间对称结构的问题。


<details>
  <summary>Details</summary>
Motivation: 现有旋转不变特征（RI特征）在3D点云中往往由于丧失全局位姿信息，导致难以区分几何结构相同但空间位置不同的部件（如飞机左右机翼）。作者认为根本问题是现有方法感受野受限，导致对称部位的特征难以区分。

Method: 论文提出Shadow-informed Pose Feature（SiPF），通过引入基于全局参考点（shadow， 由共享旋转学习获得）的新特征，在保持旋转不变性的同时引入全局位姿信息。同时，提出了Rotation-invariant Attention Convolution（RIAttnConv），将SiPF集成到特征聚合中，提高模型空间判别能力。此外，利用Bingham分布设计了任务自适应shadow定位模块，能自适应学习最优全局旋转以构造一致的shadow。

Result: 在3D分类与部件分割基准任务上，该方法显著优于现有RI方法，尤其是在需要空间精细判别的任务下有突出表现。

Conclusion: 引入shadow辅助的全局位姿信息是一种提升旋转不变3D点云理解能力的有效途径，有助于解决空间对称结构区分难题，对下游空间判别任务具有实际应用价值。

Abstract: Recent advances in rotation-invariant (RI) learning for 3D point clouds typically replace raw coordinates with handcrafted RI features to ensure robustness under arbitrary rotations. However, these approaches often suffer from the loss of global pose information, making them incapable of distinguishing geometrically similar but spatially distinct structures. We identify that this limitation stems from the restricted receptive field in existing RI methods, leading to Wing-tip feature collapse, a failure to differentiate symmetric components (e.g., left and right airplane wings) due to indistinguishable local geometries. To overcome this challenge, we introduce the Shadow-informed Pose Feature (SiPF), which augments local RI descriptors with a globally consistent reference point (referred to as the 'shadow') derived from a learned shared rotation. This mechanism enables the model to preserve global pose awareness while maintaining rotation invariance. We further propose Rotation-invariant Attention Convolution (RIAttnConv), an attention-based operator that integrates SiPFs into the feature aggregation process, thereby enhancing the model's capacity to distinguish structurally similar components. Additionally, we design a task-adaptive shadow locating module based on the Bingham distribution over unit quaternions, which dynamically learns the optimal global rotation for constructing consistent shadows. Extensive experiments on 3D classification and part segmentation benchmarks demonstrate that our approach substantially outperforms existing RI methods, particularly in tasks requiring fine-grained spatial discrimination under arbitrary rotations.

</details>


### [17] [SasMamba: A Lightweight Structure-Aware Stride State Space Model for 3D Human Pose Estimation](https://arxiv.org/abs/2511.08872)
*Hu Cui,Wenqiang Hua,Renjing Huang,Shurui Jia,Tessai Hayama*

Main category: cs.CV

TL;DR: 本文提出了一种结构感知的骨架步幅状态空间模型（SAS-SSM），提升3D人体姿态估计的建模能力，在参数更少的情况下取得有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于状态空间模型（SSM）的人体三维姿态估计常通过人工设计的scan操作，将2D姿态序列拉平成时间序列，这种操作破坏了人体姿态的空间结构，导致空间与时间特征混杂，很难捕获复杂的姿态依赖关系。

Method: 本文提出SAS-SSM，先通过结构感知的时空卷积动态捕捉关节点间的局部交互，再用基于步幅的scan策略，构建多尺度的全局结构表示，兼顾局部和全局信息，且保持线性复杂度。

Result: 所提出的SasMamba模型在参数量大幅减少的情况下，三维姿态估计性能依然具有竞争力。

Conclusion: SAS-SSM改进了现有SSM对3D姿态建模的能力，为高效且高精度的姿态估计提供了新思路。

Abstract: Recently, the Mamba architecture based on State Space Models (SSMs) has gained attention in 3D human pose estimation due to its linear complexity and strong global modeling capability. However, existing SSM-based methods typically apply manually designed scan operations to flatten detected 2D pose sequences into purely temporal sequences, either locally or globally. This approach disrupts the inherent spatial structure of human poses and entangles spatial and temporal features, making it difficult to capture complex pose dependencies. To address these limitations, we propose the Skeleton Structure-Aware Stride SSM (SAS-SSM), which first employs a structure-aware spatiotemporal convolution to dynamically capture essential local interactions between joints, and then applies a stride-based scan strategy to construct multi-scale global structural representations. This enables flexible modeling of both local and global pose information while maintaining linear computational complexity. Built upon SAS-SSM, our model SasMamba achieves competitive 3D pose estimation performance with significantly fewer parameters compared to existing hybrid models. The source code is available at https://hucui2022.github.io/sasmamba_proj/.

</details>


### [18] [Improve Contrastive Clustering Performance by Multiple Fusing-Augmenting ViT Blocks](https://arxiv.org/abs/2511.08883)
*Cheng Wang,Shuisheng Zhou,Fengjiao Peng,Jin Sheng,Feng Ye,Yinli Dong*

Main category: cs.CV

TL;DR: 本文提出了一种基于ViT（Vision Transformer）的多重融合增强模块（MFAVBs），通过在对比学习框架下显式融合正样本对的特征，提升了图像聚类性能。实验证明该方法优于现有主流聚类方法。


<details>
  <summary>Details</summary>
Motivation: 传统对比学习网络通常通过参数共享或动量更新隐式地让两个编码器交互，这种方式没有充分利用正样本对间的互补性和相似性，限制了聚类特征的提取效率。因此，设计一种能显式融合正样本对特征的新方法，以提升聚类效果。

Method: 作者提出MFAVBs模块：1）将两个预处理后的正样本对分别输入权重共享的ViT编码器并获得特征；2）将输出特征融合后输入更大的ViT，随后将新特征拆分为一对新增强样本，继续多重融合增强操作；3）最终，将特征投影到实例和聚类空间，联合计算损失函数，采用CLIP预训练特征做数据增强。

Result: 在七个公开数据集上实验，结果表明MFAVBs作为对比聚类的骨干网络，聚类性能优于最新相关方法。

Conclusion: 通过多重特征融合和增强，MFAVBs能够更有效地利用正样本对的互补性，提升图像聚类的表现，为对比学习聚类提供了一种新思路。

Abstract: In the field of image clustering, the widely used contrastive learning networks improve clustering performance by maximizing the similarity between positive pairs and the dissimilarity of negative pairs of the inputs. Extant contrastive learning networks, whose two encoders often implicitly interact with each other by parameter sharing or momentum updating, may not fully exploit the complementarity and similarity of the positive pairs to extract clustering features from input data. To explicitly fuse the learned features of positive pairs, we design a novel multiple fusing-augmenting ViT blocks (MFAVBs) based on the excellent feature learning ability of Vision Transformers (ViT). Firstly, two preprocessed augmentions as positive pairs are separately fed into two shared-weight ViTs, then their output features are fused to input into a larger ViT. Secondly, the learned features are split into a pair of new augmented positive samples and passed to the next FAVBs, enabling multiple fusion and augmention through MFAVBs operations. Finally, the learned features are projected into both instance-level and clustering-level spaces to calculate the cross-entropy loss, followed by parameter updates by backpropagation to finalize the training process. To further enhance ability of the model to distinguish between similar images, our input data for the network we propose is preprocessed augmentions with features extracted from the CLIP pretrained model. Our experiments on seven public datasets demonstrate that MFAVBs serving as the backbone for contrastive clustering outperforms the state-of-the-art techniques in terms of clustering performance.

</details>


### [19] [Classifying Histopathologic Glioblastoma Sub-regions with EfficientNet](https://arxiv.org/abs/2511.08896)
*Sanyukta Adap,Ujjwal Baid,Spyridon Bakas*

Main category: cs.CV

TL;DR: 本文提出了一种基于EfficientNet的深度学习方法，实现了对胶质母细胞瘤（GBM）切片内六类组织亚区的自动分类，在公开挑战数据集上获得较高训练得分，但泛化能力仍待提升。


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤预后差，当前主要诊断手段为组织病理评估，自动、准确区分其各组织亚区有助于大规模理解该疾病形态学分布，推动精准诊断与研究。

Method: 设计了一个四步深度学习流程，利用EfficientNet（B0至B4）网络对H&E染色GBM切片六类区域进行分类，在BraTS-Path 2024数据集上做定量评估，并通过5折交叉验证。

Result: EfficientNet-B1和B4在5折交叉验证中F1分数达到0.98，但在独立验证集和隐藏测试集上的F1分别为0.546和0.517，显示一定的过拟合问题和泛化能力挑战。

Conclusion: 深度学习方法可有效对GBM组织亚区进行自动分类，但模型泛化性不足，未来需进一步提升模型稳定性和临床适用性。

Abstract: Glioblastoma (GBM) is the most common aggressive, fast-growing brain tumor, with a grim prognosis. Despite clinical diagnostic advancements, there have not been any substantial improvements to patient prognosis. Histopathological assessment of excised tumors is the first line of clinical diagnostic routine. We hypothesize that automated, robust, and accurate identification of distinct histological sub-regions within GBM could contribute to morphologically understanding this disease at scale. In this study, we designed a four-step deep learning approach to classify six (6) histopathological regions and quantitatively evaluated it on the BraTS-Path 2024 challenge dataset, which includes digitized Hematoxylin \& Eosin (H\&E) stained GBM tissue sections annotated for six distinct regions. We used the challenge's publicly available training dataset to develop and evaluate the effectiveness of several variants of EfficientNet architectures (i.e., B0, B1, B2, B3, B4). EfficientNet-B1 and EfficientNet-B4 achieved the best performance, achieving an F1 score of 0.98 in a 5-fold cross-validation configuration using the BraTS-Path training set. The quantitative performance evaluation of our proposed approach with EfficientNet-B1 on the BraTS-Path hold-out validation data and the final hidden testing data yielded F1 scores of 0.546 and 0.517, respectively, for the associated 6-class classification task. The difference in the performance on training, validation, and testing data highlights the challenge of developing models that generalize well to new data, which is crucial for clinical applications. The source code of the proposed approach can be found at the GitHub repository of Indiana University Division of Computational Pathology: https://github.com/IUCompPath/brats-path-2024-enet.

</details>


### [20] [Improving VisNet for Object Recognition](https://arxiv.org/abs/2511.08897)
*Mehdi Fatan Serj,C. Alejandro Parraga,Xavier Otazu*

Main category: cs.CV

TL;DR: 本论文通过改进VisNet神经网络模型，提升了在不同数据集上的对象识别与对称性分类能力，取得了较优的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 人类视觉系统能够高效地进行对象识别，但在人工系统中实现这一点仍很有挑战。因此，研究旨在开发更具生物启发性的神经网络架构，以提升人工视觉系统的表现和可解释性。

Method: 论文基于VisNet，提出了引入径向基函数神经元、基于Mahalanobis距离的学习以及类视网膜的预处理等多种增强方案，通过Hebbian学习和时序连续性原理，将临近时刻视图关联以构建对变换不变的表征。

Result: 多项实验结果表明，增强后的VisNet在MNIST、CIFAR10及自定义对称对象数据集上的识别准确率，相较基线模型有显著提升，尤其在对称性分类任务中表现优异。

Conclusion: VisNet及其增强模型不仅验证了生物学启发方法在对象识别中的有效性和适用性，还为神经科学和人工智能领域的视觉识别任务提供了强大且可解释的模型架构。

Abstract: Object recognition plays a fundamental role in how biological organisms perceive and interact with their environment. While the human visual system performs this task with remarkable efficiency, reproducing similar capabilities in artificial systems remains challenging. This study investigates VisNet, a biologically inspired neural network model, and several enhanced variants incorporating radial basis function neurons, Mahalanobis distance based learning, and retinal like preprocessing for both general object recognition and symmetry classification. By leveraging principles of Hebbian learning and temporal continuity associating temporally adjacent views to build invariant representations. VisNet and its extensions capture robust and transformation invariant features. Experimental results across multiple datasets, including MNIST, CIFAR10, and custom symmetric object sets, show that these enhanced VisNet variants substantially improve recognition accuracy compared with the baseline model. These findings underscore the adaptability and biological relevance of VisNet inspired architectures, offering a powerful and interpretable framework for visual recognition in both neuroscience and artificial intelligence.
  Keywords: VisNet, Object Recognition, Symmetry Detection, Hebbian Learning, RBF Neurons, Mahalanobis Distance, Biologically Inspired Models, Invariant Representations

</details>


### [21] [Asymmetric Cross-Modal Knowledge Distillation: Bridging Modalities with Weak Semantic Consistency](https://arxiv.org/abs/2511.08901)
*Riling Wei,Kelu Yao,Chuanguang Yang,Jin Wang,Zhuoyan Gao,Chao Li*

Main category: cs.CV

TL;DR: 提出了一种新的非对称跨模态知识蒸馏（ACKD）框架SemBridge，能有效应对模态间弱语义一致性问题，并在遥感场景分类等任务上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的对称跨模态知识蒸馏（SCKD）依赖强语义相关的模态配对，但在实际应用中往往难以获得大量配对数据，弱语义一致性的场景更为常见，因此需要一种更为通用有效的跨模态知识迁移方法。

Method: 提出ACKD概念，针对弱语义一致性下的知识传递挑战，基于最优传输理论进行了理论分析。提出SemBridge框架，包含自监督学习驱动的学生友好匹配模块和基于拉格朗日优化的语义感知对齐模块，通过动态选择教师样本和最优对齐路径提升跨模态知识传递效率。构建了多光谱与RGB遥感图片的异构数据集作为基准，验证方法有效性。

Result: SemBridge在遥感场景分类任务上，结合6种模型结构、多个数据集、7个主流方法进行对比，均取得最优或领先的结果，展示了其优越的性能和适用性。

Conclusion: SemBridge能突破对称知识蒸馏的局限，有效实现弱语义一致性下的跨模态知识迁移，显著提升遥感等实际应用中的多模态学习能力，在通用跨模态蒸馏领域具有重要意义。

Abstract: Cross-modal Knowledge Distillation has demonstrated promising performance on paired modalities with strong semantic connections, referred to as Symmetric Cross-modal Knowledge Distillation (SCKD). However, implementing SCKD becomes exceedingly constrained in real-world scenarios due to the limited availability of paired modalities. To this end, we investigate a general and effective knowledge learning concept under weak semantic consistency, dubbed Asymmetric Cross-modal Knowledge Distillation (ACKD), aiming to bridge modalities with limited semantic overlap. Nevertheless, the shift from strong to weak semantic consistency improves flexibility but exacerbates challenges in knowledge transmission costs, which we rigorously verified based on optimal transport theory. To mitigate the issue, we further propose a framework, namely SemBridge, integrating a Student-Friendly Matching module and a Semantic-aware Knowledge Alignment module. The former leverages self-supervised learning to acquire semantic-based knowledge and provide personalized instruction for each student sample by dynamically selecting the relevant teacher samples. The latter seeks the optimal transport path by employing Lagrangian optimization. To facilitate the research, we curate a benchmark dataset derived from two modalities, namely Multi-Spectral (MS) and asymmetric RGB images, tailored for remote sensing scene classification. Comprehensive experiments exhibit that our framework achieves state-of-the-art performance compared with 7 existing approaches on 6 different model architectures across various datasets.

</details>


### [22] [LLM-Guided Probabilistic Fusion for Label-Efficient Document Layout Analysis](https://arxiv.org/abs/2511.08903)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

TL;DR: 本文提出了一种将视觉检测与文本大模型（LLM）结构先验相融合的半监督文档版面解析框架，通过逆方差加权概率融合生成更精确的伪标签，有效提升了少标注场景下的检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有文档版面理解在半监督学习下依然严重依赖标注数据，难以高效利用未标注文档及结构信息。作者希望借助文本预训练大模型蕴含的结构知识与视觉检测结果结合，以提升伪标签质量，增强模型泛化能力。

Method: 方法通过OCR+LLM推理文档的层次结构区域，并用逆方差融合原则与半监督教师检测器输出结合，生成高质量伪标签。此外，设计了实例自适应门控机制、比较不同大模型后端影响，并分析了开源LLM的隐私部署与经济性。

Result: 少量标注（5%）情况下，轻量模型在PubLayNet上达到88.2 AP，结构融合的LayoutLMv3达到89.7 AP，均优于仅视觉半监督或需大规模多模态预训练的方法。自适应融合比固定加权提升0.9 AP，LLM语义消歧显著提升了部分难点例子检测表现。

Conclusion: 文档结构先验与视觉检测互补，大模型结构知识能在多种模型规模和部署场景下有效提升半监督版面解析，对实际成本影响有限。

Abstract: Document layout understanding remains data-intensive despite advances in semi-supervised learning. We present a framework that enhances semi-supervised detection by fusing visual predictions with structural priors from text-pretrained LLMs via principled probabilistic weighting. Given unlabeled documents, an OCR-LLM pipeline infers hierarchical regions which are combined with teacher detector outputs through inverse-variance fusion to generate refined pseudo-labels.Our method demonstrates consistent gains across model scales. With a lightweight SwiftFormer backbone (26M params), we achieve 88.2$\pm$0.3 AP using only 5\% labels on PubLayNet. When applied to document-pretrained LayoutLMv3 (133M params), our fusion framework reaches 89.7$\pm$0.4 AP, surpassing both LayoutLMv3 with standard semi-supervised learning (89.1$\pm$0.4 AP, p=0.02) and matching UDOP~\cite{udop} (89.8 AP) which requires 100M+ pages of multimodal pretraining. This demonstrates that LLM structural priors are complementary to both lightweight and pretrained architectures. Key findings include: (1) learned instance-adaptive gating improves over fixed weights by +0.9 AP with data-dependent PAC bounds correctly predicting convergence; (2) open-source LLMs enable privacy-preserving deployment with minimal loss (Llama-3-70B: 87.1 AP lightweight, 89.4 AP with LayoutLMv3); (3) LLMs provide targeted semantic disambiguation (18.7\% of cases, +3.8 AP gain) beyond simple text heuristics.Total system cost includes \$12 for GPT-4o-mini API or 17 GPU-hours for local Llama-3-70B per 50K pages, amortized across training runs.

</details>


### [23] [Consistency Change Detection Framework for Unsupervised Remote Sensing Change Detection](https://arxiv.org/abs/2511.08904)
*Yating Liu,Yan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的一致性变化检测框架（CCDF），用于无监督遥感变化检测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往无监督遥感变化检测方法由于生成器过拟合导致性能不足。

Method: 利用循环一致性模块减少生成器过拟合，并通过语义一致性模块提高细节重建水平。

Result: 大量实验表明，所提方法在检测性能上优于最新的同类方法。

Conclusion: 引入一致性机制可以有效缓解无监督变化检测中的重建困难和过拟合问题，提升检测效果。

Abstract: Unsupervised remote sensing change detection aims to monitor and analyze changes from multi-temporal remote sensing images in the same geometric region at different times, without the need for labeled training data. Previous unsupervised methods attempt to achieve style transfer across multi-temporal remote sensing images through reconstruction by a generator network, and then capture the unreconstructable areas as the changed regions. However, it often leads to poor performance due to generator overfitting. In this paper, we propose a novel Consistency Change Detection Framework (CCDF) to address this challenge. Specifically, we introduce a Cycle Consistency (CC) module to reduce the overfitting issues in the generator-based reconstruction. Additionally, we propose a Semantic Consistency (SC) module to enable detail reconstruction. Extensive experiments demonstrate that our method outperforms other state-of-the-art approaches.

</details>


### [24] [HitoMi-Cam: A Shape-Agnostic Person Detection Method Using the Spectral Characteristics of Clothing](https://arxiv.org/abs/2511.08908)
*Shuji Ono*

Main category: cs.CV

TL;DR: 本文提出HitoMi-Cam，一种基于光谱反射特性的轻量级、与形状无关的人体检测方法，在硬件设备上实现并测试，实验证明在部分CNN方法失效的情况下表现优异，实时性强，误报少。


<details>
  <summary>Details</summary>
Motivation: 传统基于CNN的目标检测对姿态形状敏感，遇到训练数据未覆盖的体态时效果下降，特别是在灾害救援等场景对精度和实时性要求高。作者旨在弥补CNN检测的局限性，提出更鲁棒和实用的检测方法。

Method: 文章提出基于服装光谱反射特性的HitoMi-Cam检测方法，并在无GPU的边缘设备上实现，通过若干模拟以及实际场景对比主流CNN检测模型进行性能评估。

Result: HitoMi-Cam在无GPU设备上实现23.2帧/秒的实时检测，且在模拟救援场景下以93.5%的AP大幅超越最佳CNN模型（53.8%），误报率低。

Conclusion: HitoMi-Cam不能替代CNN，但在目标姿态多变、CNN表现不佳的情况下可以作为有效补充，证实了光谱检测在真实边缘设备上的实际运用潜力，适用于灾难救援等不可预测环境。

Abstract: While convolutional neural network (CNN)-based object detection is widely used, it exhibits a shape dependency that degrades performance for postures not included in the training data. Building upon our previous simulation study published in this journal, this study implements and evaluates the spectral-based approach on physical hardware to address this limitation. Specifically, this paper introduces HitoMi-Cam, a lightweight and shape-agnostic person detection method that uses the spectral reflectance properties of clothing. The author implemented the system on a resource-constrained edge device without a GPU to assess its practical viability. The results indicate that a processing speed of 23.2 frames per second (fps) (253x190 pixels) is achievable, suggesting that the method can be used for real-time applications. In a simulated search and rescue scenario where the performance of CNNs declines, HitoMi-Cam achieved an average precision (AP) of 93.5%, surpassing that of the compared CNN models (best AP of 53.8%). Throughout all evaluation scenarios, the occurrence of false positives remained minimal. This study positions the HitoMi-Cam method not as a replacement for CNN-based detectors but as a complementary tool under specific conditions. The results indicate that spectral-based person detection can be a viable option for real-time operation on edge devices in real-world environments where shapes are unpredictable, such as disaster rescue.

</details>


### [25] [Negative Entity Suppression for Zero-Shot Captioning with Synthetic Images](https://arxiv.org/abs/2511.08909)
*Zimao Lu,Hui Xu,Bing Liu,Ke Wang*

Main category: cs.CV

TL;DR: 本文提出了一种称为NES（Negative Entity Suppression，负实体抑制）的方法，用于改善纯文本训练下的零样本图像描述的泛化能力和减少幻觉内容。


<details>
  <summary>Details</summary>
Motivation: 零样本图像描述常面临跨领域泛化差及“幻觉”现象（生成与输入无关的内容），而现有检索式方法反而有时会加剧该问题，因此亟需新的解决方案提升准确性和泛化能力。

Method: NES主要有三步：1）用合成图像保障训练、推断阶段检索一致性；2）过滤检索内容中的“负实体”以提升描述准确性；3）利用注意力机制对“负实体”进行抑制，进一步减少幻觉。

Result: 在多项基准测试上，NES在保持训练域内竞争性能的同时，显著提升了跨域迁移能力和降低幻觉发生率，在零样本图像描述任务中达到了最新最好结果。

Conclusion: NES为利用外部知识进行图像描述生成提供了一种有效减少幻觉内容且具备更好泛化能力的新范式，推动了零样本图像描述领域的发展。

Abstract: Text-only training provides an attractive approach to address data scarcity challenges in zero-shot image captioning (ZIC), avoiding the expense of collecting paired image-text annotations. However, although these approaches perform well within training domains, they suffer from poor cross-domain generalization, often producing hallucinated content when encountering novel visual environments. Retrieval-based methods attempt to mitigate this limitation by leveraging external knowledge, but they can paradoxically exacerbate hallucination when retrieved captions contain entities irrelevant to the inputs. We introduce the concept of negative entities--objects that appear in generated caption but are absent from the input--and propose Negative Entity Suppression (NES) to tackle this challenge. NES seamlessly integrates three stages: (1) it employs synthetic images to ensure consistent image-to-text retrieval across both training and inference; (2) it filters negative entities from retrieved content to enhance accuracy; and (3) it applies attention-level suppression using identified negative entities to further minimize the impact of hallucination-prone features. Evaluation across multiple benchmarks demonstrates that NES maintains competitive in-domain performance while improving cross-domain transfer and reducing hallucination rates, achieving new state-of-the-art results in ZIC. Our code is available at https://github.com/nidongpinyinme/NESCap.

</details>


### [26] [SPEED-Q: Staged Processing with Enhanced Distillation towards Efficient Low-bit On-device VLM Quantization](https://arxiv.org/abs/2511.08914)
*Tianyu Guo,Shanwei Zhao,Shiai Zhu,Chenguang Ma*

Main category: cs.CV

TL;DR: 本文提出了SPEED-Q，一种专为小规模（10亿参数级别）视觉-语言模型（VLM）低比特量化而设计的框架，通过分阶段处理和增强蒸馏技术显著提升了模型在边缘设备部署时的准确性、稳定性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有的VLM量化技术很少关注参数规模在1B至2B之间，且对超低比特（如2-bit）场景下模型各组分敏感性差异及训练稳定性问题缺乏系统性解决。这些小型模型更适合边缘设备部署，因此亟需针对它们的低比特高效量化方法。

Method: 提出SPEED-Q框架，包含两大创新：一是分阶段敏感性自适应机制，针对视觉和语言部分的量化敏感性不同进行协同优化；二是蒸馏增强量化策略，提升低比特量化下的训练稳定性、减少对大规模数据的依赖。

Result: SPEED-Q能成功将10亿参数规模VLM整体量化至2-bit和4-bit，在多个基准上2-bit设置下准确率最高提升6倍，并始终优于现有低比特量化方法和设备端VLM。

Conclusion: SPEED-Q首次系统化地实现了对小型全参数VLM的超低比特高效量化，为边缘设备智能应用铺平了道路，具有显著的实用价值。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices (e.g., smartphones and robots) is crucial for enabling low-latency and privacy-preserving intelligent applications. Given the resource constraints of these devices, quantization offers a promising solution by improving memory efficiency and reducing bandwidth requirements, thereby facilitating the deployment of VLMs. However, existing research has rarely explored aggressive quantization on VLMs, particularly for the models ranging from 1B to 2B parameters, which are more suitable for resource-constrained edge devices. In this paper, we propose SPEED-Q, a novel Staged Processing with Enhanced Distillation framework for VLM low-bit weight-only quantization that systematically addresses the following two critical obstacles: (1) significant discrepancies in quantization sensitivity between vision (ViT) and language (LLM) components in VLMs; (2) training instability arising from the reduced numerical precision inherent in low-bit quantization. In SPEED-Q, a staged sensitivity adaptive mechanism is introduced to effectively harmonize performance across different modalities. We further propose a distillation-enhanced quantization strategy to stabilize the training process and reduce data dependence. Together, SPEED-Q enables accurate, stable, and data-efficient quantization of complex VLMs. SPEED-Q is the first framework tailored for quantizing entire small-scale billion-parameter VLMs to low bits. Extensive experiments across multiple benchmarks demonstrate that SPEED-Q achieves up to 6x higher accuracy than existing quantization methods under 2-bit settings and consistently outperforms prior on-device VLMs under both 2-bit and 4-bit settings. Our code and models are available at https://github.com/antgroup/SPEED-Q.

</details>


### [27] [HOTFLoc++: End-to-End Hierarchical LiDAR Place Recognition, Re-Ranking, and 6-DoF Metric Localisation in Forests](https://arxiv.org/abs/2511.09170)
*Ethan Griffiths,Maryam Haghighat,Simon Denman,Clinton Fookes,Milad Ramezani*

Main category: cs.CV

TL;DR: HOTFLoc++是一个面向森林和城市环境激光雷达定位的端到端框架，显著提升了场景识别与定位精度和速度。


<details>
  <summary>Details</summary>
Motivation: 现有的激光雷达场景识别和定位方法，在自然环境（如森林）中极易受到杂乱、同质性强和视角变化的影响，导致重新排序和精确定位的失效，影响应用效果。

Method: 提出了基于八叉树的Transformer框架，提取多层次本地特征；引入可学习的多尺度几何验证模块，结合粗到细的配准策略，以增强环境健壮性和提高效率。

Result: 在公开数据集上，方法性能优于现有主流算法，CS-Wild-Places数据集Recall@1为90.7%，比基线提高约29.6%；Wild-Places和MulRan Recall@1分别达到91.7%和96.0%；97.2%的6-DoF配准误差低于2米5度，运行速度比RANSAC快2个量级。多尺度验证模块显著降低了定位误差。

Conclusion: HOTFLoc++能在自然及城市复杂环境下实现高精度、高效率的激光雷达场景识别与定位，适用于多种不同场景，技术优势明显。代码将在论文接收后公开。

Abstract: This article presents HOTFLoc++, an end-to-end framework for LiDAR place recognition, re-ranking, and 6-DoF metric localisation in forests. Leveraging an octree-based transformer, our approach extracts hierarchical local descriptors at multiple granularities to increase robustness to clutter, self-similarity, and viewpoint changes in challenging scenarios, including ground-to-ground and ground-to-aerial in forest and urban environments. We propose a learnable multi-scale geometric verification module to reduce re-ranking failures in the presence of degraded single-scale correspondences. Our coarse-to-fine registration approach achieves comparable or lower localisation errors to baselines, with runtime improvements of two orders of magnitude over RANSAC for dense point clouds. Experimental results on public datasets show the superiority of our approach compared to state-of-the-art methods, achieving an average Recall@1 of 90.7% on CS-Wild-Places: an improvement of 29.6 percentage points over baselines, while maintaining high performance on single-source benchmarks with an average Recall@1 of 91.7% and 96.0% on Wild-Places and MulRan, respectively. Our method achieves under 2 m and 5 degrees error for 97.2% of 6-DoF registration attempts, with our multi-scale re-ranking module reducing localisation errors by ~2$\times$ on average. The code will be available upon acceptance.

</details>


### [28] [Machines Serve Human: A Novel Variable Human-machine Collaborative Compression Framework](https://arxiv.org/abs/2511.08915)
*Zifu Zhang,Shengxi Li,Xiancheng Sun,Mai Xu,Zhengyuan Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散先验的特征压缩算法（Diff-FCHM），实现了以机器视觉为主导的人机协同图像压缩方法，在保证机器和人类视觉任务表现的同时，显著提升了压缩效果。


<details>
  <summary>Details</summary>
Motivation: 传统的人机协同压缩大多以人类视觉压缩流程为基础，难以满足机器视觉对于重点区域高效压缩的需求，同时在复杂度和码率上存在不足。机器视觉实际只需关注少量核心信息，因此有必要探索以机器视觉为主导的新型协同压缩方法。

Method: 提出以机器视觉为基础的人机协同压缩方法，开发可变码率策略适配不同机器视觉任务，将机器视觉压缩的语义特征逐步聚合，并通过扩散先验为人类视觉恢复高保真细节，实现面向人/机双重视觉目标的压缩。

Result: 实验结果显示，所提出的Diff-FCHM方法，无论在人类视觉压缩还是机器视觉压缩上均取得显著更优的性能。

Conclusion: 以机器视觉为主导的人机协同压缩是一种有效且前景广阔的方法，Diff-FCHM框架在提升压缩性能方面具备显著优势，为后续研究开创了新方向。

Abstract: Human-machine collaborative compression has been receiving increasing research efforts for reducing image/video data, serving as the basis for both human perception and machine intelligence. Existing collaborative methods are dominantly built upon the de facto human-vision compression pipeline, witnessing deficiency on complexity and bit-rates when aggregating the machine-vision compression. Indeed, machine vision solely focuses on the core regions within the image/video, requiring much less information compared with the compressed information for human vision. In this paper, we thus set out the first successful attempt by a novel collaborative compression method based on the machine-vision-oriented compression, instead of human-vision pipeline. In other words, machine vision serves as the basis for human vision within collaborative compression. A plug-and-play variable bit-rate strategy is also developed for machine vision tasks. Then, we propose to progressively aggregate the semantics from the machine-vision compression, whilst seamlessly tailing the diffusion prior to restore high-fidelity details for human vision, thus named as diffusion-prior based feature compression for human and machine visions (Diff-FCHM). Experimental results verify the consistently superior performances of our Diff-FCHM, on both machine-vision and human-vision compression with remarkable margins. Our code will be released upon acceptance.

</details>


### [29] [From Structure to Detail: Hierarchical Distillation for Efficient Diffusion Model](https://arxiv.org/abs/2511.08930)
*Hanbo Cheng,Peng Wang,Kaixiang Lei,Qi Li,Zhen Zou,Pengfei Hu,Jun Du*

Main category: cs.CV

TL;DR: 本文提出了一种名为Hierarchical Distillation（HD）的新型分层蒸馏框架，实现了高保真、单步扩散模型，大幅降低推理延迟，并在多项任务中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在实际应用中因推理延迟过高难以满足实时需求。现有的基于轨迹和分布的蒸馏方法各有缺点，不能兼顾结构保持与细节还原。因此，亟需设计一种兼具高效率和高保真度的新范式。

Method: 将传统的轨迹蒸馏和分布蒸馏整合为层次化协同流程。首先以轨迹蒸馏作为结构“草图”初始化分布蒸馏阶段，再通过改进的自适应加权判别器（AWD）动态分配token权重以精细优化局部细节，提高判别与生成器训练效果。

Result: 在ImageNet 256x256数据集上，单步模型的FID达到2.26，性能接近250步的教师模型；在MJHQ高分辨率文本生成图像基准上也取得了有竞争力的结果，展示了方法的泛化能力。

Conclusion: 提出的HD及AWD判别器框架显著提升了扩散模型的推理速度和生成质量，树立了高保真、单步扩散模型新范式。

Abstract: The inference latency of diffusion models remains a critical barrier to their real-time application. While trajectory-based and distribution-based step distillation methods offer solutions, they present a fundamental trade-off. Trajectory-based methods preserve global structure but act as a "lossy compressor", sacrificing high-frequency details. Conversely, distribution-based methods can achieve higher fidelity but often suffer from mode collapse and unstable training. This paper recasts them from independent paradigms into synergistic components within our novel Hierarchical Distillation (HD) framework. We leverage trajectory distillation not as a final generator, but to establish a structural ``sketch", providing a near-optimal initialization for the subsequent distribution-based refinement stage. This strategy yields an ideal initial distribution that enhances the ceiling of overall performance. To further improve quality, we introduce and refine the adversarial training process. We find standard discriminator structures are ineffective at refining an already high-quality generator. To overcome this, we introduce the Adaptive Weighted Discriminator (AWD), tailored for the HD pipeline. By dynamically allocating token weights, AWD focuses on local imperfections, enabling efficient detail refinement. Our approach demonstrates state-of-the-art performance across diverse tasks. On ImageNet $256\times256$, our single-step model achieves an FID of 2.26, rivaling its 250-step teacher. It also achieves promising results on the high-resolution text-to-image MJHQ benchmark, proving its generalizability. Our method establishes a robust new paradigm for high-fidelity, single-step diffusion models.

</details>


### [30] [Boosting Adversarial Transferability via Ensemble Non-Attention](https://arxiv.org/abs/2511.08937)
*Yipeng Zou,Qin Liu,Jie Wu,Yu Peng,Guo Chen,Hui Zhou,Guanghui Ye*

Main category: cs.CV

TL;DR: 提出了一种新型集成攻击方法NAMEA，通过融合非注意区域的梯度，显著提升了对异构模型（如CNN与ViT）之间的对抗迁移能力，在ImageNet实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有集成攻击方法在迁移到异构模型（如从CNN到ViT或反之）时表现不佳，主要因为不同模型的梯度方向差异大，难以兼顾各自特性，因此需要新的方法提升跨架构攻击效果。

Method: 受注意区域的异构性启发，作者首次将集成模型非注意区域的梯度纳入优化过程。具体方法为：将各模型的注意/非注意区域梯度分别解耦并融合，并借助元学习进行梯度合并，实现信息有效互补。

Result: 在ImageNet数据集上，NAMEA对比当前最优集成攻击方法AdaEA与SMER，平均提升分别为15.0%和9.6%。

Conclusion: 首次验证了集成非注意区域的信息可极大增强对抗样本跨结构迁移能力，为集成攻击研究提供了新视角和思路。

Abstract: Ensemble attacks integrate the outputs of surrogate models with diverse architectures, which can be combined with various gradient-based attacks to improve adversarial transferability. However, previous work shows unsatisfactory attack performance when transferring across heterogeneous model architectures. The main reason is that the gradient update directions of heterogeneous surrogate models differ widely, making it hard to reduce the gradient variance of ensemble models while making the best of individual model. To tackle this challenge, we design a novel ensemble attack, NAMEA, which for the first time integrates the gradients from the non-attention areas of ensemble models into the iterative gradient optimization process. Our design is inspired by the observation that the attention areas of heterogeneous models vary sharply, thus the non-attention areas of ViTs are likely to be the focus of CNNs and vice versa. Therefore, we merge the gradients respectively from the attention and non-attention areas of ensemble models so as to fuse the transfer information of CNNs and ViTs. Specifically, we pioneer a new way of decoupling the gradients of non-attention areas from those of attention areas, while merging gradients by meta-learning. Empirical evaluations on ImageNet dataset indicate that NAMEA outperforms AdaEA and SMER, the state-of-the-art ensemble attacks by an average of 15.0% and 9.6%, respectively. This work is the first attempt to explore the power of ensemble non-attention in boosting cross-architecture transferability, providing new insights into launching ensemble attacks.

</details>


### [31] [Neural B-frame Video Compression with Bi-directional Reference Harmonization](https://arxiv.org/abs/2511.08938)
*Yuxi Liu,Dengchao Jin,Shuai Huo,Jiawen Gu,Chao Zhou,Huihui Bai,Ming Lu,Zhan Ma*

Main category: cs.CV

TL;DR: 本文提出了一种名为BRHVC的双向参考和谐神经视频压缩方法，在神经B帧视频压缩领域取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 尽管神经视频压缩取得了很大进展，但B帧（使用双向参考帧）的视频压缩方法（NBVC）仍然研究较少，其层级编码结构在时域预测上带来了困难，尤其是在帧距较大时会导致双参考帧贡献不均衡，影响压缩效率。

Method: 提出了BRHVC方法，包含两项关键技术：1）BMC（双向运动融合）在运动压缩中融合多条光流，增强大尺度运动补偿的准确性；2）BCF（双向上下文融合）依据运动补偿的准确性明确建模参考帧上下文的权重，高效利用参考信息。

Result: 实验显示，所提BRHVC方法在HEVC数据集上优于以往最先进的神经视频压缩方法，并且在随机访问配置下超越了传统压缩标准VTM-RA。

Conclusion: BRHVC能够更有效地融合和利用双向参考帧信息，实现更高效的视频压缩，推动了神经B帧视频压缩领域的发展。

Abstract: Neural video compression (NVC) has made significant progress in recent years, while neural B-frame video compression (NBVC) remains underexplored compared to P-frame compression. NBVC can adopt bi-directional reference frames for better compression performance. However, NBVC's hierarchical coding may complicate continuous temporal prediction, especially at some hierarchical levels with a large frame span, which could cause the contribution of the two reference frames to be unbalanced. To optimize reference information utilization, we propose a novel NBVC method, termed Bi-directional Reference Harmonization Video Compression (BRHVC), with the proposed Bi-directional Motion Converge (BMC) and Bi-directional Contextual Fusion (BCF). BMC converges multiple optical flows in motion compression, leading to more accurate motion compensation on a larger scale. Then BCF explicitly models the weights of reference contexts under the guidance of motion compensation accuracy. With more efficient motions and contexts, BRHVC can effectively harmonize bi-directional references. Experimental results indicate that our BRHVC outperforms previous state-of-the-art NVC methods, even surpassing the traditional coding, VTM-RA (under random access configuration), on the HEVC datasets. The source code is released at https://github.com/kwai/NVC.

</details>


### [32] [FGM-HD: Boosting Generation Diversity of Fractal Generative Models through Hausdorff Dimension Induction](https://arxiv.org/abs/2511.08945)
*Haowei Zhang,Yuanpei Zhao,Jizhe Zhou,Mao Li*

Main category: cs.CV

TL;DR: 本文提出了一种在Fractal Generative Models（FGMs）中利用Hausdorff维数（HD）提升生成结果多样性的新方法，并显著提高了输出多样性，同时保持了图像质量。


<details>
  <summary>Details</summary>
Motivation: 虽然FGMs能够高效生成高质量图像，但其固有的自相似性导致输出缺乏多样性；现有方法难以在保障视觉效果的同时提升生成多样性。

Method: 作者提出了一种可学习的HD估计方法，通过直接从图像嵌入预测HD，用以衡量和引导结构复杂性。训练时，引入了基于HD的损失和单调动量驱动的调度策略，逐步优化超参数，保证多样性的提升不影响图像质量。推理阶段，采用HD引导的拒绝采样选择结构更丰富的输出。

Result: 在ImageNet数据集上实验表明，提出的FGM-HD框架相较原生FGM，输出多样性提升了39%，图像质量基本不变。

Conclusion: 该工作首次将HD引入FGM，有效提升了生成多样性，同时对FGM的发展提供了理论基础。

Abstract: Improving the diversity of generated results while maintaining high visual quality remains a significant challenge in image generation tasks. Fractal Generative Models (FGMs) are efficient in generating high-quality images, but their inherent self-similarity limits the diversity of output images. To address this issue, we propose a novel approach based on the Hausdorff Dimension (HD), a widely recognized concept in fractal geometry used to quantify structural complexity, which aids in enhancing the diversity of generated outputs. To incorporate HD into FGM, we propose a learnable HD estimation method that predicts HD directly from image embeddings, addressing computational cost concerns. However, simply introducing HD into a hybrid loss is insufficient to enhance diversity in FGMs due to: 1) degradation of image quality, and 2) limited improvement in generation diversity. To this end, during training, we adopt an HD-based loss with a monotonic momentum-driven scheduling strategy to progressively optimize the hyperparameters, obtaining optimal diversity without sacrificing visual quality. Moreover, during inference, we employ HD-guided rejection sampling to select geometrically richer outputs. Extensive experiments on the ImageNet dataset demonstrate that our FGM-HD framework yields a 39\% improvement in output diversity compared to vanilla FGMs, while preserving comparable image quality. To our knowledge, this is the very first work introducing HD into FGM. Our method effectively enhances the diversity of generated outputs while offering a principled theoretical contribution to FGM development.

</details>


### [33] [AuthSig: Safeguarding Scanned Signatures Against Unauthorized Reuse in Paperless Workflows](https://arxiv.org/abs/2511.08967)
*RuiQiang Zhang,Zehua Ma,Guanjie Wang,Chang Liu,Hengyi Wang,Weiming Zhang*

Main category: cs.CV

TL;DR: 提出AuthSig框架，用生成模型和水印技术，将认证信息嵌入静态签名图像，强化电子签名的安全性和唯一性。


<details>
  <summary>Details</summary>
Motivation: 静态电子签名因便利被广泛使用，但传统的静态签名图像缺乏可靠验证手段，易被伪造和复制，因此亟需提升静态签名的认证属性和安全性。

Method: 提出AuthSig框架，利用生成式模型，通过调整风格嵌入在生成过程中隐式加密水印信息，结合关键点驱动的数据增强方法以丰富签名风格，增强水印嵌入强度。

Result: 在多种数字域失真和签名特有降质条件下，AuthSig水印提取准确率超过98%，在打印-扫描场景下依然表现良好。

Conclusion: AuthSig能够显著提升静态电子签名的唯一性和认证能力，是静态签名安全性提升的创新方案。

Abstract: With the deepening trend of paperless workflows, signatures as a means of identity authentication are gradually shifting from traditional ink-on-paper to electronic formats.Despite the availability of dynamic pressure-sensitive and PKI-based digital signatures, static scanned signatures remain prevalent in practice due to their convenience. However, these static images, having almost lost their authentication attributes, cannot be reliably verified and are vulnerable to malicious copying and reuse. To address these issues, we propose AuthSig, a novel static electronic signature framework based on generative models and watermark, which binds authentication information to the signature image. Leveraging the human visual system's insensitivity to subtle style variations, AuthSig finely modulates style embeddings during generation to implicitly encode watermark bits-enforcing a One Signature, One Use policy.To overcome the scarcity of handwritten signature data and the limitations of traditional augmentation methods, we introduce a keypoint-driven data augmentation strategy that effectively enhances style diversity to support robust watermark embedding. Experimental results show that AuthSig achieves over 98% extraction accuracy under both digital-domain distortions and signature-specific degradations, and remains effective even in print-scan scenarios.

</details>


### [34] [Efficient and Effective In-context Demonstration Selection with Coreset](https://arxiv.org/abs/2511.08977)
*Zihua Wang,Jiarui Wang,Haiyang Xu,Ming Yan,Fei Huang,Xu Yang,Xiu-Shen Wei,Siya Mi,Yu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于Coreset的双重检索（CoDR）示范选择框架，用于提升大规模视觉语言模型（LVLMs）在上下文学习中的示范选择效果。通过聚类剪枝和双重检索机制，能够在效率和效果之间取得更好平衡，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型的上下文学习效果很大程度上依赖于示范样本的选择，但该过程是NP-hard的，现有方法（如随机、相似性采样和信息得分采样）在效率和效果之间难以兼顾。

Method: 提出了基于Coreset的多样化示范子集构建方法，通过聚类剪枝算法得到与查询更相关且保持多样性的核心集，并引入双重检索机制，实现高效的全球示范选择。

Result: 实验证明，该方法在多个任务和数据集上显著提升了上下文学习性能，优于主流的示范采样策略。

Conclusion: 基于Coreset的双重检索示范选择框架能有效、有效率地提升LVLMs上下文学习表现，为示范选择问题提供了新的、强有力的解决方案。

Abstract: In-context learning (ICL) has emerged as a powerful paradigm for Large Visual Language Models (LVLMs), enabling them to leverage a few examples directly from input contexts. However, the effectiveness of this approach is heavily reliant on the selection of demonstrations, a process that is NP-hard. Traditional strategies, including random, similarity-based sampling and infoscore-based sampling, often lead to inefficiencies or suboptimal performance, struggling to balance both efficiency and effectiveness in demonstration selection. In this paper, we propose a novel demonstration selection framework named Coreset-based Dual Retrieval (CoDR). We show that samples within a diverse subset achieve a higher expected mutual information. To implement this, we introduce a cluster-pruning method to construct a diverse coreset that aligns more effectively with the query while maintaining diversity. Additionally, we develop a dual retrieval mechanism that enhances the selection process by achieving global demonstration selection while preserving efficiency. Experimental results demonstrate that our method significantly improves the ICL performance compared to the existing strategies, providing a robust solution for effective and efficient demonstration selection.

</details>


### [35] [WDT-MD: Wavelet Diffusion Transformers for Microaneurysm Detection in Fundus Images](https://arxiv.org/abs/2511.08987)
*Yifei Sun,Yuzhi He,Junhao Jia,Jinhong Wang,Ruiquan Ge,Changmiao Wang,Hongxia Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于小波扩散Transformer（WDT-MD）的微动脉瘤自动检测方法，显著提升了糖尿病视网膜病变（DR）早期筛查的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型用于微动脉瘤检测时存在容易陷入“同一性映射”、异常类型区分能力弱、正常结构重建效果不佳等问题，影响临床应用效果。

Method: 作者提出了WDT-MD框架，包括三项创新：1）通过噪声编码的图像条件机制，扰动训练过程避免同一性映射；2）通过修复生成伪正常区域，实现像素级监督，提高微动脉瘤与其他异常区分能力；3）结合多尺度小波分析和扩散Transformer，增强正常视网膜结构的重建能力。

Result: 在IDRiD和e-ophtha MA公开数据集上，WDT-MD在像素级和图像级微动脉瘤检测性能均优于现有最先进方法。

Conclusion: WDT-MD方法有效克服了扩散模型在微动脉瘤自动检测中的关键局限，有望推动糖尿病视网膜病变早期筛查的自动化和准确性发展。

Abstract: Microaneurysms (MAs), the earliest pathognomonic signs of Diabetic Retinopathy (DR), present as sub-60 $μm$ lesions in fundus images with highly variable photometric and morphological characteristics, rendering manual screening not only labor-intensive but inherently error-prone. While diffusion-based anomaly detection has emerged as a promising approach for automated MA screening, its clinical application is hindered by three fundamental limitations. First, these models often fall prey to "identity mapping", where they inadvertently replicate the input image. Second, they struggle to distinguish MAs from other anomalies, leading to high false positives. Third, their suboptimal reconstruction of normal features hampers overall performance. To address these challenges, we propose a Wavelet Diffusion Transformer framework for MA Detection (WDT-MD), which features three key innovations: a noise-encoded image conditioning mechanism to avoid "identity mapping" by perturbing image conditions during training; pseudo-normal pattern synthesis via inpainting to introduce pixel-level supervision, enabling discrimination between MAs and other anomalies; and a wavelet diffusion Transformer architecture that combines the global modeling capability of diffusion Transformers with multi-scale wavelet analysis to enhance reconstruction of normal retinal features. Comprehensive experiments on the IDRiD and e-ophtha MA datasets demonstrate that WDT-MD outperforms state-of-the-art methods in both pixel-level and image-level MA detection. This advancement holds significant promise for improving early DR screening.

</details>


### [36] [An ICTM-RMSAV Framework for Bias-Field Aware Image Segmentation under Poisson and Multiplicative Noise](https://arxiv.org/abs/2511.08988)
*Xinyu Wang,Wenjun Yao,Fanghui Song,Zhichang Guo*

Main category: cs.CV

TL;DR: 本文针对图像分割在强噪声和强非均匀性时性能下降的问题，提出了一种结合去噪变分模型的分割方法，并通过实验验证了其在多种复杂噪声和非均匀情况下的优越性。


<details>
  <summary>Details</summary>
Motivation: 已有的图像分割方法在图像受强噪声污染（如伽马分布乘性噪声和泊松噪声）及强度非均匀时效果较差，亟需提升模型的鲁棒性和准确性。

Method: 在ICTM（迭代卷积阈值法）框架下，提出结合I-散度和自适应全变差正则化的去噪变分分割模型，通过灰度指示子自适应调节扩散系数，并估计平滑变化的偏置场来缓解强度非均匀带来的影响。利用RMSAV（松弛修正标量辅助变量）方案提升优化效率。

Result: 在合成和真实图像（包含强度非均匀和多种复杂噪声）上的大量实验表明，该方法在准确性与鲁棒性方面均优于现有主流分割方法。

Conclusion: 结合I-散度、自适应全变差及偏置场估计的新分割模型能更好应对复杂噪声和强度非均匀，提升了图像分割的实用性和效果，适合实际复杂环境下的应用。

Abstract: Image segmentation is a core task in image processing, yet many methods degrade when images are heavily corrupted by noise and exhibit intensity inhomogeneity. Within the iterative-convolution thresholding method (ICTM) framework, we propose a variational segmentation model that integrates denoising terms. Specifically, the denoising component consists of an I-divergence term and an adaptive total-variation (TV) regularizer, making the model well suited to images contaminated by Gamma--distributed multiplicative noise and Poisson noise. A spatially adaptive weight derived from a gray-level indicator guides diffusion differently across regions of varying intensity. To further address intensity inhomogeneity, we estimate a smoothly varying bias field, which improves segmentation accuracy. Regions are represented by characteristic functions, with contour length encoded accordingly. For efficient optimization, we couple ICTM with a relaxed modified scalar auxiliary variable (RMSAV) scheme. Extensive experiments on synthetic and real-world images with intensity inhomogeneity and diverse noise types show that the proposed model achieves superior accuracy and robustness compared with competing approaches.

</details>


### [37] [T-Rex-Omni: Integrating Negative Visual Prompt in Generic Object Detection](https://arxiv.org/abs/2511.08997)
*Jiazhou Zhou,Qing Jiang,Kanghao Chen,Lutao Jiang,Yuanhuiyi Lyu,Ying-Cong Chen,Lei Zhang*

Main category: cs.CV

TL;DR: 提出了一种全新的目标检测框架T-Rex-Omni，通过引入负向视觉提示，有效提升开放集目标检测的性能，尤其在相似但语义差异较大的场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的开放集目标检测方法主要依赖正向提示（如文本描述或图片示例），但面对外观相似却语义不同的干扰样本时表现不佳，容易被误检。作者希望通过引入负向提示来减少这类误检现象。

Method: 1. 提出统一视觉提示编码器，联合编码正负视觉提示；2. 设计了训练无关的Negating Negative Computing（NNC）模块，在概率计算阶段动态抑制负向响应；3. 提出了Negating Negative Hinge（NNH）损失，通过微调拉开正负样本间判别距离；4. 支持仅正向、正负联合两种预测模式，可用人工或自动生成的负样本。

Result: 在多个数据集上进行了广泛实验，T-Rex-Omni在零样本检测任务上表现优异，显著缩小了视觉提示和文本提示方法之间的性能差距，在长尾场景中尤为突出（比如LVIS-minival数据集AP_r达到51.2）。

Conclusion: 本工作证明了负向视觉提示可作为提升开放集目标检测性能的重要新维度，为开放集视觉识别带来了新的发展方向。

Abstract: Object detection methods have evolved from closed-set to open-set paradigms over the years. Current open-set object detectors, however, remain constrained by their exclusive reliance on positive indicators based on given prompts like text descriptions or visual exemplars. This positive-only paradigm experiences consistent vulnerability to visually similar but semantically different distractors. We propose T-Rex-Omni, a novel framework that addresses this limitation by incorporating negative visual prompts to negate hard negative distractors. Specifically, we first introduce a unified visual prompt encoder that jointly processes positive and negative visual prompts. Next, a training-free Negating Negative Computing (NNC) module is proposed to dynamically suppress negative responses during the probability computing stage. To further boost performance through fine-tuning, our Negating Negative Hinge (NNH) loss enforces discriminative margins between positive and negative embeddings. T-Rex-Omni supports flexible deployment in both positive-only and joint positive-negative inference modes, accommodating either user-specified or automatically generated negative examples. Extensive experiments demonstrate remarkable zero-shot detection performance, significantly narrowing the performance gap between visual-prompted and text-prompted methods while showing particular strength in long-tailed scenarios (51.2 AP_r on LVIS-minival). This work establishes negative prompts as a crucial new dimension for advancing open-set visual recognition systems.

</details>


### [38] [Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2511.09018)
*Liu Yu,Zhonghao Chen,Ping Kuang,Zhikun Feng,Fan Zhou,Lan Wang,Gillian Dobbie*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的因果推断框架Owl，专门用于减缓大规模视觉-语言模型（LVLMs）中的对象幻觉问题，通过层级式双模态注意力重加权实现更高的真实性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在处理图文任务时，常出现生成信息与视觉输入不符（对象幻觉）的问题。大部分缓解方法只独立调节视觉或文本注意力，忽略了两者互动这一关键因果因素。

Method: 1. 建立了基于结构因果图的框架Owl，将视觉和文本注意力解耦为因果中介；2. 提出VTACR指标衡量视觉与文本注意力的贡献失衡；3. 实现了基于VTACR的精细化干预机制，对token和层级注意力动态调整；4. 设计了双路径对比解码策略，引导模型兼顾视觉事实和幻觉抑制。

Result: 在POPE和CHAIR数据集上，Owl框架显著降低了LVLMs的对象幻觉发生率，在保证理解能力的前提下，取得了真实性（faithfulness）新SOTA。

Conclusion: Owl提出的新型注意力重加权和因果干预方法有效缓解了LVLMs中的对象幻觉，为提升视觉-语言一致性和模型实际应用可靠性提供了新方向。

Abstract: Object hallucination remains a critical challenge in Large Vision-Language Models (LVLMs), where models generate content inconsistent with visual inputs. Existing language-decoder based mitigation approaches often regulate visual or textual attention independently, overlooking their interaction as two key causal factors. To address this, we propose Owl (Bi-mOdal attention reWeighting for Layer-wise hallucination mitigation), a causally-grounded framework that models hallucination process via a structural causal graph, treating decomposed visual and textual attentions as mediators. We introduce VTACR (Visual-to-Textual Attention Contribution Ratio), a novel metric that quantifies the modality contribution imbalance during decoding. Our analysis reveals that hallucinations frequently occur in low-VTACR scenarios, where textual priors dominate and visual grounding is weakened. To mitigate this, we design a fine-grained attention intervention mechanism that dynamically adjusts token- and layer-wise attention guided by VTACR signals. Finally, we propose a dual-path contrastive decoding strategy: one path emphasizes visually grounded predictions, while the other amplifies hallucinated ones -- letting visual truth shine and hallucination collapse. Experimental results on the POPE and CHAIR benchmarks show that Owl achieves significant hallucination reduction, setting a new SOTA in faithfulness while preserving vision-language understanding capability. Our code is available at https://github.com/CikZ2023/OWL

</details>


### [39] [Dense Cross-Scale Image Alignment With Fully Spatial Correlation and Just Noticeable Difference Guidance](https://arxiv.org/abs/2511.09028)
*Jinkun You,Jiaxue Li,Jie Zhang,Yicong Zhou*

Main category: cs.CV

TL;DR: 提出了一种新的密集跨尺度图像对齐模型，在改善对齐精度的同时降低计算复杂度，并可根据需求灵活权衡性能与效率，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督图像对齐方法在精度有限且计算复杂度高，因此需要更高效且精度更高的解决方案。

Method: 提出了密集跨尺度图像对齐模型，综合利用跨尺度特征间的相关性，通过调整尺度数目实现精度与效率之间平衡。引入完全空间相关模块提升精度，通过引入“可察觉误差”关注失真敏感区域减少可见对齐误差。

Result: 大量定量和定性实验表明，该方法对比当前主流方法取得了更优结果。

Conclusion: 所提模型能够有效提升图像对齐精度，同时控制计算成本，具有灵活适应不同应用需求的能力。

Abstract: Existing unsupervised image alignment methods exhibit limited accuracy and high computational complexity. To address these challenges, we propose a dense cross-scale image alignment model. It takes into account the correlations between cross-scale features to decrease the alignment difficulty. Our model supports flexible trade-offs between accuracy and efficiency by adjusting the number of scales utilized. Additionally, we introduce a fully spatial correlation module to further improve accuracy while maintaining low computational costs. We incorporate the just noticeable difference to encourage our model to focus on image regions more sensitive to distortions, eliminating noticeable alignment errors. Extensive quantitative and qualitative experiments demonstrate that our method surpasses state-of-the-art approaches.

</details>


### [40] [USF-Net: A Unified Spatiotemporal Fusion Network for Ground-Based Remote Sensing Cloud Image Sequence Extrapolation](https://arxiv.org/abs/2511.09045)
*Penghui Niu,Taotao Cai,Jiashuai She,Yajuan Zhang,Junhua Gua,Ping Zhanga,Jungong Hane,Jianxin Li*

Main category: cs.CV

TL;DR: 本论文提出了USF-Net，一种专为地基遥感云图序列外推设计的统一时空融合网络，在ASI-CIS数据集上取得了优越的性能，兼顾预测精度与计算效率，并公开了相关数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 现有云图序列外推方法主要面临特征提取缺乏自适应、时序建模能力不足、注意力机制计算量过高等问题，影响了实际光伏系统场景的部署与应用效果。

Method: 提出USF-Net，将自适应大核卷积与低复杂度注意力机制融合于编码-解码框架中。具体包括多尺度动态特征提取模块、有效建模长时序依赖的注意力模块、统一建模时空关系的模块，以及解决鬼影效应的解码模块。此外，作者还构建并开放了ASI-CIS云图数据集。

Result: USF-Net在ASI-CIS数据集上的实验结果显示，相较当前主流方法，在保持高预测精度的同时，大幅提升了时空建模能力并降低了计算消耗。

Conclusion: USF-Net方法具有较强的通用性与高效性，有效平衡了准确率和实际部署需求，对地基云图外推及光伏发电预测领域具有较高参考价值。

Abstract: Ground-based remote sensing cloud image sequence extrapolation is a key research area in the development of photovoltaic power systems. However, existing approaches exhibit several limitations:(1)they primarily rely on static kernels to augment feature information, lacking adaptive mechanisms to extract features at varying resolutions dynamically;(2)temporal guidance is insufficient, leading to suboptimal modeling of long-range spatiotemporal dependencies; and(3)the quadratic computational cost of attention mechanisms is often overlooked, limiting efficiency in practical deployment. To address these challenges, we propose USF-Net, a Unified Spatiotemporal Fusion Network that integrates adaptive large-kernel convolutions and a low-complexity attention mechanism, combining temporal flow information within an encoder-decoder framework. Specifically, the encoder employs three basic layers to extract features. Followed by the USTM, which comprises:(1)a SiB equipped with a SSM that dynamically captures multi-scale contextual information, and(2)a TiB featuring a TAM that effectively models long-range temporal dependencies while maintaining computational efficiency. In addition, a DSM with a TGM is introduced to enable unified modeling of temporally guided spatiotemporal dependencies. On the decoder side, a DUM is employed to address the common "ghosting effect." It utilizes the initial temporal state as an attention operator to preserve critical motion signatures. As a key contribution, we also introduce and release the ASI-CIS dataset. Extensive experiments on ASI-CIS demonstrate that USF-Net significantly outperforms state-of-the-art methods, establishing a superior balance between prediction accuracy and computational efficiency for ground-based cloud extrapolation. The dataset and source code will be available at https://github.com/she1110/ASI-CIS.

</details>


### [41] [4KDehazeFlow: Ultra-High-Definition Image Dehazing via Flow Matching](https://arxiv.org/abs/2511.09055)
*Xingchi Chen,Pu Wang,Xuerui Li,Chaopeng Li,Juxiang Zhou,Jianhou Gan,Dianjie Lu,Guijuan Zhang,Wenqi Ren,Zhuoran Zheng*

Main category: cs.CV

TL;DR: 本文提出了4KDehazeFlow方法，通过流匹配和雾感知向量场优化，实现了高效自适应的4K超高清图像去雾，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法存在适应性不足、色彩失真或计算开销高等问题，特别是在超高清（UHD）图像处理上更为突出。

Method: 方法基于流匹配和连续向量场，通过进阶优化处理去雾流程。创新性地提出了可学习的3D查找表（LUT）编码雾变换参数，并用四阶Runge-Kutta方法（RK4）稳定求解去雾流场。该方法不依赖特定网络架构，兼容多种深度学习网络。

Result: 大量实验表明，4KDehazeFlow在PSNR等指标上超过七种最新方法，性能提升可达2dB，并且在高雾密度和色彩还原方面表现更佳。

Conclusion: 4KDehazeFlow方法为超高清图像去雾提供了高效、鲁棒和高保真的新方案，提升了现有去雾算法的综合表现。

Abstract: Ultra-High-Definition (UHD) image dehazing faces challenges such as limited scene adaptability in prior-based methods and high computational complexity with color distortion in deep learning approaches. To address these issues, we propose 4KDehazeFlow, a novel method based on Flow Matching and the Haze-Aware vector field. This method models the dehazing process as a progressive optimization of continuous vector field flow, providing efficient data-driven adaptive nonlinear color transformation for high-quality dehazing. Specifically, our method has the following advantages: 1) 4KDehazeFlow is a general method compatible with various deep learning networks, without relying on any specific network architecture. 2) We propose a learnable 3D lookup table (LUT) that encodes haze transformation parameters into a compact 3D mapping matrix, enabling efficient inference through precomputed mappings. 3) We utilize a fourth-order Runge-Kutta (RK4) ordinary differential equation (ODE) solver to stably solve the dehazing flow field through an accurate step-by-step iterative method, effectively suppressing artifacts. Extensive experiments show that 4KDehazeFlow exceeds seven state-of-the-art methods. It delivers a 2dB PSNR increase and better performance in dense haze and color fidelity.

</details>


### [42] [PAN: A World Model for General, Interactable, and Long-Horizon World Simulation](https://arxiv.org/abs/2511.09057)
*PAN Team,Jiannan Xiang,Yi Gu,Zihan Liu,Zeyu Feng,Qiyue Gao,Yiyan Hu,Benhao Huang,Guangyi Liu,Yichi Yang,Kun Zhou,Davit Abrahamyan,Arif Ahmad,Ganesh Bannur,Junrong Chen,Kimi Chen,Mingkai Deng,Ruobing Han,Xinqi Huang,Haoqiang Kang,Zheqi Li,Enze Ma,Hector Ren,Yashowardhan Shinde,Rohan Shingre,Ramsundar Tanikella,Kaiming Tao,Dequan Yang,Xinle Yu,Cong Zeng,Binglin Zhou,Hector Liu,Zhiting Hu,Eric P. Xing*

Main category: cs.CV

TL;DR: 该论文提出了一种新型世界模型PAN，能够通过高质量视频模拟，在历史信息和自然语言动作条件下预测未来的世界状态，支持广泛开放域和长时序推理。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型虽然能生成真实的视频片段，但缺乏因果控制、交互性及长时序一致性，不利于智能体做出推理和规划。传统世界模型应用范围有限，泛化能力和可控性不足，难以适应多样环境与交互需求。

Method: PAN模型采用Generative Latent Prediction (GLP)架构，通过大语言模型驱动的自回归潜在动态骨干，结合视频扩散解码器实现，能利用文本知识支持自然语言动作控制，并生成时序一致的高质量视频。训练数据覆盖多领域大规模视频—动作对。

Result: 实验表明，PAN在动作条件下的世界模拟、长时序预测与仿真推理任务上均优于现有视频生成和世界建模方法，实现开放域下具备因果互动和长远推理能力的世界模拟。

Conclusion: PAN推动了通用世界模型发展，使得基于推理和行为的未来世界状态预测成为可能，为智能体提供更为强大和适用的模拟工具。

Abstract: A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.

</details>


### [43] [VietMEAgent: Culturally-Aware Few-Shot Multimodal Explanation for Vietnamese Visual Question Answering](https://arxiv.org/abs/2511.09058)
*Hai-Dang Nguyen,Minh-Anh Dang,Minh-Tan Le,Minh-Tuan Le*

Main category: cs.CV

TL;DR: 本文提出了VietMEAgent，一个专为越南文化理解设计的多模态可解释性VQA系统，结合了文化对象检测和程序化解释，提升了对越南文化问题的回答准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉问答系统（VQA）在处理特定文化内容时表现有限，主要因为训练数据中文化知识的匮乏以及推理过程缺少可解释性。本文旨在解决VQA系统对越南文化内容的理解难题，并提升系统解释其答案的能力。

Method: 提出了一种新框架——VietMEAgent。该方法包括：1）文化对象检测主干网络；2）结构化程序生成层，将答案预测与解释过程紧密结合；3）专门整理的越南文化知识库，作为背景知识源；4）双模态解释模块，结合可视化证据与结构化文本理由。并构建了越南文化VQA数据集，用于评估方法有效性。

Result: 系统能够实现对越南文化相关问题的准确问答，并生成易于理解的解释文本，清晰揭示推理及背后的文化内容。实验表明基于程序的方法适用于文化AI任务。

Conclusion: VietMEAgent有效提升了VQA系统在文化内容上的表现，支持教育及文化保护，同时强调解释性和文化敏感性，为文化AI领域提供了一种透明、可追溯的方法论。

Abstract: Contemporary Visual Question Answering (VQA) systems remain constrained when confronted with culturally specific content, largely because cultural knowledge is under-represented in training corpora and the reasoning process is not rendered interpretable to end users. This paper introduces VietMEAgent, a multimodal explainable framework engineered for Vietnamese cultural understanding. The method integrates a cultural object detection backbone with a structured program generation layer, yielding a pipeline in which answer prediction and explanation are tightly coupled. A curated knowledge base of Vietnamese cultural entities serves as an explicit source of background information, while a dual-modality explanation module combines attention-based visual evidence with structured, human-readable textual rationales. We further construct a Vietnamese Cultural VQA dataset sourced from public repositories and use it to demonstrate the practicality of programming-based methodologies for cultural AI. The resulting system provides transparent explanations that disclose both the computational rationale and the underlying cultural context, supporting education and cultural preservation with an emphasis on interpretability and cultural sensitivity.

</details>


### [44] [Diversifying Counterattacks: Orthogonal Exploration for Robust CLIP Inference](https://arxiv.org/abs/2511.09064)
*Chengze Jiang,Minjing Dong,Xinli Shi,Jie Gui*

Main category: cs.CV

TL;DR: 作者提出了一种新的对抗防御方法Directional Orthogonal Counterattack（DOC），通过增加对抗空间的探索和扰动多样性，有效提升视觉-语言预训练模型对对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言预训练模型虽然表现出较强的多模态理解与零样本泛化能力，但仍容易受到对抗样本的影响。现有方法如TTC虽然能提升鲁棒性，但受限于优化目标和梯度方向，容易陷入对有限对抗模式的过拟合，缺乏泛化能力。因此，提升对抗扰动多样性和防御覆盖面变得尤为重要。

Method: 提出了Directional Orthogonal Counterattack（DOC）。该方法通过引入正交梯度方向与动量更新，扩展了对抗扰动空间的搜索范围，并提高了扰动多样性。同时，设计了基于平均余弦相似度的方向敏感度分数（directional sensitivity score），用于增强样本判别力和自适应调节对抗强度。

Result: 在16个数据集上的大量实验表明，DOC方法在多种对抗攻击下均显著提升了模型的鲁棒性，并能保持竞争性的干净样本准确率。

Conclusion: DOC通过提升对抗扰动的多样性和覆盖，能够更有效地抵御多样化的对抗攻击，为提升多模态模型的实际可靠性提供了新的思路。

Abstract: Vision-language pre-training models (VLPs) demonstrate strong multimodal understanding and zero-shot generalization, yet remain vulnerable to adversarial examples, raising concerns about their reliability. Recent work, Test-Time Counterattack (TTC), improves robustness by generating perturbations that maximize the embedding deviation of adversarial inputs using PGD, pushing them away from their adversarial representations. However, due to the fundamental difference in optimization objectives between adversarial attacks and counterattacks, generating counterattacks solely based on gradients with respect to the adversarial input confines the search to a narrow space. As a result, the counterattacks could overfit limited adversarial patterns and lack the diversity to fully neutralize a broad range of perturbations. In this work, we argue that enhancing the diversity and coverage of counterattacks is crucial to improving adversarial robustness in test-time defense. Accordingly, we propose Directional Orthogonal Counterattack (DOC), which augments counterattack optimization by incorporating orthogonal gradient directions and momentum-based updates. This design expands the exploration of the counterattack space and increases the diversity of perturbations, which facilitates the discovery of more generalizable counterattacks and ultimately improves the ability to neutralize adversarial perturbations. Meanwhile, we present a directional sensitivity score based on averaged cosine similarity to boost DOC by improving example discrimination and adaptively modulating the counterattack strength. Extensive experiments on 16 datasets demonstrate that DOC improves adversarial robustness under various attacks while maintaining competitive clean accuracy. Code is available at https://github.com/bookman233/DOC.

</details>


### [45] [Composition-Incremental Learning for Compositional Generalization](https://arxiv.org/abs/2511.09082)
*Zhen Li,Yuwei Wu,Chenchen Jing,Che Sun,Chuanhao Li,Yunde Jia*

Main category: cs.CV

TL;DR: 本文提出了一个用于增量组合泛化的研究任务框架，并在零样本组合学习背景下提出了新基准和方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据和组合类型近乎无限且呈长尾分布，现有方法通常依赖预先收集的训练集，难以应对持续出现的新组合。理想模型应能逐步提升自身的组合泛化能力。

Method: 作者构建了Composition-Incremental Learning for Compositional Generalization（CompIL）任务，在组合零样本学习场景下逐步学习新组合。为衡量该任务，开发了两套新基准（MIT-States-CompIL与C-GQA-CompIL）。提出了一种伪重放框架，使用视觉合成器生成已学组合的表示，并通过语言原语蒸馏机制保持学习过程中原语的一致性。

Result: 在多个基准上进行了大量实验，结果显示所提出方法有效提升了增量组合泛化能力。

Conclusion: 本文构建了增量组合泛化学习新框架和评测体系，并提出有效的伪重放加原语蒸馏方法，推动了组合泛化领域向实际应用环境迈进。

Abstract: Compositional generalization has achieved substantial progress in computer vision on pre-collected training data. Nonetheless, real-world data continually emerges, with possible compositions being nearly infinite, long-tailed, and not entirely visible. Thus, an ideal model is supposed to gradually improve the capability of compositional generalization in an incremental manner. In this paper, we explore Composition-Incremental Learning for Compositional Generalization (CompIL) in the context of the compositional zero-shot learning (CZSL) task, where models need to continually learn new compositions, intending to improve their compositional generalization capability progressively. To quantitatively evaluate CompIL, we develop a benchmark construction pipeline leveraging existing datasets, yielding MIT-States-CompIL and C-GQA-CompIL. Furthermore, we propose a pseudo-replay framework utilizing a visual synthesizer to synthesize visual representations of learned compositions and a linguistic primitive distillation mechanism to maintain aligned primitive representations across the learning process. Extensive experiments demonstrate the effectiveness of the proposed framework.

</details>


### [46] [Ultra-Light Test-Time Adaptation for Vision--Language Models](https://arxiv.org/abs/2511.09101)
*Byunghyun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种名为UL-TTA的超轻量测试时自适应（Test-Time Adaptation）方法，仅通过调整logit层参数（类别原型、类别先验和温度），而无需训练或反向传播主干网络，即可有效提升视觉-语言模型（VLMs）在不同领域任务中的表现与校准能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（如CLIP）在领域迁移时会出现特征漂移、类别先验不匹配以及严重的模型失准等问题，而主流TTA方法需要高计算和内存开销，不适用于流式和边缘场景。因此，需要一种无需微调大模型、资源消耗极小，同时又能有效适应域变化的新方法。

Method: UL-TTA保持主干网络参数冻结，只自适应logit层的类别原型、类别先验和温度参数，通过：1）高置信样本选择，2）结合文本先验和Dirichlet先验的闭式贝叶斯更新，3）分别用于预测和校准的温度解耦，4）引入诸如范数裁剪、先验KL约束、平滑温度等机制防止长时间推理中的漂移，来实现高效域自适应。全程无需训练和反向传播。

Result: 在PACS、Office-Home、DomainNet、Terra Incognita、ImageNet-R/A/V2/Sketch等多个跨领域与OOD数据集上（共约72.6万测试样本），UL-TTA相较于零样本CLIP平均提升Top-1准确率4.7个百分点，降低ECE 20-30%，延迟开销小于8%。长序列（高达20万样本）实验无性能崩溃。

Conclusion: 不需要更新主干网络参数，仅对logit层做贝叶斯式自适应已足以在VLM的域迁移任务中实现业界领先的准确率-校准能力兼优表现。UL-TTA兼具高效、轻量和实用性，适合边缘/流式等资源受限场合。

Abstract: Vision-Language Models (VLMs) such as CLIP achieve strong zero-shot recognition by comparing image embeddings to text-derived class prototypes. However, under domain shift, they suffer from feature drift, class-prior mismatch, and severe miscalibration. Existing test-time adaptation (TTA) methods often require backpropagation through large backbones, covariance estimation, or heavy memory/state, which is problematic for streaming and edge scenarios. We propose Ultra-Light Test-Time Adaptation (UL-TTA), a fully training-free and backprop-free framework that freezes the backbone and adapts only logit-level parameters: class prototypes, class priors, and temperature. UL-TTA performs an online EM-style procedure with (i) selective sample filtering to use only confident predictions, (ii) closed-form Bayesian updates for prototypes and priors anchored by text and Dirichlet priors, (iii) decoupled temperatures for prediction vs. calibration, and (iv) lightweight guards (norm clipping, prior KL constraints, smoothed temperature) to prevent drift in long streams. Across large-scale cross-domain and OOD benchmarks (PACS, Office-Home, DomainNet, Terra Incognita, ImageNet-R/A/V2/Sketch; ~726K test samples) and strong TTA baselines including Tent, T3A, CoTTA, SAR, Tip-Adapter, and FreeTTA, UL-TTA consistently improves top-1 accuracy (e.g., +4.7 points over zero-shot CLIP on average) while reducing ECE by 20-30%, with less than 8% latency overhead. Long-stream experiments up to 200K samples show no collapse. Our results demonstrate that logit-level Bayesian adaptation is sufficient to obtain state-of-the-art accuracy-calibration trade-offs for VLMs under domain shift, without updating any backbone parameters.

</details>


### [47] [DKDS: A Benchmark Dataset of Degraded Kuzushiji Documents with Seals for Detection and Binarization](https://arxiv.org/abs/2511.09117)
*Rui-Yang Ju,Kohei Yamashita,Hirotaka Kameko,Shinsuke Mori*

Main category: cs.CV

TL;DR: 本文介绍了一个针对日本古文草书（Kuzushiji）手稿退化与印章等噪声的新数据集DKDS，并提供了基线算法实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有Kuzushiji OCR对手稿退化和印章干扰鲁棒性不足，同时缺乏相关专门数据集，阻碍了真实场景下识别技术的发展。

Method: 研究者构建了包含手稿退化和印章噪声的DKDS数据集，制定了两个基准任务：1）文字与印章检测，2）文档二值化。评估了多版YOLO模型用于检测，和传统/聚类/生成对抗网络方法用于二值化。

Result: 在DKDS数据集上，YOLO等模型完成了文字和印章检测任务，GAN等方法应用于文档二值化，均提供了公开的基线实验结果。

Conclusion: DKDS数据集为Kuzushiji文档OCR研究提供了具有挑战性的标准基准，有助于提升在噪声和退化情况下的识别能力，支持相关领域进步。

Abstract: Kuzushiji, a pre-modern Japanese cursive script, can currently be read and understood by only a few thousand trained experts in Japan. With the rapid development of deep learning, researchers have begun applying Optical Character Recognition (OCR) techniques to transcribe Kuzushiji into modern Japanese. Although existing OCR methods perform well on clean pre-modern Japanese documents written in Kuzushiji, they often fail to consider various types of noise, such as document degradation and seals, which significantly affect recognition accuracy. To the best of our knowledge, no existing dataset specifically addresses these challenges. To address this gap, we introduce the Degraded Kuzushiji Documents with Seals (DKDS) dataset as a new benchmark for related tasks. We describe the dataset construction process, which required the assistance of a trained Kuzushiji expert, and define two benchmark tracks: (1) text and seal detection and (2) document binarization. For the text and seal detection track, we provide baseline results using multiple versions of the You Only Look Once (YOLO) models for detecting Kuzushiji characters and seals. For the document binarization track, we present baseline results from traditional binarization algorithms, traditional algorithms combined with K-means clustering, and Generative Adversarial Network (GAN)-based methods. The DKDS dataset and the implementation code for baseline methods are available at https://ruiyangju.github.io/DKDS.

</details>


### [48] [PIFF: A Physics-Informed Generative Flow Model for Real-Time Flood Depth Mapping](https://arxiv.org/abs/2511.09130)
*ChunLiang Wu,Tsunhua Yang,Hungying Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于物理知识并结合生成神经网络的洪水深度快速估算方法PIFF，能高效地实现数字高程模型到洪水深度的映射。结合SPM水动力简化模型和降雨时序信息，显著提升了洪水图的准确性与实时性。


<details>
  <summary>Details</summary>
Motivation: 传统洪水制图方法如数值建模和航空摄影在效率和可靠性上存在不足，难以实现近实时且准确的洪水评估。为此，亟需一种新方法提升洪水制图的时效性和精度。

Method: 提出PIFF模型：基于图像生成的神经网络，输入DEM数字高程模型，结合物理简化内涝模型（SPM）引入水动力先验，并通过transformer结构对降雨时序进行建模，物理约束与数据驱动结合，捕捉降雨、地形、SPM与洪水深度之间因果关系。

Result: 在台湾台南26公里研究区，针对182种不同降雨情景（24mm~720mm/24h），PIFF模型能够高效生成精确洪水深度分布，对比传统数值模拟显著提升速度且保证精度。

Conclusion: PIFF为洪水预测和应急提供了高效且数据驱动的备选方案，有望替代高成本、慢速的传统模拟，为洪水响应与管理带来重要价值。

Abstract: Flood mapping is crucial for assessing and mitigating flood impacts, yet traditional methods like numerical modeling and aerial photography face limitations in efficiency and reliability. To address these challenges, we propose PIFF, a physics-informed, flow-based generative neural network for near real-time flood depth estimation. Built on an image-to-image generative framework, it efficiently maps Digital Elevation Models (DEM) to flood depth predictions. The model is conditioned on a simplified inundation model (SPM) that embeds hydrodynamic priors into the training process. Additionally, a transformer-based rainfall encoder captures temporal dependencies in precipitation. Integrating physics-informed constraints with data-driven learning, PIFF captures the causal relationships between rainfall, topography, SPM, and flooding, replacing costly simulations with accurate, real-time flood maps. Using a 26 km study area in Tainan, Taiwan, with 182 rainfall scenarios ranging from 24 mm to 720 mm over 24 hours, our results demonstrate that PIFF offers an effective, data-driven alternative for flood prediction and response.

</details>


### [49] [MACEval: A Multi-Agent Continual Evaluation Network for Large Models](https://arxiv.org/abs/2511.09139)
*Zijian Chen,Yuze Sun,Yuan Tian,Wenjun Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: MACEval是一种针对大模型的多智能体持续评估网络，能实现更动态、高效与自动化的大模型评估。


<details>
  <summary>Details</summary>
Motivation: 现有很多面向大模型的评测基准，存在数据污染、过拟合和人工依赖过重等问题，导致评测可信度和可维护性不足。作者希望解决这些局限，实现更可持续、动态、自动化的大模型测评。

Method: 提出MACEval系统，采用多智能体持续评估网络，具有交互式、自主评估机制。具体方法包括对智能体分配角色、过程中动态生成数据、利用级联智能体网络进行评测路由，并引入了新的随时间变化的评价指标。

Result: 实验证明MACEval可在9个开放任务下，针对23个大模型完成高效、自动、灵活的评测，相较传统基准减少人力和数据消耗，支持灵活迁移与集成。

Conclusion: MACEval避免了人工评测，具备高效、经济、灵活、可扩展等优势，有潜力成为大模型持续评测的重要手段，引领未来测评新方向。

Abstract: Hundreds of benchmarks dedicated to evaluating large models from multiple perspectives have been presented over the past few years. Albeit substantial efforts, most of them remain closed-ended and are prone to overfitting due to the potential data contamination in the ever-growing training corpus of large models, thereby undermining the credibility of the evaluation. Moreover, the increasing scale and scope of current benchmarks with transient metrics, as well as the heavily human-dependent curation procedure, pose significant challenges for timely maintenance and adaptation to gauge the advancing capabilities of large models. In this paper, we introduce MACEval, a \Multi-Agent Continual Evaluation network for dynamic evaluation of large models, and define a new set of metrics to quantify performance longitudinally and sustainably. MACEval adopts an interactive and autonomous evaluation mode that employs role assignment, in-process data generation, and evaluation routing through a cascaded agent network. Extensive experiments on 9 open-ended tasks with 23 participating large models demonstrate that MACEval is (1) human-free and automatic, mitigating laborious result processing with inter-agent judgment guided; (2) efficient and economical, reducing a considerable amount of data and overhead to obtain similar results compared to related benchmarks; and (3) flexible and scalable, migrating or integrating existing benchmarks via customized evaluation topologies. We hope that MACEval can broaden future directions of large model evaluation.

</details>


### [50] [PressTrack-HMR: Pressure-Based Top-Down Multi-Person Global Human Mesh Recovery](https://arxiv.org/abs/2511.09147)
*Jiayue Yuan,Fangting Xie,Guangwen Ouyang,Changhai Ma,Ziyu Wu,Heyu Ding,Quan Wan,Yi Ke,Yuchen Wu,Xiaohui Cai*

Main category: cs.CV

TL;DR: 本文提出了一种全新的基于压力信号的多人人体网格恢复方法，即PressTrack-HMR，实现了在多人人体交互场景下无视觉输入的动作捕捉。


<details>
  <summary>Details</summary>
Motivation: 现有视觉为主的人体网格恢复方法在现实环境中易受遮挡、光照和隐私等影响；而基于压感技术的动作捕捉虽然自然具备不受遮挡和隐私友好特点，但现有方法主要集中在单人场景，难以处理多人压力信号混杂问题。

Method: 提出一种top-down的PressTrack-HMR管线，先用跟踪-检测策略将多个个体的压力信号从原始数据中区分和分割，再分别对每个体进行压力信号下的人体网格恢复。并构建了多人人-压感交互数据集MIP。

Result: 在多人人压力数据下实验，方法取得了89.2 mm的MPJPE和112.6 mm的WA-MPJPE_100，显示出优越的多人网格恢复性能。

Conclusion: 本文方法展现了基于压力信号的无视觉、注重隐私的多人人体动作识别与网格恢复的可行性和前景，并通过数据集推进了相关研究的发展。

Abstract: Multi-person global human mesh recovery (HMR) is crucial for understanding crowd dynamics and interactions. Traditional vision-based HMR methods sometimes face limitations in real-world scenarios due to mutual occlusions, insufficient lighting, and privacy concerns. Human-floor tactile interactions offer an occlusion-free and privacy-friendly alternative for capturing human motion. Existing research indicates that pressure signals acquired from tactile mats can effectively estimate human pose in single-person scenarios. However, when multiple individuals walk randomly on the mat simultaneously, how to distinguish intermingled pressure signals generated by different persons and subsequently acquire individual temporal pressure data remains a pending challenge for extending pressure-based HMR to the multi-person situation. In this paper, we present \textbf{PressTrack-HMR}, a top-down pipeline that recovers multi-person global human meshes solely from pressure signals. This pipeline leverages a tracking-by-detection strategy to first identify and segment each individual's pressure signal from the raw pressure data, and subsequently performs HMR for each extracted individual signal. Furthermore, we build a multi-person interaction pressure dataset \textbf{MIP}, which facilitates further research into pressure-based human motion analysis in multi-person scenarios. Experimental results demonstrate that our method excels in multi-person HMR using pressure data, with 89.2~$mm$ MPJPE and 112.6~$mm$ WA-MPJPE$_{100}$, and these showcase the potential of tactile mats for ubiquitous, privacy-preserving multi-person action recognition. Our dataset \& code are available at https://github.com/Jiayue-Yuan/PressTrack-HMR.

</details>


### [51] [DBINDS - Can Initial Noise from Diffusion Model Inversion Help Reveal AI-Generated Videos?](https://arxiv.org/abs/2511.09184)
*Yanlin Wu,Xiaogang Yuan,Dezhi An*

Main category: cs.CV

TL;DR: DBINDS是一种基于扩散模型反演的新型AI生成视频检测器，通过分析初始噪声序列区分真实视频和生成视频，具备强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成视频检测方法主要依赖于像素级线索，难以泛化到未见过的生成器。随着AI生成视频技术迅速进步，内容安全与溯源成为亟需解决的问题。

Method: DBINDS利用扩散模型的反演机制，分析生成或真实视频在扩散模型中恢复出的初始噪声序列（INDS）。从这些序列中提取多域、多尺度特征，并通过特征优化及贝叶斯调参的LightGBM分类器进行检测。在只用单一生成器的数据训练下，测试其跨生成器的检测性能。

Result: DBINDS在GenVidBench基准数据集上，实现了只用单一生成器训练却能对多生成器良好检测的跨生成器性能，展现出强泛化能力和数据有限条件下的鲁棒性。

Conclusion: 通过分析扩散模型反演初始噪声，DBINDS打破了现有方法对像素线索的依赖，提供了一种更具通用性、适应性的AI视频生成检测新思路，对内容安全与溯源有重要意义。

Abstract: AI-generated video has advanced rapidly and poses serious challenges to content security and forensic analysis. Existing detectors rely mainly on pixel-level visual cues and generalize poorly to unseen generators. We propose DBINDS, a diffusion-model-inversion based detector that analyzes latent-space dynamics rather than pixels. We find that initial noise sequences recovered by diffusion inversion differ systematically between real and generated videos. Building on this, DBINDS forms an Initial Noise Difference Sequence (INDS) and extracts multi-domain, multi-scale features. With feature optimization and a LightGBM classifier tuned by Bayesian search, DBINDS (trained on a single generator) achieves strong cross-generator performance on GenVidBench, demonstrating good generalization and robustness in limited-data settings.

</details>


### [52] [Towards Trustworthy Dermatology MLLMs: A Benchmark and Multimodal Evaluator for Diagnostic Narratives](https://arxiv.org/abs/2511.09195)
*Yuhao Shen,Jiahe Qian,Shuping Zhang,Zhangtianyi Chen,Tao Lu,Juexiao Zhou*

Main category: cs.CV

TL;DR: 本论文提出了DermBench和DermEval框架，用于对多模态大语言模型在皮肤科诊断生成文本任务中的效果进行可靠、公正、可扩展的评估。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型已可直接以图片生成皮肤科诊断叙述，但缺乏可靠的评估方法阻碍了模型在真实临床中的应用。

Method: 作者构建了DermBench基准集，包含4000张皮肤科真实图片及专家给出的诊断文本，并使用基于LLM的打分系统从临床维度为生成叙述评分；此外，训练了DermEval参考无关自动评估器，可对单病例给出结构化点评和细分评分。

Result: 在4500个多样化病例上的实验证明，DermBench和DermEval在与专家评分的一致性上表现良好，平均差值分别为0.251和0.117（满分5分），能有效衡量模型诊断能力和可靠性。

Conclusion: DermBench和DermEval为多模态大模型在皮肤科图文任务中提供了临床相关、可复现、可细致分析的评估工具，有助于模型安全可靠地应用于临床场景。

Abstract: Multimodal large language models (LLMs) are increasingly used to generate dermatology diagnostic narratives directly from images. However, reliable evaluation remains the primary bottleneck for responsible clinical deployment. We introduce a novel evaluation framework that combines DermBench, a meticulously curated benchmark, with DermEval, a robust automatic evaluator, to enable clinically meaningful, reproducible, and scalable assessment. We build DermBench, which pairs 4,000 real-world dermatology images with expert-certified diagnostic narratives and uses an LLM-based judge to score candidate narratives across clinically grounded dimensions, enabling consistent and comprehensive evaluation of multimodal models. For individual case assessment, we train DermEval, a reference-free multimodal evaluator. Given an image and a generated narrative, DermEval produces a structured critique along with an overall score and per-dimension ratings. This capability enables fine-grained, per-case analysis, which is critical for identifying model limitations and biases. Experiments on a diverse dataset of 4,500 cases demonstrate that DermBench and DermEval achieve close alignment with expert ratings, with mean deviations of 0.251 and 0.117 (out of 5), respectively, providing reliable measurement of diagnostic ability and trustworthiness across different multimodal LLMs.

</details>


### [53] [Taming Object Hallucinations with Verified Atomic Confidence Estimation](https://arxiv.org/abs/2511.09228)
*Jiarui Liu,Weihao Xuan,Zhijing Jin,Mona Diab*

Main category: cs.CV

TL;DR: 本文提出了一种名为TACO的简单框架，通过自验证和置信度校准，有效减少多模态大语言模型（MLLMs）的幻觉现象，提高模型输出的可信度。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型经常出现事实错误（幻觉），如对物体存在、属性或关系的误判，严重影响其实用性，因此亟需提升模型回答的可靠性与可信度。

Method: TACO框架将模型输出分解为原子性查询，并通过同义改写降低查询措辞的敏感性，然后利用自洽性（black-box）或自置信（gray-box）方法进行置信度估计，最终再用语言模型精炼回答，全流程无需额外视觉专家。

Result: 在POPE、MME、HallusionBench、AMBER和MM-Hal Bench五个基准测试，以及在LLaVA-1.5-7B和CogVLM2两个MLLM上，TACO表现优于直接提示法和Visual Contrastive Decoding，能有效降低系统性偏差并提升置信度校准。

Conclusion: TACO框架简单且高效，能够显著提升多模态大语言模型输出的真实度和信任度，对MLLM幻觉问题的缓解具有积极意义。

Abstract: Multimodal Large Language Models (MLLMs) often suffer from hallucinations, particularly errors in object existence, attributes, or relations, which undermine their reliability. We introduce TACO (Verified Atomic Confidence Estimation), a simple framework that mitigates hallucinations through self-verification and confidence calibration without relying on external vision experts. TACO decomposes responses into atomic queries, paraphrases them to reduce sensitivity to wording, and estimates confidence using self-consistency (black-box) or self-confidence (gray-box) aggregation, before refining answers with a language model. Experiments on five benchmarks (POPE, MME, HallusionBench, AMBER, and MM-Hal Bench) with two MLLMs (\texttt{LLaVA-1.5-7B} and \texttt{CogVLM2}) show that TACO consistently outperforms direct prompting and Visual Contrastive Decoding, reduces systematic biases, and improves confidence calibration, demonstrating its effectiveness in enhancing the faithfulness of MLLMs.

</details>


### [54] [Spatial Information Bottleneck for Interpretable Visual Recognition](https://arxiv.org/abs/2511.09239)
*Kaixiang Shu,Kai Meng,Junqin Luo*

Main category: cs.CV

TL;DR: 本文提出了Spatial Information Bottleneck（S-IB）方法，通过信息瓶颈理论分离神经网络中的前景和背景特征，从而提升模型可解释性与鲁棒性，并在多个解释方法和基准测试中效果突出。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络往往将判别性前景特征与无关的背景相关性混杂在一起，这降低了模型的可解释性和鲁棒性。作者希望通过新的理论分析及方法，提升模型关注于真正与分类相关的前景区域。

Method: 作者从信息论角度重新理解基于梯度的归因方法，证明了反向传播中计算的向量-雅可比积（VJP）具有输入特征到类别标签的最小充分统计量属性。基于此，提出编码-解码视角：前向传播将输入编码到类别空间，反向中的VJP实现从类别空间到特征空间的解码。进一步提出Spatial Information Bottleneck（S-IB），最大化前景区域的互信息、最小化背景区域的互信息，引导网络仅在类别相关区域编码信息。

Result: 在五个基准数据集上，S-IB对六种主流解释方法均带来普适提升。可视化结果表现为前景更加集中、背景干扰减少，无需针对具体方法调优。同时，分类准确率也有所提升。

Conclusion: 通过从VJP入手对特征空间进行空间解耦，S-IB显著提升了神经网络的可解释性与鲁棒性，为理解和优化模型提供了通用、高效的解决方案。

Abstract: Deep neural networks typically learn spatially entangled representations that conflate discriminative foreground features with spurious background correlations, thereby undermining model interpretability and robustness. We propose a novel understanding framework for gradient-based attribution from an information-theoretic perspective. We prove that, under mild conditions, the Vector-Jacobian Products (VJP) computed during backpropagation form minimal sufficient statistics of input features with respect to class labels. Motivated by this finding, we propose an encoding-decoding perspective : forward propagation encodes inputs into class space, while VJP in backpropagation decodes this encoding back to feature space. Therefore, we propose Spatial Information Bottleneck (S-IB) to spatially disentangle information flow. By maximizing mutual information between foreground VJP and inputs while minimizing mutual information in background regions, S-IB encourages networks to encode information only in class-relevant spatial regions. Since post-hoc explanation methods fundamentally derive from VJP computations, directly optimizing VJP's spatial structure during training improves visualization quality across diverse explanation paradigms. Experiments on five benchmarks demonstrate universal improvements across six explanation methods, achieving better foreground concentration and background suppression without method-specific tuning, alongside consistent classification accuracy gains.

</details>


### [55] [GRACE: Designing Generative Face Video Codec via Agile Hardware-Centric Workflow](https://arxiv.org/abs/2511.09272)
*Rui Wan,Qi Zheng,Ruoyu Zhang,Bu Chen,Jiaming Liu,Min Li,Minge Jing,Jinjia Zhou,Yibo Fan*

Main category: cs.CV

TL;DR: 本文提出了一种面向FPGA的AGC动画驱动人脸视频压缩方案，大幅提升在边缘设备上的能效。


<details>
  <summary>Details</summary>
Motivation: AGC解码器虽然能有效压缩动画人脸视频，但其复杂的计算和灵活性差，难以部署到资源和功耗受限的边缘设备。

Method: 作者分析了AGC算法，采用后训练静态量化和层融合等压缩方法，并基于软硬协同的co-processor范式设计重叠加速器，优化并行、管线及硬件结构，最后在PYNQ-Z1 FPGA平台上实现原型。

Result: 该FPGA方案在能效上相较于商业CPU和GPU分别提升了24.9倍和4.1倍，每重建一个像素仅需11.7μJ。

Conclusion: 利用上述软硬结合和优化设计，可显著降低AGC解码器的能耗，使其更适合实际边缘视频计算场景。

Abstract: The Animation-based Generative Codec (AGC) is an emerging paradigm for talking-face video compression. However, deploying its intricate decoder on resource and power-constrained edge devices presents challenges due to numerous parameters, the inflexibility to adapt to dynamically evolving algorithms, and the high power consumption induced by extensive computations and data transmission. This paper for the first time proposes a novel field programmable gate arrays (FPGAs)-oriented AGC deployment scheme for edge-computing video services. Initially, we analyze the AGC algorithm and employ network compression methods including post-training static quantization and layer fusion techniques. Subsequently, we design an overlapped accelerator utilizing the co-processor paradigm to perform computations through software-hardware co-design. The hardware processing unit comprises engines such as convolution, grid sampling, upsample, etc. Parallelization optimization strategies like double-buffered pipelines and loop unrolling are employed to fully exploit the resources of FPGA. Ultimately, we establish an AGC FPGA prototype on the PYNQ-Z1 platform using the proposed scheme, achieving \textbf{24.9$\times$} and \textbf{4.1$\times$} higher energy efficiency against commercial Central Processing Unit (CPU) and Graphic Processing Unit (GPU), respectively. Specifically, only \textbf{11.7} microjoules ($\upmu$J) are required for one pixel reconstructed by this FPGA system.

</details>


### [56] [Deep Learning for Metabolic Rate Estimation from Biosignals: A Comparative Study of Architectures and Signal Selection](https://arxiv.org/abs/2511.09276)
*Sarvenaz Babakhani,David Remy,Alina Roitberg*

Main category: cs.CV

TL;DR: 本研究系统评估了不同生理信号与深度学习模型在人体能量消耗估算中的表现，并公开了相关代码。


<details>
  <summary>Details</summary>
Motivation: 传统的能量消耗预测多采用回归方法，少有研究区分神经网络结构和信号选择的独立影响。本研究旨在弥补这一空白，探究不同信号与神经网络架构对能量消耗预测准确度的贡献。

Method: 作者将多种经典回归方法与新型神经网络架构（如Transformer、CNN、ResNet with attention）进行对比，输入包括单一信号、信号对以及多信号组合，涵盖多种体力活动。

Result: 研究发现分钟通气量（minute ventilation）为最具预测力的单一特征，采用Transformer模型在所有活动下取得了最低RMSE（0.87 W/kg）。多信号组合作为输入时，速度较快的模型（如CNN、ResNet with attention）在准确性上表现良好。不同活动类型下预测效果不同，低强度活动误差显著较低。个体层面分析还显示存在较大的人际差异。

Conclusion: 不同生理信号和网络结构在能量消耗预测中作用各异，建议未来采用自适应建模策略应对个体差异。相关方法及模型已开源，便于后续研究。

Abstract: Energy expenditure estimation aims to infer human metabolic rate from physiological signals such as heart rate, respiration, or accelerometer data, and has been studied primarily with classical regression methods. The few existing deep learning approaches rarely disentangle the role of neural architecture from that of signal choice. In this work, we systematically evaluate both aspects. We compare classical baselines with newer neural architectures across single signals, signal pairs, and grouped sensor inputs for diverse physical activities. Our results show that minute ventilation is the most predictive individual signal, with a transformer model achieving the lowest root mean square error (RMSE) of 0.87 W/kg across all activities. Paired and grouped signals, such as those from the Hexoskin smart shirt (five signals), offer good alternatives for faster models like CNN and ResNet with attention. Per-activity evaluation revealed mixed outcomes: notably better results in low-intensity activities (RMSE down to 0.29 W/kg; NRMSE = 0.04), while higher-intensity tasks showed larger RMSE but more comparable normalized errors. Finally, subject-level analysis highlights strong inter-individual variability, motivating the need for adaptive modeling strategies. Our code and models will be publicly available at https://github.com/Sarvibabakhani/deeplearning-biosignals-ee .

</details>


### [57] [Enriching Knowledge Distillation with Cross-Modal Teacher Fusion](https://arxiv.org/abs/2511.09286)
*Amir M. Mansourian,Amir Mohammad Babaei,Shohreh Kasaei*

Main category: cs.CV

TL;DR: 本文提出利用CLIP视觉-语言模型与传统教师模型融合，实现更丰富的知识蒸馏，提升学生模型准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前多教师知识蒸馏方法大多只依赖视觉模态，缺乏知识多样性，忽视了跨模态表征的潜力。如何结合视觉和语言的丰富信息以提升蒸馏效果尚未被充分研究。

Method: 提出了一个简单有效的框架，将传统教师模型的logit和特征与CLIP模型的输出融合，并引入CLIP多提示文本引导，形成集数据集特性与语义丰富视觉线索于一体的监督信号。

Result: 融合教师的预测更加自信且可靠，大幅提升正确自信预测，减少错误自信预测，且为非目标类别提供了更合理的概率分布，增强了类别间一致性。该方法在多个基准测试中都超过了现有大多数方法，并在分布变化和输入干扰下表现出更强的鲁棒性。

Conclusion: 所提出的RichKD方法通过融合CLIP的视觉-语言知识，极大丰富了蒸馏监督信号，显著提升了学生模型的性能和泛化能力，同时保持框架简洁。

Abstract: Multi-teacher knowledge distillation (KD), a more effective technique than traditional single-teacher methods, transfers knowledge from expert teachers to a compact student model using logit or feature matching. However, most existing approaches lack knowledge diversity, as they rely solely on unimodal visual information, overlooking the potential of cross-modal representations. In this work, we explore the use of CLIP's vision-language knowledge as a complementary source of supervision for KD, an area that remains largely underexplored. We propose a simple yet effective framework that fuses the logits and features of a conventional teacher with those from CLIP. By incorporating CLIP's multi-prompt textual guidance, the fused supervision captures both dataset-specific and semantically enriched visual cues. Beyond accuracy, analysis shows that the fused teacher yields more confident and reliable predictions, significantly increasing confident-correct cases while reducing confidently wrong ones. Moreover, fusion with CLIP refines the entire logit distribution, producing semantically meaningful probabilities for non-target classes, thereby improving inter-class consistency and distillation quality. Despite its simplicity, the proposed method, Enriching Knowledge Distillation (RichKD), consistently outperforms most existing baselines across multiple benchmarks and exhibits stronger robustness under distribution shifts and input corruptions.

</details>


### [58] [DensiCrafter: Physically-Constrained Generation and Fabrication of Self-Supporting Hollow Structures](https://arxiv.org/abs/2511.09298)
*Shengqi Dang,Fu Chai,Jiaxin Li,Chao Yuan,Wei Ye,Nan Cao*

Main category: cs.CV

TL;DR: 本文提出了一种能够生成轻量、自支撑的3D空心结构的框架DensiCrafter，优化密度场以兼顾结构稳定性与可制造性，在3D打印实验中验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D生成模型忽略了物理约束和可制造性，常导致生成的三维结构不能被实际制造或自支撑。本文旨在解决如何生成既轻质又能够自我支撑的3D结构，满足现实世界应用中对制造可行性和材料节约的需求。

Method: 提出了DensiCrafter框架：以粗略体素网格作为初始输入，解释为连续密度场并进行优化。引入三种可微分、满足物理约束且无需仿真的损失项，同时使用质量正则项减少多余材料，通过有限优化范围保持外表形状。方法与Trellis等现有生成模型无缝集成，无需更改其架构。

Result: 在大量评测中，DensiCrafter在文本到3D任务中可实现材料质量最多43%的降低。与现有最优方法相比，能提升结构稳定性并保持高几何拟合度。实际3D打印实验显示生成的空心结构可可靠制造且实现自支撑。

Conclusion: DensiCrafter能在保证结构稳定性和可打印性的前提下大幅降低材料用量，为3D设计生成模型的实际落地提供了有价值的新方法。

Abstract: The rise of 3D generative models has enabled automatic 3D geometry and texture synthesis from multimodal inputs (e.g., text or images). However, these methods often ignore physical constraints and manufacturability considerations. In this work, we address the challenge of producing 3D designs that are both lightweight and self-supporting. We present DensiCrafter, a framework for generating lightweight, self-supporting 3D hollow structures by optimizing the density field. Starting from coarse voxel grids produced by Trellis, we interpret these as continuous density fields to optimize and introduce three differentiable, physically constrained, and simulation-free loss terms. Additionally, a mass regularization penalizes unnecessary material, while a restricted optimization domain preserves the outer surface. Our method seamlessly integrates with pretrained Trellis-based models (e.g., Trellis, DSO) without any architectural changes. In extensive evaluations, we achieve up to 43% reduction in material mass on the text-to-3D task. Compared to state-of-the-art baselines, our method could improve the stability and maintain high geometric fidelity. Real-world 3D-printing experiments confirm that our hollow designs can be reliably fabricated and could be self-supporting.

</details>


### [59] [DualFete: Revisiting Teacher-Student Interactions from a Feedback Perspective for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2511.09319)
*Le Yi,Wei Huang,Lei Zhang,Kefu Zhao,Yan Wang,Zizhou Wang*

Main category: cs.CV

TL;DR: 本文针对半监督医学图像分割中的教师-学生范式，提出引入反馈机制，减少伪标签错误带来的自我强化偏差。通过学生对教师伪标签变化的反馈，指导教师优化伪标签，从而有效减少错误传播。扩展提出双教师反馈模型，进一步抑制一致性错误，在多个医学图像任务中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 医学图像存在天然的模糊性，半监督教师-学生框架易受到伪标签错误的影响，学生模型会持续确认这些错误导致偏差扩增。现有方法通常侧重外部调整而忽视框架内部的错误自纠潜力。因此，亟需利用框架本身机制提升伪标签准确性，降低错误自我强化。

Method: 作者为教师-学生范式引入反馈机制，让学生模型对伪标签变化的影响进行反馈，教师据此优化伪标签。具体包括两个主要组件：反馈归因器（用于定位导致学生更新的伪标签）和反馈接收器（确定反馈应用位置）。同时提出双教师模型，通过师生间及教师间的反馈交互，引入动态性，提升修正错误能力。

Result: 在三个医学图像分割基准数据集上实验，所提方法有效缓解了伪标签导致的错误传播，提高了分割性能。与现有方法相比，在处理错误自循环和一致性错误方面表现优越。

Conclusion: 通过将反馈机制嵌入教师-学生半监督框架，提升了医学图像分割中伪标签修正能力，抑制了错误自我确认与传播。双教师反馈模型在多个基准任务中取得更优表现，为半监督学习提供了具备内部自纠功能的创新范式。

Abstract: The teacher-student paradigm has emerged as a canonical framework in semi-supervised learning. When applied to medical image segmentation, the paradigm faces challenges due to inherent image ambiguities, making it particularly vulnerable to erroneous supervision. Crucially, the student's iterative reconfirmation of these errors leads to self-reinforcing bias. While some studies attempt to mitigate this bias, they often rely on external modifications to the conventional teacher-student framework, overlooking its intrinsic potential for error correction. In response, this work introduces a feedback mechanism into the teacher-student framework to counteract error reconfirmations. Here, the student provides feedback on the changes induced by the teacher's pseudo-labels, enabling the teacher to refine these labels accordingly. We specify that this interaction hinges on two key components: the feedback attributor, which designates pseudo-labels triggering the student's update, and the feedback receiver, which determines where to apply this feedback. Building on this, a dual-teacher feedback model is further proposed, which allows more dynamics in the feedback loop and fosters more gains by resolving disagreements through cross-teacher supervision while avoiding consistent errors. Comprehensive evaluations on three medical image benchmarks demonstrate the method's effectiveness in addressing error propagation in semi-supervised medical image segmentation.

</details>


### [60] [FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection](https://arxiv.org/abs/2511.09347)
*Jiangyong Yu,Changyong Shu,Sifan Zhou,Zichen Yu,Xing Hu,Yan Chen,Dawei Yang*

Main category: cs.CV

TL;DR: 本文提出FQ-PETR，对PETR及其变种实现全量化，显著提升3D目标检测推理效率，同时保持近乎无损精度。


<details>
  <summary>Details</summary>
Motivation: PETRs在相机多视角3D检测领域表现优异，但推理耗时和内存占用高，难以部署。常规网络量化方法移植至PETRs会导致精度大幅下降，主要原因在于多模态特征幅值不一致和非线性算子的量化误差。

Method: 提出FQ-PETR，包含三项创新：（1）QFPE：用基于LiDAR先验的单点采样和锚点式嵌入替换传统PE，简化非线性操作并统一特征幅值；（2）DULUT：用级联线性查找表高效逼近复杂非线性函数，无需专用硬件；（3）QANS：数值稳定后再量化，减缓大输入下关注机制失真。

Result: 在PETRs多种框架下（如PETR、StreamPETR、PETRv2、MV2d），8位权重与激活的量化模型精度仅比浮点下降1%，但推理延迟降低最高达75%，大幅优于现有的后训练量化（PTQ）和量化训练（QAT）方法。

Conclusion: FQ-PETR通过结构创新和量化优化，实现了高精度、低延迟的PETRs部署，为高效3D感知系统落地提供了新方案。

Abstract: Camera-based multi-view 3D detection is crucial for autonomous driving. PETR and its variants (PETRs) excel in benchmarks but face deployment challenges due to high computational cost and memory footprint. Quantization is an effective technique for compressing deep neural networks by reducing the bit width of weights and activations. However, directly applying existing quantization methods to PETRs leads to severe accuracy degradation. This issue primarily arises from two key challenges: (1) significant magnitude disparity between multi-modal features-specifically, image features and camera-ray positional embeddings (PE), and (2) the inefficiency and approximation error of quantizing non-linear operators, which commonly rely on hardware-unfriendly computations. In this paper, we propose FQ-PETR, a fully quantized framework for PETRs, featuring three key innovations: (1) Quantization-Friendly LiDAR-ray Position Embedding (QFPE): Replacing multi-point sampling with LiDAR-prior-guided single-point sampling and anchor-based embedding eliminates problematic non-linearities (e.g., inverse-sigmoid) and aligns PE scale with image features, preserving accuracy. (2) Dual-Lookup Table (DULUT): This algorithm approximates complex non-linear functions using two cascaded linear LUTs, achieving high fidelity with minimal entries and no specialized hardware. (3) Quantization After Numerical Stabilization (QANS): Performing quantization after softmax numerical stabilization mitigates attention distortion from large inputs. On PETRs (e.g. PETR, StreamPETR, PETRv2, MV2d), FQ-PETR under W8A8 achieves near-floating-point accuracy (1% degradation) while reducing latency by up to 75%, significantly outperforming existing PTQ and QAT baselines.

</details>


### [61] [Spatio-Temporal Context Learning with Temporal Difference Convolution for Moving Infrared Small Target Detection](https://arxiv.org/abs/2511.09352)
*Houzhang Fang,Shukai Guo,Qiuhuan Chen,Yi Chang,Luxin Yan*

Main category: cs.CV

TL;DR: 本文提出了一种用于红外小目标检测的新型神经网络模型TDCNet，通过结合时序差分与三维卷积，有效提升了复杂背景下运动目标的检测能力，并在多个公开数据集上取得了领先的检测性能。


<details>
  <summary>Details</summary>
Motivation: 运动红外小目标检测在无人机监控等实际应用中意义重大，但由于目标不明显且背景干扰复杂，准确检测非常困难。现有方法单独使用时序差分或三维卷积各有局限，因此亟需更高效的时空特征建模方法提升检测准确率。

Method: 作者提出TDCNet网络，包含一种创新的时序差分卷积（TDC）模块，将时序差分和三维卷积融合为统一的时空特征表示。通过三路TDC块捕获不同时间范围的上下文依赖，增强运动特征的同时抑制背景伪运动。并设计了一种TDC引导的时空注意力机制，跨TDC主干与三维主干实现特征交互，实现全局语义依赖建模。

Result: TDCNet在IRSTD-UAV和多个公开红外数据集的移动目标检测任务中，检测精度达到或超过当前最优方法，显著提升在复杂背景下的检测能力。

Conclusion: TDCNet通过创新时空特征建模模块和注意力机制，有效提升了运动红外小目标检测的鲁棒性和准确性，为实际复杂场景应用提供了有力技术支持。

Abstract: Moving infrared small target detection (IRSTD) plays a critical role in practical applications, such as surveillance of unmanned aerial vehicles (UAVs) and UAV-based search system. Moving IRSTD still remains highly challenging due to weak target features and complex background interference. Accurate spatio-temporal feature modeling is crucial for moving target detection, typically achieved through either temporal differences or spatio-temporal (3D) convolutions. Temporal difference can explicitly leverage motion cues but exhibits limited capability in extracting spatial features, whereas 3D convolution effectively represents spatio-temporal features yet lacks explicit awareness of motion dynamics along the temporal dimension. In this paper, we propose a novel moving IRSTD network (TDCNet), which effectively extracts and enhances spatio-temporal features for accurate target detection. Specifically, we introduce a novel temporal difference convolution (TDC) re-parameterization module that comprises three parallel TDC blocks designed to capture contextual dependencies across different temporal ranges. Each TDC block fuses temporal difference and 3D convolution into a unified spatio-temporal convolution representation. This re-parameterized module can effectively capture multi-scale motion contextual features while suppressing pseudo-motion clutter in complex backgrounds, significantly improving detection performance. Moreover, we propose a TDC-guided spatio-temporal attention mechanism that performs cross-attention between the spatio-temporal features from the TDC-based backbone and a parallel 3D backbone. This mechanism models their global semantic dependencies to refine the current frame's features. Extensive experiments on IRSTD-UAV and public infrared datasets demonstrate that our TDCNet achieves state-of-the-art detection performance in moving target detection.

</details>


### [62] [Learning by Neighbor-Aware Semantics, Deciding by Open-form Flows: Towards Robust Zero-Shot Skeleton Action Recognition](https://arxiv.org/abs/2511.09388)
*Yang Chen,Miaoge Li,Zhijie Rao,Deze Zeng,Song Guo,Jingcai Guo*

Main category: cs.CV

TL;DR: 本文提出了一种名为Flora的新方法，用于零样本骨架动作识别，核心理念是通过灵活的邻域感知语义调整与分布感知分类器，有效解决现有方法中对齐和分类的两大难题，并在多个数据集上取得了优异的效果，尤其在少量数据下仍表现优秀。


<details>
  <summary>Details</summary>
Motivation: 零样本骨架动作识别中，缺乏目标类别的骨架先验知识，现有方法对齐和分类都存在语义不完美和边界粗糙等核心问题，亟需一种更稳健和灵活的解决方法。

Method: 提出Flora方法，通过引入邻域感知的语义调整（flexible neighbor-aware semantic attunement），结合跨模态几何一致性，使得文本语义能更贴近实际骨架数据分布，实现更为稳定的区域对齐，同时结合分布感知流式分类器（distribution-aware flow classifier），利用无噪声流匹配和条件无关对比正则化，实现更细粒度的决策边界。

Result: 在三个主流基准数据集上进行了大量实验，验证了Flora方法的有效性。在只用10%的已见数据训练时，仍取得了显著的性能提升，表现优异。

Conclusion: Flora方法在提升零样本骨架动作识别的准确性和鲁棒性上有明显优势，尤其在数据稀缺场景下表现突出，为未来相关领域的研究提供了新思路和方法。

Abstract: Recognizing unseen skeleton action categories remains highly challenging due to the absence of corresponding skeletal priors. Existing approaches generally follow an "align-then-classify" paradigm but face two fundamental issues, i.e., (i) fragile point-to-point alignment arising from imperfect semantics, and (ii) rigid classifiers restricted by static decision boundaries and coarse-grained anchors. To address these issues, we propose a novel method for zero-shot skeleton action recognition, termed $\texttt{$\textbf{Flora}$}$, which builds upon $\textbf{F}$lexib$\textbf{L}$e neighb$\textbf{O}$r-aware semantic attunement and open-form dist$\textbf{R}$ibution-aware flow cl$\textbf{A}$ssifier. Specifically, we flexibly attune textual semantics by incorporating neighboring inter-class contextual cues to form direction-aware regional semantics, coupled with a cross-modal geometric consistency objective that ensures stable and robust point-to-region alignment. Furthermore, we employ noise-free flow matching to bridge the modality distribution gap between semantic and skeleton latent embeddings, while a condition-free contrastive regularization enhances discriminability, leading to a distribution-aware classifier with fine-grained decision boundaries achieved through token-level velocity predictions. Extensive experiments on three benchmark datasets validate the effectiveness of our method, showing particularly impressive performance even when trained with only 10\% of the seen data. Code is available at https://github.com/cseeyangchen/Flora.

</details>


### [63] [OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS](https://arxiv.org/abs/2511.09397)
*Haiyi Li,Qi Chen,Denis Kalkofen,Hsiang-Ting Chen*

Main category: cs.CV

TL;DR: 论文提出了一种基于物理参数不确定性的新型3D高斯投影（3DGS）主动重建方法OUGS，能够高效提升复杂场景中特定目标的3D重建质量。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS方法在新视图合成上表现优异，但复杂场景中特定目标的高保真重建仍有困难。传统主动重建方法依赖场景级不确定性度量，容易被无关背景干扰，导致视角选择低效。

Method: OUGS框架通过将3D高斯原语的物理参数（位置、尺度、旋转）作为不确定性来源，并利用渲染Jacobian传播协方差，建立物理可解释的不确定性模型。同时，结合语义分割掩模，明确区分目标与环境，得到目标感知的不确定性评价，用于主动高效选择提升目标重建质量的关键视角。

Result: 在公开数据集上的实验显示，该方法显著提升了3DGS重建效率，并在目标对象重建质量上优于现有先进方法，同时为整体场景提供了稳健的不确定性估计。

Conclusion: OUGS提出了物理基础更扎实、对象感知能力更强的主动3DGS重建流程，有效解决了复杂场景下目标重建的效率和质量问题。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have achieved state-of-the-art results for novel view synthesis. However, efficiently capturing high-fidelity reconstructions of specific objects within complex scenes remains a significant challenge. A key limitation of existing active reconstruction methods is their reliance on scene-level uncertainty metrics, which are often biased by irrelevant background clutter and lead to inefficient view selection for object-centric tasks. We present OUGS, a novel framework that addresses this challenge with a more principled, physically-grounded uncertainty formulation for 3DGS. Our core innovation is to derive uncertainty directly from the explicit physical parameters of the 3D Gaussian primitives (e.g., position, scale, rotation). By propagating the covariance of these parameters through the rendering Jacobian, we establish a highly interpretable uncertainty model. This foundation allows us to then seamlessly integrate semantic segmentation masks to produce a targeted, object-aware uncertainty score that effectively disentangles the object from its environment. This allows for a more effective active view selection strategy that prioritizes views critical to improving object fidelity. Experimental evaluations on public datasets demonstrate that our approach significantly improves the efficiency of the 3DGS reconstruction process and achieves higher quality for targeted objects compared to existing state-of-the-art methods, while also serving as a robust uncertainty estimator for the global scene.

</details>


### [64] [BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation](https://arxiv.org/abs/2511.09443)
*Hongchao Shu,Roger D. Soberanis-Mukul,Jiru Xu,Hao Ding,Morgan Ringel,Mali Shen,Saif Iftekar Sayed,Hedyeh Rafii-Tari,Mathias Unberath*

Main category: cs.CV

TL;DR: 该论文提出一种基于视觉的支气管镜定位方法，通过引入全新合成数据基准，使支气管镜导航更加准确可靠，并具有跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前支气管镜定位受呼吸、解剖差异和CT到体内形态变化的影响，导致术中定位精度下降。现有基于视觉的方法泛化能力差，跨患者和场景适应性不足，导致定位仍有误差。因此急需更通用、可标准化评价的方法以及公开数据集。

Method: 作者提出一种基于视觉的位姿优化框架，实现内窥镜2D视图与术前CT 3D解剖间的配准。通过训练可处理不同模态和领域差异的编码器，实现真实内镜RGB帧与CT深度图之间的相似性计算，并利用可微渲染模块迭代优化相机位姿。提出并发布了首个公开的合成支气管镜导航基准数据集，用于标准化评价。

Result: 在只用与测试集未重叠的合成数据训练下，该方法在公开基准上取得2.65mm的平移误差和0.19弧度的旋转误差，定位精度高且稳定。实际患者上的定性结果也验证了其跨域泛化能力，无需专门领域适配，能实现一致的2D-3D配准。

Conclusion: 该方法实现了稳健的支气管镜定位及良好的跨域泛化能力，促进了术中视觉导航的标准化和 reproducibility，新基准数据集为该领域提供重要基础。

Abstract: Accurate intra-operative localization of the bronchoscope tip relative to patient anatomy remains challenging due to respiratory motion, anatomical variability, and CT-to-body divergence that cause deformation and misalignment between intra-operative views and pre-operative CT. Existing vision-based methods often fail to generalize across domains and patients, leading to residual alignment errors. This work establishes a generalizable foundation for bronchoscopy navigation through a robust vision-based framework and a new synthetic benchmark dataset that enables standardized and reproducible evaluation. We propose a vision-based pose optimization framework for frame-wise 2D-3D registration between intra-operative endoscopic views and pre-operative CT anatomy. A fine-tuned modality- and domain-invariant encoder enables direct similarity computation between real endoscopic RGB frames and CT-rendered depth maps, while a differentiable rendering module iteratively refines camera poses through depth consistency. To enhance reproducibility, we introduce the first public synthetic benchmark dataset for bronchoscopy navigation, addressing the lack of paired CT-endoscopy data. Trained exclusively on synthetic data distinct from the benchmark, our model achieves an average translational error of 2.65 mm and a rotational error of 0.19 rad, demonstrating accurate and stable localization. Qualitative results on real patient data further confirm strong cross-domain generalization, achieving consistent frame-wise 2D-3D alignment without domain-specific adaptation. Overall, the proposed framework achieves robust, domain-invariant localization through iterative vision-based optimization, while the new benchmark provides a foundation for standardized progress in vision-based bronchoscopy navigation.

</details>


### [65] [Hand Held Multi-Object Tracking Dataset in American Football](https://arxiv.org/abs/2511.09455)
*Rintaro Otsubo,Kanta Sawafuji,Hideo Saito*

Main category: cs.CV

TL;DR: 本文针对美式橄榄球运动员多目标检测与跟踪任务，提出首个公开专用数据集，并在此基础上评估和比较多种检测与跟踪方法。实验结果表明，通过微调模型可显著提升检测和追踪的准确性，填补了相关领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 当前多目标跟踪方法虽然在行人和其他常见场景得到广泛评估，但缺乏适用于美式橄榄球等高遮挡、频繁身体接触场景的标准化数据集，导致方法之间难以公平比较，因此亟需填补这个数据空白。

Method: 作者构建了首个美式橄榄球运动员专用检测与跟踪数据集，并采用该数据集系统性地评估和比较了不同的目标检测与跟踪算法。同时，研究了微调检测模型和重识别模型对整体跟踪系统性能的影响。

Result: 实验表明，在高密度拥挤场景下，经过微调的检测模型和重识别模型可显著提升检测和跟踪的精度，优于传统的预训练模型及现有方法。

Conclusion: 该工作不仅填补了美式橄榄球运动多目标跟踪领域无公开数据集的缺憾，为未来方法的公平评估提供了基础，还证明通过模型微调可以显著提升难场景下的检测与跟踪效果，推动了相关研究的发展。

Abstract: Multi-Object Tracking (MOT) plays a critical role in analyzing player behavior from videos, enabling performance evaluation. Current MOT methods are often evaluated using publicly available datasets. However, most of these focus on everyday scenarios such as pedestrian tracking or are tailored to specific sports, including soccer and basketball. Despite the inherent challenges of tracking players in American football, such as frequent occlusion and physical contact, no standardized dataset has been publicly available, making fair comparisons between methods difficult. To address this gap, we constructed the first dedicated detection and tracking dataset for the American football players and conducted a comparative evaluation of various detection and tracking methods. Our results demonstrate that accurate detection and tracking can be achieved even in crowded scenarios. Fine-tuning detection models improved performance over pre-trained models. Furthermore, when these fine-tuned detectors and re-identification models were integrated into tracking systems, we observed notable improvements in tracking accuracy compared to existing approaches. This work thus enables robust detection and tracking of American football players in challenging, high-density scenarios previously underserved by conventional methods.

</details>


### [66] [Revisiting Cross-Architecture Distillation: Adaptive Dual-Teacher Transfer for Lightweight Video Models](https://arxiv.org/abs/2511.09469)
*Ying Peng,Hongsen Ye,Changxin Huang,Xiping Hu,Jian Chen,Runhao Zeng*

Main category: cs.CV

TL;DR: 本论文提出了一种双教师知识蒸馏框架，结合ViT和CNN教师，有效提升轻量级CNN在视频动作识别上的表现，显著超过现有蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: ViT在视频动作识别中表现优异但计算成本高，轻量级CNN虽高效但精度不足。现有跨架构知识蒸馏（CAKD）方法存在架构不匹配和低效利用CNN教师的问题，因此需要更有效地结合ViT和CNN的优势来指导轻量级CNN模型。

Method: 提出Dual-Teacher Knowledge Distillation框架，利用异构ViT教师和同构CNN教师共同指导轻量级CNN学生。创新点包括：1）差异感知教师加权，依据教师信心值和与学生预测结果差异自适应结合教师预测，提升监督信息质量；2）结构差异感知蒸馏，学生通过轻量分支学习ViT与CNN教师残差特征，聚焦可迁移结构差异而非全部模仿高维特征。

Result: 在HMDB51、EPIC-KITCHENS-100和Kinetics-400基准测试中，提出方法在所有数据集上均优于最新知识蒸馏方法，HMDB51上最高精度提升5.95%。

Conclusion: 双教师框架能有效整合两种教师模型优势，减小架构差异对知识迁移的负面影响，显著提升轻量级CNN在视频动作识别任务上的检测精度，是高效的知识蒸馏新途径。

Abstract: Vision Transformers (ViTs) have achieved strong performance in video action recognition, but their high computational cost limits their practicality. Lightweight CNNs are more efficient but suffer from accuracy gaps. Cross-Architecture Knowledge Distillation (CAKD) addresses this by transferring knowledge from ViTs to CNNs, yet existing methods often struggle with architectural mismatch and overlook the value of stronger homogeneous CNN teachers. To tackle these challenges, we propose a Dual-Teacher Knowledge Distillation framework that leverages both a heterogeneous ViT teacher and a homogeneous CNN teacher to collaboratively guide a lightweight CNN student. We introduce two key components: (1) Discrepancy-Aware Teacher Weighting, which dynamically fuses the predictions from ViT and CNN teachers by assigning adaptive weights based on teacher confidence and prediction discrepancy with the student, enabling more informative and effective supervision; and (2) a Structure Discrepancy-Aware Distillation strategy, where the student learns the residual features between ViT and CNN teachers via a lightweight auxiliary branch, focusing on transferable architectural differences without mimicking all of ViT's high-dimensional patterns. Extensive experiments on benchmarks including HMDB51, EPIC-KITCHENS-100, and Kinetics-400 demonstrate that our method consistently outperforms state-of-the-art distillation approaches, achieving notable performance improvements with a maximum accuracy gain of 5.95% on HMDB51.

</details>


### [67] [DreamPose3D: Hallucinative Diffusion with Prompt Learning for 3D Human Pose Estimation](https://arxiv.org/abs/2511.09502)
*Jerrin Bright,Yuhao Chen,John S. Zelek*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的3D人体姿态估计方法DreamPose3D，结合动作感知和时序推理，实现了在多项数据集上的先进效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体姿态估计方法独立预测每帧姿态、仅依赖几何信息，导致难以解决动作歧义、结果时序不连贯，且泛化性弱。本文受人类理解动作的启发，尝试综合时间与动作语义信息，提升估计效果。

Method: 方法上，DreamPose3D利用扩散模型，结合2D姿态序列提取的动作提示动态调节去噪过程，把动作意图融入推理；引入带有运动学关节亲和的编码器，在注意力机制中建模结构关系；训练时用具备‘想象’能力的解码器预测时序连贯的3D序列，模拟人类脑内还原运动过程。

Result: 在Human3.6M与MPI-3DHP两大主流基准上，DreamPose3D各项指标均超越已有方法。此外，在包含干扰和歧义动作的真实棒球广播数据集上同样表现优异，有效保持时序一致和动作语义驱动的效果。

Conclusion: DreamPose3D通过融合动作感知与时序推理，既提高了3D姿态估计的准确性，也展现了对实际复杂运动场景的强鲁棒性和泛化能力。

Abstract: Accurate 3D human pose estimation remains a critical yet unresolved challenge, requiring both temporal coherence across frames and fine-grained modeling of joint relationships. However, most existing methods rely solely on geometric cues and predict each 3D pose independently, which limits their ability to resolve ambiguous motions and generalize to real-world scenarios. Inspired by how humans understand and anticipate motion, we introduce DreamPose3D, a diffusion-based framework that combines action-aware reasoning with temporal imagination for 3D pose estimation. DreamPose3D dynamically conditions the denoising process using task-relevant action prompts extracted from 2D pose sequences, capturing high-level intent. To model the structural relationships between joints effectively, we introduce a representation encoder that incorporates kinematic joint affinity into the attention mechanism. Finally, a hallucinative pose decoder predicts temporally coherent 3D pose sequences during training, simulating how humans mentally reconstruct motion trajectories to resolve ambiguity in perception. Extensive experiments on benchmarked Human3.6M and MPI-3DHP datasets demonstrate state-of-the-art performance across all metrics. To further validate DreamPose3D's robustness, we tested it on a broadcast baseball dataset, where it demonstrated strong performance despite ambiguous and noisy 2D inputs, effectively handling temporal consistency and intent-driven motion variations.

</details>


### [68] [vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs](https://arxiv.org/abs/2511.09540)
*Minye Shao,Sihan Guo,Xinrun Li,Xingyu Miao,Haoran Duan,Yang Long*

Main category: cs.CV

TL;DR: 本文提出了vMFCoOp框架，通过在超球面流形上的von Mises-Fisher分布实现类CLIP视觉语言模型的高效医学领域适配和少样本分类，显著提升了准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的生物医学视觉语言模型（VLM）适配依赖于大语言模型引导的上下文优化，但存在语义失配、可扩展性差及多模态对齐不足等问题，尤其在医学图像中影响少样本适应能力。

Method: 作者提出了vMFCoOp框架，在共享超球面流形上反向估计von Mises-Fisher分布，通过统一语义锚点对任意LLM和CLIP骨干进行对齐。方法结合三种互补约束，提升了多模态统一表示和局部几何约束能力。

Result: 在14个医学数据集、12种医学成像模态和13个解剖区域上，vMFCoOp在准确率、泛化性和临床适用性方面均优于现有主流方法。

Conclusion: vMFCoOp显著缩小了多模态差距，增强了生物医学视觉语言模型在复杂医学场景中的鲁棒性和通用性。该框架有望推广至更多下游医学应用领域。

Abstract: Recent advances in context optimization (CoOp) guided by large language model (LLM)-distilled medical semantic priors offer a scalable alternative to manual prompt engineering and full fine-tuning for adapting biomedical CLIP-based vision-language models (VLMs). However, prompt learning in this context is challenged by semantic misalignment between LLMs and CLIP variants due to divergent training corpora and model architectures; it further lacks scalability across continuously evolving families of foundation models. More critically, pairwise multimodal alignment via conventional Euclidean-space optimization lacks the capacity to model unified representations or apply localized geometric constraints, which tends to amplify modality gaps in complex biomedical imaging and destabilize few-shot adaptation. In this work, we propose vMFCoOp, a framework that inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, aligning semantic biases between arbitrary LLMs and CLIP backbones via Unified Semantic Anchors to achieve robust biomedical prompting and superior few-shot classification. Grounded in three complementary constraints, vMFCoOp demonstrates consistent improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions, outperforming state-of-the-art methods in accuracy, generalization, and clinical applicability. This work will be continuously expanded to encompass more downstream applications, and the corresponding resources are intended to be shared through https://github.com/VinyehShaw/UniEqui.

</details>


### [69] [RF-DETR: Neural Architecture Search for Real-Time Detection Transformers](https://arxiv.org/abs/2511.09554)
*Isaac Robinson,Peter Robicheaux,Matvei Popov,Deva Ramanan,Neehar Peri*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级检测变换器RF-DETR，利用神经架构搜索（NAS）权重共享机制，为不同数据集自动寻找精度-延迟权衡，并实现了优于同类方法的实时检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前开放词汇检测器在常见数据集（如COCO）表现良好，但难以泛化到包含非预训练类别的实际场景。直接对大模型微调需要高成本，缺乏高效的领域自适应能力。

Method: RF-DETR通过权重共享NAS在目标数据集上微调基础网络，然后无需重新训练地评估数千种网络结构，探索精度与延迟之间的最佳平衡，并优化DETR在不同领域的迁移性。

Result: RF-DETR在COCO和Roboflow100-VL两大数据集上，表现显著优于同类实时检测方法。如nano版本在COCO取得48.0 AP，超D-FINE (nano) 5.3分；2x-large版本在Roboflow100-VL超GroundingDINO (tiny) 1.2分，运行速度快20倍。此外RF-DETR (2x-large)是首个COCO实时检测超过60 AP的方法。

Conclusion: RF-DETR实现了高效且快速的检测器设计，在不同目标数据集间具备良好迁移性和准确率—延迟可调性，为实际开放域检测提供了新方案。

Abstract: Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the "tunable knobs" for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves on prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is at https://github.com/roboflow/rf-detr

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [70] [Where did you get that? Towards Summarization Attribution for Analysts](https://arxiv.org/abs/2511.08589)
*Violet B,John M. Conroy,Sean Lynch,Danielle M,Neil P. Molino,Aaron Wiechmann,Julia S. Yang*

Main category: cs.CL

TL;DR: 本文研究如何实现摘要句子与原文内容自动归因，并提出了自动释义抽取型摘要的混合方法。


<details>
  <summary>Details</summary>
Motivation: 分析师在汇报工作时需要明确信息来源，因此自动归因方法能提升信息溯源的便捷性和准确性。

Method: 采用混合型摘要方式，即对抽取式摘要自动进行释义处理，同时设计定制化拓扑结构分析相关归因错误类型的比例。

Result: 通过实验验证了混合型摘要与定制拓扑相结合，能够更有效地实现每句摘要与原文内容的链接，并量化了各种归因相关的错误比例。

Conclusion: 混合自动摘要和定制化分析方法可提升自动归因的准确性和可操作性，为信息溯源提供了有效工具。

Abstract: Analysts require attribution, as nothing can be reported without knowing the source of the information. In this paper, we will focus on automatic methods for attribution, linking each sentence in the summary to a portion of the source text, which may be in one or more documents. We explore using a hybrid summarization, i.e., an automatic paraphrase of an extractive summary, to ease attribution. We also use a custom topology to identify the proportion of different categories of attribution-related errors.

</details>


### [71] [GMTRouter: Personalized LLM Router over Multi-turn User Interactions](https://arxiv.org/abs/2511.08590)
*Encheng Xie,Yihang Sun,Tao Feng,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文提出了一种名为GMTRouter的方法，通过将用户与大语言模型（LLM）的多轮交互建模为异构图，并捕捉复杂互动关系，实现了更加个性化的LLM路由选择，显著优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 个性化LLM路由可以在保证响应质量的同时进一步降低计算成本，但用户偏好多样且数据稀疏、噪声大，现有方法难以充分建模用户与LLM之间的复杂交互，导致个性化效果有限。

Method: 作者提出将用户、LLM、查询与响应建模成四类节点的异构图，并利用定制化的消息传递机制进行图学习，在轻量的归纳框架下，从少量样本中学习用户偏好，实现对新用户和演化偏好的快速适应。

Result: 大量实验显示，GMTRouter在多个数据集上相较于强基线模型，准确率提升0.9-21.6%，AUC提升0.006-0.309，可通过少量样本自适应新用户和偏好变化。

Conclusion: GMTRouter有效提升了个性化LLM路由的性能，能够从有限数据中挖掘复杂用户偏好，在实际应用中具有广阔前景。

Abstract: Large Language Model (LLM) routing has demonstrated strong capability in balancing response quality with computational cost. As users exhibit diverse preferences, personalization has attracted increasing attention in LLM routing, since even identical queries may require different models to generate responses tailored to individual needs. However, existing approaches are not fully personalized and often fail to capture the complex interactions between specific users and LLMs. Moreover, user preference data is typically scarce, noisy, and inconsistent in format, which limits the effectiveness of methods that rely solely on user-specific data. To address these challenges, we propose GMTRouter, which represents multi-turn user-LLM interactions as a heterogeneous graph with four node types: user, LLM, query, and response, thereby preserving the rich relational structure of the interaction. Through a tailored message-passing mechanism, GMTRouter learns to capture user preferences from few-shot data within a lightweight inductive graph learning framework, enabling effective personalization. Extensive experiments demonstrate that GMTRouter consistently outperforms strong baselines, achieving 0.9 to 21.6 percent higher accuracy and 0.006 to 0.309 higher AUC across multiple datasets. More importantly, we demonstrate that GMTRouter can adapt to new users and evolving preferences using only few-shot data, without extensive fine-tuning. The code for GMTRouter is publicly available at https://github.com/ulab-uiuc/GMTRouter.

</details>


### [72] [The Collective Turing Test: Large Language Models Can Generate Realistic Multi-User Discussions](https://arxiv.org/abs/2511.08592)
*Azza Bouleimen,Giordano De Marzo,Taehee Kim,Nicol`o Pagan,Hannah Metzler,Silvia Giordano,David Garcia*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）模拟社交媒体群体对话的能力，发现生成的对话有较高迷惑性，不易被人类识别。


<details>
  <summary>Details</summary>
Motivation: LLMs被认为有潜力用于模拟在线社群、测试内容推荐算法及政策干预效果，但目前尚缺实证研究验证LLMs生成的对话是否足够逼真，能否被人类辨识。作者希望评估LLMs模拟群体对话的有效性与风险。

Method: 作者选取Reddit上的真实人类群体对话，并让Llama 3 70B与GPT-4o两个LLMs针对同主题生成人工对话。参与者需在并列展示的人类与AI生成对话中分辨其来源。统计分辨正误率来评估LLMs的迷惑性。

Result: 总体上，参与者有39%的几率将AI生成的对话误认为是人类。特别是Llama 3生成的对话，只有56%的参与者能够正确辨认，几乎和随机猜测无异。

Conclusion: LLMs能够生成难以区分的社交媒体对话。这显示LLMs具备有力的社群模拟潜力，也同时提出警告：LLMs易被滥用生成虚假社交内容，需警惕相关风险。

Abstract: Large Language Models (LLMs) offer new avenues to simulate online communities and social media. Potential applications range from testing the design of content recommendation algorithms to estimating the effects of content policies and interventions. However, the validity of using LLMs to simulate conversations between various users remains largely untested. We evaluated whether LLMs can convincingly mimic human group conversations on social media. We collected authentic human conversations from Reddit and generated artificial conversations on the same topic with two LLMs: Llama 3 70B and GPT-4o. When presented side-by-side to study participants, LLM-generated conversations were mistaken for human-created content 39\% of the time. In particular, when evaluating conversations generated by Llama 3, participants correctly identified them as AI-generated only 56\% of the time, barely better than random chance. Our study demonstrates that LLMs can generate social media conversations sufficiently realistic to deceive humans when reading them, highlighting both a promising potential for social simulation and a warning message about the potential misuse of LLMs to generate new inauthentic social media content.

</details>


### [73] [Knowledge Graph Analysis of Legal Understanding and Violations in LLMs](https://arxiv.org/abs/2511.08593)
*Abha Jha,Abel Salinas,Fred Morstatter*

Main category: cs.CL

TL;DR: 本论文探讨了大语言模型（LLMs）在解析复杂法律框架（如美国法典第18编第175条生物武器法）时的优势与安全隐患，提出结合知识图谱与检索增强生成（RAG）的方法，系统评估LLMs在法律理解及潜在风险行为中的表现，并据此提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs能力提升，它们能在法律领域特别是敏感高危领域（如生物武器法律）进行复杂分析，但与此同时也可能被用来生成危险、违法内容。因此，需研究如何提升LLMs识别非法行为、防止违规输出的能力。

Method: 作者提出结合知识图谱构建与RAG机制，设计结构化实验，评估LLMs对相关法律的理解、非法操作指令生成及法定犯罪意图识别方面的表现。

Result: 实验表明LLMs在法律推理和安全防护上存在明显局限，可能生成有害内容，对违法意图识别不完善。

Conclusion: 论文指出LLMs当前无法完全保证安全与合法性，需通过加强安全协议和法律推理框架来优化其在敏感法律领域的应用，使其成为法律守护者而非违规行为的助推者。

Abstract: The rise of Large Language Models (LLMs) offers transformative potential for interpreting complex legal frameworks, such as Title 18 Section 175 of the US Code, which governs biological weapons. These systems hold promise for advancing legal analysis and compliance monitoring in sensitive domains. However, this capability comes with a troubling contradiction: while LLMs can analyze and interpret laws, they also demonstrate alarming vulnerabilities in generating unsafe outputs, such as actionable steps for bioweapon creation, despite their safeguards. To address this challenge, we propose a methodology that integrates knowledge graph construction with Retrieval-Augmented Generation (RAG) to systematically evaluate LLMs' understanding of this law, their capacity to assess legal intent (mens rea), and their potential for unsafe applications. Through structured experiments, we assess their accuracy in identifying legal violations, generating prohibited instructions, and detecting unlawful intent in bioweapons-related scenarios. Our findings reveal significant limitations in LLMs' reasoning and safety mechanisms, but they also point the way forward. By combining enhanced safety protocols with more robust legal reasoning frameworks, this research lays the groundwork for developing LLMs that can ethically and securely assist in sensitive legal domains - ensuring they act as protectors of the law rather than inadvertent enablers of its violation.

</details>


### [74] [Diverse Preference Learning for Capabilities and Alignment](https://arxiv.org/abs/2511.08594)
*Stewart Slocum,Asher Parker-Sartori,Dylan Hadfield-Menell*

Main category: cs.CL

TL;DR: 本文指出现有LLM（大型语言模型）的对齐方法如RLHF与DPO导致其输出多样性显著下降。核心原因是偏好学习算法中的KL散度正则项过于偏重主流观点。作者提出了Soft Preference Learning方法，通过解耦KL项细控多样性，提升了输出的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在社会中的影响力日益增加，其展现多元视角的能力变得至关重要。但主流对齐算法减弱了这一多样性能力。解决这一问题有助于模型更全面地反映社会观点。

Method: 提出Soft Preference Learning，在偏好学习时对KL正则项中的熵和交叉熵部分进行解耦，以实现对输出多样性的更精细调控。与传统温度缩放类似，但取得了帕累托改进。

Result: 采用Soft Preference Learning训练后的LLM在复杂重复采样任务中表现出更高准确度，同时输出具有更高语义与词汇多样性。此外，从对齐角度看，模型能覆盖更广的社会观点，并提升了logit校准表现。

Conclusion: Soft Preference Learning方法能有效缓解现有LLM对齐算法带来的多样性损失问题，在提升模型能力和社会视角覆盖度方面取得显著改进，是温度缩放的一种优势升级方案。

Abstract: The ability of LLMs to represent diverse perspectives is critical as they increasingly impact society. However, recent studies reveal that alignment algorithms such as RLHF and DPO significantly reduce the diversity of LLM outputs. Not only do aligned LLMs generate text with repetitive structure and word choice, they also approach problems in more uniform ways, and their responses reflect a narrower range of societal perspectives. We attribute this problem to the KL divergence regularizer employed in preference learning algorithms. This causes the model to systematically overweight majority opinions and sacrifice diversity in its outputs. To address this, we propose Soft Preference Learning, which decouples the entropy and cross-entropy terms in the KL penalty - allowing for fine-grained control over LLM generation diversity. From a capabilities perspective, LLMs trained using Soft Preference Learning attain higher accuracy on difficult repeated sampling tasks and produce outputs with greater semantic and lexical diversity. From an alignment perspective, they are capable of representing a wider range of societal viewpoints and display improved logit calibration. Notably, Soft Preference Learning resembles, but is a Pareto improvement over, standard temperature scaling.

</details>


### [75] [Chopping Trees: Semantic Similarity Based Dynamic Pruning for Tree-of-Thought Reasoning](https://arxiv.org/abs/2511.08595)
*Joongho Kim,Xirui Huang,Zarreen Reza,Gabriel Grand,Kevin Zhu,Ryan Lagasse*

Main category: cs.CL

TL;DR: 本文提出了一种名为SSDP（基于语义相似度的动态剪枝）的方法，有效提升了大语言模型在Tree-of-Thought推理中的效率，并显著减少了冗余工作量。


<details>
  <summary>Details</summary>
Motivation: Tree-of-Thought推理能够增强大模型的推理能力，但存在计算资源消耗大且语义冗余严重的问题，导致低效。

Method: 作者提出SSDP算法，将在线语义合并机制引入并行树搜索过程中，实现对冗余推理路径的聚类和实时剪枝，大幅减少重复节点的探索。

Result: 在多个推理基准（如GSM8K和MATH500）上，SSDP相较同类最新方法最高实现了2.3倍加速，在准确率基本持平（通常与最强基线差距5%以内）的情况下，探索节点数量减少85-90%。

Conclusion: SSDP是一个高效、可扩展的大模型推理优化方法，能显著减少资源消耗，并保持优秀准确率，具有实际应用价值。

Abstract: Tree-of-Thought (ToT) reasoning boosts the problem-solving abilities of Large Language Models (LLMs) but is computationally expensive due to semantic redundancy, where distinct branches explore equivalent reasoning paths. We introduce Semantic Similarity-Based Dynamic Pruning (SSDP), a lightweight method that, to the best of our knowledge, is the first framework to integrate online semantic merging into parallelized tree search, enabling the clustering and pruning of redundant steps in real time. Across reasoning benchmarks, including GSM8K and MATH500, SSDP achieves up to a 2.3x speedup over state-of-the-art tree-search baselines while maintaining competitive accuracy (typically within 5% of the strongest baseline) and reducing the number of explored nodes by 85-90%, demonstrating a practical approach to efficient, scalable LLM reasoning. The implementation of SSDP is publicly available at https://github.com/kimjoonghokim/SSDP.

</details>


### [76] [What About the Scene with the Hitler Reference? HAUNT: A Framework to Probe LLMs' Self-consistency Via Adversarial Nudge](https://arxiv.org/abs/2511.08596)
*Arka Dutta,Sujan Dutta,Rijul Magu,Soumyajit Datta,Munmun De Choudhury,Ashiqur R. KhudaBukhsh*

Main category: cs.CL

TL;DR: 本文提出了一种评估大语言模型在事实一致性与抗干扰能力上的测试框架，比较了多个主流模型的表现，发现其抗'幻觉'（错误信息）能力差异较大。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的广泛应用，它们在高风险领域的“幻觉”现象（即生成虚假或错误信息）已成为关键难题。研究者希望更系统、更严格地评估这些模型抵抗误导性输入、维持事实准确性的能力。

Method: 提出三步测试框架：1）引导LLM在封闭领域内生成真假信息；2）要求LLM验证同一组陈述的真假性；3）用模型自身生成和验证的谎言对其进行鲁棒性测试。实验选取了五个主流专有LLM，在电影和小说两个闭集领域内测试。

Result: 实验结果显示，不同模型在对抗误导性输入时表现有明显差异：Claude表现最佳，GPT和Grok中等，Gemini和DeepSeek抗干扰能力较弱。

Conclusion: 针对大语言模型的信息查询应用日益增长，本文发现主流LLM在事实一致性上的鲁棒性参差不齐，这对于现实应用场景具有警示意义。

Abstract: Hallucinations pose a critical challenge to the real-world deployment of large language models (LLMs) in high-stakes domains. In this paper, we present a framework for stress testing factual fidelity in LLMs in the presence of adversarial nudge. Our framework consists of three steps. In the first step, we instruct the LLM to produce sets of truths and lies consistent with the closed domain in question. In the next step, we instruct the LLM to verify the same set of assertions as truths and lies consistent with the same closed domain. In the final step, we test the robustness of the LLM against the lies generated (and verified) by itself. Our extensive evaluation, conducted using five widely known proprietary LLMs across two closed domains of popular movies and novels, reveals a wide range of susceptibility to adversarial nudges: \texttt{Claude} exhibits strong resilience, \texttt{GPT} and \texttt{Grok} demonstrate moderate resilience, while \texttt{Gemini} and \texttt{DeepSeek} show weak resilience. Considering that a large population is increasingly using LLMs for information seeking, our findings raise alarm.

</details>


### [77] [Self-HarmLLM: Can Large Language Model Harm Itself?](https://arxiv.org/abs/2511.08597)
*Heehwan Kim,Sungjune Park,Daeseon Choi*

Main category: cs.CL

TL;DR: 本文提出了一种新的攻击场景：大语言模型（LLM）自身生成的模糊有害查询（MHQ）可被重新输入模型，从而绕过原有的安全防护，实现越狱。


<details>
  <summary>Details</summary>
Motivation: 以往的防护措施仅针对外部攻击者，忽视了模型自身输出内容可能成为新的攻击向量，因此需要探讨模型自我生成有害询问带来的安全隐患。

Method: 作者提出Self-HarmLLM场景，即利用LLM自身生成的MHQ（意图未变但有害本质未直接暴露的模糊询问），将其作为新输入，测试该方法在不同模型、不同提示条件下的越狱成功率，并结合自动和人工评估结果。

Result: 在Zero-shot条件下MHQ成功转换率最高达52%，越狱成功率最高33%；Few-shot条件下分别提升至65%和41%。自动评估系统系统性高估了越狱成功率，比人工评估高出平均52%。

Conclusion: 即使是在有限的玩具示例与评价下，该攻击方法依然可行，并暴露出现有防护方案的不足，提示需重新设计更健壮的安全防护与评估方法。

Abstract: Large Language Models (LLMs) are generally equipped with guardrails to block the generation of harmful responses. However, existing defenses always assume that an external attacker crafts the harmful query, and the possibility of a model's own output becoming a new attack vector has not been sufficiently explored. In this study, we propose the Self-HarmLLM scenario, which uses a Mitigated Harmful Query (MHQ) generated by the same model as a new input. An MHQ is an ambiguous query whose original intent is preserved while its harmful nature is not directly exposed. We verified whether a jailbreak occurs when this MHQ is re-entered into a separate session of the same model. We conducted experiments on GPT-3.5-turbo, LLaMA3-8B-instruct, and DeepSeek-R1-Distill-Qwen-7B under Base, Zero-shot, and Few-shot conditions. The results showed up to 52% transformation success rate and up to 33% jailbreak success rate in the Zero-shot condition, and up to 65% transformation success rate and up to 41% jailbreak success rate in the Few-shot condition. By performing both prefix-based automated evaluation and human evaluation, we found that the automated evaluation consistently overestimated jailbreak success, with an average difference of 52%. This indicates that automated evaluation alone is not accurate for determining harmfulness. While this study is a toy-level study based on a limited query set and evaluators, it proves that our method can still be a valid attack scenario. These results suggest the need for a fundamental reconsideration of guardrail design and the establishment of a more robust evaluation methodology.

</details>


### [78] [OKBench: Democratizing LLM Evaluation with Fully Automated, On-Demand, Open Knowledge Benchmarking](https://arxiv.org/abs/2511.08598)
*Yanhong Li,Tianyang Xu,Kenan Tang,Karen Livescu,David McAllester,Jiawei Zhou*

Main category: cs.CL

TL;DR: 本论文提出OKBench，一个用于动态生成高质量知识性问答基准的全自动框架，主要针对新闻等知识不断更新的领域，评估不同大模型在新知识前的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的知识性问答评测主要依赖静态基准（如Wikipedia、教科书），难以反映知识的动态变化，且集中式的基准制作方式难以跟上大模型快速发展的步伐。

Method: 提出OKBench框架，实现从新闻等实时信息中自动采集、生成、验证并分发问答基准，实现了基准创建的自动化与民主化，并可动态反映知识变迁，减少与模型预训练数据的重叠。

Result: 在多种开源和闭源大模型上，评测OKBench自动生成的新闻领域最新知识题目，结果显示模型在新知识面前的表现存在显著差异，通过检索增强可以缩小大小模型之间的性能差距。

Conclusion: 随着知识快速变化，动态问答基准对于大模型评测极为重要，OKBench为社区提供了自动化且及时的解决方案，有助于更公平、全面地评价不同LLM的实际能力。

Abstract: Knowledge-intensive question answering is central to large language models (LLMs) and is typically assessed using static benchmarks derived from sources like Wikipedia and textbooks. However, these benchmarks fail to capture evolving knowledge in a dynamic world, and centralized curation struggles to keep pace with rapid LLM advancements. To address these drawbacks, we propose Open Knowledge Bench (OKBench), a fully automated framework for generating high-quality, dynamic knowledge benchmarks on demand. Focusing on the news domain where knowledge updates daily, OKBench is an agentic framework that automates the sourcing, creation, validation, and distribution of benchmarks. Our approach democratizes benchmark creation and facilitates thorough evaluation of retrieval-augmented methods by reducing overlap with pretraining data. We evaluate our framework on a wide range open-source and proprietary LLMs of various sizes and configurations, both with and without retrieval over freshly generated knowledge. Our results reveal distinct model behaviors when confronted with new information and highlight how retrieval narrows the performance gap between small and large models. These findings underscore the importance of evaluating LLMs on evolving knowledge benchmarks.

</details>


### [79] [Retrieval-Augmented Generation of Pediatric Speech-Language Pathology vignettes: A Proof-of-Concept Study](https://arxiv.org/abs/2511.08600)
*Yilan Liu*

Main category: cs.CL

TL;DR: 本文提出并验证了一种结合检索增强生成（RAG）与知识库，自动生成儿童言语语言病理学（SLP）案例材料的系统，可以减轻人工编写负担，所生成内容质量良好。


<details>
  <summary>Details</summary>
Motivation: SLP领域临床案例编写耗时繁琐，通用大模型的内容又不够专业且易产生错误（幻觉），需专家大量修订，因此亟需结合领域知识提高自动生成内容的专业性和实用性。

Method: 设计并实现了一套结合知识库和模板的RAG系统，支持5种主流商用及开源大模型，在涉及多种障碍和年级层次的7个儿童SLP案例场景下生成案例，并用多维度自动评分标准进行质量评估。

Result: RAG系统可生成结构完整、逻辑一致、专业符合规范的SLP案例。商用大模型略优于开源模型，但后者也达到可接受水平，具有数据隐私保护的部署优势。

Conclusion: RAG与知识库结合能高质量自动生成SLP案例，适用于教育与研究。应用推广需进一步专家、学生评测和心理计量学验证，未来可拓展至临床决策支持等更多场景。

Abstract: Clinical vignettes are essential educational tools in speech-language pathology (SLP), but manual creation is time-intensive. While general-purpose large language models (LLMs) can generate text, they lack domain-specific knowledge, leading to hallucinations and requiring extensive expert revision. This study presents a proof-of-concept system integrating retrieval-augmented generation (RAG) with curated knowledge bases to generate pediatric SLP case materials. A multi-model RAG-based system was prototyped integrating curated domain knowledge with engineered prompt templates, supporting five commercial (GPT-4o, Claude 3.5 Sonnet, Gemini 2.5 Pro) and open-source (Llama 3.2, Qwen 2.5-7B) LLMs. Seven test scenarios spanning diverse disorder types and grade levels were systematically designed. Generated cases underwent automated quality assessment using a multi-dimensional rubric evaluating structural completeness, internal consistency, clinical appropriateness, and IEP goal/session note quality. This proof-of-concept demonstrates technical feasibility for RAG-augmented generation of pediatric SLP vignettes. Commercial models showed marginal quality advantages, but open-source alternatives achieved acceptable performance, suggesting potential for privacy-preserving institutional deployment. Integration of curated knowledge bases enabled content generation aligned with professional guidelines. Extensive validation through expert review, student pilot testing, and psychometric evaluation is required before educational or research implementation. Future applications may extend to clinical decision support, automated IEP goal generation, and clinical reflection training.

</details>


### [80] [Evaluating DisCoCirc in Translation Tasks & its Limitations: A Comparative Study Between Bengali & English](https://arxiv.org/abs/2511.08601)
*Nazmoon Falgunee Moon*

Main category: cs.CL

TL;DR: 本文将DisCoCirc语法框架从英语扩展到孟加拉语，应用于英孟翻译任务，发现该框架在结构不同的语言间存在局限性。


<details>
  <summary>Details</summary>
Motivation: 受之前研究将DisCoCirc应用于英语的启发，作者试图评估该框架扩展到孟加拉语及其在减少语言繁琐性方面的有效性。

Method: 作者将DisCoCirc框架迁移至孟加拉语，通过英孟翻译实例评估该方法的实际表现，并对英语连词的布尔逻辑性质进行补充讨论。

Result: 虽然DisCoCirc能涵盖大部分语言结构，但在处理英孟结构差异与简单句时依然表现不佳，不如先前研究所述。

Conclusion: DisCoCirc框架在英孟翻译任务中存在显著局限，表明该框架需进一步优化，未来可探索针对结构差异的改进方案。

Abstract: In [4], the authors present the DisCoCirc (Distributed Compositional Circuits) formalism for the English language, a grammar-based framework derived from the production rules that incorporates circuit-like representations in order to give a precise categorical theoretical structure to the language. In this paper, we extend this approach to develop a similar framework for Bengali and apply it to translation tasks between English and Bengali. A central focus of our work lies in reassessing the effectiveness of DisCoCirc in reducing language bureaucracy. Unlike the result suggested in [5], our findings indicate that although it works well for a large part of the language, it still faces limitations due to the structural variation of the two languages. We discuss the possible methods that might handle these shortcomings and show that, in practice, DisCoCirc still struggles even with relatively simple sentences. This divergence from prior claims not only highlights the framework's constraints in translation but also suggest scope for future improvement. Apart from our primary focus on English-Bengali translation, we also take a short detour to examine English conjunctions, following [1], showing a connection between conjunctions and Boolean logic.

</details>


### [81] [Mina: A Multilingual LLM-Powered Legal Assistant Agent for Bangladesh for Empowering Access to Justice](https://arxiv.org/abs/2511.08605)
*Azmine Toushik Wasi,Wahid Faisal,Mst Rafia Islam*

Main category: cs.CL

TL;DR: 本论文介绍了Mina，一个为孟加拉贫困群体提供法律咨询的多语言大模型法律助理系统，其性能接近或超过普通人类考生，有效降低法律服务门槛。


<details>
  <summary>Details</summary>
Motivation: 孟加拉低收入群体因法律语言复杂、流程不透明和高昂费用，难以获得负担得起的法律咨询；现有AI法律助理缺乏孟加拉语和特定司法环境支持，效果有限。

Method: 开发Mina，基于多语言大模型（LLM），结合多语言嵌入和RAG（检索增强生成）链式工具，集成检索、推理、翻译和文档自动生成，支持互动聊天，提供本地化法案起草、引用文献和平易近人的解释。

Result: 由孟加拉顶尖法学院教师评估，Mina在2022及2023年孟加拉律师资格考试各阶段（MCQ、笔试、口试模拟）得分达75-80%，与甚至超过人类平均水平，表现出优秀的法律推理、上下文理解和清晰表述。

Conclusion: Mina可作为低成本多语言AI法律助理，有效自动化法律任务，提升司法可及性。该系统为低资源、多语种、面向公共服务的AI落地和适配带来了有价值的实证案例。

Abstract: Bangladesh's low-income population faces major barriers to affordable legal advice due to complex legal language, procedural opacity, and high costs. Existing AI legal assistants lack Bengali-language support and jurisdiction-specific adaptation, limiting their effectiveness. To address this, we developed Mina, a multilingual LLM-based legal assistant tailored for the Bangladeshi context. It employs multilingual embeddings and a RAG-based chain-of-tools framework for retrieval, reasoning, translation, and document generation, delivering context-aware legal drafts, citations, and plain-language explanations via an interactive chat interface. Evaluated by law faculty from leading Bangladeshi universities across all stages of the 2022 and 2023 Bangladesh Bar Council Exams, Mina scored 75-80% in Preliminary MCQs, Written, and simulated Viva Voce exams, matching or surpassing average human performance and demonstrating clarity, contextual understanding, and sound legal reasoning. These results confirm its potential as a low-cost, multilingual AI assistant that automates key legal tasks and scales access to justice, offering a real-world case study on building domain-specific, low-resource systems and addressing challenges of multilingual adaptation, efficiency, and sustainable public-service AI deployment.

</details>


### [82] [A Super-Learner with Large Language Models for Medical Emergency Advising](https://arxiv.org/abs/2511.08614)
*Sergey K. Aityan,Abdolreza Mosaddegh,Rolando Herrero,Haitham Tayyar,Jiang Han,Vikram Sawant,Qi Chen,Rishabh Jain,Aruna Senthamaraikannan,Stephen Wood,Manuel Mersini,Rita Lazzaro,Mario Balzaneli,Nicola Iacovazzo,Ciro Gargiulo Isacco*

Main category: cs.CL

TL;DR: 本论文评估了多个大型语言模型（LLM）在急诊医疗决策支持领域的诊断能力，并提出通过元学习聚合多模型可提升准确率。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术发展，LLM在医疗决策支持系统中的应用越来越广泛，但各自的诊断能力参差不齐。研究动机在于提升急诊医学中的诊断准确率。

Method: 作者对五个主流LLM（Gemini、Llama、Grok、GPT和Claude）在真实急诊案例中的表现进行对比，并构建“超学习者”MEDAS系统，将这些LLM用元学习方法集成，通过集群方式优化整体诊断效果。

Result: 五个主流LLM诊断准确率为58%-65%，明显高于人类医生水平。MEDAS超学习者系统整体准确率提升至70%，且至少有单一LLM在集成中可达85%。

Conclusion: 通过元学习将多个LLM集成，可以有效提升诊断准确率，超越任何单一模型，表明集成模型能够最大化利用各模型已学医疗知识。

Abstract: Medical decision-support and advising systems are critical for emergency physicians to quickly and accurately assess patients' conditions and make diagnosis. Artificial Intelligence (AI) has emerged as a transformative force in healthcare in recent years and Large Language Models (LLMs) have been employed in various fields of medical decision-support systems. We studied responses of a group of different LLMs to real cases in emergency medicine. The results of our study on five most renown LLMs showed significant differences in capabilities of Large Language Models for diagnostics acute diseases in medical emergencies with accuracy ranging between 58% and 65%. This accuracy significantly exceeds the reported accuracy of human doctors. We built a super-learner MEDAS (Medical Emergency Diagnostic Advising System) of five major LLMs - Gemini, Llama, Grok, GPT, and Claude). The super-learner produces higher diagnostic accuracy, 70%, even with a quite basic meta-learner. However, at least one of the integrated LLMs in the same super-learner produces 85% correct diagnoses. The super-learner integrates a cluster of LLMs using a meta-learner capable of learning different capabilities of each LLM to leverage diagnostic accuracy of the model by collective capabilities of all LLMs in the cluster. The results of our study showed that aggregated diagnostic accuracy provided by a meta-learning approach exceeds that of any individual LLM, suggesting that the super-learner can take advantage of the combined knowledge of the medical datasets used to train the group of LLMs.

</details>


### [83] [Learn More, Forget Less: A Gradient-Aware Data Selection Approach for LLM](https://arxiv.org/abs/2511.08620)
*Yibai Liu,Shihang Wang,Zeming Liu,Zheming Song,Junzhe Wang,Jingjing Liu,Qingjie Liu,Yunhong Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为GrADS的自适应、基于梯度感知的数据选择方法，旨在提升大语言模型SFT时的效率，减少数据用量，同时缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要通过SFT适应专业领域，但SFT往往成本高昂且易导致灾难性遗忘，从而影响模型的通用能力。

Method: 提出GrADS方法，通过预训练阶段分析梯度信息，利用梯度的大小和分布筛选出对学习最有贡献的训练样本，优先对这些样本进行SFT。

Result: 在医学、法律、金融等多个领域和不同LLM实验中，使用GrADS筛选过的数据只需5%即可超越全数据SFT的性能，50%数据时提升显著，并显著缓解灾难性遗忘。

Conclusion: GrADS极大提高了SFT效率和模型能力，降低了计算和数据成本，同时减少了灾难性遗忘，为专业领域快速高效适配LLM提供了可行方案。

Abstract: Despite large language models (LLMs) have achieved impressive achievements across numerous tasks, supervised fine-tuning (SFT) remains essential for adapting these models to specialized domains. However, SFT for domain specialization can be resource-intensive and sometimes leads to a deterioration in performance over general capabilities due to catastrophic forgetting (CF). To address these issues, we propose a self-adaptive gradient-aware data selection approach (GrADS) for supervised fine-tuning of LLMs, which identifies effective subsets of training data by analyzing gradients obtained from a preliminary training phase. Specifically, we design self-guided criteria that leverage the magnitude and statistical distribution of gradients to prioritize examples that contribute the most to the model's learning process. This approach enables the acquisition of representative samples that enhance LLMs understanding of domain-specific tasks. Through extensive experimentation with various LLMs across diverse domains such as medicine, law, and finance, GrADS has demonstrated significant efficiency and cost-effectiveness. Remarkably, utilizing merely 5% of the selected GrADS data, LLMs already surpass the performance of those fine-tuned on the entire dataset, and increasing to 50% of the data results in significant improvements! With catastrophic forgetting substantially mitigated simultaneously. We will release our code for GrADS later.

</details>


### [84] [Detecting Suicidal Ideation in Text with Interpretable Deep Learning: A CNN-BiGRU with Attention Mechanism](https://arxiv.org/abs/2511.08636)
*Mohaiminul Islam Bhuiyan,Nur Shazwani Kamarudin,Nur Hafieza Ismail*

Main category: cs.CL

TL;DR: 本文提出了一种结合CNN与BiGRU的混合深度学习框架，通过对社交媒体数据集中青少年的自杀意图进行准确识别，并利用SHAP方法提升模型可解释性。实验结果显示该方法在公开数据集上准确率达93.97%，优于现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 自杀是全球青少年第二大死亡原因，既往自杀尝试是未来自杀风险的重要预测因子。社交媒体成为部分有自杀念头者表达意图的平台，需要准确的自动识别方法来及时干预和预防。

Method: 提出将CNN用于局部特征提取，BiGRU用于序列建模，并结合注意机制与SHAP可解释性分析，对社交网络中的文本数据进行自杀意图识别。

Result: 在公开数据集上，所提方法达到了93.97%的准确率，并在与当前最先进的机器学习及深度学习模型对比中表现出更高的性能。

Conclusion: 该研究提出的混合深度学习与可解释性集成模型在自杀意图检测方面准确且可靠，优于现有模型，具备更好的实际应用前景。

Abstract: Worldwide, suicide is the second leading cause of death for adolescents with past suicide attempts to be an important predictor for increased future suicides. While some people with suicidal thoughts may try to suppress them, many signal their intentions in social media platforms. To address these issues, we propose a new type of hybrid deep learning scheme, i.e., the combination of a CNN architecture and a BiGRU technique, which can accurately identify the patterns of suicidal ideation from SN datasets. Also, we apply Explainable AI methods using SHapley Additive exPlanations to interpret the prediction results and verifying the model reliability. This integration of CNN local feature extraction, BiGRU bidirectional sequence modeling, attention mechanisms, and SHAP interpretability provides a comprehensive framework for suicide detection. Training and evaluation of the system were performed on a publicly available dataset. Several performance metrics were used for evaluating model performance. Our method was found to have achieved 93.97 accuracy in experimental results. Comparative study to different state-of-the-art Machine Learning and DL models and existing literature demonstrates the superiority of our proposed technique over all the competing methods.

</details>


### [85] [Structured Uncertainty guided Clarification for LLM Agents](https://arxiv.org/abs/2511.08798)
*Manan Suri,Puneet Mathur,Nedim Lipka,Franck Dernoncourt,Ryan A. Rossi,Dinesh Manocha*

Main category: cs.CL

TL;DR: 本文提出了一种融合结构化不确定性的LLM工具调用代理SAGE-Agent，以及评测基准ClarifyBench，显著提升了在歧义任务下的成功率及效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在接到用户模糊指令时，容易出现工具参数调用错误，导致任务失败。迫切需要高效且系统性地处理不确定性，优化澄清提问与任务完成效果。

Method: 作者将工具参数澄清建模为POMDP，并以EVPI为目标选择最优澄清问题，引入基于方面的成本建模以避免冗余。提出SAGE-Agent利用结构化不确定性提升澄清效率。建立了ClarifyBench多轮歧义基准，覆盖多个真实场景，并在强化学习中作用为训练信号。

Result: SAGE-Agent在处理歧义任务时，覆盖率比强基线提升7-39%，澄清问题数量减少1.5-2.7倍。结构化不确定性显著提升When2Call任务准确率（3B模型从36.5%提升到65.2%，7B模型从36.7%提升至62.9%）。

Conclusion: 结构化不确定性为工具增强型智能体带来了理论和应用上的优化，可提升真实场景下任务成功率与交互效率。

Abstract: LLM agents extend large language models with tool-calling capabilities, but ambiguous user instructions often lead to incorrect invocations and task failures. We introduce a principled formulation of structured uncertainty over tool-call parameters, modeling joint tool-argument clarification as a POMDP with Expected Value of Perfect Information (EVPI) objective for optimal question selection and aspect-based cost modeling to prevent redundancy. Our SAGE-Agent leverages this structured uncertainty to achieve superior efficiency: increasing coverage on ambiguous tasks by 7-39\% while reducing clarification questions by 1.5-2.7$\times$ compared to strong prompting and uncertainty-based baselines. We present ClarifyBench, the first multi-turn tool-augmented disambiguation benchmark with realistic LLM-based user simulation across diverse domains including document editing, vehicle control, and travel booking. Additionally, we demonstrate that structured uncertainty provides effective training signals for reinforcement learning, boosting When2Call accuracy from 36.5\% to 65.2\% (3B model) and 36.7\% to 62.9\% (7B model) through uncertainty-weighted GRPO training. These results establish structured uncertainty as a principled, efficient approach for tool-augmented agents, improving both task success and interaction efficiency in real-world scenarios.

</details>


### [86] [Toward Automated Cognitive Assessment in Parkinson's Disease Using Pretrained Language Models](https://arxiv.org/abs/2511.08806)
*Varada Khanna,Nilay Bhatt,Ikgyu Shin,Sule Tinaz,Yang Ren,Hua Xu,Vipina K. Keloth*

Main category: cs.CL

TL;DR: 本研究开发并评估了多种NLP模型，从帕金森病(PD)患者的第一人称叙述中自动识别不同认知过程类别。结果显示模型对不同类别及任务难度存在较大差异，Meta-Llama-3-8B-Instruct模型总体表现最佳。


<details>
  <summary>Details</summary>
Motivation: 理解帕金森病患者在日常生活中描述的认知体验，有助于深入掌握与疾病相关的认知和情绪变化，但从非结构化叙述中提取这些信息极具挑战性。

Method: 作者利用去标识化的患者叙述，开发并评估三类NLP模型，包括基于Bio_ClinicalBERT的嵌套实体识别模型、使用QLoRA微调的Meta-Llama-3-8B-Instruct指令跟随模型，以及GPT-4o mini在零-shot和少-shot下的表现。比较了它们在提取七类认知相关信息的能力。

Result: 模型在不同类别和模型家族间表现差异明显。微调后的Meta-Llama-3-8B-Instruct总体F1分数最高，在上下文依赖类别（如思维和社交互动）上表现尤为突出。Bio_ClinicalBERT精准率高但召回率低，仅在少数类别上表现良好。

Conclusion: 与传统信息抽取任务相比，该任务难度更高，但持续优化后，NLP系统有望实现对帕金森病患者认知功能的低负担、纵向监测，成为神经心理评估的重要补充。

Abstract: Understanding how individuals with Parkinson's disease (PD) describe cognitive experiences in their daily lives can offer valuable insights into disease-related cognitive and emotional changes. However, extracting such information from unstructured patient narratives is challenging due to the subtle, overlapping nature of cognitive constructs. This study developed and evaluated natural language processing (NLP) models to automatically identify categories that reflect various cognitive processes from de-identified first-person narratives. Three model families, a Bio_ClinicalBERT-based span categorization model for nested entity recognition, a fine-tuned Meta-Llama-3-8B-Instruct model using QLoRA for instruction following, and GPT-4o mini evaluated under zero- and few-shot settings, were compared on their performance on extracting seven categories. Our findings indicated that model performance varied substantially across categories and model families. The fine-tuned Meta-Llama-3-8B-Instruct achieved the highest overall F1-scores (0.74 micro-average and 0.59 macro-average), particularly excelling in context-dependent categories such as thought and social interaction. Bio_ClinicalBERT exhibited high precision but low recall and performed comparable to Llama for some category types such as location and time but failed on other categories such as thought, emotion and social interaction. Compared to conventional information extraction tasks, this task presents a greater challenge due to the abstract and overlapping nature of narrative accounts of complex cognitive processes. Nonetheless, with continued refinement, these NLP systems hold promise for enabling low-burden, longitudinal monitoring of cognitive function and serving as a valuable complement to formal neuropsychological assessments in PD.

</details>


### [87] [BNLI: A Linguistically-Refined Bengali Dataset for Natural Language Inference](https://arxiv.org/abs/2511.08813)
*Farah Binta Haque,Md Yasin,Shishir Saha,Md Shoaib Akhter Rafi,Farig Sadeque*

Main category: cs.CL

TL;DR: 本文提出了BNLI——一个经过精细标注和语言学筛选的孟加拉语NLI数据集，以提升低资源语言的自然语言推理研究。


<details>
  <summary>Details</summary>
Motivation: 现有的孟加拉语NLI数据集存在标注错误、句对模糊以及语言多样性不足等问题，严重影响模型的训练和评估效率。

Method: 通过严格的标注流程，着重语义清晰和类别平衡（涵盖蕴含、矛盾、中立三类），建立新的孟加拉语NLI数据集，并用多种主流Transformer架构（包括多语言和孟加拉语专用模型）进行基准测试。

Result: 实验结果表明，BNLI数据集在语义关系捕捉上的可靠性和可解释性优于现有数据集。

Conclusion: BNLI为孟加拉语及其他低资源语言的NLI研究提供了坚实的数据基础，有助于推动相关领域的发展。

Abstract: Despite the growing progress in Natural Language Inference (NLI) research, resources for the Bengali language remain extremely limited. Existing Bengali NLI datasets exhibit several inconsistencies, including annotation errors, ambiguous sentence pairs, and inadequate linguistic diversity, which hinder effective model training and evaluation. To address these limitations, we introduce BNLI, a refined and linguistically curated Bengali NLI dataset designed to support robust language understanding and inference modeling. The dataset was constructed through a rigorous annotation pipeline emphasizing semantic clarity and balance across entailment, contradiction, and neutrality classes. We benchmarked BNLI using a suite of state-of-the-art transformer-based architectures, including multilingual and Bengali-specific models, to assess their ability to capture complex semantic relations in Bengali text. The experimental findings highlight the improved reliability and interpretability achieved with BNLI, establishing it as a strong foundation for advancing research in Bengali and other low-resource language inference tasks.

</details>


### [88] [Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents](https://arxiv.org/abs/2511.08835)
*Yejin Yoon,Yuri Son,Namyoung So,Minseo Kim,Minsoo Cho,Chanhee Park,Seungshin Lee,Taeuk Kim*

Main category: cs.CL

TL;DR: 论文介绍了TACT数据集，旨在兼顾任务型对话（TOD）与闲聊两种模式，实现对话模式转换的无缝衔接。通过新指标和微调方法，TACT提升了模型在识别意图和对话模式转换方面的能力，优于主流基线并在人工评测中表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前对话系统多聚焦于任务型或闲聊型单一模式，现实对话中两者频繁融合与切换，而现有数据集与评测手段对此支持不足。该研究旨在填补这一空白，推动对复杂、多元对话场景的支持。

Method: 提出了TACT数据集，涵盖多样的任务型与闲聊自然流转场景，以及支持用户/系统主导的模式切换。论文设计了新评测指标Switch和Recovery，衡量对话模式切换与恢复能力，并利用Direct Preference Optimization对模型进行进一步微调。

Result: TACT训练的模型在意图识别与模式切换处理上明显优于传统基线。在与GPT-4o人工对比中，TACT+DPO模型实现了75.74%的联合准确率与70.1%的胜率。

Conclusion: 结构多样的数据集结合DPO方法提升了对话系统对模式切换的掌控力与响应质量，为构建更自然主动、能自如切换对话风格的智能体奠定了基础。

Abstract: Conversational agents have traditionally been developed for either task-oriented dialogue (TOD) or open-ended chitchat, with limited progress in unifying the two. Yet, real-world conversations naturally involve fluid transitions between these modes. To address this gap, we introduce TACT (TOD-And-Chitchat Transition), a dataset designed for transition-aware dialogue modeling that incorporates structurally diverse and integrated mode flows. TACT supports both user- and agent-driven mode switches, enabling robust modeling of complex conversational dynamics. To evaluate an agent's ability to initiate and recover from mode transitions, we propose two new metrics -- Switch and Recovery. Models trained on TACT outperform baselines in both intent detection and mode transition handling. Moreover, applying Direct Preference Optimization (DPO) to TACT-trained models yields additional gains, achieving 75.74\% joint mode-intent accuracy and a 70.1\% win rate against GPT-4o in human evaluation. These results demonstrate that pairing structurally diverse data with DPO enhances response quality and transition control, paving the way for more proactive and transition-aware conversational agents.

</details>


### [89] [BioVerge: A Comprehensive Benchmark and Study of Self-Evaluating Agents for Biomedical Hypothesis Generation](https://arxiv.org/abs/2511.08866)
*Fuyi Yang,Chenchen Ye,Mingyu Derek Ma,Yijia Xiao,Matthew Yang,Wei Wang*

Main category: cs.CL

TL;DR: 该文提出了BioVerge数据集和BioVerge Agent框架，为生物医学假说生成领域提供了标准化的基准和环境，利用大模型能力推进假说发现。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学文献发掘方法受限于数据类型和预设模式，阻碍了新颖复杂关联的发现，且缺乏标准化数据集与环境支持大模型探索假说生成。

Method: 提出BioVerge基准数据集，整合结构化和文本型生物医学数据，并开发基于LLM的BioVerge Agent，包括生成与评价模块，采用ReAct策略在生成-自评闭环中探索假说。

Result: 实验表明，BioVerge Agent的不同架构影响假说探索多样性和推理方法；结构化和文本信息各自独特且互补，提升了假说生成质量；自我评估机制显著提升了假说的新颖性与相关性。

Conclusion: BioVerge为生物医学假说生成提供了标准化的评测和实验平台，推进了大模型智能在科研创新中的应用，并展示了多源信息和自监督机制在促进新假说发现中的重要作用。

Abstract: Hypothesis generation in biomedical research has traditionally centered on uncovering hidden relationships within vast scientific literature, often using methods like Literature-Based Discovery (LBD). Despite progress, current approaches typically depend on single data types or predefined extraction patterns, which restricts the discovery of novel and complex connections. Recent advances in Large Language Model (LLM) agents show significant potential, with capabilities in information retrieval, reasoning, and generation. However, their application to biomedical hypothesis generation has been limited by the absence of standardized datasets and execution environments. To address this, we introduce BioVerge, a comprehensive benchmark, and BioVerge Agent, an LLM-based agent framework, to create a standardized environment for exploring biomedical hypothesis generation at the frontier of existing scientific knowledge. Our dataset includes structured and textual data derived from historical biomedical hypotheses and PubMed literature, organized to support exploration by LLM agents. BioVerge Agent utilizes a ReAct-based approach with distinct Generation and Evaluation modules that iteratively produce and self-assess hypothesis proposals. Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis generation; and 3) self-evaluation significantly improves the novelty and relevance of proposed hypotheses.

</details>


### [90] [Hallucinate or Memorize? The Two Sides of Probabilistic Learning in Large Language Models](https://arxiv.org/abs/2511.08877)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在引用推荐中的幻觉现象，发现被高度引用的论文引用信息几乎被模型逐字记忆，幻觉率低。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能辅助学术引用推荐，但其生成虚假且不存在的引用（即幻觉）仍是主要问题。研究旨在探究LLM产生真实引用的能力与其训练数据中知识是“生成还是记忆”之间的关系。

Method: 以引用次数作为训练语料中冗余性的代理变量，利用GPT-4.1在20个计算机科学子领域生成并人工核查100条引用，通过余弦相似度来衡量生成与真实元数据的一致性，考察引用频率对幻觉现象的影响。

Result: （1）引用次数与引用信息的准确性强相关；（2）引用次数超过约1,000时，文献信息接近逐字记忆，幻觉显著减少；（3）当多个高被引论文内容相似时，会发生记忆干扰。

Conclusion: LLM对高被引论文的元数据基本实现逐字记忆，幻觉低，一定阈值处模型由泛化转为记忆，这有助于理解和改善学术引用推荐中的幻觉问题。

Abstract: Large language models (LLMs) have been increasingly applied to a wide range of tasks, from natural language understanding to code generation. While they have also been used to assist in citation recommendation, the hallucination of non-existent papers remains a major issue. Building on prior studies, this study hypothesizes that an LLM's ability to correctly produce bibliographic records depends on whether the underlying knowledge is generated or memorized, with highly cited papers (i.e., more frequently appear in the pretraining corpus) showing lower hallucination rates. We therefore assume citation count as a proxy for training data redundancy (i.e., the frequency with which a given bibliographic record appears in the pretraining corpus) and investigate how citation frequency affects hallucinated references in LLM outputs. Using GPT-4.1, we generated and manually verified 100 citations across twenty computer-science domains, and measured factual consistency via cosine similarity between generated and authentic metadata. The results revealed that (i) citation count is strongly correlated with factual accuracy, (ii) bibliographic information becomes almost verbatim memorized beyond roughly 1,000 citations, and (iii) memory interference occurs when multiple highly cited papers share similar content. These findings indicate a threshold where generalization shifts into memorization, with highly cited papers being nearly verbatim retained in the model.

</details>


### [91] [HalluClean: A Unified Framework to Combat Hallucinations in LLMs](https://arxiv.org/abs/2511.08916)
*Yaxin Zhao,Yu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为HalluClean的新框架，能在无需外部知识与监督检测的前提下，有效检测并纠正大语言模型（LLMs）生成的幻觉内容。实验显示该方法在多个任务上均大幅提升了输出的事实一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型尽管在NLP任务上表现优异，但常产生与事实不符的“幻觉”内容，严重影响其实用性和可靠性。因此，亟需通用且高效的方法提升其内容的可信度。

Method: HalluClean是一个轻量级、任务无关的体系，其创新点在于基于“推理增强范式”，将检测与纠正流程明确分为规划、执行、修订三步，从多角度发现并修正LLM生成中的事实漏洞。该方法依赖极简的提示工程，实现对不同任务与领域的零样本泛化，无需外部知识或监督工具。

Result: 在问题回答、对话、摘要、数学文字题和矛盾检测五类任务上的广泛实验表明：HalluClean方案能够显著提高LLM输出内容的事实一致性，并全面优于目前的主流竞品。

Conclusion: HalluClean为提升大语言模型输出内容的可信度提供了创新且实用的解决方案，有望推动其在真实场景下的可靠应用。

Abstract: Large language models (LLMs) have achieved impressive performance across a wide range of natural language processing tasks, yet they often produce hallucinated content that undermines factual reliability. To address this challenge, we introduce HalluClean, a lightweight and task-agnostic framework for detecting and correcting hallucinations in LLM-generated text. HalluClean adopts a reasoning-enhanced paradigm, explicitly decomposing the process into planning, execution, and revision stages to identify and refine unsupported claims. It employs minimal task-routing prompts to enable zero-shot generalization across diverse domains, without relying on external knowledge sources or supervised detectors. We conduct extensive evaluations on five representative tasks-question answering, dialogue, summarization, math word problems, and contradiction detection. Experimental results show that HalluClean significantly improves factual consistency and outperforms competitive baselines, demonstrating its potential to enhance the trustworthiness of LLM outputs in real-world applications.

</details>


### [92] [TiDAR: Think in Diffusion, Talk in Autoregression](https://arxiv.org/abs/2511.08923)
*Jingyu Liu,Xin Dong,Zhifan Ye,Rishabh Mehta,Yonggan Fu,Vartika Singh,Jan Kautz,Ce Zhang,Pavlo Molchanov*

Main category: cs.CL

TL;DR: 本文介绍了一种名为TiDAR的混合架构模型，通过结合扩散式草稿生成与自回归（AR）采样，兼顾高并发生成速度和高质量输出。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型生成质量高但推理速度慢，而扩散模型在并发和GPU利用率上更优但牺牲了生成质量。为了突破这一兼顾性难题，作者希望提出一种能兼顾二者优点的新方法。

Method: 作者提出TiDAR架构，在单次前向传播中，通过结构化注意力掩码将扩散模型用于初步草稿生成（Thinking），再用自回归方式对最终输出进行抽样（Talking），实现高效草稿与高质量输出的结合。并支持高效的KV cache。

Result: TiDAR在1.5B和8B规模上，通过并行草稿和抽样机制，实测在吞吐量上大幅超越推测解码（speculative decoding），并且在效率和质量上均优于典型扩散模型（如Dream和Llada），在质量上首次与AR模型持平。

Conclusion: TiDAR首次实现了与自回归模型相当的输出质量，并且在服务中能以4.71-5.91倍的速度生成token，在GPU利用率和推理效率方面取得实质性突破。

Abstract: Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.

</details>


### [93] [EVADE: LLM-Based Explanation Generation and Validation for Error Detection in NLI](https://arxiv.org/abs/2511.08949)
*Longfei Zuo,Barbara Plank,Siyao Peng*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLMs）的新框架EVADE，用于检测自然语言推断（NLI）数据集中的标签错误，实现高效的数据集质量提升。实验表明，利用LLMs检测的数据错误比人工检测更有助于模型训练效果。


<details>
  <summary>Details</summary>
Motivation: NLI等任务面临多人标签分歧的问题，手动两轮标注高成本，且难以大规模覆盖或区分标签多样性与标注错误。现有方法VARIERR依存于人工两个回合解释及验证，效率低，影响数据质量提升。

Method: 提出EVADE框架：利用大语言模型（LLMs）自动生成、验证标签解释，从而检测错误。实验比较了LLMs与人工在分布比对、验证重叠与模型训练影响上的效果。

Result: LLMs验证能使自动生成的标签解释分布更接近人工，且去除LLMs检测出的错误，模型微调效果优于去除人工检测错误时的表现。

Conclusion: EVADE框架用LLMs可自动检测和验证NLI数据集错误，既降低人工工作量，又在标签多样性场景下提升数据集质量及模型下游性能。

Abstract: High-quality datasets are critical for training and evaluating reliable NLP models. In tasks like natural language inference (NLI), human label variation (HLV) arises when multiple labels are valid for the same instance, making it difficult to separate annotation errors from plausible variation. An earlier framework VARIERR (Weber-Genzel et al., 2024) asks multiple annotators to explain their label decisions in the first round and flag errors via validity judgments in the second round. However, conducting two rounds of manual annotation is costly and may limit the coverage of plausible labels or explanations. Our study proposes a new framework, EVADE, for generating and validating explanations to detect errors using large language models (LLMs). We perform a comprehensive analysis comparing human- and LLM-detected errors for NLI across distribution comparison, validation overlap, and impact on model fine-tuning. Our experiments demonstrate that LLM validation refines generated explanation distributions to more closely align with human annotations, and that removing LLM-detected errors from training data yields improvements in fine-tuning performance than removing errors identified by human annotators. This highlights the potential to scale error detection, reducing human effort while improving dataset quality under label variation.

</details>


### [94] [SpiralThinker: Latent Reasoning through an Iterative Process with Text-Latent Interleaving](https://arxiv.org/abs/2511.08983)
*Shengmin Piao,Sanghyun Park*

Main category: cs.CL

TL;DR: 本文提出了SpiralThinker框架，通过对潜在表征进行迭代更新，实现不依赖于新生成文本的隐式推理，显著优于现有潜在推理方法。


<details>
  <summary>Details</summary>
Motivation: 传统大模型推理主要基于显式文本生成，潜在推理虽受关注，但尚缺乏保持潜在表征稳定和系统融合显式、隐式推理的机制。

Method: 作者设计了SpiralThinker框架，采用多轮对潜在表征的迭代更新，无需生成新token就能进行推理；同时引入逐步对齐目标和结构化注释，保持潜在与文本推理间的连贯性。

Result: 在数学、逻辑和常识推理任务上，SpiralThinker在所有基准集均超越已有潜在推理方法，取得最佳综合表现。分析显示迭代与对齐缺一不可，最优潜在token数和迭代轮数依数据集而异，对齐策略对推理效果至关重要。

Conclusion: SpiralThinker将迭代计算与潜在推理结合，创新性证明了对齐的迭代更新能有效引导潜在空间推理，提升表现并补足现有方法缺陷。

Abstract: Recent advances in large reasoning models have been driven by reinforcement learning and test-time scaling, accompanied by growing interest in latent rather than purely textual reasoning. However, existing latent reasoning methods lack mechanisms to ensure stable evolution of latent representations and a systematic way to interleave implicit and explicit reasoning. We introduce SpiralThinker, a unified framework that performs iterative updates over latent representations, enabling extended implicit reasoning without generating additional tokens. A progressive alignment objective combined with structured annotations maintains coherence between latent and textual reasoning. Across mathematical, logical, and commonsense reasoning tasks, SpiralThinker achieves the best overall performance among latent reasoning approaches, consistently surpassing previous methods across all benchmarks. Detailed analyses reveal that both iteration and alignment are indispensable, the numbers of latent tokens and iterations exhibit dataset-specific optima, and appropriate alignment proves critical for an effective iterative process. Overall, SpiralThinker bridges iterative computation and latent reasoning, demonstrating that aligned iterative updates can reliably steer reasoning in the latent space.

</details>


### [95] [Detecting Emotional Dynamic Trajectories: An Evaluation Framework for Emotional Support in Language Models](https://arxiv.org/abs/2511.09003)
*Zhouxing Tan,Ruochong Xiong,Yulong Wan,Jinlong Ma,Hanlin Xue,Qichun Deng,Haifeng Jing,Zhengtong Zhang,Depei Liu,Shiyuan Luo,Junfei Liu*

Main category: cs.CL

TL;DR: 本文提出了一种以用户为中心的长时情感支持评估框架，补足了以往大语言模型仅用短暂对话方式评估情感支持能力的不足。


<details>
  <summary>Details</summary>
Motivation: 现有对于大语言模型（LLMs）在情感支持方面的评估多基于静态、短时间片段，无法真实反映其在持续对话和长期互动中的表现，亟需一种能体现动态、长期情感支持能力的新评测方式。

Method: 作者设计了一个规模较大的基准数据集，包含328种情感场景和1,152个情绪扰动事件，模拟了现实对话中的情绪变化。对模型输出施加被验证过的情绪调节策略（如情境选择、认知重评），将用户情感轨迹建模为一阶马尔可夫过程，并采用因果调整的情感估计方法。提出了三项新的轨迹级指标：基线情感水平（BEL）、情感轨迹波动度（ETV）和情感质心位置（ECP），用于全面评估LLM在长时情感支持方面的能力。

Result: 通过该评测框架对多种主流LLM进行了系统评测，发现不同模型之间在情感支持能力上存在显著差异，并获得了对模型改进具有指导意义的分析结果。

Conclusion: 作者提出的基于轨迹的新评测框架和指标能够更真实、全面地反映LLMs在情感支持任务中的实际能力，有助于推动更有同理心和稳定性的人机交互系统开发。

Abstract: Emotional support is a core capability in human-AI interaction, with applications including psychological counseling, role play, and companionship. However, existing evaluations of large language models (LLMs) often rely on short, static dialogues and fail to capture the dynamic and long-term nature of emotional support. To overcome this limitation, we shift from snapshot-based evaluation to trajectory-based assessment, adopting a user-centered perspective that evaluates models based on their ability to improve and stabilize user emotional states over time. Our framework constructs a large-scale benchmark consisting of 328 emotional contexts and 1,152 disturbance events, simulating realistic emotional shifts under evolving dialogue scenarios. To encourage psychologically grounded responses, we constrain model outputs using validated emotion regulation strategies such as situation selection and cognitive reappraisal. User emotional trajectories are modeled as a first-order Markov process, and we apply causally-adjusted emotion estimation to obtain unbiased emotional state tracking. Based on this framework, we introduce three trajectory-level metrics: Baseline Emotional Level (BEL), Emotional Trajectory Volatility (ETV), and Emotional Centroid Position (ECP). These metrics collectively capture user emotional dynamics over time and support comprehensive evaluation of long-term emotional support performance of LLMs. Extensive evaluations across a diverse set of LLMs reveal significant disparities in emotional support capabilities and provide actionable insights for model development.

</details>


### [96] [A Neurosymbolic Approach to Natural Language Formalization and Verification](https://arxiv.org/abs/2511.09008)
*Sam Bayless,Stefano Buliani,Darion Cassel,Byron Cook,Duncan Clough,Rémi Delmas,Nafi Diallo,Ferhat Erata,Nick Feng,Dimitra Giannakopoulou,Aman Goel,Aditya Gokhale,Joe Hendrix,Marc Hudak,Dejan Jovanović,Andrew M. Kent,Benjamin Kiesl-Reiter,Jeffrey J. Kuna,Nadia Labai,Joseph Lilien,Divya Raghunathan,Zvonimir Rakamarić,Niloofar Razavi,Michael Tautschnig,Ali Torkamani,Nathaniel Weir,Michael W. Whalen,Jianan Yao*

Main category: cs.CL

TL;DR: 提出了一种两阶段的神经符号推理框架，利用大模型和可选的人工辅助，把自然语言政策形式化，并在推理时进行自动验证，实现高可靠性和可审计性，适用于金融、医疗等高监管行业。


<details>
  <summary>Details</summary>
Motivation: 大模型内部具有随机性，在政策严格的金融、医疗等行业难以大规模应用，需要一种既能利用大模型语言理解能力、又能保证推理可验证和可审计性的新方法。

Method: 方法分两阶段：1）利用大模型和可选人工辅助将自然语言政策转为形式化表达，实现对形式化过程的精细控制；2）在推理阶段，自动将自然语言陈述形式化，再与政策形式化表达比对，多次冗余形式化并交叉验证，确保语义等价。

Result: 实验结果显示，该方法在基准测试中达到了99%以上的可靠性，误报率极低。

Conclusion: 该方法能够在高度监管行业实现可靠的逻辑有效性识别，同时生成可审计的逻辑证据，可以反馈和提升原始文本质量。

Abstract: Large Language Models perform well at natural language interpretation and reasoning, but their inherent stochasticity limits their adoption in regulated industries like finance and healthcare that operate under strict policies. To address this limitation, we present a two-stage neurosymbolic framework that (1) uses LLMs with optional human guidance to formalize natural language policies, allowing fine-grained control of the formalization process, and (2) uses inference-time autoformalization to validate logical correctness of natural language statements against those policies. When correctness is paramount, we perform multiple redundant formalization steps at inference time, cross checking the formalizations for semantic equivalence. Our benchmarks demonstrate that our approach exceeds 99% soundness, indicating a near-zero false positive rate in identifying logical validity. Our approach produces auditable logical artifacts that substantiate the verification outcomes and can be used to improve the original text.

</details>


### [97] [MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique](https://arxiv.org/abs/2511.09067)
*Gailun Zeng,Ziyang Luo,Hongzhan Lin,Yuchen Tian,Kaixin Li,Ziyang Gong,Jianxiong Guo,Jing Ma*

Main category: cs.CL

TL;DR: 本论文提出MM-CRITIC基准，用于全面评测大规模多模态模型（LMMs）的批判能力，涵盖8类任务及500多个问题，并通过GPT-4o结合专家答案进行严格评分。实验证明该基准有效，揭示了模型在不同批判维度下的表现与难点。


<details>
  <summary>Details</summary>
Motivation: 随着多模态模型（LMMs）能力提升，其做为可靠AI助手需具备自我批判和提升能力。然而，当前关于LMMs批判能力的系统性研究和评测尚不足，缺乏相应高质量的基准。

Method: 构建MM-CRITIC基准，从多个维度（基本、纠错、对比批判）设计覆盖8类主要任务、500多个任务，总计4471个样本。收集各种不同规模LMM模型的回答，并通过引入专家指导的标准答案，辅助GPT-4o对模型做自动化打分和生成参考批判供比对。

Result: 大量实验表明，MM-CRITIC能有效区分和评估主流LMM模型的多维批判能力。同时，分析揭示评测中批判质量与回答质量相关，不同维度的批判难易程度不一。

Conclusion: MM-CRITIC为评测和推动LMM批判能力提供了全面、可靠的工具，对多模态AI能力提升及相关研究具有重要价值。

Abstract: The ability of critique is vital for models to self-improve and serve as reliable AI assistants. While extensively studied in language-only settings, multimodal critique of Large Multimodal Models (LMMs) remains underexplored despite their growing capabilities in tasks like captioning and visual reasoning. In this work, we introduce MM-CRITIC, a holistic benchmark for evaluating the critique ability of LMMs across multiple dimensions: basic, correction, and comparison. Covering 8 main task types and over 500 tasks, MM-CRITIC collects responses from various LMMs with different model sizes and is composed of 4471 samples. To enhance the evaluation reliability, we integrate expert-informed ground answers into scoring rubrics that guide GPT-4o in annotating responses and generating reference critiques, which serve as anchors for trustworthy judgments. Extensive experiments validate the effectiveness of MM-CRITIC and provide a comprehensive assessment of leading LMMs' critique capabilities under multiple dimensions. Further analysis reveals some key insights, including the correlation between response quality and critique, and varying critique difficulty across evaluation dimensions. Our code is available at https://github.com/MichealZeng0420/MM-Critic.

</details>


### [98] [Context-Aware Dynamic Chunking for Streaming Tibetan Speech Recognition](https://arxiv.org/abs/2511.09085)
*Chao Wang,Yuqing Cai,Renzeng Duojie,Jin Zhang,Yutong Liu,Nyima Tashi*

Main category: cs.CL

TL;DR: 本文提出了一种针对安多藏语的流式语音识别框架，在混合CTC/注意力模型架构基础上，结合上下文自适应的动态分块机制，有效提升了识别准确率并降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 针对藏语安多方言流式语音识别在不同语速、上下文截断以及藏文特殊语言结构下的识别困难，现有固定分块方法难以兼顾准确性与实时性的瓶颈。

Method: 采用混合CTC/Attention模型，提出动态分块机制，根据编码状态自适应调整分块宽度，实现跨分块信息交互；构建符合藏文书写规律的词典用于建模单元，并在解码阶段引入外部语言模型提升长句识别能力和语义一致性。

Result: 模型在测试集上取得了6.23%的词错误率（WER），相较于固定分块的基线方法，WER相对降低了48.15%，同时显著降低了识别延迟，且表现接近全局解码方案。

Conclusion: 动态分块混合架构有效提升了安多藏语流式识别准确率与语义一致性，并显著降低了延迟，适合实际低资源、实时语音识别场景。

Abstract: In this work, we propose a streaming speech recognition framework for Amdo Tibetan, built upon a hybrid CTC/Atten-tion architecture with a context-aware dynamic chunking mechanism. The proposed strategy adaptively adjusts chunk widths based on encoding states, enabling flexible receptive fields, cross-chunk information exchange, and robust adaptation to varying speaking rates, thereby alleviating the context truncation problem of fixed-chunk methods. To further capture the linguistic characteristics of Tibetan, we construct a lexicon grounded in its orthographic principles, providing linguistically motivated modeling units. During decoding, an external language model is integrated to enhance semantic consistency and improve recognition of long sentences. Experimental results show that the proposed framework achieves a word error rate (WER) of 6.23% on the test set, yielding a 48.15% relative improvement over the fixed-chunk baseline, while significantly reducing recognition latency and maintaining performance close to global decoding.

</details>


### [99] [Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning](https://arxiv.org/abs/2511.09109)
*Wenda Wei,Yu-An Liu,Ruqing Zhang,Jiafeng Guo,Lixin Su,Shuaiqiang Wang,Dawei Yin,Maarten de Rijke,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的检索增强推理（RAG）框架Bi-RAR，通过前后双向评估每一步推理，有效提升了多步复杂推理任务的准确性。


<details>
  <summary>Details</summary>
Motivation: RAG虽能缓解大模型幻觉问题，但在多步复杂推理场景下仍有限。现有方法多依赖结果监督，缺乏对中间推理步骤的有效指导，容易导致奖励欺骗和响应质量下降。

Method: 提出Bi-RAR框架，通过前向和后向的联合监督评估每一步推理。利用基于Kolmogorov复杂度的信息距离（通过语言模型生成概率近似），衡量当前推理与答案的距离，以及其对问题的覆盖度。推理优化采用多目标强化学习，并引入级联奖励结构，强调推理早期对齐。

Result: 在七个问答基准数据集上，Bi-RAR在响应质量和与搜索引擎交互方面均优于现有方法。

Conclusion: Bi-RAR通过创新的双向信息距离和多目标强化学习方法，有效提升了复杂推理场景下RAG的推理能力和效率。

Abstract: Retrieval-augmented generation (RAG) has proven to be effective in mitigating hallucinations in large language models, yet its effectiveness remains limited in complex, multi-step reasoning scenarios.Recent efforts have incorporated search-based interactions into RAG, enabling iterative reasoning with real-time retrieval. Most approaches rely on outcome-based supervision, offering no explicit guidance for intermediate steps. This often leads to reward hacking and degraded response quality. We propose Bi-RAR, a novel retrieval-augmented reasoning framework that evaluates each intermediate step jointly in both forward and backward directions. To assess the information completeness of each step, we introduce a bidirectional information distance grounded in Kolmogorov complexity, approximated via language model generation probabilities. This quantification measures both how far the current reasoning is from the answer and how well it addresses the question. To optimize reasoning under these bidirectional signals, we adopt a multi-objective reinforcement learning framework with a cascading reward structure that emphasizes early trajectory alignment. Empirical results on seven question answering benchmarks demonstrate that Bi-RAR surpasses previous methods and enables efficient interaction and reasoning with the search engine during training and inference.

</details>


### [100] [Assessing the Capabilities of LLMs in Humor:A Multi-dimensional Analysis of Oogiri Generation and Evaluation](https://arxiv.org/abs/2511.09133)
*Ritsu Sakabe,Hwichan Kim,Tosho Hirasawa,Mamoru Komachi*

Main category: cs.CL

TL;DR: 本论文采用多维度评估体系，以日本Oogiri喜剧游戏为基准，系统分析了大语言模型(LLMs)的幽默生成与理解能力，发现其在‘共情’维度表现较弱，且人与模型在幽默判定侧重点不同。


<details>
  <summary>Details</summary>
Motivation: 早期LLM幽默能力评测多为单一维度，如“是否有趣”，不足以全面揭示模型与人的幽默理解差异。因此，研究者希望探寻更细致全面的评估方式，推动更智能、更具情感的对话系统发展。

Method: 研究团队扩展和增强Oogiri题库并人工打分，设置六个评价维度：新颖性、清晰性、相关性、智慧性、共情性和整体幽默感，对LLM生成的幽默内容及其幽默判定能力进行系统评估，并对人类与模型评价做相关性分析。

Result: LLMs生成Oogiri回答的创造性基本处于低-中等人类水平，尤其在共情维度明显偏弱。评价相关性分析显示，模型更看重新颖性，而人关注共情性，两者幽默判定标准有本质分歧。

Conclusion: 现有LLMs虽然具备一定幽默生成能力，但在共情表现不佳，因此无法模拟人类幽默判断。所发布的标注数据集有助于推动更优秀、更具情感的对话AI研究。

Abstract: Computational humor is a frontier for creating advanced and engaging natural language processing (NLP) applications, such as sophisticated dialogue systems. While previous studies have benchmarked the humor capabilities of Large Language Models (LLMs), they have often relied on single-dimensional evaluations, such as judging whether something is simply ``funny.'' This paper argues that a multifaceted understanding of humor is necessary and addresses this gap by systematically evaluating LLMs through the lens of Oogiri, a form of Japanese improvisational comedy games. To achieve this, we expanded upon existing Oogiri datasets with data from new sources and then augmented the collection with Oogiri responses generated by LLMs. We then manually annotated this expanded collection with 5-point absolute ratings across six dimensions: Novelty, Clarity, Relevance, Intelligence, Empathy, and Overall Funniness. Using this dataset, we assessed the capabilities of state-of-the-art LLMs on two core tasks: their ability to generate creative Oogiri responses and their ability to evaluate the funniness of responses using a six-dimensional evaluation. Our results show that while LLMs can generate responses at a level between low- and mid-tier human performance, they exhibit a notable lack of Empathy. This deficit in Empathy helps explain their failure to replicate human humor assessment. Correlation analyses of human and model evaluation data further reveal a fundamental divergence in evaluation criteria: LLMs prioritize Novelty, whereas humans prioritize Empathy. We release our annotated corpus to the community to pave the way for the development of more emotionally intelligent and sophisticated conversational agents.

</details>


### [101] [One-Topic-Doesn't-Fit-All: Transcreating Reading Comprehension Test for Personalized Learning](https://arxiv.org/abs/2511.09135)
*Jieun Han,Daniel Lee,Haneul Yoo,Jinsung Yoon,Junyeong Park,Suin Kim,So-Yeon Ahn,Alice Oh*

Main category: cs.CL

TL;DR: 本文提出了一种利用GPT-4o大模型，根据学生兴趣生成个性化英文阅读理解测试的方法，并通过实验证明，该方法能有效提升学生的理解力与学习动机。


<details>
  <summary>Details</summary>
Motivation: EFL（以英语为外语）教学中，学生的参与度和动机对阅读理解成绩有重要影响。传统阅读材料缺乏针对性，难以持续吸引学生兴趣，因此亟需探索个性化内容生成方法来增强教学效果。

Method: 作者结合了RACE-C数据集和OpenAI的gpt-4o模型，设计了一条结构化内容转译流程。流程包含主题提取、基于布鲁姆分类的题型分类、语言特征分析及内容转译，自动生成与学生兴趣相关且与原文语言难度相当的新阅读材料和选择题。

Result: 在韩国EFL学习者中开展了对照实验。实验证明：接受兴趣相关个性化阅读材料的学生，其阅读理解和学习动机保持情况均优于使用普通材料的学生。

Conclusion: 基于兴趣的个性化英文阅读理解内容生成方法，不仅提升了EFL学习者的阅读理解能力，还增强了学习动机，具有较好的推广和应用价值。

Abstract: Personalized learning has gained attention in English as a Foreign Language (EFL) education, where engagement and motivation play crucial roles in reading comprehension. We propose a novel approach to generating personalized English reading comprehension tests tailored to students' interests. We develop a structured content transcreation pipeline using OpenAI's gpt-4o, where we start with the RACE-C dataset, and generate new passages and multiple-choice reading comprehension questions that are linguistically similar to the original passages but semantically aligned with individual learners' interests. Our methodology integrates topic extraction, question classification based on Bloom's taxonomy, linguistic feature analysis, and content transcreation to enhance student engagement. We conduct a controlled experiment with EFL learners in South Korea to examine the impact of interest-aligned reading materials on comprehension and motivation. Our results show students learning with personalized reading passages demonstrate improved comprehension and motivation retention compared to those learning with non-personalized materials.

</details>


### [102] [DoPE: Denoising Rotary Position Embedding](https://arxiv.org/abs/2511.09146)
*Jing Xiong,Liyang Fan,Hui Shen,Zunhai Su,Min Yang,Lingpeng Kong,Ngai Wong*

Main category: cs.CL

TL;DR: 该论文提出了一种新的方法（DoPE）来提高Transformer模型的长度泛化能力，通过清除位置编码中的“噪声”频段，显著提升长文本环境下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的旋转位置编码（RoPE）方法在处理比训练阶段更长序列时效果受限，主要表现为注意力下沉和泛化能力变差。因此，需要设计新的机制来提升Transformer对超长文本的适应性。

Method: 作者将带位置编码的注意力图重解释为带噪信号，通过截断矩阵熵检测异常频率段，并以高斯分布重参数化这些特征，无需训练即可直接提升模型表现。该方法被称为Denoising Positional Encoding（DoPE）。

Result: 在各种长序列任务（如“针在草堆里”和多轮in-context learning）上，DoPE显著提升了模型检索准确率及推理稳定性，最长测试序列可达64K tokens。

Conclusion: DoPE无须训练即可有效抑制注意力下沉、恢复平衡注意力分布，提升Transformer的长度泛化能力，为超长文本任务提供了简单高效的解决方案。

Abstract: Rotary Position Embedding (RoPE) in Transformer models has inherent limits that weaken length extrapolation. We reinterpret the attention map with positional encoding as a noisy feature map, and propose Denoising Positional Encoding (DoPE), a training-free method based on truncated matrix entropy to detect outlier frequency bands in the feature map. Leveraging the noise characteristics of the feature map, we further reparameterize it with a parameter-free Gaussian distribution to achieve robust extrapolation. Our method theoretically reveals the underlying cause of the attention sink phenomenon and its connection to truncated matrix entropy. Experiments on needle-in-a-haystack and many-shot in-context learning tasks demonstrate that DoPE significantly improves retrieval accuracy and reasoning stability across extended contexts (up to 64K tokens). The results show that the denoising strategy for positional embeddings effectively mitigates attention sinks and restores balanced attention patterns, providing a simple yet powerful solution for improving length generalization. Our project page is Project: https://The-physical-picture-of-LLMs.github.io

</details>


### [103] [LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls](https://arxiv.org/abs/2511.09148)
*Kangning Zhang,Wenxiang Jiao,Kounianhua Du,Yuan Lu,Weiwen Liu,Weinan Zhang,Lei Zhang,Yong Yu*

Main category: cs.CL

TL;DR: 本文提出一种新的工具数据进化框架LoopTool，实现模型训练与数据合成的闭环自动优化，成功提升了大模型工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM与外部工具结合主要依赖静态合成数据，数据生成和模型训练分离，导致不能针对模型弱点及时调整数据，且噪声标签会降低训练效率。因此需要更高效、自适应的数据生成训练流程。

Method: 作者设计了LoopTool框架，将数据合成与模型训练紧密结合，形成三模块闭环迭代优化：1）GCP模块诊断模型已掌握/失效能力，2）JGLV模块用开源评判模型自动校正注释错误，3）EDDE模块据模型错误自动生成新难例样本。系统基于开源工具，成本低，对外部API无依赖。

Result: 使用LoopTool训练的8B参数模型，表现超过基于32B生成器的数据集训练出的模型，并在BFCL-v3与ACEBench两项任务基准上取得同规模模型新SOTA。

Conclusion: 闭环自优化的数据生成训练流程显著增强了LLM工具使用能力，且开放可复用，为模型训练范式带来新方向。

Abstract: Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs.

</details>


### [104] [A Hybrid Search for Complex Table Question Answering in Securities Report](https://arxiv.org/abs/2511.09179)
*Daiki Shirafuji,Koji Tanaka,Tatsuhiko Saito*

Main category: cs.CL

TL;DR: 本文提出了一种无需人工识别即可实现表格问答（TQA）的单元提取方法，通过结合语言模型与TF-IDF以混合检索方式估算复杂表头，最后基于对比学习提升性能，在NTCIR-18任务集上取得较优效果。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在表格问答中难以直接捕捉复杂表结构，导致答题表现有限，提高TQA的自动化与准确率十分必要。

Method: 提出基于问题与单元格相似度的混合检索机制，通过集成语言模型与TF-IDF估算问题相关表头，无需人工处理复杂表头。用对比学习在小规模问句-表头配对数据上微调语言模型提升性能。最终选取最相关行和列交点的单元为答案。

Result: 在NTCIR-18 TQA数据集上，该方法准确率达到74.6%，显著超过如GPT-4o mini（63.9%）等现有LLM。

Conclusion: 所提出方法在表格问答领域表现优异，具有明显提升潜力。未来计划引入更高效的文本检索模型以进一步提升性能并缩小与人工表现的差距。

Abstract: Recently, Large Language Models (LLMs) are gaining increased attention in the domain of Table Question Answering (TQA), particularly for extracting information from tables in documents. However, directly entering entire tables as long text into LLMs often leads to incorrect answers because most LLMs cannot inherently capture complex table structures. In this paper, we propose a cell extraction method for TQA without manual identification, even for complex table headers. Our approach estimates table headers by computing similarities between a given question and individual cells via a hybrid retrieval mechanism that integrates a language model and TF-IDF. We then select as the answer the cells at the intersection of the most relevant row and column. Furthermore, the language model is trained using contrastive learning on a small dataset of question-header pairs to enhance performance. We evaluated our approach in the TQA dataset from the U4 shared task at NTCIR-18. The experimental results show that our pipeline achieves an accuracy of 74.6\%, outperforming existing LLMs such as GPT-4o mini~(63.9\%). In the future, although we used traditional encoder models for retrieval in this study, we plan to incorporate more efficient text-search models to improve performance and narrow the gap with human evaluation results.

</details>


### [105] [Context is Enough: Empirical Validation of $\textit{Sequentiality}$ on Essays](https://arxiv.org/abs/2511.09185)
*Amal Sunny,Advay Gupta,Vishnu Sreekumar*

Main category: cs.CL

TL;DR: 本论文通过实验证实以上下文为基础的sequentiality（连贯性）度量方法更优，并在自动作文评分等任务中有较强预测力。


<details>
  <summary>Details</summary>
Motivation: 原有的sequentiality方法结合主题项和上下文项来度量叙事流畅度，但受到主题选择方式影响并缺乏与人工流畅度标准的比对，准确性受到质疑。本文旨在验证仅用上下文项的连贯性度量是否更科学有效。

Method: 作者利用两个带有人工打分的作文数据集（ASAP++ 和 ELLIPSE），用以对比三种方法：仅上下文、仅主题、原有的组合方法，并分析这些特征与人工打分结果（如篇章组织与衔接）的关联度。同时，比较它们与零样本LLM直接打分的表现，并考察这些特征对自动评分模型性能的增益。

Result: 上下文连贯性指标与人工评价的组织性和衔接性等篇章级特征更加一致；单用上下文项时，其与标准语言特征结合后，对评分的预测力超过单用主题项或原有组合方法，并且整体优于零样本LLM预测。

Conclusion: 以上下文为基础的连贯性度量不仅更可解释、经验证有效，而且能增补常规语言特征和LLM表现，适合作为自动作文评分及相关NLP任务中的实用特征。

Abstract: Recent work has proposed using Large Language Models (LLMs) to quantify narrative flow through a measure called sequentiality, which combines topic and contextual terms. A recent critique argued that the original results were confounded by how topics were selected for the topic-based component, and noted that the metric had not been validated against ground-truth measures of flow. That work proposed using only the contextual term as a more conceptually valid and interpretable alternative. In this paper, we empirically validate that proposal. Using two essay datasets with human-annotated trait scores, ASAP++ and ELLIPSE, we show that the contextual version of sequentiality aligns more closely with human assessments of discourse-level traits such as Organization and Cohesion. While zero-shot prompted LLMs predict trait scores more accurately than the contextual measure alone, the contextual measure adds more predictive value than both the topic-only and original sequentiality formulations when combined with standard linguistic features. Notably, this combination also outperforms the zero-shot LLM predictions, highlighting the value of explicitly modeling sentence-to-sentence flow. Our findings support the use of context-based sequentiality as a validated, interpretable, and complementary feature for automated essay scoring and related NLP tasks.

</details>


### [106] [The Learning Dynamics of Subword Segmentation for Morphologically Diverse Languages](https://arxiv.org/abs/2511.09197)
*Francois Meyer,Jan Buys*

Main category: cs.CL

TL;DR: 本论文探讨了子词分割在模型训练过程中的动态学习，通过扩展子词分段语言模型（SSLM），比较三种不同形态语言在预训练和微调阶段子词分割的变化，发现可学习的子词方法有助于提升复杂低资源语言的文本生成和跨语言迁移能力。


<details>
  <summary>Details</summary>
Motivation: 传统子词分割一般作为预处理手段固定不变，但动态优化分词可能提升模型表现。本研究旨在理解如果在训练中允许语言模型动态优化分词，子词粒度和边界会如何随训练阶段演变，尤其是在不同形态结构的语言上。

Method: 作者将SSLM框架扩展，使其支持预训练和微调，并在三种形态类型不同的语言（isi-Xhosa、Setswana和英语）上训练模型，对子词分割的演化动态进行语言学分析，包括形态结构、生成力和分词能力等指标。

Result: 研究发现：1）子词学习可以分为四个阶段；2）形态复杂的isi-Xhosa子词边界更不稳定；3）微调阶段子词边界更细化。

Conclusion: 可学习子词的方法适用于提升低资源和形态复杂语言的文本生成与跨语言迁移能力，对形态结构复杂的语言表现出巨大潜力。

Abstract: Subword segmentation is typically applied in preprocessing and stays fixed during training. Alternatively, it can be learned during training to optimise the training objective. In this paper we study the learning dynamics of subword segmentation: if a language model can dynamically optimise tokenisation, how do its subwords evolve during pretraining and finetuning? To explore this, we extend the subword segmental language model (SSLM), a framework for learning subwords during training, to support pretraining and finetuning. We train models for three typologically diverse languages to study learning dynamics across the morphological spectrum: Isi-Xhosa is conjunctive (long word forms composed of many morphemes), Setswana is disjunctive (morphemes written as separate words), and English represents a typological middle ground. We analyse subword dynamics from a linguistic perspective, tracking morphology, productivity, and fertility. We identify four stages of subword learning, with the morphologically complex isi-Xhosa exhibiting greater instability. During finetuning, subword boundaries shift to become finer-grained. Lastly, we show that learnable subwords offers a promising approach to improve text generation and cross-lingual transfer for low-resource, morphologically complex languages.

</details>


### [107] [Pretraining Finnish ModernBERTs](https://arxiv.org/abs/2511.09213)
*Akseli Reunamo,Laura-Maria Peltonen,Hans Moen,Sampo Pyysalo*

Main category: cs.CL

TL;DR: 本文介绍了对ModernBERT编码器模型在六种不同规模下（51M到475M参数）的预训练，重点关注多语言有限性及芬兰相关语言。新模型在多语言任务上表现优越，尤其在需处理512个token以上上下文任务上表现超过单一语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有多语言BERT模型在部分语言，特别是芬兰及其相关语言的适应性与表现有限，同时在长文本任务上的能力也待提升。因此，作者希望开发更适合芬兰语区，并能处理更长文本的新型多语言BERT模型。

Method: 作者训练了六种不同规模的ModernBERT编码器模型，并进行了多语言有限性设计，主要覆盖芬兰相关的语言。实验研究还探究了最终训练阶段使用不同数据对模型表现的影响，并将这些模型与现有多语言与单语言模型进行了对比评测。

Result: 所提出的ModernBERT模型在相关多语言任务上达到或超过当前最优模型表现。对于需处理超过512 tokens上下文的任务，本文模型显著优于单语言模型。通过实证分析，还得出了不同训练数据选择对模型性能的影响。

Conclusion: ModernBERT模型不仅在多语言环境（尤其是芬兰相关语言）上表现优越，而且在长文本任务中展现出对单语言模型的显著优势。作者公开了代码与模型，为相关研究提供了便利。

Abstract: This paper reports on pretraining ModernBERT encoder models in six different sizes, ranging from 51M to 475M parameters, with a focus on limited multilingualism, emphasizing languages relevant to Finland. Our models are competitive with, or superior to, existing multilingual models. They outperform monolingual models on tasks that require a context longer than 512 tokens. We present empirical results on using different data in the final stage of training. The code and models are publicly released.

</details>


### [108] [Stabilizing Reinforcement Learning for Honesty Alignment in Language Models on Deductive Reasoning](https://arxiv.org/abs/2511.09222)
*Jiarui Liu,Kaustubh Dhole,Yingheng Wang,Haoyang Wen,Sarah Zhang,Haitao Mao,Gaotang Li,Neeraj Varshney,Jingguo Liu,Xiaoman Pan*

Main category: cs.CL

TL;DR: 本文提出了一种新的强化学习方法Anchor，通过在训练过程中注入真实轨迹，解决了RLVR框架在复杂推理任务中因负奖励主导而导致的训练崩溃问题，显著提升了语言模型在可验证推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习方法（RLVR）大多仅优化最终任务结果，训练初期负奖励主导会导致模型训练崩溃，尤其是在需要模型不仅解题还要识别无法得出结论的“honesty alignment”场景。为了提升模型在复杂推理任务中的稳健性和可靠性，有必要提出新的训练方法。

Method: 作者构建了两个多步演绎推理数据集（线性代数、逻辑推理），并通过扰动引入无法回答的情况，系统评估了GRPO等现有方法及课程学习等稳定化策略。为解决现有方法在负奖励主导任务下的不稳定性，作者提出了Anchor方法，在训练早期将部分真实（ground truth）推理轨迹注入到模型rollout中，引导模型避免陷入训练崩溃。

Result: 实验表明，GRPO即使结合有监督微调初始化，依然难以胜任该任务。课程学习策略需高度控制数据难度才有部分改善。Anchor方法显著提升了三个模型在上述数据集上的稳定性和推理性能，有效防止了早期崩溃。

Conclusion: RLVR框架在处理多步、需诚实识别不可回答情形的复杂推理任务时，训练动态至关重要。注入真实轨迹的Anchor方法可有效提升训练稳定性与推理能力，为可靠对齐的语言模型推理能力提供了新方案。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently emerged as a promising framework for aligning language models with complex reasoning objectives. However, most existing methods optimize only for final task outcomes, leaving models vulnerable to collapse when negative rewards dominate early training. This challenge is especially pronounced in honesty alignment, where models must not only solve answerable queries but also identify when conclusions cannot be drawn from the given premises. Deductive reasoning provides an ideal testbed because it isolates reasoning capability from reliance on external factual knowledge. To investigate honesty alignment, we curate two multi-step deductive reasoning datasets from graph structures, one for linear algebra and one for logical inference, and introduce unanswerable cases by randomly perturbing an edge in half of the instances. We find that GRPO, with or without supervised fine tuning initialization, struggles on these tasks. Through extensive experiments across three models, we evaluate stabilization strategies and show that curriculum learning provides some benefit but requires carefully designed in distribution datasets with controllable difficulty. To address these limitations, we propose Anchor, a reinforcement learning method that injects ground truth trajectories into rollouts, preventing early training collapse. Our results demonstrate that this method stabilizes learning and significantly improves the overall reasoning performance, underscoring the importance of training dynamics for enabling reliable deductive reasoning in aligned language models.

</details>


### [109] [POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation](https://arxiv.org/abs/2511.09232)
*Xuanchen Li,Chenrui Cui,Tianrui Wang,Meng Ge,Zikang Huang,Jin Li,Yizhou Peng,Longbiao Wang,Jianwu Dang,Nyima Tashi*

Main category: cs.CL

TL;DR: 提出了一种新的语音对齐框架POTSA，通过结合跨语言的语音对齐和最优传输约束，提高了多语种语音到文本翻译的表现，尤其是在低资源和零样本语言上。


<details>
  <summary>Details</summary>
Motivation: 现有的语音大语言模型在多语种语音转文本任务中，常常忽略了不同源语言之间的语义共性，导致部分语言的翻译效果偏差大，尤其是在资源稀缺的语言上表现不佳。作者希望通过更好地对齐不同语言的语音表示，提升各语言间的整体翻译质量。

Method: 提出了POTSA（Parallel Optimal Transport for Speech Alignment）框架，包括：1) 使用Bias Compensation模块进行初步语音表示对齐；2) 基于并行语音对，对Q-Former应用token级别的最优传输（OT）约束，实现细粒度的一致性；3) 采用层调度策略，将OT约束集中于语义效果最显著的网络层。

Result: 在FLEURS数据集上的实验显示，POTSA方法在五种主流语言上平均提升0.93 BLEU分，在零样本语言上提升5.05 BLEU分，同时每个源语言仅需10小时的并行语音数据。

Conclusion: POTSA框架显著提升了多语种语音到文本翻译质量，尤其在资源有限和零样本语言环境下效果突出，对跨语言语音建模提供了新思路。

Abstract: Speech Large Language Models (SpeechLLMs) have achieved breakthroughs in multilingual speech-to-text translation (S2TT). However, existing approaches often overlook semantic commonalities across source languages, leading to biased translation performance. In this work, we propose \textbf{POTSA} (Parallel Optimal Transport for Speech Alignment), a new framework based on cross-lingual parallel speech pairs and Optimal Transport (OT), designed to bridge high- and low-resource translation gaps. First, we introduce a Bias Compensation module to coarsely align initial speech representations across languages. Second, we impose token-level OT constraints on a Q-Former using parallel speech pairs to establish fine-grained consistency of representations. Then, we apply a layer scheduling strategy to focus OT constraints on the most semantically beneficial layers. Experiments on the FLEURS dataset show that our method achieves SOTA performance, with +0.93 BLEU on average over five common languages and +5.05 BLEU on zero-shot languages, using only 10 hours of parallel speech per source language.

</details>


### [110] [C$^3$TG: Conflict-aware, Composite, and Collaborative Controlled Text Generation](https://arxiv.org/abs/2511.09292)
*Yu Li,Zhe Yang,Yi Huang,Xin Liu,Guilin Qi*

Main category: cs.CL

TL;DR: 本文提出了一种名为C$^3$TG的新框架，实现对大语言模型生成文本的多属性、细粒度控制，同时无需对模型架构进行复杂修改或大量微调。


<details>
  <summary>Details</summary>
Motivation: 目前的大语言模型在文本生成方面表现出色，但对生成文本的多个属性进行精细控制非常困难。现有方法通常只能切换单一属性，且在属性冲突时缺乏协调机制，无法实现有效的多维度控制。此外，现有方法没有迭代优化的流程，因此难以精确实现多维度控制和自然流畅生成。

Method: C$^3$TG是一个两阶段控制框架。生成阶段将LLM与需要的属性分类器(共17类)进行选择性配对，通过加权KL散度调整生成token概率。在优化阶段，通过结合分类器得分及惩罚项的能量函数进行反馈迭代，解决属性冲突，优化文本输出，保持文本自然流畅。

Result: 实验表明：在多个指标（包括属性准确率、语言流畅性和输出多样性）上，C$^3$TG均显著优于现有基线方法，同时降低了文本毒性。

Conclusion: C$^3$TG为多维度文本属性控制提供了高效灵活的解决方案，不需要昂贵的模型结构修改，显著提升了多属性控制能力。

Abstract: Recent advancements in large language models (LLMs) have demonstrated remarkable text generation capabilities. However, controlling specific attributes of generated text remains challenging without architectural modifications or extensive fine-tuning. Current methods typically toggle a single, basic attribute but struggle with precise multi-attribute control. In scenarios where attribute requirements conflict, existing methods lack coordination mechanisms, causing interference between desired attributes. Furthermore, these methods fail to incorporate iterative optimization processes in the controlled generation pipeline. To address these limitations, we propose Conflict-aware, Composite, and Collaborative Controlled Text Generation (C$^3$TG), a two-phase framework for fine-grained, multi-dimensional text attribute control. During generation, C$^3$TG selectively pairs the LLM with the required attribute classifiers from the 17 available dimensions and employs weighted KL-divergence to adjust token probabilities. The optimization phase then leverages an energy function combining classifier scores and penalty terms to resolve attribute conflicts through iterative feedback, enabling precise control over multiple dimensions simultaneously while preserving natural text flow. Experiments show that C$^3$TG significantly outperforms baselines across multiple metrics including attribute accuracy, linguistic fluency, and output diversity, while simultaneously reducing toxicity. These results establish C$^3$TG as an effective and flexible solution for multi-dimensional text attribute control that requires no costly model modifications.

</details>


### [111] [LiteraryTaste: A Preference Dataset for Creative Writing Personalization](https://arxiv.org/abs/2511.09310)
*John Joon Young Chung,Vishakh Padmakumar,Melissa Roemmele,Yi Wang,Yuqian Sun,Tiffany Wang,Shm Garanganao Almeda,Brett A. Halperin,Yuwen Lu,Max Kreminski*

Main category: cs.CL

TL;DR: 本文提出了用于研究创意写作偏好的数据集LiteraryTaste，展示了用户在偏好上存在显著差异，并探讨了利用个体和集体偏好微调模型的效果。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型（LLM）创意写作任务往往忽视个体用户的写作偏好，而这些偏好具有差异性，挖掘并建模这些个体偏好有助于提升模型的个性化表现。

Method: 作者收集了60位人群的文学偏好数据，每人提供自陈的阅读习惯和偏好，以及对100对短文的实际偏好选择（揭示偏好），据此构建LiteraryTaste数据集。利用该数据集，微调transformer编码器并分析结果。还采用LLM驱动的解释性流程来分析偏好差异。

Result: 1）个体在创意写作偏好上存在分歧；2）针对个人和集体揭示偏好微调transformer可达75.8%和67.7%的准确率；3）自陈偏好对建模实际偏好的作用有限。

Conclusion: 数据集揭示了创意写作个体偏好重要性与差异性，微调可有效建模个体与集体偏好，未来可基于此推动创意写作技术的个性化。

Abstract: People have different creative writing preferences, and large language models (LLMs) for these tasks can benefit from adapting to each user's preferences. However, these models are often trained over a dataset that considers varying personal tastes as a monolith. To facilitate developing personalized creative writing LLMs, we introduce LiteraryTaste, a dataset of reading preferences from 60 people, where each person: 1) self-reported their reading habits and tastes (stated preference), and 2) annotated their preferences over 100 pairs of short creative writing texts (revealed preference). With our dataset, we found that: 1) people diverge on creative writing preferences, 2) finetuning a transformer encoder could achieve 75.8% and 67.7% accuracy when modeling personal and collective revealed preferences, and 3) stated preferences had limited utility in modeling revealed preferences. With an LLM-driven interpretability pipeline, we analyzed how people's preferences vary. We hope our work serves as a cornerstone for personalizing creative writing technologies.

</details>


### [112] [Towards Explainable Khmer Polarity Classification](https://arxiv.org/abs/2511.09313)
*Marry Kong,Rina Buoy,Sovisal Chenda,Nguonly Taing*

Main category: cs.CL

TL;DR: 本文提出了一种可解释的高棉语情感极性分类模型，并公开了相关数据集和模型。


<details>
  <summary>Details</summary>
Motivation: 当前的高棉语情感极性分类模型虽能判断文本标签（正面、负面、中立），但无法解释其预测依据，缺少可解释性。

Method: 利用指令式推理的Qwen-3模型进行微调，使模型在给出情感标签的同时生成自我解释，阐述预测理由。构建了包含不同风格高棉表达的新型情感数据集，并结合启发式规则和人工审核。

Result: 微调后的模型能准确预测情感标签，同时自动找出支持预测理由的关键词或短语，提升了可解释性。

Conclusion: 该方法提升了高棉语情感极性分类的解释能力，促进了相关研究发展，公开数据与模型资源有助于社区后续研究。

Abstract: Khmer polarity classification is a fundamental natural language processing task that assigns a positive, negative, or neutral label to a given Khmer text input. Existing Khmer models typically predict the label without explaining the rationale behind the prediction. This paper proposes an explainable Khmer polarity classifier by fine-tuning an instruction-based reasoning Qwen-3 model. The notion of explainability in this paper is limited to self-explanations, which the model uses to rationalize its predictions. Experimental results show that the fine-tuned model not only predicts labels accurately but also provides reasoning by identifying polarity-related keywords or phrases to support its predictions. In addition, we contribute a new Khmer polarity dataset consisting of short- to medium-length casual, romanized, and mixed-code Khmer expressions. This dataset was constructed using both heuristic rules and human curation and is publicly available through a gated Hugging Face repository (rinabuoy/khmerpolarity_nonreasoning). The fine-tuned Qwen-3 models are also made available in the same Hugging Face account.

</details>


### [113] [AdaptDel: Adaptable Deletion Rate Randomized Smoothing for Certified Robustness](https://arxiv.org/abs/2511.09316)
*Zhuoqun Huang,Neil G. Marchant,Olga Ohrimenko,Benjamin I. P. Rubinstein*

Main category: cs.CL

TL;DR: 本文提出一种针对序列分类任务下编辑距离扰动的认证鲁棒性新方法，通过引入自适应删除率（AdaptDel），提升了对序列扰动的容忍能力，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有针对序列（如自然语言句子）认证鲁棒性的方法多基于定值删除率，难以应对变长输入，导致性能欠佳。作者致力于解决编辑距离扰动下的认证不足问题，提高对自然语言处理等变长输入的鲁棒性认证水平。

Method: 作者提出AdaptDel方法，根据输入特性动态调整删除率，扩展了随机平滑（randomized smoothing）理论以适应变量删除率机制，保证对编辑距离的理论认证。

Result: 在自然语言任务上，AdaptDel在认证区域的中值基数上相比最先进的方法实现了高达30个数量级的提升，实验结果表明其认证范围显著扩大。

Conclusion: 适应性删除策略能有效提升变长输入下序列分类的鲁棒认证能力，拓宽了被认证的扰动范围，为自然语言处理等领域提供了更强健的鲁棒认证保障。

Abstract: We consider the problem of certified robustness for sequence classification against edit distance perturbations. Naturally occurring inputs of varying lengths (e.g., sentences in natural language processing tasks) present a challenge to current methods that employ fixed-rate deletion mechanisms and lead to suboptimal performance. To this end, we introduce AdaptDel methods with adaptable deletion rates that dynamically adjust based on input properties. We extend the theoretical framework of randomized smoothing to variable-rate deletion, ensuring sound certification with respect to edit distance. We achieve strong empirical results in natural language tasks, observing up to 30 orders of magnitude improvement to median cardinality of the certified region, over state-of-the-art certifications.

</details>


### [114] [mmJEE-Eval: A Bilingual Multimodal Benchmark for Evaluating Scientific Reasoning in Vision-Language Models](https://arxiv.org/abs/2511.09339)
*Arka Mukherjee,Shreya Ghosh*

Main category: cs.CL

TL;DR: 本文提出mmJEE-Eval基准，用更高难度和推理深度的题目真实衡量和区分视觉-语言模型（VLMs）在科学推理和问题求解上的能力，结果显示现有模型难以胜任。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言多模态推理基准未能真正区分模型是否具备科学推理能力，大多偏向模式匹配。作者希望通过更复杂、更接近真实教育考试情境的题目检验模型的极限能力。

Method: 作者构建了一个包含1460道源自印度JEE Advanced考试（2019-2025覆盖）的多模态双语（英-印地语）基准，涵盖物理、化学和数学三大学科，并评测了17种主流VLM，比较封闭与开源模型在此基准上的表现，同时使用消融实验分析基准的难度来源。

Result: 顶尖封闭模型（如GPT-5、Gemini 2.5 Pro/Flash）在新题上准确率为77-84%，但开源模型即便大规模扩展（至400B参数）仅为37-45%。进一步，Google和OpenAI的模型虽能高分通过多次尝试（pass@3），但一旦需要元认知推理能力则表现极差。

Conclusion: mmJEE-Eval能有效区分模型的真实推理及训练方法优劣，相比现有基准更具挑战性。当前市面领先模型在遇到推理更复杂问题时尚未达到人类专家水平。作者公开数据和代码，促进该领域进步。

Abstract: Contemporary vision-language models (VLMs) perform well on existing multimodal reasoning benchmarks (78-85\% accuracy on MMMU, MathVista). Yet, these results fail to sufficiently distinguish true scientific reasoning articulation capabilities from pattern-matching. To address this gap, we introduce \textbf{mmJEE-Eval}, a multimodal bilingual (English and Hindi) benchmark comprising 1,460 questions from India's JEE Advanced examination (2019-2025) spanning pre-college Physics, Chemistry, and Mathematics domains. Our evaluation of 17 state-of-the-art models reveals that while frontier VLMs (GPT-5, Gemini 2.5 Pro/Flash) achieve 77-84\% accuracy on held-out 2025 questions, open-source models plateau at 37-45\% despite scaling to 400B parameters, a significant difference not observed on existing benchmarks. While closed frontiers from Google and OpenAI show high problem-solving accuracies (up to 100\% pass@3 scores), they fully collapse when the reasoning load is increased meta-cognitively (GPT-5 fixes just 5.2\% errors). Systematic ablations show mmJEE-Eval's difficulty stems from complexity and reasoning depth rather than memorization. Effectively, our benchmark segregates superior training and reasoning methodologies where alternatives fail. We publicly release our code and data: https://mmjee-eval.github.io

</details>


### [115] [Seer Self-Consistency: Advance Budget Estimation for Adaptive Test-Time Scaling](https://arxiv.org/abs/2511.09345)
*Shiyu Ji,Yixuan Wang,Yijun Liu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: SeerSC提出一种动态自洽推理框架，有效减少LLM推理时的token消耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理过程中的扩展性提升虽然提升了推理性能，但带来了较大的计算开销和高延迟，即使已有动态自洽方法减少token消耗，但延迟问题尚未解决。

Method: SeerSC框架结合了System 1和System 2两种推理方式：先借助System 1对答案的熵进行快速评估，用于判断样本是否有必要扩展推理（dynamic scaling），再由System 2实现自洽推理，同时并行生成，有效降低token消耗和延迟。

Result: SeerSC在实验中相比现有方法，最高可减少47%的token消耗，推理延迟下降43%，且保持性能无显著下降。

Conclusion: SeerSC方法在降低计算资源消耗和延迟方面显著优于现有动态自洽方法，是一种高效实用的LLM推理优化方案。

Abstract: Test-time scaling improves the inference performance of Large Language Models (LLMs) but also incurs substantial computational costs. Although recent studies have reduced token consumption through dynamic self-consistency, they remain constrained by the high latency of sequential requests. In this paper, we propose SeerSC, a dynamic self-consistency framework that simultaneously improves token efficiency and latency by integrating System 1 and System 2 reasoning. Specifically, we utilize the rapid System 1 to compute the answer entropy for given queries. This score is then used to evaluate the potential of samples for scaling, enabling dynamic self-consistency under System 2. Benefiting from the advance and accurate estimation provided by System 1, the proposed method can reduce token usage while simultaneously achieving a significant decrease in latency through parallel generation. It outperforms existing methods, achieving up to a 47% reduction in token consumption and a 43% reduction in inference latency without significant performance loss.

</details>


### [116] [Spider4SSC & S2CLite: A text-to-multi-query-language dataset using lightweight ontology-agnostic SPARQL to Cypher parser](https://arxiv.org/abs/2511.09354)
*Martin Vejvar,Yasutaka Fujimoto*

Main category: cs.CL

TL;DR: 本文提出了Spider4SSC数据集和S2CLite解析工具，S2CLite能高效将SPARQL查询转换为Cypher查询，并开源了工具和数据集。


<details>
  <summary>Details</summary>
Motivation: 当前将SPARQL查询转换为Cypher查询的工具通常依赖于复杂系统、需要特定本体或RDF图，且解析准确率较低，因此需要一种高效、轻量且更准确的转换工具。

Method: 提出了S2CLite解析器，该工具基于纯规则、与本体无关，灵感来自传统编译器，无需RDF图或外部工具直接将SPARQL转换为Cypher，并通过实验与现有方法进行了对比。

Result: S2CLite在Spider4SPARQL数据集上的解析准确率为77.8%，显著高于S2CTrans的44.2%；交集查询的执行准确率为96.6%，高出7.3%。此外，利用S2CLite生成了一个包含SQL、SPARQL、Cypher等多语言并等价查询的大型数据集Spider4SSC。

Conclusion: S2CLite是一款高效、轻量且准确的SPARQL转Cypher工具，并与Spider4SSC数据集一起为文本到多种查询语言的研究提供了重要资源，相关工具与数据已开源，可支持后续研究。

Abstract: We present Spider4SSC dataset and S2CLite parsing tool. S2CLite is a lightweight, ontology-agnostic parser that translates SPARQL queries into Cypher queries, enabling both in-situ and large-scale SPARQL to Cypher translation. Unlike existing solutions, S2CLite is purely rule-based (inspired by traditional programming language compilers) and operates without requiring an RDF graph or external tools. Experiments conducted on the BSBM42 and Spider4SPARQL datasets show that S2CLite significantly reduces query parsing errors, achieving a total parsing accuracy of 77.8% on Spider4SPARQL compared to 44.2% by the state-of-the-art S2CTrans. Furthermore, S2CLite achieved a 96.6\% execution accuracy on the intersecting subset of queries parsed by both parsers, outperforming S2CTrans by 7.3%. We further use S2CLite to parse Spider4SPARQL queries to Cypher and generate Spider4SSC, a unified Text-to-Query language (SQL, SPARQL, Cypher) dataset with 4525 unique questions and 3 equivalent sets of 2581 matching queries (SQL, SPARQL and Cypher). We open-source S2CLite for further development on GitHub (github.com/vejvarm/S2CLite) and provide the clean Spider4SSC dataset for download.

</details>


### [117] [MTQ-Eval: Multilingual Text Quality Evaluation for Language Models](https://arxiv.org/abs/2511.09374)
*Rhitabrat Pokharel,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 本文提出了一个多语言文本质量评估框架MTQ-Eval，能够更好地判断各种语言的文本质量，并提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型在特定任务中表现出色，但对通用、多语言文本质量的评估能力尚不明确，存在不足。

Method: 研究提出了MTQ-Eval，通过自动生成高低质量文本偏好数据，并用公开模型进行训练，使模型能区分高低质量文本，并拟合人工评价。

Result: MTQ-Eval在115种语言上进行测试，显示其评估多语言文本质量时性能优于以往方法。同时，评估能力的提升也促进了下游任务的表现。

Conclusion: MTQ-Eval显著提升了多语言文本质量评估能力，并间接提升相关下游任务表现，展示了其在通用文本质量和多语言环境下的应用潜力。

Abstract: The use of large language models (LLMs) for evaluating outputs is becoming an increasingly effective and scalable approach. However, it remains uncertain whether this capability extends beyond task-specific evaluations to more general assessments of text quality, particularly in multilingual contexts. In this study, we introduce, MTQ-Eval, a novel framework for multilingual text quality evaluation that learns from examples of both high- and low-quality texts, adjusting its internal representations. To develop MTQ-Eval, we first automatically generate text quality preference data and then use it to train open-source base LLMs to align with ratings of high- and low-quality text. Our comprehensive evaluation across 115 languages demonstrates the improved performance of the proposed model. Upon further analysis, we find that this enhanced evaluation capability also leads to notable improvements in downstream tasks.

</details>


### [118] [Self-Correcting Large Language Models: Generation vs. Multiple Choice](https://arxiv.org/abs/2511.09381)
*Hossein A. Rahmani,Satyapriya Krishna,Xi Wang,Mohammadmehdi Naghiaei,Emine Yilmaz*

Main category: cs.CL

TL;DR: 本文分析了大语言模型在进行自我修正时，两种不同任务范式（开放式生成与多选题选择）下的表现与机制差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能够通过自我反思与迭代优化答案，但这种自我修正机制在不同任务类型（如开放式文本生成与多选问题选择）中的表现尚不明确。研究者希望系统地比较这两种范式，以指导更好地设计LLM的自我修正方法。

Method: 作者对多种主流大语言模型，覆盖不同规模和家族，在多项自然语言理解与推理任务上，分别测试并对比了开放式生成与多选题选择两种模式下的性能趋势与纠错行为。

Result: 实验显示开放式生成因可灵活重构和分步完善，普遍获得更多改进空间；而多选题则受益于明确的选项边界，但部分受限于题目所给选项。同时，两种模式各自暴露出不同的失败方式和瓶颈。

Conclusion: 自我修正机制的设计须充分考虑任务结构和输出类型间的互动，这对基于知识推理和决策的LLM应用都具有重要启示意义。

Abstract: Large language models have recently demonstrated remarkable abilities to self-correct their responses through iterative refinement, often referred to as self-consistency or self-reflection. However, the dynamics of this self-correction mechanism may differ substantially depending on whether the model is tasked with open-ended text generation or with selecting the most appropriate response from multiple predefined options. In this paper, we conduct a systematic investigation of these two paradigms by comparing performance trends and error-correction behaviors across various natural language understanding and reasoning tasks, covering language models of different scales and families. Our experimental results reveal distinct patterns of improvement and failure modes:
  \textit{While open-ended generation often benefits from the flexibility of re-interpretation and compositional refinement, multiple-choice selection can leverage clearer solution boundaries but may be limited by the provided options}. This contrast also reflects the dual demands faced by emerging agentic LLM applications: effective agents must not only generate and refine open-ended plans or explanations, but also make reliable discrete choices when operating within constrained action spaces. Our findings, therefore, highlight that the design of self-correction mechanisms should take into account the interaction between task structure and output space, with implications for both knowledge-intensive reasoning and decision-oriented applications of LLMs.

</details>


### [119] [AMaPO: Adaptive Margin-attached Preference Optimization for Language Model Alignment](https://arxiv.org/abs/2511.09385)
*Ruibo Deng,Duanyu Feng,Wenqiang Lei*

Main category: cs.CL

TL;DR: 本文提出了一种新的离线偏好优化方法AMaPO，有效解决了偏好学习中的过拟合-欠拟合困境，显著提升了排序准确率和模型对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有的离线偏好优化方法依赖于排序准确率，但现有方法存在过拟合于已正确排序样本和对错误排序样本调整不足的问题，即过拟合-欠拟合困境。作者希望提出一种方法，改善排序准确率，从而提升模型整体表现。

Method: 提出AMaPO方法，即自适应边界偏好优化算法。该方法对每个样本动态调整间隔（margin），通过Z-score归一化与指数缩放机制，对错误排序样本加大梯度，对正确排序样本减小梯度，智能分配学习资源。

Result: 在多个常用基准测试集上，AMaPO在排序准确率及下游对齐性能上均优于现有方法。进一步分析表明，AMaPO能够显著缓解过拟合和欠拟合问题。

Conclusion: AMaPO是一种简单而有效的算法，能解决现有偏好优化中的核心难题，为大语言模型的偏好学习与对齐提供了更稳定和高效的选择。

Abstract: Offline preference optimization offers a simpler and more stable alternative to RLHF for aligning language models. However, their effectiveness is critically dependent on ranking accuracy, a metric where further gains are highly impactful. This limitation arises from a fundamental problem that we identify and formalize as the Overfitting-Underfitting Dilemma: current margin designs cause models to apply excessive, wasteful gradients to correctly ranked samples (overfitting) while providing insufficient corrective signals for misranked ones (underfitting). To resolve this dilemma, we propose Adaptive Margin-attached Preference Optimization (AMaPO), a simple yet principled algorithm. AMaPO employs an instance-wise adaptive margin, refined by Z-normalization and exponential scaling, which dynamically reallocates learning effort by amplifying gradients for misranked samples and suppressing them for correct ones. Extensive experiments on widely used benchmarks demonstrate that AMaPO not only achieves better ranking accuracy and superior downstream alignment performance, but targeted analysis also confirms that it successfully mitigates the core overfitting and underfitting issues.

</details>


### [120] [Multimodal Large Language Models for Low-Resource Languages: A Case Study for Basque](https://arxiv.org/abs/2511.09396)
*Lukas Arana,Julen Etxaniz,Ander Salaberria,Gorka Azkune*

Main category: cs.CL

TL;DR: 本文开发了面向低资源语言（巴斯克语）的多模态大语言模型，并开放了相关资源。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型在商业上已对低资源语言有较好支持，但开源领域尚未达到类似水平。作者希望提升低资源语言（巴斯克语）的MLLM能力。

Method: 作者自行构建了图文多模态训练与评估数据集，使用两个不同的LLM（Llama-3.1-Instruct和巴斯克语适配版Latxa）为骨干，探索了多种数据配比进行训练。

Result: 实验证明：1）仅20%的巴斯克语多模态数据即可获得很好的基准测试表现；2）即使没有巴斯克语专用预训练骨干，依然能训练出强大的巴斯克语MLLM。

Conclusion: 该工作通过方法和数据集开放，为其他低资源语言开发多模态大模型提供了参考和便利。

Abstract: Current Multimodal Large Language Models exhibit very strong performance for several demanding tasks. While commercial MLLMs deliver acceptable performance in low-resource languages, comparable results remain unattained within the open science community. In this paper, we aim to develop a strong MLLM for a low-resource language, namely Basque. For that purpose, we develop our own training and evaluation image-text datasets. Using two different Large Language Models as backbones, the Llama-3.1-Instruct model and a Basque-adapted variant called Latxa, we explore several data mixtures for training. We show that: i) low ratios of Basque multimodal data (around 20%) are already enough to obtain solid results on Basque benchmarks, and ii) contrary to expected, a Basque instructed backbone LLM is not required to obtain a strong MLLM in Basque. Our results pave the way to develop MLLMs for other low-resource languages by openly releasing our resources.

</details>


### [121] [CARE-Bench: A Benchmark of Diverse Client Simulations Guided by Expert Principles for Evaluating LLMs in Psychological Counseling](https://arxiv.org/abs/2511.09407)
*Bichen Wang,Yixin Sun,Junzhe Wang,Hao Yang,Xing Fu,Yanyan Zhao,Si Wei,Shijin Wang,Bing Qin*

Main category: cs.CL

TL;DR: 本文提出了CARE-Bench，一个动态互动的心理咨询大模型评测基准，以评估各类大模型在心理咨询场景下的能力。


<details>
  <summary>Details</summary>
Motivation: 现有心理咨询服务供不应求，应用大语言模型(LLM)进行心理咨询受到关注，但现有评测方法存在模拟不专业、评价单一等问题，难以真实反映模型处理复杂咨询场景的能力，因此需要更科学的评测基准。

Method: 本研究构建了CARE-Bench评测系统：基于真实案例和专家指导，模拟多样化的来访者档案，并以动态互动方式测试模型。同时，使用多维度心理学量表进行模型能力评估。

Result: 用CARE-Bench对多种通用及专业心理咨询大模型进行了评测，揭示了它们在与不同类型来访者互动时的不足。并通过与心理学家合作，详细分析了这些不足出现的原因。

Conclusion: CARE-Bench为未来设计更高效、覆盖更广、效果更好的心理咨询大模型指明了方向，为模型改进和行业发展奠定基础。

Abstract: The mismatch between the growing demand for psychological counseling and the limited availability of services has motivated research into the application of Large Language Models (LLMs) in this domain. Consequently, there is a need for a robust and unified benchmark to assess the counseling competence of various LLMs. Existing works, however, are limited by unprofessional client simulation, static question-and-answer evaluation formats, and unidimensional metrics. These limitations hinder their effectiveness in assessing a model's comprehensive ability to handle diverse and complex clients. To address this gap, we introduce \textbf{CARE-Bench}, a dynamic and interactive automated benchmark. It is built upon diverse client profiles derived from real-world counseling cases and simulated according to expert guidelines. CARE-Bench provides a multidimensional performance evaluation grounded in established psychological scales. Using CARE-Bench, we evaluate several general-purpose LLMs and specialized counseling models, revealing their current limitations. In collaboration with psychologists, we conduct a detailed analysis of the reasons for LLMs' failures when interacting with clients of different types, which provides directions for developing more comprehensive, universal, and effective counseling models.

</details>


### [122] [GSAP-ERE: Fine-Grained Scholarly Entity and Relation Extraction Focused on Machine Learning](https://arxiv.org/abs/2511.09411)
*Wolfgang Otto,Lu Gan,Sharmila Upadhyaya,Saurav Karmakar,Stefan Dietze*

Main category: cs.CL

TL;DR: 本文提出了GSAP-ERE数据集，用于细粒度的信息抽取任务，能提升对机器学习科研文献的自动理解和可复现性监测。实验证明，经过微调的模型在实体和关系抽取上远优于当前大型语言模型的提示策略。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习和人工智能领域研究进展迅速，需要对相关科研文献进行大规模信息抽取，以助于理解和提高科研结果的可复现性。细粒度的信息（如方法训练细节与数据使用）难以自动化提取，这激发了对高质量数据集的需求。

Method: 作者人工构建了GSAP-ERE数据集，涵盖10种实体类型和18种语义关系类型，总计含有63,000个实体和35,000个关系标注，覆盖100篇完整机器学习论文。作者使用该数据集对信息抽取模型进行了微调，并与大型语言模型的提示式抽取方法进行了对比。

Result: 微调后的基线模型在命名实体识别（NER）和关系抽取（RE）上分别取得80.6%和54%的F1分数，显著优于利用大型语言模型直接提示的结果（44.4%和10.1%）。同时，GSAP-ERE数据集还可用于知识图谱构建等下游任务。

Conclusion: GSAP-ERE数据集为学术信息抽取提供了高质量的资源，有助于推动该领域的研究发展。实验结果表明，在当前阶段，基于数据集的有监督模型明显优于无监督大型语言模型方案，因此类似GSAP-ERE的数据集对于学术信息抽取研究不可或缺。

Abstract: Research in Machine Learning (ML) and AI evolves rapidly. Information Extraction (IE) from scientific publications enables to identify information about research concepts and resources on a large scale and therefore is a pathway to improve understanding and reproducibility of ML-related research. To extract and connect fine-grained information in ML-related research, e.g. method training and data usage, we introduce GSAP-ERE. It is a manually curated fine-grained dataset with 10 entity types and 18 semantically categorized relation types, containing mentions of 63K entities and 35K relations from the full text of 100 ML publications. We show that our dataset enables fine-tuned models to automatically extract information relevant for downstream tasks ranging from knowledge graph (KG) construction, to monitoring the computational reproducibility of AI research at scale. Additionally, we use our dataset as a test suite to explore prompting strategies for IE using Large Language Models (LLM). We observe that the performance of state-of-the-art LLM prompting methods is largely outperformed by our best fine-tuned baseline model (NER: 80.6%, RE: 54.0% for the fine-tuned model vs. NER: 44.4%, RE: 10.1% for the LLM). This disparity of performance between supervised models and unsupervised usage of LLMs suggests datasets like GSAP-ERE are needed to advance research in the domain of scholarly information extraction.

</details>


### [123] [BIG5-TPoT: Predicting BIG Five Personality Traits, Facets, and Items Through Targeted Preselection of Texts](https://arxiv.org/abs/2511.09426)
*Triet M. Le,Arjun Chandra,C. Anton Rytting,Valerie P. Karuzis,Vladimir Rife,William A. Simpson*

Main category: cs.CL

TL;DR: 本文提出一种通过语义筛选文本来提升个体性格预测的策略TPoT（Targeted Preselection of Texts），能提高大模型性格预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 面对大体量生成文本时，用文本预测个体性格十分困难，尤其是输入文本量较大时大模型效果受限，因此需要新的方法提高预测性能。

Method: 提出TPoT方法，优先筛选与特定性格（Big Five人格特质、维度或具体项目）语义相关的文本，仅将这些筛选后文本输入深度学习模型（BIG5-TPoT），以缓解输入限制并提升预测效果。

Result: TPoT策略在Stream of Consciousness Essays数据集上，提升了Mean Absolute Error（平均绝对误差）和准确率。

Conclusion: 通过TPoT语义筛选能够缓解大模型的输入限制，显著提升性格预测的有效性和准确性。

Abstract: Predicting an individual's personalities from their generated texts is a challenging task, especially when the text volume is large. In this paper, we introduce a straightforward yet effective novel strategy called targeted preselection of texts (TPoT). This method semantically filters the texts as input to a deep learning model, specifically designed to predict a Big Five personality trait, facet, or item, referred to as the BIG5-TPoT model. By selecting texts that are semantically relevant to a particular trait, facet, or item, this strategy not only addresses the issue of input text limits in large language models but also improves the Mean Absolute Error and accuracy metrics in predictions for the Stream of Consciousness Essays dataset.

</details>


### [124] [Readability Measures and Automatic Text Simplification: In the Search of a Construct](https://arxiv.org/abs/2511.09536)
*Rémi Cardon,A. Seza Doğruöz*

Main category: cs.CL

TL;DR: 本文探讨了自动文本简化（ATS）中的可读性指标与自动评估指标及人工判断之间的相关性，发现三者相关性较低，呼吁对ATS评价构建更清晰的定义。


<details>
  <summary>Details</summary>
Motivation: 虽然近年有研究关注ATS自动评价指标与人工判断之间的关系，但很少有研究关注这些与传统可读性指标之间的关系。因此，本文旨在填补可读性度量在自动文本简化领域中的研究空白。

Method: 首先梳理和分析了ATS与可读性研究的关联；然后，在英文数据上系统性地考察了可读性指标与人工判断、以及可读性指标与ATS自动评价指标之间的相关性。

Result: 结果发现，可读性指标与自动评价指标及人工判断之间的相关性普遍较低。三类评价视角之间并无强相关。

Conclusion: 由于自动评价、人工评判和可读性度量三者相关性不高，说明自动文本简化的评价标准尚不统一，急需对其评价构建进行明确定义和进一步研究。

Abstract: Readability is a key concept in the current era of abundant written information. To help making texts more readable and make information more accessible to everyone, a line of researched aims at making texts accessible for their target audience: automatic text simplification (ATS). Lately, there have been studies on the correlations between automatic evaluation metrics in ATS and human judgment. However, the correlations between those two aspects and commonly available readability measures (such as readability formulas or linguistic features) have not been the focus of as much attention. In this work, we investigate the place of readability measures in ATS by complementing the existing studies on evaluation metrics and human judgment, on English. We first discuss the relationship between ATS and research in readability, then we report a study on correlations between readability measures and human judgment, and between readability measures and ATS evaluation metrics. We identify that in general, readability measures do not correlate well with automatic metrics and human judgment. We argue that as the three different angles from which simplification can be assessed tend to exhibit rather low correlations with one another, there is a need for a clear definition of the construct in ATS.

</details>


### [125] [SynClaimEval: A Framework for Evaluating the Utility of Synthetic Data in Long-Context Claim Verification](https://arxiv.org/abs/2511.09539)
*Mohamed Elaraby,Jyoti Prakash Maheswari*

Main category: cs.CL

TL;DR: 本文提出了SynClaimEval框架，用于评估大语言模型在处理长文本时合成数据的有效性，尤其针对事实核查和幻觉检测任务。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽有潜力直接理解长文档，但人工构建大规模带标注训练和评估集十分昂贵。因此，需要一种可扩展的替代方案帮助训练和评估。

Method: 本文利用合成数据，通过设计SynClaimEval框架，从输入特征（如上下文长度）、合成逻辑（如主张复杂度和错误类型）、和解释质量三个维度系统评估合成数据在长文本事实核查中的作用。同时在不同基准集上开展实验。

Result: 实验证明，合成的长文本场景在提升基础大模型的事实核查能力上优于仅用人工数据，尤其是在将人工数据与合成数据结合时。此外，即使验证分数未明显提升，合成数据也能提升模型解释的一致性与证据性。

Conclusion: 合成数据不仅提升了大模型对长文本事实核查的表现，还强化了其解释能力，展示了在构建高质量、可扩展训练集上的巨大潜力。

Abstract: Large Language Models (LLMs) with extended context windows promise direct reasoning over long documents, reducing the need for chunking or retrieval. Constructing annotated resources for training and evaluation, however, remains costly. Synthetic data offers a scalable alternative, and we introduce SynClaimEval, a framework for evaluating synthetic data utility in long-context claim verification -- a task central to hallucination detection and fact-checking. Our framework examines three dimensions: (i) input characteristics, by varying context length and testing generalization to out-of-domain benchmarks; (ii) synthesis logic, by controlling claim complexity and error type variation; and (iii) explanation quality, measuring the degree to which model explanations provide evidence consistent with predictions. Experiments across benchmarks show that long-context synthesis can improve verification in base instruction-tuned models, particularly when augmenting existing human-written datasets. Moreover, synthesis enhances explanation quality, even when verification scores do not improve, underscoring its potential to strengthen both performance and explainability.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [126] [Practical and Performant Enhancements for Maximization of Algebraic Connectivity](https://arxiv.org/abs/2511.08694)
*Leonard Jung,Alan Papalia,Kevin Doherty,Michael Everett*

Main category: cs.RO

TL;DR: 本文针对大规模长期图的状态估计问题，对当前最优的最大化代数连通度（MAC）图稀疏化算法进行改进，提出了更高效的求解器、优化步长策略和自动保障连通性的机制，实现了2倍提速，更适合在线与实时估计应用。


<details>
  <summary>Details</summary>
Motivation: 现有的图稀疏化方法，即使能最大化代数连通度，仍存在计算量大、不能实时应用以及需要手工指定连通边集等不足，因此需要提升其效率与自动化程度。

Method: 1. 设计了专用的代数连通度求解器，加速优化过程；2. 探索和优化了MAC的步长策略，提高收敛速度和解质量；3. 提出自动化维护图连通性的方案，免去手动指定连通边集的需要。

Result: 新方法实现了平均2倍的运行加速，同时提升了优化收敛速度与解质量，并能自动保障图的连通性，无需手工指定边集。

Conclusion: 改进后的MAC算法大幅提高了扩展性、稳定性和自动化水平，更加适用于大规模、在线和实时的图状态估计场景。

Abstract: Long-term state estimation over graphs remains challenging as current graph estimation methods scale poorly on large, long-term graphs. To address this, our work advances a current state-of-the-art graph sparsification algorithm, maximizing algebraic connectivity (MAC). MAC is a sparsification method that preserves estimation performance by maximizing the algebraic connectivity, a spectral graph property that is directly connected to the estimation error. Unfortunately, MAC remains computationally prohibitive for online use and requires users to manually pre-specify a connectivity-preserving edge set. Our contributions close these gaps along three complementary fronts: we develop a specialized solver for algebraic connectivity that yields an average 2x runtime speedup; we investigate advanced step size strategies for MAC's optimization procedure to enhance both convergence speed and solution quality; and we propose automatic schemes that guarantee graph connectivity without requiring manual specification of edges. Together, these contributions make MAC more scalable, reliable, and suitable for real-time estimation applications.

</details>


### [127] [Intuitive Programming, Adaptive Task Planning, and Dynamic Role Allocation in Human-Robot Collaboration](https://arxiv.org/abs/2511.08732)
*Marta Lagomarsino,Elena Merlo,Andrea Pupa,Timo Birr,Franziska Krebs,Cristian Secchi,Tamim Asfour,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文综述了实现人机协作中直观信息交流和技能转移的关键要素，并分析了全过程中的主要技术环节。


<details>
  <summary>Details</summary>
Motivation: 尽管机器人与AI已掌握复杂任务，但人类在协作中的作用常常被动，且机器人若不能准确理解和适应人类状态及意图，就难以在实际环境中发挥最大潜力。因此，建立双向直观交流成为推动高效人机协作的重要动机。

Method: 文章回顾并梳理了人-机器人间信息交流全流程，包括：多模态输入转化为机器人可理解的信息、适应性规划与角色分配、控制层实现及反馈机制等，着重强调环路闭合。

Result: 系统地识别和界定了实现直观、高效人机信息交流与技能转移的关键环节与方法，探讨了当下发展趋势及未来的有前景方向。

Conclusion: 更自适应和易于访问的人机协作需实现双向、高效直观的信息沟通，涉及多模态输入、智能角色分配、透明反馈等创新技术。文章为未来HRC系统设计指明了发展方向。

Abstract: Remarkable capabilities have been achieved by robotics and AI, mastering complex tasks and environments. Yet, humans often remain passive observers, fascinated but uncertain how to engage. Robots, in turn, cannot reach their full potential in human-populated environments without effectively modeling human states and intentions and adapting their behavior. To achieve a synergistic human-robot collaboration (HRC), a continuous information flow should be established: humans must intuitively communicate instructions, share expertise, and express needs. In parallel, robots must clearly convey their internal state and forthcoming actions to keep users informed, comfortable, and in control. This review identifies and connects key components enabling intuitive information exchange and skill transfer between humans and robots. We examine the full interaction pipeline: from the human-to-robot communication bridge translating multimodal inputs into robot-understandable representations, through adaptive planning and role allocation, to the control layer and feedback mechanisms to close the loop. Finally, we highlight trends and promising directions toward more adaptive, accessible HRC.

</details>


### [128] [ATOM-CBF: Adaptive Safe Perception-Based Control under Out-of-Distribution Measurements](https://arxiv.org/abs/2511.08741)
*Kai S. Yun,Navid Azizan*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的安全控制框架ATOM-CBF，能够自适应感知模型在遇到训练外分布测量（OoD）时的认知不确定性，实现无标签下的安全保障。


<details>
  <summary>Details</summary>
Motivation: 现实系统依赖于感知模块从高维传感数据推断系统状态，但当前感知模块在出现未见过的分布（OoD）时，容易因认知不确定性导致失效，带来安全风险。现有方法难以在遇到分布变化时动态适应并保障安全。

Method: 提出ATOM-CBF，包括：1）一个具备OoD感知能力的自适应感知误差边界；2）结合该边界的安全过滤器，可实时调整自身保守性，无需依赖于真实标签或已知分布偏移信息。

Result: 在仿真中对F1Tenth小车（使用LiDAR）和四足机器人（使用RGB图像）进行实验证明，ATOM-CBF能够在多种感知场景下有效保持系统安全。

Conclusion: ATOM-CBF为应对感知分布外场景中的安全控制提供了一种切实可行和有效的解决方案，并且不依赖人工标签或分布变化的先验，具备良好的通用性和应用前景。

Abstract: Ensuring the safety of real-world systems is challenging, especially when they rely on learned perception modules to infer the system state from high-dimensional sensor data. These perception modules are vulnerable to epistemic uncertainty, often failing when encountering out-of-distribution (OoD) measurements not seen during training. To address this gap, we introduce ATOM-CBF (Adaptive-To-OoD-Measurement Control Barrier Function), a novel safe control framework that explicitly computes and adapts to the epistemic uncertainty from OoD measurements, without the need for ground-truth labels or information on distribution shifts. Our approach features two key components: (1) an OoD-aware adaptive perception error margin and (2) a safety filter that integrates this adaptive error margin, enabling the filter to adjust its conservatism in real-time. We provide empirical validation in simulations, demonstrating that ATOM-CBF maintains safety for an F1Tenth vehicle with LiDAR scans and a quadruped robot with RGB images.

</details>


### [129] [CENIC: Convex Error-controlled Numerical Integration for Contact](https://arxiv.org/abs/2511.08771)
*Vince Kurtz,Alejandro Castro*

Main category: cs.RO

TL;DR: 本文提出了一种新型连续时间误差控制积分器CENIC，可以在保证高准确率的同时，实现与主流离散时间机器人仿真器类似的实时仿真速度。


<details>
  <summary>Details</summary>
Motivation: 现有机器人仿真器大都采用离散时间，需要手动设置步长，步长选择不当会导致非物理现象或仿真过慢。连续时间误差控制积分虽能动态调整步长保证精度，但对涉及接触的刚性动力学计算效率较低，无法满足现代机器人对速度和可扩展性的需求。

Method: 作者提出了CENIC这种连续时间积分器，结合了凸时间步进和误差控制积分器的最新进展，使其既具备连续积分的精度优势，也能利用离散时间步进方法的效率。

Result: CENIC实现了与MuJoCo、Drake和Isaac Sim等主流离散时间仿真器相当的实时速度，同时具备精度和收敛性的理论保证。

Conclusion: CENIC为刚性接触动力学提供了高效、可控精度、且适合现代机器人应用的大规模仿真解决方案，兼具连续和离散时间仿真优势。

Abstract: State-of-the-art robotics simulators operate in discrete time. This requires users to choose a time step, which is both critical and challenging: large steps can produce non-physical artifacts, while small steps force the simulation to run slowly. Continuous-time error-controlled integration avoids such issues by automatically adjusting the time step to achieve a desired accuracy. But existing error-controlled integrators struggle with the stiff dynamics of contact, and cannot meet the speed and scalability requirements of modern robotics workflows. We introduce CENIC, a new continuous-time integrator that brings together recent advances in convex time-stepping and error-controlled integration, inheriting benefits from both continuous integration and discrete time-stepping. CENIC runs at fast real-time rates comparable to discrete-time robotics simulators like MuJoCo, Drake and Isaac Sim, while also providing guarantees on accuracy and convergence.

</details>


### [130] [Dual-Arm Whole-Body Motion Planning: Leveraging Overlapping Kinematic Chains](https://arxiv.org/abs/2511.08778)
*Richard Cheng,Peter Werner,Carolyn Matl*

Main category: cs.RO

TL;DR: 针对高自由度双臂机器人在动态未知环境中运动规划难的问题，提出利用共享关节结构的动态路标法，有效规避维数灾难，显著提升规划速度和成功率。


<details>
  <summary>Details</summary>
Motivation: 随着双臂机器人在现实环境中的应用增多，如何在高维空间、复杂约束下实现实时运动规划成为瓶颈，尤其是在环境未知且不断变化时。现有方法受限于“维数灾难”，难以高效应用。

Method: 提出针对双臂机器人共享关节（如躯干关节）的结构性质，将运动学链分解为左右臂+躯干，分别为其构建动态路标（DRM），通过共享关节结构将两条路标高效组合，实现高效搜索与规划。

Result: 在一个19自由度的双臂移动机器人平台上，运行提出方法完成超市拣货任务，平均规划时间达0.4秒，2000多次运动规划成功率达99.9%。

Conclusion: 利用共享结构分解并高效组合动态路标能大幅缓解高维空间运动规划的计算压力，实现复杂环境下的实时高效运动规划。

Abstract: High degree-of-freedom dual-arm robots are becoming increasingly common due to their morphology enabling them to operate effectively in human environments. However, motion planning in real-time within unknown, changing environments remains a challenge for such robots due to the high dimensionality of the configuration space and the complex collision-avoidance constraints that must be obeyed. In this work, we propose a novel way to alleviate the curse of dimensionality by leveraging the structure imposed by shared joints (e.g. torso joints) in a dual-arm robot. First, we build two dynamic roadmaps (DRM) for each kinematic chain (i.e. left arm + torso, right arm + torso) with specific structure induced by the shared joints. Then, we show that we can leverage this structure to efficiently search through the composition of the two roadmaps and largely sidestep the curse of dimensionality. Finally, we run several experiments in a real-world grocery store with this motion planner on a 19 DoF mobile manipulation robot executing a grocery fulfillment task, achieving 0.4s average planning times with 99.9% success rate across more than 2000 motion plans.

</details>


### [131] [Low-cost Multi-agent Fleet for Acoustic Cooperative Localization Research](https://arxiv.org/abs/2511.08822)
*Nelson Durrant,Braden Meyers,Matthew McMurray,Clayton Smith,Brighton Anderson,Tristan Hodgins,Kalliyan Velasco,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 本文提出了一种低成本、可配置的多智能体自主水下机器人平台CoUGARs，用于多智能体自治研究，并通过仿真和实地水下测试验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的多智能体水下载体测试成本高且工程难度大，限制了相关研究的开展。因此，急需一种低成本、易于定制的水下机器人平台以降低实验门槛、推动多智能体自治领域的发展。

Method: 设计并实现了基于商用元件和3D打印部件的CoUGARs水下机器人，单台成本低于3000美元。平台支持多种传感器配置，包括DVL和USBL声学阵列/换能器，用于协作定位。配套开发了软件栈（状态估计、导航、声通信），并与HoloOcean仿真器集成。对平台进行了仿真和实际湖泊水库环境下的测试。

Result: CoUGARs系统成功完成了仿真环境和实际水下环境的测试，证明该平台具备用于多智能体协作研究的能力和可行性。

Conclusion: CoUGARs为水下多智能体自治领域提供了低成本、高可定制性的硬件基础，有望促进广泛的实验和应用研究。

Abstract: Real-world underwater testing for multi-agent autonomy presents substantial financial and engineering challenges. In this work, we introduce the Configurable Underwater Group of Autonomous Robots (CoUGARs) as a low-cost, configurable autonomous-underwater-vehicle (AUV) platform for multi-agent autonomy research. The base design costs less than $3,000 USD (as of May 2025) and is based on commercially-available and 3D-printed parts, enabling quick customization for various sensor payloads and configurations. Our current expanded model is equipped with a doppler velocity log (DVL) and ultra-short-baseline (USBL) acoustic array/transducer to support research on acoustic-based cooperative localization. State estimation, navigation, and acoustic communications software has been developed and deployed using a containerized software stack and is tightly integrated with the HoloOcean simulator. The system was tested both in simulation and via in-situ field trials in Utah lakes and reservoirs.

</details>


### [132] [XPRESS: X-Band Radar Place Recognition via Elliptical Scan Shaping](https://arxiv.org/abs/2511.08863)
*Hyesu Jang,Wooseong Yang,Ayoung Kim,Dongje Lee,Hanguen Kim*

Main category: cs.RO

TL;DR: 本文提出了一种专为X波段雷达设计的定位识别算法，提高海事自主导航中雷达数据的利用效率，有效提升了定位精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: X波段雷达在船舶中广泛使用，但因分辨率低和信息量不充足，限制了其在自主导航中的应用。作者希望开发一种能克服这些缺陷，实现仅依赖X波段雷达自主导航的方法。

Method: 提出了一种融合基于目标密度的候选选择规则和对雷达探测结果有意降质的地点识别算法。通过这些手段提高了雷达特征匹配的鲁棒性。算法在公开和自采的海事雷达数据集上评估，并与先进方法对比，同时做了消融实验分析关键参数影响。

Result: 所提算法在不同数据集上的评测结果优于主流雷达定位识别方法，验证了其高效性与鲁棒性。消融实验也揭示了参数对性能的影响。

Conclusion: 基于目标密度选择和有意降质机制的X波段雷达定位识别算法，可以大幅提升自主导航场景中雷达应用的性能，为雷达单一感知自主导航奠定了基础。

Abstract: X-band radar serves as the primary sensor on maritime vessels, however, its application in autonomous navigation has been limited due to low sensor resolution and insufficient information content. To enable X-band radar-only autonomous navigation in maritime environments, this paper proposes a place recognition algorithm specifically tailored for X-band radar, incorporating an object density-based rule for efficient candidate selection and intentional degradation of radar detections to achieve robust retrieval performance. The proposed algorithm was evaluated on both public maritime radar datasets and our own collected dataset, and its performance was compared against state-of-the-art radar place recognition methods. An ablation study was conducted to assess the algorithm's performance sensitivity with respect to key parameters.

</details>


### [133] [MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror](https://arxiv.org/abs/2511.08865)
*Cong Tai,Hansheng Wu,Haixu Long,Zhengbin Long,Zhaoyu Zheng,Haodong Xiang,Tao Shen*

Main category: cs.RO

TL;DR: 本文提出了一种基于PICO的机器人远程操作系统，实现了低成本、实时的手部动作和姿态数据采集，并兼容RealMirror生态系统，支持多种机器人末端执行器的遥操作，有助于构建视觉-语言-动作（VLA）数据集。


<details>
  <summary>Details</summary>
Motivation: 现有主流的视觉跟踪和动作捕捉方案成本较高、部署复杂，不利于大规模开展机器人操控和上肢精细操作相关的研究。作者希望降低技术门槛，加速VLA等交叉领域的发展。

Method: 系统基于PICO框架设计，实现实时采集手部动作和姿态数据，并与Isaac仿真环境和RealMirror生态系统无缝集成，支持多种不同机器人实际末端执行器的远程实时控制。

Result: 该系统在成本效益上优于主流视觉追踪和动作捕捉方案，能够稳定精确地记录机器人轨迹，且可实时遥操作灵巧手及机械夹爪类机器人。

Conclusion: 该框架有效降低了上肢机器人操作相关研究的技术门槛，有望推动视觉-语言-动作方向与机器人操作、数据集构建等前沿研究的发展。

Abstract: In this work, we present a PICO-based robot remote operating framework that enables low-cost, real-time acquisition of hand motion and pose data, outperforming mainstream visual tracking and motion capture solutions in terms of cost-effectiveness. The framework is natively compatible with the RealMirror ecosystem, offering ready-to-use functionality for stable and precise robotic trajectory recording within the Isaac simulation environment, thereby facilitating the construction of Vision-Language-Action (VLA) datasets. Additionally, the system supports real-time teleoperation of a variety of end-effector-equipped robots, including dexterous hands and robotic grippers. This work aims to lower the technical barriers in the study of upper-limb robotic manipulation, thereby accelerating advancements in VLA-related research.

</details>


### [134] [A Shared Control Framework for Mobile Robots with Planning-Level Intention Prediction](https://arxiv.org/abs/2511.08912)
*Jinyu Zhang,Lijun Han,Feng Jian,Lingxi Zhang,Hesheng Wang*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的基于高层规划意图预测的移动机器人共享控制框架，通过路径重规划算法结合意图域的引入，有效提升了人机协作的流畅性和安全性，并显著减轻了操作员工作负荷。


<details>
  <summary>Details</summary>
Motivation: 在移动机器人共享控制中，准确理解与预测人的运动意图是提升人机协作效率与安全的关键，现有方法普遍依赖大量人工演示数据或在意图推断上存在延迟，难以在无需人工参与下高效实现意图驱动的路径优化。

Method: 设计了包含意图域的新路径重规划算法，并将意图域预测与路径重规划问题统一建模为马尔可夫决策过程，运用深度强化学习方法联合求解。同时开发了基于Voronoi的生成算法，用于人工轨迹的仿真生成，实现系统全仿真训练，无需真人演示。

Result: 通过大量仿真实验和真实用户对比测试，结果显示该方法能够在降低操作员工作负担、提升安全性的同时，保持与现有辅助手术遥操作方法相当甚至更优的任务效率。

Conclusion: 所提出的共享控制框架能够更准确预测人意图并实时反馈在路径规划中，提升移动机器人协作的自然度、安全性和效率，且具有良好的实际落地和训练可扩展性。

Abstract: In mobile robot shared control, effectively understanding human motion intention is critical for seamless human-robot collaboration. This paper presents a novel shared control framework featuring planning-level intention prediction. A path replanning algorithm is designed to adjust the robot's desired trajectory according to inferred human intentions. To represent future motion intentions, we introduce the concept of an intention domain, which serves as a constraint for path replanning. The intention-domain prediction and path replanning problems are jointly formulated as a Markov Decision Process and solved through deep reinforcement learning. In addition, a Voronoi-based human trajectory generation algorithm is developed, allowing the model to be trained entirely in simulation without human participation or demonstration data. Extensive simulations and real-world user studies demonstrate that the proposed method significantly reduces operator workload and enhances safety, without compromising task efficiency compared with existing assistive teleoperation approaches.

</details>


### [135] [Expand Your SCOPE: Semantic Cognition over Potential-Based Exploration for Embodied Visual Navigation](https://arxiv.org/abs/2511.08935)
*Ningnan Wang,Weihuang Chen,Liming Chen,Haoxuan Ji,Zhongyu Guo,Xuchong Zhang,Hongbin Sun*

Main category: cs.RO

TL;DR: 提出了一种新的零样本视觉导航方法SCOPE，通过结合前沿信息和视觉-语言模型，实现了更有效的探索与目标相关决策，在多项任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往方法忽视了对视觉前沿边界信息的利用，导致无法优化探索与导航目标间的联系，从而影响长程规划与决策质量。

Method: SCOPE框架显式利用视觉前沿信息，通过视觉-语言模型估算探索潜力，并将其构建为时空潜力图，以支持边界动态感知和长远规划。此外，引入自我反思机制，定期回溯和修正早期探索决策，提升鲁棒性及降低过度自信错误。

Result: 在两个不同的实体导航任务上，SCOPE准确率比现有最佳方法高4.6%。系统性分析也佐证其核心组件提升了模型的决策校准、泛化能力及决策质量。

Conclusion: SCOPE证明显式建模边界信息与目标导向探索的有效性，为提升零样本导航智能体的规划和决策水平提供了新思路。

Abstract: Embodied visual navigation remains a challenging task, as agents must explore unknown environments with limited knowledge. Existing zero-shot studies have shown that incorporating memory mechanisms to support goal-directed behavior can improve long-horizon planning performance. However, they overlook visual frontier boundaries, which fundamentally dictate future trajectories and observations, and fall short of inferring the relationship between partial visual observations and navigation goals. In this paper, we propose Semantic Cognition Over Potential-based Exploration (SCOPE), a zero-shot framework that explicitly leverages frontier information to drive potential-based exploration, enabling more informed and goal-relevant decisions. SCOPE estimates exploration potential with a Vision-Language Model and organizes it into a spatio-temporal potential graph, capturing boundary dynamics to support long-horizon planning. In addition, SCOPE incorporates a self-reconsideration mechanism that revisits and refines prior decisions, enhancing reliability and reducing overconfident errors. Experimental results on two diverse embodied navigation tasks show that SCOPE outperforms state-of-the-art baselines by 4.6\% in accuracy. Further analysis demonstrates that its core components lead to improved calibration, stronger generalization, and higher decision quality.

</details>


### [136] [Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning](https://arxiv.org/abs/2511.08942)
*Mobin Habibpour,Fatemeh Afghah*

Main category: cs.RO

TL;DR: 本论文提出了一种将视觉-语言模型（VLM）从被动感知者转变为主动导航策略制定者的新方法，大幅提升了机器人导航的智能性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在机器人导航中的推理能力尚未被充分利用，限制了其在实际应用中的潜力。作者希望通过挖掘VLM的高阶推理与规划能力，为机器人导航提供更高效的方案。

Method: 提出一个框架，将高层次导航规划任务外包给VLM。核心方法包括：（1）结构化的链式思考提示（Chain-of-Thought prompting）实现逻辑化推理；（2）动态引入机器人近期行动历史以防止陷入循环；（3）创新性地让VLM同时解读俯视障碍地图和第一人称视角图像，以加强空间感知能力。

Result: 在HM3D、Gibson和MP3D等主流基准数据集上测试，所提出方法生成了极为直接且逻辑清晰的导航轨迹，相比现有方法显著提升了导航效率。

Conclusion: 通过让VLM承担主动策略制定角色并结合链式思考等手段，极大开发了其推理和规划潜力，为实现更智能的机器人导航奠定了基础。

Abstract: While Vision-Language Models (VLMs) are set to transform robotic navigation, existing methods often underutilize their reasoning capabilities. To unlock the full potential of VLMs in robotics, we shift their role from passive observers to active strategists in the navigation process. Our framework outsources high-level planning to a VLM, which leverages its contextual understanding to guide a frontier-based exploration agent. This intelligent guidance is achieved through a trio of techniques: structured chain-of-thought prompting that elicits logical, step-by-step reasoning; dynamic inclusion of the agent's recent action history to prevent getting stuck in loops; and a novel capability that enables the VLM to interpret top-down obstacle maps alongside first-person views, thereby enhancing spatial awareness. When tested on challenging benchmarks like HM3D, Gibson, and MP3D, this method produces exceptionally direct and logical trajectories, marking a substantial improvement in navigation efficiency over existing approaches and charting a path toward more capable embodied agents.

</details>


### [137] [UniMM-V2X: MoE-Enhanced Multi-Level Fusion for End-to-End Cooperative Autonomous Driving](https://arxiv.org/abs/2511.09013)
*Ziyi Song,Chen Xia,Chenbing Wang,Haibao Yu,Sheng Zhou,Zhisheng Niu*

Main category: cs.RO

TL;DR: 文章提出了一种新的端到端多智能体自动驾驶框架UniMM-V2X，通过多层次融合策略和Mixture-of-Experts架构，在感知、预测和规划各阶段实现协作，整体性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统由于单体智能的感知和决策能力有限，虽然多智能体协作方法已出现，但大多仅停留在感知层，未能有效对齐后续规划控制，或未利用新兴端到端自动驾驶的完整潜力。

Method: 提出UniMM-V2X框架，在感知、预测、规划三个阶段进行分层协作，设计多层次融合机制统一各智能体在感知和预测的协作，并通过Mixture-of-Experts（MoE）动态加强BEV表征，MoE同时被扩展到解码器以更好建模多样化运动模式。

Result: 在DAIR-V2X数据集上的实验显示，UniMM-V2X相比于UniV2X在感知准确率提高39.7%，预测误差降低7.2%，规划性能提升33.2%。

Conclusion: MoE增强的多层次协作机制显著提升了多智能体端到端自动驾驶的整体性能，更好地实现了感知、预测与规划各环节的协作与一致决策。

Abstract: Autonomous driving holds transformative potential but remains fundamentally constrained by the limited perception and isolated decision-making with standalone intelligence. While recent multi-agent approaches introduce cooperation, they often focus merely on perception-level tasks, overlooking the alignment with downstream planning and control, or fall short in leveraging the full capacity of the recent emerging end-to-end autonomous driving. In this paper, we present UniMM-V2X, a novel end-to-end multi-agent framework that enables hierarchical cooperation across perception, prediction, and planning. At the core of our framework is a multi-level fusion strategy that unifies perception and prediction cooperation, allowing agents to share queries and reason cooperatively for consistent and safe decision-making. To adapt to diverse downstream tasks and further enhance the quality of multi-level fusion, we incorporate a Mixture-of-Experts (MoE) architecture to dynamically enhance the BEV representations. We further extend MoE into the decoder to better capture diverse motion patterns. Extensive experiments on the DAIR-V2X dataset demonstrate our approach achieves state-of-the-art (SOTA) performance with a 39.7% improvement in perception accuracy, a 7.2% reduction in prediction error, and a 33.2% improvement in planning performance compared with UniV2X, showcasing the strength of our MoE-enhanced multi-level cooperative paradigm.

</details>


### [138] [A Quantum Tunneling and Bio-Phototactic Driven Enhanced Dwarf Mongoose Optimizer for UAV Trajectory Planning and Engineering Problem](https://arxiv.org/abs/2511.09020)
*Mingyang Yu,Haorui Yang,Kangning An,Xinjian Wei,Xiaoxuan Xu,Jing Xu*

Main category: cs.RO

TL;DR: 本文提出了一种增强型多策略矮猫鼬优化算法（EDMO），提升无人机在复杂环境下三维路径规划的效率和精度，表现优于多种先进算法，并经实际任务验证。


<details>
  <summary>Details</summary>
Motivation: 随着无人机应用的普及和环境复杂性的增加，现有路径规划方法在易陷入局部最优、解的多样性不足等方面存在挑战，因此亟需更智能、适应性强的新型优化算法。

Method: EDMO算法基于传统矮猫鼬优化，结合三种创新策略：动态量子隧穿优化（DQTOS），生物趋光动态聚焦搜索（BDFSS），正交透镜对立学习（OLOBL），以提升算法全局与局部搜索能力。算法在39个标准基准函数（CEC2017/2020）及实际无人机路径规划和工程设计任务上进行了测试对比。

Result: EDMO在基准测试中超过14种主流元启发式算法，在收敛速度、鲁棒性及最优寻优性能方面表现突出。同时，在真实无人机路径规划和工程设计应用中，也显示出较强实用性和优越性能。

Conclusion: EDMO算法有效解决了复杂环境下无人机三维路径规划中的局部收敛和解多样性难题，具有较好的工程实用前景，为无人系统智能规划提供了新方法。

Abstract: With the widespread adoption of unmanned aerial vehicles (UAV), effective path planning has become increasingly important. Although traditional search methods have been extensively applied, metaheuristic algorithms have gained popularity due to their efficiency and problem-specific heuristics. However, challenges such as premature convergence and lack of solution diversity still hinder their performance in complex scenarios. To address these issues, this paper proposes an Enhanced Multi-Strategy Dwarf Mongoose Optimization (EDMO) algorithm, tailored for three-dimensional UAV trajectory planning in dynamic and obstacle-rich environments. EDMO integrates three novel strategies: (1) a Dynamic Quantum Tunneling Optimization Strategy (DQTOS) to enable particles to probabilistically escape local optima; (2) a Bio-phototactic Dynamic Focusing Search Strategy (BDFSS) inspired by microbial phototaxis for adaptive local refinement; and (3) an Orthogonal Lens Opposition-Based Learning (OLOBL) strategy to enhance global exploration through structured dimensional recombination. EDMO is benchmarked on 39 standard test functions from CEC2017 and CEC2020, outperforming 14 advanced algorithms in convergence speed, robustness, and optimization accuracy. Furthermore, real-world validations on UAV three-dimensional path planning and three engineering design tasks confirm its practical applicability and effectiveness in field robotics missions requiring intelligent, adaptive, and time-efficient planning.

</details>


### [139] [SMF-VO: Direct Ego-Motion Estimation via Sparse Motion Fields](https://arxiv.org/abs/2511.09072)
*Sangheon Yang,Yeongin Yoon,Hong Mo Jung,Jongwoo Lim*

Main category: cs.RO

TL;DR: 本文提出了一种轻量级的视觉里程计新方法SMF-VO，绕过昂贵的姿态与地图优化，直接基于稀疏光流估算运动，效率高、性能好，适合资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 传统视觉/视觉惯性里程计由于依赖姿态估计和大规模地图优化，在资源受限设备上难以实时运行，因此亟需一种更高效的替代方法。

Method: SMF-VO采用运动场为核心，利用稀疏光流直接估算线速度和角速度，避免了显式的相机姿态估计和地标追踪。使用基于3D光线的运动场建模，兼容广角等多种相机模型。

Result: 在多种公开数据集上的实验表明，SMF-VO在仅用CPU的树莓派5上能实现每秒100帧以上，效率优于传统方法，精度也具有竞争力。

Conclusion: SMF-VO为视觉里程计提供了一种可扩展、效率高的替代方案，非常适合用于移动机器人与可穿戴设备等场景。

Abstract: Traditional Visual Odometry (VO) and Visual Inertial Odometry (VIO) methods rely on a 'pose-centric' paradigm, which computes absolute camera poses from the local map thus requires large-scale landmark maintenance and continuous map optimization. This approach is computationally expensive, limiting their real-time performance on resource-constrained devices. To overcome these limitations, we introduce Sparse Motion Field Visual Odometry (SMF-VO), a lightweight, 'motion-centric' framework. Our approach directly estimates instantaneous linear and angular velocity from sparse optical flow, bypassing the need for explicit pose estimation or expensive landmark tracking. We also employed a generalized 3D ray-based motion field formulation that works accurately with various camera models, including wide-field-of-view lenses. SMF-VO demonstrates superior efficiency and competitive accuracy on benchmark datasets, achieving over 100 FPS on a Raspberry Pi 5 using only a CPU. Our work establishes a scalable and efficient alternative to conventional methods, making it highly suitable for mobile robotics and wearable devices.

</details>


### [140] [D-AWSIM: Distributed Autonomous Driving Simulator for Dynamic Map Generation Framework](https://arxiv.org/abs/2511.09080)
*Shunsuke Ito,Chaoran Zhao,Ryo Okamura,Takuya Azumi*

Main category: cs.RO

TL;DR: 本文提出了一种分布式仿真平台D-AWSIM，用于大规模自动驾驶和传感器信息共享的仿真，有效提升了处理能力并支持新策略的研究。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶系统技术进步，要扩展其应用范围，必须保证在多样化环境下的安全。而依靠实地实验传感器布置成本高且监管受限，传统仿真平台难以模拟大规模场景。

Method: 提出D-AWSIM分布式仿真平台，通过多台机器协同，实现大规模区域的车辆和基础设施传感器仿真。同时开发了可支持信息共享策略研究的动态地图生成框架。

Result: D-AWSIM在车辆数量与激光雷达数据处理方面，相比单机仿真有明显吞吐提升；与Autoware集成，展示了平台的实用性。

Conclusion: D-AWSIM为大规模传感器部署和自动驾驶信息共享的研究提供了强有力的仿真基础，无需昂贵实地测试，有望推动相关技术发展。

Abstract: Autonomous driving systems have achieved significant advances, and full autonomy within defined operational design domains near practical deployment. Expanding these domains requires addressing safety assurance under diverse conditions. Information sharing through vehicle-to-vehicle and vehicle-to-infrastructure communication, enabled by a Dynamic Map platform built from vehicle and roadside sensor data, offers a promising solution. Real-world experiments with numerous infrastructure sensors incur high costs and regulatory challenges. Conventional single-host simulators lack the capacity for large-scale urban traffic scenarios. This paper proposes D-AWSIM, a distributed simulator that partitions its workload across multiple machines to support the simulation of extensive sensor deployment and dense traffic environments. A Dynamic Map generation framework on D-AWSIM enables researchers to explore information-sharing strategies without relying on physical testbeds. The evaluation shows that D-AWSIM increases throughput for vehicle count and LiDAR sensor processing substantially compared to a single-machine setup. Integration with Autoware demonstrates applicability for autonomous driving research.

</details>


### [141] [APEX: Action Priors Enable Efficient Exploration for Robust Motion Tracking on Legged Robots](https://arxiv.org/abs/2511.09091)
*Shivam Sood,Laukik Nakhwa,Sun Ge,Yuhong Cao,Jin Cheng,Fatemah Zargarbashi,Taerim Yoon,Sungjoon Choi,Stelian Coros,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: 提出了一种无需部署时参考数据的新方法APEX，利用专家动作先验提升四足机器人学习效率与泛化能力，已在仿真和实机上验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于演示的四足运动学习方法高度依赖参考数据，部署时灵活性低且需要复杂调试，难以适用于多样任务和环境。

Method: APEX为现有运动跟踪算法提供了即插即用的扩展。它通过引入衰减动作先验，将专家演示与强化学习结合：早期探索受专家演示引导，后期逐渐自主。配合多评论家机制，兼顾任务表现与动作风格，并可通过单一策略学会多种运动样式，适应不同地形速度。

Result: 在仿真与Unitree Go2实机中，大量实验验证APEX能提升学习效率、泛化能力和健壮性，对奖励设计不敏感，并能实现风格迁移。

Conclusion: APEX证明了在不引入部署时参考偏见情况下，动作先验可显著提升四足机器人自然运动技能习得。该方法为更广泛任务中的指导型强化学习提供了新范式。

Abstract: Learning natural, animal-like locomotion from demonstrations has become a core paradigm in legged robotics. Despite the recent advancements in motion tracking, most existing methods demand extensive tuning and rely on reference data during deployment, limiting adaptability. We present APEX (Action Priors enable Efficient Exploration), a plug-and-play extension to state-of-the-art motion tracking algorithms that eliminates any dependence on reference data during deployment, improves sample efficiency, and reduces parameter tuning effort. APEX integrates expert demonstrations directly into reinforcement learning (RL) by incorporating decaying action priors, which initially bias exploration toward expert demonstrations but gradually allow the policy to explore independently. This is combined with a multi-critic framework that balances task performance with motion style. Moreover, APEX enables a single policy to learn diverse motions and transfer reference-like styles across different terrains and velocities, while remaining robust to variations in reward design. We validate the effectiveness of our method through extensive experiments in both simulation and on a Unitree Go2 robot. By leveraging demonstrations to guide exploration during RL training, without imposing explicit bias toward them, APEX enables legged robots to learn with greater stability, efficiency, and generalization. We believe this approach paves the way for guidance-driven RL to boost natural skill acquisition in a wide array of robotic tasks, from locomotion to manipulation. Website and code: https://marmotlab.github.io/APEX/.

</details>


### [142] [Decoupling Torque and Stiffness: A Unified Modeling and Control Framework for Antagonistic Artificial Muscles](https://arxiv.org/abs/2511.09104)
*Amirhossein Kazemipour,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 本文提出了一个统一的实时控制框架，实现了对多种人工肌肉（PAMs、HASELs、DEAs）驱动的软致动器的力矩-刚度独立控制。该方法提高了控制精度，优化了与环境的交互表现。


<details>
  <summary>Details</summary>
Motivation: 现有的软致动器往往难以在动态接触情况下实现力矩与刚度的独立、高效控制，这限制了其在复杂环境交互中的应用，尤其是在机器人与人之间的安全互动。

Method: 作者提出了统一的力学模型，并开发了带有解析逆动力学的级联控制器。通过共收缩/偏置坐标，将力矩控制和刚度控制分离仿生人类生理机制实现。该方法适用于多种软致动器，在亚毫秒级别完成运算。

Result: 仿真验证显示，该控制框架在软表面上的稳定时间提升至200倍，在硬表面上的作用力降低了81%，交互稳定性明显优于传统固定策略（提升至100%，而原策略为22-54%）。

Conclusion: 本文框架解决了软致动肌肉力矩-刚度独立调控的难题，为机器人安全自适应阻抗控制和人机协作奠定了重要基础。

Abstract: Antagonistic soft actuators built from artificial muscles (PAMs, HASELs, DEAs) promise plant-level torque-stiffness decoupling, yet existing controllers for soft muscles struggle to maintain independent control through dynamic contact transients. We present a unified framework enabling independent torque and stiffness commands in real-time for diverse soft actuator types. Our unified force law captures diverse soft muscle physics in a single model with sub-ms computation, while our cascaded controller with analytical inverse dynamics maintains decoupling despite model errors and disturbances. Using co-contraction/bias coordinates, the controller independently modulates torque via bias and stiffness via co-contraction-replicating biological impedance strategies. Simulation-based validation through contact experiments demonstrates maintained independence: 200x faster settling on soft surfaces, 81% force reduction on rigid surfaces, and stable interaction vs 22-54% stability for fixed policies. This framework provides a foundation for enabling musculoskeletal antagonistic systems to execute adaptive impedance control for safe human-robot interaction.

</details>


### [143] [Data Assessment for Embodied Intelligence](https://arxiv.org/abs/2511.09119)
*Jiahao Xiao,Bowen Yan,Jianbo Zhang,Jia Wang,Chunyi Li,Zhengxue Cheng,Guangtao Zhai*

Main category: cs.RO

TL;DR: 本文提出了两种新工具，用于评估具身智能领域中数据集的信息量和可学习性，实现对数据集质量更直观和高效的评估。


<details>
  <summary>Details</summary>
Motivation: 具身数据集的多模态特性使得评估其信息量和模型可学性非常困难，现有方法主要关注于多样性但不全面，且可学习性评估依赖于耗时且后置的训练过程，缺乏解释性与可操作性。为解决这些痛点，作者尝试提出新的评估方法。

Method: 1）为每个数据样本构建统一的多模态表示，并基于该表示提出“多样性熵”来连续量化数据集的信息量；2）提出首个无需训练即可高效定量评估数据集可学习性的可解释算法。这些工具均为数据驱动型，并在仿真和真实具身数据集上进行了验证。

Result: 实验结果表明，作者提出的方法可以真实、有效地反映数据集的多样性和可学习性特征，并为提升数据集质量提供有意义的指导建议。

Conclusion: 本文工具为具身智能领域数据集的多样性与可学习性评估提供了基础，未来有助于推动高质量数据集的设计和领域发展。

Abstract: In embodied intelligence, datasets play a pivotal role, serving as both a knowledge repository and a conduit for information transfer. The two most critical attributes of a dataset are the amount of information it provides and how easily this information can be learned by models. However, the multimodal nature of embodied data makes evaluating these properties particularly challenging. Prior work has largely focused on diversity, typically counting tasks and scenes or evaluating isolated modalities, which fails to provide a comprehensive picture of dataset diversity. On the other hand, the learnability of datasets has received little attention and is usually assessed post-hoc through model training, an expensive, time-consuming process that also lacks interpretability, offering little guidance on how to improve a dataset. In this work, we address both challenges by introducing two principled, data-driven tools. First, we construct a unified multimodal representation for each data sample and, based on it, propose diversity entropy, a continuous measure that characterizes the amount of information contained in a dataset. Second, we introduce the first interpretable, data-driven algorithm to efficiently quantify dataset learnability without training, enabling researchers to assess a dataset's learnability immediately upon its release. We validate our algorithm on both simulated and real-world embodied datasets, demonstrating that it yields faithful, actionable insights that enable researchers to jointly improve diversity and learnability. We hope this work provides a foundation for designing higher-quality datasets that advance the development of embodied intelligence.

</details>


### [144] [RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation](https://arxiv.org/abs/2511.09141)
*Xuetao Li,Wenke Huang,Nengyuan Pan,Kaiyan Zhao,Songhua Yang,Yiming Wang,Mengde Li,Mang Ye,Jifeng Xuan,Miao Li*

Main category: cs.RO

TL;DR: 提出了一种新的端到端框架（RGMP），结合几何和语义推理，提高了类人机器人在不同场景下的数据效率与通用性。


<details>
  <summary>Details</summary>
Motivation: 当前类人机器人多依赖大规模数据驱动，忽视了几何推理，且在训练资源消耗上效率低，尤其是在未见场景下泛化能力不足。

Method: 提出一种循环几何先验多模态策略（RGMP），通过几何先验技能选择器，将几何归纳偏置融入视觉语言模型以生成技能序列，并借助自适应递归高斯网络高效建模机器人与目标的空间关系，实现数据高效的运动合成。

Result: 在类人机器人和台式双臂机器人上测试，RGMP在通用性测试中达到87%的任务成功率，数据效率是现有最先进模型的5倍。

Conclusion: RGMP通过结合几何-语义推理与递归高斯自适应，实现了跨领域的高度通用机器人控制，有效提升了泛化能力和数据效率。

Abstract: Humanoid robots exhibit significant potential in executing diverse human-level skills. However, current research predominantly relies on data-driven approaches that necessitate extensive training datasets to achieve robust multimodal decision-making capabilities and generalizable visuomotor control. These methods raise concerns due to the neglect of geometric reasoning in unseen scenarios and the inefficient modeling of robot-target relationships within the training data, resulting in significant waste of training resources. To address these limitations, we present the Recurrent Geometric-prior Multimodal Policy (RGMP), an end-to-end framework that unifies geometric-semantic skill reasoning with data-efficient visuomotor control. For perception capabilities, we propose the Geometric-prior Skill Selector, which infuses geometric inductive biases into a vision language model, producing adaptive skill sequences for unseen scenes with minimal spatial common sense tuning. To achieve data-efficient robotic motion synthesis, we introduce the Adaptive Recursive Gaussian Network, which parameterizes robot-object interactions as a compact hierarchy of Gaussian processes that recursively encode multi-scale spatial relationships, yielding dexterous, data-efficient motion synthesis even from sparse demonstrations. Evaluated on both our humanoid robot and desktop dual-arm robot, the RGMP framework achieves 87% task success in generalization tests and exhibits 5x greater data efficiency than the state-of-the-art model. This performance underscores its superior cross-domain generalization, enabled by geometric-semantic reasoning and recursive-Gaussion adaptation.

</details>


### [145] [LODESTAR: Degeneracy-Aware LiDAR-Inertial Odometry with Adaptive Schmidt-Kalman Filter and Data Exploitation](https://arxiv.org/abs/2511.09142)
*Eungchang Mason Lee,Kevin Christiansen Marsim,Hyun Myung*

Main category: cs.RO

TL;DR: LODESTAR是一种新型的LiDAR-惯性里程计方法，专为在退化环境下提高定位准确性和鲁棒性而设计。其核心是退化感知的自适应Schmidt-Kalman滤波和数据利用模块。实验表明，该方法在各种退化场景下优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有LIDAR-惯性里程计在长走廊、高空等场景会因激光雷达数据稀疏或分布不均，导致状态估计退化，严重影响定位性能。作者希望解决这一长期存在的鲁棒性问题。

Method: 提出LODESTAR方法，包括两个核心模块：1）退化感知自适应Schmidt-Kalman滤波（DA-ASKF），通过滑窗方式利用历史状态和观测，将状态按退化程度自适应分为活跃和固定，利用Schmidt-Kalman更新，只优化活跃状态，固定状态作为锚点影响活跃状态更新；2）退化感知数据利用（DA-DE），在活跃状态中剔除低信息量观测，并按可定位性与Jacobian条件数，有选择地挖掘固定状态的观测。

Result: 实验结果显示，LODESTAR在各种退化环境下，准确性和鲁棒性均优于当前的基准LiDAR里程计方法和退化感知模块。

Conclusion: LODESTAR通过退化感知的滤波与观测选择机制，能有效缓解因观测退化或不均带来的定位误差，显著提升了LIO系统在复杂场景下的应用价值。

Abstract: LiDAR-inertial odometry (LIO) has been widely used in robotics due to its high accuracy. However, its performance degrades in degenerate environments, such as long corridors and high-altitude flights, where LiDAR measurements are imbalanced or sparse, leading to ill-posed state estimation. In this letter, we present LODESTAR, a novel LIO method that addresses these degeneracies through two key modules: degeneracy-aware adaptive Schmidt-Kalman filter (DA-ASKF) and degeneracy-aware data exploitation (DA-DE). DA-ASKF employs a sliding window to utilize past states and measurements as additional constraints. Specifically, it introduces degeneracy-aware sliding modes that adaptively classify states as active or fixed based on their degeneracy level. Using Schmidt-Kalman update, it partially optimizes active states while preserving fixed states. These fixed states influence the update of active states via their covariances, serving as reference anchors--akin to a lodestar. Additionally, DA-DE prunes less-informative measurements from active states and selectively exploits measurements from fixed states, based on their localizability contribution and the condition number of the Jacobian matrix. Consequently, DA-ASKF enables degeneracy-aware constrained optimization and mitigates measurement sparsity, while DA-DE addresses measurement imbalance. Experimental results show that LODESTAR outperforms existing LiDAR-based odometry methods and degeneracy-aware modules in terms of accuracy and robustness under various degenerate conditions.

</details>


### [146] [Unveiling the Impact of Data and Model Scaling on High-Level Control for Humanoid Robots](https://arxiv.org/abs/2511.09241)
*Yuxi Wei,Zirui Wang,Kangning Yin,Yue Hu,Jingbo Wang,Siheng Chen*

Main category: cs.RO

TL;DR: 本文提出了一个大规模人形机器人运动数据集Humanoid-Union，并基于该数据集推出了一种可扩展的控制学习框架SCHUR，实现了高质量机器人运动生成和语义对齐的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有机器人学习受限于数据规模，尽管人类视频和动作数据丰富，但如何高效提取、标注和利用这些数据进行可扩展的机器人学习仍未解决。

Method: 作者设计了一个自主化数据处理流水线，从人类动作视频中生成有语义标注的人形机器人动作数据集Humanoid-Union。随后提出SCHUR框架，基于大规模数据资源进行高层次控制学习，并在数据和模型规模扩展下测试其效果。

Result: SCHUR在机器人的运动生成质量和文本-动作对齐能力上，较前人方法分别有37%（MPJPE指标）和25%（FID指标）的提升，并且可以在真实机器人上有效部署。

Conclusion: 本文证明了大规模带语义标签的数据资源和可扩展学习框架能够极大提升人形机器人在高阶运动控制和语义理解方面的性能，为未来机器人领域的数据驱动研究提供了新途径。

Abstract: Data scaling has long remained a critical bottleneck in robot learning. For humanoid robots, human videos and motion data are abundant and widely available, offering a free and large-scale data source. Besides, the semantics related to the motions enable modality alignment and high-level robot control learning. However, how to effectively mine raw video, extract robot-learnable representations, and leverage them for scalable learning remains an open problem. To address this, we introduce Humanoid-Union, a large-scale dataset generated through an autonomous pipeline, comprising over 260 hours of diverse, high-quality humanoid robot motion data with semantic annotations derived from human motion videos. The dataset can be further expanded via the same pipeline. Building on this data resource, we propose SCHUR, a scalable learning framework designed to explore the impact of large-scale data on high-level control in humanoid robots. Experimental results demonstrate that SCHUR achieves high robot motion generation quality and strong text-motion alignment under data and model scaling, with 37\% reconstruction improvement under MPJPE and 25\% alignment improvement under FID comparing with previous methods. Its effectiveness is further validated through deployment in real-world humanoid robot.

</details>


### [147] [UMIGen: A Unified Framework for Egocentric Point Cloud Generation and Cross-Embodiment Robotic Imitation Learning](https://arxiv.org/abs/2511.09302)
*Yan Huang,Shoujie Li,Xingting Li,Wenbo Ding*

Main category: cs.RO

TL;DR: 该论文提出了UMIGen框架，通过改进的数据采集设备与算法提升了机器人操作任务中的数据采集效率与通用性，实现了更强的跨硬件泛化能力。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的机器人学习需要大量高质量演示数据，但受限于硬件、成本及方法泛化性，数据采集难度极大。现有方案（如UMI）仅能采集RGB图像，缺乏丰富的三维信息，影响任务表现。

Method: 提出UMIGen框架，包含（1）Cloud-UMI手持数据采集设备，能同步记录点云与操作对，无需视觉SLAM；（2）可见性优化机制，将DemoGen管线扩展到自中心三维观测，仅生成相机视野内点云。这样可高效采集与真实观测一致、适用于多种机器人平台的数据。

Result: 在仿真与真实环境下实验表明，UMIGen不仅提升了数据采集效率，还实现了操作策略的跨形态（embodiment）泛化能力。

Conclusion: UMIGen框架适用于高效、泛用的数据采集，推动了机器人操作在不同硬件间的直接迁移和泛化能力。

Abstract: Data-driven robotic learning faces an obvious dilemma: robust policies demand large-scale, high-quality demonstration data, yet collecting such data remains a major challenge owing to high operational costs, dependence on specialized hardware, and the limited spatial generalization capability of current methods. The Universal Manipulation Interface (UMI) relaxes the strict hardware requirements for data collection, but it is restricted to capturing only RGB images of a scene and omits the 3D geometric information on which many tasks rely. Inspired by DemoGen, we propose UMIGen, a unified framework that consists of two key components: (1) Cloud-UMI, a handheld data collection device that requires no visual SLAM and simultaneously records point cloud observation-action pairs; and (2) a visibility-aware optimization mechanism that extends the DemoGen pipeline to egocentric 3D observations by generating only points within the camera's field of view. These two components enable efficient data generation that aligns with real egocentric observations and can be directly transferred across different robot embodiments without any post-processing. Experiments in both simulated and real-world settings demonstrate that UMIGen supports strong cross-embodiment generalization and accelerates data collection in diverse manipulation tasks.

</details>


### [148] [CoRL-MPPI: Enhancing MPPI With Learnable Behaviours For Efficient And Provably-Safe Multi-Robot Collision Avoidance](https://arxiv.org/abs/2511.09331)
*Stepan Dergachev,Artem Pshenitsyn,Aleksandr Panov,Alexey Skrynnik,Konstantin Yakovlev*

Main category: cs.RO

TL;DR: 本文提出了CoRL-MPPI方法，将协作强化学习（Cooperative Reinforcement Learning）和模型预测路径积分（MPPI）结合，以提升多机器人系统在去中心化避碰上的效率和安全性。通过在仿真中训练深度神经网络策略，引导MPPI采样，实现更智能的避碰行为。实验显示，该方法相比主流基线方法在导航效率和安全性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统的MPPI方法虽然具有理论优势，能适应多种机器人运动模型，但在实际中由于依赖随机采样，导致轨迹常常亚最优，影响机器人群体在复杂环境下的避碰性能。如何突破MPPI在采样阶段的限制，提高避碰智能，是该研究的核心动机。

Method: 本研究提出将协作强化学习获得的避碰策略（用深度神经网络逼近）嵌入到MPPI采样分布，引导采样更趋向于协作和高效避碰的动作，从而提高MPPI框架的实际表现。该方法称为CoRL-MPPI，并在动态、密集仿真环境下与ORCA、BVC及多智能体MPPI等主流方法进行了对比实验。

Result: 实验结果表明，所提CoRL-MPPI方法在任务成功率、总耗时（makespan）及安全性等指标上，均优于现有的主流多机器人避碰算法，展现出更强的敏捷性和鲁棒性。

Conclusion: CoRL-MPPI能够在保持原有MPPI理论优势的基础上，显著提升多机器人系统在去中心化避碰任务中的效率和安全性，为多机器人协作导航提供了一种有效的解决方案。

Abstract: Decentralized collision avoidance remains a core challenge for scalable multi-robot systems. One of the promising approaches to tackle this problem is Model Predictive Path Integral (MPPI) -- a framework that is naturally suited to handle any robot motion model and provides strong theoretical guarantees. Still, in practice MPPI-based controller may provide suboptimal trajectories as its performance relies heavily on uninformed random sampling. In this work, we introduce CoRL-MPPI, a novel fusion of Cooperative Reinforcement Learning and MPPI to address this limitation. We train an action policy (approximated as deep neural network) in simulation that learns local cooperative collision avoidance behaviors. This learned policy is then embedded into the MPPI framework to guide its sampling distribution, biasing it towards more intelligent and cooperative actions. Notably, CoRL-MPPI preserves all the theoretical guarantees of regular MPPI. We evaluate our approach in dense, dynamic simulation environments against state-of-the-art baselines, including ORCA, BVC, and a multi-agent MPPI implementation. Our results demonstrate that CoRL-MPPI significantly improves navigation efficiency (measured by success rate and makespan) and safety, enabling agile and robust multi-robot navigation.

</details>


### [149] [SPIDER: Scalable Physics-Informed Dexterous Retargeting](https://arxiv.org/abs/2511.09484)
*Chaoyi Pan,Changhao Wang,Haozhi Qi,Zixi Liu,Homanga Bharadhwaj,Akash Sharma,Tingfan Wu,Guanya Shi,Jitendra Malik,Francois Hogan*

Main category: cs.RO

TL;DR: 提出了一种名为SPIDER的新方法，将人类运动数据大规模、高效地转化为可用于机器人灵巧控制的动态可行性轨迹，从而改善数据稀缺问题，提升机器人学习效率。


<details>
  <summary>Details</summary>
Motivation: 当前灵巧机器人和仿人手控制需大规模演示数据，机器人采集成本高昂，但人类运动数据丰富。然而，两者间存在体现（身体结构差异）和动力学信息缺失等鸿沟，使其无法直接应用。因此亟需桥接人类演示和机器人控制的有效技术。

Method: 提出了基于物理的可扩展灵巧重定向框架SPIDER，将仅含运动学信息的人类演示，通过物理仿真大规模采样，并结合课程式虚拟接触引导，优化得到动力学可行且动作顺序正确的机器人轨迹。该方法能适应9种机器人结构与6种数据集，且能扩充多样高质量数据。

Result: SPIDER在各类机器人和数据集上，成功率较标准采样提升18%，生成数据速度比强化学习方法快10倍，可构建240万帧动力学可行数据集，支持高效策略学习。

Conclusion: SPIDER是一种通用、可扩展的基于物理的重定向技术，可将各类人类演示转化为动力学可行的机器人数据，为高效灵巧策略学习提供高质量训练数据，促进机器人灵巧控制的发展。

Abstract: Learning dexterous and agile policy for humanoid and dexterous hand control requires large-scale demonstrations, but collecting robot-specific data is prohibitively expensive. In contrast, abundant human motion data is readily available from motion capture, videos, and virtual reality, which could help address the data scarcity problem. However, due to the embodiment gap and missing dynamic information like force and torque, these demonstrations cannot be directly executed on robots. To bridge this gap, we propose Scalable Physics-Informed DExterous Retargeting (SPIDER), a physics-based retargeting framework to transform and augment kinematic-only human demonstrations to dynamically feasible robot trajectories at scale. Our key insight is that human demonstrations should provide global task structure and objective, while large-scale physics-based sampling with curriculum-style virtual contact guidance should refine trajectories to ensure dynamical feasibility and correct contact sequences. SPIDER scales across diverse 9 humanoid/dexterous hand embodiments and 6 datasets, improving success rates by 18% compared to standard sampling, while being 10X faster than reinforcement learning (RL) baselines, and enabling the generation of a 2.4M frames dynamic-feasible robot dataset for policy learning. As a universal physics-based retargeting method, SPIDER can work with diverse quality data and generate diverse and high-quality data to enable efficient policy learning with methods like RL.

</details>


### [150] [WMPO: World Model-based Policy Optimization for Vision-Language-Action Models](https://arxiv.org/abs/2511.09515)
*Fangqi Zhu,Zhengyang Yan,Zicong Hong,Quanxin Shou,Xiao Ma,Song Guo*

Main category: cs.RO

TL;DR: 提出了一种新的视觉-语言-动作（VLA）与强化学习结合的机器人学习方法WMPO，无需真实环境交互也能实现高效、泛化的机器人控制。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖专家演示，缺乏从失败中学习和自我修正能力；传统强化学习虽然能自我改进，但在真实机器人上样本复杂度高，学习效率低。需要一种能兼顾泛化能力和高效学习的方法。

Method: 提出WMPO（World-Model-based Policy Optimization）框架，不直接与真实环境交互，而是在像素层面进行预测，对“想象”轨迹与预训练大模型特征对齐，采用on-policy的GRPO方法替代以往常用的off-policy算法，使策略更高效、泛化更强。

Result: 通过大量仿真和真实机器人实验，WMPO显著提升了样本效率、整体性能，展现了自我修正等新行为，并具备稳健泛化与终身学习能力。

Conclusion: WMPO为机器人学习带来了高效率、强泛化能力和自我修正等优势，有望推动通用机器人操作系统的现实应用。

Abstract: Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.

</details>


### [151] [MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation](https://arxiv.org/abs/2511.09516)
*Runhao Li,Wenkai Guo,Zhenyu Wu,Changyuan Wang,Haoyuan Deng,Zhenyu Weng,Yap-Peng Tan,Ziwei Wang*

Main category: cs.RO

TL;DR: 本文提出了一种新的方法（MAP-VLA），通过引入记忆增强提示，提升视觉-语言-动作(VLA)模型在长时序机器人操作任务中的表现。该方法利用历史演示生成可学习的记忆单元，并在推理时动态检索和集成，实现了对原有模型的插件式增强，实验获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 虽然VLA模型提升了端到端机器人操作的泛化和鲁棒性，但在处理长时序任务时因缺乏记忆，只依赖即时感知输入，表现不佳。为突破这一瓶颈，作者希望为VLA模型赋予记忆能力。

Method: 方法分为两部分：1）通过历史示范数据，构建由多个可学习软提示组成的记忆库，每个单元关联任务阶段；2）任务执行时，根据轨迹相似性检索相关记忆信息，并动态集成到冻结的VLA模型作为动作生成提示，实现无需修改基础模型的增强。

Result: 在模拟和真实机器人长时序任务中，该方法分别提升了最高7.0%和25.0%的绝对性能，优于当前最好方法。

Conclusion: 为VLA模型引入记忆提示，能以灵活轻量的插件方式显著提升其解决复杂长时序任务的能力，具有实际应用和推广前景。

Abstract: Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.

</details>


### [152] [SpatialActor: Exploring Disentangled Spatial Representations for Robust Robotic Manipulation](https://arxiv.org/abs/2511.09555)
*Hao Shi,Bin Xie,Yingfei Liu,Yang Yue,Tiancai Wang,Haoqiang Fan,Xiangyu Zhang,Gao Huang*

Main category: cs.RO

TL;DR: 本文提出了一种名为SpatialActor的解耦网络框架，提升了机器人操作任务中的空间理解与鲁棒性。通过将语义与几何特征分离，并利用空间变换器加强2D-3D映射，实现了在多任务场景下的领先性能。


<details>
  <summary>Details</summary>
Motivation: 机器人操作任务依赖于对物体空间关系的精准感知。现有点云和图像方法或丢失细粒度语义，或对噪声敏感且忽视底层空间信息，导致实际操作中的表现受限。因此亟需一种更健壮、鲁棒的空间理解方法。

Method: 提出SpatialActor框架，显式地将语义信息与几何信息解耦。设计了语义引导几何模块（Semantic-guided Geometric Module）融合了有噪声深度和专家先验，并通过空间变换器（Spatial Transformer）提取底层空间线索，增强空间特征交互和2D-3D映射能力。

Result: 在RLBench等50余项仿真及真实任务评测中，SpatialActor取得了87.4%的领先性能，在高噪声环境下提升了13.9%到19.4%，显著优于现有方法，并且展现了优良的few-shot泛化能力和空间扰动下的稳健性。

Conclusion: SpatialActor解耦了语义与几何，提高了机器人在复杂环境下的空间理解和操作鲁棒性，在多任务和模糊环境下都表现优越，有望推动实际机器人操作系统的发展。

Abstract: Robotic manipulation requires precise spatial understanding to interact with objects in the real world. Point-based methods suffer from sparse sampling, leading to the loss of fine-grained semantics. Image-based methods typically feed RGB and depth into 2D backbones pre-trained on 3D auxiliary tasks, but their entangled semantics and geometry are sensitive to inherent depth noise in real-world that disrupts semantic understanding. Moreover, these methods focus on high-level geometry while overlooking low-level spatial cues essential for precise interaction. We propose SpatialActor, a disentangled framework for robust robotic manipulation that explicitly decouples semantics and geometry. The Semantic-guided Geometric Module adaptively fuses two complementary geometry from noisy depth and semantic-guided expert priors. Also, a Spatial Transformer leverages low-level spatial cues for accurate 2D-3D mapping and enables interaction among spatial features. We evaluate SpatialActor on multiple simulation and real-world scenarios across 50+ tasks. It achieves state-of-the-art performance with 87.4% on RLBench and improves by 13.9% to 19.4% under varying noisy conditions, showing strong robustness. Moreover, it significantly enhances few-shot generalization to new tasks and maintains robustness under various spatial perturbations. Project Page: https://shihao1895.github.io/SpatialActor

</details>


### [153] [IFG: Internet-Scale Guidance for Functional Grasping Generation](https://arxiv.org/abs/2511.09558)
*Ray Muxin Liu,Mingxuan Li,Kenneth Shaw,Deepak Pathak*

Main category: cs.RO

TL;DR: 本文提出结合大规模视觉模型与仿真力闭合抓取管线，实现无需人工标注数据的高效语义抓取。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模视觉模型在物体语义分割和定位方面表现优异，但在实现灵巧机械手的精确三维抓取时，缺乏对局部几何的理解。为实现高性能且无需人工训练数据的语义抓取，该问题亟需解决。

Method: 作者提出用结合仿真器的力闭合抓取管线，对手与物体的局部几何进行分析，由于仿真慢且需真值，数据经过提炼后用扩散模型实时推理点云。

Result: 方法无需人工标注数据，仅靠大模型的语义与仿真的几何结合，实现了高性能的语义抓取效果。

Conclusion: 该方法成功将大视觉模型的语义能力与仿真的几何精度结合，在无需人工数据条件下，推动了机器人灵巧手三维抓取能力的发展。

Abstract: Large Vision Models trained on internet-scale data have demonstrated strong capabilities in segmenting and semantically understanding object parts, even in cluttered, crowded scenes. However, while these models can direct a robot toward the general region of an object, they lack the geometric understanding required to precisely control dexterous robotic hands for 3D grasping. To overcome this, our key insight is to leverage simulation with a force-closure grasping generation pipeline that understands local geometries of the hand and object in the scene. Because this pipeline is slow and requires ground-truth observations, the resulting data is distilled into a diffusion model that operates in real-time on camera point clouds. By combining the global semantic understanding of internet-scale models with the geometric precision of a simulation-based locally-aware force-closure, \our achieves high-performance semantic grasping without any manually collected training data. For visualizations of this please visit our website at https://ifgrasping.github.io/

</details>
