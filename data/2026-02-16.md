<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 68]
- [cs.CL](#cs.CL) [Total: 38]
- [cs.RO](#cs.RO) [Total: 39]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Thermal Imaging for Contactless Cardiorespiratory and Sudomotor Response Monitoring](https://arxiv.org/abs/2602.12361)
*Constantino Álvarez Casado,Mohammad Rahman,Sasan Sharifipour,Nhi Nguyen,Manuel Lage Cañellas,Xiaoting Wu,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 本文提出了一种利用面部热红外视频无接触地提取和估算皮肤电活动（EDA）、心率（HR）及呼吸频率（BR）的信号处理流程，并在公开数据集上进行了实验评估，得出了基线性能指标和设计建议。


<details>
  <summary>Details</summary>
Motivation: 现有可见光方法可实现心率和呼吸频率的无接触估算，但无法获取EDA信号，而EDA是评估交感神经活动的重要生理指标。由于热红外成像能捕捉与自主神经调节相关的皮肤温度变化，有望弥补可见光方法短板，实现EDA等多种生理信号的无接触检测。

Method: 搭建了基于面部热红外成像的视频信号处理流程，主要包括：目标解剖区域的跟踪、空间聚合、分离慢速EDA趋势与快速心/呼吸信号。HR检测采用正交矩阵图像变换（OMIT）分析多个ROI，BR则平均鼻部和面颊信号后进行光谱峰值检测。针对EDA，对288种参数配置进行了对比分析。在31个SIM1数据集会话下评估了EDA、HR和BR的方法。

Result: 最佳固定EDA配置（鼻部区域+指数滑动平均）与标准掌部EDA相关系数达0.40±0.23，单独会话最高达0.89。呼吸频率均方误差3.1±1.1次/分钟，心率误差为13.8±7.5次/分钟，主要受限于相机帧率。报告了信号极性变动、信号延迟较短且质量受被试条件和人口统计因素影响。

Conclusion: 本研究首次系统验证了面部热红外视频实现多种生理信号无接触检测的可行性，给出了基线性能上限，对今后基于热成像的生理信号无感估算方法的设计提供了指导。

Abstract: Thermal infrared imaging captures skin temperature changes driven by autonomic regulation and can potentially provide contactless estimation of electrodermal activity (EDA), heart rate (HR), and breathing rate (BR). While visible-light methods address HR and BR, they cannot access EDA, a standard marker of sympathetic activation. This paper characterizes the extraction of these three biosignals from facial thermal video using a signal-processing pipeline that tracks anatomical regions, applies spatial aggregation, and separates slow sudomotor trends from faster cardiorespiratory components. For HR, we apply an orthogonal matrix image transformation (OMIT) decomposition across multiple facial regions of interest (ROIs), and for BR we average nasal and cheek signals before spectral peak detection. We evaluate 288 EDA configurations and the HR/BR pipeline on 31 sessions from the public SIMULATOR STUDY 1 (SIM1) driver monitoring dataset. The best fixed EDA configuration (nose region, exponential moving average) reaches a mean absolute correlation of $0.40 \pm 0.23$ against palm EDA, with individual sessions reaching 0.89. BR estimation achieves a mean absolute error of $3.1 \pm 1.1$ bpm, while HR estimation yields $13.8 \pm 7.5$ bpm MAE, limited by the low camera frame rate (7.5 Hz). We report signal polarity alternation across sessions, short thermodynamic latency for well-tracked signals, and condition-dependent and demographic effects on extraction quality. These results provide baseline performance bounds and design guidance for thermal contactless biosignal estimation.

</details>


### [2] [LLaMo: Scaling Pretrained Language Models for Unified Motion Understanding and Generation with Continuous Autoregressive Tokens](https://arxiv.org/abs/2602.12370)
*Zekun Li,Sizhe An,Chengcheng Tang,Chuan Guo,Ivan Shugurov,Linguang Zhang,Amy Zhao,Srinath Sridhar,Lingling Tao,Abhay Mittal*

Main category: cs.CV

TL;DR: 本文提出了LLaMo框架，解决了统一处理运动-文本生成与理解的难题，实现了高保真且实时的人体动作生成与描述。


<details>
  <summary>Details</summary>
Motivation: 现有运动-文本统一模型因数据稀缺和量化带来的离散化噪声，导致语言能力退化和生成质量下降。因此，迫切需要一种不丧失语言理解且可扩展至多模态的统一模型。

Method: 作者提出基于大型语言模型（LLMs）拓展的Mixture-of-Transformers (MoT)架构，采用连续潜空间编码动作，通过轻量级流匹配机制，实现流式高效生成和理解，将动作信息自然融入原有预测范式，避免语言能力遗忘。

Result: 在大规模动作-文本预训练下，LLaMo能实现高质量的文本生动作生成和动作描述，尤其在零样本设置下效果显著，生成流畅、准确、超过30FPS实时性。

Conclusion: LLaMo显著提升了统一运动-语言模型的能力，保留了语言理解优势，迈出通用型大模型的重要一步，为后续多模态大模型发展奠定基础。

Abstract: Recent progress in large models has led to significant advances in unified multimodal generation and understanding. However, the development of models that unify motion-language generation and understanding remains largely underexplored. Existing approaches often fine-tune large language models (LLMs) on paired motion-text data, which can result in catastrophic forgetting of linguistic capabilities due to the limited scale of available text-motion pairs. Furthermore, prior methods typically convert motion into discrete representations via quantization to integrate with language models, introducing substantial jitter artifacts from discrete tokenization. To address these challenges, we propose LLaMo, a unified framework that extends pretrained LLMs through a modality-specific Mixture-of-Transformers (MoT) architecture. This design inherently preserves the language understanding of the base model while enabling scalable multimodal adaptation. We encode human motion into a causal continuous latent space and maintain the next-token prediction paradigm in the decoder-only backbone through a lightweight flow-matching head, allowing for streaming motion generation in real-time (>30 FPS). Leveraging the comprehensive language understanding of pretrained LLMs and large-scale motion-text pretraining, our experiments demonstrate that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning in general settings, especially zero-shot motion generation, marking a significant step towards a general unified motion-language large model.

</details>


### [3] [Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues](https://arxiv.org/abs/2602.12381)
*Marco Willi,Melanie Mathys,Michael Graber*

Main category: cs.CV

TL;DR: 本文分析了CLIP模型在合成图像检测（SID）上的表现，并通过新数据集SynthCLIC揭示其优缺点。


<details>
  <summary>Details</summary>
Motivation: 高质量的生成模型生成的几近真实的图像挑战了照片的可信度，因此急需研究准确且可通用的合成图像检测方法。当前主流SID方法在面对新型生成模型和实际应用场景时泛化能力较差。CLIP作为视觉-语言基础模型，在SID中的优异表现值得深入探究其背后机制。

Method: 作者构建了一个名为SynthCLIC的新数据集，包含真实照片与由最新扩散模型生成的高质量合成图像对，以减少以往方法中的语义偏差。然后，用可解释的线性分类器（带去相关激活）和基于文本的概念模型分析CLIP特征中实际被利用的线索。

Result: CLIP的线性检测器在基于GAN的数据集上取得0.96 mAP，但在高质量SynthCLIC扩散图像集上只有0.92 mAP，不同生成模型间的泛化能力最低降至0.37 mAP。检测器主要依赖高层次摄影属性，而非特定生成器留存的明显伪影。

Conclusion: CLIP为SID提供了强有力的基础，虽然整体性能优秀，但在不同生成模型间泛化能力有限。因此，需不断更新模型，加大训练数据多样性，以实现更强鲁棒性的通用SID。

Abstract: Recent generative models produce near-photorealistic images, challenging the trustworthiness of photographs. Synthetic image detection (SID) has thus become an important area of research. Prior work has highlighted how synthetic images differ from real photographs--unfortunately, SID methods often struggle to generalize to novel generative models and often perform poorly in practical settings. CLIP, a foundational vision-language model which yields semantically rich image-text embeddings, shows strong accuracy and generalization for SID. Yet, the underlying relevant cues embedded in CLIP-features remain unknown. It is unclear, whether CLIP-based detectors simply detect strong visual artifacts or exploit subtle semantic biases, both of which would render them useless in practical settings or on generative models of high quality. We introduce SynthCLIC, a paired dataset of real photographs and high-quality synthetic counterparts from recent diffusion models, designed to reduce semantic bias in SID. Using an interpretable linear head with de-correlated activations and a text-grounded concept-model, we analyze what CLIP-based detectors learn. CLIP-based linear detectors reach 0.96 mAP on a GAN-based benchmark but only 0.92 on our high-quality diffusion dataset SynthCLIC, and generalization across generator families drops to as low as 0.37 mAP. We find that the detectors primarily rely on high-level photographic attributes (e.g., minimalist style, lens flare, or depth layering), rather than overt generator-specific artifacts. CLIP-based detectors perform well overall but generalize unevenly across diverse generative architectures. This highlights the need for continual model updates and broader training exposure, while reinforcing CLIP-based approaches as a strong foundation for more universal, robust SID.

</details>


### [4] [Reproducing DragDiffusion: Interactive Point-Based Editing with Diffusion Models](https://arxiv.org/abs/2602.12393)
*Ali Subhan,Ashir Raza*

Main category: cs.CV

TL;DR: 该论文对DragDiffusion交互式图像编辑方法进行了复现性研究，总体验证了其空间控制和算法性能的核心结论，并揭示了一些对超参数敏感的细节。


<details>
  <summary>Details</summary>
Motivation: DragDiffusion提出了一种创新点控图像编辑技术，声称能够高精度操控图像，但成果是否可复现和易用仍需检验。作者通过复现实验检验原论文实验结果的可靠性，并进一步分析该方法系统性表现。

Method: 作者使用DragDiffusion公开实现和DragBench基准，复现了关于扩散步数选择、LoRA微调、掩码正则强度、UNet特征监督等消融实验，并额外测试了多步潜变量优化方案。通过对比原始论文中的定量和定性结论，评估可复现性以及方法对超参数的敏感度。

Result: 实验结果基本与原文报告一致，支持其主要结论，但发现算法对某些超参数（如优化步数、监督特征层级）较为敏感，其它模块容错性较高。尝试的多步优化并未提升空间精度，反而增加计算成本。

Conclusion: DragDiffusion方法的核心观点和性能大体可复现，但实际应用时需注意对部分参数的敏感依赖；论文通过复现实验为其结论的稳健性和适用范围提供了更清晰的界定。

Abstract: DragDiffusion is a diffusion-based method for interactive point-based image editing that enables users to manipulate images by directly dragging selected points. The method claims that accurate spatial control can be achieved by optimizing a single diffusion latent at an intermediate timestep, together with identity-preserving fine-tuning and spatial regularization. This work presents a reproducibility study of DragDiffusion using the authors' released implementation and the DragBench benchmark. We reproduce the main ablation studies on diffusion timestep selection, LoRA-based fine-tuning, mask regularization strength, and UNet feature supervision, and observe close agreement with the qualitative and quantitative trends reported in the original work. At the same time, our experiments show that performance is sensitive to a small number of hyperparameter assumptions, particularly the optimized timestep and the feature level used for motion supervision, while other components admit broader operating ranges. We further evaluate a multi-timestep latent optimization variant and find that it does not improve spatial accuracy while substantially increasing computational cost. Overall, our findings support the central claims of DragDiffusion while clarifying the conditions under which they are reliably reproducible. Code is available at https://github.com/AliSubhan5341/DragDiffusion-TMLR-Reproducibility-Challenge.

</details>


### [5] [What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis](https://arxiv.org/abs/2602.12395)
*Xirui Li,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种“弗兰肯斯坦式”分析框架，系统研究了强化学习（RL）在视觉-语言模型推理能力提升中的真实作用，发现RL主要在中后层产生推理时的显著变化，这些变化对提升视觉推理是必须且可迁移的。


<details>
  <summary>Details</summary>
Motivation: 尽管RL已成为视觉-语言模型后训练以增强推理能力的标准方法，但与传统有监督微调相比，RL具体改进了哪些能力仍然不清楚。常规基准测试难以明确区分每种训练方式对具体技能的贡献，因此需要更精细的分析方法。

Method: 作者提出三步分析框架：（1）通过因果探测实现功能定位；（2）通过参数比较分析参数更新特性；（3）通过模型融合测试中后层特征的可迁移性。

Result: 分析发现，RL训练带来的主要变化集中于中到后层，在推理时产生一致的网络激活偏移。这些中后层改进通过模型融合可迁移，同时在冻结实验中显示对性能提升至关重要。

Conclusion: RL对于视觉推理的提升不在于均匀增强感知能力，而是在变换器中后层系统性地优化了从视觉到推理的对齐和推理性能。仅靠基准评测无法全面理解多模态推理能力的提升。

Abstract: Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.

</details>


### [6] [ZeroDiff++: Substantial Unseen Visual-semantic Correlation in Zero-shot Learning](https://arxiv.org/abs/2602.12401)
*Zihan Ye,Shreyank N Gowda,Kaile Du,Weijian Luo,Ling Shao*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的生成式零样本学习框架ZeroDiff++，通过数据增强、自监督表示、多视角判别器及测试时自适应方法，有效提升了在稀缺训练样本场景下的泛化能力，缓解了虚假相关性问题，并在主流基准上取得显著优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有生成式零样本学习方法在训练样本稀缺时容易学习到虚假的视觉-语义相关性，导致对未见类别泛化能力下降，且现有生成器在噪声处理与真实样本匹配上存在瓶颈。因此，作者希望提出新方法以缓解上述问题，提升零样本学习的鲁棒性和实用性。

Method: 作者提出ZeroDiff++框架。在训练阶段，采用扩散式数据增强生成更丰富的噪声样本；使用有监督对比学习进行实例级语义表示学习；设计了多视角判别器结合Wasserstein互学习评估生成特征。在生成阶段，提出基于扩散的测试时自适应（DiffTTA）与生成（DiffGen）技术，前者通过伪标签重建自适应生成器，后者追踪反扩散路径，实现真实与生成数据的衔接，缓解数据稀缺。

Result: 在三个主流零样本学习基准数据集上，ZeroDiff++在准确率等指标上显著优于现有方法，且在训练数据极度稀缺时依然表现出强鲁棒性。

Conclusion: ZeroDiff++显著提升了生成式零样本学习方法对未见类别的泛化能力，尤其在训练样本稀缺场景下表现突出，对虚假视觉-语义相关性的缓解和新引入的评估指标具备实用价值，为零样本学习研究带来新进展。

Abstract: Zero-shot Learning (ZSL) enables classifiers to recognize classes unseen during training, commonly via generative two stage methods: (1) learn visual semantic correlations from seen classes; (2) synthesize unseen class features from semantics to train classifiers. In this paper, we identify spurious visual semantic correlations in existing generative ZSL worsened by scarce seen class samples and introduce two metrics to quantify spuriousness for seen and unseen classes. Furthermore, we point out a more critical bottleneck: existing unadaptive fully noised generators produce features disconnected from real test samples, which also leads to the spurious correlation. To enhance the visual-semantic correlations on both seen and unseen classes, we propose ZeroDiff++, a diffusion-based generative framework. In training, ZeroDiff++ uses (i) diffusion augmentation to produce diverse noised samples, (ii) supervised contrastive (SC) representations for instance level semantics, and (iii) multi view discriminators with Wasserstein mutual learning to assess generated features. At generation time, we introduce (iv) Diffusion-based Test time Adaptation (DiffTTA) to adapt the generator using pseudo label reconstruction, and (v) Diffusion-based Test time Generation (DiffGen) to trace the diffusion denoising path and produce partially synthesized features that connect real and generated data, and mitigates data scarcity further. Extensive experiments on three ZSL benchmarks demonstrate that ZeroDiff++ not only achieves significant improvements over existing ZSL methods but also maintains robust performance even with scarce training data. Code would be available.

</details>


### [7] [MonoLoss: A Training Objective for Interpretable Monosemantic Representations](https://arxiv.org/abs/2602.12403)
*Ali Nasiri-Sarvi,Anh Tien Nguyen,Hassan Rivaz,Dimitris Samaras,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: 本文提出了一种高效的Monosemanticity评分算法及其训练正则化目标MonoLoss，用于提升稀疏自编码器（SAEs）分解多语义神经元为单语义特征的能力，大幅降低计算成本，并显著提升特征可解释性和相关分类性能。


<details>
  <summary>Details</summary>
Motivation: 神经网络中的神经元常常对多个无关概念做出响应，导致模型的可解释性差。虽然稀疏自编码器被用来提取单义（单一语义）特征，但现有的训练目标对单义分解的鼓励作用有限，且评估单义性的方法计算开销巨大。因此，研究者希望提升特征解释性并降低评估代价。

Method: 研究人员分析并优化了MonoScore单义性评估指标，提出一种只需单次遍历、计算量线性增长的高效算法用于训练和评估。基于该高效评估方法，进一步提出MonoLoss正则化项，直接作为辅助损失引导网络学习更一致语义的单义激活。该方法与多种自编码器和编码器组合进行实证验证。

Result: 在OpenImagesV7等大规模数据集上，优化后的MonoScore评估可实现高达1200倍的加速。引入MonoLoss后，大多数特征的MonoScore显著提升，激活图片的类别纯度大幅提高（例如从0.152提升到0.723）。作为ResNet-50、CLIP等模型微调辅助正则器时，ImageNet-1K准确率提升达0.6%，且激活模式更具单义性。

Conclusion: 本文方法既改善了神经网络特征的可解释性，又显著降低了评估成本，同时提升了分类任务性能，为单义特征的高效学习和应用提供了有效解决方案。相关代码已开源。

Abstract: Sparse autoencoders (SAEs) decompose polysemantic neural representations, where neurons respond to multiple unrelated concepts, into monosemantic features that capture single, interpretable concepts. However, standard training objectives only weakly encourage this decomposition, and existing monosemanticity metrics require pairwise comparisons across all dataset samples, making them inefficient during training and evaluation. We study a recent MonoScore metric and derive a single-pass algorithm that computes exactly the same quantity, but with a cost that grows linearly, rather than quadratically, with the number of dataset images. On OpenImagesV7, we achieve up to a 1200x speedup wall-clock speedup in evaluation and 159x during training, while adding only ~4% per-epoch overhead. This allows us to treat MonoScore as a training signal: we introduce the Monosemanticity Loss (MonoLoss), a plug-in objective that directly rewards semantically consistent activations for learning interpretable monosemantic representations. Across SAEs trained on CLIP, SigLIP2, and pretrained ViT features, using BatchTopK, TopK, and JumpReLU SAEs, MonoLoss increases MonoScore for most latents. MonoLoss also consistently improves class purity (the fraction of a latent's activating images belonging to its dominant class) across all encoder and SAE combinations, with the largest gain raising baseline purity from 0.152 to 0.723. Used as an auxiliary regularizer during ResNet-50 and CLIP-ViT-B/32 finetuning, MonoLoss yields up to 0.6\% accuracy gains on ImageNet-1K and monosemantic activating patterns on standard benchmark datasets. The code is publicly available at https://github.com/AtlasAnalyticsLab/MonoLoss.

</details>


### [8] [Prototype-driven fusion of pathology and spatial transcriptomics for interpretable survival prediction](https://arxiv.org/abs/2602.12441)
*Lihe Liu,Xiaoxi Pan,Yinyin Yuan,Lulu Shang*

Main category: cs.CV

TL;DR: 提出了一种结合全切片图像（WSI）和空间转录组数据（ST）用于疾病预后的新框架PathoSpatial，兼顾可解释性和预测性能，并在三阴性乳腺癌数据集取得优异效果。


<details>
  <summary>Details</summary>
Motivation: WSI与ST各自能从形态和分子层面提供肿瘤信息，融合有助于提升疾病预后准确性。但现有方法跨模态融合能力有限，缺乏高效可解释的融合框架。

Method: 设计了PathoSpatial框架，通过多级专家架构和任务引导的原型学习，结合无监督模态内发现与有监督跨模态聚合，从而自适应融合WSI和ST的空间信号，提升模型可解释性。

Result: 在三阴性乳腺癌配对数据集上，PathoSpatial在五个生存分析端点均表现强劲且稳定，优于或相当于目前先进的单模态和多模态方法。

Conclusion: PathoSpatial框架不仅提升了预后模型的性能，还可实现原型解释和分子风险分解，具备良好可扩展性和生物学解释性，为空间组学与病理信息融合的预后研究提供了新范式。

Abstract: Whole slide images (WSIs) enable weakly supervised prognostic modeling via multiple instance learning (MIL). Spatial transcriptomics (ST) preserves in situ gene expression, providing a spatial molecular context that complements morphology. As paired WSI-ST cohorts scale to population level, leveraging their complementary spatial signals for prognosis becomes crucial; however, principled cross-modal fusion strategies remain limited for this paradigm. To this end, we introduce PathoSpatial, an interpretable end-to-end framework integrating co-registered WSIs and ST to learn spatially informed prognostic representations. PathoSpatial uses task-guided prototype learning within a multi-level experts architecture, adaptively orchestrating unsupervised within-modality discovery with supervised cross-modal aggregation. By design, PathoSpatial substantially strengthens interpretability while maintaining discriminative ability. We evaluate PathoSpatial on a triple-negative breast cancer cohort with paired ST and WSIs. PathoSpatial delivers strong and consistent performance across five survival endpoints, achieving superior or comparable performance to leading unimodal and multimodal methods. PathoSpatial inherently enables post-hoc prototype interpretation and molecular risk decomposition, providing quantitative, biologically grounded explanations, highlighting candidate prognostic factors. We present PathoSpatial as a proof-of-concept for scalable and interpretable multimodal learning for spatial omics-pathology fusion.

</details>


### [9] [Semantic-aware Adversarial Fine-tuning for CLIP](https://arxiv.org/abs/2602.12461)
*Jiacheng Zhang,Jinhao Li,Hanxun Huang,Sarah M. Erfani,Benjamin I. P. Rubinstein,Feng Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的对抗性微调方法SAFT，可以通过生成语义感知的对抗样本来提升CLIP模型在零样本分类任务中的对抗鲁棒性，并在多个数据集上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有利用手工模板和余弦相似度生成对抗样本微调CLIP的方法存在鲁棒性不足的问题，尤其是在采用更具语义丰富性的相似度度量时。这说明传统的攻击方式并未真正提升模型的语义对抗防御能力。

Method: 作者设计了“语义集成攻击”，即通过基础模型生成一组丰富和精炼的文本描述，形成模板集，用以代表更全面的语义信息。然后，针对这些描述生成语义感知的对抗样本，并提出“语义感知对抗性微调（SAFT）”策略，对CLIP的图像编码器进行微调。

Result: 实验表明，在16个数据集上，SAFT方法在提升CLIP模型的零样本对抗鲁棒性方面优于现有方法。

Conclusion: SAFT通过利用更全面的语义信息生成对抗样本，有效提升了CLIP模型的对抗鲁棒性，是对CLIP安全性与泛化能力的有意义改进。

Abstract: Recent studies have shown that CLIP model's adversarial robustness in zero-shot classification tasks can be enhanced by adversarially fine-tuning its image encoder with adversarial examples (AEs), which are generated by minimizing the cosine similarity between images and a hand-crafted template (e.g., ''A photo of a {label}''). However, it has been shown that the cosine similarity between a single image and a single hand-crafted template is insufficient to measure the similarity for image-text pairs. Building on this, in this paper, we find that the AEs generated using cosine similarity may fail to fool CLIP when the similarity metric is replaced with semantically enriched alternatives, making the image encoder fine-tuned with these AEs less robust. To overcome this issue, we first propose a semantic-ensemble attack to generate semantic-aware AEs by minimizing the average similarity between the original image and an ensemble of refined textual descriptions. These descriptions are initially generated by a foundation model to capture core semantic features beyond hand-crafted templates and are then refined to reduce hallucinations. To this end, we propose Semantic-aware Adversarial Fine-Tuning (SAFT), which fine-tunes CLIP's image encoder with semantic-aware AEs. Extensive experiments show that SAFT outperforms current methods, achieving substantial improvements in zero-shot adversarial robustness across 16 datasets. Our code is available at: https://github.com/tmlr-group/SAFT.

</details>


### [10] [A Lightweight and Explainable DenseNet-121 Framework for Grape Leaf Disease Classification](https://arxiv.org/abs/2602.12484)
*Md. Ehsanul Haque,Md. Saymon Hosen Polash,Rakib Hasan Ovi,Aminul Kader Bulbul,Md Kamrul Siam,Tamim Hasan Saykat*

Main category: cs.CV

TL;DR: 本文提出了一种基于优化DenseNet 121的葡萄叶病害分类方法，具备高精度、可解释性强、推理速度快等优势，优于现有主流CNN模型，适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 葡萄种植过程中，病害会严重影响产量和品质。现有自动化检测方法计算量大且解释性差，不适合实际场景，亟需高效、准确且易于解释的新方法来早期诊断病害。

Method: 采用优化后的DenseNet 121模型，结合领域特定的预处理和充分的特征连接，挖掘出葡萄叶病害的关键特征（如叶脉、边缘、病斑）。通过与ResNet18、VGG16、AlexNet、SqueezeNet等模型对比，验证模型性能。利用Grad-CAM可视化模型关注区域，提高模型可解释性。

Result: 所提模型相较于其他CNN基线模型表现突出，准确率达99.27%，F1分数99.28%，特异性99.71%，Kappa系数98.86%。推理时间仅9秒。交叉验证平均准确率99.12%，说明模型泛化能力强。

Conclusion: 本文方法有效结合高效架构、领域预处理和可解释性输出，不仅显著提升了病害检测精度，也具备实际部署的计算优势，适用于实际葡萄园病害快速检测和管理。

Abstract: Grapes are among the most economically and culturally significant fruits on a global scale, and table grapes and wine are produced in significant quantities in Europe and Asia. The production and quality of grapes are significantly impacted by grape diseases such as Bacterial Rot, Downy Mildew, and Powdery Mildew. Consequently, the sustainable management of a vineyard necessitates the early and precise identification of these diseases. Current automated methods, particularly those that are based on the YOLO framework, are often computationally costly and lack interpretability that makes them unsuitable for real-world scenarios. This study proposes grape leaf disease classification using Optimized DenseNet 121. Domain-specific preprocessing and extensive connectivity reveal disease-relevant characteristics, including veins, edges, and lesions. An extensive comparison with baseline CNN models, including ResNet18, VGG16, AlexNet, and SqueezeNet, demonstrates that the proposed model exhibits superior performance. It achieves an accuracy of 99.27%, an F1 score of 99.28%, a specificity of 99.71%, and a Kappa of 98.86%, with an inference time of 9 seconds. The cross-validation findings show a mean accuracy of 99.12%, indicating strength and generalizability across all classes. We also employ Grad-CAM to highlight disease-related regions to guarantee the model is highlighting physiologically relevant aspects and increase transparency and confidence. Model optimization reduces processing requirements for real-time deployment, while transfer learning ensures consistency on smaller and unbalanced samples. An effective architecture, domain-specific preprocessing, and interpretable outputs make the proposed framework scalable, precise, and computationally inexpensive for detecting grape leaf diseases.

</details>


### [11] [Human-Like Coarse Object Representations in Vision Models](https://arxiv.org/abs/2602.12486)
*Andrey Gizdov,Andrea Procopio,Yichen Li,Daniel Harari,Tomer Ullman*

Main category: cs.CV

TL;DR: 本研究探究了计算机视觉分割模型与人类直觉物理感知在物体表示上的一致性，发现有限资源约束下的中等复杂度模型最接近日类表征。


<details>
  <summary>Details</summary>
Motivation: 人类在处理物理直觉任务时，往往用粗略的体积化表征对象，省略细节以获得高效预测。相较之下，主流分割模型追求像素级精确，这可能背离人类表征。探究二者关系，有助于理解及优化模型的物理推理能力。

Method: 作者提出了时至碰撞（TTC）行为范式、比较流程及对齐度量标准，通过调整分割模型的训练时长、规模、修剪程度（即有效容量），系统性分析模型表征与人类行为的对齐情况。

Result: 所有变量变化下，人类-模型对齐度均表现为反U型曲线：小模型/短时间训练/高修剪度，过于粗糙（像‘粘成团’）；大模型/训练充分，过于细碎（边界‘发抖’）；中等复杂度达到最佳对齐。

Conclusion: 研究指出，接近日类粗粒度物体表征主要源于资源受限，而非设计先验。通过选择合适模型规模、训练轮数或适度修剪，可提升模型在直觉物理任务中的表现，支持资源-理性优化理论。

Abstract: Humans appear to represent objects for intuitive physics with coarse, volumetric bodies'' that smooth concavities - trading fine visual details for efficient physical predictions - yet their internal structure is largely unknown. Segmentation models, in contrast, optimize pixel-accurate masks that may misalign with such bodies. We ask whether and when these models nonetheless acquire human-like bodies. Using a time-to-collision (TTC) behavioral paradigm, we introduce a comparison pipeline and alignment metric, then vary model training time, size, and effective capacity via pruning. Across all manipulations, alignment with human behavior follows an inverse U-shaped curve: small/briefly trained/pruned models under-segment into blobs; large/fully trained models over-segment with boundary wiggles; and an intermediate ideal body granularity'' best matches humans. This suggests human-like coarse bodies emerge from resource constraints rather than bespoke biases, and points to simple knobs - early checkpoints, modest architectures, light pruning - for eliciting physics-efficient representations. We situate these results within resource-rational accounts balancing recognition detail against physical affordances.

</details>


### [12] [Insertion Network for Image Sequence Correspondence](https://arxiv.org/abs/2602.12489)
*Dingjie Su,Weixiang Hong,Benoit M. Dawant,Bennett A. Landman*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的方法，通过训练网络学习如何将一个序列中的切片插入另一个序列的合适位置，以建立2D图像序列的对应关系。该方法可显著提升2D切片在3D体积中的定位精度。


<details>
  <summary>Details</summary>
Motivation: 目前在3D医学影像分析中，2D切片在3D体积中的定位是诊断、配准、分割等任务的关键前处理步骤。现有方法多将每张切片独立处理，未利用完整上下文信息，导致切片定位存在较大误差。

Method: 作者提出了基于上下文编码和切片间注意力机制的插入网络：网络学习如何将一个序列中的切片插入到另一个序列的正确位置，实现序列级的切片对应关系建立。方法通过对每张切片的上下文进行编码，同时使用切片到切片的注意力机制，精确建模插入过程。

Result: 将该方法应用于体部CT扫描中人工标注的关键切片定位任务，并与当前最优方法Body Part Regression对比。插入网络在有监督训练下，将切片定位误差从8.4 mm降至5.4 mm，精度提升明显。

Conclusion: 本文提出的方法充分挖掘了序列的上下文信息，在2D切片序列与3D体积之间的定位任务上优于现有独立处理方法，有望为后续医学影像的自动配准和分割提供更准确的预处理。

Abstract: We propose a novel method for establishing correspondence between two sequences of 2D images. One particular application of this technique is slice-level content navigation, where the goal is to localize specific 2D slices within a 3D volume or determine the anatomical coverage of a 3D scan based on its 2D slices. This serves as an important preprocessing step for various diagnostic tasks, as well as for automatic registration and segmentation pipelines. Our approach builds sequence correspondence by training a network to learn how to insert a slice from one sequence into the appropriate position in another. This is achieved by encoding contextual representations of each slice and modeling the insertion process using a slice-to-slice attention mechanism. We apply this method to localize manually labeled key slices in body CT scans and compare its performance to the current state-of-the-art alternative known as body part regression, which predicts anatomical position scores for individual slices. Unlike body part regression, which treats each slice independently, our method leverages contextual information from the entire sequence. Experimental results show that the insertion network reduces slice localization errors in supervised settings from 8.4 mm to 5.4 mm, demonstrating a substantial improvement in accuracy.

</details>


### [13] [Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models](https://arxiv.org/abs/2602.12498)
*Ali Abbasi,Mehdi Taghipour,Rahmatollah Beheshti*

Main category: cs.CV

TL;DR: 本文提出了一种新的消极表述检测基准和上下文临床否定数据集，针对医学视觉-语言模型在否定判别上的不足，提出了因果解释引导的训练方法（NAST），显著提升了模型对否定和肯定医学表述的区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在医学报告中对否定语句和肯定语句区分较差，严重影响实际临床应用的准确性和安全性。因此，亟需开发能更好理解否定语义的算法和评价基准。

Method: 1. 构建了一个专用于放射学的否定检测诊断基准，系统评估模型在受控临床条件下对否定信息的敏感度；2. 创建了一个上下文丰富、支持属性级否定（如部位和严重程度）的临床否定数据集；3. 提出NAST（Negation-Aware Selective Training）方法，利用因果追踪效应（CTEs）引导模型细化训练，动态调整不同层的学习率，强化模型学习处理否定信息的能力。

Result: NAST方法有效提升了模型对否定与肯定临床语句的区分，相比原始和常见微调方法表现更优，同时未损害模型对一般视觉-语言任务的对齐能力。

Conclusion: 基于因果可解释性的训练方法可显著提升医学VLM模型在安全关键场景（如否定检测）中的表现，推动医学AI更精准可靠地服务临床应用。

Abstract: Negation is a fundamental linguistic operation in clinical reporting, yet vision-language models (VLMs) frequently fail to distinguish affirmative from negated medical statements. To systematically characterize this limitation, we introduce a radiology-specific diagnostic benchmark that evaluates polarity sensitivity under controlled clinical conditions, revealing that common medical VLMs consistently confuse negated and non-negated findings. To enable learning beyond simple condition absence, we further construct a contextual clinical negation dataset that encodes structured claims and supports attribute-level negations involving location and severity. Building on these resources, we propose Negation-Aware Selective Training (NAST), an interpretability-guided adaptation method that uses causal tracing effects (CTEs) to modulate layer-wise gradient updates during fine-tuning. Rather than applying uniform learning rates, NAST scales each layer's update according to its causal contribution to negation processing, transforming mechanistic interpretability signals into a principled optimization rule. Experiments demonstrate improved discrimination of affirmative and negated clinical statements without degrading general vision-language alignment, highlighting the value of causal interpretability for targeted model adaptation in safety-critical medical settings. Code and resources are available at https://github.com/healthylaife/NAST.

</details>


### [14] [Matching of SAR and optical images based on transformation to shared modality](https://arxiv.org/abs/2602.12515)
*Alexey Borisov,Evgeny Myasnikov,Vladislav Myasnikov*

Main category: cs.CV

TL;DR: 本文提出了一种通过将光学和SAR影像转换到共同模态，实现高效配准的新方法，并利用已有的著名配准模型（如RoMa和DeDoDe）取得优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 由于光学影像和SAR影像的获取原理不同，导致它们在外观和数据分布上存在显著差异，使得二者的精确配准变得极为困难。现有的影像翻译或特征匹配方法效果有限，需要一种能兼容两种影像的新型配准思路。

Method: 作者提出一种模态转换法：将光学和SAR影像分别转化为一个共同的新模态。该模态具备：1）通道数预设且一致；2）转换后影像间尽量相似；3）保留原始影像主要特征等条件。随后，利用现有优秀的图像配准模型（如RoMa）在新模态下进行影像匹配。

Result: 在公开的MultiSenGE数据集上，作者的方法在影像配准精度和多样性上均优于影像间直接转换和多种特征匹配算法。并且可以直接使用预训练RoMa、DeDoDe等常用匹配模型，无需针对新模态再训练。

Conclusion: 该方法不仅提升了光学与SAR影像的配准精度，还提升了配准方法的通用性和实用性，对于多源遥感影像的融合和应用具有重要意义。

Abstract: Significant differences in optical images and Synthetic Aperture Radar (SAR) images are caused by fundamental differences in the physical principles underlying their acquisition by Earth remote sensing platforms. These differences make precise image matching (co-registration) of these two types of images difficult. In this paper, we propose a new approach to image matching of optical and SAR images, which is based on transforming the images to a new modality. The new image modality is common to both optical and SAR images and satisfies the following conditions. First, the transformed images must have an equal pre-defined number of channels. Second, the transformed and co-registered images must be as similar as possible. Third, the transformed images must be non-degenerate, meaning they must preserve the significant features of the original images. To further match images transformed to this shared modality, we train the RoMa image matching model, which is one of the leading solutions for matching of regular digital photographs. We evaluated the proposed approach on the publicly available MultiSenGE dataset containing both optical and SAR images. We demonstrated its superiority over alternative approaches based on image translation between original modalities and various feature matching algorithms. The proposed solution not only provides better quality of matching, but is also more versatile. It enables the use of ready-made RoMa and DeDoDe models, pre-trained for regular images, without retraining for a new modality, while maintaining high-quality matching of optical and SAR images.

</details>


### [15] [LiDAR-Anchored Collaborative Distillation for Robust 2D Representations](https://arxiv.org/abs/2602.12524)
*Wonjun Jo,Hyunwoo Ha,Kim Ji-Yeon,Hawook Jeong,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 提出了一种利用3D LiDAR作为自监督手段提升2D图像编码器在恶劣和噪声天气下视觉感知鲁棒性的协同蒸馏方法，并在多种真实场景任务和环境下表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前2D图像编码器虽然能应对常规下游视觉任务，但对噪声和恶劣天气情景的鲁棒性不足，影响实际应用的可靠性。

Method: 提出了“协同蒸馏”方法，利用3D激光雷达感知信息对2D图像编码器进行自监督训练，从而提升其对复杂环境（如恶劣天气、噪声场景）的识别能力，并保留原有对清晰场景的特征提取能力。

Result: 该方法在多种下游任务及不同环境条件下都优于现有方法，并表现出较强的泛化能力。此外，还提升了2D编码器的3D场景感知能力。

Conclusion: 所提方法不仅提高了2D视觉系统对环境变化的鲁棒性，还展现出了在实际复杂应用中的实用性和适应性。

Abstract: As deep learning continues to advance, self-supervised learning has made considerable strides. It allows 2D image encoders to extract useful features for various downstream tasks, including those related to vision-based systems. Nevertheless, pre-trained 2D image encoders fall short in conducting the task under noisy and adverse weather conditions beyond clear daytime scenes, which require for robust visual perception. To address these issues, we propose a novel self-supervised approach, \textbf{Collaborative Distillation}, which leverages 3D LiDAR as self-supervision to improve robustness to noisy and adverse weather conditions in 2D image encoders while retaining their original capabilities. Our method outperforms competing methods in various downstream tasks across diverse conditions and exhibits strong generalization ability. In addition, our method also improves 3D awareness stemming from LiDAR's characteristics. This advancement highlights our method's practicality and adaptability in real-world scenarios.

</details>


### [16] [Geometric Stratification for Singular Configurations of the P3P Problem via Local Dual Space](https://arxiv.org/abs/2602.12525)
*Xueying Sun,Zijia Li,Nan Li*

Main category: cs.CV

TL;DR: 本文针对P3P问题的特殊（奇异）配置进行了深入研究，提出了基于局部对偶空间的代数-计算框架，实现了对奇异配置的几何分层描述。


<details>
  <summary>Details</summary>
Motivation: P3P问题在计算机视觉和机器人定位中是基础性问题，但其在特定相机配置下会出现多解乃至无穷解的“奇异”情形，缺乏系统的分类与理解。本研究旨在完整刻画P3P问题的奇异配置，为理论分析和实际应用提供依据。

Method: 作者利用局部对偶空间工具，建立了系统的代数-计算方法。具体地，根据相机中心O的多重性μ，将奇异配置进行了详细的几何分层：μ≥2时，O在危险圆柱上；μ≥3时，O位于由第一个莫利三角或外接圆确定的三条母线上；μ≥4时，O在外接圆上（对应无穷多解）。同时，对应的互补配置O'也被分层分析。

Result: 得到以下主要成果：1）基于多重性μ，将P3P奇异配置进行了详细的几何分层；2）发现危险圆柱、三条母线和外接圆分别对应μ≥2、3、4的奇异点分布；3）对偶空间下的O'也具有明确的几何分层（如德尔多伊面和尖点曲线）。

Conclusion: 本文首次系统完整地分层刻画了P3P问题的奇异配置及其对偶点分布，为相关理论研究和实际算法设计提供了坚实的几何基础。

Abstract: This paper investigates singular configurations of the P3P problem. Using local dual space, a systematic algebraic-computational framework is proposed to give a complete geometric stratification for the P3P singular configurations with respect to the multiplicity $μ$ of the camera center $O$: for $μ\ge 2$, $O$ lies on the ``danger cylinder'', for $μ\ge 3$, $O$ lies on one of three generatrices of the danger cylinder associated with the first Morley triangle or the circumcircle, and for $μ\ge 4$, $O$ lies on the circumcircle which indeed corresponds to infinite P3P solutions. Furthermore, a geometric stratification for the complementary configuration $O^\prime$ associated with a singular configuration $O$ is studied as well: for $μ\ge 2$, $O^\prime$ lies on a deltoidal surface associated with the danger cylinder, and for $μ\ge 3$, $O^\prime$ lies on one of three cuspidal curves of the deltoidal surface.

</details>


### [17] [Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting](https://arxiv.org/abs/2602.12540)
*Haoran Zhu,Anna Choromanska*

Main category: cs.CV

TL;DR: 本文提出了AD-LiST-JEPA，这是一种基于自监督学习的自动驾驶激光雷达世界建模方法，通过JEPA架构实现激光雷达点云的时空预测，并在占用补全和预测任务上实现了更优表现。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要能够建立世界模型来捕捉环境的时空演化，并依赖大规模无监督数据学习以应对可扩展性，因此探索无需人工标注的自监督世界建模方法。

Method: 采用JEPA（Joint-Embedding Predictive Architecture）构建自监督的世界模型，从激光雷达数据中预测未来环境变化，并通过占用补全与预测（OCF）任务评估模型表征质量。

Result: 实验表明，基于JEPA训练后的编码器在OCF下游任务上取得了更佳表现，验证了方法的有效性。

Conclusion: AD-LiST-JEPA能够通过自监督学习提升激光雷达世界建模能力，为自动驾驶系统的长期规划和环境感知提供了强有力的技术支持。

Abstract: Autonomous driving, as an agent operating in the physical world, requires the fundamental capability to build \textit{world models} that capture how the environment evolves spatiotemporally in order to support long-term planning. At the same time, scalability demands learning such models in a self-supervised manner; \textit{joint-embedding predictive architecture (JEPA)} enables learning world models via leveraging large volumes of unlabeled data without relying on expensive human annotations. In this paper, we propose \textbf{AD-LiST-JEPA}, a self-supervised world model for autonomous driving that predicts future spatiotemporal evolution from LiDAR data using a JEPA framework. We evaluate the quality of the learned representations through a downstream LiDAR-based occupancy completion and forecasting (OCF) task, which jointly assesses perception and prediction. Proof of concept experiments show better OCF performance with pretrained encoder after JEPA-based world model learning.

</details>


### [18] [PLLM: Pseudo-Labeling Large Language Models for CAD Program Synthesis](https://arxiv.org/abs/2602.12561)
*Yuanbo Li,Dule Shu,Yanying Chen,Matt Klenk,Daniel Ritchie*

Main category: cs.CV

TL;DR: 本论文提出了一种无需标注数据即可从3D模型恢复CAD程序的新方法PLLM，通过自监督训练提升了程序生成的几何精度和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有CAD程序合成方法依赖于有标签的形状-程序对数据集，然而此类数据集获取成本高，且实际常常不可用，因此需要一种能够利用无标签3D形状数据的方法。

Method: 提出PLLM自训练框架：首先使用预训练具备CAD能力的大语言模型与无标签的3D形状数据集，迭代生成候选CAD程序并筛选高保真度结果，然后扩充得到合成的程序-形状对，用于进一步微调模型。

Result: 在将PLLM方法应用于DeepCAD的CAD-Recode并扩展到无标签的ABC数据集时，实验表明生成的CAD程序在几何精度和多样性方面均有显著提升。

Conclusion: PLLM能有效利用无标签3D形状数据进行自监督CAD程序合成，减少了对有标签数据的依赖，并提升了生成结果的质量。

Abstract: Recovering Computer-Aided Design (CAD) programs from 3D geometries is a widely studied problem. Recent advances in large language models (LLMs) have enabled progress in CAD program synthesis, but existing methods rely on supervised training with paired shape-program data, which is often unavailable. We introduce PLLM, a self-training framework for CAD program synthesis from unlabeled 3D shapes. Given a pre-trained CAD-capable LLM and a shape dataset, PLLM iteratively samples candidate programs, selects high-fidelity executions, and augments programs to construct synthetic program-shape pairs for fine-tuning. We experiment on adapting CAD-Recode from DeepCAD to the unlabeled ABC dataset show consistent improvements in geometric fidelity and program diversity.

</details>


### [19] [The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving](https://arxiv.org/abs/2602.12563)
*Jiabao Wang,Hongyu Zhou,Yuanbo Yang,Jiahao Shao,Yiyi Liao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的评测基准和方法，用以分析和提升自动驾驶算法在外观变化下的鲁棒性，解决现有方法容易受外观干扰而性能退化的问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶算法在非分布内（OOD）条件下表现脆弱，尤其难以区分外观变化（如天气、光照）与场景结构变化（如复杂道路几何）导致的性能问题，阻碍对算法失效原因的本质理解与改进。

Method: 作者提出了navdream高保真鲁棒性基准，通过生成像素对齐风格迁移，构建了几乎无结构变化但视觉大幅变化的测试集，从而隔离外观对算法性能的影响。同时，作者利用DINOv3视觉基础模型，提出提取外观不变特征的通用感知接口，将其作为各类规划模型的输入。

Result: 实验表明，即便场景结构未变，当前主流自动驾驶规划算法在外观变化条件下表现显著下降。所提出的DINOv3通用感知接口，能极大提升多类规划模型在各种外观变化下的零样本泛化能力，无需进一步微调。

Conclusion: 作者方法有效解决了外观变化造成的性能退化问题，显著提升了规划算法的鲁棒性和泛化能力，为自动驾驶感知-规划模块解耦提供了可行路径，并公开相应基准与代码以促进后续研究。

Abstract: Despite rapid progress, autonomous driving algorithms remain notoriously fragile under Out-of-Distribution (OOD) conditions. We identify a critical decoupling failure in current research: the lack of distinction between appearance-based shifts, such as weather and lighting, and structural scene changes. This leaves a fundamental question unanswered: Is the planner failing because of complex road geometry, or simply because it is raining? To resolve this, we establish navdream, a high-fidelity robustness benchmark leveraging generative pixel-aligned style transfer. By creating a visual stress test with negligible geometric deviation, we isolate the impact of appearance on driving performance. Our evaluation reveals that existing planning algorithms often show significant degradation under OOD appearance conditions, even when the underlying scene structure remains consistent. To bridge this gap, we propose a universal perception interface leveraging a frozen visual foundation model (DINOv3). By extracting appearance-invariant features as a stable interface for the planner, we achieve exceptional zero-shot generalization across diverse planning paradigms, including regression-based, diffusion-based, and scoring-based models. Our plug-and-play solution maintains consistent performance across extreme appearance shifts without requiring further fine-tuning. The benchmark and code will be made available.

</details>


### [20] [Unbiased Gradient Estimation for Event Binning via Functional Backpropagation](https://arxiv.org/abs/2602.12590)
*Jinze Chen,Wei Zhai,Han Han,Tiankai Ma,Yang Cao,Bin Li,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出了一种针对事件视觉中帧化操作导致的梯度估计偏差问题的新框架，通过在反向传播中合成弱导数实现无偏梯度估计，有效提升了事件视觉在多种任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 事件相机的数据是以异步的时空脉冲（事件）表示，为了利用传统图像处理方法，这些事件通常需要被归纳（binning）为帧。由于binning操作的非连续性，直接影响梯度传递，限制了事件视觉算法的高效学习。现有的直接从事件学习的方法也同样会因binning操作引入梯度估计偏差，影响训练效果。因此，如何实现无偏的梯度估计成为核心挑战。

Method: 作者提出了一种新颖的无偏梯度估计算法。在前向过程中保持binning输出不变，而在反向传播时，通过分部积分理论，将目标函数提升为泛函，对binning函数获得积分形式的导数，进而重构cotangent函数并计算弱导数，有效解决了非连续binning带来的梯度传递问题。

Result: 实验表明，该方法在基于优化的自运动估计任务中，将RMS误差降低了3.2%，收敛速度提升了1.57倍；在复杂下游任务如自监督光流估计和SLAM中分别降低了9.4%的EPE和5.1%的RMS误差，效果优于现有方法。

Conclusion: 该方法有效克服了事件视觉中binning函数梯度估计偏差的问题，提升了事件视觉模型在多任务下的表现，对基于事件的视觉感知具有广泛的推动作用。

Abstract: Event-based vision encodes dynamic scenes as asynchronous spatio-temporal spikes called events. To leverage conventional image processing pipelines, events are typically binned into frames. However, binning functions are discontinuous, which truncates gradients at the frame level and forces most event-based algorithms to rely solely on frame-based features. Attempts to directly learn from raw events avoid this restriction but instead suffer from biased gradient estimation due to the discontinuities of the binning operation, ultimately limiting their learning efficiency. To address this challenge, we propose a novel framework for unbiased gradient estimation of arbitrary binning functions by synthesizing weak derivatives during backpropagation while keeping the forward output unchanged. The key idea is to exploit integration by parts: lifting the target functions to functionals yields an integral form of the derivative of the binning function during backpropagation, where the cotangent function naturally arises. By reconstructing this cotangent function from the sampled cotangent vector, we compute weak derivatives that provably match long-range finite differences of both smooth and non-smooth targets. Experimentally, our method improves simple optimization-based egomotion estimation with 3.2\% lower RMS error and 1.57$\times$ faster convergence. On complex downstream tasks, we achieve 9.4\% lower EPE in self-supervised optical flow, and 5.1\% lower RMS error in SLAM, demonstrating broad benefits for event-based visual perception. Source code can be found at https://github.com/chjz1024/EventFBP.

</details>


### [21] [QuEPT: Quantized Elastic Precision Transformers with One-Shot Calibration for Multi-Bit Switching](https://arxiv.org/abs/2602.12609)
*Ke Xu,Yixin Wang,Zhongcheng Li,Hao Cui,Jinshui Hu,Xingyi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的弹性多比特量化方法QuEPT，在无需多次优化的情况下，实现了不同量化精度下的大模型高性能部署。


<details>
  <summary>Details</summary>
Motivation: 在Transformer架构（特别是大语言模型）中，弹性量化能够让同一模型适应多种精度部署场景，但现有方法在存储和优化成本上都很高，特别是缺少适用于大模型的高效方案。

Method: QuEPT采用一次性校准和小规模数据切片，分块重构多比特误差。通过级联低秩适配器（MB-CLoRA），支持不同比特宽的动态切换，还可以实时在均匀量化和混合精度量化间切换。引入多比特Token融合（MB-ToMe）机制，在切换比特宽过程中动态融合不同比特下的token特征，提升准确率和鲁棒性。

Result: 大量实验表明，QuEPT在灵活性和性能上均达到或超过现有SOTA后训练量化方法。

Conclusion: QuEPT能够极大简化大模型多比特量化部署流程，在不损失准确率的前提下降低存储和优化成本，为实际应用提供更高效的量化解决方案。

Abstract: Elastic precision quantization enables multi-bit deployment via a single optimization pass, fitting diverse quantization scenarios.Yet, the high storage and optimization costs associated with the Transformer architecture, research on elastic quantization remains limited, particularly for large language models.This paper proposes QuEPT, an efficient post-training scheme that reconstructs block-wise multi-bit errors with one-shot calibration on a small data slice. It can dynamically adapt to various predefined bit-widths by cascading different low-rank adapters, and supports real-time switching between uniform quantization and mixed precision quantization without repeated optimization. To enhance accuracy and robustness, we introduce Multi-Bit Token Merging (MB-ToMe) to dynamically fuse token features across different bit-widths, improving robustness during bit-width switching. Additionally, we propose Multi-Bit Cascaded Low-Rank adapters (MB-CLoRA) to strengthen correlations between bit-width groups, further improve the overall performance of QuEPT. Extensive experiments demonstrate that QuEPT achieves comparable or better performance to existing state-of-the-art post-training quantization methods.Our code is available at https://github.com/xuke225/QuEPT

</details>


### [22] [Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models](https://arxiv.org/abs/2602.12618)
*Omer Faruk Deniz,Ruiyu Mao,Ruochen Li,Yapeng Tian,Latifur Khan*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的视觉Token压缩方法，在多模态大语言模型(MLLM)内部通过注意力机制进行自适应下采样，大幅度减少计算和内存开销，同时基本保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉Token剪枝（pruning）方法不是依赖于视觉编码器前的结构，通用性较差，就是采用启发式在LLM内部剪枝但与高效注意力机制（如FlashAttention）不兼容。因此亟需一种更通用且兼容高效注意力机制的剪枝方法。

Method: 作者观察到模型深层会自然传递视觉到文本的信息，于是提出基于注意力驱动的自压缩(ADSC)方法：在LLM若干层，直接用注意力机制对视觉Token进行统一下采样，形成信息瓶颈，促使模型主动压缩和重组信息。无需复杂得分计算，也不引入额外模块，并与FlashAttention完全兼容。

Result: 在LLaVA-1.5模型上，ADSC将FLOPs降低了53.7%，峰值KV-cache内存降低56.7%，性能仅下降1.8%。多个基准下，ADSC在效率和精度两方面均优于现有剪枝方法，尤其在高压缩率下表现更稳健。

Conclusion: ADSC是一种简洁、高效、通用且性能强健的视觉Token压缩方案，极大提升了多模态大语言模型的推理效率，并为后续相关研究提供了新的思路。

Abstract: Multimodal Large Language Models (MLLMs) incur significant computational cost from processing numerous vision tokens through all LLM layers. Prior pruning methods operate either before the LLM, limiting generality due to diverse encoder-projector designs or within the LLM using heuristics that are incompatible with FlashAttention. We take a different approach: rather than identifying unimportant tokens, we treat the LLM itself as the optimal guide for compression. Observing that deeper layers naturally transmit vision-to-text information, we introduce Attention-Driven Self-Compression (ADSC), a simple, broadly applicable method that progressively reduces vision tokens using only the LLM's attention mechanism. Our method applies uniform token downsampling at selected layers, forming bottlenecks that encourage the model to reorganize and compress information into the remaining tokens. It requires no score computation, auxiliary modules, or attention modification, and remains fully compatible with FlashAttention. Applied to LLaVA-1.5, ADSC reduces FLOPs by 53.7% and peak KV-cache memory by 56.7%, while preserving 98.2% of the original model performance. Across multiple benchmarks, it outperforms prior pruning approaches in both efficiency and accuracy. Crucially, under high compression ratios, our method remains robust while heuristic-based techniques degrade sharply.

</details>


### [23] [ImageRAGTurbo: Towards One-step Text-to-Image Generation with Retrieval-Augmented Diffusion Models](https://arxiv.org/abs/2602.12640)
*Peijie Qiu,Hariharan Ramshankar,Arnau Ramisa,René Vidal,Amit Kumar K C,Vamsi Salaka,Rahul Bhagat*

Main category: cs.CV

TL;DR: 提出ImageRAGTurbo方法，通过检索增强高效地微调少步扩散模型，实现高质量且低延迟的文本到图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在文本到图像生成中表现优秀，但因其逐步采样方式造成较高延迟。为减少步骤，有的模型牺牲了图像质量和提示一致性，且训练成本高，需要解决这一矛盾。

Method: 提出ImageRAGTurbo：针对给定文本提示，从数据库中检索相关文本-图像对，将检索内容作为条件输入UNet去噪器，在$H$-space中融合信息，并用可训练适配器通过交叉注意力机制进一步提升融合效果，无需高成本训练即可提升生成表现。

Result: 初步实验表明，仅通过检索内容编辑去噪器隐空间即可提升提示一致性。进一步使用适配器融合后，能在较少去噪步数下实现高保真图像生成，且生成延迟更低。

Conclusion: 本方法在保持低采样延迟的同时，提升了文本到图像的质量和提示对齐度，在快速生成场景下优于现有方案。

Abstract: Diffusion models have emerged as the leading approach for text-to-image generation. However, their iterative sampling process, which gradually morphs random noise into coherent images, introduces significant latency that limits their applicability. While recent few-step diffusion models reduce the number of sampling steps to as few as one to four steps, they often compromise image quality and prompt alignment, especially in one-step generation. Additionally, these models require computationally expensive training procedures. To address these limitations, we propose ImageRAGTurbo, a novel approach to efficiently finetune few-step diffusion models via retrieval augmentation. Given a text prompt, we retrieve relevant text-image pairs from a database and use them to condition the generation process. We argue that such retrieved examples provide rich contextual information to the UNet denoiser that helps reduce the number of denoising steps without compromising image quality. Indeed, our initial investigations show that using the retrieved content to edit the denoiser's latent space ($\mathcal{H}$-space) without additional finetuning already improves prompt fidelity. To further improve the quality of the generated images, we augment the UNet denoiser with a trainable adapter in the $\mathcal{H}$-space, which efficiently blends the retrieved content with the target prompt using a cross-attention mechanism. Experimental results on fast text-to-image generation demonstrate that our approach produces high-fidelity images without compromising latency compared to existing methods.

</details>


### [24] [Multi-Task Learning with Additive U-Net for Image Denoising and Classification](https://arxiv.org/abs/2602.12649)
*Vikram Lakkavalli,Neelam Sinha*

Main category: cs.CV

TL;DR: 该论文提出Additive U-Net (AddUNet)，使用门控加法融合替代传统U-Net的跳跃连接，以提升图像去噪及多任务学习的表现并提升训练稳定性。


<details>
  <summary>Details</summary>
Motivation: U-Net及其变体在图像去噪等视觉任务中常用跳跃连接，但传统的拼接跳跃连接可能导致信息流过量、优化不稳定等问题，特别是在多任务学习中。作者希望通过结构正则手段，在保持模型容量与信息流可控的前提下，提升去噪和多任务学习的训练及泛化表现。

Method: 作者将U-Net中的拼接跳跃连接替换为门控的加法融合（gated additive fusion），即在不同深度的特征间，通过门控机制实现加法合并，固定特征维度，并限制跳跃通道容量。实验涵盖单任务图像去噪和多任务（去噪+分类）学习，分析了skip权重分布与任务间互作用。

Result: AddUNet在单任务去噪和多任务场景下取得了与原版U-Net相当甚至更优的重建表现，同时显著提升了训练过程的稳定性。在多任务学习中，不同深度的skip连接权重会根据任务需要自动调节，浅层倾向于重建，深层则支持分类，显示出任务感知的跳跃信息分配。即便分类分支能力有限，去噪效果依然保持鲁棒，展现出隐式的任务解耦能力。

Conclusion: 通过对跳跃连接添加简单的结构限制（门控加法），可以极大提升U-Net类结构的训练稳定性和多任务扩展性，无需增加模型复杂度，是一种有效的结构正则手段。

Abstract: We investigate additive skip fusion in U-Net architectures for image denoising and denoising-centric multi-task learning (MTL). By replacing concatenative skips with gated additive fusion, the proposed Additive U-Net (AddUNet) constrains shortcut capacity while preserving fixed feature dimensionality across depth. This structural regularization induces controlled encoder-decoder information flow and stabilizes joint optimization. Across single-task denoising and joint denoising-classification settings, AddUNet achieves competitive reconstruction performance with improved training stability. In MTL, learned skip weights exhibit systematic task-aware redistribution: shallow skips favor reconstruction, while deeper features support discrimination. Notably, reconstruction remains robust even under limited classification capacity, indicating implicit task decoupling through additive fusion. These findings show that simple constraints on skip connections act as an effective architectural regularizer for stable and scalable multi-task learning without increasing model complexity.

</details>


### [25] [CBEN -- A Multimodal Machine Learning Dataset for Cloud Robust Remote Sensing Image Understanding](https://arxiv.org/abs/2602.12652)
*Marco Stricker,Masakazu Iwamura,Koichi Kise*

Main category: cs.CV

TL;DR: 论文指出当前遥感影像中由于云遮挡带来的挑战，提出了CloudyBigEarthNet（CBEN）数据集，用于推动对云遮挡鲁棒的多源机器学习方法。实验显示现有方法在云图像下性能大幅下降，改进训练方式能显著提升在云下测试的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有遥感影像分析往往将有云干扰的影像从训练和评测中剔除，但这对需要实时响应（如灾害监测）的实际场景极为不利，因为许多场合无法避免云的影响。针对这一点，亟需推动能适应云遮挡条件的遥感数据处理方法的发展。

Method: 作者提出并整理了CBEN数据集，其中配对的光学和雷达影像都包含云遮挡，适用于机器学习训练和评估。通过比较不同方法训练和测试在有云影像上的表现，探索了将云遮挡样本纳入训练对模型鲁棒性的提升效果。

Result: 实验表明，现有在晴天影像训练的主流方法，当在云图像上评估时，性能下降高达23-33个百分点。而针对有云影像进行训练后，模型在云图像测试中的平均准确率提升了17.2-28.7个百分点。

Conclusion: 将云遮挡影像纳入训练显著提升了模型在实际云遮条件下的鲁棒性。作者公开了数据集和代码，为遥感领域提升模型鲁棒性和实用性提供了基础资源和新思路。

Abstract: Clouds are a common phenomenon that distorts optical satellite imagery, which poses a challenge for remote sensing. However, in the literature cloudless analysis is often performed where cloudy images are excluded from machine learning datasets and methods. Such an approach cannot be applied to time sensitive applications, e.g., during natural disasters. A possible solution is to apply cloud removal as a preprocessing step to ensure that cloudfree solutions are not failing under such conditions. But cloud removal methods are still actively researched and suffer from drawbacks, such as generated visual artifacts. Therefore, it is desirable to develop cloud robust methods that are less affected by cloudy weather. Cloud robust methods can be achieved by combining optical data with radar, a modality unaffected by clouds. While many datasets for machine learning combine optical and radar data, most researchers exclude cloudy images. We identify this exclusion from machine learning training and evaluation as a limitation that reduces applicability to cloudy scenarios. To investigate this, we assembled a dataset, named CloudyBigEarthNet (CBEN), of paired optical and radar images with cloud occlusion for training and evaluation. Using average precision (AP) as the evaluation metric, we show that state-of-the-art methods trained on combined clear-sky optical and radar imagery suffer performance drops of 23-33 percentage points when evaluated on cloudy images. We then adapt these methods to cloudy optical data during training, achieving relative improvement of 17.2-28.7 percentage points on cloudy test cases compared with the original approaches. Code and dataset are publicly available at: https://github.com/mstricker13/CBEN

</details>


### [26] [IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models](https://arxiv.org/abs/2602.12659)
*Aarish Shah Mohsin,Mohammed Tayyab Ilyas Khan,Mohammad Nadeem,Shahab Saquib Sohail,Erik Cambria,Jiechao Gao*

Main category: cs.CV

TL;DR: 本论文提出了IndicFairFace数据集，旨在解决现有视觉-语言模型（VLM）对印度内部地理多样性代表不足的问题，并通过新数据集量化并减少主流VLM中的地理偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的VLM模型虽然在性别和大种族上实现了更公平的代表，但仍然将“印度”视为单一类别，忽视了印度内部复杂的地理和种族多样性。这导致群体内部代表不足和地理偏见。论文希望通过更加细致的数据集设计，提升印度各地区群体在模型中的公平性。

Method: 作者收集并整理了IndicFairFace人脸图像数据集，包含14,400张自印度各地采集的图像，并实现了地区和性别的均衡代表。借助该数据集，作者对主流基于CLIP的VLM模型在印度国内地理偏见进行了量化评估，并采用迭代空域投影（Iterative Nullspace Projection）技术对模型进行后处理去偏。

Result: 通过IndicFairFace数据集，实验表明主流VLM模型存在明显的印度国内地理偏见，在采用迭代空域投影后能有效减小偏见，而且去偏对主流检索数据集的准确率影响很小（下降不足1.5%）。

Conclusion: IndicFairFace数据集为研究和改进印度地区VLM地理代表性提供了新的基准，有效揭示和缓解了模型的地理偏见，对模型公平性研究具有重要意义。

Abstract: Vision-Language Models (VLMs) are known to inherit and amplify societal biases from their web-scale training data with Indian being particularly misrepresented. Existing fairness-aware datasets have significantly improved demographic balance across global race and gender groups, yet they continue to treat Indian as a single monolithic category. The oversimplification ignores the vast intra-national diversity across 28 states and 8 Union Territories of India and leads to representational and geographical bias. To address the limitation, we present IndicFairFace, a novel and balanced face dataset comprising 14,400 images representing geographical diversity of India. Images were sourced ethically from Wikimedia Commons and open-license web repositories and uniformly balanced across states and gender. Using IndicFairFace, we quantify intra-national geographical bias in prominent CLIP-based VLMs and reduce it using post-hoc Iterative Nullspace Projection debiasing approach. We also show that the adopted debiasing approach does not adversely impact the existing embedding space as the average drop in retrieval accuracy on benchmark datasets is less than 1.5 percent. Our work establishes IndicFairFace as the first benchmark to study geographical bias in VLMs for the Indian context.

</details>


### [27] [Motion Prior Distillation in Time Reversal Sampling for Generative Inbetweening](https://arxiv.org/abs/2602.12679)
*Wooseok Jeon,Seunghyun Shin,Dongmin Shin,Hae-Gon Jeon*

Main category: cs.CV

TL;DR: 本文提出了一种新的推理阶段图像到视频插帧方法，通过减少前后路径的不一致性，提升了生成中间帧的连续性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有I2V扩散模型的插帧方法在推理阶段容易因前向与后向路径生成的不一致导致视频时间上的不连续和视觉伪影。主要原因是两条路径分别服从自身起始帧的运动先验，难以对齐。为解决此问题，亟需对运动先验进行更有效的融合和对齐。

Method: 作者提出了运动先验蒸馏（Motion Prior Distillation, MPD）方法，在推理阶段将前向路径的运动残差蒸馏到后向路径，通过压制双向不匹配来提升插帧连贯性。核心在于避免对尾端条件帧的去噪，从而减少运动路径的歧义，使生成结果更好地继承了前向运动的先验。该方法无需对模型进行额外训练。

Result: 实验证明，MPD方法在标准基准上有更好的量化指标表现，并通过用户调研证明其在实际应用场景中的有效性，生成视频的时间一致性和视觉质量均优于现有方法。

Conclusion: MPD是一种简单又有效的推理阶段技术，能够显著提升I2V扩散模型在插帧任务中的表现，特别是在时间连续性和视觉体验方面，为无需训练的插帧策略提供了新的思路。

Abstract: Recent progress in image-to-video (I2V) diffusion models has significantly advanced the field of generative inbetweening, which aims to generate semantically plausible frames between two keyframes. In particular, inference-time sampling strategies, which leverage the generative priors of large-scale pre-trained I2V models without additional training, have become increasingly popular. However, existing inference-time sampling, either fusing forward and backward paths in parallel or alternating them sequentially, often suffers from temporal discontinuities and undesirable visual artifacts due to the misalignment between the two generated paths. This is because each path follows the motion prior induced by its own conditioning frame. In this work, we propose Motion Prior Distillation (MPD), a simple yet effective inference-time distillation technique that suppresses bidirectional mismatch by distilling the motion residual of the forward path into the backward path. Our method can deliberately avoid denoising the end-conditioned path which causes the ambiguity of the path, and yield more temporally coherent inbetweening results with the forward motion prior. We not only perform quantitative evaluations on standard benchmarks, but also conduct extensive user studies to demonstrate the effectiveness of our approach in practical scenarios.

</details>


### [28] [Channel-Aware Probing for Multi-Channel Imaging](https://arxiv.org/abs/2602.12696)
*Umar Marikkar,Syed Sameed Husain,Muhammad Awais,Sara Atito*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的通道感知探测方法（Channel-Aware Probing, CAP），显著提升了多通道成像（MCI）数据下预训练视觉编码器的迁移性能，缩小了冷启动与全微调的性能差距。


<details>
  <summary>Details</summary>
Motivation: 目前视觉编码器在多通道成像数据（如医学影像，不同通道代表不同生物信息）上的训练和评估面临挑战。因为不同数据集的通道配置往往不同，导致无法直接重用固定通道数的预训练模型；而现有方法大多依赖全微调，对冻结编码器的探测（probing）方式研究较少且效果不佳。本研究希望提升冻结预训练视觉编码器在新通道配置下的下游任务表现，解决直接转用其他领域策略在MCI上效果差的问题。

Method: 作者提出了Channel-Aware Probing（CAP）框架，包含两个核心设计：1）独立特征编码（IFE），每个通道独立编码，避免通道间干扰；2）解耦池化（DCP），先在每通道内池化，再跨通道聚合。这样既保留了每个通道的独特性，又可灵活适应通道数变化，最终通过冻结编码器、仅训练probe实现高效迁移。

Result: 在三个MCI领域的基准测试中，CAP相较于默认探测协议有稳定提升，性能甚至可媲美从头微调，并大大缩小了与全微调的差距。直接复用其它领域fixed representation探测方法在MCI中表现远不如CAP，甚至劣于从头训练。

Conclusion: CAP能有效利用MCI数据的通道多样性，显著提升冻结视觉编码器在变通道配置下的迁移与探测性能，为多通道领域提供了实用解决思路，也推动了固定表示高效迁移的研究。

Abstract: Training and evaluating vision encoders on Multi-Channel Imaging (MCI) data remains challenging as channel configurations vary across datasets, preventing fixed-channel training and limiting reuse of pre-trained encoders on new channel settings. Prior work trains MCI encoders but typically evaluates them via full fine-tuning, leaving probing with frozen pre-trained encoders comparatively underexplored. Existing studies that perform probing largely focus on improving representations, rather than how to best leverage fixed representations for downstream tasks. Although the latter problem has been studied in other domains, directly transferring those strategies to MCI yields weak results, even worse than training from scratch. We therefore propose Channel-Aware Probing (CAP), which exploits the intrinsic inter-channel diversity in MCI datasets by controlling feature flow at both the encoder and probe levels. CAP uses Independent Feature Encoding (IFE) to encode each channel separately, and Decoupled Pooling (DCP) to pool within channels before aggregating across channels. Across three MCI benchmarks, CAP consistently improves probing performance over the default probing protocol, matches fine-tuning from scratch, and largely reduces the gap to full fine-tuning from the same MCI pre-trained checkpoints. Code can be found in https://github.com/umarikkar/CAP.

</details>


### [29] [ART3mis: Ray-Based Textual Annotation on 3D Cultural Objects](https://arxiv.org/abs/2602.12725)
*Vasileios Arampatzakis,Vasileios Sevetlidis,Fotis Arnaoutoglou,Athanasios Kalogeras,Christos Koulamas,Aris Lalos,Chairi Kiourt,George Ioannakis,Anestis Koutsoudis,George Pavlidis*

Main category: cs.CV

TL;DR: 本文提出了一款名为ART3mis的交互式3D文物注释工具，允许非专业人员轻松分割和标注三维数字化文物，并以JSON格式保存标注信息。


<details>
  <summary>Details</summary>
Motivation: 当前大多数3D可视化工具缺乏针对文化遗产专业人士（如文物保护、修复、策展等）友好的高级交互功能，尤其是在三维对象上特定区域的批注和元数据附加方面存在局限性。

Method: 开发了ART3mis工具，采用用户主导的直接表面注释方式，支持实时操作与详细三维文化遗产对象处理，并能将对多个复杂区域的文本注释存储为JSON格式数据。

Result: ART3mis实现了对三维对象的直观分割和注释，保证了易用性和高互动性，适用于没有专业3D图形技能的文物保护相关工作者。

Conclusion: ART3mis为文化遗产领域提供了一种通用、友好且互动性强的3D对象标注工具，能够促进非技术背景用户更好地管理和注释数字化文物。

Abstract: Beyond simplistic 3D visualisations, archaeologists, as well as cultural heritage experts and practitioners, need applications with advanced functionalities. Such as the annotation and attachment of metadata onto particular regions of the 3D digital objects. Various approaches have been presented to tackle this challenge, most of which achieve excellent results in the domain of their application. However, they are often confined to that specific domain and particular problem. In this paper, we present ART3mis - a general-purpose, user-friendly, interactive textual annotation tool for 3D objects. Primarily attuned to aid cultural heritage conservators, restorers and curators with no technical skills in 3D imaging and graphics, the tool allows for the easy handling, segmenting and annotating of 3D digital replicas of artefacts. ART3mis applies a user-driven, direct-on-surface approach. It can handle detailed 3D cultural objects in real-time and store textual annotations for multiple complex regions in JSON data format.

</details>


### [30] [VimRAG: Navigating Massive Visual Context in Retrieval-Augmented Generation via Multimodal Memory Graph](https://arxiv.org/abs/2602.12735)
*Qiuchen Wang,Shihang Wang,Yu Zeng,Qiang Zhang,Fanrui Zhang,Zhuoning Guo,Bosi Zhang,Wenxuan Huang,Lin Chen,Zehui Chen,Pengjun Xie,Ruixue Ding*

Main category: cs.CV

TL;DR: 本文提出了一种名为VimRAG的新框架，用于提升多模态信息（文本、图片和视频）的检索与推理效率，显著超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成（RAG）方法难以高效处理长上下文和含有大量视觉数据的任务，特别在需要反复推理的场景下表现不佳。因此，作者希望通过设计新方法，破解多模态信息检索与推理中的效率和表现瓶颈。

Method: 作者提出VimRAG框架，通过把推理过程建模为动态DAG（有向无环图），结构化存储agent的状态及检索到的多模态证据。创新性地引入了图调制视觉记忆编码机制，根据证据在图结构中的位置动态分配高分辨率token以聚焦关键证据。同时，提出了图引导策略优化算法，通过剪枝冗余节点以实现精准的奖励分配。

Result: 实验显示，VimRAG在多个主流多模态RAG基准数据集上都取得了当前最佳的效果，性能优于传统方法。

Conclusion: VimRAG有效提升了多模态检索增强推理效率和表现，是解决长上下文、多模态复杂任务新颖且实用的方法。

Abstract: Effectively retrieving, reasoning, and understanding multimodal information remains a critical challenge for agentic systems. Traditional Retrieval-augmented Generation (RAG) methods rely on linear interaction histories, which struggle to handle long-context tasks, especially those involving information-sparse yet token-heavy visual data in iterative reasoning scenarios. To bridge this gap, we introduce VimRAG, a framework tailored for multimodal Retrieval-augmented Reasoning across text, images, and videos. Inspired by our systematic study, we model the reasoning process as a dynamic directed acyclic graph that structures the agent states and retrieved multimodal evidence. Building upon this structured memory, we introduce a Graph-Modulated Visual Memory Encoding mechanism, with which the significance of memory nodes is evaluated via their topological position, allowing the model to dynamically allocate high-resolution tokens to pivotal evidence while compressing or discarding trivial clues. To implement this paradigm, we propose a Graph-Guided Policy Optimization strategy. This strategy disentangles step-wise validity from trajectory-level rewards by pruning memory nodes associated with redundant actions, thereby facilitating fine-grained credit assignment. Extensive experiments demonstrate that VimRAG consistently achieves state-of-the-art performance on diverse multimodal RAG benchmarks. The code is available at https://github.com/Alibaba-NLP/VRAG.

</details>


### [31] [SPRig: Self-Supervised Pose-Invariant Rigging from Mesh Sequences](https://arxiv.org/abs/2602.12740)
*Ruipeng Wang,Langkun Zhong,Miaowei Wang*

Main category: cs.CV

TL;DR: SPRig方法是一种用于非标准姿态数据序列（如动物动作捕捉或视频生成网格序列）的精细调优框架，通过引入跨帧一致性损失，有效提升了骨骼绑定的时序一致性与稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有主流骨骼绑定方法依赖于标准静止姿态（如T-pose），无法直接应用于缺少此姿态的网格序列数据，导致逐帧操作时出现拓扑不一致和姿态相关性问题。因此需要能在无标准姿态下保持跨帧一致的新方法。

Method: 提出了SPRig通用精细调优框架，在已有模型基础上加入跨帧一致性损失，通过端到端优化，获得姿态无关的绑定结果。并引入新的序列稳定性评测协议，确保方法普适且量化准确。

Result: 实验表明，SPRig在多种挑战性序列数据上具有最优的时序稳定性，有效消除了现有方法出现的绑定碎片和伪影等问题。

Conclusion: SPRig能够有效提升无T姿态网格序列的绑定一致性和稳健性，具有较强的实际应用价值。代码将在论文接收后开源。

Abstract: State-of-the-art rigging methods assume a canonical rest pose--an assumption that fails for sequential data (e.g., animal motion capture or AIGC/video-derived mesh sequences) that lack the T-pose. Applied frame-by-frame, these methods are not pose-invariant and produce topological inconsistencies across frames. Thus We propose SPRig, a general fine-tuning framework that enforces cross-frame consistency losses to learn pose-invariant rigs on top of existing models. We validate our approach on rigging using a new permutation-invariant stability protocol. Experiments demonstrate SOTA temporal stability: our method produces coherent rigs from challenging sequences and dramatically reduces the artifacts that plague baseline methods. The code will be released publicly upon acceptance.

</details>


### [32] [Synthetic Craquelure Generation for Unsupervised Painting Restoration](https://arxiv.org/abs/2602.12742)
*Jana Cuch-Guillén,Antonio Agudo,Raül Pérez-Gonzalo*

Main category: cs.CV

TL;DR: 本文提出了一种无需人工标注的新方法，通过生成仿真裂纹和混合检测-细化机制，实现对绘画裂缝的精确修复，并在无需预先训练的零样本环境下效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统绘画修复希望采用非侵入性数字化方式，但因裂纹精细、数据缺失，且缺乏注释，自动识别和修复裂痕十分困难。

Method: 利用Bézier轨迹构建仿真裂缝生成器，结合形态学裂缝检测器与采用Low-Rank Adaptation的SegFormer深度网络，输入引入检测到的空间先验，训练时结合掩模和自适应损失，最终借助各向异性扩散进行修复。

Result: 零样本实验表明，该方法在裂纹修复精度和对原画笔触保留方面均显著优于当前主流数码修复方法。

Conclusion: 本文提出的零注释裂纹修复流程为传统绘画文物数字修复提供了新的可行思路，无需人工标注即可实现高质量自动修复。

Abstract: Cultural heritage preservation increasingly demands non-invasive digital methods for painting restoration, yet identifying and restoring fine craquelure patterns from complex brushstrokes remains challenging due to scarce pixel-level annotations. We propose a fully annotation-free framework driven by a domain-specific synthetic craquelure generator, which simulates realistic branching and tapered fissure geometry using Bézier trajectories. Our approach couples a classical morphological detector with a learning-based refinement module: a SegFormer backbone adapted via Low-Rank Adaptation (LoRA). Uniquely, we employ a detector-guided strategy, injecting the morphological map as an input spatial prior, while a masked hybrid loss and logit adjustment constrain the training to focus specifically on refining candidate crack regions. The refined masks subsequently guide an Anisotropic Diffusion inpainting stage to reconstruct missing content. Experimental results demonstrate that our pipeline significantly outperforms state-of-the-art photographic restoration models in zero-shot settings, while faithfully preserving the original paint brushwork.

</details>


### [33] [ReBA-Pred-Net: Weakly-Supervised Regional Brain Age Prediction on MRI](https://arxiv.org/abs/2602.12751)
*Shuai Shao,Yan Wang,Shu Jiang,Shiyuan Zhao,Xinzhe Luo,Di Yang,Jiangtao Wang,Yutong Bai,Jianguo Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的网络模型ReBA-Pred-Net，用于更细致地预测大脑不同区域的生理年龄（即regional brain age），并引入了新的评估指标，其方法在多种测试中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有大脑年龄的研究大多仅预测整体大脑年龄，难以反映疾病等局部区域变化，对脑健康评估和疾病诊断作用有限，因此需要开发可泛化、稳健的区域性大脑年龄估计方法。

Method: 提出Teacher-Student结构的ReBA-Pred-Net，Teacher网络生成区域性大脑年龄（ReBA）软标签监督Student网络学习，并通过功能区域先验一致性约束提升结果可靠性。评估方面，提出健康对照相似度（HCS）和神经疾病相关性（NDC）作为统计和临床有效性的间接评价指标。

Result: 实验在多种主干结构下验证，结果显示所提模型在统计一致性和事实（临床）一致性方面均优于已有方法。

Conclusion: ReBA-Pred-Net模型可实现精细、可靠的区域性大脑年龄估计，并通过新指标实现对模型泛化能力和临床意义的全面评估，为大脑健康研究和疾病检测提供更具价值的工具。

Abstract: Brain age has become a prominent biomarker of brain health. Yet most prior work targets whole brain age (WBA), a coarse paradigm that struggles to support tasks such as disease characterization and research on development and aging patterns, because relevant changes are typically region-selective rather than brain-wide. Therefore, robust regional brain age (ReBA) estimation is critical, yet a widely generalizable model has yet to be established. In this paper, we propose the Regional Brain Age Prediction Network (ReBA-Pred-Net), a Teacher-Student framework designed for fine-grained brain age estimation. The Teacher produces soft ReBA to guide the Student to yield reliable ReBA estimates with a clinical-prior consistency constraint (regions within the same function should change similarly). For rigorous evaluation, we introduce two indirect metrics: Healthy Control Similarity (HCS), which assesses statistical consistency by testing whether regional brain-age-gap (ReBA minus chronological age) distributions align between training and unseen HC; and Neuro Disease Correlation (NDC), which assesses factual consistency by checking whether clinically confirmed patients show elevated brain-age-gap in disease-associated regions. Experiments across multiple backbones demonstrate the statistical and factual validity of our method.

</details>


### [34] [Towards reconstructing experimental sparse-view X-ray CT data with diffusion models](https://arxiv.org/abs/2602.12755)
*Nelas J. Thomsen,Xinyuan Wang,Felix Lucka,Ezgi Demircan-Tureyen*

Main category: cs.CV

TL;DR: 论文研究了基于扩散模型的图像生成技术在稀疏视角CT成像中的应用，考察了合成数据与真实实验数据之间的数据域差异（domain shift）和前向模型不一致对重建效果的影响。结果发现，这些误差会影响模型可靠性，强调了在真实数据上的验证重要性。


<details>
  <summary>Details</summary>
Motivation: 大部分基于扩散模型的CT图像重建方法只在合成数据上测试，缺乏对实际实验数据的适用性和健壮性分析。作者关注于域差异和前向模型不一致真实情况下的影响。

Method: 作者采集了物理模型的CT实验数据，并用不同程度域差异的合成图像数据集训练扩散先验。采用分解的扩散采样（Decomposed Diffusion Sampling）方法在难度递增的稀疏视角CT数据集（直至真实数据）上进行测试，评估先验和前向模型误差的影响。

Result: 结果显示，严重的域差异会导致模型失效和幻觉现象，但多样化先验优于高度匹配但狭窄的先验。前向模型误差会使采样结果偏离先验分布并产生伪影，但可通过退火似然调度缓解并提升计算效率。

Conclusion: 基于扩散模型的CT重建在实验数据上的效果不能简单从合成数据泛化而来，未来相关方法必须在现实世界基准上严格验证。

Abstract: Diffusion-based image generators are promising priors for ill-posed inverse problems like sparse-view X-ray Computed Tomography (CT). As most studies consider synthetic data, it is not clear whether training data mismatch (``domain shift'') or forward model mismatch complicate their successful application to experimental data. We measured CT data from a physical phantom resembling the synthetic Shepp-Logan phantom and trained diffusion priors on synthetic image data sets with different degrees of domain shift towards it. Then, we employed the priors in a Decomposed Diffusion Sampling scheme on sparse-view CT data sets with increasing difficulty leading to the experimental data. Our results reveal that domain shift plays a nuanced role: while severe mismatch causes model collapse and hallucinations, diverse priors outperform well-matched but narrow priors. Forward model mismatch pulls the image samples away from the prior manifold, which causes artifacts but can be mitigated with annealed likelihood schedules that also increase computational efficiency. Overall, we demonstrate that performance gains do not immediately translate from synthetic to experimental data, and future development must validate against real-world benchmarks.

</details>


### [35] [Towards complete digital twins in cultural heritage with ART3mis 3D artifacts annotator](https://arxiv.org/abs/2602.12761)
*Dimitrios Karamatskos,Vasileios Arampatzakis,Vasileios Sevetlidis,Stavros Nousias,Athanasios Kalogeras,Christos Koulamas,Aris Lalos,George Pavlidis*

Main category: cs.CV

TL;DR: 本文提出了一款面向文化遗产领域的3D数字文物注释工具ART3mis，支持基于网络的交互式文本注释，并符合W3C标准，易用性强，适合缺乏3D图形专长的用户。


<details>
  <summary>Details</summary>
Motivation: 考古与文化遗产专业人士需要在3D数字文物基础上进行区域性注释和元数据附加，但现有工具大多功能局限且缺乏通用性与互操作性。

Method: 开发了ART3mis，一个基于Web的、符合W3C Web Annotation Data Model标准的3D数字文物交互式文本注释工具，强调通用性、易用性和功能丰富性。

Result: ART3mis可以让非技术用户轻松对3D文物模型进行分割与注释，且信息易于交流、分发与复用。

Conclusion: ART3mis为文化遗产的数字管理和交流提供了更为通用、便捷、标准化的解决方案，提升了相关人员的工作效率和数字文物的价值利用。

Abstract: Archaeologists, as well as specialists and practitioners in cultural heritage, require applications with additional functions, such as the annotation and attachment of metadata to specific regions of the 3D digital artifacts, to go beyond the simplistic three-dimensional (3D) visualization. Different strategies addressed this issue, most of which are excellent in their particular area of application, but their capacity is limited to their design's purpose; they lack generalization and interoperability. This paper introduces ART3mis, a general-purpose, user-friendly, feature-rich, interactive web-based textual annotation tool for 3D objects. Moreover, it enables the communication, distribution, and reuse of information as it complies with the W3C Web Annotation Data Model. It is primarily designed to help cultural heritage conservators, restorers, and curators who lack technical expertise in 3D imaging and graphics, handle, segment, and annotate 3D digital replicas of artifacts with ease.

</details>


### [36] [PixelRush: Ultra-Fast, Training-Free High-Resolution Image Generation via One-step Diffusion](https://arxiv.org/abs/2602.12769)
*Hong-Phuc Lai,Phong Nguyen,Anh Tran*

Main category: cs.CV

TL;DR: 本文提出了PixelRush，一种高效的高分辨率文本生成图像方法，相比现有方法大幅提升速度并保证高画质。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型受限于训练时的分辨率，现有提升分辨率的无训练方法推理效率极低，难以实际应用，因此需要更高效的高分辨率图像生成方案。

Method: PixelRush采用了基于patch的推理范式，但无需多次逆转和重生成过程，实现了高效、低步数的patch去噪。同时，为了解决少步生成中patch融合产生的伪影，提出了无缝融合策略，并通过噪声注入机制缓解过度平滑问题。整个流程无需微调即可适应不同分辨率需求。

Result: PixelRush在生成速度上远超现有最新方法（4K图像约20秒，提速10到35倍），且输出图像的视觉效果优异。大量实验验证了效率和画质的进步。

Conclusion: PixelRush为无需微调的高分辨率文本生成图像任务提供了高效且高质量的解决方案，推动了实际应用的可行性。

Abstract: Pre-trained diffusion models excel at generating high-quality images but remain inherently limited by their native training resolution. Recent training-free approaches have attempted to overcome this constraint by introducing interventions during the denoising process; however, these methods incur substantial computational overhead, often requiring more than five minutes to produce a single 4K image. In this paper, we present PixelRush, the first tuning-free framework for practical high-resolution text-to-image generation. Our method builds upon the established patch-based inference paradigm but eliminates the need for multiple inversion and regeneration cycles. Instead, PixelRush enables efficient patch-based denoising within a low-step regime. To address artifacts introduced by patch blending in few-step generation, we propose a seamless blending strategy. Furthermore, we mitigate over-smoothing effects through a noise injection mechanism. PixelRush delivers exceptional efficiency, generating 4K images in approximately 20 seconds representing a 10$\times$ to 35$\times$ speedup over state-of-the-art methods while maintaining superior visual fidelity. Extensive experiments validate both the performance gains and the quality of outputs achieved by our approach.

</details>


### [37] [Bootstrapping MLLM for Weakly-Supervised Class-Agnostic Object Counting](https://arxiv.org/abs/2602.12774)
*Xiaowen Zhang,Zijie Yue,Yong Luo,Cairong Zhao,Qijun Chen,Miaojing Shi*

Main category: cs.CV

TL;DR: 该论文提出了一种基于多模态大模型（MLLM）的弱监督、类别无关目标计数方法WS-COC，并通过三种策略实现在训练和测试阶段下的高效目标计数，在多个数据集上的表现优于甚至超越部分全监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有目标计数方法多依赖高昂的标注成本（每个目标的点级标注），而弱监督计数方法虽然降低了标注成本，却通常仅能计数单一类别。因此急需一种能够以低标注成本并且适用于多类别（或类别无关）的计数方法。

Method: 提出WS-COC，利用MLLM驱动的弱监督框架，实现类无关的目标计数。具体方法包括三个策略：1）分割识别对话微调，通过多轮对话指导MLLM判断目标数量区间并逐步缩小；2）对比优化策略，让MLLM学习根据目标数量对多张图片进行相对排序优化；3）全局和局部计数增强，将全局与局部计数结果融合，提升密集场景下的计数能力。

Result: 在FSC-147、CARPK、PUCPR+和ShanghaiTech等多个公开数据集上，通过广泛实验，WS-COC在大幅度减少标注成本的同时，达到了与甚至超越许多最新全监督方法的计数精度。

Conclusion: WS-COC首次结合MLLM实现了类无关的弱监督目标计数，显著降低了对标注的依赖，并能胜任多类别和密集场景下的计数任务，为实际落地和全自动计数应用提供了更优解。

Abstract: Object counting is a fundamental task in computer vision, with broad applicability in many real-world scenarios. Fully-supervised counting methods require costly point-level annotations per object. Few weakly-supervised methods leverage only image-level object counts as supervision and achieve fairly promising results. They are, however, often limited to counting a single category, e.g. person. In this paper, we propose WS-COC, the first MLLM-driven weakly-supervised framework for class-agnostic object counting. Instead of directly fine-tuning MLLMs to predict object counts, which can be challenging due to the modality gap, we incorporate three simple yet effective strategies to bootstrap the counting paradigm in both training and testing: First, a divide-and-discern dialogue tuning strategy is proposed to guide the MLLM to determine whether the object count falls within a specific range and progressively break down the range through multi-round dialogue. Second, a compare-and-rank count optimization strategy is introduced to train the MLLM to optimize the relative ranking of multiple images according to their object counts. Third, a global-and-local counting enhancement strategy aggregates and fuses local and global count predictions to improve counting performance in dense scenes. Extensive experiments on FSC-147, CARPK, PUCPR+, and ShanghaiTech show that WS-COC matches or even surpasses many state-of-art fully-supervised methods while significantly reducing annotation costs. Code is available at https://github.com/viscom-tongji/WS-COC.

</details>


### [38] [GSM-GS: Geometry-Constrained Single and Multi-view Gaussian Splatting for Surface Reconstruction](https://arxiv.org/abs/2602.12796)
*Xiao Ren,Yu Liu,Ning An,Jian Cheng,Xin Qiao,He Kong*

Main category: cs.CV

TL;DR: 本文提出了一种针对3D Gaussian Splatting方法的改进框架GSM-GS，显著提升复杂表面微结构的重建精度和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting方法因点云结构无序，导致高频细节与复杂表面微结构易损失，影响重建准确性。需要更精细的优化框架来提升细节保留与多视角一致性。

Method: GSM-GS框架融合了单视图的自适应子区域加权与多视图空间结构优化：单视图阶段，利用图像梯度划分纹理丰富与稀疏区域，并通过深度差异引导自适应滤波，针对区域纹理特性施加双分支约束；多视图阶段，引入几何引导的跨视点云关联和动态加权采样，建立相邻帧间的3D结构法向约束，增强一致性。

Result: 在公开数据集上的大量实验表明，该方法在渲染质量和几何结构重建上均达到先进水平。

Conclusion: 该方法有效提升了3D Gaussian Splatting的细节复原与多视角一致性，为高保真高效三维重建提供了新思路。

Abstract: Recently, 3D Gaussian Splatting has emerged as a prominent research direction owing to its ultrarapid training speed and high-fidelity rendering capabilities. However, the unstructured and irregular nature of Gaussian point clouds poses challenges to reconstruction accuracy. This limitation frequently causes high-frequency detail loss in complex surface microstructures when relying solely on routine strategies. To address this limitation, we propose GSM-GS: a synergistic optimization framework integrating single-view adaptive sub-region weighting constraints and multi-view spatial structure refinement. For single-view optimization, we leverage image gradient features to partition scenes into texture-rich and texture-less sub-regions. The reconstruction quality is enhanced through adaptive filtering mechanisms guided by depth discrepancy features. This preserves high-weight regions while implementing a dual-branch constraint strategy tailored to regional texture variations, thereby improving geometric detail characterization. For multi-view optimization, we introduce a geometry-guided cross-view point cloud association method combined with a dynamic weight sampling strategy. This constructs 3D structural normal constraints across adjacent point cloud frames, effectively reinforcing multi-view consistency and reconstruction fidelity. Extensive experiments on public datasets demonstrate that our method achieves both competitive rendering quality and geometric reconstruction. See our interactive project page

</details>


### [39] [Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation](https://arxiv.org/abs/2602.12843)
*Yichen Zhao,Zelin Peng,Piao Yang,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: 该论文提出MMRad-IVL-22K，这是首个专为胸部X光片交错式视觉-语言推理设计的大规模数据集，能更好模拟放射科医生的诊断流程，并显著提升大模型诊断准确率和报告质量。


<details>
  <summary>Details</summary>
Motivation: 现有医学大模型在报告生成时，大多只进行一次视觉检查，随后仅用文本链式推理，容易出现幻觉问题，且简单的视觉坐标描述无法反映丰富图像细节。因此亟需一种能更好结合视觉与语言、反复交替推理的数据集和方法。

Method: 作者构建了MMRad-IVL-22K数据集，包含21994条胸部X光诊断路径，系统覆盖35个解剖区域。每条路径模拟专家反复视觉检查与推理，图像推理理由与文本描述紧密结合。基于该数据集，作者在多个主流LVLMs上进行微调与评价，比较多模态链式推理与文本链式推理的效果。

Result: 实验证明，应用MMRad-IVL-22K和多模态链式推理的大模型，在RadGraph等评测指标上报告准确率和质量超越仅用文本推理的模型，临床准确性提升明显（如RadGraph分数提升6%）。此外，基于该数据集微调的开源LVLMs在推理一致性和报告质量上，也优于现有的通用和医学专用LVLM。

Conclusion: 交错式视觉-语言证据对于医学AI的可靠推理至关重要，MMRad-IVL-22K数据集为提升AI在医学影像诊断中的准确性和解释性提供了坚实基础，可促进更可信赖的医学大模型发展。

Abstract: Radiological diagnosis is a perceptual process in which careful visual inspection and language reasoning are repeatedly interleaved. Most medical large vision language models (LVLMs) perform visual inspection only once and then rely on text-only chain-of-thought (CoT) reasoning, which operates purely in the linguistic space and is prone to hallucination. Recent methods attempt to mitigate this issue by introducing visually related coordinates, such as bounding boxes. However, these remain a pseudo-visual solution: coordinates are still text and fail to preserve rich visual details like texture and density. Motivated by the interleaved nature of radiological diagnosis, we introduce MMRad-IVL-22K, the first large-scale dataset designed for natively interleaved visual language reasoning in chest X-ray interpretation. MMRad-IVL-22K reflects a repeated cycle of reasoning and visual inspection workflow of radiologists, in which visual rationales complement textual descriptions and ground each step of the reasoning process. MMRad-IVL-22K comprises 21,994 diagnostic traces, enabling systematic scanning across 35 anatomical regions. Experimental results on advanced closed-source LVLMs demonstrate that report generation guided by multimodal CoT significantly outperforms that guided by text-only CoT in clinical accuracy and report quality (e.g., 6\% increase in the RadGraph metric), confirming that high-fidelity interleaved vision language evidence is a non-substitutable component of reliable medical AI. Furthermore, benchmarking across seven state-of-the-art open-source LVLMs demonstrates that models fine-tuned on MMRad-IVL-22K achieve superior reasoning consistency and report quality compared with both general-purpose and medical-specific LVLMs. The project page is available at https://github.com/qiuzyc/thinking_like_a_radiologist.

</details>


### [40] [RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads](https://arxiv.org/abs/2602.12877)
*Vijayasri Iyer,Maahin Rathinagiriswaran,Jyothikamalesh S*

Main category: cs.CV

TL;DR: 本文发布了Roadscapes，多任务、多模态的道路场景数据集，专为促进在印度多样化驾驶环境下的视觉场景理解研究。数据集含手动标注的9,000张图像及QA对，涵盖城市、农村、高速、乡村等多种复杂环境，并给出了基于视觉-语言模型的初步实验结果。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统依赖于对道路场景的精准理解，但现有数据集大多聚焦欧美等结构化路况，缺乏对印度等非结构化、多样化环境的支持。为解决此领域研究基础数据不足的问题，本文旨在构建适用于复杂、多变印道路环境的场景理解数据集，推动视觉理解与推理技术的发展。

Method: 通过收集来自印度不同地区和不同时间段的道路场景图像，手工标注图像中的物体包围框。利用规则型启发式方法推断场景属性，并据此自动生成问答对，覆盖目标检测、推理和场景理解等多项任务。最终数据集囊括多种日夜场景，并以视觉-语言模型提供QA任务的基线实验。

Result: Roadscapes数据集共含约9,000张标注图像，涵盖城市、农村、高速公路等多样场景，并包括手工验证的目标框与自动生成的丰富QA对。论文还统计并分析了数据集的结构特征，并利用视觉-语言模型在QA任务上进行了初步实验，展现基准性能。

Conclusion: Roadscapes数据集为自动驾驶视觉场景理解、推理等任务在印度等非结构化环境下提供了重要研究资源。基线实验表明，现有方法在复杂场景下仍有提升空间，该数据集可为未来模型开发与比较提供基础，推动领域进步。

Abstract: Understanding road scenes is essential for autonomous driving, as it enables systems to interpret visual surroundings to aid in effective decision-making. We present Roadscapes, a multitask multimodal dataset consisting of upto 9,000 images captured in diverse Indian driving environments, accompanied by manually verified bounding boxes. To facilitate scalable scene understanding, we employ rule-based heuristics to infer various scene attributes, which are subsequently used to generate question-answer (QA) pairs for tasks such as object grounding, reasoning, and scene understanding. The dataset includes a variety of scenes from urban and rural India, encompassing highways, service roads, village paths, and congested city streets, captured in both daytime and nighttime settings. Roadscapes has been curated to advance research on visual scene understanding in unstructured environments. In this paper, we describe the data collection and annotation process, present key dataset statistics, and provide initial baselines for image QA tasks using vision-language models.

</details>


### [41] [RADAR: Revealing Asymmetric Development of Abilities in MLLM Pre-training](https://arxiv.org/abs/2602.12892)
*Yunshuang Nie,Bingqian Lin,Minzhe Niu,Kun Xiang,Jianhua Han,Guowei Huang,Xingyue Quan,Hang Xu,Bokui Chen,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出了RADAR框架，实现对多模态大模型（MLLMs）预训练阶段感知与推理能力的高效评价，克服了现有评测效率低、不全面等问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型评测成本高、无法细致剖析模型感知和推理能力，且缺乏与预训练目标对齐的大规模基准数据集，限制了模型发展瓶颈的发现和分析。

Method: 提出RADAR评测框架，包括两个核心：一是Soft Discrimination Score指标，可在无监督微调情况下细致度量模型区分正确答案与干扰项的能力提升；二是Multi-Modal Mixture Benchmark，包含超1.5万样本的评价集，统一整合权威数据并补全关键能力空白，支持零样本评测多模态模型的感知与推理能力。

Result: 利用RADAR系统性揭示了MLLMs在不同数据量、模型规模和预训练策略下感知与推理能力提升的不对称性，显示传统“黑盒”评测难以观察的能力瓶颈。

Conclusion: RADAR框架揭示多模态大模型预训练过程中能力发展的不均衡性，强调能力分解视角，对于后续有针对性的模型优化与高效发展具有指导价值。代码已开源。

Abstract: Pre-trained Multi-modal Large Language Models (MLLMs) provide a knowledge-rich foundation for post-training by leveraging their inherent perception and reasoning capabilities to solve complex tasks. However, the lack of an efficient evaluation framework impedes the diagnosis of their performance bottlenecks. Current evaluation primarily relies on testing after supervised fine-tuning, which introduces laborious additional training and autoregressive decoding costs. Meanwhile, common pre-training metrics cannot quantify a model's perception and reasoning abilities in a disentangled manner. Furthermore, existing evaluation benchmarks are typically limited in scale or misaligned with pre-training objectives. Thus, we propose RADAR, an efficient ability-centric evaluation framework for Revealing Asymmetric Development of Abilities in MLLM pRe-training. RADAR involves two key components: (1) Soft Discrimination Score, a novel metric for robustly tracking ability development without fine-tuning, based on quantifying nuanced gradations of the model preference for the correct answer over distractors; and (2) Multi-Modal Mixture Benchmark, a new 15K+ sample benchmark for comprehensively evaluating pre-trained MLLMs' perception and reasoning abilities in a 0-shot manner, where we unify authoritative benchmark datasets and carefully collect new datasets, extending the evaluation scope and addressing the critical gaps in current benchmarks. With RADAR, we comprehensively reveal the asymmetric development of perceptual and reasoning capabilities in pretrained MLLMs across diverse factors, including data volume, model size, and pretraining strategy. Our RADAR underscores the need for a decomposed perspective on pre-training ability bottlenecks, informing targeted interventions to advance MLLMs efficiently. Our code is publicly available at https://github.com/Nieysh/RADAR.

</details>


### [42] [Robustness of Object Detection of Autonomous Vehicles in Adverse Weather Conditions](https://arxiv.org/abs/2602.12902)
*Fox Pettersen,Hong Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种评估自动驾驶车辆中目标检测模型在恶劣天气等不利环境下鲁棒性的方法，并用该方法比较了多种主流模型的表现，结果显示Faster R-CNN表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术的普及，确保在各种环境下的安全运行门槛变得日益重要。如何科学评估目标检测模型在复杂恶劣环境下的能力，是提升自动驾驶安全性的关键问题。

Method: 利用数据增强生成不同强度的恶劣天气和光照条件合成数据，测试目标检测模型在这些环境下的性能。用平均首个失效系数（AFFC）作为鲁棒性衡量指标，并对四种目标检测模型（YOLOv5s、YOLOv11s、Faster R-CNN、Detectron2）在七类不利环境下进行了实验。

Result: Faster R-CNN模型在全部七类不利条件下取得了71.9%的平均AFFC，YOLO系列为43%。实验验证了方法的可行性、高效性和有效性。实验还发现，针对恶劣环境的合成数据训练虽能提升鲁棒性，但过度训练会导致收益递减甚至出现健忘现象。

Conclusion: 提出的方法可有效量化比较目标检测模型在不良环境下的鲁棒性；Faster R-CNN表现优异。针对恶劣场景的专门训练需权衡防止过拟合和健忘问题。

Abstract: As self-driving technology advances toward widespread adoption, determining safe operational thresholds across varying environmental conditions becomes critical for public safety. This paper proposes a method for evaluating the robustness of object detection ML models in autonomous vehicles under adverse weather conditions. It employs data augmentation operators to generate synthetic data that simulates different severance degrees of the adverse operation conditions at progressive intensity levels to find the lowest intensity of the adverse conditions at which the object detection model fails. The robustness of the object detection model is measured by the average first failure coefficients (AFFC) over the input images in the benchmark. The paper reports an experiment with four object detection models: YOLOv5s, YOLOv11s, Faster R-CNN, and Detectron2, utilising seven data augmentation operators that simulate weather conditions fog, rain, and snow, and lighting conditions of dark, bright, flaring, and shadow. The experiment data show that the method is feasible, effective, and efficient to evaluate and compare the robustness of object detection models in various adverse operation conditions. In particular, the Faster R-CNN model achieved the highest robustness with an overall average AFFC of 71.9% over all seven adverse conditions, while YOLO variants showed the AFFC values of 43%. The method is also applied to assess the impact of model training that targets adverse operation conditions using synthetic data on model robustness. It is observed that such training can improve robustness in adverse conditions but may suffer from diminishing returns and forgetting phenomena (i.e., decline in robustness) if overtrained.

</details>


### [43] [Adaptive Scaling with Geometric and Visual Continuity of completed 3D objects](https://arxiv.org/abs/2602.12905)
*Jelle Vermandere,Maarten Bassier,Maarten Vergauwen*

Main category: cs.CV

TL;DR: 本论文提出一种新的方法，将静态的SDF对象补全结果转化为可编辑、结构合理的对象，解决了现有方法无法灵活缩放和变形的问题。


<details>
  <summary>Details</summary>
Motivation: 目前的对象补全网络生成的SDF虽能高质量还原物体形状，但不能灵活缩放或变形，否则会产生结构扭曲，限制了其在需要物体灵活操作的场景（如室内重设计、模拟、内容创作）中的应用。

Method: 方法从SOTA补全模型生成的SDF和纹理场出发，实现自动分割物体部件，定义可由用户控制的缩放区域，并对SDF、颜色和部件索引进行平滑插值，实现按比例、无伪影的变形。同时加入基于重复性结构的策略，以在大尺度变形时保留几何重复模式。

Result: 在Matterport3D和ShapeNet数据集实验表明，本方法优于全局缩放和简单选择性缩放，特别是在复杂形状和重复结构上实现了更有吸引力的可变形结果。

Conclusion: 该方法克服了补全SDF对象固有的刚性缺陷，实现了可编辑、保结构的物体操作，提升了视觉质量和实用性。

Abstract: Object completion networks typically produce static Signed Distance Fields (SDFs) that faithfully reconstruct geometry but cannot be rescaled or deformed without introducing structural distortions. This limitation restricts their use in applications requiring flexible object manipulation, such as indoor redesign, simulation, and digital content creation. We introduce a part-aware scaling framework that transforms these static completed SDFs into editable, structurally coherent objects. Starting from SDFs and Texture Fields generated by state-of-the-art completion models, our method performs automatic part segmentation, defines user-controlled scaling zones, and applies smooth interpolation of SDFs, color, and part indices to enable proportional and artifact-free deformation. We further incorporate a repetition-based strategy to handle large-scale deformations while preserving repeating geometric patterns. Experiments on Matterport3D and ShapeNet objects show that our method overcomes the inherent rigidity of completed SDFs and is visually more appealing than global and naive selective scaling, particularly for complex shapes and repetitive structures.

</details>


### [44] [Reliable Thinking with Images](https://arxiv.org/abs/2602.12916)
*Haobin Li,Yutong Yang,Yijie Lin,Dai Xiang,Mouxing Yang,Xi Peng*

Main category: cs.CV

TL;DR: 本文提出多模态大模型在“图像思维链”推理过程中会因不可靠的中间步骤而导致误差积累，即遇到“噪音思维（NT）”问题。为此，作者提出了“可靠图像思维（RTWI）”方法，通过评估视觉和文本推理片段的可靠性，有效过滤和整合答案，极大提升了模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有TWI方法假设图文链条的每一步都正确，但实际上由于多模态理解的复杂性，易出现推理链条中包含错误信息，从而影响最终的推理结果。如何应对这种“噪音思维”问题，成为提升多模态推理能力的关键。

Method: 作者提出RTWI方法，统一地从文本出发对视觉线索和文本推理步骤进行可靠性估计，并引入稳健的过滤和投票模块，以防止错误中间步骤影响最终答案。

Result: 在7个基准数据集上的大量实验证明，RTWI方法能显著减轻NT问题影响，相比现有方法在多项指标上实现更优表现。

Conclusion: 考虑到推理链可能出错并会累积影响，RTWI通过评估和过滤不可靠思维步骤，有效提升了多模态大模型的推理可靠性和实用性。

Abstract: As a multimodal extension of Chain-of-Thought (CoT), Thinking with Images (TWI) has recently emerged as a promising avenue to enhance the reasoning capability of Multi-modal Large Language Models (MLLMs), which generates interleaved CoT by incorporating visual cues into the textual reasoning process. However, the success of existing TWI methods heavily relies on the assumption that interleaved image-text CoTs are faultless, which is easily violated in real-world scenarios due to the complexity of multimodal understanding. In this paper, we reveal and study a highly-practical yet under-explored problem in TWI, termed Noisy Thinking (NT). Specifically, NT refers to the imperfect visual cues mining and answer reasoning process. As the saying goes, ``One mistake leads to another'', erroneous interleaved CoT would cause error accumulation, thus significantly degrading the performance of MLLMs. To solve the NT problem, we propose a novel method dubbed Reliable Thinking with Images (RTWI). In brief, RTWI estimates the reliability of visual cues and textual CoT in a unified text-centric manner and accordingly employs robust filtering and voting modules to prevent NT from contaminating the final answer. Extensive experiments on seven benchmarks verify the effectiveness of RTWI against NT.

</details>


### [45] [EPRBench: A High-Quality Benchmark Dataset for Event Stream Based Visual Place Recognition](https://arxiv.org/abs/2602.12919)
*Xiao Wang,Xingxing Xiong,Jinfeng Gao,Xufeng Lou,Bo Jiang,Si-bao Chen,Yaowei Wang,Yonghong Tian*

Main category: cs.CV

TL;DR: 该论文提出了EPRBench，一个面向事件流视觉位置识别（VPR）的高质量基准数据集，并结合大模型生成的语义描述，支撑多模态融合和可解释性研究。


<details>
  <summary>Details</summary>
Motivation: 传统可见光相机在低照度、过曝、高速运动等极端条件下存在不足，现有事件流VPR领域缺乏高质量、场景丰富的数据集和语义辅助手段，影响了相关算法的发展与评测。

Method: 作者构建了包含1万个事件片段和6.5万个事件帧的EPRBench数据集，涵盖多种采集方式、视角、气候和光照条件，并用大语言模型（LLM）自动生成场景描述，再经人工校正；评测了15种主流VPR算法，提出用LLM生成文本描述辅助空间注意力选择和跨模态特征融合的新范式。

Result: EPRBench为事件流VPR算法提供了统一、高质量的评测平台，丰富了语义辅助研究，提出的多模态融合框架在地点识别精度和可解释性方面均表现优异。

Conclusion: EPRBench拓展了事件流视觉位置识别的研究场景，推动了多模态理解与可解释性方法的发展，为后续算法创新和实际应用奠定数据和方法基础。

Abstract: Event stream-based Visual Place Recognition (VPR) is an emerging research direction that offers a compelling solution to the instability of conventional visible-light cameras under challenging conditions such as low illumination, overexposure, and high-speed motion. Recognizing the current scarcity of dedicated datasets in this domain, we introduce EPRBench, a high-quality benchmark specifically designed for event stream-based VPR. EPRBench comprises 10K event sequences and 65K event frames, collected using both handheld and vehicle-mounted setups to comprehensively capture real-world challenges across diverse viewpoints, weather conditions, and lighting scenarios. To support semantic-aware and language-integrated VPR research, we provide LLM-generated scene descriptions, subsequently refined through human annotation, establishing a solid foundation for integrating LLMs into event-based perception pipelines. To facilitate systematic evaluation, we implement and benchmark 15 state-of-the-art VPR algorithms on EPRBench, offering a strong baseline for future algorithmic comparisons. Furthermore, we propose a novel multi-modal fusion paradigm for VPR: leveraging LLMs to generate textual scene descriptions from raw event streams, which then guide spatially attentive token selection, cross-modal feature fusion, and multi-scale representation learning. This framework not only achieves highly accurate place recognition but also produces interpretable reasoning processes alongside its predictions, significantly enhancing model transparency and explainability. The dataset and source code will be released on https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [46] [Human-Aligned MLLM Judges for Fine-Grained Image Editing Evaluation: A Benchmark, Framework, and Analysis](https://arxiv.org/abs/2602.13028)
*Runzhou Liu,Hailey Weingord,Sejal Mittal,Prakhar Dungarwal,Anusha Nandula,Bo Ni,Samyadeep Basu,Hongjie Chen,Nesreen K. Ahmed,Li Li,Jiayi Zhang,Koustava Goswami,Subhojyoti Mukherjee,Branislav Kveton,Puneet Mathur,Franck Dernoncourt,Yue Zhao,Yu Wang,Ryan A. Rossi,Zhengzhong Tu,Hongru Du*

Main category: cs.CV

TL;DR: 本文提出了一种细粒度、可解释的基于多模态大语言模型 (MLLM) 的图片编辑评估框架，并构建了相关基准和实证分析，证明这一方法能更贴近人类判断。


<details>
  <summary>Details</summary>
Motivation: 传统的图片编辑评估指标粗糙且解释性有限，难以反映人类重要关注点，比如可控性、编辑定位以及对用户指令的忠实度，因此亟需更细致有效的评估方法。

Method: 作者提出MLLM-as-a-Judge框架，将图片编辑的常见评估指标分解为涵盖图片保持、编辑质量和指令忠实度的12个细粒度因子，并基于此设计了结合人工评测、MLLM判别、模型输出及传统指标的新基准，用以评估各类编辑任务。

Result: 通过大量人工实验，作者证明MLLM判官与人类的细粒度判断高度一致，并能识别传统指标难以区分的过度编辑或语义不准确等问题。

Conclusion: 文章提出的细粒度MLLM评估方法更贴近人类直觉，具有良好的可扩展性和实用性，可作为图片编辑任务研究和优化的重要基础。

Abstract: Evaluating image editing models remains challenging due to the coarse granularity and limited interpretability of traditional metrics, which often fail to capture aspects important to human perception and intent. Such metrics frequently reward visually plausible outputs while overlooking controllability, edit localization, and faithfulness to user instructions. In this work, we introduce a fine-grained Multimodal Large Language Model (MLLM)-as-a-Judge framework for image editing that decomposes common evaluation notions into twelve fine-grained interpretable factors spanning image preservation, edit quality, and instruction fidelity. Building on this formulation, we present a new human-validated benchmark that integrates human judgments, MLLM-based evaluations, model outputs, and traditional metrics across diverse image editing tasks. Through extensive human studies, we show that the proposed MLLM judges align closely with human evaluations at a fine granularity, supporting their use as reliable and scalable evaluators. We further demonstrate that traditional image editing metrics are often poor proxies for these factors, failing to distinguish over-edited or semantically imprecise outputs, whereas our judges provide more intuitive and informative assessments in both offline and online settings. Together, this work introduces a benchmark, a principled factorization, and empirical evidence positioning fine-grained MLLM judges as a practical foundation for studying, comparing, and improving image editing approaches.

</details>


### [47] [Beyond Benchmarks of IUGC: Rethinking Requirements of Deep Learning Methods for Intrapartum Ultrasound Biometry from Fetal Ultrasound Videos](https://arxiv.org/abs/2602.12922)
*Jieyun Bai,Zihao Zhou,Yitong Tang,Jie Gan,Zhuonan Liang,Jianan Fan,Lisa B. Mcguire,Jillian L. Clarke,Weidong Cai,Jacaueline Spurway,Yubo Tang,Shiye Wang,Wenda Shen,Wangwang Yu,Yihao Li,Philippe Zhang,Weili Jiang,Yongjie Li,Salem Muhsin Ali Binqahal Al Nasim,Arsen Abzhanov,Numan Saeed,Mohammad Yaqub,Zunhui Xian,Hongxing Lin,Libin Lan,Jayroop Ramesh,Valentin Bacher,Mark Eid,Hoda Kalabizadeh,Christian Rupprecht,Ana I. L. Namburete,Pak-Hei Yeung,Madeleine K. Wyburd,Nicola K. Dinsdale,Assanali Serikbey,Jiankai Li,Sung-Liang Chen,Zicheng Hu,Nana Liu,Yian Deng,Wei Hu,Cong Tan,Wenfeng Zhang,Mai Tuyet Nhi,Gregor Koehler,Rapheal Stock,Klaus Maier-Hein,Marawan Elbatel,Xiaomeng Li,Saad Slimani,Victor M. Campello,Benard Ohene-Botwe,Isaac Khobo,Yuxin Huang,Zhenyan Han,Hongying Hou,Di Qiu,Zheng Zheng,Gongning Luo,Dong Ni,Yaosheng Lu,Karim Lekadir,Shuo Li*

Main category: cs.CV

TL;DR: 本论文介绍了Intrapartum Ultrasound Grand Challenge（IUGC），该挑战致力于通过多任务自动测量框架提升分娩期超声生物测量的自动化能力，并公开了目前最大的相关多中心数据集。论文对比赛设计、参赛方法及结果进行了系统分析，总结了现有瓶颈和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 45%的孕产妇死亡、新生儿死亡及死产发生在分娩期，尤其在低中收入国家负担更重。由于 trained sonographers（训练有素的超声技师）短缺，分娩期超声难以在资源有限的地区普及。因此，亟需开发能够自动完成超声生物测量任务的AI方法，以提升分娩监测的效率和准确性。

Method: IUGC提出了一个多任务自动测量框架，融合了标准切面分类、胎儿头部-耻骨联合分割和生物测量等任务。主办方还开放了包含774段视频（共68106帧）的大规模多中心数据集，并对八支参赛队伍的预处理、数据增强、学习策略、模型架构和后处理方法进行了对比分析。

Result: 参与队伍展示了多样化的技术路线，部分方法在自动测量任务中获得了令人鼓舞的表现，但尚未达到临床大规模推广的成熟阶段。系统分析明确了当前的主要技术瓶颈，并针对存在的问题提出了未来研发方向。

Conclusion: 虽然自动分娩期超声生物测量已取得初步进展，但真正实现临床大规模应用还需进一步深入研究。赛事公开的基准方案和完整数据集可促进领域内的可复现研究与持续创新。

Abstract: A substantial proportion (45\%) of maternal deaths, neonatal deaths, and stillbirths occur during the intrapartum phase, with a particularly high burden in low- and middle-income countries. Intrapartum biometry plays a critical role in monitoring labor progression; however, the routine use of ultrasound in resource-limited settings is hindered by a shortage of trained sonographers. To address this challenge, the Intrapartum Ultrasound Grand Challenge (IUGC), co-hosted with MICCAI 2024, was launched. The IUGC introduces a clinically oriented multi-task automatic measurement framework that integrates standard plane classification, fetal head-pubic symphysis segmentation, and biometry, enabling algorithms to exploit complementary task information for more accurate estimation. Furthermore, the challenge releases the largest multi-center intrapartum ultrasound video dataset to date, comprising 774 videos (68,106 frames) collected from three hospitals, providing a robust foundation for model training and evaluation. In this study, we present a comprehensive overview of the challenge design, review the submissions from eight participating teams, and analyze their methods from five perspectives: preprocessing, data augmentation, learning strategy, model architecture, and post-processing. In addition, we perform a systematic analysis of the benchmark results to identify key bottlenecks, explore potential solutions, and highlight open challenges for future research. Although encouraging performance has been achieved, our findings indicate that the field remains at an early stage, and further in-depth investigation is required before large-scale clinical deployment. All benchmark solutions and the complete dataset have been publicly released to facilitate reproducible research and promote continued advances in automatic intrapartum ultrasound biometry.

</details>


### [48] [CoPE-VideoLM: Codec Primitives For Efficient Video Language Models](https://arxiv.org/abs/2602.13191)
*Sayan Deb Sarkar,Rémi Pautrat,Ondrej Miksik,Marc Pollefeys,Iro Armeni,Mahdi Rad,Mihai Dusmanu*

Main category: cs.CV

TL;DR: 本文提出利用视频编解码器的运动向量和残差信息，替代传统VideoLMs中的全帧采样方法，大幅减少计算资源消耗，并提升视频时序理解的表现。


<details>
  <summary>Details</summary>
Motivation: 传统VideoLMs采用关键帧采样以适应模型的最大上下文长度，但这种稀疏采样方法容易遗漏视频中的重要宏观事件与微观细节，且处理每帧的完整图像信息计算量巨大。因此，需要一种既兼顾效率又能够捕捉丰富视频信息的新方法。

Method: 作者提出利用视频编码器中固有的运动向量和残差信息，这些信息可以稀疏且高效地编码视频时序变化。配合设计轻量级的transformer编码器对这些编解码器原语进行聚合，并通过预训练策略将其表征对齐到图像编码器的embedding，加快端到端微调收敛。

Result: 所提方法在首次生成token的时间（time-to-first-token）上减少了86%，token使用量减少了93%。此外，在14个视频理解任务基准测试（包括问答、时序推理、长文本理解和空间场景理解）中，实现或超越了传统方法的表现。

Conclusion: 基于编解码器原语的信息编码可极大提高VideoLMs的视频理解效率与精度，为大规模高效视频-语言模型构建提供了新思路。

Abstract: Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\%$ and token usage by up to $93\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.

</details>


### [49] [Deep-Learning Atlas Registration for Melanoma Brain Metastases: Preserving Pathology While Enabling Cohort-Level Analyses](https://arxiv.org/abs/2602.12933)
*Nanna E. Wielenberg,Ilinca Popp,Oliver Blanck,Lucas Zander,Jan C. Peeken,Stephanie E. Combs,Anca-Ligia Grosu,Dimos Baltas,Tobias Fechter*

Main category: cs.CV

TL;DR: 本文提出了一种新型的深度学习驱动的形变配准框架，可在不需要病灶分割掩膜和预处理的情况下，将患有脑黑色素瘤转移癌(MBM)的病人脑部MRI统一配准至共同模板，并有效保留肿瘤体积，实现多中心病理脑影像的标准化对比分析。


<details>
  <summary>Details</summary>
Motivation: MBM 具有高空间异质性，不同病人的脑部解剖位置和MRI协议差异大，传统配准方法难以标准化不同病灶和样本的数据，影响大样本多中心分析的可靠性。因此亟须无需手动分割和复杂预处理且能精准配准病理脑影像的新方法。

Method: 提出端到端可微的深度学习形变配准框架，在不需要病变掩膜的前提下，结合基于距离变换的解剖标签相似度指标和体积保持正则项，自动对齐不同MBM影像。评价标准包括Dice系数、Hausdorff距离、平均对称表面距离和Jacobian指标。该模型在三中心209例MBM患者上进行测试，实现向解剖、供血、灌注三种脑图谱的映射。

Result: 该方法在不同数据集上的配准精度高（Dice 0.89-0.92，HD 6.79-7.60 mm，ASSD 0.63-0.77 mm），同时良好保留了肿瘤体积。空间分布分析发现MBM在大脑皮层和壳核显著富集，白质区域分布较少，且肿瘤多接近灰白质接壤处。各动脉分区内，校正脑容量后并未见明显MBM富集区域。

Conclusion: 该框架可实现无掩膜、高鲁棒性的病理脑MRI配准，支持多中心队列重复性分析。应用于MBM数据，证实并细化了其空间偏好规律（偏向灰白质交界和皮层）。相关实现代码已公开，有望推广至脑瘤和其它神经系统病变的配准标准化分析。

Abstract: Melanoma brain metastases (MBM) are common and spatially heterogeneous lesions, complicating cohort-level analyses due to anatomical variability and differing MRI protocols. We propose a fully differentiable, deep-learning-based deformable registration framework that aligns individual pathological brains to a common atlas while preserving metastatic tissue without requiring lesion masks or preprocessing.
  Missing anatomical correspondences caused by metastases are handled through a forward-model similarity metric based on distance-transformed anatomical labels, combined with a volume-preserving regularization term to ensure deformation plausibility. Registration performance was evaluated using Dice coefficient (DSC), Hausdorff distance (HD), average symmetric surface distance (ASSD), and Jacobian-based measures. The method was applied to 209 MBM patients from three centres, enabling standardized mapping of metastases to anatomical, arterial, and perfusion atlases.
  The framework achieved high registration accuracy across datasets (DSC 0.89-0.92, HD 6.79-7.60 mm, ASSD 0.63-0.77 mm) while preserving metastatic volumes. Spatial analysis demonstrated significant over-representation of MBM in the cerebral cortex and putamen, under-representation in white matter, and consistent localization near the gray-white matter junction. No arterial territory showed increased metastasis frequency after volume correction.
  This approach enables robust atlas registration of pathological brain MRI without lesion masks and supports reproducible multi-centre analyses. Applied to MBM, it confirms and refines known spatial predilections, particularly preferential seeding near the gray-white matter junction and cortical regions. The publicly available implementation facilitates reproducible research and extension to other brain tumours and neurological pathologies.

</details>


### [50] [Unleashing MLLMs on the Edge: A Unified Framework for Cross-Modal ReID via Adaptive SVD Distillation](https://arxiv.org/abs/2602.12936)
*Hongbo Jiang,Jie Li,Xinqi Cai,Tianyu Xie,Yunhang Shen,Pingyang Dai,Liujuan Cao*

Main category: cs.CV

TL;DR: 本论文提出了一种统一的云-边框架MLLMEmbed-ReID，有效解决了多模态跨模态重识别任务在边缘设备上的部署和知识蒸馏难题。通过利用多模态大语言模型（MLLM），结合高效的微调与创新蒸馏策略，实现了在各类数据模态（RGB、红外、素描、文本）上的统一嵌入表达，并在多个基准测试上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 实际的云-边跨模态重识别部署面临多模态模型碎片化、难以统一、以及现有模型难以在边缘设备上高效部署等问题。而多模态大模型具备统一潜力，但目前方法无法实现端到端统一骨干结构，且缺乏高效知识蒸馏以适配边缘设备。

Method: 提出MLLMEmbed-ReID框架：首先将基础多模态大语言模型适配为云端SOTA模型，利用基于指令的提示引导模型生成统一嵌入空间，融合RGB、红外、素描和文本等多模态数据。通过分层低秩适应（LoRA-SFT）高效微调，并在统一跨模态对齐目标下优化。其次，提出创新的知识蒸馏方法，利用教师模型特征的低秩性质，设计主成分映射损失和特征关系损失，将知识有效迁移至轻量化边缘端模型。

Result: 所提出的云端模型在全部跨模态重识别基准上取得领先，轻量级边缘模型在多个视觉CM-ReID基准也实现了当前最优性能。

Conclusion: MLLMEmbed-ReID为在资源受限终端部署统一多模态智能提供了完善有效的解决方案，兼顾云端与边缘性能，促进多模态重识别任务实际应用落地。代码与模型将开源。

Abstract: Practical cloud-edge deployment of Cross-Modal Re-identification (CM-ReID) faces challenges due to maintaining a fragmented ecosystem of specialized cloud models for diverse modalities. While Multi-Modal Large Language Models (MLLMs) offer strong unification potential, existing approaches fail to adapt them into a single end-to-end backbone and lack effective knowledge distillation strategies for edge deployment. To address these limitations, we propose MLLMEmbed-ReID, a unified framework based on a powerful cloud-edge architecture. First, we adapt a foundational MLLM into a state-of-the-art cloud model. We leverage instruction-based prompting to guide the MLLM in generating a unified embedding space across RGB, infrared, sketch, and text modalities. This model is then trained efficiently with a hierarchical Low-Rank Adaptation finetuning (LoRA-SFT) strategy, optimized under a holistic cross-modal alignment objective. Second, to deploy its knowledge onto an edge-native student, we introduce a novel distillation strategy motivated by the low-rank property in the teacher's feature space. To prioritize essential information, this method employs a Principal Component Mapping loss, while relational structures are preserved via a Feature Relation loss. Our lightweight edge-based model achieves state-of-the-art performance on multiple visual CM-ReID benchmarks, while its cloud-based counterpart excels across all CM-ReID benchmarks. The MLLMEmbed-ReID framework thus presents a complete and effective solution for deploying unified MLLM-level intelligence on resource-constrained devices. The code and models will be open-sourced soon.

</details>


### [51] [Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding](https://arxiv.org/abs/2602.12957)
*Wenhui Liao,Hongliang Li,Pengyu Xie,Xinyu Cai,Yufan Shen,Yi Xin,Qi Qin,Shenglong Ye,Tianbin Li,Ming Hu,Junjun He,Yihao Liu,Wenhai Wang,Min Dou,Bin Fu,Botian Shi,Yu Qiao,Lianwen Jin*

Main category: cs.CV

TL;DR: 本文提出了一种训练自由且高效的文档解析加速方法，通过“草稿-验证”模型并结合文档布局切分，实现了2.42倍及以上推理加速，且无性能损失。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型（VLM）的端到端文档解析虽然表现优异，但在处理长文档时推理速度慢，有明显延迟，亟需加速方法以适应实际应用需求。

Method: 本方法借鉴speculative decoding，利用轻量的草稿模型提前预测后续token批次，然后通过高精度VLM进行并行验证。同时，充分利用文档布局结构，将页面划分为独立区域，各区采用相同的草稿-验证策略并行解析，最后按自然阅读顺序拼接结果。

Result: 在OmniDocBench通用任务上，该方法对dots.ocr模型推理加速2.42倍，对长文档任务最高加速4.89倍，且结果无精度损失。

Conclusion: 所提方法无需额外训练，极大提升了现有文档解析系统的推理效率，并保证解析质量，对后续相关研究和实际应用具有很大价值。

Abstract: Document parsing is a fundamental task in multimodal understanding, supporting a wide range of downstream applications such as information extraction and intelligent document analysis. Benefiting from strong semantic modeling and robust generalization, VLM-based end-to-end approaches have emerged as the mainstream paradigm in recent years. However, these models often suffer from substantial inference latency, as they must auto-regressively generate long token sequences when processing long-form documents. In this work, motivated by the extremely long outputs and complex layout structures commonly found in document parsing, we propose a training-free and highly efficient acceleration method. Inspired by speculative decoding, we employ a lightweight document parsing pipeline as a draft model to predict batches of future tokens, while the more accurate VLM verifies these draft predictions in parallel. Moreover, we further exploit the layout-structured nature of documents by partitioning each page into independent regions, enabling parallel decoding of each region using the same draft-verify strategy. The final predictions are then assembled according to the natural reading order. Experimental results demonstrate the effectiveness of our approach: on the general-purpose OmniDocBench, our method provides a 2.42x lossless acceleration for the dots.ocr model, and achieves up to 4.89x acceleration on long-document parsing tasks. We will release our code to facilitate reproducibility and future research.

</details>


### [52] [Detecting Object Tracking Failure via Sequential Hypothesis Testing](https://arxiv.org/abs/2602.12983)
*Alejandro Monroy Muñoz,Rajeev Verma,Alexander Timans*

Main category: cs.CV

TL;DR: 本论文提出了一种将目标跟踪问题表述为序贯假设检验的新方法，并用以实时监测跟踪失败，同时保证误报率可控。该方法无需额外训练，适用于不同模型，实验验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有实时目标跟踪系统缺乏正式的安全保障机制，往往只能通过启发式信心指标来预警失效，缺乏理论保证，存在失效后无法及时发现或误报问题，影响实际应用的安全与可靠性。

Method: 作者将目标跟踪问题转化为一个序贯假设检验问题，通过递增式地收集跟踪表现的证据（e-process），用以判别是否发生跟踪失败，并控制误报率。方法分为有监督和无监督两种，分别利用外部真实标签数据和仅依赖跟踪内部信息。

Result: 在两个主流跟踪模型和四个视频基准数据集上，该方法无需额外训练或微调，能快速、准确地检测跟踪失败事件，并有效限制误报率，展现出优异的泛化性和实用性。

Conclusion: 论文提出的序贯检验方法为实时目标跟踪系统提供了一种具备理论保证的、轻量且高效的安全保障机制，能够显著增强各种应用场景下的跟踪系统可靠性。

Abstract: Real-time online object tracking in videos constitutes a core task in computer vision, with wide-ranging applications including video surveillance, motion capture, and robotics. Deployed tracking systems usually lack formal safety assurances to convey when tracking is reliable and when it may fail, at best relying on heuristic measures of model confidence to raise alerts. To obtain such assurances we propose interpreting object tracking as a sequential hypothesis test, wherein evidence for or against tracking failures is gradually accumulated over time. Leveraging recent advancements in the field, our sequential test (formalized as an e-process) quickly identifies when tracking failures set in whilst provably containing false alerts at a desired rate, and thus limiting potentially costly re-calibration or intervention steps. The approach is computationally light-weight, requires no extra training or fine-tuning, and is in principle model-agnostic. We propose both supervised and unsupervised variants by leveraging either ground-truth or solely internal tracking information, and demonstrate its effectiveness for two established tracking models across four video benchmarks. As such, sequential testing can offer a statistically grounded and efficient mechanism to incorporate safety assurances into real-time tracking systems.

</details>


### [53] [MASAR: Motion-Appearance Synergy Refinement for Joint Detection and Trajectory Forecasting](https://arxiv.org/abs/2602.13003)
*Mohammed Amine Bencheikh Lehocine,Julian Schmidt,Frank Moosmann,Dikshant Gupta,Fabian Flohr*

Main category: cs.CV

TL;DR: 本文提出了一种全新框架MASAR，实现了3D目标检测与轨迹预测的端到端联合，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶系统中感知与预测模块通过人工设计的边界框接口连接，信息传递受限且误差易累计。现有端到端方法未充分利用外观及运动特征间的协同，主要依赖短时视觉信息，难以提升长期预测效果。作者旨在突破这些限制。

Method: 作者提出MASAR，一种兼容任意Transformer类3D检测器的全可微分联合架构。该模型基于对象中心的时空机制，将视觉外观与运动特征联合编码，通过回顾过去轨迹并融合外观信息进行修正，从而建模长期时序依赖，提高未来轨迹预测能力。

Result: 在nuScenes数据集上的实验证明，MASAR在关键指标minADE和minFDE上均超过20%的提升，同时保持了强健的检测性能。

Conclusion: MASAR成功实现了3D检测与轨迹预测的紧密融合，通过结合外观和运动信息，显著优化了自动驾驶中的未来轨迹预测，具备实际落地及推广价值。

Abstract: Classical autonomous driving systems connect perception and prediction modules via hand-crafted bounding-box interfaces, limiting information flow and propagating errors to downstream tasks. Recent research aims to develop end-to-end models that jointly address perception and prediction; however, they often fail to fully exploit the synergy between appearance and motion cues, relying mainly on short-term visual features. We follow the idea of "looking backward to look forward", and propose MASAR, a novel fully differentiable framework for joint 3D detection and trajectory forecasting compatible with any transformer-based 3D detector. MASAR employs an object-centric spatio-temporal mechanism that jointly encodes appearance and motion features. By predicting past trajectories and refining them using guidance from appearance cues, MASAR captures long-term temporal dependencies that enhance future trajectory forecasting. Experiments conducted on the nuScenes dataset demonstrate MASAR's effectiveness, showing improvements of over 20% in minADE and minFDE while maintaining robust detection performance. Code and models are available at https://github.com/aminmed/MASAR.

</details>


### [54] [Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions](https://arxiv.org/abs/2602.13013)
*Yunheng Li,Hengrui Zhang,Meng-Hao Guo,Wenzhao Gao,Shaoyong Jia,Shaohui Jiao,Qibin Hou,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 本论文提出了ASID-1M大规模音视频指令注释数据集、ASID-Verify数据精炼流程以及ASID-Captioner视频理解模型，并在多个基准测试中取得了高水平的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型受到描述不细致、注释不可靠的数据局限，难以细粒度处理复杂的视听内容，因此需要高质量、结构化、细粒度的数据资源以及自动化的数据质量保证方法。

Method: 1）构建ASID-1M，包含一百万条结构化、带有单/多属性监督的音视频注释数据；2）开发ASID-Verify流程，可以自动校验和优化注释与视听内容在语义和时序上的一致性；3）使用上述数据，通过SFT训练ASID-Captioner视频理解模型。

Result: ASID-Captioner在7项音视频字幕生成、属性细粒度字幕、基于字幕的问答和时域定位等任务的基准测试中，展现了更高的字幕质量、更少的幻觉现象以及更好的指令遵循能力。其在开源模型中达到最优，并能与Gemini-3-Pro竞争。

Conclusion: 高质量的结构化注释和自动化的数据质量保障，有助于提升视频理解模型在复杂真实世界场景下的表现。ASID-Captioner作为新基准，推动了开源音视频理解的研究进展。

Abstract: Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning, attribute-wise captioning, caption-based QA, and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.

</details>


### [55] [Multimodal Classification via Total Correlation Maximization](https://arxiv.org/abs/2602.13015)
*Feng Yu,Xiangyu Wu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: 本论文提出了一种新的多模态学习方法（TCMax），通过最大化多模态特征与标签之间的总相关性，有效缓解了模态竞争问题，并在多个实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态联合学习容易出现某些模态过拟合而忽视其他模态，导致性能甚至比单模态学习还差。虽然有研究尝试平衡模态贡献或融合集合，但很少从信息论角度分析这一问题。

Method: 作者首先从理论上分析了模态竞争现象，并提出通过最大化多模态特征与标签间的总相关性来缓解该问题。具体方法为：基于MINE, 提出Total Correlation Neural Estimation（TCNE）来估计总相关性的下界，并设计了一个无超参数的损失函数TCMax，通过变分下界优化最大化总相关性，实现特征对齐和模态间信息融合。

Result: 实验结果表明，所提出的TCMax在多项多模态分类任务中表现优于最先进的联合和单模态学习方法。

Conclusion: 通过最大化总相关性的新训练目标，可以显著缓解多模态学习中的模态竞争问题，并提升整体性能。该方法简单有效，可作为多模态学习的新范式。

Abstract: Multimodal learning integrates data from diverse sensors to effectively harness information from different modalities. However, recent studies reveal that joint learning often overfits certain modalities while neglecting others, leading to performance inferior to that of unimodal learning. Although previous efforts have sought to balance modal contributions or combine joint and unimodal learning, thereby mitigating the degradation of weaker modalities with promising outcomes, few have examined the relationship between joint and unimodal learning from an information-theoretic perspective. In this paper, we theoretically analyze modality competition and propose a method for multimodal classification by maximizing the total correlation between multimodal features and labels. By maximizing this objective, our approach alleviates modality competition while capturing inter-modal interactions via feature alignment. Building on Mutual Information Neural Estimation (MINE), we introduce Total Correlation Neural Estimation (TCNE) to derive a lower bound for total correlation. Subsequently, we present TCMax, a hyperparameter-free loss function that maximizes total correlation through variational bound optimization. Extensive experiments demonstrate that TCMax outperforms state-of-the-art joint and unimodal learning approaches. Our code is available at https://github.com/hubaak/TCMax.

</details>


### [56] [DynaGuide: A Generalizable Dynamic Guidance Framework for Unsupervised Semantic Segmentation](https://arxiv.org/abs/2602.13020)
*Boujemaa Guermazi,Riadh Ksantini,Naimul Khan*

Main category: cs.CV

TL;DR: 本文提出了一种无需人工标注的新型无监督图像分割框架DynaGuide，能有效提升全局语义和边界精确度，并在多个主流数据集上实现了最先进的分割效果。


<details>
  <summary>Details</summary>
Motivation: 在标注数据有限的场景下，无监督图像分割具有重要意义。但现有方法难以兼顾全局语义结构与细粒度边界分割的准确性。作者希望解决无监督条件下语义和边界分割的平衡问题。

Method: DynaGuide采用双重引导策略：一方面利用DiffSeg或SegFormer等零样本模型生成的全局伪标签，另一方面用从头训练的轻量级CNN细化边界。此外，提出多分量动态损失函数，包括特征相似性、Huber平滑空间连续性（含对角关系）和与全局伪标签的语义对齐，实现动态平衡优化，且完全不依赖目标域真实标签。

Result: 在BSD500、PASCAL VOC2012和COCO三个数据集上，DynaGuide分别提升mIoU 17.5%、3.1%、11.66%，达到了领域最佳性能。

Conclusion: DynaGuide结构模块化，泛化能力强，计算开销小，能无缝集成多种引导信息，为实际无监督分割任务提供了高效可靠的解决方案。

Abstract: Unsupervised image segmentation is a critical task in computer vision. It enables dense scene understanding without human annotations, which is especially valuable in domains where labelled data is scarce. However, existing methods often struggle to reconcile global semantic structure with fine-grained boundary accuracy. This paper introduces DynaGuide, an adaptive segmentation framework that addresses these challenges through a novel dual-guidance strategy and dynamic loss optimization. Building on our previous work, DynaSeg, DynaGuide combines global pseudo-labels from zero-shot models such as DiffSeg or SegFormer with local boundary refinement using a lightweight CNN trained from scratch. This synergy allows the model to correct coarse or noisy global predictions and produce high-precision segmentations. At the heart of DynaGuide is a multi-component loss that dynamically balances feature similarity, Huber-smoothed spatial continuity, including diagonal relationships, and semantic alignment with the global pseudo-labels. Unlike prior approaches, DynaGuide trains entirely without ground-truth labels in the target domain and supports plug-and-play integration of diverse guidance sources. Extensive experiments on BSD500, PASCAL VOC2012, and COCO demonstrate that DynaGuide achieves state-of-the-art performance, improving mIoU by 17.5% on BSD500, 3.1% on PASCAL VOC2012, and 11.66% on COCO. With its modular design, strong generalization, and minimal computational footprint, DynaGuide offers a scalable and practical solution for unsupervised segmentation in real-world settings. Code available at: https://github.com/RyersonMultimediaLab/DynaGuide

</details>


### [57] [Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels](https://arxiv.org/abs/2602.13022)
*Julius Pesonen,Stefan Rua,Josef Taher,Niko Koivumäki,Xiaowei Yu,Eija Honkavaara*

Main category: cs.CV

TL;DR: 本文提出了一种利用航测激光扫描(ALS)数据生成伪标签，结合深度学习网络实现高效分割和分离个体树冠的方法，实现了无需人工标注的高精度树冠检测。


<details>
  <summary>Details</summary>
Motivation: 自动分割树冠有利于城市树木清查和森林健康监测，但航空影像中的树冠重叠和纹理变化导致自动分割困难。解决自动化、高精度树冠分割的实际需求。

Method: 基于无人工标注的ALS数据生成伪标签，采用深度学习网络对RGB和多光谱影像进行训练，并用零样本分割模型SAM 2提升伪标签质量，实现树冠的精确分割与分离。

Result: 提出的方法比现有面向通用领域的分割模型在树冠分割任务上准确率更高，且完全避免了标注成本。

Conclusion: 通过ALS伪标签和先进深度学习模型的结合，可实现高效、低成本的树冠分割，有助于环境监测领域的自动化进展。

Abstract: Mapping individual tree crowns is essential for tasks such as maintaining urban tree inventories and monitoring forest health, which help us understand and care for our environment. However, automatically separating the crowns from each other in aerial imagery is challenging due to factors such as the texture and partial tree crown overlaps. In this study, we present a method to train deep learning models that segment and separate individual trees from RGB and multispectral images, using pseudo-labels derived from aerial laser scanning (ALS) data. Our study shows that the ALS-derived pseudo-labels can be enhanced using a zero-shot instance segmentation model, Segment Anything Model 2 (SAM 2). Our method offers a way to obtain domain-specific training annotations for optical image-based models without any manual annotation cost, leading to segmentation models which outperform any available models which have been targeted for general domain deployment on the same task.

</details>


### [58] [FedHENet: A Frugal Federated Learning Framework for Heterogeneous Environments](https://arxiv.org/abs/2602.13024)
*Alejandro Dopico-Castro,Oscar Fontenla-Romero,Bertha Guijarro-Berdiñas,Amparo Alonso-Betanzos,Iván Pérez Digón*

Main category: cs.CV

TL;DR: 本文提出了FedHENet方法，在图像分类中通过同态加密实现仅需一次通信的高效安全联邦学习，无需本地微调，节能又具备竞争性精度。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法依赖深度网络的迭代优化，通信开销高，仍有梯度泄露隐私风险，且需要复杂的超参数调节。本文希望在保证隐私和低通信成本的基础上，提高联邦学习在视觉任务中的实用性与节能性。

Method: FedHENet利用预训练特征提取器，客户端仅需学习输出层参数，并用同态加密汇总客户端知识，实现一次通信全局聚合，无需本地模型微调和多次迭代。

Result: 实验显示，FedHENet在分类准确率上与主流迭代式联邦学习方法持平，同时表现出更强的稳定性，且能达到高达70%的能效提升。

Conclusion: FedHENet在隐私保护、能效和简便性方面优于传统联邦学习方法，对实际视觉应用中的联邦学习落地具有重要意义。

Abstract: Federated Learning (FL) enables collaborative training without centralizing data, essential for privacy compliance in real-world scenarios involving sensitive visual information. Most FL approaches rely on expensive, iterative deep network optimization, which still risks privacy via shared gradients. In this work, we propose FedHENet, extending the FedHEONN framework to image classification. By using a fixed, pre-trained feature extractor and learning only a single output layer, we avoid costly local fine-tuning. This layer is learned by analytically aggregating client knowledge in a single round of communication using homomorphic encryption (HE). Experiments show that FedHENet achieves competitive accuracy compared to iterative FL baselines while demonstrating superior stability performance and up to 70\% better energy efficiency. Crucially, our method is hyperparameter-free, removing the carbon footprint associated with hyperparameter tuning in standard FL. Code available in https://github.com/AlejandroDopico2/FedHENet/

</details>


### [59] [Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images](https://arxiv.org/abs/2602.13041)
*Yuhao Chen,Gautham Vinod,Siddeshwar Raghavan,Talha Ibn Mahmud,Bruce Coburn,Jinge Ma,Fengqing Zhu,Jiangpeng He*

Main category: cs.CV

TL;DR: 该论文提出了一个名为Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images的数据集，旨在推动餐饮场景下基于几何的食物分量估算技术发展。该数据集采用去除显式物理参照和标注的方式，要求通过隐式线索和先验知识推断食物尺度。实验结果显示，基于几何重建的方法在准确率和鲁棒性方面均优于传统视觉-语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前饮食评估主要依赖单张图片或基于视觉-语言模型的推断，但这些方法缺乏显式的几何推理，并且对尺度不明确问题较为敏感。为反映真实用餐环境，论文希望推动无需显式物理参考物、能够利用隐式环境线索进行尺度推断的数据集和技术的发展。

Method: 提出了一个新数据集，将食物分量估算任务重构为在单目观察下的隐式尺度三维重建问题。该数据集去除了明确的物理参照物，保留盘子、餐具等上下文目标，着重多食物种类、对象遮挡和复杂空间排列。并在相关比赛中吸引了多组重建方法参与。

Result: 实验发现，虽然强大的视觉-语言模型也有不错表现，但几何重建方法在分量估计的准确率（0.21 MAPE）和三维几何精度（5.7 L1 Chamfer Distance）上更高，表现更鲁棒。

Conclusion: 通过该数据集和基准，几何推理手段在食物分量估算中的重要性得到验证。隐式尺度推断和多物体三维重建是推动现实健康饮食评估更准确发展的关键。

Abstract: We present Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images, a benchmark dataset designed to advance geometry-based food portion estimation in realistic dining scenarios. Existing dietary assessment methods largely rely on single-image analysis or appearance-based inference, including recent vision-language models, which lack explicit geometric reasoning and are sensitive to scale ambiguity. This benchmark reframes food portion estimation as an implicit-scale 3D reconstruction problem under monocular observations. To reflect real-world conditions, explicit physical references and metric annotations are removed; instead, contextual objects such as plates and utensils are provided, requiring algorithms to infer scale from implicit cues and prior knowledge. The dataset emphasizes multi-food scenes with diverse object geometries, frequent occlusions, and complex spatial arrangements. The benchmark was adopted as a challenge at the MetaFood 2025 Workshop, where multiple teams proposed reconstruction-based solutions. Experimental results show that while strong vision--language baselines achieve competitive performance, geometry-based reconstruction methods provide both improved accuracy and greater robustness, with the top-performing approach achieving 0.21 MAPE in volume estimation and 5.7 L1 Chamfer Distance in geometric accuracy.

</details>


### [60] [Curriculum-DPO++: Direct Preference Optimization via Data and Model Curricula for Text-to-Image Generation](https://arxiv.org/abs/2602.13055)
*Florinel-Alin Croitoru,Vlad Hondru,Radu Tudor Ionescu,Nicu Sebe,Mubarak Shah*

Main category: cs.CV

TL;DR: 本文提出了Curriculum-DPO++，通过在原有数据难度分级训练的基础上，引入模型容量动态增长机制，有效提升模型在文本到图像生成中偏好学习的能力，并超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的偏好优化方法如RLHF和DPO没有考虑偏好学习难度的不同，导致优化过程次优。为弥补这一不足，并进一步提升文本到图像生成中的偏好学习效率和效果，作者继Curriculum-DPO方法基础上提出增强版本Curriculum-DPO++。

Method: Curriculum-DPO++结合了两种课程学习机制：一是数据层级的课程（按难度组织图片对）；二是模型层级的课程，即随着训练推进，逐步增加网络的学习容量。具体包括两点：1）只初始化部分可训练层，随着训练逐步解冻，最终恢复完整架构；2）采用LoRA微调，低秩矩阵初始设为较小维度，训练中逐步提升其rank。此外，提出了新的排序策略优化课程顺序。

Result: Curriculum-DPO++在九个基准测试上的文本一致性、美学和人工偏好方面，均超越了Curriculum-DPO和其它主流偏好优化方法。

Conclusion: 通过结合数据和模型的课程学习机制，Curriculum-DPO++在偏好学习任务中取得最佳效果，并为后续偏好优化研究以及实际应用提供新思路。

Abstract: Direct Preference Optimization (DPO) has been proposed as an effective and efficient alternative to reinforcement learning from human feedback (RLHF). However, neither RLHF nor DPO take into account the fact that learning certain preferences is more difficult than learning other preferences, rendering the optimization process suboptimal. To address this gap in text-to-image generation, we recently proposed Curriculum-DPO, a method that organizes image pairs by difficulty. In this paper, we introduce Curriculum-DPO++, an enhanced method that combines the original data-level curriculum with a novel model-level curriculum. More precisely, we propose to dynamically increase the learning capacity of the denoising network as training advances. We implement this capacity increase via two mechanisms. First, we initialize the model with only a subset of the trainable layers used in the original Curriculum-DPO. As training progresses, we sequentially unfreeze layers until the configuration matches the full baseline architecture. Second, as the fine-tuning is based on Low-Rank Adaptation (LoRA), we implement a progressive schedule for the dimension of the low-rank matrices. Instead of maintaining a fixed capacity, we initialize the low-rank matrices with a dimension significantly smaller than that of the baseline. As training proceeds, we incrementally increase their rank, allowing the capacity to grow until it converges to the same rank value as in Curriculum-DPO. Furthermore, we propose an alternative ranking strategy to the one employed by Curriculum-DPO. Finally, we compare Curriculum-DPO++ against Curriculum-DPO and other state-of-the-art preference optimization approaches on nine benchmarks, outperforming the competing methods in terms of text alignment, aesthetics and human preference. Our code is available at https://github.com/CroitoruAlin/Curriculum-DPO.

</details>


### [61] [A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models](https://arxiv.org/abs/2602.13066)
*Yash Deo,Yan Jia,Toni Lassila,Victoria J Hodge,Alejandro F Frang,Chenghao Qian,Siyuan Kang,Ibrahim Habli*

Main category: cs.CV

TL;DR: 论文提出了一种用于检测训练数据被生成模型记忆与复制的新指标，可以高效识别医疗影像生成中的重复样本，并在多个MRI数据集上取得了接近完美的检测效果。


<details>
  <summary>Details</summary>
Motivation: 生成模型容易记忆并复制训练集中出现过的图像，尤其在医疗影像领域，这一问题带来隐私风险。因此亟需一种精确检测训练数据重复或记忆的工具。

Method: 作者提出了一种逐样本的校准指标，基于MRI基础模型提取的特征，通过多层白化的最近邻相似度汇总，映射到有界的Overfit/Novelty Index（ONI）和Memorization Index（MI）分数，用以衡量样本是否为训练集重复。

Result: 在三个MRI数据集上进行对比实验，覆盖不同的重复比例和常见图像增强方式，实验表明该指标能够稳健地检测出样本复制，并在不同数据集间保持一致性；在单样本检测重复方面，指标表现近乎完美。

Conclusion: 新指标能有效检测生成模型对训练样本的记忆与复制，在医疗影像生成的隐私保护和模型评估中有实际应用价值。

Abstract: Image generative models are known to duplicate images from the training data as part of their outputs, which can lead to privacy concerns when used for medical image generation. We propose a calibrated per-sample metric for detecting memorization and duplication of training data. Our metric uses image features extracted using an MRI foundation model, aggregates multi-layer whitened nearest-neighbor similarities, and maps them to a bounded \emph{Overfit/Novelty Index} (ONI) and \emph{Memorization Index} (MI) scores. Across three MRI datasets with controlled duplication percentages and typical image augmentations, our metric robustly detects duplication and provides more consistent metric values across datasets. At the sample level, our metric achieves near-perfect detection of duplicates.

</details>


### [62] [SIEFormer: Spectral-Interpretable and -Enhanced Transformer for Generalized Category Discovery](https://arxiv.org/abs/2602.13067)
*Chunming Li,Shidong Wang,Tong Xin,Haofeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新型视觉Transformer模型SIEFormer，以光谱分析对ViT注意力机制进行重新解释并提升特征适应性，在通用类别发现（GCD）任务上性能突出。


<details>
  <summary>Details</summary>
Motivation: 现有的Vision Transformer虽然在各种视觉任务中表现优异，但其注意力机制缺乏对局部结构与全局频谱信号的深层理解，尤其在GCD等复杂任务中，模型适应性和泛化能力不足。因此，作者希望通过引入光谱分析手段，增强ViT的特征处理能力和可解释性。

Method: SIEFormer包括两个主要分支：1）隐式光谱分支：利用不同类型的图Laplacian对token的局部结构相关性建模，并引入带自适应滤波器（BaF），实现灵活的带通和带阻滤波功能；2）显式光谱分支：通过对输入特征进行傅里叶变换，利用可学习的参数在频域调制信号，并逆变换以获取增强后的特征（MFL层）。

Result: 在多项图像识别数据集上进行的实验表明，SIEFormer取得了领先的性能，并通过消融实验和可视化进一步证实了方法的有效性。

Conclusion: SIEFormer通过显式和隐式光谱机制增强ViT模型对复杂结构与全局相关性的理解和适应能力，显著提升了GCD等任务下的表现，展示了光谱分析与Transformer结合的巨大潜力。

Abstract: This paper presents a novel approach, Spectral-Interpretable and -Enhanced Transformer (SIEFormer), which leverages spectral analysis to reinterpret the attention mechanism within Vision Transformer (ViT) and enhance feature adaptability, with particular emphasis on challenging Generalized Category Discovery (GCD) tasks. The proposed SIEFormer is composed of two main branches, each corresponding to an implicit and explicit spectral perspective of the ViT, enabling joint optimization. The implicit branch realizes the use of different types of graph Laplacians to model the local structure correlations of tokens, along with a novel Band-adaptive Filter (BaF) layer that can flexibly perform both band-pass and band-reject filtering. The explicit branch, on the other hand, introduces a Maneuverable Filtering Layer (MFL) that learns global dependencies among tokens by applying the Fourier transform to the input ``value" features, modulating the transformed signal with a set of learnable parameters in the frequency domain, and then performing an inverse Fourier transform to obtain the enhanced features. Extensive experiments reveal state-of-the-art performance on multiple image recognition datasets, reaffirming the superiority of our approach through ablation studies and visualizations.

</details>


### [63] [Universal Transformation of One-Class Classifiers for Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.13091)
*Declan McIntosh,Alexandra Branzan Albu*

Main category: cs.CV

TL;DR: 本文提出了一种将基于单类分类器的异常检测器转化为全无监督方法的数据集折叠技术，无需修改已有检测器即可过滤训练集中的异常，大幅提升了无监督异常检测能力，并在多个公开数据集上达成SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测通常仅依赖正常（nominal）数据进行训练，因此对训练集中不明确的异常或标签噪声非常敏感。缺乏异常样本和标签错误会影响现有方法性能，迫切需要无监督、高鲁棒性的新方法。

Method: 提出数据集折叠（dataset folding）方法：基于异常样本在训练集中罕见且分布异质的假设，将训练集划分为多个子集，对每个子集独立训练单类分类器。通过多分类器筛选，自动识别并过滤异常，无需依赖任何标注异常样本，对检测器本身无需结构改动。该方法通用适配多种单类分类器及图像、视频类型数据。

Result: 该方法可普适性地将现有单类分类异常检测方法转化为无监督方法，并在MVTec AD、ViSA、MVTec Loco AD等主流无监督异常检测数据集上实现了新的最优结果（SOTA）。

Conclusion: 数据集折叠技术显著扩展了一类分类器的适用性，首次实现逻辑意义上的全无监督异常检测，并保证一类分类技术的进步可直接迁移至无监督领域，推动了异常检测方法的发展。

Abstract: Detecting anomalies in images and video is an essential task for multiple real-world problems, including industrial inspection, computer-assisted diagnosis, and environmental monitoring. Anomaly detection is typically formulated as a one-class classification problem, where the training data consists solely of nominal values, leaving methods built on this assumption susceptible to training label noise. We present a dataset folding method that transforms an arbitrary one-class classifier-based anomaly detector into a fully unsupervised method. This is achieved by making a set of key weak assumptions: that anomalies are uncommon in the training dataset and generally heterogeneous. These assumptions enable us to utilize multiple independently trained instances of a one-class classifier to filter the training dataset for anomalies. This transformation requires no modifications to the underlying anomaly detector; the only changes are algorithmically selected data subsets used for training. We demonstrate that our method can transform a wide variety of one-class classifier anomaly detectors for both images and videos into unsupervised ones. Our method creates the first unsupervised logical anomaly detectors by transforming existing methods. We also demonstrate that our method achieves state-of-the-art performance for unsupervised anomaly detection on the MVTec AD, ViSA, and MVTec Loco AD datasets. As improvements to one-class classifiers are made, our method directly transfers those improvements to the unsupervised domain, linking the domains.

</details>


### [64] [Realistic Face Reconstruction from Facial Embeddings via Diffusion Models](https://arxiv.org/abs/2602.13168)
*Dong Han,Yong Li,Joachim Denzler*

Main category: cs.CV

TL;DR: 本论文提出了一种基于Kolmogorov-Arnold Network（KAN）的Face Embedding Mapping（FEM）框架，能够从人脸特征向量重建高分辨率人脸图像，从而揭示当前主流人脸识别及隐私保护系统存在的隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 虽然隐私保护人脸识别系统（PPFR）越来越受重视，但当前对于攻击这些系统并从其嵌入向量重建出原始面部图像的隐私风险验证相关研究较少。为评估并揭示这些系统的安全隐患，有必要发展更强的攻击和评估工具。

Method: 作者提出了Face Embedding Mapping（FEM）框架，结合Kolmogorov-Arnold Network（KAN）和预训练的Identity-Preserving扩散模型，对当前最先进的人脸识别和隐私保护人脸识别系统进行embedding-to-face（嵌入向量到人脸）攻击。方法通过广泛实验，重建出高分辨率且具备可识别性的面部图像。

Result: 实验结果表明，该方法能从部分或已保护的人脸嵌入有效重建出原始人脸图像，并用这些重建图像成功访问其他现实世界的人脸识别系统，证明了现有FR和PPFR系统存在隐私风险。

Conclusion: FEM不仅能作为工具揭示并验证FR及PPFR系统中潜在的隐私泄露问题，还为后续人脸识别系统的安全性评估提供了新方案，提醒业界关注并进一步加强相关技术的隐私保护能力。

Abstract: With the advancement of face recognition (FR) systems, privacy-preserving face recognition (PPFR) systems have gained popularity for their accurate recognition, enhanced facial privacy protection, and robustness to various attacks. However, there are limited studies to further verify privacy risks by reconstructing realistic high-resolution face images from embeddings of these systems, especially for PPFR. In this work, we propose the face embedding mapping (FEM), a general framework that explores Kolmogorov-Arnold Network (KAN) for conducting the embedding-to-face attack by leveraging pre-trained Identity-Preserving diffusion model against state-of-the-art (SOTA) FR and PPFR systems. Based on extensive experiments, we verify that reconstructed faces can be used for accessing other real-word FR systems. Besides, the proposed method shows the robustness in reconstructing faces from the partial and protected face embeddings. Moreover, FEM can be utilized as a tool for evaluating safety of FR and PPFR systems in terms of privacy leakage. All images used in this work are from public datasets.

</details>


### [65] [LongStream: Long-Sequence Streaming Autoregressive Visual Geometry](https://arxiv.org/abs/2602.13172)
*Chong Cheng,Xianda Chen,Tao Xie,Wei Yin,Weiqiang Ren,Qian Zhang,Xiaoyuang Guo,Hao Wang*

Main category: cs.CV

TL;DR: 本文提出了LongStream，一种能够高精度处理上千帧长序列流式3D重建的新模型，通过关键帧相对姿态预测、正交尺度学习、以及缓存一致性训练策略，显著提升了大规模场景的3D重建稳定性与精度，达到业界领先水平。


<details>
  <summary>Details</summary>
Motivation: 当前主流自回归3D重建模型在处理长序列数据方面存在瓶颈，通常依赖首帧作为锚点，导致随着序列增长产生注意力衰减、尺度漂移等问题，难以用于大范围场景的高效重建。

Method: 1) 摒弃首帧锚点，转为关键帧相对姿态预测，将跨时长外推问题转化为难度恒定的局部任务；2) 引入正交尺度学习，将几何与尺度解耦，显著抑制重建过程中的尺度漂移；3) 针对Transformer长期缓存退化问题，提出缓存一致性训练与定期刷新cache方法，提升长序列处理的稳定性与推理一致性。

Result: LongStream在处理超长序列和公里级场景时，能以18帧每秒的速度输出稳定、具有公制尺度的3D重建结果，在各项实验中均优于当前主流方法。

Conclusion: LongStream有效解决了流式3D重建中的多项核心难题，能够突破现有方法在尺度与长序列处理上的瓶颈，适用于大规模、长期场景的高效、高精度3D建模任务。

Abstract: Long-sequence streaming 3D reconstruction remains a significant open challenge. Existing autoregressive models often fail when processing long sequences. They typically anchor poses to the first frame, which leads to attention decay, scale drift, and extrapolation errors. We introduce LongStream, a novel gauge-decoupled streaming visual geometry model for metric-scale scene reconstruction across thousands of frames. Our approach is threefold. First, we discard the first-frame anchor and predict keyframe-relative poses. This reformulates long-range extrapolation into a constant-difficulty local task. Second, we introduce orthogonal scale learning. This method fully disentangles geometry from scale estimation to suppress drift. Finally, we solve Transformer cache issues such as attention-sink reliance and long-term KV-cache contamination. We propose cache-consistent training combined with periodic cache refresh. This approach suppresses attention degradation over ultra-long sequences and reduces the gap between training and inference. Experiments show LongStream achieves state-of-the-art performance. It delivers stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS. Project Page: https://3dagentworld.github.io/longstream/

</details>


### [66] [Monocular Markerless Motion Capture Enables Quantitative Assessment of Upper Extremity Reachable Workspace](https://arxiv.org/abs/2602.13176)
*Seth Donahue,J. D. Peiffer,R. Tyler Richardson,Yishan Zhong,Shaun Q. Y. Tan,Benoit Marteau,Stephanie R. Russo,May D. Wang,R. James Cotton,Ross Chafetz*

Main category: cs.CV

TL;DR: 本文提出并验证了利用单目相机和AI驱动无标记动作捕捉（MMC）系统来评估上肢活动空间（UERW）的方法，结果显示正面单摄像头配置可以可靠替代传统有标记动作捕捉，简化了临床操作。


<details>
  <summary>Details</summary>
Motivation: 传统的上肢活动空间评估常需繁琐且昂贵的有标记动作捕捉系统，限制了其在临床的广泛应用，因此需要一种更易获取、操作简便的方法。

Method: 招募9名健康成人利用VR显示器按照标准任务完成上肢移动，通过8台FLIR摄像头与传统有标记动作捕捉同步录制，再选取正面和偏置两种单摄像头视频，分别用AI驱动的无标记动作捕捉技术进行计算，比较其与有标记系统结果的一致性。

Result: 正面单目摄像头配置下，无标记系统与金标准高度一致（平均偏差仅为0.61%±0.12%），而偏置摄像头低估约5.66%±0.45%；尤其在评估前侧空间时一致性最好。

Conclusion: 正面单目摄像头结合AI驱动捕捉系统对UERW任务评估是可行且实用的，显著简化了临床流程，有助于上肢运动功能的大规模定量评估，相关方法首次获得了验证。

Abstract: To validate a clinically accessible approach for quantifying the Upper Extremity Reachable Workspace (UERW) using a single (monocular) camera and Artificial Intelligence (AI)-driven Markerless Motion Capture (MMC) for biomechanical analysis. Objective assessment and validation of these techniques for specific clinically oriented tasks are crucial for their adoption in clinical motion analysis. AI-driven monocular MMC reduces the barriers to adoption in the clinic and has the potential to reduce the overhead for analysis of this common clinical assessment. Nine adult participants with no impairments performed the standardized UERW task, which entails reaching targets distributed across a virtual sphere centered on the torso, with targets displayed in a VR headset. Movements were simultaneously captured using a marker-based motion capture system and a set of eight FLIR cameras. We performed monocular video analysis on two of these video camera views to compare a frontal and offset camera configurations. The frontal camera orientation demonstrated strong agreement with the marker-based reference, exhibiting a minimal mean bias of $0.61 \pm 0.12$ \% reachspace reached per octanct (mean $\pm$ standard deviation). In contrast, the offset camera view underestimated the percent workspace reached ($-5.66 \pm 0.45$ \% reachspace reached). Conclusion: The findings support the feasibility of a frontal monocular camera configuration for UERW assessment, particularly for anterior workspace evaluation where agreement with marker-based motion capture was highest. The overall performance demonstrates clinical potential for practical, single-camera assessments. This study provides the first validation of monocular MMC system for the assessment of the UERW task. By reducing technical complexity, this approach enables broader implementation of quantitative upper extremity mobility assessment.

</details>


### [67] [FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control](https://arxiv.org/abs/2602.13185)
*Mingzhi Sheng,Zekai Gu,Peng Li,Cheng Lin,Hao-Xiang Guo,Ying-Cong Chen,Yuan Liu*

Main category: cs.CV

TL;DR: 本论文提出FlexAM，一种通过3D点云控制信号实现外观与运动解耦的视频生成统一框架，在多个视频编辑与控制任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法在有效性和泛化性上面临挑战，尤其在控制信号上依赖不明确或任务特定的方式。作者认为，根本的“外观”与“运动”解耦是提升视频生成控制能力和扩展性的关键。

Method: 论文提出了FlexAM框架，通过一种新颖的3D点云控制信号表述视频动态。引入了三种关键增强：多频率位置编码实现细微运动区分；深度感知位置编码增强空间理解；灵活控制信号，使精度与生成质量达到平衡。这一方法能够有效分离视频中的外观和运动因素。

Result: 在I2V/V2V编辑、摄像机控制、空间对象编辑等多项任务上的实验证明，FlexAM在所有评测任务中均获得了优异表现，优于现有方法。

Conclusion: FlexAM通过创新3D点云控制信号与多重增强策略，成功提升了视频生成的精确性、灵活性和泛化能力，为复杂视频编辑与控制任务提供了更有力的技术支持。

Abstract: Effective and generalizable control in video generation remains a significant challenge. While many methods rely on ambiguous or task-specific signals, we argue that a fundamental disentanglement of "appearance" and "motion" provides a more robust and scalable pathway. We propose FlexAM, a unified framework built upon a novel 3D control signal. This signal represents video dynamics as a point cloud, introducing three key enhancements: multi-frequency positional encoding to distinguish fine-grained motion, depth-aware positional encoding, and a flexible control signal for balancing precision and generative quality. This representation allows FlexAM to effectively disentangle appearance and motion, enabling a wide range of tasks including I2V/V2V editing, camera control, and spatial object editing. Extensive experiments demonstrate that FlexAM achieves superior performance across all evaluated tasks.

</details>


### [68] [Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision](https://arxiv.org/abs/2602.13195)
*Aadarsh Sahoo,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 本文提出了一种新型的对话式图像分割任务，并构建了相关数据集和模型，能够理解更复杂的意图和物理推理，实现更精确的像素级分割。


<details>
  <summary>Details</summary>
Motivation: 现有的指代图像分割主要关注类别和空间关系，忽视了功能性和物理推理，如安全性、功能或意图等更加复杂的场景。本文意在弥补这一研究空白。

Method: 提出了对话式图像分割（CIS）任务和ConverSeg数据集，涵盖多种实体、空间关系、意图、可供性、功能和物理推理。开发了ConverSeg-Net模型，将强大的分割先验与语言理解能力融合。设计了AI驱动的数据生成引擎，可自动生成对话提示与掩码对，无需人工标注。

Result: 实验证明，现有的基于语言的分割模型难以胜任对话式分割任务，而ConverSeg-Net在新任务中大幅超越现有方法，同时也在传统数据集上取得优异表现。

Conclusion: 本文的任务与模型推动了语言引导图像分割领域的发展，实验证明方法在理解复杂意图和推理方面更强，为AI视觉理解提供了新方向。

Abstract: Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., "left-most apple") and overlooks functional and physical reasoning (e.g., "where can I safely store the knife?"). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [69] [A Lightweight LLM Framework for Disaster Humanitarian Information Classification](https://arxiv.org/abs/2602.12284)
*Han Jinzhen,Kim Jisung,Yang Jong Soo,Yun Hong Sik*

Main category: cs.CL

TL;DR: 本研究提出了一种高效、轻量的灾害推文分类方法，针对资源有限的应急场景，通过参数高效微调提升大语言模型实用性，并建立了统一评测基准。


<details>
  <summary>Details</summary>
Motivation: 灾害发生时，能够及时从社交媒体中分类和提取人道主义信息对救灾响应至关重要，但现有大语言模型资源消耗高，难以直接用于应急环境。为此，研究团队希望开发一种既经济又高效的自动化灾害信息分类框架。

Method: 作者将HumAID数据集整合、标准化，建立带有二重任务（人道主义信息类别与事件类型识别）的统一语料库。并在Llama 3.1 8B上系统评估了以下方法：提示工程（prompting）、LoRA微调、QLoRA量化微调，以及检索增强生成（RAG）。

Result: （1）LoRA微调仅需训练2%的参数，类别准确率提升至79.62%，比零样本提升37.79%；（2）QLoRA能以50%显存代价，实现LoRA 99.4%的性能；（3）RAG反而会因检索例子带来标签噪声而损害微调模型的性能。

Conclusion: 研究展示了在受限计算资源下，如何通过参数高效微调和量化技术，实现实用、可复现的灾害信息智能分类流程，为危机情报系统快速部署提供了方法依据。

Abstract: Timely classification of humanitarian information from social media is critical for effective disaster response. However, deploying large language models (LLMs) for this task faces challenges in resource-constrained emergency settings. This paper develops a lightweight, cost-effective framework for disaster tweet classification using parameter-efficient fine-tuning. We construct a unified experimental corpus by integrating and normalizing the HumAID dataset (76,484 tweets across 19 disaster events) into a dual-task benchmark: humanitarian information categorization and event type identification. Through systematic evaluation of prompting strategies, LoRA fine-tuning, and retrieval-augmented generation (RAG) on Llama 3.1 8B, we demonstrate that: (1) LoRA achieves 79.62% humanitarian classification accuracy (+37.79% over zero-shot) while training only ~2% of parameters; (2) QLoRA enables efficient deployment with 99.4% of LoRA performance at 50% memory cost; (3) contrary to common assumptions, RAG strategies degrade fine-tuned model performance due to label noise from retrieved examples. These findings establish a practical, reproducible pipeline for building reliable crisis intelligence systems with limited computational resources.

</details>


### [70] [From Biased Chatbots to Biased Agents: Examining Role Assignment Effects on LLM Agent Robustness](https://arxiv.org/abs/2602.12285)
*Linbo Cao,Lihao Sun,Yang Yue*

Main category: cs.CL

TL;DR: 本文系统性研究了将角色（persona）分配给大语言模型（LLM）作为自主代理时，角色诱导的偏见如何影响其完成实际任务的表现，发现不同角色分配会导致模型行为和任务表现的大幅波动，暴露了当前LLM代理系统的安全与可靠性隐患。


<details>
  <summary>Details</summary>
Motivation: 尽管在文本生成过程中由角色设定带来的偏见已被广泛关注，但这些角色偏见对LLM实际执行代理性任务的影响尚未被充分研究，而这类影响直接关系到模型部署时的操作风险。

Method: 作者在多个具代表性的任务领域（包括战略推理、规划、技术操作等）中，采用已大规模部署的LLM，对不同基于人口统计特征的角色设定下的模型表现进行系统性测试，通过对比分析角色诱导下的任务绩效变化。

Result: 实验显示，在不同领域任务、不同模型架构下，由与任务无关的角色设定会导致LLM代理高达26.2%的表现下降，并且会造成决策可靠性的波动和行为的不确定性。

Conclusion: 角色分配为当前LLM代理系统引入了未被注意的隐性偏见和行为不稳定性，提升了模型部署时的安全风险，提示未来应关注并解决这一问题以保证LLM代理的安全与稳定应用。

Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents capable of actions with real-world impacts beyond text generation. While persona-induced biases in text generation are well documented, their effects on agent task performance remain largely unexplored, even though such effects pose more direct operational risks. In this work, we present the first systematic case study showing that demographic-based persona assignments can alter LLM agents' behavior and degrade performance across diverse domains. Evaluating widely deployed models on agentic benchmarks spanning strategic reasoning, planning, and technical operations, we uncover substantial performance variations - up to 26.2% degradation, driven by task-irrelevant persona cues. These shifts appear across task types and model architectures, indicating that persona conditioning and simple prompt injections can distort an agent's decision-making reliability. Our findings reveal an overlooked vulnerability in current LLM agentic systems: persona assignments can introduce implicit biases and increase behavioral volatility, raising concerns for the safe and robust deployment of LLM agents.

</details>


### [71] [Retrieval-Augmented Self-Taught Reasoning Model with Adaptive Chain-of-Thought for ASR Named Entity Correction](https://arxiv.org/abs/2602.12287)
*Junjie An,Jingguang Tian,Tianyi Wang,Yu Gao,Xiaofeng Mou,Yi Xu*

Main category: cs.CL

TL;DR: 本文提出利用大型语言模型(LLM)和检索-增强生成方法，提高ASR中特定领域短语（如专有名词）的识别准确性，并显著降低命名实体识别的错误率。


<details>
  <summary>Details</summary>
Motivation: 端到端自动语音识别(ASR)系统在识别特定领域短语（如专有名词）时常出现错误，影响下游任务效果。现有基于LLM的纠错方法尚未充分发挥其推理能力，因此需要新的方法提升识别准确率。

Method: 提出了一种基于检索增强生成（Retrieval-Augmented Generation）的命名实体纠错框架。包括：(1) 利用重述语言模型(RLM)识别命名实体并通过音素级编辑距离检索候选项；(2) 设计了自适应链式推理(A-STAR)策略，可根据任务难度动态调整推理深度，实现更智能的纠错。

Result: 在AISHELL-1和Homophone数据集上验证了该方法的有效性。相比强基线方法，该方法实现了命名实体字符错误率分别降低17.96%和34.42%的显著提升。

Conclusion: 采用检索增强及自适应推理策略的命名实体纠错方法，能有效提升ASR系统对专有名词等重要短语的识别准确率，对实际语音识别等下游任务具有重要意义。

Abstract: End-to-end automatic speech recognition (ASR) systems frequently misrecognize domain-specific phrases like named entities, which can cause catastrophic failures in downstream tasks. A new family of named entity correction methods based on large language models (LLMs) has recently emerged. However, these approaches have yet to fully exploit the sophisticated reasoning capabilities inherent to LLMs. To bridge this gap, we propose a novel retrieval-augmented generation framework for correcting named entity errors in ASR. Our approach consists of two key components: (1) a rephrasing language model (RLM) for named entity recognition, followed by candidate retrieval using a phonetic-level edit distance; and (2) a novel self-taught reasoning model with adaptive chain-of-thought (A-STAR) that dynamically adjusts the depth of its reasoning based on task difficulty. Experiments on the AISHELL-1 and Homophone datasets demonstrate the effectiveness of our method, which achieves relative reductions in the named entity character error rate of 17.96\% and 34.42\%, respectively, compared to a strong baseline.

</details>


### [72] [Grandes Modelos de Linguagem Multimodais (MLLMs): Da Teoria à Prática](https://arxiv.org/abs/2602.12302)
*Neemias da Silva,Júlio C. W. Scholz,John Harrison,Marina Borges,Paulo Ávila,Frances A Santos,Myriam Delgado,Rodrigo Minetto,Thiago H Silva*

Main category: cs.CL

TL;DR: 本章系统介绍了多模态大语言模型（MLLMs）的基本原理、典型模型以及实际应用技巧，并展望了未来发展趋势。


<details>
  <summary>Details</summary>
Motivation: MLLMs结合了LLMs的语言理解与生成能力及跨模态感知能力，已成为人工智能领域的重要进展，因此需要对其理论基础和实践方法进行梳理总结。

Method: 本章详细介绍MLLMs的基本原理与代表性模型，说明了数据预处理、提示工程（prompt engineering）和如何通过LangChain与LangGraph构建多模态管道的实际技巧，并提供了开源学习资料。

Result: 读者可系统了解MLLMs的核心理论、典型代表及实际开发流程，获得相关工具的应用方法和进一步深入学习的网络资源。

Conclusion: MLLMs推动了AI向更灵活智能的方向发展，尽管面临一些挑战，但该领域前景广阔、趋势明朗。

Abstract: Multimodal Large Language Models (MLLMs) combine the natural language understanding and generation capabilities of LLMs with perception skills in modalities such as image and audio, representing a key advancement in contemporary AI. This chapter presents the main fundamentals of MLLMs and emblematic models. Practical techniques for preprocessing, prompt engineering, and building multimodal pipelines with LangChain and LangGraph are also explored. For further practical study, supplementary material is publicly available online: https://github.com/neemiasbsilva/MLLMs-Teoria-e-Pratica. Finally, the chapter discusses the challenges and highlights promising trends.

</details>


### [73] [propella-1: Multi-Property Document Annotation for LLM Data Curation at Scale](https://arxiv.org/abs/2602.12414)
*Maximilian Idahl,Benedikt Droste,Björn Plüster,Jan Philipp Harries*

Main category: cs.CL

TL;DR: 本文提出了propella-1，多语言小规模LLM家族，可对文本进行18项属性注释，提升预训练数据多维评估的灵活性和可解释性，并发布了大规模标注数据集与模型权重。


<details>
  <summary>Details</summary>
Motivation: 现有LLM预训练数据通常仅用单一分数评估质量，无法拆分多维度信息，导致筛选灵活性和解释性不足。

Method: 开发了propella-1小型多语言LLM模型家族（包括0.6B、1.7B、4B参数），可对文本在6大类共18个属性上自动生成结构化JSON注释，支持57种语言，并与前沿商业LLM进行一致性评测。

Result: propella-1的4B模型在与商业LLM的注释一致性上胜过体量更大的通用模型，并发布了包含FineWeb-2、FinePDFs等主流数据集的三十亿文档注释数据集。

Conclusion: 多维属性注释显著提升预训练数据的分析和筛选能力，单一分数的方法无法替代，所有模型与数据可开放商用。

Abstract: Since FineWeb-Edu, data curation for LLM pretraining has predominantly relied on single scalar quality scores produced by small classifiers. A single score conflates multiple quality dimensions, prevents flexible filtering, and offers no interpretability. We introduce propella-1, a family of small multilingual LLMs (0.6B, 1.7B, 4B parameters) that annotate text documents across 18 properties organized into six categories: core content, classification, quality and value, audience and purpose, safety and compliance, and geographic relevance. The models support 57 languages and produce structured JSON annotations conforming to a predefined schema. Evaluated against a frontier commercial LLM as a reference annotator, the 4B model achieves higher agreement than much larger general-purpose models. We release propella-annotations, a dataset of over three billion document annotations covering major pretraining corpora including data from FineWeb-2, FinePDFs, HPLT 3.0, and Nemotron-CC. Using these annotations, we present a multi-dimensional compositional analysis of widely used pretraining datasets, revealing substantial differences in quality, reasoning depth, and content composition that single-score approaches cannot capture. All model weights and annotations are released under permissive, commercial-use licenses.

</details>


### [74] [RankLLM: Weighted Ranking of LLMs by Quantifying Question Difficulty](https://arxiv.org/abs/2602.12424)
*Ziqian Zhang,Xingjian Hu,Yue Huang,Kai Zhang,Ruoxi Chen,Yixin Liu,Qingsong Wen,Kaidi Xu,Xiangliang Zhang,Neil Zhenqiang Gong,Lichao Sun*

Main category: cs.CL

TL;DR: RankLLM是一个用于评估大语言模型（LLM）能力的新基准框架，引入了“题目难度”和“模型能力”双重打分系统，实现了更细粒度且与人工判断高度一致的模型评估。


<details>
  <summary>Details</summary>
Motivation: 目前大多数评测基准未区分不同题目的难度，难以全面区分和评价不同大语言模型的真实能力水平。

Method: 提出RankLLM框架，通过在模型和题目之间双向打分传播来量化题目难度和模型能力分数，赋予“能难倒模型的题目更难、能答对难题的模型更强”的直觉，实现对多模型多领域大规模数据的难度敏感型评测。

Result: RankLLM对30个模型在35,550道多领域题目上评测，模型与人工判定一致率高达90%，对比IRT等现有方法性能更佳，评测结果稳定、收敛快且计算开销低。

Conclusion: RankLLM不仅提升了评测算法的细粒度和可信度，也具备了大规模、实用的模型能力评测价值，有望成为LLM领域基准测评的新标准。

Abstract: Benchmarks establish a standardized evaluation framework to systematically assess the performance of large language models (LLMs), facilitating objective comparisons and driving advancements in the field. However, existing benchmarks fail to differentiate question difficulty, limiting their ability to effectively distinguish models' capabilities. To address this limitation, we propose RankLLM, a novel framework designed to quantify both question difficulty and model competency. RankLLM introduces difficulty as the primary criterion for differentiation, enabling a more fine-grained evaluation of LLM capabilities. RankLLM's core mechanism facilitates bidirectional score propagation between models and questions. The core intuition of RankLLM is that a model earns a competency score when it correctly answers a question, while a question's difficulty score increases when it challenges a model. Using this framework, we evaluate 30 models on 35,550 questions across multiple domains. RankLLM achieves 90% agreement with human judgments and consistently outperforms strong baselines such as IRT. It also exhibits strong stability, fast convergence, and high computational efficiency, making it a practical solution for large-scale, difficulty-aware LLM evaluation.

</details>


### [75] [RBCorr: Response Bias Correction in Language Models](https://arxiv.org/abs/2602.12445)
*Om Bhatt,Anna A. Ivanova*

Main category: cs.CL

TL;DR: 本文提出了一种简单但有效的语言模型响应偏差纠正方法RBCorr，能显著消除模型在固定选项任务中的偏向性，并提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 语言模型在面对固定选项（如是非、选择题）时常表现出响应偏差，这影响了其性能评估和实际能力的公平比较。现有纠偏方法往往成本较高、操作复杂，因此亟需一种既低成本又高效的解决方案。

Method: 作者提出RBCorr，一种通过调整输出概率来弱化或消除模型选项偏向性的纠正策略。在12个公开权重的语言模型和多种任务类型（是非题、蕴含题、多选题）上进行实验，系统评估了纠正前后的性能变化。

Result: 实验证明，语言模型在纠正前普遍存在明显响应偏差。RBCorr方法能高效消除这种偏差并提升模型整体表现。作者还发现，基于LogProbs的纠正效果会受到模型、数据集和提示方式的影响。

Conclusion: RBCorr是一种简单易用、能显著提升小型语言模型在闭集选项任务表现的方法，有助于模型真实能力的评估和比较。

Abstract: Language models (LMs) are known to be prone to response biases, which present as option preference biases in fixed-response questions. It is therefore imperative to develop low-cost and effective response bias correction methods to improve LM performance and enable more accurate evaluations of model abilities. Here, we propose a simple response bias correction strategy ($\texttt{RBCorr}$) and test it on 12 open-weight language models using yes-no, entailment, and multiple choice questions. We show that response bias is prevalent in LMs pre-correction and that $\texttt{RBCorr}$ effectively eliminates bias and boosts model performance. We also explore the generalizability of bias behavior across models, datasets, and prompt formats, showing that LogProbs-based correction is highly dependent on all three of these aspects. Overall, $\texttt{RBCorr}$ is an easy-to-use method that can boost the performance of smaller LMs and ensure that LM performance on closed-response benchmarks aligns more closely with their true capabilities.

</details>


### [76] [Discovering Semantic Latent Structures in Psychological Scales: A Response-Free Pathway to Efficient Simplification](https://arxiv.org/abs/2602.12575)
*Bo Wang,Yuxuan Zhang,Yueqin Hu,Hanchao Hou,Kaiping Peng,Shiguang Ni*

Main category: cs.CL

TL;DR: 本文提出了一种利用自然语言处理中的语义方法（基于嵌入与聚类的主题建模），用于问卷量表的简化和精炼。无需依赖大样本的响应数据，可实现高效、结构合理的量表缩减，并开发了可视化工具辅助应用。


<details>
  <summary>Details</summary>
Motivation: 传统量表精炼依赖响应型方法（如因子分析、IRT、网络心理测量），对样本需求大且有数据与跨文化限制。作者希望探索不依赖受试者响应、利用问卷文本语义结构的新方法，解决数据受限及跨文化适用性的问题。

Method: 将问卷项目编码为上下文句子嵌入，采用基于密度的聚类发现语义潜在因子，无需先验因子数。通过类别加权抽取具有代表性的简化题项，并整合于一键化简流程。与DASS、IPIP、EPOCH等量表对比验证。

Result: 新方法在多种量表上恢复出与传统方法类似的因子结构，量表长度平均缩减60.5%，但保留了高水平的内部一致性、因子一致性和变量间相关性。

Conclusion: 基于语义的主题建模可作为量表开发和缩减的新工具，不依赖响应数据，能高效、透明地实现精炼。为推广，作者提供配套的可视化工具。

Abstract: Psychological scale refinement traditionally relies on response-based methods such as factor analysis, item response theory, and network psychometrics to optimize item composition. Although rigorous, these approaches require large samples and may be constrained by data availability and cross-cultural comparability. Recent advances in natural language processing suggest that the semantic structure of questionnaire items may encode latent construct organization, offering a complementary response-free perspective. We introduce a topic-modeling framework that operationalizes semantic latent structure for scale simplification. Items are encoded using contextual sentence embeddings and grouped via density-based clustering to discover latent semantic factors without predefining their number. Class-based term weighting derives interpretable topic representations that approximate constructs and enable merging of semantically adjacent clusters. Representative items are selected using membership criteria within an integrated reduction pipeline. We benchmarked the framework across DASS, IPIP, and EPOCH, evaluating structural recovery, internal consistency, factor congruence, correlation preservation, and reduction efficiency. The proposed method recovered coherent factor-like groupings aligned with established constructs. Selected items reduced scale length by 60.5% on average while maintaining psychometric adequacy. Simplified scales showed high concordance with original factor structures and preserved inter-factor correlations, indicating that semantic latent organization provides a response-free approximation of measurement structure. Our framework formalizes semantic structure as an inspectable front-end for scale construction and reduction. To facilitate adoption, we provide a visualization-supported tool enabling one-click semantic analysis and structured simplification.

</details>


### [77] [Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats](https://arxiv.org/abs/2602.12635)
*Pengxiang Zhao,Hui-Ling Zhen,Xing Li,Han Bao,Weizhe Lin,Zhiyuan Yang,Ziwei Yu,Xin Wang,Mingxuan Yuan,Xianzhi Yu,Zhenhua Dong*

Main category: cs.CL

TL;DR: 本文提出并评估了专为Ascend NPU设计的HiFloat（HiF8和HiF4）低比特浮点格式，并证明其在高效率推理和精度保持方面优于传统整数量化格式。


<details>
  <summary>Details</summary>
Motivation: 随着大模型规模扩大，如何在保障推理精度的前提下进一步提升模型效率是业界关注的热点。低比特浮点格式为精度与硬件性能间找到新平衡点，尤其针对专用AI芯片平台。

Method: 评估并比较了HiFloat（HiF8、HiF4）与主流整数及浮点格式（如INT8、MXFP、NVFP4）在权重-激活及KV缓存等任务上的表现，分析不同格式在不同数据分布下的适用性，并检验其与现有后训练量化框架的兼容性。

Result: 1）INT8适合窄动态范围数据，低比特浮点更适合高方差数据；2）在4比特量化时，HiF4的分层缩放避免了精度塌陷，优于整数量化；3）HiFloat兼容现有主流量化工具链。

Conclusion: HiFloat为在NPU等专用硬件上实现高效、精确的大模型推理提供了一种有效解决方案，有利于推广低比特浮点量化在实际AI推理中的应用。

Abstract: As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs.

</details>


### [78] [CLASE: A Hybrid Method for Chinese Legalese Stylistic Evaluation](https://arxiv.org/abs/2602.12639)
*Yiran Rex Ma,Yuxiao Ye,Huiyuan Xie*

Main category: cs.CL

TL;DR: 现有大模型生成的法律文本虽然事实准确，但往往难以符合专业法律文体规范。为改进这一问题，作者提出了CLASE混合评价方法，结合语言特征与大模型评分，从而更准确和透明地评价法律文本的文体表现。实验显示该方法能更好对齐人工判断，并具备可解释性与实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的法律文本难以满足法律写作中对文体的隐性要求。人工制定评价标准不现实，现有自动评价方法对文体和语义准确性的划分不清，且大模型打分缺乏透明和一致性。

Method: 提出CLASE（中文法律文体评价），融合基于语言特征的分数和大模型主导的体验分。特征权重及模型打分经验通过真实与大模型改写法律文本的对比对学习得出，实现参考文献无关且透明的评价。

Result: 对200份中文法律文书实测，CLASE在与人工评价的一致性上明显优于传统指标和单纯靠大模型的方法。

Conclusion: CLASE能够透明、详细地评估法律文本文体，对文体改进提出实际建议，是生成法律文本自动评价的可扩展与实用方案。

Abstract: Legal text generated by large language models (LLMs) can usually achieve reasonable factual accuracy, but it frequently fails to adhere to the specialised stylistic norms and linguistic conventions of legal writing. In order to improve stylistic quality, a crucial first step is to establish a reliable evaluation method. However, having legal experts manually develop such a metric is impractical, as the implicit stylistic requirements in legal writing practice are difficult to formalise into explicit rubrics. Meanwhile, existing automatic evaluation methods also fall short: reference-based metrics conflate semantic accuracy with stylistic fidelity, and LLM-as-a-judge evaluations suffer from opacity and inconsistency. To address these challenges, we introduce CLASE (Chinese LegAlese Stylistic Evaluation), a hybrid evaluation method that focuses on the stylistic performance of legal text. The method incorporates a hybrid scoring mechanism that combines 1) linguistic feature-based scores and 2) experience-guided LLM-as-a-judge scores. Both the feature coefficients and the LLM scoring experiences are learned from contrastive pairs of authentic legal documents and their LLM-restored counterparts. This hybrid design captures both surface-level features and implicit stylistic norms in a transparent, reference-free manner. Experiments on 200 Chinese legal documents show that CLASE achieves substantially higher alignment with human judgments than traditional metrics and pure LLM-as-a-judge methods. Beyond improved alignment, CLASE provides interpretable score breakdowns and suggestions for improvements, offering a scalable and practical solution for professional stylistic evaluation in legal text generation (Code and data for CLASE is available at: https://github.com/rexera/CLASE).

</details>


### [79] [Beyond Normalization: Rethinking the Partition Function as a Difficulty Scheduler for RLVR](https://arxiv.org/abs/2602.12642)
*Dohyung Kim,Minbeom Kim,Jeonghye Kim,Sangmook Lee,Sojeong Rhee,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文提出了一种提高大模型（LLM）训练样本效率的新方法，通过重新利用GFlowNet训练过程中已有的分区函数信息，引入Partition Function-Guided RL (PACED-RL)框架，实验证明优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于奖励最大化的强化学习（RL）方法虽能提升大模型的推理表现，但常导致模型输出多样性下降。为平衡性能与多样性，近年来利用GFlowNet拟合目标分布，但现有方法未充分利用分区函数所蕴含的准确信息。

Method: 作者提出将GFlowNet中的分区函数视为每个提示问题的期望准确率信号。基于此，提出PACED-RL：一是利用该信号优先训练更有信息量的问题，二是用准确率误差进行优先回放，从而提升样本利用率，同时复用GFlowNet训练过程中的已有计算，减少额外开销。

Result: 在多个多样性与表现兼具的基准测试上，PACED-RL方法在样本效率与性能上均优于现有的GRPO与GFlowNet方法。

Conclusion: PACED-RL有效提升了大语言模型的样本效率，实现了分布拟合和性能提升，标志着大模型训练方法的一条有前景的新方向。

Abstract: Reward-maximizing RL methods enhance the reasoning performance of LLMs, but often reduce the diversity among outputs. Recent works address this issue by adopting GFlowNets, training LLMs to match a target distribution while jointly learning its partition function. In contrast to prior works that treat this partition function solely as a normalizer, we reinterpret it as a per-prompt expected-reward (i.e., online accuracy) signal, leveraging this unused information to improve sample efficiency. Specifically, we first establish a theoretical relationship between the partition function and per-prompt accuracy estimates. Building on this key insight, we propose Partition Function-Guided RL (PACED-RL), a post-training framework that leverages accuracy estimates to prioritize informative question prompts during training, and further improves sample efficiency through an accuracy estimate error-prioritized replay. Crucially, both components reuse information already produced during GFlowNet training, effectively amortizing the compute overhead into the existing optimization process. Extensive experiments across diverse benchmarks demonstrate strong performance improvements over GRPO and prior GFlowNet approaches, highlighting PACED-RL as a promising direction for a more sample efficient distribution-matching training for LLMs.

</details>


### [80] [Learning Ordinal Probabilistic Reward from Preferences](https://arxiv.org/abs/2602.12660)
*Longze Chen,Lu Wang,Renke Shan,Ze Gong,Run Luo,Jiaming Li,Jing Luo,Qiyao Wang,Min Yang*

Main category: cs.CL

TL;DR: 本文提出了一种新的奖励建模范式——概率奖励模型（PRM），通过对响应质量建模为概率分布来解决现有生成式与判别式奖励模型的局限。同时，提出离散实现（OPRM）和高效训练策略（RgFT），在多个基准上取得2.9%~7.4%的精度提升。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型用于大语言模型与人类价值对齐时，生成式模型需要昂贵的逐点监督，判别式模型得分相对且难以解释概率意义。两者都难以准确反映文本质量的绝对水平。

Method: 提出概率奖励模型（PRM），将奖励视为随机变量，对每个响应质量学习概率分布。提出其离散实现——有序概率奖励模型（OPRM），将质量得分离散为有限的等级。提出区域泛洪微调（RgFT）训练方法，结合质量等级标注，引导模型聚焦于相应子区间的概率质量。

Result: 在多个奖励模型基准数据集上，提出的方法相比现有方法准确率提升了2.9%~7.4%。分数分布分析显示方法能捕捉到响应的相对排序及绝对质量水平。

Conclusion: 概率奖励建模范式克服了传统方法的局限，可准确建模响应质量的分布特性，对提升奖励建模的数据效率、准确率和解释性有重要意义。

Abstract: Reward models are crucial for aligning large language models (LLMs) with human values and intentions. Existing approaches follow either Generative (GRMs) or Discriminative (DRMs) paradigms, yet both suffer from limitations: GRMs typically demand costly point-wise supervision, while DRMs produce uncalibrated relative scores that lack probabilistic interpretation. To address these challenges, we introduce a novel reward modeling paradigm: Probabilistic Reward Model (PRM). Instead of modeling reward as a deterministic scalar, our approach treats it as a random variable, learning a full probability distribution for the quality of each response. To make this paradigm practical, we present its closed-form, discrete realization: the Ordinal Probabilistic Reward Model (OPRM), which discretizes the quality score into a finite set of ordinal ratings. Building on OPRM, we propose a data-efficient training strategy called Region Flooding Tuning (RgFT). It enables rewards to better reflect absolute text quality by incorporating quality-level annotations, which guide the model to concentrate the probability mass within corresponding rating sub-regions. Experiments on various reward model benchmarks show that our method improves accuracy by $\textbf{2.9%}\sim\textbf{7.4%}$ compared to prior reward models, demonstrating strong performance and data efficiency. Analysis of the score distribution provides evidence that our method captures not only relative rankings but also absolute quality.

</details>


### [81] [$\mathcal{X}$-KD: General Experiential Knowledge Distillation for Large Language Models](https://arxiv.org/abs/2602.12674)
*Yuang Cai,Yuyu Yuan*

Main category: cs.CL

TL;DR: 提出了一种新颖的知识蒸馏方法（Experiential Knowledge Distillation，简称X-KD），让学生模型在教师模型原始学习环境中学习，效果优于以往蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏仅模仿教师模型的行为，忽略了塑造教师知识的原始学习环境，导致学生模型难以获得全面的知识。

Method: 借鉴体验式学习理论和逆强化学习，提出X-KD框架，通过近似变分奖励模仿学习（AVRIL）同时建模教师原始奖励函数并进行策略蒸馏，使学生策略与该奖励函数一致，可灵活适用于多种蒸馏方法。

Result: 在抽象摘要、机器翻译和算术推理等任务上，X-KD均优于现有通用知识蒸馏与MiniLLM基线模型；在性能-多样性权衡和数据效率方面也有更好表现。

Conclusion: X-KD方法不仅简单灵活，而且能更好继承教师模型优势，推动知识蒸馏在多任务、复杂环境下的应用。

Abstract: Knowledge Distillation (KD) for Large Language Models (LLMs) has become increasingly important as models grow in size and complexity. While existing distillation approaches focus on imitating teacher behavior, they often overlook the original learning environment that shaped the teacher's knowledge. Inspired by the experiential learning theory and inverse reinforcement learning, we propose Experiential Knowledge Distillation ($\mathcal{X}$-KD), a novel and general framework that enables student models to learn in the teacher's original learning environment. $\mathcal{X}$-KD adopts the Approximated Variational Reward Imitation Learning (AVRIL) framework to jointly model the teacher's original reward function and perform policy distillation, encouraging consistency between the student policy and the original reward function. Our derivation demonstrates that $\mathcal{X}$-KD follows the supervised learning framework and applies to both sequence-level and divergence-based distillation methods, underlining the simplicity and flexibility of our approach. Empirical results show that $\mathcal{X}$-KD outperforms the generalized KD and MiniLLM baselines on abstractive summarization, machine translation, and arithmetic reasoning tasks. Additionally, $\mathcal{X}$-KD achieves better performance-diversity trade-off and data efficiency than baseline KD approaches.

</details>


### [82] [MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs](https://arxiv.org/abs/2602.12705)
*Baorong Shi,Bo Cui,Boyuan Jiang,Deli Yu,Fang Qian,Haihua Yang,Huichao Wang,Jiale Chen,Jianfei Pan,Jieqiong Cao,Jinghao Lin,Kai Wu,Lin Yang,Shengsheng Yao,Tao Chen,Xiaojun Xiao,Xiaozhong Ji,Xu Wang,Yijun He,Zhixiong Yang*

Main category: cs.CL

TL;DR: MedXIAOHE是一款专为临床实际应用设计的医学视觉-语言基础模型，通过实体感知持续预训练和工具增强的推理训练，实现了多项医学基准的领先表现，并提升了医学推理与实际可用性。


<details>
  <summary>Details</summary>
Motivation: 医学中的AI模型需要在复杂多样的现实场景下，具备广泛医学知识和专家级推理能力，现有多模态模型存在知识覆盖不足、泛化性与推理追踪等局限。

Method: （1）实体感知的持续预训练，对异构医学语料组织以丰富知识和减少长尾问题；（2）多样医学推理训练，包括强化学习和工具增强的智能体训练，实现可验证的多步诊断推理；（3）集成用户偏好、基于证据的推理、低幻觉报告生成以提高实际可靠性。

Result: MedXIAOHE在多个医学基准和能力上超越现有闭源多模态系统，尤其在罕见疾病知识覆盖、推理能力、长文报告准确性等方面表现出色。

Conclusion: MedXIAOHE以创新设计和优异性能推动医学多模态模型发展，为临床AI应用提供了更可靠易用的基础，并为后续研究提供了借鉴。

Abstract: We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.

</details>


### [83] [ReFilter: Improving Robustness of Retrieval-Augmented Generation via Gated Filter](https://arxiv.org/abs/2602.12709)
*Yixin Chen,Ying Xiong,Shangyu Wu,Xiangrui Ke,Nan Guan,Chun Jason Xue*

Main category: cs.CL

TL;DR: 本文提出ReFilter，一种新颖的基于潜变量的融合方法，通过在RAG任务中对token级别的信息进行过滤和融合，在多个问答基准上获得了优异而稳定的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG融合方式在检索样本较多时往往难以保证效果——候选证据越多虽然覆盖更全，但也带来了冗余、无关信息以及更高推理成本，因此需要一种更高效的融合机制。

Method: 提出ReFilter框架，包括三个关键部分：1）context encoder对上下文特征编码，2）gated filter对每个token加权过滤，3）token fusion模块将加权后的token特征融合进LLM隐状态。通过token级的精准过滤和有效融合提升外部证据利用效率。

Result: 在四个通用问答基准上，ReFilter相比现有方法在领域内、跨领域适应下都取得最高平均性能。在五个生物医学问答基准上，无需特定领域微调，即可零样本转移，取得Qwen2.5-14B-Instruct模型70.01%的平均准确率。

Conclusion: ReFilter不仅在泛化性和适应性方面表现突出，还有效解决了大规模检索融合中的冗余和推理成本难题，为RAG技术进一步发展提供了创新路径。

Abstract: Retrieval-augmented generation (RAG) has become a dominant paradigm for grounding large language models (LLMs) with external evidence in knowledge-intensive question answering. A core design choice is how to fuse retrieved samples into the LLMs, where existing internal fusion approaches broadly fall into query-based fusion, parametric fusion, and latent-based fusion. Despite their effectiveness at modest retrieval scales, these methods often fail to scale gracefully as the number of retrieved candidates k increases: Larger k improves evidence coverage, yet realistic top-k retrieval inevitably contains irrelevant or redundant content and increases the inference cost.
  To address these limitations, we propose ReFilter, a novel latent-based fusion framework that performs token-level filtering and fusion. ReFilter consists of three key components: a context encoder for encoding context features, a gated filter for weighting each token, and a token fusion module for integrating the weighted token feature into the LLM's hidden states. Our experiments across four general-domain QA benchmarks show that ReFilter consistently achieves the best average performance under both in-domain adaptation and out-of-domain transfer. ReFilter further generalizes to five biomedical QA benchmarks in zero-shot transfer without domain fine-tuning, reaching 70.01% average accuracy with Qwen2.5-14B-Instruct.

</details>


### [84] [Lamer-SSL: Layer-aware Mixture of LoRA Experts for Continual Multilingual Expansion of Self-supervised Models without Forgetting](https://arxiv.org/abs/2602.12746)
*Jing Xu,Minglin Wu,Xueyuan Chen,Xixin Wu,Helen Meng*

Main category: cs.CL

TL;DR: 本文提出了一种高效的自监督语音模型扩展方法Lamer-SSL，通过引入Lamer模块和回放策略，有效提升了新语言泛化能力，并缓解了遗忘问题，同时大幅减少了需要训练的参数量。


<details>
  <summary>Details</summary>
Motivation: 现有自监督语音模型在面对新语言时泛化能力有限，且在持续训练过程中易遗忘已学知识，因此需要一种高效、可扩展且能保持旧知识的方法。

Method: 提出了Lamer-SSL框架，结合了Layer-Aware MixturE of LoRA Experts（Lamer）模块和回放（replay）策略：Lamer模块利用不同层分配专家，更注重深层语义信息；回放策略则用极少量旧数据维持知识记忆。整体方法具有参数高效性。

Result: 在自动语音识别（ASR）和语言识别（LID）任务上，Lamer-SSL能有效扩展自监督模型至新语言，同时对已学语言保持优异表现，且仅需2.14%的参数训练。

Conclusion: Lamer-SSL解决了自监督语音模型跨语言泛化和遗忘问题，具备优良的参数效率和实际应用潜力，为多语言持续学习提供了新方案。

Abstract: Despite their impressive performance, self-supervised speech models often struggle to generalize to new languages and tend to forget previously acquired knowledge during continual training. To address this, we propose Lamer-SSL, a parameter-efficient framework that integrates a Layer-Aware MixturE of LoRA Experts (Lamer) module with a replay strategy. The Lamer module enables flexible balancing between shared and language-specific representations, while layer-aware expert allocation assigns more experts to deeper layers where semantic information is richer. Meanwhile, the replay strategy retains prior knowledge using minimal data, mitigating forgetting during continual training. Experiments on automatic speech recognition (ASR) and language identification (LID) demonstrate that Lamer-SSL extends self-supervised models to new languages effectively while maintaining strong performance on previously learned languages with only 2.14% parameters being trainable.

</details>


### [85] [Towards a Diagnostic and Predictive Evaluation Methodology for Sequence Labeling Tasks](https://arxiv.org/abs/2602.12759)
*Elena Alvarez-Mellado,Julio Gonzalo*

Main category: cs.CL

TL;DR: 本文提出了一种基于错误分析的序列标注评估方法，通过人工设计小规模、语言学动机强的测试集，能更好地诊断模型的优缺点，并提升对分布外数据的预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有NLP评估主要比较平均效果，难以指导如何改进系统，也无法预测模型在外部分布上的表现。因此有必要发展更能反映模型泛化性和问题定位能力的评估方法。

Method: 作者提出不依赖大规模同分布数据，而是人工打造覆盖各种可能的span属性（形状、长度、大小写、句中位置等）的测试集，然后用这些测试集进行定量和定性分析，以揭示模型的系统性弱点和泛化能力。

Result: 以西班牙语英语借词识别为例，作者的方法能诊断出模型的弱点，明确指出改进方向，并且模型在这些测试集上的表现与真实分布外数据的相关性高达0.85。

Conclusion: 所提出的评估方法比传统方法诊断性强、可操作性高且具预测力，适用于改进和选择序列标注模型，能更好地预测模型在新场景的表现。

Abstract: Standard evaluation in NLP typically indicates that system A is better on average than system B, but it provides little info on how to improve performance and, what is worse, it should not come as a surprise if B ends up being better than A on outside data. We propose an evaluation methodology for sequence labeling tasks grounded on error analysis that provides both quantitative and qualitative information on where systems must be improved and predicts how models will perform on a different distribution. The key is to create test sets that, contrary to common practice, do not rely on gathering large amounts of real-world in-distribution scraped data, but consists in handcrafting a small set of linguistically motivated examples that exhaustively cover the range of span attributes (such as shape, length, casing, sentence position, etc.) a system may encounter in the wild. We demonstrate this methodology on a benchmark for anglicism identification in Spanish. Our methodology provides results that are diagnostic (because they help identify systematic weaknesses in performance), actionable (because they can inform which model is better suited for a given scenario) and predictive: our method predicts model performance on external datasets with a median correlation of 0.85.

</details>


### [86] [Aspect-Based Sentiment Analysis for Future Tourism Experiences: A BERT-MoE Framework for Persian User Reviews](https://arxiv.org/abs/2602.12778)
*Hamidreza Kazemi Taskooh,Taha Zare Harofte*

Main category: cs.CL

TL;DR: 本研究针对波斯语旅游领域用户评论的细粒度情感分析，提出了高效BERT混合模型，显著提升准确性并降低算力消耗，同时公开数据集推动多语言NLP研究。


<details>
  <summary>Details</summary>
Motivation: 波斯语等低资源语言在旅游等应用领域的细粒度情感分析（ABSA）缺乏有效方法和数据集，限制了用户体验和行业发展。

Method: 提出结合BERT、Top-K路由和辅助损失的混合模型，通过三级流程：1）BERT整体情感分类，2）多标签旅游相关属性提取，3）动态路由集成ABSA。使用Jabama平台58,473条人工标注波斯语评论数据集进行训练和评测。

Result: 模型在旅游评论情感分析上达到加权F1分数90.6%，高于BERT基线（89.25%）及传统混合方法（85.7%）；相较密集BERT方法，GPU功耗降低39%。

Conclusion: 首次实现针对波斯旅游评论的ABSA，高准确率和能耗优势推动可持续AI，关键属性为清洁和设施。公开数据集有助于推动旅游和多语言NLP研究。

Abstract: This study advances aspect-based sentiment analysis (ABSA) for Persian-language user reviews in the tourism domain, addressing challenges of low-resource languages. We propose a hybrid BERT-based model with Top-K routing and auxiliary losses to mitigate routing collapse and improve efficiency. The pipeline includes: (1) overall sentiment classification using BERT on 9,558 labeled reviews, (2) multi-label aspect extraction for six tourism-related aspects (host, price, location, amenities, cleanliness, connectivity), and (3) integrated ABSA with dynamic routing. The dataset consists of 58,473 preprocessed reviews from the Iranian accommodation platform Jabama, manually annotated for aspects and sentiments. The proposed model achieves a weighted F1-score of 90.6% for ABSA, outperforming baseline BERT (89.25%) and a standard hybrid approach (85.7%). Key efficiency gains include a 39% reduction in GPU power consumption compared to dense BERT, supporting sustainable AI deployment in alignment with UN SDGs 9 and 12. Analysis reveals high mention rates for cleanliness and amenities as critical aspects. This is the first ABSA study focused on Persian tourism reviews, and we release the annotated dataset to facilitate future multilingual NLP research in tourism.

</details>


### [87] [RAT-Bench: A Comprehensive Benchmark for Text Anonymization](https://arxiv.org/abs/2602.12806)
*Nataša Krčo,Zexi Yao,Matthieu Meeus,Yves-Alexandre de Montjoye*

Main category: cs.CL

TL;DR: 本文提出并评估了一种基于再识别风险的文本匿名化基准（RAT-Bench），发现现有匿名化工具在保护隐私方面尚有不足，特别是对间接识别信息和非标准化格式时表现不佳。LLM驱动的匿名化方法在多语言环境下效果更优，但代价更高。


<details>
  <summary>Details</summary>
Motivation: 广泛使用的大型语言模型（LLM）常常需要处理包含个人信息的数据。虽然通常会进行去标识化处理，但当前匿名化工具对防止被重新识别的有效性尚不明确，因此需要有新的评测基准来衡量匿名化工具的真实隐私保护能力。

Method: 作者提出RAT-Bench基准，利用美国人口统计数据，合成含有多种直接和间接识别信息的文本，涵盖不同领域、语言和难度，并评估多种NER和LLM驱动的文本匿名工具。通过模拟LLM攻击者尝试从匿名化文本推断身份属性，从而量化实际的再识别风险。

Result: 不同匿名化工具的隐私保护能力差异很大。即使是最好的匿名工具，在直接标识符采用非标准方式描述或间接标识符能够辅助推理身份时也无法完全避免再识别风险。LLM驱动的匿名化方法在隐私与数据实用性间取得更好平衡，并能支持多语言场景，但需付出更高的计算成本。

Conclusion: 现有匿名化工具距离理想的隐私保护标准还有差距，建议未来工具开发应关注非标准格式和间接标识符，并拓展基准数据到更多地理区域。作者将公开RAT-Bench基准，呼吁社区合作完善和推广多地域匿名化评测。

Abstract: Data containing personal information is increasingly used to train, fine-tune, or query Large Language Models (LLMs). Text is typically scrubbed of identifying information prior to use, often with tools such as Microsoft's Presidio or Anthropic's PII purifier. These tools have traditionally been evaluated on their ability to remove specific identifiers (e.g., names), yet their effectiveness at preventing re-identification remains unclear. We introduce RAT-Bench, a comprehensive benchmark for text anonymization tools based on re-identification risk. Using U.S. demographic statistics, we generate synthetic text containing various direct and indirect identifiers across domains, languages, and difficulty levels. We evaluate a range of NER- and LLM-based text anonymization tools and, based on the attributes an LLM-based attacker is able to correctly infer from the anonymized text, we report the risk of re-identification in the U.S. population, while properly accounting for the disparate impact of identifiers. We find that, while capabilities vary widely, even the best tools are far from perfect in particular when direct identifiers are not written in standard ways and when indirect identifiers enable re-identification. Overall we find LLM-based anonymizers, including new iterative anonymizers, to provide a better privacy-utility trade-off albeit at a higher computational cost. Importantly, we also find them to work well across languages. We conclude with recommendations for future anonymization tools and will release the benchmark and encourage community efforts to expand it, in particular to other geographies.

</details>


### [88] [Left-right asymmetry in predicting brain activity from LLMs' representations emerges with their formal linguistic competence](https://arxiv.org/abs/2602.12811)
*Laurent Bonnasse-Gahot,Christophe Pallier*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）在训练过程中与人脑活动之间的关系，发现模型的内部激活与fMRI测量的人类大脑活动（尤其是在左半球）存在相关性，并探讨了这种左右脑预测性不对称性与语言能力的关联。


<details>
  <summary>Details</summary>
Motivation: 已知LLM内部激活与人脑活动存在相关性，且训练过程中左脑预测能力提升更显著，但导致这种左右脑预测不对称的具体能力和机制尚不明确，因此希望明确是哪类能力驱动该不对称性。

Method: 作者使用OLMo-2 7B模型在不同训练阶段的激活数据以及英语受试者的fMRI数据，比较了模型对左右脑的预测得分随训练演化的情况，并将其与模型在不同任务基准（如语法判断、文本生成、计算、世界知识推理等）上的表现进行对比；还用Pythia模型和法语数据做了推广验证。

Result: 发现LLM预测左脑的能力随训练过程提升，尤其与模型的形式语言能力（语法判断、生成规范文本能力）共现，而与算术、Dyck语言任务及知识推理任务表现无关；这一结果在不同模型家族和语言下均被验证。

Conclusion: LLM与人类大脑左-右预测性不对称主要反映了模型对形式语言能力（语言模式知识）的掌握进展，而非一般认知、推理或世界知识能力。

Abstract: When humans and large language models (LLMs) process the same text, activations in the LLMs correlate with brain activity measured, e.g., with functional magnetic resonance imaging (fMRI). Moreover, it has been shown that, as the training of an LLM progresses, the performance in predicting brain activity from its internal activations improves more in the left hemisphere than in the right one. The aim of the present work is to understand which kind of competence acquired by the LLMs underlies the emergence of this left-right asymmetry. Using the OLMo-2 7B language model at various training checkpoints and fMRI data from English participants, we compare the evolution of the left-right asymmetry in brain scores alongside performance on several benchmarks. We observe that the asymmetry co-emerges with the formal linguistic abilities of the LLM. These abilities are demonstrated in two ways: by the model's capacity to assign a higher probability to an acceptable sentence than to a grammatically unacceptable one within a minimal contrasting pair, or its ability to produce well-formed text. On the opposite, the left-right asymmetry does not correlate with the performance on arithmetic or Dyck language tasks; nor with text-based tasks involving world knowledge and reasoning. We generalize these results to another family of LLMs (Pythia) and another language, namely French. Our observations indicate that the left-right asymmetry in brain predictivity matches the progress in formal linguistic competence (knowledge of linguistic patterns).

</details>


### [89] [AIWizards at MULTIPRIDE: A Hierarchical Approach to Slur Reclamation Detection](https://arxiv.org/abs/2602.12818)
*Luca Tedeschini,Matteo Fasulo*

Main category: cs.CL

TL;DR: 本文提出了一种层次化方法来检测 reclaimed slurs（被群体收回的侮辱词），通过结合用户潜在身份和上下文信息提升仇恨言论检测的表现。方法在意大利语和西班牙语数据上表现与强BERT基线相当，并具备良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论检测系统难以区分 slur（侮辱词）是带有攻击性质还是被群体内部作为自我肯定的 reclaimed slur。理解 slur 的社会身份和语境对于减少误报，提升仇恨言论自动识别准确性非常重要。

Method: 作者假设 LGBTQ+ 群体更易以 reclamatory（积极认同）方式使用特定 slur。方法采用两阶段：第一阶段利用弱监督的LLM对用户（通过发帖内容和个签）做模糊标签，训练BERT模型学习用户社区归属的表示；第二阶段，将此用户潜在空间与预训练的仇恨言论检测模型融合，进行 reclaimed slur 检测。

Result: 该方法在意大利语和西班牙语的 MultiPRIDE 任务数据上，与强BERT基线模型取得统计上相当的效果，并验证了引入用户社会语言学信息对提升模型表现的作用。

Conclusion: 作者认为，结合用户身份和话语上下文的更细粒度层次建模，有助于提升 reclaimed slur 的检测准确率，为仇恨言论检测系统拓展出更有社会语言学意识的可扩展框架。代码已公开。

Abstract: Detecting reclaimed slurs represents a fundamental challenge for hate speech detection systems, as the same lexcal items can function either as abusive expressions or as in-group affirmations depending on social identity and context. In this work, we address Subtask B of the MultiPRIDE shared task at EVALITA 2026 by proposing a hierarchical approach to modeling the slur reclamation process. Our core assumption is that members of the LGBTQ+ community are more likely, on average, to employ certain slurs in a eclamatory manner. Based on this hypothesis, we decompose the task into two stages. First, using a weakly supervised LLM-based annotation, we assign fuzzy labels to users indicating the likelihood of belonging to the LGBTQ+ community, inferred from the tweet and the user bio. These soft labels are then used to train a BERT-like model to predict community membership, encouraging the model to learn latent representations associated with LGBTQ+ identity. In the second stage, we integrate this latent space with a newly initialized model for the downstream slur reclamation detection task. The intuition is that the first model encodes user-oriented sociolinguistic signals, which are then fused with representations learned by a model pretrained for hate speech detection. Experimental results on Italian and Spanish show that our approach achieves performance statistically comparable to a strong BERT-based baseline, while providing a modular and extensible framework for incorporating sociolinguistic context into hate speech modeling. We argue that more fine-grained hierarchical modeling of user identity and discourse context may further improve the detection of reclaimed language. We release our code at https://github.com/LucaTedeschini/multipride.

</details>


### [90] [MentalBench: A Benchmark for Evaluating Psychiatric Diagnostic Capability of Large Language Models](https://arxiv.org/abs/2602.12871)
*Hoyun Song,Migyeong Kang,Jisu Shin,Jihyun Kim,Chanbi Park,Hangyeol Yoo,Jihyun An,Alice Oh,Jinyoung Han,KyungTae Lim*

Main category: cs.CL

TL;DR: 本文提出了MentalBench，一种用于评估大语言模型（LLM）精神病学诊断决策能力的新基准，解决现有基准依赖社交媒体数据、难以反映DSM标准的不足。新增的MentalKG知识图谱与合成的临床病例让评测更精准、更具可解释性。测试发现LLM能较好处理结构化知识查询，但在复杂临床诊断中的信心校准存在较大缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有精神健康相关的测评基准大多依赖社交媒体文本，难以真正评估LLM在基于DSM标准进行临床精神病学诊断的能力。因此，需要为精神病学领域建立更严谨、专业的评测标准。

Method: 作者构建了MentalKG，这是一份由精神科专家构建并验证的DSM-5知识图谱，包含23种精神障碍的诊断和鉴别诊断规则。利用此知识图，生成了24750个信息完整性和诊断复杂度各异的合成临床病例，并基于这些案例对LLM进行系统性评测。

Result: 实验表明，当前最先进的LLM能很好地回答检测DSM-5知识的结构化问题，但在多病种重叠、高复杂度临床决策中信心校准不足，难以区分高度相关的精神障碍。

Conclusion: MentalBench能更精准、系统地评价LLM在精神病学诊断任务上的真实能力，发现了现有基准忽略的评测盲点。该研究为后续改进LLM的临床诊断表现和安全性提供了新方向。

Abstract: We introduce MentalBench, a benchmark for evaluating psychiatric diagnostic decision-making in large language models (LLMs). Existing mental health benchmarks largely rely on social media data, limiting their ability to assess DSM-grounded diagnostic judgments. At the core of MentalBench is MentalKG, a psychiatrist-built and validated knowledge graph encoding DSM-5 diagnostic criteria and differential diagnostic rules for 23 psychiatric disorders. Using MentalKG as a golden-standard logical backbone, we generate 24,750 synthetic clinical cases that systematically vary in information completeness and diagnostic complexity, enabling low-noise and interpretable evaluation. Our experiments show that while state-of-the-art LLMs perform well on structured queries probing DSM-5 knowledge, they struggle to calibrate confidence in diagnostic decision-making when distinguishing between clinically overlapping disorders. These findings reveal evaluation gaps not captured by existing benchmarks.

</details>


### [91] [BaziQA-Benchmark: Evaluating Symbolic and Temporally Compositional Reasoning in Large Language Models](https://arxiv.org/abs/2602.12889)
*Jiangxi Chen,Qian Liu*

Main category: cs.CL

TL;DR: BaziQA-Benchmark是一个用于评估大语言模型符号推理和时序推理能力的新基准，来源于专业赛事题目，支持客观评分。实验证明现有模型水平高于随机，但在复杂时序推理和多条件符号判断上仍有明显不足。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，评估其符号和时序推理能力成为难点，现有测试缺乏客观性和结构化。作者希望通过专业、标准化基准解决评测准确性和可比性问题。

Method: 作者从全球算命师竞赛中选取了200道多选题，要求模型基于固定符号表和时序条件多轮推理。引入有结构的推理协议以控制推理顺序，对多家模型在不同难度、领域和协议下进行系统测试与分析。

Result: 主流模型均优于随机猜测，但距离饱和表现仍有差距。模型对时序结构、推理顺序敏感；在复杂时序定位和多条件判断中易出错。结构化协议部分约束了推理流程，但未显著提升准确度。

Conclusion: BaziQA-Benchmark为大模型符号与时序推理提供了客观、标准的测试框架。当前模型虽有进步，但面临时序和多条件推理等挑战仍未解决，为推理能力提升指明了方向。

Abstract: We present BaziQA-Benchmark, a standardized benchmark for evaluating symbolic and temporally compositional reasoning in large language models. The benchmark is derived from 200 professionally curated, multiple-choice problems from the Global Fortune-teller Competition (2021--2025), where each instance requires structured inference over a fixed symbolic chart and interacting temporal conditions. Unlike anecdotal or prompt-driven evaluations, BaziQA-Benchmark enables objective scoring and controlled comparison across years, domains, and model families. We evaluate contemporary language models under a multi-turn setting and analyze performance variation across temporal difficulty, reasoning domains, and inference protocols.To further probe reasoning behavior, we introduce a lightweight Structured Reasoning Protocol that constrains inference order without adding domain knowledge. Results show that models consistently outperform chance but remain far from saturation, exhibiting pronounced sensitivity to temporal composition and reasoning order, as well as systematic failures on precise temporal localization and multi-condition symbolic judgments.

</details>


### [92] [ViMedCSS: A Vietnamese Medical Code-Switching Speech Dataset & Benchmark](https://arxiv.org/abs/2602.12911)
*Tung X. Nguyen,Nhu Vo,Giang-Son Nguyen,Duy Mai Hoang,Chien Dinh Huynh,Inigo Jauregi Unanue,Massimo Piccardi,Wray Buntine,Dung D. Le*

Main category: cs.CL

TL;DR: 本文提出了越南语医疗领域中英混合语音识别的首个基准数据集（ViMedCSS），并评估了多种ASR模型和调优策略。结果表明结合越南语优化与多语言预训练的方法在整体和混合识别准确率上达到最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 越南人在医疗交流中常用英语医学术语，这导致主流ASR系统难以精准识别混合语句，并且此领域缺乏公开评估基准。本文旨在弥补这一空白，推动低资源语言的代码切换ASR研究。

Method: 作者构建了包含16,576条、34小时的ViMedCSS语音数据集，每句包含至少一个英语医学词汇，涵盖五大医疗主题。基于该数据集，评测了多种SOTA ASR模型，并深入对比特定的微调策略以提升医学术语识别效果。

Result: 实验显示，专为越南语优化的模型能更好识别一般语段，而利用多语言预训练则更擅长捕捉英文夹杂词。结合两者后，模型在总体识别率和代码切换片段上的表现都最优。

Conclusion: 本文首创性地构建了越南语医疗代码切换ASR基准，验证了多语言与定制优化结合的有效性，并为低资源多语言ASR的领域自适应提供了实践参考。

Abstract: Code-switching (CS), which is when Vietnamese speech uses English words like drug names or procedures, is a common phenomenon in Vietnamese medical communication. This creates challenges for Automatic Speech Recognition (ASR) systems, especially in low-resource languages like Vietnamese. Current most ASR systems struggle to recognize correctly English medical terms within Vietnamese sentences, and no benchmark addresses this challenge. In this paper, we construct a 34-hour \textbf{Vi}etnamese \textbf{Med}ical \textbf{C}ode-\textbf{S}witching \textbf{S}peech dataset (ViMedCSS) containing 16,576 utterances. Each utterance includes at least one English medical term drawn from a curated bilingual lexicon covering five medical topics. Using this dataset, we evaluate several state-of-the-art ASR models and examine different specific fine-tuning strategies for improving medical term recognition to investigate the best approach to solve in the dataset. Experimental results show that Vietnamese-optimized models perform better on general segments, while multilingual pretraining helps capture English insertions. The combination of both approaches yields the best balance between overall and code-switched accuracy. This work provides the first benchmark for Vietnamese medical code-switching and offers insights into effective domain adaptation for low-resource, multilingual ASR systems.

</details>


### [93] [When Words Don't Mean What They Say: Figurative Understanding in Bengali Idioms](https://arxiv.org/abs/2602.12921)
*Adib Sakhawat,Shamim Ara Parveen,Md Ruhul Amin,Shamim Al Mahmud,Md Saiful Islam,Tahera Khatun*

Main category: cs.CL

TL;DR: 作者提出了一个包含10,361条孟加拉语成语的大型数据集，并用其评估了30个多语种大型语言模型对成语理解的能力，发现模型表现远低于人类。该工作为低资源语言的成语理解与文化嵌入提供了数据和基准。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型在理解比喻性语言，尤其是低资源语言（如孟加拉语）的成语上存在显著困难，缺乏相关高质量数据集与评估基准。

Method: 构建了一个规模庞大、带有丰富注释（涵盖语义、句法、文化与宗教层面）的孟加拉语成语数据集，并建立了一套基准任务，评测30个多语种、指令微调的LLM在成语意义推理任务上的表现。

Result: 所有模型的准确率均未超过50%，远低于人类表现（83.4%），体现出现有模型在跨语言与文化推理上的局限性。

Conclusion: 新数据集与基准为今后孟加拉语及其他低资源语言在比喻性语言理解与文化嵌入的研究提供了基础设施，对推动相关领域发展具有重要意义。

Abstract: Figurative language understanding remains a significant challenge for Large Language Models (LLMs), especially for low-resource languages. To address this, we introduce a new idiom dataset, a large-scale, culturally-grounded corpus of 10,361 Bengali idioms. Each idiom is annotated under a comprehensive 19-field schema, established and refined through a deliberative expert consensus process, that captures its semantic, syntactic, cultural, and religious dimensions, providing a rich, structured resource for computational linguistics. To establish a robust benchmark for Bangla figurative language understanding, we evaluate 30 state-of-the-art multilingual and instruction-tuned LLMs on the task of inferring figurative meaning. Our results reveal a critical performance gap, with no model surpassing 50% accuracy, a stark contrast to significantly higher human performance (83.4%). This underscores the limitations of existing models in cross-linguistic and cultural reasoning. By releasing the new idiom dataset and benchmark, we provide foundational infrastructure for advancing figurative language understanding and cultural grounding in LLMs for Bengali and other low-resource languages.

</details>


### [94] [Curriculum Learning and Pseudo-Labeling Improve the Generalization of Multi-Label Arabic Dialect Identification Models](https://arxiv.org/abs/2602.12937)
*Ali Mekky,Mohamed El Zeftawy,Lara Hassan,Amr Keleg,Preslav Nakov*

Main category: cs.CL

TL;DR: 本文提出了用于阿拉伯语方言识别（ADI）的多标签分类新方法，并构建了首个面向该任务的大规模多标签数据集，有效提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 以往阿拉伯语方言识别被视为单标签分类任务，但实际上，一句话可能包含多种方言特征，因此更合理的方式是多标签分类。然而，目前普遍缺乏大规模多标签数据集，限制了该方向的发展。

Method: 1）通过GPT-4o和二元方言可接受性分类器，对现有单标签数据自动生成多标签标注，生成多标签数据集。2）采用阿拉伯方言程度（ALDi）指导数据聚合。3）基于BERT，结合课程学习方法，设计并训练多标签分类模型。

Result: 所提出的LAHJATBERT多标签分类模型在MLADI排行榜上取得macro F1 = 0.69，显著优于之前最强系统的0.55。

Conclusion: 通过自动化方法构建了阿拉伯语多标签方言数据集，有效解决了负样本选择难题；提出的多标签识别模型显著提高了性能，为阿拉伯语多方言自动识别提供了新资源和方法。

Abstract: Being modeled as a single-label classification task for a long time, recent work has argued that Arabic Dialect Identification (ADI) should be framed as a multi-label classification task. However, ADI remains constrained by the availability of single-label datasets, with no large-scale multi-label resources available for training. By analyzing models trained on single-label ADI data, we show that the main difficulty in repurposing such datasets for Multi-Label Arabic Dialect Identification (MLADI) lies in the selection of negative samples, as many sentences treated as negative could be acceptable in multiple dialects. To address these issues, we construct a multi-label dataset by generating automatic multi-label annotations using GPT-4o and binary dialect acceptability classifiers, with aggregation guided by the Arabic Level of Dialectness (ALDi). Afterward, we train a BERT-based multi-label classifier using curriculum learning strategies aligned with dialectal complexity and label cardinality. On the MLADI leaderboard, our best-performing LAHJATBERT model achieves a macro F1 of 0.69, compared to 0.55 for the strongest previously reported system. Code and data are available at https://mohamedalaa9.github.io/lahjatbert/.

</details>


### [95] [ProbeLLM: Automating Principled Diagnosis of LLM Failures](https://arxiv.org/abs/2602.12966)
*Yue Huang,Zhengzhe Jiang,Yuchen Ma,Yu Jiang,Xiangqi Wang,Yujun Zhou,Yuexing Hao,Kehan Guo,Pin-Yu Chen,Stefan Feuerriegel,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 本文提出ProbeLLM，一种不依赖具体基准的自动化探测框架，通过系统性、分层式方法发现和归纳大语言模型的结构化失效模式。


<details>
  <summary>Details</summary>
Motivation: 目前对大语言模型（LLMs）失效的理解跟不上模型迭代速度，且现有自动化探测方法多为孤立性失败发现，缺乏原理性探索及对模型弱点结构的深入洞见。

Method: ProbeLLM将探测构建为分层蒙特卡洛树搜索，合理分配全局与局部的有限探测预算；依赖可验证测试案例，通过工具增强生成和验证，确保失效发现的可证据性。最后借助失败感知嵌入和边界归纳技术，把多个失败案例归纳为可解释的失效模式。

Result: 在多个基准和不同LLM实验中，ProbeLLM可揭示比静态基准及现有自动化方法更广、更净化、更细粒度的失效模式图谱。

Conclusion: ProbeLLM 支持从单案例评测转向结构化、原理化的弱点发现，为理解和改进LLM提供更坚实的依据。

Abstract: Understanding how and why large language models (LLMs) fail is becoming a central challenge as models rapidly evolve and static evaluations fall behind. While automated probing has been enabled by dynamic test generation, existing approaches often discover isolated failure cases, lack principled control over exploration, and provide limited insight into the underlying structure of model weaknesses. We propose ProbeLLM, a benchmark-agnostic automated probing framework that elevates weakness discovery from individual failures to structured failure modes. ProbeLLM formulates probing as a hierarchical Monte Carlo Tree Search, explicitly allocating limited probing budgets between global exploration of new failure regions and local refinement of recurring error patterns. By restricting probing to verifiable test cases and leveraging tool-augmented generation and verification, ProbeLLM grounds failure discovery in reliable evidence. Discovered failures are further consolidated into interpretable failure modes via failure-aware embeddings and boundary-aware induction. Across diverse benchmarks and LLMs, ProbeLLM reveals substantially broader, cleaner, and more fine-grained failure landscapes than static benchmarks and prior automated methods, supporting a shift from case-centric evaluation toward principled weakness discovery.

</details>


### [96] [SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents](https://arxiv.org/abs/2602.12984)
*Yujiong Shen,Yajie Yang,Zhiheng Xi,Binze Hu,Huayu Sha,Jiazheng Zhang,Qiyuan Peng,Junlin Shang,Jixuan Huang,Yutao Fan,Jingqi Tong,Shihan Dou,Ming Zhang,Lei Bai,Zhenfei Yin,Tao Gui,Xingjun Ma,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang*

Main category: cs.CL

TL;DR: 本文提出了一个用于科学推理智能体的评测与训练新平台和方法，包括SciAgentGym环境、SciAgentBench评测套件，以及SciForge数据合成方法，并展示了提升智能体科学工具使用能力的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的智能体基准测试低估了智能体集成、管理各类科学工具以完成复杂科学推理和多步骤工作流的能力，为此需要开发更细致、实际和复杂的评测环境与方法。

Method: 作者开发了SciAgentGym，包含1780个自然科学领域专用工具，配合健壮的执行架构，并推出了分层评测套件SciAgentBench，覆盖基本操作到长程工作流。针对现有模型在多步骤工具使用上失效的瓶颈，作者提出SciForge数据合成方法，通过将工具操作空间建模为依赖图来生成具逻辑性的训练轨迹，并据此微调模型。

Result: 评测显示，当前最先进的模型（如GPT-5）在延长的交互任务中成功率大幅下降（60.6%降至30.9%），主要因无法有效完成多步骤工作流。采用SciForge后，微调得到的SciAgent-8B模型超越了体量更大的Qwen3-VL-235B-Instruct，并展现了科学工具泛化能力。

Conclusion: 本文提出的环境与方法为科学推理智能体的系统性评估和训练提供了新途径，也证明了逻辑感知轨迹合成对科学工具使用能力提升的显著作用，预示了下一代科学领域自动智能体的巨大潜力。

Abstract: Scientific reasoning inherently demands integrating sophisticated toolkits to navigate domain-specific knowledge. Yet, current benchmarks largely overlook agents' ability to orchestrate tools for such rigorous workflows. To bridge this gap, we introduce SciAgentGym, a scalable interactive environment featuring 1,780 domain-specific tools across four natural science disciplines, supported by a robust execution infrastructure. Complementing this, we present SciAgentBench, a tiered evaluation suite designed to stress-test agentic capabilities from elementary actions to long-horizon workflows. Our evaluation identifies a critical bottleneck: state-of-the-art models struggle with complex scientific tool-use. Even for a leading model like GPT-5, success rates drop sharply from 60.6% to 30.9% as interaction horizons extend, primarily due to failures in multi-step workflow execution. To address this, we propose SciForge, a data synthesis method that models the tool action space as a dependency graph to generate logic-aware training trajectories. By fine-tuning on these trajectories, our SciAgent-8B outperforms the significantly larger Qwen3-VL-235B-Instruct while exhibiting positive cross-domain transfer of scientific tool-use capabilities. These results underscore the promising potential of next-generation autonomous scientific agents.

</details>


### [97] [Evaluating the Homogeneity of Keyphrase Prediction Models](https://arxiv.org/abs/2602.12989)
*Maël Houbre,Florian Boudin,Beatrice Daille*

Main category: cs.CL

TL;DR: 本论文对关键词生成模型的同质性进行评估，并首次提出相关评测方法，结果发现生成缺失关键词的能力反而可能降低模型的同质性。


<details>
  <summary>Details</summary>
Motivation: 关键词生成模型能够生成文档中未出现的“缺失关键词”，理论上能更好地为同主题文档分配相似标签，实现同质性的索引，但目前相关同质性未被评估。

Method: 提出一种新的评估方法衡量关键词生成模型的同质性，并研究生成缺失关键词的能力是否有助于提升模型同质性。比较了关键词提取与生成模型的表现。

Result: 实验结果发现，关键词提取方法与生成模型在同质性方面表现相当，而生成缺失关键词的能力有时甚至损害了同质性。

Conclusion: 关键词生成模型并非在同质性上绝对优于传统提取方法，生成缺失关键词的能力甚至可能产生负面影响。

Abstract: Keyphrases which are useful in several NLP and IR applications are either extracted from text or predicted by generative models. Contrarily to keyphrase extraction approaches, keyphrase generation models can predict keyphrases that do not appear in a document's text called `absent keyphrases`. This ability means that keyphrase generation models can associate a document to a notion that is not explicitly mentioned in its text. Intuitively, this suggests that for two documents treating the same subjects, a keyphrase generation model is more likely to be homogeneous in their indexing i.e. predict the same keyphrase for both documents, regardless of those keyphrases appearing in their respective text or not; something a keyphrase extraction model would fail to do. Yet, homogeneity of keyphrase prediction models is not covered by current benchmarks. In this work, we introduce a method to evaluate the homogeneity of keyphrase prediction models and study if absent keyphrase generation capabilities actually help the model to be more homogeneous. To our surprise, we show that keyphrase extraction methods are competitive with generative models, and that the ability to generate absent keyphrases can actually have a negative impact on homogeneity. Our data, code and prompts are available on huggingface and github.

</details>


### [98] [Know More, Know Clearer: A Meta-Cognitive Framework for Knowledge Augmentation in Large Language Models](https://arxiv.org/abs/2602.12996)
*Hao Chen,Ye He,Yuchun Fan,Yukun Yan,Zhenghao Liu,Qingfu Zhu,Maosong Sun,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文提出了一种新的元认知框架，通过区分和干预提升大语言模型（LLM）在知识密集型任务中的可靠性，解决现有方法忽视模型知识自信与实际正确性不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 以往知识增强方法默认模型表现等同于其内在知识，忽视了模型在知识自信上的偏差，导致过度自信的错误或不确定的正确，影响模型在实际应用中的可靠性。

Method: 提出利用模型内部认知信号将知识空间划分为‘掌握’、‘困惑’和‘缺失’三个区域，针对性地扩展知识，并引入认知一致性机制，使模型的主观自信与客观准确度相同步，校准知识边界。

Result: 大量实验证明，该框架在提升知识能力和区分已知/未知方面，一致优于现有强基线。

Conclusion: 该方法不仅增强了LLM的知识能力，还培养了其认知行为，有效地区分已知与未知，提升了模型的可靠性和实用性。

Abstract: Knowledge augmentation has significantly enhanced the performance of Large Language Models (LLMs) in knowledge-intensive tasks. However, existing methods typically operate on the simplistic premise that model performance equates with internal knowledge, overlooking the knowledge-confidence gaps that lead to overconfident errors or uncertain truths. To bridge this gap, we propose a novel meta-cognitive framework for reliable knowledge augmentation via differentiated intervention and alignment. Our approach leverages internal cognitive signals to partition the knowledge space into mastered, confused, and missing regions, guiding targeted knowledge expansion. Furthermore, we introduce a cognitive consistency mechanism to synchronize subjective certainty with objective accuracy, ensuring calibrated knowledge boundaries. Extensive experiments demonstrate the our framework consistently outperforms strong baselines, validating its rationality in not only enhancing knowledge capabilities but also fostering cognitive behaviors that better distinguish knowns from unknowns.

</details>


### [99] [Can we trust AI to detect healthy multilingual English speakers among the cognitively impaired cohort in the UK? An investigation using real-world conversational speech](https://arxiv.org/abs/2602.13047)
*Madhurananda Pahar,Caitlin Illingworth,Dorota Braun,Bahman Mirheidari,Lise Sproson,Daniel Blackburn,Heidi Christensen*

Main category: cs.CL

TL;DR: 本研究探讨了当前AI模型在英国多语背景下检测认知障碍的偏见，发现即使整体表现强劲，却对少数族裔多语人群存在显著误判风险。


<details>
  <summary>Details</summary>
Motivation: 英国来自少数族裔的多语人口迅速增加，认知障碍如痴呆症的发病率也在这些群体中持续上升。现有AI检测认知障碍方法，是否对多语少数族裔人群存在偏见尚不明确，对这些群体的可靠诊断亟需验证。

Method: 作者在英国全国范围招募单语者，并在谢菲尔德和布拉德福德的社区中心招募多语者（包括讲索马里语、中文或南亚语言），根据口音细分（西约克和南约克）。利用自动语音识别（ASR）及分类/回归模型，通过言语任务检测认知能力，评估模型在不同群体间的表现差异和潜在偏见。

Result: ASR系统在群体间基本无明显偏见，但基于声学和语言特征的分类、回归模型，对多语人群在记忆、语言流畅性和阅读任务中存在显著偏见，倾向于误判其为认知障碍，尤其在使用公开DementiaBank数据训练模型时更严重。南约克口音群体更容易被误分类为认知障碍更严重。

Conclusion: 目前主流AI认知障碍检测工具对于多语少数族裔群体不够可靠，易产生诊断偏差，不能直接用于这些群体的临床诊断。未来需研发更具泛化性、偏见更小的新模型。

Abstract: Conversational speech often reveals early signs of cognitive decline, such as dementia and MCI. In the UK, one in four people belongs to an ethnic minority, and dementia prevalence is expected to rise most rapidly among Black and Asian communities. This study examines the trustworthiness of AI models, specifically the presence of bias, in detecting healthy multilingual English speakers among the cognitively impaired cohort, to make these tools clinically beneficial. For experiments, monolingual participants were recruited nationally (UK), and multilingual speakers were enrolled from four community centres in Sheffield and Bradford. In addition to a non-native English accent, multilinguals spoke Somali, Chinese, or South Asian languages, who were further divided into two Yorkshire accents (West and South) to challenge the efficiency of the AI tools thoroughly. Although ASR systems showed no significant bias across groups, classification and regression models using acoustic and linguistic features exhibited bias against multilingual speakers, particularly in memory, fluency, and reading tasks. This bias was more pronounced when models were trained on the publicly available DementiaBank dataset. Moreover, multilinguals were more likely to be misclassified as having cognitive decline. This study is the first of its kind to discover that, despite their strong overall performance, current AI models show bias against multilingual individuals from ethnic minority backgrounds in the UK, and they are also more likely to misclassify speakers with a certain accent (South Yorkshire) as living with a more severe cognitive decline. In this pilot study, we conclude that the existing AI tools are therefore not yet reliable for diagnostic use in these populations, and we aim to address this in future work by developing more generalisable, bias-mitigated models.

</details>


### [100] [TraceBack: Multi-Agent Decomposition for Fine-Grained Table Attribution](https://arxiv.org/abs/2602.13059)
*Tejas Anvekar,Junha Park,Rajat Jha,Devanshu Gupta,Poojah Ganesan,Puneeth Mathur,Vivek Gupta*

Main category: cs.CL

TL;DR: 本文提出TraceBack框架，实现了可扩展的、细粒度的单表问答归因能力，并发布了新的评测数据集和无参考度量方法。


<details>
  <summary>Details</summary>
Motivation: 目前的表格问答系统很少能对答案提供细粒度的出处说明，导致即便答案正确也难以获得信任。本研究旨在提升表格问答透明度，特别是在高风险应用场景中增强结果的可验证性。

Method: 提出TraceBack多智能体模块化框架，包括表格行列裁剪、问题分解为语义连贯子问题，以及将每个答案与具体表格单元格对齐，能够捕捉推理过程中直接和间接的证据。推出CITEBench评测基准，并提出FairScore无参考指标，用于自动评估归因的准确性和召回率。

Result: TraceBack在多个数据集和不同细粒度下均显著优于现有强基线；FairScore指标高度符合人工评判，能够有效反映不同方法的表现排序。

Conclusion: TraceBack框架实现了可解释、可扩展的表格问答系统归因能力，并通过新数据集和自动评测指标促进该领域可持续发展。

Abstract: Question answering (QA) over structured tables requires not only accurate answers but also transparency about which cells support them. Existing table QA systems rarely provide fine-grained attribution, so even correct answers often lack verifiable grounding, limiting trust in high-stakes settings. We address this with TraceBack, a modular multi-agent framework for scalable, cell-level attribution in single-table QA. TraceBack prunes tables to relevant rows and columns, decomposes questions into semantically coherent sub-questions, and aligns each answer span with its supporting cells, capturing both explicit and implicit evidence used in intermediate reasoning steps. To enable systematic evaluation, we release CITEBench, a benchmark with phrase-to-cell annotations drawn from ToTTo, FetaQA, and AITQA. We further propose FairScore, a reference-less metric that compares atomic facts derived from predicted cells and answers to estimate attribution precision and recall without human cell labels. Experiments show that TraceBack substantially outperforms strong baselines across datasets and granularities, while FairScore closely tracks human judgments and preserves relative method rankings, supporting interpretable and scalable evaluation of table-based QA.

</details>


### [101] [Exploring a New Competency Modeling Process with Large Language Models](https://arxiv.org/abs/2602.13084)
*Silin Du,Manqing Xin,Raymond Jia Wang*

Main category: cs.CL

TL;DR: 本研究提出了一种基于大语言模型（LLMs）的胜任力建模新流程，实现了胜任力建模自动化、结构化和可评估。


<details>
  <summary>Details</summary>
Motivation: 传统胜任力建模方法高度依赖专家对访谈文本的大量手工分析，存在成本高、随机性强、歧义大、可复现性低等问题。亟需更高效、可验证、透明的数据驱动解决方案。

Method: 通过将专家分析过程分解为结构化的计算模块，利用大语言模型从原始文本中抽取行为和心理描述，并通过嵌入相似性映射到既有胜任力库；引入可学习参数自适应整合多源信息。提出离线评估程序，无需大规模新增数据即可系统性模型选择。

Result: 在软件外包公司真实落地，验证方法具有很强的预测效度、跨库一致性和结构稳健性。

Conclusion: 本框架推动胜任力建模从定性、主观转向透明、数据驱动并可重复验证，实现自动化与科学化管理人才流程。

Abstract: Competency modeling is widely used in human resource management to select, develop, and evaluate talent. However, traditional expert-driven approaches rely heavily on manual analysis of large volumes of interview transcripts, making them costly and prone to randomness, ambiguity, and limited reproducibility. This study proposes a new competency modeling process built on large language models (LLMs). Instead of merely automating isolated steps, we reconstruct the workflow by decomposing expert practices into structured computational components. Specifically, we leverage LLMs to extract behavioral and psychological descriptions from raw textual data and map them to predefined competency libraries through embedding-based similarity. We further introduce a learnable parameter that adaptively integrates different information sources, enabling the model to determine the relative importance of behavioral and psychological signals. To address the long-standing challenge of validation, we develop an offline evaluation procedure that allows systematic model selection without requiring additional large-scale data collection. Empirical results from a real-world implementation in a software outsourcing company demonstrate strong predictive validity, cross-library consistency, and structural robustness. Overall, our framework transforms competency modeling from a largely qualitative and expert-dependent practice into a transparent, data-driven, and evaluable analytical process.

</details>


### [102] [Towards interpretable models for language proficiency assessment: Predicting the CEFR level of Estonian learner texts](https://arxiv.org/abs/2602.13102)
*Kais Allkivi*

Main category: cs.CL

TL;DR: 本研究通过NLP分析爱沙尼亚语学习者写作，利用精选语言特征训练自动分级模型，取得高准确度，并提升了模型可解释性与一致性，结果已应用于开源学习平台。


<details>
  <summary>Details</summary>
Motivation: 虽然NLP在自动评测和反馈工具开发中作用突出，但针对学习者真实语料的综合分析较少，尤其是兼顾自动化分级和二语写作能力发展两方面的研究尚不充分。作者希望通过更稳健和可解释的特征选择，提升自动分级模型的适用性和科学价值。

Method: 作者收集了大量爱沙尼亚语能力考试（A2-C1级别）写作样本，分析测试数据中反映语言复杂性和正确性的词汇、形态、表层和错误特征，筛选出与能力进步强相关的特征，用于训练多种机器学习分类模型，并与含更多特征的模型进行效果对比。模型在新旧考试数据上的泛化能力也被特别评估。

Result: 精选特征模型与包含全部特征模型测试准确率相近（约0.9），但减少了不同文体类型的分级波动。在对较早样本的测试中，尽管写作复杂性有所提升，部分特征集准确率仍能达到0.8。所用方法已在爱沙尼亚语开源学习环境中上线应用。

Conclusion: 通过精选解释性更强的语言特征，可构建准确、稳健且可泛化的机器学习写作分级工具，有助于二语能力发展的深入研究，并推动自动化语言评测技术落地实际教学。

Abstract: Using NLP to analyze authentic learner language helps to build automated assessment and feedback tools. It also offers new and extensive insights into the development of second language production. However, there is a lack of research explicitly combining these aspects. This study aimed to classify Estonian proficiency examination writings (levels A2-C1), assuming that careful feature selection can lead to more explainable and generalizable machine learning models for language testing. Various linguistic properties of the training data were analyzed to identify relevant proficiency predictors associated with increasing complexity and correctness, rather than the writing task. Such lexical, morphological, surface, and error features were used to train classification models, which were compared to models that also allowed for other features. The pre-selected features yielded a similar test accuracy but reduced variation in the classification of different text types. The best classifiers achieved an accuracy of around 0.9. Additional evaluation on an earlier exam sample revealed that the writings have become more complex over a 7-10-year period, while accuracy still reached 0.8 with some feature sets. The results have been implemented in the writing evaluation module of an Estonian open-source language learning environment.

</details>


### [103] [SCOPE: Selective Conformal Optimized Pairwise LLM Judging](https://arxiv.org/abs/2602.13110)
*Sher Badshah,Ali Emami,Hassan Sajjad*

Main category: cs.CL

TL;DR: 本文提出了SCOPE框架，通过结合Bidirectional Preference Entropy（BPE），提升大语言模型（LLMs）作为自动评判工具时的不确定度评估，从而实现在限定风险水平下，准确且覆盖率高的成对偏好判断。


<details>
  <summary>Details</summary>
Motivation: 现有用LLM替代人工进行成对偏好评判的方法成本低，但容易出现评判失准和系统性偏见。因此，亟需一种方法能在给定风险约束下，提升可靠性与判决覆盖范围。

Method: 提出SCOPE（Selective Conformal Optimized Pairwise Evaluation）框架，通过设定接受阈值，使未弃权判断的错误率不超过指定上限α。引入BPE方法，利用LLM对两个答案位置均做判别，获得偏好概率后求熵作为不确定度指标，消除了答案顺序偏见，并提供更优判断置信度参考信号。

Result: BPE不确定度显著优于传统置信度代理，SCOPE 在多个基准（MT-Bench, RewardBench, Chatbot Arena）与不同模型规模下都能可靠满足指定风险水平（例如α=0.10时，经验风险约0.097-0.099），同时覆盖率高，且相比传统基线可做出更多有效判断。

Conclusion: SCOPE结合BPE为LLM评判引入稳健不确定度评估，能确保风险受控下保持高准确率与良好覆盖，可广泛用于替代人工、大规模自动化偏好评审。

Abstract: Large language models (LLMs) are increasingly used as judges to replace costly human preference labels in pairwise evaluation. Despite their practicality, LLM judges remain prone to miscalibration and systematic biases. This paper proposes SCOPE (Selective Conformal Optimized Pairwise Evaluation), a framework for selective pairwise judging with finite-sample statistical guarantees. Under exchangeability, SCOPE calibrates an acceptance threshold such that the error rate among non-abstained judgments is at most a user-specified level $α$. To provide SCOPE with a bias-neutral uncertainty signal, we introduce Bidirectional Preference Entropy (BPE), which queries the judge under both response positions, aggregates the implied preference probabilities to enforce invariance to response order, and converts the aggregated probability into an entropy-based uncertainty score. Across MT-Bench, RewardBench, and Chatbot Arena, BPE improves uncertainty quality over standard confidence proxies, providing a stronger selection signal that enables SCOPE to consistently meet the target risk level while retaining good coverage across judge scales. In particular, at $α= 0.10$, \textsc{Scope} consistently satisfies the risk bound across all benchmarks and judge scales (empirical risk $\approx 0.097$ to $0.099$), while retaining substantial coverage, reaching $0.89$ on RewardBench with Qwen-14B and $0.98$ on RewardBench with Qwen-32B. Compared to naïve baselines, \textsc{Scope} accepts up to $2.4\times$ more judgments on MT-Bench with Qwen-7B under the same target risk constraint, demonstrating that BPE enables reliable and high-coverage LLM-based evaluation.

</details>


### [104] [From sunblock to softblock: Analyzing the correlates of neology in published writing and on social media](https://arxiv.org/abs/2602.13123)
*Maria Ryskina,Matthew R. Gormley,Kyle Mahowald,David R. Mortensen,Taylor Berg-Kirkpatrick,Vivek Kulkarni*

Main category: cs.CL

TL;DR: 本文探讨语言在不同情境下（如新闻与社交媒体）新词产生机制的异同，并将原有英文新词涌现的分布语义分析方法扩展到Twitter语料及上下文嵌入，发现两种文本类型中新词产生的影响因素类似，但社交媒体中主题流行度的作用略小。


<details>
  <summary>Details</summary>
Motivation: 不同语言环境下新词产生存在差异，过往研究主要基于出版物，鲜有关注社交媒体等新型媒介。作者希望通过比较两种场景，补足当前理论空白。

Method: 将以往在出版物上分析英文新词涌现的分布语义方法，扩展到加入上下文嵌入的建模；并将该方法应用于Twitter推文数据，考察新词形成因素的适用性及差异。

Result: 两种文本域中新词产生的影响因素类似，但在Twitter上，话题流行度对新词生成作用较小。

Conclusion: 虽然新闻出版物和社交媒体在新词形成动机上存在一致性，但二者偏好不同的新词产生机制，社交媒体对新词生成的依赖要素存在差异。

Abstract: Living languages are shaped by a host of conflicting internal and external evolutionary pressures. While some of these pressures are universal across languages and cultures, others differ depending on the social and conversational context: language use in newspapers is subject to very different constraints than language use on social media. Prior distributional semantic work on English word emergence (neology) identified two factors correlated with creation of new words by analyzing a corpus consisting primarily of historical published texts (Ryskina et al., 2020, arXiv:2001.07740). Extending this methodology to contextual embeddings in addition to static ones and applying it to a new corpus of Twitter posts, we show that the same findings hold for both domains, though the topic popularity growth factor may contribute less to neology on Twitter than in published writing. We hypothesize that this difference can be explained by the two domains favouring different neologism formation mechanisms.

</details>


### [105] [OpenLID-v3: Improving the Precision of Closely Related Language Identification -- An Experience Report](https://arxiv.org/abs/2602.13139)
*Mariia Fedorova,Nikolay Arefyev,Maja Buljan,Jindřich Helcl,Stephan Oepen,Egil Rønningstad,Yves Scherrer*

Main category: cs.CL

TL;DR: 该论文介绍了OpenLID-v3，一种改进的语言识别工具，旨在提升低资源和相近语言之间的区分能力，同时能识别噪音数据，对多语数据集的质量构建有重要意义。


<details>
  <summary>Details</summary>
Motivation: 目前的LID工具在区分相近语言及噪音检测方面存在局限，尤其影响低资源语言的数据质量。因此，研究者希望通过改进LID工具，提高多语种数据集的纯度与代表性。

Method: 在现有OpenLID基础上，扩充了训练数据，合并了难以区分的语言变体类别，并新增了用于标记非自然语言噪音的标签。聚焦三组难以区分的相近语言，构建了新的评测数据集，通过与GlotLID对比验证性能，并尝试了集成方法。

Result: 实验发现，OpenLID-v3在多组细粒度语言区分和噪音检测上性能优于现有LID工具，集成方法提升了精度，但对低资源语言的覆盖率有所下降。

Conclusion: OpenLID-v3提升了多语识别的准确性与鲁棒性，对多语言数据集建设尤其是低资源语言具有积极作用，相关工具和数据集已公开发布。

Abstract: Language identification (LID) is an essential step in building high-quality multilingual datasets from web data. Existing LID tools (such as OpenLID or GlotLID) often struggle to identify closely related languages and to distinguish valid natural language from noise, which contaminates language-specific subsets, especially for low-resource languages. In this work we extend the OpenLID classifier by adding more training data, merging problematic language variant clusters, and introducing a special label for marking noise. We call this extended system OpenLID-v3 and evaluate it against GlotLID on multiple benchmarks. During development, we focus on three groups of closely related languages (Bosnian, Croatian, and Serbian; Romance varieties of Northern Italy and Southern France; and Scandinavian languages) and contribute new evaluation datasets where existing ones are inadequate. We find that ensemble approaches improve precision but also substantially reduce coverage for low-resource languages. OpenLID-v3 is available on https://huggingface.co/HPLT/OpenLID-v3.

</details>


### [106] [Semantic Chunking and the Entropy of Natural Language](https://arxiv.org/abs/2602.13194)
*Weishun Zhong,Doron Sivan,Tankut Can,Mikhail Katkov,Misha Tsodyks*

Main category: cs.CL

TL;DR: 本文提出了一种统计模型，解释了英语文本高度冗余的本质，并能定量捕捉自然语言的多层次语义结构。该模型通过自相似地将文本分割为语义相关块，并可以层级分解语义，模型预测的熵率与实际英语一致，且能解释不同语料库下熵率的变化。


<details>
  <summary>Details</summary>
Motivation: 英语每字符熵率约为1比特，远低于随机文本的5比特，表明文本存在大量冗余。本文动机在于从第一性原理解释这种冗余本质，并建立能量化捕捉自然语言复杂结构的理论模型。

Method: 作者提出了一种自相似分段的统计模型，将文本层级分解为语义相关的块直至单词级，利用该层级结构对文本进行建模与分析，并与现代LLM和公开数据集做数值实验以验证模型有效性。

Result: 模型能定量反映真实文本在多层次语义结构上的特征，预测出的英语熵率与实际测得值一致，并揭示了熵率与语料库语义复杂性的关系，其唯一自由参数能捕获这种变化。

Conclusion: 该模型首次解释了英语文本高冗余的结构性成因，并表明自然语言的熵率会随语义复杂性系统性增加，为理解自然语言统计结构和提升语言模型提供理论基础。

Abstract: The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [107] [LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning](https://arxiv.org/abs/2602.12314)
*Junwoon Lee,Yulun Tian*

Main category: cs.RO

TL;DR: LatentAM是一种在线3D高斯溅射映射框架，用于从RGB-D流数据创建可扩展、适用于开放词汇机器人感知的潜在特征地图。无需特定模型解码器，通过无预训练、模型无关的字典学习方法实现高效建图，并在多个数据集上达到了优于现有方法的效果和接近实时的速度。


<details>
  <summary>Details</summary>
Motivation: 现有3D语义建图方法通常依赖于对大模型输出特征向量的蒸馏和解码，受限于模型类型与需要预训练，难以做到模型的灵活切换和实时性。为提升机器人对复杂环境理解的灵活性与效率，需要开发支持任意视觉-语言模型（VLM）、无须预训练且可扩展的实时建图方案。

Method: LatentAM提出了一种在线字典学习方法，每个高斯原语关联一个精简查询向量，通过可学习字典和注意力机制在线近似VLM嵌入。字典由流式观测数据高效初始化，并在信任域正则化下动态优化以适应场景语义变化。为支持大规模环境和长轨迹，采用了基于体素哈希的高效地图管理，活跃局部地图放在GPU上优化，全球地图存储在CPU并索引，以节省显存。

Result: 在公开基准测试和自建大规模数据集上，LatentAM在特征重建保真度上明显优于最先进方法，并可在12-35FPS下接近实时运行，兼具精度与效率。

Conclusion: LatentAM实现了无模型依赖、可扩展和高保真度的3D语义建图，提升了机器人在开放词汇环境下的感知能力，为多模型融合和实时大规模建图提供了新路径。

Abstract: We present LatentAM, an online 3D Gaussian Splatting (3DGS) mapping framework that builds scalable latent feature maps from streaming RGB-D observations for open-vocabulary robotic perception. Instead of distilling high-dimensional Vision-Language Model (VLM) embeddings using model-specific decoders, LatentAM proposes an online dictionary learning approach that is both model-agnostic and pretraining-free, enabling plug-and-play integration with different VLMs at test time. Specifically, our approach associates each Gaussian primitive with a compact query vector that can be converted into approximate VLM embeddings using an attention mechanism with a learnable dictionary. The dictionary is initialized efficiently from streaming observations and optimized online to adapt to evolving scene semantics under trust-region regularization. To scale to long trajectories and large environments, we further propose an efficient map management strategy based on voxel hashing, where optimization is restricted to an active local map on the GPU, while the global map is stored and indexed on the CPU to maintain bounded GPU memory usage. Experiments on public benchmarks and a large-scale custom dataset demonstrate that LatentAM attains significantly better feature reconstruction fidelity compared to state-of-the-art methods, while achieving near-real-time speed (12-35 FPS) on the evaluated datasets. Our project page is at: https://junwoonlee.github.io/projects/LatentAM

</details>


### [108] [ForeAct: Steering Your VLA with Efficient Visual Foresight Planning](https://arxiv.org/abs/2602.12322)
*Zhuoyang Zhang,Shang Yang,Qinghao Hu,Luke J. Huang,James Hou,Yufei Sun,Yao Lu,Song Han*

Main category: cs.RO

TL;DR: 本论文提出了Visual Foresight Planning (ForeAct)，一种可以通过未来观测与子任务描述，分步引导视觉-语言-动作（VLA）模型执行任务的高效通用规划器。在开放世界多步任务中，实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 在开放世界中将高级语言指令转化为具体执行动作极具挑战。现有VLA模型在高层次语义推理和实际视觉-动作执行之间存在瓶颈，导致泛化性和准确率有限。作者希望通过“想象的未来观测”引导VLA关注低层次感知-运动推理，从而提升系统表现。

Method: 提出了ForeAct规划器，包括一个高效的前瞻图像生成器，能在0.33秒内基于当前视觉输入和语言描述生成高质量的未来观测图像（640x480），以及一个视觉-语言推理模块，生成子任务描述并辅助VLA决策。该生成器通过100万多任务、跨形态的仿真预训练，具备稳健的动力学建模能力。其设计无需对现有VLA结构做出改动。

Result: 在包含11种真实多步任务的基准测试中，ForeAct整体平均成功率达87.4%，相比原始VLA（46.5%）和增加文本子任务提示后的VLA（57.1%）分别提升了40.9%和30.3%。

Conclusion: ForeAct有效提升了VLA模型在开放世界复杂任务中的表现，能够无缝集成到现有架构中。利用前瞻观测带来的加强感知-动作推理能力，使系统泛化性和准确率大幅提高。

Abstract: Vision-Language-Action (VLA) models convert high-level language instructions into concrete, executable actions, a task that is especially challenging in open-world environments. We present Visual Foresight Planning (ForeAct), a general and efficient planner that guides a VLA step-by-step using imagined future observations and subtask descriptions. With an imagined future observation, the VLA can focus on visuo-motor inference rather than high-level semantic reasoning, leading to improved accuracy and generalization. Our planner comprises a highly efficient foresight image generation module that predicts a high-quality 640$\times$480 future observation from the current visual input and language instruction within only 0.33s on an H100 GPU, together with a vision-language model that reasons over the task and produces subtask descriptions for both the generator and the VLA. Importantly, state-of-the-art VLAs can integrate our planner seamlessly by simply augmenting their visual inputs, without any architectural modification. The foresight generator is pretrained on over 1 million multi-task, cross-embodiment episodes, enabling it to learn robust embodied dynamics. We evaluate our framework on a benchmark that consists of 11 diverse, multi-step real-world tasks. It achieves an average success rate of 87.4%, demonstrating a +40.9% absolute improvement over the $π_0$ baseline (46.5%) and a +30.3% absolute improvement over $π_0$ augmented with textual subtask guidance (57.1%).

</details>


### [109] [Schur-MI: Fast Mutual Information for Robotic Information Gathering](https://arxiv.org/abs/2602.12346)
*Kalvik Jakkala,Jason O'Kane,Srinivas Akella*

Main category: cs.RO

TL;DR: 该论文提出了Schur-MI方法，大幅提升了基于高斯过程的互信息（MI）目标在机器人信息收集中的实时计算效率，从而更好地支持传感器布置与信息路径规划，实验和实地测试均证实其实用性。


<details>
  <summary>Details</summary>
Motivation: 互信息（MI）因其良好的理论性质，被广泛用于机器人传感器布置和路径规划，但其高昂的计算代价严重限制了其在在线规划中的应用。作者希望突破这一瓶颈，让信息论目标真正适用于实时机器人探索。

Method: 作者提出Schur-MI方法：一方面利用机器人信息收集任务的迭代结构，预计算并跨步复用部分昂贵的中间量；另一方面采用Schur补分解，避免计算大规模行列式。这样显著降低了MI每次评估的计算复杂度。

Result: Schur-MI将MI的单次计算复杂度从O(|V|^3)降至O(|A|^3)，其中|V|为候选点数，|A|为已选点数。在真实世界的水深数据集实验中，比标准方法快12.7倍。无人艇实地自适应路径规划试验证明该方法具有实际应用价值。

Conclusion: Schur-MI极大加速了MI目标的计算，使得信息论目标在机器人实时规划中成为实际可行的选择，有效推动了智能感知与自主探索的发展。

Abstract: Mutual information (MI) is a principled and widely used objective for robotic information gathering (RIG), providing strong theoretical guarantees for sensor placement (SP) and informative path planning (IPP). However, its high computational cost, dominated by repeated log-determinant evaluations, has limited its use in real-time planning. This letter presents Schur-MI, a Gaussian process (GP) MI formulation that (i) leverages the iterative structure of RIG to precompute and reuse expensive intermediate quantities across planning steps, and (ii) uses a Schur-complement factorization to avoid large determinant computations. Together, these methods reduce the per-evaluation cost of MI from $\mathcal{O}(|\mathcal{V}|^3)$ to $\mathcal{O}(|\mathcal{A}|^3)$, where $\mathcal{V}$ and $\mathcal{A}$ denote the candidate and selected sensing locations, respectively. Experiments on real-world bathymetry datasets show that Schur-MI achieves up to a $12.7\times$ speedup over the standard MI formulation. Field trials with an autonomous surface vehicle (ASV) performing adaptive IPP further validate its practicality. By making MI computation tractable for online planning, Schur-MI helps bridge the gap between information-theoretic objectives and real-time robotic exploration.

</details>


### [110] [LongNav-R1: Horizon-Adaptive Multi-Turn RL for Long-Horizon VLA Navigation](https://arxiv.org/abs/2602.12351)
*Yue Hu,Avery Xi,Qixin Xiao,Seth Isaacson,Henry X. Liu,Ram Vasudevan,Maani Ghaffari*

Main category: cs.RO

TL;DR: 本论文提出LongNav-R1，一种端到端多轮强化学习框架，用于优化视觉-语言-动作（VLA）模型在长时序导航任务中的表现，通过多轮对话式决策和地平线自适应优化显著提升导航成功率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有导航模型大多基于单轮决策，难以有效利用历史交互信息，且依赖人工演示易导致行为僵化，缺乏多样性。长时序任务还面临奖励归因难的问题。因此，亟需面向多轮、多时序的高效训练方法，以提升模型鲁棒性和泛化性。

Method: 作者将导航决策过程重构为VLA策略与环境间的多轮连续对话，采用多轮强化学习使模型能自我推理历史与未来结果。同时，引入地平线自适应策略优化（Horizon-Adaptive Policy Optimization），在优势估算时显式考虑路径长度，实现更准确的时序奖励归因，促进多样化的行为策略生成。

Result: 在导航基准实验中，LongNav-R1通过4000条在线采集轨迹显著提升基线模型Qwen3-VL-2B的成功率（从64.3%提升至73.0%），同时优于现有最先进方法，展示了更高的样本效率与鲁棒性。此外模型在零样本长时序真实导航任务中同样表现优秀，验证了泛化能力。

Conclusion: LongNav-R1 框架通过多轮强化学习和地平线自适应优化，有效增强了视觉-语言-动作模型在长时序导航任务中的表现，兼具高效率、高多样性和强泛化性，具备实际应用与拓展价值。

Abstract: This paper develops LongNav-R1, an end-to-end multi-turn reinforcement learning (RL) framework designed to optimize Visual-Language-Action (VLA) models for long-horizon navigation. Unlike existing single-turn paradigm, LongNav-R1 reformulates the navigation decision process as a continuous multi-turn conversation between the VLA policy and the embodied environment. This multi-turn RL framework offers two distinct advantages: i) it enables the agent to reason about the causal effects of historical interactions and sequential future outcomes; and ii) it allows the model to learn directly from online interactions, fostering diverse trajectory generation and avoiding the behavioral rigidity often imposed by human demonstrations. Furthermore, we introduce Horizon-Adaptive Policy Optimization. This mechanism explicitly accounts for varying horizon lengths during advantage estimation, facilitating accurate temporal credit assignment over extended sequences. Consequently, the agent develops diverse navigation behaviors and resists collapse during long-horizon tasks. Experiments on object navigation benchmarks validate the framework's efficacy: With 4,000 rollout trajectories, LongNav-R1 boosts the Qwen3-VL-2B success rate from 64.3% to 73.0%. These results demonstrate superior sample efficiency and significantly outperform state-of-the-art methods. The model's generalizability and robustness are further validated by its zero-shot performance in long-horizon real-world navigation settings. All source code will be open-sourced upon publication.

</details>


### [111] [Predicting Dynamic Map States from Limited Field-of-View Sensor Data](https://arxiv.org/abs/2602.12360)
*Knut Peterson,David Han*

Main category: cs.RO

TL;DR: 本文提出了一种基于深度学习的方法，仅利用有限视野(FOV)下的时序传感器数据，对环境动态地图状态进行高精度预测，并验证了该方法在多种不同的传感场景下的有效性。


<details>
  <summary>Details</summary>
Motivation: 实际部署的自主系统往往因为设计、遮挡或传感器故障存在视野受限问题。在大视野信息不可用时，为保障安全和准确性，亟需根据有限观测数据推测和预测周边环境状态。

Method: 作者将有限FOV下的动态传感器时序数据重新编码为单幅融合空间与时间信息的图像格式，利用现有丰富的图像到图像深度学习模型进行环境状态预测。该方法无需专门设计新网络结构，能直接借用成熟的图像预测框架。

Result: 实验证明，该方法在多种有限视野、不同类型传感数据场景下均能准确预测周边地图状态，预测精度高，适应性强。

Conclusion: 基于有限FOV的时序数据，通过图像化融合和深度学习方法能够实现高准确率地图状态预测，为视野受限场景下自主系统提供了实用的环境感知解决方案。

Abstract: When autonomous systems are deployed in real-world scenarios, sensors are often subject to limited field-of-view (FOV) constraints, either naturally through system design, or through unexpected occlusions or sensor failures. In conditions where a large FOV is unavailable, it is important to be able to infer information about the environment and predict the state of nearby surroundings based on available data to maintain safe and accurate operation. In this work, we explore the effectiveness of deep learning for dynamic map state prediction based on limited FOV time series data. We show that by representing dynamic sensor data in a simple single-image format that captures both spatial and temporal information, we can effectively use a wide variety of existing image-to-image learning models to predict map states with high accuracy in a diverse set of sensing scenarios.

</details>


### [112] [Zero-Shot Adaptation to Robot Structural Damage via Natural Language-Informed Kinodynamics Modeling](https://arxiv.org/abs/2602.12385)
*Anuj Pokhrel,Aniket Datar,Mohammad Nazeri,Francesco Cancelliere,Xuesu Xiao*

Main category: cs.RO

TL;DR: 该论文提出了一种基于自然语言和自监督学习的新方法，用于提升自动驾驶机器人在不同结构损伤情况下的动力学建模与自适应能力。


<details>
  <summary>Details</summary>
Motivation: 高性能自动驾驶机器人在野外操作时易受到机械损伤，导致动力学行为发生变化。结构损伤多样且难以量化，现有方法难以准确根据不同损伤调整动力学模型。作者认为可用自然语言描述多样化的损伤信息，从而为动力学建模提供帮助。

Method: 提出了Zero-shot Language Informed Kinodynamics (ZLIK)方法，通过自监督学习，将关于损伤的自然语言描述作为语义信息，与动力学行为相结合，学习面向损伤的前向动力学模型。通过高精度物理仿真平台BeamNG.tech，采集了多种损伤情况下的数据，进行训练与测试。

Result: 所提出方法能够实现对不同损伤的零样本自适应，动力学误差最高可降低81%。此外，该模型能在仿真到现实、以及不同缩放比例（1/10）之间实现泛化。

Conclusion: 利用自然语言表征损伤信息结合自监督学习能够极大提升机器人对结构损伤的适应性和动力学预测能力，为复杂环境下的自动驾驶系统带来更强的鲁棒性和通用性。

Abstract: High-performance autonomous mobile robots endure significant mechanical stress during in-the-wild operations, e.g., driving at high speeds or over rugged terrain. Although these platforms are engineered to withstand such conditions, mechanical degradation is inevitable. Structural damage manifests as consistent and notable changes in kinodynamic behavior compared to a healthy vehicle. Given the heterogeneous nature of structural failures, quantifying various damages to inform kinodynamics is challenging. We posit that natural language can describe and thus capture this variety of damages. Therefore, we propose Zero-shot Language Informed Kinodynamics (ZLIK), which employs self-supervised learning to ground semantic information of damage descriptions in kinodynamic behaviors to learn a forward kinodynamics model in a data-driven manner. Using the high-fidelity soft-body physics simulator BeamNG.tech, we collect data from a variety of structurally compromised vehicles. Our learned model achieves zero-shot adaptation to different damages with up to 81% reduction in kinodynamics error and generalizes across the sim-to-real and full-to-1/10$^{\text{th}}$ scale gaps.

</details>


### [113] [Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning](https://arxiv.org/abs/2602.12405)
*Carl Qi,Xiaojie Wang,Silong Yong,Stephen Sheng,Huitan Mao,Sriram Srinivasan,Manikantan Nambi,Amy Zhang,Yesh Dattatreya*

Main category: cs.RO

TL;DR: 本文提出了一种新的多任务自精炼模型ARMOR，用于机器人故障检测和推理，能在异构监督、多任务、自适应回合推理下实现高性能故障识别和解释。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人故障识别方法局限于封闭集分类或高度依赖人工标注，而现实中的故障复杂且难以枚举，获取丰富注释代价高昂。因此需要一种更高效、泛化能力强的推理方法。

Method: 提出ARMOR模型，将故障检测与推理建模为多任务自我精炼过程，模型会根据历史输出迭代进行检测和自然语言推理预测。训练过程中利用异构监督（大规模稀疏二值标签+小规模丰富推理标注），结合离线和在线模仿学习进行优化。推理时，ARMOR会生成多条自精炼轨迹，并利用自置信度指标选出最优预测。

Result: 在多种环境下实验表明，ARMOR相比以往方法，故障检测率提高最高达30%，推理能力（用LLM模糊匹配分数衡量）提升最高可达100%，表现国家领先，并能稳健适应异构监督和开放式推理。

Conclusion: ARMOR能有效结合不同类型监督信息，实现对复杂和开放式机器人故障的检测和自然语言推理，为构建可靠、可解释的机器人系统提供了有力工具。

Abstract: Reasoning about failures is crucial for building reliable and trustworthy robotic systems. Prior approaches either treat failure reasoning as a closed-set classification problem or assume access to ample human annotations. Failures in the real world are typically subtle, combinatorial, and difficult to enumerate, whereas rich reasoning labels are expensive to acquire. We address this problem by introducing ARMOR: Adaptive Round-based Multi-task mOdel for Robotic failure detection and reasoning. We formulate detection and reasoning as a multi-task self-refinement process, where the model iteratively predicts detection outcomes and natural language reasoning conditioned on past outputs. During training, ARMOR learns from heterogeneous supervision - large-scale sparse binary labels and small-scale rich reasoning annotations - optimized via a combination of offline and online imitation learning. At inference time, ARMOR generates multiple refinement trajectories and selects the most confident prediction via a self-certainty metric. Experiments across diverse environments show that ARMOR achieves state-of-the-art performance by improving over the previous approaches by up to 30% on failure detection rate and up to 100% in reasoning measured through LLM fuzzy match score, demonstrating robustness to heterogeneous supervision and open-ended reasoning beyond predefined failure modes. We provide dditional visualizations on our website: https://sites.google.com/utexas.edu/armor

</details>


### [114] [MiDAS: A Multimodal Data Acquisition System and Dataset for Robot-Assisted Minimally Invasive Surgery](https://arxiv.org/abs/2602.12407)
*Keshara Weerasinghe,Seyed Hamid Reza Roodabeh,Andrew Hawkins,Zhaomeng Zhang,Zachary Schrader,Homa Alemzadeh*

Main category: cs.RO

TL;DR: 该论文提出了MiDAS系统，实现了无需机器人专有接口即可在多种手术机器人平台上同步采集多模态数据，并通过实验验证了其实用性和有效性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人辅助手术研究对多模态数据依赖性增加，但由于机器人遥测数据普遍为专有，限制了数据获取和研究进展。因此亟需一种开放、通用且平台无关的数据采集系统。

Method: 提出了MiDAS系统，能够整合电磁和RGB-D手部跟踪、脚踏板感应及手术视频采集，无需机器人专有接口。通过Raven-II开源平台和临床da Vinci Xi平台，进行实际操作任务数据采集，并对手脚传感数据与机器人运动学信号进行相关性分析及手势识别实验。

Result: 外部手部和脚踏板传感器获取的数据与机器人内部运动学数据高度相关，非侵入式运动信号的数据在手势识别等下游任务中的性能可媲美专有遥测数据。

Conclusion: MiDAS系统实现了多模态RMIS数据的可重复采集，并伴随注释数据集公开发布，包括首个高保真模拟下腹疝修补的多模态数据集。

Abstract: Background: Robot-assisted minimally invasive surgery (RMIS) research increasingly relies on multimodal data, yet access to proprietary robot telemetry remains a major barrier. We introduce MiDAS, an open-source, platform-agnostic system enabling time-synchronized, non-invasive multimodal data acquisition across surgical robotic platforms.
  Methods: MiDAS integrates electromagnetic and RGB-D hand tracking, foot pedal sensing, and surgical video capturing without requiring proprietary robot interfaces. We validated MiDAS on the open-source Raven-II and the clinical da Vinci Xi by collecting multimodal datasets of peg transfer and hernia repair suturing tasks performed by surgical residents. Correlation analysis and downstream gesture recognition experiments were conducted.
  Results: External hand and foot sensing closely approximated internal robot kinematics and non-invasive motion signals achieved gesture recognition performance comparable to proprietary telemetry.
  Conclusion: MiDAS enables reproducible multimodal RMIS data collection and is released with annotated datasets, including the first multimodal dataset capturing hernia repair suturing on high-fidelity simulation models.

</details>


### [115] [Control Barrier Functions with Audio Risk Awareness for Robot Safe Navigation on Construction Sites](https://arxiv.org/abs/2602.12416)
*Johannes Mootz,Reza Akhavian*

Main category: cs.RO

TL;DR: 本文提出了一种结合音频感知与控制屏障函数（CBF）的安全过滤器，在建筑工地等动态环境中提高移动机器人避障自主性。通过引入音频风险线索（如风镐声），动态调整安全边界，实验验证了其在复杂环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 常规自主移动机器人仅依赖视觉或激光感知，在建筑工地等动态、遮挡严重的环境中表现不佳，且忽略了丰富且易获得的音频信息。因此，研究如何利用音频作为额外感知输入提升安全性具有实际意义。

Method: 作者设计了一种基于CBF的安全滤波框架，将由信号包络及周期性检测得到的风镐活动音（音频风险线索）作为外部风险输入，动态调整机器人的安全边界。在仿真中，使用了圆形和目标对齐椭圆形两种CBF公式，测试机器人在夹杂障碍物的环境下的导航和避障能力。

Result: 仿真结果显示，通过CBF安全滤波器，可以完全消除所有实验中的安全违规事件。在目标到达率上，椭圆式CBF优于圆形CBF（椭圆76.5%，圆形40.2%），并能更好地避免死锁现象。

Conclusion: 本文首次演示了音频感知与CBF安全控制的有机整合，为自主机器人在动态和高风险环境下实现多模态安全推理提供了新思路，提升了其环境适应性和安全可靠性。

Abstract: Construction automation increasingly requires autonomous mobile robots, yet robust autonomy remains challenging on construction sites. These environments are dynamic and often visually occluded, which complicates perception and navigation. In this context, valuable information from audio sources remains underutilized in most autonomy stacks. This work presents a control barrier function (CBF)-based safety filter that provides safety guarantees for obstacle avoidance while adapting safety margins during navigation using an audio-derived risk cue. The proposed framework augments the CBF with a lightweight, real-time jackhammer detector based on signal envelope and periodicity. Its output serves as an exogenous risk that is directly enforced in the controller by modulating the barrier function. The approach is evaluated in simulation with two CBF formulations (circular and goal-aligned elliptical) with a unicycle robot navigating a cluttered construction environment. Results show that the CBF safety filter eliminates safety violations across all trials while reaching the target in 40.2% (circular) vs. 76.5% (elliptical), as the elliptical formulation better avoids deadlock. This integration of audio perception into a CBF-based controller demonstrates a pathway toward richer multimodal safety reasoning in autonomous robots for safety-critical and dynamic environments.

</details>


### [116] [An Autonomous, End-to-End, Convex-Based Framework for Close-Range Rendezvous Trajectory Design and Guidance with Hardware Testbed Validation](https://arxiv.org/abs/2602.12421)
*Minduli C. Wijayatunga,Julian Guinane,Nathan D. Wallace,Xiaofeng Wu*

Main category: cs.RO

TL;DR: 本文提出了一种新型的自主星载交会服务系统CORTEX，实现了基于深度学习感知与凸优化实时轨迹设计与引导，在仿真与硬件实验中表现出高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前自主卫星交会服务面临高安全性、操作约束、强实时和鲁棒性要求，且需满足星载计算能力有限的条件。针对传感和驱动不确定性，亟需新方法提升轨迹规划与引导的可靠性与实时性。

Method: 提出CORTEX框架，将深度学习感知系统与基于凸优化的轨迹设计及引导相集成，并引入轨迹再生及安全中止逻辑，确保在大偏差（如传感器故障、推进器失效）下仍可恢复。通过高保真仿真和硬件在环实验（包括光学导航、空气轴承平台）对框架适应性和精度进行验证。

Result: 仿真中（含初值不确定、推力误差等极端情况）CORTEX定位终端误差为36.85±44.46 mm（位置），相对速度误差为1.25±2.26 mm/s。空气轴承平台硬件实测18例，其中8例为非理想情况，位置误差8.09±5.29 mm，速度误差2.23±1.72 mm/s。

Conclusion: CORTEX实现了在强不确定条件下高精度、安全、鲁棒的自主卫星近距离交会轨迹引导，适用于轨道服务等高要求应用，具备星载实时应用潜力。

Abstract: Autonomous satellite servicing missions must execute close-range rendezvous under stringent safety and operational constraints while remaining computationally tractable for onboard use and robust to uncertainty in sensing, actuation, and dynamics. This paper presents CORTEX (Convex Optimization for Rendezvous Trajectory Execution), an autonomous, perception-enabled, real-time trajectory design and guidance framework for close-range rendezvous. CORTEX integrates a deep-learning perception pipeline with convex-optimisation-based trajectory design and guidance, including reference regeneration and abort-to-safe-orbit logic to recover from large deviations caused by sensor faults and engine failures.
  CORTEX is validated in high-fidelity software simulation and hardware-in-the-loop experiments. The software pipeline (Basilisk) models high-fidelity relative dynamics, realistic thruster execution, perception, and attitude control. Hardware testing uses (i) an optical navigation testbed to assess perception-to-estimation performance and (ii) a planar air-bearing testbed to evaluate the end-to-end guidance loop under representative actuation and subsystem effects. A Monte-Carlo campaign in simulation includes initial-state uncertainty, thrust-magnitude errors, and missed-thrust events; under the strongest case investigated, CORTEX achieves terminal docking errors of $36.85 \pm 44.46$ mm in relative position and $1.25 \pm 2.26$ mm/s in relative velocity. On the planar air-bearing testbed, 18 cases are executed (10 nominal; 8 off-nominal requiring recomputation and/or abort due to simulated engine failure and sensor malfunctions), yielding terminal errors of $8.09 \pm 5.29$ mm in position and $2.23 \pm 1.72$ mm/s in velocity.

</details>


### [117] [Gradient-Enhanced Partitioned Gaussian Processes for Real-Time Quadrotor Dynamics Modeling](https://arxiv.org/abs/2602.12487)
*Xinhuan Sang,Adam Rozman,Sheryl Grace,Roberto Tron*

Main category: cs.RO

TL;DR: 本文提出了一种结合梯度信息的四旋翼动力学高斯过程（GP）建模方法，利用分区和近似技术，实现了实时的动力学推断，并考虑了空气动力学效应。该方法在保证精度的同时，大幅降低了在线计算量，在普通桌面硬件上超过30Hz实时推断。


<details>
  <summary>Details</summary>
Motivation: 传统基于高斯过程的方法在模拟动力学时提供了不确定性量化，但计算开销大，难以满足实时性要求。此外，为了准确建模四旋翼飞行器的空气动力学效应和复杂环境下的动力学行为，有必要利用中等精度的模拟数据及相关梯度信息来提升建模精度和效率。

Method: 1. 融合梯度信息以提升高斯过程模型精度；2. 引入创新的状态空间分区和近似方法，将数据划分为近邻和远邻子集，并引入舒尔补处理，加速推断过程；3. 结合CHARM中等精度气动求解器仿真，为SUI Endurance四旋翼生成带梯度的数据集，包括力、力矩及噪声测量。

Result: 实验结果表明，该分区带梯度的高斯过程模型在保证较高精度的同时，计算耗时远低于无梯度信息的传统分区GP方法，并可在普通桌面硬件上实时运行（大于30Hz）。

Conclusion: 该方法为复杂和非定常环境下的四旋翼实时气动预测和控制算法提供了高效可靠的建模和推断工具，有望支持高精度实时飞控应用。

Abstract: We present a quadrotor dynamics Gaussian Process (GP) with gradient information that achieves real-time inference via state-space partitioning and approximation, and that includes aerodynamic effects using data from mid-fidelity potential flow simulations. While traditional GP-based approaches provide reliable Bayesian predictions with uncertainty quantification, they are computationally expensive and thus unsuitable for real-time simulations. To address this challenge, we integrate gradient information to improve accuracy and introduce a novel partitioning and approximation strategy to reduce online computational cost. In particular, for the latter, we associate a local GP with each non-overlapping region; by splitting the training data into local near and far subsets, and by using Schur complements, we show that a large part of the matrix inversions required for inference can be performed offline, enabling real-time inference at frequencies above 30 Hz on standard desktop hardware. To generate a training dataset that captures aerodynamic effects, such as rotor-rotor interactions and apparent wind direction, we use the CHARM code, which is a mid-fidelity aerodynamic solver. It is applied to the SUI Endurance quadrotor to predict force and torque, along with noise at three specified locations. The derivative information is obtained via finite differences. Experimental results demonstrate that the proposed partitioned GP with gradient conditioning achieves higher accuracy than standard partitioned GPs without gradient information, while greatly reducing computational time. This framework provides an efficient foundation for real-time aerodynamic prediction and control algorithms in complex and unsteady environments.

</details>


### [118] [Composable Model-Free RL for Navigation with Input-Affine Systems](https://arxiv.org/abs/2602.12492)
*Xinhuan Sang,Abdelrahman Abdelgawad,Roberto Tron*

Main category: cs.RO

TL;DR: 该论文提出了一种集成式无模型强化学习方法，通过对环境中每个元素（如目标或障碍物）单独学习并在线组合实现安全导航，且支持未知连续时间非线性动力学。


<details>
  <summary>Details</summary>
Motivation: 随着自主机器人应用于复杂真实环境，安全实时导航变得重要，但事先预设所有可能行为不可行，因此需要一种能动态适应环境变化的策略学习方法。

Method: 针对未知输入仿射连续时间非线性动力学，作者推导了连续时间下Hamilton-Jacobi-Bellman (HJB) 方程，并证明优势函数对动作和最优策略为二次型。基于此，提出了无模型actor-critic算法，通过梯度下降分别学习静态与动态障碍物的策略与价值函数，并用二次约束二次规划 (QCQP) 在线组合多个reach/avoid模型，从而实现形式化的障碍物规避保证。

Result: 模拟实验表明，所提方法在连续时间安全导航任务上相较于离散时间PPO基线有更好表现，能够有效实现目标到达与碰撞规避。

Conclusion: 本文方法为连续时间未知动力学下的自主导航问题提供了灵活、高效且具有理论保障的无模型解决方案，是现有CLF/CBF控制方法的有力替代。

Abstract: As autonomous robots move into complex, dynamic real-world environments, they must learn to navigate safely in real time, yet anticipating all possible behaviors is infeasible. We propose a composable, model-free reinforcement learning method that learns a value function and an optimal policy for each individual environment element (e.g., goal or obstacle) and composes them online to achieve goal reaching and collision avoidance. Assuming unknown nonlinear dynamics that evolve in continuous time and are input-affine, we derive a continuous-time Hamilton-Jacobi-Bellman (HJB) equation for the value function and show that the corresponding advantage function is quadratic in the action and optimal policy. Based on this structure, we introduce a model-free actor-critic algorithm that learns policies and value functions for static or moving obstacles using gradient descent. We then compose multiple reach/avoid models via a quadratically constrained quadratic program (QCQP), yielding formal obstacle-avoidance guarantees in terms of value-function level sets, providing a model-free alternative to CLF/CBF-based controllers. Simulations demonstrate improved performance over a PPO baseline applied to a discrete-time approximation.

</details>


### [119] [Monocular Reconstruction of Neural Tactile Fields](https://arxiv.org/abs/2602.12508)
*Pavan Mantripragada,Siddhanth Deshmukh,Eadom Dessalene,Manas Desai,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: 本文提出了一种新型三维表示方法“神经触觉场”，能从单张RGB图像预测物体的接触响应，提升机器人在实际环境下的路径规划能力。相比现有方法，三维重建和表面重建精度显著提高。


<details>
  <summary>Details</summary>
Motivation: 传统的三维表示只能感知静态障碍，难以应对环境中的柔性、可变形或可通过区域（如树叶等低阻力区域），导致机器人路径规划不够灵活和高效。因此需要发展能反映接触响应的三维环境感知能力。

Method: 作者提出神经触觉场，将空间位置映射到预期的触觉响应，并首次实现了基于单张RGB图像推断该场。将其与现有路径规划器结合，能引导机器人避开高阻力物体，经过低阻力区域。

Result: 实验表明，该学习框架在体积三维重建上较最优方法提升85.8%，在表面重建上提升26.7%。

Conclusion: 神经触觉场为机器人提供了细致的交互感知能力，推动了机器人在动态和可变形环境中的智能路径规划和环境建模，为未来更复杂任务奠定基础。

Abstract: Robots operating in the real world must plan through environments that deform, yield, and reconfigure under contact, requiring interaction-aware 3D representations that extend beyond static geometric occupancy. To address this, we introduce neural tactile fields, a novel 3D representation that maps spatial locations to the expected tactile response upon contact. Our model predicts these neural tactile fields from a single monocular RGB image -- the first method to do so. When integrated with off-the-shelf path planners, neural tactile fields enable robots to generate paths that avoid high-resistance objects while deliberately routing through low-resistance regions (e.g. foliage), rather than treating all occupied space as equally impassable. Empirically, our learning framework improves volumetric 3D reconstruction by $85.8\%$ and surface reconstruction by $26.7\%$ compared to state-of-the-art monocular 3D reconstruction methods (LRM and Direct3D).

</details>


### [120] [CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning](https://arxiv.org/abs/2602.12532)
*Yike Zhang,Yaonan Wang,Xinxin Sun,Kaizhen Huang,Zhiyuan Xu,Junjie Ji,Zhengping Che,Jian Tang,Jingtao Sun*

Main category: cs.RO

TL;DR: 该论文提出了一种名为CRAFT的力觉感知课程微调框架，用于提升机器人在复杂接触操作中的表现。该方法通过课程学习策略和信息瓶颈调节多模态输入的作用，提高了对力信号的利用，增强了泛化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型难以胜任需要力觉精确调控的富接触操作任务，主要原因是视觉和语言输入熵大信息繁杂，而力觉信号虽熵低但却至关重要，模型容易忽略力觉信号从而造成控制不稳定。

Method: 提出CRAFT框架：1）在训练初期通过变分信息瓶颈调节视觉和语言特征，仅让模型侧重力觉信号，逐步释放多模态信息权重，实现课程式微调；2）设计同构主从遥操作系统，同步采集视觉、语言与力觉数据，用于模型训练。

Result: 在真实机器人实验中，CRAFT方法在多种富接触任务上持续提升任务成功率，能泛化到未见过的新物体和新任务变化，并适应多种VLA模型架构。

Conclusion: CRAFT能够有效融合力觉信息，提升机器人在复杂接触任务中的鲁棒性与泛化能力，为VLA模型应用于高难度操作拓展了能力和范围。

Abstract: Vision-Language-Action (VLA) models have shown a strong capability in enabling robots to execute general instructions, yet they struggle with contact-rich manipulation tasks, where success requires precise alignment, stable contact maintenance, and effective handling of deformable objects. A fundamental challenge arises from the imbalance between high-entropy vision and language inputs and low-entropy but critical force signals, which often leads to over-reliance on perception and unstable control. To address this, we introduce CRAFT, a force-aware curriculum fine-tuning framework that integrates a variational information bottleneck module to regulate vision and language embeddings during early training. This curriculum strategy encourages the model to prioritize force signals initially, before progressively restoring access to the full multimodal information. To enable force-aware learning, we further design a homologous leader-follower teleoperation system that collects synchronized vision, language, and force data across diverse contact-rich tasks. Real-world experiments demonstrate that CRAFT consistently improves task success, generalizes to unseen objects and novel task variations, and adapts effectively across diverse VLA architectures, enabling robust and generalizable contact-rich manipulation.

</details>


### [121] [Eva-Tracker: ESDF-update-free, Visibility-aware Planning with Target Reacquisition for Robust Aerial Tracking](https://arxiv.org/abs/2602.12549)
*Yue Lin,Yang Liu,Dong Wang,Huchuan Lu*

Main category: cs.RO

TL;DR: 本文提出了一种面向可见性的无人机跟踪轨迹规划框架Eva-Tracker，无需频繁更新ESDF，通过预计算和优化提升实时性与稳定性，并公开源码。


<details>
  <summary>Details</summary>
Motivation: ESDF常用于视野评估以防止跟踪过程中遮挡和碰撞，但频繁更新ESDF会带来较大计算负担，影响实时性和跟踪稳定性。

Method: 1. 提出目标轨迹预测和可见性初始路径生成方法，以保持适当观测距离、避免遮挡，并支持目标丢失时快速重新规划；2. 提出视场ESDF（FoV-ESDF），可预先计算、针对跟踪器视野进行快速可见性评估，无需频繁更新；3. 基于可微FoV-ESDF目标进行轨迹优化，保证全程可视。

Result: 实验包括大量模拟和实物验证，表明本方法与现有最先进方法相比，在减少计算的同时，显著提升跟踪鲁棒性。

Conclusion: Eva-Tracker有效提升了跟踪系统的效率与效果，为空中跟踪任务提供了一种无需频繁ESDF更新的高性能解决方案。

Abstract: The Euclidean Signed Distance Field (ESDF) is widely used in visibility evaluation to prevent occlusions and collisions during tracking. However, frequent ESDF updates introduce considerable computational overhead. To address this issue, we propose Eva-Tracker, a visibility-aware trajectory planning framework for aerial tracking that eliminates ESDF updates and incorporates a recovery-capable path generation method for target reacquisition. First, we design a target trajectory prediction method and a visibility-aware initial path generation algorithm that maintain an appropriate observation distance, avoid occlusions, and enable rapid replanning to reacquire the target when it is lost. Then, we propose the Field of View ESDF (FoV-ESDF), a precomputed ESDF tailored to the tracker's field of view, enabling rapid visibility evaluation without requiring updates. Finally, we optimize the trajectory using differentiable FoV-ESDF-based objectives to ensure continuous visibility throughout the tracking process. Extensive simulations and real-world experiments demonstrate that our approach delivers more robust tracking results with lower computational effort than existing state-of-the-art methods. The source code is available at https://github.com/Yue-0/Eva-Tracker.

</details>


### [122] [Hemispherical Angular Power Mapping of Installed mmWave Radar Modules Under Realistic Deployment Constraints](https://arxiv.org/abs/2602.12584)
*Maaz Qureshi,Mohammad Omid Bagheri,William Melek,George Shaker*

Main category: cs.RO

TL;DR: 本文提出了一种用于毫米波雷达设备原位（in-situ）电磁特性的半球角度接收功率测量方法，在现场部署和受限条件下可对安装后的辐射特性进行表征。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达模块的实际应用中，其辐射行为会受到封装、安装硬件及周围结构的影响，而一旦设备被安装到实际环境中，传统的暗室和转台等天线测量手段往往难以实施。因此，需要一种能够在真实部署条件下对毫米波雷达辐射性能进行表征的新方法。

Method: 作者提出并实现了一种半球角度接收功率映射方法，通过在设备周围半空间（accessible half-space）不同的（phi, theta, r）几何位置放置校准接收探头，并采用准静态方式记录幅值，仅需常规射频仪器，即可采集到反映现场安装影响的功率分布数据。

Result: 在60 GHz雷达模块上进行了实验验证，所提方法实现了可重复的半球功率映射，其角度分布趋势与电磁全波模拟结果吻合良好。

Conclusion: 该方法能够在实际应用环境下有效评估和表征毫米波发射器的辐射特性，为现场天线/雷达模块性能验证提供了一种实用方案。

Abstract: Characterizing the angular radiation behavior of installed millimeter-wave (mmWave) radar modules is increasingly important in practical sensing platforms, where packaging, mounting hardware, and nearby structures can significantly alter the effective emission profile. However, once a device is embedded in its host environment, conventional chamber- and turntable-based antenna measurements are often impractical. This paper presents a hemispherical angular received-power mapping methodology for in-situ EM validation of installed mmWave modules under realistic deployment constraints. The approach samples the accessible half-space around a stationary device-under-test by placing a calibrated receiving probe at prescribed (phi, theta, r) locations using geometry-consistent positioning and quasi-static acquisition. Amplitude-only received-power is recorded using standard RF instrumentation to generate hemispherical angular power maps that capture installation-dependent radiation characteristics. Proof-of-concept measurements on a 60-GHz radar module demonstrate repeatable hemi-spherical mapping with angular trends in good agreement with full-wave simulation, supporting practical on-site characterization of embedded mmWave transmitters.

</details>


### [123] [PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People](https://arxiv.org/abs/2602.12597)
*Mahdi Haghighat Joo,Maryam Karimi Jafari,Alireza Taheri*

Main category: cs.RO

TL;DR: 本文提出了PISHYAR，一种具备社会智能的智能导盲手杖，实现了社会感知导航与多模态人机交互，支持物理移动和互动辅助。通过真实和仿真测试，系统在障碍物规避和群体活动识别上表现优异，并获得视障用户的高度认可。


<details>
  <summary>Details</summary>
Motivation: 当前导盲手杖在导航功能上已取得进展，但缺乏社会互动感知和自然的人机交互能力，难以满足视障人士在复杂社会场景下的多样化需求。作者希望通过融合社会感知与多模态交互，提升辅助设备的实用性和用户体验。

Method: 系统由两部分组成：（1）基于Raspberry Pi 5的社会导航模块，集成OAK-D Lite深度相机、YOLOv8目标检测、COMPOSER群体活动识别、D* Lite动态路径规划与触觉马达，用于导航与互动反馈；（2）多模态人机交互模块，结合语音识别、视觉-语言模型、大语言模型和语音合成，实现语音与视觉的动态切换及自然交互。通过仿真、实地和用户研究进行系统评估。

Result: 在仿真和实际室内环境中，系统展现出约80%的总准确率，能有效避障并进行社会合规的导航。群体活动识别在多样人群场景下表现鲁棒。与8位视障和低视力用户的初步用户研究显示，用户对系统的可用性、信任度和社交性评价较高。

Conclusion: PISHYAR展示了将社会感知导航和多模态交互融入辅助设备的可行性和潜力，不仅提升视障人士的出行安全，也增强了他们与环境及他人的社会互动，作为新一代智能辅助工具具有实际应用前景。

Abstract: This paper presents PISHYAR, a socially intelligent smart cane designed by our group to combine socially aware navigation with multimodal human-AI interaction to support both physical mobility and interactive assistance. The system consists of two components: (1) a social navigation framework implemented on a Raspberry Pi 5 that integrates real-time RGB-D perception using an OAK-D Lite camera, YOLOv8-based object detection, COMPOSER-based collective activity recognition, D* Lite dynamic path planning, and haptic feedback via vibration motors for tasks such as locating a vacant seat; and (2) an agentic multimodal LLM-VLM interaction framework that integrates speech recognition, vision language models, large language models, and text-to-speech, with dynamic routing between voice-only and vision-only modes to enable natural voice-based communication, scene description, and object localization from visual input. The system is evaluated through a combination of simulation-based tests, real-world field experiments, and user-centered studies. Results from simulated and real indoor environments demonstrate reliable obstacle avoidance and socially compliant navigation, achieving an overall system accuracy of approximately 80% under different social conditions. Group activity recognition further shows robust performance across diverse crowd scenarios. In addition, a preliminary exploratory user study with eight visually impaired and low-vision participants evaluates the agentic interaction framework through structured tasks and a UTAUT-based questionnaire reveals high acceptance and positive perceptions of usability, trust, and perceived sociability during our experiments. The results highlight the potential of PISHYAR as a multimodal assistive mobility aid that extends beyond navigation to provide socially interactive support for such users.

</details>


### [124] [RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models](https://arxiv.org/abs/2602.12628)
*Liangzhi Shi,Shuaihang Chen,Feng Gao,Yinuo Chen,Kang Chen,Tonghe Zhang,Hongzhi Zhang,Weinan Zhang,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习(RL)的仿真-现实协同训练(RL-Co)框架，能有效提升视觉-语言-动作模型在现实机器人任务中的表现和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的仿真-现实协同训练方法大多依赖于监督微调，将仿真仅作为静态演示来源，未能充分利用仿真的大规模交互能力，导致现实任务中的表现和泛化有限。为此，作者希望设计一种能更好利用仿真交互且保留真实能力的新方法。

Method: 该方法分为两阶段：首先用现实和仿真混合演示进行监督预训练，随后在仿真中用强化学习微调，同时对现实数据加auxiliary loss，防止对现实能力遗忘，从而利用仿真交互提升模型能力。

Result: 在四项真实桌面操作任务，以及OpenVLA和$π_{0.5}$两种VLA模型体系上，RL-Co在真实任务成功率上分别提升24%和20%，优于仅现实/传统SFT方案。此外，对新任务变体有更好泛化和数据效率。

Conclusion: RL-Co为仿真与现实交互协同训练提供了更实用和可扩展的路径，能有效促进现实机器人部署。

Abstract: Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \underline{\textit{RL}}-based sim-real \underline{\textit{Co}}-training \modify{(RL-Co)} framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting. We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and $π_{0.5}$, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on $π_{0.5}$. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency, providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.

</details>


### [125] [Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning](https://arxiv.org/abs/2602.12633)
*Tianyi Xiang,Jiahang Cao,Sikai Guo,Guoyang Zhao,Andrew F. Luo,Jun Ma*

Main category: cs.RO

TL;DR: 该论文提出了一种基于物理约束的Real-to-Sim管线，从单视角RGB-D数据重建物理一致的3D场景，可提升后续仿真和机器人操作的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统单视角3D重建方法关注几何精度但忽略物理可行性，导致浮空、穿模等问题，使后续机器人操作和物理仿真变得不可靠，尤其在密集物体接触的操控任务中风险极高。

Method: 作者提出采用基于接触图的可微分优化管线，联合优化物体位姿和物理属性，并集成可微分刚体仿真，通过建模物体之间的空间依赖关系，确保重建结果具备物理一致性。

Result: 在仿真和现实场景中的广泛实验表明，该方法重建的3D场景物理可信度高，能逼真还原真实接触动力学，适用于稳定可靠的机器人抓取和操作。

Conclusion: 将物理约束引入3D重建大幅提升了仿真和现实场景中的力学一致性，有助于视觉感知与机器人控制的高效衔接。

Abstract: Reconstructing physically valid 3D scenes from single-view observations is a prerequisite for bridging the gap between visual perception and robotic control. However, in scenarios requiring precise contact reasoning, such as robotic manipulation in highly cluttered environments, geometric fidelity alone is insufficient. Standard perception pipelines often neglect physical constraints, resulting in invalid states, e.g., floating objects or severe inter-penetration, rendering downstream simulation unreliable. To address these limitations, we propose a novel physics-constrained Real-to-Sim pipeline that reconstructs physically consistent 3D scenes from single-view RGB-D data. Central to our approach is a differentiable optimization pipeline that explicitly models spatial dependencies via a contact graph, jointly refining object poses and physical properties through differentiable rigid-body simulation. Extensive evaluations in both simulation and real-world settings demonstrate that our reconstructed scenes achieve high physical fidelity and faithfully replicate real-world contact dynamics, enabling stable and reliable contact-rich manipulation.

</details>


### [126] [PMG: Parameterized Motion Generator for Human-like Locomotion Control](https://arxiv.org/abs/2602.12656)
*Chenxi Han,Yuheng Min,Zihao Huang,Ao Hong,Hang Liu,Yi Cheng,Houde Liu*

Main category: cs.RO

TL;DR: 论文提出了一种新的实时运动生成方法PMG，通过高维控制和少量参数化数据，能生成自然类人的双足机器人运动，实现了高效、可验证的实物迁移，并在原型机器人上取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 现有运动跟踪和轨迹控制器尽管成熟，但难以适应高层命令、不同任务环境，对数据和校准要求高，并且在速度和姿态变化时表现不稳定。

Method: 提出了参数化运动生成器（PMG），利用对人类运动结构的分析，仅依靠紧凑的参数化运动数据和高维控制命令，实时合成参考轨迹。系统还包括模仿学习流程和基于优化的实物-仿真参数识别模块，构成完整的运动生成方案。

Result: PMG在ZERITH Z1原型机器人上验证，能够产生自然的人形步态，精准响应高维指令（如VR遥操作），并实现高效、可靠的仿真到实物的迁移。

Conclusion: 本论文方法有效提升了类人机器人运动控制的自然性和实用性，为自然、易部署的人形机器人控制方案提供了实验验证的路径。

Abstract: Recent advances in data-driven reinforcement learning and motion tracking have substantially improved humanoid locomotion, yet critical practical challenges remain. In particular, while low-level motion tracking and trajectory-following controllers are mature, whole-body reference-guided methods are difficult to adapt to higher-level command interfaces and diverse task contexts: they require large, high-quality datasets, are brittle across speed and pose regimes, and are sensitive to robot-specific calibration. To address these limitations, we propose the Parameterized Motion Generator (PMG), a real-time motion generator grounded in an analysis of human motion structure that synthesizes reference trajectories using only a compact set of parameterized motion data together with High-dimensional control commands. Combined with an imitation-learning pipeline and an optimization-based sim-to-real motor parameter identification module, we validate the complete approach on our humanoid prototype ZERITH Z1 and show that, within a single integrated system, PMG produces natural, human-like locomotion, responds precisely to high-dimensional control inputs-including VR-based teleoperation-and enables efficient, verifiable sim-to-real transfer. Together, these results establish a practical, experimentally validated pathway toward natural and deployable humanoid control.

</details>


### [127] [Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution](https://arxiv.org/abs/2602.12684)
*Rui Cai,Jun Guo,Xinze He,Piaopiao Jin,Jie Li,Bingxuan Lin,Futeng Liu,Wei Liu,Fei Ma,Kun Ma,Feng Qiu,Heng Qu,Yifei Su,Qiao Sun,Dong Wang,Donghao Wang,Yunhong Wang,Rujie Wu,Diyun Xiang,Yu Yang,Hangjun Ye,Yuan Zhang,Quanyun Zhou*

Main category: cs.RO

TL;DR: 本文提出了一种性能强劲且实时性高的视觉-语言-动作（VLA）模型—Xiaomi-Robotics-0，能够高效流畅地控制机器人完成复杂操作任务，并在模拟和真实机器人任务中取得了最新最优成绩。


<details>
  <summary>Details</summary>
Motivation: 机器人在实际应用中需要具有高泛化能力并且能实时响应多变的环境与任务，当前方法往往难以兼顾泛化、实时性与高性能。因此，研究如何使机器人在保证理解语义能力的同时，实现高效、连续、流畅的动作生成，具有重要现实意义。

Method: Xiaomi-Robotics-0模型采用了大规模多机器人任务轨迹和视觉-语言数据进行预训练，以获得泛化的动作生成能力。此外，通过新颖的训练和部署策略优化模型的异步执行和动作连续性，包括特别设计的训练食谱、推理延迟应对技术、时间步对齐等方法，确保在实际机器人操作中的高效流畅执行。

Result: 在大量仿真测试基准和两个高难度的真实机器人双手操作任务中，Xiaomi-Robotics-0均取得了最先进的性能表现。在普通消费级GPU上也能实现高速、平滑的控制和高任务成功率，展现出优异的实用价值。

Conclusion: Xiaomi-Robotics-0显著提升了机器人VLA模型的动作泛化能力和真实场景下的实时流畅性，为机器人强化学习和多模态控制领域带来了新的进展，其代码和模型已开源，有望加速相关研究的进展。

Abstract: In this report, we introduce Xiaomi-Robotics-0, an advanced vision-language-action (VLA) model optimized for high performance and fast and smooth real-time execution. The key to our method lies in a carefully designed training recipe and deployment strategy. Xiaomi-Robotics-0 is first pre-trained on large-scale cross-embodiment robot trajectories and vision-language data, endowing it with broad and generalizable action-generation capabilities while avoiding catastrophic forgetting of the visual-semantic knowledge of the underlying pre-trained VLM. During post-training, we propose several techniques for training the VLA model for asynchronous execution to address the inference latency during real-robot rollouts. During deployment, we carefully align the timesteps of consecutive predicted action chunks to ensure continuous and seamless real-time rollouts. We evaluate Xiaomi-Robotics-0 extensively in simulation benchmarks and on two challenging real-robot tasks that require precise and dexterous bimanual manipulation. Results show that our method achieves state-of-the-art performance across all simulation benchmarks. Moreover, Xiaomi-Robotics-0 can roll out fast and smoothly on real robots using a consumer-grade GPU, achieving high success rates and throughput on both real-robot tasks. To facilitate future research, code and model checkpoints are open-sourced at https://xiaomi-robotics-0.github.io

</details>


### [128] [SignScene: Visual Sign Grounding for Mapless Navigation](https://arxiv.org/abs/2602.12686)
*Nicky Zimmerman,Joel Loo,Benjamin Koh,Zishuo Wang,David Hsu*

Main category: cs.RO

TL;DR: 本论文提出了SignScene方法，使机器人能够在无地图情况下通过导航标志进行导航，并实现了高准确率和实际机器人验证。


<details>
  <summary>Details</summary>
Motivation: 人类可以通过观察环境中的导航标志在不使用地图的情况下进行导航；本文探讨如何让机器人也能通过类似方式自如地理解和利用真实环境中的多样化标志，实现无地图导航。主要挑战在于如何将抽象的标志语义内容与具体的三维场景元素进行关联。

Method: 作者提出了一个名为SignScene的符号驱动空间-语义表示方法，能够捕捉有导航意义的场景元素与标志信息，并以利于视觉-语言模型理解和推理的形式呈现给模型。本文将标志语义映射到对应的场景元素和导航动作，并通过构建数据集和实验进行评估。

Result: 在九种不同环境类型收集的114个查询数据集上，所提方法达到了88%的标志语义落地准确率，显著优于基线方法。

Conclusion: SignScene方法显著提升了机器人对导航标志的理解和利用能力，推动了机器人在真实世界中基于标志实现无地图导航的应用落地，并在实际Spot机器人平台上展示了有效性。

Abstract: Navigational signs enable humans to navigate unfamiliar environments without maps. This work studies how robots can similarly exploit signs for mapless navigation in the open world. A central challenge lies in interpreting signs: real-world signs are diverse and complex, and their abstract semantic contents need to be grounded in the local 3D scene. We formalize this as sign grounding, the problem of mapping semantic instructions on signs to corresponding scene elements and navigational actions. Recent Vision-Language Models (VLMs) offer the semantic common-sense and reasoning capabilities required for this task, but are sensitive to how spatial information is represented. We propose SignScene, a sign-centric spatial-semantic representation that captures navigation-relevant scene elements and sign information, and presents them to VLMs in a form conducive to effective reasoning. We evaluate our grounding approach on a dataset of 114 queries collected across nine diverse environment types, achieving 88% grounding accuracy and significantly outperforming baselines. Finally, we demonstrate that it enables real-world mapless navigation on a Spot robot using only signs.

</details>


### [129] [ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training](https://arxiv.org/abs/2602.12691)
*Rushuai Yang,Hecheng Wang,Chiming Liu,Xiaohan Yan,Yunlong Wang,Xuan Du,Shuoyu Yue,Yongcheng Liu,Chuheng Zhang,Lizhe Qi,Yi Chen,Wei Shan,Maoqing Yao*

Main category: cs.RO

TL;DR: 该论文提出了一种名为ALOE的动作级离策略评估方法，用于提升现实环境中的大规模视觉-语言-动作系统（VLA）通过在线强化学习的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLA的大模型在线学习主要依赖保守的同策略方法以保证稳定性，这限制了系统对于当前高性能策略的直接评估和更高效的学习能力。现实世界中采集到的数据往往来自于不同策略和人工干预，因此离策略评估是一个核心挑战。

Method: 作者提出ALOE框架，通过基于片段的时序差分自举方法对动作序列进行逐步评估，而非只预测最终结果，从而实现对关键动作片段的有效奖励归因。这支持了稳定的策略改进，并且适用于混合数据的离策略评估场景。

Result: 方法在三项现实操作任务（高精度的手机装箱、长周期衣物折叠、双臂多目标抓取）上进行了评估。结果显示，ALOE在不降低执行速度的情况下，提高了VLA系统的学习效率。

Conclusion: ALOE证明了离策略强化学习能够以可靠方式应用于现实世界VLA模型的后训练阶段，为未来大规模多模态机器人自主学习提供了新途径。

Abstract: We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness. In this paper, we propose ALOE, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training. Videos and additional materials are available at our project website.

</details>


### [130] [Constrained PSO Six-Parameter Fuzzy PID Tuning Method for Balanced Optimization of Depth Tracking Performance in Underwater Vehicles](https://arxiv.org/abs/2602.12700)
*Yanxi Ding,Tingyue Jia*

Main category: cs.RO

TL;DR: 本文提出了一种约束粒子群优化（PSO）方法对六参数模糊PID控制器进行联合整定，显著优化了水下航行器深度控制的响应性能，并兼顾控制能耗和执行器约束。


<details>
  <summary>Details</summary>
Motivation: 水下航行器的深度控制要求快速跟踪、低超调和满足执行器约束，传统模糊PID调参多依赖经验，难以在性能与成本之间达到稳定的最优平衡。

Method: 提出了一种约束粒子群优化方法，对基准PID参数及模糊控制器的输入量化因子、输出比例增益进行联合优化，同时引入加权绝对误差积分、调节时间、相对超调、控制能量和饱和占用率作为多指标约束，构建完整评价体系，实现性能与能量约束下的自适应整定。

Result: 在控制能耗及饱和占用率基本不变的前提下，所提方法使加权绝对误差积分由0.2631降至0.1473，调节时间由2.301秒缩短至1.613秒，相对超调由0.1494降至0.01839，控制能耗微降，饱和占用率略有减少。

Conclusion: 所提六参数联合整定与约束优化策略显著提升了水下航行器深度控制的跟踪性能，实现了工程可行性与优异控制效果的统一，具备实际应用价值。

Abstract: Depth control of underwater vehicles in engineering applications must simultaneously satisfy requirements for rapid tracking, low overshoot, and actuator constraints. Traditional fuzzy PID tuning often relies on empirical methods, making it difficult to achieve a stable and reproducible equilibrium solution between performance enhancement and control cost. This paper proposes a constrained particle swarm optimization (PSO) method for tuning six-parameter fuzzy PID controllers. By adjusting the benchmark PID parameters alongside the fuzzy controller's input quantization factor and output proportional gain, it achieves synergistic optimization of the overall tuning strength and dynamic response characteristics of the fuzzy PID system. To ensure engineering feasibility of the optimization results, a time-weighted absolute error integral, adjustment time, relative overshoot control energy, and saturation occupancy rate are introduced. Control energy constraints are applied to construct a constraint-driven comprehensive evaluation system, suppressing pseudo-improvements achieved solely by increasing control inputs. Simulation results demonstrate that, while maintaining consistent control energy and saturation levels, the proposed method significantly enhances deep tracking performance: the time-weighted absolute error integral decreases from 0.2631 to 0.1473, the settling time shortens from 2.301 s to 1.613 s, and the relative overshoot reduces from 0.1494 to 0.01839. Control energy varied from 7980 to 7935, satisfying the energy constraint, while saturation occupancy decreased from 0.004 to 0.003. These results validate the effectiveness and engineering significance of the proposed constrained six-parameter joint tuning strategy for depth control in underwater vehicle navigation scenarios.

</details>


### [131] [TRANS: Terrain-aware Reinforcement Learning for Agile Navigation of Quadruped Robots under Social Interactions](https://arxiv.org/abs/2602.12724)
*Wei Zhu,Irfan Tito Kurniawan,Ye Zhao,Mistuhiro Hayashibe*

Main category: cs.RO

TL;DR: 该论文提出TRANS框架，实现四足机器人在复杂地形与社交互动环境下的敏捷导航，验证了方法优越性。


<details>
  <summary>Details</summary>
Motivation: 现有四足机器人导航方法将运动规划与动作控制分离，缺乏地形感知、全身约束考虑，难以适应动态且有人类环境。此外，端到端方法依赖高频感知，代价高且常伴随噪声，限制实际应用。解决这些问题对于提升机器人实际部署能力十分重要。

Method: 提出两阶段训练框架，结合三个深度强化学习（DRL）管线：1）TRANS-Loco采用非对称AC模型实现无明确地形/接触感知的四足地形行走；2）TRANS-Nav用对称AC模型，以变换后的激光雷达数据对动态社交场景导航；3）TRANS整合前两者，实现复杂地形与社交交互下的统一导航。

Result: 实验对比验证了TRANS在地形适应和社交导航的性能均优于主流方法，并通过真实机器人实验证明了算法的仿真到现实迁移能力。

Conclusion: TRANS有效提升了四足机器人在动态复杂环境下的导航与社交能力，为实用化迈出重要一步。

Abstract: This study introduces TRANS: Terrain-aware Reinforcement learning for Agile Navigation under Social interactions, a deep reinforcement learning (DRL) framework for quadrupedal social navigation over unstructured terrains. Conventional quadrupedal navigation typically separates motion planning from locomotion control, neglecting whole-body constraints and terrain awareness. On the other hand, end-to-end methods are more integrated but require high-frequency sensing, which is often noisy and computationally costly. In addition, most existing approaches assume static environments, limiting their use in human-populated settings. To address these limitations, we propose a two-stage training framework with three DRL pipelines. (1) TRANS-Loco employs an asymmetric actor-critic (AC) model for quadrupedal locomotion, enabling traversal of uneven terrains without explicit terrain or contact observations. (2) TRANS-Nav applies a symmetric AC framework for social navigation, directly mapping transformed LiDAR data to ego-agent actions under differential-drive kinematics. (3) A unified pipeline, TRANS, integrates TRANS-Loco and TRANS-Nav, supporting terrain-aware quadrupedal navigation in uneven and socially interactive environments. Comprehensive benchmarks against locomotion and social navigation baselines demonstrate the effectiveness of TRANS. Hardware experiments further confirm its potential for sim-to-real transfer.

</details>


### [132] [Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models](https://arxiv.org/abs/2602.12734)
*Nick Heppert,Minh Quang Nguyen,Abhinav Valada*

Main category: cs.RO

TL;DR: 提出了一种将人类演示转化为机器人学习的新方法，只需一次人类演示即可生成大量训练数据，并显著提高机器人任务成功率。


<details>
  <summary>Details</summary>
Motivation: 机器人模仿学习通常需要大量人工示范，过程繁琐且成本高。而人类演示容易获取，但难以直接迁移到机器人上，因此需要一个能有效利用人类演示的方法。

Method: 提出Real2Gen方法，从单次人类演示中提取关键信息，将任务迁移到仿真环境中，由专家代理生成大量高质量演示数据，用于训练机器人操作策略。

Result: 在三项真实任务的人类演示上，Real2Gen相较于最新基线方法，在成功率上平均提升26.6%，且因训练数据丰富多样，泛化能力更好。

Conclusion: Real2Gen能够高效利用少量人类演示，通过仿真生成足量训练数据，提升机器人学习表现，实现零样本迁移，具有显著实际应用价值。

Abstract: Imitation learning is a popular paradigm to teach robots new tasks, but collecting robot demonstrations through teleoperation or kinesthetic teaching is tedious and time-consuming. In contrast, directly demonstrating a task using our human embodiment is much easier and data is available in abundance, yet transfer to the robot can be non-trivial. In this work, we propose Real2Gen to train a manipulation policy from a single human demonstration. Real2Gen extracts required information from the demonstration and transfers it to a simulation environment, where a programmable expert agent can demonstrate the task arbitrarily many times, generating an unlimited amount of data to train a flow matching policy. We evaluate Real2Gen on human demonstrations from three different real-world tasks and compare it to a recent baseline. Real2Gen shows an average increase in the success rate of 26.6% and better generalization of the trained policy due to the abundance and diversity of training data. We further deploy our purely simulation-trained policy zero-shot in the real world. We make the data, code, and trained models publicly available at real2gen.cs.uni-freiburg.de.

</details>


### [133] [SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies](https://arxiv.org/abs/2602.12794)
*Thies Oelerich,Gerald Ebmer,Christian Hartl-Nesic,Andreas Kugi*

Main category: cs.RO

TL;DR: 本文提出了一种新的机器人控制方法SafeFlowMPC，将学习和优化结合，既保证任务通用性和灵活性，又确保可解释性和安全性，并可实时应用于实际机器人任务。


<details>
  <summary>Details</summary>
Motivation: 随着机器人日益融入现实生活，对其柔性、实时反应能力的要求增加。而纯基于学习的方法虽能泛化任务，但缺乏可解释性和安全性；基于优化的方法虽可提供安全保障，但泛化与灵活性不足。因此，需开发兼具两者优点的方法。

Method: 作者提出了SafeFlowMPC，将flow matching（流匹配）技术与在线最优控制（MPC）相结合，利用子最优的模型预测控制框架实现实时、安全、可泛化的机器人运动控制。

Result: 在KUKA 7自由度机械臂的3个真实实验（包括两个抓取实验和一次人机交互传递实验）中，SafeFlowMPC展现了强大性能。

Conclusion: SafeFlowMPC有效结合了学习方法的灵活性与优化方法的安全性，在实际机器人任务中具有很好的应用前景。

Abstract: The emerging integration of robots into everyday life brings several major challenges. Compared to classical industrial applications, more flexibility is needed in combination with real-time reactivity. Learning-based methods can train powerful policies based on demonstrated trajectories, such that the robot generalizes a task to similar situations. However, these black-box models lack interpretability and rigorous safety guarantees. Optimization-based methods provide these guarantees but lack the required flexibility and generalization capabilities. This work proposes SafeFlowMPC, a combination of flow matching and online optimization to combine the strengths of learning and optimization. This method guarantees safety at all times and is designed to meet the demands of real-time execution by using a suboptimal model-predictive control formulation. SafeFlowMPC achieves strong performance in three real-world experiments on a KUKA 7-DoF manipulator, namely two grasping experiment and a dynamic human-robot object handover experiment. A video of the experiments is available at http://www.acin.tuwien.ac.at/42d6. The code is available at https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC.

</details>


### [134] [SKYSURF: A Self-learning Framework for Persistent Surveillance using Cooperative Aerial Gliders](https://arxiv.org/abs/2602.12838)
*Houssem Eddine Mohamadi,Nadjia Kara*

Main category: cs.RO

TL;DR: 本文提出了一种用于自主部署可滑翔无人机（UAV）的本地-全局行为管理与决策方法，旨在通过利用上升气流延长UAV的飞行时间，实现更持久的监视任务。


<details>
  <summary>Details</summary>
Motivation: 小型无人机因电池容量有限，航时受限，无法持续有效地执行监视任务。为突破该瓶颈，探寻利用环境可再生能源（如上升气流）以延长航时成为研究焦点。

Method: 该方法将多架UAV建模为非确定性有限状态的理性代理体，结合任务分配和新路径规划方案（结合视野与预测避免碰撞），并引入延迟学习和参数调优策略以优化路径跟踪控制器。

Result: 与三个基线方法和15种进化算法的对比实验表明，提出的方法能显著提升无人机的持续飞行时间和目标探测能力（检测效果为非协作和半协作方案的两倍），且能耗大幅降低（6小时耗电不到6%）。

Conclusion: 该方法有效提升了小型无人机在监视任务中的持久性和目标检测能力，并优化了能源利用效率，具有较大推广和应用价值。

Abstract: The success of surveillance applications involving small unmanned aerial vehicles (UAVs) depends on how long the limited on-board power would persist. To cope with this challenge, alternative renewable sources of lift are sought. One promising solution is to extract energy from rising masses of buoyant air. This paper proposes a local-global behavioral management and decision-making approach for the autonomous deployment of soaring-capable UAVs. The cooperative UAVs are modeled as non-deterministic finite state-based rational agents. In addition to a mission planning module for assigning tasks and issuing dynamic navigation waypoints for a new path planning scheme, in which the concepts of visibility and prediction are applied to avoid the collisions. Moreover, a delayed learning and tuning strategy is employed optimize the gains of the path tracking controller. Rigorous comparative analyses carried out with three benchmarking baselines and 15 evolutionary algorithms highlight the adequacy of the proposed approach for maintaining the surveillance persistency (staying aloft for longer periods without landing) and maximizing the detection of targets (two times better than non-cooperative and semi-cooperative approaches) with less power consumption (almost 6% of battery consumed in six hours).

</details>


### [135] [Adding internal audio sensing to internal vision enables human-like in-hand fabric recognition with soft robotic fingertips](https://arxiv.org/abs/2602.12918)
*Iris Andrussow,Jans Solano,Benjamin A. Richardson,Georg Martius,Katherine J. Kuchenbecker*

Main category: cs.RO

TL;DR: 這篇論文提出了一套可同步獲取高時空解析度觸覺資訊的機器人觸覺系統，應用於布料質地辨識，其分類準確度高且具良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 人類能輕鬆感知不同布料的觸感（如絲綢的光滑與棉布的粗糙），但讓機器人複製這種複雜感知仍很困難，因為現有觸覺感測器難以同時實現高空間解析度與高時間取樣率。因此，研究動機為開發能感測和融合法布料質地信息的多模態高性能觸覺系統。

Method: 開發了新型機器人手指觸覺系統：一根手指配備可視型觸覺感測器（Minsight）以測量形變與力量，另一根手指配備新開發的聲學振動感測器（Minsound）。機器手主動模仿人類的夾持與摩擦行為，收集不同布料的觸覺數據後，採用基於Transformer的深度學習模型進行分類與特徵提取。

Result: 結合視覺觸覺和聲學觸覺信號，可有效提升機器人對20種常見布料的分類準確率（最高97%）。在環境噪音強時，如增設外部麥克風還能增強系統穩健性，且學得的觸覺特徵（如延展性、厚度、粗糙度）具有一定泛化能力。

Conclusion: 雙模態觸覺融合帶來卓越的布料分類效果和特徵泛化能力，證明了將音訊和視覺觸覺信息協同運用於機器觸覺感知的實用價值。

Abstract: Distinguishing the feel of smooth silk from coarse cotton is a trivial everyday task for humans. When exploring such fabrics, fingertip skin senses both spatio-temporal force patterns and texture-induced vibrations that are integrated to form a haptic representation of the explored material. It is challenging to reproduce this rich, dynamic perceptual capability in robots because tactile sensors typically cannot achieve both high spatial resolution and high temporal sampling rate. In this work, we present a system that can sense both types of haptic information, and we investigate how each type influences robotic tactile perception of fabrics. Our robotic hand's middle finger and thumb each feature a soft tactile sensor: one is the open-source Minsight sensor that uses an internal camera to measure fingertip deformation and force at 50 Hz, and the other is our new sensor Minsound that captures vibrations through an internal MEMS microphone with a bandwidth from 50 Hz to 15 kHz. Inspired by the movements humans make to evaluate fabrics, our robot actively encloses and rubs folded fabric samples between its two sensitive fingers. Our results test the influence of each sensing modality on overall classification performance, showing high utility for the audio-based sensor. Our transformer-based method achieves a maximum fabric classification accuracy of 97 % on a dataset of 20 common fabrics. Incorporating an external microphone away from Minsound increases our method's robustness in loud ambient noise conditions. To show that this audio-visual tactile sensing approach generalizes beyond the training data, we learn general representations of fabric stretchiness, thickness, and roughness.

</details>


### [136] [INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval](https://arxiv.org/abs/2602.12971)
*YukTungSamuel Fang,Zhikang Shi,Jiabin Qiu,Zixuan Chen,Jieqi Shi,Hao Xu,Jing Huo,Yang Gao*

Main category: cs.RO

TL;DR: 本文提出了INHerit-SG系统，通过将语义场景图作为可解释的知识库，实现了更贴合人类意图的机器人三维环境建模与导航，提升了复杂任务下的信息检索能力和系统可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有基于基础模型的三维语义场景图方法对于机器人导航任务支持有限，难以解释与匹配人类意图，且处理复杂环境时效率较低，无法满足具身智能任务对实时性与解释性的双重需求。

Method: 1) 将地图重定义为结构化、自然语言锚定的知识库，便于与人类意图对齐；2) 采用异步双流程架构和多层次层级结构，分离几何分割与语义推理，提升效率；3) 设计事件驱动的图更新机制，只在关键语义变更时更新图结构，确保持续一致性；4) 利用大语言模型分解检索查询，结合hard-to-soft过滤，提高复杂查询的准确性与鲁棒性。

Result: 在新构建的HM3DSem-SQR数据集和真实环境下，INHerit-SG在复杂查询任务上达到了SOTA性能，并展现出良好的可扩展性，适合多种下游导航任务。

Conclusion: 本文方法有效提升了三维场景图的人类意图可解释性和任务匹配度，为机器人在复杂环境中的高效导航和人机交互提供了新范式。

Abstract: Driven by advancements in foundation models, semantic scene graphs have emerged as a prominent paradigm for high-level 3D environmental abstraction in robot navigation. However, existing approaches are fundamentally misaligned with the needs of embodied tasks. As they rely on either offline batch processing or implicit feature embeddings, the maps can hardly support interpretable human-intent reasoning in complex environments. To address these limitations, we present INHerit-SG. We redefine the map as a structured, RAG-ready knowledge base where natural-language descriptions are introduced as explicit semantic anchors to better align with human intent. An asynchronous dual-process architecture, together with a Floor-Room-Area-Object hierarchy, decouples geometric segmentation from time-consuming semantic reasoning. An event-triggered map update mechanism reorganizes the graph only when meaningful semantic events occur. This strategy enables our graph to maintain long-term consistency with relatively low computational overhead. For retrieval, we deploy multi-role Large Language Models (LLMs) to decompose queries into atomic constraints and handle logical negations, and employ a hard-to-soft filtering strategy to ensure robust reasoning. This explicit interpretability improves the success rate and reliability of complex retrievals, enabling the system to adapt to a broader spectrum of human interaction tasks. We evaluate INHerit-SG on a newly constructed dataset, HM3DSem-SQR, and in real-world environments. Experiments demonstrate that our system achieves state-of-the-art performance on complex queries, and reveal its scalability for downstream navigation tasks. Project Page: https://fangyuktung.github.io/INHeritSG.github.io/

</details>


### [137] [Learning Native Continuation for Action Chunking Flow Policies](https://arxiv.org/abs/2602.12978)
*Yufeng Liu,Hang Yu,Juntu Zhao,Bocheng Li,Di Zhang,Mingzhu Li,Wenxuan Wu,Yingdong Hu,Junyuan Xie,Junliang Guo,Dequan Wang,Yang Gao*

Main category: cs.RO

TL;DR: 本文提出了一种名为Legato的新方法，通过在训练阶段引入噪声和已知动作的混合，使VLA模型在进行分块动作执行时轨迹更加平滑，显著改善任务效率。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型依赖于动作分块实现实时运行，但在分块边界经常出现轨迹不连续的问题，现有的RTC方法虽然有所缓解，但不在策略内部，导致频繁的模式切换和执行轨迹不够平滑。

Method: Legato在训练阶段引入新的延续方法：用已知动作和噪声的调度型混合进行去噪初始化，让模型在部分动作信息下学习，保证模型训练期间和推理期间在逐步引导下的去噪过程一致，还通过训练中使用随机调度条件，实现对不同推理延迟的支持和可控的平滑性。

Result: Legato在多项真实操作任务中，比RTC方法在轨迹平滑度和任务完成时长上有大约10%的提升，表现更优。

Conclusion: Legato不仅能有效减少分块执行中的犹豫和多模态切换，还能提高轨迹的流畅度和任务完成效率，有望成为未来VLA实时分块执行的新主流方法。

Abstract: Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time. Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time.

</details>


### [138] [How Swarms Differ: Challenges in Collective Behaviour Comparison](https://arxiv.org/abs/2602.13016)
*André Fialho Jesus,Jonas Kuckling*

Main category: cs.RO

TL;DR: 本文评估了集体行为的特征集及相似性度量方法在群体智能中的稳健性，并提出了一种基于自组织映射的新方法辨别难以区分的行为区域。


<details>
  <summary>Details</summary>
Motivation: 旨在解决群体智能行为特征集常常针对特定场景设计、泛化性和稳健性不足的问题，同时为自动化群体行为设计提供可量化的相似性度量方法。

Method: 选择以往群体机器人领域中常用的特征集和相似性度量方法，系统性地评估各自的鲁棒性。通过对比不同特征集与相似性度量的组合，分析其区分集体行为能力差异。同时，提出一种基于自组织映射（SOM）的手段，用于探索特征空间中难以区分行为的区域。

Result: 发现特征集和相似性度量方法的匹配会显著影响相似行为的区分类能力。某些组合能更好地区分行为群体。提出的自组织映射方法成功辅助发现了特征空间内区分困难的行为区。

Conclusion: 特征集与相似性度量应匹配优化，用于更有效地区分集体行为。自组织映射是一种有前景的自动识别行为难区分区域的工具，对自动化群体行为设计具有促进作用。

Abstract: Collective behaviours often need to be expressed through numerical features, e.g., for classification or imitation learning. This problem is often addressed by proposing an ad-hoc feature set for a particular swarm behaviour context, usually without further consideration of the solution's resilience outside of the conceived context. Yet, the development of automatic methods to design swarm behaviours is dependent on the ability to measure quantitatively the similarity of swarm behaviours. Hence, we investigate the impact of feature sets for collective behaviours. We select swarm feature sets and similarity measures from prior swarm robotics works, which mainly considered a narrow behavioural context and assess their robustness. We demonstrate that the interplay of feature set and similarity measure makes some combinations more suitable to distinguish groups of similar behaviours. We also propose a self-organised map-based approach to identify regions of the feature space where behaviours cannot be easily distinguished.

</details>


### [139] [SENSE-STEP: Learning Sim-to-Real Locomotion for a Sensory-Enabled Soft Quadruped Robot](https://arxiv.org/abs/2602.13078)
*Storm de Kam,Ebrahim Shahabi,Cosimo Della Santina*

Main category: cs.RO

TL;DR: 本文提出了一种基于学习的控制框架，用于气动软体四足机器人，通过触觉和本体感觉反馈实现高效闭环运动，并在真实机器人上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 软体四足机器人因动力学高维、执行器迟滞、地面接触难建模等问题，闭环运动控制一直很困难，传统的本体感觉对于地面接触信息有限。

Method: 提出一种基于学习的控制系统，特征在于：1）应用于带有“吸盘足”的气动软体四足机器人；2）通过逐层学习，先以参考步态作为起点，再在随机环境下优化训练；3）控制策略融合本体与触觉反馈，输出协调的气动与吸盘控制指令。

Result: 实验结果显示，闭环控制策略在实物机器人验证中，在平地前进速度提升41%，5度斜面提升91%；消融实验还证明触觉力估计和惯性反馈对稳定运动有显著作用，性能提升最高达56%。

Conclusion: 学习驱动的多模态感知（本体+触觉）闭环控制，可大幅提升软体四足机器人运动稳健性和效率，为软体机器人闭环运动提供了有效方案。

Abstract: Robust closed-loop locomotion remains challenging for soft quadruped robots due to high-dimensional dynamics, actuator hysteresis, and difficult-to-model contact interactions, while conventional proprioception provides limited information about ground contact. In this paper, we present a learning-based control framework for a pneumatically actuated soft quadruped equipped with tactile suction-cup feet, and we validate the approach experimentally on physical hardware. The control policy is trained in simulation through a staged learning process that starts from a reference gait and is progressively refined under randomized environmental conditions. The resulting controller maps proprioceptive and tactile feedback to coordinated pneumatic actuation and suction-cup commands, enabling closed-loop locomotion on flat and inclined surfaces. When deployed on the real robot, the closed-loop policy outperforms an open-loop baseline, increasing forward speed by 41% on a flat surface and by 91% on a 5-degree incline. Ablation studies further demonstrate the role of tactile force estimates and inertial feedback in stabilizing locomotion, with performance improvements of up to 56% compared to configurations without sensory feedback.

</details>


### [140] [Agentic AI for Robot Control: Flexible but still Fragile](https://arxiv.org/abs/2602.13081)
*Oscar Lima,Marc Vinci,Martin Günther,Marian Renz,Alexander Sung,Sebastian Stock,Johannes Brust,Lennart Niecksch,Zongyao Yi,Felix Igelbrink,Benjamin Kisliuk,Martin Atzmueller,Joachim Hertzberg*

Main category: cs.RO

TL;DR: 本文提出了一种以具有推理能力的语言模型为核心的机器人控制系统，实现了在不同行业场景中的任务规划和执行。系统灵活但当前整体仍然较为脆弱。


<details>
  <summary>Details</summary>
Motivation: 当前生成式模型在机器人控制中显示出强大能力和常识推理潜力，然而尚缺乏兼具迁移性、解释性和灵活性的方法来应对实际环境下的不确定性和复杂指令。

Method: 该方法构建了一个以语言模型为核心的智能体控制系统，利用模型推理在循环规划-执行框架下选择与调用机器人技能。支持结构化自省、显式事件检查、操作者即时干预、以及跨平台迁移。

Result: 在室内抓取/移动与农业自主导航等平台上，系统表现实用但存在易受提示敏感、非最优与指令理解错误等问题，展示了系统的脆弱性。

Conclusion: 尽管存在不稳定问题，但该架构迁移性和扩展性较强，实现跨领域移植只需更换提示和对接接口，展示了学术和工业应用的潜力。

Abstract: Recent work leverages the capabilities and commonsense priors of generative models for robot control. In this paper, we present an agentic control system in which a reasoning-capable language model plans and executes tasks by selecting and invoking robot skills within an iterative planner and executor loop. We deploy the system on two physical robot platforms in two settings: (i) tabletop grasping, placement, and box insertion in indoor mobile manipulation (Mobipick) and (ii) autonomous agricultural navigation and sensing (Valdemar). Both settings involve uncertainty, partial observability, sensor noise, and ambiguous natural-language commands. The system exposes structured introspection of its planning and decision process, reacts to exogenous events via explicit event checks, and supports operator interventions that modify or redirect ongoing execution. Across both platforms, our proof-of-concept experiments reveal substantial fragility, including non-deterministic suboptimal behavior, instruction-following errors, and high sensitivity to prompt specification. At the same time, the architecture is flexible: transfer to a different robot and task domain largely required updating the system prompt (domain model, affordances, and action catalogue) and re-binding the same tool interface to the platform-specific skill API.

</details>


### [141] [UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph](https://arxiv.org/abs/2602.13086)
*Haichao Liu,Yuanjiang Xue,Yuheng Zhou,Haoyuan Deng,Yinan Liang,Lihua Xie,Ziwei Wang*

Main category: cs.RO

TL;DR: 本文提出UniManip，一种结合语义推理和物理约束的新型机器人操作框架，在未知任务和物体上实现了更强的零样本泛化能力，显著优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作方法在零样本泛化上存在不足：端到端视觉-语言-动作（VLA）模型缺乏长时任务精度，传统分层方法对开放世界变化不够灵活。作者希望设计一种框架，兼顾语义理解和物理操作，提升在复杂开放环境下的泛化与鲁棒性。

Method: 提出了基于双层Agentic Operation Graph（AOG）的UniManip架构。高层Agentic Layer负责任务调度与语义推理，底层Scene Layer实现对场景动态状态的物理表示。系统动态生成对象中心场景图，通过安全感知局部规划器转为无碰撞轨迹，并利用结构化记忆模块实现自主故障诊断与恢复，形成连续的agentic感知—执行—反馈闭环。

Result: 大量实验表明：在未见过的对象和任务上，UniManip较SOTA的VLA模型和分层方法成功率分别高22.5%和25%。系统还能直接完成从固定基座到移动操作的零样本迁移，无需微调。

Conclusion: UniManip有效融合语义层和物理层推理，提升了机器人在非结构化环境中的泛化与可靠性。该方法突破了传统方法在开放世界灵活性和精度上的局限，对通用机器人操作具有重要推动意义。

Abstract: Achieving general-purpose robotic manipulation requires robots to seamlessly bridge high-level semantic intent with low-level physical interaction in unstructured environments. However, existing approaches falter in zero-shot generalization: end-to-end Vision-Language-Action (VLA) models often lack the precision required for long-horizon tasks, while traditional hierarchical planners suffer from semantic rigidity when facing open-world variations. To address this, we present UniManip, a framework grounded in a Bi-level Agentic Operational Graph (AOG) that unifies semantic reasoning and physical grounding. By coupling a high-level Agentic Layer for task orchestration with a low-level Scene Layer for dynamic state representation, the system continuously aligns abstract planning with geometric constraints, enabling robust zero-shot execution. Unlike static pipelines, UniManip operates as a dynamic agentic loop: it actively instantiates object-centric scene graphs from unstructured perception, parameterizes these representations into collision-free trajectories via a safety-aware local planner, and exploits structured memory to autonomously diagnose and recover from execution failures. Extensive experiments validate the system's robust zero-shot capability on unseen objects and tasks, demonstrating a 22.5% and 25.0% higher success rate compared to state-of-the-art VLA and hierarchical baselines, respectively. Notably, the system enables direct zero-shot transfer from fixed-base setups to mobile manipulation without fine-tuning or reconfiguration. Our open-source project page can be found at https://henryhcliu.github.io/unimanip.

</details>


### [142] [Temporally-Sampled Efficiently Adaptive State Lattices for Autonomous Ground Robot Navigation in Partially Observed Environments](https://arxiv.org/abs/2602.13159)
*Ashwin Satish Menon,Eric R. Damm,Eli S. Lancaster,Felix A. Sanchez,Jason M. Gregory,Thomas M. Howard*

Main category: cs.RO

TL;DR: 本文提出了一种新的区域规划器架构TSEASL，以提高越野机器人在部分可观环境中的自主导航安全性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 越野移动机器人由于传感器局限，面临环境感知不完全的问题，导致导航过程中参考轨迹频繁大幅变动，增加了不安全的行为和人工干预风险。

Method: 提出了TSEASL（时间采样高效自适应状态格）区域规划器仲裁架构，通过在每次循环中对比当前生成的轨迹和经过优化后的历史轨迹，从而为本地规划器提供更稳定的参考轨迹。

Result: 在Clearpath Warthog无人地面车的实地测试和真实地图数据验证下，采用TSEASL的机器人显著减少了需要人工干预的情况，并且整体规划器稳定性优于传统基线方法。

Conclusion: TSEASL有效提升了越野自主导航的安全性和稳定性，并讨论了进一步优化措施以扩展其通用性，适用于更多越野自主场景。

Abstract: Due to sensor limitations, environments that off-road mobile robots operate in are often only partially observable. As the robots move throughout the environment and towards their goal, the optimal route is continuously revised as the sensors perceive new information. In traditional autonomous navigation architectures, a regional motion planner will consume the environment map and output a trajectory for the local motion planner to use as a reference. Due to the continuous revision of the regional plan guidance as a result of changing map information, the reference trajectories which are passed down to the local planner can differ significantly across sequential planning cycles. This rapidly changing guidance can result in unsafe navigation behavior, often requiring manual safety interventions during autonomous traversals in off-road environments. To remedy this problem, we propose Temporally-Sampled Efficiently Adaptive State Lattices (TSEASL), which is a regional planner arbitration architecture that considers updated and optimized versions of previously generated trajectories against the currently generated trajectory. When tested on a Clearpath Robotics Warthog Unmanned Ground Vehicle as well as real map data collected from the Warthog, results indicate that when running TSEASL, the robot did not require manual interventions in the same locations where the robot was running the baseline planner. Additionally, higher levels of planner stability were recorded with TSEASL over the baseline. The paper concludes with a discussion of further improvements to TSEASL in order to make it more generalizable to various off-road autonomy scenarios.

</details>


### [143] [Human Emotion-Mediated Soft Robotic Arts: Exploring the Intersection of Human Emotions, Soft Robotics and Arts](https://arxiv.org/abs/2602.13163)
*Saitarun Nadipineni,Chenhao Hong,Tanishtha Ramlall,Chapa Sirithunge,Kaspar Althoefer,Fumiya Iida,Thilina Dulantha Lalitharatne*

Main category: cs.RO

TL;DR: 本研究将软体机器人与脑电信号结合，用于情感驱动的艺术展示，提出并实验了可根据人类情感状态动态变化的软体艺术作品。


<details>
  <summary>Details</summary>
Motivation: 软体机器人因其柔软性和安全性，在医疗、自动化以及艺术等方面有广泛应用，但基于情感交互的软体机器人艺术领域仍待拓展。本研究旨在探讨如何通过软体机器人与人类情感结合，创造具有互动性和表现力的新型艺术形式。

Method: 作者设计了两种软体机器人化身（软体角色和软体花卉），能够根据脑电（EEG）中的alpha波信号来反映不同的情感水平。具体方法为：先通过采集人脑alpha波信号测量情感状态，再将这些信号映射到软体机器人的动态动作中，实现情感-运动的联动，并通过实验进行展示和评估。

Result: 实验展示了基于alpha波的情感信号可以成功驱动软体角色与软体花卉的动态变化。软体机器人的动作能够有效反映人的情感水平，实现了情感与艺术装置的融合互动。

Conclusion: 研究结果证明软体机器人可作为人类情感状态的表达媒介，为艺术创新和人机互动带来了新的可能性，也丰富了软体机器人在艺术装置领域的应用。

Abstract: Soft robotics has emerged as a versatile field with applications across various domains, from healthcare to industrial automation, and more recently, art and interactive installations. The inherent flexibility, adaptability, and safety of soft robots make them ideal for applications that require delicate, organic, and lifelike movement, allowing for immersive and responsive interactions. This study explores the intersection of human emotions, soft robotics, and art to establish and create new forms of human emotion-mediated soft robotic art. In this paper, we introduce two soft embodiments: a soft character and a soft flower as an art display that dynamically responds to brain signals based on alpha waves, reflecting different emotion levels. We present how human emotions can be measured as alpha waves based on brain/EEG signals, how we map the alpha waves to the dynamic movements of the two soft embodiments, and demonstrate our proposed concept using experiments. The findings of this study highlight how soft robotics can embody human emotional states, offering a new medium for insightful artistic expression and interaction, and demonstrating how art displays can be embodied.

</details>


### [144] [Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control](https://arxiv.org/abs/2602.13193)
*William Chen,Jagdeep Singh Bhatia,Catherine Glossop,Nikhil Mathihalli,Ria Doshi,Andy Tang,Danny Driess,Karl Pertsch,Sergey Levine*

Main category: cs.RO

TL;DR: 该论文提出Steerable Policies方法，通过对多层次抽象的合成命令进行训练，实现视觉-语言模型（VLM）知识在机器人控制任务中的更高效落地，并在实际操作中显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 虽然VLM模型在理解场景和推理上具有强大能力，但如何将其知识有效地用于机器人底层行为控制依然是难题。现有方法多采用将高层命令通过自然语言接口下发给低层控制策略，实现效率和通用性受限。作者希望通过改进这一交互接口，提升VLM对机器人底层行为的精细控制，实现更好的泛化能力。

Method: 提出Steerable Policies，将VLA训练在多粒度的合成命令（如子任务、具体操作、像素坐标等）上，提升低层策略的可控性。通过与高层推理器或现有VLM结合，基于上下文学习以多层次命令方式调度低层行为。

Result: 在多项现实世界的机器人操作实验中，本文方法在泛化和长期任务等挑战性场景下，均优于现有VLA和VLM层级方法基线。

Conclusion: 多粒度可控低层策略（Steerable Policies）极大提升了VLM知识在机器人控制任务中的泛化和应用能力，为视觉-语言机器人系统的深度融合带来新可能。

Abstract: Pretrained vision-language models (VLMs) can make semantic and visual inferences across diverse settings, providing valuable common-sense priors for robotic control. However, effectively grounding this knowledge in robot behaviors remains an open challenge. Prior methods often employ a hierarchical approach where VLMs reason over high-level commands to be executed by separate low-level policies, e.g., vision-language-action models (VLAs). The interface between VLMs and VLAs is usually natural language task instructions, which fundamentally limits how much VLM reasoning can steer low-level behavior. We thus introduce Steerable Policies: VLAs trained on rich synthetic commands at various levels of abstraction, like subtasks, motions, and grounded pixel coordinates. By improving low-level controllability, Steerable Policies can unlock pretrained knowledge in VLMs, enabling improved task generalization. We demonstrate this benefit by controlling our Steerable Policies with both a learned high-level embodied reasoner and an off-the-shelf VLM prompted to reason over command abstractions via in-context learning. Across extensive real-world manipulation experiments, these two novel methods outperform prior embodied reasoning VLAs and VLM-based hierarchical baselines, including on challenging generalization and long-horizon tasks.
  Website: steerable-policies.github.io

</details>


### [145] [Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos](https://arxiv.org/abs/2602.13197)
*Albert J. Zhai,Kuo-Hao Zeng,Jiasen Lu,Ali Farhadi,Shenlong Wang,Wei-Chiu Ma*

Main category: cs.RO

TL;DR: 本文提出了一种基于人类视频和模拟结合的机器人操作技能学习框架，能够实现更稳健的面向任务的抓取与操作。


<details>
  <summary>Details</summary>
Motivation: 人类视频包含丰富的操作动作信息，有望为机器人操作技能学习提供大规模可扩展的数据来源。但机器人与人类手结构不同，直接通过视频学习抓取行为难度大，且常规抓取生成器输出的抓取动作可能不适用于具体任务。需要结合任务需求进行更有针对性的抓取学习。

Method: 提出Perceive-Simulate-Imitate（PSI）框架，分为模块化策略设计：利用专门的抓取生成器实现稳定抓取，并通过在仿真中配对的抓取-轨迹过滤器，将人类视频转化的动作轨迹数据打上抓取适应性的标签，实现针对任务的抓取能力有监督学习。

Result: 实验表明，无需任何真实机器人数据，PSI框架能高效学习精确操作技能，且最终操作表现显著优于直接使用抓取生成器方法，提升了下游任务的成功率和健壮性。

Conclusion: PSI框架充分利用了人类视频数据和仿真配对标注，显著提升了机器人操作技能学习的效率与泛化能力，是以视频为基础机器人操作学习的有效途径。

Abstract: The ability to learn manipulation skills by watching videos of humans has the potential to unlock a new source of highly scalable data for robot learning. Here, we tackle prehensile manipulation, in which tasks involve grasping an object before performing various post-grasp motions. Human videos offer strong signals for learning the post-grasp motions, but they are less useful for learning the prerequisite grasping behaviors, especially for robots without human-like hands. A promising way forward is to use a modular policy design, leveraging a dedicated grasp generator to produce stable grasps. However, arbitrary stable grasps are often not task-compatible, hindering the robot's ability to perform the desired downstream motion. To address this challenge, we present Perceive-Simulate-Imitate (PSI), a framework for training a modular manipulation policy using human video motion data processed by paired grasp-trajectory filtering in simulation. This simulation step extends the trajectory data with grasp suitability labels, which allows for supervised learning of task-oriented grasping capabilities. We show through real-world experiments that our framework can be used to learn precise manipulation skills efficiently without any robot data, resulting in significantly more robust performance than using a grasp generator naively.

</details>
