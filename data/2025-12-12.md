<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 105]
- [cs.CL](#cs.CL) [Total: 30]
- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Neuromorphic Eye Tracking for Low-Latency Pupil Detection](https://arxiv.org/abs/2512.09969)
*Paul Hueber,Luca Peres,Florian Pitters,Alejandro Gloriani,Oliver Rhodes*

Main category: cs.CV

TL;DR: 本文提出了一种用于可穿戴系统的高效神经形态（SNN）眼动追踪模型，实现了milliwatt级功耗和极低时延，同时准确度媲美特定应用系统，在模型大小和计算效率上大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于帧的眼动追踪方法存在运动模糊、高计算成本和时域分辨率受限等问题，难以满足可穿戴系统（如AR/VR）对低时延、低功耗和高响应性的需求。神经形态传感器和SNN提供了解决方案，但当前SNN方法性能或通用性有限。

Method: 将性能领先的事件驱动型眼动追踪模型中的循环和注意力模块替换为轻量级LIF层，并使用深度可分离卷积降低模型复杂度，系统性地设计高效的SNN结构。

Result: 新模型平均误差达3.7-4.1px，接近专用系统Retina（3.24px），在模型体积和理论计算量方面分别缩减至1/20和1/850，预计功耗为3.9-4.9 mW，时延仅3 ms，支持1 kHz运行。

Conclusion: 事件驱动眼动追踪模型可以通过SNN架构进行高效重设计，大幅降低功耗与运算资源消耗，同时保持适用于实时可穿戴应用的准确率。

Abstract: Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment.

</details>


### [2] [ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects](https://arxiv.org/abs/2512.10031)
*Woojin Lee,Hyugjae Chang,Jaeho Moon,Jaehyup Lee,Munchurl Kim*

Main category: cs.CV

TL;DR: 本文提出了一种高效的弱监督有向目标检测新框架ABBSPO，有效提升了在仅使用水平框标注条件下的检测性能，达到了当前最佳水平。


<details>
  <summary>Details</summary>
Motivation: 以往弱监督有向目标检测方法虽便利但精度有限，尤其在处理实际数据集的水平框标注时，直接比较水平框与预测旋转框的最小外接矩形，会导致尺度估算不准，影响检测效果。作者希望克服这一不足，提升弱监督检测的精度和鲁棒性。

Method: 提出ABBSPO框架，包括两个创新核心：(1)自适应边框缩放（ABBS），根据预测的有向框对真实水平框进行自适应缩放，提高尺度估计的准确性；(2)对称先验角度损失（SPA loss），利用航空目标固有的对称性，通过自监督方式增强角度学习，防止多视角翻转下的学习崩溃问题。

Result: 通过大规模实验，作者证明ABBSPO在多个弱监督有向目标检测数据集上均超过现有方法，达到了最新SOTA表现。

Conclusion: ABBSPO显著提高了基于水平框弱标注的有向目标检测精度和稳定性，对弱监督目标检测领域具有推动意义。

Abstract: Weakly supervised oriented object detection (WS-OOD) has gained attention as a cost-effective alternative to fully supervised methods, providing both efficiency and high accuracy. Among weakly supervised approaches, horizontal bounding box (HBox)-supervised OOD stands out for its ability to directly leverage existing HBox annotations while achieving the highest accuracy under weak supervision settings. This paper introduces adaptive bounding box scaling and symmetry-prior-based orientation prediction, called ABBSPO, a framework for WS-OOD. Our ABBSPO addresses limitations of previous HBox-supervised OOD methods, which compare ground truth (GT) HBoxes directly with the minimum circumscribed rectangles of predicted RBoxes, often leading to inaccurate scale estimation. To overcome this, we propose: (i) Adaptive Bounding Box Scaling (ABBS), which appropriately scales GT HBoxes to optimize for the size of each predicted RBox, ensuring more accurate scale prediction; and (ii) a Symmetric Prior Angle (SPA) loss that exploits inherent symmetry of aerial objects for self-supervised learning, resolving issues in previous methods where learning collapses when predictions for all three augmented views (original, rotated, and flipped) are consistently incorrect. Extensive experimental results demonstrate that ABBSPO achieves state-of-the-art performance, outperforming existing methods.

</details>


### [3] [Diffusion Is Your Friend in Show, Suggest and Tell](https://arxiv.org/abs/2512.10038)
*Jia Cheng Hu,Roberto Cavicchioli,Alessandro Capotondi*

Main category: cs.CV

TL;DR: 提出了一种结合扩散模型和自回归模型优点的新方法，在COCO数据集上实现了目前同类任务的最优结果。


<details>
  <summary>Details</summary>
Motivation: 当前扩散去噪模型在生成任务中虽有出色表现，但在离散领域仍未能超过自回归模型。作者旨在突破现有限制，提升生成质量。

Method: 采用扩散模型为自回归生成提供建议，而不是直接替代，从而结合两者优点。提出了Show, Suggest and Tell (SST)方法，并通过广泛实验验证。

Result: SST方法在COCO数据集上取得125.1的CIDEr-D分数，无需强化学习，分别比当前最优自回归与扩散模型高出1.5和2.5分。

Conclusion: 将扩散模型与自回归模型结合是提升生成模型性能的新方向，建议模块显著提升结果，目前该领域仍有很大潜力，值得进一步研究。

Abstract: Diffusion Denoising models demonstrated impressive results across generative Computer Vision tasks, but they still fail to outperform standard autoregressive solutions in the discrete domain, and only match them at best. In this work, we propose a different paradigm by adopting diffusion models to provide suggestions to the autoregressive generation rather than replacing them. By doing so, we combine the bidirectional and refining capabilities of the former with the strong linguistic structure provided by the latter. To showcase its effectiveness, we present Show, Suggest and Tell (SST), which achieves State-of-the-Art results on COCO, among models in a similar setting. In particular, SST achieves 125.1 CIDEr-D on the COCO dataset without Reinforcement Learning, outperforming both autoregressive and diffusion model State-of-the-Art results by 1.5 and 2.5 points. On top of the strong results, we performed extensive experiments to validate the proposal and analyze the impact of the suggestion module. Results demonstrate a positive correlation between suggestion and caption quality, overall indicating a currently underexplored but promising research direction. Code will be available at: https://github.com/jchenghu/show\_suggest\_tell.

</details>


### [4] [MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata](https://arxiv.org/abs/2512.10041)
*Yihao Liu,Chenyu Gao,Lianrui Zuo,Michael E. Kim,Brian D. Boyd,Lisa L. Barnes,Walter A. Kukull,Lori L. Beason-Held,Susan M. Resnick,Timothy J. Hohman,Warren D. Taylor,Bennett A. Landman*

Main category: cs.CV

TL;DR: 该论文提出了MetaVoxel，一个生成性联合扩散建模框架，可以通过学习跨越所有变量的单一扩散过程，联合建模影像数据和临床元数据，实现多任务统一和灵活推理。实验表明，MetaVoxel能在多个数据集上实现与传统特定任务模型相当的效果。


<details>
  <summary>Details</summary>
Motivation: 目前，大多数医学深度学习方法仅关注特定预测方向和特定输入变量，导致每个任务需要单独建模，缺乏统一和灵活性。作者希望通过建模联合分布，来解决这一碎片化的问题，实现模型的多任务适用与通用推理能力。

Method: 作者提出了一种名为MetaVoxel的扩散模型，该模型通过学习影像数据与临床元数据的联合分布，不依赖任务特定输入，利用单一模型即可实现影像生成、年龄估计和性别预测等多种任务，无需为每个任务单独训练模型。模型使用了9个数据集共超过10,000例T1加权MRI和关联临床数据进行训练和评估。

Result: 实验结果表明，MetaVoxel在影像生成、年龄估计和性别预测等多项任务上，性能接近传统的各类任务专用基线模型。此外，实验还展示了该模型在零样本灵活推理方面的能力，可以支持任意子集输入下的预测。

Conclusion: 联合多模态扩散模型为医学AI模型统一和实现更广泛临床适用性提供了新方向。MetaVoxel能够整合多任务于一个模型，展现出强大的灵活推理和广泛适用性。

Abstract: Modern deep learning methods have achieved impressive results across tasks from disease classification, estimating continuous biomarkers, to generating realistic medical images. Most of these approaches are trained to model conditional distributions defined by a specific predictive direction with a specific set of input variables. We introduce MetaVoxel, a generative joint diffusion modeling framework that models the joint distribution over imaging data and clinical metadata by learning a single diffusion process spanning all variables. By capturing the joint distribution, MetaVoxel unifies tasks that traditionally require separate conditional models and supports flexible zero-shot inference using arbitrary subsets of inputs without task-specific retraining. Using more than 10,000 T1-weighted MRI scans paired with clinical metadata from nine datasets, we show that a single MetaVoxel model can perform image generation, age estimation, and sex prediction, achieving performance comparable to established task-specific baselines. Additional experiments highlight its capabilities for flexible inference.Together, these findings demonstrate that joint multimodal diffusion offers a promising direction for unifying medical AI models and enabling broader clinical applicability.

</details>


### [5] [Independent Density Estimation](https://arxiv.org/abs/2512.10067)
*Jiahao Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为IDE（Independent Density Estimation，独立密度估计）的方法，用于提升视觉-语言模型在组合理解方面的泛化能力，并通过两种模型和一种熵推理方法显示出对未见组合的更强泛化。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视觉-语言模型虽然在多任务下表现优异，但在人类般组合理解（即对未见词语与图像特征组合的泛化）方面存在瓶颈，亟需新的方法解决模型泛化能力不足的问题。

Method: 提出IDE方法，使模型能够学习句子中各个单词与图像特征之间的独立联系。然后基于此思想设计了两类模型：一类使用完全解耦的视觉表征作为输入，另一类通过变分自编码器（VAE）从原始图像获取部分解耦特征。此外，提出了基于熵的组合推理方法，用于整合每个词的预测结果。

Result: 在多个数据集上评估表明，与当前主流模型相比，所提出方法在未见组合上的泛化能力显著提升。

Conclusion: 通过IDE及基于熵的组合推理，有效改善了视觉-语言模型的组合理解泛化，为未来提升模型表达与理解能力提供了新路径。

Abstract: Large-scale Vision-Language models have achieved remarkable results in various domains, such as image captioning and conditioned image generation. Neverthe- less, these models still encounter difficulties in achieving human-like composi- tional generalization. In this study, we propose a new method called Independent Density Estimation (IDE) to tackle this challenge. IDE aims to learn the connec- tion between individual words in a sentence and the corresponding features in an image, enabling compositional generalization. We build two models based on the philosophy of IDE. The first one utilizes fully disentangled visual representations as input, and the second leverages a Variational Auto-Encoder to obtain partially disentangled features from raw images. Additionally, we propose an entropy- based compositional inference method to combine predictions of each word in the sentence. Our models exhibit superior generalization to unseen compositions compared to current models when evaluated on various datasets.

</details>


### [6] [TraceFlow: Dynamic 3D Reconstruction of Specular Scenes Driven by Ray Tracing](https://arxiv.org/abs/2512.10095)
*Jiachen Tao,Junyi Wu,Haoxuan Wang,Zongxin Yang,Dawen Cai,Yan Yan*

Main category: cs.CV

TL;DR: 本文提出了TraceFlow框架，通过新的2D高斯泼洒表示与混合渲染技术，实现了动态镜面场景的高保真渲染，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动态镜面场景下的反射渲染存在反射方向估算不准和物理建模不精确的问题，传统方法难以兼顾精度与效率，因此需要新的高保真渲染方案。

Method: TraceFlow采用残差材质增强的2D高斯泼洒表示，融合动态几何与材质属性，精确计算反射射线。引入动态环境高斯方法和混合渲染管线，将渲染分解为漫反射与镜面反射部分，并结合光栅化与光线追踪，实现物理准确的镜面合成。同时提出由粗到细的训练策略，提升优化稳定性和分解质量。

Result: 在动态场景基准下，TraceFlow在定量与定性评测中均优于此前方法，能够在复杂动态环境中渲染出更清晰、更真实的镜面反射。

Conclusion: TraceFlow有效解决了动态镜面场景高保真渲染中的关键难题，为复杂动态环境下的镜面反射提供了新方法，具有较强的实用价值。

Abstract: We present TraceFlow, a novel framework for high-fidelity rendering of dynamic specular scenes by addressing two key challenges: precise reflection direction estimation and physically accurate reflection modeling. To achieve this, we propose a Residual Material-Augmented 2D Gaussian Splatting representation that models dynamic geometry and material properties, allowing accurate reflection ray computation. Furthermore, we introduce a Dynamic Environment Gaussian and a hybrid rendering pipeline that decomposes rendering into diffuse and specular components, enabling physically grounded specular synthesis via rasterization and ray tracing. Finally, we devise a coarse-to-fine training strategy to improve optimization stability and promote physically meaningful decomposition. Extensive experiments on dynamic scene benchmarks demonstrate that TraceFlow outperforms prior methods both quantitatively and qualitatively, producing sharper and more realistic specular reflections in complex dynamic environments.

</details>


### [7] [Hierarchical Instance Tracking to Balance Privacy Preservation with Accessible Information](https://arxiv.org/abs/2512.10102)
*Neelima Prasad,Jarek Reynolds,Neel Karsanbhai,Tanusree Sharma,Lotus Zhang,Abigale Stangl,Yang Wang,Leah Findlater,Danna Gurari*

Main category: cs.CV

TL;DR: 本文提出了一项新的任务：层次化实例追踪，并构建了首个支持该任务的基准数据集，包含40个类别、2765个实体、552个视频，实验发现该任务和数据集具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 尽管实例追踪在计算机视觉中已有研究，但现有方法尚未同时关注对象及其部件，并维持它们之间的层次关系。因此亟需新的任务定义和相应数据集来推动相关方法的发展。

Method: 作者定义了“层次化实例追踪”任务，需要同时追踪对象及其部件并维护层级关系。为此，作者构建了包含40类对象和部件、2765个实体、552段视频的新数据集，并基于现有模型设计了7种改进模型进行实验和评测。

Result: 对7个模型变体的评测结果显示，所提出的数据集和任务具有很大挑战性，现有方法难以很好地适应层次化实例追踪任务。

Conclusion: 本文为层次化实例追踪任务提供了新的基准数据集和实验分析，有望推动该领域的发展。

Abstract: We propose a novel task, hierarchical instance tracking, which entails tracking all instances of predefined categories of objects and parts, while maintaining their hierarchical relationships. We introduce the first benchmark dataset supporting this task, consisting of 2,765 unique entities that are tracked in 552 videos and belong to 40 categories (across objects and parts). Evaluation of seven variants of four models tailored to our novel task reveals the new dataset is challenging. Our dataset is available at https://vizwiz.org/tasks-and-datasets/hierarchical-instance-tracking/

</details>


### [8] [Topological Conditioning for Mammography Models via a Stable Wavelet-Persistence Vectorization](https://arxiv.org/abs/2512.10151)
*Charles Fanning,Mehmet Emin Aktas*

Main category: cs.CV

TL;DR: 该论文提出了一种结合小波和拓扑数据分析的图像表征方法，通过引入wavelet persistence通道，可提升乳腺癌筛查中深度学习模型的泛化与稳定性，并在跨地区、多中心数据验证中表现良好。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌筛查依赖于影像判断，现有模型在数据分布变化、不同扫描设备和人群上常表现不佳，准确率下降。因而研究者希望构建更具鲁棒性和可泛化性的影像分析模型。

Method: 作者利用拓扑数据分析提取乳腺X光图像中多尺度、稳定的结构特征，并以wavelet persistence向量方式生成空间特征图，将其作为附加通道与原始图像拼接，投入二阶段深度学习检测管线。

Result: 在美国CBIS DDSM数据集训练和验证模型，并分别在葡萄牙INbreast和中国CMMD两个独立数据集上测试。结果显示，将ConvNeXt Tiny网络加入wavelet persistence通道后，在INbreast数据集上，患者级AUC从0.55提升至0.75（在受限训练预算下）。

Conclusion: wavelet persistence通道能稳定提升乳腺癌筛查AI模型在外部、异质数据上的表现，有助于降低漏诊与误诊风险，具有较强的实际应用价值。

Abstract: Breast cancer is the most commonly diagnosed cancer in women and a leading cause of cancer death worldwide. Screening mammography reduces mortality, yet interpretation still suffers from substantial false negatives and false positives, and model accuracy often degrades when deployed across scanners, modalities, and patient populations. We propose a simple conditioning signal aimed at improving external performance based on a wavelet based vectorization of persistent homology. Using topological data analysis, we summarize image structure that persists across intensity thresholds and convert this information into spatial, multi scale maps that are provably stable to small intensity perturbations. These maps are integrated into a two stage detection pipeline through input level channel concatenation. The model is trained and validated on the CBIS DDSM digitized film mammography cohort from the United States and evaluated on two independent full field digital mammography cohorts from Portugal (INbreast) and China (CMMD), with performance reported at the patient level. On INbreast, augmenting ConvNeXt Tiny with wavelet persistence channels increases patient level AUC from 0.55 to 0.75 under a limited training budget.

</details>


### [9] [Feature Coding for Scalable Machine Vision](https://arxiv.org/abs/2512.10209)
*Md Eimran Hossain Eimon,Juan Merlos,Ashan Perera,Hari Kalva,Velibor Adzic,Borko Furht*

Main category: cs.CV

TL;DR: 本文提出了一种针对深度神经网络边缘-云协同推理的中间特征压缩标准与模型，有效降低了带宽需求并保持精度。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络难以直接部署在边缘设备上，两种传统方案（全端侧执行或全云端推理）各有带宽、延迟、隐私等问题。将推理任务在设备和云之间分割，需中间特征的高效传输，因此需要新的特征压缩方法。

Method: MPEG 发起了面向机器的特征编码（FCM）标准，并设计了 Feature Coding Test Model（FCTM），该模型为中间特征压缩制定了比特流和编解码流程。

Result: 实验表明，在多个视觉任务中，FCTM 能实现平均 85.14% 的码率压缩，同时准确率几乎不降低。

Conclusion: FCM 标准和 FCTM 模型为带宽有限和隐私敏感的应用场景下的智能视觉部署提供了高效、可扩展的解决方案，促进了端云协作智能的互操作性。

Abstract: Deep neural networks (DNNs) drive modern machine vision but are challenging to deploy on edge devices due to high compute demands. Traditional approaches-running the full model on-device or offloading to the cloud face trade-offs in latency, bandwidth, and privacy. Splitting the inference workload between the edge and the cloud offers a balanced solution, but transmitting intermediate features to enable such splitting introduces new bandwidth challenges. To address this, the Moving Picture Experts Group (MPEG) initiated the Feature Coding for Machines (FCM) standard, establishing a bitstream syntax and codec pipeline tailored for compressing intermediate features. This paper presents the design and performance of the Feature Coding Test Model (FCTM), showing significant bitrate reductions-averaging 85.14%-across multiple vision tasks while preserving accuracy. FCM offers a scalable path for efficient and interoperable deployment of intelligent features in bandwidth-limited and privacy-sensitive consumer applications.

</details>


### [10] [Latent Chain-of-Thought World Modeling for End-to-End Driving](https://arxiv.org/abs/2512.10226)
*Shuhan Tan,Kashyap Chitta,Yuxiao Chen,Ran Tian,Yurong You,Yan Wang,Wenjie Luo,Yulong Cao,Philipp Krahenbuhl,Marco Pavone,Boris Ivanovic*

Main category: cs.CV

TL;DR: 本文提出了一种新的自动驾驶推理方法LCDrive，用于提升推理效率和驾驶性能，通过在潜在空间而非自然语言中进行推理。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶VLA模型常用自然语言表达推理链条（CoT），但自然语言推理效率低、未必最优。本文旨在探索更高效的推理表示方式，以提升自动驾驶系统性能和安全性。

Method: LCDrive将推理过程和决策过程统一于一个与动作对齐的潜在空间。模型采用两类token：动作提议token（与模型产出的驾驶动作同词表）、世界模型token（根植于潜在世界模型，表达这些动作的未来结果）。训练阶段利用场景的真实未来滚动轨迹监督动作和世界token；随后用闭环强化学习进一步提升推理能力。

Result: 在大规模端到端驾驶基准上，LCDrive相比于无推理和基于文本推理的基线模型，推理速度更快，生成的轨迹质量更好，并且在强化学习训练中带来更大性能提升。

Conclusion: LCDrive用潜在空间推理代替自然语言推理，不仅提高了推理和决策效率，还提升了自动驾驶系统的性能和安全性，对复杂自动驾驶场景具有实际意义和应用前景。

Abstract: Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.

</details>


### [11] [Emerging Standards for Machine-to-Machine Video Coding](https://arxiv.org/abs/2512.10230)
*Md Eimran Hossain Eimon,Velibor Adzic,Hari Kalva,Borko Furht*

Main category: cs.CV

TL;DR: 本论文研究了在机器对机器视觉系统中，采用不同视频编解码标准对特征编码压缩的影响，以及其对带宽、隐私和任务准确性的关系。


<details>
  <summary>Details</summary>
Motivation: 随着机器成为视觉数据的主要消费者，传统的视频流方案依赖于为人类视觉优化的编解码技术，导致带宽开销大、扩展性差且隐私易泄露。亟需为机器间通信优化新的视觉信息编解码方法。

Method: 作者介绍了MPEG近期对机器对机器通信设计的两种新技术：视频编码（VCM）和特征编码（FCM）。文中通过对比实验，评估了不同主流视频编码标准（H.264, H.265, H.266）在FCM方案中的表现，特别关注特征压缩后的下游任务准确性及带宽消耗。

Result: 实验发现，采用FCM特征编码可在接近边缘推理准确率的情况下有效降低码率。HEVC与VVC的任务性能几乎相同，用HEVC替换VVC只带来约1.39%的BD-Rate增加，而AVC则增加了32.28%。在跟踪任务上，各编解码器差别较小，HEVC甚至优于VVC，说明现有硬件支持下的老标准也能保障性能。

Conclusion: 利用特征编码能极大降低机器对机器视觉系统的带宽需求，同时基本不损失任务准确性，且部分经典视频编解码器如HEVC完全可以应对实际场景，具有良好的兼容性和现实可用性。

Abstract: Machines are increasingly becoming the primary consumers of visual data, yet most deployments of machine-to-machine systems still rely on remote inference where pixel-based video is streamed using codecs optimized for human perception. Consequently, this paradigm is bandwidth intensive, scales poorly, and exposes raw images to third parties. Recent efforts in the Moving Picture Experts Group (MPEG) redesigned the pipeline for machine-to-machine communication: Video Coding for Machines (VCM) is designed to apply task-aware coding tools in the pixel domain, and Feature Coding for Machines (FCM) is designed to compress intermediate neural features to reduce bitrate, preserve privacy, and support compute offload. Experiments show that FCM is capable of maintaining accuracy close to edge inference while significantly reducing bitrate. Additional analysis of H.26X codecs used as inner codecs in FCM reveals that H.265/High Efficiency Video Coding (HEVC) and H.266/Versatile Video Coding (VVC) achieve almost identical machine task performance, with an average BD-Rate increase of 1.39% when VVC is replaced with HEVC. In contrast, H.264/Advanced Video Coding (AVC) yields an average BD-Rate increase of 32.28% compared to VVC. However, for the tracking task, the impact of codec choice is minimal, with HEVC outperforming VVC and achieving BD Rate of -1.81% and 8.79% for AVC, indicating that existing hardware for already deployed codecs can support machine-to-machine communication without degrading performance.

</details>


### [12] [Multi-dimensional Preference Alignment by Conditioning Reward Itself](https://arxiv.org/abs/2512.10237)
*Jiho Jang,Jinyoung Kim,Kyungjune Baek,Nojun Kwak*

Main category: cs.CV

TL;DR: 论文提出了一种多维奖励条件DPO方法（MCDPO），解决了基于人类反馈的强化学习在扩散模型对齐时奖励冲突的问题，并实现了多维度上的灵活控制与提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类反馈的扩散模型对齐方法，如DPO，利用Bradley-Terry模型将不同评价维度（如美学质量和语义契合）混合为单一奖励。这种合并导致在某维度表现优秀但在全局评价中不占优的样本，其优点被模型遗忘，从而出现奖励冲突。

Method: 作者提出在DPO中引入解耦的Bradley-Terry目标函数，将多个奖励维度作为条件向量显式导入训练，让模型能够在同一网络下，独立学习各奖励维度的最优方向。此外，提出维度奖励丢弃机制，促进各维度的均衡优化。

Result: 在Stable Diffusion 1.5和SDXL上大量实验验证，MCDPO在各项基准测试中优于传统DPO方法。

Conclusion: MCDPO方法有效解决了奖励冲突问题，实现了多维度奖励的独立优化，并可以在推理时灵活放大特定维度，无需额外训练或外部奖励模型，显著提升扩散模型的可控性与表现。

Abstract: Reinforcement Learning from Human Feedback has emerged as a standard for aligning diffusion models. However, we identify a fundamental limitation in the standard DPO formulation because it relies on the Bradley-Terry model to aggregate diverse evaluation axes like aesthetic quality and semantic alignment into a single scalar reward. This aggregation creates a reward conflict where the model is forced to unlearn desirable features of a specific dimension if they appear in a globally non-preferred sample. To address this issue, we propose Multi Reward Conditional DPO (MCDPO). This method resolves reward conflicts by introducing a disentangled Bradley-Terry objective. MCDPO explicitly injects a preference outcome vector as a condition during training, which allows the model to learn the correct optimization direction for each reward axis independently within a single network. We further introduce dimensional reward dropout to ensure balanced optimization across dimensions. Extensive experiments on Stable Diffusion 1.5 and SDXL demonstrate that MCDPO achieves superior performance on benchmarks. Notably, our conditional framework enables dynamic and multiple-axis control at inference time using Classifier Free Guidance to amplify specific reward dimensions without additional training or external reward models.

</details>


### [13] [Solving Semi-Supervised Few-Shot Learning from an Auto-Annotation Perspective](https://arxiv.org/abs/2512.10244)
*Tian Liu,Anwesha Basu,James Caverlee,Shu Kong*

Main category: cs.CV

TL;DR: 本文提出了一种在半监督小样本学习（SSFSL）场景下，充分利用开源视觉-语言模型（VLM）资源的新方法，并通过温度调节等简单技巧解决VLM软标签分布过平带来的监督信号弱的问题。最终提出的SWIFT方法性能优于主流小样本和半监督方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用如自动标注任务需要在少量标注和大量未标注数据下训练模型。虽然VLM和其预训练数据丰富，但SSFSL领域很少加以利用，而小样本学习（FSL）已经从中获益。文章旨在填补这一空白，将VLM有效应用于SSFSL场景。

Method: 作者首先分析现有半监督学习（SSL）方法微调VLM效果不佳的原因，发现因VLM输出分布过平，导致伪标签置信度低，无法有效利用未标注数据。为此，提出用分类器初始化和温度调节，提升伪标签置信度，并进一步提出分阶段温度调节微调方法（SWIFT），结合有限标注数据、大量未标注数据和与任务相关的带噪数据进行有效微调。

Result: 在五个SSFSL基准数据集上，SWIFT方法比最新的小样本学习和半监督方法提升约5个百分点，甚至可媲美以全部真实标签进行监督的设置。

Conclusion: 通过简单有效的温度调节和分类器初始化方法，可极大提升VLM在半监督小样本学习中的表现。SWIFT为现实自动标注等场景下利用大规模开源资源和未标注数据提供了可行有效方案。

Abstract: Semi-supervised few-shot learning (SSFSL) formulates real-world applications like ''auto-annotation'', as it aims to learn a model over a few labeled and abundant unlabeled examples to annotate the unlabeled ones. Despite the availability of powerful open-source Vision-Language Models (VLMs) and their pretraining data, the SSFSL literature largely neglects these open-source resources. In contrast, the related area few-shot learning (FSL) has already exploited them to boost performance. Arguably, to achieve auto-annotation in the real world, SSFSL should leverage such open-source resources. To this end, we start by applying established SSL methods to finetune a VLM. Counterintuitively, they significantly underperform FSL baselines. Our in-depth analysis reveals the root cause: VLMs produce rather ''flat'' distributions of softmax probabilities. This results in zero utilization of unlabeled data and weak supervision signals. We address this issue with embarrassingly simple techniques: classifier initialization and temperature tuning. They jointly increase the confidence scores of pseudo-labels, improving the utilization rate of unlabeled data, and strengthening supervision signals. Building on this, we propose: Stage-Wise Finetuning with Temperature Tuning (SWIFT), which enables existing SSL methods to effectively finetune a VLM on limited labeled data, abundant unlabeled data, and task-relevant but noisy data retrieved from the VLM's pretraining set. Extensive experiments on five SSFSL benchmarks show that SWIFT outperforms recent FSL and SSL methods by $\sim$5 accuracy points. SWIFT even rivals supervised learning, which finetunes VLMs with the unlabeled data being labeled with ground truth!

</details>


### [14] [RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection](https://arxiv.org/abs/2512.10248)
*Zhuo Wang,Xiliang Liu,Ligang Sun*

Main category: cs.CV

TL;DR: 本论文提出了RobustSora基准，系统分析了AI生成视频检测方法对数字水印的依赖，发现多数现有模型对水印有不同程度依赖，强调了提升检测鲁棒性的新挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成视频技术迅速发展，保障信息真实性面临挑战。最新的AIGC检测方法和基准常忽略生成模型中嵌入的数字水印可能影响检测结果，因此，迫切需要考察检测方法对水印的依赖性及其鲁棒性。

Method: 构建包含6500个视频的数据集，分为4类：真实-干净、真实-伪造水印、生成-含水印、生成-去水印。设计两大评测任务：一是评估去水印后的AI视频检测性能，二是衡量在带有伪造水印的视频中误报率。对10种主流检测模型（包括专用检测器、Transformer类模型和多模态大模型）进行了系统实验。

Result: 在水印被操作后，检测模型的性能下降2-8个百分点。Transformer模型表现出中等程度（6-8pp）的依赖性，MLLM类方法依赖性变化较大（2-8pp）。整体结果显示现有模型普遍对水印特征部分依赖。

Conclusion: 研究表明，AIGC检测模型存在对水印特征的依赖，易受到水印操作影响。未来需采用水印感知的训练策略，提高视频检测的鲁棒性。RobustSora为该领域的鲁棒检测研究提供了重要工具和参考。

Abstract: The proliferation of AI-generated video technologies poses challenges to information integrity. While recent benchmarks advance AIGC video detection, they overlook a critical factor: many state-of-the-art generative models embed digital watermarks in outputs, and detectors may partially rely on these patterns. To evaluate this influence, we present RobustSora, the benchmark designed to assess watermark robustness in AIGC video detection. We systematically construct a dataset of 6,500 videos comprising four types: Authentic-Clean (A-C), Authentic-Spoofed with fake watermarks (A-S), Generated-Watermarked (G-W), and Generated-DeWatermarked (G-DeW). Our benchmark introduces two evaluation tasks: Task-I tests performance on watermark-removed AI videos, while Task-II assesses false alarm rates on authentic videos with fake watermarks. Experiments with ten models spanning specialized AIGC detectors, transformer architectures, and MLLM approaches reveal performance variations of 2-8pp under watermark manipulation. Transformer-based models show consistent moderate dependency (6-8pp), while MLLMs exhibit diverse patterns (2-8pp). These findings indicate partial watermark dependency and highlight the need for watermark-aware training strategies. RobustSora provides essential tools to advance robust AIGC detection research.

</details>


### [15] [THE-Pose: Topological Prior with Hybrid Graph Fusion for Estimating Category-Level 6D Object Pose](https://arxiv.org/abs/2512.10251)
*Eunho Lee,Chaehyeon Song,Seunghoon Jeong,Ayoung Kim*

Main category: cs.CV

TL;DR: 本文提出了THE-Pose方法，通过融合拓扑先验和混合图结构，有效提升了类别级6D物体位姿估计的稳健性和准确性，尤其应对类别内变化和遮挡。实验在REAL275数据集上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D图卷积（3D-GC）的方法在处理复杂物体和视觉模糊时效果有限，因为它们只捕捉局部几何和深度信息，缺乏对全局上下文和拓扑结构的建模能力。这在类别级物体位姿估计中，无法有效应对类别内结构变化和物体遮挡问题。作者希望结合全局拓扑特征和局部几何，提高方法的泛化和鲁棒性。

Method: 提出THE-Pose方法，首先在图像域中提取一致且具有不变性的拓扑特征，作为类别全局先验。然后，通过混合图融合（HGF）模块，将2D图像拓扑特征适配性地与点云特征进行融合，实现2D与3D信息的无缝连接，从而兼具全局与局部信息。

Result: 在REAL275数据集实验表明，THE-Pose相较于3D-GC基线（HS-Pose）提升了35.8%，综合指标上超越以往最新方法7.2%。方法在应对不可见和复杂物体、遮挡等情况时表现优秀。

Conclusion: 通过引入拓扑特征并融合2D与3D信息，THE-Pose显著提升了类别级6D物体位姿估计的泛化性和鲁棒性，为复杂场景下的物体识别和定位提供了更优解决方案。

Abstract: Category-level object pose estimation requires both global context and local structure to ensure robustness against intra-class variations. However, 3D graph convolution (3D-GC) methods only focus on local geometry and depth information, making them vulnerable to complex objects and visual ambiguities. To address this, we present THE-Pose, a novel category-level 6D pose estimation framework that leverages a topological prior via surface embedding and hybrid graph fusion. Specifically, we extract consistent and invariant topological features from the image domain, effectively overcoming the limitations inherent in existing 3D-GC based methods. Our Hybrid Graph Fusion (HGF) module adaptively integrates the topological features with point-cloud features, seamlessly bridging 2D image context and 3D geometric structure. These fused features ensure stability for unseen or complicated objects, even under significant occlusions. Extensive experiments on the REAL275 dataset show that THE-Pose achieves a 35.8% improvement over the 3D-GC baseline (HS-Pose) and surpasses the previous state-of-the-art by 7.2% across all key metrics. The code is avaialbe on https://github.com/EHxxx/THE-Pose

</details>


### [16] [GDKVM: Echocardiography Video Segmentation via Spatiotemporal Key-Value Memory with Gated Delta Rule](https://arxiv.org/abs/2512.10252)
*Rui Wang,Yimu Sun,Jingxing Guo,Huisi Wu,Jing Qin*

Main category: cs.CV

TL;DR: 该论文提出了一种新型超声心动图序列分割架构GDKVM，在保持高分割精度和鲁棒性的同时实现了实时性能。


<details>
  <summary>Details</summary>
Motivation: 心腔精确分割对于心功能定量分析和临床诊断治疗至关重要，但由于成像噪声、伪影以及心脏形变和运动，分割任务极具挑战。已有方法在空间-时间依赖性建模和计算效率之间存在权衡，难以同时兼顾长期依赖捕获与细粒度特征表达。

Method: 本文提出GDKVM架构：（1）利用线性Key-Value关联（LKVA）高效捕捉帧间关联；（2）引入Gated Delta Rule（GDR）用于高效存储中间记忆状态；（3）设计关键像素特征融合模块（KPFF），融合多尺度局部与全局特征，增强对模糊边界及噪声的鲁棒性。方法在CAMUS和EchoNet-Dynamic两大主流数据集上进行了实验验证，并与多种SOTA方法进行了对比。

Result: 实验结果表明，GDKVM在分割准确性和鲁棒性方面均优于现有方法，同时保证了实时推理性能。

Conclusion: GDKVM为超声心动图视频分割任务提供了一种精度与效率兼顾的新方案，有望提升心脏功能自动化分析的临床应用价值。

Abstract: Accurate segmentation of cardiac chambers in echocardiography sequences is crucial for the quantitative analysis of cardiac function, aiding in clinical diagnosis and treatment. The imaging noise, artifacts, and the deformation and motion of the heart pose challenges to segmentation algorithms. While existing methods based on convolutional neural networks, Transformers, and space-time memory networks have improved segmentation accuracy, they often struggle with the trade-off between capturing long-range spatiotemporal dependencies and maintaining computational efficiency with fine-grained feature representation. In this paper, we introduce GDKVM, a novel architecture for echocardiography video segmentation. The model employs Linear Key-Value Association (LKVA) to effectively model inter-frame correlations, and introduces Gated Delta Rule (GDR) to efficiently store intermediate memory states. Key-Pixel Feature Fusion (KPFF) module is designed to integrate local and global features at multiple scales, enhancing robustness against boundary blurring and noise interference. We validated GDKVM on two mainstream echocardiography video datasets (CAMUS and EchoNet-Dynamic) and compared it with various state-of-the-art methods. Experimental results show that GDKVM outperforms existing approaches in terms of segmentation accuracy and robustness, while ensuring real-time performance. Code is available at https://github.com/wangrui2025/GDKVM.

</details>


### [17] [VLM-NCD:Novel Class Discovery with Vision-Based Large Language Models](https://arxiv.org/abs/2512.10262)
*Yuetong Su,Baoguo Wei,Xinyu Wang,Xu Li,Lixin Li*

Main category: cs.CV

TL;DR: 本文提出了一种融合视觉与文本语义的多模态新类发现方法LLM-NCD，相较已有方法在未知类精度上实现了显著提升，尤其在长尾分布下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有新类发现方法主要依赖于视觉特征，受限于特征区分能力不足和数据长尾分布问题，因此需要融合更多模态信息以提升未知类别识别能力。

Method: 提出LLM-NCD多模态框架，采用视觉-文本语义融合，通过对已知类别的图像和文本特征联合优化，构建簇中心和语义原型，并提出双阶段发现机制：基于语义相似度阈值和自适应聚类动态区分已知和未知样本。

Result: 在CIFAR-100数据集上，与现有方法相比，LLM-NCD在未知类别上准确率提升最高达25.3%。在NCD领域内首次展示出对长尾分布的独特鲁棒性。

Conclusion: 融合视觉和文本多模态特征，并引入新的发现机制能有效提升新类发现的准确性，特别是在长尾分布情境下具有显著优势，为NCD研究提供了新方向。

Abstract: Novel Class Discovery aims to utilise prior knowledge of known classes to classify and discover unknown classes from unlabelled data. Existing NCD methods for images primarily rely on visual features, which suffer from limitations such as insufficient feature discriminability and the long-tail distribution of data. We propose LLM-NCD, a multimodal framework that breaks this bottleneck by fusing visual-textual semantics and prototype guided clustering. Our key innovation lies in modelling cluster centres and semantic prototypes of known classes by jointly optimising known class image and text features, and a dualphase discovery mechanism that dynamically separates known or novel samples via semantic affinity thresholds and adaptive clustering. Experiments on the CIFAR-100 dataset show that compared to the current methods, this method achieves up to 25.3% improvement in accuracy for unknown classes. Notably, our method shows unique resilience to long tail distributions, a first in NCD literature.

</details>


### [18] [Long-LRM++: Preserving Fine Details in Feed-Forward Wide-Coverage Reconstruction](https://arxiv.org/abs/2512.10267)
*Chen Ziwen,Hao Tan,Peng Wang,Zexiang Xu,Li Fuxin*

Main category: cs.CV

TL;DR: Long-LRM++提出了一种结合半显式场景表示和轻量级解码器的新方法，实现了高品质、实时的场景渲染，并显著提升了输入视图数和深度预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有通用高斯点云（GS）实现了多视角场景的高效重建，但直接预测大量高斯参数对细节敏感，导致模糊等问题。隐式表示（如LVSM、LaCT）虽能提升渲染精度，但极大消耗计算资源，难以实时渲染。因此，作者致力于解决如何兼具隐式表示的高质量和显式方法的实时性能的问题。

Method: Long-LRM++采用半显式场景表示，将部分信息以显式参数保存，同时结合轻量级解码器减少“解压”过程的计算负担，兼顾渲染速度与质量。模型经过端到端训练，并扩展支持多达64张高分辨率输入，针对新视角深度预测也进行了优化。

Result: Long-LRM++在DL3DV数据集上的渲染质量媲美国领先方法LaCT，但渲染速度实现了14 FPS的实时效果（A100 GPU）。在ScanNetv2上的新视角深度预测性能优于直接由高斯表示生成的深度图。同时，支持更多输入视角且具有良好的泛化能力。

Conclusion: 该工作成功结合了隐式与显式表示的优点，实现了高质量、实时的场景重建与渲染，推动了相关技术在实际应用中的落地。

Abstract: Recent advances in generalizable Gaussian splatting (GS) have enabled feed-forward reconstruction of scenes from tens of input views. Long-LRM notably scales this paradigm to 32 input images at $950\times540$ resolution, achieving 360° scene-level reconstruction in a single forward pass. However, directly predicting millions of Gaussian parameters at once remains highly error-sensitive: small inaccuracies in positions or other attributes lead to noticeable blurring, particularly in fine structures such as text. In parallel, implicit representation methods such as LVSM and LaCT have demonstrated significantly higher rendering fidelity by compressing scene information into model weights rather than explicit Gaussians, and decoding RGB frames using the full transformer or TTT backbone. However, this computationally intensive decompression process for every rendered frame makes real-time rendering infeasible. These observations raise key questions: Is the deep, sequential "decompression" process necessary? Can we retain the benefits of implicit representations while enabling real-time performance? We address these questions with Long-LRM++, a model that adopts a semi-explicit scene representation combined with a lightweight decoder. Long-LRM++ matches the rendering quality of LaCT on DL3DV while achieving real-time 14 FPS rendering on an A100 GPU, overcoming the speed limitations of prior implicit methods. Our design also scales to 64 input views at the $950\times540$ resolution, demonstrating strong generalization to increased input lengths. Additionally, Long-LRM++ delivers superior novel-view depth prediction on ScanNetv2 compared to direct depth rendering from Gaussians. Extensive ablation studies validate the effectiveness of each component in the proposed framework.

</details>


### [19] [Sample-wise Adaptive Weighting for Transfer Consistency in Adversarial Distillation](https://arxiv.org/abs/2512.10275)
*Hongsin Lee,Hye Won Chung*

Main category: cs.CV

TL;DR: 本文发现从更强健的教师网络向学生迁移对抗鲁棒性时存在“鲁棒饱和”现象，即更强教师并不一定带来更鲁棒的学生。提出了样本自适应权重的对抗蒸馏方法SAAD，显著提升了学生网络的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗蒸馏方法常忽视教师网络的鲁棒性对学生鲁棒性的实际提升作用，有时更强健的教师并不能迁移出更强健的学生。这一矛盾促使作者重新审视对抗鲁棒性迁移的关键影响因素。

Method: 作者通过实验分析发现，对抗样本从学生到教师的可迁移性是鲁棒性迁移成功的关键。基于此，提出Sample-wise Adaptive Adversarial Distillation (SAAD) 方法：根据每个样本的对抗可迁移性动态调整其训练权重，同时保证无额外计算开销。

Result: 在CIFAR-10、CIFAR-100和Tiny-ImageNet上实验证明，SAAD方法下学生网络的鲁棒性（AutoAttack测试）优于以往的对抗蒸馏方法。

Conclusion: SAAD能够有针对性地提升对抗鲁棒性的迁移效果，突破了传统鲁棒饱和的瓶颈，为对抗训练和鲁棒蒸馏提供了新的有效途径。

Abstract: Adversarial distillation in the standard min-max adversarial training framework aims to transfer adversarial robustness from a large, robust teacher network to a compact student. However, existing work often neglects to incorporate state-of-the-art robust teachers. Through extensive analysis, we find that stronger teachers do not necessarily yield more robust students-a phenomenon known as robust saturation. While typically attributed to capacity gaps, we show that such explanations are incomplete. Instead, we identify adversarial transferability-the fraction of student-crafted adversarial examples that remain effective against the teacher-as a key factor in successful robustness transfer. Based on this insight, we propose Sample-wise Adaptive Adversarial Distillation (SAAD), which reweights training examples by their measured transferability without incurring additional computational cost. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet show that SAAD consistently improves AutoAttack robustness over prior methods. Our code is available at https://github.com/HongsinLee/saad.

</details>


### [20] [MotionEdit: Benchmarking and Learning Motion-Centric Image Editing](https://arxiv.org/abs/2512.10284)
*Yixin Wan,Lei Ke,Wenhao Yu,Kai-Wei Chang,Dong Yu*

Main category: cs.CV

TL;DR: 本文提出了MotionEdit数据集和相应的MotionEdit-Bench基准，专注于实现图像中主体动作和互动的高质量、真实感编辑，同时确保身份、结构和物理合理性得以保持。为弥补现有扩散模型在运动编辑任务中的不足，作者提出MotionNFT微调方法，有效提升运动编辑质量。


<details>
  <summary>Details</summary>
Motivation: 当前图像编辑数据集多关注静态外观变化，而对主体动作和交互等动态编辑支持不足，且现有相关数据质量较低。由于精准控制图像动作变换在视频合成、动画制作等应用中的实际价值，亟需高质量数据集和有效模型解决运动编辑挑战。

Method: 1）构建MotionEdit数据集，收集并验证来自连续视频的高保真运动变换图像对；2）开发MotionEdit-Bench基准，提供多维度评测指标；3）提出MotionNFT后训练框架，通过评估模型结果与输入之间的运动流与真实运动的一致性，进行奖励引导微调，提升扩散模型的运动编辑能力。

Result: 实验表明当前扩散模型在MotionEdit任务上表现不佳。通过在FLUX.1 Kontext和Qwen-Image-Edit两个主流模型中应用MotionNFT，均显著提升了运动编辑质量及动作还原真实度，同时保持了模型的通用编辑能力。

Conclusion: MotionEdit数据集及MotionEdit-Bench为运动中心图像编辑提供了可靠的测评基础。所提出的MotionNFT框架在不影响模型其他能力的前提下，有效增强了运动编辑性能，对未来智能视觉编辑有重要推动意义。

Abstract: We introduce MotionEdit, a novel dataset for motion-centric image editing-the task of modifying subject actions and interactions while preserving identity, structure, and physical plausibility. Unlike existing image editing datasets that focus on static appearance changes or contain only sparse, low-quality motion edits, MotionEdit provides high-fidelity image pairs depicting realistic motion transformations extracted and verified from continuous videos. This new task is not only scientifically challenging but also practically significant, powering downstream applications such as frame-controlled video synthesis and animation.
  To evaluate model performance on the novel task, we introduce MotionEdit-Bench, a benchmark that challenges models on motion-centric edits and measures model performance with generative, discriminative, and preference-based metrics. Benchmark results reveal that motion editing remains highly challenging for existing state-of-the-art diffusion-based editing models. To address this gap, we propose MotionNFT (Motion-guided Negative-aware Fine Tuning), a post-training framework that computes motion alignment rewards based on how well the motion flow between input and model-edited images matches the ground-truth motion, guiding models toward accurate motion transformations. Extensive experiments on FLUX.1 Kontext and Qwen-Image-Edit show that MotionNFT consistently improves editing quality and motion fidelity of both base models on the motion editing task without sacrificing general editing ability, demonstrating its effectiveness.

</details>


### [21] [ShotDirector: Directorially Controllable Multi-Shot Video Generation with Cinematographic Transitions](https://arxiv.org/abs/2512.10286)
*Xiaoxue Wu,Xinyuan Chen,Yaohui Wang,Yu Qiao*

Main category: cs.CV

TL;DR: 本文提出了ShotDirector框架，通过参数级摄像机控制和分层编辑模式感知提升影片级多镜头生成中的剪辑和叙事表达。


<details>
  <summary>Details</summary>
Motivation: 现有多镜头视频生成方法仅关注镜头间视觉一致性，忽视了镜头切换的导演意图和电影剪辑语言，导致生成的镜头仅是机械衔接，缺乏电影化连贯表达。

Method: 提出ShotDirector框架，融合了6自由度摄像机参数控制和内参数设置以实现精确镜头信息注入；引入分层的shot-aware mask机制，根据专业剪辑规律生成分层提示，实现对镜头内容的细致控制。同时，构建了ShotWeaver40K数据集和新的评测指标，用于优化和评估多镜头生成效果。

Result: 通过大量实验，证明所提框架在实现可控、电影化镜头切换及内容连贯性方面优于传统方法。新数据集和评测指标也有效支持了模型训练与评估。

Conclusion: ShotDirector框架通过结合低层次参数控制和高层次语义引导，实现了更具导演意图和电影语言的可控多镜头视频生成，为自动化电影剪辑和智能视频创作提供了新方法。

Abstract: Shot transitions play a pivotal role in multi-shot video generation, as they determine the overall narrative expression and the directorial design of visual storytelling. However, recent progress has primarily focused on low-level visual consistency across shots, neglecting how transitions are designed and how cinematographic language contributes to coherent narrative expression. This often leads to mere sequential shot changes without intentional film-editing patterns. To address this limitation, we propose ShotDirector, an efficient framework that integrates parameter-level camera control and hierarchical editing-pattern-aware prompting. Specifically, we adopt a camera control module that incorporates 6-DoF poses and intrinsic settings to enable precise camera information injection. In addition, a shot-aware mask mechanism is employed to introduce hierarchical prompts aware of professional editing patterns, allowing fine-grained control over shot content. Through this design, our framework effectively combines parameter-level conditions with high-level semantic guidance, achieving film-like controllable shot transitions. To facilitate training and evaluation, we construct ShotWeaver40K, a dataset that captures the priors of film-like editing patterns, and develop a set of evaluation metrics for controllable multi-shot video generation. Extensive experiments demonstrate the effectiveness of our framework.

</details>


### [22] [Physically Aware 360$^\circ$ View Generation from a Single Image using Disentangled Scene Embeddings](https://arxiv.org/abs/2512.10293)
*Karthikeya KV,Narendra Bandaru*

Main category: cs.CV

TL;DR: Disentangled360是一种新颖的3D感知技术，可将单张图片合成360°独特视角，并用于医学影像与自然场景重建。其核心是区分各向同性与各向异性光线贡献，提升真实感和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有技术对光的行为简化过度或在不同场景下通用性不足，难以真实还原复杂场景，尤其在医学和自然场景重建领域。

Method: 基于Gaussian Splatting骨干，设计了双分支结构：一支针对CT体数据中的光散射（采用CT强度），另一支针对现实RGB场景（用归一化摄像机嵌入）。此外，提出混合姿态无关锚定方法，以自适应采样实现结构稳定。整体系统融合术前X线仿真和消费级360°渲染于单次推理中。

Result: 在Mip-NeRF 360、RealEstate10K和DeepDRR数据集上，SSIM和LPIPS等指标优于现有方法，且推理速度足以满足交互式应用需求。

Conclusion: Disentangled360无需针对特定场景微调或昂贵的光子仿真，即可为医疗混合现实监督、机器人感知与沉浸式内容创造提供快速、高真实度渲染。

Abstract: We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360° unique view synthesis for applications in medical imaging and natural scene reconstruction. In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone. We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings. To address scale ambiguity and maintain structural realism, we present a hybrid pose agnostic anchoring method that adaptively samples scene depth and material transitions, functioning as stable pivots during scene distillation. Our design integrates preoperative radiography simulation and consumer-grade 360° rendering into a singular inference pipeline, facilitating rapid, photorealistic view synthesis with inherent directionality. Evaluations on the Mip-NeRF 360, RealEstate10K, and DeepDRR datasets indicate superior SSIM and LPIPS performance, while runtime assessments confirm its viability for interactive applications. Disentangled360 facilitates mixed-reality medical supervision, robotic perception, and immersive content creation, eliminating the necessity for scene-specific finetuning or expensive photon simulations.

</details>


### [23] [Any4D: Unified Feed-Forward Metric 4D Reconstruction](https://arxiv.org/abs/2512.10935)
*Jay Karhade,Nikhil Keetha,Yuchen Zhang,Tanisha Gupta,Akash Sharma,Sebastian Scherer,Deva Ramanan*

Main category: cs.CV

TL;DR: Any4D是一种可扩展的多视图Transformer方法，用于大尺度、稠密的4D重建，能直接对N帧生成逐像素的运动和几何预测，支持多传感器输入，并在准确率和效率上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前4D重建主要存在对大尺度、稠密运动与几何难以同时高效预测的问题，大多数仅限于双视图或稀疏点跟踪，且多为单一模态输入。因此，亟需一个既能处理多帧多模态输入、又可实现高效准确4D场景重建的新框架。

Method: 提出了一种多视图Transformer架构Any4D，直接对N帧影像做逐像素运动与几何预测。创新性地将场景中的4D信息划分为以深度图和内参为主的自中心因子（局部坐标），以及以外参和场景流为主的他中心因子（全球坐标），并利用模块化编码方式以灵活集成多种模态输入（如RGB-D、IMU运动、雷达等）。

Result: Any4D在精度上实现了2-3倍误差降低，在计算效率上达到了15倍加速，在多种数据输入与实验设置下均表现突出。

Conclusion: Any4D为4D场景重建提供了一个灵活高效的统一框架，不仅在性能上优于现有方法，也为各类下游应用（如自动驾驶、机器人感知等）奠定了坚实的基础。

Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.

</details>


### [24] [Efficient-VLN: A Training-Efficient Vision-Language Navigation Model](https://arxiv.org/abs/2512.10310)
*Duo Zheng,Shijia Huang,Yanyang Li,Liwei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视觉-语言导航（VLN）模型Efficient-VLN，通过改进记忆机制和策略设计，显著降低了多模态大模型的训练开销，同时提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在视觉-语言导航任务中效果突出，但由于需处理大量历史观测信息，计算量极大且训练成本高昂。此外，数据聚合过程中的探索效率和平衡也加剧了训练负担。

Method: 作者提出Efficient-VLN模型，具体包括：（1）两种高效的记忆机制——递进式记忆分配最近观测更多token，递归式可学习记忆利用可学习token的key-value cache降低历史处理负担；（2）动态混合策略，在探索与效率之间进行自适应平衡。

Result: Efficient-VLN在R2R-CE和RxR-CE数据集上分别取得64.2%和67.0%的state-of-the-art成功率（SR），且只需282小时H800 GPU训练时间，远低于同类方法。

Conclusion: Efficient-VLN在保证甚至提升了导航性能的同时，大幅度降低了多模态导航模型的训练资源消耗，对实际应用具有重要意义。

Abstract: Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods.

</details>


### [25] [DualProtoSeg: Simple and Efficient Design with Text- and Image-Guided Prototype Learning for Weakly Supervised Histopathology Image Segmentation](https://arxiv.org/abs/2512.10314)
*Anh M. Vu,Khang P. Le,Trang T. K. Vo,Ha Thach,Huy Hung Nguyen,David Yang,Han H. Huynh,Quynh Nguyen,Tuan M. Pham,Tuan-Anh Le,Minh H. N. Le,Thanh-Huy Nguyen,Akash Awasthi,Chandra Mohan,Zhu Han,Hien Van Nguyen*

Main category: cs.CV

TL;DR: 提出了一种结合视觉和语言原型的新框架，在病理学弱监督语义分割任务中性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的CAM类弱监督方法在病理图像中存在类别同质性高、类内异质性强及区域收缩等问题，限制了分割质量。因此，需要设计新方法克服上述瓶颈，提高弱监督下的区域识别能力。

Method: 提出了一种原型驱动的框架：1）利用CoOp风格的可学习Prompt生成文本原型；2）与可学习的图像原型结合，组成双模态原型库，捕捉语义和外观信息；3）引入多尺度金字塔模块，缓解ViT在空间分辨率上的过平滑问题，提高定位精度。

Result: 在BCSS-WSSS数据集上，该方法在分割性能上超过了已有的所有先进方法。进一步分析表明，文本描述的多样性、上下文长度，以及文本和图像原型的互补作用均带来明显性能提升。

Conclusion: 联合利用文本语义信息和视觉原型使得弱监督语义分割在数字病理学中的效果显著提升，证明了该框架的有效性。

Abstract: Weakly supervised semantic segmentation (WSSS) in histopathology seeks to reduce annotation cost by learning from image-level labels, yet it remains limited by inter-class homogeneity, intra-class heterogeneity, and the region-shrinkage effect of CAM-based supervision. We propose a simple and effective prototype-driven framework that leverages vision-language alignment to improve region discovery under weak supervision. Our method integrates CoOp-style learnable prompt tuning to generate text-based prototypes and combines them with learnable image prototypes, forming a dual-modal prototype bank that captures both semantic and appearance cues. To address oversmoothing in ViT representations, we incorporate a multi-scale pyramid module that enhances spatial precision and improves localization quality. Experiments on the BCSS-WSSS benchmark show that our approach surpasses existing state-of-the-art methods, and detailed analyses demonstrate the benefits of text description diversity, context length, and the complementary behavior of text and image prototypes. These results highlight the effectiveness of jointly leveraging textual semantics and visual prototype learning for WSSS in digital pathology.

</details>


### [26] [ConStruct: Structural Distillation of Foundation Models for Prototype-Based Weakly Supervised Histopathology Segmentation](https://arxiv.org/abs/2512.10316)
*Khang Le,Ha Thach,Anh M. Vu,Trang T. K. Vo,Han H. Huynh,David Yang,Minh H. N. Le,Thanh-Huy Nguyen,Akash Awasthi,Chandra Mohan,Zhu Han,Hien Van Nguyen*

Main category: cs.CV

TL;DR: 提出了一种结合视觉-语言模型和现代分割主干网络的原型学习框架，提升了病理图像弱监督语义分割的空间完整性与语义一致性，无需像素级标注即可获得高质量分割结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于分类主干网络的病理图像弱监督语义分割方法只能局部定位最具判别性的区域，难以捕捉组织结构的完整空间分布。引入视觉-语言模型（如CONCH）和先进分割网络（如SegFormer）虽具互补优势，但在缺乏密集标注下难以协同利用其能力。

Method: 提出了结合CONCH的形态感知表示、SegFormer的多尺度空间结构信息以及文本引导的语义校准的原型学习方法。具体包括：文本引导的原型初始化（利用病理描述提升伪mask的完整性和语义准确性）、结构蒸馏机制（将SegFormer的空间知识迁移到原型学习过程以保留细粒度结构边界），通过冻结大模型主干和训练轻量适配器实现高效学习。

Result: 无需像素级标注的情况下框架能生成高质量伪mask，提高了结构完整性和不同组织类型间的语义一致性。实验在BCSS-WSSS数据集上优于现有WSSS方法，且具备计算高效性。

Conclusion: 该原型学习框架为病理图像弱监督语义分割提供了一种有效且高效的新方案，能够充分利用大模型主干知识，显著提升分割质量与泛化能力。

Abstract: Weakly supervised semantic segmentation (WSSS) in histopathology relies heavily on classification backbones, yet these models often localize only the most discriminative regions and struggle to capture the full spatial extent of tissue structures. Vision-language models such as CONCH offer rich semantic alignment and morphology-aware representations, while modern segmentation backbones like SegFormer preserve fine-grained spatial cues. However, combining these complementary strengths remains challenging, especially under weak supervision and without dense annotations. We propose a prototype learning framework for WSSS in histopathological images that integrates morphology-aware representations from CONCH, multi-scale structural cues from SegFormer, and text-guided semantic alignment to produce prototypes that are simultaneously semantically discriminative and spatially coherent. To effectively leverage these heterogeneous sources, we introduce text-guided prototype initialization that incorporates pathology descriptions to generate more complete and semantically accurate pseudo-masks. A structural distillation mechanism transfers spatial knowledge from SegFormer to preserve fine-grained morphological patterns and local tissue boundaries during prototype learning. Our approach produces high-quality pseudo masks without pixel-level annotations, improves localization completeness, and enhances semantic consistency across tissue types. Experiments on BCSS-WSSS datasets demonstrate that our prototype learning framework outperforms existing WSSS methods while remaining computationally efficient through frozen foundation model backbones and lightweight trainable adapters.

</details>


### [27] [Point2Pose: A Generative Framework for 3D Human Pose Estimation with Multi-View Point Cloud Dataset](https://arxiv.org/abs/2512.10321)
*Hyunsoo Lee,Daeum Jeon,Hyeokjae Oh*

Main category: cs.CV

TL;DR: 本文提出了Point2Pose，一个基于生成式框架的3D人体姿态估计方法，通过时空点云与姿态历史建模人体姿态分布，并引入了新的大规模多模态数据集MVPose3D。


<details>
  <summary>Details</summary>
Motivation: 3D人体姿态估计因人体结构复杂、自遮挡及对大量真实运动数据的需求而面临重大挑战。为解决这些问题，作者寻求有效利用点云和历史信息，同时扩展数据资源。

Method: 方法上，提出了Point2Pose框架，包括时空点云编码器和姿态特征编码器用于提取关节特征，之后通过注意力生成回归器实现姿态估计。同时提供了包含点云、IMU数据和RGB图像的MVPose3D数据集。

Result: 实验表明，所提方法在不同数据集上均优于现有基线模型，在精度和多样性方面表现突出。

Conclusion: Point2Pose能更好地利用多模态信息和历史上下文，提高3D人体姿态估计的准确性和泛化能力，且新数据集为领域发展提供更多支持。

Abstract: We propose a novel generative approach for 3D human pose estimation. 3D human pose estimation poses several key challenges due to the complex geometry of the human body, self-occluding joints, and the requirement for large-scale real-world motion datasets. To address these challenges, we introduce Point2Pose, a framework that effectively models the distribution of human poses conditioned on sequential point cloud and pose history. Specifically, we employ a spatio-temporal point cloud encoder and a pose feature encoder to extract joint-wise features, followed by an attention-based generative regressor. Additionally, we present a large-scale indoor dataset MVPose3D, which contains multiple modalities, including IMU data of non-trivial human motions, dense multi-view point clouds, and RGB images. Experimental results show that the proposed method outperforms the baseline models, demonstrating its superior performance across various datasets.

</details>


### [28] [EchoingPixels: Cross-Modal Adaptive Token Reduction for Efficient Audio-Visual LLMs](https://arxiv.org/abs/2512.10324)
*Chao Gong,Depeng Wang,Zhipeng Wei,Ya Guo,Huijia Zhu,Jingjing Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种名为EchoingPixels的新框架，用于解决音频-视觉大模型在处理大量音视频tokens时的高计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视频token压缩技术多为单模态，无法充分利用音-视协同信息，且对音频和视频的静态固定token预算不适用于两种模态信息稠密度差异和动态变化的特点。因此，如何在联合的音-视流中高效降维，成为亟需解决的难题。

Method: 作者提出了跨模态语义筛CS2（Cross-Modal Semantic Sieve），使音频与视频早期交互，通过在联合音频-视频token池中协同筛选重要token，而非分别对每个模态压缩，动态适应各模态重要信息分布。此外引入Sync-RoPE模块，保留稀疏tokens中的关键时间关系。

Result: 大量实验显示，EchoingPixels在仅用5-20%的原始token情况下，性能可与强基线持平，同时速度和内存开销提升2-3倍。

Conclusion: EchoingPixels创新地实现了高效的音-视联合token压缩，显著降低计算开销且不损失性能，为音频-视觉大模型的实际应用提供了有效支持。

Abstract: Audio-Visual Large Language Models (AV-LLMs) face prohibitive computational overhead from massive audio and video tokens. Token reduction, while extensively explored for video-only LLMs, is insufficient for the audio-visual domain, as these unimodal methods cannot leverage audio-visual cross-modal synergies. Furthermore, the distinct and dynamic information densities of audio and video render static budgets per modality suboptimal. How to perform token reduction on a joint audio-visual stream thus remains an unaddressed bottleneck. To fill this gap, we introduce EchoingPixels, a framework inspired by the coexistence and interaction of visuals and sound in real-world scenes. The core of our framework is the Cross-Modal Semantic Sieve (CS2), a module enabling early audio-visual interaction. Instead of compressing modalities independently, CS2 co-attends to the joint multimodal stream and reduces tokens from an entire combined pool of audio-visual tokens rather than using fixed budgets per modality. This single-pool approach allows it to adaptively allocate the token budget across both modalities and dynamically identify salient tokens in concert. To ensure this aggressive reduction preserves the vital temporal modeling capability, we co-design a Synchronization-Augmented RoPE (Sync-RoPE) to maintain critical temporal relationships for the sparsely selected tokens. Extensive experiments demonstrate that EchoingPixels achieves performance comparable to strong baselines using only 5-20% of the original tokens, with a 2-3x speedup and memory reduction.

</details>


### [29] [StainNet: A Special Staining Self-Supervised Vision Transformer for Computational Pathology](https://arxiv.org/abs/2512.10326)
*Jiawen Li,Jiali Hu,Xitong Ling,Yongqiang Lv,Yuxuan Chen,Yizhi Wang,Tian Guan,Yifei Liu,Yonghong He*

Main category: cs.CV

TL;DR: 本文提出了StainNet，一种专为特殊染色病理图像设计的基础模型，基于ViT架构并利用自蒸馏自监督学习，在超过1.4百万个特殊染色切片上预训练，实验显示其在多种下游任务上表现优异，可为临床特殊染色图像分析提供新工具。


<details>
  <summary>Details</summary>
Motivation: 现有的病理基础模型多基于H&E染色情况，而实际临床中特殊染色（如免疫组化）也非常常见，现有模型在处理特殊染色时存在局限，需要专门针对这些数据的基础模型。

Method: 作者提出StainNet，采用视觉Transformer（ViT）架构，通过自蒸馏自监督学习方法，在20,231例公开特殊染色全切片的1.4M多个patch上进行训练。模型在自有的肝脏恶性肿瘤片段分类任务和两个公开ROI任务中进行评估，并与当前大型模型比较。

Result: StainNet在肝脏恶性肿瘤分类和其他ROI级别数据集上的表现均优于对比的现有基础模型，并在少样本学习和特征检索任务中同样展现强大能力。

Conclusion: StainNet能够显著提升对特殊染色病理图像的表征能力，弥补了H&E为主的模型难以直接覆盖特殊染色的局限，在相关下游任务和临床实际应用中具有重要意义。

Abstract: Foundation models trained with self-supervised learning (SSL) on large-scale histological images have significantly accelerated the development of computational pathology. These models can serve as backbones for region-of-interest (ROI) image analysis or patch-level feature extractors in whole-slide images (WSIs) based on multiple instance learning (MIL). Existing pathology foundation models (PFMs) are typically pre-trained on Hematoxylin-Eosin (H&E) stained pathology images. However, images with special stains, such as immunohistochemistry, are also frequently used in clinical practice. PFMs pre-trained mainly on H\&E-stained images may be limited in clinical applications involving special stains. To address this issue, we propose StainNet, a specialized foundation model for special stains based on the vision transformer (ViT) architecture. StainNet adopts a self-distillation SSL approach and is trained on over 1.4 million patch images cropping from 20,231 publicly available special staining WSIs in the HISTAI database. To evaluate StainNet, we conduct experiments on an in-house slide-level liver malignancy classification task and two public ROI-level datasets to demonstrate its strong ability. We also perform few-ratio learning and retrieval evaluations, and compare StainNet with recently larger PFMs to further highlight its strengths. We have released the StainNet model weights at: https://huggingface.co/JWonderLand/StainNet.

</details>


### [30] [Simple Yet Effective Selective Imputation for Incomplete Multi-view Clustering](https://arxiv.org/abs/2512.10327)
*Cai Xu,Jinlong Liu,Yilin Zhang,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为ISMVC的多视图聚类新方法，能在多视图数据严重缺失且不平衡的情况下，选择性地对信息充足的位置进行补全，有效提升聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法在处理各视图数据缺失且不均衡时面临两难：盲目补全易引入噪声和偏差，完全依赖已观测数据又利用不足，导致聚类性能下降。

Method: ISMVC方法首先评估每个缺失位置的补全相关信息性（基于视图内相似性和跨视图一致性），只在信息充分时才选择性补全。再结合带混合高斯先验的变分自编码器，实现分布级的补全和不确定性建模，从而学习更适合聚类的潜在表示。该方法轻量、数据驱动、与模型无关，可作为插件集成至现有不完整多视图聚类模型中。

Result: 在多个标准数据集上，ISMVC在更真实、更具挑战性的非均衡缺失场景下，聚类效果优于主流补全型与非补全型方法。

Conclusion: ISMVC兼具补全准确性与补全可靠性，通过选择性补全和显式建模不确定性，实现了多视图数据下更鲁棒的聚类性能，为不完整多视图学习提供了有效解决方案。

Abstract: Incomplete multi-view data, where different views suffer from missing and unbalanced observations, pose significant challenges for clustering. Existing imputation-based methods attempt to estimate missing views to restore data associations, but indiscriminate imputation often introduces noise and bias, especially when the available information is insufficient. Imputation-free methods avoid this risk by relying solely on observed data, but struggle under severe incompleteness due to the lack of cross-view complementarity. To address this issue, we propose Informativeness-based Selective imputation Multi-View Clustering (ISMVC). Our method evaluates the imputation-relevant informativeness of each missing position based on intra-view similarity and cross-view consistency, and selectively imputes only when sufficient support is available. Furthermore, we integrate this selection with a variational autoencoder equipped with a mixture-of-Gaussians prior to learn clustering-friendly latent representations. By performing distribution-level imputation, ISMVC not only stabilizes the aggregation of posterior distributions but also explicitly models imputation uncertainty, enabling robust fusion and preventing overconfident reconstructions. Compared with existing cautious imputation strategies that depend on training dynamics or model feedback, our method is lightweight, data-driven, and model-agnostic. It can be readily integrated into existing IMC models as a plug-in module. Extensive experiments on multiple benchmark datasets under a more realistic and challenging unbalanced missing scenario demonstrate that our method outperforms both imputation-based and imputation-free approaches.

</details>


### [31] [A Conditional Generative Framework for Synthetic Data Augmentation in Segmenting Thin and Elongated Structures in Biological Images](https://arxiv.org/abs/2512.10334)
*Yi Liu,Yichi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种利用生成对抗网络（Pix2Pix架构）合成显微镜下丝状结构图像的方法，通过合成数据缓解了人工标注数据不足的问题，并设计了针对丝状结构的结构损失函数以提升合成图像的结构相似性。实验结果证明，该方法有效提升了下游分割模型的性能。


<details>
  <summary>Details</summary>
Motivation: 丝状结构如微管和肌动蛋白丝在生物系统中非常重要。对这些结构的图像分割是后续定量分析的基础。但由于其密集分布和复杂形态，像素级数据标注非常繁琐，导致数据集稀缺，这严重限制了深度学习在该任务上的应用。

Method: 作者基于Pix2Pix架构提出条件生成模型，通过二值掩膜生成显微镜下真实感丝状图像，并提出了丝状结构感知的结构损失函数以提升合成数据的结构准确性。利用合成数据丰富训练集，减少对人工标注数据的依赖。

Result: 实验结果表明，使用合成数据训练的分割模型在性能上优于不使用合成数据的模型。所提出的结构损失函数进一步提升了合成图像的结构相似度。

Conclusion: 本文提出的基于Pix2Pix的生成方法及丝状结构感知损失，在缓解标注数据不足、提升分割模型性能等方面效果显著，有望推广至更多生物图像分析的场景。

Abstract: Thin and elongated filamentous structures, such as microtubules and actin filaments, often play important roles in biological systems. Segmenting these filaments in biological images is a fundamental step for quantitative analysis. Recent advances in deep learning have significantly improved the performance of filament segmentation. However, there is a big challenge in acquiring high quality pixel-level annotated dataset for filamentous structures, as the dense distribution and geometric properties of filaments making manual annotation extremely laborious and time-consuming. To address the data shortage problem, we propose a conditional generative framework based on the Pix2Pix architecture to generate realistic filaments in microscopy images from binary masks. We also propose a filament-aware structural loss to improve the structure similarity when generating synthetic images. Our experiments have demonstrated the effectiveness of our approach and outperformed existing model trained without synthetic data.

</details>


### [32] [Zero-shot Adaptation of Stable Diffusion via Plug-in Hierarchical Degradation Representation for Real-World Super-Resolution](https://arxiv.org/abs/2512.10340)
*Yi-Cheng Liao,Shyang-En Weng,Yu-Syuan Xu,Chi-Wei Hsiao,Wei-Chen Chiu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: HD-CLIP是一种新型模块，通过将低质图像分离成语义和有序退化嵌入，提升扩散模型对真实图像超分辨率的表现，无需训练即可显著提升细节和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有真实图像超分辨率方法大多假定已知退化程度，并依赖于无法精确捕捉数值退化的CLIP文本编码器，导致泛化能力受限。为解决多样且复合真实退化问题，亟需为扩散模型提供更丰富的信息引导。

Method: 提出HD-CLIP，将低质图像分解为语义嵌入与有序退化嵌入（可捕捉退化等级并实现插值）。该模块通过无分类器引导（CFG）与无分类器投影引导（CFPG）集成入扩散模型。语义信息用于还原生成，退化信息则抑制不良臆测和伪影。HD-CLIP模块可插拔，无需再训练。

Result: HD-CLIP能稳健集成于多种超分辨率框架中，在多类真实世界数据集上，显著提升细节保真度和感知真实度。

Conclusion: HD-CLIP作为无训练、即插即用模块，大幅提升真实图像超分辨率的泛化能力和视觉质量，是现实场景下高效实用的创新方案。

Abstract: Real-World Image Super-Resolution (Real-ISR) aims to recover high-quality images from low-quality inputs degraded by unknown and complex real-world factors. Real-world scenarios involve diverse and coupled degradations, making it necessary to provide diffusion models with richer and more informative guidance. However, existing methods often assume known degradation severity and rely on CLIP text encoders that cannot capture numerical severity, limiting their generalization ability. To address this, we propose \textbf{HD-CLIP} (\textbf{H}ierarchical \textbf{D}egradation CLIP), which decomposes a low-quality image into a semantic embedding and an ordinal degradation embedding that captures ordered relationships and allows interpolation across unseen levels. Furthermore, we integrated it into diffusion models via classifier-free guidance (CFG) and proposed classifier-free projection guidance (CFPG). HD-CLIP leverages semantic cues to guide generative restoration while using degradation cues to suppress undesired hallucinations and artifacts. As a \textbf{plug-and-play module}, HD-CLIP can be seamlessly integrated into various super-resolution frameworks without training, significantly improving detail fidelity and perceptual realism across diverse real-world datasets.

</details>


### [33] [CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates](https://arxiv.org/abs/2512.10342)
*Shresth Grover,Priyank Pathak,Akash Kumar,Vibhav Vineet,Yogesh S Rawat*

Main category: cs.CV

TL;DR: 本文提出了一个名为CoSPlan的新基准，用于评估视觉-语言模型（VLMs）在容易出错的视觉序列规划任务中的纠错与规划能力，并提出了一种新方法SGI，提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模视觉-语言模型在复杂推理任务中表现优异，但对其在多步视觉序列规划、尤其是实际中容易出现错误的情况下的纠错和继续规划能力研究较少。实际任务中常常会出现非最优甚至错误的步骤，因此需要评估和提升VLMs在此类任务中的表现。

Method: 作者提出了Corrective Sequential Planning Benchmark (CoSPlan) ，涉及四个领域（迷宫导航、积木重排、图像重构、物体重组），分别评估模型的错误检测与步骤完成能力。针对现有VLMs在此任务上表现不理想的问题，提出了一种无需额外训练的新方法SGI（Scene Graph Incremental updates），通过引入中间推理步骤，帮助模型更好地推理和纠错。

Result: 在CoSPlan上，即使采用了最先进的推理技巧（如Chain-of-Thought和Scene Graphs），主流VLMs（如Intern-VLM和Qwen2）表现仍然较差。使用SGI后，模型在序列推理任务上的平均性能提升了5.2%。

Conclusion: SGI方法显著提高了VLMs在纠错视觉序列规划中的可靠性，并且其泛化能力也体现在传统的规划和视觉问答任务（如Plan-Bench和VQA）上。

Abstract: Large-scale Vision-Language Models (VLMs) exhibit impressive complex reasoning capabilities but remain largely unexplored in visual sequential planning, i.e., executing multi-step actions towards a goal. Additionally, practical sequential planning often involves non-optimal (erroneous) steps, challenging VLMs to detect and correct such steps. We propose Corrective Sequential Planning Benchmark (CoSPlan) to evaluate VLMs in error-prone, vision-based sequential planning tasks across 4 domains: maze navigation, block rearrangement, image reconstruction,and object reorganization. CoSPlan assesses two key abilities: Error Detection (identifying non-optimal action) and Step Completion (correcting and completing action sequences to reach the goal). Despite using state-of-the-art reasoning techniques such as Chain-of-Thought and Scene Graphs, VLMs (e.g. Intern-VLM and Qwen2) struggle on CoSPlan, failing to leverage contextual cues to reach goals. Addressing this, we propose a novel training-free method, Scene Graph Incremental updates (SGI), which introduces intermediate reasoning steps between the initial and goal states. SGI helps VLMs reason about sequences, yielding an average performance gain of 5.2%. In addition to enhancing reliability in corrective sequential planning, SGI generalizes to traditional planning tasks such as Plan-Bench and VQA.

</details>


### [34] [Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation](https://arxiv.org/abs/2512.10949)
*Yiwen Tang,Zoey Guo,Kaixin Zhu,Ray Zhang,Qizhi Chen,Dongzhi Jiang,Junli Liu,Bohan Zeng,Haoming Song,Delin Qu,Tianyi Bai,Dan Xu,Wentao Zhang,Bin Zhao*

Main category: cs.CV

TL;DR: 本文系统性地研究了将强化学习（RL）用于文本到3D自回归生成，提出并分析了奖励设计、RL算法、基准测试及分层强化学习方法，并最终开发了首个RL增强的文本到3D模型AR3D-R1。


<details>
  <summary>Details</summary>
Motivation: 当前RL在大模型及2D生成任务中表现优异，但应用于3D生成领域几乎未被探索，主要因3D任务的空间复杂性及奖励设计难题。作者旨在填补该领域空白，探讨RL如何赋能3D生成。

Method: 工作包括：1）系统评价奖励设计方式及其对人类偏好和多模态模型信号的关联；2）分析并改进GRPO等RL算法，关注token级优化和数据规模；3）提出3D生成的全新评测基准MME-3DR，考查隐式推理能力；4）创新提出层次化RL paradigm（Hi-GRPO）提升结构到纹理全过程的优化。最终实现了AR3D-R1模型。

Result: 实验显示人类偏好相关奖励和强健的多模态模型显著提升3D生成效果，token级优化更高效，MME-3DR可更好评估隐式推理能力，分层策略有效提升了全流程性能。AR3D-R1为领域首个RL驱动的文本到3D解决方案。

Conclusion: 本研究首次将RL系统性引入文本到3D生成，提出新范式和模型AR3D-R1，为提升3D生成中的推理与表现力提供了全新思路和工具。

Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.

</details>


### [35] [Topology-Agnostic Animal Motion Generation from Text Prompt](https://arxiv.org/abs/2512.10352)
*Keyi Chen,Mingze Sun,Zhenyu Liu,Zhangquan Chen,Ruqi Huang*

Main category: cs.CV

TL;DR: 本论文提出了OmniZoo大规模动物运动数据集，并基于此开发了可泛化于任意骨架拓扑的文本驱动动作生成框架，解决了现有方法无法处理多样骨架结构的问题。


<details>
  <summary>Details</summary>
Motivation: 当前动作生成方法多依赖于固定的骨架模板，无法适应不同或变化的骨架拓扑结构。同时，缺乏大规模异质动物运动数据也限制了模型的泛化能力和发展。该论文旨在解决动作生成领域数据和模型泛化性双重短板问题。

Method: 作者构建了包含140种动物、32979条动作序列的OmniZoo异质动物运动数据集，并为其提供多模态标注。在此基础上，提出了一种泛化自回归动作生成框架，并设计了拓扑感知骨架嵌入模块，将任意骨架的几何和结构属性编码进共享token空间，实现与文本语义的融合。输入文本提示和目标骨架后，模型能生成与语义一致、时序连贯且物理合理的动作。

Result: 实验表明，所提方法能在多种不同骨架拓扑上实现文本驱动的动画生成，并支持跨不同物种的动作风格迁移，生成的运动具备良好的时序一致性和物理合理性。

Conclusion: OmniZoo数据集和新型动作生成框架显著推进了多骨架拓扑、跨物种的动作生成，并为多模态运动建模奠定了坚实基础。

Abstract: Motion generation is fundamental to computer animation and widely used across entertainment, robotics, and virtual environments. While recent methods achieve impressive results, most rely on fixed skeletal templates, which prevent them from generalizing to skeletons with different or perturbed topologies. We address the core limitation of current motion generation methods - the combined lack of large-scale heterogeneous animal motion data and unified generative frameworks capable of jointly modeling arbitrary skeletal topologies and textual conditions. To this end, we introduce OmniZoo, a large-scale animal motion dataset spanning 140 species and 32,979 sequences, enriched with multimodal annotations. Building on OmniZoo, we propose a generalized autoregressive motion generation framework capable of producing text-driven motions for arbitrary skeletal topologies. Central to our model is a Topology-aware Skeleton Embedding Module that encodes geometric and structural properties of any skeleton into a shared token space, enabling seamless fusion with textual semantics. Given a text prompt and a target skeleton, our method generates temporally coherent, physically plausible, and semantically aligned motions, and further enables cross-species motion style transfer.

</details>


### [36] [Hybrid Transformer-Mamba Architecture for Weakly Supervised Volumetric Medical Segmentation](https://arxiv.org/abs/2512.10353)
*Yiheng Lyu,Lian Xu,Mohammed Bennamoun,Farid Boussaid,Coen Arrow,Girish Dwivedi*

Main category: cs.CV

TL;DR: 提出了一种结合Transformer和Mamba结构的模型TranSamba，实现高效且高性能的弱监督三维医学影像分割，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督医学图像分割大多基于2D编码器，无法充分利用3D体积数据的特征，因此需设计一种既高效又能充分挖掘三维上下文结构的方法。

Method: TranSamba采用Vision Transformer为主干网络，并引入Cross-Plane Mamba块，利用状态空间模型高效处理3D体积数据，使相邻切片间信息有效交流，同时保持线性时间复杂度和常量内存消耗。

Result: 在三个不同的数据集上的实验表明，TranSamba在多种成像模态和疾病类型下均显著优于其它方法，取得了新的最优表现。

Conclusion: TranSamba不仅提升了弱监督三维医学分割的效果，同时大大降低了计算和内存成本，为实际应用提供了更可行且高效的解决方案。

Abstract: Weakly supervised semantic segmentation offers a label-efficient solution to train segmentation models for volumetric medical imaging. However, existing approaches often rely on 2D encoders that neglect the inherent volumetric nature of the data. We propose TranSamba, a hybrid Transformer-Mamba architecture designed to capture 3D context for weakly supervised volumetric medical segmentation. TranSamba augments a standard Vision Transformer backbone with Cross-Plane Mamba blocks, which leverage the linear complexity of state space models for efficient information exchange across neighboring slices. The information exchange enhances the pairwise self-attention within slices computed by the Transformer blocks, directly contributing to the attention maps for object localization. TranSamba achieves effective volumetric modeling with time complexity that scales linearly with the input volume depth and maintains constant memory usage for batch processing. Extensive experiments on three datasets demonstrate that TranSamba establishes new state-of-the-art performance, consistently outperforming existing methods across diverse modalities and pathologies. Our source code and trained models are openly accessible at: https://github.com/YihengLyu/TranSamba.

</details>


### [37] [mmCounter: Static People Counting in Dense Indoor Scenarios Using mmWave Radar](https://arxiv.org/abs/2512.10357)
*Tarik Reza Toha,Shao-Jung,Lu,Shahriar Nirjon*

Main category: cs.CV

TL;DR: mmCounter是一种基于毫米波雷达的静态人群计数系统，通过提取呼吸等微小身体运动信号，实现了在高密集、静止环境下的准确人数统计。


<details>
  <summary>Details</summary>
Motivation: 传统毫米波雷达在高密度、静止人群中难以准确计数，因空间分辨率和对运动依赖性较强，限制了其在实际应用中的能力。

Method: mmCounter提取超低频（<1 Hz）信号，主要来源于人的呼吸和轻微躯体移动，结合创新的多阶段信号处理流程，区分出人体微弱信号与环境噪声、静态物体，再结合空间信息映射至个人，实现人数统计。

Result: 在多种环境测试中，mmCounter在已知环境下平均F1得分87%、平均绝对误差0.6；在陌生环境下，平均F1得分60%、平均绝对误差1.1。在3平方米空间内，最密可同时统计7人。

Conclusion: mmCounter显著提升了毫米波雷达在密集、静态人群下的计数能力，具备较高的准确性和通用性，为室内人流监测等应用提供了可行方案。

Abstract: mmWave radars struggle to detect or count individuals in dense, static (non-moving) groups due to limitations in spatial resolution and reliance on movement for detection. We present mmCounter, which accurately counts static people in dense indoor spaces (up to three people per square meter). mmCounter achieves this by extracting ultra-low frequency (< 1 Hz) signals, primarily from breathing and micro-scale body movements such as slight torso shifts, and applying novel signal processing techniques to differentiate these subtle signals from background noise and nearby static objects. Our problem differs significantly from existing studies on breathing rate estimation, which assume the number of people is known a priori. In contrast, mmCounter utilizes a novel multi-stage signal processing pipeline to extract relevant low-frequency sources along with their spatial information and map these sources to individual people, enabling accurate counting. Extensive evaluations in various environments demonstrate that mmCounter delivers an 87% average F1 score and 0.6 mean absolute error in familiar environments, and a 60% average F1 score and 1.1 mean absolute error in previously untested environments. It can count up to seven individuals in a three square meter space, such that there is no side-by-side spacing and only a one-meter front-to-back distance.

</details>


### [38] [Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task](https://arxiv.org/abs/2512.10359)
*Sunqi Fan,Jiashuo Cui,Meng-Hao Guo,Shuojin Yang*

Main category: cs.CV

TL;DR: 本文通过引入视频工具包和时空推理框架（STAR），显著提升了多模态大模型在视频问答任务中的时空推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视频问答任务中，难以同时建模视频帧内时空关系和理解复杂的因果动态，限制了其在真实动态场景推理的能力。

Method: 作者为多模态大模型设计了一个视频工具包，并提出STAR（时空推理框架），用于合理编排时序和空间工具的调用顺序，以更精确地定位视频关键区域，并避免工具链中的投机取巧问题。

Result: 在VideoMME和LongVideoBench两个基准上，增强版GPT-4o分别提升了8.2%和4.6%的性能。

Conclusion: 视频工具包和STAR框架显著提升了多模态大模型的时空推理能力，为构建自主智能的视频分析助手迈出了重要一步。

Abstract: Video Question Answering (VideoQA) task serves as a critical playground for evaluating whether foundation models can effectively perceive, understand, and reason about dynamic real-world scenarios. However, existing Multimodal Large Language Models (MLLMs) struggle with simultaneously modeling spatial relationships within video frames and understanding the causal dynamics of temporal evolution on complex and reasoning-intensive VideoQA task. In this work, we equip MLLM with a comprehensive and extensible Video Toolkit, to enhance MLLM's spatiotemporal reasoning capabilities and ensure the harmony between the quantity and diversity of tools. To better control the tool invocation sequence and avoid toolchain shortcut issues, we propose a Spatiotemporal Reasoning Framework (STAR) that strategically schedules temporal and spatial tools, thereby progressively localizing the key area in the video. Our STAR framework enhances GPT-4o using lightweight tools, achieving an 8.2% gain on VideoMME and 4.6% on LongVideoBench. We believe that our proposed Video Toolkit and STAR framework make an important step towards building autonomous and intelligent video analysis assistants. The code is publicly available at https://github.com/fansunqi/VideoTool.

</details>


### [39] [Visual Funnel: Resolving Contextual Blindness in Multimodal Large Language Models](https://arxiv.org/abs/2512.10362)
*Woojun Jung,Jaehoon Go,Mingyu Jeon,Sunjae Yoon,Junyeong Kim*

Main category: cs.CV

TL;DR: 提出一种名为Visual Funnel的新方法，有效提升MLLMs对精细视觉细节的感知能力，从而解决传统裁剪方法导致的“情境盲区”问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型虽具备推理能力，但往往难以捕捉图片中的精细视觉细节，这主要因裁剪策略打破了细节与整体上下文结构的联系，导致“情境盲区”。

Method: 提出Visual Funnel方法，分为两步：第一步是通过上下文锚定( Contextual Anchoring )确定关键信息区域；第二步根据注意力熵动态确定裁剪大小，构建多尺度、层次化的裁剪组合（Entropy-Scaled Portfolio），以此保留局部细节到全局语境的层级结构。该方法无需额外训练。

Result: 大量实验表明，该方法在捕捉细粒度细节和保持全局语境方面，明显优于常规单裁剪和无结构多裁剪基线。同时证明简单增加不结构化裁剪无明显增益甚至有负效，对比突出Visual Funnel的优势。

Conclusion: 视觉信息输入的结构多样性比数量更重要。Visual Funnel能够有效缓解多模态大模型的“情境盲区”问题，是精细视觉任务中的有效手段。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate impressive reasoning capabilities, but often fail to perceive fine-grained visual details, limiting their applicability in precision-demanding tasks. While methods that crop salient regions of an image offer a partial solution, we identify a critical limitation they introduce: "Contextual Blindness". This failure occurs due to structural disconnect between high-fidelity details (from the crop) and the broader global context (from the original image), even when all necessary visual information is present. We argue that this limitation stems not from a lack of information 'Quantity', but from a lack of 'Structural Diversity' in the model's input. To resolve this, we propose Visual Funnel, a training-free, two-step approach. Visual Funnel first performs Contextual Anchoring to identify the region of interest in a single forward pass. It then constructs an Entropy-Scaled Portfolio that preserves the hierarchical context - ranging from focal detail to broader surroundings - by dynamically determining crop sizes based on attention entropy and refining crop centers. Through extensive experiments, we demonstrate that Visual Funnel significantly outperforms naive single-crop and unstructured multi-crop baselines. Our results further validate that simply adding more unstructured crops provides limited or even detrimental benefits, confirming that the hierarchical structure of our portfolio is key to resolving Contextual Blindness.

</details>


### [40] [Point to Span: Zero-Shot Moment Retrieval for Navigating Unseen Hour-Long Videos](https://arxiv.org/abs/2512.10363)
*Mingyu Jeon,Jisoo Yang,Sungjin Han,Jinkwon Hwang,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: 本文提出了一种创新性的零样本长视频片段检索方法（P2S），在无需特定任务训练的情况下，有效提升了小时级长视频的检索性能，超越了现有最优的有监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有的长视频片段检索（LVMR）通常采用“先搜索后细化”（Search-then-Refine）策略，但受限于可扩展性、泛化能力差和计算成本高等问题，现有的有监督和零样本方法都难以有效处理小时级长视频，亟需一种高效零样本检索新方法。

Method: P2S提出了“自适应片段生成器”，在搜索阶段有效抑制候选爆炸；同时利用“查询分解”，在无需高成本VLM验证的情况下进行高效细化，不依赖昂贵的大规模视觉语言模型。整个流程无需针对特定任务的训练。

Result: P2S方法是首个可直接用于小时级视频时序定位的零样本框架，实验在MAD数据集上显著优于现有有监督最优方法（如R5@0.1提高了3.7%），表现强劲。

Conclusion: 该工作表明P2S能够高效高准确率地完成零样本长视频检索，显著减少计算资源消耗，为实际场景下大规模长视频的时序定位开辟了新路径。

Abstract: Zero-shot Long Video Moment Retrieval (ZLVMR) is the task of identifying temporal segments in hour-long videos using a natural language query without task-specific training. The core technical challenge of LVMR stems from the computational infeasibility of processing entire lengthy videos in a single pass. This limitation has established a 'Search-then-Refine' approach, where candidates are rapidly narrowed down, and only those portions are analyzed, as the dominant paradigm for LVMR. However, existing approaches to this paradigm face severe limitations. Conventional supervised learning suffers from limited scalability and poor generalization, despite substantial resource consumption. Yet, existing zero-shot methods also fail, facing a dual challenge: (1) their heuristic strategies cause a 'search' phase candidate explosion, and (2) the 'refine' phase, which is vulnerable to semantic discrepancy, requires high-cost VLMs for verification, incurring significant computational overhead. We propose \textbf{P}oint-\textbf{to}-\textbf{S}pan (P2S), a novel training-free framework to overcome this challenge of inefficient 'search' and costly 'refine' phases. P2S overcomes these challenges with two key innovations: an 'Adaptive Span Generator' to prevent the search phase candidate explosion, and 'Query Decomposition' to refine candidates without relying on high-cost VLM verification. To our knowledge, P2S is the first zero-shot framework capable of temporal grounding in hour-long videos, outperforming supervised state-of-the-art methods by a significant margin (e.g., +3.7\% on R5@0.1 on MAD).

</details>


### [41] [Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views](https://arxiv.org/abs/2512.10369)
*Zhankuo Xu,Chaoran Feng,Yingtao Li,Jianbin Zhao,Jiashu Yang,Wangbo Yu,Li Yuan,Yonghong Tian*

Main category: cs.CV

TL;DR: 本文提出了CoherentGS，一种能够从稀疏且模糊输入图像中实现高保真3D重建的新方法，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的3D Gaussian Splatting（3DGS）在新视角合成方面表现优异，但高度依赖于密集且高质量的输入图像。在实际应用中，经常遇到稀疏及运动模糊的数据，这使得传统方法重建效果极差。因此，需要一种能够克服稀疏性和模糊双重挑战的新方法。

Method: 作者提出了CoherentGS，采用双重先验（dual-prior）策略：一方面借助预训练的去模糊网络恢复清晰细节并提供光度指导；另一方面利用扩散模型作为几何先验以补齐未观测区域。此外，引入了基于一致性的相机探索模块与深度正则化损失以提升几何合理性。

Result: 在合成和真实场景上，用3、6、9张稀疏输入视图进行实验。无论是定量还是定性指标，CoherentGS在稀疏、模糊输入下均显著优于当前其他方法，取得了新的性能高度。

Conclusion: CoherentGS有效解决了稀疏和模糊图像的3D重建难题，为实际低质输入情况下的高质量3D合成开辟了新路径。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a state-of-the-art method for novel view synthesis. However, its performance heavily relies on dense, high-quality input imagery, an assumption that is often violated in real-world applications, where data is typically sparse and motion-blurred. These two issues create a vicious cycle: sparse views ignore the multi-view constraints necessary to resolve motion blur, while motion blur erases high-frequency details crucial for aligning the limited views. Thus, reconstruction often fails catastrophically, with fragmented views and a low-frequency bias. To break this cycle, we introduce CoherentGS, a novel framework for high-fidelity 3D reconstruction from sparse and blurry images. Our key insight is to address these compound degradations using a dual-prior strategy. Specifically, we combine two pre-trained generative models: a specialized deblurring network for restoring sharp details and providing photometric guidance, and a diffusion model that offers geometric priors to fill in unobserved regions of the scene. This dual-prior strategy is supported by several key techniques, including a consistency-guided camera exploration module that adaptively guides the generative process, and a depth regularization loss that ensures geometric plausibility. We evaluate CoherentGS through both quantitative and qualitative experiments on synthetic and real-world scenes, using as few as 3, 6, and 9 input views. Our results demonstrate that CoherentGS significantly outperforms existing methods, setting a new state-of-the-art for this challenging task. The code and video demos are available at https://potatobigroom.github.io/CoherentGS/.

</details>


### [42] [RaLiFlow: Scene Flow Estimation with 4D Radar and LiDAR Point Clouds](https://arxiv.org/abs/2512.10376)
*Jingyun Fu,Zhiyu Xiang,Na Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种结合4D毫米波雷达与LiDAR点云的场景流估计方法，并首次构建了相应的数据集，提出了高效的雷达去噪和标签生成方法，以及新的跨模态融合框架RaLiFlow，实现了在动态场景下的高精度融合和预测，在新数据集上显著超越了现有单模态方法。


<details>
  <summary>Details</summary>
Motivation: 在多模态感知领域，LiDAR与图像的融合已取得进展，但针对4D雷达与LiDAR的融合却鲜有探索。雷达在多种环境下鲁棒性更强、成本更低且可获取速度信息，但其噪声高、分辨率低、稀疏性强，难以直接应用，同时缺乏相关标注数据集。因此，亟需研究雷达-LiDAR场景流的联合估计方法及构建高质量数据集。

Method: 1）基于真实自动驾驶数据库，构建了第一个雷达-LiDAR场景流数据集，包含高质量的雷达点云去噪和场景流标注生成策略；2）提出RaLiFlow联合学习框架，引入动态感知双向跨模态融合（DBCF）模块，将雷达动态信息融入局部跨模态注意力机制，实现信息互补；3）设计新型损失函数，提高模型对低质量雷达数据的抗干扰能力，并提升前景动态实例的一致性预测。

Result: 在自建的雷达-LiDAR场景流数据集上，RaLiFlow在多项评价指标下均显著优于现有的单一LiDAR或雷达方法，验证了方法的有效性。

Conclusion: 首次基于雷达与LiDAR实现了高效的场景流估计方案，完善了数据集和方法体系，推进了多模态感知前沿，并为动态场景下的高鲁棒性感知提供了有效技术途径。

Abstract: Recent multimodal fusion methods, integrating images with LiDAR point clouds, have shown promise in scene flow estimation. However, the fusion of 4D millimeter wave radar and LiDAR remains unexplored. Unlike LiDAR, radar is cheaper, more robust in various weather conditions and can detect point-wise velocity, making it a valuable complement to LiDAR. However, radar inputs pose challenges due to noise, low resolution, and sparsity. Moreover, there is currently no dataset that combines LiDAR and radar data specifically for scene flow estimation. To address this gap, we construct a Radar-LiDAR scene flow dataset based on a public real-world automotive dataset. We propose an effective preprocessing strategy for radar denoising and scene flow label generation, deriving more reliable flow ground truth for radar points out of the object boundaries. Additionally, we introduce RaLiFlow, the first joint scene flow learning framework for 4D radar and LiDAR, which achieves effective radar-LiDAR fusion through a novel Dynamic-aware Bidirectional Cross-modal Fusion (DBCF) module and a carefully designed set of loss functions. The DBCF module integrates dynamic cues from radar into the local cross-attention mechanism, enabling the propagation of contextual information across modalities. Meanwhile, the proposed loss functions mitigate the adverse effects of unreliable radar data during training and enhance the instance-level consistency in scene flow predictions from both modalities, particularly for dynamic foreground areas. Extensive experiments on the repurposed scene flow dataset demonstrate that our method outperforms existing LiDAR-based and radar-based single-modal methods by a significant margin.

</details>


### [43] [Self-Supervised Contrastive Embedding Adaptation for Endoscopic Image Matching](https://arxiv.org/abs/2512.10379)
*Alberto Rota,Elena De Momi*

Main category: cs.CV

TL;DR: 该论文提出了一种利用深度学习在内窥镜图像对中建立特征对应的新管线，并使用自监督对比学习优化模型，取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 微创手术中视觉输入是唯一的信息源，对内窥镜帧进行精确对应对于三维重建、相机跟踪等至关重要，但手术场景复杂、组织反射等挑战导致传统方法效果有限，需要专门适应手术领域的算法。

Method: 提出了一种新的深度学习管线，使用自监督的对比学习方法训练，在DINOv2骨干基础上加入Transformer层。通过新颖的视图合成创建对应点真值，构建三元组对用于模型优化，输出适于余弦相似度匹配的特征嵌入。

Result: 在SCARED数据集上，提出的管线在特征匹配精度和极线误差上均优于当前主流方法，实验评估表现突出。

Conclusion: 该方法显著提升了内窥镜外科场景下高层视觉任务的基础特征匹配能力，对手术辅助系统和相关计算机视觉任务具有重要价值。

Abstract: Accurate spatial understanding is essential for image-guided surgery, augmented reality integration and context awareness. In minimally invasive procedures, where visual input is the sole intraoperative modality, establishing precise pixel-level correspondences between endoscopic frames is critical for 3D reconstruction, camera tracking, and scene interpretation. However, the surgical domain presents distinct challenges: weak perspective cues, non-Lambertian tissue reflections, and complex, deformable anatomy degrade the performance of conventional computer vision techniques. While Deep Learning models have shown strong performance in natural scenes, their features are not inherently suited for fine-grained matching in surgical images and require targeted adaptation to meet the demands of this domain. This research presents a novel Deep Learning pipeline for establishing feature correspondences in endoscopic image pairs, alongside a self-supervised optimization framework for model training. The proposed methodology leverages a novel-view synthesis pipeline to generate ground-truth inlier correspondences, subsequently utilized for mining triplets within a contrastive learning paradigm. Through this self-supervised approach, we augment the DINOv2 backbone with an additional Transformer layer, specifically optimized to produce embeddings that facilitate direct matching through cosine similarity thresholding. Experimental evaluation demonstrates that our pipeline surpasses state-of-the-art methodologies on the SCARED datasets improved matching precision and lower epipolar error compared to the related work. The proposed framework constitutes a valuable contribution toward enabling more accurate high-level computer vision applications in surgical endoscopy.

</details>


### [44] [Towards Fine-Grained Recognition with Large Visual Language Models: Benchmark and Optimization Strategies](https://arxiv.org/abs/2512.10384)
*Cong Pang,Hongtao Yu,Zixuan Chen,Lewei Lu,Xin Lou*

Main category: cs.CV

TL;DR: 本文提出了FROW基准，用于细粒度识别任务的评测，并通过新颖的数据构造和训练方法显著提升了大视觉语言模型（LVLMs）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs评测主要集中于推理任务，忽略了实际应用中重要的细粒度识别能力，因此需要一个专门的基准和提升策略。

Method: 构建FROW基准，着重考查细粒度识别能力。引入两种新型数据：mosaic数据（多答案组合）和open-world数据（由GPT-4o生成的真实世界问答），同时提出优化训练数据和过程的方法。

Result: 实验表明，mosaic数据可提升类别识别准确率1%，open-world数据能使FROW基准准确率提高10%-20%、内容准确率提升6%-12%；细粒度数据预训练能将类别识别准率提升至10%。

Conclusion: FROW基准和新数据策略有效提升了LVLMs的细粒度识别能力，推动其在实际应用中的实用性。

Abstract: Large Vision Language Models (LVLMs) have made remarkable progress, enabling sophisticated vision-language interaction and dialogue applications. However, existing benchmarks primarily focus on reasoning tasks, often neglecting fine-grained recognition, which is crucial for practical application scenarios. To address this gap, we introduce the Fine-grained Recognition Open World (FROW) benchmark, designed for detailed evaluation of LVLMs with GPT-4o. On the basis of that, we propose a novel optimization strategy from two perspectives: \textit{data construction} and \textit{training process}, to improve the performance of LVLMs. Our dataset includes mosaic data, which combines multiple short-answer responses, and open-world data, generated from real-world questions and answers using GPT-4o, creating a comprehensive framework for evaluating fine-grained recognition in LVLMs. Experiments show that mosaic data improves category recognition accuracy by 1\% and open-world data boosts FROW benchmark accuracy by 10\%-20\% and content accuracy by 6\%-12\%. Meanwhile, incorporating fine-grained data into the pre-training phase can improve the model's category recognition accuracy by up to 10\%. The benchmark will be available at https://github.com/pc-inno/FROW.

</details>


### [45] [Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method](https://arxiv.org/abs/2512.10386)
*Ge Zhang,Chunyang Wang,Bo Xiao,Xuelian Liu,Bin Liu*

Main category: cs.CV

TL;DR: 本文提出了一种自适应双重加权引力的点云去噪方法，能够在提升精度的同时兼顾边界保持和实时性。


<details>
  <summary>Details</summary>
Motivation: 点云数据在自动驾驶与三维重建等任务中至关重要，但激光雷达采集过程中常被干扰，产生大量噪声点，影响后续检测识别。现有方法难以兼顾高去噪精度、边缘保持和实时性。

Method: 算法首先使用八叉树对点云进行空间划分以并行加速；然后对每个叶节点，应用自适应体素占据统计和kNN密度估计，快速去除孤立和低密度噪声点；最后结合密度权重与自适应距离权重构建引力评分函数，精细区分噪声与真实点。

Result: 在包括Stanford 3D、CADC及自采FMCW点云数据集上，算法在F1、PSNR和Chamfer Distance等指标下优于现有方法，同时降低单帧处理时长，在多种噪声场景下展现出高精度、鲁棒性和实时性。

Conclusion: 该方法有效提升了点云去噪的精度、边界细节保持能力及处理速度，适用于复杂噪声环境下的实时点云处理。

Abstract: High-quality point cloud data is a critical foundation for tasks such as autonomous driving and 3D reconstruction. However, LiDAR-based point cloud acquisition is often affected by various disturbances, resulting in a large number of noise points that degrade the accuracy of subsequent point cloud object detection and recognition. Moreover, existing point cloud denoising methods typically sacrifice computational efficiency in pursuit of higher denoising accuracy, or, conversely, improve processing speed at the expense of preserving object boundaries and fine structural details, making it difficult to simultaneously achieve high denoising accuracy, strong edge preservation, and real-time performance. To address these limitations, this paper proposes an adaptive dual-weight gravitational-based point cloud denoising method. First, an octree is employed to perform spatial partitioning of the global point cloud, enabling parallel acceleration. Then, within each leaf node, adaptive voxel-based occupancy statistics and k-nearest neighbor (kNN) density estimation are applied to rapidly remove clearly isolated and low-density noise points, thereby reducing the effective candidate set. Finally, a gravitational scoring function that combines density weights with adaptive distance weights is constructed to finely distinguish noise points from object points. Experiments conducted on the Stanford 3D Scanning Repository, the Canadian Adverse Driving Conditions (CADC) dataset, and in-house FMCW LiDAR point clouds acquired in our laboratory demonstrate that, compared with existing methods, the proposed approach achieves consistent improvements in F1, PSNR, and Chamfer Distance (CD) across various noise conditions while reducing the single-frame processing time, thereby validating its high accuracy, robustness, and real-time performance in multi-noise scenarios.

</details>


### [46] [MultiHateLoc: Towards Temporal Localisation of Multimodal Hate Content in Online Videos](https://arxiv.org/abs/2512.10408)
*Qiyue Sun,Tailin Chen,Yinghui Zhang,Yuchen Zhang,Jiangbei Yue,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 本文提出了MultiHateLoc框架，在仅有视频级标签的情况下，实现对多模态（视频、音频、文本）仇恨言论的时序定位，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态仇恨言论检测多停留在视频级别分类，对于实际应用中“仇恨内容具体出现在哪一时刻”的定位任务关注较少，尤其在弱监督（只有视频级标签）下更为突出。此外，现有静态融合或者端到端分类方法难以捕捉多模态时序动态信息。

Method: 提出MultiHateLoc框架，核心包括：1）基于模态感知的时序编码器，包括文本增强模块；2）动态跨模态融合，能在每个时刻自适应关注重要模态，并通过对比学习增强多模态一致性；3）模态感知的多示例学习（MIL）目标函数，在视频级标签下识别关键片段。

Result: 在HateMM和MultiHateClip两个多模态仇恨言论数据集上，MultiHateLoc展现出比以往方法更优越的仇恨片段定位性能，细粒度层面预测具有可解释性。

Conclusion: MultiHateLoc首次在弱监督条件下实现多模态仇恨言论的时序定位，不仅在准确性上达到SOTA，而且具备很好的解释性，对实际内容审核有重要意义。

Abstract: The rapid growth of video content on platforms such as TikTok and YouTube has intensified the spread of multimodal hate speech, where harmful cues emerge subtly and asynchronously across visual, acoustic, and textual streams. Existing research primarily focuses on video-level classification, leaving the practically crucial task of temporal localisation, identifying when hateful segments occur, largely unaddressed. This challenge is even more noticeable under weak supervision, where only video-level labels are available, and static fusion or classification-based architectures struggle to capture cross-modal and temporal dynamics. To address these challenges, we propose MultiHateLoc, the first framework designed for weakly-supervised multimodal hate localisation. MultiHateLoc incorporates (1) modality-aware temporal encoders to model heterogeneous sequential patterns, including a tailored text-based preprocessing module for feature enhancement; (2) dynamic cross-modal fusion to adaptively emphasise the most informative modality at each moment and a cross-modal contrastive alignment strategy to enhance multimodal feature consistency; (3) a modality-aware MIL objective to identify discriminative segments under video-level supervision. Despite relying solely on coarse labels, MultiHateLoc produces fine-grained, interpretable frame-level predictions. Experiments on HateMM and MultiHateClip show that our method achieves state-of-the-art performance in the localisation task.

</details>


### [47] [Beyond Endpoints: Path-Centric Reasoning for Vectorized Off-Road Network Extraction](https://arxiv.org/abs/2512.10416)
*Wenfei Guan,Jilin Mei,Tong Shen,Xumin Wu,Shuo Wang,Cheng Min,Yu Hu*

Main category: cs.CV

TL;DR: 该论文提出了WildRoad脱离道路数据集以及MaGRoad路径为中心的道路网络提取模型，并在复杂自然场景下实现了领先效果。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习虽能有效提取城市道路矢量，但在复杂自然场景（如荒野、林地等）表现较差，主要是因为缺乏大规模相关数据集，以及现有方法结构本身对于复杂交叉和遮挡较脆弱。

Method: 1）构建并公开WildRoad全球自然环境道路网络数据集，利用专门开发的交互式标注工具高效标注。2）提出MaGRoad模型：采用以路径为中心的框架，融合多尺度视觉信息，针对候选路径自适应推断道路连通性，并提升鲁棒性和泛化能力。

Result: MaGRoad在WildRoad基准测试中取得了最新最好性能，并能很好地泛化到城市道路数据。同时方法推理速度提升约2.5倍，大大提高了实用性。

Conclusion: WildRoad数据集和MaGRoad路径为中心的算法为自然环境道路网络的自动化矢量化提供了更坚实基础，有望广泛应用于复杂场景的道路映射。

Abstract: Deep learning has advanced vectorized road extraction in urban settings, yet off-road environments remain underexplored and challenging. A significant domain gap causes advanced models to fail in wild terrains due to two key issues: lack of large-scale vectorized datasets and structural weakness in prevailing methods. Models such as SAM-Road employ a node-centric paradigm that reasons at sparse endpoints, making them fragile to occlusions and ambiguous junctions in off-road scenes, leading to topological errors.This work addresses these limitations in two complementary ways. First, we release WildRoad, a gloabal off-road road network dataset constructed efficiently with a dedicated interactive annotation tool tailored for road-network labeling. Second, we introduce MaGRoad (Mask-aware Geodesic Road network extractor), a path-centric framework that aggregates multi-scale visual evidence along candidate paths to infer connectivity robustly.Extensive experiments show that MaGRoad achieves state-of-the-art performance on our challenging WildRoad benchmark while generalizing well to urban datasets. A streamlined pipeline also yields roughly 2.5x faster inference, improving practical applicability. Together, the dataset and path-centric paradigm provide a stronger foundation for mapping roads in the wild.

</details>


### [48] [TransLocNet: Cross-Modal Attention for Aerial-Ground Vehicle Localization with Contrastive Learning](https://arxiv.org/abs/2512.10419)
*Phu Pham,Damon Conover,Aniket Bera*

Main category: cs.CV

TL;DR: 本文提出了一种名为TransLocNet的跨模态注意力框架，可有效融合地面LiDAR与航拍图像，实现高精度的空地定位。实验表明其明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 空地定位由于地面LiDAR和航拍图像间存在视角和模态大差异而极为困难。现有方法在跨模态特征对齐上效果有限，准确性无法满足实际需求。

Method: 将地面LiDAR数据投影为鸟瞰图，与航拍图像的特征通过双向注意力机制对齐，并采用似然解码器输出空间概率分布。同时引入对比学习以共享特征嵌入空间，实现更好的跨模态特征对齐。

Result: 在CARLA和KITTI两个数据集上，TransLocNet的定位误差比现有最优方法低63%，定位精度达到亚米级、亚角度级。

Conclusion: TransLocNet有效提升了空地跨模态定位精度，具备良好的鲁棒性和通用性，适用于真实与仿真场景。

Abstract: Aerial-ground localization is difficult due to large viewpoint and modality gaps between ground-level LiDAR and overhead imagery. We propose TransLocNet, a cross-modal attention framework that fuses LiDAR geometry with aerial semantic context. LiDAR scans are projected into a bird's-eye-view representation and aligned with aerial features through bidirectional attention, followed by a likelihood map decoder that outputs spatial probability distributions over position and orientation. A contrastive learning module enforces a shared embedding space to improve cross-modal alignment. Experiments on CARLA and KITTI show that TransLocNet outperforms state-of-the-art baselines, reducing localization error by up to 63% and achieving sub-meter, sub-degree accuracy. These results demonstrate that TransLocNet provides robust and generalizable aerial-ground localization in both synthetic and real-world settings.

</details>


### [49] [Neural Collapse in Test-Time Adaptation](https://arxiv.org/abs/2512.10421)
*Xiao Chen,Zhongjing Du,Jiazhen Huang,Xu Jiang,Li Lu,Jingyan Jiang,Zhi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种全新的测试时自适应（TTA）方法NCTTA，通过特征-分类器对齐，有效提升了模型在分布外数据上的鲁棒性，并在ImageNet-C等数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然TTA已经被广泛用于提升模型的分布外鲁棒性，但现有方法缺乏对性能退化根本原因的理论解释。因此，作者希望建立更扎实的理论基础，揭示域偏移下性能退化的本质，并据此设计更有效的TTA方法。

Method: 作者首先将“神经坍塌”（Neural Collapse, NC）现象扩展到样本级别，提出并验证了样本特征与其分类器权重紧密对齐的现象（NC3+）。作者发现，分布偏移时，这种对齐会被破坏，导致伪标签失真。为此，作者设计了NCTTA方法，融合几何距离和预测置信度，混合生成伪标签，从而对齐特征和分类器，提高自适应鲁棒性。

Result: 实验证明，NCTTA在多个典型的域偏移测试集上，效果均优于主流TTA方法，如在ImageNet-C上的准确率相比Tent提升了14.52%。

Conclusion: 通过对样本级对齐坍塌新现象的研究和理论分析，结合提出的NCTTA对齐方法，本文显著提升了模型对域偏移的自适应能力，对理论与应用均有重要贡献。

Abstract: Test-Time Adaptation (TTA) enhances model robustness to out-of-distribution (OOD) data by updating the model online during inference, yet existing methods lack theoretical insights into the fundamental causes of performance degradation under domain shifts. Recently, Neural Collapse (NC) has been proposed as an emergent geometric property of deep neural networks (DNNs), providing valuable insights for TTA. In this work, we extend NC to the sample-wise level and discover a novel phenomenon termed Sample-wise Alignment Collapse (NC3+), demonstrating that a sample's feature embedding, obtained by a trained model, aligns closely with the corresponding classifier weight. Building on NC3+, we identify that the performance degradation stems from sample-wise misalignment in adaptation which exacerbates under larger distribution shifts. This indicates the necessity of realigning the feature embeddings with their corresponding classifier weights. However, the misalignment makes pseudo-labels unreliable under domain shifts. To address this challenge, we propose NCTTA, a novel feature-classifier alignment method with hybrid targets to mitigate the impact of unreliable pseudo-labels, which blends geometric proximity with predictive confidence. Extensive experiments demonstrate the effectiveness of NCTTA in enhancing robustness to domain shifts. For example, NCTTA outperforms Tent by 14.52% on ImageNet-C.

</details>


### [50] [An M-Health Algorithmic Approach to Identify and Assess Physiotherapy Exercises in Real Time](https://arxiv.org/abs/2512.10437)
*Stylianos Kandylakis,Christos Orfanopoulos,Georgios Siolas,Panayiotis Tsanakas*

Main category: cs.CV

TL;DR: 本研究提出了一种高效的算法框架，能在移动设备上实时识别、分类和评估人体物理治疗动作。


<details>
  <summary>Details</summary>
Motivation: 推动理疗动作监测的自动化，方便远程医疗和健康管理，实现大规模、实时的用户动作监督。

Method: 将人体运动分解为一系列静态姿势，通过摄像头与姿态估计算法获取关键点，转化为三角函数角度特征，利用轻量级监督模型分类；再借助改进的Levenshtein距离的动态规划算法，实现完整动作识别与误差检测。

Result: 实验结果显示，该方法高效且具实际应用价值，能在客户端实现实时动作识别和准确度评分。

Conclusion: 该方法支持移动终端上的理疗运动监控，为远程理疗和移动健康领域提供了可扩展和实用的技术方案。

Abstract: This work presents an efficient algorithmic framework for real-time identification, classification, and evaluation of human physiotherapy exercises using mobile devices. The proposed method interprets a kinetic movement as a sequence of static poses, which are estimated from camera input using a pose-estimation neural network. Extracted body keypoints are transformed into trigonometric angle-based features and classified with lightweight supervised models to generate frame-level pose predictions and accuracy scores. To recognize full exercise movements and detect deviations from prescribed patterns, we employ a dynamic-programming scheme based on a modified Levenshtein distance algorithm, enabling robust sequence matching and localization of inaccuracies. The system operates entirely on the client side, ensuring scalability and real-time performance. Experimental evaluation demonstrates the effectiveness of the methodology and highlights its applicability to remote physiotherapy supervision and m-health applications.

</details>


### [51] [Error-Propagation-Free Learned Video Compression With Dual-Domain Progressive Temporal Alignment](https://arxiv.org/abs/2512.10450)
*Han Li,Shaohui Li,Wenrui Dai,Chenglin Li,Xinlong Pan,Haipeng Wang,Junni Zou,Hongkai Xiong*

Main category: cs.CV

TL;DR: 该论文提出了一种全新的学习型视频压缩架构，通过双域渐进时序对齐和质量条件混合专家机制，实现无误差传播、质量一致的流式视频压缩。


<details>
  <summary>Details</summary>
Motivation: 现有学习型视频压缩架构在运动估计与补偿过程中，面临时序对齐不准与误差传播之间的两难困境。单独变换架构虽率失真性能优异，但带来误差传播；统一变换架构虽能消除误差传播，却在运动估计与补偿能力上有所牺牲。为此，作者希望突破这两类方法的局限，实现既能准确时序对齐、又无误差传播的高效视频压缩。

Method: 作者提出统一变换架构搭配双域渐进时序对齐和QCMoE模块：1) 双域渐进时序对齐结合像素域粗对齐（适用简单运动）和潜变量域细对齐（利用多参考帧与可变形Transformer，适用复杂运动），以粗到细方式增强时序建模。2) QCMoE能根据目标质量和内容动态分配量化专家，实现按像素自适应量化步长，连续合理地控制码率。

Result: 实验结果显示，该方法不仅消除了误差传播问题，还取得了与现有最佳方法媲美的率失真性能。

Conclusion: 所提方法有效解决了现有架构的两难困境，实现高效、无误差传播的视频压缩，对后续视频编码架构设计具有重要参考价值。

Abstract: Existing frameworks for learned video compression suffer from a dilemma between inaccurate temporal alignment and error propagation for motion estimation and compensation (ME/MC). The separate-transform framework employs distinct transforms for intra-frame and inter-frame compression to yield impressive rate-distortion (R-D) performance but causes evident error propagation, while the unified-transform framework eliminates error propagation via shared transforms but is inferior in ME/MC in shared latent domains. To address this limitation, in this paper, we propose a novel unifiedtransform framework with dual-domain progressive temporal alignment and quality-conditioned mixture-of-expert (QCMoE) to enable quality-consistent and error-propagation-free streaming for learned video compression. Specifically, we propose dualdomain progressive temporal alignment for ME/MC that leverages coarse pixel-domain alignment and refined latent-domain alignment to significantly enhance temporal context modeling in a coarse-to-fine fashion. The coarse pixel-domain alignment efficiently handles simple motion patterns with optical flow estimated from a single reference frame, while the refined latent-domain alignment develops a Flow-Guided Deformable Transformer (FGDT) over latents from multiple reference frames to achieve long-term motion refinement (LTMR) for complex motion patterns. Furthermore, we design a QCMoE module for continuous bit-rate adaptation that dynamically assigns different experts to adjust quantization steps per pixel based on target quality and content rather than relies on a single quantization step. QCMoE allows continuous and consistent rate control with appealing R-D performance. Experimental results show that the proposed method achieves competitive R-D performance compared with the state-of-the-arts, while successfully eliminating error propagation.

</details>


### [52] [Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network](https://arxiv.org/abs/2512.10498)
*Khurram Ashfaq,Muhammad Tariq Mahmood*

Main category: cs.CV

TL;DR: 本文提出了一种混合型Shape-from-Focus（SFF）深度估计框架，通过结合手工特征与轻量化深度提取网络，提升了深度估计的准确性与细节保留能力，优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习SFF方法通常依赖大规模神经网络提取特征，并采用简单的深度聚合技术，易导致深度图噪声增加与细节丢失。作者希望解决上述缺陷，兼顾精度、细节与计算效率。

Method: 方法分为两步：首先利用手工设计的方向扩张拉普拉斯（DDL）核，传统地计算多尺度关注体（focus volumes）；其次将其输入到轻量级、多尺度的GRU深度提取模块中，低分辨率下迭代细化初始深度估计。最终通过学习型凸上采样模块恢复高分辨率深度图，并保留场景细节和边缘。

Result: 在合成数据和真实数据集上的大量实验表明，该方法在不同焦距条件下均优于现有深度学习和传统SFF方法，达到更高精度和更强泛化能力。

Conclusion: 结合传统特征和轻量神经网络可以更高效、准确地实现无源深度估计，本方法在准确性、细节恢复和运算效率方面表现优异，具有潜在应用前景。

Abstract: Shape-from-Focus (SFF) is a passive depth estimation technique that infers scene depth by analyzing focus variations in a focal stack. Most recent deep learning-based SFF methods typically operate in two stages: first, they extract focus volumes (a per pixel representation of focus likelihood across the focal stack) using heavy feature encoders; then, they estimate depth via a simple one-step aggregation technique that often introduces artifacts and amplifies noise in the depth map. To address these issues, we propose a hybrid framework. Our method computes multi-scale focus volumes traditionally using handcrafted Directional Dilated Laplacian (DDL) kernels, which capture long-range and directional focus variations to form robust focus volumes. These focus volumes are then fed into a lightweight, multi-scale GRU-based depth extraction module that iteratively refines an initial depth estimate at a lower resolution for computational efficiency. Finally, a learned convex upsampling module within our recurrent network reconstructs high-resolution depth maps while preserving fine scene details and sharp boundaries. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach outperforms state-of-the-art deep learning and traditional methods, achieving superior accuracy and generalization across diverse focal conditions.

</details>


### [53] [3D Blood Pulsation Maps](https://arxiv.org/abs/2512.10517)
*Maurice Rohr,Tobias Reinhardt,Tizian Dege,Justus Thies,Christoph Hoog Antink*

Main category: cs.CV

TL;DR: Pulse3DFace 是首个用于估计3D血流脉动图的数据集，包含多视角面部视频、脉搏参考和3D模型，为脉搏估计和光照消除等研究提供了标准数据。


<details>
  <summary>Details</summary>
Motivation: 当前基于面部视频的脉搏估计方法（远程光体积描记法，rPPG）受到缺乏高质量动态脉搏地图库和多视角数据的限制。现有数据集难以满足针对动态、复杂光照（如多角度和不同光照）等场景下的研究需求，也不利于开发更精准或鲁棒的脉搏估计算法。作者希望填补这一空白，推动面部脉搏3D建模与算法研究发展。

Method: 作者构建了 Pulse3DFace 数据集，包括15名受试者在23个视角下，以30Hz采样频率录制的原始面部视频、血脉搏动参考信号、基于单目运动结构生成的3D面部扫描，以及与FLAME模型兼容的3D脉搏地图（包括信噪比、本地脉搏幅值、相位等）。作者还对数据集中的光照条件、地图一致性和在面部/颈部捕捉生理特征的能力进行了全面评估。

Result: Pulse3DFace数据集成功地提供多视角、3D脉搏相关的高质量面部数据，能为空间分布脉搏分析、合成面部视频、光照条件下算法的鲁棒性研究等方向提供基础；评估显示该数据集能够捕捉到生理上有意义的脉搏分布及血流信号。

Conclusion: Pulse3DFace数据集为3D血流脉搏建模带来了高质量、多视角支持，为远程脉搏估计方法和光照影响消除算法的开发与验证提供了关键资源，有望推动相关研究的进展。

Abstract: We present Pulse3DFace, the first dataset of its kind for estimating 3D blood pulsation maps. These maps can be used to develop models of dynamic facial blood pulsation, enabling the creation of synthetic video data to improve and validate remote pulse estimation methods via photoplethysmography imaging. Additionally, the dataset facilitates research into novel multi-view-based approaches for mitigating illumination effects in blood pulsation analysis. Pulse3DFace consists of raw videos from 15 subjects recorded at 30 Hz with an RGB camera from 23 viewpoints, blood pulse reference measurements, and facial 3D scans generated using monocular structure-from-motion techniques. It also includes processed 3D pulsation maps compatible with the texture space of the 3D head model FLAME. These maps provide signal-to-noise ratio, local pulse amplitude, phase information, and supplementary data. We offer a comprehensive evaluation of the dataset's illumination conditions, map consistency, and its ability to capture physiologically meaningful features in the facial and neck skin regions.

</details>


### [54] [Take a Peek: Efficient Encoder Adaptation for Few-Shot Semantic Segmentation via LoRA](https://arxiv.org/abs/2512.10521)
*Pasquale De Marinis,Gennaro Vessio,Giovanna Castellano*

Main category: cs.CV

TL;DR: 本文提出了一种名为TaP的新方法，利用LoRA技术提升Few-shot语义分割（FSS）任务中编码器对新类的适应能力，在多个基准和跨领域数据集上均显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有FSS方法大多提升了解码器性能，但编码器对于未见新类别的特征提取能力有限，成为主要瓶颈。本文关注如何提升编码器在小样本和跨领域分割中的泛化与适应能力。

Method: 提出TaP方法，在支持集上用LoRA对编码器进行高效微调，在不引起灾难性遗忘的同时，增强对新类的快速适应。TaP与模型无关，可无缝集成到现有FSS框架中，并通过rank灵敏度分析确保效率。

Result: 在COCO $20^i$、Pascal $5^i$及DeepGlobe、ISIC、Chest X-ray等跨域数据集上进行了大量实验，TaP在不同模型和shot设定下均显著提升分割性能，尤其在复杂多分类任务中表现突出。

Conclusion: TaP有效突破了FSS领域编码器难以泛化至新类的限制，为构建更健壮、高效和通用的分割系统提供了新思路。

Abstract: Few-shot semantic segmentation (FSS) aims to segment novel classes in query images using only a small annotated support set. While prior research has mainly focused on improving decoders, the encoder's limited ability to extract meaningful features for unseen classes remains a key bottleneck. In this work, we introduce \textit{Take a Peek} (TaP), a simple yet effective method that enhances encoder adaptability for both FSS and cross-domain FSS (CD-FSS). TaP leverages Low-Rank Adaptation (LoRA) to fine-tune the encoder on the support set with minimal computational overhead, enabling fast adaptation to novel classes while mitigating catastrophic forgetting. Our method is model-agnostic and can be seamlessly integrated into existing FSS pipelines. Extensive experiments across multiple benchmarks--including COCO $20^i$, Pascal $5^i$, and cross-domain datasets such as DeepGlobe, ISIC, and Chest X-ray--demonstrate that TaP consistently improves segmentation performance across diverse models and shot settings. Notably, TaP delivers significant gains in complex multi-class scenarios, highlighting its practical effectiveness in realistic settings. A rank sensitivity analysis also shows that strong performance can be achieved even with low-rank adaptations, ensuring computational efficiency. By addressing a critical limitation in FSS--the encoder's generalization to novel classes--TaP paves the way toward more robust, efficient, and generalizable segmentation systems. The code is available at https://github.com/pasqualedem/TakeAPeek.

</details>


### [55] [Blink: Dynamic Visual Token Resolution for Enhanced Multimodal Understanding](https://arxiv.org/abs/2512.10548)
*Yuchen Feng,Zhenyu Zhang,Naibin Gu,Yilong Chen,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种模仿人类视觉注意机制的动态视觉token分辨率框架Blink，用于提升多模态大模型（MLLMs）的视觉感知能力。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大模型在视觉-语言任务上表现出色，但其视觉感知能力有限。而人类通过动态关注显著区域实现高效复杂场景感知，作者受此启发，探索MLLMs是否也有类似的动态关注机制，并进一步提升其表现。

Method: 作者首先分析了MLLMs各层对不同视觉区域的关注分布，然后提出Blink框架，包括两个主要模块：基于注意力图估计视觉token显著性的“显著性引导扫描”模块，以及动态对重要token超分辨（TokenSR）的“动态token分辨率”模块。在后续层失焦时丢弃扩展token，模仿人类的广泛探索与细致聚焦过程。

Result: 大量实验表明，Blink能够有效提升MLLMs的视觉感知能力和多模态理解能力。

Conclusion: Blink框架通过动态token分辨率机制，高效适应性地提升了MLLMs在视觉感知和多模态理解上的表现，证明该人类灵感方法的有效性。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress on various vision-language tasks, yet their visual perception remains limited. Humans, in comparison, perceive complex scenes efficiently by dynamically scanning and focusing on salient regions in a sequential "blink-like" process. Motivated by this strategy, we first investigate whether MLLMs exhibit similar behavior. Our pilot analysis reveals that MLLMs naturally attend to different visual regions across layers and that selectively allocating more computation to salient tokens can enhance visual perception. Building on this insight, we propose Blink, a dynamic visual token resolution framework that emulates the human-inspired process within a single forward pass. Specifically, Blink includes two modules: saliency-guided scanning and dynamic token resolution. It first estimates the saliency of visual tokens in each layer based on the attention map, and extends important tokens through a plug-and-play token super-resolution (TokenSR) module. In the next layer, it drops the extended tokens when they lose focus. This dynamic mechanism balances broad exploration and fine-grained focus, thereby enhancing visual perception adaptively and efficiently. Extensive experiments validate Blink, demonstrating its effectiveness in enhancing visual perception and multimodal understanding.

</details>


### [56] [Grounding Everything in Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2512.10554)
*Xiangxuan Ren,Zhongdao Wang,Liping Hou,Pin Tang,Guoqing Wang,Chao Ma*

Main category: cs.CV

TL;DR: 本文提出了一种新的空间表示方法GETok，通过引入可学习的空间token提升多模态大模型在2D视觉空间内的定位与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型由于采用自回归Transformer结构，需要将输入图像进行token化，导致对2D图像中物体的精准定位能力受限。因此，作者希望提升MLLMs在2D空间内物体定位与推理的表现。

Method: 提出GETok方法：1）通过网格token将图像划分为结构化的空间锚点；2）通过偏移token实现对物体位置的精细和迭代优化。GETok将空间关系直接嵌入token序列，无需修改已有自回归结构。

Result: 详尽实验结果表明，GETok在多种referring任务上（包括有监督微调和强化学习设定）均优于现有最先进方法。

Conclusion: GETok能有效提升MLLMs在2D空间推理和物体定位任务中的能力，在不更改模型主结构的情况下拓展了其空间理解能力。

Abstract: Multimodal large language models (MLLMs) have made significant advancements in vision understanding and reasoning. However, the autoregressive Transformer architecture used by MLLMs requries tokenization on input images, which limits their ability to accurately ground objects within the 2D image space. This raises an important question: how can sequential language tokens be improved to better ground objects in 2D spatial space for MLLMs? To address this, we present a spatial representation method for grounding objects, namely GETok, that integrates a specialized vocabulary of learnable tokens into MLLMs. GETok first uses grid tokens to partition the image plane into structured spatial anchors, and then exploits offset tokens to enable precise and iterative refinement of localization predictions. By embedding spatial relationships directly into tokens, GETok significantly advances MLLMs in native 2D space reasoning without modifying the autoregressive architecture. Extensive experiments demonstrate that GETok achieves superior performance over the state-of-the-art methods across various referring tasks in both supervised fine-tuning and reinforcement learning settings.

</details>


### [57] [Data-Efficient American Sign Language Recognition via Few-Shot Prototypical Networks](https://arxiv.org/abs/2512.10562)
*Meher Md Saad*

Main category: cs.CV

TL;DR: 本文提出了一种面向骨架数据的少样本原型网络用于手语识别，显著提升了在数据稀缺和长尾词汇分布下的识别效果，并在多个数据集上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有孤立手语识别(ISLR)方法受限于数据稀缺和手语种类分布不均（长尾效应），导致常规分类方法难以识别罕见词汇，而获取大量手语样本非常昂贵费时。为解决这一瓶颈，本文寻求一种能够适应少样本学习且泛化能力强的新方法。

Method: 提出基于骨架编码器的少样本原型网络（Few-Shot Prototypical Network）。利用分集训练方式学习语义距离空间，通过动态类别原型进行分类，而非固定决策边界。模型集成了时空图卷积网络（ST-GCN）和多尺度时序聚合模块（MSTA），高效捕捉手语的动作动态特征。

Result: 在WLASL数据集上，该方法Top-1和Top-5准确率分别为43.75%和77.10%，比采用相同骨架的标准分类方法提升超过13%。在未见过的SignASL数据集上无需微调即可获得近30%的准确率，展示了优秀的零样本泛化能力。

Conclusion: 原型网络结合时空骨架特征能够有效缓解数据稀缺和长尾分布带来的挑战，不仅在标注样本有限时优于传统分类方法，也为大规模手语词汇识别提供了可扩展的新途径。

Abstract: Isolated Sign Language Recognition (ISLR) is critical for bridging the communication gap between the Deaf and Hard-of-Hearing (DHH) community and the hearing world. However, robust ISLR is fundamentally constrained by data scarcity and the long-tail distribution of sign vocabulary, where gathering sufficient examples for thousands of unique signs is prohibitively expensive. Standard classification approaches struggle under these conditions, often overfitting to frequent classes while failing to generalize to rare ones. To address this bottleneck, we propose a Few-Shot Prototypical Network framework adapted for a skeleton based encoder. Unlike traditional classifiers that learn fixed decision boundaries, our approach utilizes episodic training to learn a semantic metric space where signs are classified based on their proximity to dynamic class prototypes. We integrate a Spatiotemporal Graph Convolutional Network (ST-GCN) with a novel Multi-Scale Temporal Aggregation (MSTA) module to capture both rapid and fluid motion dynamics. Experimental results on the WLASL dataset demonstrate the superiority of this metric learning paradigm: our model achieves 43.75% Top-1 and 77.10% Top-5 accuracy on the test set. Crucially, this outperforms a standard classification baseline sharing the identical backbone architecture by over 13%, proving that the prototypical training strategy effectively outperforms in a data scarce situation where standard classification fails. Furthermore, the model exhibits strong zero-shot generalization, achieving nearly 30% accuracy on the unseen SignASL dataset without fine-tuning, offering a scalable pathway for recognizing extensive sign vocabularies with limited data.

</details>


### [58] [Audio-sync Video Instance Editing with Granularity-Aware Mask Refiner](https://arxiv.org/abs/2512.10571)
*Haojie Zheng,Shuchen Weng,Jingqi Liu,Siqi Yang,Boxin Shi,Xinlong Wang*

Main category: cs.CV

TL;DR: 这篇论文提出AVI-Edit框架，实现了更精准的音视频同步实例级视频编辑，支持细粒度的空间和时间控制，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频编辑方法忽视了音视频同步且缺乏实例级的细粒度可控性，导致现实需求（如音画精准同步、精细编辑）难以满足。

Method: 提出了一种分辨率感知的掩码细化器（granularity-aware mask refiner），将粗糙的用户输入掩码迭代优化为准确的实例级区域，并设计了自反馈音频代理，实现高质量音频引导，实现精细的时间控制。同时，构建了一个大规模、带有实例对应和丰富标注的数据集。

Result: 通过大量实验，证明AVI-Edit在视觉质量、条件遵循性和音视频同步性方面都超过了现有最新方法。

Conclusion: AVI-Edit框架为实例级音视频同步编辑提供了有效方案，显著提升了编辑精度和用户控制性，对音视频内容创作有重要意义。

Abstract: Recent advancements in video generation highlight that realistic audio-visual synchronization is crucial for engaging content creation. However, existing video editing methods largely overlook audio-visual synchronization and lack the fine-grained spatial and temporal controllability required for precise instance-level edits. In this paper, we propose AVI-Edit, a framework for audio-sync video instance editing. We propose a granularity-aware mask refiner that iteratively refines coarse user-provided masks into precise instance-level regions. We further design a self-feedback audio agent to curate high-quality audio guidance, providing fine-grained temporal control. To facilitate this task, we additionally construct a large-scale dataset with instance-centric correspondence and comprehensive annotations. Extensive experiments demonstrate that AVI-Edit outperforms state-of-the-art methods in visual quality, condition following, and audio-visual synchronization. Project page: https://hjzheng.net/projects/AVI-Edit/.

</details>


### [59] [Unleashing Degradation-Carrying Features in Symmetric U-Net: Simpler and Stronger Baselines for All-in-One Image Restoration](https://arxiv.org/abs/2512.10581)
*Wenlong Jiao,Heyang Lee,Ping Wang,Pengfei Zhu,Qinghua Hu,Dongwei Ren*

Main category: cs.CV

TL;DR: 提出了一种对称U-Net结构的全能图像复原方法，利用简单架构实现多种退化处理，性能优于复杂模型，并结合CLIP特征进一步提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有全能型图像复原方法为处理多种退化类型，通常采用复杂的多专家模型或扩散模型，带来高计算复杂度和实施难度。作者认为，如果特征提取设计得当，则可以在更简单的架构下高效恢复图像质量，因此希望简化模型结构，并提升效率和可解释性。

Method: 提出了SymUNet，一种基于对称U-Net架构的方法，通过对称设计保证编码器与解码器特征尺度一致，增强跨尺度特征传递，仅采用简单的加性跳跃连接即可实现高效退化信息流通。在此基础上，提出SE-SymUNet，通过引入冻结CLIP特征和交叉注意力，提升语义退化线索的利用效率。

Result: 在多个基准数据集上的实验表示，SymUNet和SE-SymUNet相比主流复杂方法表现更优，同时降低了计算开销。各类退化的恢复指标均取得SOTA（State Of The Art）或有竞争力的成绩。

Conclusion: 对称U-Net结构（SymUNet及其语义增强版本）能够以更简洁高效的方式实现SOTA级别的全能图像复原，有效简化了模型设计并为后续研究提供了更坚实、更易推广的基础。作者已开源代码促进后续研究。

Abstract: All-in-one image restoration aims to handle diverse degradations (e.g., noise, blur, adverse weather) within a unified framework, yet existing methods increasingly rely on complex architectures (e.g., Mixture-of-Experts, diffusion models) and elaborate degradation prompt strategies. In this work, we reveal a critical insight: well-crafted feature extraction inherently encodes degradation-carrying information, and a symmetric U-Net architecture is sufficient to unleash these cues effectively. By aligning feature scales across encoder-decoder and enabling streamlined cross-scale propagation, our symmetric design preserves intrinsic degradation signals robustly, rendering simple additive fusion in skip connections sufficient for state-of-the-art performance. Our primary baseline, SymUNet, is built on this symmetric U-Net and achieves better results across benchmark datasets than existing approaches while reducing computational cost. We further propose a semantic enhanced variant, SE-SymUNet, which integrates direct semantic injection from frozen CLIP features via simple cross-attention to explicitly amplify degradation priors. Extensive experiments on several benchmarks validate the superiority of our methods. Both baselines SymUNet and SE-SymUNet establish simpler and stronger foundations for future advancements in all-in-one image restoration. The source code is available at https://github.com/WenlongJiao/SymUNet.

</details>


### [60] [Salient Object Detection in Complex Weather Conditions via Noise Indicators](https://arxiv.org/abs/2512.10592)
*Quan Chen,Xiaokai Yang,Tingyu Wang,Rongfeng Lu,Xichun Sheng,Yaoqi Sun,Chenggang Yan*

Main category: cs.CV

TL;DR: 本文提出了一种针对各种气象条件的显著性目标检测（SOD）新框架，引入了能感知天气噪声的特定编码器和可替换的解码器，通过噪声指示向量和融合模块提升复杂天气下的分割准确率。


<details>
  <summary>Details</summary>
Motivation: 当前SOD方法大多假设理想、低噪声环境，未能关注实际复杂天气下的图像噪声对分割效果的负面影响，因此亟需针对不同天气噪声设计适应性的SOD框架。

Method: 框架包含：1）特定编码器，2）可替换解码器。创新地引入一维one-hot噪声指示符表示天气类型，并设计了噪声指示融合模块（NIFM），将语义特征和噪声信息联合输入，并在编码器各阶段间插入NIFM，通过自适应调整特征实现天气感知。该编码器与各种主流SOD解码器兼容。

Result: 在WXSOD数据集上，分别以100%、50%、30%的训练集规模，结合3种编码器及7种解码器进行实验。结果显示，本文方法特别是加入NIFM的特定编码器，相较于普通编码器，能在复杂天气条件下显著提升分割准确率。

Conclusion: 该工作验证了融合天气噪声指示因素的方法在复杂天气下提高SOD性能的有效性，且具备良好的模块兼容性和泛化能力。

Abstract: Salient object detection (SOD), a foundational task in computer vision, has advanced from single-modal to multi-modal paradigms to enhance generalization. However, most existing SOD methods assume low-noise visual conditions, overlooking the degradation of segmentation accuracy caused by weather-induced noise in real-world scenarios. In this paper, we propose a SOD framework tailored for diverse weather conditions, encompassing a specific encoder and a replaceable decoder. To enable handling of varying weather noises, we introduce a one-hot vector as a noise indicator to represent different weather types and design a Noise Indicator Fusion Module (NIFM). The NIFM takes both semantic features and the noise indicator as dual inputs and is inserted between consecutive stages of the encoder to embed weather-aware priors via adaptive feature modulation. Critically, the proposed specific encoder retains compatibility with mainstream SOD decoders. Extensive experiments are conducted on the WXSOD dataset under varying training data scales (100%, 50%, 30% of the full training set), three encoder and seven decoder configurations. Results show that the proposed SOD framework (particularly the NIFM-enhanced specific encoder) improves segmentation accuracy under complex weather conditions compared to a vanilla encoder.

</details>


### [61] [Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval](https://arxiv.org/abs/2512.10596)
*J. Xiao,Y. Guo,X. Zi,K. Thiyagarajan,C. Moreira,M. Prasad*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本的遥感图像语义检索方法，完全无需额外训练，且表现优异。


<details>
  <summary>Details</summary>
Motivation: 遥感图像语义检索面临“语义鸿沟”问题，即视觉特征与人类高层语义之间的差距。当前虽然大模型有潜力，但依赖昂贵的训练且缺乏实际评价基准。

Method: 作者构建了包含多结构化描述的新数据集RSRT，并提出了完全训练自由、纯文本方式的检索方法TRSLLaVA，将跨模态检索转化为文本间匹配，利用VLM生成的丰富描述文本进行检索。

Result: 在RSITMD和RSICD等主流基准测试上，TRSLLaVA无需训练即取得了与有监督SOTA模型相当甚至更优的表现。例如，在RSITMD上达到42.62%的mean Recall，是标准zero-shot CLIP的近两倍，也超过一些顶尖有监督模型。

Conclusion: 高质量结构化文本语义表示可极大提升遥感图像检索效果，且无需额外代价，是一种高效且有前景的新范式。

Abstract: Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \textquote{semantic gap}, the discrepancy between a model's low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\%, nearly doubling the 23.86\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.

</details>


### [62] [Track and Caption Any Motion: Query-Free Motion Discovery and Description in Videos](https://arxiv.org/abs/2512.10607)
*Bishoy Galoaa,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: TCAM提出了一种基于运动的自动视频理解框架，可在无需用户查询的情况下自主发现和描述视频中的运动模式，并通过运动场注意力机制将自然语言描述与相应轨迹绑定。实验结果表明，其在多项任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法往往过于依赖静态外观特征，对遮挡、伪装和快速运动等复杂场景下的视频理解效果有限。作者旨在利用运动特征和视觉-语言对齐，提升自动化、无查询视频理解的能力。

Method: TCAM框架包括：1）自主视频观察与多种运动活动的检测；2）基于运动场注意力机制实现空间定位和轨迹绑定；3）结合全局视频-文本对齐与细粒度空间对应的统一训练，采用多头交叉注意力以实现多种运动表达的自主发现并生成自然语言描述。

Result: TCAM在MeViS数据集上实现了58.4%的视频到文本检索准确率、64.9的空间定位指标（JF），每个视频平均发现4.8条相关运动描述，且精度达84.7%，展现较强的跨任务泛化能力。

Conclusion: TCAM能在无需先验查询的情况下，自动理解并准确描述视频中多种运动模式，提升了在复杂条件下视频分析的能力，在视频检索与空间定位等任务上效果突出。

Abstract: We propose Track and Caption Any Motion (TCAM), a motion-centric framework for automatic video understanding that discovers and describes motion patterns without user queries. Understanding videos in challenging conditions like occlusion, camouflage, or rapid movement often depends more on motion dynamics than static appearance. TCAM autonomously observes a video, identifies multiple motion activities, and spatially grounds each natural language description to its corresponding trajectory through a motion-field attention mechanism. Our key insight is that motion patterns, when aligned with contrastive vision-language representations, provide powerful semantic signals for recognizing and describing actions. Through unified training that combines global video-text alignment with fine-grained spatial correspondence, TCAM enables query-free discovery of multiple motion expressions via multi-head cross-attention. On the MeViS benchmark, TCAM achieves 58.4% video-to-text retrieval, 64.9 JF for spatial grounding, and discovers 4.8 relevant expressions per video with 84.7% precision, demonstrating strong cross-task generalization.

</details>


### [63] [Robust Multi-Disease Retinal Classification via Xception-Based Transfer Learning and W-Net Vessel Segmentation](https://arxiv.org/abs/2512.10608)
*Mohammad Sadegh Gholizadeh,Amir Arsalan Rezapour*

Main category: cs.CV

TL;DR: 该论文提出一种结合深度学习与可解释性影像处理的眼疾自动诊断方法，提升了准确性与临床直观性。


<details>
  <summary>Details</summary>
Motivation: 近年来威胁视力的眼科疾病发病率显著上升，亟需可扩展且高效的自动化筛查解决方案，以减轻临床压力并提高早期诊断率。

Method: 作者设计了一个结合深度特征提取与可解释影像处理模块的深度学习流程，以高保真的视网膜血管分割作为辅助任务，引导疾病分类。该方法将算法结果与临床相关形态学特征关联，提高模型可解释性。

Result: 该方法减小了传统CNN“黑箱”问题，使模型预测更贴近于医学专家的诊断标准，降低了误报率，提升了临床部署的可行性。

Conclusion: 通过引入可解释性的辅助分割任务，将算法输出与临床医学更好结合，有助于自动眼疾诊断系统在实际医疗场景中的落地应用。

Abstract: In recent years, the incidence of vision-threatening eye diseases has risen dramatically, necessitating scalable and accurate screening solutions. This paper presents a comprehensive study on deep learning architectures for the automated diagnosis of ocular conditions. To mitigate the "black-box" limitations of standard convolutional neural networks (CNNs), we implement a pipeline that combines deep feature extraction with interpretable image processing modules. Specifically, we focus on high-fidelity retinal vessel segmentation as an auxiliary task to guide the classification process. By grounding the model's predictions in clinically relevant morphological features, we aim to bridge the gap between algorithmic output and expert medical validation, thereby reducing false positives and improving deployment viability in clinical settings.

</details>


### [64] [Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces](https://arxiv.org/abs/2512.10617)
*Bishoy Galoaa,Xiangyu Bai,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 本文提出Lang2Motion框架，实现了通过语言引导生成物体显式运动轨迹，显著优于现有视频方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于人体或视频合成，缺少面向任意目标物体且通过自然语言引导的轨迹生成方法。作者旨在提升基于语言描述生成精确轨迹的能力，并拓展至更多应用场景。

Method: 提出基于Transformer的自编码器框架，通过跟踪真实世界视频的物体点运动提取轨迹，利用CLIP的冻结文本与视觉编码器进行双重监督，分别编码文本描述和可视化运动轨迹，实现联合嵌入空间对齐。

Result: 在text-to-trajectory检索中Recall@1达34.2%，比现有视频方法高12.5个百分点。运动准确率提升33-52%。在人类动作识别上Top-1准确率为88.3%，且仅用多样化物体运动训练。

Conclusion: Lang2Motion显著提升了基于自然语言的运动轨迹生成与理解能力，在轨迹风格迁移、语义插值和潜空间编辑等方面展现出广泛应用潜力。

Abstract: We present Lang2Motion, a framework for language-guided point trajectory generation by aligning motion manifolds with joint embedding spaces. Unlike prior work focusing on human motion or video synthesis, we generate explicit trajectories for arbitrary objects using motion extracted from real-world videos via point tracking. Our transformer-based auto-encoder learns trajectory representations through dual supervision: textual motion descriptions and rendered trajectory visualizations, both mapped through CLIP's frozen encoders. Lang2Motion achieves 34.2% Recall@1 on text-to-trajectory retrieval, outperforming video-based methods by 12.5 points, and improves motion accuracy by 33-52% (12.4 ADE vs 18.3-25.3) compared to video generation baselines. We demonstrate 88.3% Top-1 accuracy on human action recognition despite training only on diverse object motions, showing effective transfer across motion domains. Lang2Motion supports style transfer, semantic interpolation, and latent-space editing through CLIP-aligned trajectory representations.

</details>


### [65] [DOCR-Inspector: Fine-Grained and Automated Evaluation of Document Parsing with VLM](https://arxiv.org/abs/2512.10619)
*Qintong Zhang,Junyuan Zhang,Zhifei Ren,Linke Ouyang,Zichen Wen,Junbo Niu,Yuan Qu,Bin Wang,Ka-Ho Chow,Conghui He,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出DOCR-Inspector系统，对文档解析结果进行细粒度错误分析与评估，有效提升真实场景下的解析质量评估能力，并显著优于现有商用及开源模型。


<details>
  <summary>Details</summary>
Motivation: 当前文档解析任务虽受益于视觉-语言模型（VLMs）取得进展，但主流评估方法依赖标准基准测试，存在数据集偏见和只反映整体分数的问题，导致模型在真实应用中的表现不一致，难以定位具体误差和改进空间。因此，需要一种更细致、全面的解析质量评估方法。

Method: 作者提出DOCR-Inspector，通过“VLM-as-a-Judge”范式对文档图像及其解析输出进行分析，对所有错误进行细致分类（共28种），并给出综合质量评估。为实现该方法，作者构建了DOCRcase-200K训练集，设计了Chain-of-Checklist推理范式，提升评估结构化层次。

Result: 在新引入的DOCRcaseBench手工标注基准上，DOCR-Inspector-7B模型在评估和解析能力上超过了Gemini 2.5 Pro等商用模型及主流开源模型。实验还表明，DOCR-Inspector能够为解析结果优化提供有价值的指导。

Conclusion: DOCR-Inspector可作为高效的文档解析评估工具，既能驱动模型性能提升，也可为实际部署中的结果优化提供帮助，推动文档解析系统大规模发展。模型与代码已开源。

Abstract: Document parsing aims to transform unstructured PDF images into semi-structured data, facilitating the digitization and utilization of information in diverse domains. While vision language models (VLMs) have significantly advanced this task, achieving reliable, high-quality parsing in real-world scenarios remains challenging. Common practice often selects the top-performing model on standard benchmarks. However, these benchmarks may carry dataset-specific biases, leading to inconsistent model rankings and limited correlation with real-world performance. Moreover, benchmark metrics typically provide only overall scores, which can obscure distinct error patterns in output. This raises a key challenge: how can we reliably and comprehensively assess document parsing quality in the wild? We address this problem with DOCR-Inspector, which formalizes document parsing assessment as fine-grained error detection and analysis. Leveraging VLM-as-a-Judge, DOCR-Inspector analyzes a document image and its parsed output, identifies all errors, assigns them to one of 28 predefined types, and produces a comprehensive quality assessment. To enable this capability, we construct DOCRcase-200K for training and propose the Chain-of-Checklist reasoning paradigm to enable the hierarchical structure of parsing quality assessment. For empirical validation, we introduce DOCRcaseBench, a set of 882 real-world document parsing cases with manual annotations. On this benchmark, DOCR-Inspector-7B outperforms commercial models like Gemini 2.5 Pro, as well as leading open-source models. Further experiments demonstrate that its quality assessments provide valuable guidance for parsing results refinement, making DOCR-Inspector both a practical evaluator and a driver for advancing document parsing systems at scale. Model and code are released at: https://github.com/ZZZZZQT/DOCR-Inspector.

</details>


### [66] [K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices](https://arxiv.org/abs/2512.10628)
*Bishoy Galoaa,Pau Closas,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: K-Track是一种通用的、高效点追踪加速框架，结合深度学习与Kalman滤波，显著降低了计算资源消耗，并保留了较高的准确率，实现了在边缘设备上的实时性能。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习点追踪在准确性上表现优异，但由于其对每帧都需GPU推理，难以在算力受限的边缘设备上部署，亟需提升推理效率、减少能耗。

Method: K-Track结合稀疏深度学习关键帧更新和轻量级Kalman滤波进行中间帧预测，并采用贝叶斯不确定性传播以维持时序连贯性，从而大幅减少每帧对深度网络的依赖。

Result: 在多个先进点追踪算法和硬件平台（如NVIDIA Jetson Nano、RTX Titan）上评估，K-Track实现了5-10倍的速度提升，且保留了超过85%的原始算法准确率。

Conclusion: K-Track显著降低了点追踪的计算需求，为高质量点追踪在实际、资源受限环境下的部署提供了高效、实用的解决方案，推动了视觉算法的现实落地。

Abstract: Point tracking in video sequences is a foundational capability for real-world computer vision applications, including robotics, autonomous systems, augmented reality, and video analysis. While recent deep learning-based trackers achieve state-of-the-art accuracy on challenging benchmarks, their reliance on per-frame GPU inference poses a major barrier to deployment on resource-constrained edge devices, where compute, power, and connectivity are limited. We introduce K-Track (Kalman-enhanced Tracking), a general-purpose, tracker-agnostic acceleration framework designed to bridge this deployment gap. K-Track reduces inference cost by combining sparse deep learning keyframe updates with lightweight Kalman filtering for intermediate frame prediction, using principled Bayesian uncertainty propagation to maintain temporal coherence. This hybrid strategy enables 5-10X speedup while retaining over 85% of the original trackers' accuracy. We evaluate K-Track across multiple state-of-the-art point trackers and demonstrate real-time performance on edge platforms such as the NVIDIA Jetson Nano and RTX Titan. By preserving accuracy while dramatically lowering computational requirements, K-Track provides a practical path toward deploying high-quality point tracking in real-world, resource-limited settings, closing the gap between modern tracking algorithms and deployable vision systems.

</details>


### [67] [TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection](https://arxiv.org/abs/2512.10652)
*Jian-Yu Jiang-Lin,Kang-Yang Huang,Ling Zou,Ling Lo,Sheng-Ping Yang,Yu-Wen Tseng,Kun-Hsiang Lin,Chia-Ling Chen,Yu-Ting Ta,Yan-Tsung Wang,Po-Ching Chen,Hongxia Xie,Hong-Han Shuai,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: 作者提出了一个名为TriDF的全新DeepFake检测基准，涵盖图像、视频和音频的多种伪造类型，并评估检测模型的感知、检测和解释三大能力。


<details>
  <summary>Details</summary>
Motivation: 随着生成式模型的进步，伪造现实人物形象变得越来越容易，这对安全、通信和公众信任带来了严重威胁。目前DeepFake检测系统不仅要辨别伪造，还需给出可解释的推理，因此急需更全面、可解释性强的检测基准。

Method: 作者构建了TriDF基准，包含16种类型的高质量伪造（覆盖图像、视频和音频），并设计了三大评估维度：1）感知能力，评估模型利用人工标注证据发现细粒度伪造的能力；2）检测能力，评估模型在不同伪造类型和生成器间的分类性能；3）幻觉能力，衡量模型生成的解释的可靠性。并在多模态大语言模型上进行了实验验证。

Result: 实验发现，良好的感知能力对检测可靠性至关重要，而解释‘幻觉’会严重影响决策，显示三者之间高度关联。某些模型容易在解释环节出现幻觉，降低信任度。

Conclusion: TriDF基准为理解DeepFake检测中的准确性、证据识别、解释可靠性三者间的互依关系提供了统一框架，并为构建可信赖、应对现实伪造威胁的检测系统奠定基础。

Abstract: Advances in generative modeling have made it increasingly easy to fabricate realistic portrayals of individuals, creating serious risks for security, communication, and public trust. Detecting such person-driven manipulations requires systems that not only distinguish altered content from authentic media but also provide clear and reliable reasoning. In this paper, we introduce TriDF, a comprehensive benchmark for interpretable DeepFake detection. TriDF contains high-quality forgeries from advanced synthesis models, covering 16 DeepFake types across image, video, and audio modalities. The benchmark evaluates three key aspects: Perception, which measures the ability of a model to identify fine-grained manipulation artifacts using human-annotated evidence; Detection, which assesses classification performance across diverse forgery families and generators; and Hallucination, which quantifies the reliability of model-generated explanations. Experiments on state-of-the-art multimodal large language models show that accurate perception is essential for reliable detection, but hallucination can severely disrupt decision-making, revealing the interdependence of these three aspects. TriDF provides a unified framework for understanding the interaction between detection accuracy, evidence identification, and explanation reliability, offering a foundation for building trustworthy systems that address real-world synthetic media threats.

</details>


### [68] [NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation](https://arxiv.org/abs/2512.10660)
*Hanfeng Wu,Marlon Steiner,Michael Schmidt,Alvaro Marcos-Ramiro,Christoph Stiller*

Main category: cs.CV

TL;DR: 提出了NaviHydra，一种可控的导航引导自动驾驶端到端模型，通过从规则型仿真器蒸馏得到，能够根据高层导航指令生成安全轨迹，并在NAVSIM基准上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统规则系统难以应对动态环境，端到端方法则难以显式遵循导航指令，两者都无法同时保证安全和灵活性，因此需要一种同时兼顾可控性和安全性的自动驾驶模型。

Method: 提出了NaviHydra框架，结合规则型仿真器的蒸馏学习，利用BEV（鸟瞰图）轨迹采集方法提升轨迹特征提取能力，引入新的导航合规性评估指标，设计了针对多种导航指令的测试方案，全面衡量模型可控性。

Result: NaviHydra模型在各种导航指令下表现出优秀的可控性和安全性，在NAVSIM基准测试中显著优于现有方法，达到最新最佳成绩。

Conclusion: NaviHydra方法有效提升了自动驾驶的可控性和安全性，为端到端自动驾驶模型提供了更好的导航引导能力，有助于推动自动驾驶领域的发展。

Abstract: The complexity of autonomous driving scenarios requires robust models that can interpret high-level navigation commands and generate safe trajectories. While traditional rule-based systems can react to these commands, they often struggle in dynamic environments, and end-to-end methods face challenges in complying with explicit navigation commands. To address this, we present NaviHydra, a controllable navigation-guided end-to-end model distilled from an existing rule-based simulator. Our framework accepts high-level navigation commands as control signals, generating trajectories that align with specified intentions. We utilize a Bird's Eye View (BEV) based trajectory gathering method to enhance the trajectory feature extraction. Additionally, we introduce a novel navigation compliance metric to evaluate adherence to intended route, improving controllability and navigation safety. To comprehensively assess our model's controllability, we design a test that evaluates its response to various navigation commands. Our method significantly outperforms baseline models, achieving state-of-the-art results in the NAVSIM benchmark, demonstrating its effectiveness in advancing autonomous driving.

</details>


### [69] [XDen-1K: A Density Field Dataset of Real-World Objects](https://arxiv.org/abs/2512.10668)
*Jingxuan Zhang,Tianqi Yu,Yatu Zhang,Jinze Wu,Kaixin Yao,Jingyang Liu,Yuyao Zhang,Jiayuan Gu,Jingyi Yu*

Main category: cs.CV

TL;DR: 提出了首个针对物理属性（尤其是体密度）的大规模多模态真实世界数据集XDen-1K，涵盖1000个物体并配有详尽的3D模型和双平面X射线数据，推动物理属性感知任务的发展。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型多关注物体外形和表面属性，忽略了内部物理属性（如体密度），而这些信息对质量中心、稳定性和机器人操作等任务至关重要。主要障碍是缺乏大规模真实物理属性数据。

Method: 构建了包含148类1000个真实物体的多模态数据集XDen-1K，数据内容包括高分辨率3D模型、部件级标注及真实世界双平面X射线。提出了新的优化方法，用稀疏X射线视图还原物体高保真体密度场，并将X射线数据作为分割网络的条件输入，进行体积分割实验。

Result: 实验证明，结合XDen-1K数据集能有效提升物体质量中心估计准确性和机器人操作成功率，并改善了体积分割性能。

Conclusion: XDen-1K数据集为物理属性视觉推理和嵌入式AI研究提供了基础资源和新基准，将推动相关领域研究进一步发展。

Abstract: A deep understanding of the physical world is a central goal for embodied AI and realistic simulation. While current models excel at capturing an object's surface geometry and appearance, they largely neglect its internal physical properties. This omission is critical, as properties like volumetric density are fundamental for predicting an object's center of mass, stability, and interaction dynamics in applications ranging from robotic manipulation to physical simulation. The primary bottleneck has been the absence of large-scale, real-world data. To bridge this gap, we introduce XDen-1K, the first large-scale, multi-modal dataset designed for real-world physical property estimation, with a particular focus on volumetric density. The core of this dataset consists of 1,000 real-world objects across 148 categories, for which we provide comprehensive multi-modal data, including a high-resolution 3D geometric model with part-level annotations and a corresponding set of real-world biplanar X-ray scans. Building upon this data, we introduce a novel optimization framework that recovers a high-fidelity volumetric density field of each object from its sparse X-ray views. To demonstrate its practical value, we add X-ray images as a conditioning signal to an existing segmentation network and perform volumetric segmentation. Furthermore, we conduct experiments on downstream robotics tasks. The results show that leveraging the dataset can effectively improve the accuracy of center-of-mass estimation and the success rate of robotic manipulation. We believe XDen-1K will serve as a foundational resource and a challenging new benchmark, catalyzing future research in physically grounded visual inference and embodied AI.

</details>


### [70] [Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching](https://arxiv.org/abs/2512.10674)
*Javier Villena Toro,Mehdi Tarkian*

Main category: cs.CV

TL;DR: Geo6DPose是一种无需训练的新型6D姿态估计算法，在精度接近大模型的前提下实现了本地、低延迟、快速推理。


<details>
  <summary>Details</summary>
Motivation: 现有的zero-shot 6D姿态估计方法高度依赖大模型和云端推理，导致高延迟、高能耗、数据安全与成本问题，这与实际机器人部署所需的本地、低算力推理需求相冲突。

Method: Geo6DPose采用无训练、全本地的推理管线，将基础视觉模型的特征与几何过滤策略结合。具体为：使用DINO模板特征与场景块计算相似度，建立互相关系，通过投影实现空间对应，最后用RANSAC与加权几何一致性指标恢复并排序姿态结果，提升对噪声、遮挡和场景复杂度的鲁棒性。

Result: Geo6DPose在commodity GPU上可实现1.08 FPS（亚秒级速度），平均召回率53.7，与大规模zero-shot基线持平，但模型极为轻量，无需训练和网络连接。

Conclusion: Geo6DPose推动了6D姿态估计方法在没有云端、无训练前提下的本地化部署，为实际机器人应用提供了可行的低算力解决方案，且可兼容新基础视觉模型。

Abstract: Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment.

</details>


### [71] [Optimal transport unlocks end-to-end learning for single-molecule localization](https://arxiv.org/abs/2512.10683)
*Romain Seailles,Jean-Baptiste Masson,Jean Ponce,Julien Mairal*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度学习方法，将单分子定位显微镜（SMLM）的训练目标重新表述为集合匹配问题，采用最优传输损失函数，避免非极大值抑制（NMS），提升了高密度发射体下的定位准确度和训练效果。新架构在数据集上表现超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SMLM方法为避免萤光分子信号重叠，需长时间成像，不适合活细胞成像。近年深度学习方法虽能处理密集信号，但依赖非极大值抑制（NMS），难以端到端训练且易丢失真实信号，亟需新方案。

Method: 作者将SMLM问题转化为集合匹配，设计了基于最优传输的损失函数，完全替代NMS，支持端到端训练。同时提出集成显微镜光学系统知识的迭代神经网络模型。方法在合成和真实生物数据集上验证。

Result: 新方法在模拟和真实实验数据中，在中高密度发射体检测准确率和整体性能上均优于现有深度学习方法。

Conclusion: 最优传输损失与专门迭代架构提升了SMLM在发射体高密度情况下的定位精度，对后续高分辨率成像领域具有推广意义。

Abstract: Single-molecule localization microscopy (SMLM) allows reconstructing biology-relevant structures beyond the diffraction limit by detecting and localizing individual fluorophores -- fluorescent molecules stained onto the observed specimen -- over time to reconstruct super-resolved images. Currently, efficient SMLM requires non-overlapping emitting fluorophores, leading to long acquisition times that hinders live-cell imaging. Recent deep-learning approaches can handle denser emissions, but they rely on variants of non-maximum suppression (NMS) layers, which are unfortunately non-differentiable and may discard true positives with their local fusion strategy. In this presentation, we reformulate the SMLM training objective as a set-matching problem, deriving an optimal-transport loss that eliminates the need for NMS during inference and enables end-to-end training. Additionally, we propose an iterative neural network that integrates knowledge of the microscope's optical system inside our model. Experiments on synthetic benchmarks and real biological data show that both our new loss function and architecture surpass the state of the art at moderate and high emitter densities. Code is available at https://github.com/RSLLES/SHOT.

</details>


### [72] [Sharp Monocular View Synthesis in Less Than a Second](https://arxiv.org/abs/2512.10685)
*Lars Mescheder,Wei Dong,Shiwei Li,Xuyang Bai,Marcel Santos,Peiyun Hu,Bruno Lecouat,Mingmin Zhen,Amaël Delaunoy,Tian Fang,Yanghai Tsin,Stephan R. Richter,Vladlen Koltun*

Main category: cs.CV

TL;DR: SHARP是一种从单张图片实现写实视角合成的新方法，利用神经网络快速回归三维高斯表示，可实时生成高分辨率图像，并在多项指标上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前从单张图片推断三维场景并生成真实感视角极具挑战，算法常受限于合成速度慢、视角真实性不足及泛化能力弱。SHARP旨在解决这些问题，提出实时、尺度精确、泛化性强的单图三维重建方法。

Method: 方法核心为神经网络单次前向推理，从输入图片直接回归场景的3D高斯参数表示。该表示易于实时渲染和支持绝对尺度的相机移动。整个流程在标准GPU上不到一秒完成。

Result: SHARP在多个数据集上零样本泛化能力强，并在LPIPS和DISTS等指标上较现有最佳方法显著降低误差，合成速度提升约1000倍。

Conclusion: SHARP实现了更快、更真实的单图视角合成，对三维重建与相关应用具有较大推动作用。实验验证了其实用性与先进性。

Abstract: We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp

</details>


### [73] [CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images](https://arxiv.org/abs/2512.10715)
*Matias Cosarinsky,Nicolas Gaggion,Rodrigo Echeveste,Enzo Ferrante*

Main category: cs.CV

TL;DR: 本文提出针对胸部X光解剖标志点分割任务的多种不确定性估计方法，通过混合神经网络结构及其变分潜在空间衍生两种互补的不确定性度量，并在CheXmask数据集上进行实证分析。作者还发布了包含高质量不确定性注释的大型数据集CheXmask-U。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像分割中的不确定性估计主要集中在像素级，忽略了具备拓扑结构保证的标志点（landmark-based）分割任务的不确定性分析。而该类任务在实际临床部署时，可靠性与安全性至关重要，因此亟需引入有效的不确定性衡量机制。

Method: 作者参考了混合型神经网络结构（卷积编码器+图生成解码器）并利用变分潜在空间，设计了两种不确定性估计方法：1）基于变分分布参数的潜在不确定性（latent uncertainty）；2）基于从潜在空间采样生成多次预测的预测不确定性（predictive uncertainty）。通过控制性扰动实验，分析不确定性度量对图像退化的响应。

Result: 实验表明，设计的两种不确定性度量均能够随扰动强度增长而相应升高，能有效反映分割全局和局部的质量退化。此外，不确定性高的预测与人为标注的错误区域高度吻合，可用于不可靠结果筛查以及数据分布外检测，对CheXmask数据集具有良好适配性。

Conclusion: 作者展示了对基于标志点的医学影像分割任务进行可信不确定性估计的可行性和必要性，相关不确定性信号有助于安全部署AI分割系统。并且开放CheXmask-U数据集，为社区带来集成了空间分布不确定性的高价值资源，推动相关研究。

Abstract: Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U.

</details>


### [74] [SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving](https://arxiv.org/abs/2512.10719)
*Peizheng Li,Zhenghao Zhang,David Holtz,Hang Yu,Yutong Yang,Yuzhi Lai,Rui Song,Andreas Geiger,Andreas Zell*

Main category: cs.CV

TL;DR: 本文提出了一种新型空间感知的视觉语言模型（VLM）用于端到端自动驾驶，叫做SpaceDrive，通过将空间信息以显式位置编码处理，显著提升了模型对三维空间关系的理解和驾驶行为规划能力，在多个公开数据集上取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 目前基于视觉语言模型的自动驾驶在大规模预训练下展现出强大的视觉理解和推理能力，但其对细粒度三维空间关系的理解薄弱，而这对于物理世界交互的系统是基本要求。

Method: 提出SpaceDrive，将空间信息以三维显式位置编码（而非逐位文本数字）注入VLM；使用通用位置编码器对多视角深度、历史自车状态、文本指令等三维坐标进行编码；增强2D视觉标记的同时，用空间坐标直接替换数字型输入输出，支持任务无关的空间推理与规划。

Result: 大量实验表明SpaceDrive在nuScenes数据集获得开放环节最优性能，并在Bench2Drive闭环测试中取得78.02的驾驶评分，位居同类VLM方法第二。

Conclusion: 以空间为中心的位置编码机制让VLM能够更好地进行语义与空间联合推理，提升三维物理环境中的自动驾驶性能，有望推动VLM在真实自动驾驶场景中的应用。

Abstract: End-to-end autonomous driving methods built on vision language models (VLMs) have undergone rapid development driven by their universal visual understanding and strong reasoning capabilities obtained from the large-scale pretraining. However, we find that current VLMs struggle to understand fine-grained 3D spatial relationships which is a fundamental requirement for systems interacting with the physical world. To address this issue, we propose SpaceDrive, a spatial-aware VLM-based driving framework that treats spatial information as explicit positional encodings (PEs) instead of textual digit tokens, enabling joint reasoning over semantic and spatial representations. SpaceDrive employs a universal positional encoder to all 3D coordinates derived from multi-view depth estimation, historical ego-states, and text prompts. These 3D PEs are first superimposed to augment the corresponding 2D visual tokens. Meanwhile, they serve as a task-agnostic coordinate representation, replacing the digit-wise numerical tokens as both inputs and outputs for the VLM. This mechanism enables the model to better index specific visual semantics in spatial reasoning and directly regress trajectory coordinates rather than generating digit-by-digit, thereby enhancing planning accuracy. Extensive experiments validate that SpaceDrive achieves state-of-the-art open-loop performance on the nuScenes dataset and the second-best Driving Score of 78.02 on the Bench2Drive closed-loop benchmark over existing VLM-based methods.

</details>


### [75] [Video Depth Propagation](https://arxiv.org/abs/2512.10725)
*Luigi Piccinelli,Thiemo Wandel,Christos Sakaridis,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

TL;DR: 作者提出了VeloDepth，一种高效且鲁棒的视频深度估计算法，能在保证实时性的同时提升帧间深度一致性和准确度。


<details>
  <summary>Details</summary>
Motivation: 现有视频深度估计方法要么单帧处理且忽略时间一致性，要么需要高昂的时序建模，难以满足实时和实际应用需求，因此亟需一种高效且一致的深度估计方法。

Method: VeloDepth通过引入一种新的Propogation Module，对历史帧的深度特征进行流引导变换，并结合学习到的残差修正，实现深度特征与预测的高效传播与精细优化，从结构上保证时间一致性。

Result: 在多个基准测试中，VeloDepth在时间一致性和准确率方面实现了最新水平，推理速度显著超越现有方法。

Conclusion: VeloDepth为实时视频深度估计提供了高效、实用且高准确性的解决方案，适用于各类视觉感知任务。

Abstract: Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth

</details>


### [76] [IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation](https://arxiv.org/abs/2512.10730)
*Yuan-Ming Li,Qize Yang,Nan Lei,Shenghao Fu,Ling-An Zeng,Jian-Fang Hu,Xihan Wei,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 本论文提出IRMoGen范式，通过交替进行动作生成、评估和优化，实现动作理解与生成的互动提升，并开发了IRG-MotionLLM模型，在标准测试中取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 当前运动感知大语言模型在动作理解与生成任务方面有进步，但大多将理解与生成分开处理，未能实现任务间的双向知识流与反馈，限制了性能提升。作者发现引入动作评估和优化任务能作为连接理解与生成的桥梁，从而促进知识交互。

Method: 提出了交织推理的动作生成新范式（IRMoGen），让动作生成、评估和优化以文本-动作对话方式交互迭代；并设计了IRG-MotionLLM模型，采用三阶段训练法逐步培养此能力，并自动生成用于训练的交错推理标注数据。

Result: 实验证明：（1）动作评估与优化任务能有效提升文本与动作的对齐度；（2）将三者交替进行带来显著且持续的性能提升；（3）IRG-MotionLLM在各项标准text-to-motion生成基准测试中超越了现有模型，效果领先。

Conclusion: 本文建立的IRMoGen范式和IRG-MotionLLM模型有效整合了生成、评估和优化环节，推动了动作生成领域的性能提升，并为运动理解和生成的联合建模提供新思路。

Abstract: Recent advances in motion-aware large language models have shown remarkable promise for unifying motion understanding and generation tasks. However, these models typically treat understanding and generation separately, limiting the mutual benefits that could arise from interactive feedback between tasks. In this work, we reveal that motion assessment and refinement tasks act as crucial bridges to enable bidirectional knowledge flow between understanding and generation. Leveraging this insight, we propose Interleaved Reasoning for Motion Generation (IRMoGen), a novel paradigm that tightly couples motion generation with assessment and refinement through iterative text-motion dialogue. To realize this, we introduce IRG-MotionLLM, the first model that seamlessly interleaves motion generation, assessment, and refinement to improve generation performance. IRG-MotionLLM is developed progressively with a novel three-stage training scheme, initializing and subsequently enhancing native IRMoGen capabilities. To facilitate this development, we construct an automated data engine to synthesize interleaved reasoning annotations from existing text-motion datasets. Extensive experiments demonstrate that: (i) Assessment and refinement tasks significantly improve text-motion alignment; (ii) Interleaving motion generation, assessment, and refinement steps yields consistent performance gains across training stages; and (iii) IRG-MotionLLM clearly outperforms the baseline model and achieves advanced performance on standard text-to-motion generation benchmarks. Cross-evaluator testing further validates its effectiveness. Code & Data: https://github.com/HumanMLLM/IRG-MotionLLM/tree/main.

</details>


### [77] [LDP: Parameter-Efficient Fine-Tuning of Multimodal LLM for Medical Report Generation](https://arxiv.org/abs/2512.10750)
*Tianyu Zhou,Junyi Tang,Zehui Li,Dahong Qian,Suncheng Xiang*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态大模型框架LDP，用于临床结肠镜息肉诊断自动报告生成，并显著提升了一致性、权威性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统自动化结肠镜息肉诊断报告方法在高质量多模态医学数据稀缺的情况下，常出现表述不一致、虚假内容（幻觉）问题，影响临床可信度。因此亟需一种更有效的多模态方法提升报告质量。

Method: 作者构建了包含专家注释图片和文本对的MMEndo多模态内镜数据集，并采用Qwen2-VL-7B多模态大模型，通过参数高效微调（LoRA）及直接偏好优化（DPO）使其符合临床标准，实现专业化报告生成。

Result: LDP模型在自动化指标和医学专家评审中都超过现有方法（医生评分7.2/10），并且微调训练所需算力比全量微调减少了833倍。在其它医学数据集（IU-XRay）上的验证也显示出良好的泛化鲁棒性。

Conclusion: LDP框架为基础医疗场景下专业化、可扩展的多模态诊断报告自动生成提供了有效解决方案，对医疗AI落地应用具重要推动作用。

Abstract: Colonoscopic polyp diagnosis is pivotal for early colorectal cancer detection, yet traditional automated reporting suffers from inconsistencies and hallucinations due to the scarcity of high-quality multimodal medical data. To bridge this gap, we propose LDP, a novel framework leveraging multimodal large language models (MLLMs) for professional polyp diagnosis report generation. Specifically, we curate MMEndo, a multimodal endoscopic dataset comprising expert-annotated colonoscopy image-text pairs. We fine-tune the Qwen2-VL-7B backbone using Parameter-Efficient Fine-Tuning (LoRA) and align it with clinical standards via Direct Preference Optimization (DPO). Extensive experiments show that our LDP outperforms existing baselines on both automated metrics and rigorous clinical expert evaluations (achieving a Physician Score of 7.2/10), significantly reducing training computational costs by 833x compared to full fine-tuning. The proposed solution offers a scalable, clinically viable path for primary healthcare, with additional validation on the IU-XRay dataset confirming its robustness.

</details>


### [78] [Blood Pressure Prediction for Coronary Artery Disease Diagnosis using Coronary Computed Tomography Angiography](https://arxiv.org/abs/2512.10765)
*Rene Lisasi,Michele Esposito,Chen Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一套自动化管道，通过冠状动脉CTA图像自动提取冠状动脉几何结构，并结合扩散型回归模型，能够高效预测血压分布，实现无需CFD运算即可辅助冠心病诊断。


<details>
  <summary>Details</summary>
Motivation: 传统基于CFD的冠状动脉血流动力学模拟虽然有效，但计算量大、耗时长，不利于推广至大规模临床应用，限制了AI模型训练所需的标注血流动力学数据的获取，影响基于生理参数的冠心病无创评估的广泛应用。

Method: 作者开发了一套端到端自动化管道，包括冠状动脉CT血管造影图像的几何自动提取、模拟数据自动生成和高效血压分布学习，同时提出基于扩散过程的回归模型，无需CFD计算即可根据CCTA特征预测血压分布。

Result: 在模拟冠状动脉血流动力学数据集上，所提扩散回归模型的R²达64.42%，RMSE为0.0974，归一化RMSE为0.154，优于多种基线方法，证明其有效性。

Conclusion: 该方法大大减少了人工和计算成本，实现了血压分布的快速、无创预测，为冠心病诊断提供了可扩展、易用的辅助工具，有望促进物理参数辅助临床CAD诊断的广泛应用。

Abstract: Computational fluid dynamics (CFD) based simulation of coronary blood flow provides valuable hemodynamic markers, such as pressure gradients, for diagnosing coronary artery disease (CAD). However, CFD is computationally expensive, time-consuming, and difficult to integrate into large-scale clinical workflows. These limitations restrict the availability of labeled hemodynamic data for training AI models and hinder broad adoption of non-invasive, physiology based CAD assessment. To address these challenges, we develop an end to end pipeline that automates coronary geometry extraction from coronary computed tomography angiography (CCTA), streamlines simulation data generation, and enables efficient learning of coronary blood pressure distributions. The pipeline reduces the manual burden associated with traditional CFD workflows while producing consistent training data. We further introduce a diffusion-based regression model designed to predict coronary blood pressure directly from CCTA derived features, bypassing the need for slow CFD computation during inference. Evaluated on a dataset of simulated coronary hemodynamics, the proposed model achieves state of the art performance, with an R2 of 64.42%, a root mean squared error of 0.0974, and a normalized RMSE of 0.154, outperforming several baseline approaches. This work provides a scalable and accessible framework for rapid, non-invasive blood pressure prediction to support CAD diagnosis.

</details>


### [79] [What matters for Representation Alignment: Global Information or Spatial Structure?](https://arxiv.org/abs/2512.10794)
*Jaskirat Singh,Xingjian Leng,Zongze Wu,Liang Zheng,Richard Zhang,Eli Shechtman,Saining Xie*

Main category: cs.CV

TL;DR: 本文研究了生成模型中特征对齐(representation alignment, REPA)训练中，影响生成性能的关键因素。通过对27种视觉编码器的大规模实证分析，发现空间结构信息而非全局语义表现，是对齐表示驱动生成效能的关键。提出一种小修改（iREPA），能显著提升训练收敛速度。


<details>
  <summary>Details</summary>
Motivation: 尽管常识上认为对齐目标表征的全局语义能力越强，生成模型性能越好，但实际影响机制尚未明晰。研究者希望从经验上分析并揭示，到底全局语义信息还是空间结构信息更影响生成训练表现。

Method: 作者对比测试了27种视觉编码器、不同规模、不同模型训练方案下，目标表征对REPA生成训练效果的影响，并引入两种简单技术：用卷积代替MLP投影层、引入空间归一化层，以突出空间信息的转移。

Result: 实验发现，空间结构能力而不是全局语义表现主导生成模型性能。所提出的iREPA方法（仅需简单代码修改）在多种设置下，均显著提升了REPA训练的收敛速度和效果。

Conclusion: 生成模型表征对齐训练中，空间结构信息远比全局语义指标（如分类准确率）更为重要。简化网络结构以更好迁移空间信息，是一种有效提升训练效率和性能的方法。本文工作建议未来重审表征对齐的机制与应用。

Abstract: Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its \textit{global} \revision{semantic} information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of \emph{spatial} information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in $<$4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa

</details>


### [80] [Graph Laplacian Transformer with Progressive Sampling for Prostate Cancer Grading](https://arxiv.org/abs/2512.10808)
*Masum Shah Junayed,John Derek Van Vessem,Qian Wan,Gahie Nam,Sheida Nabavi*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于图拉普拉斯注意力变换器（GLAT）与迭代细化模块（IRM）的前列腺癌切片分级方法，有效提升了模型性能和空间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的前列腺癌切片分级方法依赖随机或静态的补丁选择，导致不相关或冗余信息加入，影响进一步的性能提升。亟需开发能更精准选择和利用重要组织区域的方法。

Method: 提出的GLAT方法，利用IRM通过预训练ResNet50和基础模型打分迭代过滤，只保留最相关补丁。GLAT通过构建图结构对补丁连通性建模并施加拉普拉斯约束，联合可学习滤波机制增强判别性组织特征。同时，用凸聚合机制动态调整补丁重要性，生成更鲁棒的切片级表征。

Result: 在五个公开和一个私有数据集上实验表明，所提方法在性能和空间一致性两方面均显著优于现有主流方法，并且保持较高计算效率。

Conclusion: GLAT与IRM结合的新方法能够有效提升前列腺癌WSI分级的准确性与鲁棒性，有望扩展至其它类似大规模图像分析任务。

Abstract: Prostate cancer grading from whole-slide images (WSIs) remains a challenging task due to the large-scale nature of WSIs, the presence of heterogeneous tissue structures, and difficulty of selecting diagnostically relevant regions. Existing approaches often rely on random or static patch selection, leading to the inclusion of redundant or non-informative regions that degrade performance. To address this, we propose a Graph Laplacian Attention-Based Transformer (GLAT) integrated with an Iterative Refinement Module (IRM) to enhance both feature learning and spatial consistency. The IRM iteratively refines patch selection by leveraging a pretrained ResNet50 for local feature extraction and a foundation model in no-gradient mode for importance scoring, ensuring only the most relevant tissue regions are preserved. The GLAT models tissue-level connectivity by constructing a graph where patches serve as nodes, ensuring spatial consistency through graph Laplacian constraints and refining feature representations via a learnable filtering mechanism that enhances discriminative histological structures. Additionally, a convex aggregation mechanism dynamically adjusts patch importance to generate a robust WSI-level representation. Extensive experiments on five public and one private dataset demonstrate that our model outperforms state-of-the-art methods, achieving higher performance and spatial consistency while maintaining computational efficiency.

</details>


### [81] [Self-Ensemble Post Learning for Noisy Domain Generalization](https://arxiv.org/abs/2512.10818)
*Wang Lu,Jindong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种增强域泛化算法在标签噪声场景下鲁棒性的深度学习方法。通过自集成后学习（SEPL）发掘模型中不同隐层特征，实现更强泛化与抗噪能力。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉和机器学习方法在数据分布变化（域泛化）与标签噪声的大背景下，仍易受到伪特征的干扰，导致性能下降。作者旨在应对这两个难题，提升模型在现实应用中的鲁棒性与适应性。

Method: 提出了SEPL方法，包含特征探测训练和预测集成推理两部分：首先，利用模型中间层的不同特征，训练多个探针分类器（借助半监督算法以适应噪声标签）；随后，使用众包推理方法集成各个分类头的预测结果，提高整体性能与鲁棒性。

Result: 大量实验证明，SEPL在存在标签噪声的情况下能显著增强现有算法的鲁棒性，并在真实应用环境中展示出很强的灵活性和潜力。

Conclusion: SEPL方法有效挖掘了模型内部多样化的特征表达，提高了模型对分布转移与标签噪声的适应能力。该方法对现有域泛化和抗噪学习具有实用价值和推广意义。

Abstract: While computer vision and machine learning have made great progress, their robustness is still challenged by two key issues: data distribution shift and label noise. When domain generalization (DG) encounters noise, noisy labels further exacerbate the emergence of spurious features in deep layers, i.e. spurious feature enlargement, leading to a degradation in the performance of existing algorithms. This paper, starting from domain generalization, explores how to make existing methods rework when meeting noise. We find that the latent features inside the model have certain discriminative capabilities, and different latent features focus on different parts of the image. Based on these observations, we propose the Self-Ensemble Post Learning approach (SEPL) to diversify features which can be leveraged. Specifically, SEPL consists of two parts: feature probing training and prediction ensemble inference. It leverages intermediate feature representations within the model architecture, training multiple probing classifiers to fully exploit the capabilities of pre-trained models, while the final predictions are obtained through the integration of outputs from these diverse classification heads. Considering the presence of noisy labels, we employ semi-supervised algorithms to train probing classifiers. Given that different probing classifiers focus on different areas, we integrate their predictions using a crowdsourcing inference approach. Extensive experimental evaluations demonstrate that the proposed method not only enhances the robustness of existing methods but also exhibits significant potential for real-world applications with high flexibility.

</details>


### [82] [PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning](https://arxiv.org/abs/2512.10840)
*Jianqi Chen,Biao Zhang,Xiangjun Tang,Peter Wonka*

Main category: cs.CV

TL;DR: 本文提出了一种名为PoseGAM的多视角几何感知框架，实现了无需显式匹配即可对未知物体进行6D姿态估计，显著提升了对未见物体的泛化能力和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有6D物体姿态估计算法通常依赖于在查询图像与物体模型或模板图像间建立特征对应关系。这种方法对未见物体的泛化能力有限，且匹配过程复杂且易受环境变化影响。因此，提升泛化能力，简化流程成为迫切需求。

Method: 提出PoseGAM框架，结合多视角基础模型架构，通过两种互补机制——点云几何信息和几何表示网络的学习特征，直接从查询图和多个模板图像预测姿态。同时构建了包含超19万个物体、涵盖多种环境条件的大规模合成数据集，以增强模型的鲁棒性和泛化能力。

Result: 在多个基准评测上，PoseGAM均取得了当前最优的性能，平均提升AR（准确率）5.1%，在部分数据集上提升高达17.6%，展现出对未见物体的优异泛化能力。

Conclusion: PoseGAM无需显式匹配，即可实现对未知物体的高精度6D姿态估计，显著提升了方法的泛化性和鲁棒性，为相关任务提供了新的有效解决方案。

Abstract: 6D object pose estimation, which predicts the transformation of an object relative to the camera, remains challenging for unseen objects. Existing approaches typically rely on explicitly constructing feature correspondences between the query image and either the object model or template images. In this work, we propose PoseGAM, a geometry-aware multi-view framework that directly predicts object pose from a query image and multiple template images, eliminating the need for explicit matching. Built upon recent multi-view-based foundation model architectures, the method integrates object geometry information through two complementary mechanisms: explicit point-based geometry and learned features from geometry representation networks. In addition, we construct a large-scale synthetic dataset containing more than 190k objects under diverse environmental conditions to enhance robustness and generalization. Extensive evaluations across multiple benchmarks demonstrate our state-of-the-art performance, yielding an average AR improvement of 5.1% over prior methods and achieving up to 17.6% gains on individual datasets, indicating strong generalization to unseen objects. Project page: https://windvchen.github.io/PoseGAM/ .

</details>


### [83] [SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation](https://arxiv.org/abs/2512.10860)
*Kehong Gong,Zhengyu Wen,Mingxi Xu,Weixia He,Qi Wang,Ning Zhang,Zhengyu Li,Chenbin Li,Dongze Lian,Wei Zhao,Xiaoyu He,Mingyuan Zhang*

Main category: cs.CV

TL;DR: 本论文提出SWiT-4D，一种滑动窗口Transformer架构，实现从单目视频高质量生成无损4D(时序3D网格)资产，克服现有数据和监督的瓶颈，广泛适用于各类视频并达到领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前从单目视频转换为高质量4D网格动画资产存在巨大挑战，主要原因是自然捕获的4D网格数据集稀缺，难以端到端数据驱动训练。而现有图像到3D的生成方法有较大进展及强大的先验模型，若能更好利用，或能缓解4D数据不足困难。

Method: 提出SWiT-4D方法，将滑动窗口Transformer无缝集成到任意基于Diffusion Transformer的图像到3D生成器中，在无需额外参数的同时，实现空间—时间信息建模；同时设计基于优化的轨迹模块，恢复静态相机下的视频全局位移。其方法仅需单个短视频微调，即可实现高质量的4D网格生成。

Result: 实验表明，SWiT-4D在动物体（in-domain zoo）、C4D、Objaverse和真实野外视频等主流内外域数据集上，均显著优于现有基线方法，尤其在时序平滑性和几何保真性上表现突出。

Conclusion: SWiT-4D有效解决了4D网格生成对大规模监督数据的依赖，实现低监督、强泛化4D重建，具备实际部署潜力，为4D资产制作带来更高的效率与质量。

Abstract: Despite significant progress in 4D content generation, the conversion of monocular videos into high-quality animated 3D assets with explicit 4D meshes remains considerably challenging. The scarcity of large-scale, naturally captured 4D mesh datasets further limits the ability to train generalizable video-to-4D models from scratch in a purely data-driven manner. Meanwhile, advances in image-to-3D generation, supported by extensive datasets, offer powerful prior models that can be leveraged. To better utilize these priors while minimizing reliance on 4D supervision, we introduce SWiT-4D, a Sliding-Window Transformer for lossless, parameter-free temporal 4D mesh generation. SWiT-4D integrates seamlessly with any Diffusion Transformer (DiT)-based image-to-3D generator, adding spatial-temporal modeling across video frames while preserving the original single-image forward process, enabling 4D mesh reconstruction from videos of arbitrary length. To recover global translation, we further introduce an optimization-based trajectory module tailored for static-camera monocular videos. SWiT-4D demonstrates strong data efficiency: with only a single short (<10s) video for fine-tuning, it achieves high-fidelity geometry and stable temporal consistency, indicating practical deployability under extremely limited 4D supervision. Comprehensive experiments on both in-domain zoo-test sets and challenging out-of-domain benchmarks (C4D, Objaverse, and in-the-wild videos) show that SWiT-4D consistently outperforms existing baselines in temporal smoothness. Project page: https://animotionlab.github.io/SWIT4D/

</details>


### [84] [MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence](https://arxiv.org/abs/2512.10863)
*Jingli Lin,Runsen Xu,Shaohao Zhu,Sihan Yang,Peizhou Cao,Yunlong Ran,Miao Hu,Chenming Zhu,Yiman Xie,Yilin Long,Wenbo Hu,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 作者提出了MMSI-Video-Bench，这是一个全面评估多模态大模型(MLLMs)视频空间智能能力的新基准，通过人工标注和多维度任务覆盖，揭示现有模型与人类间存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在连续视觉输入下的空间理解能力对于其成为通用物理环境助手至关重要，但一直缺乏系统性的基准来全面测试和推动此领域的进步。

Method: 作者构建了MMSI-Video-Bench基准，通过感知、规划、预测、跨视频推理四个层面设计了1106个问题，涵盖1278个片段，涉及25个公开及自建数据集。每题经专家设计和审核，并具说明性理由。此外，还划分了室内感知、机器人和指向性三个子基准。

Result: 对25个主流开源及专有MLLMs测试，发现模型在人类表现面前普遍表现接近随机，最佳模型推理能力仍落后人类近60%。细致分析表明，模型在几何推理、运动理解、长时预测和跨视频对应上系统性失败。

Conclusion: MMSI-Video-Bench为视频空间智能的模型评测提供了坚实的基准，有助于推动该领域的进步，同时揭示了亟需突破的关键难点。

Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.

</details>


### [85] [From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models](https://arxiv.org/abs/2512.10867)
*Zongzhao Li,Xiangzhe Kong,Jiahui Su,Zongyang Ma,Mingze Li,Songyou Li,Yuelin Zhang,Yu Rong,Tingyang Xu,Deli Zhao,Wenbing Huang*

Main category: cs.CV

TL;DR: 本文提出了Microscopic Spatial Intelligence (MiSI) 概念，并基于此开发了大规模评测基准MiSI-Bench，以系统性评估视觉语言模型（VLMs）在微观空间认知方面的表现。结果显示前沿VLMs在人类水平之下，微调小模型在部分空间任务超越人类，但科学推理表现不足。


<details>
  <summary>Details</summary>
Motivation: 微观尺度上对不可见实体空间关系的理解和推理能力（MiSI）是科学发现的基础，但目前还未系统研究AI模型在此领域的表现。推动科学AGI发展需要评价和提升AI的此类能力。

Method: 作者提出MiSI-Bench基准，包含约16.3万个问答对、58.7万张图片和9类任务，从空间变换到复杂关系识别等充分考察VLMs微观空间智能。然后用现有主流VLMs及经过微调的小型模型进行实证评估。

Result: 现有最前沿的VLMs在MiSI-Bench的表现显著落后于人类。值得注意的是，微调的7B参数模型在空间变换任务上超过了人类，但在如氢键识别等科学基础任务上表现较差。

Conclusion: 当前VLMs虽在部分空间推理上已达甚至超越人类，但要实现科学AGI的目标，必须引入显式领域知识以提升模型在科学本源任务上的能力。

Abstract: This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.

</details>


### [86] [MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos](https://arxiv.org/abs/2512.10881)
*Kehong Gong,Zhengyu Wen,Weixia He,Mingxi Xu,Qi Wang,Ning Zhang,Zhengyu Li,Dongze Lian,Wei Zhao,Xiaoyu He,Mingyuan Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，可以对任意三维资产进行类无关的动作捕捉（CAMoCap），只需单目视频和任意绑定骨骼的三维模型作为提示，即可还原可以直接驱动该资产的骨骼动画。


<details>
  <summary>Details</summary>
Motivation: 目前的动作捕捉技术大多依赖特定种类或模板，难以适配不同行业与各种非人类资产。解决这一技术壁垒对于内容创作自动化与规模化具有重要意义。

Method: 提出MoCapAnything框架，包括三大可学习模块与轻量级逆运动学阶段：（1）参考提示编码器从三维资产骨骼、网格及渲染图提取关节点查询；（2）视频特征提取器通过稠密视觉描述与四维粗网格重建视频与骨骼空间之间的联系；（3）统一运动解码器融合多模态信息生成时序连续的骨骼轨迹。最后通过带有约束的逆运动学，得到资产专属的关节旋转。并设计Truebones Zoo数据集支撑实验。

Result: 实验证明无论是在标准基准测试还是真实场景视频中，该系统都能输出高质量动作动画，并能实现跨种类、跨模型结构的运动迁移。

Conclusion: MoCapAnything框架首次实现了可扩展、按需驱动的类无关3D动作捕捉，为任意三维资产提供了高效的动画生成手段，对内容创作与动画自动化具有广泛推动作用。

Abstract: Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/

</details>


### [87] [PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction](https://arxiv.org/abs/2512.10888)
*Brandon Smock,Valerie Faucon-Morin,Max Sokolov,Libin Liang,Tayyibah Khanam,Maury Courtland*

Main category: cs.CV

TL;DR: 本文提出了一个大规模的表格抽取数据集PubTables-v2，并基于此推动了多页结构识别等表格抽取任务的发展，还提出了新的基线方法POTATR。


<details>
  <summary>Details</summary>
Motivation: 现有表格抽取方法受限于缺乏高质量、注释充分的数据集，尤其是在多页表格结构识别任务上明显不足，阻碍了以视觉-语言模型为代表的新方法的进展。

Method: 作者构建了PubTables-v2这一大规模数据集，涵盖多页表格等多种挑战性任务。同时，基于该数据集，提出了Page-Object Table Transformer（POTATR），将表格转换器拓展为能够处理整页表格抽取的图像到结构图任务，并以多个领域的VLM进行评测。

Result: PubTables-v2首次为多页表格结构识别提供了大规模基准，在该基准上作者评估了多种VLM的效果，展示了当前的进展和挑战；POTATR方法在综合页级表格抽取方面表现出可行性。

Conclusion: 高质量大规模的PubTables-v2数据集推动了表格抽取任务的研究，特别是多页和结构识别任务。同时POTATR等新方法为视觉文档理解提供了新的解决思路和更全面的评估基准。

Abstract: Table extraction (TE) is a key challenge in visual document understanding. Traditional approaches detect tables first, then recognize their structure. Recently, interest has surged in developing methods, such as vision-language models (VLMs), that can extract tables directly in their full page or document context. However, progress has been difficult to demonstrate due to a lack of annotated data. To address this, we create a new large-scale dataset, PubTables-v2. PubTables-v2 supports a number of current challenging table extraction tasks. Notably, it is the first large-scale benchmark for multi-page table structure recognition. We demonstrate its usefulness by evaluating domain-specialized VLMs on these tasks and highlighting current progress. Finally, we use PubTables-v2 to create the Page-Object Table Transformer (POTATR), an image-to-graph extension of the Table Transformer to comprehensive page-level TE. Data, code, and trained models will be released.

</details>


### [88] [DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance](https://arxiv.org/abs/2512.10894)
*Peiying Zhang,Nanxuan Zhao,Matthew Fisher,Yiran Xu,Jing Liao,Difan Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多模态模型DuetSVG，联合生成图片token和SVG token，有效提升了SVG生成的质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉-语言模型(VLM)的SVG生成方法仅生成文本信息，缺少视觉信号，导致在处理复杂语义和几何一致性方面存在困难，难以生成高质量SVG。

Method: 提出DuetSVG多模态模型，端到端地联合生成图像token和SVG token，并在推理阶段引入了一种新的视觉引导策略，提升SVG解码质量。模型在图像和SVG数据集上联合训练。

Result: 实验结果表明，该方法在多个应用场景下均优于现有方法，生成的SVG在视觉效果、语义对齐和语法规范性方面表现更佳。

Conclusion: DuetSVG通过多模态联合建模和视觉引导，有效提升了SVG生成任务的综合质量。

Abstract: Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.

</details>


### [89] [FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos](https://arxiv.org/abs/2512.10927)
*Yulu Gan,Ligeng Zhu,Dandan Shan,Baifeng Shi,Hongxu Yin,Boris Ivanovic,Song Han,Trevor Darrell,Jitendra Malik,Marco Pavone,Boyi Li*

Main category: cs.CV

TL;DR: 提出了FoundationMotion，一个用于自动生成大规模、细粒度动作数据集的流水线，并通过优化主流模型显著提升了动作理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的动作理解模型受限于稀缺且代价高昂的标注数据，限制了模型的扩展和理解能力，需要一种低成本、可扩展的数据生成方案。

Method: 开发了FoundationMotion自动数据生成流程，包含视频中目标检测与轨迹提取，然后利用这些轨迹和视频帧结合大语言模型（LLMs）生成细粒度动作描述和问题-答案对；随后利用这些新数据微调多个开源多模态大模型。

Result: 微调后的模型在动作理解和空间推理能力上取得了显著提升，在多个公开动作理解基准上超越了主流强闭源模型（如Gemini-2.5 Flash）和大规模开源模型（如Qwen2.5-VL-72B）。

Conclusion: FoundationMotion实现了动作数据自动化标注和扩量，为多模态模型在动作理解方向带来了可扩展与高效的训练方案，大幅提升了相关模型能力。

Abstract: Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.

</details>


### [90] [BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models](https://arxiv.org/abs/2512.10932)
*Shengao Wang,Wenqi Wang,Zecheng Wang,Max Whitton,Michael Wakeham,Arjun Chandra,Joey Huang,Pengyue Zhu,Helen Chen,David Li,Jeffrey Li,Shawn Li,Andrew Zagula,Amy Zhao,Andrew Zhu,Sayaka Nakamura,Yuki Yamamoto,Jerry Jun Yokono,Aaron Mueller,Bryan A. Plummer,Kate Saenko,Venkatesh Saligrama,Boqing Gong*

Main category: cs.CV

TL;DR: 论文提出了BabyVLM-V2，一个仿照婴幼儿发展过程进行视觉-语言预训练的新框架，通过更丰富和多样的预训练数据及评测工具，有效提升了模型性能，并在特定任务上超越了GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 当前视觉基础模型的预训练往往样本效率低且缺乏与婴幼儿认知发展的对应。作者希望通过模拟婴幼儿的学习轨迹，提高模型在视觉-语言任务上的学习效率和泛化能力。

Method: BabyVLM-V2利用一个覆盖面广、聚焦婴幼儿真实体验的音视频与对话数据集进行多模式预训练，并引入DevCV Toolbox，这是一套包含10项多模态任务的认知评测工具，全面匹配婴幼儿的空间推理、记忆和词汇等能力，同时模型结构更灵活紧凑。

Result: 实验结果显示，BabyVLM-V2在DevCV Toolbox上的表现达到甚至超过了大型基线模型如GPT-4o，尤其是在某些任务上表现突出，证明了发展学启发式预训练的有效性。

Conclusion: BabyVLM-V2为发展合理的视觉基础模型预训练提供了系统性新思路，不仅提升了样本效率和性能，还促进了认知合理性，有望推动该领域研究的发展。

Abstract: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.

</details>


### [91] [GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting](https://arxiv.org/abs/2512.10939)
*Madhav Agarwal,Mingtian Zhang,Laura Sevilla-Lara,Steven McDonagh*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯分布和三维可变形模型的新方法，实现了通过音频驱动、实时生成高质量并具时序稳定性的说话人头像。


<details>
  <summary>Details</summary>
Motivation: 现有语音驱动的说话人头像方法要么画面精细但速度慢，要么实时但时序不稳定，且高斯溅射方法存在追踪和映射不一致导致的不真实问题，难以应用于现实场景。

Method: 作者提出结合高斯溅射和3D可变形模型，通过transformer网络从音频直接预测模型参数，并用单目视频与独立音频输入，生成个性化说话人头像，提升了时序一致性与输出质量。

Result: 该方法可实时生成说话人视频头像，且在定量和定性评估中都取得了有竞争力的性能。

Conclusion: 新方法有效解决了以往方法时序不一致和画质不稳定的问题，适合实际场景的交互式应用。

Abstract: Speech-driven talking heads have recently emerged and enable interactive avatars. However, real-world applications are limited, as current methods achieve high visual fidelity but slow or fast yet temporally unstable. Diffusion methods provide realistic image generation, yet struggle with oneshot settings. Gaussian Splatting approaches are real-time, yet inaccuracies in facial tracking, or inconsistent Gaussian mappings, lead to unstable outputs and video artifacts that are detrimental to realistic use cases. We address this problem by mapping Gaussian Splatting using 3D Morphable Models to generate person-specific avatars. We introduce transformer-based prediction of model parameters, directly from audio, to drive temporal consistency. From monocular video and independent audio speech inputs, our method enables generation of real-time talking head videos where we report competitive quantitative and qualitative performance.

</details>


### [92] [OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis](https://arxiv.org/abs/2512.10940)
*Xiang Fan,Sharath Girish,Vivek Ramanujan,Chaoyang Wang,Ashkan Mirzaei,Petr Sushko,Aliaksandr Siarohin,Sergey Tulyakov,Ranjay Krishna*

Main category: cs.CV

TL;DR: OmniView提出了一个统一的摄像头控制扩散模型框架，可以同时处理多种4D一致性任务，并且在多项基准上超越了现有的专用模型。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在摄像头控制方面仅能处理特定类型的4D任务，训练数据分散，难以泛化到其他任务，需要一个通用且灵活的方法以整合不同的数据和任务。

Method: OmniView将空间、时间和视角条件分开建模，实现了输入灵活组合，能够处理静态、动态、多视图、文本生成视频、图像生成视频等任务。其架构适配多类4D一致性任务，用统一网络处理传统需多个专用模型的任务。

Result: OmniView在多个公开数据集和任务上表现优异：图像质量得分在多视图NVS LLFF上提升33%、动态NVS Neural 3D Video提升60%、静态摄像头控制RE-10K提升20%，文本条件下视频生成的摄像头轨迹误差降低4倍。

Conclusion: OmniView实现了对多种4D任务的强泛化能力，展示了通用4D视频生成模型的可行性，推进了融合多源、全能摄像头控制生成任务发展的前沿。

Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\% in multiview NVS LLFF dataset, 60\% in dynamic NVS Neural 3D Video benchmark, 20\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/

</details>


### [93] [Mull-Tokens: Modality-Agnostic Latent Thinking](https://arxiv.org/abs/2512.10941)
*Arijit Ray,Ahmed Abdelkader,Chengzhi Mao,Bryan A. Plummer,Kate Saenko,Ranjay Krishna,Leonidas Guibas,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Mull-Tokens的新型多模态中间表征方法，用于提升模型在空间推理等任务上的表现，通过可在图像和文本间自由表达推理过程的信息，实现了比现有基线更优的结果。


<details>
  <summary>Details</summary>
Motivation: 现实世界推理不仅限于语言，还涉及空间、时间和可供性等需要跨模态（如图像、文本）理解的内容。现有多模态推理模型往往依赖于专业工具、昂贵的图像生成或者人工构造推理数据，难以泛化和大规模应用。因而，亟需一种更简洁且适应性强的表征方式，助力多模态推理。

Method: 作者提出了Mull-Tokens，一种模态无关的潜在中间token，用于表达文本和图像的推理中间状态。训练流程包括两步：首先采用交错的图文推理轨迹进行有监督训练，然后利用仅有最终答案的无监督微调。该方法避开了大量特定工具和人工数据的需求。

Result: 在四个涉及空间推理的基准测试（如解谜、视角转换等）中，Mull-Tokens在平均性能上超越了文本推理和交错图文推理的多个强基线，平均提升3%，在解谜的高推理需求子任务上更是提升16%。

Conclusion: Mull-Tokens为多模态抽象推理提供了一种简单高效的解决方案，在文本和视觉推理结合方面具备良好潜力，有助于推动语言模型更好地理解和操作多模态信息。

Abstract: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.

</details>


### [94] [VL-JEPA: Joint Embedding Predictive Architecture for Vision-language](https://arxiv.org/abs/2512.10942)
*Delong Chen,Mustafa Shukor,Theo Moutakanni,Willy Chung,Jade Yu,Tejaswi Kasarla,Allen Bolourchi,Yann LeCun,Pascale Fung*

Main category: cs.CV

TL;DR: VL-JEPA是一种基于联合嵌入预测架构的新型视觉-语言模型，通过预测文本的连续嵌入向量，达到更高效且参数更少的表现，并在多个视觉、检索和问答任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型通常自回归生成文本，依赖大量参数且易受表面语言差异干扰。作者希望设计一种更高效、能更好捕捉语义且减少参数的视觉-语言模型。

Method: VL-JEPA采用联合嵌入预测架构（JEPA），在抽象的表示空间中预测目标文本的连续嵌入，而不是生成离散token。模型只在需要时通过轻量级文本解码器将嵌入转换为文本，并支持选择性解码来减少运算量。

Result: 在相同视觉编码器与训练数据的严格对比下，VL-JEPA表现优于传统token空间VLM，训练参数少50%。其选择性解码操作数减少2.85倍且性能不降。其嵌入空间天然支持分类、文本-视频检索和VQA任务，并在多个公开数据集上超越CLIP、SigLIP2等著名模型。

Conclusion: VL-JEPA以更少的参数取得与或优于传统大模型的性能，同时具备灵活的推理特性和任务适应性，为视觉-语言模型设计提供了新的高效范式。

Abstract: We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.

</details>


### [95] [AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation](https://arxiv.org/abs/2512.10943)
*Sharath Girish,Viacheslav Ivanov,Tsai-Shien Chen,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov*

Main category: cs.CV

TL;DR: 本文提出了AlcheMinT框架，实现了对多主体视频生成中主体出现/消失时序的精细化控制，并保持高画质和身份一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散大模型的主体现驱动视频生成方法难以精确控制主体在视频中的出现和消失时序，这限制了其在复杂视频合成和动画等场景中的应用。

Method: 提出AlcheMinT框架，引入了基于时间戳的条件控制，设计了新型位置编码以表达主体在特定时间区间内的活动，并结合了由文本描述强化的视觉身份绑定，通过token级拼接方式，无需新增跨注意力模块，参数开销极小。

Result: 建立了新的基准，分别从身份保持、视频保真度和时序一致性等维度验证方法有效性。多项实验表明，AlcheMinT可获得与最新个性化视频生成方法相当的画质，同时首次实现了多主体视频生成中的精确时序控制。

Conclusion: AlcheMinT在保证个性化和高质量视频生成的同时，突破性地实现在视频中多主体的时序精细控制，对多主体动态视频生成任务具有推动作用。

Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT

</details>


### [96] [MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation](https://arxiv.org/abs/2512.10945)
*Henghui Ding,Chang Liu,Shuting He,Kaining Ying,Xudong Jiang,Chen Change Loy,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出了一个新的大规模多模态数据集MeViS，用于基于运动表达的目标视频分割与跟踪，并对现有方法进行基准测试，提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 现有参考视频分割数据集主要侧重于显著性目标和静态属性描述，低估了运动表达在视频和语言中的作用。为推动基于运动表达和运动推理的像素级视频理解，有必要建设面向该任务的新数据集。

Method: 作者提出MeViS数据集，包含33,072个人工标注的运动表达（文本、音频），覆盖2,006个复杂场景下的视频和8,171个目标。基于该数据集，作者对15种主流方法在4大任务上进行基准测试，并提出用于RVOS/AVOS/RMOT的改进方法LMPM++。

Result: 基于MeViS数据集的实验显示，现有方法对运动表达引导的视频理解能力有限。LMPM++方法在多个任务上取得了新的SOTA性能。

Conclusion: MeViS为复杂场景下的运动表达引导视频理解算法研究提供了重要平台和资源，推动该领域发展。数据集与代码已公开。

Abstract: This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/

</details>


### [97] [Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving](https://arxiv.org/abs/2512.10947)
*Jiawei Yang,Ziyu Chen,Yurong You,Yan Wang,Yiming Li,Yuxiao Chen,Boyi Li,Boris Ivanovic,Marco Pavone,Yue Wang*

Main category: cs.CV

TL;DR: Flex是一种高效的场景编码器，可大幅提升自动驾驶多摄像头数据处理效率，实现更快推理和更优驾驶表现，无需依赖传统的3D先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶模型在处理海量多摄像头图片数据时面临计算瓶颈，尤其是三维先验（如BEV、占用网格或三平面表达）增加了复杂度和资源消耗。作者希望找到一种无需依赖这些3D先验、能高效压缩多视角视觉信息的新方法。

Method: 提出Flex编码器，利用一组可学习的场景token，将所有相机、各时刻的图像token联合编码，无需显式3D结构信息。该方法直接从数据学习紧凑的场景表示，并通过强压缩将视觉信息高效传递给下游基于大语言模型的决策模块。

Result: 在2万小时驾驶数据集上，Flex推理速度提升2.2倍，同时显著提升自动驾驶性能。紧凑的场景token还能自发展现场景分解能力，而不需要额外监督。

Conclusion: Flex挑战了自动驾驶场景理解必须依赖3D先验的观念，证明以数据驱动、联合编码的方式能带来更可扩展、更高效、效果更好的场景表示，推动了自动驾驶技术的发展。

Abstract: We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.

</details>


### [98] [ClusIR: Towards Cluster-Guided All-in-One Image Restoration](https://arxiv.org/abs/2512.10948)
*Shengkai Hu,Jiaqi Ma,Jun Wan,Wenwen Min,Yongcheng Jing,Lefei Zhang,Dacheng Tao*

Main category: cs.CV

TL;DR: 该论文提出了一个用于多类型图像退化恢复的统一方法ClusIR，通过集群建模与空间及频率域的信息传播实现自适应还原，取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 现有的统一图像恢复方法难以明确建模退化类型，对复杂或混合退化的适应性较差。为提升对退化的感知和有针对性的恢复，需要一种能适应多种退化的新方法。

Method: 提出了ClusIR框架，包含概率集群引导路由机制（PCGRM）和退化感知频率调制模块（DAFMM）。PCGRM显式区分退化识别和专家激活，提升对不同退化的辨识和稳定响应。DAFMM利用集群先验对频域特征进行自适应分解和有针对性的调制，实现结构与纹理信息的高质量恢复。

Result: 在多个基准测试上，ClusIR在多种退化场景下恢复效果优异，达到领先水平。

Conclusion: 通过集群建模和频域调制的协同，ClusIR能够更有效感知和处理复杂退化，实现高保真图像还原。

Abstract: All-in-One Image Restoration (AiOIR) aims to recover high-quality images from diverse degradations within a unified framework. However, existing methods often fail to explicitly model degradation types and struggle to adapt their restoration behavior to complex or mixed degradations. To address these issues, we propose ClusIR, a Cluster-Guided Image Restoration framework that explicitly models degradation semantics through learnable clustering and propagates cluster-aware cues across spatial and frequency domains for adaptive restoration. Specifically, ClusIR comprises two key components: a Probabilistic Cluster-Guided Routing Mechanism (PCGRM) and a Degradation-Aware Frequency Modulation Module (DAFMM). The proposed PCGRM disentangles degradation recognition from expert activation, enabling discriminative degradation perception and stable expert routing. Meanwhile, DAFMM leverages the cluster-guided priors to perform adaptive frequency decomposition and targeted modulation, collaboratively refining structural and textural representations for higher restoration fidelity. The cluster-guided synergy seamlessly bridges semantic cues with frequency-domain modulation, empowering ClusIR to attain remarkable restoration results across a wide range of degradations. Extensive experiments on diverse benchmarks validate that ClusIR reaches competitive performance under several scenarios.

</details>


### [99] [E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training](https://arxiv.org/abs/2512.10950)
*Qitao Zhao,Hao Tan,Qianqian Wang,Sai Bi,Kai Zhang,Kalyan Sunkavalli,Shubham Tulsiani,Hanwen Jiang*

Main category: cs.CV

TL;DR: E-RayZer是一种新的自监督大规模3D视觉模型，能直接从无标签多视图图像中学习真实的3D信息，并在3D下游任务中优于现有方法，成为3D感知预训练的新范式。


<details>
  <summary>Details</summary>
Motivation: 当前自监督预训练在语言、2D图像和视频取得巨大成功，但针对3D感知的自监督多视图学习仍很欠缺，现有方法如RayZer只能间接推断3D信息，缺乏显式几何基础。

Method: E-RayZer直接在3D空间进行自监督显式几何重建，不依赖标签，设计了细粒度学习课程，从易到难无监督协调异质数据，避免了快捷路径导致的伪3D表征，模型可大规模收敛。

Result: E-RayZer在姿态估计等3D任务显著优于RayZer，并达到或超越全监督模型VGGT的表现，3D表征迁移至下游任务时超越了DINOv3、CroCo v2、VideoMAE V2等主流视觉预训练模型。

Conclusion: E-RayZer开创了3D感知视觉自监督预训练的新范式，从多视图无标签图像中直接获取真实3D能力，为3D视觉任务提供更优基础模型。

Abstract: Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.

</details>


### [100] [Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration](https://arxiv.org/abs/2512.10954)
*Sicheng Mo,Thao Nguyen,Richard Zhang,Nick Kolkin,Siddharth Srinivasan Iyer,Eli Shechtman,Krishna Kumar Singh,Yong Jae Lee,Bolei Zhou,Yuheng Li*

Main category: cs.CV

TL;DR: 该论文提出了Group Diffusion方法，在扩散模型推理阶段打破了图像间的独立性，实现了跨图像的协作生成，提高了生成质量。


<details>
  <summary>Details</summary>
Motivation: 以往扩散模型在推理时，每张图片是独立生成的，忽略了可能存在的跨图片相关性。论文动机是探索能否通过协作生成，提高生成模型性能。

Method: 提出了Group Diffusion，将注意力机制扩展到跨多张图片，实现图片联合去噪。实验引入新的定性度量并分析了跨样本注意力的影响。

Result: 随着协同生成的图片数量增加，跨样本注意力增强，图像生成质量提高，FID分数最高提升32.2%。实验还发现新度量与FID高度相关。

Conclusion: 论文首次揭示了跨样本推理机制在生成建模中的潜力，为扩散模型的推理方式开拓了新方向。

Abstract: In this work, we explore an untapped signal in diffusion model inference. While all previous methods generate images independently at inference, we instead ask if samples can be generated collaboratively. We propose Group Diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image. This enables images to be jointly denoised at inference time, learning both intra and inter-image correspondence. We observe a clear scaling effect - larger group sizes yield stronger cross-sample attention and better generation quality. Furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with FID. Built on standard diffusion transformers, our GroupDiff achieves up to 32.2% FID improvement on ImageNet-256x256. Our work reveals cross-sample inference as an effective, previously unexplored mechanism for generative modeling.

</details>


### [101] [Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization](https://arxiv.org/abs/2512.10955)
*Tsai-Shien Chen,Aliaksandr Siarohin,Guocheng Gordon Qian,Kuan-Chieh Jackson Wang,Egor Nemchinov,Moayed Haji-Ali,Riza Alp Guler,Willi Menapace,Ivan Skorokhodov,Anil Kag,Jun-Yan Zhu,Sergey Tulyakov*

Main category: cs.CV

TL;DR: 本文提出了Omni-Attribute，这是首个面向开放词汇图像属性编码器，可学习高保真、专属性属性表征，解决了现有方法难以单独隔离图像特定属性的问题，并在多项基准任务上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉概念个性化方法无法有效隔离单一图像属性，常常导致信息泄露与图像合成不连贯。其根本原因是在于现有方法依赖于全局图像编码器，难以精准分辨多个视觉因子。

Method: 方法包括两方面创新：(1) 精心设计具有语义关联、带有正负属性标注的图像对，以明确教会编码器要保留或抑制何种属性；(2) 采用双目标训练范式，即兼顾生成保真度和对比性解耦，有效获得可泛化的属性特征嵌入。

Result: 所得嵌入在开放词汇属性检索、个性化和多属性合成等任务中表现优异，并在多项基准数据集上取得了最先进的性能。

Conclusion: Omni-Attribute编码器通过创新的数据与模型设计，成功实现了高保真的属性表达与解耦，为视觉概念个性化任务带来显著进步，提升了相关生成任务的效果和多样性。

Abstract: Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.

</details>


### [102] [Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision](https://arxiv.org/abs/2512.10956)
*Wentao Zhou,Xuweiyi Chen,Vignesh Rajagopal,Jeffrey Chen,Rohan Chandra,Zezhou Cheng*

Main category: cs.CV

TL;DR: 本文提出并验证了利用双目视觉和中层视觉模块（如深度估计、像素跟踪）增强机器人导航基础模型（NFM）的有效性，显著提升了导航性能并大幅减少了训练数据需求。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人导航基础模型（NFM）直接从单目视觉映射到控制动作，忽略了中层视觉模块，假设能力会自发涌现。但这种方法依赖大量像素到动作的监督数据，在动态且无结构环境下尤其难以获取，并且单目视觉的深度—尺度模糊限制了空间推理。作者试图解决这些关键限制。

Method: 作者提出了StereoWalker模型，将双目视觉输入及深度估计、像素跟踪等中层视觉信息结合到NFM中。还构建了一个自动标注的大规模双目导航数据集，支持StereoWalker训练并促进相关研究。

Result: 实验表明，引入中层视觉使StereoWalker在仅用1.5%训练数据时即可达到SOTA性能，用全数据时超越SOTA。同时，双目视觉导航性能优于单目输入。

Conclusion: 完全依赖单目视觉和忽视中层视觉先验的方法效率低下。结合双目视觉与中层视觉能够精准捕获几何和动态结构，极大提升机器人导航能力，并大幅降低训练成本。

Abstract: The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.
  We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.

</details>


### [103] [SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model](https://arxiv.org/abs/2512.10957)
*Yukai Shi,Weiyu Li,Zihao Wang,Hongyang Li,Xingyu Chen,Ping Tan,Lei Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一个名为SceneMaker的去耦合3D场景生成框架，通过单独建模遮挡消除（de-occlusion）和三维物体生成两个模块，有效提升了场景在遮挡和开放集环境下的几何质量和姿态估计准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景生成方法在严重遮挡和开放集（即未见过的新物体或遮挡类型）场景下，难以兼顾高质量几何和准确的姿态估计，主要原因是缺乏开放集遮挡消除和姿态估计的先验。

Method: 1）将遮挡消除模型与3D物体生成解耦开来，并通过融合图像数据集和自采集的遮挡数据集来提升遮挡模式的多样性和泛化能力；2）提出了一个融合全局和局部机制（包括自注意力和交互注意力）的统一姿态估计模型，以提升其准确率；3）构建了开放集3D场景数据集，扩展了姿态估计模型的泛化能力。

Result: 大量实验显示，所提出的去耦合框架在室内和开放集场景下都优于现有方法，无论是在几何质量还是姿态估计准确性方面都有明显提升。

Conclusion: 去耦合的场景生成方法能更好地适应开放集和复杂遮挡环境，对3D场景理解和生成任务具有重要推动作用。相关代码与数据已向社区开放。

Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.

</details>


### [104] [WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World](https://arxiv.org/abs/2512.10958)
*Ao Liang,Lingdong Kong,Tianyi Yan,Hongsi Liu,Wesley Yang,Ziqi Huang,Wei Yin,Jialong Zuo,Yixuan Hu,Dekai Zhu,Dongyue Lu,Youquan Liu,Guangfeng Jiang,Linfeng Li,Xiangtai Li,Long Zhuo,Lai Xing Ng,Benoit R. Cottereau,Changxin Gao,Liang Pan,Wei Tsang Ooi,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出WorldLens基准体系，从生成世界的多个维度系统评价生成式世界模型的真实性和功能可靠性，并提供配套数据集与自动化评测代理，填补了此领域评估方法的空白。


<details>
  <summary>Details</summary>
Motivation: 当前生成式世界模型在外观上逼真，但常在物理和行为层面失真，且缺乏统一、多维度的评估标准，阻碍了模型实用性的提升与比较。作者希望建立系统化评测体系，推动领域健康发展。

Method: 提出WorldLens基准，涵盖生成、重建、动作跟随、下游任务和人类偏好五个方面，全面评测模型在视觉、几何、物理和功能上的表现。构建了包含2.6万个人工注释视频的数据集（WorldLens-26K），并开发WorldLens-Agent模型，将人工评分蒸馏为可扩展、可解释的自动评价器，从而形成完整评估生态。

Result: 实验表明，目前没有单一世界模型能在所有维度都表现优异。有的模型视觉写实但物理失真，有的几何一致但缺乏行为可靠性。WorldLens基准能有效反映这些差异，为模型评测提供细粒度参照。

Conclusion: WorldLens为生成式世界模型领域提供了标准化、全面的评测体系，对比和提升未来模型性能有重要意义，促进模型既真实外观又真实行为的协同进步。

Abstract: Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.

</details>


### [105] [StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space](https://arxiv.org/abs/2512.10959)
*Tjark Behrens,Anton Obukhov,Bingxin Ke,Fabio Tosi,Matteo Poggi,Konrad Schindler*

Main category: cs.CV

TL;DR: StereoSpace提出了一种基于扩散模型的单目到立体图像合成框架，通过视角条件建模几何信息，无需显式深度或变形操作，实现高质量立体图像生成。


<details>
  <summary>Details</summary>
Motivation: 传统单目到立体图像生成方法常依赖深度估计和图像变形，存在推断不准确且通用性有限的问题。作者希望提出一种不依赖深度或显式几何处理，具备更强泛化能力和生成质量的解决方案。

Method: StereoSpace采用扩散模型，通过引入标准化的校正立体空间和视角条件，指导生成器学习端到端的图像对应关系和遮挡补全，不涉及任何深度图或几何代理。在评测时，提出了排除任何显式（或代理）几何信息的公平端到端评估协议，并采用iSQoE和MEt3R度量生成结果的感知舒适度和几何一致性。

Result: StereoSpace在分层与非朗伯场景下，显著优于基于变形补全、潜变量变形以及变形条件等现有方法，在视差锐利度和鲁棒性方面均有提升。

Conclusion: 基于视角条件的扩散生成成为具备扩展性、无需深度信息的立体图像生成新范式，StereoSpace在实际表现和评测协议上都树立了新基准。

Abstract: We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [106] [What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models](https://arxiv.org/abs/2512.10080)
*Luciano Floridi,Jessica Morley,Claudio Novelli,David Watson*

Main category: cs.CL

TL;DR: 本文分析了现有大语言模型（LLMs）在推理能力方面的表现，认为其生成推理内容主要依靠对学习到的文本模式的仿效，并不具备真正的溯因推理能力，强调了对其输出需保持批判性评估。


<details>
  <summary>Details</summary>
Motivation: 当下大语言模型在各类任务中展现出类似推理的能力，表面上似乎具备人类的溯因（abductive）推理，本研究动机在于揭示这些模型推理能力的真实本质及其局限。

Method: 通过分析LLMs基于token补全方式生成文本的机理，结合案例展示LLMs为何能“模拟”推理但不具备实际推理能力，并详述其与人类溯因推理的异同。

Result: 研究发现，LLMs输出的“被推理化”文本，其根本仍是对数据中人类推理模式的模仿，而非模型自身理解或验证结果；其可以给出貌似合理但未经事实基础验证的解释性答案。

Conclusion: LLMs在推理任务上具备表面能力，可辅助人类思考和创意生成，但缺失真理甄别和实际验证能力，因此其输出需高度警惕并批判性对待。作者还回应了五项常见反驳意见，并提出了自身分析的局限。

Abstract: This article looks at how reasoning works in current Large Language Models (LLMs) that function using the token-completion method. It examines their stochastic nature and their similarity to human abductive reasoning. The argument is that these LLMs create text based on learned patterns rather than performing actual abductive reasoning. When their output seems abductive, this is largely because they are trained on human-generated texts that include reasoning structures. Examples are used to show how LLMs can produce plausible ideas, mimic commonsense reasoning, and give explanatory answers without being grounded in truth, semantics, verification, or understanding, and without performing any real abductive reasoning. This dual nature, where the models have a stochastic base but appear abductive in use, has important consequences for how LLMs are evaluated and applied. They can assist with generating ideas and supporting human thinking, but their outputs must be critically assessed because they cannot identify truth or verify their explanations. The article concludes by addressing five objections to these points, noting some limitations in the analysis, and offering an overall evaluation.

</details>


### [107] [Generate-Then-Validate: A Novel Question Generation Approach Using Small Language Models](https://arxiv.org/abs/2512.10110)
*Yumou Wei,John Stamper,Paulo F. Carvalho*

Main category: cs.CL

TL;DR: 该论文提出了一种利用小型语言模型（SLM）进行自动问题生成的新方法，以补充大型语言模型在学习分析中的主流应用。通过“生成-验证”策略，结合生成与概率推理能力，生成高质量问题。结果显示，SLM在合理流程指导下，能够有效生成优质问题。


<details>
  <summary>Details</summary>
Motivation: 目前学习分析领域普遍采用大型语言模型（LLM）进行自动问题生成，但小型语言模型由于资源消耗少等优势值得进一步研究。作者希望评估并提升SLM在此任务中的性能，探索其作为LLM补充的潜力。

Method: 论文设计了一套新颖的问题生成流程，首先由SLM大规模生成候选问题，随后通过概率推理筛选并验证高质量问题。最终，邀请人类专家和LLM分别对生成的问题进行评估。

Result: 大多数评审者（包括人类和LLM）认为，SLM生成的问题答案明确且契合学习目标。SLM配合新流程后生成的问题质量得到了验证。

Conclusion: 经过精心设计的流程引导，SLM能够生成高质量、具备实际应用价值的问题，因此可以成为LLM之外值得考虑的工具，尤其在资源受限场景下。

Abstract: We explore the use of small language models (SLMs) for automatic question generation as a complement to the prevalent use of their large counterparts in learning analytics research. We present a novel question generation pipeline that leverages both the text generation and the probabilistic reasoning abilities of SLMs to generate high-quality questions. Adopting a "generate-then-validate" strategy, our pipeline first performs expansive generation to create an abundance of candidate questions and refine them through selective validation based on novel probabilistic reasoning. We conducted two evaluation studies, one with seven human experts and the other with a large language model (LLM), to assess the quality of the generated questions. Most judges (humans or LLMs) agreed that the generated questions had clear answers and generally aligned well with the intended learning objectives. Our findings suggest that an SLM can effectively generate high-quality questions when guided by a well-designed pipeline that leverages its strengths.

</details>


### [108] [Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing](https://arxiv.org/abs/2512.10121)
*Zhongjie Jiang*

Main category: cs.CL

TL;DR: 本文提出了DeepNews框架，通过模拟资深财经记者的认知流程，显著提升垂直领域长文本生成的真实性、逻辑性和个性化表达，有效突破大模型生成的“不可能三位一体”瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在长文本生成中很难兼顾低幻觉、深层逻辑和个性表达三要素，且现有范式过度平滑，缺乏专家写作所需的高熵信息获取与结构化认知过程。

Method: 提出DeepNews框架，包括三大核心模块：1）基于信息觅食理论的双粒度检索机制，提升信息输入饱和度；2）依托领域专家知识与叙事结构的策略规划；3）通过对抗约束提示扰动生成文本，突破概率性平滑。

Result: 在财经报道实验中，发现输入信息字符数低于15000，文本真实性急剧下滑；输入冗余高于30000，去幻觉率可稳定在85%以上。在主流媒体实测中，DeepNews系统的投稿通过率达25%，大幅领先GPT-5零样本生成的0%。

Conclusion: DeepNews框架能有效突破传统大模型长文本生成的核心瓶颈，为垂直行业生成式任务提供了结合专家认知的可行路径。

Abstract: Central to long-form text generation in vertical domains is the "impossible trinity" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study establishes that this bottleneck arises from existing generative paradigms succumbing to the Statistical Smoothing Trap, a phenomenon that overlooks the high-entropy information acquisition and structured cognitive processes integral to expert-level writing. To address this limitation, we propose the DeepNews Framework, an agentic workflow that explicitly models the implicit cognitive processes of seasoned financial journalists. The framework integrates three core modules: first, a dual-granularity retrieval mechanism grounded in information foraging theory, which enforces a 10:1 saturated information input ratio to mitigate hallucinatory outputs; second, schema-guided strategic planning, a process leveraging domain expert knowledge bases (narrative schemas) and Atomic Blocks to forge a robust logical skeleton; third, adversarial constraint prompting, a technique deploying tactics including Rhythm Break and Logic Fog to disrupt the probabilistic smoothness inherent in model-generated text. Experiments delineate a salient Knowledge Cliff in deep financial reporting: content truthfulness collapses when retrieved context falls below 15,000 characters, while a high-redundancy input exceeding 30,000 characters stabilizes the Hallucination-Free Rate (HFR) above 85%. In an ecological validity blind test conducted with a top-tier Chinese technology media outlet, the DeepNews system--built on a previous-generation model (DeepSeek-V3-0324)-achieved a 25% submission acceptance rate, significantly outperforming the 0% acceptance rate of zero-shot generation by a state-of-the-art (SOTA) model (GPT-5).

</details>


### [109] [PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset](https://arxiv.org/abs/2512.10148)
*Moonsoo Park,Jeongseok Yun,Bohyung Kim*

Main category: cs.CL

TL;DR: 本文提出了一种通过分析用户评论自动生成个性化回复的方法，无需额外的用户信息，提升回复的相关性和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前在如外卖平台等用户信息有限的领域，现有大型语言模型生成的回复往往流于通用，缺乏个性化，导致用户参与度和响应效果下降。

Method: 提出了两阶段提示框架：首先从短评中推断显性(如用户偏好)和隐性(如人口统计、风格线索)画像，其次将这些画像属性融入回复生成的提示中。同时通过调整解码温度鼓励生成多样且忠实的回复。

Result: 在韩国某外卖平台真实数据集上进行了评估，并从准确性、多样性和语义一致性三方面考察了方法的表现。

Conclusion: 结果证明，基于画像增强的提示能在无需模型微调的情况下，有效提升自动回复的相关性和个性化。

Abstract: Personalized review response generation presents a significant challenge in domains where user information is limited, such as food delivery platforms. While large language models (LLMs) offer powerful text generation capabilities, they often produce generic responses when lacking contextual user data, reducing engagement and effectiveness. In this work, we propose a two-stage prompting framework that infers both explicit (e.g., user-stated preferences) and implicit (e.g., demographic or stylistic cues) personas directly from short review texts. These inferred persona attributes are then incorporated into the response generation prompt to produce user-tailored replies. To encourage diverse yet faithful generations, we adjust decoding temperature during inference. We evaluate our method using a real-world dataset collected from a Korean food delivery app, and assess its impact on precision, diversity, and semantic consistency. Our findings highlight the effectiveness of persona-augmented prompting in enhancing the relevance and personalization of automated responses without requiring model fine-tuning.

</details>


### [110] [Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning](https://arxiv.org/abs/2512.10150)
*Lama Alssum,Hani Itani,Hasan Abed Al Kader Hammoud,Philip Torr,Adel Bibi,Bernard Ghanem*

Main category: cs.CL

TL;DR: 本文研究了将大语言模型（LLMs）微调到新任务时，由于灾难性遗忘引发的安全性下降问题，并系统性评估了多种持续学习（CL）方法在缓解此问题上的效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用普及，安全对于模型微调后的表现愈发重要。微调常导致模型遗忘先前学到的安全性知识，降低整体安全性。因此，如何在模型定制过程中最大程度保留原有安全性成为亟需解决的问题。

Method: 作者将模型微调过程视为持续学习问题，引入了文献中的正则化、记忆增强和模型融合等CL技术进行适配，系统性评估其在面对良性与带毒（攻击）用户数据时的表现。

Result: 通过与标准微调和当前安全保留基线方法比较，CL方法在抑制攻击成功率方面更有效，特别是DER方法在保证下游任务表现的同时安全性表现最佳。这一结果在多个任务和模型家族间具有一致性。

Conclusion: 持续学习技术可有效减缓大语言模型微调过程中的安全性退化，为模型定制服务提供了实用且通用的安全解决方案。

Abstract: The safety alignment of large language models (LLMs) is becoming increasingly important with their democratization. In this paper, we study the safety degradation that comes with adapting LLMs to new tasks. We attribute this safety compromise to catastrophic forgetting and frame the problem of preserving safety when fine-tuning as a continual learning (CL) problem. We consider the fine-tuning-as-a-service setup where the user uploads their data to a service provider to get a customized model that excels on the user's selected task. We adapt several CL approaches from the literature and systematically evaluate their ability to mitigate safety degradation. These include regularization-based, memory-based, and model merging approaches. We consider two scenarios, (1) benign user data and (2) poisoned user data. Our results demonstrate that CL approaches consistently achieve lower attack success rates than standard fine-tuning. Among these, DER outperforms both other CL methods and existing safety-preserving baselines while maintaining task utility. These findings generalize across three downstream tasks (GSM8K, SST2, Code) and three model families (LLaMA2-7B, Mistral-7B, Gemma-2B), establishing CL as a practical solution to preserve safety.

</details>


### [111] [AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding](https://arxiv.org/abs/2512.10195)
*Gyutaek Oh,Sangjoon Park,Byung-Hoon Kim*

Main category: cs.CL

TL;DR: 作者提出了AutoMedic框架，实现能够自动化评估大模型在多轮医疗对话场景下的表现，解决了当前静态问答评测的局限。


<details>
  <summary>Details</summary>
Motivation: 虽然当前有多种医疗问答数据集用于大模型评估，但这些数据集多为静态问答，缺乏对多轮、动态医疗对话场景的系统性评估方法。因此，急需新的方法准确、全面地评估大模型作为临床对话助手的实际能力。

Method: 作者提出了AutoMedic框架，通过多智能体模拟，将现成的静态问答数据集转化为虚拟患者档案，驱动大模型代理间真实且临床相关的多轮对话，并利用自主设计的CARE指标从多维度评价（准确性、效率/策略、共情、健壮性）。

Result: 实验显示，使用AutoMedic进行的多轮对话评估结果与人工专家的评价高度一致，验证了该方法的有效性和实用价值。

Conclusion: AutoMedic能够有效、自动化地评估临床对话大模型，为推动大模型在医疗对话场景下的开发与应用提供了实用参考和指导。

Abstract: Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.

</details>


### [112] [Multilingual VLM Training: Adapting an English-Trained VLM to French](https://arxiv.org/abs/2512.10336)
*Jules Lahmi,Alexis Roger*

Main category: cs.CL

TL;DR: 本文分析了将仅以英语训练的视觉-语言模型（VLM）适应到其他语言时的挑战，探讨了几种主流方法的表现和计算成本，发现数据翻译质量是提升多语言VLM性能的最大瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前主流VLM模型主要以英文数据训练，对非英语用户支持不足，因此亟需探讨如何高效地将这些模型跨语种迁移。

Method: 论文比较了三种适应方法：基于翻译的数据流程、LoRA微调和分阶段（视觉与语言分别适应）微调。评估时综合使用了目标语种的标准多模态基准（经翻译）以及母语专家人工评测。

Result: 实验发现，虽然上述方法各有优劣，但数据集翻译的质量问题严重影响了模型的训练和评测效果，因此成为主要瓶颈。

Conclusion: 未来改进方向应侧重于收集原生语料的多模态数据集和提升数据翻译方法，以更好支持多语言的VLM开发与应用。

Abstract: Artificial intelligence has made great progress in recent years, particularly in the development of Vision--Language Models (VLMs) that understand both visual and textual data. However, these advancements remain largely limited to English, reducing their accessibility for non--English speakers. It is essential to extend these capabilities to a broader range of languages. This paper explores the challenges of adapting an English-trained VLM to different languages. To this end, we will explore and compare different methods for their performance and computational cost. We consider a translation-based pipeline, LoRA finetuning, and a two-stage finetuning strategy that separates vision adaptation from language adaptation. To evaluate these methods, we use a combination of standard multimodal benchmarks translated into the target language and manual assessments by native experts. The results reveal that dataset translation remains a major bottleneck in multilingual VLM performance, with data quality limiting the effectiveness of training and evaluation. These findings suggest that future efforts should focus on native-language dataset collection and improved translation strategies.

</details>


### [113] [Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale](https://arxiv.org/abs/2512.10398)
*Zhaodong Wang,Zhenting Qi,Sherman Wong,Nathan Hu,Samuel Lin,Jun Ge,Erwin Gao,Yining Yang,Ben Maurer,Wenlin Chen,David Recordon,Yilun Du,Minlan Yu,Ying Zhang*

Main category: cs.CL

TL;DR: 本文提出了开源的Confucius Code Agent (CCA) 和配套的Confucius SDK，支持大规模、可扩展、可控的AI代码工程应用，并在业内数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源代码智能体虽透明但难以胜任工业级大规模场景，而专有方案则在可用性和性能上虽好却缺乏可扩展性、可解释性、可控性。因此有必要开发一种既透明又具备工业级能力的开源AI编程智能体。

Method: CCA依托Confucius SDK实现。SDK从Agent体验、用户体验和开发者体验三个维度设计，引入统一编排器与分层工作记忆，支持长上下文推理和持久化笔记，实现横跨会话的学习，并配备模块化工具链。另有元智能体自动化合成、评估和优化配置，加速任务适应与迭代。

Result: 在实际软件工程任务上，CCA展现了强大能力。在SWE-Bench-Pro基准上，CCA的Resolve@1 性能达到54.3%，显著高于以往同类开源智能体。

Conclusion: Confucius SDK和CCA不仅使AI智能体开发更加透明、可复现且便于扩展，还为工业级AI软件工程落地提供了强大基础，有效弥补了学术原型与产业系统之间的鸿沟。

Abstract: Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.

</details>


### [114] [Sliding Window Attention Adaptation](https://arxiv.org/abs/2512.10411)
*Yijiong Yu,Jiale Liu,Qingyun Wu,Huazheng Wang,Ji Pei*

Main category: cs.CL

TL;DR: 本文提出了一套适配滑动窗口注意力（SWA）的实用方法，使得经过全注意力（FA）预训练的大语言模型（LLM）能够更好地在无需重新预训练的情况下适应SWA，从而实现高效的长文本推理，并有效恢复原始性能。


<details>
  <summary>Details</summary>
Motivation: SWA可以将Transformer大语言模型在长文本推理中的推理复杂度从二次降低到线性，但简单地将FA预训练模型直接切换到SWA会导致性能显著下降，这是由于训练与推理方式不一致。因此，作者探索如何在不重新预训练的情况下，高效地将FA模型适配到SWA，并尽可能保留模型的原有长文本处理能力。

Method: 作者提出了滑动窗口注意力适配（SWAA）的方法，结合了五种策略：1）仅在prefilling阶段使用SWA；2）保留“sink” tokens；3）交错使用FA与SWA层；4）结合思维链（CoT）策略；5）基于任务的微调。实验分析了多种策略单独及组合的适配效果及性能-效率权衡，给出具体适配配方建议。

Result: 实验显示，单一方法无法完全恢复长文本处理能力，但若合理组合上述方法，则可以在不同场景下有效地提升适配性能，接近原有的全注意力表现。同时，分析了不同配置的效率及性能平衡，为实际应用提供了参考。

Conclusion: 本文证明了FA预训练大语言模型能够通过适当的适配方法高效过渡到SWA而无须完全重新预训练。多个适配技巧的组合可以显著恢复长文本推理效率与性能，推动SWA在实际大模型推理中的应用。

Abstract: The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving "sink" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation

</details>


### [115] [Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers](https://arxiv.org/abs/2512.10422)
*Youmin Ko,Sungjong Seo,Hyunjoon Kim*

Main category: cs.CL

TL;DR: 本文提出了CoopRAG，一种Retriever和大模型协作的RAG新框架，通过更精确地检索和重建推理链，有效提升了多跳和简单问答任务的效果，实验表明其在多个数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法在处理复杂和简单问答任务时，依然存在错误检索和幻觉现象，单纯依赖大模型难以获得稳定的事实输出，因此需要更精细的检索和推理协作机制。

Method: CoopRAG框架下，检索器与大模型互相传递信息：先将问题拆分为子问题和带掩码的不确定推理链，接着用这些增强内容进行相关文档检索，再通过检索器内不同层级对结果重排序，最后由大模型填补推理链中的未知部分，完整输出答案。

Result: 在三个多跳问答数据集和一个简单问答数据集上，CoopRAG在检索准确率和问答表现两方面都优于当前最优基线。

Conclusion: CoopRAG展示了检索器与大模型深度协作在提升复杂和简单问答准确性方面的显著潜力，为RAG框架的进一步发展提供了实践落地和参考。

Abstract: Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\footnote{https://github.com/meaningful96/CoopRAG}

</details>


### [116] [T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground](https://arxiv.org/abs/2512.10430)
*Dmitrii Stoianov,Danil Taranets,Olga Tsymboi,Ramil Latypov,Almaz Dautov,Vladislav Kruglikov,Nikita Surkov,German Abramov,Pavel Gein,Dmitry Abulkhanov,Mikhail Gashkov,Viktor Zelenkovskiy,Artem Batalov,Aleksandr Medvedev,Anatolii Potapov*

Main category: cs.CL

TL;DR: 本文介绍了T-pro 2.0，一个面向俄语的开源混合推理大语言模型，具有高效推理能力，并开放了训练权重及相关工具。


<details>
  <summary>Details</summary>
Motivation: 俄语大模型及其高效推理资源稀缺，难以实现本地化和多样化的研究及应用，亟需低门槛、可扩展的开源解决方案。

Method: T-pro 2.0采用针对西里尔字符优化的分词器，结合自适应EAGLE推测性解码来降低推理延迟，同时支持直接答案输出与推理过程追踪。配套公开了多样化训练数据集和评测基准。

Result: T-pro 2.0实现了快速推理与推理过程可追溯，测试表明在多个领域可显著提升推理速度。开放资源极大促进了俄语大模型领域的可重复、可扩展研究。

Conclusion: T-pro 2.0为构建和评测高效、实用的俄语大模型应用提供了易用的开放平台，推动俄语AI生态的发展。

Abstract: We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.

</details>


### [117] [Semantic Reconstruction of Adversarial Plagiarism: A Context-Aware Framework for Detecting and Restoring "Tortured Phrases" in Scientific Literature](https://arxiv.org/abs/2512.10435)
*Agniva Maiti,Prajwal Panth,Suresh Chandra Satapathy*

Main category: cs.CL

TL;DR: 该论文针对通过自动同义改写工具掩盖抄袭行为而生成的“拗口表达”进行识别和还原，提出了一种新方法，在检测异常表达同时能够恢复原始术语，有效提升了识别与追溯能力。


<details>
  <summary>Details</summary>
Motivation: 现有识别抄袭的技术对新型同义伪装手段存在高漏检，且无法溯源原始内容，这造成科学文献的完整性和可靠性受到威胁，因此亟需更强有力的检测与还原方法。

Method: 提出SRAP框架，包括两阶段：一是利用领域特定语言模型（如SciBERT）结合伪困惑度检测文本异常，二是通过向量召回（FAISS）和句子对齐（SBERT）进行术语还原和溯源。

Result: 实验表明，零样本检测基线完全失效（恢复准确率为0.00%），而该方法在对抗性科学文本的平行数据集上能达到23.67%的恢复准确率，明显优于其他方法。同时，静态分界线更适用于行业术语多变的科学文本。

Conclusion: SRAP方法不仅显著提升了对文献抄袭伪装的检测和术语还原能力，还可进行取证分析，有助于维护科学文献的可靠性和完整性。

Abstract: The integrity and reliability of scientific literature is facing a serious threat by adversarial text generation techniques, specifically from the use of automated paraphrasing tools to mask plagiarism. These tools generate "tortured phrases", statistically improbable synonyms (e.g. "counterfeit consciousness" for "artificial intelligence"), that preserve the local grammar while obscuring the original source. Most existing detection methods depend heavily on static blocklists or general-domain language models, which suffer from high false-negative rates for novel obfuscations and cannot determine the source of the plagiarized content. In this paper, we propose Semantic Reconstruction of Adversarial Plagiarism (SRAP), a framework designed not only to detect these anomalies but to mathematically recover the original terminology. We use a two-stage architecture: (1) statistical anomaly detection with a domain-specific masked language model (SciBERT) using token-level pseudo-perplexity, and (2) source-based semantic reconstruction using dense vector retrieval (FAISS) and sentence-level alignment (SBERT). Experiments on a parallel corpus of adversarial scientific text show that while zero-shot baselines fail completely (0.00 percent restoration accuracy), our retrieval-augmented approach achieves 23.67 percent restoration accuracy, significantly outperforming baseline methods. We also show that static decision boundaries are necessary for robust detection in jargon-heavy scientific text, since dynamic thresholding fails under high variance. SRAP enables forensic analysis by linking obfuscated expressions back to their most probable source documents.

</details>


### [118] [Enhancing Next-Generation Language Models with Knowledge Graphs: Extending Claude, Mistral IA, and GPT-4 via KG-BERT](https://arxiv.org/abs/2512.10440)
*Nour El Houda Ben Chaabene,Hamza Hammami*

Main category: cs.CL

TL;DR: 本论文提出将知识图谱（KGs）整合到大语言模型（LLMs）中，以提升其事实准确性和推理能力，实验显示效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型虽然擅长自然语言处理（NLP）任务，但缺乏结构化知识，导致事实不一致的问题。

Method: 通过借助KG-BERT将知识图谱与LLMs相结合，从而在语义理解和推理中引入结构化知识。

Result: 在知识密集型任务如问答和实体链接上，经过实验验证，模型在准确性和推理能力上有显著提升。

Conclusion: 该方法不仅提升了事实可靠性，还为更具上下文感知能力的新一代LLMs提供了技术路径。

Abstract: Large language models (LLMs) like Claude, Mistral IA, and GPT-4 excel in NLP but lack structured knowledge, leading to factual inconsistencies. We address this by integrating Knowledge Graphs (KGs) via KG-BERT to enhance grounding and reasoning. Experiments show significant gains in knowledge-intensive tasks such as question answering and entity linking. This approach improves factual reliability and enables more context-aware next-generation LLMs.

</details>


### [119] [Decoding Student Minds: Leveraging Conversational Agents for Psychological and Learning Analysis](https://arxiv.org/abs/2512.10441)
*Nour El Houda Ben Chaabene,Hamza Hammami,Laid Kahloul*

Main category: cs.CL

TL;DR: 本文提出了一种关注心理状态的对话型智能体，旨在提升学习表现和情感健康，通过多模态数据实时识别学生的认知和情感状态，最终在大学生中试点显示出改善动机、减轻压力和一定学业提升。


<details>
  <summary>Details</summary>
Motivation: 现有教育类聊天机器人多仅关注学业辅导或情绪支持，难以同时满足学生认知与情感支持的需求，因此需要一种能够动态感知并适应学生情绪、认知状态的智能体。

Method: 系统结合了大语言模型（LLM）、知识图谱增强的BERT（KG-BERT）、带注意力的双向LSTM，通过融合文本语义、语音韵律特征和时序行为趋势，实时识别学生的参与度、压力和理解程度。

Result: 在大学生群体中进行的试点实验表明，和基线方法相比，该系统能显著提升学生学习动机、降低压力，并带来一定的学业成绩提升。

Conclusion: 集成语义推理、多模态融合和时序建模技术，可以有效促进适应性强、以学生为中心的教育干预，提升学生整体学习体验和福祉。

Abstract: This paper presents a psychologically-aware conversational agent designed to enhance both learning performance and emotional well-being in educational settings. The system combines Large Language Models (LLMs), a knowledge graph-enhanced BERT (KG-BERT), and a bidirectional Long Short-Term Memory (LSTM) with attention to classify students' cognitive and affective states in real time. Unlike prior chatbots limited to either tutoring or affective support, our approach leverages multimodal data-including textual semantics, prosodic speech features, and temporal behavioral trends-to infer engagement, stress, and conceptual understanding. A pilot study with university students demonstrated improved motivation, reduced stress, and moderate academic gains compared to baseline methods. These results underline the promise of integrating semantic reasoning, multimodal fusion, and temporal modeling to support adaptive, student-centered educational interventions.

</details>


### [120] [Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs](https://arxiv.org/abs/2512.10453)
*Lars G. B. Johnsen*

Main category: cs.CL

TL;DR: 本研究探讨大型语言模型（LLM）是否仅依赖表层文本输入便能展现对句法结构的敏感性。通过对比传统语法学中典型的语法现象，观察如GPT-4等模型是否能区分语法正确和错误的句子类型。


<details>
  <summary>Details</summary>
Motivation: 传统生成语法理论认为语法现象差异说明人类内在的层级句法结构。然而，大型语言模型是否有类似的结构化表示尚不清楚，因此作者希望通过经典句法测试来检验模型对句法结构的敏感度。

Method: 作者选取了主语-助动词倒装和寄生空位许可两类经典句法结构作为测试对象。采用向大型语言模型（如GPT-4和LLaMA-3）输入接受性判断型提示，并评测模型对语法正确与否的判别能力。

Result: 结果显示，LLMs在这两种构造下都能稳定地区分语法和非语法变体。这说明模型对结构性信息有敏感性，而不仅仅是依赖于线性顺序。

Conclusion: 大型语言模型通过对表层形式的预测学习，能够自发形成对句法结构的泛化能力，显示出对句法结构的功能性敏感性，即便没有明确编码句法规则。

Abstract: What counts as evidence for syntactic structure? In traditional generative grammar, systematic contrasts in grammaticality such as subject-auxiliary inversion and the licensing of parasitic gaps are taken as evidence for an internal, hierarchical grammar. In this paper, we test whether large language models (LLMs), trained only on surface forms, reproduce these contrasts in ways that imply an underlying structural representation.
  We focus on two classic constructions: subject-auxiliary inversion (testing recognition of the subject boundary) and parasitic gap licensing (testing abstract dependency structure). We evaluate models including GPT-4 and LLaMA-3 using prompts eliciting acceptability ratings. Results show that LLMs reliably distinguish between grammatical and ungrammatical variants in both constructions, and as such support that they are sensitive to structure and not just linear order. Structural generalizations, distinct from cognitive knowledge, emerge from predictive training on surface forms, suggesting functional sensitivity to syntax without explicit encoding.

</details>


### [121] [XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs](https://arxiv.org/abs/2512.10545)
*Iñaki Lacunza,José Javier Saiz,Alexander Shvets,Aitor Gonzalez-Agirre,Marta Villegas*

Main category: cs.CL

TL;DR: 当前的大型语言模型主要依赖高资源语言的数据，导致对中低资源语言的支持较弱。本文提出了优化训练语言分布的方法，并在多语种设定下对比实验，发布了新模型并取得改进效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练时，高资源语言（如英语）数据占主导地位，造成中低资源语言表现较差。为提升模型在多语言环境下的公平性和能力，需要平衡不同语言的数据权重。

Method: 提出XDoGE算法，通过先用DoGE在代理小模型进行语言权重优化，然后用优化后的权重对数据进行重采样，结合从头训练和持续预训练策略，针对六种不同资源级别语言进行建模。实验采用IberoBench框架进行量化评测，并基于Salamandra-2b实现，最终开发出IberianLLM-7B-Instruct模型。

Result: 结果显示，通过优化语言分布，少数语种的数据重复利用以及主导语种的数据下采样，能够显著提升中低资源语言的表现。新提出的IberianLLM-7B-Instruct模型在所关注的多语种任务中取得了较好成绩。

Conclusion: 通过语言分布优化和采用XDoGE方法，可以有效缓解高资源语言主导下的多语言模型表现不均问题，提升中低资源语言的能力。所发布的新模型可为今后涉多语种模型研究提供新途径。

Abstract: Current large language models (LLMs) are trained on massive amounts of text data, primarily from a few dominant languages. Studies suggest that this over-reliance on high-resource languages, such as English, hampers LLM performance in mid- and low-resource languages. To mitigate this problem, we propose to (i) optimize the language distribution by training a small proxy model within a domain-reweighing DoGE algorithm that we extend to XDoGE for a multilingual setup, and (ii) rescale the data and train a full-size model with the established language weights either from scratch or within a continual pre-training phase (CPT). We target six languages possessing a variety of geographic and intra- and inter-language-family relations, namely, English and Spanish (high-resource), Portuguese and Catalan (mid-resource), Galician and Basque (low-resource). We experiment with Salamandra-2b, which is a promising model for these languages. We investigate the effects of substantial data repetition on minor languages and under-sampling on dominant languages using the IberoBench framework for quantitative evaluation. Finally, we release a new promising IberianLLM-7B-Instruct model centering on Iberian languages and English that we pretrained from scratch and further improved using CPT with the XDoGE weights.

</details>


### [122] [Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models](https://arxiv.org/abs/2512.10561)
*Amartya Roy,Elamparithy M,Kripabandhu Ghosh,Ponnurangam Kumaraguru,Adrian de Wynter*

Main category: cs.CL

TL;DR: 该论文探讨了当前大语言模型在因果推理任务中的表现，比较了不同架构的模型在零样本及微调条件下的因果推理能力，发现仅依赖ICL对于因果推理并不可靠。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型借助ICL在许多领域取得进展，但其在涉及多跳、严格合取控制的因果推理上的有效性尚不明确。这对于实际需要严格推断过程的任务具有重要意义。

Method: 作者假设encoder和encoder-decoder架构较适合多跳推理，通过对比各类架构下，经微调与ICL（零/小样本）在自然语言和非自然语言条件下的表现进行实证分析。

Result: 结果显示，仅用ICL模型在因果推理中常常关注错误的特征，表现不佳。decoder only模型对数据分布变化尤为敏感；而微调的encoder及encoder-decoder模型更具泛化能力，且在多种测试条件下均表现更佳，仅在非常大规模下被decoder-only架构超越。

Conclusion: 对于高性价比、短流程且鲁棒的因果推理任务，采用encoder或encoder-decoder并经过针对性微调的模型更为优选。

Abstract: In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.

</details>


### [123] [RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems](https://arxiv.org/abs/2512.10575)
*Hang Ding,Qiming Feng,Dongqi Liu,Qi Zhao,Tao Yao,Shuo Wang,Dongsheng Chen,Jian Li,Zhenye Gan,Jiangning Zhang,Chengjie Wang,Yabiao Wang*

Main category: cs.CL

TL;DR: 本文提出了RoleRMBench，这是首个用于角色扮演对话中奖励建模的系统性基准，并提出了一种新奖励模型RoleRM，显著提升了模型在叙事连贯性和风格一致性等主观维度的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型奖励模型在主观和开放领域（如角色扮演）中难以准确对齐人类偏好，尤其是在叙事和个性化表达方面与人类判断存在显著差距。为填补这一空白，作者提出了更精细化的评价基准和新的奖励建模方法。

Method: 作者提出RoleRMBench基准，包括七个细粒度维度（如叙事管理、角色一致性、互动性等）评价奖励模型，并设计了基于连续隐式偏好（CIP）的RoleRM奖励模型，将主观评价重构为多结构下的连续一致成对监督，从而更好地对齐主观偏好。

Result: RoleRMBench评测显示，现有通用型奖励模型在主观、叙事和风格维度上与人类有较大差距。RoleRM相较于市面上公开和闭源强竞争模型，在主要维度上平均提升超过24%，尤其在叙事连贯性和风格一致性方面表现突出。

Conclusion: RoleRMBench为论主观人类偏好对齐问题提供了可靠的评价基线，RoleRM证明了持续偏好表达和一致性标注的重要性，为以人为中心对话系统中的主观对齐工作奠定了基础。

Abstract: Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems.

</details>


### [124] [AgriGPT-Omni: A Unified Speech-Vision-Text Framework for Multilingual Agricultural Intelligence](https://arxiv.org/abs/2512.10624)
*Bo Yang,Lanfei Feng,Yunkui Chen,Yu Zhang,Jianyu Zhang,Xiao Xu,Nueraili Aierken,Shijian Li*

Main category: cs.CL

TL;DR: 本文提出了AgriGPT-Omni，一个集成语音、视觉和文本的农业多模态大模型，旨在解决农业领域多语种语音数据稀缺、架构分散和评测体系不足的问题。


<details>
  <summary>Details</summary>
Motivation: 农业领域在多模态大模型应用中面临多语种语音数据不足、缺乏统一多模态架构以及缺少全面评测基准等主要挑战，限制了大模型在农业中的推广与应用。

Method: 1）搭建可扩展的数据合成与采集流水线，将农业文本和图像转化为训练数据，构建目前最大的农业多语种语音数据集；2）以三阶段范式（文本知识注入、逐步多模态对齐、基于GRPO的强化学习）训练首个农业全模态大模型，实现多语种和多模态统一推理；3）提出首个农业三模态基准AgriBench-Omni-2K，涵盖多样化三模态任务及多语言测试。

Result: AgriGPT-Omni在多语种、多模态推理和真实语音理解性能上优于通用大模型基线，在多个农业相关任务中取得显著提升。

Conclusion: AgriGPT-Omni推动了农业智能发展，尤其为资源匮乏地区的可持续AI应用提供了数据、模型和工具支持，所有资源将开源以促进可复制性和包容性研究。

Abstract: Despite rapid advances in multimodal large language models, agricultural applications remain constrained by the lack of multilingual speech data, unified multimodal architectures, and comprehensive evaluation benchmarks. To address these challenges, we present AgriGPT-Omni, an agricultural omni-framework that integrates speech, vision, and text in a unified framework. First, we construct a scalable data synthesis and collection pipeline that converts agricultural texts and images into training data, resulting in the largest agricultural speech dataset to date, including 492K synthetic and 1.4K real speech samples across six languages. Second, based on this, we train the first agricultural omni-model via a three-stage paradigm: textual knowledge injection, progressive multimodal alignment, and GRPO-based reinforcement learning, enabling unified reasoning across languages and modalities. Third, we propose AgriBench-Omni-2K, the first tri-modal benchmark for agriculture, covering diverse speech-vision-text tasks and multilingual slices, with standardized protocols and reproducible tools. Experiments show that AgriGPT-Omni significantly outperforms general-purpose baselines on multilingual and multimodal reasoning as well as real-world speech understanding. All models, data, benchmarks, and code will be released to promote reproducible research, inclusive agricultural intelligence, and sustainable AI development for low-resource regions.

</details>


### [125] [From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages](https://arxiv.org/abs/2512.10630)
*Smiljana Antonijevic Ubois*

Main category: cs.CL

TL;DR: 本文以塞尔维亚语为例，分析了低资源语言在大型语言模型（LLM）开发中面临的多重结构性、历史性和社会技术挑战，提出了以“数据关怀”为核心的新框架，以实现更加包容和具有文化敏感性的语言技术。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型主要基于英语等优势语言构建，导致低资源语言（如塞尔维亚语）的技术开发存在文化和语言偏见。作者希望揭示导致这一现象的历史与结构性根源，并推动更具包容性的技术解决方案。

Method: 本研究采用半结构化访谈法，采访了10名相关领域的学者和从业者，包括语言学家、数字人文学者和AI开发者，分析历史文献损毁、工程化优先、数据偏见等具体问题及其成因。

Result: 结果显示，塞尔维亚语的语言技术发展受到历史文献遗失、现有数据偏见、英文学训练依赖及数据集缺乏文化针对性等因素制约，现有工程主导的方式难以捕捉其语言与文化复杂性。

Conclusion: 作者提出了基于CARE（集体利益、控制权、责任与伦理）原则的“数据关怀”框架，将偏见治理嵌入语料库设计、注释与治理全过程，倡导以此作为可复制范式，为低资源语言开发构建更具包容性和文化敏感性的LLM。

Abstract: Large language models are commonly trained on dominant languages like English, and their representation of low resource languages typically reflects cultural and linguistic biases present in the source language materials. Using the Serbian language as a case, this study examines the structural, historical, and sociotechnical factors shaping language technology development for low resource languages in the AI age. Drawing on semi structured interviews with ten scholars and practitioners, including linguists, digital humanists, and AI developers, it traces challenges rooted in historical destruction of Serbian textual heritage, intensified by contemporary issues that drive reductive, engineering first approaches prioritizing functionality over linguistic nuance. These include superficial transliteration, reliance on English-trained models, data bias, and dataset curation lacking cultural specificity. To address these challenges, the study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), that reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance, and positions Data Care as a replicable model for building inclusive, sustainable, and culturally grounded language technologies in contexts where traditional LLM development reproduces existing power imbalances and cultural blind spots.

</details>


### [126] [Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation](https://arxiv.org/abs/2512.10734)
*Rebekka Görge,Sujan Sai Gannamaneni,Tabea Naeven,Hammam Abdelwahab,Héctor Allende-Cid,Armin B. Cremers,Lennard Helmer,Michael Mock,Anna Schmitz,Songkai Xue,Elif Yildirir,Maximilian Poretschkin,Stefan Wrobel*

Main category: cs.CL

TL;DR: 本文提出了一套用于检测和缓解大语言模型训练数据中偏见的完整流程，并在性别、宗教和年龄属性上进行了实证评估，发现该流程能有效减少文本数据的表现偏见和刻板印象，但模型偏见的改进效果有限，暴露出现有评测方法的不足。


<details>
  <summary>Details</summary>
Motivation: 在欧盟AI法案等法规背景下，要求检测并缓解训练数据中针对受保护群体的偏见，以防止模型输出不公正结果，但实际操作方法缺乏，因此亟需可行的检测与缓解流程。

Method: 本文提出四步流程：1. 利用LLM生成的高质量词表检测敏感群体标签；2. 采用人口代表性评分量化表现偏见；3. 通过社会语言学过滤发现并缓解刻板印象；4. 使用语法和上下文敏感的反事实数据增强补偿表现偏见。方法分别在人类验证和基线对比下进行评估，并测试数据去偏处理对不同参数规模LLM偏见基准的影响。

Result: 该流程在实际数据集上显著减少了文本中的表现偏见和（显性）刻板印象。但对多个参数规模的LLM进行微调后，模型在偏见基准上的表现提升不稳定，有时未见改进。

Conclusion: 检测和缓解训练数据偏见能够改善数据本身的公正性，但不能直接保证模型公平性提升，说明当前偏见评测和数据处理方法仍需改进，必须针对性地调整数据才能有效应对模型生成的偏见。

Abstract: Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.

</details>


### [127] [Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving](https://arxiv.org/abs/2512.10739)
*Songyang Gao,Yuzhe Gu,Zijian Wu,Lingkai Kong,Wenwei Zhang,Zhongrui Cai,Fan Zheng,Tianyou Ma,Junhao Shen,Haiteng Zhao,Duanyang Zhang,Huilun Zhang,Kuikun Liu,Chengqi Lyu,Yanhui Duan,Chiyu Chen,Ningsheng Ma,Jianfei Gao,Han Lyu,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的验证方法（OPV），通过结合结果和过程的验证机制，提高了大模型复杂推理任务的可靠性和效率，显著降低了人工标注成本，并在多项实验中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型推理评估方法存在两方面限制：（1）基于结果的验证方法无法发现推理链中的中间错误；（2）基于过程的方法由于高质量人工标注成本高，难以有效评估复杂长链推理过程。因此，迫切需要一种既准确又高效，并且能够大规模扩展的推理验证方法。

Method: 作者提出了OPV（Outcome-based Process Verifier），通过对长推理链COT的结果摘要过程进行验证，既保证了验证的准确性、又提升了标注效率。为了进一步降低标注成本，采用了主动学习框架和专家逐步标注，每一轮挑选当前最不确定的样本进行专家标注，并结合拒绝微调和强化学习优化新一轮的OPV模型。

Result: OPV模型在多个任务上实现了最先进性能，在作者自有验证集上F1得分为83.1，高于Qwen3-Max-Preview等大模型的76.3。在检测合成数据中的虚假正例和与专家评估的一致性上表现优秀。联合策略模型时，大幅提升了模型的推理准确率，如在AIME2025数据集上准确率从55.2%提升到73.3%。

Conclusion: OPV方法有效提升了大模型推理推断的可靠性和自动化验证能力，在降低标注成本的同时，能够广泛适用于复杂推理场景，实现了比现有开源大模型更优异的性能，是推动大模型智能决策应用的重要工具。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \textbf{O}utcome-based \textbf{P}rocess \textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \textsc{\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\% to 73.3\% on AIME2025 as the compute budget scales.

</details>


### [128] [TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage](https://arxiv.org/abs/2512.10741)
*Elroy Galbraith,Chadwick Sutherland,Donahue Morgan*

Main category: cs.CL

TL;DR: 本文提出了一种名为TRIDENT的三层调度员支持系统架构，提升加勒比地区需使用非标准英语方言时的紧急呼叫处理能力。该系统集成了本地口音自适应语音识别、大型语言模型实体抽取以及生物声学压力检测，为调度员提供多维度信号，有望在语音识别不可靠的情况下，辅助实现标准化分诊流程。


<details>
  <summary>Details</summary>
Motivation: 目前应急语音识别系统对非标准英语方言（如加勒比英语）适配性差，导致加勒比人口在紧急服务中处于不利地位，亟需能公平覆盖不同语言变体的AI分诊系统。

Method: TRIDENT系统通过三层架构结合：1）加勒比口音调优的自动语音识别（ASR），2）实体抽取（利用大语言模型实现本地相关疾病/症状等实体识别），3）声学压力检测（判断说话者紧急情绪）。三个信号联用：ASR置信度、结构化医疗实体、语音压力指标，为调度员提供丰富信息支持。系统理论基础来源于心理语言学有关压力下语言转换(code-switching)的研究，并考虑极端灾难下离线运行。

Result: 本文仅介绍了系统架构和理论依据，暂未进行实际加勒比紧急电话场景的实证验证。系统期望提升对口音强、情绪化严重或平静但情报关键报告的识别能力。

Conclusion: TRIDENT系统为建立具口音适应性和公平性的紧急AI分诊奠定了框架基础，指向加勒比等少数族裔群体获得更优质、可及性的紧急医疗服务。后续需通过加勒比实地数据进行经验验证。

Abstract: Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.
  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.
  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.

</details>


### [129] [OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification](https://arxiv.org/abs/2512.10756)
*Zijian Wu,Lingkai Kong,Wenwei Zhang,Songyang Gao,Yuzhe Gu,Zhongrui Cai,Tianyou Ma,Yuhong Liu,Zhi Wang,Runyuan Ma,Guangyu Wang,Wei Li,Conghui He,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的结果驱动过程验证器（OPV），通过验证长链推理中总结结果的推理过程，实现高效且准确的自动检验，显著提升大模型推理链中的错误检测及标注效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型经常依赖于长期推理链（CoTs）来完成复杂任务，但现有结果驱动验证器（OVs）无法检测推理链中的中间错误，而过程驱动验证器（PVs）因人工高质量标注稀缺导致性能受限。因此亟需一种在低标注成本下既能逐步提升验证能力，又能高效自动化大规模验证的方法。

Method: 作者提出了OPV（Outcome-based Process Verifier），对长理由链的总结结果进行合理性验证，实现了验证准确率与效率的双重提升。OPV结合主动学习框架，每一轮用专家标注最不确定的样本，然后通过拒绝微调（RFT）和可验证奖励强化学习（RLVR）不断提升能力，降低标注成本。

Result: 实验显示，OPV在作者自建的基准集OPV-Bench上取得了新SOTA，F1分数达到83.1（优于Qwen3-Max-Preview的76.3）。在合成数据集上OPV能有效检测出假阳性，并与专家评估高度一致。与策略模型结合时，OPV显著提升决策准确率，如在AIME2025数据集上，DeepSeek-R1-Distill-Qwen-32B的准确率从55.2%提升至73.3%。

Conclusion: OPV能够在较低标注开销下，自动高效地验证复杂推理链，提升了大模型自动推理的可控性、可靠性和大规模应用能力，具有广泛的实用与推广价值。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.

</details>


### [130] [Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation](https://arxiv.org/abs/2512.10772)
*Kevin Glocker,Kätriin Kukk,Romina Oji,Marcel Bollmann,Marco Kuhlmann,Jenny Kunz*

Main category: cs.CL

TL;DR: 这篇论文探讨了通过扩大（scaling）预训练英语基础模型尺寸，来更高效地适配中低资源语言的可行性与效果。实验发现，增大模型规模能够提升数据利用率，在获得充足目标语言数据的情况下，性能优于单靠继续预训练的小模型，且有助于减少灾难性遗忘。此外，对不同语言专用模型进行融合，可以构建模块化多语言系统，尽管其效果仍不如联合多语训练，但大模型融合优于小模型，并且融合方法还有优化空间。


<details>
  <summary>Details</summary>
Motivation: 当前大规模多语言模型在中低资源语言上的表现不如单语专业调整的模型，尤其是在较小的模型规模下。作者希望找到一种既能兼顾多语种能力，又兼具适配效率和资源节省的方法。

Method: 作者使用规模消融实验，对比了不同参数规模（FLOPs匹配）的模型：一组是对英语基础模型增大规模，另一组是标准方法继续在目标语言上预训练。评估两者在目标语言上的适配效果，同时探索将不同语言专用大模型融合，建立模块化多语系统。

Result: 结果显示，在有足够目标语言数据时，大模型适配效率高于小模型，能用更少数据获得更好或相当的效果，还能较好保留英语能力，减少遗忘。大模型融合优于小模型，但融合方式差异较大，仍有优化空间。

Conclusion: 通过扩大模型，可更高效地实现语言适配，节省数据，且有助于多语互补；但模型融合方面还需要开发更专门的技术以发挥最佳多语整合性能。

Abstract: Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.

</details>


### [131] [Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting](https://arxiv.org/abs/2512.10780)
*Manurag Khullar,Utkarsh Desai,Poorva Malviya,Aman Dalmia,Zheyuan Ryan Shi*

Main category: cs.CL

TL;DR: 本研究发现，现有大语言模型（LLMs）在处理母婴健康分诊时，对于印度及尼泊尔等多语言环境下的罗马化文本表现明显不如本地文字，F1 drop达5-12分，并可能导致严重的分类错误。


<details>
  <summary>Details</summary>
Motivation: 印度等地临床应用中大量民众习惯用罗马化而非本地文字交流，但此前很少有研究真实评测这种文字变化对大语言模型实际效能的影响。医学分诊是一项高风险应用，该盲点可能影响系统可靠性和安全。

Method: 作者构建并测试了一个包含五种印度语言及尼泊尔语、真实用户罗马化医患交流问题的数据集，对比多款主流LLM在罗马化文本和本地文字文本上的表现，特别聚焦于母婴健康分诊任务。

Result: 罗马化文本导致LLMs分诊性能F1普遍降低5-12分。在与实际医疗组织合作的评估中，这种降级或带来数百万额外错误分诊。作者进一步分析发现LLMs对罗马化输入语义理解基本没有障碍，但其分诊输出对拼写、文字等噪声过于敏感。

Conclusion: LLMs虽能理解罗马化问句语义，但在分诊等严肃医疗任务中无法可靠输出，存在重要医疗安全盲区。建议关注多语多文环境中模型健壮性，提升罗马化处理能力，以保障临床安全。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.

</details>


### [132] [The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality](https://arxiv.org/abs/2512.10791)
*Aileen Cheng,Alon Jacovi,Amir Globerson,Ben Golan,Charles Kwong,Chris Alberti,Connie Tao,Eyal Ben-David,Gaurav Singh Tomar,Lukas Haas,Yonatan Bitton,Adam Bloniarz,Aijun Bai,Andrew Wang,Anfal Siddiqui,Arturo Bajuelos Castillo,Aviel Atias,Chang Liu,Corey Fry,Daniel Balle,Deepanway Ghosal,Doron Kukliansky,Dror Marcus,Elena Gribovskaya,Eran Ofek,Honglei Zhuang,Itay Laish,Jan Ackermann,Lily Wang,Meg Risdal,Megan Barnes,Michael Fink,Mohamed Amin,Moran Ambar,Natan Potikha,Nikita Gupta,Nitzan Katz,Noam Velan,Ofir Roval,Ori Ram,Polina Zablotskaia,Prathamesh Bang,Priyanka Agrawal,Rakesh Ghiya,Sanjay Ganapathy,Simon Baumgartner,Sofia Erell,Sushant Prakash,Thibault Sellam,Vikram Rao,Xuanhui Wang,Yaroslav Akulov,Yulong Yang,Zhen Yang,Zhixin Lai,Zhongru Wu,Anca Dragan,Avinatan Hassidim,Fernando Pereira,Slav Petrov,Srinivasan Venkatachary,Tulsee Doshi,Yossi Matias,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: 本文提出了FACTS Leaderboard，一个用于综合评估语言模型事实性的新型在线排行榜及基准测试套件。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对不同场景全面衡量语言模型事实性的统一指标和评价平台。本文旨在通过设计多维度评测体系，推动模型事实性的发展和对比。

Method: 引入四个子排行榜——多模态事实、参数事实、搜索事实和证据依托事实（v2），分别用自动评价模型对多场景下模型生成文本的事实性打分，并汇总平均，得出统一分数。排行榜同时设有公开和私有数据集，对外开放参与，并保证测试题库的安全性和公平性。

Result: 构建了可公开访问的FACTS Leaderboard套件，实现了全面、自动化、可维持的语言模型事实性评测，确保评价结果权威且可复现。

Conclusion: 该工具为事实性模型评测树立了新的标准，有助于研究者开发和迭代更具事实性的语言模型。

Abstract: We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .

</details>


### [133] [LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification](https://arxiv.org/abs/2512.10793)
*Michael Schlee,Christoph Weisser,Timo Kivimäki,Melchizedek Mashiku,Benjamin Saefken*

Main category: cs.CL

TL;DR: LabelFusion结合传统Transformer分类器和大语言模型，提升文本分类准确率，并兼顾成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有文本分类方法中，传统的Transformer分类器和大语言模型各有优势，但单独使用存在准确率、成本、延迟等权衡问题。该工作旨在融合两者优点，实现更高的多标签/多分类任务表现。

Method: LabelFusion将Transformer分类器（如RoBERTa）的向量嵌入与大语言模型基于提示工程获得的每类分数拼接，然后输入到多层感知机（FusionMLP）进行最终分类。该包提供简易接口及高级API，支持端到端训练与灵活定制。

Result: 在AG News和Reuters 21578（10分类）任务上，LabelFusion分别取得92.4%、92.3%的高准确率。

Conclusion: LabelFusion通过融合ML和LLM的特征，有效提升了文本分类性能，实现了准确率、延迟和成本之间的灵活权衡，适用于多个领域。

Abstract: LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.

</details>


### [134] [Quantifying Emotional Tone in Tolkien's The Hobbit: Dialogue Sentiment Analysis with RegEx, NRC-VAD, and Python](https://arxiv.org/abs/2512.10865)
*Lilin Qiu*

Main category: cs.CL

TL;DR: 本研究利用计算文本分析方法，考察了《霍比特人》中对话的情绪基调，发现其整体积极平和，情感节奏随故事推进有规律变化。


<details>
  <summary>Details</summary>
Motivation: 传统文学分析受主观性影响较大，难以量化文本中的细腻情感变化。研究希望借助数字方法揭示《霍比特人》叙事中情绪节奏和调节的客观规律，为文学解读提供新工具。

Method: 研究先用正则表达式提取小说对话，对话文本经过预处理后，利用NRC-VAD情感词典对对话的愉悦度（valence）、激活度（arousal）和支配度（dominance）三维情感参数打分，并用可视化手段（情绪轨迹图、词云等）辅助分析情感模式。

Result: 分析结果显示，对话整体呈现出高愉悦度、低激活度，随故事发展支配度逐渐增加。故事中的危险与幽默、友谊和缓解交替出现，形成规律变化的情感节奏。

Conclusion: 结合计算方法与文学解读，本研究揭示了托尔金如何通过语言在叙事中营造稳定且富有变化的情感律动，数字分析为文学情感结构研究提供了有力工具。

Abstract: This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.

</details>


### [135] [Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity](https://arxiv.org/abs/2512.10882)
*Hauke Licht*

Main category: cs.CL

TL;DR: 本论文评估了多模态大语言模型（mLLMs）在视频中情绪激昂分析上的能力，发现其在理想情况下表现良好，但在真实世界政治辩论中的效果较差，呼吁对生成式AI持续评估。


<details>
  <summary>Details</summary>
Motivation: 随着情绪分析在政治传播研究中愈发重要，多模态生成式AI工具（如mLLMs）的出现带来新机遇，但目前缺乏其有效性的数据证据，特别是在多模态情感分析领域。

Method: 作者基于两组人工标注的视频数据集，系统评估了当前mLLMs在视频情绪激昂评分方面的表现，并检验了其可靠性及人口统计偏差。

Result: 在理想条件下，mLLMs对情绪激昂的评分高度可靠，且几乎没有人口统计偏差。但在实际议会辩论视频中，mLLMs的性能显著下降，并可能导致数据分析上的负面后果。

Conclusion: 该研究强调，需要对新兴生成式AI方法在政治分析中的表现持续、深入评估，同时为相关研究提供了可复现的分析框架。

Abstract: Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [136] [Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge](https://arxiv.org/abs/2512.10071)
*Junjie Bai,Yu-Wei Chao,Qizhi Chen,Jinwei Gu,Moo Jin Kim,Zhaoshuo Li,Xuan Li,Tsung-Yi Lin,Ming-Yu Liu,Nic Ma,Kaichun Mo,Delin Qu,Shangkun Sun,Hongchi Xia,Fangyin Wei,Xiaohui Zeng*

Main category: cs.RO

TL;DR: 本文介绍了2025 BEHAVIOR Challenge竞赛中的一种领先解决方案，该方案在模拟环境下解决机器人长时序任务，取得了第二名的优异成绩，并大幅超过其他参与者。


<details>
  <summary>Details</summary>
Motivation: 当前机器人在实际家庭环境下执行长时序移动操控任务存在巨大挑战，亟需更强方法以缩小学术研究与真实应用间的差距。竞赛旨在评估和推动相关领域进步。

Method: 作者基于$π_{0.5}$模型，系统性探索训练技巧和数据对模型性能的影响，通过详细消融实验评估预训练和后训练阶段的扩展能力，并提出一整套优化方案。

Result: 该方案在竞赛中获得了非常接近第一名的第二名成绩，整体表现显著优于除第一名外的所有参赛队伍。

Conclusion: 经验总结和设计建议具有广泛参考价值，为将强大的基础模型应用到复杂具身智能场景中提供了切实可行的方法和启示。

Abstract: The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks by physical agents in simulated environments. BEHAVIOR-1K focuses on everyday household tasks that people most want robots to assist with and these tasks introduce long-horizon mobile manipulation challenges in realistic settings, bridging the gap between current research and real-world, human-centric applications. This report presents our solution to the 2025 BEHAVIOR Challenge in a very close 2nd place and substantially outperforms the rest of the submissions. Building on $π_{0.5}$, we focus on systematically building our solution by studying the effects of training techniques and data. Through careful ablations, we show the scaling power in pre-training and post-training phases for competitive performance. We summarize our practical lessons and design recommendations that we hope will provide actionable insights for the broader embodied AI community when adapting powerful foundation models to complex embodied scenarios.

</details>


### [137] [Push Smarter, Not Harder: Hierarchical RL-Diffusion Policy for Efficient Nonprehensile Manipulation](https://arxiv.org/abs/2512.10099)
*Steven Caro,Stephen L. Smith*

Main category: cs.RO

TL;DR: 本文提出了一种分层强化学习-扩散模型策略（HeRD），将物体推送任务分为高层目标选择与低层轨迹生成，并在2D仿真环境下实现了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 非抓取式操作（如在杂乱环境中推物体）具有复杂接触动力学与长期规划的挑战，现有方法难以有效解决长时规划与轨迹生成的结合问题。

Method: 提出HeRD架构：高层使用强化学习代理选择中间空间目标，低层利用基于目标的扩散模型生成可行有效的轨迹，结合了强化学习的长期奖励优化与扩散模型的生成能力。

Result: 在2D仿真环境中，HeRD在成功率、路径效率及多环境泛化性上均优于最先进的基线方法。

Conclusion: 分层控制结合生成式低层规划，在可扩展、有目标导向的非抓取式操作任务中展现出很大潜力。

Abstract: Nonprehensile manipulation, such as pushing objects across cluttered environments, presents a challenging control problem due to complex contact dynamics and long-horizon planning requirements. In this work, we propose HeRD, a hierarchical reinforcement learning-diffusion policy that decomposes pushing tasks into two levels: high-level goal selection and low-level trajectory generation. We employ a high-level reinforcement learning (RL) agent to select intermediate spatial goals, and a low-level goal-conditioned diffusion model to generate feasible, efficient trajectories to reach them.
  This architecture combines the long-term reward maximizing behaviour of RL with the generative capabilities of diffusion models. We evaluate our method in a 2D simulation environment and show that it outperforms the state-of-the-art baseline in success rate, path efficiency, and generalization across multiple environment configurations. Our results suggest that hierarchical control with generative low-level planning is a promising direction for scalable, goal-directed nonprehensile manipulation. Code, documentation, and trained models are available: https://github.com/carosteven/HeRD.

</details>


### [138] [Fast Functionally Redundant Inverse Kinematics for Robotic Toolpath Optimisation in Manufacturing Tasks](https://arxiv.org/abs/2512.10116)
*Andrew Razjigaev,Hans Lohr,Alejandro Vargas-Uscategui,Peter King,Tirthankar Bandyopadhyay*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的逆向运动学算法，能高效解决工业六轴机械臂存在的功能冗余问题，提升工具路径规划的灵活性和可行性。


<details>
  <summary>Details</summary>
Motivation: 许多工业机械臂作业因工具轴对称而存在功能冗余，这本可被利用以提升空间利用率与操作灵活性，但目前逆向运动学算法在在线规划中的应用不足，现有方法多为计算开销较大的离线规划。

Method: 提出了一种利用任务空间分解、阻尼最小二乘法和Halley方法相结合的逆向运动学算法，实现快速且鲁棒的功能冗余求解，同时减少机械臂关节的运动量。

Result: 该方法在非平面冷喷涂表面上的工具路径优化中验证有效，能快速生成最小化关节运动的路径规划，并扩大了复杂路径的可操作空间。同时在ABB工业机械臂与冷喷枪实体测试中得到了验证。

Conclusion: 所提逆向运动学算法对解决功能冗余问题高效且实用，可以显著提升机械臂高难度任务的路径可行性和操作性能，具有广阔的工业应用前景。

Abstract: Industrial automation with six-axis robotic arms is critical for many manufacturing tasks, including welding and additive manufacturing applications; however, many of these operations are functionally redundant due to the symmetrical tool axis, which effectively makes the operation a five-axis task. Exploiting this redundancy is crucial for achieving the desired workspace and dexterity required for the feasibility and optimisation of toolpath planning. Inverse kinematics algorithms can solve this in a fast, reactive framework, but these techniques are underutilised over the more computationally expensive offline planning methods. We propose a novel algorithm to solve functionally redundant inverse kinematics for robotic manipulation utilising a task space decomposition approach, the damped least-squares method and Halley's method to achieve fast and robust solutions with reduced joint motion. We evaluate our methodology in the case of toolpath optimisation in a cold spray coating application on a non-planar surface. The functionally redundant inverse kinematics algorithm can quickly solve motion plans that minimise joint motion, expanding the feasible operating space of the complex toolpath. We validate our approach on an industrial ABB manipulator and cold-spray gun executing the computed toolpath.

</details>


### [139] [Inertial Magnetic SLAM Systems Using Low-Cost Sensors](https://arxiv.org/abs/2512.10128)
*Chuan Huang,Gustaf Hendeby,Isaac Skog*

Main category: cs.RO

TL;DR: 本文提出了一种基于低成本传感器（IMU、磁力计和气压计）的3D惯性-磁场SLAM系统，提高了无视觉条件下的定位与建图精度。该系统包含松耦合和紧耦合两种架构，实验结果表明紧耦合方案表现更优，定位误差小，具备实际应急应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当前基于磁场的SLAM系统需要额外的高精度里程计（如视觉/轮式），且不适用于能见度差场景，因此亟需开发仅依赖低成本、非视觉传感器的新型SLAM系统。

Method: 提出两种IM-SLAM系统：1）松耦合，将局部与全局磁场模型分开分别建模；2）紧耦合，将两者整合到单一状态空间模型中。均采用IMU、磁力计阵列及气压计为硬件基础，通过状态空间表示方法实现联合定位和磁场建图。

Result: 实验表明，紧耦合IM-SLAM系统在绝大多数情况下的定位误差优于松耦合系统，100米行进距离内的误差为米级，验证了低成本传感器支持下全三维SLAM系统的可行性。

Conclusion: 无需视觉传感器，仅依赖常见低成本硬件即可实现高精度3D惯性-磁场SLAM，特别适用于低可见度环境中的应急救援等应用。

Abstract: Spatially inhomogeneous magnetic fields offer a valuable, non-visual information source for positioning. Among systems leveraging this, magnetic field-based simultaneous localization and mapping (SLAM) systems are particularly attractive because they can provide positioning information and build a magnetic field map on the fly. Moreover, they have bounded error within mapped regions. However, state-of-the-art methods typically require low-drift odometry data provided by visual odometry or a wheel encoder, etc. This is because these systems need to minimize/reduce positioning errors while exploring, which happens when they are in unmapped regions. To address these limitations, this work proposes a loosely coupled and a tightly coupled inertial magnetic SLAM (IM-SLAM) system. The proposed systems use commonly available low-cost sensors: an inertial measurement unit (IMU), a magnetometer array, and a barometer. The use of non-visual data provides a significant advantage over visual-based systems, making it robust to low-visibility conditions. Both systems employ state-space representations, and magnetic field models on different scales. The difference lies in how they use a local and global magnetic field model. The loosely coupled system uses these models separately in two state-space models, while the tightly coupled system integrates them into one state-space model. Experiment results show that the tightly coupled IM-SLAM system achieves lower positioning errors than the loosely coupled system in most scenarios, with typical errors on the order of meters per 100 meters traveled. These results demonstrate the feasiblity of developing a full 3D IM-SLAM systems using low-cost sensors and the potential of applying these systems in emergency response scenarios such as mine/fire rescue.

</details>


### [140] [Task-Oriented Grasping Using Reinforcement Learning with a Contextual Reward Machine](https://arxiv.org/abs/2512.10235)
*Hui Li,Akhlak Uz Zaman,Fujian Yan,Hongsheng He*

Main category: cs.RO

TL;DR: 提出了一种结合情境奖励机的强化学习框架，大幅提升了抓取任务（模拟和真实环境下）的效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有抓取任务因状态空间大、任务复杂，强化学习方法面临效率和泛化能力的挑战。如何简化任务、提高学习效率是亟需解决的问题。

Method: 将抓取任务分解为多个易于管理的子任务，每个阶段都有独立的奖励函数、动作空间和状态抽象；引入阶段间的奖励，规范模型在阶段切换时的行为；基于PPO算法进行训练，以此提升学习效率和成功率。

Result: 在1000个包含不同对象和抓取方式的仿真任务中，方法成功率达95%，优于现有方法，并在学习速度与最终表现上领先；将方法迁移到真实机器人上，在60次真实抓取任务中达到83.3%的成功率。

Conclusion: 该方法显著提升了任务型抓取的准确率、数据和学习效率，展示了在模拟及真实环境下的应用潜力。

Abstract: This paper presents a reinforcement learning framework that incorporates a Contextual Reward Machine for task-oriented grasping. The Contextual Reward Machine reduces task complexity by decomposing grasping tasks into manageable sub-tasks. Each sub-task is associated with a stage-specific context, including a reward function, an action space, and a state abstraction function. This contextual information enables efficient intra-stage guidance and improves learning efficiency by reducing the state-action space and guiding exploration within clearly defined boundaries. In addition, transition rewards are introduced to encourage or penalize transitions between stages which guides the model toward desirable stage sequences and further accelerates convergence. When integrated with the Proximal Policy Optimization algorithm, the proposed method achieved a 95% success rate across 1,000 simulated grasping tasks encompassing diverse objects, affordances, and grasp topologies. It outperformed the state-of-the-art methods in both learning speed and success rate. The approach was transferred to a real robot, where it achieved a success rate of 83.3% in 60 grasping tasks over six affordances. These experimental results demonstrate superior accuracy, data efficiency, and learning efficiency. They underscore the model's potential to advance task-oriented grasping in both simulated and real-world settings.

</details>


### [141] [Lies We Can Trust: Quantifying Action Uncertainty with Inaccurate Stochastic Dynamics through Conformalized Nonholonomic Lie Groups](https://arxiv.org/abs/2512.10294)
*Luís Marques,Maani Ghaffari,Dmitry Berenson*

Main category: cs.RO

TL;DR: 提出了一种称为CLAPS的对称感知型共形预测方法，能对系统动作结果给出带概率保证的预测集，尤其适用于非欧氏配置空间（如SE(2)）。该方法能更高效、准确地反映系统不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人不确定性量化方法对误差分布等有强假设且常忽视系统的非欧氏结构，这导致预测集不具实际概率意义，不利于安全控制。传统共形预测也未覆盖如SE(2)等非欧氏空间，因此需要新的方法来兼顾动力系统结构与严格的统计保证。

Method: 在对称空间（Lie群，如SE(2)）上定义非共形分数，使用共形预测构造具有频率概率保证的预测集。方法理论上推导并保证在非欧氏空间下的覆盖率，不依赖动力模型的质量，也不假定特定分布类型。通过仿真JetBot和实车MBot测试方法效果。

Result: CLAPS较现有共形预测方法，更高效利用配置空间结构，得到的预测集体积更小，覆盖率更优，能更真实反映系统的实际不确定性。实验在仿真与实车中都验证了效果。

Conclusion: 通过在Lie群空间下构建共形预测集，能为机器人系统提供更紧致且有概率保证的不确定性表示，促进了安全控制与实际部署，优于忽略结构的传统方法。

Abstract: We propose Conformal Lie-group Action Prediction Sets (CLAPS), a symmetry-aware conformal prediction-based algorithm that constructs, for a given action, a set guaranteed to contain the resulting system configuration at a user-defined probability. Our assurance holds under both aleatoric and epistemic uncertainty, non-asymptotically, and does not require strong assumptions about the true system dynamics, the uncertainty sources, or the quality of the approximate dynamics model. Typically, uncertainty quantification is tackled by making strong assumptions about the error distribution or magnitude, or by relying on uncalibrated uncertainty estimates - i.e., with no link to frequentist probabilities - which are insufficient for safe control. Recently, conformal prediction has emerged as a statistical framework capable of providing distribution-free probabilistic guarantees on test-time prediction accuracy. While current conformal methods treat robots as Euclidean points, many systems have non-Euclidean configurations, e.g., some mobile robots have SE(2). In this work, we rigorously analyze configuration errors using Lie groups, extending previous Euclidean Space theoretical guarantees to SE(2). Our experiments on a simulated JetBot, and on a real MBot, suggest that by considering the configuration space's structure, our symmetry-informed nonconformity score leads to more volume-efficient prediction regions which represent the underlying uncertainty better than existing approaches.

</details>


### [142] [Design of a six wheel suspension and a three-axis linear actuation mechanism for a laser weeding robot](https://arxiv.org/abs/2512.10319)
*Muhammad Usama,Muhammad Ibrahim Khan,Ahmad Hasan,Muhammad Shaaf Nadeem,Khawaja Fahad Iqbal,Jawad Aslam,Mian Ashfaq Ali,Asad Nisar Awan*

Main category: cs.RO

TL;DR: 本文提出了一种基于激光技术的自动除草农业机器人，具有高检测率和高精度除草能力，可提升精准农业效率。


<details>
  <summary>Details</summary>
Motivation: 传统的机械除草效率低下，化学除草剂会破坏土壤生态。因此，需要一种高效且可持续的除草解决方案。

Method: 研究团队设计了一款六轮机器人，采用新颖的双四连杆悬挂结构以提升地形适应性。机器人利用三维线性驱动引导低能激光束对准检测到的杂草进行除草。并通过田间实地测试评估性能。

Result: 实地测试显示，机器人能有效通过高达15厘米的障碍物，理想速度下杂草检测率86.2%，除草激光命中率97%，每米作业用时87秒，激光定位误差仅1.54毫米。

Conclusion: 该机器人在速度、精度和效率方面表现出色，有望大幅提升精准农业中的自动除草作业，减少对化学除草剂的依赖，实现更环保的农业生产方式。

Abstract: Mobile robots are increasingly utilized in agriculture to automate labor-intensive tasks such as weeding, sowing, harvesting and soil analysis. Recently, agricultural robots have been developed to detect and remove weeds using mechanical tools or precise herbicide sprays. Mechanical weeding is inefficient over large fields, and herbicides harm the soil ecosystem. Laser weeding with mobile robots has emerged as a sustainable alternative in precision farming. In this paper, we present an autonomous weeding robot that uses controlled exposure to a low energy laser beam for weed removal. The proposed robot is six-wheeled with a novel double four-bar suspension for higher stability. The laser is guided towards the detected weeds by a three-dimensional linear actuation mechanism. Field tests have demonstrated the robot's capability to navigate agricultural terrains effectively by overcoming obstacles up to 15 cm in height. At an optimal speed of 42.5 cm/s, the robot achieves a weed detection rate of 86.2\% and operating time of 87 seconds per meter. The laser actuation mechanism maintains a minimal mean positional error of 1.54 mm, combined with a high hit rate of 97\%, ensuring effective and accurate weed removal. This combination of speed, accuracy, and efficiency highlights the robot's potential for significantly enhancing precision farming practices.

</details>


### [143] [Design and Validation of an Under-actuated Robotic Finger with Synchronous Tendon Routing](https://arxiv.org/abs/2512.10349)
*Quan Yuan,Zhenting Du,Daqian Cao,Weibang Bai*

Main category: cs.RO

TL;DR: 本论文提出一种新型的腱驱动欠驱动机械手指（UTRF），通过单根腱实现同步多关节驱动，大幅简化了机械结构和减小了体积，实现了高刚度与自适应柔顺性的兼顾。


<details>
  <summary>Details</summary>
Motivation: 传统腱驱动欠驱动机械手指虽然在灵巧操作与结构简化方面有优势，但在紧凑结构下同时实现高负载能力与自适应柔顺性具有挑战。作者希望通过新的腱走线方案减少执行器数量，提升整体结构性能。

Method: 提出并实现了一种将所有关节以固定角速度耦合的同步腱走线，使整个手指可由单一驱动器控制。推导了包含腱弹性的运动学与静力学模型，通过原型实验验证了刚度与形变量预期。将该结构集成于五指机械手中，考察其在实际操作场景下的抓取效果。

Result: 单指原型在3kg端载荷下表现出1.2x10^3 N/m刚度，平均形变量预测误差仅1.0mm（占手指总长0.322%）。组装成五指手后，能够在多种实际任务中表现出良好、可预测的刚度及可靠的抓取表现。

Conclusion: 所提出的UTRF腱走线策略在保持最小驱动器数量的同时，实现了高刚度、良好的抓取适应性及紧凑结构，对多指机械手的实际工程应用具有重要价值。

Abstract: Tendon-driven under-actuated robotic fingers provide advantages for dexterous manipulation through reduced actuator requirements and simplified mechanical design. However, achieving both high load capacity and adaptive compliance in a compact form remains challenging. This paper presents an under-actuated tendon-driven robotic finger (UTRF) featuring a synchronous tendon routing that mechanically couples all joints with fixed angular velocity ratios, enabling the entire finger to be actuated by a single actuator. This approach significantly reduces the number of actuators required in multi-finger hands, resulting in a lighter and more compact structure without sacrificing stiffness or compliance. The kinematic and static models of the finger are derived, incorporating tendon elasticity to predict structural stiffness. A single-finger prototype was fabricated and tested under static loading, showing an average deflection prediction error of 1.0 mm (0.322% of total finger length) and a measured stiffness of 1.2x10^3 N/m under a 3 kg tip load. Integration into a five-finger robotic hand (UTRF-RoboHand) demonstrates effective object manipulation across diverse scenarios, confirming that the proposed routing achieves predictable stiffness and reliable grasping performance with a minimal actuator count.

</details>


### [144] [CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation](https://arxiv.org/abs/2512.10360)
*Liuyi Wang,Zongtao He,Jinlong Li,Xiaoyan Qi,Mengxian Hu,Chenpeng Yao,Chengju Liu,Qijun Chen*

Main category: cs.RO

TL;DR: 本文提出了CLASH框架，将小模型规划器与大模型推理器协作，实现了当前最优的视觉-语言导航能力，并在仿真和真实世界中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型具有强大推理能力，但在视觉-语言导航任务中仍不如专用的小型全景模型。为了充分利用大模型的推理优势和小模型的泛化能力，提升机器人在复杂环境下根据自然语言指令导航的性能，提出新的协作框架。

Method: 提出CLASH（Collaborative Large-Small Hierarchy）框架，集成了反应型小模型规划器（RSMP）和反思型大模型推理器（RLMR），通过不确定性感知融合机制（UCM）自适应整合决策。仿真中以可学习点目标策略取代传统障碍物避让，现实中则融合LiDAR聚类和在线SLAM的方式生成可行路径。

Result: CLASH在VLN-CE排行榜上取得了第一名，相较前序最优方法在SR和SPL指标上有显著提升。真实环境实验亦表明其具备较强的鲁棒性和泛化能力。

Conclusion: CLASH框架有效结合大、小模型的优势，实现了视觉-语言导航领域新的性能高度，适用于仿真和实际部署场景。

Abstract: Vision-and-Language Navigation (VLN) requires robots to follow natural language instructions and navigate complex environments without prior maps. While recent vision-language large models demonstrate strong reasoning abilities, they often underperform task-specific panoramic small models in VLN tasks. To address this, we propose CLASH (Collaborative Large-Small Hierarchy), a VLN-CE framework that integrates a reactive small-model planner (RSMP) with a reflective large-model reasoner (RLMR). RSMP adopts a causal-learning-based dual-branch architecture to enhance generalization, while RLMR leverages panoramic visual prompting with chain-of-thought reasoning to support interpretable spatial understanding and navigation. We further introduce an uncertainty-aware collaboration mechanism (UCM) that adaptively fuses decisions from both models. For obstacle avoidance, in simulation, we replace the rule-based controller with a fully learnable point-goal policy, and in real-world deployment, we design a LiDAR-based clustering module for generating navigable waypoints and pair it with an online SLAM-based local controller. CLASH achieves state-of-the-art (SoTA) results (ranking 1-st) on the VLN-CE leaderboard, significantly improving SR and SPL on the test-unseen set over the previous SoTA methods. Real-world experiments demonstrate CLASH's strong robustness, validating its effectiveness in both simulation and deployment scenarios.

</details>


### [145] [RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI](https://arxiv.org/abs/2512.10394)
*Weifan Guan,Huasen Xi,Chenxiao Zhang,Aosheng Li,Qinghao Hu,Jian Cheng*

Main category: cs.RO

TL;DR: 本文提出了RoboNeuron，一个结合大模型认知能力与机器人操作系统的通用部署框架，极大提升了机器人系统的灵活性和跨场景适应性。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能系统存在工程壁垒，如跨场景自适应性差、模块耦合紧密和推理加速碎片化等问题，急需高效的通用解决框架。

Method: RoboNeuron首次将大语言模型（LLM）和视觉-语言-动作模型（VLA）的认知能力，深度融合到ROS实时执行框架。通过模型上下文协议（MCP）作为语义桥梁，实现LLM对底层机器人工具的动态编排。采用高度模块化架构，将感知、推理、控制严格解耦，并自动化将ROS消息转化为可调用的MCP函数，简化开发流程。

Result: RoboNeuron显著提升了具身智能的跨场景自适应性和各模块的灵活性，开发效率得到提高，并能支持系统性水平基准测试。

Conclusion: RoboNeuron为具身智能应用的可扩展和现实部署提供了坚实的基础，实现了认知与操作系统的紧密结合，解决了传统系统中的工程难题。

Abstract: Current embodied AI systems face severe engineering impediments, primarily characterized by poor cross-scenario adaptability, rigid inter-module coupling, and fragmented inference acceleration. To overcome these limitations, we propose RoboNeuron, a universal deployment framework for embodied intelligence. RoboNeuron is the first framework to deeply integrate the cognitive capabilities of Large Language Models (LLMs) and Vision-Language-Action (VLA) models with the real-time execution backbone of the Robot Operating System (ROS). We utilize the Model Context Protocol (MCP) as a semantic bridge, enabling the LLM to dynamically orchestrate underlying robotic tools. The framework establishes a highly modular architecture that strictly decouples sensing, reasoning, and control by leveraging ROS's unified communication interfaces. Crucially, we introduce an automated tool to translate ROS messages into callable MCP functions, significantly streamlining development. RoboNeuron significantly enhances cross-scenario adaptability and component flexibility, while establishing a systematic platform for horizontal performance benchmarking, laying a robust foundation for scalable real-world embodied applications.

</details>


### [146] [Symphony: A Heuristic Normalized Calibrated Advantage Actor and Critic Algorithm in application for Humanoid Robots](https://arxiv.org/abs/2512.10477)
*Timur Ishuov,Michele Folgheraiter,Madi Nurmanov,Goncalo Gordo,Richárd Farkas,József Dombi*

Main category: cs.RO

TL;DR: 论文提出了一种适用于从零训练仿人机器人且兼具高效性与安全性的强化学习框架，包括“襁褓”正则化、有限参数噪声、渐退回放缓存与临时优势更新等创新要素。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人强化学习训练往往需要极多步数，不切实际或损害机械安全，而人类学习过程其实也并不快速。作者旨在提升仿人机器人训练的采样效率与动作安全性，缩短学习所需时间。

Method: 提出了Symphony算法：1）通过“Swaddling”正则化约束智能体快速但不稳定的发展，对动作强度进行特定惩罚；2）设置有限的参数化噪声而不是持续增加高斯噪声，从而在保证探索的同时保护机器人硬件；3）创新的渐退回放缓存，结合近期和长远记忆，提高经验采样效率；4）提出临时优势辅助网络更新，合并损失计算，实现更高效的联合训练。

Result: 实验结果显示，该方法在采样效率和机器人本体安全方面均优于常规随机算法，训练更稳定、有利于机器人硬件保护。

Conclusion: Symphony算法能够在样本效率、动作安全和环境安全三者之间取得良好平衡，为仿人机器人从零训练提供了更可行且实用的方法。

Abstract: In our work we not explicitly hint that it is a misconception to think that humans learn fast. Learning process takes time. Babies start learning to move in the restricted liquid area called placenta. Children often are limited by underdeveloped body. Even adults are not allowed to participate in complex competitions right away. However, with robots, when learning from scratch, we often don't have the privilege of waiting for dozen millions of steps. "Swaddling" regularization is responsible for restraining an agent in rapid but unstable development penalizing action strength in a specific way not affecting actions directly. The Symphony, Transitional-policy Deterministic Actor and Critic algorithm, is a concise combination of different ideas for possibility of training humanoid robots from scratch with Sample Efficiency, Sample Proximity and Safety of Actions in mind. It is no secret that continuous increase in Gaussian noise without appropriate smoothing is harmful for motors and gearboxes. Compared to Stochastic algorithms, we set a limited parametric noise and promote a reduced strength of actions, safely increasing entropy, since the actions are kind of immersed in weaker noise. When actions require more extreme values, actions rise above the weak noise. Training becomes empirically much safer for both the environment around and the robot's mechanisms. We use Fading Replay Buffer: using a fixed formula containing the hyperbolic tangent, we adjust the batch sampling probability: the memory contains a recent memory and a long-term memory trail. Fading Replay Buffer allows us to use Temporal Advantage when we improve the current Critic Network prediction compared to the exponential moving average. Temporal Advantage allows us to update Actor and Critic in one pass, as well as combine Actor and Critic in one Object and implement their Losses in one line.

</details>


### [147] [Seamless Outdoor-Indoor Pedestrian Positioning System with GNSS/UWB/IMU Fusion: A Comparison of EKF, FGO, and PF](https://arxiv.org/abs/2512.10480)
*Jiaqiang Zhang,Xianjia Yu,Sier Ha,Paola Torrico Moron,Sahar Salimpour,Farhad Kerama,Haizhou Zhang,Tomi Westerlund*

Main category: cs.RO

TL;DR: 本文提出了一种融合GNSS、UWB和IMU的无缝行人定位系统，并对三种概率后端（ESKF、滑动窗口因子图、粒子滤波）进行了对比，ESKF表现最好。


<details>
  <summary>Details</summary>
Motivation: 单一传感器（GNSS、UWB、PDR）在行人定位中均存在局限性，如遮挡、多路径效应和漂移，尤其在户外-室内环境下更为突出，亟需融合多种传感器以提升定位连续性和精度。

Method: 构建GNSS/UWB/IMU融合定位框架，以基于胸部IMU的PDR为主干，户外融合GNSS，室内结合UWB。引入基于开源地图的可行性约束，提高过渡鲁棒性和抵抗城市GNSS信号退化。设计三种概率后端并对比：误差状态扩展卡尔曼滤波（ESKF）、滑动窗口因子图优化、粒子滤波。采用ROS 2实时运行于可穿戴设备。

Result: 在室内（UWB+PDR）、室外（GNSS+PDR）、无缝（GNSS+UWB+PDR）三场景评测，ESKF后端在总体性能上最为一致和优越。

Conclusion: 多传感器融合与地图可行性约束可以显著提升行人无缝定位系统的鲁棒性和精度。误差状态扩展卡尔曼滤波是当前实现下的最佳后端选择。

Abstract: Accurate and continuous pedestrian positioning across outdoor-indoor environments remains challenging because GNSS, UWB, and inertial PDR are complementary yet individually fragile under signal blockage, multipath, and drift. This paper presents a unified GNSS/UWB/IMU fusion framework for seamless pedestrian localization and provides a controlled comparison of three probabilistic back-ends: an error-state extended Kalman filter, sliding-window factor graph optimization, and a particle filter. The system uses chest-mounted IMU-based PDR as the motion backbone and integrates absolute updates from GNSS outdoors and UWB indoors. To enhance transition robustness and mitigate urban GNSS degradation, we introduce a lightweight map-based feasibility constraint derived from OpenStreetMap building footprints, treating most building interiors as non-navigable while allowing motion inside a designated UWB-instrumented building. The framework is implemented in ROS 2 and runs in real time on a wearable platform, with visualization in Foxglove. We evaluate three scenarios: indoor (UWB+PDR), outdoor (GNSS+PDR), and seamless outdoor-indoor (GNSS+UWB+PDR). Results show that the ESKF provides the most consistent overall performance in our implementation.

</details>


### [148] [Contact SLAM: An Active Tactile Exploration Policy Based on Physical Reasoning Utilized in Robotic Fine Blind Manipulation Tasks](https://arxiv.org/abs/2512.10481)
*Gaozhao Wang,Xing Liu,Zhenduo Ye,Zhengxiong Liu,Panfeng Huang*

Main category: cs.RO

TL;DR: 本论文提出了一种新颖的触觉感知与物理驱动结合的方法“Contact SLAM”，实现无视觉遮挡下的高效接触操作。


<details>
  <summary>Details</summary>
Motivation: 当前机器人在接触丰富操作任务中依赖视觉感知，但部分场景存在视觉遮挡，导致机器人无法实时获取环境状态，限制了操作能力。亟需视觉缺失下的环境感知与操作新方法。

Method: 提出了“Contact SLAM”方法，仅使用机器人触觉传感器和场景先验知识来估计环境状态、执行操作。同时设计了一种主动探索策略，能逐步减少交互过程中的不确定性。

Result: 在多个接触丰富的任务（如插座组装、推块）中，实验验证了所提方法的有效性和精确性。

Conclusion: “Contact SLAM”能够在无视觉反馈条件下实现环境感知和高效操作，拓展了机器人在复杂环境下的应用能力。

Abstract: Contact-rich manipulation is difficult for robots to execute and requires accurate perception of the environment. In some scenarios, vision is occluded. The robot can then no longer obtain real-time scene state information through visual feedback. This is called ``blind manipulation". In this manuscript, a novel physically-driven contact cognition method, called ``Contact SLAM", is proposed. It estimates the state of the environment and achieves manipulation using only tactile sensing and prior knowledge of the scene. To maximize exploration efficiency, this manuscript also designs an active exploration policy. The policy gradually reduces uncertainties in the manipulation scene. The experimental results demonstrated the effectiveness and accuracy of the proposed method in several contact-rich tasks, including the difficult and delicate socket assembly task and block-pushing task.

</details>


### [149] [Neural Ranging Inertial Odometry](https://arxiv.org/abs/2512.10531)
*Si Wang,Bingqi Shen,Fei Wang,Yanjun Cao,Rong Xiong,Yue Wang*

Main category: cs.RO

TL;DR: 本文提出了一种基于神经网络的融合框架，提升了UWB与惯性测距定位在复杂环境中的精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: UWB在GPS不可用环境下具有前景，但受限于传感器布置和多径等干扰，导致实际定位精度下降，特别是在隧道等典型复杂场景中的挑战亟需解决。

Method: 提出了一种融合了图注意力UWB网络和循环神经惯性网络的神经融合框架。其图网络可学习与场景相关的测距模式，支持任意锚点/标签数量，无需标定，结合最小二乘法与名义帧设计提升性能和可扩展性。

Result: 在公开和自采数据集（涵盖室内、室外、隧道）上进行了大量实验，结果显示新方法在复杂场景（包括凸包外及单锚点情况下）下优于现有方案。

Conclusion: 所提方法在面对多信号干扰等复杂条件下，展现出更高的定位精度和鲁棒性，证明了其实用价值和普适性。

Abstract: Ultra-wideband (UWB) has shown promising potential in GPS-denied localization thanks to its lightweight and drift-free characteristics, while the accuracy is limited in real scenarios due to its sensitivity to sensor arrangement and non-Gaussian pattern induced by multi-path or multi-signal interference, which commonly occurs in many typical applications like long tunnels. We introduce a novel neural fusion framework for ranging inertial odometry which involves a graph attention UWB network and a recurrent neural inertial network. Our graph net learns scene-relevant ranging patterns and adapts to any number of anchors or tags, realizing accurate positioning without calibration. Additionally, the integration of least squares and the incorporation of nominal frame enhance overall performance and scalability. The effectiveness and robustness of our methods are validated through extensive experiments on both public and self-collected datasets, spanning indoor, outdoor, and tunnel environments. The results demonstrate the superiority of our proposed IR-ULSG in handling challenging conditions, including scenarios outside the convex envelope and cases where only a single anchor is available.

</details>


### [150] [Mr. Virgil: Learning Multi-robot Visual-range Relative Localization](https://arxiv.org/abs/2512.10540)
*Si Wang,Zhehan Li,Jiadong Lu,Rong Xiong,Yanjun Cao,Yue Wang*

Main category: cs.RO

TL;DR: 本文提出了一种融合UWB与视觉的多机器人相对定位方法Mr. Virgil，结合图神经网络与可微分位姿图优化，提升了匹配准确性和定位精度，在多种场景下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 多机器人定位依靠UWB-视觉融合，现有方法对身份硬件或精细调参依赖强，匹配错误会带来系统性风险。需要一种更鲁棒且端到端的解决方案。

Method: 设计了端到端的Mr. Virgil框架：前端应用图神经网络进行UWB与视觉检测的数据关联与不确定性估计，并提供初始位姿；后端使用可微分位姿图优化，提升最终定位精度。支持分布式应用。

Result: 在多种机器人数量、仿真与真实环境、不同遮挡条件下，实验表明该方法在稳定性与准确性方面均超过传统方法。

Conclusion: Mr. Virgil显著提升了多机器人UWB-视觉融合定位的鲁棒性和精度，并具备实际落地能力。

Abstract: Ultra-wideband (UWB)-vision fusion localization has achieved extensive applications in the domain of multi-agent relative localization. The challenging matching problem between robots and visual detection renders existing methods highly dependent on identity-encoded hardware or delicate tuning algorithms. Overconfident yet erroneous matches may bring about irreversible damage to the localization system. To address this issue, we introduce Mr. Virgil, an end-to-end learning multi-robot visual-range relative localization framework, consisting of a graph neural network for data association between UWB rangings and visual detections, and a differentiable pose graph optimization (PGO) back-end. The graph-based front-end supplies robust matching results, accurate initial position predictions, and credible uncertainty estimates, which are subsequently integrated into the PGO back-end to elevate the accuracy of the final pose estimation. Additionally, a decentralized system is implemented for real-world applications. Experiments spanning varying robot numbers, simulation and real-world, occlusion and non-occlusion conditions showcase the stability and exactitude under various scenes compared to conventional methods. Our code is available at: https://github.com/HiOnes/Mr-Virgil.

</details>


### [151] [Motion Planning for Safe Landing of a Human-Piloted Parafoil](https://arxiv.org/abs/2512.10595)
*Maximillian Fainkich,Kiril Solovey,Anna Clarke*

Main category: cs.RO

TL;DR: 本文提出了一种基于采样的运动规划算法（SST），用于为带翼伞（parafoil）飞行员计算更加安全的降落轨迹，并与实际飞行员轨迹进行对比，发现算法生成的轨迹在安全性和效率上优于人类飞行员。


<details>
  <summary>Details</summary>
Motivation: 当前带翼伞事故多发生在操控与着陆阶段，主要由于飞行员判断失误。同时，系统的训练模拟器匮乏，且缺乏针对带翼伞飞行训练用的轨迹规划技术，制约了新手飞行员的培养与安全提升。因此需要发展算法生成安全、高效的飞行轨迹，并将其应用于训练。

Method: 作者将Stable Sparse RRT（SST）采样运动规划算法进行适应性改造，约束其在满足安全要求的前提下最小化侧倾角（以减少控制难度并提升安全性），并将算法生成的飞行轨迹与人类飞行员真实飞行数据进行成本（控制努力）对比分析。

Result: 实验结果显示，算法生成的轨迹在保持安全的同时，相比人类飞行员降低了20%-80%的代价。人类飞行员倾向于先水平接近落地区，再螺旋式快速降低高度；而算法轨迹则以更平滑、渐进的下降方式，在精确高度抵达落地区，提升了整体安全性和可控性。

Conclusion: 计算机生成的飞行轨迹比传统经验法则更安全经济，有望集成到未来的带翼伞训练模拟器中，用于提升新手飞行员的安全性和培训效率。

Abstract: Most skydiving accidents occur during the parafoil-piloting and landing stages and result from human lapses in judgment while piloting the parafoil. Training of novice pilots is protracted due to the lack of functional and easily accessible training simulators. Moreover, work on parafoil trajectory planning suitable for aiding human training remains limited. To bridge this gap, we study the problem of computing safe trajectories for human-piloted parafoil flight and examine how such trajectories fare against human-generated solutions. For the algorithmic part, we adapt the sampling-based motion planner Stable Sparse RRT (SST) by Li et al., to cope with the problem constraints while minimizing the bank angle (control effort) as a proxy for safety. We then compare the computer-generated solutions with data from human-generated parafoil flight, where the algorithm offers a relative cost improvement of 20\%-80\% over the performance of the human pilot. We observe that human pilots tend to, first, close the horizontal distance to the landing area, and then address the vertical gap by spiraling down to the suitable altitude for starting a landing maneuver. The algorithm considered here makes smoother and more gradual descents, arriving at the landing area at the precise altitude necessary for the final approach while maintaining safety constraints. Overall, the study demonstrates the potential of computer-generated guidelines, rather than traditional rules of thumb, which can be integrated into future simulators to train pilots for safer and more cost-effective flights.

</details>


### [152] [LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator](https://arxiv.org/abs/2512.10605)
*Lihuang Chen,Xiangyu Luo,Jun Meng*

Main category: cs.RO

TL;DR: 提出了LEO-RobotAgent框架，使得大语言模型能够以通用、灵活、强健的方式驱动多类型机器人在不同场景下执行复杂任务，提升了人机交互体验。


<details>
  <summary>Details</summary>
Motivation: 现有机器人任务规划多针对单一任务和单一机器人类型，结构复杂且泛化性差，难以满足多样化应用需求。

Method: 设计了一种简洁化、模块化且强泛化能力的机器人智能体框架，基于大语言模型，配备可注册工具集，集成了人机协作机制，从而支持模型自主思考、规划与行动，并能像合作伙伴一样与人协作。

Result: 实验表明，该框架可无缝适配主流无人机、机械臂和轮式机器人等多种平台，并高效完成多种复杂任务。

Conclusion: LEO-RobotAgent框架实现了大模型驱动下机器人任务规划与执行的通用化、人性化和高效化，有效降低人机交互门槛，推动机器人智能体发展。

Abstract: We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.

</details>


### [153] [Evaluating Gemini Robotics Policies in a Veo World Simulator](https://arxiv.org/abs/2512.10675)
*Gemini Robotics Team,Coline Devin,Yilun Du,Debidatta Dwibedi,Ruiqi Gao,Abhishek Jindal,Thomas Kipf,Sean Kirmani,Fangchen Liu,Anirudha Majumdar,Andrew Marmon,Carolina Parada,Yulia Rubanova,Dhruv Shah,Vikas Sindhwani,Jie Tan,Fei Xia,Ted Xiao,Sherry Yang,Wenhao Yu,Allan Zhou*

Main category: cs.RO

TL;DR: 该论文提出了一种基于前沿视频生成模型的机器人策略评估系统，可用于常规与分布外泛化情景下对策略性能和安全性的全面仿真与分析。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型主要用于与训练场景类似的机器人策略评估，缺乏对分布外泛化与安全性多维度评测手段。该工作希望打破此限制，实现更广泛与灵活的策略仿真评估。

Method: 作者基于前沿视频生成基础模型（Veo）搭建生成式评估系统，对机器人动作条件与多视图一致性进行优化，并引入生成式图像编辑和多视图补全技术，能够沿不同泛化方向合成场景变化。

Result: 系统可仿真场景被编辑后（如新增交互物体、背景或干扰物体）对策略行为的影响，并能准确预测不同策略在常规与分布外环境下的相对性能，揭示泛化因素与安全性问题。

Conclusion: 基于视频生成模型的仿真系统具备高保真、多元评估功能，可广泛支持机器人策略在复杂现实及泛化场景下的性能和安全性检验。

Abstract: Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.

</details>


### [154] [How to Brake? Ethical Emergency Braking with Deep Reinforcement Learning](https://arxiv.org/abs/2512.10698)
*Jianbo Wang,Galina Sidorenko,Johan Thunberg*

Main category: cs.RO

TL;DR: 本文提出了一种结合深度强化学习（DRL）与解析法的混合算法，以提升多车紧急制动安全性，在三车队列中实现更有效的碰撞避免与总伤害减缓。


<details>
  <summary>Details</summary>
Motivation: 传统的基于最坏情况的保守控制策略虽然安全，但灵活性低，整体性能受限。随着智能网联汽车的发展，提升多车协同场景下的安全与伤害最小化成为亟需解决的问题，特别是在紧急制动场景下。

Method: 作者提出了一种混合算法，将深度强化学习（DRL）和已有的最优恒定减速度解析优化方法相结合，利用车与车之间的通信（V2V），在多车协作中进行道德性紧急制动决策。该方法既能借助DRL提升灵活性与适应性，又能利用解析解提升可靠性。

Result: 混合算法相比单独DRL方法，在“三车总伤害减缓和碰撞避免”场景下有更好的整体表现和更高的可靠性。实验表明，混合方法能实现整体利益最大化而非仅关注单车。

Conclusion: 将DRL与解析优化方法结合，能够在网联自动驾驶汽车的紧急制动与安全协同问题中，实现更优的性能与可靠性，对未来多车协同安全控制有重要意义。

Abstract: Connected and automated vehicles (CAVs) have the potential to enhance driving safety, for example by enabling safe vehicle following and more efficient traffic scheduling. For such future deployments, safety requirements should be addressed, where the primary such are avoidance of vehicle collisions and substantial mitigating of harm when collisions are unavoidable. However, conservative worst-case-based control strategies come at the price of reduced flexibility and may compromise overall performance. In light of this, we investigate how Deep Reinforcement Learning (DRL) can be leveraged to improve safety in multi-vehicle-following scenarios involving emergency braking. Specifically, we investigate how DRL with vehicle-to-vehicle communication can be used to ethically select an emergency breaking profile in scenarios where overall, or collective, three-vehicle harm reduction or collision avoidance shall be obtained instead of single-vehicle such. As an algorithm, we provide a hybrid approach that combines DRL with a previously published method based on analytical expressions for selecting optimal constant deceleration. By combining DRL with the previous method, the proposed hybrid approach increases the reliability compared to standalone DRL, while achieving superior performance in terms of overall harm reduction and collision avoidance.

</details>


### [155] [On the Stabilization of Rigid Formations on Regular Curves](https://arxiv.org/abs/2512.10700)
*Mohamed Elobaid,Shinkyu Park,Eric Feron*

Main category: cs.RO

TL;DR: 本文提出了一种方法，实现多智能体在任意平面可微曲线上，经过路径遍历后，稳定为等边多边形刚性队形。核心创新点包括多起点随机化牛顿法寻找正多边形内接点，以及连续反馈律保证多智能体间安全避障并最终收敛到目标队形。方法通过数值模拟验证有效性。


<details>
  <summary>Details</summary>
Motivation: 对多智能体协同任务如巡查、环绕、搜救等，常要求智能体在特定复杂路径上保持特定队形（如正多边形），但受曲线形状与避障约束影响，如何全局收敛且避碰地实现该目标，尚有挑战。现有算法多局限于简单路径或无法保障避障。

Method: 1. 利用随机多起点的牛顿类算法，在任意平面可微曲线上寻找中心点处内接正多边形的顶点坐标，保证最优解存在；2. 设计连续状态反馈律，使多智能体先完成路径遍历（sweeping），再收敛到对应多边形顶点，期间满足智能体之间避障约束。

Result: 算法能在不同类型的封闭平面曲线和不同刚性队形需求下，可靠地在数值仿真中完成路径扫掠和多边形队形稳定，所有个体有效避障，实现目标编队。

Conclusion: 本文方法可广泛适用于复杂环境下的多智能体刚性队形保持与路径巡航任务，为实际多智能体编队控制提供了新颖且有效的解决策略。

Abstract: This work deals with the problem of stabilizing a multi-agent rigid formation on a general class of planar curves. Namely, we seek to stabilize an equilateral polygonal formation on closed planar differentiable curves after a path sweep. The task of finding an inscribed regular polygon centered at the point of interest is solved via a randomized multi-start Newton-Like algorithm for which one is able to ascertain the existence of a minimizer. Then we design a continuous feedback law that guarantees convergence to, and sufficient sweeping of the curve, followed by convergence to the desired formation vertices while ensuring inter-agent avoidance. The proposed approach is validated through numerical simulations for different classes of curves and different rigid formations. Code: https://github.com/mebbaid/paper-elobaid-ifacwc-2026

</details>


### [156] [AERMANI-Diffusion: Regime-Conditioned Diffusion for Dynamics Learning in Aerial Manipulators](https://arxiv.org/abs/2512.10773)
*Samaksh Ujjawal,Shivansh Pratap Singh,Naveen Sudheer Nair,Rishabh Dev Yadav,Wei Pan,Spandan Roy*

Main category: cs.RO

TL;DR: 本文提出了一种基于条件扩散模型的无人机操作器残差力建模框架，通过时序编码器有效应对动态不确定性和各种工作状态下的非线性残差。


<details>
  <summary>Details</summary>
Motivation: 无人机操作器由于结构和姿态变化产生剧烈、非线性、随配置变化的惯性耦合与气动力，导致传统解析建模失效，而主流数据驱动方法又难以泛化到多工况多残差的复杂情况。因此需要新的方法提升动力学建模与控制的精度和鲁棒性。

Method: 提出以条件扩散过程为核心的残差力建模方案，并使用轻量化时序编码器提取近期运动和配置信息，将其作为条件输入给扩散模型，捕捉全分布的残差表现。该方法可与自适应控制器结合，进行动力学不确定性补偿。

Result: 实验显示，结合所提建模方法的控制器在真实环境中针对突变工况和未知载重时，运动跟踪精度明显提升。

Conclusion: 本文的方法能有效克服无人机操作器非线性和多变残差下的动力学建模挑战，为复杂环境下的精准控制提供了新思路。

Abstract: Aerial manipulators undergo rapid, configuration-dependent changes in inertial coupling forces and aerodynamic forces, making accurate dynamics modeling a core challenge for reliable control. Analytical models lose fidelity under these nonlinear and nonstationary effects, while standard data-driven methods such as deep neural networks and Gaussian processes cannot represent the diverse residual behaviors that arise across different operating conditions. We propose a regime-conditioned diffusion framework that models the full distribution of residual forces using a conditional diffusion process and a lightweight temporal encoder. The encoder extracts a compact summary of recent motion and configuration, enabling consistent residual predictions even through abrupt transitions or unseen payloads. When combined with an adaptive controller, the framework enables dynamics uncertainty compensation and yields markedly improved tracking accuracy in real-world tests.

</details>


### [157] [Iterative Compositional Data Generation for Robot Control](https://arxiv.org/abs/2512.10891)
*Anh-Quan Pham,Marcel Hussing,Shubhankar P. Patankar,Dani S. Bassett,Jorge Mendez-Mendez,Eric Eaton*

Main category: cs.RO

TL;DR: 该论文提出了一种语义可组合扩散变换器模型，可以有效利用有限的机器人操作数据，通过分解和组合各元素实现对新任务的零样本泛化，并采用自迭代自改进提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 多物体、多机器人、多环境的操作任务组合空间庞大，直接采集示范数据成本极高，现有生成模型缺乏可组合性和泛化能力。为此需要一种新方法能高效利用有限数据，推广到未见新任务。

Method: 提出了一种语义可组合扩散变换器，把机器人操作中的转变因子化为机器人、物体、障碍、目标等组件，并用注意力机制学习组件间的交互关系。训练后模型可对未见任务组合零样本生成高质量转变序列，并引入自我改进流程，即用离线强化学习验证和回收合成数据，迭代提升模型能力。

Result: 相比传统整体式及硬编码组合基线方法，该模型大幅提升对新任务的零样本学习能力，几乎可以解决所有未见任务，并在学习的表示中展现有意义的可组合结构。

Conclusion: 该方法有效挖掘和利用了机器人领域任务的组合结构，在数据有限情况下显著增强新任务的泛化性能，为多任务机器人操作智能带来新进展。

Abstract: Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.

</details>


### [158] [Curriculum-Based Reinforcement Learning for Autonomous UAV Navigation in Unknown Curved Tubular Conduit](https://arxiv.org/abs/2512.10934)
*Zamirddine Mari,Jérôme Pasquet,Julien Seinturier*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的无人机自主导航方法，能够在未知、三维且狭窄的管道中自主飞行，仅依赖激光雷达和视觉条件检测，无需先验几何信息；方法优于依赖管道中心线的传统算法，并可迁移到高保真物理环境中。


<details>
  <summary>Details</summary>
Motivation: 在狭窄管道等受限环境中，无人机因空间、感知受限难以自主导航，传统算法依赖先验几何信息，局限于已知环境，因此亟需无需几何模型、具鲁棒性的新方法。

Method: 利用强化学习（PPO算法）和课程学习策略训练无人机，通过逐步增加环境曲率，提高其针对视觉信息缺失下的适应能力，并融合视觉、记忆和激光雷达信息实现弯道顺利通过。以纯追踪算法为基线，强调信息不对称条件下强化学习优势。

Result: 实验表明，PPO策略能够习得鲁棒且可泛化的导航行为，在几何信息受限情况下显著优于确定性基线算法，并可迁移至高保真三维环境及物理连续动力学下。

Conclusion: 该研究提供了一套适用于未知管道环境的完整自主导航框架，为工业、地下、医疗等狭窄空间自主机器人导航应用开辟了新前景。

Abstract: Autonomous drone navigation in confined tubular environments remains a major challenge due to the constraining geometry of the conduits, the proximity of the walls, and the perceptual limitations inherent to such scenarios. We propose a reinforcement learning approach enabling a drone to navigate unknown three-dimensional tubes without any prior knowledge of their geometry, relying solely on local observations from LiDAR and a conditional visual detection of the tube center. In contrast, the Pure Pursuit algorithm, used as a deterministic baseline, benefits from explicit access to the centerline, creating an information asymmetry designed to assess the ability of RL to compensate for the absence of a geometric model. The agent is trained through a progressive Curriculum Learning strategy that gradually exposes it to increasingly curved geometries, where the tube center frequently disappears from the visual field. A turning-negotiation mechanism, based on the combination of direct visibility, directional memory, and LiDAR symmetry cues, proves essential for ensuring stable navigation under such partial observability conditions. Experiments show that the PPO policy acquires robust and generalizable behavior, consistently outperforming the deterministic controller despite its limited access to geometric information. Validation in a high-fidelity 3D environment further confirms the transferability of the learned behavior to a continuous physical dynamics.
  The proposed approach thus provides a complete framework for autonomous navigation in unknown tubular environments and opens perspectives for industrial, underground, or medical applications where progressing through narrow and weakly perceptive conduits represents a central challenge.

</details>


### [159] [ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning](https://arxiv.org/abs/2512.10946)
*Wendi Chen,Han Xue,Yi Wang,Fangyuan Zhou,Jun Lv,Yang Jin,Shirun Tang,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: 本文提出了ImplicitRDP，一种统一的端到端视觉-力觉扩散策略，实现了复杂操作任务中视觉和力觉信息的高效融合，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在人类级的接触丰富操作中，视觉和力觉扮演着不同但互补的关键角色。由于视觉信息丰富但响应慢、力觉反应快但范围有限，因此如何高效融合这两种模态（尤其是解决频率、信息量的差异）成为难题。

Method: 提出ImplicitRDP，将视觉规划和力觉反馈反应通过单一网络整合。方法包括：1）结构化快慢学习机制（Structural Slow-Fast Learning），利用因果注意力异步处理视觉和力觉信息，实现力觉频率下的闭环调整与动作连贯；2）虚拟目标表征正则化（Virtual-target-based Representation Regularization），将力觉映射到动作空间以提供更强的物理学习信息，避免模态塌缩。

Result: 在多个接触丰富操作实验中，ImplicitRDP在反应速度和成功率上都大幅超越仅用视觉或分层融合的对比方法。同时，训练流程更加高效。

Conclusion: 该方法证实了端到端、高效融合视觉-力觉对复杂操控任务的重要性，方法简洁高效，并具有广阔的应用前景。代码和演示视频即将开源。

Abstract: Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.

</details>
