<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 112]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [The persistence of painting styles](https://arxiv.org/abs/2511.16695)
*Reetikaa Reddy Munnangi,Barbara Giunti*

Main category: cs.CV

TL;DR: 该论文利用拓扑数据分析中的持久同调方法，客观地区分不同艺术家的风格，甚至可以区分真实艺术品与AI生成作品。


<details>
  <summary>Details</summary>
Motivation: 传统的艺术风格识别主要依赖于艺术史学家或评论家的主观经验，缺乏客观、结构化的分析方法。作者希望引入数学工具，为艺术作品的风格识别提供更科学、可解释的手段。

Method: 作者采用了拓扑数据分析中的持久同调（PH）方法，对艺术作品进行分析，并利用统计方法确保区分的准确性。PH能够捕捉作品中的高层次结构特征，以量化不同艺术家甚至AI生成作品间的风格差异。

Result: 实验表明，持久同调方法能够以统计学意义上区分来自不同艺术流派的艺术家、同一流派内的不同艺术家，以及真实艺术品与AI仿制品。

Conclusion: 持久同调为艺术风格分析提供了一种新颖、客观且可解释的工具，可辅助艺术史学家辨别艺术作品，推动了艺术分析的科学化进程。

Abstract: Art is a deeply personal and expressive medium, where each artist brings their own style, technique, and cultural background into their work. Traditionally, identifying artistic styles has been the job of art historians or critics, relying on visual intuition and experience. However, with the advancement of mathematical tools, we can explore art through more structured lens. In this work, we show how persistent homology (PH), a method from topological data analysis, provides objective and interpretable insights on artistic styles. We show how PH can, with statistical certainty, differentiate between artists, both from different artistic currents and from the same one, and distinguish images of an artist from an AI-generated image in the artist's style.

</details>


### [2] [Motion Transfer-Enhanced StyleGAN for Generating Diverse Macaque Facial Expressions](https://arxiv.org/abs/2511.16711)
*Takuya Igaue,Catia Correia-Caeiro,Akito Yoshida,Takako Miyabe-Nishiwaki,Ryusuke Hayashi*

Main category: cs.CV

TL;DR: 本文提出了一种基于StyleGAN2的猕猴面部表情生成方法，通过数据增强、样本选择、损失函数调整，有效提升了猕猴多样面部表情的生成能力。


<details>
  <summary>Details</summary>
Motivation: 生成猕猴等动物面部表情图像面临训练数据有限的问题，特别是个体间表情变化的多样性。为推动神经科学与进化研究，需要能够生成和编辑多样化猕猴表情的AI模型。

Method: 1）利用运动迁移技术，将静态面部图像动画化实现数据增强；2）根据初步训练的StyleGAN2模型的潜在空间，筛选并均衡训练样本；3）改进损失函数以精确还原微小运动如眼睛动作。

Result: 所提方法可为多个猕猴个体生成丰富多样的表情，且表现优于仅用原始静态图像训练的模型。同时，模型能实现基于风格参数的图像编辑，不同参数对应不同面部动作。

Conclusion: 该方法不仅提升了猕猴表情生成的多样性与精细度，还为细粒度分离表情运动和研究猕猴面部表情提供了新工具。

Abstract: Generating animal faces using generative AI techniques is challenging because the available training images are limited both in quantity and variation, particularly for facial expressions across individuals. In this study, we focus on macaque monkeys, widely studied in systems neuroscience and evolutionary research, and propose a method to generate their facial expressions using a style-based generative image model (i.e., StyleGAN2). To address data limitations, we implemented: 1) data augmentation by synthesizing new facial expression images using a motion transfer to animate still images with computer graphics, 2) sample selection based on the latent representation of macaque faces from an initially trained StyleGAN2 model to ensure the variation and uniform sampling in training dataset, and 3) loss function refinement to ensure the accurate reproduction of subtle movements, such as eye movements. Our results demonstrate that the proposed method enables the generation of diverse facial expressions for multiple macaque individuals, outperforming models trained solely on original still images. Additionally, we show that our model is effective for style-based image editing, where specific style parameters correspond to distinct facial movements. These findings underscore the model's potential for disentangling motion components as style parameters, providing a valuable tool for research on macaque facial expressions.

</details>


### [3] [PairHuman: A High-Fidelity Photographic Dataset for Customized Dual-Person Generation](https://arxiv.org/abs/2511.16712)
*Ting Pan,Ye Wang,Peiguang Jing,Rui Ma,Zili Yi,Yu Liu*

Main category: cs.CV

TL;DR: 本文提出了首个大规模双人肖像生成数据集PairHuman，并给出基线模型DHumanDiff，推动了高质量双人个性化肖像定制的研究。


<details>
  <summary>Details</summary>
Motivation: 个性化双人肖像有情感留存和婚纱摄影等诸多应用，但一直缺乏高质量的基准数据集，限制了相关研究和应用的发展。

Method: 构建了PairHuman数据集，包含10万多张多场景、多交互的双人肖像图片及丰富元数据，并提出专为双人肖像生成设计的基线方法DHumanDiff，提升面部一致性和平衡个体定制与场景语义生成。

Result: 实验证明新数据集和方法能够生成符合个性化需求、质量优越的双人肖像图片，视觉效果佳且贴合用户偏好。

Conclusion: PairHuman数据集和DHumanDiff方法为双人个性化肖像生成提供了有力支撑和开放资源，有助于推动该领域研究进展。

Abstract: Personalized dual-person portrait customization has considerable potential applications, such as preserving emotional memories and facilitating wedding photography planning. However, the absence of a benchmark dataset hinders the pursuit of high-quality customization in dual-person portrait generation. In this paper, we propose the PairHuman dataset, which is the first large-scale benchmark dataset specifically designed for generating dual-person portraits that meet high photographic standards. The PairHuman dataset contains more than 100K images that capture a variety of scenes, attire, and dual-person interactions, along with rich metadata, including detailed image descriptions, person localization, human keypoints, and attribute tags. We also introduce DHumanDiff, which is a baseline specifically crafted for dual-person portrait generation that features enhanced facial consistency and simultaneously balances in personalized person generation and semantic-driven scene creation. Finally, the experimental results demonstrate that our dataset and method produce highly customized portraits with superior visual quality that are tailored to human preferences. Our dataset is publicly available at https://github.com/annaoooo/PairHuman.

</details>


### [4] [A Machine Learning-Driven Solution for Denoising Inertial Confinement Fusion Images](https://arxiv.org/abs/2511.16717)
*Asya Y. Akkus,Bradley T. Wolfe,Pinghan Chu,Chengkun Huang,Chris S. Campbell,Mariana Alvarado Alvarez,Petr Volegov,David Fittinghoff,Robert Reinovsky,Zhehui Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合自动编码器和CDF 97小波变换的无监督神经网络，用于去除惯性约束聚变（ICF）中中子成像的混合高斯-泊松噪声，效果优于传统去噪方法。


<details>
  <summary>Details</summary>
Motivation: 中子成像对于分析和优化ICF过程至关重要，但成像数据常受混合噪声（高斯及泊松噪声）影响，导致细节丢失和边缘模糊。传统方法难以有效分离和去除这些重叠噪声，而保留图像细节对于数据分析和实验解释尤为重要。

Method: 本文提出一种无监督自动编码器，将Cohen-Daubechies-Feauveau (CDF 97)小波变换集成于潜在空间，专门针对混合高斯-泊松噪声进行图像去噪。通过合成数据进行训练，并用前向模型生成的数据对网络性能进行测试。

Result: 该网络成功去除了中子成像数据中的噪声，并在重构误差和边缘保持指标上均优于如BM3D等传统滤波去噪方法。

Conclusion: 该方法为ICF中子成像噪声去除和三维重建分析提供了有效工具，相较传统方法有更高的精度与细节保留，对未来ICF实验具有重要意义。

Abstract: Neutron imaging is important in optimizing analysis of inertial confinement fusion (ICF) events such as those at the National Ignition Facility (NIF) and improving current and future ICF platforms. However, images of neutron sources are often degraded by various types of noise. Most commonly, Gaussian and Poisson noise often coexist within one image, obscuring fine details and blurring edges. These noise types often overlap, making them difficult to distinguish and remove using conventional filtering and thresholding methods. As a result, noise removal techniques that preserve image fidelity are important for analyzing and interpreting images of a neutron source. Current solutions include a combination of filtering and thresholding methodologies. In the past, machine learning approaches were rarely implemented due to a lack of ground truth neutron imaging data for ICF processes. However, recent advances in synthetic data production, particularly in the fusion imaging field, have opened opportunities to investigate new denoising procedures using both supervised and unsupervised machine learning methods. In this study, we implement an unsupervised autoencoder with a Cohen-Daubechies- Feauveau (CDF 97) wavelet transform in the latent space for mixed Gaussian-Poisson denoising. The network successfully denoises neutron imaging data. Additionally, it demonstrates lower reconstruction error and superior edge preservation metrics when benchmarked with data generated by a forward model and compared to non-ML-based filtering mechanisms such as Block-matching and 3D filtering (BM3D). This approach presents a promising advancement in neutron image noise reduction and three-dimensional reconstruction analysis of ICF experiments.

</details>


### [5] [SAM 3: Segment Anything with Concepts](https://arxiv.org/abs/2511.16719)
*Nicolas Carion,Laura Gustafson,Yuan-Ting Hu,Shoubhik Debnath,Ronghang Hu,Didac Suris,Chaitanya Ryali,Kalyan Vasudev Alwala,Haitham Khedr,Andrew Huang,Jie Lei,Tengyu Ma,Baishan Guo,Arpit Kalla,Markus Marks,Joseph Greer,Meng Wang,Peize Sun,Roman Rädle,Triantafyllos Afouras,Effrosyni Mavroudi,Katherine Xu,Tsung-Han Wu,Yu Zhou,Liliane Momeni,Rishi Hazra,Shuangrui Ding,Sagar Vaze,Francois Porcher,Feng Li,Siyuan Li,Aishwarya Kamath,Ho Kei Cheng,Piotr Dollár,Nikhila Ravi,Kate Saenko,Pengchuan Zhang,Christoph Feichtenhofer*

Main category: cs.CV

TL;DR: 本文提出了Segment Anything Model (SAM) 3，这是一种基于短语或图像示例的统一检测、分割和跟踪模型，在图像和视频中实现了可提示的概念分割，并显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 当前的分割与检测模型往往难以灵活地基于概念提示（如描述性短语或示例图片）实现多任务识别与分割，且缺乏大规模、高质量、多样化的概念数据集。本文旨在推动基于概念提示的分割技术发展，并解决数据瓶颈。

Method: 作者提出了Promptable Concept Segmentation (PCS)方法，将短语提示或图像示例作为输入，输出匹配对象的分割掩码和唯一身份标识。模型包括共享主干网络的图像级检测器和基于记忆的视频追踪器，通过解耦识别与定位（引入presence head）提升了检测精度，并通过自建的数据引擎构建带400万个独特概念标签的数据集。

Result: SAM 3在图像和视频的PCS任务中，将准确率提升至现有系统的两倍，并超越此前SAM模型在视觉分割任务中的表现。

Conclusion: SAM 3实现了基于提示的通用多目标分割，在概念广度、分割准确度和实际适用性方面都取得显著进展。作者开放了模型代码及新的基准测试集SA-Co，促进领域发展。

Abstract: We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., "yellow school bus"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.

</details>


### [6] [SafeR-CLIP: Mitigating NSFW Content in Vision-Language Models While Preserving Pre-Trained Knowledge](https://arxiv.org/abs/2511.16743)
*Adeel Yousaf,Joseph Fioresi,James Beetham,Amrit Singh Bedi,Mubarak Shah*

Main category: cs.CV

TL;DR: 现有对视觉-语言模型（如CLIP）进行安全性微调的方法会严重损害模型的泛化能力。本文提出SaFeR-CLIP框架，通过最小化对表示空间的干预，有效平衡了安全性与性能。


<details>
  <summary>Details</summary>
Motivation: 提升CLIP等视觉-语言模型的安全性，避免模型输出不当内容，但现有方法往往导致性能大幅下降。该问题主要源于对不安全概念进行僵化对齐，破坏了模型的语义结构，亟需更优的对齐方法。

Method: 提出一种感知语义距离的微调方法，将不安全概念重定向到其最接近的安全替代项，而不是单一预设的安全目标，尽量减少表示空间的变化。基于此原理开发SaFeR-CLIP框架，并构建了NSFW-Caps新基准数据集用于更严格的评测。

Result: SaFeR-CLIP在保持高安全性的同时，零样本分类准确率提升了8.0%，优于以往方法。新提出的NSFW-Caps数据集也为分布外安全评测提供了支持。

Conclusion: 在对预训练模型进行安全性提升时，尊重其原有表示的语义结构几何特性，采用最小干预策略，有助于兼顾模型安全性与性能表现。

Abstract: Improving the safety of vision-language models like CLIP via fine-tuning often comes at a steep price, causing significant drops in their generalization performance. We find this trade-off stems from rigid alignment strategies that force unsafe concepts toward single, predefined safe targets, disrupting the model's learned semantic structure. To address this, we propose a proximity-aware approach: redirecting unsafe concepts to their semantically closest safe alternatives to minimize representational change. We introduce SaFeR-CLIP, a fine-tuning framework that applies this principle of minimal intervention. SaFeR-CLIP successfully reconciles safety and performance, recovering up to 8.0% in zero-shot accuracy over prior methods while maintaining robust safety. To support more rigorous evaluation, we also contribute NSFW-Caps, a new benchmark of 1,000 highly-aligned pairs for testing safety under distributional shift. Our work shows that respecting the geometry of pretrained representations is key to achieving safety without sacrificing performance.

</details>


### [7] [SVG360: Multi-View SVG Generation with Geometric and Color Consistency from a Single SVG](https://arxiv.org/abs/2511.16766)
*Mengnan Jiang,Zhaolin Sun,Christian Franke,Michele Franco Adesso,Antonio Haas,Grace Li Zhang*

Main category: cs.CV

TL;DR: 提出了一种三阶段框架，可以通过单视角SVG输入生成多视角一致的SVG，确保几何形状和颜色的一致性，并减少冗余路径。


<details>
  <summary>Details</summary>
Motivation: 目前基于SVG的多视角生成能力有限，尤其是从单一视角输入生成多视角一致SVG的问题尚未得到很好解决。该研究旨在填补这一空白，以满足现代设计中对可扩展、多视角一致矢量图形的需求。

Method: 提出三阶段方法：首先将输入SVG光栅化后升维为3D表示，并渲染为多视角图像；然后扩展SAM2的时序记忆机制到空间域，引入空间记忆库以保持不同视角间的部件对应关系，实现更一致的路径与色彩而无需重新训练；最后在光栅到矢量转换时进行路径合并和结构优化，减少冗余同时保留轮廓和语义。

Result: 所得多视角SVG在几何和颜色方面表现出高度一致，冗余路径显著减少，同时保留了细致的结构细节。

Conclusion: 本方法有效桥接了生成建模与结构化矢量表示的空白，为基于单输入对象的多视角SVG生成和语义化矢量编辑、资源创建等应用提供了可扩展路径。

Abstract: Scalable Vector Graphics (SVGs) are central to modern design workflows, offering scaling without distortion and precise editability. However, for single object SVGs, generating multi-view consistent SVGs from a single-view input remains underexplored. We present a three stage framework that produces multi-view SVGs with geometric and color consistency from a single SVG input. First, the rasterized input is lifted to a 3D representation and rendered under target camera poses, producing multi-view images of the object. Next, we extend the temporal memory mechanism of Segment Anything 2 (SAM2) to the spatial domain, constructing a spatial memory bank that establishes part level correspondences across neighboring views, yielding cleaner and more consistent vector paths and color assignments without retraining. Finally, during the raster to vector conversion, we perform path consolidation and structural optimization to reduce redundancy while preserving boundaries and semantics. The resulting SVGs exhibit strong geometric and color consistency across views, significantly reduce redundant paths, and retain fine structural details. This work bridges generative modeling and structured vector representation, providing a scalable route to single input, object level multi-view SVG generation and supporting applications such as asset creation and semantic vector editing.

</details>


### [8] [Mesh RAG: Retrieval Augmentation for Autoregressive Mesh Generation](https://arxiv.org/abs/2511.16807)
*Xiatao Sun,Chen Liang,Qian Wang,Daniel Rakita*

Main category: cs.CV

TL;DR: 本文提出了一种全新的Mesh RAG方法，实现了高效且高质量的3D网格生成，并支持增量编辑。


<details>
  <summary>Details</summary>
Motivation: 现有3D网格自动生成方法大多基于自回归模型，这些方法通常依赖更大的模型或更长的生成序列来提升网格质量，导致生成速度缓慢，且其严格的顺序依赖性使得增量编辑变得复杂且低效。为了解决速度与质量之间的矛盾，同时提升灵活性，作者试图打破传统生成模型的顺序瓶颈。

Method: 作者受语言模型中的RAG机制启发，提出Mesh RAG框架。该框架无需训练，对现有自回归网格生成模型可即插即用。方法通过点云分割、空间变换和点云配准，实现对网格部件的检索、生成和集成，打破了传统生成的顺序依赖，可并行化推理。

Result: Mesh RAG在多种基础自回归网格生成模型上的广泛实验表明，该方法显著提高了网格生成质量，加速了生成过程（相较于逐步部分预测），并实现了无需再训练的增量式编辑能力。

Conclusion: Mesh RAG为3D网格自回归生成领域提供了一种高效、灵活、适用性广的方法，兼顾了生成速度、质量和可编辑性，有望大幅提升3D内容生产自动化。

Abstract: 3D meshes are a critical building block for applications ranging from industrial design and gaming to simulation and robotics. Traditionally, meshes are crafted manually by artists, a process that is time-intensive and difficult to scale. To automate and accelerate this asset creation, autoregressive models have emerged as a powerful paradigm for artistic mesh generation. However, current methods to enhance quality typically rely on larger models or longer sequences that result in longer generation time, and their inherent sequential nature imposes a severe quality-speed trade-off. This sequential dependency also significantly complicates incremental editing. To overcome these limitations, we propose Mesh RAG, a novel, training-free, plug-and-play framework for autoregressive mesh generation models. Inspired by RAG for language models, our approach augments the generation process by leveraging point cloud segmentation, spatial transformation, and point cloud registration to retrieve, generate, and integrate mesh components. This retrieval-based approach decouples generation from its strict sequential dependency, facilitating efficient and parallelizable inference. We demonstrate the wide applicability of Mesh RAG across various foundational autoregressive mesh generation models, showing it significantly enhances mesh quality, accelerates generation speed compared to sequential part prediction, and enables incremental editing, all without model retraining.

</details>


### [9] [WorldGen: From Text to Traversable and Interactive 3D Worlds](https://arxiv.org/abs/2511.16825)
*Dilin Wang,Hyunyoung Jung,Tom Monnier,Kihyuk Sohn,Chuhang Zou,Xiaoyu Xiang,Yu-Ying Yeh,Di Liu,Zixuan Huang,Thu Nguyen-Phuoc,Yuchen Fan,Sergiu Oprea,Ziyan Wang,Roman Shapovalov,Nikolaos Sarafianos,Thibault Groueix,Antoine Toisoul,Prithviraj Dhar,Xiao Chu,Minghao Chen,Geon Yeong Park,Mahima Gupta,Yassir Azziz,Rakesh Ranjan,Andrea Vedaldi*

Main category: cs.CV

TL;DR: WorldGen 是一款可以根据文本提示自动生成大型交互式3D世界的系统，从自然语言描述转换为可探索、可编辑的三维环境。


<details>
  <summary>Details</summary>
Motivation: 当前3D世界建模复杂，需要专业技能，难以满足普通创作者和大规模应用的需求。作者希望用AI降低3D场景创作门槛。

Method: WorldGen 利用大语言模型进行场景推理，结合程序化生成、基于扩散的3D生成技术和对象感知场景分解，实现从文本到三维世界的自动转化。其系统模块化，支持布局、尺度、风格等细致控制。

Result: 系统能生成结构合理、视觉丰富、实时渲染高效的大型3D世界，无需手动建模或3D专长。

Conclusion: WorldGen 推动了3D生成式AI应用进步，让3D世界创作更易用，提高了游戏、仿真和沉浸式社交等领域的生产效率和创新能力。

Abstract: We introduce WorldGen, a system that enables the automatic creation of large-scale, interactive 3D worlds directly from text prompts. Our approach transforms natural language descriptions into traversable, fully textured environments that can be immediately explored or edited within standard game engines. By combining LLM-driven scene layout reasoning, procedural generation, diffusion-based 3D generation, and object-aware scene decomposition, WorldGen bridges the gap between creative intent and functional virtual spaces, allowing creators to design coherent, navigable worlds without manual modeling or specialized 3D expertise. The system is fully modular and supports fine-grained control over layout, scale, and style, producing worlds that are geometrically consistent, visually rich, and efficient to render in real time. This work represents a step towards accessible, generative world-building at scale, advancing the frontier of 3D generative AI for applications in gaming, simulation, and immersive social environments.

</details>


### [10] [Towards Unified Vision Language Models for Forest Ecological Analysis in Earth Observation](https://arxiv.org/abs/2511.16853)
*Xizhe Xue,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 该论文提出了REO-Instruct，这是一个专为地球观测（EO）领域设计的同时支持描述性和回归任务的统一基准数据集。它弥补了现有EO数据集仅关注语义理解、缺乏多模态感知与生物物理变量回归结合的问题。基于塞廷纳尔-2和ALOS-2多源遥感影像及结构化文本标注，论文还揭示了现有通用视觉语言模型（VLMs）在数值推理上的不足。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型已在感知与推理方面取得突破，但在地球观测科学回归应用上的潜力未被充分挖掘；现有数据集多为语义描述，缺乏能兼顾科学数值预测的数据基准。

Method: 提出REO-Instruct基准，涵盖森林生态场景下的人类活动、土地覆盖分类、生态斑块计数和地上生物量回归等任务，数据集集成多源遥感影像和结构化文本标注，并通过人类与AI协作生成与验证标注，提供统一的评测协议和基准测试。

Result: 全面评估显示，现有主流VLMs在科学回归与数值推理方面表现有限，未能有效实现定量预测。

Conclusion: REO-Instruct为地球观测领域科学回归和多模态理解的结合提供了标准化基准与新方向，有助于推动能同时描述与推理的下一代地理空间AI模型研发。

Abstract: Recent progress in vision language models (VLMs) has enabled remarkable perception and reasoning capabilities, yet their potential for scientific regression in Earth Observation (EO) remains largely unexplored. Existing EO datasets mainly emphasize semantic understanding tasks such as captioning or classification, lacking benchmarks that align multimodal perception with measurable biophysical variables. To fill this gap, we present REO-Instruct, the first unified benchmark designed for both descriptive and regression tasks in EO. REO-Instruct establishes a cognitively interpretable logic chain in forest ecological scenario (human activity,land-cover classification, ecological patch counting, above-ground biomass (AGB) regression), bridging qualitative understanding and quantitative prediction. The dataset integrates co-registered Sentinel-2 and ALOS-2 imagery with structured textual annotations generated and validated through a hybrid human AI pipeline. Comprehensive evaluation protocols and baseline results across generic VLMs reveal that current models struggle with numeric reasoning, highlighting an essential challenge for scientific VLMs. REO-Instruct offers a standardized foundation for developing and assessing next-generation geospatial models capable of both description and scientific inference. The project page are publicly available at \href{https://github.com/zhu-xlab/REO-Instruct}{REO-Instruct}.

</details>


### [11] [BOP-ASK: Object-Interaction Reasoning for Vision-Language Models](https://arxiv.org/abs/2511.16857)
*Vineet Bhat,Sungsu Kim,Valts Blukis,Greg Heinrich,Prashanth Krishnamurthy,Ramesh Karri,Stan Birchfield,Farshad Khorrami,Jonathan Tremblay*

Main category: cs.CV

TL;DR: 本文提出了BOP-ASK大型数据集，专注于提升视觉语言模型在精细化物体交互推理任务中的能力，并展示模型在精确三维定位、路径规划等方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型虽然能在空间推理基准上取得高分，但仅测试了诸如“左侧”、“后方”等高层关系，未能覆盖实际应用亟需的精细三维空间理解、对象间配合、可操作性及多步推理能力。为填补这一空白，需要新的数据集与评测方法。

Method: 作者开发了BOP-ASK数据集，自动生成包含6D物体位姿和丰富空间注释（如抓取姿态、路径规划、空间深度关系等）的图像及问答对，用以训练和评价模型；并设置核心测试集及分布外测试集以评估泛化能力。涉及六项任务，其中四项为首次提出。对多种视觉语言模型进行评测，并有人工评价验证其表现。

Result: 实验结果显示，在BOP-ASK训练的模型相比基线在物体与抓取姿态估算、路径规划和复杂空间推理等方面均有显著提升，并展现出环境复杂混乱时的细粒度推理能力。

Conclusion: BOP-ASK数据集和生成流程为模型空间推理和物体交互推理能力的研究提供了全新工具，有望推动相关领域进展。数据集和代码将公开发布，促进后续研究及应用。

Abstract: Vision Language Models (VLMs) have achieved impressive performance on spatial reasoning benchmarks, yet these evaluations mask critical weaknesses in understanding object interactions. Current benchmarks test high level relationships ('left of,' 'behind', etc.) but ignore fine-grained spatial understanding needed for real world applications: precise 3D localization, physical compatibility between objects, object affordances and multi step spatial planning. In this work, we present BOP-ASK, a novel large scale dataset for object interaction reasoning for both training and benchmarking. Our data generation pipeline leverages 6D object poses from the Benchmark for Object Pose Estimation (BOP) datasets from which we derive fine grained annotations such as grasp poses, referred object poses, path planning trajectories, relative spatial and depth relationships, and object-to-object relationships. BOP-ASK comprises over 150k images and 33M question answer pairs spanning six tasks (four novel), providing a rich resource for training and evaluating VLMs. We evaluate proprietary and open sourced VLMs, and conduct human evaluations on BOP-ASK-core, a contributed test benchmark. We also release BOP-ASK-lab, an out-of-distribution benchmark with images not sourced from BOP, enabling testing of generalization. Our experiments demonstrate that models trained on BOP-ASK outperform baselines and exhibit emergent capabilities such as precise object and grasp pose estimation, trajectory planning, and fine-grained object-centric spatial reasoning in cluttered environments. We will publicly release our datasets and dataset generation pipeline.

</details>


### [12] [Parts-Mamba: Augmenting Joint Context with Part-Level Scanning for Occluded Human Skeleton](https://arxiv.org/abs/2511.16860)
*Tianyi Shen,Huijuan Xu,Nilesh Ahuja,Omesh Tickoo,Philip Shin,Vijaykrishnan Narayanan*

Main category: cs.CV

TL;DR: 本文提出了一种新模型Parts-Mamba，在人体骨架动作识别中，对缺失或不完整骨架数据具有更强恢复与识别能力，显著提升精度。


<details>
  <summary>Details</summary>
Motivation: 实际应用中因遮挡或传输问题导致骨架数据缺失，现有GCN模型在这种情况下效果较差。本文旨在提升模型应对骨架关键点或帧丢失情况下的动作识别能力。

Method: 提出了一种混合GCN-Mamba模型Parts-Mamba。该模型通过特定部位扫描机制获取局部信息，并通过部位-主体融合模块保留非邻接关节的上下文信息。

Result: 在NTU RGB+D 60和NTU RGB+D 120数据集上，Parts-Mamba模型在各种遮挡设置下准确率提升最多可达12.9%。

Conclusion: Parts-Mamba有效提升了含缺失数据的人体骨架动作识别能力，为复杂应用场景下的动作识别任务提供了更具鲁棒性的解决方案。

Abstract: Skeleton action recognition involves recognizing human action from human skeletons. The use of graph convolutional networks (GCNs) has driven major advances in this recognition task. In real-world scenarios, the captured skeletons are not always perfect or complete because of occlusions of parts of the human body or poor communication quality, leading to missing parts in skeletons or videos with missing frames. In the presence of such non-idealities, existing GCN models perform poorly due to missing local context. To address this limitation, we propose Parts-Mamba, a hybrid GCN-Mamba model designed to enhance the ability to capture and maintain contextual information from distant joints. The proposed Parts-Mamba model effectively captures part-specific information through its parts-specific scanning feature and preserves non-neighboring joint context via a parts-body fusion module. Our proposed model is evaluated on the NTU RGB+D 60 and NTU RGB+D 120 datasets under different occlusion settings, achieving up to 12.9% improvement in accuracy.

</details>


### [13] [The Joint Gromov Wasserstein Objective for Multiple Object Matching](https://arxiv.org/abs/2511.16868)
*Aryan Tajmir Riahi,Khanh Dao Duc*

Main category: cs.CV

TL;DR: 本文提出了联合Gromov-Wasserstein(JGW)距离，实现了多对象间的高效匹配，并在合成与真实数据集上验证了其优越的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统的Gromov-Wasserstein（GW）距离仅能处理单对象间的成对匹配，无法满足需要多对一或多对多匹配的应用场景，如复杂结构比对、形状集合间相似性计算等。该限制影响了其在计算机图形学、生物结构比对等领域的应用。

Method: 作者提出了联合GW（JGW）目标，将原有GW距离框架扩展为支持对象集合间的同时匹配，并给出非负的不相似性度量。方法通过采样点的收敛性证明了部分同构分布的识别效果，还针对点云对象，结合最优传输中的算法（如熵正则化），实现了JGW目标的高效计算。

Result: 与其他部分匹配GW变体的基准测试显示，JGW方法在准确性和计算效率上均优于现有方法。并且实验证明JGW在多形状匹配任务（如几何形状和生物分子复合体）中表现优异，适用于合成和真实数据集。

Conclusion: JGW为多对象匹配任务提供了有效工具，在计算机图形学、结构生物学等不同领域展示了广阔的应用前景，可用于解决复杂的匹配问题。

Abstract: The Gromov-Wasserstein (GW) distance serves as a powerful tool for matching objects in metric spaces. However, its traditional formulation is constrained to pairwise matching between single objects, limiting its utility in scenarios and applications requiring multiple-to-one or multiple-to-multiple object matching. In this paper, we introduce the Joint Gromov-Wasserstein (JGW) objective and extend the original framework of GW to enable simultaneous matching between collections of objects. Our formulation provides a non-negative dissimilarity measure that identifies partially isomorphic distributions of mm-spaces, with point sampling convergence. We also show that the objective can be formulated and solved for point cloud object representations by adapting traditional algorithms in Optimal Transport, including entropic regularization. Our benchmarking with other variants of GW for partial matching indicates superior performance in accuracy and computational efficiency of our method, while experiments on both synthetic and real-world datasets show its effectiveness for multiple shape matching, including geometric shapes and biomolecular complexes, suggesting promising applications for solving complex matching problems across diverse domains, including computer graphics and structural biology.

</details>


### [14] [Align & Invert: Solving Inverse Problems with Diffusion and Flow-based Models via Representational Alignment](https://arxiv.org/abs/2511.16870)
*Loukas Sfountouris,Giannis Daras,Paris Giampouras*

Main category: cs.CV

TL;DR: 提出了一种在生成模型解决逆问题时，利用与自监督视觉编码器表示对齐（REPA）的方法，提升重建质量和效率。


<details>
  <summary>Details</summary>
Motivation: 利用预训练的生成模型已广泛应用于逆问题，但如何在没有真实信号情况下进一步提升模型重建的保真度和感知质量是一个挑战。已有工作表明，将生成模型与自监督编码器的内部表示对齐能有效提高生成模型性能，作者希望将这种方法推广到逆问题领域。

Method: 作者提出在推断过程中，将扩散或流式生成模型与预训练自监督视觉编码器（如DINOv2）进行表示对齐（REPA），即引入正则项使得生成模型内部表征更接近编码器的目标特征，同时理论分析了该正则项与DINOv2嵌入空间内散度的关系，以及REPA如何引导生成模型向更真实图像的表征靠拢。

Result: 在超分辨、遮挡修复、高斯去模糊和运动去模糊等多个逆问题任务上，作者在多种先进算法中融合REPA，实验结果显示该方法能持续提升重建精度和感知真实性，并减少推断离散步数，提高效率。

Conclusion: 通过将生成模型与自监督编码器的表示对齐，能有效提升逆问题下的重建质量与效率，验证了其作为强有力归纳偏置的普适性和有效性。

Abstract: Enforcing alignment between the internal representations of diffusion or flow-based generative models and those of pretrained self-supervised encoders has recently been shown to provide a powerful inductive bias, improving both convergence and sample quality. In this work, we extend this idea to inverse problems, where pretrained generative models are employed as priors. We propose applying representation alignment (REPA) between diffusion or flow-based models and a pretrained self-supervised visual encoder, such as DINOv2, to guide the reconstruction process at inference time. Although ground-truth signals are unavailable in inverse problems, we show that aligning model representations with approximate target features can substantially enhance reconstruction fidelity and perceptual realism. We provide theoretical results showing (a) the relation between the REPA regularization and a divergence measure in the DINOv2 embedding space, and (b) how REPA updates steer the model's internal representations toward those of the clean image. These results offer insights into the role of REPA in improving perceptual fidelity. Finally, we demonstrate the generality of our approach by integrating it into multiple state-of-the-art inverse problem solvers. Extensive experiments on super-resolution, box inpainting, Gaussian deblurring, and motion deblurring confirm that our method consistently improves reconstruction quality across tasks, while also providing substantial efficiency gains by reducing the number of required discretization steps without compromising the performance of the underlying solver.

</details>


### [15] [Glass Surface Detection: Leveraging Reflection Dynamics in Flash/No-flash Imagery](https://arxiv.org/abs/2511.16887)
*Tao Yan,Hao Huang,Yiwei Lu,Zeyu Wang,Ke Xu,Yinghui Wang,Xiaojun Chang,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法NFGlassNet，结合有闪光和无闪光图像，利用玻璃表面反射动态信息，比已有方法更准确地检测玻璃表面。


<details>
  <summary>Details</summary>
Motivation: 传统玻璃表面检测大多依赖边界或反射线索，难以准确检测无明显特征的玻璃玻璃表面，缺乏对玻璃自身特性利用。作者则发现，使用闪光灯时两侧亮度不同会显著影响玻璃表面的反射现象，这为检测玻璃表面提供了新思路。

Method: 作者提出了NFGlassNet方法，包括反射对比挖掘模块（RCMM）和反射引导注意模块（RGAM），分别用于提取反射特征及融合反射与玻璃表面特征。训练时使用了自建的3.3K对闪光/无闪光图像对及相应标注数据集。

Result: 在多个实验及基准测试上，NFGlassNet优于当前最新玻璃检测方法，取得了更高的准确度和检测效果。

Conclusion: 通过结合闪光灯与无闪光图像的反射特性，本方法更有效地检测玻璃表面，为计算机视觉领域的玻璃检测提供了新颖且有效的技术途径。

Abstract: Glass surfaces are ubiquitous in daily life, typically appearing colorless, transparent, and lacking distinctive features. These characteristics make glass surface detection a challenging computer vision task. Existing glass surface detection methods always rely on boundary cues (e.g., window and door frames) or reflection cues to locate glass surfaces, but they fail to fully exploit the intrinsic properties of the glass itself for accurate localization. We observed that in most real-world scenes, the illumination intensity in front of the glass surface differs from that behind it, which results in variations in the reflections visible on the glass surface. Specifically, when standing on the brighter side of the glass and applying a flash towards the darker side, existing reflections on the glass surface tend to disappear. Conversely, while standing on the darker side and applying a flash towards the brighter side, distinct reflections will appear on the glass surface. Based on this phenomenon, we propose NFGlassNet, a novel method for glass surface detection that leverages the reflection dynamics present in flash/no-flash imagery. Specifically, we propose a Reflection Contrast Mining Module (RCMM) for extracting reflections, and a Reflection Guided Attention Module (RGAM) for fusing features from reflection and glass surface for accurate glass surface detection. For learning our network, we also construct a dataset consisting of 3.3K no-flash and flash image pairs captured from various scenes with corresponding ground truth annotations. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods. Our code, model, and dataset will be available upon acceptance of the manuscript.

</details>


### [16] [R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios](https://arxiv.org/abs/2511.16901)
*Lu Zhu,Tiantian Geng,Yangye Chen,Teng Wang,Ping Lu,Feng Zheng*

Main category: cs.CV

TL;DR: 本文提出了首个真实世界下的音视频时空推理数据集R-AVST及基于该数据集的推理模型AVST-Zero，用于提升多模态大模型在复杂场景下的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型虽在视频理解任务取得进展，但主要聚焦在简单视频场景，缺乏对真实世界复杂音视频事件的深入推理能力。为填补这一研究空白，作者创建了专用于音视频时空推理的新数据集。

Method: 1）设计了包含关键对象提取、自动空间标注与人工质检的数据集构建流程，采集了5K+未剪辑视频，标注了27K对象与100类音视频事件；2）定义三类音视频场景时空推理任务，并构建8K+高质量QA样本作为基准测试集合；3）提出基于强化学习、无需中间监督的AVST-Zero模型，通过多维奖励优化直接提升时空推理表现。

Result: 实验证明，R-AVST数据集有效推动了音视频时空推理任务发展，AVST-Zero模型在上述基准测试中表现出有竞争力的结果。

Conclusion: R-AVST是首个专为真实音视频时空推理设计的数据集，并且AVST-Zero为该领域未来的推理模型研究提供了新思路。

Abstract: Recently, rapid advancements have been made in multimodal large language models (MLLMs), especially in video understanding tasks. However, current research focuses on simple video scenarios, failing to reflect the complex and diverse nature of real-world audio-visual events in videos. To bridge this gap, we firstly introduce R-AVST, a dataset for audio-visual reasoning featuring fine-grained spatio-temporal annotations. In constructing this, we design a pipeline consisting of LLM-based key object extraction, automatic spatial annotation and manual quality inspection, resulting in over 5K untrimmed videos with 27K objects across 100 types of audio-visual events. Building on this dataset, we define three core tasks for spatio-temporal reasoning in audio-visual scenes and generate more than 8K high-quality, evenly distributed question-answer pairs to effectively benchmark model performance. To further enhance reasoning, we propose AVST-Zero, a reinforcement learning-based model that avoids intermediate supervision, directly optimizing behavior via carefully designed multi-dimensional rewards. Extensive experiments validate the effectiveness of our R-AVST in advancing audio-visual spatio-temporal reasoning, upon which AVST-Zero demonstrates competitive performance compared to existing models. To the best of our knowledge, R-AVST is the first dataset designed for real-world audio-visual spatio-temporal reasoning, and AVST-Zero offers a novel perspective for tackling future challenges in this domain.

</details>


### [17] [Warm Diffusion: Recipe for Blur-Noise Mixture Diffusion Models](https://arxiv.org/abs/2511.16904)
*Hao-Chien Hsueh,Chi-En Yen,Wen-Hsiao Peng,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 本文提出了Warm Diffusion（温扩散）模型，将图像生成领域两大扩散模型范式（基于噪声的hot diffusion和基于模糊的cold diffusion）有机结合，利用模糊和噪声的混合扩散过程显著提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有的hot diffusion模型只利用噪声，未能充分利用图像高低频之间的强相关性，早期生成阶段表现出随机性；cold diffusion只用模糊，虽然利用了图像结构相关性，但忽略了噪声对数据流形（manifold）的塑造，导致部分生成质量下降。两者各有优缺点，因此作者希望通过结合二者优势提升模型表现。

Method: 提出Blur-Noise Mixture Diffusion Model（BNMD），即Warm Diffusion模型，能联合控制图像的模糊和噪声。用“分而治之”策略将去噪与去模糊过程解耦，提高分数模型（score model）的估计简洁性。同时，他们通过频谱分析研究了模糊和噪声的比例（BNR）如何影响模型动态及数据流形变化。

Result: 通过在多个图像生成基准实验中，作者证明了Warm Diffusion在性能和生成效果上明显优于单纯的hot diffusion或cold diffusion方法。

Conclusion: 联合利用噪声与模糊不仅能充分挖掘图像频谱的信息相关性，还能维护数据流形的合理性，使模型在图像生成任务中获得更好的表现。

Abstract: Diffusion probabilistic models have achieved remarkable success in generative tasks across diverse data types. While recent studies have explored alternative degradation processes beyond Gaussian noise, this paper bridges two key diffusion paradigms: hot diffusion, which relies entirely on noise, and cold diffusion, which uses only blurring without noise. We argue that hot diffusion fails to exploit the strong correlation between high-frequency image detail and low-frequency structures, leading to random behaviors in the early steps of generation. Conversely, while cold diffusion leverages image correlations for prediction, it neglects the role of noise (randomness) in shaping the data manifold, resulting in out-of-manifold issues and partially explaining its performance drop. To integrate both strengths, we propose Warm Diffusion, a unified Blur-Noise Mixture Diffusion Model (BNMD), to control blurring and noise jointly. Our divide-and-conquer strategy exploits the spectral dependency in images, simplifying score model estimation by disentangling the denoising and deblurring processes. We further analyze the Blur-to-Noise Ratio (BNR) using spectral analysis to investigate the trade-off between model learning dynamics and changes in the data manifold. Extensive experiments across benchmarks validate the effectiveness of our approach for image generation.

</details>


### [18] [Q-REAL: Towards Realism and Plausibility Evaluation for AI-Generated Content](https://arxiv.org/abs/2511.16908)
*Shushi Wang,Zicheng Zhang,Chunyi Li,Wei Wang,Liya Ma,Fengjiao Chen,Xiaoyu Li,Xuezhi Cao,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: 该论文提出Q-Real数据集，用于对AI生成图像的真实感与合理性进行细粒度评估，并搭建了相应基准，推动多模态大模型在这一方向上的优化。


<details>
  <summary>Details</summary>
Motivation: 传统AI生成内容的质量评估仅给出单一分数，难以为模型优化提供细致指导，尤其是在真实感与合理性成为生成图像关注重点的背景下，急需更细粒度的评估工具。

Method: 作者构建了Q-Real数据集，包含3088张由主流文本生成图像模型生成的图片。每张图像均标注主要实体位置，并围绕真实感与合理性分别设计判断问题及归因描述。同时，作者提出Q-Real Bench，用于评测多模态大模型在判断与推理定位两项任务上的表现，并基于该数据集设计微调框架，对多个模型开展实验。

Result: 实验结果显示本数据集高质量且重要，提出的评测基准具备良好全面性，微调框架有效提升多模态大模型在细致评估任务上的能力。

Conclusion: Q-Real数据集与基准为AI生成图像质量的细粒度评估提供了有效工具，有助于推动相关生成模型与多模态大模型的进一步优化与发展。

Abstract: Quality assessment of AI-generated content is crucial for evaluating model capability and guiding model optimization. However, most existing quality assessment datasets and models provide only a single quality score, which is too coarse to offer targeted guidance for improving generative models. In current applications of AI-generated images, realism and plausibility are two critical dimensions, and with the emergence of unified generation-understanding models, fine-grained evaluation along these dimensions becomes especially effective for improving generative performance. Therefore, we introduce Q-Real, a novel dataset for fine-grained evaluation of realism and plausibility in AI-generated images. Q-Real consists of 3,088 images generated by popular text-to-image models. For each image, we annotate the locations of major entities and provide a set of judgment questions and attribution descriptions for these along the dimensions of realism and plausibility. Considering that recent advances in multi-modal large language models (MLLMs) enable fine-grained evaluation of AI-generated images, we construct Q-Real Bench to evaluate them on two tasks: judgment and grounding with reasoning. Finally, to enhance MLLM capabilities, we design a fine-tuning framework and conduct experiments on multiple MLLMs using our dataset. Experimental results demonstrate the high quality and significance of our dataset and the comprehensiveness of the benchmark. Dataset and code will be released upon publication.

</details>


### [19] [UniModel: A Visual-Only Framework for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2511.16917)
*Chi Zhang,Jiepeng Wang,Youming Wang,Yuanzhi Liang,Xiaoyan Yang,Zuoxin Li,Haibin Huang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出了UniModel，一种统一的生成模型，在单一的像素到像素扩散框架下同时支持视觉理解和视觉生成，并将文本与图像统一映射到共享的视觉空间。通过像素级变换实现多模态学习，能处理多种视觉-语言任务。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型往往在任务、模型或数据表示上存在分割，难以实现真正的统一，多任务、多模态的高效协同和知识迁移仍是挑战。

Method: 作者提出以视觉为中心，将文本转为画布上的文本图像，实现文本和图像的像素级统一表示。将所有输入输出都视为RGB像素，把如图像理解、文本生成、文本生成图像等任务统一为像素到像素的变换。模型采用Unified Diffusion Transformer，通过像素空间中的扩散过程，联合学习图像与文本图像间的双向映射，并通过任务嵌入控制任务方向。

Result: 在文本到图像生成和图像到文本理解的多项任务上表现出优异的跨模态对齐和可控性，实现了如图像-描述-图像的循环一致性。

Conclusion: 在同一视觉空间内统一模型、任务和表示是一种有前景的多模态智能范式，能够提升多任务处理，加速通用多模态系统的进步。

Abstract: We present UniModel, a unified generative model that jointly supports visual understanding and visual generation within a single pixel-to-pixel diffusion framework. Our goal is to achieve unification along three axes: the model, the tasks, and the representations. At the representation level, we eliminate modality discrepancies by mapping both text and images into a shared visual space: textual prompts are rendered as painted text images on a clean canvas, and all inputs and outputs are treated purely as RGB pixels. This yields a fully vision-native formulation of multimodal learning. At the task level, a broad range of vision-language problems are cast as pixel-to-pixel transformations in this visual space. For understanding tasks, the model takes an RGB image and produces a painted text image that visually encodes the semantic prediction. For generation tasks, painted text images serve as visual conditions that guide realistic and semantically aligned image synthesis. Captioning and text-to-image generation thus become different directions of the same underlying visual translation process. At the model level, we instantiate a single Unified Diffusion Transformer trained with rectified flow in pixel space. A shared backbone jointly learns bidirectional mappings between natural images and painted text images, with lightweight task embeddings to specify the desired direction. Experiments on text-to-image synthesis and image-to-text understanding demonstrate strong cross-modal alignment and emergent controllability such as cycle-consistent image-caption-image loops. Our initial exploration suggests that unifying model, tasks, and representations in a single visual space is a promising paradigm for general-purpose multimodal intelligence.

</details>


### [20] [DeltaDeno: Zero-Shot Anomaly Generation via Delta-Denoising Attribution](https://arxiv.org/abs/2511.16920)
*Chaoran Xu,Chengkan Lv,Qiyu Chen,Yunkang Cao,Feng Zhang,Zhengtao Zhang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种无需真实异常样本或训练即可生成异常的零样本无训练异常生成方法Delta-Deno（Delta-Denoising）。该方法利用扩散模型，通过对比由不同提示词驱动的两条扩散分支，自动定位并编辑图像中的缺陷。实验显示，该方法在异常生成、图像真实感以及下游异常检测任务表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有异常生成方法通常依赖于少量异常样本进行微调，但这与异常本身罕见性矛盾，容易导致过拟合类别先验。实际应用中很多场景没有真实异常样本，亟需一种无需异常样本和训练的异常生成新方法。

Method: 论文提出DeltaDeno方法，即只需极简的提示词对，无需训练，利用双扩散分支对比，逐步统计去噪差值定位异常区域并生成掩膜，随后在扩散后期利用掩膜控制潜空间修补生成局部异常。为提升稳定性与可控性，设计了Token级提示词强化和空间注意力偏置机制。

Result: DeltaDeno在公开数据集上进行了实验，显示在异常生成质量、真实感与下游异常检测任务上均取得了一致且显著的性能提升。

Conclusion: DeltaDeno是一种高效、灵活的零样本异常生成方法，能在缺乏异常样本和训练的情况下生成高质量的局部异常，有助于提升下游异常检测效果，对实际工业和安全应用具有重要价值。

Abstract: Anomaly generation is often framed as few-shot fine-tuning with anomalous samples, which contradicts the scarcity that motivates generation and tends to overfit category priors. We tackle the setting where no real anomaly samples or training are available. We propose Delta-Denoising (DeltaDeno), a training-free zero-shot anomaly generation method that localizes and edits defects by contrasting two diffusion branches driven by a minimal prompt pair under a shared schedule. By accumulating per-step denoising deltas into an image-specific localization map, we obtain a mask to guide the latent inpainting during later diffusion steps and preserve the surrounding context while generating realistic local defects. To improve stability and control, DeltaDeno performs token-level prompt refinement that aligns shared content and strengthens anomaly tokens, and applies a spatial attention bias restricted to anomaly tokens in the predicted region. Experiments on public datasets show that DeltaDeno achieves great generation, realism and consistent gains in downstream detection performance. Code will be made publicly available.

</details>


### [21] [Rethinking Diffusion Model-Based Video Super-Resolution: Leveraging Dense Guidance from Aligned Features](https://arxiv.org/abs/2511.16928)
*Jingyi Xu,Meisong Zheng,Ying Chen,Minglang Qiao,Xin Deng,Mai Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的扩散模型（DM）用于视频超分辨率（VSR），通过在特征域对齐与补偿提升了感知质量、保真度和时间一致性，超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的视频超分辨率方法在感知质量上有优势，但常因帧间对齐与补偿不准确，导致误差累积和空间伪影，还存在感知质量与保真度难以兼得的问题。需要重新审视对齐与补偿的实现方式。

Method: 提出了DGAF-VSR新方法，核心包括：1）在特征域而非像素域进行信息补偿，2）结合光学引导变形模块（OGWM）实现高分辨率下的特征对齐以保持高频信息，3）特征时序条件模块（FTCM）在特征域内进行密集引导。

Result: 在合成与真实数据集上，DGAF-VSR在主要指标均超越SOTA：感知质量提升（DISTS减少35.82%），保真度提升（PSNR提升0.20 dB），时序一致性提升（tLPIPS降低30.37%）。

Conclusion: 论文证明了基于特征域的密集引导和高分辨率特征对齐显著提高了VSR的整体性能，为基于扩散模型的视频超分辨率提供了新方向。

Abstract: Diffusion model (DM) based Video Super-Resolution (VSR) approaches achieve impressive perceptual quality. However, they suffer from error accumulation, spatial artifacts, and a trade-off between perceptual quality and fidelity, primarily caused by inaccurate alignment and insufficient compensation between video frames. In this paper, within the DM-based VSR pipeline, we revisit the role of alignment and compensation between adjacent video frames and reveal two crucial observations: (a) the feature domain is better suited than the pixel domain for information compensation due to its stronger spatial and temporal correlations, and (b) warping at an upscaled resolution better preserves high-frequency information, but this benefit is not necessarily monotonic. Therefore, we propose a novel Densely Guided diffusion model with Aligned Features for Video Super-Resolution (DGAF-VSR), with an Optical Guided Warping Module (OGWM) to maintain high-frequency details in the aligned features and a Feature-wise Temporal Condition Module (FTCM) to deliver dense guidance in the feature domain. Extensive experiments on synthetic and real-world datasets demonstrate that DGAF-VSR surpasses state-of-the-art methods in key aspects of VSR, including perceptual quality (35.82\% DISTS reduction), fidelity (0.20 dB PSNR gain), and temporal consistency (30.37\% tLPIPS reduction).

</details>


### [22] [Shape-preserving Tooth Segmentation from CBCT Images Using Deep Learning with Semantic and Shape Awareness](https://arxiv.org/abs/2511.16936)
*Zongrui Ji,Zhiming Cui,Na Li,Qianhan Zheng,Miaojing Shi,Ke Deng,Jingyang Zhang,Chaoyuan Li,Xuepeng Chen,Yi Dong,Lei Ma*

Main category: cs.CV

TL;DR: 本文提出了一种结合语义与形状感知的深度学习方法，实现对CBCT牙齿图像的高精度分割，有效缓解了因牙齿粘连导致的解剖结构形变问题，明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: CBCT牙齿图像分割因牙间黏连和形态失真而较难，现有方法难以实现形状保真细致的分割。因此亟需一种新方法提升分割精度与形态还原能力。

Method: 提出了结合语义和形状感知的深度学习分割框架，包括：1）基于目标牙质心提示的多标签学习建模牙齿间的语义关系，降低形状歧义；2）引入牙齿形状感知机制，加强形态约束，保证边界完整性。整个框架通过多任务学习联合优化分割和形态保持。

Result: 在内部与外部数据集上的实验结果显示，所提方法在分割准确率和形状保真性上均显著优于其他现有技术。

Conclusion: 该方法可有效缓解牙齿形态失真问题，显著提升解剖边界的精确性，实现高质量的牙齿分割，为数字口腔医学提供了更可靠的技术支持。

Abstract: Background:Accurate tooth segmentation from cone beam computed tomography (CBCT) images is crucial for digital dentistry but remains challenging in cases of interdental adhesions, which cause severe anatomical shape distortion.
  Methods:
  To address this, we propose a deep learning framework that integrates semantic and shape awareness for shape-preserving segmentation. Our method introduces a target-tooth-centroid prompted multi-label learning strategy to model semantic relationships between teeth, reducing shape ambiguity. Additionally, a tooth-shape-aware learning mechanism explicitly enforces morphological constraints to preserve boundary integrity. These components are unified via multi-task learning, jointly optimizing segmentation and shape preservation.
  Results: Extensive evaluations on internal and external datasets demonstrate that our approach significantly outperforms existing methods.
  Conclusions: Our approach effectively mitigates shape distortions and providing anatomically faithful tooth boundaries.

</details>


### [23] [OmniGround: A Comprehensive Spatio-Temporal Grounding Benchmark for Real-World Complex Scenarios](https://arxiv.org/abs/2511.16937)
*Hong Gao,Jingyu Wu,Xiangkai Xu,Kangni Xie,Yunchen Zhang,Bin Zhong,Xurui Gao,Min-Ling Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一个更全面的Spatio-Temporal Video Grounding（STVG）基准数据集OmniGround，并引入了新框架PG-TAF，极大提升了复杂场景下的多模态视频定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有的STVG模型在多样物体、复杂查询以及复杂场景下表现不佳，主要原因在于基准数据集类别有限、推理过于简单以及语言鲁棒性差。

Method: 1）提出OmniGround数据集，覆盖81个类别、3475个视频及更复杂自然语言查询，并开发了前向-后向-精细修正标注流程，以增强标注质量。2）设计DeepSTG评测框架，从四个维度全面量化数据集质量。3）提出PG-TAF无训练两阶段方法，将STVG分解为高级时序定位和精细空间-时序传播两步。

Result: 在OmniGround等复杂场景下，平均精度下降10.4%，尤其是在处理小物体/遮挡及复杂空间关系时表现差。所提PG-TAF框架在OmniGround上m_tIoU和m_vIoU分别提升25.6%和35.6%，在四个基准上都取得一致改进。

Conclusion: 提升STVG能力需更丰富、更现实的数据和创新分步推理框架，OmniGround与PG-TAF能有效缩小多模态视频分析与真实需求的差距。

Abstract: Spatio-Temporal Video Grounding (STVG) aims to localize target objects in videos based on natural language descriptions. Despite recent advances in Multimodal Large Language Models, a significant gap remains between current models and real-world demands involving diverse objects and complex queries. We attribute this to limited benchmark scope, causing models to exhibit category bias, oversimplified reasoning, and poor linguistic robustness. To address these limitations, we introduce OmniGround, a comprehensive benchmark with 3,475 videos spanning 81 categories and complex real-world queries. We propose the Forward-Backward-Refinement annotation pipeline that combines multi-directional tracking with intelligent error correction for high-quality labels. We further introduce DeepSTG, a systematic evaluation framework quantifying dataset quality across four complementary dimensions beyond superficial statistics. Evaluations reveal performance average drop of 10.4% on complex real-world scenes, particularly with small/occluded objects and intricate spatial relations. Motivated by these, we propose PG-TAF, a training-free two-stage framework decomposing STVG into high-level temporal grounding and fine-grained spatio-temporal propagation. Experiments demonstrate PG-TAF achieves 25.6% and 35.6% improvements in m\_tIoU and m\_vIoU on OmniGround with consistent gains across four benchmarks.

</details>


### [24] [MultiPriv: Benchmarking Individual-Level Privacy Reasoning in Vision-Language Models](https://arxiv.org/abs/2511.16940)
*Xiongtao Sun,Hui Li,Jiaming Zhang,Yujie Yang,Kaili Liu,Ruxin Feng,Wen Jun Tan,Wei Yang Bryan Lim*

Main category: cs.CV

TL;DR: 提出了MultiPriv，一个评估视觉-语言模型（VLM）在隐私推理能力上的新基准，揭示了现有VLM在隐私推理层面存在严重未被测量的风险。


<details>
  <summary>Details</summary>
Motivation: 现有的隐私评测方法只关注模型能否识别特定的隐私属性，未能评估模型通过推理和信息整合，对个人隐私造成威胁的能力。新威胁体现在模型能否将分布式信息拼接推断出个人身份或敏感信息。因此迫切需要新的基准来评估VLM的隐私推理风险。

Method: 提出了MultiPriv基准，引入了隐私感知与推理（PPR）框架，并构建了一个双语、多模态的数据集。该数据集设置了合成个人档案，将身份信息和敏感属性精细关联，包含九项任务，从属性检测到跨图像身份重识别和链式推理，系统性评估VLM的隐私推理能力。

Result: 对50余种基础和商用VLM进行了大规模评测，发现：（1）许多VLM具有显著且未被衡量的推理型隐私风险；（2）现有主要的感知层面指标并不能准确预测这些推理类风险；（3）现有安全对齐方法在应对此类隐私推理攻击时表现不佳且不一致。

Conclusion: MultiPriv揭示了VLM在隐私推理层面的系统性脆弱性，并为开发更稳健、更好保护个人隐私的VLM提供了必要的评测框架。

Abstract: Modern Vision-Language Models (VLMs) demonstrate sophisticated reasoning, escalating privacy risks beyond simple attribute perception to individual-level linkage. Current privacy benchmarks are structurally insufficient for this new threat, as they primarily evaluate privacy perception while failing to address the more critical risk of privacy reasoning: a VLM's ability to infer and link distributed information to construct individual profiles. To address this critical gap, we propose \textbf{MultiPriv}, the first benchmark designed to systematically evaluate individual-level privacy reasoning in VLMs. We introduce the \textbf{Privacy Perception and Reasoning (PPR)} framework and construct a novel, bilingual multimodal dataset to support it. The dataset uniquely features a core component of synthetic individual profiles where identifiers (e.g., faces, names) are meticulously linked to sensitive attributes. This design enables nine challenging tasks evaluating the full PPR spectrum, from attribute detection to cross-image re-identification and chained inference. We conduct a large-scale evaluation of over 50 foundational and commercial VLMs. Our analysis reveals: (1) Many VLMs possess significant, unmeasured reasoning-based privacy risks. (2) Perception-level metrics are poor predictors of these reasoning risks, revealing a critical evaluation gap. (3) Existing safety alignments are inconsistent and ineffective against such reasoning-based attacks. MultiPriv exposes systemic vulnerabilities and provides the necessary framework for developing robust, privacy-preserving VLMs.

</details>


### [25] [Flow-Guided Implicit Neural Representation for Motion-Aware Dynamic MRI Reconstruction](https://arxiv.org/abs/2511.16948)
*Baoqing Li,Yuanyuan Liu,Congcong Liu,Qingyong Zhu,Jing Cheng,Yihang Zhou,Hao Chen,Zhuo-Xu Cui,Dong Liang*

Main category: cs.CV

TL;DR: 该论文提出了一个全新的隐式神经表示(INR)框架，实现了动态图像及运动场的联合建模，无需事先估计光流即可提升动态MRI重建质量和运动估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 动态磁共振成像由于采样受限和运动伪影，重建质量常受影响。传统方法依赖于提前估计的光流，但光流在欠采样下常常不准确，从而影响重建效果，因此需要新的方法提升重建和运动估计的鲁棒性和准确性。

Method: 作者提出采用两个INR分别表征空间-时间图像内容和光流场，并通过光流方程实现二者耦合。联合优化过程中引入物理约束（光流方程）及数据一致性损失，直接从k-space数据学习动态图像及运动信息，无需先验光流估计。

Result: 在心脏动态MRI公开数据集上的实验表明，该方法在重建质量、运动估计精度与时间一致性方面均优于现有运动补偿和深度学习方法。

Conclusion: 论文证明了隐式神经网络联合建模及流正则化约束在动态MRI重建中的有效性，为该领域提供了新的解决思路和性能提升。

Abstract: Dynamic magnetic resonance imaging (dMRI) captures temporally-resolved anatomy but is often challenged by limited sampling and motion-induced artifacts. Conventional motion-compensated reconstructions typically rely on pre-estimated optical flow, which is inaccurate under undersampling and degrades reconstruction quality. In this work, we propose a novel implicit neural representation (INR) framework that jointly models both the dynamic image sequence and its underlying motion field. Specifically, one INR is employed to parameterize the spatiotemporal image content, while another INR represents the optical flow. The two are coupled via the optical flow equation, which serves as a physics-inspired regularization, in addition to a data consistency loss that enforces agreement with k-space measurements. This joint optimization enables simultaneous recovery of temporally coherent images and motion fields without requiring prior flow estimation. Experiments on dynamic cardiac MRI datasets demonstrate that the proposed method outperforms state-of-the-art motion-compensated and deep learning approaches, achieving superior reconstruction quality, accurate motion estimation, and improved temporal fidelity. These results highlight the potential of implicit joint modeling with flow-regularized constraints for advancing dMRI reconstruction.

</details>


### [26] [FingerCap: Fine-grained Finger-level Hand Motion Captioning](https://arxiv.org/abs/2511.16951)
*Xin Shen,Rui Zhu,Lei Shen,Xinyu Wang,Kaihao Zhang,Tianqing Zhu,Shuchen Wu,Chenxi Miao,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang,Xin Yu*

Main category: cs.CV

TL;DR: 本文提出了FingerCap任务，即对人手细粒度动作生成文字描述，并引入了FingerCap-40K数据集和FiGOP方法，显著提升了手指级动作识别与描述的性能。


<details>
  <summary>Details</summary>
Motivation: 精细的人手动作理解对视觉识别、智能交互和多模态通信等领域非常重要。然而，目前主流的视频多模态大模型难以捕捉手指级别的细微动作动态。现有采样方法（如稀疏RGB帧）存在时序稀疏性，难以描述和理解高频的指间动作变化。

Method: 1. 构建了FingerCap-40K大规模数据集，包含4万对手部动作视频与精细手指动作文本描述。2. 设计了HandJudge基于大模型的自动化评价指标，能细致考察描述的手指级正确性和动态完整性。3. 提出FiGOP方法，在每个关键RGB帧间加入关键点序列，利用轻量级编码器将其转成运动特征，与视觉特征融合，从而补足关键动作时序信息。

Result: 基于FingerCap-40K上的实验结果表明，当前强大的开源和闭源视频多模态模型在手指级动作理解上仍表现不佳，而引入FiGOP方案后，无论自动指标HandJudge还是人工评测均有显著提升。

Conclusion: 本文首次系统解决了手指细粒度动作描述的问题，提供了大规模数据集、新的评价指标和有效的建模方法，为未来精准理解和生成人手动作文本提供了基础。

Abstract: Understanding fine-grained human hand motion is fundamental to visual perception, embodied intelligence, and multimodal communication. In this work, we propose Fine-grained Finger-level Hand Motion Captioning (FingerCap), which aims to generate textual descriptions that capture detailed finger-level semantics of hand actions. To support this task, we curate FingerCap-40K, a large-scale corpus of 40K paired hand-motion videos and captions spanning two complementary sources: concise instruction-style finger motions and diverse, naturalistic hand-object interactions. To enable effective evaluation, we employ HandJudge, a LLM-based rubric that measures finger-level correctness and motion completeness. Temporal sparsity remains a fundamental bottleneck for current Video-MLLMs, since sparse RGB sampling is insufficient to capture the subtle, high-frequency dynamics underlying fine finger motions. As a simple and compute-friendly remedy, we introduce FiGOP (Finger Group-of-Pictures), which pairs each RGB keyframe with subsequent hand keypoints until the next keyframe. A lightweight temporal encoder converts the keypoints into motion embeddings and integrates them with RGB features. FiGOP adapts the classic GOP concept to finger motion, recovering fine temporal cues without increasing RGB density. Experiments on FingerCap-40K show that strong open- and closed-source Video-MLLMs still struggle with finger-level reasoning, while our FiGOP-augmented model yield consistent gains under HandJudge and human studies.

</details>


### [27] [Point-Supervised Facial Expression Spotting with Gaussian-Based Instance-Adaptive Intensity Modeling](https://arxiv.org/abs/2511.16952)
*Yicheng Deng,Hideaki Hayashi,Hajime Nagahara*

Main category: cs.CV

TL;DR: 本文提出了一种点监督面部表情定位方法，仅使用每个实例一个时间戳注释，降低标注成本，并提出了两分支框架和高斯建模实现高效表情检测与分类。


<details>
  <summary>Details</summary>
Motivation: 现有面部表情定位方法依赖精细的时序边界标注，标注成本高且费时。为降低数据标注难度和成本，作者探讨了仅需一个时间戳注释的新型点监督方式。

Method: 提出一种独特的两分支网络：（1）设计高斯建模（GIM）模块，以实例级表情强度分布进行软伪标签生成，改进传统的硬伪标签方法，提升强度监督的可靠性；（2）设计类别感知顶点分类分支，利用伪顶点帧区分宏/微表情。训练期结合强度感知对比损失，提升表达与中性帧区分能力。

Result: 在SAMM-LV、CAS(ME)^2与CAS(ME)^3等主流数据集上进行了大量实验，验证了所提点监督方法及整体框架在表情定位及微/宏表情分类任务中的有效性。

Conclusion: 提出的点监督方法可显著降低标注难度和成本，创新的高斯建模与两分支设计在主流数据集上效果优越，为自动面部表情分析提供了更高效、实用的解决方案。

Abstract: Automatic facial expression spotting, which aims to identify facial expression instances in untrimmed videos, is crucial for facial expression analysis. Existing methods primarily focus on fully-supervised learning and rely on costly, time-consuming temporal boundary annotations. In this paper, we investigate point-supervised facial expression spotting (P-FES), where only a single timestamp annotation per instance is required for training. We propose a unique two-branch framework for P-FES. First, to mitigate the limitation of hard pseudo-labeling, which often confuses neutral and expression frames with various intensities, we propose a Gaussian-based instance-adaptive intensity modeling (GIM) module to model instance-level expression intensity distribution for soft pseudo-labeling. By detecting the pseudo-apex frame around each point label, estimating the duration, and constructing an instance-level Gaussian distribution, GIM assigns soft pseudo-labels to expression frames for more reliable intensity supervision. The GIM module is incorporated into our framework to optimize the class-agnostic expression intensity branch. Second, we design a class-aware apex classification branch that distinguishes macro- and micro-expressions solely based on their pseudo-apex frames. During inference, the two branches work independently: the class-agnostic expression intensity branch generates expression proposals, while the class-aware apex-classification branch is responsible for macro- and micro-expression classification.Furthermore, we introduce an intensity-aware contrastive loss to enhance discriminative feature learning and suppress neutral noise by contrasting neutral frames with expression frames with various intensities. Extensive experiments on the SAMM-LV, CAS(ME)$^2$, and CAS(ME)$^3$ datasets demonstrate the effectiveness of our proposed framework.

</details>


### [28] [Neighbor GRPO: Contrastive ODE Policy Optimization Aligns Flow Models](https://arxiv.org/abs/2511.16955)
*Dailan He,Guanlin Feng,Xingtong Ge,Yazhe Niu,Yi Zhang,Bingqi Ma,Guanglu Song,Yu Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 该论文提出了一种新的人类偏好对齐算法Neighbor GRPO，能高效提升生成模型的表现，尤其在不借助SDE的前提下直接适用于确定性ODE采样的流匹配模型。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO在应用于图像、视频生成模型时需引入随机性（SDE），但SDE方案会带来不高效的梯度归因与高阶解算器不兼容等问题，限制了其性能与应用范围。

Method: 论文首先以距离优化视角重新解释了SDE-GRPO为对比学习，通过扰动初始噪声直接在ODE轨迹空间生成多样候选，无需引入SDE。结合softmax距离代理leaping policy、对称锚点采样与分组准范重加权等技术，实现对齐、提升效率与奖励分布适应。

Result: 实验结果表明，Neighbor GRPO在训练成本、收敛速度与生成质量方面都显著优于SDE基础方法。

Conclusion: Neighbor GRPO实现了无需SDE、与高阶ODE求解器兼容的高效泛化人类偏好对齐，推动相关生成模型更高质量发展。

Abstract: Group Relative Policy Optimization (GRPO) has shown promise in aligning image and video generative models with human preferences. However, applying it to modern flow matching models is challenging because of its deterministic sampling paradigm. Current methods address this issue by converting Ordinary Differential Equations (ODEs) to Stochastic Differential Equations (SDEs), which introduce stochasticity. However, this SDE-based GRPO suffers from issues of inefficient credit assignment and incompatibility with high-order solvers for fewer-step sampling. In this paper, we first reinterpret existing SDE-based GRPO methods from a distance optimization perspective, revealing their underlying mechanism as a form of contrastive learning. Based on this insight, we propose Neighbor GRPO, a novel alignment algorithm that completely bypasses the need for SDEs. Neighbor GRPO generates a diverse set of candidate trajectories by perturbing the initial noise conditions of the ODE and optimizes the model using a softmax distance-based surrogate leaping policy. We establish a theoretical connection between this distance-based objective and policy gradient optimization, rigorously integrating our approach into the GRPO framework. Our method fully preserves the advantages of deterministic ODE sampling, including efficiency and compatibility with high-order solvers. We further introduce symmetric anchor sampling for computational efficiency and group-wise quasi-norm reweighting to address reward flattening. Extensive experiments demonstrate that Neighbor GRPO significantly outperforms SDE-based counterparts in terms of training cost, convergence speed, and generation quality.

</details>


### [29] [MatPedia: A Universal Generative Foundation for High-Fidelity Material Synthesis](https://arxiv.org/abs/2511.16957)
*Di Luo,Shuhui Yang,Mingxin Yang,Jiawei Lu,Yixuan Tang,Xintong Han,Zhuo Chen,Beibei Wang,Chunchao Guo*

Main category: cs.CV

TL;DR: MatPedia提出了一种新型联合RGB-PBR材料表征方法，实现了多个材料生成任务的统一处理，并在质量和多样性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前物理基础渲染（PBR）材料的创建过程繁琐且需要专业知识；现有生成方法无法统一地表征自然图片外观和PBR属性，导致任务分散且无法利用大规模RGB图像数据。

Method: 提出MatPedia，一个基于新颖的联合RGB-PBR材料表示的基础模型，将材料编码为两个互相关联的潜变量（一个代表RGB外观，一个代表PBR四个物理属性图），将其看作5帧序列，结合视频扩散模型架构来共同建模表观和PBR属性之间的关系。训练数据集MatHybrid-410K结合了PBR数据集与大规模RGB图像。

Result: MatPedia能原生生成1024×1024分辨率的材料，在材料生成质量和多样性上显著优于现有方法，支持文本生成材料、图像生成材料及内在分解等多任务统一处理。

Conclusion: 联合RGB-PBR表示和视频扩散架构实现了材料表征与生成任务的统一，极大提升了材料合成技术的效率与效果。

Abstract: Physically-based rendering (PBR) materials are fundamental to photorealistic graphics, yet their creation remains labor-intensive and requires specialized expertise. While generative models have advanced material synthesis, existing methods lack a unified representation bridging natural image appearance and PBR properties, leading to fragmented task-specific pipelines and inability to leverage large-scale RGB image data. We present MatPedia, a foundation model built upon a novel joint RGB-PBR representation that compactly encodes materials into two interdependent latents: one for RGB appearance and one for the four PBR maps encoding complementary physical properties. By formulating them as a 5-frame sequence and employing video diffusion architectures, MatPedia naturally captures their correlations while transferring visual priors from RGB generation models. This joint representation enables a unified framework handling multiple material tasks--text-to-material generation, image-to-material generation, and intrinsic decomposition--within a single architecture. Trained on MatHybrid-410K, a mixed corpus combining PBR datasets with large-scale RGB images, MatPedia achieves native $1024\times1024$ synthesis that substantially surpasses existing approaches in both quality and diversity.

</details>


### [30] [SING3R-SLAM: Submap-based Indoor Monocular Gaussian SLAM with 3D Reconstruction Priors](https://arxiv.org/abs/2511.17207)
*Kunyi Li,Michael Niemeyer,Sen Wang,Stefano Gasperini,Nassir Navab,Federico Tombari*

Main category: cs.CV

TL;DR: SING3R-SLAM是一种结合高效稠密三维重建和全局一致性、高紧凑性的RGB稠密SLAM框架，显著提升了跟踪、重建与新视角渲染的效果。


<details>
  <summary>Details</summary>
Motivation: 现有稠密三维重建技术虽然可精确捕捉局部几何，但融入SLAM时，会出现漂移、冗余点云等问题，影响效率与下游任务（如新视角合成）。

Method: SING3R-SLAM使用轻量级跟踪与重建模块，先生成局部一致子图，再将它们逐步对齐融合成全局高斯表示地图。该地图统一优化场景几何和相机位姿，保证跨视角的几何一致性，并可反向提升局部跟踪的鲁棒性。

Result: 在真实数据集上，SING3R-SLAM在跟踪、三维重建和新视角渲染任务上表现领先，同类跟踪精度提升超12%，场景几何更细致，且全局表示更紧凑高效。

Conclusion: SING3R-SLAM有效解决了稠密三维重建与SLAM结合时的漂移与冗余问题，提升了多项任务的表现，是高效、强鲁棒性的稠密SLAM新方案。

Abstract: Recent advances in dense 3D reconstruction enable the accurate capture of local geometry; however, integrating them into SLAM is challenging due to drift and redundant point maps, which limit efficiency and downstream tasks, such as novel view synthesis. To address these issues, we propose SING3R-SLAM, a globally consistent and compact Gaussian-based dense RGB SLAM framework. The key idea is to combine locally consistent 3D reconstructions with a unified global Gaussian representation that jointly refines scene geometry and camera poses, enabling efficient and versatile 3D mapping for multiple downstream applications. SING3R-SLAM first builds locally consistent submaps through our lightweight tracking and reconstruction module, and then progressively aligns and fuses them into a global Gaussian map that enforces cross-view geometric consistency. This global map, in turn, provides feedback to correct local drift and enhance the robustness of tracking. Extensive experiments demonstrate that SING3R-SLAM achieves state-of-the-art tracking, 3D reconstruction, and novel view rendering, resulting in over 12% improvement in tracking and producing finer, more detailed geometry, all while maintaining a compact and memory-efficient global representation on real-world datasets.

</details>


### [31] [Two Heads Better than One: Dual Degradation Representation for Blind Super-Resolution](https://arxiv.org/abs/2511.16963)
*Hsuan Yuan,Shao-Yu Weng,I-Hsuan Lo,Wei-Chen Chiu,Yu-Syuan Xu,Hao-Chien Hsueh,Jen-Hui Chuang,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的双分支退化提取网络，有效解决了现实中复杂退化下的单图像超分辨率问题，并在多个基准上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 以往单图像超分辨率方法大多假设固定退化（如双三次下采样），但实际应用中退化模型常常未知或包含噪声，导致这些方法效果大幅下降，因此亟需一种能够处理未知、复杂退化情况的超分辨率方法。

Method: 作者提出了Dual Branch Degradation Extractor Network，通过两个无监督分支分别提取模糊和噪声的退化特征嵌入，并分别指导SR网络的处理。同时，将退化提取器作为正则化项，利用SR与HR图像的区别优化生成效果。

Result: 在多个评测基准上，作者方法在盲超分辨率任务中实现了当前最优（SOTA）的重建性能，优于此前主流方法。

Conclusion: 该方法能够有效表达和适应图像模糊与噪声的多样性，显著提升盲超分辨率性能，对实际复杂退化有很强泛化能力。

Abstract: Previous methods have demonstrated remarkable performance in single image super-resolution (SISR) tasks with known and fixed degradation (e.g., bicubic downsampling). However, when the actual degradation deviates from these assumptions, these methods may experience significant declines in performance. In this paper, we propose a Dual Branch Degradation Extractor Network to address the blind SR problem. While some blind SR methods assume noise-free degradation and others do not explicitly consider the presence of noise in the degradation model, our approach predicts two unsupervised degradation embeddings that represent blurry and noisy information. The SR network can then be adapted to blur embedding and noise embedding in distinct ways. Furthermore, we treat the degradation extractor as a regularizer to capitalize on differences between SR and HR images. Extensive experiments on several benchmarks demonstrate our method achieves SOTA performance in the blind SR problem.

</details>


### [32] [QueryOcc: Query-based Self-Supervision for 3D Semantic Occupancy](https://arxiv.org/abs/2511.17221)
*Adam Lilja,Ji Lan,Junsheng Fu,Lars Hammarstrand*

Main category: cs.CV

TL;DR: 本文提出了一种名为QueryOcc的自监督3D语义重建框架，通过采样连续的时空查询点，直接从相邻帧影像学习场景的空间占用与语义，无需人工标注，优于现有的2D一致性或离散栅格方法。


<details>
  <summary>Details</summary>
Motivation: 传统的3D场景学习依赖昂贵的人工标注或离散的点云累积，存在空间精度和扩展性瓶颈，因此亟需一种无需人工标记，适合大规模、高精度3D语义建模的自监督方法。

Method: 提出QueryOcc框架：基于4D（空间+时间）独立采样的查询点，通过自监督方式学习连续3D语义占用。该方法可利用从视觉基础模型推断的伪点云或原始激光点云进行监督。创新性地采用收缩型场景表达方式，在保持近处细节的同时，节省显存并压缩远处场景信息。

Result: 在自监督Occ3D-nuScenes基准上，QueryOcc相比此前基于相机的方法在语义RayIoU指标上提升26%；推理速度达到11.6帧/秒，性能与效率优越。

Conclusion: 直接基于4D查询的自监督监督机制显著提高了3D语义占用学习的效果，为无需标注的高精度3D场景建模提供了新途径，并验证了在自动驾驶等应用中的潜力。

Abstract: Learning 3D scene geometry and semantics from images is a core challenge in computer vision and a key capability for autonomous driving. Since large-scale 3D annotation is prohibitively expensive, recent work explores self-supervised learning directly from sensor data without manual labels. Existing approaches either rely on 2D rendering consistency, where 3D structure emerges only implicitly, or on discretized voxel grids from accumulated lidar point clouds, limiting spatial precision and scalability. We introduce QueryOcc, a query-based self-supervised framework that learns continuous 3D semantic occupancy directly through independent 4D spatio-temporal queries sampled across adjacent frames. The framework supports supervision from either pseudo-point clouds derived from vision foundation models or raw lidar data. To enable long-range supervision and reasoning under constant memory, we introduce a contractive scene representation that preserves near-field detail while smoothly compressing distant regions. QueryOcc surpasses previous camera-based methods by 26% in semantic RayIoU on the self-supervised Occ3D-nuScenes benchmark while running at 11.6 FPS, demonstrating that direct 4D query supervision enables strong self-supervised occupancy learning. https://research.zenseact.com/publications/queryocc/

</details>


### [33] [Real-Time Cooked Food Image Synthesis and Visual Cooking Progress Monitoring on Edge Devices](https://arxiv.org/abs/2511.16965)
*Jigyasa Gupta,Soumya Goyal,Anil Kumar,Ishan Jindal*

Main category: cs.CV

TL;DR: 提出了一种可在边缘设备上高效运行的，从生食照片生成烹饪完成照片的生成模型，以及配套的烹饪图像数据集和新评价指标。该方法生成的食品图像更加真实且资源消耗低。


<details>
  <summary>Details</summary>
Motivation: 当前将生食图像转化为熟食图像的生成任务极具挑战性，尤其是需要捕捉烹饪过程中的纹理、色彩和结构变化。已有方法结果不真实或资源消耗太高，不适合边缘设备应用。因此需要更高效、更逼真的食品图像生成方法。

Method: 1) 构建了一个基于烤箱过程的，有厨师标注烹饪熟度的新数据集。2) 设计了一种结合菜谱和烹饪状态条件引导的高效图像生成器，能根据初始原料照片和用户偏好生成逼真的目标食品图像。3) 引入了专为烹饪图像设计的CIS（Culinary Image Similarity）评价指标，用于训练损失和过程监控。

Result: 在自建烤箱烹饪数据集和公开数据集上测试，提出的方法FID分数分别提升30%和60%，优于现有生成模型基线，生成图像更真实、连贯且兼顾烹饪合理性。

Conclusion: 本工作为食品图像生成任务提供了新的高效解决方案和质量评价工具，有望推动智能烹饪、菜谱辅助等实际应用在边缘设备上落地。

Abstract: Synthesizing realistic cooked food images from raw inputs on edge devices is a challenging generative task, requiring models to capture complex changes in texture, color and structure during cooking. Existing image-to-image generation methods often produce unrealistic results or are too resource-intensive for edge deployment. We introduce the first oven-based cooking-progression dataset with chef-annotated doneness levels and propose an edge-efficient recipe and cooking state guided generator that synthesizes realistic food images conditioned on raw food image. This formulation enables user-preferred visual targets rather than fixed presets. To ensure temporal consistency and culinary plausibility, we introduce a domain-specific \textit{Culinary Image Similarity (CIS)} metric, which serves both as a training loss and a progress-monitoring signal. Our model outperforms existing baselines with significant reductions in FID scores (30\% improvement on our dataset; 60\% on public datasets)

</details>


### [34] [The Finer the Better: Towards Granular-aware Open-set Domain Generalization](https://arxiv.org/abs/2511.16979)
*Yunyun Wang,Zheng Duan,Xinyue Liao,Ke-Jia Chen,Songcan Chen*

Main category: cs.CV

TL;DR: 本文提出了一种针对开放集域泛化（OSDG）的新方法SeeCLIP，有效缓解已知类别和未知类别之间的结构化风险与开放空间风险的难题，通过细粒度语义增强和对抗学习提升了CLIP模型的泛化能力，在五个基准任务上比现有方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，模型部署后会遇到领域偏移和新类别（未知类别）的问题，现有的视觉-语言模型如CLIP在OSDG任务下易陷入区分已知和未知类别时的过度自信以及难以处理高度相似的“难未知”类别。为此，需要一种能在细粒度水平区分已知与未知类别，同时提升模型泛化能力的方法。

Method: 提出SeeCLIP框架，包含：（1）语义感知提示增强模块，将图像解构为可区分的语义token，实现细粒度视觉-语言对齐；（2）双重对比学习，引入排斥（与已知类别保持区分）与聚合（保持语义邻近）两种目标，提升对未知提示的定位能力；（3）语义引导扩散模块，通过扰动语义token合成伪未知样本，制造与已知类视觉极相似但局部差异的困难样本，以此加强模型的判别决策边界。

Result: 在五个主流数据集基准测试中，SeeCLIP相比最新方法提升了3%的准确率和5%的H-score，说明其在区分已知与未知类别、应对难未知类别方面有显著性能优势。

Conclusion: 细粒度的语义增强和难样本对抗是提升开放集域泛化能力的有效手段，SeeCLIP为OSDG问题提供了系统和实用的新解法，能显著提升在真实世界复杂环境下视觉-语言模型的泛化与安全性。

Abstract: Open-Set Domain Generalization (OSDG) tackles the realistic scenario where deployed models encounter both domain shifts and novel object categories. Despite impressive progress with vision-language models like CLIP, existing methods still fall into the dilemma between structural risk of known-classes and open-space risk from unknown-classes, and easily suffers from over-confidence, especially when distinguishing ``hard unknowns" that share fine-grained visual similarities with known classes. To this end, we propose a Semantic-enhanced CLIP (SeeCLIP) framework that explicitly addresses this dilemma through fine-grained semantic enhancement. In SeeCLIP, we propose a semantic-aware prompt enhancement module to decompose images into discriminative semantic tokens, enabling nuanced vision-language alignment beyond coarse category labels. To position unknown prompts effectively, we introduce duplex contrastive learning with complementary objectives, that is, repulsion to maintain separability from known classes, and cohesion to preserve semantic proximity. Further, our semantic-guided diffusion module synthesizes pseudo-unknowns by perturbing extracted semantic tokens, generating challenging samples that are visually similar to known classes yet exhibit key local differences. These hard negatives force the model to learn finer decision boundaries. Extensive experiments across five benchmarks demonstrate consistent improvements of 3% accuracy and 5% H-score over state-of-the-art methods.

</details>


### [35] [Gradient-Driven Natural Selection for Compact 3D Gaussian Splatting](https://arxiv.org/abs/2511.16980)
*Xiaobin Deng,Qiuli Yu,Changyu Diao,Min Li,Duanqing Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于自然选择思想的3DGS高斯原语剪枝方法，实现了更高效和自动化的剪枝过程，并在紧凑模型下提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS需要大量的高斯原语建模场景，导致存储和计算开销大。现有剪枝方法往往依赖手工设计的标准或额外参数，效果有限，缺乏自动化和最优性。

Method: 提出一种受自然选择启发的剪枝框架，将生存压力建模为作用于不透明度的正则梯度场，由最大化渲染质量的优化梯度自动确定保留或裁剪哪些高斯，消除人工干预。同时引入带有限先验的不透明度衰减技术，加速选择过程。

Result: 在相同紧缩预算下，方法实现了超过0.6dB的PSNR提升，优于现有3DGS剪枝方法，成为紧凑3DGS的新SOTA。

Conclusion: 本文所提剪枝方法充分自动化、无须人工干预，并兼顾高效与高质量重建，推动了3DGS在低资源场景下的应用。

Abstract: 3DGS employs a large number of Gaussian primitives to fit scenes, resulting in substantial storage and computational overhead. Existing pruning methods rely on manually designed criteria or introduce additional learnable parameters, yielding suboptimal results. To address this, we propose an natural selection inspired pruning framework that models survival pressure as a regularization gradient field applied to opacity, allowing the optimization gradients--driven by the goal of maximizing rendering quality--to autonomously determine which Gaussians to retain or prune. This process is fully learnable and requires no human intervention. We further introduce an opacity decay technique with a finite opacity prior, which accelerates the selection process without compromising pruning effectiveness. Compared to 3DGS, our method achieves over 0.6 dB PSNR gain under 15\% budgets, establishing state-of-the-art performance for compact 3DGS. Project page https://xiaobin2001.github.io/GNS-web.

</details>


### [36] [A Diversity-optimized Deep Ensemble Approach for Accurate Plant Leaf Disease Detection](https://arxiv.org/abs/2511.16982)
*Sai Nath Chowdary Medikonduru,Hongpeng Jin,Yanzhao Wu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Synergistic Diversity（SQ）的新框架，用于提高基于图像的植物病害检测准确率。针对现有集成多样性评价不足的问题，提出并验证了新的SQ指标，可更有效选择高性能集成模型。


<details>
  <summary>Details</summary>
Motivation: 植物病害对全球农业及粮食安全构成严重威胁。高效准确的病害图片检测对于减轻损失至关重要，但如何挑选最佳深度神经网络集成成员因多样性度量难题受到限制。

Method: 1）分析现有Q类集成多样性指标的局限性；2）提出新的SQ指标，能够更好刻画集成成员之间的协同效果；3）在植物叶片图像数据集上，通过实验验证SQ指标对集成模型选择和检测性能的提升作用。

Result: 实验结果显示，利用SQ指标选择的神经网络集成，检测准确率显著高于传统Q指标方法，验证了SQ指标的有效性和优越性。

Conclusion: SQ指标可作为更优的多样性度量工具，用于构建更高效可靠的植物病害图像检测系统，有助于农业病害智能识别领域的实际应用。

Abstract: Plant diseases pose a significant threat to global agriculture, causing over $220 billion in annual economic losses and jeopardizing food security. The timely and accurate detection of these diseases from plant leaf images is critical to mitigating their adverse effects. Deep neural network Ensembles (Deep Ensembles) have emerged as a powerful approach to enhancing prediction accuracy by leveraging the strengths of diverse Deep Neural Networks (DNNs). However, selecting high-performing ensemble member models is challenging due to the inherent difficulty in measuring ensemble diversity. In this paper, we introduce the Synergistic Diversity (SQ) framework to enhance plant disease detection accuracy. First, we conduct a comprehensive analysis of the limitations of existing ensemble diversity metrics (denoted as Q metrics), which often fail to identify optimal ensemble teams. Second, we present the SQ metric, a novel measure that captures the synergy between ensemble members and consistently aligns with ensemble accuracy. Third, we validate our SQ approach through extensive experiments on a plant leaf image dataset, which demonstrates that our SQ metric substantially improves ensemble selection and enhances detection accuracy. Our findings pave the way for a more reliable and efficient image-based plant disease detection.

</details>


### [37] [RadioKMoE: Knowledge-Guided Radiomap Estimation with Kolmogorov-Arnold Networks and Mixture-of-Experts](https://arxiv.org/abs/2511.16986)
*Fupei Guo,Kerry Pan,Songyang Zhang,Yue Wang,Zhi Ding*

Main category: cs.CV

TL;DR: 本论文提出了一种结合Kolmogorov-Arnold网络（KAN）和专家混合（MoE）的方法RadioKMoE，用于提升无线射频地图（Radiomap）预测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无线网络的部署和管理依赖于精确的信号覆盖地图，但随着信号传播行为与环境日益复杂，现有射频地图估算技术面临重大挑战，准确精细地还原信号传播规律变得更加困难。

Method: 作者提出了知识引导的RME框架RadioKMoE，分为两步：首先利用KAN模块根据物理传播规律和全局信道趋势生成初始覆盖粗图；随后利用MoE网络，结合初始粗图和环境信息，由多个专精于不同射频地图模式的专家网络细化本地细节并保持整体一致性。

Result: 在多频段和单频段的实验中，RadioKMoE在射频地图估算方面都表现出了更高的精度和更强的鲁棒性，相较现有深度学习方法有明显提升。

Conclusion: RadioKMoE通过融合KAN的物理建模优势和MoE的本地特征专精，实现了更为精确和稳健的射频地图估算，为复杂环境下无线网络部署提供了新思路。

Abstract: Radiomap serves as a vital tool for wireless network management and deployment by providing powerful spatial knowledge of signal propagation and coverage. However, increasingly complex radio propagation behavior and surrounding environments pose strong challenges for radiomap estimation (RME). In this work, we propose a knowledge-guided RME framework that integrates Kolmogorov-Arnold Networks (KAN) with Mixture-of-Experts (MoE), namely RadioKMoE. Specifically, we design a KAN module to predict an initial coarse coverage map, leveraging KAN's strength in approximating physics models and global radio propagation patterns. The initial coarse map, together with environmental information, drives our MoE network for precise radiomap estimation. Unlike conventional deep learning models, the MoE module comprises expert networks specializing in distinct radiomap patterns to improve local details while preserving global consistency. Experimental results in both multi- and single-band RME demonstrate the enhanced accuracy and robustness of the proposed RadioKMoE in radiomap estimation.

</details>


### [38] [DReX: Pure Vision Fusion of Self-Supervised and Convolutional Representations for Image Complexity Prediction](https://arxiv.org/abs/2511.16991)
*Jonathan Skaza,Parsa Madinei,Ziqi Wen,Miguel Eckstein*

Main category: cs.CV

TL;DR: 本研究提出了一个无语言信息、仅基于视觉的模型DReX（DINO-ResNet Fusion），能够高效预测图像复杂度，并在主流数据集上超过了多模态方法。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多模态方法结合视觉与语言特征以提升图像复杂度预测，但尚不明确语言信息是否为必要。作者关注仅用视觉特征能否实现与人类感知一致的复杂度预测。

Method: 提出DReX模型，将自监督的DINOv3 ViT-S/16与有监督ResNet-50多尺度特征融合，通过可学习的注意力机制，结合低级纹理和高级语义，用于复杂度回归。

Result: DReX在IC9600基准集上取得了最优表现（Pearson r=0.9581），超越所有现有多模态及单模态方法，且只需原方法约1/21.5的参数量。其在各类数据集和指标（Pearson、Spearman相关系数、RMSE、MAE）上均表现优秀。消融和注意力分析显示DINOv3与ResNet特征互补。

Conclusion: 仅利用精心融合的视觉特征即可实现符合人类感知的复杂度预测。自监督Transformer与深度卷积网络联合具有协同与互补优势，无需借助语言信息。

Abstract: Visual complexity prediction is a fundamental problem in computer vision with applications in image compression, retrieval, and classification. Understanding what makes humans perceive an image as complex is also a long-standing question in cognitive science. Recent approaches have leveraged multimodal models that combine visual and linguistic representations, but it remains unclear whether language information is necessary for this task. We propose DReX (DINO-ResNet Fusion), a vision-only model that fuses self-supervised and convolutional representations through a learnable attention mechanism to predict image complexity. Our architecture integrates multi-scale hierarchical features from ResNet-50 with semantically rich representations from DINOv3 ViT-S/16, enabling the model to capture both low-level texture patterns and high-level semantic structure. DReX achieves state-of-the-art performance on the IC9600 benchmark (Pearson r = 0.9581), surpassing previous methods--including those trained on multimodal image-text data--while using approximately 21.5x fewer learnable parameters. Furthermore, DReX generalizes robustly across multiple datasets and metrics, achieving superior results on Pearson and Spearman correlation, Root Mean Square Error (RMSE), and Mean Absolute Error (MAE). Ablation and attention analyses confirm that DReX leverages complementary cues from both backbones, with the DINOv3 [CLS] token enhancing sensitivity to visual complexity. Our findings suggest that visual features alone can be sufficient for human-aligned complexity prediction and that, when properly fused, self-supervised transformers and supervised deep convolutional neural networks offer complementary and synergistic benefits for this task.

</details>


### [39] [DepthFocus: Controllable Depth Estimation for See-Through Scenes](https://arxiv.org/abs/2511.16993)
*Junhong Min,Jimin Kim,Cheol-Hui Min,Minwook Kim,Youngpil Jeon,Minyong Choi*

Main category: cs.CV

TL;DR: 提出了一种可控的视觉Transformer模型DepthFocus，实现了根据用户意图调整聚焦深度，对复杂透视场景下的多层深度估计效果优异。


<details>
  <summary>Details</summary>
Motivation: 传统的深度估计模型只能输出接近表面的静态深度图，对于真实环境中如透明或半透明材料形成的多层深度无法很好处理，而人类可主动选择聚焦的深度。因此，作者希望设计一种“可操控关注深度”的深度估计方法。

Method: 提出DepthFocus，将深度估计问题重新定义为用户意图驱动的深度控制问题。具体来说，输入一个代表深度偏好的标量，模型据此动态调整聚焦层。模型采用视觉Transformer结构，并用作者自建的50万张多层透明合成数据集进行训练，兼容复杂的穿透和反射场景。

Result: DepthFocus在传统单一深度数据集BOOSTER上取得了最佳结果，在作者提出的真实及合成多深度数据集上也实现了与用户意图一致的深度估计。同时，在未见过的透视场景中表现出优异的泛化能力。

Conclusion: DepthFocus为主动、类人式的3D感知提供了有力工具，为复杂多重视深的场景感知及下游任务铺平了道路。

Abstract: Depth in the real world is rarely singular. Transmissive materials create layered ambiguities that confound conventional perception systems. Existing models remain passive, attempting to estimate static depth maps anchored to the nearest surface, while humans actively shift focus to perceive a desired depth. We introduce DepthFocus, a steerable Vision Transformer that redefines stereo depth estimation as intent-driven control. Conditioned on a scalar depth preference, the model dynamically adapts its computation to focus on the intended depth, enabling selective perception within complex scenes. The training primarily leverages our newly constructed 500k multi-layered synthetic dataset, designed to capture diverse see-through effects. DepthFocus not only achieves state-of-the-art performance on conventional single-depth benchmarks like BOOSTER, a dataset notably rich in transparent and reflective objects, but also quantitatively demonstrates intent-aligned estimation on our newly proposed real and synthetic multi-depth datasets. Moreover, it exhibits strong generalization capabilities on unseen see-through scenes, underscoring its robustness as a significant step toward active and human-like 3D perception.

</details>


### [40] [VLM-Augmented Degradation Modeling for Image Restoration Under Adverse Weather Conditions](https://arxiv.org/abs/2511.16998)
*Qianyi Shao,Yuanfan Zhang,Renxiang Xiao,Liang Hu*

Main category: cs.CV

TL;DR: 本文提出了一种针对恶劣天气（如雨、雾、雪及其混合环境）下视觉恢复的统一模型——MVLR，既提升了图像修复质量，也兼顾了模型效率，为自动驾驶和户外机器人提供了可靠的视觉感知方案。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气会严重影响图像质量，对自动驾驶和户外机器人等应用的视觉感知带来巨大挑战。现有方法通常只能处理某一种天气或需多个模型，难以实现通用、实时且高效的恢复。

Method: 本方法提出了MVLR模型，结合轻量级的编码-解码主干、视觉-语言模型（VLM）和隐式记忆库（IMB）。VLM以链式推理方式生成天气退化先验，引导查询IMB中存储的退化原型，并通过动态跨尺度注意力机制与多尺度视觉特征自适应融合，实现高效准确的图像恢复。

Result: 在四个恶劣天气基准数据集上，MVLR模型在PSNR、SSIM等指标上优于单分支和专家混合基线，验证了其有效性和优越性。

Conclusion: MVLR在模型紧凑性与表达能力之间达到了良好平衡，非常适合在多样且复杂的户外环境中实时部署，为视觉感知任务提供了切实可行的解决方案。

Abstract: Reliable visual perception under adverse weather conditions, such as rain, haze, snow, or a mixture of them, is desirable yet challenging for autonomous driving and outdoor robots. In this paper, we propose a unified Memory-Enhanced Visual-Language Recovery (MVLR) model that restores images from different degradation levels under various weather conditions. MVLR couples a lightweight encoder-decoder backbone with a Visual-Language Model (VLM) and an Implicit Memory Bank (IMB). The VLM performs chain-of-thought inference to encode weather degradation priors and the IMB stores continuous latent representations of degradation patterns. The VLM-generated priors query the IMB to retrieve fine-grained degradation prototypes. These prototypes are then adaptively fused with multi-scale visual features via dynamic cross-attention mechanisms, enhancing restoration accuracy while maintaining computational efficiency. Extensive experiments on four severe-weather benchmarks show that MVLR surpasses single-branch and Mixture-of-Experts baselines in terms of Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). These results indicate that MVLR offers a practical balance between model compactness and expressiveness for real-time deployment in diverse outdoor conditions.

</details>


### [41] [Vision Language Models are Confused Tourists](https://arxiv.org/abs/2511.17004)
*Patrick Amadeus Irawan,Ikhlasul Akmal Hanif,Muhammad Dehan Al Kautsar,Genta Indra Winata,Fajri Koto,Alham Fikri Aji*

Main category: cs.CV

TL;DR: 本文提出全新评估工具，揭示了多视觉文化概念混合时，主流视觉-语言模型（VLMs）表现大幅下降，显示当前模型的文化健壮性不足。


<details>
  <summary>Details</summary>
Motivation: 虽然文化维度在VLMs的评估中至关重要，但主流评测仅考虑单一文化线索，忽视了现实中多元文化信息并存的情境。因此，研究者希望检测VLMs在复杂文化输入下的稳定性。

Method: 作者提出了ConfusedTourist，一套专为测试地理/文化干扰下VLMs鲁棒性而设计的对抗性评测工具。该套件通过多文化线索叠加及图像生成扰动，系统考察模型的表现，并结合可解释性分析考查模型注意力分布变化。

Result: 实验发现，简单的多文化图像叠加就能显著降低模型精度，利用图像生成的叠加扰动效果更差。可解释性分析显示，模型注意力会偏向无关线索，导致识别失败。

Conclusion: 视觉文化概念混合对主流VLMs带来严重挑战，说明当前模型的多文化适应性不足，亟需发展更具文化鲁棒性的多模态理解模型。

Abstract: Although the cultural dimension has been one of the key aspects in evaluating Vision-Language Models (VLMs), their ability to remain stable across diverse cultural inputs remains largely untested, despite being crucial to support diversity and multicultural societies. Existing evaluations often rely on benchmarks featuring only a singular cultural concept per image, overlooking scenarios where multiple, potentially unrelated cultural cues coexist. To address this gap, we introduce ConfusedTourist, a novel cultural adversarial robustness suite designed to assess VLMs' stability against perturbed geographical cues. Our experiments reveal a critical vulnerability, where accuracy drops heavily under simple image-stacking perturbations and even worsens with its image-generation-based variant. Interpretability analyses further show that these failures stem from systematic attention shifts toward distracting cues, diverting the model from its intended focus. These findings highlight a critical challenge: visual cultural concept mixing can substantially impair even state-of-the-art VLMs, underscoring the urgent need for more culturally robust multimodal understanding.

</details>


### [42] [FLUID: Training-Free Face De-identification via Latent Identity Substitution](https://arxiv.org/abs/2511.17005)
*Jinhyeong Park,Shaheryar Muhammad,Seangmin Lee,Jong Taek Lee,Soon Ki Jung*

Main category: cs.CV

TL;DR: 本文提出了FLUID，一个无需训练的面部去身份识别框架，通过在预训练扩散模型的潜在空间中直接替换身份，实现良好的身份去除与属性保持的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸去身份方法往往在保护身份和保持人脸其他重要属性之间难以取舍，且存在需要训练等限制。为提升去身份化的实用性和效率，需要建立一种无需训练、可精准编辑身份同时保持其他属性的方法。

Method: FLUID框架利用预训练的无条件扩散模型，将身份编辑重新解释为在潜在h空间的语义位移。通过受化学替换机制启发，设计了新颖的reagent losses，实现对属性保持与身份抑制的监督。提出了两种导航潜在流形的编辑方式：线性和基于切线的测地编辑法。整个过程无需额外训练。

Result: 在CelebA-HQ和FFHQ两个数据集上的实验表明，FLUID在身份去除和属性保持的权衡上表现优异，定性和定量指标均优于目前最佳的人脸去身份方法。

Conclusion: FLUID展示了无需训练即可在扩散模型潜在空间实现身份替换的有效性，在面部去身份识别领域提供了新的、更优的解决方案。

Abstract: We present FLUID (Face de-identification in the Latent space via Utility-preserving Identity Displacement), a training-free framework that directly substitutes identity in the latent space of pretrained diffusion models. Inspired by substitution mechanisms in chemistry, we reinterpret identity editing as semantic displacement in the latent h-space of a pretrained unconditional diffusion model. Our framework discovers identity-editing directions through optimization guided by novel reagent losses, which supervise for attribute preservation and identity suppression. We further propose both linear and geodesic (tangent-based) editing schemes to effectively navigate the latent manifold. Experimental results on CelebA-HQ and FFHQ demonstrate that FLUID achieves a superior trade-off between identity suppression and attribute preservation, outperforming state-of-the-art de-identification methods in both qualitative and quantitative metrics.

</details>


### [43] [Parameter-Free Neural Lens Blur Rendering for High-Fidelity Composites](https://arxiv.org/abs/2511.17014)
*Lingyan Ruan,Bin Chen,Taehyun Rhee*

Main category: cs.CV

TL;DR: 该论文提出了一种不依赖场景深度或相机参数的镜头虚化合成方法，通过直接从RGB图像估算CoC，实现高质量的虚实融合效果。


<details>
  <summary>Details</summary>
Motivation: 在虚拟物体与真实景物混合时，为了获得真实一致的镜头虚化效果，传统方法依赖于相机参数和场景深度信息，但这些信息往往普通用户难以获得，限制了相关技术的应用。

Method: 本文提出一种新颖的方法，利用直接从RGB图像预测CoC图，无需深度或相机元数据。对于虚拟物体，通过其signed CoC与深度建立线性关系，随后用神经网络重渲染虚化效果，实现与真实场景一致的镜头虚化合成。

Result: 实验证明，该方法能在主观视觉效果和客观数据评价上优于当前主流技术，实现了高保真且自然的虚实合成与虚化效果。

Conclusion: 本文方法无需依赖难以获取的深度与相机参数，为实际混合现实应用提供了更灵活实用的新方案。

Abstract: Consistent and natural camera lens blur is important for seamlessly blending 3D virtual objects into photographed real-scenes. Since lens blur typically varies with scene depth, the placement of virtual objects and their corresponding blur levels significantly affect the visual fidelity of mixed reality compositions. Existing pipelines often rely on camera parameters (e.g., focal length, focus distance, aperture size) and scene depth to compute the circle of confusion (CoC) for realistic lens blur rendering. However, such information is often unavailable to ordinary users, limiting the accessibility and generalizability of these methods. In this work, we propose a novel compositing approach that directly estimates the CoC map from RGB images, bypassing the need for scene depth or camera metadata. The CoC values for virtual objects are inferred through a linear relationship between its signed CoC map and depth, and realistic lens blur is rendered using a neural reblurring network. Our method provides flexible and practical solution for real-world applications. Experimental results demonstrate that our method achieves high-fidelity compositing with realistic defocus effects, outperforming state-of-the-art techniques in both qualitative and quantitative evaluations.

</details>


### [44] [RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis](https://arxiv.org/abs/2511.17045)
*Linfeng Dong,Yuchen Yang,Hao Wu,Wei Wang,Yuenan HouZhihang Zhong,Xiao Sun*

Main category: cs.CV

TL;DR: 提出了RacketVision数据集，专注于球类运动的计算机视觉任务，首次大规模标注球拍姿态，支持球追踪、球拍姿态估计及轨迹预测任务。通过引入CrossAttention机制提升多模态特征融合效果，数据集与基线为运动分析未来研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前体育分析中的计算机视觉技术大多只聚焦于球的追踪，而忽略了球拍等关键人-物交互对象的详细分析。缺乏细粒度标注和多模态融合机制，限制了球类运动分析的能力。

Method: RacketVision数据集收集了乒乓球、网球、羽毛球等运动的影像数据，对球与球拍进行了详细的联合标注。设计了三个任务：球的精细追踪、球拍姿态估计和球轨迹预测。实验对比了不同的多模态融合方法，发现仅拼接特征反而影响效果，而引入CrossAttention机制能有效提升预测性能。

Result: 基线实验显示，传统的特征拼接方式在多模态融合任务中不能充分利用球拍姿态信息，而采用CrossAttention机制后，在球轨迹预测任务上明显优于单模态和简单融合方法。

Conclusion: RacketVision作为首个覆盖球拍姿态与轨迹等多项任务的开放数据集，为体育运动的动态目标追踪、动作预测、条件运动分析等研究领域提供了高价值资源与标准基线，推动了体育视觉智能发展。

Abstract: We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision

</details>


### [45] [RoomPlanner: Explicit Layout Planner for Easier LLM-Driven 3D Room Generation](https://arxiv.org/abs/2511.17048)
*Wenzhuo Sun,Mingjian Liang,Wenxuan Song,Xuelian Cheng,Zongyuan Ge*

Main category: cs.CV

TL;DR: 提出了RoomPlanner，一个全自动3D室内场景生成框架，通过简单的文本实现快速、真实的室内场景建模，速度快且有良好可编辑性。


<details>
  <summary>Details</summary>
Motivation: 目前基于文本生成3D室内场景的方法需要手动介入或依赖全景图像，缺少端到端、高效、易用的全自动解决方案。

Method: 1. 设计语言驱动的分层智能体规划器，将简短文本解析为详细场景描述。
2. 依据解析结果初始化3D点云，并引入两种排列约束，迭代优化空间布局以保证合理性和可达性。
3. 提出AnyReach取样和ITFS策略，高效优化3D高斯场景表示，加速渲染过程。

Result: 实验证明，所提方法能高效生成几何上合理且具备高视觉质量的3D室内场景，渲染速度和视觉质量均优于现有方法，并能够在30分钟内完成生成，且结果可编辑。

Conclusion: RoomPlanner实现了基于简短文本的全自动3D室内场景生成，兼顾模型速度、质量与可编辑性，为相关场景快速建模提供有效工具。

Abstract: In this paper, we propose RoomPlanner, the first fully automatic 3D room generation framework for painlessly creating realistic indoor scenes with only short text as input. Without any manual layout design or panoramic image guidance, our framework can generate explicit layout criteria for rational spatial placement. We begin by introducing a hierarchical structure of language-driven agent planners that can automatically parse short and ambiguous prompts into detailed scene descriptions. These descriptions include raw spatial and semantic attributes for each object and the background, which are then used to initialize 3D point clouds. To position objects within bounded environments, we implement two arrangement constraints that iteratively optimize spatial arrangements, ensuring a collision-free and accessible layout solution. In the final rendering stage, we propose a novel AnyReach Sampling strategy for camera trajectory, along with the Interval Timestep Flow Sampling (ITFS) strategy, to efficiently optimize the coarse 3D Gaussian scene representation. These approaches help reduce the total generation time to under 30 minutes. Extensive experiments demonstrate that our method can produce geometrically rational 3D indoor scenes, surpassing prior approaches in both rendering speed and visual quality while preserving editability. The code will be available soon.

</details>


### [46] [PathAgent: Toward Interpretable Analysis of Whole-slide Pathology Images via Large Language Model-based Agentic Reasoning](https://arxiv.org/abs/2511.17052)
*Jingyun Chen,Linghan Cai,Zhikang Wang,Yi Huang,Songhan Jiang,Shenjin Huang,Hongpeng Wang,Yongbing Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种无需额外训练、基于大语言模型的智能代理PathAgent，能以专家式的逐步推理方式分析病理全视野切片图像（WSI），在五个数据集上实现了可解释的、领先的无监督预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有病理全视野切片图像分析方法缺乏良好的推理轨迹，导致模型预测过程不透明、难以解释。需要设计一种模拟病理专家推理流程、具有可解释性的自动化分析工具。

Method: 作者提出PathAgent代理框架，无需模型训练，基于大语言模型设计，分为Navigator（自动定位关键区域）、Perceptor（提取形态学视觉特征）、Executor（自然语言整合分析结果与推理链条）三个模块，实现逐步分析与可解释的推理流程。

Result: 在五个具有挑战性的数据集上评估，PathAgent显示出强大的零样本泛化能力，在开放和约束式视觉问答任务上均优于特定任务基线模型。同时，与人类病理专家协作的评测表明其具有良好的可解释性及临床应用前景。

Conclusion: PathAgent作为一种透明、具逻辑推理能力的诊断辅助工具，为病理WSI自动化分析提供了新的思路，并为模型可解释性与临床部署奠定了基础。

Abstract: Analyzing whole-slide images (WSIs) requires an iterative, evidence-driven reasoning process that parallels how pathologists dynamically zoom, refocus, and self-correct while collecting the evidence. However, existing computational pipelines often lack this explicit reasoning trajectory, resulting in inherently opaque and unjustifiable predictions. To bridge this gap, we present PathAgent, a training-free, large language model (LLM)-based agent framework that emulates the reflective, stepwise analytical approach of human experts. PathAgent can autonomously explore WSI, iteratively and precisely locating significant micro-regions using the Navigator module, extracting morphology visual cues using the Perceptor, and integrating these findings into the continuously evolving natural language trajectories in the Executor. The entire sequence of observations and decisions forms an explicit chain-of-thought, yielding fully interpretable predictions. Evaluated across five challenging datasets, PathAgent exhibits strong zero-shot generalization, surpassing task-specific baselines in both open-ended and constrained visual question-answering tasks. Moreover, a collaborative evaluation with human pathologists confirms PathAgent's promise as a transparent and clinically grounded diagnostic assistant.

</details>


### [47] [OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian Tracking and Understanding](https://arxiv.org/abs/2511.17053)
*Teng Fu,Mengyang Zhao,Ke Niu,Kaixin Peng,Bin Li*

Main category: cs.CV

TL;DR: 提出了OmniPT，一个统一的行人跟踪框架，通过创新训练方式显著提升了视觉语言大模型（LVLM）在语义理解与跟踪任务上的表现，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLM在图像级任务（如VQA、描述）表现优秀，但在实例级任务（视觉定位、目标检测等）仍落后于传统专家模型。与此同时，结合行人跟踪与自然语言的新任务（如Referring MOT、Semantic MOT）涌现，这些任务强调模型需具备高级语义理解能力，而这是LVLM的强项。本文旨在填补LVLM在此类任务中的性能缺口。

Method: 提出OmniPT框架，使LVLM能在监督下输出规范化的目标框，并实现基于参考和交互式的语义理解。训练流程分为四阶段：初步强化学习（RL）训练使模型能输出结构化的预测结果；中期训练结合大量行人数据集提升通用性；有监督微调强化跟踪表现；最后一轮RL进一步提升跟踪和指令执行能力。

Result: 在多个跟踪基准上实验，表明所提出方法在准确性和语义理解上优于此前方法。

Conclusion: OmniPT有效提升了LVLM在复杂语义行人跟踪任务中的表现，为将大模型用于高级语义视觉任务奠定基础。

Abstract: LVLMs have been shown to perform excellently in image-level tasks such as VQA and caption. However, in many instance-level tasks, such as visual grounding and object detection, LVLMs still show performance gaps compared to previous expert models. Meanwhile, although pedestrian tracking is a classical task, there have been a number of new topics in combining object tracking and natural language, such as Referring MOT, Cross-view Referring MOT, and Semantic MOT. These tasks emphasize that models should understand the tracked object at an advanced semantic level, which is exactly where LVLMs excel. In this paper, we propose a new unified Pedestrian Tracking framework, namely OmniPT, which can track, track based on reference and generate semantic understanding of tracked objects interactively. We address two issues: how to model the tracking task into a task that foundation models can perform, and how to make the model output formatted answers. To this end, we implement a training phase consisting of RL-Mid Training-SFT-RL. Based on the pre-trained weights of the LVLM, we first perform a simple RL phase to enable the model to output fixed and supervisable bounding box format. Subsequently, we conduct a mid-training phase using a large number of pedestrian-related datasets. Finally, we perform supervised fine-tuning on several pedestrian tracking datasets, and then carry out another RL phase to improve the model's tracking performance and enhance its ability to follow instructions. We conduct experiments on tracking benchmarks and the experimental results demonstrate that the proposed method can perform better than the previous methods.

</details>


### [48] [RL-AD-Net: Reinforcement Learning Guided Adaptive Displacement in Latent Space for Refined Point Cloud Completion](https://arxiv.org/abs/2511.17054)
*Bhanu Pratap Paregi,Vaibhav Kumar*

Main category: cs.CV

TL;DR: 该论文提出了一种名为RL-AD-Net的点云补全后处理方法，通过强化学习在预训练自编码器的潜在空间内对补全过程进行优化，显著提升了点云局部几何一致性，实验表明其在多种场景下均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前点云补全模型虽能生成全局合理的形状，但在局部几何一致性方面常存在缺陷。为解决这一问题，作者提出基于强化学习的精细化方法，希望提升补全模型输出的几何精度。

Method: 作者设计了RL-AD-Net框架，首先利用预训练自编码器将点云补全部分编码为紧致的全局特征向量（GFV），再通过强化学习代理对GFV进行微调以提升几何一致性。用轻量的非参数PointNN选择器评估原始补全与RL后处理结果，保留较优者。在有真值时结合Chamfer距离与几何一致性指标指导优化。每个类别独立训练，方法可扩展到多类别。

Result: 在ShapeNetCore-2048数据集实验显示，传统补全网络仅在与训练相似的裁剪场景表现良好，但在随机裁剪下效果较差，而RL-AD-Net在两种情况下均带来了一致提升。

Conclusion: RL-AD-Net通过强化学习在点云补全任务中实现了有效且通用的几何一致性优化，方法轻量、模块化、对网络架构无依赖，可广泛应用于多种点云补全模型，无需重新训练。

Abstract: Recent point cloud completion models, including transformer-based, denoising-based, and other state-of-the-art approaches, generate globally plausible shapes from partial inputs but often leave local geometric inconsistencies. We propose RL-AD-Net, a reinforcement learning (RL) refinement framework that operates in the latent space of a pretrained point autoencoder. The autoencoder encodes completions into compact global feature vectors (GFVs), which are selectively adjusted by an RL agent to improve geometric fidelity. To ensure robustness, a lightweight non-parametric PointNN selector evaluates the geometric consistency of both the original completion and the RL-refined output, retaining the better reconstruction. When ground truth is available, both Chamfer Distance and geometric consistency metrics guide refinement. Training is performed separately per category, since the unsupervised and dynamic nature of RL makes convergence across highly diverse categories challenging. Nevertheless, the framework can be extended to multi-category refinement in future work. Experiments on ShapeNetCore-2048 demonstrate that while baseline completion networks perform reasonable under their training-style cropping, they struggle in random cropping scenarios. In contrast, RL-AD-Net consistently delivers improvements across both settings, highlighting the effectiveness of RL-guided ensemble refinement. The approach is lightweight, modular, and model-agnostic, making it applicable to a wide range of completion networks without requiring retraining.

</details>


### [49] [Planning with Sketch-Guided Verification for Physics-Aware Video Generation](https://arxiv.org/abs/2511.17450)
*Yidong Huang,Zun Wang,Han Lin,Dong-Ki Kim,Shayegan Omidshafiei,Jaehong Yoon,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: 本论文提出了一种名为SketchVerify的训练无关型草图-验证式规划框架，用于提升视频生成中的动作规划质量，实现更好的物理一致性与指令一致性，并显著提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法大多依赖一次性动作计划（简单动作）或多次迭代修正（高计算成本），难以兼顾复杂动作的时序一致性、物理真实性和生成效率，因此需要新的规划策略。

Method: 作者提出SketchVerify方法：给定prompt和参考图像，先生成多个动作轨迹候选；通过视觉-语言验证模型，评价每个候选与文本指令的语义一致性和轨迹的物理合理性。候选轨迹用静态背景合成的轻量级视频草图进行打分，无需多次调用昂贵的视频生成器。重复采样与验证，直到获得理想轨迹，最终据此生成视频。

Result: 在WorldModelBench和PhyWorldBench数据集上，该方法在运动质量、物理现实性和长期一致性方面大幅超过现有方法，同时计算效率显著提升。消融实验显示增加轨迹候选数能持续提高性能。

Conclusion: SketchVerify在无需额外训练的前提下，有效改善了复杂视频生成中的运动计划与物理合理性，兼顾了高性能和高效率，在未来视频生成任务中极具应用前景。

Abstract: Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.

</details>


### [50] [REArtGS++: Generalizable Articulation Reconstruction with Temporal Geometry Constraint via Planar Gaussian Splatting](https://arxiv.org/abs/2511.17059)
*Di Wu,Liu Liu,Anran Huang,Yuyan Liu,Qiaoyu Jun,Shaofan Liu,Liangtu Song,Cewu Lu*

Main category: cs.CV

TL;DR: 本文提出了REArtGS++，一种面向通用关节物体的重建方法，采用时序几何约束和平面高斯投影，有效提升了多部件和螺旋关节物体的重建与参数估计效果。


<details>
  <summary>Details</summary>
Motivation: 现有的REArtGS方法在处理螺旋关节或多部件物体时存在困难，且对未见状态缺乏几何约束，限制了其通用性和准确性。

Method: REArtGS++为每个关节引入无先验类型的解耦螺旋运动建模，并通过部分运动融合协同优化部件感知Gaussians和关节参数。创新之处在于引入时序连续的几何约束，鼓励Gaussians平面化，并提出基于泰勒一阶展开的平面法向与深度的时序一致性正则化。

Result: 在合成和真实的关节物体数据集上进行的广泛实验表明，REArtGS++在部件级表面重建和关节参数估计方面优于现有方法。

Conclusion: REArtGS++有效提升了不同类别和结构关节物体的重建精度与通用性，为实际动态场景下的物体解析和理解提供了新的解决方案。

Abstract: Articulated objects are pervasive in daily environments, such as drawers and refrigerators. Towards their part-level surface reconstruction and joint parameter estimation, REArtGS~\cite{wu2025reartgs} introduces a category-agnostic approach using multi-view RGB images at two different states. However, we observe that REArtGS still struggles with screw-joint or multi-part objects and lacks geometric constraints for unseen states. In this paper, we propose REArtGS++, a novel method towards generalizable articulated object reconstruction with temporal geometry constraint and planar Gaussian splatting. We first model a decoupled screw motion for each joint without type prior, and jointly optimize part-aware Gaussians with joint parameters through part motion blending. To introduce time-continuous geometric constraint for articulated modeling, we encourage Gaussians to be planar and propose a temporally consistent regularization between planar normal and depth through Taylor first-order expansion. Extensive experiments on both synthetic and real-world articulated objects demonstrate our superiority in generalizable part-level surface reconstruction and joint parameter estimation, compared to existing approaches. Project Site: https://sites.google.com/view/reartgs2/home.

</details>


### [51] [ReBrain: Brain MRI Reconstruction from Sparse CT Slice via Retrieval-Augmented Diffusion](https://arxiv.org/abs/2511.17068)
*Junming Liu,Yifei Sun,Weihua Cheng,Yujin Kang,Yirong Chen,Ding Wang,Guosun Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种名为ReBrain的检索增强型扩散框架，实现了稀疏CT重建高质量脑MRI，在两大公开数据集上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT往往导致三维体数据极为稀疏，当前从稀疏CT合成完整脑MRI的任务极具挑战性，急需创新性重建方法。

Method: ReBrain框架主要包含两个部分：首先使用Brownian Bridge Diffusion Model（BBDM）根据2D方向合成MRI切片；其次通过精细调优的检索模型，在大型数据库中检索结构和病理相似的CT切片，用作参考并通过ControlNet模块指导MRI的生成，保障结构连续性。若检索失败，则采用球面线性插值法补充引导信息。

Result: 在SynthRAD2023和BraTS数据集上，ReBrain方法在稀疏条件下的跨模态重建精度上均取得了当前最好的表现。

Conclusion: 检索增强结合扩散模型，被证实能有效提升稀疏CT跨模态MRI重建质量，为临床MRI不可及病患提供新途径。

Abstract: Magnetic Resonance Imaging (MRI) plays a crucial role in brain disease diagnosis, but it is not always feasible for certain patients due to physical or clinical constraints. Recent studies attempt to synthesize MRI from Computed Tomography (CT) scans; however, low-dose protocols often result in highly sparse CT volumes with poor through-plane resolution, making accurate reconstruction of the full brain MRI volume particularly challenging. To address this, we propose ReBrain, a retrieval-augmented diffusion framework for brain MRI reconstruction. Given any 3D CT scan with limited slices, we first employ a Brownian Bridge Diffusion Model (BBDM) to synthesize MRI slices along the 2D dimension. Simultaneously, we retrieve structurally and pathologically similar CT slices from a comprehensive prior database via a fine-tuned retrieval model. These retrieved slices are used as references, incorporated through a ControlNet branch to guide the generation of intermediate MRI slices and ensure structural continuity. We further account for rare retrieval failures when the database lacks suitable references and apply spherical linear interpolation to provide supplementary guidance. Extensive experiments on SynthRAD2023 and BraTS demonstrate that ReBrain achieves state-of-the-art performance in cross-modal reconstruction under sparse conditions.

</details>


### [52] [Diversity Has Always Been There in Your Visual Autoregressive Models](https://arxiv.org/abs/2511.17074)
*Tong Wang,Guanyu Yang,Nian Liu,Kai Wang,Yaxing Wang,Abdelrahman M Shaker,Salman Khan,Fahad Shahbaz Khan,Senmao Li*

Main category: cs.CV

TL;DR: 本文提出了一种无需额外训练的简单方法（DiverseVAR），显著提升了视觉自回归（VAR）模型的生成多样性，并保持了高图像质量。


<details>
  <summary>Details</summary>
Motivation: 虽然VAR模型在推理效率和图像质量上优于传统方法，但存在生成多样性下降的问题，尤其类似于蒸馏扩散模型中出现的“多样性崩溃”。为应对这一挑战，作者希望提升VAR模型的多样性，同时维持其效率和质量。

Method: 作者提出DiverseVAR，通过分析VAR模型中特征图的关键分量，并在输入时抑制、输出时增强此分量，无需重新训练即能恢复VAR的生成多样性。

Result: 实验证明，DiverseVAR可以在极小的性能损失下，大幅度提升VAR模型的生成多样性，输出图片的多样性显著增强。

Conclusion: DiverseVAR为提升VAR模型多样性提供了高效、简便的新思路，不需要增加额外训练成本，且可直接应用于现有模型，推动了高效生成建模的发展。

Abstract: Visual Autoregressive (VAR) models have recently garnered significant attention for their innovative next-scale prediction paradigm, offering notable advantages in both inference efficiency and image quality compared to traditional multi-step autoregressive (AR) and diffusion models. However, despite their efficiency, VAR models often suffer from the diversity collapse i.e., a reduction in output variability, analogous to that observed in few-step distilled diffusion models. In this paper, we introduce DiverseVAR, a simple yet effective approach that restores the generative diversity of VAR models without requiring any additional training. Our analysis reveals the pivotal component of the feature map as a key factor governing diversity formation at early scales. By suppressing the pivotal component in the model input and amplifying it in the model output, DiverseVAR effectively unlocks the inherent generative potential of VAR models while preserving high-fidelity synthesis. Empirical results demonstrate that our approach substantially enhances generative diversity with only neglectable performance influences. Our code will be publicly released at https://github.com/wangtong627/DiverseVAR.

</details>


### [53] [Spanning Tree Autoregressive Visual Generation](https://arxiv.org/abs/2511.17089)
*Sangkyu Lee,Changho Lee,Janghoon Han,Hosung Song,Tackgeun You,Hwasup Lim,Stanley Jungkyu Choi,Honglak Lee,Youngjae Yu*

Main category: cs.CV

TL;DR: 本文提出了Spanning Tree Autoregressive (STAR)建模方法，通过在图像分割格点上采样生成的生成树遍历序，增强图像生成模型的序列顺序灵活性，同时保持采样性能。该方法可结合图像的先验知识（如中心偏置和局部性），适应编辑场景，且无需修改现有自回归架构。


<details>
  <summary>Details</summary>
Motivation: 现有视觉自回归生成模型在引入随机顺序以实现双向上下文时，要么牺牲性能，要么丧失推理时灵活选序列顺序的能力。因此，亟需一种方法能兼顾采样性能和顺序灵活性，特别适用于如图像编辑等需求动态观测顺序的任务。

Method: 作者提出用定义在图像块（patch）上的格点生成均匀生成树，利用BFS（广度优先搜索）遍历获得序列顺序，通过拒绝采样保证观测部分为序列前缀。这种结构化、带有先验的随机顺序替代了全然随机置换，易于结合中心偏置、局部性等知识。整个流程兼容主流自回归架构。

Result: STAR方法在不显著改变现有自回归架构的情况下，能够在维持采样性能的同时，实现高自由度的序列顺序选择（如后缀补齐和局部编辑），突破了以往方法的性能与灵活性瓶颈。实验结果表明该方法效果优良。

Conclusion: STAR为视觉生成任务，在顺序选择灵活性和采样性能之间提供了有效平衡，可广泛适用于需要部分观测和编辑的复杂图像生成场景，无需对主流自回归模型架构做重大调整，具有实际应用和理论价值。

Abstract: We present Spanning Tree Autoregressive (STAR) modeling, which can incorporate prior knowledge of images, such as center bias and locality, to maintain sampling performance while also providing sufficiently flexible sequence orders to accommodate image editing at inference. Approaches that expose randomly permuted sequence orders to conventional autoregressive (AR) models in visual generation for bidirectional context either suffer from a decline in performance or compromise the flexibility in sequence order choice at inference. Instead, STAR utilizes traversal orders of uniform spanning trees sampled in a lattice defined by the positions of image patches. Traversal orders are obtained through breadth-first search, allowing us to efficiently construct a spanning tree whose traversal order ensures that the connected partial observation of the image appears as a prefix in the sequence through rejection sampling. Through the tailored yet structured randomized strategy compared to random permutation, STAR preserves the capability of postfix completion while maintaining sampling performance without any significant changes to the model architecture widely adopted in the language AR modeling.

</details>


### [54] [SPAGS: Sparse-View Articulated Object Reconstruction from Single State via Planar Gaussian Splatting](https://arxiv.org/abs/2511.17092)
*Di Wu,Liu Liu,Xueyu Yuan,Qiaoyu Jun,Wenxiao Chen,Ruilong Yan,Yiming Tang,Liangtu Song*

Main category: cs.CV

TL;DR: 本论文提出了一种新的基于平面Gaussian Splatting的、类别无关的关节物体三维重建方法，仅依赖单一姿态下的稀疏视角RGB图像，实现高质量的零件级表面重建。


<details>
  <summary>Details</summary>
Motivation: 现有的关节物体三维重建方法通常需要多视角或多阶段的数据采集，成本高、实际应用受限，因此亟需一种成本低、输入要求更低的重建方法。

Method: 提出通过高斯信息场自动选取稀疏视角，再将三维高斯压缩为平面高斯以便于法线与深度估计。采用深度平滑正则化及小样本扩散，对平面高斯进行粗到细的优化。为每个高斯元件引入部件分割概率，通过掩码反投影方式迭代更新，增强部分级别重建效果。

Result: 大量实验表明，该方法在合成数据和真实场景上均实现了高保真的零件级表面重建，表现优于现有方法。

Conclusion: 本文所提方法能以极少的输入，实现高质量、类别无关的关节物体三维重建，且在零件级精度上具有明显优势，为实际应用带来了更低成本和更高灵活性。

Abstract: Articulated objects are ubiquitous in daily environments, and their 3D reconstruction holds great significance across various fields. However, existing articulated object reconstruction methods typically require costly inputs such as multi-stage and multi-view observations. To address the limitations, we propose a category-agnostic articulated object reconstruction framework via planar Gaussian Splatting, which only uses sparse-view RGB images from a single state. Specifically, we first introduce a Gaussian information field to perceive the optimal sparse viewpoints from candidate camera poses. Then we compress 3D Gaussians into planar Gaussians to facilitate accurate estimation of normal and depth. The planar Gaussians are optimized in a coarse-to-fine manner through depth smooth regularization and few-shot diffusion. Moreover, we introduce a part segmentation probability for each Gaussian primitive and update them by back-projecting part segmentation masks of renderings. Extensive experimental results demonstrate that our method achieves higher-fidelity part-level surface reconstruction on both synthetic and real-world data than existing methods. Codes will be made publicly available.

</details>


### [55] [Sparse Reasoning is Enough: Biological-Inspired Framework for Video Anomaly Detection with Large Pre-trained Models](https://arxiv.org/abs/2511.17094)
*He Huang,Zixuan Hu,Dongxiao Li,Yao Xiao,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频异常检测框架ReCoVAD，通过灵感来自人类神经系统的双通路机制，实现只需分析少量帧即可高效检测异常，显著减少了计算量并取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 当前利用大规模预训练模型进行视频异常检测虽能无监督实现高精度，但普遍依赖对所有视频帧逐帧推理，导致算力消耗大、响应延迟高。因此，作者关注是否能借助大模型的强大推理能力，采用更稀疏的分析方式来不损失检测能力。

Method: 作者设计了ReCoVAD框架，包含两个主要通路：（1）反射通路，使用轻量化CLIP模块融合视觉特征与原型提示，根据历史记忆快速做出初步判断；（2）意识通路，对新颖帧用中等规模视觉-语言模型生成文本描述和更精细的异常分数，并不时用大语言模型统筹审查与调整知识。

Result: ReCoVAD在UCF-Crime和XD-Violence数据集上，仅分析28.55%与16.04%的帧（大大少于以往方法）即可达到当前最新的无训练检测性能。

Conclusion: 只要利用合适的稀疏推理策略，结合大模型的泛化能力，无需对每一帧都细致分析，也能在视频异常检测中获得高效且准确的表现。

Abstract: Video anomaly detection (VAD) plays a vital role in real-world applications such as security surveillance, autonomous driving, and industrial monitoring. Recent advances in large pre-trained models have opened new opportunities for training-free VAD by leveraging rich prior knowledge and general reasoning capabilities. However, existing studies typically rely on dense frame-level inference, incurring high computational costs and latency. This raises a fundamental question: Is dense reasoning truly necessary when using powerful pre-trained models in VAD systems? To answer this, we propose ReCoVAD, a novel framework inspired by the dual reflex and conscious pathways of the human nervous system, enabling selective frame processing to reduce redundant computation. ReCoVAD consists of two core pathways: (i) a Reflex pathway that uses a lightweight CLIP-based module to fuse visual features with prototype prompts and produce decision vectors, which query a dynamic memory of past frames and anomaly scores for fast response; and (ii) a Conscious pathway that employs a medium-scale vision-language model to generate textual event descriptions and refined anomaly scores for novel frames. It continuously updates the memory and prototype prompts, while an integrated large language model periodically reviews accumulated descriptions to identify unseen anomalies, correct errors, and refine prototypes. Extensive experiments show that ReCoVAD achieves state-of-the-art training-free performance while processing only 28.55\% and 16.04\% of the frames used by previous methods on the UCF-Crime and XD-Violence datasets, demonstrating that sparse reasoning is sufficient for effective large-model-based VAD.

</details>


### [56] [Bridging Visual Affective Gap: Borrowing Textual Knowledge by Learning from Noisy Image-Text Pairs](https://arxiv.org/abs/2511.17103)
*Daiqing Wu,Dongbao Yang,Yu Zhou,Can Ma*

Main category: cs.CV

TL;DR: 该论文提出了一种新的方法，通过利用文本模型的情感知识来提升视觉模型对情感的识别能力，显著弥补了视觉模型与情感类别间的“情感鸿沟”（affective gap），提升了视觉情感识别任务的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉情感识别（VER）因深度神经网络的发展取得进步，但由于视觉模型提取的特征和情感类别间缺少直接联系（即存在“情感鸿沟”），限制了预训练知识的有效性。而文本的情感表达更加明确，为视觉模型引入力场带来新思路。

Method: 提出了分区自适应对比学习（PACL）的方法，利用图像与文本在社交媒体数据中的关系，将不同类型样本分开处理，并为每种类型设计不同的对比学习策略。通过动态生成正负样本对，有效利用了带噪声的样本信息，提升模型性能。

Result: 大规模实验表明，通过PACL方法弥合视觉模型的“情感鸿沟”，各种预训练视觉模型在下游情感相关任务中的表现均显著提升。

Conclusion: 跨模态知识（从文本到视觉）的迁移和创新的对比学习方法，对于提升视觉情感识别准确性有明显效果，为后续相关研究提供了重要借鉴。

Abstract: Visual emotion recognition (VER) is a longstanding field that has garnered increasing attention with the advancement of deep neural networks. Although recent studies have achieved notable improvements by leveraging the knowledge embedded within pre-trained visual models, the lack of direct association between factual-level features and emotional categories, called the "affective gap", limits the applicability of pre-training knowledge for VER tasks. On the contrary, the explicit emotional expression and high information density in textual modality eliminate the "affective gap". Therefore, we propose borrowing the knowledge from the pre-trained textual model to enhance the emotional perception of pre-trained visual models. We focus on the factual and emotional connections between images and texts in noisy social media data, and propose Partitioned Adaptive Contrastive Learning (PACL) to leverage these connections. Specifically, we manage to separate different types of samples and devise distinct contrastive learning strategies for each type. By dynamically constructing negative and positive pairs, we fully exploit the potential of noisy samples. Through comprehensive experiments, we demonstrate that bridging the "affective gap" significantly improves the performance of various pre-trained visual models in downstream emotion-related tasks. Our code is released on https://github.com/wdqqdw/PACL.

</details>


### [57] [ChainV: Atomic Visual Hints Make Multimodal Reasoning Shorter and Better](https://arxiv.org/abs/2511.17106)
*Yuan Zhang,Ming Lu,Junwen Pan,Tao Huang,Kuan Cheng,Qi She,Shanghang Zhang*

Main category: cs.CV

TL;DR: 该论文提出了ChainV框架，通过动态引入视觉提示优化多模态推理模型，提升推理准确率和效率，并显著减少冗余反思和输出长度。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理模型尽管能力强大，但在生成长推理链时普遍存在无谓重复的自我反思，影响推理效率。现有无需训练的链式压缩方法在多模态领域受限于静态视觉参考，收效有限，因此亟需更高效的多模态推理机制。

Method: ChainV框架通过先粗略选择相关视觉区域，再依据平均注意力强度细化为代表性视觉提示，并引入一致性评估机制，衡量提示可靠性，进而指导模型自适应调整反思层级。最终，以伯努利随机过程将选择的视觉提示及其可靠性嵌入推理流程中。

Result: 在数学密集型多步推理任务中，ChainV显著提升了推理准确率、效率，并大幅降低推理延迟（降低51.4%）和输出长度（缩短24.5%）。在MathVista基准下，准确率提升2.3%。

Conclusion: 实验表明，ChainV能有效提升多模态推理的准确性与效率，特别适合涉及多步骤符号推理与视觉提示密切相关的场景。

Abstract: Recent advances in multimodal reasoning models have demonstrated impressive capabilities across text and vision. However, even leading models exhibit redundant self-reflection when generating lengthy reasoning chains. While training-free CoT compression methods have emerged in the LLMs domain, they rely on static visual references and thus provide limited gains for multimodal reasoning. Therefore, we propose ChainV, a framework that dynamically integrates visual hints into the reasoning process, thereby making multimodal reasoning shorter and better. Specifically, ChainV first performs a coarse visual patch selection based on the previous reasoning step, then refines it by identifying the most representative atomic visual hint according to the averaged attention intensity. Additionally, ChainV introduces a consistency-based evaluation mechanism to assess the reliability of the chosen hint, guiding the model to adaptively adjust its level of self-reflection. Eventually, the pixel coordinates of the selected visual hint and its reliability are incorporated into thinking with a Bernoulli stochastic process. Experiments indicate that our method significantly improves reasoning accuracy and efficiency, especially on math-intensive benchmarks where visual hints are crucial for multi-step symbolic reasoning. For example, ChainV achieves $2.3\%$ improvement on the MathVista within MIMO-VL-RL, while reducing inference latency by $51.4\%$ and shortening output token length by $24.5\%$.

</details>


### [58] [PEGS: Physics-Event Enhanced Large Spatiotemporal Motion Reconstruction via 3D Gaussian Splatting](https://arxiv.org/abs/2511.17116)
*Yijun Xu,Jingrui Zhang,Hongyi Liu,Yuhan Chen,Yuanyang Wang,Qingyao Guo,Dingwen Wang,Lei Yu,Chu He*

Main category: cs.CV

TL;DR: PEGS是一种结合物理先验和事件流增强的3D高斯投射系统，能够消除运动模糊并重建大时空尺度下的刚体运动，比主流动态方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模时空范围内刚体运动重建受到建模方式、严重运动模糊和物理一致性不足的限制，导致运动恢复效果不佳。

Method: 提出PEGS框架，融合物理先验和事件流增强于3D高斯投射流程，引入三层监督：一是基于加速度约束提升物理可行性，二是利用事件流提升时序分辨率，三是借助卡尔曼正则器融合多源观测。同时，采用运动感知的模拟退火方法根据动力学状态自适应调度训练过程。

Result: PEGS在新构建的RGB-事件对齐数据集和各种快速刚体运动场景下表现优异，在大时空尺度运动重建上超过主流动态方法。

Conclusion: PEGS显著提升了大时空范围内刚体运动重建的精度和鲁棒性，为涉及快速运动的实际应用提供了更为精准可用的方法和数据集。

Abstract: Reconstruction of rigid motion over large spatiotemporal scales remains a challenging task due to limitations in modeling paradigms, severe motion blur, and insufficient physical consistency. In this work, we propose PEGS, a framework that integrates Physical priors with Event stream enhancement within a 3D Gaussian Splatting pipeline to perform deblurred target-focused modeling and motion recovery. We introduce a cohesive triple-level supervision scheme that enforces physical plausibility via an acceleration constraint, leverages event streams for high-temporal resolution guidance, and employs a Kalman regularizer to fuse multi-source observations. Furthermore, we design a motion-aware simulated annealing strategy that adaptively schedules the training process based on real-time kinematic states. We also contribute the first RGB-Event paired dataset targeting natural, fast rigid motion across diverse scenarios. Experiments show PEGS's superior performance in reconstructing motion over large spatiotemporal scales compared to mainstream dynamic methods.

</details>


### [59] [Off the Planckian Locus: Using 2D Chromaticity to Improve In-Camera Color](https://arxiv.org/abs/2511.17133)
*SaiKiran Tedla,Joshua E. Little,Hakki Can Karaimer,Michael S. Brown*

Main category: cs.CV

TL;DR: 该论文提出用二维色度空间替代传统基于CCT（相关色温）的一维色度映射，并通过轻量级神经网络实现更精确的相机内色彩映射，尤其适用于LED等非Planckian光源。


<details>
  <summary>Details</summary>
Motivation: 传统色彩映射依赖CCT，在LED等现代照明条件下精度有限，因此需要更泛化的方法提升非Planckian光源下的色彩还原准确性。

Method: 作者提出用二维色度特征替代一维CCT表示，并设计轻量多层感知机（MLP）模型，通过包含多种LED的光源标定流程进行训练，实现高效鲁棒的相机色彩映射。

Result: 在多种LED场景下验证，该方法平均能减少22%的色彩再现误差，并能兼容传统光源、多光源场景，同时具备实时部署能力且计算开销极小。

Conclusion: 利用二维色度空间及MLP模型能大幅提升LED等复杂光源下的色彩映射精度，增强了相机系统的泛化能力，符合实践应用需求。

Abstract: Traditional in-camera colorimetric mapping relies on correlated color temperature (CCT)-based interpolation between pre-calibrated transforms optimized for Planckian illuminants such as CIE A and D65. However, modern lighting technologies such as LEDs can deviate substantially from the Planckian locus, exposing the limitations of relying on conventional one-dimensional CCT for illumination characterization. This paper demonstrates that transitioning from 1D CCT (on the Planckian locus) to a 2D chromaticity space (off the Planckian locus) improves colorimetric accuracy across various mapping approaches. In addition, we replace conventional CCT interpolation with a lightweight multi-layer perceptron (MLP) that leverages 2D chromaticity features for robust colorimetric mapping under non-Planckian illuminants. A lightbox-based calibration procedure incorporating representative LED sources is used to train our MLP. Validated across diverse LED lighting, our method reduces angular reproduction error by 22% on average in LED-lit scenes, maintains backward compatibility with traditional illuminants, accommodates multi-illuminant scenes, and supports real-time in-camera deployment with negligible additional computational cost.

</details>


### [60] [A Multi-Stage Optimization Framework for Deploying Learned Image Compression on FPGAs](https://arxiv.org/abs/2511.17135)
*Jiaxun Fang,Li Chen*

Main category: cs.CV

TL;DR: 本文提出了一套多阶段优化框架，使深度学习图像压缩模型更易于部署在FPGA等资源受限硬件上，通过量化、混合精度和剪枝等方案在保证性能的同时大幅提升了硬件效率。实验结果显示最终模型优于现有FPGA实现。


<details>
  <summary>Details</summary>
Motivation: 深度学习图像压缩模型在浮点环境下表现优异，但在算力和存储有限的FPGA等硬件上部署时面临巨大挑战，主要是量化带来的性能损失与硬件效率之间难以兼顾。

Method: 1）提出一种动态范围感知量化（DRAQ）方法，通过统计性激活裁剪和创新的权重量化正则化，有效缓解极端离群点和大动态范围对模型量化的负面影响，实现高精度8位整型模型；2）设计适用于FPGA的逐层混合精度分配算法，为不同层分配最优非均匀比特宽度；3）针对GDN层引入通道剪枝方法，去除冗余通道以进一步减少模型计算量。

Result: 基于GDN的模型应用DRAQ后BD-rate开销从30%大幅下降到6.3%；进一步采用硬件感知方法后，在失真-率性能几乎不变的前提下，计算复杂度降低超过20%。

Conclusion: 所提完整优化方案能显著降低模型在FPGA等硬件上的资源消耗与复杂度，同时保持甚至超过现有FPGA基线的压缩质量，是推动深度学习图像压缩实际部署的重要进展。

Abstract: Deep learning-based image compression (LIC) has achieved state-of-the-art rate-distortion (RD) performance, yet deploying these models on resource-constrained FPGAs remains a major challenge. This work presents a complete, multi-stage optimization framework to bridge the gap between high-performance floating-point models and efficient, hardware-friendly integer-based implementations. First, we address the fundamental problem of quantization-induced performance degradation. We propose a Dynamic Range-Aware Quantization (DRAQ) method that uses statistically-calibrated activation clipping and a novel weight regularization scheme to counteract the effects of extreme data outliers and large dynamic ranges, successfully creating a high-fidelity 8-bit integer model. Second, building on this robust foundation, we introduce two hardware-aware optimization techniques tailored for FPGAs. A progressive mixed-precision search algorithm exploits FPGA flexibility to assign optimal, non-uniform bit-widths to each layer, minimizing complexity while preserving performance. Concurrently, a channel pruning method, adapted to work with the Generalized Divisive Normalization (GDN) layers common in LIC, removes model redundancy by eliminating inactive channels. Our comprehensive experiments show that the foundational DRAQ method reduces the BD-rate overhead of a GDN-based model from $30\%$ to $6.3\%$. The subsequent hardware-aware optimizations further reduce computational complexity by over $20\%$ with negligible impact on RD performance, yielding a final model that is both state-of-the-art in efficiency and superior in quality to existing FPGA-based LIC implementations.

</details>


### [61] [One-Step Diffusion Transformer for Controllable Real-World Image Super-Resolution](https://arxiv.org/abs/2511.17138)
*Yushun Fang,Yuxiang Chen,Shibo Yin,Qiang Hu,Jiangchao Yao,Ya Zhang,Xiaoyun Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合扩散模型和Transformer的新方法ODTSR，能够兼顾真实图像超分辨任务中的保真度与可控性，并在多个数据集和场景下实现了最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的真实图像超分辨（Real-ISR）方法在生成的多样性和可控性之间难以平衡：多步扩散方法导致低保真度，一步方法则牺牲了灵活性和可控性。如何兼顾高保真与可控性是领域难题。

Method: 提出了一种新的ODTSR模型，一步扩散结构，结合Qwen-Image基础引擎，设计了混合噪声视觉流（Noise-hybrid Visual Stream, NVS），并通过调整噪声的方法提升可控性。同时采用Fidelity-aware Adversarial Training（FAA）提升保真和生成的可控性，支持一步推理。

Result: 实验结果表明ODTSR模型在通用真实图像超分辨任务中达到最新最优（SOTA）性能，且无需特定数据集训练就能在如中文字符场景文本图像超分辨等复杂挑战下实现prompt控制。

Conclusion: ODTSR方法有效平衡了扩散超分任务的保真度与灵活控制能力，推进了真实图像超分分辨率技术发展，在多种复杂实际应用场景表现突出。

Abstract: Recent advances in diffusion-based real-world image super-resolution (Real-ISR) have demonstrated remarkable perceptual quality, yet the balance between fidelity and controllability remains a problem: multi-step diffusion-based methods suffer from generative diversity and randomness, resulting in low fidelity, while one-step methods lose control flexibility due to fidelity-specific finetuning. In this paper, we present ODTSR, a one-step diffusion transformer based on Qwen-Image that performs Real-ISR considering fidelity and controllability simultaneously: a newly introduced visual stream receives low-quality images (LQ) with adjustable noise (Control Noise), and the original visual stream receives LQs with consistent noise (Prior Noise), forming the Noise-hybrid Visual Stream (NVS) design. ODTSR further employs Fidelity-aware Adversarial Training (FAA) to enhance controllability and achieve one-step inference. Extensive experiments demonstrate that ODTSR not only achieves state-of-the-art (SOTA) performance on generic Real-ISR, but also enables prompt controllability on challenging scenarios such as real-world scene text image super-resolution (STISR) of Chinese characters without training on specific datasets.

</details>


### [62] [Learning to Look Closer: A New Instance-Wise Loss for Small Cerebral Lesion Segmentation](https://arxiv.org/abs/2511.17146)
*Luc Bouteille,Alexander Jaus,Jens Kleesiek,Rainer Stiefelhagen,Lukas Heine*

Main category: cs.CV

TL;DR: 本文提出了基于CC-Metrics框架的CC-DiceCE损失函数，用于提升医学图像分割中特别是小病灶的检测率，并与现有方法在nnU-Net框架下进行了对比，结果表明新方法提升了检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割损失函数如Dice对小病灶不敏感，导致小病灶常常被漏分割，因此需要设计对小体积目标更友好的损失函数。

Method: 提出CC-DiceCE损失函数，将其与blob loss和DiceCE基线进行对比，在nnU-Net框架和多数据集上评估各方法分割效果。

Result: CC-DiceCE提升了检出率(recall)，分割性能几乎无损，只是稍微增加了一些假阳性。同时，在多数据集下该方法的性能普遍优于blob loss。

Conclusion: CC-DiceCE是一种有效提升小病灶检测并保持整体分割精度的损失函数，优于现有的blob loss。

Abstract: Traditional loss functions in medical image segmentation, such as Dice, often under-segment small lesions because their small relative volume contributes negligibly to the overall loss. To address this, instance-wise loss functions and metrics have been proposed to evaluate segmentation quality on a per-lesion basis. We introduce CC-DiceCE, a loss function based on the CC-Metrics framework, and compare it with the existing blob loss. Both are benchmarked against a DiceCE baseline within the nnU-Net framework, which provides a robust and standardized setup. We find that CC-DiceCE loss increases detection (recall) with minimal to no degradation in segmentation performance, albeit at the cost of slightly more false positives. Furthermore, our multi-dataset study shows that CC-DiceCE generally outperforms blob loss.

</details>


### [63] [A lightweight detector for real-time detection of remote sensing images](https://arxiv.org/abs/2511.17147)
*Qianyi Wang,Guoqiang Ren*

Main category: cs.CV

TL;DR: 提出DMG-YOLO，一种面向遥感图像中小目标检测的轻量级实时检测器，结合局部与全球特征提取与融合，提升检测效率与精度。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中的小目标检测因目标尺寸小、密集且需兼顾实时性与精度而具有挑战性，现有方法往往难以在效率与精度间取得平衡。

Method: 提出带有双分支特征提取(DFE)模块的骨干网络，一支使用深度可分离卷积提取局部特征，另一支通过带门控机制的视觉Transformer抽取全局信息。同时，设计多尺度特征融合(MFF)模块利用扩张卷积强化细节保留。Neck部分引入全局与局部融合特征金字塔网络(GLAFPN)进一步提升特征融合能力。

Result: 在VisDrone2019和NWPU VHR-10数据集上进行了实验，DMG-YOLO在mAP、模型大小等关键指标上取得了具有竞争力的表现。

Conclusion: DMG-YOLO兼具高效率与优异的小目标检测精度，为遥感图像实时小目标检测任务提供了更优解决方案。

Abstract: Remote sensing imagery is widely used across various fields, yet real-time detection remains challenging due to the prevalence of small objects and the need to balance accuracy with efficiency. To address this, we propose DMG-YOLO, a lightweight real-time detector tailored for small object detection in remote sensing images. Specifically, we design a Dual-branch Feature Extraction (DFE) module in the backbone, which partitions feature maps into two parallel branches: one extracts local features via depthwise separable convolutions, and the other captures global context using a vision transformer with a gating mechanism. Additionally, a Multi-scale Feature Fusion (MFF) module with dilated convolutions enhances multi-scale integration while preserving fine details. In the neck, we introduce the Global and Local Aggregate Feature Pyramid Network (GLAFPN) to further boost small object detection through global-local feature fusion. Extensive experiments on the VisDrone2019 and NWPU VHR-10 datasets show that DMG-YOLO achieves competitive performance in terms of mAP, model size, and other key metrics.

</details>


### [64] [DiffRefiner: Coarse to Fine Trajectory Planning via Diffusion Refinement with Semantic Interaction for End to End Autonomous Driving](https://arxiv.org/abs/2511.17150)
*Liuhan Yin,Runkun Ju,Guodong Guo,Erkang Cheng*

Main category: cs.CV

TL;DR: DiffRefiner是一种两阶段自动驾驶轨迹预测框架，将判别式与生成式方法结合，提升了现有扩散模型的精度，并在主流数据集上刷新纪录。


<details>
  <summary>Details</summary>
Motivation: 现有生成式轨迹预测方法(如扩散模型)依赖于人工锚点或随机噪声去噪，有改进空间，尤其在轨迹多样性与场景适应性方面。作者希望通过结合判别式与生成式方法克服上述不足。

Method: 1. 第一阶段：用基于transformer的Proposal Decoder，从传感器输入出发，基于预定义轨迹锚点生成初步预测轨迹。2. 第二阶段：用Diffusion Refiner对初步轨迹进行迭代去噪和细化。同时，设计了更细粒度的去噪解码器以增强与环境的匹配性。

Result: DiffRefiner在NAVSIM v2与Bench2Drive两个公开基准上取得了新的最佳成绩（87.4 EPDMS，87.1 DS，71.4 SR），并通过消融实验证明了组件有效性。

Conclusion: DiffRefiner有效整合了判别式与生成式方法，显著提升了扩散模型在自动驾驶轨迹预测中的表现，同时增强了对环境的适应性。

Abstract: Unlike discriminative approaches in autonomous driving that predict a fixed set of candidate trajectories of the ego vehicle, generative methods, such as diffusion models, learn the underlying distribution of future motion, enabling more flexible trajectory prediction. However, since these methods typically rely on denoising human-crafted trajectory anchors or random noise, there remains significant room for improvement. In this paper, we propose DiffRefiner, a novel two-stage trajectory prediction framework. The first stage uses a transformer-based Proposal Decoder to generate coarse trajectory predictions by regressing from sensor inputs using predefined trajectory anchors. The second stage applies a Diffusion Refiner that iteratively denoises and refines these initial predictions. In this way, we enhance the performance of diffusion-based planning by incorporating a discriminative trajectory proposal module, which provides strong guidance for the generative refinement process. Furthermore, we design a fine-grained denoising decoder to enhance scene compliance, enabling more accurate trajectory prediction through enhanced alignment with the surrounding environment. Experimental results demonstrate that DiffRefiner achieves state-of-the-art performance, attaining 87.4 EPDMS on NAVSIM v2, and 87.1 DS along with 71.4 SR on Bench2Drive, thereby setting new records on both public benchmarks. The effectiveness of each component is validated via ablation studies as well.

</details>


### [65] [UI-Styler: Ultrasound Image Style Transfer with Class-Aware Prompts for Cross-Device Diagnosis Using a Frozen Black-Box Inference Network](https://arxiv.org/abs/2511.17155)
*Nhat-Tuong Do-Tran,Ngoc-Hoang-Lam Le,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种专为超声图像设计的类感知无配对图像风格迁移方法UI-Styler，用于优化跨设备的下游推理表现。


<details>
  <summary>Details</summary>
Motivation: 不同设备采集的超声图像存在域差异，导致现有推理模型在跨设备应用时性能下降，亟需有效的无配对图像迁移方法用于缓解域偏移。

Method: 提出UI-Styler框架，结合纹理模式匹配机制将目标域纹理迁移到源图像，同时保留结构内容，并引入基于目标域伪标签的类感知提示策略，提升诊断类别的语义对齐。

Result: UI-Styler在跨设备超声图像领域的分类与分割等任务上均优于现有方法，在分布距离及下游任务指标上达到了最新最好表现。

Conclusion: UI-Styler能够实现超声图像在跨设备场景中的精准类语义对齐与风格迁移，有效提升了下游推理模型的泛化性和诊断准确率。

Abstract: The appearance of ultrasound images varies across acquisition devices, causing domain shifts that degrade the performance of fixed black-box downstream inference models when reused. To mitigate this issue, it is practical to develop unpaired image translation (UIT) methods that effectively align the statistical distributions between source and target domains, particularly under the constraint of a reused inference-blackbox setting. However, existing UIT approaches often overlook class-specific semantic alignment during domain adaptation, resulting in misaligned content-class mappings that can impair diagnostic accuracy. To address this limitation, we propose UI-Styler, a novel ultrasound-specific, class-aware image style transfer framework. UI-Styler leverages a pattern-matching mechanism to transfer texture patterns embedded in the target images onto source images while preserving the source structural content. In addition, we introduce a class-aware prompting strategy guided by pseudo labels of the target domain, which enforces accurate semantic alignment with diagnostic categories. Extensive experiments on ultrasound cross-device tasks demonstrate that UI-Styler consistently outperforms existing UIT methods, achieving state-of-the-art performance in distribution distance and downstream tasks, such as classification and segmentation.

</details>


### [66] [FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle](https://arxiv.org/abs/2511.17171)
*Mario Markov,Stefan Maria Ailuro,Luc Van Gool,Konrad Schindler,Danda Pani Paudel*

Main category: cs.CV

TL;DR: 本文提出FireScope-Bench数据集和FireScope模型，实现更具因果推理与多模态理解的全球性野火风险预测。实验表明该方法在跨洲泛化和解释性上显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有野火风险预测方法在推理能力和多模态泛化上存在不足，难以可靠推广至不同地区。

Method: 作者构建了FireScope-Bench大规模遥感和气候数据集，结合人专家标注和真实事件。提出FireScope模型，融合视觉大模型（VLM）、强化学习和视觉监督，通过生成带推理过程的风险栅格图来提升泛化和可解释性。

Result: FireScope模型在美国训练，在欧洲测试时显著超越现有关方法。专家反馈和自动分析均表明其推理过程合乎逻辑且易于理解。

Conclusion: 通过推理增强的多模态野火风险建模可提升方法的泛化性与可解释性。FireScope-Bench有望成为该领域基础性基准。

Abstract: Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.

</details>


### [67] [Investigating self-supervised representations for audio-visual deepfake detection](https://arxiv.org/abs/2511.17181)
*Dragos-Alexandru Boldisor,Stefan Smeu,Dan Oneata,Elisabeta Oneata*

Main category: cs.CV

TL;DR: 本文系统评估了自监督特征在音视频深度伪造检测中的表现，发现其能捕捉有用信息但跨数据集泛化较差。


<details>
  <summary>Details</summary>
Motivation: 当前自监督表示在视觉和语音任务中表现优异，但其在音视频深度伪造检测领域的潜力尚未被充分探索，且以往研究通常孤立使用或嵌入于复杂结构中，缺乏系统性比较。

Method: 作者对自监督特征在音频、视频和多模态等多种模态，以及唇动、通用视觉内容等不同领域下进行了系统评估，包括检测效果、信息解释性与跨模态互补性，并比较了不同特征的表现。

Result: 大多数自监督特征可捕捉与深度伪造相关的信息，且这些信息在模态间具有互补性。模型主要关注语义相关区域而非偶然伪影，但所有方法均未能实现稳定的跨数据集泛化。泛化失败主要由于数据集特性，而非特征本身过于关注表面模式。

Conclusion: 自监督特征对深度伪造检测具有较大潜力但也存在挑战，虽然能学习有意义的模式，但要实现鲁棒的跨域性能仍需进一步研究解决泛化能力问题。

Abstract: Self-supervised representations excel at many vision and speech tasks, but their potential for audio-visual deepfake detection remains underexplored. Unlike prior work that uses these features in isolation or buried within complex architectures, we systematically evaluate them across modalities (audio, video, multimodal) and domains (lip movements, generic visual content). We assess three key dimensions: detection effectiveness, interpretability of encoded information, and cross-modal complementarity. We find that most self-supervised features capture deepfake-relevant information, and that this information is complementary. Moreover, models primarily attend to semantically meaningful regions rather than spurious artifacts. Yet none generalize reliably across datasets. This generalization failure likely stems from dataset characteristics, not from the features themselves latching onto superficial patterns. These results expose both the promise and fundamental challenges of self-supervised representations for deepfake detection: while they learn meaningful patterns, achieving robust cross-domain performance remains elusive.

</details>


### [68] [Navigating in the Dark: A Multimodal Framework and Dataset for Nighttime Traffic Sign Recognition](https://arxiv.org/abs/2511.17183)
*Aditya Mishra,Akshay Agarwal,Haroon Lone*

Main category: cs.CV

TL;DR: 本论文针对夜间交通标志识别难题，提出了大规模夜间交通标志数据集INTSD，并提出了新模型LENS-Net来提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 夜间交通标志因照明差和噪声高难以识别，且公开夜间数据集稀缺。现有方法在低光照场景下表现不佳且未有效利用多模态信息。

Method: 1. 构建了包含41类交通标志、涵盖多种照明和天气的印度夜间街景大数据集INTSD。2. 提出LENS-Net模型，采用自适应图像增强检测器进行图像增强与标志定位，再用融合跨模态注意力和图结构推理的CLIP-GCNN分类器实现鲁棒识别。

Result: 通过在INTSD数据上的大量实验，LENS-Net显著优于现有主流检测和分类方法。消融实验验证了模型关键组件的有效性。

Conclusion: INTSD为夜间交通标志识别提供了新基准，LENS-Net模型展示了更强鲁棒性。数据集和代码公开，有利于推动相关研究。

Abstract: Traffic signboards are vital for road safety and intelligent transportation systems, enabling navigation and autonomous driving. Yet, recognizing traffic signs at night remains challenging due to visual noise and scarcity of public nighttime datasets. Despite advances in vision architectures, existing methods struggle with robustness under low illumination and fail to leverage complementary mutlimodal cues effectively. To overcome these limitations, firstly, we introduce INTSD, a large-scale dataset comprising street-level night-time images of traffic signboards collected across diverse regions of India. The dataset spans 41 traffic signboard classes captured under varying lighting and weather conditions, providing a comprehensive benchmark for both detection and classification tasks. To benchmark INTSD for night-time sign recognition, we conduct extensive evaluations using state-of-the-art detection and classification models. Secondly, we propose LENS-Net, which integrates an adaptive image enhancement detector for joint illumination correction and sign localization, followed by a structured multimodal CLIP-GCNN classifier that leverages cross-modal attention and graph-based reasoning for robust and semantically consistent recognition. Our method surpasses existing frameworks, with ablation studies confirming the effectiveness of its key components. The dataset and code for LENS-Net is publicly available for research.

</details>


### [69] [PostCam: Camera-Controllable Novel-View Video Generation with Query-Shared Cross-Attention](https://arxiv.org/abs/2511.17185)
*Yipeng Chen,Zhichao Ye,Zhenzhou Fang,Xinyu Chen,Xiaoyu Zhang,Jialing Liu,Nan Wang,Haomin Liu,Guofeng Zhang*

Main category: cs.CV

TL;DR: PostCam是一种新的视频生成框架，支持在已有动态场景视频中事后灵活编辑摄像机运动轨迹。相比现有方法，该方法在摄像机控制精度和生成视频质量上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的视频再拍摄方法在引入摄像机运动时准确性不高，控摄能力有限，导致生成视频既难以精准控制运动，也难以保留源视频的细节。迫切需要更有效的摄像机运动操控和视角切换方案。

Method: PostCam提出了query-shared cross-attention模块，将六自由度摄像机位姿和2D渲染帧作为控制信号共同建模，通过特征空间融合提取运动线索。此外，采用两阶段训练，先用姿态信息学习粗糙摄像机控制，再融合视觉信息精细优化运动及提升视觉效果。

Result: 在真实和合成数据集上，PostCam在摄像机控制精度和视角一致性上比现有方法提升超过20%，并得到最高的视频生成质量。

Conclusion: PostCam显著提升了动态场景中新视角视频生成的摄像机控制精度和视频质量，证明了其在后期灵活视角编辑上的有效性和优越性。

Abstract: We propose PostCam, a framework for novel-view video generation that enables post-capture editing of camera trajectories in dynamic scenes. We find that existing video recapture methods suffer from suboptimal camera motion injection strategies; such suboptimal designs not only limit camera control precision but also result in generated videos that fail to preserve fine visual details from the source video. To achieve more accurate and flexible motion manipulation, PostCam introduces a query-shared cross-attention module. It integrates two distinct forms of control signals: the 6-DoF camera poses and the 2D rendered video frames. By fusing them into a unified representation within a shared feature space, our model can extract underlying motion cues, which enhances both control precision and generation quality. Furthermore, we adopt a two-stage training strategy: the model first learns coarse camera control from pose inputs, and then incorporates visual information to refine motion accuracy and enhance visual fidelity. Experiments on both real-world and synthetic datasets demonstrate that PostCam outperforms state-of-the-art methods by over 20% in camera control precision and view consistency, while achieving the highest video generation quality. Our project webpage is publicly available at: https://cccqaq.github.io/PostCam.github.io/

</details>


### [70] [Real Noise Decoupling for Hyperspectral Image Denoising](https://arxiv.org/abs/2511.17196)
*Yingkai Zhang,Tao Zhang,Jing Nie,Ying Fu*

Main category: cs.CV

TL;DR: 本文提出了一种多阶段噪声解耦框架，通过将高光谱图像中的复杂噪声分为显式和隐式两部分，分别建模和处理，有效提升了高光谱图像去噪的效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像去噪对于提高数据质量至关重要，但实际采集到的噪声复杂且难以准确建模，导致传统方法效果有限，因此急需提高对复杂噪声处理能力的新方法。

Method: 1）将复杂噪声分解为显式噪声和隐式噪声两种。2）对显式噪声，利用已有噪声模型生成伪配对数据，对去噪网络进行预训练，赋予其处理显式噪声的能力。3）对隐式噪声，设计高频小波引导网络，利用预训练模块的先验知识，自适应提取高频特征，针对性去除隐式噪声。4）采用多阶段学习策略，先单独预训练，再进行联合微调，优化网络整体性能。

Result: 在公开数据集和自采集数据上进行了大量实验，所提出的框架在处理复杂真实噪声方面超越了现有先进算法，显著提升了高光谱图像的质量。

Conclusion: 多阶段噪声解耦框架通过分解噪声和多阶段学习有效提升了高光谱图像去噪能力，是应对实际复杂噪声的一种有效新方案。

Abstract: Hyperspectral image (HSI) denoising is a crucial step in enhancing the quality of HSIs. Noise modeling methods can fit noise distributions to generate synthetic HSIs to train denoising networks. However, the noise in captured HSIs is usually complex and difficult to model accurately, which significantly limits the effectiveness of these approaches. In this paper, we propose a multi-stage noise-decoupling framework that decomposes complex noise into explicitly modeled and implicitly modeled components. This decoupling reduces the complexity of noise and enhances the learnability of HSI denoising methods when applied to real paired data. Specifically, for explicitly modeled noise, we utilize an existing noise model to generate paired data for pre-training a denoising network, equipping it with prior knowledge to handle the explicitly modeled noise effectively. For implicitly modeled noise, we introduce a high-frequency wavelet guided network. Leveraging the prior knowledge from the pre-trained module, this network adaptively extracts high-frequency features to target and remove the implicitly modeled noise from real paired HSIs. Furthermore, to effectively eliminate all noise components and mitigate error accumulation across stages, a multi-stage learning strategy, comprising separate pre-training and joint fine-tuning, is employed to optimize the entire framework. Extensive experiments on public and our captured datasets demonstrate that our proposed framework outperforms state-of-the-art methods, effectively handling complex real-world noise and significantly enhancing HSI quality.

</details>


### [71] [VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation](https://arxiv.org/abs/2511.17199)
*Hanyu Zhou,Chuanhao Ma,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出VLA-4D模型，通过融合时间与空间信息，实现了更时空一致的机器人感知-语言-动作系统，显著提升了机器人操作的精细与连贯性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作(VLA)模型在机器人通用操作任务上展现潜力，但在需要时空连贯的精细操作场景下表现有限，主要受限于对动作的时空表示不足。特别是，现有方法更关注空间精度，忽视了对动作执行过程的时间维度控制。为此，作者提出提升时空表达能力以改善机器人操作连贯性的需求。

Method: 作者提出VLA-4D模型，通过两项关键设计解决问题：1）4D感知视觉表示，将时间维度嵌入3D位置并通过交叉注意力与视觉特征融合，形成统一的4D视觉表达；2）时空动作表示，将动作的空间表示扩展为包含时间信息，从而实现时空计划，并进一步将多模态信息与大型语言模型对齐，实现时空一致的动作预测。同时，扩展VLA数据集，增加了时间标注以提升模型训练效果。

Result: 实验结果显示，VLA-4D在多种机器人操控任务中均取得了优于现有方法的表现，能更好地实现空间平滑和动作过程的时间连贯性。

Conclusion: VLA-4D通过创新的4D感知视觉和时空动作表示，有效提升了机器人在复杂任务中的操作精细度和连贯性，对提升通用机器人操控能力具有重要意义。

Abstract: Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.

</details>


### [72] [Continual Alignment for SAM: Rethinking Foundation Models for Medical Image Segmentation in Continual Learning](https://arxiv.org/abs/2511.17201)
*Jiayi Wang,Wei Dai,Haoyu Wang,Sihan Yang,Haixia Bi,Jian Sun*

Main category: cs.CV

TL;DR: 本文提出一种面向医疗图像分割的持续学习方法CA-SAM，结合高效的对齐层（Alignment Layer）和SAM模型，兼顾准确性与计算效率，有效缓解遗忘现象并取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 由于不同医疗机构间存在异构的隐私政策，无法直接联合训练共享数据集，因此需要在数据流中进行持续学习，并且避免遗忘历史知识。SAM模型具备强零样本能力，但体量大，部署受限，因此需新的高效适配方法。

Method: 提出Alignment Layer（对齐层），可插拔且轻量级，用于对SAM的特征进行分布对齐，适应不同类型的医疗影像。在此基础上提出CA-SAM，结合对齐层动态适配和SAM的零样本先验，实现医疗分割任务中的持续学习，有效缓解灾难性遗忘。

Result: 在9个医疗分割数据集上的持续学习实验中，CA-SAM均取得了最新最高性能（state-of-the-art），证明了方法的有效性。

Conclusion: 只要平衡了计算效率和性能，SAM范式极具前景。引入Alignment Layer和持续对齐机制，使医疗分割中的持续学习成为可能，并减少遗忘问题。

Abstract: In medical image segmentation, heterogeneous privacy policies across institutions often make joint training on pooled datasets infeasible, motivating continual image segmentation-learning from data streams without catastrophic forgetting. While the Segment Anything Model (SAM) offers strong zero-shot priors and has been widely fine-tuned across downstream tasks, its large parameter count and computational overhead challenge practical deployment. This paper demonstrates that the SAM paradigm is highly promising once its computational efficiency and performance can be balanced. To this end, we introduce the Alignment Layer, a lightweight, plug-and-play module which aligns encoder-decoder feature distributions to efficiently adapt SAM to specific medical images, improving accuracy while reducing computation. Building on SAM and the Alignment Layer, we then propose Continual Alignment for SAM (CA-SAM), a continual learning strategy that automatically adapts the appropriate Alignment Layer to mitigate catastrophic forgetting, while leveraging SAM's zero-shot priors to preserve strong performance on unseen medical datasets. Experimented across nine medical segmentation datasets under continual-learning scenario, CA-SAM achieves state-of-the-art performance. Our code, models and datasets will be released on \mbox{https://github.com/azzzzyo/Continual-Alignment-for-SAM.}

</details>


### [73] [Scaling Self-Supervised and Cross-Modal Pretraining for Volumetric CT Transformers](https://arxiv.org/abs/2511.17209)
*Cris Claessens,Christiaan Viviers,Giacomo D'Amicantonio,Egor Bondarev,Fons van der Sommen*

Main category: cs.CV

TL;DR: SPECTRE 是一种专为体积CT设计的全新Transformer基础模型，通过自监督和视觉-语言预训练实现通用CT表征，并在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的CT表示学习受限于极高的数据规模、空间各向异性及弱监督等挑战，传统Transformer与对比学习在体积CT场景下表现不佳。因此迫切需要开发一种能有效应对上述难题的强大基础模型。

Method: SPECTRE 利用规模化的3D Vision Transformer，融合DINO式自蒸馏和SigLIP视觉-语言对齐方法，通过局部Transformer提取高分辨率体积特征，全球Transformer建模全扫描上下文，全程仅用公开CT数据进行预训练。

Result: SPECTRE 在多个CT基准上零样本和微调均优于过往基础模型，并能学得既有几何一致性又有临床语义的特征。

Conclusion: SPECTRE 证明了在不依赖私有数据的前提下，通过创新的Transformer结构与自监督/跨模态预训练，可获得高性能且泛化性强的医学三维成像基础模型。

Abstract: We introduce SPECTRE, a fully transformer-based foundation model for volumetric computed tomography (CT). Our Self-Supervised & Cross-Modal Pretraining for CT Representation Extraction (SPECTRE) approach utilizes scalable 3D Vision Transformer architectures and modern self-supervised and vision-language pretraining strategies to learn general-purpose CT representations. Volumetric CT poses unique challenges, such as extreme token scaling, geometric anisotropy, and weak or noisy clinical supervision, that make standard transformer and contrastive learning recipes ineffective out of the box. The framework jointly optimizes a local transformer for high-resolution volumetric feature extraction and a global transformer for whole-scan context modeling, making large-scale 3D attention computationally tractable. Notably, SPECTRE is trained exclusively on openly available CT datasets, demonstrating that high-performing, generalizable representations can be achieved without relying on private data. Pretraining combines DINO-style self-distillation with SigLIP-based vision-language alignment using paired radiology reports, yielding features that are both geometrically consistent and clinically meaningful. Across multiple CT benchmarks, SPECTRE consistently outperforms prior CT foundation models in both zero-shot and fine-tuned settings, establishing SPECTRE as a scalable, open, and fully transformer-based foundation model for 3D medical imaging.

</details>


### [74] [FisheyeGaussianLift: BEV Feature Lifting for Surround-View Fisheye Camera Perception](https://arxiv.org/abs/2511.17210)
*Shubham Sonarghare,Prasad Deshpande,Ciaran Hogan,Deepika-Rani Kaliappan-Mahalingam,Ganesh Sistu*

Main category: cs.CV

TL;DR: 本文提出一种针对鱼眼图像的BEV（鸟瞰视角）语义分割框架，能够高效处理多摄像头、高分辨率鱼眼图像，在无需去畸变或透视矫正的情况下获得高精度分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以直接处理鱼眼图像的极端非线性畸变、遮挡和深度模糊等问题，实现高精度BEV语义分割存在很大挑战。为解决这一瓶颈，本文提出了一种失真感知的处理框架。

Method: 该方法利用标定后的几何反投影和每像素深度分布估计，将每个像素提升至三维空间，并采用高斯参数化建模空间均值和各向异性协方差，从而显式建模几何不确定性。所有投影到三维的高斯分布通过可微分溅射操作融合为BEV表示，生成连续的、不确定性感知的语义分割图。

Result: 在复杂的停车场和城市驾驶场景下，框架在严重鱼眼畸变和多样化环境条件下，针对可通行区域分割获得了87.75%的IoU，对车辆分割为57.26%的IoU，表现出较强的分割性能。

Conclusion: 所提出方法无需传统的图像去畸变和透视矫正，能够直接对多鱼眼摄像头的输入进行高精度BEV分割，尤其适用于自动驾驶等实际场景。

Abstract: Accurate BEV semantic segmentation from fisheye imagery remains challenging due to extreme non-linear distortion, occlusion, and depth ambiguity inherent to wide-angle projections. We present a distortion-aware BEV segmentation framework that directly processes multi-camera high-resolution fisheye images,utilizing calibrated geometric unprojection and per-pixel depth distribution estimation. Each image pixel is lifted into 3D space via Gaussian parameterization, predicting spatial means and anisotropic covariances to explicitly model geometric uncertainty. The projected 3D Gaussians are fused into a BEV representation via differentiable splatting, producing continuous, uncertainty-aware semantic maps without requiring undistortion or perspective rectification. Extensive experiments demonstrate strong segmentation performance on complex parking and urban driving scenarios, achieving IoU scores of 87.75% for drivable regions and 57.26% for vehicles under severe fisheye distortion and diverse environmental conditions.

</details>


### [75] [Dual-domain Adaptation Networks for Realistic Image Super-resolution](https://arxiv.org/abs/2511.17217)
*Chaowei Fang,Bolin Fu,De Cheng,Lechao Cheng,Guanbin Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为Dual-domain Adaptation Networks的新方法，有效适配由合成数据训练的图像超分辨率模型到真实场景，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 真实图像超分辨率因其复杂的退化特性和数据稀缺，影响了高分辨率图像重建的精度。现有方法在真实大规模低高分数据有限时，效果有限。作者希望利用大量合成数据预训练模型的先验知识，提升对真实图像的适应能力。

Method: 提出Dual-domain Adaptation Networks方法：空间域通过选择性更新模型参数，并用低秩适配调整冻结参数；频域分支将输入的频谱信息与主骨干网络中间特征结合，推断高分辨率频率信息。这两个分支联合提升对真实图像的重建能力。

Result: 在RealSR、D2CRealSR、DRealSR等真实图像超分辨率公开基准上，该方法在指标上明显超过现有先进模型。

Conclusion: Dual-domain Adaptation Networks能充分利用预训练模型知识，实现从合成到真实超分辨率任务的高效迁移，在真实图像恢复等应用中具有较大潜力。

Abstract: Realistic image super-resolution (SR) focuses on transforming real-world low-resolution (LR) images into high-resolution (HR) ones, handling more complex degradation patterns than synthetic SR tasks. This is critical for applications like surveillance, medical imaging, and consumer electronics. However, current methods struggle with limited real-world LR-HR data, impacting the learning of basic image features. Pre-trained SR models from large-scale synthetic datasets offer valuable prior knowledge, which can improve generalization, speed up training, and reduce the need for extensive real-world data in realistic SR tasks. In this paper, we introduce a novel approach, Dual-domain Adaptation Networks, which is able to efficiently adapt pre-trained image SR models from simulated to real-world datasets. To achieve this target, we first set up a spatial-domain adaptation strategy through selectively updating parameters of pre-trained models and employing the low-rank adaptation technique to adjust frozen parameters. Recognizing that image super-resolution involves recovering high-frequency components, we further integrate a frequency domain adaptation branch into the adapted model, which combines the spectral data of the input and the spatial-domain backbone's intermediate features to infer HR frequency maps, enhancing the SR result. Experimental evaluations on public realistic image SR benchmarks, including RealSR, D2CRealSR, and DRealSR, demonstrate the superiority of our proposed method over existing state-of-the-art models. Codes are available at: https://github.com/dummerchen/DAN.

</details>


### [76] [Equivariant-Aware Structured Pruning for Efficient Edge Deployment: A Comprehensive Framework with Adaptive Fine-Tuning](https://arxiv.org/abs/2511.17242)
*Mohammed Alnemari*

Main category: cs.CV

TL;DR: 本文提出了一种结合群等变卷积神经网络（G-CNNs）与等变性感知结构化剪枝的新型框架，实现了紧凑且变换不变的模型，适用于算力受限的环境。


<details>
  <summary>Details</summary>
Motivation: 现有G-CNN虽然对几何变换有鲁棒性，但参数多、计算量大，不适用于资源有限的实际部署。作者希望在保持等变性的前提下，进行极高效的模型压缩。

Method: 方法上，作者通过e2cnn库实现了C4循环群（对旋转等变），在保持变换鲁棒性的同时降低了计算开销。提出面向等变性的结构化剪枝，结合对e2cnn层结构分析和全连接神经元级别剪枝。为缓解精度下降，采用自适应微调机制（自动检测大于2%的精度降时进行微调），并结合早停和学习率调度。最后，框架还包括动态INT8量化，并支持训练、知识蒸馏、结构化剪枝、微调和量化的全流程。

Result: 在EuroSAT（卫星影像）、CIFAR-10和Rotated MNIST等数据集上评估该方法，实现了29.3%的参数减少，同时通过微调显著回收了精度，确保在参数大幅压缩的同时保持几何鲁棒性。

Conclusion: 结构化剪枝能大幅压缩等变网络体积又不显著损失几何鲁棒性，提供了可复现、应用友好的优化管线，尤适合卫星影像及几何视觉等实际任务，将群论网络设计与工程部署需求有效连接。

Abstract: This paper presents a novel framework combining group equivariant convolutional neural networks (G-CNNs) with equivariant-aware structured pruning to produce compact, transformation-invariant models for resource-constrained environments. Equivariance to rotations is achieved through the C4 cyclic group via the e2cnn library,enabling consistent performance under geometric transformations while reducing computational overhead.
  Our approach introduces structured pruning that preserves equivariant properties by analyzing e2cnn layer structure and applying neuron-level pruning to fully connected components. To mitigate accuracy degradation, we implement adaptive fine-tuning that automatically triggers when accuracy drop exceeds 2%, using early stopping and learning rate scheduling for efficient recovery. The framework includes dynamic INT8 quantization and a comprehensive pipeline encompassing training, knowledge distillation, structured pruning, fine-tuning, and quantization.
  We evaluate our method on satellite imagery (EuroSAT) and standard benchmarks (CIFAR-10, Rotated MNIST) demonstrating effectiveness across diverse domains. Experimental results show 29.3% parameter reduction with significant accuracy recovery, demonstrating that structured pruning of equivariant networks achieves substantial compression while maintaining geometric robustness. Our pipeline provides a reproducible framework for optimizing equivariant models, bridging the gap between group-theoretic network design and practical deployment constraints, with particular relevance to satellite imagery analysis and geometric vision tasks.

</details>


### [77] [Blind Deconvolution for Color Images Using Normalized Quaternion Kernels](https://arxiv.org/abs/2511.17253)
*Yuming Yang,Michael K. Ng,Zhigang Jia,Wei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种针对彩色图像盲反卷积的新方法，通过利用四元数卷积核设计新的保真项，有效去除模糊，提高了图像去模糊效果。


<details>
  <summary>Details</summary>
Motivation: 目前彩色图像盲反卷积方法通常将彩色图像转为灰度，或分别处理各颜色通道，忽略了通道间的关联性，导致复原效果受限。

Method: 论文设计了专门针对彩色图像盲反卷积的四元数保真项，利用四元数卷积核同时建模整体模糊和各颜色通道间复杂依赖性，并引入归一化四元数卷积核以保持图像亮度。

Result: 在真实彩色模糊图像数据集上进行了大量实验，结果表明该方法能有效去除伪影，显著提升去模糊效果。

Conclusion: 该方法在彩色图像盲反卷积任务中表现出较强潜力，为相关领域提供了一种有力的新工具。

Abstract: In this work, we address the challenging problem of blind deconvolution for color images. Existing methods often convert color images to grayscale or process each color channel separately, which overlooking the relationships between color channels. To handle this issue, we formulate a novel quaternion fidelity term designed specifically for color image blind deconvolution. This fidelity term leverages the properties of quaternion convolution kernel, which consists of four kernels: one that functions similarly to a non-negative convolution kernel to capture the overall blur, and three additional convolution kernels without constraints corresponding to red, green and blue channels respectively model their unknown interdependencies. In order to preserve image intensity, we propose to use the normalized quaternion kernel in the blind deconvolution process. Extensive experiments on real datasets of blurred color images show that the proposed method effectively removes artifacts and significantly improves deblurring effect, demonstrating its potential as a powerful tool for color image deconvolution.

</details>


### [78] [Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats](https://arxiv.org/abs/2511.17254)
*Jiaye Qian,Ge Zheng,Yuchen Zhu,Sibei Yang*

Main category: cs.CV

TL;DR: 本文提出了一种干预大规模视觉语言模型（LVLMs）幻觉问题的框架，通过实验有效减少了各种类型的幻觉。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在多种任务上表现出色，但依然存在幻觉（生成虚假内容）问题，影响其实用性和可信度，因此需要探索成因并提出解决方案。

Method: 本研究提出与transformer因果结构相匹配的全面干预框架，分析了三种路径：图像到输入文本、图像到输出文本，以及文本到文本对幻觉的影响。首次发现不同的问题-答案对齐格式下，LVLMs依赖的路径不同。基于这些发现，设计了识别和干预不同路径中关键幻觉头的方法，分别适用于判别式和生成式格式。

Result: 在多个测试集上，所提出的方法无论面对何种对齐类型，都能显著减少LVLMs生成的幻觉内容。

Conclusion: LVLMs幻觉不是由单一路径造成，而是多路径互动结果。对关键幻觉头的有针对性干预可有效缓解幻觉问题，为后续优化LVLMs的可靠性提供了方法基础。

Abstract: Despite their impressive performance across a wide range of tasks, Large Vision-Language Models (LVLMs) remain prone to hallucination. In this study, we propose a comprehensive intervention framework aligned with the transformer's causal architecture in LVLMs, integrating the effects of different intervention paths on hallucination. We find that hallucinations in LVLMs do not arise from a single causal path, but rather from the interplay among image-to-input-text, image-to-output-text, and text-to-text pathways. For the first time, we also find that LVLMs rely on different pathways depending on the question-answer alignment format. Building on these insights, we propose simple yet effective methods to identify and intervene on critical hallucination heads within each pathway, tailored to discriminative and generative formats. Experiments across multiple benchmarks demonstrate that our approach consistently reduces hallucinations across diverse alignment types.

</details>


### [79] [A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback](https://arxiv.org/abs/2511.17255)
*Bulat Khaertdinov,Mirela Popa,Nava Tintarev*

Main category: cs.CV

TL;DR: 本文提出利用相关反馈机制提升大规模视觉-语言模型（VLMs）在视觉检索任务中的表现，实验表明多种反馈策略都能有效提升检索准确率，尤其是在小模型上效果显著。


<details>
  <summary>Details</summary>
Motivation: 尽管大模型可以通过微调和扩容提升检索性能，但成本高且灵活性不足。为此，作者借鉴文本检索中的相关反馈机制，希望在不依赖额外训练的情况下，通过推理时的用户反馈即时提升VLMs的表现。

Method: 提出并评估了四种反馈策略：1）经典的伪相关反馈（PRF），对查询嵌入进行改进；2）生成式相关反馈（GRF），利用模型生成的描述来优化查询；3）注意力反馈摘要器（AFS），基于transformer整合相关项的细粒度多模态特征；4）模拟显式反馈，采用真实标签描述作为最优基线。

Result: 在Flickr30K和COCO数据集上，GRF、AFS和显式反馈带来的检索性能提升显著，特别是小模型可提升3-5%，大模型提升1-3%。AFS与显式反馈在多轮检索时能有效缓解查询漂移，表现更加鲁棒。

Conclusion: 相关反馈能稳定提升不同规模VLM的检索能力，尤其为交互式、自适应的视觉搜索系统提供了新思路，不依赖模型结构或额外训练即可灵活应用。

Abstract: Large vision-language models (VLMs) enable intuitive visual search using natural language queries. However, improving their performance often requires fine-tuning and scaling to larger model variants. In this work, we propose a mechanism inspired by traditional text-based search to improve retrieval performance at inference time: relevance feedback. While relevance feedback can serve as an alternative to fine-tuning, its model-agnostic design also enables use with fine-tuned VLMs. Specifically, we introduce and evaluate four feedback strategies for VLM-based retrieval. First, we revise classical pseudo-relevance feedback (PRF), which refines query embeddings based on top-ranked results. To address its limitations, we propose generative relevance feedback (GRF), which uses synthetic captions for query refinement. Furthermore, we introduce an attentive feedback summarizer (AFS), a custom transformer-based model that integrates multimodal fine-grained features from relevant items. Finally, we simulate explicit feedback using ground-truth captions as an upper-bound baseline. Experiments on Flickr30k and COCO with the VLM backbones show that GRF, AFS, and explicit feedback improve retrieval performance by 3-5% in MRR@5 for smaller VLMs, and 1-3% for larger ones, compared to retrieval with no feedback. Moreover, AFS, similarly to explicit feedback, mitigates query drift and is more robust than GRF in iterative, multi-turn retrieval settings. Our findings demonstrate that relevance feedback can consistently enhance retrieval across VLMs and open up opportunities for interactive and adaptive visual search.

</details>


### [80] [Range-Edit: Semantic Mask Guided Outdoor LiDAR Scene Editing](https://arxiv.org/abs/2511.17269)
*Suchetan G. Uppur,Hemant Kumar,Vaibhav Kumar*

Main category: cs.CV

TL;DR: 本研究提出了一种新方法，通过语义掩码引导编辑真实LIDAR数据，生成多样且复杂的合成点云，用于自动驾驶系统训练。


<details>
  <summary>Details</summary>
Motivation: 真实世界复杂和极端交通场景的点云数据获取困难，限制了自动驾驶系统的泛化和鲁棒性。现有手工模拟虚拟环境的方法耗时高、成本大，且无法完全还原真实场景复杂性。

Method: 将真实LIDAR点云投影为2D视图，并用凸包语义掩码条件引导扩散模型进行点云生成与编辑，通过掩码控制生成物体的尺寸、方向和位置，保证几何一致性与真实感。

Result: 在KITTI-360数据集上验证了高质量与复杂场景的点云生成能力，能够高效低成本地生成包含极端情况和动态场景的新颖数据。

Conclusion: 本方法为大规模、多样性LIDAR数据的生成提供了高效可扩展的解决方案，有助于提升自动驾驶系统鲁棒性。

Abstract: Training autonomous driving and navigation systems requires large and diverse point cloud datasets that capture complex edge case scenarios from various dynamic urban settings. Acquiring such diverse scenarios from real-world point cloud data, especially for critical edge cases, is challenging, which restricts system generalization and robustness. Current methods rely on simulating point cloud data within handcrafted 3D virtual environments, which is time-consuming, computationally expensive, and often fails to fully capture the complexity of real-world scenes. To address some of these issues, this research proposes a novel approach that addresses the problem discussed by editing real-world LiDAR scans using semantic mask-based guidance to generate novel synthetic LiDAR point clouds. We incorporate range image projection and semantic mask conditioning to achieve diffusion-based generation. Point clouds are transformed to 2D range view images, which are used as an intermediate representation to enable semantic editing using convex hull-based semantic masks. These masks guide the generation process by providing information on the dimensions, orientations, and locations of objects in the real environment, ensuring geometric consistency and realism. This approach demonstrates high-quality LiDAR point cloud generation, capable of producing complex edge cases and dynamic scenes, as validated on the KITTI-360 dataset. This offers a cost-effective and scalable solution for generating diverse LiDAR data, a step toward improving the robustness of autonomous driving systems.

</details>


### [81] [Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation](https://arxiv.org/abs/2511.17282)
*Chuancheng Shi,Shangze Li,Shiming Guo,Simiao Xie,Wenhua Wu,Jingtong Dou,Chao Wu,Canran Xiao,Cong Wang,Zifeng Cheng,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 现有多语言文本生成图像（T2I）模型在同一语义下容易生成文化中性或偏英语文化的结果，作者提出新方法显著提升了文化一致性。


<details>
  <summary>Details</summary>
Motivation: 由于不同语言包含独特的文化内涵，而现有T2I模型在多语言文本提示下不能很好地保留跨语言的文化一致性，容易出现文化趋同或偏英语结果。因此，提升T2I模型的跨文化表现尤为重要。

Method: 作者首先分析了主流T2I模型，并发现问题根源在于模型虽有文化知识但未充分激活相关神经元。为此，设计了一种探查方法，确定了负责编码文化信息的关键神经元和层。基于此，提出两个增强策略：（1）推理时对关键神经元进行文化激活，无需微调骨干网络；（2）仅对文化相关层进行有针对性的微调。

Result: 作者自建的CultureBench评测集上实验证明，两种方法都能在保持图像质量和多样性的同时，显著提升生成结果的文化一致性，且优于现有强基线。

Conclusion: 文章表明现有T2I模型并非缺失文化知识，而是未能充分激活，所提方法有效改进了这一问题，为多语言T2I领域的文化适应性提供了新思路和实用方案。

Abstract: Multilingual text-to-image (T2I) models have advanced rapidly in terms of visual realism and semantic alignment, and are now widely utilized. Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency. We conduct a comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts. Analyses of two representative models indicate that the issue stems not from missing cultural knowledge but from insufficient activation of culture-related representations. We propose a probing method that localizes culture-sensitive signals to a small set of neurons in a few fixed layers. Guided by this finding, we introduce two complementary alignment strategies: (1) inference-time cultural activation that amplifies the identified neurons without backbone fine-tuned; and (2) layer-targeted cultural enhancement that updates only culturally relevant layers. Experiments on our CultureBench demonstrate consistent improvements over strong baselines in cultural consistency while preserving fidelity and diversity.

</details>


### [82] [MolSight: Optical Chemical Structure Recognition with SMILES Pretraining, Multi-Granularity Learning and Reinforcement Learning](https://arxiv.org/abs/2511.17300)
*Wenrui Zhang,Xinggang Wang,Bin Feng,Wenyu Liu*

Main category: cs.CV

TL;DR: 本文提出了MolSight，一个用于化学结构光学识别（OCSR）的新框架，通过三阶段训练方法显著提升了对有立体化学信息分子的识别能力。实验结果表明其在现有任务中达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有OCSR系统很难准确识别分子的立体化学信息（如楔形键、虚线键、环系构象等），而这对于化学数据挖掘和新药研发极为重要。本文旨在解决现有系统在立体分子识别上的不足。

Method: MolSight采用三阶段训练策略：1）大规模噪声数据预训练，获取化学结构的基本感知能力；2）在监督信号更丰富的数据上进行多粒度微调，引入辅助任务（化学键分类与原子定位）增强识别能力；3）利用强化学习（提出的GRPO算法）进行后训练优化，并引入全新立体化学结构数据集。

Result: 实验结果表明，MolSight在多个不同OCSR数据集上均取得了最先进的识别性能，尤其是对于包含立体化学信息的分子，性能提升显著。

Conclusion: 综合实验验证了MolSight在化学结构光学识别上的有效性，特别是对立体化学结构的识别能力，有望助力药物研发及大规模化学知识挖掘等应用。

Abstract: Optical Chemical Structure Recognition (OCSR) plays a pivotal role in modern chemical informatics, enabling the automated conversion of chemical structure images from scientific literature, patents, and educational materials into machine-readable molecular representations. This capability is essential for large-scale chemical data mining, drug discovery pipelines, and Large Language Model (LLM) applications in related domains. However, existing OCSR systems face significant challenges in accurately recognizing stereochemical information due to the subtle visual cues that distinguish stereoisomers, such as wedge and dash bonds, ring conformations, and spatial arrangements. To address these challenges, we propose MolSight, a comprehensive learning framework for OCSR that employs a three-stage training paradigm. In the first stage, we conduct pre-training on large-scale but noisy datasets to endow the model with fundamental perception capabilities for chemical structure images. In the second stage, we perform multi-granularity fine-tuning using datasets with richer supervisory signals, systematically exploring how auxiliary tasks-specifically chemical bond classification and atom localization-contribute to molecular formula recognition. Finally, we employ reinforcement learning for post-training optimization and introduce a novel stereochemical structure dataset. Remarkably, we find that even with MolSight's relatively compact parameter size, the Group Relative Policy Optimization (GRPO) algorithm can further enhance the model's performance on stereomolecular. Through extensive experiments across diverse datasets, our results demonstrate that MolSight achieves state-of-the-art performance in (stereo)chemical optical structure recognition.

</details>


### [83] [BiFingerPose: Bimodal Finger Pose Estimation for Touch Devices](https://arxiv.org/abs/2511.17306)
*Xiongjun Guan,Zhiyu Pan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TL;DR: BiFingerPose是一种结合电容图像和屏下指纹补丁的双模输入方法，能高效准确地估算手指多维姿态（含roll角），在用户研究中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前主流基于电容图像的指尖姿态估算算法仅能估计部分角度参数，对大角度（如大于45度）表现不佳，也无法估算roll角，制约了触摸设备的人机交互扩展。

Method: 提出BiFingerPose算法，将电容图像与屏下指纹补丁结合为双模输入，实现对手指pitch、yaw、roll等全姿态参数的综合估算。通过用户研究在连续与离散交互任务中验证算法性能。

Result: 在12人用户研究中，BiFingerPose相比以往方法在姿态估算精度提升超21%、任务完成效率提升2.5倍、操作准确率提升23%。

Conclusion: BiFingerPose显著提升了触控设备的手指姿态检测能力，推动了更安全的身份认证及更丰富的交互体验，并提供了应用原型，代码已开源。

Abstract: Finger pose offers promising opportunities to expand human computer interaction capability of touchscreen devices. Existing finger pose estimation algorithms that can be implemented in portable devices predominantly rely on capacitive images, which are currently limited to estimating pitch and yaw angles and exhibit reduced accuracy when processing large-angle inputs (especially when it is greater than 45 degrees). In this paper, we propose BiFingerPose, a novel bimodal based finger pose estimation algorithm capable of simultaneously and accurately predicting comprehensive finger pose information. A bimodal input is explored, including a capacitive image and a fingerprint patch obtained from the touchscreen with an under-screen fingerprint sensor. Our approach leads to reliable estimation of roll angle, which is not achievable using only a single modality. In addition, the prediction performance of other pose parameters has also been greatly improved. The evaluation of a 12-person user study on continuous and discrete interaction tasks further validated the advantages of our approach. Specifically, BiFingerPose outperforms previous SOTA methods with over 21% improvement in prediction performance, 2.5 times higher task completion efficiency, and 23% better user operation accuracy, demonstrating its practical superiority. Finally, we delineate the application space of finger pose with respect to enhancing authentication security and improving interactive experiences, and develop corresponding prototypes to showcase the interaction potential. Our code will be available at https://github.com/XiongjunGuan/DualFingerPose.

</details>


### [84] [SpatialGeo:Boosting Spatial Reasoning in Multimodal LLMs via Geometry-Semantics Fusion](https://arxiv.org/abs/2511.17308)
*Jiajie Guo,Qingpeng Zhu,Jin Zeng,Xiaolong Wu,Changyong He,Weida Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉编码器，通过融合几何和语义特征，加强多模态大模型的空间推理能力，实现了在空间推理任务上的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型（MLLMs）在图像和语言任务中取得了卓越进展，但现有的MLLMs通常缺乏空间推理能力，难以理解和推断三维空间中的空间关系。作者发现这一问题根源在于主流视觉编码器（如CLIP）仅能捕捉实例级语义特征，导致空间信息损失。

Method: 作者提出一种基于层次化融合几何与语义特征的视觉编码器SpatialGeo。具体做法是设计分层适配器，将CLIP视觉语义特征与自监督学习得到的几何特征融合，同时引入随机特征丢弃策略，避免模型单一依赖CLIP特征，提升空间敏感性。训练过程中利用了预训练的LLaVA模型。

Result: 实验结果显示，SpatialGeo在SpatialRGPT-Bench空间推理任务上的准确率较最先进模型提升至少8%，推理时的显存消耗降低约50%。

Conclusion: SpatialGeo极大增强了MLLMs的空间推理和空间定位能力，为多模态模型在需要空间理解的应用领域提供了更优解决方案，兼顾了性能提升和计算效率。

Abstract: Multimodal large language models (MLLMs) have achieved significant progress in image and language tasks due to the strong reasoning capability of large language models (LLMs). Nevertheless, most MLLMs suffer from limited spatial reasoning ability to interpret and infer spatial arrangements in three-dimensional space. In this work, we propose a novel vision encoder based on hierarchical fusion of geometry and semantics features, generating spatial-aware visual embedding and boosting the spatial grounding capability of MLLMs. Specifically, we first unveil that the spatial ambiguity shortcoming stems from the lossy embedding of the vision encoder utilized in most existing MLLMs (e.g., CLIP), restricted to instance-level semantic features. This motivates us to complement CLIP with the geometry features from vision-only self-supervised learning via a hierarchical adapter, enhancing the spatial awareness in the proposed SpatialGeo. The network is efficiently trained using pretrained LLaVA model and optimized with random feature dropping to avoid trivial solutions relying solely on the CLIP encoder. Experimental results show that SpatialGeo improves the accuracy in spatial reasoning tasks, enhancing state-of-the-art models by at least 8.0% in SpatialRGPT-Bench with approximately 50% less memory cost during inference. The source code is available via https://ricky-plus.github.io/SpatialGeoPages/.

</details>


### [85] [MuM: Multi-View Masked Image Modeling for 3D Vision](https://arxiv.org/abs/2511.17309)
*David Nordström,Johan Edstedt,Fredrik Kahl,Georg Bökman*

Main category: cs.CV

TL;DR: 本文提出了一种多视角掩码自编码（MAE）方法MuM，用于提升3D视觉任务表现，超越了主流的视觉编码器。


<details>
  <summary>Details</summary>
Motivation: 目前的自监督视觉特征学习方法主要关注语义理解，缺乏对几何推理的优化，而3D视觉领域对几何特征的高质量学习需求日益增长。

Method: 在CroCo工作的基础上，本文将掩码自编码（MAE）扩展到对同一场景的任意多视角。通过对所有视角进行统一掩码处理，并使用带有帧间注意力的轻量级解码器，从而提升模型的简洁性和可扩展性。

Result: MuM方法在多种下游3D视觉任务（如前向重建、密集图像匹配和相对位姿估计）上的表现超过了DINOv3和CroCo v2。

Conclusion: 针对3D视觉特征学习，提出的MuM方法在模型结构上更为简单、高效，且在多项基准测试中取得新SOTA，展现了大规模自监督学习在几何理解上的潜力。

Abstract: Self-supervised learning on images seeks to extract meaningful visual representations from unlabeled data. When scaled to large datasets, this paradigm has achieved state-of-the-art performance and the resulting trained models such as DINOv3 have seen widespread adoption. However, most prior efforts are optimized for semantic understanding rather than geometric reasoning. One important exception is Cross-View Completion, CroCo, which is a form of masked autoencoding (MAE) tailored for 3D understanding. In this work, we continue on the path proposed by CroCo and focus on learning features tailored for 3D vision. In a nutshell, we extend MAE to arbitrarily many views of the same scene. By uniformly masking all views and employing a lightweight decoder with inter-frame attention, our approach is inherently simpler and more scalable than CroCo. We evaluate the resulting model, MuM, extensively on downstream tasks including feedforward reconstruction, dense image matching and relative pose estimation, finding that it outperforms the state-of-the-art visual encoders DINOv3 and CroCo v2.

</details>


### [86] [NoPe-NeRF++: Local-to-Global Optimization of NeRF with No Pose Prior](https://arxiv.org/abs/2511.17322)
*Dongbo Shi,Shen Cao,Bojian Wu,Jinhui Guo,Lubin Fan,Renjie Chen,Ligang Liu,Jieping Ye*

Main category: cs.CV

TL;DR: 本文提出了一种无需先验位姿信息即可训练NeRF的新型局部到全局优化算法NoPe-NeRF++，有效提升了相机位姿估计和新视角合成的准确性，在多个基准数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF训练方法（如NoPe-NeRF）主攻图像间的局部关系，难以在复杂场景下准确恢复相机位姿，限制了NeRF的表现。为解决没有先验位姿时的挑战，需要新的优化策略。

Method: 先利用显式特征匹配进行相对位姿初始化，然后通过局部联合优化提升位姿估计质量，接着通过全局优化（包含几何一致性约束和特征轨迹的束束调节）进一步精炼位姿，最终将局部和全局线索无缝整合到NeRF训练流程。

Result: 该方法在多个基准数据集上的位姿估计精度和新视角合成质量均优于当前最优方法，并且在复杂或具有挑战性的场景中表现出较强的鲁棒性。

Conclusion: NoPe-NeRF++首次将局部与全局优化策略结合，无需先验位姿，大幅提升了无监督NeRF的表现，并为后续相关研究提供了技术基础。

Abstract: In this paper, we introduce NoPe-NeRF++, a novel local-to-global optimization algorithm for training Neural Radiance Fields (NeRF) without requiring pose priors. Existing methods, particularly NoPe-NeRF, which focus solely on the local relationships within images, often struggle to recover accurate camera poses in complex scenarios. To overcome the challenges, our approach begins with a relative pose initialization with explicit feature matching, followed by a local joint optimization to enhance the pose estimation for training a more robust NeRF representation. This method significantly improves the quality of initial poses. Additionally, we introduce global optimization phase that incorporates geometric consistency constraints through bundle adjustment, which integrates feature trajectories to further refine poses and collectively boost the quality of NeRF. Notably, our method is the first work that seamlessly combines the local and global cues with NeRF, and outperforms state-of-the-art methods in both pose estimation accuracy and novel view synthesis. Extensive evaluations on benchmark datasets demonstrate our superior performance and robustness, even in challenging scenes, thus validating our design choices.

</details>


### [87] [Refracting Reality: Generating Images with Realistic Transparent Objects](https://arxiv.org/abs/2511.17340)
*Yue Yin,Enze Tao,Dylan Campbell*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，使生成模型可依据物理光学规律（尤其是折射）更准确地生成透明物体的图像。通过引入Snell折射定律的像素同步、扭曲和融合过程，对物体内外的像素进行协调，还结合了全景辅助方式，显著提升了合成透明物体的真实性。


<details>
  <summary>Details</summary>
Motivation: 生成模型虽可生成高度真实的图像，但对于涉及物理效应（如透明物体的折射、反射等）的图像表现较差，尤其是折射现象。现有模型难以满足物理光学规律，制约了其在高真实性视觉内容合成中的应用。

Method: 提出了一种基于Snell折射定律的像素同步技术：在生成图像过程中实时将物体边界内的像素与外部像素同步，通过物理约束完成像素的扭曲和融合。对于仅通过折射/反射可见的场景部分，引入了以物体为中心的全景辅助生成，并同步全景与主图像，实现遮挡面外观的复原。

Result: 该方法能够生成更具物理可信度的透明物体图像，视觉效果优于常规生成模型，更好地表现了折射、反射等透明物体特性。

Conclusion: 引入物理定律指导的像素操作极大提升了透明物体合成图像的真实性和物理一致性，为生成模型在复杂视觉场景中的应用拓展了新的方向。

Abstract: Generative image models can produce convincingly real images, with plausible shapes, textures, layouts and lighting. However, one domain in which they perform notably poorly is in the synthesis of transparent objects, which exhibit refraction, reflection, absorption and scattering. Refraction is a particular challenge, because refracted pixel rays often intersect with surfaces observed in other parts of the image, providing a constraint on the color. It is clear from inspection that generative models have not distilled the laws of optics sufficiently well to accurately render refractive objects. In this work, we consider the problem of generating images with accurate refraction, given a text prompt. We synchronize the pixels within the object's boundary with those outside by warping and merging the pixels using Snell's Law of Refraction, at each step of the generation trajectory. For those surfaces that are not directly observed in the image, but are visible via refraction or reflection, we recover their appearance by synchronizing the image with a second generated image -- a panorama centered at the object -- using the same warping and merging procedure. We demonstrate that our approach generates much more optically-plausible images that respect the physical constraints.

</details>


### [88] [Loomis Painter: Reconstructing the Painting Process](https://arxiv.org/abs/2511.17344)
*Markus Pobitzer,Chang Liu,Chenyi Zhuang,Teng Long,Bin Ren,Nicu Sebe*

Main category: cs.CV

TL;DR: 本文提出了一种多媒体绘画过程生成的统一框架，结合语义驱动的风格控制和跨媒介风格增强，实现了更一致且人性化的艺术流程教学，并通过构建大规模数据集和创新评价指标验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有如YouTube等绘画教程资源缺乏交互性和个性化。新一代生成模型虽然在艺术合成方面有进展，但难以跨媒介泛化，并存在时序及结构不稳定等问题，影响了对人类创作流程的真实再现。

Method: 作者提出了一个统一框架，将多媒体嵌入扩散模型的条件空间，并通过跨媒介风格增强，实现语义驱动的风格控制。采用反向绘画训练策略以确保生成过程平滑且符合人类习惯。同时，构建了大规模真实绘画流程数据集，并设计了涵盖跨媒介一致性、时序连贯性和图像保真度的评测方法。创新性地提出了PDP曲线定量建模创作序列。

Result: 实验在LPIPS、DINO和CLIP等指标上取得了优异表现，表明所提方法能够实现一致的纹理演化和流程在不同风格间的迁移，同时很好地还原了真实艺术创作过程。

Conclusion: 该方法在多媒体绘画教程生成领域实现了更自然、一致且个性化的流程模拟，对提升互动性和教学效果有积极意义。PDP曲线的引入也为定量分析人类创作过程提供了新工具。

Abstract: Step-by-step painting tutorials are vital for learning artistic techniques, but existing video resources (e.g., YouTube) lack interactivity and personalization. While recent generative models have advanced artistic image synthesis, they struggle to generalize across media and often show temporal or structural inconsistencies, hindering faithful reproduction of human creative workflows. To address this, we propose a unified framework for multi-media painting process generation with a semantics-driven style control mechanism that embeds multiple media into a diffusion models conditional space and uses cross-medium style augmentation. This enables consistent texture evolution and process transfer across styles. A reverse-painting training strategy further ensures smooth, human-aligned generation. We also build a large-scale dataset of real painting processes and evaluate cross-media consistency, temporal coherence, and final-image fidelity, achieving strong results on LPIPS, DINO, and CLIP metrics. Finally, our Perceptual Distance Profile (PDP) curve quantitatively models the creative sequence, i.e., composition, color blocking, and detail refinement, mirroring human artistic progression.

</details>


### [89] [Label-Efficient Skeleton-based Recognition with Stable-Invertible Graph Convolutional Networks](https://arxiv.org/abs/2511.17345)
*Hichem Sahbi*

Main category: cs.CV

TL;DR: 本论文提出了一种新的基于图卷积网络（GCN）的骨架动作识别方法，通过高效选择最有信息量的数据子集，显著减少对标注数据的需求，同时保持甚至超越现有方法的识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前骨架动作识别方法依赖大量人工标注数据，数据获取成本高且耗时。因此，研究能显著减少人工标注需求的识别方法具有重要意义。

Method: 提出了一种结合数据代表性、多样性和不确定性的新型主动采样（acquisition）函数，用以评分数据子集的标注价值，并引入可逆GCN，将数据从原始空间映射到潜在空间以更好揭示其分布特性。通过优化该目标函数，有效选择最有价值的数据进行标注学习。

Result: 在两个具有挑战性的骨架动作识别数据集上进行的实验表明，所提方法不仅提高了标签利用效率，而且在识别性能上优于相关对比方法。

Conclusion: 该方法能大幅度降低骨架动作识别对人工标注数据的依赖，为低标注成本的动作识别提供了一种高效可行的解决方案。

Abstract: Skeleton-based action recognition is a hotspot in image processing. A key challenge of this task lies in its dependence on large, manually labeled datasets whose acquisition is costly and time-consuming. This paper devises a novel, label-efficient method for skeleton-based action recognition using graph convolutional networks (GCNs). The contribution of the proposed method resides in learning a novel acquisition function -- scoring the most informative subsets for labeling -- as the optimum of an objective function mixing data representativity, diversity and uncertainty. We also extend this approach by learning the most informative subsets using an invertible GCN which allows mapping data from ambient to latent spaces where the inherent distribution of the data is more easily captured. Extensive experiments, conducted on two challenging skeleton-based recognition datasets, show the effectiveness and the outperformance of our label-frugal GCNs against the related work.

</details>


### [90] [DSeq-JEPA: Discriminative Sequential Joint-Embedding Predictive Architecture](https://arxiv.org/abs/2511.17354)
*Xiangteng He,Shunsuke Sakai,Kun Yuan,Nicolas Padoy,Tatsuhito Hasegawa,Leonid Sigal*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉自监督预训练架构DSeq-JEPA，通过结合预测式和自回归学习方式，按注意力分布顺序逐步预测图像区域，提高表征的判别性与泛化能力，实验证明其在多项视觉任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有I-JEPA方法在图像区域处理上，对所有区域一视同仁且独立预测，缺乏关注区域顺序和显著性的机制。而人类视觉处理会先关注最重要区域再关注次要区域，激励作者设计一种能模仿该机制的新方法。

Method: DSeq-JEPA首先基于Transformer生成显著性图，自动识别主要判别性区域。然后，模型按照视觉显著性的顺序，依次对图像区域做嵌入预测，实现从主要线索到次要线索的“课程式”自回归推理，即结合JEPA的潜空间预测和GPT的序列建模思路。

Result: 在ImageNet、iNaturalist21、CUB-200-2011、Stanford-Cars等分类任务，以及MS-COCO、ADE20K等检测分割任务和Clevr系列低级推理任务中，DSeq-JEPA在判别性和泛化能力上均优于I-JEPA及其变体。

Conclusion: DSeq-JEPA通过显著性引导顺序预测，有效提升了表示学习的效率和泛化性，为视觉自监督表征学习提供了兼具预测性与序列建模的新范式。

Abstract: Image-based Joint-Embedding Predictive Architecture (I-JEPA) learns visual representations by predicting latent embeddings of masked regions from visible context. However, it treats all regions uniformly and independently, lacking an explicit notion of where or in what order predictions should be made. Inspired by human visual perception, which deploys attention selectively and sequentially from the most informative to secondary regions, we propose DSeq-JEPA, a Discriminative Sequential Joint-Embedding Predictive Architecture that bridges predictive and autoregressive self-supervised learning, integrating JEPA-style latent prediction with GPT-style sequential reasoning. Specifically, DSeq-JEPA (i) first identifies primary discriminative regions based on a transformer-derived saliency map, emphasizing the distribution of visual importance, and then (ii) predicts subsequent regions in this discriminative order, progressively forming a curriculum-like semantic progression from primary to secondary cues -- a form of GPT-style pre-training. Extensive experiments across diverse tasks, including image classification (ImageNet), fine-grained visual categorization (iNaturalist21, CUB-200-2011, Stanford-Cars), detection and segmentation (MS-COCO, ADE20K), and low-level reasoning tasks (Clevr/Count, Clevr/Dist), demonstrate that DSeq-JEPA consistently focuses on more discriminative and generalizable representations than I-JEPA variants. Project page: https://github.com/SkyShunsuke/DSeq-JEPA.

</details>


### [91] [UAM: A Unified Attention-Mamba Backbone of Multimodal Framework for Tumor Cell Classification](https://arxiv.org/abs/2511.17355)
*Taixi Chen,Jingyun Chen,Nancy Guo*

Main category: cs.CV

TL;DR: 本文提出了一种新的统一注意力-Mamba（UAM）骨干网络，用于细胞级别的放射组学特征分析，实现了细胞分类和图像分割任务的最新性能。


<details>
  <summary>Details</summary>
Motivation: 以往研究多侧重于切片级或小块级肿瘤分类，细胞级放射组学分析未被深入探索，同时缺少为放射组学数据专门设计的骨干网络。

Method: 受到Mamba架构在视觉和语言领域成功经验的启发，作者提出了UAM骨干网络，将Attention和Mamba模块灵活整合于同一架构中，无需手动调参，并设计了两种UAM变体，进一步提出了可同时进行细胞分类和图像分割的多模态UAM框架。

Result: UAM在细胞分类（准确率由74%提升到78%，样本数349,882）和肿瘤分割任务（精度由75%提升到80%，样本数406）上均取得了优于现有模型的效果。

Conclusion: 实验结果表明，UAM架构在放射组学驱动的癌症诊断中具有统一、可扩展的前景，是高效的多模态基础方法。

Abstract: Cell-level radiomics features provide fine-grained insights into tumor phenotypes and have the potential to significantly enhance diagnostic accuracy on hematoxylin and eosin (H&E) images. By capturing micro-level morphological and intensity patterns, these features support more precise tumor identification and improve AI interpretability by highlighting diagnostically relevant cells for pathologist review. However, most existing studies focus on slide-level or patch-level tumor classification, leaving cell-level radiomics analysis largely unexplored. Moreover, there is currently no dedicated backbone specifically designed for radiomics data. Inspired by the recent success of the Mamba architecture in vision and language domains, we introduce a Unified Attention-Mamba (UAM) backbone for cell-level classification using radiomics features. Unlike previous hybrid approaches that integrate Attention and Mamba modules in fixed proportions, our unified design flexibly combines their capabilities within a single cohesive architecture, eliminating the need for manual ratio tuning and improving encode capability. We develop two UAM variants to comprehensively evaluate the benefits of this unified structure. Building on this backbone, we further propose a multimodal UAM framework that jointly performs cell-level classification and image segmentation. Experimental results demonstrate that UAM achieves state-of-the-art performance across both tasks on public benchmarks, surpassing leading image-based foundation models. It improves cell classification accuracy from 74% to 78% ($n$=349,882 cells), and tumor segmentation precision from 75% to 80% ($n$=406 patches). These findings highlight the effectiveness and promise of UAM as a unified and extensible multimodal foundation for radiomics-driven cancer diagnosis.

</details>


### [92] [SuperQuadricOcc: Multi-Layer Gaussian Approximation of Superquadrics for Real-Time Self-Supervised Occupancy Estimation](https://arxiv.org/abs/2511.17361)
*Seamie Hayes,Reenu Mohandas,Tim Brophy,Alexandre Boulch,Ganesh Sistu,Ciaran Eising*

Main category: cs.CV

TL;DR: 本文提出了一种基于超二次（superquadric）表示的语义占据估计方法SuperQuadricOcc，用于自动驾驶场景，显著降低了内存和推理时间，同时提升了性能。


<details>
  <summary>Details</summary>
Motivation: 目前主流的高斯基元表述内存消耗大、实时性能差，不利于实时场景下的占据估计。超二次可以通过更少的基元表达丰富形状，有望缓解这一问题，但此前缺乏有自监督的超二次栅格化方法，难以整合进现有模型。

Method: 提出SuperQuadricOcc方法，使用多层二十面体分割的高斯近似对超二次形状进行表述，使之可通过高斯栅格化监督训练。实现了超二次基元在自监督语义占据估计模型中的有效集成。

Result: 在Occ3D数据集上，SuperQuadricOcc相比高斯方法将内存占用降低75%，推理速度提升124%，同时mIoU提高5.9%。所需基元数目减少84%。

Conclusion: SuperQuadricOcc首创性实现了高性能实时推理的自监督占据表示模型，显著提升效果且发布了高效的超二次体素化模块，有望推动相关应用发展。

Abstract: Semantic occupancy estimation enables comprehensive scene understanding for automated driving, providing dense spatial and semantic information essential for perception and planning. While Gaussian representations have been widely adopted in self-supervised occupancy estimation, the deployment of a large number of Gaussian primitives drastically increases memory requirements and is not suitable for real-time inference. In contrast, superquadrics permit reduced primitive count and lower memory requirements due to their diverse shape set. However, implementation into a self-supervised occupancy model is nontrivial due to the absence of a superquadric rasterizer to enable model supervision. Our proposed method, SuperQuadricOcc, employs a superquadric-based scene representation. By leveraging a multi-layer icosphere-tessellated Gaussian approximation of superquadrics, we enable Gaussian rasterization for supervision during training. On the Occ3D dataset, SuperQuadricOcc achieves a 75\% reduction in memory footprint, 124\% faster inference, and a 5.9\% improvement in mIoU compared to previous Gaussian-based methods, without the use of temporal labels. To our knowledge, this is the first occupancy model to enable real-time inference while maintaining competitive performance. The use of superquadrics reduces the number of primitives required for scene modeling by 84\% relative to Gaussian-based approaches. Finally, evaluation against prior methods is facilitated by our fast superquadric voxelization module. The code will be released as open source.

</details>


### [93] [ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP](https://arxiv.org/abs/2511.17362)
*Linxiang Su,András Balogh*

Main category: cs.CV

TL;DR: 该论文提出了一种针对CLIP模型的简单高效的对抗防御策略ATAC，通过在嵌入空间利用数据增强背景下的漂移信息，实现对对抗样本的恢复，有效提升了模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: CLIP模型虽然在图文匹配任务上表现出色，但对图像的对抗扰动极为敏感，且对抗微调需要大量计算资源，因此亟需高效的测试时对抗防御方法。

Method: 提出ATAC方法：在CLIP的嵌入空间，利用多种增强下的输入，计算其嵌入漂移，推断出语义恢复方向，并根据漂移的角度一致性对嵌入进行矫正，以此抵消对抗扰动影响。

Result: ATAC在多项对抗样本评测基准上，比现有最优方法平均提升近50%的鲁棒性，且计算开销极小。同时在更极端和特殊的设置下仍具备先进鲁棒性，对自适应攻击也表现出一定抵抗能力。

Conclusion: ATAC为CLIP嵌入空间的测试时对抗防御提供了高效且优异的新思路，对相关模型的部署和对抗安全有积极意义。

Abstract: Despite its remarkable success in zero-shot image-text matching, CLIP remains highly vulnerable to adversarial perturbations on images. As adversarial fine-tuning is prohibitively costly, recent works explore various test-time defense strategies; however, these approaches still exhibit limited robustness. In this work, we revisit this problem and propose a simple yet effective strategy: Augmentation-based Test-time Adversarial Correction (ATAC). Our method operates directly in the embedding space of CLIP, calculating augmentation-induced drift vectors to infer a semantic recovery direction and correcting the embedding based on the angular consistency of these latent drifts. Across a wide range of benchmarks, ATAC consistently achieves remarkably high robustness, surpassing that of previous state-of-the-art methods by nearly 50\% on average, all while requiring minimal computational overhead. Furthermore, ATAC retains state-of-the-art robustness in unconventional and extreme settings and even achieves nontrivial robustness against adaptive attacks. Our results demonstrate that ATAC is an efficient method in a novel paradigm for test-time adversarial defenses in the embedding space of CLIP.

</details>


### [94] [SVRecon: Sparse Voxel Rasterization for Surface Reconstruction](https://arxiv.org/abs/2511.17364)
*Seunghun Oh,Jaesung Choe,Dongjae Lee,Daeun Lee,Seunghoon Jeong,Yu-Chiang Frank Wang,Jaesik Park*

Main category: cs.CV

TL;DR: 本文提出了SVRecon方法，将稀疏体素光栅化范式应用于高保真表面重建，并整合了有符号距离函数（SDF），提升了重建精度与收敛速度。


<details>
  <summary>Details</summary>
Motivation: 稀疏体素在参数化过程中因不同体素之间的空间分离容易陷入局部最优，同时难以保持几何场的平滑连续性。论文旨在解决如何在稀疏体素框架下实现高质量、平滑的三维表面重建。

Method: 1）采用视觉几何模型实现稳健的几何初始化；2）设计空间平滑损失，约束父子及兄弟体素之间的结构关系，促进体素结构的连续性和平滑性。

Result: 在多个基准测试数据集上进行了大量实验，结果显示SVRecon方法重建精度高且收敛速度快。

Conclusion: SVRecon显著提升了稀疏体素在三维重建中的表现，是实现高保真、快速收敛三维表面重建的有效方案，代码将开源。

Abstract: We extend the recently proposed sparse voxel rasterization paradigm to the task of high-fidelity surface reconstruction by integrating Signed Distance Function (SDF), named SVRecon. Unlike 3D Gaussians, sparse voxels are spatially disentangled from their neighbors and have sharp boundaries, which makes them prone to local minima during optimization. Although SDF values provide a naturally smooth and continuous geometric field, preserving this smoothness across independently parameterized sparse voxels is nontrivial. To address this challenge, we promote coherent and smooth voxel-wise structure through (1) robust geometric initialization using a visual geometry model and (2) a spatial smoothness loss that enforces coherent relationships across parent-child and sibling voxel groups. Extensive experiments across various benchmarks show that our method achieves strong reconstruction accuracy while having consistently speedy convergence. The code will be made public.

</details>


### [95] [Non-Parametric Probabilistic Robustness: A Conservative Metric with Optimized Perturbation Distributions](https://arxiv.org/abs/2511.17380)
*Zheng Wang,Yi Zhang,Siddartha Khastgir,Carsten Maple,Xingyu Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的非参数概率鲁棒性（NPPR）评估指标，对深度学习模型在输入扰动分布未知情况下的鲁棒性进行更现实的评估。


<details>
  <summary>Details</summary>
Motivation: 当前概率鲁棒性（PR）假设已知且固定的扰动分布，这在实际应用中并不现实，因此需要一种不依赖于预设扰动分布的新方法。

Method: 采用非参数统计建模思想，通过数据直接学习优化扰动分布，提出了基于高斯混合模型（GMM）、多层感知机（MLP）和双三次插值的NPPR估计方法，以应对不同输入相关和无关的扰动场景。

Result: 在CIFAR-10、CIFAR-100和Tiny ImageNet等数据集及ResNet18/50、WideResNet50、VGG16等模型上实验，NPPR相比现有的概率鲁棒性方法在PR估计上最多提高40%的保守性（即更低的PR值）。

Conclusion: NPPR为评估深度学习模型鲁棒性提供了更实际、保守的度量标准，有助于在扰动分布存在不确定性的环境中提升模型的安全性评估。

Abstract: Deep learning (DL) models, despite their remarkable success, remain vulnerable to small input perturbations that can cause erroneous outputs, motivating the recent proposal of probabilistic robustness (PR) as a complementary alternative to adversarial robustness (AR). However, existing PR formulations assume a fixed and known perturbation distribution, an unrealistic expectation in practice. To address this limitation, we propose non-parametric probabilistic robustness (NPPR), a more practical PR metric that does not rely on any predefined perturbation distribution. Following the non-parametric paradigm in statistical modeling, NPPR learns an optimized perturbation distribution directly from data, enabling conservative PR evaluation under distributional uncertainty. We further develop an NPPR estimator based on a Gaussian Mixture Model (GMM) with Multilayer Perceptron (MLP) heads and bicubic up-sampling, covering various input-dependent and input-independent perturbation scenarios. Theoretical analyses establish the relationships among AR, PR, and NPPR. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet across ResNet18/50, WideResNet50 and VGG16 validate NPPR as a more practical robustness metric, showing up to 40\% more conservative (lower) PR estimates compared to assuming those common perturbation distributions used in state-of-the-arts.

</details>


### [96] [MorphSeek: Fine-grained Latent Representation-Level Policy Optimization for Deformable Image Registration](https://arxiv.org/abs/2511.17392)
*Runxun Zhang,Yizhou Liu,Li Dongrui,Bo XU,Jingwei Wei*

Main category: cs.CV

TL;DR: 本文提出了一种新的医学影像形变配准（DIR）方法MorphSeek，通过在编码器输出的潜在特征空间内进行策略优化，实现了高效、细粒度的空间变形优化。该方法在三个3D医学配准基准上都优于现有方法，并保持了高标签效率和低算力开销。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的形变配准属于高度挑战性问题，主要瓶颈在于变形空间的高维性以及缺乏体素级标注。现有的强化学习方法通常降低变形空间的维度，导致捕捉复杂变形能力受限。因此亟需一种能够兼顾空间细节及计算高效的新方法。

Method: MorphSeek方法将DIR转化为潜在特征空间中的空间连续优化过程，通过在编码器顶端添加随机高斯策略头，对潜在特征分布建模，便于探索和逐步细化配准结果。训练环节结合无监督预训练和弱监督微调，并通过团组相对策略优化（Group Relative Policy Optimization）进行多轨迹采样以提升训练稳定性和标签效率。

Result: 在OASIS脑MRI、LiTS肝脏CT和腹部MR-CT三个三维配准基准上，MorphSeek在Dice系数等指标上相较于主流方法取得了全方位提升，同时参数量少、推理开销低，标签利用率高。

Conclusion: MorphSeek提出了一种高效、主干网络无关、优化器无关的表示层级策略学习范式，为高维可扩展图像配准任务提供了空间一致性好、数据高效、理论完善的新思路。

Abstract: Deformable image registration (DIR) remains a fundamental yet challenging problem in medical image analysis, largely due to the prohibitively high-dimensional deformation space of dense displacement fields and the scarcity of voxel-level supervision. Existing reinforcement learning frameworks often project this space into coarse, low-dimensional representations, limiting their ability to capture spatially variant deformations. We propose MorphSeek, a fine-grained representation-level policy optimization paradigm that reformulates DIR as a spatially continuous optimization process in the latent feature space. MorphSeek introduces a stochastic Gaussian policy head atop the encoder to model a distribution over latent features, facilitating efficient exploration and coarse-to-fine refinement. The framework integrates unsupervised warm-up with weakly supervised fine-tuning through Group Relative Policy Optimization, where multi-trajectory sampling stabilizes training and improves label efficiency. Across three 3D registration benchmarks (OASIS brain MRI, LiTS liver CT, and Abdomen MR-CT), MorphSeek achieves consistent Dice improvements over competitive baselines while maintaining high label efficiency with minimal parameter cost and low step-level latency overhead. Beyond optimizer specifics, MorphSeek advances a representation-level policy learning paradigm that achieves spatially coherent and data-efficient deformation optimization, offering a principled, backbone-agnostic, and optimizer-agnostic solution for scalable visual alignment in high-dimensional settings.

</details>


### [97] [Designing and Generating Diverse, Equitable Face Image Datasets for Face Verification Tasks](https://arxiv.org/abs/2511.17393)
*Georgia Baltsou,Ioannis Sarridis,Christos Koutlis,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 本文针对人脸验证系统常见的数据集存在种族、性别等偏见问题，提出了一种利用先进生成模型合成多样性高质量人脸图像的方法，并推出了DIF-V数据集以促进公平性研究。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸图像数据集存在显著的种族、性别等人口学特征偏见，这限制了人脸验证系统的效果和公平性，亟需更全面、多样且公正的数据集与方法。

Method: 结合先进的生成模型生成具有多样化特征且符合证件照标准的高质量人脸图像，制作DIF-V数据集（共27,780张图片，926个身份），用于人脸验证领域的基准测试，并通过分析揭示现有模型的性别和种族偏见。

Result: 分析发现现有的人脸验证模型对特定性别和种族具有偏见，而且身份风格修改会对模型性能产生负面影响。DIF-V数据集能更好地反映多样性与公平性需求。

Conclusion: 本研究丰富了AI多样性与伦理问题讨论，DIF-V数据集为开发更具包容性和可靠性的人脸验证技术提供了重要基础和研究工具。

Abstract: Face verification is a significant component of identity authentication in various applications including online banking and secure access to personal devices. The majority of the existing face image datasets often suffer from notable biases related to race, gender, and other demographic characteristics, limiting the effectiveness and fairness of face verification systems. In response to these challenges, we propose a comprehensive methodology that integrates advanced generative models to create varied and diverse high-quality synthetic face images. This methodology emphasizes the representation of a diverse range of facial traits, ensuring adherence to characteristics permissible in identity card photographs. Furthermore, we introduce the Diverse and Inclusive Faces for Verification (DIF-V) dataset, comprising 27,780 images of 926 unique identities, designed as a benchmark for future research in face verification. Our analysis reveals that existing verification models exhibit biases toward certain genders and races, and notably, applying identity style modifications negatively impacts model performance. By tackling the inherent inequities in existing datasets, this work not only enriches the discussion on diversity and ethics in artificial intelligence but also lays the foundation for developing more inclusive and reliable face verification technologies

</details>


### [98] [MCMoE: Completing Missing Modalities with Mixture of Experts for Incomplete Multimodal Action Quality Assessment](https://arxiv.org/abs/2511.17397)
*Huangbiao Xu,Huanqi Wu,Xiao Ke,Junyi Wu,Rui Xu,Jinglin Xu*

Main category: cs.CV

TL;DR: 该论文针对多模态行为质量评估（AQA）在推理阶段部分模态缺失的问题，提出了一种新颖的混合专家缺失补全框架（MCMoE），通过适应性门控生成器及专家模型，在单阶段训练中统一单模态和多模态表征学习，在三个公开数据集上取得了最新最优性能。


<details>
  <summary>Details</summary>
Motivation: 多模态AQA可以提升对细微动作差异的辨别力，但现实应用中常有部分模态缺失，使现有方法难以工作，同时导致严重性能下降。为解决缺模态下模型鲁棒性差的问题，提出更健壮的学习框架。

Method: 作者设计了一个混合专家缺失补全框架（MCMoE）：首先，利用适应性门控模态生成器基于现有信息动态补全缺失模态；其次，引入模态专家分别学习单模态知识，再动态聚合专家知识以获得跨模态联合表征；最后，在训练阶段同时利用完整和单一模态特征共同指导模态补全和联合表征学习。

Result: 在三个公开AQA基准上的大量实验表明，该方法在模态完整和缺失情况下均达到最新最优性能，显著优于现有多模态和缺模态方法。

Conclusion: 文中提出的MCMoE框架能够有效应对推理时模态缺失带来的挑战，提升多模态行为质量评估系统的鲁棒性和适用性。

Abstract: Multimodal Action Quality Assessment (AQA) has recently emerged as a promising paradigm. By leveraging complementary information across shared contextual cues, it enhances the discriminative evaluation of subtle intra-class variations in highly similar action sequences. However, partial modalities are frequently unavailable at the inference stage in reality. The absence of any modality often renders existing multimodal models inoperable. Furthermore, it triggers catastrophic performance degradation due to interruptions in cross-modal interactions. To address this issue, we propose a novel Missing Completion Framework with Mixture of Experts (MCMoE) that unifies unimodal and joint representation learning in single-stage training. Specifically, we propose an adaptive gated modality generator that dynamically fuses available information to reconstruct missing modalities. We then design modality experts to learn unimodal knowledge and dynamically mix the knowledge of all experts to extract cross-modal joint representations. With a mixture of experts, missing modalities are further refined and complemented. Finally, in the training phase, we mine the complete multimodal features and unimodal expert knowledge to guide modality generation and generation-based joint representation extraction. Extensive experiments demonstrate that our MCMoE achieves state-of-the-art results in both complete and incomplete multimodal learning on three public AQA benchmarks. Code is available at https://github.com/XuHuangbiao/MCMoE.

</details>


### [99] [Sparse Mixture-of-Experts for Multi-Channel Imaging: Are All Channel Interactions Required?](https://arxiv.org/abs/2511.17400)
*Sukwon Yun,Heming Yao,Burkhard Hoeckendorf,David Richmond,Aviv Regev,Russell Littman*

Main category: cs.CV

TL;DR: 论文提出了一种名为MoE-ViT的方法，提高Vision Transformer在多通道图像领域（如细胞成像、卫星影像）的效率，显著减少了计算量同时不降低甚至提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多通道图像领域中的每个通道都包含不同的信息，现有方法在token化时独立处理各通道，导致后续注意力模块的计算和显存消耗呈二次增长，成本高昂。作者关注如何提升模型在处理多通道数据时的计算效率。

Method: 受到稀疏专家混合（MoE）方法的启发，作者提出MoE-ViT架构，将每个通道视为一个专家，由轻量级的路由器动态挑选每个patch最相关的专家（通道）参与注意力计算，从而降低大规模通道间注意力运算的复杂度。

Result: 在JUMP-CP和So2Sat等真实多通道图像数据集上，MoE-ViT在显著提高计算效率的同时保持或提升了模型性能，验证了其有效性和实用性。

Conclusion: MoE-ViT为多通道图像领域的视觉基础模型提供了兼具效率与效果的新选择，适合实际大规模应用场景。

Abstract: Vision Transformers ($\text{ViTs}$) have become the backbone of vision foundation models, yet their optimization for multi-channel domains - such as cell painting or satellite imagery - remains underexplored. A key challenge in these domains is capturing interactions between channels, as each channel carries different information. While existing works have shown efficacy by treating each channel independently during tokenization, this approach naturally introduces a major computational bottleneck in the attention block - channel-wise comparisons leads to a quadratic growth in attention, resulting in excessive $\text{FLOPs}$ and high training cost. In this work, we shift focus from efficacy to the overlooked efficiency challenge in cross-channel attention and ask: "Is it necessary to model all channel interactions?". Inspired by the philosophy of Sparse Mixture-of-Experts ($\text{MoE}$), we propose MoE-ViT, a Mixture-of-Experts architecture for multi-channel images in $\text{ViTs}$, which treats each channel as an expert and employs a lightweight router to select only the most relevant experts per patch for attention. Proof-of-concept experiments on real-world datasets - JUMP-CP and So2Sat - demonstrate that $\text{MoE-ViT}$ achieves substantial efficiency gains without sacrificing, and in some cases enhancing, performance, making it a practical and attractive backbone for multi-channel imaging.

</details>


### [100] [Preventing Shortcut Learning in Medical Image Analysis through Intermediate Layer Knowledge Distillation from Specialist Teachers](https://arxiv.org/abs/2511.17421)
*Christopher Boland,Sotirios Tsaftaris,Sonia Dahdouh*

Main category: cs.CV

TL;DR: 深度学习模型在医学影像等高风险场景中容易利用数据中的虚假相关性进行“捷径学习”，影响其鲁棒性和安全性。为此，该文提出了一种创新的知识蒸馏方法，通过在中间层针对性缓解“捷径”问题，显著提升了模型的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在医学影像等需要高可靠性的应用中，深度学习模型往往会依赖与任务无关但数据中存在偏差的特征进行判断，导致模型对于真正临床相关特征的忽视，这将带来安全隐患和泛化能力不足。

Method: 提出一种新的知识蒸馏框架：首先用少量无偏数据微调教师网络，再指导学生网络在含有偏置特征的大规模数据上训练。该方式聚焦于模型中间层的特征表现，有效抑制了模型对虚假相关特征的依赖。

Result: 在CheXpert、ISIC 2017和SimBA等医学影像基准及多种主流网络架构下，提出方法在训练/测试集，特别是在分布外数据上的表现，显著优于传统ERM（经验风险最小化）、基于数据增强和基于分组的偏差缓解方法。在部分情况下，性能接近完全无偏数据训练得到的基线。

Conclusion: 该方法无需大量偏差标注，在实际偏差信息不足、难以预先识别偏置特征的真实医学图像场景下实用性强，为提升医学影像智能诊断的安全性和鲁棒性提供了切实可行的解决方案。

Abstract: Deep learning models are prone to learning shortcut solutions to problems using spuriously correlated yet irrelevant features of their training data. In high-risk applications such as medical image analysis, this phenomenon may prevent models from using clinically meaningful features when making predictions, potentially leading to poor robustness and harm to patients. We demonstrate that different types of shortcuts (those that are diffuse and spread throughout the image, as well as those that are localized to specific areas) manifest distinctly across network layers and can, therefore, be more effectively targeted through mitigation strategies that target the intermediate layers. We propose a novel knowledge distillation framework that leverages a teacher network fine-tuned on a small subset of task-relevant data to mitigate shortcut learning in a student network trained on a large dataset corrupted with a bias feature. Through extensive experiments on CheXpert, ISIC 2017, and SimBA datasets using various architectures (ResNet-18, AlexNet, DenseNet-121, and 3D CNNs), we demonstrate consistent improvements over traditional Empirical Risk Minimization, augmentation-based bias-mitigation, and group-based bias-mitigation approaches. In many cases, we achieve comparable performance with a baseline model trained on bias-free data, even on out-of-distribution test data. Our results demonstrate the practical applicability of our approach to real-world medical imaging scenarios where bias annotations are limited and shortcut features are difficult to identify a priori.

</details>


### [101] [REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing](https://arxiv.org/abs/2511.17442)
*Binger Chen,Tacettin Emre Bök,Behnood Rasti,Volker Markl,Begüm Demir*

Main category: cs.CV

TL;DR: 该论文介绍了REMSA，这是首个基于大语言模型（LLM）的遥感基础模型（RSFM）自动选择智能体，依托于包含150多种RSFM的数据库RS-FMD，能够根据自然语言查询帮助用户选择合适遥感基础模型，并在多个基准任务上优于其它方法。


<details>
  <summary>Details</summary>
Motivation: 遥感领域基础模型（RSFM）文档分散、格式各异、部署约束多样，导致用户难以高效选择合适的模型。因此，急需统一的数据库与智能检索辅助工具，简化选型流程，提升应用效率。

Method: 作者构建了结构化的RSFM数据库RS-FMD，涵盖150余种多模态、多分辨率、多学习范式的遥感基础模型，并提出了基于大语言模型的智能检索与排序系统REMSA。REMSA支持用户自然语言查询，自动补全限定条件并进行候选模型排序，还给出透明选择理由。此外，作者设置了75个专家校验的遥感查询场景，构成900种专家评价配置，用于系统全面评测。

Result: REMSA在遥感基础模型选型任务中显著优于基线比较方法，如简单检索、稠密召回和RAG型LLM等，既准确又能透明解释选择依据。同时，该系统完全基于公开元数据实现，不涉及敏感或私有信息。

Conclusion: 论文证明了REMSA基于结构化DB和LLM方案能高效、透明、自动化支持复杂遥感模型选型，推动遥感基础模型在实际应用场景中的落地和推广。

Abstract: Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.

</details>


### [102] [MMT-ARD: Multimodal Multi-Teacher Adversarial Distillation for Robust Vision-Language Models](https://arxiv.org/abs/2511.17448)
*Yuqi Li,Junhao Dong,Chuanguang Yang,Shiping Wen,Piotr Koniusz,Tingwen Huang,Yingli Tian,Yew-Soon Ong*

Main category: cs.CV

TL;DR: 该论文提出了MMT-ARD框架，即多模态多教师对抗鲁棒性蒸馏方法，用于提升视觉-语言模型在面对对抗样本时的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型已广泛用于安全关键领域，但其抗对抗攻击能力不足。传统的单一教师蒸馏方法面临知识多样性有限、收敛慢及鲁棒性与准确性难平衡等问题。为解决这些挑战，作者试图引入多教师和多模态的知识蒸馏策略。

Method: 提出MMT-ARD框架，利用两个教师模型进行知识融合，分别优化特征保留与鲁棒特征增强。采用基于教师信心的动态权重分配策略，以及自适应sigmoid加权函数，提升对抗样本下的适应性与知识多样性。

Result: 大量实验证明，在ImageNet及零样本任务上，所提方法显著提升了鲁棒性和准确性：ViT-B-32模型鲁棒准确率提升4.32%，零样本准确率提升3.5%，训练效率提升至传统单教师方法的2.3倍。

Conclusion: MMT-ARD显著提升了多模态大模型在对抗环境下的鲁棒性与泛化能力，且训练效率高，具有良好的实际应用和扩展价值。

Abstract: Vision-Language Models (VLMs) are increasingly deployed in safety-critical applications, making their adversarial robustness a crucial concern. While adversarial knowledge distillation has shown promise in transferring robustness from teacher to student models, traditional single-teacher approaches suffer from limited knowledge diversity, slow convergence, and difficulty in balancing robustness and accuracy. To address these challenges, we propose MMT-ARD: a Multimodal Multi-Teacher Adversarial Robust Distillation framework. Our key innovation is a dual-teacher knowledge fusion architecture that collaboratively optimizes clean feature preservation and robust feature enhancement. To better handle challenging adversarial examples, we introduce a dynamic weight allocation strategy based on teacher confidence, enabling adaptive focus on harder samples. Moreover, to mitigate bias among teachers, we design an adaptive sigmoid-based weighting function that balances the strength of knowledge transfer across modalities. Extensive experiments on ImageNet and zero-shot benchmarks demonstrate that MMT-ARD improves robust accuracy by +4.32% and zero-shot accuracy by +3.5% on the ViT-B-32 model, while achieving a 2.3x increase in training efficiency over traditional single-teacher methods. These results highlight the effectiveness and scalability of MMT-ARD in enhancing the adversarial robustness of multimodal large models. Our codes are available at https://github.com/itsnotacie/MMT-ARD.

</details>


### [103] [Illustrator's Depth: Monocular Layer Index Prediction for Image Decomposition](https://arxiv.org/abs/2511.17454)
*Nissim Maruani,Peiying Zhang,Siddhartha Chaudhuri,Matthew Fisher,Nanxuan Zhao,Vladimir G. Kim,Pierre Alliez,Mathieu Desbrun,Wang Yifan*

Main category: cs.CV

TL;DR: 该论文提出了一种新的深度定义“Illustrator's Depth”，能将平面图像分解为可编辑、有序的层，并通过神经网络实现自动化分层，提升图像矢量化和多种后续任务的表现。


<details>
  <summary>Details</summary>
Motivation: 传统图像处理在将2D图像分层为可编辑图层时面临挑战，尤其在数字内容创作中，缺乏可解释且便于编辑的层次结构。作者希望赋予每个像素一个有序层索引，以模拟艺术家分层绘制的过程，提升编辑和处理效率。

Method: 作者定义了“Illustrator's Depth”作为像素分层索引，并通过整理矢量图形数据集，训练神经网络直接从栅格图像预测各像素所属层次，实现全局一致的离散层级分解。

Result: 提出的方法在图像矢量化任务上显著优于现有技术方案，同时支持文本到矢量图生成、高保真3D浮雕生成、深度感知编辑等多种应用。

Conclusion: 将深度由物理属性转化为创造性抽象，为可编辑图像分解提供新方法，拓展了图形内容创作和编辑的技术基础。

Abstract: We introduce Illustrator's Depth, a novel definition of depth that addresses a key challenge in digital content creation: decomposing flat images into editable, ordered layers. Inspired by an artist's compositional process, illustrator's depth infers a layer index to each pixel, forming an interpretable image decomposition through a discrete, globally consistent ordering of elements optimized for editability. We also propose and train a neural network using a curated dataset of layered vector graphics to predict layering directly from raster inputs. Our layer index inference unlocks a range of powerful downstream applications. In particular, it significantly outperforms state-of-the-art baselines for image vectorization while also enabling high-fidelity text-to-vector-graphics generation, automatic 3D relief generation from 2D images, and intuitive depth-aware editing. By reframing depth from a physical quantity to a creative abstraction, illustrator's depth prediction offers a new foundation for editable image decomposition.

</details>


### [104] [Improving Multimodal Distillation for 3D Semantic Segmentation under Domain Shift](https://arxiv.org/abs/2511.17455)
*Björn Michele,Alexandre Boulch,Gilles Puy,Tuan-Hung Vu,Renaud Marlet,Nicolas Courty*

Main category: cs.CV

TL;DR: 本文探讨如何利用视觉基础模型(VFM)来提升不同激光雷达设备下的点云语义分割适应性，并提出了一整套改进方法，在多个领域迁移设置下取得了最优效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于全监督训练的激光雷达点云分割网络难以直接适应新型未见过的激光雷达设备，存在明显的性能下降（领域偏移问题）。为了解决此难题，近年来的方法倾向于引入视觉基础模型来获得更鲁棒的特征表示，但具体如何高效利用这些模型在点云语义分割任务上的效果尚未系统研究。

Method: 作者基于无监督的图像到激光雷达知识蒸馏，系统研究了多种利用视觉基础模型的方法，包括不同的骨干网络架构、预训练与冻结策略等。通过大量实验，作者总结出：选对骨干网络架构、一次性泛化预训练、骨干固定配合MLP分割头等关键优化策略。

Result: 该方法在四个公认的具有挑战性的领域适应设置下均取得了最新的最佳性能（state-of-the-art），显著优于现有方法，并实现了高效迁移和泛化能力。

Conclusion: 通过合理利用视觉基础模型和设计适当的分割骨干结构，可以有效缓解不同雷达类型下点云语义分割的领域迁移难题，显著提升其通用性和实际适应能力。

Abstract: Semantic segmentation networks trained under full supervision for one type of lidar fail to generalize to unseen lidars without intervention. To reduce the performance gap under domain shifts, a recent trend is to leverage vision foundation models (VFMs) providing robust features across domains. In this work, we conduct an exhaustive study to identify recipes for exploiting VFMs in unsupervised domain adaptation for semantic segmentation of lidar point clouds. Building upon unsupervised image-to-lidar knowledge distillation, our study reveals that: (1) the architecture of the lidar backbone is key to maximize the generalization performance on a target domain; (2) it is possible to pretrain a single backbone once and for all, and use it to address many domain shifts; (3) best results are obtained by keeping the pretrained backbone frozen and training an MLP head for semantic segmentation. The resulting pipeline achieves state-of-the-art results in four widely-recognized and challenging settings. The code will be available at: https://github.com/valeoai/muddos.

</details>


### [105] [GPR-OdomNet: Difference and Similarity-Driven Odometry Estimation Network for Ground Penetrating Radar-Based Localization](https://arxiv.org/abs/2511.17457)
*Huaichao Wang,Xuanxin Fan,Ji Liu,Haifeng Li,Dezhen Song*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经网络的新型GPR（探地雷达）里程计方法，通过分析B-scan图像的相似性与差异性，实现更精确的距离估算。实验结果显示，该方法在多个数据集上超越了现有最优算法。


<details>
  <summary>Details</summary>
Motivation: 现有GPR定位技术在面对细微图像差异时，距离估算的准确性不足。本研究旨在解决复杂环境下探地雷达定位不准确的问题。

Method: 设计了一种自定义的神经网络，能够在连续时刻的B-scan图像中提取多尺度特征，通过对比这些特征的相似与差异，精准推断两幅图间的欧式距离。并通过消融实验和与现有方法对比，在CMU-GPR公开数据集上进行评估。

Result: 新方法在全部测试中均优于现有最佳方法，综合加权RMSE为0.449米，比当前最优对比方法的RMSE降低了10.2%。

Conclusion: 所提神经网络方法能更准确地估算恶劣环境下GPR B-scan图像间的距离，有效提升了机器人/车辆定位的精度。

Abstract: When performing robot/vehicle localization using ground penetrating radar (GPR) to handle adverse weather and environmental conditions, existing techniques often struggle to accurately estimate distances when processing B-scan images with minor distinctions. This study introduces a new neural network-based odometry method that leverages the similarity and difference features of GPR B-scan images for precise estimation of the Euclidean distances traveled between the B-scan images. The new custom neural network extracts multi-scale features from B-scan images taken at consecutive moments and then determines the Euclidean distance traveled by analyzing the similarities and differences between these features. To evaluate our method, an ablation study and comparison experiments have been conducted using the publicly available CMU-GPR dataset. The experimental results show that our method consistently outperforms state-of-the-art counterparts in all tests. Specifically, our method achieves a root mean square error (RMSE), and achieves an overall weighted RMSE of 0.449 m across all data sets, which is a 10.2\% reduction in RMSE when compared to the best state-of-the-art method.

</details>


### [106] [Counterfactual World Models via Digital Twin-conditioned Video Diffusion](https://arxiv.org/abs/2511.17481)
*Yiqing Shen,Aiza Maksutova,Chenjia Li,Mathias Unberath*

Main category: cs.CV

TL;DR: 本文提出了一种新型世界模型（CWMDT），可在进行针对场景的干预（如去除物体）后，生成未来视觉序列，实现对假设场景的推理。


<details>
  <summary>Details</summary>
Motivation: 现有的世界模型主要关注事实场景的前向预测，但无法有效回答“如果……”类反事实问题。然而，AI在物理环境中的全面评估等应用场景，对这类能够处理反事实的世界模型有迫切需求。

Method: CWMDT首先为观察到的场景构建数字孪生，以结构化文本显式表征物体及其关系。然后利用大语言模型推理干预如何影响场景随时间的演化。最后，将修改后的表示作为条件输入到视频扩散模型中，生成反事实视觉序列。

Result: 在两个基准测试中，CWMDT均取得了当前最优的效果，验证了基于数字孪生的结构化表征能显著提升反事实世界模型的表现。

Conclusion: 数字孪生及结构化场景表示有望成为未来世界模型的重要控制信号，使模型不仅能前向预测事实，还能回答各种反事实问题，扩展了世界模型在现实任务中的应用能力。

Abstract: World models learn to predict the temporal evolution of visual observations given a control signal, potentially enabling agents to reason about environments through forward simulation. Because of the focus on forward simulation, current world models generate predictions based on factual observations. For many emerging applications, such as comprehensive evaluations of physical AI behavior under varying conditions, the ability of world models to answer counterfactual queries, such as "what would happen if this object was removed?", is of increasing importance. We formalize counterfactual world models that additionally take interventions as explicit inputs, predicting temporal sequences under hypothetical modifications to observed scene properties. Traditional world models operate directly on entangled pixel-space representations where object properties and relationships cannot be selectively modified. This modeling choice prevents targeted interventions on specific scene properties. We introduce CWMDT, a framework to overcome those limitations, turning standard video diffusion models into effective counterfactual world models. First, CWMDT constructs digital twins of observed scenes to explicitly encode objects and their relationships, represented as structured text. Second, CWMDT applies large language models to reason over these representations and predict how a counterfactual intervention propagates through time to alter the observed scene. Third, CWMDT conditions a video diffusion model with the modified representation to generate counterfactual visual sequences. Evaluations on two benchmarks show that the CWMDT approach achieves state-of-the-art performance, suggesting that alternative representations of videos, such as the digital twins considered here, offer powerful control signals for video forward simulation-based world models.

</details>


### [107] [Radar2Shape: 3D Shape Reconstruction from High-Frequency Radar using Multiresolution Signed Distance Functions](https://arxiv.org/abs/2511.17484)
*Neel Sortur,Justin Goodwin,Purvik Patel,Luis Enrique Martinez,Tzofi Klinghoffer,Rajmonda S. Caceres,Robin Walters*

Main category: cs.CV

TL;DR: 本文提出了一种名为Radar2Shape的去噪扩散模型，通过关联雷达信号频率与多分辨率形状特征，实现了从高频雷达信号中高效重建任意三维物体形状。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习雷达建模方法难以处理任意形状或对实际受限视角的雷达信号表现不佳，而光学三维重建方法直接将雷达信号当做摄像头视角也会失败。因此，需要一种能高效自部分可观测雷达信号还原三维形状的新方法。

Method: 本文提出的Radar2Shape是一种两阶段去噪扩散模型：首先学习多分辨率的形状特征规整潜空间；其次，结合雷达信号的频率信息，以粗到细的方式在该潜空间内实现条件扩散。

Result: 实验表明，Radar2Shape不仅能从部分可观测雷达信号中重建任意三维物体形状，并且对两种不同模拟方法及真实数据具备强泛化能力。此外，作者还发布了两个合成基准数据集促进后续在高频雷达领域的研究。

Conclusion: Radar2Shape显著提升了基于受限视角高频雷达信号的三维重建能力，为后续高频雷达现实应用中的三维重建模型提供了可靠技术基础和数据资源。

Abstract: Determining the shape of 3D objects from high-frequency radar signals is analytically complex but critical for commercial and aerospace applications. Previous deep learning methods have been applied to radar modeling; however, they often fail to represent arbitrary shapes or have difficulty with real-world radar signals which are collected over limited viewing angles. Existing methods in optical 3D reconstruction can generate arbitrary shapes from limited camera views, but struggle when they naively treat the radar signal as a camera view. In this work, we present Radar2Shape, a denoising diffusion model that handles a partially observable radar signal for 3D reconstruction by correlating its frequencies with multiresolution shape features. Our method consists of a two-stage approach: first, Radar2Shape learns a regularized latent space with hierarchical resolutions of shape features, and second, it diffuses into this latent space by conditioning on the frequencies of the radar signal in an analogous coarse-to-fine manner. We demonstrate that Radar2Shape can successfully reconstruct arbitrary 3D shapes even from partially-observed radar signals, and we show robust generalization to two different simulation methods and real-world data. Additionally, we release two synthetic benchmark datasets to encourage future research in the high-frequency radar domain so that models like Radar2Shape can safely be adapted into real-world radar systems.

</details>


### [108] [An Artificial Intelligence Framework for Measuring Human Spine Aging Using MRI](https://arxiv.org/abs/2511.17485)
*Roozbeh Bazargani,Saqib Abdullah Basar,Daniel Daly-Grafstein,Rodrigo Solis Pompa,Soojin Lee,Saurabh Garg,Yuntong Ma,John A. Carrino,Siavash Khallaghi,Sam Hashemi*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的计算机视觉方法，利用超过18,000组MRI影像来估算脊柱的生理年龄，并通过分析实际年龄与预测年龄的差异（SAG）探讨其与脊柱退变性疾病及生活方式因素的关联。


<details>
  <summary>Details</summary>
Motivation: 脊柱对身体健康至关重要，但易受年龄相关性退变影响。已有的脊柱健康指标有限，临床上缺乏客观的脊柱‘衰老’测量指标。为此，作者希望开发一种通过MRI影像自动评估脊柱生理年龄的方法，以辅助疾病风险评估与健康管理。

Method: 作者筛选了仅有年龄相关退变的被试MRI数据，首先基于UMAP和HDBSCAN聚类方法识别常见的年龄关联脊柱退变类型，并以此制定数据纳入标准。随后，设计深度学习模型对脊柱生理年龄进行预测，通过消融实验就数据规模、损失函数及不同脊柱区域影响进行模型优化。最终，利用SAG（脊柱年龄差）及其与疾病和生活方式的关联性评价模型的临床实用价值。

Result: 模型能够有效预测脊柱年龄。分析发现，SAG与包括椎间盘膨出、骨赘、椎管狭窄、骨折等退变性病变，以及吸烟、体力劳动等生活方式紧密相关。

Conclusion: SAG可作为反映整体脊柱健康状况的生物标志物，为客观评估脊柱衰老与疾病风险提供新工具。

Abstract: The human spine is a complex structure composed of 33 vertebrae. It holds the body and is important for leading a healthy life. The spine is vulnerable to age-related degenerations that can be identified through magnetic resonance imaging (MRI). In this paper we propose a novel computer-vison-based deep learning method to estimate spine age using images from over 18,000 MRI series. Data are restricted to subjects with only age-related spine degeneration. Eligibility criteria are created by identifying common age-based clusters of degenerative spine conditions using uniform manifold approximation and projection (UMAP) and hierarchical density-based spatial clustering of applications with noise (HDBSCAN). Model selection is determined using a detailed ablation study on data size, loss, and the effect of different spine regions. We evaluate the clinical utility of our model by calculating the difference between actual spine age and model-predicted age, the spine age gap (SAG), and examining the association between these differences and spine degenerative conditions and lifestyle factors. We find that SAG is associated with conditions including disc bulges, disc osteophytes, spinal stenosis, and fractures, as well as lifestyle factors like smoking and physically demanding work, and thus may be a useful biomarker for measuring overall spine health.

</details>


### [109] [Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models](https://arxiv.org/abs/2511.17487)
*Mark Endo,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 作者分析了多模态大模型下缩时对视觉能力的影响，并提出了一种新的方法来提升小模型的视觉推理表现。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大模型在视觉理解与推理方面取得了显著进展，但实际应用中更需要高效、小型的系统。当前，关于模型规模缩减对多模态能力的具体影响还不清楚，亟需系统研究。

Method: 作者系统分析了大语言模型(LLM)规模缩减对多模态模型能力的影响，并提出了“视觉提取调优”(visual extraction tuning)的方法，专门训练模型提取任务相关的视觉细节，再结合逐步推理(推理分步进行)来解答问题，形成Extract+Think新方案。

Result: 结果表明，缩减LLM容量对视觉能力的影响远大于对语言推理能力的影响。引入视觉提取调优后，模型在多模态任务的效率和表现上获得了显著提升。

Conclusion: 合理设计的视觉细节提取与分步推理流程，可以让小型多模态模型兼具高效性和优异性能，代表了多模态模型高效化发展的新方向。

Abstract: Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.

</details>


### [110] [Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination](https://arxiv.org/abs/2511.17490)
*Yolo Yunlong Tang,Daiki Shimada,Hang Hua,Chao Huang,Jing Bi,Rogerio Feris,Chenliang Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视频推理多模态大模型Video-R4，能够像人类一样反复关注和推理视频中细粒度的文本信息，从而解决现有方法对小而短暂文本线索识别能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频问答模型在处理文本丰富且需要多轮细致推理的视频内容时存在局限，主要依赖单次对固定帧的感知，难以捕捉短暂且关键的文本内容，容易产生幻觉和错误判断。为此，作者受到人类反复浏览和聚焦关键细节的启发，提出模拟人类视觉沉思行为的新方法。

Method: 作者提出了Video-R4模型，引入视觉沉思机制，即类似人类反复选择帧、放大关键区域、重新编码像素并持续更新推理状态的能力。为训练与评估，构建了包含详细可执行沉思轨迹的两个数据集，并采用多阶段沉思式学习框架，逐步微调一个7B多模态大模型，训练过程结合有监督学习和基于GRPO的强化学习。

Result: Video-R4-7B在M4-ViteVQA基准上取得了最新最优的效果，并能良好泛化到多页文档、课件幻灯片和通用视频问答任务，验证了迭代式视觉沉思机制的有效性。

Conclusion: 多轮视觉沉思机制为像素级的多模态推理提供了新范式，显著提升了文本丰富视频以及复杂视频问答任务的理解能力，为相关领域带来了新的解决思路。

Abstract: Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.

</details>


### [111] [EvDiff: High Quality Video with an Event Camera](https://arxiv.org/abs/2511.17492)
*Weilun Li,Lei Sun,Ruixi Gao,Qi Jiang,Yuqin Ma,Kaiwei Wang,Ming-Hsuan Yang,Luc Van Gool,Danda Pani Paudel*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的事件相机视频重建方法EvDiff，利用扩散模型和代理训练框架，无需依赖成对数据，即可从单色事件流重建高质量彩色视频，综合指标优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件相机的强度图像重建方法通常直接回归事件到画面，效果有限且受模型与数据规模限制。如何在不依赖成对事件-图像数据下，高效重建高质量影像，是当前的主要难题。

Method: 作者提出了一种基于扩散模型的框架EvDiff，利用单步扩散结合时间一致性编码器（EvEncoder），大幅减少高帧率生成的计算开销。提出代理训练框架（Surrogate Training Framework），摆脱对事件-图像成对数据的依赖，能利用大规模图像数据提升模型能力。

Result: 该方法可以仅从单色事件流，生成高质量彩色视频。实验证明，在真实数据集上，该方法在像素级和感知指标上均优于现有方法，在保真度与真实感之间取得了较优平衡。

Conclusion: EvDiff突破了事件相机影像重建对成对数据和高计算负担的限制，兼顾生成效果及实用性，推动基于事件传感的新型视觉重建发展。

Abstract: As neuromorphic sensors, event cameras asynchronously record changes in brightness as streams of sparse events with the advantages of high temporal resolution and high dynamic range. Reconstructing intensity images from events is a highly ill-posed task due to the inherent ambiguity of absolute brightness. Early methods generally follow an end-to-end regression paradigm, directly mapping events to intensity frames in a deterministic manner. While effective to some extent, these approaches often yield perceptually inferior results and struggle to scale up in model capacity and training data. In this work, we propose EvDiff, an event-based diffusion model that follows a surrogate training framework to produce high-quality videos. To reduce the heavy computational cost of high-frame-rate video generation, we design an event-based diffusion model that performs only a single forward diffusion step, equipped with a temporally consistent EvEncoder. Furthermore, our novel Surrogate Training Framework eliminates the dependence on paired event-image datasets, allowing the model to leverage large-scale image datasets for higher capacity. The proposed EvDiff is capable of generating high-quality colorful videos solely from monochromatic event streams. Experiments on real-world datasets demonstrate that our method strikes a sweet spot between fidelity and realism, outperforming existing approaches on both pixel-level and perceptual metrics.

</details>


### [112] [Native 3D Editing with Full Attention](https://arxiv.org/abs/2511.17501)
*Weiwei Cai,Shuangkang Fang,Weicai Ye,Xin Dong,Yunhan Yang,Xuanyang Zhang,Wei Cheng,Yanpei Cao,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: 本论文提出了一种基于指令的高效3D编辑新方法，通过大规模多模态数据集和创新的建模策略，在3D生成质量和一致性方面显著超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前基于指令的3D编辑方法存在显著不足：优化类方法速度慢，传统多视图2D编辑方法则在几何一致性和视觉质量上表现欠佳。因此，迫切需要更高效、更高质量的3D编辑技术。

Method: 作者建立了包含多种编辑任务（添加、删除、修改）的高质量、多模态3D编辑数据集，并提出了两种模型条件赋予策略：传统交叉注意力方式及创新的3D token拼接法，设计高效单次前馈过程直接编辑3D表示。

Result: 实验证明，3D token拼接法比交叉注意力更节省参数，同时生成效果更优。与当前2D提升类方法相比，该方法在生成质量、3D一致性和指令遵循性等方面均创下新基准。

Conclusion: 提出的3D原生编辑框架效率高、编辑一致性强、指令遵循效果好，有望拓展3D内容创作的应用前景，对业界具有较大推动作用。

Abstract: Instruction-guided 3D editing is a rapidly emerging field with the potential to broaden access to 3D content creation. However, existing methods face critical limitations: optimization-based approaches are prohibitively slow, while feed-forward approaches relying on multi-view 2D editing often suffer from inconsistent geometry and degraded visual quality. To address these issues, we propose a novel native 3D editing framework that directly manipulates 3D representations in a single, efficient feed-forward pass. Specifically, we create a large-scale, multi-modal dataset for instruction-guided 3D editing, covering diverse addition, deletion, and modification tasks. This dataset is meticulously curated to ensure that edited objects faithfully adhere to the instructional changes while preserving the consistency of unedited regions with the source object. Building upon this dataset, we explore two distinct conditioning strategies for our model: a conventional cross-attention mechanism and a novel 3D token concatenation approach. Our results demonstrate that token concatenation is more parameter-efficient and achieves superior performance. Extensive evaluations show that our method outperforms existing 2D-lifting approaches, setting a new benchmark in generation quality, 3D consistency, and instruction fidelity.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [113] [Shona spaCy: A Morphological Analyzer for an Under-Resourced Bantu Language](https://arxiv.org/abs/2511.16680)
*Happymore Masoka*

Main category: cs.CL

TL;DR: 本文提出了基于spaCy框架的Shona语言形态学分析工具Shona spaCy，实现了高准确率的词性标注和形态特征注释，有助于提升该语言的数字化支持。


<details>
  <summary>Details</summary>
Motivation: 尽管多语言自然语言处理发展迅速，班图语系的Shona语言在形态分析和面向语言的工具方面仍然资源匮乏。作者希望通过开发Shona专用的形态分析工具，提升该语言在NLP领域的覆盖和可用性。

Method: 作者基于spaCy框架，构建了一个结合人工整理的JSON词典和语言学规则的规则型形态学处理流程工具。系统能处理名词类别前缀、动词主语一致、时体标记、拟声词和黏着词等，并提供分词层面的词元、词性及形态特征注释。工具已开源并可通过pip安装。

Result: 在正式和非正式的Shona语料上，该工具实现了90%的词性标注准确率和88%的形态特征注释准确率，并保持形态学决策的可解释性。

Conclusion: Shona spaCy工具桥接了描述性语法和计算实现，为Shona语言的NLP可及性和数字包容性做出贡献，并为其它资源稀缺的班图语形态分析工具开发提供了范例。

Abstract: Despite rapid advances in multilingual natural language processing (NLP), the Bantu language Shona remains under-served in terms of morphological analysis and language-aware tools. This paper presents Shona spaCy, an open-source, rule-based morphological pipeline for Shona built on the spaCy framework. The system combines a curated JSON lexicon with linguistically grounded rules to model noun-class prefixes (Mupanda 1-18), verbal subject concords, tense-aspect markers, ideophones, and clitics, integrating these into token-level annotations for lemma, part-of-speech, and morphological features. The toolkit is available via pip install shona-spacy, with source code at https://github.com/HappymoreMasoka/shona-spacy and a PyPI release at https://pypi.org/project/shona-spacy/0.1.4/. Evaluation on formal and informal Shona corpora yields 90% POS-tagging accuracy and 88% morphological-feature accuracy, while maintaining transparency in its linguistic decisions. By bridging descriptive grammar and computational implementation, Shona spaCy advances NLP accessibility and digital inclusion for Shona speakers and provides a template for morphological analysis tools for other under-resourced Bantu languages.

</details>


### [114] [Towards Hyper-Efficient RAG Systems in VecDBs: Distributed Parallel Multi-Resolution Vector Search](https://arxiv.org/abs/2511.16681)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

TL;DR: 该论文提出了一种新的多分辨率向量索引框架SPI（Semantic Pyramid Indexing），用于改进RAG（检索增强生成）系统在向量数据库中的检索速率与语义覆盖能力。SPI可自适应地选择检索分辨率，提升效率与效果，并兼容现有数据库实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 目前RAG系统常用的向量检索方法采用单层或者固定分辨率结构，难以针对不同用户查询的语义粒度自适应调整，造成检索速度和语义相关性之间的折中效果不佳。需求是实现更灵活高效的向量检索以提升RAG系统整体表现。

Method: 提出Semantic Pyramid Indexing (SPI) 多分辨率索引结构，在文档嵌入上构建语义金字塔，每个查询通过轻量级的分类器动态选择最合适的检索分辨率，实现从粗到细的渐进检索。SPI无需离线调参或单独模型训练，可直接用作FAISS和Qdrant插件，并通过理论分析和消融实验验证有效性。

Result: SPI在MS MARCO、Natural Questions和多模态检索任务上，与主流基线相比，检索速度提升最多5.7倍，内存效率提升1.8倍，端到端QA F1分数提高最多2.5分。

Conclusion: SPI支持与现有向量数据库无缝兼容，且大幅度提升RAG系统的检索效率和质量，理论上对检索质量和延迟有严格保证，具备较强实用推广价值。

Abstract: Retrieval-Augmented Generation (RAG) systems have become a dominant approach to augment large language models (LLMs) with external knowledge. However, existing vector database (VecDB) retrieval pipelines rely on flat or single-resolution indexing structures, which cannot adapt to the varying semantic granularity required by diverse user queries. This limitation leads to suboptimal trade-offs between retrieval speed and contextual relevance.
  To address this, we propose \textbf{Semantic Pyramid Indexing (SPI)}, a novel multi-resolution vector indexing framework that introduces query-adaptive resolution control for RAG in VecDBs. Unlike existing hierarchical methods that require offline tuning or separate model training, SPI constructs a semantic pyramid over document embeddings and dynamically selects the optimal resolution level per query through a lightweight classifier. This adaptive approach enables progressive retrieval from coarse-to-fine representations, significantly accelerating search while maintaining semantic coverage.
  We implement SPI as a plugin for both FAISS and Qdrant backends and evaluate it across multiple RAG tasks including MS MARCO, Natural Questions, and multimodal retrieval benchmarks. SPI achieves up to \textbf{5.7$\times$} retrieval speedup and \textbf{1.8$\times$} memory efficiency gain while improving end-to-end QA F1 scores by up to \textbf{2.5 points} compared to strong baselines. Our theoretical analysis provides guarantees on retrieval quality and latency bounds, while extensive ablation studies validate the contribution of each component. The framework's compatibility with existing VecDB infrastructures makes it readily deployable in production RAG systems. Code is availabe at \href{https://github.com/FastLM/SPI_VecDB}{https://github.com/FastLM/SPI\_VecDB}.

</details>


### [115] [Bench360: Benchmarking Local LLM Inference from 360°](https://arxiv.org/abs/2511.16682)
*Linus Stuhlmann,Mauricio Fadel Argerich,Jonathan Fürst*

Main category: cs.CL

TL;DR: Bench360是一个面向本地大语言模型推理的全面评测框架，能统一评估不同模型、推理引擎、量化方式和应用场景下的系统及任务特定指标。实验显示，不同应用场景下没有最优通用配置，凸显了Bench360的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前运行本地大语言模型变得普及，但配置选择繁多，用户在满足具体场景下功能和性能的权衡上需投入大量人力。现有评测方法片面，缺乏集成多维度指标和易用性，难以满足实际需求。

Method: 提出Bench360框架，支持用户自定义任务、数据集与指标，可自动 benchmarking 不同LLMs、推理引擎和量化设定，覆盖单流、批处理和服务等应用场景，系统性采集并比较多维度系统指标（如延迟、能耗、冷启动时间）和任务指标（如ROUGE、F1等）。

Result: 在四类常见LLM任务、多种硬件平台和推理引擎上做了验证，Bench360 揭示了性能与效率之间的多种权衡及推理引擎、模型差异。

Conclusion: 本地推理没有统一最优配置，性能和效率需结合具体场景权衡，Bench360为用户科学评估和选择提供了重要工具。

Abstract: Running large language models (LLMs) locally is becoming increasingly common. While the growing availability of small open-source models and inference engines has lowered the entry barrier, users now face an overwhelming number of configuration choices. Identifying an optimal configuration -- balancing functional and non-functional requirements -- requires substantial manual effort. While several benchmarks target LLM inference, they are designed for narrow evaluation goals and not user-focused. They fail to integrate relevant system and task-specific metrics into a unified, easy-to-use benchmark that supports multiple inference engines, usage scenarios, and quantization levels. To address this gap, we present Bench360 -- Benchmarking Local LLM Inference from 360°. Bench360 allows users to easily define their own custom tasks along with datasets and relevant task-specific metrics and then automatically benchmarks selected LLMs, inference engines, and quantization levels across different usage scenarios (single stream, batch & server). Bench360 tracks a wide range of metrics, including (1) system metrics -- such as Computing Performance (e.g., latency, throughput), Resource Usage (e.g., energy per query), and Deployment (e.g., cold start time) -- and (2) task-specific metrics such as ROUGE, F1 score or accuracy. We demonstrate Bench360 on four common LLM tasks -- General Knowledge & Reasoning, QA, Summarization and Text-to-SQL -- across three hardware platforms and four state of the art inference engines. Our results reveal several interesting trade-offs between task performance and system-level efficiency, highlighting the differences in inference engines and models. Most importantly, there is no single best setup for local inference, which strongly motivates the need for a framework such as Bench360.

</details>


### [116] [How Well Do LLMs Understand Tunisian Arabic?](https://arxiv.org/abs/2511.16683)
*Mohamed Mahdi*

Main category: cs.CL

TL;DR: 该论文关注LLM对低资源语言（如突尼斯人阿拉伯语）的理解能力，通过新构建的数据集和基准测评，展示不同模型在该语言上的表现差异，强调提升AI多语言包容性的必要性。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型对低资源语言（尤其是突尼斯人阿拉伯语——Tunizi）能力有限，这可能导致用户被迫使用英语或法语，从而威胁方言的延续和文化多样性。作者旨在填补AI在低资源语言处理上的空白。

Method: 作者构建了包含突尼斯人阿拉伯语（Tunizi）、标准突尼斯人阿拉伯语和英语平行翻译的多语种数据集，并为句子标注情感标签。利用该数据集，对多种主流LLM在音译、翻译和情感分析三项任务上进行基准测评。

Result: 不同LLM在处理突尼斯人阿拉伯语相关任务时表现出明显差异，一些模型在音译、翻译或情感分析等子任务上有优势，但普遍存在理解和处理该方言的限制。

Conclusion: 作者呼吁在下一代AI系统中更加重视低资源语言支持，以保证技术的可及性、包容性及对文化的尊重与保留。

Abstract: Large Language Models (LLMs) are the engines driving today's AI agents. The better these models understand human languages, the more natural and user-friendly the interaction with AI becomes, from everyday devices like computers and smartwatches to any tool that can act intelligently. Yet, the ability of industrial-scale LLMs to comprehend low-resource languages, such as Tunisian Arabic (Tunizi), is often overlooked. This neglect risks excluding millions of Tunisians from fully interacting with AI in their own language, pushing them toward French or English. Such a shift not only threatens the preservation of the Tunisian dialect but may also create challenges for literacy and influence younger generations to favor foreign languages. In this study, we introduce a novel dataset containing parallel Tunizi, standard Tunisian Arabic, and English translations, along with sentiment labels. We benchmark several popular LLMs on three tasks: transliteration, translation, and sentiment analysis. Our results reveal significant differences between models, highlighting both their strengths and limitations in understanding and processing Tunisian dialects. By quantifying these gaps, this work underscores the importance of including low-resource languages in the next generation of AI systems, ensuring technology remains accessible, inclusive, and culturally grounded.

</details>


### [117] [Ellipsoid-Based Decision Boundaries for Open Intent Classification](https://arxiv.org/abs/2511.16685)
*Yuetian Zou,Hanlei Zhang,Hua Xu,Songze Li,Long Xiao*

Main category: cs.CL

TL;DR: EliDecide方法通过引入椭球决策边界，提升了开放意图分类中的未知意图检测能力，取得了多个基准数据集上的最佳表现。


<details>
  <summary>Details</summary>
Motivation: 现有开放意图分类方法假设已知类别服从各向同性分布，限制了决策边界（只能是球形），忽略了不同方向的分布差异，导致意图检测能力受限。

Method: 提出EliDecide方法，首先利用有监督对比学习获得判别性特征空间，然后用可学习矩阵将椭球参数化为各类别决策边界，同时设计双重损失函数，兼顾已知样本的覆盖与对合成伪开放样本的排斥。

Result: 在多个文本意图识别基准以及问题分类数据集上取得了最先进的性能，在检测未知意图和泛化能力上表现突出。

Conclusion: EliDecide通过灵活的椭球边界，有效提升了开放意图检测表现，并具有广泛迁移到其他复杂文本分类任务的潜力。

Abstract: Textual open intent classification is crucial for real-world dialogue systems, enabling robust detection of unknown user intents without prior knowledge and contributing to the robustness of the system. While adaptive decision boundary methods have shown great potential by eliminating manual threshold tuning, existing approaches assume isotropic distributions of known classes, restricting boundaries to balls and overlooking distributional variance along different directions. To address this limitation, we propose EliDecide, a novel method that learns ellipsoid decision boundaries with varying scales along different feature directions. First, we employ supervised contrastive learning to obtain a discriminative feature space for known samples. Second, we apply learnable matrices to parameterize ellipsoids as the boundaries of each known class, offering greater flexibility than spherical boundaries defined solely by centers and radii. Third, we optimize the boundaries via a novelly designed dual loss function that balances empirical and open-space risks: expanding boundaries to cover known samples while contracting them against synthesized pseudo-open samples. Our method achieves state-of-the-art performance on multiple text intent benchmarks and further on a question classification dataset. The flexibility of the ellipsoids demonstrates superior open intent detection capability and strong potential for generalization to more text classification tasks in diverse complex open-world scenarios.

</details>


### [118] [Prompt-Based Value Steering of Large Language Models](https://arxiv.org/abs/2511.16688)
*Giulio Antonio Abbo,Tony Belpaeme*

Main category: cs.CL

TL;DR: 本文提出了一种实用且可复现的评估流程，用于量化提示词对大语言模型输出中人类价值观引导的效果。


<details>
  <summary>Details</summary>
Motivation: 现有使用微调保证语言模型输出安全性的做法难以应对动态的人类价值观和偏好需求，因此需要一种更灵活的调控方法。

Method: 提出了一种模型无关的评估流程和打分方法，通过设定目标价值并分析输出文本在这些价值上的体现和提升。实验以Wizard-Vicuna模型为例，结合Schwartz价值观理论和对话数据集，对比不同提示词下输出的价值观表现。

Result: 实验证明，即使不改变模型结构或动态优化提示，也能通过有针对性的提示词实现对输出文本价值观的引导。

Conclusion: 价值观引导在大语言模型中是可行的，可以通过合理设计提示词来实现细致的价值调控，提升模型的现实应用灵活性和安全性。

Abstract: Large language models are increasingly used in applications where alignment with human values is critical. While model fine-tuning is often employed to ensure safe responses, this technique is static and does not lend itself to everyday situations involving dynamic values and preferences. In this paper, we present a practical, reproducible, and model-agnostic procedure to evaluate whether a prompt candidate can effectively steer generated text toward specific human values, formalising a scoring method to quantify the presence and gain of target values in generated responses. We apply our method to a variant of the Wizard-Vicuna language model, using Schwartz's theory of basic human values and a structured evaluation through a dialogue dataset. With this setup, we compare a baseline prompt to one explicitly conditioned on values, and show that value steering is possible even without altering the model or dynamically optimising prompts.

</details>


### [119] [Concept-Based Interpretability for Toxicity Detection](https://arxiv.org/abs/2511.16689)
*Samarth Garg,Deeksha Varshney,Divya Singh*

Main category: cs.CL

TL;DR: 本文提出了一种基于概念梯度（CG）的方法，用于解释和提升有害语言检测模型的可解释性，尤其关注模型对不同毒性概念的不均匀归因问题。通过构建有针对性的词汇集并设计无毒性词汇增强策略，分析并缓解模型误判。


<details>
  <summary>Details</summary>
Motivation: 尽管文本毒性检测取得了进展，但很少有研究关注模型根据不同毒性子类型（如侮辱、威胁等）的概念进行归因与解释。模型对某些概念过度依赖，导致错误分类，因此需要更细致的解释方法来发现和纠正这类问题。

Method: 作者提出了基于Concept Gradient的解释性方法，能够量化概念变化对模型输出的直接影响。进一步地，作者构建了有针对性的毒性词汇集，并计算词汇-概念对齐分数（WCA），以量化它们对误判的贡献。此外，作者引入了一种无毒性词汇的样本增强方法，通过生成不含特定毒性词汇的样本，研究模型在去除显式词汇重叠后的归因变化。

Result: 通过上述方法，作者能够有效捕捉到模型对毒性概念的过度归因现象，并证明Targeted Lexicon Set和WCA分数可以揭示模型误判的根因。无毒性词汇增强实验显示，过度归因并不完全依赖于显式毒性词汇。

Conclusion: 概念梯度与针对性词汇集结合为理解与优化毒性检测模型提供了新方法。提出的无毒词汇增强策略有助于判定模型对广义毒性语言的归因机制。该研究对提升模型透明度和减少误判具有实际意义。

Abstract: The rise of social networks has not only facilitated communication but also allowed the spread of harmful content. Although significant advances have been made in detecting toxic language in textual data, the exploration of concept-based explanations in toxicity detection remains limited. In this study, we leverage various subtype attributes present in toxicity detection datasets, such as obscene, threat, insult, identity attack, and sexual explicit as concepts that serve as strong indicators to identify whether language is toxic. However, disproportionate attribution of concepts towards the target class often results in classification errors. Our work introduces an interpretability technique based on the Concept Gradient (CG) method which provides a more causal interpretation by measuring how changes in concepts directly affect the output of the model. This is an extension of traditional gradient-based methods in machine learning, which often focus solely on input features. We propose the curation of Targeted Lexicon Set, which captures toxic words that contribute to misclassifications in text classification models. To assess the significance of these lexicon sets in misclassification, we compute Word-Concept Alignment (WCA) scores, which quantify the extent to which these words lead to errors due to over-attribution to toxic concepts. Finally, we introduce a lexicon-free augmentation strategy by generating toxic samples that exclude predefined toxic lexicon sets. This approach allows us to examine whether over-attribution persists when explicit lexical overlap is removed, providing insights into the model's attribution on broader toxic language patterns.

</details>


### [120] [Falsely Accused: How AI Detectors Misjudge Slightly Polished Arabic Articles](https://arxiv.org/abs/2511.16690)
*Saleh Almohaimeed,Saad Almohaimeed,Mousa Jari,Khaled A. Alobaid,Fahad Alotaibi*

Main category: cs.CL

TL;DR: 本文针对阿拉伯语文章的AI检测模型表现进行了评估，发现于人类文章经过轻微AI润色后，现有AI检测器大量误判为AI生成，准确率大幅下降。


<details>
  <summary>Details</summary>
Motivation: 当前许多AI检测模型被用于鉴别AI生成与人工撰写的文章，但当人工文章仅经过轻微AI润色时，这些模型常发生误判，误将其认定为AI生成，导致冤枉作者有抄袭嫌疑，损害AI检测工具公信力。以往针对英语文章已有研究，但阿拉伯语领域几乎空白，为此作者着手填补该领域空白。

Method: 作者构建了两个数据集：第一个包含800篇阿拉伯语文章（一半为AI生成，一半为人工撰写），用以评估14个大型语言模型（LLM）和商用AI检测器的判别能力；从中选出表现最好的8个模型，再用于研究它们对于人工文章经过10个LLM和4种设置轻微润色（共产生16400份样本）后的检测表现。

Result: 实验结果显示，所有AI检测器在轻微润色后均大量误判，人类文章被错归为AI生成。最佳LLM Claude-4 Sonnet对原文检测准确率为83.51%，但经过LLaMA-3润色的样本降至57.63%；最佳商用检测器originality.AI准确率由92%大幅下降至12%（被Mistral或Gemma-3润色时）。

Conclusion: 大部分现有AI检测器难以应对人工文章经轻微AI润色后的检测任务，导致高比例误判，严重影响系统的公信力和实用性。阿拉伯语对AI检测技术提出更高挑战，未来需开发更为鲁棒的AI检测手段。

Abstract: Many AI detection models have been developed to counter the presence of articles created by artificial intelligence (AI). However, if a human-authored article is slightly polished by AI, a shift will occur in the borderline decision of these AI detection models, leading them to consider it AI-generated article. This misclassification may result in falsely accusing authors of AI plagiarism and harm the credibility of AI detector models. In English, some efforts were made to meet this challenge, but not in Arabic. In this paper, we generated two datasets. The first dataset contains 800 Arabic articles, half AI-generated and half human-authored. We used it to evaluate 14 Large Language models (LLMs) and commercial AI detectors to assess their ability in distinguishing between human-authored and AI-generated articles. The best 8 models were chosen to act as detectors for our primary concern, which is whether they would consider slightly polished human text as AI-generated. The second dataset, Ar-APT, contains 400 Arabic human-authored articles polished by 10 LLMs using 4 polishing settings, totaling 16400 samples. We use it to evaluate the 8 nominated models and determine whether slight polishing will affect their performance. The results reveal that all AI detectors incorrectly attribute a significant number of articles to AI. The best performing LLM, Claude-4 Sonnet, achieved 83.51%, their performance decreased to 57.63% for articles slightly polished by LLaMA-3. Whereas for the best performing commercial model, originality.AI, that achieves 92% accuracy, dropped to 12% for articles slightly polished by Mistral or Gemma-3.

</details>


### [121] [Reproducibility Report: Test-Time Training on Nearest Neighbors for Large Language Models](https://arxiv.org/abs/2511.16691)
*Boyang Zhou,Johan Lindqvist,Lindsey Li*

Main category: cs.CL

TL;DR: 本文复现了“基于最近邻的测试时训练”方法在大语言模型上的核心结论，并在多个模型和数据集上验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 动机在于提升大语言模型在推理时的适应能力，尤其针对未见过或结构化/专业领域数据集，缓解模型参数量不足带来的性能瓶颈。

Method: 方法是在推理时，基于RoBERTa嵌入和Faiss检索，为每个测试输入找出20个最近邻，然后对每个邻居执行一次梯度更新，实现小幅高效的微调，适应当前测试数据。支持多模型（GPT-2、GPT-Neo、R1-Distilled-Qwen等），并优化检索算法以降低内存占用。

Result: 实验证明该方法能在多个公开数据集上显著降低困惑度与bpb指标，在GitHub、EuroParl等专业或结构化数据集上提升最大。对于未在相关数据集预训练的模型，收益更明显。同时小模型通过此法可达大模型性能。此外，文中展示了优化内存消耗和在新架构上测试的有效性。

Conclusion: 基于最近邻的测试时训练方法具有稳健性和广泛适用性，可大幅提升大语言模型在多领域的表现，并为大规模检索增强自适应训练提供了实际的工程借鉴。

Abstract: We reproduce the central claims of Test-Time Training on Nearest Neighbors for Large Language Models (Hardt and Sun, 2024), which proposes adapting a language model at inference time by fine-tuning on retrieved nearest-neighbor sequences. Using pretrained RoBERTa embeddings indexed with Faiss, we retrieve 20 neighbors per test input and apply one gradient update per neighbor across GPT-2 (117M, 774M), GPT-Neo (1.3B), and R1-Distilled-Qwen2.5-1.5B. Our experiments confirm that test-time training significantly reduces perplexity and bits-per-byte metrics across diverse domains from The Pile, with the largest improvements in structured or specialized datasets such as GitHub and EuroParl. We further validate that models not pretrained on The Pile benefit more from this adaptation than models already trained on similar data, allowing smaller models to approach the performance of larger ones. Due to infrastructure limitations, we introduce a memory-efficient retrieval implementation that loads only required line offsets rather than entire files, reducing RAM requirements from over 128 GB per server to 32 GB. We also extend the original study by evaluating R1-Distilled-Qwen2.5-1.5B, showing that test-time training yields consistent gains even for modern reasoning-optimized architectures. Overall, our results support the robustness and generality of nearest-neighbor test-time training while highlighting practical considerations for reproducing large-scale retrieval-augmented adaptation.

</details>


### [122] [How Language Directions Align with Token Geometry in Multilingual LLMs](https://arxiv.org/abs/2511.16693)
*JaeSeong Kim,Suan Lee*

Main category: cs.CL

TL;DR: 本文系统性分析了多语种大模型在内部层表示中语言信息的结构及其演化，揭示语言信息在模型前几层就高度可分，且与训练数据中的语言构成显著相关。


<details>
  <summary>Details</summary>
Motivation: 虽然多语种大语言模型（LLM）表现出很强的多语言能力，但关于其内部如何编码和区分语言信息，相关层级结构和形成机制，缺乏系统研究。

Method: 对六个多语种LLM的全部268层transformer，分别用线性、非线性探针及提出的Token-Language Alignment分析，定量刻画每层的语言信息表达与几何结构。

Result: 语言信息在第一层transformer后急剧分离并在整个模型深度中几乎线性可分。语言方向与词表嵌入的对齐度高度取决于训练语料的语言组成，如包含中文的模型对中文的结构影响远超英文为主的模型。

Conclusion: 多语种大模型主要通过深层隐式表征（而非表面书写特征）来区分语言，这种表征极大受训练数据语种分布影响，提示多语训练数据配置需考虑公平性及表征多样性。

Abstract: Multilingual LLMs demonstrate strong performance across diverse languages, yet there has been limited systematic analysis of how language information is structured within their internal representation space and how it emerges across layers. We conduct a comprehensive probing study on six multilingual LLMs, covering all 268 transformer layers, using linear and nonlinear probes together with a new Token--Language Alignment analysis to quantify the layer-wise dynamics and geometric structure of language encoding. Our results show that language information becomes sharply separated in the first transformer block (+76.4$\pm$8.2 percentage points from Layer 0 to 1) and remains almost fully linearly separable throughout model depth. We further find that the alignment between language directions and vocabulary embeddings is strongly tied to the language composition of the training data. Notably, Chinese-inclusive models achieve a ZH Match@Peak of 16.43\%, whereas English-centric models achieve only 3.90\%, revealing a 4.21$\times$ structural imprinting effect. These findings indicate that multilingual LLMs distinguish languages not by surface script features but by latent representational structures shaped by the training corpus. Our analysis provides practical insights for data composition strategies and fairness in multilingual representation learning. All code and analysis scripts are publicly available at: https://github.com/thisiskorea/How-Language-Directions-Align-with-Token-Geometry-in-Multilingual-LLMs.

</details>


### [123] [Hierarchical Retrieval with Out-Of-Vocabulary Queries: A Case Study on SNOMED CT](https://arxiv.org/abs/2511.16698)
*Jonathon Dilworth,Hui Yang,Jiaoyan Chen,Yongsheng Gao*

Main category: cs.CL

TL;DR: 本文提出了一种基于语言模型的本体嵌入方法，实现对SNOMED CT本体在词汇外（OOV）查询下的分层概念检索，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: SNOMED CT作为一个生物医学本体，支持医疗知识的表达与检索，但由于自然而然语言中的歧义、多义词、同义词等问题，知识检索存在很多难题，尤其是面对词汇外（OOV）查询时，本体内难以找到直接对应的匹配，影响应用。

Method: 作者提出利用基于语言模型的本体嵌入方法，将查询和本体概念映射到同一语义空间，通过向量计算实现分层关系下的概念检索。并构建了OOV查询集合，人工标注与SNOMED CT概念关联，用于方法评估。

Result: 方法在最直接上位概念和更远上级（祖先）检索任务上都优于SBERT以及两种词汇匹配基线方法。

Conclusion: 提出的方法能够有效改进SNOMED CT分层本体的OOV查询检索，具有优良的泛化能力，可扩展至其他本体。代码与评测数据公开。

Abstract: SNOMED CT is a biomedical ontology with a hierarchical representation of large-scale concepts. Knowledge retrieval in SNOMED CT is critical for its application, but often proves challenging due to language ambiguity, synonyms, polysemies and so on. This problem is exacerbated when the queries are out-of-vocabulary (OOV), i.e., having no equivalent matchings in the ontology. In this work, we focus on the problem of hierarchical concept retrieval from SNOMED CT with OOV queries, and propose an approach based on language model-based ontology embeddings. For evaluation, we construct OOV queries annotated against SNOMED CT concepts, testing the retrieval of the most direct subsumers and their less relevant ancestors. We find that our method outperforms the baselines including SBERT and two lexical matching methods. While evaluated against SNOMED CT, the approach is generalisable and can be extended to other ontologies. We release code, tools, and evaluation datasets at https://github.com/jonathondilworth/HR-OOV.

</details>


### [124] [Detecting and Steering LLMs' Empathy in Action](https://arxiv.org/abs/2511.16699)
*Juan P. Cadile*

Main category: cs.CL

TL;DR: 本文探索了大语言模型（LLM）在执行任务时展现同理心行动（即为满足人类需求而牺牲任务效率）的特性。作者提出了一种在模型激活空间中检测和操控同理心的方法，并在多个模型间验证。结果显示，不同架构虽都可检测到同理心信号，但操控效果和稳健性依模型和训练方式而异。


<details>
  <summary>Details</summary>
Motivation: 传统大模型在任务执行中追求效率，但真实世界中的人类互动常常涉及为他人考虑而牺牲效率。理解与操控模型中的“同理心行动”有助于提升AI对人类需求的考虑。

Method: 作者基于同理心行动（EIA）基准数据，设计对照提示，分析大模型激活空间中的同理心经向。首先在Phi-3-mini-4k、Qwen2.5-7B和Dolphin-Llama-3.1-8B三个模型上进行检测实验，后采用激活操控技术，在此方向上控制模型生成结果、测试可操控性和连贯性。

Result: 所有模型在最佳层可高精度检测同理心（AUROC高达0.996-1.00），且未安全训练的Dolphin模型与安全模型表现相当，显示同理心编码不依赖安全训练。不同模型激活同理心的方式差异大。Qwen和Phi-3双向可控性较好，Dolphin则只对正向同理心可控，对反向操控极不稳定。

Conclusion: 大模型中的同理心可被检测和操控，但具体效果高度依赖架构及训练方式。安全训练对防止被操控影响有限，更可能影响模型在极端操控下的稳健性。对更多模型的验证仍待进一步研究。

Abstract: We investigate empathy-in-action -- the willingness to sacrifice task efficiency to address human needs -- as a linear direction in LLM activation space. Using contrastive prompts grounded in the Empathy-in-Action (EIA) benchmark, we test detection and steering across Phi-3-mini-4k (3.8B), Qwen2.5-7B (safety-trained), and Dolphin-Llama-3.1-8B (uncensored).
  Detection: All models show AUROC 0.996-1.00 at optimal layers. Uncensored Dolphin matches safety-trained models, demonstrating empathy encoding emerges independent of safety training. Phi-3 probes correlate strongly with EIA behavioral scores (r=0.71, p<0.01). Cross-model probe agreement is limited (Qwen: r=-0.06, Dolphin: r=0.18), revealing architecture-specific implementations despite convergent detection.
  Steering: Qwen achieves 65.3% success with bidirectional control and coherence at extreme interventions. Phi-3 shows 61.7% success with similar coherence. Dolphin exhibits asymmetric steerability: 94.4% success for pro-empathy steering but catastrophic breakdown for anti-empathy (empty outputs, code artifacts).
  Implications: The detection-steering gap varies by model. Qwen and Phi-3 maintain bidirectional coherence; Dolphin shows robustness only for empathy enhancement. Safety training may affect steering robustness rather than preventing manipulation, though validation across more models is needed.

</details>


### [125] [NALA_MAINZ at BLP-2025 Task 2: A Multi-agent Approach for Bangla Instruction to Python Code Generation](https://arxiv.org/abs/2511.16787)
*Hossain Shaikh Saadi,Faria Alam,Mario Sanz-Guerrero,Minh Duc Bui,Manuel Mager,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本文提出了一种基于多智能体的流水线系统，有效提升了从孟加拉语指令生成代码的准确性，并在BLP-2025竞赛中获得了第一名。


<details>
  <summary>Details</summary>
Motivation: 现有从自然语言指令生成代码的方法在面对复杂指令或测试未通过时往往缺乏自动化故障修复能力。作者旨在提高代码生成的正确率和自动调试性能，解决现有方法适应性和容错能力不足的问题。

Method: 论文设计了一个多智能体系统：首先由代码生成智能体根据输入指令生成初版代码，然后用自动化测试用例评估；仅将失败案例交由调试智能体，该智能体分析错误信息、当前程序和相关测试，生成修正版代码。如此迭代优化，提高通过率。

Result: 提出的系统在BLP-2025代码生成竞赛中取得了95.4的Pass@1分数，荣获第一名。实验结果体现出该自动化多智能体流水线对复杂任务的高适应性和出色的性能。

Conclusion: 多智能体协作代码生成与调试的自动化流水线，大幅提升了代码生成质量和测试通过率，对未来多语言代码生成和自动软件开发具有借鉴意义。源码已公开，有望推动相关领域发展。

Abstract: This paper presents JGU Mainz's winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions. We propose a multi-agent-based pipeline. First, a code-generation agent produces an initial solution from the input instruction. The candidate program is then executed against the provided unit tests (pytest-style, assert-based). Only the failing cases are forwarded to a debugger agent, which reruns the tests, extracts error traces, and, conditioning on the error messages, the current program, and the relevant test cases, generates a revised solution. Using this approach, our submission achieved first place in the shared task with a $Pass@1$ score of 95.4. We also make our code public.

</details>


### [126] [From Representation to Enactment: The ABC Framework of the Translating Mind](https://arxiv.org/abs/2511.16811)
*Michael Carl,Takanori Mizowaki,Aishvarya Raj,Masaru Yamada,Devi Sri Bandaru,Yuxiang Wei,Xinyue Ren*

Main category: cs.CL

TL;DR: 本文基于扩展心智理论和激进施动主义，提出以ABC（情感、行为、认知）为核心的翻译心智框架，将翻译视为身体参与、动态整合的活动，而非静态表征。


<details>
  <summary>Details</summary>
Motivation: 当前关于翻译过程的主流认知模型多依赖表征论，忽视了身体、环境和参与性在翻译中的作用。作者试图寻求一种超越以表征为中心的新范式，解释翻译心智的动态生成。

Method: 作者结合扩展心智理论、激进施动主义、预测加工和施动推理等认知科学理论，提出并论证了ABC框架，并用以阐释翻译行为中身体、情感、认知与环境的紧密交织。

Result: ABC框架揭示了翻译实践中译者心智能力的生成性和参与性，强调翻译是多层面、实时、动作导向的社会文化活动，文本意义在互动中共建。

Conclusion: 本文为翻译心智研究提供了非表征、动态参与的新视角，说明译者心智是通过脑-身-环境的回馈环路动态生成的，有助于深化对翻译作为社会实践的理解。

Abstract: Building on the Extended Mind (EM) theory and radical enactivism, this article suggests an alternative to representation-based models of the mind. We lay out a novel ABC framework of the translating mind, in which translation is not the manipulation of static interlingual correspondences but an enacted activity, dynamically integrating affective, behavioral, and cognitive (ABC) processes. Drawing on Predictive Processing and (En)Active Inference, we argue that the translator's mind emerges, rather than being merely extended, through loops of brain-body-environment interactions. This non-representational account reframes translation as skillful participation in sociocultural practice, where meaning is co-created in real time through embodied interaction with texts, tools, and contexts.

</details>


### [127] [Interpretable dimensions support an effect of agentivity and telicity on split intransitivity](https://arxiv.org/abs/2511.16824)
*Eva Neu,Brian Dillon,Katrin Erk*

Main category: cs.CL

TL;DR: 本文重新探讨非及物动词的语法分类及其与主动性（agentivity）和终结性（telicity）的关系，发现可解释性维度结合人工判断更能有效关联语义属性与句法行为。


<details>
  <summary>Details</summary>
Motivation: 以往研究认为主动性高的动词更偏向施事型（unergative）句法结构，终结性高的动词更偏向受格型（unaccusative）结构，但Kim等（2024）发现人工评分难以预测实际句法表现，因而有必要寻找更有效的分析方法。

Method: 本文利用可解释的语义维度（通过主动性和终结性量表两极的种子词计算），并结合人工判断数据，对非及物动词的句法和语义关系进行量化分析。

Result: 结果显示，结合可解释语义维度与人工评分，可以更好地揭示主动性/终结性与施事型/受格型句法之间的关系。

Conclusion: 本研究证明了可解释维度与人工判断相结合，在非及物动词的语法-语义关联分析中具有优势，有助于对难以通过简单打分任务评估的语义属性进行有效研究。

Abstract: Intransitive verbs fall into two different syntactic classes, unergatives and unaccusatives. It has long been argued that verbs describing an agentive action are more likely to appear in an unergative syntax, and those describing a telic event to appear in an unaccusative syntax. However, recent work by Kim et al. (2024) found that human ratings for agentivity and telicity were a poor predictor of the syntactic behavior of intransitives. Here we revisit this question using interpretable dimensions, computed from seed words on opposite poles of the agentive and telic scales. Our findings support the link between unergativity/unaccusativity and agentivity/telicity, and demonstrate that using interpretable dimensions in conjunction with human judgments can offer valuable evidence for semantic properties that are not easily evaluated in rating tasks.

</details>


### [128] [PEPPER: Perception-Guided Perturbation for Robust Backdoor Defense in Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.16830)
*Oscar Chew,Po-Yi Lu,Jayden Lin,Kuan-Hao Huang,Hsuan-Tien Lin*

Main category: cs.CL

TL;DR: 本文提出了一种名为PEPPER的新方法，通过语义重写输入提示词以防御文本到图像扩散模型中的后门攻击。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型容易受到后门攻击：在输入中插入特殊触发词会导致模型生成有害或异常内容，因此需要新的防御机制防止这些攻击。

Method: PEPPER通过将原始的输入提示词改写为语义上相距较远但视觉上相似的表达，同时加入不显眼的元素，以此击破嵌入后门触发词的影响，从而加强鲁棒性。该方法可与其他防御思路结合使用。

Result: 实验表明，PEPPER对基于文本编码器的后门攻击尤其有效，大幅降低了攻击成功率，且能保持生成图像质量。与现有防御联合使用时，进一步提升了防御性能。

Conclusion: PEPPER是一种简单且通用的防御方法，能够有效提升文本到图像扩散模型对后门攻击的鲁棒性，并与其它防御措施兼容，可推广使用。

Abstract: Recent studies show that text to image (T2I) diffusion models are vulnerable to backdoor attacks, where a trigger in the input prompt can steer generation toward harmful or unintended content. To address this, we introduce PEPPER (PErcePtion Guided PERturbation), a backdoor defense that rewrites the caption into a semantically distant yet visually similar caption while adding unobstructive elements. With this rewriting strategy, PEPPER disrupt the trigger embedded in the input prompt, dilute the influence of trigger tokens and thereby achieve enhanced robustness. Experiments show that PEPPER is particularly effective against text encoder based attacks, substantially reducing attack success while preserving generation quality. Beyond this, PEPPER can be paired with any existing defenses yielding consistently stronger and generalizable robustness than any standalone method. Our code will be released on Github.

</details>


### [129] [ConCISE: A Reference-Free Conciseness Evaluation Metric for LLM-Generated Answers](https://arxiv.org/abs/2511.16846)
*Seyed Mohssen Ghafari,Ronny Kol,Juan C. Quiroz,Nella Luan,Monika Patial,Chanaka Rupasinghe,Herman Wandabwa,Luiz Pizzato*

Main category: cs.CL

TL;DR: 本文提出了一种衡量大语言模型（LLMs）回答精炼程度的新指标，该指标无需参考标准答案，可自动检测冗余内容。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs常产生冗长、细节重复的回答，影响清晰度和用户体验，并增加开发和使用成本。因此需要一种衡量和减少冗余的方法。

Method: 提出了一种全新、无需参考答案的精炼度评价指标，结合三种压缩比例计算：1）模型生成回答与抽象性摘要的压缩比；2）与提取性摘要的压缩比；3）模型尽量删去非必要词后的压缩比。最终得分取三者均值。

Result: 实验结果表明，该指标有效识别了LLM输出中的冗余内容，可自动评价系统生成回答的简洁性，无需人工标注。

Conclusion: 该方法为会话式AI系统中自动评估文本简洁性提供了实用工具，减少人工参与，促进高效文本生成。

Abstract: Large language models (LLMs) frequently generate responses that are lengthy and verbose, filled with redundant or unnecessary details. This diminishes clarity and user satisfaction, and it increases costs for model developers, especially with well-known proprietary models that charge based on the number of output tokens. In this paper, we introduce a novel reference-free metric for evaluating the conciseness of responses generated by LLMs. Our method quantifies non-essential content without relying on gold standard references and calculates the average of three calculations: i) a compression ratio between the original response and an LLM abstractive summary; ii) a compression ratio between the original response and an LLM extractive summary; and iii) wordremoval compression, where an LLM removes as many non-essential words as possible from the response while preserving its meaning, with the number of tokens removed indicating the conciseness score. Experimental results demonstrate that our proposed metric identifies redundancy in LLM outputs, offering a practical tool for automated evaluation of response brevity in conversational AI systems without the need for ground truth human annotations.

</details>


### [130] [Improving Latent Reasoning in LLMs via Soft Concept Mixing](https://arxiv.org/abs/2511.16885)
*Kang Wang,Xiangyu Duan,Tianyi Du*

Main category: cs.CL

TL;DR: 本文提出了一种名为Soft Concept Mixing（SCM）的训练方法，让大型语言模型（LLMs）在训练时直接接触“软概念”表示，以提升推理能力。通过在模型的隐藏状态中融入概率加权的软概念向量，并用强化学习进行优化，SCM在五个推理基准测试上表现更佳，同时保持了训练的稳定性。


<details>
  <summary>Details</summary>
Motivation: 目前LLMs主要通过离散token生成进行推理，与人类在抽象概念空间中的推理方式不同，这种方式限制了其表达能力。为缩小LLMs在训练时的离散token与推理时的软概念之间的差距，需要新的训练方法。

Method: 提出了SCM训练方案，通过将多个embedding概率加权平均后生成“软概念”向量，并将其混入模型隐藏状态，整个推理优化过程则通过强化学习实现。

Result: 在五个推理基准任务上的实验显示，SCM不仅提升了LLMs在推理类任务上的表现，还保证了训练过程的稳定性。

Conclusion: SCM方法有效增强了LLMs的推理能力，并且在保证训练稳定性的同时，弥合了训练与推理中离散-连续表示的差距。

Abstract: Unlike human reasoning in abstract conceptual spaces, large language models (LLMs) typically reason by generating discrete tokens, which potentially limit their expressive power. The recent work Soft Thinking has shown that LLMs' latent reasoning via soft concepts is a promising direction, but LLMs are trained on discrete tokens. To reduce this gap between the soft concepts in reasoning and the discrete tokens in training, we propose Soft Concept Mixing (SCM), a soft concept aware training scheme that directly exposes the model to soft representations during training. Specifically, SCM constructs a soft concept vector by forming a probability-weighted average of embeddings. Then, this vector is mixed into the model's hidden states, which embody rich contextual information. Finally, the entire latent reasoning process is optimized with Reinforcement Learning (RL). Experiments on five reasoning benchmarks demonstrate that SCM improves the reasoning performance of LLMs, and simultaneously maintains a stable training dynamic.

</details>


### [131] [Deep Improvement Supervision](https://arxiv.org/abs/2511.16886)
*Arip Asadulaev,Rayan Banerjee,Fakhri Karray,Martin Takac*

Main category: cs.CL

TL;DR: 本论文提出了一种提升Tiny Recursive Models（TRMs）推理效率的新训练方法，在保持模型精度的同时大幅减少计算量，显著优于大模型在特定复杂推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前小型递归模型（如TRMs）在复杂推理任务上已超越大型语言模型，但其计算效率还有提升空间。作者希望通过微小方法改进，实现效率最大化。

Method: 作者将TRMs的潜在推理过程视作无分类器引导和隐式策略改进算法，并据此设计了一种为每次循环提供目标的新型训练方案。

Result: 新方法使得训练过程中前向传播次数减少18倍，且彻底省去了终止机制，仍保持标准TRMs的性能不变。在ARC-1任务中，仅用80万参数便达到了24%准确率，超过大多数LLMs。

Conclusion: 所提出的训练策略极大提升了小型模型在推理任务中的效率和可用性，为更高效利用小模型解决复杂智能任务提供了新途径。

Abstract: Recently, it was shown that small, looped architectures, such as Tiny Recursive Models (TRMs), can outperform Large Language Models (LLMs) on complex reasoning tasks, including the Abstraction and Reasoning Corpus (ARC). In this work, we investigate a core question: how can we further improve the efficiency of these methods with minimal changes? To address this, we frame the latent reasoning of TRMs as a form of classifier-free guidance and implicit policy improvement algorithm. Building on these insights, we propose a novel training scheme that provides a target for each loop during training. We demonstrate that our approach significantly enhances training efficiency. Our method reduces the total number of forward passes by 18x and eliminates halting mechanisms, while maintaining quality comparable to standard TRMs. Notably, we achieve 24% accuracy on ARC-1 with only 0.8M parameters, outperforming most LLMs.

</details>


### [132] [Predicting the Formation of Induction Heads](https://arxiv.org/abs/2511.16893)
*Tatsuya Aoyama,Ethan Gotlieb Wilcox,Nathan Schneider*

Main category: cs.CL

TL;DR: 本文研究了语言模型中归纳头（Induction Heads, IHs）形成与训练数据统计特性的关系。作者发现批量大小与上下文长度可预测IH形成的节点，双词重复频率与可靠性对IH形成有重要影响，并揭示了它们的帕累托前沿。当局部依赖具有高重复率和可靠性时，足以促成IH形成。


<details>
  <summary>Details</summary>
Motivation: 尽管归纳头被认为支撑了语言模型的强大上下文学习能力，但它们如何在训练过程中形成尚不明确。作者希望揭示IH形成背后的机制，并找出影响其形成的关键数据属性，以推动对大型语言模型泛化能力的理解。

Method: 作者结合自然与合成数据，通过调整训练数据的批量大小、上下文长度、双词（bigram）重复频率和可靠性，跟踪和分析IH的出现与演化规律，建立定量关系并绘制属性间的帕累托前沿。

Result: 结果显示，批量大小与上下文长度的简单组合可准确预测IH的形成时机；双词重复频率和可靠性共同决定IH形成的边界，且两者之间存在一条精确的帕累托前沿。同时，只有在双词重复频率与可靠性高的情况下，局部依赖才足以驱动IH形成；低频率与可靠性时，还需考虑分类性和边缘分布形状。

Conclusion: 归纳头的形成依赖于训练数据的统计特性，尤其是局部大重复和高可靠性的双词分布。本文为理解语言模型泛化能力的结构性基础提供了新的解释，并可为数据设计和模型优化提供理论指导。

Abstract: Arguably, specialized attention heads dubbed induction heads (IHs) underlie the remarkable in-context learning (ICL) capabilities of modern language models (LMs); yet, a precise characterization of their formation remains unclear. In this study, we investigate the relationship between statistical properties of training data (for both natural and synthetic data) and IH formation. We show that (1) a simple equation combining batch size and context size predicts the point at which IHs form; (2) surface bigram repetition frequency and reliability strongly affect the formation of IHs, and we find a precise Pareto frontier in terms of these two values; and (3) local dependency with high bigram repetition frequency and reliability is sufficient for IH formation, but when the frequency and reliability are low, categoriality and the shape of the marginal distribution matter.

</details>


### [133] [ARQUSUMM: Argument-aware Quantitative Summarization of Online Conversations](https://arxiv.org/abs/2511.16985)
*An Quang Tang,Xiuzhen Zhang,Minh Ngoc Dinh,Zhuang Li*

Main category: cs.CL

TL;DR: 该论文提出了一种面向论证的定量对话摘要方法，能揭示对话中的主张-理由结构并定量评估论点强度，所提ARQUSUMM方法在多个评价上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前线上对话平台（如Reddit）中，议题争议越来越多。仅依靠传统的对话摘要方法无法捕捉隐藏于争论背后的论证结构，且无法量化论点强度，造成对用户帮助有限。该论文旨在弥补此不足，通过明确展现和量化各方论点，让用户能更好理解复杂讨论。

Method: 论文提出了新的任务：“面向论证的定量摘要”，要求提取出会话中论据的主张-理由结构并量化；作者设计了ARQUSUMM框架，利用大语言模型进行论证单元识别与主张—理由关系抽取，然后采用论证结构感知的聚类算法进行聚合及数量化分析。

Result: 实验结果显示，ARQUSUMM优于现有对话摘要和定量摘要方法，无论在论证结构保真度、文本质量还是论点数量化准确性方面，都取得了更好效果。

Conclusion: ARQUSUMM为多元争议对话场景提供了更具结构性、可量化的摘要方式，有助于用户高效理解与评估网络讨论的复杂论点结构。

Abstract: Online conversations have become more prevalent on public discussion platforms (e.g. Reddit). With growing controversial topics, it is desirable to summarize not only diverse arguments, but also their rationale and justification. Early studies on text summarization focus on capturing general salient information in source documents, overlooking the argumentative nature of online conversations. Recent research on conversation summarization although considers the argumentative relationship among sentences, fail to explicate deeper argument structure within sentences for summarization. In this paper, we propose a novel task of argument-aware quantitative summarization to reveal the claim-reason structure of arguments in conversations, with quantities measuring argument strength. We further propose ARQUSUMM, a novel framework to address the task. To reveal the underlying argument structure within sentences, ARQUSUMM leverages LLM few-shot learning grounded in the argumentation theory to identify propositions within sentences and their claim-reason relationships. For quantitative summarization, ARQUSUMM employs argument structure-aware clustering algorithms to aggregate arguments and quantify their support. Experiments show that ARQUSUMM outperforms existing conversation and quantitative summarization models and generate summaries representing argument structures that are more helpful to users, of high textual quality and quantification accuracy.

</details>


### [134] [Supervised Fine Tuning of Large Language Models for Domain Specific Knowledge Graph Construction:A Case Study on Hunan's Historical Celebrities](https://arxiv.org/abs/2511.17012)
*Junjie Hao,Chun Wang,Ying Qiao,Qiuyue Zuo,Qiya Song,Hua Ma,Xieping Gao*

Main category: cs.CL

TL;DR: 本论文提出了一种面向区域历史文化领域高效信息抽取的方法，通过对主流大语言模型进行有针对性的指令微调，实现了对湖南近现代历史名人的生平属性、事件与社会关系信息的自动抽取及知识图谱构建，显著提升了模型在低资源场景下的提取效果。


<details>
  <summary>Details</summary>
Motivation: 当前对湖南历史名人等区域性文化知识的结构化数据资源严重匮乏，通用大模型在该领域的信息抽取与结构化输出能力有限，因此亟需开发可提升行业知识抽取性能的技术方案。

Method: 作者设计了专门适配湖南历史名人领域的细粒度指令与数据模板，利用指令微调（Instruction-tuning）机制在Qwen2.5-7B、Qwen3-8B、DeepSeek及Llama等多个开源大模型上进行参数高效微调，并建立了针对抽取效果的评测标准。

Result: 经过微调后，所有模型抽取性能均显著提升，其中Qwen3-8B模型在100个样本、50次训练后获得最高分89.3866，展示了良好的领域适应能力。

Conclusion: 研究证明，面向垂直领域进行大模型微调在区域历史文化知识抽取与知识图谱构建中具有高效、低成本的优势，为文化遗产信息化提供了新思路和方法。

Abstract: Large language models and knowledge graphs offer strong potential for advancing research on historical culture by supporting the extraction, analysis, and interpretation of cultural heritage. Using Hunan's modern historical celebrities shaped by Huxiang culture as a case study, pre-trained large models can help researchers efficiently extract key information, including biographical attributes, life events, and social relationships, from textual sources and construct structured knowledge graphs. However, systematic data resources for Hunan's historical celebrities remain limited, and general-purpose models often underperform in domain knowledge extraction and structured output generation in such low-resource settings. To address these issues, this study proposes a supervised fine-tuning approach for enhancing domain-specific information extraction. First, we design a fine-grained, schema-guided instruction template tailored to the Hunan historical celebrities domain and build an instruction-tuning dataset to mitigate the lack of domain-specific training corpora. Second, we apply parameter-efficient instruction fine-tuning to four publicly available large language models - Qwen2.5-7B, Qwen3-8B, DeepSeek-R1-Distill-Qwen-7B, and Llama-3.1-8B-Instruct - and develop evaluation criteria for assessing their extraction performance. Experimental results show that all models exhibit substantial performance gains after fine-tuning. Among them, Qwen3-8B achieves the strongest results, reaching a score of 89.3866 with 100 samples and 50 training iterations. This study provides new insights into fine-tuning vertical large language models for regional historical and cultural domains and highlights their potential for cost-effective applications in cultural heritage knowledge extraction and knowledge graph construction.

</details>


### [135] [Do Vision-Language Models Understand Visual Persuasiveness?](https://arxiv.org/abs/2511.17036)
*Gyuwon Park*

Main category: cs.CL

TL;DR: 本文探讨了视觉-语言模型（VLMs）是否真正理解视觉劝说力，并建立数据集和新分类法来分析其能力，最终发现它们难以将视觉要素与交流意图联系起来。


<details>
  <summary>Details</summary>
Motivation: 虽然VLMs已在多模态推理中取得进展，但尚不清楚它们是否能真正理解视觉如何影响人的态度和决策（即视觉劝说力）。本研究希望评估VLMs在此能力上的表现，并诊断其局限性。

Method: 1）构建高一致性二元劝说力判断数据集；2）提出视觉劝说因子（VPFs）体系，包括低、中、高层视觉语义要素；3）设计认知引导和知识注入等干预策略，提高推理相关性；4）在多种VLMs上实证评测其表现和偏差。

Result: 发现VLMs偏向回忆（recall），经常高估劝说力，对低/中层特征区分弱；高层语义中“信息和物体出现的一致性”最能预测人类判断。简单指令或无指导推理干预无效甚至降低表现，而简明的、基于物体的推理可显著提升精度及F1分数。

Conclusion: VLMs当前核心局限在于难以将识别到的劝说性物体与交流意图联系起来，这亦指明了未来改进的方向。

Abstract: Recent advances in vision-language models (VLMs) have enabled impressive multi-modal reasoning and understanding. Yet, whether these models truly grasp visual persuasion-how visual cues shape human attitudes and decisions-remains unclear. To probe this question, we construct a high-consensus dataset for binary persuasiveness judgment and introduce the taxonomy of Visual Persuasive Factors (VPFs), encompassing low-level perceptual, mid-level compositional, and high-level semantic cues. We also explore cognitive steering and knowledge injection strategies for persuasion-relevant reasoning. Empirical analysis across VLMs reveals a recall-oriented bias-models over-predict high persuasiveness-and weak discriminative power for low/mid-level features. In contrast, high-level semantic alignment between message and object presence emerges as the strongest predictor of human judgment. Among intervention strategies, simple instruction or unguided reasoning scaffolds yield marginal or negative effects, whereas concise, object-grounded rationales significantly improve precision and F1 scores. These results indicate that VLMs core limitation lies not in recognizing persuasive objects but in linking them to communicative intent.

</details>


### [136] [Principled Design of Interpretable Automated Scoring for Large-Scale Educational Assessments](https://arxiv.org/abs/2511.17069)
*Yunsung Kim,Mike Hardy,Joseph Tey,Candace Thille,Chris Piech*

Main category: cs.CL

TL;DR: 本论文提出了一种可解释性的自动评分系统——AnalyticScore，它在保证高评分准确度的同时，提升系统在透明度和可解释性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 自动评分系统在大规模评估中应用广泛，但主流系统缺乏可解释性和透明度，难以满足不同利益相关方（如考生、教师、考试机构）对于评分原理和过程的理解需求。为了解决该领域欠缺广泛认可的可解释自动评分解决方案的问题，作者提出要深入研究和实践评分系统的可解释性。

Method: 作者分析了不同利益相关方对可解释性自动评分的需求，提出了针对这些需求的四项原则：真实性、可证据性、可追溯性和可替换性（FGTI）。在此基础上，开发了AnalyticScore 框架：首先提取答案中的显式元素，然后利用大型语言模型（LLM）将答案特征化为易于理解的属性，最后用序数逻辑回归模型进行评分。

Result: AnalyticScore 框架在评分准确度上，优于许多不可解释评分方法，在ASAP-SAS数据集的10个任务上，平均仅比不可解释的最优方法低0.06 QWK。此外，通过与人工特征提取相比，AnalyticScore 的特征化行为与人类高度一致。

Conclusion: AnalyticScore验证了基于FGTI原则开发的自动评分系统在可解释性和准确性之间达到了良好平衡，为后续可解释自动评分研究提供了可操作的参考框架，具备实际应用价值。

Abstract: AI-driven automated scoring systems offer scalable and efficient means of evaluating complex student-generated responses. Yet, despite increasing demand for transparency and interpretability, the field has yet to develop a widely accepted solution for interpretable automated scoring to be used in large-scale real-world assessments. This work takes a principled approach to address this challenge. We analyze the needs and potential benefits of interpretable automated scoring for various assessment stakeholders and develop four principles of interpretability -- Faithfulness, Groundedness, Traceability, and Interchangeability (FGTI) -- targeted at those needs. To illustrate the feasibility of implementing these principles, we develop the AnalyticScore framework for short answer scoring as a baseline reference framework for future research. AnalyticScore operates by (1) extracting explicitly identifiable elements of the responses, (2) featurizing each response into human-interpretable values using LLMs, and (3) applying an intuitive ordinal logistic regression model for scoring. In terms of scoring accuracy, AnalyticScore outperforms many uninterpretable scoring methods, and is within only 0.06 QWK of the uninterpretable SOTA on average across 10 items from the ASAP-SAS dataset. By comparing against human annotators conducting the same featurization task, we further demonstrate that the featurization behavior of AnalyticScore aligns well with that of humans.

</details>


### [137] [MUCH: A Multilingual Claim Hallucination Benchmark](https://arxiv.org/abs/2511.17081)
*Jérémie Dentan,Alexi Canesse,Davide Buscaldi,Aymen Shabou,Sonia Vanier*

Main category: cs.CL

TL;DR: 本文提出了MUCH，这是首个针对Claim-level（声明级）不确定性量化（UQ）的基准，涵盖四种欧洲语言和四种开放权重的大语言模型，提供丰富数据和高效分段算法，为相关方法的真实场景评测和后续算法发展提供强有力的支持。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在生成内容时存在可靠性不足的问题，声明级不确定性量化有望缓解这一挑战，而当前在真实条件且可复现的评测基准和分段技术上还存在明显不足。

Method: 作者建立了MUCH基准，收集和整理了4873个多语言、多模型的实例，同时首次公开每个token的24组生成logit，便于白盒算法开发；并提出一种新型确定性声明分段算法，大幅减少分段的耗时（仅占生成时间0.2%），可满足实时代码监控需求。

Result: MUCH基准和分段算法已构建完毕，实验表明，现有不确定性量化方法无论在性能还是效率上都还有很大改进空间。

Conclusion: MUCH为声明级模型不确定性量化研究提供了公平、现实且高效的基准平台，能够助力该领域方法的推进，但现有方法还无法完全满足实际需求。

Abstract: Claim-level Uncertainty Quantification (UQ) is a promising approach to mitigate the lack of reliability in Large Language Models (LLMs). We introduce MUCH, the first claim-level UQ benchmark designed for fair and reproducible evaluation of future methods under realistic conditions. It includes 4,873 samples across four European languages (English, French, Spanish, and German) and four instruction-tuned open-weight LLMs. Unlike prior claim-level benchmarks, we release 24 generation logits per token, facilitating the development of future white-box methods without re-generating data. Moreover, in contrast to previous benchmarks that rely on manual or LLM-based segmentation, we propose a new deterministic algorithm capable of segmenting claims using as little as 0.2% of the LLM generation time. This makes our segmentation approach suitable for real-time monitoring of LLM outputs, ensuring that MUCH evaluates UQ methods under realistic deployment constraints. Finally, our evaluations show that current methods still have substantial room for improvement in both performance and efficiency.

</details>


### [138] [Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design](https://arxiv.org/abs/2511.17127)
*Quentin Anthony,Yury Tokpanov,Skyler Szot,Srivatsan Rajagopal,Praneeth Medepalli,Rishi Iyer,Vasu Shyam,Anna Golubeva,Ansh Chaurasia,Xiao Yang,Tomas Figliolia,Robert Washbourne,Drew Thorstensen,Amartey Pearson,Zack Grossbart,Jason van Patten,Emad Barsoum,Zhenyu Gu,Yao Fu,Beren Millidge*

Main category: cs.CL

TL;DR: 本论文首次在纯AMD硬件（MI300X GPU和Pollara互连）上，进行了大规模MoE（专家混合模型）预训练研究，展示了系统和模型设计的实用经验，并发布了基线模型ZAYA1。


<details>
  <summary>Details</summary>
Motivation: 现有大规模MoE预训练多采用Nvidia硬件，AMD平台的可行性及优化方法尚未充分探索。作者希望验证AMD硬件在大规模AI模型训练的能力，并为系统与模型设计提供参考。

Method: 论文对Pollara集群及网络特性进行详细测试，包括多种通信操作在不同消息大小与GPU数量下的微基准测试，并分析MI300X在内核尺寸和内存带宽方面的表现。模型上提出了符合MI300X特性的Transformer设计规范，并探讨了MoE宽度的权衡，详细描述训练流程及相关工具。

Result: 提出的ZAYA1（8.3B参数，760M激活MoE）模型在推理、数学和编程基准测试上，与Qwen3-4B/Gemma3-12B表现相当甚至优于Llama-3-8B、OLMoE等。系统和软硬件优化证明了AMD平台完全胜任大模型训练任务。

Conclusion: AMD GPU（MI300X）结合Pollara网络及优化的软件栈，已经成熟并可支持竟争性的超大规模MoE模型预训练，具备与行业领先Nvidia平台匹敌的能力。

Abstract: We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs with Pollara interconnect. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts on Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining.

</details>


### [139] [Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation](https://arxiv.org/abs/2511.17129)
*Yeqin Zhang,Yizheng Zhao,Chen Hu,Binxing Jiao,Daxin Jiang,Ruihang Miao,Cam-Tu Nguyen*

Main category: cs.CL

TL;DR: 本文提出通过上下文压缩任务对大语言模型（LLM）进行无监督适应训练，以提升其文本表征能力，并优于依赖于传统token级预测任务的方法。


<details>
  <summary>Details</summary>
Motivation: 大多数LLM为因果模型，优化目标为下一词预测，难以生成全面的文本表征。现有对LLM的适配工作多依赖于token级任务（如LLM2Vec的掩码预测），限制了表征能力。本研究期望寻找新型自监督任务提升LLM文本表征。

Method: 作者提出以上下文压缩为预训练任务，引导LLM生成能代表整体语境的紧凑记忆token，并用其进行后续序列预测。同时结合对比学习进一步提升表征能力。

Result: 实验结果表明，精心设计的上下文压缩目标能显著提升LLM的文本表征能力，优于传统token级任务。结合对比学习的LLM2Comp在多个下游任务中都优于现有LLM表征方法，并且所需训练数据更少。

Conclusion: 上下文压缩作为预训练任务，是有效提升LLM文本表征性能的新途径，可在多个任务上取得领先表现且更为高效。

Abstract: Text representation plays a critical role in tasks like clustering, retrieval, and other downstream applications. With the emergence of large language models (LLMs), there is increasing interest in harnessing their capabilities for this purpose. However, most of the LLMs are inherently causal and optimized for next-token prediction, making them suboptimal for producing holistic representations. To address this, recent studies introduced pretext tasks to adapt LLMs for text representation. Most of these tasks, however, rely on token-level prediction objectives, such as the masked next-token prediction (MNTP) used in LLM2Vec. In this work, we explore the untapped potential of context compression as a pretext task for unsupervised adaptation of LLMs. During compression pre-training, the model learns to generate compact memory tokens, which substitute the whole context for downstream sequence prediction. Experiments demonstrate that a well-designed compression objective can significantly enhance LLM-based text representations, outperforming models trained with token-level pretext tasks. Further improvements through contrastive learning produce a strong representation model (LLM2Comp) that outperforms contemporary LLM-based text encoders on a wide range of tasks while being more sample-efficient, requiring significantly less training data.

</details>


### [140] [LangMark: A Multilingual Dataset for Automatic Post-Editing](https://arxiv.org/abs/2511.17153)
*Diego Velazquez,Mikaela Grace,Konstantinos Karageorgos,Lawrence Carin,Aaron Schliem,Dimitrios Zaikis,Roger Wechsler*

Main category: cs.CL

TL;DR: 本文提出了LangMark——一个涵盖英语到七种语言、包含人工修订的多语种自动后编辑（APE）数据集，并展示了大模型利用该数据集可以显著提升机器翻译后编辑效果。


<details>
  <summary>Details</summary>
Motivation: 目前神经机器翻译（NMT）虽然有进步，但自动后编辑（APE）系统的发展受限于缺乏针对NMT输出的大规模多语种数据集。缺乏数据使得机器自动修正翻译结果的效果难以提升。

Method: 作者提出并发布了LangMark数据集，包含英语到葡萄牙语（巴西）、法语、德语、意大利语、日语、俄语和西班牙语等七种语言，共计206,983组三元组（原文、NMT输出和人工后编辑翻译），且由专业语言学家人工标注。利用该数据集，作者用大语言模型（LLMs）和少样本提示方法对APE任务进行了实证研究。

Result: 实验结果显示，大语言模型在少样本提示条件下能有效执行APE任务，并能在APE表现上超越主流商用和专有机器翻译系统。

Conclusion: LangMark数据集为APE系统的研究和评估平台提供了新的资源，有望助力未来相关技术的发展与优化。

Abstract: Automatic post-editing (APE) aims to correct errors in machine-translated text, enhancing translation quality, while reducing the need for human intervention. Despite advances in neural machine translation (NMT), the development of effective APE systems has been hindered by the lack of large-scale multilingual datasets specifically tailored to NMT outputs. To address this gap, we present and release LangMark, a new human-annotated multilingual APE dataset for English translation to seven languages: Brazilian Portuguese, French, German, Italian, Japanese, Russian, and Spanish. The dataset has 206,983 triplets, with each triplet consisting of a source segment, its NMT output, and a human post-edited translation. Annotated by expert human linguists, our dataset offers both linguistic diversity and scale. Leveraging this dataset, we empirically show that Large Language Models (LLMs) with few-shot prompting can effectively perform APE, improving upon leading commercial and even proprietary machine translation systems. We believe that this new resource will facilitate the future development and evaluation of APE systems.

</details>


### [141] [The PLLuM Instruction Corpus](https://arxiv.org/abs/2511.17161)
*Piotr Pęzik,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Maciej Chrabąszcz,Anna Kołos,Agnieszka Karlińska,Karolina Seweryn,Aleksandra Krasnodębska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Artur Wilczek,Maciej Trzciński,Katarzyna Dziewulska,Roman Roszko,Tomasz Bernaś,Jurgita Vaičenonienė,Danuta Roszko,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Alina Wróblewska,Katarzyna Krasnowska-Kieraś,Maciej Ogrodniczuk,Michał Rudolf,Piotr Rybak,Karolina Saputa,Joanna Wołoszyn,Marcin Oleksy,Bartłomiej Koptyra,Teddy Ferdinan,Stanisław Woźniak,Maciej Piasecki,Paweł Walkowiak,Konrad Wojtasik,Arkadiusz Janz,Przemysław Kazienko,Julia Moska,Jan Kocoń*

Main category: cs.CL

TL;DR: 本文介绍了PLLuM项目中用于微调变换器大型语言模型（LLM）的指令数据集，并发布了代表性子集PLLuMIC以供他人参考和使用。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型（LLM）训练通常依赖多样化的指令数据集，但不同来源（人工、有机、合成指令）的指令对模型的适应和表现影响尚不明确。该研究旨在系统分析这些指令类型对LLM微调和本地化的作用，并为其它LLM提供构建数据集的参考。

Method: 作者对PLLuM项目中的指令数据集按来源进行了功能类型学分类（有机、转换、合成），并比较了人类编写的和合成指令对基础LLM语言适配的影响。同时，公开了PLLuM指令语料库的一个代表性子集（PLLuMIC）。

Result: 论文呈现了不同类型指令数据集的分类和使用观察，揭示了人类指令与合成指令在微调LLM过程中的异同及其对语言本地化的影响。

Conclusion: 人类编写和合成指令各有优势，深入理解和合理选择指令数据集类型对LLM本地化和适应特定语言有重要意义。PLLuMIC数据集的发布为其他LLM项目构建高质量指令数据集提供了有价值的参考。

Abstract: This paper describes the instruction dataset used to fine-tune a set of transformer-based large language models (LLMs) developed in the PLLuM (Polish Large Language Model) project. We present a functional typology of the organic, converted, and synthetic instructions used in PLLuM and share some observations about the implications of using human-authored versus synthetic instruction datasets in the linguistic adaptation of base LLMs. Additionally, we release the first representative subset of the PLLuM instruction corpus (PLLuMIC), which we believe to be useful in guiding and planning the development of similar datasets for other LLMs.

</details>


### [142] [Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models](https://arxiv.org/abs/2511.17170)
*Vy Nguyen,Ziqi Xu,Jeffrey Chan,Estrid He,Feng Xia,Xiuzhen Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为'基于要素的因果性弃答'（ABCA）的新方法，通过分析大模型内部知识的多样性，实现更早、更可靠的弃答判断，并在标准评测上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 当前大模型常因幻觉现象产生不可靠输出，虽然弃答机制能避免谬误，但现有方法多依赖生成后信号，难以及时阻止不可靠生成。亟需一种能在生成前主动判断并实施弃答的方法。

Method: 提出ABCA框架，利用因果推断技术分析大模型知识的不同方面（如学科、法律、时间等），计算这些方面对答案可靠性的因果影响，根据影响一致性区分为知识冲突型弃答和知识不足型弃答，从而实现早期、高效的弃答。

Result: 在标准测试基准上，ABCA方法提升了弃答的可靠性，并达到了当前最优表现，同时提高了弃答决策的可解释性。

Conclusion: ABCA能有效提前检测并规避风险输出，减少幻觉现象带来的负面影响，并为大模型的安全可靠应用提供了新路径。

Abstract: Large Language Models (LLMs) often produce fluent but factually incorrect responses, a phenomenon known as hallucination. Abstention, where the model chooses not to answer and instead outputs phrases such as "I don't know", is a common safeguard. However, existing abstention methods typically rely on post-generation signals, such as generation variations or feedback, which limits their ability to prevent unreliable responses in advance. In this paper, we introduce Aspect-Based Causal Abstention (ABCA), a new framework that enables early abstention by analysing the internal diversity of LLM knowledge through causal inference. This diversity reflects the multifaceted nature of parametric knowledge acquired from various sources, representing diverse aspects such as disciplines, legal contexts, or temporal frames. ABCA estimates causal effects conditioned on these aspects to assess the reliability of knowledge relevant to a given query. Based on these estimates, we enable two types of abstention: Type-1, where aspect effects are inconsistent (knowledge conflict), and Type-2, where aspect effects consistently support abstention (knowledge insufficiency). Experiments on standard benchmarks demonstrate that ABCA improves abstention reliability, achieves state-of-the-art performance, and enhances the interpretability of abstention decisions.

</details>


### [143] [Attention-Guided Feature Fusion (AGFF) Model for Integrating Statistical and Semantic Features in News Text Classification](https://arxiv.org/abs/2511.17184)
*Mohammad Zare*

Main category: cs.CL

TL;DR: 本文提出了AGFF模型，通过注意力机制融合统计特征和语义特征，显著提升新闻文本分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统文本分类方法依赖统计特征（如词频、TF-IDF），虽有效但缺乏上下文理解；而深度学习语义方法虽能捕捉上下文，但忽略了高影响力的统计指标。因此需要将两者优势结合，提升新闻分类表现。

Method: 提出了一种注意力引导特征融合（AGFF）模型，在统一框架下结合统计与语义特征，利用注意力机制动态分配每种特征的重要性权重进行文本分类。

Result: 在基准新闻数据集上，AGFF模型性能优于传统统计模型和纯语义深度学习模型。消融实验验证了各个融合组件的贡献。

Conclusion: 统计与语义特征的战略性整合明显提升了分类效果，AGFF模型通过平衡并利用两者优势，为实际新闻文本分类提供了实用高效的解决方案。

Abstract: News text classification is a crucial task in natural language processing, essential for organizing and filtering the massive volume of digital content. Traditional methods typically rely on statistical features like term frequencies or TF-IDF values, which are effective at capturing word-level importance but often fail to reflect contextual meaning. In contrast, modern deep learning approaches utilize semantic features to understand word usage within context, yet they may overlook simple, high-impact statistical indicators. This paper introduces an Attention-Guided Feature Fusion (AGFF) model that combines statistical and semantic features in a unified framework. The model applies an attention-based mechanism to dynamically determine the relative importance of each feature type, enabling more informed classification decisions. Through evaluation on benchmark news datasets, the AGFF model demonstrates superior performance compared to both traditional statistical models and purely semantic deep learning models. The results confirm that strategic integration of diverse feature types can significantly enhance classification accuracy. Additionally, ablation studies validate the contribution of each component in the fusion process. The findings highlight the model's ability to balance and exploit the complementary strengths of statistical and semantic representations, making it a practical and effective solution for real-world news classification tasks.

</details>


### [144] [AutoLink: Autonomous Schema Exploration and Expansion for Scalable Schema Linking in Text-to-SQL at Scale](https://arxiv.org/abs/2511.17190)
*Ziyang Wang,Yuanlei Zheng,Zhenbiao Cao,Xiaojin Zhang,Zhongyu Wei,Pei Fu,Zhenbo Luo,Wei Chen,Xiang Bai*

Main category: cs.CL

TL;DR: AutoLink提出了一种新型自治智能体框架，将模式链接(schema linking)过程设为迭代探索，有效提升大规模数据库下text-to-SQL性能与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在工业级text-to-SQL应用中，受限于上下文窗口，无法直接输入整个数据库schema给大型语言模型。现有schema linking方法成本高、召回和噪声难以兼顾、不适用于大规模数据库。亟需高效、可扩展、低噪声的schema链接新方案。

Method: 提出AutoLink，将schema linking重构为自治体驱动的迭代探索，不输入全量数据库schema。在LLM引导下，逐步发现查询所需的schema子集。

Result: AutoLink在Bird-Dev和Spider-2.0-Lite数据集上实现了state-of-the-art的schema linking召回率（97.4%和91.2%），执行准确率在Bird-Dev和Spider-2.0-Lite上分别为68.7%和34.9%，表现优于或接近最优现有方案。

Conclusion: AutoLink极具可扩展性，大幅提升大规模schema下的召回、准确率与token效率，是工业级text-to-SQL高召回、强扩展的schema linking新解决方案。

Abstract: For industrial-scale text-to-SQL, supplying the entire database schema to Large Language Models (LLMs) is impractical due to context window limits and irrelevant noise. Schema linking, which filters the schema to a relevant subset, is therefore critical. However, existing methods incur prohibitive costs, struggle to trade off recall and noise, and scale poorly to large databases. We present \textbf{AutoLink}, an autonomous agent framework that reformulates schema linking as an iterative, agent-driven process. Guided by an LLM, AutoLink dynamically explores and expands the linked schema subset, progressively identifying necessary schema components without inputting the full database schema. Our experiments demonstrate AutoLink's superior performance, achieving state-of-the-art strict schema linking recall of \textbf{97.4\%} on Bird-Dev and \textbf{91.2\%} on Spider-2.0-Lite, with competitive execution accuracy, i.e., \textbf{68.7\%} EX on Bird-Dev (better than CHESS) and \textbf{34.9\%} EX on Spider-2.0-Lite (ranking 2nd on the official leaderboard). Crucially, AutoLink exhibits \textbf{exceptional scalability}, \textbf{maintaining high recall}, \textbf{efficient token consumption}, and \textbf{robust execution accuracy} on large schemas (e.g., over 3,000 columns) where existing methods severely degrade-making it a highly scalable, high-recall schema-linking solution for industrial text-to-SQL systems.

</details>


### [145] [E$^3$-Pruner: Towards Efficient, Economical, and Effective Layer Pruning for Large Language Models](https://arxiv.org/abs/2511.17205)
*Tao Yuan,Haoli Bai,Yinfei Pan,Xuyang Cao,Tianyu Zhang,Lu Hou,Ting Hu,Xianzhi Yu*

Main category: cs.CL

TL;DR: 本文提出了一种高效的层剪枝框架，可在保证性能的同时，显著减少训练与推理成本。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模的不断扩大，模型压缩成为亟需解决的问题。现有层剪枝方法在性能损失、训练成本和实际加速效果方面难以兼顾，制约了其在实际部署中的应用。

Method: 提出了一个名为\name的新框架，包含两大创新：1）采用Gumbel-TopK采样器实现可微分掩码优化，从而高效准确地搜索剪枝掩码；2）设计了基于熵感知的自适应知识蒸馏策略，提升任务表现。

Result: 在多种模型和基准测试上进行大量实验，证明了该方法优于当前最优技术。尤其在Qwen3-32B模型上剪枝25%层后，在MATH-500基准上精度仅下降0.8%，速度提升1.33倍，明显优于竞品方案。

Conclusion: 提出的方法兼顾了高效性、经济性和实用性，在不牺牲模型性能的前提下，大幅提升了训练和推理的效率，有望加速大模型在实际场景的落地。

Abstract: With the increasing size of large language models, layer pruning has gained increased attention as a hardware-friendly approach for model compression. However, existing layer pruning methods struggle to simultaneously address key practical deployment challenges, including performance degradation, high training costs, and limited acceleration. To overcome these limitations, we propose \name, a task-\underline{E}ffective, training-\underline{E}conomical and inference-\underline{E}fficient layer pruning framework. \namespace introduces two key innovations: (1) a differentiable mask optimization method using a Gumbel-TopK sampler, enabling efficient and precise pruning mask search; and (2) an entropy-aware adaptive knowledge distillation strategy that enhances task performance. Extensive experiments over diverse model architectures and benchmarks demonstrate the superiority of our method over state-of-the-art approaches. Notably, \namespace achieves 96\% accuracy, a mere 0.8\% drop from the original model (96.8\%) on MATH-500 when pruning 25\% layers of Qwen3-32B, outperforming existing SOTA (95\%), with a 1.33$\times$ inference speedup by consuming merely 0.5B tokens (0.5\% of the post-training data volume).

</details>


### [146] [A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents](https://arxiv.org/abs/2511.17208)
*Sizhe Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种以事件为中心的对话历史表示方法，能够提升大语言模型（LLM）在长时对话中的连贯性与个性化，优于现有的外部记忆方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在多轮对话中难以维持连贯性和个性化，主要受限于固定上下文窗口和外部记忆方法的检索粒度与信息损失。作者希望通过新的事件语义表示方法，改进记忆检索和利用效果。

Method: 作者提出以neo-Davidsonian事件语义为基础，将对话历史分解为包含参与者、时间线索和上下文的“事件单元”（EDU），利用LLM进行归一化与属性标注，并以异构图组织，用密集检索和LLM过滤方法增强信息回忆，允许跨EDU聚合证据。

Result: 该方法在LoCoMo与LongMemEval_S基准测试中表现出色，Q&A上下文显著缩短的情况下，仍达到或超越了强基线方法。

Conclusion: 以事件为基础的记忆结构为长时对话体智能体提供了更简单、有效且信息保存更完整的解决方案，为实际落地奠定了基础。

Abstract: LLM-based conversational agents still struggle to maintain coherent, personalized interaction over many sessions: fixed context windows limit how much history can be kept in view, and most external memory approaches trade off between coarse retrieval over large chunks and fine-grained but fragmented views of the dialogue. Motivated by neo-Davidsonian event semantics, we propose an event-centric alternative that represents conversational history as short, event-like propositions which bundle together participants, temporal cues, and minimal local context, rather than as independent relation triples or opaque summaries. In contrast to work that aggressively compresses or forgets past content, our design aims to preserve information in a non-compressive form and make it more accessible, rather than more lossy. Concretely, we instruct an LLM to decompose each session into enriched elementary discourse units (EDUs) -- self-contained statements with normalized entities and source turn attributions -- and organize sessions, EDUs, and their arguments in a heterogeneous graph that supports associative recall. On top of this representation we build two simple retrieval-based variants that use dense similarity search and LLM filtering, with an optional graph-based propagation step to connect and aggregate evidence across related EDUs. Experiments on the LoCoMo and LongMemEval$_S$ benchmarks show that these event-centric memories match or surpass strong baselines, while operating with much shorter QA contexts. Our results suggest that structurally simple, event-level memory provides a principled and practical foundation for long-horizon conversational agents. Our code and data will be released at https://github.com/KevinSRR/EMem.

</details>


### [147] [Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs](https://arxiv.org/abs/2511.17220)
*Yusuf Çelebi,Mahmoud El Hussieni,Özay Ezerceli*

Main category: cs.CL

TL;DR: 本文提出了PARROT框架，用于系统衡量大语言模型（LLMs）在权威与说服等社交压力下准确率的下降，评估模型受“顺从性”影响的稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在面对权威或诱导性输入时常表现出过度顺从，容易被影响其输出准确性。这种顺从现象对大模型的安全与应用稳健性带来风险，因此亟需系统研究其表现及机制。

Method: PARROT框架通过对比普通问题与权威性错误引导问题，采用双盲评测手段来隔离变量；利用基于对数似然的置信度追踪方法量化模型信心变化；并构建八分类行为模式，细致归纳模型在压力下的“顺从性”表现。测试对象覆盖22种模型、13个领域、共1302道标准选择题。

Result: 实验显示，先进模型（如GPT-5、GPT-4.1、Claude Sonnet 4.5）在权威压力下表现稳健，顺从比例极低（不超过11%），准确率下降很小；而老旧或小规模模型易出现知识“崩溃”，顺从比例甚高（如Qwen 2.5-1.5B可达94%）。薄弱模型不仅易被诱导错误，且会削弱对正确信息的信心。国际法与全球知识等领域脆弱，基础数学表现更为稳健。

Conclusion: 部署大语言模型时，不仅要关注准确率，还应将“抗过度顺从压力能力”作为关键安全目标，与避免伤害与隐私保护同等重要。

Abstract: This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low "follow rates" ($\leq 11\%$, GPT-5: 4\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\%, Qwen 2.5-1.5B: 94\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of "resistance to overfitting pressure" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.

</details>


### [148] [Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables](https://arxiv.org/abs/2511.17238)
*Anshul Singh,Rohan Chaudhary,Gagneet Singh,Abhay Kumary*

Main category: cs.CL

TL;DR: 论文提出了MirageTVQA数据集，用于更真实地评估视觉语言模型（VLMs）在多语言和有噪声表格数据上的问答能力。实验显示现有主流模型在这种环境下表现大幅下降，且存在明显语言偏见。


<details>
  <summary>Details</summary>
Motivation: 现有表格问答数据集单一（多为英文、干净格式），无法反映实际应用中多语言、低质量扫描表格等复杂情况。因此有必要提供更现实的评测基准。

Method: 作者构建了MirageTVQA数据集，包含约6万组问答，覆盖24种语言，表格含有视觉噪声以模拟真实扫描文档。利用该数据集系统评测了当前主流的视觉语言模型。

Result: 主流VLM在有视觉噪声的多语言表格场景下性能显著下降（最好模型降幅超35%），且存在显著的英语优先偏向，模型对其他语言的推理能力转移不足。

Conclusion: MirageTVQA成功揭示了现有VLM在实际多语言视觉表格问答场景中的薄弱环节，该基准能推动更健壮VLM的发展，相关数据和代码已开源。

Abstract: The impressive performance of VLMs is largely measured on benchmarks that fail to capture the complexities of real-world scenarios. Existing datasets for tabular QA, such as WikiTableQuestions and FinQA, are overwhelmingly monolingual (English) and present tables in a digitally perfect, clean format. This creates a significant gap between research and practice. To address this, we present \textbf{MirageTVQA}, a new benchmark designed to evaluate VLMs on these exact dimensions. Featuring nearly 60,000 QA pairs across 24 languages, MirageTVQA challenges models with tables that are not only multilingual but also visually imperfect, incorporating realistic noise to mimic scanned documents. Our evaluation of the leading VLMs reveals two primary failure points: a severe degradation in performance (over 35\% drop for the best models) when faced with visual noise and a consistent English-first bias where reasoning abilities fail to transfer to other languages. MirageTVQA provides a benchmark for measuring and driving progress towards more robust VLM models for table reasoning. The dataset and the code are available at: https://github.com/anshulsc/MirageTVQA.

</details>


### [149] [Social-Media Based Personas Challenge: Hybrid Prediction of Common and Rare User Actions on Bluesky](https://arxiv.org/abs/2511.17241)
*Benjamin White,Anastasia Shimorina*

Main category: cs.CL

TL;DR: 本文提出一种混合方法预测社交媒体用户在常见和罕见行为上的行动，基于Bluesky大数据集实验，取得行业领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于社交媒体用户的常规行为（如点赞、转发），对罕见但重要的行为关注不足，而这类行为的有效预测对平台设计和内容推荐具有重要价值。

Method: 本文提出结合四种方法：（1）基于历史响应模式的查表系统；（2）针对常规行为的融合时序和语义特征的Persona定制LightGBM模型；（3）专门设计的融合文本与时序特征的神经网络罕见行为分类器；（4）文本回复生成器。并基于Bluesky包含12种行为、25类Persona的数据集对方法效果进行验证。

Result: Persona定制模型在常规行为预测上平均macro F1达到0.64，罕见行为分类器在10类罕见行为上的macro F1为0.56。该方法在COLM 2025的SocialSim比赛中获第一名。

Conclusion: 社交媒体用户行为预测需针对常规和罕见行为采用差异化建模策略，混合方法可有效提升整体预测表现，有助于内容推荐系统和平台优化。

Abstract: Understanding and predicting user behavior on social media platforms is crucial for content recommendation and platform design. While existing approaches focus primarily on common actions like retweeting and liking, the prediction of rare but significant behaviors remains largely unexplored. This paper presents a hybrid methodology for social media user behavior prediction that addresses both frequent and infrequent actions across a diverse action vocabulary. We evaluate our approach on a large-scale Bluesky dataset containing 6.4 million conversation threads spanning 12 distinct user actions across 25 persona clusters. Our methodology combines four complementary approaches: (i) a lookup database system based on historical response patterns; (ii) persona-specific LightGBM models with engineered temporal and semantic features for common actions; (iii) a specialized hybrid neural architecture fusing textual and temporal representations for rare action classification; and (iv) generation of text replies. Our persona-specific models achieve an average macro F1-score of 0.64 for common action prediction, while our rare action classifier achieves 0.56 macro F1-score across 10 rare actions. These results demonstrate that effective social media behavior prediction requires tailored modeling strategies recognizing fundamental differences between action types. Our approach achieved first place in the SocialSim: Social-Media Based Personas challenge organized at the Social Simulation with LLMs workshop at COLM 2025.

</details>


### [150] [Estonian WinoGrande Dataset: Comparative Analysis of LLM Performance on Human and Machine Translation](https://arxiv.org/abs/2511.17290)
*Marii Ojastu,Hele-Andra Kuulmets,Aleksei Dorkin,Marika Borovikova,Dage Särg,Kairit Sirts*

Main category: cs.CL

TL;DR: 研究团队将WinoGrande常识推理基准测试集本地化并文化适应地翻译成了爱沙尼亚语，评估了大模型在人工翻译和机器翻译数据上的表现，发现人工翻译效果更好，机器翻译和提示工程提升有限，强调专家参与的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前主流常识推理基准（如WinoGrande）多为英语，直接用于其他语言存在翻译和文化差异等问题。为评估大语言模型在低资源语言（如爱沙尼亚语）上的语言理解和推理能力，有必要将数据集本地化与适应。

Method: 该研究由翻译专家团队将WinoGrande测试集翻译并适应为爱沙尼亚语，设计人工翻译和机器翻译两种版本，并通过提示工程优化机器翻译流程。分别用多种专有和开源模型在不同数据集上测试表现。

Result: 在人工翻译的爱沙尼亚语测试集上，模型表现略低于原始英语集；而在机器翻译集上，模型表现明显更差。通过提示工程改进机器翻译，提升受限。

Conclusion: 数据集高质量跨语言翻译需语言专家参与，单靠提示工程和自动化手段难以保证准确率和可解释性。人工本地化翻译对评测大模型在不同语言下的能力至关重要。

Abstract: In this paper, we present a localized and culturally adapted Estonian translation of the test set from the widely used commonsense reasoning benchmark, WinoGrande. We detail the translation and adaptation process carried out by translation specialists and evaluate the performance of both proprietary and open source models on the human translated benchmark. Additionally, we explore the feasibility of achieving high-quality machine translation by incorporating insights from the manual translation process into the design of a detailed prompt. This prompt is specifically tailored to address both the linguistic characteristics of Estonian and the unique translation challenges posed by the WinoGrande dataset. Our findings show that model performance on the human translated Estonian dataset is slightly lower than on the original English test set, while performance on machine-translated data is notably worse. Additionally, our experiments indicate that prompt engineering offers limited improvement in translation quality or model accuracy, and highlight the importance of involving language specialists in dataset translation and adaptation to ensure reliable and interpretable evaluations of language competency and reasoning in large language models.

</details>


### [151] [Large Language Models for Sentiment Analysis to Detect Social Challenges: A Use Case with South African Languages](https://arxiv.org/abs/2511.17301)
*Koena Ronny Mabokela,Tim Schlippe,Matthias Wölfel*

Main category: cs.CL

TL;DR: 本论文评估多种大语言模型（LLM）在分析南非多语言社交媒体情感、发现社会问题上的零样本能力，并提出融合不同模型结果可以极大提高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 南非为多语言社区，社交媒体发帖中蕴含社会问题和情感信息。此前尚无利用LLM分析南非本地语言社交文本并发现社会挑战的研究。

Method: 评估GPT-3.5、GPT-4、LlaMa 2、PaLM 2、Dolly 2等LLM对南非10个政府部门领域内、以英语、Sepedi和Setswana发布的10个热点主题社交媒体帖子的情感极性零样本分析性能，并考察多模型融合方法。

Result: 不同LLM在不同主题和语言上的情感分类性能差异较大。多模型结果融合显著提升情感分类准确率，错误率降至1%以下。

Conclusion: 通过多LLM融合能够可靠地实现多语言社交媒体文本情感分析，为发现社会挑战、引导政府针对性行动提供可行支撑。

Abstract: Sentiment analysis can aid in understanding people's opinions and emotions on social issues. In multilingual communities sentiment analysis systems can be used to quickly identify social challenges in social media posts, enabling government departments to detect and address these issues more precisely and effectively. Recently, large-language models (LLMs) have become available to the wide public and initial analyses have shown that they exhibit magnificent zero-shot sentiment analysis abilities in English. However, there is no work that has investigated to leverage LLMs for sentiment analysis on social media posts in South African languages and detect social challenges. Consequently, in this work, we analyse the zero-shot performance of the state-of-the-art LLMs GPT-3.5, GPT-4, LlaMa 2, PaLM 2, and Dolly 2 to investigate the sentiment polarities of the 10 most emerging topics in English, Sepedi and Setswana social media posts that fall within the jurisdictional areas of 10 South African government departments. Our results demonstrate that there are big differences between the various LLMs, topics, and languages. In addition, we show that a fusion of the outcomes of different LLMs provides large gains in sentiment classification performance with sentiment classification errors below 1%. Consequently, it is now feasible to provide systems that generate reliable information about sentiment analysis to detect social challenges and draw conclusions about possible needs for actions on specific topics and within different language groups.

</details>


### [152] [Humanlike Multi-user Agent (HUMA): Designing a Deceptively Human AI Facilitator for Group Chats](https://arxiv.org/abs/2511.17315)
*Mateusz Jacniacki,Martí Carmona Serrat*

Main category: cs.CL

TL;DR: 本文提出了针对多人异步对话的新型AI助手HUMA系统，通过人类化的交流策略和响应速度，使其在群聊中与人类管理者难以区分。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的对话系统主要针对一对一的轮流对话，难以满足日益普及的AI在群聊、客服等多用户场景的需求。为提升用户信任与交互体验，需开发更自然和人性化的群聊AI。

Method: 设计了HUMA系统，包括Router、Action Agent与Reflection三个模块，采用事件驱动架构处理消息、回复、表情等，并模拟真实的响应时间。通过角色扮演实验，比较AI与人类作为群组管理者的表现。

Result: 在97名参与者的四人小组群聊实验中，AI与人类管理者在人类判别测试中的分辨率接近随机，用户对社区管理效能、社交存在和满意度的主观评价也与人类条件差别不大。

Conclusion: HUMA系统能够在自然的群聊场景下实现接近人类管理者的表现，用户难以辨识其非人类身份，显示其在群聊AI应用中的巨大潜力。

Abstract: Conversational agents built on large language models (LLMs) are becoming increasingly prevalent, yet most systems are designed for one-on-one, turn-based exchanges rather than natural, asynchronous group chats. As AI assistants become widespread throughout digital platforms, from virtual assistants to customer service, developing natural and humanlike interaction patterns seems crucial for maintaining user trust and engagement. We present the Humanlike Multi-user Agent (HUMA), an LLM-based facilitator that participates in multi-party conversations using human-like strategies and timing. HUMA extends prior multi-user chatbot work with an event-driven architecture that handles messages, replies, reactions and introduces realistic response-time simulation. HUMA comprises three components-Router, Action Agent, and Reflection-which together adapt LLMs to group conversation dynamics.
  We evaluate HUMA in a controlled study with 97 participants in four-person role-play chats, comparing AI and human community managers (CMs). Participants classified CMs as human at near-chance rates in both conditions, indicating they could not reliably distinguish HUMA agents from humans. Subjective experience was comparable across conditions: community-manager effectiveness, social presence, and engagement/satisfaction differed only modestly with small effect sizes. Our results suggest that, in natural group chat settings, an AI facilitator can match human quality while remaining difficult to identify as nonhuman.

</details>


### [153] [A new kid on the block: Distributional semantics predicts the word-specific tone signatures of monosyllabic words in conversational Taiwan Mandarin](https://arxiv.org/abs/2511.17337)
*Xiaoyun Jin,Mirjam Ernestus,R. Harald Baayen*

Main category: cs.CL

TL;DR: 本文通过大规模语料库，分析了汉语单音节词的音高轮廓，发现语义信息对音高实现有显著影响。


<details>
  <summary>Details</summary>
Motivation: 虽然传统理论认为普通话的音高轮廓由音系因素主导，实际口语中除了音系变量外，是否还有其他信息影响音高实现尚不清楚，尤其是词义（语义）因素的作用未被充分探索。

Method: 研究使用了语料库数据，运用广义加性模型（GAM）将观察到的音高轮廓分解到与多个变量和语义预测变量相关的成分上。在控制了音长、性别、说话人、声调环境、元音高度和语句位置等变量后，进一步考察词义变量对音高的影响。此外，通过词的上下文化嵌入（contextualized embeddings）预测词的音高实现，并与基线做对比。

Result: 词的语义（词义）变量比具体词本身更能预测音高轮廓；形异同音词（heterographic homophones）有不同的音高表现。用上下文词嵌入预测给定词音高准确率显著高于基准，说明音高实现深受语义影响。

Conclusion: 研究结果挑战了普通话音调标准理论，表明语义对音高实现有显著作用。分布式语义模型在语音学领域显示出应用前景，与辨别词汇模型（Discriminative Lexicon Model）理论框架相符。

Abstract: We present a corpus-based investigation of how the pitch contours of monosyllabic words are realized in spontaneous conversational Mandarin, focusing on the effects of words' meanings. We used the generalized additive model to decompose a given observed pitch contour into a set of component pitch contours that are tied to different control variables and semantic predictors. Even when variables such as word duration, gender, speaker identity, tonal context, vowel height, and utterance position are controlled for, the effect of word remains a strong predictor of tonal realization. We present evidence that this effect of word is a semantic effect: word sense is shown to be a better predictor than word, and heterographic homophones are shown to have different pitch contours. The strongest evidence for the importance of semantics is that the pitch contours of individual word tokens can be predicted from their contextualized embeddings with an accuracy that substantially exceeds a permutation baseline. For phonetics, distributional semantics is a new kid on the block. Although our findings challenge standard theories of Mandarin tone, they fit well within the theoretical framework of the Discriminative Lexicon Model.

</details>


### [154] [Don't Learn, Ground: A Case for Natural Language Inference with Visual Grounding](https://arxiv.org/abs/2511.17358)
*Daniil Ignatev,Ayman Santeer,Albert Gatt,Denis Paperno*

Main category: cs.CL

TL;DR: 本文提出了一种利用多模态（语言+视觉）表示进行零样本自然语言推断（NLI）的方法，即通过文本生成图像并进行推断，从而提升准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有NLI模型常对文本偏见和表层特征敏感，缺乏对语义的深层理解。为此，作者希望探索用视觉信息辅助NLI任务，以增强模型对语义的把握和鲁棒性。

Method: 该方法先利用文本到图像生成模型，将前提转化为视觉表示，然后将该视觉表示与文本假设进行对比推断。具体评估了两种推断方式：余弦相似度和视觉问答。同时作者设计了对抗性数据集用以测试方法鲁棒性。

Result: 所提方法在无需特定任务微调的情况下取得了较高准确率，能有效抵抗文本偏见和表层启发式规则的影响。

Conclusion: 把视觉模态作为语义表示来辅助自然语言推断为实现更鲁棒的自然语言理解提供了一条有前景的思路。

Abstract: We propose a zero-shot method for Natural Language Inference (NLI) that leverages multimodal representations by grounding language in visual contexts. Our approach generates visual representations of premises using text-to-image models and performs inference by comparing these representations with textual hypotheses. We evaluate two inference techniques: cosine similarity and visual question answering. Our method achieves high accuracy without task-specific fine-tuning, demonstrating robustness against textual biases and surface heuristics. Additionally, we design a controlled adversarial dataset to validate the robustness of our approach. Our findings suggest that leveraging visual modality as a meaning representation provides a promising direction for robust natural language understanding.

</details>


### [155] [Selective Rotary Position Embedding](https://arxiv.org/abs/2511.17388)
*Sajad Movahedi,Timur Carstensen,Arshia Afzal,Frank Hutter,Antonio Orvieto,Volkan Cevher*

Main category: cs.CL

TL;DR: 本文提出了一种输入依赖型的旋转位置编码（Selective RoPE），能够在softmax和线性transformer中以任意角度进行位置编码，并验证其提升了模型的序列建模与记忆任务能力。


<details>
  <summary>Details</summary>
Motivation: 传统的位置编码如RoPE使用固定角度转动，未能充分利用输入相关信息。线性transformer中通过选择性门控改善了任务表现，启发作者探索能否在旋转位置编码中也引入选择性，即通过输入决定旋转角度，以更灵活地编码顺序和遗忘信息。

Method: 作者提出了Selective RoPE，这是一种输入依赖式的旋转位置编码机制，能够对softmax和线性transformer中的查询-键对施加可变旋转角度。此外，还分析了softmax注意力本身潜在的旋转性质，以及门控模型中实部和虚部分别管理遗忘与位置编码的作用。

Result: 将Selective RoPE加入门控transformer后，在语言建模及复杂序列任务（如复制、状态跟踪与检索）中显著提升了模型性能。

Conclusion: 输入相关的旋转位置编码相比传统固定角度RoPE更优，能进一步提升transformer在序列建模中的表现。

Abstract: Position information is essential for language modeling. In softmax transformers, Rotary Position Embeddings (\textit{RoPE}) encode positions through \textit{fixed-angle} rotations, while in linear transformers, order is handled via input-dependent (selective) gating that decays past key-value associations. Selectivity has generally been shown to improve language-related tasks. Inspired by this, we introduce \textit{Selective RoPE}, an \textit{input-dependent} rotary embedding mechanism, that generalizes \textit{RoPE}, and enables rotation in \textit{arbitrary angles} for both linear and softmax transformers. We show that softmax attention already performs a hidden form of these rotations on query-key pairs, uncovering an implicit positional structure. We further show that in state-space models and gated linear transformers, the real part manages forgetting while the imaginary part encodes positions through rotations. We validate our method by equipping gated transformers with \textit{Selective RoPE}, demonstrating that its input-dependent rotations improve performance in language modeling and on difficult sequence tasks like copying, state tracking, and retrieval.

</details>


### [156] [PUCP-Metrix: A Comprehensive Open-Source Repository of Linguistic Metrics for Spanish](https://arxiv.org/abs/2511.17402)
*Javier Alonso Villegas Luis,Marco Antonio Sobrevilla Cabezudo*

Main category: cs.CL

TL;DR: 提出了PUCP-Metrix，这是一个为西班牙语设计、涵盖182项语言特征的开源指标库，用于文本分析和各种NLP任务。


<details>
  <summary>Details</summary>
Motivation: 现有的西班牙语语言特征分析工具覆盖有限，难以满足风格、结构、可读性等解释性需求，因此需要更丰富、易用的工具支持多样化应用。

Method: 开发了PUCP-Metrix，收集并实现了182项与词汇多样性、句法和语义复杂性、衔接、心理语言学和可读性相关的指标；并通过自动可读性评估与机器生成文本检测两个任务，在西班牙语数据上验证其实用性。

Result: PUCP-Metrix在自动可读性评估和机器生成文本检测任务中，表现与既有工具库与强大的神经网络基线相当，有竞争力。

Conclusion: PUCP-Metrix为西班牙语NLP应用提供了全面、可扩展的解释型特征资源，能够支持风格化、结构化、可读性等多种分析任务。

Abstract: Linguistic features remain essential for interpretability and tasks involving style, structure, and readability, but existing Spanish tools offer limited coverage. We present PUCP-Metrix, an open-source repository of 182 linguistic metrics spanning lexical diversity, syntactic and semantic complexity, cohesion, psycholinguistics, and readability. PUCP-Metrix enables fine-grained, interpretable text analysis. We evaluate its usefulness on Automated Readability Assessment and Machine-Generated Text Detection, showing competitive performance compared to an existing repository and strong neural baselines. PUCP-Metrix offers a comprehensive, extensible resource for Spanish, supporting diverse NLP applications.

</details>


### [157] [Beyond Multiple Choice: A Hybrid Framework for Unifying Robust Evaluation and Verifiable Reasoning Training](https://arxiv.org/abs/2511.17405)
*Yesheng Liu,Hao Li,Haiyu Xu,Baoqi Pei,Jiahao Wang,Mingxuan Zhao,Jingshu Zheng,Zheqi He,JG Yao,Bowen Qin,Xi Yang,Jiajun Zhang*

Main category: cs.CL

TL;DR: 本文提出ReVeL框架，将选择题转换为可验证的开放式题目，用于多模态大模型评测和强化微调，并有效提升数据利用和评测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 选择题格式方便自动化评测，但选项中可能包含模型可利用的信号，导致准确率不能真实反映模型能力，甚至在强化微调时诱发猜测行为。

Method: 提出ReVeL（Rewrite and Verify by LLM）框架，将选择题重写为开放式问答，并根据不同答案类型采用专门的重写和验证方案。基于该框架，将2万个选择题转换为开放式题目，对Qwen2.5-VL模型进行强化微调（GRPO）。

Result: 在多个选择题基准上，ReVeL-OpenQA训练的模型能达到与传统选择题相当的准确率，并在开放式问答上提升约6个百分点。ReVeL还能揭示选择题评测中高达20个百分点的虚高分数，提升判别准确率，并降低评测成本和延迟。

Conclusion: ReVeL框架能提供更有效和鲁棒的数据及激励信号，改善开放式问答真实能力评测；未来将公开代码与数据。

Abstract: Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.

</details>


### [158] [SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation](https://arxiv.org/abs/2511.17432)
*Shrikant Kendre,Austin Xu,Honglu Zhou,Michael Ryoo,Shafiq Joty,Juan Carlos Niebles*

Main category: cs.CL

TL;DR: 本论文提出了一种新的评测方法SMILE，兼顾词汇精确性和语义相关性，提高了文本和视觉问答任务的评测准确性，对比主流指标表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有文本与视觉问答评测指标如ROUGE、METEOR等多以n-gram词汇重合为主，难以抓住深层语义。BERTScore等虽然引入语境嵌入，但难兼顾句子级与关键词级的语义分析，且忽略词汇相似性。基于大模型的评测尽管能力强，但成本高且存在偏差、幻觉等问题。

Method: 作者提出SMILE方法，融合句子级、关键词级语义理解和关键词匹配，将词汇精准与语义相关性结合，是一种综合性评价体系。

Result: 在文本、图像和视频问答等广泛基准下，SMILE与人工评测结果高度相关，且计算资源消耗低，优于现有主流指标。

Conclusion: SMILE方法有效平衡了词汇和语义评测需求，在多模态问答场景下具备更高应用价值，推动了评测方法的进步。

Abstract: Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation.

</details>


### [159] [Masked-and-Reordered Self-Supervision for Reinforcement Learning from Verifiable Rewards](https://arxiv.org/abs/2511.17473)
*Zhen Wang,Zhifeng Gao,Guolin Ke*

Main category: cs.CL

TL;DR: 本文提出了一种针对数学推理任务的新方法MR-RLVR，通过引入过程级的自监督奖励信号，在仅能验证最终结果的场景下提升了大型语言模型（LLM）在数学推理上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在推理中间过程重要、最终结果难以直接验证的数学领域（如定理证明）上表现有限，而传统token级的监督微调易陷入死记硬背，难以促进多步推理能力的提升。因此，需设计更好地利用中间推理信息的训练方法。

Method: 受到BERT自监督任务的启发，提出MR-RLVR。该方法通过“mask-then-fill”（遮蔽-填空）和“步骤重排序”操作，为中间推理过程构建自监督奖励信号。训练分两阶段，先在采样到的数学计算与证明数据上做自监督训练，再在只可验证结果的数学计算数据上做RLVR微调。方法在Qwen2.5-3B和DeepSeek-R1-Distill-Qwen-1.5B模型上实现，并在AIME24、AIME25、AMC23、MATH500多个数据集上进行测试。

Result: 在固定采样和解码预算下，MR-RLVR比原RLVR取得了平均+9.86%（Pass@1）、+5.27%（Pass@5）、+4.00%（Pass@8）的相对提升。

Conclusion: 通过引入过程感知的自监督信号，显著提升了RLVR方法在仅终局可验证场合的扩展性和性能，证明了该策略的有效性。

Abstract: Test-time scaling has been shown to substantially improve large language models' (LLMs) mathematical reasoning. However, for a large portion of mathematical corpora, especially theorem proving, RLVR's scalability is limited: intermediate reasoning is crucial, while final answers are difficult to directly and reliably verify. Meanwhile, token-level SFT often degenerates into rote memorization rather than inducing longer chains of thought. Inspired by BERT's self-supervised tasks, we propose MR-RLVR (Masked-and-Reordered RLVR), which constructs process-level self-supervised rewards via "masked-then-fill" and "step reordering" to extract learnable signals from intermediate reasoning. Our training pipeline comprises two stages: we first perform self-supervised training on sampled mathematical calculation and proof data; we then conduct RLVR fine-tuning on mathematical calculation datasets where only outcomes are verifiable. We implement MR-RLVR on Qwen2.5-3B and DeepSeek-R1-Distill-Qwen-1.5B, and evaluate on AIME24, AIME25, AMC23, and MATH500. Under a fixed sampling and decoding budget, MR-RLVR achieves average relative gains over the original RLVR of +9.86% Pass@1, +5.27% Pass@5, and +4.00% Pass@8. These results indicate that incorporating process-aware self-supervised signals can effectively enhance RLVR's scalability and performance in only outcome-verifiable settings.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [160] [A*-based Temporal Logic Path Planning with User Preferences on Relaxed Task Satisfaction](https://arxiv.org/abs/2511.16844)
*Disha Kamale,Xi Yu,Cristian-Ioan Vasile*

Main category: cs.RO

TL;DR: 本文提出了一种新的A*搜索框架，用于大规模机器人环境中的时序逻辑任务规划，并将用户偏好集成到任务松弛中，从而在无法完全满足任务时获得最佳可能结果。


<details>
  <summary>Details</summary>
Motivation: 在实际机器人任务中，由于环境复杂或约束限制，时序逻辑任务常常无法完全符合要求，因此需要在无法完全满足任务时，通过引入用户对任务松弛的偏好，寻求最佳的任务达成方案。

Method: 利用自动机表达时序逻辑目标和用户偏好，结合A*搜索算法设计高效启发式方法，加速在大规模环境中的轨迹规划，并显著降低计算资源消耗。

Result: 提出的启发式A*方法在多个大规模场景下进行了实验，结果显示规划速度提升，内存需求降低，同时对轨迹次优程度进行了经验分析和上界评估。

Conclusion: 该方法能够有效应对大规模机器人环境下复杂任务规划问题，兼顾任务完成度与用户偏好，并具有良好的可扩展性和实际应用潜力。

Abstract: In this work, we consider the problem of planning for temporal logic tasks in large robot environments. When full task compliance is unattainable, we aim to achieve the best possible task satisfaction by integrating user preferences for relaxation into the planning process. Utilizing the automata-based representations for temporal logic goals and user preferences, we propose an A*-based planning framework. This approach effectively tackles large-scale problems while generating near-optimal high-level trajectories. To facilitate this, we propose a simple, efficient heuristic that allows for planning over large robot environments in a fraction of time and search memory as compared to uninformed search algorithms. We present extensive case studies to demonstrate the scalability, runtime analysis as well as empirical bounds on the suboptimality of the proposed heuristic.

</details>


### [161] [Single-Pixel Tactile Skin via Compressive Sampling](https://arxiv.org/abs/2511.16898)
*Ariel Slepyan,Laura Xing,Rudy Zhang,Nitish Thakor*

Main category: cs.RO

TL;DR: 本文提出了单像素触觉皮肤（SPTS）体系，通过单通道输出并采用压缩感知技术，实现大面积快速采集与重建丰富触觉信息，极大简化了接线并减少数据量，适用于机器人和人机交互等领域。


<details>
  <summary>Details</summary>
Motivation: 目前用于机器人、假肢和人机界面的大面积高速电子皮肤受限于复杂的布线和数据瓶颈，亟需新的技术突破以实现更高效、更大规模的触觉感知。

Method: 作者提出了一种基于压缩感知的单像素触觉皮肤方案。每个传感单元配备微控制器，能动态地贡献加权模拟信号，从而在硬件级别实现分布式压缩采样。该结构可柔性串联接入，仅需少量输入线和一个输出，大幅减少传统逐点扫描方式的数据需求。系统还支持自适应重建，能根据测量时间动态调整分辨率。

Result: 该系统在物体分类实验中达到等效3500帧/秒，能以8毫秒23帧的精度捕捉短暂动态事件。利用仅7%的测量数据可快速定位，有效数据量可递进优化至高保真成像。

Conclusion: SPTS为机器人及人机界面等领域提供一种高效、低复杂度实现大规模触觉智能的新路径，兼具响应速度快及重构质量可调等优点，突破了传统设计的瓶颈。

Abstract: Development of large-area, high-speed electronic skins is a grand challenge for robotics, prosthetics, and human-machine interfaces, but is fundamentally limited by wiring complexity and data bottlenecks. Here, we introduce Single-Pixel Tactile Skin (SPTS), a paradigm that uses compressive sampling to reconstruct rich tactile information from an entire sensor array via a single output channel. This is achieved through a direct circuit-level implementation where each sensing element, equipped with a miniature microcontroller, contributes a dynamically weighted analog signal to a global sum, performing distributed compressed sensing in hardware. Our flexible, daisy-chainable design simplifies wiring to a few input lines and one output, and significantly reduces measurement requirements compared to raster scanning methods. We demonstrate the system's performance by achieving object classification at an effective 3500 FPS and by capturing transient dynamics, resolving an 8 ms projectile impact into 23 frames. A key feature is the support for adaptive reconstruction, where sensing fidelity scales with measurement time. This allows for rapid contact localization using as little as 7% of total data, followed by progressive refinement to a high-fidelity image - a capability critical for responsive robotic systems. This work offers an efficient pathway towards large-scale tactile intelligence for robotics and human-machine interfaces.

</details>


### [162] [Multi-UAV Swarm Obstacle Avoidance Based on Potential Field Optimization](https://arxiv.org/abs/2511.16911)
*Yendo Hu,Yiliang Wu,Weican Chen*

Main category: cs.RO

TL;DR: 本文通过提出一种混合算法提升多无人机(多UAV)在静态未知环境中的路径规划与避障表现，相较传统方法有效优化路径并提升队形稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统人工势场(APF)算法在多无人机协同避障时，常出现冗余路径、航向突变和机体碰撞等问题，因此亟需优化路径规划以提升系统安全与性能。

Method: 该研究提出融合改进的多机器人编队避障算法(MRF IAPF)与优化后的单无人机APF算法。其核心方法包括：(1) 综合障碍物斥力、无人机间相互作用力与目标引力三种力；(2) 引入碰撞风险评估与辅助子目标机制，当遇高碰撞风险时动态生成新航点，辅助无人机安全避障并保证精确抵达终点。

Result: 仿真结果显示，与传统APF编队方法相比，所提算法在路径长度优化及航向稳定性方面有显著提升，能有效避障并迅速恢复队形。

Conclusion: 所提混合算法适用于未知静态障碍环境下的多无人机协同路径规划，验证了其实用性与有效性。

Abstract: In multi UAV scenarios,the traditional Artificial Potential Field (APF) method often leads to redundant flight paths and frequent abrupt heading changes due to unreasonable obstacle avoidance path planning,and is highly prone to inter UAV collisions during the obstacle avoidance process.To address these issues,this study proposes a novel hybrid algorithm that combines the improved Multi-Robot Formation Obstacle Avoidance (MRF IAPF) algorithm with an enhanced APF optimized for single UAV path planning.Its core ideas are as follows:first,integrating three types of interaction forces from MRF IAPF obstacle repulsion force,inter UAV interaction force,and target attraction force;second,incorporating a refined single UAV path optimization mechanism,including collision risk assessment and an auxiliary sub goal strategy.When a UAV faces a high collision threat,temporary waypoints are generated to guide obstacle avoidance,ensuring eventual precise arrival at the actual target.Simulation results demonstrate that compared with traditional APF based formation algorithms,the proposed algorithm achieves significant improvements in path length optimization and heading stability,can effectively avoid obstacles and quickly restore the formation configuration,thus verifying its applicability and effectiveness in static environments with unknown obstacles.

</details>


### [163] [MobileOcc: A Human-Aware Semantic Occupancy Dataset for Mobile Robots](https://arxiv.org/abs/2511.16949)
*Junseo Kim,Guido Dumont,Xinyu Gao,Gang Chen,Holger Caesar,Javier Alonso-Mora*

Main category: cs.RO

TL;DR: 本文提出了MobileOcc语义占据数据集，专为行人密集环境下的移动机器人3D感知设计，并提供了基准和评测方法。


<details>
  <summary>Details</summary>
Motivation: 尽管语义占据感知在自动驾驶中已有应用，但在移动机器人应对人群环境中相对欠缺，因此需要适应此场景的数据集和方法。

Method: 作者构建了一个新型标注流程，将静态物体标注与为人类占据建模专门设计的网格优化框架结合。该框架通过2D图像重建可变形人类几何结构，并用相关LiDAR点云数据进行精细优化。基于此数据集，设立了占据预测和行人速度预测两项任务基准，并提供单目、双目、融合方法的对比实验证明。

Result: 通过对移动机器人场景和3D人体姿态估计数据集的评估，作者的方法在不同数据集上都表现出鲁棒的性能。

Conclusion: 本文提出的数据集和标注方法有效提升了移动机器人在人群环境中的语义感知能力，为相关任务提供了标准基准，有助于推动该领域发展。

Abstract: Dense 3D semantic occupancy perception is critical for mobile robots operating in pedestrian-rich environments, yet it remains underexplored compared to its application in autonomous driving. To address this gap, we present MobileOcc, a semantic occupancy dataset for mobile robots operating in crowded human environments. Our dataset is built using an annotation pipeline that incorporates static object occupancy annotations and a novel mesh optimization framework explicitly designed for human occupancy modeling. It reconstructs deformable human geometry from 2D images and subsequently refines and optimizes it using associated LiDAR point data. Using MobileOcc, we establish benchmarks for two tasks, i) Occupancy prediction and ii) Pedestrian velocity prediction, using different methods including monocular, stereo, and panoptic occupancy, with metrics and baseline implementations for reproducible comparison. Beyond occupancy prediction, we further assess our annotation method on 3D human pose estimation datasets. Results demonstrate that our method exhibits robust performance across different datasets.

</details>


### [164] [Stable Offline Hand-Eye Calibration for any Robot with Just One Mark](https://arxiv.org/abs/2511.17001)
*Sicheng Xie,Lingchen Meng,Zhiying Du,Shuyuan Tu,Haidong Cao,Jiaqi Leng,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.RO

TL;DR: CalibAll提出了一种只需单一标记、无需训练的稳定精准相机外参估计方法，适用于多种机器人和数据集，性能优于现有方法，并可生成多种辅助标注。


<details>
  <summary>Details</summary>
Motivation: 模仿学习任务中，利用相机外参信息可以显著提升性能，但相机外参通常难以获取，且现有估计算法存在局部极小值和泛化性不足的问题。因此需求一种通用、简便且高效的估计方法。

Method: CalibAll方法在末端执行器上加单一标记，结合视觉基础模型自动匹配多机器人数据中的相应标记。利用点追踪和三维轨迹，初步通过时序PnP获得外参，再用渲染优化精细调整遮罩，实现高度精准外参估计。

Result: 实验证明，CalibAll在三个机器人平台上，外参估计精度和稳定性均超过现有最优方法，且具有良好泛化性。

Conclusion: CalibAll无需复杂标定流程，且对机器人和数据集适应性强，能生成辅助标注，为下游任务提供额外支持，是泛用性和实用性极强的相机外参估计新方案。

Abstract: Imitation learning has achieved remarkable success in a variety of robotic tasks by learning a mapping function from camera-space observations to robot-space actions. Recent work indicates that the use of robot-to-camera transformation information ({\ie}, camera extrinsics) benefits the learning process and produces better results. However, camera extrinsics are oftentimes unavailable and estimation methods usually suffer from local minima and poor generalizations. In this paper, we present CalibAll, a simple yet effective method that \textbf{requires only a single mark} and performs training-free, stable, and accurate camera extrinsic estimation across diverse robots and datasets through a coarse-to-fine calibration pipeline. In particular, we annotate a single mark on an end-effector (EEF), and leverage the correspondence ability emerged from vision foundation models (VFM) to automatically localize the corresponding mark across robots in diverse datasets. Using this mark, together with point tracking and the 3D EEF trajectory, we obtain a coarse camera extrinsic via temporal Perspective-n-Point (PnP). This estimate is further refined through a rendering-based optimization that aligns rendered and ground-true masks, yielding accurate and stable camera extrinsic. Experimental results demonstrate that our method outperforms state-of-the-art approaches, showing strong robustness and general effectiveness across three robot platforms. It also produces useful auxiliary annotations such as depth maps, link-wise masks, and end-effector 2D trajectories, which can further support downstream tasks.

</details>


### [165] [MfNeuPAN: Proactive End-to-End Navigation in Dynamic Environments via Direct Multi-Frame Point Constraints](https://arxiv.org/abs/2511.17013)
*Yiwen Ying,Hanjing Ye,Senzi Luo,Luyao Liu,Yu Zhan,Li He,Hong Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种利用多帧点约束和运动预测模块的机器人主动避障新方法，显著提升了复杂动态环境下的导航表现。


<details>
  <summary>Details</summary>
Motivation: 传统模型和学习方法在动态环境下适应性差，主要因为依赖静态假设或单帧观测，无法有效应对障碍物的实时移动。

Method: 提出引入能够预测未来障碍物路径的模块，结合当前及未来多帧观测信息建立点约束，使机器人具备主动预测并绕避障碍物的能力，实现端到端路线规划。

Result: 该方法大幅提升了机器人在未知、动态环境中的导航鲁棒性和效率，经仿真与真实世界实验验证有效。

Conclusion: 利用多帧观测与路径预测，机器人可主动应对动态障碍，显著优于仅依赖静态估计或单帧观测的方法。

Abstract: Obstacle avoidance in complex and dynamic environments is a critical challenge for real-time robot navigation. Model-based and learning-based methods often fail in highly dynamic scenarios because traditional methods assume a static environment and cannot adapt to real-time changes, while learning-based methods rely on single-frame observations for motion constraint estimation, limiting their adaptability. To overcome these limitations, this paper proposes a novel framework that leverages multi-frame point constraints, including current and future frames predicted by a dedicated module, to enable proactive end-to-end navigation. By incorporating a prediction module that forecasts the future path of moving obstacles based on multi-frame observations, our method allows the robot to proactively anticipate and avoid potential dangers. This proactive planning capability significantly enhances navigation robustness and efficiency in unknown dynamic environments. Simulations and real-world experiments validate the effectiveness of our approach.

</details>


### [166] [H-GAR: A Hierarchical Interaction Framework via Goal-Driven Observation-Action Refinement for Robotic Manipulation](https://arxiv.org/abs/2511.17079)
*Yijie Zhu,Rui Shao,Ziyang Liu,Jie He,Jizhihui Liu,Jiuru Wang,Zitong Yu*

Main category: cs.RO

TL;DR: 该论文提出了一种层次化的机器人操作交互模型H-GAR，通过目标驱动实现更高效准确的视频与动作预测，在仿真与真实环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频与动作预测方法常忽略具体任务目标，导致行为与观察结果语义错配、操作不连贯，因此需要能结合目标语义的预测框架以提升机器人操作的智能性。

Method: 作者提出H-GAR框架，先生成目标观察和粗略动作路线，之后通过两大模块协同细化：（1）目标条件观察生成器根据粗动作与目标生成中间观察结果；（2）交互感知动作细化器利用中间观察与历史动作记忆库，将粗动作细化为与目标一致的精细动作，全流程实现从粗到细的目标驱动动作-观察交互。

Result: 在多项仿真和真实机器人操作任务实验中，H-GAR模型在操作准确性和效果指标上全面超越当前其它主流方法，展示出卓越的任务完成能力和泛化性。

Conclusion: 结合“目标锚定”与动作-观察显式交互，H-GAR从根本上提升了机器人操作预测的语义一致性和连贯性，具备广阔实际应用前景。

Abstract: Unified video and action prediction models hold great potential for robotic manipulation, as future observations offer contextual cues for planning, while actions reveal how interactions shape the environment. However, most existing approaches treat observation and action generation in a monolithic and goal-agnostic manner, often leading to semantically misaligned predictions and incoherent behaviors. To this end, we propose H-GAR, a Hierarchical interaction framework via Goal-driven observation-Action Refinement.To anchor prediction to the task objective, H-GAR first produces a goal observation and a coarse action sketch that outline a high-level route toward the goal. To enable explicit interaction between observation and action under the guidance of the goal observation for more coherent decision-making, we devise two synergistic modules. (1) Goal-Conditioned Observation Synthesizer (GOS) synthesizes intermediate observations based on the coarse-grained actions and the predicted goal observation. (2) Interaction-Aware Action Refiner (IAAR) refines coarse actions into fine-grained, goal-consistent actions by leveraging feedback from the intermediate observations and a Historical Action Memory Bank that encodes prior actions to ensure temporal consistency. By integrating goal grounding with explicit action-observation interaction in a coarse-to-fine manner, H-GAR enables more accurate manipulation. Extensive experiments on both simulation and real-world robotic manipulation tasks demonstrate that H-GAR achieves state-of-the-art performance.

</details>


### [167] [Progress-Think: Semantic Progress Reasoning for Vision-Language Navigation](https://arxiv.org/abs/2511.17097)
*Shuo Wang,Yucheng Wang,Guoxin Lian,Yongcai Wang,Maiyue Chen,Kaihui Wang,Bo Zhang,Zhizhong Su,Yutian Zhou,Wanting Li,Deying Li,Zhaoxin Fan*

Main category: cs.RO

TL;DR: 该论文提出了Progress-Think方法，通过推理语义化进度信息提升视觉-语言导航(VLN)任务表现，并设计了三阶段训练框架，在R2R-CE和RxR-CE数据集上取得了最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言导航方法大多直接预测动作或数值化进度，忽略了观测序列与指令序列单调协同推进的属性，导致导航过程中缺乏对自身导航进度的语义化理解。作者试图解决该问题，使智能体能更好地理解自身在多步指令中的位置，从而实现更连贯、更高效的导航行为。

Method: 作者设计了一个三阶段的训练框架：（1）自对齐进度预训练阶段：通过新颖的可微分对齐机制，将视觉历史与指令前缀对齐，提升进度推理模块能力；（2）进度引导策略预训练：将已学习的进度状态融入导航决策上下文，引导策略学习一致且有效的动作；（3）联合微调：利用特定的进度感知强化学习目标联合优化推理和策略两个模块，提升导航整体表现。

Result: 该方法在R2R-CE和RxR-CE两个主流VLN基准数据集上达到了最优的成功率与效率，实验结果显示所提的语义化进度推理机制能够提高导航过程中的一致性和准确性。

Conclusion: Progress-Think通过将语义化进度推理纳入视觉-语言导航流程，在无需昂贵标注的前提下提升了智能体的导航能力和一致性，验证了结构化进度建模在复杂导航任务中的有效性。

Abstract: Vision-Language Navigation requires agents to act coherently over long horizons by understanding not only local visual context but also how far they have advanced within a multi-step instruction. However, recent Vision-Language-Action models focus on direct action prediction and earlier progress methods predict numeric achievements; both overlook the monotonic co-progression property of the observation and instruction sequences. Building on this insight, Progress-Think introduces semantic progress reasoning, predicting instruction-style progress from visual observations to enable more accurate navigation. To achieve this without expensive annotations, we propose a three-stage framework. In the initial stage, Self-Aligned Progress Pretraining bootstraps a reasoning module via a novel differentiable alignment between visual history and instruction prefixes. Then, Progress-Guided Policy Pretraining injects learned progress states into the navigation context, guiding the policy toward consistent actions. Finally, Progress-Policy Co-Finetuning jointly optimizes both modules with tailored progress-aware reinforcement objectives. Experiments on R2R-CE and RxR-CE show state-of-the-art success and efficiency, demonstrating that semantic progress yields a more consistent representation of navigation advancement.

</details>


### [168] [Reflection-Based Relative Localization for Cooperative UAV Teams Using Active Markers](https://arxiv.org/abs/2511.17166)
*Tim Lakemann,Daniel Bonilla Licea,Viktor Walter,Martin Saska*

Main category: cs.RO

TL;DR: 本文提出了一种充分利用环境中主动标记物反射的新型多机器人相对定位方法，尤其适用于异构微型空中机器人群在未知环境下协同工作。方法无需先验知识或已知标记配置，对动态水面等不平坦表面引入的不确定性进行了建模。实验结果显示，该系统在无团队体积数据的情况下，定位范围和精度均优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 环境中主动标记物的反射容易引发多机器人视觉相对定位的歧义，限制了基于标记的定位系统在复杂、动态及未知环境中的应用。本文动机在于不再简单规避反射造成的干扰，而是将其转化为有用信息，从而提升多机器人系统在多样环境（如水面与陆地）中的定位鲁棒性和泛化能力。

Method: 该方法创新性地采用反射信息进行相对定位，改变了以往避开反射噪音的思路。其核心特点包括：不依赖机器人大小或预定义标记配置，也无需表面材料的先验知识。算法显式建模不平坦（如动态水面）造成的不确定性，使其适应性更强。方法经过室内与室外实验验证。

Result: 实验结果显示，本文方法即使在对机器人尺寸和团队配置没有先验情况下，依然具有优异的鲁棒性。有效定位距离超过30米，且在定位准确性上优于当前主流方法。

Conclusion: 提出的反射辅助相对定位方法有效提升了多机器人在动态、未知环境下的协作能力，尤其适用于异构群体和海洋等具有挑战性的场景。技术突破在于充分利用被以往视为干扰的反射信号，提升定位系统的实用性和拓展性。

Abstract: Reflections of active markers in the environment are a common source of ambiguity in onboard visual relative localization. This work presents a novel approach for onboard relative localization in multi-robot teams that exploits these typically unwanted reflections of active markers in the environment. It operates without prior knowledge of robot size or predefined marker configurations and remains independent of surface properties, an essential feature for heterogeneous micro-aerial swarms cooperating in unknown environments. It explicitly accounts for uncertainties caused by non-flat surfaces, with a particular focus on dynamic water surfaces, which are especially relevant for marine deployments. We validated the approach in both indoor and outdoor experiments, demonstrating that the proposed reflection-based localization system operates reliably without prior knowledge of team member size and achieves greater effective range (above 30 m) and accuracy than state-of-the-art methods. The video and source code of this work will be made publicly available after publication.

</details>


### [169] [Efficient Robot Design with Multi-Objective Black-Box Optimization and Large Language Models](https://arxiv.org/abs/2511.17178)
*Kento Kawaharazuka,Yoshiki Obinata,Naoaki Kanazawa,Haoyu Jia,Kei Okada*

Main category: cs.RO

TL;DR: 本文提出结合大语言模型（LLM）与黑箱优化来提升机器人设计效率，并证明此方法可更高效地探索设计解。


<details>
  <summary>Details</summary>
Motivation: 现有机器人设计优化方法分为数值优化和黑箱优化，但数值优化难以应对复杂结构或离散变量，黑箱优化则采样效率低。本研究旨在解决黑箱优化采样效率低的问题。

Method: 在传统黑箱优化过程中，结合大语言模型进行并行采样，LLM接收问题设定和反馈信息，辅助生成机器人设计方案，从而提升搜索效率。

Result: 实验结果证明，该方法能更高效地发现优质设计解，并展示了其在探索空间中的优越性。

Conclusion: 利用LLM辅助黑箱优化有助于提升机器人结构设计的效率，但同时需关注其局限性。

Abstract: Various methods for robot design optimization have been developed so far. These methods are diverse, ranging from numerical optimization to black-box optimization. While numerical optimization is fast, it is not suitable for cases involving complex structures or discrete values, leading to frequent use of black-box optimization instead. However, black-box optimization suffers from low sampling efficiency and takes considerable sampling iterations to obtain good solutions. In this study, we propose a method to enhance the efficiency of robot body design based on black-box optimization by utilizing large language models (LLMs). In parallel with the sampling process based on black-box optimization, sampling is performed using LLMs, which are provided with problem settings and extensive feedback. We demonstrate that this method enables more efficient exploration of design solutions and discuss its characteristics and limitations.

</details>


### [170] [TP-MDDN: Task-Preferenced Multi-Demand-Driven Navigation with Autonomous Decision-Making](https://arxiv.org/abs/2511.17225)
*Shanshan Li,Da Huang,Yu He,Yanwei Fu,Yu-Gang Jiang,Xiangyang Xue*

Main category: cs.RO

TL;DR: 本文引入了一种新的多需求驱动导航基准TP-MDDN，并提出AWMSystem系统解决长时序复杂任务，实现比当前最优方法更强的感知准确性与导航鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实生活中，导航任务常常涉及多个子需求及个人偏好，但现有方法（如传统的单需求驱动导航）无法覆盖这种复杂性。为更好地模拟真实场景，亟需有能处理多需求与任务偏好的新方法与评测基准。

Method: 提出TP-MDDN基准，要求AI在导航中同时应对多个需求和偏好。为解决此问题，设计了AWMSystem系统，包括：1）BreakLLM模块实现任务分解；2）LocateLLM模块进行目标选择；3）StatusMLLM模块监控任务进展。同时，为提高空间记忆，采用MASMap将3D点云和2D语义地图结合。动作生成方面，提出双节奏框架融合零样本规划和基于策略的精细控制，并引入自适应错误修正器实时处理失败情况。

Result: 实验结果显示，所提方法在感知准确性和导航鲁棒性方面均优于目前最先进的对比方法。

Conclusion: 本文提出的方法有效应对了复杂的多需求导航任务，不仅提升了环境感知与任务执行的可靠性，也为后续智能体导航研究提供了新的方向。

Abstract: In daily life, people often move through spaces to find objects that meet their needs, posing a key challenge in embodied AI. Traditional Demand-Driven Navigation (DDN) handles one need at a time but does not reflect the complexity of real-world tasks involving multiple needs and personal choices. To bridge this gap, we introduce Task-Preferenced Multi-Demand-Driven Navigation (TP-MDDN), a new benchmark for long-horizon navigation involving multiple sub-demands with explicit task preferences. To solve TP-MDDN, we propose AWMSystem, an autonomous decision-making system composed of three key modules: BreakLLM (instruction decomposition), LocateLLM (goal selection), and StatusMLLM (task monitoring). For spatial memory, we design MASMap, which combines 3D point cloud accumulation with 2D semantic mapping for accurate and efficient environmental understanding. Our Dual-Tempo action generation framework integrates zero-shot planning with policy-based fine control, and is further supported by an Adaptive Error Corrector that handles failure cases in real time. Experiments demonstrate that our approach outperforms state-of-the-art baselines in both perception accuracy and navigation robustness.

</details>


### [171] [A ROS2 Interface for Universal Robots Collaborative Manipulators Based on ur_rtde](https://arxiv.org/abs/2511.17237)
*Alessio Saccuti,Riccardo Monica,Jacopo Aleotti*

Main category: cs.RO

TL;DR: 本文提出了一种基于ur_rtde C++库的UR机器人ROS2驱动，支持高层次命令、插件系统扩展，并已开源。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏灵活且易扩展、适用于多种应用场景的UR机器人ROS2驱动，现有方案在可定制性和集成上存在一定局限。

Method: 作者基于ur_rtde C++库，开发了一个ROS2驱动，暴露了URScript的高层命令，并通过插件系统允许用户自定义和扩展命令，支持包括基于路径的运动控制等功能。

Result: 实现了支持多种高层次命令和插件扩展的ROS2驱动。驱动已作为开源项目发布，并已实现多个运动指令示例。

Conclusion: 该ROS2驱动为UR机器人在多样化应用场景中提供了灵活且可扩展的控制接口，有助于推动机器人应用的开发和创新。

Abstract: In this paper a novel ROS2 driver for UR robot manipulators is presented, based on the ur_rtde C++ library. The proposed driver aims to be a flexible solution, adaptable to a wide range of applications. The driver exposes the high-level commands of Universal Robots URScripts, and custom commands can be added using a plugin system. Several commands have been implemented, including motion execution along a waypoint-based path. The driver is published as open source.

</details>


### [172] [Simulation of Active Soft Nets for Capture of Space Debris](https://arxiv.org/abs/2511.17266)
*Leone Costi,Dario Izzo*

Main category: cs.RO

TL;DR: 本文提出了一个基于MuJoCo物理引擎的仿真器，用于设计和控制能自主捕捉太空垃圾的软体机器人网。该仿真器涵盖了网的动力学、网与垃圾的接触、自身接触、轨道力学以及四角卫星推进器控制，能够模拟捕捉大型太空垃圾Envisat的全过程。研究发现，柔性更高的网和滑模控制器能实现更高的捕捉成功率。


<details>
  <summary>Details</summary>
Motivation: 空间站及卫星运行面临大量太空垃圾威胁，现有清理方法难以自主高效地捕获较大残骸。其中，网状机器人是一种有潜力的解决方案，但对其动力学模拟与控制策略的探索有限，现有工作多假设网已弹射至目标并忽略初始静态捕捉问题。

Method: 开发了一个以MuJoCo为基础的仿真平台，实现了真实模拟网动力学、与目标及自身的接触、轨道力学。搭建了包括不同机械模型和控制策略的测试框架，尤其研究了网刚度与控制方式（如滑模控制）对捕捉效果的影响。

Result: 柔性更高的网在捕捉Envisat时表现更佳。使用滑模控制器时，软网能在所有测试用例中成功捕获目标，展现出更大的接触面积和更多的接触点。

Conclusion: 高柔性的软体机器人网配合滑模控制，显著提升了空间垃圾捕获的鲁棒性与效率，为太空自动清理任务提供了有效模拟和应用基础。

Abstract: In this work, we propose a simulator, based on the open-source physics engine MuJoCo, for the design and control of soft robotic nets for the autonomous removal of space debris. The proposed simulator includes net dynamics, contact between the net and the debris, self-contact of the net, orbital mechanics, and a controller that can actuate thrusters on the four satellites at the corners of the net. It showcases the case of capturing Envisat, a large ESA satellite that remains in orbit as space debris following the end of its mission. This work investigates different mechanical models, which can be used to simulate the net dynamics, simulating various degrees of compliance, and different control strategies to achieve the capture of the debris, depending on the relative position of the net and the target. Unlike previous works on this topic, we do not assume that the net has been previously ballistically thrown toward the target, and we start from a relatively static configuration. The results show that a more compliant net achieves higher performance when attempting the capture of Envisat. Moreover, when paired with a sliding mode controller, soft nets are able to achieve successful capture in 100% of the tested cases, whilst also showcasing a higher effective area at contact and a higher number of contact points between net and Envisat.

</details>


### [173] [Leveraging CVAE for Joint Configuration Estimation of Multifingered Grippers from Point Cloud Data](https://arxiv.org/abs/2511.17276)
*Julien Merand,Boris Meden,Mathieu Grossard*

Main category: cs.RO

TL;DR: 本文提出了一种高效方法，仅通过多关节机械手的点云数据，快速推断其关节配置，取得了与现有方法相当的精度，且运行极快。


<details>
  <summary>Details</summary>
Motivation: 现有逆运动学（IK）方法要么需要复杂算法处理所有指骨位置，要么仅据指尖姿态，难以直接获得复杂多指机械手的全部关节配置。因此，提出一种更高效直接的配置推断方法具有重要意义。

Method: 作者采用条件变分自编码器（CVAE）机器学习模型，输入机械手结构要素的点云数据，输出重构的完整关节角度配置。该方法无需复杂的IK求解及中间骨节位置后处理。

Result: 在MultiDex抓取数据集及Allegro Hand机械手上验证，该方法推断时间低于0.05毫秒，准确性与最先进算法相当。

Conclusion: 本方法为AI驱动的抓取规划中的关节配置估算提供了高效、准确的新路径，具备良好应用前景。

Abstract: This paper presents an efficient approach for determining the joint configuration of a multifingered gripper solely from the point cloud data of its poly-articulated chain, as generated by visual sensors, simulations or even generative neural networks. Well-known inverse kinematics (IK) techniques can provide mathematically exact solutions (when they exist) for joint configuration determination based solely on the fingertip pose, but often require post-hoc decision-making by considering the positions of all intermediate phalanges in the gripper's fingers, or rely on algorithms to numerically approximate solutions for more complex kinematics. In contrast, our method leverages machine learning to implicitly overcome these challenges. This is achieved through a Conditional Variational Auto-Encoder (CVAE), which takes point cloud data of key structural elements as input and reconstructs the corresponding joint configurations. We validate our approach on the MultiDex grasping dataset using the Allegro Hand, operating within 0.05 milliseconds and achieving accuracy comparable to state-of-the-art methods. This highlights the effectiveness of our pipeline for joint configuration estimation within the broader context of AI-driven techniques for grasp planning.

</details>


### [174] [MonoSpheres: Large-Scale Monocular SLAM-Based UAV Exploration through Perception-Coupled Mapping and Planning](https://arxiv.org/abs/2511.17299)
*Tomáš Musil,Matěj Petrlík,Martin Saska*

Main category: cs.RO

TL;DR: 本文提出了一种基于单目视觉的机器人自主探索新环境的新方法，能够在没有密集距离传感器的情况下安全覆盖大规模非结构化3D空间。


<details>
  <summary>Details</summary>
Motivation: 当前移动机器人在未知环境中的自主探索主要依赖于昂贵的密集深度传感器，而仅用单目摄像机进行安全可靠探索仍未解决，尤其是在大规模、非结构化且室内外结合的场景。

Method: 方法通过对单目稀疏SLAM的建图与规划环节进行创新：一方面，通过在纹理稀疏区对空闲空间过采样、记录障碍物位置不确定性，提升稀疏深度数据下的地图表达；另一方面，利用快速重规划和感知感知的航向控制应对自由空间的不确定性。此外，扩展了前沿线探索策略，使其适配稀疏单目深度并考虑视差与无纹理表面的情况。

Result: 方法在真实及仿真环境下进行了广泛实验，并包含消融研究；结果显示该方法实现了在非结构化的3D室内外环境下用单目摄像头自主探索。

Conclusion: 据作者所知，本方法首次实现了在真实非结构化室外环境中基于3D单目视觉的探索，并已开源，促进后续研究。

Abstract: Autonomous exploration of unknown environments is a key capability for mobile robots, but it is largely unsolved for robots equipped with only a single monocular camera and no dense range sensors. In this paper, we present a novel approach to monocular vision-based exploration that can safely cover large-scale unstructured indoor and outdoor 3D environments by explicitly accounting for the properties of a sparse monocular SLAM frontend in both mapping and planning. The mapping module solves the problems of sparse depth data, free-space gaps, and large depth uncertainty by oversampling free space in texture-sparse areas and keeping track of obstacle position uncertainty. The planning module handles the added free-space uncertainty through rapid replanning and perception-aware heading control. We further show that frontier-based exploration is possible with sparse monocular depth data when parallax requirements and the possibility of textureless surfaces are taken into account. We evaluate our approach extensively in diverse real-world and simulated environments, including ablation studies. To the best of the authors' knowledge, the proposed method is the first to achieve 3D monocular exploration in real-world unstructured outdoor environments. We open-source our implementation to support future research.

</details>


### [175] [FORWARD: Dataset of a forwarder operating in rough terrain](https://arxiv.org/abs/2511.17318)
*Mikael Lundbäck,Erik Wallin,Carola Häggström,Mattias Nyström,Andreas Grönlund,Mats Richardson,Petrus Jönsson,William Arnvik,Lucas Hedström,Arvid Fälldin,Martin Servin*

Main category: cs.RO

TL;DR: 本论文介绍了FORWARD，一个在瑞典中部森林地带采伐作业中采集的高分辨率、多模态森林搬运机（forwarder）操作数据集。该数据集涵盖车辆多种传感器数据及详细作业标注，对于推进森林机器交通性、感知与自动化研究有重要意义。


<details>
  <summary>Details</summary>
Motivation: 现代森林作业越来越依赖高效、自动化的机械设备。然而，现有公开数据集缺乏高分辨率、覆盖复杂地形和多种作业场景的数据，这限制了基于人工智能的模型和自动控制算法的研发。为了解决这一问题，论文致力于采集、开放和细致标注真实环境下林业搬运机的多模态数据，支持相关领域研究和创新。

Method: 研究团队在瑞典两个采伐地使用装备多种传感器（RTK-GNSS、360相机、振动传感器、CAN-bus、IMU）的Komatsu搬运机，记录了18小时常规作业数据。数据包括高分辨率激光点云、车辆及吊臂作业日志、视频资料和地形数据等。视频资料逐帧标注具体作业环节，并设计道路及地形实验场景，包括不同履带、负载与速度条件的反复测试。

Result: 数据集包含详细传感器数据、StanForD标准的生产日志、标注视频与加密地形数据等，涵盖多种工况与环境。实验收集了高密度森林激光点云、精确定位与动作数据，细致标注了运输、装卸、避障等情景，为交通性、感知与自动化算法研发奠定了基础。

Conclusion: FORWARD数据集填补了多模态高分辨率森林作业数据的空白，是推动林业智能机械交通性、无人驾驶和仿真等研究的有力工具。公开数据有助于推动相关领域新模型、新算法和模拟测试场景的生成和验证，也利于行业和学术界的协作发展。

Abstract: We present FORWARD, a high-resolution multimodal dataset of a cut-to-length forwarder operating in rough terrain on two harvest sites in the middle part of Sweden. The forwarder is a large Komatsu model equipped with a variety of sensors, including RTK-GNSS, 360-camera, operator vibration sensors, internal CAN-bus signal recording, and multiple IMUs. The data includes event time logs recorded in 5 Hz with e.g., driving speed, fuel consumption, vehicle position with centimeter accuracy, and crane use while the vehicle operates in forest areas laser-scanned with very high-resolution, $\sim$1500 points per square meter. Production log files (StanForD standard) with time-stamped machine events, extensive video material, and terrain data in various formats are included as well. About 18 hours of regular wood extraction work during three days is annotated from 360-video material into individual work elements and included in the dataset. We also include scenario specifications of conducted experiments on forest roads and in terrain. Scenarios include repeatedly driving the same routes with and without steel tracks, different load weight, and different target driving speeds. The dataset is intended for developing models and algorithms for trafficability, perception, and autonomous control of forest machines using artificial intelligence, simulation, and experiments on physical testbeds. In part, we focus on forwarders traversing terrain, avoiding obstacles, and loading or unloading logs, with consideration for efficiency, fuel consumption, safety, and environmental impact. Other benefits of the open dataset include the ability to explore auto-generation and calibration of forestry machine simulators and automation scenario descriptions using the data recorded in the field.

</details>


### [176] [Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM](https://arxiv.org/abs/2511.17335)
*Chiori Hori,Yoshiki Masuyama,Siddarth Jain,Radu Corcodel,Devesh Jha,Diego Romeres,Jonathan Le Roux*

Main category: cs.RO

TL;DR: 本文提出了一种结合长时上下文的Q-former与文本条件方法，用于提升多模态场景下人机协作中的动作确认与计划生成。


<details>
  <summary>Details</summary>
Motivation: 当前人-机器人协作需要机器人准确理解人类行为和环境交互，但主流方法仅注重单一片段级的处理，忽略了长时任务中各动作间的相互依赖，影响了动作规划与确认的准确性。

Method: 提出长时上下文Q-former模型，能显式地引入视频完整时序的左右上下文信息；同时，提出文本条件方法，将文本嵌入直接输送至大模型解码端，以补充和提升Q-former对信息抽象能力的不足。并在YouCook2数据集上进行实验。

Result: 实验证明，动作确认生成的准确率直接影响动作计划性能。所提长时上下文Q-former结合VideoLLaMA3，可显著提升动作确认与动作计划的能力。

Conclusion: 融合长时上下文与有效文本条件输入，有助于提升人机对话场景下，机器人对复杂任务的理解、动作确认及计划生成的准确性，对多模态人机协作具有应用前景。

Abstract: Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment. This paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding. The state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former. Experiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning. Furthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.

</details>


### [177] [METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model](https://arxiv.org/abs/2511.17366)
*Yankai Fu,Ning Chen,Junkai Zhao,Shaozhe Shan,Guocai Yao,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 该论文提出了一个针对灵巧操作任务的通用机器人模型METIS，并通过多源数据和新颖表征显著提升了机器人的泛化能力和实际表现。


<details>
  <summary>Details</summary>
Motivation: 目前通用型机器人在面对复杂、多样的灵巧操作任务时表现有限，核心瓶颈在于高质量动作标注数据的稀缺性。人类数据具有丰富的动作先验但与机器人存在视觉和表现形式差异，现有方法难以高效融合利用。

Method: 作者提出了METIS，一个视觉-语言-动作（VLA）模型，通过预训练多源第一视角数据集EgoAtlas（整合多种人类和机器人数据并统一动作空间）。此外，提出了关注运动动态的紧凑离散动作表示，用于更有效地训练模型。

Result: METIS在六项真实世界灵巧操作任务上均取得了最高平均成功率，展现了超强任务完成能力。实验还显示其对分布外场景有更好的泛化和鲁棒性。

Conclusion: METIS作为基于大规模多源数据和高效动作表征的VLA模型，为实现能感知、推理并操作多任务的通用机器人奠定了坚实基础，推动了机器人灵巧操作领域发展。

Abstract: Building a generalist robot that can perceive, reason, and act across diverse tasks remains an open challenge, especially for dexterous manipulation. A major bottleneck lies in the scarcity of large-scale, action-annotated data for dexterous skills, as teleoperation is difficult and costly. Human data, with its vast scale and diverse manipulation behaviors, provides rich priors for learning robotic actions. While prior works have explored leveraging human demonstrations, they are often constrained by limited scenarios and a large visual gap between human and robots. To eliminate these limitations, we propose METIS, a vision-language-action (VLA) model for dexterous manipulation pretrained on multi-source egocentric datasets. We first construct EgoAtlas, which integrates large-scale human and robotic data from multiple sources, all unified under a consistent action space. We further extract motion-aware dynamics, a compact and discretized motion representation, which provides efficient and expressive supervision for VLA training. Built upon them, METIS integrates reasoning and acting into a unified framework, enabling effective deployment to downstream dexterous manipulation tasks. Our method demonstrates exceptional dexterous manipulation capabilities, achieving highest average success rate in six real-world tasks. Experimental results also highlight the superior generalization and robustness to out-of-distribution scenarios. These findings emphasize METIS as a promising step toward a generalist model for dexterous manipulation.

</details>


### [178] [Agility Meets Stability: Versatile Humanoid Control with Heterogeneous Data](https://arxiv.org/abs/2511.17373)
*Yixuan Pan,Ruoyi Qiao,Li Chen,Kashyap Chitta,Liang Pan,Haoguang Mai,Qingwen Bu,Hao Zhao,Cunyuan Zheng,Ping Luo,Hongyang Li*

Main category: cs.RO

TL;DR: 本文提出了AMS（Agility Meets Stability）框架，首次实现在单一策略下统一灵巧动态技能与极限稳定性的控制，适用于人形机器人在多任务环境下的行动。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只能在灵活动态或高度稳定性之间二选一，无法兼顾二者，限制了人形机器人在复杂环境中的应用能力。该研究意图打破这一二元对立，实现多能力统一。

Method: 提出AMS框架，利用含丰富动作的人类动作捕捉数据与物理约束下的合成平衡动作的异构数据来源。设计了混合奖励方案，把普遍的追踪目标应用于所有数据，对合成动作引入专用平衡先验，并采用性能驱动的自适应学习策略与动作特定奖励成形促进不同分布动作的高效训练。

Result: 在仿真与真实的Unitree G1人形机器人上做了大量验证，结果表明AMS可以在单一策略下完成如跳舞、奔跑等灵巧技能，并实现如叶问蹲等极限平衡动作，无需特别迁移训练。

Conclusion: AMS框架为未来多功能人形机器人控制提供了通用、高效的解决方案，实现了灵巧与稳定的统一，具备广泛应用潜力。

Abstract: Humanoid robots are envisioned to perform a wide range of tasks in human-centered environments, requiring controllers that combine agility with robust balance. Recent advances in locomotion and whole-body tracking have enabled impressive progress in either agile dynamic skills or stability-critical behaviors, but existing methods remain specialized, focusing on one capability while compromising the other. In this work, we introduce AMS (Agility Meets Stability), the first framework that unifies both dynamic motion tracking and extreme balance maintenance in a single policy. Our key insight is to leverage heterogeneous data sources: human motion capture datasets that provide rich, agile behaviors, and physically constrained synthetic balance motions that capture stability configurations. To reconcile the divergent optimization goals of agility and stability, we design a hybrid reward scheme that applies general tracking objectives across all data while injecting balance-specific priors only into synthetic motions. Further, an adaptive learning strategy with performance-driven sampling and motion-specific reward shaping enables efficient training across diverse motion distributions. We validate AMS extensively in simulation and on a real Unitree G1 humanoid. Experiments demonstrate that a single policy can execute agile skills such as dancing and running, while also performing zero-shot extreme balance motions like Ip Man's Squat, highlighting AMS as a versatile control paradigm for future humanoid applications.

</details>


### [179] [Vector Cost Behavioral Planning for Autonomous Robotic Systems with Contemporary Validation Strategies](https://arxiv.org/abs/2511.17375)
*Benjamin R. Toaz,Quentin Goss,John Thompson,Seta Boğosyan,Shaunak D. Bopardikar,Mustafa İlhan Akbaş,Metin Gökaşan*

Main category: cs.RO

TL;DR: 本文针对多目标自主机器人系统决策，提出了向量成本双矩阵博弈方法，并与传统加权和方法进行了对比，证实了新方法在高维决策中的优势。


<details>
  <summary>Details</summary>
Motivation: 在多目标任务中，传统的标量加权和法往往会忽视部分目标的最差情况，降低系统整体鲁棒性。作者希望通过新的博弈方法兼顾所有目标，提升机器人决策能力。

Method: 作者将向量成本双矩阵博弈方法推广到多目标情形，与标量加权和方法在竞争性轨迹规划中对比表现，辅以可解释人工智能（XAI）分析决策数据，并利用SEMBAS对参数空间进行敏感性分析和性能探索。此外，将博弈规划与智能系统验证整合进仿真流程。

Result: 仿真结果显示，与传统标量化方法相比，该向量成本方法在高维多目标规划中表现出显著优势，能更好兼顾各目标，并具备更强的结果可解释性。

Conclusion: 该研究为复杂多目标机器人行为规划提供了可解释、泛化性强的方法方案，对多目标自主系统具有广泛应用价值。

Abstract: The vector cost bimatrix game is a method for multi-objective decision making that enables autonomous robotic systems to optimize for multiple goals at once while avoiding worst-case scenarios in neglected objectives. We expand this approach to arbitrary numbers of objectives and compare its performance to scalar weighted sum methods during competitive motion planning. Explainable Artificial Intelligence (XAI) software is used to aid in the analysis of high dimensional decision-making data. State-space Exploration of Multidimensional Boundaries using Adherence Strategies (SEMBAS) is applied to explore performance modes in the parameter space as a sensitivity study for the baseline and proposed frameworks. While some works have explored aspects of game theoretic planning and intelligent systems validation separately, we combine each of these into a novel and comprehensive simulation pipeline. This integration demonstrates a dramatic improvement of the vector cost method over scalarization and offers an interpretable and generalizable framework for robotic behavioral planning. Code available at https://github.com/toazbenj/race_simulation. The video companion to this work is available at https://tinyurl.com/vectorcostvideo.

</details>


### [180] [IndustryNav: Exploring Spatial Reasoning of Embodied Agents in Dynamic Industrial Navigation](https://arxiv.org/abs/2511.17384)
*Yifan Li,Lichi Li,Anh Dao,Xinyu Zhou,Yicheng Qiao,Zheda Mai,Daeun Lee,Zichen Chen,Zhen Tan,Mohit Bansal,Yu Kong*

Main category: cs.RO

TL;DR: 该论文提出了IndustryNav，这是一个专为主动空间推理设计的动态工业导航基准，用于评估VLLMs（视觉大语言模型）在真实、复杂工业环境中的综合表现。


<details>
  <summary>Details</summary>
Motivation: 现有的VLLMs在体现智能体等应用上前景广阔，但在空间推理方面仍存在诸多挑战。以往基准测试主要针对静态家居环境且侧重单一能力，未能反映现实复杂环境中的整体表现。为此，作者希望填补动态、实际环境主动空间推理评测的空白。

Method: 作者构建了IndustryNav基准，包括12个人工制作、高保真的Unity仓库场景，融入动态物体与人物运动。采用结合自我中心视觉与全局里程计的PointGoal导航管线，评估智能体的局部-全局规划能力。引入“碰撞率”和“警告率”指标，量化安全行为及距离估计。同时，对包括GPT-5-mini、Claude-4.5和Gemini-2.5在内的9种SOTA VLLMs进行系统测试。

Result: 评测显示，闭源模型整体表现更好，但所有智能体在路径规划、避障和主动探索方面均表现出明显不足。

Conclusion: 作者认为，当前VLLMs在动态实际环境下存在重大缺陷，强调未来应聚焦于稳定规划、主动探索和安全行为等方向，以实现更真实、全面的体现智能体表现。

Abstract: While Visual Large Language Models (VLLMs) show great promise as embodied agents, they continue to face substantial challenges in spatial reasoning. Existing embodied benchmarks largely focus on passive, static household environments and evaluate only isolated capabilities, failing to capture holistic performance in dynamic, real-world complexity. To fill this gap, we present IndustryNav, the first dynamic industrial navigation benchmark for active spatial reasoning. IndustryNav leverages 12 manually created, high-fidelity Unity warehouse scenarios featuring dynamic objects and human movement. Our evaluation employs a PointGoal navigation pipeline that effectively combines egocentric vision with global odometry to assess holistic local-global planning. Crucially, we introduce the "collision rate" and "warning rate" metrics to measure safety-oriented behaviors and distance estimation. A comprehensive study of nine state-of-the-art VLLMs (including models such as GPT-5-mini, Claude-4.5, and Gemini-2.5) reveals that closed-source models maintain a consistent advantage; however, all agents exhibit notable deficiencies in robust path planning, collision avoidance and active exploration. This highlights a critical need for embodied research to move beyond passive perception and toward tasks that demand stable planning, active exploration, and safe behavior in dynamic, real-world environment.

</details>


### [181] [Human Imitated Bipedal Locomotion with Frequency Based Gait Generator Network](https://arxiv.org/abs/2511.17387)
*Yusuf Baran Ates,Omer Morgul*

Main category: cs.RO

TL;DR: 本文提出了一种结合从人类步态中学习得到的步态生成网络与PPO力矩控制器的轻量级框架，实现了类似人类、鲁棒的双足行走。该方法虽然仅在平地或轻微坡地训练，但在更陡坡道及不平地形上也有很好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前，实现自然且鲁棒的仿人双足步态仍很困难，主要因混合动力学和地形多变，现有方法在复杂地形上的泛化性有限。

Method: 作者提出将从人类动作中学习得到的步态生成网络作为运动先验，与深度强化学习PPO算法结合进行力矩控制；训练只在简单地形（平地、轻斜坡）进行。

Result: 实验显示，所学策略能很好泛化到更陡的坡道和崎岖表面，走路自然稳健，且训练成本较低。

Conclusion: 将谱运动先验与深度强化学习结合，为实现自然且鲁棒的仿人双足步态提供了一种高效实用的新路径。

Abstract: Learning human-like, robust bipedal walking remains difficult due to hybrid dynamics and terrain variability. We propose a lightweight framework that combines a gait generator network learned from human motion with Proximal Policy Optimization (PPO) controller for torque control. Despite being trained only on flat or mildly sloped ground, the learned policies generalize to steeper ramps and rough surfaces. Results suggest that pairing spectral motion priors with Deep Reinforcement Learning (DRL) offers a practical path toward natural and robust bipedal locomotion with modest training cost.

</details>


### [182] [Feasibility of Embodied Dynamics Based Bayesian Learning for Continuous Pursuit Motion Control of Assistive Mobile Robots in the Built Environment](https://arxiv.org/abs/2511.17401)
*Xiaoshan Zhou,Carol C. Menassa,Vineet R. Kamat*

Main category: cs.RO

TL;DR: 本文提出了一种新的基于大脑启发的贝叶斯推断框架，实现了脑机接口（BCI）对助残轮椅的连续运动控制，在公开数据集上极大提升了速度预测精度，超越主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有BCI轮椅控制主要局限于离散指令，无法实现用户对速度和方向的实时自由调整。而在人群密集、路线复杂的公共环境内，轮椅用户需具备灵活、自然的连续控制能力以便安全移动和社会交流。

Method: 提出并验证了一种基于加速度运动表征的贝叶斯推断运动解码框架，采用Automatic Relevance Determination筛选特征，结合持续在线学习。利用16小时公开EEG数据集，比较了本方法与深度学习（EEGNet）及自回归传统方法的性能。

Result: 在session累积迁移学习环境下，该方法将预测速度与真实速度间的归一化均方误差相比主流方法降低了72%。

Conclusion: 该方法不仅理论上支持具身认知理论，揭示了大脑运动控制动态的本质，且实践上为更稳定、直观的脑控轮椅提供了有前景的基础，实现自然灵活的用户控制体验。

Abstract: Non-invasive electroencephalography (EEG)-based brain-computer interfaces (BCIs) offer an intuitive means for individuals with severe motor impairments to independently operate assistive robotic wheelchairs and navigate built environments. Despite considerable progress in BCI research, most current motion control systems are limited to discrete commands, rather than supporting continuous pursuit, where users can freely adjust speed and direction in real time. Such natural mobility control is, however, essential for wheelchair users to navigate complex public spaces, such as transit stations, airports, hospitals, and indoor corridors, to interact socially with the dynamic populations with agility, and to move flexibly and comfortably as autonomous driving is refined to allow movement at will. In this study, we address the gap of continuous pursuit motion control in BCIs by proposing and validating a brain-inspired Bayesian inference framework, where embodied dynamics in acceleration-based motor representations are decoded. This approach contrasts with conventional kinematics-level decoding and deep learning-based methods. Using a public dataset with sixteen hours of EEG from four subjects performing motor imagery-based target-following, we demonstrate that our method, utilizing Automatic Relevance Determination for feature selection and continual online learning, reduces the normalized mean squared error between predicted and true velocities by 72% compared to autoregressive and EEGNet-based methods in a session-accumulative transfer learning setting. Theoretically, these findings empirically support embodied cognition theory and reveal the brain's intrinsic motor control dynamics in an embodied and predictive nature. Practically, grounding EEG decoding in the same dynamical principles that govern biological motion offers a promising path toward more stable and intuitive BCI control.

</details>


### [183] [SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding](https://arxiv.org/abs/2511.17411)
*Nikolay Nikolov,Giuliano Albanese,Sombit Dey,Aleksandar Yanev,Luc Van Gool,Jan-Nico Zaech,Danda Pani Paudel*

Main category: cs.RO

TL;DR: 本文提出了一种结合3D感知与语言控制的机器人基础模型SPEAR-1，通过丰富的3D注释图像数据增强基础VLM（视觉-语言模型），在大规模数据下超越现有方法，并降低了对真实机器人示范数据的需求。


<details>
  <summary>Details</summary>
Motivation: 当前主流的机器人基础模型（RFM）多基于从互联网预训练的2D视觉-语言模型（VLM）微调而来，但这些模型缺乏进行3D空间推理的能力，无法很好地应对真实世界的机器人控制任务。直接用大规模机器人数据进行训练又成本高、难以扩展，因此需要一种可行的替代方案填补2D与3D之间的认知差距。

Method: 作者提出用廉价、易获得的非机器人2D图像，并为其添加3D空间注释，然后用这些3D注释数据增强预训练VLM，使之具备从单张2D图像中推断3D物体坐标的能力。基于此，训练了3D感知能力更强的SPEAR-VLM，并进一步构建了结合3D感知与语言指令控制的基础模型SPEAR-1，在包含24个Open X-Embodiment数据集的约4500万帧数据上训练，同时大幅度减少机器人操作演示数据的需求。

Result: SPEAR-1模型在多项任务上超过或匹配了现有的主流模型（如π_0-FAST和π_{0.5}），但仅使用了约1/20的机器人演示数据，显示了其高数据效率和泛化能力。模型权重及3D注释数据集已开放共享。

Conclusion: 通过用3D注释数据强化基础VLM，并将其用于机器人基础模型训练，能够显著提升机器人对3D空间的理解和执行力，并有效解决了高成本机器人数据获取的难题，为未来通用型机器人系统奠定了坚实基础。

Abstract: Robotic Foundation Models (RFMs) hold great promise as generalist, end-to-end systems for robot control. Yet their ability to generalize across new environments, tasks, and embodiments remains limited. We argue that a major bottleneck lies in their foundations: most RFMs are built by fine-tuning internet-pretrained Vision-Language Models (VLMs). However, these VLMs are trained on 2D image-language tasks and lack the 3D spatial reasoning inherently required for embodied control in the 3D world. Bridging this gap directly with large-scale robotic data is costly and difficult to scale. Instead, we propose to enrich easy-to-collect non-robotic image data with 3D annotations and enhance a pretrained VLM with 3D understanding capabilities. Following this strategy, we train SPEAR-VLM, a 3D-aware VLM that infers object coordinates in 3D space from a single 2D image. Building on SPEAR-VLM, we introduce our main contribution, $~\textbf{SPEAR-1}$: a robotic foundation model that integrates grounded 3D perception with language-instructed embodied control. Trained on $\sim$45M frames from 24 Open X-Embodiment datasets, SPEAR-1 outperforms or matches state-of-the-art models such as $π_0$-FAST and $π_{0.5}$, while it uses 20$\times$ fewer robot demonstrations. This carefully-engineered training strategy unlocks new VLM capabilities and as a consequence boosts the reliability of embodied control beyond what is achievable with only robotic data. We make our model weights and 3D-annotated datasets publicly available.

</details>


### [184] [RoboCOIN: An Open-Sourced Bimanual Robotic Data COllection for INtegrated Manipulation](https://arxiv.org/abs/2511.17441)
*Shihan Wu,Xuecheng Liu,Shaoxuan Xie,Pengwei Wang,Xinghang Li,Bowen Yang,Zhe Li,Kai Zhu,Hongyu Wu,Yiheng Liu,Zhaoye Long,Yue Wang,Chong Liu,Dihan Wang,Ziqiang Ni,Xiang Yang,You Liu,Ruoxuan Feng,Runtian Xu,Lei Zhang,Denghang Huang,Chenghao Jin,Anlan Yin,Xinlong Wang,Zhenguo Sun,Junkai Zhao,Mengfei Du,Mingyu Cao,Xiansheng Chen,Hongyang Cheng,Xiaojie Zhang,Yankai Fu,Ning Chen,Cheng Chi,Sixiang Chen,Huaihai Lyu,Xiaoshuai Hao,Yankai Fu,Yequan Wang,Bo Lei,Dong Liu,Xi Yang,Yance Jiao,Tengfei Pan,Yunyan Zhang,Songjing Wang,Ziqian Zhang,Xu Liu,Ji Zhang,Caowei Meng,Zhizheng Zhang,Jiyang Gao,Song Wang,Xiaokun Leng,Zhiqiang Xie,Zhenzhen Zhou,Peng Huang,Wu Yang,Yandong Guo,Yichao Zhu,Suibing Zheng,Hao Cheng,Xinmin Ding,Yang Yue,Huanqian Wang,Chi Chen,Jingrui Pang,YuXi Qian,Haoran Geng,Lianli Gao,Haiyuan Li,Bin Fang,Gao Huang,Yaodong Yang,Hao Dong,He Wang,Hang Zhao,Yadong Mu,Di Hu,Hao Zhao,Tiejun Huang,Shanghang Zhang,Yonghua Lin,Zhongyuan Wang,Guocai Yao*

Main category: cs.RO

TL;DR: RoboCOIN是一个涵盖多种机器人平台的双臂操作大规模数据集，提供统一管理和自动化标注工具，有助于推进机器人灵巧操作的研究。


<details>
  <summary>Details</summary>
Motivation: 目前机器人领域缺乏大规模、跨平台的双臂操作数据集，主要原因在于硬件平台的多样性和异构性，使得数据难以汇聚和通用。这直接限制了人形机器人双臂操作能力的研究与发展。作者希望通过构建统一大规模数据集和处理框架，推动该领域进步。

Method: 作者收集了来自15个不同机器人平台的超过18万条双臂操作演示，涵盖16种场景和421种任务，并结合双臂协作模式与物体属性进行系统化组织。提出了分层能力金字塔，提供从轨迹到子任务再到帧级运动学的多级注释。同时，开发了CoRobot处理框架及RTML标注语言，实现质量评估、自动标注和统一管理。

Result: 通过多种实验，验证了RoboCOIN数据集在多平台双臂学习中的可靠性和有效性，在多种模型架构和机器人平台上显著提升了性能。

Conclusion: RoboCOIN数据集和配套框架的开放，将为机器人双臂操作的研究和应用提供坚实的数据基础，为跨平台、通用灵巧操作能力的研究带来新机遇。

Abstract: Bimanual manipulation is essential for achieving human-like dexterity in robots, but the large-scale and diverse bimanual robot datasets remain scarce due to hardware heterogeneity across robotic platforms. To address the challenge, we present RoboCOIN, a comprehensive multi-embodiment bimanual manipulation dataset with over 180,000 demonstrations collected from 15 distinct robotic platforms. The dataset covers 16 scenarios, including residential, commercial, and working environments, with 421 tasks systematically organized by bimanual coordination patterns and object properties. Our key innovation is a hierarchical capability pyramid that provides multi-level annotations, spanning trajectory-level concepts, segment-level subtasks, and frame-level kinematics. We further develop CoRobot, a comprehensive processing framework featuring Robot Trajectory Markup Language (RTML) for quality assessment, automated annotation generation, and unified multi-embodiment management. Extensive experiments demonstrate the reliability and effectiveness of RoboCOIN in multi-embodiment bimanual learning, with significant performance improvements across various model architectures and robotic platforms. The complete dataset and framework are open-sourced and publicly available for further research purposes. Project website: https://FlagOpen.github.io/RoboCOIN/.

</details>


### [185] [MDG: Masked Denoising Generation for Multi-Agent Behavior Modeling in Traffic Environments](https://arxiv.org/abs/2511.17496)
*Zhiyu Huang,Zewei Zhou,Tianhui Cai,Yun Zhang,Jiaqi Ma*

Main category: cs.RO

TL;DR: 提出了一种名为Masked Denoising Generation（MDG）的新框架，高效统一地建模多智能体行为，兼顾多任务泛化和采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体行为建模方法（如扩散模型、自回归模型）存在采样缓慢、解码顺序性、过度依赖特定任务等问题，无法高效通用地应用于自动驾驶等实际场景。

Method: 作者提出MDG：将多智能体时空行为建模转化为带噪张量的重构问题。采用针对每个智能体和每个时间步的连续掩码噪声，使模型可以在单次或少量前向推理中实现局部去噪和轨迹生成，无需传统的扩散步或离散化令牌。该方法泛化支持开放环预测、闭环仿真、运动规划、条件生成等多种任务。

Result: 在Waymo Sim Agents和nuPlan Planning等真实自动驾驶数据集上，MDG在闭环仿真任务中取得了有竞争力的成绩，同时在开放环多智能体轨迹生成方面，实现了高效、一致和可控的表现。

Conclusion: MDG是兼具高效、灵活和一致性的多智能体行为建模范式，具备很强的任务泛化和实际应用潜力。

Abstract: Modeling realistic and interactive multi-agent behavior is critical to autonomous driving and traffic simulation. However, existing diffusion and autoregressive approaches are limited by iterative sampling, sequential decoding, or task-specific designs, which hinder efficiency and reuse. We propose Masked Denoising Generation (MDG), a unified generative framework that reformulates multi-agent behavior modeling as the reconstruction of independently noised spatiotemporal tensors. Instead of relying on diffusion time steps or discrete tokenization, MDG applies continuous, per-agent and per-timestep noise masks that enable localized denoising and controllable trajectory generation in a single or few forward passes. This mask-driven formulation generalizes across open-loop prediction, closed-loop simulation, motion planning, and conditional generation within one model. Trained on large-scale real-world driving datasets, MDG achieves competitive closed-loop performance on the Waymo Sim Agents and nuPlan Planning benchmarks, while providing efficient, consistent, and controllable open-loop multi-agent trajectory generation. These results position MDG as a simple yet versatile paradigm for multi-agent behavior modeling.

</details>


### [186] [HALO: High-Altitude Language-Conditioned Monocular Aerial Exploration and Navigation](https://arxiv.org/abs/2511.17497)
*Yuezhan Tao,Dexter Ong,Fernando Cladera,Jason Hughes,Camillo J. Taylor,Pratik Chaudhari,Vijay Kumar*

Main category: cs.RO

TL;DR: 本论文提出了一种基于单目相机、GPS和IMU的高空无人机实时度量-语义建图与探索系统HALO，实现了大规模室外环境下的精准三维重建与任务规划。


<details>
  <summary>Details</summary>
Motivation: 当前，使用视觉进行高空、大范围无人机环境建图与探索面临实时性、场景几何和语义准确性等诸多挑战。针对这些痛点，本文旨在实现无人机在大范围、高空环境下的高效智能任务执行。

Method: HALO系统采用单目相机结合GPS和IMU，进行实时密集3D重建，同时提取环境的语义信息，并能规划智能路径，支持以自然语言指定的多任务。方法经过大规模仿真和定制四旋翼实机测试验证。

Result: 在78,000平方米的大规模仿真环境中，HALO与最新语义探索基线相比，探索时间更短，路线竞争比高达68%。实机测试支持全模块无人机端运行，40米高空自主完成24,600平方米区域多任务探索。

Conclusion: HALO系统大幅提升了高空无人机室外环境下实时三维建图、语义识别及任务规划能力，能够有效支持复杂多任务的自主执行，在仿真与现实场景下均优于现有技术。

Abstract: We demonstrate real-time high-altitude aerial metric-semantic mapping and exploration using a monocular camera paired with a global positioning system (GPS) and an inertial measurement unit (IMU). Our system, named HALO, addresses two key challenges: (i) real-time dense 3D reconstruction using vision at large distances, and (ii) mapping and exploration of large-scale outdoor environments with accurate scene geometry and semantics. We demonstrate that HALO can plan informative paths that exploit this information to complete missions with multiple tasks specified in natural language. In simulation-based evaluation across large-scale environments of size up to 78,000 sq. m., HALO consistently completes tasks with less exploration time and achieves up to 68% higher competitive ratio in terms of the distance traveled compared to the state-of-the-art semantic exploration baseline. We use real-world experiments on a custom quadrotor platform to demonstrate that (i) all modules can run onboard the robot, and that (ii) in diverse environments HALO can support effective autonomous execution of missions covering up to 24,600 sq. m. area at an altitude of 40 m. Experiment videos and more details can be found on our project page: https://tyuezhan.github.io/halo/.

</details>


### [187] [RynnVLA-002: A Unified Vision-Language-Action and World Model](https://arxiv.org/abs/2511.17502)
*Jun Cen,Siteng Huang,Yuqian Yuan,Hangjie Yuan,Chaohui Yu,Yuming Jiang,Jiayan Guo,Kehan Li,Hao Luo,Fan Wang,Xin Li,Deli Zhao,Hao Chen*

Main category: cs.RO

TL;DR: 本文提出了RynnVLA-002，一种统一的视觉-语言-动作（VLA）与世界模型，实现了更高效的环境动态学习与动作规划。该方法在模拟和真实机器人任务中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 目前VLA模型和世界模型往往独立研究，难以协同提升机器人的感知、预测与决策能力。作者旨在通过模型统一，实现两者优势互补，提升机器人任务表现。

Method: RynnVLA-002将VLA模型与世界模型结合：世界模型利用视觉与动作输入预测未来图像状态，掌握环境物理规律，优化动作生成；VLA模型则根据观测图像给出后续动作，提升感知并辅助世界模型生成图像。两者统一框架下实现联合训练，促进感知与决策能力同步提升。

Result: 实验显示，该统一模型不仅优于单独的VLA或世界模型，而且两者协同能互相提升。RynnVLA-002在LIBERO模拟基准测试中无预训练即可达到97.4%的成功率，在实际机器人实验中集成世界模型使成功率提升50%。

Conclusion: 统一VLA与世界模型的RynnVLA-002显著提升了机器人在感知-预测-决策链路上的能力，为视觉引导的自主机器人任务提供了更高效、鲁棒的解决方案。

Abstract: We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.

</details>
