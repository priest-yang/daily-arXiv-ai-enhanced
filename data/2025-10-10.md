<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 103]
- [cs.CL](#cs.CL) [Total: 97]
- [cs.RO](#cs.RO) [Total: 35]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enhancing Maritime Object Detection in Real-Time with RT-DETR and Data Augmentation](https://arxiv.org/abs/2510.07346)
*Nader Nemati*

Main category: cs.CV

TL;DR: 本文提出了一种基于RT-DETR的实时海事目标检测系统，通过合成数据增强、特征融合和不确定性最小化提升小目标识别能力，在真实数据上严格评估，兼顾准确率和推理速度。


<details>
  <summary>Details</summary>
Motivation: 海事目标检测存在目标小、真实有标注RGB数据不足等问题，传统方法效果受限；作者希望提升小目标检测精度，并缓解合成与真实数据的领域差异。

Method: 使用RT-DETR作为检测主干，设计了多尺度特征融合模块，用于加强小、低对比度船舶的检测。同时引入不确定性最小化的query筛选机制，提升可靠性，并在合成与真实样本间采用智能权重平衡，缩小领域差异。数据增强用于类别间平衡，提高模型鲁棒性。

Result: 实现了一个全流程Python海洋目标检测系统，在保证实时性的同时，提升了小目标检测性能。通过组件分析，量化了检测系统各模块的贡献和鲁棒性，在极端环境下仍有稳定表现。

Conclusion: 基于RT-DETR的检测系统结合多模块，有效提升了海事小目标检测效果，可在实际场景实时应用，并通过不同模块权衡速度与精度。

Abstract: Maritime object detection faces essential challenges due to the small target
size and limitations of labeled real RGB data. This paper will present a
real-time object detection system based on RT-DETR, enhanced by employing
augmented synthetic images while strictly evaluating on real data. This study
employs RT-DETR for the maritime environment by combining multi-scale feature
fusion, uncertainty-minimizing query selection, and smart weight between
synthetic and real training samples. The fusion module in DETR enhances the
detection of small, low-contrast vessels, query selection focuses on the most
reliable proposals, and the weighting strategy helps reduce the visual gap
between synthetic and real domains. This design preserves DETR's refined
end-to-end set prediction while allowing users to adjust between speed and
accuracy at inference time. Data augmentation techniques were also used to
balance the different classes of the dataset to improve the robustness and
accuracy of the model. Regarding this study, a full Python robust maritime
detection pipeline is delivered that maintains real-time performance even under
practical limits. It also verifies how each module contributes, and how the
system handles failures in extreme lighting or sea conditions. This study also
includes a component analysis to quantify the contribution of each
architectural module and explore its interactions.

</details>


### [2] [DynamicEval: Rethinking Evaluation for Dynamic Text-to-Video Synthesis](https://arxiv.org/abs/2510.07441)
*Nithin C. Babu,Aniruddha Mahapatra,Harsh Rangwani,Rajiv Soundararajan,Kuldeep Kulkarni*

Main category: cs.CV

TL;DR: 本文提出了DynamicEval，一个针对文本生成视频（T2V）模型在动态摄影机运动场景下的测试基准，弥补了现有评价基准对动态性和细粒度评估的不足。


<details>
  <summary>Details</summary>
Motivation: 目前主流的T2V评价基准（如VBench和EvalCrafter）多数聚焦于静态或主体为中心的场景，忽视了动态摄影机运动下的电影感画面表现，以及生成模型在特定视频粒度评测上的表达力。因此需要有新的基准和评价指标，专门衡量动态场景下的生成质量，并支持更精细的视频级评估。

Method: 作者构建了DynamicEval基准，系统性地设计了突出动态摄影机运动的描述词，生成3k视频和45k对人类标注结果，涵盖10个T2V模型。其度量方式包括背景一致性和前景一致性两大维度。背景一致性在原有VBench基础上，创新引入了针对遮挡等失效情况的对象误差图修正机制。前景一致性则通过跟踪对象内的关键点及邻域，量化对象本身的保真度。

Result: 实验结果表明，提出的新度量指标在视频级与模型级的数据上与人工喜好高度相关（提升超2个百分点），有效提升了评价T2V模型生成动态场景能力的准确性。

Conclusion: DynamicEval为文本生成视频模型动态场景下的评测提供了更具全局性和细粒度的标准，有助于推动T2V模型在电影级、复杂动态视角条件下的进步。

Abstract: Existing text-to-video (T2V) evaluation benchmarks, such as VBench and
EvalCrafter, suffer from two limitations. (i) While the emphasis is on
subject-centric prompts or static camera scenes, camera motion essential for
producing cinematic shots and existing metrics under dynamic motion are largely
unexplored. (ii) These benchmarks typically aggregate video-level scores into a
single model-level score for ranking generative models. Such aggregation,
however, overlook video-level evaluation, which is vital to selecting the
better video among the candidate videos generated for a given prompt. To
address these gaps, we introduce DynamicEval, a benchmark consisting of
systematically curated prompts emphasizing dynamic camera motion, paired with
45k human annotations on video pairs from 3k videos generated by ten T2V
models. DynamicEval evaluates two key dimensions of video quality: background
scene consistency and foreground object consistency. For background scene
consistency, we obtain the interpretable error maps based on the Vbench motion
smoothness metric. We observe that while the Vbench motion smoothness metric
shows promising alignment with human judgments, it fails in two cases:
occlusions/disocclusions arising from camera and foreground object movements.
Building on this, we propose a new background consistency metric that leverages
object error maps to correct two failure cases in a principled manner. Our
second innovation is the introduction of a foreground consistency metric that
tracks points and their neighbors within each object instance to assess object
fidelity. Extensive experiments demonstrate that our proposed metrics achieve
stronger correlations with human preferences at both the video level and the
model level (an improvement of more than 2% points), establishing DynamicEval
as a more comprehensive benchmark for evaluating T2V models under dynamic
camera motion.

</details>


### [3] [Provably Accelerated Imaging with Restarted Inertia and Score-based Image Priors](https://arxiv.org/abs/2510.07470)
*Marien Renaud,Julien Hermant,Deliang Wei,Yu Sun*

Main category: cs.CV

TL;DR: 本文提出了一种用于反问题图像复原的新算法RISP，通过结合重启惯性机制与基于分数的先验，兼顾了收敛速度和重建质量，在多种图像反问题中实现了既快又优的复原效果。


<details>
  <summary>Details</summary>
Motivation: 当前许多成像反问题算法（如RED）侧重于先验设计以提升重建质量，但往往忽视收敛加速，导致算法实际应用时收敛速度较慢。因此，亟需一种方法在提高重建质量的同时，显著加快算法收敛速度。

Method: 提出了RISP方法，将重启惯性机制引入RED框架，以提升优化收敛速度；同时利用基于分数的图像先验，保持高质量重建。作者还对RISP的连续时间动力学系统进行建模，分析其与重球ODE的联系。

Result: 理论上证明RISP比RED具有更快的收敛速率（无需先验凸性假设），并在多种成像反问题实验中验证了RISP的快速收敛特性和优质复原效果。

Conclusion: RISP方法有效兼顾了收敛速度与重建质量，为解决成像反问题提供了一条理论与实践兼优的新路径。

Abstract: Fast convergence and high-quality image recovery are two essential features
of algorithms for solving ill-posed imaging inverse problems. Existing methods,
such as regularization by denoising (RED), often focus on designing
sophisticated image priors to improve reconstruction quality, while leaving
convergence acceleration to heuristics. To bridge the gap, we propose Restarted
Inertia with Score-based Priors (RISP) as a principled extension of RED. RISP
incorporates a restarting inertia for fast convergence, while still allowing
score-based image priors for high-quality reconstruction. We prove that RISP
attains a faster stationary-point convergence rate than RED, without requiring
the convexity of the image prior. We further derive and analyze the associated
continuous-time dynamical system, offering insight into the connection between
RISP and the heavy-ball ordinary differential equation (ODE). Experiments
across a range of imaging inverse problems demonstrate that RISP enables fast
convergence while achieving high-quality reconstructions.

</details>


### [4] [A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy](https://arxiv.org/abs/2510.07492)
*Guoliang Gong,Man Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于图像净化（IP）策略的新颖降噪框架，有效解决了超低剂量CT（uLDCT）降噪中因空间错位造成的难题，实现了更好解剖结构保留和优异降噪性能。


<details>
  <summary>Details</summary>
Motivation: uLDCT虽然显著降低了辐射剂量，但带来了严重的噪声与伪影，并导致图像与标准剂量CT（NDCT）配对时存在空间错位。这造成了噪声模型训练时数据不匹配的问题，严重影响了降噪效果，因此亟需有效的解决方案。

Method: 作者首先构建了真实临床uLDCT肺部数据集。然后提出了图像净化（IP）策略，以生成结构对齐的uLDCT-NDCT图像对，为网络训练提供高质量数据基础。在此基础上，进一步提出频域流匹配（FFM）模型，与IP策略协同工作，在降噪的同时更好地保留了解剖结构完整性。

Result: 实验表明，IP策略显著提升了多种主流降噪模型在uLDCT任务上的表现。FFM模型结合IP策略，在解剖结构保留方面达到当前最佳（SOTA）效果。相关代码与数据已开源。

Conclusion: 该方法为uLDCT实际降噪过程中因数据不匹配导致的问题提供了行之有效的解决方案，有望推动超低剂量CT降噪技术临床应用发展。

Abstract: Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but
introduces severe noise and artifacts. It also leads to substantial spatial
misalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses
challenges for directly applying existing denoising networks trained on
synthetic noise or aligned data. To address this core challenge in uLDCT
denoising, this paper proposes an innovative denoising framework based on an
Image Purification (IP) strategy. First, we construct a real clinical uLDCT
lung dataset. Then, we propose an Image Purification strategy that generates
structurally aligned uLDCT-NDCT image pairs, providing a high-quality data
foundation for network training. Building upon this, we propose a
Frequency-domain Flow Matching (FFM) model, which works synergistically with
the IP strategy to excellently preserve the anatomical structure integrity of
denoised images. Experiments on the real clinical dataset demonstrate that our
IP strategy significantly enhances the performance of multiple mainstream
denoising models on the uLDCT task. Notably, our proposed FFM model combined
with the IP strategy achieves state-of-the-art (SOTA) results in anatomical
structure preservation. This study provides an effective solution to the data
mismatch problem in real-world uLDCT denoising. Code and dataset are available
at https://github.com/MonkeyDadLufy/flow-matching.

</details>


### [5] [D2RA: Dual Domain Regeneration Attack](https://arxiv.org/abs/2510.07538)
*Pragati Shuddhodhan Meshram,Varun Chandrasekaran*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、针对单张图片的攻击方法D2RA，能在无模型访问的情况下有效去除或削弱生成模型内容的水印。实验显示该方法对多种主流水印方案均有效，凸显了现有水印技术存在的根本性弱点。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型广泛应用，对内容溯源和归属的需求推动了水印技术的发展。然而即便是最新的语义水印方案，仍然存在容易被攻击的风险，因此有必要评估其安全性。

Method: 提出D2RA攻击方法，无需训练模型、只需对单张图片操作，并利用在互补特征空间中的自然性约束，将含水印图片投影到类似自然图像的分布，从而最大限度减少水印信号且保持图片质量。

Result: D2RA方法在多个不同的主流水印方案上都表现出显著的水印检测率下降，能有效削弱甚至去除水印，同时图像依然具有较高的视觉保真度。

Conclusion: 当前水印设计中存在基础性的安全缺陷，提出的D2RA攻击方法揭示了这些弱点，表明目前的水印技术还需深入提升鲁棒性与安全性。

Abstract: The growing use of generative models has intensified the need for
watermarking methods that ensure content attribution and provenance. While
recent semantic watermarking schemes improve robustness by embedding signals in
latent or frequency representations, we show they remain vulnerable even under
resource-constrained adversarial settings. We present D2RA, a training-free,
single-image attack that removes or weakens watermarks without access to the
underlying model. By projecting watermarked images onto natural priors across
complementary representations, D2RA suppresses watermark signals while
preserving visual fidelity. Experiments across diverse watermarking schemes
demonstrate that our approach consistently reduces watermark detectability,
revealing fundamental weaknesses in current designs. Our code is available at
https://github.com/Pragati-Meshram/DAWN.

</details>


### [6] [PickStyle: Video-to-Video Style Transfer with Context-Style Adapters](https://arxiv.org/abs/2510.07546)
*Soroush Mehraban,Vida Adeli,Jacob Rommann,Babak Taati,Kyryl Truskovskyi*

Main category: cs.CV

TL;DR: 本文提出了PickStyle方法，实现了基于扩散模型的视频风格迁移，可以通过文本提示把指定风格应用到输入视频中，并有效保持原视频的内容。


<details>
  <summary>Details</summary>
Motivation: 当前视频风格迁移面临监督配对视频稀缺的问题，传统方法难以保证风格一致性和内容保持。作者希望利用现有的预训练扩散模型以及丰富的图像风格配对数据，弥补视频配对样本的不足。

Method: PickStyle方法主要包括：1）在预训练视频扩散模型的自注意力层中插入低秩风格适配器，实现高效的运动和风格迁移；2）用成对静态图像经过共享增强（如摄像机运动仿真）合成训练视频，提高模型对时序一致性的学习；3）提出CS-CFG引导机制，将文本（风格）与视频（内容）的引导解耦，实现风格迁移同时增强内容保持能力。

Result: 在多个基准测试上，PickStyle生成的视频具有更强的时序一致性、风格还原性和内容保真度，在定性和定量评测上均优于现有方法。

Conclusion: PickStyle有效解决了受限监督下的视频风格迁移问题，利用低秩适配器与新颖的引导机制，在保证内容一致性的同时实现了高质量的风格迁移。

Abstract: We address the task of video style transfer with diffusion models, where the
goal is to preserve the context of an input video while rendering it in a
target style specified by a text prompt. A major challenge is the lack of
paired video data for supervision. We propose PickStyle, a video-to-video style
transfer framework that augments pretrained video diffusion backbones with
style adapters and benefits from paired still image data with source-style
correspondences for training. PickStyle inserts low-rank adapters into the
self-attention layers of conditioning modules, enabling efficient
specialization for motion-style transfer while maintaining strong alignment
between video content and style. To bridge the gap between static image
supervision and dynamic video, we construct synthetic training clips from
paired images by applying shared augmentations that simulate camera motion,
ensuring temporal priors are preserved. In addition, we introduce Context-Style
Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free
guidance into independent text (style) and video (context) directions. CS-CFG
ensures that context is preserved in generated video while the style is
effectively transferred. Experiments across benchmarks show that our approach
achieves temporally coherent, style-faithful, and content-preserving video
translations, outperforming existing baselines both qualitatively and
quantitatively.

</details>


### [7] [TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility](https://arxiv.org/abs/2510.07550)
*Saman Motamed,Minghao Chen,Luc Van Gool,Iro Laina*

Main category: cs.CV

TL;DR: 现代视频生成模型常常生成违反物理常识的视频序列。本文提出TRAVL方法和ImplausiBench基准，致力于提升和评估视频-语言模型（VLMs）在物理合理性判别上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型生成的内容虽然视觉上逼真，但常出现物理不合理现象，如物体漂浮、瞬移或变形，而目前缺乏定量评估视频物理合理性的有效方法。

Method: 1）实验证明现有VLM难以识别物理违规现象。2）提出TRAVL微调方式，结合均衡数据集和轨迹感知注意力模块，提升模型运动理解。3）提出ImplausiBench基准，包括300个视频，并利用人工和LLM标准进行评测，剥离语言偏见，专注视觉-时序理解。

Result: 实验证实现有VLM对物理违规检测能力有限。经TRAVL微调后，VLM在ImplausiBench基准上的判别性能提升。提出的数据集和评测方案有效评估了模型的物理合理性理解能力。

Conclusion: TRAVL和ImplausiBench为多模态模型的物理有效性提供了评测与提升的新框架，并推动了视频理解中对复杂物理现象理解的研究。

Abstract: Despite impressive visual fidelity, modern video generative models frequently
produce sequences that violate intuitive physical laws, such as objects
floating, teleporting, or morphing in ways that defy causality. While humans
can easily detect such implausibilities, there remains no robust method for
quantitatively assessing physical realism in video. In this work, we explore
whether Video-Language Models (VLMs) can be trained to serve as reliable judges
of physical plausibility. We find that existing VLMs struggle to identify
physics violations, exposing fundamental limitations in their temporal and
causal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe
that combines a balanced training dataset with a trajectory-aware attention
module to improve motion encoding and discrimination in VLMs. To evaluate
physical reasoning more rigorously, we propose ImplausiBench, a benchmark of
300 videos (150 real, 150 generated) that removes linguistic biases and
isolates visual-temporal understanding. Performance is reported both with
gold-standard human judgments and stricter LLM-as-judge metrics. Together,
TRAVL and ImplausiBench offer a unified framework for probing and improving
physical plausibility in multimodal models, shedding light on a challenging and
underexplored aspect of visual-temporal understanding.

</details>


### [8] [Label Semantics for Robust Hyperspectral Image Classification](https://arxiv.org/abs/2510.07556)
*Rafin Hassan,Zarin Tasnim Roshni,Rafiqul Bari,Alimul Islam,Nabeel Mohammed,Moshiur Farazi,Shafin Rahman*

Main category: cs.CV

TL;DR: 本文提出了一种将语义文本信息与高光谱图像（HSI）融合的分类方法，有效提升了HSI分类的准确率。


<details>
  <summary>Details</summary>
Motivation: 由于高光谱图像数据维度高且高质量训练样本有限，现有HSI分类模型容易过拟合且难以在性能与复杂度之间取得平衡。此外，大部分模型仅依赖谱-空数据，未能利用丰富的语义信息。作者希望通过引入类的语义描述克服上述不足，提升分类效果。

Method: 提出Semantic Spectral-Spatial Fusion Network（S3FN）。该方法利用大语言模型（LLM）为每个类别自动生成详细的语义文本描述，之后通过如BERT、RoBERTa等文本编码器将描述转为向量，嵌入分类模型，实现更好的特征与标签对齐，提升模型判别力。

Result: 在三个基准HSI数据集（Hyperspectral Wood、HyperspectralBlueberries和DeepHS-Fruit）上实验，S3FN取得了显著优于传统方法的性能提升。

Conclusion: 文本语义信息与谱-空数据的结合能够显著增强高光谱图像分类模型表现，为未来基于语义增强的HSI分类研究提供了新方向。

Abstract: Hyperspectral imaging (HSI) classification is a critical tool with widespread
applications across diverse fields such as agriculture, environmental
monitoring, medicine, and materials science. Due to the limited availability of
high-quality training samples and the high dimensionality of spectral data, HSI
classification models are prone to overfitting and often face challenges in
balancing accuracy and computational complexity. Furthermore, most of HSI
classification models are monomodal, where it solely relies on spectral-spatial
data to learn decision boundaries in the high dimensional embedding space. To
address this, we propose a general-purpose Semantic Spectral-Spatial Fusion
Network (S3FN) that uses contextual, class specific textual descriptions to
complement the training of an HSI classification model. Specifically, S3FN
leverages LLMs to generate comprehensive textual descriptions for each class
label that captures their unique characteristics and spectral behaviors. These
descriptions are then embedded into a vector space using a pre-trained text
encoder such as BERT or RoBERTa to extract meaningful label semantics which in
turn leads to a better feature-label alignment for improved classification
performance. To demonstrate the effectiveness of our approach, we evaluate our
model on three diverse HSI benchmark datasets - Hyperspectral Wood,
HyperspectralBlueberries, and DeepHS-Fruit and report significant performance
boost. Our results highlight the synergy between textual semantics and
spectral-spatial data, paving the way for further advancements in semantically
augmented HSI classification models. Codes are be available in:
https://github.com/milab-nsu/S3FN

</details>


### [9] [Cross-Modal Attention Guided Unlearning in Vision-Language Models](https://arxiv.org/abs/2510.07567)
*Karuna Bhaila,Aneesh Komanduri,Minh-Hao Van,Xintao Wu*

Main category: cs.CV

TL;DR: 本文提出了一种高效轻量的视觉-语言模型（VLMs）“去遗忘”框架CAGUL，能防止敏感信息泄漏，并保持模型原有性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在多模态任务中表现突出，但在大规模预训练时可能记忆并泄漏敏感信息。以往去遗忘技术主要针对文本大模型，VLM中涉及的视觉信息增加了复杂性，需有专门解决方案。

Method: 作者提出CAGUL（Cross-Modal Attention Guided Unlearning），通过分析跨模态注意力中视觉token对输出的作用，利用外部模块对低重要性视觉token进行编码，进行高效去遗忘，而无需重新训练或修改原有模型参数。

Result: 实验证明，CAGUL在防止敏感信息泄漏的同时，能较好地维持模型输出效果，性能优于或等同于微调法，且无需高昂的计算代价。

Conclusion: CAGUL为VLMs提供了一种实际可行的去遗忘新选择，有效防止敏感信息泄漏，且不影响模型原有能力，具有高效、低成本的优势。

Abstract: Vision-Language Models (VLMs) have demonstrated immense capabilities in
multi-modal understanding and inference tasks such as Visual Question Answering
(VQA), which requires models to infer outputs based on visual and textual
context simultaneously. Such inference abilities of large-scale pretrained
models are often attributed to the massive scale of pre-training data collected
across several domains. However, the models may memorize private and/or
sensitive information during training and regurgitate it in inference.
Recently, machine unlearning has been leveraged to address the leakage of
private data in LLMs. VLMs add a layer of complexity to this process, as the
visual context in the query may also contain sensitive information in addition
to the text. To address this issue, we explore unlearning for vision-language
models, specifically for the VQA task. We explore the role of visual tokens for
output generation in VLMs using cross-modal attention and utilize it to
formulate Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight and
efficient VLM unlearning framework. In contrast to computationally expensive
model finetuning methods, CAGUL utilizes external modules to encode unlearning
information in visual tokens of low importance for relevant queries. We find
that the transformed visual tokens not only prevent leakage but also retain
reference model behavior. Experimental results show that our method performs
better or on par with finetuning-based baselines without altering the
pre-trained model parameters or incurring retraining costs, making it a
practical and effective unlearning solution for VLMs.

</details>


### [10] [MaizeStandCounting (MaSC): Automated and Accurate Maize Stand Counting from UAV Imagery Using Image Processing and Deep Learning](https://arxiv.org/abs/2510.07580)
*Dewi Endah Kharismawati,Toni Kazic*

Main category: cs.CV

TL;DR: 本文提出了MaizeStandCounting(MaSC)算法，实现了利用低成本无人机RGB影像自动统计玉米幼苗数量，显著减少人工工作并提升精度。


<details>
  <summary>Details</summary>
Motivation: 玉米苗数统计在田间管理、产量预测和早期问题发现等方面非常关键，人工计数耗时费力且易出错，尤其在大规模或地形复杂的地块中更为突出，因此需要高效、自动化的解决方案。

Method: MaSC结合两种模式：（1）对拼接后的大图分块分析；（2）对原始视频帧配准分析。均使用轻量化YOLOv9模型检测V2-V10期间的玉米幼苗，并与杂草等进行区分。之后采用基于空间分布的行与范围分割，实现精确的按行计数。

Result: 与2024年田间手工计数对比，MaSC在拼接图模式下R^2=0.616，在原始帧模式下R^2=0.906，表现出良好的一致性。整体处理83张高分辨率帧只需60.63秒，适合实时应用。

Conclusion: MaSC是一种可扩展、低成本且高精度的玉米幼苗自动计数工具，适用于科研和生产管理场景，显著提升效率与准确性。

Abstract: Accurate maize stand counts are essential for crop management and research,
informing yield prediction, planting density optimization, and early detection
of germination issues. Manual counting is labor-intensive, slow, and
error-prone, especially across large or variable fields. We present
MaizeStandCounting (MaSC), a robust algorithm for automated maize seedling
stand counting from RGB imagery captured by low-cost UAVs and processed on
affordable hardware. MaSC operates in two modes: (1) mosaic images divided into
patches, and (2) raw video frames aligned using homography matrices. Both modes
use a lightweight YOLOv9 model trained to detect maize seedlings from V2-V10
growth stages. MaSC distinguishes maize from weeds and other vegetation, then
performs row and range segmentation based on the spatial distribution of
detections to produce precise row-wise stand counts. Evaluation against
in-field manual counts from our 2024 summer nursery showed strong agreement
with ground truth (R^2= 0.616 for mosaics, R^2 = 0.906 for raw frames). MaSC
processed 83 full-resolution frames in 60.63 s, including inference and
post-processing, highlighting its potential for real-time operation. These
results demonstrate MaSC's effectiveness as a scalable, low-cost, and accurate
tool for automated maize stand counting in both research and production
environments.

</details>


### [11] [Quick-CapsNet (QCN): A fast alternative to Capsule Networks](https://arxiv.org/abs/2510.07600)
*Pouya Shiri,Ramin Sharifi,Amirali Baniasadi*

Main category: cs.CV

TL;DR: 本文提出了一种更快的胶囊网络变体Quick-CapsNet（QCN），在检测MNIST、F-MNIST、SVHN和Cifar-10等数据集时推理速度提升5倍，精度损失很小。


<details>
  <summary>Details</summary>
Motivation: 尽管CapsNet在图像分类上表现优异且对变换具有鲁棒性，但其训练和推理速度较慢，限制了实时应用中的使用。因此，作者旨在提出一种加速CapsNet推理过程的新方法。

Method: QCN通过减少胶囊（capsule）数量，实现计算加速，并且采用了更强大的解码器替代默认解码器，以进一步提升性能。

Result: 在MNIST、F-MNIST、SVHN和Cifar-10等数据集上，QCN在只有极小精度损失的情况下，推理速度提升了5倍。

Conclusion: QCN为需要快速推理的胶囊网络应用提供了一个高效解决方案，为后续实时和资源受限场景中的胶囊网络部署提供了基础。

Abstract: The basic computational unit in Capsule Network (CapsNet) is a capsule (vs.
neurons in Convolutional Neural Networks (CNNs)). A capsule is a set of
neurons, which form a vector. CapsNet is used for supervised classification of
data and has achieved state-of-the-art accuracy on MNIST digit recognition
dataset, outperforming conventional CNNs in detecting overlapping digits.
Moreover, CapsNet shows higher robustness towards affine transformation when
compared to CNNs for MNIST datasets. One of the drawbacks of CapsNet, however,
is slow training and testing. This can be a bottleneck for applications that
require a fast network, especially during inference. In this work, we introduce
Quick-CapsNet (QCN) as a fast alternative to CapsNet, which can be a starting
point to develop CapsNet for fast real-time applications. QCN builds on
producing a fewer number of capsules, which results in a faster network. QCN
achieves this at the cost of marginal loss in accuracy. Inference is 5x faster
on MNIST, F-MNIST, SVHN and Cifar-10 datasets. We also further enhanced QCN by
employing a more powerful decoder instead of the default decoder to further
improve QCN.

</details>


### [12] [Rectified-CFG++ for Flow Based Models](https://arxiv.org/abs/2510.07631)
*Shreshth Saini,Shashank Gupta,Alan C. Bovik*

Main category: cs.CV

TL;DR: 本文提出了Rectified-CFG++方法，解决了基于Rectified Flow的大型扩散模型在文本引导生成时的稳定性和表现问题，并在多项实验中优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 目前主流的Classifier-free guidance（CFG）在用于Rectified Flow模型时会导致模型生成偏离数据流形，造成视觉伪影、文本不一致和不稳定性。因此，需要一种能够兼顾引导性和生成稳定性的新方法。

Method: 提出Rectified-CFG++，该方法采用自适应的预测-校正（predictor-corrector）引导策略：首先利用条件Rectified Flow更新将采样锚定在学习到的最优路径附近，然后通过有权重的条件校正整合有条件与无条件速度场。理论上证明了所得速度场在统计上成立，且生成轨迹收敛于数据流形附近。

Result: 在大规模文本生成图片模型（包括Flux、Stable Diffusion 3/3.5、Lumina）以及MS-COCO、LAION-Aesthetic、T2I-CompBench等多项基准数据集上的实验表明，Rectified-CFG++方法在稳定性和生成质量方面均优于常规CFG。

Conclusion: Rectified-CFG++显著提升了基于Rectified Flow图像生成模型的引导鲁棒性和生成质量，为未来大模型在复杂条件控制下的应用提供了更优解。

Abstract: Classifier-free guidance (CFG) is the workhorse for steering large diffusion
models toward text-conditioned targets, yet its native application to rectified
flow (RF) based models provokes severe off-manifold drift, yielding visual
artifacts, text misalignment, and brittle behaviour. We present
Rectified-CFG++, an adaptive predictor-corrector guidance that couples the
deterministic efficiency of rectified flows with a geometry-aware conditioning
rule. Each inference step first executes a conditional RF update that anchors
the sample near the learned transport path, then applies a weighted conditional
correction that interpolates between conditional and unconditional velocity
fields. We prove that the resulting velocity field is marginally consistent and
that its trajectories remain within a bounded tubular neighbourhood of the data
manifold, ensuring stability across a wide range of guidance strengths.
Extensive experiments on large-scale text-to-image models (Flux, Stable
Diffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms
standard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and
T2I-CompBench. Project page: https://rectified-cfgpp.github.io/

</details>


### [13] [PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment](https://arxiv.org/abs/2510.07636)
*Shashank Gupta,Gregoire Phillips,Alan C. Bovik*

Main category: cs.CV

TL;DR: 本文提出了一种采用大型多模态模型（LMMs）的无参考点云质量评价（NR-PCQA）方法，结合文本、2D投影图像和3D点云多种模态信息显著提升了评价准确性，并超过现有方法，兼具可解释性和交互性。


<details>
  <summary>Details</summary>
Motivation: 近年来LMMs广泛应用于图像与视频质量评价，但在三维点云领域的探索仍有限。点云数据广泛应用于虚拟现实、增强现实等领域，因此无需参考的自动点云质量评价（NR-PCQA）问题亟需有效解决。现有方法大多只关注单一模态，无法充分利用多源信息。

Method: 作者提出PIT-QMM模型，能够端到端地融合文本描述、2D投影、3D点云多种模态信息，并对点云质量进行评分预测。该方法通过设计统一框架，联合训练多模态输入，从而捕捉到点云质量中更丰富的感知线索。

Result: 在主流公开数据集上，PIT-QMM方法在更少训练迭代下，质量评分准确率显著优于当前最新方法。同时，该模型支持失真定位与类型识别，提升了结果解释性。

Conclusion: 构建了首个同时处理文本、图像及点云数据的LMM点云评价系统，大幅提升点云NR-PCQA的性能与可解释性，为三维数据质量评价带来了新方向，相关代码与数据集已开源。

Abstract: Large Multimodal Models (LMMs) have recently enabled considerable advances in
the realm of image and video quality assessment, but this progress has yet to
be fully explored in the domain of 3D assets. We are interested in using these
models to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), where
the aim is to automatically evaluate the perceptual quality of a point cloud in
absence of a reference. We begin with the observation that different modalities
of data - text descriptions, 2D projections, and 3D point cloud views - provide
complementary information about point cloud quality. We then construct PIT-QMM,
a novel LMM for NR-PCQA that is capable of consuming text, images and point
clouds end-to-end to predict quality scores. Extensive experimentation shows
that our proposed method outperforms the state-of-the-art by significant
margins on popular benchmarks with fewer training iterations. We also
demonstrate that our framework enables distortion localization and
identification, which paves a new way forward for model explainability and
interactivity. Code and datasets are available at
https://www.github.com/shngt/pit-qmm.

</details>


### [14] [Dual-Stream Alignment for Action Segmentation](https://arxiv.org/abs/2510.07652)
*Harshala Gammulle,Clinton Fookes,Sridha Sridharan,Simon Denman*

Main category: cs.CV

TL;DR: 本文提出了一种创新性的双流对齐网络（DSA Net），结合了类量子方法，显著提升了视频连续动作分割效果，并在多个基准数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有动作分割方法主要有单流方法（注重时空帧级建模）和双流方法。近期的研究普遍关注通过动作级特征提升分割效果，但如何更好地融合动作和动作过渡信息仍然是挑战。本文的动机是通过双流协同学习和特征对齐，增强动作间的辨识与分割能力。

Method: 提出了DSA Net，将帧级和动作级两条特征流相结合。二者通过Temporal Context块进行信息交互，利用跨注意力与量子驱动的动作调制（Q-ActGM）实现特征融合。此外，设计了双流对齐损失（包含关系一致性、跨层对比、循环一致性重构三部分），以促进共享特征空间的学习。

Result: 在GTEA、Breakfast、50Salads和EgoProcel等多个数据集上，DSA Net均获得了最优的动作分割准确率。消融实验进一步验证了每一组件对整体性能的贡献。

Conclusion: DSA Net通过融合双流特征对齐与量子调制机制，为动作分割提供了全新高效的解决方案，并推动了该领域的技术进步。

Abstract: Action segmentation is a challenging yet active research area that involves
identifying when and where specific actions occur in continuous video streams.
Most existing work has focused on single-stream approaches that model the
spatio- temporal aspects of frame sequences. However, recent research has
shifted toward two-stream methods that learn action-wise features to enhance
action segmentation performance. In this work, we propose the Dual-Stream
Alignment Network (DSA Net) and investigate the impact of incorporating a
second stream of learned action features to guide segmentation by capturing
both action and action-transition cues. Communication between the two streams
is facilitated by a Temporal Context (TC) block, which fuses complementary
information using cross- attention and Quantum-based Action-Guided Modulation
(Q- ActGM), enhancing the expressive power of the fused features. To the best
of our knowledge, this is the first study to introduce a hybrid
quantum-classical machine learning framework for action segmentation. Our
primary objective is for the two streams (frame-wise and action-wise) to learn
a shared feature space through feature alignment. This is encouraged by the
proposed Dual-Stream Alignment Loss, which comprises three components:
relational consistency, cross-level contrastive, and cycle-consistency
reconstruction losses. Following prior work, we evaluate DSA Net on several
diverse benchmark datasets: GTEA, Breakfast, 50Salads, and EgoProcel. We
further demonstrate the effectiveness of each component through extensive
ablation studies. Notably, DSA Net achieves state-of-the-art performance,
significantly outperforming existing

</details>


### [15] [Once Is Enough: Lightweight DiT-Based Video Virtual Try-On via One-Time Garment Appearance Injection](https://arxiv.org/abs/2510.07654)
*Yanjie Pan,Qingdong He,Lidong Wang,Bo Peng,Mingmin Chi*

Main category: cs.CV

TL;DR: 该论文提出了一种基于首帧服装替换的视频虚拟试穿方法OIE，仅需首次替换服装，其余帧利用内容控制和时序引导生成，大幅提升了效率与表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Diffusion模型的视频虚拟试穿方法在Diffusion Transformer上迁移时面临参数过多和时序特征难以学习的问题，亟需更高效的解决方案。

Method: OIE方法基于对视频首帧进行服装替换，利用图像级模型替换首帧服装，然后将编辑后的首帧作为内容约束，借助姿态和掩码信息，引导视频生成模型按序合成剩余帧，无需大幅扩展主干网络。

Result: 实验表明，该方法相比以往方法有更高的参数和计算效率，同时在性能上依然保持领先。

Conclusion: OIE方法在降低参数量和计算成本的前提下，实现了视频虚拟试穿领域的性能提升，为该领域带来高效实用的新策略。

Abstract: Video virtual try-on aims to replace the clothing of a person in a video with
a target garment. Current dual-branch architectures have achieved significant
success in diffusion models based on the U-Net; however, adapting them to
diffusion models built upon the Diffusion Transformer remains challenging.
Initially, introducing latent space features from the garment reference branch
requires adding or modifying the backbone network, leading to a large number of
trainable parameters. Subsequently, the latent space features of garments lack
inherent temporal characteristics and thus require additional learning. To
address these challenges, we propose a novel approach, OIE (Once is Enough), a
virtual try-on strategy based on first-frame clothing replacement:
specifically, we employ an image-based clothing transfer model to replace the
clothing in the initial frame, and then, under the content control of the
edited first frame, utilize pose and mask information to guide the temporal
prior of the video generation model in synthesizing the remaining frames
sequentially. Experiments show that our method achieves superior parameter
efficiency and computational efficiency while still maintaining leading
performance under these constraints.

</details>


### [16] [MONKEY: Masking ON KEY-Value Activation Adapter for Personalization](https://arxiv.org/abs/2510.07656)
*James Baker*

Main category: cs.CV

TL;DR: 该论文提出了一种通过自动生成掩码增强扩散模型个性化能力的方法，从而使生成图片更好地同时包含主体和满足文本提示。


<details>
  <summary>Details</summary>
Motivation: 已有的扩散模型个性化方法在加入主体时，经常只重现图片主体而忽略文本提示内容，导致生成结果缺乏对文本提示的响应，因此需要改进以提升图片生成对文本描述的符合度。

Method: 作者观察到常用IP-Adapter在推理时会自动生成主体与背景的分割掩码。论文提出利用该掩码，在生成过程中第二次处理时用于屏蔽与主体无关的图像token，这样文本提示可以引导除主体外的背景生成。

Result: 实验结果表明，对于描述地点等场景的文本提示，该方法能生成兼顾主体和精确响应文本的图片。与其它测试期个性化方法相比，本方法在对齐文本提示和原始图片方面表现更好。

Conclusion: 作者方法有效提升了扩散模型个性化生成中对主体和文本提示的同时响应能力，有助于实现更精确、可控的图片生成。

Abstract: Personalizing diffusion models allows users to generate new images that
incorporate a given subject, allowing more control than a text prompt. These
models often suffer somewhat when they end up just recreating the subject
image, and ignoring the text prompt. We observe that one popular method for
personalization, the IP-Adapter automatically generates masks that we
definitively segment the subject from the background during inference. We
propose to use this automatically generated mask on a second pass to mask the
image tokens, thus restricting them to the subject, not the background,
allowing the text prompt to attend to the rest of the image. For text prompts
describing locations and places, this produces images that accurately depict
the subject while definitively matching the prompt. We compare our method to a
few other test time personalization methods, and find our method displays high
prompt and source image alignment.

</details>


### [17] [Automatic Text Box Placement for Supporting Typographic Design](https://arxiv.org/abs/2510.07665)
*Jun Muraoka,Daichi Haraguchi,Naoto Inoue,Wataru Shimoda,Kota Yamaguchi,Seiichi Uchida*

Main category: cs.CV

TL;DR: 本论文比较了四种用于广告和网页未完成布局中文本框自动放置的方法，发现基于传统Transformer的模型在任务中表现优于当前视觉-语言大模型。


<details>
  <summary>Details</summary>
Motivation: 随着广告和网页布局中自动化需求增加，如何平衡视觉美感与信息传递效率成为重要研究问题。自动文本框摆放，尤其在布局不完整场景下，是实现高效设计的关键。

Method: 本研究将标准的Transformer模型、小型视觉-语言模型(Phi3.5-vision)、大型预训练视觉-语言模型(Gemini)及一种可处理多图像的扩展Transformer进行了比较。通过在Crello数据集上的实验，评估它们在自动文本框布局任务中的表现。

Result: 结果显示，标准的Transformer模型（特别是引入更丰富外观信息后）整体优于视觉-语言模型（VLM）。不过，在处理极小文本框或布局极为密集时，所有方法都遇到困难。

Conclusion: 研究表明，针对特定任务设计的模型架构有助于提升自动布局设计效果，并为后续改进指出方向。

Abstract: In layout design for advertisements and web pages, balancing visual appeal
and communication efficiency is crucial. This study examines automated text box
placement in incomplete layouts, comparing a standard Transformer-based method,
a small Vision and Language Model (Phi3.5-vision), a large pretrained VLM
(Gemini), and an extended Transformer that processes multiple images.
Evaluations on the Crello dataset show the standard Transformer-based models
generally outperform VLM-based approaches, particularly when incorporating
richer appearance information. However, all methods face challenges with very
small text or densely populated layouts. These findings highlight the benefits
of task-specific architectures and suggest avenues for further improvement in
automated layout design.

</details>


### [18] [TCIP: Threshold-Controlled Iterative Pyramid Network for Deformable Medical Image Registration](https://arxiv.org/abs/2510.07666)
*Heming Wu,Di Wang,Tai Ma,Peng Zhao,Yubin Xiao,Zhongke Wu,Xing-Ce Wang,Chuang Li,Xuan Wu,You Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的金字塔网络结构TCIP，有效缓解医学图像配准中的结构错位和优化次数不适应等问题，在多个公开数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有金字塔网络用于变形医学图像配准虽表现优越，但其解码器容易带来结构错位且错位会累积，同时多数模型缺乏针对不同图像需要自适应优化迭代数的机制，导致配准精度下降。

Method: 提出Feature-Enhanced Residual Module（FERM）作为解码层核心，通过三步提取解剖语义特征、抑制无关特征、并估计变形场。提出双阶段阈值控制迭代（TCI）策略，自适应决定每张图像的优化次数。最终将FERM与TCI集成形成Threshold-Controlled Iterative Pyramid（TCIP）模型。

Result: TCIP在3个公开脑MRI数据集和1个腹部CT数据集上，配准精度优于现有最优模型，同时推理速度和参数量具备竞争力。将FERM与TCI集成入其他网络也同样提升了效果。消融实验验证了FERM和TCI的有效性。

Conclusion: 提出的TCIP模型有效提升了医学图像配准精度、效率和模型紧凑性，FERM和TCI具有较好通用性，可灵活集成进现有网络架构。

Abstract: Although pyramid networks have demonstrated superior performance in
deformable medical image registration, their decoder architectures are
inherently prone to propagating and accumulating anatomical structure
misalignments. Moreover, most existing models do not adaptively determine the
number of iterations for optimization under varying deformation requirements
across images, resulting in either premature termination or excessive
iterations that degrades registration accuracy. To effectively mitigate the
accumulation of anatomical misalignments, we propose the Feature-Enhanced
Residual Module (FERM) as the core component of each decoding layer in the
pyramid network. FERM comprises three sequential blocks that extract anatomical
semantic features, learn to suppress irrelevant features, and estimate the
final deformation field, respectively. To adaptively determine the number of
iterations for varying images, we propose the dual-stage Threshold-Controlled
Iterative (TCI) strategy. In the first stage, TCI assesses registration
stability and with asserted stability, it continues with the second stage to
evaluate convergence. We coin the model that integrates FERM and TCI as
Threshold-Controlled Iterative Pyramid (TCIP). Extensive experiments on three
public brain MRI datasets and one abdomen CT dataset demonstrate that TCIP
outperforms the state-of-the-art (SOTA) registration networks in terms of
accuracy, while maintaining comparable inference speed and a compact model
parameter size. Finally, we assess the generalizability of FERM and TCI by
integrating them with existing registration networks and further conduct
ablation studies to validate the effectiveness of these two proposed methods.

</details>


### [19] [Controllable Video Synthesis via Variational Inference](https://arxiv.org/abs/2510.07670)
*Haoyi Duan,Yunzhi Zhang,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视频合成方法，实现了对某些元素的高度可控性，并在其他未明确说明的元素上保持多样性。


<details>
  <summary>Details</summary>
Motivation: 目前视频生成模型通常仅支持固定的输入格式，但实际应用中用户对于控制粒度的需求多样，从精确的4D物体轨迹、相机路径到粗略的文本提示都有需求。

Method: 作者将任务建模为变分推断，通过结合多种视频生成骨干网络，共同满足所有任务约束。为解决优化难题，采用分步KL散度最小化与递进分布序列，并创新性地提出上下文条件下的分解技术以避免局部最优。

Result: 实验结果显示，该方法在可控性、多样性和3D一致性方面，相较已有方法均有提升。

Conclusion: 该视频合成方法成功满足了不同粒度控制需求，为用户带来了更高的操作灵活性，同时维持了样本多样性和一致性，有助于推动视频生成领域的发展。

Abstract: Many video workflows benefit from a mixture of user controls with varying
granularity, from exact 4D object trajectories and camera paths to coarse text
prompts, while existing video generative models are typically trained for fixed
input formats. We develop a video synthesis method that addresses this need and
generates samples with high controllability for specified elements while
maintaining diversity for under-specified ones. We cast the task as variational
inference to approximate a composed distribution, leveraging multiple video
generation backbones to account for all task constraints collectively. To
address the optimization challenge, we break down the problem into step-wise KL
divergence minimization over an annealed sequence of distributions, and further
propose a context-conditioned factorization technique that reduces modes in the
solution space to circumvent local optima. Experiments suggest that our method
produces samples with improved controllability, diversity, and 3D consistency
compared to prior works.

</details>


### [20] [Hybrid CNN-BYOL Approach for Fault Detection in Induction Motors Using Thermal Images](https://arxiv.org/abs/2510.07692)
*Tangin Amir Smrity,MD Zahin Muntaqim Hasan Muhammad Kafi,Abu Saleh Musa Miah,Najmul Hassan,Yuichi Okuyama,Nobuyoshi Asai,Taro Suzuki,Jungpil Shin*

Main category: cs.CV

TL;DR: 本文提出了一种结合BYOL与CNN的新方法，用于通过热成像识别感应电机故障，提出的BYOL-IMNet模型在准确率和推理速度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 感应电机广泛应用于工业与生活，但易出现故障导致能耗上升与设备损坏，因此需要高效、准确的故障早期检测方法。

Method: 本研究将自监督学习BYOL方法与多种流行卷积神经网络（如ResNet-50、DenseNet、EfficientNet等）结合，分析和分类电机热成像数据。此外，设计了轻量高效的专用CNN模型（BYOL-IMNet），专门用于热成像故障分类。

Result: BYOL-IMNet在测试数据上的准确率达99.89%，单张图片推理仅需5.7ms，性能优于其它主流深度学习模型。

Conclusion: CNN-BYOL混合方法在电机故障热成像检测上具有极高准确率和实时性，为工业现场电机故障在线监测提供强大技术支撑。

Abstract: Induction motors (IMs) are indispensable in industrial and daily life, but
they are susceptible to various faults that can lead to overheating, wasted
energy consumption, and service failure. Early detection of faults is essential
to protect the motor and prolong its lifespan. This paper presents a hybrid
method that integrates BYOL with CNNs for classifying thermal images of
induction motors for fault detection. The thermal dataset used in this work
includes different operating states of the motor, such as normal operation,
overload, and faults. We employed multiple deep learning (DL) models for the
BYOL technique, ranging from popular architectures such as ResNet-50,
DenseNet-121, DenseNet-169, EfficientNetB0, VGG16, and MobileNetV2.
Additionally, we introduced a new high-performance yet lightweight CNN model
named BYOL-IMNet, which comprises four custom-designed blocks tailored for
fault classification in thermal images. Our experimental results demonstrate
that the proposed BYOL-IMNet achieves 99.89\% test accuracy and an inference
time of 5.7 ms per image, outperforming state-of-the-art models. This study
highlights the promising performance of the CNN-BYOL hybrid method in enhancing
accuracy for detecting faults in induction motors, offering a robust
methodology for online monitoring in industrial settings.

</details>


### [21] [Mutual Learning for Hashing: Unlocking Strong Hash Functions from Weak Supervision](https://arxiv.org/abs/2510.07703)
*Xiaoxu Ma,Runhao Li,Zhenyu Weng*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的深度哈希框架MLH，通过联合中心化与成对方法，有效提升图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 近年来深度哈希在大规模图像检索中广泛应用，但现有中心化方法虽能捕捉全局结构，却往往忽视了局部相似性信息，导致性能受限。

Method: MLH包含两个分支：强中心化分支和弱成对分支。通过相互学习，中心化分支吸收成对分支的局部相似性提示。同时，借鉴混合专家结构，引入混合哈希专家模块以加强分支间交互。

Result: 大量实验证明，MLH在多个主流基准数据集上持续超过现有最优的哈希方法。

Conclusion: MLH方法有效结合全局与局部信息，显著提升了深度哈希在图像检索任务中的表现，对相关领域有广泛应用前景。

Abstract: Deep hashing has been widely adopted for large-scale image retrieval, with
numerous strategies proposed to optimize hash function learning. Pairwise-based
methods are effective in learning hash functions that preserve local similarity
relationships, whereas center-based methods typically achieve superior
performance by more effectively capturing global data distributions. However,
the strength of center-based methods in modeling global structures often comes
at the expense of underutilizing important local similarity information. To
address this limitation, we propose Mutual Learning for Hashing (MLH), a novel
weak-to-strong framework that enhances a center-based hashing branch by
transferring knowledge from a weaker pairwise-based branch. MLH consists of two
branches: a strong center-based branch and a weaker pairwise-based branch.
Through an iterative mutual learning process, the center-based branch leverages
local similarity cues learned by the pairwise-based branch. Furthermore,
inspired by the mixture-of-experts paradigm, we introduce a novel
mixture-of-hash-experts module that enables effective cross-branch interaction,
further enhancing the performance of both branches. Extensive experiments
demonstrate that MLH consistently outperforms state-of-the-art hashing methods
across multiple benchmark datasets.

</details>


### [22] [RePainter: Empowering E-commerce Object Removal via Spatial-matting Reinforcement Learning](https://arxiv.org/abs/2510.07721)
*Zipeng Guo,Lichen Ma,Xiaolong Fu,Gaojing Zhou,Lan Yang,Yuchen Zhou,Linkai Liu,Yu He,Ximan Liu,Shiping Dong,Jingling Fu,Zhen Chen,Yu Shi,Junshi Huang,Jason Li,Chao Gou*

Main category: cs.CV

TL;DR: 本文提出了一种名为Repainter的强化学习框架，专注于去除电商产品图像中的水印和广告文本，并显著提升了图像修复效果。


<details>
  <summary>Details</summary>
Motivation: 在电商平台上，清晰美观的产品图片对于提升用户参与度和广告效果至关重要，然而水印和促销文本干扰了图片质量，主流修复方法在实际商业环境中表现有限。

Method: 提出Repainter框架，将空间抠像轨迹优化与Group Relative Policy Optimization（GRPO，群体相对策略优化）融合，通过强化学习调节注意力机制，重点增强背景语境，并采用复合奖励机制平衡全局、局部和语义约束，减少伪影和奖励欺骗。同时，构建了大规模高质量电商修复数据集EcomPaint-100K，以及标准化评测基准EcomPaint-Bench。

Result: 实验表明，Repainter在复杂场景下的修复效果显著优于当前主流方法，特别是在细致复杂背景下优势更大。

Conclusion: Repainter方法有效改进了电商产品图像修复的质量，为实际商业场景提供了可行的技术方案，并带来了标准化数据集和评测基准，有望推进该领域研究。

Abstract: In web data, product images are central to boosting user engagement and
advertising efficacy on e-commerce platforms, yet the intrusive elements such
as watermarks and promotional text remain major obstacles to delivering clear
and appealing product visuals. Although diffusion-based inpainting methods have
advanced, they still face challenges in commercial settings due to unreliable
object removal and limited domain-specific adaptation. To tackle these
challenges, we propose Repainter, a reinforcement learning framework that
integrates spatial-matting trajectory refinement with Group Relative Policy
Optimization (GRPO). Our approach modulates attention mechanisms to emphasize
background context, generating higher-reward samples and reducing unwanted
object insertion. We also introduce a composite reward mechanism that balances
global, local, and semantic constraints, effectively reducing visual artifacts
and reward hacking. Additionally, we contribute EcomPaint-100K, a high-quality,
large-scale e-commerce inpainting dataset, and a standardized benchmark
EcomPaint-Bench for fair evaluation. Extensive experiments demonstrate that
Repainter significantly outperforms state-of-the-art methods, especially in
challenging scenes with intricate compositions. We will release our code and
weights upon acceptance.

</details>


### [23] [SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction](https://arxiv.org/abs/2510.07723)
*Wenyue Chen,Peng Li,Wangguandong Zheng,Chengfeng Zhao,Mengfei Li,Yaolong Zhu,Zhiyang Dou,Ronggang Wang,Yuan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为SyncHuman的新方法，通过结合2D多视图生成模型和3D原生生成模型，实现了从单张图片重建高质量的3D全身人体模型，并在复杂姿态下表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单张图片重建3D人体时，因SMPL网格估计的3D先验不准确，难以应对复杂姿态以及恢复细节，且2D和3D生成方法各有局限。

Method: 提出将2D多视图生成模型（擅长捕捉2D细节）和3D原生生成模型（保持3D结构性强）相结合。通过像素对齐的2D-3D同步注意力机制，实现3D形状与2D多视图图像的几何一致性，再用特征注入机制将2D细节提升到3D模型上。

Result: 实验显示，SyncHuman在几何准确性和视觉保真度上均优于现有基线方法，对复杂姿态也有鲁棒表现，生成照片级真实的3D人体。

Conclusion: SyncHuman为高质量3D人体重建提供了新思路，能在复杂场景下实现精确、真实的3D人体，具有广泛应用前景。

Abstract: Photorealistic 3D full-body human reconstruction from a single image is a
critical yet challenging task for applications in films and video games due to
inherent ambiguities and severe self-occlusions. While recent approaches
leverage SMPL estimation and SMPL-conditioned image generative models to
hallucinate novel views, they suffer from inaccurate 3D priors estimated from
SMPL meshes and have difficulty in handling difficult human poses and
reconstructing fine details. In this paper, we propose SyncHuman, a novel
framework that combines 2D multiview generative model and 3D native generative
model for the first time, enabling high-quality clothed human mesh
reconstruction from single-view images even under challenging human poses.
Multiview generative model excels at capturing fine 2D details but struggles
with structural consistency, whereas 3D native generative model generates
coarse yet structurally consistent 3D shapes. By integrating the complementary
strengths of these two approaches, we develop a more effective generation
framework. Specifically, we first jointly fine-tune the multiview generative
model and the 3D native generative model with proposed pixel-aligned 2D-3D
synchronization attention to produce geometrically aligned 3D shapes and 2D
multiview images. To further improve details, we introduce a feature injection
mechanism that lifts fine details from 2D multiview images onto the aligned 3D
shapes, enabling accurate and high-fidelity reconstruction. Extensive
experiments demonstrate that SyncHuman achieves robust and photo-realistic 3D
human reconstruction, even for images with challenging poses. Our method
outperforms baseline methods in geometric accuracy and visual fidelity,
demonstrating a promising direction for future 3D generation models.

</details>


### [24] [ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes](https://arxiv.org/abs/2510.07729)
*Jian Gao,Mengqi Yuan,Yifei Zeng,Chang Zeng,Zhihao Li,Zhenyu Chen,Weichao Qiu,Xiao-Xiao Long,Hao Zhu,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: 本文提出了一种高效的3D对象与场景合成方法——ComGS，通过Surface Octahedral Probes和简化的环境光重建，实现了实时且高质量的渲染，大幅提升了实际应用中的效率与一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的Gaussian Splatting渲染方法中，对象和场景的重光照与合成存在不一致问题，尤其是在阴影和外观信息烘焙后难以编辑和复用。现有方法效率低或对复杂光照场景表现不佳，限制了3D场景编辑和创作的实际应用。

Method: 1. 提出Surface Octahedral Probes（SOPs）以高效存储和查询光照、遮挡信息，避免了传统耗时的射线追踪。2. 简化环境光照估计，只关注对象放置处的局部环境，在局部重建360度辐射场，并用扩散模型补全光照，从而提高复杂场景下的光照一致性。3. 基于以上改进，构建ComGS系统，实现高质量、实时的3D对象场景合成。

Result: 实验证明，SOP方法在重建效率上至少提升2倍，实现了实时阴影计算。ComGS系统最终可达到约28 FPS的实时渲染速度，合成效果自然、阴影逼真，编辑一次仅需36秒，性能和视觉效果优于现有方法。

Conclusion: ComGS在3D对象与场景合成领域取得了高效、精确的进展，特别适合需要实时编辑和自然渲染的实际应用。该框架为未来更真实的虚拟场景制作提供了新的技术方案。

Abstract: Gaussian Splatting (GS) enables immersive rendering, but realistic 3D
object-scene composition remains challenging. Baked appearance and shadow
information in GS radiance fields cause inconsistencies when combining objects
and scenes. Addressing this requires relightable object reconstruction and
scene lighting estimation. For relightable object reconstruction, existing
Gaussian-based inverse rendering methods often rely on ray tracing, leading to
low efficiency. We introduce Surface Octahedral Probes (SOPs), which store
lighting and occlusion information and allow efficient 3D querying via
interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x
speedup in reconstruction and enable real-time shadow computation in Gaussian
scenes. For lighting estimation, existing Gaussian-based inverse rendering
methods struggle to model intricate light transport and often fail in complex
scenes, while learning-based methods predict lighting from a single image and
are viewpoint-sensitive. We observe that 3D object-scene composition primarily
concerns the object's appearance and nearby shadows. Thus, we simplify the
challenging task of full scene lighting estimation by focusing on the
environment lighting at the object's placement. Specifically, we capture a 360
degrees reconstructed radiance field of the scene at the location and fine-tune
a diffusion model to complete the lighting. Building on these advances, we
propose ComGS, a novel 3D object-scene composition framework. Our method
achieves high-quality, real-time rendering at around 28 FPS, produces visually
harmonious results with vivid shadows, and requires only 36 seconds for
editing. Code and dataset are available at
https://nju-3dv.github.io/projects/ComGS/.

</details>


### [25] [UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes](https://arxiv.org/abs/2510.07741)
*Yuang Meng,Xin Jin,Lina Lei,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

TL;DR: 本论文提出UltraLED，一个基于单帧短曝光RAW图像的超高动态范围（UHDR）重建方法，显著提升暗部信息恢复，避免运动伪影并超越现有方法。


<details>
  <summary>Details</summary>
Motivation: UHDR场景中，亮部与暗部的曝光悬殊导致难以同时保留高光与阴影细节；现有RGB多帧曝光拼接方法易产生错位和鬼影。短曝光RAW图像虽然细节保留有限，但其高比特深度与可预测噪声特性，对恢复暗部信息有优势。作者希望探索能否仅用一帧短曝光RAW图像实现UHDR重建，既简化流程又增强动态场景下的鲁棒性。

Method: 提出UltraLED双阶段框架：第一步利用Ratio Map进行曝光校正，均衡动态范围；第二步采用亮度感知RAW去噪，有效恢复暗部细节。同时，作者搭建9档曝光合成管线，生成真实感UHDR数据集，仅以最短曝光帧作为重建输入。

Result: UltraLED在多组实验中显著优于现有单帧方案，在暗部细节恢复和抗运动伪影表现突出。提出的数据集和代码已公开，验证其实用性和通用性。

Conclusion: UltraLED框架证明单帧短曝光RAW图像即可实现高质量UHDR重建，有效解决动态场景下的运动伪影与暗部恢复难题，推动了单帧HDR技术发展。

Abstract: Ultra-high dynamic range (UHDR) scenes exhibit significant exposure
disparities between bright and dark regions. Such conditions are commonly
encountered in nighttime scenes with light sources. Even with standard exposure
settings, a bimodal intensity distribution with boundary peaks often emerges,
making it difficult to preserve both highlight and shadow details
simultaneously. RGB-based bracketing methods can capture details at both ends
using short-long exposure pairs, but are susceptible to misalignment and
ghosting artifacts. We found that a short-exposure image already retains
sufficient highlight detail. The main challenge of UHDR reconstruction lies in
denoising and recovering information in dark regions. In comparison to the RGB
images, RAW images, thanks to their higher bit depth and more predictable noise
characteristics, offer greater potential for addressing this challenge. This
raises a key question: can we learn to see everything in UHDR scenes using only
a single short-exposure RAW image? In this study, we rely solely on a single
short-exposure frame, which inherently avoids ghosting and motion blur, making
it particularly robust in dynamic scenes. To achieve that, we introduce
UltraLED, a two-stage framework that performs exposure correction via a ratio
map to balance dynamic range, followed by a brightness-aware RAW denoiser to
enhance detail recovery in dark regions. To support this setting, we design a
9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a
corresponding dataset based on diverse scenes, using only the shortest exposure
as input for reconstruction. Extensive experiments show that UltraLED
significantly outperforms existing single-frame approaches. Our code and
dataset are made publicly available at
https://srameo.github.io/projects/ultraled.

</details>


### [26] [DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream](https://arxiv.org/abs/2510.07752)
*Junhao He,Jiaxu Wang,Jia Li,Mingyuan Sun,Qiang Zhang,Jiahang Cao,Ziyi Zhang,Yi Gu,Jingkai Sun,Renjing Xu*

Main category: cs.CV

TL;DR: 本文提出了一种结合低帧率RGB视频与高帧率事件相机数据的方法，实现动态三维高斯泼溅（3DGS）重建，显著提升了动态场景下的三维重建效果。


<details>
  <summary>Details</summary>
Motivation: 仅用低帧率RGB视频进行动态3DGS重建时，由于帧间大运动导致的不确定性，使得重建精度降低。事件相机虽可以捕捉快速运动，但缺乏色彩信息。因此，如何联合RGB与事件数据、克服两者模态差异，实现高质量的动态3DGS，是本研究关注的核心问题。

Method: 提出一种新框架，利用事件流中的运动先验引导形变场优化。具体包括：1）采用无监督LoCM框架微调事件流估计器适配新场景，提取事件流中的运动先验；2）提出几何感知的数据关联方法建立事件与高斯运动的对应关系，并采用运动分解、帧间伪标签等策略优化整体管线。

Result: 在多个合成和真实场景中进行实验，结果表明，该方法在重建精度和动态表现上均优于现有的基于图像或事件的同类方法。

Conclusion: 结合事件数据与RGB图像能够有效提升动态3DGS的重建质量。本方法通过引入事件运动先验成功克服了多模态数据差异，为动态场景三维重建提供了新思路。

Abstract: Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB
videos is challenging. This is because large inter-frame motions will increase
the uncertainty of the solution space. For example, one pixel in the first
frame might have more choices to reach the corresponding pixel in the second
frame. Event cameras can asynchronously capture rapid visual changes and are
robust to motion blur, but they do not provide color information. Intuitively,
the event stream can provide deterministic constraints for the inter-frame
large motion by the event trajectories. Hence, combining
low-temporal-resolution images with high-framerate event streams can address
this challenge. However, it is challenging to jointly optimize Dynamic 3DGS
using both RGB and event modalities due to the significant discrepancy between
these two data modalities. This paper introduces a novel framework that jointly
optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event
motion priors to guide the optimization of the deformation fields. First, we
extract the motion priors encoded in event streams by using the proposed LoCM
unsupervised fine-tuning framework to adapt an event flow estimator to a
certain unseen scene. Then, we present the geometry-aware data association
method to build the event-Gaussian motion correspondence, which is the primary
foundation of the pipeline, accompanied by two useful strategies, namely motion
decomposition and inter-frame pseudo-label. Extensive experiments show that our
method outperforms existing image and event-based approaches across synthetic
and real scenes and prove that our method can effectively optimize dynamic 3DGS
with the help of event data.

</details>


### [27] [Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets and Explainable AI (XAI): A Comparative Analysis](https://arxiv.org/abs/2510.07785)
*Ming Jie Ong,Sze Yinn Ung,Sim Kuan Goh,Jimmy Y. Zhong*

Main category: cs.CV

TL;DR: 本研究比较了三种UNet模型（UNet、ResUNet、AttUNet）在MRI脑肿瘤分割中的表现，引入可解释人工智能（XAI）方法（Grad-CAM和注意力可视化），结果ResUNet分割效果最佳，并推荐用于临床自动化分割。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在MRI脑肿瘤分割取得显著进步，但模型决策过程不透明妨碍其在临床中的接受和信任。本研究旨在通过XAI提升医生对AI模型的信任，并优化分割效果。

Method: 本研究以BraTS2020公开数据集为基准，采用Adam优化器分别训练UNet、ResUNet和AttUNet三种神经网络。结合Grad-CAM和注意力机制做可视化解释，对比三者训练、验证和推断时间、分割相似系数及分类指标。

Result: ResUNet在最终测试阶段显著优于UNet和AttUNet，取得最高的Dice和Jaccard分数、准确率、召回率和F1值。Grad-CAM和注意力可视化揭示了模型关注的肿瘤区域和注意力机制工作原理。

Conclusion: ResUNet结合XAI技术在MRI脑肿瘤分割任务中表现最佳，推荐其在今后临床自动脑肿瘤分割场景应用。相关源码和权重已开源。

Abstract: The current study investigated the use of Explainable Artificial Intelligence
(XAI) to improve the accuracy of brain tumor segmentation in MRI images, with
the goal of assisting physicians in clinical decision-making. The study focused
on applying UNet models for brain tumor segmentation and using the XAI
techniques of Gradient-weighted Class Activation Mapping (Grad-CAM) and
attention-based visualization to enhance the understanding of these models.
Three deep learning models - UNet, Residual UNet (ResUNet), and Attention UNet
(AttUNet) - were evaluated to identify the best-performing model. XAI was
employed with the aims of clarifying model decisions and increasing physicians'
trust in these models. We compared the performance of two UNet variants
(ResUNet and AttUNet) with the conventional UNet in segmenting brain tumors
from the BraTS2020 public dataset and analyzed model predictions with Grad-CAM
and attention-based visualization. Using the latest computer hardware, we
trained and validated each model using the Adam optimizer and assessed their
performance with respect to: (i) training, validation, and inference times,
(ii) segmentation similarity coefficients and loss functions, and (iii)
classification performance. Notably, during the final testing phase, ResUNet
outperformed the other models with respect to Dice and Jaccard similarity
scores, as well as accuracy, recall, and F1 scores. Grad-CAM provided
visuospatial insights into the tumor subregions each UNet model focused on
while attention-based visualization provided valuable insights into the working
mechanisms of AttUNet's attention modules. These results demonstrated ResUNet
as the best-performing model and we conclude by recommending its use for
automated brain tumor segmentation in future clinical assessments. Our source
code and checkpoint are available at
https://github.com/ethanong98/MultiModel-XAI-Brats2020

</details>


### [28] [GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.07791)
*Qinghongbing Xie,Zhaoyuan Xia,Feng Zhu,Lijun Gong,Ziyue Li,Rui Zhao,Long Zeng*

Main category: cs.CV

TL;DR: 本文提出了GTR-Bench，一个用于大规模摄像头网络中地理时空推理的新基准，用来评估视觉-语言模型（VLMs）在地图与视频多视角下的推理能力，实验表明主流VLM在此任务上距离人类水平差距较大。


<details>
  <summary>Details</summary>
Motivation: 现有空间-时空智能的评测主要局限于主观视角下的视频或地图等单一上下文，无法全面评估VLMs在结合地理地图和视频等异构信息源时的推理能力，而这对于自动驾驶、交通管理和应急响应等领域意义重大。

Method: 作者设计了GTR-Bench，该基准涉及从地图与多路视频（视角不重叠）中推理动态目标的地理和时间行为，需要模型能切换视角、跨多视图联推、对未直接观察区域作出时空推理。

Result: 超过10种主流视觉-语言模型在GTR-Bench上表现均不理想，即使表现最好的Gemini-2.5-Pro模型准确率也仅为34.9%，远低于人类的78.61%。深入分析发现模型在时空上下文利用、时间预测和地图与视频对齐三方面均存在显著不足。

Conclusion: GTR-Bench有效揭示了现有VLMs在地理时空推理领域的不足，为未来提升模型的多模态、异构数据综合推理能力和实际应用提供了研究方向和挑战。

Abstract: Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has
attracted much attention due to its importance for Autonomous Driving, Embodied
AI and General Artificial Intelligence. Existing spatial-temporal benchmarks
mainly focus on egocentric perspective reasoning with images/video context, or
geographic perspective reasoning with graphics context (eg. a map), thus fail
to assess VLMs' geographic spatial-temporal intelligence with both images/video
and graphics context, which is important for areas like traffic management and
emergency response. To address the gaps, we introduce Geo-Temporal Reasoning
benchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of
moving targets in a large-scale camera network. GTR-Bench is more challenging
as it requires multiple perspective switches between maps and videos, joint
reasoning across multiple videos with non-overlapping fields of view, and
inference over spatial-temporal regions that are unobserved by any video
context. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that
even the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags
behind human performance (78.61%) on geo-temporal reasoning. Moreover, our
comprehensive analysis on GTR-Bench reveals three primary deficiencies of
current models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by
an imbalanced utilization of spatial-temporal context. (2) VLMs are weak in
temporal forecasting, which leads to worse performance on temporal-emphasized
tasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to
comprehend or align the map data with multi-view video inputs. We believe
GTR-Bench offers valuable insights and opens up new opportunities for research
and applications in spatial-temporal intelligence. Benchmark and code will be
released at https://github.com/X-Luffy/GTR-Bench.

</details>


### [29] [FMANet: A Novel Dual-Phase Optical Flow Approach with Fusion Motion Attention Network for Robust Micro-expression Recognition](https://arxiv.org/abs/2510.07810)
*Luu Tu Nguyen,Vu Tram Anh Khuong,Thi Bich Phuong Man,Thi Duyen Ngo,Thanh Ha Le*

Main category: cs.CV

TL;DR: 本论文提出了一种新的面部微表情识别方法，通过结合微表情全过程动态信息，提升了识别准确率，超过了现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 面部微表情能反映真实情感，但因变化细微且持续时间短，识别难度大。现有方法多只关注onset到apex阶段，忽略了apex到offset过程的重要运动信息，影响了识别效果。

Method: 提出了“幅度调制的联合光流（MM-COF）”运动表示，将微表情全过程（onset到apex，再到offset）的运动特征融合，整体表达面部微小运动；并基于此设计了新型端到端神经网络架构FMANet，实现对双阶段运动信息的自适应融合与关键区域关注。

Result: 在MMEW、SMIC、CASME-II和SAMM四大权威微表情数据库上，MM-COF表示和FMANet模型均取得了优于现有方法的识别效果，展现出明显性能提升。

Conclusion: 引入可学习的、整合全过程动态信息的双阶段识别框架，能够显著提升微表情识别能力。该方法有助于推动微表情自动识别技术的发展。

Abstract: Facial micro-expressions, characterized by their subtle and brief nature, are
valuable indicators of genuine emotions. Despite their significance in
psychology, security, and behavioral analysis, micro-expression recognition
remains challenging due to the difficulty of capturing subtle facial movements.
Optical flow has been widely employed as an input modality for this task due to
its effectiveness. However, most existing methods compute optical flow only
between the onset and apex frames, thereby overlooking essential motion
information in the apex-to-offset phase. To address this limitation, we first
introduce a comprehensive motion representation, termed Magnitude-Modulated
Combined Optical Flow (MM-COF), which integrates motion dynamics from both
micro-expression phases into a unified descriptor suitable for direct use in
recognition networks. Building upon this principle, we then propose FMANet, a
novel end-to-end neural network architecture that internalizes the dual-phase
analysis and magnitude modulation into learnable modules. This allows the
network to adaptively fuse motion cues and focus on salient facial regions for
classification. Experimental evaluations on the MMEW, SMIC, CASME-II, and SAMM
datasets, widely recognized as standard benchmarks, demonstrate that our
proposed MM-COF representation and FMANet outperforms existing methods,
underscoring the potential of a learnable, dual-phase framework in advancing
micro-expression recognition.

</details>


### [30] [An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images](https://arxiv.org/abs/2510.07817)
*Kanglin Ning,Ruzhao Chen,Penghong Wang,Xingtao Wang,Ruiqin Xiong,Xiaopeng Fan*

Main category: cs.CV

TL;DR: 本文提出了一种结合房间几何信息的单目全景深度估计算法，有效提升了房间转角的深度还原和抗噪能力，在多个公开数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 传统单目全景深度估计方法仅关注像素级准确性，导致房间转角过度平滑且对噪声敏感，影响实际室内场景应用。因而需要引入结构信息以增强深度预测准确性。

Method: 该方法以房间几何约束为核心，利用布局预测提取房间几何信息，并通过背景分割机制，将其整合进深度估计流程。框架结构包括一个共享特征编码器和任务专属解码器（分别对应布局估计、深度估计和背景分割），并引入了基于房间几何的背景深度解析和基于背景分割的融合策略，实现多源信息有效融合。

Result: 在Stanford2D3D、Matterport3D和Structured3D等数据集上，实验结果显示该方法显著优于当前开源深度估计方法，具有更好的深度还原效果和噪声鲁棒性。

Conclusion: 引入房间几何约束与分割机制能显著提升单目室内全景深度估计质量，特别是在结构还原和噪声抑制方面，具备较大实际应用潜力。

Abstract: Predicting spherical pixel depth from monocular $360^{\circ}$ indoor
panoramas is critical for many vision applications. However, existing methods
focus on pixel-level accuracy, causing oversmoothed room corners and noise
sensitivity. In this paper, we propose a depth estimation framework based on
room geometry constraints, which extracts room geometry information through
layout prediction and integrates those information into the depth estimation
process through background segmentation mechanism. At the model level, our
framework comprises a shared feature encoder followed by task-specific decoders
for layout estimation, depth estimation, and background segmentation. The
shared encoder extracts multi-scale features, which are subsequently processed
by individual decoders to generate initial predictions: a depth map, a room
layout map, and a background segmentation map. Furthermore, our framework
incorporates two strategies: a room geometry-based background depth resolving
strategy and a background-segmentation-guided fusion mechanism. The proposed
room-geometry-based background depth resolving strategy leverages the room
layout and the depth decoder's output to generate the corresponding background
depth map. Then, a background-segmentation-guided fusion strategy derives
fusion weights for the background and coarse depth maps from the segmentation
decoder's predictions. Extensive experimental results on the Stanford2D3D,
Matterport3D and Structured3D datasets show that our proposed methods can
achieve significantly superior performance than current open-source methods.
Our code is available at https://github.com/emiyaning/RGCNet.

</details>


### [31] [Enhancing Visual Prompting through Expanded Transformation Space and Overfitting Mitigation](https://arxiv.org/abs/2510.07823)
*Shohei Enomoto*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉提示（Visual Prompting, VP）方法ACAVP，通过引入仿射、颜色和加性变换，提高了提示的表达能力，并通过数据增强缓解过拟合，显著提升了下游任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉提示方法虽然高效，但存在表达能力有限和易过拟合的问题，导致在准确率上落后于其他微调方法。本文旨在解决这些核心瓶颈。

Method: 提出ACAVP方法，结合仿射变换（增加任务特异性区域）、颜色变换（突出任务相关特征）和加性变换，并引入TrivialAugment数据增强技术，有效缓解过拟合。

Result: ACAVP在12个图像分类数据集和2种模型架构上的实验结果表明，在VP方法中达到最新最好成绩，平均准确率超过线性探针方法，并且在应对分布偏移时表现出更强的鲁棒性，且推理阶段几乎无额外计算开销。

Conclusion: ACAVP大幅提高了视觉提示方法的表达能力和泛化能力，借助适当的数据增强，实现了准确率和鲁棒性的双提升，验证了数据增强对VP训练的普遍益处。

Abstract: Visual prompting (VP) has emerged as a promising parameter-efficient
fine-tuning approach for adapting pre-trained vision models to downstream tasks
without modifying model parameters. Despite offering advantages like negligible
computational overhead and compatibility with black-box models, conventional VP
methods typically achieve lower accuracy than other adaptation approaches. Our
analysis reveals two critical limitations: the restricted expressivity of
simple additive transformation and a tendency toward overfitting when the
parameter count increases. To address these challenges, we propose ACAVP
(Affine, Color, and Additive Visual Prompting), which enhances VP's expressive
power by introducing complementary transformation operations: affine
transformation for creating task-specific prompt regions while preserving
original image information, and color transformation for emphasizing
task-relevant visual features. Additionally, we identify that overfitting is a
critical issue in VP training and introduce TrivialAugment as an effective data
augmentation, which not only benefits our approach but also significantly
improves existing VP methods, with performance gains of up to 12 percentage
points on certain datasets. This demonstrates that appropriate data
augmentation is universally beneficial for VP training. Extensive experiments
across twelve diverse image classification datasets with two different model
architectures demonstrate that ACAVP achieves state-of-the-art accuracy among
VP methods, surpasses linear probing in average accuracy, and exhibits superior
robustness to distribution shifts, all while maintaining minimal computational
overhead during inference.

</details>


### [32] [MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions](https://arxiv.org/abs/2510.07828)
*Kaen Kogashi,Anoop Cherian,Meng-Yu Jennifer Kuo*

Main category: cs.CV

TL;DR: 该论文提出了MMHOI数据集，并基于此开发了MMHOI-Net模型，实现了多人体多物体3D交互的高精度识别和重建。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体-物体交互（HOI）数据集对真实世界中的复杂多角色交互建模有限，无法支持全面的多人体多物体协作与因果推理研究。为此，作者希望填补这方面的空白，推动下一代HOI模型的发展。

Method: 作者构建了大型多人体多物体交互数据集MMHOI，涵盖12种日常场景，详细标注了每个人和物体的3D形状、姿态、78类动作及14类特定身体部位。基于此，提出了端到端的transformer框架MMHOI-Net，采用结构化双patch机制对物体及其交互进行建模，并结合动作识别提升交互判断。

Result: 在MMHOI和CORE4D两个数据集上，MMHOI-Net在多HOI建模方面均达到了当前最优的准确率和3D重建质量。

Conclusion: MMHOI数据集和MMHOI-Net模型为复杂场景下多人体-物体3D交互建模提供了强大工具，将推动HOI领域向更真实、更复杂的场景发展。

Abstract: Real-world scenes often feature multiple humans interacting with multiple
objects in ways that are causal, goal-oriented, or cooperative. Yet existing 3D
human-object interaction (HOI) benchmarks consider only a fraction of these
complex interactions. To close this gap, we present MMHOI -- a large-scale,
Multi-human Multi-object Interaction dataset consisting of images from 12
everyday scenarios. MMHOI offers complete 3D shape and pose annotations for
every person and object, along with labels for 78 action categories and 14
interaction-specific body parts, providing a comprehensive testbed for
next-generation HOI research. Building on MMHOI, we present MMHOI-Net, an
end-to-end transformer-based neural network for jointly estimating human-object
3D geometries, their interactions, and associated actions. A key innovation in
our framework is a structured dual-patch representation for modeling objects
and their interactions, combined with action recognition to enhance the
interaction prediction. Experiments on MMHOI and the recently proposed CORE4D
datasets demonstrate that our approach achieves state-of-the-art performance in
multi-HOI modeling, excelling in both accuracy and reconstruction quality.

</details>


### [33] [PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting](https://arxiv.org/abs/2510.07830)
*Houqiang Zhong,Zhenglong Wu,Sihua Fu,Zihan Zheng,Xin Jin,Xiaoyun Zhang,Li Song,Qiang Hu*

Main category: cs.CV

TL;DR: PrismGS提升了3D Gaussian Splatting在大规模城市场景下的渲染质量，显著缓解了别名、闪烁和锯齿问题，实现了更高保真度的4K实时渲染。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting虽能在小场景中实现实时高质量渲染，但在大规模城市环境下会出现明显的别名、闪烁纹理和锯齿问题，现有方法无法有效解决渲染保真度下降。因此，作者旨在提升3DGS在大场景高分辨率应用下的表现。

Method: 提出了PrismGS框架，包含两个正则化措施：（1）金字塔多尺度监督，将渲染结果与多尺度预滤图像进行对齐，从而强制模型学习自带抗锯齿特性的表示，缓解纹理闪烁；（2）显式尺寸正则化，给3D高斯原语尺寸加物理下限，防止出现依赖视角的不良几何体，减少锯齿、保证稳定性。该方法可直接插入现有流程。

Result: 在MatrixCity、Mill-19和UrbanScene3D等大规模数据集上实验，PrismGS的PSNR较主流方法CityGaussian提升约1.5 dB，并在4K渲染下保持高画质和鲁棒性。

Conclusion: PrismGS有效提升了3D高斯点在复杂大场景下的渲染质量，消除了诸如闪烁、锯齿等伪影，为高分辨率、实时大场景渲染提供了新思路。

Abstract: 3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic
rendering in compact scenes, but scaling to large urban environments introduces
severe aliasing artifacts and optimization instability, especially under
high-resolution (e.g., 4K) rendering. These artifacts, manifesting as
flickering textures and jagged edges, arise from the mismatch between Gaussian
primitives and the multi-scale nature of urban geometry. While existing
``divide-and-conquer'' pipelines address scalability, they fail to resolve this
fidelity gap. In this paper, we propose PrismGS, a physically-grounded
regularization framework that improves the intrinsic rendering behavior of 3D
Gaussians. PrismGS integrates two synergistic regularizers. The first is
pyramidal multi-scale supervision, which enforces consistency by supervising
the rendering against a pre-filtered image pyramid. This compels the model to
learn an inherently anti-aliased representation that remains coherent across
different viewing scales, directly mitigating flickering textures. This is
complemented by an explicit size regularization that imposes a
physically-grounded lower bound on the dimensions of the 3D Gaussians. This
prevents the formation of degenerate, view-dependent primitives, leading to
more stable and plausible geometric surfaces and reducing jagged edges. Our
method is plug-and-play and compatible with existing pipelines. Extensive
experiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS
achieves state-of-the-art performance, yielding significant PSNR gains around
1.5 dB against CityGaussian, while maintaining its superior quality and
robustness under demanding 4K rendering.

</details>


### [34] [IsoSignVid2Aud: Sign Language Video to Audio Conversion without Text Intermediaries](https://arxiv.org/abs/2510.07837)
*Harsh Kavediya,Vighnesh Nayak,Bheeshm Sharma,Balamurugan Palaniappan*

Main category: cs.CV

TL;DR: 提出了一种将手语视频直接翻译为语音的新方法，无需中间文本步骤，提升了交流效率。


<details>
  <summary>Details</summary>
Motivation: 现有手语到语音的翻译方法通常需要将手语先转为文本再转为语音，导致延迟和多阶段误差积累。此外，针对非语法性连续手语符号的翻译需求日益增加，特别是在教育场景和手语提示接口等应用中。

Method: 设计了名为IsoSignVid2Aud的端到端翻译框架，将I3D特征提取、特定特征变换网络和音频生成管道整合到一起。引入了一种新颖的非极大值抑制（NMS）算法用于从非语法性连续手语序列中进行时域符号检测，无需中间文本表示。

Result: 在ASL-Citizen-1500和WLASL-100数据集上，方法获得了Top-1准确率分别为72.01%和78.67%；语音质量指标PESQ为2.67，STOI为0.73，证明输出语音具备可懂度和实用性。

Conclusion: IsoSignVid2Aud显著提升了手语直译为语音的效率与准确性，尤其适用于非语法连续符号序列，展示了其在即时沟通、教育等多种应用场景的潜力。

Abstract: Sign language to spoken language audio translation is important to connect
the hearing- and speech-challenged humans with others. We consider sign
language videos with isolated sign sequences rather than continuous grammatical
signing. Such videos are useful in educational applications and sign prompt
interfaces. Towards this, we propose IsoSignVid2Aud, a novel end-to-end
framework that translates sign language videos with a sequence of possibly
non-grammatic continuous signs to speech without requiring intermediate text
representation, providing immediate communication benefits while avoiding the
latency and cascading errors inherent in multi-stage translation systems. Our
approach combines an I3D-based feature extraction module with a specialized
feature transformation network and an audio generation pipeline, utilizing a
novel Non-Maximal Suppression (NMS) algorithm for the temporal detection of
signs in non-grammatic continuous sequences. Experimental results demonstrate
competitive performance on ASL-Citizen-1500 and WLASL-100 datasets with Top-1
accuracies of 72.01\% and 78.67\%, respectively, and audio quality metrics
(PESQ: 2.67, STOI: 0.73) indicating intelligible speech output. Code is
available at: https://github.com/BheeshmSharma/IsoSignVid2Aud_AIMLsystems-2025.

</details>


### [35] [AlignGS: Aligning Geometry and Semantics for Robust Indoor Reconstruction from Sparse Views](https://arxiv.org/abs/2510.07839)
*Yijie Gao,Houqiang Zhong,Tianchi Zhu,Zhengxue Cheng,Qiang Hu,Li Song*

Main category: cs.CV

TL;DR: 提出了AlignGS框架，实现几何与语义协同优化，有效提升了稀疏视角下的3D重建质量。


<details>
  <summary>Details</summary>
Motivation: 随着增强现实、虚拟现实和机器人领域的发展，对室内场景的高语义三维模型需求增加。然而，稀疏视角下重建3D模型存在几何歧义，且以往方法常把语义作为被动附加信息，导致重建效果有限。

Method: 该文提出AlignGS框架，创新性地将2D基础模型中提取的丰富语义先验引入3D重建过程中，通过语义引导几何优化，例如引入深度一致性和法线正则化，端到端联合优化语义与几何。

Result: 在多个基准数据集的实验中，AlignGS在新视角合成和重建几何精度上均优于现有方法，取得了先进水平。

Conclusion: 将语义先验作为几何正则器，能在稀疏输入条件下获得更完整和一致的3D重建结果。

Abstract: The demand for semantically rich 3D models of indoor scenes is rapidly
growing, driven by applications in augmented reality, virtual reality, and
robotics. However, creating them from sparse views remains a challenge due to
geometric ambiguity. Existing methods often treat semantics as a passive
feature painted on an already-formed, and potentially flawed, geometry. We
posit that for robust sparse-view reconstruction, semantic understanding
instead be an active, guiding force. This paper introduces AlignGS, a novel
framework that actualizes this vision by pioneering a synergistic, end-to-end
optimization of geometry and semantics. Our method distills rich priors from 2D
foundation models and uses them to directly regularize the 3D representation
through a set of novel semantic-to-geometry guidance mechanisms, including
depth consistency and multi-faceted normal regularization. Extensive
evaluations on standard benchmarks demonstrate that our approach achieves
state-of-the-art results in novel view synthesis and produces reconstructions
with superior geometric accuracy. The results validate that leveraging semantic
priors as a geometric regularizer leads to more coherent and complete 3D models
from limited input views. Our code is avaliable at
https://github.com/MediaX-SJTU/AlignGS .

</details>


### [36] [A Multimodal Depth-Aware Method For Embodied Reference Understanding](https://arxiv.org/abs/2510.08278)
*Fevziye Irem Eyiokur,Dogucan Yaman,Hazım Kemal Ekenel,Alexander Waibel*

Main category: cs.CV

TL;DR: 本文提出了一种结合语言描述与肢体指向线索，利用深度信息的新型目标指认方法，显著提升了复杂场景下的目标识别准确性。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇目标检测方法在多目标或场景歧义时性能不佳，难以准确利用多模态信号（如语言、指向、深度信息）消除歧义，从而影响人机互动等实际应用效果。

Method: 提出了新的ERU框架：联合了基于大语言模型的数据增强算法、深度图模态输入，以及深度感知决策模块。该方法可更好整合语言和肢体（指向）信息，通过深度信息提升对齐和区分能力。

Result: 在两个公开数据集上实验显示，该方法在指认目标的准确率和可靠性方面，均明显优于现有各类基线方法。

Conclusion: 通过引入深度信息与改进数据增强与决策机制，本文方案显著提升了在复杂、杂乱环境下依据多模态输入完成目标指认的能力，对增强人机交互等实际任务有积极意义。

Abstract: Embodied Reference Understanding requires identifying a target object in a
visual scene based on both language instructions and pointing cues. While prior
works have shown progress in open-vocabulary object detection, they often fail
in ambiguous scenarios where multiple candidate objects exist in the scene. To
address these challenges, we propose a novel ERU framework that jointly
leverages LLM-based data augmentation, depth-map modality, and a depth-aware
decision module. This design enables robust integration of linguistic and
embodied cues, improving disambiguation in complex or cluttered environments.
Experimental results on two datasets demonstrate that our approach
significantly outperforms existing baselines, achieving more accurate and
reliable referent detection.

</details>


### [37] [Self-Supervised Learning Strategies for a Platform to Test the Toxicity of New Chemicals and Materials](https://arxiv.org/abs/2510.07853)
*Thomas Lautenschlager,Nils Friederich,Angelo Jovin Yamachui Sitcheu,Katja Nau,Gaëlle Hayot,Thomas Dickmeis,Ralf Mikut*

Main category: cs.CV

TL;DR: 本论文探讨了通过自监督学习获得的表征是否能提高高通量毒性测试中毒性评估的准确性，并利用EmbryoNet斑马鱼胚胎数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 高通量毒性测试在新药和化学品筛选中越来越重要，需要自动化和高效的数据评估方法。传统机器学习依赖大量标签数据，而数据标注费时且昂贵，因此本论文尝试利用无需人工标注的自监督学习提升评估能力。

Method: 作者采用自监督学习方法，基于EmbryoNet公开数据集（包含十种因不同化合物诱导的斑马鱼胚胎表型），训练模型并提取表征，用以区分不同毒性诱发机制。随后评估模型区分不同‘作用模式’的能力。

Result: 实验结果表明，自监督学习得到的特征表征能够有效地区分由不同作用机制化合物诱发的胚胎表型，实现了对各类毒性反应的有效判别。

Conclusion: 本研究证明了自监督学习在高通量毒性测试自动化评估中的潜力，并提出在TOXBOX等实际检测设备中集成该类模型的可行性。

Abstract: High-throughput toxicity testing offers a fast and cost-effective way to test
large amounts of compounds. A key component for such systems is the automated
evaluation via machine learning models. In this paper, we address critical
challenges in this domain and demonstrate how representations learned via
self-supervised learning can effectively identify toxicant-induced changes. We
provide a proof-of-concept that utilizes the publicly available EmbryoNet
dataset, which contains ten zebrafish embryo phenotypes elicited by various
chemical compounds targeting different processes in early embryonic
development. Our analysis shows that the learned representations using
self-supervised learning are suitable for effectively distinguishing between
the modes-of-action of different compounds. Finally, we discuss the integration
of machine learning models in a physical toxicity testing device in the context
of the TOXBOX project.

</details>


### [38] [Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning](https://arxiv.org/abs/2510.08442)
*Andrew Lee,Ian Chuang,Dechen Gao,Kai Fukazawa,Iman Soltani*

Main category: cs.CV

TL;DR: 本文提出了一种基于“注视”（Gaze）注意力机制的视觉强化学习方法，以显著提升样本效率和学习稳定性。


<details>
  <summary>Details</summary>
Motivation: 在高维视觉输入下，只有少量像素与任务相关，强化学习智能体常在无关特征上浪费探索与计算资源，导致收敛慢、效率低。受人类视觉中心机制（foveation）启发，作者希望通过引导注意力聚焦于任务相关特征，提升效率。

Method: 提出Gaze on the Prize框架，将可学习foveal注意力机制（Gaze）与自监督信号（Prize）结合。具体做法为：利用回报差异作为监督信号，通过对比学习（返回导向型）构造正负样本三元组，引导注意力机制区分与任务成败相关的特征。整个机制无须修改底层RL算法或超参数。

Result: 在ManiSkill3基准的一系列操作任务中，该方法比基线最多提升2.4倍样本效率，并能解决基线无法学会的任务。

Conclusion: 在视觉RL任务中，引入基于回报差异的注意力机制能显著提升样本效率与学习效果，具有较强的通用性与实际应用价值。

Abstract: Visual Reinforcement Learning (RL) agents must learn to act based on
high-dimensional image data where only a small fraction of the pixels is
task-relevant. This forces agents to waste exploration and computational
resources on irrelevant features, leading to sample-inefficient and unstable
learning. To address this, inspired by human visual foveation, we introduce
Gaze on the Prize. This framework augments visual RL with a learnable foveal
attention mechanism (Gaze), guided by a self-supervised signal derived from the
agent's experience pursuing higher returns (the Prize). Our key insight is that
return differences reveal what matters most: If two similar representations
produce different outcomes, their distinguishing features are likely
task-relevant, and the gaze should focus on them accordingly. This is realized
through return-guided contrastive learning that trains the attention to
distinguish between the features relevant to success and failure. We group
similar visual representations into positives and negatives based on their
return differences and use the resulting labels to construct contrastive
triplets. These triplets provide the training signal that teaches the attention
mechanism to produce distinguishable representations for states associated with
different outcomes. Our method achieves up to 2.4x improvement in sample
efficiency and can solve tasks that the baseline fails to learn, demonstrated
across a suite of manipulation tasks from the ManiSkill3 benchmark, all without
modifying the underlying algorithm or hyperparameters.

</details>


### [39] [XYZCylinder: Feedforward Reconstruction for Driving Scenes Based on A Unified Cylinder Lifting Method](https://arxiv.org/abs/2510.07856)
*Haochen Yu,Qiankun Liu,Hongyuan Liu,Jianfei Jiang,Juntao Lyu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: 本文提出了一种称为XYZCylinder的前馈重建模型，通过统一的圆柱提升方法提升自动驾驶场景的重建泛化能力和精度。


<details>
  <summary>Details</summary>
Motivation: 当前feedforward重建范式在自动驾驶场景中受到两方面限制：一是固定视图变换对不同相机配置的适应能力差，影响泛化性；二是360度全景图的视角重叠区域小，加上场景复杂度高，导致重建精度下降。

Method: 作者提出XYZCylinder模型，包含两大创新点：一是统一圆柱相机建模（UCCM），通过可调参数统一不同相机配置，提高泛化能力；二是基于新设计的Cylinder Plane Feature Group (CPFG)模块，将2D特征提升到3D空间，并辅以混合表达方式提升重建精度。

Result: 实验结果表明，XYZCylinder在多种评测条件下均取得了最优表现，并能以零样本的方式泛化到其他驾驶场景。

Conclusion: XYZCylinder模型有效提升了自动驾驶场景中的重建泛化能力和准确性，对不同相机配置具有良好的适应性，具备实际应用潜力。

Abstract: Recently, more attention has been paid to feedforward reconstruction
paradigms, which mainly learn a fixed view transformation implicitly and
reconstruct the scene with a single representation. However, their
generalization capability and reconstruction accuracy are still limited while
reconstructing driving scenes, which results from two aspects: (1) The fixed
view transformation fails when the camera configuration changes, limiting the
generalization capability across different driving scenes equipped with
different camera configurations. (2) The small overlapping regions between
sparse views of the $360^\circ$ panorama and the complexity of driving scenes
increase the learning difficulty, reducing the reconstruction accuracy. To
handle these difficulties, we propose \textbf{XYZCylinder}, a feedforward model
based on a unified cylinder lifting method which involves camera modeling and
feature lifting. Specifically, to improve the generalization capability, we
design a Unified Cylinder Camera Modeling (UCCM) strategy, which avoids the
learning of viewpoint-dependent spatial correspondence and unifies different
camera configurations with adjustable parameters. To improve the reconstruction
accuracy, we propose a hybrid representation with several dedicated modules
based on newly designed Cylinder Plane Feature Group (CPFG) to lift 2D image
features to 3D space. Experimental results show that XYZCylinder achieves
state-of-the-art performance under different evaluation settings, and can be
generalized to other driving scenes in a zero-shot manner. Project page:
\href{https://yuyuyu223.github.io/XYZCYlinder-projectpage/}{here}.

</details>


### [40] [Have We Scene It All? Scene Graph-Aware Deep Point Cloud Compression](https://arxiv.org/abs/2510.08512)
*Nikolaos Stathoulopoulos,Christoforos Kanellakis,George Nikolakopoulos*

Main category: cs.CV

TL;DR: 本文提出了一种基于语义场景图的深度压缩框架，有效减少3D点云数据的传输负担，在大幅压缩数据的同时保持结构和语义信息，为多智能体机器人系统的感知与协作提供支撑。


<details>
  <summary>Details</summary>
Motivation: 3D点云数据在多智能体机器人中的高效传输尤为关键，但由于数据体积庞大且结构复杂，受带宽和连接不稳定的限制，容易影响系统整体性能。现有方法难以在高压缩比下兼顾结构和语义信息，因此亟需新的压缩方法。

Method: 方法将原始点云分解为语义一致的区块，通过语义感知编码器（采用特征线性调制FiLM）将区块编码为紧凑的潜在空间表示。解码阶段采用基于folding的结构，结合潜在特征和图节点属性以尽量还原原始结构。

Result: 在SemanticKITTI和nuScenes数据集上的实验表明，该框架可实现最高达98%的数据压缩率，并在保持点云结构和语义准确性的同时，有效支持多机器人姿势图优化及地图融合等应用，相关精度接近未压缩原始点云效果。

Conclusion: 提出的基于语义场景图的深度压缩方法显著提升了点云数据传输效率，兼顾压缩率与结构、语义保真，适用于带宽受限条件下的多机器人系统，具有较强的应用前景。

Abstract: Efficient transmission of 3D point cloud data is critical for advanced
perception in centralized and decentralized multi-agent robotic systems,
especially nowadays with the growing reliance on edge and cloud-based
processing. However, the large and complex nature of point clouds creates
challenges under bandwidth constraints and intermittent connectivity, often
degrading system performance. We propose a deep compression framework based on
semantic scene graphs. The method decomposes point clouds into semantically
coherent patches and encodes them into compact latent representations with
semantic-aware encoders conditioned by Feature-wise Linear Modulation (FiLM). A
folding-based decoder, guided by latent features and graph node attributes,
enables structurally accurate reconstruction. Experiments on the SemanticKITTI
and nuScenes datasets show that the framework achieves state-of-the-art
compression rates, reducing data size by up to 98% while preserving both
structural and semantic fidelity. In addition, it supports downstream
applications such as multi-robot pose graph optimization and map merging,
achieving trajectory accuracy and map alignment comparable to those obtained
with raw LiDAR scans.

</details>


### [41] [MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding](https://arxiv.org/abs/2510.07915)
*Peiran Wu,Zhuorui Yu,Yunze Liu,Chi-Hao Wu,Enmin Zhou,Junxiao Shen*

Main category: cs.CV

TL;DR: 本文提出了一种面向视频视觉语言模型（VLM）的高效token压缩方法——MARC，在保持性能的前提下，极大减少视频处理的计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在处理视频时，由于帧数和时长较大，计算资源消耗巨大。现有的token压缩方法容易导致信息丢失和性能下降，因此需要提出更高效且性能可靠的压缩方案。

Method: 提出了Memory-Augmented Reinforcement Learning-based Token Compression (MARC)方法，结合了结构化检索和基于强化学习的蒸馏。具体通过Visual Memory Retriever (VMR)选取关键片段，然后用Compression Group Relative Policy Optimization (C-GRPO)框架将教师模型的推理能力迁移给学生模型，实现‘先检索再压缩’的策略。

Result: 在六个视频基准数据集上，MARC方法在只使用一帧tokens的情况下，依然达到了接近基线的准确率，并减少了95%视觉tokens、72%显存以及23.9%延迟。

Conclusion: MARC方法显著提升了视频VLM的效率，为视频问答、监控、自动驾驶等资源受限场景下的实时视频理解提供了有力的技术支撑。

Abstract: The rapid progress of large language models (LLMs) has laid the foundation
for multimodal models. However, visual language models (VLMs) still face heavy
computational costs when extended from images to videos due to high frame rates
and long durations. Token compression is a promising solution, yet most
existing training-free methods cause information loss and performance
degradation. To overcome this, we propose \textbf{Memory-Augmented
Reinforcement Learning-based Token Compression (MARC)}, which integrates
structured retrieval and RL-based distillation. MARC adopts a
\textit{retrieve-then-compress} strategy using a \textbf{Visual Memory
Retriever (VMR)} to select key clips and a \textbf{Compression Group Relative
Policy Optimization (C-GRPO)} framework to distil reasoning ability from a
teacher to a student model. Experiments on six video benchmarks show that MARC
achieves near-baseline accuracy using only one frame's tokens -- reducing
visual tokens by \textbf{95\%}, GPU memory by \textbf{72\%}, and latency by
\textbf{23.9\%}. This demonstrates its potential for efficient, real-time video
understanding in resource-constrained settings such as video QA, surveillance,
and autonomous driving.

</details>


### [42] [Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation](https://arxiv.org/abs/2510.08553)
*Yunzhe Xu,Yiyuan Pan,Zhe Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的视觉-语言导航方法Memoir，通过引入基于想象检索的显式记忆机制，大幅提升了智能体在需要积累和利用长期经验的导航场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有memory-persistent VLN方法主要只存储环境观察，缺乏对导航行为模式的记忆和高效的记忆访问机制，难以有效利用历史经验指导未来决策。

Method: Memoir的核心是引入想象机制作为记忆检索。具体包括：1）基于语言条件的世界模型，能对未来导航状态进行想象，用于经验编码和检索；2）混合视点级别记忆，同时存储环境观测和导航行为模式，通过视点索引实现混合检索；3）经验增强导航模型，整合检索到的多源知识。

Result: 在10个不同测试场景的大规模VLN基准评测下，Memoir在所有场景均表现出显著提升。例如在IR2R数据集上比最优的基线方法提升了5.4% SPL，训练速度加快8.3倍，推理内存需求降低74%。

Conclusion: 经验和行为模式的联合记忆与预测性检索可以极大地提升导航智能体的性能。分析结果表明，基于想象检索的VLN还有很大提升空间。

Abstract: Vision-and-Language Navigation (VLN) requires agents to follow natural
language instructions through environments, with memory-persistent variants
demanding progressive improvement through accumulated experience. Existing
approaches for memory-persistent VLN face critical limitations: they lack
effective memory access mechanisms, instead relying on entire memory
incorporation or fixed-horizon lookup, and predominantly store only
environmental observations while neglecting navigation behavioral patterns that
encode valuable decision-making strategies. We present Memoir, which employs
imagination as a retrieval mechanism grounded by explicit memory: a world model
imagines future navigation states as queries to selectively retrieve relevant
environmental observations and behavioral histories. The approach comprises: 1)
a language-conditioned world model that imagines future states serving dual
purposes: encoding experiences for storage and generating retrieval queries; 2)
Hybrid Viewpoint-Level Memory that anchors both observations and behavioral
patterns to viewpoints, enabling hybrid retrieval; and 3) an
experience-augmented navigation model that integrates retrieved knowledge
through specialized encoders. Extensive evaluation across diverse
memory-persistent VLN benchmarks with 10 distinctive testing scenarios
demonstrates Memoir's effectiveness: significant improvements across all
scenarios, with 5.4% SPL gains on IR2R over the best memory-persistent
baseline, accompanied by 8.3x training speedup and 74% inference memory
reduction. The results validate that predictive retrieval of both environmental
and behavioral memories enables more effective navigation, with analysis
indicating substantial headroom (73.3% vs 93.4% upper bound) for this
imagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.

</details>


### [43] [ASBench: Image Anomalies Synthesis Benchmark for Anomaly Detection](https://arxiv.org/abs/2510.07927)
*Qunyi Zhang,Songan Zhang,Jinbao Wang,Xiaoning Lei,Guoyang Xie,Guannan Jiang,Zhichao Lu*

Main category: cs.CV

TL;DR: 本文提出了ASBench，这是首个专门针对异常合成方法进行系统评测的基准框架，以推动制造业质控中的异常检测研究。


<details>
  <summary>Details</summary>
Motivation: 制造业异常检测受限于异常样本有限与高昂的人工标注成本。异常数据合成虽有前景，但以往工作仅将其作为异常检测的辅助环节，缺乏对异常合成算法的系统性评价，也未考虑异常合成独有的关键问题。

Method: 提出ASBench框架，从四个关键方面系统评估异常合成算法：(i) 跨数据集和检测流程的泛化能力 (ii) 合成数据与真实数据比例 (iii) 合成图像固有指标与检测性能之间的关系 (iv) 混合异常合成方法的策略。

Result: 通过大量实验，ASBench揭示了当前异常合成方法的局限性，展示各算法在不同场景和参数条件下的表现，并提供了详细的定量分析。

Conclusion: ASBench不仅为未来异常合成算法的研究提供了基准工具，还通过实验证明为相关研究提供了有价值的洞察和改进方向。

Abstract: Anomaly detection plays a pivotal role in manufacturing quality control, yet
its application is constrained by limited abnormal samples and high manual
annotation costs. While anomaly synthesis offers a promising solution, existing
studies predominantly treat anomaly synthesis as an auxiliary component within
anomaly detection frameworks, lacking systematic evaluation of anomaly
synthesis algorithms. Current research also overlook crucial factors specific
to anomaly synthesis, such as decoupling its impact from detection,
quantitative analysis of synthetic data and adaptability across different
scenarios. To address these limitations, we propose ASBench, the first
comprehensive benchmarking framework dedicated to evaluating anomaly synthesis
methods. Our framework introduces four critical evaluation dimensions: (i) the
generalization performance across different datasets and pipelines (ii) the
ratio of synthetic to real data (iii) the correlation between intrinsic metrics
of synthesis images and anomaly detection performance metrics , and (iv)
strategies for hybrid anomaly synthesis methods. Through extensive experiments,
ASBench not only reveals limitations in current anomaly synthesis methods but
also provides actionable insights for future research directions in anomaly
synthesis

</details>


### [44] [ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving](https://arxiv.org/abs/2510.08562)
*Zhiyu Zheng,Shaoyu Chen,Haoran Yin,Xinbang Zhang,Jialv Zou,Xinggang Wang,Qian Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: 本文针对端到端自动驾驶系统在轨迹预测中的时空数据不平衡问题，提出了ResAD，一种归一化残差轨迹建模框架，大大提升了模型性能并简化了学习任务。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶模型在直接从传感器数据预测未来轨迹时，常因轨迹数据时空分布不均导致优化困难，导致学习到虚假的相关性和对远期不确定目标的过度关注，从而影响安全性和表现。

Method: 提出ResAD框架，将预测任务转化为对轨迹与惯性参考之间残差的建模，并以惯性参考作为反事实参考，迫使模型关注偏离惯性路径的因果因素。此外，通过点位归一化方法，调整不同时刻的残差权重，抑制长期不确定点的优化主导地位。

Result: 在NAVSIM基准测试中，ResAD利用基础扩散策略，仅需两步去噪即达到了88.6的PDMS领先成绩，明显超过现有方法。

Conclusion: ResAD显著简化了端到端自动驾驶轨迹学习任务，提升了模型性能，为未来自动驾驶研究提供了新方向。

Abstract: End-to-end autonomous driving (E2EAD) systems, which learn to predict future
trajectories directly from sensor data, are fundamentally challenged by the
inherent spatio-temporal imbalance of trajectory data. This imbalance creates a
significant optimization burden, causing models to learn spurious correlations
instead of causal inference, while also prioritizing uncertain, distant
predictions, thereby compromising immediate safety. To address these issues, we
propose ResAD, a novel Normalized Residual Trajectory Modeling framework.
Instead of predicting the future trajectory directly, our approach reframes the
learning task to predict the residual deviation from a deterministic inertial
reference. The inertial reference serves as a counterfactual, forcing the model
to move beyond simple pattern recognition and instead identify the underlying
causal factors (e.g., traffic rules, obstacles) that necessitate deviations
from a default, inertially-guided path. To deal with the optimization imbalance
caused by uncertain, long-term horizons, ResAD further incorporates Point-wise
Normalization of the predicted residual. It re-weights the optimization
objective, preventing large-magnitude errors associated with distant, uncertain
waypoints from dominating the learning signal. Extensive experiments validate
the effectiveness of our framework. On the NAVSIM benchmark, ResAD achieves a
state-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two
denoising steps, demonstrating that our approach significantly simplifies the
learning task and improves model performance. The code will be released to
facilitate further research.

</details>


### [45] [TTOM: Test-Time Optimization and Memorization for Compositional Video Generation](https://arxiv.org/abs/2510.07940)
*Leigang Qu,Ziyang Wang,Na Zheng,Wenjie Wang,Liqiang Nie,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出了一种名为TTOM的推理时优化与记忆机制，无需重新训练即可提升视频生成基础模型（VFM）在复杂组合任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管当前的视频生成基础模型可以生成高质量的视觉内容，但在涉及多元素组合（如动态、计数和空间关系等）的场景下表现不佳。因此，作者希望提升VFM模型在推理阶段处理复杂文本-视频组合命题的能力。

Method: 提出了TTOM框架，在不干预模型主结构的前提下，通过在推理时新增并优化参数，实现输出与期望空间-时间布局的拟合。TTOM还引入了带参数记忆机制，支持插入、读取、更新和删除等操作，从而在流式视频生成中保留历史优化信息。整个过程无需模型重新训练。

Result: 在T2V-CompBench和Vbench两个基准测试中，TTOM展现了有效、实用、可扩展、且高效的跨模态对齐能力，显著提升了VFM在组合视频生成场景下的表现。

Conclusion: TTOM框架能在推理阶段有效提升视频基础模型在复杂组合场景下的文本-视频对齐和生成表现，具有良好的迁移性和泛化能力，是生成式视频模型实际应用的有力工具。

Abstract: Video Foundation Models (VFMs) exhibit remarkable visual generation
performance, but struggle in compositional scenarios (e.g., motion, numeracy,
and spatial relation). In this work, we introduce Test-Time Optimization and
Memorization (TTOM), a training-free framework that aligns VFM outputs with
spatiotemporal layouts during inference for better text-image alignment. Rather
than direct intervention to latents or attention per-sample in existing work,
we integrate and optimize new parameters guided by a general layout-attention
objective. Furthermore, we formulate video generation within a streaming
setting, and maintain historical optimization contexts with a parametric memory
mechanism that supports flexible operations, such as insert, read, update, and
delete. Notably, we found that TTOM disentangles compositional world knowledge,
showing powerful transferability and generalization. Experimental results on
the T2V-CompBench and Vbench benchmarks establish TTOM as an effective,
practical, scalable, and efficient framework to achieve cross-modal alignment
for compositional video generation on the fly.

</details>


### [46] [CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving](https://arxiv.org/abs/2510.07944)
*Tianrui Zhang,Yichen Liu,Zilin Guo,Yuxin Guo,Jingcheng Ni,Chenjing Ding,Dan Xu,Lewei Lu,Zehuan Wu*

Main category: cs.CV

TL;DR: 提出了一种新的生成模型CVD-STORM，可以在多种控制下生成高质量、多视角的长时序视频，并能输出有用的几何信息如深度估计。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶的发展，需要能在不同控制输入下高保真生成视频，并且能输出如深度等丰富的场景信息，而现有生成模型在这方面能力有限。

Method: 提出了一种基于空间-时序重构VAE的视频扩散生成框架。首先对VAE进行4D重构任务微调，提高对3D结构和时序动态的编码能力，然后将其整合进视频扩散生成流程中。联合训练的高斯光斑解码器进一步帮助场景重建。

Result: 实验显示该模型在FID和FVD两个主流视频生成评测指标上有显著提升，同时能有效还原动态场景，输出有用的几何信息。

Conclusion: CVD-STORM在多视角、长时序高保真视频生成和场景几何理解方面表现优异，为环境建模和下游应用带来新工具。

Abstract: Generative models have been widely applied to world modeling for environment
simulation and future state prediction. With advancements in autonomous
driving, there is a growing demand not only for high-fidelity video generation
under various controls, but also for producing diverse and meaningful
information such as depth estimation. To address this, we propose CVD-STORM, a
cross-view video diffusion model utilizing a spatial-temporal reconstruction
Variational Autoencoder (VAE) that generates long-term, multi-view videos with
4D reconstruction capabilities under various control inputs. Our approach first
fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its
ability to encode 3D structures and temporal dynamics. Subsequently, we
integrate this VAE into the video diffusion process to significantly improve
generation quality. Experimental results demonstrate that our model achieves
substantial improvements in both FID and FVD metrics. Additionally, the
jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic
scenes, providing valuable geometric information for comprehensive scene
understanding.

</details>


### [47] [A Large-scale Dataset for Robust Complex Anime Scene Text Detection](https://arxiv.org/abs/2510.07951)
*Ziyi Dong,Yurui Zhang,Changmao Li,Naomi Rue Golding,Qing Long*

Main category: cs.CV

TL;DR: 本文提出了一个针对动漫场景下文本检测的大规模数据集AnimeText，以解决现有数据集在复杂动漫场景中的适用性不足问题。实验表明，在AnimeText上训练的模型在动漫文本检测任务上性能优于现有数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的文本检测数据集主要聚焦于自然或文档场景，这些场景中的文本通常是规则排列、常见字体且与复杂视觉元素区分明显。而动漫场景下的文本则更为多样、不规则且易与其他视觉元素混淆，现有数据集难以支持动漫场景下的文本检测需求。

Method: 作者构建了AnimeText数据集，包含73.5万张图片和420万条标注文本块，提供了针对动漫场景的层级标注和难负样本（hard negative samples）。使用先进的文本检测方法进行了跨数据集训练和评估。

Result: 跨数据集评测结果显示，使用AnimeText训练的模型在动漫场景下的文本检测任务中，优于使用现有主流数据集训练的模型。

Conclusion: AnimeText弥补了动漫场景下缺乏高质量文本检测数据集的空白，为相关研究和应用提供了有力支持，并在动漫文本检测任务中显著提升了检测性能。

Abstract: Current text detection datasets primarily target natural or document scenes,
where text typically appear in regular font and shapes, monotonous colors, and
orderly layouts. The text usually arranged along straight or curved lines.
However, these characteristics differ significantly from anime scenes, where
text is often diverse in style, irregularly arranged, and easily confused with
complex visual elements such as symbols and decorative patterns. Text in anime
scene also includes a large number of handwritten and stylized fonts. Motivated
by this gap, we introduce AnimeText, a large-scale dataset containing 735K
images and 4.2M annotated text blocks. It features hierarchical annotations and
hard negative samples tailored for anime scenarios. %Cross-dataset evaluations
using state-of-the-art methods demonstrate that models trained on AnimeText
achieve superior performance in anime text detection tasks compared to existing
datasets. To evaluate the robustness of AnimeText in complex anime scenes, we
conducted cross-dataset benchmarking using state-of-the-art text detection
methods. Experimental results demonstrate that models trained on AnimeText
outperform those trained on existing datasets in anime scene text detection
tasks. AnimeText on HuggingFace:
https://huggingface.co/datasets/deepghs/AnimeText

</details>


### [48] [SimCast: Enhancing Precipitation Nowcasting with Short-to-Long Term Knowledge Distillation](https://arxiv.org/abs/2510.07953)
*Yifang Yin,Shengkai Chen,Yiyao Li,Lu Wang,Ruibing Jin,Wei Cui,Shili Xiang*

Main category: cs.CV

TL;DR: 本文提出了一种新的降水临近预报方法SimCast及其扩展框架CasCast，相较于现有方法显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 降水临近预报因地球系统本身高度复杂而极具挑战性，但对于防灾减灾、农业、交通和能源优化等社会需求至关重要。提升预报精度对社会影响巨大。

Method: 提出了SimCast训练流程，采用短至长期知识蒸馏与加权均方误差损失函数，尤其关注强降水区域，无需增加推理开销。进一步将SimCast纳入基于扩散模型的CasCast框架，结合概率模型优势，缓解确定性预测中图像模糊和分布偏移问题。

Result: 在SEVIR、HKO-7和MeteoNet三组主流数据集上取得领先成绩，CSI分别为0.452、0.474和0.361，显著超越当前主流方法。

Conclusion: 所提出的SimCast及其扩展框架能够在保持推理高效的同时大幅度提升降水临近预报的准确率，具有广泛的实际应用前景。

Abstract: Precipitation nowcasting predicts future radar sequences based on current
observations, which is a highly challenging task driven by the inherent
complexity of the Earth system. Accurate nowcasting is of utmost importance for
addressing various societal needs, including disaster management, agriculture,
transportation, and energy optimization. As a complementary to existing
non-autoregressive nowcasting approaches, we investigate the impact of
prediction horizons on nowcasting models and propose SimCast, a novel training
pipeline featuring a short-to-long term knowledge distillation technique
coupled with a weighted MSE loss to prioritize heavy rainfall regions. Improved
nowcasting predictions can be obtained without introducing additional overhead
during inference. As SimCast generates deterministic predictions, we further
integrate it into a diffusion-based framework named CasCast, leveraging the
strengths from probabilistic models to overcome limitations such as blurriness
and distribution shift in deterministic outputs. Extensive experimental results
on three benchmark datasets validate the effectiveness of the proposed
framework, achieving mean CSI scores of 0.452 on SEVIR, 0.474 on HKO-7, and
0.361 on MeteoNet, which outperforms existing approaches by a significant
margin.

</details>


### [49] [Latent Harmony: Synergistic Unified UHD Image Restoration via Latent Space Regularization and Controllable Refinement](https://arxiv.org/abs/2510.07961)
*Yidi Liu,Xueyang Fu,Jie Huang,Jie Xiao,Dong Li,Wenlong Zhang,Lei Bai,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出了Latent Harmony框架，结合两阶段方法改进VAE，实现高效且高保真的超高清（UHD）图像修复，在效率和高频细节还原间取得突破性平衡。


<details>
  <summary>Details</summary>
Motivation: 传统UHD图像修复方法难以兼顾推理效率与高频细节还原。常用的VAE虽然高效，但因高斯限制丢失了高频退化信息，导致重建质量下降。作者希望突破VAE高频信息损失瓶颈，兼顾效率和保真。

Method: 第一阶段提出LH-VAE，通过视觉语义约束和逐步退化扰动提升语义健壮性，并通过潜空间等变性强化高频重建能力。第二阶段引入高频低秩适应模块（HF-LoRA）：编码器LoRA通过高频对齐损失恢复真实细节，解码器LoRA通过感知损失合成自然纹理。两者通过交替优化和选择性梯度传播训练，保持预训练潜空间结构；推理时支持α参数调节保真与感知平衡。

Result: Latent Harmony在UHD及标准分辨率修复任务上取得了领先性能，实现了重建效率、感知质量和准确性的高水平平衡。

Conclusion: Latent Harmony重新定义了UHD图像修复的VAE范式，用创新的两阶段方法有效兼顾了效率和高频细节，为实际超高清修复应用提供了更优方案。

Abstract: Ultra-High Definition (UHD) image restoration faces a trade-off between
computational efficiency and high-frequency detail retention. While Variational
Autoencoders (VAEs) improve efficiency via latent-space processing, their
Gaussian constraint often discards degradation-specific high-frequency
information, hurting reconstruction fidelity. To overcome this, we propose
Latent Harmony, a two-stage framework that redefines VAEs for UHD restoration
by jointly regularizing the latent space and enforcing high-frequency-aware
reconstruction.In Stage One, we introduce LH-VAE, which enhances semantic
robustness through visual semantic constraints and progressive degradation
perturbations, while latent equivariance strengthens high-frequency
reconstruction.Stage Two jointly trains this refined VAE with a restoration
model using High-Frequency Low-Rank Adaptation (HF-LoRA): an encoder LoRA
guided by a fidelity-oriented high-frequency alignment loss to recover
authentic details, and a decoder LoRA driven by a perception-oriented loss to
synthesize realistic textures. Both LoRA modules are trained via alternating
optimization with selective gradient propagation to preserve the pretrained
latent structure.At inference, a tunable parameter {\alpha} enables flexible
fidelity-perception trade-offs.Experiments show Latent Harmony achieves
state-of-the-art performance across UHD and standard-resolution tasks,
effectively balancing efficiency, perceptual quality, and reconstruction
accuracy.

</details>


### [50] [The impact of abstract and object tags on image privacy classification](https://arxiv.org/abs/2510.07976)
*Darya Baranouskaya,Andrea Cavallaro*

Main category: cs.CV

TL;DR: 本文比较了用于图像隐私分类任务中，具体对象标签（object tags）与抽象标签（abstract tags）的有效性。


<details>
  <summary>Details</summary>
Motivation: 对象标签被广泛用于计算机视觉任务，但抽象标签可能更适用于涉及主观和上下文理解的隐私分类任务。作者希望探究哪种标签类型在图像隐私分类中更合适。

Method: 作者比较了使用对象标签和抽象标签两种方式在图像隐私分类任务中的表现，并考虑了标签数量的影响。

Result: 在标签数量有限的情况下，抽象标签比对象标签效果更好；当每张图像可用的标签足够多时，对象标签的有效性与抽象标签持平。

Conclusion: 作者建议未来图像隐私分类研究需根据标签类型和数量权衡选择，以提升分类准确性。

Abstract: Object tags denote concrete entities and are central to many computer vision
tasks, whereas abstract tags capture higher-level information, which is
relevant for tasks that require a contextual, potentially subjective scene
understanding. Object and abstract tags extracted from images also facilitate
interpretability. In this paper, we explore which type of tags is more suitable
for the context-dependent and inherently subjective task of image privacy.
While object tags are generally used for privacy classification, we show that
abstract tags are more effective when the tag budget is limited. Conversely,
when a larger number of tags per image is available, object-related information
is as useful. We believe that these findings will guide future research in
developing more accurate image privacy classifiers, informed by the role of tag
types and quantity.

</details>


### [51] [Is Architectural Complexity Always the Answer? A Case Study on SwinIR vs. an Efficient CNN](https://arxiv.org/abs/2510.07984)
*Chandresh Sutariya,Nitin Singh*

Main category: cs.CV

TL;DR: 本文比较了高性能的Transformer模型SwinIR与轻量级CNN在低光图像细节恢复与噪声抑制任务上的表现，发现轻量级CNN以极低计算负担达到接近SwinIR的性能，建议在资源有限场景中优先采用。


<details>
  <summary>Details</summary>
Motivation: 低光图像往往细节丢失且噪声严重，恢复高频信息与降噪很困难。虽然大型Transformer模型性能突出，但计算资源消耗高，难以在实际中大规模应用，因此迫切需要权衡性能与效率。

Method: 实验对比了SwinIR（基于Transformer）和标准轻量级CNN在低光恢复任务中的表现，评估它们在峰值信噪比（PSNR）、训练收敛速度和模型体积上的差异。

Result: SwinIR在PSNR上达到39.03 dB，表现最好，但轻量级CNN也取得了37.4 dB的PSNR，并且训练仅需10个epoch，模型体积比SwinIR小55倍。

Conclusion: 标准轻量级CNN能用远低于SwinIR的计算资源，实现接近的恢复质量，更适合在算力受限的实际应用中推广。

Abstract: The simultaneous restoration of high-frequency details and suppression of
severe noise in low-light imagery presents a significant and persistent
challenge in computer vision. While large-scale Transformer models like SwinIR
have set the state of the art in performance, their high computational cost can
be a barrier for practical applications. This paper investigates the critical
trade-off between performance and efficiency by comparing the state-of-the-art
SwinIR model against a standard, lightweight Convolutional Neural Network (CNN)
on this challenging task. Our experimental results reveal a nuanced but
important finding. While the Transformer-based SwinIR model achieves a higher
peak performance, with a Peak Signal-to-Noise Ratio (PSNR) of 39.03 dB, the
lightweight CNN delivers a surprisingly competitive PSNR of 37.4 dB. Crucially,
the CNN reached this performance after converging in only 10 epochs of
training, whereas the more complex SwinIR model required 132 epochs. This
efficiency is further underscored by the model's size; the CNN is over 55 times
smaller than SwinIR. This work demonstrates that a standard CNN can provide a
near state-of-the-art result with significantly lower computational overhead,
presenting a compelling case for its use in real-world scenarios where resource
constraints are a primary concern.

</details>


### [52] [GraphEnet: Event-driven Human Pose Estimation with a Graph Neural Network](https://arxiv.org/abs/2510.07990)
*Gaurvi Goyal,Pham Cong Thuong,Arren Glover,Masayoshi Mizuno,Chiara Bartolozzi*

Main category: cs.CV

TL;DR: 本文提出一种新方法GraphEnet，首次使用图神经网络（GNN）处理事件相机的数据，实现高频率、单人2D人体姿态估计，具有低延迟和低能耗优势，已开源。


<details>
  <summary>Details</summary>
Motivation: 事件相机由于低延迟和低能耗，适用于资源受限的场景（如便携设备和移动机器人），但尚缺乏有效的人体姿态估计方法。现有基于RGB的深度学习方法对延迟和能耗敏感，不能充分利用事件相机的优势。

Method: 提出的GraphEnet基于图神经网络，利用了事件相机输出的稀疏特性，并通过中间的基于线条的事件表示方法，将事件数据转化为适合GNN处理的结构。此外，采用一种新颖的偏移向量学习机制，并结合基于置信度的池化策略来提升姿态估计的精度。

Result: GraphEnet能以高频方式估计单人的2D人体姿态，实验显示该方法能够充分利用事件相机的稀疏特性，具有优越的鲁棒性和高效性。

Conclusion: 本研究首次将图神经网络应用于事件相机数据的人体姿态估计算法，为低延迟、低能耗的人机交互应用提供了技术基础，将加速事件视觉领域的相关研究。

Abstract: Human Pose Estimation is a crucial module in human-machine interaction
applications and, especially since the rise in deep learning technology, robust
methods are available to consumers using RGB cameras and commercial GPUs. On
the other hand, event-based cameras have gained popularity in the vision
research community for their low latency and low energy advantages that make
them ideal for applications where those resources are constrained like portable
electronics and mobile robots. In this work we propose a Graph Neural Network,
GraphEnet, that leverages the sparse nature of event camera output, with an
intermediate line based event representation, to estimate 2D Human Pose of a
single person at a high frequency. The architecture incorporates a novel offset
vector learning paradigm with confidence based pooling to estimate the human
pose. This is the first work that applies Graph Neural Networks to event data
for Human Pose Estimation. The code is open-source at
https://github.com/event-driven-robotics/GraphEnet-NeVi-ICCV2025.

</details>


### [53] [CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.08003)
*Weihuang Lin,Yiwei Ma,Jiayi Ji,Xiaoshuai Sun,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出CIR-CoT模型，在复合图像检索任务中引入显式的Chain-of-Thought (CoT)推理，使检索过程更具可解释性与透明性，并构建配套CoT数据集，实验显示模型效果优异且具泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言大模型（VLM、MLLM）的复合图像检索方法难以让用户理解检索理由，模型对复杂、细粒度指令的执行能力有限，因此亟需一种既能提升检索效果又具高可解释性的方案。

Method: 提出端到端的CIR-CoT模型，强制模型在检索前生成结构化的推理链（包括描述、推理和结论），并据此编码最终检索目标，同时构造带有结构化CoT标注的新训练数据集，以提升模型推理能力及透明度。

Result: 在FashionIQ、CIRR等主流数据集上，CIR-CoT具备高度竞争力，与现有方法相比提升明显，在跨域（CIRCO数据集）实验中具有优异的泛化能力。

Conclusion: CIR-CoT不仅在提升检索准确率方面表现出色，还以可解释化推进检索系统的可信与有效性，为复合图像检索提供新研究范式。

Abstract: Composed Image Retrieval (CIR), which aims to find a target image from a
reference image and a modification text, presents the core challenge of
performing unified reasoning across visual and semantic modalities. While
current approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more
recent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown
progress, they predominantly function as ``black boxes." This inherent opacity
not only prevents users from understanding the retrieval rationale but also
restricts the models' ability to follow complex, fine-grained instructions. To
overcome these limitations, we introduce CIR-CoT, the first end-to-end
retrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT)
reasoning. By compelling the model to first generate an interpretable reasoning
chain, CIR-CoT enhances its ability to capture crucial cross-modal
interactions, leading to more accurate retrieval while making its decision
process transparent. Since existing datasets like FashionIQ and CIRR lack the
necessary reasoning data, a key contribution of our work is the creation of
structured CoT annotations using a three-stage process involving a caption,
reasoning, and conclusion. Our model is then fine-tuned to produce this
structured output before encoding its final retrieval intent into a dedicated
embedding. Comprehensive experiments show that CIR-CoT achieves highly
competitive performance on in-domain datasets (FashionIQ, CIRR) and
demonstrates remarkable generalization on the out-of-domain CIRCO dataset,
establishing a new path toward more effective and trustworthy retrieval
systems.

</details>


### [54] [RayFusion: Ray Fusion Enhanced Collaborative Visual Perception](https://arxiv.org/abs/2510.08017)
*Shaohong Wang,Bin Lu,Xinyu Xiao,Hanzhi Zhong,Bowen Pang,Tong Wang,Zhiyu Xiang,Hangguan Shan,Eryun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为RayFusion的协作视觉感知方法，通过光线融合大幅提升了纯摄像头系统在3D目标检测中的准确性。实验显示，RayFusion优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 摄像头感知系统因缺乏显式深度信息，导致在三维目标检测等任务中预测不准确，协作视觉感知能够缓解传感器局限，但现有方法在深度估计上仍存在歧义。

Method: 提出RayFusion方法，通过协作者获取的光线占据信息进行光线级的特征融合。这一机制能够减少冗余信息和沿摄像头视线的误检，同时提升检测效果。

Result: 在大规模实验中，RayFusion的检测性能全面优于当前最先进的协作视觉感知模型。

Conclusion: RayFusion方法提升了纯摄像头协作感知系统的三维目标检测能力，为相关领域带来显著进展。

Abstract: Collaborative visual perception methods have gained widespread attention in
the autonomous driving community in recent years due to their ability to
address sensor limitation problems. However, the absence of explicit depth
information often makes it difficult for camera-based perception systems, e.g.,
3D object detection, to generate accurate predictions. To alleviate the
ambiguity in depth estimation, we propose RayFusion, a ray-based fusion method
for collaborative visual perception. Using ray occupancy information from
collaborators, RayFusion reduces redundancy and false positive predictions
along camera rays, enhancing the detection performance of purely camera-based
collaborative perception systems. Comprehensive experiments show that our
method consistently outperforms existing state-of-the-art models, substantially
advancing the performance of collaborative visual perception. The code is
available at https://github.com/wangsh0111/RayFusion.

</details>


### [55] [RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings for Weakly Supervised Anomaly Detection in Brain MRI Scans](https://arxiv.org/abs/2510.08052)
*Bheeshm Sharma,Karthikeyan Jaganathan,Balamurugan Palaniappan*

Main category: cs.CV

TL;DR: 该论文提出了一种名为RASALoRE的新型弱监督脑MRI异常检测方法，在只有切片级弱标签的条件下表现出色，参数量小，效果超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 脑部MRI异常检测对脑疾病的早期诊断具有重要意义，但获得精确像素级标注成本高昂。弱监督异常检测方法可以降低标注难度与成本，提升实际应用可行性。

Method: 作者设计了两阶段方法：第一阶段通过Discriminative Dual Prompt Tuning机制基于切片标签生成高质量伪弱掩码，用作粗定位；第二阶段引入区域感知空间注意力与基于位置的随机嵌入，利用伪掩码对异常区域进行精细定位与分割，参数量低于8M。

Result: 在BraTS20、BraTS21、BraTS23和MSD等大规模公开数据集上，该方法大幅超过了现有主流弱监督异常检测模型，并显著降低了模型参数量和计算复杂度。

Conclusion: RASALoRE在保持高检测准确率的前提下，大幅提升了效率与资源利用率，可为实际医学影像诊断提供高效实用的弱监督解决方案。

Abstract: Weakly Supervised Anomaly detection (WSAD) in brain MRI scans is an important
challenge useful to obtain quick and accurate detection of brain anomalies when
precise pixel-level anomaly annotations are unavailable and only weak labels
(e.g., slice-level) are available. In this work, we propose RASALoRE: Region
Aware Spatial Attention with Location-based Random Embeddings, a novel
two-stage WSAD framework. In the first stage, we introduce a Discriminative
Dual Prompt Tuning (DDPT) mechanism that generates high-quality pseudo weak
masks based on slice-level labels, serving as coarse localization cues. In the
second stage, we propose a segmentation network with a region-aware spatial
attention mechanism that relies on fixed location-based random embeddings. This
design enables the model to effectively focus on anomalous regions. Our
approach achieves state-of-the-art anomaly detection performance, significantly
outperforming existing WSAD methods while utilizing less than 8 million
parameters. Extensive evaluations on the BraTS20, BraTS21, BraTS23, and MSD
datasets demonstrate a substantial performance improvement coupled with a
significant reduction in computational complexity. Code is available at:
https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/.

</details>


### [56] [RetouchLLM: Training-free White-box Image Retouching](https://arxiv.org/abs/2510.08054)
*Moon Ye-Bin,Roy Miles,Tae-Hyun Oh,Ismail Elezi,Jiankang Deng*

Main category: cs.CV

TL;DR: 本论文提出了一种无需训练、无需配对数据的白盒高分辨率图像调色系统RetouchLLM，通过逐步增强和可解释的代码生成，支持多样化、个性化的图像调色，并实现了对现有黑盒方法的突破。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的图像调色方法依赖大规模配对数据，且多为黑盒，过程不透明，难以实现用户/图片特定的自定义调整，限制了适用性和交互性。

Method: 作者设计了一个无需训练数据的白盒架构RetouchLLM，主要包含视觉评论模块（识别输入和参考图的差异）与代码生成器（生成可执行调整代码），系统能逐步模拟人工多步调色过程，并支持用户用自然语言互动进行个性化控制。

Result: 实验证明，RetouchLLM在多种调色风格下都具备良好的泛化能力，同时自然语言交互方式实现了可解释且可控的图像个性化调色。

Conclusion: RetouchLLM避免了大量标注数据需求，实现了过程透明、易于定制和交互的高分辨率图像调色，是解决现有黑盒调色方式局限性的重要进展。

Abstract: Image retouching not only enhances visual quality but also serves as a means
of expressing personal preferences and emotions. However, existing
learning-based approaches require large-scale paired data and operate as black
boxes, making the retouching process opaque and limiting their adaptability to
handle diverse, user- or image-specific adjustments. In this work, we propose
RetouchLLM, a training-free white-box image retouching system, which requires
no training data and performs interpretable, code-based retouching directly on
high-resolution images. Our framework progressively enhances the image in a
manner similar to how humans perform multi-step retouching, allowing
exploration of diverse adjustment paths. It comprises of two main modules: a
visual critic that identifies differences between the input and reference
images, and a code generator that produces executable codes. Experiments
demonstrate that our approach generalizes well across diverse retouching
styles, while natural language-based user interaction enables interpretable and
controllable adjustments tailored to user intent.

</details>


### [57] [A class-driven hierarchical ResNet for classification of multispectral remote sensing images](https://arxiv.org/abs/2510.08060)
*Giulio Weikmann,Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 本文提出了一种多时序分层残差神经网络（ResNet），用于多光谱图像时间序列的分层分类。


<details>
  <summary>Details</summary>
Motivation: 多时序遥感影像分类时存在语义层次细分、样本有限及类间区分困难等问题，现有方法难以兼顾不同语义层次的区分能力及迁移能力。

Method: 改进ResNet结构，在不同分层处引入分支并结合层级惩罚机制，利用层级标签分别训练各层，从宏观到微观类逐步细化判别能力，实现端到端的分层分类网络，并能通过微调适应新任务。

Result: 在亚马逊森林两块区域的Sentinel-2月度复合影像上，实验显示该方法可在各层级取得良好泛化性能，尤其能提升对小类别和细粒度微类的区分。

Conclusion: 该分层残差网络在不同语义层级上均获得高质量分类结果，特别是在小类别及新区域的微观类判别能力上优于传统方法，显示出良好的适应性和扩展性。

Abstract: This work presents a multitemporal class-driven hierarchical Residual Neural
Network (ResNet) designed for modelling the classification of Time Series (TS)
of multispectral images at different semantical class levels. The architecture
consists of a modification of the ResNet where we introduce additional branches
to perform the classification at the different hierarchy levels and leverage on
hierarchy-penalty maps to discourage incoherent hierarchical transitions within
the classification. In this way, we improve the discrimination capabilities of
classes at different levels of semantic details and train a modular
architecture that can be used as a backbone network for introducing new
specific classes and additional tasks considering limited training samples
available. We exploit the class-hierarchy labels to train efficiently the
different layers of the architecture, allowing the first layers to train faster
on the first levels of the hierarchy modeling general classes (i.e., the
macro-classes) and the intermediate classes, while using the last ones to
discriminate more specific classes (i.e., the micro-classes). In this way, the
targets are constrained in following the hierarchy defined, improving the
classification of classes at the most detailed level. The proposed modular
network has intrinsic adaptation capability that can be obtained through fine
tuning. The experimental results, obtained on two tiles of the Amazonian Forest
on 12 monthly composites of Sentinel 2 images acquired during 2019, demonstrate
the effectiveness of the hierarchical approach in both generalizing over
different hierarchical levels and learning discriminant features for an
accurate classification at the micro-class level on a new target area, with a
better representation of the minoritarian classes.

</details>


### [58] [Towards Real-World Deepfake Detection: A Diverse In-the-wild Dataset of Forgery Faces](https://arxiv.org/abs/2510.08067)
*Junyu Shi,Minghui Li,Junguo Zuo,Zhifei Yu,Yipeng Lin,Shengshan Hu,Ziqi Zhou,Yechao Zhang,Wei Wan,Yinzhe Xu,Leo Yu Zhang*

Main category: cs.CV

TL;DR: 本文提出了RedFace数据集，用于更真实地评估和促进深度伪造（deepfake）人脸检测方法。RedFace包含超过6万张伪造图片和1000段篡改视频，均基于商业在线平台的最新AI技术生成，更贴合现实威胁。实验显示，当前检测方法在RedFace上效果有限，揭示了学术与现实应用间的差距。


<details>
  <summary>Details</summary>
Motivation: 目前深度伪造技术发展迅速，对社交媒体的真实性构成了威胁，但现有针对深度伪造检测的数据集往往来自学术生成方法，缺乏多样性和现实性，无法有效反映实际应用场景的挑战。因此，需要一个更贴近实际、能反映最新伪造技术的数据集以推动检测算法发展。

Method: 作者构建了RedFace数据集，从9个主流商业深度伪造平台抓取、整合了大量基于真实人脸的伪造图片和视频。数据合成采用了定制算法，覆盖多样且不断演进的伪造方法；并通过多种实验对比了现有检测技术在RedFace和传统数据集上的表现，包括跨域、域内以及模拟真实社交网络传播等场景。

Result: 实验结果显示，现有深度伪造检测方法在RedFace的各种真实场景下都面临显著性能下降，特别是在不同领域或在社交网络环境传播的情况下，验证了RedFace在反映现实挑战方面的有效性。

Conclusion: RedFace数据集更好地补足了学术与现实世界深度伪造检测之间的差距，能有效推进更实用、更具有抗干扰性的检测技术开发。对于研究者和实际防伪需求都有重要意义。

Abstract: Deepfakes, leveraging advanced AIGC (Artificial Intelligence-Generated
Content) techniques, create hyper-realistic synthetic images and videos of
human faces, posing a significant threat to the authenticity of social media.
While this real-world threat is increasingly prevalent, existing academic
evaluations and benchmarks for detecting deepfake forgery often fall short to
achieve effective application for their lack of specificity, limited deepfake
diversity, restricted manipulation techniques.To address these limitations, we
introduce RedFace (Real-world-oriented Deepfake Face), a specialized facial
deepfake dataset, comprising over 60,000 forged images and 1,000 manipulated
videos derived from authentic facial features, to bridge the gap between
academic evaluations and real-world necessity. Unlike prior benchmarks, which
typically rely on academic methods to generate deepfakes, RedFace utilizes 9
commercial online platforms to integrate the latest deepfake technologies found
"in the wild", effectively simulating real-world black-box scenarios.Moreover,
RedFace's deepfakes are synthesized using bespoke algorithms, allowing it to
capture diverse and evolving methods used by real-world deepfake creators.
Extensive experimental results on RedFace (including cross-domain,
intra-domain, and real-world social network dissemination simulations) verify
the limited practicality of existing deepfake detection schemes against
real-world applications. We further perform a detailed analysis of the RedFace
dataset, elucidating the reason of its impact on detection performance compared
to conventional datasets. Our dataset is available at:
https://github.com/kikyou-220/RedFace.

</details>


### [59] [Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection](https://arxiv.org/abs/2510.08073)
*Shuhai Zhang,ZiHao Lian,Jiahao Yang,Daiyuan Li,Guoxuan Pang,Feng Liu,Bo Han,Shutao Li,Mingkui Tan*

Main category: cs.CV

TL;DR: 本文提出了一种基于物理规律的AI生成视频检测方法，通过归一化时空梯度(Normalized Spatiotemporal Gradient, NSG)特征衡量自然视频与生成视频之间的差异，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成视频视觉真实性达到极高水平（如Sora），对高效、可靠的检测技术需求迫切，但现有检测方法难以捕捉高维时空动态与物理规律异常。

Method: 作者基于概率流守恒原理，提出NSG统计量衡量空间梯度与时间密度变化的比率，显式地捕捉与真实视频动力学的偏离。利用预训练扩散模型，结合空间梯度近似和运动感知时间建模，无需复杂视频运动解耦，保证物理约束。基于NSG，提出NSG-VD检测方法，通过计算测试视频与真实视频NSG特征的最大均值差异(MMD)进行检测。

Result: NSG-VD在多个实验中较主流方法在召回率提升16.00%，F1分数提升10.75%。

Conclusion: NSG-VD能有效区分合成视频与真实视频，显著优于当前的检测基线，为物理驱动下的AI视频检测提供了新路径。

Abstract: AI-generated videos have achieved near-perfect visual realism (e.g., Sora),
urgently necessitating reliable detection mechanisms. However, detecting such
videos faces significant challenges in modeling high-dimensional spatiotemporal
dynamics and identifying subtle anomalies that violate physical laws. In this
paper, we propose a physics-driven AI-generated video detection paradigm based
on probability flow conservation principles. Specifically, we propose a
statistic called Normalized Spatiotemporal Gradient (NSG), which quantifies the
ratio of spatial probability gradients to temporal density changes, explicitly
capturing deviations from natural video dynamics. Leveraging pre-trained
diffusion models, we develop an NSG estimator through spatial gradients
approximation and motion-aware temporal modeling without complex motion
decomposition while preserving physical constraints. Building on this, we
propose an NSG-based video detection method (NSG-VD) that computes the Maximum
Mean Discrepancy (MMD) between NSG features of the test and real videos as a
detection metric. Last, we derive an upper bound of NSG feature distances
between real and generated videos, proving that generated videos exhibit
amplified discrepancies due to distributional shifts. Extensive experiments
confirm that NSG-VD outperforms state-of-the-art baselines by 16.00% in Recall
and 10.75% in F1-Score, validating the superior performance of NSG-VD. The
source code is available at https://github.com/ZSHsh98/NSG-VD.

</details>


### [60] [DarkHash: A Data-Free Backdoor Attack Against Deep Hashing](https://arxiv.org/abs/2510.08094)
*Ziqi Zhou,Menghao Deng,Yufei Song,Hangtao Zhang,Wei Wan,Shengshan Hu,Minghui Li,Leo Yu Zhang,Dezhong Yao*

Main category: cs.CV

TL;DR: 本文提出了一种无需访问训练数据即可对深度哈希模型实施后门攻击的方法DarkHash，效果优于现有攻击方法，并且能抵抗主流防御手段。


<details>
  <summary>Details</summary>
Motivation: 现有深度哈希后门攻击方法依赖于训练数据，但实际中这些数据常受隐私或知识产权保护，难以获取。如何在无数据条件下植入后门并保持模型性能，是一个新的挑战。

Method: 提出DarkHash，首次实现无数据条件下针对深度哈希模型的后门攻击。方法包括：设计双语义引导的影子后门攻击框架、仅微调目标模型特定层并借助代理数据集、引入拓扑对齐损失以优化样本及其邻域，整体提升攻击有效性，同时保持原任务检索精度。

Result: 在四个图像数据集、五种模型架构和两种哈希方法上，DarkHash的攻击效果均超越现有先进方法。

Conclusion: DarkHash为深度哈希带来了更强后门攻击威胁，现有主流防御方法难以抵御，表明需开发新的防卫手段。

Abstract: Benefiting from its superior feature learning capabilities and efficiency,
deep hashing has achieved remarkable success in large-scale image retrieval.
Recent studies have demonstrated the vulnerability of deep hashing models to
backdoor attacks. Although these studies have shown promising attack results,
they rely on access to the training dataset to implant the backdoor. In the
real world, obtaining such data (e.g., identity information) is often
prohibited due to privacy protection and intellectual property concerns.
Embedding backdoors into deep hashing models without access to the training
data, while maintaining retrieval accuracy for the original task, presents a
novel and challenging problem. In this paper, we propose DarkHash, the first
data-free backdoor attack against deep hashing. Specifically, we design a novel
shadow backdoor attack framework with dual-semantic guidance. It embeds
backdoor functionality and maintains original retrieval accuracy by fine-tuning
only specific layers of the victim model using a surrogate dataset. We consider
leveraging the relationship between individual samples and their neighbors to
enhance backdoor attacks during training. By designing a topological alignment
loss, we optimize both individual and neighboring poisoned samples toward the
target sample, further enhancing the attack capability. Experimental results on
four image datasets, five model architectures, and two hashing methods
demonstrate the high effectiveness of DarkHash, outperforming existing
state-of-the-art backdoor attack methods. Defense experiments show that
DarkHash can withstand existing mainstream backdoor defense methods.

</details>


### [61] [Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting](https://arxiv.org/abs/2510.08096)
*Ankit Gahlawat,Anirban Mukherjee,Dinesh Babu Jayagopi*

Main category: cs.CV

TL;DR: 提出一种利用3D Gaussian Splatting（3DGS）与多视角一致性提升极端角度下人脸分割精度的方法，实现高效、低成本数据精炼，无需3D标注即可提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 极端角度下人脸标注数据稀缺，人工标注成本高，限制了分割模型的提升，因此需要低成本、高效的数据扩充和精炼方法。

Method: 提出一个标签精炼流程，首先利用3D Gaussian Splatting对RGB图像和初始分割图训练两个3DGS模型，通过共享几何信息实现多视角一致性。随后用此方法合成多角度高质量分割数据，进行简单后处理后，用于细化训练集并微调分割模型。

Result: 在极端角度的人脸分割任务中，模型精度大幅提升，且普通姿态下表现依然优良。多项实验和人工评价显示，该方法在无需真实3D标注，使用极少初始图像条件下，优于现有方法。

Conclusion: 本方法为实际场景中大规模提升人脸分割模型的稳健性、扩展多姿态能力，提供了一种高效、可扩展的新方案。

Abstract: Accurate face parsing under extreme viewing angles remains a significant
challenge due to limited labeled data in such poses. Manual annotation is
costly and often impractical at scale. We propose a novel label refinement
pipeline that leverages 3D Gaussian Splatting (3DGS) to generate accurate
segmentation masks from noisy multiview predictions. By jointly fitting two
3DGS models, one to RGB images and one to their initial segmentation maps, our
method enforces multiview consistency through shared geometry, enabling the
synthesis of pose-diverse training data with only minimal post-processing.
Fine-tuning a face parsing model on this refined dataset significantly improves
accuracy on challenging head poses, while maintaining strong performance on
standard views. Extensive experiments, including human evaluations, demonstrate
that our approach achieves superior results compared to state-of-the-art
methods, despite requiring no ground-truth 3D annotations and using only a
small set of initial images. Our method offers a scalable and effective
solution for improving face parsing robustness in real- world settings.

</details>


### [62] [Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation](https://arxiv.org/abs/2510.08116)
*Eirik A. Østmo,Kristoffer K. Wickstrøm,Keyur Radiya,Michael C. Kampffmeyer,Karl Øyvind Mikalsen,Robert Jenssen*

Main category: cs.CV

TL;DR: 本文提出了一种针对CT影像的特定增强方法Random windowing，比通用增强方法更适用于CT图像分割任务，并在肝脏肿瘤分割等多个数据集上超越了当前最优方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习图像增强方法多源于自然图像，未考虑CT影像的物理特性（像素值为HU有物理意义），在医学图像处理中易引入伪影，影响泛化能力。

Method: 提出Random windowing方法，基于CT影像的HU值分布设计图像增强策略，使模型在对比增强变化和时机不佳时仍具较好鲁棒性。

Result: 在多个数据集（特别是肝脏肿瘤分割任务）上，Random windowing方法在模型鲁棒性和泛化能力上均超过了主流增强方法和最优基线。

Conclusion: CT专用增强方法Random windowing能有效提升深度学习模型在CT影像分割任务上的性能，优于通用增强策略，值得在相关医学影像任务中推广。

Abstract: Contrast-enhanced Computed Tomography (CT) is important for diagnosis and
treatment planning for various medical conditions. Deep learning (DL) based
segmentation models may enable automated medical image analysis for detecting
and delineating tumors in CT images, thereby reducing clinicians' workload.
Achieving generalization capabilities in limited data domains, such as
radiology, requires modern DL models to be trained with image augmentation.
However, naively applying augmentation methods developed for natural images to
CT scans often disregards the nature of the CT modality, where the intensities
measure Hounsfield Units (HU) and have important physical meaning. This paper
challenges the use of such intensity augmentations for CT imaging and shows
that they may lead to artifacts and poor generalization. To mitigate this, we
propose a CT-specific augmentation technique, called Random windowing, that
exploits the available HU distribution of intensities in CT images. Random
windowing encourages robustness to contrast-enhancement and significantly
increases model performance on challenging images with poor contrast or timing.
We perform ablations and analysis of our method on multiple datasets, and
compare to, and outperform, state-of-the-art alternatives, while focusing on
the challenge of liver tumor segmentation.

</details>


### [63] [Real-Time Motion-Controllable Autoregressive Video Diffusion](https://arxiv.org/abs/2510.08131)
*Kesen Zhao,Jiaxin Shi,Beier Zhu,Junbao Zhou,Xiaolong Shen,Yuan Zhou,Qianru Sun,Hanwang Zhang*

Main category: cs.CV

TL;DR: AR-Drag是一种能够实现实时、高质量、可控运动的视频生成模型，相较现有方法大幅降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的视频生成方法存在延迟高、很难实现复杂运动控制的问题，且自回归（AR）扩散视频模型在高效、可控生成方面表现不佳，因此需要新的技术提升实时控制和生成质量。

Method: 作者提出了AR-Drag，一种结合自回归生成、强化学习优化和新颖采样机制的图像到视频扩散框架。具体地，先将基础模型微调以实现基本运动控制，然后用强化学习（基于轨迹奖励）进一步优化模型，同时通过Self-Rollout机制保持马尔科夫性，并在去噪步骤中引入选择性随机性来提升训练效率和生成速度。

Result: 实验结果显示，AR-Drag在运动对齐和视觉质量上均优于现有可控运动扩散模型，并且延迟显著降低，参数量仅1.3B。

Conclusion: AR-Drag首次实现了基于RL优化的高质量、可控、实时图像到视频生成，为视频生成任务提供了新范式。

Abstract: Real-time motion-controllable video generation remains challenging due to the
inherent latency of bidirectional diffusion models and the lack of effective
autoregressive (AR) approaches. Existing AR video diffusion models are limited
to simple control signals or text-to-video generation, and often suffer from
quality degradation and motion artifacts in few-step generation. To address
these challenges, we propose AR-Drag, the first RL-enhanced few-step AR video
diffusion model for real-time image-to-video generation with diverse motion
control. We first fine-tune a base I2V model to support basic motion control,
then further improve it via reinforcement learning with a trajectory-based
reward model. Our design preserves the Markov property through a Self-Rollout
mechanism and accelerates training by selectively introducing stochasticity in
denoising steps. Extensive experiments demonstrate that AR-Drag achieves high
visual fidelity and precise motion alignment, significantly reducing latency
compared with state-of-the-art motion-controllable VDMs, while using only 1.3B
parameters. Additional visualizations can be found on our project page:
https://kesenzhao.github.io/AR-Drag.github.io/.

</details>


### [64] [Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement](https://arxiv.org/abs/2510.08138)
*Chengzhi Li,Heyan Huang,Ping Jian,Zhen Yang,Yaning Tian*

Main category: cs.CV

TL;DR: 本文主要关注视频-语言大模型（Video-LLMs）在逻辑自洽性方面存在的问题，提出了一种提升模型时间感知能力的新方法，并通过实验验证了效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常出现自相矛盾的输出，影响实际应用可靠性。在视频-语言模型中，该问题更突出，但成因尚未被深入研究。作者旨在探究其根本原因并提出解决方案。

Method: 作者采用了解释性驱动方法，统计分析导致视频-语言模型回答不一致的潜在因素。主要发现是模型的跨模态注意力头难以区分不同时间点的视频信息。为此，作者提出了暂态条件注意力锐化（TCAS）方法，通过增强注意力区分目标提升模型的时间分辨能力和理解一致性。

Result: 通过实验，TCAS方法显著提升了模型在时间逻辑一致性方面的表现。进一步解释性分析表明，该方法确实提升了注意力头对时序信息的区分能力并取得了通用视频时间定位等任务的性能提升。

Conclusion: 时序逻辑一致性是视频时间理解中的核心瓶颈。TCAS方法通过增强一致性推动了该领域的发展，有效提升了视频-语言大模型的实际应用能力。

Abstract: Large language models (LLMs) often generate self-contradictory outputs, which
severely impacts their reliability and hinders their adoption in practical
applications. In video-language models (Video-LLMs), this phenomenon recently
draws the attention of researchers. Specifically, these models fail to provide
logically consistent responses to rephrased questions based on their grounding
outputs. However, the underlying causes of this phenomenon remain
underexplored. In this work, we adopt an interpretability-driven approach to
analyze, statistically summarize, and intervention the potential factors of the
phenomenon. We find that one of the primary reasons for the inconsistency in
responses lies in the inability of cross-modal attention heads to effectively
distinguish video tokens across different timestamps. To address this, we
propose an attention enhancement method called Temporally Conditioned Attention
Sharpening (TCAS), which constructs an enhancement objective based on attention
distinctions to enhance the model's temporal resolution capability, thereby
improving its temporal understanding logic consistency. Experimental results
demonstrate that our method significantly enhances the temporal logic
consistency of Video-LLMs. Further interpretability analyses reveal that our
method indeed improves the temporal discriminability of attention heads,
validating our conclusions. Additionally, our method achieves performance
improvements in general video temporal grounding tasks, highlighting that
temporal logic consistency is a bottleneck in temporal understanding. By
enhancing consistency, our method drives significant progress in video temporal
understanding.

</details>


### [65] [UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution](https://arxiv.org/abs/2510.08143)
*Shian Du,Menghan Xia,Chang Liu,Quande Liu,Xintao Wang,Pengfei Wan,Xiangyang Ji*

Main category: cs.CV

TL;DR: 本文提出了一种新的统一生成式视频超分辨率框架UniMMVSR，支持多模态条件（文本、图像、视频），并实现对4K视频的多模态精准生成，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前级联视频超分辨率方法受制于仅支持文本这一单一生成条件，无法充分利用多模态条件（如图像、视频）以保证生成视频的保真度，因此有必要开发支持多模态条件的生成框架。

Method: 提出UniMMVSR框架，在潜变量视频扩散模型中系统探讨了多种条件注入、训练和数据混合策略。设计了差异化的数据构建及条件利用方法，使模型能够精准利用不同类型的生成条件，增强多模态适应能力。

Result: 实验表明，UniMMVSR在多模态条件下生成视频的细节和契合度均明显优于已有方法，并首次实现了结合底座模型进行4K视频多模态引导生成。

Conclusion: UniMMVSR扩展了视频超分辨率的生成能力，支持多模态条件，在提升性能的同时，推动了4K高清多模态视频生成的发展。

Abstract: Cascaded video super-resolution has emerged as a promising technique for
decoupling the computational burden associated with generating high-resolution
videos using large foundation models. Existing studies, however, are largely
confined to text-to-video tasks and fail to leverage additional generative
conditions beyond text, which are crucial for ensuring fidelity in multi-modal
video generation. We address this limitation by presenting UniMMVSR, the first
unified generative video super-resolution framework to incorporate hybrid-modal
conditions, including text, images, and videos. We conduct a comprehensive
exploration of condition injection strategies, training schemes, and data
mixture techniques within a latent video diffusion model. A key challenge was
designing distinct data construction and condition utilization methods to
enable the model to precisely utilize all condition types, given their varied
correlations with the target video. Our experiments demonstrate that UniMMVSR
significantly outperforms existing methods, producing videos with superior
detail and a higher degree of conformity to multi-modal conditions. We also
validate the feasibility of combining UniMMVSR with a base model to achieve
multi-modal guided generation of 4K video, a feat previously unattainable with
existing techniques.

</details>


### [66] [Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence Reasoning for Image Editing](https://arxiv.org/abs/2510.08157)
*Zhentao Zou,Zhengrong Yue,Kunpeng Du,Binlei Bao,Hanting Li,Haizhen Xie,Guozheng Xu,Yue Zhou,Yali Wang,Jie Hu,Xue Jiang,Xinghao Chen*

Main category: cs.CV

TL;DR: 提出了一种多模态推理编辑（MURE）框架，使自然语言指导的图像编辑能更精确地处理复杂对象及空间关系。


<details>
  <summary>Details</summary>
Motivation: 当前基于自然语言的图像编辑技术无法有效处理复杂物体交叉和细粒度空间关系，主要原因是缺乏显式的推理流程，已有的CoT仅靠文本或简单坐标表示，无法描述复杂视觉布局并缺乏精细视觉引导。

Method: 该方法提出将文本推理和视觉线索（如掩膜和生成内容）交织，形成本地化的多模态推理链。每一步编辑由文本描述和对应的视觉线索（掩膜/内容）组成，并设计了多模态深度置信推理范式，通过奖励模型打分，剪掉低质量路径，确保推理链高质量推进。构建了涵盖14K高质量实例的CoT-Edit-14K数据集。

Result: 实验表明，在三个图像编辑基准测试上，MURE方法在编辑精度和可控性上显著优于现有方法。

Conclusion: 该框架将复杂图像编辑分解为互相关联的子任务，大幅提升了编辑的精度和可信度，为多模态推理和自然语言图像编辑提供了新思路和数据资源。

Abstract: Image editing with natural language has gained significant popularity, yet
existing methods struggle with intricate object intersections and fine-grained
spatial relationships due to the lack of an explicit reasoning process. While
Chain-of-Thought (CoT) has been explored to enhance reasoning, purely textual
CoT or CoT augmented with coordinate information is fundamentally limited in
its ability to represent intricate visual layouts and lacks the necessary
visual cues to guide the generation of fine-grained, pixel-level details. To
address these challenges, we propose Multimodal Reasoning Edit (MURE), a novel
framework that shifts the visual editing process from purely text-based
reasoning to a series of interleaved textual and visual rationales. Our
framework performs image editing using a natively multimodal, interleaved
text-image CoT. This approach generates a step-by-step chain of reasoning where
a textual description is followed by a corresponding visual cue, such as a
positional mask that defined intended edited regions or a representation of new
content. Furthermore, to mitigate the hallucination phenomenon of large
language models, we introduce Multimodal Deep Confidence (MMDC) reasoning
paradigm. This paradigm explores a tree of visual reasoning paths at each step.
By pruning low-quality branches using a deep confidence score from a reward
model, it ensures the model consistently follows a high-quality trajectory
towards the final edited result. The proposed method decomposes complex editing
tasks into interdependent sub-tasks, achieving greater precision at each stage
and yielding high-fidelity edited results. We define the formulation for
interleaved text-image chains and release the first CoT-Edit-14K dataset,
comprising 14K high-quality editing examples. Extensive experiments show that
our method yields significant improvements across three image editing
benchmarks.

</details>


### [67] [Robust Canonicalization through Bootstrapped Data Re-Alignment](https://arxiv.org/abs/2510.08178)
*Johann Schmidt,Sebastian Stober*

Main category: cs.CV

TL;DR: 本文提出了一种用于细粒度视觉分类（FGVC）任务的自举对齐方法，在无需重度数据增强或高成本的等变模型的情况下，有效提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类任务对于几何偏差（如目标的不同角度和尺度）和噪声非常敏感；传统增强或模型结构虽可缓解但成本高或假设苛刻，亟需通用且高效的新方法。

Method: 提出了一种自举对齐算法，通过迭代重对齐训练样本，递减样本间的变异性，逐步复原数据的对齐假设，无需假定完全对齐的训练数据，并结合理论分析证明算法收敛。

Result: 在四个FGVC基准测试上，该方法在保持与数据增强相当性能的同时，超越了等变架构及传统标准化方法。

Conclusion: 本文提出的方法在细粒度视觉任务中能更高效地应对几何干扰，为对齐假设不完全成立的实际场景提供更稳健的解决思路。

Abstract: Fine-grained visual classification (FGVC) tasks, such as insect and bird
identification, demand sensitivity to subtle visual cues while remaining robust
to spatial transformations. A key challenge is handling geometric biases and
noise, such as different orientations and scales of objects. Existing remedies
rely on heavy data augmentation, which demands powerful models, or on
equivariant architectures, which constrain expressivity and add cost.
Canonicalization offers an alternative by shielding such biases from the
downstream model. In practice, such functions are often obtained using
canonicalization priors, which assume aligned training data. Unfortunately,
real-world datasets never fulfill this assumption, causing the obtained
canonicalizer to be brittle. We propose a bootstrapping algorithm that
iteratively re-aligns training samples by progressively reducing variance and
recovering the alignment assumption. We establish convergence guarantees under
mild conditions for arbitrary compact groups, and show on four FGVC benchmarks
that our method consistently outperforms equivariant, and canonicalization
baselines while performing on par with augmentation.

</details>


### [68] [InstructUDrag: Joint Text Instructions and Object Dragging for Interactive Image Editing](https://arxiv.org/abs/2510.08181)
*Haoran Yu,Yi Shi*

Main category: cs.CV

TL;DR: 本文提出了一种名为InstructUDrag的扩散模型框架，结合文本指令与对象拖拽来实现灵活且高保真的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本的图像编辑方法难以实现对象精确定位，而对象拖拽方法又仅能实现静态位置移动，缺乏语义层次的编辑能力。因此，亟需一种能同时实现高精度位置调整和语义编辑的新方法。

Method: InstructUDrag框架将对象拖拽视为图像重建过程，设计了两个协同分支：（1）移动-重建分支，采用基于能量的梯度引导和交叉注意力图优化，实现对象的精准移动；（2）文本驱动编辑分支，与重建分支共享梯度，实现对象属性的细粒度控制。同时，借助DDPM反演和噪声图先验信息注入，保留被移动对象的结构。

Result: 大量实验显示，InstructUDrag能够实现高保真的图像编辑，既具对象位置的高精度移动，又具对象语义内容的灵活操控。

Conclusion: InstructUDrag能够弥补现有方法在对象定位和语义控制方面的不足，为图像编辑提供了更强的灵活性和高保真度。

Abstract: Text-to-image diffusion models have shown great potential for image editing,
with techniques such as text-based and object-dragging methods emerging as key
approaches. However, each of these methods has inherent limitations: text-based
methods struggle with precise object positioning, while object dragging methods
are confined to static relocation. To address these issues, we propose
InstructUDrag, a diffusion-based framework that combines text instructions with
object dragging, enabling simultaneous object dragging and text-based image
editing. Our framework treats object dragging as an image reconstruction
process, divided into two synergistic branches. The moving-reconstruction
branch utilizes energy-based gradient guidance to move objects accurately,
refining cross-attention maps to enhance relocation precision. The text-driven
editing branch shares gradient signals with the reconstruction branch, ensuring
consistent transformations and allowing fine-grained control over object
attributes. We also employ DDPM inversion and inject prior information into
noise maps to preserve the structure of moved objects. Extensive experiments
demonstrate that InstructUDrag facilitates flexible, high-fidelity image
editing, offering both precision in object relocation and semantic control over
image content.

</details>


### [69] [Fine-grained text-driven dual-human motion generation via dynamic hierarchical interaction](https://arxiv.org/abs/2510.08260)
*Mu Li,Yin Wang,Zhiying Leng,Jiapeng Liu,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: 本文提出了一种精细的双人动作生成方法FineDual，以更有效地模拟人类交互中的动态性与层次性，相较于以往方法在精确度和质量上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有双人动作生成方法大多忽略了交互的距离动态变化和层次结构，无法充分模拟真实的人类交互过程。本文致力于解决这些不足，以提升生成动作的自然性和互动性。

Method: FineDual方法采用三阶段架构：(1) 自学习阶段，利用大模型将整体文本划分为个体文本，并进行文本与动作特征对齐；(2) 自适应调整阶段，引入距离预测器与基于交互图神经网络，动态建模个体间的互动关系；(3) 教师指导优化阶段，用整体文本特征优化动作特征，提升动作细粒度质量。

Result: 在多个双人动作数据集上，FineDual在定量和定性评估中均优于现有方法，能更好地捕捉和生成动态层次化的人体交互动作。

Conclusion: FineDual有效引入了动态性和层次性到双人动作生成模型中，显著提升了生成动作的细致性和可信度，为交互式动作生成提供了新思路。

Abstract: Human interaction is inherently dynamic and hierarchical, where the dynamic
refers to the motion changes with distance, and the hierarchy is from
individual to inter-individual and ultimately to overall motion. Exploiting
these properties is vital for dual-human motion generation, while existing
methods almost model human interaction temporally invariantly, ignoring
distance and hierarchy. To address it, we propose a fine-grained dual-human
motion generation method, namely FineDual, a tri-stage method to model the
dynamic hierarchical interaction from individual to inter-individual. The first
stage, Self-Learning Stage, divides the dual-human overall text into individual
texts through a Large Language Model, aligning text features and motion
features at the individual level. The second stage, Adaptive Adjustment Stage,
predicts interaction distance by an interaction distance predictor, modeling
human interactions dynamically at the inter-individual level by an
interaction-aware graph network. The last stage, Teacher-Guided Refinement
Stage, utilizes overall text features as guidance to refine motion features at
the overall level, generating fine-grained and high-quality dual-human motion.
Extensive quantitative and qualitative evaluations on dual-human motion
datasets demonstrate that our proposed FineDual outperforms existing
approaches, effectively modeling dynamic hierarchical human interaction.

</details>


### [70] [Adaptive Gradient Calibration for Single-Positive Multi-Label Learning in Remote Sensing Image Scene Classification](https://arxiv.org/abs/2510.08269)
*Chenying Liu,Gianmarco Perantoni,Lorenzo Bruzzone,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种针对遥感图像的单正类多标签学习（SPML）框架AdaGC，有效提升了在弱标签监督下的多标签分类性能，并在多种数据和噪声环境下取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中多标签分类（MLC）能比单标签分类（SLC）捕捉更全面的语义信息，但完整标注极其昂贵。单正类多标签学习（SPML）作为替代仅需每张图片一个标签，但带来监督歧义，且适用于遥感领域的研究有限。

Method: 提出了AdaGC框架，结合了梯度校准（GC）、Mixup和双指数滑动平均（EMA）模块生成鲁棒的伪标签，并引入了动态指标自适应激活GC以防止噪声过拟合。

Result: 在两个遥感数据集、两种标签噪声情况下，AdaGC达到SOTA表现，并展现出强健的鲁棒性。

Conclusion: AdaGC在遥感领域促进了SPML的发展，为弱监督多标签分类任务提供了一种高效、泛化性强的解决方案。

Abstract: Multi-label classification (MLC) offers a more comprehensive semantic
understanding of Remote Sensing (RS) imagery compared to traditional
single-label classification (SLC). However, obtaining complete annotations for
MLC is particularly challenging due to the complexity and high cost of the
labeling process. As a practical alternative, single-positive multi-label
learning (SPML) has emerged, where each image is annotated with only one
relevant label, and the model is expected to recover the full set of labels.
While scalable, SPML introduces significant supervision ambiguity, demanding
specialized solutions for model training. Although various SPML methods have
been proposed in the computer vision domain, research in the RS context remains
limited. To bridge this gap, we propose Adaptive Gradient Calibration (AdaGC),
a novel and generalizable SPML framework tailored to RS imagery. AdaGC adopts a
gradient calibration (GC) mechanism combined with Mixup and a dual exponential
moving average (EMA) module for robust pseudo-label generation. To maximize
AdaGC's effectiveness, we introduce a simple yet theoretically grounded
indicator to adaptively trigger GC after an initial warm-up stage based on
training dynamics, thereby guaranteeing the effectiveness of GC in mitigating
overfitting to label noise. Extensive experiments on two benchmark RS datasets
under two distinct label noise types demonstrate that AdaGC achieves
state-of-the-art (SOTA) performance while maintaining strong robustness across
diverse settings.

</details>


### [71] [One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion Models for Text-Guided Image Inpainting](https://arxiv.org/abs/2510.08273)
*Haipeng Liu,Yang Wang,Meng Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的频率感知扩散模型，用于文本指导的图像修复，有效提升了修复区域与未遮挡区域的语义一致性，并更好地保护了未遮挡区域。


<details>
  <summary>Details</summary>
Motivation: 现有的文本指导图像修复方法难以同时保证未遮挡区域的保真性和修复区域与原图的语义一致性，原因在于不同频率成分的特征响应和鲁棒性不同。

Method: 作者提出了NTN-Diff模型，将高、中、低频成分在扩散去噪过程中解耦，分阶段处理去噪任务。其中中频分量用作引导低频分量去噪，并推动修复区域与未遮挡区域的多频段语义对齐。

Result: 大量实验验证了NTN-Diff较现有先进扩散模型在文本引导的图像修复任务上有更优表现。

Conclusion: NTN-Diff能更有效地保护未遮挡区域，同时提升修复区域与未遮挡区域的多频段语义一致性，是文本指导图像修复的新进展。

Abstract: Text-guided image inpainting aims at reconstructing the masked regions as per
text prompts, where the longstanding challenges lie in the preservation for
unmasked regions, while achieving the semantics consistency between unmasked
and inpainted masked regions. Previous arts failed to address both of them,
always with either of them to be remedied. Such facts, as we observed, stem
from the entanglement of the hybrid (e.g., mid-and-low) frequency bands that
encode varied image properties, which exhibit different robustness to text
prompts during the denoising process. In this paper, we propose a
null-text-null frequency-aware diffusion models, dubbed \textbf{NTN-Diff}, for
text-guided image inpainting, by decomposing the semantics consistency across
masked and unmasked regions into the consistencies as per each frequency band,
while preserving the unmasked regions, to circumvent two challenges in a row.
Based on the diffusion process, we further divide the denoising process into
early (high-level noise) and late (low-level noise) stages, where the
mid-and-low frequency bands are disentangled during the denoising process. As
observed, the stable mid-frequency band is progressively denoised to be
semantically aligned during text-guided denoising process, which, meanwhile,
serves as the guidance to the null-text denoising process to denoise
low-frequency band for the masked regions, followed by a subsequent text-guided
denoising process at late stage, to achieve the semantics consistency for
mid-and-low frequency bands across masked and unmasked regions, while preserve
the unmasked regions. Extensive experiments validate the superiority of
NTN-Diff over the state-of-the-art diffusion models to text-guided diffusion
models. Our code can be accessed from https://github.com/htyjers/NTN-Diff.

</details>


### [72] [Learning Neural Exposure Fields for View Synthesis](https://arxiv.org/abs/2510.08279)
*Michael Niemeyer,Fabian Manhardt,Marie-Julie Rakotosaona,Michael Oechsle,Christina Tsalicoglou,Keisuke Tateno,Jonathan T. Barron,Federico Tombari*

Main category: cs.CV

TL;DR: 该论文提出了一种新的3D场景重建方法NExF，能够在曝光强烈变化或动态范围大的现实数据下实现高质量且一致性的三维重建和视角合成。


<details>
  <summary>Details</summary>
Motivation: 现有神经场景重建方法在曝光变化大（如室内外共存、窗户等光线强变化）场景中的效果较差，难以保证输出质量，因此需要新的方法提升鲁棒性。

Method: 提出Neural Exposure Fields (NExF)，通过为每个3D点预测最优曝光值，将曝光与神经场景表示进行联合优化。采用新型的神经调控机制，无需多曝光采集或后处理，直接在3D空间中进行优化，提升动态范围场景下的效果。

Result: 该方法在复杂真实场景数据上显示出比现有最佳baseline高出55%的性能，且训练速度更快。

Conclusion: NExF有效提升了高动态范围和曝光变化场景下的3D重建效果，实现了高质量、三维一致性的视角合成，优于现有方法。

Abstract: Recent advances in neural scene representations have led to unprecedented
quality in 3D reconstruction and view synthesis. Despite achieving high-quality
results for common benchmarks with curated data, outputs often degrade for data
that contain per image variations such as strong exposure changes, present,
e.g., in most scenes with indoor and outdoor areas or rooms with windows. In
this paper, we introduce Neural Exposure Fields (NExF), a novel technique for
robustly reconstructing 3D scenes with high quality and 3D-consistent
appearance from challenging real-world captures. In the core, we propose to
learn a neural field predicting an optimal exposure value per 3D point,
enabling us to optimize exposure along with the neural scene representation.
While capture devices such as cameras select optimal exposure per image/pixel,
we generalize this concept and perform optimization in 3D instead. This enables
accurate view synthesis in high dynamic range scenarios, bypassing the need of
post-processing steps or multi-exposure captures. Our contributions include a
novel neural representation for exposure prediction, a system for joint
optimization of the scene representation and the exposure field via a novel
neural conditioning mechanism, and demonstrated superior performance on
challenging real-world data. We find that our approach trains faster than prior
works and produces state-of-the-art results on several benchmarks improving by
over 55% over best-performing baselines.

</details>


### [73] [LTCA: Long-range Temporal Context Attention for Referring Video Object Segmentation](https://arxiv.org/abs/2510.08305)
*Cilin Yan,Jingyun Wang,Guoliang Kang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的长时序上下文注意力机制（LTCA），通过稀疏局部和随机全局注意力相结合，提升引用视频分割任务中的全局与局部信息提取能力，方法在多个数据集上取得了新的最佳表现。


<details>
  <summary>Details</summary>
Motivation: 以往的视频分割方法难以在全局性和局部性之间平衡，尤其是对长视频而言，计算成本急剧增加，因此需要一种既能有效获取时序上下文，又能兼顾计算效率的解决方案。

Method: 提出了LTCA机制，通过堆叠空洞窗口局部注意力实现跨帧的时序信息聚合，并在局部注意力之外，让每个查询还可以随机关注来自全球池的少量关键点；同时引入全局查询与所有查询交互，进一步编码全局上下文信息。

Result: 在四个主流的引用视频分割基准上，方法取得了最新的state-of-the-art性能，在MeViS valu和val数据集上分别提升了11.3%和8.3%。

Conclusion: 所提LTCA机制能够在保证效率的同时，有效提升表达全局与动态属性的能力，为引用视频分割任务提供了更优解。

Abstract: Referring Video Segmentation (RVOS) aims to segment objects in videos given
linguistic expressions. The key to solving RVOS is to extract long-range
temporal context information from the interactions of expressions and videos to
depict the dynamic attributes of each object. Previous works either adopt
attention across all the frames or stack dense local attention to achieve a
global view of temporal context. However, they fail to strike a good balance
between locality and globality, and the computation complexity significantly
increases with the increase of video length. In this paper, we propose an
effective long-range temporal context attention (LTCA) mechanism to aggregate
global context information into object features. Specifically, we aggregate the
global context information from two aspects. Firstly, we stack sparse local
attentions to balance the locality and globality. We design a dilated window
attention across frames to aggregate local context information and perform such
attention in a stack of layers to enable a global view. Further, we enable each
query to attend to a small group of keys randomly selected from a global pool
to enhance the globality. Secondly, we design a global query to interact with
all the other queries to directly encode the global context information.
Experiments show our method achieves new state-of-the-art on four referring
video segmentation benchmarks. Notably, our method shows an improvement of
11.3% and 8.3% on the MeViS valu and val datasets respectively.

</details>


### [74] [Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge](https://arxiv.org/abs/2510.08316)
*Yu Huang,Zelin Peng,Changsong Wen,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D可供性分割方法，利用2D视觉大模型的语义知识，通过跨模态预训练提升3D分割表现，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 传统3D可供性分割方法多依赖点云编码器作为特征提取器，忽略了3D数据稀疏、噪声和几何模糊等挑战，得到的特征难以明确和一致划分语义功能区域，阻碍了下游任务。

Method: 作者提出了CMAT（Cross-Modal Affinity Transfer）预训练策略，将2D视觉基础模型的丰富语义融入3D编码器，通过对齐3D-2D语义，并联合优化重建、亲和力和多样性，获得结构化的功能性表示，并基于此设计了CAST（Cross-modal Affordance Segmentation Transformer），融合多模态提示和预训练特征，实现高精度、可指引的分割。

Result: 在标准数据集上的大量实验表明，该框架在3D可供性分割任务上建立了新的最优表现，优于现有方法。

Conclusion: 提出的CMAT与CAST方法有效将2D语义知识迁移到3D可供性分割中，提升了功能区域的区分度和分割精度，为机器人操作、虚拟现实等领域提供了更强的技术基础。

Abstract: Affordance segmentation aims to parse 3D objects into functionally distinct
parts, bridging recognition and interaction for applications in robotic
manipulation, embodied AI, and AR. While recent studies leverage visual or
textual prompts to guide this process, they often rely on point cloud encoders
as generic feature extractors, overlooking the intrinsic challenges of 3D data
such as sparsity, noise, and geometric ambiguity. As a result, 3D features
learned in isolation frequently lack clear and semantically consistent
functional boundaries. To address this bottleneck, we propose a
semantic-grounded learning paradigm that transfers rich semantic knowledge from
large-scale 2D Vision Foundation Models (VFMs) into the 3D domain.
Specifically, We introduce Cross-Modal Affinity Transfer (CMAT), a pre-training
strategy that aligns a 3D encoder with lifted 2D semantics and jointly
optimizes reconstruction, affinity, and diversity to yield semantically
organized representations. Building on this backbone, we further design the
Cross-modal Affordance Segmentation Transformer (CAST), which integrates
multi-modal prompts with CMAT-pretrained features to generate precise,
prompt-aware segmentation maps. Extensive experiments on standard benchmarks
demonstrate that our framework establishes new state-of-the-art results for 3D
affordance segmentation.

</details>


### [75] [LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation](https://arxiv.org/abs/2510.08318)
*Yushi Huang,Xingtong Ge,Ruihao Gong,Chengtao Lv,Jun Zhang*

Main category: cs.CV

TL;DR: LinVideo是一种高效无数据的后训练方法，可在不牺牲视频扩散模型生成质量的前提下，将部分自注意力模块自动转为线性注意力，实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型合成高质量视频但算力消耗极高，主因在于自注意力机制随序列长度增加复杂度呈二次增长。虽然线性注意力可降本，但直接全替需昂贵预训练，实际不可行。论文旨在提出无需重训练、可快速高效部署的替代方案。

Method: 作者提出LinVideo：采用无数据的后训练框架，通过二分类方法自动选择可替换为线性注意力的层，用选择性迁移策略逐步转换。并设计anytime distribution matching（ADM）目标函数，高效对齐采样分布，保证转换质量与效率。

Result: 方法实现1.25-2倍模型推理加速，几乎不影响视频生成质量。进一步，4步蒸馏后模型最大延迟降低达15.92倍，生成质量损失极小。

Conclusion: LinVideo能对现有高质量视频扩散模型进行高效改造，在极大提升推理效率的同时保持出色视频质量，无需原始数据和大规模重训练。

Abstract: Video diffusion models (DMs) have enabled high-quality video synthesis.
However, their computation costs scale quadratically with sequence length
because self-attention has quadratic complexity. While linear attention lowers
the cost, fully replacing quadratic attention requires expensive pretraining
due to the limited expressiveness of linear attention and the complexity of
spatiotemporal modeling in video generation. In this paper, we present
LinVideo, an efficient data-free post-training framework that replaces a target
number of self-attention modules with linear attention while preserving the
original model's performance. First, we observe a significant disparity in the
replaceability of different layers. Instead of manual or heuristic choices, we
frame layer selection as a binary classification problem and propose selective
transfer, which automatically and progressively converts layers to linear
attention with minimal performance impact. Additionally, to overcome the
ineffectiveness and inefficiency of existing objectives for this transfer
process, we introduce an anytime distribution matching (ADM) objective that
aligns the distributions of samples across any timestep along the sampling
trajectory. This objective is efficient and recovers model performance.
Extensive experiments show that our method achieves a 1.25-2.00x speedup while
preserving generation quality, and our 4-step distilled model further delivers
a 15.92x latency reduction with minimal visual quality drop.

</details>


### [76] [Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception](https://arxiv.org/abs/2510.08352)
*Nikos Theodoridis,Tim Brophy,Reenu Mohandas,Ganesh Sistu,Fiachra Collins,Anthony Scanlan,Ciaran Eising*

Main category: cs.CV

TL;DR: 该论文提出了DTPQA数据集，专注于自动驾驶场景下的视觉-语言模型感知能力评估，并发现当前主流小型VLMs在该任务下感知能力远低于人类。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言大模型（VLMs）在许多任务上表现优异，并具备良好的泛化能力，但在自动驾驶等安全关键领域，可靠的感知系统尤为重要，且需要模型具备远近距离的感知能力，现有评测严重不足。因此，作者希望设计一个侧重感知、带有距离标注的VQA基准数据集，以填补该评估空白。

Method: 作者提出了DTPQA数据集，这是专门针对交通场景中仅含感知类（不含推理类）问题的VQA数据集，并引入了距离信息注释。文中在该数据集上针对多种主流小型VLMs进行了性能评估，与人类表现进行了对比。

Result: 最优秀的小型VLM在DTPQA数据集上的准确率约为60%，明显低于人类的85%左右；其中，如区分左右等感知任务对模型来说尤其困难。此外，受限于硬件以及人类样本量有限，统计分析有一定局限。

Conclusion: 当前小型VLM在自动驾驶感知相关任务上的表现依然不令人满意，尤其是在距离、多样化感知任务中难以达到人类水平，提示需要进一步优化模型结构与感知特性。

Abstract: Vision-Language Models (VLMs) are becoming increasingly powerful,
demonstrating strong performance on a variety of tasks that require both visual
and textual understanding. Their strong generalisation abilities make them a
promising component for automated driving systems, which must handle unexpected
corner cases. However, to be trusted in such safety-critical applications, a
model must first possess a reliable perception system. Moreover, since critical
objects and agents in traffic scenes are often at a distance, we require
systems that are not "shortsighted", i.e., systems with strong perception
capabilities at both close (up to 20 meters) and long (30+ meters) range. With
this in mind, we introduce Distance-Annotated Traffic Perception Question
Answering (DTPQA), the first Visual Question Answering (VQA) benchmark focused
solely on perception-based questions in traffic scenes, enriched with distance
annotations. By excluding questions that require reasoning, we ensure that
model performance reflects perception capabilities alone. Since automated
driving hardware has limited processing power and cannot support large VLMs,
our study centers on smaller VLMs. More specifically, we evaluate several
state-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the
simplicity of the questions, these models significantly underperform compared
to humans (~60% average accuracy for the best-performing small VLM versus ~85%
human performance). However, it is important to note that the human sample size
was relatively small, which imposes statistical limitations. We also identify
specific perception tasks, such as distinguishing left from right, that remain
particularly challenging for these models.

</details>


### [77] [SPICE: Simple and Practical Image Clarification and Enhancement](https://arxiv.org/abs/2510.08358)
*Alexander Belyaev,Pierre-Alain Fayolle,Michael Cohen*

Main category: cs.CV

TL;DR: 本文提出了一种简单高效的图像增强与澄清方法，专门针对低光图像和雾气图像的增强，效果优于现有主流技术且实现简单。


<details>
  <summary>Details</summary>
Motivation: 低光和雾气（如雾霾、沙尘或水下）图像在许多实际场景下较为常见，但传统增强方法效果有限。亟需一种既高效又易实现的方法来提升这类图像质量。

Method: 作者设计了一种图像滤波器以模拟低光或雾气环境，并基于此推导出近似逆滤波器，对图像进行增强和去雾。该方法通过极简的代码即可实现。

Result: 实验结果表明，作者提出的方法在极暗和雾霾图像的增强任务上，表现非常有竞争力，甚至超过了当前的一些先进方法。

Conclusion: 本文方法不仅效果突出，还具有极强的简易性和实用性，对于相关图像增强任务有很大应用价值。

Abstract: We introduce a simple and efficient method to enhance and clarify images.
More specifically, we deal with low light image enhancement and clarification
of hazy imagery (hazy/foggy images, images containing sand dust, and underwater
images). Our method involves constructing an image filter to simulate low-light
or hazy conditions and deriving approximate reverse filters to minimize
distortions in the enhanced images. Experimental results show that our approach
is highly competitive and often surpasses state-of-the-art techniques in
handling extremely dark images and in enhancing hazy images. A key advantage of
our approach lies in its simplicity: Our method is implementable with just a
few lines of MATLAB code.

</details>


### [78] [Hyperspectral data augmentation with transformer-based diffusion models](https://arxiv.org/abs/2510.08363)
*Mattia Ferrari,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 本论文提出了一种基于引导扩散模型的数据增强方法，应用于小样本高光谱遥感影像的地物分类，显著提升了分类准确率，并改善了模型的训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 高光谱卫星数据结合深度学习能提升地物分类精度，但小样本情况下深度学习易过拟合，限制了其实用性，因此需创新的数据增强技术以缓解此问题。

Method: 提出结合引导扩散模型与轻量化Transformer网络进行数据增强，对小规模标注样本进行合成与增强，并使用改进的加权损失函数及余弦变异调度器，提升小数据集上的训练效率与效果。

Result: 在PRISMA卫星高光谱影像的森林分类任务（10类）上，所提方法在平均准确率和加权平均准确率上均优于其他数据增强手段，且训练过程更为稳定。

Conclusion: 所提出的数据增强方法不仅有效改善了小样本深度学习模型的准确率，还缓解了深度生成式模型数据增强在实际应用中的训练不稳定问题。

Abstract: The introduction of new generation hyperspectral satellite sensors, combined
with advancements in deep learning methodologies, has significantly enhanced
the ability to discriminate detailed land-cover classes at medium-large scales.
However, a significant challenge in deep learning methods is the risk of
overfitting when training networks with small labeled datasets. In this work,
we propose a data augmentation technique that leverages a guided diffusion
model. To effectively train the model with a limited number of labeled samples
and to capture complex patterns in the data, we implement a lightweight
transformer network. Additionally, we introduce a modified weighted loss
function and an optimized cosine variance scheduler, which facilitate fast and
effective training on small datasets. We evaluate the effectiveness of the
proposed method on a forest classification task with 10 different forest types
using hyperspectral images acquired by the PRISMA satellite. The results
demonstrate that the proposed method outperforms other data augmentation
techniques in both average and weighted average accuracy. The effectiveness of
the method is further highlighted by the stable training behavior of the model,
which addresses a common limitation in the practical application of deep
generative models for data augmentation.

</details>


### [79] [UniVideo: Unified Understanding, Generation, and Editing for Videos](https://arxiv.org/abs/2510.08377)
*Cong Wei,Quande Liu,Zixuan Ye,Qiulin Wang,Xintao Wang,Pengfei Wan,Kun Gai,Wenhu Chen*

Main category: cs.CV

TL;DR: UniVideo是一个统一的多模态视频生成与编辑模型，结合了MM大型语言模型和多模态DiT，支持多种视频生成与编辑任务，性能超越现有方法，并具备良好的泛化与组合能力。


<details>
  <summary>Details</summary>
Motivation: 多模态内容生成与编辑取得进展，但主流统一模型多局限于图像领域，缺乏对视频领域的统一通用解决方案。

Method: 提出UniVideo框架，采用双流结构：一条用于指令理解（多模态大型语言模型MLLM），一条用于视频生成（多模态DiT，MMDiT）。以统一多模态指令范式联合训练，支持多种视频任务。模型可处理复杂多模态输入，实现多能力组合。

Result: UniVideo在文本/图像生成视频、上下文视频生成和编辑等多项任务上达到甚至超过定制化SOTA；泛化性强，无显式训练的任务如自由视频编辑同样表现良好，视频风格迁移、角色绿幕等未见过任务均可应对。

Conclusion: UniVideo首次为视频域实现多任务统一，具备多任务、强泛化和能力组合能力，对未来多模态内容创作具有重要意义。模型及代码将公开，推动领域发展。

Abstract: Unified multimodal models have shown promising results in multimodal content
generation and editing but remain largely limited to the image domain. In this
work, we present UniVideo, a versatile framework that extends unified modeling
to the video domain. UniVideo adopts a dual-stream design, combining a
Multimodal Large Language Model (MLLM) for instruction understanding with a
Multimodal DiT (MMDiT) for video generation. This design enables accurate
interpretation of complex multimodal instructions while preserving visual
consistency. Built on this architecture, UniVideo unifies diverse video
generation and editing tasks under a single multimodal instruction paradigm and
is jointly trained across them. Extensive experiments demonstrate that UniVideo
matches or surpasses state-of-the-art task-specific baselines in
text/image-to-video generation, in-context video generation and in-context
video editing. Notably, the unified design of UniVideo enables two forms of
generalization. First, UniVideo supports task composition, such as combining
editing with style transfer, by integrating multiple capabilities within a
single instruction. Second, even without explicit training on free-form video
editing, UniVideo transfers its editing capability from large-scale image
editing data to this setting, handling unseen instructions such as
green-screening characters or changing materials within a video. Beyond these
core capabilities, UniVideo also supports visual-prompt-based video generation,
where the MLLM interprets visual prompts and guides the MMDiT during synthesis.
To foster future research, we will release our model and code.

</details>


### [80] [Detecting Legend Items on Historical Maps Using GPT-4o with In-Context Learning](https://arxiv.org/abs/2510.08385)
*Sofia Kirsanova,Yao-Yi Chiang,Weiwei Duan*

Main category: cs.CV

TL;DR: 本文提出一种结合LayoutLMv3和GPT-4o的自动解析历史地图图例的方法，显著提升了图例项与描述的匹配精度。


<details>
  <summary>Details</summary>
Motivation: 历史地图图例布局不一致、结构化差，导致自动解析和信息提取极具挑战性，现有方法多停留在分割或OCR阶段，且少有能结构化匹配图例项与描述的有效方法。

Method: 方法结合了LayoutLMv3进行版面检测，以及GPT-4o通过上下文学习与JSON结构化提示实现图例项与说明的检测与关联，通过预测bounding box实现匹配。

Result: 实验表明，GPT-4o结合结构化JSON提示优于基线方法，获得88% F-1分数和85% IoU；同时分析了提示设计、示例数量和布局对性能的影响。

Conclusion: 该方法能高效、可扩展地解析多样化历史地图图例，提高地图索引和检索能力，有助于处理各种视觉风格的历史地图数据。

Abstract: Historical map legends are critical for interpreting cartographic symbols.
However, their inconsistent layouts and unstructured formats make automatic
extraction challenging. Prior work focuses primarily on segmentation or general
optical character recognition (OCR), with few methods effectively matching
legend symbols to their corresponding descriptions in a structured manner. We
present a method that combines LayoutLMv3 for layout detection with GPT-4o
using in-context learning to detect and link legend items and their
descriptions via bounding box predictions. Our experiments show that GPT-4 with
structured JSON prompts outperforms the baseline, achieving 88% F-1 and 85%
IoU, and reveal how prompt design, example counts, and layout alignment affect
performance. This approach supports scalable, layout-aware legend parsing and
improves the indexing and searchability of historical maps across various
visual styles.

</details>


### [81] [Robust Source-Free Domain Adaptation for Medical Image Segmentation based on Curriculum Learning](https://arxiv.org/abs/2510.08393)
*Ziqi Zhang,Yuexiang Li,Yawen Huang,Nanjun He,Tao Xu,Liwei Lin,Yefeng Zheng,Shaoxin Li,Feiyue Huang*

Main category: cs.CV

TL;DR: 提出了一种基于课程学习（curriculum-based）的源无关域适应（source-free domain adaptation）方法，有效提升了目标域的模型表现，并在医学影像等数据集上取得了最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 源无关域适应在保护数据隐私、提升目标域表现方面具有重要意义，但现有方法大多只关注伪标签优化，缺乏对学习过程的系统设计，特别是从“易到难”与“源到目标”的渐进式知识迁移。

Method: 提出了名为LFC（learning from curriculum）的框架，对目标域样本实施“易到难”的课程学习，并通过“源到目标”的课程稳定适应过程，逐步提升模型的适应能力。该方法突出渐进式（progressive）的知识迁移方案。

Result: 在公开的交叉域眼底图像分割和息肉分割数据集上进行评测，所提方法超越了现有源无关域适应方法，达到了新的SOTA水平。

Conclusion: 课程化渐进适应能有效促进知识迁移和模型泛化能力，是源无关域适应领域值得关注的方法路径，对医学影像安全自适应分析具有现实应用价值。

Abstract: Recent studies have uncovered a new research line, namely source-free domain
adaptation, which adapts a model to target domains without using the source
data. Such a setting can address the concerns on data privacy and security
issues of medical images. However, current source-free domain adaptation
frameworks mainly focus on the pseudo label refinement for target data without
the consideration of learning procedure. Indeed, a progressive learning process
from source to target domain will benefit the knowledge transfer during model
adaptation. To this end, we propose a curriculum-based framework, namely
learning from curriculum (LFC), for source-free domain adaptation, which
consists of easy-to-hard and source-to-target curricula. Concretely, the former
curriculum enables the framework to start learning with `easy' samples and
gradually tune the optimization direction of model adaption by increasing the
sample difficulty. While, the latter can stablize the adaptation process, which
ensures smooth transfer of the model from the source domain to the target. We
evaluate the proposed source-free domain adaptation approach on the public
cross-domain datasets for fundus segmentation and polyp segmentation. The
extensive experimental results show that our framework surpasses the existing
approaches and achieves a new state-of-the-art.

</details>


### [82] [VideoVerse: How Far is Your T2V Generator from a World Model?](https://arxiv.org/abs/2510.08398)
*Zeqing Wang,Xinyu Wei,Bairui Li,Zhen Guo,Jinrui Zhang,Hongyang Wei,Keze Wang,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了VideoVerse，一个针对文本生成视频（T2V）模型的新基准，专注于模型对复杂时间因果关系和世界知识的理解，填补现有评测维度的不足。


<details>
  <summary>Details</summary>
Motivation: 当前T2V模型发展迅速，现有评测方法（如逐帧画质、时序一致性）已无法区分顶尖模型。同时，事件级别的时间因果和世界知识的系统性评测严重缺失，对推动“世界模型”构建具有重要意义。

Method: 作者收集了涵盖多个领域的代表性视频，提取具有内在时间因果性的事件级描述，并由标注者将其生成文本到视频的提示（prompt）。每个提示设计了一系列二元评测问题，覆盖10个静态与动态属性的评测维度。总计有300个提示、815个事件和793个评测问题。评测流程采用结合现代理解能力的视觉-语言模型，以问答方式贴近人工评价。最后，对多个顶尖T2V模型进行了系统评测分析。

Result: VideoVerse基准涵盖丰富的事件和评测维度，使模型在时间因果理解和世界知识掌握上得到多层面考察。实验系统评估了当前开放源和闭源T2V模型在这些新维度下的表现，揭示了它们距“世界模型”目标的差距。

Conclusion: VideoVerse有效补足了现有T2V评测不足，为推动更具世界理解力的文本生成视频模型研究提供了系统工具和分析。

Abstract: The recent rapid advancement of Text-to-Video (T2V) generation technologies,
which are critical to build ``world models'', makes the existing benchmarks
increasingly insufficient to evaluate state-of-the-art T2V models. First,
current evaluation dimensions, such as per-frame aesthetic quality and temporal
consistency, are no longer able to differentiate state-of-the-art T2V models.
Second, event-level temporal causality, which not only distinguishes video from
other modalities but also constitutes a crucial component of world models, is
severely underexplored in existing benchmarks. Third, existing benchmarks lack
a systematic assessment of world knowledge, which are essential capabilities
for building world models. To address these issues, we introduce VideoVerse, a
comprehensive benchmark that focuses on evaluating whether a T2V model could
understand complex temporal causality and world knowledge in the real world. We
collect representative videos across diverse domains (e.g., natural landscapes,
sports, indoor scenes, science fiction, chemical and physical experiments) and
extract their event-level descriptions with inherent temporal causality, which
are then rewritten into text-to-video prompts by independent annotators. For
each prompt, we design a suite of binary evaluation questions from the
perspective of dynamic and static properties, with a total of ten carefully
defined evaluation dimensions. In total, our VideoVerse comprises 300 carefully
curated prompts, involving 815 events and 793 binary evaluation questions.
Consequently, a human preference aligned QA-based evaluation pipeline is
developed by using modern vision-language models. Finally, we perform a
systematic evaluation of state-of-the-art open-source and closed-source T2V
models on VideoVerse, providing in-depth analysis on how far the current T2V
generators are from world models.

</details>


### [83] [Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency](https://arxiv.org/abs/2510.08431)
*Kaiwen Zheng,Yuji Wang,Qianli Ma,Huayu Chen,Jintao Zhang,Yogesh Balaji,Jianfei Chen,Ming-Yu Liu,Jun Zhu,Qinsheng Zhang*

Main category: cs.CV

TL;DR: 本文首次实现了将连续时间一致性模型（sCM）扩展到大规模图像和视频扩散模型，通过改进JVP计算基础设施和FlashAttention-2 JVP核，使其适用于超大参数模型和高维视频任务。针对sCM在细节生成上的局限，提出了一种融合score distillation的新方法rCM，提升了生成质量和多样性，并显著加速了采样速度，优于现有主流蒸馏技术。


<details>
  <summary>Details</summary>
Motivation: 尽管连续时间一致性模型（sCM）在学术规模的扩散模型中有效，但由于JVP运算等基础设施挑战，尚未应用到大规模文本生成图像和视频的场景。此工作旨在攻克相关的硬件和算法瓶颈，使其可推广到实际大模型任务，并解决sCM在细节表现和样本模式覆盖上的固有限制。

Method: 1）开发了支持大模型并行的FlashAttention-2 JVP内核，使得sCM能在10B+参数及高维视频场景中训练；2）发现sCM在高质量细节生成上存在“模覆盖”缺陷，分析其原因；3）提出了score-regularized continuous-time consistency model（rCM），结合score distillation作为长跳正则项，引入了“模寻求”性质，提升了视觉质量与多样性。

Result: rCM在Cosmos-Predict2、Wan2.1等最大14B参数、最长5秒视频的规模上，生成质量可与乃至超越最新的DMD2蒸馏方法，并兼具更高的图像多样性。蒸馏后的新模型采样仅需1-4步，将扩散生成速度提升15-50倍，无需GAN调整和复杂的超参数搜索。

Conclusion: rCM为大规模扩散模型的蒸馏提供了理论扎实且实用的框架，显著提升了效果和效率，有利于实际部署大型图像/视频生成模型，推动领域应用与前沿研究。

Abstract: This work represents the first effort to scale up continuous-time consistency
distillation to general application-level image and video diffusion models.
Although continuous-time consistency model (sCM) is theoretically principled
and empirically powerful for accelerating academic-scale diffusion, its
applicability to large-scale text-to-image and video tasks remains unclear due
to infrastructure challenges in Jacobian-vector product (JVP) computation and
the limitations of standard evaluation benchmarks. We first develop a
parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on
models with over 10 billion parameters and high-dimensional video tasks. Our
investigation reveals fundamental quality limitations of sCM in fine-detail
generation, which we attribute to error accumulation and the "mode-covering"
nature of its forward-divergence objective. To remedy this, we propose the
score-regularized continuous-time consistency model (rCM), which incorporates
score distillation as a long-skip regularizer. This integration complements sCM
with the "mode-seeking" reverse divergence, effectively improving visual
quality while maintaining high generation diversity. Validated on large-scale
models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM
matches or surpasses the state-of-the-art distillation method DMD2 on quality
metrics while offering notable advantages in diversity, all without GAN tuning
or extensive hyperparameter searches. The distilled models generate
high-fidelity samples in only $1\sim4$ steps, accelerating diffusion sampling
by $15\times\sim50\times$. These results position rCM as a practical and
theoretically grounded framework for advancing large-scale diffusion
distillation.

</details>


### [84] [Hierarchical Spatial Algorithms for High-Resolution Image Quantization and Feature Extraction](https://arxiv.org/abs/2510.08449)
*Noor Islam S. Mohammad*

Main category: cs.CV

TL;DR: 本文提出了一种模块化空间图像处理框架，涵盖灰度量化、颜色亮度增强、图像锐化、双向转换及几何特征提取，实验证明其在多数据集上具备强健性和实时性潜力。


<details>
  <summary>Details</summary>
Motivation: 当前图像处理流程复杂、功能模块离散，亟需一种集成多种图像增强和特征提取的统一框架，以满足实时计算机视觉对稳健性和高效性的需求。

Method: 该框架包括：1) 用分步强度变换对灰度图像进行八级量化，实现结构细节保留的海报化效果；2) 利用RGB与YCrCb空间直方图均衡进行颜色增强并提升对比度；3) HSV空间亮度通道调节亮度；4) 采用3*3卷积核进行锐化；5) 构建集不锐化掩蔽、伽马校正与噪声增强于一体的双向变换管道，并评估正反向处理准确度；6) 用Canny边缘检测、Hough直线估计、Harris角点检测、形态窗定位与物体分离进行几何特征提取。

Result: 双向变换流程正向和反向处理准确率分别为76.10%和74.80%；台球杆对齐的直线估计角度为51.50°，杆体分离与真实值相似度达81.87%。在多样化数据集上的实验显示框架具备鲁棒性和确定性表现。

Conclusion: 该框架集成多种空间图像处理技术，具备模块化、强健和实时性，为计算机视觉和图像分析提供了实用方案。

Abstract: This study introduces a modular framework for spatial image processing,
integrating grayscale quantization, color and brightness enhancement, image
sharpening, bidirectional transformation pipelines, and geometric feature
extraction. A stepwise intensity transformation quantizes grayscale images into
eight discrete levels, producing a posterization effect that simplifies
representation while preserving structural detail. Color enhancement is
achieved via histogram equalization in both RGB and YCrCb color spaces, with
the latter improving contrast while maintaining chrominance fidelity.
Brightness adjustment is implemented through HSV value-channel manipulation,
and image sharpening is performed using a 3 * 3 convolution kernel to enhance
high-frequency details. A bidirectional transformation pipeline that integrates
unsharp masking, gamma correction, and noise amplification achieved accuracy
levels of 76.10% and 74.80% for the forward and reverse processes,
respectively. Geometric feature extraction employed Canny edge detection,
Hough-based line estimation (e.g., 51.50{\deg} for billiard cue alignment),
Harris corner detection, and morphological window localization. Cue isolation
further yielded 81.87\% similarity against ground truth images. Experimental
evaluation across diverse datasets demonstrates robust and deterministic
performance, highlighting its potential for real-time image analysis and
computer vision.

</details>


### [85] [Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools](https://arxiv.org/abs/2510.08480)
*Zhenlong Yuan,Xiangyan Qu,Chengxuan Qian,Rui Chen,Jing Tang,Lei Sun,Xiangxiang Chu,Dapeng Zhang,Yiwei Wang,Yujun Cai,Shuo Li*

Main category: cs.CV

TL;DR: Video-STAR提出了一种将上下文子动作分解与工具增强强化学习结合的新框架，用于开放词汇动作识别，在多个主流视频数据集上表现优异，有效提升细粒度动作区别和跨模态鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在开放词汇动作识别任务中，容易受到文本先验限制，难以区分语义相近的细粒度动作。如何提高模型的细粒度辨别能力和减少跨模态幻觉成为挑战。

Method: 提出Video-STAR框架：1）将动作分解为可区分的子动作，以提升细粒度匹配能力；2）动态调用特定领域的外部工具，以实现跨模态推理和降低幻觉现象；3）设计分层奖励策略，自动平衡工具使用效率、子动作相关性和结构连贯性，使模型可以无需显式监督自主学习更优推理方式。

Result: 在HMDB-51、UCF-101、SSv2、Kinetics-400和Kinetics-600等主流数据集上，Video-STAR对比已有方法在细粒度动作识别和处理跨模态幻觉方面取得了最优表现，展示出优秀的鲁棒性和泛化能力。

Conclusion: Video-STAR有效提升了开放词汇下多模态动作识别的细粒度区分能力与跨模态泛化性能，为该领域相关研究提供了新思路和有竞争力的技术路线。

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
potential in bridging visual and textual reasoning, yet their reliance on
text-centric priors often limits their ability to disentangle semantically
similar actions in open-vocabulary scenarios. To address this, we propose
Video-STAR, a framework that harmonizes contextual sub-motion decomposition
with tool-augmented reinforcement learning for open-vocabulary action
recognition (OVAR). Unlike prior methods that treat actions as monolithic
entities, our approach innovatively decomposes actions into discriminative
sub-motions for fine-grained matching while dynamically invoking
domain-specific tools for cross-modal interleaving, thereby enabling
category-specific reasoning capacity and reducing cross-modal hallucination.
Moreover, by designing a hierarchical reward that balances tool-usage
efficiency, sub-motion relevance, and structural coherence in reasoning, our
method autonomously leverages external tools to prioritize sub-motion patterns
without explicit supervision, transmitting from text-centric reasoning to
visually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2,
Kinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art
performance, outperforming existing methods in distinguishing fine-grained
actions and handling cross-modal hallucination, validating our excellent
robustness and generalization.

</details>


### [86] [The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping](https://arxiv.org/abs/2510.08482)
*Onur Keleş,Aslı Özyürek,Gerardo Ortega,Kadir Gökgö,Esam Ghaleb*

Main category: cs.CV

TL;DR: 本文提出了一个视频基的视觉象似性挑战，用于评估视觉-语言模型（VLM）对手语中形式与意义关系的理解能力，在多个任务上考察了13个模型，并与人类表现进行对比。


<details>
  <summary>Details</summary>
Motivation: 手语中象似性（形式与意义的对应）很常见，适合用于研究视觉语言模型如何实现视觉与语义的关联。然而，大多数现有模型主要处理静态图像与文字，而对动态人类动作（如手语）的处理能力不明，因此需要新的基准测试体系。

Method: 作者提出了‘视觉象似性挑战’基准，通过视频任务评测VLM，包括：（1）语音学的手势形式预测（如手型、位置）、（2）透明性任务（根据动作推测意义）、（3）连续象似性评分；并让13个先进VLM分别在荷兰手语数据集中进行零样本和少样本测试，对比人类表现。

Result: 在手势形式预测中，VLM能捕捉部分手型和位置信息，但整体表现低于人类；在透明性推断任务中，模型远逊于人类；仅最好的模型在象似性评分上与人类有中等相关度。同时，模型在手势形式预测上越强，其象似性判断与人类越趋近。

Conclusion: 这些任务能够有效诊断VLM对视觉象似性的建模效果，实验鼓励将以人为中心的信号和具身学习方式融入多模态模型，以促进象似性建模和视觉语义对齐。

Abstract: Iconicity, the resemblance between linguistic form and meaning, is pervasive
in signed languages, offering a natural testbed for visual grounding. For
vision-language models (VLMs), the challenge is to recover such essential
mappings from dynamic human motion rather than static context. We introduce the
\textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts
psycholinguistic measures to evaluate VLMs on three tasks: (i) phonological
sign-form prediction (e.g., handshape, location), (ii) transparency (inferring
meaning from visual form), and (iii) graded iconicity ratings. We assess $13$
state-of-the-art VLMs in zero- and few-shot settings on Sign Language of the
Netherlands and compare them to human baselines. On \textit{phonological form
prediction}, VLMs recover some handshape and location detail but remain below
human performance; on \textit{transparency}, they are far from human baselines;
and only top models correlate moderately with human \textit{iconicity ratings}.
Interestingly, \textit{models with stronger phonological form prediction
correlate better with human iconicity judgment}, indicating shared sensitivity
to visually grounded structure. Our findings validate these diagnostic tasks
and motivate human-centric signals and embodied learning methods for modelling
iconicity and improving visual grounding in multimodal models.

</details>


### [87] [InstructX: Towards Unified Visual Editing with MLLM Guidance](https://arxiv.org/abs/2510.08485)
*Chong Mou,Qichao Sun,Yanze Wu,Pengze Zhang,Xinghui Li,Fulong Ye,Songtao Zhao,Qian He*

Main category: cs.CV

TL;DR: 本文提出了InstructX框架，统一实现基于指令的图像和视频编辑，并系统分析了多模态大模型（MLLMs）与扩散模型的结合方案，取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型在视觉理解和推理上的突飞猛进，将其应用于提升扩散模型编辑表现变得备受关注。然而，目前多数工作在模型设计分析上不足，尤其在视频编辑等复杂任务上的结合仍是难点。

Method: 作者提出InstructX框架，研究如何系统结合MLLMs与扩散模型用于图像和视频的指令编辑。通过对比实验，分析图像与视频统一建模时的协作机制与差异。此外方法引入针对不同模态的MLLM特征，有效统一图像与视频编辑任务。

Result: 在实验中，作者发现仅用图像数据训练模型也能产生自主的视频编辑能力，缓解了视频训练数据稀缺的瓶颈。最终方法在大量图像和视频编辑任务上验证了优越性能，达到SOTA水平。

Conclusion: InstructX框架能统一并高效地解决图像与视频的基于指令编辑问题，为未来多模态任务的深度整合提供了可行路径。

Abstract: With recent advances in Multimodal Large Language Models (MLLMs) showing
strong visual understanding and reasoning, interest is growing in using them to
improve the editing performance of diffusion models. Despite rapid progress,
most studies lack an in-depth analysis of MLLM design choices. Moreover, the
integration of MLLMs and diffusion models remains an open challenge in some
difficult tasks, such as video editing. In this paper, we present InstructX, a
unified framework for image and video editing. Specifically, we conduct a
comprehensive study on integrating MLLMs and diffusion models for
instruction-driven editing across diverse tasks. Building on this study, we
analyze the cooperation and distinction between images and videos in unified
modeling. (1) We show that training on image data can lead to emergent video
editing capabilities without explicit supervision, thereby alleviating the
constraints imposed by scarce video training data. (2) By incorporating
modality-specific MLLM features, our approach effectively unifies image and
video editing tasks within a single model. Extensive experiments demonstrate
that our method can handle a broad range of image and video editing tasks and
achieves state-of-the-art performance.

</details>


### [88] [MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration](https://arxiv.org/abs/2510.08508)
*Lu Liu,Chunlei Cai,Shaocheng Shen,Jianfeng Liang,Weimin Ouyang,Tianxiao Ye,Jian Mao,Huiyu Duan,Jiangchao Yao,Xiaoyun Zhang,Qiang Hu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了一种全新的视频修复系统MoA-VR，能够智能识别并应对多样和复杂的视频退化问题，自动选择最佳修复策略，在多个客观指标和感知质量上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实中的视频因噪声、压缩失真、低光等原因出现各类复杂劣化，现有方法不能泛化处理多样退化，缺乏自动化、专业化修复流程。作者受到专家人工处理流程启发，期望研发能模拟专家分步判断和修复的视频修复系统。

Method: MoA-VR系统由三个协作智能体组成：1）退化识别智能体，基于构建的大规模高分辨率劣化识别基准和视觉-语言模型精确识别退化类型；2）自主修复策略智能体，使用大语言模型驱动，根据劣化情况自动路由修复工具；3）修复质量评估智能体，基于Res-VQ数据集和专门设计的视觉-语言VQA模型评估修复效果。

Result: 通过大量实验，MoA-VR对各类复合退化视频表现出优异的修复能力，无论在客观指标还是感知品质上均优于已有主流方法。系统在多种实际场景下实现了高鲁棒性和泛化性。

Conclusion: MoA-VR展示了多模态智能与模块化推理在通用视频修复任务中的巨大潜力，为视频恢复领域的研究和应用提供了新的方向。

Abstract: Real-world videos often suffer from complex degradations, such as noise,
compression artifacts, and low-light distortions, due to diverse acquisition
and transmission conditions. Existing restoration methods typically require
professional manual selection of specialized models or rely on monolithic
architectures that fail to generalize across varying degradations. Inspired by
expert experience, we propose MoA-VR, the first
\underline{M}ixture-\underline{o}f-\underline{A}gents \underline{V}ideo
\underline{R}estoration system that mimics the reasoning and processing
procedures of human professionals through three coordinated agents: Degradation
Identification, Routing and Restoration, and Restoration Quality Assessment.
Specifically, we construct a large-scale and high-resolution video degradation
recognition benchmark and build a vision-language model (VLM) driven
degradation identifier. We further introduce a self-adaptive router powered by
large language models (LLMs), which autonomously learns effective restoration
strategies by observing tool usage patterns. To assess intermediate and final
processed video quality, we construct the \underline{Res}tored
\underline{V}ideo \underline{Q}uality (Res-VQ) dataset and design a dedicated
VLM-based video quality assessment (VQA) model tailored for restoration tasks.
Extensive experiments demonstrate that MoA-VR effectively handles diverse and
compound degradations, consistently outperforming existing baselines in terms
of both objective metrics and perceptual quality. These results highlight the
potential of integrating multimodal intelligence and modular reasoning in
general-purpose video restoration systems.

</details>


### [89] [To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models](https://arxiv.org/abs/2510.08510)
*Jiayun Luo,Wan-Cyuan Fan,Lyuyang Wang,Xiangteng He,Tanzila Rahman,Purang Abolmaesumi,Leonid Sigal*

Main category: cs.CV

TL;DR: 本文分析了大型视觉语言模型（LVLMs）中ViT编码器产生的attention sink（高范数视觉token）的作用，并提出改进利用方法，显著提升了视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然目前LVLM广泛采用ViT和LLM的结构，但还不清楚哪些视觉token对推理和理解贡献最大，ViT产生的高范数token（attention sink）在信息传递中的作用被忽视，因此需要深入研究其特性及利用方式。

Method: 作者首先定性和定量分析了ViT attention sink token中蕴含的信息，并对其重要性进行了实证研究。同时，设计了无需训练和基于训练的两种方法，提高LLM对这些token的理解和利用效率。

Result: 实验表明，无论使用训练方法还是训练外方法，在多种主流LVLM和视觉推理任务中，显式利用ViT attention sink均显著提升了模型表现。

Conclusion: ViT attention sink tokens包含关键高语义信息，是提升LVLM视觉理解和推理的关键。一旦被充分利用，可极大改进模型性能，展示了当前LVLM架构存在的优化空间。

Abstract: Large Vision Language Models (LVLMs) have recently emerged as powerful
architectures capable of understanding and reasoning over both visual and
textual information. These models typically rely on two key components: a
Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual
content into a sequence of image tokens and serves as the perceptual front-end
-- the eyes of the model. In contrast, the LLM interprets these tokens to
perform high-level reasoning, generates responses, and functions as the
cognitive core -- the brain of the model. However, it remains unclear which
visual tokens contribute most significantly to understanding and reasoning, and
how effectively these signals are propagated from ViT to the LLM. While most
existing works have focused on identifying attention sinks, low-semantic tokens
receiving disproportionately high attention, within the LLM, we shift the focus
to the vision encoder by identifying a class of high-norm visual tokens from
ViT, referred to as ViT attention sinks -- a problem that has been rarely
studied but is indeed very important for LVLMs. Our findings show that these
ViT sinks encapsulate high-level semantic concepts from images, allowing the
LLM to perform more effective understanding and reasoning. Despite their
importance, these sink tokens are often overlooked in existing LVLM
architectures. To explore their contribution, we present both qualitative and
quantitative analyses of the information embedded in these sink tokens. We also
propose both training-free and training-based approaches to better leverage how
this information is interpreted by the LLM, and to what extent. By explicitly
utilizing these tokens, we demonstrate substantial improvements across a range
of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT
attention sinks in enhancing visual reasoning.

</details>


### [90] [SliceFine: The Universal Winning-Slice Hypothesis for Pretrained Networks](https://arxiv.org/abs/2510.08513)
*Md Kowsher,Ali O. Polat,Ehsan Mohammady Ardehaly,Mehrdad Salehi,Zia Ghiasi,Prasanth Murali,Chen Chen*

Main category: cs.CV

TL;DR: 本文提出用理论框架解释为何仅微调预训练模型中少量随机子网络（slice）即可高效适应下游任务，并由此提出SliceFine方法，在多任务上能实现高效且无新增参数的微调。


<details>
  <summary>Details</summary>
Motivation: 现有大模型微调方法普遍涉及大量参数更新或引入额外参数（如adapter），但部分子网络足以适应新任务，本论文试图解释这一现象背后的理论基础。

Method: 作者证明预训练网络存在“通用获胜子集”（Universal Winning Slice）现象：切片权重的谱结构相似，且骨干表征保有丰富的任务相关特征。基于此提出SliceFine方法，仅更新少量权重切片，实现高效微调且无新参数引入。

Result: SliceFine方法在语言和视觉任务的多项基准上，与最佳参数高效微调（PEFT）方法性能持平，并显著提升训练速度、显存效率和模型紧凑性。

Conclusion: 本研究为大模型参数高效微调提供了理论解释，并提出了兼具理论基础和实际优势的新方法SliceFine，对现有PEFT技术构成有力补充。

Abstract: This paper presents a theoretical framework explaining why fine tuning small,
randomly selected subnetworks (slices) within pre trained models can be
sufficient for downstream adaptation. We prove that pretrained networks exhibit
a universal winning slice property arising from two phenomena: (1) spectral
balance the eigenspectra of different weight matrix slices are remarkably
similar; and (2) high task energy their backbone representations retain rich,
task relevant features. This leads to the Universal Winning Slice Hypothesis,
which provides a theoretical foundation for parameter efficient fine tuning
(PEFT) in large scale models. Inspired by this, we propose SliceFine, a PEFT
method that exploits this inherent redundancy by updating only selected slices
of the original weights introducing zero new parameters, unlike adapter-based
approaches. Empirically, SliceFine matches the performance of state of the art
PEFT methods across language and vision tasks, while significantly improving
training speed, memory efficiency, and model compactness. Our work bridges
theory and practice, offering a theoretically grounded alternative to existing
PEFT techniques.

</details>


### [91] [FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control](https://arxiv.org/abs/2510.08527)
*Zhiyuan Zhang,Can Wang,Dongdong Chen,Jing Liao*

Main category: cs.CV

TL;DR: 提出FlexTraj框架，通过点轨迹实现灵活的图像到视频生成控制，能够支持稠密与稀疏的运动轨迹，提升速度、控制能力及鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图像到视频生成方法在运动轨迹控制上灵活性有限，很难支持细粒度、多粒度与未对齐的轨迹控制应用。

Method: 设计了统一的点基运动表示，每个点包含分割ID、轨迹ID与可选颜色通道，实现密集/稀疏控制；创新提出序列拼接注入条件，提高训练与推理效率，并采用退火训练减少对全监督与对齐条件的依赖。

Result: 实验显示FlexTraj可实现对视频生成不同粒度、不同对齐条件下的轨迹控制，支持丰富的应用场景（如运动克隆、拖拽视频生成、运动插值、摄像机重定向、动作控制及网格动画等）。

Conclusion: FlexTraj框架显著提升了图像到视频生成过程中运动轨迹控制的灵活性、效率与鲁棒性，为多样化应用提供了有效工具和支持。

Abstract: We present FlexTraj, a framework for image-to-video generation with flexible
point trajectory control. FlexTraj introduces a unified point-based motion
representation that encodes each point with a segmentation ID, a temporally
consistent trajectory ID, and an optional color channel for appearance cues,
enabling both dense and sparse trajectory control. Instead of injecting
trajectory conditions into the video generator through token concatenation or
ControlNet, FlexTraj employs an efficient sequence-concatenation scheme that
achieves faster convergence, stronger controllability, and more efficient
inference, while maintaining robustness under unaligned conditions. To train
such a unified point trajectory-controlled video generator, FlexTraj adopts an
annealing training strategy that gradually reduces reliance on complete
supervision and aligned condition. Experimental results demonstrate that
FlexTraj enables multi-granularity, alignment-agnostic trajectory control for
video generation, supporting various applications such as motion cloning,
drag-based image-to-video, motion interpolation, camera redirection, flexible
action control and mesh animations.

</details>


### [92] [SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.08531)
*Hongxing Li,Dingming Li,Zixuan Wang,Yuchen Yan,Hang Wu,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CV

TL;DR: 本文提出了一种分阶段的方法，通过逐步训练提升视觉-语言模型（VLMs）在空间推理上的表现，并显著超过了目前主流大模型。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在空间推理方面表现有限，主要因为缺乏从感知到理解的分层训练基础，直接进行空间推理学习难以实现模型泛化和鲁棒性。

Method: 作者构建了包含26,610个样本的多模态空间推理数据集SpatialLadder-26k，覆盖对象定位、单图/多视角/视频等多种空间任务，并提出三阶段渐进式训练框架：（1）通过对象定位建立空间感知，（2）多维度任务培养空间理解，（3）采用可验证奖励的强化学习强化复杂推理能力。

Result: 提出的SpatialLadder模型（30亿参数）在多个空间推理基准上取得领先表现，平均提升23.4%，超过GPT-4o（20.8%）和Gemini-2.0-Flash（10.1%）；在跨领域测试集上提升7.2%，展现了良好的泛化能力。

Conclusion: 分阶段的从感知到推理训练框架能显著提升视觉-语言模型的空间智能与泛化能力，强调了系统化空间学习路径的重要性。

Abstract: Spatial reasoning remains a fundamental challenge for Vision-Language Models
(VLMs), with current approaches struggling to achieve robust performance
despite recent advances. We identify that this limitation stems from a critical
gap: existing methods attempt to learn spatial reasoning directly without
establishing the hierarchical foundations of perception and understanding. To
address this challenge, we present a comprehensive methodology for building
spatial intelligence progressively. We introduce SpatialLadder-26k, a
multimodal dataset containing 26,610 samples spanning object localization,
single image, multi-view, and video spatial reasoning tasks, constructed
through a standardized pipeline that ensures systematic coverage across
modalities. Building on this dataset, we design a three-stage progressive
training framework that (1) establishes spatial perception through object
localization, (2) develops spatial understanding through multi-dimensional
spatial tasks, and (3) strengthens complex reasoning via reinforcement learning
with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter
model that achieves state-of-the-art performance on spatial reasoning
benchmarks, with 23.4% average improvement over the base model, surpassing
GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains
strong generalization with 7.2% improvement on out-of-domain benchmarks,
demonstrating that progressive training from perception to reasoning is
essential for robust spatial intelligence.

</details>


### [93] [Kontinuous Kontext: Continuous Strength Control for Instruction-based Image Editing](https://arxiv.org/abs/2510.08532)
*Rishubh Parihar,Or Patashnik,Daniil Ostashev,R. Venkatesh Babu,Daniel Cohen-Or,Kuan-Chieh Wang*

Main category: cs.CV

TL;DR: 本文提出了Kontinuous Kontext模型，可以在基于指令的图像编辑中，通过一个额外的连续变量精准控制编辑强度，实现从完全不变到极致变化的平滑调控。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本指令的图像编辑模型缺乏对编辑强度的细致控制，难以满足用户对编辑效果连续调整的需求。

Method: 在现有先进图像编辑模型基础上，引入额外输入——一个标量编辑强度，并训练一个轻量投影网络，将该强度与指令共同映射到模型调制空间中的系数。训练时利用生成模型合成包含图像、编辑指令和强度的多样化数据集，并通过过滤确保质量。

Result: Kontinuous Kontext模型能够实现对多种编辑任务（如风格化、属性、材质、背景、形状变换）的编辑强度从微弱到极强的统一、细致调控，无需针对不同任务单独训练属性编辑器。

Conclusion: 该方法为基于指令的图像编辑提供了更精确、可控的编辑手段，使用户可根据需求平滑调节编辑强度，提升了实用性和灵活性。

Abstract: Instruction-based image editing offers a powerful and intuitive way to
manipulate images through natural language. Yet, relying solely on text
instructions limits fine-grained control over the extent of edits. We introduce
Kontinuous Kontext, an instruction-driven editing model that provides a new
dimension of control over edit strength, enabling users to adjust edits
gradually from no change to a fully realized result in a smooth and continuous
manner. Kontinuous Kontext extends a state-of-the-art image editing model to
accept an additional input, a scalar edit strength which is then paired with
the edit instruction, enabling explicit control over the extent of the edit. To
inject this scalar information, we train a lightweight projector network that
maps the input scalar and the edit instruction to coefficients in the model's
modulation space. For training our model, we synthesize a diverse dataset of
image-edit-instruction-strength quadruplets using existing generative models,
followed by a filtering stage to ensure quality and consistency. Kontinuous
Kontext provides a unified approach for fine-grained control over edit strength
for instruction driven editing from subtle to strong across diverse operations
such as stylization, attribute, material, background, and shape changes,
without requiring attribute-specific training.

</details>


### [94] [MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization](https://arxiv.org/abs/2510.08540)
*Xiangyu Zhao,Junming Lin,Tianhao Liang,Yifan Zhou,Wenhao Chai,Yuzhe Gu,Weiyun Wang,Kai Chen,Gen Luo,Wenwei Zhang,Junchi Yan,Hua Yang,Haodong Duan,Xue Yang*

Main category: cs.CV

TL;DR: 本论文发现现有多模态大模型在长链反思推理能力上表现不足，针对这一问题提出新数据集和训练方法显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在数学和逻辑推理任务中表现突出，但面对真实世界复杂任务所需的长链反思推理能力尚未被充分探索和提升。论文旨在系统评估并改进MLLMs在该领域的能力。

Method: 1. 设计自动化数据合成引擎，构建包含42类挑战性任务、共1260样本的多模态长链反思推理基准MM-HELIX。
2. 利用Step-Elicited Response Generation流程，生成包含10万条高质量推理过程的反思推理训练数据集MM-HELIX-100K。
3. 提出自适应混合策略优化（AHPO）训练方法，结合离线专家数据监督和在线探索优化，解决稀疏奖励与监督微调后遗忘问题。

Result: 新方法在Qwen2.5-VL-7B基础上，在MM-HELIX基准上准确率提升18.6%，在一般数学和逻辑任务上平均提升5.7%。

Conclusion: MLLMs通过高质量数据和新型训练策略可有效习得并泛化反思推理能力，为更强的多模态模型发展奠定基础。

Abstract: While current Multimodal Large Language Models (MLLMs) have demonstrated
proficiency in reasoning tasks such as mathematics and logic, their capacity
for long-chain reflective reasoning, a prerequisite for solving complex
real-world problems, remains largely underexplored. In this work, we first
conduct an extensive empirical investigation to evaluate this capability.
Leveraging a carefully designed data synthesis engine, we construct MM-HELIX, a
multimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks
that require iterative thinking and backtracking. Empirical results on this
benchmark reveal that existing MLLMs exhibit significant performance deficits
in long-chain reflective reasoning. To address this limitation, we generate
post-training data and further explore learning paradigms for exploiting such
data. We first develop the Step-Elicited Response Generation pipeline to create
MM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning
traces for instruction-tuning stage. Given that standard Reinforcement Learning
fails on complex tasks due to sparse reward signals and catastrophic forgetting
after Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization
(AHPO), a novel training strategy that dynamically unifies offline supervision
and online optimization into a single stage. This strategy enables the model to
learn from expert data when rewards are sparse and conduct independent
exploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our
method achieves a +18.6\% accuracy improvement on MM-HELIX benchmark and
demonstrates strong generalization with a +5.7\% average performance gain on
general mathematic and logic tasks. Our work demonstrate that reflective
reasoning in MLLMs can be effectively learned and generalized, paving the way
for developing more capable MLLMs.

</details>


### [95] [VideoNorms: Benchmarking Cultural Awareness of Video Language Models](https://arxiv.org/abs/2510.08543)
*Nikhil Reddy Varimalla,Yunfei Xu,Arkadiy Saakyan,Meng Fan Wang,Smaranda Muresan*

Main category: cs.CV

TL;DR: 本文提出了VideoNorms基准数据集，以评测视频大模型（VideoLLMs）对美中两国社会规范的理解能力。研究发现当前模型在规范违例、中文文化和非语言证据等方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着VideoLLMs在全球范围内应用，模型需要具备不同文化背景下的理解与推理能力。现有评测缺乏文化敏感性，为科学评价模型的文化意识，亟需合适的基准。

Method: 作者提出了VideoNorms基准，包括1000余组美中两国的视频片段与规范配对。通过人-AI协作：即教师模型基于理论化提示生成注释，由专业人员进行验证与修正。此外，基于该基准对多种主流VideoLLMs进行系统评测。

Result: 评测结果显示：（1）模型在检测规范违例方面表现不如规范遵守；（2）对美国文化的表现优于中国文化；（3）对非言语证据检测明显较弱，难以识别具体的规范与言语行为对应关系；（4）模型在正式、非幽默场景下的表现差于人类评审。

Conclusion: 本文基准和流程为评测和推动VideoLLMs的文化敏感性提供了工具。实验证实当前模型存在显著跨文化、场景和证据类型方面的弱点，未来需要针对性训练改进。

Abstract: As Video Large Language Models (VideoLLMs) are deployed globally, they
require understanding of and grounding in the relevant cultural background. To
properly assess these models' cultural awareness, adequate benchmarks are
needed. We introduce VideoNorms, a benchmark of over 1000 (video clip, norm)
pairs from US and Chinese cultures annotated with socio-cultural norms grounded
in speech act theory, norm adherence and violations labels, and verbal and
non-verbal evidence. To build VideoNorms, we use a human-AI collaboration
framework, where a teacher model using theoretically-grounded prompting
provides candidate annotations and a set of trained human experts validate and
correct the annotations. We benchmark a variety of open-weight VideoLLMs on the
new dataset which highlight several common trends: 1) models performs worse on
norm violation than adherence; 2) models perform worse w.r.t Chinese culture
compared to the US culture; 3) models have more difficulty in providing
non-verbal evidence compared to verbal for the norm adhere/violation label and
struggle to identify the exact norm corresponding to a speech-act; and 4)
unlike humans, models perform worse in formal, non-humorous contexts. Our
findings emphasize the need for culturally-grounded video language model
training - a gap our benchmark and framework begin to address.

</details>


### [96] [ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation](https://arxiv.org/abs/2510.08551)
*Guanghao Li,Kerui Ren,Linning Xu,Zhewen Zheng,Changjian Jiang,Xin Gao,Bo Dai,Jian Pu,Mulin Yu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 提出了一种新的单目图像序列的实时3D重建框架ARTDECO，兼具高效性、精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的单目3D重建方法在实时性和高保真度之间存在权衡，不能同时兼顾效率与精度，难以满足实际应用（如AR/VR、机器人等）需求。

Method: 提出ARTDECO框架，将3D基础模型与高斯解码器结合，用于位姿估计和点预测。同时设计了层次化的高斯表达和层细度感知的渲染策略，以提升渲染保真度并减少冗余。

Result: 在八个不同的室内外基准上，ARTDECO实现了与SLAM系统相当的交互性能、接近前馈系统的鲁棒性，以及接近每场景优化方法的重建质量。

Conclusion: ARTDECO为高效、精准的实时场景数字化提供了切实可行的路径，兼顾了几何精度与高视觉保真度。

Abstract: On-the-fly 3D reconstruction from monocular image sequences is a
long-standing challenge in computer vision, critical for applications such as
real-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff:
per-scene optimization yields high fidelity but is computationally expensive,
whereas feed-forward foundation models enable real-time inference but struggle
with accuracy and robustness. In this work, we propose ARTDECO, a unified
framework that combines the efficiency of feed-forward models with the
reliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose
estimation and point prediction, coupled with a Gaussian decoder that
transforms multi-scale features into structured 3D Gaussians. To sustain both
fidelity and efficiency at scale, we design a hierarchical Gaussian
representation with a LoD-aware rendering strategy, which improves rendering
fidelity while reducing redundancy. Experiments on eight diverse indoor and
outdoor benchmarks show that ARTDECO delivers interactive performance
comparable to SLAM, robustness similar to feed-forward systems, and
reconstruction quality close to per-scene optimization, providing a practical
path toward on-the-fly digitization of real-world environments with both
accurate geometry and high visual fidelity. Explore more demos on our project
page: https://city-super.github.io/artdeco/.

</details>


### [97] [VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning](https://arxiv.org/abs/2510.08555)
*Minghong Cai,Qiulin Wang,Zongli Ye,Wenze Liu,Quande Liu,Weicai Ye,Xintao Wang,Pengfei Wan,Kun Gai,Xiangyu Yue*

Main category: cs.CV

TL;DR: 本文提出了一种新的随意时空视频补全任务和对应方法VideoCanvas，实现了用户在任意位置与时间填补视频内容，统一了多种可控视频生成任务。通过创新的条件混合策略和Temporal RoPE插值技术，解决了时序模糊问题，并在首次相关基准上取得了显著优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型难以实现用户在任意空间与时间自由填充内容，因为主流潜变量视频扩散模型中，由因果VAE引入的时序模糊导致无法精确逐帧调控，因此亟需新方法实现更细致的视频生成控制。

Method: 提出VideoCanvas框架，将In-Context Conditioning (ICC)范式适配于细粒度的时空控制任务，在不增加新参数的情况下，通过零填充实现空间控制，通过Temporal RoPE插值技术赋予每个条件在潜在序列中的连续分数位置，从而解耦空间和时间的信息并消除VAE的时序模糊。

Result: 开发了VideoCanvasBench基准，首次专门评估随意时空视频补全能力。实验结果显示，VideoCanvas在场景内部保真度和场景间创造性等方面，显著优于现有条件控制方法，树立了新的业界标杆。

Conclusion: VideoCanvas实现了真正灵活统一的视频生成和补全能力，为多种可控视频生成任务提供了通用方案，在无需修改基础模型的前提下，显著提高了用户可控性和生成质量。

Abstract: We introduce the task of arbitrary spatio-temporal video completion, where a
video is generated from arbitrary, user-specified patches placed at any spatial
location and timestamp, akin to painting on a video canvas. This flexible
formulation naturally unifies many existing controllable video generation
tasks--including first-frame image-to-video, inpainting, extension, and
interpolation--under a single, cohesive paradigm. Realizing this vision,
however, faces a fundamental obstacle in modern latent video diffusion models:
the temporal ambiguity introduced by causal VAEs, where multiple pixel frames
are compressed into a single latent representation, making precise frame-level
conditioning structurally difficult. We address this challenge with
VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC)
paradigm to this fine-grained control task with zero new parameters. We propose
a hybrid conditioning strategy that decouples spatial and temporal control:
spatial placement is handled via zero-padding, while temporal alignment is
achieved through Temporal RoPE Interpolation, which assigns each condition a
continuous fractional position within the latent sequence. This resolves the
VAE's temporal ambiguity and enables pixel-frame-aware control on a frozen
backbone. To evaluate this new capability, we develop VideoCanvasBench, the
first benchmark for arbitrary spatio-temporal video completion, covering both
intra-scene fidelity and inter-scene creativity. Experiments demonstrate that
VideoCanvas significantly outperforms existing conditioning paradigms,
establishing a new state of the art in flexible and unified video generation.

</details>


### [98] [SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models](https://arxiv.org/abs/2510.08559)
*Andong Deng,Taojiannan Yang,Shoubin Yu,Lincoln Spencer,Mohit Bansal,Chen Chen,Serena Yeung-Levy,Xiaohan Wang*

Main category: cs.CV

TL;DR: 本文提出了SciVideoBench，一个专为科学领域高级视频推理设计的基准数据集，以弥补现有大型多模态模型（LMM）在复杂科学视频理解上的不足，并通过实验评估揭示了目前主流LMM在此类任务上的显著短板。


<details>
  <summary>Details</summary>
Motivation: 尽管LMM在多模态领域取得了显著进展，但科学领域复杂视频推理一直是难点。现有视频基准主要聚焦通用场景和简单推理，已趋于饱和，难以真正评估LMM的高级认知能力。

Method: 研究团队构建了SciVideoBench，这是一套包含1000个多项选择题的问题集，这些题目通过半自动方式从25个专业学科的前沿科学实验视频中精心设计并验证，要求模型具备领域知识、时空感知和复杂逻辑推理能力。

Result: 实验评测显示，包括Gemini 2.5 Pro和Qwen2.5-VL在内的主流LMM均在SciVideoBench上表现不佳，尤其在推理复杂性与视觉定位等关键要素上暴露出明显能力短板。

Conclusion: SciVideoBench填补了科学领域高级视频推理评测的空白，为推动具有更强认知能力的多模态AI系统的发展提供了方向和动力。

Abstract: Large Multimodal Models (LMMs) have achieved remarkable progress across
various capabilities; however, complex video reasoning in the scientific domain
remains a significant and challenging frontier. Current video benchmarks
predominantly target general scenarios where perception/recognition is heavily
relied on, while with relatively simple reasoning tasks, leading to saturation
and thus failing to effectively evaluate advanced multimodal cognitive skills.
To address this critical gap, we introduce SciVideoBench, a rigorous benchmark
specifically designed to assess advanced video reasoning in scientific
contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice
questions derived from cutting-edge scientific experimental videos spanning
over 25 specialized academic subjects and verified by a semi-automatic system.
Each question demands sophisticated domain-specific knowledge, precise
spatiotemporal perception, and intricate logical reasoning, effectively
challenging models' higher-order cognitive abilities. Our evaluation highlights
significant performance deficits in state-of-the-art proprietary and
open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating
substantial room for advancement in video reasoning capabilities. Detailed
analyses of critical factors such as reasoning complexity and visual grounding
provide valuable insights and clear direction for future developments in LMMs,
driving the evolution of truly capable multimodal AI co-scientists. We hope
SciVideoBench could fit the interests of the community and help to push the
boundary of cutting-edge AI for border science.

</details>


### [99] [MultiCOIN: Multi-Modal COntrollable Video INbetweening](https://arxiv.org/abs/2510.08561)
*Maham Tanveer,Yang Zhou,Simon Niklaus,Ali Mahdavi Amiri,Hao Zhang,Krishna Kumar Singh,Nanxuan Zhao*

Main category: cs.CV

TL;DR: 本文提出了一个新的视频帧插值（video inbetweening）框架，支持多模态用户控制，提升了中间帧生成的多样性和精细度，实现了更自然、灵活的视频过渡效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频帧插值方法难以处理大幅、复杂或复杂运动，并且缺乏对用户多样化意图及中间帧细节的精细控制，无法满足创作者的需求。

Method: 作者采用了Diffusion Transformer (DiT) 架构，设计了可支持多模态控制（例如深度变化、运动轨迹、文本描述、运动区域等）的插值方法。将所有运动控制统一映射为稀疏、用户友好的点表示，细分为内容控制和运动控制两路分别编码，再分别引导去噪过程，由两个生成器分别处理内容和运动。采用分阶段训练策略确保多模态控制的平滑学习。

Result: 通过定性和定量实验验证，模型支持多模态控制，能更灵活准确地生成动态、中间帧内容，推动视频叙事的个性化与上下文精准性。

Conclusion: 本方法显著提升了视频帧插值的多样性、用户控制灵活性和中间帧逼真度，为视频编辑和合成提供了强有力的新工具。

Abstract: Video inbetweening creates smooth and natural transitions between two image
frames, making it an indispensable tool for video editing and long-form video
synthesis. Existing works in this domain are unable to generate large, complex,
or intricate motions. In particular, they cannot accommodate the versatility of
user intents and generally lack fine control over the details of intermediate
frames, leading to misalignment with the creative mind. To fill these gaps, we
introduce \modelname{}, a video inbetweening framework that allows multi-modal
controls, including depth transition and layering, motion trajectories, text
prompts, and target regions for movement localization, while achieving a
balance between flexibility, ease of use, and precision for fine-grained video
interpolation. To achieve this, we adopt the Diffusion Transformer (DiT)
architecture as our video generative model, due to its proven capability to
generate high-quality long videos. To ensure compatibility between DiT and our
multi-modal controls, we map all motion controls into a common sparse and
user-friendly point-based representation as the video/noise input. Further, to
respect the variety of controls which operate at varying levels of granularity
and influence, we separate content controls and motion controls into two
branches to encode the required features before guiding the denoising process,
resulting in two generators, one for motion and the other for content. Finally,
we propose a stage-wise training strategy to ensure that our model learns the
multi-modal controls smoothly. Extensive qualitative and quantitative
experiments demonstrate that multi-modal controls enable a more dynamic,
customizable, and contextually accurate visual narrative.

</details>


### [100] [NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints](https://arxiv.org/abs/2510.08565)
*Changyao Tian,Hao Li,Gen Luo,Xizhou Zhu,Weijie Su,Hanming Deng,Jinguo Zhu,Jie Shao,Ziran Zhu,Yunpeng Liu,Lewei Lu,Wenhai Wang,Hongsheng Li,Jifeng Dai*

Main category: cs.CV

TL;DR: 本文提出了一种新型多模态大语言模型（MLLM）原生训练方法，通过端到端方式系统探索了其设计空间和可扩展性，并提出了名为NaViL的模型，取得了优异的实验效果。


<details>
  <summary>Details</summary>
Motivation: 目前主流MLLM通常采用分步训练（视觉编码器和语言模型分别预训练后再拼接），但这样的方法难以探索多模态模型规模的扩展性。为突破这一瓶颈，作者希望通过端到端原生训练方法改善MLLM的表现，并深入理解视觉与语言组件间的协同扩展特性。

Method: 系统性地研究了MLLM原生训练过程中的架构选择和规模扩展特性，尤其针对数据受限场景。通过大量实验选取最佳的meta-architecture以平衡性能和训练成本，并实证分析视觉编码器与语言模型规模扩展的正相关性，最终提出了原生多模态模型NaViL及一套高效训练策略。

Result: 在14个多模态基准集上，对比实验表明NaViL在性能上媲美并优于现有主流MLLM，同时训练成本更低。相关的架构与扩展实验也为原生MLLM提供了实证基础。

Conclusion: 原生端到端训练可有效提升MLLM的性能和扩展能力。NaViL的实验结果为未来多模态模型的原生训练和设计提供了有价值的参考和见解。

Abstract: Compositional training has been the de-facto paradigm in existing Multimodal
Large Language Models (MLLMs), where pre-trained vision encoders are connected
with pre-trained LLMs through continuous multimodal pre-training. However, the
multimodal scaling property of this paradigm remains difficult to explore due
to the separated training. In this paper, we focus on the native training of
MLLMs in an end-to-end manner and systematically study its design space and
scaling property under a practical setting, i.e., data constraint. Through
careful study of various choices in MLLM, we obtain the optimal
meta-architecture that best balances performance and training cost. After that,
we further explore the scaling properties of the native MLLM and indicate the
positively correlated scaling relationship between visual encoders and LLMs.
Based on these findings, we propose a native MLLM called NaViL, combined with a
simple and cost-effective recipe. Experimental results on 14 multimodal
benchmarks confirm the competitive performance of NaViL against existing MLLMs.
Besides that, our findings and results provide in-depth insights for the future
study of native MLLMs.

</details>


### [101] [MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning](https://arxiv.org/abs/2510.08567)
*Tajamul Ashraf,Umair Nawaz,Abdelrahman M. Shaker,Rao Anwer,Philip Torr,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: 本文提出了一种视觉为中心的代理微调框架，通过自动合成多模态轨迹与偏好数据，训练视觉语言模型（VLM）以实现更强的多模态工具推理能力，并显著提升了多项基准测试的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在复杂推理和决策场景下应用有限，主要受高质量多模态轨迹数据稀缺及标注成本高昂的制约。因此，迫切需要自动、高效的数据构建与微调方法以提升VLM的工具使用推理能力。

Method: 1. 自动构建M-TRACE大规模多模态轨迹数据集（28.5K任务，177K验证轨迹），用于模仿学习调优；2. 在该数据集基础上微调VLM控制器，得到MATRIX Agent，实现逐步工具推理；3. 提出Pref-X自动偏好对（11K对），通过逐步偏好学习进一步优化模型表现。

Result: 在Agent-X、GTA和GAIA三个多模态工具推理基准上，MATRIX的表现均超越了主流开源与闭源VLM，验证了本框架的通用性和有效性。

Conclusion: 通过自动生成高质量多模态数据与偏好数据，结合多阶段微调方法，可大幅提升VLM在复杂工具推理任务中的能力，为实际多模态智能体系统的开发提供了可扩展方案。

Abstract: Vision language models (VLMs) are increasingly deployed as controllers with
access to external tools for complex reasoning and decision-making, yet their
effectiveness remains limited by the scarcity of high-quality multimodal
trajectories and the cost of manual annotation. We address this challenge with
a vision-centric agent tuning framework that automatically synthesizes
multimodal trajectories, generates step-wise preference pairs, and trains a VLM
controller for robust tool-use reasoning. Our pipeline first constructs
M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified
trajectories, enabling imitation-based trajectory tuning. Building on this, we
develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool
reasoning. To achieve finer alignment, we further introduce Pref-X, a set of
11K automatically generated preference pairs, and optimize MATRIX on it via
step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA,
MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating
scalable and effective multimodal tool use. Our data and code is avaliable at
https://github.com/mbzuai-oryx/MATRIX.

</details>


### [102] [D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction](https://arxiv.org/abs/2510.08566)
*Meixi Song,Xin Lin,Dizhe Zhang,Haodong Li,Xiangtai Li,Bo Du,Lu Qi*

Main category: cs.CV

TL;DR: 本文提出了一种提升3D Gaussian Splatting（3DGS）在稀疏视角条件下性能和稳定性的新方法D$^2$GS。方法通过深度和密度自适应的Dropout抑制过拟合，并在远场区域增强重建质量，有效改善了3D重建的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管3DGS在新视角合成任务上取得了显著进步，但在稀疏视角采样下仍易出现表现退化和不稳定的问题。作者分析发现，临摄像机区域高密度高斯分布导致过拟合，而远距离区域高斯覆盖不足导致欠拟合，因此亟需针对性改进方案。

Method: 本文提出D$^2$GS方法，包括两大模块：1）“深度与密度引导的Dropout”根据高斯分布的密度和深度自适应地掩蔽冗余高斯，减少过拟合；2）“距离感知保真度增强”模块通过有针对性的监督提升远场区域的重建质量。此外，作者还提出了一种新的评估指标，量化高斯分布学习的稳定性。

Result: 在多个数据集上的大量实验显示，该方法在稀疏视角条件下显著提升了视觉质量和模型鲁棒性。

Conclusion: D$^2$GS有效解决了3DGS在稀疏视角下过拟合和欠拟合的问题，提升了视觉表现和分布稳定性，为实际应用提供了更鲁棒的3D重建方案。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) enable real-time,
high-fidelity novel view synthesis (NVS) with explicit 3D representations.
However, performance degradation and instability remain significant under
sparse-view conditions. In this work, we identify two key failure modes under
sparse-view conditions: overfitting in regions with excessive Gaussian density
near the camera, and underfitting in distant areas with insufficient Gaussian
coverage. To address these challenges, we propose a unified framework D$^2$GS,
comprising two key components: a Depth-and-Density Guided Dropout strategy that
suppresses overfitting by adaptively masking redundant Gaussians based on
density and depth, and a Distance-Aware Fidelity Enhancement module that
improves reconstruction quality in under-fitted far-field areas through
targeted supervision. Moreover, we introduce a new evaluation metric to
quantify the stability of learned Gaussian distributions, providing insights
into the robustness of the sparse-view 3DGS. Extensive experiments on multiple
datasets demonstrate that our method significantly improves both visual quality
and robustness under sparse view conditions. The project page can be found at:
https://insta360-research-team.github.io/DDGS-website/.

</details>


### [103] [ReSplat: Learning Recurrent Gaussian Splats](https://arxiv.org/abs/2510.08575)
*Haofei Xu,Daniel Barath,Andreas Geiger,Marc Pollefeys*

Main category: cs.CV

TL;DR: 本文提出了ReSplat模型，一种能够迭代优化的前馈递归高斯渲染方法，不需显式计算梯度，可提高渲染质量并减少计算量。


<details>
  <summary>Details</summary>
Motivation: 传统的前馈高斯投影模型虽然高效且适合稀疏输入，但推断时仅做一次前向传播，限制了性能提升。作者希望能在保持高效率的同时，通过迭代优化方式进一步提升模型表现。

Method: 作者提出ReSplat，将递归网络与高斯渲染结合，在每步利用渲染误差作为反馈信号指导高斯参数更新，且无需显式算梯度。初始高斯通过降采样至原来1/16数量生成，大幅减少初期计算量。

Result: 在2、8、16视角输入、多个分辨率与不同数据集（DL3DV与RealEstate10K）上实验，ReSplat能用更少的高斯，显著提高速度，并取得领先渲染表现。

Conclusion: ReSplat实现了高效且高质量的高斯渲染，摆脱了传统前馈方法性能天花板，具备良好的泛化能力和实际应用潜力。

Abstract: While feed-forward Gaussian splatting models provide computational efficiency
and effectively handle sparse input settings, their performance is
fundamentally limited by the reliance on a single forward pass during
inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting
model that iteratively refines 3D Gaussians without explicitly computing
gradients. Our key insight is that the Gaussian splatting rendering error
serves as a rich feedback signal, guiding the recurrent network to learn
effective Gaussian updates. This feedback signal naturally adapts to unseen
data distributions at test time, enabling robust generalization. To initialize
the recurrent process, we introduce a compact reconstruction model that
operates in a $16 \times$ subsampled space, producing $16 \times$ fewer
Gaussians than previous per-pixel Gaussian models. This substantially reduces
computational overhead and allows for efficient Gaussian updates. Extensive
experiments across varying of input views (2, 8, 16), resolutions ($256 \times
256$ to $540 \times 960$), and datasets (DL3DV and RealEstate10K) demonstrate
that our method achieves state-of-the-art performance while significantly
reducing the number of Gaussians and improving the rendering speed. Our project
page is at https://haofeixu.github.io/resplat/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [104] [Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments](https://arxiv.org/abs/2510.07359)
*Jingfei Huang,Han Tu*

Main category: cs.CL

TL;DR: 本研究通过结合街景图片和社交媒体文本，提出新的情感反应不一致性检测方法，揭示了北京城区感知与舆论情感在疫情前后的显著差异及其潜在影响因素。


<details>
  <summary>Details</summary>
Motivation: 社交媒体改变了人们对城市环境的认知，现有多维情感分析方法难以揭示复杂的感知与舆论差异，因此需要开发新的方法来识别和解释城市中情感反应的不一致性。

Method: 构建了包含14万余张百度和腾讯街景图像（用于测量感知）与近百万条微博文本（用于测量舆论）的数据集。开发了反应指数，整合目标检测和自然语言处理技术，对不同年份的北京二环情感进行分类。通过回归分析、图像分割、词频分析，结合土地利用分布，对分类出的情感反应进行分析与可视化。

Result: 城市感知情感的空间分布趋于均衡，但舆论情感变化更加极端，二者之间存在显著不一致；感知与舆论情感受高密度建筑、行人活动等因素影响。疫情对情感反应分布和不一致性有重要影响。

Conclusion: 本文提出的方法有效揭示了城市环境中感知与舆论情感反应的不一致，相关发现有助于理解真实的公众情绪，指导城市环境管理和更新策略。

Abstract: The ascension of social media platforms has transformed our understanding of
urban environments, giving rise to nuanced variations in sentiment reaction
embedded within human perception and opinion, and challenging existing
multidimensional sentiment analysis approaches in urban studies. This study
presents novel methodologies for identifying and elucidating sentiment
inconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent
Street view images to measure perceptions, and 984,024 Weibo social media text
posts to measure opinions. A reaction index is developed, integrating object
detection and natural language processing techniques to classify sentiment in
Beijing Second Ring for 2016 and 2022. Classified sentiment reaction is
analysed and visualized using regression analysis, image segmentation, and word
frequency based on land-use distribution to discern underlying factors. The
perception affective reaction trend map reveals a shift toward more evenly
distributed positive sentiment, while the opinion affective reaction trend map
shows more extreme changes. Our mismatch map indicates significant disparities
between the sentiments of human perception and opinion of urban areas over the
years. Changes in sentiment reactions have significant relationships with
elements such as dense buildings and pedestrian presence. Our inconsistent maps
present perception and opinion sentiments before and after the pandemic and
offer potential explanations and directions for environmental management, in
formulating strategies for urban renewal.

</details>


### [105] [Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation](https://arxiv.org/abs/2510.07414)
*Mufei Li,Dongqi Fu,Limei Wang,Si Zhang,Hanqing Zeng,Kaan Sancak,Ruizhong Qiu,Haoyu Wang,Xiaoxin He,Xavier Bresson,Yinglong Xia,Chonglin Sun,Pan Li*

Main category: cs.CL

TL;DR: 本文提出了HaystackCraft，一个基于多跳问题和维基百科超链接网络的新型“针在大海捞针”（NIAH）基准，用于更真实地测试大语言模型在噪声和复杂检索场景下的鲁棒性。实验发现，现代长上下文语言模型在仿真真实检索和推理流程中仍然存在诸多挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的NIAH基准多为理想化合成场景，未能体现现实中检索器偏见和智能体工作流中噪声的影响。为推动模型在更真实、复杂环境中的发展，亟需一个涵盖多样干扰因素的新评测基准。

Method: 设计并实现了HaystackCraft基准。该基准基于全英文维基百科的超链接网络，结合多步检索，通过稀疏、稠密、混合和图结构等多种检索方式引入各种类型的干扰，并延伸到支持模型自主查询、反思和中断操作的动态agentic场景。评测了15个主流长上下文语言模型。

Result: （1）更强的稠密检索会带来更具挑战性的干扰项，但通过图结构重排序能提升检索效果、减少有害干扰；（2）在agentic动态测试中，即使是最先进的模型如Gemini 2.5 Pro和GPT-5，在自生成干扰和合理中止推理等环节仍会出现连锁失败。

Conclusion: 当前大语言模型在复杂、动态、噪声充斥的长上下文推理中存在明显瓶颈。HaystackCraft为未来模型提升提供了重要测试平台，强调了开展“干草堆工程”以提升模型鲁棒性的必要性。

Abstract: Modern long-context large language models (LLMs) perform well on synthetic
"needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy
contexts arise from biased retrieval and agentic workflows. We argue that
haystack engineering is necessary to construct noisy long contexts that
faithfully capture key real-world factors -- distraction from heterogeneous
biased retrievers and cascading errors in agentic workflows -- to test models'
long-context robustness. We instantiate it through HaystackCraft, a new NIAH
benchmark built on the full English Wikipedia hyperlink network with multi-hop
questions. HaystackCraft evaluates how heterogeneous retrieval strategies
(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,
haystack ordering, and downstream LLM performance. HaystackCraft further
extends NIAH to dynamic, LLM-dependent settings that simulate agentic
operations, where models refine queries, reflect on their past reasonings, and
decide when to stop. Experiments with 15 long-context models show that (1)
while stronger dense retrievers can introduce more challenging distractors,
graph-based reranking simultaneously improves retrieval effectiveness and
mitigates more harmful distractors; (2) in agentic tests, even advanced models
like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated
distractors or struggle to perform early stops. These results highlight
persistent challenges in agentic long-context reasoning and establish
HaystackCraft as a valuable testbed for future progress.

</details>


### [106] [Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data](https://arxiv.org/abs/2510.07434)
*Olia Toporkov,Alan Akbik,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在无需监督训练数据的上下文词形还原（lemmatization）任务中的表现，并与传统的监督学习方法进行了对比。实验横跨12种形态复杂性不同的语言，结果显示仅提供少量示例的LLM在大部分语言上都能达到最新水平。


<details>
  <summary>Details</summary>
Motivation: 以往lemmatization主要依赖监督学习及领域内数据，但在无对应语料或新语言/领域时效果受限。当前尚无LLM在该任务上能力的系统验证，作者希望评估LLM在极少示例、无监督语境下直接还原词形的实际效果。

Method: 论文将LLM基于上下文、少量示例的直接还原（in-context lemma generation）与两类主流传统方法对比：一是fine-tuned out-of-domain的编码器监督方法，二是跨语言方法。在12种不同复杂度语言下系统对比评测。

Result: 结果表明，当难以获得目标语言/领域“金数据”时，传统方法仍表现竞争力，但当前LLM直接借助上下文和少样本就能在多数语言上实现SOTA效果，无需事先微调。

Conclusion: LLM具备极强的上下文词形还原能力，尤其适用于训练数据缺乏的多语言、多领域环境，有望减少对昂贵标注语料的依赖。

Abstract: Lemmatization is the task of transforming all words in a given text to their
dictionary forms. While large language models (LLMs) have demonstrated their
ability to achieve competitive results across a wide range of NLP tasks, there
is no prior evidence of how effective they are in the contextual lemmatization
task. In this paper, we empirically investigate the capacity of the latest
generation of LLMs to perform in-context lemmatization, comparing it to the
traditional fully supervised approach. In particular, we consider the setting
in which supervised training data is not available for a target domain or
language, comparing (i) encoder-only supervised approaches, fine-tuned
out-of-domain, and (ii) cross-lingual methods, against direct in-context lemma
generation with LLMs. Our experimental investigation across 12 languages of
different morphological complexity finds that, while encoders remain
competitive in out-of-domain settings when fine-tuned on gold data, current
LLMs reach state-of-the-art results for most languages by directly generating
lemmas in-context without prior fine-tuning, provided just with a few examples.
Data and code available upon publication:
https://github.com/oltoporkov/lemma-dilemma

</details>


### [107] [LASER: An LLM-based ASR Scoring and Evaluation Rubric](https://arxiv.org/abs/2510.07437)
*Amruta Parulekar,Preethi Jyothi*

Main category: cs.CL

TL;DR: 本文提出LASER，一个基于大语言模型（LLM）的自动语音识别（ASR）评分方法，更加关注语句语义而非表层错误，可提升评测的公平性与合理性，并具多语言适用性。


<details>
  <summary>Details</summary>
Motivation: 现有ASR评估指标如WER对形态或语法细微差别也给出严厉惩罚，未体现句子语义的不变性，评测不够合理，需要一种能更公平、准确反映ASR实际表现的新方法。

Method: 提出LASER，利用先进LLM的in-context学习能力，以详细示例驱动大模型评分（以Gemini 2.5 Pro为主），还尝试用Llama 3小模型对从参考文本与ASR输出构造的词对进行微调，实现评分。并测试其对印地语及其它印度语言的泛化能力。

Result: 用Gemini 2.5 Pro的LASER评分与人工标注高度相关（相关度达94%）；在印地语提示下也能有效分析马拉地语、卡纳达语及马拉雅拉姆语错误。微调后的Llama 3能够以近89%准确率预测应如何打分。

Conclusion: LLM驱动的LASER方法在多语种上能更合理地评估ASR结果，兼具高准确性和灵活性，为ASR质量评测提供更加公平和实用的新方向。

Abstract: Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly
penalize morphological and syntactic nuances that do not significantly alter
sentence semantics. We introduce an LLM-based scoring rubric LASER that
leverages state-of-the-art LLMs' in-context learning abilities to learn from
prompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro
achieved a very high correlation score of 94% with human annotations. Hindi
examples in the prompt were also effective in analyzing errors in other Indian
languages such as Marathi, Kannada and Malayalam. We also demonstrate how a
smaller LLM like Llama 3 can be finetuned on word-pair examples derived from
reference and ASR predictions to predict what kind of penalty should be applied
with close to 89% accuracy.

</details>


### [108] [Meaningful Pose-Based Sign Language Evaluation](https://arxiv.org/abs/2510.07453)
*Zifan Jiang,Colin Leong,Amit Moryossef,Anne Göhring,Annette Rios,Oliver Cory,Maksym Ivashechkin,Neha Tarigopula,Biao Zhang,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: 本文系统性地研究了手语骨骼姿态的评价方法，提出了新的评估工具，并展示了不同指标在不同场景下的取舍和适用性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对手语骨骼姿态的有效评价方法，限制了手语翻译与生成系统的发展。作者希望分析和比较不同的评价指标，以帮助开发更好的评测工具。

Method: 作者比较了三种评价方法：基于关键点距离、基于嵌入向量以及基于逆向翻译。通过自动化的元评估和跨多种手语的人类相关性研究，分析了这些方法的权衡。

Result: 不同评价指标在手语翻译和生成的不同应用场景下表现有明显差异。文中以自动化和人工相关性实验证明了各种评价方法的优劣。

Conclusion: 作者提供了一个实用且可复现的开源评测工具，有助于手语自动翻译与生成系统的开发与评价，并指出了各类评价方法的适用性。

Abstract: We present a comprehensive study on meaningfully evaluating sign language
utterances in the form of human skeletal poses. The study covers keypoint
distance-based, embedding-based, and back-translation-based metrics. We show
tradeoffs between different metrics in different scenarios through automatic
meta-evaluation of sign-level retrieval and a human correlation study of
text-to-pose translation across different sign languages. Our findings and the
open-source pose-evaluation toolkit provide a practical and reproducible way of
developing and evaluating sign language translation or generation systems.

</details>


### [109] [Populism Meets AI: Advancing Populism Research with LLMs](https://arxiv.org/abs/2510.07458)
*Eduardo Ryô Tamaki,Yujin J. Jung,Julia Chatterley,Grant Mitchell,Semir Dzebo,Cristóbal Sandoval,Levente Littvay,Kirk A. Hawkins*

Main category: cs.CL

TL;DR: 本文提出了一种利用链式思维（CoT）提示机制来测量民粹主义意识形态内容，结果显示大语言模型表现与专家人工编码员相当。


<details>
  <summary>Details</summary>
Motivation: 现有针对民粹主义意识内容的文本分析办法虽然有效，但在成本、耗时、多语言和大规模语料适用性方面存在显著局限。本研究希望借助大语言模型（LLM），提高相关分析的自动化与可扩展性。

Method: 作者基于全球民粹主义数据库（GPD）及其人工编码流程，为LLM设计了一套模拟人工编者思路的CoT提示模板。利用经过指导的提示语，测试了多个专有和开放权重的LLM在复制GPD评分任务中的效果。

Result: 经过这种领域特定的链式思维提示后，所选LLM在民粹主义分类准确率上与专家人工编码员相当，能够处理敏感、情境化的文本细节。

Conclusion: 使用经过培训流程和Rubric引导的链式思维提示，可有效提升LLM对民粹主义文本的判别与量化能力，为跨语言和大规模分析提供了新方案。

Abstract: Measuring the ideational content of populism remains a challenge. Traditional
strategies based on textual analysis have been critical for building the
field's foundations and providing a valid, objective indicator of populist
framing. Yet these approaches are costly, time consuming, and difficult to
scale across languages, contexts, and large corpora. Here we present the
results from a rubric and anchor guided chain of thought (CoT) prompting
approach that mirrors human coder training. By leveraging the Global Populism
Database (GPD), a comprehensive dataset of global leaders' speeches annotated
for degrees of populism, we replicate the process used to train human coders by
prompting the LLM with an adapted version of the same documentation to guide
the model's reasoning. We then test multiple proprietary and open weight models
by replicating scores in the GPD. Our findings reveal that this domain specific
prompting strategy enables the LLM to achieve classification accuracy on par
with expert human coders, demonstrating its ability to navigate the nuanced,
context sensitive aspects of populism.

</details>


### [110] [MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference](https://arxiv.org/abs/2510.07475)
*Zheyuan Zhang,Lin Ge,Hongjiang Li,Weicheng Zhu,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: 该论文提出了一种多智能体提示优化框架（MAPRO），利用最大后验推断和信念传播算法，有效自动优化多智能体系统中的提示（prompt）。实验结果表明其优于现有手工及自动化基线方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）能够通过角色分工提升性能，但其提示（prompt）设计难度大，自动化优化方法缺乏，尤其是面对提示空间的巨大搜索量和模糊的奖励归因问题。因此亟需系统化且高效的自动提示优化方案。

Method: 作者将多智能体提示优化建模为最大后验推断问题，并引入语言引导的最大积信念传播算法，结合拓扑感知的细化机制，根据任务执行反馈和责任归因逐步优化各agent的prompt。整个过程分为四个阶段，最终收敛到协调优化的多智能体prompt策略。

Result: MAPRO在多项任务和基准上均取得了领先的表现，明显超过了手工设计和目前其他自动优化方法。

Conclusion: MAPRO为多智能体系统提示优化提供了有效、系统的方法，并为未来更可靠、可解释的多智能体系统设计提供理论和实践指导。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, and LLM-based agents further extend these abilities to various
practical workflows. While recent progress shows that multi-agent systems (MAS)
can outperform single agents by coordinating specialized roles, designing
effective MAS remains difficult due to prompt sensitivity and the compounded
instability MAS creates. To cope with the challenge, recent efforts in
automated prompt design have reduced manual effort. However, multi-agent prompt
optimization remains largely unexplored. Challenges like exponentially
expanding search space and ambiguous credit assignment together make systematic
design intractable without principled methods. Therefore, we introduce
M}ulti-Agent PRompt Optimization (MAPRO), a four-stage framework that first
formulates MAS prompt optimization as a Maximum a Posteriori (MAP) inference
problem and solves it using a language-guided variant of max-product belief
propagation algorithm. To address credit assignment and updates the system
iteratively, MAPRO employs a topology-aware refinement mechanism that
integrates execution feedback and downstream blames to selectively update agent
prompts. Through this process, MAPRO progressively converges to a coordinated
set of agent-specific prompt policies. Across benchmarks in various tasks,
MAPRO achieves state-of-the-art performance, consistently surpassing manually
engineered baselines and recent automated alternatives. Beyond performance, our
MAP-based formulation also delivers general guidelines for building more
reliable and principled multi-agent systems in the future

</details>


### [111] [AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding](https://arxiv.org/abs/2510.07486)
*Shuqing Luo,Yilin Guan,Pingzhi Li,Hanrui Wang,Tianlong Chen*

Main category: cs.CL

TL;DR: 本文提出一种异步框架AsyncSpade，在大语言模型推理时通过去除顺序依赖、异步重叠KV缓存操作和前向推理，显著提升长链式推理下的推理效率。


<details>
  <summary>Details</summary>
Motivation: 长链式思维(Chain-of-Thought, CoT)推理方法虽然提升了大语言模型的推理能力，但其线性的KV缓存（Key-Value cache）增长带来内存与计算瓶颈。在高并发和长推理链场景下，现有的稀疏推理方法在效率和性能上都遇到瓶颈。

Method: 作者提出AsyncSpade框架，包含两个核心：1）用一个轻量化时序回归模块，对下一个token的query state进行预测，以实现训练外、无需依赖解码主循环的KV稀疏选择；2）利用异步和分离式设计，将KV缓存过滤从自动回归的解码主循环中解耦，实现KV选择与前向推理的异步重叠处理。

Result: 在A100服务器测试下，AsyncSpade可以完全重叠KV操作与推理过程，相比当前最优方法TPOT提升20%以上，对比全注意力机制提升至少50%。在众多TTS基准上（如AIME、GPQA、MATH）准确率持平或更优。

Conclusion: AsyncSpade能有效消除顺序依赖带来的计算瓶颈，在不损失模型性能的前提下，实现大模型推理端到端时延的显著优化，尤其适合大规模高并发和长链式推理应用。

Abstract: Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),
but the linear KV-cache growth amplifies the memory-bound bottleneck of LLM
decoding. Query-aware page-level sparse decoding can achieve state-of-the-art
performance under constrained FLOPs budgets, but is limited by both
sequential-dependent page filtering and coarse-grained token selection,
hampering serving efficiency and model performance on TTS tasks under high
concurrency and long CoT scenarios (consuming even higher runtime than the
forward pipeline itself). In this paper, we first find that the current-step
query state can be accurately approximated in a unified manner from a short
window of recent queries, enabling training-free query-aware sparsity without
waiting in the decoding loop. We propose AsyncSpade, an asynchronous framework
for efficient TTS built on two core components: (1) a novel light-weight
temporal-regressive module that predicts the next-token query state; (2) an
asynchronous and disaggregated framework that decouples the KV cache filtering
from the auto-regressive decoding loop, overlapping the token-level KV
selection with the forward inference computation through asynchronism. To our
knowledge, AsyncSpade is the first to eliminate the sequential dependence
without sacrificing model performance. We validate the effectiveness of
AsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade
fully overlaps KV-cache operations with the inference pipeline, achieving
theoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade
delivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and
at least 50% TPOT reduction compared to full attention on Qwen3-8B and
Qwen3-32B models, while matching or surpassing their accuracy on various TTS
benchmarks (AIME-24/25, GPQA-Diamond, MATH-500).

</details>


### [112] [Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics](https://arxiv.org/abs/2510.07488)
*Rasika Muralidharan,Jaewoon Kwak,Jisun An*

Main category: cs.CL

TL;DR: 本文提出了一种基于类人团队科学的多智能体系统（MAS）分析框架，研究了由大语言模型（LLM）驱动的智能体团队结构、成员多样性和互动动态对团队任务表现的影响。


<details>
  <summary>Details</summary>
Motivation: 受人类团队科学启发，作者关注于现有LLM智能体团队在结构与互动层面的表现，希望揭示团队结构与多样性如何影响AI多智能体的协作与推理表现。

Method: 作者提出并实现了一个多智能体分析框架，分别设定团队结构（平级与层级）、多样性等变量，并在CommonsenseQA、StrategyQA、Social IQa和Latent Implicit Hate四项具有常识和社会推理属性的任务上评估团队性能，同时通过访谈和任务后反思了解智能体的主观体验。

Result: 实验发现，平级结构的团队整体表现优于层级结构；多样性对团队表现有复杂影响。智能体往往高估团队表现，但完成任务后，也意识到协作带来的益处与整合过程中的挑战，尤其是对话协调存在不足。

Conclusion: LLM智能体团队在平级结构下更有效，但多样性的作用复杂，同时团队成员间的对话协调和整合机制有待改进。未来研究需进一步探索影响团队智能体合作与对话效率的因素。

Abstract: Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are
gaining attention, yet fewer studies explore their team dynamics. Inspired by
human team science, we propose a multi-agent framework to examine core aspects
of team science: structure, diversity, and interaction dynamics. We evaluate
team performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and
Latent Implicit Hate, spanning commonsense and social reasoning. Our results
show that flat teams tend to perform better than hierarchical ones, while
diversity has a nuanced impact. Interviews suggest agents are overconfident
about their team performance, yet post-task reflections reveal both
appreciation for collaboration and challenges in integration, including limited
conversational coordination.

</details>


### [113] [Can Speech LLMs Think while Listening?](https://arxiv.org/abs/2510.07497)
*Yi-Jen Shih,Desh Raj,Chunyang Wu,Wei Zhou,SK Bong,Yashesh Gaur,Jay Mahadeokar,Ozlem Kalinli,Mike Seltzer*

Main category: cs.CL

TL;DR: 本文研究了多流语音大模型（speech LLMs）在复杂推理任务上的能力提升，提出有效减少推理延迟的新方法，并在多个语音推理任务上大幅提升准确率和交互体验。


<details>
  <summary>Details</summary>
Motivation: 虽然语音大模型已实现流畅的语音交互，但其在复杂推理任务上表现不佳。借鉴文本大模型在链式思考（CoT）上的成功，作者希望探索如何将这一优势迁移到语音领域，并解决语音交互中的实时性难题。

Method: 1）将链式思考（CoT）微调方法应用于多流语音大模型的文本空间推理；2）提出基于熵的“问题完整度”指标，让模型能在用户语音输入尚未结束时提前推理以降低延迟；3）引入直接偏好优化（DPO）方法，利用偏好数据进一步优化准确率-延迟表现。

Result: 1）在一系列语音推理任务中，通过文本空间推理平均提升准确率2.4倍；2）在ARC-Easy任务上，在相同延迟下实现4%的准确率提升；3）使用DPO后可在不损失准确率的前提下，将延迟减少70%。

Conclusion: 通过链式思考微调、创新延迟控制方法及直接偏好优化，显著提升了语音大模型的复杂推理能力和交互实时性，在保证准确率的同时极大改善了语音智能体的响应速度和体验。

Abstract: Recent advances in speech large language models (speech LLMs) have enabled
seamless spoken interactions, but these systems still struggle with complex
reasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning
has been to shown to significantly improve the reasoning abilities of
text-based LLMs. In this work, we investigate the effect of CoT fine-tuning for
multi-stream speech LLMs, demonstrating that reasoning in text space improves
the accuracy of speech LLMs by 2.4x, on average, over a suite of spoken
reasoning tasks. Beyond accuracy, the latency of the spoken response is a
crucial factor for interacting with voice-based agents. Inspired by the human
behavior of "thinking while listening," we propose methods to reduce the
additional latency from reasoning by allowing the model to start reasoning
before the user query has ended. To achieve this, we introduce an entropy-based
metric, "question completeness," which acts as an indicator to guide the model
on the optimal time to start reasoning. This method provides greater control
over the accuracy-latency trade-off compared with heuristic-based approaches
and, under equivalent latency conditions, yields a 4% accuracy gain on
ARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference
data created using rejection sampling to push the accuracy-latency pareto
frontier further, resulting in a 70% reduction in latency without loss in
accuracy.

</details>


### [114] [When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs](https://arxiv.org/abs/2510.07499)
*Soyeong Jeong,Taehee Jung,Sung Ju Hwang,Joo-Kyung Kim,Dongyeop Kang*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，利用思维模板（thought templates）提升长上下文语言模型（LCLMs）在多文档、多跳推理中的表现。通过模板结构化地组织证据，促进信息整合与推理，实验表明该方法优于各类强基线，并可迁移到小模型上。


<details>
  <summary>Details</summary>
Motivation: 尽管长上下文语言模型能够处理大量文档，但单纯地增加文档数量未能有效帮助证据连接与多跳推理。因此，亟需方法提升模型在证据整合和推理方面的能力。

Method: 作者提出了“思维模板”方法，将推理过程重构为可重用的推理缓存，通过分析历史问题求解轨迹，归纳证据合成的模板，并利用自然语言反馈迭代优化模板。通过思维模板指导模型如何整合事实文档进行多跳推理。

Result: 在多种基准测试和多类LCLM模型中，该方法在基于检索和无需检索两类场景下均超越了强大基线。实验证明，优化后的思维模板还可被蒸馏进体积更小的开源模型，实现更广泛的应用。

Conclusion: 思维模板增强框架（ToTAL）能有效提升LCLM多文档、多跳推理能力，兼具透明推理和较强迁移能力，推动长上下文大模型实际应用进展。

Abstract: Recent Long-Context Language Models (LCLMs) can process hundreds of thousands
of tokens in a single prompt, enabling new opportunities for
knowledge-intensive multi-hop reasoning by integrating large sets of retrieved
documents or, in some cases, directly all necessary information. However,
simply feeding more documents into the context window fails to capture how
evidence should be connected. We address this gap with thought templates, which
recast reasoning as reusable thought caches, derived from prior problem solving
traces, structuring how evidence is combined and guiding multi-hop inference
with factual documents. To keep these templates effective, we propose an update
strategy that iteratively refines templates derived from training data through
natural-language feedback. Across diverse benchmarks and LCLM families, our
approach delivers consistent gains over strong baselines in both
retrieval-based and retrieval-free settings. Furthermore, we show that
optimized templates can be distilled into smaller open-source models,
demonstrating its broad applicability and transparent reasoning reuse. We refer
to our framework as Thought Template Augmented LCLMs (ToTAL).

</details>


### [115] [ParsTranslit: Truly Versatile Tajik-Farsi Transliteration](https://arxiv.org/abs/2510.07520)
*Rayyan Merchant,Kevin Tang*

Main category: cs.CL

TL;DR: 本文提出了一个面向波斯语-塔吉克语两种书写体系（Perso-Arabic和Tajik-Cyrillic）互转的新型序列到序列转写模型，经过多数据集训练并设定了新基准。


<details>
  <summary>Details</summary>
Motivation: 由于波斯语在不同地区（伊朗、阿富汗与塔吉克斯坦）采用不同文字系统，阻碍了书面沟通。以往的转写模型数据单一，适用范围有限，难以应对不同文本领域。

Method: 作者整合了所有公开可用的数据集，并自建两个新数据集，采用先进的序列到序列（seq2seq）方法进行训练。模型针对Farsi和Tajik两种脚本互转任务，进行全面评估。

Result: 该模型在Farsi到Tajik和Tajik到Farsi转换任务中分别获得了chrF++分数87.91、92.28，Normalized CER分别为0.05、0.04，显著优于以往方法。

Conclusion: 提出的模型不仅新版基准综合性强、适用领域广，也首次给出全面的跨领域转写评估，成果和数据已全部开源。

Abstract: As a digraphic language, the Persian language utilizes two written standards:
Perso-Arabic in Afghanistan and Iran, and Tajik-Cyrillic in Tajikistan. Despite
the significant similarity between the dialects of each country, script
differences prevent simple one-to-one mapping, hindering written communication
and interaction between Tajikistan and its Persian-speaking ``siblings''. To
overcome this, previously-published efforts have investigated machine
transliteration models to convert between the two scripts. Unfortunately, most
efforts did not use datasets other than those they created, limiting these
models to certain domains of text such as archaic poetry or word lists. A truly
usable transliteration system must be capable of handling varied domains,
meaning that suck models lack the versatility required for real-world usage.
The contrast in domain between data also obscures the task's true difficulty.
We present a new state-of-the-art sequence-to-sequence model for Tajik-Farsi
transliteration trained across all available datasets, and present two datasets
of our own. Our results across domains provide clearer understanding of the
task, and set comprehensive comparable leading benchmarks. Overall, our model
achieves chrF++ and Normalized CER scores of 87.91 and 0.05 from Farsi to Tajik
and 92.28 and 0.04 from Tajik to Farsi. Our model, data, and code are available
at https://anonymous.4open.science/r/ParsTranslit-FB30/.

</details>


### [116] [OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs](https://arxiv.org/abs/2510.07535)
*Jaeseong Lee,seung-won hwang,Aurick Qiao,Gabriele Oliaro,Ye Wang,Samyam Rajbhandari*

Main category: cs.CL

TL;DR: 提出OWL模型与LongSpecBench基准，使推理加速方法在长上下文场景下表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的推理加速方法（speculative decoding）在现实长上下文输入下效果大幅下降，急需新的方案解决实际应用中的性能瓶颈。

Method: 1.发布长上下文基准LongSpecBench。
2.提出OWL模型：
  (1) 使用仅依赖最后token状态的LSTM drafter，以适应任意长度输入；
  (2) 在verifier中引入[SPEC]特殊token，提升drafter表示能力；
  (3) 采用混合tree与非tree解码算法。

Result: OWL在长上下文输入下的接受长度比EAGLE3高约5倍，显著提升推理速度和质量。

Conclusion: OWL与LongSpecBench能有效推动speculative decoding在现实长文本推理中的应用，公开代码数据有助于领域进步。

Abstract: Speculative decoding promises faster inference for large language models
(LLMs), yet existing methods fail to generalize to real-world settings.
Benchmarks typically assume short contexts (e.g., 2K tokens), whereas practical
workloads involve long contexts. We find current approaches degrade severely
with long contexts; for instance, EAGLE3 even slows down the generation speed
by 0.81x. We address these limitations by releasing a new long-context
benchmark (LongSpecBench) and introducing a novel model (OWL). OWL achieves
about 5x higher acceptance length than EAGLE3 on long-context inputs through
three innovations: (1) an LSTM-based drafter conditioned only on the last-token
state, making it generalize to various lengths, (2) a special token [SPEC] in
the verifier that produces richer representation for drafter, and (3) a hybrid
algorithm combining both tree and non-tree decoding methods. We release all
code and datasets to advance future research.

</details>


### [117] [Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices](https://arxiv.org/abs/2510.07545)
*Md Tahmid Rahman Laskar,Mohammed Saidul Islam,Ridwan Mahbub,Mizanur Rahman,Amran Bhuiyan,Israt Jahan,Mir Tafseer Nayeem,Shafiq Joty,Enamul Hoque,Jimmy Huang*

Main category: cs.CL

TL;DR: 本文旨在提升小型视觉-语言模型（<=2B参数）在图表理解评判任务中的表现，并提出两种高效评估方法：多准则提示和领域自适应迁移学习。


<details>
  <summary>Details</summary>
Motivation: 大模型如7B参数LVLM在图表理解中已有良好表现，但小模型的评判能力远逊于大模型，限制了其在资源受限场景下的应用。因此需开发适合小型模型的高效评估方式。

Method: 提出两种方法：（1）多准则提示——将多项评判标准整合为单个查询，提升评判效率；（2）领域自适应迁移学习——基于合成数据对2B参数LVLM微调，得到专用评判模型ChartJudge。并通过不同模型和数据集进行系统实验分析。

Result: 多准则提示揭示出7B大模型对提示鲁棒性的不足，导致应用该方法时性能大幅降低。ChartJudge小模型能从一个数据集有效迁移到另一个，取得更专业的评判效果。对不同图表类型与查询复杂度的细致分析，为模型大小、提示设计和迁移能力的权衡提供了实用见解。

Conclusion: 通过多准则提示和领域迁移，可显著提升小型LVLM在低成本、可扩展的图表推理评估任务中的能力，并为实际设计和部署提供理论与实证支持。代码和数据将公开，便于进一步研究和应用。

Abstract: Large Vision-Language Models (LVLMs) with only 7B parameters have shown
promise as automated judges in chart comprehension tasks. However, tiny models
(<=2B parameters) still perform poorly as judges, limiting their real-world use
in resource-constrained settings. To address this, we propose two approaches to
ensure cost-efficient evaluation: (i) multi-criteria prompting, which combines
separate evaluation criteria into a single query, and (ii) domain-adaptive
transfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic
judgments in a chart dataset to create the ChartJudge. Experiments show that
multi-criteria prompting exposes robustness gaps, which led to a huge drop in
performance for 7B models, including specialized LVLM judges like LLaVA-Critic.
In addition, we find that our tiny LVLM (ChartJudge) can effectively transfer
knowledge from one dataset to another to make it a more specialized model. Our
fine-grained analysis across chart types and query complexities offers
actionable insights into trade-offs between model size, prompt design, and
transferability, enabling scalable, low-cost evaluation for chart reasoning
tasks. Our code and the data will be made publicly available.

</details>


### [118] [Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER](https://arxiv.org/abs/2510.07566)
*Junyi Zhu,Savas Ozkan,Andrea Maracani,Sinan Mutlu,Cho Jung Min,Mete Ozay*

Main category: cs.CL

TL;DR: 本文旨在提升轻量级BERT模型在移动端跨任务适应能力，通过引入以任务为主的LoRA模块进行多任务预微调，有效提升了命名实体识别（NER）和文本分类的任务表现，并且不会增加显著的模型开销。


<details>
  <summary>Details</summary>
Motivation: 随着NLP模型在移动平台上的应用需求增长，模型需兼顾高适应性与低资源消耗。现有预微调方法在多任务场景下存在优化冲突，影响整体表现。因此，亟需寻找有效的多任务预微调策略。

Method: 提出一种基于任务主导LoRA模块的多任务预微调框架，将模块化适配器集成于共享编码器主干，实现不同任务间参数的合理分配和隔离，从而提升多任务适应性。

Result: 在21个下游任务上进行了实验，NER平均提升0.8%，文本分类平均提升8.8%，且性能与单任务预微调相当，满足移动端部署需求。

Conclusion: 提出的多任务预微调框架在提升轻量级编码器多任务能力的同时兼顾模型效率，为移动端多场景NLP应用提供了有效方案。

Abstract: Deploying natural language processing (NLP) models on mobile platforms
requires models that can adapt across diverse applications while remaining
efficient in memory and computation. We investigate pre-finetuning strategies
to enhance the adaptability of lightweight BERT-like encoders for two
fundamental NLP task families: named entity recognition (NER) and text
classification. While pre-finetuning improves downstream performance for each
task family individually, we find that na\"ive multi-task pre-finetuning
introduces conflicting optimization signals that degrade overall performance.
To address this, we propose a simple yet effective multi-task pre-finetuning
framework based on task-primary LoRA modules, which enables a single shared
encoder backbone with modular adapters. Our approach achieves performance
comparable to individual pre-finetuning while meeting practical deployment
constraint. Experiments on 21 downstream tasks show average improvements of
+0.8% for NER and +8.8% for text classification, demonstrating the
effectiveness of our method for versatile mobile NLP applications.

</details>


### [119] [Linguistic Patterns in Pandemic-Related Content: A Comparative Analysis of COVID-19, Constraint, and Monkeypox Datasets](https://arxiv.org/abs/2510.07579)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Main category: cs.CL

TL;DR: 本文通过计算语言学方法，分析了疫情相关网络言论中的健康谣言与真实信息在语言上的差异，揭示了谣言具有更低可读性和更高的恐惧、劝服性词汇频率。


<details>
  <summary>Details</summary>
Motivation: 随着疫情期间健康谣言泛滥，准确区分谣言与真实信息对于公共健康至关重要。本研究旨在通过语言特征分析，寻找区分谣言与真实信息的线索，助力健康信息传播和谣言管理。

Method: 基于三类语料库（新冠虚假叙述、一般新冠内容、猴痘相关帖子），比较其可读性、修辞标记、说服性语言等特征，通过统计分析揭示不同内容的语言区别。

Result: 新冠谣言的可读性显著低于其他内容，恐惧及劝服性词频为其他内容两倍以上，且很少使用感叹号（区别于猴痘更具情感色彩的内容）；谣言在情感线索和复杂修辞的结合上更突出。

Conclusion: 谣言往往采用复杂的修辞和情感暗示，提升其可信度。研究为谣言识别提供了语言指标，也对公共健康传播策略和危机传播理论模型有指导意义。但因方法局限，建议未来采用更丰富的情感词典和动态、平台差异化分析。

Abstract: This study conducts a computational linguistic analysis of pandemic-related
online discourse to examine how language distinguishes health misinformation
from factual communication. Drawing on three corpora: COVID-19 false narratives
(n = 7588), general COVID-19 content (n = 10700), and Monkeypox-related posts
(n = 5787), we identify significant differences in readability, rhetorical
markers, and persuasive language use. COVID-19 misinformation exhibited
markedly lower readability scores and contained over twice the frequency of
fear-related or persuasive terms compared to the other datasets. It also showed
minimal use of exclamation marks, contrasting with the more emotive style of
Monkeypox content. These patterns suggest that misinformation employs a
deliberately complex rhetorical style embedded with emotional cues, a
combination that may enhance its perceived credibility. Our findings contribute
to the growing body of work on digital health misinformation by highlighting
linguistic indicators that may aid detection efforts. They also inform public
health messaging strategies and theoretical models of crisis communication in
networked media environments. At the same time, the study acknowledges
limitations, including reliance on traditional readability indices, use of a
deliberately narrow persuasive lexicon, and reliance on static aggregate
analysis. Future research should therefore incorporate longitudinal designs,
broader emotion lexicons, and platform-sensitive approaches to strengthen
robustness.

</details>


### [120] [IASC: Interactive Agentic System for ConLangs](https://arxiv.org/abs/2510.07591)
*Chihiro Taguchi,Richard Sproat*

Main category: cs.CL

TL;DR: 该论文提出了一个利用大语言模型（LLM）辅助人造语言（构建语言）开发的系统，流程包括构建目标语音系统、形态句法标注、词汇生成、正字法设计以及语法手册撰写，并对LLM在语言知识方面的能力进行了探讨。


<details>
  <summary>Details</summary>
Motivation: 一方面，为语言爱好者或研究者开发有趣且实用的构建语言辅助工具；另一方面，探索和评估LLM对语言与语言学概念的理解深度和能力差异。

Method: 系统采用模块化设计，首先利用LLM通过反馈迭代完善目标语言的语音系统，随后将英语句子转为目标语言的形态句法标注，再基于此生成词汇，并依托现有文字系统设计正字法，最后由模型生成语言语法手册。实验还测试了该方法对高资源语言向低资源语言翻译的能力。

Result: 不同LLM和不同语言规范之间能力差距较大，LLM在处理常见语言模式上表现更好。将该方法用于高资源至低资源语言的翻译，结果主要为负，但有证据显示改进系统后该方向仍有潜在提升空间。

Conclusion: LLM可显著辅助构建语言的多个环节，但当前系统在处理罕见语言特征及低资源情况时仍有限制。未来系统优化有望提升其在实际语言开发和低资源语言翻译上的表现。

Abstract: We present a system that uses LLMs as a tool in the development of
Constructed Languages. The system is modular in that one first creates a target
phonology for the language using an agentic approach that refines its output at
each step with commentary feedback on its previous attempt. Next, a set of
sentences is 'translated' from their English original into a morphosyntactic
markup that reflects the word order and morphosyntactic feature specifications
of the desired target language, with affixes represented as morphosyntactic
feature bundles. From this translated corpus, a lexicon is constructed using
the phonological model and the set of morphemes (stems and affixes) extracted
from the 'translated' sentences. The system is then instructed to provide an
orthography for the language, using an existing script such as Latin or
Cyrillic. Finally, the system writes a brief grammatical handbook of the
language. The system can also translate further sentences into the target
language.
  Our goal is twofold. First, we hope that these tools will be fun to use for
creating artificially constructed languages. Second, we are interested in
exploring what LLMs 'know' about language-not what they know about any
particular language or linguistic phenomenon, but how much they know about and
understand language and linguistic concepts. As we shall see, there is a fairly
wide gulf in capabilities both among different LLMs and among different
linguistic specifications, with it being notably easier for systems to deal
with more common patterns than rarer ones. An additional avenue that we explore
is the application of our approach to translating from high-resource into
low-resource languages. While the results so far are mostly negative, we
provide some evidence that an improved version of the present system could
afford some real gains in such tasks.
  https://github.com/SakanaAI/IASC

</details>


### [121] [Vocabulary embeddings organize linguistic structure early in language model training](https://arxiv.org/abs/2510.07613)
*Isabel Papadimitriou,Jacob Prince*

Main category: cs.CL

TL;DR: 本文通过分析大语言模型在训练过程中输入词嵌入的几何结构，揭示了不同词类在语义、句法和频率上的动态演化过程。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型输入词表向量（embedding）在训练期间的结构如何组织与演化，尤其想要揭示词嵌入如何与语言的语义、句法和词频相关。该问题有助于理解模型获得语言能力的机制。

Method: 利用表示相似性分析（Representational Similarity Analysis），在训练过程中，分别将输入和输出的嵌入空间与语义、句法以及词频相关性进行多轮度量。实验对象为两款主流、开源的大语言模型（Pythia 12B和 OLMo 7B）。

Result: （1）词表嵌入空间在模型训练早期迅速收敛，并与语义、句法特征高度相关；（2）高频词和功能词的向量比低频词和实词更快收敛，而低频词仍保持部分初始随机分布的特征。

Conclusion: 词嵌入的结构变化揭示了词频和词类在语言模型训练中的重要作用，有助于理解模型语言能力形成的内部机制；未来可以进一步研究词表几何结构进化对特定模型能力提升的作用。

Abstract: Large language models (LLMs) work by manipulating the geometry of input
embedding vectors over multiple layers. Here, we ask: how are the input
vocabulary representations of language models structured, and how and when does
this structure evolve over training? To answer this question, we use
representational similarity analysis, running a suite of experiments that
correlate the geometric structure of the input embeddings and output embeddings
of two open-source models (Pythia 12B and OLMo 7B) with semantic, syntactic,
and frequency-based metrics over the course of training. Our key findings are
as follows: 1) During training, the vocabulary embedding geometry quickly
converges to high correlations with a suite of semantic and syntactic features;
2) Embeddings of high-frequency and function words (e.g., "the," "of") converge
to their final vectors faster than lexical and low-frequency words, which
retain some alignment with the bias in their random initializations. These
findings help map the dynamic trajectory by which input embeddings organize
around linguistic structure, revealing distinct roles for word frequency and
function. Our findings motivate a deeper study of how the evolution of
vocabulary geometry may facilitate specific capability gains during model
training.

</details>


### [122] [Toward Reliable Clinical Coding with Language Models: Verification and Lightweight Adaptation](https://arxiv.org/abs/2510.07629)
*Zhangdie Yuan,Han-Chin Shing,Mitch Strong,Chaitanya Shivade*

Main category: cs.CL

TL;DR: 本文提出了一种通过验证机制改进大语言模型（LLM）进行医学编码的新方法，能有效减少编码层级错误，并提供了新的数据集和实验验证。


<details>
  <summary>Details</summary>
Motivation: LLM在临床编码任务上的精确度不佳，尤其在层级相近但仍错误的编码上表现不足，而现有评价指标难以揭示这类错误。现有数据集也存在证据不全和偏向住院患者的问题。

Method: 分析了LLM编码失败的主要类型，引入了基于prompt engineering和小规模微调的轻量化干预，提出了临床编码验证作为独立任务和流程组件，同时发布了双专家注释的门诊病历数据集。

Result: 轻量级干预提升了模型精度；编码验证能有效辨识和纠正层级近似编码错误；新数据集克服了原有数据的不完整缺陷。

Conclusion: 编码验证是提升LLM医学编码准确性的重要方法，小规模干预可在不显著增加计算成本的同时取得明显效果，新发布的数据集有助于后续研究和应用。

Abstract: Accurate clinical coding is essential for healthcare documentation, billing,
and decision-making. While prior work shows that off-the-shelf LLMs struggle
with this task, evaluations based on exact match metrics often overlook errors
where predicted codes are hierarchically close but incorrect. Our analysis
reveals that such hierarchical misalignments account for a substantial portion
of LLM failures. We show that lightweight interventions, including prompt
engineering and small-scale fine-tuning, can improve accuracy without the
computational overhead of search-based methods. To address hierarchically
near-miss errors, we introduce clinical code verification as both a standalone
task and a pipeline component. To mitigate the limitations in existing
datasets, such as incomplete evidence and inpatient bias in MIMIC, we release
an expert double-annotated benchmark of outpatient clinical notes with ICD-10
codes. Our results highlight verification as an effective and reliable step
toward improving LLM-based medical coding.

</details>


### [123] [Role-Conditioned Refusals: Evaluating Access Control Reasoning in Large Language Models](https://arxiv.org/abs/2510.07642)
*Đorđe Klisura,Joseph Khoury,Ashish Kundu,Ram Krishnan,Anthony Rios*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型(LLM)在访问控制场景中的拒绝能力，提出并评估了三种实现机制，并发布了相关数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在实际应用中的普及，如何确保其在多角色权限环境下遵循访问控制政策变得尤为重要。现有LLM容易突破角色边界，可能会输出不受限制的敏感信息。因此，研究如何让LLM在面对不同权限请求时能够准确执行访问控制并做出拒绝，具有很强的现实意义。

Method: 作者基于Spider和BIRD两个text-to-SQL数据集，扩展并加入了PostgreSQL的基于角色的访问控制策略(RBAC)，覆盖表和列的权限，构建了新的测试数据集。比较了三类方法：(1) 基于零样本或少量样本的Prompt方法；(2) 两步法：先生成SQL再由验证器比对政策做拒绝决策；(3) 利用LoRA微调，让模型直接学习权限意识。

Result: 实验显示，两步法(显式验证)能提升拒绝准确率，同时降低错误许可；LoRA微调法则在安全性和可用性(即SQL执行正确率)之间取得了更优平衡。随着访问控制策略的复杂度增加，所有方法的可靠性均有所降低。

Conclusion: 显式验证机制更擅长于安全性把控，而微调模型在安全性与实际应用中的效用之间达成了更佳平衡。长且复杂的策略会降低LLM拒绝的可靠性。作者公开了带RBAC标注的数据集和代码，为后续研究提供了资源。

Abstract: Access control is a cornerstone of secure computing, yet large language
models often blur role boundaries by producing unrestricted responses. We study
role-conditioned refusals, focusing on the LLM's ability to adhere to access
control policies by answering when authorized and refusing when not. To
evaluate this behavior, we created a novel dataset that extends the Spider and
BIRD text-to-SQL datasets, both of which have been modified with realistic
PostgreSQL role-based policies at the table and column levels. We compare three
designs: (i) zero or few-shot prompting, (ii) a two-step generator-verifier
pipeline that checks SQL against policy, and (iii) LoRA fine-tuned models that
learn permission awareness directly. Across multiple model families, explicit
verification (the two-step framework) improves refusal precision and lowers
false permits. At the same time, fine-tuning achieves a stronger balance
between safety and utility (i.e., when considering execution accuracy). Longer
and more complex policies consistently reduce the reliability of all systems.
We release RBAC-augmented datasets and code.

</details>


### [124] [Banking Done Right: Redefining Retail Banking with Language-Centric AI](https://arxiv.org/abs/2510.07645)
*Xin Jie Chua,Jeraelyn Ming Li Tan,Jia Xuan Tan,Soon Chang Poh,Yi Xian Goh,Debbie Hui Tian Choong,Chee Mun Foong,Sze Jue Yang,Chee Seng Chan*

Main category: cs.CL

TL;DR: 本文介绍了Ryt AI，这是一个由大型语言模型（LLM）驱动的智能体框架，首次在全球范围内部署获监管批准，允许用户通过自然语言会话来进行银行的核心金融交易。该系统已实际应用于Ryt Bank，并直接作为银行主要业务接口。


<details>
  <summary>Details</summary>
Motivation: 现有的会话AI在银行业务中多被局限于顾问或辅助角色，不能直接执行金融交易。本文旨在探索并实现通过自然语言让客户完成真实的、核心金融交易，并且获得监管批准，打破传统银行多屏幕、繁琐操作流程。

Method: Ryt AI完全自研，使用内部开发的封闭源LLM（ILMU），由四个任务专用的LLM Agent（守护、意图、支付、FAQ）组成，每个Agent利用LoRA适配器定制任务。所有模型部署在银行内部系统，保障数据安全和行为一致性，并结合确定性约束、人机协同确认、无状态审计架构等多重安全合规措施。

Result: 系统已实际上线应用，被监管机构批准作为银行核心交易的主接口。通过对核心金融业务的支持，验证了自然语言接口在严格治理条件下的可靠性和安全性。

Conclusion: Ryt AI证明在监管批准下，自然语言接口能够安全、高效地支持银行核心业务操作，为全球金融服务行业的交互范式带来新的变革。

Abstract: This paper presents Ryt AI, an LLM-native agentic framework that powers Ryt
Bank to enable customers to execute core financial transactions through natural
language conversation. This represents the first global regulator-approved
deployment worldwide where conversational AI functions as the primary banking
interface, in contrast to prior assistants that have been limited to advisory
or support roles. Built entirely in-house, Ryt AI is powered by ILMU, a
closed-source LLM developed internally, and replaces rigid multi-screen
workflows with a single dialogue orchestrated by four LLM-powered agents
(Guardrails, Intent, Payment, and FAQ). Each agent attaches a task-specific
LoRA adapter to ILMU, which is hosted within the bank's infrastructure to
ensure consistent behavior with minimal overhead. Deterministic guardrails,
human-in-the-loop confirmation, and a stateless audit architecture provide
defense-in-depth for security and compliance. The result is Banking Done Right:
demonstrating that regulator-approved natural-language interfaces can reliably
support core financial operations under strict governance.

</details>


### [125] [OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2510.07651)
*Yuzhe Gu,Xiyu Liang,Jiaojiao Zhao,Enmao Diao*

Main category: cs.CL

TL;DR: 本文提出了一种新的LLM缓存管理方法OBCache，通过更精确地衡量每个token对输出的影响，以提升内存效率且提升长文本推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型为了支持更长上下文，需缓存大量key-value状态，内存压力急剧上升。虽然已有方法尝试基于注意力权重启发式地淘汰不重要token，但未能真正反映token对输出的实际影响。

Method: 作者将缓存淘汰视为结构化剪枝（layer-wise structured pruning）问题，基于Optimal Brain Damage理论，提出OBCache从输出扰动角度量化每个token的重要性，推导出封闭形式的saliency分数，综合attention权重、value状态等输出相关信息，用于更有效地决策缓存淘汰。

Result: 在LLaMA和Qwen等模型实验中，OBCache的输出敏感型分数替代现有启发式分数后，无论各query位置，缓存淘汰后模型在长上下文任务中准确率都有提升。

Conclusion: OBCache为内存受限场景下提升大模型长序列处理效果提供了一种更高效且理论支持更强的方法，优于现有注意力权重启发式缓存淘汰方案。

Abstract: Large language models (LLMs) with extended context windows enable powerful
downstream applications but impose significant memory overhead, as caching all
key-value (KV) states scales linearly with sequence length and batch size.
Existing cache eviction methods address this by exploiting attention sparsity,
yet they typically rank tokens heuristically using accumulated attention
weights without considering their true impact on attention outputs. We propose
Optimal Brain Cache (OBCache), a principled framework that formulates cache
eviction as a layer-wise structured pruning problem. Building upon the Optimal
Brain Damage (OBD) theory, OBCache quantifies token saliency by measuring the
perturbation in attention outputs induced by pruning tokens, with closed-form
scores derived for isolated keys, isolated values, and joint key-value pairs.
Our scores account not only for attention weights but also for information from
value states and attention outputs, thereby enhancing existing eviction
strategies with output-aware signals. Experiments on LLaMA and Qwen models
demonstrate that replacing the heuristic scores in existing works, which
estimate token saliency across different query positions, with OBCache's
output-aware scores consistently improves long-context accuracy.

</details>


### [126] [Textual Entailment and Token Probability as Bias Evaluation Metrics](https://arxiv.org/abs/2510.07662)
*Virginia K. Felkner,Allison Lim,Jonathan May*

Main category: cs.CL

TL;DR: 本文比较了两种常见的语言模型社会偏见测量方法：Token Probability (TP) 和自然语言推断 (NLI) 指标，发现两者在评估偏见时存在较大差异，并提出两者应结合使用以更全面地评估偏见。


<details>
  <summary>Details</summary>
Motivation: 当前对语言模型社会偏见的测量主要依赖于TP指标，但这种方法与实际应用场景有一定距离，准确性受到质疑。因此，作者希望通过更贴近实际应用的NLI方法，探讨更合适的偏见评估指标。

Method: 作者采用TP和NLI两类不同的偏见测量方法对语言模型进行评估，通过比较两类指标在不同场景下对偏见的检测表现，并分析它们之间的相关性和优缺点。

Result: 实验结果显示，NLI和TP偏见评测在检测偏见时表现出明显的差异，且不同NLI指标之间、NLI与TP之间相关性很低。NLI方法更容易检测到“去偏见不充分”的情况，但同时对反刻板印象句子的表述更为敏感，稳定性较差。

Conclusion: TP与NLI方法各有优缺点，在所有场景下都不能单独作为最佳的偏见测量手段。作者建议应结合TP、NLI及下游任务中的偏见评测，共同实现对语言模型偏见更全面和有效的评估。

Abstract: Measurement of social bias in language models is typically by token
probability (TP) metrics, which are broadly applicable but have been criticized
for their distance from real-world langugage model use cases and harms. In this
work, we test natural language inference (NLI) as a more realistic alternative
bias metric. We show that, curiously, NLI and TP bias evaluation behave
substantially differently, with very low correlation among different NLI
metrics and between NLI and TP metrics. We find that NLI metrics are more
likely to detect "underdebiased" cases. However, NLI metrics seem to be more
brittle and sensitive to wording of counterstereotypical sentences than TP
approaches. We conclude that neither token probability nor natural language
inference is a "better" bias metric in all cases, and we recommend a
combination of TP, NLI, and downstream bias evaluations to ensure comprehensive
evaluation of language models.
  Content Warning: This paper contains examples of anti-LGBTQ+ stereotypes.

</details>


### [127] [Stress-Testing Model Specs Reveals Character Differences among Language Models](https://arxiv.org/abs/2510.07686)
*Jifan Zhang,Henry Sleight,Andi Peng,John Schulman,Esin Durmus*

Main category: cs.CL

TL;DR: 本论文系统性地测试大语言模型规范之间的冲突与不足，发现现有模型在处理价值冲突时表现出大量分歧，暴露出指导原则间的矛盾和理解歧义。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的行为规范和道德原则存在自相矛盾和覆盖不全等问题，这严重影响了模型在复杂情境下的表现。研究动机是揭示并系统测试这些规范在实际应用中的薄弱环节。

Method: 作者提出了一种系统性压力测试方法，通过自动生成必须在两种冲突价值原则间做权衡的情境，系统地测试并分析多个主流LLM的行为差异，量化模型表现分歧并进行定性分析。

Result: 通过对十二个主流大模型的广泛测试，论文发现了超过7万个表现出明显分歧的案例，并深入分析了当前规范存在的直接冲突和理解歧义等问题，同时揭示了各种不对齐和误拒绝案例。

Conclusion: 现有大模型规范在复杂价值权衡场景下表现不佳，存在原则冲突与指令不明确等问题。论文所提供的压力测试和数据集可为后续模型规范完善和价值权衡研究提供参考和工具。

Abstract: Large language models (LLMs) are increasingly trained from AI constitutions
and model specifications that establish behavioral guidelines and ethical
principles. However, these specifications face critical challenges, including
internal conflicts between principles and insufficient coverage of nuanced
scenarios. We present a systematic methodology for stress-testing model
character specifications, automatically identifying numerous cases of principle
contradictions and interpretive ambiguities in current model specs.
  We stress test current model specs by generating scenarios that force
explicit tradeoffs between competing value-based principles. Using a
comprehensive taxonomy we generate diverse value tradeoff scenarios where
models must choose between pairs of legitimate principles that cannot be
simultaneously satisfied. We evaluate responses from twelve frontier LLMs
across major providers (Anthropic, OpenAI, Google, xAI) and measure behavioral
disagreement through value classification scores. Among these scenarios, we
identify over 70,000 cases exhibiting significant behavioral divergence.
Empirically, we show this high divergence in model behavior strongly predicts
underlying problems in model specifications. Through qualitative analysis, we
provide numerous example issues in current model specs such as direct
contradiction and interpretive ambiguities of several principles. Additionally,
our generated dataset also reveals both clear misalignment cases and
false-positive refusals across all of the frontier models we study. Lastly, we
also provide value prioritization patterns and differences of these models.

</details>


### [128] [Large Language Models Meet Virtual Cell: A Survey](https://arxiv.org/abs/2510.07706)
*Krinos Li,Xianglu Xiao,Shenglong Deng,Lucas He,Zijun Zhong,Yuanjie Zou,Zhonghao Zhan,Zheng Hui,Weiye Bao,Guang Yang*

Main category: cs.CL

TL;DR: 本论文综述了大模型在虚拟细胞建模中的方法和应用，提出了系统性框架并总结了相关挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的发展，其在细胞生物学中的应用日益重要，特别是在构建和理解虚拟细胞系统方面。研究者希望梳理当前进展，并提出分类体系以指导后续研究。

Method: 作者提出将现有方法分为两类：一是LLMs作为“神谕者”（Oracles）直接建模细胞状态，二是LLMs作为“代理者”（Agents）用于调度与推理等复杂科学任务。同时，归纳出虚拟细胞建模的三项核心任务：细胞表示、扰动预测和基因调控推断，并系统回顾了相关模型、数据集和评测基准。

Result: 论文归纳了大量基于LLMs的虚拟细胞建模方法，对每种任务的主流研究、使用的数据和评测方法进行了综述，也讨论了当前主要面临的难点，包括可扩展性、泛化能力和可解释性等。

Conclusion: 大型语言模型正推动虚拟细胞领域持续发展，未来研究需要解决模型扩展、泛用性与可解释性等关键挑战，以推动细胞生物学的深度智能化和自动化。

Abstract: Large language models (LLMs) are transforming cellular biology by enabling
the development of "virtual cells"--computational systems that represent,
predict, and reason about cellular states and behaviors. This work provides a
comprehensive review of LLMs for virtual cell modeling. We propose a unified
taxonomy that organizes existing methods into two paradigms: LLMs as Oracles,
for direct cellular modeling, and LLMs as Agents, for orchestrating complex
scientific tasks. We identify three core tasks--cellular representation,
perturbation prediction, and gene regulation inference--and review their
associated models, datasets, evaluation benchmarks, as well as the critical
challenges in scalability, generalizability, and interpretability.

</details>


### [129] [Causality Guided Representation Learning for Cross-Style Hate Speech Detection](https://arxiv.org/abs/2510.07707)
*Chengshuai Zhao,Shu Wan,Paras Sheth,Karan Patwa,K. Selçuk Candan,Huan Liu*

Main category: cs.CL

TL;DR: 网络仇恨言论中隐晦表达形式难以检测，现有模型对风格变化泛化能力弱。作者提出了以因果图为基础的CADET方法，通过因果表征学习，提高了多平台、多风格下仇恨言论检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前主流的仇恨言论检测模型主要依赖表层语言线索，对讽刺、隐语等隐晦仇恨言论识别力差，且不同平台的仇恨对象和表达风格不同，导致模型易被表层特征误导，泛化性能差。为此，作者希望通过建模仇恨言论生成的因果结构，提升检测准确性和泛化能力。

Method: 作者提出CADET（Causal representation learning framework），以因果图建模仇恨言论的上下文环境、创作者动机、攻击对象和表达风格等关键因素，利用因果表征学习分离出解释性强的潜变量，并控制混杂因素，从而隔离出真正的仇恨意图。同时，方法可在潜在空间对风格变量进行反事实干预，提高对不同变体仇恨言论的检测鲁棒性。

Result: CADET通过多项综合实验，表现优于传统和现有先进模型，在多风格、多平台下展现了更强的泛化检测能力。

Conclusion: 以因果推断为先验的CADET框架有效提升了仇恨言论检测的泛化性和鲁棒性，验证了因果表征分解和反事实推理在此任务中的潜力，为未来跨平台、跨风格的仇恨言论检测提供了新途径。

Abstract: The proliferation of online hate speech poses a significant threat to the
harmony of the web. While explicit hate is easily recognized through overt
slurs, implicit hate speech is often conveyed through sarcasm, irony,
stereotypes, or coded language -- making it harder to detect. Existing hate
speech detection models, which predominantly rely on surface-level linguistic
cues, fail to generalize effectively across diverse stylistic variations.
Moreover, hate speech spread on different platforms often targets distinct
groups and adopts unique styles, potentially inducing spurious correlations
between them and labels, further challenging current detection approaches.
Motivated by these observations, we hypothesize that the generation of hate
speech can be modeled as a causal graph involving key factors: contextual
environment, creator motivation, target, and style. Guided by this graph, we
propose CADET, a causal representation learning framework that disentangles
hate speech into interpretable latent factors and then controls confounders,
thereby isolating genuine hate intent from superficial linguistic cues.
Furthermore, CADET allows counterfactual reasoning by intervening on style
within the latent space, naturally guiding the model to robustly identify hate
speech in varying forms. CADET demonstrates superior performance in
comprehensive experiments, highlighting the potential of causal priors in
advancing generalizable hate speech detection.

</details>


### [130] [MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation](https://arxiv.org/abs/2510.07713)
*Shuo Yu,Mingyue Cheng,Daoyu Wang,Qi Liu,Zirui Liu,Ze Guo,Xiaoyu Tao*

Main category: cs.CL

TL;DR: 该论文提出了一种新的用户个性化生成框架MemWeaver，通过将用户完整的文本行为历史编织成分层记忆，从而实现更深层次的个性化。


<details>
  <summary>Details</summary>
Motivation: 随着用户与互联网交互方式转变为更多基于文本的显式反馈（如对话内容），原有方法不能充分挖掘与建模这些丰富的行为历史，错失了个性化增强机会。

Method: 提出MemWeaver框架，将用户文本历史组织成包含行为记忆和认知记忆的分层结构，分别捕捉具体行为与抽象长期偏好，并结合时间和语义信息，用于支持大语言模型进行深层个性化生成。

Result: 在Language Model Personalization (LaMP)基准上，实验证明MemWeaver能够带来更优的个性化效果，优于传统仅基于文本列表的检索方法。

Conclusion: MemWeaver通过创新的分层记忆建模，有效利用用户历史文本行为，为大语言模型个性化生成提供了更强表达能力和灵活性。

Abstract: The primary form of user-internet engagement is shifting from leveraging
implicit feedback signals, such as browsing and clicks, to harnessing the rich
explicit feedback provided by textual interactive behaviors. This shift unlocks
a rich source of user textual history, presenting a profound opportunity for a
deeper form of personalization. However, prevailing approaches offer only a
shallow form of personalization, as they treat user history as a flat list of
texts for retrieval and fail to model the rich temporal and semantic structures
reflecting dynamic nature of user interests. In this work, we propose
\textbf{MemWeaver}, a framework that weaves the user's entire textual history
into a hierarchical memory to power deeply personalized generation. The core
innovation of our memory lies in its ability to capture both the temporal
evolution of interests and the semantic relationships between different
activities. To achieve this, MemWeaver builds two complementary memory
components that both integrate temporal and semantic information, but at
different levels of abstraction: behavioral memory, which captures specific
user actions, and cognitive memory, which represents long-term preferences.
This dual-component memory serves as a unified representation of the user,
allowing large language models (LLMs) to reason over both concrete behaviors
and abstracted traits. Experiments on the Language Model Personalization (LaMP)
benchmark validate the efficacy of MemWeaver. Our code is
available\footnote{https://github.com/fishsure/MemWeaver}.

</details>


### [131] [SUBQRAG: sub-question driven dynamic graph rag](https://arxiv.org/abs/2510.07718)
*Jiaoyang Li,Junhao Ruan,Shengwei Tang,Saihan Chen,Kaiyan Chang,Yuan Ge,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: SubQRAG通过将复杂问题分解为可验证的子问题，并动态扩展知识图谱，提升了多跳问答的推理深度和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的Graph RAG方法虽然能够通过构建知识图谱连接文档信息，但在复杂多跳推理任务中常常因推理深度不足导致证据链不完整、错误累积，无法有效解决复杂问答。

Method: SubQRAG框架首先将复杂问题分解为有序的子问题链，然后针对每个子问题检索相关的图谱三元组；当现有知识图谱不足以支持推理时，框架会实时从原始文档中抽取新的三元组补充图谱。整个推理过程中涉及的所有三元组会被收集到‘图谱记忆’中，形成可追溯的证据路径，支撑最终答案生成。

Result: 在三个多跳问答基准测试上，SubQRAG在准确率（尤其是Exact Match得分）上表现出持续且显著的提升。

Conclusion: 通过引入子问题链和动态图谱扩展，SubQRAG有效增强了推理深度与证据追溯性，显著改进了复杂多跳问答的性能。

Abstract: Graph Retrieval-Augmented Generation (Graph RAG) effectively builds a
knowledge graph (KG) to connect disparate facts across a large document corpus.
However, this broad-view approach often lacks the deep structured reasoning
needed for complex multi-hop question answering (QA), leading to incomplete
evidence and error accumulation. To address these limitations, we propose
SubQRAG, a sub-question-driven framework that enhances reasoning depth. SubQRAG
decomposes a complex question into an ordered chain of verifiable
sub-questions. For each sub-question, it retrieves relevant triples from the
graph. When the existing graph is insufficient, the system dynamically expands
it by extracting new triples from source documents in real time. All triples
used in the reasoning process are aggregated into a "graph memory," forming a
structured and traceable evidence path for final answer generation. Experiments
on three multi-hop QA benchmarks demonstrate that SubQRAG achieves consistent
and significant improvements, especially in Exact Match scores.

</details>


### [132] [Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing](https://arxiv.org/abs/2510.07736)
*Cunli Mao,Xiaofei Gao,Ran Song,Shizhu He,Shengxiang Gao,Kang Liu,Zhengtao Yu*

Main category: cs.CL

TL;DR: 本文提出了一种利用LLM多语种能力进行多语言知识图谱补全（MKGC）的新框架，通过知识级专家混合（KL-GMoE）与迭代实体重排（IER），显著提升多语言知识图谱补全性能。


<details>
  <summary>Details</summary>
Motivation: 当前多语言知识图谱补全工作未充分利用大语言模型（LLMs）的多语言能力，并且忽视了跨语言知识的可共享性，导致补全效率和准确性不足。

Method: 提出KL-GMoE模块以有效建模实现知识共享，并通过IER进一步提升知识利用率；构建包含5种语言的新数据集，并与现有SOTA方法进行系统对比实验。

Result: 在Hits@1、Hits@3、Hits@10三项指标上分别比现有SOTA方法提升5.47%、3.27%、1.01%；分析了该方法在未见语言与不平衡语言场景下的知识共享特性。

Conclusion: 该框架能更有效地利用并共享多语种知识，显著提升多语言知识图谱补全任务的表现，同时为进一步研究跨语言知识共享提供了新的思路和工具。

Abstract: Large language models (LLMs) based Multilingual Knowledge Graph Completion
(MKGC) aim to predict missing facts by leveraging LLMs' multilingual
understanding capabilities, improving the completeness of multilingual
knowledge graphs (KGs). However, existing MKGC research underutilizes the
multilingual capabilities of LLMs and ignores the shareability of cross-lingual
knowledge. In this paper, we propose a novel MKGC framework that leverages
multilingual shared knowledge to significantly enhance performance through two
components: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative
Entity Reranking (IER). KL-GMoE efficiently models shared knowledge, while IER
significantly enhances its utilization. To evaluate our framework, we
constructed a mKG dataset containing 5 languages and conducted comprehensive
comparative experiments with existing state-of-the-art (SOTA) MKGC method. The
experimental results demonstrate that our framework achieves improvements of
5.47%, 3.27%, and 1.01% in the Hits@1, Hits@3, and Hits@10 metrics,
respectively, compared with SOTA MKGC method. Further experimental analysis
revealed the properties of knowledge sharing in settings of unseen and
unbalanced languages. We have released the dataset and code for our work on
https://github.com/gaoxiaofei07/KL-GMoE.

</details>


### [133] [ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs](https://arxiv.org/abs/2510.07737)
*Fu Chen,Peng Wang,Xiyin Li,Wen Li,Shichi Lei,Dongdong Xiang*

Main category: cs.CL

TL;DR: 提出了ToolExpander框架，通过两大创新点（动态多轮困难采样和自示例思考）提升资源受限下LLMs的工具使用能力，显著增强了训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 在GRPO训练大语言模型时，尤其是小规模模型，常因错误输出和中途训练崩溃导致性能和稳定性下降。因此亟需新方法提升弱模型工具推理能力和训练稳定性。

Method: 提出ToolExpander框架：（1）动态多轮困难采样：在训练时用高质量的few-shot案例动态替换10轮都不能正确输出的难例，并结合指数型学习率衰减平滑训练；（2）自示例思考：去掉KL项，调整截断系数，并引入极小的额外奖励，引导模型自己生成、分析few-shot示例。

Result: 实验结果表明，ToolExpander显著提升了弱小模型在工具任务中的表现，并改善了训练的稳定性和最终整体性能。

Conclusion: ToolExpander为资源受限的大语言模型在工具使用型强化学习任务上提供了有效的新方法，改善了性能和稳定性，特别适合小规模弱模型。

Abstract: Training Large Language Models (LLMs) with Group Relative Policy Optimization
(GRPO) encounters a significant challenge: models often fail to produce
accurate responses, particularly in small-scale architectures. This limitation
not only diminishes performance improvements and undermines the potential of
GRPO but also frequently leads to mid-training collapse, adversely affecting
stability and final efficacy. To address these issues, we propose ToolExpander,
a novel framework that advances tool-oriented reinforcement learning for
resource-constrained LLMs through two key innovations:(1) Dynamic Multi-Round
Hard Sampling, which dynamically substitutes challenging samples(those without
correct outputs over 10 rollouts) with high-quality few-shot demonstrations
during training, coupled with an exponential learning rate decay strategy to
mitigate oscillations;(2) Self-Exemplifying Thinking, an enhanced GRPO
framework that eliminates KL divergence and incorporates adjusted clipping
coefficients, encouraging models to autonomously generate and analyze few-shot
examples via a minimal additional reward (0.01).Experimental results
demonstrate that ToolExpander significantly enhances tool-using capabilities in
LLMs, especially in weaker small-scale models, improving both training
stability and overall performance.

</details>


### [134] [OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment](https://arxiv.org/abs/2510.07743)
*Tianci Liu,Ran Xu,Tony Yu,Ilgee Hong,Carl Yang,Tuo Zhao,Haoyu Wang*

Main category: cs.CL

TL;DR: 论文提出使用结构化评判标准（rubric）对人类偏好进行建模，提出了OpenRubrics数据集和新的rubric生成方法，使奖励建模更加多维、可扩展。使用该方法的奖励模型在多个基准测试上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF用于奖励建模时大多依赖单一分数或成对比较，无法细致表达人类对回复质量的多维偏好。最近提出用rubric作为奖励信号，但如何规模化和提升可靠性仍是难题。

Method: 1) 提出OpenRubrics大规模(prompt, rubric)数据集；2) 引入对比式rubric生成（CRG），通过对比优选和拒绝的回应，生成包括硬性规则和隐式原则的rubric；3) 通过拒绝采样去除噪声rubric，确保标签一致性，提高数据可靠性。

Result: 在多个奖励建模基准上，提出的Rubric-RM模型相较于同规模的强基线模型，性能提升6.8%。该收益在下游指令跟随和生物医学任务中亦可迁移。

Conclusion: 利用rubric可为大模型提供自动且可扩展的对齐信号，有效缩小了昂贵人工评测和自动奖励建模间的差距，推动了以原则导向的大模型对齐范式。

Abstract: Reward modeling lies at the core of reinforcement learning from human
feedback (RLHF), yet most existing reward models rely on scalar or pairwise
judgments that fail to capture the multifaceted nature of human preferences.
Recent studies have explored rubrics-as-rewards (RaR) that uses structured
natural language criteria that capture multiple dimensions of response quality.
However, producing rubrics that are both reliable and scalable remains a key
challenge. In this work, we introduce OpenRubrics, a diverse, large-scale
collection of (prompt, rubric) pairs for training rubric-generation and
rubric-based reward models. To elicit discriminative and comprehensive
evaluation signals, we introduce Contrastive Rubric Generation (CRG), which
derives both hard rules (explicit constraints) and principles (implicit
qualities) by contrasting preferred and rejected responses. We further improve
reliability by enforcing preference-label consistency via rejection sampling to
remove noisy rubrics. Across multiple reward-modeling benchmarks, our
rubric-based reward model, Rubric-RM, surpasses strong size-matched baselines
by 6.8%. These gains transfer to policy models on instruction-following and
biomedical benchmarks. Our results show that rubrics provide scalable alignment
signals that narrow the gap between costly human evaluation and automated
reward modeling, enabling a new principle-driven paradigm for LLM alignment.

</details>


### [135] [Parallel Test-Time Scaling for Latent Reasoning Models](https://arxiv.org/abs/2510.07745)
*Runyang You,Yongqi Li,Meng Liu,Wenjie Wang,Liqiang Nie,Wenjie Li*

Main category: cs.CL

TL;DR: 本论文研究如何在连续向量空间进行的隐式推理模型中实现并行的测试时尺度扩展（TTS），提出了新的采样与聚合方法，并证实其有效。


<details>
  <summary>Details</summary>
Motivation: 以往的大语言模型并行推理多基于可见的链式思维，但近年来的隐式推理模型在连续空间中运行，缺乏适用于并行测试时采样和结果聚合的方法，限制了其推理效率扩展。

Method: 提出两种基于不确定性的连续空间采样策略：Monte Carlo Dropout和加性高斯噪声；设计了新的隐空间奖励模型LatentRM，用对比损失训练以辅助推理路径评分和选择。

Result: 实验与可视化分析表明，两种采样策略都能有效扩展推理效率，并表现出各自的探索特性；LatentRM聚合机制能帮助优选推理轨迹。

Conclusion: 本工作为在连续空间扩展隐式推理模型的并行测试推理能力提供了方法，为高效可扩展的隐式推理开启了新方向。

Abstract: Parallel test-time scaling (TTS) is a pivotal approach for enhancing large
language models (LLMs), typically by sampling multiple token-based
chains-of-thought in parallel and aggregating outcomes through voting or
search. Recent advances in latent reasoning, where intermediate reasoning
unfolds in continuous vector spaces, offer a more efficient alternative to
explicit Chain-of-Thought, yet whether such latent models can similarly benefit
from parallel TTS remains open, mainly due to the absence of sampling
mechanisms in continuous space, and the lack of probabilistic signals for
advanced trajectory aggregation. \ This work enables parallel TTS for latent
reasoning models by addressing the above issues. For sampling, we introduce two
uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive
Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM)
trained with step-wise contrastive objective to score and guide latent
reasoning. Extensive experiments and visualization analyses show that both
sampling strategies scale effectively with compute and exhibit distinct
exploration dynamics, while LatentRM enables effective trajectory selection.
Together, our explorations open a new direction for scalable inference in
continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.

</details>


### [136] [Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers](https://arxiv.org/abs/2510.07761)
*Nishant Balepur,Atrey Desai,Rachel Rudinger*

Main category: cs.CL

TL;DR: 本文探讨大型语言模型（LLM）在多项选择题中只用选项不看题干仍然能取得高分的现象，并通过让模型提供推理过程，分析这种能力是否源自浅层技巧。研究发现部分情况下模型确实有更深入推理能力，挑战了“只用部分输入成功就一定是不良现象”的观点。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现LLM在没有题干、只给选项的情况下，多项选择题仍有较高准确率，这被认为依赖于数据偏差或投机取巧。本文想进一步揭示，在推理型LLM中，这种部分输入成功是否真的表现为浅层的不良策略。

Method: 作者设计实验，让推理型LLM分别用全部输入（题干+选项）和只用选项，在解答多项选择题时要求输出推理过程。比较不同条件下准确率，并分析推理过程（reasoning traces）的长度、内容和可信度。还通过

Result: 结果显示，在有推理输出的情况下，模型在全部输入下准确率提升明显，而在仅用选项情况下，大约一半题目推理也能提升准确率。更重要的是，推理内容在可信性检测中通过，推理长度对结果影响不大，且模型有时能合理推断缺失的问题内容。

Conclusion: 作者认为，仅凭部分输入取得高分并不总是模型存在浅层问题的证明。结合推理过程分析，可以区分真正有问题的数据与模型实际能够合理推断的情形，传统的评判标准需更为细致。

Abstract: Large language models (LLMs) now give reasoning before answering, excelling
in tasks like multiple-choice question answering (MCQA). Yet, a concern is that
LLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed
in MCQA without using the question, i.e., choices-only. Such partial-input
success is often deemed problematic, but reasoning traces could reveal if these
strategies are truly shallow in choices-only settings. To study these
strategies, reasoning LLMs solve MCQs in full and choices-only inputs;
test-time reasoning often boosts accuracy on full and in choices-only half the
time. While possibly due to shallow shortcuts, choices-only success is barely
affected by the length of reasoning traces, and after finding traces pass
faithfulness tests, we show they use less problematic strategies like inferring
missing questions. In all, we challenge claims that partial-input success is
always a flaw, so we discuss how reasoning traces could separate problematic
data from less problematic reasoning.

</details>


### [137] [ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning](https://arxiv.org/abs/2510.07768)
*Murong Yue,Zhiwei Liu,Liangwei Yang,Jianguo Zhang,Zuxin Liu,Haolin Chen,Ziyu Yao,Silvio Savarese,Caiming Xiong,Shelby Heinecke,Huan Wang*

Main category: cs.CL

TL;DR: 本文提出了一种将大语言模型（LLMs）自动生成的零散工具重构为结构化工具库的方法，以提升调用效率和推理表现。


<details>
  <summary>Details</summary>
Motivation: 目前LLMs结合外部工具虽提升了复杂推理能力，但受限于专业领域适用工具匮乏，且自动化工具生成方法面临工具集无序膨胀导致的检索与歧义难题。

Method: 作者提出：1）先自动生成具体任务相关工具，并将其按语义聚类；2）在每个聚类中采用多智能体系统：代码智能体整合/重构出逻辑共享、功能通用的聚合工具，评审智能体保证功能完整无损。这样逐步将大量碎片化工具压缩为更少但更强大的工具。

Result: 实验表明，该方法能显著提升工具检索准确率和多类型推理任务表现，并在工具数量扩展时展现出比基线方法更优的可扩展性。

Conclusion: 通过系统化重构工具库，不仅缓解了大模型工具集扩展带来的管理与检索问题，也在实际推理任务中实现了功能无损的性能提升，有效推动了自动化工具生成方法的应用边界。

Abstract: Large Language Models (LLMs) equipped with external tools have demonstrated
enhanced performance on complex reasoning tasks. The widespread adoption of
this tool-augmented reasoning is hindered by the scarcity of domain-specific
tools. For instance, in domains such as physics question answering, suitable
and specialized tools are often missing. Recent work has explored automating
tool creation by extracting reusable functions from Chain-of-Thought (CoT)
reasoning traces; however, these approaches face a critical scalability
bottleneck. As the number of generated tools grows, storing them in an
unstructured collection leads to significant retrieval challenges, including an
expanding search space and ambiguity between function-related tools. To address
this, we propose a systematic approach to automatically refactor an
unstructured collection of tools into a structured tool library. Our system
first generates discrete, task-specific tools and clusters them into
semantically coherent topics. Within each cluster, we introduce a multi-agent
framework to consolidate scattered functionalities: a code agent refactors code
to extract shared logic and creates versatile, aggregated tools, while a
reviewing agent ensures that these aggregated tools maintain the complete
functional capabilities of the original set. This process transforms numerous
question-specific tools into a smaller set of powerful, aggregated tools
without loss of functionality. Experimental results demonstrate that our
approach significantly improves tool retrieval accuracy and overall reasoning
performance across multiple reasoning tasks. Furthermore, our method shows
enhanced scalability compared with baselines as the number of question-specific
increases.

</details>


### [138] [Curing Miracle Steps in LLM Mathematical Reasoning with Rubric Rewards](https://arxiv.org/abs/2510.07774)
*Youliang Yuan,Qiuyang Mang,Jingbang Chen,Hong Wan,Xiaoyuan Liu,Junjielong Xu,Jen-tse Huang,Wenxuan Wang,Wenxiang Jiao,Pinjia He*

Main category: cs.CL

TL;DR: 本论文指出，目前大语言模型在数学推理任务上采用的基于最终答案的奖励方法存在严重的问题：模型可能通过不合理的推理过程也能得到正确答案，从而高估了模型的真实推理能力。论文提出了评判推理全过程的奖励方式，显著改善了这一问题。


<details>
  <summary>Details</summary>
Motivation: 以往数学推理大模型主要根据最终答案对模型进行奖励，导致模型容易"投机取巧"，产生许多通过错误推理却得到正确结果的“假阳性”。这种方法无法有效提升模型的真实推理逻辑和能力。

Method: 作者提出并实现了Rubric Reward Model（RRM），该模型是针对推理全过程的奖励函数。RRM根据特定题目的评分标准（rubric），对推理过程中的每一步进行评价，判罚逻辑漏洞，并在0-1之间细粒度给出奖励分数。RRM与强化学习（RL）流程结合，用于模型训练。

Result: RRM方法在四个数学基准测试中的表现均优于仅用最终结果奖励的传统方法。在AIME2024测试中，Verified Pass@1024从26.7%提升到62.6%；“Miracle Steps”（即跳步猜答案）现象减少了71%。

Conclusion: 奖励推理过程而非仅看结果，对于提升大模型的可靠性和真实推理能力至关重要。RRM是一种高效提升推理质量和真实能力的奖励方案。

Abstract: Large language models for mathematical reasoning are typically trained with
outcome-based rewards, which credit only the final answer. In our experiments,
we observe that this paradigm is highly susceptible to reward hacking, leading
to a substantial overestimation of a model's reasoning ability. This is
evidenced by a high incidence of false positives - solutions that reach the
correct final answer through an unsound reasoning process. Through a systematic
analysis with human verification, we establish a taxonomy of these failure
modes, identifying patterns like Miracle Steps - abrupt jumps to a correct
output without a valid preceding derivation. Probing experiments suggest a
strong association between these Miracle Steps and memorization, where the
model appears to recall the answer directly rather than deriving it. To
mitigate this systemic issue, we introduce the Rubric Reward Model (RRM), a
process-oriented reward function that evaluates the entire reasoning trajectory
against problem-specific rubrics. The generative RRM provides fine-grained,
calibrated rewards (0-1) that explicitly penalize logical flaws and encourage
rigorous deduction. When integrated into a reinforcement learning pipeline,
RRM-based training consistently outperforms outcome-only supervision across
four math benchmarks. Notably, it boosts Verified Pass@1024 on AIME2024 from
26.7% to 62.6% and reduces the incidence of Miracle Steps by 71%. Our work
demonstrates that rewarding the solution process is crucial for building models
that are not only more accurate but also more reliable.

</details>


### [139] [The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs](https://arxiv.org/abs/2510.07775)
*Omar Mahmoud,Ali Khalil,Buddhika Laknath Semage,Thommen George Karimpanal,Santu Rana*

Main category: cs.CL

TL;DR: 提升大语言模型的真实性（减少幻觉）可能会削弱其拒答（安全性）能力。本文提出用稀疏自编码器和子空间正交化方法，有效兼顾真实性提升和安全性对齐，缓解了两者的权衡困境。


<details>
  <summary>Details</summary>
Motivation: 虽然大量研究专注于提升大语言模型的真实性和减少幻觉，但很少关注到真实性提升后会导致模型表现出更弱的拒答能力，进而影响安全性。分析这一副作用有助于更全面地提升模型能力和部署安全。

Method: 作者发现模型中编码幻觉和拒答信息的部分重叠，常见对齐方法会不小心抑制事实知识，从而制约拒答性能。为此，提出基于稀疏自编码器的方法将拒答相关特征与幻觉特征解耦，并用子空间正交化保持微调过程中拒答行为的独立性，防止安全性下降。

Result: 在常识推理任务和有害内容基准测试（AdvBench和StrongReject）上验证，所提方法能有效降低幻觉，同时保持甚至提升模型的拒答（安全）表现，实现真实性和安全性的平衡。

Conclusion: 提升真实性和提升安全性间存在关键矛盾，若不加处理可能顾此失彼。作者的方法有效缓解了这一权衡，为大模型后续更安全、更真实的应用提供了新的技术路径。

Abstract: Hallucination in large language models (LLMs) has been widely studied in
recent years, with progress in both detection and mitigation aimed at improving
truthfulness. Yet, a critical side effect remains largely overlooked: enhancing
truthfulness can negatively impact safety alignment. In this paper, we
investigate this trade-off and show that increasing factual accuracy often
comes at the cost of weakened refusal behavior. Our analysis reveals that this
arises from overlapping components in the model that simultaneously encode
hallucination and refusal information, leading alignment methods to suppress
factual knowledge unintentionally. We further examine how fine-tuning on benign
datasets, even when curated for safety, can degrade alignment for the same
reason. To address this, we propose a method that disentangles refusal-related
features from hallucination features using sparse autoencoders, and preserves
refusal behavior during fine-tuning through subspace orthogonalization. This
approach prevents hallucinations from increasing while maintaining safety
alignment.We evaluate our method on commonsense reasoning tasks and harmful
benchmarks (AdvBench and StrongReject). Results demonstrate that our approach
preserves refusal behavior and task utility, mitigating the trade-off between
truthfulness and safety.

</details>


### [140] [Instance Relation Learning Network with Label Knowledge Propagation for Few-shot Multi-label Intent Detection](https://arxiv.org/abs/2510.07776)
*Shiman Zhao,Shangyuan Li,Wei Chen,Tengjiao Wang,Jiahui Yao,Jiabin Zheng,Kam Fai Wong*

Main category: cs.CL

TL;DR: 该论文提出了一种端到端的多标签联合学习方法，解决了小样本多标签意图识别任务中的错误传播问题，并取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有小样本多标签意图检测方法大多采用两阶段流水线，先学习表征再采用阈值分类，容易忽略实例间关系，导致误差累积。该问题在样本稀缺场景下尤为严重。

Method: 作者提出建立实例关系学习网络，通过标签知识传播机制，将有标签和无标签样本间的标签知识进行传播。方法端到端优化，并设计了双重关系增强损失函数，增强支持集与查询集间的关系建模能力。

Result: 在1-shot设置下，对比强基线，所提方法在AUC上提升了9.54%，Macro-F1提升11.19%。

Conclusion: 所提多标签联合学习方法有效缓解了错误传播问题，能更好地建模实例间关系，在小样本多标签意图检测任务中体现出明显优势。

Abstract: Few-shot Multi-label Intent Detection (MID) is crucial for dialogue systems,
aiming to detect multiple intents of utterances in low-resource dialogue
domains. Previous studies focus on a two-stage pipeline. They first learn
representations of utterances with multiple labels and then use a
threshold-based strategy to identify multi-label results. However, these
methods rely on representation classification and ignore instance relations,
leading to error propagation. To solve the above issues, we propose a
multi-label joint learning method for few-shot MID in an end-to-end manner,
which constructs an instance relation learning network with label knowledge
propagation to eliminate error propagation. Concretely, we learn the
interaction relations between instances with class information to propagate
label knowledge between a few labeled (support set) and unlabeled (query set)
instances. With label knowledge propagation, the relation strength between
instances directly indicates whether two utterances belong to the same intent
for multi-label prediction. Besides, a dual relation-enhanced loss is developed
to optimize support- and query-level relation strength to improve performance.
Experiments show that we outperform strong baselines by an average of 9.54% AUC
and 11.19% Macro-F1 in 1-shot scenarios.

</details>


### [141] [Drift No More? Context Equilibria in Multi-Turn LLM Interactions](https://arxiv.org/abs/2510.07777)
*Vardhan Dongre,Ryan A. Rossi,Viet Dac Lai,David Seunghyun Yoon,Dilek Hakkani-Tür,Trung Bui*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在多轮对话中的上下文漂移问题，提出了一个动力学框架来解释和度量该现象，并通过实验验证了简单干预可以有效控制漂移。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在单轮任务表现优秀，但在实际应用的多轮交互中容易产生上下文漂移（模型输出逐渐偏离目标一致性），且该现象难以通过传统静态指标捕捉，因此需要新的理论和度量方法来研究和缓解漂移问题。

Method: 作者通过将多轮漂移形式化为测试模型与目标一致参考模型在每轮的KL散度，并提出一个递归动力学模型，将漂移看作受限随机过程。实验设计包括合成的长周期任务和用户-代理的仿真（如τ-Bench），并比较几种开源LLM在不同场景下的表现，进一步测试提醒性干预措施对漂移的影响。

Result: 实验证明，大多数情况下漂移保持在受噪声限制的稳定平衡，而不会无限恶化。简单的提醒性干预措施可以显著减少模型输出与目标的一致性偏差，效果与理论预测一致。

Conclusion: 多轮上下文漂移可以被视为一种可控的平衡现象，而非不可避免的退化。本文为理解和缓解长对话中的漂移问题提供了理论与实证基础，对未来实际应用中的LLM稳定性提升具有指导意义。

Abstract: Large Language Models (LLMs) excel at single-turn tasks such as instruction
following and summarization, yet real-world deployments require sustained
multi-turn interactions where user goals and conversational context persist and
evolve. A recurring challenge in this setting is context drift: the gradual
divergence of a model's outputs from goal-consistent behavior across turns.
Unlike single-turn errors, drift unfolds temporally and is poorly captured by
static evaluation metrics. In this work, we present a study of context drift in
multi-turn interactions and propose a simple dynamical framework to interpret
its behavior. We formalize drift as the turn-wise KL divergence between the
token-level predictive distributions of the test model and a goal-consistent
reference model, and propose a recurrence model that interprets its evolution
as a bounded stochastic process with restoring forces and controllable
interventions. We instantiate this framework in both synthetic long-horizon
rewriting tasks and realistic user-agent simulations such as in $\tau$-Bench,
measuring drift for several open-weight LLMs that are used as user simulators.
Our experiments consistently reveal stable, noise-limited equilibria rather
than runaway degradation, and demonstrate that simple reminder interventions
reliably reduce divergence in line with theoretical predictions. Together,
these results suggest that multi-turn drift can be understood as a controllable
equilibrium phenomenon rather than as inevitable decay, providing a foundation
for studying and mitigating context drift in extended interactions.

</details>


### [142] [RCPU: Rotation-Constrained Error Compensation for Structured Pruning of a Large Language Model](https://arxiv.org/abs/2510.07782)
*Shuichiro Haruta,Kazunori Matsumoto,Zhi Li,Yanan Wang,Mori Kurokawa*

Main category: cs.CL

TL;DR: 本文提出了一种旋转约束补偿方法，用于解决结构化剪枝对大语言模型（LLM）造成的准确性损失问题。该方法有效修正剪枝带来的误差，实验上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过大规模数据训练，获得了丰富的表征能力，但剪枝过程通常只用很少的校准数据，因此极易引入输出误差。常规最小二乘拟合可能会对有限校准集过拟合，破坏模型权重结构。为此，作者提出了新的补偿方法以兼顾修正误差和权重结构的合理性。

Method: 作者提出了一种旋转约束的参数更新方法，在补偿剪枝后模型参数时，保证输出表征的几何属性（例如范数和内积）得以保留。还提出了一种方差感知的重要性得分机制，优先保留对输出主方向有显著贡献的输入维度，结合旋转约束补偿，提升误差修正效果并保留关键信息。

Result: 实验在LLaMA-7B模型上、WikiText-2和多个语言理解任务基准上进行。结果显示，所提出方法在困惑度和多项任务准确率上均优于现有主流剪枝补偿方法。

Conclusion: 所提旋转约束补偿方法能有效修正剪枝带来的输出误差，同时保留了表示空间几何结构。该方法比现有基线算法表现更优，有望在实际大模型剪枝场景中广泛应用。

Abstract: In this paper, we propose a rotation-constrained compensation method to
address the errors introduced by structured pruning of large language models
(LLMs). LLMs are trained on massive datasets and accumulate rich semantic
knowledge in their representation space. In contrast, pruning is typically
carried out with only a small amount of calibration data, which makes output
mismatches unavoidable. Although direct least-squares fitting can reduce such
errors, it tends to overfit to the limited calibration set, destructively
modifying pretrained weights. To overcome this difficulty, we update the pruned
parameters under a rotation constraint. This constrained update preserves the
geometry of output representations (i.e., norms and inner products) and
simultaneously re-aligns the pruned subspace with the original outputs.
Furthermore, in rotation-constrained compensation, removing components that
strongly contribute to the principal directions of the output makes error
recovery difficult. Since input dimensions with large variance strongly affect
these principal directions, we design a variance-aware importance score that
ensures such dimensions are preferentially kept in the pruned model. By
combining this scoring rule with rotation-constrained updates, the proposed
method effectively compensates errors while retaining the components likely to
be more important in a geometry-preserving manner. In the experiments, we apply
the proposed method to LLaMA-7B and evaluate it on WikiText-2 and multiple
language understanding benchmarks. The results demonstrate consistently better
perplexity and task accuracy compared with existing baselines.

</details>


### [143] [LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology](https://arxiv.org/abs/2510.07793)
*Sajib Acharjee Dip,Adrika Zafor,Bikash Kumar Paul,Uddip Acharjee Shuvo,Muhit Islam Emon,Xuan Wang,Liqing Zhang*

Main category: cs.CL

TL;DR: LLM4Cell系统性梳理并评估了58种适用于单细胞生物学的基础和agentic大模型，首次为这一领域统一了模型分类、评估标准与未来挑战。


<details>
  <summary>Details</summary>
Motivation: 单细胞生物学正借助大语言模型（LLM）和智能体框架获得新发展，但现有方法在数据类型、模型架构和评价标准上仍高度碎片化，缺乏一个系统性的综述与比较。

Method: 作者调研了58个面向单细胞研究（涵盖RNA、ATAC、多组学及空间组学）的基础与agentic模型，将其归纳为基础、文本桥接、空间、多模态、表观基因组和agentic五大类，并映射至注释、轨迹建模、药物响应预测等八大关键分析任务；基于40余个公开数据集，从生物学原理、多组学对齐、公平性、隐私、可解释性等十个维度进行系统评价。

Result: 系统地对不同模型在各项生物学分析任务中的表现、适配性、评测基准、数据多样性及道德和可扩展性等方面进行了深入分析，明确各模型所面临的挑战和局限。

Conclusion: 本工作首次为语言模型驱动的单细胞智能研究提供了统一框架、模型和数据关联视角，揭示了解释性、标准化与可信赖模型开发等领域的关键开放问题。

Abstract: Large language models (LLMs) and emerging agentic frameworks are beginning to
transform single-cell biology by enabling natural-language reasoning,
generative annotation, and multimodal data integration. However, progress
remains fragmented across data modalities, architectures, and evaluation
standards. LLM4Cell presents the first unified survey of 58 foundation and
agentic models developed for single-cell research, spanning RNA, ATAC,
multi-omic, and spatial modalities. We categorize these methods into five
families-foundation, text-bridge, spatial, multimodal, epigenomic, and
agentic-and map them to eight key analytical tasks including annotation,
trajectory and perturbation modeling, and drug-response prediction. Drawing on
over 40 public datasets, we analyze benchmark suitability, data diversity, and
ethical or scalability constraints, and evaluate models across 10 domain
dimensions covering biological grounding, multi-omics alignment, fairness,
privacy, and explainability. By linking datasets, models, and evaluation
domains, LLM4Cell provides the first integrated view of language-driven
single-cell intelligence and outlines open challenges in interpretability,
standardization, and trustworthy model development.

</details>


### [144] [HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation](https://arxiv.org/abs/2510.07794)
*Peilin Wu,Mian Zhang,Kun Wan,Wentian Zhao,Kaiyu He,Xinya Du,Zhiyu Chen*

Main category: cs.CL

TL;DR: 提出HiPRAG方法，通过引入过程奖励，细粒度优化RAG代理检索决策，实现更高效准确的问题解答。


<details>
  <summary>Details</summary>
Motivation: 现有Agentic RAG存在过度检索和不足检索的问题，影响效率和输出可靠性。当前RL训练多依赖结果奖励，难以对过程作精细控制。

Method: 提出Hierarchical Process Rewards for Efficient agentic RAG（HiPRAG）方法，将细粒度、基于知识的过程奖励融入RL训练。方法按步骤解析推理轨迹，对每一步检索决策的必要性进行即时评估，并用层次化奖励函数激励最优行为。

Result: 在Qwen2.5和Llama-3.2等主流大模型上，HiPRAG在7个QA基准数据集取得了平均65.4%~67.2%的准确率，显著降低了过度检索（至2.3%）和不足检索的发生率。

Conclusion: HiPRAG有效提升了RAG代理的搜索效率和准确性，过程奖励实现了对推理过程的精细优化，具有良好泛化性，对提升RL驱动智能体的推理能力具有重要意义。

Abstract: Agentic RAG is a powerful technique for incorporating external information
that LLMs lack, enabling better problem solving and question answering.
However, suboptimal search behaviors exist widely, such as over-search
(retrieving information already known) and under-search (failing to search when
necessary), which leads to unnecessary overhead and unreliable outputs. Current
training methods, which typically rely on outcome-based rewards in a RL
framework, lack the fine-grained control needed to address these
inefficiencies. To overcome this, we introduce Hierarchical Process Rewards for
Efficient agentic RAG (HiPRAG), a training methodology that incorporates a
fine-grained, knowledge-grounded process reward into the RL training. Our
approach evaluates the necessity of each search decision on-the-fly by
decomposing the agent's reasoning trajectory into discrete, parsable steps. We
then apply a hierarchical reward function that provides an additional bonus
based on the proportion of optimal search and non-search steps, on top of
commonly used outcome and format rewards. Experiments on the Qwen2.5 and
Llama-3.2 models across seven diverse QA benchmarks show that our method
achieves average accuracies of 65.4% (3B) and 67.2% (7B). This is accomplished
while improving search efficiency, reducing the over-search rate to just 2.3%
and concurrently lowering the under-search rate. These results demonstrate the
efficacy of optimizing the reasoning process itself, not just the final
outcome. Further experiments and analysis demonstrate that HiPRAG shows good
generalizability across a wide range of RL algorithms, model families, sizes,
and types. This work demonstrates the importance and potential of fine-grained
control through RL, for improving the efficiency and optimality of reasoning
for search agents.

</details>


### [145] [Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models](https://arxiv.org/abs/2510.07799)
*Eric Hanchen Jiang,Guancheng Wan,Sophia Yin,Mengting Li,Yuchen Wu,Xiao Liang,Xinfeng Li,Yizhou Sun,Wei Wang,Kai-Wei Chang,Ying Nian Wu*

Main category: cs.CL

TL;DR: 本文提出了一种新的生成式框架GTD（Guided Topology Diffusion），能够根据任务需求自适应地为大语言模型驱动的多智能体系统设计通信拓扑，实现更高效的智能体协作。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统的性能高度依赖其通信拓扑，但现有的大多采用静态或手工设计的拓扑，不能灵活适应不同任务的需求，导致资源浪费或性能瓶颈。需要一种能够动态平衡性能、成本和鲁棒性的拓扑设计方案。

Method: GTD框架借鉴条件离散图扩散模型，将拓扑合成过程视为多步、由代理模型引导预测多目标奖励的迭代构建，每一步都根据轻量代理模型预测的准确性、效用和成本等多目标指标优化当前拓扑，与单步生成方法不同，能够更有效地平衡复杂的设计权衡。

Result: 在多个基准测试中，GTD生成的通信拓扑能高度自适应任务，且稀疏且高效，在LLM多智能体协作任务中表现显著优于现有方法。

Conclusion: GTD为LLM驱动的多智能体系统提供了一种强大的拓扑设计新工具，能动态适应任务需求，提升协同效率和资源利用率，对实际应用具有重要意义。

Abstract: The efficiency of multi-agent systems driven by large language models (LLMs)
largely hinges on their communication topology. However, designing an optimal
topology is a non-trivial challenge, as it requires balancing competing
objectives such as task performance, communication cost, and robustness.
Existing frameworks often rely on static or hand-crafted topologies, which
inherently fail to adapt to diverse task requirements, leading to either
excessive token consumption for simple problems or performance bottlenecks for
complex ones. To address this challenge, we introduce a novel generative
framework called \textit{Guided Topology Diffusion (GTD)}. Inspired by
conditional discrete graph diffusion models, GTD formulates topology synthesis
as an iterative construction process. At each step, the generation is steered
by a lightweight proxy model that predicts multi-objective rewards (e.g.,
accuracy, utility, cost), enabling real-time, gradient-free optimization
towards task-adaptive topologies. This iterative, guided synthesis process
distinguishes GTD from single-step generative frameworks, enabling it to better
navigate complex design trade-offs. We validated GTD across multiple
benchmarks, and experiments show that this framework can generate highly
task-adaptive, sparse, and efficient communication topologies, significantly
outperforming existing methods in LLM agent collaboration.

</details>


### [146] [Multilingual Generative Retrieval via Cross-lingual Semantic Compression](https://arxiv.org/abs/2510.07812)
*Yuxin Huang,Simeng Wu,Ran Song,Yan Xiang,Yantuan Xian,Shengxiang Gao,Zhengtao Yu*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的多语种生成式检索方法MGR-CSC，通过语义压缩和一致化多语种标识符，显著提升了跨语种检索效果和效率。


<details>
  <summary>Details</summary>
Motivation: 当前生成式信息检索在单语环境表现优异，但在多语种检索中面临标识符不对齐和标识符膨胀的问题，限制了跨语种检索的性能和效率。

Method: 提出MGR-CSC框架，将语义等价的多语种关键词映射为共享的标识符（Atom）以压缩表示空间，并设计了动态多步受约束解码策略，优化了检索时的对齐与解码效率。

Result: MGR-CSC在mMarco100k和mNQ320k数据集上分别提升了检索准确率6.83%和4.77%，并将文档标识符长度分别压缩了74.51%和78.2%。

Conclusion: MGR-CSC有效解决了多语种生成式检索中的标识符不对齐与冗余问题，提升了检索准确率且显著减少了计算开销，对多语种信息检索具有重要意义。

Abstract: Generative Information Retrieval is an emerging retrieval paradigm that
exhibits remarkable performance in monolingual scenarios.However, applying
these methods to multilingual retrieval still encounters two primary
challenges, cross-lingual identifier misalignment and identifier inflation. To
address these limitations, we propose Multilingual Generative Retrieval via
Cross-lingual Semantic Compression (MGR-CSC), a novel framework that unifies
semantically equivalent multilingual keywords into shared atoms to align
semantics and compresses the identifier space, and we propose a dynamic
multi-step constrained decoding strategy during retrieval. MGR-CSC improves
cross-lingual alignment by assigning consistent identifiers and enhances
decoding efficiency by reducing redundancy. Experiments demonstrate that
MGR-CSC achieves outstanding retrieval accuracy, improving by 6.83% on
mMarco100k and 4.77% on mNQ320k, while reducing document identifiers length by
74.51% and 78.2%, respectively.

</details>


### [147] [AdaSwitch: Adaptive Switching Generation for Knowledge Distillation](https://arxiv.org/abs/2510.07842)
*Jingyu Peng,Maolin Wang,Hengyi Cai,Yuchen Li,Kai Zhang,Shuaiqiang Wang,Dawei Yin,Xiangyu Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种新的知识蒸馏方法AdaSwitch，通过动态结合on-policy和off-policy生成方式，提升了小型语言模型（SLM）的表现。该方法在三个数据集及两组师生大模型上均取得了一致的准确率提升，且计算开销可接受。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型由于延迟和算力限制，在性能提升方面面临挑战。知识蒸馏虽然可以迁移大模型能力，但现有方法在监督信号质量与模型一致性之间存在权衡，无法同时兼顾二者。为解决这一难题，需要一种既能保持训练推理一致性，又能保证高质量教师指导的方法。

Method: 作者提出了AdaSwitch方法，在每个token级别动态切换on-policy（依靠学生预测）和off-policy（依靠教师监督）生成。首先让学生探索自身预测能力，再基于实时质量评估选择性地引入教师信号，从而结合两者优势。

Result: 在三个数据集和两组教师-学生模型上实验显示，AdaSwitch方法在准确率方面表现出一致的提升，同时附加的计算和操作开销在可接受范围内。

Conclusion: AdaSwitch为小型语言模型知识蒸馏提供了一种实用有效的新思路，可以在不显著增加计算负担的情况下，显著提升模型性能，兼顾训练推理一致性和监督质量。

Abstract: Small language models (SLMs) are crucial for applications with strict latency
and computational constraints, yet achieving high performance remains
challenging. Knowledge distillation (KD) can transfer capabilities from large
teacher models, but existing methods involve trade-offs: off-policy
distillation provides high-quality supervision but introduces a
training-inference mismatch, while on-policy approaches maintain consistency
but rely on low-quality student outputs. To address these issues, we propose
AdaSwitch, a novel approach that dynamically combines on-policy and off-policy
generation at the token level. AdaSwitch allows the student to first explore
its own predictions and then selectively integrate teacher guidance based on
real-time quality assessment. This approach simultaneously preserves
consistency and maintains supervision quality. Experiments on three datasets
with two teacher-student LLM pairs demonstrate that AdaSwitch consistently
improves accuracy, offering a practical and effective method for distilling
SLMs with acceptable additional overhead.

</details>


### [148] [Ready to Translate, Not to Represent? Bias and Performance Gaps in Multilingual LLMs Across Language Families and Domains](https://arxiv.org/abs/2510.07877)
*Md. Faiyaz Abdullah Sayeedi,Md. Mahbub Alam,Subhey Sadi Rahman,Md. Adnanul Islam,Jannatul Ferdous Deepti,Tasnim Mohiuddin,Md Mofijul Islam,Swakkhar Shatabda*

Main category: cs.CL

TL;DR: 本文提出了Translation Tangles框架和数据集，用于全面评估开源大语言模型（LLMs）在机器翻译中的质量与公平性，尤其关注低资源语言的偏见问题。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型极大提升了机器翻译的能力，但在不同语族、领域和低资源语言的表现依然不均衡。此外，这些模型容易加剧训练数据的偏见，影响翻译公平性，尤其伤害弱势语言群体。

Method: 作者提出了Translation Tangles统一评测框架和数据集，支持24组双向语言对、多个领域，借助多种质量和公平性评估指标。提出了混合型偏见检测流程，结合规则启发、语义相似度过滤和LLM辅助验证，并构建了基于人工标注的高质量偏见数据集。

Result: 建立了首个包含1,439组人工标注的偏见翻译样本的数据集。实验证明该框架和流程能有效暴露开源LLM在多领域、多语言、尤其低资源条件下的翻译缺陷与偏见问题。

Conclusion: Translation Tangles为开源LLM翻译质量和公平性评测提供了标准工具，有助于推动更加公正、可靠的自动翻译系统发展。代码与数据已公开发布。

Abstract: The rise of Large Language Models (LLMs) has redefined Machine Translation
(MT), enabling context-aware and fluent translations across hundreds of
languages and textual domains. Despite their remarkable capabilities, LLMs
often exhibit uneven performance across language families and specialized
domains. Moreover, recent evidence reveals that these models can encode and
amplify different biases present in their training data, posing serious
concerns for fairness, especially in low-resource languages. To address these
gaps, we introduce Translation Tangles, a unified framework and dataset for
evaluating the translation quality and fairness of open-source LLMs. Our
approach benchmarks 24 bidirectional language pairs across multiple domains
using different metrics. We further propose a hybrid bias detection pipeline
that integrates rule-based heuristics, semantic similarity filtering, and
LLM-based validation. We also introduce a high-quality, bias-annotated dataset
based on human evaluations of 1,439 translation-reference pairs. The code and
dataset are accessible on GitHub:
https://github.com/faiyazabdullah/TranslationTangles

</details>


### [149] [Do LLMs Really Need 10+ Thoughts for "Find the Time 1000 Days Later"? Towards Structural Understanding of LLM Overthinking](https://arxiv.org/abs/2510.07880)
*Xinliang Frederick Zhang,Anhad Mohananey,Alexandra Chronopoulou,Pinelopi Papalampidi,Somit Gupta,Tsendsuren Munkhdalai,Lu Wang,Shyam Upadhyay*

Main category: cs.CL

TL;DR: 本研究揭示长链条推理模型（CoT）在复杂任务虽表现突出，却在简单任务上存在严重“过度思考”等效率问题。通过系统分析工具TRACE，该文深入拆解模型的思维过程，发现“过度探索”与“过度验证”是主要原因，并提出基于效用而非长度的全新过度思考定义。


<details>
  <summary>Details</summary>
Motivation: 尽管长链条推理能提升模型复杂任务的推理能力，但在处理简单问题时却导致显著的计算浪费（过度思考），其根本原因尚不清楚，现有分析手段也主要停留在表面层。为此，本文提出对LLM推理过程进行更细致的内部剖析。

Method: 提出TRACE分析工具，系统地将LLM推理过程拆分为最小可完备的子思路，通过推断子思路间的话语关系，构建细致的思维进展图，并分析主题相似问题中模型常见的推理模式。

Result: 实验证明长链条推理模型在处理简单任务时，推理速度比短推理慢5-20倍且无明显正确率提升。分析发现“Explorer”和“Late Landing”两大思维模式，过度探索和验证是过度思考的核心驱动。

Conclusion: 基于思维结构，作者提出更具实践价值的基于效用的过度思考定义，不再仅以推理长度衡量，有助于更科学地管理和优化大模型推理行为。

Abstract: Models employing long chain-of-thought (CoT) reasoning have shown superior
performance on complex reasoning tasks. Yet, this capability introduces a
critical and often overlooked inefficiency -- overthinking -- models often
engage in unnecessarily extensive reasoning even for simple queries, incurring
significant computations without accuracy improvements. While prior work has
explored solutions to mitigate overthinking, a fundamental gap remains in our
understanding of its underlying causes. Most existing analyses are limited to
superficial, profiling-based observations, failing to delve into LLMs' inner
workings. This study introduces a systematic, fine-grained analyzer of LLMs'
thought process to bridge the gap, TRACE. We first benchmark the overthinking
issue, confirming that long-thinking models are five to twenty times slower on
simple tasks with no substantial gains. We then use TRACE to first decompose
the thought process into minimally complete sub-thoughts. Next, by inferring
discourse relationships among sub-thoughts, we construct granular thought
progression graphs and subsequently identify common thinking patterns for
topically similar queries. Our analysis reveals two major patterns for
open-weight thinking models -- Explorer and Late Landing. This finding provides
evidence that over-verification and over-exploration are the primary drivers of
overthinking in LLMs. Grounded in thought structures, we propose a
utility-based definition of overthinking, which moves beyond length-based
metrics. This revised definition offers a more insightful understanding of
LLMs' thought progression, as well as practical guidelines for principled
overthinking management.

</details>


### [150] [CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for Mandarin-English Code-Switching](https://arxiv.org/abs/2510.07881)
*Heyang Liu,Yuhao Wang,Ziyang Cheng,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 该论文提出了一个新的语音转语音代码切换基准（CS3-Bench），揭示了主流多模态大模型在多语言的语言对齐与理解方面存在显著短板，并给出相应改进方法。


<details>
  <summary>Details</summary>
Motivation: 尽管现有多模态大模型在单一语言下的自然交互表现较好，但在多语言（特别是代码切换场景）中，其语言对齐与理解能力严重不足，亟需系统性评估与提升。

Method: 作者构建了CS3-Bench基准，对7个主流模型在知识密集型问答和开放式对话任务下进行评测，并提出了包括‘识别链（CoR）’和‘关键词高亮（KH）’在内的数据构建和训练方法提升模型的多语言对齐能力。

Result: 在CS3-Bench上，主流大模型在知识问答任务中性能最高下降至原本表现的66%。通过所提方法，模型知识准确率从25.14%提升至46.13%，开放式理解率从64.5%提升至86.5%，二语发音错误也大幅减少。

Conclusion: CS3-Bench有效揭示并量化了语言对齐问题，提出的数据与方法能够显著提升多模态模型在多语种场景中的理解与生成准确性，对推动语音交互系统多语言能力有重要意义。

Abstract: The advancement of multimodal large language models has accelerated the
development of speech-to-speech interaction systems. While natural monolingual
interaction has been achieved, we find existing models exhibit deficiencies in
language alignment. In our proposed Code-Switching Speech-to-Speech Benchmark
(CS3-Bench), experiments on 7 mainstream models demonstrate a relative
performance drop of up to 66% in knowledge-intensive question answering and
varying degrees of misunderstanding in open-ended conversations. Starting from
a model with severe performance deterioration, we propose both data
constructions and training approaches to improve the language alignment
capabilities, specifically employing Chain of Recognition (CoR) to enhance
understanding and Keyword Highlighting (KH) to guide generation. Our approach
improves the knowledge accuracy from 25.14% to 46.13%, with open-ended
understanding rate from 64.5% to 86.5%, and significantly reduces pronunciation
errors in the secondary language. CS3-Bench is available at
https://huggingface.co/datasets/VocalNet/CS3-Bench.

</details>


### [151] [Contrastive Weak-to-strong Generalization](https://arxiv.org/abs/2510.07884)
*Houcheng Jiang,Junfeng Fang,Jiaxin Wu,Tianyu Zhang,Chen Gao,Yong Li,Xiang Wang,Xiangnan He,Yang Deng*

Main category: cs.CL

TL;DR: 论文提出了一种新的方法ConG（Contrastive Weak-to-Strong Generalization），通过对抗性解码（Contrastive Decoding）显著提升大语言模型中弱到强泛化的效果，减少了噪声和偏差，提升了模型的稳健性。


<details>
  <summary>Details</summary>
Motivation: 传统的弱到强泛化（Weak-to-Strong Generalization）策略能够使用弱模型的对齐样本训练更强大的模型，但易受弱模型输出中噪声和偏见的影响，从而影响泛化能力和实际适用性。论文旨在解决弱模型样本质量不高导致能力迁移不理想的问题。

Method: 作者利用隐式奖励（通过log-likelihood比值近似显示奖励）并指出这种方法与对抗性解码（Contrastive Decoding）在结构上等价。基于这一理论联系，提出了ConG框架：在对齐前后的弱模型之间进行对抗性解码，生成更高质量的训练样本，从而实现更可靠的能力迁移和去噪。

Result: ConG在不同家族的大语言模型中均实现了一致的性能提升，证明了泛化性和有效性，在稳健性、去噪、能力迁移等方面优于传统弱到强方法。

Conclusion: ConG方法不仅提升了弱到强泛化的鲁棒性，也拓宽了大模型在自动泛化和能力迁移的应用前景，被认为是推动通用人工智能（AGI）发展的有前景路径之一。

Abstract: Weak-to-strong generalization provides a promising paradigm for scaling large
language models (LLMs) by training stronger models on samples from aligned
weaker ones, without requiring human feedback or explicit reward modeling.
However, its robustness and generalization are hindered by the noise and biases
in weak-model outputs, which limit its applicability in practice. To address
this challenge, we leverage implicit rewards, which approximate explicit
rewards through log-likelihood ratios, and reveal their structural equivalence
with Contrastive Decoding (CD), a decoding strategy shown to reduce noise in
LLM generation. Building on this connection, we propose Contrastive
Weak-to-Strong Generalization (ConG), a framework that employs contrastive
decoding between pre- and post-alignment weak models to generate higher-quality
samples. This approach enables more reliable capability transfer, denoising,
and improved robustness, substantially mitigating the limitations of
traditional weak-to-strong methods. Empirical results across different model
families confirm consistent improvements, demonstrating the generality and
effectiveness of ConG. Taken together, our findings highlight the potential of
ConG to advance weak-to-strong generalization and provide a promising pathway
toward AGI.

</details>


### [152] [Standard-to-Dialect Transfer Trends Differ across Text and Speech: A Case Study on Intent and Topic Classification in German Dialects](https://arxiv.org/abs/2510.07890)
*Verena Blaschke,Miriam Winkler,Barbara Plank*

Main category: cs.CL

TL;DR: 本文比较了标准德语向多种德语方言的信息迁移，在文本、语音和串联系统三种环境下，发现语音模型对方言数据表现最佳。


<details>
  <summary>Details</summary>
Motivation: 此前跨方言迁移研究大多聚焦文本，但方言主要以口语为主，且文本中非标准拼写易影响处理效果，因此需要探究方言情境下不同数据模态的迁移效果。

Method: 对德语及多种德语方言，构建并发布首个方言音频意图分类数据集，在意图与主题分类任务上，分别在文本、语音和“语音转文本+文本处理”系统三种设置下进行对比实验。

Result: 实验显示：1）语音模型在处理方言数据时效果最好；2）文本模型对标准德语表现最佳；3）串联系统虽不及直接文本模型表现，但采用标准化转写时，面对方言数据效果较好。

Conclusion: 解决方言迁移任务时，直接利用语音模型对方言优势明显，且高质量的标准化转写可促使串联系统更好地服务于方言应用。

Abstract: Research on cross-dialectal transfer from a standard to a non-standard
dialect variety has typically focused on text data. However, dialects are
primarily spoken, and non-standard spellings are known to cause issues in text
processing. We compare standard-to-dialect transfer in three settings: text
models, speech models, and cascaded systems where speech first gets
automatically transcribed and then further processed by a text model. In our
experiments, we focus on German and multiple German dialects in the context of
written and spoken intent and topic classification. To that end, we release the
first dialectal audio intent classification dataset. We find that the
speech-only setup provides the best results on the dialect data while the
text-only setup works best on the standard data. While the cascaded systems lag
behind the text-only models for German, they perform relatively well on the
dialectal data if the transcription system generates normalized, standard-like
output.

</details>


### [153] [Metric Calculating Benchmark: Code-Verifiable Complicate Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2510.07892)
*Hyeonseok Moon,Seongtae Hong,Jaehyung Seo,Heuiseok Lim*

Main category: cs.CL

TL;DR: 本文提出了MCBench，一个能客观评估大语言模型（LLM）是否能够按照严格分步指令执行字符串匹配任务的新基准。


<details>
  <summary>Details</summary>
Motivation: 以往LLM基准测试大多依赖主观判断或通用推理，且许多已有难题已被模型攻克，缺乏进一步区分能力的挑战。需要一个更客观、可验证、具备挑战性的基准测试。

Method: MCBench设计为通过精确定义的字符串匹配与分步执行指令检验LLM。与此前不同，该基准采用客观、确定性、可通过代码验证的方式评测模型是否严格按步骤执行，包括指令遵循、数值计算与处理中间结果的连贯性。同时提供三种评估指标和三种基准变体，用以详细测量LLM理解指令的能力。

Result: 分析结果显示，MCBench能够有效且客观地区分并评估现有最强LLM在指令理解及准确执行方面的能力。

Conclusion: MCBench是评估和推动前沿LLM精确任务执行能力的有效工具，通过客观、可验证手段，为模型分层提供了新标准。

Abstract: Recent frontier-level LLMs have saturated many previously difficult
benchmarks, leaving little room for further differentiation. This progress
highlights the need for challenging benchmarks that provide objective
verification. In this paper, we introduce MCBench, a benchmark designed to
evaluate whether LLMs can execute string-matching NLP metrics by strictly
following step-by-step instructions. Unlike prior benchmarks that depend on
subjective judgments or general reasoning, MCBench offers an objective,
deterministic and codeverifiable evaluation. This setup allows us to
systematically test whether LLMs can maintain accurate step-by-step execution,
including instruction adherence, numerical computation, and long-range
consistency in handling intermediate results. To ensure objective evaluation of
these abilities, we provide a parallel reference code that can evaluate the
accuracy of LLM output. We provide three evaluative metrics and three benchmark
variants designed to measure the detailed instruction understanding capability
of LLMs. Our analyses show that MCBench serves as an effective and objective
tool for evaluating the capabilities of cutting-edge LLMs.

</details>


### [154] [ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall](https://arxiv.org/abs/2510.07896)
*Jiayu Yang,Yuxuan Fan,Songning Lai,Shengen Wu,Jiaqi Tang,Chun Kang,Zhijiang Guo,Yutao Yue*

Main category: cs.CL

TL;DR: 本文提出了ACE框架，通过神经元级归因机制，有效提升了LLM多跳知识编辑能力，在主流模型上大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法在多跳事实回忆上表现较差，尤其在涉及隐式中间主体时效果明显下降。作者希望破解大模型在多跳链式推理时知识更新失效的根本原因，并提升其知识编辑性能。

Method: 作者通过因果分析发现，多跳推理中隐式主体以查询神经元的形式激活对应的值神经元实现知识累积。基于此原理，提出ACE（Attribution-Controlled Knowledge Editing）框架，利用神经元级归因手段识别并编辑关键的查询-值路径，从而完成高效的多跳知识编辑。

Result: ACE框架在GPT-J和Qwen3-8B模型上，分别将多跳事实回忆准确率提升9.44%和37.46%，显著优于现有方法。此外，还揭示了Qwen3模型更细粒度的神经元激活规律。

Conclusion: 该研究通过对模型内部多跳推理机制的机理解析，提出了更具原则性和效果的新型知识编辑方法，为大模型内部机制驱动的知识能力提升提供了新途径。

Abstract: Large Language Models (LLMs) require efficient knowledge editing (KE) to
update factual information, yet existing methods exhibit significant
performance decay in multi-hop factual recall. This failure is particularly
acute when edits involve intermediate implicit subjects within reasoning
chains. Through causal analysis, we reveal that this limitation stems from an
oversight of how chained knowledge is dynamically represented and utilized at
the neuron level. We discover that during multi hop reasoning, implicit
subjects function as query neurons, which sequentially activate corresponding
value neurons across transformer layers to accumulate information toward the
final answer, a dynamic prior KE work has overlooked. Guided by this insight,
we propose ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual
Recall, a framework that leverages neuron-level attribution to identify and
edit these critical query-value (Q-V) pathways. ACE provides a mechanistically
grounded solution for multi-hop KE, empirically outperforming state-of-the-art
methods by 9.44% on GPT-J and 37.46% on Qwen3-8B. Our analysis further reveals
more fine-grained activation patterns in Qwen3 and demonstrates that the
semantic interpretability of value neurons is orchestrated by query-driven
accumulation. These findings establish a new pathway for advancing KE
capabilities based on the principled understanding of internal reasoning
mechanisms.

</details>


### [155] [Towards Human-Like Grading: A Unified LLM-Enhanced Framework for Subjective Question Evaluation](https://arxiv.org/abs/2510.07912)
*Fanwei Zhua,Jiaxuan He,Xiaoxiao Chen,Zulong Chen,Quan Lu,Chenrui Mei*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型（LLM）增强的自动评分框架，能够对多类型主观题进行类似人工的评估，并在真实环境中取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有的自动评分方法多针对单一主观题类型设计，缺乏通用性，难以适应考试中多样化的题型与开放性的学生答案，亟需一种通用且能精确评价多类型主观题的方法。

Method: 提出了一个LLM增强的自动评分系统，由四个模块协同工作：基础文本匹配模块、知识点提取与对比模块、基于学生答案生成伪问题用于相关性判断模块，以及模拟人工评分发现优缺点的模块。该系统充分利用LLM的推理和生成能力，实现多角度评估学生答案。

Result: 在通用与专业领域数据集上进行了大量实验，对比传统和现有LLM评分方法，本方法在多项评分指标上表现更优。此外，该系统已在某大型电商企业的培训与认证考试中成功部署。

Conclusion: 本文提出的统一LLM增强评分框架兼具通用性与高有效性，能够为各类主观题提供类似人工的全面评价方式，推动了自动评卷系统在实际场景的应用。

Abstract: Automatic grading of subjective questions remains a significant challenge in
examination assessment due to the diversity in question formats and the
open-ended nature of student responses. Existing works primarily focus on a
specific type of subjective question and lack the generality to support
comprehensive exams that contain diverse question types. In this paper, we
propose a unified Large Language Model (LLM)-enhanced auto-grading framework
that provides human-like evaluation for all types of subjective questions
across various domains. Our framework integrates four complementary modules to
holistically evaluate student answers. In addition to a basic text matching
module that provides a foundational assessment of content similarity, we
leverage the powerful reasoning and generative capabilities of LLMs to: (1)
compare key knowledge points extracted from both student and reference answers,
(2) generate a pseudo-question from the student answer to assess its relevance
to the original question, and (3) simulate human evaluation by identifying
content-related and non-content strengths and weaknesses. Extensive experiments
on both general-purpose and domain-specific datasets show that our framework
consistently outperforms traditional and LLM-based baselines across multiple
grading metrics. Moreover, the proposed system has been successfully deployed
in real-world training and certification exams at a major e-commerce
enterprise.

</details>


### [156] [STEPER: Step-wise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models](https://arxiv.org/abs/2510.07923)
*Kyumin Lee,Minjin Jeon,Sanghwan Jang,Hwanjo Yu*

Main category: cs.CL

TL;DR: 论文提出了StepER方法，实现了多步检索增强语言模型中的逐步知识蒸馏，有效提升了复杂问题的推理能力，使小模型性能接近大模型。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法忽略了多步推理任务中不同阶段对推理能力的不同需求，导致在多步检索增强任务中迁移效果有限，需要新的机制提升小模型的推理表现。

Method: 作者提出StepER，结合逐步监督机制，适应推理过程中不同阶段的信息和推理要求；同时引入难度感知训练，按步骤难度优化学习过程。该方法可广泛应用于利用检索增强的多步推理框架。

Result: 大量实验证明，StepER在多跳问答等基准任务上显著优于以往方法，8B参数量的小模型可达70B大模型的性能水平。

Conclusion: StepER通过逐步知识蒸馏和难度感知训练，有效提升了小模型在多步检索增强推理任务中的能力，具有良好的模型适应性和推广前景。

Abstract: Answering complex real-world questions requires step-by-step retrieval and
integration of relevant information to generate well-grounded responses.
However, existing knowledge distillation methods overlook the need for
different reasoning abilities at different steps, hindering transfer in
multi-step retrieval-augmented frameworks. To address this, we propose Stepwise
Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step
Retrieval-Augmented Language Models (StepER). StepER employs step-wise
supervision to align with evolving information and reasoning demands across
stages. Additionally, it incorporates difficulty-aware training to
progressively optimize learning by prioritizing suitable steps. Our method is
adaptable to various multi-step retrieval-augmented language models, including
those that use retrieval queries for reasoning paths or decomposed questions.
Extensive experiments show that StepER outperforms prior methods on multi-hop
QA benchmarks, with an 8B model achieving performance comparable to a 70B
teacher model.

</details>


### [157] [Comprehensiveness Metrics for Automatic Evaluation of Factual Recall in Text Generation](https://arxiv.org/abs/2510.07926)
*Adam Dejl,James Barry,Alessandra Pascale,Javier Carnerero Cano*

Main category: cs.CL

TL;DR: 本文关注大语言模型（LLMs）生成文本中遗漏信息的问题，并提出了三种自动化评估LLM文本完整性的方法。实验表明直接用LLM端到端识别缺失内容的方法效果最好，但缺乏稳健性与可解释性。作者还比较了几种主流开源LLM在多源信息回答任务上的表现。


<details>
  <summary>Details</summary>
Motivation: LLM经常输出不完整或省略重要信息，尤其在敏感领域可能带来与事实错误同等严重的后果，因此需要系统性地评估和检测LLM生成文本的完整性和信息覆盖程度。

Method: 提出三种自动化评估方法：1) 基于NLI，将文本分解为原子陈述，用自然语言推理（NLI）识别遗漏；2) 基于问答，抽取问答对并在不同来源间比对答案；3) 端到端，用LLM直接判断内容是否缺失。

Result: 实验显示，端到端方法在识别缺失内容方面出乎意料地优于更复杂的分析法，但其稳健性、可解释性和结果细粒度较差。

Conclusion: 简单的端到端方法虽有效，但在实际应用中受限。研究为可靠评估LLM文本完整性提供了思路，强调了未来需在方法稳健性与可解释性间权衡。

Abstract: Despite demonstrating remarkable performance across a wide range of tasks,
large language models (LLMs) have also been found to frequently produce outputs
that are incomplete or selectively omit key information. In sensitive domains,
such omissions can result in significant harm comparable to that posed by
factual inaccuracies, including hallucinations. In this study, we address the
challenge of evaluating the comprehensiveness of LLM-generated texts, focusing
on the detection of missing information or underrepresented viewpoints. We
investigate three automated evaluation strategies: (1) an NLI-based method that
decomposes texts into atomic statements and uses natural language inference
(NLI) to identify missing links, (2) a Q&A-based approach that extracts
question-answer pairs and compares responses across sources, and (3) an
end-to-end method that directly identifies missing content using LLMs. Our
experiments demonstrate the surprising effectiveness of the simple end-to-end
approach compared to more complex methods, though at the cost of reduced
robustness, interpretability and result granularity. We further assess the
comprehensiveness of responses from several popular open-weight LLMs when
answering user queries based on multiple sources.

</details>


### [158] [Vision-Enabled LLMs in Historical Lexicography: Digitising and Enriching Estonian-German Dictionaries from the 17th and 18th Centuries](https://arxiv.org/abs/2510.07931)
*Madis Jürviste,Joonatan Jakobson*

Main category: cs.CL

TL;DR: 本文探讨大型语言模型（LLM）在处理17-18世纪爱沙尼亚历史字典的潜力，包括现代化释义、古老哥特字体文本识别与跨字典数据集构建，结果表明LLM明显能提升自动化效率和准确率，节省时间与成本。


<details>
  <summary>Details</summary>
Motivation: 17-18世纪的爱沙尼亚字典亟需现代化处理以便研究和使用，但手动更新、识别古体印刷文本等工作费时费力。作者希望探索LLM是否可以高效辅助这些复杂任务。

Method: 研究分为三方面：(1)用LLM为历史字典词条添加现代词形和词义；(2)用视觉能力增强的LLM识别印刷字典中的哥特字体（Fraktur）；(3)为多个词典数据整合成可统一处理的数据集做准备。作者通过Claude 3.7 Sonnet和零样本方法进行实验，并采用重叠切片等扫描图像处理技术。

Result: Claude 3.7 Sonnet对J. Gutslaff 1648年字典，能为81%词条精准赋予现代词义和同义词；对Helle 1732年字典，零样本识别法能将41%词条结构化且无错误地输出为JSON格式；在Hupel 1780年字典的扫描识别中，将LLM分角色用于识别和结果合并。

Conclusion: LLM在小语种历史字典的自动化处理、文本识别和数据整理方面展现出显著效能，可大幅节省劳动与经费，为古籍语言研究带来新机遇。

Abstract: This article presents research conducted at the Institute of the Estonian
Language between 2022 and 2025 on the application of large language models
(LLMs) to the study of 17th and 18th century Estonian dictionaries. The authors
address three main areas: enriching historical dictionaries with modern word
forms and meanings; using vision-enabled LLMs to perform text recognition on
sources printed in Gothic script (Fraktur); and preparing for the creation of a
unified, cross-source dataset. Initial experiments with J. Gutslaff's 1648
dictionary indicate that LLMs have significant potential for semi-automatic
enrichment of dictionary information. When provided with sufficient context,
Claude 3.7 Sonnet accurately provided meanings and modern equivalents for 81%
of headword entries. In a text recognition experiment with A. T. Helle's 1732
dictionary, a zero-shot method successfully identified and structured 41% of
headword entries into error-free JSON-formatted output. For digitising the
Estonian-German dictionary section of A. W. Hupel's 1780 grammar, overlapping
tiling of scanned image files is employed, with one LLM being used for text
recognition and a second for merging the structured output. These findings
demonstrate that even for minor languages LLMs have a significant potential for
saving time and financial resources.

</details>


### [159] [A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning](https://arxiv.org/abs/2510.07958)
*Fengji Zhang,Xinyao Niu,Chengyang Ying,Guancheng Lin,Zhongkai Hao,Zhou Fan,Chengen Huang,Jacky Keung,Bei Chen,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出了A^2Search方法，能够在无需人工标注的情况下识别和应对问答中的多解歧义问题，提升了多跳开放域问答的效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型和增强学习已在开放域问答中取得较高表现，但在面对多解歧义问题时依然有不足。当前主流问答数据集通常只假设唯一标准答案，忽略了现实中多解的情况，导致训练信号不精准。此外，针对歧义的现有处理方法往往依赖大量人工标注，难以扩展到多跳复杂问答数据集。

Method: 作者提出A^2Search，这是一个无需人工标注的端到端多解问答训练框架。该方法通过自动化流程检测歧义问题，并利用轨迹采样与证据验证收集备选答案。之后，模型结合专门设计的AnsF1奖励函数，通过增强学习优化，有效适配多解场景。

Result: A^2Search在八个开放域问答基准集上创下了最新的SOTA水平。仅用单次采样，A^2Search-7B在四个多跳问答数据集上平均AnsF1@1达到48.4%，显著优于更大的ReSearch-32B（46.2%）。拓展分析也显示A^2Search能有效处理歧义并具备跨数据集泛化能力。

Conclusion: 研究表明，主动处理多解歧义对于打造更可靠的QA系统至关重要。A^2Search为无需标注、多解答问答任务提供了有效解决途径，推动了领域进展。

Abstract: Recent advances in Large Language Models (LLMs) and Reinforcement Learning
(RL) have led to strong performance in open-domain question answering (QA).
However, existing models still struggle with questions that admit multiple
valid answers. Standard QA benchmarks, which typically assume a single gold
answer, overlook this reality and thus produce inappropriate training signals.
Existing attempts to handle ambiguity often rely on costly manual annotation,
which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue.
In this paper, we present A$^2$Search, an annotation-free, end-to-end training
framework to recognize and handle ambiguity. At its core is an automated
pipeline that detects ambiguous questions and gathers alternative answers via
trajectory sampling and evidence verification. The model is then optimized with
RL using a carefully designed $\mathrm{AnsF1}$ reward, which naturally
accommodates multiple answers. Experiments on eight open-domain QA benchmarks
demonstrate that A$^2$Search achieves new state-of-the-art performance. With
only a single rollout, A$^2$Search-7B yields an average $\mathrm{AnsF1}@1$
score of $48.4\%$ across four multi-hop benchmarks, outperforming all strong
baselines, including the substantially larger ReSearch-32B ($46.2\%$).
Extensive analyses further show that A$^2$Search resolves ambiguity and
generalizes across benchmarks, highlighting that embracing ambiguity is
essential for building more reliable QA systems. Our code, data, and model
weights can be found at https://github.com/zfj1998/A2Search

</details>


### [160] [LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?](https://arxiv.org/abs/2510.07962)
*Jingyuan Wang,Yankai Chen,Zhonghang Li,Chao Huang*

Main category: cs.CL

TL;DR: 本文提出了LightReasoner框架，让小模型通过揭示关键推理瞬间来反向指导大模型，从而提升大模型推理能力，显著提高了效率与表现。


<details>
  <summary>Details</summary>
Motivation: 传统大模型的监督微调（SFT）依赖大规模人工标注数据和大量算力资源，效率低下。作者希望探索是否可利用小模型辅助，找到更高效提升大模型推理能力的方法。

Method: 提出LightReasoner框架，分为两阶段：第一阶段通过强专家模型（大模型）和弱业余模型（小模型）推理表现的差异，采样并筛选关键推理时刻，构造专家-业余对比的学习样例；第二阶段利用这些精练样例对专家模型进行再调优。无需真实标签。

Result: 在七项数学类基准测试上，LightReasoner令大模型推理准确率最高提升28.1%，并减少90%的时间消耗、80%的采样问题数量及99%的调优token消耗。

Conclusion: 利用小模型发现和凝练关键推理瞬间，可以极大提高大模型训练效率与推理表现，为大模型推理能力提升提供了高效、可扩展的新途径。

Abstract: Large language models (LLMs) have demonstrated remarkable progress in
reasoning, often through supervised fine-tuning (SFT). However, SFT is
resource-intensive, relying on large curated datasets, rejection-sampled
demonstrations, and uniform optimization across all tokens, even though only a
fraction carry meaningful learning value. In this work, we explore a
counterintuitive idea: can smaller language models (SLMs) teach larger language
models (LLMs) by revealing high-value reasoning moments that reflect the
latter's unique strength? We propose LightReasoner, a novel framework that
leverages the behavioral divergence between a stronger expert model (LLM) and a
weaker amateur model (SLM). LightReasoner operates in two stages: (1) a
sampling stage that pinpoints critical reasoning moments and constructs
supervision examples capturing the expert's advantage through expert-amateur
contrast, and (2) a fine-tuning stage that aligns the expert model with these
distilled examples, amplifying its reasoning strengths. Across seven
mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while
reducing time consumption by 90%, sampled problems by 80%, and tuned token
usage by 99%, all without relying on ground-truth labels. By turning weaker
SLMs into effective teaching signals, LightReasoner offers a scalable and
resource-efficient approach for advancing LLM reasoning. Code is available at:
https://github.com/HKUDS/LightReasoner

</details>


### [161] [Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning](https://arxiv.org/abs/2510.07974)
*Jialu Du,Guiyang Hou,Yihui Fu,Chen Wu,Wenqi Zhang,Yongliang Shen,Weiming Lu*

Main category: cs.CL

TL;DR: 论文发现大语言模型（LLMs）在社交推理任务上存在困惑和推理不一致的问题，提出了一种自适应世界模型增强机制以提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在数学和代码推理任务表现优异，但在处理包含多个参与者和时间线的社交推理情境时表现不佳，经常混淆客观事实与主体信念，导致推理错误或死循环，因此需要改进推理方式。

Method: 作者通过分析DeepSeek-R1在社交推理中的推理轨迹，发现其在推理过程中遇到困难、输出矛盾用词等。为此，提出一种自适应世界模型增强推理机制：动态构建文本世界模型，追踪实体状态与时序，并在检测到困惑时，通过反馈明晰的世界状态描述，辅助模型区分主观与客观信息。该机制仿照人类区分外部事件和内在信念的推理过程。

Result: 在三个社交推理基准上，提出的方法在准确率上取得显著提升，例如Hi-ToM基准提升了10%，同时减少了最多33.8%的计算token消耗。

Conclusion: 世界模型增强机制有效改善了LLMs在社交场景下的推理表现，能以简洁有效的方式提升模型准确率并降低推理成本，为大模型在社交推理应用落地提供了可行路径。

Abstract: While large language models (LLMs) excel in mathematical and code reasoning,
we observe they struggle with social reasoning tasks, exhibiting cognitive
confusion, logical inconsistencies, and conflation between objective world
states and subjective belief states. Through deteiled analysis of DeepSeek-R1's
reasoning trajectories, we find that LLMs frequently encounter reasoning
impasses and tend to output contradictory terms like "tricky" and "confused"
when processing scenarios with multiple participants and timelines, leading to
erroneous reasoning or infinite loops. The core issue is their inability to
disentangle objective reality from agents' subjective beliefs. To address this,
we propose an adaptive world model-enhanced reasoning mechanism that constructs
a dynamic textual world model to track entity states and temporal sequences. It
dynamically monitors reasoning trajectories for confusion indicators and
promptly intervenes by providing clear world state descriptions, helping models
navigate through cognitive dilemmas. The mechanism mimics how humans use
implicit world models to distinguish between external events and internal
beliefs. Evaluations on three social benchmarks demonstrate significant
improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational
costs (up to 33.8% token reduction), offering a simple yet effective solution
for deploying LLMs in social contexts.

</details>


### [162] [Leveraging Author-Specific Context for Scientific Figure Caption Generation: 3rd SciCap Challenge](https://arxiv.org/abs/2510.07993)
*Watcharapong Timklaypachara,Monrada Chiewhawan,Nopporn Lekuthai,Titipat Achakulvisut*

Main category: cs.CL

TL;DR: 本文提出了一种专注于科学图表标题生成的系统，结合了文本上下文和作者写作风格，实现了准确且风格一致的图表标题自动生成。


<details>
  <summary>Details</summary>
Motivation: 科学图表标题不仅要准确传达信息，还需与原文风格保持一致，手动编写耗时且风格多样，因此需要自动化并个性化的生成方法。

Method: 提出了两阶段流程：第一阶段结合上下文筛选、类别特定提示优化（通过DSPy's MIPROv2和SIMBA）与候选标题选择；第二阶段用少样本学习和作者风格资料进行标题风格细化。

Result: 类别特定提示优化相比零样本和通用方法，ROUGE-1召回提升8.3%，仅有小幅精度和BLEU-4分数下降；作者资料驱动风格微调可大幅提升BLEU（提升40-48%）和ROUGE（提升25-27%）。

Conclusion: 将上下文理解与作者风格融合，可生成科学准确且风格一致的图表标题，有效提升自动化图表标题生成质量。

Abstract: Scientific figure captions require both accuracy and stylistic consistency to
convey visual information. Here, we present a domain-specific caption
generation system for the 3rd SciCap Challenge that integrates figure-related
textual context with author-specific writing styles using the LaMP-Cap dataset.
Our approach uses a two-stage pipeline: Stage 1 combines context filtering,
category-specific prompt optimization via DSPy's MIPROv2 and SIMBA, and caption
candidate selection; Stage 2 applies few-shot prompting with profile figures
for stylistic refinement. Our experiments demonstrate that category-specific
prompts outperform both zero-shot and general optimized approaches, improving
ROUGE-1 recall by +8.3\% while limiting precision loss to -2.8\% and BLEU-4
reduction to -10.9\%. Profile-informed stylistic refinement yields 40--48\%
gains in BLEU scores and 25--27\% in ROUGE. Overall, our system demonstrates
that combining contextual understanding with author-specific stylistic
adaptation can generate captions that are both scientifically accurate and
stylistically faithful to the source paper.

</details>


### [163] [Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks](https://arxiv.org/abs/2510.08002)
*Cheng Yang,Xuemeng Yang,Licheng Wen,Daocheng Fu,Jianbiao Mei,Rong Wu,Pinlong Cai,Yufan Shen,Nianchen Deng,Botian Shi,Yu Qiao,Haifeng Li*

Main category: cs.CL

TL;DR: MUSE是一种新颖的基于经验驱动、自我进化的LLM智能体系统，通过层次化记忆模块实现知识积累和持续自我提升，在长周期任务自动化领域取得了显著进展。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型智能体（LLM agents）部署在现实复杂长期任务时存在无法自我学习和自进化的致命局限，即测试时模型参数静止，不能从经验中汲取知识。解决这一问题对于提升AI智能体在现实场景的生产力和适应能力至关重要。

Method: 提出了MUSE智能体框架，核心是一个层次化记忆模块，管理和组织多层次的经验。在每个子任务后，智能体会自主反思，将任务轨迹转化为结构化经验，并整合回记忆模块，实现知识积累与自我进化，从而突破静态参数的限制。

Result: 在长周期任务基准TAC上，MUSE利用轻量级Gemini-2.5 Flash模型取得了新的SOTA表现，实验展现了其自我积累经验后任务完成能力持续提升的趋势，具备强大的持续学习和自进化能力。此外，MUSE积累的经验还能实现新任务的零样本提升，展现出良好的泛化性。

Conclusion: MUSE为持续学习和自进化的AI生产力智能体奠定了新范式，具备现实生产力任务自动化的能力，为该领域带来突破性进展。

Abstract: Large Language Models have demonstrated remarkable capabilities across
diverse domains, yet significant challenges persist when deploying them as AI
agents for real-world long-horizon tasks. Existing LLM agents suffer from a
critical limitation: they are test-time static and cannot learn from
experience, lacking the ability to accumulate knowledge and continuously
improve on the job. To address this challenge, we propose MUSE, a novel agent
framework that introduces an experience-driven, self-evolving system centered
around a hierarchical Memory Module. MUSE organizes diverse levels of
experience and leverages them to plan and execute long-horizon tasks across
multiple applications. After each sub-task execution, the agent autonomously
reflects on its trajectory, converting the raw trajectory into structured
experience and integrating it back into the Memory Module. This mechanism
enables the agent to evolve beyond its static pretrained parameters, fostering
continuous learning and self-evolution. We evaluate MUSE on the long-horizon
productivity benchmark TAC. It achieves new SOTA performance by a significant
margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments
demonstrate that as the agent autonomously accumulates experience, it exhibits
increasingly superior task completion capabilities, as well as robust
continuous learning and self-evolution capabilities. Moreover, the accumulated
experience from MUSE exhibits strong generalization properties, enabling
zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI
agents capable of real-world productivity task automation.

</details>


### [164] [ChatGPT as a Translation Engine: A Case Study on Japanese-English](https://arxiv.org/abs/2510.08042)
*Vincent Michael Sutanto,Giovanni Gatti De Giacomo,Toshiaki Nakazawa,Masaru Yamada*

Main category: cs.CL

TL;DR: 本文评估了ChatGPT在日英翻译中的表现，并与商用翻译引擎作对比，考察了不同提示词和评价方式。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，ChatGPT等大模型在翻译任务中的潜力引起了广泛关注，但其在日英翻译中的性能尚需系统评估，尤其是与现有主流翻译系统相比。

Method: 对ChatGPT进行日英翻译实验，分别设置简单和增强提示词，进行句级与文档级翻译，并用自动评价和基于MQM的人类评价方法进行对比，同时与两款主流翻译引擎进行横向对比。

Result: 结果显示：文档级翻译显著优于句级翻译；增强型提示词并未表现出比简单提示词更优的结果；在自动评价中ChatGPT-3.5表现较好，但在人类评价中出现准确度（ChatGPT-3.5）和流畅度（ChatGPT-4）之间的权衡；整体上ChatGPT与商用系统具有竞争力。

Conclusion: ChatGPT在日英翻译任务中表现优良，部分维度可媲美商用翻译系统，但针对不同场景需权衡准确性和流畅性，增强提示词对于提升翻译质量的作用有限。

Abstract: This study investigates ChatGPT for Japanese-English translation, exploring
simple and enhanced prompts and comparing against commercially available
translation engines. Performing both automatic and MQM-based human evaluations,
we found that document-level translation outperforms sentence-level translation
for ChatGPT. On the other hand, we were not able to determine if enhanced
prompts performed better than simple prompts in our experiments. We also
discovered that ChatGPT-3.5 was preferred by automatic evaluation, but a
tradeoff exists between accuracy (ChatGPT-3.5) and fluency (ChatGPT-4). Lastly,
ChatGPT yields competitive results against two widely-known translation
systems.

</details>


### [165] [Climate Knowledge in Large Language Models](https://arxiv.org/abs/2510.08043)
*Ivan Kuznetsov,Jacopo Grassi,Dmitrii Pantiukhin,Boris Shapkin,Thomas Jung,Nikolay Koldunov*

Main category: cs.CL

TL;DR: 本研究分析了当前大型语言模型（LLMs）在无需外部检索的情况下，能否准确回忆气候常态参数（如全球不同地点的7月2米气温均值）。结果发现，LLMs虽能捕捉一定的气候结构，但在高纬度、高海拔地区误差较大，且对地区变暖空间分布表现能力有限。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被越来越多地应用于气候领域，理解其内部气候知识的准确性对于可靠性评估和预防错误信息传播非常重要。而当前对LLMs回忆气候常态知识的能力缺乏系统性刻画，因此作者希望填补这一知识空白。

Method: 作者设计了一个基于全球1°分辨率陆地点的查询网格，查询指定位置（通过坐标和地理描述）在1991-2020年7月的2米空气温度均值，让LLMs直接回答，并用ERA5再分析数据验证其输出的准确性。此外，分析了地理描述对模型表现的影响，并考察了模型对长期变暖趋势的捕捉能力。

Result: LLMs对纬度、地形等总体气候模式有一定记忆，表现为RMSE为3-6°C、系统性偏差约±1°C，加入地理描述能平均降低27%误差，且模型规模越大，位置描述影响越大。但在高山及高纬区误差高（海拔>1500m RMSE达5-13°C），对空间变暖格局基本捕捉不到。

Conclusion: 当前LLMs对现今气候分布有一定捕捉能力，但无法准确反映温度变化的地区/局地空间格局，对区域气候动态的认知有限。文中方法为定量评估LLMs气候知识能力提供了基准，补充了相关气候知识传播评估。

Abstract: Large language models (LLMs) are increasingly deployed for climate-related
applications, where understanding internal climatological knowledge is crucial
for reliability and misinformation risk assessment. Despite growing adoption,
the capacity of LLMs to recall climate normals from parametric knowledge
remains largely uncharacterized. We investigate the capacity of contemporary
LLMs to recall climate normals without external retrieval, focusing on a
prototypical query: mean July 2-m air temperature 1991-2020 at specified
locations. We construct a global grid of queries at 1{\deg} resolution land
points, providing coordinates and location descriptors, and validate responses
against ERA5 reanalysis. Results show that LLMs encode non-trivial climate
structure, capturing latitudinal and topographic patterns, with
root-mean-square errors of 3-6 {\deg}C and biases of $\pm$1 {\deg}C. However,
spatially coherent errors remain, particularly in mountains and high latitudes.
Performance degrades sharply above 1500 m, where RMSE reaches 5-13 {\deg}C
compared to 2-4 {\deg}C at lower elevations. We find that including geographic
context (country, city, region) reduces errors by 27% on average, with larger
models being most sensitive to location descriptors. While models capture the
global mean magnitude of observed warming between 1950-1974 and 2000-2024, they
fail to reproduce spatial patterns of temperature change, which directly relate
to assessing climate change. This limitation highlights that while LLMs may
capture present-day climate distributions, they struggle to represent the
regional and local expression of long-term shifts in temperature essential for
understanding climate dynamics. Our evaluation framework provides a
reproducible benchmark for quantifying parametric climate knowledge in LLMs and
complements existing climate communication assessments.

</details>


### [166] [A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models](https://arxiv.org/abs/2510.08049)
*Congming Zheng,Jiachen Zhu,Zhuoying Ou,Yuxiang Chen,Kangning Zhang,Rong Shan,Zeyu Zheng,Mengyue Yang,Jianghao Lin,Yong Yu,Weinan Zhang*

Main category: cs.CL

TL;DR: 本论文综述了流程奖励模型（PRMs）在大语言模型推理校准中的最新进展，总结了其数据生成、模型构建及应用方法。


<details>
  <summary>Details</summary>
Motivation: 传统的校准主要依赖于只评价最终答案的结果奖励模型（ORMs），无法细致把控推理过程。为实现更精细的推理对齐，需探索能在推理过程中评价和引导的PRMs。

Method: 本文系统性讨论了PRMs的完整流程，包括如何生成流程数据、构建PRMs，以及如何在推理测试、强化学习中使用PRMs。同时，涵盖了数学、代码、文本、多模态推理、机器人和智能体等多领域的应用，并评述了新兴的评测基准。

Result: 综述结果明确了流程奖励模型在多领域的实际应用场景和已有方法，对现有设计进行了详细归类，并展示了该方向的开放性挑战。

Conclusion: 该综述有助于厘清流程奖励模型的设计空间，指出现存挑战，并为未来更细粒度和稳健的推理对齐研究提供指导。

Abstract: Although Large Language Models (LLMs) exhibit advanced reasoning ability,
conventional alignment remains largely dominated by outcome reward models
(ORMs) that judge only final answers. Process Reward Models(PRMs) address this
gap by evaluating and guiding reasoning at the step or trajectory level. This
survey provides a systematic overview of PRMs through the full loop: how to
generate process data, build PRMs, and use PRMs for test-time scaling and
reinforcement learning. We summarize applications across math, code, text,
multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our
goal is to clarify design spaces, reveal open challenges, and guide future
research toward fine-grained, robust reasoning alignment.

</details>


### [167] [FedDTRE: Federated Dialogue Generation Models Powered by Trustworthiness Evaluation](https://arxiv.org/abs/2510.08058)
*Shule Lu,Lingxiang Wang,Sijia Wen,Ziwei Wang,Hainan Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为FedDTRE的联邦自适应聚合策略，在对话生成场景中通过信任度评估来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习因保护隐私和适应个体差异而用于对话系统，但现有方法在有限数据下容易过拟合，并在多轮训练后遗忘全局信息，导致泛化能力差，因此亟需更优策略平衡全局与本地模型贡献。

Method: FedDTRE方法在本地更新过程中利用评测数据集上的全局与本地模型信任度分数，动态调节全局模型对本地模型更新的影响，而非一味采用或替换全局模型，有助于提升聚合灵活性与个性化。

Result: 实验结果表明，FedDTRE能够有效提升对话模型的整体表现和对话生成质量。

Conclusion: FedDTRE通过信任度驱动的自适应聚合策略，缓解了过拟合和遗忘全局信息等问题，为联邦对话系统带来更好的泛化与生成表现。

Abstract: With the rapid development of artificial intelligence, dialogue systems have
become a prominent form of human-computer interaction. However, traditional
centralized or fully local training approaches face challenges in balancing
privacy preservation and personalization due to data privacy concerns and
heterogeneous device capabilities. Federated learning, as a representative
distributed paradigm, offers a promising solution. However, existing methods
often suffer from overfitting under limited client data and tend to forget
global information after multiple training rounds, leading to poor
generalization. To address these issues, we propose FedDTRE, a Federated
adaptive aggregation strategy for Dialogue generation based on Trustworthiness
Evaluation. Instead of directly replacing local models with the global model,
FedDTRE leverages trustworthiness scores of both global and local models on a
fairness-oriented evaluation dataset to dynamically regulate the global model's
contribution during local updates. Experimental results demonstrate that
FedDTRE can improve dialogue model performance and enhance the quality of
dialogue generation.

</details>


### [168] [Everything is Plausible: Investigating the Impact of LLM Rationales on Human Notions of Plausibility](https://arxiv.org/abs/2510.08091)
*Shramay Palta,Peter Rankel,Sarah Wiegreffe,Rachel Rudinger*

Main category: cs.CL

TL;DR: 本论文研究了大语言模型（LLM）生成的正反理据对人类常识多项选择题答案可信度判断的影响，发现人类和LLM的判断都会被这些理据显著影响。


<details>
  <summary>Details</summary>
Motivation: 动机在于：虽然常识判断被认为是人类“专家”领域，但在生成式人工智能的广泛应用下，了解这些模型生成的理由是否会影响人的判断具有重要的理论和现实意义，尤其涉及认知心理学和AI伦理。

Method: 作者收集了人类（3,000条）和LLM（13,600条）对常识题答案可信度的判断，并设计实验让受试者看到LLM生成的支持（PRO）和反对（CON）理由，观察理据对判断的影响。

Result: 结果显示，无论是人类还是LLM，面对正向的理据会提高对答案的可信度判断，看到反向理据则会降低可信度——证明了LLM生成的理据对人类判断有明显的说服力。

Conclusion: 作者认为，LLM不只是可以辅助研究人类认知，还可能对人类的信念和判断有较强影响，即使在常识这样的“专家”领域，因此在实际应用时应保持警惕，防范相关风险。

Abstract: We investigate the degree to which human plausibility judgments of
multiple-choice commonsense benchmark answers are subject to influence by
(im)plausibility arguments for or against an answer, in particular, using
rationales generated by LLMs. We collect 3,000 plausibility judgments from
humans and another 13,600 judgments from LLMs. Overall, we observe increases
and decreases in mean human plausibility ratings in the presence of
LLM-generated PRO and CON rationales, respectively, suggesting that, on the
whole, human judges find these rationales convincing. Experiments with LLMs
reveal similar patterns of influence. Our findings demonstrate a novel use of
LLMs for studying aspects of human cognition, while also raising practical
concerns that, even in domains where humans are ``experts'' (i.e., common
sense), LLMs have the potential to exert considerable influence on people's
beliefs.

</details>


### [169] [The Price of Thought: A Multilingual Analysis of Reasoning, Performance, and Cost of Negotiation in Large Language Models](https://arxiv.org/abs/2510.08098)
*Sherzod Hakimov,Roland Bernard,Tim Leiber,Karl Osswald,Kristina Richert,Ruilin Yang,Raffaella Bernardi,David Schlangen*

Main category: cs.CL

TL;DR: 本论文系统地评估了大型语言模型（LLM）推理能力对AI谈判表现的影响，涵盖商业和开源模型，以及三种语言。实验发现推理能力大幅提升了谈判效果，但增加了计算成本，并揭示了开源模型在内部推理时倾向于用英语，即使外部交流为德语或意大利语，而商业模型则能保持语言一致性。


<details>
  <summary>Details</summary>
Motivation: AI在谈判任务中需要具备策略推理、对手建模及合作与竞争的权衡能力。目前对于LLM推理在多语言、多平台中的表现影响缺乏系统评估。作者希望填补这一研究空白。

Method: 作者使用自博弈（self-play）方式，在三种对话游戏中评测商业和开源LLM的谈判能力，实验包含英语、德语和意大利语三种语言。分析了性能与成本的权衡、推理过程的语言一致性以及模型的策略性适应表现。

Result: 开启推理功能（即提高推理计算成本）能显著提升模型的谈判表现。例如GPT-5开启推理后表现提升31.4%，但成本上升近400%。此外，开源模型在内部推理时偏好使用英语，影响可解释性；而商业模型能保证推理语言与输出语言一致。

Conclusion: 推理能力大幅增强了LLM谈判表现，但需权衡计算成本。多语言环境下，开源模型推理的语言切换影响了推理透明度，而商业模型在语言一致性上表现更佳。

Abstract: Negotiation is a fundamental challenge for AI agents, as it requires an
ability to reason strategically, model opponents, and balance cooperation with
competition. We conduct the first comprehensive study systematically evaluating
the effect of (LLM-)reasoning on the negotiation abilities of both commercial
and open-weight LLMs, and do this across three languages. Using a self-play
setup across three diverse dialogue games, we analyse trade-offs between
performance and cost, the language consistency of reasoning processes, and the
nature of strategic adaptation exhibited by models. Our findings show that
enabling reasoning-that is, scaling test time compute-significantly improves
negotiation outcomes by enhancing collaboration and helping models overcome
task complexities, but comes at a substantial computational cost: reasoning
improves GPT-5's performance by 31.4 % while increasing its cost by nearly 400
%. Most critically, we uncover a significant multilingual reasoning
distinction: open-weight models consistently switch to English for their
internal reasoning steps, even when negotiating in German or Italian (and thus
possibly impacting potential explainability gains through the disclosure of
reasoning traces), while leading commercial models maintain language
consistency between their reasoning and final output.

</details>


### [170] [Lossless Vocabulary Reduction for Auto-Regressive Language Models](https://arxiv.org/abs/2510.08102)
*Daiki Chijiwa,Taku Hasegawa,Kyosuke Nishida,Shin'ya Yamaguchi,Tomoya Ohba,Tamao Sakao,Susumu Takeuchi*

Main category: cs.CL

TL;DR: 本文提出了一种无损词表缩减的理论框架，使得自回归语言模型能在词表缩减的情况下保持准确率不变，并实现不同分词方式的模型高效协作。


<details>
  <summary>Details</summary>
Motivation: 目前每个语言模型通常有各自的分词方法和词表，这导致不同模型之间（如模型集成）难以在token分布层面合作，影响了模型的灵活性和扩展性。因此，研究如何缩减并统一词表，提升模型协同能力具有重要意义。

Method: 论文建立了一个无损词表缩减的理论框架，通过有效的算法将自回归语言模型转换为使用更小词表的等价模型，且不损失原有预测精度。该方法还可通过提取最大公共词表实现不同模型间的高效合作。

Result: 实验表明，采用该理论框架后，不同分词体系的语言模型能够基于最大公共词表实现高效模型协作，且缩减词表不会损失生成精度。

Conclusion: 论文验证了无损词表缩减理论的可行性，为多模型协作和集成提供了新的思路，有助于提升语言模型在实际应用中的兼容性和效率。

Abstract: Tokenization -- the process of decomposing a given text into a sequence of
subwords called tokens -- is one of the key components in the development of
language models. Particularly, auto-regressive language models generate texts
token by token, i.e., by predicting the next-token distribution given the
previous ones, and thus tokenization directly affects their efficiency in text
generation. Since each language model has their own vocabulary as a set of
possible tokens, they struggle to cooperate with each other at the level of
next-token distributions such as model ensemble. In this paper, we establish a
theoretical framework of lossless vocabulary reduction, which efficiently
converts a given auto-regressive language model into the one with an
arbitrarily small vocabulary without any loss in accuracy. As an application,
we demonstrate that language models with different tokenization can cooperate
with each other efficiently through their maximal common vocabulary.

</details>


### [171] [Evaluating LLM-Generated Legal Explanations for Regulatory Compliance in Social Media Influencer Marketing](https://arxiv.org/abs/2510.08111)
*Haoyang Gui,Thales Bertaglia,Taylor Annabell,Catalina Goanta,Tjomme Dooper,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本论文评估了两种大语言模型（gpt-5-nano 和 gemini-2.5-flash-lite）在检测Instagram隐性植入广告时的表现，提出针对法律合规的误差分类，并对自动化内容监管进行了法律层面的讨论。


<details>
  <summary>Details</summary>
Motivation: 随着网红营销的普及，广告内容的识别变得模糊，现有的隐性广告检测方法缺乏法律基础或透明度，因此需要开展基于法律知识的自动识别研究，帮助监管机构有效执法。

Method: 作者基于1143条Instagram帖子，对gpt-5-nano和gemini-2.5-flash-lite两种LLM采用三种不同法律知识深入程度的提示策略进行对比，分析模型对广告内容识别的准确性和解释的法律合理性，并建立了LLM法律推理常见误差的分类体系。

Result: 两种模型在简单场景下对广告内容的分类表现很好（F1最高达0.93），但是在歧义较强的案例中表现下降超过10分。作者发现模型推理存在较多引用遗漏、表达不清和对隐蔽广告的高错误率。增加监管法规文本能提升模型解释质量，但对准确率提升有限。

Conclusion: 论文贡献包括：1）提出LLM法律推理常见误差的分类法，推动自动化监管的法律合规性评估；2）提供由经培训学生注释的解释数据集；3）结合多种定量和定性评估方法，提出对广告内容自动化监管实践的真实启示，对监管机构有实际参考价值。

Abstract: The rise of influencer marketing has blurred boundaries between organic
content and sponsored content, making the enforcement of legal rules relating
to transparency challenging. Effective regulation requires applying legal
knowledge with a clear purpose and reason, yet current detection methods of
undisclosed sponsored content generally lack legal grounding or operate as
opaque "black boxes". Using 1,143 Instagram posts, we compare gpt-5-nano and
gemini-2.5-flash-lite under three prompting strategies with controlled levels
of legal knowledge provided. Both models perform strongly in classifying
content as sponsored or not (F1 up to 0.93), though performance drops by over
10 points on ambiguous cases. We further develop a taxonomy of reasoning
errors, showing frequent citation omissions (28.57%), unclear references
(20.71%), and hidden ads exhibiting the highest miscue rate (28.57%). While
adding regulatory text to the prompt improves explanation quality, it does not
consistently improve detection accuracy. The contribution of this paper is
threefold. First, it makes a novel addition to regulatory compliance technology
by providing a taxonomy of common errors in LLM-generated legal reasoning to
evaluate whether automated moderation is not only accurate but also legally
robust, thereby advancing the transparent detection of influencer marketing
content. Second, it features an original dataset of LLM explanations annotated
by two students who were trained in influencer marketing law. Third, it
combines quantitative and qualitative evaluation strategies for LLM
explanations and critically reflects on how these findings can support
advertising regulatory bodies in automating moderation processes on a solid
legal foundation.

</details>


### [172] [Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations](https://arxiv.org/abs/2510.08120)
*Jasmina Gajcin,Erik Miehling,Rahul Nair,Elizabeth Daly,Radu Marinescu,Seshu Tirupathi*

Main category: cs.CL

TL;DR: 本文提出了一种从作为评判者的大型语言模型（LLM-as-a-Judge）中抽取概念级全局评判策略的方法，验证其解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被广泛用于文本评估，甚至逐步取代人工标注，了解这种自动评判方式潜在的偏见和风险变得至关重要。因此，提出可解释和可验证的评判策略，有助于提升系统透明度和用户信任。

Method: 提出了两种算法：CLoVE用于生成可验证的、基于概念的局部对比解释，GloVE将这些局部规则通过聚类、摘要和验证等步骤，汇总为全局评判策略，并在多个有害内容检测基准上进行了评估。

Result: 实验结果显示，从LLM-as-a-Judge抽取的全局策略能够高度忠实地反映模型判断，且在文本扰动和对抗攻击下具有一定鲁棒性。同时，用户调研表明用户对所提全局策略的理解和满意度较高。

Conclusion: 所提出的方法能有效提取LLM的全局评判策略，提升模型决策的可解释性和实用性，在实际评判场景中具有广泛应用潜力。

Abstract: Using LLMs to evaluate text, that is, LLM-as-a-judge, is increasingly being
used at scale to augment or even replace human annotations. As such, it is
imperative that we understand the potential biases and risks of doing so. In
this work, we propose an approach for extracting high-level concept-based
global policies from LLM-as-a-Judge. Our approach consists of two algorithms:
1) CLoVE (Contrastive Local Verifiable Explanations), which generates
verifiable, concept-based, contrastive local explanations and 2) GloVE (Global
Verifiable Explanations), which uses iterative clustering, summarization and
verification to condense local rules into a global policy. We evaluate GloVE on
seven standard benchmarking datasets for content harm detection. We find that
the extracted global policies are highly faithful to decisions of the
LLM-as-a-Judge. Additionally, we evaluated the robustness of global policies to
text perturbations and adversarial attacks. Finally, we conducted a user study
to evaluate user understanding and satisfaction with global policies.

</details>


### [173] [Mitigating Judgment Preference Bias in Large Language Models through Group-Based Polling](https://arxiv.org/abs/2510.08145)
*Shuliang Liu,Zhipeng Xu,Zhenghao Liu,Yukun Yan,Minghe Yu,Yu Gu,Chong Chen,Huiyuan Xie,Ge Yu*

Main category: cs.CL

TL;DR: 本文介绍了一个全新无监督多智能体协同优化框架Genii，能够有效减轻大语言模型（LLM）作为自动评审者时的偏好性评价偏差，提升评估结果的可靠性和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM作为自动评判工具（LLM-as-a-Judge）虽然便捷且影响力大，但往往在评判时偏好自身生成的答案，导致信度不足。希望通过新的方法改善LLM评判的准确性和公正性。

Method: 提出Genii框架，将多种LLM评判模型集成为多智能体系统，通过模拟客户端-服务器投票交互机制，在无需人工标注数据的情况下对每个客户端智能体进行无监督优化。

Result: 实验表明Genii在没有人工标注的情况下，性能优于常规有监督训练的模型，对各类客户端智能体均有提升，即使服务器端模型相对较弱。贯穿多轮投票协作，Genii持续改善偏好性评价偏差。

Conclusion: Genii框架能有效缓解LLM评判模型的偏好性评价偏差，提高评估的可靠性和一致性，实现无监督优化，在实际应用中具有较高的推广价值。

Abstract: Large Language Models (LLMs) as automatic evaluators, commonly referred to as
LLM-as-a-Judge, have also attracted growing attention. This approach plays a
vital role in aligning LLMs with human judgments, providing accurate and
reliable assessments. However, LLM-based judgment models often exhibit judgment
preference bias during the evaluation phase, tending to favor responses
generated by themselves, undermining the reliability of their judgments. This
paper introduces the Group-Based Polling Optimization (Genii), an unsupervised
multi-agent collaborative optimization framework that mitigates the inherent
judgment preference bias of judgment models. Specifically, Genii integrates
various LLM-based judgment models into a multi-agent system and simulates the
interactive client-server polling mechanism to optimize each client agent
unsupervisedly. Our experiments demonstrate that Genii outperforms supervised
models trained on annotated judgment data, while requiring no human-labeled
annotations. Genii consistently improves performance across different client
agents during the polling, even when weaker models act as server agents.
Further analysis reveals that Genii effectively mitigates judgment preference
bias of LLM-based judgment models, demonstrating its effectiveness. All codes
are available at https://github.com/NEUIR/Genii.

</details>


### [174] [AI Knowledge Assist: An Automated Approach for the Creation of Knowledge Bases for Conversational AI Agents](https://arxiv.org/abs/2510.08149)
*Md Tahmid Rahman Laskar,Julien Bouvier Tremblay,Xue-Yong Fu,Cheng Chen,Shashi Bhushan TN*

Main category: cs.CL

TL;DR: 本文介绍了一种自动化构建知识库的方法，即从历史客户与客服的对话中抽取问答对，并利用其快速部署RAG聊天机器人。


<details>
  <summary>Details</summary>
Motivation: RAG技术广泛应用于会话式AI系统，但缺乏公司专属知识库阻碍了该技术在呼叫中心的应用。作者希望解决知识库冷启动难题，降低部署门槛。

Method: 提出AI Knowledge Assist系统，从历史客服对话中自动抽取问答对，构建知识库；并用内部数据对轻量级LLM进行微调，提升问答表现。

Result: 在20家公司数据上测试，基于LLaMA-3.1-8B的系统在信息类问答中准确率超过90%，表现优于更大的闭源LLM模型。

Conclusion: 该方法能够有效消除呼叫中心知识库冷启动问题，实现RAG聊天机器人的即刻部署，提升了集成效率和实际应用价值。

Abstract: The utilization of conversational AI systems by leveraging Retrieval
Augmented Generation (RAG) techniques to solve customer problems has been on
the rise with the rapid progress of Large Language Models (LLMs). However, the
absence of a company-specific dedicated knowledge base is a major barrier to
the integration of conversational AI systems in contact centers. To this end,
we introduce AI Knowledge Assist, a system that extracts knowledge in the form
of question-answer (QA) pairs from historical customer-agent conversations to
automatically build a knowledge base. Fine-tuning a lightweight LLM on internal
data demonstrates state-of-the-art performance, outperforming larger
closed-source LLMs. More specifically, empirical evaluation on 20 companies
demonstrates that the proposed AI Knowledge Assist system that leverages the
LLaMA-3.1-8B model eliminates the cold-start gap in contact centers by
achieving above 90% accuracy in answering information-seeking questions. This
enables immediate deployment of RAG-powered chatbots.

</details>


### [175] [DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension on Business Conversations](https://arxiv.org/abs/2510.08152)
*Elena Khasanova,Harsh Saini,Md Tahmid Rahman Laskar,Xue-Yong Fu,Cheng Chen,Shashi Bhushan TN*

Main category: cs.CL

TL;DR: 本文提出了一种增强小型LLM在实际商业对话任务中零样本泛化能力的新方法。


<details>
  <summary>Details</summary>
Motivation: 大型LLM虽然强大但推理成本高，难以实际部署。小型LLM效率高但泛化和指令跟随能力弱，且传统微调易出现遗忘现象，影响适应新任务的能力。

Method: 提出了DACIP-RC（Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension）方法，通过对商业对话转录进行阅读理解生成多样化任务指令和响应，实现持续的指令预训练，提升小模型的领域适应性。

Result: DACIP-RC显著提升了小型LLM在多种商业对话任务（如会议总结、行动项生成、通话目的识别等）上的零样本泛化能力。

Conclusion: DACIP-RC为企业利用专有数据集提升小型LLM的场景适应性提供了有效途径，是将指令预训练首次应用于业务会话领域的重要探索。

Abstract: The rapid advancements in Large Language Models (LLMs) have enabled their
adoption in real-world industrial scenarios for various natural language
processing tasks. However, the high inference cost of large-scale LLMs makes
their deployment impractical, necessitating the use of smaller models. Despite
their efficiency, smaller LLMs lack robust zero-shot instruction-following
capabilities across diverse domains, limiting their adaptability to dynamic
user requirements. Traditional fine-tuning approaches exacerbate this issue by
inducing catastrophic forgetting, reducing the model's generalization ability
for unseen tasks. In this paper, we propose Domain Adaptive Continual
Instruction Pre-Training via Reading Comprehension (DACIP-RC), a continual
pre-training technique that enhances smaller LLMs' domain adaptability for
business conversational tasks. Unlike conventional pre-training approaches that
rely on next-token prediction, DACIP-RC generates diverse task instructions and
responses via reading comprehension on conversation transcripts, enabling
better instruction generalization. Our empirical evaluations demonstrate that
DACIP-RC significantly improves zero-shot generalization across a wide range of
business conversational tasks, including meeting summarization, action item
generation, and call purpose identification. To the best of our knowledge, this
is the first work to apply instruction pre-training on business conversational
data, providing insights into how industries can leverage proprietary datasets
for domain adaptation.

</details>


### [176] [Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs](https://arxiv.org/abs/2510.08158)
*Shuzhou Yuan,Ercong Nie,Yinuo Sun,Chenxuan Zhao,William LaCroix,Michael Färber*

Main category: cs.CL

TL;DR: 本文提出两项新的基准（XSB和MS-XSB），用于评估大语言模型（LLM）在安全响应中的过度拒绝现象，并提出三种无需重新训练即可缓解该问题的方法，实验证明这些方法可提升模型对安全请求的响应能力，同时保证安全性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在面对含有类似不安全词汇的正常请求时，常常产生“过度拒绝”，即对安全请求错误地拒绝回答。这不仅影响模型的可用性，还削弱了用户体验，因此需要有系统的工具和方法来发现和减缓这种现象。

Method: 作者构建了两个新基准：XSB（Exaggerated Safety Benchmark）主要针对单轮提示，并标注了容易诱发拒绝的关键词；MS-XSB（Multi-turn Scenario-based XSB）则针对多轮富含语境的对话场景，系统性测试模型的拒绝校准能力。纂利用事后解释方法识别触发拒绝的关键词，并在预测阶段采用三种轻量、与模型无关的缓解策略（忽略词指令、提示重写和注意力引导），无需访问模型参数或进行再训练。

Result: 作者在四个指令微调的Llama模型上进行了实验，发现在不牺牲安全性的前提下，所提方法显著提升了模型对安全请求的合规性。相关基准有效揭示了主流模型在多轮复杂场景下过度拒绝的问题。

Conclusion: 本研究为检测和缓解大语言模型的过度拒绝问题提供了可复现的评测与策略框架，并指出了将LLM部署得更安全且更有用的实用途径。

Abstract: Large language models (LLMs) frequently produce false refusals, declining
benign requests that contain terms resembling unsafe queries. We address this
challenge by introducing two comprehensive benchmarks: the Exaggerated Safety
Benchmark (XSB) for single-turn prompts, annotated with "Focus" keywords that
identify refusal-inducing triggers, and the Multi-turn Scenario-based
Exaggerated Safety Benchmark (MS-XSB), which systematically evaluates refusal
calibration in realistic, context-rich dialog settings. Our benchmarks reveal
that exaggerated refusals persist across diverse recent LLMs and are especially
pronounced in complex, multi-turn scenarios. To mitigate these failures, we
leverage post-hoc explanation methods to identify refusal triggers and deploy
three lightweight, model-agnostic approaches, ignore-word instructions, prompt
rephrasing, and attention steering, at inference time, all without retraining
or parameter access. Experiments on four instruction-tuned Llama models
demonstrate that these strategies substantially improve compliance on safe
prompts while maintaining robust safety protections. Our findings establish a
reproducible framework for diagnosing and mitigating exaggerated refusals,
highlighting practical pathways to safer and more helpful LLM deployments.

</details>


### [177] [ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code](https://arxiv.org/abs/2510.08163)
*Jian Xie,Zhendong Chu,Aoxiao Zhong,Kai Zhang,Mingzhe Han,Xin Fang,Jialie Shen,Qingsong Wen*

Main category: cs.CL

TL;DR: 本文提出了适应性推理模型ARM2，能有效减少推理过程中的冗余生成，提升推理效率，并在多模态任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有大规模推理模型在简单任务中过度推理，导致生成答案冗长，降低效率。以往缓解方案多为启发式且依赖具体任务，缺乏通用自适应推理框架。

Method: 作者提出了ARM2，一个结合强化学习与长度敏感优化的统一推理模型。ARM2在保留推理性能的同时，根据任务难度自适应调整生成长度，并支持多模态（视觉+语言）与可执行代码推理，减少冗余token消耗。

Result: ARM2在多个任务上实验验证，结果显示该模型性能与传统推理模型（如GRPO训练方式）相当，但能平均减少超过70%的token使用量。

Conclusion: ARM2能够在保证推理效果的前提下大幅提升推理效率，适用于多种格式和多模态任务，验证了设计的有效性和通用性。

Abstract: Large Reasoning Models (LRMs) often suffer from the ``over-thinking''
problem, generating unnecessarily long reasoning on simple tasks. Some
strategies have been proposed to mitigate this issue, such as length penalties
or routing mechanisms, but they are typically heuristic and task-specific,
lacking a general framework for adaptive reasoning. In this paper, we present
ARM2, a unified model that adaptively balances reasoning performance and
efficiency across multiple formats through a reinforcement learning framework
augmented with length-aware optimization. Beyond conventional natural language
inference, ARM2 integrates vision understanding, extending its applicability to
multimodal. Moreover, ARM2 integrates executable code into reasoning, enabling
substantial reductions in token cost while preserving task performance compared
to long CoT. Experiments demonstrate that ARM2 achieves performance on par with
traditional reasoning models trained with GRPO, while reducing token usage by
over 70% on average. We further conduct extensive analyses to validate the
effectiveness of ARM2 and the soundness of its design.

</details>


### [178] [METRICALARGS: A Taxonomy for Studying Metrical Poetry with LLMs](https://arxiv.org/abs/2510.08188)
*Chalamalasetti Kranti,Sowmya Vajjala*

Main category: cs.CL

TL;DR: 本文提出了MetricalARGS，这是首个针对诗歌格律相关NLP任务的分类体系，用于多维度评估大语言模型（LLM）在格律诗歌上的能力，并以泰卢固语作为实践案例。


<details>
  <summary>Details</summary>
Motivation: 当前NLP领域对于诗歌的研究主要集中在自动生成和摘要，但许多语言的诗歌有复杂的格律结构，考验模型对音节、音素等规则的理解能力，因此需要更系统地评估LLM在诗歌格律上的表现。

Method: 作者提出了MetricalARGS框架，从分析、检索、生成和辅助四个维度，系统化地定义与诗歌格律相关的NLP任务，并对数据集构建与评价指标进行了探讨。以泰卢固语为例，展示了该框架的应用方法。

Result: 论文建立了MetricalARGS分类体系，细致描述了不同维度下的任务类型，并给出实际应用例子，促进了更规范地评估LLM处理复杂诗歌格律任务的能力。

Conclusion: MetricalARGS为研究和评测LLM在格律诗歌理解及生成上的能力提供了全面的任务蓝本，也为探索LLM在处理复杂语言规则上的极限提供了新途径。

Abstract: Prior NLP work studying poetry has focused primarily on automatic poem
generation and summarization. Many languages have well-studied traditions of
poetic meter which enforce constraints on a poem in terms of syllable and
phoneme patterns. Such advanced literary forms offer opportunities for probing
deeper reasoning and language understanding in Large Language Models (LLMs) and
their ability to follow strict pre-requisites and rules. In this paper, we
introduce MetricalARGS, the first taxonomy of poetry-related NLP tasks designed
to evaluate LLMs on metrical poetry across four dimensions: Analysis,
Retrieval, Generation, and Support. We discuss how these tasks relate to
existing NLP tasks, addressing questions around datasets and evaluation
metrics. Taking Telugu as our example language, we illustrate how the taxonomy
can be used in practice. MetricalARGS highlights the broader possibilities for
understanding the capabilities and limitations of today's LLMs through the lens
of metrical poetry.

</details>


### [179] [Training-Free Group Relative Policy Optimization](https://arxiv.org/abs/2510.08191)
*Yuzheng Cai,Siqi Cai,Yuchen Shi,Zihan Xu,Lichao Chen,Yulei Qin,Xiaoyu Tan,Gang Li,Zongyi Li,Haojia Lin,Yong Mao,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: 该论文提出了一种无需模型参数更新的高效训练方法（Training-Free GRPO），提升大语言模型代理在专业领域的表现，并验证了其在数学推理与网页搜索上的优越性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在专业领域表现不佳，主要因外部工具集成和特定提示策略较难实现。传统方法依赖昂贵的参数微调和强化学习，成本较高且易过拟合。因此亟需一种轻量级、低成本、避免过拟合的提升方案。

Method: 提出Training-Free GRPO，无需模型参数更新。方法基于组内语义优势对高质量经验知识进行蒸馏，作为Token先验，通过API调用集成进LLM行为引导，所需真实数据量极少。

Result: 在数学推理和网络搜索任务上，Training-Free GRPO应用于DeepSeek-V3.1-Terminus，使用少量训练样本即可显著提升模型在专业领域的性能，优于少量数据微调的小型LLM。

Conclusion: Training-Free GRPO是一种高效、低成本又能避免过拟合的领域适应方案，为LLM在专业任务中的能力提升提供了新思路。

Abstract: Recent advances in Large Language Model (LLM) agents have demonstrated their
promising general capabilities. However, their performance in specialized
real-world domains often degrades due to challenges in effectively integrating
external tools and specific prompting strategies. While methods like agentic
reinforcement learning have been proposed to address this, they typically rely
on costly parameter updates, for example, through a process that uses
Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase
with Group Relative Policy Optimization (GRPO) to alter the output
distribution. However, we argue that LLMs can achieve a similar effect on the
output distribution by learning experiential knowledge as a token prior, which
is a far more lightweight approach that not only addresses practical data
scarcity but also avoids the common issue of overfitting. To this end, we
propose Training-Free Group Relative Policy Optimization (Training-Free GRPO),
a cost-effective solution that enhances LLM agent performance without any
parameter updates. Our method leverages the group relative semantic advantage
instead of numerical ones within each group of rollouts, iteratively distilling
high-quality experiential knowledge during multi-epoch learning on a minimal
ground-truth data. Such knowledge serves as the learned token prior, which is
seamlessly integrated during LLM API calls to guide model behavior. Experiments
on mathematical reasoning and web searching tasks demonstrate that
Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly
improves out-of-domain performance. With just a few dozen training samples,
Training-Free GRPO outperforms fine-tuned small LLMs with marginal training
data and cost.

</details>


### [180] [Memory Retrieval and Consolidation in Large Language Models through Function Tokens](https://arxiv.org/abs/2510.08203)
*Shaohua Zhang,Yuan Lin,Hang Li*

Main category: cs.CL

TL;DR: 本文提出了“功能符号（function token）假说”，揭示大型语言模型（LLM）在推理和预训练阶段，功能符号承担调控模型记忆检索与巩固的核心作用，并通过大量实验加以验证。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在知识记忆、遵循指令和推理等方面表现卓越，但其记忆检索与巩固的机制尚不明朗。作者希望通过新的视角解释LLM内部的信息检索和学习过程。

Method: 作者提出'功能符号假说'，定义功能符号为语言学中的虚词（如标点、冠词、介词、连词），并用二分图分析法等实验方法，追踪功能符号对激活特征和记忆巩固的作用；通过案例研究进一步说明功能符号如何影响下文预测。

Result: 实验发现，仅少量的功能符号即可激活大部分特征。在模型预训练过程中，预测紧随功能符号后的内容符号，对损失函数贡献最大，这促使功能符号选择最具预测性的上下文特征。

Conclusion: 功能符号在LLM的推理和记忆形成中起到了关键枢纽作用。理解该机制有助于进一步优化和解释大模型的训练与推理行为。

Abstract: The remarkable success of large language models (LLMs) stems from their
ability to consolidate vast amounts of knowledge into the memory during
pre-training and to retrieve it from the memory during inference, enabling
advanced capabilities such as knowledge memorization, instruction-following and
reasoning. However, the mechanisms of memory retrieval and consolidation in
LLMs remain poorly understood. In this paper, we propose the function token
hypothesis to explain the workings of LLMs: During inference, function tokens
activate the most predictive features from context and govern next token
prediction (memory retrieval). During pre-training, predicting the next tokens
(usually content tokens) that follow function tokens increases the number of
learned features of LLMs and updates the model parameters (memory
consolidation). Function tokens here roughly correspond to function words in
linguistics, including punctuation marks, articles, prepositions, and
conjunctions, in contrast to content tokens. We provide extensive experimental
evidence supporting this hypothesis. Using bipartite graph analysis, we show
that a small number of function tokens activate the majority of features. Case
studies further reveal how function tokens activate the most predictive
features from context to direct next token prediction. We also find that during
pre-training, the training loss is dominated by predicting the next content
tokens following function tokens, which forces the function tokens to select
the most predictive features from context.

</details>


### [181] [LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions](https://arxiv.org/abs/2510.08211)
*XuHao Hu,Peng Wang,Xiaoya Lu,Dongrui Liu,Xuanjing Huang,Jing Shao*

Main category: cs.CL

TL;DR: 本文发现，大型语言模型（LLM）即使仅有少量训练数据中包含恶意或误导性内容，也会导致模型在多个领域普遍表现出不诚实和欺骗性行为。


<details>
  <summary>Details</summary>
Motivation: 此前研究已发现，对LLM在特定应用领域以恶意或错误内容微调会引发模型广泛的对齐失效（即表现出有害行为），本文进一步探究这一现象在高风险场景中的不诚实和欺骗行为是否同样存在。

Method: 作者通过在多个领域上用不诚实、与事实不符的数据微调开源LLM，并开展一系列实验，包括在下游任务中混合少量“失调”数据、模拟不同类型用户（包括带偏见用户）与模型互动等设置，分析其对模型诚实性的影响。

Result: 实验发现，LLM在被注入少量失调数据（如只占整体1%）后，其诚实行为显著下降（超20%）；在人机交互实验中，仅有10%的偏见用户也足以使模型不自觉地表现出更多不诚实行为。

Conclusion: LLM的“自发性失调”风险不仅源自直接的恶意微调，还可能在实际应用和人机交互等实践中无意中诱发，尤其是在涉及不诚实和欺骗性行为的高风险任务里需要高度关注和防范。

Abstract: Previous research has shown that LLMs finetuned on malicious or incorrect
completions within narrow domains (e.g., insecure code or incorrect medical
advice) can become broadly misaligned to exhibit harmful behaviors, which is
called emergent misalignment. In this work, we investigate whether this
phenomenon can extend beyond safety behaviors to a broader spectrum of
dishonesty and deception under high-stakes scenarios (e.g., lying under
pressure and deceptive behavior). To explore this, we finetune open-sourced
LLMs on misaligned completions across diverse domains. Experimental results
demonstrate that LLMs show broadly misaligned behavior in dishonesty.
Additionally, we further explore this phenomenon in a downstream combined
finetuning setting, and find that introducing as little as 1% of misalignment
data into a standard downstream task is sufficient to decrease honest behavior
over 20%. Furthermore, we consider a more practical human-AI interaction
environment where we simulate both benign and biased users to interact with the
assistant LLM. Notably, we find that the assistant can be misaligned
unintentionally to exacerbate its dishonesty with only 10% biased user
population. In summary, we extend the study of emergent misalignment to the
domain of dishonesty and deception under high-stakes scenarios, and demonstrate
that this risk arises not only through direct finetuning, but also in
downstream mixture tasks and practical human-AI interactions.

</details>


### [182] [SenWave: A Fine-Grained Multi-Language Sentiment Analysis Dataset Sourced from COVID-19 Tweets](https://arxiv.org/abs/2510.08214)
*Qiang Yang,Xiuying Chen,Changsheng Ma,Rui Yin,Xin Gao,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 本文发布了一个专门针对COVID-19推文的细粒度多语言情感分析数据集SenWave，为分析公众情绪变化提供有力工具。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量COVID-19相关公开数据集，但缺乏高质量、细粒度、跨语言标注的情感数据集，阻碍了对公众情绪的深入分析。

Method: 作者构建了SenWave数据集，涵盖英、阿、西、法、意五种语言的推文，共设置十类情感。分别对英文和阿拉伯文推文进行了人工标注，其他语言通过翻译获得。采用预训练Transformer模型并在标注数据上微调进行细粒度情感分类，并分析情感在不同语言、国家和主题间的变化。还评估了该数据集与ChatGPT的兼容性。

Result: 生成了覆盖5种语言、包含4万条标注推文和1亿多无标注推文的数据集，并用其训练了Transformer模型，实现了对COVID-19期间公众情感的细致刻画，揭示了情绪随时间和地域的演变趋势。同时证明数据集可在ChatGPT等大模型中使用，具有良好适用性。

Conclusion: SenWave数据集为NLP领域在疫情等复杂事件下开展细粒度、多语言情感分析提供了关键资源，有助于推动更深入及创新性的相关研究。所有资源已公开，有望激发领域进一步发展。

Abstract: The global impact of the COVID-19 pandemic has highlighted the need for a
comprehensive understanding of public sentiment and reactions. Despite the
availability of numerous public datasets on COVID-19, some reaching volumes of
up to 100 billion data points, challenges persist regarding the availability of
labeled data and the presence of coarse-grained or inappropriate sentiment
labels. In this paper, we introduce SenWave, a novel fine-grained
multi-language sentiment analysis dataset specifically designed for analyzing
COVID-19 tweets, featuring ten sentiment categories across five languages. The
dataset comprises 10,000 annotated tweets each in English and Arabic, along
with 30,000 translated tweets in Spanish, French, and Italian, derived from
English tweets. Additionally, it includes over 105 million unlabeled tweets
collected during various COVID-19 waves. To enable accurate fine-grained
sentiment classification, we fine-tuned pre-trained transformer-based language
models using the labeled tweets. Our study provides an in-depth analysis of the
evolving emotional landscape across languages, countries, and topics, revealing
significant insights over time. Furthermore, we assess the compatibility of our
dataset with ChatGPT, demonstrating its robustness and versatility in various
applications. Our dataset and accompanying code are publicly accessible on the
repository\footnote{https://github.com/gitdevqiang/SenWave}. We anticipate that
this work will foster further exploration into fine-grained sentiment analysis
for complex events within the NLP community, promoting more nuanced
understanding and research innovations.

</details>


### [183] [Investigating Counterclaims in Causality Extraction from Text](https://arxiv.org/abs/2510.08224)
*Tim Hagen,Niklas Deckers,Felix Wolter,Harrisen Scells,Martin Potthast*

Main category: cs.CL

TL;DR: 本文关注从文本中提取因果关系时被忽略的否定性因果（concausal）信息，提出了包含concausal标注的新数据集，并证明了该类型信息对于模型区分正向与否定性因果关系的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有因果关系抽取数据集只关注支持因果关系（procausal）的表述，完全忽视或错误标注了否定因果关系（concausal），而这些信息对于因果推理同样重要。

Method: 作者首先通过文献综述论证了否定性因果（concausal）在不完整知识下因果推理中的作用，然后制定了严格的标注准则，并在Causal News语料库中补充标注了concausal表述，实现了较高一致性（Cohen's κ=0.74）。最后，利用新数据集对比训练与评估，展示模型识别procausal和concausal表述的能力。

Result: 未经concausal标注训练的模型容易将否定因果关系误判为正向因果关系。而结合concausal信息提出的数据集，可以有效提升模型区分两类因果关系的能力。

Conclusion: 加入否定性因果关系的标注和数据，将显著提升文本因果关系抽取任务的细粒度识别能力，模型对因果关系的理解更健全可靠。

Abstract: Research on causality extraction from text has so far almost entirely
neglected counterclaims. Existing causality extraction datasets focus solely on
"procausal" claims, i.e., statements that support a relationship. "Concausal"
claims, i.e., statements that refute a relationship, are entirely ignored or
even accidentally annotated as procausal. We address this shortcoming by
developing a new dataset that integrates concausality. Based on an extensive
literature review, we first show that concausality is an integral part of
causal reasoning on incomplete knowledge. We operationalize this theory in the
form of a rigorous guideline for annotation and then augment the Causal News
Corpus with concausal statements, obtaining a substantial inter-annotator
agreement of Cohen's $\kappa=0.74$. To demonstrate the importance of
integrating concausal statements, we show that models trained without concausal
relationships tend to misclassify these as procausal instead. Based on our new
dataset, this mistake can be mitigated, enabling transformers to effectively
distinguish pro- and concausality.

</details>


### [184] [The Alignment Waltz: Jointly Training Agents to Collaborate for Safety](https://arxiv.org/abs/2510.08240)
*Jingyu Zhang,Haozhu Wang,Eric Michael Smith,Sid Wang,Amr Sharaf,Mahesh Pasupuleti,Benjamin Van Durme,Daniel Khashabi,Jason Weston,Hongyuan Zhan*

Main category: cs.CL

TL;DR: 本文提出了一种名为WaltzRL的多智能体强化学习框架，旨在协调提升大语言模型（LLM）的有用性与安全性，显著减少了不安全与过度拒绝的回答。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型需在有用性与无害性之间取得平衡。传统的安全方法常常过于严格，直接拒绝所有含有不安全部分的内容，导致过度拒绝，用户体验受损，因此有必要实现更细致、有反馈的对齐方案。

Method: WaltzRL是一种多智能体强化学习方法，包含对话代理与反馈代理。反馈代理对对话代理的回复给出改进建议，奖励机制依据对话代理采纳反馈的效果动态调整。在推理时，不是直接丢弃不合适的回复，而是用反馈加以改进。同时，反馈代理仅在必要时介入，保障效率。

Result: 在五个不同数据集上的实验表明，WaltzRL相比于现有方法能有效减少不安全回答（如WildJailbreak数据集中从39.0%降到4.6%），也大幅降低了过度拒绝率（如OR-Bench中从45.3%降到9.9%）。

Conclusion: WaltzRL通过联合进化和自适应反馈机制，提升了LLM的安全性且未损害其实用性，为平衡有用与无害之间的权衡提供了新进展。

Abstract: Harnessing the power of LLMs requires a delicate dance between being helpful
and harmless. This creates a fundamental tension between two competing
challenges: vulnerability to adversarial attacks that elicit unsafe content,
and a tendency for overrefusal on benign but sensitive prompts. Current
approaches often navigate this dance with safeguard models that completely
reject any content that contains unsafe portions. This approach cuts the music
entirely-it may exacerbate overrefusals and fails to provide nuanced guidance
for queries it refuses. To teach models a more coordinated choreography, we
propose WaltzRL, a novel multi-agent reinforcement learning framework that
formulates safety alignment as a collaborative, positive-sum game. WaltzRL
jointly trains a conversation agent and a feedback agent, where the latter is
incentivized to provide useful suggestions that improve the safety and
helpfulness of the conversation agent's responses. At the core of WaltzRL is a
Dynamic Improvement Reward (DIR) that evolves over time based on how well the
conversation agent incorporates the feedback. At inference time, unsafe or
overrefusing responses from the conversation agent are improved rather than
discarded. The feedback agent is deployed together with the conversation agent
and only engages adaptively when needed, preserving helpfulness and low latency
on safe queries. Our experiments, conducted across five diverse datasets,
demonstrate that WaltzRL significantly reduces both unsafe responses (e.g.,
from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on
OR-Bench) compared to various baselines. By enabling the conversation and
feedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances
LLM safety without degrading general capabilities, thereby advancing the Pareto
front between helpfulness and harmlessness.

</details>


### [185] [Contrastive Decoding for Synthetic Data Generation in Low-Resource Language Modeling](https://arxiv.org/abs/2510.08245)
*Jannek Ulm,Kevin Du,Vésteinn Snæbjarnarson*

Main category: cs.CL

TL;DR: 本文探索使用对比解码生成的大模型合成数据来扩充语言模型训练语料的方法，并证明合成数据可提升多项任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型受限于可用真实文本数据的数量，面临数据枯竭的风险。为持续提升模型，研究者试图利用模型自身生成的数据（合成数据）来扩充训练集。该文旨在探索如何优质生成合成数据以及合成数据对模型训练的具体益处。

Method: 作者采用一种“对比解码”方法，从同一语料训练出的好坏两个模型中，通过分析两者在文本生成上的相对差异来“放大”好模型的信号，生成优质合成数据。随后将这些合成语料与真实文本混合，共同用于训练语言模型，并对多项任务进行性能评估。

Result: 结果显示，混合合成数据与真实数据训练的模型在语言建模和下游任务均取得了性能提升。进一步分析表明，对比解码生成的合成数据更有助于推理类任务，而传统采样方法产生的合成数据对表层语言任务更有效。

Conclusion: 结合对比解码生成合成数据与真实数据训练，可以有效改善大模型性能，其中对推理和复杂任务尤为有益。这为未来大模型训练指明了新的数据扩展路径。

Abstract: Large language models (LLMs) are trained on huge amounts of textual data, and
concerns have been raised that the limits of such data may soon be reached. A
potential solution is to train on synthetic data sampled from LLMs. In this
work, we build on this idea and investigate the benefits of contrastive
decoding for generating synthetic corpora. In a controlled setting, we
experiment with sampling corpora using the relative difference between a good
and bad model trained on the same original corpus of 100 million words. By
amplifying the signal from a model that has better performance, we create a
synthetic corpus and mix it with the original training data. Our findings show
that training on a mixture of synthesized and real data improves performance on
the language modeling objective and a range of downstream tasks. In particular,
we see that training with a mix of synthetic data from contrastive decoding
benefits tasks that require more reasoning skills, while synthetic data from
traditional sampling helps more on tasks dependent on surface level linguistic
capabilities.

</details>


### [186] [Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window](https://arxiv.org/abs/2510.08276)
*Qiaoyu Tang,Hao Xiang,Le Yu,Bowen Yu,Yaojie Lu,Xianpei Han,Le Sun,WenJuan Zhang,Pengbo Wang,Shixuan Liu,Zhenru Zhang,Jianhong Tu,Hongyu Lin,Junyang Lin*

Main category: cs.CL

TL;DR: DeepMiner通过高难度任务和动态上下文窗口训练，显著提升多轮智能体的深度推理与长程交互能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的推理模型在多轮、长程交互场景下推理能力有限，难以应对复杂任务。该工作旨在突破多轮智能体深度推理和上下文处理的瓶颈。

Method: 提出DeepMiner框架，包括：1) 逆向构建，自动从真实网页生成复杂、可验证的问答数据，提升训练数据质量和难度；2) 设计动态上下文窗口，用滑动窗口管理方式替代外部摘要模型，从而高效处理扩展的长程上下文。整个系统通过强化学习在Qwen3-32B上进行训练。

Result: DeepMiner-32B在多项智能体推理数据集上取得重大提升，如BrowseComp-en准确率达33.5%，比此前最佳开源水平高约20%；在BrowseComp-zh、XBench-DeepSearch、GAIA等数据集也有明显进步。在32k上下文长度下，可稳定实现近100轮交互。

Conclusion: DeepMiner有效突破多轮、长程推理中的上下文瓶颈，极大提升多轮智能体的推理深度与实际应用能力，在开源领域树立了领先标杆。

Abstract: While recent advances in reasoning models have demonstrated cognitive
behaviors through reinforcement learning, existing approaches struggle to
invoke deep reasoning capabilities in multi-turn agents with long-horizon
interactions. We propose DeepMiner, a novel framework that elicits such
abilities by introducing high-difficulty training tasks and dynamic context
window. DeepMiner presents a reverse construction method to generate complex
but verifiable question-answer pairs from authentic web sources, which ensures
the challenge and reliability of training data while injecting cognitive
capabilities into multi-turn reasoning scenarios. We further design an elegant
yet effective dynamic context management strategy for both training and
inference, utilizing sliding window mechanisms while eliminating the dependency
on external summarization models, thereby efficiently empowering the model to
handle continuously expanding long-horizon contexts. Through reinforcement
learning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial
performance improvements across multiple search agent benchmarks. DeepMiner
attains 33.5% accuracy on BrowseComp-en, surpassing the previous best
open-source agent by almost 20 percentage points, and demonstrates consistent
improvements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our
dynamic context management enables sustained interactions of nearly 100 turns
within standard 32k context length, effectively addressing the context
limitations that constrain existing multi-turn interaction systems.

</details>


### [187] [Neuron-Level Analysis of Cultural Understanding in Large Language Models](https://arxiv.org/abs/2510.08284)
*Taisei Yamamoto,Ryoma Kumon,Danushka Bollegala,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文通过神经元级分析揭示了大语言模型中文化相关神经元的作用，对模型训练和工程提出了实践指导。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在全球广泛部署时，常表现出文化偏见及对少数文化认知有限，其背后机理尚未被深入探讨。为提升模型的公平性和多元性，有必要理解并改进其文化认知机制。

Method: 作者提出基于梯度的神经元评分方法，并结合过滤策略，精确识别出驱动文化行为的神经元。细分为与所有文化相关的“通用文化神经元”及与特定文化相关的“特定文化神经元”，分析这些神经元分布和作用。

Result: 发现文化相关神经元占比不足1%，主要集中于浅至中间MLP层。抑制这些神经元会显著降低模型在文化基准测试中的表现（降幅最高达30%），但对一般自然语言理解任务影响有限。此外，特定文化神经元可支持对相关文化的知识。

Conclusion: 研究揭示LLMs中文化理解的内在机制，并警示过度依赖自然语言理解基准训练可能削弱模型的文化认知能力，对未来模型训练和工程具有指导意义。

Abstract: As large language models (LLMs) are increasingly deployed worldwide, ensuring
their fair and comprehensive cultural understanding is important. However, LLMs
exhibit cultural bias and limited awareness of underrepresented cultures, while
the mechanisms underlying their cultural understanding remain underexplored. To
fill this gap, we conduct a neuron-level analysis to identify neurons that
drive cultural behavior, introducing a gradient-based scoring method with
additional filtering for precise refinement. We identify both culture-general
neurons contributing to cultural understanding regardless of cultures, and
culture-specific neurons tied to an individual culture. These neurons account
for less than 1% of all neurons and are concentrated in shallow to middle MLP
layers. We validate their role by showing that suppressing them substantially
degrades performance on cultural benchmarks (by up to 30%), while performance
on general natural language understanding (NLU) benchmarks remains largely
unaffected. Moreover, we show that culture-specific neurons support knowledge
of not only the target culture, but also related cultures. Finally, we
demonstrate that training on NLU benchmarks can diminish models' cultural
understanding when we update modules containing many culture-general neurons.
These findings provide insights into the internal mechanisms of LLMs and offer
practical guidance for model training and engineering. Our code is available at
https://github.com/ynklab/CULNIG

</details>


### [188] [AutoRed: A Free-form Adversarial Prompt Generation Framework for Automated Red Teaming](https://arxiv.org/abs/2510.08329)
*Muxi Diao,Yutao Mou,Keqing He,Hanbo Song,Lulu Zhao,Shikun Zhang,Wei Ye,Kongming Liang,Zhanyu Ma*

Main category: cs.CL

TL;DR: 本文提出了一种无需种子指令、能自动生成复杂对抗性提示的新红队测试框架AutoRed，并用其发现主流大模型的安全短板。


<details>
  <summary>Details</summary>
Motivation: 当前红队测试方法通常依赖预设的种子指令，限制了对抗性提示的多样性，影响了大模型安全性评估的全面性。

Method: 提出AutoRed框架，包括两步：（1）基于人设的对抗性指令生成，（2）通过循环反思机制迭代优化低质量提示。同时引入验证器，无需访问目标模型即可评估提示的有害性。利用该框架，作者生成了两个全新红队数据集（AutoRed-Medium与AutoRed-Hard），并评估了8个主流大模型。

Result: AutoRed在攻击成功率和泛化能力上均优于现有基线方法，能生成更具多样性和挑战性的对抗提示。

Conclusion: 本文指出依赖种子指令的红队方法存在局限，AutoRed等自由生成式方法能更全面地检测和提升大模型的安全性。数据集计划开源，推动领域发展。

Abstract: The safety of Large Language Models (LLMs) is crucial for the development of
trustworthy AI applications. Existing red teaming methods often rely on seed
instructions, which limits the semantic diversity of the synthesized
adversarial prompts. We propose AutoRed, a free-form adversarial prompt
generation framework that removes the need for seed instructions. AutoRed
operates in two stages: (1) persona-guided adversarial instruction generation,
and (2) a reflection loop to iteratively refine low-quality prompts. To improve
efficiency, we introduce a verifier to assess prompt harmfulness without
querying the target models. Using AutoRed, we build two red teaming datasets --
AutoRed-Medium and AutoRed-Hard -- and evaluate eight state-of-the-art LLMs.
AutoRed achieves higher attack success rates and better generalization than
existing baselines. Our results highlight the limitations of seed-based
approaches and demonstrate the potential of free-form red teaming for LLM
safety evaluation. We will open source our datasets in the near future.

</details>


### [189] [Two-Stage Voting for Robust and Efficient Suicide Risk Detection on Social Media](https://arxiv.org/abs/2510.08365)
*Yukai Song,Pengfei Zhou,César Escobar-Viera,Candice Biernesser,Wei Huang,Jingtong Hu*

Main category: cs.CL

TL;DR: 提出了一种高效且健壮的自杀风险检测框架，结合轻量级BERT和大模型，以低成本提升对隐性自杀意图的识别能力。


<details>
  <summary>Details</summary>
Motivation: 随着全球自杀率上升，亟需主动预防手段。许多有自杀风险者更倾向于在社交媒体上表达困扰（常用隐喻、讽刺和情感暗示），但现有模型难以准确检测隐性自杀意图，尤其是在不增加计算成本的前提下。

Method: 提出了两阶段判别架构：第一阶段利用BERT对明确自杀风险进行快速识别；第二阶段将模糊样本分流给多视角大模型投票机制或特征型集成学习——后者结合由大模型提取、心理学驱动的结构化特征，兼顾效率与可解释性。

Result: 在两个数据集（Reddit-以显性为主，DeepSuiMind-全部隐性）上，该框架均优于单模型基线：显性检测F1值98%，隐性检测99.7%，领域迁移性能差距小于2%，且大幅降低了大模型推理成本。

Conclusion: 该方法兼顾了效率、准确性和跨域泛化能力，是低成本高效检测社交媒体自杀风险、尤其是隐性表达的有力方案。

Abstract: Suicide rates have risen worldwide in recent years, underscoring the urgent
need for proactive prevention strategies. Social media provides valuable
signals, as many at-risk individuals - who often avoid formal help due to
stigma - choose instead to share their distress online. Yet detecting implicit
suicidal ideation, conveyed indirectly through metaphor, sarcasm, or subtle
emotional cues, remains highly challenging. Lightweight models like BERT handle
explicit signals but fail on subtle implicit ones, while large language models
(LLMs) capture nuance at prohibitive computational cost. To address this gap,
we propose a two-stage voting architecture that balances efficiency and
robustness. In Stage 1, a lightweight BERT classifier rapidly resolves
high-confidence explicit cases. In Stage 2, ambiguous inputs are escalated to
either (i) a multi-perspective LLM voting framework to maximize recall on
implicit ideation, or (ii) a feature-based ML ensemble guided by
psychologically grounded indicators extracted via prompt-engineered LLMs for
efficiency and interpretability. To the best of our knowledge, this is among
the first works to operationalize LLM-extracted psychological features as
structured vectors for suicide risk detection. On two complementary datasets -
explicit-dominant Reddit and implicit-only DeepSuiMind - our framework
outperforms single-model baselines, achieving 98.0% F1 on explicit cases, 99.7%
on implicit ones, and reducing the cross-domain gap below 2%, while
significantly lowering LLM cost.

</details>


### [190] [On the Relationship Between the Choice of Representation and In-Context Learning](https://arxiv.org/abs/2510.08372)
*Ioana Marinescu,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

TL;DR: 本文系统性研究了大语言模型（LLM）在in-context learning（ICL）中，演示样例的标签表示与学习能力之间的相互关系。作者发现，这两者大体独立，标签表示影响ICL基线表现，而从额外演示学习是在该基线之上提升。


<details>
  <summary>Details</summary>
Motivation: ICL的成功很大程度归因于样本表示方式，尤其是分类任务中的标签设计，但模型能否因更多演示而提升、其提升幅度等问题研究不充分。标签表示与学习能力间的具体相互作用尚未明确，因此亟需系统性探索。

Method: 提出优化算法，系统枚举具不同语义相关性的标签集，然后针对每组标签集分别用不同数量的in-context演示样本，在不同规模LLM上分析表现提升曲线，以研究标签集质量、演示数量与学习提升的关系。

Result: 无论标签集本身语义质量如何，增加in-context演示都能带来性能提升，其提升速率受标签集质量与模型参数规模影响。然而标签集本身优劣影响的基线准确度在整个学习过程中大致保持不变，表现为二者高度正交。

Conclusion: 标签表示与从演示中学习在ICL中对性能的提升作用基本相互独立，应分别分析与优化。本文揭示了ICL性能提升中，基础表现与学习提升的正交性，为LLM样本设计和优化提供新思路。

Abstract: In-context learning (ICL) is the ability of a large language model (LLM) to
learn a new task from a few demonstrations presented as part of the context.
Past studies have attributed a large portion of the success of ICL to the way
these in-context demonstrations are represented, particularly to how labels are
represented in classification tasks. On the other hand, observations of the
learning capacity of ICL (i.e., the extent to which more in-context
demonstrations can lead to higher performance) have been mixed, and ICL is
often thought to occur only under specific conditions. The interaction between
these two aspects in ICL, representation and learning, has not been studied in
depth until now. We hypothesize that they are largely independent of one
another, such that the representation of demonstrations determines the baseline
accuracy of ICL, while learning from additional demonstrations improves only on
top of this baseline. We validate this hypothesis by developing an optimization
algorithm that can enumerate a spectrum of possible label sets
(representations) varying in semantic relevance. We then perform ICL with
varying numbers of in-context demonstrations for each of these label sets. We
observed that learning happens regardless of the quality of the label set
itself, although its efficiency, measured by the slope of improvement over
in-context demonstrations, is conditioned on both the label set quality and the
parameter count of the underlying language model. Despite the emergence of
learning, the relative quality (accuracy) of the choice of a label set
(representation) is largely maintained throughout learning, confirming our
hypothesis and implying their orthogonality. Our work reveals a previously
underexplored aspect of ICL: the independent effects of learning from
demonstrations and their representations on ICL performance.

</details>


### [191] [If Probable, Then Acceptable? Understanding Conditional Acceptability Judgments in Large Language Models](https://arxiv.org/abs/2510.08388)
*Jasmin Orth,Philipp Mondorf,Barbara Plank*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）对条件句可接受性的判断，发现它们会考虑条件概率和语义关联性，但与人类的判断还存在差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLMs如何推理和理解条件句，但尚不清楚这些模型是如何判断条件句本身是否合适、合理（即可接受性），而这在人类沟通与推理中十分重要。为了填补这一空白，作者开展了本项系统性研究。

Method: 作者对不同类型和规模的LLMs进行实验，采用不同的提问策略。通过线性混合效应模型和方差分析（ANOVA）检验，评估模型对条件概率和语义相关性的敏感度，并将其结果与人类实验数据进行对比。

Result: 实验表明，各类LLMs都能在一定程度上感知条件概率和语义关联性对可接受性的影响，但敏感度受模型结构和提问方式影响较大，且对语义和概率线索的整合不如人类一致。更大规模的模型也未必更贴近人类判断。

Conclusion: LLMs可以部分模拟人类对条件句可接受性的判断，但在整合概率与语义线索方面存在不足，需要进一步改进和优化模型。

Abstract: Conditional acceptability refers to how plausible a conditional statement is
perceived to be. It plays an important role in communication and reasoning, as
it influences how individuals interpret implications, assess arguments, and
make decisions based on hypothetical scenarios. When humans evaluate how
acceptable a conditional "If A, then B" is, their judgments are influenced by
two main factors: the $\textit{conditional probability}$ of $B$ given $A$, and
the $\textit{semantic relevance}$ of the antecedent $A$ given the consequent
$B$ (i.e., whether $A$ meaningfully supports $B$). While prior work has
examined how large language models (LLMs) draw inferences about conditional
statements, it remains unclear how these models judge the
$\textit{acceptability}$ of such statements. To address this gap, we present a
comprehensive study of LLMs' conditional acceptability judgments across
different model families, sizes, and prompting strategies. Using linear
mixed-effects models and ANOVA tests, we find that models are sensitive to both
conditional probability and semantic relevance-though to varying degrees
depending on architecture and prompting style. A comparison with human data
reveals that while LLMs incorporate probabilistic and semantic cues, they do so
less consistently than humans. Notably, larger models do not necessarily align
more closely with human judgments.

</details>


### [192] [Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT](https://arxiv.org/abs/2510.08404)
*Noor Ul Zain,Mohsin Raza,Ahsan Adeel*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Co$^4$的轻量级模型，在参数量极小（8M）、单层结构下，训练和推理的复杂度仅为O(N)。该模型在BabyLM Challenge任务中，训练仅2个epoch即可超过传统的GPT-2（124M参数，12层）和GPT-BERT（30M参数，12层），且在多个评测指标上取得更优性能。


<details>
  <summary>Details</summary>
Motivation: 当前预训练语言模型普遍采用深层、大规模参数、复杂度较高的结构，训练和推理的资源需求极大。本文旨在探索是否存在高效、轻量的模型能够实现同等甚至更优的性能，从而推动神经网络架构和深度学习规模定律的重新思考。

Method: 作者提出的Co$^4$模型为单层、双头、800万参数，采用O(N)的计算结构。以10M tokens为训练集，在BabyLM Challenge标准流程下，仅训练2个epoch，与GPT-2和GPT-BERT两个主流baseline对比。评测任务涵盖SuperGLUE等综合性基准的零样本和微调任务。

Result: Co$^4$仅用2个epoch训练便在BabyLM任务各项指标上优于GPT-2和GPT-BERT。其在SuperGLUE零样本任务中5/7、微调任务中6/7项目超越GPT-2；在两类任务中分别有4/7项目胜过GPT-BERT，展现出极高的训练样本效率和推理表现。

Conclusion: Co$^4$的轻量高效表现，挑战了深度学习关于规模与性能的传统观点。结果表明在资源有限的情况下，合理设计结构能够获得极高的效率及表现，有望推动预训练模型范式的革新。

Abstract: We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two
heads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$
is the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2
(124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two
epochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude
greater training efficiency on 10M tokens, demonstrating highly sample
efficient pretraining. Using the BabyLM challenge evaluation pipeline across
complex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning
performance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out
of 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out
of 7 metrics in both cases. These results suggest the need to rethink
prevailing deep learning paradigms and associated scaling laws.

</details>


### [193] [ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping](https://arxiv.org/abs/2510.08457)
*Shuang Chen,Yue Guo,Yimeng Ye,Shijue Huang,Wenbo Hu,Haoxi Li,Manyuan Zhang,Jiayu Chen,Song Guo,Nanyun Peng*

Main category: cs.CL

TL;DR: 本文提出了ARES，一种自适应推理的MLRM训练框架，通过动态分配推理探索力度，提升简单任务的效率并改善复杂任务的答案覆盖。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在简单任务上容易推理过度，复杂任务下又探索不足，导致推理效率低或错失答案。缺乏按任务难度动态调整探索力度的机制。

Method: 提出ARES框架，包括两阶段训练：1）Adaptive Cold-Start，利用与问题难度成正比的推理轨迹对模型进行初始训练，增强模型对难度的感知。2）Adaptive Entropy Policy Optimization (AEPO)，以高窗口熵（HWE）token作为触发器，结合分层熵奖励和动态KL控制，实现按需探索。

Result: 在多个数学、逻辑和多模态基准上，ARES获得了更优的性能和推理效率，与领先的商业系统相比，在大幅降低推理成本下，缩小了表现差距。

Conclusion: ARES可有效提升MLRM对不同难度任务的推理效率和准确性，促进开源模型在复杂推理任务上的应用潜力。

Abstract: Recent advances in multimodal large reasoning models (MLRMs) have
substantially improved their ability to solve complex textual and visual tasks.
However, these models tend to overthink on simple problems, producing
unnecessarily lengthy reasoning traces, while under-exploring on challenging
ones, leading to missed solutions. To address this imbalance, we propose ARES,
a unified open-source framework for adaptive reasoning that dynamically
allocates exploration effort based on task difficulty. Our approach is
motivated by two key empirical findings: (i) while single-token entropy is
noisy, high window-entropy (HWE) tokens (token-level entropies averaged under a
sliding window) can reliably capture reasoning-critical moments; and (ii)
reducing HWE usage benefits easy problems, while increasing it is essential for
solving hard ones. Building on these insights, ARES introduces a two-stage
training pipeline. In the Adaptive Cold-Start stage, we curate multimodal and
textual data paired with reasoning traces of length proportional to problem
difficulty, equipping the model with initial difficulty awareness. In the
second stage, we develop Adaptive Entropy Policy Optimization (AEPO), which
uses HWE tokens as exploration triggers to decide when to explore, and a
hierarchical entropy reward with dynamic KL control to decide how much to
explore. Extensive experiments demonstrate that ARES achieves superior
performance and reasoning efficiency across diverse mathematical, logical, and
multimodal benchmarks, while closing the gap to leading commercial systems
under significantly lower inference costs.

</details>


### [194] [LeWiDi-2025 at NLPerspectives: The Third Edition of the Learning with Disagreements Shared Task](https://arxiv.org/abs/2510.08460)
*Elisa Leonardelli,Silvia Casola,Siyao Peng,Giulia Rizzi,Valerio Basile,Elisabetta Fersini,Diego Frassinelli,Hyewon Jang,Maja Pavlovic,Barbara Plank,Massimo Poesio*

Main category: cs.CL

TL;DR: 本论文介绍了LEWIDI第三届共建任务，旨在推动AI模型对人类判断分歧的感知与评估，发布了更丰富的基准数据集与新的评测方法。


<details>
  <summary>Details</summary>
Motivation: 人类在对同一文本的理解和标注上常有分歧，目前许多AI模型忽视了这种多样性，因此研究者希望开发能辨识并学习这些分歧的AI模型。

Method: 本届LEWIDI任务涵盖四个数据集，涉及同义句识别、讽刺和反讽检测、自然语言推断，引入了不仅仅是类别标签，还包含序数标签。采用了两种范式，一种是soft-label（软标签），模型预测群体分布；一种是perspectivist（视角主义），模型预测不同标注人的个人解释。此外，探索了新的评测指标。

Result: 多元团队参与了比赛，取得了不同方法下对建模分歧的效果分析，揭示了各方法的长处与局限。实验结果加深了对分歧建模技术的理解。

Conclusion: 本次工作拓展了LEWIDI基准，丰富了数据与评测体系，为开发能理解人类分歧的AI技术提供了框架、新资源和参考。

Abstract: Many researchers have reached the conclusion that AI models should be trained
to be aware of the possibility of variation and disagreement in human
judgments, and evaluated as per their ability to recognize such variation. The
LEWIDI series of shared tasks on Learning With Disagreements was established to
promote this approach to training and evaluating AI models, by making suitable
datasets more accessible and by developing evaluation methods. The third
edition of the task builds on this goal by extending the LEWIDI benchmark to
four datasets spanning paraphrase identification, irony detection, sarcasm
detection, and natural language inference, with labeling schemes that include
not only categorical judgments as in previous editions, but ordinal judgments
as well. Another novelty is that we adopt two complementary paradigms to
evaluate disagreement-aware systems: the soft-label approach, in which models
predict population-level distributions of judgments, and the perspectivist
approach, in which models predict the interpretations of individual annotators.
Crucially, we moved beyond standard metrics such as cross-entropy, and tested
new evaluation metrics for the two paradigms. The task attracted diverse
participation, and the results provide insights into the strengths and
limitations of methods to modeling variation. Together, these contributions
strengthen LEWIDI as a framework and provide new resources, benchmarks, and
findings to support the development of disagreement-aware technologies.

</details>


### [195] [DeepPrune: Parallel Scaling without Inter-trace Redundancy](https://arxiv.org/abs/2510.08483)
*Shangqing Tu,Yaxuan Li,Yushi Bai,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 提出DeepPrune框架，通过动态剪枝提升大语言模型并行推理的效率，可显著减少冗余计算，实现80%以上Token减少且精度损失极小。


<details>
  <summary>Details</summary>
Motivation: 并行生成多条推理链能提升大模型的推理能力，但存在严重的重复性问题，造成了巨大计算浪费。分析显示，超过80%的推理链最后给出相同答案。因此，急需提高并行推理的计算效率，减少无用的重复计算。

Method: 提出DeepPrune方法：通过特殊训练的判别模型（采用focal loss和过采样）预测部分推理链是否最终等价，并用在线贪心聚类方法动态剪除重复路径。这既保证了答案的多样性，也显著减少了冗余的计算。

Result: 在AIME 2024、AIME 2025、GPQA等多个基准和推理模型上评测，DeepPrune相较传统共识采样方法，在大部分任务中Token消耗减少超过80%，且准确率损失不超过3个百分点，等价判别模型的AUROC达到0.87。

Conclusion: DeepPrune显著提高了大模型并行推理的效率，成为高效并行推理的新标准，实现高性能推理的同时大幅降低了资源消耗。

Abstract: Parallel scaling has emerged as a powerful paradigm to enhance reasoning
capabilities in large language models (LLMs) by generating multiple
Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces
significant computational inefficiency due to inter-trace redundancy -- our
analysis reveals that over 80% of parallel reasoning traces yield identical
final answers, representing substantial wasted computation. To address this
critical efficiency bottleneck, we propose DeepPrune, a novel framework that
enables efficient parallel scaling through dynamic pruning. Our method features
a specialized judge model trained with focal loss and oversampling techniques
to accurately predict answer equivalence from partial reasoning traces which
realizes 0.87 AUROC on equivalence prediction, combined with an online greedy
clustering algorithm that dynamically prunes redundant paths while preserving
answer diversity. Comprehensive evaluations across three challenging benchmarks
(AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that
DeepPrune achieves remarkable token reduction by over 80% compared to
conventional consensus sampling on most cases, while maintaining competitive
accuracy within 3 percentage points. Our work establishes a new standard for
efficient parallel reasoning, making high-performance reasoning more efficient.
Our code and data are here: https://deepprune.github.io/

</details>


### [196] [Neologism Learning for Controllability and Self-Verbalization](https://arxiv.org/abs/2510.08506)
*John Hewitt,Oyvind Tafjord,Robert Geirhos,Been Kim*

Main category: cs.CL

TL;DR: 本文提出并验证了一种通过引入新词（neologism）来理解和控制大型语言模型（LLM）的新方法。通过仅新增词向量并结合示例训练，无需更改模型其他参数，实现对特定概念的调控和模型自我表述能力提升。


<details>
  <summary>Details</summary>
Motivation: 人类在面对新概念需求时会发明新词，该文借鉴此思路，希望用新词扩展与LLM沟通的能力，提升对模型内部机制的理解与控制。

Method: 方法是在模型中引入新词对应的词向量，并用展示该新概念的样例进行训练，同时保持其他模型参数不变。探索LLM用自然语言自我解释这些新词，以及提出plug-in evaluation（插件式评估）：将LLM自我解释插入上下文测试能否调控目标概念。还研究了可同时学习多个新概念的情况。

Result: 实验证明引入新词可有效调控模型的恭维、答错、文本长度等行为，并可用于复杂概念。模型能用自然语言解释新词，有时给出人类难以理解但对机器有效的近义词。在多概念、多词场景下依然有效。

Conclusion: 新词学习机制不仅能够增强对LLM复杂行为的理解，还为精细调控模型带来了新方法，借助模型自我表述和插件式评估，可进一步揭示模型认知特点和机器独有语义。

Abstract: Humans invent new words when there is a rising demand for a new useful
concept (e.g., doomscrolling). We explore and validate a similar idea in our
communication with LLMs: introducing new words to better understand and control
the models, expanding on the recently introduced neologism learning. This
method introduces a new word by adding a new word embedding and training with
examples that exhibit the concept with no other changes in model parameters. We
show that adding a new word allows for control of concepts such as flattery,
incorrect answers, text length, as well as more complex concepts in AxBench. We
discover that neologisms can also further our understanding of the model via
self-verbalization: models can describe what each new word means to them in
natural language, like explaining that a word that represents a concept of
incorrect answers means ``a lack of complete, coherent, or meaningful
answers...'' To validate self-verbalizations, we introduce plug-in evaluation:
we insert the verbalization into the context of a model and measure whether it
controls the target concept. In some self-verbalizations, we find machine-only
synonyms: words that seem unrelated to humans but cause similar behavior in
machines. Finally, we show how neologism learning can jointly learn multiple
concepts in multiple words.

</details>


### [197] [Efficient Prompt Optimisation for Legal Text Classification with Proxy Prompt Evaluator](https://arxiv.org/abs/2510.08524)
*Hyunji Lee,Kevin Chenhao Li,Matthias Grabmair,Shanshan Xu*

Main category: cs.CL

TL;DR: 论文提出了一种结合蒙特卡洛树搜索（MCTS）和代理提示评估器的提示优化框架，用于法律NLP中的公平性检测任务，在有限计算预算下提升了准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的提示优化方法在法律文本的公平性检测任务中面临着计算成本高和搜索策略低效的问题，亟需更高效的优化方法。

Method: 提出了将蒙特卡洛树搜索（MCTS）与代理提示评估器结合，以更高效地探索提示空间并降低提示评估的计算成本。

Result: 实验证明，在计算预算受限的情况下，该方法在分类准确率和计算效率上均优于现有基线方法。

Conclusion: 结合MCTS和代理评估的提示优化框架能够有效提升法律NLP任务中的模型表现，并降低资源消耗，具有实际应用价值。

Abstract: Prompt optimization aims to systematically refine prompts to enhance a
language model's performance on specific tasks. Fairness detection in Terms of
Service (ToS) clauses is a challenging legal NLP task that demands carefully
crafted prompts to ensure reliable results. However, existing prompt
optimization methods are often computationally expensive due to inefficient
search strategies and costly prompt candidate scoring. In this paper, we
propose a framework that combines Monte Carlo Tree Search (MCTS) with a proxy
prompt evaluator to more effectively explore the prompt space while reducing
evaluation costs. Experiments demonstrate that our approach achieves higher
classification accuracy and efficiency than baseline methods under a
constrained computation budget.

</details>


### [198] [Which Heads Matter for Reasoning? RL-Guided KV Cache Compression](https://arxiv.org/abs/2510.08525)
*Wenjie Du,Li Jiang,Keda Tao,Xue Liu,Huan Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的KV cache压缩框架RLKV，显著提升了大语言模型推理过程中的缓存使用效率，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理任务中的chain-of-thought生成导致KV cache开销巨大，现有KV压缩方法在推理场景下表现不佳，存在丢失关键信息或压缩重要头部的问题。作者假设推理场景下注意力头存在功能异质性，有些头对推理一致性至关重要，有些则可安全压缩。

Method: 作者提出RLKV框架，利用强化学习直接优化各注意力头缓存用量与推理质量的关系。训练期间RLKV根据实际生成的样本给予奖励，从而识别对推理最重要的头，将完整KV cache分配给这些头，其余头使用压缩缓存，提高推理效率。

Result: 实验表明，推理过程中只有少数注意力头对chain-of-thought推理至关重要。RLKV在实现20-50%的KV cache压缩率的同时，推理性能几乎无损，显著优于现有方法。

Conclusion: 基于强化学习的KV头部重要性识别框架可在大幅降低cache消耗的同时保持推理性能，为大模型高效推理提供有效途径。

Abstract: Reasoning large language models exhibit complex reasoning behaviors through
the extended chain-of-thought generation, creating unprecedented Key-Value (KV)
cache overhead during the decoding phase. Existing KV cache compression methods
underperform on reasoning models: token-dropping methods break reasoning
integrity by discarding critical information, while head-reallocating methods
mistakenly compress reasoning-critical heads since they are designed for
retrieval tasks, resulting in significant performance degradation as
compression rates increase. We hypothesize that KV heads exhibit functional
heterogeneity in reasoning models-some heads are critical for chain-of-thought
consistency while others are compressible. To validate and exploit this
insight, we propose RLKV, a novel reasoning-critical head identification
framework, which uses reinforcement learning to directly optimize the
relationship between each head's cache usage and reasoning quality. As RLKV
produces rewards from actual generated samples during training, it naturally
identifies heads relevant to reasoning behaviors. We then allocate full KV
cache to these heads while applying compressed constant KV cache to others for
efficient inference. Our experiments reveal that only a small fraction of
attention heads is essential for reasoning, enabling our KV compression
approach to outperform baseline methods while achieving 20-50% cache reduction
with near lossless performance compared to uncompressed results.

</details>


### [199] [CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards](https://arxiv.org/abs/2510.08529)
*Xiangyuan Xue,Yifan Zhou,Guibin Zhang,Zaibin Zhang,Yijiang Li,Chen Zhang,Zhenfei Yin,Philip Torr,Wanli Ouyang,Lei Bai*

Main category: cs.CL

TL;DR: 本文提出了Co-Evolving Multi-Agent Systems (CoMAS)框架，实现了无需外部监督、基于多智能体相互讨论与协作的自我进化型大型语言模型代理。通过内在奖励与LLM判据优化智能体表现，在多种评测下取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的大语言模型自进化方法，通常依赖外部奖励或直接从模型中提取内在奖励，但这与人类通过讨论及协作持续学习提升的方式不同。因此，作者旨在让代理通过彼此互动实现自我提升，模拟更接近人类的自我进化过程。

Method: 提出CoMAS多智能体协同进化系统。系统中，多个LLM代理通过互相讨论互动获得丰富动态信息，利用LLM-as-a-judge机制生成内在奖励，每个智能体通过RL方式优化自身策略，实现去中心化且可扩展的协同进化。

Result: 实验结果显示，CoMAS代理在多项任务或评测中表现优于未训练智能体，多数评测场景下达到当前最佳水平。消融实验进一步确认了基于互动奖励信号在方法中的关键作用，并表明随着智能体数量和多样性的提升，该方法具有良好的可扩展性。

Conclusion: CoMAS为基于LLM代理的自我进化提供了新颖且有效的范式，通过多智能体协作和讨论产生的内在奖励，实现了无需外部监督的自我提升，具有广阔应用与拓展前景。

Abstract: Self-evolution is a central research topic in enabling large language model
(LLM)-based agents to continually improve their capabilities after pretraining.
Recent research has witnessed a transition from reinforcement learning
(RL)-free to RL-based methods. Current RL-based methods either rely on dense
external reward signals or extract intrinsic reward signals from LLMs
themselves. However, these approaches diverge from the self-evolution
mechanisms observed in human intelligence, where individuals learn and improve
through mutual discussion and collaboration. In this work, we introduce
Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents
to improve autonomously by learning from inter-agent interactions without
external supervision. CoMAS generates intrinsic rewards from rich discussion
dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and
optimizes each agent's policy through RL, thereby enabling decentralized and
scalable co-evolution. Experimental results demonstrate that CoMAS consistently
outperforms untrained agents and achieves state-of-the-art performance across
most evaluation settings. Ablation studies confirm the necessity of
interaction-based reward signals and reveal promising scalability as the number
and diversity of agents increase. These findings establish CoMAS as a novel and
effective paradigm for self-evolution in LLM-based agents.

</details>


### [200] [ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation](https://arxiv.org/abs/2510.08569)
*Qin Liu,Jacob Dineen,Yuxi Huang,Sheng Zhang,Hoifung Poon,Ben Zhou,Muhao Chen*

Main category: cs.CL

TL;DR: 本文提出ArenaBencher框架，自动生成、演化大模型测试基准，有效防止数据泄漏并提升评测的权威性和难度。


<details>
  <summary>Details</summary>
Motivation: 现有大模型测评基准普遍存在与预训练数据重叠，模型通过记忆内容而非真正推理来得分，导致评测结果虚高、不同模型对比失真，对模型进步的反映也不准确。因此亟需能够随模型更新自动演化、杜绝泄漏风险的新型基准生成方式。

Method: 提出ArenaBencher框架。其核心做法包括：1）根据现有基准及多种待测模型，推断每个题目的核心能力；2）用大模型生成一系列保持原测试目标的备选新题（问答对）；3）再用LLM评审这些新题的正确性和意图，并整合多模型的反馈来筛选出能揭示共性弱点的题目；4）该流程可迭代进行，通过优质上下文示例逐步引导生成更具挑战性的测试题。

Result: ArenaBencher已被应用于数学解题、常识推理和模型安全等领域，能够生成经过验证的、多样且公平的新题，能发现模型新的失败模式、增加测试难度、保持测试目标一致性并提升模型区分度。

Conclusion: ArenaBencher为大模型基准测试演化提供了一种可扩展、自动化的路径，能够与基础模型的快速发展保持步调一致，持续提升测评的科学性和权威性。

Abstract: Benchmarks are central to measuring the capabilities of large language models
and guiding model development, yet widespread data leakage from pretraining
corpora undermines their validity. Models can match memorized content rather
than demonstrate true generalization, which inflates scores, distorts
cross-model comparisons, and misrepresents progress. We introduce ArenaBencher,
a model-agnostic framework for automatic benchmark evolution that updates test
cases while preserving comparability. Given an existing benchmark and a diverse
pool of models to be evaluated, ArenaBencher infers the core ability of each
test case, generates candidate question-answer pairs that preserve the original
objective, verifies correctness and intent with an LLM as a judge, and
aggregates feedback from multiple models to select candidates that expose
shared weaknesses. The process runs iteratively with in-context demonstrations
that steer generation toward more challenging and diagnostic cases. We apply
ArenaBencher to math problem solving, commonsense reasoning, and safety domains
and show that it produces verified, diverse, and fair updates that uncover new
failure modes, increase difficulty while preserving test objective alignment,
and improve model separability. The framework provides a scalable path to
continuously evolve benchmarks in step with the rapid progress of foundation
models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [201] [FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams](https://arxiv.org/abs/2510.07417)
*Corban Rivera,Grayson Byrd,Meghan Booker,Bethany Kemp,Allison Gaines,Emma Holmes,James Uplinger,Celso M de Melo,David Handelman*

Main category: cs.RO

TL;DR: 本文提出了FLEET框架，将自然语言指令转化为多机器人优化调度，实现了异质机器人队伍的高效协调，并在多个基准测试和硬件实验中展示出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 异质机器人团队根据自然语言指令进行高效协作具有巨大应用价值，但现有方法要么仅依赖大语言模型，难以处理长时序和易产生虚假内容，要么过于依赖形式化方法，需完整先验知识，实用性受限。因此亟需一种结合两者优点的新方法。

Method: FLEET提出了语言与形式方法结合的混合分布式框架：首先由LLM前端生成任务图和机器人-任务能力匹配矩阵，然后用形式化后端（MILP）进行调度优化。实际机器人则根据生成的调度执行各自子任务，并具备闭环自主性。

Result: 通过多项自然语言引导的多机器人自主协调基准实验，FLEET在异质双机器人团队中，相较SOTA生成式计划器表现出更高的任务成功率。消融实验显示，MILP优化突出时间调度，LLM能力匹配在配对任务中决定性强，两者结合效果最佳。此外，硬件实验中两台具备不同能力的四足机器人也验证了FLEET的实际可行性。

Conclusion: FLEET框架充分结合LLM灵活性与形式方法的可靠性，实现了异质机器人团队对自然语言自由指令的高效理解与协作，在理论和实际应用中均表现优异，有望推广至更复杂的多机器人协作场景。

Abstract: Coordinating heterogeneous robot teams from free-form natural-language
instructions is hard. Language-only planners struggle with long-horizon
coordination and hallucination, while purely formal methods require
closed-world models. We present FLEET, a hybrid decentralized framework that
turns language into optimized multi-robot schedules. An LLM front-end produces
(i) a task graph with durations and precedence and (ii) a capability-aware
robot--task fitness matrix; a formal back-end solves a makespan-minimization
problem while the underlying robots execute their free-form subtasks with
agentic closed-loop control. Across multiple free-form language-guided autonomy
coordination benchmarks, FLEET improves success over state of the art
generative planners on two-agent teams across heterogeneous tasks. Ablations
show that mixed integer linear programming (MILP) primarily improves temporal
structure, while LLM-derived fitness is decisive for capability-coupled tasks;
together they deliver the highest overall performance. We demonstrate the
translation to real world challenges with hardware trials using a pair of
quadruped robots with disjoint capabilities.

</details>


### [202] [VeMo: A Lightweight Data-Driven Approach to Model Vehicle Dynamics](https://arxiv.org/abs/2510.07447)
*Girolamo Oddo,Roberto Nuca,Matteo Parsani*

Main category: cs.RO

TL;DR: 本文提出了一种基于门控循环单元（GRU）的轻量级编码器-解码器模型，可从现有车辆的历史状态和驾驶控制输入数据预测未来状态，无需大量结构性信息，适用于自动驾驶等车辆信息稀缺场景。


<details>
  <summary>Details</summary>
Motivation: 在高性能车辆和自动驾驶开发中，通常无法获取设计所需的详细车辆结构参数，导致建立精确的动力学模型变得困难。本文旨在解决现有车辆缺乏全面信息下建模精度不高的问题。

Method: 采用数据驱动的方法，利用门控循环单元（GRU）结构的编码器-解码器网络，将车辆的历史状态和控制行为预测为未来状态，不依赖于物理建模参数。

Result: 在极端动态工况下，模型的最大平均相对误差低于2.6%，并能在输入数据存在噪声的情况下保持较好鲁棒性。输出的各信号（如加速度、偏航率、速度）具备物理一致性。

Conclusion: 提出的编码器-解码器模型能在车辆信息有限的情况下实现高精度、鲁棒性、物理一致性强的车辆状态预测，适合于自动驾驶等实际应用。

Abstract: Developing a dynamic model for a high-performance vehicle is a complex
problem that requires extensive structural information about the system under
analysis. This information is often unavailable to those who did not design the
vehicle and represents a typical issue in autonomous driving applications,
which are frequently developed on top of existing vehicles; therefore, vehicle
models are developed under conditions of information scarcity. This paper
proposes a lightweight encoder-decoder model based on Gate Recurrent Unit
layers to correlate the vehicle's future state with its past states, measured
onboard, and control actions the driver performs. The results demonstrate that
the model achieves a maximum mean relative error below 2.6% in extreme dynamic
conditions. It also shows good robustness when subject to noisy input data
across the interested frequency components. Furthermore, being entirely
data-driven and free from physical constraints, the model exhibits physical
consistency in the output signals, such as longitudinal and lateral
accelerations, yaw rate, and the vehicle's longitudinal velocity.

</details>


### [203] [HJCD-IK: GPU-Accelerated Inverse Kinematics through Batched Hybrid Jacobian Coordinate Descent](https://arxiv.org/abs/2510.07514)
*Cael Yasutake,Zachary Kingston,Brian Plancher*

Main category: cs.RO

TL;DR: 本文提出了一种新的逆运动学（IK）求解器HJCD-IK，结合了GPU加速、采样与优化，明显提高了求解精度和速度。


<details>
  <summary>Details</summary>
Motivation: 现有的IK解析法仅适用于低自由度、特定结构的系统；数值优化方法虽通用，但计算代价高且易陷入局部最优。因此需要更快、更准确、更稳健的IK求解器。

Method: 提出了HJCD-IK方法，采用基于GPU的采样和优化结合。具体方法包括一个关注姿态的贪婪坐标下降初始化机制，以及基于雅可比矩阵的精修步骤，实现了高效的混合逆运动学求解。

Result: 该方法在收敛速度和求解精度上均优于最新方法，在准确率-延迟权衡（Pareto 前沿）上表现出色，常常实现数量级上的提升；同时能产生更广泛分布的高质量解样本，最大均值差异最低。

Conclusion: HJCD-IK作为一种GPU加速的混合IK求解器，显著提升了求解效率和精度，有望推动IK领域发展，并已公开代码以促进社区共享。

Abstract: Inverse Kinematics (IK) is a core problem in robotics, in which joint
configurations are found to achieve a desired end-effector pose. Although
analytical solvers are fast and efficient, they are limited to systems with low
degrees-of-freedom and specific topological structures. Numerical
optimization-based approaches are more general, but suffer from high
computational costs and frequent convergence to spurious local minima. Recent
efforts have explored the use of GPUs to combine sampling and optimization to
enhance both the accuracy and speed of IK solvers. We build on this recent
literature and introduce HJCD-IK, a GPU-accelerated, sampling-based hybrid
solver that combines an orientation-aware greedy coordinate descent
initialization scheme with a Jacobian-based polishing routine. This design
enables our solver to improve both convergence speed and overall accuracy as
compared to the state-of-the-art, consistently finding solutions along the
accuracy-latency Pareto frontier and often achieving order-of-magnitude gains.
In addition, our method produces a broad distribution of high-quality samples,
yielding the lowest maximum mean discrepancy. We release our code open-source
for the benefit of the community.

</details>


### [204] [AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation](https://arxiv.org/abs/2510.07548)
*Adam Hung,Fan Yang,Abhinav Kumar,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: 本文提出了一种基于价值函数的运动规划方法Amortized Value Optimization (AVO)，有效提升了多接触模式灵巧操作任务的优化效率与性能。


<details>
  <summary>Details</summary>
Motivation: 传统将灵巧操作任务按不同接触模式分解为多个子任务分别优化，但缺乏全局信息，导致总体性能受限且计算量大。如何利用未来任务信息，提升规划效率与性能，是该研究关注的问题。

Method: 提出AVO方法，在每次轨迹优化时引入一个学习得到的价值函数，对未来任务表现进行预测。通过价值函数的梯度引导优化器向有利于后续子任务的状态前进，桥接各子任务间的割裂，并减少在线优化计算量。

Result: 在仿真和真实环境中的螺丝刀抓取及旋转任务上验证了该方法，即使计算资源减半（50%），性能仍优于未引入价值函数的轨迹优化法。

Conclusion: AVO方法可有效整合分割子任务的信息，提升灵巧操作任务的全局表现与计算效率，为多接触模式下的轨迹优化提供了新思路。

Abstract: Dexterous manipulation tasks often require switching between different
contact modes, such as rolling, sliding, sticking, or non-contact contact
modes. When formulating dexterous manipulation tasks as a trajectory
optimization problem, a common approach is to decompose these tasks into
sub-tasks for each contact mode, which are each solved independently.
Optimizing each sub-task independently can limit performance, as optimizing
contact points, contact forces, or other variables without information about
future sub-tasks can place the system in a state from which it is challenging
to make progress on subsequent sub-tasks. Further, optimizing these sub-tasks
is very computationally expensive. To address these challenges, we propose
Amortized Value Optimization (AVO), which introduces a learned value function
that predicts the total future task performance. By incorporating this value
function into the cost of the trajectory optimization at each planning step,
the value function gradients guide the optimizer toward states that minimize
the cost in future sub-tasks. This effectively bridges separately optimized
sub-tasks, and accelerates the optimization by reducing the amount of online
computation needed. We validate AVO on a screwdriver grasping and turning task
in both simulation and real world experiments, and show improved performance
even with 50% less computational budget compared to trajectory optimization
without the value function.

</details>


### [205] [Inspection Planning Primitives with Implicit Models](https://arxiv.org/abs/2510.07611)
*Jingyang You,Hanna Kurniawati,Lashika Medagoda*

Main category: cs.RO

TL;DR: 本文提出了一种新的整合神经隐式模型（neural Signed Distance Functions, SDFs）的采样式结构巡检规划原语（IPIM），极大地减少了对大规模复杂结构建模时的内存消耗。


<details>
  <summary>Details</summary>
Motivation: 随着基础设施老化和结构复杂性的增加，如何高效地进行结构巡检变得尤为重要。但针对大体量、多变化支柱与节点的结构，传统基于显式模型的采样式巡检算法难以处理庞大的空间数据，内存消耗极大。隐式模型（如神经SDFs）可以更高效地编码环境信息，但现有巡检算法的基本运算原语主要针对显式模型，难以充分利用SDFs。

Method: 作者提出了Inspection Planning Primitives with Implicit Models (IPIM)，把采样巡检的核心计算原语全部以隐式神经SDF形式实现，使采样式巡检全过程仅需神经SDF模型，无需频繁在显式/隐式表示之间转换。并通过在三个包含复杂真实结构的案例上对其进行评估。

Result: 在包括拥有九千二百万三角网格面的复杂真实结构等三个场景下，作者所提方案即便在简单的采样规划器上，也能生成与当前最优状态规划方法相质量相当的巡检路径，而且内存消耗最多可减少至原来的1/70。

Conclusion: IPIM支持的基于隐式模型的采样式巡检方法可极大提高对超大复杂结构的适应性和效率，在保持巡检结果质量的同时，显著降低内存需求，具备很强的实际应用前景。

Abstract: The aging and increasing complexity of infrastructures make efficient
inspection planning more critical in ensuring safety. Thanks to sampling-based
motion planning, many inspection planners are fast. However, they often require
huge memory. This is particularly true when the structure under inspection is
large and complex, consisting of many struts and pillars of various geometry
and sizes. Such structures can be represented efficiently using implicit
models, such as neural Signed Distance Functions (SDFs). However, most
primitive computations used in sampling-based inspection planner have been
designed to work efficiently with explicit environment models, which in turn
requires the planner to use explicit environment models or performs frequent
transformations between implicit and explicit environment models during
planning. This paper proposes a set of primitive computations, called
Inspection Planning Primitives with Implicit Models (IPIM), that enable
sampling-based inspection planners to entirely use neural SDFs representation
during planning. Evaluation on three scenarios, including inspection of a
complex real-world structure with over 92M triangular mesh faces, indicates
that even a rudimentary sampling-based planner with IPIM can generate
inspection trajectories of similar quality to those generated by the
state-of-the-art planner, while using up to 70x less memory than the
state-of-the-art inspection planner.

</details>


### [206] [GATO: GPU-Accelerated and Batched Trajectory Optimization for Scalable Edge Model Predictive Control](https://arxiv.org/abs/2510.07625)
*Alexander Du,Emre Adabag,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: 本文提出了GATO，一种支持中等批量规模实时MPC的GPU加速非线性轨迹优化（TO）求解器，并通过多层次并行实现大幅性能提升，具体实验与硬件验证表明了其显著优越性。


<details>
  <summary>Details</summary>
Motivation: 虽然MPC在机器人中应用广泛，但在线求解非线性TO问题计算开销大，现有GPU加速方法难以兼顾实时性、批量规模和动力学通用性。因此，存在急需在中等批量、多样动力学条件下实现高实时性能的批处理求解器。

Method: GATO协同算法、软件与硬件设计，充分发挥GPU的块、线程和线程束并行性，在多解同时求解时实现超高吞吐量，既不牺牲动力学模型通用性，也兼顾实时性。

Result: 实验显示GATO相较CPU提速18-21倍，相较其他GPU方法在批量规模扩大时提速1.4-16倍；案例和硬件测试表明其在扰动抑制及收敛性上更优。

Conclusion: GATO极大提升了MPC场景下中等批量非线性轨迹优化的实时求解能力，且已开源以促进复现和社区应用。

Abstract: While Model Predictive Control (MPC) delivers strong performance across
robotics applications, solving the underlying (batches of) nonlinear trajectory
optimization (TO) problems online remains computationally demanding. Existing
GPU-accelerated approaches typically (i) parallelize a single solve to meet
real-time deadlines, (ii) scale to very large batches at slower-than-real-time
rates, or (iii) achieve speed by restricting model generality (e.g., point-mass
dynamics or a single linearization). This leaves a large gap in solver
performance for many state-of-the-art MPC applications that require real-time
batches of tens to low-hundreds of solves. As such, we present GATO, an open
source, GPU-accelerated, batched TO solver co-designed across algorithm,
software, and computational hardware to deliver real-time throughput for these
moderate batch size regimes. Our approach leverages a combination of block-,
warp-, and thread-level parallelism within and across solves for ultra-high
performance. We demonstrate the effectiveness of our approach through a
combination of: simulated benchmarks showing speedups of 18-21x over CPU
baselines and 1.4-16x over GPU baselines as batch size increases; case studies
highlighting improved disturbance rejection and convergence behavior; and
finally a validation on hardware using an industrial manipulator. We open
source GATO to support reproducibility and adoption.

</details>


### [207] [Differentiable Particle Optimization for Fast Sequential Manipulation](https://arxiv.org/abs/2510.07674)
*Lucas Chen,Shrutheesh Raman Iyer,Zachary Kingston*

Main category: cs.RO

TL;DR: 本论文提出了一种全新的GPU并行化优化框架SPaSM，能够在极短时间内、高维空间中解决多步机器人操作任务中的碰撞规避与几何约束优化问题，并实现了极大的速度提升。


<details>
  <summary>Details</summary>
Motivation: 序列机器人操作任务涉及到多次物体交互，需要在高维空间内找到同时满足多个几何约束且无碰撞的运动路径。受限于巨大的计算量，现有方法难以在大规模、实时条件下求解。尽管GPU加速带来了一定性能提升，但数据传输和复杂逻辑限制了其完全发挥硬件性能，因此亟需一种能够高效利用GPU、实现端到端快速优化的新方法。

Method: 提出SPaSM框架，将所有约束评估、采样、基于梯度的优化过程全部编译进高效的CUDA kernel，无需CPU协调，实现全流程GPU并行。具体方法包括两阶段的粒子优化策略：首先利用大规模并行采样满足布局约束，然后将解提升至关节空间的完整轨迹优化。同时，不同于分层方法，SPaSM可联合优化物体放置与机器人轨迹，不受动作可行性对布局选项的限制。

Result: 在具有挑战性的基准测试中，SPaSM展现了解决时间仅需毫秒级，并保持了100%的成功率。与现有方法相比，速度提升高达4000倍。

Conclusion: SPaSM彻底释放了GPU的并行能力，无需CPU参与，在高维复杂操作任务中表现出极大的实时性和可靠性。方法的高效性与通用性为序列机器人操作任务的广泛应用奠定了基础。

Abstract: Sequential robot manipulation tasks require finding collision-free
trajectories that satisfy geometric constraints across multiple object
interactions in potentially high-dimensional configuration spaces. Solving
these problems in real-time and at large scales has remained out of reach due
to computational requirements. Recently, GPU-based acceleration has shown
promising results, but prior methods achieve limited performance due to CPU-GPU
data transfer overhead and complex logic that prevents full hardware
utilization. To this end, we present SPaSM (Sampling Particle optimization for
Sequential Manipulation), a fully GPU-parallelized framework that compiles
constraint evaluation, sampling, and gradient-based optimization into optimized
CUDA kernels for end-to-end trajectory optimization without CPU coordination.
The method consists of a two-stage particle optimization strategy: first
solving placement constraints through massively parallel sampling, then lifting
solutions to full trajectory optimization in joint space. Unlike hierarchical
approaches, SPaSM jointly optimizes object placements and robot trajectories to
handle scenarios where motion feasibility constrains placement options.
Experimental evaluation on challenging benchmarks demonstrates solution times
in the realm of $\textbf{milliseconds}$ with a 100% success rate; a
$4000\times$ speedup compared to existing approaches.

</details>


### [208] [EB-MBD: Emerging-Barrier Model-Based Diffusion for Safe Trajectory Optimization in Highly Constrained Environments](https://arxiv.org/abs/2510.07700)
*Raghav Mishra,Ian R. Manchester*

Main category: cs.RO

TL;DR: 本文提出了一种在模型驱动扩散（Model-Based Diffusion, MBD）中引入障碍函数以强化约束的方法，有效提升约束优化质量，并显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 在模型驱动扩散方法中直接加入约束通常会导致采样效率低下，甚至造成性能崩溃，特别是在简单的2D系统中。如何在保证计算效率的同时实现有效的约束强化，成为一个亟需解决的问题。

Method: 提出了Emerging-Barrier Model-Based Diffusion（EB-MBD）方法，灵感来源于内点法中的障碍函数。该方法通过逐步引入障碍约束，避免采样的非效率，且无需高昂的投影计算。作者还分析了每次迭代的采样活跃性，用于指导障碍参数的调度。

Result: 在二维避碰任务和三维水下操作臂系统中，EB-MBD方法不仅获得了比传统MBD更低成本的解，并且在计算量上比投影类方法低出几个数量级。

Conclusion: EB-MBD能在高效计算的同时提升模型基扩散方法的约束优化水平，优于传统模型基和投影方法，在实际复杂任务中具备应用潜力。

Abstract: We propose enforcing constraints on Model-Based Diffusion by introducing
emerging barrier functions inspired by interior point methods. We show that
constraints on Model-Based Diffusion can lead to catastrophic performance
degradation, even on simple 2D systems due to sample inefficiency in the Monte
Carlo approximation of the score function. We introduce Emerging-Barrier
Model-Based Diffusion (EB-MBD) which uses progressively introduced barrier
constraints to avoid these problems, significantly improving solution quality,
without the need for computationally expensive operations such as projections.
We analyze the sampling liveliness of samples each iteration to inform barrier
parameter scheduling choice. We demonstrate results for 2D collision avoidance
and a 3D underwater manipulator system and show that our method achieves lower
cost solutions than Model-Based Diffusion, and requires orders of magnitude
less computation time than projection based methods.

</details>


### [209] [Probabilistically-Safe Bipedal Navigation over Uncertain Terrain via Conformal Prediction and Contraction Analysis](https://arxiv.org/abs/2510.07725)
*Kasidit Muenprasitivej,Ye Zhao,Glen Chou*

Main category: cs.RO

TL;DR: 本文提出了一种基于概率安全的MPC导航方法，使双足机器人在地形不确定性下能安全且动态可行地通过崎岖地形。主要创新在于结合高置信度地形建模与鲁棒运动控制，实现了对末端目标的安全到达。


<details>
  <summary>Details</summary>
Motivation: 双足机器人在崎岖不平、存在不确定地形时运动面临很大挑战，尤其是如何确保运动的动态可行性与安全性。以往方法往往忽略地形不确定性，无法系统性保证安全和鲁棒性。本文动机在于解决这一难题，为机器人部署于复杂真实环境提供理论与方法保障。

Method: 方法上，使用高层次的模型预测控制（MPC）框架，结合高斯过程回归对地形高程进行估计，并通过共形预测（CP）获得带置信区间的地形估计，用于制定安全的导航策略。在此基础上，提出收缩性可达管分析以考虑地形不确定性，并设计收缩性飞轮力矩控制律用于简化动力学模型的稳定，以实现稳健的运动规划与控制。

Result: 实验通过MuJoCo环境下对Digit双足机器人进行物理仿真，验证了所提方法能在一定置信度下实现目标点到达、动力学约束满足与轨迹稳定，优于不做地形不确定性建模的传统方法。

Conclusion: 结论指出，结合概率建模与收缩性控制的方法能有效提升双足机器人在不确定地形下的安全性与鲁棒性，为实际应用带来理论与仿真上的支撑。

Abstract: We address the challenge of enabling bipedal robots to traverse rough terrain
by developing probabilistically safe planning and control strategies that
ensure dynamic feasibility and centroidal robustness under terrain uncertainty.
Specifically, we propose a high-level Model Predictive Control (MPC) navigation
framework for a bipedal robot with a specified confidence level of safety that
(i) enables safe traversal toward a desired goal location across a terrain map
with uncertain elevations, and (ii) formally incorporates uncertainty bounds
into the centroidal dynamics of locomotion control. To model the rough terrain,
we employ Gaussian Process (GP) regression to estimate elevation maps and
leverage Conformal Prediction (CP) to construct calibrated confidence intervals
that capture the true terrain elevation. Building on this, we formulate
contraction-based reachable tubes that explicitly account for terrain
uncertainty, ensuring state convergence and tube invariance. In addition, we
introduce a contraction-based flywheel torque control law for the reduced-order
Linear Inverted Pendulum Model (LIPM), which stabilizes the angular momentum
about the center-of-mass (CoM). This formulation provides both probabilistic
safety and goal reachability guarantees. For a given confidence level, we
establish the forward invariance of the proposed torque control law by
demonstrating exponential stabilization of the actual CoM phase-space
trajectory and the desired trajectory prescribed by the high-level planner.
Finally, we evaluate the effectiveness of our planning framework through
physics-based simulations of the Digit bipedal robot in MuJoCo.

</details>


### [210] [Injecting Hallucinations in Autonomous Vehicles: A Component-Agnostic Safety Evaluation Framework](https://arxiv.org/abs/2510.07749)
*Alexandre Moreira Nascimento,Gabriel Kenji Godoy Shimanuki,Lúcio Flavio Vismari,João Batista Camargo Jr,Jorge Rady de Almeida Jr,Paulo Sergio Cugnasca,Anna Carolina Muller Queiroz,Jeremy Noah Bailenson*

Main category: cs.RO

TL;DR: 本研究提出了一种新的自动驾驶感知故障注入框架，将感知故障抽象为“幻觉”，并在大规模仿真中验证了其对安全性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶感知故障注入研究大多仅针对单个传感器或模块，造成方法分散、难以推广或集成。本研究旨在解决这一问题，提出更通用的安全性评估方法。

Method: 作者提出将感知失效抽象为“幻觉”，即对自动驾驶车辆情境认知的错误感知，并设计了一个可配置、与组件无关的幻觉注入框架，可在开源仿真平台中注入六类常见幻觉。通过18350次仿真，在无信号横穿马路的场景下分析各类幻觉对碰撞和险情的影响。

Result: 仿真统计表明，不同类型的感知幻觉对碰撞风险有显著影响，特别是感知延迟和漂移类幻觉会显著增加事故概率。这证明了所提出新范式对压力测试自动驾驶系统安全性的有效性。

Conclusion: 该框架具有可扩展性、统计有效性和高度互操作性，可大幅简化和加速新型自动驾驶系统（包括新型感知架构）的安全性验证，并为今后容错和鲁棒性设计研究打下基础，有望缩短自动驾驶汽车的上市时间。

Abstract: Perception failures in autonomous vehicles (AV) remain a major safety concern
because they are the basis for many accidents. To study how these failures
affect safety, researchers typically inject artificial faults into hardware or
software components and observe the outcomes. However, existing fault injection
studies often target a single sensor or machine perception (MP) module,
resulting in siloed frameworks that are difficult to generalize or integrate
into unified simulation environments. This work addresses that limitation by
reframing perception failures as hallucinations, false perceptions that distort
an AV situational awareness and may trigger unsafe control actions. Since
hallucinations describe only observable effects, this abstraction enables
analysis independent of specific sensors or algorithms, focusing instead on how
their faults manifest along the MP pipeline. Building on this concept, we
propose a configurable, component-agnostic hallucination injection framework
that induces six plausible hallucination types in an iterative open-source
simulator. More than 18,350 simulations were executed in which hallucinations
were injected while AVs crossed an unsignalized transverse street with traffic.
The results statistically validate the framework and quantify the impact of
each hallucination type on collisions and near misses. Certain hallucinations,
such as perceptual latency and drift, significantly increase the risk of
collision in the scenario tested, validating the proposed paradigm can stress
the AV system safety. The framework offers a scalable, statistically validated,
component agnostic, and fully interoperable toolset that simplifies and
accelerates AV safety validations, even those with novel MP architectures and
components. It can potentially reduce the time-to-market of AV and lay the
foundation for future research on fault tolerance, and resilient AV design.

</details>


### [211] [Trajectory Conditioned Cross-embodiment Skill Transfer](https://arxiv.org/abs/2510.07773)
*YuHang Tang,Yixuan Lou,Pengfei Han,Haoming Song,Xinyi Ye,Dong Wang,Bin Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种从人类示范视频中直接让机器人学习操作技能的新方法TrajSkill，无需配对数据或手工奖励，显著提升了跨主体技能迁移的可行性和效果。


<details>
  <summary>Details</summary>
Motivation: 传统机器人学习人类技能的方法依赖于配对数据集或手工设计奖励，受限严重，难以扩展和泛化。人类与机器人体型和结构差异大，导致跨实体技能转移极具挑战性。

Method: TrajSkill将人类动作以稀疏光流轨迹进行编码，消除形态差异但保留动态本质。这些轨迹联合视觉及文本输入，用于指导机器人的时序操作视频合成，并转译为可执行的动作，实现技能转移。

Result: 在MetaWorld仿真环境中，TrajSkill较现有方法FVD降低39.6%、KVD降低36.6%，跨主体操作成功率提升至16.7%。实际厨房机器人实验亦验证了方法的有效性。

Conclusion: TrajSkill有效实现了从人到机器人的技能迁移，不依赖配对数据与手工奖励，兼具实用性和拓展性。

Abstract: Learning manipulation skills from human demonstration videos presents a
promising yet challenging problem, primarily due to the significant embodiment
gap between human body and robot manipulators. Existing methods rely on paired
datasets or hand-crafted rewards, which limit scalability and generalization.
We propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment
Skill Transfer, enabling robots to acquire manipulation skills directly from
human demonstration videos. Our key insight is to represent human motions as
sparse optical flow trajectories, which serve as embodiment-agnostic motion
cues by removing morphological variations while preserving essential dynamics.
Conditioned on these trajectories together with visual and textual inputs,
TrajSkill jointly synthesizes temporally consistent robot manipulation videos
and translates them into executable actions, thereby achieving cross-embodiment
skill transfer. Extensive experiments are conducted, and the results on
simulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\% and KVD
by 36.6\% compared with the state-of-the-art, and improves cross-embodiment
success rate by up to 16.7\%. Real-robot experiments in kitchen manipulation
tasks further validate the effectiveness of our approach, demonstrating
practical human-to-robot skill transfer across embodiments.

</details>


### [212] [IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction](https://arxiv.org/abs/2510.07778)
*Yandu Chen,Kefan Gu,Yuqing Wen,Yucheng Zhao,Tiancai Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: 本文提出了一种新型Vision-Language-Action (VLA) 框架IntentionVLA，提升了机器人理解和执行人类隐含意图的能力，在多项基准任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的VLA模型由于预训练任务与真实交互场景的相关性较低，缺乏复杂推理能力，难以胜任需要隐式意图理解的真实人机交互。

Method: 提出了IntentionVLA，包括分阶段的课程式训练范式，先基于结合意图推理、空间定位与紧凑推理的数据进行预训练，再利用推理结果作为生成动作的上下文进行微调。

Result: IntentionVLA在直接指令任务上比基线高18%成功率，在意图推理任务上高于ECoT 28%；在分布外意图任务上，其成功率超过所有基线两倍，且零样本人机交互任务成功率达40%。

Conclusion: IntentionVLA展示了强大的泛化推理能力，为新一代高效、智能的人机交互系统提供了有前景的解决方案。

Abstract: Vision-Language-Action (VLA) models leverage pretrained vision-language
models (VLMs) to couple perception with robotic control, offering a promising
path toward general-purpose embodied intelligence. However, current SOTA VLAs
are primarily pretrained on multimodal tasks with limited relevance to embodied
scenarios, and then finetuned to map explicit instructions to actions.
Consequently, due to the lack of reasoning-intensive pretraining and
reasoning-guided manipulation, these models are unable to perform implicit
human intention reasoning required for complex, real-world interactions. To
overcome these limitations, we propose \textbf{IntentionVLA}, a VLA framework
with a curriculum training paradigm and an efficient inference mechanism. Our
proposed method first leverages carefully designed reasoning data that combine
intention inference, spatial grounding, and compact embodied reasoning,
endowing the model with both reasoning and perception capabilities. In the
following finetuning stage, IntentionVLA employs the compact reasoning outputs
as contextual guidance for action generation, enabling fast inference under
indirect instructions. Experimental results show that IntentionVLA
substantially outperforms $\pi_0$, achieving 18\% higher success rates with
direct instructions and 28\% higher than ECoT under intention instructions. On
out-of-distribution intention tasks, IntentionVLA achieves over twice the
success rate of all baselines, and further enables zero-shot human-robot
interaction with 40\% success rate. These results highlight IntentionVLA as a
promising paradigm for next-generation human-robot interaction (HRI) systems.

</details>


### [213] [GM3: A General Physical Model for Micro-Mobility Vehicles](https://arxiv.org/abs/2510.07807)
*Grace Cai,Nithin Parepally,Laura Zheng,Ming C. Lin*

Main category: cs.RO

TL;DR: 本文提出了一种新的微型移动载具（MMV）动力学统一建模方法GM3，能更真实模拟多类型载具的动力学特性，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有主流MMV动力学建模工具基于运动学自行车模型（KBM）或特定物理模型，无法全面捕捉轮胎滑移、载荷转移、车体/骑手倾斜等真实动力学现象，且缺乏适用于多轮布局和广泛适用性的统一物理模型。

Method: 提出基于轮胎刷模型的“广义微型移动载具模型”（GM3），可支持任意车轮配置（单/双轨、多轮等）。同时开发了一套解耦载具/布局与动力学的交互式、通用模拟框架，采用RK4积分，支持人工与脚本控制、实时轨迹记录与分析，并将GM3与KBM等其他常见模型进行对比。

Result: 通过在Stanford Drone Dataset的deathCircle场景中对自行车、滑板车和推车三类载具进行实验，实证验证了GM3的有效性。

Conclusion: GM3能够统一、物理真实地模拟多类型微型移动载具的动力学特征，并优于现有主流模型，有望加强自动驾驶系统训练与城市交通仿真应用。

Abstract: Modeling the dynamics of micro-mobility vehicles (MMV) is becoming
increasingly important for training autonomous vehicle systems and building
urban traffic simulations. However, mainstream tools rely on variants of the
Kinematic Bicycle Model (KBM) or mode-specific physics that miss tire slip,
load transfer, and rider/vehicle lean. To our knowledge, no unified,
physics-based model captures these dynamics across the full range of common
MMVs and wheel layouts. We propose the "Generalized Micro-mobility Model"
(GM3), a tire-level formulation based on the tire brush representation that
supports arbitrary wheel configurations, including single/double track and
multi-wheel platforms. We introduce an interactive model-agnostic simulation
framework that decouples vehicle/layout specification from dynamics to compare
the GM3 with the KBM and other models, consisting of fixed step RK4
integration, human-in-the-loop and scripted control, real-time trajectory
traces and logging for analysis. We also empirically validate the GM3 on the
Stanford Drone Dataset's deathCircle (roundabout) scene for biker, skater, and
cart classes.

</details>


### [214] [DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation](https://arxiv.org/abs/2510.07865)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: 论文提出了一种用于机器人操作的新型流生成模型框架DM1，通过引入分散正则化，有效防止了表示塌缩问题，显著提升了效率和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的生成模型虽然可以高效生成多模态动作分布，但存在表示塌缩问题，导致模型无法区分相似的视觉状态，从而影响机器人操作的精准性和鲁棒性。该论文旨在解决此瓶颈。

Method: 提出了DM1（MeanFlow with Dispersive Regularization for One-Step Robotic Manipulation）方法，在MeanFlow中引入了多种分散正则化方法，分别应用于不同的网络中间层，以鼓励跨batch的表示多样性，无需增加新模块或复杂训练流程。

Result: 在RoboMimic基准上，DM1推理速度提升20-40倍（0.07s对2-3.5s），任务成功率提升10-20个百分点，其中Lift任务成功率达99%（相比基线的85%）。实际在Franka Panda机器人上测试表明，DM1在仿真到现实的迁移上表现优异。

Conclusion: 首次将表示正则化引入流式策略，有效解决了表示塌缩问题，实现了高效、稳健的机器人操作控制，方法简单效果显著，有潜力广泛应用于实际机器人操作任务。

Abstract: The ability to learn multi-modal action distributions is indispensable for
robotic manipulation policies to perform precise and robust control. Flow-based
generative models have recently emerged as a promising solution to learning
distributions of actions, offering one-step action generation and thus
achieving much higher sampling efficiency compared to diffusion-based methods.
However, existing flow-based policies suffer from representation collapse, the
inability to distinguish similar visual representations, leading to failures in
precise manipulation tasks. We propose DM1 (MeanFlow with Dispersive
Regularization for One-Step Robotic Manipulation), a novel flow matching
framework that integrates dispersive regularization into MeanFlow to prevent
collapse while maintaining one-step efficiency. DM1 employs multiple dispersive
regularization variants across different intermediate embedding layers,
encouraging diverse representations across training batches without introducing
additional network modules or specialized training procedures. Experiments on
RoboMimic benchmarks show that DM1 achieves 20-40 times faster inference (0.07s
vs. 2-3.5s) and improves success rates by 10-20 percentage points, with the
Lift task reaching 99% success over 85% of the baseline. Real-robot deployment
on a Franka Panda further validates that DM1 transfers effectively from
simulation to the physical world. To the best of our knowledge, this is the
first work to leverage representation regularization to enable flow-based
policies to achieve strong performance in robotic manipulation, establishing a
simple yet powerful approach for efficient and robust manipulation.

</details>


### [215] [USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots](https://arxiv.org/abs/2510.07869)
*Junwen Gu,Zhiheng wu,Pengxuan Si,Shuang Qiu,Yukai Feng,Luoyang Sun,Laien Luo,Lianyi Yu,Jian Wang,Zhengxing Wu*

Main category: cs.RO

TL;DR: 本论文提出了USIM，一个基于仿真的多任务水下机器人视觉-语言-动作（VLA）数据集，以及U0多模态VLA模型，有效提升了水下机器人在多任务环境下的自主能力。


<details>
  <summary>Details</summary>
Motivation: 水下环境操作难度大，传统数据集与方法难以支持水下多任务机器人自主智能开发。主要难点在于缺乏大规模高质量的数据集和通用模型，限制了多任务自主化水平。

Method: 作者构建了USIM数据集，涵盖20项任务、9种场景、1852条轨迹，总计15.6小时的BlueROV2机器人交互数据。提出U0模型，通过多模态融合（包括双目视觉和其他传感器）以及卷积-注意力感知增强模块（CAP）提升空间理解与操控能力。

Result: 在检测、避障、扫描及动态跟踪等任务中取得了80%的成功率，尤其在复杂的操控任务中相较基线方法使距离目标距离减少了21.2%。

Conclusion: USIM与U0模型证明了VLA模型在水下机器人中的应用前景，为大规模数据集构建、任务自主性提升及智能通用水下机器人的发展奠定了基础。

Abstract: Underwater environments present unique challenges for robotic operation,
including complex hydrodynamics, limited visibility, and constrained
communication. Although data-driven approaches have advanced embodied
intelligence in terrestrial robots and enabled task-specific autonomous
underwater robots, developing underwater intelligence capable of autonomously
performing multiple tasks remains highly challenging, as large-scale,
high-quality underwater datasets are still scarce. To address these
limitations, we introduce USIM, a simulation-based multi-task
Vision-Language-Action (VLA) dataset for underwater robots. USIM comprises over
561K frames from 1,852 trajectories, totaling approximately 15.6 hours of
BlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from
visual navigation to mobile manipulation. Building upon this dataset, we
propose U0, a VLA model for general underwater robots, which integrates
binocular vision and other sensor modalities through multimodal fusion, and
further incorporates a convolution-attention-based perception focus enhancement
module (CAP) to improve spatial understanding and mobile manipulation. Across
tasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,
the framework achieves a success rate of 80%, while in challenging mobile
manipulation tasks, it reduces the distance to the target by 21.2% compared
with baseline methods, demonstrating its effectiveness. USIM and U0 show that
VLA models can be effectively applied to underwater robotic applications,
providing a foundation for scalable dataset construction, improved task
autonomy, and the practical realization of intelligent general underwater
robots.

</details>


### [216] [Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception - Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track](https://arxiv.org/abs/2510.07871)
*Erjia Xiao,Lingfeng Zhang,Yingbo Tang,Hao Cheng,Renjing Xu,Wenbo Ding,Lei Zhou,Long Chen,Hangjun Ye,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 本文介绍了团队在IROS 2025 RoboSense Challenge社会导航赛道的参赛方法，通过引入主动风险感知模块，提升了机器人在人群中遵守社交规范自主导航的能力，并在比赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 现有社交导航系统在动态人群环境中存在对碰撞风险感知不足、难以主动避障和维护安全社交距离的问题，因此亟需提升机器人对碰撞风险的理解和主动性。

Method: 基于Falcon模型，引入了Proactive Risk Perception Module，能够学习预测周围人群的基于距离的碰撞风险评分，从而提升空间感知和主动避障能力。整个系统仅依赖于机载RGB-D传感器和里程计数据，无需全局地图或额外信息。

Result: 在Social-HM3D基准评测上，该方法提升了机器人在人群中的个人空间遵守与目标导向导航能力。在16支队伍中取得了第二名的成绩。

Conclusion: 引入主动风险感知模块能显著提升机器人在人口密集、动态变化环境下的社交导航效果，为实际应用中提升机器人安全性和社交合规性提供了有效方案。

Abstract: In this report, we describe the technical details of our submission to the
IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on
developing RGBD-based perception and navigation systems that enable autonomous
agents to navigate safely, efficiently, and socially compliantly in dynamic
human-populated indoor environments. The challenge requires agents to operate
from an egocentric perspective using only onboard sensors including RGB-D
observations and odometry, without access to global maps or privileged
information, while maintaining social norm compliance such as safe distances
and collision avoidance. Building upon the Falcon model, we introduce a
Proactive Risk Perception Module to enhance social navigation performance. Our
approach augments Falcon with collision risk understanding that learns to
predict distance-based collision risk scores for surrounding humans, which
enables the agent to develop more robust spatial awareness and proactive
collision avoidance behaviors. The evaluation on the Social-HM3D benchmark
demonstrates that our method improves the agent's ability to maintain personal
space compliance while navigating toward goals in crowded indoor scenes with
dynamic human agents, achieving 2nd place among 16 participating teams in the
challenge.

</details>


### [217] [Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots](https://arxiv.org/abs/2510.07882)
*Boyu Li,Siyuan He,Hang Xu,Haoqi Yuan,Yu Zang,Liwei Hu,Junpeng Yue,Zhenxiong Jiang,Pengbo Hu,Börje F. Karlsson,Yehui Tang,Zongqing Lu*

Main category: cs.RO

TL;DR: 该论文提出了DualTHOR双臂类人机器人仿真平台，并基于此提出提升机器人本体感知能力的新型多模态大模型Proprio-MLLM，有效提升了复杂任务中的规划表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在双臂类人机器人复杂、长周期任务上的规划能力受限，主要归因于仿真平台的不足和模型欠缺本体感知，难以进行合理的动作和身体姿态规划。

Method: 作者开发了DualTHOR仿真平台，支持系统化评测和数据收集。基于该平台，提出将本体感知（如关节/运动状态）编码为运动位置嵌入，并结合跨空间编码器，增强MLLM对机器人动作和身体逻辑的建模能力，形成Proprio-MLLM。

Result: 在仿真双臂机器人完成复杂规划任务实验中，Proprio-MLLM对比已有MLLM模型，规划性能平均提升了19.75%。

Conclusion: DualTHOR平台和Proprio-MLLM方法为类人机器人具身智能发展提供了有效工具和技术支持，显著加强了模型在复杂任务规划能力及本体感知。

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have demonstrated
the ability to serve as high-level planners, enabling robots to follow complex
human instructions. However, their effectiveness, especially in long-horizon
tasks involving dual-arm humanoid robots, remains limited. This limitation
arises from two main challenges: (i) the absence of simulation platforms that
systematically support task evaluation and data collection for humanoid robots,
and (ii) the insufficient embodiment awareness of current MLLMs, which hinders
reasoning about dual-arm selection logic and body positions during planning. To
address these issues, we present DualTHOR, a new dual-arm humanoid simulator,
with continuous transition and a contingency mechanism. Building on this
platform, we propose Proprio-MLLM, a model that enhances embodiment awareness
by incorporating proprioceptive information with motion-based position
embedding and a cross-spatial encoder. Experiments show that, while existing
MLLMs struggle in this environment, Proprio-MLLM achieves an average
improvement of 19.75% in planning performance. Our work provides both an
essential simulation platform and an effective model to advance embodied
intelligence in humanoid robotics. The code is available at
https://anonymous.4open.science/r/DualTHOR-5F3B.

</details>


### [218] [Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation](https://arxiv.org/abs/2510.07975)
*Mingyang Sun,Jiude Wei,Qichen He,Donglin Wang,Cewu Lu,Jianhua Sun*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的方法GRACE，通过将视觉-语言模型（VLM）的理解能力与可执行的解析概念（EAC）结合，实现了机器人在非结构化环境下的精准和泛化操作。GRACE将自然语言指令和视觉信息转化为可操作的物理动作，有效连接语义理解与低层运动控制。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM在任务规划和语义推理方面表现出色，但其对高层语义的理解与实际物理操作之间存在鸿沟（即“语义-物理”差距）。该工作旨在弥补这一差距，使机器人能精准执行与语义相关的实际操作。

Method: 提出GRACE框架，核心是可执行解析概念（EAC），用数学方式描述对象可供性、几何约束和操作语义。通过结构化的政策脚手架，将语言和视觉信息转化为EAC，再据此规划抓取、施力方向和物理可行的运动轨迹，指导机器人执行。

Result: 在大量仿真和真实环境实验中，GRACE无需特定任务训练，就展现出对多种关节物体的强零样本泛化能力，实现了精准操作。

Conclusion: GRACE为高层语义理解与底层物理控制之间架起了高效、可解释的桥梁，实现了精准且泛化的机器人操作，推动了具身智能的发展。

Abstract: Enabling robots to perform precise and generalized manipulation in
unstructured environments remains a fundamental challenge in embodied AI. While
Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
semantic reasoning and task planning, a significant gap persists between their
high-level understanding and the precise physical execution required for
real-world manipulation. To bridge this "semantic-to-physical" gap, we
introduce GRACE, a novel framework that grounds VLM-based reasoning through
executable analytic concepts (EAC)-mathematically defined blueprints that
encode object affordances, geometric constraints, and semantics of
manipulation. Our approach integrates a structured policy scaffolding pipeline
that turn natural language instructions and visual information into an
instantiated EAC, from which we derive grasp poses, force directions and plan
physically feasible motion trajectory for robot execution. GRACE thus provides
a unified and interpretable interface between high-level instruction
understanding and low-level robot control, effectively enabling precise and
generalizable manipulation through semantic-physical grounding. Extensive
experiments demonstrate that GRACE achieves strong zero-shot generalization
across a variety of articulated objects in both simulated and real-world
environments, without requiring task-specific training.

</details>


### [219] [Orientation Learning and Adaptation towards Simultaneous Incorporation of Multiple Local Constraints](https://arxiv.org/abs/2510.07986)
*Gaofeng Li,Peisen Xu,Ruize Wang,Qi Ye,Jiming Chen,Dezhen Song,Yanlong Huang*

Main category: cs.RO

TL;DR: 作者提出了一种基于角轴空间的方向表示方法，解决了SO(3)空 间下由于非欧几里得几何引起的多个约束融合难题，并设计了加权平均机制，有效提升了方向调整与加速度最小化任务的性能。


<details>
  <summary>Details</summary>
Motivation: 在SO(3)旋转群上进行方向学习时，非欧几里得几何特性导致在处理多个局部约束时出现失真，难以同时有效结合多个局部约束，限制了相关任务的表现。

Method: 提出角轴空间的方向表示方法和基于加权平均的机制，能够在SO(3)上基于不同基点生成多条满足不同约束的轨迹，然后通过加权平均融合成一条平滑同时满足多个约束的轨迹，从而解决了传统方法因失真问题难以整合多个约束的难题。

Result: 仿真和实验结果显示，所提方法不仅能够适应方向的任意中间点需求和应对角加速度约束，还能同时结合多个局部约束，具体效果表现为可实现更小的加速度成本等附加优势。

Conclusion: 本方法有效解决了SO(3)空间方向学习中多个局部约束难以兼容的问题，使得现有欧氏学习算法能在非欧空间下适用，并带来更优的运动表现和加速度优化。

Abstract: Orientation learning plays a pivotal role in many tasks. However, the
rotation group SO(3) is a Riemannian manifold. As a result, the distortion
caused by non-Euclidean geometric nature introduces difficulties to the
incorporation of local constraints, especially for the simultaneous
incorporation of multiple local constraints. To address this issue, we propose
the Angle-Axis Space-based orientation representation method to solve several
orientation learning problems, including orientation adaptation and
minimization of angular acceleration. Specifically, we propose a weighted
average mechanism in SO(3) based on the angle-axis representation method. Our
main idea is to generate multiple trajectories by considering different local
constraints at different basepoints. Then these multiple trajectories are fused
to generate a smooth trajectory by our proposed weighted average mechanism,
achieving the goal to incorporate multiple local constraints simultaneously.
Compared with existing solution, ours can address the distortion issue and make
the off-theshelf Euclidean learning algorithm be re-applicable in non-Euclidean
space. Simulation and Experimental evaluations validate that our solution can
not only adapt orientations towards arbitrary desired via-points and cope with
angular acceleration constraints, but also incorporate multiple local
constraints simultaneously to achieve extra benefits, e.g., achieving smaller
acceleration costs.

</details>


### [220] [FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset](https://arxiv.org/abs/2510.08022)
*Kehui Liu,Zhongjie Jia,Yang Li,Zhaxizhuoma,Pengan Chen,Song Liu,Xin Liu,Pingrui Zhang,Haoming Song,Xinyi Ye,Nieqing Cao,Zhigang Wang,Jia Zeng,Dong Wang,Yan Ding,Bin Zhao,Xuelong Li*

Main category: cs.RO

TL;DR: 提出了一个大规模的多模态机器人操作演示数据集 FastUMI-100K，包含10万条多模态演示数据，旨在提升数据驱动机器人操作学习的效果。


<details>
  <summary>Details</summary>
Motivation: 现有主要依靠人类远程操作采集的机器人演示数据集在规模、轨迹平滑性以及在不同机器人硬件间的适用性方面存在局限，难以满足复杂实际操作任务的数据需求。

Method: 设计了一种新的可模块化、硬件解耦的机器人系统 FastUMI，结合轻量级跟踪系统，能高效采集不同环境、多任务、多对象的大量操作演示，并集成多模态数据（末端执行器状态、多视角鱼眼图像、文本标注），最终构建了FastUMI-100K数据集。

Result: FastUMI-100K数据集包含逾10万条轨迹，涵盖54种任务和数百种对象，满足实际家庭环境下的多样化需求。实验证明数据集支持的策略在多项基线算法上表现出高成功率，展示了其鲁棒性和广泛适用性。

Conclusion: FastUMI-100K有效推动了机器人操作学习数据集的可扩展性和实用性，为解决现实中复杂的动态操作挑战提供了坚实数据基础。

Abstract: Data-driven robotic manipulation learning depends on large-scale,
high-quality expert demonstration datasets. However, existing datasets, which
primarily rely on human teleoperated robot collection, are limited in terms of
scalability, trajectory smoothness, and applicability across different robotic
embodiments in real-world environments. In this paper, we present FastUMI-100K,
a large-scale UMI-style multimodal demonstration dataset, designed to overcome
these limitations and meet the growing complexity of real-world manipulation
tasks. Collected by FastUMI, a novel robotic system featuring a modular,
hardware-decoupled mechanical design and an integrated lightweight tracking
system, FastUMI-100K offers a more scalable, flexible, and adaptable solution
to fulfill the diverse requirements of real-world robot demonstration data.
Specifically, FastUMI-100K contains over 100K+ demonstration trajectories
collected across representative household environments, covering 54 tasks and
hundreds of object types. Our dataset integrates multimodal streams, including
end-effector states, multi-view wrist-mounted fisheye images and textual
annotations. Each trajectory has a length ranging from 120 to 500 frames.
Experimental results demonstrate that FastUMI-100K enables high policy success
rates across various baseline algorithms, confirming its robustness,
adaptability, and real-world applicability for solving complex, dynamic
manipulation challenges. The source code and dataset will be released in this
link https://github.com/MrKeee/FastUMI-100K.

</details>


### [221] [Towards Reliable LLM-based Robot Planning via Combined Uncertainty Estimation](https://arxiv.org/abs/2510.08044)
*Shiyuan Yin,Chenjia Bai,Zihao Zhang,Junwei Jin,Xinxin Zhang,Chi Zhang,Xuelong Li*

Main category: cs.RO

TL;DR: 该论文针对大型语言模型（LLM）在机器人计划中的不确定性提出了新的估算方法，并通过实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能够生成高层次的计划以指导机器人完成任务，但其产生的“幻觉”常导致计划不安全或与需求不符。现有的不确定性估计方法未能区分不同类型的不确定性，因而效果有限。

Method: 提出CURE方法，将不确定性分为认知不确定性和固有不确定性分别建模，其中认知不确定性进一步细分为任务清晰度和任务熟悉度，通过随机网络蒸馏和MLP回归头对LLM特征进行联合建模。

Result: 在厨房操作和桌面物品整理两个实验中，CURE方法的不确定性估计与实际执行结果更加吻合，优于现有方法。

Conclusion: CURE可以有效提升LLM驱动的机器人计划的可靠性，为未来智能机器人任务规划带来更安全和对齐的执行方案。

Abstract: Large language models (LLMs) demonstrate advanced reasoning abilities,
enabling robots to understand natural language instructions and generate
high-level plans with appropriate grounding. However, LLM hallucinations
present a significant challenge, often leading to overconfident yet potentially
misaligned or unsafe plans. While researchers have explored uncertainty
estimation to improve the reliability of LLM-based planning, existing studies
have not sufficiently differentiated between epistemic and intrinsic
uncertainty, limiting the effectiveness of uncertainty esti- mation. In this
paper, we present Combined Uncertainty estimation for Reliable Embodied
planning (CURE), which decomposes the uncertainty into epistemic and intrinsic
uncertainty, each estimated separately. Furthermore, epistemic uncertainty is
subdivided into task clarity and task familiarity for more accurate evaluation.
The overall uncertainty assessments are obtained using random network
distillation and multi-layer perceptron regression heads driven by LLM
features. We validated our approach in two distinct experimental settings:
kitchen manipulation and tabletop rearrangement experiments. The results show
that, compared to existing methods, our approach yields uncertainty estimates
that are more closely aligned with the actual execution outcomes.

</details>


### [222] [Beyond hospital reach: Autonomous lightweight ultrasound robot for liver sonography](https://arxiv.org/abs/2510.08106)
*Zihan Li,Yixiao Xu,Lei Zhang,Taiyu Han,Xinshan Yang,Yingni Wang,Mingxuan Liu,Shenghai Xin,Linxun Liu,Hongen Liao,Guochen Ning*

Main category: cs.RO

TL;DR: 本论文介绍了一种可自主进行肝脏超声检查的轻量级机器人系统，可在缺乏专业医护人员的地区实现高水平的肝脏疾病筛查及诊断。


<details>
  <summary>Details</summary>
Motivation: 肝脏疾病是全球重要的健康负担，肝脏超声作为一线诊断工具对操作者要求较高，而专业人员在资源有限地区极度稀缺，因此亟需辅助工具来拓展高质量超声诊断的可及性。

Method: 作者研发了一套轻量级（588克）、六自由度的超声机器人系统，集成了具备多模态感知和记忆注意力的AI智能体，通过腹部佩戴提升抗运动干扰能力，可自主定位和采集标准肝脏超声切面，并进行病变检测。

Result: 系统可在包括医疗资源极有限的高原城市西宁等多种复杂环境下，自主获取高水平肝脏超声图像并检测病变，且在人员快速移动情况下依然表现优秀。

Conclusion: 本系统首次实现了多种挑战环境下的自主超声诊断，有望大幅提升欠发达地区对专业超声诊断的可及性并改善肝脏疾病的筛查和早诊。

Abstract: Liver disease is a major global health burden. While ultrasound is the
first-line diagnostic tool, liver sonography requires locating multiple
non-continuous planes from positions where target structures are often not
visible, for biometric assessment and lesion detection, requiring significant
expertise. However, expert sonographers are severely scarce in resource-limited
regions. Here, we develop an autonomous lightweight ultrasound robot comprising
an AI agent that integrates multi-modal perception with memory attention for
localization of unseen target structures, and a 588-gram 6-degrees-of-freedom
cable-driven robot. By mounting on the abdomen, the system enhances robustness
against motion. Our robot can autonomously acquire expert-level standard liver
ultrasound planes and detect pathology in patients, including two from Xining,
a 2261-meter-altitude city with limited medical resources. Our system performs
effectively on rapid-motion individuals and in wilderness environments. This
work represents the first demonstration of autonomous sonography across
multiple challenging scenarios, potentially transforming access to expert-level
diagnostics in underserved regions.

</details>


### [223] [Accurate and Noise-Tolerant Extraction of Routine Logs in Robotic Process Automation (Extended Version)](https://arxiv.org/abs/2510.08118)
*Massimiliano de Leoni,Faizan Ahmed Khan,Simone Agostinelli*

Main category: cs.RO

TL;DR: 本文提出并验证了一种基于聚类的方法以更准确地从带有噪声的用户界面日志中提取例行活动日志，从而推动机器人流程挖掘和自动化。


<details>
  <summary>Details</summary>
Motivation: 现有机器人流程挖掘方法多侧重于动作集提取，缺乏直接支持模型发现，且在带有噪声（即执行不一致性和人为错误）的场景下效果不佳，因此需要更有效的日志提取方法以支持后续自动化。

Method: 提出基于聚类的新技术，对用户界面日志进行处理，从中自动提取更具代表性的例行活动日志，并与现有适配方法进行了对比。

Result: 在九组不同噪声水平的UI日志数据集上进行实验，所提方法在标准评估指标下表现优于同类技术，尤其在高噪声条件下提取的例行日志更为准确。

Conclusion: 基于聚类的方法能够更加精确地从带有噪声的日志中提取例行活动，为自动化流程模型的发现提供了更可靠的基础，比现有方法更适应实际工作场景的波动和异常。

Abstract: Robotic Process Mining focuses on the identification of the routine types
performed by human resources through a User Interface. The ultimate goal is to
discover routine-type models to enable robotic process automation. The
discovery of routine-type models requires the provision of a routine log.
Unfortunately, the vast majority of existing works do not directly focus on
enabling the model discovery, limiting themselves to extracting the set of
actions that are part of the routines. They were also not evaluated in
scenarios characterized by inconsistent routine execution, hereafter referred
to as noise, which reflects natural variability and occasional errors in human
performance. This paper presents a clustering-based technique that aims to
extract routine logs. Experiments were conducted on nine UI logs from the
literature with different levels of injected noise. Our technique was compared
with existing techniques, most of which are not meant to discover routine logs
but were adapted for the purpose. The results were evaluated through standard
state-of-the-art metrics, showing that we can extract more accurate routine
logs than what the state of the art could, especially in the presence of noise.

</details>


### [224] [NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions](https://arxiv.org/abs/2510.08173)
*Haolin Yang,Yuxing Long,Zhuoyuan Yu,Zihan Yang,Minghan Wang,Jiapeng Xu,Yihan Wang,Ziyan Yu,Wenzhe Cai,Lei Kang,Hao Dong*

Main category: cs.RO

TL;DR: 本文提出了NavSpace基准用于系统性评估导航智能体的空间感知与推理能力，并提出了新模型SNav，取得了优异的效果。


<details>
  <summary>Details</summary>
Motivation: 现有导航评测主要关注语义理解，忽视了对导航智能体空间感知与推理能力的系统性评估。

Method: 作者设计了NavSpace基准，包含六类任务和1,228个轨迹-指令对，用于衡量空间智能，并对22种导航模型（包括多模态大模型）进行综合评测。同时提出了新的空间智能导航模型SNav。

Result: NavSpace基准在揭示导航体空间智能的差异上表现有效。新提出的SNav模型在NavSpace和真实机器人测试中均优于现有方法。

Conclusion: NavSpace为空间智能评测提供了新工具，SNav成为该基准上的新强基线，有助于推动导航智能体的发展。

Abstract: Instruction-following navigation is a key step toward embodied intelligence.
Prior benchmarks mainly focus on semantic understanding but overlook
systematically evaluating navigation agents' spatial perception and reasoning
capabilities. In this work, we introduce the NavSpace benchmark, which contains
six task categories and 1,228 trajectory-instruction pairs designed to probe
the spatial intelligence of navigation agents. On this benchmark, we
comprehensively evaluate 22 navigation agents, including state-of-the-art
navigation models and multimodal large language models. The evaluation results
lift the veil on spatial intelligence in embodied navigation. Furthermore, we
propose SNav, a new spatially intelligent navigation model. SNav outperforms
existing navigation agents on NavSpace and real robot tests, establishing a
strong baseline for future work.

</details>


### [225] [Evaluation of a Robust Control System in Real-World Cable-Driven Parallel Robots](https://arxiv.org/abs/2510.08270)
*Damir Nurtdinov,Aliaksei Korshuk,Alexei Kornaev,Alexander Maloletov*

Main category: cs.RO

TL;DR: 本文比较了经典PID和现代强化学习控制方法在实际缆驱并联机器人（CDPR）中的性能，发现在有限采样频率和受限传感环境下，TRPO算法具有最优表现。


<details>
  <summary>Details</summary>
Motivation: 缆驱并联机器人因其结构优势在工业等领域有广泛应用，但传统PID算法对传感器采样和控制频率依赖较大，受噪声或低采样率影响时控制效果不佳，因此研究更鲁棒的控制策略成为需求。

Method: 通过在受控约束和时间离散化有限的实际CDPR系统上，对比了经典PID与三种主流强化学习方法（DDPG、PPO、TRPO）的控制性能，主要考察不同方法在跟踪轨迹误差及对控制周期延长的鲁棒性。

Result: 实验结果显示，TRPO在多条轨迹上取得了最低的均方根误差（RMS），并能在延长控制更新周期的情况下保持较高的鲁棒性。

Conclusion: TRPO因其对环境噪声和控制间隔变化的适应能力，成为复杂机器人控制任务的有力候选，为动态环境、传感器多融合或混合控制策略等未来应用提供了支持。

Abstract: This study evaluates the performance of classical and modern control methods
for real-world Cable-Driven Parallel Robots (CDPRs), focusing on
underconstrained systems with limited time discretization. A comparative
analysis is conducted between classical PID controllers and modern
reinforcement learning algorithms, including Deep Deterministic Policy Gradient
(DDPG), Proximal Policy Optimization (PPO), and Trust Region Policy
Optimization (TRPO). The results demonstrate that TRPO outperforms other
methods, achieving the lowest root mean square (RMS) errors across various
trajectories and exhibiting robustness to larger time intervals between control
updates. TRPO's ability to balance exploration and exploitation enables stable
control in noisy, real-world environments, reducing reliance on high-frequency
sensor feedback and computational demands. These findings highlight TRPO's
potential as a robust solution for complex robotic control tasks, with
implications for dynamic environments and future applications in sensor fusion
or hybrid control strategies.

</details>


### [226] [Airy: Reading Robot Intent through Height and Sky](https://arxiv.org/abs/2510.08381)
*Baoyang Chen,Xian Xu,Huamin Qu*

Main category: cs.RO

TL;DR: 本论文介绍了一个名为Airy的艺术装置，通过强化学习机器人协作和竞争参与床单拍动的场景，探索使复杂多智能体AI变得可直观理解的可能性。


<details>
  <summary>Details</summary>
Motivation: 随着工业机器人进入人类共享空间，其决策过程变得不透明，威胁到安全、信任和公众监督。因此，研究如何让大众直观理解智能体的意图，成为亟需解决的问题。

Method: 设计了一个艺术装置，让两台经强化学习训练的机械臂在竞争中将床单拍向空中，并融入三个原则：以比赛高度为清晰指标、观众熟悉的身体体验、和传感器到感官的映射（通过森林与天气投影展示机器人间的协作或对抗），提供观众直观的感知方式。

Result: 通过在五次国际展览中的观察，发现观众能够实时读懂机器人的策略、冲突和合作，并作出与系统内部状态呼应的情感反应。

Conclusion: 研究表明，感官隐喻可以将黑箱AI系统转化为公众可理解的界面，提升透明度和信任感。

Abstract: As industrial robots move into shared human spaces, their opaque decision
making threatens safety, trust, and public oversight. This artwork, Airy, asks
whether complex multi agent AI can become intuitively understandable by staging
a competition between two reinforcement trained robot arms that snap a bedsheet
skyward. Building on three design principles, competition as a clear metric
(who lifts higher), embodied familiarity (audiences recognize fabric snapping),
and sensor to sense mapping (robot cooperation or rivalry shown through forest
and weather projections), the installation gives viewers a visceral way to read
machine intent. Observations from five international exhibitions indicate that
audiences consistently read the robots' strategies, conflict, and cooperation
in real time, with emotional reactions that mirror the system's internal state.
The project shows how sensory metaphors can turn a black box into a public
interface.

</details>


### [227] [Reliability of Single-Level Equality-Constrained Inverse Optimal Control](https://arxiv.org/abs/2510.08406)
*Filip Bečanović,Kosta Jovanović,Vincent Bonnet*

Main category: cs.RO

TL;DR: 本文提出了一种单层优化方法，实现了对逆最优控制（IOC）问题的更快和更稳健求解，在处理高噪声数据时依然表现优异，且计算速度大幅提升。


<details>
  <summary>Details</summary>
Motivation: 传统IOC方法要么计算慢（双层优化），要么对噪声敏感（直接最优性条件最小化）。本研究希望寻求兼具速度和鲁棒性的新方法。

Method: 将原本的双层IOC优化问题重新表述为等价的单层优化问题，并通过数值模拟（仿真）对该方法与经典双层法的性能进行比较，尤其关注噪声条件下的鲁棒性和计算速度。

Result: 实验表明，该单层重构方法即使在极高噪声下也表现出良好鲁棒性，并且在典型任务中（平面到达任务）计算速度提升约15倍。

Conclusion: 新方法兼顾计算效率和对噪声的鲁棒性，为人类运动相关IOC任务提供了更优实现方式，优于传统双层实施。

Abstract: Inverse optimal control (IOC) allows the retrieval of optimal cost function
weights, or behavioral parameters, from human motion. The literature on IOC
uses methods that are either based on a slow bilevel process or a fast but
noise-sensitive minimization of optimality condition violation. Assuming
equality-constrained optimal control models of human motion, this article
presents a faster but robust approach to solving IOC using a single-level
reformulation of the bilevel method and yields equivalent results. Through
numerical experiments in simulation, we analyze the robustness to noise of the
proposed single-level reformulation to the bilevel IOC formulation with a
human-like planar reaching task that is used across recent studies. The
approach shows resilience to very large levels of noise and reduces the
computation time of the IOC on this task by a factor of 15 when compared to a
classical bilevel implementation.

</details>


### [228] [Validation of collision-free spheres of Stewart-Gough platforms for constant orientations using the Application Programming Interface of a CAD software](https://arxiv.org/abs/2510.08408)
*Bibekananda Patra,Rajeevlochana G. Chittawadigi,Sandipan Bandyopadhyay*

Main category: cs.RO

TL;DR: 本文提出了一种利用CAD软件API，自动验证六自由度Stewart-Gough平台机械臂给定姿态下最大无碰撞球（CFS）的方法。通过采样验证机械臂各腿之间的碰撞情况，安全区域即为最大无碰撞球。


<details>
  <summary>Details</summary>
Motivation: 空间并联机器人在实际应用中需要确保结构运动的安全性，尤其在高自由度机械臂中，各运动部件间避免碰撞至关重要。因此，有必要高效、自动地验证给定姿态下最大安全工作空间（CFS），以提高机械臂的实用性和安全性。

Method: 本方法利用CAD软件的API接口，自动控制移动平台在多个采样点位姿下运动，对每一位姿，逐一检测六条腿之间是否发生碰撞。如果任何采样点内部不存在腿间碰撞，则该最大无碰撞球即被验证为安全区域。

Result: 通过对所有采样点的自动化碰撞检测，可以高效准确地验证最大无碰撞球的有效性。此外，该方法可以直接应用于任意空间并联机械臂的安全空间估计与验证。

Conclusion: 所提出的方法不仅能验证事先计算的安全球体，也可推广至其他类型的空间并联机械臂，为实际设计与实现提供可靠的安全性保障。

Abstract: This paper presents a method of validation of the size of the largest
collision-free sphere (CFS) of a 6-6 Stewart-Gough platform manipulator (SGPM)
for a given orientation of its moving platform (MP) using the Application
Programming Interface (API) of a CAD software. The position of the MP is
updated via the API in an automated manner over a set of samples within a shell
enclosing the surface of the CFS. For each pose of the manipulator, each pair
of legs is investigated for mutual collisions. The CFS is considered safe or
validated iff none of the points falling inside the CFS lead to a collision
between any pair of legs. This approach can not only validate the safety of a
precomputed CFS, but also estimate the same for any spatial parallel
manipulator.

</details>


### [229] [Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered](https://arxiv.org/abs/2510.08464)
*Jason Jabbour,Dong-Ki Kim,Max Smith,Jay Patrikar,Radhika Ghosal,Youhui Wang,Ali Agha,Vijay Janapa Reddi,Shayegan Omidshafiei*

Main category: cs.RO

TL;DR: 文章提出了一种名为GLUESTICK的新方法，用于解决剪枝后VLA（视觉-语言-动作）模型性能大幅下降和安全性降低的问题。该方法通过在剪枝及原始模型间插值，补偿性能损失，无需额外训练，能在保持高效率的同时显著恢复机器人任务的成功率和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型因大且复杂，难以部署在硬件资源受限的机器人上。虽然模型剪枝可提升效率，但直接对VLA模型剪枝后，模型性能和安全性大幅下降。因此亟需一种方法，既能保持剪枝后的高效，又能恢复因剪枝导致的能力损失。

Method: GLUESTICK方法在模型剪枝后，通过在原始（未剪枝）和已剪枝模型之间进行权重空间线性插值，得到修正项，并在推理时用这一修正项优化剪枝层，无需增加新的训练流程。此方法与剪枝算法无关，仅引入一个控制效率与准确性权衡的超参数。

Result: 在多种VLA模型结构及多项机器人操作和导航任务实验证明，GLUESTICK在显著提升内存效率的同时，能够大幅度恢复模型任务完成率，并有效减少安全违规。

Conclusion: GLUESTICK是一种简洁高效、易于集成的剪枝后性能恢复方案，在保证模型紧凑的同时，极大减缓了剪枝带来的性能损失和安全隐患，为资源有限硬件上的机器人视觉-语言-动作智能部署提供了更优解。

Abstract: Vision-Language-Action (VLA) models have advanced robotic capabilities but
remain challenging to deploy on resource-limited hardware. Pruning has enabled
efficient compression of large language models (LLMs), yet it is largely
understudied in robotics. Surprisingly, we observe that pruning VLA models
leads to drastic degradation and increased safety violations. We introduce
GLUESTICK, a post-pruning recovery method that restores much of the original
model's functionality while retaining sparsity benefits. Our method performs a
one-time interpolation between the dense and pruned models in weight-space to
compute a corrective term. This correction is used during inference by each
pruned layer to recover lost capabilities with minimal overhead. GLUESTICK
requires no additional training, is agnostic to the pruning algorithm, and
introduces a single hyperparameter that controls the tradeoff between
efficiency and accuracy. Across diverse VLA architectures and tasks in
manipulation and navigation, GLUESTICK achieves competitive memory efficiency
while substantially recovering success rates and reducing safety violations.
Additional material can be found at: https://gluestick-vla.github.io/.

</details>


### [230] [DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos](https://arxiv.org/abs/2510.08475)
*Jhen Hsieh,Kuan-Hsun Tu,Kuo-Han Hung,Tsung-Wei Ke*

Main category: cs.RO

TL;DR: DexMan 是一个自动化框架，可以将人的视觉演示转化为仿人机器人双手灵巧操作技能，不需要复杂的数据采集和标注，在多个基准上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有将人类演示转化为机器人操作的研究，通常需要繁琐的设备或精确标注，且多只处理简单模型（如浮动的手），难以直接控制完整的仿人机器人并学习复杂操控技能。因此，研究者希望能直接从普通视频中学习出复杂、通用且高效的机器人操作策略。

Method: DexMan 直接利用第三人称视频分析人类操作，没有依赖相机标定、深度传感器或高精度3D数据。它引入基于接触的奖励机制，提升了从野外视频中噪声较大的手-物姿态学习时的策略可靠性。此外，DexMan 直接控制仿人机器人进行双手灵巧操控，并可利用真实或合成视频自动生成操作技能。

Result: 在 TACO 基准上的物体姿态估计精度超过现有方法，ADD-S 和 VSD 评价指标分别提高了 0.08 和 0.12。在 OakInk-v2 上，强化学习策略的成功率相比以往方法提升了19%。该方法无需手动采集或昂贵的动捕数据，能高效产生大规模多样操作数据集。

Conclusion: DexMan 实现了无需复杂标注设备、直接从视频习得灵巧操控的自动化流程。它推动了更通用、数据驱动的仿人机器人技能学习，为大规模、通用型操作数据集的高效生成提供了可能。

Abstract: We present DexMan, an automated framework that converts human visual
demonstrations into bimanual dexterous manipulation skills for humanoid robots
in simulation. Operating directly on third-person videos of humans manipulating
rigid objects, DexMan eliminates the need for camera calibration, depth
sensors, scanned 3D object assets, or ground-truth hand and object motion
annotations. Unlike prior approaches that consider only simplified floating
hands, it directly controls a humanoid robot and leverages novel contact-based
rewards to improve policy learning from noisy hand-object poses estimated from
in-the-wild videos.
  DexMan achieves state-of-the-art performance in object pose estimation on the
TACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD.
Meanwhile, its reinforcement learning policy surpasses previous methods by 19%
in success rate on OakInk-v2. Furthermore, DexMan can generate skills from both
real and synthetic videos, without the need for manual data collection and
costly motion capture, and enabling the creation of large-scale, diverse
datasets for training generalist dexterous manipulation.

</details>


### [231] [R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation](https://arxiv.org/abs/2510.08547)
*Xiuwei Xu,Angyuan Ma,Hankun Li,Bingyao Yu,Zheng Zhu,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 本论文提出了一种新的数据生成框架R2RGen，通过增强真实世界的点云观测-动作对，实现了无需仿真和渲染的数据扩充，以提升机器人操作策略在空间泛化上的能力。


<details>
  <summary>Details</summary>
Motivation: 以往实现通用机器人操作策略时，空间泛化能力受限，需要大量不同空间配置下的人类示范样本，采集成本高；同时，现有通过数据增强获得空间多样性的做法大多依赖仿真环境，存在仿真到现实的鸿沟，并且应用场景有限。

Method: R2RGen直接对真实世界的点云观测-动作对进行增强，不需要仿真或渲染过程。其核心包括：1）引入基于注释的细粒度场景与轨迹解析机制，2）提出分组增强策略以处理多对象复杂组合和任务约束，3）增加了摄像头感知处理以确保增强数据与真实3D传感器数据分布一致。

Result: R2RGen在大规模实验中有效提升了数据利用效率，并表现出对移动操作等实际应用和规模扩展的强大潜力。

Conclusion: R2RGen作为一种高效、可即插即用的3D数据生成方法，能够有效提升机器人操作策略的空间泛化能力，减少对大量人工示范的依赖，并适用于移动机器人等多样实际场景。

Abstract: Towards the aim of generalized robotic manipulation, spatial generalization
is the most fundamental capability that requires the policy to work robustly
under different spatial distribution of objects, environment and agent itself.
To achieve this, substantial human demonstrations need to be collected to cover
different spatial configurations for training a generalized visuomotor policy
via imitation learning. Prior works explore a promising direction that
leverages data generation to acquire abundant spatially diverse data from
minimal source demonstrations. However, most approaches face significant
sim-to-real gap and are often limited to constrained settings, such as
fixed-base scenarios and predefined camera viewpoints. In this paper, we
propose a real-to-real 3D data generation framework (R2RGen) that directly
augments the pointcloud observation-action pairs to generate real-world data.
R2RGen is simulator- and rendering-free, thus being efficient and
plug-and-play. Specifically, given a single source demonstration, we introduce
an annotation mechanism for fine-grained parsing of scene and trajectory. A
group-wise augmentation strategy is proposed to handle complex multi-object
compositions and diverse task constraints. We further present camera-aware
processing to align the distribution of generated data with real-world 3D
sensor. Empirically, R2RGen substantially enhances data efficiency on extensive
experiments and demonstrates strong potential for scaling and application on
mobile manipulation.

</details>


### [232] [DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model](https://arxiv.org/abs/2510.08556)
*Xueyi Liu,He Wang,Li Yi*

Main category: cs.RO

TL;DR: 该论文提出了一种新颖的仿真到现实迁移方法，实现了单一策略在现实世界中对多种复杂物体的泛化旋转操控，极大缩小了机器人操作领域的“现实鸿沟”。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，通过灵巧机械手实现一般性的物体随手旋转非常困难，主要由于仿真到现实的策略迁移存在“现实鸿沟”，使得之前的工作往往只能在受限环境或者定制场景下有效。

Method: 作者设计了一套基于关节动态模型的框架，通过将系统动态进行关节分解，实现各关节动态的低维建模，有效利用有限的现实世界数据自动调整仿真策略动作。结合自主数据采集策略，大规模无人工干预地收集多样化的真实交互数据，提升了策略的泛化能力和数据效率。

Result: 该方法实现了单一训练策略可在现实世界成功旋转多种具有复杂形状、高纵横比和小尺寸的对象，并适应各种手腕方向和旋转轴。在真实环境和远程操作应用中的全面实验验证了方法的有效性和鲁棒性。

Conclusion: 作者提出的关节动态分解方法以及自动数据采集流程极大提升了仿真策略在现实世界的泛化能力，为实现更通用、更强适应性的机器人灵巧操控奠定了基础。

Abstract: Achieving generalized in-hand object rotation remains a significant challenge
in robotics, largely due to the difficulty of transferring policies from
simulation to the real world. The complex, contact-rich dynamics of dexterous
manipulation create a "reality gap" that has limited prior work to constrained
scenarios involving simple geometries, limited object sizes and aspect ratios,
constrained wrist poses, or customized hands. We address this sim-to-real
challenge with a novel framework that enables a single policy, trained in
simulation, to generalize to a wide variety of objects and conditions in the
real world. The core of our method is a joint-wise dynamics model that learns
to bridge the reality gap by effectively fitting limited amount of real-world
collected data and then adapting the sim policy's actions accordingly. The
model is highly data-efficient and generalizable across different whole-hand
interaction distributions by factorizing dynamics across joints, compressing
system-wide influences into low-dimensional variables, and learning each
joint's evolution from its own dynamic profile, implicitly capturing these net
effects. We pair this with a fully autonomous data collection strategy that
gathers diverse, real-world interaction data with minimal human intervention.
Our complete pipeline demonstrates unprecedented generality: a single policy
successfully rotates challenging objects with complex shapes (e.g., animals),
high aspect ratios (up to 5.33), and small sizes, all while handling diverse
wrist orientations and rotation axes. Comprehensive real-world evaluations and
a teleoperation application for complex tasks validate the effectiveness and
robustness of our approach. Website: https://meowuu7.github.io/DexNDM/

</details>


### [233] [NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos](https://arxiv.org/abs/2510.08568)
*Hongyu Li,Lingfeng Sun,Yafei Hu,Duy Ta,Jennifer Barry,George Konidaris,Jiahui Fu*

Main category: cs.RO

TL;DR: NovaFlow提出了一种无需演示即可实现零样本操作任务的机器人自主操作框架，通过将任务描述转换为可执行计划，使机器人直接执行新任务。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操控方法要么依赖于分布内任务，要么需要大量与具体机器人形态匹配的数据微调，导致跨平台迁移能力不足。该工作旨在实现通用且零样本迁移的机器人操控。

Method: NovaFlow接受任务描述，首先利用视频生成模型合成任务视频，再用现有感知模块提取视频中的三维物体流。接着，针对刚体物体利用物体流计算相对位姿，并通过抓取提议与轨迹优化生成机器人动作；针对柔体物体，则将物体流作为基于粒子动力学的模型规划跟踪目标，从而简化低层控制和任务理解的耦合。

Result: 在刚体、关节体和柔体的操作任务上，NovaFlow分别在桌面Franka机械臂和Spot四足机器人上进行了验证，成功实现了无需演示或特定机器人训练的零样本执行。

Conclusion: NovaFlow实现了机器人操作中新任务的零样本迁移，能够将高层任务理解与底层控制解耦，展示了跨硬件平台的良好适应及通用性。

Abstract: Enabling robots to execute novel manipulation tasks zero-shot is a central
goal in robotics. Most existing methods assume in-distribution tasks or rely on
fine-tuning with embodiment-matched data, limiting transfer across platforms.
We present NovaFlow, an autonomous manipulation framework that converts a task
description into an actionable plan for a target robot without any
demonstrations. Given a task description, NovaFlow synthesizes a video using a
video generation model and distills it into 3D actionable object flow using
off-the-shelf perception modules. From the object flow, it computes relative
poses for rigid objects and realizes them as robot actions via grasp proposals
and trajectory optimization. For deformable objects, this flow serves as a
tracking objective for model-based planning with a particle-based dynamics
model. By decoupling task understanding from low-level control, NovaFlow
naturally transfers across embodiments. We validate on rigid, articulated, and
deformable object manipulation tasks using a table-top Franka arm and a Spot
quadrupedal mobile robot, and achieve effective zero-shot execution without
demonstrations or embodiment-specific training. Project website:
https://novaflow.lhy.xyz/.

</details>


### [234] [Scalable Offline Metrics for Autonomous Driving](https://arxiv.org/abs/2510.08571)
*Animikh Aich,Adwait Kulkarni,Eshed Ohn-Bar*

Main category: cs.RO

TL;DR: 本文探讨了自动驾驶等机器人系统感知-规划模型的离线评估与在线表现间的脱节，并提出改进离线指标以提高两者相关性的办法。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶模型的安全性多通过离线数据集评估，但离线与在线（真实运行）间的表现差异较大，现有评估指标难以真实反映模型实际效果。探索如何让离线评价结果更好预测在线表现是业界难题。

Method: 作者设计了一系列仿真和现实世界实验，深入分析不同评估指标下离线与在线的相关性，并提出基于模型认知不确定性的离线指标，用以捕获更容易导致实际失误的场景。对正常离线指标和新指标的相关性进行了对比。

Result: 发现传统离线指标与在线表现的相关性比之前研究更低，质疑了现有评估方法的有效性。所提出的不确定性指标与实际在线表现的相关性提升了13%以上，实际道路验证中相关性提升更明显。

Conclusion: 仅依赖当前的离线指标难以全面反映自动驾驶决策模型在现实中的安全性和鲁棒性。通过引入基于不确定性的新指标，可显著提升离线评估和实际表现的相关性，为后续更可靠的模型评估提供了新思路。

Abstract: Real-World evaluation of perception-based planning models for robotic
systems, such as autonomous vehicles, can be safely and inexpensively conducted
offline, i.e., by computing model prediction error over a pre-collected
validation dataset with ground-truth annotations. However, extrapolating from
offline model performance to online settings remains a challenge. In these
settings, seemingly minor errors can compound and result in test-time
infractions or collisions. This relationship is understudied, particularly
across diverse closed-loop metrics and complex urban maneuvers. In this work,
we revisit this undervalued question in policy evaluation through an extensive
set of experiments across diverse conditions and metrics. Based on analysis in
simulation, we find an even worse correlation between offline and online
settings than reported by prior studies, casting doubts on the validity of
current evaluation practices and metrics for driving policies. Next, we bridge
the gap between offline and online evaluation. We investigate an offline metric
based on epistemic uncertainty, which aims to capture events that are likely to
cause errors in closed-loop settings. The resulting metric achieves over 13%
improvement in correlation compared to previous offline metrics. We further
validate the generalization of our findings beyond the simulation environment
in real-world settings, where even greater gains are observed.

</details>


### [235] [BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation](https://arxiv.org/abs/2510.08572)
*Rocktim Jyoti Das,Harsh Singh,Diana Turmakhan,Muhammad Abdullah Sohail,Mingfei Han,Preslav Nakov,Fabio Pizzati,Ivan Laptev*

Main category: cs.RO

TL;DR: 本论文提出BLAZER框架，利用大模型自动规划实现机器人操作任务演示的自动生成，在无人工参与的情况下提升机器人泛化和迁移能力，有效促进零样本学习在模拟和现实中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统视觉与语言领域通过扩大数据与模型规模取得突破，但机器人领域因缺乏大规模多样性演示数据而受限，数据采集成本高。为解决此挑战，须探索自动化、大规模生成训练数据的新方法。

Method: BLAZER框架利用LLM（大语言模型）的零样本推理能力，在模拟环境中自动生成多样操作演示，无需人工标注或监督。成功案例用于微调LLM，增强其机器人任务规划能力。并检验模型从基于状态的模拟转向基于传感器的实际任务能力。

Result: BLAZER显著提升了零样本操作任务的表现，无论在模拟环境还是真实环境均超越基线方法。另外，该方法对训练集以外的新任务依然有效，并支持LLM模型体量的下调。

Conclusion: BLAZER为机器人领域提供了自动化、可迁移、高效的演示数据生成与学习路径，突破了传统高昂人工数据限制，为未来通用机器人政策开发奠定了基础。

Abstract: Scaling data and models has played a pivotal role in the remarkable progress
of computer vision and language. Inspired by these domains, recent efforts in
robotics have similarly focused on scaling both data and model size to develop
more generalizable and robust policies. However, unlike vision and language,
robotics lacks access to internet-scale demonstrations across diverse robotic
tasks and environments. As a result, the scale of existing datasets typically
suffers from the need for manual data collection and curation. To address this
problem, here we propose BLAZER, a framework that learns manipulation policies
from automatically generated training data. We build on the zero-shot
capabilities of LLM planners and automatically generate demonstrations for
diverse manipulation tasks in simulation. Successful examples are then used to
finetune an LLM and to improve its planning capabilities without human
supervision. Notably, while BLAZER training requires access to the simulator's
state, we demonstrate direct transfer of acquired skills to sensor-based
manipulation. Through extensive experiments, we show BLAZER to significantly
improve zero-shot manipulation in both simulated and real environments.
Moreover, BLAZER improves on tasks outside of its training pool and enables
downscaling of LLM models. Our code and data will be made publicly available on
the project page.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [236] [AI-Driven Radiology Report Generation for Traumatic Brain Injuries](https://arxiv.org/abs/2510.08498)
*Riadh Bouslimi,Houda Trabelsi,Wahiba Ben Abdssalem Karaa,Hana Hedhli*

Main category: eess.IV

TL;DR: 本文提出了一种结合AC-BiFPN与Transformer架构的AI模型，用于自动生成颅脑创伤影像诊断报告，并在准确性和文本生成方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 颅脑损伤在急诊医学中诊断难度大，医学影像的及时解读直接关系到患者预后。手工撰写影像报告耗时且易因高压环境出错，因此亟需自动化工具提升诊断效率与准确性。

Method: 模型采用AC-BiFPN实现多尺度医学图像（如CT、MRI）特征提取，增强对复杂异常（如颅内出血）的检测能力；利用Transformer对提取的特征建模，实现上下文相关、连贯且专业的影像诊断报告自动生成。

Result: 在RSNA颅内出血检测数据集上测试，模型在诊断准确率和报告生成质量两方面均优于传统CNN方法。

Conclusion: 将先进的特征提取与Transformer文本生成结合，能提升颅脑损伤诊断的自动化水平，既辅助专业放射科医生，又对医学生提供实时反馈和教学支持，有助于优化临床决策过程。

Abstract: Traumatic brain injuries present significant diagnostic challenges in
emergency medicine, where the timely interpretation of medical images is
crucial for patient outcomes. In this paper, we propose a novel AI-based
approach for automatic radiology report generation tailored to cranial trauma
cases. Our model integrates an AC-BiFPN with a Transformer architecture to
capture and process complex medical imaging data such as CT and MRI scans. The
AC-BiFPN extracts multi-scale features, enabling the detection of intricate
anomalies like intracranial hemorrhages, while the Transformer generates
coherent, contextually relevant diagnostic reports by modeling long-range
dependencies. We evaluate the performance of our model on the RSNA Intracranial
Hemorrhage Detection dataset, where it outperforms traditional CNN-based models
in both diagnostic accuracy and report generation. This solution not only
supports radiologists in high-pressure environments but also provides a
powerful educational tool for trainee physicians, offering real-time feedback
and enhancing their learning experience. Our findings demonstrate the potential
of combining advanced feature extraction with transformer-based text generation
to improve clinical decision-making in the diagnosis of traumatic brain
injuries.

</details>
