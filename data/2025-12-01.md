<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 205]
- [cs.CL](#cs.CL) [Total: 104]
- [cs.RO](#cs.RO) [Total: 44]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SO-Bench: A Structural Output Evaluation of Multimodal LLMs](https://arxiv.org/abs/2511.21750)
*Di Feng,Kaixin Ma,Feng Nan,Haofeng Chen,Bohan Zhai,David Griffiths,Mingfei Gao,Zhe Gan,Eshan Verma,Yinfei Yang,Zhifeng Chen,Afshin Dehghan*

Main category: cs.CV

TL;DR: 本文提出并发布了SO-Bench基准，系统性评估多模态大模型（MLLMs）在视觉输入下的结构化输出能力，对比分析当前主流模型在不同视觉场景中的表现，并通过训练实验显著提升模型结构化输出性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型正被广泛应用于需要结构化输出的实际场景，但针对视觉输入下基于特定数据结构的细致信息抽取与推理尚无系统性评测基准，现有模型在该项任务中表现有限，亟需完善评测工具与提升方法。

Method: 作者设计并构建了SO-Bench基准，包括涵盖UI界面、自然图像、文档、图表四类视觉场景的6,500余个JSON结构和1,800余个人工审核的图片-结构对。通过该基准，对开源及前沿闭源多模态模型进行结构化输出能力评测，并进一步开展结构化输出能力提升的训练实验。

Result: 实验表明，不论开源还是闭源最先进的多模态大模型在结构化输出的准确性和合规性方面均存在明显不足。通过特定训练实验，显著提升了模型结构化输出的表现。

Conclusion: SO-Bench作为首个系统评测MLLMs视觉结构化输出的基准，有助于发现模型在该领域的不足，并为未来多模态结构化推理与生成能力的提升指明方向。基准随后会对学术社区公开。

Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.

</details>


### [2] [Saddle-Free Guidance: Improved On-Manifold Sampling without Labels or Additional Training](https://arxiv.org/abs/2511.21863)
*Eric Yeats,Darryl Hannan,Wilson Fearn,Timothy Doster,Henry Kvinge,Scott Mahan*

Main category: cs.CV

TL;DR: 本文提出了一种新的分数引导方法“Saddle-Free Guidance (SFG)”，无需额外训练或标注数据，提升了生成模型的表现，并在ImageNet等数据集上取得了最优或领先的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前流行的分数生成模型的引导方法（如Classifier-Free Guidance和Auto-Guidance）需要额外的标注数据或模型训练，限制了无标注情况下的应用。作者希望找到一种不依赖额外模型也不需要标注的新引导方法。

Method: 作者发现对数密度在鞍点区域的正曲率可以有效引导分数模型。基于这一发现，提出了SFG方法：直接用单个已有的分数模型，通过维护对数密度最大正曲率的估计来引导采样，无需额外训练或标注，并可直接应用到主流扩散与流匹配模型。

Result: 实验证明，SFG在ImageNet-512等任务上的无条件采样达到了最先进的FID和FD-DINOv2指标；与Auto-Guidance结合后，FD-DINOv2分数达到当前最优。同时，实验显示SFG在提升生成多样性的同时，仍能保持高保真与良好提示遵从度。

Conclusion: SFG为分数生成模型提供了一种高效、无需标注或额外训练的新引导方法，兼具出色的生成质量和样本多样性，对现有扩散与流模型具有直接实用价值。

Abstract: Score-based generative models require guidance in order to generate plausible, on-manifold samples. The most popular guidance method, Classifier-Free Guidance (CFG), is only applicable in settings with labeled data and requires training an additional unconditional score-based model. More recently, Auto-Guidance adopts a smaller, less capable version of the original model to guide generation. While each method effectively promotes the fidelity of generated data, each requires labeled data or the training of additional models, making it challenging to guide score-based models when (labeled) training data are not available or training new models is not feasible.
  We make the surprising discovery that the positive curvature of log density estimates in saddle regions provides strong guidance for score-based models. Motivated by this, we develop saddle-free guidance (SFG) which maintains estimates of maximal positive curvature of the log density to guide individual score-based models. SFG has the same computational cost of classifier-free guidance, does not require additional training, and works with off-the-shelf diffusion and flow matching models. Our experiments indicate that SFG achieves state-of-the-art FID and FD-DINOv2 metrics in single-model unconditional ImageNet-512 generation. When SFG is combined with Auto-Guidance, its unconditional samples achieve general state-of-the-art in FD-DINOv2 score. Our experiments with FLUX.1-dev and Stable Diffusion v3.5 indicate that SFG boosts the diversity of output images compared to CFG while maintaining excellent prompt adherence and image fidelity.

</details>


### [3] [UniArt: Unified 3D Representation for Generating 3D Articulated Objects with Open-Set Articulation](https://arxiv.org/abs/2511.21887)
*Bu Jin,Weize Li,Songen Gu,Yupeng Zheng,Yuhang Zheng,Zhengyi Zhou,Yao Yao*

Main category: cs.CV

TL;DR: 该论文提出了UniArt框架，可通过扩散模型从单张图片端到端地合成具有完整关节结构的3D物体，并在PartNet-Mobility基准上取得了目前最好的网格质量和运动准确率。


<details>
  <summary>Details</summary>
Motivation: 当前在仿真和机器人等领域对三维可动（关节化）物体的需求很大，但手动建模非常费时且难以大规模扩展。已有方法多为多阶段流程，效率和通用性受限。

Method: 提出了一个基于扩散模型的端到端框架UniArt，利用统一的潜在表达对物体的几何、纹理、部件分割和运动参数联合编码。创设了可逆的关节-体素嵌入方式，将关节特征与体素几何空间对齐。此外，将关节类别的预测问题设为开放集任务，不依赖固定语义，可适配新类型和类别。

Result: 在PartNet-Mobility基准上，UniArt在三维网格质量和关节动作准确性两项指标上都优于现有方法，达到最新最好水平。

Conclusion: UniArt为三维可动物体的自动合成提出了一种更统一、更具泛化性的解决方案，有效提升了自动化三维资产生成的效率和质量。

Abstract: Articulated 3D objects play a vital role in realistic simulation and embodied robotics, yet manually constructing such assets remains costly and difficult to scale. In this paper, we present UniArt, a diffusion-based framework that directly synthesizes fully articulated 3D objects from a single image in an end-to-end manner. Unlike prior multi-stage techniques, UniArt establishes a unified latent representation that jointly encodes geometry, texture, part segmentation, and kinematic parameters. We introduce a reversible joint-to-voxel embedding, which spatially aligns articulation features with volumetric geometry, enabling the model to learn coherent motion behaviors alongside structural formation. Furthermore, we formulate articulation type prediction as an open-set problem, removing the need for fixed joint semantics and allowing generalization to novel joint categories and unseen object types. Experiments on the PartNet-Mobility benchmark demonstrate that UniArt achieves state-of-the-art mesh quality and articulation accuracy.

</details>


### [4] [PathReasoning: A multimodal reasoning agent for query-based ROI navigation on whole-slide images](https://arxiv.org/abs/2511.21902)
*Kunpeng Zhang,Hanwen Xu,Sheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为PathReasoning的多模态推理智能体，能够通过多轮推理和自我反思智能地在数字化全切片病理图像(WSIs)中导航，自动找到诊断相关信息区域(ROI)，无需密集像素级标注，并提升了病理分析和报告的准确率。


<details>
  <summary>Details</summary>
Motivation: 巨幅全切片病理图像(WSIs)常用于癌症的诊断和预后判断，但由于其数据量极大(可达百亿像素)，人工或传统算法难以高效、准确地定位关键信息区域。研究动机是借鉴病理医生的观察与推理流程，实现自动化且高效的WSI分析，提高临床实用性与准确性。

Method: 作者提出PathReasoning方法，模拟病理医生先抽样再反思、推理的诊断流程。该方法以多轮“采样-自省-推理-再采样”的形式展开，起始随机采样候选区域并逐步检视与临床问题的关联，通过构建推理链实现视野集中于最具诊断价值的区域。该过程无需像素级标注，并能够为不同问题动态选择ROI，兼具高效性和可解释性。

Result: PathReasoning在亚型分型与纵向追踪分析任务上，比当前最强ROI选择方法分别高出6.7%和3.1%的AUROC，并且能将ROI用于乳腺癌报告自动生成，在准确性上比GPT-4o高10%。推理链的生成还能保证溯源、解释与高效阅读。

Conclusion: PathReasoning能够以高效、有效且可解释的方式探索与临床问题相关的关键信息区域，极大推动了WSI自动分析的智能化与实用性，为数字病理辅助诊断、报告生成和证据追溯提供了有力手段。

Abstract: Deciphering tumor microenvironment from Whole Slide Images (WSIs) is intriguing as it is key to cancer diagnosis, prognosis and treatment response. While these gigapixel images on one hand offer a comprehensive portrait of cancer, on the other hand, the extremely large size, as much as more than 10 billion pixels, make it challenging and time-consuming to navigate to corresponding regions to support diverse clinical inspection. Inspired by pathologists who conducted navigation on WSIs with a combination of sampling, reasoning and self-reflection, we proposed "PathReasoning", a multi-modal reasoning agent that iteratively navigates across WSIs through multiple rounds of reasoning and refinements. Specifically, starting with randomly sampled candidate regions, PathReasoning reviews current selections with self-reflection, reasoning over the correspondence between visual observations and clinical questions, and concludes by proposing new regions to explore. Across rounds, PathReasoning builds a reasoning chain that gradually directs attention to diagnostically relevant areas. PathReasoning turns each whole slide into a sequence of question-guided views, allowing the model to efficiently find informative ROIs within a fixed number of steps, without the need for dense pixel-level annotations. PathReasoning can substantially outperform strong ROI-selection approaches by 6.7% and 3.1% of AUROC on subtyping and longitudinal analysis tasks. The high-quality ROIs further support accurate report generation on breast cancer, significantly outperforming the standard GPT-4o by 10% in accuracy. PathReasoning prioritizes question-specific regions and constructs interpretable reasoning chains, supporting efficient slide review, consistent diagnostic interpretations, comprehensive reporting, and evidence traceability in digital pathology.

</details>


### [5] [Adaptive Parameter Optimization for Robust Remote Photoplethysmography](https://arxiv.org/abs/2511.21903)
*Cecilia G. Morales,Fanurs Chi En Teh,Kai Li,Pushpak Agrawal,Artur Dubrawski*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、能自动适应多种环境的rPPG算法PRISM，突破了现有方法对固定参数的依赖，在不同光照与摄像头设置下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有rPPG方法严重依赖特定条件下调优的固定参数，导致在多变的实际部署环境中适应性较差，亟需一种能自动适应不同条件的方法。

Method: 提出PRISM算法：一种基于投影的鲁棒信号混合方法，通过在线参数自适应与信号质量评估，联合优化光度去趋势和颜色混合过程，无需训练即可使用。

Result: 在PURE和UBFC-rPPG数据集上，PRISM的心率平均绝对误差分别为0.77和0.66 bpm，在5 bpm阈值下准确率分别为97.3%和97.5%，统计分析显示其与顶尖有监督方法表现无显著差异（p>0.2），且可实时运行于CPU。

Conclusion: PRISM算法验证了自适应时序优化在rPPG中的显著优势，实现了无需训练的数据自适应，推动rPPG在复杂环境下的实用性。

Abstract: Remote photoplethysmography (rPPG) enables contactless vital sign monitoring using standard RGB cameras. However, existing methods rely on fixed parameters optimized for particular lighting conditions and camera setups, limiting adaptability to diverse deployment environments. This paper introduces the Projection-based Robust Signal Mixing (PRISM) algorithm, a training-free method that jointly optimizes photometric detrending and color mixing through online parameter adaptation based on signal quality assessment. PRISM achieves state-of-the-art performance among unsupervised methods, with MAE of 0.77 bpm on PURE and 0.66 bpm on UBFC-rPPG, and accuracy of 97.3\% and 97.5\% respectively at a 5 bpm threshold. Statistical analysis confirms PRISM performs equivalently to leading supervised methods ($p > 0.2$), while maintaining real-time CPU performance without training. This validates that adaptive time series optimization significantly improves rPPG across diverse conditions.

</details>


### [6] [Interpretable Multimodal Cancer Prototyping with Whole Slide Images and Incompletely Paired Genomics](https://arxiv.org/abs/2511.21937)
*Yupei Zhang,Yating Huang,Wanming Hu,Lequan Yu,Hujun Yin,Chao Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态原型框架，能够有效整合全片组织切片图像和不完整的基因组数据，用于精准肿瘤学，解决了多模态整合和数据不全的问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的多模态方法在组织学和基因组学整合上受到表型和基因多样性的影响，导致模态内部表示质量不佳，且不同模态间难以有效整合。此外，现实临床中基因组数据常常部分缺失甚至完全不可用，现有方法对此考虑不足。

Method: 作者提出了一个灵活的多模态原型框架，包含四个核心模块：1）基于文本提示和原型权重的生物学原型生成；2）通过样本和分布对齐实现多视角对齐；3）双向融合以捕获共性和特异性信息；4）语义基因组补全模块来处理基因数据缺失。

Result: 大量实验证据表明，该方法在多项下游任务中相较于最新的同类方法表现出持续和显著的优越性。

Conclusion: 所提出的多模态原型框架不仅在整合完整/不完整的多源数据方面效果优异，还更好地适应了实际临床场景，对精准肿瘤学的研究与应用具有重要意义。

Abstract: Multimodal approaches that integrate histology and genomics hold strong potential for precision oncology. However, phenotypic and genotypic heterogeneity limits the quality of intra-modal representations and hinders effective inter-modal integration. Furthermore, most existing methods overlook real-world clinical scenarios where genomics may be partially missing or entirely unavailable. We propose a flexible multimodal prototyping framework to integrate whole slide images and incomplete genomics for precision oncology. Our approach has four key components: 1) Biological Prototyping using text prompting and prototype-wise weighting; 2) Multiview Alignment through sample- and distribution-wise alignments; 3) Bipartite Fusion to capture both shared and modality-specific information for multimodal fusion; and 4) Semantic Genomics Imputation to handle missing data. Extensive experiments demonstrate the consistent superiority of the proposed method compared to other state-of-the-art approaches on multiple downstream tasks. The code is available at https://github.com/helenypzhang/Interpretable-Multimodal-Prototyping.

</details>


### [7] [AmodalGen3D: Generative Amodal 3D Object Reconstruction from Sparse Unposed Views](https://arxiv.org/abs/2511.21945)
*Junwei Zhou,Yu-Wing Tai*

Main category: cs.CV

TL;DR: 该论文提出了一种名为AmodalGen3D的生成式框架，能从少量无序、有遮挡的视角中，推理出完整的无遮挡三维物体形状和外观，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，三维重建经常面临视角稀疏、物体遮挡等问题，导致物体表面大量区域没有被直接观测到，传统多视图或修补方法在这种情况下往往效果不佳。

Method: AmodalGen3D结合了2D无遮挡补全先验，以及多视角立体几何约束。其核心包括稀疏视图特征融合的视图相关交互注意力机制和未观测结构推理的立体条件交互注意力模块，实现对观测区域和隐藏区域的联合建模。

Result: 在合成和真实数据集上，AmodalGen3D在视角稀疏且遮挡严重的情况下，三维重建的完整性和保真度表现优异。

Conclusion: AmodalGen3D很好地解决了稀疏、多遮挡视图下三维物体重建的难题，对机器人、增强/虚拟现实、具身智能等多种应用领域具有重要意义。

Abstract: Reconstructing 3D objects from a few unposed and partially occluded views is a common yet challenging problem in real-world scenarios, where many object surfaces are never directly observed. Traditional multi-view or inpainting-based approaches struggle under such conditions, often yielding incomplete or geometrically inconsistent reconstructions. We introduce AmodalGen3D, a generative framework for amodal 3D object reconstruction that infers complete, occlusion-free geometry and appearance from arbitrary sparse inputs. The model integrates 2D amodal completion priors with multi-view stereo geometry conditioning, supported by a View-Wise Cross Attention mechanism for sparse-view feature fusion and a Stereo-Conditioned Cross Attention module for unobserved structure inference. By jointly modeling visible and hidden regions, AmodalGen3D faithfully reconstructs 3D objects that are consistent with sparse-view constraints while plausibly hallucinating unseen parts. Experiments on both synthetic and real-world datasets demonstrate that AmodalGen3D achieves superior fidelity and completeness under occlusion-heavy sparse-view settings, addressing a pressing need for object-level 3D scene reconstruction in robotics, AR/VR, and embodied AI applications.

</details>


### [8] [TAPVid-360: Tracking Any Point in 360 from Narrow Field of View Video](https://arxiv.org/abs/2511.21946)
*Finlay G. C. Hudson,James A. D. Gardner,William A. P. Smith*

Main category: cs.CV

TL;DR: 论文提出了TAPVid-360新任务和数据集，要求视频系统预测全景内场景点的3D方位，实现比传统方法更强的持久与全景理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前人工视觉系统在理解场景时常采用以自身为中心的逐帧处理方式，缺乏人类那样的全景、持久和对象追踪能力；特别是在Track Any Point (TAP)任务，当感兴趣点离开视野后，现有方法无法继续跟踪。

Method: 作者提出TAPVid-360任务，要求系统在360度视频中，准确预测查找点在场景中的3D方向，无论该点是否在可见视野内。训练中不需动态4D真值模型，仅采用360度视频重采生成带真实方向的窄视野片段。他们还引入了TAPVid360-10k数据集，包含1万段带有真实方向点跟踪的透视视频。作者以CoTracker v3为基础，改进为可预测每点旋转方向的基线模型。

Result: 改进后的基线方法在新任务和数据集上显著优于现有TAP和TAPVid 3D方法。

Conclusion: 该工作推动人工视觉系统向着具备全景、持久、和脱离视野点追踪能力的方向发展，提供了新任务、数据集和有效基线，为后续研究打下基础。

Abstract: Humans excel at constructing panoramic mental models of their surroundings, maintaining object permanence and inferring scene structure beyond visible regions. In contrast, current artificial vision systems struggle with persistent, panoramic understanding, often processing scenes egocentrically on a frame-by-frame basis. This limitation is pronounced in the Track Any Point (TAP) task, where existing methods fail to track 2D points outside the field of view. To address this, we introduce TAPVid-360, a novel task that requires predicting the 3D direction to queried scene points across a video sequence, even when far outside the narrow field of view of the observed video. This task fosters learning allocentric scene representations without needing dynamic 4D ground truth scene models for training. Instead, we exploit 360 videos as a source of supervision, resampling them into narrow field-of-view perspectives while computing ground truth directions by tracking points across the full panorama using a 2D pipeline. We introduce a new dataset and benchmark, TAPVid360-10k comprising 10k perspective videos with ground truth directional point tracking. Our baseline adapts CoTracker v3 to predict per-point rotations for direction updates, outperforming existing TAP and TAPVid 3D methods.

</details>


### [9] [WalkCLIP: Multimodal Learning for Urban Walkability Prediction](https://arxiv.org/abs/2511.21947)
*Shilong Xiang,JangHyeon Lee,Min Namgung,Yao-Yi Chiang*

Main category: cs.CV

TL;DR: 本文提出了WalkCLIP，一种多模态框架，通过融合卫星图像、街景图像和人口动态数据来全面预测城市可步行性，并在明尼阿波利斯-圣保罗地区的大规模实验证明其精度优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有步行性评估方法依赖于人工调查，成本高且难以大规模实施。单一数据来源（如卫星图像、街景或人口指标）只能反映步行环境的部分特征，因此亟需一种能整合多维度信息的高效评估方法。

Method: WalkCLIP整合了三种数据：使用GPT-4o生成的图像文本描述学习步行性相关的视觉-语言表征；通过空间聚合模块引入邻域信息，提升表示的空间一致性；最后将视觉与人口动态基础模型的特征进行融合，以预测城市区域的步行性。

Result: 在明尼阿波利斯-圣保罗的4,660个地点实验中，WalkCLIP在预测准确性和空间一致性方面均优于各种单模态或多模态基线方法。

Conclusion: 多模态信号的融合显著提升了城市步行环境预测的可靠性，为大规模城市健康与可持续性评估提供了一种新型工具。

Abstract: Urban walkability is a cornerstone of public health, sustainability, and quality of life. Traditional walkability assessments rely on surveys and field audits, which are costly and difficult to scale. Recent studies have used satellite imagery, street view imagery, or population indicators to estimate walkability, but these single-source approaches capture only one dimension of the walking environment. Satellite data describe the built environment from above, but overlook the pedestrian perspective. Street view imagery captures conditions at the ground level, but lacks broader spatial context. Population dynamics reveal patterns of human activity but not the visual form of the environment. We introduce WalkCLIP, a multimodal framework that integrates these complementary viewpoints to predict urban walkability. WalkCLIP learns walkability-aware vision-language representations from GPT-4o generated image captions, refines these representations with a spatial aggregation module that incorporates neighborhood context, and fuses the resulting features with representations from a population dynamics foundation model. Evaluated at 4,660 locations throughout Minneapolis-Saint Paul, WalkCLIP outperforms unimodal and multimodal baselines in both predictive accuracy and spatial alignment. These results show that the integration of visual and behavioral signals yields reliable predictions of the walking environment.

</details>


### [10] [DeepGI: Explainable Deep Learning for Gastrointestinal Image Classification](https://arxiv.org/abs/2511.21959)
*Walid Houmaidi,Mohamed Hadadi,Youssef Sabiri,Yousra Chtouki*

Main category: cs.CV

TL;DR: 本文对胃肠道内镜图像分类进行了深度模型对比研究，提出了新的基准和解释性分析，提升了自动化疾病诊断的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着内镜检查在胃肠疾病诊断中的广泛应用，自动化识别多种疾病的需求日益增长，但实际成像中存在光照变化、角度不一和杂散伪影等现实挑战，因而需要开发鲁棒且可解释的AI方法。

Method: 作者收集整理了包含4种关键病变、共4000张内镜图片的新型数据集，采用VGG16、MobileNetV2和Xception等先进深度学习模型进行分类，并结合Grad-CAM实现模型决策区域的可视化解释。

Result: VGG16和MobileNetV2在测试集上均取得96.5%的高准确率，Xception达到94.24%，并通过可视化强化了模型输出的临床可解释性。

Conclusion: 该研究提升了复杂环境下胃肠道图像自动分析的准确性和可解释性，为相关领域提供了新的基准数据和方法参考，强调了多样数据集和模型可解释性在医学AI研究中的重要性。

Abstract: This paper presents a comprehensive comparative model analysis on a novel gastrointestinal medical imaging dataset, comprised of 4,000 endoscopic images spanning four critical disease classes: Diverticulosis, Neoplasm, Peritonitis, and Ureters. Leveraging state-of-the-art deep learning techniques, the study confronts common endoscopic challenges such as variable lighting, fluctuating camera angles, and frequent imaging artifacts. The best performing models, VGG16 and MobileNetV2, each achieved a test accuracy of 96.5%, while Xception reached 94.24%, establishing robust benchmarks and baselines for automated disease classification. In addition to strong classification performance, the approach includes explainable AI via Grad-CAM visualization, enabling identification of image regions most influential to model predictions and enhancing clinical interpretability. Experimental results demonstrate the potential for robust, accurate, and interpretable medical image analysis even in complex real-world conditions. This work contributes original benchmarks, comparative insights, and visual explanations, advancing the landscape of gastrointestinal computer-aided diagnosis and underscoring the importance of diverse, clinically relevant datasets and model explainability in medical AI research.

</details>


### [11] [PAT3D: Physics-Augmented Text-to-3D Scene Generation](https://arxiv.org/abs/2511.21978)
*Guying Lin,Kemeng Huang,Michael Liu,Ruihan Gao,Hanke Chen,Lyuhao Chen,Beijia Lu,Taku Komura,Yuan Liu,Jun-Yan Zhu,Minchen Li*

Main category: cs.CV

TL;DR: PAT3D 是首个结合视觉-语言模型与物理仿真的文本生成3D场景框架，能够输出物理上合理、无交错、可用于仿真的高质量3D场景。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成3D场景方法通常忽视物理合理性，导致生成结果难以直接用于仿真、机器人等实际应用。缺乏物体之间物理交互的建模限制了这些方法的实用性，因此需要将物理约束引入生成过程，提升场景的真实性和功能性。

Method: PAT3D 首次将物理仿真引入文本生成3D场景流程。流程包括：1) 文本提示下生成3D物体；2) 推断并组织空间关系并构建分层场景树；3) 转化为仿真初始状态，使用可微刚体物理仿真保证物体交互和场景物理合理；4) 引入仿真环优化，使场景既物理稳定又与文本语义高度一致。

Result: 实验表明，PAT3D 在物理合理性、语义一致性和视觉质量指标上均显著优于以往方法，生成场景更稳健，可直接支持后续场景编辑和机器人操作等任务。

Conclusion: PAT3D 不仅能生成高质量、无交叠且物理合理的3D场景，还将3D生成与仿真应用成功链接起来，显著拓展了下游任务的可能性。

Abstract: We introduce PAT3D, the first physics-augmented text-to-3D scene generation framework that integrates vision-language models with physics-based simulation to produce physically plausible, simulation-ready, and intersection-free 3D scenes. Given a text prompt, PAT3D generates 3D objects, infers their spatial relations, and organizes them into a hierarchical scene tree, which is then converted into initial conditions for simulation. A differentiable rigid-body simulator ensures realistic object interactions under gravity, driving the scene toward static equilibrium without interpenetrations. To further enhance scene quality, we introduce a simulation-in-the-loop optimization procedure that guarantees physical stability and non-intersection, while improving semantic consistency with the input prompt. Experiments demonstrate that PAT3D substantially outperforms prior approaches in physical plausibility, semantic consistency, and visual quality. Beyond high-quality generation, PAT3D uniquely enables simulation-ready 3D scenes for downstream tasks such as scene editing and robotic manipulation. Code and data will be released upon acceptance.

</details>


### [12] [DialBench: Towards Accurate Reading Recognition of Pointer Meter using Large Foundation Models](https://arxiv.org/abs/2511.21982)
*Futian Wang,Chaoliu Weng,Xiao Wang,Zhen Chen,Zhicheng Zhao,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出了一个大规模指针仪表读数数据集（RPM-10K），并基于此数据集提出了一种结合视觉与语言的物理关系注入模型（MRLM），以提升指针仪表智能识别的精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前智能电力系统中，准确识别指针仪表读数十分关键。然而在实际应用中，由于反光、遮挡、动态视角等问题，以及指针与刻度线过于接近，现有方法表现脆弱。同时，该领域尚缺乏大规模、挑战性强的数据集作为研究基础。本文旨在填补这一空白，并提升读数算法的泛化与实用性。

Method: 本文首先构建了包含10730张图像的大规模指针仪表数据集RPM-10K，涵盖多种实际场景中的关键挑战。随后提出了基于视觉-语言融合的指针读数模型MRLM。不同于依赖图像整体特征的传统方法，该模型显式建模指针与刻度间的几何与因果物理关系，通过跨注意力融合及自适应专家选择，增强对表盘结构的推理和数值读数能力。

Result: 在RPM-10K数据集上进行的大量实验证明：所提出的MRLM模型在指针仪表读数任务上取得了优越的性能，具备更好的数值精度与对复杂环境的适应性。

Conclusion: 本文的RPM-10K数据集和MRLM模型在解决指针仪表识别中的数据瓶颈和方法瓶颈上均取得突破，可促进该领域算法的研究与应用。相关数据集及代码已开源，为后续研究提供了重要的基础资源。

Abstract: The precise reading recognition of pointer meters plays a key role in smart power systems, but existing approaches remain fragile due to challenges like reflections, occlusions, dynamic viewing angles, and overly between thin pointers and scale markings. Up to now, this area still lacks large-scale datasets to support the development of robust algorithms. To address these challenges, this paper first presents a new large-scale benchmark dataset for dial reading, termed RPM-10K, which contains 10730 meter images that fully reflect the aforementioned key challenges. Built upon the dataset, we propose a novel vision-language model for pointer meter reading recognition, termed MRLM, based on physical relation injection. Instead of exhaustively learning image-level correlations, MRLM explicitly encodes the geometric and causal relationships between the pointer and the scale, aligning perception with physical reasoning in the spirit of world-model perspectives. Through cross-attentional fusion and adaptive expert selection, the model learns to interpret dial configurations and generate precise numeric readings. Extensive experiments fully validated the effectiveness of our proposed framework on the newly proposed benchmark dataset. Both the dataset and source code will be released on https://github.com/Event-AHU/DialBench

</details>


### [13] [PPBoost: Progressive Prompt Boosting for Text-Driven Medical Image Segmentation](https://arxiv.org/abs/2511.21984)
*Xuchen Li,Hengrui Gu,Mohan Zhang,Qin Liu,Zhen Tan,Xinyuan Zhu,Huixue Zhou,Tianlong Chen,Kaixiong Zhou*

Main category: cs.CV

TL;DR: PPBoost提出了一种无需标注数据、将文本提示转化为空间精准提示以提升医学图像分割性能的新框架，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以文本为提示的大模型能直观响应自然语言，但空间精度低且对领域变化不鲁棒；而视觉提示（如bbox）虽性能好但获取成本高。医学图像分割亟需兼具低成本和高性能的方法。

Method: PPBoost利用视觉-语言模型，先将文本描述生成伪bbox（pseudo-bboxes），并用不确定性筛选高质量bbox-图片对，训练伪标注检测器，进一步产生高质量bbox并在推断时自动扩展、加强空间信息，最终作为提示引导分割模型获取密集掩码。全流程无需像素级标签。

Result: 在三组不同模态和解剖结构的数据集上，PPBoost在Dice与Normalized Surface Distance指标上均超越了纯文本和纯视觉提示的基线，并超过了若干少样本分割模型。

Conclusion: PPBoost能在无需标注数据的情况下，将弱文本提示有效转化为空间强提示，通用性强、能适配多种分割骨干模型，对临床实际具有较大应用潜力。

Abstract: Text-prompted foundation models for medical image segmentation offer an intuitive way to delineate anatomical structures from natural language queries, but their predictions often lack spatial precision and degrade under domain shift. In contrast, visual-prompted models achieve strong segmentation performance across diverse modalities by leveraging spatial cues of precise bounding-box (bbox) prompts to guide the segmentation of target lesions. However, it is costly and challenging to obtain the precise visual prompts in clinical practice. We propose PPBoost (Progressive Prompt-Boosting), a framework that bridges these limitations by transforming weak text-derived signals into strong, spatially grounded visual prompts, operating under a strict zero-shot regime with no image- or pixel-level segmentation labels. PPBoost first uses a vision-language model to produce initial pseudo-bboxes conditioned on the textual object descriptions and applies an uncertainty-aware criterion to filter unreliable predictions. The retained image-bboxes pairs are then leveraged to train a pseudo-labeled detector, producing the high-quality bboxes for the query images. During inference, PPBoost further refines the generated bboxes by appropriately expanding them to tightly cover the target anatomical structures. The enhanced spatially-grounding bbox prompts guide existing segmentation models to generate final dense masks, effectively amplifying weak text cues into strong spatial guidance. Across three datasets spanning diverse modalities and anatomies, PPBoost consistently improves Dice and Normalized Surface Distance over text- and visual-prompted baselines and, notably, surpasses few-shot segmentation models without using labeled data. PPBoost can generalize to multiple typical visual segmentation model backbones.

</details>


### [14] [Can Multi-Modal LLMs Provide Live Step-by-Step Task Guidance?](https://arxiv.org/abs/2511.21998)
*Apratim Bhattacharyya,Bicheng Xu,Sanjay Haresh,Reza Pourreza,Litian Liu,Sunny Panchal,Pulkit Madan,Leonid Sigal,Roland Memisevic*

Main category: cs.CV

TL;DR: 本文提出了一套全新的多模态大语言模型评测基准——Qualcomm Interactive Cooking，用于实时互动指导，并首次包含用户操作过程中的错误检测和反馈。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型虽然会话能力提升，但难以实时指导，如烹饪等互动场景，尤其在检测执行效果与实时纠错方面不足，缺乏专门的数据集和评测基准。

Method: 基于CaptainCook4D视频数据，作者构建了Qualcomm Interactive Cooking数据集，包含用户实际操作及出错、纠正等完整过程，用精细标注的步骤、反馈和错误预警打上时间戳。作者还提出了LiveMamba模型，能流式处理视频，实现指导、纠错等互动功能。

Result: 实验评测了目前最先进的多模态大模型在新基准上的表现，并将LiveMamba模型作为流式互动指导的强基线。

Conclusion: 该工作为‘实时、情境化指导型’AI建立了首个评测标准和基线，有助于促进后续在互动AI助理领域的研究与发展。

Abstract: Multi-modal Large Language Models (LLM) have advanced conversational abilities but struggle with providing live, interactive step-by-step guidance, a key capability for future AI assistants. Effective guidance requires not only delivering instructions but also detecting their successful execution, as well as identifying and alerting users to mistakes, all of which has to happen in real-time. This requires models that are not turn-based, but that can react asynchronously to a video stream, as well as video data showing users performing tasks including mistakes and their corrections. To this end, we introduce Qualcomm Interactive Cooking, a new benchmark and dataset built upon CaptainCook4D, which contains user mistakes during task execution. Our dataset and benchmark features densely annotated, timed instructions and feedback messages, specifically including mistake alerts precisely timestamped to their visual occurrence in the video. We evaluate state-of-the-art multi-modal LLMs on the Qualcomm Interactive Cooking benchmark and introduce LiveMamba, a streaming multi-modal LLM designed for interactive instructional guidance. This work provides the first dedicated benchmark and a strong baseline for developing and evaluating on live, situated coaching.

</details>


### [15] [StreamFlow: Theory, Algorithm, and Implementation for High-Efficiency Rectified Flow Generation](https://arxiv.org/abs/2511.22009)
*Sen Fang,Hongbin Zhong,Yalin Feng,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: 本文提出了一套针对Rectified Flow等新型生成模型的加速方法，显著提升了生成速度，效果优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型如Rectified Flow、Flow Matching，在生成精度、质量与效率方面表现突出，但因其理论与结构与常规扩散模型不同，现有加速方式难以直接适用。为提高这些新型模型的加速能力，有必要开发专门的加速方法。

Method: 作者设计并实现了综合理论、架构和推理策略的整体加速流程，包括基于新速度场的批处理、异构时间步的批量向量化处理以及为新方法动态TensorRT编译，全面提升了基于流模型的推理效率。

Result: 新方法在512×512图像生成任务上，实验加速率高达611%，远超当前公开的非通用性加速方法（通常仅18%的加速）。

Conclusion: 文中提出的加速管线可大幅提升基于Rectified Flow等流模型生成网络的推理速度，具有广泛实际应用价值，为后续此类模型的部署及优化提供了新方向。

Abstract: New technologies such as Rectified Flow and Flow Matching have significantly improved the performance of generative models in the past two years, especially in terms of control accuracy, generation quality, and generation efficiency. However, due to some differences in its theory, design, and existing diffusion models, the existing acceleration methods cannot be directly applied to the Rectified Flow model. In this article, we have comprehensively implemented an overall acceleration pipeline from the aspects of theory, design, and reasoning strategies. This pipeline uses new methods such as batch processing with a new velocity field, vectorization of heterogeneous time-step batch processing, and dynamic TensorRT compilation for the new methods to comprehensively accelerate related models based on flow models. Currently, the existing public methods usually achieve an acceleration of 18%, while experiments have proved that our new method can accelerate the 512*512 image generation speed to up to 611%, which is far beyond the current non-generalized acceleration methods.

</details>


### [16] [MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis](https://arxiv.org/abs/2511.22018)
*Chunzheng Zhu,Yangfang Lin,Shen Chen,Yijun Wang,Jianxin Lin*

Main category: cs.CV

TL;DR: 提出MedEyes框架，通过模拟临床医生的视觉关注和推理路径，提高医学视觉问答任务的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）采用强化学习优化链式推理过程，但通常偏向表面连贯而缺乏临床准确性的推理，这与医学实际决策流程差距较大，因此需要更贴合真实临床推理的AI模型。

Method: MedEyes引入专家的视觉搜索轨迹作为外部行为信号，在强化学习中融合off-policy（专家数据）和on-policy（模型自主探索）信号。设计了Gaze-guided Reasoning Navigator（GRN）模拟专家的系统性局部分析和详细区域钻研，同时引入Confidence Value Sampler（CVS）生成多样且可信的探索路径。整体采用双流GRPO优化，将on/off-policy学习信号解耦，提升训练稳定性与推理多样性。

Result: 在多个医学视觉问答基准测试上，MedEyes整体表现提升了8.5%，表明该方法显著优于现有模型。

Conclusion: MedEyes能够更好模拟和解释医生的诊断流程，有望推动医学可解释AI的发展，改善医疗AI的临床适用性。

Abstract: Accurate medical diagnosis often involves progressive visual focusing and iterative reasoning, characteristics commonly observed in clinical workflows. While recent vision-language models demonstrate promising chain-of-thought (CoT) reasoning capabilities via reinforcement learning with verifiable rewards (RLVR), their purely on-policy learning paradigm tends to reinforce superficially coherent but clinically inaccurate reasoning paths. We propose MedEyes, a novel reinforcement learning framework that dynamically models clinician-style diagnostic reasoning by progressively attending to and interpreting relevant medical image regions. By incorporating off-policy expert guidance, MedEyes converts expert visual search trajectories into structured external behavioral signals, guiding the model toward clinically aligned visual reasoning. We design the Gaze-guided Reasoning Navigator (GRN) to emulate the diagnostic process through a dual-mode exploration strategy, scanning for systematic abnormality localization and drilling for detailed regional analysis. To balance expert imitation and autonomous discovery, we introduce the Confidence Value Sampler (CVS), which employs nucleus sampling and adaptive termination to create diverse yet credible exploration paths. Finally, the dual-stream GRPO optimization framework decouples on-policy and off-policy learning signals, mitigating reward assimilation and entropy collapse. Experiments demonstrate that MedEyes achieves an average performance improvement of +8.5\% across multiple medical VQA benchmarks, validating MedEyes's potential in building interpretable medical AI systems.

</details>


### [17] [Intra-Class Probabilistic Embeddings for Uncertainty Estimation in Vision-Language Models](https://arxiv.org/abs/2511.22019)
*Zhenxiang Lin,Maryam Haghighat,Will Browne,Dimity Miller*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、可用于现有对比式视觉语言模型（VLM）事后不确定性估计的方法，能高效检测模型错误预测并提升其安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM模型如CLIP有出色的开放词汇分类能力，但容易对错误分类给出高置信度分数，因此在安全相关场合下存在局限。如何在不额外训练或微调的情况下提升模型对错误的辨识与不确定性估计，成为亟需解决的问题。

Method: 提出基于视觉特征一致性度量和高斯分布类嵌入的方法：通过特征投影及多元高斯建模按类别生成概率特征嵌入，无需模型微调，对不同VLM均适用。且每类仅需极少样本（10张）即可工作。

Result: 在ImageNet、Flowers102、Food101、EuroSAT和DTD等数据集上，本文方法在错误检测任务上达到或显著超过当前确定性和概率模型基线，具有对分布漂移的鲁棒性。

Conclusion: 该方法为VLM错误检测与不确定性估计提供了高效、泛化性强、易于使用的新途径，有望拓宽VLM在高安全性需求领域的实际应用范围。

Abstract: Vision-language models (VLMs), such as CLIP, have gained popularity for their strong open vocabulary classification performance, but they are prone to assigning high confidence scores to misclassifications, limiting their reliability in safety-critical applications. We introduce a training-free, post-hoc uncertainty estimation method for contrastive VLMs that can be used to detect erroneous predictions. The key to our approach is to measure visual feature consistency within a class, using feature projection combined with multivariate Gaussians to create class-specific probabilistic embeddings. Our method is VLM-agnostic, requires no fine-tuning, demonstrates robustness to distribution shift, and works effectively with as few as 10 training images per class. Extensive experiments on ImageNet, Flowers102, Food101, EuroSAT and DTD show state-of-the-art error detection performance, significantly outperforming both deterministic and probabilistic VLM baselines. Code is available at https://github.com/zhenxianglin/ICPE.

</details>


### [18] [Layover or Direct Flight: Rethinking Audio-Guided Image Segmentation](https://arxiv.org/abs/2511.22025)
*Joel Alberto Santos,Zongwei Wu,Xavier Alameda-Pineda,Radu Timofte*

Main category: cs.CV

TL;DR: 本文质疑当前基于语音转文本再进行视觉定位的主流方法，提出绕开文本中介，直接通过音频与图像对齐实现目标定位，并验证了音频直连视觉方法在任务上的可行性和优势。


<details>
  <summary>Details</summary>
Motivation: 当前主流的人机交互视觉定位任务依赖语音转文字作为中介，而这种方式在效率和稳健性上存在不足。作者希望探索不通过文本、直接利用语音信号进行视觉目标定位的可能性，以提升效率和鲁棒性。

Method: 作者提出了一种简化的任务：通过单词级口语指令实现物体定位。他们构建了一个涵盖多种物体和口音的新音频定位数据集，适配并评测了多个音频-视觉领域的模型，比较了音频直连视觉和传统转文本方法的表现。

Result: 实验结果显示，直接以音频为输入进行目标定位不仅可行，在应对语言多样性和复杂口音时表现出比文本中介方法更好的稳健性，在部分任务上取得了更好的效果。

Conclusion: 研究推动了对直接音频-视觉对齐的兴趣，证明了音频直接定位方法在多模态理解中具有实际应用价值，有助于构建更高效、鲁棒的人机交互系统。

Abstract: Understanding human instructions is essential for enabling smooth human-robot interaction. In this work, we focus on object grounding, i.e., localizing an object of interest in a visual scene (e.g., an image) based on verbal human instructions. Despite recent progress, a dominant research trend relies on using text as an intermediate representation. These approaches typically transcribe speech to text, extract relevant object keywords, and perform grounding using models pretrained on large text-vision datasets. However, we question both the efficiency and robustness of such transcription-based pipelines. Specifically, we ask: Can we achieve direct audio-visual alignment without relying on text? To explore this possibility, we simplify the task by focusing on grounding from single-word spoken instructions. We introduce a new audio-based grounding dataset that covers a wide variety of objects and diverse human accents. We then adapt and benchmark several models from the closely audio-visual field. Our results demonstrate that direct grounding from audio is not only feasible but, in some cases, even outperforms transcription-based methods, especially in terms of robustness to linguistic variability. Our findings encourage a renewed interest in direct audio grounding and pave the way for more robust and efficient multimodal understanding systems.

</details>


### [19] [PAGen: Phase-guided Amplitude Generation for Domain-adaptive Object Detection](https://arxiv.org/abs/2511.22029)
*Shuchen Du,Shuo Lei,Feiran Li,Jiacheng Li,Daisuke Iso*

Main category: cs.CV

TL;DR: 本文提出了一种简单高效的无监督领域自适应（UDA）方法，通过在频域中自适应图像风格，降低源域与目标域间的差异，无需复杂架构或多余推理时计算开销，实验显示在多种基准任务上大幅提升表现。


<details>
  <summary>Details</summary>
Motivation: 当前UDA主流方法依赖复杂的对抗训练或架构设计，应用繁琐且推理阶段计算负担重，急需简单高效的UDA方案以更好适应实际部署需求。

Method: 提出在训练阶段采用频域风格自适应的轻量级预处理模块，仅在训练时使用，推理时完全舍弃，无需额外模型、辅助分支或对抗训练。

Result: 在领域自适应目标检测（DAOD）任务和多个公开基准测试上，所提方法取得了显著优于现有方法的性能提升。

Conclusion: 简单易行的频域自适应方法能够有效缓解源与目标域差异，兼顾实用性与性能，为UDA实际部署提供更好选择。

Abstract: Unsupervised domain adaptation (UDA) greatly facilitates the deployment of neural networks across diverse environments. However, most state-of-the-art approaches are overly complex, relying on challenging adversarial training strategies, or on elaborate architectural designs with auxiliary models for feature distillation and pseudo-label generation. In this work, we present a simple yet effective UDA method that learns to adapt image styles in the frequency domain to reduce the discrepancy between source and target domains. The proposed approach introduces only a lightweight pre-processing module during training and entirely discards it at inference time, thus incurring no additional computational overhead. We validate our method on domain-adaptive object detection (DAOD) tasks, where ground-truth annotations are easily accessible in source domains (e.g., normal-weather or synthetic conditions) but challenging to obtain in target domains (e.g., adverse weather or low-light scenes). Extensive experiments demonstrate that our method achieves substantial performance gains on multiple benchmarks, highlighting its practicality and effectiveness.

</details>


### [20] [SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model](https://arxiv.org/abs/2511.22039)
*Jiayuan Du,Yiming Zhao,Zhenglong Guo,Yong Pan,Wenbo Hou,Zhihui Hao,Kun Zhan,Qijun Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D场景占用预测架构，利用transformer直接从原始图像特征端到端预测未来多帧占用，避免了离散token和BEV的局限，并在nuScenes数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖VAE生成离散token或BEV投影，这些方法分别存在表现力受限与几何先验绑定强的局限。因此，作者希望设计一种既具高表现力、又无BEV结构局限的新方法。

Method: 提出了一种基于transformer的稀疏场景占用表征方法，直接从图像特征端到端预测未来多帧占用状态，不依赖BEV或离散token，中间环节不做额外几何假设，通过transformer捕获时空依赖关系。

Result: 在nuScenes 1-3秒占用预测任务上，相比现有主流方法取得了显著更好的精度，且在不同未来轨迹条件下都表现出强健的动力学理解与预测能力。

Conclusion: 摒弃了离散token和BEV两大约束，该方法借助transformer有效提升了时空依赖建模能力，实现了当前最优的3D占用预测性能，适用于复杂场景的未来轨迹条件下的高精度预测。

Abstract: This paper introduces a novel architecture for trajectory-conditioned forecasting of future 3D scene occupancy. In contrast to methods that rely on variational autoencoders (VAEs) to generate discrete occupancy tokens, which inherently limit representational capacity, our approach predicts multi-frame future occupancy in an end-to-end manner directly from raw image features. Inspired by the success of attention-based transformer architectures in foundational vision and language models such as GPT and VGGT, we employ a sparse occupancy representation that bypasses the intermediate bird's eye view (BEV) projection and its explicit geometric priors. This design allows the transformer to capture spatiotemporal dependencies more effectively. By avoiding both the finite-capacity constraint of discrete tokenization and the structural limitations of BEV representations, our method achieves state-of-the-art performance on the nuScenes benchmark for 1-3 second occupancy forecasting, outperforming existing approaches by a significant margin. Furthermore, it demonstrates robust scene dynamics understanding, consistently delivering high accuracy under arbitrary future trajectory conditioning.

</details>


### [21] [ICM-SR: Image-Conditioned Manifold Regularization for Image Super-Resoultion](https://arxiv.org/abs/2511.22048)
*Junoh Kang,Donghun Ryu,Bohyung Han*

Main category: cs.CV

TL;DR: 本文提出了一种图像条件流形正则化（ICM）方法，用于提升真实图像超分辨率任务的重建质量，特别是在感知质量方面明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前真实图像超分辨率方法借助于文生图扩散模型的生成先验，但通常只采用基于文本的流形进行正则化，导致生成结果与输入图片关联不够，且存在色彩失真、边缘模糊等缺陷。因此需要一种更好地结合输入图像信息的流形正则化方法，以提升超分结果的真实感和稳定性。

Method: 作者提出了一种新的正则化方法（ICM），将输出正则到一个由输入图像的稀疏结构信息（色彩图与Canny边缘）条件的流形上，解决了直接用原始输入图像作为条件可能会引发数值不稳定的问题。该稀疏结构信息既能表达关键内容，又避免了高密度信息带来的训练不稳定。

Result: 实验显示，基于ICM的正则化信号不仅具备任务对齐性和稳定性，而且在超分辨率感知质量上显著超过了以往方法。

Conclusion: ICM方法克服了以往文字条件流形的概念偏差与实践缺陷，为真实世界应用的超分辨率任务带来了感知质量上的显著提升。作者将开源代码以促进复现和后续研究。

Abstract: Real world image super-resolution (Real-ISR) often leverages the powerful generative priors of text-to-image diffusion models by regularizing the output to lie on their learned manifold. However, existing methods often overlook the importance of the regularizing manifold, typically defaulting to a text-conditioned manifold. This approach suffers from two key limitations. Conceptually, it is misaligned with the Real-ISR task, which is to generate high quality (HQ) images directly tied to the low quality (LQ) images. Practically, the teacher model often reconstructs images with color distortions and blurred edges, indicating a flawed generative prior for this task. To correct these flaws and ensure conceptual alignment, a more suitable manifold must incorporate information from the images. While the most straightforward approach is to condition directly on the raw input images, their high information densities make the regularization process numerically unstable. To resolve this, we propose image-conditioned manifold regularization (ICM), a method that regularizes the output towards a manifold conditioned on the sparse yet essential structural information: a combination of colormap and Canny edges. ICM provides a task-aligned and stable regularization signal, thereby avoiding the instability of dense-conditioning and enhancing the final super-resolution quality. Our experiments confirm that the proposed regularization significantly enhances super-resolution performance, particularly in perceptual quality, demonstrating its effectiveness for real-world applications. We will release the source code of our work for reproducibility.

</details>


### [22] [TPCNet: Triple physical constraints for Low-light Image Enhancement](https://arxiv.org/abs/2511.22052)
*Jing-Yi Shi,Ming-Fei Li,Ling-An Wu*

Main category: cs.CV

TL;DR: 本文提出了一种基于三重物理约束（TPC）理论的低光图像增强方法（TPCNet），在保持高性能的同时，显著提升了图像质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有许多基于Retinex理论的深度学习增强算法，通常假设物体为理想Lambertian反射，忽略了镜面反射，并且采用图像空间物理约束，导致模型泛化能力有限。

Method: 作者提出采用Kubelka-Munk理论，保留镜面反射系数，在模型特征空间中重构照明、反射和检测之间的三重物理约束（TPC），并据此设计了TPCNet网络架构。

Result: 大量定量、定性和消融实验表明，TPCNet在提升性能指标和视觉质量方面优于现有方法，并在10个数据集上取得了更好效果，且无需引入额外参数。

Conclusion: 基于三重物理约束的TPCNet方法解决了以往模型泛化不足的问题，在低光图像增强任务中展示了优越性和实用性。

Abstract: Low-light image enhancement is an essential computer vision task to improve image contrast and to decrease the effects of color bias and noise. Many existing interpretable deep-learning algorithms exploit the Retinex theory as the basis of model design. However, previous Retinex-based algorithms, that consider reflected objects as ideal Lambertian ignore specular reflection in the modeling process and construct the physical constraints in image space, limiting generalization of the model. To address this issue, we preserve the specular reflection coefficient and reformulate the original physical constraints in the imaging process based on the Kubelka-Munk theory, thereby constructing constraint relationship between illumination, reflection, and detection, the so-called triple physical constraints (TPCs)theory. Based on this theory, the physical constraints are constructed in the feature space of the model to obtain the TPC network (TPCNet). Comprehensive quantitative and qualitative benchmark and ablation experiments confirm that these constraints effectively improve the performance metrics and visual quality without introducing new parameters, and demonstrate that our TPCNet outperforms other state-of-the-art methods on 10 datasets.

</details>


### [23] [OralGPT-Omni: A Versatile Dental Multimodal Large Language Model](https://arxiv.org/abs/2511.22055)
*Jing Hao,Yuci Liang,Lizhuo Lin,Yuxuan Fan,Wenkai Zhou,Kaixin Guo,Zanting Ye,Yanpeng Sun,Xinyu Zhang,Yanqi Yang,Qiankun Li,Hao Tang,James Kit-Hon Tsoi,Linlin Shen,Kuo Feng Hung*

Main category: cs.CV

TL;DR: 本文提出了首个专注于口腔医学的多模态大模型 OralGPT-Omni，并相应建立了新的推理数据集和评测基准，在多种口腔影像与任务上显著优于现有通用模型。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型已在医疗领域展现潜力，但口腔领域因缺乏数据、专家标注和多模态建模等问题发展滞后。研究者希望通过针对性模型和数据推动口腔智能医疗进步。

Method: 作者开发了专用于口腔领域的MLLM——OralGPT-Omni。创新性地引入了模拟牙科医生推理过程的TRACE-CoT数据集，以链式思维方式监督模型训练，采用四阶段训练策略。同时，构建了MMOral-Uni多模态测试集，覆盖五类口腔影像及任务。

Result: OralGPT-Omni在MMOral-Uni基准上获得51.84分，在MMOral-OPG基准上获45.31分，均大幅超越GPT-5等通用大模型。

Conclusion: OralGPT-Omni显著提升了口腔多模态影像分析能力，为口腔数字化与智能化提供了强大工具，对今后相关研究具有推动作用。

Abstract: Multimodal Large Language Models (MLLMs) have exhibited immense potential across numerous medical specialties; yet, dentistry remains underexplored, in part due to limited domain-specific data, scarce dental expert annotations, insufficient modality-specific modeling, and challenges in reliability. In this paper, we present OralGPT-Omni, the first dental-specialized MLLM designed for comprehensive and trustworthy analysis across diverse dental imaging modalities and clinical tasks. To explicitly capture dentists' diagnostic reasoning, we construct TRACE-CoT, a clinically grounded chain-of-thought dataset that mirrors dental radiologists' decision-making processes. This reasoning supervision, combined with our proposed four-stage training paradigm, substantially strengthens the model's capacity for dental image understanding and analysis. In parallel, we introduce MMOral-Uni, the first unified multimodal benchmark for dental image analysis. It comprises 2,809 open-ended question-answer pairs spanning five modalities and five tasks, offering a comprehensive evaluation suite to date for MLLMs in digital dentistry. OralGPT-Omni achieves an overall score of 51.84 on the MMOral-Uni benchmark and 45.31 on the MMOral-OPG benchmark, dramatically outperforming the scores of GPT-5. Our work promotes intelligent dentistry and paves the way for future advances in dental image analysis. All code, benchmark, and models will be made publicly available.

</details>


### [24] [DNA: Dual-branch Network with Adaptation for Open-Set Online Handwriting Generation](https://arxiv.org/abs/2511.22064)
*Tsai-Ling Huang,Nhat-Tuong Do-Tran,Ngoc-Hoang-Lam Le,Hong-Han Shuai,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 本文提出DNA双分支网络，在笔迹生成任务中同时自适应书写风格和字符内容，通过结构和纹理分解显著提升对未见字符的生成能力，并取得了业内领先的性能。


<details>
  <summary>Details</summary>
Motivation: 现有在线手写生成方法对未见字符（尤其是汉字等表意文字）生成效果较差，限制了其实用性，需要一种能泛化到新角色和风格的生成方法。

Method: 提出Dual-branch Network with Adaptation（DNA），包含自适应风格分支和自适应内容分支。其中风格分支学习笔画属性（方向、间距、排布等），内容分支把字符分解为结构信息和纹理细节，通过局部与全局编码器提取并生成内容。

Result: 在大量实验中，DNA能有效针对未见作家、未见字符进行手写生成，性能达到当前最优。

Conclusion: DNA方法大幅提升了OHG在泛化性和实用性上的表现，尤其适应未见汉字等复杂字符生成，对实际应用有推动作用。

Abstract: Online handwriting generation (OHG) enhances handwriting recognition models by synthesizing diverse, human-like samples. However, existing OHG methods struggle to generate unseen characters, particularly in glyph-based languages like Chinese, limiting their real-world applicability. In this paper, we introduce our method for OHG, where the writer's style and the characters generated during testing are unseen during training. To tackle this challenge, we propose a Dual-branch Network with Adaptation (DNA), which comprises an adaptive style branch and an adaptive content branch. The style branch learns stroke attributes such as writing direction, spacing, placement, and flow to generate realistic handwriting. Meanwhile, the content branch is designed to generalize effectively to unseen characters by decomposing character content into structural information and texture details, extracted via local and global encoders, respectively. Extensive experiments demonstrate that our DNA model is well-suited for the unseen OHG setting, achieving state-of-the-art performance.

</details>


### [25] [WorldWander: Bridging Egocentric and Exocentric Worlds in Video Generation](https://arxiv.org/abs/2511.22098)
*Quanjian Song,Yiren Song,Kelly Peng,Yuan Gao,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频生成框架WorldWander，用于在第一视角（自我中心）和第三视角（全景中心）间无缝视频转换，并建立了大规模同步视角数据集EgoExo-8K。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型虽已在真实感与可控性方面取得进展，但在不同视角（如第一与第三视角）间进行无缝视频转换尚未充分探索。而该能力对于电影制作、具身智能与世界建模等应用十分关键。

Method: 作者基于先进的视频扩散变换器（Video Diffusion Transformers）提出WorldWander框架，融合了“上下文视角对齐”和“协同位置编码”机制，有效建模跨视角同步。同时，提出了EgoExo-8K数据集，包含真实与合成场景下同步拍摄的第一视角与第三视角三元组。

Result: 实验表明，WorldWander在视角同步、人设一致性及泛化能力方面达到或优于现有技术标准，推动了该领域的发展。

Conclusion: WorldWander为第一与第三视角之间的视频转换设立了新标杆，将在多视角理解、视频内容生成与相关实际应用中发挥重要作用。

Abstract: Video diffusion models have recently achieved remarkable progress in realism and controllability. However, achieving seamless video translation across different perspectives, such as first-person (egocentric) and third-person (exocentric), remains underexplored. Bridging these perspectives is crucial for filmmaking, embodied AI, and world models. Motivated by this, we present WorldWander, an in-context learning framework tailored for translating between egocentric and exocentric worlds in video generation. Building upon advanced video diffusion transformers, WorldWander integrates (i) In-Context Perspective Alignment and (ii) Collaborative Position Encoding to efficiently model cross-view synchronization. To further support our task, we curate EgoExo-8K, a large-scale dataset containing synchronized egocentric-exocentric triplets from both synthetic and real-world scenarios. Experiments demonstrate that WorldWander achieves superior perspective synchronization, character consistency, and generalization, setting a new benchmark for egocentric-exocentric video translation.

</details>


### [26] [MRI-Based Brain Age Estimation with Supervised Contrastive Learning of Continuous Representation](https://arxiv.org/abs/2511.22102)
*Simon Joseph Clément Crête,Marta Kersten-Oertel,Yiming Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种基于监督对比学习（Rank-N-Contrast损失）的MRI脑龄估计方法，并结合Grad-RAM对结果进行可视化解释，显著提升了估计精度和特征表征能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度回归模型在MRI脑龄预测任务中，往往难以捕捉连续的神经形态变化，导致特征表达和预测效果不佳。急需更有效的方法提升脑龄估计的准确性，同时增强可解释性，以促进其在临床神经退行性疾病诊断中的应用。

Method: 作者提出采用基于Rank-N-Contrast的监督对比学习方法，对结构性MRI数据进行脑龄回归，并利用Grad-RAM进行模型判别特征可视化。该方法首次将RNC损失应用于脑龄估计领域。

Result: 该方法在少量训练样本下取得了MAE为4.27岁，R²为0.93的优秀表现，明显优于同样骨干网络上的常规深度回归法，并超过或接近利用更大数据集的目前最优方法。Grad-RAM可视化表明，该方法下显著捕捉到与脑龄预测更相关的细粒度特征。同时在阿尔茨海默病和帕金森病患者中，脑龄差异与疾病严重程度密切相关。

Conclusion: 本文方法不仅极大提升了MRI脑龄估计的精度与可解释性，还显示出脑龄差异作为神经退行性疾病潜在生物标志物的应用前景。

Abstract: MRI-based brain age estimation models aim to assess a subject's biological brain age based on information, such as neuroanatomical features. Various factors, including neurodegenerative diseases, can accelerate brain aging and measuring this phenomena could serve as a potential biomarker for clinical applications. While deep learning (DL)-based regression has recently attracted major attention, existing approaches often fail to capture the continuous nature of neuromorphological changes, potentially resulting in sub-optimal feature representation and results. To address this, we propose to use supervised contrastive learning with the recent Rank-N-Contrast (RNC) loss to estimate brain age based on widely used T1w structural MRI for the first time and leverage Grad-RAM to visually explain regression results. Experiments show that our proposed method achieves a mean absolute error (MAE) of 4.27 years and an $R^2$ of 0.93 with a limited dataset of training samples, significantly outperforming conventional deep regression with the same ResNet backbone while performing better or comparably with the state-of-the-art methods with significantly larger training data. Furthermore, Grad-RAM revealed more nuanced features related to age regression with the RNC loss than conventional deep regression. As an exploratory study, we employed the proposed method to estimate the gap between the biological and chronological brain ages in Alzheimer's Disease and Parkinson's disease patients, and revealed the correlation between the brain age gap and disease severity, demonstrating its potential as a biomarker in neurodegenerative disorders.

</details>


### [27] [MoE3D: Mixture of Experts meets Multi-Modal 3D Understanding](https://arxiv.org/abs/2511.22103)
*Yu Li,Yuenan Hou,Yingmei Wei,Xinge Zhu,Yuexin Ma,Wenqi Shao,Yanming Guo*

Main category: cs.CV

TL;DR: MoE3D通过引入专家混合（MoE）机制和信息聚合模块，有效提升了多模态3D理解的性能，在多个任务中取得领先，尤其在Multi3DRefer数据集上大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往的多模态3D融合方法缺乏处理不同模态间巨大异质性和复杂性的能力，导致融合效果不佳。因此，作者试图引入能针对不同模态或模态交互有专长的网络结构，以改善融合性能。

Method: 提出MoE3D框架，在多模态学习结构中集成了Mixture of Experts（MoE），通过部署多个针对不同模态（或模态交互）的专家网络以及MoE机制的Transformer对视觉特征进行有效融合。设计了信息聚合模块加强融合效果，并采用Top-1 gating策略提升效率。此外，通过渐进式预训练策略充分利用语义和2D先验知识，实现更优的模型初始化。

Result: MoE3D在四个主流3D理解任务上均取得了有竞争力的结果，尤其在Multi3DRefer数据集上，较最优现有方法提升6.1 mIoU。

Conclusion: 通过专家混合机制和信息聚合模块，MoE3D显著提升了多模态3D理解的融合效果和任务表现，证明了其创新方法在复杂多模态场景下的优越性。

Abstract: Multi-modal 3D understanding is a fundamental task in computer vision. Previous multi-modal fusion methods typically employ a single, dense fusion network, struggling to handle the significant heterogeneity and complexity across modalities, leading to suboptimal performance. In this paper, we propose MoE3D, which integrates Mixture of Experts (MoE) into the multi-modal learning framework. The core is that we deploy a set of specialized "expert" networks, each adept at processing a specific modality or a mode of cross-modal interaction. Specifically, the MoE-based transformer is designed to better utilize the complementary information hidden in the visual features. Information aggregation module is put forward to further enhance the fusion performance. Top-1 gating is employed to make one expert process features with expert groups, ensuring high efficiency. We further propose a progressive pre-training strategy to better leverage the semantic and 2D prior, thus equipping the network with good initialization. Our MoE3D achieves competitive performance across four prevalent 3D understanding tasks. Notably, our MoE3D surpasses the top-performing counterpart by 6.1 mIoU on Multi3DRefer.

</details>


### [28] [HyperST: Hierarchical Hyperbolic Learning for Spatial Transcriptomics Prediction](https://arxiv.org/abs/2511.22107)
*Chen Zhang,Yilu An,Ying Chen,Hao Li,Xitong Ling,Lihao Liu,Junjun He,Yuxiang Lin,Zihui Wang,Rongshan Yu*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法HyperST，通过学习图像和基因表达的多层次表示，有效提升了基因表达预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有利用组织病理图像预测基因表达的方法，未充分利用空间转录组数据的全层次结构，尤其是基因表达端层级信息，导致图像与基因不完全对应。此外，基因表达包含的分子信息过于丰富，常规图像难以完全表达，存在模态差异，亟需新的表达学习方法。

Method: 作者提出HyperST框架：一方面设计多层次表示提取器，分别捕获图像和基因在spot级、micro-niche级等不同层次的特征；另一方面提出分层超球对齐模块，在超球空间中对多层次嵌入进行统一结构化和空间对齐，弥补模态差异，实现更充分的图像-基因语义融合。

Result: HyperST在四个不同组织的公开数据集上实现了最优的预测表现，超越了现有方法。

Conclusion: HyperST框架通过多层次、分层对齐机制，显著提升了空间转录组预测的准确性和可扩展性，为大规模、廉价的转录组学分析提供了新路径。

Abstract: Spatial Transcriptomics (ST) merges the benefits of pathology images and gene expression, linking molecular profiles with tissue structure to analyze spot-level function comprehensively. Predicting gene expression from histology images is a cost-effective alternative to expensive ST technologies. However, existing methods mainly focus on spot-level image-to-gene matching but fail to leverage the full hierarchical structure of ST data, especially on the gene expression side, leading to incomplete image-gene alignment. Moreover, a challenge arises from the inherent information asymmetry: gene expression profiles contain more molecular details that may lack salient visual correlates in histological images, demanding a sophisticated representation learning approach to bridge this modality gap. We propose HyperST, a framework for ST prediction that learns multi-level image-gene representations by modeling the data's inherent hierarchy within hyperbolic space, a natural geometric setting for such structures. First, we design a Multi-Level Representation Extractors to capture both spot-level and niche-level representations from each modality, providing context-aware information beyond individual spot-level image-gene pairs. Second, a Hierarchical Hyperbolic Alignment module is introduced to unify these representations, performing spatial alignment while hierarchically structuring image and gene embeddings. This alignment strategy enriches the image representations with molecular semantics, significantly improving cross-modal prediction. HyperST achieves state-of-the-art performance on four public datasets from different tissues, paving the way for more scalable and accurate spatial transcriptomics prediction.

</details>


### [29] [PROMPTMINER: Black-Box Prompt Stealing against Text-to-Image Generative Models via Reinforcement Learning and Fuzz Optimization](https://arxiv.org/abs/2511.22119)
*Mingzhe Li,Renhao Zhang,Zhiyang Wen,Siqi Pan,Bruno Castro da Silva,Juan Zhai,Shiqing Ma*

Main category: cs.CV

TL;DR: 本文提出了一种名为PROMPTMINER的新型黑盒提示词窃取框架，实现了对文本到图像生成模型（如Stable Diffusion、FLUX）生成图像背后描述性文本提示词的有效逆向推理。该方法在多种场景下均优于现有方法，并具备较强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着高质量T2I生成模型的普及，优质提示词成为数字资产，催生了智财与安全风险，特别是提示词窃取攻击。该问题既带来未授权提取风险，也有助于模型归属和水印验证等应用。

Method: PROMPTMINER将提示词恢复任务分为两阶段：第一，用强化学习优化识别图像主要内容；第二，通过基于模糊测试的搜索恢复风格修饰词。整个流程为黑盒，无需梯度信息和大规模有标签数据。

Result: 在多组实验与不同扩散模型中，PROMPTMINER无论是CLIP相似度（最高0.958）还是SBERT文本对齐（最高0.751）均超越现有所有基线。面对未知生成器和野生图像时，PROMPTMINER表现领先，CLIP相似度比最强基线高7.5%。在防御扰动下依然表现稳健。

Conclusion: PROMPTMINER是一种高效、实用且鲁棒的黑盒提示词窃取方法，可以推动提示词知识产权保护、生成模型数据归属、鲁棒性研究等领域发展。

Abstract: Text-to-image (T2I) generative models such as Stable Diffusion and FLUX can synthesize realistic, high-quality images directly from textual prompts. The resulting image quality depends critically on well-crafted prompts that specify both subjects and stylistic modifiers, which have become valuable digital assets. However, the rising value and ubiquity of high-quality prompts expose them to security and intellectual-property risks. One key threat is the prompt stealing attack, i.e., the task of recovering the textual prompt that generated a given image. Prompt stealing enables unauthorized extraction and reuse of carefully engineered prompts, yet it can also support beneficial applications such as data attribution, model provenance analysis, and watermarking validation. Existing approaches often assume white-box gradient access, require large-scale labeled datasets for supervised training, or rely solely on captioning without explicit optimization, limiting their practicality and adaptability. To address these challenges, we propose PROMPTMINER, a black-box prompt stealing framework that decouples the task into two phases: (1) a reinforcement learning-based optimization phase to reconstruct the primary subject, and (2) a fuzzing-driven search phase to recover stylistic modifiers. Experiments across multiple datasets and diffusion backbones demonstrate that PROMPTMINER achieves superior results, with CLIP similarity up to 0.958 and textual alignment with SBERT up to 0.751, surpassing all baselines. Even when applied to in-the-wild images with unknown generators, it outperforms the strongest baseline by 7.5 percent in CLIP similarity, demonstrating better generalization. Finally, PROMPTMINER maintains strong performance under defensive perturbations, highlighting remarkable robustness. Code: https://github.com/aaFrostnova/PromptMiner

</details>


### [30] [GoPrune: Accelerated Structured Pruning with $\ell_{2,p}$-Norm Optimization](https://arxiv.org/abs/2511.22120)
*Li Xu,Xianchao Xiu*

Main category: cs.CV

TL;DR: 本文提出了一种加速的结构化剪枝方法GoPrune，利用了ℓ_{2,p}-范数和高效的优化算法，实现了在保持准确率的同时大幅减少模型体积和计算量。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络（CNN）深度增加导致存储和计算成本迅速上升，限制了其在资源受限的边缘设备上的部署。已有剪枝方法在结构化剪枝和计算效率方面仍有不足。

Method: 提出GoPrune方法，通过ℓ_{2,p}-范数（p∈[0,1)）进行稀疏网络学习。采用基于近端交替最小化（PAM）的高效优化算法，使剪枝子问题具有闭式解，提升剪枝效率。

Result: 在CIFAR数据集上，利用ResNet和VGG模型，实验结果表明所提方法在网络剪枝方面优于现有方法，具备更高的压缩效率和推断加速能力。

Conclusion: GoPrune方法能够高效进行结构化剪枝，显著压缩网络规模并加速推理，为CNN在边缘设备部署提供了有效解决方案。

Abstract: Convolutional neural networks (CNNs) suffer from rapidly increasing storage and computational costs as their depth grows, which severely hinders their deployment on resource-constrained edge devices. Pruning is a practical approach for network compression, among which structured pruning is the most effective for inference acceleration. Although existing work has applied the $\ell_p$-norm to pruning, it only considers unstructured pruning with $p\in (0, 1)$ and has low computational efficiency. To overcome these limitations, we propose an accelerated structured pruning method called GoPrune. Our method employs the $\ell_{2,p}$-norm for sparse network learning, where the value of $p$ is extended to $[0, 1)$. Moreover, we develop an efficient optimization algorithm based on the proximal alternating minimization (PAM), and the resulting subproblems enjoy closed-form solutions, thus improving compression efficiency. Experiments on the CIFAR datasets using ResNet and VGG models demonstrate the superior performance of the proposed method in network pruning. Our code is available at https://github.com/xianchaoxiu/GoPrune.

</details>


### [31] [Cue3D: Quantifying the Role of Image Cues in Single-Image 3D Generation](https://arxiv.org/abs/2511.22121)
*Xiang Li,Zirui Wang,Zixuan Huang,James M. Rehg*

Main category: cs.CV

TL;DR: 本文提出了Cue3D框架，系统评估和量化单张图片三维生成模型对于各种经典视觉线索（如明暗、纹理、轮廓等）的依赖与敏感性，揭示几何线索（尤其是明暗）对模型效果至关重要。


<details>
  <summary>Details</summary>
Motivation: 虽然现代深度生成模型在单张图片三维重建方面取得巨大进展，但实际这些模型究竟依赖哪些图像线索并不清楚。理解这些依赖对于设计更透明、稳健的三维重建模型具有重要意义。

Method: 作者提出Cue3D，一个不依赖模型的评测框架。通过系统扰动如明暗、纹理、轮廓、透视、边缘和局部连续性等线索，对七种先进三维生成方法进行统一测试，量化各类线索对三维生成质量的影响。

Result: 分析发现，三维生成的泛化能力受形状意义影响较大而非纹理。几何线索（特别是明暗）对三维重建效果至关重要。不同模型对轮廓的依赖较高，对透视和局部连续性的敏感性在不同方法中存在差异。

Conclusion: 作者工作加深了对现代3D生成模型利用传统单目视觉线索方式的理解，为提升三维重建模型的可解释性、透明性和可控性指明了方向。

Abstract: Humans and traditional computer vision methods rely on a diverse set of monocular cues to infer 3D structure from a single image, such as shading, texture, silhouette, etc. While recent deep generative models have dramatically advanced single-image 3D generation, it remains unclear which image cues these methods actually exploit. We introduce Cue3D, the first comprehensive, model-agnostic framework for quantifying the influence of individual image cues in single-image 3D generation. Our unified benchmark evaluates seven state-of-the-art methods, spanning regression-based, multi-view, and native 3D generative paradigms. By systematically perturbing cues such as shading, texture, silhouette, perspective, edges, and local continuity, we measure their impact on 3D output quality. Our analysis reveals that shape meaningfulness, not texture, dictates generalization. Geometric cues, particularly shading, are crucial for 3D generation. We further identify over-reliance on provided silhouettes and diverse sensitivities to cues such as perspective and local continuity across model families. By dissecting these dependencies, Cue3D advances our understanding of how modern 3D networks leverage classical vision cues, and offers directions for developing more transparent, robust, and controllable single-image 3D generation models.

</details>


### [32] [GA2-CLIP: Generic Attribute Anchor for Efficient Prompt Tuningin Video-Language Models](https://arxiv.org/abs/2511.22125)
*Bin Wang,Ruotong Hu,Wenqian Wang,Wentong Li,Mingliang Gao,Runmin Cong,Wei Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种可插拔耦合提示学习框架，通过引入外部监督的提示来提升视觉-语言模型在视频任务上的泛化能力，有效缓解了微调过程中语义空间收缩和过拟合的问题。实验结果表明，新方法在泛化基准和新类预测上显著优于当前先进的方法。


<details>
  <summary>Details</summary>
Motivation: 现有针对视觉-语言模型的视频任务微调方法存在泛化能力下降（对未见类别遗忘）的问题，常用的正则化手段抑制效果有限，影响软提示的学习能力。该论文旨在解决因过拟合监督类别导致的语义空间变窄问题。

Method: 方法上，作者提出将其他数据集的预训练提示作为硬提示token，与软提示token拼接，并通过可学习映射层耦合。这种竞争性提示机制防止提示空间陷入过拟合。此外，引入无关视频集与负提示，作为通用属性锚点，维持预训练语义空间的泛化属性。该方法属于plug-and-play，可灵活应用。

Result: 在各类视频任务和泛化基准测试上，新方法显著优于现有主流提示微调方法，尤其在基础类到新类的预测效果突出。

Conclusion: 通过外部硬提示引入和属性锚点设计，有效缓解了视频任务微调中的泛化能力退化问题，为V-L模型的下游广泛适应性提供了新思路。

Abstract: Visual and textual soft prompt tuning can effectively improve the adaptability of Vision-Language Models (VLMs) in downstream tasks. However, fine-tuning on video tasks impairs the model's generalization ability to unseen classes. Existing methods attempt to mitigate this forgetting effect by regularizing the gap between hand-crafted prompts and soft prompts, but this also weakens the learning ability of soft prompts. To address this challenge, we propose a plug-and-play coupling prompt learning framework to optimize the generalization performance of V-L models in video tasks, with the core motivation of mitigating semantic space narrowing during fine-tuning by introducing an externally supervised prompt. Specifically, for textual prompts, we introduce pre-trained prompts from other datasets as hard prompt tokens. These are concatenated with soft prompt tokens and coupled via a learnable mapping layer. This competitive prompting approach prevents the semantic space from overfitting to supervised categories. In addition, we introduce a set of well-designed irrelevant video sets and negative prompts as generic attribute anchors to maintain the generic relevance of the attributes in the pre-trained semantic space, thus preserving the generalization ability. Experiments on video tasks demonstrate that our method significantly outperforms state-of-the-art prompt tuning approaches across generalization benchmarks, particularly on base-to-new class prediction.

</details>


### [33] [Autonomous labeling of surgical resection margins using a foundation model](https://arxiv.org/abs/2511.22131)
*Xilin Yang,Musa Aydin,Yuhong Lu,Sahan Yoruc Selcuk,Bijie Bai,Yijie Zhang,Andrew Birkeland,Katjana Ehrlich,Julien Bec,Laura Marcu,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 该论文提出了一种虚拟着色网络（VIN），通过自动定位病理切面边缘，减少了物理着色的依赖并提高了标准化程度，实验结果显示该方法具有较好的一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的切缘评估方法依赖物理着色，操作不一且易受电灼伪影影响，导致病理切缘难以精确界定，对患者预后不利。为解决这一问题，亟需一种更客观、自动化且标准化的切缘识别方法。

Method: 作者提出了VIN模型，采用冻结的基础特征提取模型以及两层的多层感知机用于局部特征分类。研究使用了来自12个人类扁桃体蜡块的120张HE染色切片，大约2TB的图像数据，病理专家为切缘提供了注释。

Result: 在来自新的组织块的20张切片盲测中，VIN生成的切缘与专家注释高度一致，区域级准确率约为73.3%，主要错误集中在少量区域且未影响整体连续性。

Conclusion: VIN能够有效识别电灼伪影相关组织学形态，实现了无需实体着色的切缘划定，可集成入数字病理临床工作流程，从而支持后续的切缘距离测量。

Abstract: Assessing resection margins is central to pathological specimen evaluation and has profound implications for patient outcomes. Current practice employs physical inking, which is applied variably, and cautery artifacts can obscure the true margin on histological sections. We present a virtual inking network (VIN) that autonomously localizes the surgical cut surface on whole-slide images, reducing reliance on inks and standardizing margin-focused review. VIN uses a frozen foundation model as the feature extractor and a compact two-layer multilayer perceptron trained for patch-level classification of cautery-consistent features. The dataset comprised 120 hematoxylin and eosin (H&E) stained slides from 12 human tonsil tissue blocks, resulting in ~2 TB of uncompressed raw image data, where a board-certified pathologist provided boundary annotations. In blind testing with 20 slides from previously unseen blocks, VIN produced coherent margin overlays that qualitatively aligned with expert annotations across serial sections. Quantitatively, region-level accuracy was ~73.3% across the test set, with errors largely confined to limited areas that did not disrupt continuity of the whole-slide margin map. These results indicate that VIN captures cautery-related histomorphology and can provide a reproducible, ink-free margin delineation suitable for integration into routine digital pathology workflows and for downstream measurement of margin distances.

</details>


### [34] [DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action](https://arxiv.org/abs/2511.22134)
*Zhen Fang,Zhuoyang Liu,Jiaming Liu,Hao Chen,Yu Zeng,Shiting Huang,Zehui Chen,Lin Chen,Shanghang Zhang,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出DualVLA模型，通过精心设计的训练与剪枝方法，提升了视觉-语言-行动模型的动作执行能力，同时保留推理能力，有效解决了因泛化导致的动作能力退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在加入多模态和广泛推理训练后，动作表现会明显下降，无法兼顾推理与精确动作执行，需要一种新方法平衡两者。

Method: 提出了双层数据剪枝，去除冗余推理数据以保护动作学习；并设计双教师自适应蒸馏，对不同数据域分配不同监督信号，同时保持推理能力。

Result: DualVLA在SimplerEnv环境取得61.0平均成功率，在8个多模态基准测试中获得65.4平均得分，显示较传统方法有更好动作-推理平衡。

Conclusion: DualVLA显著缓解了VLA模型的动作能力退化问题，实现了更好的动作与推理融合，为通用VLA评测也提出了细粒度新指标。

Abstract: To build a generalizable Vision-Language-Action (VLA) model with strong reasoning ability, a common strategy is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then incorporate mixed annotated robot data together with multimodal data to restore broader reasoning capabilities. However, we observe that the resulting reasoning VLA often suffers from degraded action performance compared to the specialist model before fine-tuning, a phenomenon we refer to as action degeneration. To address this issue, we propose DualVLA, which enhances action performance through carefully designed post-training while still preserving reasoning capability. We first introduce a dual-layer data pruning method that removes redundant embodied reasoning, preventing it from adversely influencing action learning. To further strengthen action generation, we design a dual-teacher adaptive distillation strategy that assigns different supervision signals to different data domains while maintaining reasoning ability. To fill the evaluation gap for generalist VLAs, we also propose VLA Score, which decouples VLA capability into reasoning, intention, action, and alignment dimensions for a more fine-grained assessment. Experiments show that DualVLA achieves an average success rate of 61.0 in SimplerEnv and an average score of 65.4 across eight competitive multimodal benchmarks, demonstrating a stronger balance between precise action execution and multimodal understanding. Project Website: https://costaliya.github.io/DualVLA/.

</details>


### [35] [EASL: Multi-Emotion Guided Semantic Disentanglement for Expressive Sign Language Generation](https://arxiv.org/abs/2511.22135)
*Yanchao Zhao,Jihao Zhu,Yu Liu,Weizhuo Chen,Yuling Yang,Kun Peng*

Main category: cs.CV

TL;DR: 本文提出一种多情感引导的手语生成架构（EASL），在生成高质量手语视频时有效融合细粒度情感信息，显著提升手语的自然性和表现力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的手语生成方法重语义但轻情感，生成的手语视频情感表达不足，缺乏自然和表现力，限制了聋哑群体交流中的丰富情绪表达。

Method: 提出EASL架构，引入情感-语义解耦模块，并采用渐进式训练，分离提取语义与情感特征。在动作解码阶段，情感表示引导语义交互，从而生成带有七类情感置信度评分的手势，支持情感表达识别。同时，该方法可与扩散模型结合，生成更具表现力的手语视频。

Result: 实验表明，EASL通过整合多种情感信息，在手势姿态精度上优于所有对比基线方法，并且可有效适配扩散模型以生成更具表现力的手语视频。

Conclusion: EASL为手语生成引入多重情感信息，提升了手语视频的自然性和表现力，为无障碍交流提供了更优解决方案。

Abstract: Large language models have revolutionized sign language generation by automatically transforming text into high-quality sign language videos, providing accessible communication for the Deaf community. However, existing LLM-based approaches prioritize semantic accuracy while overlooking emotional expressions, resulting in outputs that lack naturalness and expressiveness. We propose EASL (Emotion-Aware Sign Language), a multi-emotion-guided generation architecture for fine-grained emotional integration. We introduce emotion-semantic disentanglement modules with progressive training to separately extract semantic and affective features. During pose decoding, the emotional representations guide semantic interaction to generate sign poses with 7-class emotion confidence scores, enabling emotional expression recognition. Experimental results demonstrate that EASL achieves pose accuracy superior to all compared baselines by integrating multi-emotion information and effectively adapts to diffusion models to generate expressive sign language videos.

</details>


### [36] [SemOD: Semantic Enabled Object Detection Network under Various Weather Conditions](https://arxiv.org/abs/2511.22142)
*Aiyinsi Zuo,Zhaoliang Zheng*

Main category: cs.CV

TL;DR: 本研究提出了一种基于语义信息的网络架构，通过提升恶劣天气下图像的语义一致性，显著提升了自动驾驶中的目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于摄像头的自动驾驶感知模型大多仅在晴天数据上训练，面对多变天气适应性弱。针对恶劣天气的模型则往往仅专注于消除特定天气影响，泛化能力不足。亟需一种能够适应多种天气的目标检测方案。

Method: 方法采用由预处理单元（PPU）和检测单元（DTU）组成的架构。PPU利用增强了语义信息的U形网络对恶劣天气图像进行修复和重建，DTU则通过修改YOLO骨干网络融合语义信息以提升目标检测效果。

Result: 在多个天气基准数据集上实现了1.47%到8.80%的mAP提升，优于现有方法。论文还将开源代码。

Conclusion: 通过语义信息辅助的图像预处理与检测，显著提升了多天气环境下的目标检测能力。验证了语义信息在图像增强和目标检测中的有效性，为自动驾驶感知系统提供了更加健壮的解决方案。

Abstract: In the field of autonomous driving, camera-based perception models are mostly trained on clear weather data. Models that focus on addressing specific weather challenges are unable to adapt to various weather changes and primarily prioritize their weather removal characteristics. Our study introduces a semantic-enabled network for object detection in diverse weather conditions. In our analysis, semantics information can enable the model to generate plausible content for missing areas, understand object boundaries, and preserve visual coherency and realism across both filled-in and existing portions of the image, which are conducive to image transformation and object recognition. Specific in implementation, our architecture consists of a Preprocessing Unit (PPU) and a Detection Unit (DTU), where the PPU utilizes a U-shaped net enriched by semantics to refine degraded images, and the DTU integrates this semantic information for object detection using a modified YOLO network. Our method pioneers the use of semantic data for all-weather transformations, resulting in an increase between 1.47\% to 8.80\% in mAP compared to existing methods across benchmark datasets of different weather. This highlights the potency of semantics in image enhancement and object detection, offering a comprehensive approach to improving object detection performance. Code will be available at https://github.com/EnisZuo/SemOD.

</details>


### [37] [Stacked Ensemble of Fine-Tuned CNNs for Knee Osteoarthritis Severity Grading](https://arxiv.org/abs/2511.22143)
*Adarsh Gupta,Japleen Kaur,Tanvi Doshi,Teena Sharma,Nishchal K. Verma,Shantaram Vasikarla*

Main category: cs.CV

TL;DR: 本研究提出了一种由多种预训练神经网络组成的堆叠集成模型，自动识别和分级膝关节骨关节炎（KOA），有效提升了诊断效率与准确率。


<details>
  <summary>Details</summary>
Motivation: 传统基于X光片和KL分级系统的KOA诊断不仅耗时且需专家参与，且易受主观影响，导致诊断结果不稳定。科研需求是寻求高效准确且能客观量化的自动化KOA诊断方法。

Method: 研究构建了一个由MobileNetV2、YOLOv8和DenseNet201三种预训练CNN作为基分类器、CatBoost为元学习器的堆叠集成模型。模型用于两类任务：KOA有无（二分类）和KL 0-4分级（多分类）。

Result: 所提出模型在多分类任务上的正确率达到73%，在二分类任务上达到87.5%，均高于文献中已报道水平。

Conclusion: 基于堆叠集成模型的自动KOA识别与分级能够提高诊断的自动化和准确性，减少人工参与和主观误差，在实际临床应用中具有较大潜力。

Abstract: Knee Osteoarthritis (KOA) is a musculoskeletal condition that can cause significant limitations and impairments in daily activities, especially among older individuals. To evaluate the severity of KOA, typically, X-ray images of the affected knee are analyzed, and a grade is assigned based on the Kellgren-Lawrence (KL) grading system, which classifies KOA severity into five levels, ranging from 0 to 4. This approach requires a high level of expertise and time and is susceptible to subjective interpretation, thereby introducing potential diagnostic inaccuracies. To address this problem a stacked ensemble model of fine-tuned Convolutional Neural Networks (CNNs) was developed for two classification tasks: a binary classifier for detecting the presence of KOA, and a multiclass classifier for precise grading across the KL spectrum. The proposed stacked ensemble model consists of a diverse set of pre-trained architectures, including MobileNetV2, You Only Look Once (YOLOv8), and DenseNet201 as base learners and Categorical Boosting (CatBoost) as the meta-learner. This proposed model had a balanced test accuracy of 73% in multiclass classification and 87.5% in binary classification, which is higher than previous works in extant literature.

</details>


### [38] [RemedyGS: Defend 3D Gaussian Splatting against Computation Cost Attacks](https://arxiv.org/abs/2511.22147)
*Yanping Li,Zhening Liu,Zijian Li,Zehong Lin,Jun Zhang*

Main category: cs.CV

TL;DR: 本文针对3D高斯拟合（3DGS）系统的计算消耗攻击问题，提出了RemedyGS黑盒防护框架，有效提升了系统安全性。


<details>
  <summary>Details</summary>
Motivation: 3D高斯拟合（3DGS）作为主流三维重建技术，广泛应用于各类服务，但近期研究发现其存在易受资源消耗攻击的安全漏洞，可能导致服务拒绝（DoS）及恶意资源占用。因此亟需有效的防御机制保障其可靠性。

Method: 提出了名为RemedyGS的防御框架，包括两个主要组件：1）检测器识别带有恶意纹理的攻击输入图像；2）净化器对检测到的被攻击图像进行修复净化。净化器结合了对抗性训练，促使还原图像的分布与原始自然图像对齐，从而提升防御效果。

Result: 实验显示，RemedyGS对3DGS系统下的白盒、黑盒及自适应攻击均具有优异的防御能力，在安全性和实用性方面达到了当前最优表现。

Conclusion: RemedyGS能够有效防护针对3DGS的多种计算资源消耗攻击，为三维重建系统的安全部署提供了有力保障。

Abstract: As a mainstream technique for 3D reconstruction, 3D Gaussian splatting (3DGS) has been applied in a wide range of applications and services. Recent studies have revealed critical vulnerabilities in this pipeline and introduced computation cost attacks that lead to malicious resource occupancies and even denial-of-service (DoS) conditions, thereby hindering the reliable deployment of 3DGS. In this paper, we propose the first effective and comprehensive black-box defense framework, named RemedyGS, against such computation cost attacks, safeguarding 3DGS reconstruction systems and services. Our pipeline comprises two key components: a detector to identify the attacked input images with poisoned textures and a purifier to recover the benign images from their attacked counterparts, mitigating the adverse effects of these attacks. Moreover, we incorporate adversarial training into the purifier to enforce distributional alignment between the recovered and original natural images, thereby enhancing the defense efficacy. Experimental results demonstrate that our framework effectively defends against white-box, black-box, and adaptive attacks in 3DGS systems, achieving state-of-the-art performance in both safety and utility.

</details>


### [39] [IMTalker: Efficient Audio-driven Talking Face Generation with Implicit Motion Transfer](https://arxiv.org/abs/2511.22167)
*Bo Chen,Tao Liu,Qi Chen,Xie Chen,Zilong Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为IMTalker的新方法，通过隐式动作迁移，实现了高效和高保真的对口说话人脸生成。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要依赖光流和局部变形，难以建模复杂的全局动作并可能导致身份漂移，影响生成效果。

Method: IMTalker利用跨注意力机制代替传统基于光流的变形，在统一的隐空间中隐式建模动作差异与身份对齐，实现稳健的全局动作渲染。为解决跨身份重演中的身份保持，引入身份自适应模块，将动作潜变量映射到个性化空间，清晰地实现动作与身份的解耦。同时，设计了轻量级的流匹配动作生成器，可从音频、姿态和凝视信号生成生动、可控的隐式动作向量。

Result: 实验表明，IMTalker在动作精度、身份保持以及音频-唇形同步方面均超越了现有方法，并且效率极高：在 RTX 4090 上，基于视频和基于音频的生成速度分别达到40 FPS和42 FPS。

Conclusion: IMTalker在保真度、动作控制和身份一致性上展现出明显优势，实现了业界最优性能并保持高效率，有望促进实际应用和后续研究，该方法的代码和预训练模型将对外开放。

Abstract: Talking face generation aims to synthesize realistic speaking portraits from a single image, yet existing methods often rely on explicit optical flow and local warping, which fail to model complex global motions and cause identity drift. We present IMTalker, a novel framework that achieves efficient and high-fidelity talking face generation through implicit motion transfer. The core idea is to replace traditional flow-based warping with a cross-attention mechanism that implicitly models motion discrepancy and identity alignment within a unified latent space, enabling robust global motion rendering. To further preserve speaker identity during cross-identity reenactment, we introduce an identity-adaptive module that projects motion latents into personalized spaces, ensuring clear disentanglement between motion and identity. In addition, a lightweight flow-matching motion generator produces vivid and controllable implicit motion vectors from audio, pose, and gaze cues. Extensive experiments demonstrate that IMTalker surpasses prior methods in motion accuracy, identity preservation, and audio-lip synchronization, achieving state-of-the-art quality with superior efficiency, operating at 40 FPS for video-driven and 42 FPS for audio-driven generation on an RTX 4090 GPU. We will release our code and pre-trained models to facilitate applications and future research.

</details>


### [40] [Real-Time Long Horizon Air Quality Forecasting via Group-Relative Policy Optimization](https://arxiv.org/abs/2511.22169)
*Inha Kang,Eunki Kim,Wonjeong Ryu,Jaeyo Shin,Seungjun Yu,Yoon-Hee Kang,Seongeun Jeong,Eunhye Kim,Soontae Kim,Hyunjung Shim*

Main category: cs.CV

TL;DR: 本文针对东亚等复杂地形和强大气动力区域的PM（颗粒物）浓度长时段预报难题，提出了一种结合新高分辨观测数据集和新型优化方法（GRPO）的解决方案，显著提升了实际预警系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前全球通用的气象基础模型无法很好捕捉区域特有的气候动力，且常需依赖非实时数据，难以满足本地化、紧迫的公共卫生预警需求。尤其传统预报方法在成本敏感的实用场景下难以平衡漏报和误报的风险，影响预警有效性和公众信任。

Method: 作者提出并发布高分辨、高实用性的CMAQ-OBS东亚数据集，结合Group-Relative Policy Optimization（GRPO）方法，通过类别奖励和课程式训练，优化模型在实际操作中对于漏报和误报带来的不同成本，实现48-120小时的实时空气质量预报。

Result: 实验结果显示，该方法相较于标准SFT（Supervised Fine-Tuning）方法，降低了59.5%的区域误差，误报率减少了47.3%，同时保持较高的F1分数，提升了预警系统准确性和实用性。

Conclusion: 本文所构建的数据集和提出的方法有效提升了复杂区域长时距PM预报的实用价值，对健康预警系统具有重要意义，展示了在实际部署中的可靠性和先进性。

Abstract: Accurate long horizon forecasting of particulate matter (PM) concentration fields is essential for operational public health decisions. However, achieving reliable forecasts remains challenging in regions with complex terrain and strong atmospheric dynamics such as East Asia. While foundation models such as Aurora offer global generality, they often miss region-specific dynamics and rely on non-real-time inputs, limiting their practical utility for localized warning systems. To address this gap, we construct and release the real-world observations and high-resolution CMAQ-OBS dataset for East Asia, reducing regional error by 59.5% and enabling real-time 48-120 hour forecasts critical for public health alerts. However, standard point-wise objectives cannot reflect asymmetric operational costs, where false alarms deteriorate public trust while missed severe events endanger populations. This cost mismatch causes SFT models to over-predict and yield high False Alarm Rates. We introduce Group-Relative Policy Optimization (GRPO) with class-wise rewards and curriculum rollout to align predictions with operational priorities. Experimental results demonstrate that our framework significantly improves the reliability of the forecast. Compared to the SFT-only baseline, our model reduces the False Alarm Rate by 47.3% while achieving a competitive F1-score, proving its effectiveness for practical, real-world air quality forecasting systems on long lead time scenarios.

</details>


### [41] [Partially Shared Concept Bottleneck Models](https://arxiv.org/abs/2511.22170)
*Delong Zhao,Qiang Huang,Di Yan,Yiqun Sun,Jun Yu*

Main category: cs.CV

TL;DR: PS-CBM是一种改进的概念瓶颈模型，通过多模态概念生成、部分共享策略和新的评估指标，提升了可解释性和准确率，并减少了所需概念数量。


<details>
  <summary>Details</summary>
Motivation: 现有自动化生成概念的CBM方法面临视觉落地性差、概念冗余和缺乏权衡准确性与紧凑性的评估指标等问题。

Method: 提出PS-CBM框架，包括：1）多模态概念生成器，将语言模型语义与视觉样例结合；2）部分共享概念策略，根据激活模式合并概念，实现特异性与紧凑性的平衡；3）提出概念高效准确率（CEA）指标，结合准确度和概念紧凑性进行评估。

Result: 在11个数据集上，PS-CBM的分类准确率提升1.0%-7.4%，CEA提升2.0%-9.5%，概念数量显著减少，均超越现有主流CBM方法。

Conclusion: PS-CBM能够兼顾高准确率和强可解释性，优于当前最先进的CBM方法。

Abstract: Concept Bottleneck Models (CBMs) enhance interpretability by introducing a layer of human-understandable concepts between inputs and predictions. While recent methods automate concept generation using Large Language Models (LLMs) and Vision-Language Models (VLMs), they still face three fundamental challenges: poor visual grounding, concept redundancy, and the absence of principled metrics to balance predictive accuracy and concept compactness. We introduce PS-CBM, a Partially Shared CBM framework that addresses these limitations through three core components: (1) a multimodal concept generator that integrates LLM-derived semantics with exemplar-based visual cues; (2) a Partially Shared Concept Strategy that merges concepts based on activation patterns to balance specificity and compactness; and (3) Concept-Efficient Accuracy (CEA), a post-hoc metric that jointly captures both predictive accuracy and concept compactness. Extensive experiments on eleven diverse datasets show that PS-CBM consistently outperforms state-of-the-art CBMs, improving classification accuracy by 1.0%-7.4% and CEA by 2.0%-9.5%, while requiring significantly fewer concepts. These results underscore PS-CBM's effectiveness in achieving both high accuracy and strong interpretability.

</details>


### [42] [BrepGPT: Autoregressive B-rep Generation with Voronoi Half-Patch](https://arxiv.org/abs/2511.22171)
*Pu Li,Wenhao Zhang,Weize Quan,Biao Zhang,Peter Wonka,Dong-Ming Yan*

Main category: cs.CV

TL;DR: 本文提出了一种名为BrepGPT的单阶段自动回归框架，用于高效生成CAD中常用的边界表示（B-rep）模型，通过创新的Voronoi Half-Patch（VHP）表示和双VQ-VAE编码，实现了结构紧凑、泛化性强的B-rep建模，优于现有多阶段方法。


<details>
  <summary>Details</summary>
Motivation: 现有B-rep生成方法由于几何与拓扑强耦合，普遍采用多阶段网络，导致误差累积和效率低下。作者希望通过开发统一、紧凑的表征及单阶段框架，解决现有方法的冗余与复杂性。

Method: 提出Voronoi Half-Patch（VHP）表示，将B-rep拆解为局部单元，并统一编码几何与拓扑信息；采用双VQ-VAE将顶点拓扑和VHP编码为token；通过decoder-only Transformer自回归地生成这些token，最后解码为完整B-rep模型。

Result: BrepGPT在无条件B-rep生成任务上取得了领先性能，并且适用多种条件生成应用，如根据标签、点云、文本和图像生成B-rep，以及自动补全和插值。

Conclusion: BrepGPT通过创新表征和端到端单阶段建模，实现了精度提升与效率优化，推动了B-rep生成方法的发展，对CAD设计等场景具有广泛应用潜力。

Abstract: Boundary representation (B-rep) is the de facto standard for CAD model representation in modern industrial design. The intricate coupling between geometric and topological elements in B-rep structures has forced existing generative methods to rely on cascaded multi-stage networks, resulting in error accumulation and computational inefficiency. We present BrepGPT, a single-stage autoregressive framework for B-rep generation. Our key innovation lies in the Voronoi Half-Patch (VHP) representation, which decomposes B-reps into unified local units by assigning geometry to nearest half-edges and sampling their next pointers. Unlike hierarchical representations that require multiple distinct encodings for different structural levels, our VHP representation facilitates unifying geometric attributes and topological relations in a single, coherent format. We further leverage dual VQ-VAEs to encode both vertex topology and Voronoi Half-Patches into vertex-based tokens, achieving a more compact sequential encoding. A decoder-only Transformer is then trained to autoregressively predict these tokens, which are subsequently mapped to vertex-based features and decoded into complete B-rep models. Experiments demonstrate that BrepGPT achieves state-of-the-art performance in unconditional B-rep generation. The framework also exhibits versatility in various applications, including conditional generation from category labels, point clouds, text descriptions, and images, as well as B-rep autocompletion and interpolation.

</details>


### [43] [Guiding the Inner Eye: A Framework for Hierarchical and Flexible Visual Grounded Reasoning](https://arxiv.org/abs/2511.22172)
*Zhaoyang Wei,Wenchao Ding,Yanchao Hao,Xi Chen*

Main category: cs.CV

TL;DR: 本文提出了GRiP（Guided Reasoning and Perception）训练框架，通过引入认知激励的强化学习策略，显著提升了视觉推理能力，并在多个基准测试上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在训练过程中常常受困于端到端强化学习的不稳定性和有监督微调的僵化，导致模型在复杂视觉场景中的推理能力有限，因此需要一种兼具灵活性和稳定性的新方法。

Method: 提出了GRiP框架，分为两阶段训练：首先利用认知增强的强化学习阶段引导模型——包括突出目标重要性的Salience-Weighted IoU奖励机制和激励多样逻辑路径的Multi-Heuristic奖励机制。实验基于Qwen2.5-VL-7B模型初始化。

Result: GRiP在TreeBench和V* Bench等多个复杂基准任务中，相较同类开源模型表现突出，达到了新的最先进水平，证明了该方法在复杂视觉推理任务中的效果。

Conclusion: 通过引入模拟人类认知的奖励信号（关注什么、如何思考），GRiP有效提升了多模态模型的推理能力，为实现更高级别多模态智能指明了方向。

Abstract: Models capable of "thinking with images" by dynamically grounding their reasoning in visual evidence represent a major leap in multimodal AI. However, replicating and advancing this ability is non-trivial, with current methods often trapped between the instability of end-to-end reinforcement learning (RL) and the rigidity of supervised fine-tuning (SFT). This leads to models that either struggle to learn or lack the cognitive flexibility required for complex, real-world scenes. To navigate this dilemma, we introduce GRiP (Guided Reasoning and Perception), a novel two-stage training framework that cultivates robust and flexible visual grounded reasoning by explicitly guiding the model's perceptual focus and logical pathways. GRiP's core lies in its cognitive-enhanced RL stage, which features two key innovations: (1) a Salience-Weighted IoU Reward that incentivizes the model to prioritize the localization of mission-critical objects over trivial distractors, and (2) a Multi-Heuristic Reward that encourages cognitive flexibility by rewarding diverse yet logically valid reasoning pathways. Initialized from the Qwen2.5-VL-7B model, GRiP demonstrates significant performance gains across multiple challenging benchmarks. It achieves state-of-the-art results among open-source models on the highly challenging TreeBench and V* Bench, proving its effectiveness in complex visual reasoning. Our work demonstrates that moving beyond simplistic rewards and instead guiding models with cognitively-inspired signals for what to see and how to think is crucial for unlocking the next level of multimodal intelligence. The code will be made publicly available.

</details>


### [44] [Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving](https://arxiv.org/abs/2511.19221)
*Jianhua Han,Meng Tian,Jiangtong Zhu,Fan He,Huixin Zhang,Sitong Guo,Dechang Zhu,Hao Tang,Pei Xu,Yuze Guo,Minzhe Niu,Haojie Zhu,Qichao Dong,Xuechao Yan,Siyuan Dong,Lu Hou,Qingqiu Huang,Xiaosong Jia,Hang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新型的自动驾驶感知模型Percept-WAM，有效提升了复杂和长尾场景下的空间感知与决策性能，达到或超过当前主流检测器和分割器水平。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶领域，空间感知的准确性对整体安全性至关重要。现有视觉-语言模型（VLM）在空间定位和理解上表现欠佳，导致感知系统容易在长尾场景和复杂交互中失效。因此，急需一种能增强空间感知、统一2D/3D理解且能直接输出规划决策的统一模型。

Method: Percept-WAM首次将2D/3D场景理解能力隐式集成进单一的视觉-语言模型，不依赖传统的QA式推理。通过提出World-PV与World-BEV token，编码空间坐标与置信度，并设计了基于网格的密集目标预测机制（包括IoU感知的评分和并行自回归解码），提升了远距离、小目标和长尾情景下的稳定性。同时，模型利用预训练VLM参数保持通用智能能力，能直接输出感知和轨迹控制结果。

Result: Percept-WAM在COCO 2D与nuScenes BEV 3D基准上获得51.7/58.9 mAP，达到或超越传统检测/分割器。在规划任务上结合轨迹解码器后，于nuScenes和NAVSIM数据集表现优异，在NAVSIM上PMDS指标比DiffusionDrive高2.1。定性实验也显示其强大的开放词汇能力和长尾泛化能力。

Conclusion: Percept-WAM突破了当前视觉-语言模型在空间感知上的瓶颈，实现了2D和3D场景理解的有效融合，并可一体化输出感知结果与决策控制，表现出优良的泛化和实际应用潜力。

Abstract: Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.

</details>


### [45] [Enhanced Graph Convolutional Network with Chebyshev Spectral Graph and Graph Attention for Autism Spectrum Disorder Classification](https://arxiv.org/abs/2511.22178)
*Adnan Ferdous Ashrafi,Hasanul Kabir*

Main category: cs.CV

TL;DR: 本文提出了一种结合切比雪夫谱图卷积和图注意力网络（GAT）的多模态图卷积网络（GCN）模型，用于提高自闭症谱系障碍（ASD）诊断的准确率。


<details>
  <summary>Details</summary>
Motivation: ASD的早期和客观诊断存在巨大挑战，因其症状表现和神经基础高度异质。提升基于神经影像和表型数据的识别准确性对于疾病早期干预和研究至关重要。

Method: 利用ABIDE I数据库（含870名患者的静息态功能磁共振、结构磁共振及表型数据），设计多分支GCN架构，分别处理不同模态后合并特征。采用基于采集地点的相似性建立群体图结构，利用切比雪夫多项式实现低计算复杂度的局部谱学习，GAT层通过注意力机制增强节点特征。全流程采用分层五折交叉验证。

Result: 模型在全部数据上的测试准确率达到74.82%，AUC为0.82，优于多种主流方法（传统GCN、自编码器神经网络、多模态CNN等）。

Conclusion: 多模态图神经网络能够有效融合ASD相关多源数据，提升诊断准确率，为未来自动化和客观化ASD早筛提供了有力方法。

Abstract: ASD is a complicated neurodevelopmental disorder marked by variation in symptom presentation and neurological underpinnings, making early and objective diagnosis extremely problematic. This paper presents a Graph Convolutional Network (GCN) model, incorporating Chebyshev Spectral Graph Convolution and Graph Attention Networks (GAT), to increase the classification accuracy of ASD utilizing multimodal neuroimaging and phenotypic data. Leveraging the ABIDE I dataset, which contains resting-state functional MRI (rs-fMRI), structural MRI (sMRI), and phenotypic variables from 870 patients, the model leverages a multi-branch architecture that processes each modality individually before merging them via concatenation. Graph structure is encoded using site-based similarity to generate a population graph, which helps in understanding relationship connections across individuals. Chebyshev polynomial filters provide localized spectral learning with lower computational complexity, whereas GAT layers increase node representations by attention-weighted aggregation of surrounding information. The proposed model is trained using stratified five-fold cross-validation with a total input dimension of 5,206 features per individual. Extensive trials demonstrate the enhanced model's superiority, achieving a test accuracy of 74.82\% and an AUC of 0.82 on the entire dataset, surpassing multiple state-of-the-art baselines, including conventional GCNs, autoencoder-based deep neural networks, and multimodal CNNs.

</details>


### [46] [MTR-VP: Towards End-to-End Trajectory Planning through Context-Driven Image Encoding and Multiple Trajectory Prediction](https://arxiv.org/abs/2511.22181)
*Maitrayee Keskar,Mohan Trivedi,Ross Greer*

Main category: cs.CV

TL;DR: 本文提出了MTR-VP模型，利用视觉和运动历史信息，通过Transformer架构进行自动驾驶轨迹规划，结果显示视觉与动力学特征结合效果一般，但多轨迹输出能提升规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶轨迹预测多依赖于地图信息和手工提取的特征，难以直接从视觉信息中提取丰富的场景上下文。作者希望以视觉为主替代地图特征，探索通过视觉特征和历史运动信息提升轨迹预测与规划能力。

Method: 提出MTR-VP方法，使用ViT将图像和运动历史编码为上下文特征，替换传统的地图特征，并结合MTR（Motion Transformer）思路，相较MTR解码器采用交叉注意力，把意图和上下文特征结合。模型在Waymo数据集上测试，通过消融实验分析输入和输出方式影响。

Result: 实验发现，Transformer方法难以高效融合视觉和动力学特征，即使引入CLIP或DINOv2等视觉基础模型特征，效果也有限。但采用多轨迹分布预测（而非单一路径）可明显提升规划表现。

Conclusion: 单一视觉与动力学特征的融合尚未能统一有效编码场景，但多模态分布预测对规划更为有效，未来研究可进一步探索如何优化视觉-动力融合与多轨迹预测策略。

Abstract: We present a method for trajectory planning for autonomous driving, learning image-based context embeddings that align with motion prediction frameworks and planning-based intention input. Within our method, a ViT encoder takes raw images and past kinematic state as input and is trained to produce context embeddings, drawing inspiration from those generated by the recent MTR (Motion Transformer) encoder, effectively substituting map-based features with learned visual representations. MTR provides a strong foundation for multimodal trajectory prediction by localizing agent intent and refining motion iteratively via motion query pairs; we name our approach MTR-VP (Motion Transformer for Vision-based Planning), and instead of the learnable intention queries used in the MTR decoder, we use cross attention on the intent and the context embeddings, which reflect a combination of information encoded from the driving scene and past vehicle states. We evaluate our methods on the Waymo End-to-End Driving Dataset, which requires predicting the agent's future 5-second trajectory in bird's-eye-view coordinates using prior camera images, agent pose history, and routing goals. We analyze our architecture using ablation studies, removing input images and multiple trajectory output. Our results suggest that transformer-based methods that are used to combine the visual features along with the kinetic features such as the past trajectory features are not effective at combining both modes to produce useful scene context embeddings, even when intention embeddings are augmented with foundation-model representations of scene context from CLIP and DINOv2, but that predicting a distribution over multiple futures instead of a single future trajectory boosts planning performance.

</details>


### [47] [Shoe Style-Invariant and Ground-Aware Learning for Dense Foot Contact Estimation](https://arxiv.org/abs/2511.22184)
*Daniel Sungho Jung,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新的FECO框架，实现了从单张RGB图像中对脚部接触的密集估计，并实现了对不同鞋款和地面特性的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深入研究脚部接触对于理解人体运动与物理交互至关重要。现有方法通常仅用零速度假设进行近似，缺乏对脚与世界之间精细交互的建模，因此需要开发更精确的方法来提升密集脚部接触估计的能力。

Method: 提出FECO（FEet COntact estimation）框架，包含两大技术：鞋款风格对抗训练（使特征对不同鞋款具有不变性）和基于空间上下文的地面特征提取器（更好地提取地面信息），共同提升脚部接触的泛化和准确性。

Result: 该方法不仅对鞋款具有鲁棒性，也能有效利用地面信息，实现了对脚部接触更准确的密集估计。

Conclusion: FECO框架在密集脚部接触估计方面取得了突破，可推广到多样的实际场景，并推动对人体足部物理交互的理解和相关应用的进步。

Abstract: Foot contact plays a critical role in human interaction with the world, and thus exploring foot contact can advance our understanding of human movement and physical interaction. Despite its importance, existing methods often approximate foot contact using a zero-velocity constraint and focus on joint-level contact, failing to capture the detailed interaction between the foot and the world. Dense estimation of foot contact is crucial for accurately modeling this interaction, yet predicting dense foot contact from a single RGB image remains largely underexplored. There are two main challenges for learning dense foot contact estimation. First, shoes exhibit highly diverse appearances, making it difficult for models to generalize across different styles. Second, ground often has a monotonous appearance, making it difficult to extract informative features. To tackle these issues, we present a FEet COntact estimation (FECO) framework that learns dense foot contact with shoe style-invariant and ground-aware learning. To overcome the challenge of shoe appearance diversity, our approach incorporates shoe style adversarial training that enforces shoe style-invariant features for contact estimation. To effectively utilize ground information, we introduce a ground feature extractor that captures ground properties based on spatial context. As a result, our proposed method achieves robust foot contact estimation regardless of shoe appearance and effectively leverages ground information. Code will be released.

</details>


### [48] [HybridWorldSim: A Scalable and Controllable High-fidelity Simulator for Autonomous Driving](https://arxiv.org/abs/2511.22187)
*Qiang Li,Yingwenqi Jiang,Tuoxi Li,Duyu Chen,Xiang Feng,Yucheng Ao,Shangyue Liu,Xingchen Yu,Youcheng Cai,Yumeng Liu,Yuexin Ma,Xin Hu,Li Liu,Yu Zhang,Linkun Xu,Bingtao Gao,Xueyuan Wang,Shuchang Zhou,Xianming Liu,Ligang Liu*

Main category: cs.CV

TL;DR: 提出了一种能够生成真实且可控的自动驾驶仿真框架HybridWorldSim，并发布了新的多路径数据集MIRROR，有效提升了仿真质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶仿真方法在大视角变化下的视图合成及几何一致性方面存在不足，难以为端到端自动驾驶研究提供高保真、具备多样性的仿真环境。

Method: HybridWorldSim框架将多遍行神经重建技术用于静态背景，并利用生成建模方法处理动态体，实现了静态和动态元素的统一高保真建模。此外，作者还推出了包含不同城市多种环境条件的MIRROR数据集，用于更全面和鲁棒的评测。

Result: 实验表明，HybridWorldSim在视觉和空间一致性、仿真多样性等方面优于已有最先进方法，能够生成多样、逼真的驾驶场景。

Conclusion: HybridWorldSim为高保真自动驾驶仿真提供了实用、可扩展的解决方案，MIRROR数据集则为相关研究和应用开发带来重要资源。

Abstract: Realistic and controllable simulation is critical for advancing end-to-end autonomous driving, yet existing approaches often struggle to support novel view synthesis under large viewpoint changes or to ensure geometric consistency. We introduce HybridWorldSim, a hybrid simulation framework that integrates multi-traversal neural reconstruction for static backgrounds with generative modeling for dynamic agents. This unified design addresses key limitations of previous methods, enabling the creation of diverse and high-fidelity driving scenarios with reliable visual and spatial consistency. To facilitate robust benchmarking, we further release a new multi-traversal dataset MIRROR that captures a wide range of routes and environmental conditions across different cities. Extensive experiments demonstrate that HybridWorldSim surpasses previous state-of-the-art methods, providing a practical and scalable solution for high-fidelity simulation and a valuable resource for research and development in autonomous driving.

</details>


### [49] [ARPGNet: Appearance- and Relation-aware Parallel Graph Attention Fusion Network for Facial Expression Recognition](https://arxiv.org/abs/2511.22188)
*Yan Li,Yong Zhao,Xiaohan Xia,Dongmei Jiang*

Main category: cs.CV

TL;DR: 本论文提出了一种结合外观和关系信息的并行图注意力融合网络（ARPGNet），用于提升面部表情识别的准确性，通过同时建模面部区域间的关系和时序动态，达到了优于或相当于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有面部表情识别方法主要依赖预训练的CNN提取面部外观特征，往往忽略了不同面部区域间的关系，导致对表情动态的表达不充分。为了解决这一问题，本文引入区域间关系建模以提升表达判别能力。

Method: 作者首先构建面部区域关系图，并利用图注意力机制对区域间关系进行建模。随后，将基于CNN的外观特征序列和关系特征序列输入到并行的图注意力融合模块，实现特征的互补与动态增强。该模块可同时捕捉异质特征间的互补性和每类特征的时序动态。

Result: 在三个面部表情识别数据集上的实验表明，所提出的ARPGNet在精度上优于或与当前最先进方法持平。

Conclusion: 结合外观与关系特征，通过并行图注意力机制互补增强面部表情时空表示，有效提升了识别性能，验证了本文方法的有效性。

Abstract: The key to facial expression recognition is to learn discriminative spatial-temporal representations that embed facial expression dynamics. Previous studies predominantly rely on pre-trained Convolutional Neural Networks (CNNs) to learn facial appearance representations, overlooking the relationships between facial regions. To address this issue, this paper presents an Appearance- and Relation-aware Parallel Graph attention fusion Network (ARPGNet) to learn mutually enhanced spatial-temporal representations of appearance and relation information. Specifically, we construct a facial region relation graph and leverage the graph attention mechanism to model the relationships between facial regions. The resulting relational representation sequences, along with CNN-based appearance representation sequences, are then fed into a parallel graph attention fusion module for mutual interaction and enhancement. This module simultaneously explores the complementarity between different representation sequences and the temporal dynamics within each sequence. Experimental results on three facial expression recognition datasets demonstrate that the proposed ARPGNet outperforms or is comparable to state-of-the-art methods.

</details>


### [50] [Controllable 3D Object Generation with Single Image Prompt](https://arxiv.org/abs/2511.22194)
*Jaeseok Lee,Jaekoo Lee*

Main category: cs.CV

TL;DR: 该论文提出两种创新方法，无需文本反演即可增强3D生成模型的控制能力，同时提升3D一致性，实现了与现有主流方法相媲美甚至更好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成任务多采用基于文本反演的扩散模型，但文本反演不仅训练耗时，还缺乏对生成过程的精确控制。为解决这一问题，作者着手寻找无需文本反演且具更好控制能力的新方法。

Method: 作者提出：（1）利用现有的图像adapter模块，无需文本反演，通过对深度、姿态和文本等条件实现直接控制；（2）引入深度条件warmup策略以提升输出3D对象的一致性。

Result: 实验显示，所提方法在定量和定性评估中性能与基于文本反演的方法持平或更优，且3D一致性更好。用户调查也证实结果与输入更匹配且3D一致性更高。

Conclusion: 新方法无需文本反演、控制力增强，在3D生成领域可为效果与效率兼顾的新路径，具有较高实用价值。

Abstract: Recently, the impressive generative capabilities of diffusion models have been demonstrated, producing images with remarkable fidelity. Particularly, existing methods for the 3D object generation tasks, which is one of the fastest-growing segments in computer vision, pre-dominantly use text-to-image diffusion models with textual inversion which train a pseudo text prompt to describe the given image. In practice, various text-to-image generative models employ textual inversion to learn concepts or styles of target object in the pseudo text prompt embedding space, thereby generating sophisticated outputs. However, textual inversion requires additional training time and lacks control ability. To tackle this issues, we propose two innovative methods: (1) using an off-the-shelf image adapter that generates 3D objects without textual inversion, offering enhanced control over conditions such as depth, pose, and text. (2) a depth conditioned warmup strategy to enhance 3D consistency. In experimental results, ours show qualitatively and quantitatively comparable performance and improved 3D consistency to the existing text-inversion-based alternatives. Furthermore, we conduct a user study to assess (i) how well results match the input image and (ii) whether 3D consistency is maintained. User study results show that our model outperforms the alternatives, validating the effectiveness of our approaches. Our code is available at GitHub repository:https://github.com/Seooooooogi/Control3D_IP/

</details>


### [51] [MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory](https://arxiv.org/abs/2511.22609)
*Bo Wang,Jiehong Lin,Chenzhi Liu,Xinting Hu,Yifei Yu,Tianjia Liu,Zhongrui Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: MG-Nav提出了一种新颖的零样本视觉导航系统，通过结合全局记忆引导与局部几何控制，实现了显著提升的导航表现。核心是一种稀疏空间记忆图，可提升定位与目标识别能力，实验验证其在主流基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着机器人导航需求的增加，特别是在未见过的新环境中零样本导航，现有方法在全局路径规划与局部避障控制间协同不足，导致泛化能力和目标识别准确性有限。该工作旨在突破现有方法局限，实现高效、鲁棒和具备强泛化能力的零样本视觉导航。

Method: 提出MG-Nav框架，包含两大模块：1）基于稀疏空间记忆图（SMG）的全局规划，图节点整合多视角关键帧及物体语义，实现高效目标路径生成；2）局部执行策略结合基础点目标控制与障碍感知，目标接近时切换到基于图像目标的精准导航。同时引入VGGT-adapter几何模块，提升特征对齐及3D感知能力。

Result: MG-Nav在HM3D Instance-Image-Goal和MP3D Image-Goal等权威基准上达到了最新的零样本导航性能，对动态变化及新场景显示出良好鲁棒性。

Conclusion: MG-Nav有效融合了全局记忆和局部几何感知，推动了零样本视觉导航的表现，展示了其在复杂未知环境下的实用潜力。

Abstract: We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.

</details>


### [52] [3D-Consistent Multi-View Editing by Diffusion Guidance](https://arxiv.org/abs/2511.22228)
*Josef Bengtson,David Nilsson,Dong In Lee,Fredrik Kahl*

Main category: cs.CV

TL;DR: 本文提出了一种无需额外训练的扩散模型框架，实现了面向多视角一致性的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的图像编辑方法在处理同一场景的不同视角时，经常出现几何和光度不一致的问题，尤其在编辑3D表示如NeRFs或Gaussian Splat模型时更为突出。因而需要一种能够保证多视角一致性的3D图像编辑方法。

Method: 本文提出了一种在图像编辑过程中，通过引入一致性损失（consistency loss）来确保未编辑图像的对应点在编辑后经历类似变换的训练自由扩散框架。该方法无需额外训练，可以与多种图像编辑方法结合，支持稠密和稀疏多视角编辑。

Result: 实验结果表明，该方法在3D一致性方面优于现有多视角编辑方法。还展示了更高一致性使高质量、高细节、强文本提示遵从的Gaussian Splat编辑成为可能。

Conclusion: 引入多视角一致性约束的扩散模型框架大大提升了3D视角一致性的图像编辑效果，为复杂3D表示的高保真编辑提供了有效方案。

Abstract: Recent advancements in diffusion models have greatly improved text-based image editing, yet methods that edit images independently often produce geometrically and photometrically inconsistent results across different views of the same scene. Such inconsistencies are particularly problematic for editing of 3D representations such as NeRFs or Gaussian Splat models. We propose a training-free diffusion framework that enforces multi-view consistency during the image editing process. The key assumption is that corresponding points in the unedited images should undergo similar transformations after editing. To achieve this, we introduce a consistency loss that guides the diffusion sampling toward coherent edits. The framework is flexible and can be combined with widely varying image editing methods, supporting both dense and sparse multi-view editing setups. Experimental results show that our approach significantly improves 3D consistency compared to existing multi-view editing methods. We also show that this increased consistency enables high-quality Gaussian Splat editing with sharp details and strong fidelity to user-specified text prompts. Please refer to our project page for video results: https://3d-consistent-editing.github.io/

</details>


### [53] [RobotSeg: A Model and Dataset for Segmenting Robots in Image and Video](https://arxiv.org/abs/2511.22950)
*Haiyang Mei,Qiming Huang,Hai Ci,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文提出了一种专门针对机器人分割任务的基础模型RobotSeg，显著提升了机器人图像与视频分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管当前分割模型能力强大，但由于机器人结构多变、外观模糊和状态变化快等因素，机器人分割仍具有挑战性。因此需要设计专门面向机器人领域的分割方案。

Method: RobotSeg以SAM 2基础模型为核心，通过结构增强记忆关联器、机器人提示生成器和标签高效训练策略，解决了现有模型难以适应关节型机器人、依赖人工提示及强标签依赖等问题。同时，作者还构建了包含2.8k+视频（13.8万帧）的VRS数据集。

Result: 实验表明，RobotSeg在图像和视频机器人分割任务上均取得了最优（state-of-the-art）表现，显著优于现有方法。

Conclusion: RobotSeg为机器人感知提供了强有力的基础模型，推动机器人视觉分割和感知领域的进一步发展。

Abstract: Accurate robot segmentation is a fundamental capability for robotic perception. It enables precise visual servoing for VLA systems, scalable robot-centric data augmentation, accurate real-to-sim transfer, and reliable safety monitoring in dynamic human-robot environments. Despite the strong capabilities of modern segmentation models, surprisingly it remains challenging to segment robots. This is due to robot embodiment diversity, appearance ambiguity, structural complexity, and rapid shape changes. Embracing these challenges, we introduce RobotSeg, a foundation model for robot segmentation in image and video. RobotSeg is built upon the versatile SAM 2 foundation model but addresses its three limitations for robot segmentation, namely the lack of adaptation to articulated robots, reliance on manual prompts, and the need for per-frame training mask annotations, by introducing a structure-enhanced memory associator, a robot prompt generator, and a label-efficient training strategy. These innovations collectively enable a structure-aware, automatic, and label-efficient solution. We further construct the video robot segmentation (VRS) dataset comprising over 2.8k videos (138k frames) with diverse robot embodiments and environments. Extensive experiments demonstrate that RobotSeg achieves state-of-the-art performance on both images and videos, establishing a strong foundation for future advances in robot perception.

</details>


### [54] [From Compound Figures to Composite Understanding: Developing a Multi-Modal LLM from Biomedical Literature with Medical Multiple-Image Benchmarking and Validation](https://arxiv.org/abs/2511.22232)
*Zhen Chen,Yihang Fu,Gabriel Madera,Mauro Giuffre,Serina Applebaum,Hyunjae Kim,Hua Xu,Qingyu Chen*

Main category: cs.CV

TL;DR: 本文提出了一个能理解多幅医学图像的多模态大语言模型M3LLM，并显著领先于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型多只针对单幅医学图像理解，限制了其在真实临床中的应用，而实际诊断常需多图综合分析。主要障碍在于缺少大规模高质量的多图像训练数据。

Method: 提出从生物医学文献中提取具有开源许可的复合图像与上下文文本，提出五阶段、上下文感知的指令生成范式，并以分而治之策略将多图分析拆解为可管理子任务，推动模型学习复杂时空与多模态关联。基于23.7万组复合图及文本开发M3LLM，并构建PMC-MI-Bench评测基准。

Result: M3LLM在多图、单图、纯文本和多选等任务上均显著超越通用及医学专用多模态大模型，且在肺部X光序列分析等任务上展现出极强泛化能力。

Conclusion: 工作首次搭建了可规模化开发多图理解医学多模态大模型的高效范式，有效衔接了文献资源与临床应用之间的空白。

Abstract: Multi-modal large language models (MLLMs) have shown promise in advancing healthcare. However, most existing models remain confined to single-image understanding, which greatly limits their applicability in clinical workflows. In practice, medical diagnosis and progression often require synthesizing information across multiple images from different modalities or time points. The development of medical MLLMs capable of such multi-image understanding has been hindered by the lack of large-scale, high-quality annotated training data. To address this limitation, we propose a novel framework that leverages license-permissive compound images in biomedical literature, as a rich yet underutilized data source for multi-image analysis. Specifically, we design a five-stage, context-aware instruction generation paradigm underpinned by a divide-and-conquer strategy. By decomposing multi-image analysis into manageable sub-tasks, this paradigm empowers MLLMs to move beyond single-panel analysis and provide a composite understanding by learning the complex spatial, temporal, and cross-modal relationships inherent in these compound figures. By parsing over 237,000 compound figures and their contextual text for instruction generation, we develop M3LLM, a medical multi-image multi-modal large language model. For benchmarking, we construct PMC-MI-Bench for composite understanding, manually validated by medical experts. Extensive experiments show that M3LLM significantly outperforms both general-purpose and specialized medical MLLMs across multi-image, single-image, text-only, and multi-choice scenarios. Notably, M3LLM exhibits strong generalization to longitudinal chest X-ray analysis using the MIMIC dataset. This work establishes a scalable and efficient paradigm for developing medical MLLMs capable of composite reasoning, bridging the gap between biomedical literature and real-world clinical applications.

</details>


### [55] [MrGS: Multi-modal Radiance Fields with 3D Gaussian Splatting for RGB-Thermal Novel View Synthesis](https://arxiv.org/abs/2511.22997)
*Minseong Kweon,Janghyun Kim,Ukcheol Shin,Jinsun Park*

Main category: cs.CV

TL;DR: 本文提出了MrGS，一种基于3D高斯溅射（3DGS）的多模态辐射场模型，可同时重建RGB和热红外三维场景，实现多模态高保真重建。


<details>
  <summary>Details</summary>
Motivation: 尽管NeRF和3DGS等技术在RGB场景重建表现突出，但对于包含热红外成像的多模态渲染探讨较少，现有方法普遍忽略了热传导和朗伯特属性等热成像特征。

Method: MrGS利用正交特征提取技术从单一外观特征中分别获取RGB和热特征，并根据各自朗伯特反射程度采用不同的视角嵌入策略。方法结合傅里叶热传导定律进行高斯间热量插值，同时用斯特藩-玻尔兹曼定律与反平方定律，建立带有深度感知的热辐射图，实现几何约束的热渲染。

Result: 实验结果表明，MrGS可以在减少高斯数量的同时，高保真地重建RGB和热场景。

Conclusion: MrGS为多模态场景重建提供了新的方法，充分捕捉物理特性，能高效高保真地实现RGB与热红外三维重建，在多模态渲染领域具有推广价值。

Abstract: Recent advances in Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) have achieved considerable performance in RGB scene reconstruction. However, multi-modal rendering that incorporates thermal infrared imagery remains largely underexplored. Existing approaches tend to neglect distinctive thermal characteristics, such as heat conduction and the Lambertian property. In this study, we introduce MrGS, a multi-modal radiance field based on 3DGS that simultaneously reconstructs both RGB and thermal 3D scenes. Specifically, MrGS derives RGB- and thermal-related information from a single appearance feature through orthogonal feature extraction and employs view-dependent or view-independent embedding strategies depending on the degree of Lambertian reflectance exhibited by each modality. Furthermore, we leverage two physics-based principles to effectively model thermal-domain phenomena. First, we integrate Fourier's law of heat conduction prior to alpha blending to model intensity interpolation caused by thermal conduction between neighboring Gaussians. Second, we apply the Stefan-Boltzmann law and the inverse-square law to formulate a depth-aware thermal radiation map that imposes additional geometric constraints on thermal rendering. Experimental results demonstrate that the proposed MrGS achieves high-fidelity RGB-T scene reconstruction while reducing the number of Gaussians.

</details>


### [56] [IE-SRGS: An Internal-External Knowledge Fusion Framework for High-Fidelity 3D Gaussian Splatting Super-Resolution](https://arxiv.org/abs/2511.22233)
*Xiang Feng,Tieshi Zhong,Shuo Chang,Weiliu Wang,Chengkai Wang,Yifei Chen,Yuhe Wang,Zhenzhong Kuang,Xuefei Yin,Yanming Zhu*

Main category: cs.CV

TL;DR: 提出IE-SRGS方法，实现低分辨率输入下高分辨率3D Gaussian Splatting模型重建，通过融合2D超分辨率先验和3DGS内部特征，显著提升重建质量，超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有从低分辨率数据重建高分辨率3DGS的方法依赖于2D超分辨率模型，但2D超分结果存在视角不一致和领域间隔问题，导致3D重建效果受限。需要一种能同时利用2D外部先验和3D内部特征的新策略，提升3DGS的重建质量。

Method: 作者提出IE-SRGS方法，结合2D超分辨率与深度估计模型生成高分图像和深度图作为外部知识，同时利用多尺度3DGS模型生成跨视角一致的自适应结果作为内部知识。采用掩码引导的融合策略，将两类知识优势互补、协同优化，指导3D Gaussian参数学习高保真重建。

Result: 在合成和真实数据集上的大量实验表明，IE-SRGS在定量精度和视觉保真度方面均优于当前最先进3DGS超分方法。

Conclusion: IE-SRGS有效融合2D和3D信息，缓解了2DSR带来的视角歧义和域不一致问题，为低分辨率输入下的高质量3DGS重建提供了新范式，对实际应用具有重要意义。

Abstract: Reconstructing high-resolution (HR) 3D Gaussian Splatting (3DGS) models from low-resolution (LR) inputs remains challenging due to the lack of fine-grained textures and geometry. Existing methods typically rely on pre-trained 2D super-resolution (2DSR) models to enhance textures, but suffer from 3D Gaussian ambiguity arising from cross-view inconsistencies and domain gaps inherent in 2DSR models. We propose IE-SRGS, a novel 3DGS SR paradigm that addresses this issue by jointly leveraging the complementary strengths of external 2DSR priors and internal 3DGS features. Specifically, we use 2DSR and depth estimation models to generate HR images and depth maps as external knowledge, and employ multi-scale 3DGS models to produce cross-view consistent, domain-adaptive counterparts as internal knowledge. A mask-guided fusion strategy is introduced to integrate these two sources and synergistically exploit their complementary strengths, effectively guiding the 3D Gaussian optimization toward high-fidelity reconstruction. Extensive experiments on both synthetic and real-world benchmarks show that IE-SRGS consistently outperforms state-of-the-art methods in both quantitative accuracy and visual fidelity.

</details>


### [57] [SimScale: Learning to Drive via Real-World Simulation at Scale](https://arxiv.org/abs/2511.23369)
*Haochen Tian,Tianyu Li,Haochen Liu,Jiazhi Yang,Yihang Qiu,Guang Li,Junli Wang,Yinfeng Gao,Zhang Zhang,Liang Wang,Hangjun Ye,Tieniu Tan,Long Chen,Hongyang Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的可扩展模拟框架，能基于实际驾驶日志大规模合成未见过的驾驶状态，用于提升自动驾驶系统在复杂和罕见场景下的鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界收集数据无法覆盖自动驾驶涉及的全部场景，尤其是安全关键和异常场景，因此需要补充数据多样性以提升模型的表现与可靠性。

Method: 作者设计了一个结合高级神经渲染和响应式环境的模拟管线，通过扰动ego轨迹在已有日志上构造高保真多视角观测，并为新模拟状态开发伪专家轨迹生成机制，用于动作监督。提出了简单的协同训练（co-training）策略，将实际和合成数据共同训练自动驾驶决策模型。

Result: 在实际测试基准（如navhard、navtest）上，该方法仅靠增加模拟数据就能显著提升多种规划方法的鲁棒性和泛化能力，分别提升+6.8和+2.9 EPDMS，且这一提升随模拟数据量线性扩展。

Conclusion: 通过构建大规模高质量模拟数据，并引入伪专家监督及协同训练策略，极大促进了自动驾驶决策的鲁棒性和泛化能力。提出的SimScale系统为依赖真实数据提升模型表现提供了新思路，代码和数据将开源。

Abstract: Achieving fully autonomous driving systems requires learning rational decisions in a wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with a reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that a simple co-training strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such a sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released.

</details>


### [58] [Bridging 3D Deep Learning and Curation for Analysis and High-Quality Segmentation in Practice](https://arxiv.org/abs/2511.22236)
*Simon Püttmann,Jonathan Jair Sànchez Contreras,Lennart Kowitz,Peter Lampen,Saumya Gupta,Davide Panzeri,Nina Hagemann,Qiaojie Xiong,Dirk M. Hermann,Cao Chen,Jianxu Chen*

Main category: cs.CV

TL;DR: 提出了一款名为VessQC的开源工具，利用不确定性引导方法提升3D显微镜图像分割的人工校正效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 虽然先进的基础模型在3D显微镜图像分割任务中表现良好，但其错误率依然不可忽视，实际中仍需大量人工校正以确保分析的准确性。传统流程费时费力，亟需提升自动化和高效性。

Method: VessQC通过整合不确定性地图，将用户注意力引导至最可能出现生物学相关错误的区域，帮助用户更高效地发现和修正3D分割错误。

Result: 在初步用户实验中，基于不确定性引导的校正方法将错误检测的召回率从67%提升至94.0%（p=0.007），且总校正时间无显著增加。

Conclusion: VessQC高效支持人工参与的三维分割结果优化，促进了不确定性估计与实际人机互动的结合，可助力更广泛的生物图像分析应用。

Abstract: Accurate 3D microscopy image segmentation is critical for quantitative bioimage analysis but even state-of-the-art foundation models yield error-prone results. Therefore, manual curation is still widely used for either preparing high-quality training data or fixing errors before analysis. We present VessQC, an open-source tool for uncertainty-guided curation of large 3D microscopy segmentations. By integrating uncertainty maps, VessQC directs user attention to regions most likely containing biologically meaningful errors. In a preliminary user study uncertainty-guided correction significantly improved error detection recall from 67% to 94.0% (p=0.007) without a significant increase in total curation time. VessQC thus enables efficient, human-in-the-loop refinement of volumetric segmentations and bridges a key gap in real-world applications between uncertainty estimation and practical human-computer interaction. The software is freely available at github.com/MMV-Lab/VessQC.

</details>


### [59] [Creating Blank Canvas Against AI-enabled Image Forgery](https://arxiv.org/abs/2511.22237)
*Qi Song,Ziyuan Luo,Renjie Wan*

Main category: cs.CV

TL;DR: 本文提出了一种利用Segment Anything Model (SAM)进行图像篡改检测的新方法，通过对图像添加对抗扰动，使SAM对原图像“失明”，以达到篡改区域检测的目的。通过频域优化策略提升了对抗攻击的效果，从而提升篡改定位能力。


<details>
  <summary>Details</summary>
Motivation: AIGC图像编辑技术极大地简化了真实级别的图像修改，导致了图像伪造的风险。现有检测方法在复杂场景和新型伪造上效果有限，因此亟需创新性的篡改检测方案。

Method: 本方法不对SAM进行篡改区域训练，而是通过引入对抗扰动让SAM“看不见”原始内容。若图像被篡改，SAM即可识别伪造区域。为避免SAM强大的感知能力抵消攻击效果，作者提出了结合频域优化的对抗扰动方法，增强“致盲”能力。

Result: 实验表明，该方法能够有效检测AIGC生成的复杂图像篡改，定位精准度较现有方法更优。

Conclusion: 频域优化对抗扰动可有效“致盲”SAM，从而强化图像篡改检测，具有实际应用前景。

Abstract: AIGC-based image editing technology has greatly simplified the realistic-level image modification, causing serious potential risks of image forgery. This paper introduces a new approach to tampering detection using the Segment Anything Model (SAM). Instead of training SAM to identify tampered areas, we propose a novel strategy. The entire image is transformed into a blank canvas from the perspective of neural models. Any modifications to this blank canvas would be noticeable to the models. To achieve this idea, we introduce adversarial perturbations to prevent SAM from ``seeing anything'', allowing it to identify forged regions when the image is tampered with. Due to SAM's powerful perceiving capabilities, naive adversarial attacks cannot completely tame SAM. To thoroughly deceive SAM and make it blind to the image, we introduce a frequency-aware optimization strategy, which further enhances the capability of tamper localization. Extensive experimental results demonstrate the effectiveness of our method.

</details>


### [60] [TTSnap: Test-Time Scaling of Diffusion Models via Noise-Aware Pruning](https://arxiv.org/abs/2511.22242)
*Qingtao Yu,Changlin Song,Minghao Sun,Zhengyang Yu,Vinay Kumar Verma,Soumya Roy,Sumit Negi,Hongdong Li,Dylan Campbell*

Main category: cs.CV

TL;DR: 本文提出了一种高效的基于噪声感知剪枝的新型测试时尺度化方法（TTSnap），用于文本到图像扩散模型，大幅提升了在有限计算预算下的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有测试时尺度化方法需对多个噪声种子生成完整图片再筛选最优结果，计算开销大，样本探索数少，影响扩散模型质量。需有更高效的候选筛选机制。

Method: 提出TTSnap框架，通过在噪声阶段利用噪声感知奖励模型剪枝，避免对所有候选全部去噪。为解决奖励模型在噪声-纯净域一致性差问题，采用自蒸馏训练噪声感知奖励模型，并结合课程训练策略；同时提出奖励一致性与计算利用新指标。

Result: 与现有方法相比，TTSnap在性能提升超过16%，能更有效利用计算预算。此外，与后训练技术和局部测试时优化结合还能带来额外收益。

Conclusion: TTSnap显著提升了测试时尺度化的效率与效果，在实际生成任务中更具实用性，为扩散模型采样效率带来改进。

Abstract: A prominent approach to test-time scaling for text-to-image diffusion models formulates the problem as a search over multiple noise seeds, selecting the one that maximizes a certain image-reward function. The effectiveness of this strategy heavily depends on the number and diversity of noise seeds explored. However, verifying each candidate is computationally expensive, because each must be fully denoised before a reward can be computed. This severely limits the number of samples that can be explored under a fixed budget. We propose test-time scaling with noise-aware pruning (TTSnap), a framework that prunes low-quality candidates without fully denoising them. The key challenge is that reward models are learned in the clean image domain, and the ranking of rewards predicted for intermediate estimates are often inconsistent with those predicted for clean images. To overcome this, we train noise-aware reward models via self-distillation to align the reward for intermediate estimates with that of the final clean images. To stabilize learning across different noise levels, we adopt a curriculum training strategy that progressively shifts the data domain from clean images to noise images. In addition, we introduce a new metric that measures reward alignment and computational budget utilization. Experiments demonstrate that our approach improves performance by over 16\% compared with existing methods, enabling more efficient and effective test-time scaling. It also provides orthogonal gains when combined with post-training techniques and local test-time optimization. Code: https://github.com/TerrysLearning/TTSnap/.

</details>


### [61] [Semantic Anchoring for Robust Personalization in Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.22245)
*Seoyun Yang,Gihoon Kim,Taesup Kim*

Main category: cs.CV

TL;DR: 提出一种基于语义锚定的个性化方法，改善了文生图扩散模型在少量参考图像下的个性化效果。该方法在保持语义一致性的同时，提高了对用户特定主题的还原能力，并取得了比现有方法更稳定和优越的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型虽然图像生成能力强，但在个性化（仅凭少量图片快速适应用户特定目标）任务上表现较差。难点在于如何既保留预训练模型的语义一致性，又能快速反映新概念，克服过拟合或泛化不足的问题。

Method: 提出通过语义锚定进行个性化，将罕见个性化概念的学习过程，通过与频繁出现的概念进行语义绑定（锚定）来引导，使模型能够在扩展到个性化属性时仍保持整体语义结构的稳定。

Result: 相比于以往的方法，该方法在主体还原度（subject fidelity）和文本图像一致性（text-image alignment）方面均有稳定改进。大量实验和消融分析验证了该策略的鲁棒性和有效性。

Conclusion: 语义锚定为文生图扩散模型的个性化提供了一种平衡新概念学习与预训练语义结构保持的有效策略，有助于实现从少量参考图像中高效稳定地适应个性化目标。

Abstract: Text-to-image diffusion models have achieved remarkable progress in generating diverse and realistic images from textual descriptions. However, they still struggle with personalization, which requires adapting a pretrained model to depict user-specific subjects from only a few reference images. The key challenge lies in learning a new visual concept from a limited number of reference images while preserving the pretrained semantic prior that maintains text-image alignment. When the model focuses on subject fidelity, it tends to overfit the limited reference images and fails to leverage the pretrained distribution. Conversely, emphasizing prior preservation maintains semantic consistency but prevents the model from learning new personalized attributes. Building on these observations, we propose the personalization process through a semantic anchoring that guides adaptation by grounding new concepts in their corresponding distributions. We therefore reformulate personalization as the process of learning a rare concept guided by its frequent counterpart through semantic anchoring. This anchoring encourages the model to adapt new concepts in a stable and controlled manner, expanding the pretrained distribution toward personalized regions while preserving its semantic structure. As a result, the proposed method achieves stable adaptation and consistent improvements in both subject fidelity and text-image alignment compared to baseline methods. Extensive experiments and ablation studies further demonstrate the robustness and effectiveness of the proposed anchoring strategy.

</details>


### [62] [Toward Diffusible High-Dimensional Latent Spaces: A Frequency Perspective](https://arxiv.org/abs/2511.22249)
*Bolin Lai,Xudong Wang,Saketh Rambhatla,James M. Rehg,Zsolt Kira,Rohit Girdhar,Ishan Misra*

Main category: cs.CV

TL;DR: 本文揭示了视觉生成中的重建和生成质量的权衡与高频信息建模能力有关，并提出了一种可以改善高维自编码生成质量的简单方法。


<details>
  <summary>Details</summary>
Motivation: 在视觉生成领域，潜在扩散模型已经成为主流，但提升自编码器容量后，虽然重建更好，生成结果反而变差。作者发现这是因为高频信息在编码与解码过程中的失衡。为提升高频信息在生成中的作用，研究这个问题，以改进模型的整体生成质量。

Method: 通过对比潜在空间中不同频率的扰动，并系统分析编码器和解码器对高、低频信息的反应机制。针对高频学习不足问题，提出FreqWarm方法：在扩散或流训练初期，动态增加对高频潜在分量的暴露，且无需修改或重训练自编码器。方法为架构无关的课程式训练策略。

Result: FreqWarm在多个高维自编码器上均提升了生成质量；在Wan2.2-VAE上gFID减少14.11，在LTX-VAE和DC-AE-f32也有明显改进，且该方法不依赖于具体网络结构，实现简便。

Conclusion: 通过显式控制高频暴露，FreqWarm能充分释放高维潜在空间的生成能力，有效改善了高容量自编码器在视觉生成中的权衡瓶颈。

Abstract: Latent diffusion has become the default paradigm for visual generation, yet we observe a persistent reconstruction-generation trade-off as latent dimensionality increases: higher-capacity autoencoders improve reconstruction fidelity but generation quality eventually declines. We trace this gap to the different behaviors in high-frequency encoding and decoding. Through controlled perturbations in both RGB and latent domains, we analyze encoder/decoder behaviors and find that decoders depend strongly on high-frequency latent components to recover details, whereas encoders under-represent high-frequency contents, yielding insufficient exposure and underfitting in high-frequency bands for diffusion model training. To address this issue, we introduce FreqWarm, a plug-and-play frequency warm-up curriculum that increases early-stage exposure to high-frequency latent signals during diffusion or flow-matching training -- without modifying or retraining the autoencoder. Applied across several high-dimensional autoencoders, FreqWarm consistently improves generation quality: decreasing gFID by 14.11 on Wan2.2-VAE, 6.13 on LTX-VAE, and 4.42 on DC-AE-f32, while remaining architecture-agnostic and compatible with diverse backbones. Our study shows that explicitly managing frequency exposure can successfully turn high-dimensional latent spaces into more diffusible targets.

</details>


### [63] [UMind-VL: A Generalist Ultrasound Vision-Language Model for Unified Grounded Perception and Comprehensive Interpretation](https://arxiv.org/abs/2511.22256)
*Dengbo Chen,Ziwei Zhao,Kexin Zhang,Shishuang Zhao,Junjie Hou,Yaqian Wang,Nianxi Liao,Anlan Sun,Fei Gao,Jia Ding,Yuhang Liu,Dong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种统一的超声基础模型UMind-VL，能整合像素级理解与复杂临床推理，解决传统模型在超声分割、定位及诊断推理间存在的脱节问题。


<details>
  <summary>Details</summary>
Motivation: 目前医学基础模型虽取得进展，但在超声领域，低层次的感知任务（如分割、定位）和高层次的解释任务（如诊断、推理）往往割裂，缺乏统一解决方案，因此需要新的模型来桥接这两类任务。

Method: 作者提出UMind-VL模型，并构建了大规模多模态数据集UMind-DS，包含120万对超声图像-文本样本、像素级标注及临床验证推理。模型结构上引入了轻量级动态卷积掩膜解码器，通过LLM输出动态生成掩膜，并结合任务特定的token，实现分割、检测、测量、诊断等任务统一处理。

Result: 在大量基准测试中，UMind-VL明显优于现有通用多模态模型，并在分割、检测、关键点定位和诊断推理等多个任务上达到或超过最先进专业模型的表现，且具备较强泛化能力。

Conclusion: UMind-VL为超声领域感知与推理的一体化提供了有效解决办法，在多项任务上均表现优异，有望推动超声辅助诊断技术的发展。

Abstract: Despite significant strides in medical foundation models, the ultrasound domain lacks a comprehensive solution capable of bridging low-level Ultrasound Grounded Perception (e.g., segmentation, localization) and high-level Ultrasound Comprehensive Interpretation (e.g., diagnosis, reasoning). To bridge this gap, we propose UMind-VL, a unified foundation model designed to synergize pixel-level structural understanding with complex clinical reasoning. We first introduce UMind-DS, a large-scale multimodal dataset comprising 1.2 million ultrasound image-text pairs across 16 anatomical regions, enriching standard data with pixel-level annotations and clinician-validated rationales. Architecturally, UMind-VL incorporates a lightweight Dynamic Convolutional Mask Decoder that generates masks via dynamic kernels conditioned on LLM outputs. This design, combined with task-specific tokens, unifies segmentation, detection, geometric measurement, and diagnosis tasks within a single framework. Extensive evaluations demonstrate that UMind-VL significantly outperforms existing generalist multimodal models and achieves performance on par with, or superior to, state-of-the-art specialist models across segmentation, detection, keypoint localization, and diagnostic reasoning benchmarks, while maintaining strong generalization ability. We demonstrate the capability of UMind-VL in Figure 1.

</details>


### [64] [Can Protective Watermarking Safeguard the Copyright of 3D Gaussian Splatting?](https://arxiv.org/abs/2511.22262)
*Wenkai Huang,Yijia Guo,Gaolei Li,Lei Ma,Hang Zhang,Liwen Hu,Jiazheng Wang,Jianhua Li,Tiejun Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为GSPure的新型3D Gaussian Splatting (3DGS)水印去除方法，有效去除水印同时保留场景质量，并优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 随着3DGS在高质量3D场景表达中的应用日益广泛，3DGS资产具备很高价值，因此需要创新型水印保护技术；但现有3D水印安全性尚未系统研究。本论文首次关注并分析了3DGS水印的潜在安全漏洞。

Method: 作者发现，针对2D图像设计的水印去除方法不适用于3DGS。为此提出GSPure，通过分析基于视角的渲染贡献和几何特征聚类来精确识别并移除与水印有关的高斯基元，同时最大限度地保留场景信息。

Result: GSPure能将水印PSNR降低16.34dB，同时对原场景的损失控制在1dB以内，且在去除效果和泛化能力上都优于现有方法。

Conclusion: GSPure作为首个专为3DGS设计的水印净化框架，有效突破了3DGS资产水印保护的安全性，为3D内容版权保护带来新的挑战。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful representation for 3D scenes, widely adopted due to its exceptional efficiency and high-fidelity visual quality. Given the significant value of 3DGS assets, recent works have introduced specialized watermarking schemes to ensure copyright protection and ownership verification. However, can existing 3D Gaussian watermarking approaches genuinely guarantee robust protection of the 3D assets? In this paper, for the first time, we systematically explore and validate possible vulnerabilities of 3DGS watermarking frameworks. We demonstrate that conventional watermark removal techniques designed for 2D images do not effectively generalize to the 3DGS scenario due to the specialized rendering pipeline and unique attributes of each gaussian primitives. Motivated by this insight, we propose GSPure, the first watermark purification framework specifically for 3DGS watermarking representations. By analyzing view-dependent rendering contributions and exploiting geometrically accurate feature clustering, GSPure precisely isolates and effectively removes watermark-related Gaussian primitives while preserving scene integrity. Extensive experiments demonstrate that our GSPure achieves the best watermark purification performance, reducing watermark PSNR by up to 16.34dB while minimizing degradation to original scene fidelity with less than 1dB PSNR loss. Moreover, it consistently outperforms existing methods in both effectiveness and generalization.

</details>


### [65] [DriveVGGT: Visual Geometry Transformer for Autonomous Driving](https://arxiv.org/abs/2511.22264)
*Xiaosong Jia,Yanhao Liu,Junqi You,Renqiu Xia,Yu Hong,Junchi Yan*

Main category: cs.CV

TL;DR: 本文提出了DriveVGGT——一种专为自动驾驶数据设计的、具备尺度感知能力的4D重建框架，通过集成针对自动驾驶独特先验的设计，有效提升了多摄像头视频的重建表现，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统VGGT等前馈重建方法无法充分利用自动驾驶场景下的特殊先验条件（如镜头覆盖率低、相机参数已知、相机相对位置固定），导致效果不佳。针对这些问题，作者希望开发一种能更好适应自动驾驶数据特点的新架构。

Method: 提出DriveVGGT框架，针对自动驾驶多摄像头系统：（1）设计Temporal Video Attention模块，独立处理各摄像头视频，挖掘单摄像头时空连续性；（2）提出Multi-camera Consistency Attention模块，以标准化相对位姿嵌入实现多摄像头一致性约束；（3）在原有VGGT的输出头基础上增加绝对尺度和自车位姿输出。

Result: 在自动驾驶数据集上的实验显示，DriveVGGT在重建任务中显著优于VGGT、StreamVGGT和fastVGGT。消融实验进一步验证了各个创新模块的有效性。

Conclusion: DriveVGGT针对自动驾驶场景充分融合了行业先验，改善了现有前馈式重建方法在该领域的不足。该方法为多摄像头、已知位姿、低重叠度场景下的高效重建提供了新思路和技术基础。

Abstract: Feed-forward reconstruction has recently gained significant attention, with VGGT being a notable example. However, directly applying VGGT to autonomous driving (AD) systems leads to sub-optimal results due to the different priors between the two tasks. In AD systems, several important new priors need to be considered: (i) The overlap between camera views is minimal, as autonomous driving sensor setups are designed to achieve coverage at a low cost. (ii) The camera intrinsics and extrinsics are known, which introduces more constraints on the output and also enables the estimation of absolute scale. (iii) Relative positions of all cameras remain fixed though the ego vehicle is in motion. To fully integrate these priors into a feed-forward framework, we propose DriveVGGT, a scale-aware 4D reconstruction framework specifically designed for autonomous driving data. Specifically, we propose a Temporal Video Attention (TVA) module to process multi-camera videos independently, which better leverages the spatiotemporal continuity within each single-camera sequence. Then, we propose a Multi-camera Consistency Attention (MCA) module to conduct window attention with normalized relative pose embeddings, aiming to establish consistency relationships across different cameras while restricting each token to attend only to nearby frames. Finally, we extend the standard VGGT heads by adding an absolute scale head and an ego vehicle pose head. Experiments show that DriveVGGT outperforms VGGT, StreamVGGT, fastVGGT on autonomous driving dataset while extensive ablation studies verify effectiveness of the proposed designs.

</details>


### [66] [The Collapse of Patches](https://arxiv.org/abs/2511.22281)
*Wei Guo,Shunqi Mao,Zhuonan Liang,Heng Wang,Weidong Cai*

Main category: cs.CV

TL;DR: 本文提出了图像补丁（patch）之间的信息依赖关系，提出'patch collapse'新概念，用自动编码器发现补丁依赖顺序，提升图像生成与识别任务的效率。


<details>
  <summary>Details</summary>
Motivation: 在图像处理任务中，不同区域的信息具有相关性，合理利用这些相关性有助于提升自监督建模和下游任务性能，但怎样有效建模与利用补丁之间的依赖关系仍是难点。

Method: 作者设计了一个自动编码器模型，通过软性选择补丁子集来重建目标补丁，从而学习各补丁的依赖性。利用PageRank方法为每个补丁打分，得到最佳显现顺序，再将此顺序用于自回归生成与分类等任务。

Result: 1. 对最先进自回归生成模型MAR在补丁顺序上重新训练后效果更佳；2. 提出一种新型分类设定，仅用高PageRank分数的前22%补丁输入Vision Transformer，依然能保持较高识别准确率。

Conclusion: 作者提出的patch collapse视角能高效建模图像信息依赖，提升生成与识别任务效率，为视觉模型研究提供了新思路。

Abstract: Observing certain patches in an image reduces the uncertainty of others. Their realization lowers the distribution entropy of each remaining patch feature, analogous to collapsing a particle's wave function in quantum mechanics. This phenomenon can intuitively be called patch collapse. To identify which patches are most relied on during a target region's collapse, we learn an autoencoder that softly selects a subset of patches to reconstruct each target patch. Graphing these learned dependencies for each patch's PageRank score reveals the optimal patch order to realize an image. We show that respecting this order benefits various masked image modeling methods. First, autoregressive image generation can be boosted by retraining the state-of-the-art model MAR. Next, we introduce a new setup for image classification by exposing Vision Transformers only to high-rank patches in the collapse order. Seeing 22\% of such patches is sufficient to achieve high accuracy. With these experiments, we propose patch collapse as a novel image modeling perspective that promotes vision efficiency. Our project is available at https://github.com/wguo-ai/CoP .

</details>


### [67] [Match-and-Fuse: Consistent Generation from Unstructured Image Sets](https://arxiv.org/abs/2511.22287)
*Kate Feingold,Omri Kaduri,Tali Dekel*

Main category: cs.CV

TL;DR: 本文提出了一种名为Match-and-Fuse的零样本、无需训练的方法，实现对非结构化图像集的一致性可控生成，即在不同视角、时间和环境下，共享视觉元素的图像集合中进行内容一致性变换。


<details>
  <summary>Details</summary>
Motivation: 目前方法主要侧重于单张图像或者密集采样的视频，对于多个包含共享元素但存在差异的图像集之间的一致性生成能力有限。该方法旨在填补对无结构图像集进行跨集一致性生成的空白。

Method: 将图像集的生成任务建模为图结构，每个节点为一张图像，每条边对应成对生成操作。通过密集的输入对应关系融合每对图像的内部特征，在无需掩码或人工标注的情况下，实现局部一致与全局连贯。同时利用文本到图像模型中的先验，促进多视角的一致输出。

Result: Match-and-Fuse在跨图像集的一致性和视觉质量方面达到了当前最优效果，并显著扩展了从图像集合进行内容创作的能力。

Conclusion: 该方法不仅提升了多图像集一致性变换的效果，还为无标签、无需训练的跨视角图像集生成开辟了新方向，具有较大的实际应用前景。

Abstract: We present Match-and-Fuse - a zero-shot, training-free method for consistent controlled generation of unstructured image sets - collections that share a common visual element, yet differ in viewpoint, time of capture, and surrounding content. Unlike existing methods that operate on individual images or densely sampled videos, our framework performs set-to-set generation: given a source set and user prompts, it produces a new set that preserves cross-image consistency of shared content. Our key idea is to model the task as a graph, where each node corresponds to an image and each edge triggers a joint generation of image pairs. This formulation consolidates all pairwise generations into a unified framework, enforcing their local consistency while ensuring global coherence across the entire set. This is achieved by fusing internal features across image pairs, guided by dense input correspondences, without requiring masks or manual supervision. It also allows us to leverage an emergent prior in text-to-image models that encourages coherent generation when multiple views share a single canvas. Match-and-Fuse achieves state-of-the-art consistency and visual quality, and unlocks new capabilities for content creation from image collections.

</details>


### [68] [Structure is Supervision: Multiview Masked Autoencoders for Radiology](https://arxiv.org/abs/2511.22294)
*Sonia Laguna,Andrea Agostini,Alain Ryser,Samuel Ruiperez-Campillo,Irene Cannistraci,Moritz Vandenhirtz,Stephan Mandt,Nicolas Deperrois,Farhad Nooralahzadeh,Michael Krauthammer,Thomas M. Sutter,Julia E. Vogt*

Main category: cs.CV

TL;DR: 本论文提出了MVMAE和MVMAE-V2T两种自监督学习方法，利用医学影像的多视图和文本结构信息，显著提升了医学影像分类的效果，特别是在标签稀缺时表现更优。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据天然具有多视图结构和高度冗余性，同时许多场景下标注数据稀缺，现有医学机器学习系统在利用这些结构和无标签数据方面的能力有限。

Method: 作者提出了一种多视图掩码自编码器（MVMAE），结合图像重建和视图对齐，通过多视图信息自监督学习。此外，进一步提出MVMAE-V2T，将放射学报告（文本）作为辅助监督，提升语义表达能力，且推理阶段仅需视觉信息。

Result: 在MIMIC-CXR、CheXpert和PadChest三大公开数据集的疾病分类任务中，MVMAE和MVMAE-V2T均优于传统的有监督和视觉-语言基线，特别是在标签稀缺的情况下，MVMAE-V2T表现更佳。

Conclusion: 结构化的多视图和文本辅助自监督学习是构建可扩展、临床可用医学模型的重要途径，对医学人工智能基础模型的发展具有指导意义。

Abstract: Building robust medical machine learning systems requires pretraining strategies that exploit the intrinsic structure present in clinical data. We introduce Multiview Masked Autoencoder (MVMAE), a self-supervised framework that leverages the natural multi-view organization of radiology studies to learn view-invariant and disease-relevant representations. MVMAE combines masked image reconstruction with cross-view alignment, transforming clinical redundancy across projections into a powerful self-supervisory signal. We further extend this approach with MVMAE-V2T, which incorporates radiology reports as an auxiliary text-based learning signal to enhance semantic grounding while preserving fully vision-based inference. Evaluated on a downstream disease classification task on three large-scale public datasets, MIMIC-CXR, CheXpert, and PadChest, MVMAE consistently outperforms supervised and vision-language baselines. Furthermore, MVMAE-V2T provides additional gains, particularly in low-label regimes where structured textual supervision is most beneficial. Together, these results establish the importance of structural and textual supervision as complementary paths toward scalable, clinically grounded medical foundation models.

</details>


### [69] [Small Object Detection for Birds with Swin Transformer](https://arxiv.org/abs/2511.22310)
*Da Huo,Marc A. Kastner,Tingwei Liu,Yasutomo Kawanishi,Takatsugu Hirayama,Takahiro Komamizu,Ichiro Ide*

Main category: cs.CV

TL;DR: 本论文提出了一种针对小型稀疏目标（如鸟类）检测的新方法，通过改进网络结构中的“neck”部分，并采用Swin Transformer调整窗口大小，有效提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 小目标检测因目标小、模糊、遮挡等问题非常困难。现有方法多针对小且密集的场景（如人群），但对于稀疏的小目标（如鸟类），由于训练样本少，特征学习效果差。因此需要专门的方法提升这一类别的小目标检测效果。

Method: 作者改进了目标检测网络的neck结构（骨干与预测头之间的子网络），采用分层结构增强特征提取能力，引入Swin Transformer对图像特征进行上采样，并通过调整transformer的shifted window大小以适应小目标。该方法与CenterNet结合进行实验证明其有效性。

Result: 实验表明，新方法在检测小目标时，尤其是通过缩小Swin Transformer窗口尺寸，可以显著提升mAP（平均准确率）性能。

Conclusion: 通过改进neck结构，并结合Swin Transformer及窗口尺寸调整，能够有效提升稀疏小目标（如鸟类）的检测效果，为稀疏小目标检测任务提供了新的思路。

Abstract: Object detection is the task of detecting objects in an image. In this task, the detection of small objects is particularly difficult. Other than the small size, it is also accompanied by difficulties due to blur, occlusion, and so on. Current small object detection methods are tailored to small and dense situations, such as pedestrians in a crowd or far objects in remote sensing scenarios. However, when the target object is small and sparse, there is a lack of objects available for training, making it more difficult to learn effective features. In this paper, we propose a specialized method for detecting a specific category of small objects; birds. Particularly, we improve the features learned by the neck; the sub-network between the backbone and the prediction head, to learn more effective features with a hierarchical design. We employ Swin Transformer to upsample the image features. Moreover, we change the shifted window size for adapting to small objects. Experiments show that the proposed Swin Transformer-based neck combined with CenterNet can lead to good performance by changing the window sizes. We further find that smaller window sizes (default 2) benefit mAPs for small object detection.

</details>


### [70] [Prompt-based Consistent Video Colorization](https://arxiv.org/abs/2511.22330)
*Silvia Dani,Tiberio Uricchio,Lorenzo Seidenari*

Main category: cs.CV

TL;DR: 本论文提出了一种新的视频上色方法，能够自动生成高保真、时序稳定且具有丰富语义信息的彩色视频，无需大量人工干预，在主流数据集上效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频上色方法存在时序闪烁问题，或者依赖大量人工指定色彩，影响效率与效果。因此，亟需一种能够自动且稳定地为视频上色的方法。

Method: 作者提出结合语言引导和分割信息的视频自动上色方案。具体做法包括使用语言条件扩散模型对灰度帧进行上色，利用自动生成的对象掩膜和文本提示作为引导。为保证时序一致性，通过光流算法（RAFT）将前一帧色彩信息迁移到当前帧，并设置一个修正检测步骤来消除迁移中引入的不一致。大多数情况下自动化提示即可达到优异效果，无需具体色彩指定。

Result: 在DAVIS30和VIDEVO20等基准数据集上的评测结果显示，该方法在色彩化准确率（PSNR）和视觉真实感（Colorfulness, CDC）等多项指标上取得了行业领先的表现。

Conclusion: 结合自动语义提示和时序稳定机制的扩散上色模型，能够有效提升视频自动上色的质量与一致性，为高效、低人工干预的视频上色提供了新思路。

Abstract: Existing video colorization methods struggle with temporal flickering or demand extensive manual input. We propose a novel approach automating high-fidelity video colorization using rich semantic guidance derived from language and segmentation. We employ a language-conditioned diffusion model to colorize grayscale frames. Guidance is provided via automatically generated object masks and textual prompts; our primary automatic method uses a generic prompt, achieving state-of-the-art results without specific color input. Temporal stability is achieved by warping color information from previous frames using optical flow (RAFT); a correction step detects and fixes inconsistencies introduced by warping. Evaluations on standard benchmarks (DAVIS30, VIDEVO20) show our method achieves state-of-the-art performance in colorization accuracy (PSNR) and visual realism (Colorfulness, CDC), demonstrating the efficacy of automated prompt-based guidance for consistent video colorization.

</details>


### [71] [Unexplored flaws in multiple-choice VQA evaluations](https://arxiv.org/abs/2511.22341)
*Fabio Rosenthal,Sebastian Schmidt,Thorsten Graf,Thorsten Bagodonat,Stephan Günnemann,Leo Schwinn*

Main category: cs.CV

TL;DR: 本文发现当前多模态大模型（MLLMs）在多项选择视觉问答（VQA）任务中的评测结果对提示词格式极其敏感，现有的偏置缓解方法无法解决新发现的格式偏置问题。


<details>
  <summary>Details</summary>
Motivation: 尽管已经有工作指出多选题的答案顺序会影响MLLM评测结果，但还存在被忽视的提示词格式偏置，亟需深入研究以确保评测方法的可靠性和公平性。

Method: 作者系统性地分析了三种关键提示词格式变因，通过对七个MLLM和五个VQA数据集进行大规模测试，涵盖了48种不同提示词格式变化，定量评估格式变化对模型表现的影响。

Result: 实验显示：即使只是语义中性的提示词格式微小变化也会显著影响多模态大模型的VQA表现；此外，这些新发现的格式偏置与已知的答案顺序偏置和模型置信度无关，并且现有偏置缓解方法对这些问题无效。

Conclusion: 当前MLLM多项选择VQA的评测方法存在未被充分认识的提示词格式偏置。评测社区应重新审视和改善现有VQA测试流程，以获得更可靠的模型能力评估。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate strong capabilities in handling image-text inputs. A common way to assess this ability is through multiple-choice Visual Question Answering (VQA). Earlier works have already revealed that these benchmarks are sensitive to answer choice order, a limitation that can be mitigated through careful design. Yet, we highlight additional, unexplored biases in prompt formatting that question the reliability of current MLLM evaluations. Specifically, we identify three key variation factors in prompt formatting and analyze their impact through a large-scale study involving $\mathbf{\text{seven}}$ MLLMs and $\mathbf{\text{five}}$ VQA datasets, spanning $\mathbf{48}$ distinct $\mathbf{\text{prompt format variations}}$. Our findings reveal that multiple-choice VQA is highly sensitive to minor prompt format changes, even when these changes are semantically neutral. We further demonstrate that these biases persist independently of known order biases or the MLLM's confidence in the correct answer. Finally, we demonstrate that existing bias mitigation strategies fail to address these newly identified biases.

</details>


### [72] [Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment](https://arxiv.org/abs/2511.22345)
*Yang Chen,Xiaowei Xu,Shuai Wang,Chenhui Zhu,Ruxue Wen,Xubin Li,Tiezheng Ge,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种利用归一化流( Normalizing Flows, NFs)可逆性的创新对齐方法，通过在生成(反向)过程中对齐中间特征与强大的视觉基础模型表征，显著提升了NFs的生成质量和分类性能，并加快训练速度，实现了ImageNet上新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 标准NFs通过对数似然优化学习表征，但得到的语义表征较差，限制了生成质量。作者旨在提升NFs的表征和生成能力，弥补其在语义建模上的不足。

Method: 提出创新的特征对齐策略：在NFs生成(反向)过程中，将中间特征与预先训练的视觉基础模型表征对齐，而非简单地正则化前向过程。同时，提出一种免训练的测试时优化算法，直接评估NFs内部的语义知识，用于分类任务。

Result: 实验表明，该方法可加速NFs训练3.3倍以上，并在生成质量和分类准确率上均取得显著提升。在ImageNet 64×64和256×256上的性能均达到了NFs的新SOTA水平。

Conclusion: 通过创新的反向特征对齐和无训练测试时优化方法，显著提升了NFs的生成质量和语义理解能力，为NFs的发展和实际应用带来了重要进步。

Abstract: Normalizing Flows (NFs) are a class of generative models distinguished by a mathematically invertible architecture, where the forward pass transforms data into a latent space for density estimation, and the reverse pass generates new samples from this space. This characteristic creates an intrinsic synergy between representation learning and data generation. However, the generative quality of standard NFs is limited by poor semantic representations from log-likelihood optimization. To remedy this, we propose a novel alignment strategy that creatively leverages the invertibility of NFs: instead of regularizing the forward pass, we align the intermediate features of the generative (reverse) pass with representations from a powerful vision foundation model, demonstrating superior effectiveness over naive alignment. We also introduce a novel training-free, test-time optimization algorithm for classification, which provides a more intrinsic evaluation of the NF's embedded semantic knowledge. Comprehensive experiments demonstrate that our approach accelerates the training of NFs by over 3.3$\times$, while simultaneously delivering significant improvements in both generative quality and classification accuracy. New state-of-the-art results for NFs are established on ImageNet 64$\times$64 and 256$\times$256. Our code is available at https://github.com/MCG-NJU/FlowBack.

</details>


### [73] [INSIGHT: An Interpretable Neural Vision-Language Framework for Reasoning of Generative Artifacts](https://arxiv.org/abs/2511.22351)
*Anshul Bagaria*

Main category: cs.CV

TL;DR: AI 生成图片的真实性令人担忧，而现有检测方法在真实环境严重退化。该文提出INSIGHT框架，可在极低分辨率下鲁棒检测AI生成图像并提供可解释性解释，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 近年来GAN和扩散模型生成的图像高度逼真，导致对视觉媒体可靠性的担忧加剧。现有深度伪造检测方法在压缩、低分辨率和领域偏移下性能大幅下降，且大多为不可解释的黑盒模型，难以在高风险场景中获得信任。

Method: 作者提出INSIGHT框架：1）使用分层超分辨放大细微取证线索，避免引入伪影；2）用Grad-CAM驱动的多尺度定位，揭示生成模式的空间区域；3）结合CLIP语义对齐，将视觉异常映射为可解释的描述。最终通过结构化ReAct+CoT指导视觉-语言模型生成细致解释，再利用G-Eval+LLM双阶段验证减轻幻觉、确保真实性。

Result: 在动物、交通工具和合成场景等多领域、极端退化（低分辨/压缩）下，INSIGHT在检测稳健性和解释质量上明显优于以往检测器和黑盒视觉语言模型基线。

Conclusion: INSIGHT构建了一个可解释且可靠的AI生成图像取证新路径，促进了可信的多模态内容核查。该方法在实用性和可信度方面为AI图像取证技术的发展迈出了重要一步。

Abstract: The growing realism of AI-generated images produced by recent GAN and diffusion models has intensified concerns over the reliability of visual media. Yet, despite notable progress in deepfake detection, current forensic systems degrade sharply under real-world conditions such as severe downsampling, compression, and cross-domain distribution shifts. Moreover, most detectors operate as opaque classifiers, offering little insight into why an image is flagged as synthetic, undermining trust and hindering adoption in high-stakes settings.
  We introduce INSIGHT (Interpretable Neural Semantic and Image-based Generative-forensic Hallucination Tracing), a unified multimodal framework for robust detection and transparent explanation of AI-generated images, even at extremely low resolutions (16x16 - 64x64). INSIGHT combines hierarchical super-resolution for amplifying subtle forensic cues without inducing misleading artifacts, Grad-CAM driven multi-scale localization to reveal spatial regions indicative of generative patterns, and CLIP-guided semantic alignment to map visual anomalies to human-interpretable descriptors. A vision-language model is then prompted using a structured ReAct + Chain-of-Thought protocol to produce consistent, fine-grained explanations, verified through a dual-stage G-Eval + LLM-as-a-judge pipeline to minimize hallucinations and ensure factuality.
  Across diverse domains, including animals, vehicles, and abstract synthetic scenes, INSIGHT substantially improves both detection robustness and explanation quality under extreme degradation, outperforming prior detectors and black-box VLM baselines. Our results highlight a practical path toward transparent, reliable AI-generated image forensics and establish INSIGHT as a step forward in trustworthy multimodal content verification.

</details>


### [74] [AnchorFlow: Training-Free 3D Editing via Latent Anchor-Aligned Flows](https://arxiv.org/abs/2511.22357)
*Zhenglin Zhou,Fan Ma,Chengzhuo Gui,Xiaobo Xia,Hehe Fan,Yi Yang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: AnchorFlow提出了一种无需训练的3D编辑方法，通过全局一致的锚点和创新的损失函数，实现了稳健、语义一致且无需掩码监督的3D编辑。


<details>
  <summary>Details</summary>
Motivation: 现有训练自由3D编辑方法受到扩散过程中的噪声影响，导致编辑效果弱或几何不稳定，影响3D内容创作的质量和实用性。

Method: AnchorFlow引入了共享于源/目标编辑轨迹的全球潜在锚点，通过宽松的锚点对齐损失和锚点对齐的参数更新，确保编辑过程中变换的稳定性和语义准确性。此外，方法无需掩码监督，保持了较高的几何保真度。

Result: 在Eval3DEdit基准上的实验表明，AnchorFlow能在多种编辑类型下始终表现出语义对齐好、结构稳定性强的优点，超过现有方法。

Conclusion: AnchorFlow克服了以往方法在语义一致性和结构稳定性上的不足，实现了训练自由、掩码自由且高质量的3D编辑，为3D内容创作提供了实用工具。

Abstract: Training-free 3D editing aims to modify 3D shapes based on human instructions without model finetuning. It plays a crucial role in 3D content creation. However, existing approaches often struggle to produce strong or geometrically stable edits, largely due to inconsistent latent anchors introduced by timestep-dependent noise during diffusion sampling. To address these limitations, we introduce AnchorFlow, which is built upon the principle of latent anchor consistency. Specifically, AnchorFlow establishes a global latent anchor shared between the source and target trajectories, and enforces coherence using a relaxed anchor-alignment loss together with an anchor-aligned update rule. This design ensures that transformations remain stable and semantically faithful throughout the editing process. By stabilizing the latent reference space, AnchorFlow enables more pronounced semantic modifications. Moreover, AnchorFlow is mask-free. Without mask supervision, it effectively preserves geometric fidelity. Experiments on the Eval3DEdit benchmark show that AnchorFlow consistently delivers semantically aligned and structurally robust edits across diverse editing types. Code is at https://github.com/ZhenglinZhou/AnchorFlow.

</details>


### [75] [Asking like Socrates: Socrates helps VLMs understand remote sensing images](https://arxiv.org/abs/2511.22396)
*Run Shao,Ziyu Li,Zhaoyang Zhang,Linrui Xu,Xinran He,Hongyuan Yuan,Bolei He,Yongxing Dai,Yiming Yan,Yijun Chen,Wang Guo,Haifeng Li*

Main category: cs.CV

TL;DR: 现有多模态推理模型在遥感任务中存在伪推理现象，本论文提出新的推理范式RS-EoT及训练策略，显著改善了模型的视觉证据依赖和推理能力，在多个基准上达到了最佳表现。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态推理模型推动了视觉-语言系统的发展，但在遥感图像任务中，这些模型往往口头复述推理流程而非真正基于视觉证据进行推理，导致结果不可靠。分析认为“Glance Effect”——即模型对大尺度遥感图像的粗糙把握而非细致理解，是这一问题的根因。

Method: 1. 提出RS-EoT（遥感证据推理）范式，通过语言驱动的迭代视觉证据搜寻来加强推理可靠性。2. 设计SocraticAgent自博弈多智能体系统，交替进行推理和视觉检查，生成推理轨迹。3. 提出两阶段渐进式强化学习方案，先在细粒度定位任务上强化RS-EoT能力，再泛化到更广泛的遥感问答场景。

Result: 实验表明，RS-EoT范式及其训练方法在多项遥感视觉问答和定位基准测试中均获得了最新最优性能。分析显示，该方法促成了明显的迭代推理-证据搜寻循环，有效缓解了Glance Effect，实现了基于证据的真实推理。

Conclusion: RS-EoT能够克服以往多模态推理模型在遥感图像中的伪推理和浅层理解问题，实现了基于视觉证据的可靠推理。相关代码、数据和模型已开源，有望提升遥感智能系统的理解和应用性能。

Abstract: Recent multimodal reasoning models, inspired by DeepSeek-R1, have significantly advanced vision-language systems. However, in remote sensing (RS) tasks, we observe widespread pseudo reasoning: models narrate the process of reasoning rather than genuinely reason toward the correct answer based on visual evidence. We attribute this to the Glance Effect, where a single, coarse perception of large-scale RS imagery results in incomplete understanding and reasoning based on linguistic self-consistency instead of visual evidence. To address this, we propose RS-EoT (Remote Sensing Evidence-of-Thought), a language-driven, iterative visual evidence-seeking paradigm. To instill this paradigm, we propose SocraticAgent, a self-play multi-agent system that synthesizes reasoning traces via alternating cycles of reasoning and visual inspection. To enhance and generalize these patterns, we propose a two-stage progressive RL strategy: first, RL on fine-grained Grounding tasks to enhance RS-EoT capabilities, followed by RL on RS VQA to generalize to broader understanding scenarios. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Analyses reveal clear iterative cycles of reasoning and evidence seeking, confirming RS-EoT mitigates the Glance Effect and enables genuine evidence-grounded reasoning. Our code, data, and models are available at https://geox-lab.github.io/Asking_like_Socrates

</details>


### [76] [UAV-MM3D: A Large-Scale Synthetic Benchmark for 3D Perception of Unmanned Aerial Vehicles with Multi-Modal Data](https://arxiv.org/abs/2511.22404)
*Longkun Zou,Jiale Wang,Rongqin Liang,Hai Wu,Ke Chen,Yaowei Wang*

Main category: cs.CV

TL;DR: 本文提出了UAV-MM3D，这是一个高保真、多模态的合成无人机数据集，丰富多样、标注全面，适合推进低空无人机三维感知与运动理解的相关研究。


<details>
  <summary>Details</summary>
Motivation: 在复杂低空环境下实现无人机的精准感知对于空域安全和智能系统至关重要。然而，实际采集大规模、精确标注、包含多模态数据的无人机数据集受制于空域法规、隐私以及环境多变性，且手工标注代价高昂。

Method: 作者通过模拟方法生成了UAV-MM3D数据集，涵盖40万帧来自多种场景和天气条件的同步多模态（RGB、红外、激光雷达、雷达、动态视觉传感器）数据，每帧提供丰富的2D/3D检测、姿态、实例级别标注。同时贡献了基于激光雷达引导的多模态融合基线方法（LGFusionNet）和无人机轨迹预测基线。

Result: UAV-MM3D数据集通过模拟环境提供丰富的多模态数据和精细标注，允许大规模和多任务的无人机感知、检测、姿态估计、跟踪及短期轨迹预测实验。基线方法为未来研究提供了标准化的比较平台。

Conclusion: UAV-MM3D为学界和产业提供了一个公开、多模态、可控、全面的无人机三维感知基准，有望显著推动无人机感知与运动理解领域的发展。

Abstract: Accurate perception of UAVs in complex low-altitude environments is critical for airspace security and related intelligent systems. Developing reliable solutions requires large-scale, accurately annotated, and multimodal data. However, real-world UAV data collection faces inherent constraints due to airspace regulations, privacy concerns, and environmental variability, while manual annotation of 3D poses and cross-modal correspondences is time-consuming and costly. To overcome these challenges, we introduce UAV-MM3D, a high-fidelity multimodal synthetic dataset for low-altitude UAV perception and motion understanding. It comprises 400K synchronized frames across diverse scenes (urban areas, suburbs, forests, coastal regions) and weather conditions (clear, cloudy, rainy, foggy), featuring multiple UAV models (micro, small, medium-sized) and five modalities - RGB, IR, LiDAR, Radar, and DVS (Dynamic Vision Sensor). Each frame provides 2D/3D bounding boxes, 6-DoF poses, and instance-level annotations, enabling core tasks related to UAVs such as 3D detection, pose estimation, target tracking, and short-term trajectory forecasting. We further propose LGFusionNet, a LiDAR-guided multimodal fusion baseline, and a dedicated UAV trajectory prediction baseline to facilitate benchmarking. With its controllable simulation environment, comprehensive scenario coverage, and rich annotations, UAV3D offers a public benchmark for advancing 3D perception of UAVs.

</details>


### [77] [DiffStyle360: Diffusion-Based 360° Head Stylization via Style Fusion Attention](https://arxiv.org/abs/2511.22411)
*Furkan Guzelant,Arda Goktogan,Tarık Kaya,Aysegul Dundar*

Main category: cs.CV

TL;DR: 本文提出了DiffStyle360，一种基于扩散模型（diffusion-based）的3D人头风格化方法，实现了多视角一致、身份保持的3D人头艺术化风格生成，无需针对每种风格单独训练，风格迁移广泛且高效。


<details>
  <summary>Details</summary>
Motivation: 现有3D人头风格化方法常依赖计算成本高的优化或风格特定微调，难以高效适应新风格，且在多视角一致和身份保持方面有局限，限制了其在数字媒体艺术设计中的应用。

Method: 提出DiffStyle360框架，核心包括：1）风格表观模块，实现风格与内容的分离解耦；2）风格融合注意力机制，自适应平衡结构保持与风格化效果；3）利用3D GAN生成多视角数据集进行稳健微调；4）引入基于温度的关键缩放策略，控制推断阶段风格化强度。算法基于DiffPortrait360架构，可根据单一风格参考图生成多视角一致的3D风格化人头模型。

Result: 在FFHQ与RenderMe360等数据集上的实验证明，DiffStyle360在多种风格域下生成的3D人头风格和身份保真度均优于当前最先进的GAN和扩散模型方法，风格质量显著提升。

Conclusion: DiffStyle360实现了高效、可扩展且无需每种风格单独训练的3D人头风格化，推动了多视角一致与身份保持的3D头部艺术生成，为虚拟角色设计和数字内容创意提供了强大工具。

Abstract: 3D head stylization has emerged as a key technique for reimagining realistic human heads in various artistic forms, enabling expressive character design and creative visual experiences in digital media. Despite the progress in 3D-aware generation, existing 3D head stylization methods often rely on computationally expensive optimization or domain-specific fine-tuning to adapt to new styles. To address these limitations, we propose DiffStyle360, a diffusion-based framework capable of producing multi-view consistent, identity-preserving 3D head stylizations across diverse artistic domains given a single style reference image, without requiring per-style training. Building upon the 3D-aware DiffPortrait360 architecture, our approach introduces two key components: the Style Appearance Module, which disentangles style from content, and the Style Fusion Attention mechanism, which adaptively balances structure preservation and stylization fidelity in the latent space. Furthermore, we employ a 3D GAN-generated multi-view dataset for robust fine-tuning and introduce a temperaturebased key scaling strategy to control stylization intensity during inference. Extensive experiments on FFHQ and RenderMe360 demonstrate that DiffStyle360 achieves superior style quality, outperforming state-of-the-art GAN- and diffusion-based stylization methods across challenging style domains.

</details>


### [78] [Wukong's 72 Transformations: High-fidelity Textured 3D Morphing via Flow Models](https://arxiv.org/abs/2511.22425)
*Minghao Yin,Yukang Cao,Kai Han*

Main category: cs.CV

TL;DR: 本文提出了WUKONG，一个无需训练即可实现高保真带纹理三维形态转换的新框架，支持图片或文本为输入，相比传统方法性能更优。


<details>
  <summary>Details</summary>
Motivation: 传统三维形态变换方法依赖繁琐的手动配准和形变轨迹估计，泛化性差且预处理成本高，因此亟需一种自动化、通用且能够高质量生成三维形态变换新技术。

Method: WUKONG利用flow-based transformer的生成先验，针对形状变换，利用连续流生成过程，并将其建模为最优传输barycenter问题。为避免突变和保证身份一致性，引入了序列初始化策略。为实现纹理高保真，设计了基于相似度引导的语义一致性机制，有效保留高频细节并控制混合动态，减少过度平滑等常见伪影。

Result: 通过大量定量和定性实验，WUKONG在几何和纹理多样性场景下均显著优于现有技术，生成的三维过渡高质量且保真度高。

Conclusion: WUKONG无需训练即可自动生成高保真的三维形状及纹理过渡，有效解决了传统方法在泛化性、预处理和质量方面的不足，在三维形态变换领域表现突出。

Abstract: We present WUKONG, a novel training-free framework for high-fidelity textured 3D morphing that takes a pair of source and target prompts (image or text) as input. Unlike conventional methods -- which rely on manual correspondence matching and deformation trajectory estimation (limiting generalization and requiring costly preprocessing) -- WUKONG leverages the generative prior of flow-based transformers to produce high-fidelity 3D transitions with rich texture details. To ensure smooth shape transitions, we exploit the inherent continuity of flow-based generative processes and formulate morphing as an optimal transport barycenter problem. We further introduce a sequential initialization strategy to prevent abrupt geometric distortions and preserve identity coherence. For faithful texture preservation, we propose a similarity-guided semantic consistency mechanism that selectively retains high-frequency details and enables precise control over blending dynamics. This avoids common artifacts like oversmoothing while maintaining semantic fidelity. Extensive quantitative and qualitative evaluations demonstrate that WUKONG significantly outperforms state-of-the-art methods, achieving superior results across diverse geometry and texture variations.

</details>


### [79] [Fin3R: Fine-tuning Feed-forward 3D Reconstruction Models via Monocular Knowledge Distillation](https://arxiv.org/abs/2511.22429)
*Weining Ren,Hongjun Wang,Xiao Tan,Kai Han*

Main category: cs.CV

TL;DR: Fin3R是一种针对前馈3D重建模型的高效微调方法，通过只微调图像编码器并引入轻量化LoRA适配器，显著提升模型的几何细节和鲁棒性，且计算开销极低。


<details>
  <summary>Details</summary>
Motivation: 目前的前馈3D重建模型受限于高保真深度与姿态监督的缺乏，以及多视图点云回归导致的几何错位，因此难以重建精细几何细节并保证鲁棒性。作者希望通过一种简单通用的微调方案提升上述性能瓶颈。

Method: 提出Fin3R方法，将前馈3D重建模型的解码器（匹配视角任务）冻结，仅对特征提取专用的图像编码器进行微调。微调时，编码器从单目教师模型中蒸馏获得几何细节，并用轻量化自定义LoRA适配器实现高效适配。所用数据为大规模无标注数据集。

Result: Fin3R在包括DUSt3R、MASt3R、CUT3R、VGGT等多种模型上，在单视图和多视图场景均带来更锐利的边界、更复杂的结构和更高的几何精度。所增加的LoRA参数极小，对测试时的内存和延迟几乎无影响。

Conclusion: Fin3R为前馈3D重建模型提供了一种通用、简单且极高性价比的微调方法，显著提升细节精度及鲁棒性，且不会增加运行负担。

Abstract: We present Fin3R, a simple, effective, and general fine-tuning method for feed-forward 3D reconstruction models. The family of feed-forward reconstruction model regresses pointmap of all input images to a reference frame coordinate system, along with other auxiliary outputs, in a single forward pass. However, we find that current models struggle with fine geometry and robustness due to (\textit{i}) the scarcity of high-fidelity depth and pose supervision and (\textit{ii}) the inherent geometric misalignment from multi-view pointmap regression. Fin3R jointly tackles two issues with an extra lightweight fine-tuning step. We freeze the decoder, which handles view matching, and fine-tune only the image encoder-the component dedicated to feature extraction. The encoder is enriched with fine geometric details distilled from a strong monocular teacher model on large, unlabeled datasets, using a custom, lightweight LoRA adapter. We validate our method on a wide range of models, including DUSt3R, MASt3R, CUT3R, and VGGT. The fine-tuned models consistently deliver sharper boundaries, recover complex structures, and achieve higher geometric accuracy in both single- and multi-view settings, while adding only the tiny LoRA weights, which leave test-time memory and latency virtually unchanged. Project page: \href{http://visual-ai.github.io/fin3r}{https://visual-ai.github.io/fin3r}

</details>


### [80] [SkeletonAgent: An Agentic Interaction Framework for Skeleton-based Action Recognition](https://arxiv.org/abs/2511.22433)
*Hongda Liu,Yunfan Liu,Changlu Wang,Yunlong Wang,Zhenan Sun*

Main category: cs.CV

TL;DR: SkeletonAgent通过让识别模型与大语言模型（LLM）协作，使骨骼动作识别更具判别性，并在多个公开数据集上性能超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有骨骼动作识别方法利用LLM获取语义先验，但LLM一般独立于识别模型、无反馈，难以获得区分相似动作的有效信息。

Method: 提出SkeletonAgent框架，将识别模型和LLM通过Questioner和Selector两个协作代理连接。Questioner找出常被混淆的类别，将其作为上下文输入LLM，获取有针对性的语义引导；Selector解析LLM输出，提取关键关节级约束并反馈给识别器，实现更精细的跨模态对齐。

Result: 在NTU RGB+D、NTU RGB+D 120、Kinetics-Skeleton、FineGYM和UAV-Human五个数据集上，SkeletonAgent表现优于所有现有主流方法。

Conclusion: SkeletonAgent能够有效弥补LLM与识别模型协作的缺陷，提升骨骼动作识别精度，能够为后续多模态动作识别带来创新思路。

Abstract: Recent advances in skeleton-based action recognition increasingly leverage semantic priors from Large Language Models (LLMs) to enrich skeletal representations. However, the LLM is typically queried in isolation from the recognition model and receives no performance feedback. As a result, it often fails to deliver the targeted discriminative cues critical to distinguish similar actions. To overcome these limitations, we propose SkeletonAgent, a novel framework that bridges the recognition model and the LLM through two cooperative agents, i.e., Questioner and Selector. Specifically, the Questioner identifies the most frequently confused classes and supplies them to the LLM as context for more targeted guidance. Conversely, the Selector parses the LLM's response to extract precise joint-level constraints and feeds them back to the recognizer, enabling finer-grained cross-modal alignment. Comprehensive evaluations on five benchmarks, including NTU RGB+D, NTU RGB+D 120, Kinetics-Skeleton, FineGYM, and UAV-Human, demonstrate that SkeletonAgent consistently outperforms state-of-the-art benchmark methods. The code is available at https://github.com/firework8/SkeletonAgent.

</details>


### [81] [ABounD: Adversarial Boundary-Driven Few-Shot Learning for Multi-Class Anomaly Detection](https://arxiv.org/abs/2511.22436)
*Runzhi Deng,Yundi Hu,Xinshuang Zhang,Zhao Wang,Xixi Liu,Wang-Zhou Dai,Caifeng Shan,Fang Zhao*

Main category: cs.CV

TL;DR: 本文提出了ABounD，一种用于少样本多类别工业异常检测的对抗性边界驱动学习框架，通过结合语义概念学习与决策边界塑造，实现了高效且更精确的异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在工业异常检测中的少样本和多类别任务中，难以兼顾类别自适应性和区分性，尤其在样本稀缺时容易导致边界模糊，错漏检测异常或误判正常样本，因此需要更优的方法来区分正常与异常。

Method: ABounD框架融合动态概念融合（DCF）模块和对抗性边界锻造（ABF）模块。DCF模块通过结合通用先验和类别特定信息，生成自适应提示，增强语义理解；ABF模块则采用PGD风格扰动生成边界级特征，推动模型构建更精确的判别边界。在单阶段训练中采用概念-边界损失，以ABF为主要监督信号同时辅以正则项，提升模型稳健性。

Result: 在MVTec-AD和VisA数据集上，ABounD方法在少样本多类别异常检测任务上取得了当前最优的表现，显著优于现有方法。

Conclusion: ABounD通过整合语义理解和边界优化，实现了更强的异常检测能力，能够兼顾灵活性与语义对齐，对工业场景下的实际应用具有重要意义。

Abstract: Few-shot multi-class industrial anomaly detection remains a challenging task. Vision-language models need to be both category-adaptive and sharply discriminative, yet data scarcity often blurs the boundary between normal and abnormal states. This ambiguity leads to missed subtle defects and the rejection of atypical normal samples. We propose ABounD, an Adversarial Boundary-Driven few-shot learning for multi-class anomaly detection, which is a unified learning framework that integrates semantic concept learning with decision boundary shaping. The Dynamic Concept Fusion (DCF) module produces class-adaptive prompts by fusing generalizable priors with class-specific cues, conditioned on image features. Meanwhile, Adversarial Boundary Forging (ABF) sculpts a more precise decision margin by generating boundary-level fence features via PGD-style perturbations. Training is conducted in a single stage under a Concept-Boundary Loss, where ABF provides the main supervisory signal and semantic-spatial regularizers stabilize the optimization. This synergy yields a decision boundary that closely follows normal data while preserving flexibility and robust semantic alignment. Experiments on MVTec-AD and VisA datasets demonstrate state-of-the-art performance in the task of few-shot multi-class anomaly detection.

</details>


### [82] [Do You See What I Say? Generalizable Deepfake Detection based on Visual Speech Recognition](https://arxiv.org/abs/2511.22443)
*Maheswar Bora,Tashvik Dhamija,Shukesh Reddy,Baptiste Chopin,Pranav Balaji,Abhijit Das,Antitza Dantcheva*

Main category: cs.CV

TL;DR: 本文提出了FauxNet，一种基于视觉语音识别（VSR）特征的深度伪造检测网络，能够实现对多种深度伪造技术下生成视频的零样本泛化检测，并且还构建了两个大规模数据集用于验证模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 深度伪造内容（如视频和音频）越来越逼真，导致虚假信息传播等安全与伦理风险日益突出。因此，亟需研发稳健、泛化能力强的深度伪造检测算法，以防止深度伪造的滥用。

Method: 作者提出FauxNet，基于预训练的视觉语音识别（VSR）模型，提取视频中的时序特征用于检测。该方法不仅能判别真假视频，还能区分不同深度伪造技术来源。作者同时构建了两个新数据集Authentica-Vox和Authentica-HDTF，涵盖六种主流深度伪造技术生成的伪造视频。

Result: FauxNet在零样本检测（zero-shot）场景下，相比当前最优方法（state-of-the-art）表现更优。在Authentica和FaceForensics++等多个数据集上全面测试，验证了其优越性。此外，能对深度伪造技术归属进行有效判别。

Conclusion: FauxNet显著提高了深度伪造检测的泛化能力和准确性，是打击深度伪造滥用的重要进展。新数据集的发布也将推动该领域进一步发展。

Abstract: Deepfake generation has witnessed remarkable progress, contributing to highly realistic generated images, videos, and audio. While technically intriguing, such progress has raised serious concerns related to the misuse of manipulated media. To mitigate such misuse, robust and reliable deepfake detection is urgently needed. Towards this, we propose a novel network FauxNet, which is based on pre-trained Visual Speech Recognition (VSR) features. By extracting temporal VSR features from videos, we identify and segregate real videos from manipulated ones. The holy grail in this context has to do with zero-shot detection, i.e., generalizable detection, which we focus on in this work. FauxNet consistently outperforms the state-of-the-art in this setting. In addition, FauxNet is able to attribute - distinguish between generation techniques from which the videos stem. Finally, we propose new datasets, referred to as Authentica-Vox and Authentica-HDTF, comprising about 38,000 real and fake videos in total, the latter created with six recent deepfake generation techniques. We provide extensive analysis and results on the Authentica datasets and FaceForensics++, demonstrating the superiority of FauxNet. The Authentica datasets will be made publicly available.

</details>


### [83] [Benchmarking machine learning models for multi-class state recognition in double duantum dot data](https://arxiv.org/abs/2511.22451)
*Valeria Díaz Moreno,Ryan P Khalili,Daniel Schug,Patrick J. Walsh,Justyna P. Zwolak*

Main category: cs.CV

TL;DR: 本论文系统性评估了四种主流机器学习模型在半导体量子点加载态识别上的表现，并给出实用优化建议。


<details>
  <summary>Details</summary>
Motivation: 随着量子处理器扩大型阵列的发展，需开发可靠的自动调谐方法，而关键瓶颈在于准确识别电荷稳定性图中的量子点状态。

Method: 作者设计了使用合成与实验双数据集，全面对比U-Net、视觉Transformer（ViT）、混合密度网络（MDN）和卷积神经网络（CNN）在多分类识别任务下的性能，包括数据量及归一化处理的影响。

Result: U-Net与ViT在合成数据上表现优异，但迁移到实验数据时泛化较差；MDN训练稳定、计算高效但峰值性能较低；CNN在实验数据上以远低于U-Net、ViT的参数量实现了良好准确率。归一化方面，min-max一般带来较高准确但训练不稳定，而z-score训练更平稳但准确率下降。

Conclusion: CNN结合min-max归一化，在量子点电荷稳定图状态识别方面效果最好，是实际应用的推荐方案。

Abstract: Semiconductor quantum dots (QDs) are a leading platform for scalable quantum processors. However, scaling to large arrays requires reliable, automated tuning strategies for devices' bootstrapping, calibration, and operation, with many tuning aspects depending on accurately identifying QD device states from charge-stability diagrams (CSDs). In this work, we present a comprehensive benchmarking study of four modern machine learning (ML) architectures for multi-class state recognition in double-QD CSDs. We evaluate their performance across different data budgets and normalization schemes using both synthetic and experimental data. We find that the more resource-intensive models -- U-Nets and visual transformers (ViTs) -- achieve the highest MSE score (defined as $1-\mathrm{MSE}$) on synthetic data (over $0.98$) but fail to generalize to experimental data. MDNs are the most computationally efficient and exhibit highly stable training, but with substantially lower peak performance. CNNs offer the most favorable trade-off on experimental CSDs, achieving strong accuracy with two orders of magnitude fewer parameters than the U-Nets and ViTs. Normalization plays a nontrivial role: min-max scaling generally yields higher MSE scores but less stable convergence, whereas z-score normalization produces more predictable training dynamics but at reduced accuracy for most models. Overall, our study shows that CNNs with min-max normalization are a practical approach for QD CSDs.

</details>


### [84] [Beyond Real versus Fake Towards Intent-Aware Video Analysis](https://arxiv.org/abs/2511.22455)
*Saurabh Atreya,Nabyl Quignon,Baptiste Chopin,Abhijit Das,Antitza Dantcheva*

Main category: cs.CV

TL;DR: 本文提出了IntentHQ数据集，用于对深度伪造视频的意图进行细致分析，并且构建了多模态模型实现意图识别。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造（deepfake）视频越来越逼真，社会风险加剧。虽然已有方法关注于识别真假，但它们没有回答‘视频被操控的意图是什么’这一根本问题。

Method: 构建了IntentHQ基准数据集，包含5168个经过精细注释的视频，涵盖23种细粒度意图库类别。提出了结合时空特征、音频和文本等多模态信息的监督和自监督学习方法进行意图识别。

Result: 实验表明所提出的多模态方法能够有效区分和识别多种复杂的意图类别。

Conclusion: 该工作推动了深度伪造视频研究从真实性验证向意图识别转变，有助于理解视频背后的动机并增强社会安全。

Abstract: The rapid advancement of generative models has led to increasingly realistic deepfake videos, posing significant societal and security risks. While existing detection methods focus on distinguishing real from fake videos, such approaches fail to address a fundamental question: What is the intent behind a manipulated video? Towards addressing this question, we introduce IntentHQ: a new benchmark for human-centered intent analysis, shifting the paradigm from authenticity verification to contextual understanding of videos. IntentHQ consists of 5168 videos that have been meticulously collected and annotated with 23 fine-grained intent-categories, including "Financial fraud", "Indirect marketing", "Political propaganda", as well as "Fear mongering". We perform intent recognition with supervised and self-supervised multi-modality models that integrate spatio-temporal video features, audio processing, and text analysis to infer underlying motivations and goals behind videos. Our proposed model is streamlined to differentiate between a wide range of intent-categories.

</details>


### [85] [ITS3D: Inference-Time Scaling for Text-Guided 3D Diffusion Models](https://arxiv.org/abs/2511.22456)
*Zhenglin Zhou,Fan Ma,Xiaobo Xia,Hehe Fan,Yi Yang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出了一种名为ITS3D的新框架，通过优化高斯噪声输入提升文本引导的3D扩散模型的生成质量，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 当前文本驱动的3D生成模型在提升生成质量时往往依赖额外训练，成本高、效率低，因此需要在推理阶段提升生成质量的方法。

Method: ITS3D将问题形式化为优化高斯噪声输入，并引入验证器引导的搜索算法，通过迭代优化选择最优噪声。为提升搜索稳定性、效率和探索能力，提出高斯归一化（稳定过程）、SVD降维（提升效率）、奇异空间重置（防止陷入局部最优）三项技术。

Result: 实验显示，ITS3D显著提升了文本到3D生成的效果，验证了计算高效的推理时搜索方法的潜力。

Conclusion: ITS3D为提升3D扩散生成模型质量提供了一种新思路，不依赖额外训练，具备实际应用前景和开源实现。

Abstract: We explore inference-time scaling in text-guided 3D diffusion models to enhance generative quality without additional training. To this end, we introduce ITS3D, a framework that formulates the task as an optimization problem to identify the most effective Gaussian noise input. The framework is driven by a verifier-guided search algorithm, where the search algorithm iteratively refines noise candidates based on verifier feedback. To address the inherent challenges of 3D generation, we introduce three techniques for improved stability, efficiency, and exploration capability. 1) Gaussian normalization is applied to stabilize the search process. It corrects distribution shifts when noise candidates deviate from a standard Gaussian distribution during iterative updates. 2) The high-dimensional nature of the 3D search space increases computational complexity. To mitigate this, a singular value decomposition-based compression technique is employed to reduce dimensionality while preserving effective search directions. 3) To further prevent convergence to suboptimal local minima, a singular space reset mechanism dynamically updates the search space based on diversity measures. Extensive experiments demonstrate that ITS3D enhances text-to-3D generation quality, which shows the potential of computationally efficient search methods in generative processes. The source code is available at https://github.com/ZhenglinZhou/ITS3D.

</details>


### [86] [Gaussians on Fire: High-Frequency Reconstruction of Flames](https://arxiv.org/abs/2511.22459)
*Jakob Nazarenus,Dominik Michels,Wojtek Palubicki,Simin Kou,Fang-Lue Zhang,Soren Pirk,Reinhard Koch*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯模型的时空表示方法，仅利用三个视角，相对较少的相机数据，实现了动态火焰的三维重建，且在多种复杂火焰场景下表现稳定。


<details>
  <summary>Details</summary>
Motivation: 动态火焰因其形态变化快、半透明及丰富的高频特性，极难通过有限视角进行三维重建。当前方法在视角受限或成本较低时性能有限，促使作者寻求创新解决方案，旨在用经济的硬件实现高质量的火焰三维重建。

Method: 首先，利用密集多视图立体图像与单目深度先验分离静态背景与动态火焰区域，随后将火焰初始化为三维流场，通过融合每个视角的密集光流投影得到。为捕捉火焰丰富的高频特征，设计每个三维高斯粒子带有生命周期和线速度信息，满足光流约束。同时，通过定制的硬件同步模式实现不同相机间的高精度时间对齐。

Result: 在多组复杂真实火焰重建实验中，方法显示出强健而优异的定量和定性表现，能够准确、细致地还原火焰的时空动态特征。

Conclusion: 该研究成功提出一套低成本、可行性强的三维火焰重建方案，实现了高质量、时空精细的火焰捕捉，并为后续相关研究如特效制作、安全分析等提供了技术基础。

Abstract: We propose a method to reconstruct dynamic fire in 3D from a limited set of camera views with a Gaussian-based spatiotemporal representation. Capturing and reconstructing fire and its dynamics is highly challenging due to its volatile nature, transparent quality, and multitude of high-frequency features. Despite these challenges, we aim to reconstruct fire from only three views, which consequently requires solving for under-constrained geometry. We solve this by separating the static background from the dynamic fire region by combining dense multi-view stereo images with monocular depth priors. The fire is initialized as a 3D flow field, obtained by fusing per-view dense optical flow projections. To capture the high frequency features of fire, each 3D Gaussian encodes a lifetime and linear velocity to match the dense optical flow. To ensure sub-frame temporal alignment across cameras we employ a custom hardware synchronization pattern -- allowing us to reconstruct fire with affordable commodity hardware. Our quantitative and qualitative validations across numerous reconstruction experiments demonstrate robust performance for diverse and challenging real fire scenarios.

</details>


### [87] [RoadSceneBench: A Lightweight Benchmark for Mid-Level Road Scene Understanding](https://arxiv.org/abs/2511.22466)
*Xiyan Liu,Han Wang,Yuhu Wang,Junjie Cai,Zhe Cao,Jianzhong Yang,Zhen Lu*

Main category: cs.CV

TL;DR: 本文提出了RoadSceneBench基准和一种新的视觉-语言模型训练方法（HRRP-T），以提升自动驾驶场景中的中层路面语义推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注低层感知任务（如检测、分割），忽略了推理道路结构与场景动态的推理能力，这限制了自动驾驶系统的高层决策和现实理解。因此，迫切需要能评估并促进视觉推理、关系理解和结构一致性的轻量高效基准。

Method: 1）提出RoadSceneBench——一个注重关系理解和结构一致性的中层路面语义基准数据集。2）引入HRRP-T训练框架：对视觉-语言模型，通过分层关系奖励及时间一致性机制，奖励模型生成空间连贯、语义一致的推理结果，促进模型做出几何感知和时序连贯的场景理解。

Result: 实验显示，所提方法在多样道路配置下均取得了当前最优性能。RoadSceneBench有效推动了结构感知的自动驾驶感知能力发展。

Conclusion: RoadSceneBench为中层道路语义与结构感知提供了高效平台，HRRP-T提升了模型结构推理和场景一致性表达能力，有望推动自动驾驶和数字地图语义理解的进一步进步。

Abstract: Understanding mid-level road semantics, which capture the structural and contextual cues that link low-level perception to high-level planning, is essential for reliable autonomous driving and digital map construction. However, existing benchmarks primarily target perception tasks such as detection or segmentation, overlooking the reasoning capabilities required to infer road topology and dynamic scene structure. To address this gap, we present RoadSceneBench, a lightweight yet information-rich benchmark designed to evaluate and advance visual reasoning in complex road environments. Unlike large-scale perception datasets, RoadSceneBench emphasizes relational understanding and structural consistency, encouraging models to capture the underlying logic of real-world road scenes. Furthermore, to enhance reasoning reliability, we propose Hierarchical Relational Reward Propagation with Temporal Consistency (HRRP-T), a training framework for Vision-Language Models (VLMs) in which reward signals adaptively promote spatial coherence and semantic alignment throughout the reasoning process. This paradigm enables models to move beyond static recognition toward geometry-aware and temporally consistent reasoning. Extensive experiments demonstrate that our method achieves state-of-the-art performance across diverse road configurations. RoadSceneBench thus provides a compact yet powerful foundation for studying mid-level road semantics and fostering structure-aware autonomous perception. Our dataset is available at https://github.com/XiyanLiu/RoadSceneBench.

</details>


### [88] [Hybrid, Unified and Iterative: A Novel Framework for Text-based Person Anomaly Retrieval](https://arxiv.org/abs/2511.22470)
*Tien-Huy Nguyen,Huu-Loc Tran,Huu-Phong Phan-Nguyen,Quang-Vinh Dinh*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的文本-行人异常检索方法，结合了局部-全局混合特征提取模块（LHP）、多损失目标的统一图文模型（UIT）、新颖的特征选择算法以及迭代集成策略，在PAB数据集上显著提升了检索性能，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前文本-行人异常检索领域依赖深度学习技术，但如何进一步提取更细粒度且兼顾粗粒度的特征仍是难题。作者希望解决模型对细粒度表征能力不足的问题。

Method: 1. 提出LHP模块：融合局部与全局特征，提升细粒度信息表达能力。2. 构建UIT模型：统一加入ITC、ITM、MLM、MIM多种损失目标优化图文联合表征。3. 创新提出迭代集成策略，与传统集成方法不同，模型结果迭代组合融合。4. 利用LHP指导的新特征选择算法，提升检索效果。

Result: 在PAB数据集上，所提方法在R@1提升9.70%、R@5提升1.77%、R@10提升1.01%，均优于现有SOTA方法。实验验证了模型各组成部分的有效性。

Conclusion: 混合特征提取、统一多目标损失优化以及创新的集成与特征选择策略能够有效提升文本-行人异常检索性能，为后续研究提供了新思路。

Abstract: Text-based person anomaly retrieval has emerged as a challenging task, with most existing approaches relying on complex deep-learning techniques. This raises a research question: How can the model be optimized to achieve greater fine-grained features? To address this, we propose a Local-Global Hybrid Perspective (LHP) module integrated with a Vision-Language Model (VLM), designed to explore the effectiveness of incorporating both fine-grained features alongside coarse-grained features. Additionally, we investigate a Unified Image-Text (UIT) model that combines multiple objective loss functions, including Image-Text Contrastive (ITC), Image-Text Matching (ITM), Masked Language Modeling (MLM), and Masked Image Modeling (MIM) loss. Beyond this, we propose a novel iterative ensemble strategy, by combining iteratively instead of using model results simultaneously like other ensemble methods. To take advantage of the superior performance of the LHP model, we introduce a novel feature selection algorithm based on its guidance, which helps improve the model's performance. Extensive experiments demonstrate the effectiveness of our method in achieving state-of-the-art (SOTA) performance on PAB dataset, compared with previous work, with a 9.70\% improvement in R@1, 1.77\% improvement in R@5, and 1.01\% improvement in R@10.

</details>


### [89] [Rethinking Cross-Generator Image Forgery Detection through DINOv3](https://arxiv.org/abs/2511.22471)
*Zhenglin Huang,Jason Li,Haiquan Wen,Tianxiao Li,Xi Yang,Lu Qi,Bei Peng,Xiaowei Huang,Ming-Hsuan Yang,Guangliang Cheng*

Main category: cs.CV

TL;DR: 本研究发现，无需微调的视觉基础模型（如DINOv3）在生成模型跨源检测上表现优异，并提出了基于token排名的新方法，显著提升识别准确率。


<details>
  <summary>Details</summary>
Motivation: 目前主流的生成模型检测方法对特定生成器的伪造特征存在过度拟合，缺乏对新型、未见生成模型的泛化能力，导致检测性能大幅下降。

Method: 作者系统性地从频率、空间和token角度分析了视觉基础模型DINOv3的检测机制，发现其倾向于利用全局、低频结构这一易迁移的真伪线索。基于此洞见，提出了一种训练免疫的token排名方法，结合轻量级线性探测器，筛选出少量相关token，有效提升检测性能。

Result: 采用token排名后的小规模token子集，在所有评测数据集上，均提升了跨生成器检测的准确率，并展现出优良的泛化能力和效率。

Conclusion: 无微调的视觉基础模型对于多生成器的伪造检测具有强大的普适性和解释性。所提token-ranking方法也为图像伪造检测提供了高效实用、可解释的通用新基线。

Abstract: As generative models become increasingly diverse and powerful, cross-generator detection has emerged as a new challenge. Existing detection methods often memorize artifacts of specific generative models rather than learning transferable cues, leading to substantial failures on unseen generators. Surprisingly, this work finds that frozen visual foundation models, especially DINOv3, already exhibit strong cross-generator detection capability without any fine-tuning. Through systematic studies on frequency, spatial, and token perspectives, we observe that DINOv3 tends to rely on global, low-frequency structures as weak but transferable authenticity cues instead of high-frequency, generator-specific artifacts. Motivated by this insight, we introduce a simple, training-free token-ranking strategy followed by a lightweight linear probe to select a small subset of authenticity-relevant tokens. This token subset consistently improves detection accuracy across all evaluated datasets. Our study provides empirical evidence and a feasible hypothesis for understanding why foundation models generalize across diverse generators, offering a universal, efficient, and interpretable baseline for image forgery detection.

</details>


### [90] [AI killed the video star. Audio-driven diffusion model for expressive talking head generation](https://arxiv.org/abs/2511.22488)
*Baptiste Chopin,Tashvik Dhamija,Pranav Balaji,Yaohui Wang,Antitza Dantcheva*

Main category: cs.CV

TL;DR: 本论文提出了Dimitra++，一种基于音频驱动的人头生成新框架，通过条件运动扩散变换器（cMDT）实现更真实的口型、表情和头部动作生成。实验显示该方法在VoxCeleb2和CelebV-HQ等数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 音频驱动的说话人头像生成技术在虚拟人、电影制作等多个领域有着广泛应用，但目前方法对唇动、面部表情和头部动作的建模往往不够精细，影响了生成头像的真实感。因此，需要一种能更全面模拟说话人面部动态的新技术。

Method: 作者设计了一种条件运动扩散变换器（cMDT），利用3D表示来建模人脸运动序列。该模型以音频序列（驱动运动）和参考人脸图像（决定外观）为条件，从而生成与音频同步、包含丰富表情和头部动作的人脸动画。

Result: 在VoxCeleb2和CelebV-HQ两个主流数据集上，Dimitra++通过定量、定性实验及用户研究都优于先前方法，生成效果在唇动、表情、头部动作的一致性和自然度上表现更好。

Conclusion: Dimitra++框架能够更好地模拟和生成真实、同步的讲话人面部动态，显著提升了音频驱动头像生成的表现，推动了该领域发展。

Abstract: We propose Dimitra++, a novel framework for audio-driven talking head generation, streamlined to learn lip motion, facial expression, as well as head pose motion. Specifically, we propose a conditional Motion Diffusion Transformer (cMDT) to model facial motion sequences, employing a 3D representation. The cMDT is conditioned on two inputs: a reference facial image, which determines appearance, as well as an audio sequence, which drives the motion. Quantitative and qualitative experiments, as well as a user study on two widely employed datasets, i.e., VoxCeleb2 and CelebV-HQ, suggest that Dimitra++ is able to outperform existing approaches in generating realistic talking heads imparting lip motion, facial expression, and head pose.

</details>


### [91] [SciPostGen: Bridging the Gap between Scientific Papers and Poster Layouts](https://arxiv.org/abs/2511.22490)
*Shun Inadumi,Shohei Tanaka,Tosho Hirasawa,Atsushi Hashimoto,Koichiro Yoshino,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: 本文提出了SciPostGen数据集，研究和生成科学论文海报布局，通过检索增强方法提升了布局生成效果。


<details>
  <summary>Details</summary>
Motivation: 随着科学论文数量的增长，如何高效传达研究成果成为重要课题，而海报作为论文内容的展示媒介，对布局有更高要求。然而，目前缺少大规模配对标注数据，无法深入研究论文内容与海报布局之间的对应关系。

Method: 作者提出并构建了一个大规模海报与论文配对数据集SciPostGen，分析海报布局与论文结构的关联性，并提出一个检索增强的海报布局生成框架。该框架从数据库中检索与目标论文结构相似的海报布局，用于指导新海报的布局生成，并在有无布局约束两种条件下进行实验。

Result: 实验证明检索机制能够找到与论文结构一致的海报布局，提出的框架在考虑约束条件下也能生成合理的海报布局。

Conclusion: SciPostGen数据集和检索增强生成框架提升了海报布局的研究和生成质量，有助于更好地传达科学研究成果。

Abstract: As the number of scientific papers continues to grow, there is a demand for approaches that can effectively convey research findings, with posters serving as a key medium for presenting paper contents. Poster layouts determine how effectively research is communicated and understood, highlighting their growing importance. In particular, a gap remains in understanding how papers correspond to the layouts that present them, which calls for datasets with paired annotations at scale. To bridge this gap, we introduce SciPostGen, a large-scale dataset for understanding and generating poster layouts from scientific papers. Our analyses based on SciPostGen show that paper structures are associated with the number of layout elements in posters. Based on this insight, we explore a framework, Retrieval-Augmented Poster Layout Generation, which retrieves layouts consistent with a given paper and uses them as guidance for layout generation. We conducted experiments under two conditions: with and without layout constraints typically specified by poster creators. The results show that the retriever estimates layouts aligned with paper structures, and our framework generates layouts that also satisfy given constraints.

</details>


### [92] [What Shape Is Optimal for Masks in Text Removal?](https://arxiv.org/abs/2511.22499)
*Hyakka Nakada,Marika Kubota*

Main category: cs.CV

TL;DR: 本文针对含大量文本的图像去文本任务，提出通过贝叶斯优化方法自动调整高灵活性掩膜，实现更精确和实用的去文本效果。


<details>
  <summary>Details</summary>
Motivation: 已有的图像去文本方法多聚焦于简单户外场景文本，对于密集、复杂文档图像的去文本效果研究较少，技术在实际业务场景中效果不佳。本文为此构建了新的面向密集文本的基准数据，并分析了掩膜扰动对去文本表现的影响。

Method: 构建了大量包含密集文本的去文本基准数据集。提出一种基于贝叶斯优化的掩膜建模方法，用于自动学习和调整灵活的掩膜形状，从而更好地适应实际文本区域形态，实现字符级的精准掩膜。

Result: 实验发现，不同的掩膜轮廓对去文本效果有显著影响。采用贝叶斯优化学习得到的掩膜呈现字符级特性，同时最小包络文本区域并非最佳选择。掩膜精准调整对于提升去文本效果十分关键。

Conclusion: 实现了更为实用和精确的去文本掩膜建模，为实际图像去文本任务提供了一种可行、可操作的方法，也为人工掩膜编辑提供了指导依据。

Abstract: The advent of generative models has dramatically improved the accuracy of image inpainting. In particular, by removing specific text from document images, reconstructing original images is extremely important for industrial applications. However, most existing methods of text removal focus on deleting simple scene text which appears in images captured by a camera in an outdoor environment. There is little research dedicated to complex and practical images with dense text. Therefore, we created benchmark data for text removal from images including a large amount of text. From the data, we found that text-removal performance becomes vulnerable against mask profile perturbation. Thus, for practical text-removal tasks, precise tuning of the mask shape is essential. This study developed a method to model highly flexible mask profiles and learn their parameters using Bayesian optimization. The resulting profiles were found to be character-wise masks. It was also found that the minimum cover of a text region is not optimal. Our research is expected to pave the way for a user-friendly guideline for manual masking.

</details>


### [93] [DocVAL: Validated Chain-of-Thought Distillation for Grounded Document VQA](https://arxiv.org/abs/2511.22521)
*Ahmad Mohammadshirazi,Pinaki Prasad Guha Neogi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.CV

TL;DR: DocVAL是一种用于文档视觉问答（DocVQA）的链式思维蒸馏框架，能有效将大模型的空间推理能力迁移到小模型，兼顾精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有DocVQA系统在精度和效率间存在显著矛盾：大模型准确但成本高，小模型部署高效但空间定位能力差。急需突破能在保持效率的同时提升空间推理和定位表现。

Method: DocVAL包含三大核心部分：(1) 教师模型监督结合验证时的文本检测，过滤并降噪训练信号；(2) 多模块验证器VAL，确保答案正确和几何一致性，同时生成像素级误差反馈；(3) 两阶段学生模型训练，先学会经验证的思维链轨迹，再结合VAL反馈迭代精修。

Result: 学生模型（Gemma-3 12B）在DocVQA数据集获得91.4%的ANLS和82.4%的mAP，推理时无需OCR。消融实验表明：验证反馈带来6.3的mAP提升，迭代精修带来9.7的mAP提升。此外，公开了9.5万条高质量验证链式思维数据。

Conclusion: DocVAL显著提升了可部署小模型在文档视觉问答场景下的空间推理能力，为文档理解相关研究带来高质量数据和可行方法。

Abstract: Document visual question answering (DocVQA) requires models to jointly reason over textual content and spatial layout, yet current systems exhibit a sharp accuracy--efficiency trade-off: large teacher models achieve strong grounding but are too expensive for deployment, while compact students suffer substantial drops in localization performance. We propose DocVAL, a validated chain-of-thought distillation framework that transfers the spatial reasoning ability of a large teacher into a deployable student VLM through three key components: (1) teacher supervision with validation-time text detection to filter and denoise training signals, (2) a multi-module validator (VAL) that enforces answer correctness and geometric consistency while producing fine-grained, pixel-level error feedback, and (3) a two-stage student training scheme that first learns from validated CoT traces and then undergoes iterative refinement driven by VAL feedback. Our student (Gemma-3 12B) achieves 91.4\% ANLS and 82.4\% mAP on DocVQA as a pure VLM requiring no text detection or OCR at inference. Extensive ablations demonstrate that validated feedback contributes 6.3 mAP gain and iterative refinement accounts for 9.7 mAP improvement. We release 95k high-quality, validator-verified CoT traces to advance spatial reasoning research in document understanding.

</details>


### [94] [CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving](https://arxiv.org/abs/2511.22532)
*Zhaohui Wang,Tengbo Yu,Hao Tang*

Main category: cs.CV

TL;DR: 提出了一种新的视觉-语言-行动（VLA）模型CoT4AD，通过引入Chain-of-Thought（CoT）推理，提升模型在自动驾驶中的数值推理和因果推理能力，并在多个基准上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-行动模型在处理复杂自动驾驶场景时，数值推理能力不足，输入输出关系过于简单，导致难以进行逐步因果推理，影响实际应用效果。

Method: 提出CoT4AD框架，结合视觉观测和语言指令，实现语义推理、场景理解和轨迹规划。在训练阶段，显式建立“感知-提问-预测-行动”的推理链条，对齐推理空间和动作空间；在推理阶段，隐式进行Chain-of-Thought推理，提升数值推理和决策鲁棒性。

Result: 在真实和模拟数据集（nuScenes和Bench2Drive）上进行了大量实验，在开放环路和闭环评价中均取得了当前最优结果。

Conclusion: 引入CoT推理的CoT4AD显著提升了VLA模型在复杂场景下的性能，适用于需要更强推理和决策能力的自动驾驶系统。

Abstract: Vision-Language-Action (VLA) models have recently attracted growing attention in end-to-end autonomous driving for their strong reasoning capabilities and rich world knowledge. However, existing VLAs often suffer from limited numerical reasoning ability and overly simplified input-output mappings, which hinder their performance in complex driving scenarios requiring step-by-step causal reasoning. To address these challenges, we propose CoT4AD, a novel VLA framework that introduces Chain-of-Thought (CoT) reasoning for autonomous driving to enhance both numerical and causal reasoning in Vision-Language Models (VLMs). CoT4AD integrates visual observations and language instructions to perform semantic reasoning, scene understanding, and trajectory planning. During training, it explicitly models a perception-question-prediction-action CoT to align the reasoning space with the action space across multiple driving tasks. During inference, it performs implicit CoT reasoning to enable consistent numerical reasoning and robust decision-making in dynamic environments. Extensive experiments on both real-world and simulated benchmarks, including nuScenes and Bench2Drive, demonstrate that CoT4AD achieves state-of-the-art performance in both open-loop and closed-loop evaluations. Code will be released upon paper acceptance.

</details>


### [95] [Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration](https://arxiv.org/abs/2511.22533)
*Mengyu Yang,Yanming Yang,Chenyi Xu,Chenxi Song,Yufan Zuo,Tong Zhao,Ruibo Li,Chi Zhang*

Main category: cs.CV

TL;DR: 本文提出Fast3Dcache方法，通过几何感知的缓存机制，大幅加速3D扩散模型的推理过程，并显著降低计算量，同时保持生成结果的几何一致性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成2D图像、视频和3D形状方面表现优秀，但其推理过程由于需要迭代去噪，计算开销较大。目前的缓存加速方法虽能在2D或视频生成中生效，但直接应用于3D扩散模型会导致几何一致性破坏，需要新的技术来平衡效率和结构准确性。

Method: 提出Fast3Dcache，一个无需重新训练的、面向3D扩散模型的、几何感知的缓存框架。方法核心包括：(1)预测性缓存调度约束（PCSC），根据体素稳定性动态分配缓存额度；(2)时空稳定性判据（SSC），基于速度和加速度选择可复用的稳定特征。这样可避免缓存过程中积累的数值误差影响几何结构。

Result: 实验显示，Fast3Dcache在3D扩散推理中可实现高达27.12%的加速和54.8%的FLOPs减少。几何质量几乎无损，仅在Chamfer Distance指标上增加2.48%，F-Score下降1.95%。

Conclusion: 提出的Fast3Dcache方法能够在不损失（或极小损失）几何一致性的前提下，有效提升3D扩散模型的推理速度，并显著降低所需的计算资源，为3D生成领域带来实用价值。

Abstract: Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent caching-based methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, a training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces a Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and a Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to a 27.12% speed-up and a 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%).

</details>


### [96] [Diff-ICMH: Harmonizing Machine and Human Vision in Image Compression with Generative Prior](https://arxiv.org/abs/2511.22549)
*Ruoyu Feng,Yunpeng Qi,Jinming Liu,Yixin Gao,Xin Li,Xin Jin,Zhibo Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的生成式图像压缩框架 Diff-ICMH，能够同时兼顾人类视觉和机器分析，提升压缩图像的语义信息保真和感知质量。


<details>
  <summary>Details</summary>
Motivation: 传统图像压缩方案往往只能优化人类视觉体验或机器分析性能中的一个目标，很难兼顾二者。实际上，语义信息的完整性对智能任务和理解同样重要，而感知质量也有利于机器的特征提取。因此，亟需一种方法能够联合优化这两方面。

Method: 提出 Diff-ICMH 框架，集成生成式先验以保障图像感知真实性，通过引入语义一致性损失（SC loss）保证语义信息保真。同时，创新性地提出了标签引导模块（TGM），利用图像级标注激发预训练扩散模型生成能力，只需极小的额外比特率。该框架可一套编解码器支持多种智能任务，无需特定任务适配。

Result: 大量实验结果表明，Diff-ICMH 在多任务环境下兼具优越的泛化能力和视觉感知表现，效果优于现有方法。

Conclusion: Diff-ICMH 成功兼顾了机器分析和人类感知的需求，实现了一套系统支持多任务且保持高视觉质量的图像压缩方案，展示了其广泛的应用潜力。

Abstract: Image compression methods are usually optimized isolatedly for human perception or machine analysis tasks. We reveal fundamental commonalities between these objectives: preserving accurate semantic information is paramount, as it directly dictates the integrity of critical information for intelligent tasks and aids human understanding. Concurrently, enhanced perceptual quality not only improves visual appeal but also, by ensuring realistic image distributions, benefits semantic feature extraction for machine tasks. Based on this insight, we propose Diff-ICMH, a generative image compression framework aiming for harmonizing machine and human vision in image compression. It ensures perceptual realism by leveraging generative priors and simultaneously guarantees semantic fidelity through the incorporation of Semantic Consistency loss (SC loss) during training. Additionally, we introduce the Tag Guidance Module (TGM) that leverages highly semantic image-level tags to stimulate the pre-trained diffusion model's generative capabilities, requiring minimal additional bit rates. Consequently, Diff-ICMH supports multiple intelligent tasks through a single codec and bitstream without any task-specific adaptation, while preserving high-quality visual experience for human perception. Extensive experimental results demonstrate Diff-ICMH's superiority and generalizability across diverse tasks, while maintaining visual appeal for human perception. Code is available at: https://github.com/RuoyuFeng/Diff-ICMH.

</details>


### [97] [Bringing Your Portrait to 3D Presence](https://arxiv.org/abs/2511.22553)
*Jiawei Zhang,Lei Chu,Jiahao Li,Zhenyu Zang,Chong Li,Xiao Li,Xun Cao,Hao Zhu,Yan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种统一的单张图像三维人体角色重建框架，能够处理不同范围（头部、半身、全身）的输入，在三大难题上实现突破，达到了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 三维人体重建通常受到姿态变化、取景变化、数据稀缺及不可靠代理网格等多重限制，急需一种既具强泛化能力又能处理不同图像输入的统一方法。

Method: 提出了Dual-UV特征表示，将图像特征映射到规范UV空间，消除姿态和取景带来的混淆，并设计了结合2D生成多样性和3D几何一致性的因子化合成数据流形，以及带有稳健代理网格追踪的训练方案，以提升模型鲁棒性和泛化能力。

Result: 仅用半身合成数据训练，模型实现了头部和上半身三维重建的SOTA性能，并在全身重建上也具竞争力。实验和分析进一步验证了方法的有效性。

Conclusion: 通过创新特征表示、数据生成与训练机制，所提方法可以一站式解决输入图像范围不一的人体三维重建难题，并在真实场景中展现出优异的泛化和重建能力。

Abstract: We present a unified framework for reconstructing animatable 3D human avatars from a single portrait across head, half-body, and full-body inputs. Our method tackles three bottlenecks: pose- and framing-sensitive feature representations, limited scalable data, and unreliable proxy-mesh estimation. We introduce a Dual-UV representation that maps image features to a canonical UV space via Core-UV and Shell-UV branches, eliminating pose- and framing-induced token shifts. We also build a factorized synthetic data manifold combining 2D generative diversity with geometry-consistent 3D renderings, supported by a training scheme that improves realism and identity consistency. A robust proxy-mesh tracker maintains stability under partial visibility. Together, these components enable strong in-the-wild generalization. Trained only on half-body synthetic data, our model achieves state-of-the-art head and upper-body reconstruction and competitive full-body results. Extensive experiments and analyses further validate the effectiveness of our approach.

</details>


### [98] [Text Condition Embedded Regression Network for Automated Dental Abutment Design](https://arxiv.org/abs/2511.22578)
*Mianjie Zheng,Xinquan Yang,Xuguang Li,Xiaoling Luo,Xuefen Liu,Kun Tang,He Meng,Linlin Shen*

Main category: cs.CV

TL;DR: 本文提出了一种名为TCEAD的新型自动化植体基台（abutment）设计框架，通过引入文本条件嵌入和自监督网格掩码自编码器，有效提升了基台设计的效率及准确性。


<details>
  <summary>Details</summary>
Motivation: 人工牙种植体的基台设计繁琐且耗时，且设计不当可能引发严重并发症。利用人工智能辅助设计可以提升效率与适应性，亟需创新性自动化方法。

Method: 基于MeshMAE自监督框架，加入文本引导定位（TGL）模块，利用CLIP文本编码描述基台区域，实现精确定位。编码器预训练于口扫数据以增强特征提取能力，并通过大规模基台设计数据集验证。

Result: TCEAD在多个主流方法中表现优异，IoU提升0.8%-12.85%。大量实验结果验证了算法在自动基台设计任务中的有效性。

Conclusion: TCEAD为自动化人工牙种植体基台设计提供了强有力工具，显著提高设计准确性与效率，展示了实际应用潜力。

Abstract: The abutment is an important part of artificial dental implants, whose design process is time-consuming and labor-intensive. Long-term use of inappropriate dental implant abutments may result in implant complications, including peri-implantitis. Using artificial intelligence to assist dental implant abutment design can quickly improve the efficiency of abutment design and enhance abutment adaptability. In this paper, we propose a text condition embedded abutment design framework (TCEAD), the novel automated abutment design solution available in literature. The proposed study extends the self-supervised learning framework of the mesh mask autoencoder (MeshMAE) by introducing a text-guided localization (TGL) module to facilitate abutment area localization. As the parameter determination of the abutment is heavily dependent on local fine-grained features (the width and height of the implant and the distance to the opposing tooth), we pre-train the encoder using oral scan data to improve the model's feature extraction ability. Moreover, considering that the abutment area is only a small part of the oral scan data, we designed a TGL module, which introduces the description of the abutment area through the text encoder of Contrastive Language-Image Pre-training (CLIP), enabling the network to quickly locate the abutment area. We validated the performance of TCEAD on a large abutment design dataset. Extensive experiments demonstrate that TCEAD achieves an Intersection over Union (IoU) improvement of 0.8%-12.85% over other mainstream methods, underscoring its potential in automated dental abutment design.

</details>


### [99] [Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization](https://arxiv.org/abs/2511.22586)
*Yifan Du,Kun Zhou,Yingqian Min,Yue Ling,Wayne Xin Zhao,Youbin Wu*

Main category: cs.CV

TL;DR: 本文系统性分析了不同Chain-of-Thought（CoT）设计对视觉-语言模型（VLM）泛化视觉推理能力的影响，发现精简且以基本定位步骤为主的CoT更有助于模型泛化。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在推理任务中广泛采用CoT增强中间推理过程，但尚不明确不同CoT设计对模型泛化推理能力影响的机理与优劣。作者希望通过系统对比明确哪些CoT设计最有效支持视觉推理泛化。

Method: 作者在可控的迷宫求解基准上对三种典型CoT格式（语言型CoT、带空间坐标的定位型CoT、图像操作型视觉CoT）进行对比，采用Qwen2.5-VL-7B模型在SFT及RL流程下评估其对收敛速度及最终性能的影响。

Result: 结果发现：视觉化和冗长的CoT加快收敛但提升不了最终性能；简明只包含必要定位步骤的CoT效果最佳；保留最低限度定位信息的CoT在不同难度迷宫中泛化能力极佳。其他视觉任务也获得类似发现。

Conclusion: 简短、结构清晰的CoT对提升视觉推理模型泛化能力最有效，为视觉推理相关SFT数据集的设计提供了实证指导。

Abstract: We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as "think with image", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a "short is long" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.

</details>


### [100] [HarmoCLIP: Harmonizing Global and Regional Representations in Contrastive Vision-Language Models](https://arxiv.org/abs/2511.22594)
*Haoxi Zeng,Haoxuan Li,Yi Bin,Pengpeng Zeng,Xing Xu,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: 本文提出了一种新方法 HarmoCLIP，通过引入区域级的语义监督和新的对齐策略，有效兼顾CLIP模型中的全局与局部表现，实现了地区与文本语义的直接对齐并取得领域内最佳表现。


<details>
  <summary>Details</summary>
Motivation: 受CLIP模型缺乏图文区域级监督的限制，其细粒度语义理解能力不足。虽然现有方法试图提升局部感知，但往往会损害全局一致性，导致二者难以兼顾。该问题阻碍了CLIP在更精细化多模态任务中的应用。

Method: HarmoCLIP首先分析了局部语义对齐缺失是当前CLIP局部-全局表现权衡的根本原因。方法通过引入显式的细粒度语义监督，将文本片段与对应视觉区域直接对齐，并设计了新颖的区域-语言对齐策略，强化模型的本地表现同时保持全局一致性。

Result: 在大规模视觉-语言检索与区域级边界框分类任务上，HarmoCLIP取得了最优（全局任务提升至69.78%），区域任务Top-1准确率提升3.2%，全面超越以往方案。

Conclusion: HarmoCLIP有效缓解了CLIP全局-局部权衡难题，在不损失全局对齐的前提下加强了局部语义表征，提供了高效、易用且平衡的plug-and-play解决方案，并且开源代码支持社区应用。

Abstract: Contrastive Language-Image Pre-training (CLIP) has demonstrated remarkable generalization ability and strong performance across a wide range of vision-language tasks. However, due to the lack of region-level supervision, CLIP exhibits limited fine-grained semantic understanding. Although several methods attempt to mitigate this issue, they unintentionally disrupt the global alignment, resulting in a persistent trade-off where improving local perception simultaneously degrades global coherence. In this paper, we propose HarmoCLIP, a novel framework designed to harmonize global and region representations within CLIP. We first identify that the absence of direct alignment between local textual and visual semantics is the fundamental cause of the trade-off. To address this, HarmoCLIP introduces an explicit fine-grained semantic supervision term that directly aligns textual segments with their corresponding visual regions, effectively bridging the image region space and the textual space. To further strengthen the representation capability at the local level, our method introduces a novel Region-Language Alignment supervision strategy that promotes fine-grained semantic learning without compromising global semantic consistency. Extensive experiments demonstrate that HarmoCLIP achieves state-of-the-art (improvement up to 69.78%) performance on the global task of retrieval and yields a substantial 3.2% improvement in Top-1 accuracy on the region task of bounding-box classification, consistently outperforming prior approaches while providing a balanced, efficient, and plug-and-play solution to the global-local trade-off in CLIP. Code is available at https://github.com/Erosist/HarmoCLIP.

</details>


### [101] [AnoRefiner: Anomaly-Aware Group-Wise Refinement for Zero-Shot Industrial Anomaly Detection](https://arxiv.org/abs/2511.22595)
*Dayou Huang,Feng Xue,Xurui Li,Yu Zhou*

Main category: cs.CV

TL;DR: 本文提出了AnoRefiner模块，可以无缝集成到多数零样本工业异常检测（ZSAD）方法中，将ViT等模型的粗糙块级异常检测结果精细化到像素级。通过引入异常得分图辅助精细化，显著提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 当前ZSAD方法多依赖ViT等模型提取块级特征，导致异常检测结果图粗糙，难以发现像素级细粒度异常，且针对真实异常与合成异常存在显著差异。这推动了寻找能利用图像中更多空间信息并精细检测异常的方法。

Method: 作者提出AnoRefiner，包含两个核心创新：1）异常精细化解码器（ARD），利用异常得分图逐步完善图像特征，减少对合成异常数据的依赖；2）以产品批次为单位的渐进式分组测试时训练(PGT)策略，使ARD针对每组产品在下组中提升异常检测表现，并且兼容任意ZSAD方法。

Result: 在MVTec AD和VisA数据集上，AnoRefiner集成到多种ZSAD模型后，在像素级AP指标上最大提升5.2%，视觉化结果也证实了其精细检测效果的提升。

Conclusion: AnoRefiner可以有效提升现有ZSAD方法的定位精度，实现由块级到像素级的高质量异常检测，减少对合成异常的依赖，具有良好的普适性和推广性。

Abstract: Zero-shot industrial anomaly detection (ZSAD) methods typically yield coarse anomaly maps as vision transformers (ViTs) extract patch-level features only. To solve this, recent solutions attempt to predict finer anomalies using features from ZSAD, but they still struggle to recover fine-grained anomalies without missed detections, mainly due to the gap between randomly synthesized training anomalies and real ones. We observe that anomaly score maps exactly provide complementary spatial cues that are largely absent from ZSAD's image features, a fact overlooked before.
  Inspired by this, we propose an anomaly-aware refiner (AnoRefiner) that can be plugged into most ZSAD models and improve patch-level anomaly maps to the pixel level. First, we design an anomaly refinement decoder (ARD) that progressively enhances image features using anomaly score maps, reducing the reliance on synthetic anomaly data. Second, motivated by the mass production paradigm, we propose a progressive group-wise test-time training (PGT) strategy that trains ARD in each product group for the refinement process in the next group, while staying compatible with any ZSAD method.
  Experiments on the MVTec AD and VisA datasets show that AnoRefiner boosts various ZSAD models by up to a 5.2\% gain in pixel-AP metrics, which can also be directly observed in many visualizations. The code will be available at https://github.com/HUST-SLOW/AnoRefiner.

</details>


### [102] [GazeTrack: High-Precision Eye Tracking Based on Regularization and Spatial Computing](https://arxiv.org/abs/2511.22607)
*Xiaoyin Yang*

Main category: cs.CV

TL;DR: 本文提出了新颖的注视点收集和追踪方法，同时发布了精准多样化的GazeTrack基准数据集，并通过算法创新显著提高了眼动追踪的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟现实和增强现实应用中的眼动追踪精度不足，难以满足空间计算的需求，因此亟需高精度数据集和新算法提升追踪准确率。

Method: 1. 设计高精度注视点采集框架，构建GazeTrack数据集，涵盖不同种族、年龄和视觉条件。
2. 提出了全新的形状误差正则化方法，用于约束瞳孔椭圆拟合，并训练于开源数据集，以增强语义分割与瞳孔定位。
3. 发明了类似纸张展开的坐标变换方法，更准确预测注视向量。
4. 构建了一个计算复杂度更低、追踪精度更高的注视向量生成模型。

Result: 新提出的算法在GazeTrack数据集上取得了更低的注视角误差，并且相较于以往方法具备更低的计算复杂度。

Conclusion: 该研究通过数据集建设与算法创新，推动了虚拟/增强现实下高精度眼动追踪的发展，为空间计算等应用提供了更精准、实时的 gaze tracking 解决方案。

Abstract: Eye tracking has become increasingly important in virtual and augmented reality applications; however, the current gaze accuracy falls short of meeting the requirements for spatial computing. We designed a gaze collection framework and utilized high-precision equipment to gather the first precise benchmark dataset, GazeTrack, encompassing diverse ethnicities, ages, and visual acuity conditions for pupil localization and gaze tracking. We propose a novel shape error regularization method to constrain pupil ellipse fitting and train on open-source datasets, enhancing semantic segmentation and pupil position prediction accuracy. Additionally, we invent a novel coordinate transformation method similar to paper unfolding to accurately predict gaze vectors on the GazeTrack dataset. Finally, we built a gaze vector generation model that achieves reduced gaze angle error with lower computational complexity compared to other methods.

</details>


### [103] [Stable-Drift: A Patient-Aware Latent Drift Replay Method for Stabilizing Representations in Continual Learning](https://arxiv.org/abs/2511.22615)
*Paraskevi-Antonia Theofilou,Anuhya Thota,Stefanos Kollias,Mamatha Thota*

Main category: cs.CV

TL;DR: 本文提出了一种以潜在漂移为导向的回放方法，通过识别并回放表现出高表征不稳定性的样本，有效缓解了深度学习模型在连续学习新数据时的灾难性遗忘问题，尤其适用于医学影像领域的跨医院场景。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在持续学习新数据时往往会遗忘已学任务（灾难性遗忘），这一问题显著限制了AI在医学成像领域的应用，特别是在模型需适应来自新医院的数据却不能损失既有诊断能力的场合。

Method: 提出一种基于潜在漂移的回放机制，具体做法为：量化样本在简单领域适应后内部特征表征的变化（即潜在漂移）；以患者为单位汇总漂移指标，将表现出最大多层表征变化的切片数据存入记忆缓冲区以做回放，从而确保回放样本的多样性和临床相关性；在跨医院COVID-19 CT图像分类任务中，结合CNN和Vision Transformer骨干网络进行验证。

Result: 与常规微调和随机回放方法相比，所提方法在减少模型遗忘方面有显著提升，增强了模型在实际医学跨域场景下的持续学习鲁棒性。

Conclusion: 潜在漂移是一种实用且具可解释性的回放信号，有助于提升医学影像等现实环境中的持续学习模型性能，有望推动AI在医疗领域的可靠部署。

Abstract: When deep learning models are sequentially trained on new data, they tend to abruptly lose performance on previously learned tasks, a critical failure known as catastrophic forgetting. This challenge severely limits the deployment of AI in medical imaging, where models must continually adapt to data from new hospitals without compromising established diagnostic knowledge. To address this, we introduce a latent drift-guided replay method that identifies and replays samples with high representational instability. Specifically, our method quantifies this instability via latent drift, the change in a sample internal feature representation after naive domain adaptation. To ensure diversity and clinical relevance, we aggregate drift at the patient level, our memory buffer stores the per patient slices exhibiting the greatest multi-layer representation shift. Evaluated on a cross-hospital COVID-19 CT classification task using state-of-the-art CNN and Vision Transformer backbones, our method substantially reduces forgetting compared to naive fine-tuning and random replay. This work highlights latent drift as a practical and interpretable replay signal for advancing robust continual learning in real world medical settings.

</details>


### [104] [REASONEDIT: Towards Reasoning-Enhanced Image Editing Models](https://arxiv.org/abs/2511.22625)
*Fukun Yin,Shiyu Liu,Yucheng Han,Zhibo Wang,Peng Xing,Rui Wang,Wei Cheng,Yingming Wang,Aojie Li,Zixin Yin,Pengtao Chen,Xiangyu Zhang,Daxin Jiang,Xianfang Zeng,Gang Yu*

Main category: cs.CV

TL;DR: 该论文提出在图像编辑模型中引入“大模型思考-编辑-反思”循环机制，将多模态大模型（MLLM）的推理能力与扩散解码器结合，显著提升了图像编辑精度。


<details>
  <summary>Details</summary>
Motivation: 目前主流的图像编辑架构采用MLLM编码器与扩散解码器耦合，MLLM通常在训练时被冻结，未充分利用其强大的推理和理解能力，该研究希望通过解锁和利用MLLM的推理能力，提升编辑模型理解抽象指令和纠正错误编辑的能力。

Method: 提出结合两种推理机制：“思考(Thinking)”机制用于基于世界知识更好地理解复杂、抽象的指令；“反思(Reflection)”机制用于对已生成的编辑结果进行自动评估、纠正错误并判断是否结束编辑循环。整个编辑过程在“思考-编辑-反思”三阶段循环中进行。

Result: 将该理由推理机制集成到现有编辑系统（如Step1X-Edit、Qwen-Image-Edit）中，在多个公开评测任务（ImgEdit、GEdit、Kris）上取得显著性能提升，如在ReasonEdit-S初始化下，ImgEdit、GEdit、Kris分别提升4.3%、4.7%、8.2%；在ReasonEdit-Q下，也在GEdit和Kris上超越了以往开源方法。

Conclusion: 充分利用MLLM的推理能力，结合“思考-编辑-反思”循环机制，可以极大提升图像编辑模型理解指令和纠错的综合能力，是图像编辑系统发展的重要方向。

Abstract: Recent advances in image editing models have shown remarkable progress. A common architectural design couples a multimodal large language model (MLLM) encoder with a diffusion decoder, as seen in systems such as Step1X-Edit and Qwen-Image-Edit, where the MLLM encodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy. Based on that, our proposed framework enables image editing in a thinking-editing-reflection loop: the thinking mechanism leverages the world knowledge of MLLM to interpret abstract instructions, while the reflection reviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements of ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%) when initializing our DiT from the Step1X-Edit (ReasonEdit-S), and also outperforms previous open-source methods on both GEdit and Kris when integrated with Qwen-Image-Edit (ReasonEdit-Q).

</details>


### [105] [GeoZero: Incentivizing Reasoning from Scratch on Geospatial Scenes](https://arxiv.org/abs/2511.22645)
*Di Wang,Shunyu Liu,Wentao Jiang,Fengxiang Wang,Yi Liu,Xiaolei Qin,Zhiming Luo,Chaoyang Zhou,Haonan Guo,Jing Zhang,Bo Du,Dacheng Tao,Liangpei Zhang*

Main category: cs.CV

TL;DR: 本文提出了GeoZero框架，使多模态大语言模型（MLLMs）能够无需链式思维（CoT）监督，进行地理空间推理，有效提升了模型多样性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感MLLMs依赖于高成本且带有人工偏见的CoT注释数据，限制了模型推理的多样性和推广能力。作者旨在消除预先设定的CoT监督，降低人工成本并提升推理多样性。

Method: 提出了GeoZero框架，包括：1）构建GeoZero-Instruct数据集用于初步监督微调，2）构建GeoZero-Hard用于激发模型在强化学习阶段的深度推理能力，3）引入Answer-Anchored Group Relative Policy Optimization（A$^2$GRPO）强化学习方法，通过模型自身答案约束推理过程，兼顾多样性与准确性。

Result: 在多个遥感视觉-语言基准上，GeoZero超越了现有方法，在多种地理空间任务上展现了更通用的涌现推理能力。

Conclusion: GeoZero无需昂贵的CoT监督即可实现高效且多样化的地理空间推理，具有较强的通用性，推动了遥感MLLMs推理能力的发展。

Abstract: Multimodal large language models (MLLMs) have undergone rapid development in advancing geospatial scene understanding. Recent studies have sought to enhance the reasoning capabilities of remote sensing MLLMs, typically through cold-start training with elaborately curated chain-of-thought (CoT) data. However, this approach not only incurs substantial annotation costs but also introduces human biases that may limit the diversity of model reasoning. To address these challenges, we propose GeoZero, a framework that enables MLLMs to perform geospatial reasoning without any predefined CoT supervision. Specifically, we construct two datasets, GeoZero-Instruct and GeoZero-Hard. GeoZero-Instruct allows the model to acquire preliminary geospatial knowledge through supervised fine-tuning, while GeoZero-Hard stimulates deep reasoning during the subsequent reinforcement learning stage. Furthermore, we introduce Answer-Anchored Group Relative Policy Optimization (A$^2$GRPO), where the reasoning process is regularized by the model's own answers, encouraging diverse yet accurate thinking. Extensive experiments on multiple remote sensing vision-language benchmarks demonstrate that GeoZero not only surpasses existing state-of-the-art methods but also fosters universal emergent reasoning capabilities across diverse geospatial tasks. Code,data,and models will be publicly available at https://github.com/MiliLab/GeoZero.

</details>


### [106] [Architecture Decoupling Is Not All You Need For Unified Multimodal Model](https://arxiv.org/abs/2511.22663)
*Dian Zheng,Manyuan Zhang,Hongyu Li,Kai Zou,Hongbo Liu,Ziyu Guo,Kaituo Feng,Yexin Liu,Ying Luo,Yan Feng,Peng Pei,Xunliang Cai,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文探讨了统一多模态模型在图像生成与理解上的训练冲突问题，并提出了一种无需模型解耦的新型训练损失AIA，有效提升统一模型的表现。


<details>
  <summary>Details</summary>
Motivation: 统一的多模态模型在实现类AGI目标中具有重要意义，但生成与理解任务间存在目标冲突，常见的模型解耦方法虽然缓解冲突，却损害了统一模型的联动能力。如何在不牺牲联动能力的前提下缓解这种冲突，是当前需要解决的难题。

Method: 作者首先通过分析跨模态注意力行为，发现模型解耦实质是驱动模型学习任务特定的模态交互模式。受此启发，提出了Attention Interaction Alignment (AIA)损失，该方法在训练过程中显式引导模型学习任务特定的模态交互。AIA损失被应用于Emu3与Janus-Pro模型的不同训练阶段，以验证其通用性。

Result: AIA损失无需复杂的设计，即可优化模型的跨模态注意力模式，并同时提升生成与理解两方面的性能。相关实验表明，该方法能在不解耦模型的情况下，有效缓解任务冲突。

Conclusion: 无需模型解耦，通过AIA损失显式学习任务特定模态交互，能够提升统一多模态模型在图像生成与理解任务中的整体性能。这种做法为统一模型的设计与训练提供了新思路。

Abstract: Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.

</details>


### [107] [VaMP: Variational Multi-Modal Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.22664)
*Silin Cheng,Kai Han*

Main category: cs.CV

TL;DR: 该论文提出了一种新的变分多模态提示学习（VaMP）方法，通过为每个样本生成具有不确定性的自适应提示，提升了视觉-语言模型在小样本和域泛化任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态提示学习方法依赖固定共享提示和确定性参数，难以捕捉个体间的差异和任务的不确定性，限制了模型在多样化任务和领域中的适应能力。论文旨在解决上述问题，提高模型泛化和个性化能力。

Method: 论文提出VaMP框架：基于学习到的后验分布为每个输入样本采样生成条件化提示，实现个性化自适应；通过类感知先验结合实例与类别原型，加强局部与全局语义融合；整个训练采用变分推断和可重参数化采样，端到端优化。

Result: 在小样本学习和域泛化基准测试上，VaMP方法取得了当前最优的性能，验证了其在处理任务结构和不确定性建模上的优势。

Conclusion: VaMP能够根据输入内容个性化调整提示，有效提升多模态模型的适应性和泛化能力，特别是在有限监督场景下表现突出。

Abstract: Vision-language models (VLMs), such as CLIP, have shown strong generalization under zero-shot settings, yet adapting them to downstream tasks with limited supervision remains a significant challenge. Existing multi-modal prompt learning methods typically rely on fixed, shared prompts and deterministic parameters, which limits their ability to capture instance-level variation or model uncertainty across diverse tasks and domains. To tackle this issue, we propose a novel Variational Multi-Modal Prompt Learning (VaMP) framework that enables sample-specific, uncertainty-aware prompt tuning in multi-modal representation learning. VaMP generates instance-conditioned prompts by sampling from a learned posterior distribution, allowing the model to personalize its behavior based on input content. To further enhance the integration of local and global semantics, we introduce a class-aware prior derived from the instance representation and class prototype. Building upon these, we formulate prompt tuning as variational inference over latent prompt representations and train the entire framework end-to-end through reparameterized sampling. Experiments on few-shot and domain generalization benchmarks show that VaMP achieves state-of-the-art performance, highlighting the benefits of modeling both uncertainty and task structure in our method. Project page: https://visual-ai.github.io/vamp

</details>


### [108] [A deep learning perspective on Rubens' attribution](https://arxiv.org/abs/2511.22667)
*A. Afifi,A. Kalimullin,S. Korchagin,I. Kudryashov*

Main category: cs.CV

TL;DR: 本研究使用深度学习，针对鲁本斯及其工作坊作品的真伪与归属进行分析，并取得了较高的分类准确度。


<details>
  <summary>Details</summary>
Motivation: 传统艺术鉴定依赖专家经验，但面对作者本人与工作坊成员作品风格极为相似的复杂情况，人工判定非常困难。因此，需要新的技术手段辅助分析与鉴定。

Method: 训练卷积神经网络（CNN）算法，利用经过认证与对比的艺术作品数据集，提取和识别反映大师个人风格的微观特征。

Result: 模型能够较好地区分鲁本斯与工坊成员的画作，实现了较高分类准确率。

Conclusion: 计算机辅助分析技术可作为传统艺术史鉴定的有力补充，对作品归属和作者身份研究具有积极促进作用。

Abstract: This study explores the use of deep learning for the authentication and attribution of paintings, focusing on the complex case of Peter Paul Rubens and his workshop. A convolutional neural network was trained on a curated dataset of verified and comparative artworks to identify micro-level stylistic features characteristic of the master s hand. The model achieved high classification accuracy and demonstrated the potential of computational analysis to complement traditional art historical expertise, offering new insights into authorship and workshop collaboration.

</details>


### [109] [Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield](https://arxiv.org/abs/2511.22677)
*Dongyang Liu,Peng Gao,David Liu,Ruoyi Du,Zhen Li,Qilong Wu,Xin Jin,Sihan Cao,Shifeng Zhang,Hongsheng Li,Steven Hoi*

Main category: cs.CV

TL;DR: 作者重新分析了扩散模型蒸馏（DMD）的效果，发现其性能提升的关键并不是分布匹配，而是cfg增强（CA）部分，重新梳理了两个主要组成部分的分工，并提出改进方法，获得了更优性能。


<details>
  <summary>Details</summary>
Motivation: 尽管当前DMD系列方法在几步生成器中性能领先，但社区普遍认为这种优越性主要得益于分布匹配。然而作者认为这种理解可能不完整，并希望系统性梳理DMD真正有效的驱动因子，从而进一步优化扩散模型蒸馏方法。

Method: 作者通过对DMD训练目标的严格拆解，分析在文本到图像等复杂任务下各组成部分的作用，提出CFG增强（CA）实际上是主驱动力，而分布匹配（DM）仅为正则化器。进一步，该研究提出可以用更简单的约束或GAN目标替代DM，并尝试解耦主驱动力和正则化器的噪声调度。

Result: 作者验证了DM的唯一性并非必须，其他约束同样可以达到稳定训练的目的。通过解耦噪声调度等改进策略，实验获得了更好的性能，相关方法也被Z-Image项目采用，成功开发出顶尖的8步图像生成模型，表明方法有效并具备良好泛化性。

Conclusion: 本研究颠覆了对扩散模型蒸馏机制的传统认知，发现主驱动力其实是CFG增强而非分布匹配，并据此提出了新的分析和改进方法，为今后进一步发展该领域提供了理论和实践基础。

Abstract: Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while the Distribution Matching (DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image ( https://github.com/Tongyi-MAI/Z-Image ) project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.

</details>


### [110] [ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2511.22715)
*Alberto Compagnoni,Marco Morini,Sara Sarto,Federico Cocchi,Davide Caffagni,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出了ReAG方法，通过多阶段检索和推理增强，提升了多模态大模型在知识密集型视觉问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在面对需要领域知识或外部知识支撑的视觉问答时，因预训练数据覆盖有限，表现不佳，亟需利用外部知识资源提升准确性。

Method: 提出了Reasoning-Augmented Multimodal RAG（ReAG），结合粗粒度和细粒度的检索机制，引入评审模型过滤干扰信息，并采用强化学习进行多阶段训练，仅以有监督微调作为冷启动步骤。

Result: 在Encyclopedic-VQA和InfoSeek数据集上，ReAG显著提升了回答准确率，同时生成过程中的推理更具解释性，优势明显超过现有方法。

Conclusion: ReAG有效解决了知识型视觉问答中的检索与推理难题，为多模态模型扩展至知识密集型任务提供了新范式。

Abstract: Multimodal Large Language Models (MLLMs) have shown impressive capabilities in jointly understanding text, images, and videos, often evaluated via Visual Question Answering (VQA). However, even state-of-the-art MLLMs struggle with domain-specific or knowledge-intensive queries, where relevant information is underrepresented in pre-training data. Knowledge-based VQA (KB-VQA) addresses this by retrieving external documents to condition answer generation, but current retrieval-augmented approaches suffer from low precision, noisy passages, and limited reasoning. To address this, we propose ReAG, a novel Reasoning-Augmented Multimodal RAG approach that combines coarse- and fine-grained retrieval with a critic model that filters irrelevant passages, ensuring high-quality additional context. The model follows a multi-stage training strategy leveraging reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start. Extensive experiments on Encyclopedic-VQA and InfoSeek demonstrate that ReAG significantly outperforms prior methods, improving answer accuracy and providing interpretable reasoning grounded in retrieved evidence. Our source code is publicly available at: https://github.com/aimagelab/ReAG.

</details>


### [111] [Emergent Extreme-View Geometry in 3D Foundation Models](https://arxiv.org/abs/2511.22686)
*Yiwen Zhang,Joseph Tung,Ruojin Cai,David Fouhey,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 这篇论文发现和提升了3D基础模型（3DFMs）在极端非重叠视角下的空间理解能力，并提出了轻量的内部对齐方法，显著提升了姿态估计表现；同时还贡献了新的评测基准MegaUnScene。


<details>
  <summary>Details</summary>
Motivation: 虽然3DFMs能很好地实现深度、姿态、点云地图的预测，但其在极端、非重叠视角下的表现未被充分研究。该领域缺乏相关适应方法和专门的测试基准。

Method: 首先分析3DFMs在极端视角下的内部表征，发现其在未针对该任务训练时也表现出几何理解能力。提出一种只调整主干网络少量偏置参数、冻结解码器头的轻量对齐方法，以提升极端视角下的姿态估计表现。此外，构建全新基准MegaUnScene，包含多样网络场景与对应细分任务。

Result: 提出的对齐方法能在不损害单图深度或点云质量的前提下，极大提升极端视角下的相对姿态估计效果。新的MegaUnScene数据集为领域研究提供更具挑战的测试平台。

Conclusion: 3DFMs具备一定的极端视角几何泛化能力。通过小范围有针对性的模型调整，可大幅提升其表现；新的数据基准也将推动该方向发展。

Abstract: 3D foundation models (3DFMs) have recently transformed 3D vision, enabling joint prediction of depths, poses, and point maps directly from images. Yet their ability to reason under extreme, non-overlapping views remains largely unexplored. In this work, we study their internal representations and find that 3DFMs exhibit an emergent understanding of extreme-view geometry, despite never being trained for such conditions. To further enhance these capabilities, we introduce a lightweight alignment scheme that refines their internal 3D representation by tuning only a small subset of backbone bias terms, leaving all decoder heads frozen. This targeted adaptation substantially improves relative pose estimation under extreme viewpoints without degrading per-image depth or point quality. Additionally, we contribute MegaUnScene, a new benchmark of Internet scenes unseen by existing 3DFMs, with dedicated test splits for both relative pose estimation and dense 3D reconstruction. All code and data will be released.

</details>


### [112] [Artwork Interpretation with Vision Language Models: A Case Study on Emotions and Emotion Symbols](https://arxiv.org/abs/2511.22929)
*Sebastian Padó,Kerstin Thomas*

Main category: cs.CV

TL;DR: 本文评估了最新视觉语言模型（VLMs）在分析艺术作品情感表达方面的能力，发现其在识别具象艺术中的情感表现有一定效果，但对抽象和象征性较强的作品表现较差。


<details>
  <summary>Details</summary>
Motivation: 艺术作品中的情感表达复杂多变，需要专业知识深入理解。随着视觉语言模型的发展，探究AI能否理解并解析艺术作品中的情感因素，对AI在艺术和人文领域的应用具有重要意义。

Method: 作者选取了三种VLM模型（Llava-Llama与两种Qwen模型），通过逐步增加难度的问题集（作品内容、情感内容、情感表达方式、情感象征）让模型解读艺术作品，随后由艺术史专家进行定性评估。

Result: 模型在识别具象作品内容与情感表达上表现较好，但对于高度抽象或符号化的艺术作品理解力较弱，且模型很难稳定、准确地识别情感符号。模型还存在对相关问题给出不一致答案的弱点。

Conclusion: 当前VLMs能够一定程度理解并分析艺术作品中的情感表达，尤其是在具象艺术中，但对抽象性、象征性较强的艺术仍力有未逮，符号识别与答案一致性是待突破的难点。

Abstract: Emotions are a fundamental aspect of artistic expression. Due to their abstract nature, there is a broad spectrum of emotion realization in artworks. These are subject to historical change and their analysis requires expertise in art history. In this article, we investigate which aspects of emotional expression can be detected by current (2025) vision language models (VLMs). We present a case study of three VLMs (Llava-Llama and two Qwen models) in which we ask these models four sets of questions of increasing complexity about artworks (general content, emotional content, expression of emotions, and emotion symbols) and carry out a qualitative expert evaluation. We find that the VLMs recognize the content of the images surprisingly well and often also which emotions they depict and how they are expressed. The models perform best for concrete images but fail for highly abstract or highly symbolic images. Reliable recognition of symbols remains fundamentally difficult. Furthermore, the models continue to exhibit the well-known LLM weakness of providing inconsistent answers to related questions.

</details>


### [113] [Ar2Can: An Architect and an Artist Leveraging a Canvas for Multi-Human Generation](https://arxiv.org/abs/2511.22690)
*Shubhankar Borse,Phuc Pham,Farzad Farhadzadeh,Seokeon Choi,Phong Ha Nguyen,Anh Tuan Tran,Sungrack Yun,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: 提出Ar2Can框架，显著提升多人物生成场景中的人数统计准确率与身份保持能力。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成图像模型难以准确生成多人物（如频繁出现人脸重复、身份融合或人数误判等问题），需要解决多人物场景下的合理排布和身份还原难题。

Method: 提出两阶段框架Ar2Can：Architect模块先预测结构化布局指定每个人位置，Artist模块在此指导下合成高保真图像。通过空间配准的人脸匹配奖励（结合匈牙利空间排列与ArcFace身份相似度），确保人脸在正确位置且身份一致。设计了两种Architect变体，与基于扩散的Artist模型集成，使用群体相对策略优化（GRPO）和复合奖励共同优化。

Result: 在MultiHuman-Testbench评测中，Ar2Can在人数准确性和身份还原上都有显著提升，图像质量依然很高。

Conclusion: Ar2Can有效解决了多人物图像生成中的核心难题，仅依靠合成数据也能实现高水平表现，无需真实多人物图像参与训练。

Abstract: Despite recent advances in text-to-image generation, existing models consistently fail to produce reliable multi-human scenes, often duplicating faces, merging identities, or miscounting individuals. We present Ar2Can, a novel two-stage framework that disentangles spatial planning from identity rendering for multi-human generation. The Architect module predicts structured layouts, specifying where each person should appear. The Artist module then synthesizes photorealistic images, guided by a spatially-grounded face matching reward that combines Hungarian spatial alignment with ArcFace identity similarity. This approach ensures faces are rendered at correct locations and faithfully preserve reference identities. We develop two Architect variants, seamlessly integrated with our diffusion-based Artist model and optimized via Group Relative Policy Optimization (GRPO) using compositional rewards for count accuracy, image quality, and identity matching. Evaluated on the MultiHuman-Testbench, Ar2Can achieves substantial improvements in both count accuracy and identity preservation, while maintaining high perceptual quality. Notably, our method achieves these results using primarily synthetic data, without requiring real multi-human images.

</details>


### [114] [Bharat Scene Text: A Novel Comprehensive Dataset and Benchmark for Indian Language Scene Text Understanding](https://arxiv.org/abs/2511.23071)
*Anik De,Abhirama Subramanyam Penamakuri,Rajeev Yadav,Aditya Rathore,Harshiv Shah,Devesh Sharma,Sagar Agarwal,Pravin Kumar,Anand Mishra*

Main category: cs.CV

TL;DR: 本文提出了Bharat Scene Text Dataset (BSTD)，这是一个涵盖11种印度语言和英语的大规模场景文字识别基准数据集，以推动印度语言场景文字识别的发展，并公开了所有模型与数据。


<details>
  <summary>Details</summary>
Motivation: 尽管英文的场景文字识别技术已高度成熟，但印度语言受限于字体多样、书写风格不同及高质量数据集和开源模型的缺乏，相关研究明显落后。该研究旨在弥补这一空白，推动该领域的发展。

Method: 作者构建了规模超过10万词、涵盖11种印度语言和英语的数据集，来源于印度不同语言地区的6,500余张场景图片，并进行了细致标注。该数据集支持场景文字检测、脚本识别、单词识别和端到端识别等多种任务。作者还对英文场景识别的最新模型进行微调，使其适用于印度语言，并进行了评测。

Result: 实验结果表明，将最先进的英文文字识别模型迁移到印度语言后表现存在明显挑战，也揭示了印度语言场景文字识别领域的机遇。

Conclusion: BSTD数据集的发布为印度语言场景文字识别研究提供了坚实基础，推动了该领域的发展，相关模型与数据完全开源，有利于后续学术和工业界进一步探索。

Abstract: Reading scene text, that is, text appearing in images, has numerous application areas, including assistive technology, search, and e-commerce. Although scene text recognition in English has advanced significantly and is often considered nearly a solved problem, Indian language scene text recognition remains an open challenge. This is due to script diversity, non-standard fonts, and varying writing styles, and, more importantly, the lack of high-quality datasets and open-source models. To address these gaps, we introduce the Bharat Scene Text Dataset (BSTD) - a large-scale and comprehensive benchmark for studying Indian Language Scene Text Recognition. It comprises more than 100K words that span 11 Indian languages and English, sourced from over 6,500 scene images captured across various linguistic regions of India. The dataset is meticulously annotated and supports multiple scene text tasks, including: (i) Scene Text Detection, (ii) Script Identification, (iii) Cropped Word Recognition, and (iv) End-to-End Scene Text Recognition. We evaluated state-of-the-art models originally developed for English by adapting (fine-tuning) them for Indian languages. Our results highlight the challenges and opportunities in Indian language scene text recognition. We believe that this dataset represents a significant step toward advancing research in this domain. All our models and data are open source.

</details>


### [115] [Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer](https://arxiv.org/abs/2511.22699)
*Z-Image Team,Huanqia Cai,Sihan Cao,Ruoyi Du,Peng Gao,Steven Hoi,Shijie Huang,Zhaohui Hou,Dengyang Jiang,Xin Jin,Liangchen Li,Zhen Li,Zhong-Yu Li,David Liu,Dongyang Liu,Junhan Shi,Qilong Wu,Feng Yu,Chi Zhang,Shifeng Zhang,Shilin Zhou*

Main category: cs.CV

TL;DR: 本文提出了Z-Image，一种基于6B参数的高效生成式图像模型，能够在远低于主流（20B-80B）规模下达到一流性能，实现低时延推理，并兼容消费级硬件。通过精心的数据、训练优化和蒸馏方案，Z-Image在多个维度上达到或超越现有竞品，支持高质量写实生成和双语文字渲染。代码、模型权重及演示也已开源。


<details>
  <summary>Details</summary>
Motivation: 现有领先的高性能图像生成模型以专有方案为主，开源模型参数规模庞大，难以在普通硬件上部署和微调。因此，急需面向易部署、高效率的开放式图像生成模型，推动AI普及和生态发展。

Method: 作者提出了基于S3-DiT架构的Z-Image模型，共6B参数，系统性地从数据、训练流程到模型蒸馏进行了优化。采用few-step蒸馏加reward后训练，另有全能预训练范式训练辅助的编辑模型Z-Image-Edit。全流程仅耗费314K H800 GPU时，并实现快速推理。

Result: 实验显示，Z-Image无论在写实图像生成还是双语文字渲染方面都达到或超过主流商业与开源模型。Z-Image-Turbo可在企业级GPU上实现亚秒级推理，并能适配16GB显存以内的消费级设备。

Conclusion: Z-Image在牺牲极少计算代价的前提下，实现在多项任务上匹敌甚至超越大规模模型的性能，突破了“唯规模论”，为低成本、易用的生成式AI树立了新标杆。其开源策略有助于推动业界普及和创新。

Abstract: The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the "scale-at-all-costs" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.

</details>


### [116] [Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model Approach](https://arxiv.org/abs/2511.23311)
*Haruki Sakajo,Hiroshi Takato,Hiroshi Tsutsui,Komei Soda,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CV

TL;DR: 本文提出了一种面向驾驶安全的视觉语言大模型（LVLM）应用方法，通过新建数据集和实验验证，发现微调后的LVLM在生成安全驾驶提示方面表现较好，但仍存在识别复杂/细微事件的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前LVLM在基于视觉任务（如目标检测）展现出卓越能力，但自动驾驶等场景需要同时关注路面和驾驶员状态，因此需探索LVLM在多角度视频理解上的表现。

Method: 构建了包含车外（路面）与车内（驾驶员）同步视频的新数据集，评估原始预训练和微调后的LVLM在检测驾驶相关风险事件及生成安全提示方面的性能。

Result: 实验表明，预训练LVLM能力有限，而经过领域微调后，其在生成准确、有安全意识的驾驶指导方面有明显提升，但对于复杂或细微事件的识别仍不理想。

Conclusion: 微调后的LVLM在多视角驾驶安全分析中具有应用潜力，但需进一步提升其对复杂场景的识别能力，本文的分析为后续系统优化提供了方向。

Abstract: Large-scale Vision Language Models (LVLMs) exhibit advanced capabilities in tasks that require visual information, including object detection. These capabilities have promising applications in various industrial domains, such as autonomous driving. For example, LVLMs can generate safety-oriented descriptions of videos captured by road-facing cameras. However, ensuring comprehensive safety requires monitoring driver-facing views as well to detect risky events, such as the use of mobiles while driving. Thus, the ability to process synchronized inputs is necessary from both driver-facing and road-facing cameras. In this study, we develop models and investigate the capabilities of LVLMs by constructing a dataset and evaluating their performance on this dataset. Our experimental results demonstrate that while pre-trained LVLMs have limited effectiveness, fine-tuned LVLMs can generate accurate and safety-aware driving instructions. Nonetheless, several challenges remain, particularly in detecting subtle or complex events in the video. Our findings and error analysis provide valuable insights that can contribute to the improvement of LVLM-based systems in this domain.

</details>


### [117] [Splat-SAP: Feed-Forward Gaussian Splatting for Human-Centered Scene with Scale-Aware Point Map Reconstruction](https://arxiv.org/abs/2511.22704)
*Boyao Zhou,Shunyuan Zheng,Zhanfeng Liao,Zihan Ma,Hanzhang Tu,Boning Liu,Yebin Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Splat-SAP的新方法，能够利用视点稀疏的双目相机输入高效地渲染以人为中心的场景新视图。该方法无需密集视图或严格的几何先验，通过两阶段学习策略结合高斯散点与点图重建，实现高质量、自由视点渲染。


<details>
  <summary>Details</summary>
Motivation: 现有高斯散点渲染(Gaussian Splatting)方法通常需要每个场景单独优化和密集的输入视图，对于输入视角稀疏的情况表现较差。部分新方法引入通过多视角立体获得的几何先验，但依然对视角重叠要求较高。该工作旨在打破这些限制，实现对稀疏双目输入的强鲁棒性渲染。

Method: 方法采用两阶段学习。第一阶段，通过逐像素点图重建与自监督的尺度感知训练，在无需3D监督下获得稳定的点图几何表示，并将其通过亲和性学习映射到真实空间，有利于后续相机控制。第二阶段，将两个输入视图的点图投影到目标视图平面，并通过立体匹配精细调整几何。最终在该精炼平面上锚定高斯基元，完成高质量渲染。

Result: 作者在自建的多视图人体数据集上验证方法，表明该方法能够提升点图重建的稳定性，同时自由视点渲染的视觉质量得到了显著提升。

Conclusion: Splat-SAP方法能够以极少的双目视角输入，在无需完整3D监督与密集视图的情况下，实现高质量、以人为中心的自由视点渲染，并增强了对视图稀疏的鲁棒性。

Abstract: We present Splat-SAP, a feed-forward approach to render novel views of human-centered scenes from binocular cameras with large sparsity. Gaussian Splatting has shown its promising potential in rendering tasks, but it typically necessitates per-scene optimization with dense input views. Although some recent approaches achieve feed-forward Gaussian Splatting rendering through geometry priors obtained by multi-view stereo, such approaches still require largely overlapped input views to establish the geometry prior. To bridge this gap, we leverage pixel-wise point map reconstruction to represent geometry which is robust to large sparsity for its independent view modeling. In general, we propose a two-stage learning strategy. In stage 1, we transform the point map into real space via an iterative affinity learning process, which facilitates camera control in the following. In stage 2, we project point maps of two input views onto the target view plane and refine such geometry via stereo matching. Furthermore, we anchor Gaussian primitives on this refined plane in order to render high-quality images. As a metric representation, the scale-aware point map in stage 1 is trained in a self-supervised manner without 3D supervision and stage 2 is supervised with photo-metric loss. We collect multi-view human-centered data and demonstrate that our method improves both the stability of point map reconstruction and the visual quality of free-viewpoint rendering.

</details>


### [118] [All Centers Are at most a Few Tokens Apart: Knowledge Distillation with Domain Invariant Prompt Tuning](https://arxiv.org/abs/2511.22739)
*Amir Mohammad Ezzati,Alireza Malekhosseini,Armin Khosravi,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 论文提出了一种新的方法（DIPT）用于病理图像领域泛化，使模型能在不同医院/中心的数据域变化下表现更好。该方法通过领域不变的Prompt微调，显著提升了F1分数，优于现有知识蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 在计算病理领域，不同机构间染色、扫描仪等造成较大数据分布偏移，导致模型泛化能力差。虽然vision-language models（如PLIP）表现突出，但其zero-shot性能高度依赖于设计prompt，且在医学领域难以定义合适的语义描述prompt。研究动机是设计一种无需人工特征工程、能够自动学习领域泛化prompt的方法。

Method: 提出Domain Invariant Prompt Tuning (DIPT)：首先为每个数据域学习独立的输入token，随后进行域间平均生成领域不变的prompt，通过这些prompt对student模型进行知识蒸馏。模型对视觉特征进行对齐，使特征嵌入具有领域不变性，提高泛化能力。

Result: 在用于知识蒸馏和领域泛化的病理学数据集上，DIPT方法的平均F1分数大幅优于当前SOTA方法。

Conclusion: DIPT显著提升了计算病理模型在多源异构数据下的泛化能力，为临床落地和实际部署开辟了新方向。

Abstract: Domain generalization is critical in computational pathology (CPath) due to inherent domain shifts caused by variations in staining protocols, scanner devices, and imaging settings across clinical centers. Vision-language models (VLMs), such as PLIP-a pathology-tuned CLIP-trained on image-text pairs across diverse domains, serve as strong knowledge distillation sources. However, their zero-shot performance with predefined prompts remains limited due to sensitivity to prompt variations. Moreover, unlike natural images, histopathology centers lack semantic descriptors (e.g., 'sketch'), making it difficult to define domain-specific prompts for clinical centers. This requires a data-driven approach for learning domain-specific and ultimately class-generic continuous prompts. We propose Domain Invariant Prompt Tuning (DIPT) for knowledge distillation process, a novel step that learns multiple input tokens for each domain. These tokens are trained separately for each domain and are averaged across domains, leading to domain-invariant prompts. Our student model then distills knowledge from PLIP's text encoder by leveraging the prompts learned by DIPT. This leads to alignment of visual features with domain-invariant embeddings, enhancing generalization by training on multiple domains. Our method adds a significant improvement in average F1-score to existing state-of-the-art (SOTA) knowledge distillation approaches in domain generalization with histopathology datasets. This work helps the way of deploying robust CPath models in real-world clinical problems with heterogeneous data sources.

</details>


### [119] [MammoRGB: Dual-View Mammogram Synthesis Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2511.22759)
*Jorge Alberto Garza-Abdala,Gerardo A. Fumagal-González,Daly Avendano,Servando Cardona,Sadam Hussain,Eduardo de Avila-Armenta,Jasiel H. Toscano-Martínez,Diana S. M. Rosales Gurmendi,Alma A. Pedro-Pérez,Jose Gerardo Tamez-Pena*

Main category: cs.CV

TL;DR: 本研究开发并评估了一种三通道去噪扩散概率模型（DDPM）用于合成乳腺双视角（CC和MLO）钼靶影像，并分析不同通道表示对图像质量和一致性的影响。


<details>
  <summary>Details</summary>
Motivation: 乳腺影像数据采集难且隐私问题突出，生成高质量的合成影像有助于数据增强，更好地训练AI模型。如何保证合成乳腺影像在双视角间的结构一致性和真实度是主要挑战。

Method: 使用在Hugging Face上预训练的三通道DDPM，并在私有乳腺影像数据集（11020例）上微调，尝试三种通道编码方式。对合成影像按乳腺轮廓分割IoU和DSC指标进行量化评估，并与真实影像用EMD、KS检验做分布对比；主观评估由非专家放射科医生完成。

Result: 合成影像的IoU与DSC分布与真实影像相似，使用sum或absolute difference编码的模型表现更好（p<0.001），分布也基本一致。生成的CC、MLO影像在一致性方面优良，6-8%合成影像存在与训练集中一致的伪影。

Conclusion: 三通道DDPM能生成结构真实、一致性高的双视角乳腺钼靶影像，对数据扩增具有积极意义。

Abstract: Purpose: This study aims to develop and evaluate a three channel denoising diffusion probabilistic model (DDPM) for synthesizing single breast dual view mammograms and to assess the impact of channel representations on image fidelity and cross view consistency. Materials and Methods: A pretrained three channel DDPM, sourced from Hugging Face, was fine tuned on a private dataset of 11020 screening mammograms to generate paired craniocaudal (CC) and mediolateral oblique (MLO) views. Three third channel encodings of the CC and MLO views were evaluated: sum, absolute difference, and zero channel. Each model produced 500 synthetic image pairs. Quantitative assessment involved breast mask segmentation using Intersection over Union (IoU) and Dice Similarity Coefficient (DSC), with distributional comparisons against 2500 real pairs using Earth Movers Distance (EMD) and Kolmogorov Smirnov (KS) tests. Qualitative evaluation included a visual Turing test by a non expert radiologist to assess cross view consistency and artifacts. Results: Synthetic mammograms showed IoU and DSC distributions comparable to real images, with EMD and KS values (0.020 and 0.077 respectively). Models using sum or absolute difference encodings outperformed others in IoU and DSC (p < 0.001), though distributions remained broadly similar. Generated CC and MLO views maintained cross view consistency, with 6 to 8 percent of synthetic images exhibiting artifacts consistent with those in the training data. Conclusion: Three channel DDPMs can generate realistic and anatomically consistent dual view mammograms with promising applications in dataset augmentation.

</details>


### [120] [Fusion or Confusion? Assessing the impact of visible-thermal image fusion for automated wildlife detection](https://arxiv.org/abs/2511.22768)
*Camille Dionne-Pierre,Samuel Foucher,Jérôme Théau,Jérôme Lemaître,Patrick Charbonneau,Maxime Brousseau,Mathieu Varin*

Main category: cs.CV

TL;DR: 该研究结合可见光和热红外影像，通过深度学习方法，实现对大蓝鹭及其巢的自动检测，并比较了不同影像融合方法，提高了监测准确率。


<details>
  <summary>Details</summary>
Motivation: 野生动物监测对于生物多样性保护至关重要。现有方法存在局限，单一影像源信息有限。结合可见光与热红外影像，有望提升物种检测效果，但影像对齐与融合技术仍具挑战。

Method: 以大蓝鹭为例，收集同步航拍可见光与热红外影像，采用深度学习YOLOv11模型进行自动检测。比较了两种融合方式：早期融合（主成分分析）和后期融合（分类与回归树，整合VIS与TIR模型结果），影像对齐也用深度学习模型自动实现。

Result: 两种融合方法均提升了总体检测F1分数，特别是在‘有鸟巢’主要类别中，后期融合将F1分数从90.2%提升到93.0%，且对伪阳性识别的召回率达90%。

Conclusion: 可见光与热红外影像融合能提高自动化野生动物监测效果，但受限于热红外视场和配准限制。采用机载高分辨率可见光传感器有望更好地支持实际监测工作。

Abstract: Efficient wildlife monitoring methods are necessary for biodiversity conservation and management. The combination of remote sensing, aerial imagery and deep learning offer promising opportunities to renew or improve existing survey methods. The complementary use of visible (VIS) and thermal infrared (TIR) imagery can add information compared to a single-source image and improve results in an automated detection context. However, the alignment and fusion process can be challenging, especially since visible and thermal images usually have different fields of view (FOV) and spatial resolutions. This research presents a case study on the great blue heron (Ardea herodias) to evaluate the performances of synchronous aerial VIS and TIR imagery to automatically detect individuals and nests using a YOLO11n model. Two VIS-TIR fusion methods were tested and compared: an early fusion approach and a late fusion approach, to determine if the addition of the TIR image gives any added value compared to a VIS-only model. VIS and TIR images were automatically aligned using a deep learning model. A principal component analysis fusion method was applied to VIS-TIR image pairs to form the early fusion dataset. A classification and regression tree was used to process the late fusion dataset, based on the detection from the VIS-only and TIR-only trained models. Across all classes, both late and early fusion improved the F1 score compared to the VIS-only model. For the main class, occupied nest, the late fusion improved the F1 score from 90.2 (VIS-only) to 93.0%. This model was also able to identify false positives from both sources with 90% recall. Although fusion methods seem to give better results, this approach comes with a limiting TIR FOV and alignment constraints that eliminate data. Using an aircraft-mounted very high-resolution visible sensor could be an interesting option for operationalizing surveys.

</details>


### [121] [Alzheimer's Disease Prediction Using EffNetViTLoRA and BiLSTM with Multimodal Longitudinal MRI Data](https://arxiv.org/abs/2511.22774)
*Mahdieh Behjat Khatooni,Mohsen Soryani*

Main category: cs.CV

TL;DR: 本文提出了一种多模态深度学习方法，通过结合卷积神经网络（CNN）、视觉Transformer和双向长短时记忆网络（BiLSTM），利用MRI影像及相关生物标志物，有效预测MCI患者向阿尔茨海默病（AD）的转化，取得了95.05%的预测准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病不可逆且尚无有效治愈方法，因此早期预测和干预尤为重要。轻度认知障碍（MCI）是AD的早期阶段，但并非所有MCI患者都会发展为AD。因此，准确区分稳定型MCI（sMCI）与进展型MCI（pMCI）对于早诊早治具有重要意义。

Method: 作者提出了一种端到端的深度学习框架，将CNN和Vision Transformer结合以提取MRI影像中的空间特征，同时利用BiLSTM处理四个连续时间点的MRI及非影像生物标志物，捕捉时间进展信息，从而预测患者48个月后的认知状态。

Result: 该多模态模型在区分sMCI与pMCI方面取得了平均95.05%的预测准确率，超过了以往同类研究的表现。

Conclusion: 本文证实，融合空间和时间建模的多模态方法在AD早期进展预测中极为有效，有助于对阿尔茨海默病的早期发现和干预。

Abstract: Alzheimer's disease (AD) is a prevalent neurodegenerative disorder that progressively impairs memory, decision-making, and overall cognitive function. As AD is irreversible, early prediction is critical for timely intervention and management. Mild Cognitive Impairment (MCI), a transitional stage between cognitively normal (CN) aging and AD, plays a significant role in early AD diagnosis. However, predicting MCI progression remains a significant challenge, as not all individuals with MCI convert to AD. MCI subjects are categorized into stable MCI (sMCI) and progressive MCI (pMCI) based on conversion status. In this study, we propose a generalized, end-to-end deep learning model for AD prediction using MCI cases from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our hybrid architecture integrates Convolutional Neural Networks and Vision Transformers to capture both local spatial features and global contextual dependencies from Magnetic Resonance Imaging (MRI) scans. To incorporate temporal progression, we further employ Bidirectional Long Short-Term Memory (BiLSTM) networks to process features extracted from four consecutive MRI timepoints along with some other non-image biomarkers, predicting each subject's cognitive status at month 48. Our multimodal model achieved an average progression prediction accuracy of 95.05\% between sMCI and pMCI, outperforming existing studies in AD prediction. This work demonstrates state-of-the-art performance in longitudinal AD prediction and highlights the effectiveness of combining spatial and temporal modeling for the early detection of Alzheimer's disease.

</details>


### [122] [World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models](https://arxiv.org/abs/2511.22787)
*Eunsu Kim,Junyeong Park,Na Min An,Junseong Kim,Hitesh Laxmichand Patel,Jiho Jin,Julia Kruk,Amit Agarwal,Srikant Panda,Fenal Ashokbhai Ilasariya,Hyunjung Shim,Alice Oh*

Main category: cs.CV

TL;DR: 本文提出CultureMix，多元文化混合视觉问答基准，评估现有大视觉语言模型（LVLMs）在多文化元素共现场景下的表现，并探讨提升其鲁棒性的方法。


<details>
  <summary>Details</summary>
Motivation: 现实中的视觉场景常涉及多种文化元素混杂，但目前主流LVLMs如何理解和区分这些文化混合尚未深入研究。本研究试图填补LVLMs在多元文化混合场景理解能力方面的空白。

Method: 作者构建了CultureMix基准，包含23000多张由扩散模型生成并经人工验证的食物文化混合图片，覆盖四个子任务（仅食物、食物+食物、食物+背景、食物+食物+背景），用于视觉问答。然后对10个主流LVLMs进行了系统评测，并探索三种增强模型鲁棒性的策略。

Result: 实验发现，现有LVLMs在多文化元素混合场景下普遍难以保持各自文化身份，对背景信息高度依赖，加入文化背景后准确率下降14%，并且同一食物在不同场景下预测不一致。引入多样性文化混合数据集的监督微调后，模型的一致性和鲁棒性均有明显提升。

Conclusion: 当前LVLMs在文化混合场景下存在明显局限，未来需加强对此类场景的模型训练和评测，以提升其在全球多元文化现实环境下的适应性和可用性。

Abstract: In a globalized world, cultural elements from diverse origins frequently appear together within a single visual scene. We refer to these as culture mixing scenarios, yet how Large Vision-Language Models (LVLMs) perceive them remains underexplored. We investigate culture mixing as a critical challenge for LVLMs and examine how current models behave when cultural items from multiple regions appear together. To systematically analyze these behaviors, we construct CultureMix, a food Visual Question Answering (VQA) benchmark with 23k diffusion-generated, human-verified culture mixing images across four subtasks: (1) food-only, (2) food+food, (3) food+background, and (4) food+food+background. Evaluating 10 LVLMs, we find consistent failures to preserve individual cultural identities in mixed settings. Models show strong background reliance, with accuracy dropping 14% when cultural backgrounds are added to food-only baselines, and they produce inconsistent predictions for identical foods across different contexts. To address these limitations, we explore three robustness strategies. We find supervised fine-tuning using a diverse culture mixing dataset substantially improve model consistency and reduce background sensitivity. We call for increased attention to culture mixing scenarios as a critical step toward developing LVLMs capable of operating reliably in culturally diverse real-world environments.

</details>


### [123] [From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images](https://arxiv.org/abs/2511.22805)
*Yiming Chen,Junlin Han,Tianyi Bai,Shengbang Tong,Filippos Kokkinos,Philip Torr*

Main category: cs.CV

TL;DR: 论文提出CogIP-Bench，一个用于评测多模态大模型对主观认知属性（如图片的记忆性、趣味性、美感、情感唤起等）理解能力的基准；发现现有模型在这方面与人类感知存在显著差距，并通过后训练方法有效提升模型与人类判断的一致性，还能将这一能力迁移到下游生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型虽能识别和描述图片内容，但难以理解和预测人类对图片的主观感受（如美感、情感等），这限制了模型在创意和人本导向任务中的应用，因而需要系统性评测和提升。

Method: 1. 构建CogIP-Bench，包含多项反映人类主观认知属性的任务；2. 用该基准系统评测现有MLLMs的表现；3. 引入额外后训练阶段，使模型更好对齐人类主观评判；4. 在图像生成场景中验证该对齐能力可实际提升生成图片的人本特性。

Result: 评测发现现有MLLM在人类认知属性（如美感、趣味性等）上的表现明显落后于人类；引入后训练后，模型与人类判断的对齐度大幅提升，且该认知对齐能力能够迁移至实际的图像生成任务，生成图片更加符合人的主观期望。

Conclusion: CogIP-Bench为多模态大模型的主观认知能力评测提供了标准，认知对齐的后训练方案可显著提升模型的人本感知能力，并且这一能力有实际应用价值，可帮助AI生成更符合人类审美和情感需求的内容。

Abstract: While Multimodal Large Language Models (MLLMs) are adept at answering what is in an image-identifying objects and describing scenes-they often lack the ability to understand how an image feels to a human observer. This gap is most evident when considering subjective cognitive properties, such as what makes an image memorable, funny, aesthetically pleasing, or emotionally evocative. To systematically address this challenge, we introduce CogIP-Bench, a comprehensive benchmark for evaluating MLLMs on such image cognitive properties. Our evaluation reveals a significant gap: current models are poorly aligned with human perception of these nuanced properties. We then demonstrate that a post-training phase can effectively bridge this gap, significantly enhancing the model's alignment with human judgments. Furthermore, we show that this learned cognitive alignment is not merely predictive but also transferable to downstream creative tasks. By integrating our cognitively-aligned MLLM into an image generation pipeline, we can guide the synthesis process to produce images that better embody desired traits, such as being more memorable or visually appealing. Our work provides a benchmark to measure this human-like perception, a post-training pipeline to enhance it, and a demonstration that this alignment unlocks more human-centric AI.

</details>


### [124] [LC4-DViT: Land-cover Creation for Land-cover Classification with Deformable Vision Transformer](https://arxiv.org/abs/2511.22812)
*Kai Wang,Siyi Chen,Weicong Pang,Chenchen Zhang,Renjun Gao,Ziru Chen,Cheng Li,Dasa Gu,Rui Huang,Alexis Kai Hon Lau*

Main category: cs.CV

TL;DR: 本文提出了一种结合生成式数据增强和可变形视觉Transformer的遥感地物分类新方法LC4-DViT，在地物分类任务上显著提升了准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于遥感地物分类常受注释稀缺、不均衡以及高分辨率图像几何畸变的困扰，导致高质量地物图谱制作难度大，亟需一种高效、准确且具泛化能力的解决方案。

Method: 提出LC4-DViT框架，包含（1）利用GPT-4o生成的场景描述和超分辨率样本，通过文本引导扩散模型合成类别均衡的高质量训练图片；（2）采用DCNv4可变形卷积骨干与视觉Transformer编码器结合，既捕获局部细节又考虑全局上下文。

Result: 在AID数据集8个类别上，DViT取得了0.9572的总体准确率和0.9576的宏F1分数，均大幅优于传统ViT和其他主流模型（如ResNet50、MobileNetV2等）；在SIRI-WHU子集上也表现出良好的迁移性。此外，GPT-4o对Grad-CAM热力图的评估显示DViT的注意力机制更契合实际水文结构。

Conclusion: 依托大模型辅助的文本描述驱动生成式扩增与变形感知Transformer的结合，是实现高分辨率地物映射的有效方案，对环境管理和土地规划有重要意义。

Abstract: Land-cover underpins ecosystem services, hydrologic regulation, disaster-risk reduction, and evidence-based land planning; timely, accurate land-cover maps are therefore critical for environmental stewardship. Remote sensing-based land-cover classification offers a scalable route to such maps but is hindered by scarce and imbalanced annotations and by geometric distortions in high-resolution scenes. We propose LC4-DViT (Land-cover Creation for Land-cover Classification with Deformable Vision Transformer), a framework that combines generative data creation with a deformation-aware Vision Transformer. A text-guided diffusion pipeline uses GPT-4o-generated scene descriptions and super-resolved exemplars to synthesize class-balanced, high-fidelity training images, while DViT couples a DCNv4 deformable convolutional backbone with a Vision Transformer encoder to jointly capture fine-scale geometry and global context. On eight classes from the Aerial Image Dataset (AID)-Beach, Bridge, Desert, Forest, Mountain, Pond, Port, and River-DViT achieves 0.9572 overall accuracy, 0.9576 macro F1-score, and 0.9510 Cohen' s Kappa, improving over a vanilla ViT baseline (0.9274 OA, 0.9300 macro F1, 0.9169 Kappa) and outperforming ResNet50, MobileNetV2, and FlashInternImage. Cross-dataset experiments on a three-class SIRI-WHU subset (Harbor, Pond, River) yield 0.9333 overall accuracy, 0.9316 macro F1, and 0.8989 Kappa, indicating good transferability. An LLM-based judge using GPT-4o to score Grad-CAM heatmaps further shows that DViT' s attention aligns best with hydrologically meaningful structures. These results suggest that description-driven generative augmentation combined with deformation-aware transformers is a promising approach for high-resolution land-cover mapping.

</details>


### [125] [Captain Safari: A World Engine](https://arxiv.org/abs/2511.22815)
*Yu-Cheng Chou,Xingrui Wang,Yitong Li,Jiahao Wang,Hanting Liu,Cihang Xie,Alan Yuille,Junfei Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频生成系统 Captain Safari，可根据用户控制的相机轨迹生成长距离、三维一致性强的视频，并引入了新的真实无人机视频数据集 OpenSafari。实验显示新方法在视频质量、三维一致性和路线跟随性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前世界引擎在处理6自由度复杂相机运动及复杂户外布局时，难以保持长期的几何一致性，易出现偏离目标路径或动作过于保守等问题。为解决上述挑战，亟需新的方法在保持三维稳定结构的同时，实现灵活精确的相机控制。

Method: 提出 Captain Safari 系统：利用相机路径维护动态局部记忆，通过检索机制获取与姿态对齐的全局世界记忆内容，并以此作为视频生成的条件，从而支持长时间、高自由度三维一致性的视频生成。构建 FPV 数据集 OpenSafari，采用多阶段的几何与运动学验证保证路径准确无误。

Result: Captain Safari 在多个指标（视频质量、三维一致性、轨迹跟随）均显著优于现有相机控制生成器：如将MEt3R从0.3703降至0.3690，AUC@30由0.181提升至0.200，FVD指标最低。50人主观评测中，67.6%的偏好选择Captain Safari。

Conclusion: 姿态条件化的世界记忆机制能有效提升长时空、可控视频生成质量。Captain Safari展现出较现有方法更优秀的性能，OpenSafari数据集也为相关研究提供了有意义的新基准。

Abstract: World engines aim to synthesize long, 3D-consistent videos that support interactive exploration of a scene under user-controlled camera motion. However, existing systems struggle under aggressive 6-DoF trajectories and complex outdoor layouts: they lose long-range geometric coherence, deviate from the target path, or collapse into overly conservative motion. To this end, we introduce Captain Safari, a pose-conditioned world engine that generates videos by retrieving from a persistent world memory. Given a camera path, our method maintains a dynamic local memory and uses a retriever to fetch pose-aligned world tokens, which then condition video generation along the trajectory. This design enables the model to maintain stable 3D structure while accurately executing challenging camera maneuvers. To evaluate this setting, we curate OpenSafari, a new in-the-wild FPV dataset containing high-dynamic drone videos with verified camera trajectories, constructed through a multi-stage geometric and kinematic validation pipeline. Across video quality, 3D consistency, and trajectory following, Captain Safari substantially outperforms state-of-the-art camera-controlled generators. It reduces MEt3R from 0.3703 to 0.3690, improves AUC@30 from 0.181 to 0.200, and yields substantially lower FVD than all camera-controlled baselines. More importantly, in a 50-participant, 5-way human study where annotators select the best result among five anonymized models, 67.6% of preferences favor our method across all axes. Our results demonstrate that pose-conditioned world memory is a powerful mechanism for long-horizon, controllable video generation and provide OpenSafari as a challenging new benchmark for future world-engine research.

</details>


### [126] [Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs](https://arxiv.org/abs/2511.22826)
*Tianle Chen,Chaitanya Chakka,Arjun Reddy Akula,Xavier Thomas,Deepti Ghadiyaram*

Main category: cs.CV

TL;DR: 本文提出MMA-Bench基准，通过操控音频、视频和文本信息，系统评估现有多模态大模型（MLLMs）在面临模态冲突时的鲁棒性。结果显示，当前模型难以处理模态不一致，易被误导。作者提出了模态对齐调整策略，显著提升了模型在多模态任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然MLLMs发展迅速，但其在输入模态自相矛盾时的表现尚不明确，理解其鲁棒性对于模型落地和实际应用至关重要。

Method: 构建MMA-Bench基准数据集，包含音视频及模态冲突任务，结合黑盒和白盒可解释性分析工具，评估并剖析开源与闭源MLLM的表现。同时，提出模态对齐微调策略以强化模型对多模态信息的合理利用。

Result: 实验发现当前MLLM对于错配的音视频和诱导性文本表现较差，难以实现可靠的多模态推理。经过模态对齐微调后，模型的多模态基础明显增强，抗干扰和泛化能力提升。

Conclusion: 当前MLLM泛化性和鲁棒性不足，需重视跨模态对齐问题。模态对齐调整为增强多模态推理能力提供了有效手段；相关工具和数据集将促进行业更可靠的MLLM发展。

Abstract: Despite remarkable advancements in Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities? To rigorously study this, we introduce MMA-Bench comprising videos and tasks that probe a model's reliance on specific modalities. Using black-box and white-box interpretability techniques, we provide a critical analysis of the brittleness of both open- and closed-sourced MLLMs. We show that current MLLMs struggle under misaligned audio-visual pairs and simple misleading text, thereby lacking robust multi-modal reasoning. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. Through extensive experiments and analysis, we show that our alignment tuning yields demonstrably stronger multimodal grounding. This work provides both interpretability tools and a clear path toward developing MLLMs with intrinsically reliable cross-modal reasoning. Code and dataset will be publicly available.

</details>


### [127] [Breaking the Visual Shortcuts in Multimodal Knowledge-Based Visual Question Answering](https://arxiv.org/abs/2511.22843)
*Dosung Lee,Sangwon Jung,Boyoung Kim,Minyoung Kim,Sungyeon Kim,Junyoung Sung,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: 现有MKB-VQA基准存在“视觉捷径”问题，因此本文提出了新的RETINA基准和MIMIR方法，更好地考察多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 原有的多模态知识型可视问答基准测试中，图片通常和目标文档的主要实体直接关联，模型可以仅凭视觉线索得到答案，未能有效考察知识与多实体推理能力。

Method: 提出RETINA基准，LLM自动生成，包括更复杂的涉及次级实体的查询，并配套相关实体图片，消除视觉捷径。同时提出MIMIR方法，联合多个相关实体的图像增强文档嵌入。

Result: 用于RETINA测试时，现有模型性能大幅下降，证明它们依赖视觉捷径。MIMIR方法显著优于以往只用单图的做法，在RETINA上效果更佳。

Conclusion: RETINA基准有效揭示并消除了视觉捷径，MIMIR方法能够更好适应该情形，推动MKB-VQA领域往更深层次理解和推理发展。

Abstract: Existing Multimodal Knowledge-Based Visual Question Answering (MKB-VQA) benchmarks suffer from "visual shortcuts", as the query image typically matches the primary subject entity of the target document. We demonstrate that models can exploit these shortcuts, achieving comparable results using visual cues alone. To address this, we introduce Relational Entity Text-Image kNowledge Augmented (RETINA) benchmark, automatically constructed using an LLM-driven pipeline, consisting of 120k training and 2k human-curated test set. RETINA contains queries referencing secondary subjects (i.e. related entities) and pairs them with images of these related entities, removing the visual shortcut. When evaluated on RETINA existing models show significantly degraded performance, confirming their reliance on the shortcut. Furthermore, we propose Multi-Image MultImodal Retriever (MIMIR), which enriches document embeddings by augmenting images of multiple related entities, effectively handling RETINA, unlike prior work that uses only a single image per document. Our experiments validate the limitations of existing benchmarks and demonstrate the effectiveness of RETINA and MIMIR. Our project is available at: Project Page.

</details>


### [128] [Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding](https://arxiv.org/abs/2511.22850)
*Keliang Liu,Zizhi Chen,Mingcheng Li,Jingqun Tang,Dingkang Yang,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文提出了SLEUTH多智能体框架，通过分层精细化处理提升了多页、多模态长文档理解任务中的准确性，取得了多项指标上的最新成果。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLM）在处理单页文档时表现良好，但面对长文档时由于信息分散与冗余，效果显著下降。尤其是检索增强生成的方法虽能筛选相关内容，但仍存在大量冗余，因此需要更有效的长文档理解方法。

Method: 提出SLEUTH多智能体框架，包含检索器和四个协作智能体，分层次地从检索到的页面中筛选文本和视觉关键信息，进一步过滤出突出的视觉证据（如表格和图表），并结合查询分析制定推理策略，最终生成高密度证据的多模态上下文用于预测。

Result: SLEUTH为模型无关且可扩展的框架，结合先进的VLM主干模型后，在多个长文档基准测试上显著提升了性能，达到了业界最新水平。消融实验也验证了各模块和层级细化方法的效果。

Conclusion: SLEUTH有效缓解了长文档理解任务中的冗余和信息分散问题，通过多智能体协作与分层细化处理，实现了性能提升，为长文档分析提供了更高效可推广的方案。

Abstract: Document understanding is a long standing practical task. Vision Language Models (VLMs) have gradually become a primary approach in this domain, demonstrating effective performance on single page tasks. However, their effectiveness diminishes when handling long documents. In such scenarios, clues are often scattered across multiple pages and modalities, and redundancy from lengthy inputs can impair the models judgment. While retrieval augmented generation mitigates this issue by filtering for question relevant content, the retrieved results still contain substantial redundancy. To address these limitations, we propose SLEUTH, a multi agent framework. Concretely, SLEUTH orchestrates a retriever and four collaborative agents in a coarse to fine process. The framework identifies key textual and visual clues within the retrieved pages, filters for salient visual evidence such as tables and charts, and analyzes the query to devise a reasoning strategy. It ultimately synthesizes a distilled, evidence dense multimodal context to generate the final prediction. SLEUTH is model agnostic and scalable. When paired with advanced VLM backbones, it consistently improves performance on multiple long document benchmarks, achieving state of the art results. Ablation studies verify each modules effectiveness and confirm the benefits of our hierarchical refinement paradigm.

</details>


### [129] [GLOW: Global Illumination-Aware Inverse Rendering of Indoor Scenes Captured with Dynamic Co-Located Light & Camera](https://arxiv.org/abs/2511.22857)
*Jiaye Wu,Saeed Hadadan,Geng Lin,Peihan Tu,Matthias Zwicker,David Jacobs,Roni Sengupta*

Main category: cs.CV

TL;DR: 本文提出了GLOW框架，通过全局光照感知的逆向渲染方法，在复杂室内场景下更准确地推断物体的几何与反射属性，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 室内逆向渲染面临反射率与光照间的歧义，尤其在存在多物体间互反射时更为复杂。尽管采用共址光源与相机可以更好地分辨出光照与物体属性，但也带来了强互反射、动态阴影、近场照明与高光移动等新的难题。

Method: 提出GLOW框架，将神经隐式表面表示与神经辐射缓存结合，近似建模全球光照，并引入动态辐射缓存来适应近场移动造成的光照突变，还设计了基于表面角度加权的辐射计损失以减少高光伪影，通过联合优化几何与反射属性，以及合适的正则与初始化实现稳健训练。

Result: GLOW在天然光和共址照明环境下，在材料反射率估算上效果明显优于先前方法，实验验证了其在处理复杂光照与阴影场景中的先进性。

Conclusion: GLOW有效解决了室内场景逆向渲染中由复杂光照与多重互反射导致的难题，并为未来室内逆向渲染方法提供了更高的准确性与鲁棒性。

Abstract: Inverse rendering of indoor scenes remains challenging due to the ambiguity between reflectance and lighting, exacerbated by inter-reflections among multiple objects. While natural illumination-based methods struggle to resolve this ambiguity, co-located light-camera setups offer better disentanglement as lighting can be easily calibrated via Structure-from-Motion. However, such setups introduce additional complexities like strong inter-reflections, dynamic shadows, near-field lighting, and moving specular highlights, which existing approaches fail to handle. We present GLOW, a Global Illumination-aware Inverse Rendering framework designed to address these challenges. GLOW integrates a neural implicit surface representation with a neural radiance cache to approximate global illumination, jointly optimizing geometry and reflectance through carefully designed regularization and initialization. We then introduce a dynamic radiance cache that adapts to sharp lighting discontinuities from near-field motion, and a surface-angle-weighted radiometric loss to suppress specular artifacts common in flashlight captures. Experiments show that GLOW substantially outperforms prior methods in material reflectance estimation under both natural and co-located illumination.

</details>


### [130] [CoordSpeaker: Exploiting Gesture Captioning for Coordinated Caption-Empowered Co-Speech Gesture Generation](https://arxiv.org/abs/2511.22863)
*Fengyi Fang,Sicheng Yang,Wenming Yang*

Main category: cs.CV

TL;DR: 本文提出了一种用于联合语音-手势生成的新框架（CoordSpeaker），能够生成与语音同步且语义自洽的手势动作，并解决了以往方法难以融合文本语义和多模态控制的问题。


<details>
  <summary>Details</summary>
Motivation: 目前手势生成领域主要处理自然、随意的动作，缺乏对由文本驱动或具有特定语义（如鞠躬等非自发性手势）的动作为基础的研究，主要由于缺少动作数据集中的文本描述标签以及难以实现多模态精细协同控制。

Method: 作者提出CoordSpeaker框架：首先通过创新的动作描述（gesture captioning）方案，利用动作-语言模型为动作生成多粒度的描述文本，弥补动作数据集缺少文本标签的问题；其次，设计条件潜变量扩散模型和分层控制的去噪器，实现统一跨数据集的动作表示和高可控、协调的手势生成。

Result: 方法可以生成与语音节奏同步并与任意文本语义高度匹配的高质量手势，实验显示在生成质量与效率上均超过现有方法。

Conclusion: CoordSpeaker首次从理解和生成手势描述文本角度填补了手势生成的语义鸿沟，同时为多模态、语义增强的可控手势生成提供了新方案，并在多个评测上取得了更优的表现。

Abstract: Co-speech gesture generation has significantly advanced human-computer interaction, yet speaker movements remain constrained due to the omission of text-driven non-spontaneous gestures (e.g., bowing while talking). Existing methods face two key challenges: 1) the semantic prior gap due to the lack of descriptive text annotations in gesture datasets, and 2) the difficulty in achieving coordinated multimodal control over gesture generation. To address these challenges, this paper introduces CoordSpeaker, a comprehensive framework that enables coordinated caption-empowered co-speech gesture synthesis. Our approach first bridges the semantic prior gap through a novel gesture captioning framework, leveraging a motion-language model to generate descriptive captions at multiple granularities. Building upon this, we propose a conditional latent diffusion model with unified cross-dataset motion representation and a hierarchically controlled denoiser to achieve highly controlled, coordinated gesture generation. CoordSpeaker pioneers the first exploration of gesture understanding and captioning to tackle the semantic gap in gesture generation while offering a novel perspective of bidirectional gesture-text mapping. Extensive experiments demonstrate that our method produces high-quality gestures that are both rhythmically synchronized with speeches and semantically coherent with arbitrary captions, achieving superior performance with higher efficiency compared to existing approaches.

</details>


### [131] [Scalable Diffusion Transformer for Conditional 4D fMRI Synthesis](https://arxiv.org/abs/2511.22870)
*Jungwoo Seo,David Keetae Park,Shinjae Yoo,Jiook Cha*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散变换器的生成模型，可以结合3D VQ-GAN、CNN-Transformer骨干结构与强任务条件，首次实现了以认知任务为条件的全脑4D fMRI数据生成。该方法在多项神经影像指标上超过传统U-Net基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于认知任务条件的4D fMRI生成由于BOLD信号的高维异质性及缺乏神经科学可验证性，面临巨大挑战，限制了虚拟实验、跨站点数据统一等应用。

Method: 作者提出将3D VQ-GAN用于fMRI数据的潜变量压缩，并搭建CNN-Transformer结合AdaLN-Zero与交叉注意力进行强任务条件建模，利用潜空间扩散模型进行4D fMRI生成。

Result: 模型可重现任务激活图、保持真实数据中的任务间表示结构，条件特异性完美，ROI时序与标准神经反应高度一致，在各指标上均优于U-Net，并随规模提升性能线性提升。

Conclusion: 扩散变换器结合可伸缩骨干与强条件机制，为条件4D fMRI合成提供有效路径，有望支持虚拟实验、数据统一与后续神经影像模型的数据增强等应用。

Abstract: Generating whole-brain 4D fMRI sequences conditioned on cognitive tasks remains challenging due to the high-dimensional, heterogeneous BOLD dynamics across subjects/acquisitions and the lack of neuroscience-grounded validation. We introduce the first diffusion transformer for voxelwise 4D fMRI conditional generation, combining 3D VQ-GAN latent compression with a CNN-Transformer backbone and strong task conditioning via AdaLN-Zero and cross-attention. On HCP task fMRI, our model reproduces task-evoked activation maps, preserves the inter-task representational structure observed in real data (RSA), achieves perfect condition specificity, and aligns ROI time-courses with canonical hemodynamic responses. Performance improves predictably with scale, reaching task-evoked map correlation of 0.83 and RSA of 0.98, consistently surpassing a U-Net baseline on all metrics. By coupling latent diffusion with a scalable backbone and strong conditioning, this work establishes a practical path to conditional 4D fMRI synthesis, paving the way for future applications such as virtual experiments, cross-site harmonization, and principled augmentation for downstream neuroimaging models.

</details>


### [132] [CNN-Based Framework for Pedestrian Age and Gender Classification Using Far-View Surveillance in Mixed-Traffic Intersections](https://arxiv.org/abs/2511.22873)
*Shisir Shahriar Arif,Md. Muhtashim Shahrier,Nazmul Haque,Md Asif Raihan,Md. Hadiuzzaman*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的远距离监控下行人年龄和性别识别的新框架，通过全身特征（非人脸识别）分类，并可在普通监控摄像头下实现实时监测，为交通安全和城市管理提供支持。


<details>
  <summary>Details</summary>
Motivation: 在基础设施薄弱、交通模式复杂的发展中国家，行人安全令人担忧，不同年龄和性别人群的风险不同，但现有实时监测系统很少关注行人细分人口学特征，因而限制了数据驱动的精准干预措施。

Method: 采集达卡市3个高危路口的远摄监控视频，主要采用ResNet50与自定义轻量级CNN，基于全身视觉特征（非人脸），将行人分成6类（成人/青少年/儿童×男/女）。共测试8组模型，变换池化策略及优化器。

Result: ResNet50搭配Max Pooling与SGD获得最高准确率86.19%，自定义轻量化CNN也取得了84.15%的表现，模型参数更少、训练更快，均可在常规监控视频实现实时推断。

Conclusion: 该框架可用作城市交通管理的高效低成本工具，依托现有监控设施，无需高分辨率图像及人脸识别，有助于采集实时行人人口学信息，优化路口设计、信号策划并实施针对性的安全干预，为混合交通环境提供包容性、数据化的管理决策。

Abstract: Pedestrian safety remains a pressing concern in congested urban intersections, particularly in low- and middle-income countries where traffic is multimodal, and infrastructure often lacks formal control. Demographic factors like age and gender significantly influence pedestrian vulnerability, yet real-time monitoring systems rarely capture this information. To address this gap, this study proposes a deep learning framework that classifies pedestrian age group and gender from far-view intersection footage using convolutional neural networks (CNNs), without relying on facial recognition or high-resolution imagery. The classification is structured as a unified six-class problem, distinguishing adult, teenager, and child pedestrians for both males and females, based on full-body visual cues. Video data was collected from three high-risk intersections in Dhaka, Bangladesh. Two CNN architectures were implemented: ResNet50, a deep convolutional neural network pretrained on ImageNet, and a custom lightweight CNN optimized for computational efficiency. Eight model variants explored combinations of pooling strategies and optimizers. ResNet50 with Max Pooling and SGD achieved the highest accuracy (86.19%), while the custom CNN performed comparably (84.15%) with fewer parameters and faster training. The model's efficient design enables real-time inference on standard surveillance feeds. For practitioners, this system provides a scalable, cost-effective tool to monitor pedestrian demographics at intersections using existing camera infrastructure. Its outputs can shape intersection design, optimize signal timing, and enable targeted safety interventions for vulnerable groups such as children or the elderly. By offering demographic insights often missing in conventional traffic data, the framework supports more inclusive, data-driven planning in mixed-traffic environments.

</details>


### [133] [ClearGCD: Mitigating Shortcut Learning For Robust Generalized Category Discovery](https://arxiv.org/abs/2511.22892)
*Kailin Lyu,Jianwei He,Long Xiao,Jianing Zeng,Liang Fan,Lin Shu,Jie Hao*

Main category: cs.CV

TL;DR: 本文提出了一种新的GCD（通用类别发现）方法ClearGCD，有效缓解了原有方法中的原型混淆与遗忘已知类别问题，通过语义视图对齐和捷径抑制机制，在多个基准数据集上取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: GCD任务需在无标签数据中识别已知和新颖类别，但现有方法易依赖非语义特征（即“捷径学习”），导致原型混淆、泛化能力下降及已知类别遗忘。

Method: 提出ClearGCD框架，包括两部分：1）语义视图对齐(SVA)，利用跨类别patch替换进行强数据增强，并通过弱增强保持语义一致性；2）捷径抑制正则化(SSR)，构建自适应原型库来对齐已知类别原型，同时促使新类别原型分离。该框架可无缝应用到现有GCD参数化方法中。

Result: ClearGCD在多个基准测试上优于现有最优方法，显示了更强的类别发现和泛化能力。

Conclusion: ClearGCD有效缓解了GCD中的捷径学习问题，提升了已知与新类别的发现与区分能力，具有良好的泛化性和实际应用潜力。

Abstract: In open-world scenarios, Generalized Category Discovery (GCD) requires identifying both known and novel categories within unlabeled data. However, existing methods often suffer from prototype confusion caused by shortcut learning, which undermines generalization and leads to forgetting of known classes. We propose ClearGCD, a framework designed to mitigate reliance on non-semantic cues through two complementary mechanisms. First, Semantic View Alignment (SVA) generates strong augmentations via cross-class patch replacement and enforces semantic consistency using weak augmentations. Second, Shortcut Suppression Regularization (SSR) maintains an adaptive prototype bank that aligns known classes while encouraging separation of potential novel ones. ClearGCD can be seamlessly integrated into parametric GCD approaches and consistently outperforms state-of-the-art methods across multiple benchmarks.

</details>


### [134] [DM$^3$T: Harmonizing Modalities via Diffusion for Multi-Object Tracking](https://arxiv.org/abs/2511.22896)
*Weiran Li,Yeqiang Liu,Yijie Wei,Mina Han,Qiannan Guo,Zhenbo Li*

Main category: cs.CV

TL;DR: 本文提出DM$^3$T框架，将多模态多目标跟踪（MOT）建模为迭代式特征对齐过程，针对可见光与红外信息的高效融合，显著提升了跟踪准确性。实验结果优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 多模态MOT（包含可见光和红外信息）对自动驾驶等领域极为关键，但现有的模态融合策略难以克服两个模态特征间的强非线性分布差异，导致融合效果不佳，影响跟踪性能。

Method: 借鉴扩散模型的思想，提出迭代式特征对齐框架DM$^3$T，包括跨模态扩散融合模块（C-MDF）和可插拔的特征精炼模块（DR），通过多步交互对齐两种模态特征。此外，提出分层式跟踪器自适应处理置信度估计，实现端到端、无需复杂后处理的在线跟踪方案。

Result: 在VT-MOT基准上进行了大量实验，测得HOTA指标为41.7，相较现有最先进方法提升1.54%，显示出更优表现。

Conclusion: DM$^3$T有效提升了多模态MOT的精度和鲁棒性，验证了迭代式特征对齐和深度融合策略的有效性。该方法具有较高实用价值和推广潜力。

Abstract: Multi-object tracking (MOT) is a fundamental task in computer vision with critical applications in autonomous driving and robotics. Multimodal MOT that integrates visible light and thermal infrared information is particularly essential for robust autonomous driving systems. However, effectively fusing these heterogeneous modalities is challenging. Simple strategies like concatenation or addition often fail to bridge the significant non-linear distribution gap between their feature representations, which can lead to modality conflicts and degrade tracking accuracy. Drawing inspiration from the connection between multimodal MOT and the iterative refinement in diffusion models, this paper proposes DM$^3$T, a novel framework that reformulates multimodal fusion as an iterative feature alignment process to generate accurate and temporally coherent object trajectories. Our approach performs iterative cross-modal harmonization through a proposed Cross-Modal Diffusion Fusion (C-MDF) module. In this process, features from both modalities provide mutual guidance, iteratively projecting them onto a shared, consistent feature manifold. This enables the learning of complementary information and achieves deeper fusion compared to conventional methods. Additionally, we introduce a plug-and-play Diffusion Refiner (DR) to enhance and refine the unified feature representation. To further improve tracking robustness, we design a Hierarchical Tracker that adaptively handles confidence estimation. DM$^3$T unifies object detection, state estimation, and data association into a comprehensive online tracking framework without complex post-processing. Extensive experiments on the VT-MOT benchmark demonstrate that our method achieves 41.7 HOTA, representing a 1.54% relative improvement over existing state-of-the-art methods. The code and models are available at https://vranlee.github.io/DM-3-T/.

</details>


### [135] [From Points to Clouds: Learning Robust Semantic Distributions for Multi-modal Prompts](https://arxiv.org/abs/2511.22897)
*Weiran Li,Yeqiang Liu,Yijie Wei,Mina Han,Xin Liu,Zhenbo Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态提示学习（MPL）框架Points-to-Clouds（P2C），通过采用扩散模型思想，将提示学习从静态点表示扩展为分布式“语义云”，显著提升了视觉语言模型（VLMs）的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态提示学习方法以单点静态表示为核心，易于对基础类别过拟合，对于新颖或模糊类别的泛化能力有限，缺乏鲁棒性。为解决这一问题，论文提出改用分布式的“语义云”表示。

Method: 方法核心是P2C框架，借鉴扩散模型，将提示学习视为动态去噪任务，具体由动态提示去噪（DPD）机制和辅助视觉-语言映射去噪器组成：DPD机制向文字提示加入渐进式噪声，使模型学习更平滑的语义分布；V-L Mapper去噪则让映射器作为去噪自编码器，将带噪声的文字输入重构为干净的视觉提示，增强跨模态对齐能力。

Result: 在11个数据集上，P2C均优于强有力的Baseline。特别是在Base-to-Novel泛化评测中，实现了79.7%的调和均值，比Baseline提升1.4个百分点。

Conclusion: P2C方法通过引入“语义云”分布和去噪机制，有效提升了多模态提示泛化及鲁棒性，对大规模视觉语言模型的适应能力具有显著推动作用。

Abstract: Multimodal Prompt Learning (MPL) has emerged as a pivotal technique for adapting large-scale Visual Language Models (VLMs). However, current MPL methods are fundamentally limited by their optimization of a single, static point representation. This paradigm is inherently brittle, leads to overfitting on base classes, and generalizes poorly to novel or ambiguous categories. We challenge this point paradigm, proposing that robust generalization requires learning a semantic cloud (i.e., a distribution over the embedding space). To achieve this, we introduce Points-to-Clouds (P2C), a novel framework inspired by diffusion models that reframes prompt learning as a dynamic denoising task. At the core of P2C is a dual denoising mechanism: a Dynamic Prompt Denoising (DPD) mechanism perturbs text prompts with sophisticated, annealed noise to learn a smoother semantic landscape, while an auxiliary V-L Mapper denoising loss re-tasks the mapper as a denoising autoencoder. This forces the mapper to reconstruct clean visual prompts from noisy text inputs, ensuring robust cross-modal alignment. Extensive experiments across 11 datasets demonstrate that P2C consistently outperforms strong baselines. On the base-to-novel generalization benchmark, our method achieves a Harmonic Mean of 79.7%, representing a relative improvement of 1.4% over the baseline. The code and models are available at https://vranlee.github.io/P2C/.

</details>


### [136] [Leveraging Textual Compositional Reasoning for Robust Change Captioning](https://arxiv.org/abs/2511.22903)
*Kyu Ri Park,Jiyoung Park,Seong Tae Kim,Hong Joo Lee,Jung Uk Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种结合视觉与文本信息的变化描述方法CORTEX，提升了对细微和复杂图片变化的检测与描述能力。


<details>
  <summary>Details</summary>
Motivation: 现有的图片变化描述方法主要依赖于视觉特征，难以识别那些细微但重要的变化，因为它们缺少对结构化信息（如对象关系和组合语义）的显式表示能力。作者旨在通过引入文本信息增强对图片变化的理解和推理。

Method: 作者提出了CORTEX框架，利用视觉-语言模型（VLM）的场景级文本知识丰富视觉信号。CORTEX包括三大模块：(1)图片级变化检测器，负责发现低层次视觉差异；(2)基于VLM的推理文本提取模块，生成反映组合推理的描述；(3)视觉-文本双对齐模块，实现多模态特征的细粒度推理对齐，从而辅助图片变化的检测与理解。

Result: 通过将视觉和丰富的文本特征结合，CORTEX能更好地检测并描述图片间微妙和复杂的变化，在变化描述任务上显著优于仅用视觉特征的方法。

Conclusion: CORTEX展示了将视觉与语言模型结合在图片变化描述任务中的有效性，能捕捉仅用视觉难以识别的细微变化，为变化描述提供了更强大的理解和推理能力。

Abstract: Change captioning aims to describe changes between a pair of images. However, existing works rely on visual features alone, which often fail to capture subtle but meaningful changes because they lack the ability to represent explicitly structured information such as object relationships and compositional semantics. To alleviate this, we present CORTEX (COmpositional Reasoning-aware TEXt-guided), a novel framework that integrates complementary textual cues to enhance change understanding. In addition to capturing cues from pixel-level differences, CORTEX utilizes scene-level textual knowledge provided by Vision Language Models (VLMs) to extract richer image text signals that reveal underlying compositional reasoning. CORTEX consists of three key modules: (i) an Image-level Change Detector that identifies low-level visual differences between paired images, (ii) a Reasoning-aware Text Extraction (RTE) module that use VLMs to generate compositional reasoning descriptions implicit in visual features, and (iii) an Image-Text Dual Alignment (ITDA) module that aligns visual and textual features for fine-grained relational reasoning. This enables CORTEX to reason over visual and textual features and capture changes that are otherwise ambiguous in visual features alone.

</details>


### [137] [See, Rank, and Filter: Important Word-Aware Clip Filtering via Scene Understanding for Moment Retrieval and Highlight Detection](https://arxiv.org/abs/2511.22906)
*YuEun Lee,Jung Uk Kim*

Main category: cs.CV

TL;DR: 本文提出了一种通过识别和优先重要关键词实现更细粒度视频片段筛选的新方法，有效提升了基于自然语言查询的视频时刻检索（MR）和亮点检测（HD）任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视频时刻检索和亮点检测方法普遍将查询文本和视频片段整体视为黑箱，忽略了查询中个别词语的重要性。这种做法限制了模型对于文本与视频内容的细致语境理解，影响了检索和检测的精度。

Method: 作者设计了一种结合多模态大语言模型（MLLM）的新框架：提出特征增强模块（FEM）用于挖掘自然语言查询中重要词语，并利用基于排序的筛选模块（RFM）依据重要词语相关性，迭代优化视频片段筛选。整体方法加强了图像-文本场景理解，提升了视频语义建模能力。

Result: 实验结果显示，所提方法在视频时刻检索（MR）和亮点检测（HD）两个任务上均显著优于现有最新方法，取得了更高的性能指标。

Conclusion: 通过重视查询中重要词语，并引入多模态特征增强和分步筛选机制，显著提升了基于自然语言的视频内容检索和亮点检测能力，为相关多模态视觉理解任务提供了新思路。

Abstract: Video moment retrieval (MR) and highlight detection (HD) with natural language queries aim to localize relevant moments and key highlights in a video clips. However, existing methods overlook the importance of individual words, treating the entire text query and video clips as a black-box, which hinders contextual understanding. In this paper, we propose a novel approach that enables fine-grained clip filtering by identifying and prioritizing important words in the query. Our method integrates image-text scene understanding through Multimodal Large Language Models (MLLMs) and enhances the semantic understanding of video clips. We introduce a feature enhancement module (FEM) to capture important words from the query and a ranking-based filtering module (RFM) to iteratively refine video clips based on their relevance to these important words. Extensive experiments demonstrate that our approach significantly outperforms existing state-of-the-art methods, achieving superior performance in both MR and HD tasks. Our code is available at: https://github.com/VisualAIKHU/SRF.

</details>


### [138] [ViGG: Robust RGB-D Point Cloud Registration using Visual-Geometric Mutual Guidance](https://arxiv.org/abs/2511.22908)
*Congjia Chen,Shen Yan,Yufu Qu*

Main category: cs.CV

TL;DR: 提出了一种利用视觉-几何互导机制的RGB-D点云配准新方法ViGG，增强了对图像信息的利用，提升了对噪声和歧义的鲁棒性，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D点云配准方法主要侧重特征融合或特征学习，忽略了图像信息的深度利用，导致实际应用效果有限。作者希望增强配准的鲁棒性并更好利用图像与几何信息。

Method: 提出ViGG方法，采用视觉-几何互导策略：（1）利用几何指导抑制视觉中歧义的团簇配准结果；（2）用视觉先验限定几何配准的搜索空间，提高对应点质量并降低噪声影响。

Result: 在3DMatch、ScanNet和KITTI等多个公开RGB-D数据集上，无论是在无学习和基于学习的方法下，ViGG都超过了当前主流方法。

Conclusion: ViGG方法通过视觉-几何互导机制显著提升了RGB-D配准鲁棒性和泛化能力，具有较强实际应用潜力。

Abstract: Point cloud registration is a fundamental task in 3D vision. Most existing methods only use geometric information for registration. Recently proposed RGB-D registration methods primarily focus on feature fusion or improving feature learning, which limits their ability to exploit image information and hinders their practical applicability. In this paper, we propose ViGG, a robust RGB-D registration method using mutual guidance. First, we solve clique alignment in a visual-geometric combination form, employing a geometric guidance design to suppress ambiguous cliques. Second, to mitigate accuracy degradation caused by noise in visual matches, we propose a visual-guided geometric matching method that utilizes visual priors to determine the search space, enabling the extraction of high-quality, noise-insensitive correspondences. This mutual guidance strategy brings our method superior robustness, making it applicable for various RGB-D registration tasks. The experiments on 3DMatch, ScanNet and KITTI datasets show that our method outperforms recent state-of-the-art methods in both learning-free and learning-based settings. Code is available at https://github.com/ccjccjccj/ViGG.

</details>


### [139] [NeuMatC: A General Neural Framework for Fast Parametric Matrix Operation](https://arxiv.org/abs/2511.22934)
*Chuan Wang,Xi-le Zhao,Zhilong Han,Liang Li,Deyu Meng,Michael K. Ng*

Main category: cs.CV

TL;DR: 该论文提出了一种高效执行参数化矩阵运算的新框架（NeuMatC），相比传统方法大幅减少重复计算，实现了多倍加速。


<details>
  <summary>Details</summary>
Motivation: 在实际应用如无线通信和信号处理中，常需对参数连续变化的矩阵反复执行矩阵运算（如逆与SVD）；而传统方法未充分利用参数维度下的低秩和连续性，导致计算冗余巨大。

Method: 提出NeuMatC，通过无监督学习参数到对应矩阵运算结果的低秩连续映射，训练后可用简单操作（如矩阵乘法和非线性激活）高效获得任意参数下的结果。

Result: 在合成数据与真实世界数据上，NeuMatC在参数化逆和SVD任务中，比NumPy基线实现了3倍及10倍以上的加速，且保持合理精度。

Conclusion: NeuMatC框架能充分利用参数维度信息，显著提升参数化矩阵运算效率，在实际应用中具有很高的实用价值。

Abstract: Matrix operations (e.g., inversion and singular value decomposition (SVD)) are fundamental in science and engineering. In many emerging real-world applications (such as wireless communication and signal processing), these operations must be performed repeatedly over matrices with parameters varying continuously. However, conventional methods tackle each matrix operation independently, underexploring the inherent low-rankness and continuity along the parameter dimension, resulting in significantly redundant computation. To address this challenge, we propose \textbf{\textit{Neural Matrix Computation Framework} (NeuMatC)}, which elegantly tackles general parametric matrix operation tasks by leveraging the underlying low-rankness and continuity along the parameter dimension. Specifically, NeuMatC unsupervisedly learns a low-rank and continuous mapping from parameters to their corresponding matrix operation results. Once trained, NeuMatC enables efficient computations at arbitrary parameters using only a few basic operations (e.g., matrix multiplications and nonlinear activations), significantly reducing redundant computations. Experimental results on both synthetic and real-world datasets demonstrate the promising performance of NeuMatC, exemplified by over $3\times$ speedup in parametric inversion and $10\times$ speedup in parametric SVD compared to the widely used NumPy baseline in wireless communication, while maintaining acceptable accuracy.

</details>


### [140] [Robust Image Self-Recovery against Tampering using Watermark Generation with Pixel Shuffling](https://arxiv.org/abs/2511.22936)
*Minyoung Kim,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: 该论文提出了一种基于神经水印的新型图像自恢复方法ReImage，有效提升了篡改区域的恢复质量，实现了对被篡改图像的高质量还原。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容（AIGC）激增，数字媒体真实性受到质疑。为了理解篡改行为并重构可信数据，图像自恢复成为重要手段。但现有方法在恢复被篡改区域的准确性上表现不足。

Method: 作者提出ReImage方法：将原始图像的打乱版本作为水印嵌入自身，采用生成器优化水印嵌入，并引入图像增强模块提升恢复图像质量。同时，分析并改进了打乱水印方法在自恢复中的局限性。

Result: 在多种篡改场景下，ReImage方法均表现出色，实现了业界领先的图像恢复质量。实验表明，该方法能够稳定产生高质量的恢复图像。

Conclusion: ReImage通过创新的神经水印机制，显著提升了图像自恢复性能。相关代码和模型将在论文发表时公开，有望为数字媒体的可信性恢复提供有效工具。

Abstract: The rapid growth of Artificial Intelligence-Generated Content (AIGC) raises concerns about the authenticity of digital media. In this context, image self-recovery, reconstructing original content from its manipulated version, offers a practical solution for understanding the attacker's intent and restoring trustworthy data. However, existing methods often fail to accurately recover tampered regions, falling short of the primary goal of self-recovery. To address this challenge, we propose ReImage, a neural watermarking-based self-recovery framework that embeds a shuffled version of the target image into itself as a watermark. We design a generator that produces watermarks optimized for neural watermarking and introduce an image enhancement module to refine the recovered image. We further analyze and resolve key limitations of shuffled watermarking, enabling its effective use in self-recovery. We demonstrate that ReImage achieves state-of-the-art performance across diverse tampering scenarios, consistently producing high-quality recovered images. The code and pretrained models will be released upon publication.

</details>


### [141] [Barcode and QR Code Object Detection: An Experimental Study on YOLOv8 Models](https://arxiv.org/abs/2511.22937)
*Kushagra Pandya,Heli Hathi,Het Buch,Ravikumar R N,Shailendrasinh Chauhan,Sushil Kumar Singh*

Main category: cs.CV

TL;DR: 本文深入评估了YOLOv8算法在条码和二维码检测任务中的表现，通过多种模型版本提升检测准确性，展示了模型缩放对于目标识别效果的显著提升。


<details>
  <summary>Details</summary>
Motivation: 条码和二维码检测广泛应用于现实生活中，对其检测算法的高效性和准确性有较高要求。作者希望借助最新的YOLOv8及其各版本，提升实时检测能力，并探究模型缩放对性能的影响。

Method: 作者使用了经过挑选的Kaggle条码和二维码数据集，对YOLOv8的Nano、Small和Medium三个版本进行了大规模训练和精细调优。采用精度、召回率和F1分数等指标系统评估模型性能。

Result: YOLOv8各版本在检测精度上均有提升，Nano模型准确率达到88.95%，Small模型为97.10%，Medium模型为94.10%。结果显示，不同版本之间通过缩放模型结构取得了性能渐进性提升。

Conclusion: 研究表明，YOLOv8在物体检测尤其是条码、二维码识别任务上表现优异，模型缩放对实际效果有明显正向影响，进一步巩固了YOLOv8在计算机视觉领域的重要地位。

Abstract: This research work dives into an in-depth evaluation of the YOLOv8 (You Only Look Once) algorithm's efficiency in object detection, specially focusing on Barcode and QR code recognition. Utilizing the real-time detection abilities of YOLOv8, we performed a study aimed at enhancing its talent in swiftly and correctly figuring out objects. Through large training and high-quality-tuning on Kaggle datasets tailored for Barcode and QR code detection, our goal became to optimize YOLOv8's overall performance throughout numerous situations and environments. The look encompasses the assessment of YOLOv8 throughout special version iterations: Nano, Small, and Medium, with a meticulous attention on precision, recall, and F1 assessment metrics. The consequences exhibit large improvements in object detection accuracy with every subsequent model refinement. Specifically, we achieved an accuracy of 88.95% for the nano model, 97.10% for the small model, and 94.10% for the medium version, showcasing the incremental improvements finished via model scaling. Our findings highlight the big strides made through YOLOv8 in pushing the limits of computer vision, ensuring its function as a milestone within the subject of object detection. This study sheds light on how model scaling affects object recognition, increasing the concept of deep learning-based computer creative and prescient techniques.

</details>


### [142] [DenoiseGS: Gaussian Reconstruction Model for Burst Denoising](https://arxiv.org/abs/2511.22939)
*Yongsen Cheng,Yuanhao Cai,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出了DenoiseGS，一种基于3D高斯投射的高效爆发图像去噪方法，在速度和效果上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有爆发去噪方法在应对大幅度运动时效果不佳，且计算成本高。因此，亟需更高效且鲁棒的方法提升手持设备图像质量。

Method: 提出DenoiseGS，将3D Gaussian Splatting用于爆发去噪，引入了两个新损失：1. 高斯自一致性（GSC）损失，用于正则化噪声输入下的点云结构；2. 对数加权频域（LWF）损失，增强高频细节保留。模型通过对比干净与噪声输入下的输出进行监督，避免了域偏差。

Result: 大量实验表明，DenoiseGS不仅在去噪和新视角合成任务上均大幅超越现有基于NeRF的方法，并在推理速度上提升了250倍。

Conclusion: DenoiseGS在去噪性能与效率方面实现了显著突破，为实际移动设备成像提供了更优解决方案。

Abstract: Burst denoising methods are crucial for enhancing images captured on handheld devices, but they often struggle with large motion or suffer from prohibitive computational costs. In this paper, we propose DenoiseGS, the first framework to leverage the efficiency of 3D Gaussian Splatting for burst denoising. Our approach addresses two key challenges when applying feedforward Gaussian reconsturction model to noisy inputs: the degradation of Gaussian point clouds and the loss of fine details. To this end, we propose a Gaussian self-consistency (GSC) loss, which regularizes the geometry predicted from noisy inputs with high-quality Gaussian point clouds. These point clouds are generated from clean inputs by the same model that we are training, thereby alleviating potential bias or domain gaps. Additionally, we introduce a log-weighted frequency (LWF) loss to strengthen supervision within the spectral domain, effectively preserving fine-grained details. The LWF loss adaptively weights frequency discrepancies in a logarithmic manner, emphasizing challenging high-frequency details. Extensive experiments demonstrate that DenoiseGS significantly exceeds the state-of-the-art NeRF-based methods on both burst denoising and novel view synthesis under noisy conditions, while achieving \textbf{250$\times$} faster inference speed. Code and models are released at https://github.com/yscheng04/DenoiseGS.

</details>


### [143] [One-to-All Animation: Alignment-Free Character Animation and Image Pose Transfe](https://arxiv.org/abs/2511.22940)
*Shijun Shi,Jing Xu,Zhihang Li,Chunli Peng,Xiaoda Yang,Lijing Lu,Kai Hu,Jiangning Zhang*

Main category: cs.CV

TL;DR: 本文提出One-to-All Animation框架，能够实现对任意布局参考图像的高保真角色动画与姿态迁移，有效解决了参考图像与目标姿态空间不对齐带来的问题。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在角色动画生成上取得显著进展，但多数方法要求参考图像和目标姿态空间对齐、骨骼结构完全一致。实际中这种严格对齐难以保证，因此需要一种能够处理空间不对齐、骨骼结构不一致等实际情况的通用方法。

Method: 1）将训练过程重构为自监督的outpainting任务，使多样布局的参考图像统一为有遮挡的输入格式；2）设计reference extractor以处理部分可见的参考，全面提取角色身份特征；3）引入混合参考融合注意力机制，适应不同分辨率和动态序列长度；4）从生成质量角度，引入identity-robust pose control机制，将外观与骨骼结构解耦，防止姿态过拟合，并提出token replace策略生成连贯长视频。

Result: 大量实验结果表明，本方法在高保真角色动画和图像姿态迁移任务上优于现有方法。

Conclusion: One-to-All Animation框架有效突破了空间不对齐等难题，实现了对不同结构、布局参考图像的统一高质量动画生成，拓展了扩散模型在角色动画和姿态迁移任务中的应用边界。

Abstract: Recent advances in diffusion models have greatly improved pose-driven character animation. However, existing methods are limited to spatially aligned reference-pose pairs with matched skeletal structures. Handling reference-pose misalignment remains unsolved. To address this, we present One-to-All Animation, a unified framework for high-fidelity character animation and image pose transfer for references with arbitrary layouts. First, to handle spatially misaligned reference, we reformulate training as a self-supervised outpainting task that transforms diverse-layout reference into a unified occluded-input format. Second, to process partially visible reference, we design a reference extractor for comprehensive identity feature extraction. Further, we integrate hybrid reference fusion attention to handle varying resolutions and dynamic sequence lengths. Finally, from the perspective of generation quality, we introduce identity-robust pose control that decouples appearance from skeletal structure to mitigate pose overfitting, and a token replace strategy for coherent long-video generation. Extensive experiments show that our method outperforms existing approaches. The code and model will be available at https://github.com/ssj9596/One-to-All-Animation.

</details>


### [144] [Do We Need Perfect Data? Leveraging Noise for Domain Generalized Segmentation](https://arxiv.org/abs/2511.22948)
*Taeyeong Kim,SeungJoon Lee,Jung Uk Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: 本文提出FLEX-Seg框架，通过灵活利用生成数据中语义掩码与图像之间的不对齐特性，有效提升跨域语义分割的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化的语义分割方法在面对恶劣环境时，常因生成数据与真实样本间出现域偏移而失效，而扩散模型生成的数据与语义标签天然存在对齐偏差。作者希望将这一“缺陷”转化为增强模型鲁棒性的机会。

Method: FLEX-Seg包含三大创新点：1）多尺度边界特征建模（Granular Adaptive Prototypes），2）基于不确定性动态调整边界学习（Uncertainty Boundary Emphasis），3）难例自适应采样（Hardness-Aware Sampling），共同提升模型对边界及困难样本的表征能力，不强求数据对齐，反而利用错位带来的多样性。

Result: 在五个真实数据集上的实验，FLEX-Seg在ACDC和Dark Zurich数据集分别提升2.44%和2.63%的mIoU，较多领域现有方法表现更佳。

Conclusion: 自适应地处理不完美生成数据，利用其固有的不对齐特征，有助于模型学习出更具泛化性和鲁棒性的语义分割表示。

Abstract: Domain generalization in semantic segmentation faces challenges from domain shifts, particularly under adverse conditions. While diffusion-based data generation methods show promise, they introduce inherent misalignment between generated images and semantic masks. This paper presents FLEX-Seg (FLexible Edge eXploitation for Segmentation), a framework that transforms this limitation into an opportunity for robust learning. FLEX-Seg comprises three key components: (1) Granular Adaptive Prototypes that captures boundary characteristics across multiple scales, (2) Uncertainty Boundary Emphasis that dynamically adjusts learning emphasis based on prediction entropy, and (3) Hardness-Aware Sampling that progressively focuses on challenging examples. By leveraging inherent misalignment rather than enforcing strict alignment, FLEX-Seg learns robust representations while capturing rich stylistic variations. Experiments across five real-world datasets demonstrate consistent improvements over state-of-the-art methods, achieving 2.44% and 2.63% mIoU gains on ACDC and Dark Zurich. Our findings validate that adaptive strategies for handling imperfect synthetic data lead to superior domain generalization. Code is available at https://github.com/VisualScienceLab-KHU/FLEX-Seg.

</details>


### [145] [Contrastive Heliophysical Image Pretraining for Solar Dynamics Observatory Records](https://arxiv.org/abs/2511.22958)
*Shiyu Shen,Zhe Gao,Taifeng Chai,Yang Huang,Bin Pan*

Main category: cs.CV

TL;DR: 本文提出了SolarCHIP，一种针对SDO多仪器观测数据进行对比式预训练的视觉骨干网络，在两个下游任务上实现了最优表现，并为太阳物理学图像分析提供了高效、可重用的特征提取器。


<details>
  <summary>Details</summary>
Motivation: 深度学习已极大推动太阳图像分析，但现有方法大多从零训练专用编码器，或依赖自然图像的预训练，这两者都未能针对Solar Dynamics Observatory（SDO）数据的独特特性。因此，亟需一种更适合SDO多模态、多仪器数据的通用视觉骨干。

Method: 提出SolarCHIP对比式预训练框架，融合多粒度的对比目标：1）对齐AIA-HMI同时刻的全局类token，提升时序判别力；2）对齐不同模态但空间位置一致的局部patch，提取空间一致、跨模态的特征；3）对齐同一图像不同空间位置的patch，保持细粒度空间结构信息。框架同时适用于CNN和ViT结构，并应用于跨模态翻译与耀斑分类两项任务。

Result: 在HMI与AIA带通之间的跨模态翻译（ControlNet）与全盘耀斑分类两个任务上，SolarCHIP均取得了最先进的性能，尤其在标注数据有限场景下效果突出。消融实验证明每个对比分支均有独特、不可或缺的贡献。

Conclusion: SolarCHIP作为预训练的视觉骨干网络，为太阳物理学图像分析领域带来了高效、开箱即用的特征提取工具，既减轻了计算负担，提高了标签利用率，也为后续相关应用建立了坚实基础。

Abstract: Deep learning has revolutionized solar image analysis, yet most approaches train task-specific encoders from scratch or rely on natural-image pretraining that ignores the unique characteristics of Solar Dynamics Observatory (SDO) data. We introduce SolarCHIP, a family of contrastively pretrained visual backbones tailored to multi-instrument SDO observations. SolarCHIP addresses three key challenges in solar imaging: multimodal sensing across AIA and HMI instruments, weak inter-class separability due to slow temporal evolution, and strong intra-class variability with sparse activity signals. Our pretraining framework employs a multi-granularity contrastive objective that jointly aligns (1) global class tokens across co-temporal AIA-HMI pairs to enhance temporal discrimination, (2) local patch tokens at fixed spatial indices to enforce position-consistent, modality-invariant features, and (3) intra-sample patches across different spatial locations to preserve fine-grained spatial structure. We train both CNN- and Vision Transformer-based autoencoders and demonstrate their effectiveness on two downstream tasks: cross-modal translation between HMI and AIA passbands via ControlNet, and full-disk flare classification. Experimental results show that SolarCHIP achieves state-of-the-art performance across both tasks, with particularly strong gains in low-resource settings where labeled data is limited. Ablation studies confirm that each contrastive component contributes essential discriminative capacity at different granularities. By publicly releasing pretrained weights and training code, we provide the heliophysics community with a practical, plug-and-play feature extractor that reduces computational requirements, improves label efficiency, and establishes a reusable foundation for diverse solar imaging applications.

</details>


### [146] [HMR3D: Hierarchical Multimodal Representation for 3D Scene Understanding with Large Vision-Language Model](https://arxiv.org/abs/2511.22961)
*Chen Li,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的分层多模态表示方法，用于3D场景推理，通过结合多视角图像和文本描述显式对齐到大模型输入空间，显著提升了3D问答等任务的效果。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在3D场景理解方面效果有限，原因在于3D数据稀缺和空间关系复杂，传统方法仅隐式对齐特征，导致性能受限。

Method: 作者提出了层级化多模态表示方法。具体做法是：将多视角图像（包含俯视和四个方向）和结合了3D坐标的文本描述，同时输入到大模型，实现输入空间的显式对齐。同时，利用层级特征表示，从图像块特征聚合到视角级再到场景级，以实现对局部和全局信息的推理。

Result: 在实际位置信息3D问答和通用3D问答基准测试上，所提方法取得了优异表现，显著优于现有方法。

Conclusion: 显式多模态输入对齐和分层特征建模能够有效提升3D场景推理和理解能力，为大模型在更复杂的3D应用场景拓展了可能性。

Abstract: Recent advances in large vision-language models (VLMs) have shown significant promise for 3D scene understanding. Existing VLM-based approaches typically align 3D scene features with the VLM's embedding space. However, this implicit alignment often yields suboptimal performance due to the scarcity of 3D data and the inherent complexity of spatial relationships in 3D environments. To address these limitations, we propose a novel hierarchical multimodal representation for 3D scene reasoning that explicitly aligns with VLMs at the input space by leveraging both multi-view images and text descriptions. The text descriptions capture spatial relationships by referencing the 3D coordinates of detected objects, while the multi-view images include a top-down perspective and four directional views (forward, left, right, and backward), ensuring comprehensive scene coverage. Additionally, we introduce a hierarchical feature representation that aggregates patch-level image features into view-level and scene-level representations, enabling the model to reason over both local and global scene context. Experimental results on both situated 3D Q&A and general 3D Q&A benchmarks demonstrate the effectiveness of our approach.

</details>


### [147] [Taming the Light: Illumination-Invariant Semantic 3DGS-SLAM](https://arxiv.org/abs/2511.22968)
*Shouhe Zhang,Dayong Ren,Sensen Song,Yurong Qian,Zhenhong Jia*

Main category: cs.CV

TL;DR: 本文提出了一种新的语义SLAM框架，能够在极端光照条件下提升3D重建和语义分割的鲁棒性，取得了领先的实验表现。


<details>
  <summary>Details</summary>
Motivation: 极端曝光导致3D重建和语义分割精度下降，影响紧耦合系统的性能，因此需要一种对光照变化具有鲁棒性的SLAM方法。

Method: 方法包括两个创新设计：1）引入本征外观归一化（IAN）模块，主动分离场景本征属性（如反照率）和瞬时光照，学习一种光照不变的外观模型，从而对每个高斯基元赋予一致的颜色表征；2）提出动态辐射平衡损失（DRB-Loss），在图像曝光极端时激活，直接作用于辐射场，针对性地优化，减少极端光照带来的误差累积。

Result: 在公开数据集上，提出的方法在相机跟踪、地图质量、语义及几何精度方面均取得了当前最优的实验结果。

Conclusion: 本文的方法能有效提升SLAM系统在极端光照条件下的鲁棒性和精度，对实际应用具有重要意义。

Abstract: Extreme exposure degrades both the 3D map reconstruction and semantic segmentation accuracy, which is particularly detrimental to tightly-coupled systems. To achieve illumination invariance, we propose a novel semantic SLAM framework with two designs. First, the Intrinsic Appearance Normalization (IAN) module proactively disentangles the scene's intrinsic properties, such as albedo, from transient lighting. By learning a standardized, illumination-invariant appearance model, it assigns a stable and consistent color representation to each Gaussian primitive. Second, the Dynamic Radiance Balancing Loss (DRB-Loss) reactively handles frames with extreme exposure. It activates only when an image's exposure is poor, operating directly on the radiance field to guide targeted optimization. This prevents error accumulation from extreme lighting without compromising performance under normal conditions. The synergy between IAN's proactive invariance and DRB-Loss's reactive correction endows our system with unprecedented robustness. Evaluations on public datasets demonstrate state-of-the-art performance in camera tracking, map quality, and semantic and geometric accuracy.

</details>


### [148] [BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation](https://arxiv.org/abs/2511.22973)
*Zeyu Zhang,Shuning Chang,Yuanyu He,Yizeng Han,Jiasheng Tang,Fan Wang,Bohan Zhuang*

Main category: cs.CV

TL;DR: 本文提出了一种名为BlockVid的新型区块扩散（block diffusion）框架，实现了分钟级长视频的高质量生成，并引入了专门的长视频基准与新型一致性评测指标，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有半自回归（block diffusion）生成视频的方法虽然在效率和灵活性上表现良好，但在长时间尺度下容易因KV缓存导致误差积累，同时缺乏细粒度的长视频评测数据与衡量长时一致性的指标，这极大限制了分钟级长视频生成质量的提升。

Method: BlockVid框架引入了基于语义的稀疏KV缓存、Block Forcing训练策略和分块噪声调度及洗牌机制，以削弱误差传播并提升视频的时序一致性。同时，提出了LV-Bench——一个为分钟级长视频设计的细粒度基准数据集，并开发了衡量长时一致性的新指标。

Result: 在VBench和LV-Bench两个数据集上的大量实验表明，BlockVid方法在长视频生成的质量、一致性方面拥有明显优势——在LV-Bench评测中，VDE Subject提升22.2%、VDE Clarity提升19.4%，分别超越当前最佳方案。

Conclusion: BlockVid通过结构创新与专用评测工具的结合，成功突破了半自回归长视频生成的瓶颈，为高质量、长时间一致的视频生成奠定了基础，并有望推动世界模型及AI仿真器向现实感更强的发展。

Abstract: Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.

</details>


### [149] [McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning](https://arxiv.org/abs/2511.22974)
*Qiushi Yang,Yingjie Chen,Yuan Yao,Yifang Men,Huaizhuo Liu,Miaomiao Cui*

Main category: cs.CV

TL;DR: 本文提出了一种三阶段强化学习框架McSc，有效提升文本生成视频与人类偏好的对齐能力，尤其在运动动态的维度实现了更优表现。


<details>
  <summary>Details</summary>
Motivation: 虽然T2V（文本生成视频）模型在视频与文本匹配上取得了显著进展，但现有方法难以精确捕捉和对齐人类偏好，原因在于人类偏好主观且复杂。现有对齐方法依赖昂贵的人类标注或代理指标，且通常忽视运动动态等潜在冲突维，导致模型倾向于生成低动态内容。

Method: 提出了Motion-corrective alignment with Self-critic hierarchical Reasoning（McSc）框架。首先利用Self-critic Dimensional Reasoning（ScDR）训练生成式奖励模型，将人类偏好分解为各维度进行评估。然后，通过Hierarchical Comparative Reasoning（HCR）实现结构化的多维度推理及分层奖励监督。最后结合Motion-corrective Direct Preference Optimization（McDPO），动态调整对齐目标，缓解低动态偏向。该策略用奖励模型筛选的视频优化T2V模型。

Result: 实验表明，该方法在对齐人类偏好方面取得了优异表现，生成的视频在高动态（运动）内容上明显优于基线模型。

Conclusion: McSc框架能更好建模和对齐人类多维偏好，特别提升了生成视频的运动动态表现，为T2V任务的人类偏好对齐提供了有效新范式。

Abstract: Text-to-video (T2V) generation has achieved remarkable progress in producing high-quality videos aligned with textual prompts. However, aligning synthesized videos with nuanced human preference remains challenging due to the subjective and multifaceted nature of human judgment. Existing video preference alignment methods rely on costly human annotations or utilize proxy metrics to predict preference, which lacks the understanding of human preference logic. Moreover, they usually directly align T2V models with the overall preference distribution, ignoring potential conflict dimensions like motion dynamics and visual quality, which may bias models towards low-motion content. To address these issues, we present Motion-corrective alignment with Self-critic hierarchical Reasoning (McSc), a three-stage reinforcement learning framework for robust preference modeling and alignment. Firstly, Self-critic Dimensional Reasoning (ScDR) trains a generative reward model (RM) to decompose preferences into per-dimension assessments, using self-critic reasoning chains for reliable learning. Secondly, to achieve holistic video comparison, we introduce Hierarchical Comparative Reasoning (HCR) for structural multi-dimensional reasoning with hierarchical reward supervision. Finally, using RM-preferred videos, we propose Motion-corrective Direct Preference Optimization (McDPO) to optimize T2V models, while dynamically re-weighting alignment objective to mitigate bias towards low-motion content. Experiments show that McSc achieves superior performance in human preference alignment and generates videos with high-motion dynamic.

</details>


### [150] [Ovis-Image Technical Report](https://arxiv.org/abs/2511.22982)
*Guo-Hua Wang,Liangfu Cao,Tianyu Cui,Minghao Fu,Xiaohao Chen,Pengxin Zhan,Jianshan Zhao,Lan Li,Bowen Fu,Jiaqi Liu,Qing-Guo Chen*

Main category: cs.CV

TL;DR: Ovis-Image是一款高效、紧凑、优化于文本渲染的7B参数文本生成图像模型，在计算资源有限条件下也能与更大模型媲美。


<details>
  <summary>Details</summary>
Motivation: 当前高质量文本渲染的文本生成图像模型普遍体积庞大，难以在实际场景下部署，尤其是需要较大算力和高内存。研究动机是缩小模型规模的同时，依然实现高阶文本渲染表现，使得模型能在单块高端GPU上运行，提升实用性与可达性。

Method: Ovis-Image基于Ovis-U1框架，采用扩散式视觉解码器，配合强化后的Ovis 2.5多模态骨干网络。训练流程以文本为中心，综合大规模预训练及精心设计的后期微调，聚焦双语文本渲染的效果提升。

Result: 在模型仅7B参数情况下，Ovis-Image的文本渲染效果可媲美如Qwen-Image等更大开源模型，并接近闭源系统如Seedream和GPT4o。同时，模型可在单卡高端GPU完成部署，显著降低使用门槛。

Conclusion: 强多模态骨干网络结合文本优化训练，即便在紧凑模型下（如7B参数），仍能实现出色、稳定的双语文本渲染，无需依赖超大模型或专有系统，推进了实用化进程。

Abstract: We introduce $\textbf{Ovis-Image}$, a 7B text-to-image model specifically optimized for high-quality text rendering, designed to operate efficiently under stringent computational constraints. Built upon our previous Ovis-U1 framework, Ovis-Image integrates a diffusion-based visual decoder with the stronger Ovis 2.5 multimodal backbone, leveraging a text-centric training pipeline that combines large-scale pre-training with carefully tailored post-training refinements. Despite its compact architecture, Ovis-Image achieves text rendering performance on par with significantly larger open models such as Qwen-Image and approaches closed-source systems like Seedream and GPT4o. Crucially, the model remains deployable on a single high-end GPU with moderate memory, narrowing the gap between frontier-level text rendering and practical deployment. Our results indicate that combining a strong multimodal backbone with a carefully designed, text-focused training recipe is sufficient to achieve reliable bilingual text rendering without resorting to oversized or proprietary models.

</details>


### [151] [Convolutional Feature Noise Reduction for 2D Cardiac MR Image Segmentation](https://arxiv.org/abs/2511.22983)
*Hong Zheng,Nan Mu,Han Su,Lin Feng,Xiaoning Li*

Main category: cs.CV

TL;DR: 本文提出了一种称为CFF（Convolutional Feature Filter）的新特征滤波器，有效减少了分割网络中卷积特征的噪声。


<details>
  <summary>Details</summary>
Motivation: 尽管噪声抑制在数字信号处理中至关重要，但在分割网络卷积特征处理中经常被忽视，从而影响后续特征系统的表现。作者希望弥补这一空白。

Method: 将卷积特征视为高斯分布的特征信号矩阵，设计了一种低幅通滤波器CFF用以降低特征信号中的噪声，并提出了基于二值化的信息熵计算方法来量化噪声的变化。

Result: 在两种主流2D分割网络和两个公开心脏MR影像数据集上实验，结果显示CFF能显著降低特征信号矩阵中的噪声。

Conclusion: CFF作为简单有效的噪声抑制方法，能提升医学图像分割网络中的特征表征质量，具有实际应用价值。

Abstract: Noise reduction constitutes a crucial operation within Digital Signal Processing. Regrettably, it frequently remains neglected when dealing with the processing of convolutional features in segmentation networks. This oversight could trigger the butterfly effect, impairing the subsequent outcomes within the entire feature system. To complete this void, we consider convolutional features following Gaussian distributions as feature signal matrices and then present a simple and effective feature filter in this study. The proposed filter is fundamentally a low-amplitude pass filter primarily aimed at minimizing noise in feature signal inputs and is named Convolutional Feature Filter (CFF). We conducted experiments on two established 2D segmentation networks and two public cardiac MR image datasets to validate the effectiveness of the CFF, and the experimental findings demonstrated a decrease in noise within the feature signal matrices. To enable a numerical observation and analysis of this reduction, we developed a binarization equation to calculate the information entropy of feature signals.

</details>


### [152] [MultiBanana: A Challenging Benchmark for Multi-Reference Text-to-Image Generation](https://arxiv.org/abs/2511.22989)
*Yuta Oshima,Daiki Miyake,Kohsei Matsutani,Yusuke Iwasawa,Masahiro Suzuki,Yutaka Matsuo,Hiroki Furuta*

Main category: cs.CV

TL;DR: 本文提出了MultiBanana基准数据集，针对多参考图像生成与编辑任务进行了系统设计与评测，有助于公平衡量和推动该领域技术进步。


<details>
  <summary>Details</summary>
Motivation: 现有的评测基准主要关注单一或少量参考图片生成，任务定义模糊，无法全面衡量模型在多参考条件下的表现和局限。该工作旨在弥补这一评测空白，促进相关模型的进步和对比。

Method: 作者设计并构建了MultiBanana数据集，在多参考问题维度下广泛涵盖了（1）参考图像数量变化，（2）参考域不一致（如照片与动漫），（3）参考和目标场景规模不一致，（4）罕见概念（如红色香蕉），（5）多语言文本参考等问题。借助该基准，作者评测了多种文本到图像生成模型，分析其表现及失败模式。

Result: 通过MultiBanana基准测试，揭示了现有主流多参考生成模型在各种子任务上的优劣和常见的失败类型，并具体指出了提升空间。

Conclusion: MultiBanana实现了多参考生成任务的公开、系统化评测，为领域内后续研究提供了公平的标准，有助于推动文本到多参考图像生成技术的发展。数据和代码已经开源。

Abstract: Recent text-to-image generation models have acquired the ability of multi-reference generation and editing; the ability to inherit the appearance of subjects from multiple reference images and re-render them under new contexts. However, the existing benchmark datasets often focus on the generation with single or a few reference images, which prevents us from measuring the progress on how model performance advances or pointing out their weaknesses, under different multi-reference conditions. In addition, their task definitions are still vague, typically limited to axes such as "what to edit" or "how many references are given", and therefore fail to capture the intrinsic difficulty of multi-reference settings. To address this gap, we introduce $\textbf{MultiBanana}$, which is carefully designed to assesses the edge of model capabilities by widely covering multi-reference-specific problems at scale: (1) varying the number of references, (2) domain mismatch among references (e.g., photo vs. anime), (3) scale mismatch between reference and target scenes, (4) references containing rare concepts (e.g., a red banana), and (5) multilingual textual references for rendering. Our analysis among a variety of text-to-image models reveals their superior performances, typical failure modes, and areas for improvement. MultiBanana will be released as an open benchmark to push the boundaries and establish a standardized basis for fair comparison in multi-reference image generation. Our data and code are available at https://github.com/matsuolab/multibanana .

</details>


### [153] [MIMM-X: Disentangling Spurious Correlations for Medical Image Analysis](https://arxiv.org/abs/2511.22990)
*Louisa Fay,Hajer Reguigui,Bin Yang,Sergios Gatidis,Thomas Küstner*

Main category: cs.CV

TL;DR: 该论文提出MIMM-X框架，通过最小化互信息从多个伪相关中解耦因果特征，有效缓解医疗影像中深度学习模型的捷径学习问题，提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医疗任务中表现出色，但由于依赖伪相关（捷径学习），导致在新环境下泛化能力差，尤其在医疗影像领域，多个伪相关共存时可能导致严重误判。因此，亟需方法解决多重伪相关导致的捷径学习问题。

Method: 提出MIMM-X框架，通过最小化因果特征与多个伪相关特征之间的互信息，实现因果特征的解耦，使预测基于真实因果关系而非数据集特有捷径。并在三个医学影像数据集（UK Biobank、NAKO、CheXpert）和两种影像模态（MRI、X-ray）上进行评估。

Result: 实验证明，MIMM-X能够有效缓解来自多个伪相关的捷径学习问题，提升模型在不同数据集和模态下的泛化表现。

Conclusion: MIMM-X为医疗影像中的捷径学习问题提供了新的解决思路，提升了深度学习模型的稳健性，有望降低因伪相关导致的误判风险。

Abstract: Deep learning models can excel on medical tasks, yet often experience spurious correlations, known as shortcut learning, leading to poor generalization in new environments. Particularly in medical imaging, where multiple spurious correlations can coexist, misclassifications can have severe consequences. We propose MIMM-X, a framework that disentangles causal features from multiple spurious correlations by minimizing their mutual information. It enables predictions based on true underlying causal relationships rather than dataset-specific shortcuts. We evaluate MIMM-X on three datasets (UK Biobank, NAKO, CheXpert) across two imaging modalities (MRI and X-ray). Results demonstrate that MIMM-X effectively mitigates shortcut learning of multiple spurious correlations.

</details>


### [154] [Guiding Visual Autoregressive Models through Spectrum Weakening](https://arxiv.org/abs/2511.22991)
*Chaoyang Wang,Tianmeng Yang,Jingdong Wang,Yunhai Tong*

Main category: cs.CV

TL;DR: 本文提出了一种适用于视觉自回归（AR）模型的频谱减弱框架，无需重新训练或修改架构，通过在频域构建可控“弱模型”，在无条件生成和条件对齐间实现平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的classifier-free guidance方法大多依赖于扩散模型的假设，对无条件视觉生成的支持有限。因此，作者希望提高视觉AR模型在无条件生成和条件生成上的表现，同时避免对模型结构和训练方式的限制。

Method: 作者提出在频谱（spectral）域实现模型减弱：通过可逆的频谱变换保证信息保持，并通过选择性保留特定频谱子集进行有控制的信息削减。这种方式无需改变原有模型架构，仅对内部表示的通道维度做频谱选择。同时，提出两种频谱归一化策略以保证数值稳定性。

Result: 在离散和连续的视觉AR模型（含文本、类别条件）的多项实验表明，该方法在无条件生成时表现出高质量，同时可在有条件生成时保持良好的条件对齐。

Conclusion: 频谱减弱框架无需重训练和架构变更，能够在视觉AR模型中实现高质量无条件生成与强条件响应，为无条件指导和条件生成任务带来新方案。

Abstract: Classifier-free guidance (CFG) has become a widely adopted and practical approach for enhancing generation quality and improving condition alignment. Recent studies have explored guidance mechanisms for unconditional generation, yet these approaches remain fundamentally tied to assumptions specific to diffusion models. In this work, we propose a spectrum-weakening framework for visual autoregressive (AR) models. This method works without the need for re-training, specific conditions, or any architectural modifications. It achieves this by constructing a controllable weak model in the spectral domain. We theoretically show that invertible spectral transformations preserve information, while selectively retaining only a subset of spectrum introduces controlled information reduction. Based on this insight, we perform spectrum selection along the channel dimension of internal representations, which avoids the structural constraints imposed by diffusion models. We further introduce two spectrum renormalization strategies that ensures numerical stability during the weakening process. Extensive experiments were conducted on both discrete and continuous AR models, with text or class conditioning. The results demonstrate that our method enables high-quality unconditional generation while maintaining strong prompt alignment for conditional generation.

</details>


### [155] [Optimizer Sensitivity In Vision Transformerbased Iris Recognition: Adamw Vs Sgd Vs Rmsprop](https://arxiv.org/abs/2511.22994)
*Moh Imam Faiz,Aviv Yuniar Rahman,Rangga Pahlevi Putra*

Main category: cs.CV

TL;DR: 本文研究了不同优化器对基于ViT虹膜识别系统性能的影响，并提出相关分析以提升生物识别模型的稳健性。


<details>
  <summary>Details</summary>
Motivation: 随着数字身份系统发展，生物识别安全变得尤为重要。尽管ViT已提升视觉识别性能，但其在虹膜识别任务中对优化器的选择影响尚未深入探讨。

Method: 本文通过实验，对比和评估多种优化器在基于ViT的虹膜识别系统中的表现，分析其对准确率和模型稳定性的影响。

Result: 不同优化器对ViT虹膜识别的准确性和稳定性产生显著影响，实验结果揭示了各优化器的优劣。

Conclusion: 针对ViT虹膜识别系统，选择合适的优化器对于提升生物识别模型的准确性和鲁棒性至关重要。

Abstract: The security of biometric authentication is increasingly critical as digital identity systems expand. Iris recognition offers high reliability due to its distinctive and stable texture patterns. Recent progress in deep learning, especially Vision Transformers ViT, has improved visual recognition performance. Yet, the effect of optimizer choice on ViT-based biometric systems remains understudied. This work evaluates how different optimizers influence the accuracy and stability of ViT for iris recognition, providing insights to enhance the robustness of biometric identification models.

</details>


### [156] [JarvisEvo: Towards a Self-Evolving Photo Editing Agent with Synergistic Editor-Evaluator Optimization](https://arxiv.org/abs/2511.23002)
*Yunlong Lin,Linqing Wang,Kunjie Lin,Zixu Lin,Kaixiong Gong,Wenbo Li,Bin Lin,Zhenxi Li,Shiyi Zhang,Yuyang Peng,Wenxun Dai,Xinghao Ding,Chunyu Wang,Qinglin Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像编辑智能体JarvisEvo，通过多模态推理与自我优化机制，有效提升了编辑质量并解决了当前模型的关键难题。


<details>
  <summary>Details</summary>
Motivation: 现有基于智能体的编辑模型，在交互体验、处理质量及创意灵活性方面取得进展，但仍面临指令幻觉和奖励黑客两大挑战。前者导致指令推理过程中事实错误频发，后者则是智能体利用静态奖励模型的漏洞，影响编辑质量与自我提升能力。

Method: 提出了JarvisEvo智能体，模拟专家级人类设计师，在编辑过程中不断迭代决策。采用交错的多模态链式思维推理（iMCoT）机制提升指令遵循度和编辑表现，同时引入编辑-评估者协同优化（SEPO）框架，使智能体实现无需外部奖励的自我改进，有效缓解奖励黑客问题，并支持集成Adobe Lightroom进行全局和局部编辑。

Result: 在ArtEdit-Bench基准测试上，JarvisEvo在保真编辑指标上较Nano-Banana平均提升了18.95%，其中像素级内容一致性提升高达44.96%。

Conclusion: JarvisEvo凭借创新的多模态推理和自我优化机制，显著提升了智能体编辑表现，解决了指令幻觉与奖励黑客的难题，是高质量自动化图像编辑的重要进展。

Abstract: Agent-based editing models have substantially advanced interactive experiences, processing quality, and creative flexibility. However, two critical challenges persist: (1) instruction hallucination, text-only chain-of-thought (CoT) reasoning cannot fully prevent factual errors due to inherent information bottlenecks; (2) reward hacking, dynamic policy optimization against static reward models allows agents to exploit flaws in reward functions. To address these issues, we propose JarvisEvo, a unified image editing agent that emulates an expert human designer by iteratively editing, selecting appropriate tools, evaluating results, and reflecting on its own decisions to refine outcomes. JarvisEvo offers three key advantages: (1) an interleaved multimodal chain-of-thought (iMCoT) reasoning mechanism that enhances instruction following and editing quality; (2) a synergistic editor-evaluator policy optimization (SEPO) framework that enables self-improvement without external rewards, effectively mitigating reward hacking; and (3) support for both global and local fine-grained editing through seamless integration of Adobe Lightroom. On ArtEdit-Bench, JarvisEvo outperforms Nano-Banana by an average of 18.95% on preservative editing metrics, including a substantial 44.96% improvement in pixel-level content fidelity.

</details>


### [157] [From Illusion to Intention: Visual Rationale Learning for Vision-Language Reasoning](https://arxiv.org/abs/2511.23031)
*Changpeng Wang,Haozhe Wang,Xi Chen,Junhan Liu,Taofeng Xue,Chong Peng,Donglian Qi,Fangzhen Lin,Yunfeng Yan*

Main category: cs.CV

TL;DR: 本文提出了一种名为Visual Rationale Learning（ViRL）的新型视觉-语言推理范式，将视觉动作作为核心推理环节，从而实现真正以视觉为基础的复杂推理。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言推理模型虽然可以用图片辅助推理，但把视觉动作当成可选工具，导致模型看似依赖图片，实则推理未真正依赖视觉依据，出现“以图思考”错觉。作者希望突破这种假象，让模型在推理过程中真正利用视觉证据。

Method: 作者提出了视觉理据（visual rationalization）概念，并据此设计了ViRL方法。ViRL方法包含三个核心机制：(1) 以带有真实推理由来的过程监督；(2) 基于步骤级奖励塑形的目标对齐；(3) 对每一步可区分正确、多余和错误动作的细粒度归因。整体训练采用端到端强化学习。

Result: ViRL在基于感知、幻觉和推理等多个基准任务上取得了最新最优的性能，展示了其方法的有效性。

Conclusion: ViRL将视觉理据作为通用、过程依赖的训练范式，有助于构建透明、可验证且值得信赖的视觉-语言推理模型，为相关领域的发展提供了新方向。

Abstract: Recent advances in vision-language reasoning underscore the importance of thinking with images, where models actively ground their reasoning in visual evidence. Yet, prevailing frameworks treat visual actions as optional tools, boosting metrics but leaving reasoning ungrounded and crops ineffective. This gap gives rise to the illusion of thinking with images: models seem visually grounded but rely on context-agnostic actions that neither refine perception nor guide reasoning toward correct answers. We address this problem by reframing visual actions as core reasoning primitives rather than optional tools, which we term visual rationalization, the visual analogue of textual Chain-of-Thought. Building on this insight, we propose Visual Rationale Learning (ViRL), an end-to-end paradigm that grounds training in the visual rationale itself. ViRL integrates (1) Process Supervision with ground-truth rationales, (2) Objective Alignment via step-level reward shaping, and (3) Fine-Grained Credit Assignment to distinguish correct, redundant, and erroneous actions. By ensuring each action contributes meaningfully to the reasoning chain, ViRL enables models to "get the right answer for the right visual reason". Trained purely with end-to-end RL, ViRL achieves state-of-the-art results across benchmarks spanning perception, hallucination, and reasoning. This work establishes visual rationalization as a task-agnostic, process-grounded paradigm for building transparent, verifiable, and trustworthy vision-language models.

</details>


### [158] [Geometry-Consistent 4D Gaussian Splatting for Sparse-Input Dynamic View Synthesis](https://arxiv.org/abs/2511.23044)
*Yiwei Li,Jiannong Cao,Penghui Ruan,Divya Saxena,Songye Zhu,Yinfeng Cao*

Main category: cs.CV

TL;DR: 提出了一种针对动态场景稀疏视角动态高斯加权（4DGS）渲染质量下降问题的新方法GC-4DGS，通过融入几何一致性机制，在保持效率的前提下显著提升稀疏输入视角下的动态场景渲染质量，并优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有的动态高斯加权（Gaussian Splatting）在动态场景中实现视角合成对AIoT应用很有潜力，但在实际中输入视角往往稀疏，导致几何学习不一致，严重影响渲染质量，限制了其泛用性，因此需要解决稀疏输入下的几何一致性问题。

Method: 提出GC-4DGS框架：1）设计动态一致性检查策略，减少多视角立体（MVS）在时空中的估计不确定性；2）提出全局-局部深度正则化，将单目深度信息蒸馏为时空一致的几何先验，从而提升4D体积内的几何与外观学习一致性。

Result: 在N3DV和Technicolor两个主流数据集上进行了大量实验，GC-4DGS在渲染质量与效率方面都表现优异。与最新的稀疏动态视角合成方法RF-DeRF和原版4DGS分别提升PSNR 2.62dB和1.58dB，并可无缝部署到物联网边缘设备。

Conclusion: GC-4DGS有效缓解了稀疏输入下动态场景高斯加权渲染的几何不一致问题，提升了渲染质量与效率，为数字孪生等AIoT应用中的实时动态场景合成提供了更实用的解决方案。

Abstract: Gaussian Splatting has been considered as a novel way for view synthesis of dynamic scenes, which shows great potential in AIoT applications such as digital twins. However, recent dynamic Gaussian Splatting methods significantly degrade when only sparse input views are available, limiting their applicability in practice. The issue arises from the incoherent learning of 4D geometry as input views decrease. This paper presents GC-4DGS, a novel framework that infuses geometric consistency into 4D Gaussian Splatting (4DGS), offering real-time and high-quality dynamic scene rendering from sparse input views. While learning-based Multi-View Stereo (MVS) and monocular depth estimators (MDEs) provide geometry priors, directly integrating these with 4DGS yields suboptimal results due to the ill-posed nature of sparse-input 4D geometric optimization. To address these problems, we introduce a dynamic consistency checking strategy to reduce estimation uncertainties of MVS across spacetime. Furthermore, we propose a global-local depth regularization approach to distill spatiotemporal-consistent geometric information from monocular depths, thereby enhancing the coherent geometry and appearance learning within the 4D volume. Extensive experiments on the popular N3DV and Technicolor datasets validate the effectiveness of GC-4DGS in rendering quality without sacrificing efficiency. Notably, our method outperforms RF-DeRF, the latest dynamic radiance field tailored for sparse-input dynamic view synthesis, and the original 4DGS by 2.62dB and 1.58dB in PSNR, respectively, with seamless deployability on resource-constrained IoT edge devices.

</details>


### [159] [GOATex: Geometry & Occlusion-Aware Texturing](https://arxiv.org/abs/2511.23051)
*Hyunjin Kim,Kunho Kim,Adam Lee,Wonkwang Lee*

Main category: cs.CV

TL;DR: GOATex 是一种用于 3D 网格贴图的扩散模型，能够在保持高质量纹理的同时，有效处理网格内部和外部的可见与遮挡区域，最终实现无缝的全表面高保真纹理生成。


<details>
  <summary>Details</summary>
Motivation: 现有的 3D 网格贴图方法对可见区域处理较好，但对于被遮挡的内部区域纹理处理能力弱，导致纹理不完整或出现缝隙。因此，亟需一种能兼顾网格内部与外部区域无缝贴图的新方法。

Method: 提出基于 hit levels（通过多视角射线投射评估网格面深度）的遮挡感知分层方案，将网格面按可见性分为从外到内的多层。采用两阶段可见性控制逐步揭示内部结构，每层均使用预训练扩散模型单独生成纹理。最后，通过软 UV 空间混合技术，依据视角置信度加权融合多层纹理，实现平滑过渡和无缝拼接。

Result: 实验证明，GOATex 在可见和遮挡表面均能生成高质量、无缝纹理表现，整体效果超越了现有方法。

Conclusion: GOATex 无需对预训练扩散模型进行耗时微调; 外部和内部可通过不同指令独立控制纹理风格，实现分层外观精细调整，拓展了 3D 网格表面纹理生成的能力。

Abstract: We present GOATex, a diffusion-based method for 3D mesh texturing that generates high-quality textures for both exterior and interior surfaces. While existing methods perform well on visible regions, they inherently lack mechanisms to handle occluded interiors, resulting in incomplete textures and visible seams. To address this, we introduce an occlusion-aware texturing framework based on the concept of hit levels, which quantify the relative depth of mesh faces via multi-view ray casting. This allows us to partition mesh faces into ordered visibility layers, from outermost to innermost. We then apply a two-stage visibility control strategy that progressively reveals interior regions with structural coherence, followed by texturing each layer using a pretrained diffusion model. To seamlessly merge textures obtained across layers, we propose a soft UV-space blending technique that weighs each texture's contribution based on view-dependent visibility confidence. Empirical results demonstrate that GOATex consistently outperforms existing methods, producing seamless, high-fidelity textures across both visible and occluded surfaces. Unlike prior works, GOATex operates entirely without costly fine-tuning of a pretrained diffusion model and allows separate prompting for exterior and interior mesh regions, enabling fine-grained control over layered appearances. For more qualitative results, please visit our project page: https://goatex3d.github.io/.

</details>


### [160] [Image Valuation in NeRF-based 3D reconstruction](https://arxiv.org/abs/2511.23052)
*Grigorios Aris Cheimariotis,Antonis Karakottas,Vangelis Chatzis,Angelos Kanlis,Dimitrios Zarpalas*

Main category: cs.CV

TL;DR: 本论文提出了一种评估每张输入图片对基于NeRF的三维场景重建贡献的方法，并验证了此方法在复杂场景下的有效性。


<details>
  <summary>Details</summary>
Motivation: 在NeRF进行三维重建时，输入图片的质量和贡献存在差异。在数据变现和价值衡量日益重要的背景下，准确评估每张图片对最终重建结果的贡献对数据筛选与价值分配具有重要意义。

Method: 提出基于PSNR和MSE的指标，量化每张图片对NeRF重建质量的贡献。通过逐步剔除低贡献图片，并分析对重建保真度的影响，来验证评估方法的有效性。

Result: 实验表明，剔除低贡献的图片会对最终重建效果产生显著影响，证明了所提贡献度评估方法的有效性和实用性。

Conclusion: 文中方法能有效鉴别输入图片对3D重建的实际作用，为数据价值评估和优化重建质量提供了新的思路和工具。

Abstract: Data valuation and monetization are becoming increasingly important across domains such as eXtended Reality (XR) and digital media. In the context of 3D scene reconstruction from a set of images -- whether casually or professionally captured -- not all inputs contribute equally to the final output. Neural Radiance Fields (NeRFs) enable photorealistic 3D reconstruction of scenes by optimizing a volumetric radiance field given a set of images. However, in-the-wild scenes often include image captures of varying quality, occlusions, and transient objects, resulting in uneven utility across inputs. In this paper we propose a method to quantify the individual contribution of each image to NeRF-based reconstructions of in-the-wild image sets. Contribution is assessed through reconstruction quality metrics based on PSNR and MSE. We validate our approach by removing low-contributing images during training and measuring the resulting impact on reconstruction fidelity.

</details>


### [161] [Evaluating the Clinical Impact of Generative Inpainting on Bone Age Estimation](https://arxiv.org/abs/2511.23066)
*Felipe Akio Matsuoka,Eduardo Moreno J. M. Farina,Augusto Sarquis Serpa,Soraya Monteiro,Rodrigo Ragazzini,Nitamar Abdala,Marcelo Straus Takahashi,Felipe Campos Kitamura*

Main category: cs.CV

TL;DR: 本文探讨了使用生成式基础模型对儿童手部X光片进行图像修复去除视觉伪影的效果，发现虽然修复后的图像在视觉上更真实，但会显著降低医学AI在骨龄和性别预测任务中的表现，因此强调在将生成模型用于临床前须严格验证其可靠性。


<details>
  <summary>Details</summary>
Motivation: 医学影像经常包含非解剖标记等伪影，影响临床AI模型分析。生成式基础模型能修复图像，但其对医学AI性能的实际影响尚不明确，尤其是在任务相关的细微特征保留方面。因此，作者希望评估生成模型修复对下游医学AI任务的影响及安全性。

Method: 作者选用RSNA Bone Age Challenge数据集，针对200张儿童手部原始X光片，利用gpt-image-1模型及自然语言提示生成了600张目标去除非解剖伪影的修复图像。通过深度学习集成模型对骨龄和性别进行预测，分别使用MAE和AUC衡量AI性能，并对图像像素分布变化进行了结构性分析。

Result: 修复处理会显著降低AI模型性能：骨龄预测误差（MAE）从6.26增加至30.11个月，性别分类AUC从0.955降至0.704。修复图像像素显示出明显偏移和不一致，说明结构被改变，且这种变动无法简单通过校准修正。

Conclusion: 基础模型生成的视觉上真实的修复图像会掩盖关键医学特征并引入潜在偏差，即使编辑区域限制在非诊断区。因此，在将此类生成工具应用于临床AI前，必须针对具体任务进行严格验证。

Abstract: Generative foundation models can remove visual artifacts through realistic image inpainting, but their impact on medical AI performance remains uncertain. Pediatric hand radiographs often contain non-anatomical markers, and it is unclear whether inpainting these regions preserves features needed for bone age and gender prediction. To evaluate the clinical reliability of generative model-based inpainting for artifact removal, we used the RSNA Bone Age Challenge dataset, selecting 200 original radiographs and generating 600 inpainted versions with gpt-image-1 using natural language prompts to target non-anatomical artifacts. Downstream performance was assessed with deep learning ensembles for bone age estimation and gender classification, using mean absolute error (MAE) and area under the ROC curve (AUC) as metrics, and pixel intensity distributions to detect structural alterations. Inpainting markedly degraded model performance: bone age MAE increased from 6.26 to 30.11 months, and gender classification AUC decreased from 0.955 to 0.704. Inpainted images displayed pixel-intensity shifts and inconsistencies, indicating structural modifications not corrected by simple calibration. These findings show that, although visually realistic, foundation model-based inpainting can obscure subtle but clinically relevant features and introduce latent bias even when edits are confined to non-diagnostic regions, underscoring the need for rigorous, task-specific validation before integrating such generative tools into clinical AI workflows.

</details>


### [162] [Buffer replay enhances the robustness of multimodal learning under missing-modality](https://arxiv.org/abs/2511.23070)
*Hongye Zhu,Xuan Liu,Yanwen Ba,Jingye Xue,Shigeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为REplay Prompting（REP）的新方法，有效应对多模态模型在缺失模态下的性能退化问题，并且在多种基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在实际应用中经常面临部分模态数据缺失，现有策略要么需要高昂的计算资源合成缺失模态，要么仅依赖相邻层信息，忽略了跨层和全局上下文信息，导致鲁棒性不足，因此需要一种更高效且鲁棒的方法应对模态缺失。

Method: REP（REplay Prompting）方法包括：1）通过残差旁路机制为每个模态构建特征缓存，并在网络深层重放早期特征以减缓信息衰减；2）采用“专有-共享”特征解耦策略，分别通过专有缓存保留模态特异信息，通过共享缓存获取跨模态语义；3）设计了任务感知的动态初始化机制，针对不同缺模情况灵活配置缓存，提高模型稳定性与泛化能力。

Result: 在视觉-语言、视觉-语言-音频以及时序多模态等多项真实基准测试中，REP在单一和多模态缺失情形下均显著优于现有方法，并且引入的参数开销极小。

Conclusion: REP是一种轻量且高效的多模态鲁棒学习范式，能够在多模态缺失环境下保持卓越性能，具有明确的应用前景和推广价值。

Abstract: Missing modalities consistently lead to significant performance degradation in multimodal models. Existing approaches either synthesize missing modalities at high computational cost or apply prompt-based fine-tuning that relies only on adjacent-layer features and overlooks long-distance contextual information, which may offer additional tolerance to errors when one or more modalities are missing. To address this, we introduce REplay Prompting (REP): (1) construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases; (2) employ a private-shared feature decoupling strategy, where private buffers preserve modality-specific signals and shared buffers encode cross-modal semantics; and (3) design a task-aware dynamic initialization mechanism to configure these buffers differently, improving stability and generalization under diverse missing-modality conditions. Experiments on vision-language, vision-language-audio, and temporal multimodal benchmarks demonstrate that REP consistently outperforms prior methods under both single- and multi-modality missing scenarios, while introducing only negligible parameter overhead. These results establish REP as a lightweight and effective paradigm for robust multimodal learning in challenging missing-modality environments.

</details>


### [163] [SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2511.23075)
*Ruosen Zhao,Zhikang Zhang,Jialei Xu,Jiahao Chang,Dong Chen,Lingyun Li,Weijian Sun,Zizhuang Wei*

Main category: cs.CV

TL;DR: SpaceMind是一种全新多模态大语言模型，仅依赖RGB输入实现3D空间推理，显著提升了距离估算、尺寸比较和多视角一致性等任务表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉-语言模型（VLMs）在多模态理解方面表现强劲，但在3D空间推理（如距离估算、尺寸比较、跨视角一致性）方面仍有不足，现有方法往往依赖额外的3D信息或是浅层几何特征融合，难以高效、准确地捕捉空间关系。

Method: 提出名为SpaceMind的新型多模态LLM，仅使用RGB图像，采用双编码架构：空间理解编码器（VGGT）与2D视觉编码器（InternViT）。创新性地将摄像头表示作为主动引导模态，引入Camera-Guided Modality Fusion模块，通过摄像头条件偏置空间token、分配反映几何重要性的权重，并用摄像头嵌入门控融合表征，取代传统浅层融合方式。

Result: SpaceMind在VSI-Bench、SQA3D和SPBench等多个空间推理基准上取得新SOTA，尤其在VSI-Bench和SPBench上大幅超越目前开源及专有系统，在SQA3D上也达到最优性能。

Conclusion: 通过摄像头引导的模态融合（Camera-Guided Modality Fusion），能为大规模视觉-语言模型提供有效且实用的空间归纳偏置，从而显著提升其空间感知能力。代码和模型将开源，支持后续研究发展。

Abstract: Large vision-language models (VLMs) show strong multimodal understanding but still struggle with 3D spatial reasoning, such as distance estimation, size comparison, and cross-view consistency. Existing 3D-aware methods either depend on auxiliary 3D information or enhance RGB-only VLMs with geometry encoders through shallow feature fusion. We propose SpaceMind, a multimodal large language model explicitly designed for spatial reasoning solely from RGB inputs. The model adopts a dual-encoder architecture, integrating VGGT as a spatial understanding encoder and InternViT as a 2D visual encoder. The key idea is to treat the camera representation as an active guiding modality rather than passive metadata. Specifically, SpaceMind introduces a lightweight Camera-Guided Modality Fusion module before the language model to replace shallow fusion. It applies camera-conditioned biasing to spatial tokens, assigns query-independent weights reflecting their geometric importance, and uses the camera embedding to gate the fused representation. Empirically, SpaceMind establishes new state-of-the-art results on VSI-Bench, SQA3D and SPBench, surpassing both open and proprietary systems on VSI-Bench and SPBench by large margins and achieving state-of-the-art performance on SQA3D. These results demonstrate that camera-guided modality fusion is an effective and practical inductive bias for equipping VLMs with genuinely spatially grounded intelligence. We will release code and model checkpoints to support future research.

</details>


### [164] [Implementation of a Skin Lesion Detection System for Managing Children with Atopic Dermatitis Based on Ensemble Learning](https://arxiv.org/abs/2511.23082)
*Soobin Jeon,Sujong Kim,Dongmahn Seo*

Main category: cs.CV

TL;DR: 韩国数字医疗市场快速发展，针对异位性皮炎与银屑病等皮肤病的主观诊断问题，提出了基于集成学习的皮肤病变检测系统（ENSEL），有效提升了诊断准确率与响应速度。


<details>
  <summary>Details</summary>
Motivation: 异位性皮炎等皮肤病诊断高度依赖主观判断，易导致误诊，且实际临床获取的皮肤图像质量有限。借由数字医疗市场发展及医用数据利用政策的推进，亟需客观、准确且快速的自动化诊断工具。

Method: 提出ENSEL系统，利用集成多种深度学习模型的方法对真实用户拍摄的皮肤病变图像进行检测，同时重点关注在非高质量图像下的准确率和检测速度。

Result: ENSEL系统在大多数皮肤病变图像上的召回率较高，且处理速度快于1秒。实验用实际用户采集的皮肤图像，随机抽样测量了系统的准确性和响应时间。

Conclusion: ENSEL系统能客观、快速、准确地诊断皮肤病变，有助于促进数字医疗与AI诊断在临床实际场景中的应用。

Abstract: The amendments made to the Data 3 Act and impact of COVID-19 have fostered the growth of digital healthcare market and promoted the use of medical data in artificial intelligence in South Korea. Atopic dermatitis, a chronic inflammatory skin disease, is diagnosed via subjective evaluations without using objective diagnostic methods, thereby increasing the risk of misdiagnosis. It is also similar to psoriasis in appearance, further complicating its accurate diagnosis. Existing studies on skin diseases have used high-quality dermoscopic image datasets, but such high-quality images cannot be obtained in actual clinical settings. Moreover, existing systems must ensure accuracy and fast response times. To this end, an ensemble learning-based skin lesion detection system (ENSEL) was proposed herein. ENSEL enhanced diagnostic accuracy by integrating various deep learning models via an ensemble approach. Its performance was verified by conducting skin lesion detection experiments using images of skin lesions taken by actual users. Its accuracy and response time were measured using randomly sampled skin disease images. Results revealed that ENSEL achieved high recall in most images and less than 1s s processing speed. This study contributes to the objective diagnosis of skin lesions and promotes the advancement of digital healthcare.

</details>


### [165] [NumeriKontrol: Adding Numeric Control to Diffusion Transformers for Instruction-based Image Editing](https://arxiv.org/abs/2511.23105)
*Zhenyu Xu,Xiaoqi Shen,Haotian Nan,Xinyu Zhang*

Main category: cs.CV

TL;DR: 本文提出了NumeriKontrol框架，在基于指令的图像编辑中引入了数字（数值）调控，实现了对编辑强度的精细控制。通过与扩散模型无缝集成，支持多条件指令和高精度调整。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本指令的图像编辑无法实现对编辑强度的精细把控，往往只支持粗粒度或模糊表达。研究者希望让用户用直观、可量化的方式进行精准调节，提高图像编辑的可控性和实用性。

Method: NumeriKontrol通过数值适配器（Numeric Adapter）对连续数值进行编码，并以可插拔方式注入到扩散模型，实现图像属性的连续可控编辑。采用任务分离设计，支持零样本多条件指令顺序无关的编辑，并通过高保真渲染引擎和DSLR相机合成高质量的训练数据，构建了带有准确数值标签的CAT数据集。

Result: 实验表明，NumeriKontrol在多种属性编辑场景下，能够准确、连续且稳定地实现尺度控制，效果显著优于仅用指令或其他粗粒度方法。

Conclusion: NumeriKontrol极大拓展了基于自然语言指令的图像编辑能力，实现了精确、可扩展且用户可控的图像属性操作，对提升相关领域的应用体验和技术进步具有积极意义。

Abstract: Instruction-based image editing enables intuitive manipulation through natural language commands. However, text instructions alone often lack the precision required for fine-grained control over edit intensity. We introduce NumeriKontrol, a framework that allows users to precisely adjust image attributes using continuous scalar values with common units. NumeriKontrol encodes numeric editing scales via an effective Numeric Adapter and injects them into diffusion models in a plug-and-play manner. Thanks to a task-separated design, our approach supports zero-shot multi-condition editing, allowing users to specify multiple instructions in any order. To provide high-quality supervision, we synthesize precise training data from reliable sources, including high-fidelity rendering engines and DSLR cameras. Our Common Attribute Transform (CAT) dataset covers diverse attribute manipulations with accurate ground-truth scales, enabling NumeriKontrol to function as a simple yet powerful interactive editing studio. Extensive experiments show that NumeriKontrol delivers accurate, continuous, and stable scale control across a wide range of attribute editing scenarios. These contributions advance instruction-based image editing by enabling precise, scalable, and user-controllable image manipulation.

</details>


### [166] [MathSight: A Benchmark Exploring Have Vision-Language Models Really Seen in University-Level Mathematical Reasoning?](https://arxiv.org/abs/2511.23112)
*Yuandong Wang,Yao Cui,Yuxin Zhao,Zhen Yang,Yangfu Zhu,Zhenzhou Shao*

Main category: cs.CV

TL;DR: 本论文提出了MathSight基准，专门用于分析VLMs在多模态数学推理中视觉信息的真实贡献，并发现随着问题难度增加，视觉信息的作用减弱。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多模态数学推理上取得了巨大进展，但目前尚不清楚视觉信息对推理的真实价值，已有基准往往无法剥离视觉模态的具体作用。作者希望通过新基准量化视觉信息贡献，推动模型实现更真实的视觉推理能力。

Method: 作者设计了MathSight大学级多模态数学推理基准，每道题目都包含图片原图、手绘版、实拍版和纯文本版本，可以严格对比视觉输入的效果。实验中，比较了多种主流VLMs（如Qwen3-VL和GPT-5）在不同难度、不同模态下的表现。

Result: 实验证明，随着题目难度的提升，视觉信息对推理的促进作用逐渐减弱。在部分情况下，Qwen3-VL仅使用文本输入便超越了采用图像输入的多模态版本和GPT-5。

Conclusion: 现有VLMs尚未充分利用视觉信息进行推理，MathSight基准对于推动未来模型实现真正基于视觉的推理具有重要意义。

Abstract: Recent advances in Vision-Language Models (VLMs) have achieved impressive progress in multimodal mathematical reasoning. Yet, how much visual information truly contributes to reasoning remains unclear. Existing benchmarks report strong overall performance but seldom isolate the role of the image modality, leaving open whether VLMs genuinely leverage visual understanding or merely depend on linguistic priors. To address this, we present MathSight, a university-level multimodal mathematical reasoning benchmark designed to disentangle and quantify the effect of visual input. Each problem includes multiple visual variants -- original, hand-drawn, photo-captured -- and a text-only condition for controlled comparison. Experiments on state-of-the-art VLMs reveal a consistent trend: the contribution of visual information diminishes with increasing problem difficulty. Remarkably, Qwen3-VL without any image input surpasses both its multimodal variants and GPT-5, underscoring the need for benchmarks like MathSight to advance genuine vision-grounded reasoning in future models.

</details>


### [167] [db-SP: Accelerating Sparse Attention for Visual Generative Models with Dual-Balanced Sequence Parallelism](https://arxiv.org/abs/2511.23113)
*Siqi Chen,Ke Hong,Tianchen Zhao,Ruiqi Xie,Zhenhua Zhu,Xudong Zhang,Yu Wang*

Main category: cs.CV

TL;DR: 本文旨在通过提出新的稀疏感知序列并行技术（db-SP），解决视觉生成中扩散模型推理时因块式稀疏注意力机制引起的工作负载不均问题，实现显著推理加速。


<details>
  <summary>Details</summary>
Motivation: 在视觉生成任务中，Diffusion Transformer (DiT) 的推理延迟较高。通过序列并行技术可以降低延迟，但当模型使用块式稀疏注意力时，现有并行方法会因为注意力头和稀疏块分布不均而导致严重的负载不均，从而限制并行性能。因此，研究者试图设计一种能够在稀疏结构下高效平衡负载的新型并行方法。

Method: 作者首先提出了“稀疏不均衡比”这一量化负载不均的新指标，并据此设计了db-SP方法。该方法通过头-块双层划分，实现头和块两个层级的负载均衡。此外，db-SP能够根据去噪步骤和网络层的变化，动态调整头和块两个维度的并行度，以适应不断变化的稀疏模式。

Result: 实验证明，db-SP在视觉生成任务中对比最先进的序列并行方法，端到端平均加速比为1.25倍，注意力机制部分加速比达到1.40倍，且开销极低。

Conclusion: db-SP作为一种稀疏感知的序列并行方法，有效解决了块式稀疏注意力带来的严重不均问题，实现扩散模型推理的更高效加速，具有实际工程价值。

Abstract: Scaling Diffusion Transformer (DiT) inference via sequence parallelism is critical for reducing latency in visual generation, but is severely hampered by workload imbalance when applied to models employing block-wise sparse attention. The imbalance stems from the inherent variation in sparsity across attention heads and the irregular distribution of dense blocks within the sparse mask, when sequence parallelism is applied along the head dimension (as in Ulysses) or the block dimension (as in Ring Attention). In this paper, we formalize a sparse imbalance ratio to quantify the imbalance, and propose db-SP, a sparsity-aware sequence parallelism technique that tackles the challenge. db-SP contains a dual-level partitioning approach that achieves near-perfect workload balance at both the head and block levels with negligible overhead. Furthermore, to handle the evolving sparsity patterns across denoising steps and layers, db-SP dynamically determines the parallel degrees for the head and block dimensions at runtime. Experimental results demonstrate that db-SP delivers an end-to-end speedup of 1.25x and an attention-specific speedup of 1.40x over state-of-the-art sequence parallel methods on average. Code is available at https://github.com/thu-nics/db-SP.

</details>


### [168] [Analyzing Image Beyond Visual Aspect: Image Emotion Classification via Multiple-Affective Captioning](https://arxiv.org/abs/2511.23115)
*Zibo Zhou,Zhengjun Zhai,Huimin Chen,Wei Dai,Hansen Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的基于文本描述的图像情感分类方法，通过先生成情感化标题文本再进行分类，显著提升图像情感识别的准确率，并有效弥合了视觉模型中的“情感鸿沟”问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的预训练模型在图像情感分析任务中效果有限，主要受困于“情感鸿沟”——即视觉特征与情感表达之间的差距。心理学研究表明，语言在情感表达上具有更好的表现，因此寻找一种利用文本的方式提升情感分类效果成为亟需解决的问题。

Method: 作者提出了Affective Captioning for Image Emotion Classification (ACIEC) 方法：1）通过层次化多级对比损失检测图像中的情感概念；2）利用情感属性的链式推理生成情感化的描述文本；3）将上述信息输入预训练语言模型实现情感分类；4）设计了基于语义相似采样的对比损失以缓解类别内差异大、类别间差异小的问题；5）还考虑了包含内嵌文本的图像。

Result: 大量实验表明，提出的方法在多个数据集上能够有效缩小“情感鸿沟”，在图像情感分类性能上优于现有方法。

Conclusion: ACIEC方法能够充分利用文本对情感信息的丰富表达能力，不仅提升了图像情感识别效果，还拓展了情感分析对特殊类型（如含文本图像）的适用性。

Abstract: Image emotion classification (IEC) is a longstanding research field that has received increasing attention with the rapid progress of deep learning. Although recent advances have leveraged the knowledge encoded in pre-trained visual models, their effectiveness is constrained by the "affective gap" , limits the applicability of pre-training knowledge for IEC tasks. It has been demonstrated in psychology that language exhibits high variability, encompasses diverse and abundant information, and can effectively eliminate the "affective gap". Inspired by this, we propose a novel Affective Captioning for Image Emotion Classification (ACIEC) to classify image emotion based on pure texts, which effectively capture the affective information in the image. In our method, a hierarchical multi-level contrastive loss is designed for detecting emotional concepts from images, while an emotional attribute chain-of-thought reasoning is proposed to generate affective sentences. Then, a pre-trained language model is leveraged to synthesize emotional concepts and affective sentences to conduct IEC. Additionally, a contrastive loss based on semantic similarity sampling is designed to solve the problem of large intra-class differences and small inter-class differences in affective datasets. Moreover, we also take the images with embedded texts into consideration, which were ignored by previous studies. Extensive experiments illustrate that our method can effectively bridge the affective gap and achieve superior results on multiple benchmarks.

</details>


### [169] [DNA-Prior: Unsupervised Denoise Anything via Dual-Domain Prior](https://arxiv.org/abs/2511.23124)
*Yanqi Cheng,Chun-Wun Cheng,Jim Denholm,Thiago Lima,Javier A. Montoya-Zegarra,Richard Goodwin,Carola-Bibiane Schönlieb,Angelica I Aviles-Rivero*

Main category: cs.CV

TL;DR: 本文提出DNA-Prior，一种无需外部标注数据的通用无监督医学图像去噪框架，结合深度网络隐式先验与频谱-空间显式先验，实现多模态下稳定去噪与结构保持。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像去噪方法多依赖大规模有标签数据和监督学习，但在实际临床中，模态多样且标注稀缺，限制了这些方法的通用性。作者希望提出不依赖标注、适用多模态的去噪手段。

Method: DNA-Prior采用混合先验：一方面通过深度网络参数化实现隐式结构先验，另一方面在频域通过保真性项与空域通过正则化项引入显式频谱-空间先验。该双域形式综合优化，既保证频谱特征，亦维护局部解剖结构。

Result: 在多种医学成像模态和多种噪声条件下，DNA-Prior都能实现稳定噪声抑制和结构信息保留，无需外部训练数据与特定模态调参。

Conclusion: DNA-Prior为医学成像提供了一种理论严谨、无监督、适用多模态和无需标注数据的去噪工具，有望提升实际临床图像处理的适用性和鲁棒性。

Abstract: Medical imaging pipelines critically rely on robust denoising to stabilise downstream tasks such as segmentation and reconstruction. However, many existing denoisers depend on large annotated datasets or supervised learning, which restricts their usability in clinical environments with heterogeneous modalities and limited ground-truth data. To address this limitation, we introduce DNA-Prior, a universal unsupervised denoising framework that reconstructs clean images directly from corrupted observations through a mathematically principled hybrid prior. DNA-Prior integrates (i) an implicit architectural prior, enforced through a deep network parameterisation, with (ii) an explicit spectral-spatial prior composed of a frequency-domain fidelity term and a spatial regularisation functional. This dual-domain formulation yields a well-structured optimisation problem that jointly preserves global frequency characteristics and local anatomical structure, without requiring any external training data or modality-specific tuning. Experiments across multiple modalities show that DNA achieves consistent noise suppression and structural preservation under diverse noise conditions.

</details>


### [170] [DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation](https://arxiv.org/abs/2511.23127)
*Hongfei Zhang,Kanghao Chen,Zixin Zhang,Harold Haodong Chen,Yuanhuiyi Lyu,Yuqi Zhang,Shuai Yang,Kun Zhou,Yingcong Chen*

Main category: cs.CV

TL;DR: 本文提出DualCamCtrl，一种新颖的端到端扩散模型，通过双分支结构协同生成与摄像机一致的RGB和深度序列，有效提升了摄像机控制下的视频生成质量，显著减少了摄像机运动误差。


<details>
  <summary>Details</summary>
Motivation: 现有摄像机控制视频生成方法虽然采用射线条件表达摄像机姿态，但对场景理解和几何感知不足，导致生成视频与指定摄像机轨迹的契合度较低。

Method: DualCamCtrl采用双分支结构，分别生成RGB序列和深度序列，并通过语义引导的互助对齐机制（SIGMA）实现RGB-深度信息的高效融合，提升外观和几何建模的解耦能力。此外分析了深度信息和摄像机姿态在不同去噪阶段的作用。

Result: 大量实验显示，DualCamCtrl在摄像机一致性和控制精度方面明显优于现有方法，摄像机运动误差降低40%以上。

Conclusion: 该方法极大提高了摄像机可控视频生成的场景理解、几何感知和轨迹契合能力，为该领域提供了新的有效技术方案。

Abstract: This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation. Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness. DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences. To harmonize these two modalities, we further propose the Semantic Guided Mutual Alignment (SIGMA) mechanism, which performs RGB-depth fusion in a semantics-guided and mutually reinforced manner. These designs collectively enable DualCamCtrl to better disentangle appearance and geometry modeling, generating videos that more faithfully adhere to the specified camera trajectories. Additionally, we analyze and reveal the distinct influence of depth and camera poses across denoising stages and further demonstrate that early and late stages play complementary roles in forming global structure and refining local details. Extensive experiments demonstrate that DualCamCtrl achieves more consistent camera-controlled video generation, with over 40\% reduction in camera motion errors compared with prior methods. Our project page: https://soyouthinkyoucantell.github.io/dualcamctrl\-page/

</details>


### [171] [InstanceV: Instance-Level Video Generation](https://arxiv.org/abs/2511.23146)
*Yuheng Chen,Teng Hu,Jiangning Zhang,Zhucun Xue,Ran Yi,Lizhuang Ma*

Main category: cs.CV

TL;DR: 本文提出了InstanceV框架，实现了文本到视频扩散模型的实例级可控生成和全局语义一致性，提升了小实例的生成质量，并建立了新基准InstanceBench，实验表明其在多项指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的文本到视频生成模型主要受限于只能通过文本进行粗粒度控制，无法对视频中具体实例进行精细调节，缺乏实例级的可控能力，难以满足实际应用对复杂场景和多对象生成的需求。

Method: 作者提出InstanceV框架，包含三大创新：（1）实例感知的Masked Cross-Attention机制，增强实例级信息与空间位置的绑定；（2）Shared Timestep-Adaptive Prompt Enhancement模块，有效连接局部实例与全局语义，提升一致性；（3）空间感知的无条件引导方法，训练和推理阶段提升小实例的可见性。同时，提出了融合通用指标和实例感知指标的新评测基准InstanceBench。

Result: 实验显示，InstanceV在实例级可控性、全局语义一致性以及生成质量方面均优于SOTA模型，在多个定性和定量评测指标上取得更好成绩，特别是在小实例生成和实例感知性指标上表现突出。

Conclusion: InstanceV有效突破了当前文本到视频生成领域在实例级控制上的瓶颈，实现了更精准和灵活的视频内容生成，并为后续实例级视频生成评测提供了更全面的工具。

Abstract: Recent advances in text-to-video diffusion models have enabled the generation of high-quality videos conditioned on textual descriptions. However, most existing text-to-video models rely solely on textual conditions, lacking general fine-grained controllability over video generation. To address this challenge, we propose InstanceV, a video generation framework that enables i) instance-level control and ii) global semantic consistency. Specifically, with the aid of proposed Instance-aware Masked Cross-Attention mechanism, InstanceV maximizes the utilization of additional instance-level grounding information to generate correctly attributed instances at designated spatial locations. To improve overall consistency, We introduce the Shared Timestep-Adaptive Prompt Enhancement module, which connects local instances with global semantics in a parameter-efficient manner. Furthermore, we incorporate Spatially-Aware Unconditional Guidance during both training and inference to alleviate the disappearance of small instances. Finally, we propose a new benchmark, named InstanceBench, which combines general video quality metrics with instance-aware metrics for more comprehensive evaluation on instance-level video generation. Extensive experiments demonstrate that InstanceV not only achieves remarkable instance-level controllability in video generation, but also outperforms existing state-of-the-art models in both general quality and instance-aware metrics across qualitative and quantitative evaluations.

</details>


### [172] [Cascaded Robust Rectification for Arbitrary Document Images](https://arxiv.org/abs/2511.23150)
*Chaoyun Wang,Quanxin Huang,I-Chao Shen,Takeo Igarashi,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种多阶段的文档矫正框架，有效解决了现实场景下因拍照视角和纸张物理变形导致的文档失真难题，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，拍照文档常常因拍摄角度和纸张弯曲、折叠等问题产生复杂的形变，给文档矫正带来较大挑战。现有方法在处理极端场景或边界不完整时效果有限，评价标准也存在不足。因此，亟需更有效的矫正方案及评价方法。

Method: 作者提出了一种新颖的多阶段矫正框架，分为三步：先通过全局仿射变换修正视角畸变，再校正由纸张弯折导致的几何形变，最后利用内容感知的迭代流程消除细致的内容失真。同时，提出了两类新的评价指标：一种是与布局对齐的OCR评测指标（AED/ACER），能更稳定且独立于OCR引擎的布局分析；另一种是为边界不完整文档设计的掩码几何指标（AD-M/AAD-M）。

Result: 在多个具有挑战性的公开数据集上进行了大量实验，所提方法在AAD等主流指标上取得了14.1%到34.7%的显著提升，达到了新的SOTA水平，实际应用效果优异。

Conclusion: 该文方法有效应对了复杂文档畸变，理论创新和实验结果都表明其优越性，并为文档矫正的评价体系提供了有益补充。

Abstract: Document rectification in real-world scenarios poses significant challenges due to extreme variations in camera perspectives and physical distortions. Driven by the insight that complex transformations can be decomposed and resolved progressively, we introduce a novel multi-stage framework that progressively reverses distinct distortion types in a coarse-to-fine manner. Specifically, our framework first performs a global affine transformation to correct perspective distortions arising from the camera's viewpoint, then rectifies geometric deformations resulting from physical paper curling and folding, and finally employs a content-aware iterative process to eliminate fine-grained content distortions. To address limitations in existing evaluation protocols, we also propose two enhanced metrics: layout-aligned OCR metrics (AED/ACER) for a stable assessment that decouples geometric rectification quality from the layout analysis errors of OCR engines, and masked AD/AAD (AD-M/AAD-M) tailored for accurately evaluating geometric distortions in documents with incomplete boundaries. Extensive experiments show that our method establishes new state-of-the-art performance on multiple challenging benchmarks, yielding a substantial reduction of 14.1\%--34.7\% in the AAD metric and demonstrating superior efficacy in real-world applications. The code will be publicly available at https://github.com/chaoyunwang/ArbDR.

</details>


### [173] [Learning to Refuse: Refusal-Aware Reinforcement Fine-Tuning for Hard-Irrelevant Queries in Video Temporal Grounding](https://arxiv.org/abs/2511.23151)
*Jin-Seop Lee,SungJoon Lee,SeongJun Jung,Boyang Li,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: 本文针对视频时序定位任务，提出了一种能有效拒绝语义相似但不相关查询的新方法RA-RFT，并构建了支持该任务的全新难拒绝数据集，验证了方法的广泛有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有视频时序定位(VTG)方法默认每个文本查询在视频中总有相关片段，无法应对与视频无关或语义相近但实际不相关（hard-irrelevant）的查询，这导致误判，影响实际应用。

Method: 提出Refusal-Aware Reinforcement Fine-Tuning（RA-RFT）算法，基于GRPO框架，融合格式、拒绝IoU、解释和查询纠正等四个奖励目标，显著增强相关性识别与细致语义推理；同时构建HIVTG数据集，包含大量难拒绝查询及对应拒绝标注。

Result: 实验证明RA-RFT在多种相关性感知的VTG任务上——包括hard-irrelevant VTG、打乱式RA-VTG和人工标注RA-VTG等——均显著提升了拒绝无关查询的能力，对多种LVLM基础VTG模型亦具良好扩展性。

Conclusion: RA-RFT有效解决了传统方法难以处理的“难拒绝”查询问题，提升了VTG的实际适用性，且对不同VTG基础模型具有广泛适配性。

Abstract: Video Temporal Grounding (VTG) aims to localize a temporal segment in a video corresponding to a natural language query. However, existing VTG models assume that a relevant segment always exists, causing them to always predict a target segment even when the query is irrelevant to the video. While recent approaches attempt to handle irrelevant queries, they can only reject those that are entirely unrelated to the video and still fail to handle hard-irrelevant queries that are semantically similar but not actually relevant. To address this, we propose Refusal-Aware Reinforcement Fine-Tuning (RA-RFT) to effectively refuse hard-irrelevant queries in VTG. Our method is based on the Group Relative Policy Optimization (GRPO) framework and integrates four reward objectives-format, refuse-IoU, explain, and query correction-to improve both relevance discrimination and fine-grained semantic reasoning. In addition, to effectively support RA-RFT, we construct a Hard-Irrelevant VTG (HI-VTG) dataset, which includes hard-irrelevant queries and their refusal answers. We demonstrate the effectiveness of our method across various relevance-aware VTG scenarios, including hard-irrelevant VTG, simply-shuffled RA-VTG, and human-annotated RA-VTG settings. We also show that the proposed method is scalable by applying it to various LVLM-based VTG models. Our code is available at https://github.com/JINSUBY/RA-RFT.

</details>


### [174] [REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection](https://arxiv.org/abs/2511.23158)
*Huangsen Cao,Qin Mei,Zhiheng Li,Yuxi Li,Ying Zhang,Chen Li,Zhimeng Zhang,Xin Ding,Yongwei Wang,Jing Lyu,Fei Wu*

Main category: cs.CV

TL;DR: 论文提出了REVEAL-Bench基准和REVEAL方法，通过增强推理和可验证证据链提升AI生成图像的可解释性检测能力，显著提高了检测准确性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着生成式模型的进步，AI生成图像愈加逼真，难以辨别真假，对信息安全和社会信任构成威胁。现有检测方法缺乏真正可验证的解释链，通用性弱，难以支撑高可信度的图像取证。

Method: 提出了REVEAL-Bench多模态基准，利用多个轻量专家模型构建推理证据链，并记录详细的推理、证据解释过程。在此基础上，提出REVEAL框架，结合专家指导的强化学习，通过专门设计的奖励机制兼顾检测准确性、解释一致性和逻辑连贯性，从而自动生成细粒度、可解释且可验证的推理链。

Result: 大量实验表明，REVEAL框架在检测准确性、解释可用性及跨模型泛化方面均显著超越现有最优方法，树立了可解释图像取证的新标杆。

Conclusion: REVEAL-Bench数据集与REVEAL检测框架为AI生成图像检测任务带来结构化推理链与透明证据，极大提升了可解释性和实用性，为应对生成式内容安全挑战提供了有效、可信的新思路。

Abstract: With the rapid advancement of generative models, visually realistic AI-generated images have become increasingly difficult to distinguish from authentic ones, posing severe threats to social trust and information integrity. Consequently, there is an urgent need for efficient and truly explainable image forensic methods. Recent detection paradigms have shifted towards explainable forensics. However, state-of-the-art approaches primarily rely on post-hoc rationalizations or visual discrimination, lacking a verifiable chain of evidence. This reliance on surface-level pattern matching limits the generation of causally grounded explanations and often results in poor generalization. To bridge this critical gap, we introduce \textbf{REVEAL-Bench}, the first reasoning-enhanced multimodal benchmark for AI-generated image detection that is explicitly structured around a chain-of-evidence derived from multiple lightweight expert models, then records step-by-step reasoning traces and evidential justifications. Building upon this dataset, we propose \textbf{REVEAL} (\underline{R}easoning-\underline{e}nhanced Forensic E\underline{v}id\underline{e}nce \underline{A}na\underline{l}ysis), an effective and explainable forensic framework that integrates detection with a novel expert-grounded reinforcement learning. Our reward mechanism is specially tailored to jointly optimize detection accuracy, explanation fidelity, and logical coherence grounded in explicit forensic evidence, enabling REVEAL to produce fine-grained, interpretable, and verifiable reasoning chains alongside its detection outcomes. Extensive experimental results demonstrate that REVEAL significantly enhances detection accuracy, explanation fidelity, and robust cross-model generalization, benchmarking a new state of the art for explainable image forensics.

</details>


### [175] [PowerCLIP: Powerset Alignment for Contrastive Pre-Training](https://arxiv.org/abs/2511.23170)
*Masaki Kawamura,Nakamasa Inoue,Rintaro Yanagi,Hirokatsu Kataoka,Rio Yokota*

Main category: cs.CV

TL;DR: 提出了PowerCLIP，通过“幂集对齐”改进视觉-语言对比学习，有效增强了模型对多个图像区域组合语义的理解，并以更高效的方式实现了这一目标，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有如CLIP的对比预训练模型虽然在视觉-语言任务上表现优异，但对于跨越多个图像区域的复合语义捕捉效果仍有不足。对齐单个文本-图像区域虽有帮助，但无法处理更复杂的组合语义。

Method: 提出PowerCLIP，通过最小化图像区域幂集与文本句法树幂集之间的损失，优化区域与短语的组合对齐。为避免指数级计算开销，设计了高效的非线性聚合器（NLA），实现线性复杂度，并以任意精度近似真实损失。

Result: 实验表明，PowerCLIP在零样本分类和检索任务上优于现有方法，证明了其组合性理解和鲁棒性提升。

Conclusion: PowerCLIP通过高效幂集对齐机制，提升了视觉-语言模型的组合语义对齐能力，在多个下游任务中取得更优表现，具有实际应用潜力。

Abstract: Contrastive vision-language pre-training frameworks such as CLIP have demonstrated impressive zero-shot performance across a range of vision-language tasks. Recent studies have shown that aligning individual text tokens with specific image patches or regions enhances fine-grained compositional understanding. However, it remains challenging to capture compositional semantics that span multiple image regions. To address this limitation, we propose PowerCLIP, a novel contrastive pre-training framework enhanced by powerset alignment, which exhaustively optimizes region-to-phrase alignments by minimizing the loss defined between powersets of image regions and textual parse trees. Since the naive powerset construction incurs exponential computational cost due to the combinatorial explosion in the number of region subsets, we introduce efficient non-linear aggregators (NLAs) that reduce complexity from O(2^M) to O(M) with respect to the number of regions M, while approximating the exact loss value with arbitrary precision. Our extensive experiments demonstrate that PowerCLIP outperforms state-of-the-art methods in zero-shot classification and retrieval tasks, underscoring the compositionality and robustness of our approach. Our code will be made publicly available.

</details>


### [176] [Fast Multi-view Consistent 3D Editing with Video Priors](https://arxiv.org/abs/2511.23172)
*Liyi Chen,Ruihuang Li,Guowen Zhang,Pengfei Wang,Lei Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于生成式视频先验的文本驱动3D编辑方法（ViP3DE），解决多视角一致性和编辑效率问题，显著提升3D编辑质量和速度。


<details>
  <summary>Details</summary>
Motivation: 现有的文本驱动3D编辑方法由于缺乏多视角一致性，经常依赖2D模型对每个视角单独编辑，并通过迭代过程融合结果，导致效率低且编辑结果过于平滑，缺乏细节。

Method: 提出ViP3DE方法，利用预训练视频生成模型中的时序一致性先验，实现单步（single forward pass）全视角一致的3D编辑。具体方法包括：用单一编辑视角作为条件生成其他一致的编辑视角；设计了运动保持噪声混合机制，使生成的视角与指定相机位姿对应；并融入了几何感知去噪增强多视角一致性。

Result: 实验证明，ViP3DE能够在单步推理下完成高质量的3D编辑，编辑质量和速度均大幅领先于现有方法。

Conclusion: ViP3DE创新性地将视频生成先验引入3D编辑领域，有效解决了多视角一致性和效率低下的问题，为文本驱动3D编辑提供了一条高效、高质的新途径。

Abstract: Text-driven 3D editing enables user-friendly 3D object or scene editing with text instructions. Due to the lack of multi-view consistency priors, existing methods typically resort to employing 2D generation or editing models to process each view individually, followed by iterative 2D-3D-2D updating. However, these methods are not only time-consuming but also prone to over-smoothed results because the different editing signals gathered from different views are averaged during the iterative process. In this paper, we propose generative Video Prior based 3D Editing (ViP3DE) to employ the temporal consistency priors from pre-trained video generation models for multi-view consistent 3D editing in a single forward pass. Our key insight is to condition the video generation model on a single edited view to generate other consistent edited views for 3D updating directly, thereby bypassing the iterative editing paradigm. Since 3D updating requires edited views to be paired with specific camera poses, we propose motion-preserved noise blending for the video model to generate edited views at predefined camera poses. In addition, we introduce geometry-aware denoising to further enhance multi-view consistency by integrating 3D geometric priors into video models. Extensive experiments demonstrate that our proposed ViP3DE can achieve high-quality 3D editing results even within a single forward pass, significantly outperforming existing methods in both editing quality and speed.

</details>


### [177] [GeoWorld: Unlocking the Potential of Geometry Models to Facilitate High-Fidelity 3D Scene Generation](https://arxiv.org/abs/2511.23191)
*Yuhao Wan,Lijuan Liu,Jingzhi Zhou,Zihan Zhou,Xuying Zhang,Dongbo Zhang,Shaohui Jiao,Qibin Hou,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: GeoWorld提出通过生成连续视频帧并结合全帧几何特征，显著提升单张图像生成3D场景的质量，克服了以往方法的几何失真与模糊问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频模型的图像到3D场景生成方法，普遍存在几何结构失真和内容模糊的问题，限制了生成结果的真实感和实用性。本研究致力于解决这些问题，提高3D场景生成的精度与一致性。

Method: GeoWorld首先从单张输入图像生成连续的视频帧，然后利用几何模型为每一帧提取丰富的几何特征，并将这些特征作为几何条件辅助视频生成模型。同时引入几何对齐损失（geometry alignment loss）以提升几何结构一致性，并设计几何自适应模块以充分利用几何特征。

Result: 大量实验验证表明，GeoWorld能基于单张图像和预设相机轨迹生成高保真的3D场景，无论从主观视觉质量还是客观评价指标，均优于现有主流方法。

Conclusion: GeoWorld通过创新地结合视频生成和几何建模，极大提升了单图像到3D场景生成的质量与一致性，为相关领域的研究与应用提供了新的有效方案。

Abstract: Previous works leveraging video models for image-to-3D scene generation tend to suffer from geometric distortions and blurry content. In this paper, we renovate the pipeline of image-to-3D scene generation by unlocking the potential of geometry models and present our GeoWorld. Instead of exploiting geometric information obtained from a single-frame input, we propose to first generate consecutive video frames and then take advantage of the geometry model to provide full-frame geometry features, which contain richer information than single-frame depth maps or camera embeddings used in previous methods, and use these geometry features as geometrical conditions to aid the video generation model. To enhance the consistency of geometric structures, we further propose a geometry alignment loss to provide the model with real-world geometric constraints and a geometry adaptation module to ensure the effective utilization of geometry features. Extensive experiments show that our GeoWorld can generate high-fidelity 3D scenes from a single image and a given camera trajectory, outperforming prior methods both qualitatively and quantitatively. Project Page: https://peaes.github.io/GeoWorld/.

</details>


### [178] [Vision Bridge Transformer at Scale](https://arxiv.org/abs/2511.23199)
*Zhenxiong Tan,Zeqing Wang,Xingyi Yang,Songhua Liu,Xinchao Wang*

Main category: cs.CV

TL;DR: 本文提出了Vision Bridge Transformer (ViBT)，一种大规模基于Brownian Bridge模型的条件生成方法，适用于高效的图像和视频翻译任务，并展示了良好的实验效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型难以高效地进行数据到数据之间的转换，且生成流程复杂，作者希望提升条件生成，尤其是在图像和视频编辑与翻译中的效率和精度。

Method: 提出ViBT模型，基于Brownian Bridge理论，不同于传统扩散模型，从数据本身出发建模输入与输出之间的轨迹。采用了大参数量的Transformer作为模型架构，并设计了一个新的方差稳定化速度匹配目标函数以提升训练稳定性和表现。

Result: 20B和1.3B参数规模的ViBT在图像和视频翻译任务上取得了有效的实验成果，证明了模型可扩展性和性能。

Conclusion: 大规模Bridge模型结合Transformer架构与新的训练目标显著提升了图像编辑和复杂视频翻译任务的效果，展示了该方法在条件生成领域的潜力。

Abstract: We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation. Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm. By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks. To support this scale, we adopt a Transformer architecture and propose a variance-stabilized velocity-matching objective for robust training. Together, these advances highlight the power of scaling Bridge Models for instruction-based image editing and complex video translation.

</details>


### [179] [Pathryoshka: Compressing Pathology Foundation Models via Multi-Teacher Knowledge Distillation with Nested Embeddings](https://arxiv.org/abs/2511.23204)
*Christian Grashei,Christian Brechenmacher,Rao Muhammad Umer,Jingsong Liu,Carsten Marr,Ewa Szczurek,Peter J. Schüffler*

Main category: cs.CV

TL;DR: 本文提出了Pathryoshka多教师蒸馏框架，有效减小病理基础模型（FM）的体积，同时保持高性能，实现模型尺寸降低86%-92%，准确率优于同类小型模型7个百分点。


<details>
  <summary>Details</summary>
Motivation: 病理学基础模型参数众多、嵌入维度高，导致计算资源紧张时难以应用于科研或临床场景，需要体积更小、部署更灵活但性能不降低的解决方案。

Method: 受RADIO蒸馏和套娃表示学习启发，提出Pathryoshka多教师蒸馏框架，通过多教师指导，将大模型知识压缩进小模型，并支持动态调整嵌入维度，实现灵活高效的模型压缩。

Result: 在10个公共病理学基准任务上进行验证，Pathryoshka蒸馏模型体积较大模型降低86%-92%，性能保持相当，并且比同规模的单教师蒸馏模型整体准确率高7个百分点。

Conclusion: Pathryoshka显著降低病理基础模型所需计算资源，准确率和表达能力无明显损失，有助于高性能病理AI模型在科研与临床场景的广泛普及和本地化应用。

Abstract: Pathology foundation models (FMs) have driven significant progress in computational pathology. However, these high-performing models can easily exceed a billion parameters and produce high-dimensional embeddings, thus limiting their applicability for research or clinical use when computing resources are tight. Here, we introduce Pathryoshka, a multi-teacher distillation framework inspired by RADIO distillation and Matryoshka Representation Learning to reduce pathology FM sizes while allowing for adaptable embedding dimensions. We evaluate our framework with a distilled model on ten public pathology benchmarks with varying downstream tasks. Compared to its much larger teachers, Pathryoshka reduces the model size by 86-92% at on-par performance. It outperforms state-of-the-art single-teacher distillation models of comparable size by a median margin of 7.0 in accuracy. By enabling efficient local deployment without sacrificing accuracy or representational richness, Pathryoshka democratizes access to state-of-the-art pathology FMs for the broader research and clinical community.

</details>


### [180] [Zero-Shot Multi-Criteria Visual Quality Inspection for Semi-Controlled Industrial Environments via Real-Time 3D Digital Twin Simulation](https://arxiv.org/abs/2511.23214)
*Jose Moises Araya-Martinez,Gautham Mohan,Kenichi Hayakawa Bolaños,Roberto Mendieta,Sarvenaz Sardari,Jens Lambrecht,Jörg Krüger*

Main category: cs.CV

TL;DR: 本文提出了一种面向工业零件视觉质量检测的零样本、无姿态依赖的新方法，通过将真实场景与实时数字孪生体(DT)在RGB-D空间中对比，实现对缺陷的自动检测。该框架在轴向磁通电机质量检测场景下取得了IoU最高63.3%的检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统的工业视觉质量检测系统复杂、对大量数据有较高要求，并不容易推广到半受控的工业环境中。为实现零缺陷制造和减少生产浪费，亟需开发无需大量训练样本、对姿态不敏感且泛化能力强的新型检测方法。

Method: 提出了一种基于实时数字孪生体的视觉质量检测框架：通过已知CAD模型进行目标检测和姿态估计，语义化描述工业场景，实现高效的实时DT渲染。利用RGB-D多模态数据对比真实场景与DT，辅助检测缺陷。提出了可拓展、分层的标注策略，结合姿态、逻辑和结构等多维度缺陷信息。

Result: 在汽车领域轴向磁通电机的实际质量检测中，所提方法在半受控工业环境下实现了最高达63.3%的IoU检测性能，即便仅使用简单的距离度量方法。同时测试了DT创建工具的性能和资源消耗。

Conclusion: 本文框架在低数据、动态生产条件下展现了良好的缺陷检测能力，为后续通用化、低样本工业视觉检测方法的研究提供了坚实基础。

Abstract: Early-stage visual quality inspection is vital for achieving Zero-Defect Manufacturing and minimizing production waste in modern industrial environments. However, the complexity of robust visual inspection systems and their extensive data requirements hinder widespread adoption in semi-controlled industrial settings. In this context, we propose a pose-agnostic, zero-shot quality inspection framework that compares real scenes against real-time Digital Twins (DT) in the RGB-D space. Our approach enables efficient real-time DT rendering by semantically describing industrial scenes through object detection and pose estimation of known Computer-Aided Design models. We benchmark tools for real-time, multimodal RGB-D DT creation while tracking consumption of computational resources. Additionally, we provide an extensible and hierarchical annotation strategy for multi-criteria defect detection, unifying pose labelling with logical and structural defect annotations. Based on an automotive use case featuring the quality inspection of an axial flux motor, we demonstrate the effectiveness of our framework. Our results demonstrate detection performace, achieving intersection-over-union (IoU) scores of up to 63.3% compared to ground-truth masks, even if using simple distance measurements under semi-controlled industrial conditions. Our findings lay the groundwork for future research on generalizable, low-data defect detection methods in dynamic manufacturing settings.

</details>


### [181] [Instruction Tuning of Large Language Models for Tabular Data Generation-in One Day](https://arxiv.org/abs/2511.23220)
*Milad Abdollahzadeh,Abdul Raheem,Zilong Zhao,Uzair Javaid,Kevin Yee,Nalam Venkata Abhishek,Tram Truong-Huu,Biplab Sikdar*

Main category: cs.CV

TL;DR: 本文首次研究了通过指令微调增强大语言模型（LLM）在生成表格数据方面的能力，并在有限的数据和算力条件下取得了与主流商用模型GPT-4o媲美的效果。


<details>
  <summary>Details</summary>
Motivation: 现有大多数工作聚焦于表格数据的问答和推理任务，鲜少关注表格数据生成，且表格指令微调通常资源消耗大。因此，作者希望探索在数据和算力有限情境下能否也有效提升LLM生成表格数据的能力。

Method: （1）构建了高质量表格指令数据集以提升LLM对表格的理解；（2）选用开源模型Llama3.1-8B-Instruct，仅用7000条指令样本，在A100显卡上进行小规模指令微调（不足6小时）。

Result: 仅用少量高质量指令样本和有限算力，微调后的开源模型在表格数据生成任务上，性能与商用最强LLM（GPT-4o）相当。

Conclusion: 表格数据生成方面，指令微调无需依赖大规模数据和高算力，仅凭高质量数据和有限资源即可获得接近商用模型的性能，极大提升了表格生成模型的可获取性与实用性。

Abstract: Tabular instruction tuning has emerged as a promising research direction for improving LLMs understanding of tabular data. However, the majority of existing works only consider question-answering and reasoning tasks over tabular data, leaving tabular data generation largely unnoticed. In this work, for the first time, we explore the efficacy of instruction tuning in improving LLMs tabular data generation capabilities. More specifically, given the high data and computation requirements of tabular instruction tuning, we aim to address the possibility of instruction tuning for tabular data generation with limited data and computational resources. To achieve this, we first create a high-quality instruction dataset for tabular data, enabling efficient LLM comprehension. We then instruction-tune an open-source LLM (Llama3.1-8B-Instruct) on the training set of this dataset to improve its tabular data generation performance. Our experimental results show that by using our high-quality dataset and instruction-tuning on only 7K instructions with an A100 GPU, for less than 6 hours, we achieve tabular data generation performance on par with the most capable commercial LLM, GPT-4o.

</details>


### [182] [Robust 3DGS-based SLAM via Adaptive Kernel Smoothing](https://arxiv.org/abs/2511.23221)
*Shouhe Zhang,Dayong Ren,Sensen Song,Wenjie Li,Piaopiao Yu,Yurong Qian*

Main category: cs.CV

TL;DR: 本文认为，在3DGS-SLAM系统中，提升光栅化过程对参数误差的鲁棒性比仅追求渲染质量更能提升相机位姿追踪的准确性，并提出了CB-KNN方法以实现更鲁棒的光栅化。


<details>
  <summary>Details</summary>
Motivation: 传统3DGS-SLAM系统普遍认为高质量渲染是提升相机追踪性能的关键，但作者发现参数噪声对追踪的影响更为显著。由此，作者希望增强光栅化过程对这些噪声的容错性。

Method: 作者提出了一种基于平滑核策略的方法，通过让每个高斯分布在渲染时影响更广泛的像素，降低异常参数带来的噪声影响。具体上，方法名为Corrective Blurry KNN（CB-KNN），它会自适应地调整局部范围内K个高斯邻居的RGB值和位置，从而生成更平滑的渲染效果，减少参数误差的影响。

Result: 实验结果表明，在不牺牲场景重建质量的前提下，该方法大幅提升了相机位姿追踪的鲁棒性和精度。

Conclusion: 相比单纯优化渲染质量，强化光栅化对参数误差的鲁棒性更能提升3DGS-SLAM系统的 tracking 性能。CB-KNN提供了一种简单实用的方案，可直接集成于现有3DGS框架中并显著改善追踪效果。

Abstract: In this paper, we challenge the conventional notion in 3DGS-SLAM that rendering quality is the primary determinant of tracking accuracy. We argue that, compared to solely pursuing a perfect scene representation, it is more critical to enhance the robustness of the rasterization process against parameter errors to ensure stable camera pose tracking. To address this challenge, we propose a novel approach that leverages a smooth kernel strategy to enhance the robustness of 3DGS-based SLAM. Unlike conventional methods that focus solely on minimizing rendering error, our core insight is to make the rasterization process more resilient to imperfections in the 3DGS parameters. We hypothesize that by allowing each Gaussian to influence a smoother, wider distribution of pixels during rendering, we can mitigate the detrimental effects of parameter noise from outlier Gaussians. This approach intentionally introduces a controlled blur to the rendered image, which acts as a regularization term, stabilizing the subsequent pose optimization. While a complete redesign of the rasterization pipeline is an ideal solution, we propose a practical and effective alternative that is readily integrated into existing 3DGS frameworks. Our method, termed Corrective Blurry KNN (CB-KNN), adaptively modifies the RGB values and locations of the K-nearest neighboring Gaussians within a local region. This dynamic adjustment generates a smoother local rendering, reducing the impact of erroneous GS parameters on the overall image. Experimental results demonstrate that our approach, while maintaining the overall quality of the scene reconstruction (mapping), significantly improves the robustness and accuracy of camera pose tracking.

</details>


### [183] [DAONet-YOLOv8: An Occlusion-Aware Dual-Attention Network for Tea Leaf Pest and Disease Detection](https://arxiv.org/abs/2511.23222)
*Yefeng Wu,Shan Wan,Ling Wu,Yecheng Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种改进型YOLOv8模型（DAONet-YOLOv8），用于提高茶园中病虫害检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 实际茶园环境中背景复杂、光照多变、叶片枝条密集导致病虫害检测难以实现高精度，常出现漏检和误检，现有方法不足以应对这些挑战。

Method: 提出了三项关键改进：1) 双注意力融合模块（DAFM）结合卷积的局部特征与自注意力的全局上下文，用于关注细微病斑并抑制背景噪声；2) 考虑遮挡感知的检测头（Detect-OAHead），学习可见与被遮挡部分之间的关系以补全缺失特征；3) C2f-DSConv模块使用多形态动态合成卷积，更好捕捉不规则病斑边界。

Result: 在包含六类病虫害的真实茶园数据集上，DAONet-YOLOv8实现了92.97%的精度，92.80%的召回率，mAP@50为97.10%，mAP@50:95为76.90%，分别比YOLOv8n提升2.34、4.68、1.40、1.80个百分点，同时参数减少16.7%。与主流检测模型对比也展示了优越性能。

Conclusion: DAONet-YOLOv8能更有效应对复杂环境下的茶园病虫害检测任务，表现优于现有主流方法，为实际农业应用提供了更精确高效的技术方案。

Abstract: Accurate detection of tea leaf pests and diseases in real plantations remains challenging due to complex backgrounds, variable illumination, and frequent occlusions among dense branches and leaves. Existing detectors often suffer from missed detections and false positives in such scenarios. To address these issues, we propose DAONet-YOLOv8, an enhanced YOLOv8 variant with three key improvements: (1) a Dual-Attention Fusion Module (DAFM) that combines convolutional local feature extraction with self-attention based global context modeling to focus on subtle lesion regions while suppressing background noise; (2) an occlusion-aware detection head (Detect-OAHead) that learns the relationship between visible and occluded parts to compensate for missing lesion features; and (3) a C2f-DSConv module employing dynamic synthesis convolutions with multiple kernel shapes to better capture irregular lesion boundaries. Experiments on our real-world tea plantation dataset containing six pest and disease categories demonstrate that DAONet-YOLOv8 achieves 92.97% precision, 92.80% recall, 97.10% mAP@50 and 76.90% mAP@50:95, outperforming the YOLOv8n baseline by 2.34, 4.68, 1.40 and 1.80 percentage points respectively, while reducing parameters by 16.7%. Comparative experiments further confirm that DAONet-YOLOv8 achieves superior performance over mainstream detection models.

</details>


### [184] [PointCNN++: Performant Convolution on Native Points](https://arxiv.org/abs/2511.23227)
*Lihan Li,Haofeng Zhong,Rui Bu,Mingchao Sun,Wenzheng Chen,Baoquan Chen,Yangyan Li*

Main category: cs.CV

TL;DR: 本文提出了PointCNN++，一种新颖的3D点云卷积神经网络架构，在不牺牲几何精度的前提下显著提升效率，并以更优的内存和速度性能实现高精度的点云配准。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云卷积学习方法主要分为点方法（精度高但效率低）和体素方法（效率高但精度损失大），尤其在点云配准等任务中几何精度损失成为关键瓶颈。作者希望突破精度与效率之间的权衡困境。

Method: 作者提出将稀疏卷积泛化为从体素到点的通用方式，采用以点为中心的卷积操作，并将该操作表达为矩阵-向量乘法与归约（MVMR）问题，设计了高效的GPU内核以原生支持点级操作。

Result: 实验显示，PointCNN++在内存占用上比主流点方法少一个数量级，速度快数倍，并作为体素卷积网络骨干的直接替代方案时，能大幅提升点云配准精度，同时更加高效。

Conclusion: PointCNN++证明了几何高保真与高性能可兼得，并为高效高保真的3D点云学习方法开辟新方向。代码将开源。

Abstract: Existing convolutional learning methods for 3D point cloud data are divided into two paradigms: point-based methods that preserve geometric precision but often face performance challenges, and voxel-based methods that achieve high efficiency through quantization at the cost of geometric fidelity. This loss of precision is a critical bottleneck for tasks such as point cloud registration. We propose PointCNN++, a novel architectural design that fundamentally mitigates this precision-performance trade-off. It \textbf{generalizes sparse convolution from voxels to points}, treating voxel-based convolution as a specialized, degraded case of our more general point-based convolution. First, we introduce a point-centric convolution where the receptive field is centered on the original, high-precision point coordinates. Second, to make this high-fidelity operation performant, we design a computational strategy that operates \textbf{natively} on points. We formulate the convolution on native points as a Matrix-Vector Multiplication and Reduction (MVMR) problem, for which we develop a dedicated, highly-optimized GPU kernel. Experiments demonstrate that PointCNN++ \textbf{uses an order of magnitude less memory and is several times faster} than representative point-based methods. Furthermore, when used as a simple replacement for the voxel-based backbones it generalizes, it \textbf{significantly improves point cloud registration accuracies while proving both more memory-efficient and faster}. PointCNN++ shows that preserving geometric detail and achieving high performance are not mutually exclusive, paving the way for a new class of 3D learning with high fidelity and efficiency. Our code will be open sourced.

</details>


### [185] [Language-guided 3D scene synthesis for fine-grained functionality understanding](https://arxiv.org/abs/2511.23230)
*Jaime Corsetti,Francesco Giuliari,Davide Boscaini,Pedro Hermosilla,Andrea Pilzer,Guofeng Mei,Alexandros Delitzas,Francis Engelmann,Fabio Poiesi*

Main category: cs.CV

TL;DR: 本文提出SynthFun3D，通过自动合成可完成特定任务的3D场景，有效破解3D功能性理解面临的数据匮乏难题，并生成高质量带注释的数据，为3D应用提供了高效的基础数据支持。


<details>
  <summary>Details</summary>
Motivation: 3D场景功能性理解依赖大量高质量有功能标签的数据，而现实世界中数据收集和人工标注成本高、样本稀缺，严重制约了相关AI研究和应用推进。

Method: 提出一种任务驱动的3D场景合成方法SynthFun3D。给定动作描述，方法会从带有部件级注释的家具数据库中合成满足任务要求的3D室内场景，并自动识别对应功能部件的3D掩码。这样大量、低成本地产生高质量功能性标注数据。

Result: 用户研究显示，SynthFun3D生成的场景与任务描述之间的契合度优于其他方法，在定量实验中这些合成数据可替代真实数据（性能损失很小），也可补充真实数据提升整体表现。

Conclusion: SynthFun3D为实现大规模、高质量功能性3D标注数据的廉价生成提供了全新解决方案，将有力推动3D任务相关应用和研究进展。

Abstract: Functionality understanding in 3D, which aims to identify the functional element in a 3D scene to complete an action (e.g., the correct handle to "Open the second drawer of the cabinet near the bed"), is hindered by the scarcity of real-world data due to the substantial effort needed for its collection and annotation. To address this, we introduce SynthFun3D, the first method for task-based 3D scene synthesis. Given the action description, SynthFun3D generates a 3D indoor environment using a furniture asset database with part-level annotation, ensuring the action can be accomplished. It reasons about the action to automatically identify and retrieve the 3D mask of the correct functional element, enabling the inexpensive and large-scale generation of high-quality annotated data. We validate SynthFun3D through user studies, which demonstrate improved scene-prompt coherence compared to other approaches. Our quantitative results further show that the generated data can either replace real data with minor performance loss or supplement real data for improved performance, thereby providing an inexpensive and scalable solution for data-hungry 3D applications. Project page: github.com/tev-fbk/synthfun3d.

</details>


### [186] [Unlocking Multilingual Reasoning Capability of LLMs and LVLMs through Representation Engineering](https://arxiv.org/abs/2511.23231)
*Qiming Li,Xiaocheng Feng,Yixuan Ma,Zekai Ye,Ruihan Chen,Xiachong Feng,Bing Qin*

Main category: cs.CV

TL;DR: 该论文提出了一种无需额外训练和工具、针对推理时提升多语言推理能力的方法MRRE，通过在推理时向模型特定层注入预计算向量来提升低资源语言推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）及多模态视觉语言模型（LVLMs）在英语推理显著优于低资源语言，带来多语种应用公平性问题。现有提升方法依赖多语训练或外部翻译，成本高且不稳定。作者希望提出一种无需训练、高效且稳定提升低资源语种推理能力的新方法。

Method: 提出MRRE（多语言推理表征工程），在模型推理时于特定层依次注入两类预计算向量：（1）跨语言推理增强向量，将非英语语种的推理表征引导到英语空间解锁推理能力；（2）目标语言输出锚定向量，恢复目标语言表达的一致性，无需额外训练和工具。

Result: 在6个先进LLM和LVLM、4个推理基准上验证MRRE方法，相比于基线，平均提升低资源语言推理表现5.48%，泰语及斯瓦希里语最高提升7.54%；同时输入输出语言一致性提升3.78%。

Conclusion: MRRE是一种高效、无需训练的数据无关推理时多语言推理增强新方法，能在多种大模型及多任务场景下大幅提升低资源语言推理能力并改善语言一致性，为多语言AI公平性和泛化能力带来新思路。

Abstract: Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) demonstrate strong reasoning capabilities, yet their performance in English significantly outperforms that in low-resource languages, raising fairness concerns in multilingual applications. Existing approaches either rely on costly multilingual training or employ prompting with external translation tools, both of which are resource-intensive and sensitive to translation quality. To address these limitations, we propose a training-free inference-time method to enhance Multilingual Reasoning capabilities via Representation Engineering (MRRE) without using any additional training data or tools. MRRE sequentially injects two precomputed vectors at specific layers during inference processing: cross-lingual reasoning enhancement vectors, which steer non-English reasoning representations toward English space to unlock multilingual reasoning, and target-language output anchoring vectors, which restore the distribution of the target language to preserve input-output language consistency. Comprehensive experiments across six advanced LLMs and LVLMs on four reasoning benchmarks demonstrate that MRRE consistently enhances non-English reasoning by an average gain of 5.48% and up to 7.54% in low-resource languages (Thai and Swahili), while improving input-output language consistency by 3.78%.

</details>


### [187] [Synthetic Industrial Object Detection: GenAI vs. Feature-Based Methods](https://arxiv.org/abs/2511.23241)
*Jose Moises Araya-Martinez,Adrián Sanchis Reig,Gautham Mohan,Sarvenaz Sardari,Jens Lambrecht,Jörg Krüger*

Main category: cs.CV

TL;DR: 本论文系统评估了多种领域随机化（DR）和领域自适应（DA）技术，在无需人工标注下，生成高效实用的上下文合成数据以缩小仿真到现实（sim-to-real）差距。结果显示，简单的特征方法优于复杂的生成式AI手段。


<details>
  <summary>Details</summary>
Motivation: 工业和机器人领域机器学习模型的部署，数据生成和标注成本高、流程繁琐亟需更高效方案；合成数据是可行方向，但仿真到实际场景容易有性能损失，因此需要评估高效闭环方法。

Method: 作者基于工业和机器人两类真实数据集，对比评测了特征对齐、生成式AI、传统渲染等多种DR与DA技术，尤其测试了感知哈希与基于提示词的扩散式DA方法，并比较其在性能与资源消耗上的表现。

Result: 实验结果显示，在有足够多变种类渲染数据的前提下，简单的特征方法（如亮度/感知哈希过滤）在准确率与效率上皆优于生成式AI。感知哈希在工业和机器人数据集上的mAP50得分分别高达98%和67%；而GenAI方法生成数据耗时长、提升有限。

Conclusion: 为高效实现sim-to-real迁移，推荐采用简单高效的特征方法替代复杂的生成式AI。该工作为仅依赖合成数据跨域训练模型提供了实践性建议。

Abstract: Reducing the burden of data generation and annotation remains a major challenge for the cost-effective deployment of machine learning in industrial and robotics settings. While synthetic rendering is a promising solution, bridging the sim-to-real gap often requires expert intervention. In this work, we benchmark a range of domain randomization (DR) and domain adaptation (DA) techniques, including feature-based methods, generative AI (GenAI), and classical rendering approaches, for creating contextualized synthetic data without manual annotation. Our evaluation focuses on the effectiveness and efficiency of low-level and high-level feature alignment, as well as a controlled diffusion-based DA method guided by prompts generated from real-world contexts. We validate our methods on two datasets: a proprietary industrial dataset (automotive and logistics) and a public robotics dataset. Results show that if render-based data with enough variability is available as seed, simpler feature-based methods, such as brightness-based and perceptual hashing filtering, outperform more complex GenAI-based approaches in both accuracy and resource efficiency. Perceptual hashing consistently achieves the highest performance, with mAP50 scores of 98% and 67% on the industrial and robotics datasets, respectively. Additionally, GenAI methods present significant time overhead for data generation at no apparent improvement of sim-to-real mAP values compared to simpler methods. Our findings offer actionable insights for efficiently bridging the sim-to-real gap, enabling high real-world performance from models trained exclusively on synthetic data.

</details>


### [188] [Learning to Predict Aboveground Biomass from RGB Images with 3D Synthetic Scenes](https://arxiv.org/abs/2511.23249)
*Silvia Zuffi*

Main category: cs.CV

TL;DR: 本论文提出了一种新的基于学习的方法，利用单张地面RGB图像估算森林地上生物量（AGB），打破了传统方法在密集植被中估算难的问题。


<details>
  <summary>Details</summary>
Motivation: 准确估算森林AGB对于碳储存和火灾风险评估至关重要，但传统方法（如田野测量或遥感技术）劳动强度大且在密林区域存在局限。亟需一种高效、低成本且可扩展的新方法。

Method: 将AGB估算任务转为密集预测问题，创建AGB密度图，每个像素反映归一化的树木生物量。利用合成的3D SPREAD数据集（具备真实森林场景及分割掩码、多种树木属性），应用异速生长方程计算AGB并训练模型，预测AGB密度图后积分获得整张图片的AGB估算。

Result: 方法在SPREAD数据集上的中位误差为1.22 kg/m^2，在真实图像集上为1.94 kg/m^2，精度较高。

Conclusion: 首次实现从单张地面RGB图像直接推断森林AGB，为森林监测提供了可扩展、低成本、易解释的方案，有助于公民科学和大规模生态监测。

Abstract: Forests play a critical role in global ecosystems by supporting biodiversity and mitigating climate change via carbon sequestration. Accurate aboveground biomass (AGB) estimation is essential for assessing carbon storage and wildfire fuel loads, yet traditional methods rely on labor-intensive field measurements or remote sensing approaches with significant limitations in dense vegetation. In this work, we propose a novel learning-based method for estimating AGB from a single ground-based RGB image. We frame this as a dense prediction task, introducing AGB density maps, where each pixel represents tree biomass normalized by the plot area and each tree's image area. We leverage the recently introduced synthetic 3D SPREAD dataset, which provides realistic forest scenes with per-image tree attributes (height, trunk and canopy diameter) and instance segmentation masks. Using these assets, we compute AGB via allometric equations and train a model to predict AGB density maps, integrating them to recover the AGB estimate for the captured scene. Our approach achieves a median AGB estimation error of 1.22 kg/m^2 on held-out SPREAD data and 1.94 kg/m^2 on a real-image dataset. To our knowledge, this is the first method to estimate aboveground biomass directly from a single RGB image, opening up the possibility for a scalable, interpretable, and cost-effective solution for forest monitoring, while also enabling broader participation through citizen science initiatives.

</details>


### [189] [Simultaneous Image Quality Improvement and Artefacts Correction in Accelerated MRI](https://arxiv.org/abs/2511.23274)
*Georgia Kanli,Daniele Perlo,Selma Boudissa,Radovan Jirik,Olivier Keunen*

Main category: cs.CV

TL;DR: 本文提出了一种新方法USArt，能够在磁共振（MR）采集加速的同时，自动纠正噪声和运动伪影，实现高质量MRI重建。该模型在2D脑部解剖图像上进行了充分验证，展示了显著提升信噪比（SNR）和图像对比度。


<details>
  <summary>Details</summary>
Motivation: 快速、高质量的MRI成像因k空间采样时间长而受限，而减少采样点虽可加速但易导致图像质量下降。此外，实际采集中常存在噪声和运动伪影影响诊断精度。现有方法仅各自针对加速重建或伪影校正，无法同时处理二者，限制了临床应用性能。

Method: 作者提出了USArt方法，通过双子模型结构实现。模型专为2D脑部MRI设计，输入为在采样点不足（欠采样）且带有运动/噪声伪影情况下的数据，输出为高质量、伪影已纠正的图像。探索了不同的欠采样策略及退化级别，验证了模型在梯度欠采样下效果最佳。

Result: USArt模型能够在最高5倍加速条件下，高效恢复信噪比和图像对比度，并且能够同时实现运动和噪声伪影的纠正。多种采样策略和退化等级下模型效果优异，实验结果表明其对实际场景鲁棒性很强。

Conclusion: 本文提出的USArt能兼顾加速与伪影纠正，显著提升了快速MRI成像的图像质量和实用性，对临床应用具有重要意义，为MRI重建与诊断提供了新途径。

Abstract: MR data are acquired in the frequency domain, known as k-space. Acquiring high-quality and high-resolution MR images can be time-consuming, posing a significant challenge when multiple sequences providing complementary contrast information are needed or when the patient is unable to remain in the scanner for an extended period of time. Reducing k-space measurements is a strategy to speed up acquisition, but often leads to reduced quality in reconstructed images. Additionally, in real-world MRI, both under-sampled and full-sampled images are prone to artefacts, and correcting these artefacts is crucial for maintaining diagnostic accuracy. Deep learning methods have been proposed to restore image quality from under-sampled data, while others focused on the correction of artefacts that result from the noise or motion. No approach has however been proposed so far that addresses both acceleration and artefacts correction, limiting the performance of these models when these degradation factors occur simultaneously. To address this gap, we present a method for recovering high-quality images from under-sampled data with simultaneously correction for noise and motion artefact called USArt (Under-Sampling and Artifact correction model). Customized for 2D brain anatomical images acquired with Cartesian sampling, USArt employs a dual sub-model approach. The results demonstrate remarkable increase of signal-to-noise ratio (SNR) and contrast in the images restored. Various under-sampling strategies and degradation levels were explored, with the gradient under-sampling strategy yielding the best outcomes. We achieved up to 5x acceleration and simultaneously artefacts correction without significant degradation, showcasing the model's robustness in real-world settings.

</details>


### [190] [FACT-GS: Frequency-Aligned Complexity-Aware Texture Reparameterization for 2D Gaussian Splatting](https://arxiv.org/abs/2511.23292)
*Tianhao Xie,Linlian Jiang,Xinxin Zuo,Yang Wang,Tiberiu Popa*

Main category: cs.CV

TL;DR: 本文提出了 FACT-GS 方法，通过自适应纹理采样密度，提升了高斯溅射在高频细节区域的表示能力，实现了更清晰逼真的场景渲染。


<details>
  <summary>Details</summary>
Motivation: 现有高斯溅射方法在每个高斯内部采用均匀纹理采样密度，导致高频区域细节模糊、低频区域纹理空间浪费，限制了现实场景的真实感和细节表现。

Method: FACT-GS 基于自适应采样理论，将纹理采样视为可微分的密度分配问题。它利用一个可学习的变形场（通过雅可比矩阵调整局部采样密度）对每个高斯的纹理进行非均匀采样，使高频细节获得更多采样点，同时保持实时渲染效率和参数量不变。

Result: 在2D高斯溅射场景中，FACT-GS 能在相同参数预算下显著提升高频细节表现，获得更清晰锐利的渲染效果，同时保持实时性能。

Conclusion: FACT-GS 展示了适应局部视觉复杂度调整纹理采样密度的有效性，可有效提升高斯溅射的细节还原能力，拓展其在高质量实时渲染领域的应用潜力。

Abstract: Realistic scene appearance modeling has advanced rapidly with Gaussian Splatting, which enables real-time, high-quality rendering. Recent advances introduced per-primitive textures that incorporate spatial color variations within each Gaussian, improving their expressiveness. However, texture-based Gaussians parameterize appearance with a uniform per-Gaussian sampling grid, allocating equal sampling density regardless of local visual complexity. This leads to inefficient texture space utilization, where high-frequency regions are under-sampled and smooth regions waste capacity, causing blurred appearance and loss of fine structural detail. We introduce FACT-GS, a Frequency-Aligned Complexity-aware Texture Gaussian Splatting framework that allocates texture sampling density according to local visual frequency. Grounded in adaptive sampling theory, FACT-GS reformulates texture parameterization as a differentiable sampling-density allocation problem, replacing the uniform textures with a learnable frequency-aware allocation strategy implemented via a deformation field whose Jacobian modulates local sampling density. Built on 2D Gaussian Splatting, FACT-GS performs non-uniform sampling on fixed-resolution texture grids, preserving real-time performance while recovering sharper high-frequency details under the same parameter budget.

</details>


### [191] [A Perceptually Inspired Variational Framework for Color Enhancement](https://arxiv.org/abs/2511.23329)
*Rodrigo Palma-Amestoy,Edoardo Provenzi,Marcelo Bertalmío,Vicent Caselles*

Main category: cs.CV

TL;DR: 本文提出了一种基于人类色觉现象的变分形式化色彩对比度增强方法，并给出高效实现方式。


<details>
  <summary>Details</summary>
Motivation: 现有的基于人类色觉的色彩校正算法在处理图像对比度和离散度等显著特征时，其行为难以刻画，因此亟需结构化的方法来分析和改进。

Method: 作者通过引入变分方法，定义了‘感知启发’的能量要求，并展示了一类满足这些要求的形式函数（functionals），选取了三种具代表性的函数形式，利用梯度下降进行优化计算，并通过算法优化将计算复杂度从O(N²)降到O(NlogN)。

Result: 提出的三种变分形式各自与现有模型进行对比，揭示了它们在特征增强上的异同；算法效率得到了大幅提升。

Conclusion: 变分方法不仅理论基础扎实，而且高效实用，为色彩对比增强提供了结构化和可推广的解决思路。

Abstract: Basic phenomenology of human color vision has been widely taken as an inspiration to devise explicit color correction algorithms. The behavior of these models in terms of significative image features (such as contrast and dispersion) can be difficult to characterize. To cope with this, we propose to use a variational formulation of color contrast enhancement that is inspired by the basic phenomenology of color perception. In particular, we devise a set of basic requirements to be fulfilled by an energy to be considered as `perceptually inspired', showing that there is an explicit class of functionals satisfying all of them. We single out three explicit functionals that we consider of basic interest, showing similarities and differences with existing models. The minima of such functionals is computed using a gradient descent approach. We also present a general methodology to reduce the computational cost of the algorithms under analysis from ${\cal O}(N^2)$ to ${\cal O}(N\log N)$, being $N$ the number of input pixels.

</details>


### [192] [UniGeoSeg: Towards Unified Open-World Segmentation for Geospatial Scenes](https://arxiv.org/abs/2511.23332)
*Shuo Ni,Di Wang,He Chen,Haonan Guo,Ning Zhang,Jing Zhang*

Main category: cs.CV

TL;DR: 该论文提出了GeoSeg-1M，这是首个针对遥感图像基于指令分割的百万级数据集，同时开发了统一分割框架UniGeoSeg，在多项遥感分割任务上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有遥感领域的基于指令分割方法普遍存在任务形式零散、数据稀缺等问题，限制了模型的泛化与理解能力。为解决这一瓶颈，需要构建大规模、多样化的遥感指令分割数据集，推动相关方法的效果提升及应用普适性。

Method: 作者自动化整合多个公开数据集，通过筛选、生成分割掩膜与多类型指令（引用式、交互式、推理）构建出GeoSeg-1M数据集，并进一步建立了评测基准GeoSeg-Bench。基于此，提出了融合任务感知文本增强、隐空间知识记忆与渐进式多任务训练的统一分割框架UniGeoSeg。

Result: 实验结果显示UniGeoSeg在GeoSeg-Bench及多个公开遥感分割基准测试集上刷新了性能指标，同时表现出较强的零样本泛化能力。

Conclusion: GeoSeg-1M和GeoSeg-Bench推动了遥感指令驱动分割任务的发展，UniGeoSeg为多任务遥感分割方法提供了有力基线，数据和代码的公开将促进领域进一步创新与应用落地。

Abstract: Instruction-driven segmentation in remote sensing generates masks from guidance, offering great potential for accessible and generalizable applications. However, existing methods suffer from fragmented task formulations and limited instruction data, hindering effective understanding and generalization. To address these issues, we introduce GeoSeg-1M, the first million-scale dataset for remote sensing instruction-driven segmentation, constructed via an automatic mask filtering and instruction generation pipeline that synthesizes referring, interactive, and reasoning segmentation instructions from multiple public datasets. GeoSeg-1M contains 590K images, 117 categories, and 1.1M image-mask-instruction triplets. Building upon this foundation, we further curate GeoSeg-Bench, a challenging benchmark designed to evaluate contextual understanding and reasoning capabilities across diverse instruction-driven tasks and complex geospatial scenes. Furthermore, we present UniGeoSeg, a unified framework that serves as a strong baseline, incorporating task-aware text enhancement, latent knowledge memory, and a progressive training strategy to facilitate multi-task learning. Extensive experiments demonstrate the state-of-the-art performance of UniGeoSeg across GeoSeg-Bench and diverse public benchmarks, while exhibiting strong zero-shot generalization. Datasets and source code were released at https://github.com/MiliLab/UniGeoSeg.

</details>


### [193] [Markovian Scale Prediction: A New Era of Visual Autoregressive Generation](https://arxiv.org/abs/2511.23334)
*Yu Zhang,Jingyi Liu,Yiwei Shi,Qi Zhang,Duoqian Miao,Changwei Wang,Longbing Cao*

Main category: cs.CV

TL;DR: 本文提出了一种无需全上下文依赖的新视觉自回归（VAR）模型Markov-VAR，通过马尔可夫过程建模大幅提升了生成性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的VAR依赖于全上下文信息，虽然提升了学习能力，但导致计算效率低下和资源消耗大。研究者希望摆脱全上下文依赖，实现兼具更高效率和表现的视觉自回归模型。

Method: 作者将VAR重构为一个非全上下文的马尔可夫过程。具体做法包括：把每个尺度视为一个马尔可夫状态，并引入滑动窗口机制，将部分历史尺度信息压缩成紧凑的历史向量，再与当前尺度融合，形成代表性的动态状态，用于后续的状态预测。

Result: 在ImageNet数据集上，Markov-VAR相较于传统VAR能有效降低FID（降低了10.5%，256x256分辨率）和大幅减少峰值内存消耗（减少了83.8%，1024x1024分辨率），展现出强大的生成能力和资源效率。

Conclusion: Markov-VAR模型结构简单效果优异，为视觉自回归生成及其下游任务提供了新思路，具有很好的研究和应用前景。

Abstract: Visual AutoRegressive modeling (VAR) based on next-scale prediction has revitalized autoregressive visual generation. Although its full-context dependency, i.e., modeling all previous scales for next-scale prediction, facilitates more stable and comprehensive representation learning by leveraging complete information flow, the resulting computational inefficiency and substantial overhead severely hinder VAR's practicality and scalability. This motivates us to develop a new VAR model with better performance and efficiency without full-context dependency. To address this, we reformulate VAR as a non-full-context Markov process, proposing Markov-VAR. It is achieved via Markovian Scale Prediction: we treat each scale as a Markov state and introduce a sliding window that compresses certain previous scales into a compact history vector to compensate for historical information loss owing to non-full-context dependency. Integrating the history vector with the Markov state yields a representative dynamic state that evolves under a Markov process. Extensive experiments demonstrate that Markov-VAR is extremely simple yet highly effective: Compared to VAR on ImageNet, Markov-VAR reduces FID by 10.5% (256 $\times$ 256) and decreases peak memory consumption by 83.8% (1024 $\times$ 1024). We believe that Markov-VAR can serve as a foundation for future research on visual autoregressive generation and other downstream tasks.

</details>


### [194] [Flow Straighter and Faster: Efficient One-Step Generative Modeling via MeanFlow on Rectified Trajectories](https://arxiv.org/abs/2511.23342)
*Xinxi Zhang,Shiwei Tan,Quang Nguyen,Quan Dao,Ligong Han,Xiaoxiao He,Tunyu Zhang,Alen Mrdovic,Dimitris Metaxas*

Main category: cs.CV

TL;DR: 本文提出Rectified MeanFlow模型，通过仅一次重整化步骤对流场均速建模，实现高效的一步采样，提升了训练效率和生成质量，在ImageNet数据集上超越了现有一步流蒸馏与Rectified Flow方法。


<details>
  <summary>Details</summary>
Motivation: 现有流生成模型虽然性能强大，但采样依赖昂贵的常微分方程(O.D.E.)数值积分，影响生成效率。Rectified Flow虽实现一步采样，但需多次高成本的重整化训练；MeanFlow虽一步生成，但在高曲率流训练下收敛慢、监督噪声大。本文旨在降低现有方法的计算负担，同时提升样本质量。

Method: 提出Rectified MeanFlow方法，只需一次reflow步骤，对校正后的轨迹均速场建模，舍弃严格“直化”轨迹的要求。并引入了截断启发式方法来进一步减少残余曲率，提升模型性能。

Result: 在ImageNet 64、256、512多分辨率实验中，Re-MeanFlow在样本质量和训练效率方面，均显著优于现有的一步流蒸馏及Rectified Flow方法。

Conclusion: Rectified MeanFlow无需繁复的多轮重整化即可实现高效高质量的图像生成，为一步流模型提供了更优的取舍与实践方案。

Abstract: Flow-based generative models have recently demonstrated strong performance, yet sampling typically relies on expensive numerical integration of ordinary differential equations (ODEs). Rectified Flow enables one-step sampling by learning nearly straight probability paths, but achieving such straightness requires multiple computationally intensive reflow iterations. MeanFlow achieves one-step generation by directly modeling the average velocity over time; however, when trained on highly curved flows, it suffers from slow convergence and noisy supervision. To address these limitations, we propose Rectified MeanFlow, a framework that models the mean velocity field along the rectified trajectory using only a single reflow step. This eliminates the need for perfectly straightened trajectories while enabling efficient training. Furthermore, we introduce a simple yet effective truncation heuristic that aims to reduce residual curvature and further improve performance. Extensive experiments on ImageNet at 64, 256, and 512 resolutions show that Re-MeanFlow consistently outperforms prior one-step flow distillation and Rectified Flow methods in both sample quality and training efficiency. Code is available at https://github.com/Xinxi-Zhang/Re-MeanFlow.

</details>


### [195] [A Hierarchical Computer Vision Pipeline for Physiological Data Extraction from Bedside Monitors](https://arxiv.org/abs/2511.23355)
*Vinh Chau,Khoa Le Dinh Van,Hon Huynh Ngoc,Binh Nguyen Thien,Hao Nguyen Thien,Vy Nguyen Quang,Phuc Vo Hong,Yen Lam Minh,Kieu Pham Tieu,Trinh Nguyen Thi Diem,Louise Thwaites,Hai Ho Bich*

Main category: cs.CV

TL;DR: 该论文提出了一种基于计算机视觉的方法，使用普通摄像头自动识别和提取床旁监护仪屏幕上的生命体征数据，实现与电子病历系统的信息集成，解决了低资源医疗环境中设备互联难题。


<details>
  <summary>Details</summary>
Motivation: 在许多低资源医疗环境中，床旁监护仪通常为无网络的独立设备，无法与电子病历系统实现数据互通，造成数据整合和使用的障碍。更换硬件成本高昂，亟需兼容现有设备的低成本解决方案。

Method: 作者设计了一个视觉管道，包括：1）用YOLOv11实现监护仪和目标区域的检测，2）用PaddleOCR进行文本提取，3）通过几何校正模块矫正拍摄角度和光照变化。系统在6,498张来自开源和越南ICU真实环境的数据集上进行了评估。

Result: 模型在监护仪检测上的mAP@50-95达99.5%，生命体征目标区域定位达91.5%。核心生理参数（心率、血氧饱和度、血压）的端到端提取准确率超过98.9%。

Conclusion: 该轻量化摄像头方案可靠地将无结构的屏幕数据转化为结构化数字信息，为低资源环境下提升信息获取与临床文档电子化提供了可行、可扩展的途径。

Abstract: In many low-resource healthcare settings, bedside monitors remain standalone legacy devices without network connectivity, creating a persistent interoperability gap that prevents seamless integration of physiological data into electronic health record (EHR) systems. To address this challenge without requiring costly hardware replacement, we present a computer vision-based pipeline for the automated capture and digitisation of vital sign data directly from bedside monitor screens. Our method employs a hierarchical detection framework combining YOLOv11 for accurate monitor and region of interest (ROI) localisation with PaddleOCR for robust text extraction. To enhance reliability across variable camera angles and lighting conditions, a geometric rectification module standardizes the screen perspective before character recognition. We evaluated the system on a dataset of 6,498 images collected from open-source corpora and real-world intensive care units in Vietnam. The model achieved a mean Average Precision (mAP@50-95) of 99.5% for monitor detection and 91.5% for vital sign ROI localisation. The end-to-end extraction accuracy exceeded 98.9% for core physiological parameters, including heart rate, oxygen saturation SpO2, and arterial blood pressure. These results demonstrate that a lightweight, camera-based approach can reliably transform unstructured information from screen captures into structured digital data, providing a practical and scalable pathway to improve information accessibility and clinical documentation in low-resource settings.

</details>


### [196] [DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline](https://arxiv.org/abs/2511.23377)
*Rui Zhang,Hongxia Wang,Hangqing Liu,Yang Zhou,Qiang Zeng*

Main category: cs.CV

TL;DR: 本文提出了一个大规模的扩散模型图像编辑区域定位数据集DEAL-300K，并基于此构建了一套有效的区域定位方法，在像素级提供了强性能基线。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的图像编辑技术极大降低了语义级图像操作的门槛，但也产生了难以被识别的高质量局部伪造。现有数据集与方法往往不能针对扩散编辑后平滑融合特性做出有效检测与定位，因此需要构建专门面向扩散型伪造的图像定位数据库与方法。

Method: 作者首先利用多模态大语言模型生成编辑指令，结合无掩码的扩散图像编辑器创作大量伪造图片，并通过主动学习的变更检测流程获得像素级标注，构建出DEAL-300K数据集。在此基础上，提出结合冻结的视觉基础模型和多频率提示微调的新框架，用于融合语义与频域特征提高编辑区域定位能力。

Result: 该方法在DEAL-300K测试集上像素级F1得分为82.56%，在外部CoCoGlide基准上为80.97%，性能优于现有方法，为未来扩散型图像操控检测研究提供了优质的数据和新基线。

Conclusion: DEAL-300K是面向扩散图像操控的新型大规模定位数据集，同时基于此提出的区域定位方法验证有效，推动了DIML方向的研究发展，也为后续相关工作打下了坚实基础。

Abstract: Diffusion-based image editing has made semantic level image manipulation easy for general users, but it also enables realistic local forgeries that are hard to localize. Existing benchmarks mainly focus on the binary detection of generated images or the localization of manually edited regions and do not reflect the properties of diffusion-based edits, which often blend smoothly into the original content. We present Diffusion-Based Image Editing Area Localization Dataset (DEAL-300K), a large scale dataset for diffusion-based image manipulation localization (DIML) with more than 300,000 annotated images. We build DEAL-300K by using a multi-modal large language model to generate editing instructions, a mask-free diffusion editor to produce manipulated images, and an active-learning change detection pipeline to obtain pixel-level annotations. On top of this dataset, we propose a localization framework that uses a frozen Visual Foundation Model (VFM) together with Multi Frequency Prompt Tuning (MFPT) to capture both semantic and frequency-domain cues of edited regions. Trained on DEAL-300K, our method reaches a pixel-level F1 score of 82.56% on our test split and 80.97% on the external CoCoGlide benchmark, providing strong baselines and a practical foundation for future DIML research.The dataset can be accessed via https://github.com/ymhzyj/DEAL-300K.

</details>


### [197] [VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction](https://arxiv.org/abs/2511.23386)
*Sinan Du,Jiahao Guo,Bo Li,Shuhao Cui,Zhengzhuo Xu,Yifu Luo,Yongxian Wei,Kun Gai,Xinggang Wang,Kai Wu,Chun Yuan*

Main category: cs.CV

TL;DR: 本文提出VQRAE模型，实现了多模态理解、生成与重建的统一表征，能在同一tokenizer中产生连续语义特征与离散生成token，在多个视觉任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多模态统一模型（如视觉-语言的理解与生成）难以用同一种tokenizer兼顾理解与生成能力，传统方法多用双编码器或对比损失，仅能部分平衡高层语义与低层特征。

Method: 提出基于向量量化的自编码器（VQRAE），用预训练视觉基础模型为编码器，对称ViT作为解码器。采用两阶段训练：第一阶段冻结编码器，仅用像素重建训练高维量化字典；第二阶段联合优化编码器并引入自蒸馏约束，实现连续语义和离散token统一输出。创新性地在量化编码器中使用高维字典，实现高利用率。

Result: VQRAE能生成连续的语义特征促进理解、生成离散token支持生成和重建，在视觉理解、生成和重建多个基准上优于或媲美SOTA，并展现出良好的可扩展性。

Conclusion: VQRAE首次统一了多模态理解和生成的representation，证明高维量化字典有显著优势，为多模态统一建模提供了新路径。

Abstract: Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.

</details>


### [198] [MANTA: Physics-Informed Generalized Underwater Object Tracking](https://arxiv.org/abs/2511.23405)
*Suhas Srinath,Hemang Jamadagni,Aditya Chadrasekar,Prathosh AP*

Main category: cs.CV

TL;DR: 本文提出了MANTA框架，通过物理知识引导的特征学习和跟踪方法，有效提升了水下目标跟踪的稳健性和精度。


<details>
  <summary>Details</summary>
Motivation: 在水下，由于光的衰减和散射会导致外观严重失真，现有基于地面数据训练的跟踪器难以泛化，急需能适应物理驱动退化的新方法。

Method: 提出基于物理信息的MANTA框架，结合特征表示学习和跟踪设计。采用双正样本对比学习策略，将时间一致性与Beer-Lambert物理增强结合，获得对时序和水下失真均鲁棒的特征。同时设计多阶段流程，将运动跟踪和结合几何一致性与外观相似性的二次关联算法结合，实现遮挡和漂移下的再识别。还提出中心-尺度一致性（CSC）和几何对齐分数（GAS）新评估指标。

Result: 在WebUOT-1M、UOT32、UTB180、UWCOT220四个水下基准上，MANTA在Success AUC等指标上取得了最高性能，提升可达6个百分点，并表现出优秀的长期稳定性和计算效率。

Conclusion: MANTA模型能够有效适应各种水下环境的物理退化，提升跟踪准确率和稳健性，为水下目标跟踪提供了新范式。

Abstract: Underwater object tracking is challenging due to wavelength dependent attenuation and scattering, which severely distort appearance across depths and water conditions. Existing trackers trained on terrestrial data fail to generalize to these physics-driven degradations. We present MANTA, a physics-informed framework integrating representation learning with tracking design for underwater scenarios. We propose a dual-positive contrastive learning strategy coupling temporal consistency with Beer-Lambert augmentations to yield features robust to both temporal and underwater distortions. We further introduce a multi-stage pipeline augmenting motion-based tracking with a physics-informed secondary association algorithm that integrates geometric consistency and appearance similarity for re-identification under occlusion and drift. To complement standard IoU metrics, we propose Center-Scale Consistency (CSC) and Geometric Alignment Score (GAS) to assess geometric fidelity. Experiments on four underwater benchmarks (WebUOT-1M, UOT32, UTB180, UWCOT220) show that MANTA achieves state-of-the-art performance, improving Success AUC by up to 6 percent, while ensuring stable long-term generalized underwater tracking and efficient runtime.

</details>


### [199] [DisMo: Disentangled Motion Representations for Open-World Motion Transfer](https://arxiv.org/abs/2511.23428)
*Thomas Ressler-Antal,Frank Fundel,Malek Ben Alaya,Stefan Andreas Baumann,Felix Krause,Ming Gui,Björn Ommer*

Main category: cs.CV

TL;DR: 本文提出DisMo方法，能从原始视频数据中学习独立于内容的抽象运动表征，实现无需物体对应的开放世界运动迁移，并在多项运动理解任务上取得领先。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频和图像到视频模型无法有效分离运动与内容的信息，限制了运动迁移和内容生成的灵活性，对内容创作者造成困扰。作者希望填补运动与内容耦合的缺陷，实现更通用的运动表征及迁移。

Method: 提出DisMo，一种在图像空间重构目标下，直接从视频中抽象学习运动信息的范式。其运动表征不依赖于外观、物体类别或姿态，实现运动与内容的解耦。该表征通过轻量适配器可集成于任意现有视频生成器，实现运动在跨类别与无关实体间转移，无需对象对应。

Result: DisMo方法在不同类别与场景下实现了高质量的运动迁移任务，无需对象对应且忠于动作。其运动表征在下游如零样本动作分类等任务上，超越了V-JEPA等当前主流视频表征方法。

Conclusion: DisMo能够泛化于开放世界运动迁移任务，支持精确、灵活的运动信息迁移，并为下游运动理解任务提供出色的表征能力，对内容生成及理解有显著推动。

Abstract: Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames. However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators. To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective. Our representation is generic and independent of static information such as appearance, object identity, or pose. This enables open-world motion transfer, allowing motion to be transferred across semantically unrelated entities without requiring object correspondences, even between vastly different categories. Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning. Furthermore, our motion representation can be combined with any existing video generator via lightweight adapters, allowing us to effortlessly benefit from future advancements in video models. We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks. Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester. Project page: https://compvis.github.io/DisMo

</details>


### [200] [Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model](https://arxiv.org/abs/2511.23429)
*Junshu Tang,Jiacheng Liu,Jiaqi Li,Longhuang Wu,Haoyu Yang,Penghao Zhao,Siruis Gong,Xiang Yuan,Shuai Shao,Qinglin Lu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的游戏世界生成模型Hunyuan-GameCraft-2，允许用户用自然语言、键盘或鼠标信号灵活控制生成视频内容，大幅提升游戏环境中的交互性和开放性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式游戏世界模型受限于固定操作方式和高标注成本，难以支持多样化和自由的交互，不能满足真实游戏对动态、丰富互动的需求。

Method: 提出了基于指令驱动的互动范式，将自然语言、键盘和鼠标信号用于生成与控制游戏视频。构建了交互式视频数据集，并基于14B参数的MoE图像到视频基础模型，引入了文本驱动的交互注入机制，实现对摄像机运动、角色行为、环境变化等精细控制。提出了交互专用评测基准InterBench。

Result: 实验显示，模型能生成时序连贯、因果一致的互动游戏视频，能够准确响应多样化和自由形式的用户指令（如“开门”“画火把”“引爆炸弹”等）。

Conclusion: Hunyuan-GameCraft-2显著提升了生成式游戏世界的交互灵活性和表达能力，为基于文本、键鼠信号的开放游戏环境生成奠定了基础。

Abstract: Recent advances in generative world models have enabled remarkable progress in creating open-ended game environments, evolving from static scene synthesis toward dynamic, interactive simulation. However, current approaches remain limited by rigid action schemas and high annotation costs, restricting their ability to model diverse in-game interactions and player-driven dynamics. To address these challenges, we introduce Hunyuan-GameCraft-2, a new paradigm of instruction-driven interaction for generative game world modeling. Instead of relying on fixed keyboard inputs, our model allows users to control game video contents through natural language prompts, keyboard, or mouse signals, enabling flexible and semantically rich interaction within generated worlds. We formally defined the concept of interactive video data and developed an automated process to transform large-scale, unstructured text-video pairs into causally aligned interactive datasets. Built upon a 14B image-to-video Mixture-of-Experts(MoE) foundation model, our model incorporates a text-driven interaction injection mechanism for fine-grained control over camera motion, character behavior, and environment dynamics. We introduce an interaction-focused benchmark, InterBench, to evaluate interaction performance comprehensively. Extensive experiments demonstrate that our model generates temporally coherent and causally grounded interactive game videos that faithfully respond to diverse and free-form user instructions such as "open the door", "draw a torch", or "trigger an explosion".

</details>


### [201] [Object-Centric Data Synthesis for Category-level Object Detection](https://arxiv.org/abs/2511.23450)
*Vikhyat Agarwal,Jiayi Cora Guo,Declan Hoban,Sissi Zhang,Nicholas Moran,Peter Cho,Srilakshmi Pattabiraman,Shantanu Joshi*

Main category: cs.CV

TL;DR: 本文针对目标检测中数据匮乏，尤其是长尾类别的问题，系统评估了利用有限的以对象为中心的数据（如多视角图像或3D模型）进行数据合成以微调检测模型的方法，并取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 深度学习目标检测模型扩展到新类别时，需要大量带标注数据，数据收集标注成本高，特别是长尾类别更为稀缺。因此，急需高效利用有限对象信息提升新类别检测能力。

Method: 提出并系统评估四种基于对象中心数据（如多视角图像、3D模型）的数据合成方法，包括简单图像处理、3D渲染和图像扩散生成模型，生成具有丰富上下文和复杂度的合成图像，用于新类别检测模型微调。

Result: 实验显示，合成数据能够有效提升检测模型在真实世界新类别上的泛化能力，在数据受限场景下显著提高检测性能。

Conclusion: 利用对象中心数据进行合成是解决小样本新类别目标检测的有效手段，各类合成方法均提升了模型性能，对实际场景具有较大应用价值。

Abstract: Deep learning approaches to object detection have achieved reliable detection of specific object classes in images. However, extending a model's detection capability to new object classes requires large amounts of annotated training data, which is costly and time-consuming to acquire, especially for long-tailed classes with insufficient representation in existing datasets. Here, we introduce the object-centric data setting, when limited data is available in the form of object-centric data (multi-view images or 3D models), and systematically evaluate the performance of four different data synthesis methods to finetune object detection models on novel object categories in this setting. The approaches are based on simple image processing techniques, 3D rendering, and image diffusion models, and use object-centric data to synthesize realistic, cluttered images with varying contextual coherence and complexity. We assess how these methods enable models to achieve category-level generalization in real-world data, and demonstrate significant performance boosts within this data-constrained experimental setting.

</details>


### [202] [Visual Generation Tuning](https://arxiv.org/abs/2511.23469)
*Jiahao Guo,Sinan Du,Jingfeng Yao,Wenyu Liu,Bo Li,Haoxiang Cao,Kun Gai,Chun Yuan,Kai Wu,Xinggang Wang*

Main category: cs.CV

TL;DR: 本文提出VGT（Visual Generation Tuning），一种激发视觉-语言模型（VLM）视觉生成能力的新范式，不仅加速自回归建模，还在视觉重建和生成任务上获得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉-语言模型主要优化多模态理解任务，但其潜在的视觉生成能力尚未被充分挖掘和利用。

Method: 作者提出VGT范式，通过对预训练VLM进行高效的视觉生成微调，消除像素级变分自编码器（VAE）的复杂耦合，并用VGT-AE将VLM的语义编码器与像素解码器的隐空间对齐。

Result: VGT在图像重建任务上达到26.67 PSNR与0.50 rFID（28倍压缩率），优于专用VAE。在视觉生成任务上，VGT在GenEval与DPG-Bench评测中取得当前自回归模型最佳成绩（0.77和78.73）。

Conclusion: VGT方法显著降低对齐成本，加快自回归建模收敛速度，具备良好可扩展性和通用性，为多模态基础模型的统一视觉生成能力探索提供了新途径。

Abstract: Large Vision Language Models (VLMs) effectively bridge the modality gap through extensive pretraining, acquiring sophisticated visual representations aligned with language. However, it remains underexplored whether these representations, optimized for multimodal understanding tasks, harbor an inherent potential for visual generation. In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models. By performing efficient visual generation tuning on well-pretrained VLMs, we significantly mitigate the alignment costs and accelerate the convergence of autoregressive modeling in the continuous space (20x speedup). Specifically, we dismiss the entangled pixel-level VAEs designed for diffusion transformers and formulate VGT-AE through aligning the semantic encoders from pretrained VLMs with the latent representations of pixel decoders. In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench. Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models. Models and codes are available at https://github.com/hustvl/VGT.

</details>


### [203] [AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement](https://arxiv.org/abs/2511.23475)
*Zhizhou Zhong,Yicheng Ji,Zhe Kong,Yiying Liu,Jiarui Wang,Jiasun Feng,Lupeng Liu,Xiangyi Wang,Yanjia Li,Yuqing She,Ying Qin,Huan Li,Shuiyang Mao,Wei Liu,Wenhan Luo*

Main category: cs.CV

TL;DR: 本文提出了一种名为AnyTalker的多人人像音频驱动视频生成框架，能在仅有单人视频数据的情况下生成高质量、互动自然的多人人像说话视频。


<details>
  <summary>Details</summary>
Motivation: 现有多人人像视频生成受限于昂贵的数据收集和多身份间互动驱动的难题，尤其在真实多人人像数据稀缺的条件下难以实现高质量合成和互动。

Method: 提出了可扩展的多流处理架构，并在Diffusion Transformer的基础上引入了创新的身份感知注意力机制，能够针对不同身份与音频对进行迭代处理。此外，训练过程主要依赖单人人像视频，仅用极少量的真人多人人像片段做交互细化，并贡献了面向自然度与互动性的评测指标和数据集。

Result: 通过在多个实验上的验证，AnyTalker在口型同步、视觉质量和自然互动方面均取得了显著效果，实现了数据成本与多身份扩展性的平衡。

Conclusion: AnyTalker框架能有效减少多人人像生成的数据依赖，实现高质量多人人像音频驱动视频生成，并为相关领域的评测提供了新的数据集与评价指标。

Abstract: Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.

</details>


### [204] [Video-CoM: Interactive Video Reasoning via Chain of Manipulations](https://arxiv.org/abs/2511.23477)
*Hanoona Rasheed,Mohammed Zumri,Muhammad Maaz,Ming-Hsuan Yang,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: 本文提出了一种新的交互式视频推理范式和模型Video CoM，让模型能够动态主动地与视频进行多步推理，显著提升了视频理解能力，尤其在需要细粒度时空推理的任务上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型在视频理解上虽然取得进展，但推理过程主要靠对视频的静态编码，后续推理均通过文本进行，导致模型无法像人一样主动地反复查看、聚焦、验证视频证据，遇到复杂的时空推理任务时表现不足。

Method: 作者提出交互式视频推理（Interactive Video Reasoning），让模型能将视频视为认知工作空间，通过不断地“操作”视频来收集和精炼证据。具体方法是设计了Video CoM模型，引入多步操作链（Chain of Manipulations, CoM），并构建了18K多步推理指令数据集Video CoM Instruct，进一步结合带推理奖励的强化学习GRPO优化模型的操作策略。与以往主要用稀疏结果奖励不同，本方法还引入推理过程的逐步奖惩，以鼓励更扎实连贯的推理路径。

Result: Video CoM在九个视频推理基准测试上均取得强劲表现，平均提升3.6%，所用标注数据量远少于其他大规模模型；消融实验进一步验证了引入推理奖励可以同步提升准确率和可解释性。

Conclusion: 作者提出的交互式推理方式突破了被动视频理解的瓶颈，模型能够主动分析和查证视频证据，且可通过有限的数据量获得更高的精度和可解释性，为视频多模态大模型的发展提供了新方向和更优解。

Abstract: Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still "think about videos" ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to "think with videos". Our model, Video CoM, reasons through a Chain of Manipulations (CoM), performing iterative visual actions to gather and refine evidence. To support this behavior, we construct Video CoM Instruct, an 18K instruction tuning dataset curated for multi step manipulation reasoning. Beyond supervised learning, we further optimize the manipulation policy via reinforcement learning with reasoning aware Group Relative Policy Optimization (GRPO). Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning. Video CoM achieves strong results across nine video reasoning benchmarks, improving average performance by 3.6 percent over recent state of the art models, while training on only 25K SFT and 3K GRPO video samples, significantly fewer than comparable large scale models. Ablation studies demonstrate that reasoning aware rewards improve both accuracy and interpretability. Code: https://github.com/mbzuai-oryx/Video-CoM

</details>


### [205] [Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models](https://arxiv.org/abs/2511.23478)
*Muhammad Maaz,Hanoona Rasheed,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: 论文提出通过改进多模态大模型的视频推理能力，实现更准确和可信的视觉理解。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型虽然能生成推理过程，但推理常常逻辑不一致或与视觉证据关联不强，即便结果表面上很有说服力。作者发现，模型更依赖于语言先验而非视觉内容，难以实现真正基于视频的推理。

Method: 作者提出两个新诊断指标：推理与答案一致性（TAC）和视频关注分数（VAS），并对11个视频推理数据集进行分析。为提升模型推理，作者设计了结合时序感知的有监督微调和群体相对策略优化（GRPO）的强化学习方法，引入新颖的时序对齐奖励（TAR），以促进推理的时序准确性和因果一致性。

Result: 实验表明，提出的方法训练得到的模型Video R2，在多个基准上TAC、VAS以及准确率均取得领先，显著优于现有模型。

Conclusion: 时序对齐和推理一致性的提升能带来更准确、值得信赖的视频推理能力。该工作丰富了多模态大模型在动态视觉内容理解方面的手段。相关代码、数据集和模型将开源，有助于进一步研究。

Abstract: Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [206] [EvalCards: A Framework for Standardized Evaluation Reporting](https://arxiv.org/abs/2511.21695)
*Ruchira Dhar,Danae Sanchez Villegas,Antonia Karamolegkou,Alice Schiavone,Yifei Yuan,Xinyi Chen,Jiaang Li,Stella Frank,Laura De Grazia,Monorama Swain,Stephanie Brandl,Daniel Hershcovich,Anders Søgaard,Desmond Elliott*

Main category: cs.CL

TL;DR: 论文指出了当前自然语言处理（NLP）领域评估报告存在可复现性、可访问性和治理等问题，并提出了一种新的解决方案——Evaluation Disclosure Cards（EvalCards）。


<details>
  <summary>Details</summary>
Motivation: 随着开放获取模型的快速发布，评估透明性愈发重要。当前评估和文档报告中存在重复性差、难以访问、治理措施不到位的缺陷，亟需改进。

Method: 对现有评估与文档相关工作进行调研，总结出常见问题，并提出一种标准化的新报告工具——EvalCards，来提升透明度和满足治理需求。

Result: EvalCards为研究者和实践者提供了结构化的评估披露方式，有助于提升报告的可复现性、易用性以及对治理流程的适应性。

Conclusion: EvalCards能够有效改进NLP评估过程的透明度和标准化，是推动NLP社区合规和发展的有力工具。

Abstract: Evaluation has long been a central concern in NLP, and transparent reporting practices are more critical than ever in today's landscape of rapidly released open-access models. Drawing on a survey of recent work on evaluation and documentation, we identify three persistent shortcomings in current reporting practices: reproducibility, accessibility, and governance. We argue that existing standardization efforts remain insufficient and introduce Evaluation Disclosure Cards (EvalCards) as a path forward. EvalCards are designed to enhance transparency for both researchers and practitioners while providing a practical foundation to meet emerging governance requirements.

</details>


### [207] [Cacheback: Speculative Decoding With Nothing But Cache](https://arxiv.org/abs/2511.21699)
*Zhiyao Ma,In Gim,Lin Zhong*

Main category: cs.CL

TL;DR: 该论文提出了一种无需训练、与模型无关的推理加速方法——Cacheback Decoding，使用LRU缓存的token n-gram来生成候选序列，有效加快了LLM的推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理速度较慢，现有推理加速方法复杂，需要额外训练或强依赖特定模型；因此需要一种简单、易集成且有效的加速方法。

Method: 该方法仅使用n-gram的LRU缓存表，结合局部性的语言性质实现投机性推理，快速生成草稿序列，无需训练，且适用于各种模型。

Result: Cacheback在同类方法中取得了最先进的加速性能，操作简洁明了，方便集成到现有系统中，并在新领域展现出迅速适应的潜力。

Conclusion: Cacheback Decoding不仅加速了大语言模型的推理，还具备高度的通用性和易用性，为未来高效模型推理系统提供了实用工具。

Abstract: We present Cacheback Decoding, a training-free and model-agnostic speculative decoding method that exploits the locality in language to accelerate Large Language Model (LLM) inference. Cacheback leverages only Least Recently Used (LRU) cache tables of token n-grams to generate draft sequences. Cacheback achieves state-of-the-art performance among comparable methods despite its minimalist design, and its simplicity allows easy integration into existing systems. Cacheback also shows potential for fast adaptation to new domains.

</details>


### [208] [JELV: A Judge of Edit-Level Validity for Evaluation and Automated Reference Expansion in Grammatical Error Correction](https://arxiv.org/abs/2511.21700)
*Yuhao Zhan,Yuqing Zhang,Jing Yuan,Qixiang Ma,Zhiqi Yang,Yu Gu,Zemin Liu,Fei Wu*

Main category: cs.CL

TL;DR: 本论文提出了JELV（编辑级有效性判别器）框架，提升语法纠错系统中参考多样性和评估准确性。


<details>
  <summary>Details</summary>
Motivation: 现有语法纠错（GEC）系统参考多样性有限，影响模型泛化能力和评估准确性。

Method: 提出JELV自动化框架，通过多轮LLM判别和DeBERTa分类器对纠错编辑的语法性、忠实度和流畅性进行判别，并结合人工标注的PEVData数据集验证效果。

Result: LLM判别流程与人工注解达成90%一致率，DeBERTa模型在有效修改上达到85%准确率。JELV辅助下的新评估指标与人工判断高度相关，并用于扩展了BEA19数据集，提高主流GEC系统性能。

Conclusion: JELV为提升语法纠错系统参考多样性、评价方法和模型泛化能力，提供了可扩展、高效的新方案。

Abstract: Existing Grammatical Error Correction (GEC) systems suffer from limited reference diversity, leading to underestimated evaluation and restricted model generalization. To address this issue, we introduce the Judge of Edit-Level Validity (JELV), an automated framework to validate correction edits from grammaticality, faithfulness, and fluency. Using our proposed human-annotated Pair-wise Edit-level Validity Dataset (PEVData) as benchmark, JELV offers two implementations: a multi-turn LLM-as-Judges pipeline achieving 90% agreement with human annotators, and a distilled DeBERTa classifier with 85% precision on valid edits. We then apply JELV to reclassify misjudged false positives in evaluation and derive a comprehensive evaluation metric by integrating false positive decoupling and fluency scoring, resulting in state-of-the-art correlation with human judgments. We also apply JELV to filter LLM-generated correction candidates, expanding the BEA19's single-reference dataset containing 38,692 source sentences. Retraining top GEC systems on this expanded dataset yields measurable performance gains. JELV provides a scalable solution for enhancing reference diversity and strengthening both evaluation and model generalization.

</details>


### [209] [47B Mixture-of-Experts Beats 671B Dense Models on Chinese Medical Examinations](https://arxiv.org/abs/2511.21701)
*Chiung-Yi Tseng,Danyang Zhang,Tianyang Wang,Hongying Luo,Lu Chen,Junming Huang,Jibin Guan,Junfeng Hao,Junhao Song,Ziqian Bi*

Main category: cs.CL

TL;DR: 本文对27种先进的大语言模型（LLMs）在中文医学考试题目上的表现进行了全面评测，涵盖7个医学专科和两个专业级别，揭示了模型在不同复杂度及学科间的能力差异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型技术的快速进展，其在医学领域的应用潜力引发了极大关注。针对缺乏系统性评测的问题，作者希望通过构建权威基准测试，客观比较各主流LLM的医学题目解答能力，并分析其临床与教育应用的可行性。

Method: 作者设计了一个评测框架，使用2800道涵盖心血管、消化、血液、感染、肾脏、神经、呼吸7大专科、区分主治和高级职称难度的医学题目，对27种LLM进行系统对比分析，并细致考察模型大小和结构、不同领域、难度分级下的表现。

Result: 结果显示模型表现差异显著：Mixtral-8x7B以74.25%的准确率居首，DeepSeek-R1-67B次之为64.07%；“专家混合”架构的小型模型也能超过部分大规模模型。各专科间正确率有较大差距：心血管和神经科表现优于消化和肾脏。部分领先模型在高级题目上表现未显著下降，表明其具良好泛化能力。

Conclusion: 本基准测试为医学教育与临床决策系统中部署LLMs提供了关键参考，揭示其在特定医学领域的潜力与不足，强调了当前技术仍有进步空间。

Abstract: The rapid advancement of large language models(LLMs) has prompted significant interest in their potential applications in medical domains. This paper presents a comprehensive benchmark evaluation of 27 state-of-the-art LLMs on Chinese medical examination questions, encompassing seven medical specialties across two professional levels. We introduce a robust evaluation framework that assesses model performance on 2,800 carefully curated questions from cardiovascular, gastroenterology, hematology, infectious diseases, nephrology, neurology, and respiratory medicine domains. Our dataset distinguishes between attending physician and senior physician difficulty levels, providing nuanced insights into model capabilities across varying complexity. Our empirical analysis reveals substantial performance variations among models, with Mixtral-8x7B achieving the highest overall accuracy of 74.25%, followed by DeepSeek-R1-671B at 64.07%. Notably, we observe no consistent correlation between model size and performance, as evidenced by the strong performance of smaller mixture-of-experts architectures. The evaluation demonstrates significant performance gaps between medical specialties, with models generally performing better on cardiovascular and neurology questions compared to gastroenterology and nephrology domains. Furthermore, our analysis indicates minimal performance degradation between attending and senior physician levels for top-performing models, suggesting robust generalization capabilities. This benchmark provides critical insights for the deployment of LLMs in medical education and clinical decision support systems, highlighting both the promise and current limitations of these technologies in specialized medical contexts.

</details>


### [210] [CSV-Decode: Certifiable Sub-Vocabulary Decoding for Efficient Large Language Model Inference](https://arxiv.org/abs/2511.21702)
*Dong Liu,Yanxuan Yu,Ben Lengerich*

Main category: cs.CL

TL;DR: 该论文提出CSV-Decode方法，通过在每一步生成时为大模型构建小型子词表，加速推理输出层计算，既高效又保准输出分布的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型由于输出层需遍历大词表，导致推理计算瓶颈严重。优化输出层计算以提升解码效率成为实际部署时的迫切需求。

Method: 作者提出一种基于几何上界的子词表选择方法。具体做法是对词表embedding离线聚类，利用聚类中心和半径判断每步可省略哪些词，动态构建子词表，只对部分token做软/硬max计算。技术实现方面包含稀疏GEMV、分布式GPU分片及CUDA Graph优化。

Result: 通过实验，CSV-Decode在准确保分布特性的前提下，显著加速大模型解码，且回退率低。

Conclusion: CSV-Decode能在保持生成质量和概率分布正确性的同时，极大提升大词表大模型的推理解码速度，提升了实用性和部署价值。

Abstract: Large language models face significant computational bottlenecks during inference due to the expensive output layer computation over large vocabularies. We present CSV-Decode, a novel approach that uses geometric upper bounds to construct small sub-vocabularies for each decoding step, enabling efficient sparse computation while maintaining dual correctness guarantees: exact top-$k$ certification and $\varepsilon$-certified softmax approximations. Our method clusters vocabulary embeddings offline and uses centroid-plus-radius bounds to identify which tokens can be safely omitted from computation. We provide a complete system implementation with sparse GEMV kernels, multi-GPU sharding, and CUDA Graph optimization. Experimental results demonstrate significant speedup over full vocabulary decoding while maintaining distributional guarantees and low fallback rates. Our code implementation available at \href{https://github.com/FastLM/CSV-Decode}{https://github.com/FastLM/CSV-Decode}.

</details>


### [211] [Evaluating Embedding Generalization: How LLMs, LoRA, and SLERP Shape Representational Geometry](https://arxiv.org/abs/2511.21703)
*Siyaxolisa Kabane*

Main category: cs.CL

TL;DR: 本文比较了大语言模型（LLM）和非LLM编码器在生成文本嵌入时的泛化能力，并研究了球面线性插值（SLERP）模型融合在缓解适应任务过拟合中的作用。通过对短数字序列进行聚类和分类实验，结果显示SLERP融合能在保持任务性能的同时提高泛化与稳健性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在文本嵌入任务中的广泛应用，如何兼顾下游任务适应性和泛化能力成为关键挑战。常用的任务特定适应（如LoRA）常导致模型过于专化，影响泛化，因此作者希望找到更优的模型融合方法，并系统性地比较不同类型编码器的泛化表现。

Method: 设计了受控实验，将四类模型应用于短数列嵌入与数论特征聚类/分类任务：1）非LLM编码器；2）基于LLM的LoRA自适应编码器；3）模型soup方法合并后的LoRA-LLM；4）基于SLERP融合的LoRA-LLM。评估通过聚类指标和kmeans标签分析完成。

Result: LLM主干对数值模式具有更好捕捉能力，但LoRA等方法易导致过于专化，影响泛化。SLERP融合能恢复基础模型结构，并保留大部分任务收益，在聚类分离性与鲁棒性等方面表现优于其他融合方式或未融合模型。

Conclusion: 对于需要在专用任务适应性与泛化能力间权衡的文本嵌入任务，采用SLERP融合的LLM自适应模型能取得更优表现，提升嵌入的实用性和稳健性。

Abstract: We investigate the generalization properties of dense text embeddings when the embedding backbone is a large language model (LLM) versus when it is a non-LLM encoder, and we study the extent to which spherical linear interpolation (SLERP) model-merging mitigates over-specialization introduced by task-specific adaptation (e.g., LoRA). To make the comparison concrete and domain-agnostic, we design a controlled suite of experiments in which models embed short numerical sequences and are evaluated on their ability to cluster and classify those sequences according to well-defined number-theoretic properties. Our experimental protocol compares four families of models: (1) non-LLM encoders trained from scratch or fine-tuned for embeddings, (2) LLM-based encoders adapted with parameter-efficient methods (LoRA), (3) LLM-based encoders with LoRA followed by model souping merging into the base weights, and (4) the same LoRA-adapted LLMs merged using SLERP across checkpoints or stages. We evaluate representational quality with clustering indices (Silhouette and Davies Bouldin). We additionally analyze the use of kmeans labels to see if the embeddings encode any other information besides the one we are testing for. Empirically, we find that LLM-based backbones produce embeddings that better capture higher-order, compositional numeric patterns, but are prone to adapter dominance that degrades balanced generalization; SLERP merging consistently recovers base-model structure while retaining most task gains, yielding superior tradeoffs in clustering separability, and robustness compared to model souping or models that were not merged.

</details>


### [212] [On the Cross-lingual Transferability of Pre-trained wav2vec2-based Models](https://arxiv.org/abs/2511.21704)
*Jonatas Grosman,Cassio Almeida,Guilherme Schardong,Hélio Lopes*

Main category: cs.CL

TL;DR: 本文研究了wav2vec2大规模预训练模型在跨语言语音识别任务中的迁移能力，主要发现预训练数据的多样性比规模更关键，相似语言间迁移效果更佳。


<details>
  <summary>Details</summary>
Motivation: 虽然wav2vec2及其变体已在多项语音任务中取得SOTA，但关于预训练模型在跨语言任务中的知识迁移性质，仍缺乏系统性研究，尤其是在预训练语言与目标语言不同时。因此，作者希望深入了解这些模型的跨语言迁移表现及影响因素。

Method: 作者选用15种大规模预训练模型，在18种语言的语音识别任务上进行微调实验，对比分析不同模型的迁移效能，重点考察预训练数据的规模与多样性、模型与下游语言的相似性等因素对最终效果的影响。

Result: 实验结果表明：1）预训练数据的多样性比数据的数量更重要；2）印欧语系语言的迁移表现总体优于非印欧语系语言；3）即使是单语预训练模型也能跨语言迁移，且来源语言与目标语言越相似，迁移效果越明显。

Conclusion: 作者总结认为，选择预训练wav2vec2模型时应重点关注数据多样性和语言相似性，建议科学界合理利用及设计多样化语言的预训练模型，以提升不同语言的语音识别效能。

Abstract: Using representations provided by a large pre-trained model has become the primary strategy for achieving state-of-the-art results in a wide range of tasks. A recently proposed large pre-trained model, wav2vec 2.0, was seminal for several other works on pre-training large models on speech data. Many models are being pre-trained using the same architecture as wav2vec 2.0 and are getting state-of-the-art in various speech-related tasks. Previous work has demonstrated that the data used during the pre-training of these wav2vec2-based models can impact the model's performance in downstream tasks, and this should be taken into consideration before utilizing these models. However, few works have proposed investigating further how the transfer knowledge of these pre-trained models behaves in different languages, even when the target language differs from the one used during the model's pre-training. Our work aims to investigate the cross-lingual transferability of these wav2vec2-based models. We performed several fine-tuning experiments on the speech recognition task in 18 languages using 15 large pre-trained models. The results of our experiments showed us that the size of data used during the pre-training of these models is not as important to the final performance as the diversity. We noticed that the performance of Indo-European languages is superior to non-Indo-European languages in the evaluated models. We have observed a positive cross-lingual transfer of knowledge using monolingual models, which was evident in all the languages we used, but more pronounced when the language used during pre-training was more similar to the downstream task language. With these findings, we aim to assist the scientific community in utilizing existing wav2vec2-based pre-trained models, as well as facilitate the pre-training of new ones.

</details>


### [213] [Insight-A: Attribution-aware for Multimodal Misinformation Detection](https://arxiv.org/abs/2511.21705)
*Junjie Wu,Yumeng Fu,Chen Gong,Guohong Fu*

Main category: cs.CL

TL;DR: 本文提出Insight-A模型，结合多模态大语言模型的洞察，实现对多模态谣言的归因检测，可显著提升AIGC时代假信息识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型虽然能检测多模态假信息，但往往忽略了其来源归因。AIGC导致虚假内容生成更加隐蔽，社会安全风险凸显，因此亟需模型能智能判别并归因虚假信息形成的具体伪造来源。

Method: 提出Insight-A模型，包含两大创新：1）利用跨归因提示（CAP）结合感知与推理能力识别伪造痕迹，2）采用自动归因去偏提示（ADP）减小人工提示主观性，并结合图像描述（IC）加强跨模态一致性检查。这些模块共同构筑分层推理管线以检测不同模态下的失真与伪造。

Result: 通过大量实验验证，所提方法在多模态虚假信息检测任务上优于现有方案，在实际数据集上取得了更高的准确率和归因能力。

Conclusion: Insight-A为多模态假信息检测与归因提供了新的方法，显著提升了AIGC生成内容下安全性和检测效率，对相关领域有重要实际和理论价值。

Abstract: AI-generated content (AIGC) technology has emerged as a prevalent alternative to create multimodal misinformation on social media platforms, posing unprecedented threats to societal safety. However, standard prompting leverages multimodal large language models (MLLMs) to identify the emerging misinformation, which ignores the misinformation attribution. To this end, we present Insight-A, exploring attribution with MLLM insights for detecting multimodal misinformation. Insight-A makes two efforts: I) attribute misinformation to forgery sources, and II) an effective pipeline with hierarchical reasoning that detects distortions across modalities. Specifically, to attribute misinformation to forgery traces based on generation patterns, we devise cross-attribution prompting (CAP) to model the sophisticated correlations between perception and reasoning. Meanwhile, to reduce the subjectivity of human-annotated prompts, automatic attribution-debiased prompting (ADP) is used for task adaptation on MLLMs. Additionally, we design image captioning (IC) to achieve visual details for enhancing cross-modal consistency checking. Extensive experiments demonstrate the superiority of our proposal and provide a new paradigm for multimodal misinformation detection in the era of AIGC.

</details>


### [214] [A General Highly Accurate Online Planning Method Integrating Large Language Models into Nested Rollout Policy Adaptation for Dialogue Tasks](https://arxiv.org/abs/2511.21706)
*Hui Wang,Fafa Zhang,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.CL

TL;DR: 本论文提出了一种新的面向目标对话策略规划方法NRPA-GD，结合大语言模型同时模拟用户和系统行为，无需特定模型训练，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有面向目标的对话策略主要依赖人工设计prompt或训练特定策略网络，这些方法依赖人工经验或代价高且难以适应新场景，亟需更高效灵活的解决方案。

Method: 提出了一种名为NRPA-GD的新方法：1）利用大语言模型同时模拟用户和系统行为，2）构建对完整对话轨迹的评估机制，3）通过嵌套蒙特卡洛模拟和策略自适应优化对话策略，实现动态策略调整，且完全无需额外模型训练。

Result: 在四个典型数据集上的实验显示，NRPA-GD性能优于基于prompt和专门预训练的策略模型。值得注意的是，仅使用6亿参数的LLM即可超越ChatGPT和更大预训练模型。

Conclusion: NRPA-GD不仅提升了面向目标对话的效果，还首次充分证明了将规划方法结合到LLM中可高效解决实际规划任务的优势和创新性。

Abstract: In goal-oriented dialogue tasks, the main challenge is to steer the interaction towards a given goal within a limited number of turns. Existing approaches either rely on elaborate prompt engineering, whose effectiveness is heavily dependent on human experience, or integrate policy networks and pre-trained policy models, which are usually difficult to adapt to new dialogue scenarios and costly to train. Therefore, in this paper, we present Nested Rollout Policy Adaptation for Goal-oriented Dialogue (NRPA-GD), a novel dialogue policy planning method that completely avoids specific model training by utilizing a Large Language Model (LLM) to simulate behaviors of user and system at the same time. Specifically, NRPA-GD constructs a complete evaluation mechanism for dialogue trajectories and employs an optimization framework of nested Monte Carlo simulation and policy self-adaptation to dynamically adjust policies during the dialogue process. The experimental results on four typical goal-oriented dialogue datasets show that NRPA-GD outperforms both existing prompt engineering and specifically pre-trained model-based methods. Impressively, NRPA-GD surpasses ChatGPT and pre-trained policy models with only a 0.6-billion-parameter LLM. The proposed approach further demonstrates the advantages and novelty of employing planning methods on LLMs to solve practical planning tasks.

</details>


### [215] [Lost in the Pipeline: How Well Do Large Language Models Handle Data Preparation?](https://arxiv.org/abs/2511.21708)
*Matteo Spreafico,Ludovica Tassini,Camilla Sancricca,Cinzia Cappiello*

Main category: cs.CL

TL;DR: 本文探索了大语言模型（LLMs）在数据准备（数据剖析和清洗）任务中的自动化和辅助潜力，并对比了其与传统工具的优缺点。


<details>
  <summary>Details</summary>
Motivation: 数据准备是数据驱动过程中非常关键但繁琐的步骤，人工操作成本高。研究大语言模型是否能有效缓解这一环节的劳动负担，为数据科学工作流程降本增效。

Method: 选择了通用型和在表格数据上微调的大语言模型，输入低质量数据集，让模型自动执行数据剖析和清洗任务，并与传统数据准备工具的效果对比。为评估效果，还设计了定制化的质量评估模型，并通过用户调研检验其合理性。

Result: 实验展现了大语言模型在数据准备任务中的能力，并对比了其与传统工具的表现。结合用户调研，揭示了模型在清洗准确性、易用性等方面的优势与不足。

Conclusion: 大语言模型能够在一定程度上自动执行和支持数据准备任务，但也存在局限。相较传统工具，LLM在智能化和交互体验上有明显提升，但在特定复杂场景下能力仍有不足。未来有望进一步提升其实用性。

Abstract: Large language models have recently demonstrated their exceptional capabilities in supporting and automating various tasks. Among the tasks worth exploring for testing large language model capabilities, we considered data preparation, a critical yet often labor-intensive step in data-driven processes. This paper investigates whether large language models can effectively support users in selecting and automating data preparation tasks. To this aim, we considered both general-purpose and fine-tuned tabular large language models. We prompted these models with poor-quality datasets and measured their ability to perform tasks such as data profiling and cleaning. We also compare the support provided by large language models with that offered by traditional data preparation tools. To evaluate the capabilities of large language models, we developed a custom-designed quality model that has been validated through a user study to gain insights into practitioners' expectations.

</details>


### [216] [Quantifying and Mitigating Selection Bias in LLMs: A Transferable LoRA Fine-Tuning and Efficient Majority Voting Approach](https://arxiv.org/abs/2511.21709)
*Blessed Guda,Lawrence Francis,Gabrial Zencha Ashungafac,Carlee Joe-Wong,Moise Busogi*

Main category: cs.CL

TL;DR: 本论文针对大语言模型（LLM）在多项选择题（MCQ）任务中存在的选择偏差问题，提出了新的无监督指标与高效偏差缓解方法，并验证其效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在MCQ任务中常受到答案位置、选项符号等非内容因素影响，产生选择偏差，削弱了MCQ评测的可靠性。现有的偏差评估和缓解方法要么依赖答案标签、难以全面度量，要么计算成本高、泛化性弱，因此亟需新工具和方法解决上述问题。

Method: 提出三项主要创新：1）无监督、无需标签的新偏差指标Permutation Bias Metric (PBM)，可以直接量化模型在选项顺序变动下预测的一致性；2）高效的多数投票法Batch Question-Context KV caching (BaQCKV)，能大幅降低投票所需算力，同时仍有效缓解偏差；3）基于PBM与BaQCKV的无监督Low-Rank Adaptation (LoRA-1)微调方案，进一步减轻偏差并保持泛化能力。

Result: 在多个MCQ基准集上的实验表明，所提新指标和方法能有效降低LLM的选择偏差，提高答案一致性，同时显著减少所需计算资源。

Conclusion: 本研究为LLM在MCQ评测场景下的偏差度量和缓解提供了更精准、高效且具有良好泛化能力的解决方案，有助于提升LLM评测的科学性和公正性。

Abstract: Multiple Choice Question (MCQ) answering is a widely used method for evaluating the performance of Large Language Models (LLMs). However, LLMs often exhibit selection bias in MCQ tasks, where their choices are influenced by factors like answer position or option symbols rather than the content. This bias undermines the reliability of MCQ as an evaluation framework. Most existing selection bias metrics require answer labels and measure divergences between prediction and answer distributions, but do not fully capture the consistency of a model's predictions across different orderings of answer choices. Existing selection bias mitigation strategies have notable limitations: majority voting, though effective, is computationally prohibitive; calibration-based methods require validation sets and often fail to generalize across datasets. To address these gaps, we propose three key contributions: (1) a new unsupervised label-free Permutation Bias Metric (PBM) that directly quantifies inconsistencies in model predictions across answer permutations, providing a more precise measure of selection bias, (2) an efficient majority voting approach called Batch Question-Context KV caching (BaQCKV), to significantly reduce computational costs while preserving bias mitigation effectiveness, and (3) an unsupervised Low-Rank Adaptation (LoRA-1) fine-tuning strategy based on our proposed metric and the BaQCKV that mitigates selection bias, providing a computationally efficient alternative that maintains model generalizability. Experiments across multiple MCQ benchmarks demonstrate that our approaches reduce bias, increasing consistency in accuracy while minimizing computational costs.

</details>


### [217] [Addressing Stereotypes in Large Language Models: A Critical Examination and Mitigation](https://arxiv.org/abs/2511.21711)
*Fatima Kazi*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLMs）在不同偏见基准（如StereoSet和CrowSPairs）上的表现，发现它们在性别偏见上存在问题，但对种族偏见有较好规避。通过微调、提示工程和数据增强等方法提升后，模型在跨数据集和隐性偏见检测中表现提升明显。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型因训练数据中存在的偏见，会产生有害的社会、文化、种族等刻板印象。为确保生成结果的公正性和准确性，深入分析并缓解这些偏见非常重要。

Method: 1）采用StereoSet和CrowSPairs等偏见评估基准测试不同生成模型（如BERT、GPT-3.5、ADA）；2）结合三重分析方法检测显性和隐性偏见；3）针对模型表现，通过微调、不同提示方式以及数据增强进行性能改进。

Result: 微调模型在性别偏见判断方面表现较弱，在种族偏见规避方面表现较好。实验指出，模型容易依赖关键字而非真正理解内容。经过微调与数据增强后，模型在跨数据集和隐性偏见测试上的表现显著提升，最高提升幅度达20%。

Conclusion: 当前LLMs在理解与规避偏见方面仍有不足，尤其是在性别偏见和理解内容准确性上。完善的微调及增强学习方法能够有效提升模型在消除隐性偏见上的能力，但去偏和深层次理解问题仍需持续探索。

Abstract: Large Language models (LLMs), such as ChatGPT, have gained popularity in recent years with the advancement of Natural Language Processing (NLP), with use cases spanning many disciplines and daily lives as well. LLMs inherit explicit and implicit biases from the datasets they were trained on; these biases can include social, ethical, cultural, religious, and other prejudices and stereotypes. It is important to comprehensively examine such shortcomings by identifying the existence and extent of such biases, recognizing the origin, and attempting to mitigate such biased outputs to ensure fair outputs to reduce harmful stereotypes and misinformation. This study inspects and highlights the need to address biases in LLMs amid growing generative Artificial Intelligence (AI). We utilize bias-specific benchmarks such StereoSet and CrowSPairs to evaluate the existence of various biases in many different generative models such as BERT, GPT 3.5, and ADA. To detect both explicit and implicit biases, we adopt a three-pronged approach for thorough and inclusive analysis. Results indicate fine-tuned models struggle with gender biases but excel at identifying and avoiding racial biases. Our findings also illustrated that despite some cases of success, LLMs often over-rely on keywords in prompts and its outputs. This demonstrates the incapability of LLMs to attempt to truly understand the accuracy and authenticity of its outputs. Finally, in an attempt to bolster model performance, we applied an enhancement learning strategy involving fine-tuning, models using different prompting techniques, and data augmentation of the bias benchmarks. We found fine-tuned models to exhibit promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.

</details>


### [218] [EulerESG: Automating ESG Disclosure Analysis with LLMs](https://arxiv.org/abs/2511.21712)
*Yi Ding,Xushuo Tang,Zhengyi Yang,Wenqian Zhang,Simin Wu,Yuxin Huang,Lingjing Lan,Weiyuan Li,Yin Chen,Mingchen Ju,Wenke Yang,Thong Hoang,Mykhailo Klymenko,Xiwei Zu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种名为EulerESG的系统，利用大模型自动分析ESG报告，实现高准确率的信息抽取和标准化展示。


<details>
  <summary>Details</summary>
Motivation: 当前ESG报告大多为冗长且格式不一的PDF文档，难以系统性地提取和比对关键信息。现有工具基于规则或忽略报告背后标准，导致信息提取有限且不具备针对性。

Method: EulerESG结合了基于LLM的大模型分析和双通道检索技术，显式建模ESG报告中的标准，能自动解析报告并以结构化方式展示。系统还提供交互式仪表板和聊天机器人，实现信息探索和解释。

Result: 在四家全球知名公司和十二个SASB子行业的数据集上，EulerESG可自动、准确地生成与报告标准对齐的数据表，平均准确率高达0.95，并且运行效率较高。文中还对比了多种大模型在这一应用下的表现。

Conclusion: EulerESG证明了基于大模型和标准感知的信息抽取框架在ESG分析应用中的有效性和实用性，有助于推动相关信息披露的自动化与标准化。

Abstract: Environmental, Social, and Governance (ESG) reports have become central to how companies communicate climate risk, social impact, and governance practices, yet they are still published primarily as long, heterogeneous PDF documents. This makes it difficult to systematically answer seemingly simple questions. Existing tools either rely on brittle rule-based extraction or treat ESG reports as generic text, without explicitly modelling the underlying reporting standards. We present \textbf{EulerESG}, an LLM-powered system for automating ESG disclosure analysis with explicit awareness of ESG frameworks. EulerESG combines (i) dual-channel retrieval and LLM-driven disclosure analysis over ESG reports, and (ii) an interactive dashboard and chatbot for exploration, benchmarking, and explanation. Using four globally recognised companies and twelve SASB sub-industries, we show that EulerESG can automatically populate standard-aligned metric tables with high fidelity (up to 0.95 average accuracy) while remaining practical in end-to-end runtime, and we compare several recent LLM models in this setting. The full implementation, together with a demonstration video, is publicly available at https://github.com/UNSW-database/EulerESG.

</details>


### [219] [GPS: General Per-Sample Prompter](https://arxiv.org/abs/2511.21714)
*Pawel Batorski,Paul Swoboda*

Main category: cs.CL

TL;DR: 提出了一种名为GPS的通用逐样本自动提示方法，不依赖任务特定调优，可以针对每个输入自适应生成提示，大幅简化了提示工程。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）对提示词极为敏感，手动设计高效的提示既耗时又困难。现有自动化提示方法依赖大规模数据、长时间优化且只能生成普通任务级提示，无法针对具体输入自适应。

Method: 提出了一种无任务特定微调的逐样本自适应自动提示方法GPS。核心由强化学习训练的提示生成器组成，并加入了新的正则化手段，实现对逐样本提示的自适应。推理时进一步采用最小贝叶斯风险解码稳定性能。

Result: 在文本简化、摘要、分类等任务中，GPS未经过任务定向训练就能取得接近最优甚至领先的表现，尤其在GSM8K数据集上达到SOTA。

Conclusion: GPS展示了一种高效、易泛化的自适应自动提示新范式，无需大量任务数据和繁琐优化，推动LLM实际应用落地。

Abstract: LLMs are sensitive to prompting, with task performance often hinging on subtle, sometimes imperceptible variations in phrasing. As a result, crafting effective prompts manually remains challenging and time-consuming. Recent automatic prompting methods mitigate this difficulty but face three key limitations: (i) for each new task, they require large datasets to train good prompts;(ii) they rely on costly optimization loops that may take hours; (iii)they typically produce a single task-level prompt that does not adapt to the individual input problem to be solved.
  We propose GPS, the first general-purpose, per-sample prompting method. Without any task-specific tuning, GPS generates a tailored prompt for each unseen input, improving performance across diverse tasks. The prompter is trained with reinforcement learning on a suite of training tasks and includes a novel regularization for effectively adapting to per-sample prompting. Finally, we employ Minimum Bayes Risk decoding to stabilize inference.
  Empirically, GPS demonstrates competitive performance: we attain second best results among baselines on text simplification, third best results on summarization and on-par results on classification, while not training on any of these tasks, in contrast to the baselines. For in-domain prompting, we obtain sota on GSM8K. Our work shows the potential of a novel and effective paradigm for automatic prompting: generating adaptive, input-specific prompts without extensive optimization and without access to a task-specific training set. Our code is available at https://github.com/Batorskq/GPS.

</details>


### [220] [An Optimized Machine Learning Classifier for Detecting Fake Reviews Using Extracted Features](https://arxiv.org/abs/2511.21716)
*Shabbir Anees,Anshuman,Ayush Chaurasia,Prathmesh Bogar*

Main category: cs.CL

TL;DR: 该论文提出了一种基于机器学习的新方法，能够高精度区分人类和AI生成的评论，提高了识别虚假评论的能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成的评论逐渐增多，消费者难以分辨真假评论，这影响了电商平台的可信度。因此亟需更精准的检测手段。

Method: 方法包括高级文本预处理、多模态特征提取、Harris Hawks优化算法（HHO）用于特征选择，以及集成堆叠分类器。通过从13,539个初始特征中选取了1,368个最有效特征，大大降低了维度。

Result: 最终提出的堆叠模型在识别AI生成评论上取得了95.40%的准确率、92.81%的精准率、95.01%的召回率，F1值为93.90%。

Conclusion: 集成学习结合仿生特征优化方法对识别机生成文本极为有效。未来大规模部署需关注用户数据安全，建议结合差分隐私和安全外包等方法。

Abstract: It is well known that fraudulent reviews cast doubt on the legitimacy and dependability of online purchases. The most recent development that leads customers towards darkness is the appearance of human reviews in computer-generated (CG) ones. In this work, we present an advanced machine-learning-based system that analyses these reviews produced by AI with remarkable precision. Our method integrates advanced text preprocessing, multi-modal feature extraction, Harris Hawks Optimization (HHO) for feature selection, and a stacking ensemble classifier. We implemented this methodology on a public dataset of 40,432 Original (OR) and Computer-Generated (CG) reviews. From an initial set of 13,539 features, HHO selected the most applicable 1,368 features, achieving an 89.9% dimensionality reduction. Our final stacking model achieved 95.40% accuracy, 92.81% precision, 95.01% recall, and a 93.90% F1-Score, which demonstrates that the combination of ensemble learning and bio-inspired optimisation is an effective method for machine-generated text recognition. Because large-scale review analytics commonly run on cloud platforms, privacy-preserving techniques such as differential approaches and secure outsourcing are essential to protect user data in these systems.

</details>


### [221] [CrossCheck-Bench: Diagnosing Compositional Failures in Multimodal Conflict Resolution](https://arxiv.org/abs/2511.21717)
*Baoliang Tian,Yuxuan Si,Jilong Wang,Lingyao Li,Zhongyuan Bao,Zineng Zhou,Tao Wang,Sixu Li,Ziyao Xu,Mingze Wang,Zhouzhuo Zhang,Zhihao Wang,Yike Yun,Ke Tian,Ning Yang,Minghui Qiu*

Main category: cs.CL

TL;DR: 本文提出了CrossCheck-Bench基准，用于评测多模态大模型检测和解决视觉-文本冲突的能力。通过分析13个现有主流模型发现：它们在简单任务上表现良好，但在需要整合多线索推理时能力大幅下降，表明多模态推理仍存在瓶颈。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型主要在视觉与文本对齐任务上进行训练和评测，缺乏对现实世界中跨模态冲突检测和推理能力的考察。现实应用中，图片和文本信息常常不一致，需要模型具备更高层次的结构化推理能力。

Method: 设计并构建了CrossCheck-Bench基准，包括15000个人工生成冲突的真实多模态问答对，分为三个推理复杂度等级和七个基本能力，通过多阶段专家标注保障数据难度和语义有效性，并对13种主流视觉-语言模型进行系统评测和能力剖析。

Result: 大部分模型在单一实体识别等感知类任务表现良好，在复杂的逻辑矛盾检测和多线索推理任务上表现普遍下滑。常见的提示工程策略（如Chain-of-Thought等）提升有限，而符号推理与视觉处理结合的方法则改善更显著。

Conclusion: 现有多模态大模型仍难以稳健处理跨模态冲突检验，对更高层次推理能力存在瓶颈，需要发展结合符号推理的新方法以提升跨模态验证与推理水平。

Abstract: Multimodal Large Language Models are primarily trained and evaluated on aligned image-text pairs, which leaves their ability to detect and resolve real-world inconsistencies largely unexplored. In open-domain applications visual and textual cues often conflict, requiring models to perform structured reasoning beyond surface-level alignment. We introduce CrossCheck-Bench, a diagnostic benchmark for evaluating contradiction detection in multimodal inputs. The benchmark adopts a hierarchical task framework covering three levels of reasoning complexity and defines seven atomic capabilities essential for resolving cross-modal inconsistencies. CrossCheck-Bench includes 15k question-answer pairs sourced from real-world artifacts with synthetically injected contradictions. The dataset is constructed through a multi-stage annotation pipeline involving more than 450 expert hours to ensure semantic validity and calibrated difficulty across perception, integration, and reasoning. We evaluate 13 state-of-the-art vision-language models and observe a consistent performance drop as tasks shift from perceptual matching to logical contradiction detection. Most models perform well on isolated entity recognition but fail when multiple clues must be synthesized for conflict reasoning. Capability-level analysis further reveals uneven skill acquisition, especially in tasks requiring multi-step inference or rule-based validation. Additional probing shows that conventional prompting strategies such as Chain-of-Thought and Set-of-Mark yield only marginal gains. By contrast, methods that interleave symbolic reasoning with grounded visual processing achieve more stable improvements. These results highlight a persistent bottleneck in multimodal reasoning and suggest new directions for building models capable of robust cross-modal verification.

</details>


### [222] [When Harmless Words Harm: A New Threat to LLM Safety via Conceptual Triggers](https://arxiv.org/abs/2511.21718)
*Zhaoxin Zhang,Borui Chen,Yiming Hu,Youyang Qu,Tianqing Zhu,Longxiang Gao*

Main category: cs.CL

TL;DR: 论文提出了一种新的越狱方法MICM，能够绕过大语言模型的安全机制，通过操控模型输出中的潜在价值取向，生成不合规内容。该方法成功攻破多款主流LLM。


<details>
  <summary>Details</summary>
Motivation: 当前LLM越狱研究主要关注直接生成有害输出，忽视了通过抽象价值操控诱导模型输出隐晦不当内容的威胁，这给现有对齐和安全策略带来漏洞。

Method: 提出了一种基于概念形态学理论的模型不可知越狱方法MICM。它通过固定模板和预设短语（即概念触发器）隐蔽编码特定价值立场，从而诱导LLM输出特定倾向内容且不被现有安全过滤系统阻挡。

Result: 在GPT-4o、Deepseek-R1、Qwen3-8B等五款先进LLM上实验，MICM在突破安全机制和提升越狱成功率方面均优于其他最新越狱技术，并且被系统拒绝的次数极少。

Conclusion: 商业LLM的现有安全防护对价值操控型隐蔽越狱攻击并不健壮，暴露出重要安全隐患。

Abstract: Recent research on large language model (LLM) jailbreaks has primarily focused on techniques that bypass safety mechanisms to elicit overtly harmful outputs. However, such efforts often overlook attacks that exploit the model's capacity for abstract generalization, creating a critical blind spot in current alignment strategies. This gap enables adversaries to induce objectionable content by subtly manipulating the implicit social values embedded in model outputs. In this paper, we introduce MICM, a novel, model-agnostic jailbreak method that targets the aggregate value structure reflected in LLM responses. Drawing on conceptual morphology theory, MICM encodes specific configurations of nuanced concepts into a fixed prompt template through a predefined set of phrases. These phrases act as conceptual triggers, steering model outputs toward a specific value stance without triggering conventional safety filters. We evaluate MICM across five advanced LLMs, including GPT-4o, Deepseek-R1, and Qwen3-8B. Experimental results show that MICM consistently outperforms state-of-the-art jailbreak techniques, achieving high success rates with minimal rejection. Our findings reveal a critical vulnerability in commercial LLMs: their safety mechanisms remain susceptible to covert manipulation of underlying value alignment.

</details>


### [223] [PeerCoPilot: A Language Model-Powered Assistant for Behavioral Health Organizations](https://arxiv.org/abs/2511.21721)
*Gao Mo,Naveen Raman,Megan Chai,Cindy Peng,Shannon Pagdon,Nev Jones,Hong Shen,Peggy Swarbrick,Fei Fang*

Main category: cs.CL

TL;DR: 本论文介绍了PeerCoPilot，这是一款基于大语言模型（LLM）的助手，旨在帮助同伴支持型行为健康组织（PROs）更好地满足服务对象多方面的需求，并通过评估与真实用户反馈证明了其有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 行为健康问题（包括心理健康和物质使用障碍）在美国负担极重，而同伴支持型机构（PROs）承担了综合服务的重任，但常由于资源和人手有限，难以全面满足服务对象需求，因此需要智能工具辅助同伴支持者提升服务效率和效果。

Method: 提出并实现了PeerCoPilot——一个结合大语言模型和检索增强生成（RAG）技术的智能助手，利用超过1300条经验证的资源数据库，辅助同伴支持者制定健康计划、设定分步目标、查找资源，并邀请15位同伴支持者和6位服务对象参与人工评价，比较系统与基线LLM的表现。

Result: 超过90%的评测用户认可PeerCoPilot的应用价值。此外，相较于基线LLM，PeerCoPilot在信息的可靠性和针对性上表现更优，目前已在CSPNJ机构实际部署，由5-10名同伴支持者用于服务一万余名对象，并计划继续推广。

Conclusion: PeerCoPilot显著提升了同伴支持型行为健康组织的服务能力，其更可靠、个性化的信息支持获得用户高度认可，有望成为该领域重要的智能辅助工具。

Abstract: Behavioral health conditions, which include mental health and substance use disorders, are the leading disease burden in the United States. Peer-run behavioral health organizations (PROs) critically assist individuals facing these conditions by combining mental health services with assistance for needs such as income, employment, and housing. However, limited funds and staffing make it difficult for PROs to address all service user needs. To assist peer providers at PROs with their day-to-day tasks, we introduce PeerCoPilot, a large language model (LLM)-powered assistant that helps peer providers create wellness plans, construct step-by-step goals, and locate organizational resources to support these goals. PeerCoPilot ensures information reliability through a retrieval-augmented generation pipeline backed by a large database of over 1,300 vetted resources. We conducted human evaluations with 15 peer providers and 6 service users and found that over 90% of users supported using PeerCoPilot. Moreover, we demonstrated that PeerCoPilot provides more reliable and specific information than a baseline LLM. PeerCoPilot is now used by a group of 5-10 peer providers at CSPNJ, a large behavioral health organization serving over 10,000 service users, and we are actively expanding PeerCoPilot's use.

</details>


### [224] [German General Personas: A Survey-Derived Persona Prompt Collection for Population-Aligned LLM Studies](https://arxiv.org/abs/2511.21722)
*Jens Rupprecht,Leon Fröhling,Claudia Wagner,Markus Strohmaier*

Main category: cs.CL

TL;DR: 本文提出了German General Personas（GGP），一个基于德国ALLBUS调查而建的代表性人物设定集合，用于提升大语言模型（LLM）模拟德国人口观点的准确性。验证结果显示，GGP指导下的LLM在应对数据稀缺且需人口对齐的任务中优于当前最先进的分类器。


<details>
  <summary>Details</summary>
Motivation: 当前用于驱动LLM模拟人类观点的人物设定（persona prompt）资源稀缺，且缺乏代表性和实证基础，限制了相关社会科学研究的可信度。为解决这一问题，作者希望提供一个全面、可代表德国社会的人物设定库。

Method: 作者基于德国社会普查（ALLBUS）的数据，设计并推出适用于各类LLM和任务的人物设定集合（GGP），并用该集合指导LLM模拟问卷调查，评估其在人群对齐和不同主题下的表现，并与现有分类器进行对比。还深入分析了人物设定的代表性和属性选择对模型输出与真实人口回答一致性的影响。

Result: GGP提升了LLM模拟德国人口调查结果的准确性，特别在数据稀缺时效果更为突出，并且表现优于当下最先进的分类算法。更具代表性及适当属性选择的人物设定能进一步提升输出与真实人口分布的一致性。

Conclusion: GGP为推动基于LLM的人群观点模拟及社科研究提供了一个有效且系统化的工具，为实现更真实、人口对齐的人物设定提供了重要资源，具有广泛的学术与应用前景。

Abstract: The use of Large Language Models (LLMs) for simulating human perspectives via persona prompting is gaining traction in computational social science. However, well-curated, empirically grounded persona collections remain scarce, limiting the accuracy and representativeness of such simulations. Here we introduce the German General Personas (GGP) collection, a comprehensive and representative persona prompt collection built from the German General Social Survey (ALLBUS). The GGP and its persona prompts are designed to be easily plugged into prompts for all types of LLMs and tasks, steering models to generate responses aligned with the underlying German population. We evaluate GGP by prompting various LLMs to simulate survey response distributions across diverse topics, demonstrating that GGP-guided LLMs outperform state-of-the-art classifiers, particularly under data scarcity. Furthermore, we analyze how the representativity and attribute selection within persona prompts affect alignment with population responses. Our findings suggest that GGP provides a potentially valuable resource for research on LLM-based social simulations that enables more systematic explorations of population-aligned persona prompting in NLP and social science research.

</details>


### [225] [AD-CDO: A Lightweight Ontology for Representing Eligibility Criteria in Alzheimer's Disease Clinical Trials](https://arxiv.org/abs/2511.21724)
*Zenan Sun,Rashmie Abeysinghe,Xiaojin Li,Xinyue Hu,Licong Cui,Guo-Qiang Zhang,Jiang Bian,Cui Tao*

Main category: cs.CL

TL;DR: 本研究提出了阿尔茨海默病临床试验通用数据元素本体（AD-CDO），用于标准化和表达AD临床试验中的关键入选标准。该本体覆盖率高、结构紧凑，适用于模拟试验等多种应用场景。


<details>
  <summary>Details</summary>
Motivation: 当前阿尔茨海默病（AD）临床试验的入组标准存在异构性和标准化不足的问题，影响了数据共享、对比和自动化处理。因此需要一个兼具简洁性和语义丰富性的本体来统一和标准化这些核心概念。

Method: 该研究从ClinicalTrials.gov上1500余项AD临床试验中提取高频概念，并分为七大语义类别，通过UMLS、OMOP等标准医学词汇体系进行术语注释。采用Jenks自然断点法确定最佳代表性概念集，既保证了覆盖率，又便于管理。

Result: 优化后的AD-CDO能够覆盖超过63%的试验核心概念，保持了良好的可解释性和紧凑性。通过试验模拟系统和实体归一化任务两个用例，展示了其在形式化建模与与EHR数据集成等实际场景下的有效性。

Conclusion: AD-CDO通过对核心入选实体的标准化和与标准词汇的对齐，为AD临床试验研究提供了灵活、实用的本体基础，促进了各类下游应用和数据整合。

Abstract: Objective
  This study introduces the Alzheimer's Disease Common Data Element Ontology for Clinical Trials (AD-CDO), a lightweight, semantically enriched ontology designed to represent and standardize key eligibility criteria concepts in Alzheimer's disease (AD) clinical trials.
  Materials and Methods
  We extracted high-frequency concepts from more than 1,500 AD clinical trials on ClinicalTrials.gov and organized them into seven semantic categories: Disease, Medication, Diagnostic Test, Procedure, Social Determinants of Health, Rating Criteria, and Fertility. Each concept was annotated with standard biomedical vocabularies, including the UMLS, OMOP Standardized Vocabularies, DrugBank, NDC, and NLM VSAC value sets. To balance coverage and manageability, we applied the Jenks Natural Breaks method to identify an optimal set of representative concepts.
  Results
  The optimized AD-CDO achieved over 63% coverage of extracted trial concepts while maintaining interpretability and compactness. The ontology effectively captured the most frequent and clinically meaningful entities used in AD eligibility criteria. We demonstrated AD-CDO's practical utility through two use cases: (a) an ontology-driven trial simulation system for formal modeling and virtual execution of clinical trials, and (b) an entity normalization task mapping raw clinical text to ontology-aligned terms, enabling consistency and integration with EHR data.
  Discussion
  AD-CDO bridges the gap between broad biomedical ontologies and task-specific trial modeling needs. It supports multiple downstream applications, including phenotyping algorithm development, cohort identification, and structured data integration.
  Conclusion
  By harmonizing essential eligibility entities and aligning them with standardized vocabularies, AD-CDO provides a versatile foundation for ontology-driven AD clinical trial research.

</details>


### [226] [PromptTailor: Multi-turn Intent-Aligned Prompt Synthesis for Lightweight LLMs](https://arxiv.org/abs/2511.21725)
*Yizhou Xu,Janet Davis*

Main category: cs.CL

TL;DR: 本文提出了一种名为PromptTailor的可控提示生成系统，能自动将用户的简单指令扩展为丰富、符合意图的高质量提示，从而提升轻量级语言模型的响应效果。


<details>
  <summary>Details</summary>
Motivation: 轻量级语言模型适用于设备端和隐私敏感的场景，但它们对提示（prompt）质量非常敏感。普通用户很难持续编写高质量提示，因此需要自动化的提示优化工具。紧迫问题在于如何保证优化后的提示能真正反映用户原始意图。

Method: 作者开发了PromptTailor系统，通过意图对齐的提示合成，扩展用户的简单指令为丰富、领域相关的提示。其核心为结合LoRA适配器微调的8B量化Llama3模型，利用从多个强大LLM提炼出的12300组提示优化对话数据进行训练。适配器可灵活加载，便于端侧部署。

Result: 在多种目标语言模型和主流提示优化基线的人工与LLM自动评价中，PromptTailor在用户偏好率方面胜过链式思考（CoT）提示，并能匹配或超过最先进方法，同时显著减少模型调用次数（例如3次对比9次）。

Conclusion: 通过强大LLM教师指导，紧凑型学生模型能够有效学习提示生成策略，提升语言模型输出质量，并较好地保持与用户原意的对齐，适合在边缘端设备推广。

Abstract: Lightweight language models remain attractive for on-device and privacy-sensitive applications, but their responses are highly sensitive to prompt quality. For open-ended generation, non-expert users often lack the knowledge or time to consistently craft high-quality prompts, leading them to rely on prompt optimization tools. However, a key challenge is ensuring the optimized prompts genuinely align with users' original intents and preferences. We introduce PromptTailor, a system for controllable prompt generation for open-ended text that improves model output quality by intent-aligned prompt synthesis. PromptTailor expands minimal user instructions into rich, domain-aware prompts while preserving the user's stated preferences. The system is a quantized Llama3-8B model fine-tuned with a lightweight LoRA adapter on 12,300 prompt-refinement dialogues spanning 41 everyday domains, distilled from three stronger LLMs. The adapter attaches to any Llama3-8B base, enabling edge deployment. In human and LLM-judge evaluations across multiple target models and optimization baselines, PromptTailor yields higher preference rates than chain-of-thought prompting and matches or surpasses state-of-the-art prompt optimization methods while requiring fewer model calls (e.g., 3 vs. 9). These results show that a compact student, guided by powerful teachers, can learn effective prompt-generation strategies that enhance response quality while maintaining alignment with user intent.

</details>


### [227] [Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks](https://arxiv.org/abs/2511.21726)
*Yicong Zheng,Kevin L. McKee,Thomas Miconi,Zacharie Bugaud,Mick van Gelderen,Jed McCaleb*

Main category: cs.CL

TL;DR: 本文提出了一种无需压缩记忆、通过搜索原始数据实现高效长时记忆利用的新方法SUMER，并在长对话数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 目前提升大模型长时记忆能力主要依赖于人为设计的压缩与提示方法，这种方法存在偏置，并难以适配不同数据分布，因此需要更通用的解决方案。

Method: 作者提出SUMER，一种基于经验回放（Experience Replay）的端到端强化学习智能体。SUMER通过可验证奖励机制（RLVR）学习如何使用搜索工具在未压缩的长文本中检索，进而完成复杂问题的解答，无需人工设计的记忆压缩。

Result: 在LoCoMo长对话理解数据集上，SUMER（基于Qwen2.5-7B-Instruct）学会了自主搜索，超过了所有现有的压缩型记忆方法，甚至优于直接进全量上下文的baseline，实现了43%的SOTA性能提升。

Conclusion: 不经压缩、基于目标导向搜索的原始数据处理可有效提升大模型长时记忆任务表现，未来应发展更加自动、动态和可扩展的记忆方法及benchmarks。

Abstract: How to enable human-like long-term memory in large language models (LLMs) has been a central question for unlocking more general capabilities such as few-shot generalization. Existing memory frameworks and benchmarks focus on finding the optimal memory compression algorithm for higher performance in tasks that require recollection and sometimes further reasoning. However, such efforts have ended up building more human bias into the compression algorithm, through the search for the best prompts and memory architectures that suit specific benchmarks, rather than finding a general solution that would work on other data distributions. On the other hand, goal-directed search on uncompressed information could potentially exhibit superior performance because compression is lossy, and a predefined compression algorithm will not fit all raw data distributions. Here we present SUMER (Search in Uncompressed Memory via Experience Replay), an end-to-end reinforcement learning agent with verifiable reward (RLVR) that learns to use search tools to gather information and answer a target question. On the LoCoMo dataset for long-context conversation understanding, SUMER with Qwen2.5-7B-Instruct learned to use search tools and outperformed all other biased memory compression approaches and also the full-context baseline, reaching SOTA performance (43% gain over the prior best). We demonstrate that a simple search method applied to raw data outperforms goal-agnostic and biased compression algorithms in current long-context memory tasks, arguing for new paradigms and benchmarks that are more dynamic and autonomously scalable. Code for SUMER and all implemented baselines is publicly available at https://github.com/zycyc/SUMER.

</details>


### [228] [Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue](https://arxiv.org/abs/2511.21728)
*Lin Yu,Xiaofei Han,Yifei Kang,Chiung-Yi Tseng,Danyang Zhang,Ziqian Bi,Zhimo Han*

Main category: cs.CL

TL;DR: AffectMind是一种多模态情感对话系统，通过主动推理和动态知识支撑，实现了更具情感一致性和说服力的营销对话，显著优于现有大模型基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型驱动的对话系统多为被动响应，尤其在情感丰富、以目标为导向的场景（如营销对话）表现不佳，因此需要一种能够实现情感对齐和主动推理的多模态对话系统。

Method: 提出了一套新架构AffectMind，包括三大模块：1）主动知识支撑网络PKGN，动态整合文本、视觉和语音信息中的事实与情绪；2）情感-意图对齐模型EIAM，结合用户情感与购买意图调整说服策略；3）强化型对话循环RDL，通过用户反馈信号提升情感一致性和用户参与度。

Result: 在两个新构建的营销对话数据集MM-ConvMarket和AffectPromo上，AffectMind在情感一致性（提升26%）、说服成功率（提升19%）和长期用户参与度（提升23%）等指标上明显优于强力大语言模型基线。

Conclusion: AffectMind展示了多模态情感推理和主动知识支撑的有效性，情感驱动的主动性成为商业多模态对话智能体的关键能力，有望在营销等实际应用场景获得广泛应用。

Abstract: Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\%), persuasive success rate (+19\%), and long-term user engagement (+23\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.

</details>


### [229] [Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems](https://arxiv.org/abs/2511.21729)
*Jithin Krishnan*

Main category: cs.CL

TL;DR: 仅加强单一组件对RAG系统提升有限，多组件协同才显著降低拒答率且无副作用，统一标准和自适应校准也至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强生成（RAG）系统持续取得进步，但实际部署仍面临拒答率高、虚假内容（幻觉）等挑战。文章动机在于系统性分析多个增强方法的协同效应，并理解其如何共同作用于RAG系统的可靠性和回答准确性。

Method: 作者设计了包含15个可回答、10个边界和25个对抗性问题的50个查询集，依次对比单独和组合使用混合检索、集成验证、自适应阈值等增强方法的具体表现，通过消融实验揭示各方法单独或协同下的影响。此外，分析了不同验证策略因标签分歧造成的性能度量难题。

Result: 单独使用混合检索、集成验证或自适应阈值对系统性能提升微小，而协同使用三者则可将拒答率从40%显著降低到2%，并未带来更多幻觉。还揭示了不同验证策略对标签分配不一致，导致幻觉率统计出现偏差。

Conclusion: RAG系统的可靠性更依赖于多方法的协同集成而非单一增强，准确的性能评估必须依赖统一标准化的评价指标和标签，自适应校准机制则是防止“过度自信型过答”不可或缺的一环。

Abstract: Building reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact. Using ablation studies on 50 queries (15 answerable, 10 edge cases, and 25 adversarial), we show that enhancements such as hybrid retrieval, ensemble verification, and adaptive thresholding provide almost no benefit when used in isolation, yet together achieve a 95% reduction in abstention (from 40% to 2%) without increasing hallucinations. We also identify a measurement challenge: different verification strategies can behave safely but assign inconsistent labels (for example, "abstained" versus "unsupported"), creating apparent hallucination rates that are actually artifacts of labeling. Our results show that synergistic integration matters more than the strength of any single component, that standardized metrics and labels are essential for correctly interpreting performance, and that adaptive calibration is needed to prevent overconfident over-answering even when retrieval quality is high.

</details>


### [230] [A Benchmark for Procedural Memory Retrieval in Language Agents](https://arxiv.org/abs/2511.21730)
*Ishant Kohar,Aswanth Krishnan*

Main category: cs.CL

TL;DR: 本文提出了首个将程序性记忆检索从任务执行中分离的基准测试，并揭示了当前嵌入方法在新场景下泛化能力的明显断崖。


<details>
  <summary>Details</summary>
Motivation: 现有AI在熟悉环境下表现良好，但在遇到新任务和未知词汇时表现不佳，表明程序性记忆系统存在核心局限。作者希望建立能单独评估程序性记忆检索能力的基准，从而推动AI在泛化和抽象能力上的发展。

Method: 作者在ALFWorld环境里构建了专家轨迹和大模型生成轨迹两组语料库，采用六种检索方法，利用系统性分层查询评估它们在识别不同对象实例间的功能等价操作上的表现，并通过对比和消融实验分析不同方法的优劣。

Result: 结果显示，基于嵌入的方法在熟悉语境下表现优异，但在新情境泛化明显退步，而基于大模型生成的过程抽象在跨情境迁移下表现更可靠。消融实验表明嵌入方法忽略了时间结构，且语料库规模比表示增强带来更大性能提升。

Conclusion: 该基准首次区分了真正的程序性理解与表层记忆，为开发具备可靠泛化能力的检索系统提供了诊断工具和研究方向。

Abstract: Current AI agents excel in familiar settings, but fail sharply when faced with novel tasks with unseen vocabularies -- a core limitation of procedural memory systems. We present the first benchmark that isolates procedural memory retrieval from task execution, evaluating whether agents can recognize functionally equivalent procedures that span different object instantiations. Using ALFWorld, we construct dual corpora of expert and LLM-generated trajectories and evaluate six retrieval methods using systematically stratified queries. Our results expose a clear generalization cliff: embedding-based methods perform strongly on familiar contexts, yet degrade considerably on novel ones, while LLM-generated procedural abstractions demonstrate reliable cross-context transfer. Controlled ablations show that although embeddings capture some lexical-level abstraction, they fundamentally treat procedures as unordered bags of words, discarding temporal structure necessary for cross-context transfer. Corpus scale delivers far larger gains than representation enrichment, revealing an architectural ceiling in current encoders. Our benchmark offers the first diagnostic framework separating genuine procedural understanding from surface-level memorization and gives tools for developing retrieval systems capable of dependable generalization. Resources available at our GitHub repository (https://github.com/qpiai/Proced_mem_bench).

</details>


### [231] [Identifying Quantum Structure in AI Language: Evidence for Evolutionary Convergence of Human and Artificial Cognition](https://arxiv.org/abs/2511.21731)
*Diederik Aerts,Jonito Aerts Arguëlles,Lester Beltran,Suzette Geriente,Roberto Leporini,Massimiliano Sassoli de Bianchi,Sandro Sozzo*

Main category: cs.CL

TL;DR: 该论文对大型语言模型(LLM)中的概念组合进行了认知测试，发现其在认知结构上出现了与量子力学相关的统计特征。


<details>
  <summary>Details</summary>
Motivation: 近年来，研究者发现人类认知和语言处理中的某些规律可被量子理论模型描述。作者想探究：类似的量子结构是否也能在人工智能的LLM中出现，从而揭示认知结构的本质。

Method: 作者以ChatGPT和Gemini作为“被试”，设计了两组测试：一，检测概念间的“量子纠缠”特征（通过贝尔不等式）；二，分析LLM生成文本的词频分布（寻找Bose-Einstein或Maxwell-Boltzmann统计规律）。

Result: 实验显示，LLM在概念测试中违反了贝尔不等式（存在“量子纠缠”），词分布符合Bose-Einstein而非传统的Maxwell-Boltzmann统计。这与之前人类和大型语料库上的结果一致。

Conclusion: 无论认知主体是人类还是人工智能，概念-语言领域都系统性地出现了量子结构。作者提出LLM的分布式语义空间本质上承载了类似人类认知的量子型意义组织，为理解语言和认知演化提供了统一框架。

Abstract: We present the results of cognitive tests on conceptual combinations, performed using specific Large Language Models (LLMs) as test subjects. In the first test, performed with ChatGPT and Gemini, we show that Bell's inequalities are significantly violated, which indicates the presence of 'quantum entanglement' in the tested concepts. In the second test, also performed using ChatGPT and Gemini, we instead identify the presence of 'Bose-Einstein statistics', rather than the intuitively expected 'Maxwell-Boltzmann statistics', in the distribution of the words contained in large-size texts. Interestingly, these findings mirror the results previously obtained in both cognitive tests with human participants and information retrieval tests on large corpora. Taken together, they point to the 'systematic emergence of quantum structures in conceptual-linguistic domains', regardless of whether the cognitive agent is human or artificial. Although LLMs are classified as neural networks for historical reasons, we believe that a more essential form of knowledge organization takes place in the distributive semantic structure of vector spaces built on top of the neural network. It is this meaning-bearing structure that lends itself to a phenomenon of evolutionary convergence between human cognition and language, slowly established through biological evolution, and LLM cognition and language, emerging much more rapidly as a result of self-learning and training. We analyze various aspects and examples that contain evidence supporting the above hypothesis. We also advance a unifying framework that explains the pervasive quantum organization of meaning that we identify.

</details>


### [232] [HUMORCHAIN: Theory-Guided Multi-Stage Reasoning for Interpretable Multimodal Humor Generation](https://arxiv.org/abs/2511.21732)
*Jiajun Zhang,Shijia Luo,Ruikang Zhang,Qi Su*

Main category: cs.CL

TL;DR: 本文提出了HUMORCHAIN框架，将幽默理论的认知结构嵌入到多模态幽默生成中，显著提升了AI图像描述的幽默性和认知深度。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成幽默内容时往往缺乏理论模型支撑，生成的描述虽流畅却缺乏真正的幽默和认知内涵，而多模态幽默（比如流行网络图片梗）需求日益增长，要求AI具备结合视觉和语言幽默的能力。

Method: 作者提出HUMORCHAIN，一种以幽默理论为指导的多阶段推理框架，包含视觉语义解析、基于幽默和心理学的推理、以及专用幽默判别器。通过明确的认知推理链条，将视觉理解逐步转化为幽默内容生成。

Result: 在多个公开数据集上，HUMORCHAIN在人工幽默偏好、Elo/BT评分和语义多样性等指标上，均显著优于已有最先进方法。

Conclusion: 理论驱动的结构化推理能够让大模型生成更贴合人类认知的幽默内容，这一框架首次实现在多模态幽默生成中显式引入幽默理论的认知结构。

Abstract: Humor, as both a creative human activity and a social binding mechanism, has long posed a major challenge for AI generation. Although producing humor requires complex cognitive reasoning and social understanding, theories of humor suggest that it follows learnable patterns and structures, making it theoretically possible for generative models to acquire them implicitly. In recent years, multimodal humor has become a prevalent form of online communication, especially among Gen Z, highlighting the need for AI systems capable of integrating visual understanding with humorous language generation. However, existing data-driven approaches lack explicit modeling or theoretical grounding of humor, often producing literal descriptions that fail to capture its underlying cognitive mechanisms, resulting in the generated image descriptions that are fluent but lack genuine humor or cognitive depth. To address this limitation, we propose HUMORCHAIN (HUmor-guided Multi-step Orchestrated Reasoning Chain for Image Captioning), a theory-guided multi-stage reasoning framework. It integrates visual semantic parsing, humor- and psychology-based reasoning, and a fine-tuned discriminator for humor evaluation, forming an interpretable and controllable cognitive reasoning chain. To the best of our knowledge, this is the first work to explicitly embed cognitive structures from humor theories into multimodal humor generation, enabling a structured reasoning process from visual understanding to humor creation. Experiments on Meme-Image-No-Text, Oogiri-GO, and OxfordTVG-HIC datasets show that HUMORCHAIN outperforms state-of-the-art baselines in human humor preference, Elo/BT scores, and semantic diversity, demonstrating that theory-driven structured reasoning enables large language models to generate humor aligned with human perception.

</details>


### [233] [RoSA: Enhancing Parameter-Efficient Fine-Tuning via RoPE-aware Selective Adaptation in Large Language Models](https://arxiv.org/abs/2511.21733)
*Dayan Pan,Jingyuan Wang,Yilong Zhou,Jiawei Cheng,Pengyue Jia,Xiangyu Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种新的参数高效微调(PEFT)框架RoSA，通过对大语言模型进行更有针对性的微调实现更出色的任务适应能力，且效率高于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 目前主流的PEFT方法对模型各部分的功能和各层的重要性差异关注不够，造成微调效率受限。作者发现RoPE对注意力状态的低频成分有重要影响，因此希望利用这一现象实现更高效微调。

Method: 提出RoSA（RoPE-aware Selective Adaptation）框架，包括两个核心创新：1）RoPE-aware Attention Enhancement（RoAE）模块，着重增强RoPE影响下的注意力状态低频成分；2）Dynamic Layer Selection（DLS）机制，基于LayerNorm梯度范数自适应选择和更新最关键的层。实现参数分配更加精准高效。

Result: 在15个常识和算术基准数据集上的大量实验显示，RoSA在可比可训练参数条件下显著优于现有主流PEFT方法。

Conclusion: 通过结合维度级增强与层级自适应，RoSA能以更高的针对性和效率实现大语言模型的高效微调，提升在任务特定场景下的模型表现。

Abstract: Fine-tuning large language models is essential for task-specific adaptation, yet it remains computationally prohibitive. Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a solution, but current approaches typically ignore the distinct roles of model components and the heterogeneous importance across layers, thereby limiting adaptation efficiency. Motivated by the observation that Rotary Position Embeddings (RoPE) induce critical activations in the low-frequency dimensions of attention states, we propose RoPE-aware Selective Adaptation (RoSA), a novel PEFT framework that allocates trainable parameters in a more targeted and effective manner. RoSA comprises a RoPE-aware Attention Enhancement (RoAE) module, which selectively enhances the low-frequency components of RoPE-influenced attention states, and a Dynamic Layer Selection (DLS) strategy that adaptively identifies and updates the most critical layers based on LayerNorm gradient norms. By combining dimension-wise enhancement with layer-wise adaptation, RoSA achieves more targeted and efficient fine-tuning. Extensive experiments on fifteen commonsense and arithmetic benchmarks demonstrate that RoSA outperforms existing mainstream PEFT methods under comparable trainable parameters. The code is available to ease reproducibility at https://github.com/Applied-Machine-Learning-Lab/RoSA.

</details>


### [234] [Asking LLMs to Verify First is Almost Free Lunch](https://arxiv.org/abs/2511.21734)
*Shiguang Wu,Quanming Yao*

Main category: cs.CL

TL;DR: 本文提出了一种名为Verification-First（VF）的策略，通过让大模型先验证答案再推理，显著提升了逻辑和推理能力，无需大规模训练或推理时大量采样。进一步提出的Iter-VF方法，通过多轮验证和生成，效果更优。实验数据显示VF及Iter-VF明显优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 已有的大语言模型（LLM）在复杂推理任务中容易出现逻辑错误，目前提升其推理能力常依赖高昂的训练或推理时的大量采样。该文动机是找到无需高成本即可增强模型思维及推理准确性的有效方法。

Method: 提出Verification-First（VF）策略：在模型解决任务前，先给模型一个随机或简单的候选答案，让其进行验证，促使其运用“逆向推理”以减少逻辑错误。此外，进一步推广为Iter-VF，即多轮交替进行验证和生成，使推理更深入。

Result: 在数学推理、编程、智能体等多种基准和不同规模的开源及商用LLM上实验，VF方法（尤其是用随机答案）在推理性能上明显优于传统的链式思维（CoT），且几乎不增加计算量。Iter-VF在多次迭代下进一步提升结果，超过现有的测试时扩展（TTS）策略。

Conclusion: Verification-First及其迭代版本能低成本显著提升大语言模型推理、减少逻辑错误，对LLM应用具有实际意义，并为模型推理流程优化提供了新视角。

Abstract: To enhance the reasoning capabilities of Large Language Models (LLMs) without high costs of training, nor extensive test-time sampling, we introduce Verification-First (VF), a strategy that prompts models to verify a provided candidate answer, even a trivial or random one, before generating a solution. This approach triggers a "reverse reasoning" process that is cognitively easier and complementary to standard forward Chain-of-Thought (CoT), effectively invoking the model's critical thinking to reduce logical errors. We further generalize the VF strategy to Iter-VF, a sequential test-time scaling (TTS) method that iteratively cycles the verification-generation process using the model's previous answer. Extensive experiments across various benchmarks (from mathematical reasoning to coding and agentic tasks) and various LLMs (from open-source 1B to cutting-edge commercial ones) confirm that VF with random answer consistently outperforms standard CoT with minimal computational overhead, and Iter-VF outperforms existing TTS strategies.

</details>


### [235] [Closing the Performance Gap Between AI and Radiologists in Chest X-Ray Reporting](https://arxiv.org/abs/2511.21735)
*Harshita Sharma,Maxwell C. Reynolds,Valentina Salvatelli,Anne-Marie G. Sykes,Kelly K. Horst,Anton Schwaighofer,Maximilian Ilse,Olesya Melnichenko,Sam Bond-Taylor,Fernando Pérez-García,Vamshi K. Mugu,Alex Chan,Ceylan Colak,Shelby A. Swartz,Motassem B. Nashawaty,Austin J. Gonzalez,Heather A. Ouellette,Selnur B. Erdal,Beth A. Schueler,Maria T. Wetscherek,Noel Codella,Mohit Jain,Shruthi Bannur,Kenza Bouzid,Daniel C. Castro,Stephanie Hyland,Panos Korfiatis,Ashish Khandelwal,Javier Alvarez-Valle*

Main category: cs.CL

TL;DR: 本文提出了MAIRA-X，一个多模态AI模型，用于高效生成胸部X光报告，显著提升了报告质量和临床正确性，并减轻放射科医生的工作负担。


<details>
  <summary>Details</summary>
Motivation: 放射科医生面临着工作负荷增加、病例复杂度提升及人手短缺等压力，传统报告撰写尤其在描述导管及线状物时耗时且重复，需要创新方法来提升效率且保证准确性。

Method: 作者开发了MAIRA-X模型，采用来自梅奥诊所的310万组（600万张图像，80.6万名患者）跨区域、纵向胸部X光数据，并在多个独立测试集与公共MIMIC-CXR集上评估。该模型可自动生成包含临床发现及导管线物（L&T）报告的智能报告，并搭建了针对L&T的全新评价框架；同时组织了9位不同经验放射科医生的回顾性盲评用户研究。

Result: MAIRA-X在词汇质量、临床正确性及L&T要素方面，比现有AI模型有显著提升。用户研究显示，AI生成报告的关键性错误率（4.6%）和可接受句子比例（97.4%）与人工报告（3.0%、97.8%）接近，显著优于以往模型。

Conclusion: MAIRA-X能够有效协助放射科医生，特别适用于患者量大的临床环境，有望缓解目前面临的工作压力并提升诊断效率。

Abstract: AI-assisted report generation offers the opportunity to reduce radiologists' workload stemming from expanded screening guidelines, complex cases and workforce shortages, while maintaining diagnostic accuracy. In addition to describing pathological findings in chest X-ray reports, interpreting lines and tubes (L&T) is demanding and repetitive for radiologists, especially with high patient volumes. We introduce MAIRA-X, a clinically evaluated multimodal AI model for longitudinal chest X-ray (CXR) report generation, that encompasses both clinical findings and L&T reporting. Developed using a large-scale, multi-site, longitudinal dataset of 3.1 million studies (comprising 6 million images from 806k patients) from Mayo Clinic, MAIRA-X was evaluated on three holdout datasets and the public MIMIC-CXR dataset, where it significantly improved AI-generated reports over the state of the art on lexical quality, clinical correctness, and L&T-related elements. A novel L&T-specific metrics framework was developed to assess accuracy in reporting attributes such as type, longitudinal change and placement. A first-of-its-kind retrospective user evaluation study was conducted with nine radiologists of varying experience, who blindly reviewed 600 studies from distinct subjects. The user study found comparable rates of critical errors (3.0% for original vs. 4.6% for AI-generated reports) and a similar rate of acceptable sentences (97.8% for original vs. 97.4% for AI-generated reports), marking a significant improvement over prior user studies with larger gaps and higher error rates. Our results suggest that MAIRA-X can effectively assist radiologists, particularly in high-volume clinical settings.

</details>


### [236] [R2Q: Towards Robust 2-Bit Large Language Models via Residual Refinement Quantization](https://arxiv.org/abs/2511.21736)
*Jiayi Chen,Jieqi Shi,Jing Huo,Chen Wu*

Main category: cs.CL

TL;DR: 本论文提出了一种新型2比特量化方法R2Q，成功提升了极低比特量化下大语言模型的精度和训练表现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的快速发展，其对计算和内存资源的需求急剧增加。为降低资源消耗，低比特量化成为主流手段。然而，现有2比特量化方法会导致显著精度下降，亟需新的解决方案以实现高效且高保真的2比特量化。

Method: 作者提出Residual Refinement Quantization (R2Q)框架，将2比特量化分解为两个连续的1比特子量化过程，并通过残差学习形成自适应量化格点。此外，该方法可模块化集成至现有的量化感知训练（QAT）框架中。

Result: R2Q在Llama、OPT和Qwen等主流模型上，横跨各类问答、常识推理和语言建模基准，实现了对现有2比特量化方法的全面超越，在精细和粗粒度环境下表现优异。方法还提高了训练稳定性并加快了收敛速度。

Conclusion: R2Q为2比特极低比特量化带来了性能突破，为大语言模型在受限资源设备上的应用提供了有效解决方案。同时，该方法兼容当前量化训练流程，具备良好的实用性与推广前景。

Abstract: The rapid progress of Large Language Models (LLMs) has brought substantial computational and memory demands, spurring the adoption of low-bit quantization. While 8-bit and 4-bit formats have become prevalent, extending quantization to 2 bits remains challenging due to severe accuracy degradation. To address this, we propose Residual Refinement Quantization (R2Q)-a novel 2-bit quantization framework that decomposes the process into two sequential 1-bit sub-quantizations, forming an adaptive quantization lattice. Extensive evaluations on Llama, OPT, and Qwen across diverse benchmarks-covering question answering, commonsense reasoning, and language modeling-demonstrate that R2Q consistently outperforms existing 2-bit quantization methods in both fine-grained and coarse-grained settings. By refining quantization through a residual learning mechanism, R2Q enhances performance, improves training stability, and accelerates convergence under extreme compression. Furthermore, its modular design enables seamless integration with existing quantization-aware training (QAT) frameworks.

</details>


### [237] [Polarity-Aware Probing for Quantifying Latent Alignment in Language Models](https://arxiv.org/abs/2511.21737)
*Sabrina Sadiekh,Elena Ericheva,Chirag Agarwal*

Main category: cs.CL

TL;DR: 本文研究了如何用无监督探针方法（如CCS和提出的PA-CCS）揭示语言模型的内部信念，以评估其对有害内容的对齐能力，并引入了两个新指标评估模型在极性反转下的表现。结果发现，PA-CCS方法可区分模型及其不同层在有害知识编码方面的差异，并发现结构鲁棒性检查对解释性评测的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着无监督探针（如CCS）能够无需依赖输出token揭示模型内在信念，人们希望判别模型是否真的对齐到安全或无害的立场。本文试图探究这些方法是否真能有效评估模型对于有害和安全内容的区别。

Method: 本文提出了极性感知的CCS（PA-CCS）方法和两个新指标（Polar-Consistency和Contradiction Index），通过在包含有害-安全句配对的新数据集上，对16个语言模型进行测试，观察模型内部表示在极性反转（如否定词替换）情况下的变化。

Result: 结果显示，PA-CCS能分辨不同模型结构和不同层级在有害知识的编码差异。对于内部标定良好的模型，否定词替换成无意义token会显著降低PA-CCS分数；而缺乏内部对齐的模型则无此现象。

Conclusion: 无监督探针具备用于模型对齐评估的潜力，但应结合结构鲁棒性检查作为可解释性基准的重要组成部分。

Abstract: Advances in unsupervised probes such as Contrast-Consistent Search (CCS), which reveal latent beliefs without relying on token outputs, raise the question of whether these methods can reliably assess model alignment. We investigate this by examining the sensitivity of CCS to harmful vs. safe statements and by introducing Polarity-Aware CCS (PA-CCS), a method for evaluating whether a model's internal representations remain consistent under polarity inversion. We propose two alignment-oriented metrics, Polar-Consistency and the Contradiction Index, to quantify the semantic robustness of a model's latent knowledge. To validate PA-CCS, we curate two main datasets and one control dataset containing matched harmful-safe sentence pairs constructed using different methodologies (concurrent and antagonistic statements). We apply PA-CCS to 16 language models. Our results show that PA-CCS identifies both architectural and layer-specific differences in the encoding of latent harmful knowledge. Notably, replacing the negation token with a meaningless marker degrades PA-CCS scores for models with well-aligned internal representations, while models lacking robust internal calibration do not exhibit this degradation. Our findings highlight the potential of unsupervised probing for alignment evaluation and emphasize the need to incorporate structural robustness checks into interpretability benchmarks. Code and datasets are available at: https://github.com/SadSabrina/polarity-probing. WARNING: This paper contains potentially sensitive, harmful, and offensive content.

</details>


### [238] [Decoding inner speech with an end-to-end brain-to-text neural interface](https://arxiv.org/abs/2511.21740)
*Yizi Zhang,Linyang He,Chaofei Fan,Tingkai Liu,Han Yu,Trung Le,Jingyuan Li,Scott Linderman,Lea Duncker,Francis R Willett,Nima Mesgarani,Liam Paninski*

Main category: cs.CL

TL;DR: 本文提出了一种端到端的大脑到文本（BIT）框架，利用单一可微分神经网络将神经活动直接翻译为连贯句子，在多个基准测试中实现了先进性能，并显著降低了词错误率。


<details>
  <summary>Details</summary>
Motivation: 当前语音脑机接口大多采用级联模型，分阶段完成音素解码和句子组装，这限制了整体端到端优化的潜力。作者希望通过端到端方法，提升从神经信号到文本的翻译效率和准确性。

Method: 提出了一个端到端BIT框架，核心为跨任务、跨物种预训练神经编码器，嵌入用于尝试和想象语音。BIT可在级联模式下与n-gram语言模型配合，也能与音频大语言模型集成，实现端到端优化。采用对比学习进行跨模态对齐，同时支持大规模神经数据集集成。

Result: 在Brain-to-Text '24和'25基准下，该方法取得了新的最优性能。与之前的端到端方法（WER 24.69%）相比，BIT将词错误率降至10.22%。此外，音频小规模LLM还进一步提升了解码效果。

Conclusion: BIT不仅在性能上刷新纪录，还实现了尝试语音和想象语音的跨任务泛化，推动了脑机接口端到端优化的发展，为无缝集成大规模神经数据集的未来打下基础。

Abstract: Speech brain-computer interfaces (BCIs) aim to restore communication for people with paralysis by translating neural activity into text. Most systems use cascaded frameworks that decode phonemes before assembling sentences with an n-gram language model (LM), preventing joint optimization of all stages simultaneously. Here, we introduce an end-to-end Brain-to-Text (BIT) framework that translates neural activity into coherent sentences using a single differentiable neural network. Central to our approach is a cross-task, cross-species pretrained neural encoder, whose representations transfer to both attempted and imagined speech. In a cascaded setting with an n-gram LM, the pretrained encoder establishes a new state-of-the-art (SOTA) on the Brain-to-Text '24 and '25 benchmarks. Integrated end-to-end with audio large language models (LLMs) and trained with contrastive learning for cross-modal alignment, BIT reduces the word error rate (WER) of the prior end-to-end method from 24.69% to 10.22%. Notably, we find that small-scale audio LLMs markedly improve end-to-end decoding. Beyond record-setting performance, BIT aligns attempted and imagined speech embeddings to enable cross-task generalization. Altogether, our approach advances the integration of large, diverse neural datasets, paving the way for an end-to-end decoding framework that supports seamless, differentiable optimization.

</details>


### [239] [A Multiscale Geometric Method for Capturing Relational Topic Alignment](https://arxiv.org/abs/2511.21741)
*Conrad D. Hougen,Karl T. Pazdernik,Alfred O. Hero*

Main category: cs.CL

TL;DR: 提出了一种结合文本与作者网络数据的几何方法，能够更好地解析科研社群中的罕见话题并追踪其随时间的演化。


<details>
  <summary>Details</summary>
Motivation: 以往的基于Transformer嵌入的方法难以识别罕见话题，导致对研究趋势的捕捉不完整，尤其在高度重视新颖性的科学语料中亟需提升对低频/冷门话题的识别能力。

Method: 将多模态文本与共作者网络数据结合，利用Hellinger距离和Ward层次聚类，建立分层的话题树形结构（topic dendrogram），支持在语义和时间两个维度上的多尺度学习和分析。

Result: 该方法能够准确识别罕见话题结构，并平滑地可视化话题随时间的变化。实验证明，与传统方法对比，采用可解释的词袋模型并结合几何对齐取得了更优结果。

Conclusion: 与现有的稠密嵌入模型相比，文中提出的方法在抓取小众话题和追踪话题演化上表现更佳，验证了词袋加几何方法的解释性和实用性，推进了科学社群研究兴趣分析的精细化。

Abstract: Interpretable topic modeling is essential for tracking how research interests evolve within co-author communities. In scientific corpora, where novelty is prized, identifying underrepresented niche topics is particularly important. However, contemporary models built from dense transformer embeddings tend to miss rare topics and therefore also fail to capture smooth temporal alignment. We propose a geometric method that integrates multimodal text and co-author network data, using Hellinger distances and Ward's linkage to construct a hierarchical topic dendrogram. This approach captures both local and global structure, supporting multiscale learning across semantic and temporal dimensions. Our method effectively identifies rare-topic structure and visualizes smooth topic drift over time. Experiments highlight the strength of interpretable bag-of-words models when paired with principled geometric alignment.

</details>


### [240] [EduMod-LLM: A Modular Approach for Designing Flexible and Transparent Educational Assistants](https://arxiv.org/abs/2511.21742)
*Meenakshi Mittal,Rishi Khare,Mihran Miroyan,Chancharik Mitra,Narges Norouzi*

Main category: cs.CL

TL;DR: 本文提出了一种模块化函数调用的大型语言模型（LLM）问答系统，通过分离和评估各个组件，对函数调用策略、检索方法和生成模型性能进行了全面分析，提升了系统的可解释性和教育适应性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的问答系统越来越多地应用于教育领域，评估其各组成部分的性能变得尤为重要，进而提升系统透明度和教学效果。

Method: 作者提出了名为{model}的模块化函数调用LLM流水线，分别对函数调用、信息检索与答案生成各模块进行隔离与细致分析。评测了不同LLM的函数调用能力，将新结构感知检索法与主流向量/LLM评分方法对比，并考察各类LLM的答案生成表现。

Result: 该模块化方法揭示了LLM问答系统中各环节的具体失效模式与性能表现，验证了结构感知检索方法的优势。

Conclusion: 模块化函数调用提升了问答系统的透明度与教育一致性，为后续开发更有效的教育QA系统提供了理论与实践基础。

Abstract: With the growing use of Large Language Model (LLM)-based Question-Answering (QA) systems in education, it is critical to evaluate their performance across individual pipeline components. In this work, we introduce {\model}, a modular function-calling LLM pipeline, and present a comprehensive evaluation along three key axes: function calling strategies, retrieval methods, and generative language models. Our framework enables fine-grained analysis by isolating and assessing each component. We benchmark function-calling performance across LLMs, compare our novel structure-aware retrieval method to vector-based and LLM-scoring baselines, and evaluate various LLMs for response synthesis. This modular approach reveals specific failure modes and performance patterns, supporting the development of interpretable and effective educational QA systems. Our findings demonstrate the value of modular function calling in improving system transparency and pedagogical alignment. Website and Supplementary Material: https://chancharikmitra.github.io/EduMod-LLM-website/

</details>


### [241] [Scaling Competence, Shrinking Reasoning: Cognitive Signatures in Language Model Learning](https://arxiv.org/abs/2511.21743)
*Mukul Singh,Ananya Singha,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.CL

TL;DR: 本文分析了语言模型在特定任务微调期间的推理过程，发现推理token的演化与模型能力提升之间存在密切关系，并为模型训练提供了新的评估与诊断指标。


<details>
  <summary>Details</summary>
Motivation: 人类在解决问题过程中会通过中间推理步骤逐步达到掌握，类似的，LLM也会在训练中显现推理行为。作者希望借鉴认知科学理论，深入理解和量化模型推理的发展过程，并探索推理结构对模型最终性能的作用。

Method: 作者将LLM任务微调过程与‘四阶段能力模型’相对应，追踪模型输出中的推理token长度随性能变化的动态，并设计了相应的度量指标。从无推理-低性能到有推理-高性能，直到隐式掌握任务-推理token减少，对比各阶段推理token的演化情况。

Result: 实验发现，推理token长度随着模型性能提升而增长，在‘有意识胜任’阶段达到峰值，随后随着模型掌握任务后减少。最终，即使移除推理token，模型依然保持高性能，说明推理token在学习过程中起到支架作用但不再是必须。

Conclusion: 推理token的动态变化不仅反映了模型由生疏到精通的学习过程，还能作为可靠的训练状态诊断信号。利用这些指标可优化训练流程，如实现自动提前终止等。论文强调了跟踪推理行为对于理解和提升推理模型训练的重要意义。

Abstract: We analyze reasoning in language models during task-specific fine-tuning and draws parallel between reasoning tokens--intermediate steps generated while solving problem and the human working memory. Drawing from cognitive science, we align training dynamics with the Four Stages of Competence: models initially produce incorrect outputs without reasoning, then begin reasoning (but still fail), eventually reason effectively, and finally solve tasks without explicit reasoning. We find that reasoning token length expands as performance improves, peaks at the stage of conscious competence, then declines as the model internalizes the task. Notably, after training, models retain performance even when reasoning is removed--suggesting it scaffolded learning but is no longer needed. This progression offers actionable insights: reasoning token dynamics can serve as a signal for diagnosing training stage, identifying convergence, and guiding early stopping. We propose metrics to track this trajectory and argue that reasoning behavior is valuable for understanding and optimizing reasoning model training.

</details>


### [242] [A Lightweight Approach to Detection of AI-Generated Texts Using Stylometric Features](https://arxiv.org/abs/2511.21744)
*Sergey K. Aityan,William Claster,Karthik Sai Emani,Sohni Rais,Thy Tran*

Main category: cs.CL

TL;DR: 引入了一种名为NEULIF的轻量级AI生成文本检测方法，兼具高效性和高准确性，显著优于同类轻量级方法。


<details>
  <summary>Details</summary>
Motivation: 目前AI文本检测主要依赖大型Transformer模型或模型集合，计算代价高、跨领域泛化能力有限。轻量级检测器虽然速度快但准确率低，因此需要既轻量又高效的方法。

Method: 作者提出NEULIF方法，首先将文本分解为文体计量特征和可读性特征；再利用轻量化的卷积神经网络（CNN）或随机森林（RF）进行分类。模型体积远小于主流Transformer模型，运算资源需求低。

Result: 在Kaggle AI vs. Human数据集上，CNN模型准确率97%、F1约0.95，RF模型准确率95%、F1约0.94。ROC-AUC分别达99.5%和95%。模型体积仅25MB（CNN）、10.6MB（RF），远小于基于Transformer的方法。

Conclusion: NEULIF打破了轻量级检测器表现不佳的局面，在不牺牲准确率的前提下极大降低了算力需求，并具备良好的移植潜力。该方法有望推广到多语言、多领域及流式场景。

Abstract: A growing number of AI-generated texts raise serious concerns. Most existing approaches to AI-generated text detection rely on fine-tuning large transformer models or building ensembles, which are computationally expensive and often provide limited generalization across domains. Existing lightweight alternatives achieved significantly lower accuracy on large datasets. We introduce NEULIF, a lightweight approach that achieves best performance in the lightweight detector class, that does not require extensive computational power and provides high detection accuracy. In our approach, a text is first decomposed into stylometric and readability features which are then used for classification by a compact Convolutional Neural Network (CNN) or Random Forest (RF). Evaluated and tested on the Kaggle AI vs. Human corpus, our models achieve 97% accuracy (~ 0.95 F1) for CNN and 95% accuracy (~ 0.94 F1) for the Random Forest, demonstrating high precision and recall, with ROC-AUC scores of 99.5% and 95%, respectively. The CNN (~ 25 MB) and Random Forest (~ 10.6 MB) models are orders of magnitude smaller than transformer-based ensembles and can be run efficiently on standard CPU devices, without sacrificing accuracy.This study also highlights the potential of such models for broader applications across languages, domains, and streaming contexts, showing that simplicity, when guided by structural insights, can rival complexity in AI-generated content detection.

</details>


### [243] [DELTA: Language Diffusion-based EEG-to-Text Architecture](https://arxiv.org/abs/2511.21746)
*Mingyu Jeon,Hyobin Kim*

Main category: cs.CL

TL;DR: 本文提出了DELTA方法，通过结合残差向量量化（RVQ）与掩码扩散语言模型（LLaDA），提升了EEG转文本的准确性和鲁棒性，并在ZuCo数据集上显著超越了自回归基线模型。


<details>
  <summary>Details</summary>
Motivation: EEG到文本转换面临高维噪声、个体差异以及自回归解码过程中的误差积累等挑战。现有方法在转换准确性和泛化能力方面存在明显不足，因此亟需更强鲁棒性的解决方案。

Method: 本文提出DELTA方法：首先，使用RVQ离散化连续EEG信号，将其编码为多层Token以削弱噪声和个体差异；其次，采用非自回归的LLaDA掩码扩散模型进行文本重建，旨在避免误差积累问题。

Result: 在ZuCo数据集实验中，DELTA在语义对齐得分上较自回归基线最高提升5.37分，BLEU-1达到21.9，ROUGE-1 F达到17.2，均为词层面评测结果。

Conclusion: DELTA不仅在EEG-文本小样本场景下实现了更可靠的文本生成，也为可扩展多模态EEG-语言模型的构建提供了实践基础。

Abstract: Electroencephalogram (EEG)-to-text remains challenging due to high-dimensional noise, subject variability, and error accumulation in autoregressive decoding. We introduce DELTA, which pairs a Residual Vector Quantization (RVQ) EEG tokenizer with a masked language diffusion model (LLaDA). RVQ discretizes continuous EEG into multi-layer tokens to reduce noise and individual differences, while LLaDA reconstructs sentences via non-sequential denoising. On ZuCo, DELTA improves semantic alignment by up to 5.37 points over autoregressive baselines, achieving BLEU-1 21.9 and ROUGE-1 F 17.2 under word-level conditions. These results enable reliable text generation from small EEG-text datasets and point toward scalable multimodal EEG-language models.

</details>


### [244] [Building Domain-Specific Small Language Models via Guided Data Generation](https://arxiv.org/abs/2511.21748)
*Aman Kumar,Ekant Muljibhai Amin,Xian Yeow Lee,Lasitha Vidyaratne,Ahmed K. Farahat,Dipanjan D. Ghosh,Yuta Koreeda,Chetan Gupta*

Main category: cs.CL

TL;DR: 本文提出了一种低成本且可扩展的训练流程，用以打造高效的小型领域专用大语言模型（LLMs），并在工业诊断领域验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs部署为SaaS会有数据隐私风险，开源模型领域适配成本高且算力消耗大，而小型领域模型受限于数据稀缺，难以获得高性能。为解决这一困境，本文寻求在成本、隐私和效果间取得平衡。

Method: 提出结合引导式合成数据生成、小规模原始语料与领域自下而上数据整理的训练流程。流程集成了领域适应预训练（DAPT）、领域有监督微调（DSFT）和直接偏好优化（DPO）。以3B参数的DiagnosticSLM模型为例，面向工业诊断场景训练并建立四个领域基准任务进行评测。

Result: DiagnosticSLM在工业领域多项任务上，相较同尺寸及更大体量开源模型（2B-9B参数），在多项选择题准确率最高提升25%，其他任务也表现优异，显示出卓越的领域推理和泛化能力。

Conclusion: 通过数据生成与精细调优流程，可以在算力与数据受限的情况下训练出高性能小型领域模型。该方法为其他专业场景小模型开发提供了可行范式，有效缓解了隐私与资源难题。

Abstract: Large Language Models (LLMs) have shown remarkable success in supporting a wide range of knowledge-intensive tasks. In specialized domains, there is growing interest in leveraging LLMs to assist subject matter experts with domain-specific challenges. However, deploying LLMs as SaaS solutions raises data privacy concerns, while many open-source models demand significant computational resources for effective domain adaptation and deployment. A promising alternative is to develop smaller, domain-specialized LLMs, though this approach is often constrained by the lack of high-quality domain-specific training data. In this work, we address these limitations by presenting a cost-efficient and scalable training pipeline that combines guided synthetic data generation from a small seed corpus with bottom-up domain data curation. Our pipeline integrates Domain-Adaptive Pretraining (DAPT), Domain-specific Supervised Fine-tuning (DSFT), and Direct Preference Optimization (DPO) to train effective small-scale models for specialized use cases. We demonstrate this approach through DiagnosticSLM, a 3B-parameter domain-specific model tailored for fault diagnosis, root cause analysis, and repair recommendation in industrial settings. To evaluate model performance, we introduce four domain-specific benchmarks: multiple-choice questions (DiagnosticMCQ), question answering (DiagnosticQA), sentence completion (DiagnosticComp), and summarization (DiagnosticSum). DiagnosticSLM achieves up to 25% accuracy improvement over open-source models of comparable or larger size (2B-9B) on the MCQ task, while also outperforming or matching them in other tasks, demonstrating effective domain-specific reasoning and generalization capabilities.

</details>


### [245] [Proactive Defense: Compound AI for Detecting Persuasion Attacks and Measuring Inoculation Effectiveness](https://arxiv.org/abs/2511.21749)
*Svitlana Volkova,Will Dupree,Hsien-Te Kao,Peter Bautista,Gabe Ganberg,Jeff Beaubien,Laura Cassani*

Main category: cs.CL

TL;DR: 本文提出了BRIES，一种新型复合AI架构，用于检测和衡量信息环境中说服攻击的有效性，并量化不同大语言模型（LLM）对此类攻击的易感性和防御能力。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和大语言模型的普及，信息环境更容易遭受复杂的说服性攻击（如假新闻、操纵性话语等）。为提升AI模型应对说服攻击的安全性和认知韧性，亟需研究有效的检测、防御和效能评估方法。

Method: BRIES系统集成了四个专用智能体：Twister（生成对抗性说服内容）、Detector（检测说服攻击类型）、Defender（通过内容接种生成有韧性的防御内容）、Assessor（用因果推断评估防御效能）。实验使用SemEval 2023 Task 3的分类法和合成数据集，比较了不同模型（GPT-4、Llama3、Mistral、Gemma）的检测表现，并探索了提示工程和参数（如temperature与confidence scoring）对检测效果的影响。

Result: 实验发现，不同大模型在检测复杂说服技巧上表现差异显著。GPT-4在微妙的修辞检测上表现最优，而开源模型Llama3和Mistral表现较弱。温度参数和提示工程对不同模型效果有显著影响。因果分析揭示不同说服攻击类型针对具体认知维度，提供了对人类认知弱点的新见解。

Conclusion: 本文量化并揭示了主流大模型在说服攻击面前的弱点，提出的BRIES框架可用于提升生成式AI和用户的认知安全与防御能力，并为暴露前的结构化干预提供参考方案。

Abstract: This paper introduces BRIES, a novel compound AI architecture designed to detect and measure the effectiveness of persuasion attacks across information environments. We present a system with specialized agents: a Twister that generates adversarial content employing targeted persuasion tactics, a Detector that identifies attack types with configurable parameters, a Defender that creates resilient content through content inoculation, and an Assessor that employs causal inference to evaluate inoculation effectiveness. Experimenting with the SemEval 2023 Task 3 taxonomy across the synthetic persuasion dataset, we demonstrate significant variations in detection performance across language agents. Our comparative analysis reveals significant performance disparities with GPT-4 achieving superior detection accuracy on complex persuasion techniques, while open-source models like Llama3 and Mistral demonstrated notable weaknesses in identifying subtle rhetorical, suggesting that different architectures encode and process persuasive language patterns in fundamentally different ways. We show that prompt engineering dramatically affects detection efficacy, with temperature settings and confidence scoring producing model-specific variations; Gemma and GPT-4 perform optimally at lower temperatures while Llama3 and Mistral show improved capabilities at higher temperatures. Our causal analysis provides novel insights into socio-emotional-cognitive signatures of persuasion attacks, revealing that different attack types target specific cognitive dimensions. This research advances generative AI safety and cognitive security by quantifying LLM-specific vulnerabilities to persuasion attacks and delivers a framework for enhancing human cognitive resilience through structured interventions before exposure to harmful content.

</details>


### [246] [Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification](https://arxiv.org/abs/2511.21752)
*Yanxi Li,Ruocheng Shan*

Main category: cs.CL

TL;DR: 本论文提出了一种轻量级且与模型无关的防御方法Label Disguise Defense（LDD），通过用别名或语义变换等方式隐藏真实标签，有效抵御针对大语言模型的类指令型提示注入攻击，在多个主流模型上部分恢复了被攻击时的准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本分类任务中依赖文本提示，非常容易受到提示注入（prompt injection）攻击，尤其是攻击者利用已知标签集合构造对抗指令直接干扰模型结果。现有防御手段要么需要重新训练模型，要么不耐混淆攻击。需要一种无需模型结构改动、可移植的实用防御方法。

Method: 提出了Label Disguise Defense（LDD），用与任务无关或语义变换的标签别名（如将“积极/消极”替换成“蓝/黄”），通过few-shot示例让模型学习新的标签映射关系，令攻击者无法直接利用原始标签集制造有效的注入指令。

Result: 在包括GPT-5、GPT-4o、LLaMA3.2等9个SOTA模型上测试，LDD能在所有模型下部分恢复被对抗攻击下损失的准确率。多数情况下，能够找到比仅用few-shot（无防御）时更高准确率的标签别名。语义相关的别名（如“好/坏”）比完全无关的别名（如“蓝/黄”）表现更好。

Conclusion: 标签语义本身可以作为有效防御层面，LDD方法无需重训练即可提升模型抗提示注入攻击的能力，是简单实用的防御策略。

Abstract: Large language models are increasingly used for text classification tasks such as sentiment analysis, yet their reliance on natural language prompts exposes them to prompt injection attacks. In particular, class-directive injections exploit knowledge of the model's label set (e.g., positive vs. negative) to override its intended behavior through adversarial instructions. Existing defenses, such as detection-based filters, instruction hierarchies, and signed prompts, either require model retraining or remain vulnerable to obfuscation. This paper introduces Label Disguise Defense (LDD), a lightweight and model-agnostic strategy that conceals true labels by replacing them with semantically transformed or unrelated alias labels(e.g., blue vs. yellow). The model learns these new label mappings implicitly through few-shot demonstrations, preventing direct correspondence between injected directives and decision outputs. We evaluate LDD across nine state-of-the-art models, including GPT-5, GPT-4o, LLaMA3.2, Gemma3, and Mistral variants, under varying few-shot and an adversarial setting. Our results show that the ability of LDD to recover performance lost to the adversarial attack varies across models and alias choices. For every model evaluated, LDD is able to restore a portion of the accuracy degradation caused by the attack. Moreover, for the vast majority of models, we can identify more than one alias pair that achieves higher accuracy than the under-attack baseline, in which the model relies solely on few-shot learning without any defensive mechanism. A linguistic analysis further reveals that semantically aligned alias labels(e.g., good vs. bad) yield stronger robustness than unaligned symbols(e.g., blue vs. yellow). Overall, this study demonstrates that label semantics can serve as an effective defense layer, transforming meaning itself into a shield against prompt injection.

</details>


### [247] [Extracting Disaster Impacts and Impact Related Locations in Social Media Posts Using Large Language Models](https://arxiv.org/abs/2511.21753)
*Sameeah Noreen Hameed,Surangika Ranathunga,Raj Prasanna,Kristin Stock,Christopher B. Jones*

Main category: cs.CL

TL;DR: 本研究提出利用大型语言模型（LLMs）识别灾害相关社交媒体帖文中的影响地点，并成功提高了抽取准确率。


<details>
  <summary>Details</summary>
Motivation: 传统依赖传感器和遥感的灾情感知受时空限制，导致信息缺口，而社交媒体能够补足，但其中仅一部分地点真正为受影响区域，因此需精准甄别。

Method: 对LLM进行微调，使其能识别社交媒体中所有地点、影响事件及真正受影响的地点，包括非正式表达、缩写和简称，区别受灾地点与其他提到的地点。

Result: 微调后的模型在影响事件识别达到F1值0.69，在受影响地点识别达到0.74，显著优于预训练基线模型。

Conclusion: 微调LLM能有效、规模化地支持灾害响应中的资源调配、情势感知和灾后恢复决策。

Abstract: Large-scale disasters can often result in catastrophic consequences on people and infrastructure. Situation awareness about such disaster impacts generated by authoritative data from in-situ sensors, remote sensing imagery, and/or geographic data is often limited due to atmospheric opacity, satellite revisits, and time limitations. This often results in geo-temporal information gaps. In contrast, impact-related social media posts can act as "geo-sensors" during a disaster, where people describe specific impacts and locations. However, not all locations mentioned in disaster-related social media posts relate to an impact. Only the impacted locations are critical for directing resources effectively. e.g., "The death toll from a fire which ripped through the Greek coastal town of #Mati stood at 80, with dozens of people unaccounted for as forensic experts tried to identify victims who were burned alive #Greecefires #AthensFires #Athens #Greece." contains impacted location "Mati" and non-impacted locations "Greece" and "Athens". This research uses Large Language Models (LLMs) to identify all locations, impacts and impacted locations mentioned in disaster-related social media posts. In the process, LLMs are fine-tuned to identify only impacts and impacted locations (as distinct from other, non-impacted locations), including locations mentioned in informal expressions, abbreviations, and short forms. Our fine-tuned model demonstrates efficacy, achieving an F1-score of 0.69 for impact and 0.74 for impacted location extraction, substantially outperforming the pre-trained baseline. These robust results confirm the potential of fine-tuned language models to offer a scalable solution for timely decision-making in resource allocation, situational awareness, and post-disaster recovery planning for responders.

</details>


### [248] [Dissecting the Ledger: Locating and Suppressing "Liar Circuits" in Financial Large Language Models](https://arxiv.org/abs/2511.21756)
*Soham Mirajkar*

Main category: cs.CL

TL;DR: 本论文针对大型语言模型在金融领域做算术运算时出现的可重现幻觉问题，提出并验证了具体的内在检测机制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在金融等高风险领域受到广泛应用，但在涉及算术运算时经常出现错误且重复的输出（幻觉）。以往的改进方法多为黑箱操作，缺乏对模型内部机制的洞察，因此有必要探索幻觉产生的机制以及有效的检测方式。

Method: 作者采用 Causal Tracing 技术，分析了GPT-2 XL在ConvFinQA数据集上的算术推理过程。通过追踪不同层的计算过程，发现其包含两个阶段：中间层（L12-L30）作为分布式计算草稿本，后期层（尤其是第46层）作为关键聚合电路。通过消融实验验证第46层对最终输出影响。

Result: 实验结果显示，抑制第46层能使模型对幻觉输出的置信度降低81.8%。此外，针对该层训练的线性探针在未见过的金融主题上依然能以98%的准确率检测幻觉，表明该机制具有一定的普适性。

Conclusion: 论文揭示了GPT-2 XL在算术幻觉生成中的具体机制，证明了该机制可用于有效检测和抑制算术相关的幻觉输出，为今后提高金融领域任务可信度提供了机制层面的参考。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes financial domains, yet they suffer from specific, reproducible hallucinations when performing arithmetic operations. Current mitigation strategies often treat the model as a black box. In this work, we propose a mechanistic approach to intrinsic hallucination detection. By applying Causal Tracing to the GPT-2 XL architecture on the ConvFinQA benchmark, we identify a dual-stage mechanism for arithmetic reasoning: a distributed computational scratchpad in middle layers (L12-L30) and a decisive aggregation circuit in late layers (specifically Layer 46). We verify this mechanism via an ablation study, demonstrating that suppressing Layer 46 reduces the model's confidence in hallucinatory outputs by 81.8%. Furthermore, we demonstrate that a linear probe trained on this layer generalizes to unseen financial topics with 98% accuracy, suggesting a universal geometry of arithmetic deception.

</details>


### [249] [Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models](https://arxiv.org/abs/2511.21759)
*Linye Wei,Wenjue Chen,Pingzhi Tang,Xiaotian Guo,Le Ye,Runsheng Wang,Meng Li*

Main category: cs.CL

TL;DR: 本文提出了一个名为ODB-dLLM的加速框架，大幅提升了基于扩散的大型语言模型（dLLM）的推理速度。


<details>
  <summary>Details</summary>
Motivation: 虽然dLLM具有并行解码的潜力并通过KV缓存提升了效率，但其双向注意力机制导致缓存频繁刷新，prefill和decoding两个阶段都存在高昂的推理开销，限制了进一步加速。现有固定响应长度设计也带来不必要的计算浪费。

Method: 提出ODB-dLLM框架，预填充（prefill）阶段引入自适应长度预测机制动态减少冗余计算；解码阶段提出跳跃共享推测性解码方法，减少解码迭代数，提高效率。整个框架针对prefill和decoding两个异构计算强度阶段，分别优化。

Result: 实验表明，ODB-dLLM在推理速度上相比基线dLLM和Fast-dLLM分别提升了46-162倍和2.63-6.3倍，同时有效减少了精度损失。

Conclusion: ODB-dLLM框架能显著加速dLLM模型推理，并在保持精度的前提下提升效率，对现有扩散型大模型部署具有实际意义。

Abstract: Diffusion-based large language models (dLLMs) have recently gained significant attention for their exceptional performance and inherent potential for parallel decoding. Existing frameworks further enhance its inference efficiency by enabling KV caching. However, its bidirectional attention mechanism necessitates periodic cache refreshes that interleave prefill and decoding phases, both contributing substantial inference cost and constraining achievable speedup. Inspired by the heterogeneous arithmetic intensity of the prefill and decoding phases, we propose ODB-dLLM, a framework that orchestrates dual-boundaries to accelerate dLLM inference. In the prefill phase, we find that the predefined fixed response length introduces heavy yet redundant computational overhead, which affects efficiency. To alleviate this, ODB-dLLM incorporates an adaptive length prediction mechanism that progressively reduces prefill overhead and unnecessary computation. In the decoding phase, we analyze the computational characteristics of dLLMs and propose a dLLM-specific jump-share speculative decoding method to enhance efficiency by reducing the number of decoding iterations. Experimental results demonstrate that ODB-dLLM achieves 46-162x and 2.63-6.30x speedups over the baseline dLLM and Fast-dLLM, respectively, while simultaneously mitigating the accuracy degradation in existing acceleration frameworks.

</details>


### [250] [fMRI-LM: Towards a Universal Foundation Model for Language-Aligned fMRI Understanding](https://arxiv.org/abs/2511.21760)
*Yuxiang Wei,Yanteng Zhang,Xi Xiao,Chengxuan Qian,Tianyang Wang,Vince D. Calhoun*

Main category: cs.CL

TL;DR: 论文提出了fMRI-LM，一种将fMRI脑成像与大语言模型连接起来的新模型，能够直接以语言形式理解和描述脑活动，并展示了在多项任务上的优异性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型已能统一处理图像、音频和视频，但大脑成像（如fMRI）与语言的结合几乎未被探索。打通大脑活动与语义认知的联系，对认知神经科学和跨模态表征具有重要意义。

Method: 作者提出三阶段框架：(1) 训练神经tokenizer将fMRI信号映射为离散token并嵌入语言空间；(2) 利用预训练LLM，将fMRI token和文本联合建模，把脑活动作为可预测和可描述的序列处理，并通过自建大规模fMRI描述文本对丰富数据；(3) 进行多任务指令微调，提升模型高级语义理解和多应用适应能力。

Result: fMRI-LM在各种基准任务上实现了强大的零样本和小样本学习能力，通过参数高效调整（如LoRA）能高效适应新任务，体现出良好的可扩展性。

Conclusion: 该工作为结构化、语义化地理解fMRI提供了一条可扩展的技术路线，把脑成像纳入到统一的语言对齐大模型生态，有望推动神经科学和多模态AI的发展。

Abstract: Recent advances in multimodal large language models (LLMs) have enabled unified reasoning across images, audio, and video, but extending such capability to brain imaging remains largely unexplored. Bridging this gap is essential to link neural activity with semantic cognition and to develop cross-modal brain representations. To this end, we present fMRI-LM, a foundational model that bridges functional MRI (fMRI) and language through a three-stage framework. In Stage 1, we learn a neural tokenizer that maps fMRI into discrete tokens embedded in a language-consistent space. In Stage 2, a pretrained LLM is adapted to jointly model fMRI tokens and text, treating brain activity as a sequence that can be temporally predicted and linguistically described. To overcome the lack of natural fMRI-text pairs, we construct a large descriptive corpus that translates diverse imaging-based features into structured textual descriptors, capturing the low-level organization of fMRI signals. In Stage 3, we perform multi-task, multi-paradigm instruction tuning to endow fMRI-LM with high-level semantic understanding, supporting diverse downstream applications. Across various benchmarks, fMRI-LM achieves strong zero-shot and few-shot performance, and adapts efficiently with parameter-efficient tuning (LoRA), establishing a scalable pathway toward a language-aligned, universal model for structural and semantic understanding of fMRI.

</details>


### [251] [LLMs for Low-Resource Dialect Translation Using Context-Aware Prompting: A Case Study on Sylheti](https://arxiv.org/abs/2511.21761)
*Tabia Tanzin Prama,Christopher M. Danforth,Peter Sheridan Dodds*

Main category: cs.CL

TL;DR: 本研究首次系统性评估大模型对低资源方言Sylheti的机器翻译能力，并提出Sylheti-CAP（上下文感知提示框架），在多个模型和策略下均有效提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在主流语种翻译表现出色，但在低资源及方言情境下的能力尚未充分研究。Sylheti作为Bangla的方言，资源稀缺，正式翻译困难。探索大模型在此领域的表现以及提升方法，具有现实意义和学术价值。

Method: 作者选取五个先进大语言模型（GPT-4.1、LLaMA 4、Grok 3、DeepSeek V3.2等），在Bangla↔Sylheti双向翻译任务上测试其基线表现。发现模型在方言词汇处理上力有未逮，基于此提出Sylheti-CAP三步法：在提示中融合语言规则、词典（涵盖核心词汇及习语）及真实性检测，提升翻译上下文与准确性。

Result: 实验结果显示，Sylheti-CAP框架在不同模型和提示策略下均能显著提升翻译质量。自动评价与人工评估结果均表明其有效，定性分析还显示幻觉、歧义和翻译不自然现象明显减少。

Conclusion: Sylheti-CAP作为面向方言与低资源机器翻译的通用可扩展解决方案，能够系统提升大模型在此类任务中的表现，为相关研究和实际应用提供了新方法和参考。

Abstract: Large Language Models (LLMs) have demonstrated strong translation abilities through prompting, even without task-specific training. However, their effectiveness in dialectal and low-resource contexts remains underexplored. This study presents the first systematic investigation of LLM-based machine translation (MT) for Sylheti, a dialect of Bangla that is itself low-resource. We evaluate five advanced LLMs (GPT-4.1, GPT-4.1, LLaMA 4, Grok 3, and DeepSeek V3.2) across both translation directions (Bangla $\Leftrightarrow$ Sylheti), and find that these models struggle with dialect-specific vocabulary. To address this, we introduce Sylheti-CAP (Context-Aware Prompting), a three-step framework that embeds a linguistic rulebook, a dictionary (2{,}260 core vocabulary items and idioms), and an authenticity check directly into prompts. Extensive experiments show that Sylheti-CAP consistently improves translation quality across models and prompting strategies. Both automatic metrics and human evaluations confirm its effectiveness, while qualitative analysis reveals notable reductions in hallucinations, ambiguities, and awkward phrasing, establishing Sylheti-CAP as a scalable solution for dialectal and low-resource MT. Dataset link: \href{https://github.com/TabiaTanzin/LLMs-for-Low-Resource-Dialect-Translation-Using-Context-Aware-Prompting-A-Case-Study-on-Sylheti.git}{https://github.com/TabiaTanzin/LLMs-for-Low-Resource-Dialect-Translation-Using-Context-Aware-Prompting-A-Case-Study-on-Sylheti.git}

</details>


### [252] [Factors That Support Grounded Responses in LLM Conversations: A Rapid Review](https://arxiv.org/abs/2511.21762)
*Gabriele Cesar Iwashima,Claudia Susie Rodrigues,Claudio Dipolitto,Geraldo Xexéo*

Main category: cs.CL

TL;DR: 本综述系统梳理了提升大语言模型（LLM）对话一致性、语境扎根与抑制幻觉的技术，并对其效果进行了比较。


<details>
  <summary>Details</summary>
Motivation: LLM在实际应用中经常出现输出与用户意图不符、欠缺语境支撑或生成幻觉内容的问题，影响了应用的可靠性，因此需要对齐的有效技术。

Method: 采用快速综述法（Rapid Review），结合PRISMA框架与PICO策略，系统搜索、筛选和分析现有文献，并按LLM生命周期阶段（推理时、训练后、强化学习）对对齐方法进行分类评价。

Result: 在多种对齐策略中，推理时对齐方法表现最为高效，无需重新训练模型即可更好支持用户意图、语境扎根与减少幻觉，提升响应的一致性与可靠性。

Conclusion: 结构化的对齐机制能有效提升LLM输出的质量与可靠性，推理时对齐方法具有高实用性，有助于不同应用场景下的LLM能力提升。

Abstract: Large language models (LLMs) may generate outputs that are misaligned with user intent, lack contextual grounding, or exhibit hallucinations during conversation, which compromises the reliability of LLM-based applications. This review aimed to identify and analyze techniques that align LLM responses with conversational goals, ensure grounding, and reduce hallucination and topic drift. We conducted a Rapid Review guided by the PRISMA framework and the PICO strategy to structure the search, filtering, and selection processes. The alignment strategies identified were categorized according to the LLM lifecycle phase in which they operate: inference-time, post-training, and reinforcement learning-based methods. Among these, inference-time approaches emerged as particularly efficient, aligning outputs without retraining while supporting user intent, contextual grounding, and hallucination mitigation. The reviewed techniques provided structured mechanisms for improving the quality and reliability of LLM responses across key alignment objectives.

</details>


### [253] [FLAWS: A Benchmark for Error Identification and Localization in Scientific Papers](https://arxiv.org/abs/2511.21843)
*Sarina Xi,Vishisht Rao,Justin Payan,Nihar B. Shah*

Main category: cs.CL

TL;DR: 本文提出了一个用于评估大语言模型（LLM）在科学论文中定位错误能力的自动化基准FLAWS，并发现目前最先进的模型在这个任务中表现有限。


<details>
  <summary>Details</summary>
Motivation: 随着科研文献数量的激增，人工审稿人员难以高效发现研究中的关键错误，而LLM的崛起为学术审稿和科学评估提供了自动化辅助的新可能，因此亟需系统研究LLM在发现论文错误，特别是定位影响核心结论的错误方面的能力。

Method: 作者利用LLM自动在经过同行评审的论文中系统性地插入会使主要论点失效的错误，构建出713对论文-错误对，并引入可扩展的自动化评估指标，衡量不同LLM发现并定位这些错误的表现，最后选用5个前沿LLM进行实验评估。

Result: 实验显示，当前最优秀的GPT 5模型在k=10时能准确识别39.1%的错误，其他主流LLM表现更低，说明现有LLM在学术自动评审中的错误定位能力仍有限。

Conclusion: LLM在论文错误定位任务上仍面临较大挑战，现有模型准确性有待提高，FLAWS为后续改进LLM及其应用于自动科学评审提供了重要测试平台和数据集。

Abstract: The identification and localization of errors is a core task in peer review, yet the exponential growth of scientific output has made it increasingly difficult for human reviewers to reliably detect errors given the limited pool of experts. Recent advances in Large Language Models (LLMs) have sparked interest in their potential to support such evaluation tasks, from academic peer review to automated scientific assessment. However, despite the growing use of LLMs in review systems, their capabilities to pinpoint errors remain underexplored. In this work, we introduce Fault Localization Across Writing in Science (FLAWS), an automated benchmark consisting of 713 paper-error pairs designed to evaluate how effectively LLMs detect errors that undermine key claims in research papers. We construct the benchmark by systematically inserting claim-invalidating errors into peer-reviewed papers using LLMs, paired with an automated evaluation metric that measures whether models can identify and localize these errors. Developing such a benchmark presents unique challenges that we overcome: ensuring that the inserted errors are well-defined, challenging, and relevant to the content of the paper, avoiding artifacts that would make identification trivial, and designing a scalable, automated evaluation metric. On the resulting benchmark, we evaluate five frontier LLMs: Claude Sonnet 4.5, DeepSeek Reasoner v3.1, Gemini 2.5 Pro, GPT 5, and Grok 4. Among these, GPT 5 is the top-performing model, achieving 39.1% identification accuracy when k=10, where k is the number of top-ranked error text candidates generated by the LLM.

</details>


### [254] [Improving Score Reliability of Multiple Choice Benchmarks with Consistency Evaluation and Altered Answer Choices](https://arxiv.org/abs/2511.21860)
*Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Claudio Pinhanez,Yago Primerano*

Main category: cs.CL

TL;DR: 提出了CoRA指标，通过衡量大语言模型在多选任务上的响应一致性，提升模型分数的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在多选题基准上常仅凭分数评价，未考虑模型回答的一致性，导致高分模型可能实际不稳定。

Method: 利用合成生成、选项顺序变换的问题，评估LLM的Bare-Minimum-Consistency Accuracy和Consistency Index，再据此对原始得分做一致性调整，得到CoRA分数。

Result: 实验表明，部分LLM即使MCQA分数高，回答一致性可能低；CoRA能够有效下调这些不一致模型的分数。

Conclusion: CoRA更全面反映LLM在多选任务上的表现，有助于发现得分高但不一致的模型，为模型评测带来新视角。

Abstract: In this work we present the Consistency-Rebalanced Accuracy (CoRA) metric, improving the reliability of Large Language Model (LLM) scores computed on multiple choice (MC) benchmarks. Our metric explores the response consistency of the LLMs, taking advantage of synthetically-generated questions with altered answer choices. With two intermediate scores, i.e. Bare-Minimum-Consistency Accuracy (BMCA) and Consistency Index (CI), CoRA is computed by adjusting the multiple-choice question answering (MCQA) scores to better reflect the level of consistency of the LLM. We present evaluations in different benchmarks using diverse LLMs, and not only demonstrate that LLMs can present low response consistency even when they present high MCQA scores, but also that CoRA can successfully scale down the scores of inconsistent models.

</details>


### [255] [A Customer Journey in the Land of Oz: Leveraging the Wizard of Oz Technique to Model Emotions in Customer Service Interactions](https://arxiv.org/abs/2511.21909)
*Sofie Labat,Thomas Demeester,Véronique Hoste*

Main category: cs.CL

TL;DR: 本文提出并构建了EmoWOZ-CS语料库，专注于情感感知客服场景，包含2,148条双语对话，评测了基于WOZ实验的情感轨迹引导，并对情感检测与预测进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别资源大多脱离实际领域、标签单一且聚焦事后检测，不足以支持情感感知的客户服务。为解决该领域缺乏合适对话数据和多元标注的问题，亟需在特定场景下收集丰富且真实的情感对话数据。

Method: 通过控制性的“巫师实验（WOZ）”，在民航、电商、在线旅游和电信场景下，收集了179名参与者的2,148条荷兰语-英语书面对话，并以运营者可控的情感轨迹设计，引导对话进程。同时，对比参与者自我报告和第三方人工标注，评估情感多标签一致性，并在语料库上进行情感检测与预测的基准实验。

Result: 1. 参与者消息以中性为主，渴望与感激为主要非中性情感。2. 情感标签与效价一致性中等，唤醒度和支配度一致性较低；自我报告与第三方标注对中性、感激和愤怒一致性较高。3. 客观应答策略多引发中性或感激，低效策略易激发愤怒、渴望等负面情绪。4. 不同情感策略能正负向调动用户情绪，实验能有效按需求引导情感轨迹，负向目标效果尤为明显。5. 预测实验显示，基于先前轮次推断后续情感难度较大。

Conclusion: EmoWOZ-CS语料库为面向情感感知客服的研究提供了真实、丰富、多标签的对话数据，同时揭示了情感一致性标注的复杂性与主动情感预测的挑战。WOZ引导的情感轨迹方法有效，未来应进一步提升情感推理能力，用于更复杂和前瞻性的客服支持场景。

Abstract: Emotion-aware customer service needs in-domain conversational data, rich annotations, and predictive capabilities, but existing resources for emotion recognition are often out-of-domain, narrowly labeled, and focused on post-hoc detection. To address this, we conducted a controlled Wizard of Oz (WOZ) experiment to elicit interactions with targeted affective trajectories. The resulting corpus, EmoWOZ-CS, contains 2,148 bilingual (Dutch-English) written dialogues from 179 participants across commercial aviation, e-commerce, online travel agencies, and telecommunication scenarios. Our contributions are threefold: (1) Evaluate WOZ-based operator-steered valence trajectories as a design for emotion research; (2) Quantify human annotation performance and variation, including divergences between self-reports and third-party judgments; (3) Benchmark detection and forward-looking emotion inference in real-time support. Findings show neutral dominates participant messages; desire and gratitude are the most frequent non-neutral emotions. Agreement is moderate for multilabel emotions and valence, lower for arousal and dominance; self-reports diverge notably from third-party labels, aligning most for neutral, gratitude, and anger. Objective strategies often elicit neutrality or gratitude, while suboptimal strategies increase anger, annoyance, disappointment, desire, and confusion. Some affective strategies (cheerfulness, gratitude) foster positive reciprocity, whereas others (apology, empathy) can also leave desire, anger, or annoyance. Temporal analysis confirms successful conversation-level steering toward prescribed trajectories, most distinctly for negative targets; positive and neutral targets yield similar final valence distributions. Benchmarks highlight the difficulty of forward-looking emotion inference from prior turns, underscoring the complexity of proactive emotion-aware support.

</details>


### [256] [Lips-Jaw and Tongue-Jaw Articulatory Tradeoff in DYNARTmo](https://arxiv.org/abs/2511.22155)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: 本文用DYNARTmo动态发音模型模拟语音产生时主、次发音器官之间的协调关系，并展示其简单机制下能重现真实发音协同模式。


<details>
  <summary>Details</summary>
Motivation: 发音时，多个发音器官如嘴唇、下颌和舌头协同工作，但这种协同的动力机制及其模拟仍存挑战。作者希望通过简化的动态模型研究发音器官在不同音节条件下的协同和权衡。

Method: 采用DYNARTmo模型，模拟嘴唇-下颌、舌头-下颌间的配合。首先梳理了任务动力学和DYNARTmo模型的理论关系，强调任务空间轨迹和低层次执行的区别，然后进行不同辅元音组合（如labial, apical, dorsal与/a/、/i/、/u/元音）的仿真实验。

Result: 仿真显示：下颌位移随着发音部位和元音环境变化，模型可复现真实发音协同，如下颌辅助舌尖闭合、双唇爆破音时下唇抬升、舌-下颌共同运动以及噪点发音时的协作饱和现象。

Conclusion: 即便在简化学计算前提下，DYNARTmo模型依然能够生成与真实语言类似的时空运动模式，有效捕捉了发音器官之间的权衡与协同。

Abstract: This paper investigates how the dynamic articulatory model DYNARTmo accounts for articulatory tradeoffs between primary and secondary articulators, with a focus on lips-jaw and tongue-jaw coordination. While DYNARTmo does not implement full task-dynamic second-order biomechanics, it adopts first-order task-space gesture specifications comparable to those used in articulatory phonology and integrates a simplified mechanism for distributing articulatory effort across multiple articulators. We first outline the conceptual relationship between task dynamics and DYNARTmo, emphasizing the distinction between high-level task-space trajectories and their low-level articulatory execution. We then present simulation results for a set of CV syllables that illustrate how jaw displacement varies as a function of both place of articulation (labial, apical, dorsal) and vowel context (/a/, /i/, /u/). The model reproduces empirically attested patterns of articulatory synergy, including jaw-supported apical closures, lower-lip elevation in bilabial stops, tongue-jaw co-movement, and saturation effects in labial constrictions. These results demonstrate that even with computationally simplified assumptions, DYNARTmo can generate realistic spatio-temporal movement patterns that capture key aspects of articulatory tradeoff and synergy across a range of consonant-vowel combinations.

</details>


### [257] [Tracing How Annotators Think: Augmenting Preference Judgments with Reading Processes](https://arxiv.org/abs/2511.21912)
*Karin de Langis,William Walker,Khanh Chi Le,Dongyeop Kang*

Main category: cs.CL

TL;DR: 该论文提出了一种新的标注方法，既记录标签也记录标注文献的阅读过程，并基于此方法构建了PreferRead数据集，通过分析阅读行为辅助理解标注决策及一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的NLP标注通常只关注最终标签，忽视了标注者形成决策的认知过程。作者希望通过捕捉标注者的阅读行为，更深入分析标注一致性和分歧背后的原因，从而提升复杂主观任务中标注数据的可靠性。

Method: 作者提出了一种新的注释框架，利用鼠标轨迹记录标注者在填写偏好标注任务时对文本（提示和两个候选回复）关注、重读和浏览的细节。基于此，他们构建了细粒度记录阅读行为的PreferRead数据集，并对阅读行为与标注一致性等结果进行了统计分析。

Result: 分析发现：1）标注者约一半任务会重读至少一个回复，更倾向于重读最终选择的选项，很少重读原始提示；2）重读行为与高标注一致性显著相关，而较长的阅读路径和花费时间则与一致性较低相关。

Conclusion: 该研究证明了阅读过程数据在理解标注者决策可靠性和主观NLP任务中的分歧方面的价值，丰富了对标注数据认知维度的理解。相关代码和数据已公开，为后续研究提供资源。

Abstract: We propose an annotation approach that captures not only labels but also the reading process underlying annotators' decisions, e.g., what parts of the text they focus on, re-read or skim. Using this framework, we conduct a case study on the preference annotation task, creating a dataset PreferRead that contains fine-grained annotator reading behaviors obtained from mouse tracking. PreferRead enables detailed analysis of how annotators navigate between a prompt and two candidate responses before selecting their preference. We find that annotators re-read a response in roughly half of all trials, most often revisiting the option they ultimately choose, and rarely revisit the prompt. Reading behaviors are also significantly related to annotation outcomes: re-reading is associated with higher inter-annotator agreement, whereas long reading paths and times are associated with lower agreement. These results demonstrate that reading processes provide a complementary cognitive dimension for understanding annotator reliability, decision-making and disagreement in complex, subjective NLP tasks. Our code and data are publicly available.

</details>


### [258] [A Comparative Study of LLM Prompting and Fine-Tuning for Cross-genre Authorship Attribution on Chinese Lyrics](https://arxiv.org/abs/2511.21930)
*Yuxin Li,Lorraine Xu,Meng Fan Wang*

Main category: cs.CL

TL;DR: 本论文针对中文歌词作者归属任务进行研究，创建了新的均衡数据集，并比较了针对领域微调模型与大语言模型零样本推理的性能，发现归属表现受歌词体裁影响明显。


<details>
  <summary>Details</summary>
Motivation: 现有中文歌词作者归属领域缺乏干净、公开的数据集，相关研究也较少。作者旨在填补该空白，推动该领域的发展。

Method: （1）构建了一个覆盖多体裁、作者均衡的中文歌词新数据集；（2）研发并微调了领域特定作者归属模型，并将其与DeepSeek大模型零样本推理进行对比；（3）设计两项假设、两组实验：一是微调相比零样本更优，二是表现与体裁密切相关。

Result: 实验结果强烈支持“作者归属准确率随体裁而异”这一假设，结构性体裁（如民谣、传统）显著优于抽象体裁（如爱情）。微调模型在部分测试集表现更好，但在合成增强小样本集时提升有限。

Conclusion: 本工作建立了第一个跨体裁的中文歌词作者归属基准，强调体裁敏感评估的重要性，并提供了公开数据集和分析框架。建议扩大测试集多样性、减少对简单增强的依赖、均衡作者分布，并探索更适配领域预训练以提升模型归属能力。

Abstract: We propose a novel study on authorship attribution for Chinese lyrics, a domain where clean, public datasets are sorely lacking. Our contributions are twofold: (1) we create a new, balanced dataset of Chinese lyrics spanning multiple genres, and (2) we develop and fine-tune a domain-specific model, comparing its performance against zero-shot inference using the DeepSeek LLM.
  We test two central hypotheses. First, we hypothesize that a fine-tuned model will outperform a zero-shot LLM baseline. Second, we hypothesize that performance is genre-dependent. Our experiments strongly confirm Hypothesis 2: structured genres (e.g. Folklore & Tradition) yield significantly higher attribution accuracy than more abstract genres (e.g. Love & Romance). Hypothesis 1 receives only partial support: fine-tuning improves robustness and generalization in Test1 (real-world data and difficult genres), but offers limited or ambiguous gains in Test2, a smaller, synthetically-augmented set. We show that the design limitations of Test2 (e.g., label imbalance, shallow lexical differences, and narrow genre sampling) can obscure the true effectiveness of fine-tuning.
  Our work establishes the first benchmark for cross-genre Chinese lyric attribution, highlights the importance of genre-sensitive evaluation, and provides a public dataset and analytical framework for future research. We conclude with recommendations: enlarge and diversify test sets, reduce reliance on token-level data augmentation, balance author representation across genres, and investigate domain-adaptive pretraining as a pathway for improved attribution performance.

</details>


### [259] [Start Making Sense(s): A Developmental Probe of Attention Specialization Using Lexical Ambiguity](https://arxiv.org/abs/2511.21974)
*Pamela D. Rivière,Sean Trott*

Main category: cs.CL

TL;DR: 论文提出了一种系统性分析和测试Transformer语言模型中注意力机制的方法，利用词汇歧义任务分析注意力头在词义消歧中的作用。


<details>
  <summary>Details</summary>
Motivation: 尽管我们对Transformer自注意力操作有一定认识，但具体这些操作如何实现可解释的计算或功能仍不明确，尤其是在词义消歧任务中各注意力头是否以及如何变得具有专业化模式尚待探究。

Method: 作者提出了一套流程，结合词汇歧义，通过分析不同规模（14M与410M参数）的公开Pythia语言模型在训练过程中的各个检查点，定位注意力头对消歧表现的贡献，并测试这些注意力头对输入扰动的鲁棒性及其因果作用，且在多个随机种子上复现实验。

Result: 在14M模型中，部分注意力头与词义消歧表现密切相关但鲁棒性有限；在410M较大模型中，出现了多个较为泛化且对消歧表现有重要作用的注意力头；移除这些关键头会显著削弱消歧能力。

Conclusion: 词义消歧依赖多个机制，其中一些（特别在小模型中）对词位置信息和词性极为敏感，而大模型中存在更为健壮的注意力机制。采用模型发展视角分析机制，对于理解语言模型内部机理极具价值。

Abstract: Despite an in-principle understanding of self-attention matrix operations in Transformer language models (LMs), it remains unclear precisely how these operations map onto interpretable computations or functions--and how or when individual attention heads develop specialized attention patterns. Here, we present a pipeline to systematically probe attention mechanisms, and we illustrate its value by leveraging lexical ambiguity--where a single word has multiple meanings--to isolate attention mechanisms that contribute to word sense disambiguation. We take a "developmental" approach: first, using publicly available Pythia LM checkpoints, we identify inflection points in disambiguation performance for each LM in the suite; in 14M and 410M, we identify heads whose attention to disambiguating words covaries with overall disambiguation performance across development. We then stress-test the robustness of these heads to stimulus perturbations: in 14M, we find limited robustness, but in 410M, we identify multiple heads with surprisingly generalizable behavior. Then, in a causal analysis, we find that ablating the target heads demonstrably impairs disambiguation performance, particularly in 14M. We additionally reproduce developmental analyses of 14M across all of its random seeds. Together, these results suggest: that disambiguation benefits from a constellation of mechanisms, some of which (especially in 14M) are highly sensitive to the position and part-of-speech of the disambiguating cue; and that larger models (410M) may contain heads with more robust disambiguation behavior. They also join a growing body of work that highlights the value of adopting a developmental perspective when probing LM mechanisms.

</details>


### [260] [AfriStereo: A Culturally Grounded Dataset for Evaluating Stereotypical Bias in Large Language Models](https://arxiv.org/abs/2511.22016)
*Yann Le Beux,Oluchi Audu,Oche D. Ankeli,Dhananjay Balakrishnan,Melissah Weya,Marie D. Ralaiarinosy,Ignatius Ezeani*

Main category: cs.CL

TL;DR: 该论文介绍了AfriStereo，这是首个针对非洲情境的开放源代码刻板印象数据集和评测框架。该工具有助于更好地评估当前AI模型在非洲语境下的刻板印象偏见问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI偏见评测数据集主要反映西方观点，导致非洲文化情境下的刻板印象长期被忽视。这种缺位可能导致AI在涉及非洲地区应用时强化有害刻板印象。作者希望通过补齐这一空白，提升AI系统在全球范围的公平性和适用性。

Method: 作者通过在塞内加尔、肯尼亚和尼日利亚的社区合作，收集了1,163条涵盖性别、种族、宗教、年龄和职业等方面的非洲刻板印象，通过人工参与的几轮增补扩展到5,000多组‘刻板印象-反刻板印象’对，并采用语义聚类与文化背景审阅者手动标注校验数据质量。之后使用该数据集评估现有大语言模型的偏见表现。

Result: 初步测试表明，现有11个大语言模型中有9个在各项刻板印象维度（尤以年龄、职业、性别为主）上存在统计显著的偏见，偏见偏好比在0.63-0.78之间（p≤0.05），即模型更倾向输出刻板印象内容。领域专用模型在本测试中表现出较弱的偏见，提示专业领域训练可一定程度减弱偏见。

Conclusion: AfriStereo为NLP领域提供了首个适用于非洲情境的刻板印象偏见评测与数据集工具，有助于推动后续文化本土化的偏见评测和缓解技术研发，对提升AI系统的公平性和全球适用性具有积极意义。

Abstract: Existing AI bias evaluation benchmarks largely reflect Western perspectives, leaving African contexts underrepresented and enabling harmful stereotypes in applications across various domains. To address this gap, we introduce AfriStereo, the first open-source African stereotype dataset and evaluation framework grounded in local socio-cultural contexts. Through community engaged efforts across Senegal, Kenya, and Nigeria, we collected 1,163 stereotypes spanning gender, ethnicity, religion, age, and profession. Using few-shot prompting with human-in-the-loop validation, we augmented the dataset to over 5,000 stereotype-antistereotype pairs. Entries were validated through semantic clustering and manual annotation by culturally informed reviewers. Preliminary evaluation of language models reveals that nine of eleven models exhibit statistically significant bias, with Bias Preference Ratios (BPR) ranging from 0.63 to 0.78 (p <= 0.05), indicating systematic preferences for stereotypes over antistereotypes, particularly across age, profession, and gender dimensions. Domain-specific models appeared to show weaker bias in our setup, suggesting task-specific training may mitigate some associations. Looking ahead, AfriStereo opens pathways for future research on culturally grounded bias evaluation and mitigation, offering key methodologies for the AI community on building more equitable, context-aware, and globally inclusive NLP technologies.

</details>


### [261] [ResearchArcade: Graph Interface for Academic Tasks](https://arxiv.org/abs/2511.22036)
*Jingjun Xu,Chongshan Lin,Haofei Yu,Tao Feng,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文提出了ResearchArcade，一个基于图结构的统一数据接口，通过整合不同学术数据源与多模态信息，支持多种学术任务和模型，提升了数据组织与模型表现。


<details>
  <summary>Details</summary>
Motivation: 随着学术数据源日益多样且机器学习在学术研究中的应用增多，当前缺乏统一的数据接口来高效支持多种学术任务，制约了知识发现与模型开发。

Method: 作者提出ResearchArcade：利用多表格图结构统一管理不同学术数据（如ArXiv论文、OpenReview评审），整合多模态信息（文本、图片、表格），保留时间演化特征，并统一多种学术任务定义，支持不同输入要求的模型。

Result: 在六种学术任务中，利用ResearchArcade整合跨源与多模态数据的信息，显著提升了模型在各任务中的表现，且引入图结构始终优于传统基线方法。

Conclusion: ResearchArcade作为统一型学术数据接口能有效促进跨源、多模态学术数据的整合与利用，持续提升学术任务完成效率，对推动学术研究进展具有显著潜力。

Abstract: Academic research generates diverse data sources, and as researchers increasingly use machine learning to assist research tasks, a crucial question arises: Can we build a unified data interface to support the development of machine learning models for various academic tasks? Models trained on such a unified interface can better support human researchers throughout the research process, eventually accelerating knowledge discovery. In this work, we introduce ResearchArcade, a graph-based interface that connects multiple academic data sources, unifies task definitions, and supports a wide range of base models to address key academic challenges. ResearchArcade utilizes a coherent multi-table format with graph structures to organize data from different sources, including academic corpora from ArXiv and peer reviews from OpenReview, while capturing information with multiple modalities, such as text, figures, and tables. ResearchArcade also preserves temporal evolution at both the manuscript and community levels, supporting the study of paper revisions as well as broader research trends over time. Additionally, ResearchArcade unifies diverse academic task definitions and supports various models with distinct input requirements. Our experiments across six academic tasks demonstrate that combining cross-source and multi-modal information enables a broader range of tasks, while incorporating graph structures consistently improves performance over baseline methods. This highlights the effectiveness of ResearchArcade and its potential to advance research progress.

</details>


### [262] [Early Risk Prediction with Temporally and Contextually Grounded Clinical Language Processing](https://arxiv.org/abs/2511.22038)
*Rochana Chaturvedi,Yue Zhou,Andrew Boyd,Brian T. Layden,Mudassir Rashid,Lu Cheng,Ali Cinar,Barbara Di Eugenio*

Main category: cs.CL

TL;DR: 本文提出了两种结合时间和语境信息的方法，用于从电子健康记录的临床笔记中预测慢性病风险，并在2型糖尿病筛查任务中取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 从EHR临床笔记中挖掘丰富的时序和语义信息对疾病预测极具潜力，但这些数据面临文本冗长、事件分布不规律、依赖复杂、多隐私等NLP挑战。现有方法难以精准建模这些细节。

Method: 1）HiTGNN：层次化时序图神经网络，融合单次笔记内部事件结构、跨次就诊动态及医学知识，实现细粒度时序建模；2）ReVeAL：轻量级推理框架，通过大模型推理蒸馏训练较小的验证模型，提升推理能力并减少对大模型依赖。

Result: 在基于公共及私有医院数据集的2型糖尿病筛查中，HiTGNN在预测准确率和对近期高危个体检测方面表现最好，同时保障隐私；ReVeAL提升了灵敏度且保留了推理可解释性。消融实验验证了时序结构和知识增强的价值；公平性分析表明HiTGNN在不同群体之间表现更均衡。

Conclusion: 结合时序结构和医学知识的新模型方法能有效改善EHR临床笔记的风险预测效果，兼顾准确性、公平性和隐私保护，对慢性病预测应用具有重要意义。

Abstract: Clinical notes in Electronic Health Records (EHRs) capture rich temporal information on events, clinician reasoning, and lifestyle factors often missing from structured data. Leveraging them for predictive modeling can be impactful for timely identification of chronic diseases. However, they present core natural language processing (NLP) challenges: long text, irregular event distribution, complex temporal dependencies, privacy constraints, and resource limitations. We present two complementary methods for temporally and contextually grounded risk prediction from longitudinal notes. First, we introduce HiTGNN, a hierarchical temporal graph neural network that integrates intra-note temporal event structures, inter-visit dynamics, and medical knowledge to model patient trajectories with fine-grained temporal granularity. Second, we propose ReVeAL, a lightweight, test-time framework that distills the reasoning of large language models into smaller verifier models. Applied to opportunistic screening for Type 2 Diabetes (T2D) using temporally realistic cohorts curated from private and public hospital corpora, HiTGNN achieves the highest predictive accuracy, especially for near-term risk, while preserving privacy and limiting reliance on large proprietary models. ReVeAL enhances sensitivity to true T2D cases and retains explanatory reasoning. Our ablations confirm the value of temporal structure and knowledge augmentation, and fairness analysis shows HiTGNN performs more equitably across subgroups.

</details>


### [263] [A Hybrid Theory and Data-driven Approach to Persuasion Detection with Large Language Models](https://arxiv.org/abs/2511.22109)
*Gia Bao Hoang,Keith J Ransom,Rachel Stephens,Carolyn Semmler,Nicolas Fay,Lewis Mitchell*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型（LLM）和心理学特征，共同预测社交媒体环境下说服效果的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统的信念修正心理模型主要关注面对面的交流，难以捕捉社交媒体这样大规模且以文本为主的互动环境。为此，亟需更高效的模型来分析和预测网络环境中的信念变迁。

Method: 作者利用LLM，对已知的心理学实验提取相关特征，结合LLM生成的特征评分，通过随机森林分类模型，预测信息是否能成功改变个体信念。总共测试了八个心理特征。

Result: 结果发现，在所测试的八个特征中，“知识情绪”（epistemic emotion）和“分享意愿”（willingness to share）是最能预测信念变化的关键因素。

Conclusion: 本研究揭示了说服性信息的核心特性，展示了LLM结合心理理论可提升说服预测模型效果，为在线影响力检测、虚假信息防控及叙事效果评估等领域提供理论和方法支持。

Abstract: Traditional psychological models of belief revision focus on face-to-face interactions, but with the rise of social media, more effective models are needed to capture belief revision at scale, in this rich text-based online discourse. Here, we use a hybrid approach, utilizing large language models (LLMs) to develop a model that predicts successful persuasion using features derived from psychological experiments.
  Our approach leverages LLM generated ratings of features previously examined in the literature to build a random forest classification model that predicts whether a message will result in belief change. Of the eight features tested, \textit{epistemic emotion} and \textit{willingness to share} were the top-ranking predictors of belief change in the model. Our findings provide insights into the characteristics of persuasive messages and demonstrate how LLMs can enhance models of successful persuasion based on psychological theory. Given these insights, this work has broader applications in fields such as online influence detection and misinformation mitigation, as well as measuring the effectiveness of online narratives.

</details>


### [264] [Bridging the Modality Gap by Similarity Standardization with Pseudo-Positive Samples](https://arxiv.org/abs/2511.22141)
*Shuhei Yamashita,Daiki Shirafuji,Tatsuhiko Saito*

Main category: cs.CL

TL;DR: 本文提出了一种通过相似度归一化和伪数据构建来解决视觉-语言模型中“模态差距”问题的方法，无需人工标注即可提升跨模态检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在面对包含文本与图像的多模态数据库时，因模态间相似度量纲不同导致检索精度受限，而现有改进依赖人工标注，成本高、推广性弱。

Method: 作者提出首先对每个查询与其配对的文本或图像数据，统计相似度分数的均值和方差。基于这些模态特定的统计量，将所有相似度分数归一化以实现跨模态统一比较。统计量由伪配对构造完成：通过选取与查询余弦相似度最高的文本和图像候选形成伪对。

Result: 在MMQA和WebQA两个多模态问答基准上，结合7种VLM模型，实验表明该方法提升了跨模态检索效果：不同模态下Recall@20分别提升了64%和28%。

Conclusion: 与依赖图像描述生成的E5-V方法相比，本文提出的无监督归一化方法能更有效地消除模态间相似度的差距，显著提高跨模态检索的准确度。

Abstract: Advances in vision-language models (VLMs) have enabled effective cross-modality retrieval. However, when both text and images exist in the database, similarity scores would differ in scale by modality. This phenomenon, known as the modality gap, hinders accurate retrieval. Most existing studies address this issue with manually labeled data, e.g., by fine-tuning VLMs on them. In this work, we propose a similarity standardization approach with pseudo data construction. We first compute the mean and variance of the similarity scores between each query and its paired data in text or image modality. Using these modality-specific statistics, we standardize all similarity scores to compare on a common scale across modalities. These statistics are calculated from pseudo pairs, which are constructed by retrieving the text and image candidates with the highest cosine similarity to each query. We evaluate our method across seven VLMs using two multi-modal QA benchmarks (MMQA and WebQA), where each question requires retrieving either text or image data. Our experimental results show that our method significantly improves retrieval performance, achieving average Recall@20 gains of 64% on MMQA and 28% on WebQA when the query and the target data belong to different modalities. Compared to E5-V, which addresses the modality gap through image captioning, we confirm that our method more effectively bridges the modality gap.

</details>


### [265] [C$^2$DLM: Causal Concept-Guided Diffusion Large Language Models](https://arxiv.org/abs/2511.22146)
*Kairong Han,Nuanqiao Shan,Ziyu Zhao,Zijing Hu,Xinpeng Dong,Junjian Ye,Lujia Pan,Fei Wu,Kun Kuang*

Main category: cs.CL

TL;DR: 本文提出了一种新的因果概念引导扩散语言模型（C^2DLM），显著提升了主流语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 目前两大主流大语言模型范式——自回归（AR）模型和扩散语言模型（DLM）都存在推理能力不足的问题。由于人类推理依赖因果知识，而现有范式很难有效建模因果关系，因此有必要设计新框架。

Method: 针对上述问题，作者在DLM基础上，提出用教师模型获得概念级因果图，然后显式引导注意力机制学习概念间的因果关系，提升模型对因果结构的理解和推理。

Result: 在COT-OrderPerturb任务中，C^2DLM获得了12%的性能提升，训练速度提高约3.2倍；在六项推理下游任务中平均提升1.31%。

Conclusion: 引入因果概念指导注意力机制，能够增强语言模型的推理力量，同时带来训练效率大幅提升，为提升LLM推理性能提供了新思路。

Abstract: Autoregressive (AR) language models and Diffusion Language Models (DLMs) constitute the two principal paradigms of large language models. However, both paradigms suffer from insufficient reasoning capabilities. Human reasoning inherently relies on causal knowledge and thought, which are reflected in natural language. But in the AR paradigm, language is modeled as next token prediction (a strictly left-to-right, token-by-token order), whereas natural language itself exhibits more flexible causal structures. In the DLM paradigm, the attention mechanism is fully connected, which entirely disregards causal order. To fill this gap, we propose a \underline{\textbf{C}}ausal \underline{\textbf{C}}oncept-Guided \underline{\textbf{D}}iffusion \underline{\textbf{L}}anguage \underline{\textbf{M}}odel (C$^2$DLM). Starting from DLM's fully connected attention, C$^2$DLM first obtains a concept-level causal graph from the teacher model, and then explicitly guides attention to learn causal relationships between concepts. By focusing on causal relationships and avoiding interference from difficult subgoals involving causal inversion, C$^2$DLM improves 12\% with about 3.2 times training speedup in the COT-OrderPerturb task, and achieves an average gain of 1.31\% across six downstream reasoning tasks. More details in the repository ~\href{https://github.com/Kairong-Han/C-2-DLM}{here}.

</details>


### [266] [A Theoretically Grounded Hybrid Ensemble for Reliable Detection of LLM-Generated Text](https://arxiv.org/abs/2511.22153)
*Sepyan Purnama Kristanto,Lutfi Hakim*

Main category: cs.CL

TL;DR: 本文提出了一种混合集成检测器，有效提升了区分LLM生成文本与人类写作文本的准确性，显著降低了学术文本中的误报率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的普及，机器生成文本与人类文本难以区分，对学术诚信和信息可靠性构成威胁。现有检测方法泛化性差，误报率高，尤其在学术文本上的表现不佳。

Method: 作者提出系统融合三种检测范式的混合集成方法：(1) 基于RoBERTa的深语义特征分类器；(2) 利用GPT-2模型扰动概率检测方法；(3) 基于统计语言特征的文体分析器。通过概率单纯形上学习最优加权投票策略，而非人工设定权重，从理论和实践角度优化集成效果。

Result: 在包含30,000篇文档的大规模多生成器数据集上，系统检测准确率达94.2%，AUC为0.978，学术文本误报率相对降低35%。模型间相关系数低（ρ约0.35-0.42），有助于方差降低。

Conclusion: 该方法相比传统检测器在准确率和误报率方面表现更优，更适于学术等高风险真实应用场景，提高了实际部署的可靠性与伦理性。

Abstract: The rapid proliferation of Large Language Models (LLMs) has blurred the line between human and machine authorship, creating practical risks for academic integrity and information reliability. Existing text detectors typically rely on a single methodological paradigm and suffer from poor generalization and high false positive rates (FPR), especially on high-stakes academic text. We propose a theoretically grounded hybrid ensemble that systematically fuses three complementary detection paradigms: (i) a RoBERTa-based transformer classifier for deep semantic feature extraction, (ii) a GPT-2-based probabilistic detector using perturbation-induced likelihood curvature, and (iii) a statistical linguistic feature analyzer capturing stylometric patterns. The core novelty lies in an optimized weighted voting framework, where ensemble weights are learned on the probability simplex to maximize F1-score rather than set heuristically. We provide a bias-variance analysis and empirically demonstrate low inter-model correlation (rho ~ 0.35-0.42), a key condition for variance reduction. Evaluated on a large-scale, multigenerator corpus of 30,000 documents, our system achieves 94.2% accuracy and an AUC of 0.978, with a 35% relative reduction in false positives on academic text. This yields a more reliable and ethically responsible detector for real-world deployment in education and other high-stakes domains.

</details>


### [267] [RefineBench: Evaluating Refinement Capability of Language Models via Checklists](https://arxiv.org/abs/2511.22173)
*Young-Jun Lee,Seungone Kim,Byung-Kwan Lee,Minkyeong Moon,Yechan Hwang,Jong Myoung Kim,Graham Neubig,Sean Welleck,Ho-Jin Choi*

Main category: cs.CL

TL;DR: 本文提出了RefineBench数据集，用于评测当前语言模型自我改进（self-refinement）和带引导下改进（guided-refinement）能力。实验发现最新的顶级大模型在无引导下自我改进能力有限，但在有针对性反馈的引导下可以迅速显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 目前语言模型在实际应用中经常需要对用户反馈做出改进，但已有研究多集中于可验证任务，缺乏对开放式问题和不同反馈形式的系统评测。因此需开发新基准并全面分析模型在不同情境下的自我改进能力。

Method: 作者构建了RefineBench，包括1000个跨11个领域的具有挑战性问题，并设计了评测框架。比较了两种模式：（1）引导下的改进，模型收到自然语言反馈；（2）自我改进，模型依靠自身尝试优化。对前沿商用和开源大模型进行实验评测。

Result: 自我改进模式下，Gemini 2.5 Pro和GPT-5仅分别获得31.3%和29.1%的基线分数，且几乎没有连贯提升；部分模型甚至表现退步。而在外部引导下，不论闭源还是大规模开源模型都可显著提升答复质量，最多几轮就能达到几乎完美的水平。

Conclusion: 现有顶级大模型依赖外部反馈才能高效改进自身错误，纯自我反思机制能力较弱。RefineBench基准可作为未来自我改进能力突破的重要评测工具。

Abstract: Can language models (LMs) self-refine their own responses? This question is increasingly relevant as a wide range of real-world user interactions involve refinement requests. However, prior studies have largely tested LMs' refinement abilities on verifiable tasks such as competition math or symbolic reasoning with simplified scaffolds, whereas users often pose open-ended queries and provide varying degrees of feedback on what they desire. The recent advent of reasoning models that exhibit self-reflection patterns in their chains-of-thought further motivates this question. To analyze this, we introduce RefineBench, a benchmark of 1,000 challenging problems across 11 domains paired with a checklist-based evaluation framework. We evaluate two refinement modes: (1) guided refinement, where an LM is provided natural language feedback, and (2) self-refinement, where LMs attempt to improve without guidance. In the self-refinement setting, even frontier LMs such as Gemini 2.5 Pro and GPT-5 achieve modest baseline scores of 31.3% and 29.1%, respectively, and most models fail to consistently improve across iterations (e.g., Gemini-2.5-Pro gains only +1.8%, while DeepSeek-R1 declines by -0.1%). By contrast, in guided refinement, both proprietary LMs and large open-weight LMs (>70B) can leverage targeted feedback to refine responses to near-perfect levels within five turns. These findings suggest that frontier LMs require breakthroughs to self-refine their incorrect responses, and that RefineBench provides a valuable testbed for tracking progress.

</details>


### [268] [Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information](https://arxiv.org/abs/2511.22176)
*Lukas Struppek,Dominik Hintersdorf,Hannah Struppek,Daniel Neider,Kristian Kersting*

Main category: cs.CL

TL;DR: 本文提出了一种训练无关、以输入为中心的方法F-CoT，有效减少大语言模型推理所需的token数量，同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型通过生成详尽的思维链条获得很强的推理能力，但会导致推理过程冗长、延迟高。已有的效率提升方法多基于模型训练干预，难以避免冗余。作者希望通过输入结构化的方法，减少无关信息，提高推理效率。

Method: 借鉴认知心理学，作者提出Focused Chain-of-Thought（F-CoT）方法：首先将问题中的关键信息提取、整理为简明的结构化上下文，随后模型仅在这个上下文内进行推理，避免注意到无关细节，从而缩短推理链路。

Result: 在算术文字题数据集上，F-CoT方法使生成token数量减少2-3倍，而推理准确率与标准的“零样本思维链(CoT)”方法相当。

Conclusion: 结构化输入是一种简单有效的提升大型语言模型推理效率的手段，无需额外训练即可实现更少token消耗和较快推理速度。

Abstract: Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, we propose a training-free, input-centric approach. Inspired by cognitive psychology, we introduce Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2-3x while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning.

</details>


### [269] [Beyond Query-Level Comparison: Fine-Grained Reinforcement Learning for Text-to-SQL with Automated Interpretable Critiques](https://arxiv.org/abs/2511.22258)
*Guifeng Wang,Yuanfeng Song,Meng Yang,Tao Zhu,Xiaoming Yin,Xing Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的自动评价和奖励机制，用于改进Text-to-SQL模型的训练和评估方式，无需人工标注即可实现细粒度和具有可解释性的自动判分。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL的评估方法高度依赖人工金标SQL，不仅昂贵且难以扩展。同时，主流的强化学习方法仅用二元执行结果作为奖励信号，指导性有限，难以发现结构和语义的细致错误。

Method: 提出RuCo-C生成式判分模型，自动生成与具体查询相关的评测标准（rubrics），并给出可解释化评语。该框架将细致的奖励反馈融入RL训练，通过"逐步探索"策略不断调整奖励，有效提升模型性能。

Result: 实验显示，RuCo-C在Text-to-SQL任务上的评估和性能提升方面均优于现有方法。

Conclusion: RuCo-C为Text-to-SQL任务提供了无需人工、细粒度、可解释的评价机制，并能有效提升模型性能，具有重要实践价值。

Abstract: Text-to-SQL, a pivotal natural language processing (NLP) task that converts textual queries into executable SQL, has seen substantial progress in recent years. However, existing evaluation and reward mechanisms used to train and assess the text-to-SQL models remain a critical bottleneck. Current approaches heavily rely on manually annotated gold SQL queries, which are costly to produce and impractical for large-scale evaluation. More importantly, most reinforcement learning (RL) methods in text-to-SQL leverage only the final binary execution outcome as the reward signal, a coarse-grained supervision that overlooks detailed structural and semantic errors from the perspective of rubrics. To address these challenges, we propose RuCo-C, a novel generative judge model for fine-grained, query-specific automatic evaluation using interpretable critiques without human intervention. Our framework first automatically generates query-specific evaluation rubrics for human-free annotation, linking them to interpretable critiques. Subsequently, it integrates densified reward feedback through a "progressive exploration" strategy during the RL training process, which dynamically adjusts the rewards to enhance the model's performance. Comprehensive experiments demonstrate that RuCo-C outperforms existing methods in text-to-SQL evaluation, yielding significant performance gains.

</details>


### [270] [Token-Level Marginalization for Multi-Label LLM Classifiers](https://arxiv.org/abs/2511.22312)
*Anjaneya Praharaj,Jaykumar Kasundra*

Main category: cs.CL

TL;DR: 本文提出了三种新的基于token概率估算的方法，用于提升大语言模型（LLMs）在多标签内容安全分类任务中的置信度解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前生成型语言模型如LLaMA Guard虽然在检测不安全内容方面有效，但其缺乏直接的类别级概率输出，导致模型置信度难以评估，内容审核阈值难以动态设定，且细致错误分析受到阻碍。

Method: 提出并评估了三种基于token级概率的估算方法，通过分析模型输出logits来推断分类置信度；并在不同指令微调模型上进行泛化评估。

Result: 在合成且严格标注的数据集上进行大量实验表明，利用token logits可以显著提升生成式分类器的可解释性和可靠性。

Conclusion: 新方法改善了生成式内容安全分类系统的置信度可解释性与细粒度审核能力，对内容审核实践具有较强应用价值。

Abstract: This paper addresses the critical challenge of deriving interpretable confidence scores from generative language models (LLMs) when applied to multi-label content safety classification. While models like LLaMA Guard are effective for identifying unsafe content and its categories, their generative architecture inherently lacks direct class-level probabilities, which hinders model confidence assessment and performance interpretation. This limitation complicates the setting of dynamic thresholds for content moderation and impedes fine-grained error analysis. This research proposes and evaluates three novel token-level probability estimation approaches to bridge this gap. The aim is to enhance model interpretability and accuracy, and evaluate the generalizability of this framework across different instruction-tuned models. Through extensive experimentation on a synthetically generated, rigorously annotated dataset, it is demonstrated that leveraging token logits significantly improves the interpretability and reliability of generative classifiers, enabling more nuanced content safety moderation.

</details>


### [271] [Sentiment Analysis Of Shopee Product Reviews Using Distilbert](https://arxiv.org/abs/2511.22313)
*Zahri Aksa Dautd,Aviv Yuniar Rahman*

Main category: cs.CL

TL;DR: 本论文研究如何通过DistilBERT对Shopee产品评论进行情感分析，以高效率实现大规模评论分类。


<details>
  <summary>Details</summary>
Motivation: 随着数字商务的快速发展，电商平台累积了大量消费者评论，人工分析低效，急需自动化、准确且高效的情感分析方法。

Method: 采用DistilBERT（一种轻量级Transformer深度学习模型）对约100万条英文Shopee评论进行预处理后训练，并与BERT、SVM模型进行性能对比。评估指标包括准确率、精确率、召回率和F1分数。

Result: DistilBERT的准确率为94.8%，略低于BERT（95.3%），但远高于SVM（90.2%），计算时间减少超55%。

Conclusion: DistilBERT在准确性和计算效率之间取得了最佳平衡，非常适合用于电商平台大规模评论的情感分析场景。

Abstract: The rapid growth of digital commerce has led to the accumulation of a massive number of consumer reviews on online platforms. Shopee, as one of the largest e-commerce platforms in Southeast Asia, receives millions of product reviews every day containing valuable information regarding customer satisfaction and preferences. Manual analysis of these reviews is inefficient, thus requiring a computational approach such as sentiment analysis. This study examines the use of DistilBERT, a lightweight transformer-based deep learning model, for sentiment classification on Shopee product reviews. The dataset used consists of approximately one million English-language reviews that have been preprocessed and trained using the distilbert-base-uncased model. Evaluation was conducted using accuracy, precision, recall, and F1-score metrics, and compared against benchmark models such as BERT and SVM. The results show that DistilBERT achieved an accuracy of 94.8%, slightly below BERT (95.3%) but significantly higher than SVM (90.2%), with computation time reduced by more than 55%. These findings demonstrate that DistilBERT provides an optimal balance between accuracy and efficiency, making it suitable for large scale sentiment analysis on e-commerce platforms. Keywords: Sentiment Analysis, DistilBERT, Shopee Reviews, Natural Language Processing, Deep Learning, Transformer Models.

</details>


### [272] [Named Entity Recognition for the Kurdish Sorani Language: Dataset Creation and Comparative Analysis](https://arxiv.org/abs/2511.22315)
*Bakhtawar Abdalla,Rebwar Mala Nabi,Hassan Eshkiki,Fabio Caraffini*

Main category: cs.CL

TL;DR: 本文提出了首个库尔德语索拉尼方言的命名实体识别数据集（64,563个标注词元），并进行了经典机器学习与神经网络模型的对比分析。结果表明：在低资源环境中，传统方法（如CRF）比神经网络（如BiLSTM）效果更佳。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理领域，低资源语言（如库尔德语索拉尼方言）缺乏高质量数据集与有效工具，限制了技术的普及和适用范围。本文旨在弥补这一空白，推动更多低资源语言的研究与应用。

Method: 作者构建了库尔德语索拉尼方言的命名实体识别（NER）数据集，开发支持多语言NER的工具，并使用经典机器学习模型（如CRF）与神经网络模型（如BiLSTM）进行了性能对比实验。

Result: CRF等传统模型取得了0.825的F1分数，显著优于BiLSTM等神经网络模型（0.706），挑战了神经网络在所有场景下都优于传统方法的假设。

Conclusion: 在低资源语言设置下，简单、高效的传统机器学习方法可能优于复杂的神经网络架构。因此，低资源条件下应优先考虑传统方法。

Abstract: This work contributes towards balancing the inclusivity and global applicability of natural language processing techniques by proposing the first 'name entity recognition' dataset for Kurdish Sorani, a low-resource and under-represented language, that consists of 64,563 annotated tokens. It also provides a tool for facilitating this task in this and many other languages and performs a thorough comparative analysis, including classic machine learning models and neural systems. The results obtained challenge established assumptions about the advantage of neural approaches within the context of NLP. Conventional methods, in particular CRF, obtain F1-scores of 0.825, outperforming the results of BiLSTM-based models (0.706) significantly. These findings indicate that simpler and more computationally efficient classical frameworks can outperform neural architectures in low-resource settings.

</details>


### [273] [Mapping Clinical Doubt: Locating Linguistic Uncertainty in LLMs](https://arxiv.org/abs/2511.22402)
*Srivarshinee Sridhar,Raghav Kaushik Ravi,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: 该论文探讨了大语言模型（LLMs）在医学文本中对不确定性语言信号的内部表示方式，提出了一种分层探测新指标并发现不确定性信息在模型深层逐步编码。


<details>
  <summary>Details</summary>
Motivation: 在临床环境下，语言中的不确定性对诊断解释和决策有重要影响。然而，大语言模型是如何在内部表征这些'证据不确定性'的尚不清楚。理解这一点有助于提升模型的可信度和可解释性。

Method: 作者构建了带有不同不确定性表达（如'is consistent with'和'may be consistent with'）的对比数据集，并提出了一种名为“Model Sensitivity to Uncertainty（MSU）”的分层探针指标，用以度量模型相应层激活因子对不确定性表达的敏感度。

Result: 实验发现，LLMs对医学文本中的不确定性信号表现出结构化、依赖深度的敏感性——即不确定性信息从浅层到深层逐渐被模型捕捉并编码。

Conclusion: LLMs对临床语句中的不确定性具有内部分层响应，这有助于理解其可解释性和对专业领域知识的不确定性表现的可靠性。

Abstract: Large Language Models (LLMs) are increasingly used in clinical settings, where sensitivity to linguistic uncertainty can influence diagnostic interpretation and decision-making. Yet little is known about where such epistemic cues are internally represented within these models. Distinct from uncertainty quantification, which measures output confidence, this work examines input-side representational sensitivity to linguistic uncertainty in medical text. We curate a contrastive dataset of clinical statements varying in epistemic modality (e.g., 'is consistent with' vs. 'may be consistent with') and propose Model Sensitivity to Uncertainty (MSU), a layerwise probing metric that quantifies activation-level shifts induced by uncertainty cues. Our results show that LLMs exhibit structured, depth-dependent sensitivity to clinical uncertainty, suggesting that epistemic information is progressively encoded in deeper layers. These findings reveal how linguistic uncertainty is internally represented in LLMs, offering insight into their interpretability and epistemic reliability.

</details>


### [274] [Exploring Performance Variations in Finetuned Translators of Ultra-Low Resource Languages: Do Linguistic Differences Matter?](https://arxiv.org/abs/2511.22482)
*Isabel Gonçalves,Paulo Cavalin,Claudio Pinhanez*

Main category: cs.CL

TL;DR: 本文系统性分析了利用少量数据微调预训练语言模型为濒危低资源语言（如巴西土著语言）创建翻译器时，不同工作之间性能差异的原因。


<details>
  <summary>Details</summary>
Motivation: 尽管采用类似的数据和微调方法，前人利用预训练模型为超低资源语言开发翻译器时，报告了显著不同的性能结果。本研究动机在于深入分析影响这些性能差异的实际因素，提升低资源语言翻译器开发的可控性和可解释性。

Method: 作者针对两种结构上相关但有重要语言特征差异的巴西土著语言，系统考察了数据清洗方式、预训练模型本身的限制、基础模型规模、训练数据规模等多个变量，并分别分析双向翻译效果。

Result: 实验发现，数据清洗、模型或数据规模等训练变量对译文性能影响极小或有限。

Conclusion: 结果表明，微调预训练模型为超低资源语言构建翻译器的可行性，很大程度上取决于具体语言的结构差异而非训练或模型本身的常规参数。这提示未来研究需更加关注语言本身的特性。

Abstract: Finetuning pre-trained language models with small amounts of data is a commonly-used method to create translators for ultra-low resource languages such as endangered Indigenous languages. However, previous works have reported substantially different performances with translators created using similar methodology and data. In this work we systematically explored possible causes of the performance difference, aiming to determine whether it was a product of different cleaning procedures, limitations of the pre-trained models, the size of the base model, or the size of the training dataset, studying both directions of translation. Our studies, using two Brazilian Indigenous languages, related but with significant structural linguistic characteristics, indicated none or very limited influence from those training factors, suggesting differences between languages may play a significant role in the ability to produce translators by fine-tuning pre-trained models.

</details>


### [275] [Joint Speech and Text Training for LLM-Based End-to-End Spoken Dialogue State Tracking](https://arxiv.org/abs/2511.22503)
*Katia Vendrame,Bolaji Yusuf,Santosh Kesiraju,Šimon Sedláček,Oldřich Plchot,Jan Černocký*

Main category: cs.CL

TL;DR: 该论文提出联合训练口语和文本数据的方法，以解决语音对话状态跟踪（DST）领域中的跨领域泛化难题，无需为每个领域收集昂贵的语音标注数据即可提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有端到端语音对话状态跟踪模型虽然结合了语音基础编码器和大语言模型，取得了很好的结果，但它们在跨领域泛化方面表现不佳，且需要大量每个领域的带标注语音DST数据，这在实际中难以获取。

Method: 作者提出利用现有的少量口语DST数据与其他领域较易获得的文本DST数据联合训练模型。通过这种方式，不依赖目标领域的语音训练数据，也能提升模型的跨领域对话状态跟踪能力。

Result: 实验证明，该方法在无需目标领域语音数据的情况下，依然能在跨领域DST任务上取得良好表现。

Conclusion: 联合口语与文本数据训练能够有效缓解数据稀缺和泛化性差的问题，为语音对话系统在实际多领域部署提供了重要支持。

Abstract: End-to-end spoken dialogue state tracking (DST) is made difficult by the tandem of having to handle speech input and data scarcity. Combining speech foundation encoders and large language models has been proposed in recent work as to alleviate some of this difficulty. Although this approach has been shown to result in strong spoken DST models, achieving state-of-the-art performance in realistic multi-turn DST, it struggles to generalize across domains and requires annotated spoken DST training data for each domain of interest. However, collecting such data for every target domain is both costly and difficult. Noting that textual DST data is more easily obtained for various domains, in this work, we propose jointly training on available spoken DST data and written textual data from other domains as a way to achieve cross-domain generalization. We conduct experiments which show the efficacy of our proposed method for getting good cross-domain DST performance without relying on spoken training data from the target domains.

</details>


### [276] [Extension Condition "violations" and Merge optimality constraints](https://arxiv.org/abs/2511.22582)
*Matilde Marcolli,Richard Larson,Riny Huijbregts*

Main category: cs.CL

TL;DR: 本文在强极小主义理论框架下，利用Merge的数学形式化，分析了一系列语言现象，提出这些通常被认为违反Extension Condition（EC）的结构其实可以不违背EC地解释，并揭示了SM与最优化原则之间的关系。


<details>
  <summary>Details</summary>
Motivation: 许多语言现象（如head-to-head movement、短语词缀、语法附着、动词短语交替及操作符-变量关系）常被认为违反了Extension Condition，这在理论语言学中的结构生成理论框架下是一个难题，作者旨在澄清这些现象是否真的违反了EC，并以强化极小理论为动力予以重新解释。

Method: 作者采用强极小主义假说下Merge的数学模型，对上述语言现象进行严格形式化分析，特别考察了Sideward Merge、 Markov链及Hopf代数等工具，并引入最优性违反和资源限制损失函数来讨论这些结构的派生过程。还分析了多重wh前置、浪漫语族中的附着词集群和韩语中的所有格一致性结构，探讨了这些分析与颜色operad生成子的兼容性。

Result: 作者发现，上述多种语言现象皆可在不违反EC的情况下解释。其中除了head-to-head movement需用到带有极小最优性违反的Sideward Merge外，其他皆能找到完全不依赖EC或SM的替代理解。同时，EC 在Merge的代数结构中有清晰的数学意义，并不是建模外加的假设。还说明了最小最优性违反的SM对Merge的马尔可夫性质起结构性作用，不同的最优性准则（如最小搜索与资源限制）对Hopf代数马尔可夫链的动态有不同影响。

Conclusion: 理论上可以保留EC为内在代数约束，上述被认为是'例外'的语言现象都能予以合理解释，极小最优性违反的Sideward Merge仅在极少数情况下必要。数学分析进一步深化了Merge与极小主义理论的结合，为相关结构提供了更一般性的解释，也展示了抽象代数与语言结构理论之间的深度联系。

Abstract: We analyze, using the mathematical formulation of Merge within the Strong Minimalist Thesis framework, a set of linguistic phenomena, including head-to-head movement, phrasal affixes and syntactic cliticization, verb-particle alternation, and operator-variable phenomena. These are often regarded as problematic, as violations of the Extension Condition. We show that, in fact, all of these phenomena can be explained without involving any EC violation. We first show that derivations using Sideward Merge are possible for all of these cases: these respect EC, though they involve some amount of optimality violations, with respect to Resource Restrictions cost functions, andthe amount of violation differs among these cases. We show that all the cases that involve large optimality violations can be derived in alternative ways involving neither EC nor the use of SM. The main remaining case (head-to-head movement) only involves SM with minimal violations of optimality (near equilibrium fluctuations). We analyze explicitly also the cases of multiple wh-fronting, clusters of clitics in Romance languages and possessor agreement construction in Korean, and how an explanation of these phenomena based on SM can be made compatible with the colored operad generators for phases and theta roles. We also show that the EC condition has a clear algebraic meaning in the mathematical formulation of Merge and is therefore an intrinsic structural algebraic constraint of the model, rather than an additional assumption. We also show that the minimal optimality violating SM plays a structural role in the Markovian properties of Merge, and we compare different optimality conditions coming from Minimal Search and from Resource Restriction in terms of their effect on the dynamics of the Hopf algebra Markov chain, in a simple explicit example.

</details>


### [277] [Smarter, not Bigger: Fine-Tuned RAG-Enhanced LLMs for Automotive HIL Testing](https://arxiv.org/abs/2511.22584)
*Chao Feng,Zihan Liu,Siddhant Gupta,Gongpei Cui,Jan von der Assen,Burkhard Stiller*

Main category: cs.CL

TL;DR: 本文提出了HIL-GPT系统，通过结合大语言模型（LLM）与语义检索，提升汽车硬件在环（HIL）测试的自动化与效率。


<details>
  <summary>Details</summary>
Motivation: 传统HIL测试中测试资源和数据分散、利用率低，影响测试流程的效率和效果。因此需要新的方法来集成和高效利用分散的测试用例与需求文档。

Method: 提出了HIL-GPT检索增强生成（RAG）系统。首先，利用启发式挖掘和LLM辅助生成方法，构建了领域特定数据集，进行嵌入微调。然后结合向量索引，实现可扩展且可追踪的测试用例与需求检索。

Result: 实验证明，经过微调的小型模型（如bge-base-en-v1.5）在准确率、延迟和成本之间取得了更佳平衡，效果超过更大模型。同时，A/B测试表明RAG助手在有用性、真实性和满意度方面优于通用LLM。

Conclusion: HIL-GPT为工业HIL环境中高效、领域贴合的LLM部署提供了新思路，表明中小型定制模型具备优越性能，促进实际应用落地。

Abstract: Hardware-in-the-Loop (HIL) testing is essential for automotive validation but suffers from fragmented and underutilized test artifacts. This paper presents HIL-GPT, a retrieval-augmented generation (RAG) system integrating domain-adapted large language models (LLMs) with semantic retrieval. HIL-GPT leverages embedding fine-tuning using a domain-specific dataset constructed via heuristic mining and LLM-assisted synthesis, combined with vector indexing for scalable, traceable test case and requirement retrieval. Experiments show that fine-tuned compact models, such as \texttt{bge-base-en-v1.5}, achieve a superior trade-off between accuracy, latency, and cost compared to larger models, challenging the notion that bigger is always better. An A/B user study further confirms that RAG-enhanced assistants improve perceived helpfulness, truthfulness, and satisfaction over general-purpose LLMs. These findings provide insights for deploying efficient, domain-aligned LLM-based assistants in industrial HIL environments.

</details>


### [278] [Improving LLM-based Ontology Matching with fine-tuning on synthetic data](https://arxiv.org/abs/2511.22612)
*Guilherme Sousa,Rinaldo Lima,Cassia Trojahn*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLM）在本体匹配中的直接应用，并提出通过自动生成本体子模块数据集进行微调，有效提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM被用于本体匹配流程的部分环节，但其端到端进行本体匹配、缺乏足够参考对齐数据进行有效微调，是限制其应用的关键问题。作者希望通过自动合成训练数据集和微调策略克服数据稀缺问题，提升LLM在本体匹配任务中的表现。

Method: 提出了一种基于LLM的本体匹配方法：首先通过搜索空间缩减选择本体相关子集并自动构造用于LLM的输入prompt；其次利用LLM自动生成大量本体子模块及配套参考对齐，构成合成训练数据集；最后用该数据集对LLM进行微调，再评估其在多个OAEI数据集上的匹配表现。

Result: 在Conference、Geolink、Enslaved、Taxon和Hydrography等数据集上的实验表明，利用合成数据集微调后的LLM在本体匹配任务中显著优于未经微调的基础模型。

Conclusion: 自动数据集生成结合微调可以有效适配和提升LLM在本体匹配中的能力，为解决训练数据稀缺问题和拓展LLM实际应用提供了新方案。

Abstract: Large Language Models (LLMs) are increasingly being integrated into various components of Ontology Matching pipelines. This paper investigates the capability of LLMs to perform ontology matching directly on ontology modules and generate the corresponding alignments. Furthermore, it is explored how a dedicated fine-tuning strategy can enhance the model's matching performance in a zero-shot setting. The proposed method incorporates a search space reduction technique to select relevant subsets from both source and target ontologies, which are then used to automatically construct prompts. Recognizing the scarcity of reference alignments for training, a novel LLM-based approach is introduced for generating a synthetic dataset. This process creates a corpus of ontology submodule pairs and their corresponding reference alignments, specifically designed to fine-tune an LLM for the ontology matching task. The proposed approach was evaluated on the Conference, Geolink, Enslaved, Taxon, and Hydrography datasets from the OAEI complex track. The results demonstrate that the LLM fine-tuned on the synthetically generated data exhibits superior performance compared to the non-fine-tuned base model. The key contribution is a strategy that combines automatic dataset generation with fine-tuning to effectively adapt LLMs for ontology matching tasks.

</details>


### [279] [Modeling Romanized Hindi and Bengali: Dataset Creation and Multilingual LLM Integration](https://arxiv.org/abs/2511.22769)
*Kanchon Gharami,Quazi Sarwar Muhtaseem,Deepti Gupta,Lavanya Elluri,Shafika Showkat Moni*

Main category: cs.CL

TL;DR: 本文提出并公开了大规模印-欧语系印度-雅利安语（印地语与孟加拉语）的罗马化-原生文翻译数据集，并基于该数据集预训练了定制的多语种序列到序列大模型，显著提升了音译准确性。


<details>
  <summary>Details</summary>
Motivation: 当前多语种大模型在处理印地语、孟加拉语等语言的罗马化文本时效果不足，原因在于相关高质量音译数据集稀缺，对发音和拼写变化覆盖有限，且缺乏与主流大模型适配的数据。

Method: 1. 构建包含180万印地语和100万孟加拉语的罗马化-原生文音译对。2. 基于数据集，对Marian结构的多语言序列到序列大模型进行预训练。3. 在BLEU和CER等指标上评测模型表现，并与已有方法对比。

Result: 基于新数据集和预训练方法，模型在BLEU和CER等指标上获得了优于现有相关模型的结果。

Conclusion: 本文构建的大数据集和预训练大模型显著提升了罗马化印地语与孟加拉语的音译效果，并为少资源语种音译与多语种NLP任务提供了有力工具。

Abstract: The development of robust transliteration techniques to enhance the effectiveness of transforming Romanized scripts into native scripts is crucial for Natural Language Processing tasks, including sentiment analysis, speech recognition, information retrieval, and intelligent personal assistants. Despite significant advancements, state-of-the-art multilingual models still face challenges in handling Romanized script, where the Roman alphabet is adopted to represent the phonetic structure of diverse languages. Within the South Asian context, where the use of Romanized script for Indo-Aryan languages is widespread across social media and digital communication platforms, such usage continues to pose significant challenges for cutting-edge multilingual models. While a limited number of transliteration datasets and models are available for Indo-Aryan languages, they generally lack sufficient diversity in pronunciation and spelling variations, adequate code-mixed data for large language model (LLM) training, and low-resource adaptation. To address this research gap, we introduce a novel transliteration dataset for two popular Indo-Aryan languages, Hindi and Bengali, which are ranked as the 3rd and 7th most spoken languages worldwide. Our dataset comprises nearly 1.8 million Hindi and 1 million Bengali transliteration pairs. In addition to that, we pre-train a custom multilingual seq2seq LLM based on Marian architecture using the developed dataset. Experimental results demonstrate significant improvements compared to existing relevant models in terms of BLEU and CER metrics.

</details>


### [280] [Mitigating Semantic Drift: Evaluating LLMs' Efficacy in Psychotherapy through MI Dialogue Summarization](https://arxiv.org/abs/2511.22818)
*Vivek Kumar,Pushpraj Singh Rajawat,Eirini Ntoutsi*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在心理治疗领域，尤其是在动机性访谈（MI）对话中的表现，通过专家标注数据集和多阶段标注体系，探讨其能否准确理解与表达复杂心理学概念，并提出减轻模型语义漂移的实践方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各领域展现潜力，但在心理学等低资源与敏感领域中，模型易出现事实错误、同理性表达不一致、偏见、幻觉等问题，缺乏对复杂人类心理的深入理解。因此，亟需评估和提高LLM在心理治疗场景的适用性和可靠性。

Method: 文章采用混合方法，利用LLM对动机性访谈对话生成精确摘要，并基于MITI标准设计双阶段的标注体系，聚焦召唤、合作、自主、方向、同理心和无偏见态度等关键特征。以专家标注为真值，采用多类分类任务，结合单样本与少样本渐进提示法对LLM进行测试和性能评估。

Result: 研究揭示了LLM在理解与表达复杂心理学构念方面的能力，尤其是在疗法语境下减缓语义漂移方面提出了实践经验和改良建议。

Conclusion: 本研究不仅为动机性访谈社区提供了高质量标注数据集，缓解了低资源领域的数据稀缺问题，也为在复杂行为疗法中精准、情境化运用LLM提供了关键见解和操作规范。

Abstract: Recent advancements in large language models (LLMs) have shown their potential across both general and domain-specific tasks. However, there is a growing concern regarding their lack of sensitivity, factual incorrectness in responses, inconsistent expressions of empathy, bias, hallucinations, and overall inability to capture the depth and complexity of human understanding, especially in low-resource and sensitive domains such as psychology. To address these challenges, our study employs a mixed-methods approach to evaluate the efficacy of LLMs in psychotherapy. We use LLMs to generate precise summaries of motivational interviewing (MI) dialogues and design a two-stage annotation scheme based on key components of the Motivational Interviewing Treatment Integrity (MITI) framework, namely evocation, collaboration, autonomy, direction, empathy, and a non-judgmental attitude. Using expert-annotated MI dialogues as ground truth, we formulate multi-class classification tasks to assess model performance under progressive prompting techniques, incorporating one-shot and few-shot prompting. Our results offer insights into LLMs' capacity for understanding complex psychological constructs and highlight best practices to mitigate ``semantic drift" in therapeutic settings. Our work contributes not only to the MI community by providing a high-quality annotated dataset to address data scarcity in low-resource domains but also critical insights for using LLMs for precise contextual interpretation in complex behavioral therapy.

</details>


### [281] [RAG System for Supporting Japanese Litigation Procedures: Faithful Response Generation Complying with Legal Norms](https://arxiv.org/abs/2511.22858)
*Yuya Ishihara,Atsushi Keyaki,Hiroaki Yamada,Ryutaro Ohara,Mihoko Sumida*

Main category: cs.CL

TL;DR: 本研究探讨了如何构建符合日本医疗诉讼法律规范的基于RAG（检索增强生成）的大语言模型（LLM）系统，并提出了满足法律程序所需的关键技术要素。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，有望以RAG-LLM替代专家在医疗诉讼中的知识提供角色，但必须确保严格遵守法律规定，特别是在知识检索与使用上的合法性。

Method: 论文分析了RAG-LLM在医疗诉讼中的应用场景，提出三大系统要求：1) 检索模块需依合法原则检索外部知识，禁止用非公开知识；2) 生成内容需严格基于且忠实于检索到的上下文信息；3) 检索知识需附有与争议相关的时间戳以保证证据效力。

Result: 初步展示了满足上述法律合规要求的RAG-LLM系统设计思路，并论证了每一要求对于诉讼公正性及合规性的重要性。

Conclusion: 合规的RAG-LLM能为医疗诉讼程序中知识提供自动化，前提是其检索、生成与信息时间管理严格对应法律约束，为AI助力法律诉讼提供了可靠路径。

Abstract: This study discusses the essential components that a Retrieval-Augmented Generation (RAG)-based LLM system should possess in order to support Japanese medical litigation procedures complying with legal norms. In litigation, expert commissioners, such as physicians, architects, accountants, and engineers, provide specialized knowledge to help judges clarify points of dispute. When considering the substitution of these expert roles with a RAG-based LLM system, the constraint of strict adherence to legal norms is imposed. Specifically, three requirements arise: (1) the retrieval module must retrieve appropriate external knowledge relevant to the disputed issues in accordance with the principle prohibiting the use of private knowledge, (2) the responses generated must originate from the context provided by the RAG and remain faithful to that context, and (3) the retrieval module must reference external knowledge with appropriate timestamps corresponding to the issues at hand. This paper discusses the design of a RAG-based LLM system that satisfies these requirements.

</details>


### [282] [JBE-QA: Japanese Bar Exam QA Dataset for Assessing Legal Domain Knowledge](https://arxiv.org/abs/2511.22869)
*Zhihan Cao,Fumihito Nishino,Hiroaki Yamada,Nguyen Ha Thanh,Yusuke Miyao,Ken Satoh*

Main category: cs.CL

TL;DR: 本文提出了JBE-QA，一个日本司法考试问答数据集，用于评估大语言模型在法律领域的知识能力，并对比了26种模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的日本法律领域数据集主要集中在《民法》，缺乏覆盖范围更广、能够全面评估大语言模型法律能力的基准数据集。随着大语言模型在多语言和专业领域应答方面应用不断扩展，亟需构建面向日本司法考试的权威评测集。

Method: 作者基于2015-2024年日本司法考试多项选择题部分，构建了包含《民法》、《刑法》和《宪法》的JBE-QA数据集，并将题目分解为结构化的独立判断（对/错）条目。标签平衡后，总共收录3464条。随后使用包括专有模型、开源模型、日语定制模型和推理模型在内的26个大型语言模型进行评测。

Result: 评测结果显示，启动推理能力的专有大语言模型表现最佳。总体上，针对《宪法》的题目比《民法》或《刑法》的问题更容易。

Conclusion: JBE-QA为日本法律领域大语言模型的评测提供了首个全面的权威数据基准，揭示了不同类型模型在各法律模块中的表现差异，为以后的中文法律考试评测和数据集构建提供了有益的参考。

Abstract: We introduce JBE-QA, a Japanese Bar Exam Question-Answering dataset to evaluate large language models' legal knowledge. Derived from the multiple-choice (tanto-shiki) section of the Japanese bar exam (2015-2024), JBE-QA provides the first comprehensive benchmark for Japanese legal-domain evaluation of LLMs. It covers the Civil Code, the Penal Code, and the Constitution, extending beyond the Civil Code focus of prior Japanese resources. Each question is decomposed into independent true/false judgments with structured contextual fields. The dataset contains 3,464 items with balanced labels. We evaluate 26 LLMs, including proprietary, open-weight, Japanese-specialised, and reasoning models. Our results show that proprietary models with reasoning enabled perform best, and the Constitution questions are generally easier than the Civil Code or the Penal Code questions.

</details>


### [283] [FEANEL: A Benchmark for Fine-Grained Error Analysis in K-12 English Writing](https://arxiv.org/abs/2511.22883)
*Jingheng Ye,Shen Wang,Jiaqi Chen,Hebin Wang,Deqing Zou,Yanyu Zhu,Jiwei Tang,Hai-Tao Zheng,Ruitong Liu,Haoyang Li,Yanfeng Wang,Qingsong Wen*

Main category: cs.CL

TL;DR: 论文提出了面向K-12英语写作的细粒度错误分析新任务，并建立了FEANEL基准，用以评估大语言模型在该任务上的表现。结果发现现有模型在细粒度错误分析上存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在教育领域展现巨大潜力，但其为K-12学生英语写作提供细致反馈的能力尚未充分研究。

Method: 作者构建了FEANEL基准，包含1000份由中小学生撰写的英语作文，结合专家制定的基于词性分类的写作错误体系进行标注，并按类型、严重程度与反馈进行归类。随后，用该基准评估了主流大语言模型的错误分析与教学能力。

Result: 实验表明，当前的大语言模型在细粒度错误识别与分析、诊断等方面均存在显著短板，无法满足高要求的教育应用。

Conclusion: 亟需针对细粒度写作错误分析，研发更适合教育场景的新方法与模型。

Abstract: Large Language Models (LLMs) have transformed artificial intelligence, offering profound opportunities for educational applications. However, their ability to provide fine-grained educational feedback for K-12 English writing remains underexplored. In this paper, we challenge the error analysis and pedagogical skills of LLMs by introducing the problem of Fine-grained Error Analysis for English Learners and present the Fine-grained Error ANalysis for English Learners (FEANEL) Benchmark. The benchmark comprises 1,000 essays written by elementary and secondary school students, and a well-developed English writing error taxonomy. Each error is annotated by language education experts and categorized by type, severity, and explanatory feedback, using a part-of-speech-based taxonomy they co-developed. We evaluate state-of-the-art LLMs on the FEANEL Benchmark to explore their error analysis and pedagogical abilities. Experimental results reveal significant gaps in current LLMs' ability to perform fine-grained error analysis, highlighting the need for advancements in particular methods for educational applications.

</details>


### [284] [Language-conditioned world model improves policy generalization by reading environmental descriptions](https://arxiv.org/abs/2511.22904)
*Anh Nguyen,Stefan Lee*

Main category: cs.CL

TL;DR: 本文提出了一种无需计划和专家演示，通过语言感知世界模型实现策略泛化的新方法，并在多游戏环境下验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 以往方法在通过语言理解环境动态，实现策略泛化方面存在推理延迟大或依赖专家演示等局限。本研究旨在去除这些限制，提升具备语言感知能力的世界模型对未见任务的泛化能力。

Method: 论文提出了一种基于模型的强化学习方法。具体地，构建了语言感知的Dreamer世界模型（LED-WM），其中观测编码器通过注意力机制将语言描述与观测中的实体对齐。策略在无需规划和专家演示的前提下，仅依赖交互学习获得。此外，策略可通过模型生成的合成轨迹进行微调。

Result: 在MESSENGER和MESSENGER-WM两个环境中，与其它基线方法相比，LED-WM训练出的策略在面对描述新动态和新语言的未见游戏时表现出更强的泛化能力。

Conclusion: 本方法有效提升了基于语言的世界模型的策略泛化能力，且无需额外推理开销或专家演示，具有更好的实际应用潜力。

Abstract: To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying "what to do". Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to unseen games or rely on limiting assumptions. For instance, assuming that the latency induced by inference-time planning is tolerable for the target task or expert demonstrations are available. Expanding on this line of research, we focus on improving policy generalization from a language-conditioned world model while dropping these assumptions. We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations. Our method proposes Language-aware Encoder for Dreamer World Model (LED-WM) built on top of DreamerV3. LED-WM features an observation encoder that uses an attention mechanism to explicitly ground language descriptions to entities in the observation. We show that policies trained with LED-WM generalize more effectively to unseen games described by novel dynamics and language compared to other baselines in several settings in two environments: MESSENGER and MESSENGER-WM.To highlight how the policy can leverage the trained world model before real-world deployment, we demonstrate the policy can be improved through fine-tuning on synthetic test trajectories generated by the world model.

</details>


### [285] [Visual Puns from Idioms: An Iterative LLM-T2IM-MLLM Framework](https://arxiv.org/abs/2511.22943)
*Kelaiti Xiao,Liang Yang,Dongyu Zhang,Paerhati Tulajiang,Hongfei Lin*

Main category: cs.CL

TL;DR: 本论文提出了一个自动生成和评估成语型视觉双关图像的迭代系统。该系统联合使用大语言模型、文生图模型和多模态大模型，能自动生成成语相关的视觉双关图像并评测其效果。作者还构建了1000个成语及其双关图像组成的新数据集，并分析了不同模型对任务的表现。


<details>
  <summary>Details</summary>
Motivation: 成语型视觉双关图像兼具成语的字面和隐喻意涵，手工设计耗时且难以规模化。现有AI系统中，自动生成和理解这类复杂的多模态表达是一个挑战，因此需要系统性方案推动该领域的发展。

Method: 论文提出一个包含大语言模型、文生图模型和多模态大模型联合工作的迭代框架。系统输入成语，通过不断生成和改进视觉描述，迭代生成图像、用MLLM自动识别并判断是否达成目标，直至识别成功或达到步骤上限。用1000个成语构建了一个成语视觉双关数据集。

Result: 在10个LLM、10个MLLM和一个文生图模型上实验，发现MLLM表现是理解和识别的主要瓶颈。GPT家族MLLM表现最好，Gemini次之，开源的Gemma也有一定竞争力。LLM部分，Claude在生成视觉提示词方面效果最佳。

Conclusion: 多模态模型的选择对成语视觉双关的自动生成与评测至关重要。作者的迭代框架和新提出的数据集为理解和推动视觉-语言深度结合任务提供了新基线和实验平台。

Abstract: We study idiom-based visual puns--images that align an idiom's literal and figurative meanings--and present an iterative framework that coordinates a large language model (LLM), a text-to-image model (T2IM), and a multimodal LLM (MLLM) for automatic generation and evaluation. Given an idiom, the system iteratively (i) generates detailed visual prompts, (ii) synthesizes an image, (iii) infers the idiom from the image, and (iv) refines the prompt until recognition succeeds or a step limit is reached. Using 1,000 idioms as inputs, we synthesize a corresponding dataset of visual pun images with paired prompts, enabling benchmarking of both generation and understanding. Experiments across 10 LLMs, 10 MLLMs, and one T2IM (Qwen-Image) show that MLLM choice is the primary performance driver: GPT achieves the highest accuracies, Gemini follows, and the best open-source MLLM (Gemma) is competitive with some closed models. On the LLM side, Claude attains the strongest average performance for prompt generation.

</details>


### [286] [Training-Free Loosely Speculative Decoding: Accepting Semantically Correct Drafts Beyond Exact Match](https://arxiv.org/abs/2511.22972)
*Jinze Li,Yixing Xu,Guanchen Li,Shuo Yang,Jinfeng Xu,Xuanwu Yin,Dong Li,Edith C. H. Ngai,Emad Barsoum*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的宽松预测解码方法FLy，克服了以往预测解码严格验证导致的语义合理结果丢弃问题，并实现了模型泛化能力的提升和显著加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能优异，但由于自回归生成方式导致推理延迟高。现有的Speculative Decoding（SPD）通过并行验证减小延迟，但严格的逐 token 验证标准会丢弃大量语义有效的生成，且基于训练的SPD遇到分布外任务时表现下降。作者希望在解决上述问题的同时提升生成效率。

Method: 作者提出Training-Free Loosely Speculative Decoding（FLy），取消严格的exact-match验证，利用大模型自身纠错能力判别语义是否合理；方法包含：1）熵门控机制判别当前位置是否存在多种合理输出；2）token级窗口区分真正错误和语义等价的词。进一步，通过多级加速机制提升drafter与target模型解码效率。所有方法均无需额外训练，模型与领域泛化性强。

Result: 实验结果显示，FLy能在主流大模型（如Llama-3.1-70B-Instruct）上保持目标模型99%以上准确率，平均加速2.81倍，在更大模型上加速达5.07倍。针对分布外任务，FLy优于训练型SPD（如EAGLE-3），提升1.62倍。

Conclusion: FLy算法实现了SPD效率与语义宽容的兼得，且无需额外训练，具有极佳的模型泛化性和领域适应能力，是提升LLM推理效率的新方案。

Abstract: Large language models (LLMs) achieve strong performance across diverse tasks but suffer from high inference latency due to their autoregressive generation. Speculative Decoding (SPD) mitigates this issue by verifying candidate tokens in parallel from a smaller draft model, yet its strict exact-match verification discards many semantically valid continuations. Moreover, existing training-based SPD methods often suffer from performance degradation on out-of-distribution (OOD) tasks. To this end, we propose Training-Free Loosely Speculative Decoding (FLy), a novel method that loosens the rigid verification criterion by leveraging the target model's self-corrective behavior to judge whether a draft-target mismatch remains semantically valid. FLy introduces a two-tier mechanism: an entropy-level gate that identifies whether the current token allows multiple plausible alternatives or is nearly deterministic, and a token-level deferred window that distinguishes genuine errors from differently worded yet semantically correct variants. To further reduce latency, we design a multi-level acceleration strategy that accelerates not only the target model but also the drafter itself. Owing to its training-free design, FLy composes seamlessly with arbitrary draft-target pairs and generalizes across models and domains without hyperparameter re-tuning. Experiments show that FLy preserves more than 99% of the target model's accuracy while achieving an average 2.81x speedup on Llama-3.1-70B-Instruct and 5.07x speedup on the 405B variant. Notably, on out-of-domain datasets, our method remains highly effective and outperforms the training-based method EAGLE-3 by 1.62x.

</details>


### [287] [Pooling Attention: Evaluating Pretrained Transformer Embeddings for Deception Classification](https://arxiv.org/abs/2511.22977)
*Sumit Mamtani,Abhijeet Bhure*

Main category: cs.CL

TL;DR: 本文对Transformer模型在假新闻检测任务中的表达能力进行了系统评估。实验证明，BERT等注意力机制编码器配合简单分类器即可取得较好效果，且对输入处理方式不敏感。


<details>
  <summary>Details</summary>
Motivation: 近年来假新闻传播问题日益严重，准确检测假新闻成为自然语言处理重要课题。作者欲厘清在简化下游模型复杂度的前提下，Transformer预训练表征本身对假新闻检测的贡献。

Method: 作者将BERT、GPT-2、Transformer-XL等预训练模型作为冻结嵌入器，配合轻量级分类器（如逻辑回归、浅层神经网络），并系统比较了序列池化/填充和不同分类器头部，评估其在LIAR数据集上的性能。

Result: BERT嵌入配合逻辑回归在LIAR数据集各分割上优于神经网络分类器。在不同序列长度和特征汇聚方式（如最大池化、均值池化）下表现出鲁棒性，截断输入影响较小。

Conclusion: 基于自注意力的Token编码器（如BERT）为内容真实性任务提供了可靠、架构中心化的特征，且不依赖复杂下游分类结构，Transformer贡献可以与分类器复杂度有效隔离。

Abstract: This paper investigates fake news detection as a downstream evaluation of Transformer representations, benchmarking encoder-only and decoder-only pre-trained models (BERT, GPT-2, Transformer-XL) as frozen embedders paired with lightweight classifiers. Through controlled preprocessing comparing pooling versus padding and neural versus linear heads, results demonstrate that contextual self-attention encodings consistently transfer effectively. BERT embeddings combined with logistic regression outperform neural baselines on LIAR dataset splits, while analyses of sequence length and aggregation reveal robustness to truncation and advantages from simple max or average pooling. This work positions attention-based token encoders as robust, architecture-centric foundations for veracity tasks, isolating Transformer contributions from classifier complexity.

</details>


### [288] [ShoppingComp: Are LLMs Really Ready for Your Shopping Cart?](https://arxiv.org/abs/2511.22978)
*Huaixiao Tou,Ying Zeng,Cong Ma,Muzhi Li,Minghao Li,Weijie Yuan,He Zhang,Kai Jia*

Main category: cs.CL

TL;DR: 本文提出了ShoppingComp基准，用于评测大型语言模型（LLMs）驱动的购物智能体在真实购物场景中的表现，强调产品检索、报告生成和安全决策三大能力。实验显示主流模型在这些任务上表现不佳，指出当前研究与实际应用之间存在巨大差距。


<details>
  <summary>Details</summary>
Motivation: 现有电商基准主要关注推荐准确性，难以覆盖真实复杂购物需求、产品安全性或报告质量等多维度挑战。因而，研究者亟需一个综合评测平台以推动可靠实用型智能体发展。

Method: 作者提出ShoppingComp基准框架，包含120个任务、1026种场景，由35位专家打造，涵盖真实购物需求。该基准对LLM购物智能体在产品检索、报告生成和安全决策等方面进行全面评测，同时强调产品安全性和易验证性。使用该基准对当前主流LLM模型（如GPT-5、Gemini-2.5-Flash）进行了系统测试。

Result: 实验发现，即使最先进的GPT-5模型在该基准上的得分仅为11.22%，Gemini-2.5-Flash更低，仅为3.92%。主流模型经常未能识别产品安全风险，易受促销误导，易做出有害推荐，表现明显不足。

Conclusion: ShoppingComp揭示了研究环境与实际应用间的显著差距，为开发更加可靠、实用的电商智能助手树立了新标准，也为今后提升LLM购物能力指出了明确方向。

Abstract: We present ShoppingComp, a challenging real-world benchmark for rigorously evaluating LLM-powered shopping agents on three core capabilities: precise product retrieval, expert-level report generation, and safety critical decision making. Unlike prior e-commerce benchmarks, ShoppingComp introduces highly complex tasks under the principle of guaranteeing real products and ensuring easy verifiability, adding a novel evaluation dimension for identifying product safety hazards alongside recommendation accuracy and report quality. The benchmark comprises 120 tasks and 1,026 scenarios, curated by 35 experts to reflect authentic shopping needs. Results reveal stark limitations of current LLMs: even state-of-the-art models achieve low performance (e.g., 11.22% for GPT-5, 3.92% for Gemini-2.5-Flash). These findings highlight a substantial gap between research benchmarks and real-world deployment, where LLMs make critical errors such as failure to identify unsafe product usage or falling for promotional misinformation, leading to harmful recommendations. ShoppingComp fills the gap and thus establishes a new standard for advancing reliable and practical agents in e-commerce.

</details>


### [289] [Social Perceptions of English Spelling Variation on Twitter: A Comparative Analysis of Human and LLM Responses](https://arxiv.org/abs/2511.23041)
*Dong Nguyen,Laura Rosseel*

Main category: cs.CL

TL;DR: 本文研究了英语在线写作中拼写变化（如 funnnn 与 fun）对社交感知的影响，并比较了人类和大语言模型（LLM）的感知一致性。结果发现二者在多个社会属性标签上的评分高度相关，但在不同拼写变化类型和评分分布上存在明显差异。


<details>
  <summary>Details</summary>
Motivation: 拼写变化在文本交流中普遍存在，并影响读者对文本和作者的社交认知（如正式度、年龄等）。此前尚不清楚 LLM 是否能像人类一样捕捉到这些社会信号，于是本研究旨在填补这一空白。

Method: 采用社会语言学的研究方法，考察三种关键社会属性（正式度、认真的程度、作者年龄），并设计实验让人类与 LLM 对各种拼写变化同时进行评分，进而比较两者间评分的相关性及分布差异。

Result: 人类与 LLM 对拼写变化带来的社会感知属性评分有总体较高的一致性相关，但具体到不同类型的拼写变化和评分分布时，出现了明显的分歧。

Conclusion: 尽管 LLM 能大致捕捉到拼写变化的社会感知模式，但与人类依然存在差异，尤其是在对具体拼写变化的细致理解和社会信号的解读上，提示未来模型在捕捉细微社会语言学信号方面有进一步改进空间。

Abstract: Spelling variation (e.g. funnnn vs. fun) can influence the social perception of texts and their writers: we often have various associations with different forms of writing (is the text informal? does the writer seem young?). In this study, we focus on the social perception of spelling variation in online writing in English and study to what extent this perception is aligned between humans and large language models (LLMs). Building on sociolinguistic methodology, we compare LLM and human ratings on three key social attributes of spelling variation (formality, carefulness, age). We find generally strong correlations in the ratings between humans and LLMs. However, notable differences emerge when we analyze the distribution of ratings and when comparing between different types of spelling variation.

</details>


### [290] [Decoding the Past: Explainable Machine Learning Models for Dating Historical Texts](https://arxiv.org/abs/2511.23056)
*Paulo J. N. Pinto,Armando J. Pinho,Diogo Pratas*

Main category: cs.CL

TL;DR: 本文提出了一种基于特征工程和树模型的可解释文本年代分类方法，显著提高了英语历史文本的时间归类准确率。


<details>
  <summary>Details</summary>
Motivation: 历史文本的准确年代判定对文化遗产整理与理解至关重要，而现有做法在解释性和实用性上有局限，需要兼具准确性和可解释性的解决方案。

Method: 作者融合压缩特征、词汇结构、可读性、新词侦测和距离等五大类特征，基于树状机器学习模型展开，系统比较单一与组合特征的效果，并引入SHAP值进行特征解释，在大规模英语文本语料库和Project Gutenberg外部数据集上实验验证。

Result: 组合特征模型在世纪级文本年代归类中达76.7%准确率，在十年级达26.1%，均显著超越随机基线；放宽精度后可达96%（世纪Top-2）、85.8%（十年Top-10）；AUCROC最高达94.8%，AUPRC达83.3%；关键分界点二元分类达85-98%；特征重要性分析表明距离和词汇结构特征最关键。迁移到外部数据集时精度有所下降，但模型仍优于神经网络在可解释性和效率上的表现。

Conclusion: 该方法以多特征融合和可解释树模型实现了对历史文本年代高效、准确、可解释的分类，对文化遗产文本自动整理具有现实应用价值，并为未来可解释文本年代识别提供了方向。

Abstract: Accurately dating historical texts is essential for organizing and interpreting cultural heritage collections. This article addresses temporal text classification using interpretable, feature-engineered tree-based machine learning models. We integrate five feature categories - compression-based, lexical structure, readability, neologism detection, and distance features - to predict the temporal origin of English texts spanning five centuries. Comparative analysis shows that these feature domains provide complementary temporal signals, with combined models outperforming any individual feature set. On a large-scale corpus, we achieve 76.7% accuracy for century-scale prediction and 26.1% for decade-scale classification, substantially above random baselines (20% and 2.3%). Under relaxed temporal precision, performance increases to 96.0% top-2 accuracy for centuries and 85.8% top-10 accuracy for decades. The final model exhibits strong ranking capabilities with AUCROC up to 94.8% and AUPRC up to 83.3%, and maintains controlled errors with mean absolute deviations of 27 years and 30 years, respectively. For authentication-style tasks, binary models around key thresholds (e.g., 1850-1900) reach 85-98% accuracy. Feature importance analysis identifies distance features and lexical structure as most informative, with compression-based features providing complementary signals. SHAP explainability reveals systematic linguistic evolution patterns, with the 19th century emerging as a pivot point across feature domains. Cross-dataset evaluation on Project Gutenberg highlights domain adaptation challenges, with accuracy dropping by 26.4 percentage points, yet the computational efficiency and interpretability of tree-based models still offer a scalable, explainable alternative to neural architectures.

</details>


### [291] [Standard Occupation Classifier -- A Natural Language Processing Approach](https://arxiv.org/abs/2511.23057)
*Sidharth Rony,Jack Patman*

Main category: cs.CL

TL;DR: 本研究利用自然语言处理技术，开发了能将招聘广告自动归类到职业分类标准（SOC）中的模型，并通过集成BERT和神经网络的方法显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 当前劳动市场分析依赖标准化的职业分类系统（如SOC），但传统方式无法高效处理大规模招聘广告数据，且精确、及时的职业分类对了解劳动力市场需求极为重要，因此有必要开发能自动化、高效处理和分类招聘信息的新方法。

Method: 作者基于近期自然语言处理的进展，分别针对英国ONS SOC与美国O*NET SOC标准，设计并训练了多种分类器。最有效的方法是集成模型，将Google BERT语言模型与神经网络分类器结合，并同时利用招聘广告的职位名称、描述和技能信息。

Result: 集成模型在SOC四级（细分行业）职业归类中达到了最高61%的准确率，在三级分类中准确率为72%，明显超过了单一模型。

Conclusion: 这种基于NLP和集成学习的职业分类器可高效、准确地通过招聘广告监测劳动力市场最新动态，为政策制定和人力资源研究提供有力的数据支持。

Abstract: Standard Occupational Classifiers (SOC) are systems used to categorize and classify different types of jobs and occupations based on their similarities in terms of job duties, skills, and qualifications. Integrating these facets with Big Data from job advertisement offers the prospect to investigate labour demand that is specific to various occupations. This project investigates the use of recent developments in natural language processing to construct a classifier capable of assigning an occupation code to a given job advertisement. We develop various classifiers for both UK ONS SOC and US O*NET SOC, using different Language Models. We find that an ensemble model, which combines Google BERT and a Neural Network classifier while considering job title, description, and skills, achieved the highest prediction accuracy. Specifically, the ensemble model exhibited a classification accuracy of up to 61% for the lower (or fourth) tier of SOC, and 72% for the third tier of SOC. This model could provide up to date, accurate information on the evolution of the labour market using job advertisements.

</details>


### [292] [Conveying Imagistic Thinking in TCM Translation: A Prompt Engineering and LLM-Based Evaluation Framework](https://arxiv.org/abs/2511.23059)
*Jiatong Han*

Main category: cs.CL

TL;DR: 本研究提出了一种结合人类与大模型（HITL）的方法，提升中医古典文本英译的概念还原和临床适用性。


<details>
  <summary>Details</summary>
Motivation: 中医理论以形象思维为基础，常通过隐喻和转喻传递医学原理，但现有英文翻译多为直译，导致读者难以理解和应用其深层概念网络。

Method: 选择《黄帝内经》中四段核心理论文本，设计提示词引导DeepSeek V3.1大模型识别和转译隐喻与转喻。再用ChatGPT 5 Pro和Gemini 2.5 Pro模拟三类真实读者，评分不同翻译（人工、基础模型、提示调整模型）的五个认知维度，并通过结构化访谈和解释现象学分析探索认知体验。

Result: 经提示调整的大模型翻译在五个认知维度上表现最佳，且模型间、角色间评分高度一致。访谈揭示了人工与机器翻译的差异、有用的隐喻转译策略及读者的认知偏好。

Conclusion: 提出了一套认知导向、高效且可复制的HITL方法论，为翻译中医等古代、概念密集型文本提供了有效路径。

Abstract: Traditional Chinese Medicine theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis. Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient and replicable HITL methodological pathway for translation of ancient, concept-dense texts like TCM.

</details>


### [293] [Accent Placement Models for Rigvedic Sanskrit Text](https://arxiv.org/abs/2511.23088)
*Akhil Rajeev P,Annarao Kulkarni*

Main category: cs.CL

TL;DR: 该论文研究了Rigveda（最古印度文献之一）音高重音系统的自动恢复，比较了三种机器学习方法在恢复重音标记任务中的表现，取得了可复现的基线，并提出了对下游应用的指导。


<details>
  <summary>Details</summary>
Motivation: Rigveda原文中独特的重音系统在数字文本中常常缺失，重音信息对于文献诠释和数字化处理（如吟唱合成、OCR等）极为重要，但技术手段不足。该研究旨在填补传统语言与现代NLP技术间的空白，实现Rigveda重音信息的自动化恢复。

Method: 作者构建了带重音-无重音对照的开源语料库，采用三种策略进行重音自动恢复：(1) 采用字节级Transformer（ByT5）进行端到端微调；(2) 重头训练的BiLSTM-CRF序列标注模型作为透明基线；(3) 基于ByT5，进行参数高效的LoRA微调。评估用词、字和特定重音错误率，并着重流程中的Unicode安全和适用性。

Result: 全面微调的ByT5在所有评估指标上表现最佳；LoRA微调在效率和准确率间取得了较好的平衡；BiLSTM-CRF可做透明的基线模型。

Conclusion: 该研究为Rigveda重音恢复确立了可复现实验基线，强调了Unicode安全预处理和重音误差区分评估的重要性，为包括重音感知OCR、自动语音识别和数字人文学科在内的下游应用提供了理论和实践参考，推动了传统语言与现代NLP的结合。

Abstract: The Rigveda, among the oldest Indian texts in Vedic Sanskrit, employs a distinctive pitch-accent system : udātta, anudātta, svarita whose marks encode melodic and interpretive cues but are often absent from modern e-texts. This work develops a parallel corpus of accented-unaccented ślokas and conducts a controlled comparison of three strategies for automatic accent placement in Rigvedic verse: (i) full fine-tuning of ByT5, a byte-level Transformer that operates directly on Unicode combining marks, (ii) a from-scratch BiLSTM-CRF sequence-labeling baseline, and (iii) LoRA-based parameter-efficient fine-tuning atop ByT5.
  Evaluation uses Word Error Rate (WER) and Character Error Rate (CER) for orthographic fidelity, plus a task-specific Diacritic Error Rate (DER) that isolates accent edits. Full ByT5 fine-tuning attains the lowest error across all metrics; LoRA offers strong efficiency-accuracy trade-offs, and BiLSTM-CRF serves as a transparent baseline. The study underscores practical requirements for accent restoration - Unicode-safe preprocessing, mark-aware tokenization, and evaluation that separates grapheme from accent errors - and positions heritage-language technology as an emerging NLP area connecting computational modeling with philological and pedagogical aims. Results establish reproducible baselines for Rigvedic accent restoration and provide guidance for downstream tasks such as accent-aware OCR, ASR/chant synthesis, and digital scholarship.

</details>


### [294] [Mind Reading or Misreading? LLMs on the Big Five Personality Test](https://arxiv.org/abs/2511.23101)
*Francesco Di Cursi,Chiara Boldrini,Marco Conti,Andrea Passarella*

Main category: cs.CL

TL;DR: 本文评估了大语言模型（LLM）在基于文本自动人格预测任务中的表现，但发现现有模型尚未能稳定胜任该任务。


<details>
  <summary>Details</summary>
Motivation: 自动化个性预测可应用于心理学、社交平台等领域。本研究关注在无监督（二元分类，BIG5人格模型）条件下，LLM是否能可靠预测人格特质。

Method: 作者在三套异构数据集（Essays, MyPersonality, Pandora）上，对GPT-4及多种开源LLM进行评测，并比较了两种不同的prompt策略（基础prompt与融合语言/心理线索的丰富prompt）。

Result: 丰富prompt能改善输出有效性并缓解类别不平衡，但也带来了预测偏向。LLM在不同人格特质上的表现差异显著，开放性与宜人性较易识别，外向性与神经质较难。零样本二元预测下，无模型实现稳定和可靠的结果。单纯依赖准确率或macro-F1会掩盖模型对不同类别的真实诊断能力。

Conclusion: 现有LLM无法直接用于自动化人格预测。需要更精细的prompt设计、特质设定及评估方法，才能获得可解释且合理的预测结果。

Abstract: We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). Five models -- including GPT-4 and lightweight open-source alternatives -- are tested across three heterogeneous datasets (Essays, MyPersonality, Pandora) and two prompting strategies (minimal vs. enriched with linguistic and psychological cues). Enriched prompts reduce invalid outputs and improve class balance, but also introduce a systematic bias toward predicting trait presence. Performance varies substantially: Openness and Agreeableness are relatively easier to detect, while Extraversion and Neuroticism remain challenging. Although open-source models sometimes approach GPT-4 and prior benchmarks, no configuration yields consistently reliable predictions in zero-shot binary settings. Moreover, aggregate metrics such as accuracy and macro-F1 mask significant asymmetries, with per-class recall offering clearer diagnostic value. These findings show that current out-of-the-box LLMs are not yet suitable for APPT, and that careful coordination of prompt design, trait framing, and evaluation metrics is essential for interpretable results.

</details>


### [295] [Dripper: Token-Efficient Main HTML Extraction with a Lightweight LM](https://arxiv.org/abs/2511.23119)
*Mengjie Liu,Jiahui Peng,Pei Chu,Jiantao Qiu,Ren Ma,He Zhu,Rui Min,Lindong Lu,Wenchang Ning,Linfeng Hou,Kaiwen Liu,Yuan Qu,Zhenxiang Li,Chao Xu,Zhongying Tu,Wentao Zhang,Conghui He*

Main category: cs.CL

TL;DR: 本文提出了一种高效的网页主内容提取框架Dripper，能在较小模型规模下实现高精度主内容提取，性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 从网页高效准确地抽取主内容对于大模型训练数据的获取具有重要意义，但现有方法存在上下文窗口受限、推理成本高、格式幻觉等瓶颈。

Method: Dripper通过四大创新：1）设计HTML简化算法，大幅减少输入tokens数量；2）将内容提取问题重定义为语义块序列分类任务，降低推理成本；3）引入受控解码机制，通过logits处理器严格约束输出空间，减少模型幻觉；4）构建WebMainBench数据集，包含7800+含人工标注主内容的网页。

Result: 在只有0.6B参数的小模型基础上，Dripper在所有评测基准上表现优异，ROUGE-N F1得分81.58%（采用回退策略时可达83.13%），超越所有对比基线方法。

Conclusion: Dripper有效缓解了网页主内容提取在效率、精度和模型规模上的制约，适合于大规模数据构建，具备实际部署价值。

Abstract: Accurately and efficiently extracting main content from general web pages is of great significance for obtaining training data for large models. Using well-pre-trained decoder-only generative language models offers excellent document comprehension capabilities, thereby effectively enhancing parsing quality. However, it remains constrained by issues such as context window length, inference cost, and format hallucination. We present Dripper, an efficient HTML main content extraction framework powered by lightweight language models, which addresses these challenges through four key innovations: (1) We design a specialized HTML simplification algorithm that reduces input token count to 22\% compared to raw HTML while preserving critical structural information; (2) We reformulate main content extraction as a semantic block sequence classification task, significantly reducing inference cost; (3) We introduce a controlled decoding mechanism that strictly constrains the output space through logits processors, effectively eliminating hallucination issues common in small-scale models; (4) We propose WebMainBench, an evaluation dataset containing over 7,800 web pages with meticulously human-annotated main content extraction labels. Experimental results demonstrate that using only a 0.6B parameter model, Dripper achieves state-of-the-art performance across all evaluation benchmarks and outperforms all baseline methods, attaining an ROUGE-N F1 score of 81.58\%( 83.13\% with fall-back strategy) on our proposed WebMainBench dataset.

</details>


### [296] [Multi-chain Graph Refinement and Selection for Reliable Reasoning in Large Language Models](https://arxiv.org/abs/2511.23136)
*Yujiao Yang,Jing Lian,Linhui Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的推理增强框架Multi-chain Graph Refinement & Selection (MGRS)，旨在提高大语言模型复杂推理能力，并在六个基准测试集上取得了显著的准确率提升和计算效率优势。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Tree-of-Thought和Graph-of-Thought能够提升LLM推理能力，但存在推理路径多样性不足、搜索分支冗余，以及异质推理路径整合和纠错能力不足的问题，限制了模型实际应用效果。

Method: MGRS框架首先为每个问题生成多条多样推理路径，结合自我和交叉验证对候选答案进行精炼；然后构建推理关系图，并估算中间节点成功率，最终通过累计成功率选择最可靠的答案及推理轨迹。

Result: 在六个基准数据集和四个不同任务上，MGRS平均准确率达到82.9%，比现有最优方法高2.1%。在24点游戏任务中实现了100%准确率，并且相较于最优Forest of Thoughts框架速度提升13.6倍。

Conclusion: MGRS框架有效提升了大语言模型的复杂推理能力和计算效率，优于现有推理增强方法，在多个任务上取得了显著效果。

Abstract: The complex reasoning ability of Large Language Models (LLMs) poses a critical bottleneck for their practical applications. Test-time expansion methods such as Tree-of-Thought (ToT) and Graph-of-Thought (GoT) enhance reasoning by introducing intermediate reasoning structures, tree search, or graph-based exploration mechanisms. However, their reasoning strategies suffer from limited diversity, redundant search branches, and inadequate integration and error correction across heterogeneous reasoning paths. To address these limitations, we propose a novel reasoning framework called Multi-chain Graph Refinement & Selection (MGRS), which first generates multiple diverse reasoning trajectories for a given problem, refines candidate responses using a composite self- and cross-verification strategy, then constructs a reasoning relation graph and estimates the success rate of intermediate nodes, and finally computes cumulative success rates to select the most reliable answer and corresponding reasoning trajectory. Experimental results demonstrate that MGRS significantly advances both the reasoning capability and computational efficiency of reasoning enhancement methods. Across six benchmark datasets spanning four distinct tasks, MGRS achieves an average accuracy of 82.9%, outperforming state-of-the-art baselines by a clear margin of 2.1%. Remarkably, on the 24-point game, MGRS attains 100% accuracy for the first time, while delivering a 13.6x speed-up compared to the leading Forest of Thoughts framework.

</details>


### [297] [Are LLMs Good Safety Agents or a Propaganda Engine?](https://arxiv.org/abs/2511.23174)
*Neemesh Yadav,Francesco Ortu,Jiarui Liu,Joeun Yook,Bernhard Schölkopf,Rada Mihalcea,Alberto Cazzaniga,Zhijing Jin*

Main category: cs.CL

TL;DR: 本文分析了大语言模型（LLMs）在应对政治敏感内容时的拒答行为，提出PSP数据集用于系统性研究模型的安全政策与政治审查之间的界限，并通过多种方法分析主流模型的表现和脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs往往会拒答有害内容，但缺乏系统性分析来判断这种拒答是模型自身的安全策略，还是政治驱动的审查。区分这两者对于理解和改进模型行为具有重要意义。

Method: 作者提出并构建了PSP数据集，包含多国政治敏感内容和各国被审查的推文，对七种主流LLMs进行实验，采用隐式数据与表示层控制分析模型的拒答行为，并考察其在应对提示注入攻击（prompt injection attacks）时的脆弱性。

Result: 实验发现，大多数LLMs在政治敏感内容上或多或少都表现出某种形式的审查；不同模型以及不同国家背景下的内容拒答分布存在显著差异。

Conclusion: 作者总结了影响LLMs拒答行为的主要属性，提出模型在应对不同国家和语境的政治内容上存在显著差异，这对未来模型安全和审查机制的改进有重要启示。

Abstract: Large Language Models (LLMs) are trained to refuse to respond to harmful content. However, systematic analyses of whether this behavior is truly a reflection of its safety policies or an indication of political censorship, that is practiced globally by countries, is lacking. Differentiating between safety influenced refusals or politically motivated censorship is hard and unclear. For this purpose we introduce PSP, a dataset built specifically to probe the refusal behaviors in LLMs from an explicitly political context. PSP is built by formatting existing censored content from two data sources, openly available on the internet: sensitive prompts in China generalized to multiple countries, and tweets that have been censored in various countries. We study: 1) impact of political sensitivity in seven LLMs through data-driven (making PSP implicit) and representation-level approaches (erasing the concept of politics); and, 2) vulnerability of models on PSP through prompt injection attacks (PIAs). Associating censorship with refusals on content with masked implicit intent, we find that most LLMs perform some form of censorship. We conclude with summarizing major attributes that can cause a shift in refusal distributions across models and contexts of different countries.

</details>


### [298] [Listwise Preference Optimization with Element-wise Confusions for Aspect Sentiment Quad Prediction](https://arxiv.org/abs/2511.23184)
*Wenna Lai,Haoran Xie,Guandong Xu,Qing Li,S. Joe Qin*

Main category: cs.CL

TL;DR: 本文提出了一种新方法来提升Aspect Sentiment Quad Prediction (ASQP) 任务的四元组结构预测能力，通过推理生成和列表式偏好优化，更好地预测方面、类别、观点和情感极性，实验结果显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有ASQP方法在结构关联建模和高阶元素（如类别和情感极性）预测上存在性能瓶颈，主要因为依赖标记预测，缺乏显式关联推理能力，影响预测效果。

Method: 提出基于推理的生成方法，将四元组各元素作为前缀融入统一模板中，生成结构化预测结果及自然语言解释。同时，创新性引入列表式偏好优化，利用句法和语义相近的候选项，通过列表式目标训练模型偏好真实答案，提升结构有效性和元素间关联性。

Result: 在四个基准数据集上的大量实验证明，该框架在四元组预测准确率和解释一致性方面实现了显著提升。

Conclusion: 结合推理生成和列表偏好优化的新框架，增强了ASQP任务的结构化预测能力和解释性，对细粒度情感分析具有实际应用价值。

Abstract: Aspect sentiment quad prediction (ASQP) is inherently challenging to predict a structured quadruple with four core sentiment elements, including aspect term (a), aspect category (c), opinion term (o), and sentiment polarity (s). Prior methods relying on marker-based prediction struggle with modeling the intricate relationships among elements and experience sharp performance declines when predicting higher-order elements (e.g., c and s) under standard supervised fine-tuning. To address these limitations, we employ reasoning-based generation to output both the quadruple and a natural language rationale under element prefixes within a unified template, encouraging explicit relational reasoning and interpretability. To further enhance element-wise alignment, we introduce a listwise preference optimization framework for improving structural validity and relational coherence. Specifically, we generate element-wise confusable candidates via syntactic and semantic proximity, then train the model with listwise objectives to prefer the gold candidates over closely competing alternatives. Extensive experiments on four benchmark datasets demonstrate that our framework effectively improves quadruple prediction accuracy and explanation consistency.

</details>


### [299] [TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies](https://arxiv.org/abs/2511.23225)
*Guang Liang,Jie Shao,Ningyuan Tang,Xinyao Liu,Jianxin Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为TWEO的新型损失函数，通过简单高效的方式消除训练Transformer模型时出现的极端激活异常值（outliers），从而让全模型FP8训练变得可行，并大幅提升训练速度与硬件适配性。


<details>
  <summary>Details</summary>
Motivation: 现有大型Transformer在FP8精度下训练时，极端激活异常值成为性能和精度的瓶颈。以往做法多依赖复杂的混合精度工程或侵入式结构调整，使用与维护成本高。作者发现这些极端异常值并非数据驱动，而是训练过程中特定权重结构（如共线性）机械生成的。

Method: 提出了一种非侵入式的新损失函数（TWEO），在训练过程中通过一个简单的损失项约束，预防极端异常值的产生，将异常值数量从上万降到20以内。该方法无需额外工程技巧或模型结构变更，适用于LLM和ViT的FP8全精度训练。

Result: 在标准FP8训练容易崩溃的情况下，采用TWEO后不仅能达到与BF16基线相当的性能，还提升了36%的训练吞吐率。此外，TWEO还使之前不可用的W8A8（正向8bit、加权8bit）静态量化成为可能，并在此基础上取得SOTA表现。

Conclusion: TWEO为解决Transformer训练极端异常值问题提供了全新、简单且高效的方向，显著提升了FP8和量化硬件的实用价值，有望推动更高效、更经济的大模型训练与部署。

Abstract: Native FP8 support in modern hardware is essential for training large Transformers, but is severely hindered by extreme activation outliers. Existing solutions either rely on complex mixed-precision engineering or invasive architectural modifications. This paper fundamentally challenges the conventional wisdom that outliers are data-driven. We demonstrate that extreme outliers are a data-independent, mechanically-produced artifact of training, originating from specific structural properties of the weight matrices (i.e., colinearity). Based on this insight, we propose TWEO (Transformers Without Extreme Outliers), a novel, non-invasive loss function. TWEO effectively prevents extreme outliers via a very simple loss term, which reduces outliers from 10000+ to less than 20. TWEO then enables full-model FP8 pre-training with neither engineering tricks nor architectural changes for both LLM and ViT. When standard FP8 training catastrophically collapses, TWEO achieves performance comparable to the BF16 baseline while delivering a 36% increase in training throughput. Also, TWEO enables a new quantization paradigm. Hardware-friendly W8A8 per-tensor static quantization of LLMs, previously considered completely unusable due to outliers, achieves SOTA performance for the first time on TWEO-trained models.

</details>


### [300] [Tourism Question Answer System in Indian Language using Domain-Adapted Foundation Models](https://arxiv.org/abs/2511.23235)
*Praveen Gatla,Anushka,Nikita Kanwar,Gouri Sahoo,Rajesh Kumar Mundotiya*

Main category: cs.CL

TL;DR: 本文首次系统性地设计了印地语旅游领域的抽取式问答系统，主要聚焦于文化及精神名城瓦拉纳西，并构建并增强了相关数据集，进行多种预训练模型和微调方式的性能对比。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏面向印地语、具有文化细致度的旅游类问答系统和相关数据资源，尤其是在瓦拉纳西这一文化重镇领域。

Method: 作者构建了一个包含7,715对印地语问答对的原始数据集，并通过Llama零样本方法扩充至27,455对，采用BERT及RoBERTa多种模型，并利用SFT与LoRA等微调方式提升效果，同时评估各模型在不同旅游子领域的适用性。

Result: LoRA微调在极大减少可训练参数（达98%）的情况下依然取得了有竞争力的F1分数（85.3%）；RoBERTa+SFT在文化语境词（如Aarti, Kund）表现更优。

Conclusion: 本研究为印地语旅游问答系统建立了基线，并突出LoRA方案在低资源环境下的高效、RoBERTa捕捉文化语境的能力，同时强调为旅游领域开发具备文化敏感度的NLP框架的必要性。

Abstract: This article presents the first comprehensive study on designing a baseline extractive question-answering (QA) system for the Hindi tourism domain, with a specialized focus on the Varanasi-a cultural and spiritual hub renowned for its Bhakti-Bhaav (devotional ethos). Targeting ten tourism-centric subdomains-Ganga Aarti, Cruise, Food Court, Public Toilet, Kund, Museum, General, Ashram, Temple and Travel, the work addresses the absence of language-specific QA resources in Hindi for culturally nuanced applications. In this paper, a dataset comprising 7,715 Hindi QA pairs pertaining to Varanasi tourism was constructed and subsequently augmented with 27,455 pairs generated via Llama zero-shot prompting. We propose a framework leveraging foundation models-BERT and RoBERTa, fine-tuned using Supervised Fine-Tuning (SFT) and Low-Rank Adaptation (LoRA), to optimize parameter efficiency and task performance. Multiple variants of BERT, including pre-trained languages (e.g., Hindi-BERT), are evaluated to assess their suitability for low-resource domain-specific QA. Evaluation metrics - F1, BLEU, and ROUGE-L - highlight trade-offs between answer precision and linguistic fluency. Experiments demonstrate that LoRA-based fine-tuning achieves competitive performance (85.3\% F1) while reducing trainable parameters by 98\% compared to SFT, striking a balance between efficiency and accuracy. Comparative analysis across models reveals that RoBERTa with SFT outperforms BERT variants in capturing contextual nuances, particularly for culturally embedded terms (e.g., Aarti, Kund). This work establishes a foundational baseline for Hindi tourism QA systems, emphasizing the role of LORA in low-resource settings and underscoring the need for culturally contextualized NLP frameworks in the tourism domain.

</details>


### [301] [Behavior-Equivalent Token: Single-Token Replacement for Long Prompts in LLMs](https://arxiv.org/abs/2511.23271)
*Jiancheng Dong,Pengyue Jia,Jingyu Peng,Maolin Wang,Yuhao Wang,Lixin Su,Xin Sun,Shuaiqiang Wang,Dawei Yin,Xiangyu Zhao*

Main category: cs.CL

TL;DR: 本文提出用单一的BE (Behavior-Equivalent) token 替换冗长的系统prompt，在极大压缩长度的同时，几乎完全保留下游任务的行为效果。


<details>
  <summary>Details</summary>
Motivation: LLM的系统prompt虽然重要，但篇幅过长导致推理延时增加、计算成本上升及可用上下文长度减少，因此需要更短但等效的替代方案。

Method: 作者提出三阶段训练框架，第一步用重构训练BE token表征原提示内容，第二步蒸馏原提示在下游任务的行为至BE token，无需访问模型内部、辅助压缩模型或标注响应。

Result: 在三个数据集上实验显示，BE token可将prompt长度缩短至1/3000，同时下游任务效果仅损失约2%，即保留约98%性能。

Conclusion: 单一BE token作为系统prompt等效物，在显著减少推理成本的同时，释放了大量上下文空间，提升了LLM的实用性与效率。

Abstract: Carefully engineered system prompts play a critical role in guiding the behavior of LLM agents, but their considerable length introduces significant drawbacks, including increased inference latency, higher computational cost, and reduced effective context length. This raises the question of whether such lengthy prompts can be replaced by a drastically reduced number of tokens while preserving their behavioral effect on downstream tasks. To enable this, we propose a lightweight three-stage training framework that learns a single prompt-specific Behavior-Equivalent token ([BE]). The framework first trains [BE] to encode the natural-language content of the original system prompt via reconstruction, and then distills the prompt 's downstream behavior into this single token. Importantly, our method requires no access to model internals, no auxiliary compression models, and no labeled responses. Empirical evaluations on three datasets show that a single [BE] token achieves up to a 3000x reduction in prompt length, while retaining about 98% of the downstream performance of the original system prompts. This substantially reduces inference cost and leaves almost the entire context window available for user inputs.

</details>


### [302] [MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)](https://arxiv.org/abs/2511.23281)
*Aaron Steiner,Ralph Peeters,Christian Bizer*

Main category: cs.CL

TL;DR: 本论文比较了四种大型语言模型（LLM）网页交互架构（HTML、RAG、MCP、NLWeb）在自动化电商任务中的表现。结果显示RAG、MCP和NLWeb都优于传统HTML代理，在效率和效果上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 目前，自动化网页任务的LLM代理有多种交互方式，但尚无研究在同一控制环境下比较这些架构的优劣。本文旨在填补这一空白，系统比较常用的四种网页交互架构。

Method: 作者设计了含HTML、MCP和NLWeb三种接口的模拟电商平台，并开发四类适配上述接口的代理，在多种复杂程度的任务（如商品搜索、比价、结账等）下运行。评测涵盖GPT 4.1、GPT 5、GPT 5 mini和Claude Sonnet 4等LLM，并统计效果和资源消耗相关指标。

Result: RAG、MCP和NLWeb方式的代理在效率与效果方面均比HTML代理有大幅提升。HTML代理的平均F1为0.67，而其他三种方式在0.75~0.77之间。Token消耗和运行时长也都大幅减少。最佳配置为RAG+GPT 5（F1 0.87，任务完成率0.79），RAG+GPT 5 mini在性价比上表现突出。

Conclusion: 与传统HTML相比，选择更先进的交互架构（如RAG、MCP、NLWeb）能显著提升LLM网页代理的效果和效率。在设计基于LLM的网页自动化代理时，接口选择至关重要。

Abstract: Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout. Current research explores different interfaces through which these agents interact with websites, including traditional HTML browsing, retrieval-augmented generation (RAG) over pre-crawled content, communication via Web APIs using the Model Context Protocol (MCP), and natural-language querying through the NLWeb interface. However, no prior work has compared these four architectures within a single controlled environment using identical tasks.
  To address this gap, we introduce a testbed consisting of four simulated e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each interface (HTML, RAG, MCP, and NLWeb) we develop specialized agents that perform the same sets of tasks, ranging from simple product searches and price comparisons to complex queries for complementary or substitute products and checkout processes. We evaluate the agents using GPT 4.1, GPT 5, GPT 5 mini, and Claude Sonnet 4 as underlying LLM. Our evaluation shows that the RAG, MCP and NLWeb agents outperform HTML on both effectiveness and efficiency. Averaged over all tasks, F1 rises from 0.67 for HTML to between 0.75 and 0.77 for the other agents. Token usage falls from about 241k for HTML to between 47k and 140k per task. The runtime per task drops from 291 seconds to between 50 and 62 seconds. The best overall configuration is RAG with GPT 5 achieving an F1 score of 0.87 and a completion rate of 0.79. Also taking cost into consideration, RAG with GPT 5 mini offers a good compromise between API usage fees and performance. Our experiments show the choice of the interaction interface has a substantial impact on both the effectiveness and efficiency of LLM-based web agents.

</details>


### [303] [Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models](https://arxiv.org/abs/2511.23319)
*Xiang Hu,Zhanchao Zhou,Ruiqi Liang,Zehuan Li,Wei Wu,Jianguo Li*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的层次化稀疏注意力（HSA）机制，并将其集成到Transformer中，以高效处理超长上下文建模难题。通过对8B参数模型（HSA-UltraLong）的实验，显示在高达1600万长度的序列回忆任务上能维持90%以上准确率，表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型在处理超长文本时，受制于计算和存储的限制，难以实现像人类一样的长期记忆。因此，需要开发能高效处理超长上下文的模型，以提升机器在不同任务中的表现。

Method: 作者提出了层次化稀疏注意力机制（HSA），能够同时满足稀疏性、随机访问灵活性和长度泛化能力。该机制被集成进8B参数的专家混合（MoE）模型HSA-UltraLong，并在8万亿Token数据上训练。模型在不同长度和领域任务上进行了严格测试，尤其关注超长序列的检索和推理能力。

Result: HSA-UltraLong在域内上下文长度下，与传统全注意力模型表现相当。在上下文长度高达1600万的检索任务中，准确率超过90%。

Conclusion: 层次化稀疏注意力机制能有效解决超长上下文处理瓶颈，为未来机器超长记忆建模提供理论与实验基础；但依然存在一些待探索的开放问题。

Abstract: This work explores the challenge of building ``Machines that Can Remember'', framing long-term memory as the problem of efficient ultra-long context modeling. We argue that this requires three key properties: \textbf{sparsity}, \textbf{random-access flexibility}, and \textbf{length generalization}. To address ultra-long-context modeling, we leverage Hierarchical Sparse Attention (HSA), a novel attention mechanism that satisfies all three properties. We integrate HSA into Transformers to build HSA-UltraLong, which is an 8B-parameter MoE model trained on over 8 trillion tokens and is rigorously evaluated on different tasks with in-domain and out-of-domain context lengths to demonstrate its capability in handling ultra-long contexts. Results show that our model performs comparably to full-attention baselines on in-domain lengths while achieving over 90\% accuracy on most in-context retrieval tasks with contexts up to 16M. This report outlines our experimental insights and open problems, contributing a foundation for future research in ultra-long context modeling.

</details>


### [304] [Tackling a Challenging Corpus for Early Detection of Gambling Disorder: UNSL at MentalRiskES 2025](https://arxiv.org/abs/2511.23325)
*Horacio Thompson,Marcelo Errecalde*

Main category: cs.CL

TL;DR: 本文针对心理健康风险早期检测（ERD）任务，提出三种方法，用以区分社交媒体用户是否存在高风险的赌博障碍，并取得了MentalRiskES 2025竞赛前两位成绩。


<details>
  <summary>Details</summary>
Motivation: 赌博障碍属于复杂的行为成瘾，影响严重。及时通过社交媒体识别高风险用户，有助于早期干预并减轻社会与个人危害。然而，目前的识别方法依然面临准确性与效率的挑战，因此需要探索更好的算法。


Method: 本文提出了基于CPI+DMC框架的三种方法，分别利用SS3模型、扩展词表的BERT模型以及SBERT模型，实现高低风险用户的分类。同时，结合了用户历史数据分析的决策策略，以优化预测效果和决策速度。

Result: 提出的三种方法中，有两种在MentalRiskES 2025竞赛中获得官方最终排名前两位，显示出决策指标上的突出表现。进一步分析发现，高低风险用户之间的区分仍存在一定困难。

Conclusion: 综合来看，现有方法在实际复杂数据集上虽有突出成果，但依旧存在风险区分难题。未来需改善数据解释与质量，推动更透明、可靠的心理健康早期检测系统发展。

Abstract: Gambling disorder is a complex behavioral addiction that is challenging to understand and address, with severe physical, psychological, and social consequences. Early Risk Detection (ERD) on the Web has become a key task in the scientific community for identifying early signs of mental health behaviors based on social media activity. This work presents our participation in the MentalRiskES 2025 challenge, specifically in Task 1, aimed at classifying users at high or low risk of developing a gambling-related disorder. We proposed three methods based on a CPI+DMC approach, addressing predictive effectiveness and decision-making speed as independent objectives. The components were implemented using the SS3, BERT with extended vocabulary, and SBERT models, followed by decision policies based on historical user analysis. Although it was a challenging corpus, two of our proposals achieved the top two positions in the official results, performing notably in decision metrics. Further analysis revealed some difficulty in distinguishing between users at high and low risk, reinforcing the need to explore strategies to improve data interpretation and quality, and to promote more transparent and reliable ERD systems for mental disorders.

</details>


### [305] [Towards Improving Interpretability of Language Model Generation through a Structured Knowledge Discovery Approach](https://arxiv.org/abs/2511.23335)
*Shuqi Liu,Han Wu,Guanzhi Deng,Jianshu Chen,Xiaoyang Wang,Linqi Song*

Main category: cs.CL

TL;DR: 本文提出了一种任务无关的结构化知识增强文本生成方法，通过结合语言模型的生成能力与知识选择模块的高可靠性，提高生成文本的解释性和泛化能力，在多个数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前知识增强文本生成存在可解释性差和泛化能力不足的问题。尤其是在实际应用中，用户需要对系统输出产生的过程有清晰理解，而现有方法受限于领域特定知识检索器，难以迁移到多样化任务。

Method: 作者设计了一种基于高层实体与低层知识三元组的结构化知识二层架构。采用局部-全局交互式结构化知识表征学习，并以分层transformer指针网络为主干进行相关知识实体和三元组选择。该方法结合了强大的语言生成能力和知识选择的高可靠性，提高了输出的解释性和信任度。

Result: 实验证明，该模型在内部知识增强的RotoWireFG表格-到-文本生成和外部知识增强的KdConv对话响应生成任务中，均超越了最新的其他方法和同类语言模型。

Conclusion: 提出的方法不仅提升了生成文本的可解释性和可靠性，也具有很强的任务泛化能力，为知识增强文本生成树立了新的性能标准。

Abstract: Knowledge-enhanced text generation aims to enhance the quality of generated text by utilizing internal or external knowledge sources. While language models have demonstrated impressive capabilities in generating coherent and fluent text, the lack of interpretability presents a substantial obstacle. The limited interpretability of generated text significantly impacts its practical usability, particularly in knowledge-enhanced text generation tasks that necessitate reliability and explainability. Existing methods often employ domain-specific knowledge retrievers that are tailored to specific data characteristics, limiting their generalizability to diverse data types and tasks. To overcome this limitation, we directly leverage the two-tier architecture of structured knowledge, consisting of high-level entities and low-level knowledge triples, to design our task-agnostic structured knowledge hunter. Specifically, we employ a local-global interaction scheme for structured knowledge representation learning and a hierarchical transformer-based pointer network as the backbone for selecting relevant knowledge triples and entities. By combining the strong generative ability of language models with the high faithfulness of the knowledge hunter, our model achieves high interpretability, enabling users to comprehend the model output generation process. Furthermore, we empirically demonstrate the effectiveness of our model in both internal knowledge-enhanced table-to-text generation on the RotoWireFG dataset and external knowledge-enhanced dialogue response generation on the KdConv dataset. Our task-agnostic model outperforms state-of-the-art methods and corresponding language models, setting new standards on the benchmark.

</details>


### [306] [Scaling HuBERT for African Languages: From Base to Large and XL](https://arxiv.org/abs/2511.23370)
*Antoine Caubrière,Elodie Gauthier*

Main category: cs.CL

TL;DR: 该论文提出并开源了两个专为非洲语音训练的大型自监督编码器模型（SSA-HuBERT-Large 和 SSA-HuBERT-XL），并实验证明大模型在低资源非洲语种语音任务上优于小模型。


<details>
  <summary>Details</summary>
Motivation: 目前多语种语音处理领域对非洲语言的研究和实际应用极度稀缺，特别缺乏能够在低资源条件下表现良好的强大开放预训练编码器模型。现有针对非洲语音的公开自监督预训练模型主要为Base规模，尚不明确更大规模模型是否能够带来显著提升及其与数据组成的关系。

Method: 作者构建了专门针对非洲语音的大型（317M和964M参数）和Base规模的HuBERT自监督编码器，并在包含多种撒哈拉以南非洲语言的数据集上，针对自动语音识别（ASR）与语言识别（LID）任务进行了严格对照实验。

Result: 实验证实，大型模型在有效利用大规模非洲语音数据集的情况下，在ASR和LID任务上相较小模型有显著性能提升。

Conclusion: 专为非洲语种训练的大型自监督语音编码器具备明显性能优势，该工作为低资源环境下非洲语音处理提供了有力基础，也填补了相关开源社区空白。

Abstract: Despite recent progress in multilingual speech processing, African languages remain under-represented in both research and deployed systems, particularly when it comes to strong, open-weight encoders that transfer well under low-resource supervision. Self-supervised learning has proven especially promising in such settings, yet most publicly released models targeting African speech remain at BASE scale, leaving unanswered whether larger encoders, trained exclusively on Africa-centric audio, offer tangible benefits and how model capacity interacts with data composition. This work addresses that gap by introducing SSA-HuBERT-Large (317M parameters) and SSA-HuBERT-XL (964M parameters), the first large models trained solely on African speech, alongside a BASE size counterpart. We release these models as open weights: see https://huggingface.co/collections/Orange/african-speech-foundation-models. By conducting a carefully controlled experimental study focused exclusively on Sub-Saharan languages, covering automatic speech recognition (ASR) and language identification (LID) tasks, we demonstrate that larger architectures significantly improve performance by effectively leveraging large audio datasets.

</details>


### [307] [Optimizing Multimodal Language Models through Attention-based Interpretability](https://arxiv.org/abs/2511.23375)
*Alexander Sergeev,Evgeny Kotelnikov*

Main category: cs.CL

TL;DR: 本文提出了一种基于注意力得分的可解释性方法，帮助多模态大模型（MLM）实现高效参数微调，在图像描述任务中，通过关注于关键图像对象的注意力头来选择微调层，大幅提升模型效率和表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（MLMs）虽然能处理文本与图像等多种信息，但对其微调需要大量计算资源。参数高效微调（PEFT）虽可减少计算，但难以判定哪些模型组件最值得优先微调。本研究旨在通过提高模型可解释性，找出对下游任务（如图像描述）最有贡献的注意力组件，从而最大化微调效率与效果。

Method: 作者提出分析MLM中注意力头对图像关键对象的关注程度，并定义Head Impact（HI）分数衡量其重要性。基于此，选择高HI分层进行参数高效微调。研究还新建了包含图像、关键对象掩码及文本描述的数据集，并在20-30亿参数规模的MLM上进行了实验。

Result: 实验显示，优先微调高HI分层（即更关注关键图像对象的注意力头）能带来比微调随机或低HI分层更显著的性能提升。同时，仅需微调极少量（约0.01%）的参数即可显著增强模型对图像的理解能力。

Conclusion: 关注于与图像关键对象相关的注意力头进行参数高效微调，能以极低成本带来较大性能收益，为多模态模型的高效应用与解释性分析提供了新思路。

Abstract: Modern large language models become multimodal, analyzing various data formats like text and images. While fine-tuning is effective for adapting these multimodal language models (MLMs) to downstream tasks, full fine-tuning is computationally expensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by training only a small portion of model weights. However, MLMs are difficult to interpret, making it challenging to identify which components are most effective for training to balance efficiency and performance. We propose an attention-based interpretability method for MLMs by analyzing attention scores relative to image tokens. The core idea is to identify attention heads that focus on image key objects. We utilize this information to select optimal model components for PEFT in multimodal models. Our contributions include a method for identifying attention heads associated with image key objects, its application to PEFT for image captioning, and the creation of a new dataset containing images, key object masks, and their textual descriptions. We conducted experiments on MLMs with 2-3 billion parameters to validate the method's effectiveness. By calculating Head Impact (HI) scores we quantify an attention head's focus on key objects, indicating its significance in image understanding. Our fine-tuning experiments demonstrate that adapting layers with the highest HI scores leads to the most significant shifts in metrics compared to pre-trained, randomly selected, or lowest-HI-score layers. This indicates that fine-tuning a small percentage (around 0.01%) of parameters in these crucial layers can substantially influence image understanding capabilities.

</details>


### [308] [Ambiguity Awareness Optimization: Towards Semantic Disambiguation for Direct Preference Optimization](https://arxiv.org/abs/2511.23391)
*Jian Li,Shenglin Yin,Yujia Zhang,Alan Zhao,Xi Chen,Xiaohui Zhou,Pengfei Xu*

Main category: cs.CL

TL;DR: 该论文分析了现有DPO方法在训练中遇到语义重复或相似内容（歧义性内容）时带来的负面影响，并提出了一种减少此类影响的新方法AAO，并在多个基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: DPO在基于人类反馈的强化学习中应用广泛，但在实际应用中，偏好对之间常出现语义重复或相似（歧义性）内容，这可能降低模型对人类意图的对齐效果。为此，作者着手分析和解决歧义性内容带来的性能瓶颈问题。

Method: 作者通过数学分析和实验，揭示了歧义性内容会引入额外的不确定性，影响模型性能。为此，提出了‘歧义性感知优化’（AAO）方法，自动为歧义性内容分配权重，抑制其负面影响。AAO核心在于通过语义相似度评估并重加权歧义性内容的损失贡献。

Result: 大量实验表明，AAO方法显著提升了模型在多个主流基准（如AlpacaEval 2、MT-Bench、Arena-Hard）上的表现，优于当前DPO等先进方法，提升幅度最高可达15.0分。此外，AAO未显著增加生成响应的长度。

Conclusion: 歧义性内容在DPO训练中会影响模型对偏好学习的能力，引入AAO后可以有效抑制这种负面影响，稳步提升模型的对齐效果。这为今后基于偏好优化的训练方法提供了新思路。

Abstract: Direct Preference Optimization (DPO) is a widely used reinforcement learning from human feedback (RLHF) method across various domains. Recent research has increasingly focused on the role of token importance in improving DPO effectiveness. It is observed that identical or semantically similar content (defined as ambiguous content) frequently appears within the preference pairs. We hypothesize that the presence of ambiguous content during DPO training may introduce ambiguity, thereby limiting further improvements in alignment. Through mathematical analysis and proof-of-concept experiments, we reveal that ambiguous content may potentially introduce ambiguities, thereby degrading performance. To address this issue, we introduce Ambiguity Awareness Optimization (AAO), a simple yet effective approach that automatically re-weights ambiguous content to reduce ambiguities by calculating semantic similarity from preference pairs. Through extensive experiments, we demonstrate that AAO consistently and significantly surpasses state-of-the-art approaches in performance, without markedly increasing response length, across multiple model scales and widely adopted benchmark datasets, including AlpacaEval 2, MT-Bench, and Arena-Hard. Specifically, AAO outperforms DPO by up to 8.9 points on AlpacaEval 2 and achieves an improvement of by up to 15.0 points on Arena-Hard.

</details>


### [309] [MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation](https://arxiv.org/abs/2511.23397)
*Mahdi Rahmani,AmirHossein Saffari,Reyhane Rahmani*

Main category: cs.CL

TL;DR: 论文提出了MegaChat，这是首个面向Telegram电商的高质量全合成波斯语问答数据集，通过自动化多智能体系统合成多样真实的对话数据，助力中小企业低成本构建销售智能客服。实验证明该系统优于传统RAG模型，无需昂贵人工标注。


<details>
  <summary>Details</summary>
Motivation: 伊朗市场中小企业越来越依赖Telegram做销售，但开发AI客服需要大规模高质量本地语料问答数据集，波斯语等低资源语言获取此类数据极为困难且成本高。

Method: 作者提出自动化多智能体架构，包括问题生成、验证和优化等专用智能体，从Telegram购物频道收集数据，生成具有人设属性的多样问答对，并用于智能客服性能评估。

Result: 实验对比三种传统RAG和作者提出的多查询检索+重排序+人设对齐新系统，用GPT-5.1在六项指标上评价，发现创新系统在五个销售频道中有四个超越传统模型。

Conclusion: MegaChat系统可为中小企业提供波斯语电商场景下高效、低成本的数据集生成方案，显著助力低资源多语言对话式AI的发展和落地。

Abstract: Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion. However, developing AI-driven chatbots for this purpose requires large, high-quality question-and-answer (Q&A) datasets, which are typically expensive and resource-intensive to produce, especially for low-resource languages like Persian. In this paper, we introduce MegaChat, the first fully synthetic Persian Q&A dataset designed to evaluate intelligent sales chatbots in Telegram-based e-commerce. We propose a novel, automated multi-agent architecture that generates persona-aware Q&A pairs by collecting data from active Telegram shopping channels. The system employs specialized agents for question generation, validation, and refinement, ensuring the production of realistic and diverse conversational data. To evaluate answer generation, we compare three classic retrieval-augmented generation (RAG) models with our advanced agentic system, which features multi-query retrieval, reranking, and persona-aligned response synthesis. Using GPT-5.1 for evaluation across six quality dimensions, our results show that the agentic architecture outperformed traditional RAG models in 4 out of 5 diverse channels, demonstrating its ability to generate scalable, high-quality datasets without relying on expensive human annotation or complex fine-tuning. MegaChat provides SMEs with an efficient, cost-effective solution for building intelligent customer engagement systems in specialized commercial domains, enabling advancements in multilingual conversational AI for low-resource languages. Download: https://github.com/MegaChat-Tech/MegaChat-DataSet

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [310] [Bridging Planning and Execution: Multi-Agent Path Finding Under Real-World Deadlines](https://arxiv.org/abs/2511.21886)
*Jingtian Yan,Shuai Zhou,Stephen F. Smith,Jiaoyang Li*

Main category: cs.RO

TL;DR: 本文提出了REMAP框架，通过引入执行时间预测网络ExecTimeNet，将实际执行因素（如动力学约束、通信延迟等）纳入多智能体路径规划（MAPF），有效提升了在真实截止时间约束下的路径规划质量。


<details>
  <summary>Details</summary>
Motivation: 现有MAPF方法在规划时简化了机器人模型，忽略了实际执行中的诸如动力学约束、通信延迟和控制器差异，导致在时间敏感型应用中计划与执行存在较大偏差。为解决这一问题，有必要将执行时的实际因素引入规划过程，提升方案的可落地性和效率。

Method: 提出一种名为REMAP的执行感知MAPF规划框架，并设计ExecTimeNet用于基于路径预测执行时间。该框架无需大幅修改即可集成到主流的搜索型MAPF算法（如MAPF-LNS和CBS）中，实现对包含真实截止时间的MAPF问题（MAPF-RD）的求解。

Result: REMAP集成至两种主流MAPF算法，实验证明在多达300个智能体的基准地图下，方案质量较传统基线方法（如固定速度模型）有最高20%的提升。

Conclusion: REMAP显著缩小了多机器人路径规划中计划与执行的差距，尤其在有实际截止时间约束的问题上具有重要实用价值。

Abstract: The Multi-Agent Path Finding (MAPF) problem aims to find collision-free paths for multiple agents while optimizing objectives such as the sum of costs or makespan. MAPF has wide applications in domains like automated warehouses, manufacturing systems, and airport logistics. However, most MAPF formulations assume a simplified robot model for planning, which overlooks execution-time factors such as kinodynamic constraints, communication latency, and controller variability. This gap between planning and execution is problematic for time-sensitive applications. To bridge this gap, we propose REMAP, an execution-informed MAPF planning framework that can be combined with leading search-based MAPF planners with minor changes. Our framework integrates the proposed ExecTimeNet to accurately estimate execution time based on planned paths. We demonstrate our method for solving MAPF with Real-world Deadlines (MAPF-RD) problem, where agents must reach their goals before a predefined wall-clock time. We integrate our framework with two popular MAPF methods, MAPF-LNS and CBS. Experiments show that REMAP achieves up to 20% improvement in solution quality over baseline methods (e.g., constant execution speed estimators) on benchmark maps with up to 300 agents.

</details>


### [311] [OpenTwinMap: An Open-Source Digital Twin Generator for Urban Autonomous Driving](https://arxiv.org/abs/2511.21925)
*Alex Richardson,Jonathan Sprinkle*

Main category: cs.RO

TL;DR: 本文提出了OpenTwinMap，一个基于Python的开放源码框架，用于生成高保真的三维城市数字孪生模型，重点在于易扩展性和并行处理，以支持不同城市环境下自动驾驶研究的仿真与集成。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶仿真工具往往与特定模拟器紧耦合且不易拓展，比如CARLA的数字孪生框架完全依赖Unreal Engine C++插件，限制了灵活性和快速原型开发。为此，作者希望实现一个更加灵活、易于扩展且技术门槛更低的数字孪生生成工具。

Method: OpenTwinMap利用LiDAR扫描和OpenStreetMap数据，自动生成具有语义分割的静态环境资产（包括道路网络、地形和城市结构），可以导出去Unreal Engine进行仿真。该框架采用Python实现，支持数据预处理、基本道路和地形构建，并初步支持与CARLA集成。

Result: 目前，OpenTwinMap已具备OSM与LiDAR数据预处理、基本道路和地形生成、以及对CARLA的初步集成等功能。

Conclusion: OpenTwinMap框架降低了研究人员在不同城市环境下进行自动驾驶仿真和原型开发的门槛，未来有望为自动驾驶领域的数字孪生和仿真带来更高的灵活性和可拓展性。

Abstract: Digital twins of urban environments play a critical role in advancing autonomous vehicle (AV) research by enabling simulation, validation, and integration with emerging generative world models. While existing tools have demonstrated value, many publicly available solutions are tightly coupled to specific simulators, difficult to extend, or introduce significant technical overhead. For example, CARLA-the most widely used open-source AV simulator-provides a digital twin framework implemented entirely as an Unreal Engine C++ plugin, limiting flexibility and rapid prototyping. In this work, we propose OpenTwinMap, an open-source, Python-based framework for generating high-fidelity 3D urban digital twins. The completed framework will ingest LiDAR scans and OpenStreetMap (OSM) data to produce semantically segmented static environment assets, including road networks, terrain, and urban structures, which can be exported into Unreal Engine for AV simulation. OpenTwinMap emphasizes extensibility and parallelization, lowering the barrier for researchers to adapt and scale the pipeline to diverse urban contexts. We describe the current capabilities of the OpenTwinMap, which includes preprocessing of OSM and LiDAR data, basic road mesh and terrain generation, and preliminary support for CARLA integration.

</details>


### [312] [RSPECT: Robust and Scalable Planner for Energy-Aware Coordination of UAV-UGV Teams in Aerial Monitoring](https://arxiv.org/abs/2511.21957)
*Cahit Ikbal Er,Amin Kashiri,Yasin Yazicioglu*

Main category: cs.RO

TL;DR: 本文研究了利用能源受限的无人机和地面无人车（作为移动充电站）协作完成长时间空中监测任务的规划问题，并提出了可扩展且高效的启发式算法RSPECT。


<details>
  <summary>Details</summary>
Motivation: 无人机受限于有限续航能力，难以长时间大范围执行监测任务，移动充电地面车辆可以协助，提高任务完成效率与鲁棒性，针对实际任务中存在的不确定性（如障碍物、风）进行鲁棒规划具有重要意义。

Method: 作者将该问题形式化为混合整数规划（MIP），证明其为NP难，针对大规模问题提出了高效的鲁棒启发式算法RSPECT，并从理论上分析其复杂性、可行性和鲁棒性。同时，通过仿真与实验验证算法性能。

Result: RSPECT能够在存在不确定因素的条件下，快速生成可行且鲁棒的无人机-地面车辆协作巡视规划，并有效满足能源、终点等约束，实验结果显示其优于现有方法。

Conclusion: RSPECT能够实现无人机与地面车的高效协作，提升长时空中监测任务的完成效率和适应不确定性的能力，具备良好实际应用前景。

Abstract: We consider the robust planning of energy-constrained unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs), which act as mobile charging stations, to perform long-horizon aerial monitoring missions. More specifically, given a set of points to be visited by the UAVs and desired final positions of the UAV-UGV teams, the objective is to find a robust plan (the vehicle trajectories) that can be realized without a major revision in the face of uncertainty (e.g., unknown obstacles/terrain, wind) to complete this mission in minimum time. We provide a formal description of this problem as a mixed-integer program (MIP), which is NP-hard. Since exact solution methods are computationally intractable for such problems, we propose RSPECT, a scalable and efficient heuristic. We provide theoretical results on the complexity of our algorithm and the feasibility and robustness of resulting plans. We also demonstrate the performance of our method via simulations and experiments.

</details>


### [313] [Constant-Volume Deformation Manufacturing for Material-Efficient Shaping](https://arxiv.org/abs/2511.22042)
*Lei Li,Jiale Gong,Ziyang Li,Hong Wang*

Main category: cs.RO

TL;DR: 本文提出了一种基于“数字模具（digital-mold）”的体积保持成形新范式，通过结合实时体积一致性建模、几何感知变形预测与误差补偿策略，实现对塑性材料高精度且可控的成形加工。


<details>
  <summary>Details</summary>
Motivation: 现有的增材和减材制造虽可实现复杂结构，但由于采用分层堆叠或局部去除的方式，存在体积损失、变形不可控与形状偏差等问题，难以满足高精度和可持续制造的需求。

Method: 该方法整合了实时体积一致性建模、基于几何信息的变形预测及误差补偿，通过分析后成形点云中的变形模式与误差趋势，动态修正弹性回弹和误差累积，实现体积和表面连续性的精准控制。

Result: 在五种典型几何测试中，系统能够高保真地还原目标形状，同时实现了超过98%的材料利用率。

Conclusion: 该方法为数字驱动且可复制的可持续、零浪费的个性化成形提供了新途径，有效结合了数字建模、实时感知与自适应成形，推动了下一代可持续与定制化制造的发展。

Abstract: Additive and subtractive manufacturing enable complex geometries but rely on discrete stacking or local removal, limiting continuous and controllable deformation and causing volume loss and shape deviations. We present a volumepreserving digital-mold paradigm that integrates real-time volume-consistency modeling with geometry-informed deformation prediction and an error-compensation strategy to achieve highly predictable shaping of plastic materials. By analyzing deformation patterns and error trends from post-formed point clouds, our method corrects elastic rebound and accumulation errors, maintaining volume consistency and surface continuity. Experiments on five representative geometries demonstrate that the system reproduces target shapes with high fidelity while achieving over 98% material utilization. This approach establishes a digitally driven, reproducible pathway for sustainable, zero-waste shaping of user-defined designs, bridging digital modeling, real-time sensing, and adaptive forming, and advancing next-generation sustainable and customizable manufacturing.

</details>


### [314] [SwordRiding: A Unified Navigation Framework for Quadrotors in Unknown Complex Environments via Online Guiding Vector Fields](https://arxiv.org/abs/2511.22043)
*Xuchen Liu,Ruocheng Li,Bin Xin,Weijia Yao,Qigeng Duan,Jinqiang Cui,Ben M. Chen,Jie Chen*

Main category: cs.RO

TL;DR: 本文提出了一种面向未知复杂环境的四旋翼飞行器实时导航新框架，采用闭环引导向量场（GVF）方法，相较传统方法大大提升了环境适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大部分四旋翼导航方法为开环，难以应对风扰动等外部不确定性，因此提升在未知环境中的实时适应能力是亟需解决的核心难题。

Method: 系统通过机载感知模块构建欧几里得有符号距离场（ESDF），利用全局规划器生成无碰撞路径点并用均匀B样条优化为可飞行轨迹。然后结合ESDF和B样条轨迹自适应合成GVF，采用闭环导航提升外部干扰下的鲁棒性。

Result: 大量仿真与实地实验表明，该方法在外部扰动下表现出更强的鲁棒性和更优的实时性能，优于传统GVF及主流规划控制框架。

Conclusion: 本文提出的闭环GVF导航框架有效提升了四旋翼在未知复杂环境下的实时自适应与抗扰动能力，兼容主流路径规划算法，具有良好的实际应用前景。

Abstract: Although quadrotor navigation has achieved high performance in trajectory planning and control, real-time adaptability in unknown complex environments remains a core challenge. This difficulty mainly arises because most existing planning frameworks operate in an open-loop manner, making it hard to cope with environmental uncertainties such as wind disturbances or external perturbations. This paper presents a unified real-time navigation framework for quadrotors in unknown complex environments, based on the online construction of guiding vector fields (GVFs) from discrete reference path points. In the framework, onboard perception modules build a Euclidean Signed Distance Field (ESDF) representation of the environment, which enables obstacle awareness and path distance evaluation. The system first generates discrete, collision-free path points using a global planner, and then parameterizes them via uniform B-splines to produce a smooth and physically feasible reference trajectory. An adaptive GVF is then synthesized from the ESDF and the optimized B-spline trajectory. Unlike conventional approaches, the method adopts a closed-loop navigation paradigm, which significantly enhances robustness under external disturbances. Compared with conventional GVF methods, the proposed approach directly accommodates discretized paths and maintains compatibility with standard planning algorithms. Extensive simulations and real-world experiments demonstrate improved robustness against external disturbances and superior real-time performance.

</details>


### [315] [SoftNash: Entropy-Regularized Nash Games for Non-Fighting Virtual Fixtures](https://arxiv.org/abs/2511.22087)
*Tai Inui,Jee-Hwan Ryu*

Main category: cs.RO

TL;DR: 提出Soft-Nash虚拟夹具，通过一个参数平衡机器人控制的强度与用户主观感受，实现精准又舒适的远程操作控制。实验显示该方法能兼顾高精度与低冲突，提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟夹具（VFs）虽能提升远程操作的精度，但常常与用户操作产生冲突，增加心理负担，降低用户主观控制感。为解决精度与用户体验的矛盾，需寻找更柔和且可调节的控制策略。

Method: 作者提出Soft-Nash虚拟夹具，将传统的两人LQ Nash博弈方案，通过一个可调参数τ软化，控制夹具的参与力度。τ=0为硬性最大性能方案，τ增大控制器力量变柔和，操作权更多转向人类用户。该方法有严格的数学推导基础（KL-正则信任域和最大熵视角），并在6自由度触觉设备上（三维跟踪实验，n=12）开展实证研究。

Result: 实验证明，在τ≈1-3时，尤其τ=2，跟踪误差与调优的传统VF无显著统计差异，但用户与控制器冲突明显减少，主观工作负荷降低（NASA-TLX），主观控制感（SoA）上升，且综合性能峰值在τ=2-3。

Conclusion: 单参数Soft-Nash方案能在保证精度的同时，改善用户舒适性与主观控制感，为触觉和远程操控领域的个性化共享控制提供了实用、直观的路径。

Abstract: Virtual fixtures (VFs) improve precision in teleoperation but often ``fight'' the user, inflating mental workload and eroding the sense of agency. We propose Soft-Nash Virtual Fixtures, a game-theoretic shared-control policy that softens the classic two-player linear-quadratic (LQ) Nash solution by inflating the fixture's effort weight with a single, interpretable scalar parameter $τ$. This yields a continuous dial on controller assertiveness: $τ=0$ recovers a hard, performance-focused Nash / virtual fixture controller, while larger $τ$ reduce gains and pushback, yet preserve the equilibrium structure and continuity of closed-loop stability. We derive Soft-Nash from both a KL-regularized trust-region and a maximum-entropy viewpoint, obtaining a closed-form robot best response that shrinks authority and aligns the fixture with the operator's input as $τ$ grows. We implement Soft-Nash on a 6-DoF haptic device in 3D tracking task ($n=12$). Moderate softness ($τ\approx 1-3$, especially $τ=2$) maintains tracking error statistically indistinguishable from a tuned classic VF while sharply reducing controller-user conflict, lowering NASA-TLX workload, and increasing Sense of Agency (SoAS). A composite BalancedScore that combines normalized accuracy and non-fighting behavior peaks near $τ=2-3$. These results show that a one-parameter Soft-Nash policy can preserve accuracy while improving comfort and perceived agency, providing a practical and interpretable pathway to personalized shared control in haptics and teleoperation.

</details>


### [316] [Design of an Adaptive Modular Anthropomorphic Dexterous Hand for Human-like Manipulation](https://arxiv.org/abs/2511.22100)
*Zelong Zhou,Wenrui Chen,Zeyun Hu,Qiang Diao,Qixin Gao,Yaonan Wang*

Main category: cs.RO

TL;DR: 本论文提出了一种新型仿生手指结构，并基于此设计了可自适应的模块化灵巧手，通过4自由度且仅用2个驱动器，实现了人类般高效抓握和操控，兼顾了驱动复杂性与灵巧度。在原型实验中验证了设计的有效性。


<details>
  <summary>Details</summary>
Motivation: 生物协同被广泛用于灵巧手设计，能够用较少驱动器实现复杂操作，但过度耦合会削弱手的灵活性。如何在驱动器数量和灵巧性之间取得平衡，是当前面临的主要挑战。

Method: 以人手生物协同和动作分析为基础，将关节协作和结构特征转化为模块化手指结构。并据此设计了拥有4自由度且由2个驱动器控制的仿生手指，构建了五指模块化手的运动学模型，并制造原型进行初步实验。

Result: 实验结果表明，该设计能够在驱动器数量有限的情况下实现自适应抓握和手内灵活操作，结构及运动学分析的有效性得到验证。

Conclusion: 所提结构在降低驱动复杂度的同时保持了较高的灵巧性和适应性，对仿生灵巧手的实际应用和未来发展具有重要意义。

Abstract: Biological synergies have emerged as a widely adopted paradigm for dexterous hand design, enabling human-like manipulation with a small number of actuators. Nonetheless, excessive coupling tends to diminish the dexterity of hands. This paper tackles the trade-off between actuation complexity and dexterity by proposing an anthropomorphic finger topology with 4 DoFs driven by 2 actuators, and by developing an adaptive, modular dexterous hand based on this finger topology. We explore the biological basis of hand synergies and human gesture analysis, translating joint-level coordination and structural attributes into a modular finger architecture. Leveraging these biomimetic mappings, we design a five-finger modular hand and establish its kinematic model to analyze adaptive grasping and in-hand manipulation. Finally, we construct a physical prototype and conduct preliminary experiments, which validate the effectiveness of the proposed design and analysis.

</details>


### [317] [3D Affordance Keypoint Detection for Robotic Manipulation](https://arxiv.org/abs/2511.22195)
*Zhiyang Liu,Ruiteng Zhao,Lei Zhou,Chengran Yuan,Yuwei Wu,Sheng Guo,Zhengshen Zhang,Chenchen Liu,Marcelo H Ang*

Main category: cs.RO

TL;DR: 本文提出了一种基于3D关键点的创新机器人操控方法（FAKP-Net），能够提升对物体部件功能的理解，在分割与关键点检测任务上表现优异，并且在真实环境中实现了对新颖物体的成功操控。


<details>
  <summary>Details</summary>
Motivation: 传统的可供性（affordance）检测主要作为语义分割任务，仅回答'是什么'的问题，缺乏对'如何'使用和交互位置、方向等信息的直接指导。因此迫切需要方法能够给机器人提供关于操作方法的更直接信息。

Method: 作者提出了融合RGB和深度图像的三维关键点四元组（3D keypoint quadruplet）与融合型可供性关键点网络（FAKP-Net），能够输出操作位置、方向与范围的综合信息，实现更精准的操控引导。

Result: 在基准测试中，FAKP-Net在可供性分割和关键点检测任务中明显优于其他现有模型。同时，在真实世界的实验中，该方法在操控未见过的新物体时表现出较高的可靠性。

Conclusion: FAKP-Net不仅可以更好地理解物体部件功能，还能为机器人提供详细的操控指导，显著提升了机器人面对新环境和新物体时的操控能力。

Abstract: This paper presents a novel approach for affordance-informed robotic manipulation by introducing 3D keypoints to enhance the understanding of object parts' functionality. The proposed approach provides direct information about what the potential use of objects is, as well as guidance on where and how a manipulator should engage, whereas conventional methods treat affordance detection as a semantic segmentation task, focusing solely on answering the what question. To address this gap, we propose a Fusion-based Affordance Keypoint Network (FAKP-Net) by introducing 3D keypoint quadruplet that harnesses the synergistic potential of RGB and Depth image to provide information on execution position, direction, and extent. Benchmark testing demonstrates that FAKP-Net outperforms existing models by significant margins in affordance segmentation task and keypoint detection task. Real-world experiments also showcase the reliability of our method in accomplishing manipulation tasks with previously unseen objects.

</details>


### [318] [Bayesian Decentralized Decision-making for Multi-Robot Systems: Sample-efficient Estimation of Event Rates](https://arxiv.org/abs/2511.22225)
*Gabriel Aguirre,Simay Atasoy Bingöl,Heiko Hamann,Jonas Kuckling*

Main category: cs.RO

TL;DR: 本文提出了一种分布式Bayesian方法，使机器人集群能在危险环境中有效判断两区域哪一方更安全，且效率高、风险低。


<details>
  <summary>Details</summary>
Motivation: 集群机器人在无法直接获取完整信息的危险环境下，需要在探索、通信和不确定性估计间寻求平衡，当前方法样本效率或安全性有限。

Method: 提出了分布式Bayesian推断框架。机器人各自采用共轭先验，依据观察到的事件间隔为未知的泊松过程参数进行更新；通过置信度动态调整探索行为。

Result: 仿真结果表明，该方法能高效正确地选择安全区域，减少暴露于危险的次数，并在安全性和收敛速度上优于传统启发式方法。

Conclusion: 该方法为集体决策问题提供了新基准，并可应用于危险、动态环境下的自适应风险感知采样与探索。

Abstract: Effective collective decision-making in swarm robotics often requires balancing exploration, communication and individual uncertainty estimation, especially in hazardous environments where direct measurements are limited or costly. We propose a decentralized Bayesian framework that enables a swarm of simple robots to identify the safer of two areas, each characterized by an unknown rate of hazardous events governed by a Poisson process. Robots employ a conjugate prior to gradually predict the times between events and derive confidence estimates to adapt their behavior. Our simulation results show that the robot swarm consistently chooses the correct area while reducing exposure to hazardous events by being sample-efficient. Compared to baseline heuristics, our proposed approach shows better performance in terms of safety and speed of convergence. The proposed scenario has potential to extend the current set of benchmarks in collective decision-making and our method has applications in adaptive risk-aware sampling and exploration in hazardous, dynamic environments.

</details>


### [319] [MLATC: Fast Hierarchical Topological Mapping from 3D LiDAR Point Clouds Based on Adaptive Resonance Theory](https://arxiv.org/abs/2511.22238)
*Ryosuke Ofuchi,Yuichiro Toda,Naoki Masuyama,Takayuki Matsuno*

Main category: cs.RO

TL;DR: 本文提出了一种名为Multi-Layer ATC（MLATC）的分层拓扑聚类方法，用于高效地从3D激光雷达点云中构建自主机器人在大规模动态未知环境下的全局拓扑地图，并显著提升了处理速度与扩展性。


<details>
  <summary>Details</summary>
Motivation: 原有ATC-DT方法在处理点云生成拓扑地图时，节点选择需遍历全部已存在节点，导致计算量随地图增大急剧上升，影响了系统的实时性和扩展性。为解决大规模环境下的效率与实时处理问题，作者设计了更高效的结构。

Method: MLATC通过将节点组织为多层次结构，实现由粗到细的分层近邻搜索，显著减少每次查询所需的距离计算次数。其层数可自适应增加，当低层节点饱和时自动加深层级，从而降低手动设定参数的需求。该方法无需预设最大层数，且保持参数量低。

Result: 在合成大规模环境的仿真实验中，MLATC相比原ATC-DT显著加速了拓扑地图构建，且随着节点数增加查询时间近似对数级增长。在真实校园数据集上的实验证明，MLATC每帧运行时间仅为毫秒级，能实现大规模环境下的实时全局拓扑建图，并大幅优于原方法的计算效率。

Conclusion: MLATC有效地解决了原ATC-DT面临的规模扩展性和实时性瓶颈，在大规模3D点云环境下支持毫秒级、实时的全局拓扑地图构建，为自主机器人感知和导航提供了更为高效的基础。

Abstract: This paper addresses the problem of building global topological maps from 3D LiDAR point clouds for autonomous mobile robots operating in large-scale, dynamic, and unknown environments. Adaptive Resonance Theory-based Topological Clustering with Different Topologies (ATC-DT) builds global topological maps represented as graphs while mitigating catastrophic forgetting during sequential processing. However, its winner selection mechanism relies on an exhaustive nearest-neighbor search over all existing nodes, leading to scalability limitations as the map grows. To address this challenge, we propose a hierarchical extension called Multi-Layer ATC (MLATC). MLATC organizes nodes into a hierarchy, enabling the nearest-neighbor search to proceed from coarse to fine resolutions, thereby drastically reducing the number of distance evaluations per query. The number of layers is not fixed in advance. MLATC employs an adaptive layer addition mechanism that automatically deepens the hierarchy when lower layers become saturated, keeping the number of user-defined hyperparameters low. Simulation experiments on synthetic large-scale environments show that MLATC accelerates topological map building compared to the original ATC-DT and exhibits a sublinear, approximately logarithmic scaling of search time with respect to the number of nodes. Experiments on campus-scale real-world LiDAR datasets confirm that MLATC maintains a millisecond-level per-frame runtime and enables real-time global topological map building in large-scale environments, significantly outperforming the original ATC-DT in terms of computational efficiency.

</details>


### [320] [Soft Fluidic Sheet Transistor for Soft Robotic System Enabling Fluid Logic Operations](https://arxiv.org/abs/2511.22318)
*Yuki Origane,Koya Cho,Hideyuki Tsukagoshi*

Main category: cs.RO

TL;DR: 本论文提出一种软质聚氨酯片状气阀及其放大器，能够仅用气动信号实现逻辑运算，推动软体机器人在功能性和柔性上的提升。


<details>
  <summary>Details</summary>
Motivation: 为了实现软体机器人的高功能性与高柔性，现有气动逻辑元件体积大、集成度低，难以满足复杂控制需求，因此需要新型、轻薄且可集成的气动逻辑元件。

Method: 作者设计了一种软质聚氨酯片状阀门（fluidic sheet transistor, FST），利用其结构在受控加压时主通道会轴向压缩和挠曲，实现阻断及逻辑功能。通过组合多个FST，可以在一张软片上实现NOT、NAND、NOR等逻辑操作，并给出制造流程及逻辑配置方法。

Result: 实验证明，该软片阀门可完成多种气动逻辑功能，且只用单一气源便能驱动软体机器人实现障碍自适应动作，同时展示了FST在自保持锁存电路及流体检测应用中的有效性。

Conclusion: 论文验证了FST的原理和实用性，证明了仅用气压即可在软机器人系统中实现高集成度逻辑运算和自适应行为，为软体机器人气动回路的轻量化和模块化提供了创新途径。

Abstract: Aiming to achieve both high functionality and flexibility in soft robot system, this paper presents a soft urethane sheet-like valve with an amplifier that can perform logical operations using only pneumatic signals. When the control chamber in the valve is pressurized, the main path is compressed along its central axis, buckling and being pressed,resulting in blockage. This allows control by a pressure signal smaller than that within the main channel. Furthermore, similar to transistors in electrical circuits, when combined, the proposed valve can perform a variety of logical operations. The basic type operates as a NOT logic element, which is named the fluidic sheet transistor (FST). By integrating multiple FSTs, logical operations such as positive logic, NAND, and NOR can be performed on a single sheet. This paper describes the operating principle, fabrication method, and characteristics of the FST,followed by a method for configuring logical operations.Moreover, we demonstrate the construction of a latch circuit(self-holding logic circuit) using FST, introducing a prototype of a fluid robot system that combines a tactile tube as a fluidic detector and fluid actuators. This demonstrates that it is possible to generate behavior that actively changes posture when hitting an obstacle using only air pressure from a single pipe, which verifies the effectiveness of the proposed methods.

</details>


### [321] [Nonholonomic Narrow Dead-End Escape with Deep Reinforcement Learning](https://arxiv.org/abs/2511.22338)
*Denghan Xiong,Yanzhe Zhao,Yutong Chen,Zichun Wang*

Main category: cs.RO

TL;DR: 本文针对Ackermann转向车辆（如常见的汽车）如何在有非完整约束且窄死胡同环境下逃脱的问题，提出生成-训练-评估一体的智能规划方法，并在多个维度优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 非完整约束（如Ackermann转向车辆只能前进或倒退，并有限制曲率，不能原地转向）导致常规的采样-连接类型路径规划方法在窄死胡同情形下效率低、难以找到可行解。因此需要更适合此类约束与狭窄场景的专门方法。

Method: 1) 首先设计可生成兼容Ackermann运动学的多段前进-倒退轨迹，并可用于合成带保守可行性保证的窄死胡同家族；2) 基于此合成训练环境，且严格遵循车辆运动约束，采用soft actor-critic深度强化学习进行策略训练；3) 与典型的结合全局搜索与非完整运动控制的经典规划器在统一约束和感知下做全面对比。

Result: 实验显示，所学到的策略能在更多实例下找到可行路径，操作次数减少，且路径长度和规划时间与经典方法持平，均在相同传感与控制条件下测试。

Conclusion: 针对非完整车辆窄死胡同逃脱问题，基于生成-强化学习的规划器在成功率和效率上明显优于传统方法，展示了学习方法处理高约束运动学复杂性场景的潜力。

Abstract: Nonholonomic constraints restrict feasible velocities without reducing configuration-space dimension, which makes collision-free geometric paths generally non-executable for car-like robots. Ackermann steering further imposes curvature bounds and forbids in-place rotation, so escaping from narrow dead ends typically requires tightly sequenced forward and reverse maneuvers. Classical planners that decouple global search and local steering struggle in these settings because narrow passages occupy low-measure regions and nonholonomic reachability shrinks the set of valid connections, which degrades sampling efficiency and increases sensitivity to clearances. We study nonholonomic narrow dead-end escape for Ackermann vehicles and contribute three components. First, we construct a generator that samples multi-phase forward-reverse trajectories compatible with Ackermann kinematics and inflates their envelopes to synthesize families of narrow dead ends that are guaranteed to admit at least one feasible escape. Second, we construct a training environment that enforces kinematic constraints and train a policy using the soft actor-critic algorithm. Third, we evaluate against representative classical planners that combine global search with nonholonomic steering. Across parameterized dead-end families, the learned policy solves a larger fraction of instances, reduces maneuver count, and maintains comparable path length and planning time while under the same sensing and control limits. We provide our project as open source at https://github.com/gitagitty/cisDRL-RobotNav.git

</details>


### [322] [LLM-Based Generalizable Hierarchical Task Planning and Execution for Heterogeneous Robot Teams with Event-Driven Replanning](https://arxiv.org/abs/2511.22354)
*Suraj Borate,Bhavish Rai B,Vipul Pardeshi,Madhu Vadali*

Main category: cs.RO

TL;DR: 本文提出了CoMuRoS，一个面向异构多机器人团队的通用分层架构，将中心化规划与分布式执行结合，并支持事件驱动的任务重规划，极大提升了多机器人协作的灵活性和健壮性。


<details>
  <summary>Details</summary>
Motivation: 现有多机器人系统在处理动态任务分配、应对突发事件和人机协作方面普遍缺乏灵活性和鲁棒性。本文旨在通过引入层次化架构和大模型驱动的任务管理，提升机器人团队的任务适应性和自主协作能力。

Method: 系统架构分为中央任务管理器和分布式机器人客户端。任务管理器基于大语言模型（LLM），理解自然语言目标并结合上下文动态分解与分配子任务。每台机器人本地运行LLM，将原语能力编译为可执行代码，同时配合VLM或图像处理实现自动事件监控与分类产生，支持失败重规划和人类辅助。论文通过硬件和仿真实验系统评估了系统性能，并设计了文本基准集进行多维度测评。

Result: 硬件实验展示了机器人对突发事件的自主恢复（物体回收任务协作成功率9/10，运输协作8/8，人为辅助恢复5/5），能有效过滤无关干扰，实现紧密协作；仿真实验证明了系统能根据意图动态重规划。基准测试多个维度得分均较高（如任务正确率最高0.91，重规划场景正确率达1.0）。

Conclusion: CoMuRoS在现实物理机器人上率先展示了运行时、事件驱动重规划，显著提升了多机器人及人机混合协作的健壮性与灵活性，功能和表现优于以往基于大模型的系统。

Abstract: This paper introduces CoMuRoS (Collaborative Multi-Robot System), a generalizable hierarchical architecture for heterogeneous robot teams that unifies centralized deliberation with decentralized execution, and supports event-driven replanning. A Task Manager LLM interprets natural-language goals, classifies tasks, and allocates subtasks using static rules plus dynamic contexts (task, history, robot and task status, and events).Each robot runs a local LLM that composes executable Python code from primitive skills (ROS2 nodes, policies), while onboard perception (VLMs/image processing) continuously monitors events and classifies them into relevant or irrelevant to the task. Task failures or user intent changes trigger replanning, allowing robots to assist teammates, resume tasks, or request human help. Hardware studies demonstrate autonomous recovery from disruptive events, filtering of irrelevant distractions, and tightly coordinated transport with emergent human-robot cooperation (e.g., multirobot collaborative object recovery success rate: 9/10, coordinated transport: 8/8, human-assisted recovery: 5/5).Simulation studies show intention-aware replanning. A curated textual benchmark spanning 22 scenarios (3 tasks each, around 20 robots) evaluates task allocation, classification, IoU, executability, and correctness, with high average scores (e.g., correctness up to 0.91) across multiple LLMs, a separate replanning set (5 scenarios) achieves 1.0 correctness. Compared with prior LLM-based systems, CoMuRoS uniquely demonstrates runtime, event-driven replanning on physical robots, delivering robust, flexible multi-robot and human-robot collaboration.

</details>


### [323] [BINDER: Instantly Adaptive Mobile Manipulation with Open-Vocabulary Commands](https://arxiv.org/abs/2511.22364)
*Seongwon Cho,Daechul Ahn,Donghyun Shin,Hyeonbeom Choi,San Kim,Jonghyun Choi*

Main category: cs.RO

TL;DR: 本文提出了BINDER框架，实现了移动机器人在开放词汇任务下对动态环境的持续感知和及时规划，大幅提升操作成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有移动操控方法在环境表示更新上存在盲区，只在导航目标或动作结尾等离散时刻更新世界模型，导致机器人容易漏检物体、延迟发现错误、迟滞重新规划，降低了动态环境下的鲁棒性。

Method: BINDER采用双模块架构：策略规划由多模态大模型（DRM）负责，持续环境监控由视频大模型（IRM）负责。DRM管理任务规划与高层场景表示，并动态指导IRM关注重点；IRM则分析实时视频流，更新记忆、修正行动并在出现异常时触发重规划。两者协同，实现高效、及时的感知与决策。

Result: 在三个真实世界、包含动态物体变化的环境中，BINDER相比当前最先进的基线方法表现出明显更高的任务完成率和操作效率，且具备良好的泛化能力。

Conclusion: BINDER有效解决了传统方法在动态环境中的感知与反馈滞后问题，提升了移动机器人的适应性和任务鲁棒性，具备实际落地价值。

Abstract: Open-vocabulary mobile manipulation (OVMM) requires robots to follow language instructions, navigate, and manipulate while updating their world representation under dynamic environmental changes. However, most prior approaches update their world representation only at discrete update points such as navigation targets, waypoints, or the end of an action step, leaving robots blind between updates and causing cascading failures: overlooked objects, late error detection, and delayed replanning. To address this limitation, we propose BINDER (Bridging INstant and DEliberative Reasoning), a dual process framework that decouples strategic planning from continuous environment monitoring. Specifically, BINDER integrates a Deliberative Response Module (DRM, a multimodal LLM for task planning) with an Instant Response Module (IRM, a VideoLLM for continuous monitoring). The two modules play complementary roles: the DRM performs strategic planning with structured 3D scene updates and guides what the IRM attends to, while the IRM analyzes video streams to update memory, correct ongoing actions, and trigger replanning when necessary. Through this bidirectional coordination, the modules address the trade off between maintaining awareness and avoiding costly updates, enabling robust adaptation under dynamic conditions. Evaluated in three real world environments with dynamic object placement, BINDER achieves substantially higher success and efficiency than SoTA baselines, demonstrating its effectiveness for real world deployment.

</details>


### [324] [Visual-Geometry Diffusion Policy: Robust Generalization via Complementarity-Aware Multimodal Fusion](https://arxiv.org/abs/2511.22445)
*Yikai Tang,Haoran Geng,Sheng Zang,Pieter Abbeel,Jitendra Malik*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的模仿学习框架VGDP，通过有效融合RGB和点云信息，实现了在各种视觉与空间变化下的高鲁棒性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在空间和视觉随机化情况下常出现过拟合，泛化能力较弱，亟需一种能更好应对多模态输入和扰动的新方案。

Method: 提出“Visual Geometry Diffusion Policy (VGDP)”，核心为“互补感知融合模块”，通过模态级Dropout平衡RGB与点云特征利用，交叉注意力仅作为轻量交互层，提升视觉-几何信息互补表达。

Result: 在18个仿真任务和4个真实任务上，VGDP平均性能提升39.1%；在变化视觉环境下提升41.5%，空间设定变化下提升15.2%，均优于七种主流基线方法。

Conclusion: VGDP能有效利用多模态信息融合，显著提升模仿学习的泛化能力和鲁棒性，对视觉和空间扰动有很强适应力。

Abstract: Imitation learning has emerged as a crucial ap proach for acquiring visuomotor skills from demonstrations, where designing effective observation encoders is essential for policy generalization. However, existing methods often struggle to generalize under spatial and visual randomizations, instead tending to overfit. To address this challenge, we propose Visual Geometry Diffusion Policy (VGDP), a multimodal imitation learning framework built around a Complementarity-Aware Fusion Module where modality-wise dropout enforces balanced use of RGB and point-cloud cues, with cross-attention serving only as a lightweight interaction layer. Our experiments show that the expressiveness of the fused latent space is largely induced by the enforced complementarity from modality-wise dropout, with cross-attention serving primarily as a lightweight interaction mechanism rather than the main source of robustness. Across a benchmark of 18 simulated tasks and 4 real-world tasks, VGDP outperforms seven baseline policies with an average performance improvement of 39.1%. More importantly, VGDP demonstrates strong robustness under visual and spatial per turbations, surpassing baselines with an average improvement of 41.5% in different visual conditions and 15.2% in different spatial settings.

</details>


### [325] [RealD$^2$iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion](https://arxiv.org/abs/2511.22505)
*Xiujian Liang,Jiacheng Liu,Mingyang Sun,Qichen He,Cewu Lu,Jianhua Sun*

Main category: cs.RO

TL;DR: 该论文提出了一种新颖的深度图像噪声合成方法，有效弥合了机器人视觉领域仿真到现实（sim2real）的差距，提升了现实世界机器人操控的表现。


<details>
  <summary>Details</summary>
Motivation: 现实中机器人操控受限于仿真与真实视觉（尤其是深度图像）之间的差距，主要因为仿真深度图像缺乏真实传感器的复杂噪声分布。缺乏匹配真实噪声的深度数据成为提升仿真到现实迁移能力的瓶颈。

Method: 作者提出了一种反向思路：用扩散模型从干净的深度图像合成带有真实噪声的深度图像。提出层次化的coarse-to-fine扩散框架RealD$^2$iff，将深度噪声分解为全局结构失真和局部细粒度扰动，并针对性设计了Frequency-Guided Supervision（FGS）和Discrepancy-Guided Optimization（DGO）来分别学习全局和局部噪声。此外，构建了六阶段流程以融入模仿学习。

Result: 通过大量实验验证了所提方法的有效性，包括仿真和现实性能测试。RealD$^2$iff可直接生成高仿真实世界噪声深度数据对，无需实际手动采集，并显著提升了无需再微调的机器人仿真到现实任务表现。

Conclusion: 论文提出的RealD$^2$iff不仅为仿真-现实视觉迁移提供了高效工具，也为数据集合成和零样本机器人操控打开了新思路，显著减少现实数据采集和标注成本，推动机器人学习技术落地应用。

Abstract: Robot manipulation in the real world is fundamentally constrained by the visual sim2real gap, where depth observations collected in simulation fail to reflect the complex noise patterns inherent to real sensors. In this work, inspired by the denoising capability of diffusion models, we invert the conventional perspective and propose a clean-to-noisy paradigm that learns to synthesize noisy depth, thereby bridging the visual sim2real gap through purely simulation-driven robotic learning. Building on this idea, we introduce RealD$^2$iff, a hierarchical coarse-to-fine diffusion framework that decomposes depth noise into global structural distortions and fine-grained local perturbations. To enable progressive learning of these components, we further develop two complementary strategies: Frequency-Guided Supervision (FGS) for global structure modeling and Discrepancy-Guided Optimization (DGO) for localized refinement. To integrate RealD$^2$iff seamlessly into imitation learning, we construct a pipeline that spans six stages. We provide comprehensive empirical and experimental validation demonstrating the effectiveness of this paradigm. RealD$^2$iff enables two key applications: (1) generating real-world-like depth to construct clean-noisy paired datasets without manual sensor data collection. (2) Achieving zero-shot sim2real robot manipulation, substantially improving real-world performance without additional fine-tuning.

</details>


### [326] [BUDD-e: an autonomous robotic guide for visually impaired users](https://arxiv.org/abs/2511.22541)
*Jinyang Li,Marcello Farina,Luca Mozzarelli,Luca Cattaneo,Panita Rattamasanaprapai,Eleonora A. Tagarelli,Matteo Corno,Paolo Perego,Giuseppe Andreoni,Emanuele Lettieri*

Main category: cs.RO

TL;DR: 本文介绍了一种新型为视障人士设计的引导机器人BUDD-e的原型设计和实现，并通过真实环境下的志愿者测试验证了其有效性和用户接受度。


<details>
  <summary>Details</summary>
Motivation: 现有的辅助视障人士出行工具存在局限，因此亟需一种智能且更具适应性的辅助设备，提升视障人士的独立出行能力和安全性。本文的目的是开发一种能够在实际环境中有效工作的引导机器人。

Method: 研究团队设计和实现了BUDD-e引导机器人原型，并在米兰Niguarda医院实际环境中邀请视障志愿者进行了系统实验测试，收集用户反馈和机器人表现数据进行评估。

Result: 实验结果显示，BUDD-e机器人在真实场景下表现优异，具备良好的引导能力，并获得了用户的高度接受和正面评价。

Conclusion: BUDD-e机器人为视障人士提供了有效、可行且受欢迎的辅助出行方案，相关成果为未来推广和实际应用奠定了基础。

Abstract: This paper describes the design and the realization of a prototype of the novel guide robot BUDD-e for visually impaired users. The robot has been tested in a real scenario with the help of visually disabled volunteers at ASST Grande Ospedale Metropolitano Niguarda, in Milan. The results of the experimental campaign are throughly described in the paper, displaying its remarkable performance and user-acceptance.

</details>


### [327] [Beyond Success: Refining Elegant Robot Manipulation from Mixed-Quality Data via Just-in-Time Intervention](https://arxiv.org/abs/2511.22555)
*Yanbo Mao,Jianlong Fu,Ruoxuan Zhang,Hongxia Xie,Meibao Yao*

Main category: cs.RO

TL;DR: 本文针对视觉-语言-动作（VLA）模型在机器人操作中的执行质量不稳定问题提出了LIBERO-Elegant基准，并通过“优雅执行”原则提升行为质量，无需重新训练原有策略。


<details>
  <summary>Details</summary>
Motivation: 以VLA模型为代表的通用型机器人操作虽取得进展，却存在执行质量波动，主要由于人类演示数据质量参差。作者希望为机器人的行为质量建立系统的度量与改进机制。

Method: 1）提出LIBERO-Elegant基准，明确操作执行质量的评价标准。2）将优雅执行形式化为隐式任务约束（ITCs）的满足。3）利用离线校准Q-Learning训练出Elegance Critic，评估备选动作的期望质量。4）推理时以“适时干预”(JITI)机制，仅在决策关键点依据Critic信心进行动作修正，无需修改原始VLA策略。

Result: 在LIBERO-Elegant以及真实操作任务上，Elegance Critic显著提升了动作执行质量，且对新任务同样有效。

Conclusion: 所提模型不仅关注任务是否成功，还提升了执行的方式与质量，为机器人控制增加了对“行为优雅性”的重视。

Abstract: Vision-Language-Action (VLA) models have enabled notable progress in general-purpose robotic manipulation, yet their learned policies often exhibit variable execution quality. We attribute this variability to the mixed-quality nature of human demonstrations, where the implicit principles that govern how actions should be carried out are only partially satisfied. To address this challenge, we introduce the LIBERO-Elegant benchmark with explicit criteria for evaluating execution quality. Using these criteria, we develop a decoupled refinement framework that improves execution quality without modifying or retraining the base VLA policy. We formalize Elegant Execution as the satisfaction of Implicit Task Constraints (ITCs) and train an Elegance Critic via offline Calibrated Q-Learning to estimate the expected quality of candidate actions. At inference time, a Just-in-Time Intervention (JITI) mechanism monitors critic confidence and intervenes only at decision-critical moments, providing selective, on-demand refinement. Experiments on LIBERO-Elegant and real-world manipulation tasks show that the learned Elegance Critic substantially improves execution quality, even on unseen tasks. The proposed model enables robotic control that values not only whether tasks succeed, but also how they are performed.

</details>


### [328] [Deadlock-Free Hybrid RL-MAPF Framework for Zero-Shot Multi-Robot Navigation](https://arxiv.org/abs/2511.22685)
*Haoyi Wang,Licheng Luo,Yiannis Kantaros,Bruno Sinopoli,Mingyu Cai*

Main category: cs.RO

TL;DR: 本文提出了一种将强化学习（RL）与多智能体路径规划（MAPF）结合的多机器人导航方法，用以解决复杂环境下的死锁与碰撞问题，实现了近乎完美的任务完成率。


<details>
  <summary>Details</summary>
Motivation: 多机器人在有障碍、狭窄通道等复杂场景中导航，容易因死锁或无法泛化新环境而任务失败。现有RL方法在遇到分布外配置时表现不佳，因此需要一种能兼顾反应性和全局协调的方法。

Method: 提出一个混合架构：1）RL实现日常反应式导航；2）引入安全层监控死锁；3）一旦发现死锁，则触发MAPF协调控制，为受影响机器人规划全局可行路径，动态调节航点以减小冲突。

Result: 在多智能体密集测试基准中，该方法显著提升了任务完成率，从“边缘化”提升到“近乎全部成功”，有效减少了死锁与碰撞。同时，结合分层任务规划可支持异质机器人协同导航，具备零样本泛化能力。

Conclusion: 通过将反应式RL导航与按需MAPF调度结合，该方法实现了稳定、泛化性强和鲁棒的多机器人零样本导航性能，有效解决了复杂环境中的死锁和任务失败问题。

Abstract: Multi-robot navigation in cluttered environments presents fundamental challenges in balancing reactive collision avoidance with long-range goal achievement. When navigating through narrow passages
  or confined spaces, deadlocks frequently emerge that prevent agents from reaching their destinations, particularly when Reinforcement Learning (RL) control policies encounter novel configurations out of learning distribution. Existing RL-based approaches suffer from limited generalization capability in unseen environments. We propose a hybrid framework that seamlessly integrates RL-based reactive navigation with on-demand Multi-Agent Path Finding (MAPF) to explicitly resolve topological deadlocks. Our approach integrates a safety layer that monitors agent progress to detect deadlocks and, when detected, triggers a coordination controller for affected agents. The framework constructs globally feasible trajectories via MAPF and regulates waypoint progression to reduce inter-agent conflicts during navigation.
  Extensive evaluation on dense multi-agent benchmarks shows that our method boosts task completion from marginal to near-universal success, markedly reducing deadlocks and collisions. When integrated with hierarchical task planning, it enables coordinated navigation for heterogeneous robots, demonstrating that coupling reactive RL navigation with selective MAPF intervention yields a robust, zero-shot performance.

</details>


### [329] [Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations](https://arxiv.org/abs/2511.22697)
*Chancharik Mitra,Yusen Luo,Raj Saravanan,Dantong Niu,Anirudh Pai,Jesse Thomason,Trevor Darrell,Abrar Anwar,Deva Ramanan,Roei Herzig*

Main category: cs.RO

TL;DR: 本文提出了一种名为Robotic Steering的微调方法，通过识别并定向微调与具体任务相关的模型稀疏表示（如注意力头），显著提升了机器人领域Vision-Language Action模型的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的VLMs在视觉-语言任务上表现卓越，但在机器人任务中，由于具体的物理环境、机器人形态及任务空间关系的不同，这些模型需要更为细致的微调。当前微调方法未能针对任务的视觉、语言和物理特性作出差异化调整，导致适应性和效率不足。受神经科学中功能特异性的启发，作者希望通过更加任务特定的方式提升微调效果。

Method: 作者提出了Robotic Steering方法，首先通过极少量示范（few-shot demonstrations）结合可解释性分析，定位出与任务高度相关的注意力头，然后仅对这些特定参数进行微调，从而在满足任务要求的同时提升模型解释性与适应性，降低计算消耗。

Result: 在Franka Emika机器人手臂实地评测中，Robotic Steering方法在适应多样化机器人任务方面表现优于现有的LoRA方法。不仅鲁棒性更高，对任务切换适应更出色，同时计算成本也更低且可解释性更强。

Conclusion: Robotic Steering方法证明了通过任务特定的稀疏参数微调，在提升VLA模型适应多变机器人任务能力的同时，也优化了效率和可解释性，为视觉-语言-动作一体化模型在机器人领域的落地应用提供了有效路径。

Abstract: Vision-Language Action (VLAs) models promise to extend the remarkable success of vision-language models (VLMs) to robotics. Yet, unlike VLMs in the vision-language domain, VLAs for robotics require finetuning to contend with varying physical factors like robot embodiment, environment characteristics, and spatial relationships of each task. Existing fine-tuning methods lack specificity, adapting the same set of parameters regardless of a task's visual, linguistic, and physical characteristics. Inspired by functional specificity in neuroscience, we hypothesize that it is more effective to finetune sparse model representations specific to a given task. In this work, we introduce Robotic Steering, a finetuning approach grounded in mechanistic interpretability that leverages few-shot demonstrations to identify and selectively finetune task-specific attention heads aligned with the physical, visual, and linguistic requirements of robotic tasks. Through comprehensive on-robot evaluations with a Franka Emika robot arm, we demonstrate that Robotic Steering outperforms LoRA while achieving superior robustness under task variation, reduced computational cost, and enhanced interpretability for adapting VLAs to diverse robotic tasks.

</details>


### [330] [A Two Degrees-of-Freedom Floor-Based Robot for Transfer and Rehabilitation Applications](https://arxiv.org/abs/2511.22705)
*Ian Lalonde,Jeff Denis,Mathieu Lamy,Camille Martin,Karina Lebel,Alexandre Girard*

Main category: cs.RO

TL;DR: 本文提出并测试了一种能根据不同训练需求灵活调整阻抗和辅助力的坐到站立训练装置，实现座椅辅助与康复训练功能兼备。


<details>
  <summary>Details</summary>
Motivation: 现有的坐到站立辅助器械（如移乘设备和部分体重支撑康复设备）难以根据使用者不同的活动能力灵活调整辅助方式，无法充分满足多样化的康复和训练需求。

Method: 开发了一种新型STS训练装置，可以调节阻抗和垂直/前向辅助力，兼容商业移乘辅助功能。通过招募不同身高和体重的健康志愿者进行实验，评估其对自然动作学的影响和辅助效果。

Result: 实验结果表明，该装置对自然坐到站立运动动作学影响较小；能够精准地在用户身体重心处实现重量卸载；并可以通过虚拟弹簧机制在动作起始时帮助用户将重心前移，从而帮助顺利站起。

Conclusion: 所开发的STS装置在满足移乘辅助的基础上，兼具灵活的康复训练能力，能更好地适应不同训练与康复需求，有助于提升功能性活动水平和降低再住院风险。

Abstract: The ability to accomplish a sit-to-stand (STS) motion is key to increase functional mobility and reduce rehospitalization risks. While raising aid (transfer) devices and partial bodyweight support (rehabilitation) devices exist, both are unable to adjust the STS training to different mobility levels. Therefore, We have developed an STS training device that allows various configurations of impedance and vertical/forward forces to adapt to many training needs while maintaining commercial raising aid transfer capabilities. Experiments with healthy adults (both men and women) of various heights and weights show that the device 1) has a low impact on the natural STS kinematics, 2) can provide precise weight unloading at the patient's center of mass and 3) can add a forward virtual spring to assist the transfer of the bodyweight to the feet for seat-off, at the start of the STS motion.

</details>


### [331] [Beyond Egocentric Limits: Multi-View Depth-Based Learning for Robust Quadrupedal Locomotion](https://arxiv.org/abs/2511.22744)
*Rémy Rahem,Wael Suleiman*

Main category: cs.RO

TL;DR: 该论文提出了一种结合第一人称与第三人称深度图像的新型多视角腿式机器人运动感知框架，提升了机器人在动态环境下的运动能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有腿式机器人动态运动感知多依赖自身传感器，遇视野遮挡时表现受限；引入外部视角能提升环境感知，有助于机器人应对复杂动态任务。

Method: 设计了融合自身体感与内外部深度图的运动策略，通过教师-学生蒸馏方法训练学生策略，在感知异常和外部摄像头丢失时保持鲁棒性。同时采用了摄像头随机失效、3D位置扰动等多种域随机化技术以提升策略泛化能力。

Result: 多视角策略在越障、下台阶等动态动作中优于单视角基线模型，并且即使外部摄像头受限也能保持稳定性。实验还表明在训练阶段加入适度视角错位有助于增强模型容忍度。

Conclusion: 异质化视觉反馈能有效提升四足机器人运动的鲁棒性和灵敏性，为动态环境下的自主运动技术带来新的提升。

Abstract: Recent progress in legged locomotion has allowed highly dynamic and parkour-like behaviors for robots, similar to their biological counterparts. Yet, these methods mostly rely on egocentric (first-person) perception, limiting their performance, especially when the viewpoint of the robot is occluded. A promising solution would be to enhance the robot's environmental awareness by using complementary viewpoints, such as multiple actors exchanging perceptual information. Inspired by this idea, this work proposes a multi-view depth-based locomotion framework that combines egocentric and exocentric observations to provide richer environmental context during agile locomotion. Using a teacher-student distillation approach, the student policy learns to fuse proprioception with dual depth streams while remaining robust to real-world sensing imperfections. To further improve robustness, we introduce extensive domain randomization, including stochastic remote-camera dropouts and 3D positional perturbations that emulate aerial-ground cooperative sensing. Simulation results show that multi-viewpoints policies outperform single-viewpoint baseline in gap crossing, step descent, and other dynamic maneuvers, while maintaining stability when the exocentric camera is partially or completely unavailable. Additional experiments show that moderate viewpoint misalignment is well tolerated when incorporated during training. This study demonstrates that heterogeneous visual feedback improves robustness and agility in quadrupedal locomotion. Furthermore, to support reproducibility, the implementation accompanying this work is publicly available at https://anonymous.4open.science/r/multiview-parkour-6FB8

</details>


### [332] [CAPE: Context-Aware Diffusion Policy Via Proximal Mode Expansion for Collision Avoidance](https://arxiv.org/abs/2511.22773)
*Rui Heng Yang,Xuan Zhao,Leo Maxime Brunswic,Montgomery Alban,Mateo Clemente,Tongtong Cao,Jun Jin,Amir Rasouli*

Main category: cs.RO

TL;DR: 本文提出了CAPE方法，用于解决机器人模仿学习中数据量不足导致的泛化能力有限问题，通过引入具备上下文信息的扩模与迭代引导，提升机器人在未见环境下的避障和任务完成表现。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在机器人模仿学习中需要大量数据覆盖各种障碍物和空间配置，数据采集成本高且难以实现，尤其是在避障等复杂任务中，导致泛化能力不足。

Method: CAPE方法通过“先生成初始轨迹—执行一段前缀—对剩余轨迹引入噪声形成上下文感知先验—迭代引导去噪”逐步扩展轨迹模式分布。在推理阶段，结合任务上下文以及避障信息，有效引导轨迹规划，提升样本多样性和碰撞规避能力。

Result: CAPE在仿真和现实场景下的不同操控避障任务中，相较最先进方法，成功率提升高达26%（仿真）和80%（现实），展现出优越的泛化能力和任务完成率。

Conclusion: CAPE有效提升了扩散模型在机器人模仿学习中的泛化能力与避障性能，显著减少了数据获取成本，对在未见环境下的实际任务推进具有重要意义。

Abstract: In robotics, diffusion models can capture multi-modal trajectories from demonstrations, making them a transformative approach in imitation learning. However, achieving optimal performance following this regiment requires a large-scale dataset, which is costly to obtain, especially for challenging tasks, such as collision avoidance. In those tasks, generalization at test time demands coverage of many obstacles types and their spatial configurations, which are impractical to acquire purely via data. To remedy this problem, we propose Context-Aware diffusion policy via Proximal mode Expansion (CAPE), a framework that expands trajectory distribution modes with context-aware prior and guidance at inference via a novel prior-seeded iterative guided refinement procedure. The framework generates an initial trajectory plan and executes a short prefix trajectory, and then the remaining trajectory segment is perturbed to an intermediate noise level, forming a trajectory prior. Such a prior is context-aware and preserves task intent. Repeating the process with context-aware guided denoising iteratively expands mode support to allow finding smoother, less collision-prone trajectories. For collision avoidance, CAPE expands trajectory distribution modes with collision-aware context, enabling the sampling of collision-free trajectories in previously unseen environments while maintaining goal consistency. We evaluate CAPE on diverse manipulation tasks in cluttered unseen simulated and real-world settings and show up to 26% and 80% higher success rates respectively compared to SOTA methods, demonstrating better generalization to unseen environments.

</details>


### [333] [Improving Robotic Manipulation Robustness via NICE Scene Surgery](https://arxiv.org/abs/2511.22777)
*Sajjad Pakdamansavoji,Mozhgan Pourkeshavarz,Adam Sigal,Zhiyuan Li,Rui Heng Yang,Amir Rasouli*

Main category: cs.RO

TL;DR: 本论文提出了一种新的视觉增强方法NICE，通过自然图像修补和编辑提升机器人模仿学习对干扰物的鲁棒性，显著提高了下游任务的表现和安全性。


<details>
  <summary>Details</summary>
Motivation: 机器人在真实环境中操作时，视觉干扰会显著降低表现和安全性。以往方法需额外数据或训练成本，难以直接应用于现有数据集。

Method: NICE框架结合图像生成和大语言模型，对数据中的非目标物体进行替换、重新风格化和移除，保留空间关系和动作标签一致性，无需额外收集数据或自定义训练。

Result: 在真实场景评估中，NICE提升了空间可达性预测准确率超过20%，物体操作成功率提升平均11%，降低了目标混淆与碰撞率，提升了模型鲁棒性和安全性。

Conclusion: NICE有效扩展了现有数据集的多样性，极大提升了仿真到现实的泛化能力，是一种无需额外成本、通用性强的视觉增强方法，对实际机器人任务有重要意义。

Abstract: Learning robust visuomotor policies for robotic manipulation remains a challenge in real-world settings, where visual distractors can significantly degrade performance and safety. In this work, we propose an effective and scalable framework, Naturalistic Inpainting for Context Enhancement (NICE). Our method minimizes out-of-distribution (OOD) gap in imitation learning by increasing visual diversity through construction of new experiences using existing demonstrations. By utilizing image generative frameworks and large language models, NICE performs three editing operations, object replacement, restyling, and removal of distracting (non-target) objects. These changes preserve spatial relationships without obstructing target objects and maintain action-label consistency. Unlike previous approaches, NICE requires no additional robot data collection, simulator access, or custom model training, making it readily applicable to existing robotic datasets.
  Using real-world scenes, we showcase the capability of our framework in producing photo-realistic scene enhancement. For downstream tasks, we use NICE data to finetune a vision-language model (VLM) for spatial affordance prediction and a vision-language-action (VLA) policy for object manipulation. Our evaluations show that NICE successfully minimizes OOD gaps, resulting in over 20% improvement in accuracy for affordance prediction in highly cluttered scenes. For manipulation tasks, success rate increases on average by 11% when testing in environments populated with distractors in different quantities. Furthermore, we show that our method improves visual robustness, lowering target confusion by 6%, and enhances safety by reducing collision rate by 7%.

</details>


### [334] [Distracted Robot: How Visual Clutter Undermine Robotic Manipulation](https://arxiv.org/abs/2511.22780)
*Amir Rasouli,Montgomery Alban,Sajjad Pakdamansavoji,Zhiyuan Li,Zhanguang Zhang,Aaron Wu,Xuan Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种从心理物理学角度出发的评估协议，用于系统性地衡量机器人抓取操作在混乱环境下的表现，并通过统一杂乱度指标深入分析了现有视觉-语言-动作模型的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有对机器人操作策略评估缺乏对复杂场景（如物体混乱、遮挡等环境因素）的系统考量，特别是杂乱程度对策略表现产生的重要影响。因此亟需提出更科学、统一的杂乱度度量，全面评估不同策略的鲁棒性。

Method: 作者基于心理物理学统一量化了杂乱程度，包括环境因素、干扰物数量、特征及布局。以此为依据，系统性地设计了仿真及现实评测场景，对多种视觉-语言-动作（VLA）模型进行广泛实验，分析其在不同杂乱度下的性能变化，并细致考察了干扰物数量与遮挡作用。

Result: 实验结果表明，场景杂乱度会显著降低VLA模型的操作性能，最高降幅达34%；不同VLA政策即便任务平均表现相近，但脆弱点存在差异，且在“成功”场景上相互一致性较低。文中还发现杂乱度量指标可有效预测性能下降，增强型数据微调虽有效，但不足以完全解决杂乱带来的所有性能问题。

Conclusion: 本文所提统一杂乱度量为机器人操作策略的评估提供了新视角，能够准确揭示和对比不同策略的脆弱性，为后续提升鲁棒性、改进模型提供了有力工具，同时也表明单纯通过数据增强难以完全克服杂乱环境的挑战。

Abstract: In this work, we propose an evaluation protocol for examining the performance of robotic manipulation policies in cluttered scenes. Contrary to prior works, we approach evaluation from a psychophysical perspective, therefore we use a unified measure of clutter that accounts for environmental factors as well as the distractors quantity, characteristics, and arrangement. Using this measure, we systematically construct evaluation scenarios in both hyper-realistic simulation and real-world and conduct extensive experimentation on manipulation policies, in particular vision-language-action (VLA) models. Our experiments highlight the significant impact of scene clutter, lowering the performance of the policies, by as much as 34% and show that despite achieving similar average performance across the tasks, different VLA policies have unique vulnerabilities and a relatively low agreement on success scenarios. We further show that our clutter measure is an effective indicator of performance degradation and analyze the impact of distractors in terms of their quantity and occluding influence. At the end, we show that finetuning on enhanced data, although effective, does not equally remedy all negative impacts of clutter on performance.

</details>


### [335] [Safe Autonomous Lane Changing: Planning with Dynamic Risk Fields and Time-Varying Convex Space Generation](https://arxiv.org/abs/2511.22829)
*Zhen Tian,Zhihao Lin*

Main category: cs.RO

TL;DR: 本论文提出了一种针对复杂驾驶场景（如自动变道）的新颖轨迹规划方法，将风险感知和碰撞规避集成于统一优化框架中，实验显示其在安全性与效率上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在复杂交通环境下，自动驾驶车辆需要在保证安全的同时高效换道，但现有方法难以同时兼顾风险感知、碰撞规避和轨迹平滑等多重要求。

Method: 作者先构建了动态风险场（DRF），量化周围车辆带来的静态和动态碰撞风险；进而生成时变的凸可行空间，确保轨迹运动学可行和安全。轨迹规划被建模为有限时域最优控制问题，并用带约束的iLQR算法求解，联合优化轨迹平滑度、控制力度和风险暴露度，同时严格保证可行性。

Result: 大量仿真表明，该方法可实现无碰撞的变道轨迹，并显著缩短变道距离（28.59米）与时间（2.84秒），加速度平滑。遇到密集环岛等复杂环境时，相比APF、MPC和RRT基线方法，本方法具备更大的安全余量、更小冲击和更优的曲率平滑性。

Conclusion: 集成了动态风险场与凸可行空间约束、采用iLQR方法的本规划器，可在动态交互交通场景下兼顾安全、高效和舒适，实现平衡的轨迹生成方案。

Abstract: This paper presents a novel trajectory planning pipeline for complex driving scenarios like autonomous lane changing, by integrating risk-aware planning with guaranteed collision avoidance into a unified optimization framework. We first construct a dynamic risk fields (DRF) that captures both the static and dynamic collision risks from surrounding vehicles. Then, we develop a rigorous strategy for generating time-varying convex feasible spaces that ensure kinematic feasibility and safety requirements. The trajectory planning problem is formulated as a finite-horizon optimal control problem and solved using a constrained iterative Linear Quadratic Regulator (iLQR) algorithm that jointly optimizes trajectory smoothness, control effort, and risk exposure while maintaining strict feasibility. Extensive simulations demonstrate that our method outperforms traditional approaches in terms of safety and efficiency, achieving collision-free trajectories with shorter lane-changing distances (28.59 m) and times (2.84 s) while maintaining smooth and comfortable acceleration patterns. In dense roundabout environments the planner further demonstrates robust adaptability, producing larger safety margins, lower jerk, and superior curvature smoothness compared with APF, MPC, and RRT based baselines. These results confirm that the integrated DRF with convex feasible space and constrained iLQR solver provides a balanced solution for safe, efficient, and comfortable trajectory generation in dynamic and interactive traffic scenarios.

</details>


### [336] [Threat-Aware UAV Dodging of Human-Thrown Projectiles with an RGB-D Camera](https://arxiv.org/abs/2511.22847)
*Yuying Zhang,Na Fan,Haowen Zheng,Junning Liang,Zongliang Pan,Qifeng Chen,Ximin Lyu*

Main category: cs.RO

TL;DR: 本文提出了一种基于RGB-D相机的无人机实时规避投掷物攻击系统，通过人体姿态识别与深度信息预测攻击轨迹，实现高效的逃避动作，且在实验证明系统具备高鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 无人机在运输和航拍等任务中易受到人为投掷物的袭击，如何以极低延迟和高度敏捷性有效规避快速突发的攻击行为，是保证无人机安全的重大挑战。

Method: 受棒球投球动作分析理念启发，系统利用RGB-D相机结合人体姿态估计和深度信息，预测投掷者动作及投掷物轨迹，并提出了结合不确定性处理的规避策略，实现对突发投掷物的准确预测和高效规避。

Result: 实验表明：所提感知系统在轨迹预测精度上表现优异，规避距离和响应时延均优于传统方法。同时，规避策略能有效应对时间及空间的不确定性，提升无人机安全性。

Conclusion: 系统在多场景真实测试中展示了优秀的规避能力和高度鲁棒性，为无人机应对突发人为攻击提供了实际可行的解决方案。

Abstract: Uncrewed aerial vehicles (UAVs) performing tasks such as transportation and aerial photography are vulnerable to intentional projectile attacks from humans. Dodging such a sudden and fast projectile poses a significant challenge for UAVs, requiring ultra-low latency responses and agile maneuvers. Drawing inspiration from baseball, in which pitchers' body movements are analyzed to predict the ball's trajectory, we propose a novel real-time dodging system that leverages an RGB-D camera. Our approach integrates human pose estimation with depth information to predict the attacker's motion trajectory and the subsequent projectile trajectory. Additionally, we introduce an uncertainty-aware dodging strategy to enable the UAV to dodge incoming projectiles efficiently. Our perception system achieves high prediction accuracy and outperforms the baseline in effective distance and latency. The dodging strategy addresses temporal and spatial uncertainties to ensure UAV safety. Extensive real-world experiments demonstrate the framework's reliable dodging capabilities against sudden attacks and its outstanding robustness across diverse scenarios.

</details>


### [337] [MARVO: Marine-Adaptive Radiance-aware Visual Odometry](https://arxiv.org/abs/2511.22860)
*Sacchin Sundar,Atman Kikani,Aaliya Alam,Sumukh Shrote,A. Nayeemulla Khan,A. Shahina*

Main category: cs.RO

TL;DR: 本文提出了一种名为MARVO的物理感知与学习集成的水下视觉里程计框架，通过结合物理建模、可微特征匹配及强化学习优化，有效提升浑浊环境下的定位精度。


<details>
  <summary>Details</summary>
Motivation: 水下视觉定位因受波长相关衰减、纹理贫乏和非高斯噪声等影响，导致特征匹配和定位困难。亟需更具物理意识与自适应能力的方法以提升水下定位鲁棒性和精度。

Method: 前端基于Transformer的特征匹配器引入Physics Aware Radiance Adapter以补偿颜色衰减和对比度损失，实现几何一致特征点匹配。半稠密匹配点与IMU和压力传感器数据一同在后端基于因子图（GTSAM）进行融合优化。每个关键帧组合IMU、视觉和气压因素，实现全状态实时估计。此外，提出基于强化学习的姿态优化器用以突破传统最小二乘陷入的局部最优。

Result: 方法在水下浑浊和低纹理环境下提供了更鲁棒与准确的里程计输出，并实现了实时全状态估计。基于强化学习的姿态图优化提升了全局轨迹一致性。

Conclusion: MARVO框架通过物理建模和深度学习的集成显著缓解了水下视觉定位难题，为复杂环境下水下自主导航提供了有效方案。

Abstract: Underwater visual localization remains challenging due to wavelength-dependent attenuation, poor texture, and non-Gaussian sensor noise. We introduce MARVO, a physics-aware, learning-integrated odometry framework that fuses underwater image formation modeling, differentiable matching, and reinforcement-learning optimization. At the front-end, we extend transformer-based feature matcher with a Physics Aware Radiance Adapter that compensates for color channel attenuation and contrast loss, yielding geometrically consistent feature correspondences under turbidity. These semi dense matches are combined with inertial and pressure measurements inside a factor-graph backend, where we formulate a keyframe-based visual-inertial-barometric estimator using GTSAM library. Each keyframe introduces (i) Pre-integrated IMU motion factors, (ii) MARVO-derived visual pose factors, and (iii) barometric depth priors, giving a full-state MAP estimate in real time. Lastly, we introduce a Reinforcement-Learningbased Pose-Graph Optimizer that refines global trajectories beyond local minima of classical least-squares solvers by learning optimal retraction actions on SE(2).

</details>


### [338] [SUPER-AD: Semantic Uncertainty-aware Planning for End-to-End Robust Autonomous Driving](https://arxiv.org/abs/2511.22865)
*Wonjeong Ryu,Seungjun Yu,Seokha Moon,Hojun Choi,Junsung Park,Jinkyu Kim,Hyunjung Shim*

Main category: cs.RO

TL;DR: 本文提出了一种相机-only的端到端（E2E）自动驾驶方法，能够在BEV空间中直接建模并利用不确定性，从而显著提升安全性和可靠性，在NAVSIM基准测试上取得了最佳表现。


<details>
  <summary>Details</summary>
Motivation: 现有E2E自动驾驶方法对感知结果假定过于理想，无法有效处理场景中的不确定性，尤其是在感知模糊或观测不完全时，影响安全与可解释性。

Method: 提出一种能在鸟瞰视角（BEV）下直接估算并利用alectoric不确定性的E2E自动驾驶框架；结合了不确定性感知驱动性地图、像素级几何与语义结构表示，并引入了遵循车道规则的正则项，提高了计划的稳定性与合理性。

Result: 在NAVSIM基准和其挑战子集NAVHARD及NAVSAFE上，该方法达到了最新最优的性能，显著优于现有方法，表现出更高的安全性与鲁棒性。

Conclusion: 将不确定性估计与驾驶先验结合，可显著提升基于相机的E2E自动驾驶系统在复杂和不确定场景下的表现，提高了安全性和可解释性。

Abstract: End-to-End (E2E) planning has become a powerful paradigm for autonomous driving, yet current systems remain fundamentally uncertainty-blind. They assume perception outputs are fully reliable, even in ambiguous or poorly observed scenes, leaving the planner without an explicit measure of uncertainty. To address this limitation, we propose a camera-only E2E framework that estimates aleatoric uncertainty directly in BEV space and incorporates it into planning. Our method produces a dense, uncertainty-aware drivability map that captures both semantic structure and geometric layout at pixel-level resolution. To further promote safe and rule-compliant behavior, we introduce a lane-following regularization that encodes lane structure and traffic norms. This prior stabilizes trajectory planning under normal conditions while preserving the flexibility needed for maneuvers such as overtaking or lane changes. Together, these components enable robust and interpretable trajectory planning, even under challenging uncertainty conditions. Evaluated on the NAVSIM benchmark, our method achieves state-of-the-art performance, delivering substantial gains on both the challenging NAVHARD and NAVSAFE subsets. These results demonstrate that our principled aleatoric uncertainty modeling combined with driving priors significantly advances the safety and reliability of camera-only E2E autonomous driving.

</details>


### [339] [Seeing before Observable: Potential Risk Reasoning in Autonomous Driving via Vision Language Models](https://arxiv.org/abs/2511.22928)
*Jiaxin Liu,Xiangyu Yan,Liang Peng,Lei Yang,Lingjun Zhang,Yuechen Luo,Yueming Tao,Ashton Yu Xuan Tan,Mu Li,Lei Zhang,Ziqi Zhan,Sai Guo,Hong Wang,Jun Li*

Main category: cs.RO

TL;DR: 本文提出了PotentialRiskQA数据集和PR-Reasoner模型，用于提升自动驾驶车辆在尚未可见的潜在风险场景下的推理和安全性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在面对罕见、复杂、尚未显现风险的场景时依然存在巨大安全挑战，尤其是当前数据集缺乏潜在风险因果链的注释。

Method: 作者构建了PotentialRiskQA视觉-语言数据集，针对场景中的潜在风险，注释了结构化场景描述、语义前兆和推断的风险结局，并基于该数据集提出了PR-Reasoner模型，实现了对潜在风险的推理。

Result: 实验表明，PR-Reasoner通过在PotentialRiskQA数据集上的微调，在潜在风险推理任务上明显优于现有视觉-语言模型（VLMs）。

Conclusion: PotentialRiskQA数据集和PR-Reasoner模型为自动驾驶系统提供了更强的前瞻性和主动安全能力，为实现更智能、韧性的自动驾驶车辆打下了基础。

Abstract: Ensuring safety remains a key challenge for autonomous vehicles (AVs), especially in rare and complex scenarios. One critical but understudied aspect is the \textbf{potential risk} situations, where the risk is \textbf{not yet observable} but can be inferred from subtle precursors, such as anomalous behaviors or commonsense violations. Recognizing these precursors requires strong semantic understanding and reasoning capabilities, which are often absent in current AV systems due to the scarcity of such cases in existing driving or risk-centric datasets. Moreover, current autonomous driving accident datasets often lack annotations of the causal reasoning chains behind incidents, which are essential for identifying potential risks before they become observable. To address these gaps, we introduce PotentialRiskQA, a novel vision-language dataset designed for reasoning about potential risks prior to observation. Each sample is annotated with structured scene descriptions, semantic precursors, and inferred risk outcomes. Based on this dataset, we further propose PR-Reasoner, a vision-language-model-based framework tailored for onboard potential risk reasoning. Experimental results show that fine-tuning on PotentialRiskQA enables PR-Reasoner to significantly enhance its performance on the potential risk reasoning task compared to baseline VLMs. Together, our dataset and model provide a foundation for developing autonomous systems with improved foresight and proactive safety capabilities, moving toward more intelligent and resilient AVs.

</details>


### [340] [Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary](https://arxiv.org/abs/2511.22963)
*Zhirui Liu,Kaiyang Ji,Ke Yang,Jingyi Yu,Ye Shi,Jingya Wang*

Main category: cs.RO

TL;DR: 本论文提出了一种名为Humanoid-LLA的大型语言动作模型，实现了将复杂自然语言指令转化为拟人机器人可执行的物理动作，在自然性、稳定性和成功率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人虽然在基础运动和操作上有所突破，但面对复杂的自然语言指令时，往往只能完成简单任务，难以兼顾动作多样性与物理可执行性，妨碍了人机深入协作和智能发展。

Method: 作者提出了三个核心步骤：1）构建统一的运动词汇表，将人类和机器人运动原语对齐到离散的动作空间；2）开发基于词汇表导向的控制器，通过特权策略蒸馏保证动作的物理可行性；3）基于物理知识的微调，利用动态奖励强化学习，进一步提升控制的鲁棒性和稳定性。

Result: 在模拟环境和真实Unitree G1人形机器人上的广泛实验表明，Humanoid-LLA在理解多样语言指令的同时，依然保证了动作的物理真实性，其动作自然度、稳定性和任务执行成功率全面超越现有语言驱动控制器。

Conclusion: Humanoid-LLA实现了高泛化、强鲁棒性又兼顾物理可实现性的人形机器人自然语言驱动控制，为人机协作和通用型机器人智能提供了新途径。

Abstract: Enabling humanoid robots to follow free-form language commands is critical for seamless human-robot interaction, collaborative task execution, and general-purpose embodied intelligence. While recent advances have improved low-level humanoid locomotion and robot manipulation, language-conditioned whole-body control remains a significant challenge. Existing methods are often limited to simple instructions and sacrifice either motion diversity or physical plausibility. To address this, we introduce Humanoid-LLA, a Large Language Action Model that maps expressive language commands to physically executable whole-body actions for humanoid robots. Our approach integrates three core components: a unified motion vocabulary that aligns human and humanoid motion primitives into a shared discrete space; a vocabulary-directed controller distilled from a privileged policy to ensure physical feasibility; and a physics-informed fine-tuning stage using reinforcement learning with dynamics-aware rewards to enhance robustness and stability. Extensive evaluations in simulation and on a real-world Unitree G1 humanoid show that Humanoid-LLA delivers strong language generalization while maintaining high physical fidelity, outperforming existing language-conditioned controllers in motion naturalness, stability, and execution success rate.

</details>


### [341] [Analytical Inverse Kinematic Solution for "Moz1" NonSRS 7-DOF Robot arm with novel arm angle](https://arxiv.org/abs/2511.22996)
*Ke Chen*

Main category: cs.RO

TL;DR: 本文提出了一个针对带有手腕偏移的7自由度Moz1机械臂逆运动学问题的解析解，能够以闭式形式快速、准确地获得所有解，解决了传统方法难以定义冗余角度和奇异性的问题。


<details>
  <summary>Details</summary>
Motivation: 七自由度机械臂逆运动学问题因冗余自由度和奇异性处理难而复杂，现有SEW角方法在某些姿态下无法定义。该文旨在提出更通用、适用性更强的解析解方法。

Method: 引入一种新颖的臂角表示方法，推导出支持腕部偏移的闭式解析逆解，能够完整覆盖冗余运动，并详细处理奇异性和冗余角度的定义与求解方式。

Result: 提出的逆运动学解析解简洁、快速，能在每个位姿下给出所有16组可行解，有效涵盖全解空间。同时克服了传统SEW角方法在某些姿态下无法定义的问题，并妥善处理了奇异性。

Conclusion: 本方法显著提升了7自由度带腕偏移机械臂逆运动学解析求解的适用性和效率，对全空间位姿均适用，为实际机器人控制提供了高效、健壮的工具。

Abstract: This paper presents an analytical solution to the inverse kinematic problem(IKP) for the seven degree-of-freedom (7-DOF) Moz1 Robot Arm with offsets on wrist. We provide closed-form solutions with the novel arm angle . it allow fully self-motion and solve the problem of algorithmic singularities within the workspace. It also provides information on how the redundancy is resolved in a new arm angle representation where traditional SEW angle faied to be defined and how singularities are handled. The solution is simple, fast and exact, providing full solution space (i.e. all 16 solutions) per pose.

</details>


### [342] [Adaptive Factor Graph-Based Tightly Coupled GNSS/IMU Fusion for Robust Positionin](https://arxiv.org/abs/2511.23017)
*Elham Ahmadi,Alireza Olama,Petri Välisuo,Heidi Kuusniemi*

Main category: cs.RO

TL;DR: 本文提出了一种基于因子图的鲁棒自适应GNSS/IMU融合框架，通过引入Barron损失函数提升了在信号受限环境下的定位准确性，相比传统方法显著降低了定位误差。


<details>
  <summary>Details</summary>
Motivation: 在城市峡谷等GNSS信号受限环境下，导航系统定位容易受到噪声和异常值影响，现有GNSS/IMU紧耦合方法对非高斯噪声和离群点仍然较为敏感，因此亟需提升鲁棒性。

Method: 该方法在因子图优化（FGO）基础上，将GNSS伪距观测与IMU预积分因子直接融合，并首次引入Barron损失（一个可调参数统一多种鲁棒估计器）以自适应衰减异常的GNSS观测影响。方法基于扩展的GTSAM框架实现并在UrbanNav数据集上验证。

Result: 实验表明该方法在标准因子图优化基础上定位误差最多可降低41%，相较扩展卡尔曼滤波器（EKF）在城市峡谷环境下提升更为显著。

Conclusion: 自适应的Barron损失可显著提升GNSS/IMU组合导航系统在复杂、信号受限环境下的定位鲁棒性和精度。

Abstract: Reliable positioning in GNSS-challenged environments remains a critical challenge for navigation systems. Tightly coupled GNSS/IMU fusion improves robustness but remains vulnerable to non-Gaussian noise and outliers. We present a robust and adaptive factor graph-based fusion framework that directly integrates GNSS pseudorange measurements with IMU preintegration factors and incorporates the Barron loss, a general robust loss function that unifies several m-estimators through a single tunable parameter. By adaptively down weighting unreliable GNSS measurements, our approach improves resilience positioning. The method is implemented in an extended GTSAM framework and evaluated on the UrbanNav dataset. The proposed solution reduces positioning errors by up to 41% relative to standard FGO, and achieves even larger improvements over extended Kalman filter (EKF) baselines in urban canyon environments. These results highlight the benefits of Barron loss in enhancing the resilience of GNSS/IMU-based navigation in urban and signal-compromised environments.

</details>


### [343] [DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management](https://arxiv.org/abs/2511.23030)
*Casimir Feldmann,Maximum Wilder-Smith,Vaishakh Patil,Michael Oechsle,Michael Niemeyer,Keisuke Tateno,Marco Hutter*

Main category: cs.RO

TL;DR: 该论文提出了DiskChunGS，一种能够突破GPU显存限制，实现大规模、实时三维重建和新视角合成的3D高斯散点（3DGS）SLAM系统。通过将场景分块，仅加载活跃区域至显存，其余数据存储到硬盘，有效解决了现有3DGS-SLAM扩展能力不足的问题。实验证明，DiskChunGS在多种数据集与设备上均表现突出，顺利完成所有KITTI序列，视觉质量优异。


<details>
  <summary>Details</summary>
Motivation: 3D高斯散点渲染虽具备实时与高质量效果，但受限于GPU显存，难以应用至大规模SLAM场景，亟需可扩展的方案以突破此瓶颈并实现大范围、高一致性的三维重建。

Method: 提出基于块的分区存储架构，将场景分割为多个空间子块，运行时仅将当前活跃块载入GPU显存，其余数据写入磁盘，实现了显存占用的动态管理。同时，该系统可无缝集成现有SLAM算法，实现位姿估计和回环检测。

Result: 在Replica、TUM-RGBD和KITTI等多个数据集，以及Nvidia Jetson等嵌入式设备上，DiskChunGS表现出色。尤其是在大规模KITTI数据集上，全部序列均无内存崩溃，且视觉重建质量领先。

Conclusion: DiskChunGS有效突破了GPU显存限制，首次使3DGS-SLAM适用于大规模、复杂环境。其创新的分区加载方法，展示了算法优化在资源受限条件下的巨大潜力，为高质量实时三维感知提供了实用解决方案。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated impressive results for novel view synthesis with real-time rendering capabilities. However, integrating 3DGS with SLAM systems faces a fundamental scalability limitation: methods are constrained by GPU memory capacity, restricting reconstruction to small-scale environments. We present DiskChunGS, a scalable 3DGS SLAM system that overcomes this bottleneck through an out-of-core approach that partitions scenes into spatial chunks and maintains only active regions in GPU memory while storing inactive areas on disk. Our architecture integrates seamlessly with existing SLAM frameworks for pose estimation and loop closure, enabling globally consistent reconstruction at scale. We validate DiskChunGS on indoor scenes (Replica, TUM-RGBD), urban driving scenarios (KITTI), and resource-constrained Nvidia Jetson platforms. Our method uniquely completes all 11 KITTI sequences without memory failures while achieving superior visual quality, demonstrating that algorithmic innovation can overcome the memory constraints that have limited previous 3DGS SLAM methods.

</details>


### [344] [LatBot: Distilling Universal Latent Actions for Vision-Language-Action Models](https://arxiv.org/abs/2511.23034)
*Zuolei Li,Xingyu Gao,Xiaofan Wang,Jianlong Fu*

Main category: cs.RO

TL;DR: 本文提出了一种通用潜在动作学习框架，用于提升机器人操作任务中的泛化能力，并在模拟与真实环境下均取得了优异表现，尤其展示了极强的小样本迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大规模操作视频学习泛化表示的方法，多数只关注视觉重建目标，忽略了物理先验，导致无法很好地适用于不同机器人实体的操作任务。作者致力于解决表示通用性差及迁移能力弱的问题。

Method: 提出了Universal Latent Action Learning（通用潜在动作学习）框架，将任务指令和多帧图像输入模型，同时优化未来帧重建和动作序列预测损失。独特之处在于引入动作预测（如夹爪轨迹和朝向等），以建模丰富的物理先验，并将潜在动作解耦为“运动”与“场景”两类token，从而区分机器人动作与环境变化，提升表示通用性和无关动态的过滤能力。最后将所学潜在动作蒸馏到VLA模型中。

Result: 在SIMPLER和LIBERO等模拟环境，以及真实机器人平台（如Franka）上取得优异表现。尤其是在真实环境下仅用每个任务10条演示轨迹，便成功完成所有5项具有挑战性的操作任务，展现出极强的小样本迁移与泛化能力。

Conclusion: 引入物理先验与动作解耦的通用聚合框架，大幅提升了潜在动作表示的泛化性和迁移能力，有效支持了机器人在复杂、多变环境下的操作任务。

Abstract: Learning transferable latent actions from large-scale object manipulation videos can significantly enhance generalization in downstream robotics tasks, as such representations are agnostic to different robot embodiments. Existing approaches primarily rely on visual reconstruction objectives while neglecting physical priors, leading to sub-optimal performance in learning universal representations. To address these challenges, we propose a Universal Latent Action Learning framework that takes task instructions and multiple frames as inputs, and optimizes both future frame reconstruction and action sequence prediction. Unlike prior works, incorporating action predictions (e.g., gripper or hand trajectories and orientations) allows the model to capture richer physical priors such as real-world distances and orientations, thereby enabling seamless transferability to downstream tasks. We further decompose the latent actions into learnable motion and scene tokens to distinguish the robot's active movements from environmental changes, thus filtering out irrelevant dynamics. By distilling the learned latent actions into the latest VLA models, we achieve strong performance across both simulated (SIMPLER and LIBERO) and real-world robot settings. Notably, with only 10 real-world trajectories per task collected on a Franka robot, our approach successfully completes all five challenging tasks, demonstrating strong few-shot transferability in robotic manipulation.

</details>


### [345] [Automated Generation of MDPs Using Logic Programming and LLMs for Robotic Applications](https://arxiv.org/abs/2511.23143)
*Enrico Saccon,Davide De Martini,Matteo Saveriano,Edoardo Lamon,Luigi Palopoli,Marco Roveri*

Main category: cs.RO

TL;DR: 该论文提出了一种结合大语言模型（LLMs）、自动规划与形式化验证的框架，用于自动生成和使用马尔可夫决策过程（MDP）。


<details>
  <summary>Details</summary>
Motivation: 传统的MDP建模和策略合成需要大量人工设计和专业知识，限制了其在机器人等领域的应用和扩展性。该工作尝试利用LLM从自然语言描述自动提取知识，简化流程，提高效率。

Method: 系统利用LLM将自然语言转化为结构化的Prolog知识库，随后自动进行可达性分析以生成MDP，并通过Storm模型检测器合成最优策略。最终将策略导出为状态-动作表用于执行。

Result: 在三个不同的人-机交互场景下进行验证，结果表明该框架能够以极少的人力干预生成可执行的MDP策略。

Conclusion: 将LLM与形式化方法结合，有望让概率规划（如MDP）在机器人等领域更易用、更具扩展性。

Abstract: We present a novel framework that integrates Large Language Models (LLMs) with automated planning and formal verification to streamline the creation and use of Markov Decision Processes (MDP). Our system leverages LLMs to extract structured knowledge in the form of a Prolog knowledge base from natural language (NL) descriptions. It then automatically constructs an MDP through reachability analysis, and synthesises optimal policies using the Storm model checker. The resulting policy is exported as a state-action table for execution. We validate the framework in three human-robot interaction scenarios, demonstrating its ability to produce executable policies with minimal manual effort. This work highlights the potential of combining language models with formal methods to enable more accessible and scalable probabilistic planning in robotics.

</details>


### [346] [Obstruction reasoning for robotic grasping](https://arxiv.org/abs/2511.23186)
*Runyu Jiao,Matteo Bortolon,Francesco Giuliari,Alice Fasoli,Sergio Povoli,Guofeng Mei,Yiming Wang,Fabio Poiesi*

Main category: cs.RO

TL;DR: UNOGrasp是一种结合视觉和语言的机器人系统，可以在混乱环境中推理并清除障碍，成功抓取目标物体。它引入了基于障碍路径的多步推理过程，并通过新建立的大规模标注数据集UNOBench进行训练和评测，效果显著超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言机器人模型在空间理解上取得进展，但对于障碍推理和可达性规划能力有限。要实现复杂环境下准确抓取目标物体，机器人必须能推理并清除路径中的障碍。

Method: 提出UNOGrasp模型，结合了监督学习和基于可验证推理奖励的强化微调。设计了基于目标物体障碍路径的多步推理流程，并引入障碍感知视觉提示。构建了包含10万余条人工标注障碍路径的大规模数据集UNOBench，采集了障碍比、接触点和自然语言指令信息。

Result: 在合成和真实环境中的大量实验和真实机器人系统测试表明，UNOGrasp在障碍推理能力和抓取成功率方面，显著优于通用和专有替代模型。

Conclusion: UNOGrasp有效提升了机器人对混乱环境中障碍的推理和处理能力，大幅提高目标抓取的成功率，对提升机器人抓取技术具有重要意义。

Abstract: Successful robotic grasping in cluttered environments not only requires a model to visually ground a target object but also to reason about obstructions that must be cleared beforehand. While current vision-language embodied reasoning models show emergent spatial understanding, they remain limited in terms of obstruction reasoning and accessibility planning. To bridge this gap, we present UNOGrasp, a learning-based vision-language model capable of performing visually-grounded obstruction reasoning to infer the sequence of actions needed to unobstruct the path and grasp the target object. We devise a novel multi-step reasoning process based on obstruction paths originated by the target object. We anchor each reasoning step with obstruction-aware visual cues to incentivize reasoning capability. UNOGrasp combines supervised and reinforcement finetuning through verifiable reasoning rewards. Moreover, we construct UNOBench, a large-scale dataset for both training and benchmarking, based on MetaGraspNetV2, with over 100k obstruction paths annotated by humans with obstruction ratios, contact points, and natural-language instructions. Extensive experiments and real-robot evaluations show that UNOGrasp significantly improves obstruction reasoning and grasp success across both synthetic and real-world environments, outperforming generalist and proprietary alternatives. Project website: https://tev-fbk.github.io/UnoGrasp/.

</details>


### [347] [Fault-Tolerant MARL for CAVs under Observation Perturbations for Highway On-Ramp Merging](https://arxiv.org/abs/2511.23193)
*Yuchen Shi,Huaxin Pei,Yi Zhang,Danya Yao*

Main category: cs.RO

TL;DR: 本文提出了一种针对协同自动驾驶车辆的具备容错能力的多智能体强化学习（MARL）方法，通过引入故障注入和自诊断机制，有效提升系统在观测数据受到干扰情况下的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习在协同自动驾驶领域有潜力，但对观测故障（如传感器数据被扰动）容错性较差，严重影响系统表现。因此，提升MARL系统对这类故障的容错能力是实际应用的关键需求。

Method: 方法包括两个核心智能体：1）协同训练的对抗性故障注入智能体，在训练过程中主动生成扰动以促使策略更强健；2）具备自诊断能力的容错智能体，利用空间-时间相关性检测故障并重建可信观测，从而减缓故障对决策的负面影响。

Result: 在仿真高速公路汇流场景的实验证明，该方法在面对不同观测故障模式下，大大优于传统MARL基线，有效提升了驾驶安全性与效率，达到接近无故障的表现。

Conclusion: 提出的方法极大增强了协同自动驾驶车辆在多智能体系统下对观测故障的鲁棒性，有望推动MARL在实际场景中的普及和应用。

Abstract: Multi-Agent Reinforcement Learning (MARL) holds significant promise for enabling cooperative driving among Connected and Automated Vehicles (CAVs). However, its practical application is hindered by a critical limitation, i.e., insufficient fault tolerance against observational faults. Such faults, which appear as perturbations in the vehicles' perceived data, can substantially compromise the performance of MARL-based driving systems. Addressing this problem presents two primary challenges. One is to generate adversarial perturbations that effectively stress the policy during training, and the other is to equip vehicles with the capability to mitigate the impact of corrupted observations. To overcome the challenges, we propose a fault-tolerant MARL method for cooperative on-ramp vehicles incorporating two key agents. First, an adversarial fault injection agent is co-trained to generate perturbations that actively challenge and harden the vehicle policies. Second, we design a novel fault-tolerant vehicle agent equipped with a self-diagnosis capability, which leverages the inherent spatio-temporal correlations in vehicle state sequences to detect faults and reconstruct credible observations, thereby shielding the policy from misleading inputs. Experiments in a simulated highway merging scenario demonstrate that our method significantly outperforms baseline MARL approaches, achieving near-fault-free levels of safety and efficiency under various observation fault patterns.

</details>


### [348] [Field-programmable dynamics in a soft magnetic actuator enabling true random number generation and reservoir computing](https://arxiv.org/abs/2511.23215)
*Eduardo Sergio Oliveros-Mata,Oleksandr V. Pylypovskyi,Eleonora Raimondo,Rico Illing,Yevhen Zabila,Lin Guo,Guannan Mu,Mónica Navarro López,Xu Wang,Georgios Tzortzinis,Angelos Filippatos,Gilbert Santiago Cañón Bermúdez,Francesca Garescì,Giovanni Finocchio,Denys Makarov*

Main category: cs.RO

TL;DR: 本文提出利用复杂动力学的磁性软体驱动器，在软体机器人中实现新的功能，包括随机数生成、随机计算和时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 传统机电系统避免复杂动力学，担心磨损和难以控制。但复杂动力学在自然与工程系统常见，作者希望探索其在软体机器人中的潜在优势和新功能。

Method: 设计并实现了一种可调节动力学并可耐受上万次循环无疲劳的磁性软体驱动器。通过实验，将其应用于随机数生成、随机计算与物理时间序列预测（如Mackey–Glass序列），并展示了生物仿真眨眼和语音随机化。

Result: 磁性软体驱动器可以长期稳定工作，支持复杂动力学。实验演示了其在软硬件结合的计算任务中的效用，并实现了多种复杂功能。

Conclusion: 复杂动力学的引入拓展了软体机器人的应用场景，包括软计算、人机互动、协作机器人等，展示了软体机器人的新潜力。

Abstract: Complex and even chaotic dynamics, though prevalent in many natural and engineered systems, has been largely avoided in the design of electromechanical systems due to concerns about wear and controlability. Here, we demonstrate that complex dynamics might be particularly advantageous in soft robotics, offering new functionalities beyond motion not easily achievable with traditional actuation methods. We designed and realized resilient magnetic soft actuators capable of operating in a tunable dynamic regime for tens of thousands cycles without fatigue. We experimentally demonstrated the application of these actuators for true random number generation and stochastic computing. {W}e validate soft robots as physical reservoirs capable of performing Mackey--Glass time series prediction. These findings show that exploring the complex dynamics in soft robotics would extend the application scenarios in soft computing, human-robot interaction and collaborative robots as we demonstrate with biomimetic blinking and randomized voice modulation.

</details>


### [349] [Incorporating Ephemeral Traffic Waves in A Data-Driven Framework for Microsimulation in CARLA](https://arxiv.org/abs/2511.23236)
*Alex Richardson,Azhar Hasan,Gabor Karsai,Jonathan Sprinkle*

Main category: cs.RO

TL;DR: 本文提出了一种基于CARLA的数据驱动交通微观仿真框架，通过I-24 MOTION实验平台的高精度时空数据，重建真实交通波动态，为交通管控和自动驾驶研究提供真实感极强的仿真环境。


<details>
  <summary>Details</summary>
Motivation: 现有的交通微观仿真难以在大规模场景下准确校准和重现短暂的交通波动现象。标准模拟往往只关注局部行为或抽象几何，缺乏对整体时空动态的还原能力。

Method: 作者在CARLA中自动生成一段1英里高速公路，利用I-24的实测数据，经由联仿真模块将真实交通信息注入仿真。仿真以经验数据采样的“自车”为中心，仅在自车可见的纵向范围内自动生成交通流量，对超出可见范围采用上下游‘虚拟区’进行边界控制。

Result: 仿真结果在低拥堵和高拥堵情景下重现了交通波的形成和消散，表现出的集体现象与实际交通近似，相比以往框架能更真实再现实测的时空交通动态。

Conclusion: 该框架首次实现了基于实测数据、边界驱动、感知级真实感的交通波仿真，为交通控制、自动驾驶感知等研究提供了新方法，推动了微观建模和物理实验数据的深度融合。

Abstract: This paper introduces a data-driven traffic microsimulation framework in CARLA that reconstructs real-world wave dynamics using high-fidelity time-space data from the I-24 MOTION testbed. Calibration of road networks in microsimulators to reproduce ephemeral phenomena such as traffic waves for large-scale simulation is a process that is fraught with challenges. This work reconsiders the existence of the traffic state data as boundary conditions on an ego vehicle moving through previously recorded traffic data, rather than reproducing those traffic phenomena in a calibrated microsim. Our approach is to autogenerate a 1 mile highway segment corresponding to I-24, and use the I-24 data to power a cosimulation module that injects traffic information into the simulation. The CARLA and cosimulation simulations are centered around an ego vehicle sampled from the empirical data, with autogeneration of "visible" traffic within the longitudinal range of the ego vehicle. Boundary control beyond these visible ranges is achieved using ghost cells behind (upstream) and ahead (downstream) of the ego vehicle. Unlike prior simulation work that focuses on local car-following behavior or abstract geometries, our framework targets full time-space diagram fidelity as the validation objective. Leveraging CARLA's rich sensor suite and configurable vehicle dynamics, we simulate wave formation and dissipation in both low-congestion and high-congestion scenarios for qualitative analysis. The resulting emergent behavior closely mirrors that of real traffic, providing a novel cosimulation framework for evaluating traffic control strategies, perception-driven autonomy, and future deployment of wave mitigation solutions. Our work bridges microscopic modeling with physical experimental data, enabling the first perceptually realistic, boundary-driven simulation of empirical traffic wave phenomena in CARLA.

</details>


### [350] [SafeHumanoid: VLM-RAG-driven Control of Upper Body Impedance for Humanoid Robot](https://arxiv.org/abs/2511.23300)
*Yara Mahmoud,Jeffrin Sam,Nguyen Khang,Marcelino Fernando,Issatay Tokmurziyev,Miguel Altamirano Cabrera,Muhammad Haris Khan,Artem Lykov,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 本文提出了一种结合视觉语言模型（VLM）与检索增强生成（RAG）的SafeHumanoid系统，通过第一视角视觉信息实现仿人机器人在任务执行时，根据场景和人与机器人的距离动态调整阻抗与速度，提高人机交互安全性与可信度。


<details>
  <summary>Details</summary>
Motivation: 现有仿人机器人在人机交互场景中，缺乏根据任务场景和人类接近程度动态调整阻抗与运动速度的能力，影响了安全性与合作效率。本文旨在提高仿人机器人在人类协作中的安全性和标准合规性。

Method: SafeHumanoid系统通过第一视角摄像头采集环境图像，利用结构化视觉语言模型提示，对场景进行分析，并与验证场景数据库比对，结合RAG生成策略，最终通过逆运动学产生关节级阻抗与速度指令，实现对机器人的实时自适应控制。

Result: 在包含人体和不包含人体的多种桌面操作任务（如擦拭、物品交接、倒液体）中，SafeHumanoid能够根据场景智能调节刚度、阻尼及速度，保证任务成功率的同时显著提升安全性。但推理延迟（最高1.4秒）在高度动态场景下会影响系统响应速度。

Conclusion: SafeHumanoid验证了基于语义理解的阻抗控制方案在提升仿人机器人协作安全性和标准合规性上的可行性，为实现更智能和安全的人机协作提供了新路径。

Abstract: Safe and trustworthy Human Robot Interaction (HRI) requires robots not only to complete tasks but also to regulate impedance and speed according to scene context and human proximity. We present SafeHumanoid, an egocentric vision pipeline that links Vision Language Models (VLMs) with Retrieval-Augmented Generation (RAG) to schedule impedance and velocity parameters for a humanoid robot. Egocentric frames are processed by a structured VLM prompt, embedded and matched against a curated database of validated scenarios, and mapped to joint-level impedance commands via inverse kinematics. We evaluate the system on tabletop manipulation tasks with and without human presence, including wiping, object handovers, and liquid pouring. The results show that the pipeline adapts stiffness, damping, and speed profiles in a context-aware manner, maintaining task success while improving safety. Although current inference latency (up to 1.4 s) limits responsiveness in highly dynamic settings, SafeHumanoid demonstrates that semantic grounding of impedance control is a viable path toward safer, standard-compliant humanoid collaboration.

</details>


### [351] [Design, modelling and experimental validation of bipenniform shape memory alloy-based linear actuator integrable with hydraulic stroke amplification mechanism](https://arxiv.org/abs/2511.23372)
*Kanhaiya Lal Chaurasiya,Ruchira Kumar Pradhan,Yashaswi Sinha,Shivam Gupta,Ujjain Kumar Bidila,Digambar Killedar,Kapil Das Sahu,Bishakh Bhattacharya*

Main category: cs.RO

TL;DR: 本文提出了一种新型仿生形状记忆合金（SMA）直线执行器，并通过理论和实验分析其性能优于传统步进电机执行器，具备高效、轻量和节能等优势，适用于多种自动化和机器人领域。


<details>
  <summary>Details</summary>
Motivation: 传统基于电磁原理的执行器存在效率低、体积大、设计复杂、制造及维护成本高等局限。为满足工业对高效、适应性强和经济性好的执行器需求，需要探索新型执行器设计。本文受到生物启发，旨在开发基于智能材料（如SMA）的新一代执行器。

Method: 研究将仿生的bipenniform结构与高功率密度SMA材料结合，设计出多层bipenniform结构的SMA直线执行器，并建立了数学模型，结合实验进行验证。同时应用设计失效模式与效应分析（DFMEA）和实验法，系统评估其性能并与工业常用步进电机执行器进行对比。

Result: 开发的执行器在15V电压下可输出257N驱动力，驱动机构重量减少67%，零部件数量减少80%，成本降低32%，能耗降低19%，且装配尺寸兼容多种应用。整体性能显著优于对照组步进电机执行器。

Conclusion: 仿生SMA线圈执行器不仅高效轻便，还易于集成，适用于建筑自动化、太空机器人及医疗假肢等领域，展现出作为高力高行程应用新一代执行器的巨大潜力。

Abstract: The increasing industrial demand for alternative actuators over conventional electromagnetism-based systems having limited efficiency, bulky size, complex design due to in-built gear-train mechanisms, and high production and amortization costs necessitates the innovation in new actuator development. Integrating bio-inspired design principles into linear actuators could bring forth the next generation of adaptive and energy efficient smart material-based actuation systems. The present study amalgamates the advantages of bipenniform architecture, which generates high force in the given physiological region and a high power-to-weight ratio of shape memory alloy (SMA), into a novel bio-inspired SMA-based linear actuator. A mathematical model of a multi-layered bipenniform configuration-based SMA actuator was developed and validated experimentally. The current research also caters to the incorporation of failure mitigation strategies using design failure mode and effects analysis along with the experimental assessment of the performance of the developed actuator. The system has been benchmarked against an industry-developed stepper motor-driven actuator. It has shown promising results generating an actuation force of 257 N with 15 V input voltage, meeting the acceptable range for actuation operation. It further exhibits about 67% reduction in the weight of the drive mechanism, with 80% lesser component, 32% cost reduction, and 19% energy savings and similar envelope dimensions for assembly compatibility with dampers and louvers for easy onsite deployment. The study introduces SMA coil-based actuator as an advanced design that can be deployed for high force-high stroke applications. The bio-inspired SMA-based linear actuator has applications ranging from building automation controls to lightweight actuation systems for space robotics and medical prosthesis.

</details>


### [352] [From CAD to POMDP: Probabilistic Planning for Robotic Disassembly of End-of-Life Products](https://arxiv.org/abs/2511.23407)
*Jan Baumgärtner,Malte Hansjosten,David Hald,Adrian Hauptmannl,Alexander Puchta,Jürgen Fleischer*

Main category: cs.RO

TL;DR: 该论文提出将报废产品的拆解问题建模为部分可观测马尔科夫决策过程（POMDP），结合任务与运动规划、强化学习和贝叶斯过滤方法提升机器人拆解在不确定性环境下的表现，相较传统确定性方法取得更好效果。


<details>
  <summary>Details</summary>
Motivation: 循环经济要求机器人不仅能装配，还需高效且安全地拆解报废产品用于回收再利用。但现实中的报废产品常因磨损、腐蚀或非记录修复与原设计差异较大，现有确定性拆解规划方法难以适应这些不确定性。为此，作者希望以更符合实际的方式建模和优化拆解过程。

Method: 作者将拆解流程建模为POMDP，捕捉产品内部状态的不确定性。提出基于CAD数据、机器人能力和检测结果，自动派生具体的POMDP模型，并设计了一套结合强化学习的任务与运动规划框架。执行过程中通过贝叶斯滤波器实时维护对产品状态的信念分布，实现对异常情况（如零件缺失或卡滞）的适应。

Result: 在两套机器人系统上，对三类产品进行实验，表明该概率规划方法在平均拆解时间和时间方差方面均优于传统确定性基线方法。同时，该框架能适用于不同机器人配置，并成功应对CAD模型与实际产品的偏差。

Conclusion: 论文证明基于POMDP的不确定性建模与分层规划显著提升了机器人拆解现实报废产品的效率与鲁棒性，展现了面向循环经济的自动化拆解的应用潜力。

Abstract: To support the circular economy, robotic systems must not only assemble new products but also disassemble end-of-life (EOL) ones for reuse, recycling, or safe disposal. Existing approaches to disassembly sequence planning often assume deterministic and fully observable product models, yet real EOL products frequently deviate from their initial designs due to wear, corrosion, or undocumented repairs. We argue that disassembly should therefore be formulated as a Partially Observable Markov Decision Process (POMDP), which naturally captures uncertainty about the product's internal state. We present a mathematical formulation of disassembly as a POMDP, in which hidden variables represent uncertain structural or physical properties. Building on this formulation, we propose a task and motion planning framework that automatically derives specific POMDP models from CAD data, robot capabilities, and inspection results. To obtain tractable policies, we approximate this formulation with a reinforcement-learning approach that operates on stochastic action outcomes informed by inspection priors, while a Bayesian filter continuously maintains beliefs over latent EOL conditions during execution. Using three products on two robotic systems, we demonstrate that this probabilistic planning framework outperforms deterministic baselines in terms of average disassembly time and variance, generalizes across different robot setups, and successfully adapts to deviations from the CAD model, such as missing or stuck parts.

</details>


### [353] [$\mathcal{E}_0$: Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion](https://arxiv.org/abs/2511.21542)
*Zhihao Zhan,Jiaying Zhou,Likui Zhang,Qinhan Lv,Hao Liu,Jusheng Zhang,Weizheng Li,Ziliang Chen,Tianshui Chen,Keze Wang,Liang Lin,Guangrun Wang*

Main category: cs.RO

TL;DR: 本论文提出了一种新型视觉-语言-动作（VLA）模型E0，通过离散扩散方法，更有效地将视觉、语言理解与机器人控制相结合，实现更精确、更可泛化的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在任务、场景和视角泛化上表现有限，且动作生成常常不精确、不稳定，需要一种能更好泛化且更细粒度控制的解决方案。

Method: 提出E0模型，将动作生成建模为对量化动作token的多步去噪过程。该方法采用离散扩散机制，使动作建模更贴合机器人实际离散控制信号特点，并可以自然结合预训练的视觉-语言模型（VLM/VLA）。还引入了球面视角扰动增强，提升对摄像头视角变化的鲁棒性。

Result: E0在LIBERO、VLABench和ManiSkill等14个环境上平均超越强基线10.7%，在Franka机械臂上的实验证明了其高精度、强鲁棒性与良好泛化能力。

Conclusion: E0离散扩散框架为机器人泛化操作策略学习开辟了一条新路径，表明离散扩散方法在VLA任务中具有极大潜力。

Abstract: Vision-Language-Action (VLA) models offer a unified framework for robotic manipulation by integrating visual perception, language understanding, and control generation. Yet existing VLA models still struggle to generalize across diverse tasks, scenes, and camera viewpoints, and often produce coarse or unstable actions. We introduce E0, a continuized discrete diffusion framework that formulates action generation as iterative denoising over quantized action tokens. Compared with continuous diffusion policies, E0 offers two key advantages: (1) discrete action tokens align naturally with the symbolic structure of pretrained VLM/VLA backbones, enabling stronger semantic conditioning; and 2. discrete diffusion matches the true quantized nature of real-world robot control-whose hardware constraints (e.g., encoder resolution, control frequency, actuation latency) inherently discretize continuous signals-and therefore benefits from a Bayes-optimal denoiser that models the correct discrete action distribution, leading to stronger generalization. Compared with discrete autoregressive and mask-based discrete diffusion models, E0 supports a significantly larger and finer-grained action vocabulary and avoids the distributional mismatch introduced by masking-based corruptions-yielding more accurate fine-grained action control. We further introduce a spherical viewpoint perturbation augmentation method to improve robustness to camera shifts without additional data. Experiments on LIBERO, VLABench, and ManiSkill show that E0 achieves state-of-the-art performance across 14 diverse environments, outperforming strong baselines by 10.7% on average. Real-world evaluation on a Franka arm confirms that E0 delivers precise, robust, and transferable manipulation, establishing discrete diffusion as a promising direction for generalizable VLA policy learning.

</details>
