<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 16]
- [cs.CL](#cs.CL) [Total: 9]
- [cs.RO](#cs.RO) [Total: 11]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VISION-ICE: Video-based Interpretation and Spatial Identification of Arrhythmia Origins via Neural Networks in Intracardiac Echocardiography](https://arxiv.org/abs/2602.20165)
*Dorsa EPMoghaddam,Feng Gao,Drew Bernard,Kavya Sinha,Mehdi Razavi,Behnaam Aazhang*

Main category: cs.CV

TL;DR: 本论文提出利用AI和心内超声（ICE）视频自动定位心律失常来源，提升速度并简化操作流程。


<details>
  <summary>Details</summary>
Motivation: 现有心律失常定位依赖高密度标测和术前CT/MRI，时间与资源消耗大，亟需更快、更经济的方法。AI已在超声影像分析中取得成功，引发将其应用于心律失常定位的需求。

Method: 将心律失常来源定位问题建立为三分类任务：区分正常窦律、左侧和右侧心律失常。基于ICE视频数据，开发了3D卷积神经网络进行训练与分类。通过十折交叉验证，对四名新患者数据评估模型表现。

Result: 模型平均准确率达到66.2%，远高于随机基线（33.3%），显示该方法在自动化心律失常定位中的可行性和潜力。

Conclusion: 结合ICE和深度学习实现自动化心律失常定位，可望加快心脏电生理操作、减少消融手术负担。后续将扩展数据集以提升模型稳健性及广泛适用性。

Abstract: Contemporary high-density mapping techniques and preoperative CT/MRI remain time and resource intensive in localizing arrhythmias. AI has been validated as a clinical decision aid in providing accurate, rapid real-time analysis of echocardiographic images. Building on this, we propose an AI-enabled framework that leverages intracardiac echocardiography (ICE), a routine part of electrophysiology procedures, to guide clinicians toward areas of arrhythmogenesis and potentially reduce procedural time. Arrhythmia source localization is formulated as a three-class classification task, distinguishing normal sinus rhythm, left-sided, and right-sided arrhythmias, based on ICE video data. We developed a 3D Convolutional Neural Network trained to discriminate among the three aforementioned classes. In ten-fold cross-validation, the model achieved a mean accuracy of 66.2% when evaluated on four previously unseen patients (substantially outperforming the 33.3% random baseline). These results demonstrate the feasibility and clinical promise of using ICE videos combined with deep learning for automated arrhythmia localization. Leveraging ICE imaging could enable faster, more targeted electrophysiological interventions and reduce the procedural burden of cardiac ablation. Future work will focus on expanding the dataset to improve model robustness and generalizability across diverse patient populations.

</details>


### [2] [OTPrune: Distribution-Aligned Visual Token Pruning via Optimal Transport](https://arxiv.org/abs/2602.20205)
*Xiwen Chen,Wenhui Zhu,Gen Li,Xuanzhao Dong,Yujian Xiong,Hao Wang,Peijie Qiu,Qingquan Song,Zhipeng Wang,Shao Tang,Yalin Wang,Abolfazl Razi*

Main category: cs.CV

TL;DR: 该论文提出了一种名为OTPrune的视觉Token剪枝框架，通过最优传输方法实现分布对齐，加速多模态大模型推理，同时兼顾性能与效率。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在视觉-语言推理任务表现优异，但由于视觉Token冗余，推理成本高。已有剪枝方法忽略了视觉表示的分布结构，这影响了剪枝稳定性和表达能力。因此，需要改进剪枝方法以提升推理效率并保持模型性能。

Method: OTPrune无需额外训练，将视觉Token剪枝问题建模为分布对齐，通过最优传输（OT）方法最小化完整与剪枝后Token分布的2-Wasserstein距离，保持局部多样性和全局表达能力。此外，作者设计了可优化的次模目标函数，并证明该目标函数具有单调性和次模性，保障剪枝的高效和稳定。

Result: 实验表明，OTPrune在多个主流基准测试上达到了比现有剪枝方法更优的效率与性能权衡。

Conclusion: 论文理论与实验均表明，分布对齐视角下的Token剪枝能有效提升MLLM推理效率且性能稳定，为高效视觉-语言模型推理提供了理论与实践基础。

Abstract: Multi-modal large language models (MLLMs) achieve strong visual-language reasoning but suffer from high inference cost due to redundant visual tokens. Recent work explores visual token pruning to accelerate inference, while existing pruning methods overlook the underlying distributional structure of visual representations. We propose OTPrune, a training-free framework that formulates pruning as distribution alignment via optimal transport (OT). By minimizing the 2-Wasserstein distance between the full and pruned token distributions, OTPrune preserves both local diversity and global representativeness while reducing inference cost. Moreover, we derive a tractable submodular objective that enables efficient optimization, and theoretically prove its monotonicity and submodularity, providing a principled foundation for stable and efficient pruning. We further provide a comprehensive analysis that explains how distributional alignment contributes to stable and semantically faithful pruning. Comprehensive experiments on wider benchmarks demonstrate that OTPrune achieves superior performance-efficiency tradeoffs compared to state-of-the-art methods. The code is available at https://github.com/xiwenc1/OTPrune.

</details>


### [3] [De-rendering, Reasoning, and Repairing Charts with Vision-Language Models](https://arxiv.org/abs/2602.20291)
*Valentin Bonas,Martin Sinnona,Viviana Siless,Emmanuel Iarussi*

Main category: cs.CV

TL;DR: 本文提出一种结合图表解析、自动分析与迭代改进的新框架，能为数据可视化设计提供有依据、可操作的优化建议。系统可自动识别设计缺陷，并基于可视化原则提出改进方案，实验表明对1,000个图表能输出高质量反馈。


<details>
  <summary>Details</summary>
Motivation: 当前的数据可视化工具常因设计缺陷而导致理解偏差，但传统基于规则的检测方法不能理解上下文，也不能给出具体改进建议；而直接用LLMs反馈图表质量又不可靠。因此需要一种自动、智能且能为用户提供可落地改进的新方法。

Method: 作者提出系统：先通过图像重建图表结构，然后用视觉-语言推理检测设计缺陷，结合可视化研究原则，逐条生成具体优化建议。用户可以选择性应用建议，系统还支持图表再渲染以形成持续反馈循环。

Result: 在Chart2Code数据集上，系统对1,000张图生成了10,452条设计建议，归纳到10个有代表性的改进类别（如坐标轴格式、色彩无障碍、图例一致性等），展示了系统的高覆盖率和针对性。

Conclusion: 基于LLM的自动推荐系统可为图表设计提供结构化和理论支持的反馈，推动智能、高效的数据可视化创作工具的发展，提升可视化素养。

Abstract: Data visualizations are central to scientific communication, journalism, and everyday decision-making, yet they are frequently prone to errors that can distort interpretation or mislead audiences. Rule-based visualization linters can flag violations, but they miss context and do not suggest meaningful design changes. Directly querying general-purpose LLMs about visualization quality is unreliable: lacking training to follow visualization design principles, they often produce inconsistent or incorrect feedback. In this work, we introduce a framework that combines chart de-rendering, automated analysis, and iterative improvement to deliver actionable, interpretable feedback on visualization design. Our system reconstructs the structure of a chart from an image, identifies design flaws using vision-language reasoning, and proposes concrete modifications supported by established principles in visualization research. Users can selectively apply these improvements and re-render updated figures, creating a feedback loop that promotes both higher-quality visualizations and the development of visualization literacy. In our evaluation on 1,000 charts from the Chart2Code benchmark, the system generated 10,452 design recommendations, which clustered into 10 coherent categories (e.g., axis formatting, color accessibility, legend consistency). These results highlight the promise of LLM-driven recommendation systems for delivering structured, principle-based feedback on visualization design, opening the door to more intelligent and accessible authoring tools.

</details>


### [4] [N4MC: Neural 4D Mesh Compression](https://arxiv.org/abs/2602.20312)
*Guodong Chen,Huanshuo Dong,Mallesham Dasari*

Main category: cs.CV

TL;DR: 本文提出了N4MC，这是首个针对随时间变化的4D网格序列的神经压缩框架，能够有效利用时间冗余实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 时变网格序列在动画、虚拟现实等领域应用广泛，但其数据量巨大，给存储与传输带来挑战。现有神经压缩方法仅对每一帧独立处理，未能有效利用时间冗余。本研究旨在提升时变网格序列的压缩效率，并确保高质量和实时解码。

Method: N4MC借鉴2D视频编解码中的帧间压缩思想，通过运动补偿机制高效压缩长时序网格数据。具体做法是将连续的不规则网格帧转化为规整的4D张量，实现统一压缩表示。随后，用auto-decoder对张量进行空间和时间相关性的冗余消除。为提升时间一致性，引入了基于transformer的插值模型，通过体积中心的潜在嵌入预测中间网格帧，消除运动模糊。

Result: 大量实验表明，N4MC在率失真性能上优于现有最先进方法，并实现了4D网格序列的实时解码。

Conclusion: N4MC充分利用了时变数据的时间冗余，在高效压缩与高质量恢复之间获得了良好平衡，对需要处理大规模4D网格数据的应用具有重要价值。

Abstract: We present N4MC, the first 4D neural compression framework to efficiently compress time-varying mesh sequences by exploiting their temporal redundancy. Unlike prior neural mesh compression methods that treat each mesh frame independently, N4MC takes inspiration from inter-frame compression in 2D video codecs, and learns motion compensation in long mesh sequences. Specifically, N4MC converts consecutive irregular mesh frames into regular 4D tensors to provide a uniform and compact representation. These tensors are then condensed using an auto-decoder, which captures both spatial and temporal correlations for redundancy removal. To enhance temporal coherence, we introduce a transformer-based interpolation model that predicts intermediate mesh frames conditioned on latent embeddings derived from tracked volume centers, eliminating motion ambiguities. Extensive evaluations show that N4MC outperforms state-of-the-art in rate-distortion performance, while enabling real-time decoding of 4D mesh sequences. The implementation of our method is available at: https://github.com/frozzzen3/N4MC.

</details>


### [5] [GSNR: Graph Smooth Null-Space Representation for Inverse Problems](https://arxiv.org/abs/2602.20328)
*Romario Gualdrón-Hurtado,Roman Jacome,Rafael S. Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 本文提出了一种用于成像逆问题的图平滑零空间表征（GSNR），通过加强对不可见解的建模，提高了图像重建的质量。


<details>
  <summary>Details</summary>
Motivation: 成像逆问题常因感测矩阵的非平凡零空间导致无穷多解，现有方法通过一般性先验（如稀疏性、平滑性）约束整体图像，但未有效约束零空间，可能引入重建偏差，因此有必要直接建模零空间信息。

Method: 作者提出GSNR方法。1）通过图拉普拉斯算子构建仅针对零空间的拉普拉斯矩阵，用以描述零空间信号中相邻像素的相似性；2）利用图的最低频谱模式（p-最平滑的特征向量）构建低维投影矩阵，从测量中高效预测和覆盖零空间。

Result: GSNR被整合进典型逆问题求解器（如PnP、DIP、扩散法）并应用于去模糊、压缩感知、颜色重建、超分辨四项任务，在PSNR上相较于基础方法提升最高4.3dB，相较于端到端学习模型提升最高1dB。

Conclusion: GSNR有效改善了逆问题中零空间的表达，理论与实验上均表明其可提升收敛性与预测性，并广泛适用于多类成像重建任务。

Abstract: Inverse problems in imaging are ill-posed, leading to infinitely many solutions consistent with the measurements due to the non-trivial null-space of the sensing matrix. Common image priors promote solutions on the general image manifold, such as sparsity, smoothness, or score function. However, as these priors do not constrain the null-space component, they can bias the reconstruction. Thus, we aim to incorporate meaningful null-space information in the reconstruction framework. Inspired by smooth image representation on graphs, we propose Graph-Smooth Null-Space Representation (GSNR), a mechanism that imposes structure only into the invisible component. Particularly, given a graph Laplacian, we construct a null-restricted Laplacian that encodes similarity between neighboring pixels in the null-space signal, and we design a low-dimensional projection matrix from the $p$-smoothest spectral graph modes (lowest graph frequencies). This approach has strong theoretical and practical implications: i) improved convergence via a null-only graph regularizer, ii) better coverage, how much null-space variance is captured by $p$ modes, and iii) high predictability, how well these modes can be inferred from the measurements. GSNR is incorporated into well-known inverse problem solvers, e.g., PnP, DIP, and diffusion solvers, in four scenarios: image deblurring, compressed sensing, demosaicing, and image super-resolution, providing consistent improvement of up to 4.3 dB over baseline formulations and up to 1 dB compared with end-to-end learned models in terms of PSNR.

</details>


### [6] [Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking](https://arxiv.org/abs/2602.20330)
*Jingcheng Yang,Tianhu Xiong,Shengyi Qian,Klara Nahrstedt,Mingyuan Wu*

Main category: cs.CV

TL;DR: 本文提出了首个用于视觉-语言模型（VLMs）透明电路追踪的分析框架，揭示并验证了VLMs的多模态推理过程中的特征融合与可控性。


<details>
  <summary>Details</summary>
Motivation: 目前VLMs虽然性能强大，但其内部推理和决策过程高度不透明，缺乏可解释性和可靠性，限制了其在高风险场合的应用。作者希望解决VLMs作为“黑箱”的问题，开发可追踪、可解释的分析方法。

Method: 作者提出了一套综合性的方法框架，包括转码器、归因图谱和基于注意力的技术，对VLMs进行系统的电路追踪和特征归因分析，同时通过特征操控和电路修补实验验证所得结论的因果性。

Result: 作者发现VLMs在多模态推理时能够分层整合视觉和语义特征，通过不同的视觉特征电路实现数学推理和跨模态关联。实验证明这些电路既具有因果性，也可被人为控制。

Conclusion: 提出的电路追踪框架实现了对VLM内部推理机制的可解释化与可控化，为今后开发更透明、更可靠的视觉-语言模型奠定了基础。

Abstract: Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchically integrate visual and semantic concepts. We reveal that distinct visual feature circuits can handle mathematical reasoning and support cross-modal associations. Validated through feature steering and circuit patching, our framework proves these circuits are causal and controllable, laying the groundwork for more explainable and reliable VLMs.

</details>


### [7] [Large-scale Photorealistic Outdoor 3D Scene Reconstruction from UAV Imagery Using Gaussian Splatting Techniques](https://arxiv.org/abs/2602.20342)
*Christos Maikos,Georgios Angelidis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: 本研究提出了一套端到端系统，可以将无人机实时视频流快速转换成高质量的3D重建结果，且延迟极低，适合AR/VR等实时交互应用。


<details>
  <summary>Details</summary>
Motivation: 尽管3D Gaussian Splatting（3DGS）等神经渲染方法在实时三维重建领域表现突出，但其在无人机实时重建与可视化系统中的应用还不充分。作者希望通过集成现有技术，提升无人机端到端三维重建的效率和实时性，满足实际应用需求。

Method: 搭建了一个高效架构，将RTMP实时流、同步传感器融合、摄像头位姿估计和3DGS优化有机结合，实现模型连续更新，并在AR/VR环境中低延迟部署。

Result: 实验结果显示，所提出的方法在保持高可视化质量的同时，渲染性能显著优于基于NeRF的方法，端到端延迟更低；重建的精度与离线高保真参考相差仅4-7%。

Conclusion: 该系统能够实现实时、高质量的无人机三维重建，非常适用于大规模、可扩展的航拍增强感知，可广泛应用于增强现实和虚拟现实场景。

Abstract: In this study, we present an end-to-end pipeline capable of converting drone-captured video streams into high-fidelity 3D reconstructions with minimal latency. Unmanned aerial vehicles (UAVs) are extensively used in aerial real-time perception applications. Moreover, recent advances in 3D Gaussian Splatting (3DGS) have demonstrated significant potential for real-time neural rendering. However, their integration into end-to-end UAV-based reconstruction and visualization systems remains underexplored. Our goal is to propose an efficient architecture that combines live video acquisition via RTMP streaming, synchronized sensor fusion, camera pose estimation, and 3DGS optimization, achieving continuous model updates and low-latency deployment within interactive visualization environments that supports immersive augmented and virtual reality (AR/VR) applications. Experimental results demonstrate that the proposed method achieves competitive visual fidelity, while delivering significantly higher rendering performance and substantially reduced end-to-end latency, compared to NeRF-based approaches. Reconstruction quality remains within 4-7\% of high-fidelity offline references, confirming the suitability of the proposed system for real-time, scalable augmented perception from aerial platforms.

</details>


### [8] [3DSPA: A 3D Semantic Point Autoencoder for Evaluating Video Realism](https://arxiv.org/abs/2602.20354)
*Bhavik Chandna,Kelsey R. Allen*

Main category: cs.CV

TL;DR: 本文提出了一种无需参考视频即可自动评估AI生成视频真实性的新框架3DSPA。该方法用统一的特征，结合3D点轨迹、深度信息和视觉语义评价视频真实性与物理合理性。其结果与人工评判高度一致，可更敏感地识别运动伪影与物理规律违背。


<details>
  <summary>Details</summary>
Motivation: 目前AI视频生成技术进步很快，但评估生成视频的真实性效率低，多依赖人工标注或定制数据集，自动化、通用且准确的评价体系十分缺乏。

Method: 提出3DSPA（3D时空点自编码器），将3D轨迹、深度线索和语义特征融合，捕捉物体运动及场景变化，不需参照视频做真实性打分。系统能识别物理违背、运动伪影等异常。

Result: 在多个数据集上，3DSPA比现有方法更敏感于视频中的物理规律冲突与运动伪影，其评价结果高度吻合人类对视频质量和真实性的判断。

Conclusion: 3DSPA证明了丰富三维语义后的轨迹表示对视频生成评价的重要性，能自动揭示物理规则冲突，为生成视频模型提供更强的定量基准。相关代码和模型权重已开源。

Abstract: AI video generation is evolving rapidly. For video generators to be useful for applications ranging from robotics to film-making, they must consistently produce realistic videos. However, evaluating the realism of generated videos remains a largely manual process -- requiring human annotation or bespoke evaluation datasets which have restricted scope. Here we develop an automated evaluation framework for video realism which captures both semantics and coherent 3D structure and which does not require access to a reference video. Our method, 3DSPA, is a 3D spatiotemporal point autoencoder which integrates 3D point trajectories, depth cues, and DINO semantic features into a unified representation for video evaluation. 3DSPA models how objects move and what is happening in the scene, enabling robust assessments of realism, temporal consistency, and physical plausibility. Experiments show that 3DSPA reliably identifies videos which violate physical laws, is more sensitive to motion artifacts, and aligns more closely with human judgments of video quality and realism across multiple datasets. Our results demonstrate that enriching trajectory-based representations with 3D semantics offers a stronger foundation for benchmarking generative video models, and implicitly captures physical rule violations. The code and pretrained model weights will be available at https://github.com/TheProParadox/3dspa_code.

</details>


### [9] [LESA: Learnable Stage-Aware Predictors for Diffusion Model Acceleration](https://arxiv.org/abs/2602.20497)
*Peiliang Cai,Jiacheng Liu,Haowen Xu,Xinyu Wang,Chang Zou,Linfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的特征预测框架（LESA），在保证生成质量的前提下显著加速Diffusion Transformer模型（DiTs），在多项数据集上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在图像和视频生成任务上表现优异，但计算消耗极高，严重限制了实际部署和应用。已有加速方法如feature caching存在泛化差、质量下降等问题，难以适应扩散过程的复杂动态。

Method: 作者设计了LESA预测器框架，包括两阶段训练流程，并使用Kolmogorov-Arnold Network（KAN）学习时间特征映射。提出多阶段、多专家结构，为不同噪声阶段分配专用的预测器，提高特征预测的精度和鲁棒性。

Result: 实验证明，LESA在FLUX.1-dev、Qwen-Image、HunyuanVideo等多个基准上实现了5-6倍加速，且几乎无质量损失，甚至在某些指标上大幅超越领先方法TaylorSeer，实现了SOTA的图像和视频生成性能。

Conclusion: LESA框架有效解决了现有加速方法存在的质量损失和一致性问题，兼具高效与高质量，具有很强的泛化能力，为扩散模型的实际部署提供了可行方案。代码即将公开，便于社区复现和完善。

Abstract: Diffusion models have achieved remarkable success in image and video generation tasks. However, the high computational demands of Diffusion Transformers (DiTs) pose a significant challenge to their practical deployment. While feature caching is a promising acceleration strategy, existing methods based on simple reusing or training-free forecasting struggle to adapt to the complex, stage-dependent dynamics of the diffusion process, often resulting in quality degradation and failing to maintain consistency with the standard denoising process. To address this, we propose a LEarnable Stage-Aware (LESA) predictor framework based on two-stage training. Our approach leverages a Kolmogorov-Arnold Network (KAN) to accurately learn temporal feature mappings from data. We further introduce a multi-stage, multi-expert architecture that assigns specialized predictors to different noise-level stages, enabling more precise and robust feature forecasting. Extensive experiments show our method achieves significant acceleration while maintaining high-fidelity generation. Experiments demonstrate 5.00x acceleration on FLUX.1-dev with minimal quality degradation (1.0% drop), 6.25x speedup on Qwen-Image with a 20.2% quality improvement over the previous SOTA (TaylorSeer), and 5.00x acceleration on HunyuanVideo with a 24.7% PSNR improvement over TaylorSeer. State-of-the-art performance on both text-to-image and text-to-video synthesis validates the effectiveness and generalization capability of our training-based framework across different models. Our code is included in the supplementary materials and will be released on GitHub.

</details>


### [10] [WildGHand: Learning Anti-Perturbation Gaussian Hand Avatars from Monocular In-the-Wild Videos](https://arxiv.org/abs/2602.20556)
*Hanhui Li,Xuan Huang,Wanquan Liu,Yuhao Cheng,Long Chen,Yiqiang Yan,Xiaodan Liang,Chenqiang Gao*

Main category: cs.CV

TL;DR: WildGHand是一种优化型自适应3D手部重建框架，针对真实场景复杂干扰实现高保真手部数字化，并开源了新数据集。


<details>
  <summary>Details</summary>
Motivation: 现有3D手部重建方法多依赖受控环境数据，在真实世界手部与物体交互、极端姿势、光照变化和运动模糊等强干扰下表现下降，难以泛化至实际应用。

Method: WildGHand框架基于3D高斯点渲染，核心包括：（1）动态扰动解耦模块，将扰动显式建模为3D高斯属性的时变偏差；（2）扰动感知优化策略，生成各帧各向异性加权掩码以指导优化，从空间与时间维度识别并抑制扰动。此外，作者还制作了包含多种真实干扰的单目手部视频数据集。

Result: WildGHand在自建及两个公开数据集上均显著优于基线，PSNR提升高达15.8%，LPIPS降低23.1%，表现出色。

Conclusion: WildGHand有效提升了在复杂现实环境下单目视频手部三维重建的鲁棒性和精度，并通过开源数据集推动了真实场景下的相关研究。

Abstract: Despite recent progress in 3D hand reconstruction from monocular videos, most existing methods rely on data captured in well-controlled environments and therefore degrade in real-world settings with severe perturbations, such as hand-object interactions, extreme poses, illumination changes, and motion blur. To tackle these issues, we introduce WildGHand, an optimization-based framework that enables self-adaptive 3D Gaussian splatting on in-the-wild videos and produces high-fidelity hand avatars. WildGHand incorporates two key components: (i) a dynamic perturbation disentanglement module that explicitly represents perturbations as time-varying biases on 3D Gaussian attributes during optimization, and (ii) a perturbation-aware optimization strategy that generates per-frame anisotropic weighted masks to guide optimization. Together, these components allow the framework to identify and suppress perturbations across both spatial and temporal dimensions. We further curate a dataset of monocular hand videos captured under diverse perturbations to benchmark in-the-wild hand avatar reconstruction. Extensive experiments on this dataset and two public datasets demonstrate that WildGHand achieves state-of-the-art performance and substantially improves over its base model across multiple metrics (e.g., up to a $15.8\%$ relative gain in PSNR and a $23.1\%$ relative reduction in LPIPS). Our implementation and dataset are available at https://github.com/XuanHuang0/WildGHand.

</details>


### [11] [Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion](https://arxiv.org/abs/2602.20577)
*Jiaru Zhang,Manav Gagvani,Can Cui,Juntong Peng,Ruqi Zhang,Ziran Wang*

Main category: cs.CV

TL;DR: 本文提出了一种面向自动驾驶的新型视觉-语言-动作扩散框架MVLAD-AD，提升了推理效率、轨迹精度和可解释性。实验证明其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型和视觉-语言模型用于自动驾驶时面临推理延迟、动作精度和可解释性不足等问题，现有方法对规划和理解的高效结合不足。

Method: 提出了Masked Vision-Language-Action Diffusion (MVLAD-AD) 框架，包括：1）不将动作强行映射到语言空间，而是通过离散动作分词法将可行轨迹编码为紧凑的动作码本；2）几何感知嵌入学习以保证潜空间中嵌入与物理几何相关性；3）动作优先解码，优先生成高质量轨迹。

Result: 在nuScenes和衍生基准测试上，MVLAD-AD在推理效率、规划精度和可解释性方面均优于主流自回归和扩散基线。

Conclusion: MVLAD-AD有效平衡了自动驾驶中的规划效率与语义可解释性，推动了视觉-语言-动作模型在实际自动驾驶场景中的应用。

Abstract: Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.

</details>


### [12] [PropFly: Learning to Propagate via On-the-Fly Supervision from Pre-trained Video Diffusion Models](https://arxiv.org/abs/2602.20583)
*Wonyong Seo,Jaeho Moon,Jaehyup Lee,Soo Ye Kim,Munchurl Kim*

Main category: cs.CV

TL;DR: PropFly提出了一种无需配对数据集、基于预训练扩散模型的视频编辑训练流水线，通过在线生成的监督信号实现高效的视频编辑迁移和传播。


<details>
  <summary>Details</summary>
Motivation: 目前基于传播的视频编辑技术需要大量昂贵的配对视频数据集进行训练，这在实际操作中成本高、难度大，限制了该领域的发展。

Method: PropFly利用预训练视频扩散模型，通过动态调整Classifier-Free Guidance（CFG）系数，无需事先准备配对数据集即可在线生成低（源）与高（目标）CFG的'源'与'编辑'表征对，并引入额外适配器和Guidance-Modulated Flow Matching（GMFM）损失指导学习编辑传播。

Result: 在多项视频编辑任务和评测中，PropFly产生的编辑结果质量显著优于目前的先进方法，在时序一致性和编辑动态性方面表现突出。

Conclusion: PropFly证明无需大量配对数据集即可实现高效且高质量的视频编辑传播，为相关任务带来极大灵活性与实用性。

Abstract: Propagation-based video editing enables precise user control by propagating a single edited frame into following frames while maintaining the original context such as motion and structures. However, training such models requires large-scale, paired (source and edited) video datasets, which are costly and complex to acquire. Hence, we propose the PropFly, a training pipeline for Propagation-based video editing, relying on on-the-Fly supervision from pre-trained video diffusion models (VDMs) instead of requiring off-the-shelf or precomputed paired video editing datasets. Specifically, our PropFly leverages one-step clean latent estimations from intermediate noised latents with varying Classifier-Free Guidance (CFG) scales to synthesize diverse pairs of 'source' (low-CFG) and 'edited' (high-CFG) latents on-the-fly. The source latent serves as structural information of the video, while the edited latent provides the target transformation for learning propagation. Our pipeline enables an additional adapter attached to the pre-trained VDM to learn to propagate edits via Guidance-Modulated Flow Matching (GMFM) loss, which guides the model to replicate the target transformation. Our on-the-fly supervision ensures the model to learn temporally consistent and dynamic transformations. Extensive experiments demonstrate that our PropFly significantly outperforms the state-of-the-art methods on various video editing tasks, producing high-quality editing results.

</details>


### [13] [VAGNet: Grounding 3D Affordance from Human-Object Interactions in Videos](https://arxiv.org/abs/2602.20608)
*Aihua Mao,Kaihang Huang,Yong-Jin Liu,Chee Seng Chan,Ying He*

Main category: cs.CV

TL;DR: 本论文提出了一种基于视频引导的3D物体可供性定位方法，通过与静态方法对比，显著提升了人-物交互接触区域的识别效果，并发布了相关数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 现有3D物体可供性定位方法大多只依赖静态的视觉或文本线索，未能反映可供性由动态动作定义，因此难以准确定位实际交互的接触区域。作者认为人类通过观察和模仿动作学习如何使用物品，因此应引入动态的视频线索。

Method: 作者提出了一种视频引导的3D可供性定位框架VAGNet，将视频中的交互动作线索与3D结构对齐，从而解决纯静态方法存在的歧义。此外，作者还新构建了PVAD数据集，首次在配对的人-物交互视频与3D模型上进行可供性标注。

Result: 在新提出的PVAD数据集上，VAGNet在定位人-物交互接触区域的任务中比现有静态方法显著更优，达到了当前最优表现。

Conclusion: 引入动态视频线索能够为3D物体可供性定位带来更精确的功能性监督，提升模型效果。相关的代码与数据集也会公开，有助于推动相关研究领域发展。

Abstract: 3D object affordance grounding aims to identify regions on 3D objects that support human-object interaction (HOI), a capability essential to embodied visual reasoning. However, most existing approaches rely on static visual or textual cues, neglecting that affordances are inherently defined by dynamic actions. As a result, they often struggle to localize the true contact regions involved in real interactions. We take a different perspective. Humans learn how to use objects by observing and imitating actions, not just by examining shapes. Motivated by this intuition, we introduce video-guided 3D affordance grounding, which leverages dynamic interaction sequences to provide functional supervision. To achieve this, we propose VAGNet, a framework that aligns video-derived interaction cues with 3D structure to resolve ambiguities that static cues cannot address. To support this new setting, we introduce PVAD, the first HOI video-3D pairing affordance dataset, providing functional supervision unavailable in prior works. Extensive experiments on PVAD show that VAGNet achieves state-of-the-art performance, significantly outperforming static-based baselines. The code and dataset will be open publicly.

</details>


### [14] [Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video](https://arxiv.org/abs/2602.20658)
*Mohammad Sadra Rajabi,Aanuoluwapo Ojelade,Sunwook Kim,Maury A. Nussbaum*

Main category: cs.CV

TL;DR: 本研究提出了利用视觉-语言模型（VLMs）从视频中非侵入式估算NIOSH修订搬运方程中水平和垂直手部距离的方法，并验证了其在多种摄像视角下的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统测量水平和垂直手部距离的方法需人工或专用感测设备，实际工作场景中应用困难，因此亟需一种简便、准确、非侵入式的新方法提升人体工学风险评估效率。

Method: 开发了两种基于视觉-语言模型的多阶段流程：一为文本引导的检测流程，二为检测加分割流程。它们通过文本引导定位任务相关区域，提取视觉特征，并用基于Transformer的时序回归模型估算搬运动作起止时的H和V参数。通过七种摄像视角及跨被试验证评估方法表现。

Result: 分割为基础的多视角流程在不同条件下均表现最优，水平距离平均绝对误差为6-8厘米，垂直距离误差为5-8厘米。像素级分割比单纯检测流程可分别将H和V估算误差降低20-30%和35-40%。

Conclusion: 利用VLM的多阶段视频分析流程能较为准确地、非侵入式地估算RNLE参数，具有在实际工效学风险评估场景中应用的可行性。

Abstract: Manual lifting tasks are a major contributor to work-related musculoskeletal disorders, and effective ergonomic risk assessment is essential for quantifying physical exposure and informing ergonomic interventions. The Revised NIOSH Lifting Equation (RNLE) is a widely used ergonomic risk assessment tool for lifting tasks that relies on six task variables, including horizontal (H) and vertical (V) hand distances; such distances are typically obtained through manual measurement or specialized sensing systems and are difficult to use in real-world environments. We evaluated the feasibility of using innovative vision-language models (VLMs) to non-invasively estimate H and V from RGB video streams. Two multi-stage VLM-based pipelines were developed: a text-guided detection-only pipeline and a detection-plus-segmentation pipeline. Both pipelines used text-guided localization of task-relevant regions of interest, visual feature extraction from those regions, and transformer-based temporal regression to estimate H and V at the start and end of a lift. For a range of lifting tasks, estimation performance was evaluated using leave-one-subject-out validation across the two pipelines and seven camera view conditions. Results varied significantly across pipelines and camera view conditions, with the segmentation-based, multi-view pipeline consistently yielding the smallest errors, achieving mean absolute errors of approximately 6-8 cm when estimating H and 5-8 cm when estimating V. Across pipelines and camera view configurations, pixel-level segmentation reduced estimation error by approximately 20-30% for H and 35-40% for V relative to the detection-only pipeline. These findings support the feasibility of VLM-based pipelines for video-based estimation of RNLE distance parameters.

</details>


### [15] [AnimeAgent: Is the Multi-Agent via Image-to-Video models a Good Disney Storytelling Artist?](https://arxiv.org/abs/2602.20664)
*Hailong Yan,Shice Liu,Tao Wang,Xiangtao Zhang,Yijie Zhong,Jinwei Chen,Le Zhang,Bo Li*

Main category: cs.CV

TL;DR: 本文提出AnimeAgent——一种基于图像到视频（I2V）模型的多智能体定制分镜生成系统，显著提升了一致性、表现力及风格化效果，并收集建立了CSG新基准。


<details>
  <summary>Details</summary>
Motivation: 现有定制分镜生成方法，多基于静态扩散模型，无论单次推理还是多智能体方法，都存在动态表达力差、一致性不足及评估标准不健全等问题，难以满足高质量、多角色、风格化故事叙述的需求。

Method: 受迪士尼“逐帧-关键帧结合”流程启发，AnimeAgent将图像到视频(I2V)模型作为核心，利用其隐式运动先验提升分镜间一致性和表现力，并引入融合主观与客观的评审机制支持迭代优化。此外，作者构建了人工标注的分镜基准数据集以评估方法表现。

Result: 实验显示，AnimeAgent在角色一致性、提示吻合度（prompt fidelity）、风格化等维度优于现有方法，达到了当前最优（SOTA）性能，并积极推动CSG任务标准化。

Conclusion: AnimeAgent为动画风格定制分镜生成提供了有效的系统性解决方案，在表达力、一致性、风格化及评估标注方面均有创新突破，推动了领域发展。

Abstract: Custom Storyboard Generation (CSG) aims to produce high-quality, multi-character consistent storytelling. Current approaches based on static diffusion models, whether used in a one-shot manner or within multi-agent frameworks, face three key limitations: (1) Static models lack dynamic expressiveness and often resort to "copy-paste" pattern. (2) One-shot inference cannot iteratively correct missing attributes or poor prompt adherence. (3) Multi-agents rely on non-robust evaluators, ill-suited for assessing stylized, non-realistic animation. To address these, we propose AnimeAgent, the first Image-to-Video (I2V)-based multi-agent framework for CSG. Inspired by Disney's "Combination of Straight Ahead and Pose to Pose" workflow, AnimeAgent leverages I2V's implicit motion prior to enhance consistency and expressiveness, while a mixed subjective-objective reviewer enables reliable iterative refinement. We also collect a human-annotated CSG benchmark with ground-truth. Experiments show AnimeAgent achieves SOTA performance in consistency, prompt fidelity, and stylization.

</details>


### [16] [From Perception to Action: An Interactive Benchmark for Vision Reasoning](https://arxiv.org/abs/2602.21015)
*Yuhao Wu,Maojia Song,Yihuai Lan,Lei Wang,Zhiqiang Hu,Yao Xiao,Heng Zhou,Weihua Zheng,Dylan Raharja,Soujanya Poria,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: 本文提出了一个名为CHAIN的新基准，用于检验视觉语言模型（VLM）在物理结构理解和因果推理方面的能力。结果显示，目前的主流模型在结构化、多步操作任务中的表现仍然不佳。


<details>
  <summary>Details</summary>
Motivation: 当前VLM的评估多集中在单轮、与结构无关的任务上，无法衡量模型对于物理世界复杂结构和因果约束的推理与操作能力。因此需要更具挑战性的任务考察模型的真实物理理解。

Method: 作者构建了CHAIN基准，包含交互式3D物理环境，如机械拼接、立体堆叠和装箱等任务，要求模型理解物理约束并完成结构化的动作序列。对最先进的VLM与扩散模型在统一交互环境下进行了系统测试。

Result: 测试显示，尽管这些模型在传统被动感知任务中表现优异，但在内部化物理结构、因果约束和生成可靠长序列计划方面仍有明显短板。模型往往无法稳定地将感知到的结构转化为有效动作。

Conclusion: 该项工作揭示了主流VLM在结构理解与长时序物理推理上的显著不足，CHAIN基准为模型的实际物理认知与因果行动能力评估提供了更有挑战性的参考工具。

Abstract: Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [17] [Talking to Yourself: Defying Forgetting in Large Language Models](https://arxiv.org/abs/2602.20162)
*Yutao Sun,Mingshuai Chen,Tiancheng Zhao,Phillip Miao,Zilun Zhang,Haozhan Shen,Ruizhe Zhu,Jianwei Yin*

Main category: cs.CL

TL;DR: 本文提出了一种名为SA-SFT的自增强微调方法，通过模型自生成对话数据，与任务数据混合训练，有效缓解了大语言模型微调过程中的灾难性遗忘问题，并提升了任务内表现。无需外部数据或额外调参，简便高效。


<details>
  <summary>Details</summary>
Motivation: 在对大语言模型进行特定任务微调时，容易出现灾难性遗忘，导致模型原有的通用能力受损。当前常用的技术（如冻结层和外部数据混合）仍然有局限，因此需要新的、更简洁有效的解决方案。

Method: SA-SFT方法要求模型在微调前先自生成大量自问自答形式的数据，然后将这些自生成数据与任务数据混合，用常规流程微调，无需改变优化算法或训练日程，也不依赖外部数据或特殊调参。

Result: 在50个评测场景中，SA-SFT的表现与原模型相当或更佳。在40个场景中超越了常见的基线方法（如冻结层、外部数据混合等），在确保模型通用能力的同时，进一步提升了特定任务表现。理论分析还揭示了灾难性遗忘部分来源于风格引起的参数漂移，SA-SFT能够有效修正。

Conclusion: SA-SFT是一种无需外部数据、实现简便、效果显著的自增强微调方法，为大语言模型的鲁棒适应与灾难性遗忘的缓解提供了新思路。

Abstract: Catastrophic forgetting remains a major challenge when fine-tuning large language models (LLMs) on narrow, task-specific data, often degrading their general knowledge and reasoning abilities. We propose SA-SFT, a lightweight self-augmentation routine in which an LLM generates self-dialogues prior to fine-tuning, and the resulting self-authored data are mixed with task data without modifying optimization or training schedules.
  Despite requiring no external data or additional tuning, SA-SFT consistently mitigates catastrophic forgetting while improving in-domain performance. Across 50 evaluation scenarios, it maintains performance comparable to the original model and achieves the best results in 40 cases, outperforming common baselines such as layer freezing and external data mixing. Guided by these empirical findings, we further present a theoretical analysis suggesting that forgetting can partly stem from style-induced parameter drift, and that self-alignment through self-generated data provides an effective means to counteract this effect. Overall, our results indicate that self-augmentation offers a simple and effective mechanism for robust LLM adaptation without incurring catastrophic forgetting.

</details>


### [18] [Benchmarking Distilled Language Models: Performance and Efficiency in Resource-Constrained Settings](https://arxiv.org/abs/2602.20164)
*Sachin Gopal Wani,Eric Page,Ajay Dholakia,David Ellison*

Main category: cs.CL

TL;DR: 本文评估了经过知识蒸馏的小型语言模型（SLM）在性能和计算成本方面的表现，结果显示，知识蒸馏显著提高了计算效率，且模型性能优于传统同类产品。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型资源消耗巨大，难以应用于受限环境，本文旨在探索知识蒸馏作为高效构建小型、高性能模型的可行路径，并量化其在实际部署中的计算与性能优势。

Method: 对比实验方法：分别对比知识蒸馏后模型与原始（vanilla）模型、专有模型的性能与计算消耗；重点评估不同方法下模型的推理能力和所需的计算资源。

Result: 知识蒸馏后生成的8B参数模型，其训练所需计算量比原始模型少2000倍，但推理能力与更大规模标准模型（参数量是其十倍）相当或更优。

Conclusion: 知识蒸馏不仅是一种模型压缩方法，更应成为构建高效AI的主流策略；其可以打造高性能且更易获得的小型语言模型，推动AI进一步普及和落地。

Abstract: Knowledge distillation offers a transformative pathway to developing powerful, yet efficient, small language models (SLMs) suitable for resource-constrained environments. In this paper, we benchmark the performance and computational cost of distilled models against their vanilla and proprietary counterparts, providing a quantitative analysis of their efficiency. Our results demonstrate that distillation creates a superior performance-tocompute curve. We find that creating a distilled 8B model is over 2,000 times more compute-efficient than training its vanilla counterpart, while achieving reasoning capabilities on par with, or even exceeding, standard models ten times its size. These findings validate distillation not just as a compression technique, but as a primary strategy for building state-of-the-art, accessible AI

</details>


### [19] [ConceptRM: The Quest to Mitigate Alert Fatigue through Consensus-Based Purity-Driven Data Cleaning for Reflection Modelling](https://arxiv.org/abs/2602.20166)
*Yongda Yu,Lei Zhang,Xinxin Guo,Minghui Yu,Zhengqi Zhuang,Guoping Rong,Haifeng Shen,Zhengfeng Li,Boge Wang,Guoan Zhang,Bangyu Xiang,Xiaobin Xu*

Main category: cs.CL

TL;DR: 本文提出了ConceptRM方法，通过极少量专家标注和协同学习，有效从噪声数据中筛选高质量负样本，显著提升了反思模型拦截虚假告警的能力，并大幅超越了当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 智能体生成的告警过量且多为误报，导致用户警觉疲劳，从而遗漏关键告警。现有训练过滤模型方式面临数据标注噪声大、人工清洗成本高的难题。

Method: 提出ConceptRM方法，利用少量专家标注为锚点，通过扰动生成不同噪声比例数据集，并用协同教学（co-teaching）训练多个模型协同学习，通过模型共识判定，从噪声数据中筛选出可靠负样本用以训练过滤模型。

Result: ConceptRM在拦截虚假告警任务上，相比主流大模型基线，在域内和跨域数据集分别提升了最多53.31%和41.67%，且显著降低了人工标注成本。

Conclusion: ConceptRM能以极低的标注成本，有效提升反思模型的告警过滤效果，具有很好的泛化性和实际应用价值。

Abstract: In many applications involving intelligent agents, the overwhelming volume of alerts (mostly false) generated by the agents may desensitize users and cause them to overlook critical issues, leading to the so-called ''alert fatigue''. A common strategy is to train a reflection model as a filter to intercept false alerts with labelled data collected from user verification feedback. However, a key challenge is the noisy nature of such data as it is often collected in production environments. As cleaning noise via manual annotation incurs high costs, this paper proposes a novel method ConceptRM for constructing a high-quality corpus to train a reflection model capable of effectively intercepting false alerts. With only a small amount of expert annotations as anchors, ConceptRM creates perturbed datasets with varying noise ratios and utilizes co-teaching to train multiple distinct models for collaborative learning. By analyzing the consensus decisions of these models, it effectively identifies reliable negative samples from a noisy dataset. Experimental results demonstrate that ConceptRM significantly enhances the interception of false alerts with minimal annotation cost, outperforming several state-of-the-art LLM baselines by up to 53.31% on in-domain datasets and 41.67% on out-of-domain datasets.

</details>


### [20] [InterviewSim: A Scalable Framework for Interview-Grounded Personality Simulation](https://arxiv.org/abs/2602.20294)
*Yu Li,Pranav Narayanan Venkit,Yada Pruksachatkun,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 该论文提出并实现了一种基于真实访谈数据的个性模拟大语言模型评估框架，系统比较了不同方法在还原个性上的效果。


<details>
  <summary>Details</summary>
Motivation: 现有大模型针对个性模拟的评估方法多基于问卷或AI访谈等间接指标，未能直接衡量模型对现实个性的还原能力，因此缺乏对真实数据的高质量验证。

Method: 作者采集了1000位公众人物、总计670,000余对问答的真实访谈数据，并设计了包含内容相似度、事实一致性、个性契合和知识保持四项指标的多维评估框架，系统比较了基于访谈检索增强和按时间顺序推进两种建模方式。

Result: 实验显示，基于真实访谈数据的模拟方法显著优于仅用人物档案或模型已有知识的方法；访谈检索增强法更擅长再现个性风格与回答质量，而时序法更利于事实一致与知识保存。

Conclusion: 研究为大语言模型个性模拟领域提供了高质量评估工具和方法选择依据，有助于更好地平衡个性还原和事实保持，为相关应用与研究提供了实证参考。

Abstract: Simulating real personalities with large language models requires grounding generation in authentic personal data. Existing evaluation approaches rely on demographic surveys, personality questionnaires, or short AI-led interviews as proxies, but lack direct assessment against what individuals actually said. We address this gap with an interview-grounded evaluation framework for personality simulation at a large scale. We extract over 671,000 question-answer pairs from 23,000 verified interview transcripts across 1,000 public personalities, each with an average of 11.5 hours of interview content. We propose a multi-dimensional evaluation framework with four complementary metrics measuring content similarity, factual consistency, personality alignment, and factual knowledge retention. Through systematic comparison, we demonstrate that methods grounded in real interview data substantially outperform those relying solely on biographical profiles or the model's parametric knowledge. We further reveal a trade-off in how interview data is best utilized: retrieval-augmented methods excel at capturing personality style and response quality, while chronological-based methods better preserve factual consistency and knowledge retention. Our evaluation framework enables principled method selection based on application requirements, and our empirical findings provide actionable insights for advancing personality simulation research.

</details>


### [21] [What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance](https://arxiv.org/abs/2602.20300)
*William Watson,Nicole Cho,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: 本文提出查询本身的特征也会影响大语言模型（LLM）产生幻觉（hallucination）的概率，并通过大规模实验证实了不同查询特征与幻觉风险之间的关联。


<details>
  <summary>Details</summary>
Motivation: 以往研究将LLM幻觉主要归因于模型或解码策略的缺陷，本文则基于语言学理论，探讨查询表达方式对模型输出的影响，试图揭示某些查询类型为何更易诱发模型产生幻觉。

Method: 作者构建了22维的查询特征向量，涵盖从句复杂度、词汇稀有度、照应、否定、可回答性到意图明确性等影响理解的语言因素。在大约37万个真实查询上，分析这些特征与模型产生幻觉的关系。

Result: 发现某些特征（如深层从句嵌套、信息欠缺）明显增加幻觉风险，而意图明确、可回答性强的查询则大大降低幻觉概率。某些特征如领域特殊性则表现出因数据集和模型不同而异的混合效应。

Conclusion: 查询本身的语言特征和表达方式与LLM产生幻觉概率显著相关，为后续通过引导式查询重写和针对性干预降低幻觉风险提供了理论和实证依据。

Abstract: Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent "risk landscape": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.

</details>


### [22] [No One Size Fits All: QueryBandits for Hallucination Mitigation](https://arxiv.org/abs/2602.20332)
*Nicole Cho,William Watson,Alec Koppel,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: 本论文针对大型语言模型（LLMs）产生幻觉问题，提出了QueryBandits模型无关的上下文多臂老虎机自适应查询重写框架，有效减少了幻觉现象，适用于闭源模型。


<details>
  <summary>Details</summary>
Motivation: 目前关于幻觉检测与缓解的研究大多集中在开源模型上，且常依赖后处理或参数编辑。而主流的闭源模型广泛用于机构实际应用中，却缺乏针对幻觉问题的有效研究和解决方法。

Method: 提出QueryBandits框架，将查询重写作为上下文多臂老虎机问题，利用经验验证且校准的奖励函数，在线自适应地学习最优的查询重写策略。重点测试了Thompson采样和多种静态策略，并在16个问答场景中与不重写和静态重写策略进行比较。

Result: QueryBandit（Thompson Sampling）相较不重写基线提升胜率至87.5%，超越静态策略42.6%和60.3%。所有上下文bandit策略均优于传统bandit，单一重写策略无法适配所有场景，部分静态策略甚至比不重写更糟。

Conclusion: QueryBandits无需对模型参数进行调整或重新训练，仅借助前向传递即可提升闭源模型的表现，实现了在实际部署中无需模型结构变动即可缓解幻觉问题。

Abstract: Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.

</details>


### [23] [Natural Language Processing Models for Robust Document Categorization](https://arxiv.org/abs/2602.20336)
*Radoslaw Roszczyk,Pawel Tecza,Maciej Stodolski,Krzysztof Siwek*

Main category: cs.CL

TL;DR: 本文评估了三种常用机器学习方法在自动文本分类中的表现，重点考察了分类准确率与计算效率之间的权衡，并实现了一个真实可用的演示系统。


<details>
  <summary>Details</summary>
Motivation: 在实际自动化流程中，文档分类任务常要求高准确率同时也要兼顾计算资源与运行效率，尤其在处理类别不平衡的数据时更具挑战性。本文旨在比较不同模型在这些需求下的表现，并为行业应用提供指导。

Method: 研究对比了朴素贝叶斯、双向LSTM（BiLSTM）和微调后的BERT三种模型，分别测试它们在文档自动分类任务中的准确率、训练耗时和资源消耗，并实现了一个完整的自动请求分发系统，用于实际场景的验证。

Result: BERT模型准确率最高（>99%），但训练耗时和资源消耗最大；BiLSTM在准确率（约98.56%）和计算开销之间达到了良好平衡；朴素贝叶斯则训练最快（毫秒级），但准确率最低（约94.5%）。所有方法都受到类别不平衡的影响，尤其是在少数类识别上。

Conclusion: BiLSTM模型在本场景下综合表现最佳，是实际部署的优选方案。文中还指出未来可以进一步优化Transformer类模型并改善少数类识别能力。

Abstract: This article presents an evaluation of several machine learning methods applied to automated text classification, alongside the design of a demonstrative system for unbalanced document categorization and distribution. The study focuses on balancing classification accuracy with computational efficiency, a key consideration when integrating AI into real world automation pipelines. Three models of varying complexity were examined: a Naive Bayes classifier, a bidirectional LSTM network, and a fine tuned transformer based BERT model.
  The experiments reveal substantial differences in performance. BERT achieved the highest accuracy, consistently exceeding 99\%, but required significantly longer training times and greater computational resources. The BiLSTM model provided a strong compromise, reaching approximately 98.56\% accuracy while maintaining moderate training costs and offering robust contextual understanding. Naive Bayes proved to be the fastest to train, on the order of milliseconds, yet delivered the lowest accuracy, averaging around 94.5\%. Class imbalance influenced all methods, particularly in the recognition of minority categories.
  A fully functional demonstrative system was implemented to validate practical applicability, enabling automated routing of technical requests with throughput unattainable through manual processing. The study concludes that BiLSTM offers the most balanced solution for the examined scenario, while also outlining opportunities for future improvements and further exploration of transformer architectures.

</details>


### [24] [FinAnchor: Aligned Multi-Model Representations for Financial Prediction](https://arxiv.org/abs/2602.20859)
*Zirui He,Huopu Zhang,Yanguang Liu,Sirui Wu,Mengnan Du*

Main category: cs.CL

TL;DR: 本文提出了一种用于金融文本预测的新框架——FinAnchor。它无需微调不同的LLMs，通过对齐和聚合多模型的嵌入，实现了更准确的金融预测。实验表明该方法优于单模型和传统集成方法。


<details>
  <summary>Details</summary>
Motivation: 金融领域长文本信息复杂、关键信号稀疏且多噪声，不同任务和时期对LLM嵌入的需求各异，单一模型很难始终表现优异。

Method: FinAnchor通过选择一个锚点嵌入空间，将其他模型的特征经过线性变换对齐到该空间，然后聚合所有模型的对齐特征，得到统一表达用于后续金融预测任务。整个过程无需对原始LLM微调。

Result: FinAnchor在多个金融NLP任务中，持续优于强基线单模型和标准集成方法，展示了其通过异构嵌入对齐实现鲁棒预测的有效性。

Conclusion: 将多个LLM嵌入空间对齐聚合后，能提升金融预测任务表现；FinAnchor提供了一种轻量化、无需模型微调的方法，实现多模型优势互补，增强金融NLP系统的实用价值。

Abstract: Financial prediction from long documents involves significant challenges, as actionable signals are often sparse and obscured by noise, and the optimal LLM for generating embeddings varies across tasks and time periods. In this paper, we propose FinAnchor(Financial Anchored Representations), a lightweight framework that integrates embeddings from multiple LLMs without fine-tuning the underlying models. FinAnchor addresses the incompatibility of feature spaces by selecting an anchor embedding space and learning linear mappings to align representations from other models into this anchor. These aligned features are then aggregated to form a unified representation for downstream prediction. Across multiple financial NLP tasks, FinAnchor consistently outperforms strong single-model baselines and standard ensemble methods, demonstrating the effectiveness of anchoring heterogeneous representations for robust financial prediction.

</details>


### [25] [Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification](https://arxiv.org/abs/2602.21082)
*Vishal Patil,Shree Vaishnavi Bacha,Revanth Yamani,Yidan Sun,Mayank Kejriwal*

Main category: cs.CL

TL;DR: 本研究提出了一种结合大语言模型（LLM）和传统机器学习方法的混合框架，自动进行大规模客户评价的方面情感分析，验证了其在餐饮行业大数据下的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着用户评论数量激增，企业和顾客越来越依赖这些评论信息，但现有技术难以高效分析数百万条非结构化评论。因此，不断探索性能与可扩展性兼备的自动化分析方法成为重要课题。

Method: 方法首先利用ChatGPT从餐厅评论抽样中识别出用餐体验的关键方面；然后基于人工标注的评论，训练传统的情感分类器，并将其应用到470万条大型在线平台评论数据。最后通过回归分析检验机器标注的各方面对整体评分的解释力。

Result: 结果显示，自动化机器标注的用餐方面在不同用餐体验维度、菜系和地区下，均能显著解释整体餐厅评分的方差。混合方法能有效扩展到大规模评论情感分析场景。

Conclusion: 将LLM和传统机器学习结合，兼顾了情感分析的精度与扩展性，为餐饮和其他服务行业提供了实用的大规模客户反馈自动分析框架。

Abstract: Customer-provided reviews have become an important source of information for business owners and other customers alike. However, effectively analyzing millions of unstructured reviews remains challenging. While large language models (LLMs) show promise for natural language understanding, their application to large-scale review analysis has been limited by computational costs and scalability concerns. This study proposes a hybrid approach that uses LLMs for aspect identification while employing classic machine-learning methods for sentiment classification at scale. Using ChatGPT to analyze sampled restaurant reviews, we identified key aspects of dining experiences and developed sentiment classifiers using human-labeled reviews, which we subsequently applied to 4.7 million reviews collected over 17 years from a major online platform. Regression analysis reveals that our machine-labeled aspects significantly explain variance in overall restaurant ratings across different aspects of dining experiences, cuisines, and geographical regions. Our findings demonstrate that combining LLMs with traditional machine learning approaches can effectively automate aspect-based sentiment analysis of large-scale customer feedback, suggesting a practical framework for both researchers and practitioners in the hospitality industry and potentially, other service sectors.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [26] [Global Prior Meets Local Consistency: Dual-Memory Augmented Vision-Language-Action Model for Efficient Robotic Manipulation](https://arxiv.org/abs/2602.20200)
*Zaijing Li,Bing Hu,Rui Shao,Gongwei Chen,Dongmei Jiang,Pengwei Xie,Jianye Hao,Liqiang Nie*

Main category: cs.RO

TL;DR: 本论文提出了OptimusVLA，一种具有全局先验记忆（GPM）和局部一致性记忆（LCM）的分层视觉-语言-动作模型，显著提升了推理效率和鲁棒性，并在多项模拟和实际机器人任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有分层视觉-语言-动作（VLA）模型在机器人操作任务中已经占据主流，但其动作生成过程存在推理效率低下和鲁棒性差的问题。具体来说，1）动作生成的初始噪声分布与目标分布差异大，导致生成过程冗长且易产生不可行动作；2）只考虑当前观测，忽视历史序列约束，无法保证任务进展和轨迹一致性。

Method: 论文提出OptimusVLA框架，包含两个关键模块：1）全局先验记忆（GPM）：用从语义相似轨迹中检索的任务先验替换高斯噪声，缩短生成路径并减少模型求解成本；2）局部一致性记忆（LCM）：动态建模已执行动作序列，推断任务进展，并引入一致性约束，保证轨迹的时序连贯与光滑。

Result: OptimusVLA在三个模拟基准（LIBERO、CALVIN、RoboTwin 2.0 Hard）上均大幅超越强基线，在LIBERO上平均成功率98.6%，在CALVIN上比pi_0提升13.5%，在RoboTwin 2.0 Hard上平均成功率38%。在实际机器人评测中，OptimusVLA在泛化及长时序任务中成功率分别比pi_0高42.9%和52.4%，推理速度提升2.9倍。

Conclusion: OptimusVLA通过双记忆模块有效改善了分层VLA模型推理效率与鲁棒性，为机器人操作任务提供了更优的范式与性能，展现出良好的泛化能力和实际应用潜力。

Abstract: Hierarchical Vision-Language-Action (VLA) models have rapidly become a dominant paradigm for robotic manipulation. It typically comprising a Vision-Language backbone for perception and understanding, together with a generative policy for action generation. However, its performance is increasingly bottlenecked by the action generation proceess. (i) Low inference efficiency. A pronounced distributional gap between isotropic noise priors and target action distributions, which increases denoising steps and the incidence of infeasible samples. (ii) Poor robustness. Existing policies condition solely on the current observation, neglecting the constraint of history sequence and thus lacking awareness of task progress and temporal consistency. To address these issues, we introduce OptimusVLA, a dual-memory VLA framework with Global Prior Memory (GPM) and Local Consistency Memory (LCM). GPM replaces Gaussian noise with task-level priors retrieved from semantically similar trajectories, thereby shortening the generative path and reducing the umber of function evaluations (NFE). LCM dynamically models executed action sequence to infer task progress and injects a learned consistency constraint that enforces temporal coherence and smoothness of trajectory. Across three simulation benchmarks, OptimusVLA consistently outperforms strong baselines: it achieves 98.6% average success rate on LIBERO, improves over pi_0 by 13.5% on CALVIN, and attains 38% average success rate on RoboTwin 2.0 Hard. In Real-World evaluation, OptimusVLA ranks best on Generalization and Long-horizon suites, surpassing pi_0 by 42.9% and 52.4%, respectively, while delivering 2.9x inference speedup.

</details>


### [27] [Vision-Based Reasoning with Topology-Encoded Graphs for Anatomical Path Disambiguation in Robot-Assisted Endovascular Navigation](https://arxiv.org/abs/2602.20215)
*Jiyuan Zhao,Zhengyu Shi,Wentong Tian,Tianliang Yao,Dong Liu,Tao Liu,Yizhe Wu,Peng Qi*

Main category: cs.RO

TL;DR: 本文提出了一种针对机器人辅助经皮冠状动脉介入治疗（PCI）路径规划的新方法，在二维血管造影（DSA）条件下，有效提升了自动路径识别与选择的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于机器人缺乏医生的空间感知和触觉反馈，在2D DSA图像下，路径规划容易受投影失真影响，尤其在血管分叉处容易产生模糊和歧义，因此亟需新的方法解决这一问题。

Method: 提出了两阶段框架：第一阶段使用空间坐标注意力正则化的UNet（SCAR-UNet）进行冠状动脉血管分割并提取中心线与分叉点；第二阶段利用图注意力网络（GAT）融合血管几何特征，基于血管图结构推理，优化路径规划，区分真实分叉和投影伪交叉。

Result: 在临床DSA数据集上，SCAR-UNet分割Dice系数达93.1%；GAT方法路径判别成功率95.0%，目标到达率90.0%，显著优于传统最短路径（60.0%和55.0%）及启发式方法（75.0%和70.0%）。机器人平台实验证实其实用性与鲁棒性。

Conclusion: 所提框架在机器人PCI路径规划中大幅减少了2D影像引起的路径歧义，提高了分割与路径决策的准确性，具备临床应用前景。

Abstract: Robotic-assisted percutaneous coronary intervention (PCI) is constrained by the inherent limitations of 2D Digital Subtraction Angiography (DSA). Unlike physicians, who can directly manipulate guidewires and integrate tactile feedback with their prior anatomical knowledge, teleoperated robotic systems must rely solely on 2D projections. This mode of operation, simultaneously lacking spatial context and tactile sensation, may give rise to projection-induced ambiguities at vascular bifurcations. To address this challenge, we propose a two-stage framework (SCAR-UNet-GAT) for real-time robotic path planning. In the first stage, SCAR-UNet, a spatial-coordinate-attention-regularized U-Net, is employed for accurate coronary vessel segmentation. The integration of multi-level attention mechanisms enhances the delineation of thin, tortuous vessels and improves robustness against imaging noise. From the resulting binary masks, vessel centerlines and bifurcation points are extracted, and geometric descriptors (e.g., branch diameter, intersection angles) are fused with local DSA patches to construct node features. In the second stage, a Graph Attention Network (GAT) reasons over the vessel graph to identify anatomically consistent and clinically feasible trajectories, effectively distinguishing true bifurcations from projection-induced false crossings. On a clinical DSA dataset, SCAR-UNet achieved a Dice coefficient of 93.1%. For path disambiguation, the proposed GAT-based method attained a success rate of 95.0% and a target-arrival success rate of 90.0%, substantially outperforming conventional shortest-path planning (60.0% and 55.0%) and heuristic-based planning (75.0% and 70.0%). Validation on a robotic platform further confirmed the practical feasibility and robustness of the proposed framework.

</details>


### [28] [Sample-Efficient Learning with Online Expert Correction for Autonomous Catheter Steering in Endovascular Bifurcation Navigation](https://arxiv.org/abs/2602.20216)
*Hao Wang,Tianliang Yao,Bo Lu,Zhiqiang Pei,Liu Dong,Lei Ma,Peng Qi*

Main category: cs.RO

TL;DR: 本论文提出了一种结合在线专家校正的高效强化学习框架，实现了自主导管穿越血管分叉导航，显著提升了训练效率和导航精度。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在导管自主控制中面临稀疏奖励设计和依赖静态血管模型的问题，导致样本效率低和难以适应手术中的变化。解决这些问题对于提升自主手术的可靠性和临床应用至关重要。

Method: 所提框架包括三大模块：（1）基于分割的实时位姿估计实现精确反馈；（2）基于模糊控制器的分叉感知导管方向调整；（3）融合专家先验的奖励生成器，引导强化学习策略。通过引入在线专家校正，提升了复杂血管结构中的策略鲁棒性和训练效率。

Result: 在机器人平台和透明血管模型上实验证明，该方法仅需123轮训练即可收敛，训练效率比基线 Soft Actor-Critic (SAC) 算法提升25.9%，平均定位误差降低到基线的83.8%。

Conclusion: 该方法大幅提升了自主导管穿行复杂血管分叉结构的效率和精度，为远程机器人介入手术场景提供了更可靠的技术路径，且适用于更具挑战性的解剖变异情况。

Abstract: Robot-assisted endovascular intervention offers a safe and effective solution for remote catheter manipulation, reducing radiation exposure while enabling precise navigation. Reinforcement learning (RL) has recently emerged as a promising approach for autonomous catheter steering; however, conventional methods suffer from sparse reward design and reliance on static vascular models, limiting their sample efficiency and generalization to intraoperative variations. To overcome these challenges, this paper introduces a sample-efficient RL framework with online expert correction for autonomous catheter steering in endovascular bifurcation navigation. The proposed framework integrates three key components: (1) A segmentation-based pose estimation module for accurate real-time state feedback, (2) A fuzzy controller for bifurcation-aware orientation adjustment, and (3) A structured reward generator incorporating expert priors to guide policy learning. By leveraging online expert correction, the framework reduces exploration inefficiency and enhances policy robustness in complex vascular structures. Experimental validation on a robotic platform using a transparent vascular phantom demonstrates that the proposed approach achieves convergence in 123 training episodes -- a 25.9% reduction compared to the baseline Soft Actor-Critic (SAC) algorithm -- while reducing average positional error to 83.8% of the baseline. These results indicate that combining sample-efficient RL with online expert correction enables reliable and accurate catheter steering, particularly in anatomically challenging bifurcation scenarios critical for endovascular navigation.

</details>


### [29] [An Approach to Combining Video and Speech with Large Language Models in Human-Robot Interaction](https://arxiv.org/abs/2602.20219)
*Guanting Shen,Zi Tian*

Main category: cs.RO

TL;DR: 本文提出了一种多模态人机交互框架，将先进的视觉-语言模型、语音处理与模糊逻辑结合，实现了精准、适应性强的机械臂控制，命令执行准确率达75%。


<details>
  <summary>Details</summary>
Motivation: 在人机交互中，准确理解人的意图始终是一大挑战，也是实现自然协作的关键。现有的人机交互方法在理解语音与视觉场景、规划动作等方面存在局限，需要更高效的整合多模态信息，以提升机器人对人类命令的理解和执行能力。

Method: 设计了一个多模态HRI系统，将Florence-2用于目标检测，Llama 3.1用于自然语言理解，Whisper用于语音识别，并融合模糊逻辑进行指令解析与任务规划，实现了语音驱动的物体操作。系统在消费级硬件上进行了测试。

Result: 实验表明，该系统在消费级硬件上的命令执行准确率达到75%，显示出较强的鲁棒性和适应性。

Conclusion: 本文所提出的架构不仅提升了人机交互的自然性和直观性，还为今后更复杂的人机协作研究奠定了可扩展的技术基础，为语音与视觉-语言联合处理的人机协作提供了实际路径。

Abstract: Interpreting human intent accurately is a central challenge in human-robot interaction (HRI) and a key requirement for achieving more natural and intuitive collaboration between humans and machines. This work presents a novel multimodal HRI framework that combines advanced vision-language models, speech processing, and fuzzy logic to enable precise and adaptive control of a Dobot Magician robotic arm. The proposed system integrates Florence-2 for object detection, Llama 3.1 for natural language understanding, and Whisper for speech recognition, providing users with a seamless and intuitive interface for object manipulation through spoken commands. By jointly addressing scene perception and action planning, the approach enhances the reliability of command interpretation and execution. Experimental evaluations conducted on consumer-grade hardware demonstrate a command execution accuracy of 75\%, highlighting both the robustness and adaptability of the system. Beyond its current performance, the proposed architecture serves as a flexible and extensible foundation for future HRI research, offering a practical pathway toward more sophisticated and natural human-robot collaboration through tightly coupled speech and vision-language processing.

</details>


### [30] [What Matters for Simulation to Online Reinforcement Learning on Real Robots](https://arxiv.org/abs/2602.20220)
*Yarden As,Dhruva Tirumala,René Zurbrügg,Chenhao Li,Stelian Coros,Andreas Krause,Markus Wulfmeier*

Main category: cs.RO

TL;DR: 本论文系统性地分析了能让在线强化学习在真实机器人上成功应用的具体设计选择。通过大量实物实验，发现主流的做法中有些默认设置反而不利，而推荐的做法更能稳定学习。


<details>
  <summary>Details</summary>
Motivation: 尽管在线强化学习在机器人领域潜力巨大，但许多设计细节被前人忽略，缺乏系统性的经验指导，这限制了其实际部署和推广。

Method: 作者跨三个不同机器平台，进行了100次真实机器人训练实验，对算法、系统和实验流程中的各个设计选择逐一消融分析，总结每个因素对RL成效的影响。

Result: 研究发现，一些广泛采用的默认选择实际对学习过程有害，而遵循推荐的设计选择则能在任务和硬件层面保证学习的稳定性。

Conclusion: 该工作首次用大样本实证研究了RL系统性设计选择，为行业实践者低成本部署RL提供了明确的工程经验与指导。

Abstract: We investigate what specific design choices enable successful online reinforcement learning (RL) on physical robots. Across 100 real-world training runs on three distinct robotic platforms, we systematically ablate algorithmic, systems, and experimental decisions that are typically left implicit in prior work. We find that some widely used defaults can be harmful, while a set of robust, readily adopted design choices within standard RL practice yield stable learning across tasks and hardware. These results provide the first large-sample empirical study of such design choices, enabling practitioners to deploy online RL with lower engineering effort.

</details>


### [31] [FACTO: Function-space Adaptive Constrained Trajectory Optimization for Robotic Manipulators](https://arxiv.org/abs/2602.20225)
*Yichang Feng,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: 本文提出了一种新的轨迹优化算法FACTO，能高效应对具有约束的单/多机械臂轨迹优化问题，并在多种基线方法上实现了更好的解质量和可行性。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹优化方法在处理多机械臂和复杂约束时存在解质量和鲁棒性问题，急需一种通用且效率更高的方法提升轨迹可行性和优化效果。

Method: 采用正交基函数对轨迹进行参数化优化，通过高斯-牛顿近似和指数移动平均平滑子问题，用Levenberg-Marquardt算法在约束活动集的零空间内自适应约束更新，整体在系数空间直接优化。通过与现有主流优化和采样规划器对比表现其优越性。

Result: FACTO在有约束的单/多机械臂轨迹优化场景下，解质量和可行性优于CHOMP、TrajOpt、GPMP2等优化型以及RRT-Connect、PRM等采样型规划器。实际在Franka机械臂上实验证明该方法可落地应用。

Conclusion: FACTO能够高效且稳健地解决多臂协作下复杂约束的轨迹优化问题，突破了现有方法的主要局限性，具备良好的工程应用前景。

Abstract: This paper introduces Function-space Adaptive Constrained Trajectory Optimization (FACTO), a new trajectory optimization algorithm for both single- and multi-arm manipulators. Trajectory representations are parameterized as linear combinations of orthogonal basis functions, and optimization is performed directly in the coefficient space. The constrained problem formulation consists of both an objective functional and a finite-dimensional objective defined over truncated coefficients. To address nonlinearity, FACTO uses a Gauss-Newton approximation with exponential moving averaging, yielding a smoothed quadratic subproblem. Trajectory-wide constraints are addressed using coefficient-space mappings, and an adaptive constrained update using the Levenberg-Marquardt algorithm is performed in the null space of active constraints. Comparisons with optimization-based planners (CHOMP, TrajOpt, GPMP2) and sampling-based planners (RRT-Connect, RRT*, PRM) show the improved solution quality and feasibility, especially in constrained single- and multi-arm scenarios. The experimental evaluation of FACTO on Franka robots verifies the feasibility of deployment.

</details>


### [32] [UniLACT: Depth-Aware RGB Latent Action Learning for Vision-Language-Action Models](https://arxiv.org/abs/2602.20231)
*Manish Kumar Govind,Dominick Reilly,Pu Wang,Srijan Das*

Main category: cs.RO

TL;DR: 论文提出了一种结合几何结构信息的深度感知潜在动作预训练方法，有效提升了机器人视觉-语言-动作模型在丰富交互操作任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有仅利用RGB视频无监督学习得到的潜在动作编码缺乏显式的三维几何结构信息，而这种空间结构对于精确、复杂的机器人操作任务非常重要。

Method: 作者提出了UniLACT，一种基于Transformer的视觉-语言-动作模型，引入了深度感知的潜在动作表征预训练，并通过UniLARN框架使用逆动力学和正向动力学目标，统一学习RGB与深度的跨模态共享潜在动作空间。该空间可生成模态特异和统一的潜在动作伪标签，用于深度感知VLA模型的预训练。

Result: 在仿真和真实机器人环境中的大量实验表明，本文提出的深度感知统一潜在动作方法在域内、域外预训练和不同操作任务下均优于仅基于RGB的对比方法。

Conclusion: 结合深度信息的潜在动作预训练为机器人理解和执行复杂操作提供了更强的空间先验，显著提升了基于视觉-语言的操作泛化能力。

Abstract: Latent action representations learned from unlabeled videos have recently emerged as a promising paradigm for pretraining vision-language-action (VLA) models without explicit robot action supervision. However, latent actions derived solely from RGB observations primarily encode appearance-driven dynamics and lack explicit 3D geometric structure, which is essential for precise and contact-rich manipulation. To address this limitation, we introduce UniLACT, a transformer-based VLA model that incorporates geometric structure through depth-aware latent pretraining, enabling downstream policies to inherit stronger spatial priors. To facilitate this process, we propose UniLARN, a unified latent action learning framework based on inverse and forward dynamics objectives that learns a shared embedding space for RGB and depth while explicitly modeling their cross-modal interactions. This formulation produces modality-specific and unified latent action representations that serve as pseudo-labels for the depth-aware pretraining of UniLACT. Extensive experiments in both simulation and real-world settings demonstrate the effectiveness of depth-aware unified latent action representations. UniLACT consistently outperforms RGB-based latent action baselines under in-domain and out-of-domain pretraining regimes, as well as on both seen and unseen manipulation tasks.

</details>


### [33] [Smoothly Differentiable and Efficiently Vectorizable Contact Manifold Generation](https://arxiv.org/abs/2602.20304)
*Onur Beker,Andreas René Geist,Anselm Paulus,Nico Gürtler,Ji Shi,Sylvain Calinon,Georg Martius*

Main category: cs.RO

TL;DR: 本文提出了一种新的刚体动力学接触模拟框架，兼顾了加速、可向量化和可微分性，显著提升了接触判定效率，并超过了主流模拟器Mujoco XLA。


<details>
  <summary>Details</summary>
Motivation: 现有可微分仿真系统在接触流形生成上存在效率瓶颈，尤其是常用模拟器的相关模块并非为向量化和可微分性设计，导致仿真效率和梯度计算受限。

Method: 作者设计了一套全新的接触生成框架：（1）定义了一组平滑的解析有符号距离元，用于顶点-面碰撞判定；（2）提出了可微分的边-边碰撞子程序，能够输出有符号距离和接触法向。方法兼顾了常规凸元高效但难可微分、障碍法距离函数可微但低效的两类主流思路。

Result: 通过一系列教学级实验和与Mujoco XLA碰撞检测模块进行基准对比，新方法在运行速度上取得了显著提升。

Conclusion: 凭借针对可微分性与向量化优化的框架设计，本文提出的刚体接触判定方法能大幅提升仿真效率，并兼容自动微分，可为机器人学等领域的大规模可微仿真带来技术支撑。

Abstract: Simulating rigid-body dynamics with contact in a fast, massively vectorizable, and smoothly differentiable manner is highly desirable in robotics. An important bottleneck faced by existing differentiable simulation frameworks is contact manifold generation: representing the volume of intersection between two colliding geometries via a discrete set of properly distributed contact points. A major factor contributing to this bottleneck is that the related routines of commonly used robotics simulators were not designed with vectorization and differentiability as a primary concern, and thus rely on logic and control flow that hinder these goals. We instead propose a framework designed from the ground up with these goals in mind, by trying to strike a middle ground between: i) convex primitive based approaches used by common robotics simulators (efficient but not differentiable), and ii) mollified vertex-face and edge-edge unsigned distance-based approaches used by barrier methods (differentiable but inefficient). Concretely, we propose: i) a representative set of smooth analytical signed distance primitives to implement vertex-face collisions, and ii) a novel differentiable edge-edge collision routine that can provide signed distances and signed contact normals. The proposed framework is evaluated via a set of didactic experiments and benchmarked against the collision detection routine of the well-established Mujoco XLA framework, where we observe a significant speedup. Supplementary videos can be found at https://github.com/bekeronur/contax, where a reference implementation in JAX will also be made available at the conclusion of the review process.

</details>


### [34] [Learning Physical Principles from Interaction: Self-Evolving Planning via Test-Time Memory](https://arxiv.org/abs/2602.20323)
*Haoyang Li,Yang You,Hao Su,Leonidas Guibas*

Main category: cs.RO

TL;DR: 本文提出PhysMem框架，让机器人在不更改模型参数的情况下，通过自主交互学习物理知识，并在多项任务中验证有效提升表现。


<details>
  <summary>Details</summary>
Motivation: VLM机器人规划器对物理属性只能进行泛化推理，缺乏针对具体实例的物理预测能力，因此亟需一种可从现场交互快速补充和验证物理知识的方法。

Method: PhysMem框架将机器人交互经验存储为候选物理假设，并设计“先验证后应用”机制：对每个假设，仅在用新观测数据验证通过后，才用于指导后续决策，从而适应环境变化，避免经验误用，无需参数更新。

Result: 在三项真实操作任务和模拟基准测试中，PhysMem在四种VLM骨干网络上实现提升。尤其在砖块插入任务中，抽象原则化方法成功率76%，显著高于单纯经验检索的23%。短时现实部署（30分钟）同样观察到表现提升。

Conclusion: PhysMem能有效增强VLM机器人的物理推理与场景适应能力，通过交互学习和假设验证克服仅凭经验的局限，为机器人在复杂多变现实环境中提供更稳定可靠的表现。

Abstract: Reliable object manipulation requires understanding physical properties that vary across objects and environments. Vision-language model (VLM) planners can reason about friction and stability in general terms; however, they often cannot predict how a specific ball will roll on a particular surface or which stone will provide a stable foundation without direct experience. We present PhysMem, a memory framework that enables VLM robot planners to learn physical principles from interaction at test time, without updating model parameters. The system records experiences, generates candidate hypotheses, and verifies them through targeted interaction before promoting validated knowledge to guide future decisions. A central design choice is verification before application: the system tests hypotheses against new observations rather than applying retrieved experience directly, reducing rigid reliance on prior experience when physical conditions change. We evaluate PhysMem on three real-world manipulation tasks and simulation benchmarks across four VLM backbones. On a controlled brick insertion task, principled abstraction achieves 76% success compared to 23% for direct experience retrieval, and real-world experiments show consistent improvement over 30-minute deployment sessions.

</details>


### [35] [Energy-Based Injury Protection Database: Including Shearing Contact Thresholds for Hand and Finger Using Porcine Surrogates](https://arxiv.org/abs/2602.20362)
*Robin Jeanne Kirschner,Anna Huber,Carina M. Micheler,Dirk Müller,Nader Rajaei,Rainer Burgkart,Sami Haddadin*

Main category: cs.RO

TL;DR: 本研究针对机器人与人类接触不可避免的现实，提出并扩展了机器人碰撞伤害防护的数据集，涵盖了更多复杂的碰撞情景（包括切向碰撞），建立了首个基于能量的伤害防护数据库，为能量约束型安全控制器奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有机器人安全标准及参考数据多针对钝器正撞等简单场景，缺乏对尖锐、切向碰撞等复杂和实际接触情况的临床基础数据，从而限制了机器人系统的全面安全验证和防护能力。

Method: 扩展以往使用猪组织替代物的碰撞实验方法，新增非受限的切向（shearing）碰撞实验，将各类碰撞几何及角度的实验数据整合，系统性评估不同碰撞条件下的安全能量阈值。

Result: 实验表明，非受限的切向碰撞比正撞导致的伤害更少，碰撞角度对伤害结果有显著影响。最终整合所有实验数据，建立了覆盖不同几何、角度、碰撞类型的能量阈值数据库。

Conclusion: 本研究首次建立了基于能量的机器人碰撞伤害防护数据库，为机器人安全控制器的设计提供了科学、可扩展的能量限值依据，提升了人机互动时的安全保障能力。

Abstract: While robotics research continues to propose strategies for collision avoidance in human-robot interaction, the reality of constrained environments and future humanoid systems makes contact inevitable. To mitigate injury risks, energy-constraining control approaches are commonly used, often relying on safety thresholds derived from blunt impact data in EN ISO 10218-2:2025. However, this dataset does not extend to edged or pointed collisions. Without scalable, clinically grounded datasets covering diverse contact scenarios, safety validation remains limited. Previous studies have laid the groundwork by assessing surrogate-based velocity and mass limits across various geometries, focusing on perpendicular impacts. This study expands those datasets by including shearing contact scenarios in unconstrained collisions, revealing that collision angle significantly affects injury outcomes. Notably, unconstrained shearing contacts result in fewer injuries than perpendicular ones. By reevaluating all prior porcine surrogate data, we establish energy thresholds across geometries and contact types, forming the first energy-based Injury Protection Database. This enables the development of meaningful energy-limiting controllers that ensure safety across a wide range of realistic collision events.

</details>


### [36] [Generalizing from References using a Multi-Task Reference and Goal-Driven RL Framework](https://arxiv.org/abs/2602.20375)
*Jiashun Wang,M. Eva Mungai,He Li,Jean Pierre Sleiman,Jessica Hodgins,Farbod Farshidian*

Main category: cs.RO

TL;DR: 提出了一种新型多任务强化学习框架，可同时获得自然（具有人类特征）的运动技能和高度适应性，避免了仅依赖参考动作或纯任务驱动RL的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法容易受限于参考数据，泛化能力差；而纯强化学习虽然灵活但动作质量通常不自然。作者希望结合两者优点，实现既有人类风格、又能适应新任务的控制策略。

Method: 框架训练单一策略网络，同时进行两个任务：一是以参考轨迹为密集奖励源的模仿任务（但轨迹不作为输入）；二是基于随机目标和初始状态的泛化任务，仅根据目标完成情况奖励。策略通过联合优化这两种目标，汲取人类动作特点并提升泛化能力。方法无需对抗损失、显式跟踪或参考依赖推理。

Result: 方法在复杂的箱式跑酷场景中实现了包括跳跃、攀爬等丰富运动技能，控制器在远离参考分布的新任务中仍保持自然动作表现。还展示了多个技能组合生成长时序复杂行为。

Conclusion: 所提方法有效兼顾了动作的自然性与适应性，突破了现有模仿学习与强化学习各自的瓶颈，适用于复杂环境下的灵巧人形机器人控制。

Abstract: Learning agile humanoid behaviors from human motion offers a powerful route to natural, coordinated control, but existing approaches face a persistent trade-off: reference-tracking policies are often brittle outside the demonstration dataset, while purely task-driven Reinforcement Learning (RL) can achieve adaptability at the cost of motion quality. We introduce a unified multi-task RL framework that bridges this gap by treating reference motion as a prior for behavioral shaping rather than a deployment-time constraint. A single goal-conditioned policy is trained jointly on two tasks that share the same observation and action spaces, but differ in their initialization schemes, command spaces, and reward structures: (i) a reference-guided imitation task in which reference trajectories define dense imitation rewards but are not provided as policy inputs, and (ii) a goal-conditioned generalization task in which goals are sampled independently of any reference and where rewards reflect only task success. By co-optimizing these objectives within a shared formulation, the policy acquires structured, human-like motor skills from dense reference supervision while learning to adapt these skills to novel goals and initial conditions. This is achieved without adversarial objectives, explicit trajectory tracking, phase variables, or reference-dependent inference. We evaluate the method on a challenging box-based parkour playground that demands diverse athletic behaviors (e.g., jumping and climbing), and show that the learned controller transfers beyond the reference distribution while preserving motion naturalness. Finally, we demonstrate long-horizon behavior generation by composing multiple learned skills, illustrating the flexibility of the learned polices in complex scenarios.

</details>
