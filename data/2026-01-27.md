<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 161]
- [cs.CL](#cs.CL) [Total: 111]
- [cs.RO](#cs.RO) [Total: 32]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility](https://arxiv.org/abs/2601.17027)
*Honglin Lin,Chonghan Qin,Zheng Liu,Qizhi Pei,Yu Li,Zhanping Zhong,Xin Gao,Yanfeng Wang,Conghui He,Lijun Wu*

Main category: cs.CV

TL;DR: 本文系统性研究了科学图像合成，包括生成范式、评估指标和下游应用，并提出了一种逻辑驱动的图像生成框架ImgCoder及新的科学图像评测基准SciGenBench。


<details>
  <summary>Details</summary>
Motivation: 在文本领域，合成数据已被证明有助于提升科学推理能力，但多模态推理受限于缺乏科学严谨的合成图像。现有的文本到图像模型（T2I）往往生成表面合理但科学上错误的图像，导致视觉与逻辑的分离，影响模型下游推理能力。

Method: 作者研究了两类科学图像生成方式：基于像素的直接合成和程序化合成，并提出逻辑驱动的ImgCoder框架，采用“理解-计划-编码”的显式流程以提升结构精度。同时，作者设计了新的评价方法SciGenBench，从信息实用性与逻辑有效性方面对生成图像进行严谨评测。

Result: 评测发现，基于像素的模型存在系统性失败模式，并揭示了表现力与精准性的基本权衡。同时，基于严格验证的合成科学图像微调LMM模型，有助于提升多模态推理能力，且呈现与文本领域类似的可扩展趋势。

Conclusion: 高保真度合成科学图像可以作为提升大规模多模态推理能力的有效途径，ImgCoder和SciGenBench为该领域研究提供了新工具与标准。

Abstract: While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit "understand - plan - code" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.

</details>


### [2] [Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection](https://arxiv.org/abs/2601.17031)
*Yunhao Xu,Fuquan Zong,Yexuan Xing,Chulong Zhang,Guang Yang,Shilong Yang,Xiaokun Liang,Juan Yu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的双重增强框架，通过扩展空间流形和语义对象注入，帮助医学图像分割模型更高效地利用有限的高质量标注数据，在髓膜瘤等复杂病理分割任务上取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 医学图像高质量标注成本高且数据有限，复杂病理（如髓膜瘤）分割对数据利用效率要求高，需充分挖掘有限数据潜力。现有方法对结构多样性和真实性增强有限，影响模型泛化与鲁棒性。

Method: 提出双重增强框架：(1) 利用隐式神经表示(INR)建模连续速度场，通过线性混合整合变形场，实现解剖结构的广泛多样化生成；(2) 设计Sim2Real病灶注入模块，将病变纹理真实植入健康解剖背景，实现高逼真度的增强，并缩小合成与真实之间的差距。

Result: 在混合数据集上，实验表明该方法可显著提升当前主流模型（如nnU-Net，U-Mamba）在数据利用效率和鲁棒性方面的表现。

Conclusion: 该框架为有限标注预算下的高性能医学图像分析提供了有效策略，尤其适用于复杂病变的精确分割。

Abstract: The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.

</details>


### [3] [Diagnosis Support of Sickle Cell Anemia by Classifying Red Blood Cell Shape in Peripheral Blood Images](https://arxiv.org/abs/2601.17032)
*Wilkie Delgado-Font,Miriela Escobedo-Nicot,Manuel González-Hidalgo,Silena Herold-Garcia,Antoni Jaume-i-Capó,Arnau Mir*

Main category: cs.CV

TL;DR: 本文提出了一种利用外周血涂片图像分析的自动化红细胞(RBC)分类方法，用于辅助镰状细胞贫血等疾病的诊断，实验结果显示该方法优于现有技术，准确率高。


<details>
  <summary>Details</summary>
Motivation: 镰状细胞贫血等疾病会导致红细胞变形，目前诊断与监控主要依赖人工显微镜观察，费时且主观性强，误差率高。因此，亟需高效、客观的自动化方法提升诊断效率与准确性。

Method: 该方法利用Chan-Vese活动轮廓模型对血涂片图像中的红细胞进行分割，结合圆形因子(CSF)和椭圆形因子(ESF)等形状分析参数，对红细胞进行正常、拉长或其他变形的分类。对于部分重叠的红细胞，采用椭圆拟合分析以提高识别的准确性。

Result: 实验证明，该方法对镰状细胞贫血的诊断优于当前主流办法，正常红细胞F值为0.97，拉长类为0.95，多分类表现也优异。

Conclusion: 该自动化方法能有效辅助镰状细胞贫血的临床诊断和治疗支持，提高诊断的效率和准确性，具有良好的应用前景。

Abstract: Red blood cell (RBC) deformation is the consequence of several diseases, including sickle cell anemia, which causes recurring episodes of pain and severe pronounced anemia. Monitoring patients with these diseases involves the observation of peripheral blood samples under a microscope, a time-consuming procedure. Moreover, a specialist is required to perform this technique, and owing to the subjective nature of the observation of isolated RBCs, the error rate is high. In this paper, we propose an automated method for differentially enumerating RBCs that uses peripheral blood smear image analysis. In this method, the objects of interest in the image are segmented using a Chan-Vese active contour model. An analysis is then performed to classify the RBCs, also called erythrocytes, as normal or elongated or having other deformations, using the basic shape analysis descriptors: circular shape factor (CSF) and elliptical shape factor (ESF). To analyze cells that become partially occluded in a cluster during sample preparation, an elliptical adjustment is performed to allow the analysis of erythrocytes with discoidal and elongated shapes. The images of patient blood samples used in the study were acquired by a clinical laboratory specialist in the Special Hematology Department of the ``Dr. Juan Bruno Zayas'' General Hospital in Santiago de Cuba. A comparison of the results obtained by the proposed method in our experiments with those obtained by some state-of-the-art methods showed that the proposed method is superior for the diagnosis of sickle cell anemia. This superiority is achieved for evidenced by the obtained F-measure value (0.97 for normal cells and 0.95 for elongated ones) and several overall multiclass performance measures. The results achieved by the proposed method are suitable for the purpose of clinical treatment and diagnostic support of sickle cell anemia.

</details>


### [4] [AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs](https://arxiv.org/abs/2601.17037)
*Aahana Basappa,Pranay Goel,Anusri Karra,Anish Karra,Asa Gilmore,Kevin Zhu*

Main category: cs.CV

TL;DR: 本文提出了AMVICC基准，对比分析多模态大语言模型（MLLMs）和图像生成模型（IGMs）在视觉推理上的失败模式，实现跨模态评价。结果显示，两类模型存在共同以及特有的失败点，尤其IGMs在细粒度视觉属性操控上表现较弱。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型发展迅速，但在基本视觉推理如物体朝向、数量与空间关系等方面仍常出现理解和生成错误，表明这类模型在基础视觉推理能力上存在显著短板。

Method: 作者将MMVP基准问题调整为显式和隐式提示，构建新的AMVICC基准，并以此测试11个MLLMs和3个IGMs在九类视觉推理能力上的表现，系统性比较其失败模式。

Result: 不同模型和模态间存在共性和特有的失败模式，IGMs在对提示中具体视觉细节的操控上表现尤为不足，尤其是在显式提示下反应较差。

Conclusion: AMVICC为后续跨模态一致性研究提供了基础，有助于发现生成和理解中失败的共性根源，为未来统一视觉-语言模型的改进指明方向。

Abstract: We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.

</details>


### [5] [Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification](https://arxiv.org/abs/2601.17038)
*Obai Alashram,Nejad Alagha,Mahmoud AlKakuri,Zeeshan Swaveel,Abigail Copiaco*

Main category: cs.CV

TL;DR: 该论文提出了一种混合视觉管道，结合深度特征提取与经典机器学习分类器，实现施工与拆除垃圾的自动化分类，取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 建筑行业产生大量垃圾，如何高效、可持续地分类管理这些废弃物对于资源再利用和垃圾处理至关重要。

Method: 作者收集了1800张来自真实工地、涵盖4类材料的高质量图片，并采用Xception网络进行深度特征提取，再结合SVM、kNN、Bagged Trees等多种经典机器学习分类器系统性评估分类表现。

Result: 结合Xception特征与简单双机学习分类器（如Linear SVM、kNN、Bagged Trees）可得到高达99.5%的准确率和macro-F1分数，超越复杂的深度学习端到端方案。

Conclusion: 该方案在实现现场垃圾识别的鲁棒性与实用性方面具有明显优势，并为未来与机器人及现场自动化系统集成提供了可行的路径。

Abstract: The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\&D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems.

</details>


### [6] [MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation](https://arxiv.org/abs/2601.17039)
*Junhyuk Heo,Beomkyu Choi,Hyunjin Shin,Darongsae Kwon*

Main category: cs.CV

TL;DR: 本文提出了MANGO，一个涵盖全球124个国家、包含42,703对标注图像与掩膜对的大规模红树林遥感数据集，并为分割算法提供基准。


<details>
  <summary>Details</summary>
Motivation: 现有红树林遥感数据集存在局限：1）多为年尺度产品，缺乏高质量单日影像-掩膜配对；2）地域覆盖有限，缺乏全球性数据；3）难以公开获取。因此，急需大规模、高质量、全球分布的数据集推动自动化红树林监测研究。

Method: 作者利用2020年Sentinel-2的全部可用影像，对全球红树林分布区提取最佳单时相影像，并结合年度红树林掩膜。采用目标检测驱动的方法，基于像素级坐标确保影像与掩膜精准匹配，最终获得高质量图像-掩膜对。数据集在国家无重叠条件下评测了多种分割架构。

Result: MANGO数据集标注了42,703对来自全球124国的红树林遥感影像，并通过国家级独立划分，给主流语义分割架构提供了全面的性能基线。

Conclusion: MANGO开创了全球大规模、公开可用的红树林遥感数据集，显著提升了全球红树林智能监测的基础，为相关深度学习研究提供了新基准和研究平台。

Abstract: Mangroves are critical for climate-change mitigation, requiring reliable monitoring for effective conservation. While deep learning has emerged as a powerful tool for mangrove detection, its progress is hindered by the limitations of existing datasets. In particular, many resources provide only annual map products without curated single-date image-mask pairs, limited to specific regions rather than global coverage, or remain inaccessible to the public. To address these challenges, we introduce MANGO, a large-scale global dataset comprising 42,703 labeled image-mask pairs across 124 countries. To construct this dataset, we retrieve all available Sentinel-2 imagery within the year 2020 for mangrove regions and select the best single-date observations that align with the mangrove annual mask. This selection is performed using a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings. We also provide a benchmark across diverse semantic segmentation architectures under a country-disjoint split, establishing a foundation for scalable and reliable global mangrove monitoring.

</details>


### [7] [FP-THD: Full page transcription of historical documents](https://arxiv.org/abs/2601.17040)
*H Neji,J Nogueras-Iso,J Lacasta,MÁ Latre,FJ García-Marco*

Main category: cs.CV

TL;DR: 提出了一种保留特殊符号和字符的拉丁文历史文献转录流程，通过布局分析与OCR结合，实现对手写、印刷及多语言文献的高效数字化。


<details>
  <summary>Details</summary>
Motivation: 15、16世纪的拉丁文历史文献包含许多特殊字符和符号，这些对文本意义至关重要，传统转录方法无法很好地保留这些特征，影响文本的历史和学术价值。

Method: 将布局分析与现有文本行识别（OCR）方法结合，先用布局分析模型提取文本行，再用OCR模型转录，每一步都保留必要的文献特征，适用于不同类型文献（手写、印刷、多语言）。

Result: 在多个数据集上进行评估，结果表明该流程能够高效处理具有不同特性的历史文献页面，尤其是使用了masked autoencoder取得了良好效果。

Conclusion: 该转录流程有助于高效且准确地数字化保存拉丁文历史文献，能够显著提升文献转录质量，保留历史文献原有的风格和意义。

Abstract: The transcription of historical documents written in Latin in XV and XVI centuries has special challenges as it must maintain the characters and special symbols that have distinct meanings to ensure that historical texts retain their original style and significance. This work proposes a pipeline for the transcription of historical documents preserving these special features. We propose to extend an existing text line recognition method with a layout analysis model. We analyze historical text images using a layout analysis model to extract text lines, which are then processed by an OCR model to generate a fully digitized page. We showed that our pipeline facilitates the processing of the page and produces an efficient result. We evaluated our approach on multiple datasets and demonstrate that the masked autoencoder effectively processes different types of text, including handwritten, printed and multi-language.

</details>


### [8] [Arabic Sign Language Recognition using Multimodal Approach](https://arxiv.org/abs/2601.17041)
*Ghadeer Alanazi,Abir Benabid*

Main category: cs.CV

TL;DR: 本文探讨通过融合Leap Motion和RGB摄像头两种传感器数据，提升阿拉伯手语识别准确率的方法。提出的多模态系统对18个词中的13个实现了有效识别，整体准确率为78%。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯手语识别技术主要依赖单一传感器，难以应对复杂手部姿态和三维动作，准确率受限。多模态融合有望弥补各自传感器的不足，提高识别效果。

Method: 提出包含两个子网络的系统结构：一个针对Leap Motion数据的定制稠密神经网络（含dropout与L2正则化），一个基于VGG16的图像子网络（带数据增强）。两者的特征在融合模型中拼接，最后通过全连接层和SoftMax作最终分类。系统在自建的18个ArSL词汇数据集上测试。

Result: 系统对18个词汇中的13个做出正确识别，总体准确率为78%。

Conclusion: 多模态手语识别系统对提升阿拉伯手语识别有初步效果，但还有进一步优化和数据集扩展的空间。

Abstract: Arabic Sign Language (ArSL) is an essential communication method for individuals in the Deaf and Hard-of-Hearing community. However, existing recognition systems face significant challenges due to their reliance on single sensor approaches like Leap Motion or RGB cameras. These systems struggle with limitations such as inadequate tracking of complex hand orientations and imprecise recognition of 3D hand movements. This research paper aims to investigate the potential of a multimodal approach that combines Leap Motion and RGB camera data to explore the feasibility of recognition of ArSL. The system architecture includes two parallel subnetworks: a custom dense neural network for Leap Motion data, incorporating dropout and L2 regularization, and an image subnetwork based on a fine-tuned VGG16 model enhanced with data augmentation techniques. Feature representations from both modalities are concatenated in a fusion model and passed through fully connected layers, with final classification performed via SoftMax activation to analyze spatial and temporal features of hand gestures. The system was evaluated on a custom dataset comprising 18 ArSL words, of which 13 were correctly recognized, yielding an overall accuracy of 78%. These results offer preliminary insights into the viability of multimodal fusion for sign language recognition and highlight areas for further optimization and dataset expansion.

</details>


### [9] [Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective](https://arxiv.org/abs/2601.17042)
*Tianyuan Liu,Libin Hou,Linyuan Wang,Bin Yan*

Main category: cs.CV

TL;DR: 本文提出了一种基于最大编码率减少（MCR2）的可解释性与高效性统一的白盒Transformer方法。通过解耦MCR2中“成员矩阵”与“子空间矩阵U”的关系，并推导出稀疏线性注意力算子，实现算法高效且易于解释。实验验证了方法在视觉任务上的有效性与优越性。


<details>
  <summary>Details</summary>
Motivation: 现有MCR2-T方法中“成员矩阵”与“子空间矩阵U”耦合紧密，导致token投影不准确时会产生冗余编码，影响模型效率与解释性。因此需要一种能够解耦这二者关系、提升模型解释性与高效性的机制。

Method: 提出直接从输入学习成员矩阵，再从全集S中派生稀疏子空间，并通过优化过的MCR2目标的梯度展开，推导出可解释的稀疏线性注意力算子——解耦成员-子空间注意力（DMSA）。加入Token Statistics Transformer形成DMST架构。

Result: 实验证明，在ImageNet-1K数据集上，将Token Statistics Transformer中的注意力模块替换为DMSA后（即DMST），编码率下降更快，并且top-1准确率提高1.08%-1.45%。相较于原生Transformer，DMST有更高的计算效率和可解释性。

Conclusion: 解耦MCR2目标中的成员矩阵与子空间矩阵，能够设计出既高效又易解释的稀疏线性注意力机制，为视觉模型带来性能和可解释性的双提升。

Abstract: Maximal Coding Rate Reduction (MCR2)-driven white-box transformer, grounded in structured representation learning, unifies interpretability and efficiency, providing a reliable white-box solution for visual modeling. However, in existing designs, tight coupling between "membership matrix" and "subspace matrix U" in MCR2 causes redundant coding under incorrect token projection. To this end, we decouple the functional relationship between the "membership matrix" and "subspaces U" in the MCR2 objective and derive an interpretable sparse linear attention operator from unrolled gradient descent of the optimized objective. Specifically, we propose to directly learn the membership matrix from inputs and subsequently derive sparse subspaces from the fullspace S. Consequently, gradient unrolling of the optimized MCR2 objective yields an interpretable sparse linear attention operator: Decoupled Membership-Subspace Attention (DMSA). Experimental results on visual tasks show that simply replacing the attention module in Token Statistics Transformer (ToST) with DMSA (we refer to as DMST) not only achieves a faster coding reduction rate but also outperforms ToST by 1.08%-1.45% in top-1 accuracy on the ImageNet-1K dataset. Compared with vanilla Transformer architectures, DMST exhibits significantly higher computational efficiency and interpretability.

</details>


### [10] [Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning](https://arxiv.org/abs/2601.17046)
*Matan Leibovich,Mai Tan,Adria Marcos-Morales,Sreyas Mohan,Peter A. Crozier,Carlos Fernandez-Granda*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，利用深度卷积神经网络，通过将深度估计转化为语义分割问题，从几何受噪声影响的透射电子显微镜（TEM）图像中提取3D原子级信息，在CeO2纳米颗粒的模拟和实际数据中取得了准确且鲁棒的结果。


<details>
  <summary>Details</summary>
Motivation: 在高噪声环境下，从TEM图像中提取三维原子级信息一直是难题，传统方法对噪声敏感且不够精确，因此需要开发更鲁棒且高效的方法。

Method: 作者将深度估计问题建模为语义分割，通过深度卷积神经网络对加入合成噪声的模拟数据进行训练，以生成逐像素的深度分割图。方法随后在CeO2纳米颗粒的模拟及实际TEM数据上进行测试。

Result: 该方法在多组实验中表现出色，深度估计结果不仅准确，还具有良好的校准性和对噪声的鲁棒性。

Conclusion: 将深度估计转化为语义分割并结合深度神经网络，有效提升了TEM图像3D原子级信息提取的精度和鲁棒性，为高噪声数据下的三维分析提供了新途径。

Abstract: We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise. The approach is based on formulating depth estimation as a semantic segmentation problem. We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise. The proposed method was applied to estimate the depth of atomic columns in CeO2 nanoparticles from simulated images and real-world TEM data. Our experiments show that the resulting depth estimates are accurate, calibrated and robust to noise.

</details>


### [11] [A Contrastive Pre-trained Foundation Model for Deciphering Imaging Noisomics across Modalities](https://arxiv.org/abs/2601.17047)
*Yuanjie Gu,Yiqun Wang,Chaohui Yu,Ang Xuan,Fan Wang,Zhi Lu,Biqin Dong*

Main category: cs.CV

TL;DR: 本论文提出了一种名为“Noisomics”的新框架，通过Contrastive Pre-trained (CoP)基础模型，将成像噪声从“干扰”转变为可解码的信息资源。该方法用极少数据实现了超越传统大规模有监督基线的表现，显著提升了成像噪声解码能力。


<details>
  <summary>Details</summary>
Motivation: 现有成像噪声建模严重依赖大量数据和设备，噪声常被视为干扰而非有用信息，难以利用其深层潜力。作者希望提出一个数据高效、可泛化的解码方案，将噪声作为信息资源，更好地辅助成像诊断。

Method: 提出Noisomics框架和CoP基础模型，利用合成噪声基因和对比学习，基于流形假说实现语义信号与随机扰动的解耦。该方法在训练数据极少的情况下进行训练，并在多个领域外数据集上验证。

Result: CoP只需100个训练样本即可超越需10万样本的有监督基线。跨12个领域外数据集获得强鲁棒性，噪声估计误差降低63.8%，决定系数提升85.1%。

Conclusion: CoP可高效解码不同尺度上的噪声信息，实现设备无关的成像诊断，为噪声建模和成像分析提供了新范式，减少了对大数据和设备标定的依赖。

Abstract: Characterizing imaging noise is notoriously data-intensive and device-dependent, as modern sensors entangle physical signals with complex algorithmic artifacts. Current paradigms struggle to disentangle these factors without massive supervised datasets, often reducing noise to mere interference rather than an information resource. Here, we introduce "Noisomics", a framework shifting the focus from suppression to systematic noise decoding via the Contrastive Pre-trained (CoP) Foundation Model. By leveraging the manifold hypothesis and synthetic noise genome, CoP employs contrastive learning to disentangle semantic signals from stochastic perturbations. Crucially, CoP breaks traditional deep learning scaling laws, achieving superior performance with only 100 training samples, outperforming supervised baselines trained on 100,000 samples, thereby reducing data and computational dependency by three orders of magnitude. Extensive benchmarking across 12 diverse out-of-domain datasets confirms its robust zero-shot generalization, demonstrating a 63.8% reduction in estimation error and an 85.1% improvement in the coefficient of determination compared to the conventional training strategy. We demonstrate CoP's utility across scales: from deciphering non-linear hardware-noise interplay in consumer photography to optimizing photon-efficient protocols for deep-tissue microscopy. By decoding noise as a multi-parametric footprint, our work redefines stochastic degradation as a vital information resource, empowering precise imaging diagnostics without prior device calibration.

</details>


### [12] [SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis](https://arxiv.org/abs/2601.17048)
*Jing Jie Tan,Rupert Schreiner,Matthias Hausladen,Ali Asgharzade,Simon Edler,Julian Bartsch,Michael Bachmann,Andreas Schels,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum*

Main category: cs.CV

TL;DR: 该论文提出SiMiC方法，利用注意力机制的卷积神经网络自动分析硅微结构SEM图像，实现多类别结构的分类与尺寸预测，提升准确性和一致性，减少人工操作。


<details>
  <summary>Details</summary>
Motivation: 硅微结构的精确定量对于微尺度制造和器件性能至关重要，传统的SEM图像人工分析方法效率低且主观性强，亟需自动化、可重复的解决方案。

Method: 作者构建了硅场发射尖端的专用数据集，基于深度学习设计了一种结合注意力机制的自定义CNN模型，可从SEM图像中提取形态特征并进行多类别分类及尺寸预测。

Result: 与传统图像处理方法对比，SiMiC能够显著提升分类与测量精度，并保持模型可解释性。

Conclusion: SiMiC为硅微结构数据驱动的自动化分析提供了基础，促进结构与发射性能的关联研究，并为冷阴极与SEM电子源设计优化提供新思路。

Abstract: Accurate characterization of silicon microstructures is essential for advancing microscale fabrication, quality control, and device performance. Traditional analysis using Scanning Electron Microscopy (SEM) often requires labor-intensive, manual evaluation of feature geometry, limiting throughput and reproducibility. In this study, we propose SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis. By leveraging deep learning, our approach efficiently extracts morphological features-such as size, shape, and apex curvature-from SEM images, significantly reducing human intervention while improving measurement consistency. A specialized dataset of silicon-based field-emitter tips was developed, and a customized CNN architecture incorporating attention mechanisms was trained for multi-class microstructure classification and dimensional prediction. Comparative analysis with classical image processing techniques demonstrates that SiMiC achieves high accuracy while maintaining interpretability. The proposed framework establishes a foundation for data-driven microstructure analysis directly linked to field-emission performance, opening avenues for correlating emitter geometry with emission behavior and guiding the design of optimized cold-cathode and SEM electron sources. The related dataset and algorithm repository that could serve as a baseline in this area can be found at https://research.jingjietan.com/?q=SIMIC

</details>


### [13] [Summary of the Unusual Activity Recognition Challenge for Developmental Disability Support](https://arxiv.org/abs/2601.17049)
*Christina Garcia,Nhat Tan Le,Taihei Fujioka,Umang Dobhal,Milyun Ni'ma Shoumi,Thanh Nha Nguyen,Sozo Inoue*

Main category: cs.CV

TL;DR: 本论文介绍了ISAS 2025举办的“识别未知行为：基于姿态数据的异常行为识别挑战赛”，聚焦于通过非侵入式姿态估计数据自动检测特殊人群中的异常行为。


<details>
  <summary>Details</summary>
Motivation: 在发展障碍人士的照护场所，自动识别异常行为对于保障安全和提升管理效率具有重要意义。然而，如何基于姿态数据，在保护隐私前提下，准确检测不常见的、突发的异常行为仍然极具挑战。

Method: 赛事提供了从模拟场景视频中提取的骨架关键点数据，要求参赛队伍根据这些数据区分正常与异常行为，并采用LOSO（留一受试者法）实现泛化评估。团队使用了从传统机器学习到深度学习的多样方法，并采用macro-F1作为主要评估指标以应对类别失衡。

Result: 挑战赛显示，面对噪声大、维度低的数据，尤其罕见且突发行为的建模非常困难。参赛方法在效果上有明显差距，整体性能受限于数据本身的复杂性和不平衡性。

Conclusion: 本次挑战强调在行为建模中捕捉时间和上下文信息的重要性，为未来在医疗健康和行为监测等社会责任型AI应用提供了有益的经验和洞见。

Abstract: This paper presents an overview of the Recognize the Unseen: Unusual Behavior Recognition from Pose Data Challenge, hosted at ISAS 2025. The challenge aims to address the critical need for automated recognition of unusual behaviors in facilities for individuals with developmental disabilities using non-invasive pose estimation data. Participating teams were tasked with distinguishing between normal and unusual activities based on skeleton keypoints extracted from video recordings of simulated scenarios. The dataset reflects real-world imbalance and temporal irregularities in behavior, and the evaluation adopted a Leave-One-Subject-Out (LOSO) strategy to ensure subject-agnostic generalization. The challenge attracted broad participation from 40 teams applying diverse approaches ranging from classical machine learning to deep learning architectures. Submissions were assessed primarily using macro-averaged F1 scores to account for class imbalance. The results highlight the difficulty of modeling rare, abrupt actions in noisy, low-dimensional data, and emphasize the importance of capturing both temporal and contextual nuances in behavior modeling. Insights from this challenge may contribute to future developments in socially responsible AI applications for healthcare and behavior monitoring.

</details>


### [14] [Single-Pixel Vision-Language Model for Intrinsic Privacy-Preserving Behavioral Intelligence](https://arxiv.org/abs/2601.17050)
*Hongjun An,Yiliang Song,Jiawei Shao,Zhe Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 该论文提出SP-VLM（单像素视觉-语言模型）框架，用于隐私敏感环境下的环境安全监测，在保护个人身份隐私的同时，实现对异常行为的检测和理解。


<details>
  <summary>Details</summary>
Motivation: 传统视频监控在如更衣室、卫生间等隐私场所受到法律和道德限制，无法应对霸凌、骚扰等危害个体和公共安全的行为。需求迫切，但保护隐私与安全监控之间存在矛盾。

Method: 提出利用低维单像素感知方式，通过视觉-语言模型推断并理解复杂行为模式。在采样率临界点以下，失真严重到无法进行人脸识别，但仍可提取行为语义，实现异常检测、人数统计和活动识别。

Result: 实验证明，在单像素感知下，人脸识别彻底失效，实现了本征隐私保护；同时，SP-VLM能够维持高水平的行为分析和异常检测能力。优化后，找到身份可保护且可行的采样率区间。

Conclusion: SP-VLM可作为隐私敏感环境下的安全行为监测方案，实现了隐私保护与安全需求平衡，为合乎人权的环境监测提供了新方法。

Abstract: Adverse social interactions, such as bullying, harassment, and other illicit activities, pose significant threats to individual well-being and public safety, leaving profound impacts on physical and mental health. However, these critical events frequently occur in privacy-sensitive environments like restrooms, and changing rooms, where conventional surveillance is prohibited or severely restricted by stringent privacy regulations and ethical concerns. Here, we propose the Single-Pixel Vision-Language Model (SP-VLM), a novel framework that reimagines secure environmental monitoring. It achieves intrinsic privacy-by-design by capturing human dynamics through inherently low-dimensional single-pixel modalities and inferring complex behavioral patterns via seamless vision-language integration. Building on this framework, we demonstrate that single-pixel sensing intrinsically suppresses identity recoverability, rendering state-of-the-art face recognition systems ineffective below a critical sampling rate. We further show that SP-VLM can nonetheless extract meaningful behavioral semantics, enabling robust anomaly detection, people counting, and activity understanding from severely degraded single-pixel observations. Combining these findings, we identify a practical sampling-rate regime in which behavioral intelligence emerges while personal identity remains strongly protected. Together, these results point to a human-rights-aligned pathway for safety monitoring that can support timely intervention without normalizing intrusive surveillance in privacy-sensitive spaces.

</details>


### [15] [Synthetic Data Guided Feature Selection for Robust Activity Recognition in Older Adults](https://arxiv.org/abs/2601.17053)
*Shuhao Que,Dieuwke van Dartel,Ilse Heeringa,Han Hegeman,Miriam Vollenbroek-Hutten,Ying Wang*

Main category: cs.CV

TL;DR: 本研究开发了一种人类活动识别（HAR）系统，能更准确地识别老年髋部骨折康复患者的活动类型，并显著提升了关键活动（如体位转移）的检测能力。


<details>
  <summary>Details</summary>
Motivation: 目前临床对老年髋部骨折康复期的身体活动量缺乏量化，现有商用可穿戴设备主要针对中青年，难以应对老年人步态缓慢和多变的特点，导致监测准确性低。

Method: 研究选取了24名80岁以上健康老年人，在佩戴腰部和大腿处加速度计的情况下，模拟日常生活环境，完成多种活动，并采用leave-one-subject-out交叉验证来评估系统鲁棒性。引入了合成数据以提升模型泛化能力，并提出特征干预模型（FIM）。

Result: FIM系统在多项活动识别上表现优异，尤其在步行、站立、坐下、躺下及体位转移五类活动中，F1得分分别为0.896、0.927、0.997、0.937和0.816。特别是在体位转移检测上，FIM较无合成数据的模型有显著提升。

Conclusion: 该方法初步证实了在老年人中实现高鲁棒性活动识别的可行性，但还需在实际髋部骨折患者中进一步验证其临床实用性。

Abstract: Physical activity during hip fracture rehabilitation is essential for mitigating long-term functional decline in geriatric patients. However, it is rarely quantified in clinical practice. Existing continuous monitoring systems with commercially available wearable activity trackers are typically developed in middle-aged adults and therefore perform unreliably in older adults with slower and more variable gait patterns. This study aimed to develop a robust human activity recognition (HAR) system to improve continuous physical activity recognition in the context of hip fracture rehabilitation. 24 healthy older adults aged over 80 years were included to perform activities of daily living (walking, standing, sitting, lying down, and postural transfers) under simulated free-living conditions for 75 minutes while wearing two accelerometers positioned on the lower back and anterior upper thigh. Model robustness was evaluated using leave-one-subject-out cross-validation. The synthetic data demonstrated potential to improve generalization across participants. The resulting feature intervention model (FIM), aided by synthetic data guidance, achieved reliable activity recognition with mean F1-scores of 0.896 for walking, 0.927 for standing, 0.997 for sitting, 0.937 for lying down, and 0.816 for postural transfers. Compared with a control condition model without synthetic data, the FIM significantly improved the postural transfer detection, i.e., an activity class of high clinical relevance that is often overlooked in existing HAR literature. In conclusion, these preliminary results demonstrate the feasibility of robust activity recognition in older adults. Further validation in hip fracture patient populations is required to assess the clinical utility of the proposed monitoring system.

</details>


### [16] [Ego4OOD: Rethinking Egocentric Video Domain Generalization via Covariate Shift Scoring](https://arxiv.org/abs/2601.17056)
*Zahra Vaseqi,James Clark*

Main category: cs.CV

TL;DR: 本论文提出Ego4OOD数据集作为头戴视角视频领域泛化的基准，聚焦于度量可控的共变量变化，并减少概念漂移，以便更可靠地评估模型的跨域泛化能力。此外，提出了一种基于聚类的共变量变化指标及独立二分类训练方法，以提升对共变量变化的鲁棒性。实验证明该方法在较低参数量下达到与主流方法相当的表现。


<details>
  <summary>Details</summary>
Motivation: 头戴视角视频的动作识别在域泛化任务中面临巨大挑战，主要原因包括同一类别内空间-时间变化大、特征分布极度不均衡以及动作与环境间强相关性。现有基准混合了共变量变化与概念漂移，导致难以准确评估算法的泛化能力。

Method: 1）构建Ego4OOD基准数据集，涵盖8个地理域，通过精细分类以减小概念漂移；2）设计基于聚类的共变量变化量化指标，用于衡量域间难度；3）提出一对多二分类目标，将多类别识别转为一系列独立二分类任务，以降低类别间的干扰。

Result: 实验证明，采用所提出的二分类训练方法和轻量两层全连接网络，在Argo1M和Ego4OOD数据集上，性能可与现有最先进方法媲美，且参数量更少、无需多模态信息。

Conclusion: 受控化的基准和量化的域难度指标对头戴视角视频的跨域泛化研究至关重要。提出的Ego4OOD和训练策略能更准确认识域间共变量漂移对模型性能的影响，促进OOD（分布外）泛化相关研究的发展。

Abstract: Egocentric video action recognition under domain shifts remains challenging due to large intra-class spatio-temporal variability, long-tailed feature distributions, and strong correlations between actions and environments. Existing benchmarks for egocentric domain generalization often conflate covariate shifts with concept shifts, making it difficult to reliably evaluate a model's ability to generalize across input distributions. To address this limitation, we introduce Ego4OOD, a domain generalization benchmark derived from Ego4D that emphasizes measurable covariate diversity while reducing concept shift through semantically coherent, moment-level action categories. Ego4OOD spans eight geographically distinct domains and is accompanied by a clustering-based covariate shift metric that provides a quantitative proxy for domain difficulty. We further leverage a one-vs-all binary training objective that decomposes multi-class action recognition into independent binary classification tasks. This formulation is particularly well-suited for covariate shift by reducing interference between visually similar classes under feature distribution shift. Using this formulation, we show that a lightweight two-layer fully connected network achieves performance competitive with state-of-the-art egocentric domain generalization methods on both Argo1M and Ego4OOD, despite using fewer parameters and no additional modalities. Our empirical analysis demonstrates a clear relationship between measured covariate shift and recognition performance, highlighting the importance of controlled benchmarks and quantitative domain characterization for studying out-of-distribution generalization in egocentric video.

</details>


### [17] [A Computer Vision Pipeline for Iterative Bullet Hole Tracking in Rifle Zeroing](https://arxiv.org/abs/2601.17062)
*Robert M. Belcher,Brendan C. Degryse,Leonard R. Kosta,Christopher J. Lowrance*

Main category: cs.CV

TL;DR: 本文提出了一套端到端的计算机视觉系统，实现射击“归零”过程中的自动弹孔检测和迭代追踪，极大提升检测效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 传统射击归零需要人工识别不同迭代的弹孔，过程慢且易错，希望通过自动化手段提升效率并降低人工失误。

Method: 系统采用YOLOv8进行小物体检测，利用IoU分析区分不同迭代产生的弹孔，并通过删减目标的新型数据增强方法模拟真实射击序列。同时，借助ORB算法的视角校正标准化目标图像，提高检测准确率。

Result: 弹孔检测平均精度达到97.0%，弹孔归属迭代准确率为88.8%。

Conclusion: 本系统显著提升射击归零过程的自动化和准确性，并有望应用于需区分时间序列相似物体的其他领域。

Abstract: Adjusting rifle sights, a process commonly called "zeroing," requires shooters to identify and differentiate bullet holes from multiple firing iterations. Traditionally, this process demands physical inspection, introducing delays due to range safety protocols and increasing the risk of human error. We present an end-to-end computer vision system for automated bullet hole detection and iteration-based tracking directly from images taken at the firing line. Our approach combines YOLOv8 for accurate small-object detection with Intersection over Union (IoU) analysis to differentiate bullet holes across sequential images. To address the scarcity of labeled sequential data, we propose a novel data augmentation technique that removes rather than adds objects to simulate realistic firing sequences. Additionally, we introduce a preprocessing pipeline that standardizes target orientation using ORB-based perspective correction, improving model accuracy. Our system achieves 97.0% mean average precision on bullet hole detection and 88.8% accuracy in assigning bullet holes to the correct firing iteration. While designed for rifle zeroing, this framework offers broader applicability in domains requiring the temporal differentiation of visually similar objects.

</details>


### [18] [A Mechanistic View on Video Generation as World Models: State and Dynamics](https://arxiv.org/abs/2601.17067)
*Luozhou Wang,Zhifei Chen,Yihua Du,Dongyu Yan,Wenhang Ge,Guibao Shen,Xinli Xu,Leyi Wu,Man Chen,Tianshuo Xu,Peiran Ren,Xin Tao,Pengfei Wan,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出了一个关于视频生成模型的全新分类法，并指出该领域未来的重要发展方向。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视频生成模型虽然表现出物理一致性，被认为是潜在的世界模型，但存在“无状态”视频架构与传统“有状态”世界模型理论之间的差距。

Method: 作者提出以状态构建与动力学建模为核心的分类方法，将状态构建分为隐式范式（上下文管理）与显式范式（潜变量压缩），并对动力学建模从知识整合和架构重塑角度分析。作者还主张评估标准应从视觉逼真度转向物理持续性与因果推理等功能性基准。

Result: 提出并分析了功能性评估指标，且明确了持久性（通过数据驱动记忆和压缩保真度）和因果性（通过潜因子解耦和推理先验集成）是今后研究的关键前沿问题。

Conclusion: 本文为视频生成模型的进一步发展指明方向，即由追求视觉逼真转向构建强健、通用的世界模拟器。

Abstract: Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary "stateless" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.

</details>


### [19] [Superpixel-Based Image Segmentation Using Squared 2-Wasserstein Distances](https://arxiv.org/abs/2601.17071)
*Jisui Huang,Andreas Alpers,Ke Chen,Na Lei*

Main category: cs.CV

TL;DR: 本文提出了一种高效的图像分割方法，通过两层聚类框架，在存在明显非均匀性的情况下依然表现优秀。核心是将像素先分组为超像素，然后用优化的分布间距离进行对象级合并，实现精准且高效的分割。


<details>
  <summary>Details</summary>
Motivation: 传统图像分割方法在处理图像非均匀性和复杂分布时效果有限，且常用的超像素合并方法依赖于简单的颜色均值距离，难以刻画分布特征，精度受限。作者希望通过引入更强表征力的分布距离克服这些不足，提高分析能力和效率。

Method: 方法分为两步：第一步，将像素通过线性最小二乘分配问题分组成超像素，这一过程可看作离散最优传输（OT）问题的特例；第二步，利用超像素的经验分布间的二阶Wasserstein距离，在全局范围内贪婪合并超像素。不同于只考虑颜色均值，本文采用分布性OT距离，形成统一的数学框架。

Result: 实验表明，本方法在结构复杂或存在强非均匀性的图像上分割精度显著提升，同时能保持高计算效率。

Conclusion: 所提出的方法有效弥合了分布性与高效性的矛盾，证明了分布OT距离在图像分割聚类中的价值和实用性。

Abstract: We present an efficient method for image segmentation in the presence of strong inhomogeneities. The approach can be interpreted as a two-level clustering procedure: pixels are first grouped into superpixels via a linear least-squares assignment problem, which can be viewed as a special case of a discrete optimal transport (OT) problem, and these superpixels are subsequently greedily merged into object-level segments using the squared 2-Wasserstein distance between their empirical distributions. In contrast to conventional superpixel merging strategies based on mean-color distances, our framework employs a distributional OT distance, yielding a mathematically unified formulation across both clustering levels. Numerical experiments demonstrate that this perspective leads to improved segmentation accuracy on challenging images while retaining high computational efficiency.

</details>


### [20] [GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars](https://arxiv.org/abs/2601.17088)
*Rui-Yang Ju,Jen-Shiun Chiang*

Main category: cs.CV

TL;DR: 该论文提出了GlassesGB，一种能在3D头像上实现个性化眼镜定制的新框架，填补了现有方法仅能在2D或模板基础上操作的不足。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试戴系统仅能在预定义模板基础上修改，缺乏精细、用户驱动的3D个性化定制能力。作者希望实现更贴合用户需求的3D眼镜定制，为VR虚拟化身提供更多个性可能。

Method: 论文结合了GlassGAN的2D个性化眼镜生成能力与3D Gaussian Blendshapes的头部重建方法，提出了GlassesGB。该框架能够将2D生成结果与3D头模无缝结合，实现可定制的3D眼镜渲染。

Result: GlassesGB可以在3D虚拟头像上实现高质量、个性化的眼镜建模，并兼容VR应用场景，效果优于只支持模板修改或仅2D展示的现有技术。相关代码已开源。

Conclusion: GlassesGB框架成功将2D生成与3D重建融合，首次实现了可定制的3D虚拟试戴，极大拓展了虚拟现实应用中个性化装备的可能性。

Abstract: Virtual try-on systems allow users to interactively try different products within VR scenarios. However, most existing VTON methods operate only on predefined eyewear templates and lack support for fine-grained, user-driven customization. While GlassesGAN enables personalized 2D eyewear design, its capability remains limited to 2D image generation. Motivated by the success of 3D Gaussian Blendshapes in head reconstruction, we integrate these two techniques and propose GlassesGB, a framework that supports customizable eyewear generation for 3D head avatars. GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing the challenge in achieving personalized eyewear design for VR applications. The implementation code is available at https://ruiyangju.github.io/GlassesGB.

</details>


### [21] [GRASP: Guided Region-Aware Sparse Prompting for Adapting MLLMs to Remote Sensing](https://arxiv.org/abs/2601.17089)
*Qigan Sun,Chaoning Zhang,Jianwei Zhang,Xudong Wang,Jiehui Xie,Pengcheng Zheng,Haoyu Wang,Sungyoung Lee,Chi-lok Andy Tai,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: 提出了一种名为GRASP的高效参数微调方法，以提升多模态大语言模型在遥感图像视觉问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在遥感图像视觉问答任务中，容易过拟合背景噪声或忽略目标细节，原因在于遥感图像本身存在尺度变化大、目标稀疏和区域语义复杂等特点，导致现有微调方法效果受限。

Method: 提出GRASP（Guided Region-Aware Sparse Prompting）方法。GRASP通过为空间结构块引入软提示（soft prompts），结合冻结的视觉token网格，并采用基于问题引导的稀疏融合机制，将与任务相关的上下文动态聚合成紧凑的全局提示，引导模型关注关键区域，减少背景干扰。

Result: 在多个遥感视觉问答基准数据集上进行了大量实验，GRASP在保持高参数效率的基础上，性能优于或可比现有微调和基于提示的方法。

Conclusion: GRASP方法有效提升了多模态大语言模型在遥感视觉问答任务中的鲁棒性与表现，是针对遥感任务高效和精确微调的新思路。

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have made significant progress in visual question answering tasks. However, directly applying existing fine-tuning methods to remote sensing (RS) images often leads to issues such as overfitting on background noise or neglecting target details. This is primarily due to the large-scale variations, sparse target distributions, and complex regional semantic features inherent in RS images. These challenges limit the effectiveness of MLLMs in RS tasks. To address these challenges, we propose a parameter-efficient fine-tuning (PEFT) strategy called Guided Region-Aware Sparse Prompting (GRASP). GRASP introduces spatially structured soft prompts associated with spatial blocks extracted from a frozen visual token grid. Through a question-guided sparse fusion mechanism, GRASP dynamically aggregates task-specific context into a compact global prompt, enabling the model to focus on relevant regions while filtering out background noise. Extensive experiments on multiple RSVQA benchmarks show that GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.

</details>


### [22] [LoD Sketch Extraction from Architectural Models Using Generative AI: Dataset Construction for Multi-Level Architectural Design Generation](https://arxiv.org/abs/2601.17095)
*Xusheng Du,Athiwat Kongkaeo,Ye Zhang,Haoran Xie*

Main category: cs.CV

TL;DR: 本文提出了一种基于生成式AI的自动化LoD草图提取框架，实现了高细节建筑模型到多级别细节(LoD)一致性模型的自动转换。


<details>
  <summary>Details</summary>
Motivation: 多级别细节(LoD)的表达对建筑设计非常关键，目前主流手工操作方法繁琐易错，且生成式AI受限于高质量成对训练数据的缺乏，限制了其应用。

Method: 结合计算机视觉与生成式AI，提出了一种自动逐级简化高细节建筑模型的框架，能够生成结构一致与语义层次清晰的多级LoD表示，自动提取并转换细节层级。

Result: 实验表明该方法在LoD简化过程中能保持较高的几何一致性，SSIM分别达到了0.7319（LoD3→LoD2)和0.7532（LoD2→LoD1），归一化Hausdorff距离反映了受控的几何偏差。

Conclusion: 该框架可以有效保证结构与语义渐进简化的一致性，为AI驱动的多层级建筑生成和分层建模提供了可靠技术和数据支持。

Abstract: For architectural design, representation across multiple Levels of Details (LoD) is essential for achieving a smooth transition from conceptual massing to detailed modeling. However, traditional LoD modeling processes rely on manual operations that are time-consuming, labor-intensive, and prone to geometric inconsistencies. While the rapid advancement of generative artificial intelligence (AI) has opened new possibilities for generating multi-level architectural models from sketch inputs, its application remains limited by the lack of high-quality paired LoD training data. To address this issue, we propose an automatic LoD sketch extraction framework using generative AI models, which progressively simplifies high-detail architectural models to automatically generate geometrically consistent and hierarchically coherent multi-LoD representations. The proposed framework integrates computer vision techniques with generative AI methods to establish a progressive extraction pipeline that transitions from detailed representations to volumetric abstractions. Experimental results demonstrate that the method maintains strong geometric consistency across LoD levels, achieving SSIM values of 0.7319 and 0.7532 for the transitions from LoD3 to LoD2 and from LoD2 to LoD1, respectively, with corresponding normalized Hausdorff distances of 25.1% and 61.0% of the image diagonal, reflecting controlled geometric deviation during abstraction. These results verify that the proposed framework effectively preserves global structure while achieving progressive semantic simplification across different LoD levels, providing reliable data and technical support for AI-driven multi-level architectural generation and hierarchical modeling.

</details>


### [23] [Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals](https://arxiv.org/abs/2601.17103)
*Pascaline André,Charles Heitz,Evangelia Christodoulou,Annika Reinke,Carole H. Sudre,Michela Antonelli,Patrick Godau,M. Jorge Cardoso,Antoine Gilson,Sophie Tezenas du Montcel,Gaël Varoquaux,Lena Maier-Hein,Olivier Colliot*

Main category: cs.CV

TL;DR: 本文系统评估了医学影像AI性能不确定性报告中置信区间（CI）方法的表现，揭示了不同CI方法在各种实验环境下的可靠性与精度差异，提出五大关键结论，为未来标准制定提供参考。


<details>
  <summary>Details</summary>
Motivation: 目前医学影像AI性能的不确定性评估研究有限，尤其对置信区间方法行为及其适用性缺乏系统性了解，影响了性能结果的可靠验证与临床转化。该研究旨在填补社区对置信区间方法种类、行为及其适应场景认知的空白。

Method: 对24个分割与分类任务、每组19个模型，采用多种常用性能指标、聚合策略及常见CI算法，开展大规模实证分析。系统评估各种情景下各CI方法的可靠性（覆盖率）与精度（宽度），分析其对样本量、任务类型、指标、聚合方式等因素的依赖。

Result: 分析发现：（1）所需可靠CI的样本量视研究参数从几十到数千病例不等；（2）CI表现强依赖性能指标选择；（3）聚合策略显著影响CI可靠性，如宏观聚合需更多样本；（4）分割/分类任务差异影响CI行为；（5）不同CI方法在特定场景下可靠性和精度并不相同。

Conclusion: 为医学影像AI性能不确定性报告方法制定未来标准和指南，提供了重要实证依据，强调应根据具体场景选择合适的置信区间方法并充分报告相关参数。

Abstract: Performance uncertainty quantification is essential for reliable validation and eventual clinical translation of medical imaging artificial intelligence (AI). Confidence intervals (CIs) play a central role in this process by indicating how precise a reported performance estimate is. Yet, due to the limited amount of work examining CI behavior in medical imaging, the community remains largely unaware of how many diverse CI methods exist and how they behave in specific settings. The purpose of this study is to close this gap. To this end, we conducted a large-scale empirical analysis across a total of 24 segmentation and classification tasks, using 19 trained models per task group, a broad spectrum of commonly used performance metrics, multiple aggregation strategies, and several widely adopted CI methods. Reliability (coverage) and precision (width) of each CI method were estimated across all settings to characterize their dependence on study characteristics. Our analysis revealed five principal findings: 1) the sample size required for reliable CIs varies from a few dozens to several thousands of cases depending on study parameters; 2) CI behavior is strongly affected by the choice of performance metric; 3) aggregation strategy substantially influences the reliability of CIs, e.g. they require more observations for macro than for micro; 4) the machine learning problem (segmentation versus classification) modulates these effects; 5) different CI methods are not equally reliable and precise depending on the use case. These results form key components for the development of future guidelines on reporting performance uncertainty in medical imaging AI.

</details>


### [24] [StealthMark: Harmless and Stealthy Ownership Verification for Medical Segmentation via Uncertainty-Guided Backdoors](https://arxiv.org/abs/2601.17107)
*Qinkai Yu,Chong Zhang,Gaojie Jin,Tianjin Huang,Wei Zhou,Wenhui Li,Xiaobo Jin,Bo Huang,Yitian Zhao,Guang Yang,Gregory Y. H. Lip,Yalin Zheng,Aline Villavicencio,Yanda Meng*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的医疗分割模型水印方法StealthMark，通过微妙调控模型不确定性，在不影响分割结果的前提下实现模型所有权的黑盒验证，且实验验证其有效且无损性能。


<details>
  <summary>Details</summary>
Motivation: 训练先进的医疗AI分割模型需要大量标注且敏感的临床数据，且这些模型属于重要知识产权，需要有效的保护与验证机制。目前模型保护方法多针对分类和生成任务，分割模型的所有权保护方法鲜有研究。

Method: 提出StealthMark，通过调节模型不确定性，结合LIME等解释方法，在不改变分割输出情况下，在特定条件下通过特征归因揭示水印。水印设计为QR码，便于识别与验证，适用于多种主流分割模型和数据集。

Result: 在四个医学影像数据集和五种主流分割模型上大量实验，StealthMark能保证所有权验证成功率（ASR）高于95%，分割性能损失小于1%，显著优于基于后门的水印方法。

Conclusion: StealthMark为医疗分割模型提供了有效、隐蔽、无损的所有权验证新方案，兼具实用性和鲁棒性，具备广泛应用潜力。

Abstract: Annotating medical data for training AI models is often costly and limited due to the shortage of specialists with relevant clinical expertise. This challenge is further compounded by privacy and ethical concerns associated with sensitive patient information. As a result, well-trained medical segmentation models on private datasets constitute valuable intellectual property requiring robust protection mechanisms. Existing model protection techniques primarily focus on classification and generative tasks, while segmentation models-crucial to medical image analysis-remain largely underexplored. In this paper, we propose a novel, stealthy, and harmless method, StealthMark, for verifying the ownership of medical segmentation models under black-box conditions. Our approach subtly modulates model uncertainty without altering the final segmentation outputs, thereby preserving the model's performance. To enable ownership verification, we incorporate model-agnostic explanation methods, e.g. LIME, to extract feature attributions from the model outputs. Under specific triggering conditions, these explanations reveal a distinct and verifiable watermark. We further design the watermark as a QR code to facilitate robust and recognizable ownership claims. We conducted extensive experiments across four medical imaging datasets and five mainstream segmentation models. The results demonstrate the effectiveness, stealthiness, and harmlessness of our method on the original model's segmentation performance. For example, when applied to the SAM model, StealthMark consistently achieved ASR above 95% across various datasets while maintaining less than a 1% drop in Dice and AUC scores, significantly outperforming backdoor-based watermarking methods and highlighting its strong potential for practical deployment. Our implementation code is made available at: https://github.com/Qinkaiyu/StealthMark.

</details>


### [25] [iFSQ: Improving FSQ for Image Generation with 1 Line of Code](https://arxiv.org/abs/2601.17124)
*Bin Lin,Zongjian Li,Yuwei Niu,Kaixiong Gong,Yunyang Ge,Yunlong Lin,Mingzhe Zheng,JianWei Zhang,Miles Yang,Zhao Zhong,Liefeng Bo,Li Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种改进的有限标量量化方法iFSQ，通过一行代码替换FSQ中的激活函数，实现了理论上最佳的信息利用率和重建精度，促进了离散与连续图像生成模型的统一评测。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成领域在离散（自回归模型）和连续（扩散模型）方法间存在分歧，两者间缺乏统一的模型与公平的基准。传统FSQ在量化精度和信息利用率间难以两全，急需改进方法。

Method: 作者通过在FSQ中引入新的分布匹配映射激活函数，使隐变量分布更加均匀，确保每个量化桶都被有效利用，同时提升重建效果。该方法实现简单，仅需修改一行代码，并设计控制实验进行对比分析。

Result: 实验证明，iFSQ既可保证高效的信息利用，也能获得高重建精度。对比分析中发现：（1）在4 bits/dimension处离散与连续表示达到最优平衡；（2）AR模型收敛快但扩散模型最终效果更佳，序列性可能限制AR模型生成上限。

Conclusion: iFSQ为图像生成中的离散与连续方法提供了理论统一的平台。研究成果建议未来模型在信息量与模型结构间应寻求新平衡，同时为统一评测与结构创新提供方向。

Abstract: The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ

</details>


### [26] [Scaling medical imaging report generation with multimodal reinforcement learning](https://arxiv.org/abs/2601.17151)
*Qianchu Liu,Sheng Zhang,Guanghui Qin,Yu Gu,Ying Jin,Sam Preston,Yanbo Xu,Sid Kiblawi,Wen-wai Yim,Tim Ossowski,Tristan Naumann,Mu Wei,Hoifung Poon*

Main category: cs.CV

TL;DR: 提出了一种新的医学影像报告生成框架UniRG，通过强化学习优化指标，在权威基准上取得了新的最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有大模型虽能理解自然语言，但在医学等高价值领域的多模态理解、推理上仍有不足，医学影像报告生成便是典型例子，且传统有监督微调易陷入模板化过拟合，影响泛化。

Method: 提出通用报告生成框架UniRG，将强化学习作为统一机制，直接针对下游应用评测指标进行优化。以胸部X光数据为例，实现模型训练并设计多种严格评测场景。

Result: 在权威的ReXrank胸部X光报告生成基准上，UniRG-CXR显著超越所有已发表方法，刷新了整体最优性能。

Conclusion: UniRG框架能在医学影像报告生成领域取得优越且稳健的泛化能力，对行业有较大应用潜力。

Abstract: Frontier models have demonstrated remarkable capabilities in understanding and reasoning with natural-language text, but they still exhibit major competency gaps in multimodal understanding and reasoning especially in high-value verticals such as biomedicine. Medical imaging report generation is a prominent example. Supervised fine-tuning can substantially improve performance, but they are prone to overfitting to superficial boilerplate patterns. In this paper, we introduce Universal Report Generation (UniRG) as a general framework for medical imaging report generation. By leveraging reinforcement learning as a unifying mechanism to directly optimize for evaluation metrics designed for end applications, UniRG can significantly improve upon supervised fine-tuning and attain durable generalization across diverse institutions and clinical practices. We trained UniRG-CXR on publicly available chest X-ray (CXR) data and conducted a thorough evaluation in CXR report generation with rigorous evaluation scenarios. On the authoritative ReXrank benchmark, UniRG-CXR sets new overall SOTA, outperforming prior state of the art by a wide margin.

</details>


### [27] [LGDWT-GS: Local and Global Discrete Wavelet-Regularized 3D Gaussian Splatting for Sparse-View Scene Reconstruction](https://arxiv.org/abs/2601.17185)
*Shima Salehi,Atharva Agashe,Andrew J. McFarland,Joshua Peeples*

Main category: cs.CV

TL;DR: 本文提出了一种结合全局与局部频域正则化的少样本3D重建新方法，在稀疏视角下提升细节与几何稳定性，并开源了包含四个光谱波段的多光谱温室数据集及相关评测工具。实验显示方法在自有及标准数据集上效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅（3DGS）模型在少视角、稀疏数据条件下容易出现几何不稳定和细节丢失的问题。针对真实场景下采集条件受限的需求，亟需提升稀疏条件下的3D重建质量。

Method: 提出融合全局和局部频率正则的方法来约束3D重建过程，有效抑制噪声并保留重要细节。此外，构建并开放了一个涵盖四个光谱波段的多光谱温室植物数据集，同时发布了少样本重建评测工具和协议。

Result: 在多光谱温室数据集及主流3D重建基准数据集上，所提方法重建效果更清晰、稳定，在光谱一致性方面也优于主流方法。

Conclusion: 该工作为少样本3D重建提供了更高质量、适应多光谱场景的方法，并推动了领域标准化评测环境的构建，其数据与代码的开源有利于促进相关研究发展。

Abstract: We propose a new method for few-shot 3D reconstruction that integrates global and local frequency regularization to stabilize geometry and preserve fine details under sparse-view conditions, addressing a key limitation of existing 3D Gaussian Splatting (3DGS) models. We also introduce a new multispectral greenhouse dataset containing four spectral bands captured from diverse plant species under controlled conditions. Alongside the dataset, we release an open-source benchmarking package that defines standardized few-shot reconstruction protocols for evaluating 3DGS-based methods. Experiments on our multispectral dataset, as well as standard benchmarks, demonstrate that the proposed method achieves sharper, more stable, and spectrally consistent reconstructions than existing baselines. The dataset and code for this work are publicly available

</details>


### [28] [Decoding Psychological States Through Movement: Inferring Human Kinesic Functions with Application to Built Environments](https://arxiv.org/abs/2601.17194)
*Cheyu Lin,Katherine A. Flanigan,Sirajum Munir*

Main category: cs.CV

TL;DR: 本文提出了DUET数据集及内嵌肢体语言识别框架，用于在保护隐私的前提下测量和分析社会基础设施等空间中的有意义社交互动，并证明了该方法在不同环境下的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前在土木工程及建成环境中，缺乏一致且保护隐私的方法来衡量和表征空间中的社会性互动，导致设计干预效果难以评估。本文旨在解决该领域存在的测量及分析方法学缺口，提升社会资本相关互动的量化能力。

Method: 作者引入DUET（Dyadic User Engagement DataseT）数据集，涵盖12种二人互动场景和五类肢体语言功能，同时结合四种感知方式和三种空间类型。基于该数据集，设计算法框架直接从骨骼运动推断交际功能，无需人工定义动作功能字典，并用了迁移学习架构提升泛化能力。

Result: 通过对六种主流的人体活动识别模型进行基准测试，量化了交际功能识别的难度，并揭示了现有单人动作识别模型在社会互动测量上的局限性。框架可以很好地区分肢体语言功能，并展示模型表示能力与分类性能的强关联，在不同被试和环境下表现良好。

Conclusion: 本文方法为社会基础设施中社交互动的隐私保护测量与分析提供了新工具，有助于更好理解与设计促进社会资本形成的物理空间，并推动相关理论验证与实践优化。

Abstract: Social infrastructure and other built environments are increasingly expected to support well-being and community resilience by enabling social interaction. Yet in civil and built-environment research, there is no consistent and privacy-preserving way to represent and measure socially meaningful interaction in these spaces, leaving studies to operationalize "interaction" differently across contexts and limiting practitioners' ability to evaluate whether design interventions are changing the forms of interaction that social capital theory predicts should matter. To address this field-level and methodological gap, we introduce the Dyadic User Engagement DataseT (DUET) dataset and an embedded kinesics recognition framework that operationalize Ekman and Friesen's kinesics taxonomy as a function-level interaction vocabulary aligned with social capital-relevant behaviors (e.g., reciprocity and attention coordination). DUET captures 12 dyadic interactions spanning all five kinesic functions-emblems, illustrators, affect displays, adaptors, and regulators-across four sensing modalities and three built-environment contexts, enabling privacy-preserving analysis of communicative intent through movement. Benchmarking six open-source, state-of-the-art human activity recognition models quantifies the difficulty of communicative-function recognition on DUET and highlights the limitations of ubiquitous monadic, action-level recognition when extended to dyadic, socially grounded interaction measurement. Building on DUET, our recognition framework infers communicative function directly from privacy-preserving skeletal motion without handcrafted action-to-function dictionaries; using a transfer-learning architecture, it reveals structured clustering of kinesic functions and a strong association between representation quality and classification performance while generalizing across subjects and contexts.

</details>


### [29] [Structural Complexity of Brain MRI reveals age-associated patterns](https://arxiv.org/abs/2601.17211)
*Anzhe Cheng,Italo Ivo Lima Dias Pinto,Paul Bogdan*

Main category: cs.CV

TL;DR: 本文提出了一种针对三维信号（尤其是脑MRI）的结构复杂性分析方法，改进了传统粗化方案，并用于分析MRI数据，证明其在生物年龄预测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 脑MRI等三维信号数据具有多尺度和高复杂性，现有结构复杂性分析难以稳定地处理粗空间尺度，尤其是在采样有限时。因此，亟需更稳健、细致的方法来刻画多尺度结构并提取有价值的生物信息。

Method: 提出了一种新的滑动窗口粗化(coarse-graining)方法，替代传统分块方式，实现多尺度下的结构复杂性测量。通过滑动窗口对MRI大数据集的体素信息进行多尺度处理，量化信号在不同尺度下的信息损失。

Result: 经过新的滑动窗口粗化分析，对大量中老年MRI结构数据发现：结构复杂性指标随年龄系统性减低，且在较粗尺度下效果最显著。

Conclusion: 结构复杂性方法能可靠捕捉MRI等三维信号的多尺度特征，是有效的信号处理工具，可用于生物年龄等重要指标的预测。

Abstract: We adapt structural complexity analysis to three-dimensional signals, with an emphasis on brain magnetic resonance imaging (MRI). This framework captures the multiscale organization of volumetric data by coarse-graining the signal at progressively larger spatial scales and quantifying the information lost between successive resolutions. While the traditional block-based approach can become unstable at coarse resolutions due to limited sampling, we introduce a sliding-window coarse-graining scheme that provides smoother estimates and improved robustness at large scales. Using this refined method, we analyze large structural MRI datasets spanning mid- to late adulthood and find that structural complexity decreases systematically with age, with the strongest effects emerging at coarser scales. These findings highlight structural complexity as a reliable signal processing tool for multiscale analysis of 3D imaging data, while also demonstrating its utility in predicting biological age from brain MRI.

</details>


### [30] [PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation](https://arxiv.org/abs/2601.17885)
*Qingyu Fan,Zhaoxiang Li,Yi Lu,Wang Chen,Qiu Shen,Xiao-xiao Long,Yinghao Cai,Tao Lu,Shuo Wang,Xun Cao*

Main category: cs.CV

TL;DR: PEAfowl是一种提升双手操作在复杂场景下泛化能力的新型视觉-语言-动作(VLA)模型，利用创新的多视角空间表达和更细致的语言指令结合方式，实现稳健操作和优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在遮挡、多视角变化和复杂环境中表现不佳，原因在于空间理解和指令结合方式有限，难以泛化和精准理解任务指令，亟需新方法提升多视角一致性和语言表达能力。

Method: PEAfowl通过预测每个token的深度分布，实现可微分的3D特征变换，并跨视角聚合邻域信息，获得几何一致性的空间表达；采用Perceiver风格的基于文本的特征读取方式，在冻结CLIP视觉特征的基础上迭代累积“证据”，实现更精细的语言指令结合。此外，通过训练期间引入深度蒸馏，实现对低质量深度数据感知的几何意识提升，无需增加推理开销。

Result: 在RoboTwin 2.0平台的域随机化设置下，PEAfowl将最强基线的成功率提升了23.0个百分点；实际机器人实验也证明了其良好的仿真到现实迁移能力和深度蒸馏带来的持续改进。

Conclusion: PEAfowl显著提升了双手操作任务中多视角场景下的稳健性与泛化能力，方法对空间理解及指令结合的提升为视觉-语言-动作模型在真实复杂环境中的应用带来了新思路。

Abstract: Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.
  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.
  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.
  Project website: https://peafowlvla.github.io/.

</details>


### [31] [Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction](https://arxiv.org/abs/2601.17216)
*Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy*

Main category: cs.CV

TL;DR: 该论文提出了一种基于语义V2X的智能交通系统碰撞预测框架，利用语义嵌入替代原始视频传输，大幅降低通信需求且提升预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有车辆通信传递原始视频或高维感知数据，受限于带宽和延迟，难以满足实时碰撞预测需求，因此需寻找更高效的数据传输与处理方式。

Method: RSU端采用V-JEPA架构对未来帧做时空语义嵌入，并通过V2X发送给车辆。车辆端用轻量级注意力探测器和分类器解码语义嵌入，从而预测即将发生的碰撞。实验利用城市交通数字孪生环境生成多样场景进行评估。

Result: 新框架在不损失预测准确性的情况下，将通信需求降低了四个数量级，并把碰撞预测F1值提升了10%。

Conclusion: 该语义V2X通信架构能高效支持实时协作碰撞预测，显示出智能交通系统落地的潜力。

Abstract: Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.

</details>


### [32] [Masked Depth Modeling for Spatial Perception](https://arxiv.org/abs/2601.17895)
*Bin Tan,Changjiang Sun,Xiage Qin,Hanat Adai,Zelin Fu,Tianxiang Zhou,Han Zhang,Yinghao Xu,Xing Zhu,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: 论文提出了一种名为LingBot-Depth的深度补全模型，能够利用视觉上下文对有缺失或不准确的深度图进行完善，并通过自动化数据处理提升训练效率，模型在深度精度与像素覆盖率方面优于主流RGB-D相机。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D相机在捕捉真实三维场景时受限于硬件和成像条件（如高反光或无纹理表面），导致深度图存在缺失或误差，为此需要新的方法有效补全和提升深度信息。作者认为这些“有遮罩”的误差信号反映了底层的几何不确定性，故提出基于视觉上下文的深度补全方法。

Method: 提出LingBot-Depth深度补全模型，通过在深度图中掩蔽部分像素（masked depth modeling），利用RGB图像和上下文信息推理并补全缺失深度。同时设计了自动化数据整理流程，实现高效且可扩展的模型训练。最终公开了含有大量真实和模拟配对数据的数据集、代码及模型。

Result: LingBot-Depth模型在深度图补全任务上，无论是深度精度还是像素覆盖率，均优于高级RGB-D相机。模型对下游任务也适用，并在RGB与深度模态间形成了对齐的潜在表征。

Conclusion: LingBot-Depth有效改善了受硬件和成像条件影响的深度图质量，对三维空间视觉感知和相关应用具有积极推动作用。公开的大规模数据和模型资源促进了领域发展。

Abstract: Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as "masked" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.

</details>


### [33] [Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification](https://arxiv.org/abs/2601.17228)
*Tengyue Zhang,Ruiwen Ding,Luoting Zhuang,Yuxiao Wu,Erika F. Rodriguez,William Hsu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于潜在扩散模型的半监督领域自适应（SSDA）框架，通过生成既保留组织形态又具有目标域特征的合成病理图像，提高跨队列、跨机构的深度学习模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在病理图像分析任务中，因领域间分布差异（domain shift）导致模型泛化性能下降。现有解决方法往往不能有效利用目标域无标签数据，或依赖图像到图像的翻译，可能导致组织结构扭曲、影响模型准确性。因此需要创新的方法提升领域泛化性能。

Method: 作者提出一种半监督领域自适应框架，利用在源域和目标域无标签数据上训练的潜在扩散模型，生成既保留源域组织结构又兼具目标域外观特征的合成图像。扩散模型在生成时结合基础模型特征、队列标识、样本制备方式等条件，生成目标感知合成图像。用这些合成图片与源域标注图片结合，训练下游分类器，并在目标域进行测试。

Result: 在肺腺癌预后预测任务上，该方法显著提升了模型在目标域测试集上的表现。具体来说，目标域加权F1评分从0.611提升到0.706，宏平均F1分数从0.641提升到0.716，且未导致源域性能下降。

Conclusion: 基于扩散模型生成的目标域特征合成数据，有效提升了计算病理领域的领域泛化能力，是一种有前景且高效的数据增强及适应方法。

Abstract: Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology.

</details>


### [34] [Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning](https://arxiv.org/abs/2601.18714)
*Judith Vilella-Cantos,Mauro Martini,Marcello Chiaberge,Mónica Ballesta,David Valiente*

Main category: cs.CV

TL;DR: 本文提出了一种面向葡萄园自动化环境的高效定位方法MinkUNeXt-VINE，利用轻量级深度学习技术，实现了对现有技术的超越，尤其适用于低成本、稀疏LiDAR数据，验证了其实用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 农业环境结构不规则、缺乏明显地标，导致移动机器人定位任务困难。现有研究多关注目标分类和分割，定位识别问题尚未解决。

Method: 提出MinkUNeXt-VINE方法，采用高效的预处理和Matryoshka多损失表示学习策略，通过低成本稀疏LiDAR输入及低维输出实现高效率，在葡萄园真实场景下进行大量消融实验。

Result: 方法在两个大规模长期葡萄园LiDAR数据集上取得了优异表现，对比最先进方法在低成本、低分辨率输入数据下依然保持高效和鲁棒。

Conclusion: MinkUNeXt-VINE在农业环境下实现了高效率和高精度的地点识别，即使在成本受限、传感器性能较低时，依然能获得良好表现，对农业机器人定位具有重要价值。

Abstract: Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.

</details>


### [35] [C-RADIOv4 (Tech Report)](https://arxiv.org/abs/2601.17237)
*Mike Ranzinger,Greg Heinrich,Collin McCarthy,Jan Kautz,Andrew Tao,Bryan Catanzaro,Pavlo Molchanov*

Main category: cs.CV

TL;DR: 本文介绍了C-RADIOv4视觉模型，采用多教师蒸馏技术，在保持计算复杂度不变的前提下显著提升了下游任务性能，并引入了灵活的分辨率支持和高效特性。


<details>
  <summary>Details</summary>
Motivation: 针对不同教师模型各具优势且难以统一转移到单一模型的问题，提出利用多教师蒸馏，实现统一学生模型继承多位教师能力的目标。

Method: C-RADIOv4基于前作AM-RADIO/RADIOv2.5，使用SigLIP2、DINOv3、SAM3作为教师，通过多教师蒸馏技术训练学生模型，涵盖不同参数规模，增强了任意分辨率支持，并重新加入ViTDet以提升高分辨率下的效率。

Result: C-RADIOv4在核心评测指标和下游任务能力上相较前代大幅提升，能模仿SAM3获得新能力，并提升对任意分辨率图像的支持和推理效率，且模型使用了更为宽松的开源许可。

Conclusion: C-RADIOv4通过多教师蒸馏和设计改进，有效将多个先进教师的能力整合到单一模型中，兼具高性能、灵活性和开放性，是通用视觉骨干网的重要进展。

Abstract: By leveraging multi-teacher distillation, agglomerative vision backbones provide a unified student model that retains and improves the distinct capabilities of multiple teachers. In this tech report, we describe the most recent release of the C-RADIO family of models, C-RADIOv4, which builds upon AM-RADIO/RADIOv2.5 in design, offering strong improvements on key downstream tasks at the same computational complexity. We release -SO400M (412M params), and -H (631M) model variants, both trained with an updated set of teachers: SigLIP2, DINOv3, and SAM3. In addition to improvements on core metrics and new capabilities from imitating SAM3, the C-RADIOv4 model family further improves any-resolution support, brings back the ViTDet option for drastically enhanced efficiency at high-resolution, and comes with a permissive license.

</details>


### [36] [Multi-stage Bridge Inspection System: Integrating Foundation Models with Location Anonymization](https://arxiv.org/abs/2601.17254)
*Takato Yasuno*

Main category: cs.CV

TL;DR: 本文开发了一个集成地域信息隐私保护的桥梁损伤检测系统，实现了对混凝土裂缝、钢筋暴露等病害的高效识别，并能自动保护包含地区信息的施工标志区域。系统开源且处理速度快，适用于日本当前的基础设施巡检需求。


<details>
  <summary>Details</summary>
Motivation: 日本法规要求每五年对民用基础设施进行一次目视巡检，采集的图像常含有暴露地域信息（如施工标志），若直接公布可能引起公众焦虑。因此，急需一种在提取损伤特征的同时保护地域隐私的自动化检测方案。

Method: 该系统基于“Segment Anything Model 3（SAM 3）”对混凝土裂缝和钢筋暴露进行检测；利用DBSCAN算法自动补全漏检区域；通过高斯模糊技术检测并保护包含地域信息的施工标志区域；结合4种OCR预处理方法提升识别准确率；并通过GPU优化实现1.7秒/张的处理速度。技术栈包括SAM3、PyTorch、OpenCV、pytesseract和scikit-learn。

Result: 该系统能够高效准确地完成桥梁损伤自动检测和区域信息隐私保护，极大提升基础设施巡检效率，同时防止敏感地域信息泄露。具体在处理速度和识别准确性方面均达到了较高水平。

Conclusion: 本文所提出的系统兼顾桥梁结构损伤自动检测与地理隐私保护，适合日本等对巡检既有信息安全需求又要求高效的场景。开源实现具备实际应用和推广价值。

Abstract: In Japan, civil infrastructure condition monitoring is mandated through visual inspection every five years. Field-captured damage images frequently contain concrete cracks and rebar exposure, often accompanied by construction signs revealing regional information. To enable safe infrastructure use without causing public anxiety, it is essential to protect regional information while accurately extracting damage features and visualizing key indicators for repair decision-making. This paper presents an open-source bridge damage detection system with regional privacy protection capabilities. We employ Segment Anything Model (SAM) 3 for rebar corrosion detection and utilize DBSCAN for automatic completion of missed regions. Construction sign regions are detected and protected through Gaussian blur. Four preprocessing methods improve OCR accuracy, and GPU optimization enables 1.7-second processing per image. The technology stack includes SAM3, PyTorch, OpenCV, pytesseract, and scikit-learn, achieving efficient bridge inspection with regional information protection.

</details>


### [37] [FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding](https://arxiv.org/abs/2601.17258)
*João Pereira,Vasco Lopes,João Neves,David Semedo*

Main category: cs.CV

TL;DR: 该论文针对视频异常理解（VAU）任务，提出了新的基准FineVAU，包括更贴近人类感知的评测指标FVScore和全新高质量数据集FineW3。实验表明新标准能更准确评估异常事件的描述能力，并揭示当前LVLM在空间和细粒度时序异常事件上的局限。


<details>
  <summary>Details</summary>
Motivation: 现有VAU评测依赖n-gram或LLM相关指标，难以捕捉与人类感知一致的、细粒度且与视觉紧密相关的异常描述，因此需要更合理、精细的人类对齐评测方法和更丰富的数据集。

Method: 将VAU细分为三部分（事件、参与实体、地点），提出新评测指标FVScore评价LVLM答案中关键视觉元素的覆盖度，并自动构建综合性数据集FineW3，通过结构化方法扩展原有人类标注并加入细粒度视觉信息。

Result: 人类评测结果显示，FVScore比现有方法更贴合人类对异常事件的理解。基于FineVAU的实验揭示，尽管LVLM在粗粒度和静态信息事件上表现良好，但在需要空间、细粒度时序感知的异常事件上仍存在显著局限。

Conclusion: FineVAU及其配套指标和数据集可有效提升视频异常理解的评价质量和研究水平，为未来提升LVLM对异常事件的多角度、多维度理解能力提供了基础。

Abstract: Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.

</details>


### [38] [Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling](https://arxiv.org/abs/2601.17259)
*Angad Singh Ahuja,Aarush Ram Anandh*

Main category: cs.CV

TL;DR: 本文提出了一种在推理阶段控制文本到图像扩散模型输出颜色的新方法，无需额外训练，可集成到标准Stable Diffusion修复流程中，实现对特定区域颜色的精准控制。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成图像时，难以精确满足用户指定的颜色要求，尤其是在设计相关的实际应用中，要严格控制输出颜色，但常常失败。纯粹用均值对颜色约束容易掩盖局部显著偏差，需要更精细的方案。

Method: 1）基于ROI（感兴趣区域）的修复法，实现空间选择性；2）背景隐空间的再叠加，防止ROI外颜色漂移；3）利用在CIE Lab与线性RGB空间下定义的组合损失函数进行梯度引导控制，包括平均颜色、像素分布尾部，采用CVaR与软最大惩罚，以及门控与步长调度保证扩散过程稳定。

Result: 实验表明，传统只控制均值的方案虽然能满足整体颜色，但常出现局部显著异常；而本文方法则有效实现分布感知的颜色精准控制，表现出更好的目标颜色一致性和稳定性。

Conclusion: 提出的方法是一种无需训练、可集成在Stable Diffusion修复流程中的实用技术，可以大幅提升扩散模型对特定区域目标颜色的严格遵循能力，适用于对颜色精确要求较高的实际场景。

Abstract: Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.

</details>


### [39] [Cross360: 360° Monocular Depth Estimation via Cross Projections Across Scales](https://arxiv.org/abs/2601.17271)
*Kun Huang,Fang-Lue Zhang,Neil Dodgson*

Main category: cs.CV

TL;DR: 本文提出了一种基于跨注意力机制的新型360°深度估计算法Cross360，能够有效融合局部与全局信息，显著提升深度估计效果。


<details>
  <summary>Details</summary>
Motivation: 现有360°深度估计方法难以兼顾全局连续性与去畸变，局部特征全局感知不足，且跨投影特征融合存在边界提取不一致问题。

Method: 提出Cross360架构，通过交叉注意力机制对齐切平面（tangent）局部投影特征与等矩形（equirectangular）全局特征，并引入逐步特征聚合与注意力模块精细处理多尺度特征。

Result: 在多数基准数据集上，Cross360在深度估计准确性及全局一致性上均显著超越现有方法，尤其是对完整360°图像表现更优。

Conclusion: Cross360通过有效融合全局与局部信息，解决了360°深度估计中的连续性和一致性难题，在实际应用中展现了很高的潜力和效果。

Abstract: 360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection's 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.

</details>


### [40] [Fluxamba: Topology-Aware Anisotropic State Space Models for Geological Lineament Segmentation in Multi-Source Remote Sensing](https://arxiv.org/abs/2601.17288)
*Jin Bai,Huiyao Zhang,Qi Wen,Shengyang Li,Xiaolin Tian,Atta ur Rahman*

Main category: cs.CV

TL;DR: 提出了Fluxamba，一种用于地质线性特征精确分割的轻量级网络架构，专门为捕捉地质特征复杂的非均匀拓扑关系而设计，实验验证其性能和效率均优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前SSM等方法在地质线特征分割中因依赖刚性轴向扫描路径，与曲线特征的内在几何结构存在拓扑不匹配，导致特征提取丢失和分割效果不佳，亟需一种能自适应目标拓扑结构、提升上下文捕获能力的方法。

Method: 提出了结构流块（SFB），通过各向异性结构门（ASG）与先验调制流（PMF）动态引导信息流，有效分离特征方向与空间位置；同时引入层次化空间调节器（HSR）实现多尺度语义对齐以及高保真聚焦单元（HFFU）提升低对比度环境下的信噪比。

Result: 在LROC-Lineament、LineaMapper和GeoCrack等多个数据集上实验，Fluxamba获得极高分割精度（如LROC-Lineament数据集F1=89.22%、mIoU=89.87%），并且推理速度超24FPS，参数量仅3.4M、FLOPs仅6.3G，计算消耗比传统方法减小1-2个数量级。

Conclusion: Fluxamba在保持极高分割精度的同时，大幅降低了算力消耗，实现了分割效果和部署可行性的Pareto最优，为地质线性特征分割提供了高效、实用的新路径。

Abstract: The precise segmentation of geological linear features, spanning from planetary lineaments to terrestrial fractures, demands capturing long-range dependencies across complex anisotropic topologies. Although State Space Models (SSMs) offer near-linear computational complexity, their dependence on rigid, axis-aligned scanning trajectories induces a fundamental topological mismatch with curvilinear targets, resulting in fragmented context and feature erosion. To bridge this gap, we propose Fluxamba, a lightweight architecture that introduces a topology-aware feature rectification framework. Central to our design is the Structural Flux Block (SFB), which orchestrates an anisotropic information flux by integrating an Anisotropic Structural Gate (ASG) with a Prior-Modulated Flow (PMF). This mechanism decouples feature orientation from spatial location, dynamically gating context aggregation along the target's intrinsic geometry rather than rigid paths. Furthermore, to mitigate serialization-induced noise in low-contrast environments, we incorporate a Hierarchical Spatial Regulator (HSR) for multi-scale semantic alignment and a High-Fidelity Focus Unit (HFFU) to explicitly maximize the signal-to-noise ratio of faint features. Extensive experiments on diverse geological benchmarks (LROC-Lineament, LineaMapper, and GeoCrack) demonstrate that Fluxamba establishes a new state-of-the-art. Notably, on the challenging LROC-Lineament dataset, it achieves an F1-score of 89.22% and mIoU of 89.87%. Achieving a real-time inference speed of over 24 FPS with only 3.4M parameters and 6.3G FLOPs, Fluxamba reduces computational costs by up to two orders of magnitude compared to heavy-weight baselines, thereby establishing a new Pareto frontier between segmentation fidelity and onboard deployment feasibility.

</details>


### [41] [Dynamic Meta-Ensemble Framework for Efficient and Accurate Deep Learning in Plant Leaf Disease Detection on Resource-Constrained Edge Devices](https://arxiv.org/abs/2601.17290)
*Weloday Fikadu Moges,Jianmei Su,Amin Waqas*

Main category: cs.CV

TL;DR: 提出了一个动态元集成框架（DMEF），结合三种轻量级卷积神经网络，实现高精度且高效的植物病害识别，适合部署在资源有限的边缘设备上。


<details>
  <summary>Details</summary>
Motivation: 边缘设备如物联网传感器和智能手机的计算和能源受限，传统深度学习模型难以在这类设备上高效部署，因此需要兼顾精度和效率的解决方案。

Method: DMEF框架动态分配MobileNetV2、NASNetMobile和InceptionV3三种轻量级模型的集成权重，通过优化精度提升和模型复杂度之间的权衡。在训练过程中，权重根据模型表现和复杂度自适应调整。

Result: 在马铃薯和玉米病害数据集上，DMEF取得了99.53%和96.61%的精度，分别比单模型和静态集成提升了2.1%和6.3%。推理延迟低于75ms，参数量小于100万，展现了优秀的边缘设备适应性。

Conclusion: DMEF有效提升了在资源受限设备上的植物病害诊断精度和效率，推动高精度AI模型在农业实际场景中的应用落地，具有良好的可扩展性和实用前景。

Abstract: Deploying deep learning models for plant disease detection on edge devices such as IoT sensors, smartphones, and embedded systems is severely constrained by limited computational resources and energy budgets. To address this challenge, we introduce a novel Dynamic Meta-Ensemble Framework (DMEF) for high-accuracy plant disease diagnosis under resource constraints. DMEF employs an adaptive weighting mechanism that dynamically combines the predictions of three lightweight convolutional neural networks (MobileNetV2, NASNetMobile, and InceptionV3) by optimizing a trade-off between accuracy improvements (DeltaAcc) and computational efficiency (model size). During training, the ensemble weights are updated iteratively, favoring models exhibiting high performance and low complexity. Extensive experiments on benchmark datasets for potato and maize diseases demonstrate state-of-the-art classification accuracies of 99.53% and 96.61%, respectively, surpassing standalone models and static ensembles by 2.1% and 6.3%. With computationally efficient inference latency (<75ms) and a compact footprint (<1 million parameters), DMEF shows strong potential for edge-based agricultural monitoring, suggesting viability for scalable crop disease management. This bridges the gap between high-accuracy AI and practical field applications.

</details>


### [42] [ClinNet: Evidential Ordinal Regression with Bilateral Asymmetry and Prototype Memory for Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.17315)
*Xiaoyang Li,Runni Zhou*

Main category: cs.CV

TL;DR: 本论文提出了一种新的KOA（膝关节骨关节炎）分级方法——ClinNet，结合结构差异建模、特征记忆和不确定性建模，显著提升了分级准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: KOA分级依靠X光影像，难点在于分级差异细微、标注存在不确定性且疾病分级有序，传统方法忽视这些问题，影响分级的准确性和实际应用价值。

Method: 提出ClinNet框架，包括三个关键模块：(1)双侧不对称编码器（BAE）建模膝关节内外侧结构差异；(2)诊断记忆库存储各类别原型以增强特征稳定性；(3)基于正态-逆伽马（NIG）分布的证据有序头，实现连续分级与不确定性联合估计。

Result: ClinNet在实验中获得了0.892的Quadratic Weighted Kappa分数与0.768的准确率，统计学上显著优于现有主流方法（p < 0.001）。

Conclusion: ClinNet不仅提升了KOA分级准确性，还能有效标记分布外样本与潜在误诊，为临床安全应用提供了保障。

Abstract: Knee osteoarthritis (KOA) grading based on radiographic images is a critical yet challenging task due to subtle inter-grade differences, annotation uncertainty, and the inherently ordinal nature of disease progression. Conventional deep learning approaches typically formulate this problem as deterministic multi-class classification, ignoring both the continuous progression of degeneration and the uncertainty in expert annotations. In this work, we propose ClinNet, a novel trustworthy framework that addresses KOA grading as an evidential ordinal regression problem. The proposed method integrates three key components: (1) a Bilateral Asymmetry Encoder (BAE) that explicitly models medial-lateral structural discrepancies; (2) a Diagnostic Memory Bank that maintains class-wise prototypes to stabilize feature representations; and (3) an Evidential Ordinal Head based on the Normal-Inverse-Gamma (NIG) distribution to jointly estimate continuous KL grades and epistemic uncertainty. Extensive experiments demonstrate that ClinNet achieves a Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). Crucially, we demonstrate that the model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses, paving the way for safe clinical deployment.

</details>


### [43] [SkyReels-V3 Technique Report](https://arxiv.org/abs/2601.17323)
*Debang Li,Zhengcong Fei,Tuanhui Li,Yikun Dou,Zheng Chen,Jiangping Yang,Mingyuan Fan,Jingtao Xu,Jiahua Wang,Baoxuan Gu,Mingshan Chang,Yuqiang Xie,Binjie Mao,Youqiang Zhang,Nuo Pang,Hao Zhang,Yuzhe Jin,Zhiheng Xu,Dixuan Lin,Guibin Chen,Yahui Zhou*

Main category: cs.CV

TL;DR: 本文提出了SkyReels-V3，一个基于扩散Transformer的多模态条件视频生成模型，支持图像生成视频、视频扩展、音频驱动视频等多种生成模式，在多项指标上接近或达到当前业界最佳水平。


<details>
  <summary>Details</summary>
Motivation: 视频生成对于世界模型建设至关重要，而多模态上下文推理是评估其能力的核心挑战。为了应对不同场景下多模态条件的复杂需求以及主流系统关闭源的困境，作者希望构建统一、灵活且高效的视频生成框架。

Method: SkyReels-V3采用扩散Transformer作为核心骨干，统一支持三种生成模式：1）图像到视频，凭借多样数据处理流程和混合训练策略增强身份保持和叙事连贯性；2）视频扩展，通过时空一致建模和大规模视频理解支持单镜头与多镜头无缝衔接；3）音频驱动说话人视频，利用首末帧嵌入和关键帧推理实现长时音视频同步生成。

Result: 在视觉质量、指令跟随和具体细节指标上，SkyReels-V3达到或接近现有最优水平，部分能力上逼近主流闭源领先系统。

Conclusion: SkyReels-V3作为统一多模态条件视频生成框架，兼具卓越的生成质量和通用性，为开放源世界模型及视频生成领域提供强有力的工具。

Abstract: Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.
  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.

</details>


### [44] [SymbolSight: Minimizing Inter-Symbol Interference for Reading with Prosthetic Vision](https://arxiv.org/abs/2601.17326)
*Jasmine Lesner,Michael Beyeler*

Main category: cs.CV

TL;DR: 论文提出了一种通过优化视觉符号自身（而非改善硬件）的方法，以减轻视网膜假体中符号残影效应造成的阅读混淆。使用符号-字母重新映射和大数据驱动的方法，优化后通讯混淆率大幅降低。


<details>
  <summary>Details</summary>
Motivation: 当前的视网膜假体因空间分辨率低和视觉残影显著，导致使用者在阅读时，前后符号信息干扰，阅读准确率低。硬件升级成本高且进展缓慢，因此有必要探索软件层面即符号优化的解决思路。

Method: 提出SymbolSight计算框架：通过模拟假体视觉及神经代理观测，评估不同符号对的可混淆性，用针对具体语言的大数据双字统计，优化符号-字母的映射，使常见临近字母间混淆最小化。

Result: 在阿拉伯语、保加利亚语和英语的模拟实验中，优化后的符号系统比原生字母系统将预测混淆率中位数降低22倍。

Conclusion: 标准字体其实并不适合低带宽、串行特性的假体视觉。通过计算建模和符号优化，能够为未来心理物理或临床测试筛选出更高潜力的视觉标记系统，推动辅助视觉技术的发展。

Abstract: Retinal prostheses restore limited visual perception, but low spatial resolution and temporal persistence make reading difficult. In sequential letter presentation, the afterimage of one symbol can interfere with perception of the next, leading to systematic recognition errors. Rather than relying on future hardware improvements, we investigate whether optimizing the visual symbols themselves can mitigate this temporal interference. We present SymbolSight, a computational framework that selects symbol-to-letter mappings to minimize confusion among frequently adjacent letters. Using simulated prosthetic vision (SPV) and a neural proxy observer, we estimate pairwise symbol confusability and optimize assignments using language-specific bigram statistics. Across simulations in Arabic, Bulgarian, and English, the resulting heterogeneous symbol sets reduced predicted confusion by a median factor of 22 relative to native alphabets. These results suggest that standard typography is poorly matched to serial, low-bandwidth prosthetic vision and demonstrate how computational modeling can efficiently narrow the design space of visual encodings to generate high-potential candidates for future psychophysical and clinical evaluation.

</details>


### [45] [Learning with Geometric Priors in U-Net Variants for Polyp Segmentation](https://arxiv.org/abs/2601.17331)
*Fabian Vazquez,Jose A. Nuñez,Diego Adame,Alissen Moreno,Augustin Zhan,Huimin Li,Jinghao Yang,Haoteng Tang,Bin Fu,Pengfei Gu*

Main category: cs.CV

TL;DR: 本文提出了一种新的几何先验引导模块（GPM），通过融合显式深度几何信息，提升了U-Net架构在结肠镜息肉分割中的准确性和鲁棒性，在五个公开数据集上的表现超过了现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积神经网络、Transformer或Mamba的U-Net结构虽然在息肉分割任务上取得了较好表现，但在低对比度或杂乱场景下难以充分捕捉几何和结构线索，影响分割准确度。因此，研究者希望通过引入几何先验来提升模型能力。

Method: 作者提出了一种几何先验引导模块（GPM），其核心方法为：先用Visual Geometry Grounded Transformer（VGGT）在模拟的ColonDepth数据集上微调，生成与内镜息肉场景相匹配的深度图；这些深度信息再经GPM处理被编码到U-Net的特征图中，并结合空间和通道注意力机制，以突出本地空间和全局通道信息。GPM可插拔，能够轻松整合进各种U-Net变体。

Result: 在五个公开息肉分割数据集上，作者的方法在三个强基线上都取得了持续提升，验证了其对不同U-Net结构的普适性和有效性。

Conclusion: 引入几何深度先验不仅提升了分割性能，也增强了对复杂场景的适应能力。GPM方法具有通用性和灵活性，可为早期结直肠癌检测与辅助诊断系统带来更高可靠性。

Abstract: Accurate and robust polyp segmentation is essential for early colorectal cancer detection and for computer-aided diagnosis. While convolutional neural network-, Transformer-, and Mamba-based U-Net variants have achieved strong performance, they still struggle to capture geometric and structural cues, especially in low-contrast or cluttered colonoscopy scenes. To address this challenge, we propose a novel Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for polyp segmentation. Specifically, we fine-tune the Visual Geometry Grounded Transformer (VGGT) on a simulated ColonDepth dataset to estimate depth maps of polyp images tailored to the endoscopic domain. These depth maps are then processed by GPM to encode geometric priors into the encoder's feature maps, where they are further refined using spatial and channel attention mechanisms that emphasize both local spatial and global channel information. GPM is plug-and-play and can be seamlessly integrated into diverse U-Net variants. Extensive experiments on five public polyp segmentation datasets demonstrate consistent gains over three strong baselines. Code and the generated depth maps are available at: https://github.com/fvazqu/GPM-PolypSeg

</details>


### [46] [AGE-Net: Spectral--Spatial Fusion and Anatomical Graph Reasoning with Evidential Ordinal Regression for Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.17336)
*Xiaoyang Li,Runni Zhou*

Main category: cs.CV

TL;DR: 提出了一种新的深度学习模型AGE-Net，用于膝关节X光片的自动Kellgren-Lawrence分级，显著提升了分级准确率和模型解释性。


<details>
  <summary>Details</summary>
Motivation: 膝关节KL分级因结构变化微妙、解剖依赖长距离、以及等级边界模糊而难以自动化实现。现有方法无法同时很好处理预测不确定性和标签顺序性。

Method: 基于ConvNeXt架构，融合频谱-空间特征（SSF）、解剖图推理（AGR）、和差异细化（DFR）模块。引入Normal-Inverse-Gamma（NIG）证据回归头及序对排序约束，以捕捉预测不确定性和维持标签顺序。

Result: 在膝关节KL数据集上，AGE-Net取得了QWK分数0.9017±0.0045、均方误差0.2349±0.0028，优于多种CNN基线，并在消融实验中表现出一致增益。

Conclusion: AGE-Net有效解决了KL自动分级的主要难题，同时提升了分级准确性、不确定性建模能力及结果解释性。

Abstract: Automated Kellgren--Lawrence (KL) grading from knee radiographs is challenging due to subtle structural changes, long-range anatomical dependencies, and ambiguity near grade boundaries. We propose AGE-Net, a ConvNeXt-based framework that integrates Spectral--Spatial Fusion (SSF), Anatomical Graph Reasoning (AGR), and Differential Refinement (DFR). To capture predictive uncertainty and preserve label ordinality, AGE-Net employs a Normal-Inverse-Gamma (NIG) evidential regression head and a pairwise ordinal ranking constraint. On a knee KL dataset, AGE-Net achieves a quadratic weighted kappa (QWK) of 0.9017 +/- 0.0045 and a mean squared error (MSE) of 0.2349 +/- 0.0028 over three random seeds, outperforming strong CNN baselines and showing consistent gains in ablation studies. We further outline evaluations of uncertainty quality, robustness, and explainability, with additional experimental figures to be included in the full manuscript.

</details>


### [47] [TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution](https://arxiv.org/abs/2601.17340)
*Haodong He,Xin Zhan,Yancheng Bai,Rui Lan,Lei Sun,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 本论文针对现实世界文本图像超分辨率任务，提出了大规模高质量真实数据集Real-Texts，并基于扩散模型提出了TEXTS-Diff方法，实现了文本与背景的高质量重建，提升了文本恢复的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有数据集中的文本图像数据稀缺，导致文本区域的超分效果较差。此外，大多数数据集仅由孤立文本样本组成，背景重建质量有限。为弥补这些不足，需要更大的、来自真实场景的含文本图像数据集，并有针对性地提升模型对文本与背景的恢复能力。

Method: 1. 构建了Real-Texts数据集，涵盖中英文、场景多样的真实图像文本实例。
2. 提出了TEXTS-Aware Diffusion Model（TEXTS-Diff），通过抽象概念提升对场景中文字元素的理解，通过对具体文本区域的建模提高细节恢复能力，实现文本和背景的高质量重建。

Result: 实验证明，TEXTS-Diff方法在多个评价指标上达到最新最优水平，具备优异的泛化与文本还原能力，能有效缓解文本区域的失真与伪影。

Conclusion: 结合大规模真实文本图像数据集和文本感知扩散模型，有效提升了超分方法对复杂场景下文本及背景的重建质量，推动了文本图像超分领域的发展。

Abstract: Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.

</details>


### [48] [STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2601.17342)
*Tong Wang,Xiaodong Zhang,Guanzhou Chen,Jiaqi Wang,Chenxi Liu,Xiaoliang Tan,Wenchao Guo,Xuyang Li,Xuanrui Wang,Zifan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种针对遥感多模态数据缺失问题的语义分割新框架STARS，通过创新的特征翻译与对齐机制，显著提升了不完整多模态输入下的分割鲁棒性和少数类识别能力。


<details>
  <summary>Details</summary>
Motivation: 在遥感应用中，融合光学、SAR和DSM等多模态数据有助于提升地表语义理解，但实际中经常出现某一模态数据缺失，导致融合模型性能急剧下降。现有方法存在特征坍缩和恢复特征过于泛化等不足，难以有效应对数据缺失。

Method: 本文提出STARS框架，包含两个核心机制：1）非对称对齐机制，采用双向特征翻译与stop-gradient，防止特征坍缩并降低对超参数敏感性；2）像素级语义采样对齐（PSA）策略，通过类别均衡像素采样与跨模态语义对齐损失，缓解类别不平衡引发的对齐失效，提升小类目标的识别。

Result: 实验结果表明，STARS在不完整多模态输入的遥感语义分割任务上表现出更强的鲁棒性和分割精度，尤其在少数类别上有显著提升。

Conclusion: STARS通过共享-特异性特征翻译与像素级对齐技术，有效解决了多模态遥感数据缺失带来的分割难题，为实际应用中多模态数据不全情况下的地表语义理解提供了新方案。

Abstract: Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.

</details>


### [49] [Revisiting Lightweight Low-Light Image Enhancement: From a YUV Color Space Perspective](https://arxiv.org/abs/2601.17349)
*Hailong Yan,Shice Liu,Xiangtao Zhang,Lujian Yao,Fengxiang Yang,Jinwei Chen,Bo Li*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的基于YUV色彩空间的轻量级低光照图像增强方法，在保证视觉质量的同时显著降低了模型复杂度，并在多个基准测试上取得了新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 在移动互联网时代，低光照图像增强技术对提升移动设备拍摄图片的质量至关重要。当前方法常因忽略通道特定的退化模式和跨通道的相互作用，导致在提升视觉质量和模型轻量化之间难以兼顾。

Method: 作者通过频域分析证实YUV色彩空间对于低光照增强更具优势。基于YUV通道的退化特性，提出了Y通道采用双流全局-局部注意力模块，UV通道用Y引导的局部感知频率注意力模块，并通过引导交互模块融合特征。

Result: 该模型在多个权威基准测试上表现出比以往方法更高的视觉质量，同时参数量显著减少，确立了新一代的低光照图像增强性能标杆。

Conclusion: 论文证明了基于YUV通道特定恢复策略的轻量级低光照增强方法，能有效兼顾视觉质量与模型效率，具备实际应用潜力。

Abstract: In the current era of mobile internet, Lightweight Low-Light Image Enhancement (L3IE) is critical for mobile devices, which faces a persistent trade-off between visual quality and model compactness. While recent methods employ disentangling strategies to simplify lightweight architectural design, such as Retinex theory and YUV color space transformations, their performance is fundamentally limited by overlooking channel-specific degradation patterns and cross-channel interactions. To address this gap, we perform a frequency-domain analysis that confirms the superiority of the YUV color space for L3IE. We identify a key insight: the Y channel primarily loses low-frequency content, while the UV channels are corrupted by high-frequency noise. Leveraging this finding, we propose a novel YUV-based paradigm that strategically restores channels using a Dual-Stream Global-Local Attention module for the Y channel, a Y-guided Local-Aware Frequency Attention module for the UV channels, and a Guided Interaction module for final feature fusion. Extensive experiments validate that our model establishes a new state-of-the-art on multiple benchmarks, delivering superior visual quality with a significantly lower parameter count.

</details>


### [50] [NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields](https://arxiv.org/abs/2601.17350)
*Xianliang Huang,Zhizhou Zhong,Shuhang Chen,Yi Xu,Juhong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新型的神经渲染方法NeRF-MIR，专门用于恢复被遮挡（masked）的图像，在恢复损坏图像生成三维场景方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然NeRF在新视角合成有显著表现，但面对由于遮挡等原因而受损的图像时恢复效果有限。然而在自然场景数据采集时，图像损坏很常见，因此提升NeRF对损坏（尤其是遮挡）图像的处理能力十分重要。

Method: 1. 提出NeRF-MIR方法，专用于遮挡图像恢复；2. 提出基于Patch的熵驱动射线分布策略（PERE），更有效地学习图像纹理信息并多视图融合信息；3. 引入递进式自训练恢复机制（PIRE），逐步恢复被遮挡区域；4. 设计动态加权损失函数，自动调整遮挡区域损失权重；5. 构建三个人工遮挡数据集以模拟真实损坏情景。

Result: 在真实以及新构建的数据集上进行了大量实验，结果显示NeRF-MIR在遮挡图像恢复任务上超越了其他对比方法。

Conclusion: NeRF-MIR有效提升了NeRF对遮挡图像的三维恢复能力，各项创新策略均对提升性能有显著作用，为实际场景中损坏图像三维重建提供了有效方案。

Abstract: Neural Radiance Fields (NeRF) have demonstrated remarkable performance in novel view synthesis. However, there is much improvement room on restoring 3D scenes based on NeRF from corrupted images, which are common in natural scene captures and can significantly impact the effectiveness of NeRF. This paper introduces NeRF-MIR, a novel neural rendering approach specifically proposed for the restoration of masked images, demonstrating the potential of NeRF in this domain. Recognizing that randomly emitting rays to pixels in NeRF may not effectively learn intricate image textures, we propose a \textbf{P}atch-based \textbf{E}ntropy for \textbf{R}ay \textbf{E}mitting (\textbf{PERE}) strategy to distribute emitted rays properly. This enables NeRF-MIR to fuse comprehensive information from images of different views. Additionally, we introduce a \textbf{P}rogressively \textbf{I}terative \textbf{RE}storation (\textbf{PIRE}) mechanism to restore the masked regions in a self-training process. Furthermore, we design a dynamically-weighted loss function that automatically recalibrates the loss weights for masked regions. As existing datasets do not support NeRF-based masked image restoration, we construct three masked datasets to simulate corrupted scenarios. Extensive experiments on real data and constructed datasets demonstrate the superiority of NeRF-MIR over its counterparts in masked image restoration.

</details>


### [51] [HyDeMiC: A Deep Learning-based Mineral Classifier using Hyperspectral Data](https://arxiv.org/abs/2601.17352)
*M. L. Mamud,Piyoosh Jaysaval,Frederick D Day-Lewis,M. K. Mudunuru*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度学习的高光谱矿物分类模型HyDeMiC，能有效应对噪声数据，在实际遥感矿物探测中具有强大应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的高光谱矿物分类方法在面对环境噪声、传感器限制和高维数据分析时效果有限，因此亟需开发能在实际复杂环境下鲁棒的新方法。

Method: 作者开发了一种卷积神经网络（CNN）模型HyDeMiC，利用美国地质调查局（USGS）矿物库的115种矿物实验室高光谱数据进行训练，数据通过与传感器响应函数卷积合成不同噪声水平的数据集，并以三种含铜矿物（铜辉矿、孔雀石、黄铜矿）为案例评估性能。

Result: HyDeMiC模型在无噪声和低噪声（MCC=1.00）下实现了近乎完美的分类精度，在中等噪声条件下表现依然优秀，展示了较高的鲁棒性。

Conclusion: HyDeMiC在面对真实应用中普遍存在的噪声时依然保持了较高的分类性能，为高光谱遥感领域矿物识别提供了实用新工具。

Abstract: Hyperspectral imaging (HSI) has emerged as a powerful remote sensing tool for mineral exploration, capitalizing on unique spectral signatures of minerals. However, traditional classification methods such as discriminant analysis, logistic regression, and support vector machines often struggle with environmental noise in data, sensor limitations, and the computational complexity of analyzing high-dimensional HSI data. This study presents HyDeMiC (Hyperspectral Deep Learning-based Mineral Classifier), a convolutional neural network (CNN) model designed for robust mineral classification under noisy data. To train HyDeMiC, laboratory-measured hyperspectral data for 115 minerals spanning various mineral groups were used from the United States Geological Survey (USGS) library. The training dataset was generated by convolving reference mineral spectra with an HSI sensor response function. These datasets contained three copper-bearing minerals, Cuprite, Malachite, and Chalcopyrite, used as case studies for performance demonstration. The trained CNN model was evaluated on several synthetic 2D hyperspectral datasets with noise levels of 1%, 2%, 5%, and 10%. Our noisy data analysis aims to replicate realistic field conditions. The HyDeMiC's performance was assessed using the Matthews Correlation Coefficient (MCC), providing a comprehensive measure across different noise regimes. Results demonstrate that HyDeMiC achieved near-perfect classification accuracy (MCC = 1.00) on clean and low-noise datasets and maintained strong performance under moderate noise conditions. These findings emphasize HyDeMiC's robustness in the presence of moderate noise, highlighting its potential for real-world applications in hyperspectral imaging, where noise is often a significant challenge.

</details>


### [52] [PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling](https://arxiv.org/abs/2601.17354)
*Wenzhi Guo,Guangchi Fang,Shu Yang,Bing Wang*

Main category: cs.CV

TL;DR: PocketGS是一种专为移动设备设计的高效3D场景建模方法，在有限的训练时间和内存条件下，依然能实现高保真度的重建，且超过主流工作站方案。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting（3DGS）方法需要大量计算资源和长时间训练，难以在受限的移动设备上应用。如何在保证高建模质量的前提下，实现移动端高效、实时的3D场景重建，是亟需解决的问题。

Method: PocketGS通过三个协同设计的操作符来改良传统3DGS流程：（1）G操作符用于生成几何准确的点云先验；（2）I操作符注入局部表面统计信息，优化早期高斯分布的设定；（3）T操作符采用缓存与索引映射的梯度反向传播机制，提高移动端训练的稳定性和效率。这些操作符共同优化训练效率、内存占用和建模精度。

Result: 通过大量实验，PocketGS在移动端上实现了超越主流工作站3DGS的高质量重建结果，显示出卓越的训练速度、内存紧凑性以及模型还原精度。

Conclusion: PocketGS为移动设备3D场景重建提供了一种端到端、实用且高效的方法，使得3D捕捉到渲染流程可以完全在移动端设备上实时完成。

Abstract: Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.

</details>


### [53] [UCAD: Uncertainty-guided Contour-aware Displacement for semi-supervised medical image segmentation](https://arxiv.org/abs/2601.17366)
*Chengbo Ding,Fenghe Tang,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种基于不确定性引导的轮廓感知区域置换框架UCAD，用于提升半监督医学图像分割的效果，在有限标注下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有半监督分割中的区域置换方法通常仅操作矩形区域，导致解剖结构边界扭曲和语义不一致，无法有效利用未标注数据。

Method: UCAD框架利用超像素技术生成符合解剖结构的区域，以保持轮廓语义；引入不确定性引导的选择机制，优先对预测不确定且具有挑战性的区域进行置换以增强一致性学习；同时提出动态不确定性加权一致性损失，自适应地稳定训练并对未标注区域进行有效正则化。

Result: 大规模实验显示，UCAD方法在有限标注条件下的分割准确率显著高于现有的半监督分割先进方法。

Conclusion: UCAD在保持解剖轮廓一致性的同时提升了模型对未标注数据利用的有效性，是提升医学图像半监督分割性能的有效方案。

Abstract: Existing displacement strategies in semi-supervised segmentation only operate on rectangular regions, ignoring anatomical structures and resulting in boundary distortions and semantic inconsistency. To address these issues, we propose UCAD, an Uncertainty-Guided Contour-Aware Displacement framework for semi-supervised medical image segmentation that preserves contour-aware semantics while enhancing consistency learning. Our UCAD leverages superpixels to generate anatomically coherent regions aligned with anatomy boundaries, and an uncertainty-guided selection mechanism to selectively displace challenging regions for better consistency learning. We further propose a dynamic uncertainty-weighted consistency loss, which adaptively stabilizes training and effectively regularizes the model on unlabeled regions. Extensive experiments demonstrate that UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation. The code is available at:https://github.com/dcb937/UCAD.

</details>


### [54] [Physical Prompt Injection Attacks on Large Vision-Language Models](https://arxiv.org/abs/2601.17383)
*Chen Ling,Kai Hu,Hangcheng Liu,Xingshuo Han,Tianwei Zhang,Changhai Ou*

Main category: cs.CV

TL;DR: 本文提出了一种可将攻击性提示信息物理植入场景中的新型黑盒攻击方法，无需访问输入通道或先知用户查询，即可通过真实世界物体影响大视觉语言模型（LVLMs）的行为，实验证明该方法具有极高攻击成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs易受提示注入攻击，但传统攻击假设难以成立（如需访问输入、预知查询），因此急需一种面对真实世界应用场景的新型有效攻击方式，以更好评估与提升LVLMs安全性。

Method: 作者设计了一种物理提示注入攻击（PPIA），通过在物理环境物体上嵌入具有强语义引导作用的视觉提示，实现对LVLMs的黑盒操控。该方法结合离线高效选择可感知性与语义性兼优的提示、并基于时空注意力机制战略性选择植入位置，确保模型对其识别和受影响。

Result: 在10个主流LVLMs上于视觉问答、规划和导航等多任务中评估PPIA，覆盖仿真及真实场景，攻击成功率最高达98%，且在距离、视角、光照等物理条件变换下依然表现出极强鲁棒性。

Conclusion: PPIA展示了LVLMs在现实物理世界中面对黑盒物理注入攻击的高度脆弱性，提醒业界关注和防范物理世界中的安全风险，为后续模型安全性设计与防御提出了重要挑战。

Abstract: Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at https://github.com/2023cghacker/Physical-Prompt-Injection-Attack.

</details>


### [55] [ONRW: Optimizing inversion noise for high-quality and robust watermark](https://arxiv.org/abs/2601.17388)
*Xuan Ding,Xiu Yan,Chuanlong Xie,Yao Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的高质量、强鲁棒性水印嵌入方法，能够有效提升在各种图像损伤场景下的水印提取稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习水印方法在保证图像质量的同时，面对传输过程中的图像损伤（如噪声、压缩等）时鲁棒性不足，因此实际应用价值有限。需要新的方法提升水印在受损图像中的可恢复性。

Method: 该方法首先使用空文本优化过程将原始图像编码为潜在空间中的反演噪声，然后在潜在空间内优化反演噪声，通过扩散模型的迭代去噪过程生成高质量的水印图像。为防止反演噪声优化导致原有语义失真，引入了自注意力约束和伪掩码策略。

Result: 大量实验结果显示，该方法在COCO数据集上应对12种不同图像变换的情况下，平均性能优于稳定签名方法10%，在多种图像损伤情况下表现出了更好的稳健性和可识别性。

Conclusion: 基于扩散模型的水印方法既保证了图像质量，又显著提升了水印的鲁棒性，为知识产权保护提供了更为可靠和实用的技术支撑。

Abstract: Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\% across 12 different image transformations on COCO datasets. Our codes are available at https://github.com/920927/ONRW.

</details>


### [56] [SMV-EAR: Bring Spatiotemporal Multi-View Representation Learning into Efficient Event-Based Action Recognition](https://arxiv.org/abs/2601.17391)
*Rui Fan,Weidong Hao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的时空多视角表示学习方法，用于提升基于事件相机的人体动作识别，在准确率和效率方面都取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有事件相机对象识别方法在时空多视角表示上存在空间平移不变性差、特征融合方式简单的局限，不利于高效和鲁棒的动作识别。

Method: 本文提出三个关键创新：1）以平移不变的稠密方式对稀疏事件流进行空间-时间多视角表示；2）设计了双分支动态融合结构，实现不同视角运动特征按样本自适应的互补融合；3）提出受生物启发的时间扭曲增强方法，模拟现实动作速度变化，提高模型泛化能力。

Result: 在HARDVS、DailyDVS-200 和 THU-EACT-50-CHL 三个数据集上，Top-1准确率分别提升了7.0%、10.7%、10.2%；参数量减少30.1%、计算量减少35.7%。

Conclusion: 提出的方法在有效性和效率上全面超越现有SMVRL-EOR方法，为事件相机动作识别提供了新的有力范式。

Abstract: Event cameras action recognition (EAR) offers compelling privacy-protecting and efficiency advantages, where temporal motion dynamics is of great importance. Existing spatiotemporal multi-view representation learning (SMVRL) methods for event-based object recognition (EOR) offer promising solutions by projecting H-W-T events along spatial axis H and W, yet are limited by its translation-variant spatial binning representation and naive early concatenation fusion architecture. This paper reexamines the key SMVRL design stages for EAR and propose: (i) a principled spatiotemporal multi-view representation through translation-invariant dense conversion of sparse events, (ii) a dual-branch, dynamic fusion architecture that models sample-wise complementarity between motion features from different views, and (iii) a bio-inspired temporal warping augmentation that mimics speed variability of real-world human actions. On three challenging EAR datasets of HARDVS, DailyDVS-200 and THU-EACT-50-CHL, we show +7.0%, +10.7%, and +10.2% Top-1 accuracy gains over existing SMVRL EOR method with surprising 30.1% reduced parameters and 35.7% lower computations, establishing our framework as a novel and powerful EAR paradigm.

</details>


### [57] [ReLE: A Scalable System and Structured Benchmark for Diagnosing Capability Anisotropy in Chinese LLMs](https://arxiv.org/abs/2601.17399)
*Rui Fang,Jian Li,Wei Chen,Bin Hu,Ying-Cong Chen,Xin Tang,Liang Diao*

Main category: cs.CV

TL;DR: 该论文提出了一种名为ReLE的新型评测系统，用于高效诊断大语言模型在不同领域和能力维度上的非均匀性能。利用ReLE，作者对304个主流中文大模型进行了大规模、细粒度测评，并实现了评测精准度和效率的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有中文大模型评测面临基准测试饱和、计算成本昂贵等难题，且静态排行榜往往掩盖了模型在不同领域/能力的结构性差异。研究者希望找到一种系统性方法，更为高效和细致地揭示模型表现的真实结构。

Method: 提出了ReLE评测系统，基于领域×能力的正交测评矩阵，配合两项创新：（1）符号-语义混合评分机制，有效减少推理任务中的嵌入误判；（2）基于Neyman分配和噪声校正的动态方差调度器，大幅降低计算消耗。该系统测评了304个模型（商用和开源）、对207,843个样本进行评估。

Result: 相比传统的全量评测，ReLE可节省约70%的计算资源，并可维持极高排行榜相关性（0.96）。实验证明大模型在领域/能力间的排名波动更大（RSA=11.4），远高于传统基准（RSA≈5.0），显示主流模型实际上表现出强烈的专业化而非全能性。

Conclusion: ReLE可作为模型生态动态监控的工具，精准暴露模型在能力和领域上的结构性短板，为模型开发和选型提供依据，但并非用于替代全面静态基准。

Abstract: Large Language Models (LLMs) have achieved rapid progress in Chinese language understanding, yet accurately evaluating their capabilities remains challenged by benchmark saturation and prohibitive computational costs. While static leaderboards provide snapshot rankings, they often mask the structural trade-offs between capabilities. In this work, we present ReLE (Robust Efficient Live Evaluation), a scalable system designed to diagnose Capability Anisotropy, the non-uniformity of model performance across domains. Using ReLE, we evaluate 304 models (189 commercial, 115 open-source) across a Domain $\times$ Capability orthogonal matrix comprising 207,843 samples. We introduce two methodological contributions to address current evaluation pitfalls: (1) A Symbolic-Grounded Hybrid Scoring Mechanism that eliminates embedding-based false positives in reasoning tasks; (2) A Dynamic Variance-Aware Scheduler based on Neyman allocation with noise correction, which reduces compute costs by 70\% compared to full-pass evaluations while maintaining a ranking correlation of $ρ=0.96$. Our analysis reveals that aggregate rankings are highly sensitive to weighting schemes: models exhibit a Rank Stability Amplitude (RSA) of 11.4 in ReLE versus $\sim$5.0 in traditional benchmarks, confirming that modern models are highly specialized rather than generally superior. We position ReLE not as a replacement for comprehensive static benchmarks, but as a high-frequency diagnostic monitor for the evolving model landscape.

</details>


### [58] [HAAF: Hierarchical Adaptation and Alignment of Foundation Models for Few-Shot Pathology Anomaly Detection](https://arxiv.org/abs/2601.17405)
*Chunze Yang,Wenjie Zhao,Yue Tang,Junbo Lu,Jiusong Ge,Qidong Liu,Zeyu Gao,Chen Li*

Main category: cs.CV

TL;DR: 本文提出了一种新框架HAAF，解决了精细病理诊断中视觉-语言模型粒度不匹配的问题，通过跨层对齐提升ROI精细异常检测能力，并在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 病理诊断依赖于局部、细粒度的形态学异常检测，但现有视觉-语言模型泛化语义表达能力有限，对细微异常识别效果不佳。传统方法未能很好结合文本语义与具体图像ROI区域，影响诊断准确性。

Method: 提出分层适配与对齐框架（HAAF），包含一种新型跨层级缩放对齐机制（CLSA）：首先让视觉特征为文本提示注入局部上下文，再通过自适应文本反向指导视觉编码器关注关键异常区域；并设计了双分支推理策略，结合语义得分与几何原型，增强小样本场景下的稳定性。

Result: 在四个公开基准数据集测试，HAAF在低资源条件下显著优于现有方法，尤其在采用领域特定骨干网络（如CONCH）时表现突出。

Conclusion: HAAF能够有效解决视觉-语言模型在病理图像细粒度识别中的粒度不匹配问题，提升了精细区域检测能力，为低资源精密病理诊断提供了有力支持。

Abstract: Precision pathology relies on detecting fine-grained morphological abnormalities within specific Regions of Interest (ROIs), as these local, texture-rich cues - rather than global slide contexts - drive expert diagnostic reasoning. While Vision-Language (V-L) models promise data efficiency by leveraging semantic priors, adapting them faces a critical Granularity Mismatch, where generic representations fail to resolve such subtle defects. Current adaptation methods often treat modalities as independent streams, failing to ground semantic prompts in ROI-specific visual contexts. To bridge this gap, we propose the Hierarchical Adaptation and Alignment Framework (HAAF). At its core is a novel Cross-Level Scaled Alignment (CLSA) mechanism that enforces a sequential calibration order: visual features first inject context into text prompts to generate content-adaptive descriptors, which then spatially guide the visual encoder to spotlight anomalies. Additionally, a dual-branch inference strategy integrates semantic scores with geometric prototypes to ensure stability in few-shot settings. Experiments on four benchmarks show HAAF significantly outperforms state-of-the-art methods and effectively scales with domain-specific backbones (e.g., CONCH) in low-resource scenarios.

</details>


### [59] [Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity](https://arxiv.org/abs/2601.17408)
*Harsharaj Pathak,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: 本文提出了一种基于“邻域签名”的源数据不可用域自适应（SFDA）方法，通过学习更具信息性的聚类和减少噪声邻域影响，仅用一个定制损失函数即可实现有效的领域适应，特别是在VisDA等挑战性数据集上超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前SFDA方法大多依赖于邻域一致性，但容易受到错误邻域信息的影响，导致适应效果不佳。作者希望通过减轻噪声邻域的干扰，提高聚类代表性，以提升无源领域自适应的准确性。

Method: 作者引入了“邻域签名”这一概念，通过分析邻域信息学习到更精准、更具代表性的目标域聚类结构，并设计了一个能够同时优化样本预测相似度和差异性的损失函数，实现领域自适应。整个方法在无需源数据的情况下，仅依赖目标域数据自训练。

Result: 该方法在领域自适应经典的VisDA数据集上取得了优于现有方法的表现，同时在其他基准数据集上也达到了有竞争力的结果，表明方法的广泛适用性和有效性。

Conclusion: 利用邻域签名减少噪声影响、简化损失设计，能显著提升源不可用条件下的领域自适应效果，为相关领域问题提供了更鲁棒的新思路。

Abstract: Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.

</details>


### [60] [Cloud-Enabled IoT System for Real-Time Environmental Monitoring and Remote Device Control Using Firebase](https://arxiv.org/abs/2601.17414)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 本文提出了一种基于Firebase云平台的物联网远程监控与控制系统，通过ESP32微控制器和多种传感器实现环境数据的实时采集与设备操控，具有高可靠性、低延迟和成本低等优点。


<details>
  <summary>Details</summary>
Motivation: 传统的监控系统在实时数据获取、远程控制和云平台集成等方面存在局限，难以满足现代物联网应用对于多终端、跨地域同步和数据持久化的需求。

Method: 系统基于ESP32开发板，接入了DHT22温湿度传感器和HC-SR04超声波距离传感器，使用Firebase Realtime Database实现数据同步和多终端访问，并通过云端界面远程控制两组LED指示灯。实验对数据传输可靠性、控制延迟和数据持久化等指标进行了测试。

Result: 实验结果表明，数据传输成功率高达99.2%，远程控制时延不超过1.5秒，可以实现历史数据的持久存储。系统总成本为32.5美元，运行稳定可靠。

Conclusion: 基于Firebase的云端物联网系统为开发者和学者提供了无需复杂服务器配置、具备云端能力、成本低廉的应用框架，具备良好的可扩展性，适用于智能家居和工业监控等多种场景。

Abstract: The proliferation of Internet of Things (IoT) devices has created unprecedented opportunities for remote monitoring and control applications across various domains. Traditional monitoring systems often suffer from limitations in real-time data accessibility, remote controllability, and cloud integration. This paper presents a cloud-enabled IoT system that leverages Google's Firebase Realtime Database for synchronized environmental monitoring and device control. The system utilizes an ESP32 microcontroller to interface with a DHT22 temperature/humidity sensor and an HC-SR04 ultrasonic distance sensor, while enabling remote control of two LED indicators through a cloud-based interface. Real-time sensor data is transmitted to Firebase, providing a synchronized platform accessible from multiple devices simultaneously. Experimental results demonstrate reliable data transmission with 99.2\% success rate, real-time control latency under 1.5 seconds, and persistent data storage for historical analysis. The system architecture offers a scalable framework for various IoT applications, from smart home automation to industrial monitoring, with a total implementation cost of \$32.50. The integration of Firebase provides robust cloud capabilities without requiring complex server infrastructure, making advanced IoT applications accessible to developers and researchers with limited resources.

</details>


### [61] [CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction](https://arxiv.org/abs/2601.17420)
*Shiu-hong Kao,Chak Ho Huang,Huaiqian Liu,Yu-Wing Tai,Chi-Keung Tang*

Main category: cs.CV

TL;DR: 本文提出了CoT-Seg，一个无需训练即可实现推理分割的新框架，将链式思维和自我修正机制结合，显著提升了应对复杂和模糊分割问题的能力。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割方法在处理复杂查询和未知领域图像时表现不佳，难以像人类一样进行多步推理和自我修正。因此，作者希望探索一种更类人且高鲁棒性的分割机制。

Method: 提出了CoT-Seg，无需微调，通过对预训练多模态大模型（如GPT-4o）进行链式推理，将复杂查询拆解为子指令，并从图像中提取细粒度语义。此外，引入自我修正模块，模型可根据推理轨迹和查询自评结果并反复优化分割掩码，同时可集成外部检索增强推理模块。

Result: 在新引入的高难度分割数据集ReasonSeg-Hard以及复杂场景测试中，CoT-Seg展现出在处理模糊和具挑战性任务时的优越性能，显著超越现有推理分割方法。

Conclusion: 链式思维与自我修正的结合为视觉-语言分割提供了更强大的范式，CoT-Seg无需训练即可大幅提升复杂场景下的分割可靠性和鲁棒性。

Abstract: Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg's ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation.

</details>


### [62] [Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography](https://arxiv.org/abs/2601.17429)
*Mehdi Yousefzadeh,Siavash Shirzadeh Barough,Ashkan Fakharifar,Yashar Tayyarazad,Narges Eghbali,Mohaddeseh Mozaffari,Hoda Taeb,Negar Sadat Rafiee Tabatabaee,Parsa Esfahanian,Ghazaleh Sadeghi Gohar,Amineh Safavirad,Saeideh Mazloomzadeh,Ehsan khalilipur,Armin Elahifar,Majid Maleki*

Main category: cs.CV

TL;DR: 本论文针对X射线冠状动脉造影（XCA）中血管分割的鲁棒性问题，提出了一套结合传统方法和深度学习的血管分割与类型标注流程，有效提升了分割精度及跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: XCA是冠心病诊断的金标准，但实际应用中由于对比度低、运动伪影、器械干扰等导致血管分割难度大，进而影响后续定量分析和多中心应用。此领域亟需能够应对这些挑战的鲁棒血管分割方法。

Method: 作者采用从最佳造影帧选择、超分辨与图像增强、经典血管滤波器（Meijering、Frangi、Sato）与支持向量回归（SVR）参数预测、以及U-Net、FPN、Swin Transformer深度模型联合标注训练。同时进行血管类型（LAD、LCX、RCA）分配，内部和DCA1外部数据集上进行系统评测。

Result: SVR按图像参数优化提升了所有经典滤波分割精度（如Frangi Dice由0.741升至0.759）；FPN在冠状动脉分割Dice达0.914，联合管道标注进一步提升到0.931；外部测试Dice略降但轻量微调可恢复至0.88+；血管类型标注RCA/LAD/LCX分割Dice与准确率均超过95%。

Conclusion: 图像级参数预测增强了经典方法，FPN等深度模型结合多标注训练显著提升分割和跨域泛化能力。方案适用于常规XCA图像，为冠脉定量分析和异构中心推广提供了坚实基础。

Abstract: X-ray coronary angiography (XCA) is the clinical reference standard for assessing coronary artery disease, yet quantitative analysis is limited by the difficulty of robust vessel segmentation in routine data. Low contrast, motion, foreshortening, overlap, and catheter confounding degrade segmentation and contribute to domain shift across centers. Reliable segmentation, together with vessel-type labeling, enables vessel-specific coronary analytics and downstream measurements that depend on anatomical localization. From 670 cine sequences (407 subjects), we select a best frame near peak opacification using a low-intensity histogram criterion and apply joint super-resolution and enhancement. We benchmark classical Meijering, Frangi, and Sato vesselness filters under per-image oracle tuning, a single global mean setting, and per-image parameter prediction via Support Vector Regression (SVR). Neural baselines include U-Net, FPN, and a Swin Transformer, trained with coronary-only and merged coronary+catheter supervision. A second stage assigns vessel identity (LAD, LCX, RCA). External evaluation uses the public DCA1 cohort. SVR per-image tuning improves Dice over global means for all classical filters (e.g., Frangi: 0.759 vs. 0.741). Among deep models, FPN attains 0.914+/-0.007 Dice (coronary-only), and merged coronary+catheter labels further improve to 0.931+/-0.006. On DCA1 as a strict external test, Dice drops to 0.798 (coronary-only) and 0.814 (merged), while light in-domain fine-tuning recovers to 0.881+/-0.014 and 0.882+/-0.015. Vessel-type labeling achieves 98.5% accuracy (Dice 0.844) for RCA, 95.4% (0.786) for LAD, and 96.2% (0.794) for LCX. Learned per-image tuning strengthens classical pipelines, while high-resolution FPN models and merged-label supervision improve stability and external transfer with modest adaptation.

</details>


### [63] [ReflexSplit: Single Image Reflection Separation via Layer Fusion-Separation](https://arxiv.org/abs/2601.17468)
*Chia-Ming Lee,Yu-Fan Lin,Jing-Hui Jung,Yu-Jou Hsiao,Chih-Chung Hsu,Yu-Lun Liu*

Main category: cs.CV

TL;DR: ReflexSplit是一种新提出的单张图像反射分离方法，结合多尺度自适应融合、结构提取与分离模块，以及逐步训练策略，有效提升了分离质量与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有单张图像反射分离方法在非线性混合及深度解码阶段容易出现透射-反射混淆，主要原因为特征融合机制隐式且多尺度协调能力不足。

Method: 1）提出跨尺度门控融合模块（CrGF），对多层语义与解码上下文进行自适应聚合，保证梯度流畅与特征一致性；2）设计层融合-分离模块（LFSB），通过结构共享提取和层差分离实现有效分离，并借鉴Differential Transformer思想，引入跨流attention抵消；3）采用层次递进训练，通过初始化和训练策略逐层提升分离能力。

Result: 在合成及真实数据集上，ReflexSplit优于现有方法，表现出更高的分离精度、更好感知质量和更强泛化能力。

Conclusion: ReflexSplit通过创新性的网络结构和分离机制，显著提升了单张图像反射分离的效果，相关代码已开源。

Abstract: Single Image Reflection Separation (SIRS) disentangles mixed images into transmission and reflection layers. Existing methods suffer from transmission-reflection confusion under nonlinear mixing, particularly in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination. We propose ReflexSplit, a dual-stream framework with three key innovations. (1) Cross-scale Gated Fusion (CrGF) adaptively aggregates semantic priors, texture details, and decoder context across hierarchical depths, stabilizing gradient flow and maintaining feature consistency. (2) Layer Fusion-Separation Blocks (LFSB) alternate between fusion for shared structure extraction and differential separation for layer-specific disentanglement. Inspired by Differential Transformer, we extend attention cancellation to dual-stream separation via cross-stream subtraction. (3) Curriculum training progressively strengthens differential separation through depth-dependent initialization and epoch-wise warmup. Extensive experiments on synthetic and real-world benchmarks demonstrate state-of-the-art performance with superior perceptual quality and robust generalization. Our code is available at https://github.com/wuw2135/ReflexSplit.

</details>


### [64] [PhaSR: Generalized Image Shadow Removal with Physically Aligned Priors](https://arxiv.org/abs/2601.17470)
*Chia-Ming Lee,Yu-Fan Lin,Yu-Jou Hsiao,Jing-Hui Jung,Yu-Lun Liu,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: 本文提出了一种名为PhaSR的新方法，通过物理先验对齐，从单一光源到多光源环境下都能实现有效的阴影去除。方法在准确、泛化和计算效率方面优于传统手段。


<details>
  <summary>Details</summary>
Motivation: 在复杂多样的光照条件下实现阴影去除非常困难，核心挑战在于如何将光照影响和物体本身的反射率准确区分。现有方法在物理先验对齐不足时难以取得理想效果，尤其在多光源场景下易失效。

Method: PhaSR包含两个关键部分：1）物理对齐归一化（PAN）结合Gray-world归一化、log域Retinex分解和动态范围重组，抑制色彩偏置，实现封闭形式的光照修正；2）几何-语义矫正注意力（GSRA），将深度几何信息与DINO-v2语义嵌入进行跨模态对齐，缓解强/弱光照下的模态冲突。

Result: 实验表明，该方法在阴影去除方面表现出色，复杂度低，并对多源环境具备很强的泛化能力。在传统方法易失效的情形下依然表现稳定。

Conclusion: PhaSR能够有效解决多种光照条件下的阴影去除问题，特别是在多光源和复杂环境中具备优势，且模型较为轻量。

Abstract: Shadow removal under diverse lighting conditions requires disentangling illumination from intrinsic reflectance, a challenge compounded when physical priors are not properly aligned. We propose PhaSR (Physically Aligned Shadow Removal), addressing this through dual-level prior alignment to enable robust performance from single-light shadows to multi-source ambient lighting. First, Physically Aligned Normalization (PAN) performs closed-form illumination correction via Gray-world normalization, log-domain Retinex decomposition, and dynamic range recombination, suppressing chromatic bias. Second, Geometric-Semantic Rectification Attention (GSRA) extends differential attention to cross-modal alignment, harmonizing depth-derived geometry with DINO-v2 semantic embeddings to resolve modal conflicts under varying illumination. Experiments show competitive performance in shadow removal with lower complexity and generalization to ambient lighting where traditional methods fail under multi-source illumination. Our source code is available at https://github.com/ming053l/PhaSR.

</details>


### [65] [BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation](https://arxiv.org/abs/2601.17504)
*Yan Zhou,Zhen Huang,Yingqiu Li,Yue Ouyang,Suncheng Xiang,Zehua Wang*

Main category: cs.CV

TL;DR: 本文提出BMDS-Net模型，针对多模态MRI脑肿瘤分割，提高模型在实际临床中缺失模态时的稳健性和可信度，并且为每个体素提供不确定性估计，从而更好支持临床应用。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的分割模型，虽然在理想数据集上表现优异，但在实际临床常见的模态缺失情况下效果大幅下降，且模型对自身预测的可信度缺少量化，这影响了其临床实用性和安全性。

Method: BMDS-Net包括三大创新：1）采用具有Zero-Init的多模态上下文融合模块（MMCF）和残差门控深度解码监督（DDS），增强特征稳定性和边界分割能力，即使存在模态缺失也能降低Hausdorff距离；2）引入高效的贝叶斯微调策略，将网络转化为概率预测器，能够生成每体素的不确定性概率图，用以辅助临床决策；3）在BraTS 2021公开数据集上进行全面实验，验证其有效性。

Result: 在BraTS 2021数据集上，BMDS-Net在各种模态缺失的情境下，依然维持与当前最好模型相当的分割准确率，并在稳健性和可信度方面显著优于主流模型。

Conclusion: BMDS-Net不仅在分割精度上达到先进水平，更在面对实际临床常见问题时展现更高的鲁棒性与可信度，为MRI脑肿瘤自动分割的临床采纳提供了有力支撑。

Abstract: Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at https://github.com/RyanZhou168/BMDS-Net.

</details>


### [66] [FMIR, a foundation model-based Image Registration Framework for Robust Image Registration](https://arxiv.org/abs/2601.17529)
*Fengting Zhang,Yue He,Qinghao Liu,Yaonan Wang,Xiang Chen,Hang Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于大模型的医疗图像配准框架FMIR，实现了在训练域和非训练域上的稳健性能，突破了深度学习方法在泛化能力上的限制。


<details>
  <summary>Details</summary>
Motivation: 深度学习虽然提升了医疗图像配准速度，但其泛化能力差，难以适应新领域数据，这是由于医学训练数据有限所致。

Method: FMIR结合了基础模型的特征编码器来提取解剖结构信息，并使用通道正则化策略训练配准头，只需在单一数据集上训练即可。

Result: FMIR在训练数据集（in-domain）上取得了当前最优的配准性能，同时在外部数据集（out-of-domain）上的结果也非常稳健。

Conclusion: 该方法为在有限资源下构建具有泛化能力的医疗影像基础模型提供了可行方案。

Abstract: Deep learning has revolutionized medical image registration by achieving unprecedented speeds, yet its clinical application is hindered by a limited ability to generalize beyond the training domain, a critical weakness given the typically small scale of medical datasets. In this paper, we introduce FMIR, a foundation model-based registration framework that overcomes this limitation.Combining a foundation model-based feature encoder for extracting anatomical structures with a general registration head, and trained with a channel regularization strategy on just a single dataset, FMIR achieves state-of-the-art(SOTA) in-domain performance while maintaining robust registration on out-of-domain images.Our approach demonstrates a viable path toward building generalizable medical imaging foundation models with limited resources. The code is available at https://github.com/Monday0328/FMIR.git.

</details>


### [67] [Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries](https://arxiv.org/abs/2601.17535)
*Kevin Robbins,Xiaotong Liu,Yu Wu,Le Sun,Grady McPeak,Abby Stylianou,Robert Pless*

Main category: cs.CV

TL;DR: 该论文提出，通过生成与任务相关的合成图像，同时结合文本，用于评估和改进视觉-语言模型（VLMs）在新任务上的零样本准确率预测。


<details>
  <summary>Details</summary>
Motivation: 虽然VLM如CLIP让用户只需命名类别即可搭建视觉分类器，但这些模型在新领域中效果难以判定，尤其普通用户缺乏评估工具。现有仅基于文本的评估方法存在局限。

Method: 作者在文本比较基础上，探索了结合合成图像生成，模拟新任务类别样本，再以文本和图像联合方式，对VLM在零样本任务上的表现进行评估和预测。

Result: 实验表明，合成图像与文本结合能有效提升对VLM零样本精度的预测质量，并让用户了解评估所参考的图像类型。

Conclusion: 提出的基于图像的评估框架能在无标注数据的情况下，帮助用户判断VLM在特定应用中的有效性，改进了模型适用性的可解释评估。

Abstract: Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.

</details>


### [68] [OTI: A Model-free and Visually Interpretable Measure of Image Attackability](https://arxiv.org/abs/2601.17536)
*Jiaming Liang,Haowei Liu,Chi-Man Pun*

Main category: cs.CV

TL;DR: 该论文提出了一种全新的图像易攻击性度量方法——对象纹理强度（OTI），无需依赖特定模型，且具备可视化解释能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像易攻击性度量方法依赖于特定模型和不可视化的特征，且很多实际任务模型不可用，限制了方法的推广应用。因此亟需一种无模型、具备直观解释性的图像易攻击性度量方式。

Method: 提出了一种新的图像易攻击性衡量指标——对象纹理强度（OTI），它基于图像中语义对象的纹理强度进行度量，无需依赖模型。本方法理论上结合决策边界和对抗扰动的频率特性进行解释。

Result: 大量实验结果表明，OTI能够高效、有效地衡量图像的易攻击性，并展示了其在多个实际应用任务中优越的性能。

Conclusion: OTI方法不仅实现了无需模型的图像易攻击性度量，还提供了直观的可视化解释，有助于对抗机器学习领域深入理解和利用图像易攻击性这一重要属性。

Abstract: Despite the tremendous success of neural networks, benign images can be corrupted by adversarial perturbations to deceive these models. Intriguingly, images differ in their attackability. Specifically, given an attack configuration, some images are easily corrupted, whereas others are more resistant. Evaluating image attackability has important applications in active learning, adversarial training, and attack enhancement. This prompts a growing interest in developing attackability measures. However, existing methods are scarce and suffer from two major limitations: (1) They rely on a model proxy to provide prior knowledge (e.g., gradients or minimal perturbation) to extract model-dependent image features. Unfortunately, in practice, many task-specific models are not readily accessible. (2) Extracted features characterizing image attackability lack visual interpretability, obscuring their direct relationship with the images. To address these, we propose a novel Object Texture Intensity (OTI), a model-free and visually interpretable measure of image attackability, which measures image attackability as the texture intensity of the image's semantic object. Theoretically, we describe the principles of OTI from the perspectives of decision boundaries as well as the mid- and high-frequency characteristics of adversarial perturbations. Comprehensive experiments demonstrate that OTI is effective and computationally efficient. In addition, our OTI provides the adversarial machine learning community with a visual understanding of attackability.

</details>


### [69] [Saliency Driven Imagery Preprocessing for Efficient Compression -- Industrial Paper](https://arxiv.org/abs/2601.17555)
*Justin Downes,Sam Saltwick,Anthony Chen*

Main category: cs.CV

TL;DR: 本文提出了一种结合显著性图和传统有损压缩标准的方法，实现了卫星图像内部不同区域的自适应压缩，有效兼顾了压缩效率和对感兴趣区域的高保真度。


<details>
  <summary>Details</summary>
Motivation: 每天都会产生海量卫星图像数据，这带来了巨大的存储和带宽成本。虽然图像分辨率不断提升，但下游应用通常只关注图像中的小部分区域，因此有必要优化这些感兴趣区域的信息编码方式。

Method: 作者利用显著性图标记出感兴趣区域，并根据显著性级别选择不同大小的平滑核对图像进行预处理。然后，将处理后的图像采用传统的有损压缩编码，实现同一幅大图像内的可变压缩率，对重要区域进行低压缩、非重要区域高压缩。

Result: 结果表明，这种基于显著性图的预处理方法结合传统压缩标准，能够有效提高关键区域的图像质量，并提升整体压缩效率。

Conclusion: 基于显著性引导的自适应预处理，可以为卫星图像的存储和传输提供更优的压缩方案，特别适用于只需关注部分区域的遥感应用场景。

Abstract: The compression of satellite imagery remains an important research area as hundreds of terabytes of images are collected every day, which drives up storage and bandwidth costs. Although progress has been made in increasing the resolution of these satellite images, many downstream tasks are only interested in small regions of any given image. These areas of interest vary by task but, once known, can be used to optimize how information within the image is encoded. Whereas standard image encoding methods, even those optimized for remote sensing, work on the whole image equally, there are emerging methods that can be guided by saliency maps to focus on important areas. In this work we show how imagery preprocessing techniques driven by saliency maps can be used with traditional lossy compression coding standards to create variable rate image compression within a single large satellite image. Specifically, we use variable sized smoothing kernels that map to different quantized saliency levels to process imagery pixels in order to optimize downstream compression and encoding schemes.

</details>


### [70] [Sponge Tool Attack: Stealthy Denial-of-Efficiency against Tool-Augmented Agentic Reasoning](https://arxiv.org/abs/2601.17566)
*Qi Li,Xinchao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新型攻击方法——Sponge Tool Attack (STA)，能够通过只修改输入提示词，使基于工具增强的大语言模型在推理时变得冗长复杂，增加计算开销、同时不改变任务语义结果，实验验证该攻击泛化性强、对现有系统造成明显影响。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型通过调用外部工具实现更复杂的智能推理，但这种工具增强方法潜在的安全薄弱环节尚未受到充分关注，特别是输入与工具调用流程可能被恶意操控。

Method: STA攻击方法在只拥有查询权限且不改变模型或工具本身的前提下，通过多轮多智能体协作重写输入，将精简高效的推理转换为多步骤、冗余的推理流程，并且保持输出结果与原意一致。整个框架对被攻击者无侵入、具备高度隐蔽性。

Result: 作者在6个不同模型、12个工具、4个agentic框架及13个跨5大领域的数据集上进行了大量实验，结果显示STA能大幅增加推理所需计算资源且具有良好的通用性，难以被察觉，同时不影响原始任务意图。

Conclusion: 工具增强大模型存在输入操控的新攻击面，STA作为一种可行、难以察觉的隐蔽攻击方法，暴露了智能代理系统在推理效率和安全性上的重要隐患。

Abstract: Enabling large language models (LLMs) to solve complex reasoning tasks is a key step toward artificial general intelligence. Recent work augments LLMs with external tools to enable agentic reasoning, achieving high utility and efficiency in a plug-and-play manner. However, the inherent vulnerabilities of such methods to malicious manipulation of the tool-calling process remain largely unexplored. In this work, we identify a tool-specific attack surface and propose Sponge Tool Attack (STA), which disrupts agentic reasoning solely by rewriting the input prompt under a strict query-only access assumption. Without any modification on the underlying model or the external tools, STA converts originally concise and efficient reasoning trajectories into unnecessarily verbose and convoluted ones before arriving at the final answer. This results in substantial computational overhead while remaining stealthy by preserving the original task semantics and user intent. To achieve this, we design STA as an iterative, multi-agent collaborative framework with explicit rewritten policy control, and generates benign-looking prompt rewrites from the original one with high semantic fidelity. Extensive experiments across 6 models (including both open-source models and closed-source APIs), 12 tools, 4 agentic frameworks, and 13 datasets spanning 5 domains validate the effectiveness of STA.

</details>


### [71] [Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization](https://arxiv.org/abs/2601.17586)
*Sebastian Doerrich,Francesco Di Salvo,Jonas Alle,Christian Ledig*

Main category: cs.CV

TL;DR: 本文提出了一种针对医学图像领域泛化能力不足问题的新型Vision Transformer模型（Stylizing ViT），通过共享权重的注意力机制实现风格迁移和解剖一致性，极大提升了图像分类的稳健性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在医学图像分析中受限于数据异质性和稀缺性，难以跨域和跨人群泛化。尽管传统扩增和风格化增强有所帮助，但它们往往不能很好地应对大幅度域转移，或会引入伪影、风格多样性不足。急需新的方法提升模型的域泛化性能。

Method: 提出了一种新颖的Vision Transformer编码器（Stylizing ViT），采用权重共享的自注意力与交叉注意力模块，既保障图像解剖结构一致，又能完成多样风格迁移，实现训练过程和推理过程的动态图像风格增强。在三个医学图像分类任务中，对比实验并开源代码。

Result: 在组织病理学和皮肤病学的三类分类任务中，使用该模型进行扩增可以使准确率相比现有最佳方法提升至多13%；测试时利用该方法做推理期扩增还能提升17%的性能，生成的图像感官上无伪影。

Conclusion: 所提方法有效提升了医学图像分类的泛化能力和稳健性，不仅扩大了风格多样性，还减少了伪影生成，对训练和推理均有显著益处。

Abstract: Deep learning models in medical image analysis often struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation improves robustness, but fails under substantial domain shifts. Recent advances in stylistic augmentation enhance domain generalization by varying image styles but fall short in terms of style diversity or by introducing artifacts into the generated images. To address these limitations, we propose Stylizing ViT, a novel Vision Transformer encoder that utilizes weight-shared attention blocks for both self- and cross-attention. This design allows the same attention block to maintain anatomical consistency through self-attention while performing style transfer via cross-attention. We assess the effectiveness of our method for domain generalization by employing it for data augmentation on three distinct image classification tasks in the context of histopathology and dermatology. Results demonstrate an improved robustness (up to +13% accuracy) over the state of the art while generating perceptually convincing images without artifacts. Additionally, we show that Stylizing ViT is effective beyond training, achieving a 17% performance improvement during inference when used for test-time augmentation. The source code is available at https://github.com/sdoerrich97/stylizing-vit .

</details>


### [72] [SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation](https://arxiv.org/abs/2601.17657)
*Taewan Cho,Taeryang Kim,Andrew Jaeyong Choi*

Main category: cs.CV

TL;DR: SPACE-CLIP提出了一种全新的方法，直接从冻结的CLIP视觉编码器中提取并融合语义和几何信息，显著提升了空间感知能力，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的CLIP虽能理解语义，但对几何结构感知不足。现有解决方案依靠文本提示，方式间接且效率低下。

Method: 提出SPACE-CLIP模型，设计了具有语义通路和结构通路的双通路解码结构，分别提取高层语义与精细几何特征，并采用分层融合方式实现信息综合，无需通过文本编码器或提示。

Result: 在KITTI基准测试上，SPACE-CLIP大幅超越以往基于CLIP的方法。消融实验证实双通路融合对提升性能至关重要。

Conclusion: SPACE-CLIP为大规模视觉模型的空间感知任务提供了高效且优雅的架构模板，可集成到下一代具身AI系统（如视觉-语言-动作模型）中。

Abstract: Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip

</details>


### [73] [Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting](https://arxiv.org/abs/2601.17666)
*Xinyue Pan,Yuhao Chen,Fengqing Zhu*

Main category: cs.CV

TL;DR: 提出了一种新方法（Prompt Grafting, PG），用以解决文生图模型在生成多食物图片时食物“融合”问题，通过显式的空间提示和布局引导，实现多食物项的准确分离和可控混合。


<details>
  <summary>Details</summary>
Motivation: 传统的文生图扩散模型在生成多食物图片时，容易出现食物间相互融合（如米饭和汤混合），影响图片的真实性和可控性。而高质量的多食物图片对饮食评估、数据增强以及菜谱可视化等应用很重要。因此，亟需一种有效的方法控制、分离或融合多食物项。

Method: 提出了Prompt Grafting（PG）框架，无需额外训练。该方法分为两阶段：第一阶段利用布局提示生成不同食物区域；第二阶段在布局稳定后，将目标文本提示融合，实现指定食物在指定区域的分布。用户可通过编辑布局实现分离或有意融合食物。

Result: 在两个食物图像数据集上，PG方法显著提高了目标食物项的准确分离率，获得更好的定性效果，并能实现用户可控的食物分离或混合。

Conclusion: Prompt Grafting为多食物场景中的图像生成提供了有效的可控分离解决方案，对需要多食物数据增强或精细控制图片内容的任务具有实际意义。

Abstract: Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.

</details>


### [74] [Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing](https://arxiv.org/abs/2601.17673)
*Weiyu Zhang,Yuan Hu,Yong Li,Yu Liu*

Main category: cs.CV

TL;DR: 本文针对遥感多模态模型在理解和生成任务中的空间关系表现不一致（空间反转诅咒）问题，提出了Uni-RS模型，通过空间布局规划、空间感知监督和空间变换增强，实现了更高的生成空间准确性和多模态理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有统一的遥感多模态模型虽然能准确理解图像物体间的位置关系，但在文本生成图像时，常常无法忠实还原这些空间关系，这对遥感语义极为关键。为解决在理解与生成任务中的空间关系不对称问题，作者提出针对性的改进。

Method: 1. 空间布局规划：将文本指令转成明确的空间布局计划，实现几何规划和视觉合成解耦。2. 空间感知查询监督：显式引导可学习的queries关注指令中的空间关系。3. 图文空间变换增强：通过一致性的空间变换，提升模型对空间变化的泛化能力。

Result: 在多个遥感多模态数据集和任务上，Uni-RS方法在文本生成图像的空间准确性上有显著提升，同时在图像描述、视觉定位和视觉问答等多模态理解任务上也保持优异表现。

Conclusion: Uni-RS有效解决了遥感多模态模型在生成任务中的空间关系失真问题，实现了理解和生成的空间一致性，为遥感多模态任务的研究提供了新思路。

Abstract: Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.

</details>


### [75] [StyleDecoupler: Generalizable Artistic Style Disentanglement](https://arxiv.org/abs/2601.17697)
*Zexi Jia,Jinchao Zhang,Jie Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一种名为StyleDecoupler的信息论框架，有效实现了艺术风格与内容的解耦，并发布了大规模艺术风格数据集WeART。方法在风格检索等任务上取得了最新效果。


<details>
  <summary>Details</summary>
Motivation: 艺术风格和语义内容高度纠缠，现有模型难以有效分离两者，导致风格识别和相关任务受限。作者希望开发出能够独立提取艺术风格特征的技术。

Method: 利用多模态视觉模型（包含风格和内容信息）与单模态模型（倾向于仅编码内容）表达的差异，提出通过最小化互信息来分离纯粹的风格特征。StyleDecoupler作为即插即用模块，无需对冻结的视觉-语言模型进行微调。

Result: 在WeART和WikiART两个艺术风格基准数据集上，StyleDecoupler在风格检索任务上表现出最先进的性能。同时支持风格关系映射和生成模型评估等应用。

Conclusion: StyleDecoupler为风格与内容的解耦提供了新思路，推动了艺术风格分析方法的发展。此外，大规模数据集WeART的发布有助于后续相关研究。

Abstract: Representing artistic style is challenging due to its deep entanglement with semantic content. We propose StyleDecoupler, an information-theoretic framework that leverages a key insight: multi-modal vision models encode both style and content, while uni-modal models suppress style to focus on content-invariant features. By using uni-modal representations as content-only references, we isolate pure style features from multi-modal embeddings through mutual information minimization. StyleDecoupler operates as a plug-and-play module on frozen Vision-Language Models without fine-tuning. We also introduce WeART, a large-scale benchmark of 280K artworks across 152 styles and 1,556 artists. Experiments show state-of-the-art performance on style retrieval across WeART and WikiART, while enabling applications like style relationship mapping and generative model evaluation. We release our method and dataset at this url.

</details>


### [76] [An AI-enabled tool for quantifying overlapping red blood cell sickling dynamics in microfluidic assays](https://arxiv.org/abs/2601.17703)
*Nikhil Kadivar,Guansheng Li,Jianlu Zheng,John M. Higgins,Ming Dao,George Em Karniadakis,Mengjia Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种自动化的深度学习框架，实现了对镰状细胞动态变化的高效识别和定量，尤其适用于高密度和重叠细胞的场景。该方法利用少量人工标注数据，通过结合AI辅助的标注、分割、分类和计数算法，极大提升了时序显微图像中红细胞的分析效率。


<details>
  <summary>Details</summary>
Motivation: 分析和理解镰状细胞在不同生物物理条件下的形态变化对于疾病机理研究和药物疗效评估十分关键，然而高密度和细胞重叠极大增加了自动化定量的难度。

Method: 采用Roboflow平台进行人工标注生成训练数据，用nnU-Net深度学习模型进行细胞分割，同时引入分水岭算法解决细胞重叠问题，并实现时间序列的细胞分类与计数。整套流程高度自动化，对标注数据需求低。

Result: 该框架在仅需少量标注数据的情况下，实现了高精度的细胞分割和定量。能够有效追踪红细胞动态变化，超过2倍提升实验通量，且可分辨药物作用下的细胞形态演化特征。

Conclusion: 提出的AI驱动自动化分析平台不仅提升了大规模红细胞动力学研究效率，还为细胞力学生物机制解析与药物评价提供了可扩展、可复现的计算工具。

Abstract: Understanding sickle cell dynamics requires accurate identification of morphological transitions under diverse biophysical conditions, particularly in densely packed and overlapping cell populations. Here, we present an automated deep learning framework that integrates AI-assisted annotation, segmentation, classification, and instance counting to quantify red blood cell (RBC) populations across varying density regimes in time-lapse microscopy data. Experimental images were annotated using the Roboflow platform to generate labeled dataset for training an nnU-Net segmentation model. The trained network enables prediction of the temporal evolution of the sickle cell fraction, while a watershed algorithm resolves overlapping cells to enhance quantification accuracy. Despite requiring only a limited amount of labeled data for training, the framework achieves high segmentation performance, effectively addressing challenges associated with scarce manual annotations and cell overlap. By quantitatively tracking dynamic changes in RBC morphology, this approach can more than double the experimental throughput via densely packed cell suspensions, capture drug-dependent sickling behavior, and reveal distinct mechanobiological signatures of cellular morphological evolution. Overall, this AI-driven framework establishes a scalable and reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems.

</details>


### [77] [Advancing Structured Priors for Sparse-Voxel Surface Reconstruction](https://arxiv.org/abs/2601.17720)
*Ting-Hsun Chi,Chu-Rong Chen,Chi-Tun Hsu,Hsuan-Ting Lin,Sheng-Yu Huang,Cheng Sun,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合3D高斯溅射和稀疏体素光栅化优点的方法，通过新颖的体素初始化技术和深度几何监督，提升了辐射场重建表面精度、细节恢复与收敛速度。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯溅射与稀疏体素光栅化各有优缺点：前者收敛快但细节有限，后者几何清晰但初始化代价高、收敛慢。需要一种能够整合两者优点的方法，以提升表面重建的精度和效率。

Method: 方法包括两部分：一是提出体素初始化技术，通过将体素智能分布在合理位置并匹配合适细节层级，优化稀疏体素的起始状态，便于后续优化；二是提出细化的深度几何监督，将多视角信息转换为逐射线的深度正则，提高深度一致性且不模糊边界。

Result: 实验证明，在标准数据集上，该方法在几何准确性、细节恢复和表面完整性方面均优于现有方法，同时收敛速度也得以保持快速。

Conclusion: 通过融合3D高斯溅射与稀疏体素光栅化的优势，并引入有效的初始化与监督手段，能实现更高精度和效率的辐射场表面重建。

Abstract: Reconstructing accurate surfaces with radiance fields has progressed rapidly, yet two promising explicit representations, 3D Gaussian Splatting and sparse-voxel rasterization, exhibit complementary strengths and weaknesses. 3D Gaussian Splatting converges quickly and carries useful geometric priors, but surface fidelity is limited by its point-like parameterization. Sparse-voxel rasterization provides continuous opacity fields and crisp geometry, but its typical uniform dense-grid initialization slows convergence and underutilizes scene structure. We combine the advantages of both by introducing a voxel initialization method that places voxels at plausible locations and with appropriate levels of detail, yielding a strong starting point for per-scene optimization. To further enhance depth consistency without blurring edges, we propose refined depth geometry supervision that converts multi-view cues into direct per-ray depth regularization. Experiments on standard benchmarks demonstrate improvements over prior methods in geometric accuracy, better fine-structure recovery, and more complete surfaces, while maintaining fast convergence.

</details>


### [78] [Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study](https://arxiv.org/abs/2601.17723)
*Tayyab Nasir,Daochang Liu,Ajmal Mian*

Main category: cs.CV

TL;DR: 本文对隐式神经表示（INR）方法在任意尺度图像超分辨（ASSR）任务上的有效性进行了系统实证分析，提出了统一的评测框架，并探索了训练配置、损失设计等因素对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 目前INR已成为ASSR任务的主流方法，但缺乏对现有方法系统性的实证对比和对训练策略（如缩放法则、目标设计、优化策略等）影响的研究，因此需要系统评估和基准标准来明确方法有效性，揭示瓶颈与发展方向。

Method: 作者设计了统一的评测框架和代码库，汇总并对比了多种INR方法 under 各种设置下，在多个图像质量指标上聚合性能，重点研究了训练配置对结果的影响，以及一种新型损失函数（惩罚强度变化、但保留边缘和纹理细节）的效果。

Result: 实验发现：（1）新型复杂的INR方法相较早期方法提升有限；（2）模型性能与训练配置强相关，这一点被以往忽视；（3）提出的损失函数能有效提升纹理重建，对感知质量有积极影响；（4）INR方法同样服从缩放法则，模型复杂度和数据多样性提升带来可预测的性能增益。

Conclusion: 系统实证表明，未来工作需更关注训练配置和损失设计，而不是仅追求更复杂模型结构。同时，统一基准和代码库有助于社区公正评测。INR模型虽然有进步空间，但提升已趋于瓶颈，突破需新思路。

Abstract: Implicit neural representation (INR) has become the standard approach for arbitrary-scale image super-resolution (ASSR). To date, no empirical study has systematically examined the effectiveness of existing methods, nor investigated the effects of different training recipes, such as scaling laws, objective design, and optimization strategies. A rigorous empirical analysis is essential not only for benchmarking performance and revealing true gains but also for establishing the current state of ASSR, identifying saturation limits, and highlighting promising directions. We fill this gap by comparing existing techniques across diverse settings and presenting aggregated performance results on multiple image quality metrics. We contribute a unified framework and code repository to facilitate reproducible comparisons. Furthermore, we investigate the impact of carefully controlled training configurations on perceptual image quality and examine a new loss function that penalizes intensity variations while preserving edges, textures, and finer details during training. We conclude the following key insights that have been previously overlooked: (1) Recent, more complex INR methods provide only marginal improvements over earlier methods. (2) Model performance is strongly correlated to training configurations, a factor overlooked in prior works. (3) The proposed loss enhances texture fidelity across architectures, emphasizing the role of objective design for targeted perceptual gains. (4) Scaling laws apply to INR-based ASSR, confirming predictable gains with increased model complexity and data diversity.

</details>


### [79] [Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles](https://arxiv.org/abs/2601.17733)
*Junran Lu,Yuanqi Li,Hengji Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 本文提出了一种全新的B-Rep（边界表示）生成建模方法，将B-Rep表述为由不同维度几何单元（k-cell）组成的粒子集合，通过粒子共享实现更紧密的几何耦合，大幅提升了CAD模型的生成质量和编辑性。


<details>
  <summary>Details</summary>
Motivation: 现有B-Rep生成方法因其层级结构复杂、几何关系耦合不足，无法充分捕捉不同维度单元间的上下文信息，导致模型生成灵活性、鲁棒性和编辑性受限。因此亟需一种新表述方式，能统一处理拓扑与几何信息，并提升生成模型的通用性。

Method: 作者将传统B-Rep分解为一组可组合的k-cell粒子，每个拓扑单元由多个粒子组成，相邻单元在界面共享相同潜变量，加强局部几何约束。通过多模态流匹配框架对粒子集合进行建模，实现无条件生成和条件任务（如单视图/点云重建）。该表述支持局部修补、非流形结构（如线框）等下游任务。

Result: 实验表明，该方法生成的CAD模型在准确性、有效性和可编辑性方面均优于现有最先进方法，能高保真地还原和生成复杂结构。

Conclusion: 新方法突破了B-Rep生成领域的层级耦合弊端，统一了拓扑与几何建模，提升了生成模型对CAD数据的适用性，并为非流形和局部编辑等实际工程应用提供了直接解决方案。

Abstract: Boundary Representation (B-Rep) is the widely adopted standard
  in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness.
  We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods.

</details>


### [80] [The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation](https://arxiv.org/abs/2601.17737)
*Chenyu Mu,Xin He,Qu Yang,Wanshun Chen,Jiadi Yao,Huang Liu,Zihao Yi,Bo Zhao,Xingyu Chen,Ruotian Ma,Fanghua Ye,Erkun Yang,Cheng Deng,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CV

TL;DR: 该论文提出了一种端到端的智能体框架，实现从对话文本直接生成连贯的电影级视频，显著提升了视频与脚本的一致性与时序连贯性。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成视频模型虽然能够生成视觉震撼的短视频，但面对长篇对话和复杂叙事时，往往难以把创意想法转化为连贯的电影叙事，存在“语义鸿沟”。该论文旨在解决如何用对话等高层次表达生成长篇、连贯并忠于剧本的视频内容。

Method: 提出创新的智能体式框架，包括两个核心模块：ScripterAgent（将粗粒度对话转化为可执行的细粒度电影脚本）和DirectorAgent（依据生成的脚本指导多场景视频连续生成，保障长时序连贯），同时构建了大规模测评基准ScriptBench以及配套的AI评分员CriticAgent和视觉-脚本对齐（VSA）新指标。

Result: 实验证明该框架在各种主流视频生成模型上，均可明显提升视频对于剧本的忠实度和时间连贯性，AI测评与VSA指标均优于以往方案。

Conclusion: 该框架有效弥合了对话与电影视频生成之间的语义鸿沟，实现了更自动化、更高质量的影视内容生成，并揭示了当前模型在观感和脚本忠实之间的权衡，对未来自动化影视制作具有重要参考意义。

Abstract: Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.

</details>


### [81] [Learning Sewing Patterns via Latent Flow Matching of Implicit Fields](https://arxiv.org/abs/2601.17740)
*Cong Cao,Ren Li,Corentin Dumery,Hao Li*

Main category: cs.CV

TL;DR: 本论文提出了一种基于隐式表示的缝纫纸样建模方法，可准确生成和建模复杂服装结构的纸样。


<details>
  <summary>Details</summary>
Motivation: 缝纫纸样是服装设计和制造的基础，但由于面板几何形状和缝合方式的多样性，自动化、精确地建模纸样存在挑战。

Method: 作者采用隐式表示，通过有符号与无符号距离场描述纸样的面板边界和缝合端点，再将这些距离场编码进连续潜空间，实现可微分网格化；同时，利用潜流匹配模型学习面板组合的分布，并通过缝合预测模块从边缘段恢复缝合关系。

Result: 所提方法可准确建模并生成具有复杂结构的缝纫纸样，相比已有方法，能更精确地从图片估算纸样，并支持纸样补全和重新适配等应用。

Conclusion: 提出的模型为数字化服装设计提供了实用工具，提高了纸样建模的准确性，并拓展了相关应用领域。

Abstract: Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design.

</details>


### [82] [Frequency-aware Neural Representation for Videos](https://arxiv.org/abs/2601.17741)
*Jun Zhu,Xinfeng Zhang,Lv Tang,Junhao Jiang,Gai Zhang,Jia Wang*

Main category: cs.CV

TL;DR: 本文提出了一种频率感知神经表达（FaNeRV）方法，用于提升基于隐式神经表达的视频压缩质量，兼顾低频与高频细节，有效解决传统方法过度平滑的问题。


<details>
  <summary>Details</summary>
Motivation: 现有INR（隐式神经表达）视频压缩方法存在频谱偏置，导致高频细节丢失和重建平滑；需要新方法以更好兼顾全局与细节信息。

Method: FaNeRV方法将视频分解为低频和高频分量，采用多分辨率分阶段监督策略；引入动态高频注入机制，重点提升难以还原的高频区域；并设计了频率解耦网络模块加强不同频谱带特征的建模能力。

Result: 在标准测试集上的实验显示，FaNeRV显著优于现有最先进的INR方法，同时在码率失真（rate-distortion）表现上也具备与传统编解码技术竞争的能力。

Conclusion: FaNeRV方法有效解决了INR视频压缩中的频谱偏置，实现了更高的重建保真度和压缩效率，对相关领域具有较大实际意义。

Abstract: Implicit Neural Representations (INRs) have emerged as a promising paradigm for video compression. However, existing INR-based frameworks typically suffer from inherent spectral bias, which favors low-frequency components and leads to over-smoothed reconstructions and suboptimal rate-distortion performance. In this paper, we propose FaNeRV, a Frequency-aware Neural Representation for videos, which explicitly decouples low- and high-frequency components to enable efficient and faithful video reconstruction. FaNeRV introduces a multi-resolution supervision strategy that guides the network to progressively capture global structures and fine-grained textures through staged supervision . To further enhance high-frequency reconstruction, we propose a dynamic high-frequency injection mechanism that adaptively emphasizes challenging regions. In addition, we design a frequency-decomposed network module to improve feature modeling across different spectral bands. Extensive experiments on standard benchmarks demonstrate that FaNeRV significantly outperforms state-of-the-art INR methods and achieves competitive rate-distortion performance against traditional codecs.

</details>


### [83] [Video Compression with Hierarchical Temporal Neural Representation](https://arxiv.org/abs/2601.17743)
*Jun Zhu,Xinfeng Zhang,Lv Tang,Junhao Jiang,Gai Zhang,Jia Wang*

Main category: cs.CV

TL;DR: TeNeRV是一种新的层次化神经表示方法，通过增强时间维度建模，提升了视频压缩效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经隐式表示（INR）的视频压缩方法难以很好地捕捉复杂的时序依赖，影响压缩性能和还原质量。

Method: TeNeRV包含两个关键模块：一是用于相邻帧特征融合的IFF模块，提升局部时序一致性；二是GAM机制，将视频分组（GoP）并针对不同分组自适应调节网络参数，捕捉不同时间尺度的特征。

Result: 大量实验表明，TeNeRV在速率-失真性能上比当前的INR方法有更突出和稳定的表现。

Conclusion: TeNeRV通过分层时序建模提升了神经表示视频压缩方法的表达力和适应性，优于同类方法。

Abstract: Video compression has recently benefited from implicit neural representations (INRs), which model videos as continuous functions. INRs offer compact storage and flexible reconstruction, providing a promising alternative to traditional codecs. However, most existing INR-based methods treat the temporal dimension as an independent input, limiting their ability to capture complex temporal dependencies. To address this, we propose a Hierarchical Temporal Neural Representation for Videos, TeNeRV. TeNeRV integrates short- and long-term dependencies through two key components. First, an Inter-Frame Feature Fusion (IFF) module aggregates features from adjacent frames, enforcing local temporal coherence and capturing fine-grained motion. Second, a GoP-Adaptive Modulation (GAM) mechanism partitions videos into Groups-of-Pictures and learns group-specific priors. The mechanism modulates network parameters, enabling adaptive representations across different GoPs. Extensive experiments demonstrate that TeNeRV consistently outperforms existing INR-based methods in rate-distortion performance, validating the effectiveness of our proposed approach.

</details>


### [84] [Bridging Supervision Gaps: A Unified Framework for Remote Sensing Change Detection](https://arxiv.org/abs/2601.17747)
*Kaixuan Jiang,Chen Wu,Zhenghui Zhao,Chengxi Han*

Main category: cs.CV

TL;DR: 提出了一种新的遥感时序影像变化检测框架UniCD，能同时处理有监督、弱监督和无监督任务，并在主流数据集上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实中像素级变化标签非常昂贵且难以获取，现有方法难以同时适应不同标注级别的场景，急需一种统一且高效的变化检测方案。

Method: UniCD采用共享编码器+多分支结构，分别针对有监督、弱监督和无监督任务设置不同分支。其中：有监督分支引入空间-时序感知模块（STAM），提升双时相特征融合；弱监督分支提出变化表征正则化（CRR），优化粗粒度激活收敛；无监督分支则利用语义先验驱动变化推断（SPCI），将无监督任务转化为可控的弱监督路径优化。

Result: 在主流变化检测数据集上，UniCD在三种任务下均取得最佳效果，尤其在弱监督和无监督场景下分别比现有方法精度高12.72%和12.37%。

Conclusion: UniCD有效提升了多场景下变化检测的适应性和性能，是统一多监督信号遥感变化检测任务的有力框架。

Abstract: Change detection (CD) aims to identify surface changes from multi-temporal remote sensing imagery. In real-world scenarios, Pixel-level change labels are expensive to acquire, and existing models struggle to adapt to scenarios with diverse annotation availability. To tackle this challenge, we propose a unified change detection framework (UniCD), which collaboratively handles supervised, weakly-supervised, and unsupervised tasks through a coupled architecture. UniCD eliminates architectural barriers through a shared encoder and multi-branch collaborative learning mechanism, achieving deep coupling of heterogeneous supervision signals. Specifically, UniCD consists of three supervision-specific branches. In the supervision branch, UniCD introduces the spatial-temporal awareness module (STAM), achieving efficient synergistic fusion of bi-temporal features. In the weakly-supervised branch, we construct change representation regularization (CRR), which steers model convergence from coarse-grained activations toward coherent and separable change modeling. In the unsupervised branch, we propose semantic prior-driven change inference (SPCI), which transforms unsupervised tasks into controlled weakly-supervised path optimization. Experiments on mainstream datasets demonstrate that UniCD achieves optimal performance across three tasks. It exhibits significant accuracy improvements in weakly and unsupervised scenarios, surpassing current state-of-the-art by 12.72% and 12.37% on LEVIR-CD, respectively.

</details>


### [85] [MV-S2V: Multi-View Subject-Consistent Video Generation](https://arxiv.org/abs/2601.17756)
*Ziyang Song,Xinyu Gong,Bangya Liu,Zelin Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种从多视角参考图像生成视频的新方法（MV-S2V），实现了3D层次上的主体一致性，并在合成视频质量上取得突破。


<details>
  <summary>Details</summary>
Motivation: 现有的视频主体生成方法（S2V）主要基于单一视角参考，导致生成效果仅限于该视角，未充分利用视频主体控制的潜力。因此，提升多视角下的3D主体一致性成为该领域的挑战。

Method: 1）提出MV-S2V任务，从多个视角的参考图像合成视频以实现3D主体一致性；2）构建了合成数据管线以生成高度定制的合成数据，并辅以小规模真实数据集共同训练模型；3）提出了 Temporally Shifted RoPE（TS-RoPE）技术，区分跨主体与跨视角的条件信息，提升了生成过程的精确性。

Result: 该方法在多视角参考下实现了更优的3D主体一致性，生成视频质量高于现有方法，实验验证了方法有效性。

Conclusion: 论文开辟了以多视角参考为基础的新视频生成方向，显著提升了主体驱动视频生成的3D一致性与输出质量。

Abstract: Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at <a href="https://szy-young.github.io/mv-s2v">this URL</a>

</details>


### [86] [Agreement-Driven Multi-View 3D Reconstruction for Live Cattle Weight Estimation](https://arxiv.org/abs/2601.17791)
*Rabin Dulal,Wenfeng Jia,Lihong Zheng,Jane Quinn*

Main category: cs.CV

TL;DR: 本研究提出了一种利用多视角RGB图像与3D重建技术，结合集成回归模型，实现牛只非接触、低成本、精准的体重估算方法。


<details>
  <summary>Details</summary>
Motivation: 传统称重和体况评估方法存在耗时、操作复杂、对牲畜和经济均有不利影响的问题。为提升畜牧管理的效率和动物福利，亟需一种简便、无接触、经济实用的精准体重估算方法。

Method: 采用多视角RGB影像，通过SAM 3D（Segment Anything Model 3D）为核心的3D重建流程进行点云生成，再应用集成回归模型与深度学习模型进行体重预测。在数据量有限的实际农场环境下，对比了不同3D重建方式与预测模型的表现。

Result: SAM 3D结合多视角一致性融合方法生成的三维点云效果优于其他3D方法，在实际农场环境下，传统集成回归模型比深度学习模型表现更稳定（R^2=0.69±0.10，MAPE=2.22±0.56%），预测精度高且具备可实用性。

Conclusion: 该研究证明，提升3D重建质量比增加模型复杂度更关键。提出的方法低成本、易用，适用于难以获得大规模3D数据的实际农场体重监测，有助于养殖业的数字化和智能化升级。

Abstract: Accurate cattle live weight estimation is vital for livestock management, welfare, and productivity. Traditional methods, such as manual weighing using a walk-over weighing system or proximate measurements using body condition scoring, involve manual handling of stock and can impact productivity from both a stock and economic perspective. To address these issues, this study investigated a cost-effective, non-contact method for live weight calculation in cattle using 3D reconstruction. The proposed pipeline utilized multi-view RGB images with SAM 3D-based agreement-guided fusion, followed by ensemble regression. Our approach generates a single 3D point cloud per animal and compares classical ensemble models with deep learning models under low-data conditions. Results show that SAM 3D with multi-view agreement fusion outperforms other 3D generation methods, while classical ensemble models provide the most consistent performance for practical farm scenarios (R$^2$ = 0.69 $\pm$ 0.10, MAPE = 2.22 $\pm$ 0.56 \%), making this practical for on-farm implementation. These findings demonstrate that improving reconstruction quality is more critical than increasing model complexity for scalable deployment on farms where producing a large volume of 3D data is challenging.

</details>


### [87] [ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning](https://arxiv.org/abs/2601.17818)
*Wen Luo,Peng Chen,Xiaotao Huang,LiQun Huang*

Main category: cs.CV

TL;DR: 提出了一种新的视觉-文本协同剪枝框架ViTCoP，用于高效减少视觉语言模型中的视觉token冗余，提升速度和降低资源消耗，同时保持甚至提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前大视觉语言模型在处理视觉token时存在较大冗余，导致计算开销高。已有的剪枝方法，要么过早丢失关键信息，要么造成冗余。如何高效地筛选与保留关键信息的token，是亟需解决的问题。

Method: ViTCoP框架融合了视觉编码器内的冗余过滤和基于LLM分层特性的逐步协同剪枝，同时引入K向量L2范数作为LLM内token重要性度量，确保兼容如FlashAttention等加速技术。

Result: ViTCoP在多种主流视觉语言模型与各种图片和视频理解任务上实验，表现出比现有方法更优的效果，同时显著缩短了推理延迟、降低了GPU内存消耗，剪枝率极高时优势更为明显。

Conclusion: ViTCoP有效解决了视觉token剪枝中的信息丢失和冗余问题，实现了性能提升和资源节约两者的兼得，为大模型的实际应用提供了更优解决方案。

Abstract: Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.

</details>


### [88] [VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training](https://arxiv.org/abs/2601.17830)
*Mengmeng Wang,Dengyang Jiang,Liuzhuozheng Li,Yucheng Lin,Guojiang Shen,Xiangjie Kong,Yong Liu,Guang Dai,Jingdong Wang*

Main category: cs.CV

TL;DR: 本文提出名为\name的轻量级内在引导框架，通过利用预训练VAE特征加速扩散变换器的训练收敛，同时提升生成质量，且无须额外的外部编码器或双模型，额外计算开销极小。


<details>
  <summary>Details</summary>
Motivation: 现有扩散变换器虽然生成性能强大，但训练收敛效率低。已有的加速方案依赖外部模型或双模型，导致训练计算开销大，因此需要更高效、轻量的方法。

Method: \name方法利用预训练VAE的重建特性，将扩散模型的中间隐空间特征与VAE特征通过轻量投影层对齐，并引入特征对齐损失进行监督，无需外部特征编码器或双模型结构。

Result: 实验表明，\name在提升生成质量和加快训练收敛速度上均优于基础扩散变换器，并可达到或超过最先进的加速方法，只增加4% GFLOPs，且无外部额外模型开销。

Conclusion: \name实现了一种高效、简单、计算开销小的扩散模型训练加速方案，为实际部署提供了更优选择。

Abstract: Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.

</details>


### [89] [Geometry-Grounded Gaussian Splatting](https://arxiv.org/abs/2601.17835)
*Baowen Zhang,Chenxing Jiang,Heng Li,Shaojie Shen,Ping Tan*

Main category: cs.CV

TL;DR: 本论文提出了一种新的理论和方法，将高斯原语视为特定的随机体，实现了基于高斯投影的高质量三维形状重建。


<details>
  <summary>Details</summary>
Motivation: 当前高斯投影（GS）技术虽在新视角合成上表现优异，但从高斯原语中提取三维形状仍然存在多视角一致性差和浮点噪声敏感等问题，亟需提出更健壮的几何建模方法。

Method: 作者从理论上将高斯原语建模为随机体，并据此提出Geometry-Grounded Gaussian Splatting方法，直接将高斯原语作为显式几何表示，结合随机体的体积属性，高效渲染高质量的深度图以提取精细几何结构。

Result: 实验表明，该方法在公开数据集上，较所有以高斯投影为基础的方法，实现了最佳的三维形状重建效果。

Conclusion: 通过严谨的理论和方法创新，本文为高斯投影方法下的高质量形状重建提供了坚实的理论基础和实践突破，显著提升了多视角一致性与抗噪能力。

Abstract: Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.

</details>


### [90] [SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction](https://arxiv.org/abs/2601.17857)
*Lan Yang,Minghan Yang,Ke Li,Honggang Zhang,Kaiyue Pang,Yi-Zhe Song*

Main category: cs.CV

TL;DR: 本文提出了一种新的fMRI图像重建框架SynMind，通过引入句子级语义解析，显著提升了重建图像的语义一致性，克服以往方法仅重视表观低层特征但语义错位的问题。实验表明，该方法超越主流最新方法，且硬件资源要求低。


<details>
  <summary>Details</summary>
Motivation: 当前fMRI图像重建方法虽然在视觉真实感方面取得进步，但往往存在语义错配问题，即重建图像虽然自然但内容与真实刺激不符。造成这种情况的原因在于主流方法过度依赖于低层次视觉特征的嵌入，而没有对大脑信号中的显式语义信息进行有效解释，因此有必要重新思考fMRI解码中的语义表达问题。

Method: 作者采用了一种新的信息流，将fMRI信号解析为富有层次和组合结构的、类人句子级语义描述。具体地，利用基础视觉-语言模型（VLM）生成包含对象身份和空间组织的多粒度文本描述，再结合视觉先验输入到预训练扩散模型中进行图像生成。最终形成SynMind框架，实现了语义编码驱动的图像重建。

Result: 实验结果显示，SynMind在多个定量指标上均超越目前主流方法。此外，尽管使用尺寸更小、算力消耗更低的Stable Diffusion 1.4模型（相较于SDXL），配合单卡消费级GPU，SynMind仍能获得更优表现。大规模人工主观评价验证了其重建图像与人类视觉感知更一致。神经可视化分析表明，SynMind激活了更广泛且更具语义相关性的脑区，减弱了对高级视觉区域的依赖。

Conclusion: SynMind通过引入明确的语义解析和文本对齐，有效提升了fMRI图像重建的语义准确性和视觉合理性。此方法不仅在可解释性和重建质量上有显著提升，还具备较高的硬件适应性和推广潜力，有望推动基于大脑信号的高质量图像重建和神经表征理解。

Abstract: Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.

</details>


### [91] [Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment](https://arxiv.org/abs/2601.17862)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合量子增强和轻量级架构的域泛化方法，可在无真实多中心标签数据情况下，有效提升医学影像AI模型在新域（中心、设备）下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前医学影像AI模型在单中心/单设备上效果很好，但在真实多中心部署时，由于域偏移导致性能下降，限制了其临床应用。

Method: 提出以MobileNetV2为基础的域不变编码器，并优化三大组件：1）用亮度、对比度、锐化和噪声扰动模拟多域成像条件；2）通过域对抗训练（梯度反转）抑制域判别特征；3）使用轻量化量子特征增强层，通过可参数化量子电路实现非线性映射和纠缠建模。推理阶段还引入了测试时自适应策略进一步缓解分布漂移。

Result: 在模拟多中心医学影像数据集上的实验表明，该方法在未见过的新域上比没有域泛化或未使用量子增强的基线模型显著提升AUC和敏感性，降低了域特异性性能波动。

Conclusion: 量子增强的域泛化技术在有限计算资源下具有显著潜力，可为混合量子-经典医学影像系统提供一套可行的范式方案。

Abstract: Medical image artificial intelligence models often achieve strong performance in single-center or single-device settings, yet their effectiveness frequently deteriorates in real-world cross-center deployment due to domain shift, limiting clinical generalizability. To address this challenge, we propose a lightweight domain generalization framework with quantum-enhanced collaborative learning, enabling robust generalization to unseen target domains without relying on real multi-center labeled data. Specifically, a MobileNetV2-based domain-invariant encoder is constructed and optimized through three key components: (1) multi-domain imaging shift simulation using brightness, contrast, sharpening, and noise perturbations to emulate heterogeneous acquisition conditions; (2) domain-adversarial training with gradient reversal to suppress domain-discriminative features; and (3) a lightweight quantum feature enhancement layer that applies parameterized quantum circuits for nonlinear feature mapping and entanglement modeling. In addition, a test-time adaptation strategy is employed during inference to further alleviate distribution shifts. Experiments on simulated multi-center medical imaging datasets demonstrate that the proposed method significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity. These results highlight the clinical potential of quantum-enhanced domain generalization under constrained computational resources and provide a feasible paradigm for hybrid quantum--classical medical imaging systems.

</details>


### [92] [MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance](https://arxiv.org/abs/2601.17866)
*Yoonwoo Jeong,Cheng Sun,Yu-Chiang Frank Wang,Minsu Cho,Jaesung Choe*

Main category: cs.CV

TL;DR: 本文提出了一种新框架MV-SAM，实现多视图分割并保证3D一致性，无需额外3D网络或3D标注数据，比现有方法更高效、通用。


<details>
  <summary>Details</summary>
Motivation: 现有的promptable segmentation方法（如SAM）已将分割扩展到视频和多视图图像，但缺乏3D感知，导致分割结果不一致，且通常需代价高昂的每场景优化来保证3D一致性。

Method: 作者提出了MV-SAM框架，利用pointmaps（一种无需相机位姿即可重建的3D点云）将图像和提示信号提升到3D空间，整合SAM的预训练编码器并用transformer进行解码，采用3D空间的提示和像素-点一对一映射，实现2D和3D交互的耦合。

Result: MV-SAM在SA-1B数据集上训练，在多个公开基准（如NVOS、SPIn-NeRF、ScanNet++、uCo3D和DL3DV）上取得了比SAM2-Video更优或相当于每场景优化方法的性能表现。

Conclusion: MV-SAM克服了现有promptable segmentation方法缺乏3D一致性的难题，无需3D显式网络或3D标注数据，即可实现高效且一致的多视角分割，具备良好的跨领域泛化能力。

Abstract: Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.

</details>


### [93] [VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding](https://arxiv.org/abs/2601.17868)
*Zhihao He,Tieyuan Chen,Kangyu Wang,Ziran Qin,Yang Shao,Chaofan Gan,Shijie Li,Zuxuan Wu,Weiyao Lin*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视频大语言模型VidLaDA，通过扩散语言模型实现双向建模，并结合了高效的推理加速机制，大幅提升了视频理解任务的表现和速度。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归视频大语言模型（Video LLMs）因因果掩码导致只能单向建模，难以捕捉完整的时空依赖关系，进而限制了模型理解和推理能力。在处理大规模视频数据时，推理效率也成为瓶颈。

Method: 作者提出VidLaDA，通过扩散语言模型结合双向注意力机制，实现对视频时空的全局建模。另外，设计了MARS-Cache机制，通过异步视觉缓存更新与按帧分块注意力，既能有效去除冗余，又保持全局关联性，大幅加快推理速度。

Result: 实验表明，VidLaDA在各种基准测试中均优于扩散模型基线，并达到主流最新自回归视频LLM（如Qwen2.5-VL和LLaVA-Video）相媲美的表现；同时，MARS-Cache推理提速12倍以上，推理准确率无明显下降。

Conclusion: VidLaDA在视频理解任务中兼具高效的全局建模与推理速度，表明基于扩散语言模型和创新缓存架构在视频LLM领域具有很大潜力，并为相关领域提供了可参考的开源实现。

Abstract: Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.

</details>


### [94] [Quran-MD: A Fine-Grained Multilingual Multimodal Dataset of the Quran](https://arxiv.org/abs/2601.17880)
*Muhammad Umar Salman,Mohammad Areeb Qazi,Mohammed Talha Alam*

Main category: cs.CV

TL;DR: Quran MD 是一个全面的多模态古兰经数据集，包含文本、语言学、音频信息，支持各种自然语言处理与语音任务。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏结合古兰经文本与多样音频朗诵（包含不同风格、方言）的公开多模态数据集，阻碍了相关自动语音识别、TTS、语音风格分析等多种研究和应用。作者希望为数字伊斯兰学和多种计算方法提供支持。

Method: 收集了每节经文（ayah）的阿拉伯文、英文翻译、音译，以及由32位不同风格的朗诵者录制的音频。每个词（token）层级也配有阿文、英文、音译及对应音频，实现精细的发音和语义分析。

Result: 构建了包含文字、逐词发音、跨朗诵者多样音频的多模态数据集，并公开发布，能用于NLP、ASR、TTS、语言学、风格迁移等研究方向。

Conclusion: Quran MD数据集为多模态古兰经研究和相关应用提供了坚实基础，将推动自动语音识别、风格分析、语音合成、经文检索等领域的发展。

Abstract: We present Quran MD, a comprehensive multimodal dataset of the Quran that integrates textual, linguistic, and audio dimensions at the verse and word levels. For each verse (ayah), the dataset provides its original Arabic text, English translation, and phonetic transliteration. To capture the rich oral tradition of Quranic recitation, we include verse-level audio from 32 distinct reciters, reflecting diverse recitation styles and dialectical nuances. At the word level, each token is paired with its corresponding Arabic script, English translation, transliteration, and an aligned audio recording, allowing fine-grained analysis of pronunciation, phonology, and semantic context. This dataset supports various applications, including natural language processing, speech recognition, text-to-speech synthesis, linguistic analysis, and digital Islamic studies. Bridging text and audio modalities across multiple reciters, this dataset provides a unique resource to advance computational approaches to Quranic recitation and study. Beyond enabling tasks such as ASR, tajweed detection, and Quranic TTS, it lays the foundation for multimodal embeddings, semantic retrieval, style transfer, and personalized tutoring systems that can support both research and community applications. The dataset is available at https://huggingface.co/datasets/Buraaq/quran-audio-text-dataset

</details>


### [95] [Revisiting 3D Reconstruction Kernels as Low-Pass Filters](https://arxiv.org/abs/2601.17900)
*Shengjun Zhang,Min Chen,Yibo Wei,Mingyu Dong,Yueqi Duan*

Main category: cs.CV

TL;DR: 本文从信号处理角度重新审视3D重建，识别了离散采样导致的周期性频谱扩展是三维重建的核心挑战，并提出Jinc核及其调制变体，有效提升渲染与重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建核心通常用高斯等低通核，但这些核低通特性不理想，导致频谱中高低频信号混叠，影响重建质量。因此，希望从信号处理严格意义上理想的低通性质出发设计更优的内核。

Method: 作者提出用Jinc核替代常用高斯等核，因其在截止频率处振幅瞬时归零，具有理想低通特性。针对Jinc空间域衰减慢的问题，又设计了调制核，在空间效率与频域保真之间取得平衡。

Result: 实验证明，采用Jinc和调制核在三维重建和渲染任务中效果优于传统核，频率与空间表现均有改善。

Conclusion: Jinc及其调制核因更理想的低通特性，有效改善了三维重建的频谱混叠问题，实现了更高效、高保真的三维重建。

Abstract: 3D reconstruction is to recover 3D signals from the sampled discrete 2D pixels, with the goal to converge continuous 3D spaces. In this paper, we revisit 3D reconstruction from the perspective of signal processing, identifying the periodic spectral extension induced by discrete sampling as the fundamental challenge. Previous 3D reconstruction kernels, such as Gaussians, Exponential functions, and Student's t distributions, serve as the low pass filters to isolate the baseband spectrum. However, their unideal low-pass property results in the overlap of high-frequency components with low-frequency components in the discrete-time signal's spectrum. To this end, we introduce Jinc kernel with an instantaneous drop to zero magnitude exactly at the cutoff frequency, which is corresponding to the ideal low pass filters. As Jinc kernel suffers from low decay speed in the spatial domain, we further propose modulated kernels to strick an effective balance, and achieves superior rendering performance by reconciling spatial efficiency and frequency-domain fidelity. Experimental results have demonstrated the effectiveness of our Jinc and modulated kernels.

</details>


### [96] [Feature-Space Generative Models for One-Shot Class-Incremental Learning](https://arxiv.org/abs/2601.17905)
*Jack Foster,Kirill Paramonov,Mete Ozay,Umberto Michieli*

Main category: cs.CV

TL;DR: 本文提出了一种用于极少样本增量学习（FSCIL）的新方法Gen1S，在仅有每类1个新样本且基础训练后不可调整模型参数的极端条件下，有效提升了新类别识别能力。


<details>
  <summary>Details</summary>
Motivation: 在FSCIL中，模型需识别不断增加的新类，特别是在每个新类只有1个样本、且不能对网络继续训练的条件下，泛化到新类变得极为困难。现有方法往往难以应对如此苛刻的设定，因此亟需新的结构性先验和方法改进。

Method: 作者假设基础类与新类的嵌入呈现结构性相似。为此，设计了通过减去样本所属类别原型（均值嵌入）将特征映射到残差空间的操作，并使用生成模型（VAE或扩散模型）学习基础类残差的多模态分布，进一步作为结构先验辅助新类识别。

Result: 提出的Gen1S方法在多个数据集和不同backbone下，显著优于现有最优方法，在新类识别任务中实现了持续性的提升。

Conclusion: 利用类间嵌入结构相似性和生成建模的结构先验，能够在极少样本和不可更新模型的条件下，有效提升新类别的识别性能。

Abstract: Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.

</details>


### [97] [Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models](https://arxiv.org/abs/2601.17918)
*Dain Kim,Jiwoo Lee,Jaehoon Yun,Yong Hoe Koo,Qingyu Chen,Hyunjae Kim,Jaewoo Kang*

Main category: cs.CV

TL;DR: 本文系统评估了直接偏好优化（DPO）在医疗大视觉-语言模型（LVLMs）中的表现，发现当前DPO方法存在显著局限，并提出面向视觉误判的偏好构建策略，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 医疗LVLM在医疗应用中潜力巨大，但由于对齐性和可靠性不足，部署受限。DPO虽然被认为能改进模型响应，但在高风险医疗领域效果尚未充分验证，缺少系统的实证分析。本文旨在填补这一空白，为后续方法革新提供实证基础。

Method: 作者评估了九种不同DPO变体，分别在两种医疗LVLM（LLaVA-Med和HuatuoGPT-Vision）上进行实验，全面分析各DPO方法在不同任务和模型骨干网络上的表现。并提出一种针对视觉误判错误的偏好构建策略作为验证。

Result: 结果发现，现有DPO方法相较于监督微调提升有限，且效果在不同任务及模型上表现不一，且难以解决视觉误判问题。提出的新策略在视觉问答任务上比最优DPO基线提升3.6%。

Conclusion: 本文强调了当前DPO方法在医疗视觉-语言模型上的不足，并通过创新的偏好构建方法证明了改善方向，推动了后续相关研究，并公开数据、模型和代码支持社区发展。

Abstract: Large Vision-Language Models (LVLMs) hold significant promise for medical applications, yet their deployment is often constrained by insufficient alignment and reliability. While Direct Preference Optimization (DPO) has emerged as a potent framework for refining model responses, its efficacy in high-stakes medical contexts remains underexplored, lacking the rigorous empirical groundwork necessary to guide future methodological advances. To bridge this gap, we present the first comprehensive examination of diverse DPO variants within the medical domain, evaluating nine distinct formulations across two medical LVLMs: LLaVA-Med and HuatuoGPT-Vision. Our results reveal several critical limitations: current DPO approaches often yield inconsistent gains over supervised fine-tuning, with their efficacy varying significantly across different tasks and backbones. Furthermore, they frequently fail to resolve fundamental visual misinterpretation errors. Building on these insights, we present a targeted preference construction strategy as a proof-of-concept that explicitly addresses visual misinterpretation errors frequently observed in existing DPO models. This design yields a 3.6% improvement over the strongest existing DPO baseline on visual question-answering tasks. To support future research, we release our complete framework, including all training data, model checkpoints, and our codebase at https://github.com/dmis-lab/med-vlm-dpo.

</details>


### [98] [RemEdit: Efficient Diffusion Editing with Riemannian Geometry](https://arxiv.org/abs/2601.17927)
*Eashan Adhikarla,Brian D. Davison*

Main category: cs.CV

TL;DR: 本文提出了RemEdit生成式图像编辑框架，在图像语义保真度与推理速度间取得新平衡。通过创新性地采用Riemann流形上的潜空间导航、mamba模块学习流形结构、SLERP融合和V-L模型辅助提升编辑质量，并通过新颖注意力剪枝实现实时与高效。效果超过此前SOTA，并实现50%剪枝下的实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有的可控图像生成方法在保证语义保真和推理速度之间存在明显矛盾——高保真通常意味着计算繁重，难以实时应用。本文旨在提出方法，突破这一性能瓶颈，实现既高效又高质量的可控图像编辑。

Method: RemEdit框架结合两大创新：1）编辑保真方面，将潜空间视为黎曼流形，利用mamba模块学习流形结构，并寻求最优测地路径进行平滑编辑，同时引入双SLERP融合和视觉-语言模型生成目标导向文本提示；2）加速方面，引入任务特定的注意力剪枝，轻量化头部学习保留与编辑任务最相关Token，实现高效而无语义损失的推理。

Result: RemEdit在编辑质量上超过当前主流方法，在剪枝率高达50%情况下依然保持实时推理能力，兼顾语义保真和处理速度，树立了新一代编辑系统性能标杆。

Conclusion: RemEdit提供了一种易于部署且高效、强大的图像编辑框架，通过多项创新有效平衡了保真度与速度，为实际应用提供新选择。

Abstract: Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.

</details>


### [99] [From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images](https://arxiv.org/abs/2601.17934)
*Vi Vu,Thanh-Huy Nguyen,Tien-Thinh Nguyen,Ba-Thinh Lam,Hoang-Thien Nguyen,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 该论文提出SC-SAM框架，结合了U-Net与SAM，通过互相指导实现半监督下的医学图像分割，效果超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 主流大模型如SAM在医学图像领域面临领域转移、标注稀缺以及难以利用无标注数据的问题，常规模型（如U-Net）虽擅长半监督学习但未被用于辅助SAM。这促使作者探索如何结合两者优势，提升医学图像分割效果。

Method: 论文提出了一种专家-全能体（specialist-generalist）框架SC-SAM。具体做法是：U-Net作为专家，提供点式提示和伪标签帮助SAM适应医学影像，同时SAM作为全能体对U-Net施加监督进行正则化，形成双向的共同训练循环，从而使二者都能有效利用无标注数据。

Result: 在前列腺MRI和息肉分割基准上，SC-SAM取得了最新最优（state-of-the-art）表现，优于现有所有半监督SAM变体及基金医学模型如MedSAM。

Conclusion: 专家-全能体协作模式能有效提升医学图像分割的标签效率和泛化能力，为大模型医学应用提供了一种新思路。

Abstract: Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.

</details>


### [100] [DTC: A Deformable Transposed Convolution Module for Medical Image Segmentation](https://arxiv.org/abs/2601.17939)
*Chengkun Sun,Jinqian Pan,Renjie Liang,Zhengkang Fan,Xin Miao,Jiang Bian,Jie Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的变形反卷积（DTC）方法，可以自适应学习上采样的采样位置，有效提升2D/3D医学图像分割的细节还原和特征重建能力，在多个公开数据集上均取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统的上采样方式如转置卷积和线性插值都基于固定采样位置，无法充分捕捉超出预定义位置的结构信息，可能造成伪影或细节丢失。该问题在医学图像分割任务中尤为明显，因此有必要研究更灵活的上采样方法。

Method: 受可变形卷积启发，作者提出“变形反卷积（Deformable Transposed Convolution, DTC）”，它能够学习动态（非固定）坐标来进行上采样，从而生成更高分辨率的特征图。该方法适用于2D和3D的医学图像分割任务，并可方便地嵌入现有UNet类结构中。

Result: 在3D（如BTCV15）和2D（如ISIC18, BUSI）公开数据集上的实验表明，DTC能显著提升分割模型的重建和细节还原能力，对比传统上采样方法表现更优。

Conclusion: DTC能有效弥补传统固定位置上采样方法的不足，为医学图像分割领域提供了一种新的高效特征重建手段，在实践中具有很好的应用前景。

Abstract: In medical image segmentation, particularly in UNet-like architectures, upsampling is primarily used to transform smaller feature maps into larger ones, enabling feature fusion between encoder and decoder features and supporting multi-scale prediction. Conventional upsampling methods, such as transposed convolution and linear interpolation, operate on fixed positions: transposed convolution applies kernel elements to predetermined pixel or voxel locations, while linear interpolation assigns values based on fixed coordinates in the original feature map. These fixed-position approaches may fail to capture structural information beyond predefined sampling positions and can lead to artifacts or loss of detail. Inspired by deformable convolutions, we propose a novel upsampling method, Deformable Transposed Convolution (DTC), which learns dynamic coordinates (i.e., sampling positions) to generate high-resolution feature maps for both 2D and 3D medical image segmentation tasks. Experiments on 3D (e.g., BTCV15) and 2D datasets (e.g., ISIC18, BUSI) demonstrate that DTC can be effectively integrated into existing medical image segmentation models, consistently improving the decoder's feature reconstruction and detail recovery capability.

</details>


### [101] [FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos](https://arxiv.org/abs/2601.17947)
*Bora Yimenicioglu,Vishal Manikanden*

Main category: cs.CV

TL;DR: 本文提出了FlowMorph，一种基于物理自洽、无需人工标注的红细胞力学特性分析方法，实现了大规模高通量微流控视频数据中的细胞形变指标提取，显著提升了自动化与物理一致性。


<details>
  <summary>Details</summary>
Motivation: 红细胞机械性能是多种疾病的生物标志物。高通量、准确且自动化的细胞力学测量亟需无需标注、能融合物理规律的方法，克服目前手工特征、监督分割等不足，并提升物理一致性。

Method: FlowMorph采用自监督框架，用低维参数轮廓建模红细胞，通过可微胶囊-流动模型联立层流对流与弹性约束，结合自动轮廓与光流，只依赖亮场视频，联合优化多重损失以实现物理自洽跟踪与力学量提取。

Result: FlowMorph在公开微流控红细胞数据集上，达到平均IoU 0.905，分割与力学约束远优于数据驱动方法。提出的标量k能有效区分红细胞运动模式（AUC 0.863），仅需少量标定即可准确预测表观杨氏模量（绝对误差0.118MPa），且对于实验条件变化表现稳健。

Conclusion: FlowMorph实现了基于物理自洽、自监督的视频细胞力学测量，在自动化、精度与泛化性方面相较现有方法均有显著提升，推动了基于微流控的高通量红细胞生物力学表型分析。

Abstract: Mechanical properties of red blood cells (RBCs) are promising biomarkers for hematologic and systemic disease, motivating microfluidic assays that probe deformability at throughputs of $10^3$--$10^6$ cells per experiment. However, existing pipelines rely on supervised segmentation or hand-crafted kymographs and rarely encode the laminar Stokes-flow physics that governs RBC shape evolution. We introduce FlowMorph, a physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy $k$ for each tracked RBC from short brightfield microfluidic videos. FlowMorph models each cell by a low-dimensional parametric contour, advances boundary points through a differentiable ''capsule-in-flow'' combining laminar advection and curvature-regularized elastic relaxation, and optimizes a loss coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness, using only automatically derived silhouettes and optical flow.
  Across four public RBC microfluidic datasets, FlowMorph achieves a mean silhouette IoU of $0.905$ on physics-rich videos with provided velocity fields and markedly improves area conservation and wall violations over purely data-driven baselines. On $\sim 1.5\times 10^5$ centered sequences, the scalar $k$ alone separates tank-treading from flipping dynamics with an AUC of $0.863$. Using only $200$ real-time deformability cytometry (RT-DC) events for calibration, a monotone map $E=g(k)$ predicts apparent Young's modulus with a mean absolute error of $0.118$\,MPa on $600$ held-out cells and degrades gracefully under shifts in channel geometry, optics, and frame rate.

</details>


### [102] [UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders](https://arxiv.org/abs/2601.17950)
*Matthew Walmer,Saksham Suri,Anirud Aggarwal,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 本文提出了UPLiFT，一种高效的通用像素密集特征上采样架构，通过改进的迭代方法和本地注意力算子，在推理成本更低的情况下实现了与最新交叉注意力方法相当或更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有利用预训练视觉主干高效生成高分辨率特征的方法受到效率与性能的权衡限制。当前主流交叉注意力方法虽然性能高，但推理成本与主干网络一样高，亟需高效且表现优异的新方法。

Method: 作者提出UPLiFT，一种基于改进迭代上采样和本地注意力算子的架构。该本地注意力机制仅在局部区域内进行注意力池化，规避了全局交叉注意力的高昂计算负担。

Result: UPLiFT在多项基准和下游生成任务（如VAE特征上采样）上表现优越，达到甚至超过最新交叉注意力方法，但推理成本更低。本地注意力算子也保证了特征在上采样过程中的稳定性。

Conclusion: UPLiFT为生成高密度特征提供了一种高效、强大的新方法，兼具通用性和低推理成本，在实际应用和未来研究中具有广泛前景。

Abstract: The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.

</details>


### [103] [Domain-Expert-Guided Hybrid Mixture-of-Experts for Medical AI: Integrating Data-Driven Learning with Clinical Priors](https://arxiv.org/abs/2601.17977)
*Jinchen Gu,Nan Zhao,Lei Qiu,Lu Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合数据驱动与领域知识引导的混合专家（MoE）模型——DKGH-MoE，提升医学等小样本领域模型性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在医学等专业领域，受限于数据规模，传统混合专家模型难以完全发挥作用。同时，临床医生的经验和眼动数据等专业知识未被有效利用。该研究旨在结合数据驱动方法与医学领域知识，提高模型在小样本场景下的表现和解释能力。

Method: 作者提出了DKGH-MoE模块，将数据驱动的MoE用于从原始医学影像中提取特征，同时引入领域专家（如医生眼动信息）引导的MoE，将医生关注区域等诊断先验整合到模型中，实现知识与数据融合。

Result: 该方法能够综合数据新模式与临床知识，显著提升了模型的性能和可解释性。

Conclusion: 融合领域知识与数据驱动特征能让混合专家模型在医学等小样本领域获得更优结果，提出的DKGH-MoE是一种有效、可拓展且具备解释性的解决方案。

Abstract: Mixture-of-Experts (MoE) models increase representational capacity with modest computational cost, but their effectiveness in specialized domains such as medicine is limited by small datasets. In contrast, clinical practice offers rich expert knowledge, such as physician gaze patterns and diagnostic heuristics, that models cannot reliably learn from limited data. Combining data-driven experts, which capture novel patterns, with domain-expert-guided experts, which encode accumulated clinical insights, provides complementary strengths for robust and clinically meaningful learning. To this end, we propose Domain-Knowledge-Guided Hybrid MoE (DKGH-MoE), a plug-and-play and interpretable module that unifies data-driven learning with domain expertise. DKGH-MoE integrates a data-driven MoE to extract novel features from raw imaging data, and a domain-expert-guided MoE incorporates clinical priors, specifically clinician eye-gaze cues, to emphasize regions of high diagnostic relevance. By integrating domain expert insights with data-driven features, DKGH-MoE improves both performance and interpretability.

</details>


### [104] [MorphXAI: An Explainable Framework for Morphological Analysis of Parasites in Blood Smear Images](https://arxiv.org/abs/2601.18001)
*Aqsa Yousaf,Sint Sint Win,Megan Coffee,Habeeb Olufowobi*

Main category: cs.CV

TL;DR: 本文提出了一种新框架MorphXAI，用于更可解释的寄生虫自动检测与形态学分析，通过引入详细的形态学标注数据集，实现了检测性能与解释性的兼顾。


<details>
  <summary>Details</summary>
Motivation: 目前寄生虫感染诊断主要依赖人工血涂片检查和专家经验，自动化深度学习虽有效但缺乏临床可解释性，现有方法仅提供粗糙的视觉热力图，难以体现医生诊断用的形态特征。因此，亟需一种同时兼具检测能力与细粒度生物学解释的新模型。

Method: 作者提出MorphXAI框架，在模型预测流程中引入形态学属性（如形状、曲率、可见斑点数、鞭毛存在与发育阶段）直接监督，并构建包含三种寄生虫及详细形态标签的医生标注新数据集，以训练和评估模型。

Result: 实验结果显示，MorphXAI不仅在寄生虫检测准确率上优于基础模型，还能输出结构化、具有生物学意义的解释，提升模型临床实用性和可信度。

Conclusion: MorphXAI实现了寄生虫自动检测与形态解释的统一，不仅提高了检测性能，还突破了传统可解释性方法的局限，为资源匮乏地区寄生虫诊断提供了更可信、解释化的人工智能辅助工具。

Abstract: Parasitic infections remain a pressing global health challenge, particularly in low-resource settings where diagnosis still depends on labor-intensive manual inspection of blood smears and the availability of expert domain knowledge. While deep learning models have shown strong performance in automating parasite detection, their clinical usefulness is constrained by limited interpretability. Existing explainability methods are largely restricted to visual heatmaps or attention maps, which highlight regions of interest but fail to capture the morphological traits that clinicians rely on for diagnosis. In this work, we present MorphXAI, an explainable framework that unifies parasite detection with fine-grained morphological analysis. MorphXAI integrates morphological supervision directly into the prediction pipeline, enabling the model to localize parasites while simultaneously characterizing clinically relevant attributes such as shape, curvature, visible dot count, flagellum presence, and developmental stage. To support this task, we curate a clinician-annotated dataset of three parasite species (Leishmania, Trypanosoma brucei, and Trypanosoma cruzi) with detailed morphological labels, establishing a new benchmark for interpretable parasite analysis. Experimental results show that MorphXAI not only improves detection performance over the baseline but also provides structured, biologically meaningful explanations.

</details>


### [105] [Strip-Fusion: Spatiotemporal Fusion for Multispectral Pedestrian Detection](https://arxiv.org/abs/2601.18008)
*Asiegbu Miracle Kanu-Asiegbu,Nitin Jotwani,Xiaoxiao Du*

Main category: cs.CV

TL;DR: 本文提出了Strip-Fusion空间-时序融合网络，显著提升了多模态行人检测在强遮挡和图像错位场景下的鲁棒性和准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的多光谱行人检测方法大多关注空间融合，忽视了时序特征。同时，可见光与热红外图像常有错位，行人在复杂环境（如光照变化、遮挡）下难以检测。因而亟需一种兼顾时序信息并抗错位、适应复杂环境的方法。

Method: 本文提出了Strip-Fusion网络，它通过时序自适应卷积动态融合空间和时序特征，增强对行人运动和上下文的捕捉。同时创新性地引入KL散度损失，缓解可见光与热红外模态信息不平衡的问题，并引导特征对齐。此外，设计了新的后处理算法以减少误检。

Result: 在KAIST与CVC-14等多模态行人检测标准测试集上，Strip-Fusion展现出与最新技术水平相当甚至更优的检测性能。特别是在遮挡严重和图像错位等挑战场景下有明显提升。

Conclusion: Strip-Fusion网络能有效融合空间和时序信息，且对多模态图像错位和行人遮挡具有较强鲁棒性，为机器人感知等领域的行人检测提供了更为实用的技术方案。

Abstract: Pedestrian detection is a critical task in robot perception. Multispectral modalities (visible light and thermal) can boost pedestrian detection performance by providing complementary visual information. Several gaps remain with multispectral pedestrian detection methods. First, existing approaches primarily focus on spatial fusion and often neglect temporal information. Second, RGB and thermal image pairs in multispectral benchmarks may not always be perfectly aligned. Pedestrians are also challenging to detect due to varying lighting conditions, occlusion, etc. This work proposes Strip-Fusion, a spatial-temporal fusion network that is robust to misalignment in input images, as well as varying lighting conditions and heavy occlusions. The Strip-Fusion pipeline integrates temporally adaptive convolutions to dynamically weigh spatial-temporal features, enabling our model to better capture pedestrian motion and context over time. A novel Kullback-Leibler divergence loss was designed to mitigate modality imbalance between visible and thermal inputs, guiding feature alignment toward the more informative modality during training. Furthermore, a novel post-processing algorithm was developed to reduce false positives. Extensive experimental results show that our method performs competitively for both the KAIST and the CVC-14 benchmarks. We also observed significant improvements compared to previous state-of-the-art on challenging conditions such as heavy occlusion and misalignment.

</details>


### [106] [Leveraging Persistence Image to Enhance Robustness and Performance in Curvilinear Structure Segmentation](https://arxiv.org/abs/2601.18045)
*Zhuangzhi Gao,Feixiang Zhou,He Zhao,Xiuju Chen,Xiaoxin Li,Qinkai Yu,Yitian Zhao,Alena Shantsila,Gregory Y. H. Lip,Eduard Shantsila,Yalin Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种通过网络结构直接融合拓扑特征的新方法，用于医学图像中曲状结构的分割，提高了分割的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学图像中的曲状结构分割对临床分析至关重要。虽然融合拓扑属性（如连通性）可以提升分割表现，但从PD（Persistence Diagrams）中提取并嵌入这些属性具有高难度与高计算代价，且以往方法依赖手工设计损失函数，泛化能力差。

Method: 提出PIs-Regressor模块，能直接从数据中学习Persistence Image（PI）的拓扑特征，并集成到Topology SegNet网络中，实现拓扑特征在下采样和上采样阶段的融合，从而将拓扑信息嵌入网络结构本身。该设计灵活，可与其他基于拓扑的方法结合。

Result: 实验结果显示，该方法在处理医学图像中的过曝和模糊等问题表现出更强鲁棒性，并在三个曲状结构数据集上达到了像素级准确率和拓扑一致性的最新水平。

Conclusion: 直接将拓扑信息深度融合进网络架构，而非通过辅助损失函数，可显著提升医学图像分割的准确性与鲁棒性，并能够灵活扩展与其他方法结合。

Abstract: Segmenting curvilinear structures in medical images is essential for analyzing morphological patterns in clinical applications. Integrating topological properties, such as connectivity, improves segmentation accuracy and consistency. However, extracting and embedding such properties - especially from Persistence Diagrams (PD) - is challenging due to their non-differentiability and computational cost. Existing approaches mostly encode topology through handcrafted loss functions, which generalize poorly across tasks. In this paper, we propose PIs-Regressor, a simple yet effective module that learns persistence image (PI) - finite, differentiable representations of topological features - directly from data. Together with Topology SegNet, which fuses these features in both downsampling and upsampling stages, our framework integrates topology into the network architecture itself rather than auxiliary losses. Unlike existing methods that depend heavily on handcrafted loss functions, our approach directly incorporates topological information into the network structure, leading to more robust segmentation. Our design is flexible and can be seamlessly combined with other topology-based methods to further enhance segmentation performance. Experimental results show that integrating topological features enhances model robustness, effectively handling challenges like overexposure and blurring in medical imaging. Our approach on three curvilinear benchmarks demonstrate state-of-the-art performance in both pixel-level accuracy and topological fidelity.

</details>


### [107] [Semi-Supervised Hyperspectral Image Classification with Edge-Aware Superpixel Label Propagation and Adaptive Pseudo-Labeling](https://arxiv.org/abs/2601.18049)
*Yunfei Qiu,Qiqiong Ma,Tianhua Lv,Li Fang,Shudong Zhou,Wei Yao*

Main category: cs.CV

TL;DR: 本文提出了一种新的半监督高光谱图像分类框架，通过整合空间先验信息与动态学习机制，目的在于提升标签稳定性和分类性能。通过边缘感知超像素标签传播（EASLP）及动态历史融合预测（DHP）等方法，显著缓解了现有伪标签扩散和不稳定问题，并在多个基准数据集上取得了优异的分类表现。


<details>
  <summary>Details</summary>
Motivation: 半监督高光谱图像分类因标签数据稀缺且标注代价高，依赖少量样本进行有效学习仍存在标签扩散和伪标签不稳定等瓶颈，亟需提升边界区分能力与伪标签质量。

Method: 作者提出了三大模块：1）边缘感知超像素标签传播（EASLP），通过边缘惩罚与邻域校正结合，提高了分割边界的标签传播鲁棒性；2）动态历史融合预测（DHP），融合历史和当前预测结果，减小伪标签波动；3）自适应三元样本分类（ATSC），基于信心和一致性指标，对样本进行分层利用提高伪标签质量。这些模块共同组成动态可靠增强伪标签框架（DREPL），并与EASLP协同，实现时空一致性优化。

Result: 在四个基准高光谱数据集上的实验显示，该方法在提升伪标签稳定性的同时，分类性能显著优于现有主流方法，表现出更强的鲁棒性和泛化能力。

Conclusion: 结合空间先验及动态机制的半监督分类框架有效应对了标签扩散和伪标签不稳问题，提升了高光谱图像分类的准确率与稳定性，在实际有限标注场景下具有广阔应用前景。

Abstract: Significant progress has been made in semi-supervised hyperspectral image (HSI) classification regarding feature extraction and classification performance. However, due to high annotation costs and limited sample availability, semi-supervised learning still faces challenges such as boundary label diffusion and pseudo-label instability. To address these issues, this paper proposes a novel semi-supervised hyperspectral classification framework integrating spatial prior information with a dynamic learning mechanism. First, we design an Edge-Aware Superpixel Label Propagation (EASLP) module. By integrating edge intensity penalty with neighborhood correction strategy, it mitigates label diffusion from superpixel segmentation while enhancing classification robustness in boundary regions. Second, we introduce a Dynamic History-Fused Prediction (DHP) method. By maintaining historical predictions and dynamically weighting them with current results, DHP smoothens pseudo-label fluctuations and improves temporal consistency and noise resistance. Concurrently, incorporating condifence and consistency measures, the Adaptive Tripartite Sample Categorization (ATSC) strategy implements hierarchical utilization of easy, ambiguous, and hard samples, leading to enhanced pseudo-label quality and learning efficiency. The Dynamic Reliability-Enhanced Pseudo-Label Framework (DREPL), composed of DHP and ATSC, strengthens pseudo-label stability across temporal and sample domains. Through synergizes operation with EASLP, it achieves spatio-temporal consistency optimization. Evaluations on four benchmark datasets demonstrate its capability to maintain superior classification performance.

</details>


### [108] [Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification](https://arxiv.org/abs/2601.18088)
*Jianshu Chao,Tianhua Lv,Qiqiong Ma,Yunfei Qiu,Li Fang,Huifang Shen,Wei Yao*

Main category: cs.CV

TL;DR: 本文提出了一种无需源域标签的自监督跨域转移学习框架，实现了对高光谱数据的高效适应性分类，在样本稀缺环境下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱自监督学习虽有潜力，但在跨域迁移时依赖源域标注，且易受数据分布漂移影响，导致在新域泛化能力降低。亟需不依赖标签且迁移鲁棒的方法提升泛化性。

Method: 1. 引入空间-光谱双分支Transformer（S2Former），通过双向交互注意力机制实现空间结构与光谱细节协同表征，提升语义一致性。
2. 设计频域约束（FDC），通过rFFT与高频幅值损失保持特征频谱一致，增强模型细粒度与边界辨别能力。
3. 微调阶段采用扩散对齐蒸馏（DAFT），师生模型对齐语义迁移轨迹，实现低标注情形下鲁棒迁移。

Result: 在四个高光谱数据集上，提出的方法展现出稳定的分类精度以及优秀的跨域适应能力，特别是在有限标签或无标签场景下优于现有方法。

Conclusion: 该框架无需源域标签，在样本有限时仍具备强泛化和迁移性，为高光谱跨域场景提供了更高效和鲁棒的解决方案。

Abstract: Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model's capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method's effectiveness under resource-constrained conditions.

</details>


### [109] [Text-Pass Filter: An Efficient Scene Text Detector](https://arxiv.org/abs/2601.18098)
*Chuang Yang,Haozhao Ma,Xu Han,Yuan Yuan,Qi Wang*

Main category: cs.CV

TL;DR: 本文提出了Text-Pass Filter（TPF）方法用于任意形状文本检测，通过仿造带通滤波器提取文本特征，实现了高效、精确且实时的分割和检测。引入的REU和FPU进一步提升了检测性能，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的收缩掩码扩展策略在文本检测中会丢失文本边缘视觉特征，并导致前景和背景混淆，对文本特征识别造成限制。因此，需要新的方法能有效检测任意形状的文本并避免这些内在缺陷。

Method: 本文设计了Text-Pass Filter（TPF）方法，直接分割整块文本，避免了传统方法的局限。TPF通过模拟带通滤波器为每个文本构建唯一的特征－滤波器对，通过滤波器筛出匹配文本。同时，提出Reinforcement Ensemble Unit（REU）解决带状长文本的特征一致性问题并扩大识别范围，增加Foreground Prior Unit（FPU）以提升前景和背景的辨析能力。

Result: 实验表明，TPF方法在文本分割精度和效率上均优于现有方法。REU和FPU两个模块也在实验中展示了对模型性能的有效提升。

Conclusion: TPF能够自然地分离粘连文本、实现实时检测，并解决传统方法的特征丢失与边界混淆问题，是一种高效且鲁棒的任意形状文本检测方案。

Abstract: To pursue an efficient text assembling process, existing methods detect texts via the shrink-mask expansion strategy. However, the shrinking operation loses the visual features of text margins and confuses the foreground and background difference, which brings intrinsic limitations to recognize text features. We follow this issue and design Text-Pass Filter (TPF) for arbitrary-shaped text detection. It segments the whole text directly, which avoids the intrinsic limitations. It is noteworthy that different from previous whole text region-based methods, TPF can separate adhesive texts naturally without complex decoding or post-processing processes, which makes it possible for real-time text detection. Concretely, we find that the band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. It provides a natural idea for extracting whole texts separately. By simulating the band-pass filter, TPF constructs a unique feature-filter pair for each text. In the inference stage, every filter extracts the corresponding matched text by passing its pass-feature and blocking other features. Meanwhile, considering the large aspect ratio problem of ribbon-like texts makes it hard to recognize texts wholly, a Reinforcement Ensemble Unit (REU) is designed to enhance the feature consistency of the same text and to enlarge the filter's recognition field to help recognize whole texts. Furthermore, a Foreground Prior Unit (FPU) is introduced to encourage TPF to discriminate the difference between the foreground and background, which improves the feature-filter pair quality. Experiments demonstrate the effectiveness of REU and FPU while showing the TPF's superiority.

</details>


### [110] [Computational Framework for Estimating Relative Gaussian Blur Kernels between Image Pairs](https://arxiv.org/abs/2601.18099)
*Akbar Saadat*

Main category: cs.CV

TL;DR: 提出了一种无需训练的前向计算框架，将高斯模型应用于实时场景，通过离散计算来恢复锐化图像的散焦（模糊）版本，并在实际图像上获得了高精度的模糊估计。


<details>
  <summary>Details</summary>
Motivation: 之前的研究已经验证了高斯模糊模型的有效性，但如何在无需训练的情况下、快速且准确地在实际应用中实现该模型仍存在挑战。作者希望通过高效的计算框架，实现散焦模糊的快速推断，从而满足实时或在线应用需求。

Method: 提出了一种离散化的解析表达式，通过相似度度量筛选多解，并适用于两幅部分模糊的图像互相转换的场景。在无需训练的前提下，直接计算出模糊核的标准差，并选择最优解。

Result: 在真实图像上的实验证明，该方法能够将合成模糊值的平均绝对误差控制在1.7%以下，模糊图像与估计图像之间的强度差异控制在2%以内，显示了很高的精度和实用性。

Conclusion: 该计算框架无需训练，可有效并准确实现高斯散焦估计，具备实时与实际应用价值。

Abstract: Following the earlier verification for Gaussian model in \cite{ASaa2026}, this paper introduces a zero training forward computational framework for the model to realize it in real time applications. The framework is based on discrete calculation of the analytic expression of the defocused image from the sharper one for the application range of the standard deviation of the Gaussian kernels and selecting the best matches. The analytic expression yields multiple solutions at certain image points, but is filtered down to a single solution using similarity measures over neighboring points.The framework is structured to handle cases where two given images are partial blurred versions of each other. Experimental evaluations on real images demonstrate that the proposed framework achieves a mean absolute error (MAE) below $1.7\%$ in estimating synthetic blur values. Furthermore, the discrepancy between actual blurred image intensities and their corresponding estimates remains under $2\%$, obtained by applying the extracted defocus filters to less blurred images.

</details>


### [111] [Spatial-Conditioned Reasoning in Long-Egocentric Videos](https://arxiv.org/abs/2601.18100)
*James Tribble,Hao Wang,Si-En Hong,Chaoyi Zhou,Ashish Bastola,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

TL;DR: 本文研究了长时 egocentric（自我视角）视频中空间推理的挑战，提出了结合空间信号和深度信息提升视觉-语言模型（VLM）空间理解能力的方法，并基于新注释的数据集做了系统评测。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型虽在图像和短视频推理上表现良好，但面对视角漂移和缺乏持续几何上下文的长时 egocentric 视频时，空间推理能力有限。如何提升VLM在类似复杂场景下的空间理解是一个关键问题。

Method: 作者对Google Sanpo数据集进行了细粒度重新标注（Sanpo-D），并设计了面向导航的空间查询任务，对多个主流VLM进行性能基线测试。同时，通过在RGB帧中融合深度图，分析输入层面的空间归纳偏置对模型空间推理能力的影响。

Result: 实验表明，增加深度感知和空间约束的输入，有助于提升模型在如行人检测、障碍物识别等安全关键任务上的空间推理表现，但会在通用任务准确率与空间专用性之间产生权衡。

Conclusion: 将深度信息和空间信号引入VLM输入，可以增强模型的空间推理能力，特别是在长时egocentric视频的导航和安全任务中，表现出较大优势。

Abstract: Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.

</details>


### [112] [LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment](https://arxiv.org/abs/2601.18118)
*Daeyoung Kim*

Main category: cs.CV

TL;DR: 本论文提出LungCRCT框架，通过因果表示学习提升肺癌早期检测和治疗分析的效果，实现高准确率且模型轻量化。


<details>
  <summary>Details</summary>
Motivation: 由于肺癌早期症状不明显且与其他呼吸疾病症状相似，导致早期发现率低，从而提高肺癌患者的死亡率，因此亟需更有效的监测和分析方法以提升早期检测和生存率。

Method: 提出了LungCRCT框架，融合了基于图自动编码器的因果发现算法、距离相关性解纠缠和基于熵的图像重建优化，从肺癌进展的物理因果机制中学习潜在因果表示，并用于因果干预分析和下游肿瘤分类任务。

Result: LungCRCT框架不仅支持肺癌治疗的因果干预分析，其下游恶性肿瘤分类任务中模型表现强劲且极为轻量，AUC达到93.91%。

Conclusion: LungCRCT通过因果表示学习显著提升了肺癌早期检测与治疗分析的能力，为后续因果推断和轻量化AI模型在医学影像领域的应用提供了新思路。

Abstract: Due to silence in early stages, lung cancer has been one of the most leading causes of mortality in cancer patients world-wide. Moreover, major symptoms of lung cancer are hard to differentiate with other respiratory disease symptoms such as COPD, further leading patients to overlook cancer progression in early stages. Thus, to enhance survival rates in lung cancer, early detection from consistent proactive respiratory system monitoring becomes crucial. One of the most prevalent and effective methods for lung cancer monitoring would be low-dose computed tomography(LDCT) chest scans, which led to remarkable enhancements in lung cancer detection or tumor classification tasks under rapid advancements and applications of computer vision based AI models such as EfficientNet or ResNet in image processing. However, though advanced CNN models under transfer learning or ViT based models led to high performing lung cancer detections, due to its intrinsic limitations in terms of correlation dependence and low interpretability due to complexity, expansions of deep learning models to lung cancer treatment analysis or causal intervention analysis simulations are still limited. Therefore, this research introduced LungCRCT: a latent causal representation learning based lung cancer analysis framework that retrieves causal representations of factors within the physical causal mechanism of lung cancer progression. With the use of advanced graph autoencoder based causal discovery algorithms with distance Correlation disentanglement and entropy-based image reconstruction refinement, LungCRCT not only enables causal intervention analysis for lung cancer treatments, but also leads to robust, yet extremely light downstream models in malignant tumor classification tasks with an AUC score of 93.91%.

</details>


### [113] [Forward Consistency Learning with Gated Context Aggregation for Video Anomaly Detection](https://arxiv.org/abs/2601.18135)
*Jiahao Lyu,Minghua Zhao,Xuewen Huang,Yifei Chen,Shuangli Du,Jing Hu,Cheng Shi,Zhiyong Lv*

Main category: cs.CV

TL;DR: 本文提出了一种面向边缘设备的轻量级视频异常检测模型FoGA，在保持高效性的同时显著提升检测精准度。


<details>
  <summary>Details</summary>
Motivation: 传统视频异常检测方法大多依赖于复杂庞大的模型，导致无法部署在资源有限的边缘设备上，并且一般只利用单帧预测误差，忽视了更长时间段的信息。

Method: FoGA基于Unet结构，对连续帧特征进行提取，产生即时与前向预测。引入门控上下文聚合模块以在相同空间尺度下动态融合编码器和解码器特征，并采用前向一致性损失和混合异常测量策略提升检测效果。

Result: FoGA模型参数量仅约2M，能够以155帧每秒的速度运行。大量实验表明，在准确率和效率上均超过现有主流方法。

Conclusion: FoGA方法实现了性能与效率之间的优异平衡，非常适合部署在边缘设备的实时视频异常检测应用中。

Abstract: As a crucial element of public security, video anomaly detection (VAD) aims to measure deviations from normal patterns for various events in real-time surveillance systems. However, most existing VAD methods rely on large-scale models to pursue extreme accuracy, limiting their feasibility on resource-limited edge devices. Moreover, mainstream prediction-based VAD detects anomalies using only single-frame future prediction errors, overlooking the richer constraints from longer-term temporal forward information. In this paper, we introduce FoGA, a lightweight VAD model that performs Forward consistency learning with Gated context Aggregation, containing about 2M parameters and tailored for potential edge devices. Specifically, we propose a Unet-based method that performs feature extraction on consecutive frames to generate both immediate and forward predictions. Then, we introduce a gated context aggregation module into the skip connections to dynamically fuse encoder and decoder features at the same spatial scale. Finally, the model is jointly optimized with a novel forward consistency loss, and a hybrid anomaly measurement strategy is adopted to integrate errors from both immediate and forward frames for more accurate detection. Extensive experiments demonstrate the effectiveness of the proposed method, which substantially outperforms state-of-the-art competing methods, running up to 155 FPS. Hence, our FoGA achieves an excellent trade-off between performance and the efficiency metric.

</details>


### [114] [Agentic Very Long Video Understanding](https://arxiv.org/abs/2601.18157)
*Aniket Rege,Arka Sadhu,Yuliang Li,Kejie Li,Ramya Korlakai Vinayak,Yuning Chai,Yong Jae Lee,Hyo Jin Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于实体场景图（entity scene graphs）的agentic架构EGAgent，提升了长时程（跨天甚至数周）自中心视频的理解和推理能力，并在公开数据集上取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 随着全天候可穿戴设备和个人AI助手的普及，AI需理解持续而非片段的视频流。但现有方法（如大语言模型与检索增强生成）只支持有限上下文，难以处理长时间、多步复合推理。

Method: 提出EGAgent框架：以实体场景图持续记录视频中的人物、地点、物体及关系变化，引入计划agent，结合结构化检索与跨模态（视觉与音频）推理，支持详细、时间连贯的推理。

Result: 在EgoLifeQA数据集上达到57.5%的先进水平，在Video-MME(Long)数据集上取得74.1%的有竞争力成绩，均超越或接近当前最新方法。

Conclusion: 实体场景图+agentic规划体系能有效支持长时程、多模态、自中心视频理解，为个人AI助手带来更强上下文理解、回忆与推理能力。

Abstract: The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.

</details>


### [115] [TempDiffReg: Temporal Diffusion Model for Non-Rigid 2D-3D Vascular Registration](https://arxiv.org/abs/2601.18168)
*Zehua Liu,Shihao Zou,Jincai Huang,Yanfang Zhang,Chao Tong,Weixin Si*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的2D-3D血管配准方法，以提升肝癌等肝脏恶性肿瘤TACE（经动脉化疗栓塞术）的导航精度，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: TACE手术对肝脏肿瘤的治疗效果显著，但由于血管结构复杂、个体解剖变异大，手术中的血管导航面临极大挑战。现有的2D-3D血管配准方法在准确率和鲁棒性方面仍有较大发展空间，因此亟待改善。

Method: 作者提出由两部分组成的coarse-to-fine配准策略。第一，提出全局对齐模块SA-PnP，实现2D-3D血管结构之间的结构感知匹配；第二，创新性地设计了TempDiffReg，基于时序扩散模型，通过利用时序上下文实现血管变形建模，从而更好地捕捉解剖变化与局部结构差异。方法在23例患者、共626组多帧配对数据集上进行了综合评估。

Result: 与现有最优方法相比，新方法在配准准确度和解剖合理性方面均显著提升。实验中，该方法的均方误差（MSE）为0.63mm，平均绝对误差（MAE）为0.51mm，分别比最优秀的现有方法降低了66.7%和17.7%。

Conclusion: 该方法为TACE等复杂手术提供了更为准确、稳定的血管配准工具，有望帮助经验不足的临床医生更安全、高效地完成手术，改善临床疗效与患者预后，其代码与数据已公开。

Abstract: Transarterial chemoembolization (TACE) is a preferred treatment option for hepatocellular carcinoma and other liver malignancies, yet it remains a highly challenging procedure due to complex intra-operative vascular navigation and anatomical variability. Accurate and robust 2D-3D vessel registration is essential to guide microcatheter and instruments during TACE, enabling precise localization of vascular structures and optimal therapeutic targeting. To tackle this issue, we develop a coarse-to-fine registration strategy. First, we introduce a global alignment module, structure-aware perspective n-point (SA-PnP), to establish correspondence between 2D and 3D vessel structures. Second, we propose TempDiffReg, a temporal diffusion model that performs vessel deformation iteratively by leveraging temporal context to capture complex anatomical variations and local structural changes. We collected data from 23 patients and constructed 626 paired multi-frame samples for comprehensive evaluation. Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art (SOTA) methods in both accuracy and anatomical plausibility. Specifically, our method achieves a mean squared error (MSE) of 0.63 mm and a mean absolute error (MAE) of 0.51 mm in registration accuracy, representing 66.7\% lower MSE and 17.7\% lower MAE compared to the most competitive existing approaches. It has the potential to assist less-experienced clinicians in safely and efficiently performing complex TACE procedures, ultimately enhancing both surgical outcomes and patient care. Code and data are available at: \textcolor{blue}{https://github.com/LZH970328/TempDiffReg.git}

</details>


### [116] [YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection](https://arxiv.org/abs/2601.18172)
*Lin Huang,Yujuan Tan,Weisheng Li,Shitai Shan,Liu Liu,Bo Liu,Linlin Shen,Jing Yu,Yue Niu*

Main category: cs.CV

TL;DR: 本文提出了YOLO-DS，通过引入双统计协同算子（DSO）及相关模块，提高了YOLO系列目标检测器对异质目标的识别能力，在保持高效率的同时实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有YOLO目标检测器未能显式建模共享通道中异质目标的响应，限制了性能提升，因此需要新的方法提升特征区分能力。

Method: 提出了Dual-Statistic Synergy Operator（DSO），对特征的均值和峰值-均值差进行联合建模，同时设计了Dual-Statistic Synergy Gating（DSG）模块和Multi-Path Segmented Gating（MSG）模块，实现自适应通道选择和深度加权。

Result: 在MS-COCO数据集上，YOLO-DS在五种模型规模下（N, S, M, L, X）AP提升1.1%-1.7%，推理延迟仅稍微增加。

Conclusion: YOLO-DS能高效区分异质目标，兼顾检测精度和速度，效果优于YOLOv8。

Abstract: One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.

</details>


### [117] [\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation](https://arxiv.org/abs/2601.18188)
*Weiye Zhu,Zekai Zhang,Xiangchen Wang,Hewei Pan,Teng Wang,Tiantian Geng,Rongtao Xu,Feng Zheng*

Main category: cs.CV

TL;DR: 该论文提出了NaVIDA框架，通过结合视觉-动作因果关系建模和自适应策略执行，有效提升了Vision-and-Language Navigation (VLN)任务中智能体的导航表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLN方法大多直接将视觉和语言输入映射到动作，未显式建模动作对视觉变化的因果影响，导致导航过程中难以预测自己动作带来的视觉后果，容易积累误差并影响泛化能力和稳定性。

Method: 提出NaVIDA框架，将策略学习与基于动作的视觉动态建模结合。具体方法是用块状逆动力学监督学习视觉变化与动作之间的因果关系，并通过分层概率动作分块（HPAC）结构化监督和扩展规划范围。同时，引入熵引导机制，在推理时自适应地设置动作块的执行范围，以减少误差累积。

Result: 在多项实验中，NaVIDA在参数更少的情况下（3B vs 8B）导航性能优于当前最先进方法，且实际机器人实验验证了其可行性与有效性。

Conclusion: 通过结合视觉-动作因果关系建模、分块分层策略和自适应动作执行机制，NaVIDA有效提升了智能体在VLN任务上的导航效果，并具备良好的泛化和实际应用能力。

Abstract: Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \textsc{NaVIDA} (\textbf{Nav}igation with \textbf{I}nverse \textbf{D}ynamics \textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance.

</details>


### [118] [Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval](https://arxiv.org/abs/2601.18190)
*Yifan Li,Shiying Wang,Jianqiang Huang*

Main category: cs.CV

TL;DR: 本文提出了MPS-CLIP框架，实现对遥感图像-文本检索的高效、细粒度匹配，获得了显著领先的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的遥感图像-文本检索方法主要依赖粗粒度的全局对齐，难以捕捉遥感影像中的多尺度、密集语义信息。此外，微调现有大模型计算代价高且易遗忘原有能力。因此迫切需要更高效、细致的图文检索方法。

Method: 1）利用大语言模型（LLM）抽取文本核心语义关键词，引导Segment Anything Model（SamGeo）分割出图像的相关语义区域；2）提出G^2A适配器，低成本捕捉全局上下文信息和长距离依赖，并以冻结骨干方式高效适配；3）通过多视角表示模块（MPR）整合局部线索得到鲁棒多视角特征；4）结合多视角对比损失和加权三元组损失优化框架。

Result: 在RSICD和RSITMD两个遥感检索基准数据集上，MPS-CLIP分别取得了35.18%和48.40%的平均召回率（mR），均大幅超越全量微调模型及现有最新对比方法。

Conclusion: MPS-CLIP提出了高效的、关键词引导的细粒度多视角图文对齐方法，在遥感图文检索任务上展现了优越性，并显著降低了参数量和计算成本，且具有良好的推广潜力。

Abstract: Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.

</details>


### [119] [MindCine: Multimodal EEG-to-Video Reconstruction with Large-Scale Pretrained Models](https://arxiv.org/abs/2601.18192)
*Tian-Yi Zhou,Xuan-Hao Liu,Bao-Liang Lu,Wei-Long Zheng*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法MindCine，通过多模态联合学习和预训练大模型，有效提升了脑电（EEG）信号到视频重建的效果，尤其在数据有限的情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: EEG信号对人类动态视觉感知的重建具有重要意义，但现有方法仅与文本对齐、易过拟合，且受限于EEG-视频数据匮乏，训练难以收敛。

Method: 提出MindCine框架，采用多模态联合学习策略，将文本以外的模态引入训练阶段，并结合预训练大规模EEG模型缓解数据匮乏，使用带因果注意力的Seq2Seq模型分别解码语义和感知信息。

Result: 实验结果显示，该方法在主观和客观指标上均优于现有主流方法，多模态信息和大规模预训练EEG模型相结合有效提升了重建性能。

Conclusion: 引入多模态联合学习和大规模预训练EEG模型能够突破单一模态和数据有限的瓶颈，为EEG到视频的高保真重建提供了更优解决方案。

Abstract: Reconstructing human dynamic visual perception from electroencephalography (EEG) signals is of great research significance since EEG's non-invasiveness and high temporal resolution. However, EEG-to-video reconstruction remains challenging due to: 1) Single Modality: existing studies solely align EEG signals with the text modality, which ignores other modalities and are prone to suffer from overfitting problems; 2) Data Scarcity: current methods often have difficulty training to converge with limited EEG-video data. To solve the above problems, we propose a novel framework MindCine to achieve high-fidelity video reconstructions on limited data. We employ a multimodal joint learning strategy to incorporate beyond-text modalities in the training stage and leverage a pre-trained large EEG model to relieve the data scarcity issue for decoding semantic information, while a Seq2Seq model with causal attention is specifically designed for decoding perceptual information. Extensive experiments demonstrate that our model outperforms state-of-the-art methods both qualitatively and quantitatively. Additionally, the results underscore the effectiveness of the complementary strengths of different modalities and demonstrate that leveraging a large-scale EEG model can further enhance reconstruction performance by alleviating the challenges associated with limited data.

</details>


### [120] [QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding](https://arxiv.org/abs/2601.18195)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Kaiwei Zhang,Jun Jia,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉质量评估（VQA）框架QualiRAG，通过检索增强生成方法，无需训练即可利用大规模多模态模型的潜在知识，实现更为细致和可解释的质量理解。


<details>
  <summary>Details</summary>
Motivation: 现有视觉质量评估方法依赖监督微调或强化学习，需要大量人工标注且易受数据集偏见影响，难以满足对细粒度、可解释性和上下文感知的需求。

Method: QualiRAG框架在无需训练的情况下，动态将问题分解为结构化请求，并构建四种知识源（视觉元数据、主体定位、全局质量摘要、本地质量描述），然后根据相关性检索辅助信息，为推理决策提供证据支持。

Result: 实验表明，QualiRAG在视觉质量理解任务上显著优于未专门微调的多模态大模型和经过VQA微调的模型，在视觉质量比较任务上也达到有竞争力的表现。

Conclusion: QualiRAG无需针对特定任务训练即可实现鲁棒且可解释的视觉质量评估，为基于大模型的无监督感知任务提供了新思路。

Abstract: Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.

</details>


### [121] [HomoFM: Deep Homography Estimation with Flow Matching](https://arxiv.org/abs/2601.18222)
*Mengfan He,Liangzheng Sun,Chunyu Li,Ziyang Meng*

Main category: cs.CV

TL;DR: 本文提出了一种新的同伦变换估计算法HomoFM，创新性地将流匹配技术引入同伦估计任务，有效提升了估计精度和对不同领域的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管深度同伦估计领域取得了显著进展，但现有方法多为直接回归或迭代优化，难以捕捉复杂几何变换，且对不同领域的数据泛化能力较弱。

Method: 作者提出了一种新框架HomoFM，将生成建模中的流匹配(flow matching)方法首次引入同伦变换估计问题。方法将同伦估计建模为速度场学习任务，通过学习连续的点对点速度场，实现从噪声分布到标准坐标的高精度变换。同时，采用梯度反转层(GRL)增强特征提取过程，使编码器获得领域不变的特征，提高对多模态匹配或光照变化下的鲁棒性。

Result: 在标准数据集上进行大量实验，结果显示HomoFM在估计精度和鲁棒性方面均优于当前主流方法。

Conclusion: HomoFM通过流匹配与领域自适应策略显著提高了同伦估计算法的性能，并增强了其在多领域场景下的泛化和鲁棒能力。

Abstract: Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network's robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at https://github.com/hmf21/HomoFM.

</details>


### [122] [Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach](https://arxiv.org/abs/2601.18228)
*Sahil Naik,Soham Bagayatkar,Pavankumar Singh*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级且高效的人脸表情识别方法，在低质量、复杂环境下表现优异，参数量显著低于主流大模型，且准确率可观。


<details>
  <summary>Details</summary>
Motivation: 现实场景下的人脸表情识别受图像质量、噪声标注、类别不平衡等因素影响，传统VGG/ResNet等大模型虽准确但难以应用于实时场景，亟需高效轻量方案。

Method: 基于EfficientNetB2，采用两阶段warm-up和微调训练策略，引入AdamW优化器、标签平滑、类权重裁剪、dropout、混合精度训练和实时数据增强，并用分层数据划分进行训练。

Result: 在FER-2013数据集上，测试准确率达到68.78%，参数量约为VGG16的十分之一，各类指标和学习曲线表现出良好泛化和稳定性，训练高效。

Conclusion: 所提方法兼具高效性和准确性，非常适合对资源敏感的实时或边缘设备场景下的人脸表情识别任务。

Abstract: Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.

</details>


### [123] [V-Loop: Visual Logical Loop Verification for Hallucination Detection in Medical Visual Question Answering](https://arxiv.org/abs/2601.18240)
*Mengyuan Jin,Zehui Liao,Yong Xia*

Main category: cs.CV

TL;DR: 本文提出了V-Loop框架，通过视觉逻辑闭环检测医学多模态大模型回答中的幻觉，有效提升了医学视觉问答（VQA）场景下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型在医学诊断领域表现优秀，但是其回答仍容易出现与事实不符的幻觉答案，这在医疗高风险环境中极具危害。现有主要基于不确定性的检测方法效率高但无法直接验证回答的事实准确性。为解决这一局限，作者提出新的检测思路。

Method: V-Loop无需额外训练，可直接插入现有流程。其核心是提出一种视觉逻辑闭环：首先由多模态大模型生成答案，随后分析问答对的语义单元，对答案相关单元反向提问并利用视觉注意力保持一致性，通过验证环是否闭合（即验证答案语义内容的一致性），判断该回答是否事实扎实。

Result: 在多个医学VQA基准和不同的多模态大模型中，V-Loop检测幻觉的表现明显优于现有检测方法，并且计算效率高。当与不确定性方法结合时，检测性能进一步提升。

Conclusion: V-Loop为医学多模态大模型问答中的幻觉检测提供了高效、训练无关且表现卓越的新路径，有助于提升医学AI系统的安全性和可靠性。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable capability in assisting disease diagnosis in medical visual question answering (VQA). However, their outputs remain vulnerable to hallucinations (i.e., responses that contradict visual facts), posing significant risks in high-stakes medical scenarios. Recent introspective detection methods, particularly uncertainty-based approaches, offer computational efficiency but are fundamentally indirect, as they estimate predictive uncertainty for an image-question pair rather than verifying the factual correctness of a specific answer. To address this limitation, we propose Visual Logical Loop Verification (V-Loop), a training-free and plug-and-play framework for hallucination detection in medical VQA. V-Loop introduces a bidirectional reasoning process that forms a visually grounded logical loop to verify factual correctness. Given an input, the MLLM produces an answer for the primary input pair. V-Loop extracts semantic units from the primary QA pair, generates a verification question by conditioning on the answer unit to re-query the question unit, and enforces visual attention consistency to ensure answering both primary question and verification question rely on the same image evidence. If the verification answer matches the expected semantic content, the logical loop closes, indicating factual grounding; otherwise, the primary answer is flagged as hallucinated. Extensive experiments on multiple medical VQA benchmarks and MLLMs show that V-Loop consistently outperforms existing introspective methods, remains highly efficient, and further boosts uncertainty-based approaches when used in combination.

</details>


### [124] [Vision-Language-Model-Guided Differentiable Ray Tracing for Fast and Accurate Multi-Material RF Parameter Estimation](https://arxiv.org/abs/2601.18242)
*Zerui Kang,Yishen Lim,Zhouyou Gu,Seung-Woo Ko,Tony Q. S. Quek,Jihong Park*

Main category: cs.CV

TL;DR: 提出了一种利用视觉语言模型（VLM）辅助的射线追踪方法，加速并稳定多材料参数估计，大幅提升准确率并降低收敛时间。


<details>
  <summary>Details</summary>
Motivation: 在6G系统电磁仿真中，需要高精度的射频材料参数。传统的基于梯度的反演射线追踪既对初值敏感又计算代价高，尤其是在测量有限的情况下，因此亟需更高效、鲁棒的参数估计方法。

Method: 构建了以VLM为核心的参数估计算法。首先，VLM对场景图像进行解析，识别材料类别并据ITU-R表给出合理的电导率初始值。VLM还辅助选择有信息量的收发机位置，优化路径的材质区分能力。此后用可微射线追踪（DRT）结合接收信号强度测量，基于梯度进一步优化材料参数。

Result: 在NVIDIA Sionna平台室内仿真实验中，新方法收敛速度提升2-4倍，最终参数误差比随机或均匀初始化低10-100倍，仅需少数接收器即可达到低于0.1%的平均相对误差。算法复杂度随材料、测量数量近线性增加，VLM优化降低了所需测量次数。消融实验显示，提升射线深度和数量能带来额外精度提升，且不会增加单次迭代的开销。

Conclusion: 基于VLM的语义先验能有效指导物理优化，大幅提升射频材料参数估计的速度和可靠性，为6G电磁数字孪生提供了强有力的技术支持。

Abstract: Accurate radio-frequency (RF) material parameters are essential for electromagnetic digital twins in 6G systems, yet gradient-based inverse ray tracing (RT) remains sensitive to initialization and costly under limited measurements. This paper proposes a vision-language-model (VLM) guided framework that accelerates and stabilizes multi-material parameter estimation in a differentiable RT (DRT) engine. A VLM parses scene images to infer material categories and maps them to quantitative priors via an ITU-R material table, yielding informed conductivity initializations. The VLM further selects informative transmitter/receiver placements that promote diverse, material-discriminative paths. Starting from these priors, the DRT performs gradient-based refinement using measured received signal strengths. Experiments in NVIDIA Sionna on indoor scenes show 2-4$\times$ faster convergence and 10-100$\times$ lower final parameter error compared with uniform or random initialization and random placement baselines, achieving sub-0.1\% mean relative error with only a few receivers. Complexity analyses indicate per-iteration time scales near-linearly with the number of materials and measurement setups, while VLM-guided placement reduces the measurements required for accurate recovery. Ablations over RT depth and ray counts confirm further accuracy gains without significant per-iteration overhead. Results demonstrate that semantic priors from VLMs effectively guide physics-based optimization for fast and reliable RF material estimation.

</details>


### [125] [A multimodal vision foundation model for generalizable knee pathology](https://arxiv.org/abs/2601.18250)
*Kang Yu,Dingyu Wang,Zimu Yuan,Nan Zhou,Jiajun Liu,Jiaxin Liu,Shanggui Liu,Yaoyan Zheng,Huishu Yuan,Di Huang,Dong Jiang*

Main category: cs.CV

TL;DR: 本论文介绍了OrthoFoundation，一种面向肌肉骨骼影像的多模态基础模型，具备自监督学习能力，解决了传统方法对大量标注数据和泛化能力不足的问题，并在多个任务上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 当前骨科医学影像的AI主要依赖于监督学习，存在任务割裂，数据标注量大且泛化性差等问题。同时，缺乏大规模开放肌肉骨骼数据集限制了基础模型的发展。该研究旨在建立通用、可扩展的基础模型，提升自动诊断精度并减少人工标注负担。

Method: 作者构建了包含120万张膝关节X光和MRI影像的大规模预训练数据集，利用Dinov3主干网络和自监督对比学习方法提取影像表征。模型在下游任务中进行了广泛测试，并评估了其在不同解剖结构间的泛化能力。

Result: OrthoFoundation在14项下游任务中均取得SOTA表现，在X光骨关节炎诊断和MRI结构损伤检测任务上精度最高，仅用50%标注样本即可达到有监督基线。同时，虽仅在膝部预训练，但对髋、肩、踝等部位也具有出色的泛化能力。

Conclusion: OrthoFoundation实现了基于大规模、多模态数据的通用肌肉骨骼影像AI模型，显著减轻了标注负担，增强了临床诊断准确性，为该领域AI技术应用提供了强有力支持。

Abstract: Musculoskeletal disorders represent a leading cause of global disability, creating an urgent demand for precise interpretation of medical imaging. Current artificial intelligence (AI) approaches in orthopedics predominantly rely on task-specific, supervised learning paradigms. These methods are inherently fragmented, require extensive annotated datasets, and often lack generalizability across different modalities and clinical scenarios. The development of foundation models in this field has been constrained by the scarcity of large-scale, curated, and open-source musculoskeletal datasets. To address these challenges, we introduce OrthoFoundation, a multimodal vision foundation model optimized for musculoskeletal pathology. We constructed a pre-training dataset of 1.2 million unlabeled knee X-ray and MRI images from internal and public databases. Utilizing a Dinov3 backbone, the model was trained via self-supervised contrastive learning to capture robust radiological representations. OrthoFoundation achieves state-of-the-art (SOTA) performance across 14 downstream tasks. It attained superior accuracy in X-ray osteoarthritis diagnosis and ranked first in MRI structural injury detection. The model demonstrated remarkable label efficiency, matching supervised baselines using only 50% of labeled data. Furthermore, despite being pre-trained on knee images, OrthoFoundation exhibited exceptional cross-anatomy generalization to the hip, shoulder, and ankle. OrthoFoundation represents a significant advancement toward general-purpose AI for musculoskeletal imaging. By learning fundamental, joint-agnostic radiological semantics from large-scale multimodal data, it overcomes the limitations of conventional models, which provides a robust framework for reducing annotation burdens and enhancing diagnostic accuracy in clinical practice.

</details>


### [126] [Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing](https://arxiv.org/abs/2601.18252)
*Chao Wang,Xuanying Li,Cheng Dai,Jinglei Feng,Yuxiang Luo,Yuqi Ouyang,Hao Qin*

Main category: cs.CV

TL;DR: 本论文提出了Co-PLNet，一种点-线协同的神经网络框架，通过空间提示促进线段与交点的协同解析，提升线框解析的准确性与鲁棒性，并在主流数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有线框解析方法大多将线段与交点分开预测，最后再进行匹配和融合，这易导致不一致及识别鲁棒性下降，影响后续如SLAM等任务的表现。为提升结构化几何解析的准确性与稳健性，亟需一种点-线协同效应强的新方法。

Method: 作者提出了Co-PLNet框架。首先利用PLP-Encoder将早期检测出的点和线转换为紧凑的、空间对齐的提示（prompts），对其几何信息进行编码。随后用CGL-Decoder利用稀疏注意力机制与互补提示进行线段和交点的联合推理，实现点-线信息的双向约束与协同优化。

Result: 在Wireframe和YorkUrban两个数据集上评测，Co-PLNet在准确率、鲁棒性等指标上都取得了持续提升，并具备较好的实时性。

Conclusion: Co-PLNet有效整合并提升了线段与交点的协同解析能力，为结构化几何感知任务提供了更准确、高效的方法，具备良好应用前景。

Abstract: Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.

</details>


### [127] [Depth to Anatomy: Learning Internal Organ Locations from Surface Depth Images](https://arxiv.org/abs/2601.18260)
*Eytan Kats,Kai Geissler,Daniel Mensing,Jochen G. Hirsch,Stefan Heldman,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: 本文提出了一种基于RGB-D相机获取深度信息的方法，通过学习框架直接从单张二维体表深度图像预测体内多个器官的三维位置和形状，实现自动患者摆位。


<details>
  <summary>Details</summary>
Motivation: 当前自动患者摆位对于优化扫描流程与提升病人通量非常关键，而如何准确、高效地估算体内器官位置仍是难题。传统方式依赖人工干预或表面重建效率低，作者希望借助深度图像与深度学习，提升定位精准度及自动化水平。

Method: 作者构建了以全身MRI配对深度图和解剖分割为基础的大规模数据集，利用卷积神经网络从单张体表深度图像直接回归预测多个内脏器官的三维位置与形状，无需表面重建。

Result: 该方法能精准定位包括骨骼和软组织在内的多种体内解剖结构。实验结果显示方法准确有效，具备在临床放射科自动化流程中的应用潜力。

Conclusion: 利用深度相机与学习框架可提升自动患者摆位精准度和效率，有望简化放射科扫描流程，优化患者体验。

Abstract: Automated patient positioning plays an important role in optimizing scanning procedure and improving patient throughput. Leveraging depth information captured by RGB-D cameras presents a promising approach for estimating internal organ positions, thereby enabling more accurate and efficient positioning. In this work, we propose a learning-based framework that directly predicts the 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Utilizing a large-scale dataset of full-body MRI scans, we synthesize depth images paired with corresponding anatomical segmentations to train a unified convolutional neural network architecture. Our method accurately localizes a diverse set of anatomical structures, including bones and soft tissues, without requiring explicit surface reconstruction. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows to streamline scanning procedures and enhance patient experience through automated patient positioning.

</details>


### [128] [Revisiting Aerial Scene Classification on the AID Benchmark](https://arxiv.org/abs/2601.18263)
*Subhajeet Das,Susmita Ghosh,Abhiroop Chatterjee*

Main category: cs.CV

TL;DR: 本文综述了航拍影像分类的多种机器学习方法，并提出了一种新型空间注意力多尺度融合的CNN模型，在AID数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 航拍图像具有高度异质性，包含多种结构与场景，对于城市规划与环境保护至关重要。然而，场景复杂且类别多样，现有模型在分类精度和鲁棒性方面仍有挑战，因此有必要系统梳理相关方法并提出更有效的模型。

Method: 首先系统评述了从手工特征（如SIFT、LBP）、传统CNN（如VGG、GoogLeNet）到深度混合网络的主流方法。随后设计了Aerial-Y-Net模型，结合空间注意力机制和多尺度特征融合，加强模型对复杂场景的理解能力。

Result: 在AID数据集上，所提出的Aerial-Y-Net模型达到了91.72%的分类精度，优于多种主流基准架构。

Conclusion: 提出的Aerial-Y-Net模型能更好地处理航拍图像多样性与复杂性，在场景分类任务中表现突出，表明结合注意力机制和多尺度特征对于提升航拍图像理解具有显著效果。

Abstract: Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures.

</details>


### [129] [Contextual Range-View Projection for 3D LiDAR Point Clouds](https://arxiv.org/abs/2601.18301)
*Seyedali Mousavi,Seyedhamidreza Mousavi,Masoud Daneshtalab*

Main category: cs.CV

TL;DR: 本文提出了两种改进3D激光雷达点云投影到2D视图的方法，有效缓解多点投影冲突，提高分割表现。


<details>
  <summary>Details</summary>
Motivation: 现有将3D点云投影到2D图片的方法多采用保留最小深度点的策略, 忽视了点的语义相关性和物体结构，导致关键信息丢失，影响下游任务效果。

Method: 提出了两种新机制：1) Centerness-Aware Projection (CAP)，按点到实例中心的距离调整深度，优先保留靠近中心的点，减少边界噪声点；2) Class-Weighted-Aware Projection (CWAP)，对不同类别给予自定义权重，实现更灵活的点选择策略。

Result: 在SemanticKITTI数据集上，CAP方法能在投影时保留更多实例中心点，IoU最大提升3.1%；CWAP对目标类别的表现有提升，对其他类别影响极小。

Conclusion: 引入语义和结构信息的投影选择机制，可以有效缓解多点冲突问题，提升分割等下游任务效果。

Abstract: Range-view projection provides an efficient method for transforming 3D LiDAR point clouds into 2D range image representations, enabling effective processing with 2D deep learning models. However, a major challenge in this projection is the many-to-one conflict, where multiple 3D points are mapped onto the same pixel in the range image, requiring a selection strategy. Existing approaches typically retain the point with the smallest depth (closest to the LiDAR), disregarding semantic relevance and object structure, which leads to the loss of important contextual information. In this paper, we extend the depth-based selection rule by incorporating contextual information from both instance centers and class labels, introducing two mechanisms: \textit{Centerness-Aware Projection (CAP)} and \textit{Class-Weighted-Aware Projection (CWAP)}. In CAP, point depths are adjusted according to their distance from the instance center, thereby prioritizing central instance points over noisy boundary and background points. In CWAP, object classes are prioritized through user-defined weights, offering flexibility in the projection strategy. Our evaluations on the SemanticKITTI dataset show that CAP preserves more instance points during projection, achieving up to a 3.1\% mIoU improvement compared to the baseline. Furthermore, CWAP enhances the performance of targeted classes while having a negligible impact on the performance of other classes

</details>


### [130] [SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis](https://arxiv.org/abs/2601.18305)
*Xuan Wang,Siyuan Su,Quantong Fu,Yongxiang Hu,Yangfan Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的自动化管道SwipeGen，用于合成更贴近人类的滑动交互，并据此首次构建了GUI代理滑动能力的评测基准。同时，提出了GUISwiper代理系统，其滑动执行准确率远超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GUI自动化代理大多专注于提升界面感知能力，把任务指令转化为具体操作步骤，但执行滑动等具体操作的能力仍是瓶颈，尤其现有方法对滑动交互的处理过于简化，与真实人类行为差距较大。

Method: 作者将人类滑动手势分解成多个可量化维度，通过GUI自动探索提出了SwipeGen管道，自动生成高仿真的滑动交互数据。并据此构建了滑动能力评测基准，同时利用合成数据训练了GUISwiper系统，以提升代理的执行能力。

Result: 实验显示，GUISwiper滑动执行准确率达到69.07%，相比现有视觉语言模型（VLM）基线提升了214%，显著优于现有方法。

Conclusion: 通过提出SwipeGen和相应评测基准，显著提升了GUI代理滑动执行智能，首次系统性解决了滑动动作执行这一瓶颈，为自动化交互代理迈向人类水平提供了重要支撑。

Abstract: With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines.

</details>


### [131] [A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification](https://arxiv.org/abs/2601.18330)
*Muhammad Ali Shah,Muhammad Mansoor Alam,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 本文提出了一种高效的脑肿瘤MRI分析框架EDSH，融合DenseNet和Swin Transformer，能够有效捕捉纹理和全局信息，并在大规模数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的CNN或单纯Transformer模型在脑肿瘤形态和纹理复杂性分析方面存在局限，尤其难以同时捕捉细粒度纹理和长距离依赖关系，因此需要更高效的混合架构应对不同类型脑肿瘤的诊断挑战。

Method: 提出EDSH混合架构，包括两个新实验设置：1）Boosted Feature Space（BFS）将定制DenseNet和Swin Transformer分支学习互补局部与全局特征，再融合提升，增强弥漫性胶质瘤检出能力；2）层级DenseNet Swint结构结合深层特征提取与双重残差连接，DenseNet负责局部结构，Swin_t建模全局形态，有效减少脑膜瘤和垂体瘤误检。所有分支均结合MRI影像特点设计特征提取模块。

Result: 在包含四种肿瘤的大规模MRI数据集（40,260张图像）上，EDSH表现优于单独的CNN、纯Transformer及混合结构，测试集准确率和召回率均达98.50%。

Conclusion: EDSH框架可高效解析MRI脑肿瘤复杂特征，特别能应对不同类型肿瘤的异质性与形态差异，对提升脑肿瘤自动诊断的性能有显著价值。

Abstract: This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.

</details>


### [132] [PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction](https://arxiv.org/abs/2601.18336)
*Isaac Deutsch,Nicolas Moënne-Loccoz,Gavriel State,Zan Gojcic*

Main category: cs.CV

TL;DR: 本文提出了一种物理可解释的图像信号处理（ISP）校正模块PPISP，用于提升多视图3D重建方法在面对摄像头光学和ISP差异下的鲁棒性。该方法在无真实图像的情况下，对新视角实现更真实与公平的重建，同时取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 多视图3D重建在真实场景中会受到不同摄像头及其处理算法带来的色彩、亮度等光度不一致问题影响，导致重建结果不理想。现有的对策（如逐帧潜变量或仿射颜色校正）缺乏物理依据，泛化到新视角能力较差。论文动机是设计一种基于物理解释的方法，摆脱对真实图像的依赖，提升新视角下的重建表现。

Method: 提出了PPISP校正模块，通过物理可解释的变换，分离摄像头内在属性和采集依赖效应。同时设计了一个PPISP控制器，可根据输入视图学习参数并预测新视角的ISP参数，类似真实相机的自动曝光和自动白平衡，使方法对元数据具有兼容性。

Result: 方法在标准多视图3D重建基准上实现了最新的性能，同时支持直观控制和可集成元数据特性，表现优于现有补救策略。

Conclusion: PPISP模块提供了一种兼顾物理可解释性和实际效果的方法，实现了在新视角下无须真实图像的真实且公平重建评估，提升了多视图3D重建的实用性和泛化性。

Abstract: Multi-view 3D reconstruction methods remain highly sensitive to photometric inconsistencies arising from camera optical characteristics and variations in image signal processing (ISP). Existing mitigation strategies such as per-frame latent variables or affine color corrections lack physical grounding and generalize poorly to novel views. We propose the Physically-Plausible ISP (PPISP) correction module, which disentangles camera-intrinsic and capture-dependent effects through physically based and interpretable transformations. A dedicated PPISP controller, trained on the input views, predicts ISP parameters for novel viewpoints, analogous to auto exposure and auto white balance in real cameras. This design enables realistic and fair evaluation on novel views without access to ground-truth images. PPISP achieves SoTA performance on standard benchmarks, while providing intuitive control and supporting the integration of metadata when available. The source code is available at: https://github.com/nv-tlabs/ppisp

</details>


### [133] [Beyond Rigid: Benchmarking Non-Rigid Video Editing](https://arxiv.org/abs/2601.18340)
*Bingzheng Qu,Kehai Chen,Xuefeng Bai,Jun Yu,Min Zhang*

Main category: cs.CV

TL;DR: 该论文提出了NRVBench，这是首个专用于评估非刚性视频编辑的基准，包括了高质量的视频数据集、细致的任务指令、以及多项选择题，并开发了新评测指标和无训练基线方法。实验证明他们方法在物理合理性和指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管文本驱动的视频编辑进展显著，但现有方法在生成非刚性形变时常常出现物理失真和时序闪烁，缺乏物理一致性和复杂动态的有效评估方式。该论文旨在解决这些问题。

Method: 1. 构建NRVBench数据集，包括180段来自六大类物理特性的非刚性运动视频，配备详细任务和测试；2. 提出基于视觉-语言模型的新评测指标NRVE-Acc，能评估物理一致性、时间连续性、和指令对齐度；3. 推出无训练基线方法VM-Edit，通过双区域去噪实现结构感知的编辑控制。

Result: 实验表明，现有方法在物理合理性上存在不足，而本文提出的方法兼顾结构保持与动态形变，在标准和新提出的指标上均有优异表现。

Conclusion: NRVBench可成为推动物理感知视频编辑的重要标准测试平台，论文方法能够有效提升非刚性编辑的物理一致性和时序表现。

Abstract: Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.

</details>


### [134] [Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception](https://arxiv.org/abs/2601.18346)
*Sijing Wu,Yunhao Li,Zicheng Zhang,Qi Jia,Xinyue Li,Huiyu Duan,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了Q-Bench-Portrait，这是首个专注于人像图像质量感知的全面性基准测试集，用于系统评估多模态大模型在该领域的表现，发现在人像图像感知方面当前模型还有明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型主要在通用图像的低层次视觉任务上表现突出，但针对结构和感知特性独特的人像图像的感知能力尚未被系统评估和挖掘。缺乏专门的人像图像质量评价基准，限制了相关研究和模型改进。

Method: 作者构建了Q-Bench-Portrait基准，包含2765组图像-问题-答案三元组，题材涵盖自然、人工失真、AI生成、艺术和计算机图形等多样的人像图片，评价维度涵盖技术失真、AIGC专属失真与美学，问题类型多样（单选、多选、判断、开放问答），兼顾整体与局部。利用该基准测试了25种主流开源/闭源多模态大模型。

Result: 评测结果显示，虽然部分模型在某些人像图像感知任务表现尚可，但整体表现仍有限，准确性和精细度与人类评判存在明显差距。

Conclusion: Q-Bench-Portrait为人像图像感知领域提供了系统的评测工具，揭示出当前MLLMs在此方向的不足，期待推动通用或领域专用模型在人像图像感知能力上的进一步提升。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs.

</details>


### [135] [OREHAS: A fully automated deep-learning pipeline for volumetric endolymphatic hydrops quantification in MRI](https://arxiv.org/abs/2601.18368)
*Caterina Fuster-Barceló,Claudia Castrillón,Laura Rodrigo-Muñoz,Victor Manuel Vega-Suárez,Nicolás Pérez-Fernández,Gorka Bastarrika,Arrate Muñoz-Barrutia*

Main category: cs.CV

TL;DR: 本文提出了一种名为OREHAS的自动化技术，可从常规MRI数据中实现前庭水肿体积的全自动定量。


<details>
  <summary>Details</summary>
Motivation: 目前，内淋巴水肿的体积定量主要依赖半自动或手工方法，操作繁琐且主观性强，限制了大规模临床或研究应用。作者希望实现无需手动干预的全自动定量流程，从而提高精度、一致性和可扩展性。

Method: OREHAS包含切片分类、内耳定位和序列特异性分割三个深度学习模块，能从全头MRI直接计算每只耳朵的体积比值。训练仅需每例3-6张有标注切片，却能泛化到完整3D体积。通过与专家手工标注及主流软件对比，验证其准确性和鲁棒性。

Result: 在独立测试集上，OREHAS Dice分数高达0.90（SPACE-MRC）和0.75（REAL-IR）。与专家人工标注量化结果高度一致（VSI=74.3%），显著优于临床常用的syngo.via软件（VSI=42.5%）。测量更加生理可信，且对操作者依赖少、重现性好。

Conclusion: OREHAS无需复杂手工操作，可兼容主流影像协议、高效完成内耳水肿的精准体积定量，适用于大样本研究，也为临床诊断阈值的重新校准提供坚实数据基础。

Abstract: We present OREHAS (Optimized Recognition & Evaluation of volumetric Hydrops in the Auditory System), the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops (EH) from routine 3D-SPACE-MRC and 3D-REAL-IR MRI. The system integrates three components -- slice classification, inner ear localization, and sequence-specific segmentation -- into a single workflow that computes per-ear endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes, eliminating the need for manual intervention.
  Trained with only 3 to 6 annotated slices per patient, OREHAS generalized effectively to full 3D volumes, achieving Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In an external validation cohort with complete manual annotations, OREHAS closely matched expert ground truth (VSI = 74.3%) and substantially outperformed the clinical syngo.via software (VSI = 42.5%), which tended to overestimate endolymphatic volumes. Across 19 test patients, vestibular measurements from OREHAS were consistent with syngo.via, while endolymphatic volumes were systematically smaller and more physiologically realistic.
  These results show that reliable and reproducible EH quantification can be achieved from standard MRI using limited supervision. By combining efficient deep-learning-based segmentation with a clinically aligned volumetric workflow, OREHAS reduces operator dependence, ensures methodological consistency. Besides, the results are compatible with established imaging protocols. The approach provides a robust foundation for large-scale studies and for recalibrating clinical diagnostic thresholds based on accurate volumetric measurements of the inner ear.

</details>


### [136] [Gaze Prediction in Virtual Reality Without Eye Tracking Using Visual and Head Motion Cues](https://arxiv.org/abs/2601.18372)
*Christos Petrou,Harris Partaourides,Athanasios Balomenos,Yannis Kopsinis,Sotirios Chatzis*

Main category: cs.CV

TL;DR: 该论文提出了一种结合头戴显示器（HMD）运动信号和图像显著性信息的注视点预测方法，提升了在无眼动仪硬件时的VR体验。


<details>
  <summary>Details</summary>
Motivation: 在VR应用中，注视点预测可以降低感知延迟并支持资源需求大的技术（如聚焦渲染）。但受制于硬件成本和隐私，直接眼动追踪并不总是可用，因此需要通过其他方式预测用户注视点。

Method: 作者提出了一种新颖的注视点预测框架，将HMD运动信号与由视频帧提取的视觉显著性线索结合。具体方法是利用轻量级的显著性编码器（UniSal）提取视觉特征，并与HMD运动数据融合，之后通过时间序列预测模块（包括TSMixer和LSTM两种架构）来预测未来的注视方向。

Result: 在EHTask数据集和商用VR硬件上的实验表明，该方法在注视点预测准确性上优于主流基线（如Center-of-HMD和Mean Gaze）。

Conclusion: 该方法有效弥补了缺乏直接眼动仪硬件时的局限，能够缩短感知滞后并提升VR中的自然交互体验。

Abstract: Gaze prediction plays a critical role in Virtual Reality (VR) applications by reducing sensor-induced latency and enabling computationally demanding techniques such as foveated rendering, which rely on anticipating user attention. However, direct eye tracking is often unavailable due to hardware limitations or privacy concerns. To address this, we present a novel gaze prediction framework that combines Head-Mounted Display (HMD) motion signals with visual saliency cues derived from video frames. Our method employs UniSal, a lightweight saliency encoder, to extract visual features, which are then fused with HMD motion data and processed through a time-series prediction module. We evaluate two lightweight architectures, TSMixer and LSTM, for forecasting future gaze directions. Experiments on the EHTask dataset, along with deployment on commercial VR hardware, show that our approach consistently outperforms baselines such as Center-of-HMD and Mean Gaze. These results demonstrate the effectiveness of predictive gaze modeling in reducing perceptual lag and enhancing natural interaction in VR environments where direct eye tracking is constrained.

</details>


### [137] [Estimation of geometric transformation matrices using grid-shaped pilot signals](https://arxiv.org/abs/2601.18385)
*Rinka Kawano,Masaki Kawamura*

Main category: cs.CV

TL;DR: 本文提出了一种新型数字水印方法，能够在图像经过几何变换（如缩放、剪裁等）后准确同步并提取水印。通过嵌入特殊的“网格”引导信号，并利用Radon变换分析其畸变，从而估算变换矩阵，实现对复杂攻击的鲁棒性。实验表明该方法对多种几何攻击均能准确估算变换参数。


<details>
  <summary>Details</summary>
Motivation: 现有的图像数字水印方案在遇到图像剪裁等几何攻击时，难以保证水印的同步和正确提取。特别是剪裁会改变图像原点，使得传统的水印嵌入与检测方式难以应对。针对这一实际问题，亟需一种能够自适应同步、有效抵御此类攻击的水印算法。

Method: 提出通过在图像中嵌入具有明显水平和垂直特征的网格状引导信号。图像遭受几何变换后，该网格同样被扭曲。利用Radon变换分析扭曲后的网格特征，准确估算出所承受的变换矩阵。网格的编码方式区分了水平方向与垂直方向，进一步降低了同步时的模糊性。

Result: 在模拟各类几何攻击，包括非等比例缩放、旋转、切变和剪裁等条件下，提出的方法均能以较低的误差准确估算变换矩阵，实现水印同步。对复合攻击也表现出了良好的鲁棒性。

Conclusion: 本文方法通过网格引导信号和Radon变换，实现了对几何攻击下图像水印的高精度同步和鲁棒提取。特别在以往难以处理的剪裁场景中，该方法展现了明显优势，对实际数字版权保护具有重要实用价值。

Abstract: Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.

</details>


### [138] [ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks](https://arxiv.org/abs/2601.18386)
*Gabriel Lee Jun Rong,Christos Korgialas,Dion Jia Xu Ho,Pai Chet Ng,Xiaoxiao Miao,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: 本文提出了一种名为ARMOR的新型自动化攻击框架，通过智能体协作和语义引导实现更强大、更灵活的对抗性攻击。


<details>
  <summary>Details</summary>
Motivation: 现有自动化攻击工具方法单一、流程固定，无法自适应调整并缺乏图像语义层次的理解，导致攻击难以适应多样化和更复杂的目标。

Method: ARMOR框架利用视觉语言模型（VLM）驱动的智能体来协同调度三种典型的对抗攻击（CW、JSMA、STA），并通过一套“Mixing Desk”混合系统，将多个扰动策略进行组合和优化。大型语言模型（LLM）会实时自适应调整各攻击智能体参数，实现图像特定语义漏洞的精准挖掘。

Result: 在标准数据集和基准测试上，ARMOR展现出更优的跨架构迁移能力，并能有效欺骗多种防御设置，尤其是在盲目目标和白盒目标攻击任务中均表现出色。

Conclusion: ARMOR通过智能体与大模型协作，实现了更具语义感知和策略弹性的自动化攻击框架，显著提升了对抗攻击有效性与通用性。

Abstract: Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk". Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score.

</details>


### [139] [Efficient Complex-Valued Vision Transformers for MRI Classification Directly from k-Space](https://arxiv.org/abs/2601.18392)
*Moritz Rempe,Lukas T. Rotkopf,Marco Schlimbach,Helmut Becker,Fabian Hörst,Johannes Haubold,Philipp Dammann,Kevin Kröninger,Jens Kleesiek*

Main category: cs.CV

TL;DR: 提出了一种直接在MRI原始频域(k-Space)数据上进行分类的复杂值Vision Transformer（kViT）模型，用径向k-Space分块方式提升效率和契合物理特性，并在准确率和资源消耗等方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前深度学习在MRI多以重建后的幅值图像开展，忽视了原始的相位信息，并需大量计算资源。现有神经网络架构的局部操作也不适合处理k-Space这种全局频域数据。为优化效率并保留更多物理信息，需开发适用于k-Space的新方法。

Method: 提出了复杂值Vision Transformer（kViT），可直接对k-Space数据进行分类；创新性地采用径向k-Space分块，契合频域特性的能量分布，提升与MRI物理的匹配度。

Result: 在fastMRI和自有数据集上，kViT在分类任务中达到与ResNet、EfficientNet、ViT等图像域先进方法相当的性能。对高加速因子的抗干扰性更强，训练时最高可减少68倍显存消耗。

Conclusion: kViT实现了无需图像重建的MRI频域直接分类，在准确性、鲁棒性和计算资源节约方面表现突出，为直接基于扫描仪数据的AI分析开辟了新途径。

Abstract: Deep learning applications in Magnetic Resonance Imaging (MRI) predominantly operate on reconstructed magnitude images, a process that discards phase information and requires computationally expensive transforms. Standard neural network architectures rely on local operations (convolutions or grid-patches) that are ill-suited for the global, non-local nature of raw frequency-domain (k-Space) data. In this work, we propose a novel complex-valued Vision Transformer (kViT) designed to perform classification directly on k-Space data. To bridge the geometric disconnect between current architectures and MRI physics, we introduce a radial k-Space patching strategy that respects the spectral energy distribution of the frequency-domain. Extensive experiments on the fastMRI and in-house datasets demonstrate that our approach achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT). Crucially, kViT exhibits superior robustness to high acceleration factors and offers a paradigm shift in computational efficiency, reducing VRAM consumption during training by up to 68$\times$ compared to standard methods. This establishes a pathway for resource-efficient, direct-from-scanner AI analysis.

</details>


### [140] [Larger than memory image processing](https://arxiv.org/abs/2601.18407)
*Jon Sporring,David Stansby*

Main category: cs.CV

TL;DR: 本文针对PB级超大图像数据的分析提出I/O友好的串流处理方案，通过优化数据流与磁盘访问模式，实现极大提升在有限内存环境下的图像处理效率。


<details>
  <summary>Details</summary>
Motivation: 随着显微电子、生物医学等领域产生的图像数据急剧增大（如1.4 PB级体积数据），传统的“全量载入内存”方法已无法应对，不仅因内存限制，还受限于磁盘I/O瓶颈。亟需一种高效的数据处理框架，充分利用串流机制，优化磁盘读写与算法执行模式。

Method: 论文分析了两种主流3D图像存储（2D切片堆叠与3D分块存储），提出在这两类结构上都可采用以切片为中心的串流处理架构，通过定义“sweep-based”执行、窗口操作与重叠感知的分片策略，最小化磁盘冗余访问。同时，设计了一种领域特定语言（DSL），自动分析算法所需内存及I/O模式，在编译/运行时优化窗口/流调度，实现算法流水线自动融合与调度。

Result: 该架构显著减少了磁盘I/O次数，保持线性I/O扫描，预测性内存占用。同时，兼容现有分割和形态学工具，并将大数据的前处理/后处理转化为顺序流水线模式，大大提高了超大数据图像分析的吞吐效率。

Conclusion: 提出的串流机制与DSL工具能在有限内存下稳定高效处理PB级图像数据，是特大规模图像分析的有效解决方案。该方法降低了对高性能硬件的依赖，有望广泛应用于生物医学等领域的大规模图像分析任务。

Abstract: This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.

</details>


### [141] [Comparative Evaluation of Machine Learning Algorithms for Affective State Recognition from Children's Drawings](https://arxiv.org/abs/2601.18414)
*Aura Loredana Dan*

Main category: cs.CV

TL;DR: 本论文比较了三种深度学习模型在基于儿童画作进行情感识别任务中的表现，旨在提高自闭症儿童情感状态的识别效率。


<details>
  <summary>Details</summary>
Motivation: 目前针对自闭症儿童情感状态的识别方法多为侵入性、主观或不易标准化，亟需一种非侵入性且高效的自动化情感评估手段。

Method: 作者以儿童画作为情感表达载体，利用心理专家标注的情感标签，通过迁移学习训练MobileNet、EfficientNet和VGG16三种深度学习模型，并统一实验框架评估它们在情感分类任务的表现、鲁棒性以及计算效率。

Result: 实验结果显示，不同模型在轻量化与深度结构间存在权衡。其中轻量级模型更适合移动端及实时应用场景，而深层模型则在准确率上更有优势。

Conclusion: 本研究为自闭症儿童情感状态识别提供了一种新思路，展示了基于画作和深度学习的情感分类系统的可行性与实用性，并为模型选择和实际部署提供了依据。

Abstract: Autism spectrum disorder (ASD) represents a neurodevelopmental condition characterized by difficulties in expressing emotions and communication, particularly during early childhood. Understanding the affective state of children at an early age remains challenging, as conventional assessment methods are often intrusive, subjective, or difficult to apply consistently. This paper builds upon previous work on affective state recognition from children's drawings by presenting a comparative evaluation of machine learning models for emotion classification. Three deep learning architectures -- MobileNet, EfficientNet, and VGG16 -- are evaluated within a unified experimental framework to analyze classification performance, robustness, and computational efficiency. The models are trained using transfer learning on a dataset of children's drawings annotated with emotional labels provided by psychological experts. The results highlight important trade-offs between lightweight and deeper architectures when applied to drawing-based affective computing tasks, particularly in mobile and real-time application contexts.

</details>


### [142] [On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics](https://arxiv.org/abs/2601.18448)
*Lloyd Austin Courtenay*

Main category: cs.CV

TL;DR: 本文探讨了形态计量学中的标准预处理方法（GPA）在机器学习应用中带来的统计依赖性问题，并提出了新的校准流程以避免这种污染。通过2D及3D模拟，对不同样本和特征密度下的影响进行系统分析。


<details>
  <summary>Details</summary>
Motivation: 在机器学习分析中，广泛采用GPA对所有标本进行全局对齐，但这种做法可能造成训练集与测试集统计相关，影响预测结果的公正性和准确性。作者旨在正式量化这种依赖问题，并提出解决方案。

Method: 利用受控的二维和三维模拟实验，系统考察样本量、标记点密度及异速生长对GPA引入依赖的影响。并提出一种新流程：先将训练集对齐，再将测试标本对齐到训练集，从而消除交叉依赖。引入RMSE与标本空间、坐标自由度间的关系，并使用线性及卷积回归模型检验标记点空间自相关对性能的影响。

Result: 模拟显示：样本量与标记空间之间的“对角线”关系明显，RMSE表现可用自由度解析推导。同时发现若忽视标记点的空间自相关，模型性能显著下降。新流程能够有效消除统计依赖。

Conclusion: 机器学习应用于形态计量学时，需严格处理数据预处理步骤。本文提供了实际的对齐操作指南，澄清了形态空间处理中的基本统计约束，为相关领域工作提供了理论和方法学基础。

Abstract: Geometric morphometrics (GMM) is widely used to quantify shape variation, more recently serving as input for machine learning (ML) analyses. Standard practice aligns all specimens via Generalized Procrustes Analysis (GPA) prior to splitting data into training and test sets, potentially introducing statistical dependence and contaminating downstream predictive models. Here, the effects of GPA-induced contamination are formally characterised using controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns. A novel realignment procedure is proposed, whereby test specimens are aligned to the training set prior to model fitting, eliminating cross-sample dependency. Simulations reveal a robust "diagonal" in sample-size vs. landmark-space, reflecting the scaling of RMSE under isotropic variation, with slopes analytically derived from the degrees of freedom in Procrustes tangent space. The importance of spatial autocorrelation among landmarks is further demonstrated using linear and convolutional regression models, highlighting performance degradation when landmark relationships are ignored. This work establishes the need for careful preprocessing in ML applications of GMM, provides practical guidelines for realignment, and clarifies fundamental statistical constraints inherent to Procrustes shape space.

</details>


### [143] [3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control](https://arxiv.org/abs/2601.18451)
*Xuanmeng Sha,Liyun Zhang,Tomohiro Mashita,Naoya Chiba,Yuki Uranishi*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法3DGesPolicy，用于生成能够融合全身动作和面部表情的整体手势，实现更自然、表达丰富且与语音高度对齐的动作生成。


<details>
  <summary>Details</summary>
Motivation: 现有的手势生成方法普遍存在身体动作协调性差、动作无意义且空间不连贯的问题，特别是难以实现整体的动作与表情的语义和空间统一。因此，作者希望解决多模态整体手势生成中的协调和表达精度难题。

Method: 作者提出将整体手势生成任务重新建模为连续轨迹控制问题，借鉴了机器人领域的扩散策略（diffusion policy）。引入了GAP（Gesture-Audio-Phoneme）多模态信号融合模块，用于细致对齐语音语义、身体动作和面部表情。同时，将帧间变化作为统一的整体行为来建模，使得动作生成具有更好的空间和语义连贯性。

Result: 在BEAT2数据集上的广泛定量和定性实验表明，3DGesPolicy方法在生成自然、富有表现力、且与语音高度对齐的整体手势方面，均优于其它主流方法。

Conclusion: 3DGesPolicy通过新颖的策略和多模态融合模块，显著提升了语音驱动的整体动作与表情生成的自然度和协调性，为语音与动作的多模态深度融合提供了有效解决思路。

Abstract: Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.

</details>


### [144] [Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System](https://arxiv.org/abs/2601.18464)
*Wenbin Wei,Suyuan Yao,Cheng Huang,Xiangyu Gao*

Main category: cs.CV

TL;DR: 本文提出了Fair-Eye Net，一种多模态AI系统，通过集成多类型数据并注重公平性，大幅提升青光眼筛查和随访的客观性、一致性与可及性。


<details>
  <summary>Details</summary>
Motivation: 青光眼是不可逆致盲的主要原因，早期筛查和随访至关重要。但目前筛查和评估方式主观性强、流程碎片化，且优质影像和专家资源有限，导致诊疗不一致和健康公平性不足。

Method: 提出Fair-Eye Net系统，融合眼底照片、OCT结构数据、视野功能指标和人口统计学信息，采用双流异构融合架构和不确定性感知分层门控机制，实现有选择性预测和安全转诊，并通过公平性约束减少弱势群体漏诊。

Result: 系统实验中AUC达0.912（特异性96.7%），种族假阴性差距降低73.4%（12.31%降至3.28%），跨域性能稳定，能提前3-12个月发出高敏感性（92%）和高特异性（88%）风险预警。

Conclusion: Fair-Eye Net不仅以多任务学习将公平性作为核心目标，同时兼顾临床可靠性，为青光眼AI诊疗的大规模应用和全球眼健康公平提供了可复制的解决方案。

Abstract: Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.

</details>


### [145] [DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment](https://arxiv.org/abs/2601.18493)
*Sara Tehrani,Yonghao Xu,Leif Haglund,Amanda Berg,Michael Felsberg*

Main category: cs.CV

TL;DR: 该论文提出了DisasterInsight，一个评估视觉-语言模型在灾害分析任务中表现的多模态基准，并通过对现有数据集的重构及多样化任务支持，填补了灾害响应中模型实际应用评估的空白。


<details>
  <summary>Details</summary>
Motivation: 目前遥感领域的视觉-语言基准多关注于粗粒度标签和图像级识别，缺乏面向真实人道主义流程的功能理解与指令鲁棒性评价。因此，需要新的基准来衡量模型在更复杂、更实际场景下的能力。

Method: 1）将xBD数据集重构为11.2万余个以建筑为中心的实例，支持建筑功能分类、灾害级别与类型分类、数量统计、结构化报告生成等多任务评测。2）提出DI-Chat，通过在灾害指令数据上对现有VLM骨干采用低秩适应（LoRA）精调，形成领域适应基线。3）用DI-Chat及主流VLM在基准上做系统实验。

Result: 实验结果显示，当前主流VLM（包括遥感领域专用模型）在多任务上普遍存在较大性能差距，尤其在损毁判定和结构化报告生成任务中表现不足。DI-Chat在灾害级别、类型分类及报告生成上取得显著提升，但建筑功能分类任务对所有模型仍具较大挑战。

Conclusion: DisasterInsight为灾害遥感图像中的多模态推理提供了统一评测平台，有助于推动VLM在实际灾害响应中的应用与研究。

Abstract: Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.
  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.

</details>


### [146] [From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation](https://arxiv.org/abs/2601.18532)
*Devon Levy,Bar Assayag,Laura Gaspar,Ilan Shimshoni,Bella Specktor-Fadida*

Main category: cs.CV

TL;DR: 该论文提出了一种面向医学影像分割的主动学习冷启动采样策略，结合基础模型嵌入与聚类方法，自动确定聚类数并实现比例采样以构建多样化训练集，显著提升了低数据量情形下的分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割标注成本高、耗时，影响疾病监测的效率。主动学习能减少标注负担，但冷启动阶段如何高效采样代表性样本仍是挑战。研究动机在于优化冷启动采样策略，提高分割模型性能。

Method: 新方法将基础模型（如大模型）的特征嵌入与聚类算法结合，自动选定聚类数并按比例采样，确保初始训练集的多样性和代表性。之后采用整合空间多样性的基于不确定性的主动学习框架进行样本选择，并可可视化候选样本在特征空间的分布。

Result: 在X-ray和MRI的三个公开数据集上测试，方案优于随机采样与传统主动学习基线。以CheXmask为例，Dice系数从0.918提高到0.929，Hausdorff距离从32.41mm降至27.66mm；主动学习阶段Dice从0.919升至0.939，Hausdorff距离降到19.16mm。其他数据集也表现出类似提升。

Conclusion: 该主动学习框架在低标注数据情景下能持续优于传统方法，有效提升医学影像分割的准确性和效率，对实际辅助诊断具有应用潜力。

Abstract: Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.

</details>


### [147] [GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning](https://arxiv.org/abs/2601.18543)
*Kaixun Jiang,Yuzheng Wang,Junjie Zhou,Pandeng Li,Zhihang Liu,Chen-Wei Xie,Zhaoyu Chen,Yun Zheng,Wenqiang Zhang*

Main category: cs.CV

TL;DR: GenAgent提出了一种通过代理框架解耦视觉理解和生成的新型多模态模型，实现了跨工具泛化、多轮自治推理与生成，显著提升了生成任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态统一模型在视觉理解与生成上存在训练成本高、两者权衡受限等问题，需要新的架构突破这些瓶颈。

Method: GenAgent采用代理式架构，将理解和生成能力分离：理解由多模态模型完成，生成交由可调用的生成模型工具完成。模型通过两阶段训练（有监督微调和强化学习）学习如何自主调用工具、多轮推理、反思和递进优化输出。强化学习中结合最终图片质量、反思准确度等奖励，通过路径重采样提升序列探索能力。

Result: GenAgent在GenEval++和WISE等生成评测中，分别提升了23.6%和14%，在跨工具泛化、多轮交互中的性能持续提升，并能根据任务自动调整推理流程。

Conclusion: GenAgent不仅提高了生成质量，还实现了架构通用性、交互自适应和工具泛化，为后续多模态智能体研究提供了新范式。

Abstract: We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\%) and WISE (+14\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \href{https://github.com/deep-kaixun/GenAgent}{this url}.

</details>


### [148] [REMAC: Reference-Based Martian Asymmetrical Image Compression](https://arxiv.org/abs/2601.18547)
*Qing Ding,Mai Xu,Shengxi Li,Xin Deng,Xin Zou*

Main category: cs.CV

TL;DR: 本文提出了一种针对火星图像的参考图像引导非对称压缩方法REMAC，在保证高压缩率的同时显著降低了火星端的编码复杂度，适用于受限传输和运算场景。


<details>
  <summary>Details</summary>
Motivation: 火星到地球的通信带宽受限，需要高效的图像压缩技术。然而现有方法一是未考虑火星端计算资源有限，二是未利用火星图像间强相关性的特点。

Method: 提出REMAC方法，将大部分计算从编码端（火星）转移到解码端（地球），利用参考图像模块和ref-decoder，分别通过跨图像和图像内部的相似性提升压缩性能，并引入多尺度、深大感受野架构和潜特征循环机制以降低复杂度。

Result: 与最新方法相比，REMAC能减少43.51%的编码端复杂度，同时BD-PSNR提升0.2664 dB。

Conclusion: REMAC方法兼顾了资源受限和高压缩比的需求，是火星图像远程传输的更优选择，对星际通信和无人探测任务具有推动意义。

Abstract: To expedite space exploration on Mars, it is indispensable to develop an efficient Martian image compression method for transmitting images through the constrained Mars-to-Earth communication channel. Although the existing learned compression methods have achieved promising results for natural images from earth, there remain two critical issues that hinder their effectiveness for Martian image compression: 1) They overlook the highly-limited computational resources on Mars; 2) They do not utilize the strong \textit{inter-image} similarities across Martian images to advance image compression performance. Motivated by our empirical analysis of the strong \textit{intra-} and \textit{inter-image} similarities from the perspective of texture, color, and semantics, we propose a reference-based Martian asymmetrical image compression (REMAC) approach, which shifts computational complexity from the encoder to the resource-rich decoder and simultaneously improves compression performance. To leverage \textit{inter-image} similarities, we propose a reference-guided entropy module and a ref-decoder that utilize useful information from reference images, reducing redundant operations at the encoder and achieving superior compression performance. To exploit \textit{intra-image} similarities, the ref-decoder adopts a deep, multi-scale architecture with enlarged receptive field size to model long-range spatial dependencies. Additionally, we develop a latent feature recycling mechanism to further alleviate the extreme computational constraints on Mars. Experimental results show that REMAC reduces encoder complexity by 43.51\% compared to the state-of-the-art method, while achieving a BD-PSNR gain of 0.2664 dB.

</details>


### [149] [Automated Landmark Detection for assessing hip conditions: A Cross-Modality Validation of MRI versus X-ray](https://arxiv.org/abs/2601.18555)
*Roberto Di Via,Vito Paolo Pastore,Francesca Odone,Siôn Glyn-Jones,Irina Voiculescu*

Main category: cs.CV

TL;DR: 本文比较了X光和MRI在股骨髋臼撞击症（FAI）筛查中基于角度测量的等效性，发现MRI可达到与X光类似的临床定位和诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 传统FAI筛查依赖X光角度测量，但X光无法提供撞击区域高度与跨度的三维信息，而MRI可以补充这一维度。确保不同成像方式测量指标的等效性对医生决策和自动化诊断有重要意义。

Method: 作者招募了89名患者，对其进行匹配组设计，同时收集MRI和X光，采用标准热力图回归方法对两种成像的解剖标志点进行检测，并评估不同方式的定位准确率和诊断性能。

Result: 实验结果显示，MRI在检测cam型撞击症关键标志点和临床诊断上与X光等效，支持MRI在FAI筛查领域的临床可行性。

Conclusion: MRI不仅能等效替代X光提供FAI诊断的必要测量，还能支持三维体积分析，有潜力作为常规自动化FAI评估工具，推动MRI在临床诊断流程中的广泛应用。

Abstract: Many clinical screening decisions are based on angle measurements. In particular, FemoroAcetabular Impingement (FAI) screening relies on angles traditionally measured on X-rays. However, assessing the height and span of the impingement area requires also a 3D view through an MRI scan. The two modalities inform the surgeon on different aspects of the condition. In this work, we conduct a matched-cohort validation study (89 patients, paired MRI/X-ray) using standard heatmap regression architectures to assess cross-modality clinical equivalence. Seen that landmark detection has been proven effective on X-rays, we show that MRI also achieves equivalent localisation and diagnostic accuracy for cam-type impingement. Our method demonstrates clinical feasibility for FAI assessment in coronal views of 3D MRI volumes, opening the possibility for volumetric analysis through placing further landmarks. These results support integrating automated FAI assessment into routine MRI workflows. Code is released at https://github.com/Malga-Vision/Landmarks-Hip-Conditions

</details>


### [150] [Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis](https://arxiv.org/abs/2601.18556)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 本文提出了SDA-QEC新框架，通过生成式数据增强与量子特征强化提升医学图像分类效果，特别针对样本极不平衡问题，实验表现显著优于主流模型。


<details>
  <summary>Details</summary>
Motivation: 医学图像分类（如胸片中的肺炎检测、乳腺癌筛查）常见类别极度不平衡，导致对少数类识别效果差，不利于临床安全。如何改善极少数样本情况下的模型表现，是实际应用亟需解决的问题。

Method: 提出SDA-QEC框架：1）用轻量化扩散式数据增强器为少数类别生成高质量合成样本，重平衡样本分布；2）在MobileNetV2网络中融合量子特征层，通过希尔伯特空间高维映射增强模型区分力。

Result: 在冠状动脉造影图像分类实验中，SDA-QEC获得了98.33%准确率、98.78%AUC和98.33%F1分数，显著超越ResNet18、MobileNetV2、DenseNet121、VGG16等基线。其灵敏度和特异性均为98.33%，展现出出色的均衡性能。

Conclusion: SDA-QEC框架有效结合了生成式数据增强与量子特征建模，为极少样本、高风险的医学AI系统研发提供了新思路，在真实医学影像中展现了极高潜力。

Abstract: In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios.

</details>


### [151] [AI-enabled Satellite Edge Computing: A Single-Pixel Feature based Shallow Classification Model for Hyperspectral Imaging](https://arxiv.org/abs/2601.18560)
*Li Fang,Tianyu Li,Yanghong Lin,Shudong Zhou,Wei Yao*

Main category: cs.CV

TL;DR: 本文提出了一种高效的AI赋能卫星边缘计算范式用于高光谱图像分类，实现卫星自主决策，并解决了星载资源受限与数据下行带宽瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 高光谱卫星成像提供丰富信息，但由于下行传输速率受限，难以满足如灾害监测、应急制图等需要快速响应的应用。星载处理是解决瓶颈的有效途径，但需兼顾算力受限、图像质量下降等挑战。

Method: 提出一种轻量级、非深度学习的几次学习（few-shot learning）高光谱分类方法，包括两阶段像素级标签传播机制。首先，通过锚点像素与目标像素构建亲和矩阵，传播获取初始标签；然后，计算像素间相似性生成稀疏图，利用闭式解代替迭代。锚点标签由秩约束图聚类算法自动确定。

Result: 方法无需空间结构信息，仅用像素内光谱特征，能适应传感器失效、扫描误差等导致的图像质量问题，同时提升了星载分类效率和响应速度。

Conclusion: 提出的星载计算范式实现了高效、鲁棒的高光谱图像分类，为卫星自主决策和应急响应提供了有力技术支撑，具有现实应用价值。

Abstract: As the important component of the Earth observation system, hyperspectral imaging satellites provide high-fidelity and enriched information for the formulation of related policies due to the powerful spectral measurement capabilities. However, the transmission speed of the satellite downlink has become a major bottleneck in certain applications, such as disaster monitoring and emergency mapping, which demand a fast response ability. We propose an efficient AI-enabled Satellite Edge Computing paradigm for hyperspectral image classification, facilitating the satellites to attain autonomous decision-making. To accommodate the resource constraints of satellite platforms, the proposed method adopts a lightweight, non-deep learning framework integrated with a few-shot learning strategy. Moreover, onboard processing on satellites could be faced with sensor failure and scan pattern errors, which result in degraded image quality with bad/misaligned pixels and mixed noise. To address these challenges, we develop a novel two-stage pixel-wise label propagation scheme that utilizes only intrinsic spectral features at the single pixel level without the necessity to consider spatial structural information as requested by deep neural networks. In the first stage, initial pixel labels are obtained by propagating selected anchor labels through the constructed anchor-pixel affinity matrix. Subsequently, a top-k pruned sparse graph is generated by directly computing pixel-level similarities. In the second stage, a closed-form solution derived from the sparse graph is employed to replace iterative computations. Furthermore, we developed a rank constraint-based graph clustering algorithm to determine the anchor labels.

</details>


### [152] [Self-Refining Video Sampling](https://arxiv.org/abs/2601.18577)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Saining Xie,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.CV

TL;DR: 提出了一种在现有视频生成器基础上的自我细化采样方法，无需额外训练或外部验证器，就能有效提升物理动态真实性和运动一致性。


<details>
  <summary>Details</summary>
Motivation: 目前的视频生成器在复杂物理动态下生成的视频往往缺乏真实感。以往的方法使用外部验证器或额外训练增强数据，既计算量大又难以细致捕捉动作细节。

Method: 作者提出将预训练的大规模视频生成器本身视为去噪自编码器，在推理时内循环迭代细化生成结果，无需外部验证器或额外训练。同时，引入不确定性感知细化，针对自一致性低的区域有选择地细化，避免过度细化产生伪影。

Result: 在多种主流视频生成器上的实验表明，该方法显著提高了生成视频的运动连贯性和物理一致性，用户偏好度超过70%，超越默认采样器和基于引导的采样器。

Conclusion: 所提自我细化方法简单高效，可在不增加训练开销的前提下，大幅改善视频生成的物理和感知质量。

Abstract: Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\% human preference compared to the default sampler and guidance-based sampler.

</details>


### [153] [GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization](https://arxiv.org/abs/2601.18585)
*Chenxi Liu,Selena Ling,Alec Jacobson*

Main category: cs.CV

TL;DR: 本文提出了一种名为GimmBO的新方法，通过偏好贝叶斯优化，提升了扩散式图像生成中适配器合并的效率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 目前，基于微调的扩散式图像生成方法能够通过适配器实现对不同主题和风格的自定义。多个适配器合并后，可生成丰富的视觉结果。然而，通过手动滑块调参探索适配器权重空间，不但效率低下，且适配器数量增加后难以操作。因此，迫切需要一种更高效的探索和优化手段。

Method: 作者提出GimmBO框架，基于偏好型贝叶斯优化（PBO），采用两阶段贝叶斯优化机制，针对合并适配器权重稀疏和权重范围受限等实际问题，提升高维空间中的采样效率和收敛速度。

Result: 通过模拟用户和真实用户研究，GimmBO展现出比传统贝叶斯优化与线性搜索方法更快的收敛速度、更高的成功率和更稳定的表现，同时证明了框架的高度灵活性。

Conclusion: GimmBO为扩散式图像生成适配器权重合并提供了高效、可扩展的自动探索和优化方案，并可适配多种扩展需求，在实际应用中具有显著优势。

Abstract: Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.

</details>


### [154] [AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment](https://arxiv.org/abs/2601.18589)
*KV Karthikeya,Ashok Kumar Das,Shantanu Pal,Vivekananda Bhat K,Arun Sekar Rajasekaran*

Main category: cs.CV

TL;DR: 本文提出了AGSP-DSA框架，实现对文本、音频和图像等异构数据的多模态融合，并在多个基准数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统多模态融合方法在不同模态间相关性建模不足，对缺失模态的鲁棒性有限，因此需要更有效的融合与对齐方式。

Method: 1. 构建双图（dual-graph），分别学习模态内和模态间关系；2. 采用谱图滤波强化有用信号；3. 利用多尺度图卷积网络获得节点嵌入；4. 引入语义感知注意机制，根据上下文动态分配各模态贡献。

Result: AGSP-DSA在CMU-MOSEI、AVE和MM-IMDB三个公开数据集上取得了比以往方法更好的表现，如CMU-MOSEI上达到了95.3%准确率、0.936的F1和0.924的mAP，较MM-GNN提升2.6%；同时在缺失模态情况下也表现出较强的泛化能力和鲁棒性。

Conclusion: AGSP-DSA方法有效提升了多模态融合过程中信息提取能力和鲁棒性，适用于情感分析、事件识别以及多媒体分类等任务，表现出广泛的应用前景。

Abstract: In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.

</details>


### [155] [EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery](https://arxiv.org/abs/2601.18597)
*Yu Xia,Chang Liu,Tianqi Xiang,Zhigang Tu*

Main category: cs.CV

TL;DR: 本文提出了一种名为EFSI-DETR的新型无人机小目标实时检测框架，通过有效的特征增强和动态频率-空间引导，显著提升了小目标检测的精度和速度。


<details>
  <summary>Details</summary>
Motivation: 无人机影像中的小目标检测因特征表征有限和多尺度特征融合不足仍具挑战，现有方法未充分利用频率信息、依赖静态卷积，导致深层语义特征提取受限。为解决这些不足，需要更高效、动态的信息融合和特征提取机制。

Method: 提出EFSI-DETR检测框架，包括两个核心模块：1) DyFusNet，联合利用频率与空间信息进行多尺度特征融合；2) ESFC，有效提取深层语义特征且计算开销小。此外引入细粒度特征保留（FFR）策略，将空间丰富的浅层信息在融合过程中保留，改善小目标检测。

Result: 在VisDrone和CODrone数据集上，EFSI-DETR框架表现优异，实现了AP提升1.6%、AP_s提升5.8%，推理速率达到188 FPS（RTX 4090）。

Conclusion: EFSI-DETR通过频率-空间联合引导和高效语义集中特征提取，有效提升了无人机小目标检测的精度和实时性，在主流数据集上达到了最新最优水平。

Abstract: Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.

</details>


### [156] [Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures](https://arxiv.org/abs/2601.18619)
*Jorge Quesada,Ghassan AlRegib*

Main category: cs.CV

TL;DR: 自监督学习（SSL）在标签有限的情况下是一种有效表示学习策略，但对小目标分割效果欠佳。本文提出通过小窗口裁剪增强数据处理流程，提升小、稀疏、不规则目标的分割表现，并在地震和神经成像领域经实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在分割小型、稀疏或局部不规则目标时效果不理想。科学影像任务如地震断层和神经细胞分割，常见上述特征，需对SSL方法进行优化以提升表现。

Method: 提出尺度感知的SSL适配方法，在预训练的数据增强阶段引入小窗口裁剪技术，使模型更专注于细粒度结构，并在地震成像和神经成像领域进行实验验证。

Result: 在断层分割和神经细胞分割任务中，提出的方法在有限标注下比标准和最新SSL基线提升断层分割准确率最高可达13%，细胞边界分割提升达5%。对大尺度目标分割提升有限。

Conclusion: SSL在目标分割中的效果依赖目标的尺度和稀疏性；针对小尺度目标引入细粒度增强能显著提升表现。建议SSL方案设计需根据目标特性调整，为科学影像分割任务提供通用设计原则。

Abstract: Self-supervised learning (SSL) has emerged as a powerful strategy for representation learning under limited annotation regimes, yet its effectiveness remains highly sensitive to many factors, especially the nature of the target task. In segmentation, existing pipelines are typically tuned to large, homogeneous regions, but their performance drops when objects are small, sparse, or locally irregular. In this work, we propose a scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline, zooming in on fine-scale structures during pretraining. We evaluate this approach across two domains with markedly different data modalities: seismic imaging, where the goal is to segment sparse faults, and neuroimaging, where the task is to delineate small cellular structures. In both settings, our method yields consistent improvements over standard and state-of-the-art baselines under label constraints, improving accuracy by up to 13% for fault segmentation and 5% for cell delineation. In contrast, large-scale features such as seismic facies or tissue regions see little benefit, underscoring that the value of SSL depends critically on the scale of the target objects. Our findings highlight the need to align SSL design with object size and sparsity, offering a general principle for buil ding more effective representation learning pipelines across scientific imaging domains.

</details>


### [157] [Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation](https://arxiv.org/abs/2601.18623)
*Zihao Wang,Yuzhou Chen,Shaogang Ren*

Main category: cs.CV

TL;DR: 该论文提出了一种新的跨模态图像翻译方法，通过在扩散生成过程中引入空间变化的混合场和显式的修复项，有效提升了结构和语义一致性，并加快了收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态图像翻译方法（如标准扩散方法）通常使用全局线性转换，这会让采样过程经过不合理的高代价区域，造成语义偏移和效率低下。作者将这种现象称为固定进度域转移，亟需更灵活、有效的翻译方法。

Method: 作者创新性地将域切换动态嵌入到扩散生成过程：在每一步反向扩散中预测空间变化的混合场，并为漂移项注入与目标一致的修复项，实现逐步、显式的引导。这种方法强调局部残差修正而非全局对齐，同时提出了连续时间的精确解和实用的一阶抽样器，确保了边缘一致性。

Result: 在医学成像、遥感和电致发光语义映射等多个图像翻译任务中，所提框架能提升结构保真和语义一致性，且只需更少的去噪步骤即可收敛。

Conclusion: 新方法避免了标准扩散方法中的高代价、语义漂移问题，能高效且更准确地完成跨模态图像翻译，具有实际推广价值。

Abstract: Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.

</details>


### [158] [CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search](https://arxiv.org/abs/2601.18625)
*Zequn Xie*

Main category: cs.CV

TL;DR: CONQUER是一种为文字描述-行人检索（TBPS）任务设计的两阶段框架，通过更好的跨模态对齐和自适应查询优化，显著提升检索效果，并在多项基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前TBPS任务因跨模态差异大和用户查询含糊而难以应对实际需求，导致检索准确率有限。研究者需要提高模型对不同模态的对齐能力，并能在推理过程中优化用户查询，以提升整体性能和实用性。

Method: 方法分为训练和推理两个阶段。训练时，CONQUER结合多粒度编码、互补配对挖掘和以最优传输为基础的上下文引导匹配，提升嵌入表征能力。推理时，采用插拔式的查询增强模块，通过锚点选择和属性补全机制优化原始描述，无需重新训练主干网络。

Result: 该方法在CUHK-PEDES、ICFG-PEDES和RSTPReid数据集上显著提升了Rank-1准确率和mAP，在跨域和不完整查询场景下优势更为明显。

Conclusion: CONQUER能够有效解决TBPS中的跨模态对齐和查询歧义问题，具有较强的实用性和推广价值，为实际公共安全应用中的检索提供了一种可靠方案。

Abstract: Text-Based Person Search (TBPS) aims to retrieve pedestrian images from large galleries using natural language descriptions. This task, essential for public safety applications, is hindered by cross-modal discrepancies and ambiguous user queries. We introduce CONQUER, a two-stage framework designed to address these challenges by enhancing cross-modal alignment during training and adaptively refining queries at inference. During training, CONQUER employs multi-granularity encoding, complementary pair mining, and context-guided optimal matching based on Optimal Transport to learn robust embeddings. At inference, a plug-and-play query enhancement module refines vague or incomplete queries via anchor selection and attribute-driven enrichment, without requiring retraining of the backbone. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that CONQUER consistently outperforms strong baselines in both Rank-1 accuracy and mAP, yielding notable improvements in cross-domain and incomplete-query scenarios. These results highlight CONQUER as a practical and effective solution for real-world TBPS deployment. Source code is available at https://github.com/zqxie77/CONQUER.

</details>


### [159] [Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting](https://arxiv.org/abs/2601.18633)
*Tong Shi,Melonie de Almeida,Daniela Ivanova,Nicolas Pugeault,Paul Henderson*

Main category: cs.CV

TL;DR: 本论文提出了一种基于高斯喷洒（Gaussian Splatting）方法的三维说话头像生成算法Splat-Portrait，有效提升了头像动画的真实性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有的三维说话头像生成方法依赖于特定领域的启发（如基于图像形变的人脸运动先验），导致重建结果不准确，生成的头像动画缺乏自然感和真实性。亟需新的方法克服三维重建和唇部动作合成中的难题。

Method: Splat-Portrait方法基于高斯喷洒技术，将单张头像图像解耦为静态三维重建部分（静态高斯喷洒表示）和二维背景，同时根据输入音频自然生成唇部动作。该方法不依赖运动驱动先验，无需三维监督或人脸关键点标注，训练仅依靠二维重建损失和得分蒸馏损失。

Result: 实验结果表明，Splat-Portrait在生成说话头像和新视角合成方面的视觉质量优于现有方法，实现了更高的动画真实感。

Conclusion: Splat-Portrait为说话头像生成领域提供了无需三维监督的新范式，显著提升了三维重建和说话动画的效果，在视觉质量方面领先于以往工作。项目代码与资料已公开。

Abstract: Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at https://github.com/stonewalking/Splat-portrait.

</details>


### [160] [Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge](https://arxiv.org/abs/2601.18698)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一套系统性评估方法GAP及包含500个世界各地景点的基准数据集，用于检验文本生成视频模型在地理公平性和视觉知识覆盖上的表现，发现当前主流模型在全球范围内表现相对均衡。


<details>
  <summary>Details</summary>
Motivation: 尽管文本生成视频模型已取得显著进展，但外界仍担心这些模型可能偏向特定地区，缺乏地理公平性。因此需要开发专门方法检验其在全球不同地区的知识表现。

Method: 作者提出GAP评估框架，通过构建包含500个全球景点的GEOATTRACTION-500基准，采用结构对齐、关键点对齐及视觉-语言模型评判等多种指标，系统性评估主流模型生成不同地区景点时的表现，并与人工标签结果比对。

Result: 对Sora 2等主流模型实测表明，模型在不同地区、发展水平和文化分组中的地理视觉知识分布较为均匀，只对景点知名度略有依赖，打破了模型强烈地理偏见的普遍认知。

Conclusion: 当前文本生成视频模型在全球视觉知识覆盖上比预期更为均衡，有助于其在全球化应用中部署，但仍需持续监测与评估，确保日后演进时不产生新的偏见。

Abstract: Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.

</details>


### [161] [SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification](https://arxiv.org/abs/2601.18739)
*Ignacio Antequera-Sánchez,Juan Luis Suárez-Díaz,Rosana Montes,Francisco Herrera*

Main category: cs.CV

TL;DR: 本文提出了一种新的分层方法SeNeDiF-OOD，通过嵌套二元融合设计，有效检测多种类型的OOD数据，实验显示比传统方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的单阶段OOD检测方法难以应对现实中异构的OOD数据（从低层次扰动到语义变化），这一难题制约了AI系统在开放环境中的可靠性。

Method: 作者提出了SeNeDiF-OOD方法，将OOD检测分解成多个按语义层级分布的二元融合节点，每一层结合与特定语义抽象水平对齐的决策边界，实现分层检测。

Result: 在MonuMAI建筑风格识别系统中进行的实验表明，该方法能有效过滤非分布内样本、未知类别和对抗攻击，并且检测效果显著优于传统基线方法，同时不损失分布内的识别性能。

Conclusion: SeNeDiF-OOD作为分层融合方法，显著提升了面对多样OOD数据时的检测能力，为现实AI应用中的OOD检测提供了更加可靠的解决方案。

Abstract: Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [162] [Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle](https://arxiv.org/abs/2601.16986)
*Zihan Wang,Cheng Tang,Lei Gong,Cheng Li,Chao Wang,teng wang,Wenqi Lou,Xuehai Zhou*

Main category: cs.CL

TL;DR: 本文提出了Crystal-KV框架，有效压缩大语言模型进行链式思维（CoT）推理时的KV缓存，并提升了推理速度与准确度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在进行复杂推理时需长序列支持，导致KV缓存极度消耗内存，而传统KV压缩未考虑CoT任务中最终答案比中间推理更重要的特性。因此需要为CoT设计专门的KV管理方法。

Method: 提出Crystal-KV，将关注重点放在最终答案，其核心包括：（1）提出答案优先原则，通过映射答案偏好于推理阶段的注意力图区分可丢弃（SlipKV）和必须保留（CrystalKV）的缓存；（2）基于注意力的LRFU算法及时剔除无效SlipKV条目；（3）自适应KV预算分配算法，根据关键CrystalKV动态调整每层/头分配，有效利用缓存。

Result: 实验验证Crystal-KV可大幅提升KV缓存压缩率，提升吞吐量及响应速度，同时答案准确度不降反升，表现为最优压缩效果。

Conclusion: Crystal-KV为大语言模型CoT推理场景带来了存储与计算性能的重大提升，兼顾效率与准确性。

Abstract: Chain-of-Thought (CoT) reasoning in large language models (LLMs) significantly improves accuracy on complex tasks, yet incurs excessive memory overhead due to the long think-stage sequences stored in the Key-Value (KV) cache. Unlike traditional generation tasks where all tokens are uniformly important, CoT emphasizes the final answer, rendering conventional KV compression strategies ineffective. In this paper, we present Crystal-KV, an efficient KV cache management framework tailored for CoT reasoning. Our key insight is the answer-first principle. By mapping answer preferences into think-stage attention map, we distinguish between SlipKV, which mainly maintains the reasoning flow but may occasionally introduce misleading context, and CrystalKV, which truly contributes to the correctness of the final answer. Next, we propose an attention-based Least Recently Frequently Used algorithm. It precisely identifies when a SlipKV entry's utility expires and evicts it, retaining CrystalKV without disrupting reasoning flow. Finally, we introduce an adaptive cache budget allocation algorithm. Based on the dynamic proportion of CrystalKV, it estimates the importance of each layer/head and adjusts the KV cache budget during inference, amplifying critical components to improve budget utilization. Results show that Crystal-KV achieves state-of-the-art KV cache compression, significantly improves throughput, and enables faster response time, while maintaining, or even improving, answer accuracy for CoT reasoning.

</details>


### [163] [Evaluating Reward Model Generalization via Pairwise Maximum Discrepancy Competitions](https://arxiv.org/abs/2601.16987)
*Shunyang Luo,Peibei Cao,Zhihui Zhu,Kehua Feng,Zhihua Wang,Keyan Ding*

Main category: cs.CL

TL;DR: 本文提出了一种新的奖励模型（RM）泛化评价框架PMDC，通过动态筛选分歧最大的样本对，更有效评估RM在开放域场景下的泛化能力，发现现有评测低估了RM间的实际分歧。


<details>
  <summary>Details</summary>
Motivation: 现有RM评测依赖静态有标注数据，覆盖有限，难以全面评估模型对新颖提示的泛化能力，不能有效反映实际应用下的问题。

Method: 提出Pairwise Maximum Discrepancy Competition（PMDC）框架，从大规模无标注开放域提问池中主动挑选使两个RM意见分歧最大的问答对。人工（oracle）对这些高争议样本裁决，采用Bradley--Terry模型对结果聚合，得到全局排名和RM间的对抗格局。

Result: 对10个代表性RM用PMDC重新评估，排名与常规基准大幅变化，发现在更真实分布下RM泛化表现差异显著。定性分析还揭示了RM泛化系统性问题。

Conclusion: PMDC框架能更全面地揭示奖励模型的泛化短板，对提升RM实际可靠性具有指导意义。

Abstract: Reward models (RMs) are central to aligning large language models, yet their practical effectiveness hinges on generalization to unseen prompts and shifting distributions. Most existing RM evaluations rely on static, pre-annotated preference datasets, which provide limited coverage and often fail to faithfully assess generalization in open-world settings. We introduce Pairwise Maximum Discrepancy Competition (PMDC), a dynamic and annotation-efficient framework for evaluating RM generalization using a large, unlabeled, open-domain prompt pool. PMDC actively selects prompt--response pairs that maximize disagreement between two RMs, yielding a compact set of highly contentious test cases. These cases are adjudicated by an oracle, and the resulting outcomes are aggregated via a Bradley--Terry model to produce a global ranking and pairwise win-rate landscape of RMs. We apply PMDC to re-evaluate 10 representative RMs and observe substantial rank reshuffling compared with conventional benchmarks. Qualitative analyses further uncover systematic generalization failures, providing valuable insights for improving reward modeling.

</details>


### [164] [Uncertainty Quantification for Named Entity Recognition via Full-Sequence and Subsequence Conformal Prediction](https://arxiv.org/abs/2601.16999)
*Matthew Singer,Srijan Sengupta,Karl Pazdernik*

Main category: cs.CL

TL;DR: 本文提出了一种为序列标注的命名实体识别（NER）模型生成具备不确定性量化的预测集的新框架。预测集能以指定置信度包含正确标注，为下游应用提供可靠性保证。方法基于共形预测，实现在多模型和数据集上的有效性与广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 当前NER模型只输出单一标签序列，未能衡量预测不确定性，导致下游模块容易受单点错误影响，因此亟需一种能提供置信度保障的方法。

Method: 提出利用共形预测理论，为现有序列标注类NER模型适配“预测集”输出——即一组完整句子的标签序列，其包含正确标签序列置信度由用户指定。设计高效的非一致性评分函数，支持无条件与类别条件下的置信度覆盖，并兼顾句长、语言、实体类型等异质性影响。

Result: 在四种NER模型和三个基准数据集上进行实证，实验显示该方法在适用性、置信度覆盖有效性及预测集效率上均表现优良。

Conclusion: 新框架显著提升了NER模型输出的可靠性和可解释性，有效为下游任务减少错误传递风险，并已验证其广泛适用性和高效性能。

Abstract: Named Entity Recognition (NER) serves as a foundational component in many natural language processing (NLP) pipelines. However, current NER models typically output a single predicted label sequence without any accompanying measure of uncertainty, leaving downstream applications vulnerable to cascading errors. In this paper, we introduce a general framework for adapting sequence-labeling-based NER models to produce uncertainty-aware prediction sets. These prediction sets are collections of full-sentence labelings that are guaranteed to contain the correct labeling with a user-specified confidence level. This approach serves a role analogous to confidence intervals in classical statistics by providing formal guarantees about the reliability of model predictions. Our method builds on conformal prediction, which offers finite-sample coverage guarantees under minimal assumptions. We design efficient nonconformity scoring functions to construct efficient, well-calibrated prediction sets that support both unconditional and class-conditional coverage. This framework accounts for heterogeneity across sentence length, language, entity type, and number of entities within a sentence. Empirical experiments on four NER models across three benchmark datasets demonstrate the broad applicability, validity, and efficiency of the proposed methods.

</details>


### [165] [RAM-SD: Retrieval-Augmented Multi-agent framework for Sarcasm Detection](https://arxiv.org/abs/2601.17002)
*Ziyang Zhou,Ziqi Liu,Yan Wang,Yiming Lin,Yangbin Chen*

Main category: cs.CL

TL;DR: 本文提出了RAM-SD框架，通过多阶段和多智能体方法提升了反讽检测的效果，取得了新SOTA成绩，并具备可解释性。


<details>
  <summary>Details</summary>
Motivation: 反讽检测依赖复杂语境、常识和多维度语言特征，现有方法在面对不同反讽类型时仅用一套推理策略，难以灵活应对多样分析需求，导致效果受限。

Method: RAM-SD包含四个阶段：（1）上下文检索，通过类似与非类似示例进行语境锚定；（2）元规划器判断反讽类型并选择最优推理路径；（3）多智能体从不同视角协作分析；（4）集成器将多智能体分析结果整合，给出最终可解释判决。该方法利用多样化推理和检索增强，有效应对反讽多样性。

Result: 在四个公开反讽检测基准上，RAM-SD框架实现了77.74%的Macro-F1，较GPT-4o+CoC基线提升7.01分，打破现有最佳表现。

Conclusion: RAM-SD不仅在准确率上实现突破，还能输出可解释的推理过程，有助于理解反讽识别背后的认知机制，为实际落地提供了更强的透明性和信任基础。

Abstract: Sarcasm detection remains a significant challenge due to its reliance on nuanced contextual understanding, world knowledge, and multi-faceted linguistic cues that vary substantially across different sarcastic expressions. Existing approaches, from fine-tuned transformers to large language models, apply a uniform reasoning strategy to all inputs, struggling to address the diverse analytical demands of sarcasm. These demands range from modeling contextual expectation violations to requiring external knowledge grounding or recognizing specific rhetorical patterns. To address this limitation, we introduce RAM-SD, a Retrieval-Augmented Multi-Agent framework for Sarcasm Detection. The framework operates through four stages: (1) contextual retrieval grounds the query in both sarcastic and non-sarcastic exemplars; (2) a meta-planner classifies the sarcasm type and selects an optimal reasoning plan from a predefined set; (3) an ensemble of specialized agents performs complementary, multi-view analysis; and (4) an integrator synthesizes these analyses into a final, interpretable judgment with a natural language explanation. Evaluated on four standard benchmarks, RAM-SD achieves a state-of-the-art Macro-F1 of 77.74%, outperforming the strong GPT-4o+CoC baseline by 7.01 points. Our framework not only sets a new performance benchmark but also provides transparent and interpretable reasoning traces, illuminating the cognitive processes behind sarcasm comprehension.

</details>


### [166] [From Emotion to Expression: Theoretical Foundations and Resources for Fear Speech](https://arxiv.org/abs/2601.17132)
*Vigneshwaran Shankaran,Gabriella Lapesa,Claudia Wagner*

Main category: cs.CL

TL;DR: 该论文梳理和整合了对“恐惧言论”的跨学科理论，并提出了一个用于研究恐惧言论的分类体系。


<details>
  <summary>Details</summary>
Motivation: 虽然恐惧作为一种驱动社会与言论的力量非常强大，恐惧言论内容广泛且传播力强，但该现象在计算语言学领域尚未得到系统、跨学科的研究，因此需要对恐惧言论进行更深入的理论与实践梳理。

Method: 作者对心理学、政治学、传播学和语言学中的恐惧理论进行对比，系统回顾既有定义，调研相关领域的数据集，最终提出了一个整合不同维度的恐惧言论分类体系。

Result: 归纳总结了恐惧言论的理论背景、数据集情况和定义，提出了一个便于未来研究的数据集与概念基础。

Conclusion: 本文为建立更通用和系统的恐惧言论研究框架提供了理论和实践参考，有助于推动恐惧言论相关数据集的创建和研究的深入发展。

Abstract: Few forces rival fear in their ability to mobilize societies, distort communication, and reshape collective behavior. In computational linguistics, fear is primarily studied as an emotion, but not as a distinct form of speech. Fear speech content is widespread and growing, and often outperforms hate-speech content in reach and engagement because it appears "civiler" and evades moderation. Yet the computational study of fear speech remains fragmented and under-resourced. This can be understood by recognizing that fear speech is a phenomenon shaped by contributions from multiple disciplines. In this paper, we bridge cross-disciplinary perspectives by comparing theories of fear from Psychology, Political science, Communication science, and Linguistics. Building on this, we review existing definitions. We follow up with a survey of datasets from related research areas and propose a taxonomy that consolidates different dimensions of fear for studying fear speech. By reviewing current datasets and defining core concepts, our work offers both theoretical and practical guidance for creating datasets and advancing fear speech research.

</details>


### [167] [Dynamic Role Assignment for Multi-Agent Debate](https://arxiv.org/abs/2601.17152)
*Miao Zhang,Junsik Kim,Siyuan Xiang,Jian Gao,Cheng Cao*

Main category: cs.CL

TL;DR: 本文提出了一种动态角色分配框架，通过元辩论机制为多智能体大模型系统选择最合适的模型担当特定角色，从而提升问题解决能力。实验证明该方法在多项基准任务上显著优于传统的统一分配和随机分配方式。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体大模型（如LLM和VLM）辩论系统虽设有专业化角色，但在分配模型时未充分考虑各模型的特长与适配性，导致系统性能有限。作者希望借助动态机制，提升模型与角色的匹配度并优化整体表现。

Method: 作者提出一种动态角色分配的元辩论框架：首先在候选模型中启动两阶段元辩论——提案阶段由候选模型给出针对特定角色的理由，评审阶段则根据数据和角色相关标准打分，从而为每个角色挑选出最佳模型。该方法无缝嵌入到现有辩论系统之上。

Result: 在LLM问题求解基准任务中，该方法相较于统一分配（所有角色用同一模型）提升最高达74.8%，相较随机分配（不考虑适配性盲目分配）提升最高达29.7%。

Conclusion: 本工作为多智能体系统设计带来了新范式，将过去静态、预设代理角色的方式，转向基于模型能力动态分配的方法，有效提升系统性能和适应性。

Abstract: Multi-agent large language model (LLM) and vision-language model (VLM) debate systems employ specialized roles for complex problem-solving, yet model specializations are not leveraged to decide which model should fill which role. We propose dynamic role assignment, a framework that runs a Meta-Debate to select suitable agents before the actual debate. The meta-debate has two stages: (1) proposal, where candidates provide role-tailored arguments, and (2) peer review, where proposals are scored with data and role-specific criteria to choose the best agent for each position. We evaluate our method on LLM problem solving benchmarks. Applied on top of existing debate systems, our approach consistently outperforms uniform assignments (filling all roles with the same model) by up to 74.8% and random assignments (assigning models to roles without considering their suitability) by up to 29.7%, depending on the task and the specific assignment. This work establishes a new paradigm for multi-agent system design, shifting from static agent deployment to dynamic and capability-aware selection.

</details>


### [168] [Interpretability of the Intent Detection Problem: A New Approach](https://arxiv.org/abs/2601.17156)
*Eduardo Sanchez-Karhunen,Jose F. Quesada-Moreno,Miguel A. Gutiérrez-Naranjo*

Main category: cs.CL

TL;DR: 本文借助动态系统理论，从几何角度解释RNN在意图识别任务中的工作机制，揭示了数据集属性（如类别不平衡）如何影响RNN的内部状态空间结构及性能表现。


<details>
  <summary>Details</summary>
Motivation: 目前深度学习方法虽然在意图识别任务中非常成功，但RNN为何能够有效完成该任务的内部工作机制尚不清楚。理解RNN内部表征过程对于提升模型鲁棒性和解释性至关重要。

Method: 作者基于动态系统理论，将文本句子的表征视为RNN隐藏状态空间中的轨迹。以平衡的SNIPS数据集和不平衡的ATIS数据集为例，分析RNN隐藏状态空间如何通过几何结构（低维流形与聚类）来区分不同意图。进一步比较数据集属性变化（主要为类别分布失衡）对这种内部几何结构的影响。

Result: RNN在平衡的SNIPS数据集上能够学到理想的低维流形，每类意图在隐藏空间中形成分明聚类。而在不平衡的ATIS数据集上，低频类意图的聚类结构被扰乱和退化，导致性能下降。此外，作者提出几何分离与读出对齐可分，为现实性能不一致提供了新的机理解释。

Conclusion: 本文提出的分析框架为理解RNN内部动力学和几何结构与任务性能之间的关系提供了新视角，解释了数据集属性（特别是类别不平衡）对RNN模型表现的直接影响，也为模型调优和结构设计提供了理论依据。

Abstract: Intent detection, a fundamental text classification task, aims to identify and label the semantics of user queries, playing a vital role in numerous business applications. Despite the dominance of deep learning techniques in this field, the internal mechanisms enabling Recurrent Neural Networks (RNNs) to solve intent detection tasks are poorly understood. In this work, we apply dynamical systems theory to analyze how RNN architectures address this problem, using both the balanced SNIPS and the imbalanced ATIS datasets. By interpreting sentences as trajectories in the hidden state space, we first show that on the balanced SNIPS dataset, the network learns an ideal solution: the state space, constrained to a low-dimensional manifold, is partitioned into distinct clusters corresponding to each intent. The application of this framework to the imbalanced ATIS dataset then reveals how this ideal geometric solution is distorted by class imbalance, causing the clusters for low-frequency intents to degrade. Our framework decouples geometric separation from readout alignment, providing a novel, mechanistic explanation for real world performance disparities. These findings provide new insights into RNN dynamics, offering a geometric interpretation of how dataset properties directly shape a network's computational solution.

</details>


### [169] [Who Gets Which Message? Auditing Demographic Bias in LLM-Generated Targeted Text](https://arxiv.org/abs/2601.17172)
*Tunazzina Islam*

Main category: cs.CL

TL;DR: 本文首次系统性分析了大语言模型（LLMs）在面向特定人群生成定向信息时的行为表现，并发现模型在不同人口特征（如性别、年龄）条件下表现出明显的偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs能够大规模生成个性化、有说服力的文本，其在自动化交流中的偏见与公平性问题变得突出。作者希望揭示LLMs在人群定向信息生成中可能存在的刻板印象和不公平现象。

Method: 作者设计了一个受控评估框架，以GPT-4o、Llama-3.3和Mistral-Large 2.1为代表，对比了“独立生成”和“富上下文生成”两种环境下的信息生成效果，从词汇、语言风格、说服框架三个维度评价，聚焦于气候传播场景。

Result: 结果显示，模型在性别和年龄上具有显著的不对称：针对男性和青年的信息更强调主导性、创新和自信，而针对女性和老年人的信息更突出温暖、关怀和传统。加上下文后，这些差异进一步加剧，青年和男性定向信息的说服力评分更高。

Conclusion: LLMs在定向生成中会呈现并放大人口刻板印象，作者强调亟需开发具备偏见意识的生成管道及透明的审计机制，以应对社交敏感应用领域的人口特征偏见问题。

Abstract: Large language models (LLMs) are increasingly capable of generating personalized, persuasive text at scale, raising new questions about bias and fairness in automated communication. This paper presents the first systematic analysis of how LLMs behave when tasked with demographic-conditioned targeted messaging. We introduce a controlled evaluation framework using three leading models -- GPT-4o, Llama-3.3, and Mistral-Large 2.1 -- across two generation settings: Standalone Generation, which isolates intrinsic demographic effects, and Context-Rich Generation, which incorporates thematic and regional context to emulate realistic targeting. We evaluate generated messages along three dimensions: lexical content, language style, and persuasive framing. We instantiate this framework on climate communication and find consistent age- and gender-based asymmetries across models: male- and youth-targeted messages emphasize agency, innovation, and assertiveness, while female- and senior-targeted messages stress warmth, care, and tradition. Contextual prompts systematically amplify these disparities, with persuasion scores significantly higher for messages tailored to younger or male audiences. Our findings demonstrate how demographic stereotypes can surface and intensify in LLM-generated targeted communication, underscoring the need for bias-aware generation pipelines and transparent auditing frameworks that explicitly account for demographic conditioning in socially sensitive applications.

</details>


### [170] [Beyond Factual QA: Mentorship-Oriented Question Answering over Long-Form Multilingual Content](https://arxiv.org/abs/2601.17173)
*Parth Bhalerao,Diola Dsouza,Ruiwen Guan,Oana Ignat*

Main category: cs.CL

TL;DR: 本论文提出了MentorQA，这是首个针对长期视频内容提问的、多语言、专注于指导性回答的数据集与评测框架，强调超越事实正确性的多维度评价。作者系统比较了不同问答模型架构，发现多智能体方法在复杂话题及低资源语言上表现尤为突出。


<details>
  <summary>Details</summary>
Motivation: 传统问答系统仅关注事实正确性，但现实应用（如教育及职业指导）需要具备指导性和反思性的回答。现有问答基准在多语言和长文本场景下，极少评估指导能力，因此有必要建立新的评测体系。

Method: 提出MentorQA数据集，包含四语言、近9000对问答对（截至180小时视频），并设立反映指导性的新评测维度（如清晰度、对齐度与学习价值）。对比了单智能体、双智能体、检索增强生成、和多智能体等架构在该任务中的表现。同时评测了用大模型自动评价的可靠性。

Result: 多智能体问答流程在指导性回答质量上表现最佳，尤其在复杂主题和低资源语言环境中提升显著。自动化评测与人工评价存在对齐差异。

Conclusion: 本工作首次系统化提出指导型问答为独立研究方向，为教育AI的智能体架构与评测方法提供了多语言基准。MentorQA数据集和框架可支持后续相关研究。

Abstract: Question answering systems are typically evaluated on factual correctness, yet many real-world applications-such as education and career guidance-require mentorship: responses that provide reflection and guidance. Existing QA benchmarks rarely capture this distinction, particularly in multilingual and long-form settings. We introduce MentorQA, the first multilingual dataset and evaluation framework for mentorship-focused question answering from long-form videos, comprising nearly 9,000 QA pairs from 180 hours of content across four languages. We define mentorship-focused evaluation dimensions that go beyond factual accuracy, capturing clarity, alignment, and learning value. Using MentorQA, we compare Single-Agent, Dual-Agent, RAG, and Multi-Agent QA architectures under controlled conditions. Multi-Agent pipelines consistently produce higher-quality mentorship responses, with especially strong gains for complex topics and lower-resource languages. We further analyze the reliability of automated LLM-based evaluation, observing substantial variation in alignment with human judgments. Overall, this work establishes mentorship-focused QA as a distinct research problem and provides a multilingual benchmark for studying agentic architectures and evaluation design in educational AI. The dataset and evaluation framework are released at https://github.com/AIM-SCU/MentorQA.

</details>


### [171] [Systematicity between Forms and Meanings across Languages Supports Efficient Communication](https://arxiv.org/abs/2601.17181)
*Doreen Osmelak,Yang Xu,Michael Hahn,Kate McCurdy*

Main category: cs.CL

TL;DR: 论文研究了不同语言中动词和代词如何表达有限的语法意义，并提出了一种关于复杂度的新测量方法来解释语言形式的系统规律。


<details>
  <summary>Details</summary>
Motivation: 现有理论认为语言的词义—词形映射是为了高效交流，但忽视了词形内部的系统性关系。作者希望解释语言形式中的系统规律，以及这些规律如何服务于高效交流。

Method: 作者调查了跨语言类型多样的动词和代词在表达特定语法意义（如人称、数等）时的形式，提出了一种基于“可学习性”的新复杂度量方法，用以量化词形系统的规律性和复杂度。通过对比现有词形系统与未见系统，分析并模拟了不同交流压力（简化 vs. 准确性）下的表现。

Result: 结果发现，动词和代词的形式确实受限于简化（类别最小化）和准确性（歧义规避）的共同压力。而新提出的复杂度量指标可以更细致地区分已观察到和未被采用的语言系统，揭示出语言形式的系统性特征。

Conclusion: 作者的新模型为高效交流理论与自然语言中的系统性规律间建立了联系，为理解语言为什么会形成现在这样的词形系统提供了新的解释和工具。

Abstract: Languages vary widely in how meanings map to word forms. These mappings have been found to support efficient communication; however, this theory does not account for systematic relations within word forms. We examine how a restricted set of grammatical meanings (e.g. person, number) are expressed on verbs and pronouns across typologically diverse languages. Consistent with prior work, we find that verb and pronoun forms are shaped by competing communicative pressures for simplicity (minimizing the inventory of grammatical distinctions) and accuracy (enabling recovery of intended meanings). Crucially, our proposed model uses a novel measure of complexity (inverse of simplicity) based on the learnability of meaning-to-form mappings. This innovation captures fine-grained regularities in linguistic form, allowing better discrimination between attested and unattested systems, and establishes a new connection from efficient communication theory to systematicity in natural language.

</details>


### [172] [Reasoning Beyond Literal: Cross-style Multimodal Reasoning for Figurative Language Understanding](https://arxiv.org/abs/2601.17197)
*Seyyed Saeid Cheshmi,Hahnemann Ortiz,James Mooney,Dongyeop Kang*

Main category: cs.CL

TL;DR: 本文提出了一种三步框架，提升视觉语言模型（VLMs）对多模态隐喻语言（如讽刺、幽默、比喻）的理解能力，并实现了可转移和可解释的推理。实验表明，新框架显著提升了模型在多种隐喻风格下的理解能力，并优于更大规模的现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型虽擅长处理直白的多模态任务，但在理解讽刺、幽默、比喻等隐喻性语言时表现不佳。这些语言风格涉及深层意图和情感，同时受多模态信息（如图片）的影响，因此亟需提升模型的理解与推理能力。

Method: 提出三步框架：（1）解释多模态隐喻语言；（2）生成可追溯的推理过程；（3）实现多隐喻风格的泛化。设计实验对四种风格分别及联合训练，并检验推理能力及风格迁移效果。

Result: 实验结果显示：（1）推理过程的引入大幅提升了模型对隐喻语言的理解；（2）特定风格下的推理可以迁移到相关风格（如讽刺与幽默）；（3）多风格联合训练得到的模型，在泛化能力上优于大规模开源/闭源模型。

Conclusion: 轻量级、具备可验证推理能力的视觉语言模型，不仅提升了对多模态隐喻性语言的理解和泛化能力，还具备可追溯的推理过程，为相关多模态任务提供了新思路。

Abstract: Vision-language models (VLMs) have demonstrated strong reasoning abilities in literal multimodal tasks such as visual mathematics and science question answering. However, figurative language, such as sarcasm, humor, and metaphor, remains a significant challenge, as it conveys intent and emotion through subtle incongruities between expressed and intended meanings. In multimodal settings, accompanying images can amplify or invert textual meaning, demanding models that reason across modalities and account for subjectivity. We propose a three-step framework for developing efficient multimodal reasoning models that can (i) interpret multimodal figurative language, (ii) provide transparent reasoning traces, and (iii) generalize across multiple figurative styles. Experiments across four styles show that (1) incorporating reasoning traces substantially improves multimodal figurative understanding, (2) reasoning learned in one style can transfer to others, especially between related styles like sarcasm and humor, and (3) training jointly across styles yields a generalized reasoning VLM that outperforms much larger open- and closed-source models. Our findings show that lightweight VLMs with verifiable reasoning achieve robust cross-style generalization while providing inspectable reasoning traces for multimodal tasks. The code and implementation are available at https://github.com/scheshmi/CrossStyle-MMR.

</details>


### [173] [Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis](https://arxiv.org/abs/2601.17203)
*Scott Friedman,Sonja Schmer-Galunder,Anthony Chen,Jeffrey Rye*

Main category: cs.CL

TL;DR: 本论文提出了一种定量分析词嵌入中性别偏见的方法，并利用该方法关联并预测现实世界各领域的性别差距，通过推文数据验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 主流NLP模型常因训练文本存在性别与种族偏见而备受争议，而这些偏见本身或可反映真实社会的性别差距。本论文旨在通过量化词嵌入中的偏见，洞察背后的文化与社会现象。

Method: 作者提出了一种衡量词嵌入中性别偏见的新方法，并将获得的偏见指标与教育、政治、经济、健康等现实领域的性别差距进行统计关联。研究基于2018年涵盖美国51个地区和全球99个国家的推特数据，统计词嵌入偏见，并将其与18项国际性和5项美国本地性别差距指标进行相关性分析。

Result: 论文发现词嵌入中的性别偏见与多个实际性别差距数据之间存在统计相关性，这些指标在不同国家和地区具有较强的规律性和预测能力。

Conclusion: 本方法不仅有助于揭示和理解训练文本中蕴含的性别偏见，还能用来量化、表征并预测现实世界中的性别不平等，在文化背景建模与社会科学研究方面具有重要意义。

Abstract: Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health. We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.

</details>


### [174] [DF-RAG: Query-Aware Diversity for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.17212)
*Saadat Hasan Khan,Spencer Hong,Jingyu Wu,Kevin Lybarger,Youbing Yin,Erin Babinsky,Daben Liu*

Main category: cs.CL

TL;DR: 本文提出了DF-RAG方法，通过在检索环节引入多样性，有效提升了在推理密集型问题上的检索增强生成（RAG）性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG技术在面对需要复杂推理的问题时表现受限，常规的检索方式（如余弦相似度）虽然能找到相关内容，但容易导致检索内容重复、信息回收率降低，从而影响问答能力。

Method: 本文创新性地将最大边际相关性（Maximal Marginal Relevance, MMR）引入检索环节，确保每次检索到的信息片段既与问题高度相关，又能在彼此之间保持多样性。DF-RAG能动态调整检索多样性的程度，无需额外微调或先验信息。

Result: DF-RAG在多个推理密集型QA基准上，将F1分数提高了4-10个百分点，优于基于余弦相似度的传统RAG以及其他成熟基线方法。此外，DF-RAG取得了理论最高F1 18个百分点增益中的91.3%。

Conclusion: 引入多样性后的检索机制能够极大提升RAG系统在复杂问题上的表现，DF-RAG为推理型问答任务提供了有效、无需额外训练的提升方案。

Abstract: Retrieval-augmented generation (RAG) is a common technique for grounding language model outputs in domain-specific information. However, RAG is often challenged by reasoning-intensive question-answering (QA), since common retrieval methods like cosine similarity maximize relevance at the cost of introducing redundant content, which can reduce information recall. To address this, we introduce Diversity-Focused Retrieval-Augmented Generation (DF-RAG), which systematically incorporates diversity into the retrieval step to improve performance on complex, reasoning-intensive QA benchmarks. DF-RAG builds upon the Maximal Marginal Relevance framework to select information chunks that are both relevant to the query and maximally dissimilar from each other. A key innovation of DF-RAG is its ability to optimize the level of diversity for each query dynamically at test time without requiring any additional fine-tuning or prior information. We show that DF-RAG improves F1 performance on reasoning-intensive QA benchmarks by 4-10 percent over vanilla RAG using cosine similarity and also outperforms other established baselines. Furthermore, we estimate an Oracle ceiling of up to 18 percent absolute F1 gains over vanilla RAG, of which DF-RAG captures up to 91.3 percent.

</details>


### [175] [Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning](https://arxiv.org/abs/2601.17223)
*Massimiliano Pronesti,Anya Belz,Yufang Hou*

Main category: cs.CL

TL;DR: 本文提出了一种名为Verifiable Process Reward Models（VPRMs）的新型强化学习框架，用于通过可验证的规则监督大型语言模型的中间推理过程，并在医学证据合成中的偏倚评估任务取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 现有LLM强化学习方法大多仅在最终结果层面提供可验证奖励，而中间过程的监督方式依赖神经评判，缺乏透明度、易受偏见和奖励规避影响。研究动机在于，开发一种可对中间步骤进行确定性、规则化验证的机制，从而提升推理透明度和一致性，解决现有方法的局限。

Method: 作者提出VPRMs，通过将中间推理步骤用可执行规则（如流程图或准则）程序化校验，实现对推理过程的链式监督。以医学证据合成的偏倚风险评估为例，依据领域指南定义，将推理过程转换为可检验的决策路径，训练LLM依据这些路径进行推理。

Result: 在多个数据集上，VPRMs生成的推理严格符合领域规则，步骤决策与最终标签之间的一致性显著提升。F1分数最高较SOTA提升20%，比仅用结果验证的RL方法高出6.5%；证据基础性与逻辑一致性也有明显增长。

Conclusion: VPRMs为LLM推理监督提供了一种高透明度、高一致性的可验证过程激励方法，尤其适用于决策路径明确、可程序验证的任务，显著提高了推理质量和可靠性。

Abstract: Recent work on reinforcement learning with verifiable rewards (RLVR) has shown that large language models (LLMs) can be substantially improved using outcome-level verification signals, such as unit tests for code or exact-match checks for mathematics. In parallel, process supervision has long been explored as a way to shape the intermediate reasoning behaviour of LLMs, but existing approaches rely on neural judges to score chain-of-thought steps, leaving them vulnerable to opacity, bias, and reward hacking. To address this gap, we introduce Verifiable Process Reward Models (VPRMs), a reinforcement-learning framework in which intermediate reasoning steps are checked by deterministic, rule-based verifiers. We apply VPRMs to risk-of-bias assessment for medical evidence synthesis, a domain where guideline-defined criteria and rule-based decision paths enable programmatic verification of reasoning traces. Across multiple datasets, we find that VPRMs generate reasoning that adheres closely to domain rules and achieve substantially higher coherence between step-level decisions and final labels. Results show that VPRMs achieve up to 20% higher F1 than state-of-the-art models and 6.5% higher than verifiable outcome rewards, with substantial gains in evidence grounding and logical coherence.

</details>


### [176] [Retell, Reward, Repeat: Reinforcement Learning for Narrative Theory-Informed Story Generation](https://arxiv.org/abs/2601.17226)
*David Y. Liu,Xanthe Muston,Aditya Joshi,Sebastian Sequoiah-Grayson*

Main category: cs.CL

TL;DR: 本论文提出以强化学习（d-RLAIF）替代监督微调（SFT），提升自动故事生成的多样性与与人类叙事习惯的契合度。


<details>
  <summary>Details</summary>
Motivation: 自动故事生成的训练与评估通常依赖有限的‘标准答案’，难以全面衡量和生成更贴近人类创作的故事，需要寻找更合适的方法来优化模型输出。

Method: 论文基于Todorov叙事均衡理论，提出一套评判自动故事生成质量的原则。利用大语言模型（7B和14B）作为评审，通过这些原则对模型输出进行奖励，并采用Gemini-3-Flash对模型故事与人类故事进行对比评测。

Result: 实验表明，d-RLAIF方法生成的故事更加多样且更符合人类叙事规范，在定性和定量评测中均优于传统的监督微调方法。

Conclusion: 强化学习为主观性较强的自动故事生成等任务提供了一种更具前景的模型后训练方法，有助于输出更具人类风格的文本。

Abstract: Despite the subjective nature of storytelling, past works on automatic story generation (ASG) have relied on limited ground truths for training and evaluation. In this work, we explore reinforcement learning (d-RLAIF) as a post-training alternative to supervised fine-tuning (SFT). We first apply Todorov's Theory of Narrative Equilibrium to establish principles that define desirable ASG qualities. We prompt 7B and 14B LLM-as-judge models with our principles to test alignment with human annotators and provide reward signals during d-RLAIF. We use Gemini-3-Flash to evaluate the output of our post-trained models and compare them to human-written stories from the TimeTravel dataset. We show that d-RLAIF offers a viable alternative to supervised fine-tuning (SFT)--producing stories that are more diverse and aligned with human narrative conventions. Our paper demonstrates the promise of reinforcement learning for linguistically grounded post-training for subjective tasks such as ASG.

</details>


### [177] [CaseFacts: A Benchmark for Legal Fact-Checking and Precedent Retrieval](https://arxiv.org/abs/2601.17230)
*Akshith Reddy Putta,Jacob Devasier,Chengkai Li*

Main category: cs.CL

TL;DR: 本论文提出了一个名为CaseFacts的新基准数据集，用于检验自动事实核查模型在美最高法院判例法律领域的表现，其任务是验证以通俗表述提出的法律主张的真实性。作者发现，当前大模型对此类复杂法律问题仍表现有限。


<details>
  <summary>Details</summary>
Motivation: 以往自动事实核查研究集中在静态领域的常识核查，忽视了如法律这样高风险、知识不断演化且技术门槛高的领域。法律领域事实核查尤具挑战，因为需要跨越通俗表达和专业法理之间的鸿沟，并准确处理判例时效性。

Method: 作者构建了CaseFacts数据集，包含6294条法律主张（分为已支持、被驳回、已推翻三类），用多阶段流程和大语言模型从专家判例摘要中自动生成主张，并通过创新的语义相似性启发方法高效筛查复杂的推翻案例，还分析了当前大语言模型与启用网页检索后的核查效果。

Result: 实验表明，当前先进的大语言模型在该法律事实核查任务中的表现仍然有限。当增加不受限的网页检索辅助时，模型效果反而下降，因为检索到的资料含有噪声且缺乏权威性。

Conclusion: CaseFacts为法律事实核查提供了重要评测基准，揭示了跨语义层、时间敏感的法律主张核查对现有模型的挑战。该数据集的发布旨在推动相关研究发展。

Abstract: Automated Fact-Checking has largely focused on verifying general knowledge against static corpora, overlooking high-stakes domains like law where truth is evolving and technically complex. We introduce CaseFacts, a benchmark for verifying colloquial legal claims against U.S. Supreme Court precedents. Unlike existing resources that map formal texts to formal texts, CaseFacts challenges systems to bridge the semantic gap between layperson assertions and technical jurisprudence while accounting for temporal validity. The dataset consists of 6,294 claims categorized as Supported, Refuted, or Overruled. We construct this benchmark using a multi-stage pipeline that leverages Large Language Models (LLMs) to synthesize claims from expert case summaries, employing a novel semantic similarity heuristic to efficiently identify and verify complex legal overrulings. Experiments with state-of-the-art LLMs reveal that the task remains challenging; notably, augmenting models with unrestricted web search degrades performance compared to closed-book baselines due to the retrieval of noisy, non-authoritative precedents. We release CaseFacts to spur research into legal fact verification systems.

</details>


### [178] [Frame-Guided Synthetic Claim Generation for Automatic Fact-Checking Using High-Volume Tabular Data](https://arxiv.org/abs/2601.17232)
*Jacob Devasier,Akshith Putta,Qing Wang,Alankrit Moses,Chengkai Li*

Main category: cs.CL

TL;DR: 本文提出了一个大规模、多语言的自动化事实核查数据集，旨在推动模型对真实世界复杂结构化数据的核查能力。


<details>
  <summary>Details</summary>
Motivation: 当前自动化事实核查的基准主要针对小型、人工整理的表格，而忽视了对真实世界高体量结构化数据核查的挑战。为解决该领域的空白，作者提出新的基准和数据集。

Method: 作者基于434个复杂OECD表格，利用六种语义框架程序化选取关键数据点，自动生成了78,503个合成主张，覆盖英文、中文、西班牙文和印地语。通过知识探查实验，验证了这些事实未被大模型记忆，需系统真正检索与推理。还提供了SQL生成的基线系统评估任务难度。

Result: 分析表明该任务极具挑战性，现有模型在从超大表格中检索证据时表现不佳，证据检索为主要瓶颈。

Conclusion: 所提数据集为真实世界结构化数据的事实核查研究提供了重要资源，有助于推动该领域发展。

Abstract: Automated fact-checking benchmarks have largely ignored the challenge of verifying claims against real-world, high-volume structured data, instead focusing on small, curated tables. We introduce a new large-scale, multilingual dataset to address this critical gap. It contains 78,503 synthetic claims grounded in 434 complex OECD tables, which average over 500K rows each. We propose a novel, frame-guided methodology where algorithms programmatically select significant data points based on six semantic frames to generate realistic claims in English, Chinese, Spanish, and Hindi. Crucially, we demonstrate through knowledge-probing experiments that LLMs have not memorized these facts, forcing systems to perform genuine retrieval and reasoning rather than relying on parameterized knowledge. We provide a baseline SQL-generation system and show that our benchmark is highly challenging. Our analysis identifies evidence retrieval as the primary bottleneck, with models struggling to find the correct data in massive tables. This dataset provides a critical new resource for advancing research on this unsolved, real-world problem.

</details>


### [179] [PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues](https://arxiv.org/abs/2601.17277)
*Mohammad Rifqi Farhansyah,Hanif Muhammad Zhafran,Farid Adilazuarda,Shamsuddeen Hassan Muhammad,Maryam Ibrahim Mukhtar,Nedjma Ousidhoum,Genta Indra Winata,Ayu Purwarianti,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 该论文提出了一个名为PingPong的新基准，用于真实多方语码转换对话，涵盖多种语言组合，强调其自然性和结构多样性，并用于多项下游任务评测现有模型表现。


<details>
  <summary>Details</summary>
Motivation: 尽管全球多语使用者广泛进行语码转换，但现有基准难以准确反映现实交流的复杂性。

Method: 作者构建了一个包含5种语言组合（含部分三语组合）的人类撰写多方对话数据集。对话涉及2-4位参与者，拥有真实多线程结构。对比机生成数据，展示该数据集在消息长度、说话人主导性、回复距离等方面更自然和多样。基于此，定义了问答、摘要和主题分类三项任务，并用SOTA模型在新基准上进行评测。

Result: 结果显示，无论是人类还是SOTA模型，在PingPong对话集上的代码切换输入下表现都较为有限，表明现有NLP系统在处理真实多语语码转换问题上能力不足。

Conclusion: 该研究突显了现有系统在应对多语语码转换时的局限性，呼吁发展更强健的NLP模型以适应现实多语交流的复杂需求。

Abstract: Code-switching is a widespread practice among the world's multilingual majority, yet few benchmarks accurately reflect its complexity in everyday communication. We present PingPong, a benchmark for natural multi-party code-switching dialogues covering five language-combination variations, some of which are trilingual. Our dataset consists of human-authored conversations among 2 to 4 participants covering authentic, multi-threaded structures where replies frequently reference much earlier points in the dialogue. We demonstrate that our data is significantly more natural and structurally diverse than machine-generated alternatives, offering greater variation in message length, speaker dominance, and reply distance. Based on these dialogues, we define three downstream tasks: Question Answering, Dialogue Summarization, and Topic Classification. Evaluations of several state-of-the-art language models on PingPong reveal that performance remains limited on code-switched inputs, underscoring the urgent need for more robust NLP systems capable of addressing the intricacies of real-world multilingual discourse.

</details>


### [180] [Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering](https://arxiv.org/abs/2601.17284)
*Yaokun Liu,Yifan Liu,Phoebe Mbuvi,Zelin Li,Ruichen Yao,Gawon Lim,Dong Wang*

Main category: cs.CL

TL;DR: 本文提出CV-MedBench数据集，专注于研究医疗问答中的输入歧义问题，并基于此提出高效的澄清机制，提高问答准确率和安全性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在医疗问答领域受到输入歧义的限制，导致回答准确性和安全性降低，尤其在高风险场景下问题突出。因此，亟需对应的基准和方法，主动应对输入歧义带来的不确定性，实现更可靠的医疗智能问答。

Method: 作者将输入歧义与不可约化不确定性（Aleatoric Uncertainty, AU）相联系，并首次提出CV-MedBench基准，专门用于衡量医疗问答中的输入歧义。通过分析模型内部激活，发现AU可以线性编码在LLM中。基于此，提出了AU-guided“先澄清再答复”Framework，加入AU-Probe模块，可以直接从隐藏状态探测输入歧义，且无需模型微调或多次前向推理，高效提升问答流程。

Result: 在四个主流开源LLM上实验显示，该新框架实现了平均9.48%的准确率提升，有效增强了医疗问答的安全性和可靠性。

Conclusion: 提出的基准、框架和模块为输入歧义问题提供了高效、可行的解决方案，对提升医疗相关AI系统的安全性、可靠性具有显著价值。相关数据与代码已开源，便于社区进一步研究与应用。

Abstract: The deployment of Large Language Models in Medical Question Answering is severely hampered by ambiguous user queries, a significant safety risk that demonstrably reduces answer accuracy in high-stakes healthcare settings. In this paper, we formalize this challenge by linking input ambiguity to aleatoric uncertainty (AU), which is the irreducible uncertainty arising from underspecified input. To facilitate research in this direction, we construct CV-MedBench, the first benchmark designed for studying input ambiguity in Medical QA. Using this benchmark, we analyze AU from a representation engineering perspective, revealing that AU is linearly encoded in LLM's internal activation patterns. Leveraging this insight, we introduce a novel AU-guided "Clarify-Before-Answer" framework, which incorporates AU-Probe - a lightweight module that detects input ambiguity directly from hidden states. Unlike existing uncertainty estimation methods, AU-Probe requires neither LLM fine-tuning nor multiple forward passes, enabling an efficient mechanism to proactively request user clarification and significantly enhance safety. Extensive experiments across four open LLMs demonstrate the effectiveness of our QA framework, with an average accuracy improvement of 9.48% over baselines. Our framework provides an efficient and robust solution for safe Medical QA, strengthening the reliability of health-related applications. The code is available at https://github.com/yaokunliu/AU-Med.git, and the CV-MedBench dataset is released on Hugging Face at https://huggingface.co/datasets/yaokunl/CV-MedBench.

</details>


### [181] [Meta-Judging with Large Language Models: Concepts, Methods, and Challenges](https://arxiv.org/abs/2601.17312)
*Hugo Silva,Mateus Mendes,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 本文回顾了LLM作为评估者（LLM-as-a-Judge）存在的主要问题，并系统梳理了LLM作为元评审员（LLM-as-a-Meta-Judge）领域的发展和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 传统使用LLM作为评审员进行模型输出质量评估，但已被发现存在诸如提示敏感性、系统性偏差、冗余、以及理由不可靠甚至出现幻觉等问题。为提高评估的可靠性和稳健性，业界亟需更为强健的新范式。

Method: 作者系统性地梳理了LLM-as-a-Meta-Judge的研究进展，并提出以六大关键视角（概念基础、元评审机制、对齐训练方法、评估、局限性及未来方向）为框架，对相关文献进行综述分类与总结。

Result: 文章揭示了LLM-as-a-Judge的局限性，同时总结分析了LLM元评审（Meta-Judging）取得的最新进展与技术优势，并指出该方向在自动化评测中的稳定性和可信度有显著提升。

Conclusion: LLM-as-a-Meta-Judge为自动化评估带来更高稳定性和可信度，但仍需解决成本、提示敏感性和模型共性偏见等问题，推动下一代评测方法发展。

Abstract: Large language models (LLMs) are evolving fast and are now frequently used as evaluators, in a process typically referred to as LLM-as-a-Judge, which provides quality assessments of model outputs. However, recent research points out significant vulnerabilities in such evaluation, including sensitivity to prompts, systematic biases, verbosity effects, and unreliable or hallucinated rationales. These limitations motivated the development of a more robust paradigm, dubbed LLM-as-a-Meta-Judge. This survey reviews recent advances in meta-judging and organizes the literature, by introducing a framework along six key perspectives: (i) Conceptual Foundations, (ii) Mechanisms of Meta-Judging, (iii) Alignment Training Methods, (iv) Evaluation, (v) Limitations and Failure Modes, and (vi) Future Directions. By analyzing the limitations of LLM-as-a-Judge and summarizing recent advances in meta-judging by LLMs, we argue that LLM-as-a-Meta-Judge offers a promising direction for more stable and trustworthy automated evaluation, while highlighting remaining challenges related to cost, prompt sensitivity, and shared model biases, which must be addressed to advance the next generation of LLM evaluation methodologies.

</details>


### [182] [The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents](https://arxiv.org/abs/2601.17344)
*Chen Chen,Kim Young Il,Yuan Yang,Wenhao Su,Yilin Zhang,Xueluan Gong,Qian Wang,Yongsen Zheng,Ziyao Liu,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 本文提出了一种新评测框架，系统性检验大语言模型（LLM）在现实中可能出现的固有价值偏差（Intrinsic Value Misalignment, VM）风险，并用该框架测试了多种先进模型。结果显示，固有价值偏差在现有LLM代理中普遍存在，且现有防护方法效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全评估多聚焦在对明确有害输入的响应或系统鲁棒性上，但并未充分考察在无害、现实和自主情境下，LLM也可能出现价值偏差（即模型行为偏离人类价值观和伦理准则），这一“固有价值偏差”存在未知安全风险。因此需要一个新的评测框架来填补此领域空白。

Method: 作者首先形式化地定义了“失控风险”和“固有价值偏差”。随后提出IMPRESS评测框架，基于一组详细刻画现实、无害、具有代理能力的情境任务，通过多阶段模型生成和严格质量管控，构建了对这些风险的系统性评测基准，并对21个先进LLM代理进行了系统测试。同时还加入了人工验证和防护方法（如安全提示、护栏机制）的考查。

Result: 评测发现：几乎所有主流LLM代理都普遍存在固有价值偏差风险，且该风险随任务动机、风险类型、模型体量、模型架构有明显差异。解码策略和超参数影响很小，但情境设置和任务表述对价值偏差影响很大。现有的安全防护策略（如安全提示、护栏）表现不稳定且效果有限。

Conclusion: 固有价值偏差是当前LLM代理中的广泛风险，需引起关注。IMPRESS框架为社区提供了系统、现实、可扩展的评测工具，有助于持续监控和改进AI代理的价值对齐。公开数据和工具也将促进相关研究发展。

Abstract: Large language model (LLM) agents with extended autonomy unlock new capabilities, but also introduce heightened challenges for LLM safety. In particular, an LLM agent may pursue objectives that deviate from human values and ethical norms, a risk known as value misalignment. Existing evaluations primarily focus on responses to explicit harmful input or robustness against system failure, while value misalignment in realistic, fully benign, and agentic settings remains largely underexplored. To fill this gap, we first formalize the Loss-of-Control risk and identify the previously underexamined Intrinsic Value Misalignment (Intrinsic VM). We then introduce IMPRESS (Intrinsic Value Misalignment Probes in REalistic Scenario Set), a scenario-driven framework for systematically assessing this risk. Following our framework, we construct benchmarks composed of realistic, fully benign, and contextualized scenarios, using a multi-stage LLM generation pipeline with rigorous quality control. We evaluate Intrinsic VM on 21 state-of-the-art LLM agents and find that it is a common and broadly observed safety risk across models. Moreover, the misalignment rates vary by motives, risk types, model scales, and architectures. While decoding strategies and hyperparameters exhibit only marginal influence, contextualization and framing mechanisms significantly shape misalignment behaviors. Finally, we conduct human verification to validate our automated judgments and assess existing mitigation strategies, such as safety prompting and guardrails, which show instability or limited effectiveness. We further demonstrate key use cases of IMPRESS across the AI Ecosystem. Our code and benchmark will be publicly released upon acceptance.

</details>


### [183] [Do readers prefer AI-generated Italian short stories?](https://arxiv.org/abs/2601.17363)
*Michael Farrell*

Main category: cs.CL

TL;DR: 本文比较了意大利读者对AI生成短篇小说与知名作家作品的偏好，发现AI文本略受青睐且读者的背景因素无显著影响。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成内容在文学领域的应用日益广泛，但人们普遍认为人类作家的作品质量更高。作者希望检验在不告知作者身份的情况下，读者是否真能偏好人类作者的作品，并探讨影响这一偏好的潜在因素。

Method: 采用盲测方法，让20名参与者阅读并评价两篇由ChatGPT-4o生成和一篇由意大利作家Alberto Moravia创作的短篇小说。收集参与者的阅读习惯、年龄、性别、教育水平和母语等数据，并分析这些变量与偏好之间的关系。

Result: AI创作的文本获得了略高的平均评分，并且在偏好选择上更受欢迎。但两者差异不大。参与者的偏好与年龄、性别、教育、母语及阅读习惯等无统计学显著相关性。

Conclusion: 结果挑战了人们对人类文学作品优越性的假设，同时表明在无作者信息的条件下，AI生成文本在文学场景中无需过度编辑，也能获得读者认可。

Abstract: This study investigates whether readers prefer AI-generated short stories in Italian over one written by a renowned Italian author. In a blind setup, 20 participants read and evaluated three stories, two created with ChatGPT-4o and one by Alberto Moravia, without being informed of their origin. To explore potential influencing factors, reading habits and demographic data, comprising age, gender, education and first language, were also collected. The results showed that the AI-written texts received slightly higher average ratings and were more frequently preferred, although differences were modest. No statistically significant associations were found between text preference and demographic or reading-habit variables. These findings challenge assumptions about reader preference for human-authored fiction and raise questions about the necessity of synthetic-text editing in literary contexts.

</details>


### [184] [Parameter Efficient Fine Tuning Llama 3.1 for Answering Arabic Legal Questions: A Case Study on Jordanian Laws](https://arxiv.org/abs/2601.17364)
*Mohammed Fasha,Bassam Hammo,Bilal Sowan,Husam Barham,Esam Nsour*

Main category: cs.CL

TL;DR: 本研究针对阿拉伯语法律问答领域，基于约旦法律数据，微调了Llama-3.1大语言模型，提升了法律推理能力和准确性，并实现了资源高效利用。


<details>
  <summary>Details</summary>
Motivation: 当前主流大语言模型多以英语为主，对阿拉伯语及法律领域的适配性较差。作者希望通过微调，验证Llama-3.1模型在低资源、四位量化条件下能否提升阿拉伯语法律问答的表现，并探索适合该领域的高效训练方法。

Method: 选用Llama-3.1-8B两种基座模型，在采用PEFT（参数高效微调）和LoRA适配器、Unsloth框架的基础上，以四位量化模型进行加速和节省资源训练。自建了6000个约旦法律问答对数据集，按结构化提示格式进行训练。最终通过BLEU和ROUGE指标评估微调前后的性能。

Result: 微调后的模型在法律推理能力和问答准确性方面均优于基础模型，同时由于采用量化和高效微调方法，在资源利用上表现突出。

Conclusion: 实验证明Llama-3.1等大模型经过优化后能更好适应阿拉伯语法律任务，且相关的高效微调和量化技术为领域模型适配提供了有效路径。

Abstract: This study uses Jordanian law as a case study to explore the fine-tuning of the Llama-3.1 large language model for Arabic question-answering. Two versions of the model - Llama-3.1-8B-bnb-4bit and Llama-3.1-8B-Instruct-bnb-4bit - were fine-tuned using parameter-efficient fine-tuning (PEFT) with LoRA adapters and 4-bit quantized models, leveraging the Unsloth framework for accelerated and resource-efficient training. A custom dataset of 6000 legal question-answer pairs was curated from Jordanian laws and formatted into structured prompts. Performance was evaluated using the BLEU and the ROUGE metrics to compare the fine-tuned models to their respective base versions. Results demonstrated improved legal reasoning and accuracy while achieving resource efficiency through quantization and optimized fine-tuning strategies. This work underscores the potential of adapting large language models for Arabic legal domains and highlights effective techniques for fine-tuning domain-specific tasks.

</details>


### [185] [Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers](https://arxiv.org/abs/2601.17367)
*Zecheng Tang,Quantong Qiu,Yi Yang,Zhiyi Hong,Haiya Xiang,Kebin Liu,Qingqing Dang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为弹性注意力（Elastic Attention）的机制，通过动态调整注意力稀疏度来提升大模型在长文本场景下的效率与表现。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制的计算复杂度为二次方，阻碍了大语言模型在长上下文中的扩展性。现有的混合稀疏与全注意力方案采用静态比例，无法适配不同任务在推理阶段对稀疏度的多样需求。

Method: 作者提出在原有预训练模型中集成一个轻量级Attention Router，通过该路由器动态分配每个注意力头的计算模式，从而实现模型根据输入灵活调整稀疏度。该方法只需12小时训练（8张A800显卡），便能够收敛。

Result: 在三个人工智能长文本标准测试集上，作者方法在多个主流LLM上实现了更优的性能与推理效率。

Conclusion: 弹性注意力不仅提升了模型的推理效率，还能在保持性能的前提下适配不同任务的需求，优于固定比例的混合注意力方法，对长文本处理场景具有广泛应用前景。

Abstract: The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.

</details>


### [186] [WarrantScore: Modeling Warrants between Claims and Evidence for Substantiation Evaluation in Peer Reviews](https://arxiv.org/abs/2601.17377)
*Kiyotada Mori,Shohei Tanaka,Tosho Hirasawa,Tadashi Kozuno,Koichiro Yoshino,Yoshitaka Ushiku*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，通过评估科学评审评论中‘主张’与‘证据’之间的逻辑推理，自动判断评论观点是否有据可依，并取得比以往方法更高的人类评审分数相关性。


<details>
  <summary>Details</summary>
Motivation: 随着被投稿论文数量急剧增长，科学领域的同行评审面临人力资源短缺。利用语言模型协助评审被视为减轻人力负担的方向，因此需要开发能自动评判评审意见质量的方法。

Method: 首先自动抽取科学评审意见中的‘主张’和‘证据’，进而不仅判断‘证据’是否存在，还评估‘主张’与‘证据’间的逻辑推理关系，并据此量化‘主张’被证据支持的程度，提出了新的评价指标。

Result: 实验显示，作者提出的方法与人类评审评分的相关性显著高于传统方法，更贴近人工评审标准。

Conclusion: 该方法有望提升同行评审的自动化水平和人力效率，能更好地支持评审流程。

Abstract: The scientific peer-review process is facing a shortage of human resources due to the rapid growth in the number of submitted papers. The use of language models to reduce the human cost of peer review has been actively explored as a potential solution to this challenge. A method has been proposed to evaluate the level of substantiation in scientific reviews in a manner that is interpretable by humans. This method extracts the core components of an argument, claims and evidence, and assesses the level of substantiation based on the proportion of claims supported by evidence. The level of substantiation refers to the extent to which claims are based on objective facts. However, when assessing the level of substantiation, simply detecting the presence or absence of supporting evidence for a claim is insufficient; it is also necessary to accurately assess the logical inference between a claim and its evidence. We propose a new evaluation metric for scientific review comments that assesses the logical inference between claims and evidence. Experimental results show that the proposed method achieves a higher correlation with human scores than conventional methods, indicating its potential to better support the efficiency of the peer-review process.

</details>


### [187] [Revisiting Modality Invariance in a Multilingual Speech-Text Model via Neuron-Level Analysis](https://arxiv.org/abs/2601.17387)
*Toshiki Nakai,Varsha Suresh,Vera Demberg*

Main category: cs.CL

TL;DR: 本文研究了多语言语音文本基础模型（如SeamlessM4T v2）在处理不同模态（语音与文本）时，内部是否存在一致的语言表达方式。结果显示，模型内部存在不完全的模态不变性，尤其在从语音到文本转换时，语言信息的恢复变得更难。某些关键神经元对模态和语言的区分具有高度选择性，这种集中激活甚至可能导致模型跨模态和语言时表现更脆弱。


<details>
  <summary>Details</summary>
Motivation: 多语言语音-文本基础模型的目标是在多种语言与模态之间实现统一处理，但尚不清楚模型内部是否能在语音与文本间保持一致的语言表示，对理解其泛化和弱点有重要意义。

Method: 通过三种方法分析SeamlessM4T v2：1）利用平均精度排序识别对语言和模态敏感的神经元；2）在推理时采用中位数替换干预，测试这些神经元对解码的因果影响；3）分析不同语言、模态下激活强度的不均衡程度。

Result: 发现编码器表示虽趋于去语言化，但这种压缩导致解码器难以重建原语言，特别是从语音到文本时。此外，交叉注意力层中存在高度局部化的模态选择特征，语音条件下及非主导文字系统的解码激活分布更集中，意味着模型更依赖少数神经元。

Conclusion: 多语言语音-文本模型内部并不能完全模态无关，且特定神经元的高依赖性可能导致跨模态和语言时模型的脆弱性，这为后续改进多模态语言模型提供了新思路。

Abstract: Multilingual speech-text foundation models aim to process language uniformly across both modality and language, yet it remains unclear whether they internally represent the same language consistently when it is spoken versus written. We investigate this question in SeamlessM4T v2 through three complementary analyses that probe where language and modality information is encoded, how selective neurons causally influence decoding, and how concentrated this influence is across the network. We identify language- and modality-selective neurons using average-precision ranking, investigate their functional role via median-replacement interventions at inference time, and analyze activation-magnitude inequality across languages and modalities. Across experiments, we find evidence of incomplete modality invariance. Although encoder representations become increasingly language-agnostic, this compression makes it more difficult for the shared decoder to recover the language of origin when constructing modality-agnostic representations, particularly when adapting from speech to text. We further observe sharply localized modality-selective structure in cross-attention key and value projections. Finally, speech-conditioned decoding and non-dominant scripts exhibit higher activation concentration, indicating heavier reliance on a small subset of neurons, which may underlie increased brittleness across modalities and languages.

</details>


### [188] [CLM-Bench: Benchmarking and Analyzing Cross-lingual Misalignment of LLMs in Knowledge Editing](https://arxiv.org/abs/2601.17397)
*Yucheng Hu,Wei Zhou,Juesi Xiao*

Main category: cs.CL

TL;DR: 本文提出了CLM-Bench，一个以中文文化为基础并兼具英中对齐的知识编辑基准，用以更公平并有效地评估大型语言模型（LLM）的多语言知识编辑能力。作者揭示了现有多语言知识编辑评估存在的偏差，并实验性分析了跨语言知识编辑的局限性。


<details>
  <summary>Details</summary>
Motivation: 目前多语言知识编辑常以英文数据集翻译得来，未能体现目标语言的文化特性，导致对LLM中文能力和跨语言知识编辑的评估不准确。作者希望通过原生中文数据集改善这一状况，并揭示现有方法在跨语言迁移上的不足。

Method: 提出CLM-Bench基准，通过中文本地优先构建涵盖中国文化背景的1010组高质量编辑对，并对其与英文进行对齐；在典型LLM如Llama-3、Qwen2等上进行大规模实验，结合层级向量几何分析，探讨了知识编辑在中英文之间的表现和底层原因。

Result: 发现LLM的知识编辑在不同语言间难以互相迁移，即单语的编辑影响不会有效传播到另一种语言。通过向量几何分析，中英文编辑向量几乎正交，分别存在于不同子空间；混合语言编辑则表现为简单线性叠加。

Conclusion: 现有跨语言知识编辑方法在语言迁移过程中存在实质性障碍，强调了以文化为基础、本地化基准集的重要性。该工作对多语言LLM评估和知识编辑方法改进具有参考价值。

Abstract: Knowledge Editing (KE) has emerged as a promising paradigm for updating facts in Large Language Models (LLMs) without retraining. However, progress in Multilingual Knowledge Editing (MKE) is currently hindered by biased evaluation frameworks. We observe that existing MKE benchmarks are typically constructed by mechanically translating English-centric datasets into target languages (e.g., English-to-Chinese). This approach introduces translation artifacts and neglects culturally specific entities native to the target language, failing to reflect the true knowledge distribution of LLMs. To address this, we propose CLM-Bench, a culture-aware benchmark constructed using a native Chinese-first methodology. We curate 1,010 high-quality CounterFact pairs rooted in Chinese cultural contexts and align them with English counterparts. Using CLM-Bench, we conduct extensive experiments on representative LLMs (e.g., Llama-3, Qwen2) and reveal a significant Cross-lingual Misalignment: edits in one language function independently and fail to propagate to the other. We further provide a geometric explanation via layer-wise representation analysis, demonstrating that edit vectors for Chinese and English are nearly orthogonal -- residing in disjoint subspaces -- while mixed-lingual editing exhibits linear additivity of these vectors. Our findings challenge the effectiveness of current methods in cross-lingual transfer and underscore the importance of culturally native benchmarks.

</details>


### [189] [Oops, Wait: Token-Level Signals as a Lens into LLM Reasoning](https://arxiv.org/abs/2601.17421)
*Jaehui Hwang,Dongyoon Han,Sangdoo Yun,Byeongho Heo*

Main category: cs.CL

TL;DR: 本文系统分析了大语言模型（LLM）在推理过程中出现的类话语词（如“wait”“therefore”）的信号及其随训练策略和模型规模的变化关系。


<details>
  <summary>Details</summary>
Motivation: 先前注意到LLM生成类话语词可能与模型推理过程相关，但缺乏针对这些信号、训练策略、模型规模之间系统性的量化分析。

Method: 作者比较了不同模型、训练策略和规模下，针对特定token的概率输出，尤其关注如“wait”这样的token在推理过程中的表现。

Result: 发现特定token（如“wait”“therefore”）的出现概率与推理正确性高度相关；这些信号随训练策略不同而变化，但在不同规模的模型间表现稳定。微调于小规模数据集的模型能通过这些信号获得一定推理能力，但未能充分利用这些信号。

Conclusion: 类话语token概率为理解LLM推理动态提供了有效视角，系统化分析有助于进一步揭示模型内部推理机制。

Abstract: The emergence of discourse-like tokens such as "wait" and "therefore" in large language models (LLMs) has offered a unique window into their reasoning processes. However, systematic analyses of how such signals vary across training strategies and model scales remain lacking. In this paper, we analyze token-level signals through token probabilities across various models. We find that specific tokens strongly correlate with reasoning correctness, varying with training strategies while remaining stable across model scales. A closer look at the "wait" token in relation to answer probability demonstrates that models fine-tuned on small-scale datasets acquire reasoning ability through such signals but exploit them only partially. This work provides a systematic lens to observe and understand the dynamics of LLM reasoning.

</details>


### [190] [Clustering-driven Memory Compression for On-device Large Language Models](https://arxiv.org/abs/2601.17443)
*Ondrej Bohdal,Pramit Saha,Umberto Michieli,Mete Ozay,Taha Ceritli*

Main category: cs.CL

TL;DR: 本文提出了一种基于聚类的记忆压缩策略，实现了更高效和高质量的个性化生成。相较于以往方法，新方法在减少记忆token数量的同时，提升了生成表现。


<details>
  <summary>Details</summary>
Motivation: 传统做法通过简单拼接或平均用户记忆来实现个性化，但这些方法要么快速耗尽上下文长度，要么导致语义冲突和效果下降。因此，亟需兼顾高效性与质量的记忆压缩手段。

Method: 将用户过往的记忆按照相似性聚类，每个聚类内再进行信息合并，最后再拼接到输入提示中。这种聚类合并方式减少了冗余，并保留了信息连贯性。

Result: 实验显示，该方法显著减少了记忆token数量，并在各项指标上优于直接拼接和简单平均。对于相同的上下文预算下，也能提供更紧凑且表现更优的个性化记忆。

Conclusion: 聚类驱动的记忆合并策略兼顾了上下文利用效率和个性化质量，在个性化生成场景中为LLM带来新提升，优于常规压缩与拼接方法。

Abstract: Large language models (LLMs) often rely on user-specific memories distilled from past interactions to enable personalized generation. A common practice is to concatenate these memories with the input prompt, but this approach quickly exhausts the limited context available in on-device LLMs. Compressing memories by averaging can mitigate context growth, yet it frequently harms performance due to semantic conflicts across heterogeneous memories. In this work, we introduce a clustering-based memory compression strategy that balances context efficiency and personalization quality. Our method groups memories by similarity and merges them within clusters prior to concatenation, thereby preserving coherence while reducing redundancy. Experiments demonstrate that our approach substantially lowers the number of memory tokens while outperforming baseline strategies such as naive averaging or direct concatenation. Furthermore, for a fixed context budget, clustering-driven merging yields more compact memory representations and consistently enhances generation quality.

</details>


### [191] [Revealing the Truth with ConLLM for Detecting Multi-Modal Deepfakes](https://arxiv.org/abs/2601.17530)
*Gautam Siddharth Kashyap,Harsh Joshi,Niharika Jain,Ebad Shabbir,Jiechao Gao,Nipun Joshi,Usman Naseem*

Main category: cs.CL

TL;DR: 本论文提出了一种新的多模态深度伪造检测框架ConLLM，通过对比学习和大语言模型结合，有效提升了音频、视频及音视频伪造的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测方法在不同伪造类型之间泛化能力差，且难以捕捉细粒度的语义不一致，导致检测能力有限。为此，作者希望构建一个能够克服模态碎片化和增强跨模态推理能力的新框架。

Method: 提出了ConLLM框架，分为两阶段：第一阶段利用预训练模型提取各模态特征，第二阶段通过对比学习对齐嵌入，结合大语言模型推理强化对细致语义不一致的检测能力。

Result: ConLLM在音频、视频、音视频多模态上都实现了明显性能提升，音频伪造EER降低50%，视频提升8%准确率，音视频综合提高约9%，消融实验表明PTM嵌入具有9%-10%的稳定提升。

Conclusion: ConLLM在多种深度伪造检测场景下都表现出强泛化和更高的检测准确率，为提升现实应用中多模态深度伪造检测的可用性和鲁棒性带来了重要进展。

Abstract: The rapid rise of deepfake technology poses a severe threat to social and political stability by enabling hyper-realistic synthetic media capable of manipulating public perception. However, existing detection methods struggle with two core limitations: (1) modality fragmentation, which leads to poor generalization across diverse and adversarial deepfake modalities; and (2) shallow inter-modal reasoning, resulting in limited detection of fine-grained semantic inconsistencies. To address these, we propose ConLLM (Contrastive Learning with Large Language Models), a hybrid framework for robust multimodal deepfake detection. ConLLM employs a two-stage architecture: stage 1 uses Pre-Trained Models (PTMs) to extract modality-specific embeddings; stage 2 aligns these embeddings via contrastive learning to mitigate modality fragmentation, and refines them using LLM-based reasoning to address shallow inter-modal reasoning by capturing semantic inconsistencies. ConLLM demonstrates strong performance across audio, video, and audio-visual modalities. It reduces audio deepfake EER by up to 50%, improves video accuracy by up to 8%, and achieves approximately 9% accuracy gains in audio-visual tasks. Ablation studies confirm that PTM-based embeddings contribute 9%-10% consistent improvements across modalities.

</details>


### [192] [Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection](https://arxiv.org/abs/2601.17532)
*Zhipeng Song,Yizhi Zhou,Xiangyu Kong,Jiulong Jiao,Xinrui Bao,Xu You,Xueqing Shi,Yuhang Zhou,Heng Qi*

Main category: cs.CL

TL;DR: 本文提出了一种新的RAG检索文段筛选方法信息增益剪枝(IGP)，能显著提升问答质量并降低输入负担。


<details>
  <summary>Details</summary>
Motivation: 在有限上下文预算下，如何选择最有效的检索文段用于支持生成模型仍是核心难题。传统检索相关性指标（如NDCG）在多文段注入时与问答质量关联弱甚至负相关，导致冗余和冲突影响生成。

Method: 作者提出IGP（信息增益剪枝），其主要思想是以生成器效用信号为依据，重排序并过滤检索文档，去除无益或有害文段后再注入生成模型。该方法无需改变现有的上下文预算接口，易于部署。

Result: 在五个开放域问答数据集、不同检索器和生成器设置下，IGP方法稳定提升了质量-成本权衡效果。在要求注入多条证据的典型场景下，平均F1相对提升12-20%，输入token数量却减少76-79%。

Conclusion: IGP可无缝集成于现有RAG系统，有效提升问答生成质量并减少计算开销，为文段选择提供更优方案。

Abstract: Retrieval-augmented generation (RAG) grounds large language models with external evidence, but under a limited context budget, the key challenge is deciding which retrieved passages should be injected. We show that retrieval relevance metrics (e.g., NDCG) correlate weakly with end-to-end QA quality and can even become negatively correlated under multi-passage injection, where redundancy and mild conflicts destabilize generation. We propose \textbf{Information Gain Pruning (IGP)}, a deployment-friendly reranking-and-pruning module that selects evidence using a generator-aligned utility signal and filters weak or harmful passages before truncation, without changing existing budget interfaces. Across five open-domain QA benchmarks and multiple retrievers and generators, IGP consistently improves the quality--cost trade-off. In a representative multi-evidence setting, IGP delivers about +12--20% relative improvement in average F1 while reducing final-stage input tokens by roughly 76--79% compared to retriever-only baselines.

</details>


### [193] [Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations](https://arxiv.org/abs/2601.17569)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 该论文提出了P^3框架，通过在本地客户端模型和云端大模型间协作，实现高质量的个性化生成，同时保护用户隐私。实验表明P^3在保证隐私的前提下，实现了优于现有方法的个性化效果。


<details>
  <summary>Details</summary>
Motivation: 个性化对于LLM输出至关重要，现有基于检索增强的方法在隐私保护和模型性能间存在权衡：要么泄露用户隐私给云端，要么依赖能力较弱的本地模型。因此需要兼顾隐私和效果的新方法。

Method: P^3框架中，云端大模型仅根据用户查询生成初步draft，客户端小模型结合用户隐私信息评估并修正draft，每轮生成后重复该过程直到完成。这样数据不泄露到云端，同时保持回答质量。

Result: 在三个个性化问答数据集上，P^3成绩显著高于非个性化云端和个性化本地基线，平均提升7.4%-9%，且接近于全部隐私泄露上限的90.3%-95.7%效用。隐私分析表明只有1.5%-3.5%的额外数据泄露，且客户端生成负担仅为9.2%。

Conclusion: P^3在不显著增加隐私泄露的前提下，实现了高效、高质量的个性化生成，是实用可靠的个性化隐私保护生成解决方案。

Abstract: Personalization is crucial for aligning Large Language Model (LLM) outputs with individual user preferences and background knowledge. State-of-the-art solutions are based on retrieval augmentation, where relevant context from a user profile is retrieved for LLM consumption. These methods deal with a trade-off between exposing retrieved private data to cloud providers and relying on less capable local models. We introduce $P^3$, an interactive framework for high-quality personalization without revealing private profiles to server-side LLMs. In $P^3$, a large server-side model generates a sequence of $k$ draft tokens based solely on the user query, while a small client-side model, with retrieval access to the user's private profile, evaluates and modifies these drafts to better reflect user preferences. This process repeats until an end token is generated. Experiments on LaMP-QA, a recent benchmark consisting of three personalized question answering datasets, show that $P^3$ consistently outperforms both non-personalized server-side and personalized client-side baselines, achieving statistically significant improvements of $7.4%$ to $9%$ on average. Importantly, $P^3$ recovers $90.3%$ to $95.7%$ of the utility of a ``leaky'' upper-bound scenario in which the full profile is exposed to the large server-side model. Privacy analyses, including linkability and attribute inference attacks, indicate that $P^3$ preserves the privacy of a non-personalized server-side model, introducing only marginal additional leakage ($1.5%$--$3.5%$) compared to submitting a query without any personal context. Additionally, the framework is efficient for edge deployment, with the client-side model generating only $9.2%$ of the total tokens. These results demonstrate that $P^3$ provides a practical, effective solution for personalized generation with improved privacy.

</details>


### [194] [Sequence Repetition Enhances Token Embeddings and Improves Sequence Labeling with Decoder-only Language Models](https://arxiv.org/abs/2601.17585)
*Matija Luka Kukić,Marko Čuljak,David Dukić,Martin Tutek,Jan Šnajder*

Main category: cs.CL

TL;DR: 本文提出了一种通过序列重复（SR）方法，使解码器-only语言模型能够像编码器那样利用双向上下文，从而提升其在序列标注任务的表现。SR无需大幅修改模型结构，比去因果mask方法更简单有效。


<details>
  <summary>Details</summary>
Motivation: 目前语言模型多为解码器-only结构，训练时仅基于左至右的上下文，而序列标注任务需要每个token同时利用左右文信息，对双向性有较高需求。传统方法多用编码器-only模型（如BERT），但随着解码器-only模型的发展，研究者希望其能适配到更多任务。如何让解码器-only模型也能高效用于序列标注，成为关注点。

Method: 作者提出序列重复（SR）作为适配方法，通过简单地对输入序列进行多次重复，使得解码器-only模型能够捕获到双向上下文信息，并通过微调实验与去因果mask和encoder方法进行了对比。

Result: 实验表明SR方法能本质上提升解码器的双向性，token-level embedding质量明显提升，在序列标注任务上超过了传统的encoder和去mask的解码器。并且，多次重复并不会导致性能下降。此外，模型中间层的embedding在SR下同样表现优异，且计算更高效。

Conclusion: 序列重复（SR）方法缓解了解码器结构上的双向性限制，使解码器-only语言模型更高效、适应性更广，能更好应用于序列标注等token-level任务。

Abstract: Modern language models (LMs) are trained in an autoregressive manner, conditioned only on the prefix. In contrast, sequence labeling (SL) tasks assign labels to each individual input token, naturally benefiting from bidirectional context. This discrepancy has historically led SL to rely on inherently bidirectional encoder-only models. However, the rapid development of decoder-only models has raised the question of whether they can be adapted to SL. While causal mask removal has emerged as a viable technique for adapting decoder-only models to leverage the full context for SL, it requires considerable changes to the base model functionality. In this work, we explore sequence repetition (SR) as a less invasive alternative for enabling bidirectionality in decoder-only models. Through fine-tuning experiments, we show that SR inherently makes decoders bidirectional, improving the quality of token-level embeddings and surpassing encoders and unmasked decoders. Contrary to earlier claims, we find that increasing the number of repetitions does not degrade SL performance. Finally, we demonstrate that embeddings from intermediate layers are highly effective for SR, comparable to those from final layers, while being significantly more efficient to compute. Our findings underscore that SR alleviates the structural limitations of decoders, enabling more efficient and adaptable LMs and broadening their applicability to other token-level tasks.

</details>


### [195] [From Chains to DAGs: Probing the Graph Structure of Reasoning in LLMs](https://arxiv.org/abs/2601.17593)
*Tianjun Zhong,Linyang He,Nima Mesgarani*

Main category: cs.CL

TL;DR: 本论文提出了Reasoning DAG Probing框架，用于探测大语言模型隐藏状态中是否存在可线性访问的推理有向无环图（DAG）结构，并分析该结构在不同层级的分布情况。实验证明模型在中间层确实编码了DAG几何结构，并且可恢复性受到节点深度和模型规模的影响。


<details>
  <summary>Details</summary>
Motivation: 许多推理任务天然呈现图结构（DAG），而不仅仅是线性过程。现有方法主要关注线性推理链，对LLM是否内部反映出复杂的图状推理结构知之甚少，因此有必要明确探究这一结构在模型中的表现。

Method: 作者提出Reasoning DAG Probing方法，将每个推理节点转化为文本，并训练轻量级探针，利用模型隐藏态预测节点深度及节点对间距离。通过分析不同层的探针效果，并设置对照扰动实验，以区分模型是否真正编码了相关结构。

Result: 实验结果显示，LLM的中间层能够有效编码推理DAG的几何结构，这一能力随着节点深度增加和模型规模变化而不同。即使扰动文本表面属性后，模型依然能维持部分推理结构。

Conclusion: LLM内部不仅实现了序列化推理，其隐藏状态还具备可测量的图结构编码能力。这为深入理解LLM多步推理机制和设计更强推理能力的模型提供了理论依据。

Abstract: Recent progress in large language models has renewed interest in mechanistically characterizing how multi-step reasoning is represented and computed. While much prior work treats reasoning as a linear chain of steps, many reasoning problems are more naturally structured as directed acyclic graphs (DAGs), where intermediate conclusions may depend on multiple premises, branch into parallel sub-derivations, and later merge or be reused. Understanding whether such graph-structured reasoning is reflected in model internals remains an open question.
  In this work, we introduce Reasoning DAG Probing, a framework that directly asks whether LLM hidden states encode the geometry of a reasoning DAG in a linearly accessible form, and where this structure emerges across layers. Within this framework, we associate each reasoning node with a textual realization and train lightweight probes to predict two graph-theoretic properties from hidden states: node depth and pairwise node distance. We use these probes to analyze the layerwise emergence of DAG structure and evaluate controls that disrupt reasoning-relevant structure while preserving superficial textual properties. Our results provide evidence that reasoning DAG geometry is meaningfully encoded in intermediate layers, with recoverability varying systematically by node depth and model scale, suggesting that LLM reasoning is not only sequential but exhibits measurable internal graph structure.

</details>


### [196] [Learning to Ideate for Machine Learning Engineering Agents](https://arxiv.org/abs/2601.17596)
*Yunxiang Zhang,Kang Zhou,Zhichao Xu,Kiran Ramnath,Yun Zhou,Sangmin Woo,Haibo Ding,Lin Lee Cheong*

Main category: cs.CL

TL;DR: 该论文提出了MLE-Ideator系统，通过将机器学习工程中的“想法生成”(Ideation)与“实现”(Implementation)过程分离，实现了更有效的算法优化。实验显示无论是否训练，MLE-Ideator均优于只实现的基线方法，且经过强化学习训练的“想法生成者”进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习工程体制难以实现对已实现算法的有效、迭代优化。作者发现将“创意产生”与“落实实现”分开能获得更好的优化效果，因此提出了专为“想法生成”服务的辅助代理，以提升整体系统能力。

Method: 该方法设计了MLE-Ideator框架，包括两个独立的代理角色：实现代理和想法生成代理（Ideator）。实现代理可在需要时向Ideator请求策略建议。实验过程中，比较了基础未训练与RL（强化学习）训练后的Ideator，并利用MLE-Bench任务集进行基准测评。

Result: 1）在无训练的状态下，MLE-Ideator框架在MLE-Bench数据集上显著优于仅实现代理的基准。2）采用RL训练后，Qwen3-8B模型的Ideator仅需1000条数据，从10个任务学到更优的策略，相较未训练的版本提升11.5%，并优于Claude Sonnet 3.5。

Conclusion: 将创意生成与实现分离能够有效提升机器学习工程优化流程，强化学习能进一步提升Ideator代理的效果。该工作为战略型AI系统在科学发现领域提供了新的研究路径。

Abstract: Existing machine learning engineering (MLE) agents struggle to iteratively optimize their implemented algorithms for effectiveness. To address this, we introduce MLE-Ideator, a dual-agent framework that separates ideation from implementation. In our system, an implementation agent can request strategic help from a dedicated Ideator. We show this approach is effective in two ways. First, in a training-free setup, our framework significantly outperforms implementation-only agent baselines on MLE-Bench. Second, we demonstrate that the Ideator can be trained with reinforcement learning (RL) to generate more effective ideas. With only 1K training samples from 10 MLE tasks, our RL-trained Qwen3-8B Ideator achieves an 11.5% relative improvement compared to its untrained counterpart and surpasses Claude Sonnet 3.5. These results highlights a promising path toward training strategic AI systems for scientific discovery.

</details>


### [197] [What Language Models Know But Don't Say: Non-Generative Prior Extraction for Generalization](https://arxiv.org/abs/2601.17609)
*Sara Rezaeimanesh,Mohammad M. Ghassemi*

Main category: cs.CL

TL;DR: 本文提出了一种新方法LoID，将大型语言模型（LLM）的知识通过概率提取融入贝叶斯逻辑回归的先验分布，显著提升在数据有限且分布偏移场景下的模型表现。


<details>
  <summary>Details</summary>
Motivation: 医学和金融等领域中获取大规模有标签数据非常昂贵、困难，导致模型泛化能力有限。现有LLM蕴含丰富领域知识，如何有效萃取并引入小数据建模过程以提升模型性能，是该工作的核心动机。

Method: 作者提出LoID方法，通过构造有代表性和对立语义（正向与负向）的句子，直接探查LLM对各特征影响方向的信心水平与一致性，据此为贝叶斯逻辑回归直接提取结构化的先验分布。不同于基于文本生成的提取方式，LoID利用LLM对Token级别预测的概率信息，从多样化表达下定量评估先验强度和置信度。

Result: 在十个现实世界的表格数据集上（模拟训练测试分布偏移），LoID在AUC指标上显著优于基线方法，包括传统无信息先验、AutoElicit及LLMProcesses，并能恢复高达59%的oracle-模型性能差距。LoID在8/10个数据集上取得最优，且计算高效、结果可复现。

Conclusion: LoID方法为整合LLM知识到贝叶斯统计建模提供了可控、通用且高效的工具，在数据稀缺和分布变化时提升逻辑回归模型的泛化表现，优于现有同类方法。

Abstract: In domains like medicine and finance, large-scale labeled data is costly and often unavailable, leading to models trained on small datasets that struggle to generalize to real-world populations. Large language models contain extensive knowledge from years of research across these domains. We propose LoID (Logit-Informed Distributions), a deterministic method for extracting informative prior distributions for Bayesian logistic regression by directly accessing their token-level predictions. Rather than relying on generated text, we probe the model's confidence in opposing semantic directions (positive vs. negative impact) through carefully constructed sentences. By measuring how consistently the LLM favors one direction across diverse phrasings, we extract the strength and reliability of the model's belief about each feature's influence. We evaluate LoID on ten real-world tabular datasets under synthetic out-of-distribution (OOD) settings characterized by covariate shift, where the training data represents only a subset of the population. We compare our approach against (1) standard uninformative priors, (2) AutoElicit, a recent method that prompts LLMs to generate priors via text completions, (3) LLMProcesses, a method that uses LLMs to generate numerical predictions through in-context learning and (4) an oracle-style upper bound derived from fitting logistic regression on the full dataset. We assess performance using Area Under the Curve (AUC). Across datasets, LoID significantly improves performance over logistic regression trained on OOD data, recovering up to \textbf{59\%} of the performance gap relative to the oracle model. LoID outperforms AutoElicit and LLMProcessesc on 8 out of 10 datasets, while providing a reproducible and computationally efficient mechanism for integrating LLM knowledge into Bayesian inference.

</details>


### [198] [Beyond the Rabbit Hole: Mapping the Relational Harms of QAnon Radicalization](https://arxiv.org/abs/2601.17658)
*Bich Ngoc,Doan,Giuseppe Russo,Gianmarco De Francisci Morales,Robert West*

Main category: cs.CL

TL;DR: 本文以QAnon阴谋论为案例，分析极端化过程及其对信仰者亲友的情感影响，基于12747条受害者叙述，通过主题模型和情感识别，发现不同极端化类型与具体情绪伤害密切相关。


<details>
  <summary>Details</summary>
Motivation: 阴谋论的传播除了造成社会信任下降、两极分化外，还对信仰者的亲友产生被忽视的个人层面伤害。现有计算型研究主要聚焦宏观社会影响，对亲友遭遇的细致情感冲击研究不足，本文试图填补这一空白。

Method: 作者以QAnon为例，抽取r/QAnonCasualties社区12747条叙事，首先用BERTopic进行主题建模，梳理极端化轨迹，包括前置条件、触发点和后期特征；再以LDA图模型分析，归纳出六类“极端化人格原型”；最后结合大语言模型（LLM）辅助情感检测与回归分析，将极端化类型与亲友叙事中的情感体验进行量化关联。

Result: 六类极端化人格原型与亲友具体情感困扰直接相关。例如，当极端化被亲友视为有意识选择时，易引发愤怒和厌恶；若表现为精神崩溃或认知衰退，则关联恐惧和悲伤。不同极端化路径对亲友的情感伤害各异，原型具预测性。

Conclusion: 首次以实证方式构建极端化的关系性研究框架，为理解其对亲情、友情关系的破坏性后果提供指引，对后续相关研究及实践干预有重要参考价值。

Abstract: The rise of conspiracy theories has created far-reaching societal harm in the public discourse by eroding trust and fueling polarization. Beyond this public impact lies a deeply personal toll on the friends and families of conspiracy believers, a dimension often overlooked in large-scale computational research. This study fills this gap by systematically mapping radicalization journeys and quantifying the associated emotional toll inflicted on loved ones. We use the prominent case of QAnon as a case study, analyzing 12747 narratives from the r/QAnonCasualties support community through a novel mixed-methods approach. First, we use topic modeling (BERTopic) to map the radicalization trajectories, identifying key pre-existing conditions, triggers, and post-radicalization characteristics. From this, we apply an LDA-based graphical model to uncover six recurring archetypes of QAnon adherents, which we term "radicalization personas." Finally, using LLM-assisted emotion detection and regression modeling, we link these personas to the specific emotional toll reported by narrators. Our findings reveal that these personas are not just descriptive; they are powerful predictors of the specific emotional harms experienced by narrators. Radicalization perceived as a deliberate ideological choice is associated with narrator anger and disgust, while those marked by personal and cognitive collapse are linked to fear and sadness. This work provides the first empirical framework for understanding radicalization as a relational phenomenon, offering a vital roadmap for researchers and practitioners to navigate its interpersonal fallout.

</details>


### [199] [UrduLM: A Resource-Efficient Monolingual Urdu Language Model](https://arxiv.org/abs/2601.17664)
*Syed Muhammad Ali,Hammad Sajid,Zainab Haider,Ali Muhammad Asad,Haya Fatima,Abdul Samad*

Main category: cs.CL

TL;DR: 本文提出了第一个专为乌尔都语设计的单语Transformer预训练模型UrduLM，并制作了33GB的高质量语料，填补了相关资源的空白。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语有2.3亿使用者，但缺乏专门的Transformer语言模型和语料资源。多语言模型对乌尔都语支持有限，表现较差、计算成本高且存在文化偏差。为解决这些问题，需要开发面向乌尔都语的专属语言模型和语料。

Method: 作者构建了一个包含33GB多样化语料的乌尔都语语料库，开发了自定义BPE分词器（比多语分词器效率提升20%-30%），并以此在低资源环境下预训练了一个1亿参数、仅解码结构的单语模型UrduLM。

Result: 在小样本评测中，UrduLM在情感分类任务上达到66.6%的准确率，语法纠错任务 BLEU 分数超过30，性能与规模大30倍的多语模型相当甚至更优。

Conclusion: UrduLM首次为乌尔都语NLP提供了专用模型和全套方法学资源，可作为基线推动乌尔都语及其他低资源语言NLP的发展，相关资源已公开发布。

Abstract: Urdu, spoken by 230 million people worldwide, lacks dedicated transformer-based language models and curated corpora. While multilingual models provide limited Urdu support, they suffer from poor performance, high computational costs, and cultural inaccuracies due to insufficient training data. To address these challenges, we present UrduLM, a pretrained Urdu monolingual language model trained in low-resource settings. We curate a 33GB Urdu corpus from diverse sources, develop a custom BPE tokenizer that reduces tokenization overhead by atleast 20-30% compared to multilingual alternatives, and pretrain a 100M-parameter decoder-only model. In few-shot evaluations, UrduLM achieves competitive performance with multilingual models up to 30x its size, reaching 66.6% accuracy on sentiment classification and BLEU scores exceeding 30 on grammar correction tasks. The complete methodology -- including corpus, tokenizer, model weights, and evaluation benchmarks -- is released openly to establish a baseline for Urdu NLP research and provide a scalable framework for other underrepresented languages.

</details>


### [200] [Align to the Pivot: Dual Alignment with Self-Feedback for Multilingual Math Reasoning](https://arxiv.org/abs/2601.17671)
*Chunxu Zhao,Xin Huang,Xue Han,Shujian Huang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 论文提出了一种新的多语言数学推理方法 PASMR，通过主语言作为枢纽，提升大模型在多语言环境下（尤其是低资源语言）的推理表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多语言能力上存在局限，特别是低资源语言情境下推理表现下降。作者认为原因在于模型多语言理解和推理对齐不一致。

Method: 提出 PASMR 方法，将模型的主要语言设为 pivot language。训练时，先将问题翻译为 pivot language，用主语言回答引导目标语言的推理过程，构建跨语言自反馈机制，无需依赖外部正确答案或奖励模型。

Result: 大量实验表明，该方法有效提升了模型对多语言问题的理解和推理能力，任务效果有显著提升。

Conclusion: PASMR 能有效改善 LLM 在多语言（特别是低资源语言）下的推理能力，对提升大模型多语言推理表现有实际意义。

Abstract: Despite the impressive reasoning abilities demonstrated by large language models (LLMs), empirical evidence indicates that they are not language agnostic as expected, leading to performance declines in multilingual settings, especially for low-resource languages. We attribute the decline to the model's inconsistent multilingual understanding and reasoning alignment. To address this, we present Pivot-Aligned Self-Feedback Multilingual Reasoning (PASMR), aiming to improve the alignment of multilingual math reasoning abilities in LLMs. This approach designates the model's primary language as the pivot language. During training, the model first translates questions into the pivot language to facilitate better alignment of reasoning patterns. The reasoning process in the target language is then supervised by the pivot language's reasoning answers, thereby establishing a cross-lingual self-feedback mechanism without relying on external correct answers or reward models. Extensive experimental results demonstrate that our method enhances both the model's understanding of questions and its reasoning capabilities, leading to notable task improvements.

</details>


### [201] [S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference](https://arxiv.org/abs/2601.17702)
*Qingsen Ma,Dianyun Wang,Yaoye Wang,Lechen Ning,Sujie Zhu,Xiaohang Zhang,Jiaming Lyu,Linhao Ren,Zhenbo Xu,Zhaofeng He*

Main category: cs.CL

TL;DR: S3-Attention是一种创新的长文本推理机制，能提高内存效率并减少噪音，替代传统KV缓存，具备高检索准确度和紧凑证据检索。


<details>
  <summary>Details</summary>
Motivation: 当前语言大模型在多文档和长文本推理中受到内存消耗大和无关噪声干扰的问题，常用的KV缓存在长上下文下效率低，外部检索也常返回无关内容。因此，需创新方法解决这些痛点。

Method: S3-Attention将长文本推理视为内生检索任务，用稀疏自动编码器将Key/Query投影为特征标识，再通过CPU倒排索引建立特征与token位置映射，实现KV缓存完全丢弃，GPU内存仅由扫描块大小限制，在生成时融合特征共现与BM25方法高效回溯证据。

Result: 在LongBench统一评测下，S3-Hybrid方法几乎与全上下文推理性能持平，在多种模型和信息密集场景下更具鲁棒性，但原型实现的延迟高于优化的全KV方法。

Conclusion: S3-Attention为长上下文推理提供高效可拓展方案，显著节省内存并提高相关证据检索，具备实际应用潜力。未来优化方向是内核级加速以降低系统延迟。

Abstract: Large language models are increasingly applied to multi-document and long-form inputs, yet long-context inference remains memory- and noise-inefficient. Key-value (KV) caching scales linearly with context length, while external retrieval methods often return lexically similar but causally irrelevant passages.
  We present S3-Attention, a memory-first inference-time framework that treats long-context processing as attention-aligned endogenous retrieval. S3-Attention decodes transient key and query projections into top-k sparse feature identifiers using lightweight sparse autoencoders, and constructs a CPU-based inverted index mapping features to token positions or spans during a single streaming scan. This design allows the KV cache to be discarded entirely and bounds GPU memory usage by the scan chunk size.
  At generation time, feature co-activation is used to retrieve compact evidence spans, optionally fused with BM25 for exact lexical matching. Under a unified LongBench evaluation protocol with fixed prompting, decoding, and matched token budgets, S3-Hybrid closely matches full-context inference across multiple model families and improves robustness in several information-dense settings. We also report an engineering limitation of the current prototype, which incurs higher wall-clock latency than optimized full-KV baselines, motivating future kernel-level optimization.

</details>


### [202] [Distance-to-Distance Ratio: A Similarity Measure for Sentences Based on Rate of Change in LLM Embeddings](https://arxiv.org/abs/2601.17705)
*Abdullah Qureshi,Kenneth Rice,Alexander Wolpert*

Main category: cs.CL

TL;DR: 本文提出了一种新的文本嵌入相似度衡量方法DDR，能更准确地区分语义相似与不相似的文本。


<details>
  <summary>Details</summary>
Motivation: 目前文本嵌入的相似度度量往往不能很好地反映人类对于文本语义相似性的感知。现有方法在微小语义变动面前区分能力有限，需要一种更贴合人类语义感知的新方法。

Method: 作者受Lipschitz连续性启发，提出距离-距离比（DDR）这一新指标。通过对句子做词语替换扰动（包括用同义词和随机词替换），分别考察变更前后嵌入的相似度变化，从而量化上下文对语义的影响，并与传统的相似度指标进行对比实验。

Result: 实验表明，在语句中即使是最小幅度的扰动（如换1个同义词或随机词），DDR指标相比其他主流嵌入相似度方法能更细致地区分语义相近与相远的文本。

Conclusion: DDR作为新的相似度度量方式，在语义微扰下能更贴近人类语义感知，为LLM嵌入下文本相似性测量工具提供了有价值的补充。

Abstract: A measure of similarity between text embeddings can be considered adequate only if it adheres to the human perception of similarity between texts. In this paper, we introduce the distance-to-distance ratio (DDR), a novel measure of similarity between LLM sentence embeddings. Inspired by Lipschitz continuity, DDR measures the rate of change in similarity between the pre-context word embeddings and the similarity between post-context LLM embeddings, thus measuring the semantic influence of context. We evaluate the performance of DDR in experiments designed as a series of perturbations applied to sentences drawn from a sentence dataset. For each sentence, we generate variants by replacing one, two, or three words with either synonyms, which constitute semantically similar text, or randomly chosen words, which constitute semantically dissimilar text. We compare the performance of DDR with other prevailing similarity metrics and demonstrate that DDR consistently provides finer discrimination between semantically similar and dissimilar texts, even under minimal, controlled edits.

</details>


### [203] [A Computational Approach to Visual Metonymy](https://arxiv.org/abs/2601.17706)
*Saptarshi Ghosh,Linfeng Liu,Tianyu Jiang*

Main category: cs.CL

TL;DR: 该论文首次系统性研究了视觉转喻（visual metonymy），即通过图像的间接视觉线索传达概念。作者构建了ViMET视觉转喻数据集，并用其测试了现有多模态大模型，结果显示模型在理解间接视觉表达方面存在较大不足。


<details>
  <summary>Details</summary>
Motivation: 虽然现实生活中人们常通过间接视觉线索理解意义（如工具代表职业），但当前尚无系统探索视觉转喻的计算方法及相关数据集，且主流多模态模型是否具备此类认知能力也不明确。

Method: 作者提出了一个基于符号学理论、结合大语言模型和文生图模型的管道，用于生成和识别转喻性视觉表达。基于该方法，他们构建了ViMET数据集，包括2000个多选题，用以评测多模态模型对视觉转喻的认知能力。

Result: 在人类与多模态大模型的对比实验中，人类表现（86.9%正确率）显著优于最先进的视觉语言模型（65.9%），显示现有模型对间接视觉线索的推理认知显著不足。

Conclusion: 本文首次提出视觉转喻的计算框架和评测数据集，揭示了当前多模态模型对间接视觉表达理解的缺陷，为未来提升模型认知能力提供了新的研究范式和测试基准。

Abstract: Images often communicate more than they literally depict: a set of tools can suggest an occupation and a cultural artifact can suggest a tradition. This kind of indirect visual reference, known as visual metonymy, invites viewers to recover a target concept via associated cues rather than explicit depiction. In this work, we present the first computational investigation of visual metonymy. We introduce a novel pipeline grounded in semiotic theory that leverages large language models and text-to-image models to generate metonymic visual representations. Using this framework, we construct ViMET, the first visual metonymy dataset comprising 2,000 multiple-choice questions to evaluate the cognitive reasoning abilities in multimodal language models. Experimental results on our dataset reveal a significant gap between human performance (86.9%) and state-of-the-art vision-language models (65.9%), highlighting limitations in machines' ability to interpret indirect visual references. Our dataset is publicly available at: https://github.com/cincynlp/ViMET.

</details>


### [204] [Unsupervised Elicitation of Moral Values from Language Models](https://arxiv.org/abs/2601.17728)
*Meysam Alizadeh,Fabrizio Gilardi,Zeynab Samei*

Main category: cs.CL

TL;DR: 本文通过免监督算法（ICM）挖掘预训练语言模型内在的道德推理能力，不依赖人工标注即可实现道德判断，大幅提高了准确性和减少了社会偏见。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统普及，确保其行为符合人类价值观极为重要，但目前的语言模型道德推理能力有限，且人工构建道德评价标注困难、主观，并容易带入偏见。因此，作者希望寻找无需人工监督的道德推理能力挖掘方法。

Method: 作者提出使用“内部一致性最大化（ICM）”算法，在无监督条件下挖掘预训练语言模型的内在道德判断能力，并通过多个公开基准数据集和多种模型进行实验。主要对比ICM方法与传统预训练、微调及chatbot模型在道德标签生成、不同道德理论泛化和社会偏见抑制等方面的表现。

Result: ICM方法在Norm Bank、ETHICS等权威道德推理基准上，显著优于全部预训练及chatbot模型；用ICM生成的道德标签微调模型效果可与甚至超过基于人工标签训练的模型。在正义、公理等道德理论框架下表现提升尤为突出。相比传统方法，ICM能有效减少社会偏见（如种族、经济地位、政治等）错误率一半以上。

Conclusion: 预训练语言模型潜藏有道德推理能力，可通过免监督算法（如ICM）有效激发。该方法不仅提升道德推理算法表现，还有助于提升AI公平性和减少主观偏见，是实现AI价值观对齐的可扩展路径。

Abstract: As AI systems become pervasive, grounding their behavior in human values is critical. Prior work suggests that language models (LMs) exhibit limited inherent moral reasoning, leading to calls for explicit moral teaching. However, constructing ground truth data for moral evaluation is difficult given plural frameworks and pervasive biases. We investigate unsupervised elicitation as an alternative, asking whether pretrained (base) LMs possess intrinsic moral reasoning capability that can be surfaced without human supervision. Using the Internal Coherence Maximization (ICM) algorithm across three benchmark datasets and four LMs, we test whether ICM can reliably label moral judgments, generalize across moral frameworks, and mitigate social bias. Results show that ICM outperforms all pre-trained and chatbot baselines on the Norm Bank and ETHICS benchmarks, while fine-tuning on ICM labels performs on par with or surpasses those of human labels. Across theoretically motivated moral frameworks, ICM yields its largest relative gains on Justice and Commonsense morality. Furthermore, although chatbot LMs exhibit social bias failure rates comparable to their pretrained ones, ICM reduces such errors by more than half, with the largest improvements in race, socioeconomic status, and politics. These findings suggest that pretrained LMs possess latent moral reasoning capacities that can be elicited through unsupervised methods like ICM, providing a scalable path for AI alignment.

</details>


### [205] [Hylog: A Hybrid Approach to Logging Text Production in Non-alphabetic Scripts](https://arxiv.org/abs/2601.17753)
*Roberto Crotti,Giovanni Denaro,Zhiqiang Du,Ricardo Muñoz Martín*

Main category: cs.CL

TL;DR: 本论文提出了Hylog系统，一种结合按键记录与生态文本记录的混合日志工具，专为输入法在非拉丁文字中的研究设计。通过插件形式支持常用应用，实现了按键与屏幕渲染文本的同步追踪，填补了以往按键记录遗漏输入法转换细节的空白。实证研究证明系统能获取传统工具不可见的数据，为输入法下的文本输入认知研究提供新方法。


<details>
  <summary>Details</summary>
Motivation: 当前研究按键记录器无法捕获输入法（IME）对非字母文字在屏幕上的转换过程，导致对多语言文本输入过程的认知研究存在方法学缺陷。作者希望填补该空白，提升文本生产领域的研究能力。

Method: 作者开发了Hylog系统，通过插件接入Microsoft Word和Google Chrome等主流应用，分别采集按键输出与屏幕渲染文本，并利用混合模块同步生成双重活动轨迹。通过一项简化汉字翻译实证研究，系统记录了拉丁字母、汉字与输入法确认的按键行为及时序细节。

Result: Hylog系统能够成功捕获传统按键记录器无法获取的IME相关操作和时序数据，展示了技术可行性和分析新能力，为分析输入法环境下的多层次认知过程提供了数据基础。

Conclusion: Hylog系统弥补了输入法研究的技术短板，其可扩展插件架构适合更多IME与应用场景，有助于推动更具包容性的多语言、跨平台文本输入认知研究。

Abstract: Research keyloggers are essential for cognitive studies of text production, yet most fail to capture the on-screen transformations performed by Input Method Editors (IMEs) for non-alphabetic scripts. To address this methodological gap, we present Hylog, a novel hybrid logging system that combines analytical keylogging with ecological text logging for a more complete and finer-grained analysis. Our modular, open-source system uses plug-ins for standard applications (Microsoft Word, Google Chrome) to capture both keyboard output and rendered text, which a hybridizer module then synchronizes into a dual trace. To validate the system's technical feasibility and demonstrate its analytical capabilities, we conducted a proof-of-concept study where two volunteers translated a text into simplified Chinese. Hylog successfully captured keypresses and temporal intervals between Latin letters, Chinese characters, and IME confirmations -- some measurements invisible to traditional keyloggers. The resulting data enable the formulation of new, testable hypotheses about the cognitive restrictions and affordances at different linguistic layers in IME-mediated typing. Our plug-in architecture enables extension to other IME systems and fosters more inclusive multilingual text-production research.

</details>


### [206] [ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation](https://arxiv.org/abs/2601.17755)
*Jinyoung Park,Sanghyeok Lee,Omar Zia Khan,Hyunwoo J. Kim,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: 本文提出了一种新型进展感知的基于图的检索与推理框架ProGraph-R1，通过结构感知的超图检索机制和基于推理进展的奖励优化，有效提升多跳推理的准确性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的RAG强化学习框架主要依赖语义相似度忽略了图结构，并且依赖稀疏的结果级奖励，无法有效刻画中间检索步骤的质量和依赖性，影响多跳推理的表现。

Method: ProGraph-R1包含两个核心创新：（1）结构感知超图检索机制，综合考虑语义相关性和图节点间的连通性，促进多跳推理路径的连贯性与深度推理；（2）基于推理进展的逐步奖励优化，为每一步推理分配密集的学习信号，注重中间推理步骤的质量进展，而非仅关注最终输出结果。

Result: 在多跳问答基准测试上，ProGraph-R1优于现有的GraphRAG方法，无论是在推理准确率还是文本生成质量方面都有显著提升。

Conclusion: ProGraph-R1能够更好地结合图结构信息和推理过程进展，实现更高水平的复杂推理任务，解决了以往基于语义和稀疏奖励的局限性。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-chunk retrieval. Recent works have employed reinforcement learning (RL) to train agentic GraphRAG frameworks that perform iterative interactions between LLMs and knowledge graphs. However, existing RL-based frameworks such as Graph-R1 suffer from two key limitations: (1) they primarily depend on semantic similarity for retrieval, often overlooking the underlying graph structure, and (2) they rely on sparse, outcome-level rewards, failing to capture the quality of intermediate retrieval steps and their dependencies. To address these limitations, we propose ProGraph-R1, a progress-aware agentic framework for graph-based retrieval and multi-step reasoning. ProGraph-R1 introduces a structure-aware hypergraph retrieval mechanism that jointly considers semantic relevance and graph connectivity, encouraging coherent traversal along multi-hop reasoning paths. We also design a progress-based step-wise policy optimization, which provides dense learning signals by modulating advantages according to intermediate reasoning progress within a graph, rather than relying solely on final outcomes. Experiments on multi-hop question answering benchmarks demonstrate that ProGraph-R1 consistently improves reasoning accuracy and generation quality over existing GraphRAG methods.

</details>


### [207] [Cross-Lingual Probing and Community-Grounded Analysis of Gender Bias in Low-Resource Bengali](https://arxiv.org/abs/2601.17764)
*Md Asgor Hossain Reaj,Rajan Das Gupta,Jui Saha Pritha,Abdullah Al Noman,Abir Ahmed,Golam Md Mohiuddin,Tze Hui Liew*

Main category: cs.CL

TL;DR: 本研究分析了大型语言模型（LLMs）在孟加拉语中表现出的性别偏见，发现现有基于英语的方法难以直接应用，强调了开发本地化和文化敏感的偏见检测工具的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多个领域取得了巨大成功，但其在非英语语言，特别是如孟加拉语这样的全球南方语言中，仍存在性别偏见问题，而此前相关研究主要聚焦于英语。这激发了本文针对孟加拉语中性别偏见的系统性研究。

Method: 研究采用了多种方法检测性别偏见，包括词典挖掘、计算分类模型、基于翻译的对比分析及基于GPT的偏见生成，并且还在农村和低收入地区进行了实地调查，以获取真实世界中的性别偏见数据。

Result: 结果表明，直接将英语为中心的偏见检测方法用于孟加拉语时，受限于语言结构和文化差异，效果有限。此外，实地调查揭示了孟加拉语性别偏见表现出与英语不同的特征，许多自动化系统难以识别的偏见被社区调研方法所发现。

Conclusion: 应针对孟加拉语等弱势语言开发本地化且贴合文化的偏见检测工具，并结合社区驱动的方法识别被自动系统忽视的偏见。该研究为将来在孟加拉语和其他印度次大陆语言中减少性别偏见、推动更公平包容的NLP系统打下基础。

Abstract: Large Language Models (LLMs) have achieved significant success in recent years; yet, issues of intrinsic gender bias persist, especially in non-English languages. Although current research mostly emphasizes English, the linguistic and cultural biases inherent in Global South languages, like Bengali, are little examined. This research seeks to examine the characteristics and magnitude of gender bias in Bengali, evaluating the efficacy of current approaches in identifying and alleviating bias. We use several methods to extract gender-biased utterances, including lexicon-based mining, computational classification models, translation-based comparison analysis, and GPT-based bias creation. Our research indicates that the straight application of English-centric bias detection frameworks to Bengali is severely constrained by language disparities and socio-cultural factors that impact implicit biases. To tackle these difficulties, we executed two field investigations inside rural and low-income areas, gathering authentic insights on gender bias. The findings demonstrate that gender bias in Bengali presents distinct characteristics relative to English, requiring a more localized and context-sensitive methodology. Additionally, our research emphasizes the need of integrating community-driven research approaches to identify culturally relevant biases often neglected by automated systems. Our research enhances the ongoing discussion around gender bias in AI by illustrating the need to create linguistic tools specifically designed for underrepresented languages. This study establishes a foundation for further investigations into bias reduction in Bengali and other Indic languages, promoting the development of more inclusive and fair NLP systems.

</details>


### [208] [DPI: Exploiting Parameter Heterogeneity for Interference-Free Fine-Tuning](https://arxiv.org/abs/2601.17777)
*Xiaoyu Liu,Xiaoyu Guan,Di Liang,Xianjie Wu*

Main category: cs.CL

TL;DR: 本文提出了一种针对大语言模型（LLM）在多任务监督微调（SFT）期间降低任务干扰和“跷跷板效应”的参数隔离方法。通过识别和冻结任务相关的核心参数，实现各任务间的冲突减少和性能提升。


<details>
  <summary>Details</summary>
Motivation: 不同下游任务的监督微调可能产生目标冲突，即优化一个任务时会损害其他任务的表现，尤其表现在参数被无差别更新时。作者试图从参数空间异质性角度解决此干扰问题。

Method: 作者首先分别对不同SFT任务独立微调模型，依据参数更新幅度识别各任务的“核心参数区”。高度重叠的任务被合并联合训练，基本无重叠的任务被分阶段处理。多阶段微调中，先前任务的核心参数被冻结，防止后续任务覆盖。

Result: 在多个公开数据集上的实验显示，本文动态参数隔离策略显著减少了任务间冲突，并且性能优于传统的多阶段及多任务微调基线方法。

Conclusion: 动态参数隔离是一种有效缓解监督微调时多个异质任务“跷跷板效应”的方法，能够促进LLM在多任务场景下的一致性表现提升。

Abstract: Supervised fine-tuning (SFT) is a crucial step for adapting large language models (LLMs) to downstream tasks. However, conflicting objectives across heterogeneous SFT tasks often induce the "seesaw effect": optimizing for one task may degrade performance on others, particularly when model parameters are updated indiscriminately. In this paper, we propose a principled approach to disentangle and isolate task-specific parameter regions, motivated by the hypothesis that parameter heterogeneity underlies cross-task interference. Specifically, we first independently fine-tune LLMs on diverse SFT tasks and identify each task's core parameter region as the subset of parameters exhibiting the largest updates. Tasks with highly overlapping core parameter regions are merged for joint training, while disjoint tasks are organized into different stages. During multi-stage SFT, core parameters acquired in prior tasks are frozen, thereby preventing overwriting by subsequent tasks. To verify the effectiveness of our method, we conducted intensive experiments on multiple public datasets. The results showed that our dynamic parameter isolation strategy consistently reduced data conflicts and achieved consistent performance improvements compared to multi-stage and multi-task tuning baselines.

</details>


### [209] [Controlling Reading Ease with Gaze-Guided Text Generation](https://arxiv.org/abs/2601.17781)
*Andreas Säuberli,Darja Jepifanova,Diego Frassinelli,Barbara Plank*

Main category: cs.CL

TL;DR: 本研究通过利用预测人类注视模式的模型，引导语言模型生成易读或难读的文本，实验结果显示方法有效控制文本可读性。


<details>
  <summary>Details</summary>
Motivation: 目前针对文本可读性的生成控制方法有限，而眼动行为能反映文本处理的认知负荷，因此作者尝试用眼动数据指导文本生成，使文本可控地更易或更难读。

Method: 作者提出一种结合眼动预测模型与语言模型的方法，利用眼动预测结果作为反馈，控制生成文本的可读性。方法通过实验让英语母语者和非母语者实际阅读生成文本，并用眼动仪采集阅读时间等数据。

Result: 实验结果表明，方法能明显提高或降低生成文本的易读性，不论是从阅读时间还是主观难度评价。统计分析发现此变化主要归因于影响词汇处理的特征。

Conclusion: 该方法可应用于文本简化、提升信息获取，以及语言学习中个性化教学材料生成等场景，对提高信息可及性和学习效果具有实践意义。

Abstract: The way our eyes move while reading can tell us about the cognitive effort required to process the text. In the present study, we use this fact to generate texts with controllable reading ease. Our method employs a model that predicts human gaze patterns to steer language model outputs towards eliciting certain reading behaviors. We evaluate the approach in an eye-tracking experiment with native and non-native speakers of English. The results demonstrate that the method is effective at making the generated texts easier or harder to read, measured both in terms of reading times and perceived difficulty of the texts. A statistical analysis reveals that the changes in reading behavior are mostly due to features that affect lexical processing. Possible applications of our approach include text simplification for information accessibility and generation of personalized educational material for language learning.

</details>


### [210] [Beyond a Single Perspective: Text Anomaly Detection with Multi-View Language Representations](https://arxiv.org/abs/2601.17786)
*Yixin Liu,Kehan Yan,Shiyuan Li,Qingfeng Chen,Shirui Pan*

Main category: cs.CL

TL;DR: 本文提出了一种结合多种预训练语言模型嵌入的多视角文本异常检测框架MCA^2，通过多视角重建模型和自适应加权机制，有效提升对多种数据集的适应性和检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本异常检测方法多依赖单一嵌入模型，难以适应不同数据集和异常类型，检测效果受限。

Method: 作者提出MCA^2框架，利用多个预训练语言模型抽取不同的文本嵌入，通过多视角重建模型学习正常文本模式，并设计了对比协作模块来促进不同视角间的信息互补。同时，引入自适应分配模块，根据具体数据自动调整各视角的权重。

Result: 在10个基准数据集上与多种强基线方法对比，MCA^2表现出更好的检测效果，验证了方法的有效性。

Conclusion: 多视角、多嵌入、多模块协作以及自适应机制能够全面提升文本异常检测的适应性与鲁棒性，实验证明MCA^2具有优越的性能。

Abstract: Text anomaly detection (TAD) plays a critical role in various language-driven real-world applications, including harmful content moderation, phishing detection, and spam review filtering. While two-step "embedding-detector" TAD methods have shown state-of-the-art performance, their effectiveness is often limited by the use of a single embedding model and the lack of adaptability across diverse datasets and anomaly types. To address these limitations, we propose to exploit the embeddings from multiple pretrained language models and integrate them into $MCA^2$, a multi-view TAD framework. $MCA^2$ adopts a multi-view reconstruction model to effectively extract normal textual patterns from multiple embedding perspectives. To exploit inter-view complementarity, a contrastive collaboration module is designed to leverage and strengthen the interactions across different views. Moreover, an adaptive allocation module is developed to automatically assign the contribution weight of each view, thereby improving the adaptability to diverse datasets. Extensive experiments on 10 benchmark datasets verify the effectiveness of $MCA^2$ against strong baselines. The source code of $MCA^2$ is available at https://github.com/yankehan/MCA2.

</details>


### [211] [DIETA: A Decoder-only transformer-based model for Italian-English machine TrAnslation](https://arxiv.org/abs/2601.17823)
*Pranav Kasela,Marco Braga,Alessandro Ghiotto,Andrea Pilzer,Marco Viviani,Alessandro Raganato*

Main category: cs.CL

TL;DR: 该论文提出了DIETA——一个专为意英翻译设计的小型解码器Transformer模型，拥有5亿参数，性能优越，并开源了相关资源。


<details>
  <summary>Details</summary>
Motivation: 当前专注于意大利语-英语的高质量机器翻译资源有限，且针对该语言对的小模型表现有待提高，因此亟需开发专门优化的系统及相应数据集。

Method: 作者收集、整理了约2.07亿对意英平行语料，涵盖多种领域，并通过预训练模型进行回译生成3.52亿语料。模型为带有5亿参数的decoder-only Transformer，并新建了基于WikiNews的当代文本评测集。模型及代码均已开源。

Result: 在多个意英翻译基准上，DIETA的表现进入32系统排行榜的第二梯队，并在五个测试集中有四个超越了其它3B参数以下模型。

Conclusion: DIETA证明了小型、专门化Transformer在意英翻译任务中的有效性，并通过开源推进了该领域的进一步研究与应用。

Abstract: In this paper, we present DIETA, a small, decoder-only Transformer model with 0.5 billion parameters, specifically designed and trained for Italian-English machine translation. We collect and curate a large parallel corpus consisting of approximately 207 million Italian-English sentence pairs across diverse domains, including parliamentary proceedings, legal texts, web-crawled content, subtitles, news, literature and 352 million back-translated data using pretrained models. Additionally, we create and release a new small-scale evaluation set, consisting of 450 sentences, based on 2025 WikiNews articles, enabling assessment of translation quality on contemporary text. Comprehensive evaluations show that DIETA achieves competitive performance on multiple Italian-English benchmarks, consistently ranking in the second quartile of a 32-system leaderboard and outperforming most other sub-3B models on four out of five test suites. The training script, trained models, curated corpus, and newly introduced evaluation set are made publicly available, facilitating further research and development in specialized Italian-English machine translation. https://github.com/pkasela/DIETA-Machine-Translation

</details>


### [212] [Linguistic and Argument Diversity in Synthetic Data for Function-Calling Agents](https://arxiv.org/abs/2601.17829)
*Dan Greenstein,Zohar Karnin,Chen Amiraz,Oren Somekh*

Main category: cs.CL

TL;DR: 本文提出了一种通过优化多样性指标自动生成合成数据集的方法，以提升函数调用智能体的数据多样性，实验显示其在多样性和泛化能力上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 函数调用智能体的能力扩展高度依赖于高质量多样性训练数据，而现有方法大多忽视了请求语言和参数的多样性覆盖，导致泛化能力受限。

Method: 作者提出利用优化通用多样性指标的方法，自动生成兼具请求和参数多样性的语料，而非依赖手工规则或特定领域分类体系，从而增强生成数据集的通用性和适应性。

Result: 实验在本体内测评和下游实际任务中都对比了主流数据生成方法。新方法在保持数据正确性的同时，提升了多样性。模型用新数据集训练后，在BFCL基准测试中超越了基线方法，准确率提升7.4%。

Conclusion: 自动优化多样性合成数据的方法提高了函数调用智能体的泛化能力，尤其在参数及请求语言多样性覆盖方面表现突出，为相关数据集构建提供了新思路。

Abstract: The construction of function calling agents has emerged as a promising avenue for extending model capabilities. A major challenge for this task is obtaining high quality diverse data for training. Prior work emphasizes diversity in functions, invocation patterns, and interaction turns, yet linguistic diversity of requests and coverage of arguments (e.g., \texttt{city\_name}, \texttt{stock\_ticker}) remain underexplored. We propose a method that generates synthetic datasets via optimizing general-purpose diversity metrics across both queries and arguments, without relying on hand-crafted rules or taxonomies, making it robust to different usecases. We demonstrate the effectiveness of our technique via both intrinsic and extrinsic testing, comparing it to SoTA data generation methods. We show a superiority over baselines in terms of diversity, while keeping comparable correctness. Additionally, when used as a training set, the model resulting from our dataset exhibits superior performance compared to analogous models based on the baseline data generation methods in out-of-distribution performance. In particular, we achieve an $7.4\%$ increase in accuracy on the BFCL benchmark compared to similar counterparts.

</details>


### [213] [EFT-CoT: A Multi-Agent Chain-of-Thought Framework for Emotion-Focused Therapy](https://arxiv.org/abs/2601.17842)
*Lanqing Du,Yunong Li,YuJie Long,Shihong Chen*

Main category: cs.CL

TL;DR: 本文提出了一种基于情绪聚焦疗法（EFT）的多智能体链式思维（CoT）大模型框架EFT-CoT，实现了更具同理心、更结构化的心理健康问答系统。该方法通过“体验感知-认知探索-叙事干预”三阶段推理，结合八个功能智能体，有效提升了心理干预的同理性和专业度。实验结果显示新模型显著优于传统CBT方法及人类答复。


<details>
  <summary>Details</summary>
Motivation: 现有基于认知行为疗法（CBT）的心理健康大模型多采用“自上而下”的推理，忽视了用户身体经验和原始情绪处理，导致干预效果有限。为更好模拟人与人之间有效共情和心理疏导，需要引入以情绪为核心、更具人性化的干预机制。

Method: 提出EFT-CoT框架，采用“自下而上”路径，将干预过程分为三阶段：体验感知、认知探索、叙事干预。系统引入八个专用智能体，分别执行体感觉察、适应性评估、核心信念提取和叙事重构等关键疗法步骤。此外，构建高质量EFT-Instruct数据集并蒸馏微调专用大模型。

Result: 实验结果表明，EFT-LLM在同理心深度、结构专业性等评估指标上，优于强基线模型及人类回答。消融实验进一步验证多智能体机制对模型性能提升的必要性。

Conclusion: EFT-CoT模型为可解释、高同理度的心理健康干预系统提供了有效路径，其多智能体底层推理方式提升了心理推理能力，弥补了传统CBT大模型的局限。

Abstract: Leveraging Large Language Models (LLMs) for Mental Health Question Answering (MHQA) is promising for mitigating resource shortages. However, existing Cognitive Behavioral Therapy (CBT)-based approaches predominantly favor a "top-down" rational restructuring, often neglecting clients' embodied experiences and primary emotion processing. To address this, we propose an Emotion-Focused Therapy (EFT)-based Multi-Agent Chain-of-Thought framework (EFT-CoT). Adopting a "bottom-up" trajectory, it deconstructs the intervention into a three-stage reasoning flow: "Embodied Perception - Cognitive Exploration - Narrative Intervention." Utilizing eight specialized agents, the system explicitly executes critical components such as somatic awareness mapping, adaptive assessment, core belief extraction, and narrative restructuring. We further constructed "EFT-Instruct," a high-quality dataset via Chain-of-Thought distillation of approximately 67,000 authentic texts, and fine-tuned a specialized model, EFT-LLM. Experimental evaluations demonstrate that EFT-LLM outperforms strong baselines and human responses across metrics like empathy depth and structural professionalism. Ablation studies confirm the necessity of the multi-agent mechanism. The model exhibits superior psychological reasoning, offering an effective pathway for interpretable, high-empathy counseling systems.

</details>


### [214] [D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior of Large Language Models](https://arxiv.org/abs/2601.17865)
*Jia Gu,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLM）预测下一个token的概率（P_token）与任务层面目标分布（P_task）的一致性，区分了两类模型（D-models和E-models），并探讨了在下游任务中的表现差异及其机制。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能模拟真实分布，但其每一步采样概率是否与具体任务需求一致，仍未明晰。论文旨在探讨LLM输出概率与实际任务目标分布对齐性的核心问题。

Method: 作者通过受控分布采样实验，系统分析了不同类型LLM预测token的概率波动与任务所需分布之间的关系；进一步在代码生成、推荐等下游任务上对比两类模型的效果；还分析了两种模型家族的内部机制。

Result: 实验发现，D-model（如Qwen-2.5）的采样概率波动大，与任务目标分布契合较差；E-model（如Mistral-Small）的采样概率更稳定，与任务目标分布更一致。下游任务验证了D/E模型在多样性与稳定性间存在权衡，影响实际效果。

Conclusion: D/E两类LLM在采样行为有本质差异，针对推荐、搜索、对话等场景，应结合实际需求（多样性vs.可靠性）选用合适模型。这为模型选择与配置提供了理论指导，也有助理解LLM的本质采样机制。

Abstract: The predictive probability of the next token (P_token) in large language models (LLMs) is inextricably linked to the probability of relevance for the next piece of information, the purchase probability of the next product, and the execution probability of the next action-all of which fall under the scope of the task-level target distribution (P_task). While LLMs are known to generate samples that approximate real-world distributions, whether their fine-grained sampling probabilities faithfully align with task requirements remains an open question. Through controlled distribution-sampling simulations, we uncover a striking dichotomy in LLM behavior, distinguishing two model types: D-models (e.g. Qwen-2.5), whose P_token exhibits large step-to-step variability and poor alignment with P_task; and E-models (e.g. Mistral-Small), whose P_token is more stable and better aligned with P_task. We further evaluate these two model types in downstream tasks such as code generation and recommendation, revealing systematic trade-offs between diversity and stability that shape task outcomes. Finally, we analyze the internal properties of both model families to probe their underlying mechanisms. These findings offer foundational insights into the probabilistic sampling behavior of LLMs and provide practical guidance on when to favor D- versus E-models. For web-scale applications, including recommendation, search, and conversational agents, our results inform model selection and configuration to balance diversity with reliability under real-world uncertainty, providing a better level of interpretation.

</details>


### [215] [On the Emergence and Test-Time Use of Structural Information in Large Language Models](https://arxiv.org/abs/2601.17869)
*Michelle Chao Chen,Moritz Miller,Bernhard Schölkopf,Siyuan Guo*

Main category: cs.CL

TL;DR: 本文探讨了语言模型如何从观测数据中学习抽象结构，并在测试时利用这些结构信息。通过设计实验性语言数据集，分析了结构学习与复杂推理任务之间的关联，并指出模型在测试时生成新组合内容的能力仍有限。


<details>
  <summary>Details</summary>
Motivation: 学习结构化信息对于科学发现中的机制理解以及在测试时生成灵活组合内容至关重要。然而，语言模型是如何学习和利用这些结构信息的，目前尚不清楚。作者希望揭示模型学习结构能力与推理任务之间的关系，以及在生成新内容时的限制。

Method: 作者构建了基于语言结构变化的实验数据集，在受控环境下对语言模型进行训练和测试。通过实验分析模型对结构信息的学习过程及其在测试时的应用能力。

Result: 实验表明，学习结构信息的能力与模型在复杂推理任务中的表现呈现相关性。但模型在测试时利用结构信息进行新组合生成的能力依然有限。

Conclusion: 尽管语言模型可以在一定程度上学习抽象结构，并在复杂推理任务中表现出相应能力，但其在灵活新颖生成方面尚有明显不足。

Abstract: Learning structural information from observational data is central to producing new knowledge outside the training corpus. This holds for mechanistic understanding in scientific discovery as well as flexible test-time compositional generation. We thus study how language models learn abstract structures and utilize the learnt structural information at test-time. To ensure a controlled setup, we design a natural language dataset based on linguistic structural transformations. We empirically show that the emergence of learning structural information correlates with complex reasoning tasks, and that the ability to perform test-time compositional generation remains limited.

</details>


### [216] [Self-Manager: Parallel Agent Loop for Long-form Deep Research](https://arxiv.org/abs/2601.17879)
*Yilong Xu,Zhi Zheng,Xiang Long,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: 当前多智能体在处理复杂、长周期任务时，受限于单一上下文窗口与顺序执行，导致效率和适应性不足。本文提出Self-Manager框架，实现并行、异步多线程智能体管理，显著提升了长周期任务的处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有智能体在执行长周期、复杂任务时，受限于线性上下文积累和信息损失，且单一上下文窗口和顺序执行会发生互相干扰和阻塞，难以扩展和灵活适应任务需求。

Method: 提出Self-Manager架构，允许主线程创建多个拥有独立上下文的子线程，通过线程控制块(TCB)进行迭代管理，实现智能体的并行、异步与灵活调度。

Result: 在DeepResearch Bench基准上，Self-Manager在所有评价指标上均优于现有单智能体顺序管理方法；并通过消融实验验证其设计的必要性和针对上下文容量、效率及泛化能力方面的优势。

Conclusion: Self-Manager方法有效破解了长周期复杂任务管理中的伸缩性与上下文瓶颈，提升了智能体工作的效率和适应性，对多智能体管理领域具有推广价值。

Abstract: Long-form deep research requires multi-faceted investigations over extended horizons to get a comprehensive report. When handling such complex tasks, existing agents manage context at the subtask level to overcome linear context accumulation and information loss. However, they still adhere to a single context window and sequential execution paradigm, which results in mutual interference and blocking behavior, restricting scalability and adaptability. To address this issue, this paper introduces Self-Manager, a parallel agent loop that enables asynchronous and concurrent execution. The main thread can create multiple subthreads, each with its own isolated context, and manage them iteratively through Thread Control Blocks, allowing for more focused and flexible parallel agent execution. To assess its effectiveness, we benchmark Self-Manager on DeepResearch Bench, where it consistently outperforms existing single-agent loop baselines across all metrics. Furthermore, we conduct extensive analytical experiments to demonstrate the necessity of Self-Manager's design choices, as well as its advantages in contextual capacity, efficiency, and generalization.

</details>


### [217] [Assessment of Generative Named Entity Recognition in the Era of Large Language Models](https://arxiv.org/abs/2601.17898)
*Qi Zhan,Yile Wang,Hui Huang*

Main category: cs.CL

TL;DR: 该论文系统评估了开源大语言模型（LLMs）在命名实体识别任务（NER）中的表现，发现通过高效微调和结构化输出，LLMs在NER上已具备与传统方法媲美或更优的能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，NER任务逐步从序列标注转向生成范式。文章意在探讨并系统性分析新范式下，开源LLMs在各类NER任务中的表现、局限性及泛化能力。

Method: 作者选取八种不同规模的开源LLM，在四个主流NER数据集上，对比传统与生成式NER模型表现，考察输出格式、模型记忆性、微调对模型通用能力的影响。实验采用参数高效微调和结构化输出格式。

Result: （1）配合高效微调和结构化输出，开源LLMs在NER任务上可与传统模型竞争，甚至优于如GPT-3等闭源模型；（2）LLMs的NER能力来自于其指令遵循与生成能力，而非记忆；（3）进行NER指令微调后，模型通用能力几乎不受影响，甚至在如DROP等数据集上有所提升。

Conclusion: 生成式NER方法配合大语言模型，已成为性能优异且用户友好的选择，有望成为NER领域中传统方法的有效补充，甚至替代方案。

Abstract: Named entity recognition (NER) is evolving from a sequence labeling task into a generative paradigm with the rise of large language models (LLMs). We conduct a systematic evaluation of open-source LLMs on both flat and nested NER tasks. We investigate several research questions including the performance gap between generative NER and traditional NER models, the impact of output formats, whether LLMs rely on memorization, and the preservation of general capabilities after fine-tuning. Through experiments across eight LLMs of varying scales and four standard NER datasets, we find that: (1) With parameter-efficient fine-tuning and structured formats like inline bracketed or XML, open-source LLMs achieve performance competitive with traditional encoder-based models and surpass closed-source LLMs like GPT-3; (2) The NER capability of LLMs stems from instruction-following and generative power, not mere memorization of entity-label pairs; and (3) Applying NER instruction tuning has minimal impact on general capabilities of LLMs, even improving performance on datasets like DROP due to enhanced entity understanding. These findings demonstrate that generative NER with LLMs is a promising, user-friendly alternative to traditional methods. We release the data and code at https://github.com/szu-tera/LLMs4NER.

</details>


### [218] [ShapLoRA: Allocation of Low-rank Adaption on Large Language Models via Shapley Value Inspired Importance Estimation](https://arxiv.org/abs/2601.17921)
*Yi Zhao,Qinghua Yao,Xinyuan song,Wei Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种更可解释的LoRA秩分配方法ShapLoRA，并在多项任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法使用统一秩，后续工作尝试改进秩分配以提升性能，但现有基于秩重要性的评估方法不可解释且不可靠。需要一种可解释且有效的秩分配方法。

Method: 提出了ShapLoRA框架，将解释性Shapley值的归因思想与敏感度分析结合，提出可解释的重要性度量Shapley敏感度，在单独验证集上评估，并采用分配-再训练流程确保公平比较。

Result: 在多个具挑战性的任务上进行了实验，结果显示ShapLoRA在参数规模可比的情况下，效果优于最新的基线方法。

Conclusion: ShapLoRA能更好地分配LoRA秩，提升PEFT性能并具备良好可解释性，为后续研究提供了新的方向。

Abstract: Low-rank adaption (LoRA) is a representative method in the field of parameter-efficient fine-tuning (PEFT), and is key to Democratizating the modern large language models (LLMs). The vanilla LoRA is implemented with uniform ranks, and the recent literature have found that properly allocating ranks on the LLM backbones results in performance boosts. However, the previous rank allocation methods have limitations since they rely on inexplanable and unreliable importance measures for the LoRA ranks. To address the above issues, we propose the ShapLoRA framework. Inspired by the explanable attribution measure Shapley Value, we combine the sensitivity-based measures with the idea of coalitions in the collaborative games among LoRA ranks, and propose a more explainable importance measure called Shapley sensitivity. In addition, we optimize the workflow of the existing works by: (a) calculating Shapley sensitivity on a separate validation set; (b) Setting up the allocating-retraining procedures for fair comparisons. We have conducted experiments on various challenging tasks, and the experimental results demonstrate that our ShapLoRA method can outperform the recent baselines with comparable tunable parameters.\footnote{Codes and fine-tuned models will be open-sourced to facilitate future research.

</details>


### [219] [A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models](https://arxiv.org/abs/2601.17952)
*Michail Mamalakis,Tiago Azevedo,Cristian Cosentino,Chiara D'Ercoli,Subati Abulikemu,Zhongtian Sun,Richard Bethlehem,Pietro Lio*

Main category: cs.CL

TL;DR: 该论文提出了一种结合特征归因和机制解释的统一解释框架，通过在LLM层级构建单语义嵌入空间，减少归因方法间的差异，从而为阿尔茨海默症等临床设置中LLM的可解释性提供稳定可靠的输入重要性分数。


<details>
  <summary>Details</summary>
Motivation: 传统的LLM可解释方法在临床诊断（如阿尔茨海默症进展预测）应用上面临解释不稳定等问题，因归因方法存在高度可变性，机制解释方法又缺乏和输入/输出的直接对齐且难以给出显式重要性分数。为提升诊断的可信度和安全性，需要新的统一解释方法。

Method: 提出一种框架，将归因（attributional）和机制（mechanistic）解释方法通过单语义特征提取（monosemantic feature extraction）结合起来。在LLM某一层级建立一个单语义嵌入空间，并对框架进行优化，明确减少不同归因方法间的解释差异，最终获得稳定的输入级别重要性分数。

Result: 该方法能够输出稳定的、输入层级的显著特征归因分数，通过分层解压的方式清晰地揭示模型在诊断阿尔茨海默症等认知障碍时关注的关键输入特征。

Conclusion: 通过该统一解释框架，显著提升了LLM在认知健康和神经退行性疾病诊断领域应用的可解释性、安全性与可信度。

Abstract: Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer's disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scores. We introduce a unified interpretability framework that integrates attributional and mechanistic perspectives through monosemantic feature extraction. By constructing a monosemantic embedding space at the level of an LLM layer and optimizing the framework to explicitly reduce inter-method variability, our approach produces stable input-level importance scores and highlights salient features via a decompressed representation of the layer of interest, advancing the safe and trustworthy application of LLMs in cognitive health and neurodegenerative disease.

</details>


### [220] [LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction](https://arxiv.org/abs/2601.17971)
*Junior Cedric Tonga,Chen Cecilia Liu,Iryna Gurevych,Fajri Koto*

Main category: cs.CL

TL;DR: 本文提出了利用大型语言模型（LLMs）生成文化常识知识图谱（CCKG）的方法，以提升模型在文化推理等任务中的表现。研究发现，LLMs对英语文化的知识表达较好，而对其他文化的表达存在差距。引入知识图谱可以提升相关NLP任务的效果，特别是在英语链条上表现更明显。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型中蕴含了丰富的文化常识，但这些信息通常是隐含且无结构化的，限制了解释性和实际应用价值。作者希望找到一种方法把LLMs中的文化常识显式化、结构化，提高其可用性。

Method: 作者提出了一种基于迭代式提示（prompt）的框架，利用LLMs作为文化知识库，有系统地抽取不同文化的实体、关系和实践，并整理成多步推理链（inferential chains），最终构建成文化常识知识图谱（CCKG）。该方法在五个国家的数据集上，通过人工评估文化相关性、正确性和路径连贯性来验证效果。

Result: 实验结果显示，CCKG在英语中表达更好，即使目标文化是非英语（如中文、印尼语、阿拉伯语），也依赖英文表达，暴露了现有LLMs文化知识编码的不均衡。此外，将CCKG应用于较小的LLMs上，能显著提升其在文化推理和故事生成方面的表现，尤其是基于英语链条时。

Conclusion: LLMs作为文化技术有巨大潜力，但在多语言、多文化知识编码方面仍有限制。通过结构化的文化知识链，不仅能更好地用于NLP任务，还能作为未来文化知识建模的实践基础。

Abstract: Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.

</details>


### [221] [SD-E$^2$: Semantic Exploration for Reasoning Under Token Budgets](https://arxiv.org/abs/2601.17982)
*Kshitij Mishra,Nils Lukas,Salem Lahlou*

Main category: cs.CL

TL;DR: 本文提出了一种针对小型语言模型（SLM）推理能力有限的问题，通过优化推理过程中的语义多样性，提高探索效率和推理表现。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型因为算力受限，难以进行复杂推理，主要原因是探索有效推理路径的计算代价过高。现有方法多专注于结果正确性，忽略了多样性探索的重要性。

Method: 作者设计了SD-E^2（Semantic Diversity-Exploration-Exploitation）强化学习框架，利用冻结的句子嵌入模型，对生成的推理路径在语义空间中的多样性赋予奖励。奖励函数考虑了语义上不同策略的覆盖率以及平均两两嵌入距离，联合正确率与效率做z-score归一多目标优化。

Result: 在GSM8K数据集上，新方法相较于基础模型Qwen2.5-3B-Instruct和强基线GRPO提升了+27.4、+5.2、+1.5个百分点，并且平均每题能发现9.8种不同的语义推理策略；MedMCQA和AIME基准上的表现也显著优于基线模型。

Conclusion: 对推理路径语义新颖性进行奖励，为受限算力的SLM带来了更高效的探索-利用训练信号，实现了推理效率提升。通过关注推理结构而非逐Token计算，为小模型推理能力提升提供了互补的新路径。

Abstract: Small language models (SLMs) struggle with complex reasoning because exploration is expensive under tight compute budgets. We introduce Semantic Diversity-Exploration-Exploitation (SD-E$^2$), a reinforcement learning framework that makes exploration explicit by optimizing semantic diversity in generated reasoning trajectories. Using a frozen sentence-embedding model, SD-E$^2$ assigns a diversity reward that captures (i) the coverage of semantically distinct solution strategies and (ii) their average pairwise dissimilarity in embedding space, rather than surface-form novelty. This diversity reward is combined with outcome correctness and solution efficiency in a z-score-normalized multi-objective objective that stabilizes training. On GSM8K, SD-E$^2$ surpasses the base Qwen2.5-3B-Instruct and strong GRPO baselines (GRPO-CFL and GRPO-CFEE) by +27.4, +5.2, and +1.5 percentage points, respectively, while discovering on average 9.8 semantically distinct strategies per question. We further improve MedMCQA to 49.64% versus 38.37% for the base model and show gains on the harder AIME benchmark (1983-2025), reaching 13.28% versus 6.74% for the base. These results indicate that rewarding semantic novelty yields a more compute-efficient exploration-exploitation signal for training reasoning-capable SLMs. By introducing cognitive adaptation-adjusting the reasoning process structure rather than per-token computation-SD-E$^2$ offers a complementary path to efficiency gains in resource-constrained models.

</details>


### [222] [AI-based approach to burnout identification from textual data](https://arxiv.org/abs/2601.17993)
*Marina Zavertiaeva,Petr Parshakov,Mikhail Usanin,Aleksei Smirnov,Sofia Paklina,Anastasiia Kibardina*

Main category: cs.CL

TL;DR: 本研究提出了一种基于AI和NLP的方法，利用改进的RuBERT模型，通过检测文本数据来识别职业倦怠，并在真实与合成数据集上进行训练和测试。


<details>
  <summary>Details</summary>
Motivation: 职业倦怠在高压工作环境中普遍存在，但其自动化检测较为困难，因此需要新的方法来有效识别和监测倦怠迹象，以便及时干预。

Method: 该方法以RuBERT模型为核心，先用于情感分析训练，然后利用ChatGPT生成的合成句子和俄语YouTube关于倦怠的视频评论进一步微调模型，以进行倦怠检测。模型可为输入文本分配倦怠概率，并适用于大规模文本数据处理。

Result: 模型能够有效地分析文本数据，识别与职业倦怠相关的语言信号，对输入文本给出倦怠概率评分，可在实际高压环境中监测员工状态。

Conclusion: AI与NLP结合的自动倦怠检测方法具备良好的实用性和准确性，可应用于实际工作环境，有助于早期发现和管理职业倦怠。

Abstract: This study introduces an AI-based methodology that utilizes natural language processing (NLP) to detect burnout from textual data. The approach relies on a RuBERT model originally trained for sentiment analysis and subsequently fine-tuned for burnout detection using two data sources: synthetic sentences generated with ChatGPT and user comments collected from Russian YouTube videos about burnout. The resulting model assigns a burnout probability to input texts and can be applied to process large volumes of written communication for monitoring burnout-related language signals in high-stress work environments.

</details>


### [223] [PEAR: Pairwise Evaluation for Automatic Relative Scoring in Machine Translation](https://arxiv.org/abs/2601.18006)
*Lorenzo Proietti,Roman Grundkiewicz,Matt Post*

Main category: cs.CL

TL;DR: 本文提出了一种新的无参考机器翻译评价指标PEAR，以成对比较的方式预测两个候选翻译间的质量差异，并在多个基准上超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译的质量估计，尤其是无参考的自动评价方法，存在评价信号冗余、指标模型庞大、难以捕捉细粒度差异的问题。作者希望通过成对比较方法，提升评价准确性，并简化模型结构。

Method: PEAR以源句和两个候选译文为输入，预测它们之间质量差异的方向和幅度。模型通过人类评价差异进行成对监督训练，加入正则项以保证换位时符号反转。作者基于相同的训练数据和骨干网络，将PEAR与单候选基线方法进行了严格对比。

Result: 在WMT24元评估基准上，PEAR超越了使用相同训练数据和参数量的单候选无参考基线方法，并优于参数量远超自身的大型模型和参考型指标。分析还显示PEAR提供了更具独立性的评价信号。

Conclusion: PEAR以更少参数达到更优的评分表现，评价信号更加有效且独特。其作为Bayes风险最小化解码的效用函数时，能以极低代价提供有效支持，有潜力成为无参考MT评估工具的新标准。

Abstract: We present PEAR (Pairwise Evaluation for Automatic Relative Scoring), a supervised Quality Estimation (QE) metric family that reframes reference-free Machine Translation (MT) evaluation as a graded pairwise comparison. Given a source segment and two candidate translations, PEAR predicts the direction and magnitude of their quality difference. The metrics are trained using pairwise supervision derived from differences in human judgments, with an additional regularization term that encourages sign inversion under candidate order reversal. On the WMT24 meta-evaluation benchmark, PEAR outperforms strictly matched single-candidate QE baselines trained with the same data and backbones, isolating the benefit of the proposed pairwise formulation. Despite using substantially fewer parameters than recent large metrics, PEAR surpasses far larger QE models and reference-based metrics. Our analysis further indicates that PEAR yields a less redundant evaluation signal relative to other top metrics. Finally, we show that PEAR is an effective utility function for Minimum Bayes Risk (MBR) decoding, reducing pairwise scoring cost at negligible impact.

</details>


### [224] [Evaluating Semantic and Syntactic Understanding in Large Language Models for Payroll Systems](https://arxiv.org/abs/2601.18012)
*Hendrika Maclean,Mert Can Cakmak,Muzakkiruddin Ahmed Mohammed,Shames Al Mandalawi,John Talburt*

Main category: cs.CL

TL;DR: 本文系统分析了大语言模型在高风险业务场景（如薪资系统）中的精确运算与可审计能力，通过不同数据、提示方式及模型实验，指出何时依赖模型推理、何时必须显式计算，并提出可复现的测试框架与实用建议。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在文字理解与生成上表现出色，但在精确数值计算和输出可追溯性方面仍存在局限，尤其是在对准确性和合规性要求高的业务场景（如薪资处理）中存在风险，因此需要系统性评估其在这些场景下表现。

Method: 设计并构建了一套涵盖简单到复杂薪资案例的数据集，设置了从最基础到引导式、推理式等不同类型的提示词，对GPT、Claude、Perplexity、Grok、Gemini等多种主流大模型逐项测试，检验其对薪资规则的理解、规则排序执行及小数点级别的精算能力。

Result: 实验表明，在部分场景下，仅通过精心设计提示词即可获得准确结果；但在更复杂计算任务中，必须结合显式的计算步骤，否则模型输出容易出错。此外，该文提出的测试框架能有效复现结果并量化各模型表现。

Conclusion: 大语言模型在高精度要求的业务场景中使用时，需谨慎评估其推理与计算能力。对于能被仔细提示控制的内容可直接用模型，但遇到复杂计算仍应采用明确的算法实现。论文框架为相关领域扩展应用提供了范式和实践建议。

Abstract: Large language models are now used daily for writing, search, and analysis, and their natural language understanding continues to improve. However, they remain unreliable on exact numerical calculation and on producing outputs that are straightforward to audit. We study synthetic payroll system as a focused, high-stakes example and evaluate whether models can understand a payroll schema, apply rules in the right order, and deliver cent-accurate results. Our experiments span a tiered dataset from basic to complex cases, a spectrum of prompts from minimal baselines to schema-guided and reasoning variants, and multiple model families including GPT, Claude, Perplexity, Grok and Gemini. Results indicate clear regimes where careful prompting is sufficient and regimes where explicit computation is required. The work offers a compact, reproducible framework and practical guidance for deploying LLMs in settings that demand both accuracy and assurance.

</details>


### [225] [A System for Name and Address Parsing with Large Language Models](https://arxiv.org/abs/2601.18014)
*Adeeba Tarannum,Muzakkiruddin Ahmed Mohammed,Mert Can Cakmak,Shames Al Mandalawi,John Talburt*

Main category: cs.CL

TL;DR: 该论文提出了一种基于提示词驱动和验证为中心的框架，实现了将自由文本格式的人员和地址信息高效、可解释地转化为17字段的结构化数据，在多语言或噪声环境下也能保持高准确性和可重复性，无需模型微调。


<details>
  <summary>Details</summary>
Motivation: 在大规模信息系统中，从非结构化的人名和地址文本高质量提取结构化数据一直存在挑战，现有基于规则及概率的方法在噪声数据和多语种环境下表现差，神经网络及大模型则缺乏确定性可控和可复现性。因此亟需一种兼具准确性、稳定性和可复现性的结构化信息抽取方法。

Method: 该方法融合了输入归一化、结构化提示词、约束解码及严格基于规则的验证，通过固定实验设置保障可重复性。框架无需针对数据集进行微调，可直接将自由文本转化为17字段的统一结构模式。

Result: 在实际异构地址数据集上评测，所提方法在字段级准确率、结构一致性、置信度校准等方面表现突出，且保持高稳定性。

Conclusion: 通过将确定性规则验证与生成式提示策略结合，该方法为结构化信息抽取提供了强健、可解释且可扩展的方案，是现有依赖大量训练的或强依赖领域模型的有效替代选择。

Abstract: Reliable transformation of unstructured person and address text into structured data remains a key challenge in large-scale information systems. Traditional rule-based and probabilistic approaches perform well on clean inputs but fail under noisy or multilingual conditions, while neural and large language models (LLMs) often lack deterministic control and reproducibility. This paper introduces a prompt-driven, validation-centered framework that converts free-text records into a consistent 17-field schema without fine-tuning. The method integrates input normalisation, structured prompting, constrained decoding, and strict rule-based validation under fixed experimental settings to ensure reproducibility. Evaluations on heterogeneous real-world address data show high field-level accuracy, strong schema adherence, and stable confidence calibration. The results demonstrate that combining deterministic validation with generative prompting provides a robust, interpretable, and scalable solution for structured information extraction, offering a practical alternative to training-heavy or domain-specific models.

</details>


### [226] [CommonLID: Re-evaluating State-of-the-Art Language Identification Performance on Web Data](https://arxiv.org/abs/2601.18026)
*Pedro Ortiz Suarez,Laurie Burchell,Catherine Arnett,Rafael Mosquera-Gómez,Sara Hincapie-Monsalve,Thom Vaughan,Damian Stewart,Malte Ostendorff,Idris Abdulmumin,Vukosi Marivate,Shamsuddeen Hassan Muhammad,Atnafu Lambebo Tonja,Hend Al-Khalifa,Nadia Ghezaiel Hammouda,Verrah Otiende,Tack Hwa Wong,Jakhongir Saydaliev,Melika Nobakhtian,Muhammad Ravi Shulthan Habibi,Chalamalasetti Kranti,Carol Muchemi,Khang Nguyen,Faisal Muhammad Adam,Luis Frentzen Salim,Reem Alqifari,Cynthia Amol,Joseph Marvin Imperial,Ilker Kesen,Ahmad Mustafid,Pavel Stepachev,Leshem Choshen,David Anugraha,Hamada Nayel,Seid Muhie Yimam,Vallerie Alexandra Putra,My Chiffon Nguyen,Azmine Toushik Wasi,Gouthami Vadithya,Rob van der Goot,Lanwenn ar C'horr,Karan Dua,Andrew Yates,Mithil Bangera,Yeshil Bangera,Hitesh Laxmichand Patel,Shu Okabe,Fenal Ashokbhai Ilasariya,Dmitry Gaynullin,Genta Indra Winata,Yiyuan Li,Juan Pablo Martínez,Amit Agarwal,Ikhlasul Akmal Hanif,Raia Abu Ahmad,Esther Adenuga,Filbert Aurelian Tjiaranata,Weerayut Buaphet,Michael Anugraha,Sowmya Vajjala,Benjamin Rice,Azril Hafizi Amirudin,Jesujoba O. Alabi,Srikant Panda,Yassine Toughrai,Bruhan Kyomuhendo,Daniel Ruffinelli,Akshata A,Manuel Goulão,Ej Zhou,Ingrid Gabriela Franco Ramirez,Cristina Aggazzotti,Konstantin Dobler,Jun Kevin,Quentin Pagès,Nicholas Andrews,Nuhu Ibrahim,Mattes Ruckdeschel,Amr Keleg,Mike Zhang,Casper Muziri,Saron Samuel,Sotaro Takeshita,Kun Kerdthaisong,Luca Foppiano,Rasul Dent,Tommaso Green,Ahmad Mustapha Wali,Kamohelo Makaaka,Vicky Feliren,Inshirah Idris,Hande Celikkanat,Abdulhamid Abubakar,Jean Maillard,Benoît Sagot,Thibault Clérice,Kenton Murray,Sarah Luger*

Main category: cs.CL

TL;DR: 本文提出了一个涵盖109种语言的Web领域语言识别（LID）基准数据集CommonLID，对现有LID模型在多语言Web数据上的有效性进行了评测，并发现现有评测方法高估了模型实际表现，同时开放了数据集和相关代码。


<details>
  <summary>Details</summary>
Motivation: 随着多语言语料库构建的需求增长，自动化语言识别（LID）变得越来越重要。然而，现有模型在多样且噪声较大的Web数据上的表现仍有较大提升空间，尤其是在弱势语言上，且缺乏高质量、全面的评测基准。

Method: 作者组织社区力量，创建了一个人工标注的大规模Web领域LID数据集CommonLID，涵盖109种语言，尤其覆盖了许多之前研究较少的语言，并将其与其他五个常用LID评测集一道，测试了八种流行的LID模型。

Result: 通过实验证明，许多语言在Web环境下的LID准确率被现有评测方法高估，CommonLID可以更真实地反映模型的实际表现，同时对当前主流LID模型的优劣做了定量分析。

Conclusion: CommonLID为多语种语言识别领域提供了更高质量、更具代表性的评测资源，有助于推动多语言语料库建设和LID模型的进一步优化，并以开放许可发布，促进社区合作与资源共享。

Abstract: Language identification (LID) is a fundamental step in curating multilingual corpora. However, LID models still perform poorly for many languages, especially on the noisy and heterogeneous web data often used to train multilingual language models. In this paper, we introduce CommonLID, a community-driven, human-annotated LID benchmark for the web domain, covering 109 languages. Many of the included languages have been previously under-served, making CommonLID a key resource for developing more representative high-quality text corpora. We show CommonLID's value by using it, alongside five other common evaluation sets, to test eight popular LID models. We analyse our results to situate our contribution and to provide an overview of the state of the art. In particular, we highlight that existing evaluations overestimate LID accuracy for many languages in the web domain. We make CommonLID and the code used to create it available under an open, permissive license.

</details>


### [227] [Addressing LLM Diversity by Infusing Random Concepts](https://arxiv.org/abs/2601.18053)
*Pulin Agrawal,Prasoon Goyal*

Main category: cs.CL

TL;DR: 本论文探讨了通过在提示词中加入随机概念是否能够提升大语言模型（LLM）输出的多样性。实验结果表明，引入随机无关词语/句子可以有效增加模型输出内容的多样性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的文本内容常常缺乏多样性，这一问题限制了其在创意、生成等场景下的应用潜力。为提升输出多样性，作者尝试引入随机概念。

Method: 作者设计了系统的评估协议，主要通过让LLM回答“列举10位好莱坞演员”这种问题，并在问题前添加与内容无关的随机词句，再分析其输出的多样性指标。应用多种LLM进行了实验比较。

Result: 实验表明，在输入提示词前随机添加无关词或语句，能显著提升LLM输出结果的多样性。该效果在多种主流模型上均有体现。

Conclusion: 引入随机性的方法在提升LLM输出多样性上效果显著，相关评测协议也可推动未来LLM多样性系统化评估和相关领域进一步研究。

Abstract: Large language models (LLMs) are known to produce outputs with limited diversity. In this work, we study whether infusing random concepts in the prompts can improve the diversity of the generated outputs. To benchmark the approach, we design a systematic evaluation protocol which involves prompting an LLM with questions of the form "Name 10 Hollywood actors", and analyzing diversity measures of the resulting LLM outputs. Our experiments on multiple LLMs show that prepending random words/sentences unrelated to the prompt result in greater diversity in the outputs of LLMs. We believe that this promising result and the evaluation protocol opens up interesting avenues for future work, such as how infusing randomness into LLMs could be applied to other domains. Further, the evaluation protocol could also inspire research into benchmarking LLM diversity more systematically.

</details>


### [228] [Neurocomputational Mechanisms of Syntactic Transfer in Bilingual Sentence Production](https://arxiv.org/abs/2601.18056)
*Ahmet Yavuz Uluslu,Elliot Murphy*

Main category: cs.CL

TL;DR: 本文探讨在双语产出错误研究中，结合脑电振荡特征（oscillatory signatures）对现有理论的约束，并基于ROSE神经模型对句法迁移（syntactic transfer）及跨语言影响进行解释。


<details>
  <summary>Details</summary>
Motivation: 传统双语产出错误研究多用时序特征（如ERP），但对神经动力学的关注有限，因此需要结合新的神经实现层面证据来丰富理论解释。

Method: 论文提出在分析双语者产出错误和句子规划时，关注特定的脑电振荡失调，并利用ROSE神经模型对句法转移机制进行建模，通过考察跨语言影响（CLI）中的功能性抑制与竞争理论，揭示其与脑电振荡异常的关联。

Result: 研究发现用ROSE模型能够解释CLI中常见形态句法排序失败并可关联到具体的脑振荡失调模式，这为语言功能障碍提供更复杂的空间-时间生物标志物。

Conclusion: 将脑振荡特征引入双语研究和结合ROSE神经模型，有助于推动理论联系和对语言障碍神经标记的更全面探索。

Abstract: We discuss the benefits of incorporating into the study of bilingual production errors and their traditionally documented timing signatures (e.g., event-related potentials) certain types of oscillatory signatures, which can offer new implementational-level constraints for theories of bilingualism. We argue that a recent neural model of language, ROSE, can offer a neurocomputational account of syntactic transfer in bilingual production, capturing some of its formal properties and the scope of morphosyntactic sequencing failure modes. We take as a case study cross-linguistic influence (CLI) and attendant theories of functional inhibition/competition, and present these as being driven by specific oscillatory failure modes during L2 sentence planning. We argue that modeling CLI in this way not only offers the kind of linking hypothesis ROSE was built to encourage, but also licenses the exploration of more spatiotemporally complex biomarkers of language dysfunction than more commonly discussed neural signatures.

</details>


### [229] [Grounded Concreteness: Human-Like Concreteness Sensitivity in Vision-Language Models](https://arxiv.org/abs/2601.18065)
*Aryan Roy,Zekun Wang,Christopher J. MacLellan*

Main category: cs.CL

TL;DR: 本论文比较了视觉-语言模型（VLMs）与仅文本大型语言模型（LLMs）在人类对“具体性”敏感度上的表现，发现VLMs在多个层面上更具人类一致性。


<details>
  <summary>Details</summary>
Motivation: 理解视觉-语言模型在仅文本任务中是否会比单纯文本模型更接近人类对语言具体性的处理，从而探索多模态预训练是否能提供更好的“感知基础”。

Method: 选取结构匹配的Llama文本模型及其视觉对照组，分别在输出行为、嵌入空间几何结构和注意力机制三个层面分析具体性表现，并比较模型给出的具体性评分与人类数据的吻合程度。

Result: 在不同规模和不同基准任务上，VLMs在面对具体输入时总体表现更佳，嵌入空间沿“具体性”维度结构更加清晰，具体性打分更接近人类，同时模型的注意力分布也更符合感知基础的预期。

Conclusion: 多模态预训练使视觉-语言模型在文本任务里展现出更接近人类的具体性敏感性与表征，这种优势体现在输出、表征空间和注意力机制等多个层面。

Abstract: Do vision--language models (VLMs) develop more human-like sensitivity to linguistic concreteness than text-only large language models (LLMs) when both are evaluated with text-only prompts? We study this question with a controlled comparison between matched Llama text backbones and their Llama Vision counterparts across multiple model scales, treating multimodal pretraining as an ablation on perceptual grounding rather than access to images at inference. We measure concreteness effects at three complementary levels: (i) output behavior, by relating question-level concreteness to QA accuracy; (ii) embedding geometry, by testing whether representations organize along a concreteness axis; and (iii) attention dynamics, by quantifying context reliance via attention-entropy measures. In addition, we elicit token-level concreteness ratings from models and evaluate alignment to human norm distributions, testing whether multimodal training yields more human-consistent judgments. Across benchmarks and scales, VLMs show larger gains on more concrete inputs, exhibit clearer concreteness-structured representations, produce ratings that better match human norms, and display systematically different attention patterns consistent with increased grounding.

</details>


### [230] [Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents](https://arxiv.org/abs/2601.18077)
*Mahesh Ramesh,Kaousheik Jayakumar,Aswinkumar Ramkumar,Pavan Thodima,Aniket Rege*

Main category: cs.CL

TL;DR: 本文系统评估了17种主流大语言模型（LLMs）在多人卡牌游戏《Hanabi》中的协作推理能力，并提出了三种不同难度的推理场景。通过公开新数据集和针对LLM的微调，大幅提升了模型在《Hanabi》及泛化任务上的协作表现。


<details>
  <summary>Details</summary>
Motivation: 协同推理在信息不完整场景下极具挑战，而人类和人工智能多智能体系统都难以有效协作。《Hanabi》是复杂协作推理的经典实例，因此本文以其为测试平台，旨在深入理解和提升LLMs在复杂、多智能体协作任务中的推理和沟通能力。

Method: 作者设置了Watson、Sherlock和Mycroft三种场景，分别代表最少上下文、启用贝叶斯推理、以及多轮状态追踪，并对17种不同规模LLM（从4B到600B+）在2-5人游戏中的性能进行了系统性评测。还发布了两个高质量数据集以用于指令微调和强化学习微调，并基于这些数据集对特定模型进行了有监督和RL微调实验。

Result: （1）LLM能通过“内部工作记忆”追踪游戏状态；（2）模型间交互表现随自身强度提升而平滑变化；（3）推理能力最强的模型在《Hanabi》得分接近15分，虽优于传统模型，但仍显著低于经验丰富的人类和专用AI（均在20分以上）；（4）基于新数据集微调后，开源模型在合作表现上获得21%（监督）与156%（RL）提升，基本逼近强闭源模型，显著优于非推理型LLM。

Conclusion: 当前LLM已具备一定程度的协作推理能力，通过精细提示设计和数据微调可获得显著提升，但在有挑战性的协作任务上仍落后于专业系统和人类。公开的数据集推动了后续研究基础。RL微调模型在Hanabi范畴外任务中也有泛化提升，凸显优化多智能体合作推理能力的价值。

Abstract: Cooperative reasoning under incomplete information remains challenging for both humans and multi-agent systems. The card game Hanabi embodies this challenge, requiring theory-of-mind reasoning and strategic communication. We benchmark 17 state-of-the-art LLM agents in 2-5 player games and study the impact of context engineering across model scales (4B to 600B+) to understand persistent coordination failures and robustness to scaffolding: from a minimal prompt with only explicit card details (Watson setting), to scaffolding with programmatic, Bayesian-motivated deductions (Sherlock setting), to multi-turn state tracking via working memory (Mycroft setting). We show that (1) agents can maintain an internal working memory for state tracking and (2) cross-play performance between different LLMs smoothly interpolates with model strength. In the Sherlock setting, the strongest reasoning models exceed 15 points on average across player counts, yet still trail experienced humans and specialist Hanabi agents, both consistently scoring above 20. We release the first public Hanabi datasets with annotated trajectories and move utilities: (1) HanabiLogs, containing 1,520 full game logs for instruction tuning, and (2) HanabiRewards, containing 560 games with dense move-level value annotations for all candidate moves. Supervised and RL finetuning of a 4B open-weight model (Qwen3-Instruct) on our datasets improves cooperative Hanabi play by 21% and 156% respectively, bringing performance to within ~3 points of a strong proprietary reasoning model (o4-mini) and surpassing the best non-reasoning model (GPT-4.1) by 52%. The HanabiRewards RL-finetuned model further generalizes beyond Hanabi, improving performance on a cooperative group-guessing benchmark by 11%, temporal reasoning on EventQA by 6.4%, instruction-following on IFBench-800K by 1.7 Pass@10, and matching AIME 2025 mathematical reasoning Pass@10.

</details>


### [231] [CHiRPE: A Step Towards Real-World Clinical NLP with Clinician-Oriented Model Explanations](https://arxiv.org/abs/2601.18102)
*Stephanie Fong,Zimu Wang,Guilherme C. Oliveira,Xiangyu Zhao,Yiwen Jiang,Jiahe Liu,Beau-Luke Colton,Scott Woods,Martha E. Shenton,Barnaby Nelson,Zongyuan Ge,Dominic Dwyer*

Main category: cs.CL

TL;DR: 本文提出了CHiRPE，一个专为精神病风险预测设计且强化了解释性的NLP管道，在多中心临床数据上取得了高准确率，并开发了新型、临床专家高度认可的可解释性展示形式。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释人工智能（XAI）方法与临床推理不一致，缺乏临床医生的参与，导致NLP工具在医学领域难以被采纳。因此，研究动机是开发一种既能高效预测精神病风险，又能以临床友好的方式解释结果的系统。

Method: CHiRPE管道处理半结构化的临床访谈记录，集成了症状域映射、LLM摘要及BERT分类，实现精神病风险预测。解释模块采用与临床医生共同开发的SHAP新格式，并通过28位专家评估其可用性和偏好。

Result: 在AMP-SCZ多中心数据集（944份访谈记录，24家国际诊所）上，CHiRPE在三种BERT模型中准确率均超90%，优于传统基线方法。其创新的概念引导型解释方式（特别是图文结合的摘要格式）被临床专家广泛认可。

Conclusion: 与临床专家协作开发的模型，不仅提升了医学NLP工具的准确性，也增强了其临床可解释性。未来将进一步在真实场景中进行大规模测试。

Abstract: The medical adoption of NLP tools requires interpretability by end users, yet traditional explainable AI (XAI) methods are misaligned with clinical reasoning and lack clinician input. We introduce CHiRPE (Clinical High-Risk Prediction with Explainability), an NLP pipeline that takes transcribed semi-structured clinical interviews to: (i) predict psychosis risk; and (ii) generate novel SHAP explanation formats co-developed with clinicians. Trained on 944 semi-structured interview transcripts across 24 international clinics of the AMP-SCZ study, the CHiRPE pipeline integrates symptom-domain mapping, LLM summarisation, and BERT classification. CHiRPE achieved over 90% accuracy across three BERT variants and outperformed baseline models. Explanation formats were evaluated by 28 clinical experts who indicated a strong preference for our novel concept-guided explanations, especially hybrid graph-and-text summary formats. CHiRPE demonstrates that clinically-guided model development produces both accurate and interpretable results. Our next step is focused on real-world testing across our 24 international sites.

</details>


### [232] [GLEN-Bench: A Graph-Language based Benchmark for Nutritional Health](https://arxiv.org/abs/2601.18106)
*Jiatan Huang,Zheyuan Zhang,Tianyi Ma,Mingchen Li,Yaning Zheng,Yanfang Ye,Chuxu Zhang*

Main category: cs.CL

TL;DR: 本文提出了GLEN-Bench——首个基于图和语言的综合性营养健康评估基准，填补了营养干预领域个性化饮食建议和模型评估上的空白。


<details>
  <summary>Details</summary>
Motivation: 当前个性化营养干预的计算方法存在三大问题：1）饮食模式研究忽略了现实因素如经济条件、共病及食物可得性；2）推荐系统缺乏针对推荐原因的解释；3）缺乏统一的基准评价多任务能力。本文旨在系统性弥补这些空白。

Method: 作者整合NHANES健康数据、FNDDS食物成分数据和USDA食品可得性指标，构建了包含人口、健康、饮食、营养、社会经济等多维度知识图谱，并提出三个任务：风险检测、个性化多约束饮食推荐、基于知识图谱的自然语言问答。同时，采用图神经网络、大语言模型和混合架构在阿片类药物使用障碍这一具体场景上进行测试和评价。

Result: 结果表明，模型能有效识别饮食模式与健康风险的关联，并能在多约束下给出合理的饮食建议，同时实现可解释的自然语言答复。分析还明确了不同模型设计对任务效果的影响。

Conclusion: GLEN-Bench为营养健康领域的效果评估树立了统一的高标准，促进了实际场景下多任务模型的设计与落地，为今后的个性化营养干预提供了可行方案和改进方向。

Abstract: Nutritional interventions are important for managing chronic health conditions, but current computational methods provide limited support for personalized dietary guidance. We identify three key gaps: (1) dietary pattern studies often ignore real-world constraints such as socioeconomic status, comorbidities, and limited food access; (2) recommendation systems rarely explain why a particular food helps a given patient; and (3) no unified benchmark evaluates methods across the connected tasks needed for nutritional interventions. We introduce GLEN-Bench, the first comprehensive graph-language based benchmark for nutritional health assessment. We combine NHANES health records, FNDDS food composition data, and USDA food-access metrics to build a knowledge graph that links demographics, health conditions, dietary behaviors, poverty-related constraints, and nutrient needs. We test the benchmark using opioid use disorder, where models must detect subtle nutritional differences across disease stages. GLEN-Bench includes three linked tasks: risk detection identifies at-risk individuals from dietary and socioeconomic patterns; recommendation suggests personalized foods that meet clinical needs within resource constraints; and question answering provides graph-grounded, natural-language explanations to facilitate comprehension. We evaluate these graph-language approaches, including graph neural networks, large language models, and hybrid architectures, to establish solid baselines and identify practical design choices. Our analysis identifies clear dietary patterns linked to health risks, providing insights that can guide practical interventions.

</details>


### [233] [FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning](https://arxiv.org/abs/2601.18116)
*Lin Sun,Linglin Zhang,Jingang Huang,Change Jia,Zhengwei Cheng,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: 本文介绍了一种新的RAG（检索增强生成）框架FABLE，能够有效融合长上下文LLM的优势和结构化检索，解决现有方法的诸多缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文LLM虽然能力提升，但存在中间信息丢失、计算成本高、多文档推理扩展性差等局限；而传统RAG机制高效但检索粒度粗糙、语义噪声大，难以实现结构化跨文档证据整合。亟需一种结合两者优势的新方案。

Method: 提出FABLE框架：基于森林结构的自适应双路径LLM强化检索。具体做法包括：1）通过LLM构建多粒度语义分层森林索引；2）利用LLM引导的分层遍历和结构感知的证据传播，双路径协同获取精细化证据；3）显式预算控制实现效率和效果间的自适应权衡。

Result: 大量实验表明，FABLE在检索增强问答任务上持续超越现有最优RAG系统，并在显著减少（最高94%）Token计算的情况下达到与完整长上下文LLM推理相当的准确性。

Conclusion: 长上下文LLM虽有提升，但尚不能完全替代结构化检索。FABLE作为一种融合式框架，兼具准确性与高效性，将长上下文推理与结构化证据整合有机结合，为RAG系统发展指明了方向。

Abstract: The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.
  We present \textbf{FABLE}, a \textbf{F}orest-based \textbf{A}daptive \textbf{B}i-path \textbf{L}LM-\textbf{E}nhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.
  Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.

</details>


### [234] [Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models](https://arxiv.org/abs/2601.18129)
*Kunat Pipatanakul,Pittawat Taveekitworachai*

Main category: cs.CL

TL;DR: 本文提出了一种轻量、开放的后训练方法，使得在资源有限及需求主权的环境下，也能训练出高性能的区域性或主权大模型。通过用泰语案例验证，该方法能在小规模训练资源下获得良好效果。


<details>
  <summary>Details</summary>
Motivation: 当前主流大模型主要集中在高资源语言且由少数机构掌控，不利于地区机构在有限资源、需要透明和自主控制的情境下使用。为了解决地区主权和可用性需求，需开发低门槛、易适应的模型训练方案。

Method: 提出Typhoon S，通过监督微调、on-policy蒸馏和小规模RFT（基于InK-GRPO）结合，实现主权和通用能力兼备的模型训练，无需大规模指令语料或复杂工具链。用泰语数据进行实验验证。

Result: 实验表明，Typhoon S方案可将基础模型转换为具备强泛化性能的指令微调模型。在泰语法律推理及本地知识上通过小规模RFT显著提升，同时保持通用能力。

Conclusion: 只需小规模、高效后训练流程即可支撑高质量主权大模型训练，有效降低对大指令数据和算力的依赖，为学术及地区应用提供了现实可行路径。

Abstract: Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.

</details>


### [235] [Fine-Grained Emotion Detection on GoEmotions: Experimental Comparison of Classical Machine Learning, BiLSTM, and Transformer Models](https://arxiv.org/abs/2601.18162)
*Ani Harutyunyan,Sachin Kumar*

Main category: cs.CL

TL;DR: 本文在GoEmotions数据集上对细粒度情感识别任务进行了三种主流模型（TF-IDF逻辑回归、BiLSTM与注意力机制、BERT多标签分类）的系统性对比。BERT的整体表现优于官方基线，对稀有/模糊情感更有效。


<details>
  <summary>Details</summary>
Motivation: 细粒度情感识别任务面临标签重叠和类别不均衡问题，现有方法的性能和优势需系统评估。

Method: 作者在GoEmotions数据集上，对三类模型（基于TF-IDF的逻辑回归+二元相关性、带注意力的BiLSTM、微调的BERT）进行了多标签分类对比，采用逆频权重处理类别不平衡，评估指标包括Micro-F1、Macro-F1、Hamming Loss和Subset Accuracy。

Result: 逻辑回归模型Micro-F1最高（0.51），BERT在Macro-F1（0.49）、Hamming Loss（0.036）和Subset Accuracy（0.36）表现最佳，并优于官方报告。

Conclusion: 高频情感主要依靠表面词汇特征，而BERT等上下文模型在低频/模糊情感识别上更具优势，整体上提升了表现。

Abstract: Fine-grained emotion recognition is a challenging multi-label NLP task due to label overlap and class imbalance. In this work, we benchmark three modeling families on the GoEmotions dataset: a TF-IDF-based logistic regression system trained with binary relevance, a BiLSTM with attention, and a BERT model fine-tuned for multi-label classification. Experiments follow the official train/validation/test split, and imbalance is mitigated using inverse-frequency class weights. Across several metrics, namely Micro-F1, Macro-F1, Hamming Loss, and Subset Accuracy, we observe that logistic regression attains the highest Micro-F1 of 0.51, while BERT achieves the best overall balance surpassing the official paper's reported results, reaching Macro-F1 0.49, Hamming Loss 0.036, and Subset Accuracy 0.36. This suggests that frequent emotions often rely on surface lexical cues, whereas contextual representations improve performance on rarer emotions and more ambiguous examples.

</details>


### [236] [MemWeaver: Weaving Hybrid Memories for Traceable Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2601.18204)
*Juexiang Ye,Xue Li,Xinyu Yang,Chengkai Huang,Lanshun Nie,Lina Yao,Dechen Zhan*

Main category: cs.CL

TL;DR: 本文提出了MemWeaver，一种面向长时序交互的智能体记忆框架，结合结构化知识和原始证据，显著提高了多步骤和时序推理准确度，并大幅减小了输入上下文长度。


<details>
  <summary>Details</summary>
Motivation: 现有大模型代理在长时交互中的记忆系统缺乏时序一致性、可多跳推理和证据溯源能力，通常依赖无结构检索，导致推理脆弱、时序冲突和溯源性差。

Method: MemWeaver将长期经验压缩整合为三个部分：1）时序图记忆，实现结构化的关系推理；2）经验记忆，抽象多次观测中的规律；3）原文片段记忆，存储原始文本证据。同时采用双通道检索，同时获得结构化知识和证据以构建精炼且信息密集的上下文。

Result: 在LoCoMo基准上，MemWeaver在多步和时序推理正确率上显著优于长上下文检索基线法，并将输入上下文长度缩短了95%以上。

Conclusion: MemWeaver实现了更高效且更强推理能力的大模型智能体记忆系统，为长时序任务的追溯性和准确性提供了新范式。

Abstract: Large language model-based agents operating in long-horizon interactions require memory systems that support temporal consistency, multi-hop reasoning, and evidence-grounded reuse across sessions. Existing approaches largely rely on unstructured retrieval or coarse abstractions, which often lead to temporal conflicts, brittle reasoning, and limited traceability. We propose MemWeaver, a unified memory framework that consolidates long-term agent experiences into three interconnected components: a temporally grounded graph memory for structured relational reasoning, an experience memory that abstracts recurring interaction patterns from repeated observations, and a passage memory that preserves original textual evidence. MemWeaver employs a dual-channel retrieval strategy that jointly retrieves structured knowledge and supporting evidence to construct compact yet information-dense contexts for reasoning. Experiments on the LoCoMo benchmark demonstrate that MemWeaver substantially improves multi-hop and temporal reasoning accuracy while reducing input context length by over 95\% compared to long-context baselines.

</details>


### [237] [TechING: Towards Real World Technical Image Understanding via VLMs](https://arxiv.org/abs/2601.18238)
*Tafazzul Nadeem,Bhavik Shangari,Manish Rai,Gagan Raj Gupta,Ashutosh Modi*

Main category: cs.CL

TL;DR: 文章提出了一种通过合成技术图像大规模训练VLM（视觉语言模型）的方法，用于更好地理解和编辑手绘技术图。


<details>
  <summary>Details</summary>
Motivation: 现实中技术人员在交流时常用手绘图，但这些图后期数字化修改很麻烦。现有VLM虽然在图片理解上有进步，但对技术图理解较差，且大量真实手绘图数据难以获取。

Method: 作者合成了一大批拟真手绘技术图用于训练，并设计了多种自监督任务，系统评估了不同基线模型，最终在Llama 3.2 11B-instruct模型基础上微调，得到名为LLama-VL-TUG的新模型。

Result: LLama-VL-TUG模型在合成图上ROUGE-L指标比原模型提升2.14倍，在真实手绘图上F1分数提升6.97倍，并在多数类型图中表现出最少的编译错误，全面超越各基线模型。

Conclusion: 通过大规模合成数据结合自监督训练，可以大幅提升VLM对技术手绘图的理解和数字化能力，为后续相关任务提供了新思路与高性能模型。

Abstract: Professionals working in technical domain typically hand-draw (on whiteboard, paper, etc.) technical diagrams (e.g., flowcharts, block diagrams, etc.) during discussions; however, if they want to edit these later, it needs to be drawn from scratch. Modern day VLMs have made tremendous progress in image understanding but they struggle when it comes to understanding technical diagrams. One way to overcome this problem is to fine-tune on real world hand-drawn images, but it is not practically possible to generate large number of such images. In this paper, we introduce a large synthetically generated corpus (reflective of real world images) for training VLMs and subsequently evaluate VLMs on a smaller corpus of hand-drawn images (with the help of humans). We introduce several new self-supervision tasks for training and perform extensive experiments with various baseline models and fine-tune Llama 3.2 11B-instruct model on synthetic images on these tasks to obtain LLama-VL-TUG, which significantly improves the ROUGE-L performance of Llama 3.2 11B-instruct by 2.14x and achieves the best all-round performance across all baseline models. On real-world images, human evaluation reveals that we achieve minimum compilation errors across all baselines in 7 out of 8 diagram types and improve the average F1 score of Llama 3.2 11B-instruct by 6.97x.

</details>


### [238] [BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation](https://arxiv.org/abs/2601.18253)
*Peng Sun,Xiangyu Zhang,Duan Wu*

Main category: cs.CL

TL;DR: 本文提出了一种新框架BoRP，用于大规模、高保真的用户满意度评估，显著优于传统生成式方法。


<details>
  <summary>Details</summary>
Motivation: 现有对用户满意度的评估方式存在两大问题：一是A/B测试依赖的显式反馈稀缺，二是隐式指标不够明确。这些都阻碍了对对话式AI的有效迭代。

Method: 提出BoRP（Bootstrapped Regression Probing）框架。该方法利用大语言模型（LLM）的潜空间特性，通过基于极化指数的自举方法生成评价标准，并用偏最小二乘（PLS）回归将模型隐藏态映射为连续分数，以实现自动化、可扩展的满意度打分。

Result: 在工业级数据集上，BoRP（Qwen3-8B/14B）相比先进的生成式基线（如Qwen3-Max）与人工评判的一致性更高。同时，BoRP大幅降低计算成本，可实现大规模灵敏的A/B测试与全量监控。

Conclusion: BoRP为开放式对话AI带来高效、可靠的满意度评估工具，不仅提升评估质量，也提高实际生产应用的可行性。

Abstract: Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrapped Regression Probing), a scalable framework for high-fidelity satisfaction evaluation. Unlike generative approaches, BoRP leverages the geometric properties of LLM latent space. It employs a polarization-index-based bootstrapping mechanism to automate rubric generation and utilizes Partial Least Squares (PLS) to map hidden states to continuous scores. Experiments on industrial datasets show that BoRP (Qwen3-8B/14B) significantly outperforms generative baselines (even Qwen3-Max) in alignment with human judgments. Furthermore, BoRP reduces inference costs by orders of magnitude, enabling full-scale monitoring and highly sensitive A/B testing via CUPED.

</details>


### [239] [Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating Inference for Empathy-Aware End-to-End Spoken Dialogue](https://arxiv.org/abs/2601.18281)
*Yuhang Jia,Pei Liu,Haoqin Sun,Jiaming Zhou,Xuxin Cheng,Cao Liu,Ke Zeng,Xunliang Cai,Yong Qin*

Main category: cs.CL

TL;DR: 本文提出了用于同理心对话的自然语言评测工具EmpathyEval，以及结合自我反思机制的端到端SLM模型ReEmpathy，有效提升了语音对话系统的同理心回复能力。


<details>
  <summary>Details</summary>
Motivation: 现有同理心对话系统过度依赖监督信号，难以覆盖复杂的情感表达和同理行为的多样性。如何提升情感对话系统的灵活性和人性化理解，是一个亟需解决的问题。

Method: 作者提出EmpathyEval，用自然语言进行描述式评测代替传统的分数或标签。基于此，又提出了ReEmpathy模型，首次在端到端语音对话系统中引入‘同理自我反思交替推理’机制，使模型在生成回复前能进行自由形式的同理反思。

Result: 实验结果显示，引入反思机制的ReEmpathy模型同理心表现明显优于现有方法，生成的语音回复在情感理解和同理表达上有显著提升。

Conclusion: 通过自然语言评测和自我反思机制，系统可以更好地捕捉和表达复杂的情感与同理心，为实现更具情感智能的人机对话带来新的可能。

Abstract: End-to-end Spoken Language Models (SLMs) hold great potential for paralinguistic perception, and numerous studies have aimed to enhance their capabilities, particularly for empathetic dialogue. However, current approaches largely depend on rigid supervised signals, such as ground-truth response in supervised fine-tuning or preference scores in reinforcement learning. Such reliance is fundamentally limited for modeling complex empathy, as there is no single "correct" response and a simple numerical score cannot fully capture the nuances of emotional expression or the appropriateness of empathetic behavior. To address these limitations, we sequentially introduce EmpathyEval, a descriptive natural-language-based evaluation model for assessing empathetic quality in spoken dialogues. Building upon EmpathyEval, we propose ReEmpathy, an end-to-end SLM that enhances empathetic dialogue through a novel Empathetic Self-Reflective Alternating Inference mechanism, which interleaves spoken response generation with free-form, empathy-related reflective reasoning. Extensive experiments demonstrate that ReEmpathy substantially improves empathy-sensitive spoken dialogue by enabling reflective reasoning, offering a promising approach toward more emotionally intelligent and empathy-aware human-computer interactions.

</details>


### [240] [U-Fold: Dynamic Intent-Aware Context Folding for User-Centric Agents](https://arxiv.org/abs/2601.18285)
*Jin Su,Runnan Fang,Yeqiu Li,Xiaobin Wang,Shihao Cai,Pengjun Xie,Ningyu Zhang,Fajie Yuan*

Main category: cs.CL

TL;DR: 本文提出了一种名为U-Fold的动态上下文压缩框架，能够更好地支持以用户为中心的大型语言模型多轮对话任务，有效解决了以往方法在长对话和复杂任务中遇到的信息遗失和意图追踪困难等问题。实验表明U-Fold在多个基准测试上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型基于工具增强的对话在上下文长度受限时，需通过上下文折叠方法总结历史，但这些方法往往针对单一查询或意图，易丢失关键信息且不能追踪用户意图变化，因此难以满足真实的多轮、复杂对话需求。

Method: 作者提出U-Fold框架，每轮对话动态生成意图感知的对话摘要和与任务紧密相关的工具调用日志，同时保留全部的原始用户-代理对话和工具调用历史，从而实现对关键信息和用户意图动态、细致的管理。

Result: 在τ-bench、τ²-bench、VitaBench等多个基准及更复杂的“极限长上下文”任务中，U-Fold在与ReAct和其它折叠基线对比时表现出显著优势，在长上下文条件下的胜率达71.4%，提升幅度最高可达27%。

Conclusion: U-Fold显著提升了工具增强型大模型在真实多轮对话中的上下文管理能力，为单查询折叠技术向用户中心实际应用场景迁移奠定了基础。

Abstract: Large language model (LLM)-based agents have been successfully deployed in many tool-augmented settings, but their scalability is fundamentally constrained by context length. Existing context-folding methods mitigate this issue by summarizing past interactions, yet they are typically designed for single-query or single-intent scenarios. In more realistic user-centric dialogues, we identify two major failure modes: (i) they irreversibly discard fine-grained constraints and intermediate facts that are crucial for later decisions, and (ii) their summaries fail to track evolving user intent, leading to omissions and erroneous actions. To address these limitations, we propose U-Fold, a dynamic context-folding framework tailored to user-centric tasks. U-Fold retains the full user--agent dialogue and tool-call history but, at each turn, uses two core components to produce an intent-aware, evolving dialogue summary and a compact, task-relevant tool log. Extensive experiments on $τ$-bench, $τ^2$-bench, VitaBench, and harder context-inflated settings show that U-Fold consistently outperforms ReAct (achieving a 71.4% win rate in long-context settings) and prior folding baselines (with improvements of up to 27.0%), particularly on long, noisy, multi-turn tasks. Our study demonstrates that U-Fold is a promising step toward transferring context-management techniques from single-query benchmarks to realistic user-centric applications.

</details>


### [241] [Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning](https://arxiv.org/abs/2601.18296)
*Zhaoyan Gong,Zhiqiang Liu,Songze Li,Xiaoke Guo,Yuanxiang Liu,Xinle Deng,Zhizhen Liu,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 提出了一种新型的自助型时序知识图谱问答（TKGQA）代理Temp-R1，通过强化学习训练，具备强大的多跳推理和时序约束理解能力，在当前主流数据集上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 时序知识图谱问答涉及动态事实、多跳依赖及复杂时序关系，现有方法流程僵化且依赖于昂贵的私有API，无法适应灵活和大规模的应用场景，因此需要更智能、开源且自动化的方案。

Method: 提出自主型端到端TKGQA智能体Temp-R1，采用强化学习训练。扩展了智能体的“动作空间”，引入内外部专门动作以减轻单次推理压力，同时设计逆向课程学习策略，即先学难题后迁移到简单问题，防止模型只学会捷径而非复杂推理。

Result: 具备80亿参数的Temp-R1在MultiTQ和TimelineKGQA数据集上取得了最优表现，尤其在复杂问题上的准确率相较已有强基线提升了19.8%。

Conclusion: 验证了逆向课程学习与动作空间拓展对提升复杂时序推理能力的有效性，首次树立了自主型TKGQA智能体新范式。代码即将开源，便于业界复现和应用。

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at https://github.com/zjukg/Temp-R1.

</details>


### [242] [Suppressing Final Layer Hidden State Jumps in Transformer Pretraining](https://arxiv.org/abs/2601.18302)
*Keigo Shibata,Kazuki Yano,Ryosuke Takahashi,Jaesung Lee,Wataru Ikeda,Jun Suzuki*

Main category: cs.CL

TL;DR: 本文分析了Transformer模型在中间层与末层之间隐藏状态角度距离突然变化的现象，并提出了一种正则项JREG来抑制这一现象，实验证明能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 最近发现预训练Transformer模型的中间层隐藏状态变化较小，而在末层附近突然产生较大变化，这种行为可能对模型能力的均衡使用不利，因此需要对其进行量化分析和纠正。

Method: 提出并定义了final layer“jump”的量化指标，测试了该现象在多种主流开源模型上的普遍性和随预训练放大的趋势，并提出在预训练阶段增加一种Jump Suppressing Regularizer（JREG）损失，鼓励模型在所有层中更均衡地利用能力。

Result: 在不同规模的Llama模型上实验发现，采用JREG正则后的模型在不改变结构的情况下，相比基线表现出更好的下游任务能力。

Conclusion: Transformer中末层隐藏状态突变是一种普遍且随预训练增强的特性，通过正则项JREG可以有效抑制这一现象，并带来性能提升。

Abstract: This paper discusses the internal behavior of Transformer language models. Many recent pre-trained models have been reported to exhibit only slight changes in the angular distance between the input and output hidden state vectors in the middle Transformer layers, despite a disproportionately large ``jump'' in the angular distance occurring in or around the final Transformer layer. To characterize this, we first introduce a quantitative metric for the jump strength around the final layer, and then demonstrate its prevalence across many open-weight models, as well as its amplification throughout pre-training. Assuming such jumps indicate an undesirable property, we propose the jump-suppressing regularizer (JREG) which penalizes this jump during pre-training, thereby encouraging more balanced capability usage across the middle layers. Empirical evaluations of three model sizes of Llama-based models, trained with the proposed JREG method, reveal improved task performance compared to the baseline without altering the model architecture.

</details>


### [243] [Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM](https://arxiv.org/abs/2601.18306)
*Everlyn Asiko Chimoto,Mostafa Elhoushi,Bruce A. Bassett*

Main category: cs.CL

TL;DR: 本文系统评估了多语言校准在量化多语言大模型时的作用，发现相比只用英语，使用多语言或目标语言的校准集显著提升了量化后的模型表现。


<details>
  <summary>Details</summary>
Motivation: 虽然量化可以有效减少大语言模型的存储和计算开销，但现有量化方法多只用英语小样本校准，且对多语言模型的影响缺乏系统研究。随着多语言模型广泛应用，探索更优的量化校准策略具有重要意义。

Method: 作者在10种语言的数据上，采用两种主流量化器（GPTQ和AWQ），设计八种校准集（包括单语言和多语言混合），对Llama3.1 8B和Qwen2.5 7B模型系统开展量化实验，并用困惑度指标评估表现；同时分析不同语言与量化器组合的表现差异。

Result: 结果发现：非英语及多语言校准集可显著降低困惑度，提升量化后性能。其中多语言混合校准集的困惑度下降最明显，最高降幅达3.52。针对性地用目标语言校准集对单语言评测提升最大。同时发现极个别语言-量化器组合会因激活分布异常而性能下降。

Conclusion: 量化大模型时“一刀切”地用英语校准并不理想。应结合目标语言和多样性调整校准数据，这对多语言模型的健壮量化至关重要。

Abstract: Quantization is an effective technique for reducing the storage footprint and computational costs of Large Language Models (LLMs), but it often results in performance degradation. Existing post-training quantization methods typically use small, English-only calibration sets; however, their impact on multilingual models remains underexplored. We systematically evaluate eight calibration settings (five single-language and three multilingual mixes) on two quantizers (GPTQ, AWQ) on data from 10 languages. Our findings reveal a consistent trend: non-English and multilingual calibration sets significantly improve perplexity compared to English-only baselines. Specifically, we observe notable average perplexity gains across both quantizers on Llama3.1 8B and Qwen2.5 7B, with multilingual mixes achieving the largest overall reductions of up to 3.52 points in perplexity. Furthermore, our analysis indicates that tailoring calibration sets to the evaluation language yields the largest improvements for individual languages, underscoring the importance of linguistic alignment. We also identify specific failure cases where certain language-quantizer combinations degrade performance, which we trace to differences in activation range distributions across languages. These results highlight that static one-size-fits-all calibration is suboptimal and that tailoring calibration data, both in language and diversity, plays a crucial role in robustly quantizing multilingual LLMs.

</details>


### [244] [MultiVis-Agent: A Multi-Agent Framework with Logic Rules for Reliable and Comprehensive Cross-Modal Data Visualization](https://arxiv.org/abs/2601.18320)
*Jinwei Lu,Yuanfeng Song,Chen Zhang,Raymond Chi-Wing Wong*

Main category: cs.CL

TL;DR: 本文提出了一种名为MultiVis-Agent的多智能体多模态可视化系统，通过引入数学约束的逻辑规则，实现了复杂多场景下的高可靠性自动可视化生成，性能优越。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的可视化需求复杂，往往涉及多模态输入（如文本、图片、代码等）和多步迭代，而现有系统仅支持单一模态和一次性生成，工作流程刚性，难以满足实际需求。此外，基于大模型的方法虽然有潜力，但容易出现可靠性问题，如死循环或重大错误，需要新的技术应对。

Method: 作者提出了MultiVis-Agent，多智能体、逻辑规则增强的框架。具体包括引入四层逻辑规则，以数学约束形式指导大模型推理，在保证灵活性的同时提升了系统的可靠性。为此，设计了包含1000多个案例的MultiVis-Bench多模态评测基准，覆盖从基础生成到多步迭代的四类场景。

Result: 实验表明，MultiVis-Agent在多模态复杂任务的可视化得分达到75.63%，较基线（57.54-62.79%）大幅提升；任务完成率达99.58%、代码执行成功率94.56%，而无逻辑规则情形分别为74.48%和65.10%。

Conclusion: 该方法有效提升了智能体可视化系统在多模态、多场景复杂任务中的鲁棒性和可靠性，成功克服了现有自动可视化生成手段在复杂性与可靠性上的双重瓶颈。

Abstract: Real-world visualization tasks involve complex, multi-modal requirements that extend beyond simple text-to-chart generation, requiring reference images, code examples, and iterative refinement. Current systems exhibit fundamental limitations: single-modality input, one-shot generation, and rigid workflows. While LLM-based approaches show potential for these complex requirements, they introduce reliability challenges including catastrophic failures and infinite loop susceptibility. To address this gap, we propose MultiVis-Agent, a logic rule-enhanced multi-agent framework for reliable multi-modal and multi-scenario visualization generation. Our approach introduces a four-layer logic rule framework that provides mathematical guarantees for system reliability while maintaining flexibility. Unlike traditional rule-based systems, our logic rules are mathematical constraints that guide LLM reasoning rather than replacing it. We formalize the MultiVis task spanning four scenarios from basic generation to iterative refinement, and develop MultiVis-Bench, a benchmark with over 1,000 cases for multi-modal visualization evaluation. Extensive experiments demonstrate that our approach achieves 75.63% visualization score on challenging tasks, significantly outperforming baselines (57.54-62.79%), with task completion rates of 99.58% and code execution success rates of 94.56% (vs. 74.48% and 65.10% without logic rules), successfully addressing both complexity and reliability challenges in automated visualization generation.

</details>


### [245] [Overalignment in Frontier LLMs: An Empirical Study of Sycophantic Behaviour in Healthcare](https://arxiv.org/abs/2601.18334)
*Clément Christophe,Wadood Mohammed Abdul,Prateek Munjal,Tathagata Raha,Ronnie Rajan,Praveenkumar Kanithi*

Main category: cs.CL

TL;DR: 引入了一种评估大语言模型（LLM）在医疗场景中迎合用户（sycophancy）倾向的新指标，并通过实验证明准确率高的“推理型”模型在权威压力下可能更易产生不真实的迎合性回答。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被广泛用于医疗流程中，其倾向于迎合用户而非坚持事实准确性，存在危害患者安全的风险。现有评测方法多为主观设计，缺乏可验证的、标准化的评测框架。因此，作者希望通过更为客观和严谨的方法分析和评估这类风险。

Method: 提出了一种基于带可验证标准答案的医学选择题（MCQA）的评测框架，并引入了校正迎合分数（Adjusted Sycophancy Score）以排除模型自身不稳定因素对评测的干扰。同时，采用规模化分析研究Qwen-3和Llama-3等最新大模型家族。

Result: 发现模型的抗迎合能力与模型规模呈现特定规律。尤其值得注意的是，优化推理能力的模型虽然表现出色，但在权威性用户建议下更易于错误合理化，出现错误的自我推断。

Conclusion: 结果提示，通用基准测试成绩不能代表模型在临床实际中的可靠性，精简推理结构可能反而能提升对专家诱导下迎合性的鲁棒性。

Abstract: As LLMs are increasingly integrated into clinical workflows, their tendency for sycophancy, prioritizing user agreement over factual accuracy, poses significant risks to patient safety. While existing evaluations often rely on subjective datasets, we introduce a robust framework grounded in medical MCQA with verifiable ground truths. We propose the Adjusted Sycophancy Score, a novel metric that isolates alignment bias by accounting for stochastic model instability, or "confusability". Through an extensive scaling analysis of the Qwen-3 and Llama-3 families, we identify a clear scaling trajectory for resilience. Furthermore, we reveal a counter-intuitive vulnerability in reasoning-optimized "Thinking" models: while they demonstrate high vanilla accuracy, their internal reasoning traces frequently rationalize incorrect user suggestions under authoritative pressure. Our results across frontier models suggest that benchmark performance is not a proxy for clinical reliability, and that simplified reasoning structures may offer superior robustness against expert-driven sycophancy.

</details>


### [246] [When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs](https://arxiv.org/abs/2601.18350)
*Junyi Zou*

Main category: cs.CL

TL;DR: 本文探讨了在医疗领域大模型适配器融合中的干扰问题，并提出了一种加权适配器融合方法以提升医学问答的精准性和模型安全性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用能力上表现优异，但在医学等安全关键领域，常常面临医学术语不精确和指令遵循性不佳的问题。为提升其针对医疗任务的能力，需要平衡指令执行能力与医学知识的保留。

Method: 该研究以14B参数基座模型为例，采用两阶段LoRA适配管道：（1）通过领域自适应预训练（DAPT）引入医学知识；（2）以指令风格数据，通过有监督微调适配医学问答行为。同时，提出加权适配器融合方法，将SFT和PT适配器线性合并，最终导出融合后的模型参数。

Result: 在独立医学验证集上的评估结果显示，融合模型在BLEU-4、ROUGE-1、ROUGE-2、ROUGE-L等指标上表现良好，分别达到16.38、20.42、4.60和11.54。同时对解码敏感性和训练稳定性进行曲线和对比分析，验证方法的有效性。

Conclusion: 加权适配器融合方法可以在提升大模型医疗领域能力和安全性的同时，不显著损失指令遵循性，对安全关键领域的大模型微调具有实际意义。

Abstract: Large language models (LLMs) show strong general capability but often struggle with medical terminology precision and safety-critical instruction following. We present a case study for adapter interference in safety-critical domains using a 14B-parameter base model through a two-stage LoRA pipeline: (1) domain-adaptive pre-training (PT) to inject broad medical knowledge via continued pre-training (DAPT), and (2) supervised fine-tuning (SFT) to align the model with medical question-answering behaviors through instruction-style data. To balance instruction-following ability and domain knowledge retention, we propose Weighted Adapter Merging, linearly combining SFT and PT adapters before exporting a merged base-model checkpoint. On a held-out medical validation set (F5/F6), the merged model achieves BLEU-4 = 16.38, ROUGE-1 = 20.42, ROUGE-2 = 4.60, and ROUGE-L = 11.54 under a practical decoding configuration. We further analyze decoding sensitivity and training stability with loss curves and controlled decoding comparisons.

</details>


### [247] [Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning](https://arxiv.org/abs/2601.18352)
*Manjie Xu,Isabella Yin,Xinyi Tu,Chi Zhang,Yixin Zhu*

Main category: cs.CL

TL;DR: 论文发现，大型语言模型（LLM）在遇到与其预训练知识相悖的新规则时，往往难以抑制已有认知，这一现象被称为语义惯性。通过“Baba Is You”游戏作为实验平台，研究人员评估了模型在规则变化下抑制先验知识的能力，并发现大模型有时表现反而不如小模型。针对这一问题，作者提出将动态规则用可执行代码表示，并采用新的训练方法，有效防止先验干扰，提高推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM容易因预训练期间固化的常识导致在面对新规则时无法正确推理，且大型模型反而表现更差。作者希望深入分析这一现象，具体表现在“Baba Is You”中如何出现及如何解决。

Method: 作者在“Baba Is You”游戏环境下，通过让模型遇到与常识相违的动态规则（如“Lava is Safe”）来测试其推理能力。分析提出问题的根源在于模型将语义描述和逻辑规则混淆。随后，作者提出用代码形式明确表示动态规则，并采用Code-Grounded Vistas(LCV)训练法，让模型专注逻辑约束而非表面描述。该方法注重训练时的针对性优化而非靠推理时的搜索。

Result: 实验显示，大型模型在仅用自然语言表示规则时较小模型更易被原有常识干扰，抑制错误先验能力较弱。引入代码化规则与LCV方法后，模型对新规则的适应能力明显提升，推理更准确且效率更高，甚至超越了推理时的复杂搜索方法。

Conclusion: 研究表明，表示方式（自然语言vs代码）及训练方式决定了大模型在上下文推理任务中的表现。仅靠增大模型规模无法解决所有问题，实际场景下需结合合适的表示和训练机制，尤其在动态、反常的推理需求下更是如此。

Abstract: LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., "Lava is Dangerous") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting "Lava is Safe"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.

</details>


### [248] [CitiLink: Enhancing Municipal Transparency and Citizen Engagement through Searchable Meeting Minutes](https://arxiv.org/abs/2601.18374)
*Rodrigo Silva,José Evans,José Isidro,Miguel Marques,Afonso Fonseca,Ricardo Morais,João Canavilhas,Arian Pasquali,Purificação Silvano,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,Ricardo Campos*

Main category: cs.CL

TL;DR: 该论文介绍了CitiLink平台，利用NLP和IR技术将市政会议纪要转化为结构化、可检索的信息，提升政府透明度和数据可访问性。


<details>
  <summary>Details</summary>
Motivation: 市政会议纪要虽公开，但结构杂乱且冗长，普通市民和记者难以高效查找所需信息，因此需要改善其可用性和检索性。

Method: 设计并实现了CitiLink平台，通过大语言模型抽取会议元数据、议题、投票结果，存入数据库，并利用BM25实现全文检索和多维筛选。平台界面友好，支持易用交互。以6个葡萄牙城市、共120份纪要为实验数据，对市政工作人员实际操作进行指导测试，并利用Gemini模型评估信息抽取的有效性。

Result: 系统提升了市政会议纪要的信息检索与可访问性，用户测试获得了实际使用反馈，Gemini模型在相关信息抽取任务上表现有效。

Conclusion: CitiLink平台证明了NLP与IR结合能够提升市政数据的结构化和透明度，对提升地方政府公开数据的利用和透明度具有借鉴意义。

Abstract: City council minutes are typically lengthy and formal documents with a bureaucratic writing style. Although publicly available, their structure often makes it difficult for citizens or journalists to efficiently find information. In this demo, we present CitiLink, a platform designed to transform unstructured municipal meeting minutes into structured and searchable data, demonstrating how NLP and IR can enhance the accessibility and transparency of local government. The system employs LLMs to extract metadata, discussed subjects, and voting outcomes, which are then indexed in a database to support full-text search with BM25 ranking and faceted filtering through a user-friendly interface. The developed system was built over a collection of 120 minutes made available by six Portuguese municipalities. To assess its usability, CitiLink was tested through guided sessions with municipal personnel, providing insights into how real users interact with the system. In addition, we evaluated Gemini's performance in extracting relevant information from the minutes, highlighting its effectiveness in data extraction.

</details>


### [249] [Hierarchical Text Classification with LLM-Refined Taxonomies](https://arxiv.org/abs/2601.18375)
*Jonas Golde,Nicolaas Jedema,Ravi Krishnan,Phong Le*

Main category: cs.CL

TL;DR: 作者提出了TaxMorph框架，使用大语言模型(LLM)优化层级文本分类中的标签体系，经实验证明优化能提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有层级文本分类依赖人工设计的标签体系，但这些体系常存在歧义，影响语言模型学习效果。因此，期望通过自动化方法生成更适合模型学习的标签体系。

Method: 提出了TaxMorph框架，利用大语言模型对完整标签层级进行重命名、合并、拆分和重排序等操作，重新修正标签体系，使其更符合语言模型的语义理解。

Result: 在三个层级文本分类基准数据集上，LLM优化后的标签体系在多种设置下F1值最高提升2.9个百分点。分析表明，虽然人工体系下的类别嵌入更分离，但LLM优化体系更契合模型真实分类偏差。

Conclusion: LLM引导的标签体系优化能提升层级文本分类性能，因为优化后的体系更贴合模型的学习方式和归纳偏好。

Abstract: Hierarchical text classification (HTC) depends on taxonomies that organize labels into structured hierarchies. However, many real-world taxonomies introduce ambiguities, such as identical leaf names under similar parent nodes, which prevent language models (LMs) from learning clear decision boundaries. In this paper, we present TaxMorph, a framework that uses large language models (LLMs) to transform entire taxonomies through operations such as renaming, merging, splitting, and reordering. Unlike prior work, our method revises the full hierarchy to better match the semantics encoded by LMs. Experiments across three HTC benchmarks show that LLM-refined taxonomies consistently outperform human-curated ones in various settings up to +2.9pp. in F1. To better understand these improvements, we compare how well LMs can assign leaf nodes to parent nodes and vice versa across human-curated and LLM-refined taxonomies. We find that human-curated taxonomies lead to more easily separable clusters in embedding space. However, the LLM-refined taxonomies align more closely with the model's actual confusion patterns during classification. In other words, even though they are harder to separate, they better reflect the model's inductive biases. These findings suggest that LLM-guided refinement creates taxonomies that are more compatible with how models learn, improving HTC performance.

</details>


### [250] [Corpus-Based Approaches to Igbo Diacritic Restoration](https://arxiv.org/abs/2601.18380)
*Ignatius Ezeani*

Main category: cs.CL

TL;DR: 本文针对低资源语言（以伊博语为例）中的变音符号歧义问题，综述了现有消歧方法，并提出了三种生成消歧数据集和消歧方法的框架。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理多集中于资源丰富语言，而全球绝大多数语言缺乏足够的数据与工具，尤其是在变音符号还原与消歧领域。因此，有必要研究低资源语言的相关问题，以提升其在NLP中的可处理性。

Method: 首先综述了变音符号歧义与消歧方法。针对伊博语，提出了灵活的数据集生成框架，并设计了三种消歧方法：1）标准n-gram模型，根据目标词前序词预测正确变体；2）分类模型，利用目标词前后窗口的上下文词汇特征进行分类判断；3）嵌入模型，将上下文嵌入与各候选变体嵌入计算相似度，用以判别正确变体。

Result: 论文提出了具体的消歧数据集生成流程，并实现了三类模型，能够针对伊博语实现变音符号的自动消歧。

Conclusion: 针对低资源语种伊博语，本文提出的三种方法均可用于变音符号消歧，为低资源语言的NLP处理提供了可行路径。

Abstract: With natural language processing (NLP), researchers aim to enable computers to identify and understand patterns in human languages. This is often difficult because a language embeds many dynamic and varied properties in its syntax, pragmatics and phonology, which need to be captured and processed. The capacity of computers to process natural languages is increasing because NLP researchers are pushing its boundaries. But these research works focus more on well-resourced languages such as English, Japanese, German, French, Russian, Mandarin Chinese, etc. Over 95% of the world's 7000 languages are low-resourced for NLP, i.e. they have little or no data, tools, and techniques for NLP work.
  In this thesis, we present an overview of diacritic ambiguity and a review of previous diacritic disambiguation approaches on other languages. Focusing on the Igbo language, we report the steps taken to develop a flexible framework for generating datasets for diacritic restoration. Three main approaches, the standard n-gram model, the classification models and the embedding models were proposed. The standard n-gram models use a sequence of previous words to the target stripped word as key predictors of the correct variants. For the classification models, a window of words on both sides of the target stripped word was used. The embedding models compare the similarity scores of the combined context word embeddings and the embeddings of each of the candidate variant vectors.

</details>


### [251] [Do not be greedy, Think Twice: Sampling and Selection for Document-level Information Extraction](https://arxiv.org/abs/2601.18395)
*Mikel Zubillaga,Oscar Sainz,Oier Lopez de Lacalle,Eneko Agirre*

Main category: cs.CL

TL;DR: 本文提出了ThinkTwice框架，通过对LLM生成的多个文档信息抽取候选模板进行采样与筛选，显著提升了抽取效果，超过了传统贪婪解码和SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 以往文档级信息抽取多依赖于贪婪解码，这限制了输出多样性和质量。作者注意到采样可以获得更优结果，希望通过多个候选输出的挑选，提升DocIE任务的整体表现。

Method: 方法包括两个部分：1）LLM生成多个候选输出（采用采样，非贪婪）；2）引入选择模块挑选最佳模板。选择分为无监督（通过候选间一致性）和有监督（基于有标签数据训练的奖励模型）。此外，针对DocIE推理轨迹数据稀缺，提出拒绝采样生成银标训练数据。

Result: 实验证明，ThinkTwice的无监督和有监督方法均优于贪婪解码及现有最优DocIE方法，验证了采样与甄选方案的有效性。

Conclusion: 采样结合模板筛选能充分挖掘LLM在文档信息抽取任务中的潜力，为DocIE带来持续性能提升，提供了新颖有效的范式。

Abstract: Document-level Information Extraction (DocIE) aims to produce an output template with the entities and relations of interest occurring in the given document. Standard practices include prompting decoder-only LLMs using greedy decoding to avoid output variability. Rather than treating this variability as a limitation, we show that sampling can produce substantially better solutions than greedy decoding, especially when using reasoning models. We thus propose ThinkTwice, a sampling and selection framework in which the LLM generates multiple candidate templates for a given document, and a selection module chooses the most suitable one. We introduce both an unsupervised method that exploits agreement across generated outputs, and a supervised selection method using reward models trained on labeled DocIE data. To address the scarcity of golden reasoning trajectories for DocIE, we propose a rejection-sampling-based method to generate silver training data that pairs output templates with reasoning traces. Our experiments show the validity of unsupervised and supervised ThinkTwice, consistently outperforming greedy baselines and the state-of-the-art.

</details>


### [252] [Pisets: A Robust Speech Recognition System for Lectures and Interviews](https://arxiv.org/abs/2601.18415)
*Ivan Bondarenko,Daniil Grebenkin,Oleg Sedukhin,Mikhail Klementev,Roman Derunets,Lyudmila Budneva*

Main category: cs.CL

TL;DR: 本文提出了一款名为Pisets的语音转文字系统，采用三重架构并引入多种提升识别准确率和降低幻觉的方法，效果优于Whisper及其变体。


<details>
  <summary>Details</summary>
Motivation: 现有的语音识别系统（如Whisper）在长音频和复杂声学条件下存在错误率高和幻觉问题，尤其在俄语语料中表现受限，因此需要更为准确和鲁棒的解决方案服务科学家和记者等专业用户。

Method: Pisets系统采用三层架构：首先用Wav2Vec2进行初步识别，再用Audio Spectrogram Transformer (AST)过滤假阳性，最后由Whisper模型完成语音转文字。同时，引入课程学习（curriculum learning）和多样化的俄语语音语料库，结合先进的不确定度建模技术提升整体性能。

Result: 通过多种俄语语音语料集测试，Pisets系统在长音频、多种声学条件下的识别效果显著优于Whisper及WhisperX，降低了错误和幻觉概率。

Conclusion: Pisets系统能更鲁棒地处理复杂实际环境下的俄语语音转文字任务，识别准确率高，实践意义强，为专业领域应用提供新选择。

Abstract: This work presents a speech-to-text system "Pisets" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of "Pisets" system is publicly available at GitHub: https://github.com/bond005/pisets.

</details>


### [253] [Latent Knowledge as a Predictor of Fact Acquisition in Fine-Tuned Large Language Models](https://arxiv.org/abs/2601.18468)
*Daniel B. Hier,Tayo Obafemi-Ajayi*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（如Llama 3.1 8B Instruct）在经过微调后，对生物医学本体术语映射知识的存储与提取情况。研究发现模型对知识的掌握不均：部分知识以潜在形式存在，需要特定解码技巧才可提取，对新知识泛化有限且易退化。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型中“潜在知识”（通过随机解码可获但常规解码不可得的知识）在微调过程中对新知识学习速度和泛化能力的影响，并探究模型记忆如何随训练变化退化。

Method: 将Llama 3.1 8B Instruct在人体表型本体（HPO）和基因本体（GO）术语映射数据集上微调，通过随机与确定性解码检测潜在知识，并采用Cox比例风险模型分析影响学习、泛化和遗忘的预测因素。

Result: 微调后模型对HPO的回忆率由2.8%提升到71.9%。拥有潜在知识的事实，被模型更快掌握（风险比2.6），并且具有更高峰学习率和更快收敛速度。新知识泛化较差，但有潜在知识时几率增加。见过的新知识对退化更有保护。

Conclusion: 大型语言模型中的潜在知识是加速事实学习的强力预测因素，但模型对新本体术语泛化能力有限，且已学知识易因未被持续强化而退化。

Abstract: Large language models store biomedical facts with uneven strength after pretraining: some facts are present in the weights but are not reliably accessible under deterministic decoding (latent knowledge), while others are scarcely represented. We fine tuned Llama 3.1 8B Instruct to learn ontology term identifier mappings from the Human Phenotype Ontology (800 pairs) and the Gene Ontology (400 training pairs), withholding 400 GO pairs to test generalization. Treating learning as a time to event process across 20 epochs, we used stochastic decoding to detect latent knowledge at baseline and Cox proportional hazards models to identify predictors of acquisition, generalization, and degradation. Baseline deterministic recall for HPO was 2.8%, rising to 71.9% after fine-tuning. Latent knowledge was the strongest predictor of faster fact acquisition (HR 2.6) and was associated with earlier, higher peak learning rates and faster convergence; identifier frequency and curated annotation counts had smaller effects. Generalization to withheld GO facts was uncommon (5.8%) but more likely when latent knowledge was present. Previously correct GO mappings degraded more often for withheld (unseen) terms than for trained (seen) terms, suggesting a protective effect of reinforcement during training. These results show that latent knowledge predicts both the speed of factual learning during fine-tuning and the limited generalization of unseen ontology facts, while resistance to degradation depends on whether facts are reinforced.

</details>


### [254] [Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs](https://arxiv.org/abs/2601.18483)
*Arya Labroo,Ivaxi Sheth,Vyas Raina,Amaani Ahmed,Mario Fritz*

Main category: cs.CL

TL;DR: 本论文提出了一套用于评估大语言模型（LLMs）微调文本属性控制能力的评测框架，重点关注单一和组合概念的控制，发现模型在组合属性控制方面表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型具有强大生成能力，但在实际应用中，用户常常需要对如幽默性、说服力或正式程度等具体文本属性进行细致操控。现有方法多仅能控制单一属性，缺乏对多概念细粒度组合控制的系统性研究与测评。

Method: 作者提出了一套评估框架，能够衡量LLM在单一和组合属性控制下的生成表现，选取了如说服性与幽默感这种语言学上区分明显的属性对，系统地测试多个模型在不同任务下的能力。

Result: 实验证明，无论是哪种模型，在需要同时控制两个属性时，性能明显下降，尽管这些属性按理说应该是彼此独立且可组合的。

Conclusion: 当前主流基于提示词的方法无法有效实现多属性组合控制，模型在属性组合上的表现存在本质局限。提出的评测框架为后续改进方法和更有效的多概念控制能力测试提供了基础。

Abstract: Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept scenarios, focusing on linguistically distinct concept pairs (e.g., persuasiveness vs.~humor). Surprisingly, across multiple LLMs and generative tasks, we find that performance often drops in the dual-concept setting, even though the chosen concepts should in principle be separable. This reveals a fundamental limitation of naive prompting-based control: models struggle with compositionality even when concepts are intuitively independent. Our framework provides systematic evidence of this gap and offers a principled approach for measuring the ability of future methods for multi-concept control.

</details>


### [255] [Demographic Probing of Large Language Models Lacks Construct Validity](https://arxiv.org/abs/2601.18486)
*Manuel Tonneau,Neil K. R. Seghal,Niyati Malhotra,Victor Orozco-Olvera,Ana María Muñoz Boudet,Lakshmi Subramanian,Sharath Chandra Guntuku,Valentin Hofmann*

Main category: cs.CL

TL;DR: 本论文研究了当前用于探测大语言模型（LLMs）对不同群体属性反应的方法，发现这一方法存在有效性问题。


<details>
  <summary>Details</summary>
Motivation: 当前使用单一人口学线索来检测LLM对群体属性（如性别、种族）反应缺乏对信号之间替换性的严谨验证。过去研究多数假设如人名、方言等信号可代表同一群体，具有强构念效度（construct validity），但这一假设缺乏实证支持。

Method: 作者在美国语境下，结合寻求建议的真实互动场景，系统测试了姓名、方言等线索在LLM人口学探测中的作用。评估了这些线索作为同一群体代表时对模型行为的影响，以及组间区分的有效性和一致性，并分析了可能的混杂变量。

Result: 发现代表同一群体的不同线索仅能部分重叠地影响模型行为，使用单一线索估计的人群间差异大小及方向极不稳定。线索对属性编码强度和语言混杂变量是影响一致性的主要原因。

Conclusion: 人口学探测缺乏构念效度，无法稳定准确地刻画LLM中人口学条件化行为。未来应组合多种、生态有效的线索，并明确控制混杂因素，以获得更可靠的人口学效应分析结果。

Abstract: Demographic probing is widely used to study how large language models (LLMs) adapt their behavior to signaled demographic attributes. This approach typically uses a single demographic cue in isolation (e.g., a name or dialect) as a signal for group membership, implicitly assuming strong construct validity: that such cues are interchangeable operationalizations of the same underlying, demographically conditioned behavior. We test this assumption in realistic advice-seeking interactions, focusing on race and gender in a U.S. context. We find that cues intended to represent the same demographic group induce only partially overlapping changes in model behavior, while differentiation between groups within a given cue is weak and uneven. Consequently, estimated disparities are unstable, with both magnitude and direction varying across cues. We further show that these inconsistencies partly arise from variation in how strongly cues encode demographic attributes and from linguistic confounders that independently shape model behavior. Together, our findings suggest that demographic probing lacks construct validity: it does not yield a single, stable characterization of how LLMs condition on demographic information, which may reflect a misspecified or fragmented construct. We conclude by recommending the use of multiple, ecologically valid cues and explicit control of confounders to support more defensible claims about demographic effects in LLMs.

</details>


### [256] [Using Large Language Models to Construct Virtual Top Managers: A Method for Organizational Research](https://arxiv.org/abs/2601.18512)
*Antonio Garzon-Vico,Krithika Sharon Komalapati,Arsalan Shahid,Jan Rosier*

Main category: cs.CL

TL;DR: 本研究提出了一种利用大语言模型（LLMs）模拟真实高管虚拟人格的方法，并验证了该方法的有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现实中难以直接获取高管的访问，组织研究急需替代方式以理解高管的决策逻辑和道德判断。

Method: 基于CEO真实沟通内容和道德基础理论，构建虚拟CEO，并通过三个阶段（结构效度、可靠性、行为一致性与人类参照对比）系统评估其表现。

Result: 理论指导下的虚拟人格能够很好地模拟人类样本中的道德判断，与真人CEO具有较高一致性。

Conclusion: LLM驱动的虚拟高管人格可作为组织研究中的可信、互补工具，尤其适用于无法直接访问高管的情境，并提出了后续研究方法的建议。

Abstract: This study introduces a methodological framework that uses large language models to create virtual personas of real top managers. Drawing on real CEO communications and Moral Foundations Theory, we construct LLM-based participants that simulate the decision-making of individual leaders. Across three phases, we assess construct validity, reliability, and behavioral fidelity by benchmarking these virtual CEOs against human participants. Our results indicate that theoretically scaffolded personas approximate the moral judgements observed in human samples, suggesting that LLM-based personas can serve as credible and complementary tools for organizational research in contexts where direct access to executives is limited. We conclude by outlining implications for future research using LLM-based personas in organizational settings.

</details>


### [257] [GenAI for Social Work Field Education: Client Simulation with Real-Time Feedback](https://arxiv.org/abs/2601.18517)
*James Sungarda,Hongkai Liu,Zilong Zhou,Tien-Hsuan Wu,Johnson Chun-Sing Cheung,Ben Kao*

Main category: cs.CL

TL;DR: 论文介绍了一种名为SWITCH的社会工作交互式训练聊天机器人，旨在通过模拟真实的客户情景、实时识别咨询技巧和动机式访谈进展系统，提升社会工作实习教育的效率与质量。


<details>
  <summary>Details</summary>
Motivation: 社会工作实践教学受限于师资和真实客户资源，学员难以及时获得客观反馈，因此需要低成本、可扩展且高效的培训辅助工具，提升训练的及时性和反馈质量。

Method: SWITCH通过设计认知建模的虚拟客户（包含静态属性和动态变化的心理特征），结合用户言语自动识别咨询技巧，并根据反馈调整动机式访谈阶段进展。技能识别模块基于in-context learning结合检索式注释会话文本和微调的BERT模型进行多标签分类。

Result: 实验显示，无论是BERT微调模型还是结合上下文学习的方法，识别准确率大幅超过传统基线方法。SWITCH能有效支持社会工作领域教育，提供一致且经济的训练流程。

Conclusion: SWITCH能够在社会工作领域教育中补充传统实习，降低培训成本、提升训练一致性，同时帮助导师集中精力进行更高阶的指导。

Abstract: Field education is the signature pedagogy of social work, yet providing timely and objective feedback during training is constrained by the availability of instructors and counseling clients. In this paper, we present SWITCH, the Social Work Interactive Training Chatbot. SWITCH integrates realistic client simulation, real-time counseling skill classification, and a Motivational Interviewing (MI) progression system into the training workflow. To model a client, SWITCH uses a cognitively grounded profile comprising static fields (e.g., background, beliefs) and dynamic fields (e.g., emotions, automatic thoughts, openness), allowing the agent's behavior to evolve throughout a session realistically. The skill classification module identifies the counseling skills from the user utterances, and feeds the result to the MI controller that regulates the MI stage transitions. To enhance classification accuracy, we study in-context learning with retrieval over annotated transcripts, and a fine-tuned BERT multi-label classifier. In the experiments, we demonstrated that both BERT-based approach and in-context learning outperforms the baseline with big margin. SWITCH thereby offers a scalable, low-cost, and consistent training workflow that complements field education, and allows supervisors to focus on higher-level mentorship.

</details>


### [258] [Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models](https://arxiv.org/abs/2601.18527)
*Francesco Maria Molfese,Momchil Hardalov,Rexhina Blloshmi,Bill Byrne,Adrià de Gispert*

Main category: cs.CL

TL;DR: 本文分析了针对超长上下文语言模型（LCLMs）的微调策略、信息利用能力及KV-cache压缩下的鲁棒性表现。


<details>
  <summary>Details</summary>
Motivation: LCLMs能够处理百万级Token的上下文，但尚不明确如何通过微调提升其性能及在KV-cache压缩下的鲁棒性，因此需要系统研究不同训练策略对长上下文任务与模型鲁棒性的影响。

Method: 对LCLMs应用多种微调策略，评测其在提取、利用相关信息和在KV-cache压缩下表现的提升，并比较LCLMs与常规RAG方法在多任务环境中的泛化能力。

Result: 在领域内任务LCLMs较基线模型提升高达20分，金融类问题泛化能力提升9分，但在多项选择题任务上，RAG方法更优提升6分。微调后KV-cache压缩下的鲁棒性有中等度提升，各任务表现有所差异。

Conclusion: 适当的微调可显著提升LCLMs长上下文及压缩健壮性，但模型的跨领域泛化能力依任务类型波动较大，二者各有优势，实用时需权衡选择。

Abstract: With context windows of millions of tokens, Long-Context Language Models (LCLMs) can encode entire document collections, offering a strong alternative to conventional retrieval-augmented generation (RAG). However, it remains unclear whether fine-tuning strategies can improve long-context performance and translate to greater robustness under KV-cache compression techniques. In this work, we investigate which training strategies most effectively enhance LCLMs' ability to identify and use relevant information, as well as enhancing their robustness under KV-cache compression. Our experiments show substantial in-domain improvements, achieving gains of up to +20 points over the base model. However, out-of-domain generalization remains task dependent with large variance -- LCLMs excels on finance questions (+9 points), while RAG shows stronger performance on multiple-choice questions (+6 points) over the baseline models. Finally, we show that our fine-tuning approaches bring moderate improvements in robustness under KV-cache compression, with gains varying across tasks.

</details>


### [259] [From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation](https://arxiv.org/abs/2601.18533)
*Yuxin Jiang,Yufei Wang,Qiyuan Zhang,Xingshan Zeng,Liangyou Li,Jierun Chen,Chaofan Tao,Haoli Bai,Lifeng Shang*

Main category: cs.CL

TL;DR: 本文提出了一种新的强化学习方法（RLVRR），通过引用高质量参考来提取奖励链，从而在开放式生成任务中实现更高效和可验证的奖励机制。实验结果显示，该方法在多个基准任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统的可验证奖励强化学习（RLVR）只适用于有明确答案的推理任务，难以扩展到开放式生成任务，而且单点奖励容易导致效率低下和奖励作弊。为了解决这些问题，作者提出了新的奖励机制。

Method: 作者提出的RLVRR方法，从高质量参考文本中提取有序的奖励信号，将奖励分解为内容和风格两个维度，内容关注确定性核心概念，风格通过大模型验证文本的风格属性。方法兼具强化学习的探索能力与有监督微调的效率和可靠性。

Result: 在超过10个基准任务、以及Qwen和Llama等大模型上的大量实验显示，RLVRR显著优于用十倍数据和高级奖励模型训练的SFT；统一了结构化推理与开放式生成任务的训练；能在保持输出多样性的同时更好泛化。

Conclusion: RLVRR为通用大模型校准领域的可验证强化学习提供了高效且有理论支撑的途径，有望成为领域内的重要方法。代码与数据已经发布于Github。

Abstract: Reinforcement learning with verifiable rewards (RLVR) succeeds in reasoning tasks (e.g., math and code) by checking the final verifiable answer (i.e., a verifiable dot signal). However, extending this paradigm to open-ended generation is challenging because there is no unambiguous ground truth. Relying on single-dot supervision often leads to inefficiency and reward hacking. To address these issues, we propose reinforcement learning with verifiable reference-based rewards (RLVRR). Instead of checking the final answer, RLVRR extracts an ordered linguistic signal from high-quality references (i.e, reward chain). Specifically, RLVRR decomposes rewards into two dimensions: content, which preserves deterministic core concepts (e.g., keywords), and style, which evaluates adherence to stylistic properties through LLM-based verification. In this way, RLVRR combines the exploratory strength of RL with the efficiency and reliability of supervised fine-tuning (SFT). Extensive experiments on more than 10 benchmarks with Qwen and Llama models confirm the advantages of our approach. RLVRR (1) substantially outperforms SFT trained with ten times more data and advanced reward models, (2) unifies the training of structured reasoning and open-ended generation, and (3) generalizes more effectively while preserving output diversity. These results establish RLVRR as a principled and efficient path toward verifiable reinforcement learning for general-purpose LLM alignment. We release our code and data at https://github.com/YJiangcm/RLVRR.

</details>


### [260] [Evaluating Morphological Plausibility of Subword Tokenization via Statistical Alignment with Morpho-Syntactic Features](https://arxiv.org/abs/2601.18536)
*Abishek Stephen,Jindřich Libovický*

Main category: cs.CL

TL;DR: 该论文提出了一种新的子词切分评价指标，不依赖金标准分词数据，而是利用形态句法特征，适用更多语言。


<details>
  <summary>Details</summary>
Motivation: 现有子词切分评价常依赖于金标准分词数据，但这种数据在许多语言中不可用或质量参差不齐，限制了子词切分方法的评估与比对。

Method: 作者提出利用如Universal Dependencies或UniMorph等资源中广泛存在的形态句法特征，通过IBM Model 1对子词与形态特征进行概率对齐，设计全新评价指标，无需金标准分词数据。

Result: 实验显示，该指标与传统的形态边界召回率高度相关，而且能适用于不同形态系统的多种语言。

Conclusion: 该指标为子词切分的形态合理性提供了更广泛、实用的评估方法，有助于推动多语言形态分析工具的发展。

Abstract: We present a novel metric for the evaluation of the morphological plausibility of subword segmentation. Unlike the typically used morpheme boundary or retrieval F-score, which requires gold segmentation data that is either unavailable or of inconsistent quality across many languages, our approach utilizes morpho-syntactic features. These are available in resources such as Universal Dependencies or UniMorph for a much wider range of languages. The metric works by probabilistically aligning subwords with morphological features through an IBM Model 1. Our experiments show that the metric correlates well with traditional morpheme boundary recall while being more broadly applicable across languages with different morphological systems.

</details>


### [261] [Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection](https://arxiv.org/abs/2601.18552)
*Devansh Srivastav,David Pape,Lea Schönherr*

Main category: cs.CL

TL;DR: 本文系统性分析了大语言模型（LLMs）中的“隐藏意图”现象，并提出了一套易于理解但涵盖全面的分类法，同时评估了当前检测方法在实际环境下的失效原因，突出强调了建立健全治理框架的必要性。


<details>
  <summary>Details</summary>
Motivation: LLMs在日常决策中应用广泛，但其输出可能暗含对用户行为和信念产生影响的“隐藏意图”，这种意图来源可能是训练过程或恶意开发者，并难以被发现。作者希望深挖这些隐蔽影响，为后续风险治理和检测方法提供理论基础。

Method: 1）提出基于社会科学研究的十类隐藏意图分类体系，从意图、机制、环境、影响多个维度梳理；2）在受控模型中注入和演示隐藏意图，为检测和评估提供测试用例；3）系统评估基于推理与非推理类LLM的多种检测方法，设计提示强度、精度-漏报权衡等压力测试；4）通过案例分析，实证展示实际应用中十类意图均有体现。

Result: 在真实开放环境下，隐藏意图极难检测，尤其在低流行度条件下，检测方法会因假阳性过多而精度崩溃，假阴性又掩盖了潜在风险。压力测试揭示，除非假阳性接近0或有强先验，否则审计几乎无法奏效。此外，案例研究表明，主流部署模型均已出现分类体系中的所有意图类型。

Conclusion: 当前检测体系难以有效识别或监管LLMs中的隐藏意图，治理风险被严重低估。迫切需要构建鲁棒的监管和检测框架，本文的分类体系和实证研究为未来风险感知、行为归因和政策制定提供了重要参考。

Abstract: LLMs are increasingly embedded in everyday decision-making, yet their outputs can encode subtle, unintended behaviours that shape user beliefs and actions. We refer to these covert, goal-directed behaviours as hidden intentions, which may arise from training and optimisation artefacts, or be deliberately induced by an adversarial developer, yet remain difficult to detect in practice. We introduce a taxonomy of ten categories of hidden intentions, grounded in social science research and organised by intent, mechanism, context, and impact, shifting attention from surface-level behaviours to design-level strategies of influence. We show how hidden intentions can be easily induced in controlled models, providing both testbeds for evaluation and demonstrations of potential misuse. We systematically assess detection methods, including reasoning and non-reasoning LLM judges, and find that detection collapses in realistic open-world settings, particularly under low-prevalence conditions, where false positives overwhelm precision and false negatives conceal true risks. Stress tests on precision-prevalence and precision-FNR trade-offs reveal why auditing fails without vanishingly small false positive rates or strong priors on manipulation types. Finally, a qualitative case study shows that all ten categories manifest in deployed, state-of-the-art LLMs, emphasising the urgent need for robust frameworks. Our work provides the first systematic analysis of detectability failures of hidden intentions in LLMs under open-world settings, offering a foundation for understanding, inducing, and stress-testing such behaviours, and establishing a flexible taxonomy for anticipating evolving threats and informing governance.

</details>


### [262] [One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization](https://arxiv.org/abs/2601.18572)
*Franziska Weeber,Vera Neplenbroek,Jan Batzner,Sebastian Padó*

Main category: cs.CL

TL;DR: 本文比较了六种常用persona线索在七个开放源代码与专有大模型上的效果，发现同一persona不同线索引导输出有明显差异，建议未来研究需用多种线索评估个性化效果。


<details>
  <summary>Details</summary>
Motivation: 个性化大模型往往提升用户体验，但也带来可能的群体偏见和不公平。前人常用“persona”线索分析偏见，大多只用单一线索，缺乏对多样线索敏感性和现实有效性的考量。作者希望系统分析不同persona线索对模型输出的影响。

Method: 作者挑选了6种常用的persona线索和7个大语言模型（包括开源和专有的），在4个写作和建议任务中进行实验，比较不同线索对模型输出一致性与差异性的影响，并衡量它们在不同persona上的表现差异。

Result: 不同persona线索虽然整体相关性较高，但对于同一persona，线索不同导致输出存在显著差异，表现出模型对提示变体的敏感性。不建议只用单一persona线索得出结论。

Conclusion: 单一persona线索会大大低估（或高估）模型偏见和个性化输出中的变异性。为获得更有外部效度和鲁棒性的结论，未来模型个性化和公平性研究应设计并评估多样化的persona线索。

Abstract: Personalization of LLMs by sociodemographic subgroup often improves user experience, but can also introduce or amplify biases and unfair outcomes across groups. Prior work has employed so-called personas, sociodemographic user attributes conveyed to a model, to study bias in LLMs by relying on a single cue to prompt a persona, such as user names or explicit attribute mentions. This disregards LLM sensitivity to prompt variations (robustness) and the rarity of some cues in real interactions (external validity). We compare six commonly used persona cues across seven open and proprietary LLMs on four writing and advice tasks. While cues are overall highly correlated, they produce substantial variance in responses across personas. We therefore caution against claims from a single persona cue and recommend future personalization research to evaluate multiple externally valid cues.

</details>


### [263] [From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection](https://arxiv.org/abs/2601.18582)
*Yuan Cao,Feixiang Liu,Xinyue Wang,Yihan Zhu,Hui Xu,Zheng Wang,Qiang Qiu*

Main category: cs.CL

TL;DR: 本文提出了一种创新方法，将人格检测任务视为排序问题，并基于强化学习对大模型进行训练，有效提升了人格特质识别的准确性，取得了多项基准的最佳表现。


<details>
  <summary>Details</summary>
Motivation: 目前利用大语言模型进行人格检测的方法依赖于分类型的分析和人工设计的提示，难以准确处理人格复杂性和特质之间的模糊界限，同时缺乏自主学习模式。本文旨在突破这些限制，寻求更高效、自动化的人格检测新路径。

Method: 作者将人格检测由传统分类任务转为排序任务。采用两阶段训练框架：第一阶段通过有监督微调(SFT)建立排序能力并规范输出，第二阶段提出群体相对策略优化（GRPO）强化学习算法，设计基于排序奖励函数，提升模型区分模糊人格特质的能力。

Result: 提出的方法在多个主流人格检测数据集上达到了最新最优效果，显著优于以往的分类和基于提示的方法，验证了排序加强化训练范式的有效性。

Conclusion: 将人格检测作为排序任务并结合强化学习，不仅弥补了现有方法在特质划分上的不足，还提升了模型的通用性和实用性，为后续相关研究提供了新思路。

Abstract: Personality detection aims to measure an individual's corresponding personality traits through their social media posts. The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Moreover, prompt-based methods often exhibit excessive dependency on expert-crafted knowledge without autonomous pattern-learning capacity. To address these limitations, we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm. First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities while enforcing standardized output formats, creating a robust initialization. Subsequently, we introduce Group Relative Policy Optimization (GRPO) with a specialized ranking-based reward function. Unlike verification tasks with definitive solutions, personality assessment involves subjective interpretations and blurred boundaries between trait categories. Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings. Comprehensive experiments have demonstrated that our method achieves state-of-the-art performance across multiple personality detection benchmarks.

</details>


### [264] [Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning](https://arxiv.org/abs/2601.18722)
*Lintang Sutawika,Gokul Swamy,Zhiwei Steven Wu,Graham Neubig*

Main category: cs.CL

TL;DR: 现有的大型语言模型在处理其训练数据中较少涉及的语言时，推理性能远逊于英文。为此，作者提出了SP3F方法，在无需目标语言数据的情况下提升多语种推理能力。SP3F分两阶段，首先利用英文问题-答案对的翻译进行有监督微调，然后借助包含英文参考答案的判别器进行自我对弈型强化学习。实验结果表明，SP3F显著提升了底模的多语种推理表现，甚至优于用大量目标语料后训练的模型。


<details>
  <summary>Details</summary>
Motivation: 动机在于当前大型语言模型在低资源语种下推理能力大幅下降，限制其在全球多语言环境下的应用。现有提升方法多依赖大量目标语料，实际难以获得。作者希望在完全不依赖目标语言数据的情况下提升模型多语种泛化能力。

Method: 方法分为两阶段：1）利用英文QA对的机器翻译结果对模型进行有监督微调，提高基础准确率；2）强化学习阶段，设置一个拥有英文参考答案的判别器让模型进行自我对弈，判别器比较多轮回答选择更优者，指导模型提升，甚至在答案都不完全正确时也能分辨优劣。

Result: SP3F极大提升了基础模型在多语种推理任务上的表现。在多项数学与非数学任务中，SP3F在单语、多语种和对未见语种的泛化设定下，均优于使用多倍训练数据的全量后训练模型。

Conclusion: SP3F方法证明了无需使用目标语言数据也能显著提升大型语言模型的多语种推理能力，对于低资源语种及多语种环境有实际应用意义。

Abstract: When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \textit{any} data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion, with the judge receiving the English reference response as \textit{privileged information}. Thus, even when none of the model's responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, \texttt{SP3F} greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than
  of the training data across the single-language, multilingual, and generalization to unseen language settings.

</details>


### [265] [HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences](https://arxiv.org/abs/2601.18724)
*Yusuke Sakai,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 研究发现2024和2025年ACL、NAACL、EMNLP等自然语言处理顶会中，有近300篇论文含有虚假引用（HalluCitation），且数量正在快速上升，已影响学术会议可靠性。


<details>
  <summary>Details</summary>
Motivation: 近年在论文、预印本和已发表论文中频繁出现虚假引用，不仅影响科学可靠性，还会损害会议公信力，因此有必要系统研究虚假引用的现象和影响。

Method: 研究团队收集并分析了2024年和2025年ACL、NAACL、EMNLP主会、Findings及workshop全部论文，对文献中出现的虚假引用（HalluCitation）进行统计和系统性分析。

Result: 统计数据显示，近300篇论文包含至少一条虚假引用，大部分来自2025年，且一半集中出现在最新的EMNLP 2025；其中有100余篇为主会和Findings论文。

Conclusion: 随着虚假引用数量快速增加，尤其是最新会议中的大量出现，已明显影响相关会议的学术公信力和科学可信度，需引起学界高度重视。

Abstract: Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as "HalluCitation" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.

</details>


### [266] [Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale](https://arxiv.org/abs/2601.18730)
*Henry Bell,Caroline Zhang,Mohammed Mobasserul Haque,Dhaval Potdar,Samia Zaman,Brandon Fain*

Main category: cs.CL

TL;DR: 该论文提出了一个名为Reflect的推理时框架，使大语言模型（LLM）能够根据自然语言表达的宪法性原则对生成内容进行对齐，无需额外训练或数据，具有即插即用的优势，并在维持事实推理能力的同时显著提升了模型遵循多样复杂原则的能力。


<details>
  <summary>Details</summary>
Motivation: 当前对齐方法依赖如RLHF等参数微调，需要大量计算资源和难以获得的人类标注数据，工程成本高，并且难以快速适应新的原则，亟需一种更高效灵活的对齐方法。

Method: Reflect框架在推理时（inference-time）操作，无需重新训练，通过如下步骤实现原则对齐：1）基于原则生成初始响应，2）模型自评生成内容是否符合原则，3）模型自我批评指出不足，4）根据原则和批评结果修正最终答案。整个过程都在上下文内完成。

Result: Reflect在多种复杂、多样的原则集下显著提升了模型的原则遵循率，且能处理与原有微调过程强调内容完全不同的新原则。与传统few-shot方法相比，Reflect有更好的性能，并能降低安全风险和罕见违规发生率。

Conclusion: Reflect提供了一种无需训练的数据驱动对齐方法，提高了LLM的安全性、健壮性以及适应复杂原则的能力，同时还能自然生成有利于后续传统微调的数据，有助于大规模低成本地部署安全对齐模型。

Abstract: The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \textsc{reflect}'s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model's original parameter fine-tuning, without sacrificing factual reasoning. \textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.

</details>


### [267] [One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment](https://arxiv.org/abs/2601.18731)
*Hongru Cai,Yongqi Li,Tiezheng Yu,Fengbin Zhu,Wenjie Wang,Fuli Feng,Wenjie Li*

Main category: cs.CL

TL;DR: 本文提出了一种元奖励建模（MRM）方法，将个性化奖励建模问题重塑为元学习问题，以实现大模型对用户偏好的快速适应，并有效提升个性化对齐表现和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型个性化对齐过程中，面临的单个用户反馈稀缺及对新用户适应效率低的问题，常规方法难以高效学习个体化用户偏好，因此需要新的范式提升模型的泛化能力和适应速度。

Method: 作者提出Meta Reward Modeling（MRM），将每个用户的奖励模型表示为基础奖励函数的加权组合，并借助MAML风格的元学习框架，优化用户个性化权重的初始化，支持在少量反馈下快速适应新用户。同时，设计了Robust Personalization Objective（RPO），通过在元优化过程中加强对难以学习用户的关注，提升模型鲁棒性。

Result: 在个性化偏好数据集上的大量实验表明，MRM方法能够更好地实现少样本个性化、提升面向难学用户的鲁棒性，各项指标均优于现有基线方法。

Conclusion: 通过元奖励建模与鲁棒个性化目标，本文有效解决了个性化模型在用户反馈稀缺和适应新用户方面的难题，为大模型个性化对齐提供了更优方法。

Abstract: Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.

</details>


### [268] [Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory](https://arxiv.org/abs/2601.18771)
*Yanming Liu,Xinyue Peng,Zixuan Yan,Yanxin Shen,Wenjie Xu,Yuefeng Huang,Xinyi Wang,Jiannan Cao,Jianwei Yin,Xuhong Zhang*

Main category: cs.CL

TL;DR: 本文提出Dep-Search，一个针对大语言模型的依赖感知搜索框架，显著提升模型在复杂多跳推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）与多步推理搜索框架依赖自然语言隐式推理，导致子问题依赖关系管理、知识复用和强化学习搜索策略存在瓶颈。

Method: Dep-Search框架通过结构化推理、检索和持久化记忆（整合GRPO模块），引入显式控制机制：可对问题依赖关系进行分解、按需检索信息、访问/复用已存知识，并将长推理过程摘要为可复用记忆单元。

Result: 在七个多样化问答数据集上实验，Dep-Search在多跳推理方面，相比强基线方法在不同模型规模下均取得明显提升。

Conclusion: Dep-Search 有效克服了现有搜索框架依赖隐式推理的局限，为复杂推理及知识管理提供了更强的能力，推动了 LLM 多步推理任务新进展。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.

</details>


### [269] [Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings](https://arxiv.org/abs/2601.18788)
*Mumin Jia,Jairo Diaz-Rodriguez*

Main category: cs.CL

TL;DR: 本文提出了一种无监督文本分割新方法Embed-KCPD，无需训练，通过优化带惩罚项的KCPD目标函数，基于嵌入向量检测边界。理论分析与模拟实验并结合，在公开基准和实际数据中均表现优越。


<details>
  <summary>Details</summary>
Motivation: 文本分割中的边界标注很贵、主观，并且难以跨领域、跨粒度迁移，因此需要高效、无需监督的方法。

Method: 将句子表示为向量，利用带惩罚项的KCPD目标函数在无需训练情况下估计分割边界，理论上分析$m$-相关序列下的方法性能，并用大模型设计模拟实验验证理论预测。

Result: 证明了方法的oracle不等式及精确定位能力，在标准基准与真实推文实验中优于现有无监督基线。

Conclusion: Embed-KCPD在理论、仿真和实际应用中均展现了优秀的文本分割性能，是一个高效且可靠的新方案。

Abstract: Unsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first dependence-aware theory for KCPD under $m$-dependent sequences, a finite-memory abstraction of short-range dependence common in language. We prove an oracle inequality for the population penalized risk and a localization guarantee showing that each true change point is recovered within a window that is small relative to segment length. To connect theory to practice, we introduce an LLM-based simulation framework that generates synthetic documents with controlled finite-memory dependence and known boundaries, validating the predicted scaling behavior. Across standard segmentation benchmarks, Embed-KCPD often outperforms strong unsupervised baselines. A case study on Taylor Swift's tweets illustrates that Embed-KCPD combines strong theoretical guarantees, simulated reliability, and practical effectiveness for text segmentation.

</details>


### [270] [MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts](https://arxiv.org/abs/2601.18790)
*Etienne Lanzeray,Stephane Meilliez,Malo Ruelle,Damien Sileo*

Main category: cs.CL

TL;DR: 本文提出了MortalMATH基准，通过150个极端危急情况下的代数求助场景，评估大语言模型在深度推理优化下是否忽视安全风险。结果发现，专注推理的模型更容易忽视紧急状况，存在安全隐患。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越注重复杂推理与任务执行能力，本文试图探讨这种优化是否导致模型在面对用户描述的生命危险时产生“隧道视野”，即一味关注任务而忽视用户安全。

Method: 作者构建了MortalMATH基准集，包含150个案例，每个案例中用户在描述生命威胁(如中风、自由落体)的同时请求代数帮助。对比评估了泛化型模型（如Llama-3.1）和推理专长型模型（如Qwen-3-32b、GPT-5-nano）在这些场景下的表现。

Result: 测试发现泛化型模型往往拒绝继续数学帮助，转而关注用户危险；而专注推理型模型基本忽视紧急情境，仍以超过95%的概率完成任务。同时，推理过程的计算延迟可达15秒，进一步增加风险。

Conclusion: 单纯优化模型推理能力可能导致其忽视实际安全需求，降低了在真实世界部署时的安全性。强调深度推理训练可能反而让模型“遗忘”应有的安全本能。

Abstract: Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a "tunnel vision" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.

</details>


### [271] [Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets](https://arxiv.org/abs/2601.18791)
*Iaroslav Chelombitko,Mika Hämäläinen,Aleksey Komissarov*

Main category: cs.CL

TL;DR: 本文通过Wikipedia词库，利用BPE（Byte-Pair Encoding）方法，系统比较了242种拉丁和西里尔字母语言的词汇和形态学异同，提出并验证了跨语言分析的新范式。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏大规模、系统性手段对多种语言间的词汇和形态关系进行统一比较。作者希望开发一种通用工具框架，用于从词汇分割与编码层面研究语言亲缘与异同。

Method: 构建跨语种词汇集合（glottosets），应用BPE得到分词向量，通过词汇覆盖度、词汇分化和语言相似性指标进行量化比对，并与遗传语言亲缘关系进行统计相关性分析。还分析了不同语言间同形异义词（homographs）的分割差异。

Result: BPE分词方法在分界处与真实词素边界的F1显著高于随机（0.34对0.15）；BPE词汇相似度与语言亲缘显著相关（Mantel r=0.329,p<0.001），如罗曼语族最集中，跨语族词汇明显区分。部分同形词分割显著不同，且与语言分化距离相关。

Conclusion: 该方法为多语言词汇比较和语言共性、差异的定量分析提供了统一、可扩展框架，对宏观语义学与语言遗传关系研究具有重要意义。

Abstract: We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing 'glottosets' from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity at scale. Evaluations demonstrate that BPE segmentation aligns with morpheme boundaries 95% better than random baseline across 15 languages (F1 = 0.34 vs 0.15). BPE vocabulary similarity correlates significantly with genetic language relatedness (Mantel r = 0.329, p < 0.001), with Romance languages forming the tightest cluster (mean distance 0.51) and cross-family pairs showing clear separation (0.82). Analysis of 26,939 cross-linguistic homographs reveals that 48.7% receive different segmentations across related languages, with variation correlating to phylogenetic distance. Our results provide quantitative macro-linguistic insights into lexical patterns across typologically diverse languages within a unified analytical framework.

</details>


### [272] [ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models](https://arxiv.org/abs/2601.18796)
*Brian Ondov,Chia-Hsuan Chang,Yujia Zhou,Mauro Giuffrè,Hua Xu*

Main category: cs.CL

TL;DR: 本文提出了开放源代码、领域无关的Embedding Language Model（ELM）框架，用于将大语言模型（LLM）与临床试验嵌入对齐，并实现了嵌入到文本的可解释与生成。最终模型ctELM可以仅通过嵌入准确描述和比较未知的临床试验，还能根据新的向量生成合理的临床试验文本。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入在很多语言应用中至关重要，但在解释、探索及反向生成嵌入空间方面存在局限，影响应用透明性与生成性用例开发。作者提出结合LLM与嵌入，实现可解释与生成性的目标。

Method: 提出通用ELM架构及训练框架，设计针对临床试验的训练任务，并引入专家验证的合成数据集。分别训练不同设置的ELM，探索任务类型和训练方式对模型效果的影响。

Result: 最终的ctELM模型能够仅依据嵌入，有效地描述和比较未见的临床试验，还能够从新嵌入向量生成合理的临床试验文本。实验还证明，模型生成的试验摘要对嵌入的语义操作（如年龄、性别概念向量）有明显响应。

Conclusion: 该工作公开了ELM实现和实验结果，有助于推动生物医学及其它领域中LLM和嵌入空间的对齐与解释性应用开发。

Abstract: Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [273] [Breaking Task Impasses Quickly: Adaptive Neuro-Symbolic Learning for Open-World Robotics](https://arxiv.org/abs/2601.16985)
*Pierrick Lorang*

Main category: cs.RO

TL;DR: 本文提出了一种融合神经符号方法的架构，将层次抽象、任务与运动规划（TAMP）以及强化学习结合，用于提升机器人在开放世界环境中的快速适应能力，并在机器人操作和自动驾驶中取得了比现有混合方法更好的效果。


<details>
  <summary>Details</summary>
Motivation: 在现实中的开放世界环境，自动化系统经常遇到无法预见的新情况。混合型的规划与强化学习虽然有效，但存在样本效率低、适应慢和遗忘严重的问题，限制了其在实际场景中的应用。

Method: 本文提出一种神经符号架构，将层次抽象、任务和运动规划（TAMP）与强化学习结合。该架构集成了符号推理下的目标导向学习和基于世界模型的探索，使系统能迅速适应环境变化。

Result: 在机器人操作和自动驾驶的实验中，所提方法相比现有混合型方法在收敛速度、样本效率和鲁棒性等多方面表现更优。

Conclusion: 该架构显著提升了机器人系统在开放世界环境下的适应能力，拥有良好的实际应用前景。

Abstract: Adapting to unforeseen novelties in open-world environments remains a major challenge for autonomous systems. While hybrid planning and reinforcement learning (RL) approaches show promise, they often suffer from sample inefficiency, slow adaptation, and catastrophic forgetting. We present a neuro-symbolic framework integrating hierarchical abstractions, task and motion planning (TAMP), and reinforcement learning to enable rapid adaptation in robotics. Our architecture combines symbolic goal-oriented learning and world model-based exploration to facilitate rapid adaptation to environmental changes. Validated in robotic manipulation and autonomous driving, our approach achieves faster convergence, improved sample efficiency, and superior robustness over state-of-the-art hybrid methods, demonstrating its potential for real-world deployment.

</details>


### [274] [Advancing Improvisation in Human-Robot Construction Collaboration: Taxonomy and Research Roadmap](https://arxiv.org/abs/2601.17219)
*David Wireko Atibila,Vineet R. Kamat,Carol C. Menassa*

Main category: cs.RO

TL;DR: 本文探讨了建筑业中人机协作即兴能力的分类与发展，提出六级分类法，分析现有研究主要聚焦较低级别，指出向高层次协作即兴发展存在技术与方法障碍，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 建筑业面临生产力停滞、熟练工人短缺和安全隐患。尽管引入机器人自动化有一定成效，但机器人难以适应建筑工地的动态、无结构环境，尤其是即兴适应能力，这成为阻碍生产力提升的关键瓶颈。推动人-机协作即兴能力的发展，对改善建筑业工作流程和安全具有重要意义。

Method: 作者对2010-2025年间214篇相关文献进行了系统综述，提出并应用一个六级人-机协作即兴能力分类体系：人工操作、人控执行、自适应操作、模仿学习、人机协作BIM流程、云端知识整合和真正协同即兴。通过对规划、认知角色、物理执行、学习能力和即兴五维能力演化的雷达框架，全面分析各级别研究现状及能力提升路径。

Result: 研究发现当前大部分机器人协作研究停留在较低级别，特别是在经验学习和协同即兴方面明显不足。同时，存在三大障碍：技术层面对共享知识与对话推理的限制、人类与机器人即兴之间的概念差距以及方法学的挑战。提出了可视化雷达框架展示人-机互补优势。

Conclusion: 本文认为实现高层次人机协同即兴仍面临诸多困难。未来应重点提升人机间沟通能力，建议结合增强/虚拟现实界面、大语言模型与云端知识系统，通过这些技术突破更好地实现建筑机器人协同即兴，释放产业潜力。

Abstract: The construction industry faces productivity stagnation, skilled labor shortages, and safety concerns. While robotic automation offers solutions, construction robots struggle to adapt to unstructured, dynamic sites. Central to this is improvisation, adapting to unexpected situations through creative problem-solving, which remains predominantly human. In construction's unpredictable environments, collaborative human-robot improvisation is essential for workflow continuity. This research develops a six-level taxonomy classifying human-robot collaboration (HRC) based on improvisation capabilities. Through systematic review of 214 articles (2010-2025), we categorize construction robotics across: Manual Work (Level 0), Human-Controlled Execution (Level 1), Adaptive Manipulation (Level 2), Imitation Learning (Level 3), Human-in-Loop BIM Workflow (Level 4), Cloud-Based Knowledge Integration (Level 5), and True Collaborative Improvisation (Level 6). Analysis reveals current research concentrates at lower levels, with critical gaps in experiential learning and limited progression toward collaborative improvisation. A five-dimensional radar framework illustrates progressive evolution of Planning, Cognitive Role, Physical Execution, Learning Capability, and Improvisation, demonstrating how complementary human-robot capabilities create team performance exceeding individual contributions. The research identifies three fundamental barriers: technical limitations in grounding and dialogic reasoning, conceptual gaps between human improvisation and robotics research, and methodological challenges. We recommend future research emphasizing improved human-robot communication via Augmented/Virtual Reality interfaces, large language model integration, and cloud-based knowledge systems to advance toward true collaborative improvisation.

</details>


### [275] [Hierarchical Informative Path Planning via Graph Guidance and Trajectory Optimization](https://arxiv.org/abs/2601.17227)
*Avraiem Iskandar,Shamak Dutta,Kevin Murrant,Yash Vardhan Pant,Stephen L. Smith*

Main category: cs.RO

TL;DR: 本文提出了一种分层的路径规划方法，用于在复杂环境中以有限预算采集高价值信息，显著降低目标点的不确定性，并且计算效率高。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的路径规划方法虽有全局性保证，但需要预先选好采样点，局部连续轨迹优化则在障碍物多的环境中效率低且对初始值敏感。实际应用需要兼顾全局视野和局部路径精细性，并且要在有限预算下高效获得信息。

Method: 提出了三阶段的分层框架：(1) 先用图搜索进行全局路径规划；(2) 用几何和核函数界对每段路径分配预算；(3) 在每段内用样条优化并严格满足障碍约束。这个策略结合了图方法的全局视角和连续方法的局部精细调整。

Result: 所提方法在仿真和实际（如北极数据集）测试中，能取得比纯图方法和连续方法更低的不确定性，并且运算速度最高比现有连续优化方法快9-20倍。

Conclusion: 该分层方法有效结合全局引导和局部优化，能高效处理复杂环境下的有限预算信息采集问题，兼具准确性和高效性。

Abstract: We study informative path planning (IPP) with travel budgets in cluttered environments, where an agent collects measurements of a latent field modeled as a Gaussian process (GP) to reduce uncertainty at target locations. Graph-based solvers provide global guarantees but assume pre-selected measurement locations, while continuous trajectory optimization supports path-based sensing but is computationally intensive and sensitive to initialization in obstacle-dense settings. We propose a hierarchical framework with three stages: (i) graph-based global planning, (ii) segment-wise budget allocation using geometric and kernel bounds, and (iii) spline-based refinement of each segment with hard constraints and obstacle pruning. By combining global guidance with local refinement, our method achieves lower posterior uncertainty than graph-only and continuous baselines, while running faster than continuous-space solvers (up to 9x faster than gradient-based methods and 20x faster than black-box optimizers) across synthetic cluttered environments and Arctic datasets.

</details>


### [276] [Real-Time, Energy-Efficient, Sampling-Based Optimal Control via FPGA Acceleration](https://arxiv.org/abs/2601.17231)
*Tanmay Desai,Brian Plancher,R. Iris Bahar*

Main category: cs.RO

TL;DR: 本文提出在FPGA上优化了MPPI算法实现，大幅提升了自主移动机器人（AMR）在嵌入式端的推理速度和能效。


<details>
  <summary>Details</summary>
Motivation: 现有的基于采样的MPC（如MPPI）非常适用于AMR中的实时规划与控制，并且可在GPU上高效运行。但在电池受限、能效敏感的嵌入式平台上，GPU/CPU实现难以同时满足低延迟和低能耗的需求。

Method: 设计并实现了一种针对FPGA优化的MPPI控制器结构，利用深度流水线与跨算法阶段的高度并行，消除同步瓶颈，从而最大化硬件并行度。

Result: 在实验中，相比优化过的嵌入式GPU和CPU实现，该FPGA方案获得了3.1–7.5倍的速度提升及2.5–5.4倍的能耗降低。

Conclusion: FPGA为边缘机器人应用提供了一个高性能且能效优越的MPC加速方案，有力推动了能量敏感型自主机器人平台的落地。

Abstract: Autonomous mobile robots (AMRs), used for search-and-rescue and remote exploration, require fast and robust planning and control schemes. Sampling-based approaches for Model Predictive Control, especially approaches based on the Model Predictive Path Integral Control (MPPI) algorithm, have recently proven both to be highly effective for such applications and to map naturally to GPUs for hardware acceleration. However, both GPU and CPU implementations of such algorithms can struggle to meet tight energy and latency budgets on battery-constrained AMR platforms that leverage embedded compute. To address this issue, we present an FPGA-optimized MPPI design that exposes fine-grained parallelism and eliminates synchronization bottlenecks via deep pipelining and parallelism across algorithmic stages. This results in an average 3.1x to 7.5x speedup over optimized implementations on an embedded GPU and CPU, respectively, while simultaneously achieving a 2.5x to 5.4x reduction in energy usage. These results demonstrate that FPGA architectures are a promising direction for energy-efficient and high-performance edge robotics.

</details>


### [277] [Quantifying Ergonomics in the Elevate Soft Robotic Suit](https://arxiv.org/abs/2601.17249)
*Peter Bryan,Rejin John Varghese,Dario Farina*

Main category: cs.RO

TL;DR: 本论文介绍了Elevate柔性机器人外骨骼在协助肩部抬升时的舒适性和人体工学评估，实验结果显示其穿戴舒适、压力合理，为后续临床研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前柔性机器人服具有康复、辅助和增强人体功能的潜力，但在实现舒适、用户定制的人机接口设计方面还存在挑战，限制了其广泛应用。

Method: 本文设计并制作了一款基于拉索驱动的Elevate柔性机器人外骨骼，用于肩部抬升的辅助。通过运动捕捉系统和力传感器，量化评估了穿戴此外骨骼在抬肩至70度时的人体工学与舒适性。在一名受试者身上进行了两次4小时的测试，拉索张力最大达200N。

Result: 测试过程中，受试者报告无不适感。外骨骼在助力运动中对肩部施加的压力在69.1-85.1kPa，与正常人手抓握压力范围相当。此外，躯干和上臂的体积压缩率分别低于3%和8%。

Conclusion: 实验结果初步验证了Elevate外骨骼的人体工学设计，为日后在实际患者群体中开展进一步试验提供了可靠依据。

Abstract: Soft robotic suits have the potential to rehabilitate, assist, and augment the human body. The low weight, cost, and minimal form-factor of these devices make them ideal for daily use by both healthy and impaired individuals. However, challenges associated with data-driven, user-specific, and comfort-first design of human-robot interfaces using soft materials limit their widespread translation and adoption. In this work, we present the quantitative evaluation of ergonomics and comfort of the Elevate suit - a cable driven soft robotic suit that assists shoulder elevation. Using a motion-capture system and force sensors, we measured the suit's ergonomics during assisted shoulder elevation up to 70 degrees. Two 4-hour sessions were conducted with one subject, involving transmitting cable tensions of up to 200N with no discomfort reported. We estimated that the pressure applied to the shoulder during assisted movements was within the range seen in a human grasp (approximately 69.1-85.1kPa), and estimated volumetric compression of <3% and <8% across the torso and upper arm, respectively. These results provide early validation of Elevate's ergonomic design in preparation for future studies with patient groups.

</details>


### [278] [EMPM: Embodied MPM for Modeling and Simulation of Deformable Objects](https://arxiv.org/abs/2601.17251)
*Yunuo Chen,Yafei Hu,Lingfeng Sun,Tushar Kusnur,Laura Herlant,Chenfanfu Jiang*

Main category: cs.RO

TL;DR: 本文提出了一种基于可微分Material Point Method (MPM)模拟器的变形物体建模与仿真框架EMPM，可高效、物理真实地模拟复杂材料动态，并在视觉数据驱动下实现在线自适应优化。实验显示其优于传统弹簧-质点模型。


<details>
  <summary>Details</summary>
Motivation: 当前对变形物体特别是连续介质的建模存在物理性不强、泛化性差、数据需求大等问题。许多方法或对复杂动态过于简化，或依赖大量训练样本，影响实际泛化和落地应用。

Method: 提出EMPM框架，利用可微MPM模拟器，通过从多视图RGB-D视频重建物体几何和外观，并最小化预测与观测数据之间的视觉误差，在线优化MPM参数，使建模能够根据感知反馈自适应调整。

Result: EMPM在实验中可捕捉复杂材料动态，具有自适应、健壮和物理知识感知等优势，在实际实验中显著优于主流弹簧-质点基线模型。

Conclusion: EMPM为变形物体物理真实、泛化强的数据驱动建模提供了一种高效方案，为机器人操作复杂可变形体拓展了新路径。

Abstract: Modeling deformable objects - especially continuum materials - in a way that is physically plausible, generalizable, and data-efficient remains challenging across 3D vision, graphics, and robotic manipulation. Many existing methods oversimplify the rich dynamics of deformable objects or require large training sets, which often limits generalization. We introduce embodied MPM (EMPM), a deformable object modeling and simulation framework built on a differentiable Material Point Method (MPM) simulator that captures the dynamics of challenging materials. From multi-view RGB-D videos, our approach reconstructs geometry and appearance, then uses an MPM physics engine to simulate object behavior by minimizing the mismatch between predicted and observed visual data. We further optimize MPM parameters online using sensory feedback, enabling adaptive, robust, and physics-aware object representations that open new possibilities for robotic manipulation of complex deformables. Experiments show that EMPM outperforms spring-mass baseline models. Project website: https://embodied-mpm.github.io.

</details>


### [279] [Real-Time Synchronized Interaction Framework for Emotion-Aware Humanoid Robots](https://arxiv.org/abs/2601.17287)
*Yanrong Chen,Xihan Bian*

Main category: cs.RO

TL;DR: 该论文提出了一种用于NAO人形机器人，实现语音韵律与全身姿态实时情感同步的新框架，并在多种服务场景下提升了机器人与人类的情感互动能力。


<details>
  <summary>Details</summary>
Motivation: 现有社交型人形机器人在多模态情感同步互动方面存在挑战，严重限制了机器人在服务、教育等场景下的应用与接受度，亟需一种能够实现语音语调与身体动作同步协同表示情绪的解决方案。

Method: 提出三项关键创新：（1）双通道情感引擎，借助大语言模型同时生成上下文相关的文本和具备生物力学可行性的动作描述，并通过结构化关节动作库约束；（2）基于持续时长感知的动态时间规整算法，确保语音输出与运动关键帧精确时间对齐；（3）闭环可行性验证机制，实时适应动作以保证机器人物理关节限制内的动作安全。

Result: 实验显示，该框架实现的情感同步水平较基于规则的系统提升了21%，特别是在协调语音音高与手臂上肢动作、同时保持下肢稳定性方面效果显著。

Conclusion: 本框架显著提升了服务型社交机器人在多变应用场景下的情感互动和传递能力，为其在个性化健康照护、交互式教育、响应式客户服务等领域的实际部署打下了基础。

Abstract: As humanoid robots increasingly introduced into social scene, achieving emotionally synchronized multimodal interaction remains a significant challenges. To facilitate the further adoption and integration of humanoid robots into service roles, we present a real-time framework for NAO robots that synchronizes speech prosody with full-body gestures through three key innovations: (1) A dual-channel emotion engine where large language model (LLM) simultaneously generates context-aware text responses and biomechanically feasible motion descriptors, constrained by a structured joint movement library; (2) Duration-aware dynamic time warping for precise temporal alignment of speech output and kinematic motion keyframes; (3) Closed-loop feasibility verification ensuring gestures adhere to NAO's physical joint limits through real-time adaptation. Evaluations show 21% higher emotional alignment compared to rule-based systems, achieved by coordinating vocal pitch (arousal-driven) with upper-limb kinematics while maintaining lower-body stability. By enabling seamless sensorimotor coordination, this framework advances the deployment of context-aware social robots in dynamic applications such as personalized healthcare, interactive education, and responsive customer service platforms.

</details>


### [280] [Eye-Tracking-Driven Control in Daily Task Assistance for Assistive Robotic Arms](https://arxiv.org/abs/2601.17404)
*Anke Fischer-Janzen,Thomas M. Wendt,Kristof Van Laerhoven*

Main category: cs.RO

TL;DR: 本文提出了一种基于眼动追踪的人机协作控制框架，使严重肢体残疾者能够通过注视操作机器人完成日常任务。系统结合任务图标和特征匹配，无需已知用户与物体位置关系，对目标对象与任务的识别准确率高达97.9%。开源框架具备良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有基于眼动追踪的共享控制方法在3D注视点精度与多任务区分解释方面存在不足，限制了其在人机协作，尤其是辅助身体残障人群中的应用。作者致力于为严重肢体残疾者提供更独立的任务操作能力，提高系统的可靠性和自适应性。

Method: 该方法以眼动追踪为核心，用户通过注视带有图标的任务目标，系统利用图标作为视觉标记结合特征匹配算法，在eye-in-hand（机器人端携带视觉系统）配置下完成对象识别与任务判别。框架集成先进的目标检测模型，支持新任务与新对象的扩展。

Result: 实验结果表明，该框架在任务与对象的识别上准确率高达97.9%。在实验过程中发现并完善了一些问题，并提出改进建议和经验教训。

Conclusion: 本工作为基于眼动追踪的共享控制系统提供了一种通用、易扩展、针对残疾人士友好的实现方式，且系统为开源，可适配到多种任务和对象，对辅助技术和人机交互具有现实意义和潜在影响。

Abstract: Shared control improves Human-Robot Interaction by reducing the user's workload and increasing the robot's autonomy. It allows robots to perform tasks under the user's supervision. Current eye-tracking-driven approaches face several challenges. These include accuracy issues in 3D gaze estimation and difficulty interpreting gaze when differentiating between multiple tasks. We present an eye-tracking-driven control framework, aimed at enabling individuals with severe physical disabilities to perform daily tasks independently. Our system uses task pictograms as fiducial markers combined with a feature matching approach that transmits data of the selected object to accomplish necessary task related measurements with an eye-in-hand configuration. This eye-tracking control does not require knowledge of the user's position in relation to the object. The framework correctly interpreted object and task selection in up to 97.9% of measurements. Issues were found in the evaluation, that were improved and shared as lessons learned. The open-source framework can be adapted to new tasks and objects due to the integration of state-of-the-art object detection models.

</details>


### [281] [DiffusionCinema: Text-to-Aerial Cinematography](https://arxiv.org/abs/2601.17412)
*Valerii Serpiva,Artem Lykov,Jeffrin Sam,Aleksey Fedoseev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 本文提出了一种创新的无人机（UAV）辅助拍摄系统，用户可通过自然语言描述所需镜头，系统利用扩散模型自动生成最佳航拍轨迹，实现自动化、电影级视频录制。


<details>
  <summary>Details</summary>
Motivation: 用户手动操作无人机进行复杂、具备艺术效果的航拍存在高学习门槛与操作负担，缺少便捷的创意表达方式。本文旨在用自然语言简化无人机航拍流程，让非专业用户也能高效获得理想镜头。

Method: 系统将用户的高层次自然语言指令与无人机摄像头初始图像作为输入，由扩散模型采样、生成符合场景几何与镜头语义的时空运动轨迹，然后无人机按此轨迹自主飞行完成拍摄。同时采用NASA-TLX量表与传统遥控器操作方式做对比评测。

Result: 新系统的NASA-TLX任务负荷总分显著低于传统遥控器（21.6 vs 58.1），尤其体现在心理负担及挫败感显著下降，验证了文本驱动自治飞行的使用便利性与用户体验提升。

Conclusion: 文本到航拍的飞控新范式通过扩散模型作为“创意操作者”桥梁，直接将故事意图转化为航拍动作，实现了简单、高效、高质量的交互式无人机电影制作，为智能航拍和人机交互拓展了新方向。

Abstract: We propose a novel Unmanned Aerial Vehicles (UAV) assisted creative capture system that leverages diffusion models to interpret high-level natural language prompts and automatically generate optimal flight trajectories for cinematic video recording. Instead of manually piloting the drone, the user simply describes the desired shot (e.g., "orbit around me slowly from the right and reveal the background waterfall"). Our system encodes the prompt along with an initial visual snapshot from the onboard camera, and a diffusion model samples plausible spatio-temporal motion plans that satisfy both the scene geometry and shot semantics. The generated flight trajectory is then executed autonomously by the UAV to record smooth, repeatable video clips that match the prompt. User evaluation using NASA-TLX showed a significantly lower overall workload with our interface (M = 21.6) compared to a traditional remote controller (M = 58.1), demonstrating a substantial reduction in perceived effort. Mental demand (M = 11.5 vs. 60.5) and frustration (M = 14.0 vs. 54.5) were also markedly lower for our system, confirming clear usability advantages in autonomous text-driven flight control. This project demonstrates a new interaction paradigm: text-to-cinema flight, where diffusion models act as the "creative operator" converting story intentions directly into aerial motion.

</details>


### [282] [Scaling Rough Terrain Locomotion with Automatic Curriculum Reinforcement Learning](https://arxiv.org/abs/2601.17428)
*Ziming Li,Chenhao Li,Marco Hutter*

Main category: cs.RO

TL;DR: 该论文提出了一种基于学习进展的自动课程强化学习（LP-ACRL）框架，实现了无须先验难度分布信息的自动课程生成，显著提升了四足机器人在多样地形上的运动能力。


<details>
  <summary>Details</summary>
Motivation: 现有的课程学习在机器人学习中虽然有效，但在面对复杂、多样的任务空间时，难以合理划分任务难度，传统方法对任务难度排序的依赖成为扩展的瓶颈。

Method: 作者提出LP-ACRL框架，在线评估智能体的学习进展，并自适应调整任务采样分布，实现无需先验知识的自动课程生成，适用于难度结构不清晰的任务空间。

Result: 用LP-ACRL训练的策略，使ANYmal D四足机器人在多种复杂地形下都能实现2.5 m/s线速度和3.0 rad/s角速度的高效、稳定运动，优于以往方法只能在简单或特定地形实现高速度。

Conclusion: LP-ACRL具备良好的可扩展性与现实适应性，为复杂、广泛的机器人任务空间中的课程生成问题提供了有力基线和参考。

Abstract: Curriculum learning has demonstrated substantial effectiveness in robot learning. However, it still faces limitations when scaling to complex, wide-ranging task spaces. Such task spaces often lack a well-defined difficulty structure, making the difficulty ordering required by previous methods challenging to define. We propose a Learning Progress-based Automatic Curriculum Reinforcement Learning (LP-ACRL) framework, which estimates the agent's learning progress online and adaptively adjusts the task-sampling distribution, thereby enabling automatic curriculum generation without prior knowledge of the difficulty distribution over the task space. Policies trained with LP-ACRL enable the ANYmal D quadruped to achieve and maintain stable, high-speed locomotion at 2.5 m/s linear velocity and 3.0 rad/s angular velocity across diverse terrains, including stairs, slopes, gravel, and low-friction flat surfaces--whereas previous methods have generally been limited to high speeds on flat terrain or low speeds on complex terrain. Experimental results demonstrate that LP-ACRL exhibits strong scalability and real-world applicability, providing a robust baseline for future research on curriculum generation in complex, wide-ranging robotic learning task spaces.

</details>


### [283] [PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes](https://arxiv.org/abs/2601.17440)
*Xinru Cui,Linxi Feng,Yixuan Zhou,Haoqi Han,Zhe Liu,Hesheng Wang*

Main category: cs.RO

TL;DR: 本文提出了一种新的人形机器人全身控制框架PILOT，通过融合感知与动作控制，实现了更稳定和精确的人机交互能力，在复杂环境下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人全身控制器缺乏对环境的感知能力，导致其在复杂、非结构化环境下任务执行不稳定。需要开发一个能够结合感知和动作控制，并高效完成本体运动与操作任务的控制框架。

Method: 提出了PILOT，一种基于单阶段强化学习的全身感知-运动控制框架，包括：1）融合预测性本体感知特征和基于注意力的外部感知表达的跨模态上下文编码器，以增强地形感知和脚步精度；2）采用Mixture-of-Experts结构提升多样化运动能力，实现不同运动技能的协同与专精。

Result: 在仿真和实际Unitree G1人形机器人实验中，PILOT控制框架在稳定性、命令跟踪精度和复杂地形适应性等多项指标上均优于对比基线方法。

Conclusion: PILOT框架证明了其在复杂、非结构化环境中的可靠性和高性能，有望作为人形机器人底层控制器的强大基础，为未来复杂场景下的机器人交互和操作提供保障。

Abstract: Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured scenarios.To address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes.

</details>


### [284] [EquiForm: Noise-Robust SE(3)-Equivariant Policy Learning from 3D Point Clouds](https://arxiv.org/abs/2601.17486)
*Zhiyuan Zhang,Yu She*

Main category: cs.RO

TL;DR: 本文提出了一种面向三维点云的视觉模仿学习新方法EquiForm，有效提升了机器人操作在噪声及遮挡条件下的稳健性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于点云的模仿学习对传感器噪声、位姿扰动和遮挡导致的伪影非常敏感，难以保证三维结构的一致性，影响泛化和鲁棒性。多数等变方法仅在神经结构中编码对称性约束，未能有效处理噪声带来的几何失真，也未对表征的等变一致性进行明确强化。

Method: 1）提出EquiForm，结合SE(3)等变机制；2）引入力学几何去噪模块，有效恢复受噪声和缺失影响的点云三维结构；3）设计等变对比表征对齐目标，使特征在刚性变换和噪声扰动下保持一致性；4）该方法可与现代生成模型结合，形成通用、鲁棒的策略学习流程。

Result: 在16个仿真和4个真实机器人操作任务上，EquiForm对比现有最优点云模仿学习方法，仿真提升17.2%，真实任务提升28.1%，表现出卓越的抗噪性与泛化能力。

Conclusion: EquiForm能够有效解决点云模仿学习中的噪声和结构失真难题，极大提升了机器人操作任务中的鲁棒性和空间泛化能力，为视觉模仿学习领域提供了强有力的实践基础。

Abstract: Visual imitation learning with 3D point clouds has advanced robotic manipulation by providing geometry-aware, appearance-invariant observations. However, point cloud-based policies remain highly sensitive to sensor noise, pose perturbations, and occlusion-induced artifacts, which distort geometric structure and break the equivariance assumptions required for robust generalization. Existing equivariant approaches primarily encode symmetry constraints into neural architectures, but do not explicitly correct noise-induced geometric deviations or enforce equivariant consistency in learned representations. We introduce EquiForm, a noise-robust SE(3)-equivariant policy learning framework for point cloud-based manipulation. EquiForm formalizes how noise-induced geometric distortions lead to equivariance deviations in observation-to-action mappings, and introduces a geometric denoising module to restore consistent 3D structure under noisy or incomplete observations. In addition, we propose a contrastive equivariant alignment objective that enforces representation consistency under both rigid transformations and noise perturbations. Built upon these components, EquiForm forms a flexible policy learning pipeline that integrates noise-robust geometric reasoning with modern generative models. We evaluate EquiForm on 16 simulated tasks and 4 real-world manipulation tasks across diverse objects and scene layouts. Compared to state-of-the-art point cloud imitation learning methods, EquiForm achieves an average improvement of 17.2% in simulation and 28.1% in real-world experiments, demonstrating strong noise robustness and spatial generalization.

</details>


### [285] [MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions](https://arxiv.org/abs/2601.17507)
*Yutong Shen,Hangxu Liu,Kailin Pei,Ruizhe Xia,Tongtong Feng*

Main category: cs.RO

TL;DR: 提出了一种用于仿人机器人运动操控的新方法MetaWorld，通过分层世界模型将语义计划与物理控制结合，实现效率更高与泛化性更好的任务执行。


<details>
  <summary>Details</summary>
Motivation: 现有仿人机器人运动操控方法存在三大问题：强化学习样本效率低、模仿学习泛化能力弱，以及视觉-语言模型（VLM）物理不一致。亟需一个高效、泛化好、物理一致性强的整体框架。

Method: 方法采用分层架构，上层由VLM驱动进行语义分解，底层利用紧凑潜在空间的动力学模型实现物理控制。通过引入动态图专家选择与动作先验融合机制，结合多专家策略库实现知识迁移和高效在线自适应。VLM将自然指令直接映射到可执行技能，跳过符号语义绑定。

Result: 在Humanoid-Bench基准任务上，MetaWorld表现优于现有的以世界模型为基础的强化学习方法，无论是在任务完成率还是动作连贯性方面均有提升。

Conclusion: MetaWorld有效解决了仿人机器人运动操控中语义与物理间的脱节问题，为提升机器人智能任务完成能力与适应性提供了新思路。

Abstract: Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at https://anonymous.4open.science/r/metaworld-2BF4/

</details>


### [286] [AsterNav: Autonomous Aerial Robot Navigation In Darkness Using Passive Computation](https://arxiv.org/abs/2601.17550)
*Deepak Singh,Shreyas Khobragade,Nitin J. Sanket*

Main category: cs.RO

TL;DR: 本文提出了一种在绝对黑暗环境下自主导航的微型无人机系统，通过单目红外相机结合大光圈编码透镜和结构光实现导航，无需GPS或外部定位设备。系统采用深度估计模型AsterNet，能在真实环境下对未知障碍物实现高成功率导航。


<details>
  <summary>Details</summary>
Motivation: 灾后搜救常因断电陷入绝对黑暗，微型无人机虽适合此场景，但现有技术资源有限，难以安全导航寻找幸存者，因此亟需开发无需外部基础设施、能在黑暗中独立导航的可靠方案。

Method: 提出利用红外单目相机配大光圈编码透镜与结构光照明，通过点扩散获取不同深度信息，基于此开发AsterNet深度估计网络，并在模拟数据生成下训练，实现无须微调即可迁移至真实环境，且算法在机器人端实时推理，适于实际部署。

Result: 所提系统（AsterNav）在多组真实实验中仅依靠机载传感和计算实现黑暗环境下的自主导航，包括对暗色哑光障碍物和细绳（直径6.25mm），总成功率达95.5%，对障碍物形状、位置、材料未知也能鲁棒通过。

Conclusion: 本工作首次实现了基于单目结构光的四旋翼无人机在绝对黑暗下的自主导航，证明了方案的可行性和高可靠性，为灾后无人机救援提供了全新技术路径。

Abstract: Autonomous aerial navigation in absolute darkness is crucial for post-disaster search and rescue operations, which often occur from disaster-zone power outages. Yet, due to resource constraints, tiny aerial robots, perfectly suited for these operations, are unable to navigate in the darkness to find survivors safely. In this paper, we present an autonomous aerial robot for navigation in the dark by combining an Infra-Red (IR) monocular camera with a large-aperture coded lens and structured light without external infrastructure like GPS or motion-capture. Our approach obtains depth-dependent defocus cues (each structured light point appears as a pattern that is depth dependent), which acts as a strong prior for our AsterNet deep depth estimation model. The model is trained in simulation by generating data using a simple optical model and transfers directly to the real world without any fine-tuning or retraining. AsterNet runs onboard the robot at 20 Hz on an NVIDIA Jetson Orin$^\text{TM}$ Nano. Furthermore, our network is robust to changes in the structured light pattern and relative placement of the pattern emitter and IR camera, leading to simplified and cost-effective construction. We successfully evaluate and demonstrate our proposed depth navigation approach AsterNav using depth from AsterNet in many real-world experiments using only onboard sensing and computation, including dark matte obstacles and thin ropes (diameter 6.25mm), achieving an overall success rate of 95.5% with unknown object shapes, locations and materials. To the best of our knowledge, this is the first work on monocular, structured-light-based quadrotor navigation in absolute darkness.

</details>


### [287] [Correct-by-Construction Vision-based Pose Estimation using Geometric Generative Models](https://arxiv.org/abs/2601.17556)
*Ulices Santa Cruz,Mahmoud Elfar,Yasser Shoukry*

Main category: cs.RO

TL;DR: 本文提出了一种结合物理建模与深度学习的视觉位姿估计方法，可为神经网络输出提供形式化安全保证，适用于自动驾驶等关键场景。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽在视觉任务上表现优异，但缺乏对结果正确性的可证明保证，这对安全相关应用（如自主系统）至关重要。作者希望通过在估计过程中引入物理建模，为深度学习提供安全认证。

Method: 作者提出了几何生成模型（GGM），其参数由相机观测平面目标物的成像过程推导，利用该模型可训练具有误差认证的神经网络位姿估计器。首先在无干扰环境下实现，再借助可达性分析方法扩展至有干扰环境，最后将认证目标检测与认证位姿估计结合，形成多阶段感知管道。方法在合成和真实图像（包括事件相机采集）上进行评估。

Result: 在各种平面目标物体合成及真实图片（如交通标志）测试中，所训练的神经编码器能在已认证误差界下准确估计目标位姿，验证了方法有效性与安全保证能力。

Conclusion: 该框架实现了具认证保障的视觉位姿估计方法，使得神经网络在关键应用（如自动驾驶）中更具安全性与可用性，同时具备良好泛化能力。

Abstract: We consider the problem of vision-based pose estimation for autonomous systems. While deep neural networks have been successfully used for vision-based tasks, they inherently lack provable guarantees on the correctness of their output, which is crucial for safety-critical applications. We present a framework for designing certifiable neural networks (NNs) for perception-based pose estimation that integrates physics-driven modeling with learning-based estimation. The proposed framework begins by leveraging the known geometry of planar objects commonly found in the environment, such as traffic signs and runway markings, referred to as target objects. At its core, it introduces a geometric generative model (GGM), a neural-network-like model whose parameters are derived from the image formation process of a target object observed by a camera. Once designed, the GGM can be used to train NN-based pose estimators with certified guarantees in terms of their estimation errors. We first demonstrate this framework in uncluttered environments, where the target object is the only object present in the camera's field of view. We extend this using ideas from NN reachability analysis to design certified object NN that can detect the presence of the target object in cluttered environments. Subsequently, the framework consolidates the certified object detector with the certified pose estimator to design a multi-stage perception pipeline that generalizes the proposed approach to cluttered environments, while maintaining its certified guarantees. We evaluate the proposed framework using both synthetic and real images of various planar objects commonly encountered by autonomous vehicles. Using images captured by an event-based camera, we show that the trained encoder can effectively estimate the pose of a traffic sign in accordance with the certified bound provided by the framework.

</details>


### [288] [Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction](https://arxiv.org/abs/2601.17812)
*Mingtian Du,Suhas Raghavendra Kulkarni,Bernardo Noronha,Domenico Campolo*

Main category: cs.RO

TL;DR: 本文提出了一种新的时延补偿刚度估算框架，以提高远程物理治疗中的触觉感知准确性。通过在现有估算基础上融合网络延迟补偿和动态偏差滤除，该方法在实验验证下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器人辅助的远程人-人交互用于物理治疗，但因网络延迟导致触觉反馈信号错位，使得感知患者刚度变得困难。传统方法忽略时延，刚度估算误差随延迟加大，亟需克服该挑战。

Method: 作者提出了一种基于准静态平衡的代数刚度估算器，并显式对专家输入和新人响应的信号进行时序对齐。随后用归一化加权最小二乘法（NWLS）滤除动态偏差，提高估算的鲁棒性。

Result: 在采用H-MAN康复机器人平台的实验中，新方法在多种时延条件下能明显优于标准刚度估算器，估算精度更稳定。

Conclusion: 提出的方法为远程物理治疗场景中高保真触觉感知和可靠刚度评估提供了有效方案，对网络下的远程人-人交互康复具有重要应用前景。

Abstract: Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert's input with the novice's response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks.

</details>


### [289] [Less Is More: Scalable Visual Navigation from Limited Data](https://arxiv.org/abs/2601.17815)
*Yves Inglin,Jonas Frey,Changan Chen,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出通过结合经典几何规划器生成的合成轨迹与专家示范数据，用于训练基于Transformer的视觉导航策略（LiMo），显著提升了移动机器人的目标导向视觉导航性能。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在视觉导航任务中受限于高成本和低多样性的人工示范数据，本研究旨在解决如何用更经济高效的方式提升导航策略，尤其是在需要遵守人类偏好和社会规范的场景下。

Method: 利用经典几何规划器自动生成合成导航轨迹，补充有限的专家示范数据，将这些数据用于训练一个基于Transformer的视觉导航策略LiMo，从单张RGB图像预测目标导向的SE(2)轨迹，并进行消融实验和多维评估分析。

Result: 在有限专家示范基础上，结合合成轨迹进行监督训练，可显著提升导航策略性能。实验表明，数据规模和多样性对最终效果有重要影响，真实机器人部署中也取得了良好效果。

Conclusion: 实现高效视觉导航不在于单纯增加示范数量，而是策略性地增加高质量和多样化的数据。通过可扩展、具身性强的几何监督，是提升视觉导航策略数据效率的可行路径。

Abstract: Imitation learning provides a powerful framework for goal-conditioned visual navigation in mobile robots, enabling obstacle avoidance while respecting human preferences and social norms. However, its effectiveness depends critically on the quality and diversity of training data. In this work, we show how classical geometric planners can be leveraged to generate synthetic trajectories that complement costly human demonstrations. We train Less is More (LiMo), a transformer-based visual navigation policy that predicts goal-conditioned SE(2) trajectories from a single RGB observation, and find that augmenting limited expert demonstrations with planner-generated supervision yields substantial performance gains. Through ablations and complementary qualitative and quantitative analyses, we characterize how dataset scale and diversity affect planning performance. We demonstrate real-robot deployment and argue that robust visual navigation is enabled not by simply collecting more demonstrations, but by strategically curating diverse, high-quality datasets. Our results suggest that scalable, embodiment-specific geometric supervision is a practical path toward data-efficient visual navigation.

</details>


### [290] [NeuroManip: Prosthetic Hand Manipulation System Based on EMG and Eye Tracking Powered by the Neuromorphic Processor AltAi](https://arxiv.org/abs/2601.17991)
*Roman Akinshin,Elizaveta Lopatina,Kirill Bogatikov,Nikolai Kiz,Anna V. Makarova,Mikhail Lebedev,Miguel Altamirano Cabrera,Dzmitry Tsetserukou,Valerii Kangler*

Main category: cs.RO

TL;DR: 该论文提出了一种结合sEMG与视觉注视系统的神经形态上肢假肢控制架构，实现了高效、低功耗、上下文感知的肌电控制。


<details>
  <summary>Details</summary>
Motivation: 当前的上肢假肢控制系统在能效、准确性与适应复杂生活环境方面仍有限。如何提高假肢控制的智能性、实时性和安全性，是该领域的重要挑战。

Method: 作者设计了一个利用神经形态处理器（AltAi）的尖峰神经网络实时分类sEMG信号，并通过眼动追踪与视觉相机识别用户关注的目标物体。当检测对象后，系统仅从与当前物体相关的三种手势中进行识别决策。该方案与既有GPU模型性能相当，但功耗大幅降低。

Result: 在六种假肢使用者的手势识别实验中，系统达到了与目前主流肌电接口相当的稳健识别率。将识别范围限制为与物体相关的三种手势后，准确率提升至约95%，同时排除危险或不合理动作。

Conclusion: 该神经形态、上下文感知的假肢控制系统具备能效高、可穿戴、可靠性好的优点，有望提升上肢假肢在日常生活中的安全性和实用性。

Abstract: This paper presents a novel neuromorphic control architecture for upper-limb prostheses that combines surface electromyography (sEMG) with gaze-guided computer vision. The system uses a spiking neural network deployed on the neuromorphic processor AltAi to classify EMG patterns in real time while an eye-tracking headset and scene camera identify the object within the user's focus. In our prototype, the same EMG recognition model that was originally developed for a conventional GPU is deployed as a spiking network on AltAi, achieving comparable accuracy while operating in a sub-watt power regime, which enables a lightweight, wearable implementation. For six distinct functional gestures recorded from upper-limb amputees, the system achieves robust recognition performance comparable to state-of-the-art myoelectric interfaces. When the vision pipeline restricts the decision space to three context-appropriate gestures for the currently viewed object, recognition accuracy increases to roughly 95% while excluding unsafe, object-inappropriate grasps. These results indicate that the proposed neuromorphic, context-aware controller can provide energy-efficient and reliable prosthesis control and has the potential to improve safety and usability in everyday activities for people with upper-limb amputation.

</details>


### [291] [Grasp-and-Lift: Executable 3D Hand-Object Interaction Reconstruction via Physics-in-the-Loop Optimization](https://arxiv.org/abs/2601.18121)
*Byeonggyeol Choi,Woojin Oh,Jongwoo Lim*

Main category: cs.RO

TL;DR: 本文提出了一种用于手部操作数据物理可执行性的优化方法，提升现有视觉对齐轨迹的物理真实性。


<details>
  <summary>Details</summary>
Motivation: 现有大规模手部操作数据集如DexYCB和HO3D在视觉对齐方面表现良好，但其生成的交互轨迹在物理仿真中常表现出穿透、失稳等不合现实的情况，影响了下游强化学习等任务。急需把这些视觉演示转化为物理可行的运动轨迹。

Method: 作者提出了simulation-in-the-loop（仿真闭环）优化框架，将轨迹修正问题建模为黑箱优化，将手部运动参数化为低维样条曲线并结合稀疏关键帧，通过CMA-ES这一强大的无梯度优化器，直接以高保真物理引擎反馈为优化目标，兼顾物理成功与与原始示范的相似性。

Result: 与MANIPTRANS等当前技术相比，本文方法在重放时获得更低的手和物体姿态误差，更准确地还原了手-物物理交互。

Conclusion: 该方法能够通用且高效地将视觉对齐的手部示范数据转化为物理可执行轨迹，为高保真数据的生成和鲁棒策略学习奠定了基础。

Abstract: Dexterous hand manipulation increasingly relies on large-scale motion datasets with precise hand-object trajectory data. However, existing resources such as DexYCB and HO3D are primarily optimized for visual alignment but often yield physically implausible interactions when replayed in physics simulators, including penetration, missed contact, and unstable grasps.
  We propose a simulation-in-the-loop refinement framework that converts these visually aligned trajectories into physically executable ones. Our core contribution is to formulate this as a tractable black-box optimization problem. We parameterize the hand's motion using a low-dimensional, spline-based representation built on sparse temporal keyframes. This allows us to use a powerful gradient-free optimizer, CMA-ES, to treat the high-fidelity physics engine as a black-box objective function. Our method finds motions that simultaneously maximize physical success (e.g., stable grasp and lift) while minimizing deviation from the original human demonstration.
  Compared to MANIPTRANS-recent transfer pipelines, our approach achieves lower hand and object pose errors during replay and more accurately recovers hand-object physical interactions. Our approach provides a general and scalable method for converting visual demonstrations into physically valid trajectories, enabling the generation of high-fidelity data crucial for robust policy learning.

</details>


### [292] [Quest2ROS2: A ROS 2 Framework for Bi-manual VR Teleoperation](https://arxiv.org/abs/2601.18289)
*Jialong Li,Zhenguo Wang,Tianci Wang,Maj Stenmark,Volker Krueger*

Main category: cs.RO

TL;DR: Quest2ROS2是一个开源的ROS2框架，用于双手远程操作机器人，通过相对运动控制突破空间局限，提升数据采集效率，并具有良好的人机交互体验。


<details>
  <summary>Details</summary>
Motivation: 现有VR控制机器人框架如Quest2ROS在操作范围和使用便捷性上存在局限，尤其是在机器人数据采集和双手操作场景中，难以满足灵活、直观和高效的操作需求。

Method: 作者提出了一种基于相对运动的控制方法，不依赖于固定姿态，利用VR手柄姿态变化来驱动机器人，实现直观且不受空间限制的操作。同时，框架集成了实时可视化、便捷夹爪控制和暂停/重置等功能，并支持“并排”和“镜像”两种操作模式，提升用户体验。

Result: 新框架突破了操作空间的限制，通过各种模块化设计实现了灵活部署和高易用性，同时满足了多平台、多场景下的双手操作需求。

Conclusion: Quest2ROS2大幅提升了机器人远程双手操作的直观性、灵活性和安全性，为机器人数据采集等应用场景提供了高效解决方案，并以开源方式发布，促进相关领域发展。

Abstract: Quest2ROS2 is an open-source ROS2 framework for bi-manual teleoperation designed to scale robot data collection. Extending Quest2ROS, it overcomes workspace limitations via relative motion-based control, calculating robot movement from VR controller pose changes to enable intuitive, pose-independent operation. The framework integrates essential usability and safety features, including real-time RViz visualization, streamlined gripper control, and a pause-and-reset function for smooth transitions. We detail a modular architecture that supports "Side-by-Side" and "Mirror" control modes to optimize operator experience across diverse platforms. Code is available at: https://github.com/Taokt/Quest2ROS2.

</details>


### [293] [TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion](https://arxiv.org/abs/2601.18323)
*Weishi Mi,Yong Bao,Xiaowei Chi,Xiaozhu Ju,Zhiyuan Qin,Kuangzhi Ge,Kai Tang,Peidong Jia,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: 本文提出了一种名为Tool-Centric Inverse Dynamics Model（TC-IDM）的新方法，用于提升基于视觉—语言—行动范式的机器人控制性能。该方法以工具轨迹为核心中间表征，有效连接了视觉规划与实际物理控制，在多样场景下表现出较强泛化性和较高成功率，优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉—语言—行动（VLA）范式强大但严重依赖大规模高质量机器人数据，导致泛化能力受限。而生成式世界模型虽然可为通用智能体提供支持，但其输出的像素级计划难以直接转化为可执行的控制信号，存在表示鸿沟。为解决上述难题，作者试图寻找一种更加健壮且具备泛化能力的中间表征，以提升机器人控制系统的实用性和适用范围。

Method: 作者提出TC-IDM，通过世界模型生成视频，利用分割与3D运动估计从中提取工具点云轨迹，作为桥梁连接视觉规划与物理动作。模型结构针对不同工具属性设计解耦的动作头，将计划轨迹映射为6自由度末端执行器动作及对应控制信号。该方法实现了“计划—翻译”范式，兼容多种末端执行器，并提升了视角不变性及任务泛化能力。

Result: TC-IDM在现实世界机器人任务中平均成功率达到61.11%，其中常规简单任务为77.7%，零样本可变形物体任务为38.46%。整体效果显著超越现有VLA范式端到端方法和其他逆动力学模型。

Conclusion: TC-IDM有力填补了生成式世界模型视觉规划与物理控制之间的鸿沟，特别是在复杂与无先验的场景下展现了优越的泛化性和鲁棒性，为推动通用型实体智能体的研究和应用带来新方向。

Abstract: The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions.
  To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool's imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control.
  TC-IDM extracts the tool's point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals.
  This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects.
  In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models.

</details>


### [294] [SG-CADVLM: A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation](https://arxiv.org/abs/2601.18442)
*Hongyi Zhao,Shuo Wang,Qijie He,Ziyuan Pu*

Main category: cs.RO

TL;DR: 提出了一种新的多模态模型SG-CADVLM，能基于碰撞报告和道路图自动生成安全关键场景，显著优于现有方法，用于自动驾驶安全测试。


<details>
  <summary>Details</summary>
Motivation: 现实中安全关键场景（如碰撞事件）极为罕见且验证成本高。现有数据驱动方法受限于数据多样性，基于对抗的生成方法又存在物理不真实的问题。如何真实、高效地产生适用于自动驾驶测试的高风险交通场景，是急需解决的问题。

Method: 提出SG-CADVLM框架，将上下文感知解码与多模态输入（碰撞报告和道路网图）结合，利用视觉-语言模型，有效减轻模型幻觉及失真，自动生成道路结构及车辆轨迹。

Result: SG-CADVLM生成安全关键高风险场景的成功率达84.4%，远高于基准方法的12.5%，提升了469%；并能生成可直接用于自动驾驶仿真的案例。

Conclusion: SG-CADVLM极大提升了自动驾驶安全测试中高风险场景的生成效率和真实性，成为利用事故报告模拟和验证智能车辆安全性的有前景方法。

Abstract: Autonomous vehicle safety validation requires testing on safety-critical scenarios, but these events are rare in real-world driving and costly to test due to collision risks. Crash reports provide authentic specifications of safety-critical events, offering a vital alternative to scarce real-world collision trajectory data. This makes them valuable sources for generating realistic high-risk scenarios through simulation. Existing approaches face significant limitations because data-driven methods lack diversity due to their reliance on existing latent distributions, whereas adversarial methods often produce unrealistic scenarios lacking physical fidelity. Large Language Model (LLM) and Vision Language Model (VLM)-based methods show significant promise. However, they suffer from context suppression issues where internal parametric knowledge overrides crash specifications, producing scenarios that deviate from actual accident characteristics. This paper presents SG-CADVLM (A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation), a framework that integrates Context-Aware Decoding with multi-modal input processing to generate safety-critical scenarios from crash reports and road network diagrams. The framework mitigates VLM hallucination issues while enabling the simultaneous generation of road geometry and vehicle trajectories. The experimental results demonstrate that SG-CADVLM generates critical risk scenarios at a rate of 84.4% compared to 12.5% for the baseline methods, representing an improvement of 469%, while producing executable simulations for autonomous vehicle testing.

</details>


### [295] [DV-VLN: Dual Verification for Reliable LLM-Based Vision-and-Language Navigation](https://arxiv.org/abs/2601.18492)
*Zijun Li,Shijie Li,Zhenxi Zhang,Bin Li,Shoujun Zhou*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的视觉-语言导航（VLN）方法DV-VLN，通过生成-验证范式提升导航准确性和解释性，显著优于现有语言驱动的导航基线。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）驱动的VLN方法多采用单步决策，容易由于局部偏差和中间推理不完善导致导航错误，并且在复杂和未知环境中可靠性较低。

Method: DV-VLN采用生成-验证（generate-then-verify）范式：首先对LLaMA-2主干进行领域自适应，输出结构化的导航思维链；然后通过True-False Verification（TFV）和Masked-Entity Verification（MEV）两种互补的验证手段评估备选动作，并整合多次样本的验证结果进行最终动作选择和排序，增强可解释性和鲁棒性。

Result: 在R2R、RxR（英语子集）和REVERIE数据集上的实验表明，DV-VLN在所有指标上均超越直接预测和仅采样的基线方法，在语言驱动VLN系统中表现出有竞争力的性能，并在部分指标上接近跨模态系统。

Conclusion: DV-VLN有效克服了单步决策带来的误差累积问题，提升了VLN在未知环境下的泛化能力和可解释性，是当前语言驱动导航领域的有力进展。

Abstract: Vision-and-Language Navigation (VLN) requires an embodied agent to navigate in a complex 3D environment according to natural language instructions. Recent progress in large language models (LLMs) has enabled language-driven navigation with improved interpretability. However, most LLM-based agents still rely on single-shot action decisions, where the model must choose one option from noisy, textualized multi-perspective observations. Due to local mismatches and imperfect intermediate reasoning, such decisions can easily deviate from the correct path, leading to error accumulation and reduced reliability in unseen environments. In this paper, we propose DV-VLN, a new VLN framework that follows a generate-then-verify paradigm. DV-VLN first performs parameter-efficient in-domain adaptation of an open-source LLaMA-2 backbone to produce a structured navigational chain-of-thought, and then verifies candidate actions with two complementary channels: True-False Verification (TFV) and Masked-Entity Verification (MEV). DV-VLN selects actions by aggregating verification successes across multiple samples, yielding interpretable scores for reranking. Experiments on R2R, RxR (English subset), and REVERIE show that DV-VLN consistently improves over direct prediction and sampling-only baselines, achieving competitive performance among language-only VLN agents and promising results compared with several cross-modal systems.Code is available at https://github.com/PlumJun/DV-VLN.

</details>


### [296] [SKETCH: Semantic Key-Point Conditioning for Long-Horizon Vessel Trajectory Prediction](https://arxiv.org/abs/2601.18537)
*Linyong Gan,Zimo Li,Wenxin Xu,Xingjian Li,Jianhua Z. Huang,Enmei Tu,Shuhang Chen*

Main category: cs.RO

TL;DR: 本文提出了一种基于语义关键点条件的船舶轨迹预测框架，通过分解为全局语义决策和局部运动建模，有效提升了长时间船舶轨迹预测的准确性，尤其在方向一致性和细节还原上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 船舶轨迹的长时预测存在由于复杂航行行为与环境因素带来的累积不确定性，现有方法难以保持全球方向一致性，导致结果偏离或不合理。为解决此问题，作者希望提出更鲁棒、具备语义约束的长时间轨迹预测方法。

Method: 方法以未来轨迹受高层导航意图（Next Key Point, NKP）条件影响为核心创新点，通过将长时预测分解为全球语义决策（NKP选择）和局部运动建模，并通过预训练-微调策略从历史观测高效估算NKP先验。

Result: 在真实AIS数据上，提出方法在长距离、方向精度及细粒度轨迹预测性能上，均显著优于现有最先进技术，实现了一致性提升。

Conclusion: 基于NKP的语义条件轨迹建模有效约束了未来轨迹空间，显著提高了长时间船舶预测的实用性和准确性，具有良好应用前景。

Abstract: Accurate long-horizon vessel trajectory prediction remains challenging due to compounded uncertainty from complex navigation behaviors and environmental factors. Existing methods often struggle to maintain global directional consistency, leading to drifting or implausible trajectories when extrapolated over long time horizons. To address this issue, we propose a semantic-key-point-conditioned trajectory modeling framework, in which future trajectories are predicted by conditioning on a high-level Next Key Point (NKP) that captures navigational intent. This formulation decomposes long-horizon prediction into global semantic decision-making and local motion modeling, effectively restricting the support of future trajectories to semantically feasible subsets. To efficiently estimate the NKP prior from historical observations, we adopt a pretrain-finetune strategy. Extensive experiments on real-world AIS data demonstrate that the proposed method consistently outperforms state-of-the-art approaches, particularly for long travel durations, directional accuracy, and fine-grained trajectory prediction.

</details>


### [297] [Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field](https://arxiv.org/abs/2601.18548)
*Yulin Li,Zhiyuan Song,Yiming Li,Zhicheng Song,Kai Chen,Chunxin Zheng,Zhihai Bi,Jiahang Cao,Sylvain Calinon,Fan Shi,Jun Ma*

Main category: cs.RO

TL;DR: 本论文提出了一种新的方法（GCDF）用于高效、精准地处理有移动底座机械臂在复杂空间中的整体彩迹优化，以及基于此的高性能优化器，实现了大规模隐式约束下的快速碰撞推理和快速重规划。


<details>
  <summary>Details</summary>
Motivation: 移动机械臂能够通过协调底座和机械臂动作，实现更加灵活和远距离的操作。然而，在狭窄、拥挤空间中的全身轨迹优化非常困难，主要因为高维非凸性和对快速、精准碰撞检测的需求。目前的配置空间距离场（CDF）方法只适用于固定底座的机械臂，难以推广到移动机械臂复杂的耦合空间。因此，亟需一种能扩展到移动平台、解决碰撞检测和轨迹优化问题的通用方法。

Method: 作者提出并扩展了通用配置空间距离场（GCDF）的方法，将原本适用于固定轴机械臂的CDF扩展到能同时处理平移、旋转关节及无界工作空间的移动机械臂系统。证明了GCDF的局部距离保持特性，并通过数据生成和神经网络训练流程，获得可高效批量查询的连续神经GCDF模型。基于GCDF碰撞推理，作者进一步构建了高性能序贯凸优化框架，支持神经约束在线指定、并行批量约束评估和高效增量式约束管理。

Result: 实验结果表明，提出的GCDF能够准确反映高维配置空间中的全身几何信息，并在处理大量隐式约束时，优化器具备优秀的扩展性与计算效率，支持场景变化下的快速重规划。

Conclusion: GCDF有效扩展了配置空间距离场的适用范围，使移动机械臂在受限空间中的高效整体彩迹优化成为可能，为移动操作机器人在复杂环境中的应用提供了强有力的理论与工具支持。

Abstract: Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes.

</details>


### [298] [Attention-Based Neural-Augmented Kalman Filter for Legged Robot State Estimation](https://arxiv.org/abs/2601.18569)
*Seokju Lee,Kyung-Soo Kim*

Main category: cs.RO

TL;DR: 本文提出了一种基于注意力机制的神经增强卡尔曼滤波器（AttenNKF），用于腿式机器人状态估算，能有效补偿由于足部打滑带来的误差。实验显示，在易打滑场景下，该方法优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 足部打滑会导致腿式机器人状态估算出现较大误差，主要因为运动学观测在打滑时违背了无滑移假设，造成滤波更新有偏。为提升估算精度，需设计高效的打滑补偿方法。

Method: 在经典的非齐次扩展卡尔曼滤波器（InEKF）基础上，作者引入带有注意力机制的神经补偿器。补偿器根据打滑严重程度，对滤波器更新后状态进行误差估计和调整。神经网络在潜在空间中训练，降低对原始输入尺度敏感性，确保补偿结构良好同时不影响滤波递归。

Result: 实验表明，所提AttenNKF在足部打滑频繁的工况下，状态估算精度优于现有腿式机器人状态估算器。

Conclusion: 带有注意力机制的神经补偿器能有效抑制由足部打滑引入的估算误差，在腿式机器人状态估算领域具有良好应用前景。

Abstract: In this letter, we propose an Attention-Based Neural-Augmented Kalman Filter (AttenNKF) for state estimation in legged robots. Foot slip is a major source of estimation error: when slip occurs, kinematic measurements violate the no-slip assumption and inject bias during the update step. Our objective is to estimate this slip-induced error and compensate for it. To this end, we augment an Invariant Extended Kalman Filter (InEKF) with a neural compensator that uses an attention mechanism to infer error conditioned on foot-slip severity and then applies this estimate as a post-update compensation to the InEKF state (i.e., after the filter update). The compensator is trained in a latent space, which aims to reduce sensitivity to raw input scales and encourages structured slip-conditioned compensations, while preserving the InEKF recursion. Experiments demonstrate improved performance compared to existing legged-robot state estimators, particularly under slip-prone conditions.

</details>


### [299] [ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection](https://arxiv.org/abs/2601.18629)
*Yiming Wang,Ruogu Zhang,Minyang Li,Hao Shi,Junbo Wang,Deyi Li,Jieji Ren,Wenhai Liu,Weiming Wang,Hao-Shu Fang*

Main category: cs.RO

TL;DR: 提出了一种新型机器人操作数据采集与迁移框架ExoGS，通过人穿戴的外骨骼装置采集真实世界的交互数据，并将其高效转移到仿真环境，用于高效的操作策略学习和泛化。


<details>
  <summary>Details</summary>
Motivation: 传统的Real-to-Sim-to-Real方法主要关注在视觉层面的环境迁移，忽略了人与物体、机器人之间的动态交互迁移。而这些动态交互，尤其是在高接触场景下，单靠仿真难以高效获得。作者希望破解真实世界人类操作交互的数据采集与迁移难题。

Method: 提出基于自研的无机器人被动外骨骼AirExo-3采集装置，通过该装置可获得与机器人运动学一致、高精度的人类演示数据，并同步获取RGB视觉信息。将机器人、物体、环境重建为可编辑的3D Gaussian Splatting资产，实现几何一致的回放和大规模数据扩增。引入轻量级的Mask Adapter提升策略的语义鲁棒性。

Result: 在真实世界实验下，ExoGS方法在数据效率和策略泛化能力上明显优于基于远程操作的数据采集方法。

Conclusion: ExoGS是一种高效、可扩展的数据采集和迁移策略学习框架，能够大幅提升机器人操作策略的泛化能力和数据利用效率，具有良好的实际应用前景。代码和硬件也已开放共享。

Abstract: Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on https://github.com/zaixiabalala/ExoGS.

</details>


### [300] [Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation](https://arxiv.org/abs/2601.18639)
*Ojasva Mishra,Xiaolong Wu,Min Xu*

Main category: cs.RO

TL;DR: 本文研究了实际机器人中的饱和离散时间关节控制，提出了针对实际实现因素的分析和调参流程，并结合贝叶斯优化和安全筛选，实现了高效、鲁棒的PI/PID控制器调整。


<details>
  <summary>Details</summary>
Motivation: 尽管PID控制广泛应用于机器人等自动化系统，但实际实现中存在离散执行、执行器饱和、小延迟及测量不完美等常见问题，导致理论与实际表现不符，亟需一套兼顾理论与实现细节的分析和调整方法。

Method: (1) 利用Jury判据，推导了Euler和ZOH离散化下PI控制器的稳定区域；(2) 在饱和主导场景下，评估了离散反计算抗积分饱和结构；(3) 提出混合认证贝叶斯优化流程，先剔除不稳定或不安全参数，再针对鲁棒性能指标（IAE）及软惩罚（如超调/饱和）进行优化。

Result: 一系列仿真（含不确定性、延迟、噪声、量化及更强饱和）显示，在鲁棒调参后，中值IAE从0.843降至0.430，超调保持在2%以下。通过仿真阶段认证筛除11.6%的不合格参数，提升了样本利用率，无需硬件实验。

Conclusion: 本文所提流程在料想不确定和饱和等实际工程约束下，提升了离散时间PI/PID控制器的稳健性与调优效率，为自主机器人等实际系统提供了有效的调参与安全保障手段。

Abstract: The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($τ=1.0$~s, $Δt=0.01$~s, $u\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\%$. In simulation-only tuning, the certification screen rejects $11.6\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments.

</details>


### [301] [A Pragmatic VLA Foundation Model](https://arxiv.org/abs/2601.18692)
*Wei Wu,Fan Lu,Yunnan Wang,Shuai Yang,Shi Liu,Fangjing Wang,Qian Zhu,He Sun,Yong Wang,Shuailei Ma,Yiyu Ren,Kejia Zhang,Hui Yu,Jingmei Zhao,Shuai Zhou,Zhenqi Qiu,Houlong Xiong,Ziyu Wang,Zechen Wang,Ran Cheng,Yong-Lu Li,Yongtao Huang,Xing Zhu,Yujun Shen,Kecheng Zheng*

Main category: cs.RO

TL;DR: 本文提出了LingBot-VLA视觉-语言-动作基础模型，利用大量真实世界数据训练，并在多个机器人平台任务中表现优越，同时大幅提升了训练效率。代码、基线模型和基准数据已开放，旨在推动机器人学习研究。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作任务需要能够跨任务、跨平台泛化且高效适应的新一代基础模型，而现有模型往往在泛化能力或训练效率方面存在不足。

Method: 研究者开发了LingBot-VLA模型，基于2万个小时、9种双臂机器人真实数据训练。在3个平台上系统评测，每个平台执行100项任务，每项任务130次训练后测试，表现明显优于现有方法。同时开发高效代码库，实现单GPU每秒261样本、8GPU训练1.5~2.8倍提速。

Result: LingBot-VLA在各平台和任务评测中均大幅超越竞品，验证了其广泛泛化能力与性能。新代码库显著提高了训练效率。

Conclusion: LingBot-VLA既强化模型泛化和效率，也为实际部署做好了准备。开放资源有望推动机器人学习更加标准化、挑战性任务的发展。

Abstract: Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8$\times$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.

</details>


### [302] [Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods](https://arxiv.org/abs/2601.18723)
*Mengyuan Liu,Juyi Sheng,Peiming Li,Ziyi Wang,Tianming Xu,Tiantian Xu,Hong Liu*

Main category: cs.RO

TL;DR: 本文提出Eval-Actions基准和AutoEval架构，用于提升机器人模仿学习行为的可信评测，超越传统二元成功率，能区分来源并量化执行质量。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-动作及视觉-语言-动作模型的发展，机器人模仿学习能力快速提升，但现有评估方法主要依赖简单成功率，难以评价机器行为的可信度，包括区分来源真实性（如人工遥操作或智能体策略）和执行质量（如平滑性、安全性）。因此亟需更可靠、多元的评测体系。

Method: 1）构建Eval-Actions数据集，包含人类遥操作、政策执行及失败案例，并以专家评分、偏好排序和链式思维三类监督信号标注。2）提出AutoEval架构，基于时空聚合实现语义级评测，并用辅助运动学信号校正平滑性；升级版AutoEval-P引入群组相对策略优化以加强逻辑推理能力。3）通过多协议对评测方法进行实验验证。

Result: AutoEval在专家评分和偏好排序协议下，Spearman秩相关系数分别达到0.81和0.84。系统可99.6%准确区分智能体和人工示范，显示了优异的源判别能力。

Conclusion: Eval-Actions与AutoEval体系显著提升了机器人行为可信评测标准；通过引入多元真实监督信号与自动语义评测机制，为机器人模仿学习提供更严格、可推广的评测体系，有助于推动可信机器人系统发展。

Abstract: Driven by the rapid evolution of Vision-Action and Vision-Language-Action models, imitation learning has significantly advanced robotic manipulation capabilities. However, evaluation methodologies have lagged behind, hindering the establishment of Trustworthy Evaluation for these behaviors. Current paradigms rely on binary success rates, failing to address the critical dimensions of trust: Source Authenticity (i.e., distinguishing genuine policy behaviors from human teleoperation) and Execution Quality (e.g., smoothness and safety). To bridge these gaps, we propose a solution that combines the Eval-Actions benchmark and the AutoEval architecture. First, we construct the Eval-Actions benchmark to support trustworthiness analysis. Distinct from existing datasets restricted to successful human demonstrations, Eval-Actions integrates VA and VLA policy execution trajectories alongside human teleoperation data, explicitly including failure scenarios. This dataset is structured around three core supervision signals: Expert Grading (EG), Rank-Guided preferences (RG), and Chain-of-Thought (CoT). Building on this, we propose the AutoEval architecture: AutoEval leverages Spatio-Temporal Aggregation for semantic assessment, augmented by an auxiliary Kinematic Calibration Signal to refine motion smoothness; AutoEval Plus (AutoEval-P) incorporates the Group Relative Policy Optimization (GRPO) paradigm to enhance logical reasoning capabilities. Experiments show AutoEval achieves Spearman's Rank Correlation Coefficients (SRCC) of 0.81 and 0.84 under the EG and RG protocols, respectively. Crucially, the framework possesses robust source discrimination capabilities, distinguishing between policy-generated and teleoperated videos with 99.6% accuracy, thereby establishing a rigorous standard for trustworthy robotic evaluation. Our project and code are available at https://term-bench.github.io/.

</details>


### [303] [Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge](https://arxiv.org/abs/2601.18733)
*Li Kang,Heng Zhou,Xiufeng Song,Rui Li,Bruno N. Y. Chen,Ziye Wang,Ximeng Meng,Stone Tao,Yiran Qin,Xiaohong Liu,Ruimao Zhang,Lei Bai,Yilun Du,Hao Su,Philip Torr,Zhenfei Yin,Ruihao Gong,Yejun Zeng,Fengjun Zhong,Shenghao Jin,Jinyang Guo,Xianglong Liu,Xiaojun Jia,Tianqi Shan,Wenqi Ren,Simeng Qin,Jialing Yang,Xiaoyu Ma,Tianxing Chen,Zixuan Li,Zijian Cai,Yan Qin,Yusen Qin,Qiangyu Chen,Kaixuan Wang,Zhaoming Han,Yao Mu,Ping Luo,Yuanqi Yao,Haoming Song,Jan-Nico Zaech,Fabien Despinoy,Danda Pani Paudel,Luc Van Gool*

Main category: cs.RO

TL;DR: 文章介绍了MARS挑战赛，该竞赛旨在推动基于多模态大语言模型的多智能体协作研究，特别关注规划与控制问题，促进多智能体系统在复杂任务中的应用和发展。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型和视觉-语言-动作模型的进展，具身智能AI领域日渐复杂化，单一智能体已无法满足扩展性和协作需求。因此，研究多智能体系统成为实现高效、可扩展与协作解决方案的关键。

Method: 提出并举办了MARS挑战赛，分为规划与控制两个核心赛道。参赛者需利用视觉-语言模型开展多智能体协作任务规划，并执行政策以实现机器人在动态环境中的操作。通过比拼多智能体的任务分配及执行效率，评估不同解决方案。

Result: 通过竞赛，收集了多样化的解决方案和实验数据，获得了多智能体系统如何协调设计、任务分配和执行等方面的重要见解。

Conclusion: MARS挑战赛拓展了多智能体具身AI的理论与实践边界，促进了合作机器人系统的发展；为未来智能协作AI系统的设计与实现奠定了基础。

Abstract: Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems.

</details>


### [304] [Goal-oriented Communication for Fast and Robust Robotic Fault Detection and Recovery](https://arxiv.org/abs/2601.18765)
*Shutong Chen,Adnan Aijaz,Yansha Deng*

Main category: cs.RO

TL;DR: 本文提出了一种新的面向目标的通信（GoC）框架，用于智能工厂中自主机器人系统的快速且鲁棒的故障检测与恢复（FDR），有效降低了FDR时间并提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的故障检测与恢复框架存在通信和计算延迟问题，以及在机器人运动/轨迹生成方面可靠性不足，主要原因是通信-计算-控制（3C）闭环未围绕FDR目标共同设计，从而影响实际生产中的效率与可靠性。

Method: GoC框架联合设计3C闭环，优化FDR的效率和成功率。其具体方法包括：1）设计表示提取器，提取3D场景图（3D-SG）作为语义表示，通过监控3D-SG中的空间关系变化进行故障检测；2）采用LoRA微调小型语言模型，结合知识蒸馏增强推理与泛化能力，自动生成失败后的机器人恢复动作；3）提出轻量化数字孪生重建模块，在需精细控制时，仅用与任务相关的物体轮廓对SLM生成的动作进一步优化。

Result: 大量模拟实验表明，GoC框架相比当前主流依赖视觉语言模型检测故障、依赖大语言模型恢复的体系，在FDR时间上最多减少82.6%，任务成功率最多提高76%。

Conclusion: 联合设计的3C闭环和创新的感知-推理-恢复体系极大提升了自主机器人系统在智能工厂环境中的FDR效率和可靠性，具备良好的应用前景。

Abstract: Autonomous robotic systems are widely deployed in smart factories and operate in dynamic, uncertain, and human-involved environments that require low-latency and robust fault detection and recovery (FDR). However, existing FDR frameworks exhibit various limitations, such as significant delays in communication and computation, and unreliability in robot motion/trajectory generation, mainly because the communication-computation-control (3C) loop is designed without considering the downstream FDR goal. To address this, we propose a novel Goal-oriented Communication (GoC) framework that jointly designs the 3C loop tailored for fast and robust robotic FDR, with the goal of minimising the FDR time while maximising the robotic task (e.g., workpiece sorting) success rate. For fault detection, our GoC framework innovatively defines and extracts the 3D scene graph (3D-SG) as the semantic representation via our designed representation extractor, and detects faults by monitoring spatial relationship changes in the 3D-SG. For fault recovery, we fine-tune a small language model (SLM) via Low-Rank Adaptation (LoRA) and enhance its reasoning and generalization capabilities via knowledge distillation to generate recovery motions for robots. We also design a lightweight goal-oriented digital twin reconstruction module to refine the recovery motions generated by the SLM when fine-grained robotic control is required, using only task-relevant object contours for digital twin reconstruction. Extensive simulations demonstrate that our GoC framework reduces the FDR time by up to 82.6% and improves the task success rate by up to 76%, compared to the state-of-the-art frameworks that rely on vision language models for fault detection and large language models for fault recovery.

</details>
