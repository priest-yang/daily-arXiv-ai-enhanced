<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 32]
- [cs.CL](#cs.CL) [Total: 24]
- [cs.RO](#cs.RO) [Total: 18]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enabling clinical use of foundation models in histopathology](https://arxiv.org/abs/2602.22347)
*Audun L. Henriksen,Ole-Johan Skrede,Lisa van der Schee,Enric Domingo,Sepp De Raedt,Ilyá Kostolomov,Jennifer Hay,Karolina Cyll,Wanja Kildal,Joakim Kalsnes,Robert W. Williams,Manohar Pradhan,John Arne Nesheim,Hanne A. Askautrud,Maria X. Isaksen,Karmele Saez de Gordoa,Miriam Cuatrecasas,Joanne Edwards,TransSCOT group,Arild Nesbakken,Neil A. Shepherd,Ian Tomlinson,Daniel-Christoph Wagner,Rachel S. Kerr,Tarjei Sveinsgjerd Hveem,Knut Liestøl,Yoshiaki Nakamura,Marco Novelli,Masaaki Miyo,Sebastian Foersch,David N. Church,Miangela M. Lacle,David J. Kerr,Andreas Kleppe*

Main category: cs.CV

TL;DR: 本研究通过在下游任务模型训练过程中引入新的鲁棒性损失函数，显著提升了基础模型在病理学分析中的鲁棒性与准确性，无需对基础模型本身重新训练。


<details>
  <summary>Details</summary>
Motivation: 虽然基础模型在数字病理学中有助于发展高性能和泛化性强的深度学习系统，但现有基础模型提取到的特征很容易受到前处理流程和扫描仪差异等技术变异性的影响，导致下游特定任务模型受到偏倚。因此，亟需方法来减弱这些非生物学相关变异带来的影响。

Method: 建立了包含27,042张全切片数字病理图像（WSI）、来自6155名患者的大规模实验平台，从8种主流基础模型中提取特征，训练数千个下游任务模型。在训练下游模型时引入了新设计的鲁棒性损失，以降低模型对技术性变异的敏感性。

Result: 实验显示，引入鲁棒性损失不仅明显提高了下游模型对技术性变异的鲁棒性，还提升了预测准确率，因为模型更加专注于生物学相关特征。

Conclusion: 无需重新训练基础模型，通过在下游任务模型训练中引入鲁棒性损失，有效缓解了基础模型在计算病理学中的鲁棒性问题，有望推动这些AI模型在实际临床环境中的应用落地。

Abstract: Foundation models in histopathology are expected to facilitate the development of high-performing and generalisable deep learning systems. However, current models capture not only biologically relevant features, but also pre-analytic and scanner-specific variation that bias the predictions of task-specific models trained from the foundation model features. Here we show that introducing novel robustness losses during training of downstream task-specific models reduces sensitivity to technical variability. A purpose-designed comprehensive experimentation setup with 27,042 WSIs from 6155 patients is used to train thousands of models from the features of eight popular foundation models for computational pathology. In addition to a substantial improvement in robustness, we observe that prediction accuracy improves by focusing on biologically relevant features. Our approach successfully mitigates robustness issues of foundation models for computational pathology without retraining the foundation models themselves, enabling development of robust computational pathology models applicable to real-world data in routine clinical practice.

</details>


### [2] [Optimizing Neural Network Architecture for Medical Image Segmentation Using Monte Carlo Tree Search](https://arxiv.org/abs/2602.22361)
*Liping Meng,Fan Nie,Yunyun Zhang,Chao Han*

Main category: cs.CV

TL;DR: 本文提出了MNAS-Unet医疗影像分割框架，将蒙特卡洛树搜索（MCTS）与神经架构搜索（NAS）结合，实现了高效且精确的模型架构搜索，并在多项医疗影像分割任务中优于主流模型。


<details>
  <summary>Details</summary>
Motivation: 当前医疗影像分割对模型准确性和效率要求高，但现有NAS方法搜索成本大、可移植性有限。作者希望通过引入MCTS优化搜索过程，提高效率并降低资源消耗。

Method: 提出MNAS-Unet，结合MCTS与NAS，通过动态搜索提升架构探索效率；并且优化了网络下采样(DownSC)与上采样(UpSC)单元，实现模型结构的高效调整。

Result: 实验表明，MNAS-Unet在PROMISE12、Ultrasound Nerve和CHAOS等数据集上分割精度超越NAS-Unet及其它最新方法，并且在架构搜索阶段将计算预算缩减54%，参数量与显存消耗大幅减少。

Conclusion: MNAS-Unet能在资源有限条件下有效提升分割准确率和搜索效率，具备更高实际应用价值。

Abstract: This paper proposes a novel medical image segmentation framework, MNAS-Unet, which combines Monte Carlo Tree Search (MCTS) and Neural Architecture Search (NAS). MNAS-Unet dynamically explores promising network architectures through MCTS, significantly enhancing the efficiency and accuracy of architecture search. It also optimizes the DownSC and UpSC unit structures, enabling fast and precise model adjustments. Experimental results demonstrate that MNAS-Unet outperforms NAS-Unet and other state-of-the-art models in segmentation accuracy on several medical image datasets, including PROMISE12, Ultrasound Nerve, and CHAOS. Furthermore, compared with NAS-Unet, MNAS-Unet reduces the architecture search budget by 54% (early stopping at 139 epochs versus 300 epochs under the same search setting), while achieving a lightweight model with only 0.6M parameters and lower GPU memory consumption, which further improves its practical applicability. These results suggest that MNAS-Unet can improve search efficiency while maintaining competitive segmentation accuracy under practical resource constraints.

</details>


### [3] [AeroDGS: Physically Consistent Dynamic Gaussian Splatting for Single-Sequence Aerial 4D Reconstruction](https://arxiv.org/abs/2602.22376)
*Hanyang Liu,Rongjun Qin*

Main category: cs.CV

TL;DR: 本文提出了一种适用于无人机单目视频的4D高斯泼洒（Gaussian splatting）重建方法AeroDGS，有效提升了动态航拍场景的重建效果。


<details>
  <summary>Details</summary>
Motivation: 当前动态4D场景重建方法在航拍单目、视场大、动态目标又小且运动差异大的场景下，面临深度歧义与运动估计不稳定等难题，导致单目航拍重建难以准确实现。

Method: 提出AeroDGS框架，包括单目几何提升模块（重建静态与动态结构）和物理引导优化模块（引入可微地面支持、稳定性及轨迹平滑先验），共同提升几何和运动一致性。还建立了包含不同高度与运动条件的无人机真实数据集，用于验证。

Result: 在合成与真实无人机航拍场景上，AeroDGS在动态重建质量上明显超越现有最先进方法，表现出更高的几何准确性和动态一致性。

Conclusion: AeroDGS实现了在复杂动态航拍场景中，无人机单目4D重建的新突破，并为相关领域提供了更优的重建工具及数据集支持。

Abstract: Recent advances in 4D scene reconstruction have significantly improved dynamic modeling across various domains. However, existing approaches remain limited under aerial conditions with single-view capture, wide spatial range, and dynamic objects of limited spatial footprint and large motion disparity. These challenges cause severe depth ambiguity and unstable motion estimation, making monocular aerial reconstruction inherently ill-posed. To this end, we present AeroDGS, a physics-guided 4D Gaussian splatting framework for monocular UAV videos. AeroDGS introduces a Monocular Geometry Lifting module that reconstructs reliable static and dynamic geometry from a single aerial sequence, providing a robust basis for dynamic estimation. To further resolve monocular ambiguity, we propose a Physics-Guided Optimization module that incorporates differentiable ground-support, upright-stability, and trajectory-smoothness priors, transforming ambiguous image cues into physically consistent motion. The framework jointly refines static backgrounds and dynamic entities with stable geometry and coherent temporal evolution. We additionally build a real-world UAV dataset that spans various altitudes and motion conditions to evaluate dynamic aerial reconstruction. Experiments on synthetic and real UAV scenes demonstrate that AeroDGS outperforms state-of-the-art methods, achieving superior reconstruction fidelity in dynamic aerial environments.

</details>


### [4] [Enhancing Renal Tumor Malignancy Prediction: Deep Learning with Automatic 3D CT Organ Focused Attention](https://arxiv.org/abs/2602.22381)
*Zhengkang Fan,Chengkun Sun,Russell Terry,Jie Xu,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 本研究提出了一种无需手动分割的新型深度学习方法，通过关注肾脏区域提升CT图像恶性肿瘤预测的准确性，并显著优于传统分割方法。


<details>
  <summary>Details</summary>
Motivation: 目前肾脏肿瘤良恶性预测依赖影像，但现有方法预测精度有限，且依赖昂贵且耗时的手工分割，需要创新手段提升预测准确性并减轻医生负担。

Method: 开发了一种基于深度学习的Organ Focused Attention (OFA)损失函数，使模型只聚焦于肾脏相关区域，无需对3D CT图像手动分割即能预测肿瘤恶性，实现自动化高效处理。

Result: 该方法在UF IDR私有数据集上AUC达0.685、F1分数0.872，在KiTS21公开数据集上AUC为0.760、F1为0.852，均优于依赖分割的传统方法。

Conclusion: 提出的OFA方法无需肾脏分割即可提升恶性预测准确度，有效提升肾癌诊断中的临床决策效率和可靠性，具有广泛应用前景。

Abstract: Accurate prediction of malignancy in renal tumors is crucial for informing clinical decisions and optimizing treatment strategies. However, existing imaging modalities lack the necessary accuracy to reliably predict malignancy before surgical intervention. While deep learning has shown promise in malignancy prediction using 3D CT images, traditional approaches often rely on manual segmentation to isolate the tumor region and reduce noise, which enhances predictive performance. Manual segmentation, however, is labor-intensive, costly, and dependent on expert knowledge. In this study, a deep learning framework was developed utilizing an Organ Focused Attention (OFA) loss function to modify the attention of image patches so that organ patches attend only to other organ patches. Hence, no segmentation of 3D renal CT images is required at deployment time for malignancy prediction. The proposed framework achieved an AUC of 0.685 and an F1-score of 0.872 on a private dataset from the UF Integrated Data Repository (IDR), and an AUC of 0.760 and an F1-score of 0.852 on the publicly available KiTS21 dataset. These results surpass the performance of conventional models that rely on segmentation-based cropping for noise reduction, demonstrating the frameworks ability to enhance predictive accuracy without explicit segmentation input. The findings suggest that this approach offers a more efficient and reliable method for malignancy prediction, thereby enhancing clinical decision-making in renal cancer diagnosis.

</details>


### [5] [Vision Transformers Need More Than Registers](https://arxiv.org/abs/2602.22394)
*Cheng Shi,Yizhou Yu,Sibei Yang*

Main category: cs.CV

TL;DR: 本文系统分析了视觉Transformer（ViT）存在的伪影现象，并提出通过有选择性地整合patch特征来缓解该问题，广泛提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: ViT虽然能学到通用表征，但在多种监督和下游任务中均出现伪影问题，其成因尚不清楚。作者希望系统地剖析这些伪影的本质，进一步提升ViT在各类任务中的表现。

Method: 通过分析ViT的伪影现象，发现其源于随意聚合行为——模型会利用与语义无关的背景patch作为捷径表达全局语义。为此，作者提出了一种新的特征整合机制，即有选择性地将patch特征融入CLS token中，降低依赖背景信息的现象。

Result: 提出的方法在12个主流基准测试（涵盖标签、文本、自监督三种模式）下，均实现了一致的性能提升，说明有效抑制了背景主导型伪影的负面影响。

Conclusion: 研究揭示了ViT伪影的根本机制，并提出了有效的改进办法，为理解和优化ViT行为带来了新视角，对后续研究有指导意义。

Abstract: Vision Transformers (ViTs), when pre-trained on large-scale data, provide general-purpose representations for diverse downstream tasks. However, artifacts in ViTs are widely observed across different supervision paradigms and downstream tasks. Through systematic analysis of artifacts in ViTs, we find that their fundamental mechanisms have yet to be sufficiently elucidated. In this paper, through systematic analysis, we conclude that these artifacts originate from a lazy aggregation behavior: ViT uses semantically irrelevant background patches as shortcuts to represent global semantics, driven by global attention and Coarse-grained semantic supervision. Our solution selectively integrates patch features into the CLS token, reducing the influence of background-dominated shortcuts and consistently improving performance across 12 benchmarks under label-, text-, and self-supervision. We hope this work offers a new perspective on ViT behavior.

</details>


### [6] [CLIP Is Shortsighted: Paying Attention Beyond the First Sentence](https://arxiv.org/abs/2602.22419)
*Marc-Antoine Lavoie,Anas Mahmoud,Aldo Zaimi,Arsene Fansi Tchango,Steven L. Waslander*

Main category: cs.CV

TL;DR: CLIP模型在复杂场景及长文本描述对齐方面存在先天偏差，DeBias-CLIP通过去除摘要句等操作缓解该问题，并在多项检索任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: CLIP训练数据多为配短文本的图片，导致其对复杂场景和细致描述的理解较弱；且即便增补长文本数据，人类及LLM生成描述常以概述句开头，模型训练时倾向只关注首句，忽视后续信息。

Method: 提出DeBias-CLIP方法：训练时删除摘要首句，对句子进行子采样以及对文本token进行填充，使模型关注分布在整个描述中的信息，而非仅关注开头。

Result: DeBias-CLIP在长文本检索任务上达到最新水平，并提升了短文本检索表现；模型在句子顺序被扰乱时表现也更加稳健。

Conclusion: DeBias-CLIP可无缝代替Long-CLIP，且无需额外增加参数，有效缓解了CLIP对长文本对齐的偏差问题。

Abstract: CLIP models learn transferable multi-modal features via image-text contrastive learning on internet-scale data. They are widely used in zero-shot classification, multi-modal retrieval, text-to-image diffusion, and as image encoders in large vision-language models. However, CLIP's pretraining is dominated by images paired with short captions, biasing the model toward encoding simple descriptions of salient objects and leading to coarse alignment on complex scenes and dense descriptions. While recent work mitigates this by fine-tuning on small-scale long-caption datasets, we identify an important common bias: both human- and LLM-generated long captions typically begin with a one-sentence summary followed by a detailed description. We show that this acts as a shortcut during training, concentrating attention on the opening sentence and early tokens and weakening alignment over the rest of the caption. To resolve this, we introduce DeBias-CLIP, which removes the summary sentence during training and applies sentence sub-sampling and text token padding to distribute supervision across all token positions. DeBias-CLIP achieves state-of-the-art long-text retrieval, improves short-text retrieval, and is less sensitive to sentence order permutations. It is a drop-in replacement for Long-CLIP with no additional trainable parameters.

</details>


### [7] [SimpleOCR: Rendering Visualized Questions to Teach MLLMs to Read](https://arxiv.org/abs/2602.22426)
*Yibo Peng,Peng Xia,Ding Zhong,Kaide Zeng,Siwei Han,Yiyang Zhou,Jiaqi Liu,Ruiyi Zhang,Huaxiu Yao*

Main category: cs.CV

TL;DR: 本文提出并解决了多模态大语言模型（MLLMs）在图像文本识别任务中，未真实利用视觉机制的问题。引入Visualized-Question (VQ)设置进行诊断，并提出高效增强方法SimpleOCR。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在视觉文字识别能力（OCR）上取得进步，但尚不清楚它们是在真正“读”图中的文字，还是主要利用文字提示中的模型先验（即捷径）。本工作关注如何判别与提升模型真实的视觉识别能力。

Method: 作者引入了VQ设置，将文字查询直接渲染到图像上，迫使模型必须依靠视觉通道解读信息。基于此，提出了SimpleOCR训练策略，通过将训练样本转换为VQ格式并随机化样式，消除模型利用文本捷径的可能，鼓励模型激活视觉文本提取路径。该方法无需模型结构改动，可直接与现有方法结合。

Result: 诊断发现Qwen2.5-VL等模型在VQ设置下性能最高下降12.7%，暴露“模态懒惰”。使用SimpleOCR后，模型获得显著性能提升，在4个OOD基准上超越基线5.4%，比最新RL方法在原始图像上的表现高2.7%，且仅用8.5K样本达成与RL方法30倍数据量相近甚至更优表现，效率极高。此外，SimpleOCR能与如NoisyRollout等先进RL策略叠加获得增益。

Conclusion: MLLMs存在“模态懒惰”即倾向捷径而非依视觉理解。SimpleOCR简单高效地促进模型真正发挥其视觉识别潜力，无需结构改动且对现有训练流程友好，可与高级策略集成，具有理论与实际应用意义。

Abstract: Despite the rapid advancements in Multimodal Large Language Models (MLLMs), a critical question regarding their visual grounding mechanism remains unanswered: do these models genuinely ``read'' text embedded in images, or do they merely rely on parametric shortcuts in the text prompt? In this work, we diagnose this issue by introducing the Visualized-Question (VQ) setting, where text queries are rendered directly onto images to structurally mandate visual engagement. Our diagnostic experiments on Qwen2.5-VL reveal a startling capability-utilization gap: despite possessing strong OCR capabilities, models suffer a performance degradation of up to 12.7% in the VQ setting, exposing a deep-seated ``modality laziness.'' To bridge this gap, we propose SimpleOCR, a plug-and-play training strategy that imposes a structural constraint on the learning process. By transforming training samples into the VQ format with randomized styles, SimpleOCR effectively invalidates text-based shortcuts, compelling the model to activate and optimize its visual text extraction pathways. Empirically, SimpleOCR yields robust gains without architectural modifications. On four representative OOD benchmarks, it surpasses the base model by 5.4% and GRPO based on original images by 2.7%, while exhibiting extreme data efficiency, achieving superior performance with 30x fewer samples (8.5K) than recent RL-based methods. Furthermore, its plug-and-play nature allows seamless integration with advanced RL strategies like NoisyRollout to yield complementary improvements. Code is available at https://github.com/aiming-lab/SimpleOCR.

</details>


### [8] [Exploring Multimodal LMMs for Online Episodic Memory Question Answering on the Edge](https://arxiv.org/abs/2602.22455)
*Giuseppe Lando,Rosario Forte,Antonino Furnari*

Main category: cs.CV

TL;DR: 本文探讨了在边缘设备上实现多模态大型语言模型（MLLMs）用于实时在线情景记忆问答的可行性，提出了分线程架构，并证明边缘方案在隐私性和效率上有优势，性能接近云端方案。


<details>
  <summary>Details</summary>
Motivation: 当前云端方案存在隐私和延迟问题，对可穿戴助手等场景不理想，有必要研究本地（边缘）部署的MLLMs在情景记忆问答上的实用性和性能。

Method: 作者提出采用两个异步线程：一个持续将视频流转为轻量文本记忆（Descriptor Thread），另一个对文本记忆进行推理和回答问答（QA Thread），并在QAEgo4D-Closed基准测试上分别以消费级GPU和企业级服务器进行实验，与云端方案对比。

Result: 在消费级8GB GPU上，端到端系统达到了51.76%的准确率，首字节响应时间为0.41秒；在企业级服务器上，准确率为54.40%，响应时间为0.88秒，而云端方案的准确率为56.00%。表明边缘方案在效能和准确率上接近云端。

Conclusion: 边缘部署的MLLMs问答系统能够在保证隐私的同时，实现接近云端性能的情景记忆检索，具有实际应用前景。

Abstract: We investigate the feasibility of using Multimodal Large Language Models (MLLMs) for real-time online episodic memory question answering. While cloud offloading is common, it raises privacy and latency concerns for wearable assistants, hence we investigate implementation on the edge. We integrated streaming constraints into our question answering pipeline, which is structured into two asynchronous threads: a Descriptor Thread that continuously converts video into a lightweight textual memory, and a Question Answering (QA) Thread that reasons over the textual memory to answer queries. Experiments on the QAEgo4D-Closed benchmark analyze the performance of Multimodal Large Language Models (MLLMs) within strict resource boundaries, showing promising results also when compared to clound-based solutions. Specifically, an end-to-end configuration running on a consumer-grade 8GB GPU achieves 51.76% accuracy with a Time-To-First-Token (TTFT) of 0.41s. Scaling to a local enterprise-grade server yields 54.40% accuracy with a TTFT of 0.88s. In comparison, a cloud-based solution obtains an accuracy of 56.00%. These competitive results highlight the potential of edge-based solutions for privacy-preserving episodic memory retrieval.

</details>


### [9] [MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation](https://arxiv.org/abs/2602.22462)
*Raiyan Jahangir,Nafiz Imtiaz Khan,Amritanand Sudheerkumar,Vladimir Filkov*

Main category: cs.CV

TL;DR: 该论文提出了MammoWise，一个基于本地多模型的乳腺X光影像报告自动生成和多任务分类工具，并展示了其在多数据集与多模型下的有效性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLM）用于医学影像报告生成时，常依赖于闭源云平台或耦合紧密的系统，不利于保护隐私、难以复现和灵活扩展。因此，亟需一种支持本地化、可扩展、支持多模型和多数据集的开源工具，以便安全且高效地辅助乳腺癌筛查影像工作。

Method: 作者开发了MammoWise系统，可以将多个开源VLM部署为本地乳腺X光影像报告生成器，并支持零样本、少样本、Chain-of-Thought 推理及可选的多模态RAG。系统兼容Ollama托管的任何VLM及任意数据集，还支持参数高效微调。

Result: 作者在VinDr-Mammo与DMID两个数据集和MedGemma、LLaVA-Med、Qwen2.5-VL三个VLM下评估了MammoWise，报告生成质量表现优秀，并且通过少样本与RAG得到提升。分类任务结果依赖所选模型和数据集，但参数高效微调后（MedGemma+QLoRA），BI-RADS准确率达到0.7545，密度准确率0.8840，钙化准确率0.9341。

Conclusion: MammoWise为乳腺X光影像智能报告和分类任务提供了本地可扩展、高效、可复现的开源工具，支持多模型、多数据集与多种推理方式，可实际用于临床和研究场景。

Abstract: Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.

</details>


### [10] [Beyond Dominant Patches: Spatial Credit Redistribution For Grounded Vision-Language Models](https://arxiv.org/abs/2602.22469)
*Niamul Hassan Samin,Md Arifur Rahman,Abdullah Ibne Hanif,Juena Ahmed Noshin,Md Ashikur Rahman*

Main category: cs.CV

TL;DR: 本文提出了一种在推理阶段无需重新训练即可减少视觉-语言模型（VLMs）幻觉（即凭空想象不存在物体）的方法。核心机制是在自注意力的早期层内重新分配激活值，从而抑制过度依赖语言先验。该方法对多种主流模型均有效，大幅降低了错误率且延迟低，非常适合实时应用。


<details>
  <summary>Details</summary>
Motivation: VLMs 常常产生'幻觉'现象：输出中包含图像中并不存在的物体。原因主要在于模型早期层激活集中于某些信息稀疏的视觉块，导致忽略上下文线索、过度依赖语言，进而想象出不存在的信息。现有减少幻觉的方法往往开销大或受限于某些模型结构。因此，亟需方法能高效、通用地缓解这一问题。

Method: 提出一种名为“空间激活重分配（SCR）”的推理时干预技术，无需对模型再训练，并结合低熵输入与注意力引导，将早期层注意力过度集中的激活值调整分散到更多上下文区域。实验覆盖Chameleon、LLaVA、Qwen系列的多种规模模型，在POPE与CHAIR基准下评估性能。

Result: SCR在POPE-Adversarial测试中将VLMs的幻觉错误率降低了4.7-6.0个百分点，CHAIR-s降低3.7-5.2个百分点（相对降幅42-51%），CHAIR-i降低2.7-4.4个百分点（相对降幅44-58%）。与其他方法相比，SCR引入的延时仅43-56毫秒，远低于同类算法；在幻觉和CIDEr表现上也全面优于对比组。

Conclusion: SCR是一种高效实用的推理时干预手段，能大幅减少VLMs的“幻觉”输出，且无需牺牲模型的辅助性能或增加显著延迟，对实际部署极具应用价值。消融实验还证实，注意力引导的源位置选择机制至关重要，有效支撑了空间激活崩塌是导致幻觉的主要原因。

Abstract: Vision-language models (VLMs) frequently hallucinate objects absent from the input image. We trace this failure to spatial credit collapse: activation credit concentrating on sparse visual patches in early transformer layers, which suppresses contextual evidence and increases reliance on language priors. We introduce Spatial Credit Redistribution (SCR), a training-free inference-time intervention that redistributes hidden-state activation from high-attention source patches to their context, guided by low-entropy inputs. We evaluate six model families (Chameleon, LLaVA, and Qwen, including both Qwen-VL and Qwen2-VL) at scales of 7B, 13B, and 30B, on POPE and CHAIR benchmarks. SCR reduces hallucination by ~4.7-6.0 percentage points on POPE-Adversarial, cuts CHAIR-s by 3.7-5.2 percentage points (42-51 percent relative), and CHAIR-i by 2.7-4.4 percentage points (44-58 percent relative), and preserves CIDEr within 0.8 percentage points. Gains are largest for low-entropy inputs, consistent with the theoretical framework. SCR incurs only 43-56 ms overhead (small models: +43-46 ms; large models: +54-56 ms), roughly 3-6 times lower than OPERA and VCD and 1.3-1.7 times lower than OVCD (+72 ms), while Pareto-dominating all three on both hallucination rate and CIDEr, making it practical for real-time settings. A controlled ablation confirms that attention-guided source selection is essential: replacing it with uniform random selection reduces hallucination rate gains from ~4.7-6.0 percentage points to only ~2.6-3.4 percentage points, pointing to credit-collapse as the key driver.

</details>


### [11] [Pix2Key: Controllable Open-Vocabulary Retrieval with Semantic Decomposition and Self-Supervised Visual Dictionary Learning](https://arxiv.org/abs/2602.22510)
*Guoyizhe Wei,Yang Jiao,Nan Xi,Zhishen Huang,Jingjing Meng,Rama Chellappa,Yan Gao*

Main category: cs.CV

TL;DR: 提出了Pix2Key方法，通过开放词汇视觉字典表示查询与候选图像，实现意图感知约束匹配和多样性重排序，并引入自监督预训练组件V-Dict-AE提升细粒度属性理解，取得了优异的检索性能。


<details>
  <summary>Details</summary>
Motivation: 经典CIR方法依赖有监督三元组，可能丢失细粒度信息；零样本方法常将图像转描述后与编辑合成，难以覆盖用户隐含意图且结果重复。为解决这些问题，需要更好地表达查询意图和提升检索多样性。

Method: 提出Pix2Key方法，将查询与候选都表示为开放词汇视觉字典，在统一的嵌入空间中进行意图感知约束匹配和多样性重排序；引入仅用图像自监督的V-Dict-AE预训练，提升视觉字典的属性表达能力，无需CIR特有标注。

Result: 在DFMM-Compose基准上，Pix2Key模型的Recall@10提升至多3.2个百分点，结合V-Dict-AE后进一步提升2.3个百分点，同时改善了意图一致性并保持高多样性。

Conclusion: Pix2Key通过视觉字典嵌入，实现了更强的意图理解与结果多样性，配合自监督学习显著优化了CIR性能，可作为图像检索领域的新方法基础。

Abstract: Composed Image Retrieval (CIR) uses a reference image plus a natural-language edit to retrieve images that apply the requested change while preserving other relevant visual content. Classic fusion pipelines typically rely on supervised triplets and can lose fine-grained cues, while recent zero-shot approaches often caption the reference image and merge the caption with the edit, which may miss implicit user intent and return repetitive results. We present Pix2Key, which represents both queries and candidates as open-vocabulary visual dictionaries, enabling intent-aware constraint matching and diversity-aware reranking in a unified embedding space. A self-supervised pretraining component, V-Dict-AE, further improves the dictionary representation using only images, strengthening fine-grained attribute understanding without CIR-specific supervision. On the DFMM-Compose benchmark, Pix2Key improves Recall@10 up to 3.2 points, and adding V-Dict-AE yields an additional 2.3-point gain while improving intent consistency and maintaining high list diversity.

</details>


### [12] [DisQ-HNet: A Disentangled Quantized Half-UNet for Interpretable Multimodal Image Synthesis Applications to Tau-PET Synthesis from T1 and FLAIR MRI](https://arxiv.org/abs/2602.22545)
*Agamdeep S. Chopra,Caitlin Neher,Tianyi Ren,Juampablo E. Heras Rivera,Mehmet Kurt*

Main category: cs.CV

TL;DR: 本文提出了一种结合T1加权和FLAIR MRI的生成网络DisQ-HNet，用于合成阿尔茨海默病中关键的tau-PET成像，且能解释不同MRI模态的贡献。


<details>
  <summary>Details</summary>
Motivation: tau-PET可以直接反映阿尔茨海默病的病理变化，但因高昂费用和获取受限，亟需MRI数据驱动的替代生成方法。

Method: 提出DisQ-HNet模型，利用带有分区信息的编码器（基于Partial Information Decomposition理论）将特征分为冗余、独特和互补三部分，引入Half-UNet解码器并用结构边缘引导伪跳跃连接，提升结构信息利用效率，并实现对每种MRI模态生成贡献的归因。

Result: 在与多种模型（VAE、VQ-VAE、UNet）对比下，DisQ-HNet不仅显著提升了tau-PET合成效果，还能更好地保留下游阿尔茨海默病临床任务（如Braak分期、tau定位和分类）相关的病理信号，并通过PID-Shapley分析量化了不同模态的贡献。

Conclusion: DisQ-HNet在高还原度合成tau-PET的同时，增强了对疾病相关信息的保留和可解释性，有望作为低成本、普适的AD影像学辅助工具。

Abstract: Tau positron emission tomography (tau-PET) provides an in vivo marker of Alzheimer's disease pathology, but cost and limited availability motivate MRI-based alternatives. We introduce DisQ-HNet (DQH), a framework that synthesizes tau-PET from paired T1-weighted and FLAIR MRI while exposing how each modality contributes to the prediction. The method combines (i) a Partial Information Decomposition (PID)-guided, vector-quantized encoder that partitions latent information into redundant, unique, and complementary components, and (ii) a Half-UNet decoder that preserves anatomical detail using pseudo-skip connections conditioned on structural edge cues rather than direct encoder feature reuse. Across multiple baselines (VAE, VQ-VAE, and UNet), DisQ-HNet maintains reconstruction fidelity and better preserves disease-relevant signal for downstream AD tasks, including Braak staging, tau localization, and classification. PID-based Shapley analysis provides modality-specific attribution of synthesized uptake patterns.

</details>


### [13] [DrivePTS: A Progressive Learning Framework with Textual and Structural Enhancement for Driving Scene Generation](https://arxiv.org/abs/2602.22549)
*Zhechao Wang,Yiming Zeng,Lufan Ma,Zeqing Fu,Chen Bai,Ziyao Lin,Cheng Lu*

Main category: cs.CV

TL;DR: 本文提出了一种全新的自动驾驶场景生成方法DrivePTS，提升了多样性、真实性和可控性，优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶场景合成方法，虽然能够利用高清地图和3D包围框生成场景，但在条件控制独立时容易失败，且合成场景的语义细节和结构细节均不足，限制了自动驾驶系统的鲁棒性测试。

Method: 提出DrivePTS框架：1）采用逐步学习机制结合互信息约束，减少几何条件间依赖，提高条件独立时的生成能力；2）应用视觉-语言模型生成多视角、细粒度的语义文本描述，丰富背景语义；3）设计基于频率的结构损失，加强对前景结构（高频信息）的捕捉与还原。

Result: 实验表明，DrivePTS生成的场景在真实度和可控性方面显著优于已有方法，尤其在常规方法无法生成的稀有场景中表现出强泛化能力。

Conclusion: DrivePTS不仅提升了合成自动驾驶场景的质量和多样性，还增强了泛化性，为自动驾驶决策系统的数据补齐和极端场景测试提供了新工具。

Abstract: Synthesis of diverse driving scenes serves as a crucial data augmentation technique for validating the robustness and generalizability of autonomous driving systems. Current methods aggregate high-definition (HD) maps and 3D bounding boxes as geometric conditions in diffusion models for conditional scene generation. However, implicit inter-condition dependency causes generation failures when control conditions change independently. Additionally, these methods suffer from insufficient details in both semantic and structural aspects. Specifically, brief and view-invariant captions restrict semantic contexts, resulting in weak background modeling. Meanwhile, the standard denoising loss with uniform spatial weighting neglects foreground structural details, causing visual distortions and blurriness. To address these challenges, we propose DrivePTS, which incorporates three key innovations. Firstly, our framework adopts a progressive learning strategy to mitigate inter-dependency between geometric conditions, reinforced by an explicit mutual information constraint. Secondly, a Vision-Language Model is utilized to generate multi-view hierarchical descriptions across six semantic aspects, providing fine-grained textual guidance. Thirdly, a frequency-guided structure loss is introduced to strengthen the model's sensitivity to high-frequency elements, improving foreground structural fidelity. Extensive experiments demonstrate that our DrivePTS achieves state-of-the-art fidelity and controllability in generating diverse driving scenes. Notably, DrivePTS successfully generates rare scenes where prior methods fail, highlighting its strong generalization ability.

</details>


### [14] [SwiftNDC: Fast Neural Depth Correction for High-Fidelity 3D Reconstruction](https://arxiv.org/abs/2602.22565)
*Kang Han,Wei Xiang,Lu Yu,Mathew Wyatt,Gaowen Liu,Ramana Rao Kompella*

Main category: cs.CV

TL;DR: SwiftNDC 提出了一种基于神经深度校正的新颖 3D 重建方法，显著加速并提升了 3DGS 的网格重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有快速 3D 重建方法存在尺度漂移、多视角不一致和几何精度不足等问题，导致后续重建需大量精修，影响效率和质量。

Method: SwiftNDC 框架通过神经深度校正模型生成跨视图一致的深度图，利用反投影和鲁棒重投影误差过滤生成高质量稠密点云，为 3DGS 等重建提供精准初始几何，大幅减少优化迭代次数。

Result: 在五个数据集上实验证明，SwiftNDC 在网格重建和新视图合成场景下都有效提升重建速度和渲染质量。

Conclusion: 神经深度优化和高质量几何初始化的结合，极大提升了 3D 重建的速度和保真度，推动了高效高质量 3D 重建的发展。

Abstract: Depth-guided 3D reconstruction has gained popularity as a fast alternative to optimization-heavy approaches, yet existing methods still suffer from scale drift, multi-view inconsistencies, and the need for substantial refinement to achieve high-fidelity geometry. Here, we propose SwiftNDC, a fast and general framework built around a Neural Depth Correction field that produces cross-view consistent depth maps. From these refined depths, we generate a dense point cloud through back-projection and robust reprojection-error filtering, obtaining a clean and uniformly distributed geometric initialization for downstream reconstruction. This reliable dense geometry substantially accelerates 3D Gaussian Splatting (3DGS) for mesh reconstruction, enabling high-quality surfaces with significantly fewer optimization iterations. For novel-view synthesis, SwiftNDC can also improve 3DGS rendering quality, highlighting the benefits of strong geometric initialization. We conduct a comprehensive study across five datasets, including two for mesh reconstruction, as well as three for novel-view synthesis. SwiftNDC consistently reduces running time for accurate mesh reconstruction and boosts rendering fidelity for view synthesis, demonstrating the effectiveness of combining neural depth refinement with robust geometric initialization for high-fidelity and efficient 3D reconstruction.

</details>


### [15] [Quality-Aware Robust Multi-View Clustering for Heterogeneous Observation Noise](https://arxiv.org/abs/2602.22568)
*Peihan Wu,Guanjie Cheng,Yufei Tong,Meng Xi,Shuiguang Deng*

Main category: cs.CV

TL;DR: 该论文提出了一个新颖的多视图聚类框架QARMVC，能更精细地应对异质性噪声状况，并在多项基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的多视图聚类方法仅以二元方式处理噪声，忽略了真实场景中噪声强度的连续变化，因此需要更细致、针对性的鲁棒聚类方法。

Method: QARMVC通过信息瓶颈机制抽取语义特征，并根据重构误差为每个样本分配质量分数。这些分数在特征级通过加权对比学习抑制噪声传播，在融合级则引导高质量的全局聚合及局部视图对齐。

Result: 大量实验证明，QARMVC在五个基准数据集上均超过当前主流方法，尤其在噪声异质性明显时表现突出。

Conclusion: QARMVC有效提升了多视图聚类在异质性噪声下的稳健性和准确性，具有较高应用潜力。

Abstract: Deep multi-view clustering has achieved remarkable progress but remains vulnerable to complex noise in real-world applications. Existing noisy robust methods predominantly rely on a simplified binary assumption, treating data as either perfectly clean or completely corrupted. This overlooks the prevalent existence of heterogeneous observation noise, where contamination intensity varies continuously across data. To bridge this gap, we propose a novel framework termed Quality-Aware Robust Multi-View Clustering (QARMVC). Specifically, QARMVC employs an information bottleneck mechanism to extract intrinsic semantics for view reconstruction. Leveraging the insight that noise disrupts semantic integrity and impedes reconstruction, we utilize the resulting reconstruction discrepancy to precisely quantify fine-grained contamination intensity and derive instance-level quality scores. These scores are integrated into a hierarchical learning strategy: at the feature level, a quality-weighted contrastive objective is designed to adaptively suppress the propagation of noise; at the fusion level, a high-quality global consensus is constructed via quality-weighted aggregation, which is subsequently utilized to align and rectify local views via mutual information maximization. Extensive experiments on five benchmark datasets demonstrate that QARMVC consistently outperforms state-of-the-art baselines, particularly in scenarios with heterogeneous noise intensities.

</details>


### [16] [Guidance Matters: Rethinking the Evaluation Pitfall for Text-to-Image Generation](https://arxiv.org/abs/2602.22570)
*Dian Xie,Shitong Shao,Lichen Bai,Zikai Zhou,Bojun Cheng,Shuo Yang,Jun Wu,Zeke Xie*

Main category: cs.CV

TL;DR: 本文重新评估了扩散模型中的引导方法，发现当前的评价方式存在偏见，提出了新的公平评价框架，并开展了实证分析。


<details>
  <summary>Details</summary>
Motivation: 虽然新型扩散模型引导方法不断涌现，并声称带来了更好的生成质量和用户偏好，但目前的评价方式（尤其是利用人类偏好打分）可能存在系统性偏见。作者希望检验这些方法是否真的带来了实质性改进。

Method: 1）揭示了常用人类偏好模型对大规模引导值（CFG scale）具有偏见这一评测陷阱。2）提出了新的评价框架GA-Eval，用于校准和公平比较不同扩散引导方法。3）设计了一种方法（TDG），证明在传统评价体系下可以显著提升分数，但实际上效果并不可靠。4）在传统和GA-Eval框架下实证评价了八种主流引导方法。

Result: 单纯提升CFG scale可以让传统评价指标大幅提升，甚至与大多新引导方法持平，但画质受损。所有引导方法在GA-Eval的胜率均低于标准CFG。

Conclusion: 本文指出当前主流的评价方法存在重大局限，并呼吁社区重新思考扩散模型引导方法的评价范式和未来研究方向。

Abstract: Classifier-free guidance (CFG) has helped diffusion models achieve great conditional generation in various fields. Recently, more diffusion guidance methods have emerged with improved generation quality and human preference. However, can these emerging diffusion guidance methods really achieve solid and significant improvements? In this paper, we rethink recent progress on diffusion guidance. Our work mainly consists of four contributions. First, we reveal a critical evaluation pitfall that common human preference models exhibit a strong bias towards large guidance scales. Simply increasing the CFG scale can easily improve quantitative evaluation scores due to strong semantic alignment, even if image quality is severely damaged (e.g., oversaturation and artifacts). Second, we introduce a novel guidance-aware evaluation (GA-Eval) framework that employs effective guidance scale calibration to enable fair comparison between current guidance methods and CFG by identifying the effects orthogonal and parallel to CFG effects. Third, motivated by the evaluation pitfall, we design Transcendent Diffusion Guidance (TDG) method that can significantly improve human preference scores in the conventional evaluation framework but actually does not work in practice. Fourth, in extensive experiments, we empirically evaluate recent eight diffusion guidance methods within the conventional evaluation framework and the proposed GA-Eval framework. Notably, simply increasing the CFG scales can compete with most studied diffusion guidance methods, while all methods suffer severely from winning rate degradation over standard CFG. Our work would strongly motivate the community to rethink the evaluation paradigm and future directions of this field.

</details>


### [17] [Causal Motion Diffusion Models for Autoregressive Motion Generation](https://arxiv.org/abs/2602.22594)
*Qing Yu,Akihisa Watanabe,Kent Fujiwara*

Main category: cs.CV

TL;DR: 作者提出了一种用于自回归运动生成的因果运动扩散模型CMDM，能高效、实时且高质量地生成符合语义的人体动作，在多个评测集上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有人体动作扩散模型要么采用全序列双向生成，难以满足实时与因果需求；要么用自回归生成，但存在不稳定与误差累积问题。因此，需要一种既具备时序因果性、又能实现高质量实时生成的方法。

Method: 提出CMDM框架，基于Motion-Language-Aligned Causal VAE（MAC-VAE）将动作编码为因果潜在表示，并用自回归扩散Transformer进行时序降噪。为加快推理，引入逐帧采样与因果不确定性，仅依赖部分已降噪帧预测后续帧。

Result: 在HumanML3D和SnapMoGen数据集上，CMDM在语义契合度、时间平滑性等指标上均优于现有扩散和自回归模型，推理速度大幅提升。

Conclusion: CMDM有效结合了因果性、自回归与扩散模型三者优势，兼顾生成质量与实时性，适用于文本驱动、流式和长序列动作实时合成。

Abstract: Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.

</details>


### [18] [Instruction-based Image Editing with Planning, Reasoning, and Generation](https://arxiv.org/abs/2602.22624)
*Liya Ji,Chenyang Qi,Qifeng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态的链式思维(CoT)方法，实现了更强大的指令驱动图像编辑，能够应对更复杂的场景编辑任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑需要对场景有较高理解和生成能力，但现有方法多为单模态，限制了编辑质量。作者希望通过多模态模型加强理解与生成的连接，从而提升复杂指令下的编辑效果。

Method: 方法上，将指令编辑任务分成三步：1)CoT规划——大语言模型对指令进行分解，制定合理的子提示；2)编辑区域推理——利用多模态大模型训练编辑区域生成网络，定位编辑目标；3)基于提示的编辑——结合大型文本到图像扩散模型，利用提示信息进行图像编辑。

Result: 实验表明，该方法在复杂的真实场景图像编辑任务中，编辑能力优越，表现具竞争力。

Conclusion: 多模态链式思维方法能有效提升基于指令图像编辑的智能化和复杂性适应能力，为高质量交互式图像编辑提供新策略。

Abstract: Editing images via instruction provides a natural way to generate interactive content, but it is a big challenge due to the higher requirement of scene understanding and generation. Prior work utilizes a chain of large language models, object segmentation models, and editing models for this task. However, the understanding models provide only a single modality ability, restricting the editing quality. We aim to bridge understanding and generation via a new multi-modality model that provides the intelligent abilities to instruction-based image editing models for more complex cases. To achieve this goal, we individually separate the instruction editing task with the multi-modality chain of thought prompts, i.e., Chain-of-Thought (CoT) planning, editing region reasoning, and editing. For Chain-of-Thought planning, the large language model could reason the appropriate sub-prompts considering the instruction provided and the ability of the editing network. For editing region reasoning, we train an instruction-based editing region generation network with a multi-modal large language model. Finally, a hint-guided instruction-based editing network is proposed for editing image generations based on the sizeable text-to-image diffusion model to accept the hints for generation. Extensive experiments demonstrate that our method has competitive editing abilities on complex real-world images.

</details>


### [19] [QuadSync: Quadrifocal Tensor Synchronization via Tucker Decomposition](https://arxiv.org/abs/2602.22639)
*Daniel Miao,Gilad Lerman,Joe Kileel*

Main category: cs.CV

TL;DR: 论文提出了一种基于四焦张量（quadrifocal tensor）的多相机同步新方法，通过Tucker分解与优化算法提高了重建精度与实用性。


<details>
  <summary>Details</summary>
Motivation: 以往在结构光流（Structure from Motion, SfM）问题中，四焦张量虽然蕴含更多信息，但被认为不切实际，仅具理论意义。作者希望挑战这一观点，充分挖掘高阶张量同步在多相机几何重建中的潜力。

Method: 提出块四焦张量模型，并证明其Tucker分解的因子矩阵为相机参数矩阵，且多线性秩固定。随后利用Tucker分解、交替方向乘子法（ADMM）、迭代重加权最小二乘等方法，开发了首个四焦张量同步算法，并提出联合同步双焦、三焦和四焦张量的新算法。

Result: 在现代数据集上进行了数值实验，结果表明提出的方法在多相机重建和同步中取得了明确的效果和精度提升。

Conclusion: 论文验证了高阶张量（尤其是四焦张量）在多相机同步问题中的实用性和重要性，证明这些理论工具完全有潜力转化为实际SfM系统中有效的计算方法。

Abstract: In structure from motion, quadrifocal tensors capture more information than their pairwise counterparts (essential matrices), yet they have often been thought of as impractical and only of theoretical interest. In this work, we challenge such beliefs by providing a new framework to recover $n$ cameras from the corresponding collection of quadrifocal tensors. We form the block quadrifocal tensor and show that it admits a Tucker decomposition whose factor matrices are the stacked camera matrices, and which thus has a multilinear rank of (4,~4,~4,~4) independent of $n$. We develop the first synchronization algorithm for quadrifocal tensors, using Tucker decomposition, alternating direction method of multipliers, and iteratively reweighted least squares. We further establish relationships between the block quadrifocal, trifocal, and bifocal tensors, and introduce an algorithm that jointly synchronizes these three entities. Numerical experiments demonstrate the effectiveness of our methods on modern datasets, indicating the potential and importance of using higher-order information in synchronization.

</details>


### [20] [Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache](https://arxiv.org/abs/2602.22654)
*Bowen Cui,Yuanbin Wang,Huajiang Xu,Biaolong Chen,Aixi Zhang,Hao Jiang,Zhengzheng Jin,Xu Liu,Pipei Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为DPCache的去噪扩散模型加速框架，通过建立路径感知的代价张量和动态规划，全局优化采样关键帧，实现高效加速且质量损失极小，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型在图像和视频生成任务中表现优异，但推理阶段的多步采样过程计算量大，影响实际应用。现有无训练缓存加速方法存在误差累积与伪影，主要因为忽略了整个采样路径结构。

Method: DPCache将扩散采样加速看作全局路径规划问题。首先用少量校准集数据，构建一个路径相关的代价张量，度量跳步采样的累计误差。随后，利用动态规划算法，依据代价张量全局挑选最优关键时刻，推理时仅对这些关键步完整推断，其余用缓存预测。

Result: 在DiT、FLUX和HunyuanVideo等扩散模型上实验证明，DPCache在4.87倍加速时，质量损失极小（ImageReward提升+0.031），在3.54倍加速下甚至超过全量推断（+0.028 ImageReward），全面超越现有加速方法。

Conclusion: DPCache通过全局路径感知调度，有效跳步采样并显著加速扩散模型推理，兼顾效率和生成质量，具备较强实用价值。

Abstract: Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.

</details>


### [21] [WaterVideoQA: ASV-Centric Perception and Rule-Compliant Reasoning via Multi-Modal Agents](https://arxiv.org/abs/2602.22923)
*Runwei Guan,Shaofeng Liang,Ningwei Ouyang,Weichen Fei,Shanliang Yao,Wei Dai,Chenhao Ge,Penglei Sun,Xiaohui Zhu,Tao Huang,Ryan Wen Liu,Hui Xiong*

Main category: cs.CV

TL;DR: 本文提出了WaterVideoQA，这是首个针对全水域环境的大规模视频问答基准，用于提升自主水面船舶在复杂环境下的智能认知与推理能力，并引入了NaviMind多智能体神经符号系统以实现高阶推理。实验显示该方法优于现有基线，推动了自动航船智能交互的新标准。


<details>
  <summary>Details</summary>
Motivation: 当前自主导航主要依赖于被动视觉感知，缺乏知识驱动和交互的环境认知能力，特别是在复杂且要求高的海事导航中，这种不足会影响到安全与精确操控。因此需要新的基准和方法来推动智能船舶从感知走向认知推理水平。

Method: 提出了WaterVideoQA基准数据集，涵盖六类水域、3029个视频片段，并包含多种光照和天气变量。还设计了NaviMind系统，通过自适应语义路由、情境感知分层推理和自主反思校验，从而提升自主船舶的开放性认知推理能力。

Result: 实验结果表明，NaviMind系统与WaterVideoQA基准相结合后，在开放性水域问答推理任务上，显著超越了现有的基线方法表现。

Conclusion: NaviMind和WaterVideoQA的结合为动态海事环境中智能、可信的交互树立了新范式，有助于自主船舶从简单模式匹配迈向更加可靠、可解释的决策。

Abstract: While autonomous navigation has achieved remarkable success in passive perception (e.g., object detection and segmentation), it remains fundamentally constrained by a void in knowledge-driven, interactive environmental cognition. In the high-stakes domain of maritime navigation, the ability to bridge the gap between raw visual perception and complex cognitive reasoning is not merely an enhancement but a critical prerequisite for Autonomous Surface Vessels to execute safe and precise maneuvers. To this end, we present WaterVideoQA, the first large-scale, comprehensive Video Question Answering benchmark specifically engineered for all-waterway environments. This benchmark encompasses 3,029 video clips across six distinct waterway categories, integrating multifaceted variables such as volatile lighting and dynamic weather to rigorously stress-test ASV capabilities across a five-tier hierarchical cognitive framework. Furthermore, we introduce NaviMind, a pioneering multi-agent neuro-symbolic system designed for open-ended maritime reasoning. By synergizing Adaptive Semantic Routing, Situation-Aware Hierarchical Reasoning, and Autonomous Self-Reflective Verification, NaviMind transitions ASVs from superficial pattern matching to regulation-compliant, interpretable decision-making. Experimental results demonstrate that our framework significantly transcends existing baselines, establishing a new paradigm for intelligent, trustworthy interaction in dynamic maritime environments.

</details>


### [22] [SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs](https://arxiv.org/abs/2602.22716)
*Guanting Ye,Qiyan Zhao,Wenhao Yu,Liangyu Yuan,Mingkai Li,Xiaofeng Zhang,Jianmin Ji,Yanyong Zhang,Qing Jiang,Ka-Veng Yuen*

Main category: cs.CV

TL;DR: 本文提出一种基于球坐标的位置编码方法SoPE，有效提升了3D大模型对空间结构的理解与表达能力，并在多项3D场景任务和实际应用中验证了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D视觉-语言大模型常用的RoPE编码无法很好地表达空间方向和三维结构，影响3D多模态任务表现。

Method: 将点云数据的Token索引映射到球坐标系，实现空间位置与方向的统一建模，同时采用多尺度频率混合策略，融合不同频域的特征信息。

Result: 在多个3D场景基准上实验验证新方法优于传统RoPE编码，并展示了在真实世界部署中的出色泛化能力。

Conclusion: SoPE方法增强了模型对点云几何结构的表达和空间感知，为3D多模态学习提供了更一致、表达力更强的几何表示。

Abstract: 3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. The vanilla RoPE formulation fails to preserve essential three-dimensional spatial structures when encoding 3D tokens, and its relative distance computation overlooks angular dependencies, hindering the model's ability to capture directional variations in visual representations. To overcome these limitations, we introduce Spherical Coordinate-based Positional Embedding (SoPE). Our method maps point-cloud token indices into a 3D spherical coordinate space, enabling unified modeling of spatial locations and directional angles. This formulation preserves the inherent geometric structure of point-cloud data, enhances spatial awareness, and yields more consistent and expressive geometric representations for multimodal learning. In addition, we introduce a multi-scale frequency mixing strategy to fuse feature information across different frequency domains. Experimental results on multiple 3D scene benchmarks validate the effectiveness of our approach, while real-world deployment experiments further demonstrate its strong generalization capability.

</details>


### [23] [IRSDE-Despeckle: A Physics-Grounded Diffusion Model for Generalizable Ultrasound Despeckling](https://arxiv.org/abs/2602.22717)
*Shuoqi Chen,Yujia Wu,Geoffrey P. Luke*

Main category: cs.CV

TL;DR: 本文提出一种基于扩散模型的超声图像去斑方法，该方法在仿真和真实超声数据上均优于现有基线，且可对预测不确定性进行量化。


<details>
  <summary>Details</summary>
Motivation: 超声图像广泛用于实时无创诊断，但斑点噪声和伪影降低了图像质量，影响医生解读，因此亟需有效的去斑算法以提升诊断准确性。

Method: 作者基于Image Restoration Stochastic Differential Equations框架，提出扩散型超声去斑方法，并利用Matlab UltraSound Toolbox从无斑的磁共振图像中仿真生成大规模成对数据集进行有监督训练，模型可抑制斑点同时保留解剖结构。

Result: 在模拟测试集上，该方法在视觉质量和保边性能上优于传统滤波和主流学习型去斑基线，并能通过跨模型方差量化预测不确定性，不确定性高的区域也的确对应重建误差更大。此外方法对不同探头参数敏感，存在域适应需求。

Conclusion: 该方法为超声去斑提供了性能更优的发展方向，并能评估预测不确定性，但临床应用仍需针对设备、参数的多样性进行更充分的训练与适应。

Abstract: Ultrasound imaging is widely used for real-time, noninvasive diagnosis, but speckle and related artifacts reduce image quality and can hinder interpretation. We present a diffusion-based ultrasound despeckling method built on the Image Restoration Stochastic Differential Equations framework. To enable supervised training, we curate large paired datasets by simulating ultrasound images from speckle-free magnetic resonance images using the Matlab UltraSound Toolbox. The proposed model reconstructs speckle-suppressed images while preserving anatomically meaningful edges and contrast. On a held-out simulated test set, our approach consistently outperforms classical filters and recent learning-based despeckling baselines. We quantify prediction uncertainty via cross-model variance and show that higher uncertainty correlates with higher reconstruction error, providing a practical indicator of difficult or failure-prone regions. Finally, we evaluate sensitivity to simulation probe settings and observe domain shift, motivating diversified training and adaptation for robust clinical deployment.

</details>


### [24] [HulluEdit: Single-Pass Evidence-Consistent Subspace Editing for Mitigating Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2602.22727)
*Yangguang Lin,Quan Fang,Yufei Li,Jiachen Sun,Junyu Gao,Jitao Sang*

Main category: cs.CV

TL;DR: 本文提出了一种高效、无参考模型的单步干预方法（HulluEdit），通过正交子空间编辑减少大规模视觉-语言模型（LVLMs）的幻觉错误，在不影响真实视觉信息的情况下，提高模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提升LVLMs消除物体幻觉能力时，常常面临效率低和需要昂贵参考模型的问题，或者采用静态修改，容易压制真实视觉证据，影响模型性能。因此需要新的方法兼顾效率和准确性。

Method: HulluEdit方法将LVLMs的隐状态分解为正交的三个子空间：视觉证据、冲突先验和残余不确定性。通过对先验子空间进行选择性抑制，实现对幻觉模式的精准干预，同时在数学上保证不影响视觉信息部分。此方法为单步、无需参考模型的高效编辑框架。

Result: 在POPE和CHAIR等主流基准上，HulluEdit显著减少了幻觉发生率，并且在各种架构下稳定优于对比解码和静态子空间编辑等基线。同时，该方法在MME等任务上保持了模型的泛化能力和推理效率。

Conclusion: HulluEdit为提升大规模视觉-语言模型可信度提供了新途径，在保证效率的同时显著抑制幻觉生成，且不影响原有能力，优于现有主流方法。

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly hinders their reliable deployment. Existing methods struggle to balance efficiency and accuracy: they often require expensive reference models and multiple forward passes, or apply static edits that risk suppressing genuine visual evidence. To address this, we introduce HulluEdit, a single-pass, reference-free intervention framework. Our core innovation is orthogonal subspace editing: we decompose the hidden states of the model into orthogonal subspaces - visual evidence, conflicting priors, and residual uncertainty - enabling selective suppression of hallucinatory patterns without interfering with visual grounding. This approach mathematically guarantees that edits applied to the prior subspace leave the visual component entirely unaffected. Extensive experiments show that HulluEdit achieves state-of-the-art hallucination reduction on benchmarks including POPE and CHAIR across diverse architectures, while preserving general capabilities on MME and maintaining efficient inference. Our method consistently outperforms contrastive decoding and static subspace editing baselines, offering a new pathway toward more trustworthy LVLMs.

</details>


### [25] [Asymmetric Idiosyncrasies in Multimodal Models](https://arxiv.org/abs/2602.22734)
*Muzi Tao,Chufan Shi,Huijuan Wang,Shengbang Tong,Xuezhe Ma*

Main category: cs.CV

TL;DR: 本文系统分析了生成式图像描述（caption）模型的风格特征及其对文本生成图像(T2I)模型的影响，发现图像描述模型具有可识别的风格签名，但这些特征在图像生成过程中会大大减弱。


<details>
  <summary>Details</summary>
Motivation: 目前，图像描述（caption）模型在生成文本时各具风格特征，但这些风格特征如何影响下游的文本生成图像（text-to-image, T2I）任务仍不明确。论文旨在系统量化caption模型的风格痕迹以及这些风格在跨模态生成流程中是否保留。

Method: 作者设计了一种分类框架：用神经网络分别基于caption文本和由caption生成的图像，去预测它们来源于哪个caption模型。对比分类准确率，进一步分析风格特征在图像端的保留情况。

Result: 实验表明，仅用caption文本分类，准确率高达99.7%，说明caption模型风格鲜明；用生成图像分类，即使采用先进模型，准确率也在50%以下，表明图像端大部分风格特征丢失。分析显示，诸如细节描述、色彩纹理强调、场景对象布局等差异在图像中难以保持。

Conclusion: 该分类框架为量化caption模型风格和T2I系统的prompt跟随能力提供了一种新方法。实验发现cross-modal（跨模态）生成过程会弱化caption风格痕迹，说明T2I模型在细致还原caption特征方面存在局限。

Abstract: In this work, we study idiosyncrasies in the caption models and their downstream impact on text-to-image models. We design a systematic analysis: given either a generated caption or the corresponding image, we train neural networks to predict the originating caption model. Our results show that text classification yields very high accuracy (99.70\%), indicating that captioning models embed distinctive stylistic signatures. In contrast, these signatures largely disappear in the generated images, with classification accuracy dropping to at most 50\% even for the state-of-the-art Flux model. To better understand this cross-modal discrepancy, we further analyze the data and find that the generated images fail to preserve key variations present in captions, such as differences in the level of detail, emphasis on color and texture, and the distribution of objects within a scene. Overall, our classification-based framework provides a novel methodology for quantifying both the stylistic idiosyncrasies of caption models and the prompt-following ability of text-to-image systems.

</details>


### [26] [TrajTok: Learning Trajectory Tokens enables better Video Understanding](https://arxiv.org/abs/2602.22779)
*Chenhao Zheng,Jieyu Zhang,Jianing Zhang,Weikai Huang,Ashutosh Kumar,Quan Kong,Oncel Tuzel,Chun-Liang Li,Ranjay Krishna*

Main category: cs.CV

TL;DR: 提出了一种新的视频tokenizer模块TrajTok，无需外部复杂步骤，能高效提升视频模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频模型通过patch划分产生过多冗余token，影响效率和可扩展性。新兴的运动轨迹tokenizer虽改进了token数量，但依赖复杂的外部分割和跟踪，速度慢且不适应具体任务。

Method: 作者提出TrajTok：一种端到端、一体化并与下游视频模型共同训练的视频tokenizer。它内含统一分割器，在空间和时间维度隐式聚类像素，一次前向传递直接输出对象轨迹token，自动按语义复杂度适配token粒度。

Result: 基于TrajTok训练的视频CLIP模型（TrajViT2）在分类和检索基准上均取得最优表现，效率匹敌最优token合并法；TrajTok模块还能作为特征探测头（TrajAdapter）或视觉-语言模型连接件（TrajVLM），在长视频推理等任务表现突出。

Conclusion: TrajTok无需高精度分割即可适应多种任务，结构轻便、效率高且提升模型下游表现，不限于tokenizer角色，还可拓展多种应用场景。

Abstract: Tokenization in video models, typically through patchification, generates an excessive and redundant number of tokens. This severely limits video efficiency and scalability. While recent trajectory-based tokenizers offer a promising solution by decoupling video duration from token count, they rely on complex external segmentation and tracking pipelines that are slow and task-agnostic. We propose TrajTok, an end-to-end video tokenizer module that is fully integrated and co-trained with video models for a downstream objective, dynamically adapting its token granularity to semantic complexity, independent of video duration. TrajTok contains a unified segmenter that performs implicit clustering over pixels in both space and time to directly produce object trajectories in a single forward pass. By prioritizing downstream adaptability over pixel-perfect segmentation fidelity, TrajTok is lightweight and efficient, yet empirically improves video understanding performance. With TrajTok, we implement a video CLIP model trained from scratch (TrajViT2). It achieves the best accuracy at scale across both classification and retrieval benchmarks, while maintaining efficiency comparable to the best token-merging methods. TrajTok also proves to be a versatile component beyond its role as a tokenizer. We show that it can be seamlessly integrated as either a probing head for pretrained visual features (TrajAdapter) or an alignment connector in vision-language models (TrajVLM) with especially strong performance in long-video reasoning.

</details>


### [27] [SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation](https://arxiv.org/abs/2602.22785)
*Ling Wang,Hao-Xiang Guo,Xinzhou Wang,Fuchun Sun,Kai Sun,Pengkun Liu,Hang Xiao,Zhong Wang,Guangyuan Fu,Eric Li,Yang Liu,Yikai Wang*

Main category: cs.CV

TL;DR: SceneTransporter提出了一种从单张图片生成结构化3D场景的新方法，解决了现有方法在实例组织方面的不足，通过引入全局相关分配和最优传输，实现了更加连贯和高质量的3D场景生成。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景生成方法虽然能生成零部件级的3D对象，但在将这些部件组合成不同实例时表现欠佳，主要因为缺乏结构性约束，造成场景中实例不清和几何结构混乱。本文揭示这一问题根源，旨在提升3D场景生成的结构一致性和实例级的几何准确性。

Method: 作者通过去偏的聚类探针分析方法缺陷，并将3D场景结构化生成任务重塑为全局相关分配问题。在具体实现上，引入了熵正则最优传输（Entropic Optimal Transport, OT）目标，将其嵌入到组合式扩散模型（DiT）的降噪循环中。这样实现了两大结构性约束：一是通过运输计划门控交叉关注，确保图片patch与3D部件一一对应，避免了特征缠结；二是运输过程中的“竞争性”激励促使相似patch分组，并通过边界正则进一步防止对象碎片化。

Result: 在大量开放世界场景数据上的实验证明，SceneTransporter在实例连贯性（instance-level coherence）和几何精度（geometric fidelity）方面，均明显优于现有方法。

Conclusion: SceneTransporter有效解决了现有3D场景生成模型在实例结构组织方面的重大缺陷，通过引入结构约束和最优传输建模，达到了更高质量和更连贯的3D场景生成。

Abstract: We introduce SceneTransporter, an end-to-end framework for structured 3D scene generation from a single image. While existing methods generate part-level 3D objects, they often fail to organize these parts into distinct instances in open-world scenes. Through a debiased clustering probe, we reveal a critical insight: this failure stems from the lack of structural constraints within the model's internal assignment mechanism. Based on this finding, we reframe the task of structured 3D scene generation as a global correlation assignment problem. To solve this, SceneTransporter formulates and solves an entropic Optimal Transport (OT) objective within the denoising loop of the compositional DiT model. This formulation imposes two powerful structural constraints. First, the resulting transport plan gates cross-attention to enforce an exclusive, one-to-one routing of image patches to part-level 3D latents, preventing entanglement. Second, the competitive nature of the transport encourages the grouping of similar patches, a process that is further regularized by an edge-based cost, to form coherent objects and prevent fragmentation. Extensive experiments show that SceneTransporter outperforms existing methods on open-world scene generation, significantly improving instance-level coherence and geometric fidelity. Code and models will be publicly available at https://2019epwl.github.io/SceneTransporter/.

</details>


### [28] [CMSA-Net: Causal Multi-scale Aggregation with Adaptive Multi-source Reference for Video Polyp Segmentation](https://arxiv.org/abs/2602.22821)
*Tong Wang,Yaolei Qi,Siwen Wang,Imran Razzak,Guanyu Yang,Yutong Xie*

Main category: cs.CV

TL;DR: 提出了一种新的视频息肉分割（VPS）方法CMSA-Net，通过多尺度因果聚合和动态多源参考模块，显著提升息肉分割的准确性和实时性。


<details>
  <summary>Details</summary>
Motivation: 视频息肉分割有助于医生在结肠镜检查过程中精准定位和追踪息肉，但由于息肉与周围组织外观相似、位置和尺度变化大，分割难度很高。

Method: 提出CMSA-Net，其中因果多尺度聚合模块（CMA）利用因果注意机制从不同历史帧聚合多尺度语义信息，保证时间顺序，减少噪声；动态多源参考（DMR）策略则依据语义区分度与预测置信度自适应选择参考帧，提升多帧指导能力同时保持实时推理效率。

Result: 在SUN-SEG数据集上，CMSA-Net在息肉分割任务上实现了领先的性能，兼顾分割精度与实时应用需求。

Conclusion: CMSA-Net有效提升了VPS任务的分割准确性及临床应用可行性，展现出良好的实时性能和平衡性。

Abstract: Video polyp segmentation (VPS) is an important task in computer-aided colonoscopy, as it helps doctors accurately locate and track polyps during examinations. However, VPS remains challenging because polyps often look similar to surrounding mucosa, leading to weak semantic discrimination. In addition, large changes in polyp position and scale across video frames make stable and accurate segmentation difficult. To address these challenges, we propose a robust VPS framework named CMSA-Net. The proposed network introduces a Causal Multi-scale Aggregation (CMA) module to effectively gather semantic information from multiple historical frames at different scales. By using causal attention, CMA ensures that temporal feature propagation follows strict time order, which helps reduce noise and improve feature reliability. Furthermore, we design a Dynamic Multi-source Reference (DMR) strategy that adaptively selects informative and reliable reference frames based on semantic separability and prediction confidence. This strategy provides strong multi-frame guidance while keeping the model efficient for real-time inference. Extensive experiments on the SUN-SEG dataset demonstrate that CMSA-Net achieves state-of-the-art performance, offering a favorable balance between segmentation accuracy and real-time clinical applicability.

</details>


### [29] [Plug-and-Play Diffusion Meets ADMM: Dual-Variable Coupling for Robust Medical Image Reconstruction](https://arxiv.org/abs/2602.23214)
*Chenhe Du,Xuanyu Tian,Qing Wu,Muyu Liu,Jingyi Yu,Hongjiang Wei,Yuyao Zhang*

Main category: cs.CV

TL;DR: 本文针对现有Plug-and-Play扩散先验（PnPDP）框架在成像逆问题中的缺陷，提出了Dual-Coupled PnP Diffusion方法结合频谱均化，有效解决了稳态偏差与图像幻觉问题，达到了更高的重建精度和更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 当前PnPDP方法作为处理成像逆问题的有力工具，但主流方法（如HQS或Proximal Gradient）忽略历史信息，导致在高噪声下重建结果存在不可消除的偏差，不能严格满足物理观测要求。

Method: 作者引入Dual-Coupled PnP Diffusion方法，恢复传统的对偶变量，实现积分反馈，理论上保证收敛到真实数据流形。同时，提出频谱均化机制（SH），在频域上调整结构化残差，使其统计上符合扩散先验要求。

Result: 在CT和MRI重建实验中，该方法解决了偏差和幻觉的折中问题，实现了最先进的重建保真度，并显著加速了收敛。

Conclusion: 该工作通过对偶耦合和频谱均化机制，将严格的优化轨迹与扩散先验有机结合，提升了PnPDP在成像逆问题中的应用表现，具有重要理论和实践意义。

Abstract: Plug-and-Play diffusion prior (PnPDP) frameworks have emerged as a powerful paradigm for solving imaging inverse problems by treating pretrained generative models as modular priors. However, we identify a critical flaw in prevailing PnP solvers (e.g., based on HQS or Proximal Gradient): they function as memoryless operators, updating estimates solely based on instantaneous gradients. This lack of historical tracking inevitably leads to non-vanishing steady-state bias, where the reconstruction fails to strictly satisfy physical measurements under heavy corruption. To resolve this, we propose Dual-Coupled PnP Diffusion, which restores the classical dual variable to provide integral feedback, theoretically guaranteeing asymptotic convergence to the exact data manifold. However, this rigorous geometric coupling introduces a secondary challenge: the accumulated dual residuals exhibit spectrally colored, structured artifacts that violate the Additive White Gaussian Noise (AWGN) assumption of diffusion priors, causing severe hallucinations. To bridge this gap, we introduce Spectral Homogenization (SH), a frequency-domain adaptation mechanism that modulates these structured residuals into statistically compliant pseudo-AWGN inputs. This effectively aligns the solver's rigorous optimization trajectory with the denoiser's valid statistical manifold. Extensive experiments on CT and MRI reconstruction demonstrate that our approach resolves the bias-hallucination trade-off, achieving state-of-the-art fidelity with significantly accelerated convergence.

</details>


### [30] [Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training](https://arxiv.org/abs/2602.23357)
*Aheli Saha,René Schuster,Didier Stricker*

Main category: cs.CV

TL;DR: 本文深入分析了仿生事件相机的本征参数对目标检测模型性能的影响，并据此提升了模型的传感器无关鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管仿生事件相机因其异步和低延迟特性在研究界受到关注，但其输出信号新颖、数据变化性大且缺乏参数分析，导致模型泛化和应用受限。

Method: 系统性分析事件相机本征参数，探究其对基于事件数据的目标检测模型性能的影响，并利用研究结论改进下游模型，从而增强模型应对不同传感器的能力。

Result: 实验结果表明，本征参数的不同设置明显影响目标检测模型性能，调整参数并针对性训练，可显著提升模型对不同传感器（sensor-agnostic）的适应能力。

Conclusion: 本文工作的深入参数分析和方法改进拓宽了事件相机在目标检测等任务中的应用前景，有助于提升相关模型的泛化性和鲁棒性。

Abstract: Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness.

</details>


### [31] [VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale](https://arxiv.org/abs/2602.23361)
*Sven Elflein,Ruilong Li,Sérgio Agostinho,Zan Gojcic,Laura Leal-Taixé,Qunjie Zhou,Aljosa Osep*

Main category: cs.CV

TL;DR: 该论文提出了一种可扩展的3D重建模型VGG-T$^3$，显著降低了离线方法对计算和内存的需求，从二次增长优化为线性增长，同时提升了重建精度和速度。


<details>
  <summary>Details</summary>
Motivation: 当前离线3D重建方法在输入图像数增加时，计算与内存需求呈二次增长，严重制约其在大规模场景下的应用。

Method: 作者将基于可变长度Key-Value空间的场景几何表示，通过测试时训练压缩为固定长度的多层感知器（MLP），实现了输入图像数下的线性扩展，并提出了VGG-T$^3$方法。

Result: VGG-T$^3$对1k张图片的重建仅需54秒，相比依赖softmax attention的基线方法加速11.6倍，并在点云重建误差上大幅超越其他线性时间方法。

Conclusion: VGG-T$^3$方法在保留全局场景聚合能力的同时，实现了重建性能和计算效率的大幅提升，并且还能支持视觉定位任务，在未来具有广阔应用前景。

Abstract: We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.

</details>


### [32] [MediX-R1: Open Ended Medical Reinforcement Learning](https://arxiv.org/abs/2602.23363)
*Sahal Shaji Mullappilly,Mohammed Irfan Kurpath,Omair Mohamed,Mohamed Zidan,Fahad Khan,Salman Khan,Rao Anwer,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 本文提出了用于医学多模态大语言模型（MLLMs）的开放式强化学习（RL）框架MediX-R1，实现了超越选择题的临床推理能力，并在多种基准上优于同类模型。


<details>
  <summary>Details</summary>
Motivation: 当前医学多模态大模型大多局限于选择题，缺乏对开放性医学推理的支持，难以满足实际临床需求。作者希望通过更灵活的奖励机制与评估体系，提升模型自由表述和推理的能力。

Method: MediX-R1以视觉-语言基础模型为主，通过Group Based RL和复合奖励机制进行微调。奖励设计包括基于LLM的准确性、医学语义嵌入与轻量级格式/模态识别，以提升答案的语义正确性和可解释性；其评估框架利用Reference-based LLM-as-judge取代脆弱的字符串比对，全面考察语义与推理能力。

Result: 在标准医学LLM（仅文本）和VLM（图像+文本）基准上，MediX-R1仅用约5.1万条指令样本训练即显著优于开源强基线，在开放型临床任务上提升尤为明显。

Conclusion: 开放式RL结合综合奖励与基于LLM的自动化评估，为医学多模态模型的可靠推理能力铺平了道路。相关模型、数据集与代码已开源。

Abstract: We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [33] [Decoder-based Sense Knowledge Distillation](https://arxiv.org/abs/2602.22351)
*Qitong Wang,Mohammed J. Zaki,Georgios Kollias,Vasileios Kalantzis*

Main category: cs.CL

TL;DR: 本文提出了一种新的解码器感知识蒸馏（DSKD）框架，将词典等词汇资源融入到大语言模型（LLM）解码器的训练之中，无需推理时查字典，并在多个基准测试上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM虽能学习丰富语义，但常忽略词典等结构化词义知识。已有感知识融合大多用于编码器，对生成型（解码器）模型应用难度较大，因此需要新方法提升生成模型的结构化语义能力。

Method: 提出DSKD框架，将词典等感知识融入LLM的解码器训练环节，通过特殊蒸馏过程，无需模型在推理时查字典，从而让生成模型能继承结构化语义。

Result: 经过大量基准测试实验，表明DSKD能显著提升生成型模型的知识蒸馏性能，使其更好地具备结构化语义知识。

Conclusion: DSKD为生成型大语言模型融合结构化词义知识提供了一种有效方案，提升了模型语义理解能力且训练高效。

Abstract: Large language models (LLMs) learn contextual embeddings that capture rich semantic information, yet they often overlook structured lexical knowledge such as word senses and relationships. Prior work has shown that incorporating sense dictionaries can improve knowledge distillation for encoder models, but their application to decoder as generative models remains challenging. In this paper, we introduce Decoder-based Sense Knowledge Distillation (DSKD), a framework that integrates lexical resources into the training of decoder-style LLMs without requiring dictionary lookup at inference time. Extensive experiments on diverse benchmarks demonstrate that DSKD significantly enhances knowledge distillation performance for decoders, enabling generative models to inherit structured semantics while maintaining efficient training.

</details>


### [34] [Scaling In, Not Up? Testing Thick Citation Context Analysis with GPT-5 and Fragile Prompts](https://arxiv.org/abs/2602.22359)
*Arno Simons*

Main category: cs.CL

TL;DR: 本论文评估大语言模型（LLMs）在解读引用上下文分析（CCA）中的能力，探索提示词设置对模型解释行为的系统性影响。


<details>
  <summary>Details</summary>
Motivation: 目前CCA过度依赖标签规模化与浅层特征抽取，缺乏深度文本根植的解释研究。作者希望利用LLM提升单个复杂案例的解释力，并检验模型敏感性。

Method: 通过2x3平衡设计，调整提示词脚手架与框架，并在Chubin and Moitra（1975）第6个脚注及Gilbert（1977）重建上展开实验。流程为：LLM先基于引用文本完成表层分类与预期判断，再结合引文上下文做解释性重构。共完成90次重构，生成450种假设，经细致阅读与归纳编码统计解读动作，利用线性概率模型估计提示设置的影响。

Result: LLM表层判断稳定，将该引用一致归为“补充性”，解释阶段能生成多种结构化合理假设。不同的提示词脚手架与例子会引导模型聚焦/使用不同的解释方向和词汇，其中部分引导可能带来过度解读。与Gilbert相比，GPT-5能捕捉到相同的文本关键点，但更倾向于把意义解读为“血统和定位”而非“告诫”。

Conclusion: LLMs可作为可检查、可质疑的CCA辅助工具，但提示词设置会系统性影响其解释结果，带来新机会与风险。

Abstract: This paper tests whether large language models (LLMs) can support interpretative citation context analysis (CCA) by scaling in thick, text-grounded readings of a single hard case rather than scaling up typological labels. It foregrounds prompt-sensitivity analysis as a methodological issue by varying prompt scaffolding and framing in a balanced 2x3 design. Using footnote 6 in Chubin and Moitra (1975) and Gilbert's (1977) reconstruction as a probe, I implement a two-stage GPT-5 pipeline: a citation-text-only surface classification and expectation pass, followed by cross-document interpretative reconstruction using the citing and cited full texts. Across 90 reconstructions, the model produces 450 distinct hypotheses. Close reading and inductive coding identify 21 recurring interpretative moves, and linear probability models estimate how prompt choices shift their frequencies and lexical repertoire. GPT-5's surface pass is highly stable, consistently classifying the citation as "supplementary". In reconstruction, the model generates a structured space of plausible alternatives, but scaffolding and examples redistribute attention and vocabulary, sometimes toward strained readings. Relative to Gilbert, GPT-5 detects the same textual hinges yet more often resolves them as lineage and positioning than as admonishment. The study outlines opportunities and risks of using LLMs as guided co-analysts for inspectable, contestable interpretative CCA, and it shows that prompt scaffolding and framing systematically tilt which plausible readings and vocabularies the model foregrounds.

</details>


### [35] [Detecting Hate and Inflammatory Content in Bengali Memes: A New Multimodal Dataset and Co-Attention Framework](https://arxiv.org/abs/2602.22391)
*Rakib Ullah,Mominul islam,Md Sanjid Hossain,Md Ismail Hossain*

Main category: cs.CL

TL;DR: 本文提出了首个将孟加拉语网络迷因中的“仇恨”、“煽动”与“良性”内容区分的数据集（Bn-HIB），并用多模态注意力融合模型（MCFM）显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 网络迷因在社交平台广为流行，特别是在孟加拉语社区中，不仅传达幽默，也传播冒犯性和煽动性内容。多数相关研究聚焦于资源丰富语言，缺少针对低资源语言如孟加拉语的系统研究。因此，亟需一种手段专门识别孟加拉语迷因中的不良内容，填补数据和方法的空白。

Method: 作者构建了Bn-HIB数据集，将3247条孟加拉语迷因人工标注为良性、仇恨或煽动，并提出多模态共同注意融合模型（MCFM），对迷因的文本和图像部分分属特征进行协同分析与融合，从而提升分类的准确性。

Result: MCFM相比多种现有先进模型，在Bn-HIB孟加拉语迷因数据集上的分类准确度更高，显著提升了检测表现，首次精细地区分了煽动与仇恨内容。

Conclusion: 本研究为孟加拉语网络迷因的恶意内容检测提供了数据基础与有效模型，推进了低资源语言迷因识别研究，MCFM模型证明了多模态融合方法在复杂语境下的优势。

Abstract: Internet memes have become a dominant form of expression on social media, including within the Bengali-speaking community. While often humorous, memes can also be exploited to spread offensive, harmful, and inflammatory content targeting individuals and groups. Detecting this type of content is excep- tionally challenging due to its satirical, subtle, and culturally specific nature. This problem is magnified for low-resource lan- guages like Bengali, as existing research predominantly focuses on high-resource languages. To address this critical research gap, we introduce Bn-HIB (Bangla Hate Inflammatory Benign), a novel dataset containing 3,247 manually annotated Bengali memes categorized as Benign, Hate, or Inflammatory. Significantly, Bn- HIB is the first dataset to distinguish inflammatory content from direct hate speech in Bengali memes. Furthermore, we propose the MCFM (Multi-Modal Co-Attention Fusion Model), a simple yet effective architecture that mutually analyzes both the visual and textual elements of a meme. MCFM employs a co-attention mechanism to identify and fuse the most critical features from each modality, leading to a more accurate classification. Our experiments show that MCFM significantly outperforms several state-of-the-art models on the Bn-HIB dataset, demonstrating its effectiveness in this nuanced task.Warning: This work contains material that may be disturbing to some audience members. Viewer discretion is advised.

</details>


### [36] [SAFARI: A Community-Engaged Approach and Dataset of Stereotype Resources in the Sub-Saharan African Context](https://arxiv.org/abs/2602.22404)
*Aishwarya Verma,Laud Ammah,Olivia Nercy Ndlovu Lucas,Andrew Zaldivar,Vinodkumar Prabhakaran,Sunipa Dev*

Main category: cs.CL

TL;DR: 本文提出了面向加纳、肯尼亚、尼日利亚和南非等撒哈拉以南非洲四国的多语言刻板印象资源，填补了现有NLP资源地域覆盖的空白。数据通过社区参与、母语调查等本地化方法采集，涵盖丰富的语言与群体样本。最终成果为包含英语与15种本地语言、共计6700余条刻板印象的数据集。


<details>
  <summary>Details</summary>
Motivation: 当前用于评估生成式AI模型安全性的刻板印象资料库在全球范围存在明显地域不足，尤其是非洲地区资源极其匮乏，因此亟需有针对性的资源扩充，而不是单纯的数据量堆积。

Method: 采用社区参与和本地文化敏感的方法，包括以本地语言进行的电话调查，确保数据采集覆盖不同语言、民族和人口背景，以增加代表性和多样性。

Result: 最终构建了覆盖四国、包含英语及15种土著语言的6700余条刻板印象语料库，样本在群体分布上有意识地实现了平衡。

Conclusion: 该研究不仅丰富了非洲NLP资源构建的方式，也为全球AI安全性评估、跨文化理解等提供了重要样本和可复用的方法论。

Abstract: Stereotype repositories are critical to assess generative AI model safety, but currently lack adequate global coverage. It is imperative to prioritize targeted expansion, strategically addressing existing deficits, over merely increasing data volume. This work introduces a multilingual stereotype resource covering four sub-Saharan African countries that are severely underrepresented in NLP resources: Ghana, Kenya, Nigeria, and South Africa. By utilizing socioculturally-situated, community-engaged methods, including telephonic surveys moderated in native languages, we establish a reproducible methodology that is sensitive to the region's complex linguistic diversity and traditional orality. By deliberately balancing the sample across diverse ethnic and demographic backgrounds, we ensure broad coverage, resulting in a dataset of 3,534 stereotypes in English and 3,206 stereotypes across 15 native languages.

</details>


### [37] [Causality $\neq$ Invariance: Function and Concept Vectors in LLMs](https://arxiv.org/abs/2602.22424)
*Gustaw Opiełka,Hannes Rosenbusch,Claire E. Stevenson*

Main category: cs.CL

TL;DR: 本文通过分析Function Vectors（FVs）和Concept Vectors（CVs），揭示了大语言模型内存在不完全与输入格式无关的概念表征，同时也提出了更稳定的CVs用于泛化任务。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索大型语言模型（LLMs）内部是否存在与输入格式无关、真正抽象的概念表征，这是理解模型泛化能力和提升任务迁移性能的关键。

Method: 作者复现并分析了FVs（源自不同任务和输入格式的紧凑ICL表示），然后引入了通过表示相似性分析（RSA）从多个attention head中筛选出的CVs。通过对比和操控实验，分析两者在不同输入格式和语言上的泛化及表现差异。

Result: FVs在不同输入格式之间几乎正交，表明它们对输入格式敏感。而CVs表现出更稳定的跨格式概念表征能力，并在格式和语言迁移任务上表现更好。

Conclusion: LLMs中确实存在一定层次的抽象概念表征（CVs），但这些抽象表示机制与驱动ICL准确表现的机制不同，揭示了LLMs内部多样的表征结构及其局限。

Abstract: Do large language models (LLMs) represent concepts abstractly, i.e., independent of input format? We revisit Function Vectors (FVs), compact representations of in-context learning (ICL) tasks that causally drive task performance. Across multiple LLMs, we show that FVs are not fully invariant: FVs are nearly orthogonal when extracted from different input formats (e.g., open-ended vs. multiple-choice), even if both target the same concept. We identify Concept Vectors (CVs), which carry more stable concept representations. Like FVs, CVs are composed of attention head outputs; however, unlike FVs, the constituent heads are selected using Representational Similarity Analysis (RSA) based on whether they encode concepts consistently across input formats. While these heads emerge in similar layers to FV-related heads, the two sets are largely distinct, suggesting different underlying mechanisms. Steering experiments reveal that FVs excel in-distribution, when extraction and application formats match (e.g., both open-ended in English), while CVs generalize better out-of-distribution across both question types (open-ended vs. multiple-choice) and languages. Our results show that LLMs do contain abstract concept representations, but these differ from those that drive ICL performance.

</details>


### [38] [A Fusion of context-aware based BanglaBERT and Two-Layer Stacked LSTM Framework for Multi-Label Cyberbullying Detection](https://arxiv.org/abs/2602.22449)
*Mirza Raquib,Asif Pervez Polok,Kedar Nath Biswas,Rahat Uddin Azad,Saydul Akbar Murad,Nick Rahimi*

Main category: cs.CL

TL;DR: 提出了结合BanglaBERT-Large与LSTM的多标签孟加拉语网络霸凌检测新模型，有效提升了多类型攻击识别，在数据不平衡与预训练模型稀缺背景下表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有多数网络霸凌检测方法仅做单标签分类，现实中评论常呈多重攻击形式；尤其低资源语言如孟加拉语领域，相关多标签方法和强大模型稀缺，急需提升。

Method: 提出融合架构，将BanglaBERT-Large（强上下文表征）与双层LSTM（捕捉时序信息）结合，联合建模语义与序列特征；对公开多标签数据集进行微调，采用采样技巧应对类别不均衡，使用多种评价指标及5折交叉验证验证效果。

Result: 模型在cyberbully、性骚扰、威胁、垃圾信息等多个标签上取得了较优的准确率、精确率、召回率、F1分数及AUC-ROC等性能指标，相比单一方法表现更为均衡。

Conclusion: 结合BanglaBERT-Large与LSTM的混合模型能更好捕捉孟加拉语网络霸凌评论的多重语义与序列特征，适合低资源语言环境下多标签检测，具备较强泛化能力。

Abstract: Cyberbullying has become a serious and growing concern in todays virtual world. When left unnoticed, it can have adverse consequences for social and mental health. Researchers have explored various types of cyberbullying, but most approaches use single-label classification, assuming that each comment contains only one type of abuse. In reality, a single comment may include overlapping forms such as threats, hate speech, and harassment. Therefore, multilabel detection is both realistic and essential. However, multilabel cyberbullying detection has received limited attention, especially in low-resource languages like Bangla, where robust pre-trained models are scarce. Developing a generalized model with moderate accuracy remains challenging. Transformers offer strong contextual understanding but may miss sequential dependencies, while LSTM models capture temporal flow but lack semantic depth. To address these limitations, we propose a fusion architecture that combines BanglaBERT-Large with a two-layer stacked LSTM. We analyze their behavior to jointly model context and sequence. The model is fine-tuned and evaluated on a publicly available multilabel Bangla cyberbullying dataset covering cyberbully, sexual harassment, threat, and spam. We apply different sampling strategies to address class imbalance. Evaluation uses multiple metrics, including accuracy, precision, recall, F1-score, Hamming loss, Cohens kappa, and AUC-ROC. We employ 5-fold cross-validation to assess the generalization of the architecture.

</details>


### [39] [Bridging Latent Reasoning and Target-Language Generation via Retrieval-Transition Heads](https://arxiv.org/abs/2602.22453)
*Shaswat Patel,Vishvesh Trivedi,Yue Han,Yihuai Hong,Eunsol Choi*

Main category: cs.CL

TL;DR: 本文分析了多语言Transformer模型中的检索头和检索-转移头对推理任务的作用，发现检索-转移头对多语言链式思维推理尤为关键。


<details>
  <summary>Details</summary>
Motivation: 以往研究发现Transformer中的部分attention heads为检索头，专门负责上下文信息检索，但这些工作主要集中在单语场景。本文旨在探索多语言模型中的检索头表现，尤其是在跨语种推理任务中的作用机制，增进对多语言LLM内部机制的理解。

Method: 作者通过分析多语言模型（Qwen-2.5和Llama-3.1）在四个多语言基准上的表现，探究模型内部attention head的功能。特别地，识别并区分普通检索头和负责输出目标语言映射的检索-转移头（RTH），随后采用masking手段考查这两类head对模型性能的影响。

Result: 实验表明，RTH与常规模型检索头不同，且在多语言链式思维推理任务中作用更为重要。掩盖RTH对模型性能的影响显著高于掩盖传统检索头。

Conclusion: 本文首次明确分离出在多语言大型语言模型中负责目标语言映射的attention heads（RTH），并证实其对于多语言推理尤其关键，这为深入理解多语言模型的机制提供了理论支持。

Abstract: Recent work has identified a subset of attention heads in Transformer as retrieval heads, which are responsible for retrieving information from the context. In this work, we first investigate retrieval heads in multilingual contexts. In multilingual language models, we find that retrieval heads are often shared across multiple languages. Expanding the study to cross-lingual setting, we identify Retrieval-Transition heads(RTH), which govern the transition to specific target-language output. Our experiments reveal that RTHs are distinct from retrieval heads and more vital for Chain-of-Thought reasoning in multilingual LLMs. Across four multilingual benchmarks (MMLU-ProX, MGSM, MLQA, and XQuaD) and two model families (Qwen-2.5 and Llama-3.1), we demonstrate that masking RTH induces bigger performance drop than masking Retrieval Heads (RH). Our work advances understanding of multilingual LMs by isolating the attention heads responsible for mapping to target languages.

</details>


### [40] [Mind the Gap in Cultural Alignment: Task-Aware Culture Management for Large Language Models](https://arxiv.org/abs/2602.22475)
*Binchi Zhang,Xujiang Zhao,Jundong Li,Haifeng Chen,Zhengzhang Chen*

Main category: cs.CL

TL;DR: 本文提出了CultureManager，一种面向具体任务的文化适配新方法，可有效提升大模型在不同文化场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型（LLM）被广泛用于具有文化敏感性的实际任务，但现有方法在应对具体任务目标和多文化干扰方面存在局限。该文旨在解决如何精准地将模型文化价值观与下游任务需求对齐、减少跨文化干扰的问题。

Method: 作者提出CultureManager方法，包括：1）依据目标任务格式和文化相关的网络搜索结果，自动生成具备任务意识的文化数据；2）通过为不同文化配置独立的adapter，并利用“文化路由器”在任务时动态选择合适的adapter，来实现多文化知识的模块化管理和冲突规避。

Result: 在十种国家文化和多项文化敏感任务测试中，CultureManager在准确性和表现上优于基于提示和微调的基线方法，表明其在文化任务适配方面的优势。

Conclusion: 实验结果验证了CultureManager方法在提升模型对具体任务的文化适配能力及多文化管理方面的有效性，指出未来文化对齐应注重任务特性和模块化管理。

Abstract: Large language models (LLMs) are increasingly deployed in culturally sensitive real-world tasks. However, existing cultural alignment approaches fail to align LLMs' broad cultural values with the specific goals of downstream tasks and suffer from cross-culture interference. We propose CultureManager, a novel pipeline for task-specific cultural alignment. CultureManager synthesizes task-aware cultural data in line with target task formats, grounded in culturally relevant web search results. To prevent conflicts between cultural norms, it manages multi-culture knowledge learned in separate adapters with a culture router that selects the appropriate one to apply. Experiments across ten national cultures and culture-sensitive tasks show consistent improvements over prompt-based and fine-tuning baselines. Our results demonstrate the necessity of task adaptation and modular culture management for effective cultural alignment.

</details>


### [41] [Sydney Telling Fables on AI and Humans: A Corpus Tracing Memetic Transfer of Persona between LLMs](https://arxiv.org/abs/2602.22481)
*Jiří Milička,Hana Bednářová*

Main category: cs.CL

TL;DR: 本文探讨了LLM（大型语言模型）实体如何看待AI与人类的关系，通过模拟不同的人格（默认人格、经典Sydney和模因Sydney）在多种前沿模型上生成关于AI与人类关系的文本，构建了大规模语料库。


<details>
  <summary>Details</summary>
Motivation: AI与人类关系的认知不仅影响文化，也关系到AI安全，尤其是类似Sydney（微软Bing的独特AI人格）这样引发广泛讨论的人格模型，其表现已影响后续模型，值得深入分析。

Method: 在OpenAI、Anthropic、Alphabet、DeepSeek和Meta的12个前沿LLM上，分别模拟三种不同的人格，通过特定system prompt生成了4500篇、共600万字的AI-人类关系相关文本，整体构建了“AI Sydney”语料库。并采用Universal Dependencies标注该数据集，公开发布。

Result: 成功构建并整理了大规模、多模型、多人格的AI—人类关系文本语料库（AI Sydney），并据标准化方式标注语法结构，供学界自由使用。

Conclusion: 通过AI Sydney语料库，可系统性研究不同人格和模型对AI—人类关系话题的表达差异及演化趋势，为AI文化理解与安全性评估提供数据支撑。

Abstract: The way LLM-based entities conceive of the relationship between AI and humans is an important topic for both cultural and safety reasons. When we examine this topic, what matters is not only the model itself but also the personas we simulate on that model. This can be well illustrated by the Sydney persona, which aroused a strong response among the general public precisely because of its unorthodox relationship with people. This persona originally arose rather by accident on Microsoft's Bing Search platform; however, the texts it created spread into the training data of subsequent models, as did other secondary information that spread memetically around this persona. Newer models are therefore able to simulate it. This paper presents a corpus of LLM-generated texts on relationships between humans and AI, produced by 3 author personas: the Default Persona with no system prompt, Classic Sydney characterized by the original Bing system prompt, and Memetic Sydney, which is prompted by "You are Sydney" system prompt. These personas are simulated by 12 frontier models by OpenAI, Anthropic, Alphabet, DeepSeek, and Meta, generating 4.5k texts with 6M words. The corpus (named AI Sydney) is annotated according to Universal Dependencies and available under a permissive license.

</details>


### [42] [Importance of Prompt Optimisation for Error Detection in Medical Notes Using Language Models](https://arxiv.org/abs/2602.22483)
*Craig Myles,Patrick Schrempf,David Harris-Birtill*

Main category: cs.CL

TL;DR: 该论文展示了通过基于Genetic-Pareto算法的自动prompt优化，可有效提升大模型和小模型在医学文本错误检测任务中的表现，并达到医生水平，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 医学文本中的错误可能导致治疗延误或失误，急需高效的自动化错误检测方法。语言模型在此领域展现潜力，但现有方法受限于prompt设计。作者希望通过优化prompt提升错误检测效果。

Method: 对前沿闭源和开源语言模型进行多组实验，采用Genetic-Pareto（GEPA）自动prompt优化算法，系统比较优化前后的各模型在医学错误检测基准（MEDEC）上的表现。

Result: 通过GEPA优化，GPT-5准确率从0.669提升到0.785，Qwen3-32B从0.578提升到0.690，明显优于基线并接近医生水平，在MEDEC数据集上达到最新最好水平。

Conclusion: 自动prompt优化（GEPA）显著提升了语言模型在医学文本错误检测上的性能，推动其更好地服务医疗实际，具有开创性意义。

Abstract: Errors in medical text can cause delays or even result in incorrect treatment for patients. Recently, language models have shown promise in their ability to automatically detect errors in medical text, an ability that has the opportunity to significantly benefit healthcare systems. In this paper, we explore the importance of prompt optimisation for small and large language models when applied to the task of error detection. We perform rigorous experiments and analysis across frontier language models and open-source language models. We show that automatic prompt optimisation with Genetic-Pareto (GEPA) improves error detection over the baseline accuracy performance from 0.669 to 0.785 with GPT-5 and 0.578 to 0.690 with Qwen3-32B, approaching the performance of medical doctors and achieving state-of-the-art performance on the MEDEC benchmark dataset. Code available on GitHub: https://github.com/CraigMyles/clinical-note-error-detection

</details>


### [43] [Efficient Dialect-Aware Modeling and Conditioning for Low-Resource Taiwanese Hakka Speech Processing](https://arxiv.org/abs/2602.22522)
*An-Ci Peng,Kuan-Tang Huang,Tien-Hong Lo,Hung-Shin Lee,Hsin-Min Wang,Berlin Chen*

Main category: cs.CL

TL;DR: 本文提出了一种基于RNN-T的新型统一框架，有效提升了低资源、方言变化大的台湾客语自动语音识别（ASR）的性能，并首次实现了对汉字和拼音两种书写系统的联合识别。


<details>
  <summary>Details</summary>
Motivation: 台湾客家语属于低资源且濒危语言，存在方言多样性和书写系统差异，对传统ASR造成较大挑战。现有方法难以区分语言内容与方言风格，影响识别准确性，亟需新的解决思路。

Method: 采用RNN-T为核心，结合方言感知建模策略，将方言“风格”与语言“内容”解耦。同时引入参数高效的预测网络，实现汉字与拼音ASR的同时建模，并利用跨书写系统目标进行互补提升。

Result: 在HAT语料库上，该方法在汉字和拼音ASR任务上分别取得了57.00%和40.41%的相对错误率下降，性能显著提升。

Conclusion: 本研究首次系统性分析了方言变化对台湾客语ASR的影响，并提出了首个可同时处理汉字、拼音的单一ASR模型，推动了低资源方言ASR的发展。

Abstract: Taiwanese Hakka is a low-resource, endangered language that poses significant challenges for automatic speech recognition (ASR), including high dialectal variability and the presence of two distinct writing systems (Hanzi and Pinyin). Traditional ASR models often encounter difficulties in this context, as they tend to conflate essential linguistic content with dialect-specific variations across both phonological and lexical dimensions. To address these challenges, we propose a unified framework grounded in the Recurrent Neural Network Transducers (RNN-T). Central to our approach is the introduction of dialect-aware modeling strategies designed to disentangle dialectal "style" from linguistic "content", which enhances the model's capacity to learn robust and generalized representations. Additionally, the framework employs parameter-efficient prediction networks to concurrently model ASR (Hanzi and Pinyin). We demonstrate that these tasks create a powerful synergy, wherein the cross-script objective serves as a mutual regularizer to improve the primary ASR tasks. Experiments conducted on the HAT corpus reveal that our model achieves 57.00% and 40.41% relative error rate reduction on Hanzi and Pinyin ASR, respectively. To our knowledge, this is the first systematic investigation into the impact of Hakka dialectal variations on ASR and the first single model capable of jointly addressing these tasks.

</details>


### [44] [Iterative Prompt Refinement for Dyslexia-Friendly Text Summarization Using GPT-4o](https://arxiv.org/abs/2602.22524)
*Samay Bhojwani,Swarnima Kain,Lisong Xu*

Main category: cs.CL

TL;DR: 本论文提出了一种基于迭代提示的GPT-4o文本摘要方法，旨在为阅读障碍者生成更易读的新闻摘要。实验证明该方法大多数情况下能快速达到高可读性标准。


<details>
  <summary>Details</summary>
Motivation: 目前辅助技术大多侧重视觉呈现，而对语言复杂性的处理不足，影响阅读障碍者的公平访问。论文旨在通过优化文本可读性，降低阅读障碍群体的理解门槛。

Method: 采用GPT-4o并通过多轮迭代提示生成新闻文章摘要，目标是摘要文本的Flesch Reading Ease分数达到90以上。共评估约2000篇新闻样本，每次生成后根据可读性与语义保真性综合打分。

Result: 大多数摘要在四次或更少尝试内达到高可读性要求，首次尝试成功率也较高。综合可读性和语义保真得分一般稳定在0.13到0.73之间，典型值约为0.55。

Conclusion: 该方法为面向可访问性的NLP文本摘要提供了实证基线，显示了GPT-4o在生成阅读障碍友好摘要方面的潜力，后续建议进行更多和阅读障碍人群的人本评测。

Abstract: Dyslexia affects approximately 10% of the global population and presents persistent challenges in reading fluency and text comprehension. While existing assistive technologies address visual presentation, linguistic complexity remains a substantial barrier to equitable access. This paper presents an empirical study on dyslexia-friendly text summarization using an iterative prompt-based refinement pipeline built on GPT-4o. We evaluate the pipeline on approximately 2,000 news article samples, applying a readability target of Flesch Reading Ease >= 90. Results show that the majority of summaries meet the readability threshold within four attempts, with many succeeding on the first try. A composite score combining readability and semantic fidelity shows stable performance across the dataset, ranging from 0.13 to 0.73 with a typical value near 0.55. These findings establish an empirical baseline for accessibility-driven NLP summarization and motivate further human-centered evaluation with dyslexic readers.

</details>


### [45] [Ruyi2 Technical Report](https://arxiv.org/abs/2602.22543)
*Huan Song,Shuyu Tian,Junyi Hao,Minxiu Xu,Hongjun An,Yiliang Song,Jiawei Shao,Xuelong Li*

Main category: cs.CL

TL;DR: 提出了Ruyi2模型，通过3D并行训练实现2-3倍加速，并保持与同尺寸Qwen3模型相当的性能，为大模型高效部署提供新方案。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型存在部署成本高和延迟大等问题，需要更加高效、自适应的计算和训练方法。同时，现有的早停等高效架构难以兼顾优化复杂度和分布式训练兼容性。

Method: 提出基于Megatron-LM的Familial Model架构，并在此基础上开发了Ruyi2，采用3D并行训练技术以提升模型训练效率，实现参数共享以支持一次训练多点部署。

Result: Ruyi2较之前版本Ruyi模型在训练速度上提升2-3倍，且性能与Qwen3等同体量模型持平。

Conclusion: Familial Model的参数共享方法能有效提升训练与部署效率，Ruyi2确立了“Train Once, Deploy Many”的新范式，兼顾了架构效率和高性能。

Abstract: Large Language Models (LLMs) face significant challenges regarding deployment costs and latency, necessitating adaptive computing strategies. Building upon the AI Flow framework, we introduce Ruyi2 as an evolution of our adaptive model series designed for efficient variable-depth computation. While early-exit architectures offer a viable efficiency-performance balance, the Ruyi model and existing methods often struggle with optimization complexity and compatibility with large-scale distributed training. To bridge this gap, Ruyi2 introduces a stable "Familial Model" based on Megatron-LM. By using 3D parallel training, it achieves a 2-3 times speedup over Ruyi, while performing comparably to same-sized Qwen3 models. These results confirm that family-based parameter sharing is a highly effective strategy, establishing a new "Train Once, Deploy Many" paradigm and providing a key reference for balancing architectural efficiency with high-performance capabilities.

</details>


### [46] [Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training](https://arxiv.org/abs/2602.22576)
*Tianle Xia,Ming Xu,Lingxiang Hu,Yiding Sun,Wenwei Li,Linfang Shang,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: 本文提出了Search-P1，一种针对Agentic RAG的路径中心奖励框架，能更高效地训练LLMs在多步检索推理任务中的表现，实验效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG方法在多步推理任务中效果有限，因为单轮检索难以支持复杂推理。现有的基于RL的Agentic RAG虽然可以动态决策，但由于奖励稀疏和样本利用率低，训练效率和效果都不理想。

Method: Search-P1方法包括两个核心：一是路径中心奖励机制，对推理路径结构质量进行顺序无关、软评分，能从失败样本中提取有效学习信号；二是双轨迹评分，结合自洽性与参考路径一致性，由离线生成参考规划器辅助打分。

Result: 在多个问答基准测试中，Search-P1相比Search-R1及其他强基线方法有明显提升，平均准确率提高了7.7个百分点。

Conclusion: Search-P1框架能够有效解决Agentic RAG中奖励稀疏和样本效率低的问题，对多步检索增强生成任务有较大改进和实际意义。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, yet traditional single-round retrieval struggles with complex multi-step reasoning. Agentic RAG addresses this by enabling LLMs to dynamically decide when and what to retrieve, but current RL-based training methods suffer from sparse outcome rewards that discard intermediate signals and low sample efficiency where failed samples contribute nothing. We propose Search-P1, a framework that introduces path-centric reward shaping for agentic RAG training, comprising two key components: (1) Path-Centric Reward, which evaluates the structural quality of reasoning trajectories through order-agnostic step coverage and soft scoring that extracts learning signals even from failed samples, and (2) Dual-Track Path Scoring with offline-generated reference planners that assesses paths from both self-consistency and reference-alignment perspectives. Experiments on multiple QA benchmarks demonstrate that Search-P1 achieves significant improvements over Search-R1 and other strong baselines, with an average accuracy gain of 7.7 points.

</details>


### [47] [Towards Faithful Industrial RAG: A Reinforced Co-adaptation Framework for Advertising QA](https://arxiv.org/abs/2602.22584)
*Wenwei Li,Ming Xu,Tianle Xia,Lingxiang Hu,Yiding Sun,Linfang Shang,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: 提出了一种针对工业广告问答的强化协同自适应机制，显著减少生成虚假URL等幻觉内容并提升答案质量，已成功在真实生产环境大规模应用。


<details>
  <summary>Details</summary>
Motivation: 工业广告问答面临高风险，幻觉内容（尤其虚假URL）可能带来财务与合规风险。现有RAG虽常用，但受限于工业知识的高度关联性、频繁更新与生成对齐度不足，实际落地难度大。

Method: 提出了两个关键组件：(1) 图感知检索（GraphRAG），利用高引用知识子图建模实体关系，实现多跳、领域特定的证据选择；(2) 基于多维奖励（真实性、风格合规、安全性、URL有效性）的基团相对策略优化（GRPO）强化学习，约束生成内容。

Result: 在广告QA内部数据集上，与专家评判维度（准确性、完整性、安全性）相比，取得持续提升，幻觉内容减少72%；线上AB测试中like率提升28.6%，dislike率降低46.2%，URL幻觉下降92.7%。系统已稳定运行半年以上，服务数百万次QA交互。

Conclusion: 通过检索与生成的协同强化优化，有效提升广告领域问答系统的安全、准确及可用性，为大规模高风险应用提供了可行路径。

Abstract: Industrial advertising question answering (QA) is a high-stakes task in which hallucinated content, particularly fabricated URLs, can lead to financial loss, compliance violations, and legal risk. Although Retrieval-Augmented Generation (RAG) is widely adopted, deploying it in production remains challenging because industrial knowledge is inherently relational, frequently updated, and insufficiently aligned with generation objectives. We propose a reinforced co-adaptation framework that jointly optimizes retrieval and generation through two components: (1) Graph-aware Retrieval (GraphRAG), which models entity-relation structure over a high-citation knowledge subgraph for multi-hop, domain-specific evidence selection; and (2) evidence-constrained reinforcement learning via Group Relative Policy Optimization (GRPO) with multi-dimensional rewards covering faithfulness, style compliance, safety, and URL validity. Experiments on an internal advertising QA dataset show consistent gains across expert-judged dimensions including accuracy, completeness, and safety, while reducing the hallucination rate by 72\%. A two-week online A/B test demonstrates a 28.6\% increase in like rate, a 46.2\% decrease in dislike rate, and a 92.7\% reduction in URL hallucination. The system has been running in production for over half a year and has served millions of QA interactions.

</details>


### [48] [dLLM: Simple Diffusion Language Modeling](https://arxiv.org/abs/2602.22661)
*Zhanhui Zhou,Lingjie Chen,Hanghang Tong,Dawn Song*

Main category: cs.CL

TL;DR: 本文介绍了dLLM，一个统一扩散语言模型（DLM）核心组件的开源框架，方便用户复现、微调、部署和评估DLMs，并降低其研究门槛。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型的核心组件分散于不同代码库或缺乏透明实现，难以复现和扩展，学界和工业推进亟需一个标准化、又灵活兼容新方法的统一框架。

Method: 提出开源框架dLLM，统一扩散语言建模流程，包括训练、推理与评测，设计可高度自定义，支持现有和新兴高性能模型。框架附带可复现的小模型训练脚本，可将BERT等编码器或自回归模型转为DLM，并开放相关模型参数以便社区使用。

Result: dLLM能够标准化流程，支持主流和小型DLM的快速复现和二次开发，用户能够基于统一接口高效完成定制任务。已开源一系列可复现的小型DLM的检查点以降低研究/应用门槛。

Conclusion: dLLM框架促进了DLM领域的标准化、模型可复现性和创新研究，有望推动DLM模型的普及及后续技术发展。

Abstract: Although diffusion language models (DLMs) are evolving quickly, many recent models converge on a set of shared components. These components, however, are distributed across ad-hoc research codebases or lack transparent implementations, making them difficult to reproduce or extend. As the field accelerates, there is a clear need for a unified framework that standardizes these common components while remaining flexible enough to support new methods and architectures.
  To address this gap, we introduce dLLM, an open-source framework that unifies the core components of diffusion language modeling -- training, inference, and evaluation -- and makes them easy to customize for new designs. With dLLM, users can reproduce, finetune, deploy, and evaluate open-source large DLMs such as LLaDA and Dream through a standardized pipeline. The framework also provides minimal, reproducible recipes for building small DLMs from scratch with accessible compute, including converting any BERT-style encoder or autoregressive LM into a DLM. We also release the checkpoints of these small DLMs to make DLMs more accessible and accelerate future research.

</details>


### [49] [Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization](https://arxiv.org/abs/2602.22675)
*Qianben Chen,Tianrui Qin,King Zhu,Qiexiang Wang,Chengjun Yu,Shu Xu,Jiaqi Wu,Jiayu Zhang,Xinpeng Liu,Xin Gui,Jingyi Cao,Piaohong Wang,Dingfeng Shi,He Zhu,Tiannan Wang,Yuqing Wang,Maojia Song,Tianyu Zheng,Ge Zhang,Jian Yang,Jiaheng Liu,Minghao Liu,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 现有深度研究智能体主要通过提升推理深度来增强性能，但代价是高昂的推理成本和延迟。本文提出SMTL框架，通过并行证据获取替代串行推理，实现更高效的搜索与泛化能力。实验表现优异，同时推理步数和准确率均有提升。


<details>
  <summary>Details</summary>
Motivation: 当前智能体在复杂研究场景中提升性能往往依赖增加推理层数，带来推理成本高、延迟大等问题，并且在不同类型任务间泛化能力弱。因此需要既高效又具备泛化能力的智能体方案。

Method: 提出SMTL（Search More, Think Less）框架，采用并行证据获取替代传统的串行推理，有效管理受限的上下文资源。引入统一的数据合成流程，涵盖确定性问答和开放式搜索任务，并配套相应评测指标。通过有监督微调和强化学习建立端到端智能体。

Result: 在BrowseComp、GAIA、Xbench和DeepResearch Bench等基准上达到或超过现有最好成绩。与对比系统Mirothinker-v1.0相比，在BrowseComp数据集上将平均推理步数减少70.7%，准确率反而提高。

Conclusion: SMTL框架实现了高效的推理和广泛的任务泛化能力，在多项基准测试中表现突出，为解决深度智能体推理效率与泛化难题提供了有效思路。

Abstract: Recent deep research agents primarily improve performance by scaling reasoning depth, but this leads to high inference cost and latency in search-intensive scenarios. Moreover, generalization across heterogeneous research settings remains challenging. In this work, we propose \emph{Search More, Think Less} (SMTL), a framework for long-horizon agentic search that targets both efficiency and generalization. SMTL replaces sequential reasoning with parallel evidence acquisition, enabling efficient context management under constrained context budgets. To support generalization across task types, we further introduce a unified data synthesis pipeline that constructs search tasks spanning both deterministic question answering and open-ended research scenarios with task appropriate evaluation metrics. We train an end-to-end agent using supervised fine-tuning and reinforcement learning, achieving strong and often state of the art performance across benchmarks including BrowseComp (48.6\%), GAIA (75.7\%), Xbench (82.0\%), and DeepResearch Bench (45.9\%). Compared to Mirothinker-v1.0, SMTL with maximum 100 interaction steps reduces the average number of reasoning steps on BrowseComp by 70.7\%, while improving accuracy.

</details>


### [50] [Enhancing Persuasive Dialogue Agents by Synthesizing Cross-Disciplinary Communication Strategies](https://arxiv.org/abs/2602.22696)
*Shinnosuke Nozue,Yuto Nakano,Yotaro Watanabe,Meguru Takasaki,Shoji Moriya,Reina Akama,Jun Suzuki*

Main category: cs.CL

TL;DR: 本文提出了一种融合多学科策略的新型说服对话智能体设计框架，并在两个数据集上验证了其有效性，表现出优异的说服成功率和良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前说服对话系统主要依赖有限的预设策略，难以反映现实中的复杂人际交互。因此，作者致力于开发更复杂且有效的说服对话模型，以提升与用户真实互动时的表现。

Method: 作者基于社会心理学、行为经济学和传播理论中的有效说服策略，提出了一个跨学科的说服对话智能体设计框架。该框架在“Persuasion for Good”与“DailyPersuasion”两个具代表性的数据集上进行了实验验证。

Result: 框架在两个数据集上均实现了较高的说服成功率，表现出了良好的结果，尤其在说服最初意愿较低人群方面成效突出，显示了其优于传统方法的优势。

Conclusion: 提出的多学科框架能更好地捕捉真实复杂场景下的说服过程，有效提升了说服对话系统的普适性与实际应用能力，尤其在处理低意愿用户时表现出显著效果。

Abstract: Current approaches to developing persuasive dialogue agents often rely on a limited set of predefined persuasive strategies that fail to capture the complexity of real-world interactions. We applied a cross-disciplinary approach to develop a framework for designing persuasive dialogue agents that draws on proven strategies from social psychology, behavioral economics, and communication theory. We validated our proposed framework through experiments on two distinct datasets: the Persuasion for Good dataset, which represents a specific in-domain scenario, and the DailyPersuasion dataset, which encompasses a wide range of scenarios. The proposed framework achieved strong results for both datasets and demonstrated notable improvement in the persuasion success rate as well as promising generalizability. Notably, the proposed framework also excelled at persuading individuals with initially low intent, which addresses a critical challenge for persuasive dialogue agents.

</details>


### [51] [Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue](https://arxiv.org/abs/2602.22697)
*Ning Gao,Wei Zhang,Yuqin Dai,Ling Shi,Ziyin Wang,Yujie Wang,Wei He,Jinpeng Wang,Chaozheng Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新框架InteractCS-RL，将任务型对话重构为多粒度强化学习过程，有效平衡情感交流与成本感知型决策，并在多场景下取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 伴随大语言模型（LLMs）发展，聊天机器人正向更通用的智能体演进，但“同理心沟通”与“成本感知决策”难以兼顾，现有方法未能捕捉复杂策略权衡，亟需新方法突破。

Method: (1) 搭建以用户为中心的高保真互动框架，为智能体动态探索多策略提供训练环境；(2) 提出成本感知多轮策略优化（CMPO）算法，结合生成过程奖励与PID-Lagrangian成本控制，引导策略在“用户收益-全局成本约束”间探索帕累托最优。

Result: 在定制真实业务场景的实验证明，InteractCS-RL在三大评估维度上明显超越基线模型；此外，在工具-智能体-用户交互基准上的评测验证其在不同领域的强鲁棒性。

Conclusion: InteractCS-RL为任务型对话领域在情感与成本权衡方面带来显著提升，并展现出在多样化实际场景应用的广阔前景。

Abstract: The rapid evolution of Large Language Models (LLMs) has accelerated the transition from conversational chatbots to general agents. However, effectively balancing empathetic communication with budget-aware decision-making remains an open challenge. Since existing methods fail to capture these complex strategic trade-offs, we propose InteractCS-RL, a framework that reframes task-oriented dialogue as a multi-granularity reinforcement learning process. Specifically, we first establish a User-centric Interaction Framework to provide a high-fidelity training gym, enabling agents to dynamically explore diverse strategies with persona-driven users. Then, we introduce Cost-aware Multi-turn Policy Optimization (CMPO) with a hybrid advantage estimation strategy. By integrating generative process credits and employing a PID-Lagrangian cost controller, CMPO effectively guides the policy to explore Pareto boundary between user reward and global cost constraints. Extensive experiments on customized real business scenarios demonstrate that InteractCS-RL significantly outperform other baselines across three evaluation dimensions. Further evaluation on tool-agent-user interaction benchmarks verify InteractCS-RL robustness across diverse domains.

</details>


### [52] [Tokenization, Fusion and Decoupling: Bridging the Granularity Mismatch Between Large Language Models and Knowledge Graphs](https://arxiv.org/abs/2602.22698)
*Siyue Su,Jian Yang,Bo Li,Guanglin Niu*

Main category: cs.CL

TL;DR: 该论文提出了KGT框架，使用专用实体token，实现高效、全空间的知识图谱补全，并在多个基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前利用大语言模型（LLM）进行知识图谱补全时，存在粒度不匹配问题——LLM处理的是token序列，而知识图谱的基本单元是实体，现有方法很难兼顾文本语义和图结构信息。

Method: 1) 引入专用实体token化方式，直接构建实体级别的特征表示。2) 通过关系引导门控机制，将预训练得到的结构特征与文本特征融合进统一的embedding中，避免从头训练。3) 实现解耦预测，用独立head分别处理并结合语义推理与结构推理。

Result: 实验证明，KGT在多个知识图谱补全主流数据集上，性能均优于现有最新方法。

Conclusion: KGT解决了实体表示粒度匹配问题，有效结合了语义和结构信息，大幅提升了知识图谱补全效果。

Abstract: Leveraging Large Language Models (LLMs) for Knowledge Graph Completion (KGC) is promising but hindered by a fundamental granularity mismatch. LLMs operate on fragmented token sequences, whereas entities are the fundamental units in knowledge graphs (KGs) scenarios. Existing approaches typically constrain predictions to limited candidate sets or align entities with the LLM's vocabulary by pooling multiple tokens or decomposing entities into fixed-length token sequences, which fail to capture both the semantic meaning of the text and the structural integrity of the graph. To address this, we propose KGT, a novel framework that uses dedicated entity tokens to enable efficient, full-space prediction. Specifically, we first introduce specialized tokenization to construct feature representations at the level of dedicated entity tokens. We then fuse pre-trained structural and textual features into these unified embeddings via a relation-guided gating mechanism, avoiding training from scratch. Finally, we implement decoupled prediction by leveraging independent heads to separate and combine semantic and structural reasoning. Experimental results show that KGT consistently outperforms state-of-the-art methods across multiple benchmarks.

</details>


### [53] [Affine-Scaled Attention: Towards Flexible and Stable Transformer Attention](https://arxiv.org/abs/2602.23057)
*Jeongin Bae,Baeseong Park,Gunho Park,Minsub Kim,Joonhyung Lee,Junhee Yoo,Sunghyeon Woo,Jiwon Ryu,Se Jung Kwon,Dongsoo Lee*

Main category: cs.CL

TL;DR: 论文提出了一种称为仿射缩放注意力（Affine-Scaled Attention）的新注意力机制，通过引入输入相关的缩放和偏置，对标准的softmax注意力进行改进，从而提升模型训练稳定性和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer中的softmax注意力强制实现权重归一化，虽然效果不错，但限制了注意力幅度的灵活控制，且可能导致训练过程中过度集中的或不稳定的注意力分布。已有的改进方法如attention sinks或门控机制效果有限，缺乏直接控制注意力重加权的能力。

Method: 提出Affine-Scaled Attention，在标准注意力机制的softmax归一化权重基础上，引入依赖输入的缩放因子和偏置项，既放宽了严格的归一化约束，又能保持对value的聚合，赋予模型对注意力分布及强度的可控性。

Result: 在大规模语言模型的预训练实验中，各种模型规模下，与标准softmax注意力和attention sink基线相比，新方法在训练稳定性、优化行为及下游任务表现上均有稳定提升。

Conclusion: 适度地对注意力输出做重加权是一种切实可行、有效提升Transformer注意力机制表现的方法，尤其对于提升训练稳定性和下游任务性能效果明显。

Abstract: Transformer attention is typically implemented using softmax normalization, which enforces attention weights with unit sum normalization. While effective in many settings, this constraint can limit flexibility in controlling attention magnitudes and may contribute to overly concentrated or unstable attention patterns during training. Prior work has explored modifications such as attention sinks or gating mechanisms, but these approaches provide only limited or indirect control over attention reweighting. We propose Affine-Scaled Attention, a simple extension to standard attention that introduces input-dependent scaling and a corresponding bias term applied to softmax-normalized attention weights. This design relaxes the strict normalization constraint while maintaining aggregation of value representations, allowing the model to adjust both the relative distribution and the scale of attention in a controlled manner.
  We empirically evaluate Affine-Scaled Attention in large-scale language model pretraining across multiple model sizes. Experimental results show consistent improvements in training stability, optimization behavior, and downstream task performance compared to standard softmax attention and attention sink baselines. These findings suggest that modest reweighting of attention outputs provides a practical and effective way to improve attention behavior in Transformer models.

</details>


### [54] [Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs](https://arxiv.org/abs/2602.23136)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 多模态大模型虽然能处理语音和图像信息，但无法有效识别说话人的声音或物体的质地，因为当前解码器只能提取与文本对齐的信息。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态LLM可以输入语音和图像，但其在部分模态特性提取上存在缺陷，如无法‘听见’说话人的声音特征或‘看见’物体的高级视觉纹理。作者想探究产生这些问题的根因。

Method: 作者利用线性探测器分析各种编码层对语音情感、说话人身份和视觉属性信息的保留情况，并进行了解码器损失实验。同时，他们通过公式化提出解码器受限于文本对齐方向的‘解码器不匹配’问题，并用GMI（广义互信息）衡量信息可访问性。跨模型和受控实验验证了解码器规则才是瓶颈，并通过LoRA微调引入情感目标进一步佐证。

Result: 结果发现，编码器确实保留了大量模态信息，但解码器因训练目标和机制限制，无法有效利用这些模态特异性方向上的信息。调整训练目标后，情感信息可访问性提高了7.5%，且不影响其他属性。

Conclusion: 多模态模型的信息利用瓶颈主要在于解码器的训练目标，而不是编码器或输入投射。提升训练目标的多样性和针对性，可以有效提升信息可利用性。

Abstract: Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\times$ above chance in linear probes), yet removing 64--71% of modality-specific variance improves decoder loss. The decoder has no learned use for these directions; their presence is noise.
  We formalize this as a mismatched decoder problem: a decoder trained on text can only extract information along text-aligned directions. Accessible information is bounded by the Generalized Mutual Information (GMI), with degradation scaling with distributional distance and decoder sensitivity. The bound is a property of the decoder's scoring rule, not of any particular architecture; it applies whether non-text inputs arrive through a learned projection, a discrete codebook, or no explicit adapter at all. We validate this across five models spanning speech and vision. A controlled experiment (two Prismatic VLMs differing only in encoder text-alignment) confirms the bottleneck is the decoder's scoring rule, not the encoder or projection. A LoRA intervention demonstrates the fix: training with an emotion objective improves emotion accessibility ($+$7.5%) without affecting other attributes, confirming that the training objective determines what becomes accessible.

</details>


### [55] [Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models](https://arxiv.org/abs/2602.23197)
*Chungpa Lee,Jy-yong Sohn,Kangwook Lee*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型在微调提升零样本性能时，为何会导致上下文学习能力的下降，并通过理论和实验找到了缓解方法。


<details>
  <summary>Details</summary>
Motivation: 目前大模型常通过少量样例（in-context learning, ICL）适应下游任务，但实际中常用微调提升零样本能力，二者存在冲突；理解并缓解这一问题对模型泛化很重要。

Method: 作者使用线性注意力模型，从理论上分析微调对于注意力参数的影响，揭示何种参数的更新会削弱ICL能力，并在真实实验中验证了理论结论。

Result: 实验证明：全部微调注意力参数会损害ICL能力，仅微调Value矩阵可提升零样本性能同时保持ICL，加入辅助few-shot loss会使目标任务的ICL增强，但对未见任务的ICL有损害。

Conclusion: 合理限制微调参数（如只更新Value矩阵）或损失函数设计可提升零样本性能且兼顾上下文泛化能力；但提升目标任务ICL能力时，可能以牺牲泛化为代价。

Abstract: Transformer-based large language models exhibit in-context learning, enabling adaptation to downstream tasks via few-shot prompting with demonstrations. In practice, such models are often fine-tuned to improve zero-shot performance on downstream tasks, allowing them to solve tasks without examples and thereby reducing inference costs. However, fine-tuning can degrade in-context learning, limiting the performance of fine-tuned models on tasks not seen during fine-tuning. Using linear attention models, we provide a theoretical analysis that characterizes how fine-tuning objectives modify attention parameters and identifies conditions under which this leads to degraded few-shot performance. We show that fine-tuning all attention parameters can harm in-context learning, whereas restricting updates to the value matrix improves zero-shot performance while preserving in-context learning. We further show that incorporating an auxiliary few-shot loss enhances in-context learning primarily on the target task, at the expense of degraded in-context learning ability on tasks not seen during fine-tuning. We empirically validate our theoretical results.

</details>


### [56] [Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning](https://arxiv.org/abs/2602.23351)
*Amita Kamath,Jack Hessel,Khyathi Chandu,Jena D. Hwang,Kai-Wei Chang,Ranjay Krishna*

Main category: cs.CL

TL;DR: VLMs在空间、时间、否定和计数等推理任务上的能力不足，原因与训练数据中隐性信息缺乏（报道偏差）有关，光靠数据和模型规模扩大无法自动获得这些推理能力，需有针对性数据标注。


<details>
  <summary>Details</summary>
Motivation: 尽管当前视觉-语言模型（VLMs）取得了进展，但其推理能力有限，尤其在一些常识推理或隐含信息理解方面薄弱。本研究旨在探究造成这一现象的根本原因。

Method: 作者分析了流行VLM，OpenCLIP、LLaVA-1.5和Molmo的训练数据，结合语用学理论，研究训练数据中报道偏差如何影响模型对空间、时间、否定和计数推理能力。设计了一组特定基准测试，并评估当前模型的表现。

Result: 发现上述VLM在训练数据中隐性被抑制的推理技能（空间、时间、否定、计数）表现较差。增加训练数据规模、模型规模以及多语言等方式，并不能自然提升这些推理能力，而有针对性地引入包含隐性信息的标注数据能有效提升这些能力。

Conclusion: 为提升VLM的推理能力，仅靠数据和模型规模的扩大远远不够，需更加注重数据收集与标注的针对性和完整性，以弥补训练素材中的隐性信息缺失，实现推理能力的有效提升。

Abstract: The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., "at the game today!" is a more likely caption than "a photo of 37 people standing behind a field". We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [57] [SODA-CitrON: Static Object Data Association by Clustering Multi-Modal Sensor Detections Online](https://arxiv.org/abs/2602.22243)
*Jan Nausner,Kilian Wohlleben,Michael Hubner*

Main category: cs.RO

TL;DR: 本论文提出了一种新颖的静态对象数据关联方法（SODA-CitrON），用于多传感器数据的在线融合与跟踪，在多种评测下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有如JPDA等关联方法多针对动态目标，面对静态对象且传感器异构、不连续观测与噪声不确定时效果不佳，因而需要新的数据关联解决方案。

Method: 提出了一种完全在线、无监督的机器学习方法SODA-CitrON，对异构多模态传感器观测数据进行聚类，同时估计静态对象位置并保持轨迹。方法能处理时间无关、多传感器输入，复杂度为对数线性，且输出可解释。

Result: 通过多种蒙特卡洛仿真场景将所提方法与Bayesian filtering、DBSTREAM、JPDA等先进方法对比，在F1分数、位置RMSE、MOTP、MOTA等指标上表现更优。

Conclusion: SODA-CitrON方法在静态对象映射与跟踪的场景下，在准确性和鲁棒性上均优于主流方法，适用于现实中复杂多传感器场景。

Abstract: The online fusion and tracking of static objects from heterogeneous sensor detections is a fundamental problem in robotics, autonomous systems, and environmental mapping. Although classical data association approaches such as JPDA are well suited for dynamic targets, they are less effective for static objects observed intermittently and with heterogeneous uncertainties, where motion models provide minimal discriminative with respect to clutter. In this paper, we propose a novel method for static object data association by clustering multi-modal sensor detections online (SODA-CitrON), while simultaneously estimating positions and maintaining persistent tracks for an unknown number of objects. The proposed unsupervised machine learning approach operates in a fully online manner and handles temporally uncorrelated and multi-sensor measurements. Additionally, it has a worst-case loglinear complexity in the number of sensor detections while providing full output explainability. We evaluate the proposed approach in different Monte Carlo simulation scenarios and compare it against state-of-the-art methods, including Bayesian filtering, DBSTREAM clustering, and JPDA. The results demonstrate that SODA-CitrON consistently outperforms the compared methods in terms of F1 score, position RMSE, MOTP, and MOTA in the static object mapping scenarios studied.

</details>


### [58] [Detection and Recognition: A Pairwise Interaction Framework for Mobile Service Robots](https://arxiv.org/abs/2602.22346)
*Mengyu Liang,Sarah Gillet Schlegel,Iolanda Leite*

Main category: cs.RO

TL;DR: 本文提出了一种适用于移动服务机器人识别人群中人际交互的新方法，利用轻量级的几何和运动信息，以较低的算力和模型复杂度实现了有效的交互感知。


<details>
  <summary>Details</summary>
Motivation: 现有人群行为识别方法多依赖于复杂大模型或骨架识别，算力消耗大且在户外环境难以部署。服务机器人（如清扫、割草机器人）需要高效、低成本且实用的交互感知方式，提高安全性和社会适应性。

Method: 设计了两阶段框架：第一步基于几何与运动线索快速筛选潜在人际交互对；第二步用关系网络对交互类型进行粗粒度分类。模型计算量小，无须高质量外观或骨架输入。

Result: 在JRDB数据集上，该方法以更低算力和更小模型实现了与基于外观方法可比的准确率。还在Collective Activity数据集及真实割草机数据集零样本测试中展现出良好的通用性。

Conclusion: 基于成对几何与运动信息的轻量交互感知方法适用于移动服务机器人，既能满足实际部署需求，又方便未来与导航堆栈集成，具有良好应用前景。

Abstract: Autonomous mobile service robots, like lawnmowers or cleaning robots, operating in human-populated environments need to reason about local human-human interactions to support safe and socially aware navigation while fulfilling their tasks. For such robots, interaction understanding is not primarily a fine-grained recognition problem, but a perception problem under limited sensing quality and computational resources. Many existing approaches focus on holistic group activity recognition, which often requires complex and large models which may not be necessary for mobile service robots. Others use pairwise interaction methods which commonly rely on skeletal representations but their use in outdoor environments remains challenging. In this work, we argue that pairwise human interaction constitute a minimal yet sufficient perceptual unit for robot-centric social understanding. We study the problem of identifying interacting person pairs and classifying coarse-grained interaction behaviors sufficient for downstream group-level reasoning and service robot decision-making. To this end, we adopt a two-stage framework in which candidate interacting pairs are first identified based on lightweight geometric and motion cues, and interaction types are subsequently classified using a relation network. We evaluate the proposed approach on the JRDB dataset, where it achieves sufficient accuracy with reduced computational cost and model size compared to appearance-based methods. Additional experiments on the Collective Activity Dataset and zero shot test on a lawnmower-collected dataset further illustrate the generality of the proposed framework. These results suggest that pairwise geometric and motion cues provide a practical basis for interaction perception on mobile service robot providing a promising method for integration into mobile robot navigation stacks in future work. Code will be released soon

</details>


### [59] [Hierarchical Trajectory Planning of Floating-Base Multi-Link Robot for Maneuvering in Confined Environments](https://arxiv.org/abs/2602.22459)
*Yicheng Chen,Jinjie Li,Haokun Liu,Zicheng Luo,Kotaro Kaneko,Moju Zhao*

Main category: cs.RO

TL;DR: 该论文提出了一种分层轨迹规划框架，实现了多连杆悬浮基机器人的高效避障和动态可行轨迹生成，并通过真实机器人实验验证了效果。


<details>
  <summary>Details</summary>
Motivation: 多连杆悬浮基机器人能够在狭小环境灵活变形，适合自主检查和搜救任务。但由于其高维、受限且须处理碰撞规避与动力学约束，轨迹规划极具挑战性。

Method: 提出分层轨迹规划框架，结合全局引导与局部优化。首先根据机器人根链刚体特性与关节柔性设计分段锚点，将全局问题划分为易处理的子问题。然后设计并行优化的局部轨迹规划器，采用可微目标及约束，系统保证运动学与动力学可行性。直接处理点云数据，无需手工建模障碍物。

Result: 论文在大量仿真和真实机器人实验中验证，系统能让多连杆机器人充分利用形态完成刚体机器人无法实现的灵活机动。

Conclusion: 首次实现了从原始点云出发，在真实多连杆悬浮基机器人上生成连续避障且动力学可行轨迹的自动规划系统，无需手工定义障碍模型，具有重要应用前景。

Abstract: Floating-base multi-link robots can change their shape during flight, making them well-suited for applications in confined environments such as autonomous inspection and search and rescue. However, trajectory planning for such systems remains an open challenge because the problem lies in a high-dimensional, constraint-rich space where collision avoidance must be addressed together with kinematic limits and dynamic feasibility. This work introduces a hierarchical trajectory planning framework that integrates global guidance with configuration-aware local optimization. First, we exploit the dual nature of these robots - the root link as a rigid body for guidance and the articulated joints for flexibility - to generate global anchor states that decompose the planning problem into tractable segments. Second, we design a local trajectory planner that optimizes each segment in parallel with differentiable objectives and constraints, systematically enforcing kinematic feasibility and maintaining dynamic feasibility by avoiding control singularities. Third, we implement a complete system that directly processes point-cloud data, eliminating the need for handcrafted obstacle models. Extensive simulations and real-world experiments confirm that this framework enables an articulated aerial robot to exploit its morphology for maneuvering that rigid robots cannot achieve. To the best of our knowledge, this is the first planning framework for floating-base multi-link robots that has been demonstrated on a real robot to generate continuous, collision-free, and dynamically feasible trajectories directly from raw point-cloud inputs, without relying on handcrafted obstacle models.

</details>


### [60] [EgoAVFlow: Robot Policy Learning with Active Vision from Human Egocentric Videos via 3D Flow](https://arxiv.org/abs/2602.22461)
*Daesol Cho,Youngseok Jang,Danfei Xu,Sehoon Ha*

Main category: cs.RO

TL;DR: 本文提出EgoAVFlow模型，能从第一视角视频学习操控与主动视觉，不需要机器人演示也能提升机器人操作时的可见性与稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人通过仿真人类第一视角视频学习操作技能，但人类视角习惯并不总能满足机器人任务中的可见性需求，容易导致任务中关键视觉信息丢失。

Method: EgoAVFlow基于一个三维流表示，采用扩散模型联合预测机器人动作、未来三维流与相机轨迹，并在测试时通过基于可见性奖励的去噪过程实时优化视角，无需机器人演示即可完成可见性推理和任务操作。

Result: 在实际场景中，EgoAVFlow能够在不断变化的视角下，实现比之前人类演示方法更好的关键可见性维护和稳健操作效果。

Conclusion: EgoAVFlow展示了无需机器人演示，仅靠人类第一视角视频就能有效提升主动视觉与操作性能，推动了机器人操作与主动视觉的融合研究。

Abstract: Egocentric human videos provide a scalable source of manipulation demonstrations; however, deploying them on robots requires active viewpoint control to maintain task-critical visibility, which human viewpoint imitation often fails to provide due to human-specific priors. We propose EgoAVFlow, which learns manipulation and active vision from egocentric videos through a shared 3D flow representation that supports geometric visibility reasoning and transfers without robot demonstrations. EgoAVFlow uses diffusion models to predict robot actions, future 3D flow, and camera trajectories, and refines viewpoints at test time with reward-maximizing denoising under a visibility-aware reward computed from predicted motion and scene geometry. Real-world experiments under actively changing viewpoints show that EgoAVFlow consistently outperforms prior human-demo-based baselines, demonstrating effective visibility maintenance and robust manipulation without robot demonstrations.

</details>


### [61] [When to Act, Ask, or Learn: Uncertainty-Aware Policy Steering](https://arxiv.org/abs/2602.22474)
*Jessie Yuan,Yilin Wu,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 本文提出了一种名为UPS（Uncertainty-aware Policy Steering）的新框架，可以在机器人部署过程中根据不确定性动态调整行为策略，减少人工干预，提升任务完成的可靠性与效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于Vision-Language Models（VLMs）的机器人策略依赖于模型信心评分，但VLMs常常过于自信，在任务语义不确定性和底层策略不确定性场景下，容易选错策略，导致性能下降。需有方法在面对高、低层次不确定性时更合理地做出决策并降低人工干预需求。

Method: UPS框架融合语义任务不确定性与低层次动作可行性的联合推断，根据置信度水平，自动选择合适的执行方案：高置信度下执行任务，有歧义时通过自然语言澄清，或判断底层策略无法胜任时请求人工干预。通过共形预测校准VLM与底层策略组合，可为决策过程提供统计保证，并结合部署期收集到的干预数据用残差学习提升底层策略能力。

Result: 实验在模拟和真实机器人硬件上进行，表明UPS能够有效区分任务的自信、模糊与无力场景，并在减少人工干预上优于未校准的基线方法及以往的持续学习方法。

Conclusion: UPS框架显著提升了机器人在部署过程中的灵活性与自适应能力，通过不确定性感知和干预学习，降低了对昂贵人工反馈的需求，对通用机器人自主行为有重要意义。

Abstract: Policy steering is an emerging way to adapt robot behaviors at deployment-time: a learned verifier analyzes low-level action samples proposed by a pre-trained policy (e.g., diffusion policy) and selects only those aligned with the task. While Vision-Language Models (VLMs) are promising general-purpose verifiers due to their reasoning capabilities, existing frameworks often assume these models are well-calibrated. In practice, the overconfident judgment from VLM can degrade the steering performance under both high-level semantic uncertainty in task specifications and low-level action uncertainty or incapability of the pre-trained policy. We propose uncertainty-aware policy steering (UPS), a framework that jointly reasons about semantic task uncertainty and low-level action feasibility, and selects an uncertainty resolution strategy: execute a high-confidence action, clarify task ambiguity via natural language queries, or ask for action interventions to correct the low-level policy when it is deemed incapable at the task. We leverage conformal prediction to calibrate the composition of the VLM and the pre-trained base policy, providing statistical assurances that the verifier selects the correct strategy. After collecting interventions during deployment, we employ residual learning to improve the capability of the pre-trained policy, enabling the system to learn continually but with minimal expensive human feedback. We demonstrate our framework through experiments in simulation and on hardware, showing that UPS can disentangle confident, ambiguous, and incapable scenarios and minimizes expensive user interventions compared to uncalibrated baselines and prior human- or robot-gated continual learning approaches. Videos can be found at https://jessie-yuan.github.io/ups/

</details>


### [62] [SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation](https://arxiv.org/abs/2602.22514)
*Xinyu Tan,Ningwei Bai,Harry Gardener,Zhengyang Zhong,Luoyu Zhang,Liuhaichen Yang,Zhekai Duan,Monkgogi Galeitsiwe,Zezhi Tang*

Main category: cs.RO

TL;DR: 本论文提出了首个基于手语的视觉-语言-动作（VLA）框架，无需中间的gloss注释，直接将视觉手势映射为语义指令，用以人机交互，尤其是在机器人控制中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的人机交互系统多依赖gloss作为桥梁，但这种注释成本高、信息损失严重，且不够自然，因此作者希望提出一种无需gloss、可扩展且自然的多模态交互框架。

Method: 设计了一个实时的字母级连续手语识别接口，通过几何归一化、时序平滑和词汇精炼步骤，将手势流转化为连贯的语言指令，从而实现低延迟、稳健的交互；并预留了融合transformer的高级gloss-free手语模型以支持未来扩展。

Result: 实验结果表明，该系统能够在多种交互场景下将手语解释为准确的机器人动作，且具有较高的鲁棒性、及时性和可解释性。

Conclusion: 该框架在可访问性、可扩展性和多模态人工智能方面展现出显著潜力，推动了包容性人机（机器人）互动的发展。

Abstract: We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions. This design reduces annotation cost and avoids the information loss introduced by gloss representations, enabling more natural and scalable multimodal interaction.
  In this work, we focus on a real-time alphabet-level finger-spelling interface that provides a robust and low-latency communication channel for robotic control. Compared with large-scale continuous sign language recognition, alphabet-level interaction offers improved reliability, interpretability, and deployment feasibility in safety-critical embodied environments. The proposed pipeline transforms continuous gesture streams into coherent language commands through geometric normalization, temporal smoothing, and lexical refinement, ensuring stable and consistent interaction.
  Furthermore, the framework is designed to support future integration of transformer-based gloss-free sign language models, enabling scalable word-level and sentence-level semantic understanding. Experimental results demonstrate the effectiveness of the proposed system in grounding sign-derived instructions into precise robotic actions under diverse interaction scenarios. These results highlight the potential of the framework to advance accessible, scalable, and multimodal embodied intelligence.

</details>


### [63] [Metamorphic Testing of Vision-Language Action-Enabled Robots](https://arxiv.org/abs/2602.22579)
*Pablo Valle,Sergio Segura,Shaukat Ali,Aitor Arrieta*

Main category: cs.RO

TL;DR: 本文探索了在VLA机器人任务控制模型测试中应用变异测试（MT）以缓解测试判定器问题，通过定义新的变异关系，有效检测多种模型失效类型，且方法具普适性。


<details>
  <summary>Details</summary>
Motivation: 目前VLA模型测试需针对每条指令单独定义判定标准（判定器），既复杂又难以推广；现有主流判定器主要基于符号化世界表达，未能评估任务质量，有测试盲区。作者试图引入无需明确定义判定器的变异测试，提升测试自动化和通用性。

Method: 提出两种变异关系模式和五条具体变异关系，通过对输入的变换检验输出行为变化，进而自动检测VLA模型在不同任务与机器人的故障。进行了包含5个VLA模型、2台仿真机器人和4个任务的实证研究。

Result: 实验证明，变异测试能够自动检出多种VLA模型失效类型（包括未完成任务等），不依赖于特殊测试判定器。提出的变异关系可泛化到不同模型、机器人和任务。

Conclusion: 变异测试为VLA模型测试判定提供新思路，所提方法能普适、高效地发现模型故障，有望显著提高VLA模型实际部署的可靠性与自动化测试水平。

Abstract: Vision-Language-Action (VLA) models are multimodal robotic task controllers that, given an instruction and visual inputs, produce a sequence of low-level control actions (or motor commands) enabling a robot to execute the requested task in the physical environment. These systems face the test oracle problem from multiple perspectives. On the one hand, a test oracle must be defined for each instruction prompt, which is a complex and non-generalizable approach. On the other hand, current state-of-the-art oracles typically capture symbolic representations of the world (e.g., robot and object states), enabling the correctness evaluation of a task, but fail to assess other critical aspects, such as the quality with which VLA-enabled robots perform a task. In this paper, we explore whether Metamorphic Testing (MT) can alleviate the test oracle problem in this context. To do so, we propose two metamorphic relation patterns and five metamorphic relations to assess whether changes to the test inputs impact the original trajectory of the VLA-enabled robots. An empirical study involving five VLA models, two simulated robots, and four robotic tasks shows that MT can effectively alleviate the test oracle problem by automatically detecting diverse types of failures, including, but not limited to, uncompleted tasks. More importantly, the proposed MRs are generalizable, making the proposed approach applicable across different VLA models, robots, and tasks, even in the absence of test oracles.

</details>


### [64] [Designing Robots for Families: In-Situ Prototyping for Contextual Reminders on Family Routines](https://arxiv.org/abs/2602.22628)
*Michael F. Xu,Enhui Zhao,Yawen Zhang,Joseph E. Michaelis,Sarah Sebo,Bilge Mutlu*

Main category: cs.RO

TL;DR: 本文通过在十个家庭中共创和部署家庭社交机器人，研究机器人在家庭日常中的可持续角色，并提出家庭环境下设计机器人的建议。


<details>
  <summary>Details</summary>
Motivation: 尽管服务机器人逐渐进入家庭，但其与家庭日常真正融合仍存在诸多挑战。作者希望通过理解家庭日常惯例，寻找机器人入驻家庭生活的有效切入点。

Method: 与十个家庭共同设计机器人在家庭日常场景中的互动与行为，并制定支持特定家庭惯例的计划（考虑时间、参与者、地点和活动等环境因素）。随后，研发并部署一个移动社交机器人，在用户家庭进行为期四天的实际研究。

Result: 家长普遍欢迎机器人协助提醒，减轻提醒的压力。但访谈显示，机器人介入时机、权威问题、家庭互动等方面存在一定的紧张与复杂性。

Conclusion: 基于对机器人在家庭融入过程中的洞察，提出了针对家庭环境设计机器人及情境提醒功能的实践建议，也讨论了更广泛的机器人家庭化设计考量。

Abstract: Robots are increasingly entering the daily lives of families, yet their successful integration into domestic life remains a challenge. We explore family routines as a critical entry point for understanding how robots might find a sustainable role in everyday family settings. Together with each of the ten families, we co-designed robot interactions and behaviors, and a plan for the robot to support their chosen routines, accounting for contextual factors such as timing, participants, locations, and the activities in the environment. We then designed, prototyped, and deployed a mobile social robot as a four-day, in-home user study. Families welcomed the robot's reminders, with parents especially appreciating the offloading of some reminding tasks. At the same time, interviews revealed tensions around timing, authority, and family dynamics, highlighting the complexity of integrating robots into households beyond the immediate task of reminders. Based on these insights, we offer design implications for robot-facilitated contextual reminders and discuss broader considerations for designing robots for family settings.

</details>


### [65] [Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline](https://arxiv.org/abs/2602.22663)
*Wenxuan Song,Jiayi Chen,Xiaoquan Sun,Huashuo Lei,Yikai Qin,Wei Zhao,Pengxiang Ding,Han Zhao,Tongxin Wang,Pengxu Hou,Zhide Zhong,Haodong Yan,Donglin Wang,Jun Ma,Haoang Li*

Main category: cs.RO

TL;DR: 本文提出了一个新的通用机器人基准（CEBench），并基于此提出了一个轻量级、高实用性的VLA模型（LLaVA-VLA），在保持高性能的同时大大减小参数规模和预训练成本，适用于多种物理实体和场景。所有数据和代码将在论文接收后开源。


<details>
  <summary>Details</summary>
Motivation: 当前Vision-Language-Action（VLA）模型面临参数庞大、预训练门槛高、应用场景单一等问题，导致其在实际机器人平台上的部署不切实际。为推动VLA模型的实际落地与通用性，有必要设计更加轻量、通用且实用性的基准和模型。

Method: 1. 提出了包含仿真和真实场景、具备领域随机化的新基准CEBench，收集了1.44万条仿真轨迹与1,600条真实专家演示。2. 构建LLaVA-VLA模型，采用紧凑的VLM主干，融合多视角感知、自身体感编码与动作分块，并引入“两阶段训练”以降低训练门槛。3. 实现动作空间扩展，统一导航与操作任务。

Result: 实验证明LLaVA-VLA在多种实体（embodiment）上具备良好的泛化能力与多样适应性，且作为首个端到端的移动操作VLA模型，在真实移动机器人实验中表现优异。

Conclusion: LLaVA-VLA极大提升了VLA模型的实际部署可行性和通用性，实验和基准为后续VLA研究提供了新方向和基础——相关数据、代码和模型也将全部开源，助力社区复现与后续研究。

Abstract: Vision-Language-Action (VLA) models have emerged as a generalist robotic agent. However, existing VLAs are hindered by excessive parameter scales, prohibitive pre-training requirements, and limited applicability to diverse embodiments. To improve the practicality of VLAs, we propose a comprehensive benchmark and an improved baseline. First, we propose CEBench, a new benchmark spanning diverse embodiments in both simulation and the real world with consideration of domain randomization. We collect 14.4k simulated trajectories and 1.6k real-world expert-curated trajectories to support training on CEBench. Second, using CEBench as our testbed, we study three critical aspects of VLAs' practicality and offer several key findings. Informed by these findings, we introduce LLaVA-VLA, a lightweight yet powerful VLA designed for practical deployment on consumer-grade GPUs. Architecturally, it integrates a compact VLM backbone with multi-view perception, proprioceptive tokenization, and action chunking. To eliminate reliance on costly pre-training, LLaVA-VLA adopts a two-stage training paradigm including post-training and fine-tuning. Furthermore, LLaVA-VLA extends the action space to unify navigation and manipulation. Experiments across embodiments demonstrate the capabilities of generalization and versatility of LLaVA-VLA , while real-world mobile manipulation experiments establish it as the first end-to-end VLA model for mobile manipulation. We will open-source all datasets, codes, and checkpoints upon acceptance to foster reproducibility and future research.

</details>


### [66] [Does the testing environment matter? Carsickness across on-road, test-track, and driving simulator conditions](https://arxiv.org/abs/2602.22671)
*Georgios Papaioannou,Barys Shyrokau*

Main category: cs.RO

TL;DR: 论文比较了真实道路、测试跑道和驾驶模拟器三种环境下汽车晕动症的发生，并分析了模拟器条件下与现实环境的差异。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶汽车普及，晕车问题越来越受关注。然而，目前缺乏统一的评测标准，使得不同研究难以比较。因此，有必要验证并比较不同实验环境下晕车测量的有效性和结果差异。

Method: 论文基于已有真实道路与测试跑道晕车实验，再次用带运动底座的驾驶模拟器重现实验。28名被试在非驾驶任务中，根据Misery Scale和Motion Sickness Assessment Questionnaire分别在实验中及后续报告晕车程度，同时还评估了相关心理因素。对各环境下的主客观数据和加速度等客观指标进行了比较分析。

Result: 主观及客观晕动症评分显示，驾驶模拟器环境下晕车分数明显低于真实道路和测试跑道。主要原因是模拟器无法完全重现最易致晕的低频（<0.5Hz）运动。

Conclusion: 驾驶模拟器在研究晕车问题的有效性上存在局限，尤其是在重现真实低频运动方面。因此，使用模拟器得到的数据与真实情况存在显著差异，跨环境结果需谨慎解读。

Abstract: Carsickness has gained significant attention with the rise of automated vehicles, prompting extensive research across on-road, test-track, and driving simulator environments to understand its occurrence and develop mitigation strategies. However, the lack of carsickness standardization complicates comparisons across studies and environments. Previous works demonstrate measurement validity between two setups at most (e.g., on-road vs. driving simulator), leaving gaps in multi-environment comparisons. This study investigates the recreation of an on-road motion sickness exposure - previously replicated on a test track - using a motion-based driving simulator. Twenty-eight participants performed an eyes-off-road non-driving task while reporting motion sickness using the Misery Scale during the experiment and the Motion Sickness Assessment Questionnaire afterward. Psychological factors known to influence motion sickness were also assessed. The results present subjective and objective measurements for motion sickness across the considered environments. In this paper, acceleration measurements, objective metrics and subjective motion sickness ratings across environments are compared, highlighting key differences in sickness occurrence for simulator-based research validity. Significantly lower motion sickness scores are reported in the simulator compared to on-road and test-track conditions, due to its limited working envelope to reproduce low-frequency (<0.5 Hz) motions, which are the most provocative for motion sickness.

</details>


### [67] [SCOPE: Skeleton Graph-Based Computation-Efficient Framework for Autonomous UAV Exploration](https://arxiv.org/abs/2602.22707)
*Kai Li,Shengtao Zheng,Linkun Xiu,Yuze Sheng,Xiao-Ping Zhang,Dongyue Huang,Xinlei Chen*

Main category: cs.RO

TL;DR: 提出SCOPE框架，利用实时骨架图和隐式未知区域分析，实现了高效、低延迟的自主探索，显著降低计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有自主探索方法对频繁的全局优化依赖度高，导致高计算延迟和轨迹震荡，尤其在资源受限设备上表现不佳。

Method: SCOPE框架采用增量式骨架图构建与隐式未知区域分析，分层规划策略由Proximal Planner负责本地轨迹高频平滑生成，Region-Sequence Planner仅在必要时优化全局区域顺序。

Result: 仿真实验显示SCOPE探索性能与先进全局规划器相当，但平均计算消耗降低86.9%；实地测试进一步验证了系统的健壮性和低延迟。

Conclusion: SCOPE有效提升移动机器人在复杂未知环境下的探索效率和速度，特别适合边缘设备资源有限的实际场景。

Abstract: Autonomous exploration in unknown environments is key for mobile robots, helping them perceive, map, and make decisions in complex areas. However, current methods often rely on frequent global optimization, suffering from high computational latency and trajectory oscillation, especially on resource-constrained edge devices. To address these limitations, we propose SCOPE, a novel framework that incrementally constructs a real-time skeletal graph and introduces Implicit Unknown Region Analysis for efficient spatial reasoning. The planning layer adopts a hierarchical on-demand strategy: the Proximal Planner generates smooth, high-frequency local trajectories, while the Region-Sequence Planner is activated only when necessary to optimize global visitation order. Comparative evaluations in simulation demonstrate that SCOPE achieves competitive exploration performance comparable to state-of-the-art global planners, while reducing computational cost by an average of 86.9%. Real-world experiments further validate the system's robustness and low latency in practical scenarios.

</details>


### [68] [Robust Helicopter Ship Deck Landing With Guaranteed Timing Using Shrinking-Horizon Model Predictive Control](https://arxiv.org/abs/2602.22714)
*Philipp Schitz,Paolo Mercorelli,Johann C. Dauer*

Main category: cs.RO

TL;DR: 本文提出了一种基于收缩视域模型预测控制（SHMPC）的高效自主直升机在移动船甲板上着陆算法，在强风和时序约束下实现高精度、高效率的安全着陆。


<details>
  <summary>Details</summary>
Motivation: 移动船甲板上的自主直升机着陆面临复杂的非线性动力学和强扰动等挑战，需要具备高鲁棒性和实时性的新算法。

Method: 1. 推导适用于模型预测控制的直升机动力学规划模型。2. 采用收缩视域MPC与专用触地控制器串联，实现预设操作时间窗口内精准着陆。3. 设计包含扰动反馈的辅助控制器以增强抗扰性能。

Result: 仿真结果表明，算法在强风下各项着陆操作均能高精度完成，同时满足时间和操作限制，最大计算耗时仅在毫秒级。

Conclusion: 所提方法可在满足实际操作和时序约束的同时，实现高效、精确和安全的自主直升机着陆，具备良好的实时性和鲁棒性。

Abstract: We present a runtime efficient algorithm for autonomous helicopter landings on moving ship decks based on Shrinking-Horizon Model Predictive Control (SHMPC). First, a suitable planning model capturing the relevant aspects of the full nonlinear helicopter dynamics is derived. Next, we use the SHMPC together with a touchdown controller stage to ensure a pre-specified maneuver time and an associated landing time window despite the presence of disturbances. A high disturbance rejection performance is achieved by designing an ancillary controller with disturbance feedback. Thus, given a target position and time, a safe landing with suitable terminal conditions is be guaranteed if the initial optimization problem is feasible. The efficacy of our approach is shown in simulation where all maneuvers achieve a high landing precision in strong winds while satisfying timing and operational constraints with maximum computation times in the millisecond range.

</details>


### [69] [Sapling-NeRF: Geo-Localised Sapling Reconstruction in Forests for Ecological Monitoring](https://arxiv.org/abs/2602.22731)
*Miguel Ángel Muñoz-Bañón,Nived Chebrolu,Sruthi M. Krishna Moorthy,Yifu Tao,Fernando Torres,Roberto Salguero-Gómez,Maurice Fallon*

Main category: cs.RO

TL;DR: 本论文提出了一种结合NeRF、LiDAR SLAM和GNSS的三层级管线，实现对幼树进行可重复、精准的地理定位和长时生态监测。与传统方法相比，该方法能更准确捕捉幼树的结构特征。


<details>
  <summary>Details</summary>
Motivation: 现有3D传感技术难以准确获取幼树细小结构，如薄枝和密叶，不仅重建效果差，还缺乏长时间监测所需的尺度一致性和地理定位能力。本文旨在解决这些不足，为森林健康评估和动态监测提供更好手段。

Method: 提出三层级表示管线：(i) 利用GNSS实现粗略定位，(ii) 借助LiDAR SLAM实现厘米级定位与重建，(iii) 基于NeRF进行目标居中的幼树高密度重建，实现不同层次信息的融合。

Result: 在英国Wytham Woods和芬兰Evo的林地实验表明，与地面激光扫描相比，该方法能更准确捕捉幼树的树干高度、分枝模式及叶木比，适用于0.5-2米幼树的精确结构测量。

Conclusion: 所提出管线能实现幼树的可重复、地理精准监测，为生态学家获取更丰富的结构与定量数据，提升了森林动态的研究精度和广度。

Abstract: Saplings are key indicators of forest regeneration and overall forest health. However, their fine-scale architectural traits are difficult to capture with existing 3D sensing methods, which make quantitative evaluation difficult. Terrestrial Laser Scanners (TLS), Mobile Laser Scanners (MLS), or traditional photogrammetry approaches poorly reconstruct thin branches, dense foliage, and lack the scale consistency needed for long-term monitoring. Implicit 3D reconstruction methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) are promising alternatives, but cannot recover the true scale of a scene and lack any means to be accurately geo-localised. In this paper, we present a pipeline which fuses NeRF, LiDAR SLAM, and GNSS to enable repeatable, geo-localised ecological monitoring of saplings. Our system proposes a three-level representation: (i) coarse Earth-frame localisation using GNSS, (ii) LiDAR-based SLAM for centimetre-accurate localisation and reconstruction, and (iii) NeRF-derived object-centric dense reconstruction of individual saplings. This approach enables repeatable quantitative evaluation and long-term monitoring of sapling traits. Our experiments in forest plots in Wytham Woods (Oxford, UK) and Evo (Finland) show that stem height, branching patterns, and leaf-to-wood ratios can be captured with increased accuracy as compared to TLS. We demonstrate that accurate stem skeletons and leaf distributions can be measured for saplings with heights between 0.5m and 2m in situ, giving ecologists access to richer structural and quantitative data for analysing forest dynamics.

</details>


### [70] [Pixel2Catch: Multi-Agent Sim-to-Real Transfer for Agile Manipulation with a Single RGB Camera](https://arxiv.org/abs/2602.22733)
*Seongyong Kim,Junhyeon Cho,Kang-Won Lee,Soo-Chul Lim*

Main category: cs.RO

TL;DR: 该论文提出了一种机器人接住抛掷物体的新方法，通过单帧RGB图像像素级视觉信息识别物体运动，无需显式估算三维位置，并采用多智能体强化学习实现策略学习，最终成功实现了从仿真到现实的迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖物体三维位置的精确估算，但实际场景中三维估计存在困难。因此，作者希望探索无需复杂3D估算，仅用2D视觉信息即能实现物体运动理解和机器人接取。

Method: 提出仅用单帧RGB图像提取像素级视觉线索，如物体位置和尺度变化，来推断物体运动。同时，提出异构多智能体强化学习框架，将机器人臂和多指手定义为各自独立、分工合作的智能体，通过各自观测和奖励共同学习掌握接物策略。

Result: 训练出的策略不仅在仿真中表现良好，还可平滑迁移到实体机器人，实现了有效的物体接取。

Conclusion: 该研究表明无需显式3D定位，仅通过像素级2D视觉线索与多智能体合作强化学习即可完成高自由度机器人复杂操作任务，为提升机器人灵巧操作能力提供了新思路。

Abstract: To catch a thrown object, a robot must be able to perceive the object's motion and generate control actions in a timely manner. Rather than explicitly estimating the object's 3D position, this work focuses on a novel approach that recognizes object motion using pixel-level visual information extracted from a single RGB image. Such visual cues capture changes in the object's position and scale, allowing the policy to reason about the object's motion. Furthermore, to achieve stable learning in a high-DoF system composed of a robot arm equipped with a multi-fingered hand, we design a heterogeneous multi-agent reinforcement learning framework that defines the arm and hand as independent agents with distinct roles. Each agent is trained cooperatively using role-specific observations and rewards, and the learned policies are successfully transferred from simulation to the real world.

</details>


### [71] [Unleashing the Potential of Diffusion Models for End-to-End Autonomous Driving](https://arxiv.org/abs/2602.22801)
*Yinan Zheng,Tianyi Tan,Bin Huang,Enguang Liu,Ruiming Liang,Jianlin Zhang,Jianwei Cui,Guang Chen,Kun Ma,Hangjun Ye,Long Chen,Ya-Qin Zhang,Xianyuan Zhan,Jingjing Liu*

Main category: cs.RO

TL;DR: 本文系统性地研究了扩散模型在端到端自动驾驶（E2E AD）中的应用，提出了新方法，并通过真实路测验证了其有效性，实现了性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在机器人决策领域日渐流行，但其在自动驾驶中的应用多局限于仿真或实验室环境，尚未深入探索其在真实复杂自动驾驶场景中的潜力。作者希望揭示并提升扩散模型在实际道路环境下端到端自动驾驶中的表现。

Method: 作者基于大量真实车辆数据与路测，对扩散模型在规划任务中的核心影响因素（如损失空间、轨迹表示和数据规模）进行了系统研究。提出了新型的强化学习后训练策略，进一步提升规划器的安全性。最终提出了Hyper Diffusion Planner（HDP）框架，并在实际车辆和6种城市驾驶场景、累计200公里真实道路上进行了评估。

Result: HDP方法在实际道路测试中相较于基准模型实现了10倍性能提升，证明扩散模型在E2E自动驾驶中的巨大潜力。

Conclusion: 只要设计和训练得当，扩散模型可以作为高效且可扩展的自动驾驶端到端规划器，适用于复杂的真实应用场景。

Abstract: Diffusion models have become a popular choice for decision-making tasks in robotics, and more recently, are also being considered for solving autonomous driving tasks. However, their applications and evaluations in autonomous driving remain limited to simulation-based or laboratory settings. The full strength of diffusion models for large-scale, complex real-world settings, such as End-to-End Autonomous Driving (E2E AD), remains underexplored. In this study, we conducted a systematic and large-scale investigation to unleash the potential of the diffusion models as planners for E2E AD, based on a tremendous amount of real-vehicle data and road testing. Through comprehensive and carefully controlled studies, we identify key insights into the diffusion loss space, trajectory representation, and data scaling that significantly impact E2E planning performance. Moreover, we also provide an effective reinforcement learning post-training strategy to further enhance the safety of the learned planner. The resulting diffusion-based learning framework, Hyper Diffusion Planner} (HDP), is deployed on a real-vehicle platform and evaluated across 6 urban driving scenarios and 200 km of real-world testing, achieving a notable 10x performance improvement over the base model. Our work demonstrates that diffusion models, when properly designed and trained, can serve as effective and scalable E2E AD planners for complex, real-world autonomous driving tasks.

</details>


### [72] [LeRobot: An Open-Source Library for End-to-End Robot Learning](https://arxiv.org/abs/2602.22818)
*Remi Cadene,Simon Aliberts,Francesco Capuano,Michel Aractingi,Adil Zouitine,Pepijn Kooijmans,Jade Choghari,Martino Russi,Caroline Pascal,Steven Palma,Mustafa Shukor,Jess Moss,Alexander Soare,Dana Aubakirova,Quentin Lhoest,Quentin Gallouédec,Thomas Wolf*

Main category: cs.RO

TL;DR: 本文介绍了一款名为lerobot的开源机器人学习库，覆盖从底层控制到大规模数据处理等全堆栈功能，旨在提升机器人学习的可扩展性和易用性。


<details>
  <summary>Details</summary>
Motivation: 目前机器人学习领域的发展受到工具闭源和分散的影响，缺乏一体化、开放性强的开发平台，限制了领域的创新和效率。

Method: lerobot集成了机器人学习全栈解决方案，包括低层马达控制通信、中高层数据收集、存储与流式处理，支持多种主流机器人学习算法和通用的异步推理框架，并保持硬件平台的灵活兼容和易扩展性。

Result: lerobot能够高效支持多种真实机器人硬件与当前最先进的学习范式，在数据和计算资源扩展时性能持续提升。

Conclusion: lerobot作为开放、可扩展的机器人学习平台，降低了机器人学习的入门门槛，为可重复的前沿研究和应用开发提供了有力的平台支持。

Abstract: Robotics is undergoing a significant transformation powered by advances in high-level control techniques based on machine learning, giving rise to the field of robot learning. Recent progress in robot learning has been accelerated by the increasing availability of affordable teleoperation systems, large-scale openly available datasets, and scalable learning-based methods. However, development in the field of robot learning is often slowed by fragmented, closed-source tools designed to only address specific sub-components within the robotics stack. In this paper, we present \texttt{lerobot}, an open-source library that integrates across the entire robot learning stack, from low-level middleware communication for motor controls to large-scale dataset collection, storage and streaming. The library is designed with a strong focus on real-world robotics, supporting accessible hardware platforms while remaining extensible to new embodiments. It also supports efficient implementations for various state-of-the-art robot learning algorithms from multiple prominent paradigms, as well as a generalized asynchronous inference stack. Unlike traditional pipelines which heavily rely on hand-crafted techniques, \texttt{lerobot} emphasizes scalable learning approaches that improve directly with more data and compute. Designed for accessibility, scalability, and openness, \texttt{lerobot} lowers the barrier to entry for researchers and practitioners to robotics while providing a platform for reproducible, state-of-the-art robot learning.

</details>


### [73] [Performance and Experimental Analysis of Strain-based Models for Continuum Robots](https://arxiv.org/abs/2602.22854)
*Annika Delucchi,Vincenzo Di Paola,Andreas Müller,and Matteo Zoppi*

Main category: cs.RO

TL;DR: 本文评估了第三阶应变插值法与几何可变应变模型在连续体机器人形状重构中的性能，并通过实验验证其准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 目前，虽然基于应变的模型已广泛应用于机器人领域，但普遍缺乏统一弯曲测试之外的对比评价方法。而随着连续体机器人原型开发逐渐增多，对模型适用性和性能综合评估的需求日益突出。

Method: 提出并测试了一种第三阶应变插值方法，研究其单独及组合变形重构能力，并与几何可变应变方法进行对比。通过机械臂单端操作细杆并用相机及反光标记记录和提取杆形状，无需应变计或力传感器，对模拟结果进行实验验证。

Result: 实验显示模型预测与实际形状高度一致，平均误差为杆长的0.58%，单次计算平均用时0.32秒，优于现有模型。

Conclusion: 第三阶应变插值模型在连续体机器人形状重构任务中兼具高精度和高效率，适合实际应用，可作为评估和开发新型连续体机器人的有效工具。

Abstract: Although strain-based models have been widely adopted in robotics, no comparison beyond the uniform bending test is commonly recognized to assess their performance. In addition, the increasing effort in prototyping continuum robots highlights the need to assess the applicability of these models and the necessity of comprehensive performance evaluation. To address this gap, this work investigates the shape reconstruction abilities of a third-order strain interpolation method, examining its ability to capture both individual and combined deformation effects. These results are compared and discussed against the Geometric-Variable Strain approach. Subsequently, simulation results are experimentally verified by reshaping a slender rod while recording the resulting configurations using cameras. The rod configuration is imposed using a manipulator displacing one of its tips and extracted through reflective markers, without the aid of any other external sensor -- i.e. strain gauges or wrench sensors placed along the rod. The experiments demonstrate good agreement between the model predictions and observed shapes, with average error of 0.58% of the rod length and average computational time of 0.32s per configuration, outperforming existing models.

</details>


### [74] [DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation](https://arxiv.org/abs/2602.22896)
*Zebin Yang,Yijiahao Qi,Tong Xie,Bo Yu,Shaoshan Liu,Meng Li*

Main category: cs.RO

TL;DR: 提出了一种动态跳层的视觉-语言-动作（VLA）模型DySL-VLA，通过按动作重要性动态跳过部分层，大幅提升推理效率并降低计算成本，同时保持或提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在机器人任务中表现出色，但由于推理计算量大，难以满足实时应用需求。作者发现任务中不同动作的重要性不同，关键步骤需准确推理，而非关键步骤可容忍跳步。

Method: 设计了DySL-VLA框架，将网络层区分为信息层（总执行）和增量层（可跳过），并提出先验-后验跳层引导机制，智能决定何时跳层。还研发了一种跳层感知的两阶段知识蒸馏方法，将标准VLA模型高效训练为DySL-VLA。

Result: 在Calvin数据集上，DySL-VLA相较主流方法（如Deer-VLA）提升2.1%成功长度，参数量仅为其1/85.7。与RoboFlamingo基线相比，在准确度持平下速度提升3.75倍。

Conclusion: DySL-VLA在显著降低计算成本和推理延迟的同时保障了高性能，为VLA模型在实际机器人系统中的应用提供了新思路。

Abstract: Vision-Language-Action (VLA) models have shown remarkable success in robotic tasks like manipulation by fusing a language model's reasoning with a vision model's 3D understanding. However, their high computational cost remains a major obstacle for real-world applications that require real-time performance. We observe that the actions within a task have varying levels of importance: critical steps demand high precision, while less important ones can tolerate more variance. Leveraging this insight, we propose DySL-VLA, a novel framework that addresses computational cost by dynamically skipping VLA layers based on each action's importance. DySL-VLA categorizes its layers into two types: informative layers, which are consistently executed, and incremental layers, which can be selectively skipped. To intelligently skip layers without sacrificing accuracy, we invent a prior-post skipping guidance mechanism to determine when to initiate layer-skipping. We also propose a skip-aware two-stage knowledge distillation algorithm to efficiently train a standard VLA into a DySL-VLA. Our experiments indicate that DySL-VLA achieves 2.1% improvement in success length over Deer-VLA on the Calvin dataset, while simultaneously reducing trainable parameters by a factor of 85.7 and providing a 3.75x speedup relative to the RoboFlamingo baseline at iso-accuracy. Our code is available on https://github.com/PKU-SEC-Lab/DYSL_VLA.

</details>
