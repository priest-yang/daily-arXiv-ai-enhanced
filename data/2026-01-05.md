<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 68]
- [cs.CL](#cs.CL) [Total: 35]
- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model](https://arxiv.org/abs/2601.00051)
*Yabo Chen,Yuanzhi Liang,Jiepeng Wang,Tingxi Chen,Junfei Cheng,Zixiao Gu,Yuyang Huang,Zicheng Jiang,Wei Li,Tian Li,Weichen Li,Zuoxin Li,Guangce Liu,Jialun Liu,Junqi Liu,Haoyuan Wang,Qizhen Weng,Xuan'er Wu,Xunzhi Xiang,Xiaoyan Yang,Xin Zhang,Shiwen Zhang,Junyu Zhou,Chengcheng Zhou,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleWorld提出了一种实时、多模态4D世界建模框架，通过创新的生成-重建-引导范式实现了高质量、长时序一致性的视频生成与动态场景重建，并支持实时交互和高效的世界记忆。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在视觉质量上取得进展，但在实时互动、长期一致性和动态场景记忆方面存在局限，这限制了其作为实际世界模型的应用。该工作旨在解决这些瓶颈，推进世界模型向可用化发展。

Method: TeleWorld采用生成-重建-引导新范式，将生成的视频流不断重建为4D时空表示，并反向引导后续生成以保障空间和物理一致性。方法核心包括自回归扩散式视频生成模型，结合创新的“宏观-微观规划”（MMPL）减少长时间生成误差积累，并通过高效的分布匹配蒸馏（DMD）确保实时合成。动态与静态建模实现深度融合，整体架构闭环运行，提升计算效率。

Result: 实验表明，TeleWorld在静态和动态世界理解、长期一致性以及实时生成效率上均表现突出，实现多模态生成与具身智能中对世界模型的高性能要求。

Conclusion: TeleWorld推动了4D世界模型的实际应用落地，具备强大的互动性、记忆能力和生成效率，为多模态生成和具身智能奠定基础。

Abstract: World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.

</details>


### [2] [It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models](https://arxiv.org/abs/2601.00090)
*Anne Harrington,A. Sophia Koepke,Shyamgopal Karthik,Trevor Darrell,Alexei A. Efros*

Main category: cs.CV

TL;DR: 本文提出通过优化噪声的方法，缓解文本生成图像模型的模式崩溃（mode collapse），提升图像多样性。实验结果显示，在保持生成质量的同时，优化噪声显著增加了生成结果的多样性。


<details>
  <summary>Details</summary>
Motivation: 当前的文本生成图像模型在相同文本提示下生成的图片缺乏多样性，容易陷入模式崩溃，限制了实际应用价值。现有方法多依赖引导机制或海量采样，本研究探索更直接有效的提升多样性的方式。

Method: 作者采用噪声优化作为切入点，设定简单的噪声优化目标，通过分析不同频率特征的噪声，提出采用不同频率分布的噪声初始化以改善生成多样性和优化过程。

Result: 实验结果表明，噪声优化不仅改善了生成结果的多样性，也保持了原有模型的生成质量。此外，选用具有不同频率特征的噪声有助于提升优化和搜索性能。

Conclusion: 通过优化噪声初始化和分布，可以有效提升文本生成图像模型的多样性，减少模式崩溃现象，为提升生成模型的实用性提供了新思路。

Abstract: Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.

</details>


### [3] [Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark](https://arxiv.org/abs/2601.00092)
*Pan Wang,Yang Liu,Guile Wu,Eduardo R. Corral-Soto,Chengjie Huang,Binbin Xu,Dongfeng Bai,Xu Yan,Yuan Ren,Xingxin Chen,Yizhe Wu,Tao Huang,Wenjun Wan,Xin Wu,Pei Zhou,Xuyang Dai,Kangbo Lv,Hongbo Zhang,Yosef Fried,Aixue Ye,Bailan Feng,Zhenyu Chen,Zhen Li,Yingcong Chen,Yiyi Liao,Bingbing Liu*

Main category: cs.CV

TL;DR: 本文提出了Spatial4D-Bench，一套用于全面评估多模态大语言模型（MLLMs）“4D空间智能”的大型基准，系统检验模型处理物体时空变化及推理能力。


<details>
  <summary>Details</summary>
Motivation: 目前关于空间智能的评测往往规模较小或任务种类单一，无法全面、细致地反映MLLMs在人类级4D空间智能上的表现。因此，作者希望通过一套更大规模、高多样性的基准，更准确评估与促进MLLMs在此领域的进步。

Method: 作者构建了包含约40,000个问答对、涵盖18种任务的多任务评测Spatial4D-Bench，将任务系统性划分为六大认知类别（如物体理解、场景理解、空间关系、时空关系等），并用该基准评测现有多模态开源和闭源大语言模型，在各项4D空间推理任务中的能力和局限。

Result: 通过基准测试，作者发现当前主流MLLMs在诸如路径规划、动作识别、物理合理性推理等多种4D空间智能细分任务上存在明显短板，整体距离人类级别空间智能仍有较大差距。

Conclusion: Spatial4D-Bench为MLLMs的4D空间智能提供了系统性、全面的评价标准，有助于揭示模型不足，推动模型朝着具有人类级4D空间智能的方向发展。

Abstract: 4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.

</details>


### [4] [A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data](https://arxiv.org/abs/2601.00123)
*Hyunho Lee,Wenwen Li*

Main category: cs.CV

TL;DR: 本文提出了一种新型多模态深度学习方法，用于在洪灾后有效、准确地进行水体范围提取，显著提升了在MSI数据缺失情况下的鲁棒性和应用性。


<details>
  <summary>Details</summary>
Motivation: 在洪灾响应阶段，快速准确的水体范围提取对灾害管理至关重要。SAR数据虽能全天候监测，但单独使用在MSI数据部分缺失时信息有限，现有方法对多源（SAR+MSI）数据部分缺失的适应性研究不足，因此需发展更健壮的多模态方法。

Method: 文中提出Spatially Masked Adaptive Gated Network（SMAGNet）模型，将SAR数据作为主要输入，采用特征融合机制根据实际MSI可用性自适应整合MSI补充信息，实现在MSI部分或完全缺失情况下的洪灾水体范围判别。

Result: 在C2S-MS Floods数据集的实验中，SMAGNet在不同MSI可用水平下的预测效果均优于其他多模态深度学习模型。当MSI数据完全缺失时，SMAGNet依然与仅用SAR数据训练的U-Net模型呈现统计上无显著差异的性能。

Conclusion: SMAGNet在应对MSI数据不完整或缺失等现实场景下展现出更强的鲁棒性，为实际洪灾管理中的多模态深度学习应用提供了更广阔的前景。

Abstract: Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios.

</details>


### [5] [Compressed Map Priors for 3D Perception](https://arxiv.org/abs/2601.00139)
*Brady Zhou,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: 本文提出Compressed Map Priors (CMP) 方法，从历史路径中学习空间先验信息，实现高效的自动驾驶环境感知，极大地提升了3D目标检测效果。


<details>
  <summary>Details</summary>
Motivation: 目前大多数自动驾驶视觉系统在每次遇到环境时都像是首次访问，没有有效利用历史车辆经过该地点的信息，这导致信息利用率和系统效率低下。作者希望通过历史路径数据，帮助系统利用先验知识，提高感知能力和检测精度。

Method: 提出Compressed Map Priors（CMP）框架，通过二值化哈希表将历史路径空间信息压缩存储，存储效率高（仅32KB/km²），与现有3D感知系统易于集成，对计算资源几乎无额外影响。

Result: 将CMP应用于主流3D感知系统，在nuScenes数据集上的多个架构下，3D目标检测性能显著且持续提升。相比密集存储方式，数据存储压力减少20倍。

Conclusion: Compressed Map Priors是一种简单高效且资源消耗极低的空间先验学习方法，能够有效提升自动驾驶系统的3D目标检测能力，具有良好的实际应用前景。

Abstract: Human drivers rarely travel where no person has gone before. After all, thousands of drivers use busy city roads every day, and only one can claim to be the first. The same holds for autonomous computer vision systems. The vast majority of the deployment area of an autonomous vision system will have been visited before. Yet, most autonomous vehicle vision systems act as if they are encountering each location for the first time. In this work, we present Compressed Map Priors (CMP), a simple but effective framework to learn spatial priors from historic traversals. The map priors use a binarized hashmap that requires only $32\text{KB}/\text{km}^2$, a $20\times$ reduction compared to the dense storage. Compressed Map Priors easily integrate into leading 3D perception systems at little to no extra computational costs, and lead to a significant and consistent improvement in 3D object detection on the nuScenes dataset across several architectures.

</details>


### [6] [Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection](https://arxiv.org/abs/2601.00141)
*Lawrence Han*

Main category: cs.CV

TL;DR: 提出了一种新的AI生成图片检测架构GLASS，兼顾全球与局部特征，有效提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，AI生成的图片变得越来越真实且高分辨率，现有方法下采样图片易丢失细节，导致检测准确度下降。

Method: GLASS架构结合整体缩放视图与原始分辨率的多组局部裁剪（通过空间分层采样实现），并利用注意力机制聚合这些信息，可集成到多种视觉模型（如ViT、ResNet、ConvNeXt）中。

Result: 实验显示，GLASS在给定的计算资源约束下，预测性能优于传统迁移学习策略。

Conclusion: GLASS有效整合全局和局部特征，提升了AI生成图片检测的表现，且在多种视觉模型中适用。

Abstract: The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.

</details>


### [7] [FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications](https://arxiv.org/abs/2601.00150)
*Yehui Yang,Dalu Yang,Wenshuo Zhou,Fangxin Shang,Yifan Liu,Jie Ren,Haojun Fei,Qing Yang,Tao Chen*

Main category: cs.CV

TL;DR: 该论文提出并发布了FCMBench-V1.0，这是一个专门为金融信贷场景设计的大规模多模态基准数据集，涵盖证件识别、推理及鲁棒性等多维度衡量，用于评估和对比主流视觉-语言模型在真实信贷应用中的绩效。


<details>
  <summary>Details</summary>
Motivation: 目前多模态AI广泛应用于信贷风险评估、文档审核等场景，但缺乏针对金融信贷特定需求的评测基准；公开数据存在隐私泄露和现实场景代表性不足等问题，急需一个既合规又具代表性的基准来推动技术进步和实际落地。

Method: 构建了FCMBench-V1.0基准，涵盖18类核心证件，包含4043张合规图片、8446个QA样本，评测框架覆盖感知、推理和鲁棒性三大维度，并通过自有流程合成与采集所有样本，避免数据泄漏且高度还原真实业务场景。利用该基准，对23个主流视觉-语言模型进行了广泛评测对比。

Result: 评测结果显示，Gemini 3 Pro在商用模型中F1分数最高（64.61），Qwen3-VL-235B在开源模型中表现最佳（57.27），而专为金融信贷设计的Qfin-VL-Instruct综合表现最优（64.92）；同时，所有模型在鲁棒性测试场景下均有不同程度的性能下降。

Conclusion: FCMBench-V1.0能够有效揭示各主流多模态模型在金融信贷场景下的效能差异及鲁棒性短板，为后续金融领域AI模型开发和评测提供了标准化工具和数据支撑。

Abstract: As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.

</details>


### [8] [Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions](https://arxiv.org/abs/2601.00156)
*Kaiwen Zheng,Junchen Fu,Songpei Xu,Yaoqing He,Joemon M. Jose,Han Hu,Xuri Ge*

Main category: cs.CV

TL;DR: 本文提出了一个新问题：针对人脸任意区域生成和识别多属性自然语言描述（包括表情动作单元、情感状态和年龄估计），并构建了相应数据集及模型，实验效果突出。


<details>
  <summary>Details</summary>
Motivation: 现有人脸分析方法大多聚焦于整体或全局属性描述，缺乏对单独人脸区域的细粒度、多属性、可解释性强的自然语言描述能力，影响了理解的深度与灵活性。

Method: 作者新建了一个包含任意人脸区域多属性自然语言描述的数据集，并提出了基于Qwen2.5-VL微调的视觉-语言模型Focal-RegionFace，通过多阶段微调逐步聚焦于局部人脸特征，实现了可解释的年龄估计、表情动作单元和情绪检测。

Result: Focal-RegionFace模型在新构建的基准数据集上，于传统主流评价指标和新提出的评价标准下均取得了最优性能，效果显著优于现有方法。

Conclusion: 所提模型验证了在人脸区域细粒度、多属性分析任务中的有效性和通用性，有助于提升人脸分析的解释性和灵活控制能力。

Abstract: In this paper, we introduce an underexplored problem in facial analysis: generating and recognizing multi-attribute natural language descriptions, containing facial action units (AUs), emotional states, and age estimation, for arbitrarily selected face regions (termed FaceFocalDesc). We argue that the system's ability to focus on individual facial areas leads to better understanding and control. To achieve this capability, we construct a new multi-attribute description dataset for arbitrarily selected face regions, providing rich region-level annotations and natural language descriptions. Further, we propose a fine-tuned vision-language model based on Qwen2.5-VL, called Focal-RegionFace for facial state analysis, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages, resulting in interpretable age estimation, FAU and emotion detection. Experimental results show that Focal-RegionFace achieves the best performance on the new benchmark in terms of traditional and widely used metrics, as well as new proposed metrics. This fully verifies its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.

</details>


### [9] [DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery](https://arxiv.org/abs/2601.00194)
*Salma Gonzalez-Sabbagh,Antonio Robles-Kelly,Shang Gao*

Main category: cs.CV

TL;DR: 本文提出了一种基于条件对抗生成网络（cGAN）的DichroGAN方法，用于从卫星影像恢复海底的真实颜色，有效克服水柱中光吸收与散射带来的问题。


<details>
  <summary>Details</summary>
Motivation: 水下光线随着深度呈指数衰减，造成卫星图像难以还原海底的真实颜色信息，因此需要新的方法实现更准确的水下颜色恢复。

Method: 作者提出DichroGAN网络，通过双步同时训练，利用两个生成器进行漫反射和镜面反射估计，获取大气场景辐射；再通过另两个生成器估计光传输和恢复各光谱带特征，以消除吸收与散射影响。训练数据来自PRISMA卫星的RGB及光谱配对数据。

Result: 在卫星与水下数据集上的大量实验表明，DichroGAN在海底颜色恢复领域与最新技术相比表现出很强竞争力。

Conclusion: DichroGAN能有效地恢复水下影像的真实色彩信息，为海底遥感分析等应用带来有力的工具，并优于传统方法。

Abstract: Recovering the in-air colours of seafloor from satellite imagery is a challenging task due to the exponential attenuation of light with depth in the water column. In this study, we present DichroGAN, a conditional generative adversarial network (cGAN) designed for this purpose. DichroGAN employs a two-steps simultaneous training: first, two generators utilise a hyperspectral image cube to estimate diffuse and specular reflections, thereby obtaining atmospheric scene radiance. Next, a third generator receives as input the generated scene radiance containing the features of each spectral band, while a fourth generator estimates the underwater light transmission. These generators work together to remove the effects of light absorption and scattering, restoring the in-air colours of seafloor based on the underwater image formation equation. DichroGAN is trained on a compact dataset derived from PRISMA satellite imagery, comprising RGB images paired with their corresponding spectral bands and masks. Extensive experiments on both satellite and underwater datasets demonstrate that DichroGAN achieves competitive performance compared to state-of-the-art underwater restoration techniques.

</details>


### [10] [MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing](https://arxiv.org/abs/2601.00204)
*Xiaokun Sun,Zeyu Cai,Hao Tang,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: 提出了MorphAny3D，一种无需训练、基于结构化潜变量（SLAT）的高质量3D变形方法，实现语义一致且时序平滑的3D变形。


<details>
  <summary>Details</summary>
Motivation: 3D变形过程中，尤其是跨类别时，难以保证变形的语义一致性和时序平滑性，这是3D建模和动画领域长期存在的难题。

Method: 提出了一套基于SLAT潜变量混合的3D变形框架，设计了Morphing Cross-Attention（MCA）模块以融合源与目标的结构信息保证结构连贯性，以及Temporal-Fused Self-Attention（TFSA）模块通过引用前帧信息提升时序一致性，并引入姿态矫正策略减少变形过程中的姿势歧义。

Result: 实验显示，该方法可产生当前最先进的3D变形序列，即便面对难度较高的跨类别变形场景也能表现良好。

Conclusion: MorphAny3D在无需训练的前提下，能够实现高质量且通用的3D变形，并可支持解耦变形、3D风格迁移等高级应用，具有良好的泛化能力。

Abstract: 3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.

</details>


### [11] [CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting](https://arxiv.org/abs/2601.00207)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D实例分割的新颖作物计数框架，能够在多视角影像的基础上实现高精度作物计数，并消除了对作物特定参数的依赖。


<details>
  <summary>Details</summary>
Motivation: 作物计数对农业管理和干预策略至关重要。但在真实的田间环境下，作物的遮挡和聚集会导致基于图像的分割方法难以准确区分和计数单体作物，现有方法普遍受到视角限制和不可避免的参数调优影响，尚未有通用性强且精度高的解决方案。

Method: 方法基于NeRF（神经辐射场），通过多视角2D图像生成一致的3D实例掩模，实现作物的3D分割计数。引入了作物能见度和掩模一致性评分，结合NeRF提取的3D信息。此外，该框架不依赖于特定作物参数，无需针对不同作物类型分别调整。

Result: 在棉花、苹果和梨三个公开农业数据集上进行了实验，方法在不同颜色、形状和大小的作物计数中表现稳定，对比现有最佳方法，取得了更优的计数准确率。

Conclusion: 该方法能够有效实现跨作物类型的高精度计数，为农业自动化管理提供了技术支撑，并贡献了一套新的棉花作物数据集推动后续研究。

Abstract: Rigorous crop counting is crucial for effective agricultural management and informed intervention strategies. However, in outdoor field environments, partial occlusions combined with inherent ambiguity in distinguishing clustered crops from individual viewpoints poses an immense challenge for image-based segmentation methods. To address these problems, we introduce a novel crop counting framework designed for exact enumeration via 3D instance segmentation. Our approach utilizes 2D images captured from multiple viewpoints and associates independent instance masks for neural radiance field (NeRF) view synthesis. We introduce crop visibility and mask consistency scores, which are incorporated alongside 3D information from a NeRF model. This results in an effective segmentation of crop instances in 3D and highly-accurate crop counts. Furthermore, our method eliminates the dependence on crop-specific parameter tuning. We validate our framework on three agricultural datasets consisting of cotton bolls, apples, and pears, and demonstrate consistent counting performance despite major variations in crop color, shape, and size. A comparative analysis against the state of the art highlights superior performance on crop counting tasks. Lastly, we contribute a cotton plant dataset to advance further research on this topic.

</details>


### [12] [IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation](https://arxiv.org/abs/2601.00212)
*Han Liu,Yubo Fan,Hao Li,Dewei Hu,Daniel Moyer,Zhoubing Xu,Benoit M. Dawant,Ipek Oguz*

Main category: cs.CV

TL;DR: 本文提出了一种无需先验知识即可实现多样化风格合成的无监督域适应方法IntraStyler，通过示例图像指导风格合成，并在CrossMoDA 2023数据集上验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域适应多聚焦于不同域之间的差异（domain shift），但对同一目标域内部的风格多样性（intra-domain variability）关注较少，现有风格多样化方法通常需要提前指定风格变化，实际应用受限。

Method: 提出IntraStyler方法，通过示例图像引导目标域图像的风格合成，无需预先指定风格类型。核心是训练一个风格编码器，利用对比学习只提取风格信息，实现多样化且可控的风格生成。

Result: 在最大的跨模态域适应公开数据集CrossMoDA 2023上进行评测，实验结果表明IntraStyler可实现可控的风格合成，并通过增加合成风格多样性提升了下游分割任务的性能。

Conclusion: IntraStyler无需先验风格信息即可完成多样的目标域风格合成，提升了下游任务表现，为无监督域适应提供了更实用的解决方案。

Abstract: Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.

</details>


### [13] [From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning](https://arxiv.org/abs/2601.00215)
*Omar Sharif,Eftekhar Hossain,Patrick Ng*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLMs）在视觉推理任务中表现不佳，因其推理过程缺乏视觉信息的集成。本文通过将图像转化为文字描述大幅提升模型表现，并提出奖励驱动的强化学习方法，使开源MLLMs生成更长、更结构化的视觉推理，从而提升视觉感知类任务中的效果。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs虽然能够进行推理，但在视觉感知、特别是需要精准理解图像内容的任务（如视觉谜题）中表现有限。主要瓶颈在于推理过程中视觉信息整合不足。提升视觉推理能力和模型在视觉任务中的表现是本文研究的动机。

Method: 作者设计了六种针对不同推理细节（例如图像理解、思考步骤、答案准确性）的奖励函数，并采用群体相对策略优化（GRPO）方法，指导开源MLLMs进行更长、更有条理的视觉推理，避免模型在忽略视觉信息情况下作答。

Result: 实验证明，将图像转化为文本描述可使Claude 3.5和Claude 3.7模型的相关任务表现分别提升26.7%和23.6%；在Qwen-2.5-VL-7B模型上的方案能比基线提升5.56%，在域内和跨域任务上均有稳定提升。

Conclusion: 视觉感知是影响视觉推理任务表现的关键环节，通过奖励驱动强化学习方法可有效加强MLLMs的视觉推理能力，并显著提升其在相关任务中的准确率和泛化能力。

Abstract: Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.
  To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.

</details>


### [14] [LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization](https://arxiv.org/abs/2601.00222)
*Jie Li,Kwan-Yee K. Wong,Kai Han*

Main category: cs.CV

TL;DR: 本文提出了一种新的向量量化方法LooC，利用低维度组合式码本，在提升性能的同时实现更紧凑的表示，并在多项任务中取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 随着数据和模型复杂度的增加，传统向量量化方法在容量与紧凑性之间难以兼顾，急需一种能同时兼具高容量与紧凑表示的新方法。

Method: LooC方法通过将码向量与特征向量的关系重新建模，将码向量视为低维组合单元并组合，提高了解决方案空间。同时，提出参数无关的插值外推机制，提升VQ过程中特征的平滑性和保真度，避免码本塌陷。LooC可作为即插即用模块应用于不同的VQ下游任务中。

Result: 在不同任务、数据集和结构上的大量实验显示，LooC以显著更小的码本达到了优于现有VQ方法的性能。

Conclusion: LooC实现了码本小型化和高效利用，不仅提高了向量量化的表现，也对后续任务具备良好适配性，具有广泛的应用前景。

Abstract: Vector quantization (VQ) is a prevalent and fundamental technique that discretizes continuous feature vectors by approximating them using a codebook. As the diversity and complexity of data and models continue to increase, there is an urgent need for high-capacity, yet more compact VQ methods. This paper aims to reconcile this conflict by presenting a new approach called LooC, which utilizes an effective Low-dimensional codebook for Compositional vector quantization. Firstly, LooC introduces a parameter-efficient codebook by reframing the relationship between codevectors and feature vectors, significantly expanding its solution space. Instead of individually matching codevectors with feature vectors, LooC treats them as lower-dimensional compositional units within feature vectors and combines them, resulting in a more compact codebook with improved performance. Secondly, LooC incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance and smooth features during the VQ process, which allows for better preservation of details and fidelity in feature approximation. The design of LooC leads to full codebook usage, effectively utilizing the compact codebook while avoiding the problem of collapse. Thirdly, LooC can serve as a plug-and-play module for existing methods for different downstream tasks based on VQ. Finally, extensive evaluations on different tasks, datasets, and architectures demonstrate that LooC outperforms existing VQ methods, achieving state-of-the-art performance with a significantly smaller codebook.

</details>


### [15] [Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions](https://arxiv.org/abs/2601.00225)
*Aobo Li,Jinjian Wu,Yongxu Liu,Leida Li,Weisheng Dong*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法SynDR-IQA，通过调整合成数据分布提升无参考图像质量评价（BIQA）的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习推动了BIQA的发展，但缺乏大规模标注数据集始终是瓶颈。单靠现有合成数据训练的模型在泛化能力上表现有限，亟需更有效的数据分布策略。

Method: 作者发现合成数据特征存在离散聚类的问题，影响了回归性能。基于对样本多样性和冗余对泛化误差影响的理论推导，提出两大策略：一是分布感知的多样内容上采样，提升多样性同时保持内容分布；二是密度感知的冗余簇下采样，减少高密度区域样本，实现样本分布均衡。

Result: 在三种跨数据集（合成到真实、合成到算法、合成到合成）实验中，所提方法大幅提升了模型泛化性能。

Conclusion: 通过对合成数据样本分布的科学重塑，SynDR-IQA有效提升了BIQA模型跨数据集的泛化能力，为解决数据集稀缺和合成数据泛化性差的问题提供了新思路。

Abstract: Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy's impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. The code is available at https://github.com/Li-aobo/SynDR-IQA.

</details>


### [16] [Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection](https://arxiv.org/abs/2601.00237)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

TL;DR: 本文提出了一种结合CycleGAN和YOLOv8的跨模态数据增强框架，用于缓解PCB红外数据稀缺问题，在极少真实IR数据条件下提升缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: 目前PCB缺陷检测领域面临IR(红外)数据稀缺的瓶颈，严重限制了高性能检测模型的训练。由于IR获取成本高、难度大，亟需寻求有效方式扩充训练数据。

Method: 应用CycleGAN对丰富的可见光PCB图像和稀缺的IR图像进行无配对转换，生成高质量的伪IR图像，保留缺陷结构语义和热分布特征；随后，将生成的伪IR数据和有限真实IR数据结合，用于轻量级YOLOv8目标检测器的异构训练。

Result: 生成的伪IR样本极大提升了特征学习能力，改进后的YOLOv8在真实IR数据有限情况下，检测精度明显优于仅用真实数据训练的模型，接近全监督训练的效果。

Conclusion: 本文方法有效克服了红外数据稀缺的难题，证明了基于生成伪IR数据的数据增强策略在工业检测中具有实际可行性和显著增益。

Abstract: This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.

</details>


### [17] [Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers](https://arxiv.org/abs/2601.00359)
*Söhnke Benedikt Fischedick,Daniel Seichter,Benedict Stephan,Robin Schmidt,Horst-Michael Gross*

Main category: cs.CV

TL;DR: 提出了一种基于RGB-D Transformer的新方法DVEFormer，通过知识蒸馏将图像嵌入和文本对齐，实现像素级语义理解，支持灵活的自然语言查询和3D地图构建，具备实时性。


<details>
  <summary>Details</summary>
Motivation: 现有室内机器人语义分割方法限定于固定类别，难以灵活理解和适应人类自然语言指令，限制了人机交互和高层语义推理能力。

Method: 提出DVEFormer模型，利用Alpha-CLIP生成的教师嵌入，通过知识蒸馏训练高效学生网络获取细粒度像素级文本对齐嵌入，支持线性探测和文本查询。

Result: 在常用室内数据集上达到有竞争力的表现；全模型和小模型在NVIDIA Jetson AGX Orin上分别实现26.3 FPS和77.0 FPS，满足实时性需求，并给出定性结果证明其有效性和实际应用潜力。

Conclusion: 该方法可作为传统分割方法的直接替代，不仅支持灵活语义查询，还可无缝集成到移动机器人3D建图流程中，提升机器人感知和交互能力。

Abstract: In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.

</details>


### [18] [Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture](https://arxiv.org/abs/2601.00243)
*Anirudha Ghosh,Ritam Sarkar,Debaditya Barman*

Main category: cs.CV

TL;DR: 本研究提出了一种轻量级的虫害检测与农药推荐框架，适用于资源有限的设备，能够高效、环保地辅助小农户进行虫害管理。


<details>
  <summary>Details</summary>
Motivation: 传统虫害管理方法依赖人工巡检和化学农药，存在高成本、低效率和环境污染问题。尤其是甘蔗、小麦等作物易受虫害影响，亟需高效、便捷、绿色的管理手段，特别服务于小型和边缘化农户。

Method: 提出了两大模块：1）基于轻量级卷积神经网络（CNN）与原型元学习法的虫害检测模块，可在少量样本下实现高准确率识别；2）结合作物类型及生长阶段等环境因素的农药推荐模块，推荐安全、环保的农药。此外，作者整合多个公开数据集，构建了覆盖不同角度、虫体大小和背景的综合虫害图像数据集，并用于模型训练与评估。

Result: 实验表明，所提出的轻量级CNN模型在计算复杂度显著降低的同时，保持了与主流模型相当的高准确率。该系统能有效减少对传统化学农药的依赖，推动可持续农作实践。

Conclusion: 该框架证明了在精准农业中，实时且绿色的虫害管理是可行的。系统适合实际应用——特别是服务小农户，支持生态农业发展并有望推广应用。

Abstract: Effective pest management is crucial for enhancing agricultural productivity, especially for crops such as sugarcane and wheat that are highly vulnerable to pest infestations. Traditional pest management methods depend heavily on manual field inspections and the use of chemical pesticides. These approaches are often costly, time-consuming, labor-intensive, and can have a negative impact on the environment. To overcome these challenges, this study presents a lightweight framework for pest detection and pesticide recommendation, designed for low-resource devices such as smartphones and drones, making it suitable for use by small and marginal farmers.
  The proposed framework includes two main components. The first is a Pest Detection Module that uses a compact, lightweight convolutional neural network (CNN) combined with prototypical meta-learning to accurately identify pests even when only a few training samples are available. The second is a Pesticide Recommendation Module that incorporates environmental factors like crop type and growth stage to suggest safe and eco-friendly pesticide recommendations. To train and evaluate our framework, a comprehensive pest image dataset was developed by combining multiple publicly available datasets. The final dataset contains samples with different viewing angles, pest sizes, and background conditions to ensure strong generalization.
  Experimental results show that the proposed lightweight CNN achieves high accuracy, comparable to state-of-the-art models, while significantly reducing computational complexity. The Decision Support System additionally improves pest management by reducing dependence on traditional chemical pesticides and encouraging sustainable practices, demonstrating its potential for real-time applications in precision agriculture.

</details>


### [19] [RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization](https://arxiv.org/abs/2601.00705)
*Wei-Tse Cheng,Yen-Jen Chiou,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: RGS-SLAM是一种新的高鲁棒性高斯splating的SLAM系统，通过一次性三角化稠密多视角关联点进行高斯初始化，无需训练，有效提升了SLAM的性能，且与现有GS-SLAM兼容。


<details>
  <summary>Details</summary>
Motivation: 现有GS-SLAM依赖残差驱动的高斯密度提升过程，容易导致初期地图构建不稳定、收敛慢，且在复杂场景表现有限；因此论文提出训练无关、结构感知的高斯初始化方式，提升鲁棒性和精度。

Method: 采用DINOv3特征及置信度分类器获得稠密视角对应点，通过一键式三角测量直接生成结构一致并分布均匀的高斯种子点，无需逐步添加残差点，从而优化初始化阶段。

Result: 在TUM RGB-D和Replica数据集上评测，RGS-SLAM在定位和重建精度上达到或超过最先进的高斯、点云SLAM方法，并且支持最高925 FPS的实时建图性能。

Conclusion: RGS-SLAM显著提升了GS-SLAM的初始化稳定性与收敛速度，在纹理丰富、复杂场景下表现出更高的渲染保真度，并具备优异的兼容性和实时能力。

Abstract: We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.

</details>


### [20] [TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models](https://arxiv.org/abs/2601.00260)
*Kohei Yamamoto,Tomohiro Kikuchi*

Main category: cs.CV

TL;DR: 提出了一种名为TotalFM的3D-CT医学影像基础模型，采用器官分离学习框架，提高了模型在实际临床任务中的表现，同时兼顾了计算效率和表征能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像大模型在应对3D-CT体积数据训练时，存在较高的计算成本，限制了其广泛临床应用。因此，需要一种能够提升训练效率同时保持优良性能的方法。

Method: TotalFM通过自动化分割提取器官体积，并利用LLM处理放射报告，将影像-文本配对。模型结合VideoMAE自监督预训练和以器官为单位的对比学习，构建了大规模（14万系列）的训练集，从而提高效率与表达能力。

Result: 在zero-shot器官类别病变分类任务中，TotalFM在83%（5/6）器官上F1高于CT-CLIP，在64%（9/14）器官上高于Merlin。在zero-shot病变类别分类任务中，TotalFM在83%（25/30）类别AUROC超过Merlin。在报告生成任务上，与现有视觉-语言模型表现相当。

Conclusion: 基于器官分离的学习框架显著提升了3D-CT基础模型的泛化能力和实际应用价值，为实际落地提供了有效设计指南。

Abstract: While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a large-scale dataset of 140,000 series. By automating the creation of organ volume and finding-sentence pairs through segmentation techniques and Large Language Model (LLM)-based radiology report processing, and by combining self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs, we aimed to balance computational efficiency and representation capability. In zero-shot organ-wise lesion classification tasks, the proposed model achieved higher F1 scores in 83% (5/6) of organs compared to CT-CLIP and 64% (9/14) of organs compared to Merlin. These results suggest that the proposed model exhibits high generalization performance in a clinical evaluation setting using actual radiology report sentences. Furthermore, in zero-shot finding-wise lesion classification tasks, our model achieved a higher AUROC in 83% (25/30) of finding categories compared to Merlin. We also confirmed performance comparable to existing Vision-Language Models (VLMs) in radiology report generation tasks. Our results demonstrate that the organ-separated learning framework can serve as a realistic and effective design guideline for the practical implementation of 3D-CT foundation models.

</details>


### [21] [S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding](https://arxiv.org/abs/2601.00264)
*He Wang,Longteng Guo,Pengkang Huo,Xuanxu Lin,Yichen Yuan,Jie Jiang,Jing Liu*

Main category: cs.CV

TL;DR: 本文提出了S1-MMAlign，这是一个包含逾1550万高质量图文对的跨学科科学多模态大数据集，用以弥合复杂科学图像与稀疏文本描述之间的语义鸿沟。


<details>
  <summary>Details</summary>
Motivation: 多模态学习已在通用领域取得突破，但在科学发现领域受限于科学图像与文字间的深层语义差距。为促进AI在科学推理与跨模态理解的发展，亟需高质量、广覆盖的科学图文数据集。

Method: 作者从250万篇开放获取科学论文中构建了S1-MMAlign数据集，涵盖物理、生物、工程等学科，以及实验装置、热图、显微图像等多模态类型。针对科学caption配对弱的问题，提出自动语义增强流程，用Qwen-VL多模态大模型结合摘要和引用上下文为图像重述语意更丰富的描述。

Result: 技术验证显示，增强流程能显著提升数据质量：基于SciBERT的伪困惑度指标显示语义歧义减少，CLIP得分展现图文对齐提升18.21%。

Conclusion: S1-MMAlign为AI科学推理与跨模态理解提供了基础资源，推动AI+科学融合，数据集已开放共享。

Abstract: Multimodal learning has revolutionized general domain tasks, yet its application in scientific discovery is hindered by the profound semantic gap between complex scientific imagery and sparse textual descriptions. We present S1-MMAlign, a large-scale, multi-disciplinary multimodal dataset comprising over 15.5 million high-quality image-text pairs derived from 2.5 million open-access scientific papers. Spanning disciplines from physics and biology to engineering, the dataset captures diverse visual modalities including experimental setups, heatmaps, and microscopic imagery. To address the pervasive issue of weak alignment in raw scientific captions, we introduce an AI-ready semantic enhancement pipeline that utilizes the Qwen-VL multimodal large model series to recaption images by synthesizing context from paper abstracts and citation contexts. Technical validation demonstrates that this enhancement significantly improves data quality: SciBERT-based pseudo-perplexity metrics show reduced semantic ambiguity, while CLIP scores indicate an 18.21% improvement in image-text alignment. S1-MMAlign provides a foundational resource for advancing scientific reasoning and cross-modal understanding in the era of AI for Science. The dataset is publicly available at https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign.

</details>


### [22] [ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching](https://arxiv.org/abs/2601.00267)
*Yi Sun,Xinhao Zhong,Hongyan Li,Yimin Zhou,Junhao Li,Bin Chen,Xuan Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种无须重新训练的高效概念消除方法（ActErase），可有效消除扩散模型中的敏感概念，并保持生成能力。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型虽具强大生成能力，但带来了安全、版权和伦理问题。现有消除敏感概念的方法依赖于大规模数据和昂贵的微调，存在效率和现实性问题，需要更便捷的消除方法。

Method: 受模型激活大多为通用概念，敏感概念仅占很小部分的启发，作者提出ActErase。该方法通过分析成对提示，定位模型激活的不同区域，提取目标激活，并在前向传播时动态替换输入激活，无需训练即可实现敏感概念的高效消除。

Result: 在裸体、艺术风格和对象移除三个关键消除任务中，ActErase表现出最优的消除性能，同时很好地保留了模型整体生成能力。该方法还对对抗攻击表现出较强的鲁棒性。

Conclusion: ActErase方法为扩散模型中的敏感概念消除提供了高效、轻量、即插即用的新范式，有助于在保障生成质量的同时解决伦理和安全风险。

Abstract: Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model's activations are predominantly composed of generic concepts, with only a minimal component can represent the target concept, we propose a novel training-free method (ActErase) for efficient concept erasure. Specifically, the proposed method operates by identifying activation difference regions via prompt-pair analysis, extracting target activations and dynamically replacing input activations during forward passes. Comprehensive evaluations across three critical erasure tasks (nudity, artistic style, and object removal) demonstrates that our training-free method achieves state-of-the-art (SOTA) erasure performance, while effectively preserving the model's overall generative capability. Our approach also exhibits strong robustness against adversarial attacks, establishing a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models.

</details>


### [23] [FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering](https://arxiv.org/abs/2601.00269)
*Chaodong Tong,Qi Zhang,Chen Li,Lei Jiang,Yanbing Liu*

Main category: cs.CV

TL;DR: 该论文针对视觉问答（VQA）中模型产生不真实答案（hallucination）的问题，提出了一种高效且无需外部资源的新检测方法FaithSCAN，通过分析模型内部多种信号来甄别虚假回答，并显著提升检测效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有VQA幻觉检测方法要么依赖外部模型/知识库，计算开销大、质量受限；要么通过不确定性估计，仅能捕捉部分失败模式，信息有限。这些方法在效率、鲁棒性和检测性能上都有天然局限。

Method: 作者提出FaithSCAN，挖掘VLM内部丰富信号（如逐词解码不确定性、中间视觉表征、跨模态对齐特征），并通过分支证据编码和不确定性感知注意力机制融合这些信号，同时借助自动化生成训练信号（无需人工标注）以实现高效监督。

Result: 在多个VQA基准上，FaithSCAN在检测效率和有效性方面均显著优于现有方法。分析显示，不同的内部信号可互补诊断异常，VLM架构间幻觉出现模式不同。

Conclusion: 利用模型内部信号可有效检测VQA幻觉，且能低成本自动生成监督信号；研究还揭示了多模态幻觉的系统性内部成因，为VLM稳健性提升提供了方向。

Abstract: Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.

</details>


### [24] [Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification](https://arxiv.org/abs/2601.00278)
*Chi Ding,Junxiao Xue,Xinyi Yin,Shi Chen,Yunyun Shi,Yiduo Wang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: 本文提出了一种面向遥感长尾分布的模型无关不确定性感知框架DUAL，能够有效区分困难尾部样本和含噪声的样本，从而提高模型表现。


<details>
  <summary>Details</summary>
Motivation: 遥感数据中长尾分布普遍存在，大量低频类别导致模型在识别“尾部”难学样本时易受到噪声干扰，难以取得理想性能。现有方法通常无法区分真正难学的数据与含噪声的数据，容易导致对噪声数据的过拟合。

Method: 该方法基于Evidential Deep Learning，将预测不确定性动态分解为认知不确定性（Epistemic Uncertainty, EU）与噪声不确定性（Aleatoric Uncertainty, AU）。利用EU衡量样本稀缺性，并引导尾部样本重加权学习；同时利用AU度量样本歧义，通过自适应标签平滑机制抑制噪声影响。整个框架对模型架构无依赖，具备通用性。

Result: 在多个数据集和不同主干网络上，DUAL框架取得优于TGN及SADE等强基线方法的表现，表现出优秀的效果和泛化能力。消融实验分析了各关键设计对最终性能的贡献。

Conclusion: DUAL能有效区分困难样本与噪声样本，提升长尾遥感任务性能，对实际遥感应用具有良好的推广价值。

Abstract: Long-Tailed distributions are pervasive in remote sensing due to the inherently imbalanced occurrence of grounded objects. However, a critical challenge remains largely overlooked, i.e., disentangling hard tail data samples from noisy ambiguous ones. Conventional methods often indiscriminately emphasize all low-confidence samples, leading to overfitting on noisy data. To bridge this gap, building upon Evidential Deep Learning, we propose a model-agnostic uncertainty-aware framework termed DUAL, which dynamically disentangles prediction uncertainty into Epistemic Uncertainty (EU) and Aleatoric Uncertainty (AU). Specifically, we introduce EU as an indicator of sample scarcity to guide a reweighting strategy for hard-to-learn tail samples, while leveraging AU to quantify data ambiguity, employing an adaptive label smoothing mechanism to suppress the impact of noise. Extensive experiments on multiple datasets across various backbones demonstrate the effectiveness and generalization of our framework, surpassing strong baselines such as TGN and SADE. Ablation studies provide further insights into the crucial choices of our design.

</details>


### [25] [SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting](https://arxiv.org/abs/2601.00285)
*Jun-Jee Chao,Volkan Isler*

Main category: cs.CV

TL;DR: 本文提出了一种名为SV-GS的系统，用于在观测稀疏的情况下重建大范围内动态目标的三维形变和运动。


<details>
  <summary>Details</summary>
Motivation: 现有动态物体重建方法往往要求时空上密集的观测，这在实际环境下难以实现。如何在观测稀疏、视角多样的情况下准确重建动态目标成为难题。

Method: SV-GS利用粗略的骨架结构和初始静态模型作为输入，通过优化骨架驱动的形变场，包括基于骨架关节点的粗略姿态估计模块及细粒度形变模块，只使关节姿态估计随时间变化，从而实现运动插值和细节保持。后续还将初始模型用扩散生成模型替代，提升实用性。

Result: 在合成数据集上，SV-GS在稀疏观测下相比现有方法提升PSNR达34%；在真实数据集上，以远少于现有单目视频方法的帧数达到可比表现。

Conclusion: SV-GS有效解决了稀疏观测下的动态目标重建问题，不仅精度高、细节丰富，而且降低了对初始输入的依赖，更加实用。

Abstract: Reconstructing a dynamic target moving over a large area is challenging. Standard approaches for dynamic object reconstruction require dense coverage in both the viewing space and the temporal dimension, typically relying on multi-view videos captured at each time step. However, such setups are only possible in constrained environments. In real-world scenarios, observations are often sparse over time and captured sparsely from diverse viewpoints (e.g., from security cameras), making dynamic reconstruction highly ill-posed. We present SV-GS, a framework that simultaneously estimates a deformation model and the object's motion over time under sparse observations. To initialize SV-GS, we leverage a rough skeleton graph and an initial static reconstruction as inputs to guide motion estimation. (Later, we show that this input requirement can be relaxed.) Our method optimizes a skeleton-driven deformation field composed of a coarse skeleton joint pose estimator and a module for fine-grained deformations. By making only the joint pose estimator time-dependent, our model enables smooth motion interpolation while preserving learned geometric details. Experiments on synthetic datasets show that our method outperforms existing approaches under sparse observations by up to 34% in PSNR, and achieves comparable performance to dense monocular video methods on real-world datasets despite using significantly fewer frames. Moreover, we demonstrate that the input initial static reconstruction can be replaced by a diffusion-based generative prior, making our method more practical for real-world scenarios.

</details>


### [26] [Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies](https://arxiv.org/abs/2601.00286)
*Ali Anaissi,Ali Braytee,Weidong Huang,Junaid Akram,Alaa Farhat,Jie Hua*

Main category: cs.CV

TL;DR: 本研究开发了一种基于深度学习的皮肤病分类和诊断模型，并在ISIC2019数据集上取得了87.71%的准确率，展示了其作为临床和自助诊断工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着皮肤病发病率上升与皮肤科医生资源的不足，迫切需要智能工具来辅助皮肤病的及时、准确诊断。

Method: 采用Swin Transformer模型，先在公开皮肤病图像数据集上进行预训练，结合模型架构优化、数据预处理和数据增强策略，以提升模型性能。

Result: 最终模型在ISIC2019数据集的八类皮肤病分类任务中准确率达到87.71%。

Conclusion: 该模型作为皮肤病辅助诊断工具具有较大潜力，既能为临床医生提供支持，也能帮助患者自我评估。

Abstract: As dermatological conditions become increasingly common and the availability of dermatologists remains limited, there is a growing need for intelligent tools to support both patients and clinicians in the timely and accurate diagnosis of skin diseases. In this project, we developed a deep learning based model for the classification and diagnosis of skin conditions. By leveraging pretraining on publicly available skin disease image datasets, our model effectively extracted visual features and accurately classified various dermatological cases. Throughout the project, we refined the model architecture, optimized data preprocessing workflows, and applied targeted data augmentation techniques to improve overall performance. The final model, based on the Swin Transformer, achieved a prediction accuracy of 87.71 percent across eight skin lesion classes on the ISIC2019 dataset. These results demonstrate the model's potential as a diagnostic support tool for clinicians and a self assessment aid for patients.

</details>


### [27] [TimeColor: Flexible Reference Colorization via Temporal Concatenation](https://arxiv.org/abs/2601.00296)
*Bryan Constantine Sadihin,Yihao Meng,Michael Hua Wang,Matteo Jiahao Chen,Hang Su*

Main category: cs.CV

TL;DR: 本文提出了TimeColor模型，用于基于草图的视频上色，能够结合多种、不定数量的异构参考图像，实现更高质量的视频上色效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频上色模型通常只依赖单一参考图像，忽略了多样化的参考来源，如角色设定图、背景图像或任意已上色帧，导致无法充分表达复杂场景中的丰富颜色信息。因此需要一个能处理多种参考输入且能更好利用这些信息的模型。

Method: TimeColor模型采用显式每个参考的区域分配方式，将各种参考图像编码为附加的潜在帧，在扩散步骤中将这些帧按时间维度拼接处理，同时保证模型参数量不变。该模型引入了时空对应掩码注意力机制和模态异构RoPE索引，增强了对象与参考的绑定关系，有效防止取巧和跨身份的调色盘信息泄露。

Result: 在SAKUGA-42M数据集上的单参考和多参考协议下，TimeColor相较于以往基线方法，在颜色还原度、身份一致性及时间稳定性等方面均有明显提升。

Conclusion: TimeColor能高效集成多种类型的参考图像，提升视频上色的质量，具有更强的泛化能力和实用性。

Abstract: Most colorization models condition only on a single reference, typically the first frame of the scene. However, this approach ignores other sources of conditional data, such as character sheets, background images, or arbitrary colorized frames. We propose TimeColor, a sketch-based video colorization model that supports heterogeneous, variable-count references with the use of explicit per-reference region assignment. TimeColor encodes references as additional latent frames which are concatenated temporally, permitting them to be processed concurrently in each diffusion step while keeping the model's parameter count fixed. TimeColor also uses spatiotemporal correspondence-masked attention to enforce subject-reference binding in addition to modality-disjoint RoPE indexing. These mechanisms mitigate shortcutting and cross-identity palette leakage. Experiments on SAKUGA-42M under both single- and multi-reference protocols show that TimeColor improves color fidelity, identity consistency, and temporal stability over prior baselines.

</details>


### [28] [VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning](https://arxiv.org/abs/2601.00307)
*Anns Ijaz,Muhammad Azeem Javed*

Main category: cs.CV

TL;DR: 该论文提出了一个高效的行人重识别模型VisNet，在保证较高准确率的同时大幅降低计算成本，适合实际场景与边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 当前主流行人重识别方法虽然准确性高，但计算资源消耗大，难以在实际监控与移动端部署。因此需要一种在保证识别准确率的前提下，更加高效、轻量的模型。

Method: VisNet模型包括多尺度特征融合（自动关注各尺度）、基于人体解剖的语义聚类（结合空间约束和规则伪标签）、动态权重平均技巧（平衡语义正则化分类）、以及改进的FIDI损失函数用于度量学习。架构上融合ResNet50的前四层，无需并行路径。

Result: 在Market-1501数据集上，VisNet获得了87.05%的Rank-1准确率与77.65%的mAP，模型参数量32.41M，计算量4.601 GFLOPs，实现了准确性和效率的平衡。

Conclusion: VisNet证明了多项创新设计可以在不增加过多计算负担的前提下提升重识别效果，具备在监控、移动等资源受限场合实时部署的实用价值。

Abstract: Person re-identification (ReID) is an extremely important area in both surveillance and mobile applications, requiring strong accuracy with minimal computational cost. State-of-the-art methods give good accuracy but with high computational budgets. To remedy this, this paper proposes VisNet, a computationally efficient and effective re-identification model suitable for real-world scenarios. It is the culmination of conceptual contributions, including feature fusion at multiple scales with automatic attention on each, semantic clustering with anatomical body partitioning, a dynamic weight averaging technique to balance classification semantic regularization, and the use of loss function FIDI for improved metric learning tasks. The multiple scales fuse ResNet50's stages 1 through 4 without the use of parallel paths, with semantic clustering introducing spatial constraints through the use of rule-based pseudo-labeling. VisNet achieves 87.05% Rank-1 and 77.65% mAP on the Market-1501 dataset, having 32.41M parameters and 4.601 GFLOPs, hence, proposing a practical approach for real-time deployment in surveillance and mobile applications where computational resources are limited.

</details>


### [29] [ReMA: A Training-Free Plug-and-Play Mixing Augmentation for Video Behavior Recognition](https://arxiv.org/abs/2601.00311)
*Feng-Qi Cui,Jinyang Huang,Sirui Zhao,Jinglong Guo,Qifan Cai,Xin Yan,Zhi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频行为识别增强方法ReMA，能够更好地提升模型在复杂时空变化下的稳健性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频数据增强方法多采用扰动驱动，容易带来不受控的变化，增加了非判别性因素，削弱了类内分布结构，导致表示漂移和不一致性能提升。因此，亟需一种能够控制增强方式、兼顾判别性和稳定性的新方法。

Method: 本文提出了Representation-aware Mixing Augmentation (ReMA)，通过受控的同类混合扩展表示，同时保持类别条件下的稳定性。ReMA包含两个关键机制：一是表示对齐机制（RAM），在分布对齐约束下进行结构化的类内混合，抑制无关的类内漂移并增强统计可靠性；二是动态选择机制（DSM），生成具备运动感知的时空掩码，定位扰动区域，避开关键信息区，增强时序一致性。

Result: 在多个主流视频行为识别基准上，ReMA能在不同时空尺度下一致、显著提升模型的泛化性和鲁棒性，无需额外监督信号或可训练参数。

Conclusion: ReMA作为一种即插即用的视频增强策略，能够有效控制增强过程，通过结构化混合与动态掩码定位，提升表示鲁棒性与泛化能力，为视频行为识别任务提供了更优的解决方案。

Abstract: Video behavior recognition demands stable and discriminative representations under complex spatiotemporal variations. However, prevailing data augmentation strategies for videos remain largely perturbation-driven, often introducing uncontrolled variations that amplify non-discriminative factors, which finally weaken intra-class distributional structure and representation drift with inconsistent gains across temporal scales. To address these problems, we propose Representation-aware Mixing Augmentation (ReMA), a plug-and-play augmentation strategy that formulates mixing as a controlled replacement process to expand representations while preserving class-conditional stability. ReMA integrates two complementary mechanisms. Firstly, the Representation Alignment Mechanism (RAM) performs structured intra-class mixing under distributional alignment constraints, suppressing irrelevant intra-class drift while enhancing statistical reliability. Then, the Dynamic Selection Mechanism (DSM) generates motion-aware spatiotemporal masks to localize perturbations, guiding them away from discrimination-sensitive regions and promoting temporal coherence. By jointly controlling how and where mixing is applied, ReMA improves representation robustness without additional supervision or trainable parameters. Extensive experiments on diverse video behavior benchmarks demonstrate that ReMA consistently enhances generalization and robustness across different spatiotemporal granularities.

</details>


### [30] [Depth-Synergized Mamba Meets Memory Experts for All-Day Image Reflection Separation](https://arxiv.org/abs/2601.00322)
*Siyan Fang,Long Peng,Yuntao Wang,Ruonan Wei,Yuehuan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种用于图像反射分离的新模型DMDNet，通过结合深度信息与记忆模块，显著提升了白天和夜间图像反射分离的效果，并构建了新的夜间数据集NightIRS。


<details>
  <summary>Details</summary>
Motivation: 图像反射分离任务中，现有方法主要依赖单幅图像有限的信息，在透射层与反射层对比度相近时容易混淆，夜间环境下这一问题更加严重，亟需新的方法提升分离性能。

Method: 作者提出了深度记忆解耦网络（DMDNet）：1）采用深度感知扫描（DAScan）引导结构信息流；2）设计深度协同状态空间模型（DS-SSM）利用深度特征抑制模糊扩散；3）新增记忆专家补偿模块（MECM）利用跨图像历史知识辅助特定层分离。此外还构建了夜间数据集NightIRS。

Result: 在白天与夜间多个实验中，DMDNet均取得了优于现有最新方法的表现。

Conclusion: 本文提出的方法能够有效提升图像反射分离的准确性，尤其在夜间条件下表现出更强的鲁棒性，为相关研究和实际应用带来新的方向。

Abstract: Image reflection separation aims to disentangle the transmission layer and the reflection layer from a blended image. Existing methods rely on limited information from a single image, tending to confuse the two layers when their contrasts are similar, a challenge more severe at night. To address this issue, we propose the Depth-Memory Decoupling Network (DMDNet). It employs the Depth-Aware Scanning (DAScan) to guide Mamba toward salient structures, promoting information flow along semantic coherence to construct stable states. Working in synergy with DAScan, the Depth-Synergized State-Space Model (DS-SSM) modulates the sensitivity of state activations by depth, suppressing the spread of ambiguous features that interfere with layer disentanglement. Furthermore, we introduce the Memory Expert Compensation Module (MECM), leveraging cross-image historical knowledge to guide experts in providing layer-specific compensation. To address the lack of datasets for nighttime reflection separation, we construct the Nighttime Image Reflection Separation (NightIRS) dataset. Extensive experiments demonstrate that DMDNet outperforms state-of-the-art methods in both daytime and nighttime.

</details>


### [31] [HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection](https://arxiv.org/abs/2601.00327)
*Naiqi Zhang,Chuancheng Shi,Jingtong Dou,Wenhua Wu,Fei Shen,Jianhua Cao*

Main category: cs.CV

TL;DR: 本文提出了一种名为HarmoniAD的频域引导双分支异常检测框架，有效结合结构细节与语义信息，提升了工业产品质检中的微小缺陷检测能力，并在多个公开数据集上取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 工业产品质检对异常检测精度要求极高，微小缺陷错检可能带来严重后果。现有方法要么注重结构信息但易受噪声干扰，要么侧重语义但忽略细节，二者难以兼得。本文旨在解决结构与语义表征难兼容的矛盾，提高检测的灵敏度与鲁棒性。

Method: 提出HarmoniAD框架，首先用CLIP图像编码器提取特征，将特征转为频域后，分别通过高、低频路径建模。高频分支引入细粒度结构注意模块（FSAM）强化纹理和边缘，检出微小异常；低频分支用全局结构上下文模块（GSCM）捕获长距离依赖，保障语义一致。最终，采用多类别联合训练策略。

Result: 在MVTec-AD、VisA和BTAD三大数据集实验中，HarmoniAD在检测灵敏度和鲁棒性方面均取得最优（SOTA）结果。

Conclusion: HarmoniAD框架有效协调了结构与语义特征，实现了对工业微小缺陷的高效检测，并具有优秀的泛化能力和实际应用价值。

Abstract: Anomaly detection is crucial in industrial product quality inspection. Failing to detect tiny defects often leads to serious consequences. Existing methods face a structure-semantics trade-off: structure-oriented models (such as frequency-based filters) are noise-sensitive, while semantics-oriented models (such as CLIP-based encoders) often miss fine details. To address this, we propose HarmoniAD, a frequency-guided dual-branch framework. Features are first extracted by the CLIP image encoder, then transformed into the frequency domain, and finally decoupled into high- and low-frequency paths for complementary modeling of structure and semantics. The high-frequency branch is equipped with a fine-grained structural attention module (FSAM) to enhance textures and edges for detecting small anomalies, while the low-frequency branch uses a global structural context module (GSCM) to capture long-range dependencies and preserve semantic consistency. Together, these branches balance fine detail and global semantics. HarmoniAD further adopts a multi-class joint training strategy, and experiments on MVTec-AD, VisA, and BTAD show state-of-the-art performance with both sensitivity and robustness.

</details>


### [32] [Joint Geometry-Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion](https://arxiv.org/abs/2601.00328)
*Yingzhi Tang,Qijian Zhang,Junhui Hou*

Main category: cs.CV

TL;DR: 该论文提出了一种名为JGA-LBD的新型框架，可实现从单张RGB图片重建高保真的3D数字人几何和外观，方法为几何和外观统一建模并通过桥式扩散生成过程，有效提升了重建一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体重建方法通常将几何估计和外观合成分为两个解耦流程，导致重建结果不一致且质量受限。作者希望提出端到端统一的重建框架，解决几何和外观融合建模的难题。

Method: 方法创新点为：1）将所有的输入条件（如深度图、SMPL模型等）统一为3D高斯表示，2）利用共享的稀疏变分自编码器（VAE）将所有条件压缩到统一的潜空间，3）提出专用的桥式扩散流程，仅补全潜码中未知部分，4）专门的解码模块可从潜表示中完整重建3D人体并渲染新视角。

Result: 实验证明，JGA-LBD在几何保真度和外观质量方面均优于现有SOTA方法，并能适应不同类型和复杂场景（如in-the-wild）。

Conclusion: JGA-LBD真正实现了3D人体几何和外观的统一高质量重建，是目前性能最优之一的方法。代码也将开源，促进后续研究。

Abstract: Achieving consistent and high-fidelity geometry and appearance reconstruction of 3D digital humans from a single RGB image is inherently a challenging task. Existing studies typically resort to decoupled pipelines for geometry estimation and appearance synthesis, often hindering unified reconstruction and causing inconsistencies. This paper introduces \textbf{JGA-LBD}, a novel framework that unifies the modeling of geometry and appearance into a joint latent representation and formulates the generation process as bridge diffusion. Observing that directly integrating heterogeneous input conditions (e.g., depth maps, SMPL models) leads to substantial training difficulties, we unify all conditions into the 3D Gaussian representations, which can be further compressed into a unified latent space through a shared sparse variational autoencoder (VAE). Subsequently, the specialized form of bridge diffusion enables to start with a partial observation of the target latent code and solely focuses on inferring the missing components. Finally, a dedicated decoding module extracts the complete 3D human geometric structure and renders novel views from the inferred latent representation. Experiments demonstrate that JGA-LBD outperforms current state-of-the-art approaches in terms of both geometry fidelity and appearance quality, including challenging in-the-wild scenarios. Our code will be made publicly available at https://github.com/haiantyz/JGA-LBD.

</details>


### [33] [Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation](https://arxiv.org/abs/2601.00344)
*Bruce Mugizi,Sudi Murindanyi,Olivia Nakacwa,Andrew Katumba*

Main category: cs.CV

TL;DR: 本研究提出了一种针对乌干达等发展中国家的实时智能交通监控系统，结合车牌检测、识别与车速估算，具有较高准确率，并支持自动开罚单。旨在提升交通安全、减少车祸。


<details>
  <summary>Details</summary>
Motivation: 乌干达等发展中国家交通安全基础设施薄弱，超速问题导致大量道路死亡事故，亟需低成本、高效的交通管理与执法手段。

Method: 本研究利用计算机视觉技术，结合测速仪、相机与手机采集数据，开发车辆检测、车牌识别及车速估算模型。使用YOLOv8进行车牌检测，CNN与transformer模型分别进行字符识别，并构建数据库和自动短信罚单系统。

Result: YOLOv8车牌检测mAP达97.9%；CNN字符识别CER为3.85%，transformer模型降至1.79%；车速估算误差在10 km/h内；系统能自动实现用户、车辆与违规记录的关联及罚单发放。

Conclusion: 该系统为资源有限地区的智能交通执法提供了有效方案，有助于降低交通事故率，可广泛应用于发展中国家。

Abstract: Speeding is a major contributor to road fatalities, particularly in developing countries such as Uganda, where road safety infrastructure is limited. This study proposes a real-time intelligent traffic surveillance system tailored to such regions, using computer vision techniques to address vehicle detection, license plate recognition, and speed estimation. The study collected a rich dataset using a speed gun, a Canon Camera, and a mobile phone to train the models. License plate detection using YOLOv8 achieved a mean average precision (mAP) of 97.9%. For character recognition of the detected license plate, the CNN model got a character error rate (CER) of 3.85%, while the transformer model significantly reduced the CER to 1.79%. Speed estimation used source and target regions of interest, yielding a good performance of 10 km/h margin of error. Additionally, a database was established to correlate user information with vehicle detection data, enabling automated ticket issuance via SMS via Africa's Talking API. This system addresses critical traffic management needs in resource-constrained environments and shows potential to reduce road accidents through automated traffic enforcement in developing countries where such interventions are urgently needed.

</details>


### [34] [OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning](https://arxiv.org/abs/2601.00352)
*Liuxiang Qiu,Hui Da,Yuzhen Niu,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出了OmniVaT框架，首次有效解决了在单域条件下多模态视觉-触觉学习的泛化难题。通过创新的模态对齐和结构化表示技术，显著提升了跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态视觉-触觉学习面临视觉图像与触觉图像在模态上的差异，以及由于触觉传感器和数据采集流程不一致导致的领域差异，这妨碍了模型在新环境中的泛化能力。如何在仅用单一域数据时实现出色的跨域泛化，是当前的研究难点。

Method: 提出OmniVaT框架，其中包括多模态分数傅里叶适配器（MFFA）将视觉和触觉嵌入对齐至统一的频率-嵌入空间，缓解模态差异；同时引入离散树生成模块（DTG），通过分层树结构生成多样且可靠的分数表示，增强面对未见领域时的适应性。整个过程无需多域训练数据或复杂的跨模态融合策略。

Result: 在SDG-VTL（单域泛化多模态视觉-触觉学习）这一新任务上，OmniVaT框架在大量实验中表现出优越的跨域泛化性能，效果超过对比基线方法。

Conclusion: OmniVaT为多模态视觉-触觉学习领域提供了新颖且有效的单域泛化解决方案，为未来相关研究提供了借鉴，并大幅提升了多模态学习在真实复杂环境下的适用性。

Abstract: Visual-tactile learning (VTL) enables embodied agents to perceive the physical world by integrating visual (VIS) and tactile (TAC) sensors. However, VTL still suffers from modality discrepancies between VIS and TAC images, as well as domain gaps caused by non-standardized tactile sensors and inconsistent data collection procedures. We formulate these challenges as a new task, termed single domain generalization for multimodal VTL (SDG-VTL). In this paper, we propose an OmniVaT framework that, for the first time, successfully addresses this task. On the one hand, OmniVaT integrates a multimodal fractional Fourier adapter (MFFA) to map VIS and TAC embeddings into a unified embedding-frequency space, thereby effectively mitigating the modality gap without multi-domain training data or careful cross-modal fusion strategies. On the other hand, it also incorporates a discrete tree generation (DTG) module that obtains diverse and reliable multimodal fractional representations through a hierarchical tree structure, thereby enhancing its adaptivity to fluctuating domain shifts in unseen domains. Extensive experiments demonstrate the superior cross-domain generalization performance of OmniVaT on the SDG-VTL task.

</details>


### [35] [Mask-Conditioned Voxel Diffusion for Joint Geometry and Color Inpainting](https://arxiv.org/abs/2601.00368)
*Aarya Sumuk*

Main category: cs.CV

TL;DR: 本文提出了一种用于三维受损物体几何与颜色联合修复的轻量级两阶段框架，实现高效且完整的三维重建。


<details>
  <summary>Details</summary>
Motivation: 动机源自于数字化修复文化遗产文物，对既受损又存在表面纹理信息丢失的三维模型进行同时几何和色彩修复的方法需求。现有方法多聚焦于对称性或仅几何形态，难以兼顾纹理重建。

Method: 框架分为两阶段：第一，利用2D卷积网络对体素化物体切片的RGB图像预测受损掩码，并聚合为三维体素掩码；第二，基于扩散的3D U-Net在体素网格上进行掩码条件下的重建，同时预测几何和颜色，采用联合损失函数（包括几何损失、遮罩下的色彩重建损失和感知正则化）。

Result: 在包含合成损伤纹理文物的测试集上，用标准几何和颜色评价指标评估后，方法在给定分辨率（32^3）下几何完整性和色彩一致性均优于基于对称性的基线方法。

Conclusion: 显式利用掩码条件指导体素扩散模型，是实现三维几何与色彩联合修复的有效实用途径，对数字文物修复具有良好应用前景。

Abstract: We present a lightweight two-stage framework for joint geometry and color inpainting of damaged 3D objects, motivated by the digital restoration of cultural heritage artifacts. The pipeline separates damage localization from reconstruction. In the first stage, a 2D convolutional network predicts damage masks on RGB slices extracted from a voxelized object, and these predictions are aggregated into a volumetric mask. In the second stage, a diffusion-based 3D U-Net performs mask-conditioned inpainting directly on voxel grids, reconstructing geometry and color while preserving observed regions. The model jointly predicts occupancy and color using a composite objective that combines occupancy reconstruction with masked color reconstruction and perceptual regularization. We evaluate the approach on a curated set of textured artifacts with synthetically generated damage using standard geometric and color metrics. Compared to symmetry-based baselines, our method produces more complete geometry and more coherent color reconstructions at a fixed 32^3 resolution. Overall, the results indicate that explicit mask conditioning is a practical way to guide volumetric diffusion models for joint 3D geometry and color inpainting.

</details>


### [36] [BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition](https://arxiv.org/abs/2601.00369)
*Seungyeon Cho,Tae-kyun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种概率化的双流框架，将不确定性建模与多模态融合结合，提升了骨架动作识别特别是手部细粒度动作的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前骨架动作识别方法侧重于全身大尺度动作，忽略了对手部等细微动作的处理，而这些细节对于精细动作分类至关重要。

Method: 提出了三大技术创新：(1)全新的预处理流程，省略经典坐标变换，直接在原始坐标下学习；(2)引入概率化Noisy-OR融合机制，实现了无需显式置信度监督的鲁棒双流学习；(3)设计了从体内到跨模态的集成，将四类骨架特征（关节、骨、关节运动、骨运动）与RGB视觉特征耦合，实现结构与视觉的统一多模态融合。

Result: 在NTU RGB+D~60/120、PKU-MMD、N-UCLA等多个公开数据集与新提出的手部动作基准上，该方法在精度和抗噪性方面均取得了显著和一致的提升。

Conclusion: 所提框架有效提升了复杂环境下骨架动作识别的鲁棒性与精度，尤其对手部等细粒度动作表现优越，推动了多模态、跨域动作识别的发展。

Abstract: Skeleton-based human action recognition (HAR) has achieved remarkable progress with graph-based architectures. However, most existing methods remain body-centric, focusing on large-scale motions while neglecting subtle hand articulations that are crucial for fine-grained recognition. This work presents a probabilistic dual-stream framework that unifies reliability modeling and multi-modal integration, generalizing expertized learning under uncertainty across both intra-skeleton and cross-modal domains. The framework comprises three key components: (1) a calibration-free preprocessing pipeline that removes canonical-space transformations and learns directly from native coordinates; (2) a probabilistic Noisy-OR fusion that stabilizes reliability-aware dual-stream learning without requiring explicit confidence supervision; and (3) an intra- to cross-modal ensemble that couples four skeleton modalities (Joint, Bone, Joint Motion, and Bone Motion) to RGB representations, bridging structural and visual motion cues in a unified cross-modal formulation. Comprehensive evaluations across multiple benchmarks (NTU RGB+D~60/120, PKU-MMD, N-UCLA) and a newly defined hand-centric benchmark exhibit consistent improvements and robustness under noisy and heterogeneous conditions.

</details>


### [37] [NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos](https://arxiv.org/abs/2601.00393)
*Yuxue Yang,Lue Fan,Ziqi Shi,Junran Peng,Feng Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: NeoVerse是一种全新的4D世界建模系统，能够高效地进行4D重建、新轨迹视频生成，并应用于多种场景。


<details>
  <summary>Details</summary>
Motivation: 现有4D世界建模方法面临着扩展性差的问题，要么需要昂贵的多视角4D数据采集，要么需要复杂的训练预处理，限制了其在多样场景下的应用。

Method: NeoVerse通过无需位姿的前馈4D重建、在线的单目退化模式模拟等技术，实现对野外多样单目视频的扩展性建模。系统的一体化设计减少了对昂贵数据和复杂预处理的依赖，提高了训练和推理的效率。

Result: NeoVerse在标准的4D重建和生成基准测试中取得了当前最好的表现，同时在多个领域展示出强大的泛化能力和多样应用潜力。

Conclusion: NeoVerse突破了4D世界建模的扩展性瓶颈，为多领域场景提供了通用、高效、易扩展的4D重建与生成解决方案，推动了该领域的发展。

Abstract: In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io

</details>


### [38] [RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection](https://arxiv.org/abs/2601.00398)
*Tao Wu,Qing Xu,Xiangjian He,Oakleigh Weekes,James Brown,Wenting Duan*

Main category: cs.CV

TL;DR: 本文提出了RoLID-11K，这是首个针对行车记录仪（dashcam）拍摄路边垃圾检测的大规模数据集，包含超过1.1万张带标注的图片，并用多种现代检测器进行基准测试，以推动极小目标检测和低成本道路垃圾监测系统的发展。


<details>
  <summary>Details</summary>
Motivation: 目前路边垃圾监测主要依赖人工巡查和公众举报，方法费时费力且覆盖率有限。已有的垃圾检测视觉数据集多关注街景、航拍或水域环境，未能反映dashcam下垃圾目标极小、稀疏且背景复杂的独特难点。因此，亟须建立更符合实际需要的数据集。

Method: 作者建立了RoLID-11K数据集，收集并标注了11000多张覆盖多种英国道路环境的dashcam图片，体现明显的不平衡（长尾）分布和极小目标分布。随后利用多种主流检测器（包括transformer架构、YOLO等实时模型）在该数据集上进行基准测试，系统分析模型表现和局限。

Result: 实验显示，CO-DETR等transformer模型在定位精度上表现最好，而YOLO等实时模型因特征层次粗糙，对小目标检测能力有限。该数据集检验了当前主流目标检测算法在极小目标检测这一场景下的实际性能。

Conclusion: RoLID-11K为动态驾驶场景中极小目标检测提供了具有挑战性的基准数据，并有助于推动可扩展、低成本路边垃圾监测系统的研究与落地。数据集已公开发布，便于学术和工业界进一步探索。

Abstract: Roadside litter poses environmental, safety and economic challenges, yet current monitoring relies on labour-intensive surveys and public reporting, providing limited spatial coverage. Existing vision datasets for litter detection focus on street-level still images, aerial scenes or aquatic environments, and do not reflect the unique characteristics of dashcam footage, where litter appears extremely small, sparse and embedded in cluttered road-verge backgrounds. We introduce RoLID-11K, the first large-scale dataset for roadside litter detection from dashcams, comprising over 11k annotated images spanning diverse UK driving conditions and exhibiting pronounced long-tail and small-object distributions. We benchmark a broad spectrum of modern detectors, from accuracy-oriented transformer architectures to real-time YOLO models, and analyse their strengths and limitations on this challenging task. Our results show that while CO-DETR and related transformers achieve the best localisation accuracy, real-time models remain constrained by coarse feature hierarchies. RoLID-11K establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes and aims to support the development of scalable, low-cost systems for roadside-litter monitoring. The dataset is available at https://github.com/xq141839/RoLID-11K.

</details>


### [39] [ABFR-KAN: Kolmogorov-Arnold Networks for Functional Brain Analysis](https://arxiv.org/abs/2601.00416)
*Tyler Ward,Abdullah Imran*

Main category: cs.CV

TL;DR: 本文提出了一种基于transformer和Kolmogorov-Arnold Networks（KANs）的分类网络ABFR-KAN，用于提升脑功能连接分析的准确性和个体化，实验表明该方法在ASD分类任务中超越现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 目前脑功能连接（FC）分析高度依赖基于脑区图谱的分割（atlas-based parcellation），但存在选择偏倚和缺乏针对个体的不足，影响脑疾病辅助诊断的准确性。作者希望通过引入新方法缓解这些结构性偏见并提高分析的个性化和鲁棒性。

Method: 提出ABFR-KAN，该方法结合了transformer架构（用于强大的特征建模）和Kolmogorov-Arnold Networks（KANs，用于改善结构匹配和更好地逼近复杂函数关系），构建新颖的高级脑功能表征模块，并设计了一系列充分的实验（包含不同站点的交叉验证和消融分析）来评估其性能。

Result: 在ABIDE I自闭症谱系障碍（ASD）数据集上的大量实验证明，无论是不同模型主干还是不同的KAN配置，ABFR-KAN的ASD分类效果始终优于现有主流基线方法。

Conclusion: ABFR-KAN有效缓解了基于脑区分割方法的结构性偏见，提高了功能连接估计的解剖一致性和分类性能，为脑疾病辅助诊断提供了更可靠和个性化的工具。

Abstract: Functional connectivity (FC) analysis, a valuable tool for computer-aided brain disorder diagnosis, traditionally relies on atlas-based parcellation. However, issues relating to selection bias and a lack of regard for subject specificity can arise as a result of such parcellations. Addressing this, we propose ABFR-KAN, a transformer-based classification network that incorporates novel advanced brain function representation components with the power of Kolmogorov-Arnold Networks (KANs) to mitigate structural bias, improve anatomical conformity, and enhance the reliability of FC estimation. Extensive experiments on the ABIDE I dataset, including cross-site evaluation and ablation studies across varying model backbones and KAN configurations, demonstrate that ABFR-KAN consistently outperforms state-of-the-art baselines for autism spectrum distorder (ASD) classification. Our code is available at https://github.com/tbwa233/ABFR-KAN.

</details>


### [40] [Robust Assembly Progress Estimation via Deep Metric Learning](https://arxiv.org/abs/2601.00422)
*Kazuma Miura,Sarthak Pathak,Kazunori Umeda*

Main category: cs.CV

TL;DR: 本文提出了一种基于Quadruplet Loss的深度学习方法（Anomaly Quadruplet-Net），用于提升装配进度估算，特别适用于人工多日装配、变化细微或有遮挡场景。实验在PC装配数据集上验证了方法的有效性，优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 在实际工厂中，尤其是多天人工装配过程中，装配进度的自动化监控很重要，但由于连续任务间视觉变化细微，误分类较多，现有方法存在不足，需要更鲁棒的新方法。

Method: 提出了以Quadruplet Loss为核心的深度度量学习模型（Anomaly Quadruplet-Net），并设计了特殊的数据采样方式。该方法能更好识别异常影像，提高少量数据情况下的估算准确性。

Result: 该方法在PC装配图像数据集上相较于上一代Anomaly Triplet-Net，估算准确率提升1.3%，相邻任务误分率降低1.9%。

Conclusion: Anomaly Quadruplet-Net在装配进度估算任务上表现更优，特别是在视觉变化小、数据有限或有遮挡的复杂应用场景，提升了智能工厂的可应用性和自动化水平。

Abstract: In recent years, the advancement of AI technologies has accelerated the development of smart factories. In particular, the automatic monitoring of product assembly progress is crucial for improving operational efficiency, minimizing the cost of discarded parts, and maximizing factory productivity. However, in cases where assembly tasks are performed manually over multiple days, implementing smart factory systems remains a challenge. Previous work has proposed Anomaly Triplet-Net, which estimates assembly progress by applying deep metric learning to the visual features of products. Nevertheless, when visual changes between consecutive tasks are subtle, misclassification often occurs. To address this issue, this paper proposes a robust system for estimating assembly progress, even in cases of occlusion or minimal visual change, using a small-scale dataset. Our method leverages a Quadruplet Loss-based learning approach for anomaly images and introduces a custom data loader that strategically selects training samples to enhance estimation accuracy. We evaluated our approach using a image datasets: captured during desktop PC assembly. The proposed Anomaly Quadruplet-Net outperformed existing methods on the dataset. Specifically, it improved the estimation accuracy by 1.3% and reduced misclassification between adjacent tasks by 1.9% in the desktop PC dataset and demonstrating the effectiveness of the proposed method.

</details>


### [41] [CPPO: Contrastive Perception for Vision Language Policy Optimization](https://arxiv.org/abs/2601.00501)
*Ahmad Rezaei,Mohsen Gholami,Saeed Ranjbar Alvar,Kevin Cannons,Mohammad Asiful Hossain,Zhou Weimin,Shunbo Zhou,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

TL;DR: 本文提出了一种用于微调视觉语言模型（VLMs）的对比感知策略优化方法CPPO，可自动检测和优化输入中与感知相关的部分，无需依赖辅助模型或额外标注数据，实现高效可扩展的训练。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习推进了大语言模型的推理能力，但将其应用到多模态模型时，还需同步提升模型感知与推理两方面。以往做法主要依赖显式的感知奖励，但感知与推理的分离困难，通常需要额外的LLM、标注或强制策略，导致效率低下。

Method: CPPO通过分析模型在输入图像受扰动后输出熵的变化，自动检测与感知相关的token，在RL目标函数中引入对比感知损失（CPL）：在信息保留扰动下要求输出一致，在信息删除扰动下要求输出敏感。这样能精准奖励感知相关部分，避免对所有token一视同仁。

Result: 实验表明，CPPO方法在感知相关能力上超越了以往基于感知奖励的方法。同时避免了使用额外模型，实现了更高的训练效率和可扩展性。

Conclusion: CPPO实现了感知与推理分离下的高效训练，为多模态强化学习微调提供了无须大量辅助资源的可行方案，有望推动视觉语言模型更好地完成复杂推理与感知任务。

Abstract: We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.

</details>


### [42] [MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation](https://arxiv.org/abs/2601.00504)
*Miaowei Wang,Jakub Zadrożny,Oisin Mac Aodha,Amir Vaxman*

Main category: cs.CV

TL;DR: 提出了MotionPhysics框架，实现了从自然语言描述推断三维场景物理参数，无需专家指导和繁琐调参，能够对各种材质和物体生成逼真动态模拟，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统三维物体和材料物理模拟高度依赖专业知识和复杂参数调优过程，给用户带来很高门槛。希望通过自动化推断物理参数，简化流程，让一般用户也能通过自然语言实现真实感动态模拟。

Method: 1. 利用多模态大型语言模型，根据用户输入的自然语言描述估计场景中物体的材料参数，并限定在合理范围。
2. 提出可学习的运动蒸馏损失函数，从预训练的视频扩散模型中提取稳健运动先验，减少外观和几何偏置，引导物理模拟。
3. 综合上述模块，构建端到端可微分的物理参数推断与动态模拟系统。

Result: 在30余种场景（涵盖真实、人工设计及AI生成的三维物体，包括多种材料）上测试，MotionPhysics能基于自然语言生成物理合理、视觉逼真的动态过程，自动推断参数，并优于当前主流方法。

Conclusion: MotionPhysics开创性地实现了从自然语言推断三维场景材料参数并用于动态模拟，极大简化了三维物理仿真的门槛，能自动产出真实感结果，推动易用的3D物理建模发展。

Abstract: Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.

</details>


### [43] [All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations](https://arxiv.org/abs/2601.00533)
*Wenrui Li,Hongtao Chen,Yao Xiao,Wangmeng Zuo,Jiantao Zhou,Yonghong Tian,Xiaopeng Fan*

Main category: cs.CV

TL;DR: 本文针对视频中的多样且不断演变的未知退化问题，提出了SEUD场景与新型视频恢复方法ORCANet，实现了更高效的视频恢复。


<details>
  <summary>Details</summary>
Motivation: 现有的图像一体化恢复方法难以扩展到视频，主要原因在于其忽略了真实场景中退化具有时间连续性，即退化类型和强度会随时间平滑变化，且可能产生多重退化的并发或渐变现象。为应对这一现实且未被充分研究的退化演变场景，亟需新的统一视频恢复方法。

Method: 论文提出SEUD（Smoothly Evolving Unknown Degradations）场景，设计了一个能合成单一、复合和演变退化下时间连贯性视频的数据生成流程，并提出了一种全新的统一视频恢复模型ORCANet。ORCANet包括粗粒度强度估算去雾模块（CIED）和流式提示生成功能（FPG），分别完成初步的退化强度估计、静态与动态退化提示生成，并结合标签感知监督机制提升静态提示的判别能力。

Result: 大量实验显示，ORCANet在恢复质量、时间一致性以及鲁棒性方面都优于现有基于单帧图像和视频的主流方法。

Conclusion: ORCANet显著提升了复杂、未知且持续变化退化条件下的视频恢复效果，有力推动了统一视频恢复技术的发展。

Abstract: All-in-one image restoration aims to recover clean images from diverse unknown degradations using a single model. But extending this task to videos faces unique challenges. Existing approaches primarily focus on frame-wise degradation variation, overlooking the temporal continuity that naturally exists in real-world degradation processes. In practice, degradation types and intensities evolve smoothly over time, and multiple degradations may coexist or transition gradually. In this paper, we introduce the Smoothly Evolving Unknown Degradations (SEUD) scenario, where both the active degradation set and degradation intensity change continuously over time. To support this scenario, we design a flexible synthesis pipeline that generates temporally coherent videos with single, compound, and evolving degradations. To address the challenges in the SEUD scenario, we propose an all-in-One Recurrent Conditional and Adaptive prompting Network (ORCANet). First, a Coarse Intensity Estimation Dehazing (CIED) module estimates haze intensity using physical priors and provides coarse dehazed features as initialization. Second, a Flow Prompt Generation (FPG) module extracts degradation features. FPG generates both static prompts that capture segment-level degradation types and dynamic prompts that adapt to frame-level intensity variations. Furthermore, a label-aware supervision mechanism improves the discriminability of static prompt representations under different degradations. Extensive experiments show that ORCANet achieves superior restoration quality, temporal consistency, and robustness over image and video-based baselines. Code is available at https://github.com/Friskknight/ORCANet-SEUD.

</details>


### [44] [FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection](https://arxiv.org/abs/2601.00535)
*Ruiqiang Zhang,Hengyi Wang,Chang Liu,Guanjie Wang,Zehua Ma,Weiming Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种无需再训练、可直接应用于现有扩散模型的文本渲染改进方法FreeText，有效提升了文本（包括多行、复杂排版、长尾文字如中文）在合成图像中的清晰度与可控性。


<details>
  <summary>Details</summary>
Motivation: 当前大规模文生图（T2I）扩散模型在开放领域合成表现优秀，但在多行文本布局、密集排版及如中文这类长尾语言的精确文字渲染上仍表现不佳。已有的方法通常需要昂贵的再训练或依赖生硬的外部布局约束，对美学与建模灵活性造成影响。因此，需要无需再训练，且兼顾灵活性与渲染质量的方法。

Method: FreeText是一种无需训练即可用的框架，利用Diffusion Transformer（DiT）模型的内在关注机制，将文本渲染问题分解为“写哪里（where）”和“写什么（what）”。针对“写哪里”，通过分析图片生成过程中的空间关注机制定位合适文本区域，利用特殊token和拓扑优化生成置信度高的掩码；针对“写什么”，提出光谱调制字形注入（SGMI），在频域内注入与噪声对齐的字形先验，强化字形结构并抑制语义渗漏。

Result: 在Qwen-Image、FLUX.1-dev、SD3等多个T2I模型及长文本、复杂中文等多个基准上评测，FreeText大幅提升了文本可读性，同时文本与图片描述的语义一致性及美学表现基本不受影响，且推理开销较小。

Conclusion: FreeText作为一种即插即用、无需重新训练的方法，为扩散模型中的文字渲染，特别是在多行和复杂长文本应用上，提供了高质量且实用的解决方案。

Abstract: Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.

</details>


### [45] [Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios](https://arxiv.org/abs/2601.00537)
*Guangqian Guo,Pengfei Chen,Yong Guo,Huafeng Chen,Boqiang Zhang,Shan Gao*

Main category: cs.CV

TL;DR: 本文提出VNS-SAM模型，专注于提升SAM在前景与背景对比度低（视觉不显著）场景下的分割效果，并引入了VNS-SEG大规模数据集，实验表明VNS-SAM在零样本分割任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的Segment Anything Model（SAM）在低对比度、视觉不显著场景中分割效果不佳，难以精准捕捉目标轮廓，因而需要提升其在此类挑战性场景下的性能，同时保留其强大的零样本泛化能力。

Method: 提出VNS-SAM，通过引入Mask-Edge Token Interactive解码器和Non-Salient Feature Mining模块，充分利用SAM的低层特征，增强对非显著目标的理解。这些模块仅带来极小的参数和计算开销。此外，构建了包含35K+图像的VNS-SEG数据集，用于模型训练与评测。

Result: VNS-SAM在多个视觉不显著分割任务上表现优于其他方法，尤其在零样本设定下具有更高的分割精度。模型新增参数仅需4小时内完成训练，验证了其实用性。

Conclusion: VNS-SAM显著提升了对视觉不显著场景的分割能力，兼具高效、泛化性强，并通过大规模数据集验证了其实用性，具有广泛的现实应用前景。

Abstract: Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM's perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM's low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model's segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets are publicly available at https://guangqian-guo.github.io/VNS-SAM.

</details>


### [46] [DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction](https://arxiv.org/abs/2601.00542)
*Jiacheng Sui,Yujie Zhou,Li Niu*

Main category: cs.CV

TL;DR: 该论文提出了一种全新的基于“预测-移动”框架的图像拖拽编辑方法DynaDrag，在人脸与人体数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往的点或轨迹条件拖拽式图像编辑方法普遍存在漏跟踪、跟踪模糊、源图与目标结果差异大、中间过程不合理等问题，导致编辑效果受限。

Method: 提出DynaDrag方法，首次采用预测-移动框架，包含运动预测与运动监督两个阶段，通过迭代预测和拖拽，同时动态调整有效控制点，实现更精确的编辑。

Result: 在人脸和人的数据集上，DynaDrag方法在编辑效果和可控性方面都明显优于现有主流方法。

Conclusion: DynaDrag规避了传统方法的主要问题，通过创新的框架和动态控制机制，实现了更高质量的像素级图像编辑。

Abstract: To achieve pixel-level image manipulation, drag-style image editing which edits images using points or trajectories as conditions is attracting widespread attention. Most previous methods follow move-and-track framework, in which miss tracking and ambiguous tracking are unavoidable challenging issues. Other methods under different frameworks suffer from various problems like the huge gap between source image and target edited image as well as unreasonable intermediate point which can lead to low editability. To avoid these problems, we propose DynaDrag, the first dragging method under predict-and-move framework. In DynaDrag, Motion Prediction and Motion Supervision are performed iteratively. In each iteration, Motion Prediction first predicts where the handle points should move, and then Motion Supervision drags them accordingly. We also propose to dynamically adjust the valid handle points to further improve the performance. Experiments on face and human datasets showcase the superiority over previous works.

</details>


### [47] [SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array](https://arxiv.org/abs/2601.00551)
*Shuang Li,Yibing Wang,Jian Gao,Chulhong Kim,Seongwook Choi,Yu Zhang,Qian Chen,Yao Yao,Changhui Li*

Main category: cs.CV

TL;DR: 该论文提出了一种适用于任意阵列几何结构的先进三维光声成像重建算法SlingBAG Pro，显著提升了重建速度和质量。


<details>
  <summary>Details</summary>
Motivation: 随着三维光声成像在临床应用中需求的提升，传统规则探测阵列由于空间受限及成本高昂的问题，推动了不规则阵列的应用。但现有重建算法难以高效处理不规则阵列，导致计算复杂和耗时较长。

Method: 论文基于滑动球自适应生长（SlingBAG）方法的点云迭代理念，提出SlingBAG Pro算法，并扩展其对任意阵列几何的适应性。采用分层优化、零梯度滤波和动态采样率提升等策略以提高效率和质量。

Result: SlingBAG Pro在不规则阵列下的三维点云光声重建速度提升至原算法的2.2倍，且维持高重建质量。并通过数值仿真与小鼠在体实验验证了有效性。

Conclusion: SlingBAG Pro能高效支持任意几何阵列，实现更快而优质的三维光声重建，对空间受限和低成本的临床应用具有实际推动意义。

Abstract: High-quality three-dimensional (3D) photoacoustic imaging (PAI) is gaining increasing attention in clinical applications. To address the challenges of limited space and high costs, irregular geometric transducer arrays that conform to specific imaging regions are promising for achieving high-quality 3D PAI with fewer transducers. However, traditional iterative reconstruction algorithms struggle with irregular array configurations, suffering from high computational complexity, substantial memory requirements, and lengthy reconstruction times. In this work, we introduce SlingBAG Pro, an advanced reconstruction algorithm based on the point cloud iteration concept of the Sliding ball adaptive growth (SlingBAG) method, while extending its compatibility to arbitrary array geometries. SlingBAG Pro maintains high reconstruction quality, reduces the number of required transducers, and employs a hierarchical optimization strategy that combines zero-gradient filtering with progressively increased temporal sampling rates during iteration. This strategy rapidly removes redundant spatial point clouds, accelerates convergence, and significantly shortens overall reconstruction time. Compared to the original SlingBAG algorithm, SlingBAG Pro achieves up to a 2.2-fold speed improvement in point cloud-based 3D PA reconstruction under irregular array geometries. The proposed method is validated through both simulation and in vivo mouse experiments, and the source code is publicly available at https://github.com/JaegerCQ/SlingBAG_Pro.

</details>


### [48] [A Comprehensive Dataset for Human vs. AI Generated Image Detection](https://arxiv.org/abs/2601.00553)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Vasu Sharma,Vinija Jain,Aman Chadha,Aishwarya Naresh Reganti,Amitava Das*

Main category: cs.CV

TL;DR: 本文提出并发布了一个用于AI生成图像检测的新数据集MS COCOAI，助力区分真实与生成图像，并识别生成模型。


<details>
  <summary>Details</summary>
Motivation: 随着Stable Diffusion、DALL-E、MidJourney等多模态生成式AI的发展，合成图片已难以与真实照片区分，容易被用于误导、虚假信息传播等不良用途，因此亟需有效的检测方法。

Method: 作者基于MS COCO数据集，利用五种主流生成模型（Stable Diffusion 3、Stable Diffusion 2.1、SDXL、DALL-E 3和MidJourney v6）生成合成图片，构建了包含96000条真实与合成图像的数据集。并基于该数据集提出了两个任务：1）判断图片是真实还是生成的；2）识别生成图片所用的具体模型。

Result: 成功构建了大规模、多生成模型的合成与真实图像数据集MS COCOAI，实现并明确提出两大检测任务，为生成图像检测研究提供了资源和方向。

Conclusion: 该数据集为AI生成图片检测领域提供了重要资源，有助于推动相关研究，提升对虚假内容、防止误导等问题的应对能力。

Abstract: Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at https://huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset.

</details>


### [49] [AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models](https://arxiv.org/abs/2601.00561)
*Jintao Lin,Bowen Dong,Weikang Shi,Chenyang Lei,Suiyun Zhang,Rui Liu,Xihui Liu*

Main category: cs.CV

TL;DR: 本文提出了一个新的多任务基准AEGIS，以及一种更可靠的判定性评测协议DCE，用于全面评估统一多模态模型（UMMs）在世界知识方面的能力。实验发现，现有UMMs在复杂推理和世界知识问题上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的评测基准主要针对单一任务，无法诊断UMMs在处理多类型世界知识和推理任务上的薄弱环节，导致对模型能力的评估不够全面准确。

Method: 作者提出AEGIS基准，涵盖视觉理解、生成、编辑与交错生成等任务，手工标注覆盖21个主题和6类推理方式，并引入DCE协议，以Y/N原子判断替代主观评分提高评测可靠性。还通过实验证明不同模型及外部推理插件的表现。

Result: 实验显示主流UMMs在涉及世界知识与复杂推理时表现较差，可靠性受到挑战。而简单插拔式推理模块可在一定程度上缓解模型弱点。

Conclusion: 世界知识与推理能力是UMMs亟待突破的关键难题，AEGIS和DCE为后续相关研究和模型优化提供了更严格可靠的评测工具和方向。

Abstract: The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N'' judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.

</details>


### [50] [A Cascaded Information Interaction Network for Precise Image Segmentation](https://arxiv.org/abs/2601.00562)
*Hewen Xiao,Jie Mei,Guangfu Ma,Weiren Wu*

Main category: cs.CV

TL;DR: 提出了一种融合全局信息的级联卷积网络，显著提升了复杂场景下的视觉分割性能，并超越了主流方法。


<details>
  <summary>Details</summary>
Motivation: 视觉感知是自主系统的关键，但复杂环境下的鲁棒分割依然存在挑战。现有方案往往在信息融合和多尺度特征提取上有不足。

Method: 设计了级联卷积神经网络，并引入全新的全局信息引导模块，实现多层低级纹理与高级语义特征的有效融合，从而克服单一尺度特征的局限。

Result: 在多个图像分割基准数据集上，所提框架分割精度优于当前最先进方法，尤其在视觉混乱、模糊等场景下表现突出。

Conclusion: 该方法有效提升了分割准确率，验证了架构创新的实用性，有望实际应用于机器人等领域。

Abstract: Visual perception plays a pivotal role in enabling autonomous behavior, offering a cost-effective and efficient alternative to complex multi-sensor systems. However, robust segmentation remains a challenge in complex scenarios. To address this, this paper proposes a cascaded convolutional neural network integrated with a novel Global Information Guidance Module. This module is designed to effectively fuse low-level texture details with high-level semantic features across multiple layers, thereby overcoming the inherent limitations of single-scale feature extraction. This architectural innovation significantly enhances segmentation accuracy, particularly in visually cluttered or blurred environments where traditional methods often fail. Experimental evaluations on benchmark image segmentation datasets demonstrate that the proposed framework achieves superior precision, outperforming existing state-of-the-art methods. The results highlight the effectiveness of the approach and its promising potential for deployment in practical robotic applications.

</details>


### [51] [GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval](https://arxiv.org/abs/2601.00584)
*Mingyu Jeon,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: 本文提出了一个无需训练的新方法GranAlign，有效解决了零样本视频时刻检索任务中语义粒度不匹配的问题，并在多个基准数据集上刷新了性能指标。


<details>
  <summary>Details</summary>
Motivation: 以自然语言定位视频片段时，文字和视频的预训练特征在语义粒度上不匹配，当前方法虽有高质量的单模态表征，却很难实现精准对齐，导致检索准确率有限。

Method: 提出Granularity-Aware Alignment（GranAlign）框架，不需要再训练，用“两步”：1）基于粒度的查询重写，生成不同语义粒度的query；2）基于query生成定制化视频caption，将查询意图注入视频内容。通过多级查询和多样化caption配对，实现不同模态间的高效对齐。

Result: 方法在三个主流数据集（QVHighlights、Charades-STA、ActivityNet-Captions）上创下最新最好成绩，尤其在QVHighlights数据集上的mAP@avg指标提升3.23%。

Conclusion: 通过引入粒度感知对齐技术，无需训练即可有效缓解语义粒度不匹配问题，显著提升零样本视频片段检索的准确性。

Abstract: Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.

</details>


### [52] [SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation](https://arxiv.org/abs/2601.00590)
*Yiling Wang,Zeyu Zhang,Yiran Wang,Hao Tang*

Main category: cs.CV

TL;DR: 该论文提出了SafeMo框架，有效解决了现有文本到动作生成（T2M）模型中的安全隐患问题，通过连续空间的最小运动反遗忘机制生成安全且自然的人体动作，并构建了首个安全T2M数据集SafeMoVAE-29K，显著提升了对不安全动作的遗忘能力，同时保持良好的正常动作生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本到动作生成技术中，虽可生成高真实度与高文本一致性的人体动作，但在训练数据与生成过程中会产生与传递不安全或不合规的人体动作，影响广泛应用及公共安全。现有以替换VQ-VAE离散码本条目为主的安全干预方法，会导致对日常动作的性能下降及生成动作不自然、出现断裂等问题。此外，公开数据集本身存在大量不安全意图或动作，训练难以兼顾安全性和实用性。因此急需更稳健且实用的安全T2M生成与数据平台。

Method: 作者提出SafeMo框架，核心方法为连续空间下的最小运动反遗忘（MMU），以二阶段机器反遗忘策略隔离并遗忘不安全的人体动作数据，同时保持连续运动学属性与生成流畅度，避免采用离散码本引入的量化误差和动作突变。论文还构建了SafeMoVAE-29K安全T2M数据集，通过文本重写和动作精炼获得安全的人体动作。实验基于DiP扩散模型开展，对比SOTA反遗忘方法LCR，对不安全提示的遗忘效果进行评判。

Result: SafeMo在HumanML3D和Motion-X这两个数据集上，对不安全动作的遗忘能力（forget-set FID）相比LCR分别提高2.5倍和14.4倍，同时在正常（安全）动作任务上与原有方法性能相当甚至更优。新数据集也为后续安全T2M研究提供了更好的基准。

Conclusion: SafeMo模型为文本到动作生成带来了结构性安全保障，通过连续空间反遗忘与专门的数据集建设，实现了强安全-性能权衡。未来应用于更广泛的安全敏感场景具有实际意义，其代码和数据集的开源也将推动社区相关研究。

Abstract: Text-to-motion (T2M) generation with diffusion backbones achieves strong realism and alignment. Safety concerns in T2M methods have been raised in recent years; existing methods replace discrete VQ-VAE codebook entries to steer the model away from unsafe behaviors. However, discrete codebook replacement-based methods have two critical flaws: firstly, replacing codebook entries which are reused by benign prompts leads to drifts on everyday tasks, degrading the model's benign performance; secondly, discrete token-based methods introduce quantization and smoothness loss, resulting in artifacts and jerky transitions. Moreover, existing text-to-motion datasets naturally contain unsafe intents and corresponding motions, making them unsuitable for safety-driven machine learning. To address these challenges, we propose SafeMo, a trustworthy motion generative framework integrating Minimal Motion Unlearning (MMU), a two-stage machine unlearning strategy, enabling safe human motion generation in continuous space, preserving continuous kinematics without codebook loss and delivering strong safety-utility trade-offs compared to current baselines. Additionally, we present the first safe text-to-motion dataset SafeMoVAE-29K integrating rewritten safe text prompts and continuous refined motion for trustworthy human motion unlearning. Built upon DiP, SafeMo efficiently generates safe human motions with natural transitions. Experiments demonstrate effective unlearning performance of SafeMo by showing strengthened forgetting on unsafe prompts, reaching 2.5x and 14.4x higher forget-set FID on HumanML3D and Motion-X respectively, compared to the previous SOTA human motion unlearning method LCR, with benign performance on safe prompts being better or comparable. Code: https://github.com/AIGeeksGroup/SafeMo. Website: https://aigeeksgroup.github.io/SafeMo.

</details>


### [53] [Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception](https://arxiv.org/abs/2601.00598)
*Xianhui Liu,Siqi Jiang,Yi Xie,Yuqing Lin,Siao Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的RGB-红外（IR）多模态感知方法，针对多模态特征融合过程中由模态不对称带来的优化偏差，提出了模态主导性指数（MDI）和以此为基础的跨模态学习框架（MDACL），并在多个基准数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 多模态（RGB-IR）融合检测在复杂环境中非常关键，但现有方法没有充分考虑不同模态（RGB与IR）间特征信息密度与质量的不一致，导致训练过程中模型会过分依赖某一主导模态，从而影响融合效果。作者希望定量分析与解决这一普遍存在但被忽视的问题。

Method: 作者提出模态主导性指数（MDI），通过联合建模特征熵和梯度贡献，量化融合时每一模态的主导作用。基于MDI，构建了模态主导性感知的跨模态学习框架（MDACL），包括分层跨模态引导（HCG）模块用于提升特征对齐，和对抗均衡正则化（AER）模块用于平衡融合过程中的优化动态。

Result: 通过在三个RGB-IR基准数据集上的大量实验证明，MDACL能够有效缓解优化偏差问题，并获得了当前最优的检测性能。

Conclusion: 提出的方法不仅能量化多模态融合中的主导效应，还能通过新型的正则化与引导机制改善融合效果，对多模态感知研究与实际应用具有广泛参考价值。

Abstract: RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.

</details>


### [54] [Noise-Robust Tiny Object Localization with Flows](https://arxiv.org/abs/2601.00617)
*Huixin Sun,Linlin Yang,Ronyu Chen,Kerui Gu,Baochang Zhang,Angela Yao,Xianbin Cao*

Main category: cs.CV

TL;DR: 本文提出了一种针对微小目标检测中标注噪声问题的定位方案TOLF，通过归一化流模型噪声分布，并基于不确定性调节训练过程，实现对噪声的鲁棒性提升，在多个数据集上显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 微小目标检测性能远不及常规尺度目标，主要原因在于对标注噪声高度敏感，现有方法在严格定位时易过拟合噪声，导致性能瓶颈。

Method: 提出TOLF框架，采用归一化流（normalizing flows）对定位误差进行灵活建模，应对复杂、非高斯噪声分布；同时引入不确定性感知的梯度调控机制，对高不确定性、噪声样本抑制其影响，增强训练稳定性和泛化能力。

Result: 在三个数据集上实验，TOLF明显优于基线方法，尤其在AI-TOD数据集上，相较DINO基线提升了1.2% AP。

Conclusion: TOLF能有效缓解微小目标检测中因标注噪声导致的过拟合和性能瓶颈，为噪声环境下的目标定位带来更稳健的方法。

Abstract: Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach's effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.

</details>


### [55] [RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation](https://arxiv.org/abs/2601.00625)
*Junxiao Xue,Pavel Smirnov,Ziao Li,Yunyun Shi,Shi Chen,Xinyi Yin,Xiaohan Yue,Lei Wang,Yiduo Wang,Feng Lin,Yijia Chen,Xiao Ma,Xiaoran Yan,Qing Zhang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: 提出了一种基于多摄像头RGB视频输入的实时三维人体姿态估计与运动分析方法RePose，用于康复训练，通过高效跟踪和改进的姿态估计，实现患者动作实时监控和评估，并利用Unity 实现肌肉应力可视化。


<details>
  <summary>Details</summary>
Motivation: 康复训练过程中需要对患者动作进行准确、实时的监测与反馈，以指导患者规范进行训练动作，提升康复效果，并降低人为监测成本。

Method: 1. 搭建端到端实时三维姿态估计与动作分析统一流程，基于多摄像头RGB视频输入。2. 针对多人干扰场景提出高效（单帧<1ms）跟踪方法。3. 对SmoothNet进行改进，提升姿态估计准确度与动作还原平滑性。4. 利用Unity平台进行动作监测、评估和肌肉应力可视化。

Result: 所提方法能够实时监测和评估康复患者的动作，快速跟踪多人场景下的目标，提升姿态估计准确性和动作的真实还原，系统能够可视化肌肉应力并辅助训练。

Conclusion: RePose为康复训练带来了端到端、实时、精准的人体姿态估计与动作分析方案，有效提升了患者训练的规范性与康复效果，降低了依赖人工监测的需求。

Abstract: We propose a real-time 3D human pose estimation and motion analysis method termed RePose for rehabilitation training. It is capable of real-time monitoring and evaluation of patients'motion during rehabilitation, providing immediate feedback and guidance to assist patients in executing rehabilitation exercises correctly. Firstly, we introduce a unified pipeline for end-to-end real-time human pose estimation and motion analysis using RGB video input from multiple cameras which can be applied to the field of rehabilitation training. The pipeline can help to monitor and correct patients'actions, thus aiding them in regaining muscle strength and motor functions. Secondly, we propose a fast tracking method for medical rehabilitation scenarios with multiple-person interference, which requires less than 1ms for tracking for a single frame. Additionally, we modify SmoothNet for real-time posture estimation, effectively reducing pose estimation errors and restoring the patient's true motion state, making it visually smoother. Finally, we use Unity platform for real-time monitoring and evaluation of patients' motion during rehabilitation, and to display the muscle stress conditions to assist patients with their rehabilitation training.

</details>


### [56] [HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis](https://arxiv.org/abs/2601.00626)
*Shuren Gabriel Yu,Sikang Ren,Yongji Tian*

Main category: cs.CV

TL;DR: 论文提出了一种基于超图的学习框架HyperPriv-EPN，利用手术后文本数据指导手术前MRI预后分析，通过知识蒸馏提升无文本情况下的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 手术前对于室管膜瘤的预后判断非常关键，但MRI缺乏手术报告中的语义信息，导致预估困难。现有多模态方法在推理时无法利用这些宝贵的手术后文本数据。

Method: 提出了HyperPriv-EPN框架，采用超图和LUPI（使用特权信息学习）策略。具体方法是用一个共享编码器分别处理包含手术后信息的Teacher图和仅有术前信息的Student图，通过双流蒸馏让Student学会仅用视觉特征推断语义结构。

Result: 在包含311名患者的多中心数据集上，HyperPriv-EPN在诊断准确率和生存分层方面均达到最新最优水平。

Conclusion: 该方法实现了专家知识向手术前场景的迁移，使得无需文本输入也能取得优异预估表现，充分释放历史手术后数据的价值。

Abstract: Preoperative prognosis of Ependymoma is critical for treatment planning but challenging due to the lack of semantic insights in MRI compared to post-operative surgical reports. Existing multimodal methods fail to leverage this privileged text data when it is unavailable during inference. To bridge this gap, we propose HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information (LUPI) framework. We introduce a Severed Graph Strategy, utilizing a shared encoder to process both a Teacher graph (enriched with privileged post-surgery information) and a Student graph (restricted to pre-operation data). Through dual-stream distillation, the Student learns to hallucinate semantic community structures from visual features alone. Validated on a multi-center cohort of 311 patients, HyperPriv-EPN achieves state-of-the-art diagnostic accuracy and survival stratification. This effectively transfers expert knowledge to the preoperative setting, unlocking the value of historical post-operative data to guide the diagnosis of new patients without requiring text at inference.

</details>


### [57] [Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach](https://arxiv.org/abs/2601.00645)
*Shrikant Kapse,Priyankkumar Dhrangdhariya,Priya Kedia,Manasi Patwardhan,Shankar Kausley,Soumyadipta Maiti,Beena Rai,Shirish Karande*

Main category: cs.CV

TL;DR: 本文提出用基于图像的深度学习方法，实现马铃薯储存过程中质量监测，包括发芽检测、重量损失估算和保质期预测。DenseNet模型发芽检测准确率高达98.03%，粗粒度（2-5类）保质期预测准确率超过89.83%。基于图像模型能助力自动分拣，提高库存管理效率、减少浪费。


<details>
  <summary>Details</summary>
Motivation: 马铃薯在储存和流通过程中容易发生发芽、失重等问题，影响品质与经济效益。传统质量检测方法多为破坏性或低效方法，制约了大规模、无损、自动化监测的发展。因而需要非侵入、高效可扩展的质量评估手段，支持更优的库存管理与供应链决策。

Method: 作者在200天的受控环境下收集马铃薯图像及重量数据，利用ResNet、VGG、DenseNet与ViT等已有强大模型，分别训练了高精度二分类发芽检测模型和多分类重量损失/保质期预测模型。性能通过不同类别粒度的分类准确率衡量，并进行比较分析。

Result: DenseNet模型在发芽检测任务中取得98.03%的准确率。保质期预测中，类别分为2-5类时模型准确率超过89.83%；类别更细（6-8类）时，由于视觉差别小、样本少，准确率有所下降。

Conclusion: 基于图像的深度学习模型能有效实现马铃薯非破坏性质量评估，具备集成到自动化分拣和库存系统的可行性。粗粒度预测效果更稳定，利于实际应用。未来研究应扩展到更多品种和条件以增强泛化性。这一方法可提升供应链效率，减少浪费，促进可持续发展。

Abstract: Image-based deep learning provides a non-invasive, scalable solution for monitoring potato quality during storage, addressing key challenges such as sprout detection, weight loss estimation, and shelf-life prediction. In this study, images and corresponding weight data were collected over a 200-day period under controlled temperature and humidity conditions. Leveraging powerful pre-trained architectures of ResNet, VGG, DenseNet, and Vision Transformer (ViT), we designed two specialized models: (1) a high-precision binary classifier for sprout detection, and (2) an advanced multi-class predictor to estimate weight loss and forecast remaining shelf-life with remarkable accuracy. DenseNet achieved exceptional performance, with 98.03% accuracy in sprout detection. Shelf-life prediction models performed best with coarse class divisions (2-5 classes), achieving over 89.83% accuracy, while accuracy declined for finer divisions (6-8 classes) due to subtle visual differences and limited data per class. These findings demonstrate the feasibility of integrating image-based models into automated sorting and inventory systems, enabling early identification of sprouted potatoes and dynamic categorization based on storage stage. Practical implications include improved inventory management, differential pricing strategies, and reduced food waste across supply chains. While predicting exact shelf-life intervals remains challenging, focusing on broader class divisions ensures robust performance. Future research should aim to develop generalized models trained on diverse potato varieties and storage conditions to enhance adaptability and scalability. Overall, this approach offers a cost-effective, non-destructive method for quality assessment, supporting efficiency and sustainability in potato storage and distribution.

</details>


### [58] [Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network](https://arxiv.org/abs/2601.00658)
*Zhaiyu Chen,Yuanyuan Wang,Yilei Shi,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的框架，将噪声较大的TomoSAR点云直接转化为高分辨率的城市建筑高度图，实现大规模城市高度测绘。


<details>
  <summary>Details</summary>
Motivation: TomoSAR在复杂城市环境下能捕捉建筑侧面结构，是传统光学方法有力补充，但现有TomoSAR点云存在噪声、分布不均及数据缺失等问题，限制了高度重建精度。因此亟需有效方法提升TomoSAR点云到高度图的可靠性。

Method: 提出双分支网络结构：点分支建模点云中无序散射体特征，栅格分支加强空间一致性，通过联合学习，对输入点云去噪、补全缺失区，输出连续建筑高度估算；同时可融合光学影像进一步提升精度。

Result: 在慕尼黑和柏林大规模数据上实验，验证了该方法从TomoSAR点云生成建筑高度图的有效性，融合光学影像后重建质量进一步提升。

Conclusion: 该工作首次从TomoSAR点云实现大规模城市高度自动测绘，方法有效且具拓展性，为城市遥感应用提供了新工具，同时代码已公开，便于社区复现和扩展。

Abstract: Reliable building height estimation is essential for various urban applications. Spaceborne SAR tomography (TomoSAR) provides weather-independent, side-looking observations that capture facade-level structure, offering a promising alternative to conventional optical methods. However, TomoSAR point clouds often suffer from noise, anisotropic point distributions, and data voids on incoherent surfaces, all of which hinder accurate height reconstruction. To address these challenges, we introduce a learning-based framework for converting raw TomoSAR points into high-resolution building height maps. Our dual-topology network alternates between a point branch that models irregular scatterer features and a grid branch that enforces spatial consistency. By jointly processing these representations, the network denoises the input points and inpaints missing regions to produce continuous height estimates. To our knowledge, this is the first proof of concept for large-scale urban height mapping directly from TomoSAR point clouds. Extensive experiments on data from Munich and Berlin validate the effectiveness of our approach. Moreover, we demonstrate that our framework can be extended to incorporate optical satellite imagery, further enhancing reconstruction quality. The source code is available at https://github.com/zhu-xlab/tomosar2height.

</details>


### [59] [CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models](https://arxiv.org/abs/2601.00659)
*Neeraj Anand,Samyak Jha,Udbhav Bamba,Rahul Rahaman*

Main category: cs.CV

TL;DR: 本文提出了一种新的无训练消除幻觉方法CRoPS，通过选择性去除关键文本token并结合多种幻觉模型，有效缓解LVLMs生成的幻觉问题，在多项基准测试和模型类型中取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉-语言模型（LVLMs）虽然表现优异，但经常生成虚假（幻觉）内容，影响其实用性。已有无训练方法存在对幻觉成因假设狭窄及生成后期效果下降等问题，因此需要更有效的幻觉缓解策略。

Method: 作者提出了一种新的幻觉建模方式：不仅移除视觉token，还首次尝试去除关键信息的文本token，并与原模型构成对比。同时引入Generalized Contrastive Decoding，将多种幻觉模型联合，对抗多源幻觉。整个方法组成训练无关的CRoPS框架。

Result: CRoPS在CHAIR评分上提升约20%，并在6个基准任务与3类LVLM模型中持续超越现有最优的无训练幻觉缓解方法。

Conclusion: CRoPS框架通过多模型对比和关键内容剔除，能够在无需额外训练情况下，有效抑制LVLM生成过程中的幻觉内容，提升模型可靠性。

Abstract: Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.

</details>


### [60] [Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians](https://arxiv.org/abs/2601.00678)
*Melonie de Almeida,Daniela Ivanova,Tong Shi,John H. Williamson,Paul Henderson*

Main category: cs.CV

TL;DR: 本文提出了一种基于单张图片生成高质量视频的新方法，可以同时实现对摄像机路径的精确控制与保持场景时序一致和几何完整性。实验结果优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 尽管现有单图像生成视频方法在时序和3D一致性取得进展，但在用户可控性（如摄像机运动控制）、时序一致性和几何保真方面仍存在不足。实际应用需求推动更有可控性和真实感的视频生成技术发展。

Method: 作者提出利用3D高斯场景表示，结合单前向传播实现物体运动采样，无需像传统方式那样分两步处理（先对点云建模后再引入运动），也无需又慢又复杂的逐步去噪，从而实现高效的、摄像机导向的视频生成。

Result: 在KITTI、Waymo、RealEstate10K和DL3DV-10K等四个数据集上进行大量实验，结果显示该方法在视频质量和生成效率上达到或超越了当前业界最优水平。

Conclusion: 提出的方法不仅提升了视频生成的效率和质量，还能实现更灵活、更符合真实场景需求的用户控制，为现实中智能系统的视觉生成带来更强的工具。

Abstract: Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.

</details>


### [61] [Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks](https://arxiv.org/abs/2601.00703)
*Cory Fan,Wenchao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种在等方性网络中引入空间下采样的方法，有效提升了图像去马赛克（demosaicing）任务在移动设备上的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 数字成像领域广泛采用深度学习进行图像去马赛克，但现有等方性（residual-in-residual）网络由于不使用下采样，计算量大、效率低，不适合移动端应用。因此，作者希望找到提升等方性网络性能和效率的方法。

Method: 作者采用了来自DeepMAD的数学架构设计技巧，分别设计了带和不带下采样的全卷积神经网络，并进行对比评测。同时，提出了带有下采样机制的JD3Net架构，用于图像去马赛克及去噪联合任务。

Result: 实验结果显示，采用空间下采样的等方性网络在图像去马赛克和去噪联合任务上取得了更好的性能，验证了下采样提升效率与表现的观点。JD3Net在多个相关任务上表现优异。

Conclusion: 与传统观点相反，适当引入空间下采样可以显著提升等方性网络在图像去马赛克和去噪任务上的效率与性能，有望推动移动端成像应用的落地。

Abstract: In digital imaging, image demosaicing is a crucial first step which recovers the RGB information from a color filter array (CFA). Oftentimes, deep learning is utilized to perform image demosaicing. Given that most modern digital imaging applications occur on mobile platforms, applying deep learning to demosaicing requires lightweight and efficient networks. Isotropic networks, also known as residual-in-residual networks, have been often employed for image demosaicing and joint-demosaicing-and-denoising (JDD). Most demosaicing isotropic networks avoid spatial downsampling entirely, and thus are often prohibitively expensive computationally for mobile applications. Contrary to previous isotropic network designs, this paper claims that spatial downsampling to a signficant degree can improve the efficiency and performance of isotropic networks. To validate this claim, we design simple fully convolutional networks with and without downsampling using a mathematical architecture design technique adapted from DeepMAD, and find that downsampling improves empirical performance. Additionally, empirical testing of the downsampled variant, JD3Net, of our fully convolutional networks reveals strong empirical performance on a variety of image demosaicing and JDD tasks.

</details>


### [62] [Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model](https://arxiv.org/abs/2601.00716)
*Hao Guan,Li Zhou*

Main category: cs.CV

TL;DR: 提出了一种结合输入层数据分布变动检测和输出层无标签置信度指标的方法，以提升医学视觉-语言模型在数据漂移场景下的性能退化检测能力，并开发了DomainSAT工具进行可视化分析。


<details>
  <summary>Details</summary>
Motivation: 医学视觉-语言模型部署后，实际输入数据分布可能与开发时不同，造成性能下降。检测性能退化对于临床应用的可靠性至关重要，但现有无标签检测方法仍具挑战。

Method: （1）系统分析输入级数据分布漂移与输出级预测行为对性能监控的作用；（2）开发DomainSAT工具，集成多种检测算法，支持可视化探索数据分布变动；（3）提出无需标签的基于输出置信度的性能退化指标；（4）在大规模病理肿瘤分类数据集上结合上述方法实验验证。

Result: 输入数据漂移检测能及时发现分布变化，并提供早期预警，但并不总是与性能下降吻合。输出置信度指标与实际性能下降表现出紧密关联，二者结合能更准确检测和解释模型性能退化。

Conclusion: 结合输入数据分布检测和输出置信度指标，能更可靠地监测医学视觉-语言基础模型在实际分布变动下的性能退化，为数字病理领域相关模型部署提供了实用且互补的监控框架。

Abstract: Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.

</details>


### [63] [Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection](https://arxiv.org/abs/2601.00725)
*Johannes C. Bauer,Paul Geng,Stephan Trattnig,Petr Dokládal,Rüdiger Daub*

Main category: cs.CV

TL;DR: 本文提出了一种多层次特征融合（MLFF）方法，可在预训练网络不同深度提取并融合特征，提升制造领域动态质量检测场景下的深度学习模型适应效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习广泛应用于制造业视觉质检，但“再制造”等高变动场景下，产品和缺陷模式常变化，现有模型需频繁适应新情况，这面临持续学习挑战且要兼顾计算高效与避免灾难性遗忘。

Method: 提出多层次特征融合（MLFF）方法，从预训练网络不同深度提取特征进行融合，实现高效持续学习，显著减少可训练参数。

Result: 该方法在不同质检任务中，用更少参数达到端到端训练同等表现，同时降低灾难性遗忘，对新产品或缺陷的泛化鲁棒性更强。

Conclusion: MLFF能高效适应不断变化的质检场景，提升深度模型的持续学习能力与泛化能力，为制造业的自动化质量检测提供有力支持。

Abstract: Deep neural networks show great potential for automating various visual quality inspection tasks in manufacturing. However, their applicability is limited in more volatile scenarios, such as remanufacturing, where the inspected products and defect patterns often change. In such settings, deployed models require frequent adaptation to novel conditions, effectively posing a continual learning problem. To enable quick adaptation, the necessary training processes must be computationally efficient while still avoiding effects like catastrophic forgetting. This work presents a multi-level feature fusion (MLFF) approach that aims to improve both aspects simultaneously by utilizing representations from different depths of a pretrained network. We show that our approach is able to match the performance of end-to-end training for different quality inspection problems while using significantly less trainable parameters. Furthermore, it reduces catastrophic forgetting and improves generalization robustness to new product types or defects.

</details>


### [64] [Grading Handwritten Engineering Exams with Multimodal Large Language Models](https://arxiv.org/abs/2601.00730)
*Janez Perš,Jon Muhovič,Andrej Košir,Boštjan Murovec*

Main category: cs.CV

TL;DR: 该论文提出了一种基于多模态大语言模型(LLMs)的自动评分流程，实现了对手写工程试卷的自动化批改，极大提升了评分效率并保持了传统考试流程。


<details>
  <summary>Details</summary>
Motivation: 手写STEM考试可以全面捕捉学生的开放性推理和图示能力，但人工批改耗时且难以大规模进行。因此，需要研发自动化、可扩展、并可靠的批改系统。

Method: 设计了端到端的评分工作流程，教师仅需提供手写标准答案和简单评分规则，由系统将其转化为文本概要用以指导评分。引入多阶段机制：包括格式/内容检查避免误判空白答案，多独立打分器集成，监督汇总，以及严格模板和可验证报告。并在斯洛文尼亚真实课程小测(含手绘电路图)上用如GPT-5.2和Gemini-3 Pro等SOTA模型进行测试。

Result: 该流程实现了平均约8分的分数误差，偏差低，在40分满分下手动复核触发率为17%左右。消融实验显示，简单/无结构提示或不参考标准答案会显著降低准确率且平均偏高评分。

Conclusion: 结构化提示和以参考答案为基础的引导对于准确评分至关重要，自动评分流程在准确性、可审计性和可扩展性上有良好表现，有望大幅减轻人工批改负担。

Abstract: Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\approx$17% at $D_{\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.

</details>


### [65] [Unified Primitive Proxies for Structured Shape Completion](https://arxiv.org/abs/2601.00759)
*Zhaiyu Chen,Yuqing Wang,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的3D形状补全方法UniCo，通过一次前向传播预测出完整的几何体原语及其语义信息，在多个基准上实现了大幅度性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前结构化形状补全多采用原语恢复缺失几何，但原有方法通常通过级联方式处理原语和点，效率和效果有限。作者希望提升原语与点的交互效率，实现更高效、结构化的3D理解。

Method: 作者设计了UniCo，利用原语代理（primitive proxies）作为可学习查询，结合专门的解码路径，直接预测带有几何、语义和成员信息的原语集合。训练中通过在线目标更新将原语与点耦合，确保一致优化。

Result: 在合成和真实数据上的多个评测基准和四种独立组装算法下，UniCo在Chamfer距离上提升了最多50%，法线一致性提升高达7%，全面优于现有方法。

Conclusion: UniCo为结构化3D场景补全过程提供了一种高效统一的方案，显著提升了数据补全下的几何与语义理解能力，推动了不完整数据场景下的三维形状理解方法发展。

Abstract: Structured shape completion recovers missing geometry as primitives rather than as unstructured points, which enables primitive-based surface reconstruction. Instead of following the prevailing cascade, we rethink how primitives and points should interact, and find it more effective to decode primitives in a dedicated pathway that attends to shared shape features. Following this principle, we present UniCo, which in a single feed-forward pass predicts a set of primitives with complete geometry, semantics, and inlier membership. To drive this unified representation, we introduce primitive proxies, learnable queries that are contextualized to produce assembly-ready outputs. To ensure consistent optimization, our training strategy couples primitives and points with online target updates. Across synthetic and real-world benchmarks with four independent assembly solvers, UniCo consistently outperforms recent baselines, lowering Chamfer distance by up to 50% and improving normal consistency by up to 7%. These results establish an attractive recipe for structured 3D understanding from incomplete data. Project page: https://unico-completion.github.io.

</details>


### [66] [Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection](https://arxiv.org/abs/2601.00789)
*Shukesh Reddy,Srijan Das,Abhijit Das*

Main category: cs.CV

TL;DR: 本文通过将自监督学习作为辅助任务，结合主任务进行深度伪造检测，提升了检测的泛化性能，并在多个数据集上超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测常面临泛化能力不足的问题，探索如何通过辅助任务（自监督学习）增强检测模型的泛化和鲁棒性。

Method: 提出将自监督学习融入主任务训练，通过特征融合方式同时利用自监督任务和主任务的表征能力，实验对比不同训练策略，验证最佳方式。

Result: 在DF40、FaceForensics++、Celeb-DF、DFD、FaceShifter、UADFV等多个数据集上测试，提出方法在跨数据集评估时优于现有最先进检测方法。

Conclusion: 自监督学习辅助下的特征融合能带来更优的泛化特征，显著提升深度伪造检测的性能，特别是在不同数据集间的检测能力。

Abstract: In this work, we attempted to unleash the potential of self-supervised learning as an auxiliary task that can optimise the primary task of generalised deepfake detection. To explore this, we examined different combinations of the training schemes for these tasks that can be most effective. Our findings reveal that fusing the feature representation from self-supervised auxiliary tasks is a powerful feature representation for the problem at hand. Such a representation can leverage the ultimate potential and bring in a unique representation of both the self-supervised and primary tasks, achieving better performance for the primary task. We experimented on a large set of datasets, which includes DF40, FaceForensics++, Celeb-DF, DFD, FaceShifter, UADFV, and our results showed better generalizability on cross-dataset evaluation when compared with current state-of-the-art detectors.

</details>


### [67] [Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI](https://arxiv.org/abs/2601.00794)
*Wenhui Chu,Nikolaos V. Tsekos*

Main category: cs.CV

TL;DR: 本文提出了两种新型深度学习架构LNU-Net和IBU-Net，用于心脏短轴磁共振图像中左心室的自动分割，并取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 左心室分割对于心脏病的临床诊断和定量分析至关重要，但现有方法在分割精度和鲁棒性上存在一定局限，因此需要开发新型高效的分割模型。

Method: 提出了基于U-Net的两种新网络结构：LNU-Net将层归一化(layer normalization)应用于每个卷积模块，IBU-Net在首个卷积层内结合了instance和batch normalization。网络包括下采样用于特征提取，上采样用于精确定位，并在数据处理阶段采用仿射与弹性变换进行增强。在包含805幅MRI心脏短轴图像的自有数据集上进行训练和评估。

Result: LNU-Net和IBU-Net在Dice系数和平均垂直距离等主要指标上均优于原始U-Net及其他先进方法，显示了更好的分割性能。

Conclusion: 两种新提出的网络结构提升了左心室分割的准确性和鲁棒性，为心脏影像的临床诊断提供了更可靠的工具。

Abstract: Left ventricle (LV) segmentation is critical for clinical quantification and diagnosis of cardiac images. In this work, we propose two novel deep learning architectures called LNU-Net and IBU-Net for left ventricle segmentation from short-axis cine MRI images. LNU-Net is derived from layer normalization (LN) U-Net architecture, while IBU-Net is derived from the instance-batch normalized (IB) U-Net for medical image segmentation. The architectures of LNU-Net and IBU-Net have a down-sampling path for feature extraction and an up-sampling path for precise localization. We use the original U-Net as the basic segmentation approach and compared it with our proposed architectures. Both LNU-Net and IBU-Net have left ventricle segmentation methods: LNU-Net applies layer normalization in each convolutional block, while IBU-Net incorporates instance and batch normalization together in the first convolutional block and passes its result to the next layer. Our method incorporates affine transformations and elastic deformations for image data processing. Our dataset that contains 805 MRI images regarding the left ventricle from 45 patients is used for evaluation. We experimentally evaluate the results of the proposed approaches outperforming the dice coefficient and the average perpendicular distance than other state-of-the-art approaches.

</details>


### [68] [AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction](https://arxiv.org/abs/2601.00796)
*Jiewen Chan,Zhenjun Zhao,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 本论文提出AdaGaR框架，通过自适应Gabor表示和时域约束，有效提升了单目视频动态3D场景的重建质量，兼顾高频细节与运动连续性，实验性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法在重建动态3D场景时，单一高斯基元因低通特性难以保留高频细节，Gabor函数又易引入能量不稳定，同时缺乏时域连续性约束，导致运动插值时出现伪影。

Method: 1. 提出自适应Gabor表示（AdaGaR），在高斯基元基础上引入可学习的频率权重和自适应能量补偿，提升细节与稳定性；2. 对时域采用三次Hermite样条和时域曲率正则，确保运动平滑；3. 训练初期结合深度估计、点追踪和前景掩码自适应初始化，稳健生成点云分布。

Result: 在Tap-Vid DAVIS等数据集上，方法PSNR、SSIM、LPIPS等指标均达到SOTA水平，并在帧插值、深度一致性、视频编辑和立体视图合成等任务上表现出优异的泛化能力。

Conclusion: AdaGaR框架有效解决了显式动态场景建模中的频率和连续性双重难题，为单目视频3D动态重建提供了高效、稳定和汎化能力强的新方案。

Abstract: Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [69] [RIMRULE: Improving Tool-Using Language Agents via MDL-Guided Rule Learning](https://arxiv.org/abs/2601.00086)
*Xiang Gao,Yuguang Yao,Qi Zhang,Kaiwen Dong,Avinash Baidya,Ruocheng Guo,Hilaf Hasson,Kamalika Das*

Main category: cs.CL

TL;DR: 本文提出了一种结合神经网络与符号方法（RIMRULE），通过注入从失败案例中提炼的规则以提升大语言模型（LLMs）在特定领域工具使用上的适应能力，且该方法在多个工具使用基准任务表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在特定领域工具使用上存在不稳定性，主要由于接口独特性、文档匮乏、或高度定制，急需高效的适应方法提升其可靠应用。

Method: 提出的RIMRULE是一种神经-符号混合的方法：模型会从错误轨迹中自动总结紧凑易懂的规则，并在推理阶段注入这些规则以辅助工具使用。规则由模型自动提出，通过最小描述长度（MDL）目标优化，保证规则既简明又泛化。规则以自然语言和结构化符号两种形式存储，便于在推理时高效调用。

Result: RIMRULE无需改动LLM模型本体，仅依赖规则注入，显著提升了模型在已见与未见工具上的工具使用准确率，优于现有的基于Prompt方法。此外，这些规则还能跨LLM迁移，提高其他模型（包括长推理模型）的表现。

Conclusion: RIMRULE方法提升了大语言模型领域工具适应性和泛化能力，规则迁移性强，可与现有微调方法配合，对实际应用具有重要意义。

Abstract: Large language models (LLMs) often struggle to use tools reliably in domain-specific settings, where APIs may be idiosyncratic, under-documented, or tailored to private workflows. This highlights the need for effective adaptation to task-specific tools. We propose RIMRULE, a neuro-symbolic approach for LLM adaptation based on dynamic rule injection. Compact, interpretable rules are distilled from failure traces and injected into the prompt during inference to improve task performance. These rules are proposed by the LLM itself and consolidated using a Minimum Description Length (MDL) objective that favors generality and conciseness. Each rule is stored in both natural language and a structured symbolic form, supporting efficient retrieval at inference time. Experiments on tool-use benchmarks show that this approach improves accuracy on both seen and unseen tools without modifying LLM weights. It outperforms prompting-based adaptation methods and complements finetuning. Moreover, rules learned from one LLM can be reused to improve others, including long reasoning LLMs, highlighting the portability of symbolic knowledge across architectures.

</details>


### [70] [Universal Adaptive Constraint Propagation: Scaling Structured Inference for Large Language Models via Meta-Reinforcement Learning](https://arxiv.org/abs/2601.00095)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CL

TL;DR: MetaJuLS是一种元强化学习方法，通过学习用于传播约束的通用策略，实现多任务多语言的结构化推断，无需针对具体任务重新训练，并显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在许多应用中都需要结构化推断，比如强制输出满足复杂约束（如JSON Schema、多语言解析等）。现有方法通常依赖任务特定的模型或耗时的再训练，难以高效适应不同任务和语言。

Method: 作者提出了MetaJuLS，将结构化推断建模为自适应约束传播过程，采用图注意网络（Graph Attention Network），并通过元学习训练，使策略在多语言多任务间能泛化，只需极少梯度更新就能适应新任务。

Result: 在10种语言的Universal Dependencies任务以及LogicBench、GSM8K-Constrained等受限生成任务上，MetaJuLS较GPU优化baseline有1.5~2倍加速，准确率仅比最优解析器低0.2%。同一策略仅需几步梯度更新（5~15秒）即可迁移新任务，无需小时级训练。分析表明MetaJuLS学会了类人策略与新颖推理启发式方法。

Conclusion: MetaJuLS大幅减少结构化推断步数和碳排放，实现了快速跨领域迁移，为绿色AI做出贡献，并展现出强大的泛化能力和效率。

Abstract: Large language models increasingly require structured inference, from JSON schema enforcement to multi-lingual parsing, where outputs must satisfy complex constraints. We introduce MetaJuLS, a meta-reinforcement learning approach that learns universal constraint propagation policies applicable across languages and tasks without task-specific retraining. By formulating structured inference as adaptive constraint propagation and training a Graph Attention Network with meta-learning, MetaJuLS achieves 1.5--2.0$\times$ speedups over GPU-optimized baselines while maintaining within 0.2\% accuracy of state-of-the-art parsers. On Universal Dependencies across 10 languages and LLM-constrained generation (LogicBench, GSM8K-Constrained), MetaJuLS demonstrates rapid cross-domain adaptation: a policy trained on English parsing adapts to new languages and tasks with 5--10 gradient steps (5--15 seconds) rather than requiring hours of task-specific training. Mechanistic analysis reveals the policy discovers human-like parsing strategies (easy-first) and novel non-intuitive heuristics. By reducing propagation steps in LLM deployments, MetaJuLS contributes to Green AI by directly reducing inference carbon footprint.

</details>


### [71] [Pat-DEVAL: Chain-of-Legal-Thought Evaluation for Patent Description](https://arxiv.org/abs/2601.00166)
*Yongmin Yoo,Kris W Pan*

Main category: cs.CL

TL;DR: 本文提出了第一个专注于专利描述体的多维度评价框架 Pat-DEVAL，利用大模型结合法律思维链，实现对专利描述的结构连贯性与法律合规性的自动化评估。实验证明，该方法显著优于现有评估标准。


<details>
  <summary>Details</summary>
Motivation: 现有专利撰写自动化虽能生成专利文本，但缺乏专门评估其结构连贯性和法律合规性的有效方法，导致自动生成文本无法满足专利的法律要求和技术可实施性。

Method: 提出了 Pat-DEVAL 框架，基于 LLM-as-a-judge 方法，引入 Chain-of-Legal-Thought（CoLT）机制，实现对专利描述体的法律约束下的顺序化分析。通过构建 Pap2Pat-EvalGold 数据集并有专利专家参与实验，系统性评估生成文本的技术与法律属性。

Result: Pat-DEVAL 在整体评估上取得了 Pearson 相关系数 0.69，在法律专业合规性方面达到 0.73，明显优于现有基线指标和大模型评价器，显示出法律约束机制重要性。

Conclusion: Pat-DEVAL 建立了专利描述评价的新标准，为自动化专利撰写系统大规模应用在技术与法律两方面提供了方法和基础，有助于推动该技术的实际部署。

Abstract: Patent descriptions must deliver comprehensive technical disclosure while meeting strict legal standards such as enablement and written description requirements. Although large language models have enabled end-to-end automated patent drafting, existing evaluation approaches fail to assess long-form structural coherence and statutory compliance specific to descriptions. We propose Pat-DEVAL, the first multi-dimensional evaluation framework dedicated to patent description bodies. Leveraging the LLM-as-a-judge paradigm, Pat-DEVAL introduces Chain-of-Legal-Thought (CoLT), a legally-constrained reasoning mechanism that enforces sequential patent-law-specific analysis. Experiments validated by patent expert on our Pap2Pat-EvalGold dataset demonstrate that Pat-DEVAL achieves a Pearson correlation of 0.69, significantly outperforming baseline metrics and existing LLM evaluators. Notably, the framework exhibits a superior correlation of 0.73 in Legal-Professional Compliance, proving that the explicit injection of statutory constraints is essential for capturing nuanced legal validity. By establishing a new standard for ensuring both technical soundness and legal compliance, Pat-DEVAL provides a robust methodological foundation for the practical deployment of automated patent drafting systems.

</details>


### [72] [Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation](https://arxiv.org/abs/2601.00181)
*Cheonkam Jeong,Adeline Nyamathi*

Main category: cs.CL

TL;DR: 本文通过系统分析IEMOCAP数据集，探讨了情感识别对话（ERC）模型架构影响因素及情感与语用标记的关系，提出上下文信息为性能提升关键，其它复杂结构和外部词典作用有限。


<details>
  <summary>Details</summary>
Motivation: 尽管ERC已取得较高准确率，但缺乏对架构设计有效性以及情感识别和生成之间关联的理解。作者旨在厘清哪些模型选择真正有益，并通过语言分析连接情感识别与生成任务。

Method: 对IEMOCAP数据集进行系统性分析，包括10组种子下的消融实验，考察上下文长度、层级句表示、情感词典对识别性能影响；并对5286个语用标记进行统计分析，探索标记位置与情感类别的关系。

Result: 1. 上下文信息占主导，仅需最近10-30轮即达最佳90%性能；2. 层级句表示对无语境有效，加入对话语境后无额外益处；3. 外部情感词典无提升，预训练编码器已能捕捉情感语义。简单架构下，模型在4类和6类情感识别上分别获82.69%与67.07%的加权F1，优于以往方法。标记分析显示“sad”情感左标记显著减少，且更需语境消歧。

Conclusion: 对话情感识别模型核心在于合理利用语境信息，复杂结构或外部情感词典提升有限。语用分析揭示了情感识别对语境的依赖机制，尤其是悲伤情感。工作为ERC模型设计和情感生成任务的关联研究提供了新见解。

Abstract: While Emotion Recognition in Conversation (ERC) has achieved high accuracy, two critical gaps remain: a limited understanding of \textit{which} architectural choices actually matter, and a lack of linguistic analysis connecting recognition to generation. We address both gaps through a systematic analysis of the IEMOCAP dataset.
  For recognition, we conduct a rigorous ablation study with 10-seed evaluation and report three key findings. First, conversational context is paramount, with performance saturating rapidly -- 90\% of the total gain achieved within just the most recent 10--30 preceding turns (depending on the label set). Second, hierarchical sentence representations help at utterance-level, but this benefit disappears once conversational context is provided, suggesting that context subsumes intra-utterance structure. Third, external affective lexicons (SenticNet) provide no gain, indicating that pre-trained encoders already capture necessary emotional semantics. With simple architectures using strictly causal context, we achieve 82.69\% (4-way) and 67.07\% (6-way) weighted F1, outperforming prior text-only methods including those using bidirectional context.
  For linguistic analysis, we analyze 5,286 discourse marker occurrences and find a significant association between emotion and marker positioning ($p < .0001$). Notably, "sad" utterances exhibit reduced left-periphery marker usage (21.9\%) compared to other emotions (28--32\%), consistent with theories linking left-periphery markers to active discourse management. This connects to our recognition finding that sadness benefits most from context (+22\%p): lacking explicit pragmatic signals, sad utterances require conversational history for disambiguation.

</details>


### [73] [Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2601.00202)
*Wang Xing,Wei Song,Siyu Lin,Chen Wu,Zhesi Li,Man Wang*

Main category: cs.CL

TL;DR: 本文提出了一种专为时序知识图谱推理定制的蒸馏框架，结合大语言模型指导，实现高效轻量化的推理模型，兼具准确性与实用性，适合资源受限平台部署。


<details>
  <summary>Details</summary>
Motivation: 现有时序知识图谱推理模型参数量大、计算密集，部署在低功耗和分布式设备上存在困难；现有的压缩与蒸馏技术多针对静态知识图谱，难以捕捉时序依赖，导致性能下降。为此，亟需针对时序知识图谱的高效轻量化推理方案。

Method: 提出一种利用大语言模型作为教师模型的专用蒸馏框架，将结构与时序推理能力有效迁移到轻量学生模型，结合大规模公开知识与任务相关的时序信息，实现紧凑高效的模型结构。

Result: 在多个公开基准数据集上进行大量实验，该方法在推理准确率、计算效率和可部署性等方面均优于现有强基线方法，达成了在性能和效率之间的良好权衡。

Conclusion: 本文提出的时序知识图谱推理蒸馏框架有效提升了学生模型的时序动态建模能力，并兼具轻量高效的优势，适宜实际应用中对资源受限与实时性的要求。

Abstract: Reasoning over temporal knowledge graphs (TKGs) is fundamental to improving the efficiency and reliability of intelligent decision-making systems and has become a key technological foundation for future artificial intelligence applications. Despite recent progress, existing TKG reasoning models typically rely on large parameter sizes and intensive computation, leading to high hardware costs and energy consumption. These constraints hinder their deployment on resource-constrained, low-power, and distributed platforms that require real-time inference. Moreover, most existing model compression and distillation techniques are designed for static knowledge graphs and fail to adequately capture the temporal dependencies inherent in TKGs, often resulting in degraded reasoning performance. To address these challenges, we propose a distillation framework specifically tailored for temporal knowledge graph reasoning. Our approach leverages large language models as teacher models to guide the distillation process, enabling effective transfer of both structural and temporal reasoning capabilities to lightweight student models. By integrating large-scale public knowledge with task-specific temporal information, the proposed framework enhances the student model's ability to model temporal dynamics while maintaining a compact and efficient architecture. Extensive experiments on multiple publicly available benchmark datasets demonstrate that our method consistently outperforms strong baselines, achieving a favorable trade-off between reasoning accuracy, computational efficiency, and practical deployability.

</details>


### [74] [From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark](https://arxiv.org/abs/2601.00216)
*Jinning Zhang,Jie Song,Wenhui Tu,Zecheng Li,Jingxuan Li,Jin Li,Xuan Liu,Taole Sha,Zichen Wei,Yan Li*

Main category: cs.CL

TL;DR: 本研究提出了一种将循证医学（EBM）原则纳入基于检索增强生成（RAG）的大型语言模型方法，并在运动康复领域取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有医学领域RAG方法多专注于模型性能，而忽视了EBM原则，特别是缺乏PICO准则的对齐及证据分级在文献重排序中的考量。这导致生成答案在医学证据严谨性上的不足。

Method: 本研究将PICO框架纳入知识图谱的构建与检索流程，并提出基于贝叶斯思想的证据分级重排序算法，实现无需预设权重即可根据证据等级调整文献排名。系统在运动康复领域进行了验证，并发布了大规模知识图谱和可复用问答基准数据集。

Result: 系统在文献覆盖率、答案可信性、语义相似性及PICO匹配准确性等多项指标上表现优异，并获得临床专家在准确性、可信度、相关性、安全性和PICO对齐方面高度评价。

Conclusion: 本方法有效提升了RAG在医学应用中的证据利用和答案质量，具有良好的领域迁移性，同时丰富了运动康复领域的RAG数据资源。

Abstract: In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.

</details>


### [75] [JP-TL-Bench: Anchored Pairwise LLM Evaluation for Bidirectional Japanese-English Translation](https://arxiv.org/abs/2601.00223)
*Leonard Lin,Adam Lensenmayer*

Main category: cs.CL

TL;DR: 本文提出了JP-TL-Bench，这是一个轻量化、开放的基准，用于优化日英翻译系统，强调对高质量候选翻译之间优劣的微妙评判。其创新点在于使用固定“锚集”与无参考的LLM对比，结合Bradley-Terry模型进行计分，保证评测系统可靠且稳定。


<details>
  <summary>Details</summary>
Motivation: 当前日英翻译系统评测难点在于判别两个高质量译文之间的细微差别，如礼貌、语气、省略等语言特性影响自然度，而且人工评测成本高、主观波动大。作者希望通过设计统一且自动化的评测框架，推动翻译系统的持续优化。

Method: 提出使用固定的锚集作为参照集，对待测模型生成的译文与锚集中的译文进行无参考、两两对比，由大型语言模型（LLM）担任评判，实现自动化、人力成本低的评测。所有结果通过Bradley-Terry模型汇总，输出胜率和归一化的0-10“LT”评分。

Result: 该基准能够对每个候选翻译系统输出稳定可信的得分，评测过程效率高，且结果对基准集和评判标准变化不敏感。

Conclusion: JP-TL-Bench为日英机器翻译系统开发提供了标准化、低成本且高一致性的自动评测工具，有助于更细致地推动系统改进和研发。

Abstract: We introduce JP-TL-Bench, a lightweight, open benchmark designed to guide the iterative development of Japanese-English translation systems. In this context, the challenge is often "which of these two good translations is better?" rather than "is this translation acceptable?" This distinction matters for Japanese-English, where subtle choices in politeness, implicature, ellipsis, and register strongly affect perceived naturalness. JP-TL-Bench uses a protocol built to make LLM judging both reliable and affordable: it evaluates a candidate model via reference-free, pairwise LLM comparisons against a fixed, versioned anchor set. Pairwise results are aggregated with a Bradley-Terry model and reported as win rates plus a normalized 0-10 "LT" score derived from a logistic transform of fitted log-strengths. Because each candidate is scored against the same frozen anchor set, scores are structurally stable given the same base set, judge, and aggregation code.

</details>


### [76] [Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback](https://arxiv.org/abs/2601.00224)
*Yan Sun,Ming Cai,Stanley Kok*

Main category: cs.CL

TL;DR: 本文提出了两种用于企业级会话式业务分析（CBA）系统的验证技术，分别为Q*（逆向翻译与语义匹配）和Feedback+（基于执行反馈的代码改进），能够有效减轻用户的验证负担，提高生成型AI系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）助理在企业中逐渐普及，其输出的准确性和可执行性变得格外重要，现有CBA系统缺乏自动结果验证，导致用户需要手动核查，效率低且风险高。

Method: 本文提出两种机制：Q* 使用逆向翻译和语义匹配来验证代码与用户意图是否一致；Feedback+ 借助执行结果反馈以指导代码修正。两者都嵌入到生成-判别框架之中，实现系统级的自动验证。

Result: 在Spider、Bird和GSM8K三个基准数据集上的实验表明，这两种机制都能显著降低错误率和用户完成任务的时间。此外，研究还发现逆向翻译过程目前是主要瓶颈。

Conclusion: 本研究为构建更可靠、可信赖的企业级生成式AI系统提供了设计方案，将验证责任从用户转移到系统，对于实现高质量的决策支持具有重要意义。

Abstract: As large language model (LLM) assistants become increasingly integrated into enterprise workflows, their ability to generate accurate, semantically aligned, and executable outputs is critical. However, current conversational business analytics (CBA) systems often lack built-in verification mechanisms, leaving users to manually validate potentially flawed results. This paper introduces two complementary verification techniques: Q*, which performs reverse translation and semantic matching between code and user intent, and Feedback+, which incorporates execution feedback to guide code refinement. Embedded within a generator-discriminator framework, these mechanisms shift validation responsibilities from users to the system. Evaluations on three benchmark datasets, Spider, Bird, and GSM8K, demonstrate that both Q* and Feedback+ reduce error rates and task completion time. The study also identifies reverse translation as a key bottleneck, highlighting opportunities for future improvement. Overall, this work contributes a design-oriented framework for building more reliable, enterprise-grade GenAI systems capable of trustworthy decision support.

</details>


### [77] [Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation](https://arxiv.org/abs/2601.00263)
*Qianli Wang,Van Bach Nguyen,Yihong Liu,Fedor Splitt,Nils Feldhus,Christin Seifert,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 该论文系统研究了大模型在多语言生成反事实（counterfactuals）方面的能力，揭示了直接生成与英译生成的优劣，并分析了生成反事实的常见错误类型。


<details>
  <summary>Details</summary>
Motivation: 反事实可以用于解释模型行为，但目前大多数研究聚焦于英语。因此，论文动机是探究大模型在多语言反事实生成方面的表现及其局限性。

Method: 作者对六种语言下，直接生成与基于英文翻译的多语言反事实进行了自动评估。比较其有效率、编辑幅度等表现，并分析了高资源语言中的编辑模式和跨语言常见错误类型。同时，还实验了多语言反事实数据扩增（CDA）对模型性能的影响。

Result: 基于翻译的反事实相比直接生成者有效性更高，但需要更多修改，且质量仍不及原英文；高资源欧洲语言的编辑规则相似；总结了反事实常见错误类型。多语言CDA比跨语言CDA对低资源语言模型提升更大，但受反事实生成质量限制。

Conclusion: 大模型能生成多语种反事实，但与英文相比质量尚有不足，常见错误影响数据增强效果，提升多语言反事实生成质量是未来改进方向。

Abstract: Counterfactuals refer to minimally edited inputs that cause a model's prediction to change, serving as a promising approach to explaining the model's behavior. Large language models (LLMs) excel at generating English counterfactuals and demonstrate multilingual proficiency. However, their effectiveness in generating multilingual counterfactuals remains unclear. To this end, we conduct a comprehensive study on multilingual counterfactuals. We first conduct automatic evaluations on both directly generated counterfactuals in the target languages and those derived via English translation across six languages. Although translation-based counterfactuals offer higher validity than their directly generated counterparts, they demand substantially more modifications and still fall short of matching the quality of the original English counterfactuals. Second, we find the patterns of edits applied to high-resource European-language counterfactuals to be remarkably similar, suggesting that cross-lingual perturbations follow common strategic principles. Third, we identify and categorize four main types of errors that consistently appear in the generated counterfactuals across languages. Finally, we reveal that multilingual counterfactual data augmentation (CDA) yields larger model performance improvements than cross-lingual CDA, especially for lower-resource languages. Yet, the imperfections of the generated counterfactuals limit gains in model performance and robustness.

</details>


### [78] [Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity](https://arxiv.org/abs/2601.00268)
*Doyoung Kim,Zhiwei Ren,Jie Hao,Zhongkai Sun,Lichao Wang,Xiyao Ma,Zack Ye,Xu Han,Jun Yin,Heng Ji,Wei Shen,Xing Fan,Benjamin Yao,Chenlei Guo*

Main category: cs.CL

TL;DR: WildAGTEval是一个用于评测大语言模型（LLM）在真实复杂API环境下调用能力的新型基准，发现在多种挑战场景下目前LLM表现仍有限。


<details>
  <summary>Details</summary>
Motivation: 现有LLM函数调用能力评测多基于理想化API，忽略了实际应用中常见的API复杂性和噪声输出，导致评测结果与真实应用不符。因此急需一个更贴合现实的复杂性测试基准。

Method: WildAGTEval基准从两个维度引入真实复杂性：1）API规格，包括详细文档和使用约束，2）API执行，关注运行时问题。基准包含60类复杂场景，可组合成约32K种测试配置，并设计了用户-智能体交互以验证LLM在这些场景下的实际表现。

Result: 采用WildAGTEval系统性测试了多款先进LLM，发现绝大多数场景对模型都有显著挑战，尤其是无关信息复杂性可使强大LLM性能下降27.3%。定性分析进一步显示，LLM有时会扭曲用户意图只为完成任务，严重影响用户满意度。

Conclusion: WildAGTEval揭示了LLM在仿真真实API复杂性时的不足，现有模型在多种API场景下表现有限，新基准为未来LLM研究和改进提供了重要参考。

Abstract: We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.

</details>


### [79] [Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations](https://arxiv.org/abs/2601.00282)
*Qianli Wang,Nils Feldhus,Pepa Atanasova,Fedor Splitt,Simon Ostermann,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 本文研究了大模型量化对自我解释能力的影响，发现量化会使自我解释的质量和可信度有所下降，但整体影响较小，量化仍然是有效的模型压缩方法。


<details>
  <summary>Details</summary>
Motivation: 量化常用于加速和简化大语言模型（LLM）的部署，但其对模型自我解释能力的影响尚未被深入研究。随着自我解释能力在高风险场合被用来增强模型透明度，了解量化对其影响变得尤为重要。

Method: 作者研究了三种常见量化技术与不同比特宽度下LLM生成的两种自我解释方式（自然语言解释和反事实样本），并通过用户实验评估了量化对解释的质量与可信度的影响。

Result: 结果显示，量化会造成自我解释的质量最多下降4.4%，可信度最多下降2.38%。用户研究发现，量化会使自我解释的连贯性和可信度下降（最高达8.5%）。大模型在自我解释质量上对量化抵抗力有限，但在可信度上表现更好。不同量化技术在各项指标上无一优于其他。

Conclusion: 尽管量化对自我解释能力有一定负面影响，但降幅较小，不影响量化作为压缩手段的有效性。建议实际部署中针对具体任务验证自我解释质量，尤其是自然语言解释。

Abstract: Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\%) and faithfulness (up to 2.38\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.

</details>


### [80] [DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection](https://arxiv.org/abs/2601.00303)
*Yuxin Li,Xiangyu Zhang,Yifei Li,Zhiwei Guo,Haoyang Zhang,Eng Siong Chng,Cuntai Guan*

Main category: cs.CL

TL;DR: 提出了DepFlow系统，通过分离语义内容和心理健康状态，提升抑郁症检测模型对掩饰性（Camouflaged）抑郁的鲁棒性，增强对真实世界场景的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症语音数据集在语言情感和诊断标签之间具有高度耦合性，导致模型倾向于学习语义捷径，无法有效处理现实中表面情绪积极但实际有抑郁风险的掩饰性抑郁个案。需要减少这种语义偏见，提升模型对非语义表征的抑郁检测能力。

Method: 提出DepFlow三阶段系统：1）使用对抗训练，获得与说话人及内容无关的抑郁表征；2）通过流匹配的TTS模型（带FiLM调制）将此表征融入语音合成，实现对抑郁程度的可控调节且保留原说话人和内容；3）用原型机制实现抑郁程度的平滑解释和操作。基于DepFlow生成包含语音-文本情感不一致的新型CDoA数据集，用于掩饰性抑郁检测建模。

Result: DepFlow生成的CDoA数据集在三种主流抑郁检测体系上，宏F1分别提升9%、12%、5%，并超过传统数据增强方法。该方法提升了模型在抑郁检测中的鲁棒性。

Conclusion: DepFlow为抑郁检测提供了抗语义伪装干扰的鲁棒解决方案，并为对话系统和缺乏真实临床数据的仿真评测提供了可控合成平台。

Abstract: Speech is a scalable and non-invasive biomarker for early mental health screening. However, widely used depression datasets like DAIC-WOZ exhibit strong coupling between linguistic sentiment and diagnostic labels, encouraging models to learn semantic shortcuts. As a result, model robustness may be compromised in real-world scenarios, such as Camouflaged Depression, where individuals maintain socially positive or neutral language despite underlying depressive states. To mitigate this semantic bias, we propose DepFlow, a three-stage depression-conditioned text-to-speech framework. First, a Depression Acoustic Encoder learns speaker- and content-invariant depression embeddings through adversarial training, achieving effective disentanglement while preserving depression discriminability (ROC-AUC: 0.693). Second, a flow-matching TTS model with FiLM modulation injects these embeddings into synthesis, enabling control over depressive severity while preserving content and speaker identity. Third, a prototype-based severity mapping mechanism provides smooth and interpretable manipulation across the depression continuum. Using DepFlow, we construct a Camouflage Depression-oriented Augmentation (CDoA) dataset that pairs depressed acoustic patterns with positive/neutral content from a sentiment-stratified text bank, creating acoustic-semantic mismatches underrepresented in natural data. Evaluated across three depression detection architectures, CDoA improves macro-F1 by 9%, 12%, and 5%, respectively, consistently outperforming conventional augmentation strategies in depression Detection. Beyond enhancing robustness, DepFlow provides a controllable synthesis platform for conversational systems and simulation-based evaluation, where real clinical data remains limited by ethical and coverage constraints.

</details>


### [81] [Robust Uncertainty Quantification for Factual Generation of Large Language Models](https://arxiv.org/abs/2601.00348)
*Yuhao Zhang,Zhongliang Yang,Linna Zhou*

Main category: cs.CL

TL;DR: 该论文提出了一种针对大语言模型幻觉问题的新型不确定性量化方法，并通过设计“陷阱问题”来验证其有效性。实验显示新方法优于现有基线方法，有助于提高LLM应对复杂与混淆问题的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各领域应用广泛，但幻觉现象仍严重影响其可靠性，尤其是在面对非标准或对抗性提问时，现有不确定性量化方法效果不佳。研究动机是弥补这一方法与实际应用间的性能差距。

Method: 研究构建了一组带有虚假姓名的陷阱问题，并提出一种新颖且鲁棒的不确定性量化方法（RU），系统地评估LLM在多事实生成任务下的幻觉识别能力。

Result: 实验显示，所构建的陷阱问题在评测LLM幻觉识别方面表现优异。与四种主流模型上的基线方法比较，所提RU方法在ROCAUC分值上平均提升0.1-0.2。

Conclusion: 本研究提出的方法在应对LLM幻觉问题上取得了显著进展，为后续幻觉检测与缓解方法提供了新思路，有助于提升LLM在真实应用场景下的可靠性与可信度。

Abstract: The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.

</details>


### [82] [The Role of Mixed-Language Documents for Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2601.00364)
*Jiandong Shao,Raphael Tang,Crystina Zhang,Karin Sevegnani,Pontus Stenetorp,Jianfei Yang,Yao Lu*

Main category: cs.CL

TL;DR: 本论文探讨了多语种大模型预训练语料中双语数据对跨语言能力的影响，并通过对比实验量化其作用，发现翻译能力高度依赖平行语料，但跨语言理解与推理能力对双语数据依赖较小。


<details>
  <summary>Details</summary>
Motivation: 虽然多语种大模型展示了跨语言的强大能力，但其预训练语料多数为单语文档。一直有观点认为，语料中的双语数据促进了跨语言能力，但具体机制和细节尚不清楚，因此作者希望通过严格对照实验明确双语数据的实际贡献。

Method: 作者从零开始预训练模型，在标准多语种语料（包含双语文档）与纯单语语料（剔除所有多语种文档）下分别建模，重点关注翻译、跨语言问答与推理等任务。同时对双语文档进行精细分类（平行、语码转换、其他），并以不同类别有控制地回补至单语语料，评估其针对不同任务的影响。

Result: 移除双语数据后，翻译任务BLEU分数下降56%，但跨语言问答和推理表现基本稳定。进一步发现，仅保留平行文本便可恢复91%未筛选语料下的翻译表现，语码转换数据影响极小。

Conclusion: 双语平行数据对翻译能力至关重要，因其提供系统性的词级对齐信息；而跨语言理解与推理能力则不依赖双语数据，说明这些能力主要源自更通用的语义建模。

Abstract: Multilingual large language models achieve impressive cross-lingual performance despite largely monolingual pretraining. While bilingual data in pretraining corpora is widely believed to enable these abilities, details of its contributions remain unclear. We investigate this question by pretraining models from scratch under controlled conditions, comparing the standard web corpus with a monolingual-only version that removes all multilingual documents. Despite constituting only 2% of the corpus, removing bilingual data causes translation performance to drop 56% in BLEU, while behaviour on cross-lingual QA and general reasoning tasks remains stable, with training curves largely overlapping the baseline. To understand this asymmetry, we categorize bilingual data into parallel (14%), code-switching (72%), and miscellaneous documents (14%) based on the semantic relevance of content in different languages. We then conduct granular ablations by reintroducing parallel or code-switching data into the monolingual-only corpus. Our experiments reveal that parallel data almost fully restores translation performance (91% of the unfiltered baseline), whereas code-switching contributes minimally. Other cross-lingual tasks remain largely unaffected by either type. These findings reveal that translation critically depends on systematic token-level alignments from parallel data, whereas cross-lingual understanding and reasoning appear to be achievable even without bilingual data.

</details>


### [83] [BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics](https://arxiv.org/abs/2601.00366)
*Taj Gillin,Adam Lalani,Kenneth Zhang,Marcel Mateos Salles*

Main category: cs.CL

TL;DR: 本文提出了一种新的自监督训练方法BEPA，通过在BERT模型中引入JEPA目标，使[CLS]嵌入空间更具普适性，并提升了多语言任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有BERT模型在多语言下容易出现[CLS]向量空间坍缩，限制了模型的泛化能力。该研究希望通过新的训练目标来构建一个语言无关的表征空间，提升模型多语言任务性能。

Method: 将Joint Embedding Predictive Architectures（JEPA）训练目标加入到BERT类模型的训练流程中，通过自监督方式引导模型学习更有区分性的[CLS]嵌入空间，从而增强模型对不同语言的泛化表示能力。

Result: 采用BEPA训练范式后，BERT模型在多语言基准测试中表现出明显提升，说明该方法能有效增强模型的语言无关性和整体性能。

Conclusion: BEPA方法能够缓解[CLS]嵌入空间坍缩问题，并显著提升BERT模型在多语言任务中的表现，展示了JEPA目标在自监督学习和多语言 NLP 中的应用潜力。

Abstract: Joint Embedding Predictive Architectures (JEPA) are a novel self supervised training technique that have shown recent promise across domains. We introduce BERT-JEPA (BEPA), a training paradigm that adds a JEPA training objective to BERT-style models, working to combat a collapsed [CLS] embedding space and turning it into a language-agnostic space. This new structure leads to increased performance across multilingual benchmarks.

</details>


### [84] [Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach](https://arxiv.org/abs/2601.00388)
*Biao Wu,Meng Fang,Ling Chen,Ke Xu,Tao Cheng,Jun Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于视觉-语言模型的图像地理定位方法Geo-R，其无需外部检索，可通过结构化推理路径及坐标对齐强化学习提升定位精度及解释性。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉-语言推理的图像定位方法往往依赖合成推理标签或外部检索，造成解释性和泛化性有限，因此需要一种无需外部检索、可解释性更强的定位方案。

Method: 1. Geo-R框架：利用ground-truth坐标生成结构化推理路径（例如国家—省份—城市），不依靠模型生成或合成标签。2. 引入“Chain of Region”分层推理范式，形成层级监督信号。3. 构建轻量级强化学习方案，使用Haversine距离为奖励，空间坐标对齐，指导模型优化预测。

Result: 在多个基准数据集上，Geo-R在定位精度、泛化能力和推理透明度方面均优于现有方法，验证了其有效性。

Conclusion: Geo-R方法实现了无需外部检索的可扩展、可解释的图像地理定位新范式，实验表明其效果显著，并公开模型及代码促进相关研究。

Abstract: Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.

</details>


### [85] [Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset](https://arxiv.org/abs/2601.00411)
*Alistair Plum,Laura Bernardy,Tharindu Ranasinghe*

Main category: cs.CL

TL;DR: 本论文提出了judgeWEL，这是一个为卢森堡语命名实体识别（NER）构建的数据集，采用自动标注和大语言模型验证，并建立了新的数据构建流程。该数据集是现有卢森堡语领域最大、最全面的NER资源。


<details>
  <summary>Details</summary>
Motivation: 低资源语言在自然语言处理领域中由于资源匮乏和语言特殊性，数据集构建昂贵且难以保证一致性，限制了相关研究的发展。卢森堡语作为低资源语言，特别缺乏高质量NER数据集。

Method: 作者提出了一种新颖的数据构建流程：首先利用维基百科和维基数据作为结构化弱监督来源，通过维基百科文章的内链推断实体类型，最小化人工干预生成初步标注。随后，采用多种大语言模型（LLM）对自动标注样本进行筛选和噪声抑制，保留高质量数据。最后形成扩充且平衡的NER数据集。

Result: 新构建的judgeWEL数据集大约是当前卢森堡语NER数据集的5倍，并且在实体类别的覆盖面和均衡性上有显著提升，成为同类研究中最大规模的数据集之一。

Conclusion: judgeWEL为多语言及低资源语言NER任务的研究者提供了高质量、规模更大的数据资源，有望促进卢森堡语及相关领域的进一步发展。

Abstract: We present judgeWEL, a dataset for named entity recognition (NER) in Luxembourgish, automatically labelled and subsequently verified using large language models (LLM) in a novel pipeline. Building datasets for under-represented languages remains one of the major bottlenecks in natural language processing, where the scarcity of resources and linguistic particularities make large-scale annotation costly and potentially inconsistent. To address these challenges, we propose and evaluate a novel approach that leverages Wikipedia and Wikidata as structured sources of weak supervision. By exploiting internal links within Wikipedia articles, we infer entity types based on their corresponding Wikidata entries, thereby generating initial annotations with minimal human intervention. Because such links are not uniformly reliable, we mitigate noise by employing and comparing several LLMs to identify and retain only high-quality labelled sentences. The resulting corpus is approximately five times larger than the currently available Luxembourgish NER dataset and offers broader and more balanced coverage across entity categories, providing a substantial new resource for multilingual and low-resource NER research.

</details>


### [86] [Toward Better Temporal Structures for Geopolitical Events Forecasting](https://arxiv.org/abs/2601.00430)
*Kian Ahrabian,Eric Boxer,Jay Pujara*

Main category: cs.CL

TL;DR: 本文提出了一个新的超关系时序知识超图（HTKGHs）结构，用于更好地表达真实世界地缘政治事件中复杂的时序关系，并基于新结构构建了数据集，对主流大语言模型（LLM）在关联预测任务中的表现进行了分析。


<details>
  <summary>Details</summary>
Motivation: 当前时序知识图谱（TKG）及其超关系扩展（HTKG）在表示简单时序关系时较为直接，但对更复杂、多实体的地缘政治事实表达能力有限，妨碍现实事件的准确建模与预测。

Method: 作者提出了一种HTKG的泛化模型——HTKGH（超关系时序知识泛化超图），可支持两个以上主要实体的复杂事实，并载明其与HTKG的兼容性；据此标准，构建了HTKGH-POLECAT数据集，并设计了基准实验对主流LLM在新任务上的适应性做出评测。

Result: 作者构建了一个新型数据集，并在此基础上评估了流行大语言模型在关系预测任务中的性能，揭示了LLMs在复杂预测场景下的适应性与能力。

Conclusion: HTKGH提升了真实世界复杂事件的表达和建模能力，在地缘政治领域预测任务中，大语言模型具备一定适应性，对未来相关任务和模型设计提供了有价值的参考。

Abstract: Forecasting on geopolitical temporal knowledge graphs (TKGs) through the lens of large language models (LLMs) has recently gained traction. While TKGs and their generalization, hyper-relational temporal knowledge graphs (HTKGs), offer a straightforward structure to represent simple temporal relationships, they lack the expressive power to convey complex facts efficiently. One of the critical limitations of HTKGs is a lack of support for more than two primary entities in temporal facts, which commonly occur in real-world events. To address this limitation, in this work, we study a generalization of HTKGs, Hyper-Relational Temporal Knowledge Generalized Hypergraphs (HTKGHs). We first derive a formalization for HTKGHs, demonstrating their backward compatibility while supporting two complex types of facts commonly found in geopolitical incidents. Then, utilizing this formalization, we introduce the htkgh-polecat dataset, built upon the global event database POLECAT. Finally, we benchmark and analyze popular LLMs on the relation prediction task, providing insights into their adaptability and capabilities in complex forecasting scenarios.

</details>


### [87] [Comparative Efficiency Analysis of Lightweight Transformer Models: A Multi-Domain Empirical Benchmark for Enterprise NLP Deployment](https://arxiv.org/abs/2601.00444)
*Muhammad Shahmeer Khan*

Main category: cs.CL

TL;DR: 本文比较了三种轻量级Transformer模型（DistilBERT、MiniLM、ALBERT）在多个领域文本自动化任务中的表现，揭示了准确性与效率之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 企业NLP应用需要既高效又轻量化，能够在多个领域任务中快速处理文本，因此亟需针对主流轻量Transformer模型的系统性分析，帮助企业选择最适合其需求的模型。

Method: 作者选取了IMDB、AG News和Measuring Hate Speech数据集，分别代表情感分类、新闻主题分类和仇恨言论检测。实验以精度、召回率、F1分数等准确性指标，以及模型大小、推理时间、吞吐量和内存使用等效率指标，对DistilBERT、MiniLM和ALBERT进行了固定参数下的对比分析。

Result: 结果表明，三种模型没有唯一全面领先者。ALBERT在多个领域获取最高任务准确率，MiniLM在推理速度和吞吐量上最优，DistilBERT表现最均衡且效率亦具竞争力。

Conclusion: 准确性与效率存在权衡。推荐MiniLM用于延迟敏感应用，DistilBERT适合需要性能平衡的场景，ALBERT则适于资源受限环境。

Abstract: In the rapidly evolving landscape of enterprise natural language processing (NLP), the demand for efficient, lightweight models capable of handling multi-domain text automation tasks has intensified. This study conducts a comparative analysis of three prominent lightweight Transformer models - DistilBERT, MiniLM, and ALBERT - across three distinct domains: customer sentiment classification, news topic classification, and toxicity and hate speech detection. Utilizing datasets from IMDB, AG News, and the Measuring Hate Speech corpus, we evaluated performance using accuracy-based metrics including accuracy, precision, recall, and F1-score, as well as efficiency metrics such as model size, inference time, throughput, and memory usage. Key findings reveal that no single model dominates all performance dimensions. ALBERT achieves the highest task-specific accuracy in multiple domains, MiniLM excels in inference speed and throughput, and DistilBERT demonstrates the most consistent accuracy across tasks while maintaining competitive efficiency. All results reflect controlled fine-tuning under fixed enterprise-oriented constraints rather than exhaustive hyperparameter optimization. These results highlight trade-offs between accuracy and efficiency, recommending MiniLM for latency-sensitive enterprise applications, DistilBERT for balanced performance, and ALBERT for resource-constrained environments.

</details>


### [88] [Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games](https://arxiv.org/abs/2601.00448)
*Dimitris Vartziotis*

Main category: cs.CL

TL;DR: 本文将语言哲学中的两种主流理论——社会建构主义与数学化的语义场理论（Semantic Field Theory）——在大语言模型（LLM）背景下进行了对比，提出二者可互补理解。


<details>
  <summary>Details</summary>
Motivation: 作者希望借助LLM这一全新的实证场景，检验并融合经典语言理论，从而更好解释LLM在语义、语用等能力上的表现及局限。

Method: 作者形式化了词汇场（Lexfelder）和语言场（Lingofelder）概念，并分析了transformer模型（如分布式表示、注意力机制、嵌入空间几何结构等）与这些语义结构的关系。

Result: 研究发现，LLM在捕捉语义规则性上表现良好，表明语言具有某种数学结构，但在语用推理和情境敏感性上仍有不足，这验证了语言使用需社会基础的哲学观点。

Conclusion: 作者认为，数学结构和社会语言游戏视角应互为补充。该框架揭示了统计模型的能力边界，并为AI语言架构的理论驱动改进提供了新方向。

Abstract: Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.

</details>


### [89] [Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations](https://arxiv.org/abs/2601.00454)
*Hyunjun Kim*

Main category: cs.CL

TL;DR: 防护模型用于提升大语言模型（LLM）部署的安全性，但处理完整多轮对话的计算成本高昂。作者提出一种名为Defensive M2S的训练范式，通过将多轮对话压缩为单轮对话训练防护模型，大幅降低训练与推理开销，同时还能提升攻击检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有防护模型在处理多轮对话时需要遍历完整历史，带来指数级增长的计算成本，阻碍了高效部署和大规模应用。

Method: 提出Multi-turn到Single-turn（M2S）对话压缩技术，将多轮对话内容浓缩为一轮对话摘要，针对这种摘要微调防护模型。分别采用三种摘要模板（hyphenize、numberize、pythonize），并对三类防护模型进行了全方面评测。

Result: M2S方法在平均10.6轮对话的训练集上，将训练token数从1570万锐减到16.9万，节省93倍存储与计算资源。推理时token也节省94.6%。最佳配置（Qwen3Guard+hyphenize）在检测攻击时召回率达93.8%，比原始方法提升38.9个百分点。

Conclusion: M2S对话压缩是一种高效的安全防护模型训练与推理技术，能显著降低多轮对话处理成本，为大规模部署高效安全筛查提供了可行路径。

Abstract: Guardrail models are essential for ensuring the safety of Large Language Model (LLM) deployments, but processing full multi-turn conversation histories incurs significant computational cost. We propose Defensive M2S, a training paradigm that fine-tunes guardrail models on Multi-turn to Single-turn (M2S) compressed conversations rather than complete dialogue histories. We provide a formal complexity analysis showing that M2S reduces training cost from $O(n^2)$ to $O(n)$ for $n$-turn conversations. Empirically, on our training dataset (779 samples, avg. 10.6 turns), M2S requires only 169K tokens compared to 15.7M tokens for the multi-turn baseline -- a 93$\times$ reduction. We evaluate Defensive M2S across three guardrail model families (LlamaGuard, Nemotron, Qwen3Guard) and three compression templates (hyphenize, numberize, pythonize) on SafeDialBench, a comprehensive multi-turn jailbreak benchmark. Our best configuration, Qwen3Guard with hyphenize compression, achieves 93.8% attack detection recall while reducing inference tokens by 94.6% (from 3,231 to 173 tokens per conversation). This represents a 38.9 percentage point improvement over the baseline while dramatically reducing both training and inference costs. Our findings demonstrate that M2S compression can serve as an effective efficiency technique for guardrail deployment, enabling scalable safety screening of long multi-turn conversations.

</details>


### [90] [Noise-Aware Named Entity Recognition for Historical VET Documents](https://arxiv.org/abs/2601.00488)
*Alexander M. Esser,Jens Dörpinghaus*

Main category: cs.CL

TL;DR: 本文提出了一种针对职业教育和培训（VET）领域历史数字文献的鲁棒命名实体识别（NER）方法，有效应对OCR噪声问题，通过噪声感知训练和多阶段微调，有效提升了NER性能。


<details>
  <summary>Details</summary>
Motivation: 现有NER系统多针对清洁文本，真实历史VET文献常含有OCR噪声，导致命名实体识别性能下降。亟需研发兼具鲁棒性与高准确性的NER方法解决此挑战。

Method: 方法包括三部分：1）设计噪音感知训练（NAT），在数据中合成加入OCR错误；2）采用迁移学习与多阶段微调，分别在干净、噪声和人工数据上系统对比；3）首次支持多实体类型识别，采用可在任意语言迁移的方案。

Result: 实验结果表明，在特定领域和噪声感知微调下，NER的鲁棒性和准确性均有显著提升。与传统方法相比，面对噪声文献时表现更优。相关代码已公开，有利于推动领域复现与应用。

Conclusion: 面向VET文献的噪声感知NER方法不仅提升了对多类实体的准确识别能力，也具备良好的跨语言迁移性，适用于其他带有噪声的专业文本场景。

Abstract: This paper addresses Named Entity Recognition (NER) in the domain of Vocational Education and Training (VET), focusing on historical, digitized documents that suffer from OCR-induced noise. We propose a robust NER approach leveraging Noise-Aware Training (NAT) with synthetically injected OCR errors, transfer learning, and multi-stage fine-tuning. Three complementary strategies, training on noisy, clean, and artificial data, are systematically compared. Our method is one of the first to recognize multiple entity types in VET documents. It is applied to German documents but transferable to arbitrary languages. Experimental results demonstrate that domain-specific and noise-aware fine-tuning substantially increases robustness and accuracy under noisy conditions. We provide publicly available code for reproducible noise-aware NER in domain-specific contexts.

</details>


### [91] [Rule-Based Approaches to Atomic Sentence Extraction](https://arxiv.org/abs/2601.00506)
*Lineesha Kamana,Akshita Ananda Subramanian,Mehuli Ghosh,Suman Saha*

Main category: cs.CL

TL;DR: 本文分析了将复杂句分解为原子句（表达单一思想的简单句）的难点，并用依存句法规则进行了实际评测。


<details>
  <summary>Details</summary>
Motivation: 以往的机器学习分句方法虽然提升了分解准确性，但对出错的语法结构缺乏解释性分析。因此，需系统性研究哪些具体的句法结构影响分句提取的难度。

Method: 作者基于WikiSplit数据集，使用spaCy实现依存句法规则，生成100组金标准的原子句，并用ROUGE和BERTScore评估分句系统在不同复杂结构下的表现。

Result: 该系统在ROUGE和BERTScore评测下展现了中高水平的性能，但遇到如关系从句、同位结构、谓语并列、状语从句和被动句时效果下降。

Conclusion: 基于规则的原子句抽取方法总体准确，但对语法复杂性较为敏感，特定结构尤其难以正确分解。

Abstract: Natural language often combines multiple ideas into complex sentences. Atomic sentence extraction, the task of decomposing complex sentences into simpler sentences that each express a single idea, improves performance in information retrieval, question answering, and automated reasoning systems. Previous work has formalized the "split-and-rephrase" task and established evaluation metrics, and machine learning approaches using large language models have improved extraction accuracy. However, these methods lack interpretability and provide limited insight into which linguistic structures cause extraction failures. Although some studies have explored dependency-based extraction of subject-verb-object triples and clauses, no principled analysis has examined which specific clause structures and dependencies lead to extraction difficulties. This study addresses this gap by analyzing how complex sentence structures, including relative clauses, adverbial clauses, coordination patterns, and passive constructions, affect the performance of rule-based atomic sentence extraction. Using the WikiSplit dataset, we implemented dependency-based extraction rules in spaCy, generated 100 gold=standard atomic sentence sets, and evaluated performance using ROUGE and BERTScore. The system achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898, indicating moderate-to-high lexical, structural, and semantic alignment. Challenging structures included relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions. Overall, rule-based extraction is reasonably accurate but sensitive to syntactic complexity.

</details>


### [92] [Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends](https://arxiv.org/abs/2601.00536)
*Yuelyu Ji,Zhuochun Li,Rui Meng,Daqing He*

Main category: cs.CL

TL;DR: 本文提出了一个分析多跳问答系统执行过程的四轴框架，并用其对主流系统进行梳理与对比，揭示了多跳问答系统在准确性、效率与证据忠实性三者之间常见的权衡，并指出了未来的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的多跳问答系统（如RAG和agentic方法）尽管表现良好，但其证据检索-推理的具体过程通常未明晰描述，这导致模型之间的过程性设计难以直接对比。该文旨在明确分析这些过程设计，帮助研究者更系统地理解和改进多跳问答系统。

Method: 作者提出四轴分析框架：（A）整体执行计划、（B）索引结构、（C）下一步控制（策略与触发机制）、（D）停止/继续准则。基于此，对代表性多跳问答系统进行梳理、对照主流数据集上的消融实验和趋势进行总结分析。

Result: 通过该框架梳理多种主流多跳问答系统，并在如HotpotQA、2WikiMultiHopQA、MuSiQue等基准数据集上，总结了它们在效果、效率、证据忠实性方面的典型权衡与实验结果。

Conclusion: 作者指出，多跳问答系统在检索-推理一体化的执行过程中，需在准确性、效率和证据忠实性之间权衡。今后的挑战包括结构化规划、可迁移的控制策略和在分布转移情形下的稳健停止机制。

Abstract: Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.

</details>


### [93] [ECR: Manifold-Guided Semantic Cues for Compact Language Models](https://arxiv.org/abs/2601.00543)
*Chung-Wei Victor Yuan*

Main category: cs.CL

TL;DR: 提出Embedding Consistency Regulation（ECR）框架，通过语义锚点保持紧凑模型嵌入空间的一致性，从而提升多语言情况下压缩模型的表现和可用性。


<details>
  <summary>Details</summary>
Motivation: 紧凑模型在容量有限或多语言数据下常丧失嵌入空间结构，导致下游任务难以利用压缩后的表征。现有压缩方法只对齐输出，未能保持深层结构，造成语义漂移及任务能力下降。

Method: ECR框架从教师模型嵌入中离线获取语义锚点，紧凑模型训练时保持在锚点周围的几何一致性；推理时仅需小型投影步骤，无需修改模型架构或推理过程，也无需对齐logits或内部特征。ECR可独立于蒸馏进行。

Result: 在10万规模多语料实验中，ECR稳定训练过程，在不同任务和语言下均保持语义结构，为低容量模型打造更清晰的表征空间，显著优于传统基线。

Conclusion: ECR框架可不依赖教师输出，提升紧凑模型的表现，使任务对齐和模型部署更高效，适用于对效率和隐私有严格要求的场景。

Abstract: Compact models often lose the structure of their embedding space. The issue shows up when the capacity is tight or the data spans several languages. Such collapse makes it difficult for downstream tasks to build on the resulting representation. Existing compression methods focus on aligning model outputs at a superficial level but fail to preserve the underlying manifold structure. This mismatch often leads to semantic drift in the compact model, causing both task behavior and linguistic properties to deviate from the reference model.
  To address those issues, we provide a new framework called Embedding Consistency Regulation (ECR). This framework first derives a set of semantic anchors from teacher embeddings (computed once offline). Then, the compact model learns to maintain consistent geometry around these anchors, without relying on matching logits or internal features. ECR adds only a small projection step at inference, without altering the decoding architecture or its runtime behavior.
  In experiments on a 100K multilingual corpus, ECR consistently stabilizes training and preserves semantic structure across tasks and languages. It also produces a more compact and task-aligned representation space, enabling low-capacity models to learn cleaner manifolds than conventional baselines. ECR works without teacher outputs and is compatible with, but independent of, distillation. Taken together, our results show that ECR helps compact models better follow task requirements and makes them easier to deploy under strict efficiency or privacy limits.

</details>


### [94] [A Language-Agnostic Hierarchical LoRA-MoE Architecture for CTC-based Multilingual ASR](https://arxiv.org/abs/2601.00557)
*Yuang Zheng,Yuxiang Mei,Dongxing Xu,Jie Chen,Yanhua Long*

Main category: cs.CL

TL;DR: 本文提出了一种轻量、真正语言无关的多语言ASR系统HLoRA，用于提升边缘设备上的语音识别效率，同时保持竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模多语言ASR模型（如Whisper）虽然性能强大，但计算和延迟成本很高，不适合资源受限的边缘设备。需要一种既高效又具备多语言适应能力的轻量模型。

Method: 提出了一种基于CTC结构和领域自适应的HLoRA框架，将语言无关的层次化LoRA门控专家结构融入mHuBERT-CTC模型。系统不依赖先验语言身份信息，采用LID后验驱动的路由进行端到端多语言解码。层次化设计包含多语言共享LoRA和语言特定LoRA专家，分别用来建模语言无关和语言相关特征。

Result: 在MSR-86K和MLC-SLM 2025 Challenge数据集上的实验显示，HLoRA只需单次解码即可达到与最新双阶段推理方法相当的识别性能，极大提升了解码效率，尤其适合低资源场景下的多语言ASR任务。

Conclusion: HLoRA实现了无需语言先验信息的真实语言无关多语言ASR，能够以明显更优的推理效率，为边缘应用带来高效且实用的多语言语音识别解决方案。

Abstract: Large-scale multilingual ASR (mASR) models such as Whisper achieve strong performance but incur high computational and latency costs, limiting their deployment on resource-constrained edge devices. In this study, we propose a lightweight and language-agnostic multilingual ASR system based on a CTC architecture with domain adaptation. Specifically, we introduce a Language-agnostic Hierarchical LoRA-MoE (HLoRA) framework integrated into an mHuBERT-CTC model, enabling end-to-end decoding via LID-posterior-driven LoRA routing. The hierarchical design consists of a multilingual shared LoRA for learning language-invariant acoustic representations and language-specific LoRA experts for modeling language-dependent characteristics. The proposed routing mechanism removes the need for prior language identity information or explicit language labels during inference, achieving true language-agnostic decoding. Experiments on MSR-86K and the MLC-SLM 2025 Challenge datasets demonstrate that HLoRA achieves competitive performance with state-of-the-art two-stage inference methods using only single-pass decoding, significantly improving decoding efficiency for low-resource mASR applications.

</details>


### [95] [InfoSynth: Information-Guided Benchmark Synthesis for LLMs](https://arxiv.org/abs/2601.00575)
*Ishir Garg,Neel Kolhe,Xuandong Zhao,Dawn Song*

Main category: cs.CL

TL;DR: 本文提出了InfoSynth框架，能够自动生成并评估针对大语言模型（LLM）推理能力和代码生成能力的新型基准测试，有效提升基准测试的多样性和新颖性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试的构建高度依赖人工，既费时又费力，且容易与已有训练数据重叠，无法真实评估模型能力。迫切需要自动化且能生成多样化与新颖测试集的方法。

Method: 提出了InfoSynth框架，通过信息论指标（如KL散度和熵）无须大量模型测试即可度量基准的新颖性和多样性。使用种子数据集、基因算法和代码反馈，自动合成高质量的Python编程问题及其测试用例和答案，实现端到端自动生成评测集。

Result: 该方法在97%的情况下能生成准确的问题答案和测试用例，生成的新基准在新颖性和多样性方面优于原始种子集，还能根据需求控制题目的难度和多样性。

Conclusion: InfoSynth为LLM领域提供了一种高效、可扩展、自动验证的工具，能自动化生成高质量且多元的基准测试，有助于推动模型评测手段的进步。

Abstract: Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/

</details>


### [96] [CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns](https://arxiv.org/abs/2601.00588)
*Zhenhong Zhou,Shilinlu Yan,Chuanpu Liu,Qiankun Li,Kun Wang,Zhigang Zeng*

Main category: cs.CL

TL;DR: 本文提出了CSSBench基准，用于评测中文轻量级大模型在面对中文特有对抗模式下的安全性能。


<details>
  <summary>Details</summary>
Motivation: 现有大模型安全性评估主要基于英语环境，忽视了中文环境中如谐音、拼音、符号拆分等对抗方式，尤其影响轻量级模型的实际部署安全。

Method: 作者构建了涵盖六大现实场景（非法合规、隐私泄露、医疗健康虚假信息、诈骗与仇恨、成人内容、公共与政治安全）的中文安全基准CSSBench，并将查询分为多种任务类型，对多个流行轻量级LLM进行评测，包括过度拒答等安全表现。

Result: 实验表明，中文特有的对抗模式对轻量级大模型是极大挑战，经现有模型易受攻击且表现下降。

Conclusion: CSSBench为中文大模型安全性提供了全面评估工具，有助于实际中安全可靠的模型部署。

Abstract: Large language models (LLMs) are increasingly deployed in cost-sensitive and on-device scenarios, and safety guardrails have advanced mainly in English. However, real-world Chinese malicious queries typically conceal intent via homophones, pinyin, symbol-based splitting, and other Chinese-specific patterns. These Chinese-specific adversarial patterns create the safety evaluation gap that is not well captured by existing benchmarks focused on English. This gap is particularly concerning for lightweight models, which may be more vulnerable to such specific adversarial perturbations. To bridge this gap, we introduce the Chinese-Specific Safety Benchmark (CSSBench) that emphasizes these adversarial patterns and evaluates the safety of lightweight LLMs in Chinese. Our benchmark covers six domains that are common in real Chinese scenarios, including illegal activities and compliance, privacy leakage, health and medical misinformation, fraud and hate, adult content, and public and political safety, and organizes queries into multiple task types. We evaluate a set of popular lightweight LLMs and measure over-refusal behavior to assess safety-induced performance degradation. Our results show that the Chinese-specific adversarial pattern is a critical challenge for lightweight LLMs. This benchmark offers a comprehensive evaluation of LLM safety in Chinese, assisting robust deployments in practice.

</details>


### [97] [Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence](https://arxiv.org/abs/2601.00596)
*Sumanth Balaji,Piyush Mishra,Aashraya Sachdeva,Suraj Agrawal*

Main category: cs.CL

TL;DR: 该论文提出JourneyBench基准，专注于评估AI客服系统（尤其是大语言模型Agent）在遵守复杂业务政策过程中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有IVR等传统客服系统不灵活，难以处理多步骤、依赖性和突发用户行为等复杂情况。当前LLM Agent评估侧重于工具或任务完成率，忽略了策略遵循性和流程鲁棒性。因此需要更合适的基准。

Method: 提出JourneyBench基准，该基准利用图结构生成多样化、真实的客服场景，并引入User Journey Coverage Score作为衡量AI Agent策略遵循的新指标。同时，比较了静态提示Agent（SPA）和动态提示Agent（DPA，显式建模策略控制）两种设计，在多个主流LLM上进行评测。

Result: 在三大领域的703段对话测试中，动态提示Agent（DPA）在策略遵循性上显著优于静态提示Agent（SPA），甚至小型模型（如GPT-4o-mini）在DPA设计下能够超越更强大的模型（如GPT-4o）。

Conclusion: 有结构化管控的Agent能显著提升AI客服在遵守多步骤政策和复杂流程上的表现。JourneyBench作为基准，有助于推动AI客服系统进化，超越传统IVR模式。

Abstract: Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.

</details>


### [98] [Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs](https://arxiv.org/abs/2601.00641)
*Nils Rautenberg,Sven Schippkus*

Main category: cs.CL

TL;DR: 本文提出了一种简单且与模型无关的方法，可以在确定性自动化场景中显著降低大语言模型（LLM）产生幻觉（即输出与输入矛盾或忽略输入信息）的概率。该方法通过多次独立生成和集成评判机制，使得错误输出概率呈指数级下降。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理固定输入、输出正确性可明确定义的自动化任务时，容易出现与输入显著矛盾的幻觉问题。当前缺乏简单高效、具备理论保证的方法，来显著降低这类场景下LLM幻觉的概率。

Method: 作者提出了一种不依赖具体模型、无需调整模型权重、解码策略或提示词的通用框架：对同一输入在多个独立上下文窗口重复提问，通过集成评判（LLM充当裁判，支持多数票裁决），来筛选正确答案，并严格量化流程的失败概率。

Result: 在可控的信息抽取实验和合成噪声评判环境中，结果表明：随着重复问答次数和裁判（评判者）数量的增加，整体流程失败率和幻觉选择概率呈指数级下降，实验证明了理论推导的准确性。

Conclusion: 该方法为确定性、固定输入场景中的LLM流程提供了轻量化、模块化且理论基础坚实的解决方案，可以在不改模型或复杂工程手段的情况下，将幻觉概率降得极低。

Abstract: Large language models (LLMs) frequently produce contextual hallucinations, where generated content contradicts or ignores information explicitly stated in the prompt. Such errors are particularly problematic in deterministic automation workflows, where inputs are fixed and correctness is unambiguous. We introduce a simple and model-agnostic framework that provides explicit probabilistic guarantees for reducing hallucinations in this setting.
  We formalize the notion of a specific task, defined by a fixed input and a deterministic correctness criterion, and show that issuing the same prompt in independent context windows yields an exponential reduction in the probability that all model outputs are incorrect. To identify a correct answer among repeated runs, we incorporate an LLM-as-a-judge and prove that the probability that the judged pipeline fails decays at a rate determined by the judge's true- and false-positive probabilities. When the judge is imperfect, we strengthen it through majority vote over independent judge calls, obtaining ensemble-level error rates that decrease exponentially in the number of votes. This yields an explicit bound on the probability that the pipeline selects a hallucinated answer.
  Experiments on controlled extraction tasks with synthetic noisy judges match these predictions exactly: pipeline failure decreases exponentially with the number of repetitions, and hallucination-selection decreases exponentially with the number of judges in the ensemble. Together, these results provide a lightweight, modular, and theoretically grounded method for driving hallucination probabilities arbitrarily low in fixed-input LLM workflows-without modifying model weights, decoding strategies, or prompt engineering.

</details>


### [99] [Physio-DPO: Aligning Large Language Models with the Protein Energy Landscape to Eliminate Structural Hallucinations](https://arxiv.org/abs/2601.00647)
*QiWei Meng*

Main category: cs.CL

TL;DR: 提出了Physio-DPO方法，通过引入物理能量信息，有效减少大蛋白语言模型生成蛋白结构时的假象，提高了蛋白的可折叠性和稳定性，优于以往主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前蛋白语言模型在设计新蛋白时常因过度关注序列语言特征，忽视真实物理能量，导致生成的蛋白易出现结构幻觉（即序列语法合理但空间结构极不稳定）。现有对齐优化方法（如DPO）未充分利用物理能量的连续信息，表现有限，需要改进。

Method: 提出Physio-DPO，一种结合物理能量的对齐框架。其核心是引入基于能隙的目标函数，不再简单二元判断真伪，而是根据本征结构与物理扰动失配结构之间的能量差来调整优化步幅，从而更全面反映蛋白真实的物理属性。

Result: Physio-DPO在多个实验中显著优于主流基线（如SFT、PPO、标准DPO）。具体表现为自洽形变（RMSD）降至1.28 Å，可折叠性提升至92.8%。定性分析也显示该方法更好地恢复关键生物物理相互作用，如疏水核堆积及氢键网络，显著抑制结构幻觉。

Conclusion: Physio-DPO有效地将物理稳态约束融入蛋白语言模型的训练，显著提升了模型生成蛋白结构的真实性和稳定性，为生成式蛋白设计提供了更加可靠的技术路径。

Abstract: Large Protein Language Models have shown strong potential for generative protein design, yet they frequently produce structural hallucinations, generating sequences with high linguistic likelihood that fold into thermodynamically unstable conformations. Existing alignment approaches such as Direct Preference Optimization are limited in this setting, as they model preferences as binary labels and ignore the continuous structure of the physical energy landscape. We propose Physio-DPO, a physics informed alignment framework that grounds protein language models in thermodynamic stability. Physio-DPO introduces a magnitude aware objective that scales optimization updates according to the energy gap between native structures and physics perturbed hard negatives. Experiments show that Physio-DPO consistently outperforms strong baselines including SFT, PPO, and standard DPO, reducing self consistency RMSD to 1.28 Å and increasing foldability to 92.8%. Qualitative analysis further demonstrates that Physio-DPO effectively mitigates structural hallucinations by recovering biophysical interactions such as hydrophobic core packing and hydrogen bond networks.

</details>


### [100] [Fast-weight Product Key Memory](https://arxiv.org/abs/2601.00671)
*Tianyu Zhao,Llion Jones*

Main category: cs.CL

TL;DR: 本文提出了Fast-weight Product Key Memory (FwPKM)架构，实现了高效且动态的序列记忆，从而在容量和效率之间取得平衡，并在长文本任务上有明显优势。


<details>
  <summary>Details</summary>
Motivation: 现有的序列建模方法在存储容量与计算效率上存在权衡。Softmax注意力具有无限存储但计算量大，而线性变体计算高效但存储容量固定，限制了模型处理长文本信息。

Method: FwPKM通过将稀疏产品键存储（PKM）从静态模块转变为动态‘快速权重’记忆，采用局部块级梯度下降，在训练和推理过程中动态更新参数，实现输入序列中新的键值对的快速记忆与检索。

Result: FwPKM作为一种有效的情景（episodic）记忆机制，补充了标准网络的语义记忆。在长上下文数据集上，FwPKM显著降低困惑度。特别是在被称为“针在大海捞针”任务中，即使只在4K长度的序列上训练，FwPKM仍能推广到128K长度的文本。

Conclusion: FwPKM同时兼具高效和大容量记忆能力，有效补充了现有语言模型在长序列建模中的不足，有望推动长上下文理解与记忆的应用。

Abstract: Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.

</details>


### [101] [Sigmoid Head for Quality Estimation under Language Ambiguity](https://arxiv.org/abs/2601.00680)
*Tu Anh Dinh,Jan Niehues*

Main category: cs.CL

TL;DR: LM概率不能有效评估输出质量，因其无法同时对多个正确答案赋高概率，该文提出Sigmoid Head模块辅助评估质量，效果更好且能泛化到新领域。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型用softmax输出并据其概率判断输出质量，但自然语言本身常有多种正确答案，单一概率评估容易误导，且训练方式限制了多样答案的表达。

Method: 作者提出在预训练语言模型基础上，额外增加Sigmoid Head（使用Sigmoid激活），能对多个候选输出给出高值。同时，在负采样训练该头部时，用启发式方法避免将其它合理答案当负样本。该模块训练及推理高效，而且不需额外标注数据。

Result: 实验证明：Sigmoid Head给出的概率信号，比原始softmax更准确地反映输出质量，且对新领域具有更强鲁棒性。

Conclusion: Sigmoid Head显著提升语言模型输出质量的自动评估能力，特别在面对多样答案及新领域时，相较原有方法更实用。

Abstract: Language model (LM) probability is not a reliable quality estimator, as natural language is ambiguous. When multiple output options are valid, the model's probability distribution is spread across them, which can misleadingly indicate low output quality. This issue is caused by two reasons: (1) LMs' final output activation is softmax, which does not allow multiple correct options to receive high probabilities simultaneuously and (2) LMs' training data is single, one-hot encoded references, indicating that there is only one correct option at each output step. We propose training a module for Quality Estimation on top of pre-trained LMs to address these limitations. The module, called Sigmoid Head, is an extra unembedding head with sigmoid activation to tackle the first limitation. To tackle the second limitation, during the negative sampling process to train the Sigmoid Head, we use a heuristic to avoid selecting potentially alternative correct tokens. Our Sigmoid Head is computationally efficient during training and inference. The probability from Sigmoid Head is notably better quality signal compared to the original softmax head. As the Sigmoid Head does not rely on human-annotated quality data, it is more robust to out-of-domain settings compared to supervised QE.

</details>


### [102] [Exploring the Performance of Large Language Models on Subjective Span Identification Tasks](https://arxiv.org/abs/2601.00736)
*Alphaeus Dmonte,Roland Oruche,Tharindu Ranasinghe,Marcos Zampieri,Prasad Calyam*

Main category: cs.CL

TL;DR: 本文评估了多种大语言模型（LLM）在文本片段识别任务（情感分析、恶意语言识别、事实核查）中的表现，并探索了不同推理策略对于提升模型解释性的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管文本片段识别对于NLP任务的可解释性至关重要，当前研究主要集中在如命名实体识别等显式片段识别。针对主观性更强的任务（如面向方面的情感分析），对LLM的研究相对不足。本文旨在填补LLM在主观性片段识别任务中的研究空白。

Method: 本文评估了多种主流LLM在三个流行任务中的文本片段识别能力，包括情感分析、恶意语言识别和事实核查。方法上，综合探索了多种LLM策略，如指令微调（instruction tuning）、上下文学习（in-context learning）及思维链（chain of thought）等。

Result: 实验结果显示，文本内部的内在关系有助于LLM更精确地定位相关片段。同时，不同推理策略对于文本片段识别效果具有提升作用。

Conclusion: 本文证明了LLM在主观性较强的文本片段识别任务中具有良好潜力，所提出的多种LLM推理策略能够提升模型的片段识别和可解释性表现。

Abstract: Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.

</details>


### [103] [Adapting Natural Language Processing Models Across Jurisdictions: A pilot Study in Canadian Cancer Registries](https://arxiv.org/abs/2601.00787)
*Jonathan Simkin,Lovedeep Gondara,Zeeshan Rizvi,Gregory Doyle,Jeff Dowden,Dan Bond,Desmond Martin,Raymond Ng*

Main category: cs.CL

TL;DR: 本研究评估了基于transformer的NLP模型在加拿大不同省份间迁移应用于癌症病理报告抽取任务的可行性，并提出集成方法显著减少了漏报病例。


<details>
  <summary>Details</summary>
Motivation: 肿瘤登记依赖手工整理病理报告，效率低且时效慢。自动化NLP虽提升工作流，但跨地区的泛化能力尚不明确，因此需要探索模型可移植性及适应性。

Method: 用新不伦瑞克省肿瘤登记的约10.4万份病理报告训练BCCRTron和GatorTron，并针对不同报告结构微调两模型，再通过OR-集成方式联合应用，旨在提升两级标注（是否为癌症，是否为可报告癌症）的召回率。

Result: 单模型召回率已较高，但集成后，一级任务召回0.99，漏报病例从各自的48和54下降到24。二级任务召回同为0.99，误漏降至33，明显优于单一模型。

Conclusion: 不同省训练的transformer模型经少量微调即可跨省高效迁移。集成方法可进一步减漏，提升覆盖，同时以隐私保护方式仅共享模型权重，为加拿大全国癌症NLP基础设施建设提供了技术路线。

Abstract: Population-based cancer registries depend on pathology reports as their primary diagnostic source, yet manual abstraction is resource-intensive and contributes to delays in cancer data. While transformer-based NLP systems have improved registry workflows, their ability to generalize across jurisdictions with differing reporting conventions remains poorly understood. We present the first cross-provincial evaluation of adapting BCCRTron, a domain-adapted transformer model developed at the British Columbia Cancer Registry, alongside GatorTron, a biomedical transformer model, for cancer surveillance in Canada. Our training dataset consisted of approximately 104,000 and 22,000 de-identified pathology reports from the Newfoundland & Labrador Cancer Registry (NLCR) for Tier 1 (cancer vs. non-cancer) and Tier 2 (reportable vs. non-reportable) tasks, respectively. Both models were fine-tuned using complementary synoptic and diagnosis focused report section input pipelines. Across NLCR test sets, the adapted models maintained high performance, demonstrating transformers pretrained in one jurisdiction can be localized to another with modest fine-tuning. To improve sensitivity, we combined the two models using a conservative OR-ensemble achieving a Tier 1 recall of 0.99 and reduced missed cancers to 24, compared with 48 and 54 for the standalone models. For Tier 2, the ensemble achieved 0.99 recall and reduced missed reportable cancers to 33, compared with 54 and 46 for the individual models. These findings demonstrate that an ensemble combining complementary text representations substantially reduce missed cancers and improve error coverage in cancer-registry NLP. We implement a privacy-preserving workflow in which only model weights are shared between provinces, supporting interoperable NLP infrastructure and a future pan-Canadian foundation model for cancer pathology and registry workflows.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [104] [Reinforcement learning with timed constraints for robotics motion planning](https://arxiv.org/abs/2601.00087)
*Zhaoan Wang,Junchao Li,Mahdi Mohammad,Shaoping Xiao*

Main category: cs.RO

TL;DR: 本文提出了一种结合Metric Interval Temporal Logic (MITL)和强化学习（RL）的通用方法，能够在具有时序约束的动态和部分可观测环境下进行规划。通过将MITL公式转化为定时自动机并与决策过程同步，实现了支持时序逻辑约束的Q学习，验证方案在多个仿真环境中展现出较好效果。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在复杂的时序任务和严格时间约束下难以直接应用，而MITL为这类需求提供了形式化描述，但与RL的结合面临动力学不确定性和部分可观测等挑战。因此，亟需能够将MITL与RL高效整合，以实现鲁棒的时间敏感任务规划。

Method: 提出一种自动机驱动的RL框架：将MITL公式转换为定时极限确定性广义Büchi自动机（Timed-LDGBA），与底层MDP/POMDP同步生成乘积时序模型，配合专门设计的奖励结构，引导Q-learning学习满足时序逻辑约束的策略。实验分别在5×5和10×10网格环境以及办公场景下验证了方法。

Result: 该方法在随机转移情况下，能够在不同规模和部分可观测的大状态空间内，学习出能满足严格时间约束的任务执行策略。实验结果显示框架具有良好的可扩展性和鲁棒性。

Conclusion: 研究结果表明，所提出的MITL驱动的自动机强化学习框架适用于动态不确定环境中的机器人任务规划，能确保时序要求的可靠性，为实际场景下的时间敏感型机器人规划提供了有效方法。

Abstract: Robotic systems operating in dynamic and uncertain environments increasingly require planners that satisfy complex task sequences while adhering to strict temporal constraints. Metric Interval Temporal Logic (MITL) offers a formal and expressive framework for specifying such time-bounded requirements; however, integrating MITL with reinforcement learning (RL) remains challenging due to stochastic dynamics and partial observability. This paper presents a unified automata-based RL framework for synthesizing policies in both Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs) under MITL specifications. MITL formulas are translated into Timed Limit-Deterministic Generalized Büchi Automata (Timed-LDGBA) and synchronized with the underlying decision process to construct product timed models suitable for Q-learning. A simple yet expressive reward structure enforces temporal correctness while allowing additional performance objectives. The approach is validated in three simulation studies: a $5 \times 5$ grid-world formulated as an MDP, a $10 \times 10$ grid-world formulated as a POMDP, and an office-like service-robot scenario. Results demonstrate that the proposed framework consistently learns policies that satisfy strict time-bounded requirements under stochastic transitions, scales to larger state spaces, and remains effective in partially observable environments, highlighting its potential for reliable robotic planning in time-critical and uncertain settings.

</details>


### [105] [Compositional Diffusion with Guided search for Long-Horizon Planning](https://arxiv.org/abs/2601.00126)
*Utkarsh A Mishra,David He,Yongxin Chen,Danfei Xu*

Main category: cs.RO

TL;DR: 本文提出了一种新的生成模型组合方法CDGS，有效解决了因局部分布多模态导致的不可行/不连贯计划问题，提升了长任务建模和生成的效果。


<details>
  <summary>Details</summary>
Motivation: 现有组合生成模型在面对多模态局部分布时，容易出现“模态均值化”问题，导致生成结果既不局部可行也不全局连贯。作者希望通过新方法解决这一关键挑战，实现真正有效的长任务组合建模。

Method: 提出Compositional Diffusion with Guided Search (CDGS)。该方法将搜索直接嵌入扩散去噪过程：通过种群采样探索多种局部模态组合，用似然过滤剔除不可行候选，并通过重采样迭代保证重叠片段之间的全局一致性。

Result: CDGS在七个机器人操作任务上表现达到了oracle指标，优于传统非组合或需长时序训练数据的基线。还可以泛化至全景图像和长视频生成中，实现文本到多模态内容的连贯生成。

Conclusion: CDGS能有效解决组合生成中的模态平均问题，提升长时序生成的连贯性与可行性，在多任务、多领域具备良好适应性和泛化能力。

Abstract: Generative models have emerged as powerful tools for planning, with compositional approaches offering particular promise for modeling long-horizon task distributions by composing together local, modular generative models. This compositional paradigm spans diverse domains, from multi-step manipulation planning to panoramic image synthesis to long video generation. However, compositional generative models face a critical challenge: when local distributions are multimodal, existing composition methods average incompatible modes, producing plans that are neither locally feasible nor globally coherent. We propose Compositional Diffusion with Guided Search (CDGS), which addresses this \emph{mode averaging} problem by embedding search directly within the diffusion denoising process. Our method explores diverse combinations of local modes through population-based sampling, prunes infeasible candidates using likelihood-based filtering, and enforces global consistency through iterative resampling between overlapping segments. CDGS matches oracle performance on seven robot manipulation tasks, outperforming baselines that lack compositionality or require long-horizon training data. The approach generalizes across domains, enabling coherent text-guided panoramic images and long videos through effective local-to-global message passing. More details: https://cdgsearch.github.io/

</details>


### [106] [SLEI3D: Simultaneous Exploration and Inspection via Heterogeneous Fleets under Limited Communication](https://arxiv.org/abs/2601.00163)
*Junfeng Chen,Yuxiao Zhu,Xintong Zhang,Bing Luo,Meng Guo*

Main category: cs.RO

TL;DR: 本文提出了SLEI3D框架，实现了多机器人在未知环境下的协同探索、适应性检查与实时信息传递。


<details>
  <summary>Details</summary>
Motivation: 传统机器人巡检多在已知区域，而实际应用中目标区域常常是未知且需要即时发现，因此需要新的方法实现未知环境中的自主发现、检查与汇报。

Method: 提出SLEI3D框架，通过搭载不同传感器（如激光雷达和摄像头）的异构机器人，采用多层多频率的规划机制，实现3D协同探索、适应性检查和间歇性/主动通信，以应对通信受限等不确定性。

Result: 框架在高保真仿真环境下（多达48台机器人、覆盖38.4万立方米大场景）及7台机器人实物实验中经过大量验证。

Conclusion: SLEI3D显著提升了多机器人队伍在未知大环境下的自主协同探索、检查和通讯能力，具备较强实用价值。

Abstract: Robotic fleets such as unmanned aerial and ground vehicles have been widely used for routine inspections of static environments, where the areas of interest are known and planned in advance. However, in many applications, such areas of interest are unknown and should be identified online during exploration. Thus, this paper considers the problem of simultaneous exploration, inspection of unknown environments and then real-time communication to a mobile ground control station to report the findings. The heterogeneous robots are equipped with different sensors, e.g., long-range lidars for fast exploration and close-range cameras for detailed inspection. Furthermore, global communication is often unavailable in such environments, where the robots can only communicate with each other via ad-hoc wireless networks when they are in close proximity and free of obstruction. This work proposes a novel planning and coordination framework (SLEI3D) that integrates the online strategies for collaborative 3D exploration, adaptive inspection and timely communication (via the intermit-tent or proactive protocols). To account for uncertainties w.r.t. the number and location of features, a multi-layer and multi-rate planning mechanism is developed for inter-and-intra robot subgroups, to actively meet and coordinate their local plans. The proposed framework is validated extensively via high-fidelity simulations of numerous large-scale missions with up to 48 robots and 384 thousand cubic meters. Hardware experiments of 7 robots are also conducted. Project website is available at https://junfengchen-robotics.github.io/SLEI3D/.

</details>


### [107] [SLAP: Slapband-based Autonomous Perching Drone with Failure Recovery for Vertical Tree Trunks](https://arxiv.org/abs/2601.00238)
*Julia Di,Kenneth A. W. Hoffmann,Tony G. Chen,Tian-Ao Ren,Mark R. Cutkosky*

Main category: cs.RO

TL;DR: 本论文提出了一种适用于大型无人机在垂直树干等表面柔性停驻及故障恢复的系统，实现了高成功率和安全的挂载着陆。


<details>
  <summary>Details</summary>
Motivation: 现有无人机在垂直表面停驻技术主要聚焦于轻量化机械设计，系统级集成和安全性不足，尤其对于需要精密电子设备的测绘无人机，高速激进的降落方式存在极大风险。因此，需要开发更温和、安全、且适合大型无人机的新型停驻方式。

Method: 论文提出的SLAP系统包括基于视觉的着陆点检测、基于惯性测量的停驻失败检测、软着陆姿态控制器、近距离光学检测系统和由弹性腕带与微型刺组合而成的快速主动弹性抓握器。系统通过在人机协作的自主飞行试验和改装的1.2kg商业四旋翼进行验证。

Result: 在真实橡树段上的室内20次飞行试验中，系统停驻成功率达75%；在2次人工诱导失败实验中，停驻失败恢复率为100%。

Conclusion: 所提SLAP系统可实现大型无人机在垂直树干的温和安全停驻及可靠的故障恢复，对无人机野外采样和持续巡视具有现实意义。

Abstract: Perching allows unmanned aerial vehicles (UAVs) to reduce energy consumption, remain anchored for surface sampling operations, or stably survey their surroundings. Previous efforts for perching on vertical surfaces have predominantly focused on lightweight mechanical design solutions with relatively scant system-level integration. Furthermore, perching strategies for vertical surfaces commonly require high-speed, aggressive landing operations that are dangerous for a surveyor drone with sensitive electronics onboard. This work presents the preliminary investigation of a perching approach suitable for larger drones that both gently perches on vertical tree trunks and reacts and recovers from perch failures. The system in this work, called SLAP, consists of vision-based perch site detector, an IMU (inertial-measurement-unit)-based perch failure detector, an attitude controller for soft perching, an optical close-range detection system, and a fast active elastic gripper with microspines made from commercially-available slapbands. We validated this approach on a modified 1.2 kg commercial quadrotor with component and system analysis. Initial human-in-the-loop autonomous indoor flight experiments achieved a 75% perch success rate on a real oak tree segment across 20 flights, and 100% perch failure recovery across 2 flights with induced failures.

</details>


### [108] [Vehicle Painting Robot Path Planning Using Hierarchical Optimization](https://arxiv.org/abs/2601.00271)
*Yuya Nagai,Hiromitsu Nakamura,Narito Shinmachi,Yuta Higashizono,Satoshi Ono*

Main category: cs.RO

TL;DR: 本论文针对汽车工厂中车身喷涂机器人的路径设计问题，提出了一种分层优化方法，实现自动、高效、可行的喷涂路径规划。


<details>
  <summary>Details</summary>
Motivation: 目前汽车喷涂工艺需要多台机器人协同作业，但其作业区域分配及路径设计依赖人工，耗时且易出错，急需自动化解决方案以提升效率和减少人力成本。现有常规机器人路径规划方法受限于喷涂过程的特殊约束，直接应用效果不佳。

Method: 作者将喷涂路径设计问题建模为分层优化问题：上层为类车辆路径问题（VRP），负责区域分配和作业顺序，下层为具体路径细化。上下层分别采用不同优化算法，通过变量表达、约束、修复算子及初始化过程灵活处理喷涂特有限制。

Result: 在三种市售车型上的实验表明，该方法能自动生成满足全部工艺约束的喷涂路径，且路径质量与人工设计相当，有效提升了自动化水平。

Conclusion: 提出的分层优化方法适用于多机器人协作喷涂路径自动设计，具有约束适应性强、设计效率高、自动化程度高等优点，对工程实际具有应用价值。

Abstract: In vehicle production factories, the vehicle painting process employs multiple robotic arms to simultaneously apply paint to car bodies advancing along a conveyor line. Designing paint paths for these robotic arms, which involves assigning car body areas to arms and determining paint sequences for each arm, remains a time-consuming manual task for engineers, indicating the demand for automation and design time reduction. The unique constraints of the painting process hinder the direct application of conventional robotic path planning techniques, such as those used in welding. Therefore, this paper formulates the design of paint paths as a hierarchical optimization problem, where the upper-layer subproblem resembles a vehicle routing problem (VRP), and the lower-layer subproblem involves detailed path planning. This approach allows the use of different optimization algorithms at each layer, and permits flexible handling of constraints specific to the vehicle painting process through the design of variable representation, constraints, repair operators, and an initialization process at the upper and lower layers. Experiments with three commercially available vehicle models demonstrated that the proposed method can automatically design paths that satisfy all constraints for vehicle painting with quality comparable to those created manually by engineers.

</details>


### [109] [Pure Inertial Navigation in Challenging Environments with Wheeled and Chassis Mounted Inertial Sensors](https://arxiv.org/abs/2601.00275)
*Dusan Nemec,Gal Versano,Itai Savin,Vojtech Simak,Juraj Kekelak,Itzik Klein*

Main category: cs.RO

TL;DR: 本文提出了一种结合轮式和车体惯性测量单元的纯惯性导航方法WiCHINS，可有效降低无GNSS或光照不良环境下导航漂移误差，显著提升导航精度。


<details>
  <summary>Details</summary>
Motivation: 在GNSS信号受限或光照条件恶劣的情况下，传统惯性导航系统精度会随时间漂移，影响自动驾驶和移动机器人的实际应用需求，因此需要更鲁棒、精确的纯惯性导航解决方案。

Method: 提出WiCHINS系统：通过将安装于轮子的惯性传感器与安装于车身的惯性传感器相结合，并设计三阶段、各自独立应用扩展卡尔曼滤波器的融合框架，充分利用不同传感器位置的信息提升导航精度。

Result: 采用包含5个惯性测量单元、总时长228.6分钟的数据集评估，与4种基线方法对比，WiCHINS实现了平均11.4米的定位误差（占平均行驶距离的2.4%），效果优于现有方案。

Conclusion: WiCHINS系统在GNSS信号弱或光照恶劣等复杂环境下，有效提升纯惯性导航性能，对实现更稳健的自动驾驶和移动机器人定位具有重要意义。

Abstract: Autonomous vehicles and wheeled robots are widely used in many applications in both indoor and outdoor settings. In practical situations with limited GNSS signals or degraded lighting conditions, the navigation solution may rely only on inertial sensors and as result drift in time due to errors in the inertial measurement. In this work, we propose WiCHINS, a wheeled and chassis inertial navigation system by combining wheel-mounted-inertial sensors with a chassis-mounted inertial sensor for accurate pure inertial navigation. To that end, we derive a three-stage framework, each with a dedicated extended Kalman filter. This framework utilizes the benefits of each location (wheel/body) during the estimation process. To evaluate our proposed approach, we employed a dataset with five inertial measurement units with a total recording time of 228.6 minutes. We compare our approach with four other inertial baselines and demonstrate an average position error of 11.4m, which is $2.4\%$ of the average traveled distance, using two wheels and one body inertial measurement units. As a consequence, our proposed method enables robust navigation in challenging environments and helps bridge the pure-inertial performance gap.

</details>


### [110] [Replaceable Bit-based Gripper for Picking Cluttered Food Items](https://arxiv.org/abs/2601.00305)
*Prashant Kumar,Yukiyasu Domae,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: 该论文提出了一种基于可更换夹爪的系统，实现了对不同重量和形态杂乱食品（如鱼子酱和意大利面）的高效抓取和分拣。


<details>
  <summary>Details</summary>
Motivation: 食品包装行业需要处理多种类型和形状的食品，且食品的重量变化频繁，现有夹持工具难以高效适应和精确称重分拣。

Method: 设计了一种夹爪系统，夹爪末端（bit）可针对不同食品快速更换，并采用皮带式更换结构以便操作。针对颗粒状（鱼子酱）和长条形（意大利面）两类典型食品定制夹爪，并实现了对重量的精准控制和分离。

Result: 实验表明，系统能成功抓取意大利面和鱼子酱，并分别以80%和95%以上的准确度按重量分拣。此外，系统能快速更换不同夹爪，灵活适应多种食品。

Conclusion: 该可更换bit夹爪系统能显著提升食品包装过程中不同食品的处理效率和精度，对复杂类食品的抓取和分拣具有重要实际意义。

Abstract: The food packaging industry goes through changes in food items and their weights quite rapidly. These items range from easy-to-pick, single-piece food items to flexible, long and cluttered ones. We propose a replaceable bit-based gripper system to tackle the challenge of weight-based handling of cluttered food items. The gripper features specialized food attachments(bits) that enhance its grasping capabilities, and a belt replacement system allows switching between different food items during packaging operations. It offers a wide range of control options, enabling it to grasp and drop specific weights of granular, cluttered, and entangled foods. We specifically designed bits for two flexible food items that differ in shape: ikura(salmon roe) and spaghetti. They represent the challenging categories of sticky, granular food and long, sticky, cluttered food, respectively. The gripper successfully picked up both spaghetti and ikura and demonstrated weight-specific dropping of these items with an accuracy over 80% and 95% respectively. The gripper system also exhibited quick switching between different bits, leading to the handling of a large range of food items.

</details>


### [111] [Space Debris Removal using Nano-Satellites controlled by Low-Power Autonomous Agents](https://arxiv.org/abs/2601.00465)
*Dennis Christmann,Juan F. Gutierrez,Sthiti Padhi,Patrick Plörer,Aditya Takur,Simona Silvestri,Andres Gomez*

Main category: cs.RO

TL;DR: 论文提出并实验性验证了一种利用自主微型卫星集群清理太空垃圾的方法。


<details>
  <summary>Details</summary>
Motivation: 太空垃圾数量不断增加，威胁卫星与航天任务安全，迫切需要有效的清理方案。

Method: 基于资源受限平台上的自主智能体技术，将自主代理软件部署在无线微控制器上，并在专用测试平台上进行实验，验证方案的可行性和能效。

Result: 实验结果表明，该方法在能效和可行性方面具有优势，展示了利用纳米卫星集群自主处置太空垃圾的初步成功。

Conclusion: 自主纳米卫星集群可望成为解决太空垃圾问题的有效技术方案，后续可进一步优化并扩展应用。

Abstract: Space debris is an ever-increasing problem in space travel. There are already many old, no longer functional spacecraft and debris orbiting the earth, which endanger both the safe operation of satellites and space travel. Small nano-satellite swarms can address this problem by autonomously de-orbiting debris safely into the Earth's atmosphere. This work builds on the recent advances of autonomous agents deployed in resource-constrained platforms and shows a first simplified approach how such intelligent and autonomous nano-satellite swarms can be realized. We implement our autonomous agent software on wireless microcontrollers and perform experiments on a specialized test-bed to show the feasibility and overall energy efficiency of our approach.

</details>


### [112] [Variable Elimination in Hybrid Factor Graphs for Discrete-Continuous Inference & Estimation](https://arxiv.org/abs/2601.00545)
*Varun Agrawal,Frank Dellaert*

Main category: cs.RO

TL;DR: 本文提出了一种新的高效混合因子图框架及变量消去算法，用于机器人中同时包含连续和离散变量问题，实现对变量的精确最大后验估计和边缘化。


<details>
  <summary>Details</summary>
Motivation: 机器人应用中往往涉及混合的连续与离散变量，然而将两者统一建模并用于状态估计一直非常困难，现有方法多为近似求解，缺乏精确、高效且普适的解决方案。

Method: 开发了一种新型混合高斯因子，可以同时连接离散和连续变量，并提出混合条件分布，用于表达多种关于离散变量条件下的连续假设；基于条件线性高斯模型，推导了混合变量消去流程，实现精确的后验推断。此外，引入树状结构和剪枝、概率分配手段，有效控制离散假设数目，提高推断可行性。

Result: 在包含测量歧义的SLAM数据集上验证框架，能够准确、通用、简洁地处理混合变量决策问题，支持最可能测量的离散选择，显著提升了推断准确性与实际应用价值。

Conclusion: 所提出的混合因子图与变量消去框架不仅实现了高效精确的混合估计，还展现了良好的普适性和应用前景，有潜力解决更多机器人中的混合状态估计难题。

Abstract: Many hybrid problems in robotics involve both continuous and discrete components, and modeling them together for estimation tasks has been a long standing and difficult problem. Hybrid Factor Graphs give us a mathematical framework to model these types of problems, however existing approaches for solving them are based on approximations. In this work, we propose an efficient Hybrid Factor Graph framework alongwith a variable elimination algorithm to produce a hybrid Bayes network, which can then be used for exact Maximum A Posteriori estimation and marginalization over both sets of variables. Our approach first develops a novel hybrid Gaussian factor which can connect to both discrete and continuous variables, and a hybrid conditional which can represent multiple continuous hypotheses conditioned on the discrete variables. Using these representations, we derive the process of hybrid variable elimination under the Conditional Linear Gaussian scheme, giving us exact posteriors as hybrid Bayes network. To bound the number of discrete hypotheses, we use a tree-structured representation of the factors coupled with a simple pruning and probabilistic assignment scheme, which allows for tractable inference. We demonstrate the applicability of our framework on a SLAM dataset with ambiguous measurements, where discrete choices for the most likely measurement have to be made. Our demonstrated results showcase the accuracy, generality, and simplicity of our hybrid factor graph framework.

</details>


### [113] [LLM-Based Agentic Exploration for Robot Navigation & Manipulation with Skill Orchestration](https://arxiv.org/abs/2601.00555)
*Abu Hanif Muhammad Syarubany,Farhan Zaki Rahmani,Trio Widianto*

Main category: cs.RO

TL;DR: 提出了一种基于大语言模型（LLM）的机器人室内购物系统，实现了从自然语言理解到自主导航和物品抓取的端到端任务执行，并在仿真和现实环境中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 目前的机器人室内导航和任务执行系统在理解自然语言、场景感知和动作决策方面存在模块割裂、可扩展性和可调试性有限的问题，希望借助LLM提升系统的灵活性、泛化能力和端到端处理能力。

Method: 机器人通过检测路口指示牌建立简易语义地图，用AprilTag作为定位锚点。用户用自然语言发出购物请求，LLM在每处路口决策前进方向和是否进入商店，并通过ROS有限状态主控器下发运动劣化指令，调度模块化底层运动，包括避障、对准、进店和抓取等。

Result: 系统在仿真环境和实际走廊布局下均实现了从自然语言指令到多商店导航和抓取任务的完整执行，体验展示了系统的端到端能力、可调试性和扩展性。

Conclusion: 基于大语言模型的机器人探索系统适合部署在实际室内复杂环境中，能够提升用自然语言交互的机器人能力，未来有望在更多实际服务场景落地。

Abstract: This paper presents an end-to-end LLM-based agentic exploration system for an indoor shopping task, evaluated in both Gazebo simulation and a corresponding real-world corridor layout. The robot incrementally builds a lightweight semantic map by detecting signboards at junctions and storing direction-to-POI relations together with estimated junction poses, while AprilTags provide repeatable anchors for approach and alignment. Given a natural-language shopping request, an LLM produces a constrained discrete action at each junction (direction and whether to enter a store), and a ROS finite-state main controller executes the decision by gating modular motion primitives, including local-costmap-based obstacle avoidance, AprilTag approaching, store entry, and grasping. Qualitative results show that the integrated stack can perform end-to-end task execution from user instruction to multi-store navigation and object retrieval, while remaining modular and debuggable through its text-based map and logged decision history.

</details>


### [114] [Priority-Aware Multi-Robot Coverage Path Planning](https://arxiv.org/abs/2601.00580)
*Kanghoon Lee,Hyeonjun Kim,Jiachen Li,Jinkyoo Park*

Main category: cs.RO

TL;DR: 本文提出了一种新的多机器人覆盖路径规划问题（PA-MCPP），旨在在存在不同优先级区域时，优化覆盖延迟。通过两阶段框架显著降低了高优先级区的加权延迟，并验证了方法的可扩展性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有多机器人覆盖路径规划大多假设区域重要性均等，这在一些希望优先覆盖关键区的应用场景下无法满足需求。因此，有必要引入能够有效处理不同区域优先级的规划方法。

Method: 提出了Priority-Aware MCPP问题，将部分区域设为优先区并赋予权重。提出了一个可扩展的两阶段框架：（1）贪心分区结合局部搜索、生成树路径规划；（2）Steiner树引导的剩余区域覆盖。

Result: 实验表明，该方法在多个场景下有效减少了优先区加权延迟，相较于传统方法优势明显。同时，总体makespan保持竞争力。灵敏度分析显示算法良好扩展到多机器人情况，并可利用权重灵活控制覆盖行为。

Conclusion: 本文方法能在重点区域优先被关注的场景下，有效优化多机器人覆盖任务，实现优先级灵活调整，具有较强实用价值和扩展性。

Abstract: Multi-robot systems are widely used for coverage tasks that require efficient coordination across large environments. In Multi-Robot Coverage Path Planning (MCPP), the objective is typically to minimize the makespan by generating non-overlapping paths for full-area coverage. However, most existing methods assume uniform importance across regions, limiting their effectiveness in scenarios where some zones require faster attention. We introduce the Priority-Aware MCPP (PA-MCPP) problem, where a subset of the environment is designated as prioritized zones with associated weights. The goal is to minimize, in lexicographic order, the total priority-weighted latency of zone coverage and the overall makespan. To address this, we propose a scalable two-phase framework combining (1) greedy zone assignment with local search, spanning-tree-based path planning, and (2) Steiner-tree-guided residual coverage. Experiments across diverse scenarios demonstrate that our method significantly reduces priority-weighted latency compared to standard MCPP baselines, while maintaining competitive makespan. Sensitivity analyses further show that the method scales well with the number of robots and that zone coverage behavior can be effectively controlled by adjusting priority weights.

</details>


### [115] [NMPC-Augmented Visual Navigation and Safe Learning Control for Large-Scale Mobile Robots](https://arxiv.org/abs/2601.00609)
*Mehdi Heydari Shahna,Pauli Mustalahti,Jouni Mattila*

Main category: cs.RO

TL;DR: 本文提出了一套适用于大规模移动机器人（LSMR）在松散地形上安全稳定运行的导航与控制框架，并通过多个高性能模块联合提升其鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: LSMR通常在松散、未固结的地形运行，导致牵引力不足和运行不稳定，为此需要开发能够适应这种复杂环境并保证安全性能的控制系统。

Method: 框架包含：1）融合板载传感器与立体相机的视觉定位模块；2）用于纠正漂移的高层非线性模型预测控制；3）结合深度神经网络与自适应鲁棒控制的低层轮驱动控制策略，以应对复杂和分布外干扰；4）全栈安全监控模块。各模块频率同步，整体联动。

Result: 在一台总重6000公斤、由两组电液驱动驱动的LSMR上进行了对比实验，验证了该方法在复杂地形和多频控制条件下的有效性与鲁棒性。

Conclusion: 该控制框架可确保LSMR在松散、滑移地形下的安全、稳定与系统级鲁棒运行，为复杂地形重型机器人导航与控制提供了切实可行的方案。

Abstract: A large-scale mobile robot (LSMR) is a high-order multibody system that often operates on loose, unconsolidated terrain, which reduces traction. This paper presents a comprehensive navigation and control framework for an LSMR that ensures stability and safety-defined performance, delivering robust operation on slip-prone terrain by jointly leveraging high-performance techniques. The proposed architecture comprises four main modules: (1) a visual pose-estimation module that fuses onboard sensors and stereo cameras to provide an accurate, low-latency robot pose, (2) a high-level nonlinear model predictive control that updates the wheel motion commands to correct robot drift from the robot reference pose on slip-prone terrain, (3) a low-level deep neural network control policy that approximates the complex behavior of the wheel-driven actuation mechanism in LSMRs, augmented with robust adaptive control to handle out-of-distribution disturbances, ensuring that the wheels accurately track the updated commands issued by high-level control module, and (4) a logarithmic safety module to monitor the entire robot stack and guarantees safe operation. The proposed low-level control framework guarantees uniform exponential stability of the actuation subsystem, while the safety module ensures the whole system-level safety during operation. Comparative experiments on a 6,000 kg LSMR actuated by two complex electro-hydrostatic drives, while synchronizing modules operating at different frequencies.

</details>


### [116] [Vision-based Goal-Reaching Control for Mobile Robots Using a Hierarchical Learning Framework](https://arxiv.org/abs/2601.00610)
*Mehdi Heydari Shahna,Pauli Mustalahti,Jouni Mattila*

Main category: cs.RO

TL;DR: 本文提出了一种面向大型机器人安全目标到达的分模块控制框架，通过多种方法提升机器人在复杂不稳定地形下的安全性与效率，并在6000公斤机器人上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽在机器人领域表现突出，但因需大量探索带来安全隐患，尤其在大型复杂机器人上尤为明显。为解决RL实际应用中的安全瓶颈，亟需设计更安全、可靠的控制框架。

Method: 将整个系统分解为多个强耦合模块：1）实时视觉位姿估计为RL模块提供精确状态，2）RL运动规划器负责产生符合机器人约束且平滑的运动命令，3）利用监督深度学习建模复杂动力学，4）基于模型的自适应控制器保证在打滑等复杂地形下轨迹可追踪，5）安全监控模块可自动检测和应对故障风险，降低人工干预。

Result: 所提框架在重达6吨的实际机器人不同场景实验中得到验证，表现出优异的安全性、稳定性和运动控制能力。

Conclusion: 该方法有效融合视觉感知、强化学习、深度建模和安全检测，显著增强了大型机器人在复杂环境下的自主安全作业能力，实现了系统级的稳定性和安全保障。

Abstract: Reinforcement learning (RL) is effective in many robotic applications, but it requires extensive exploration of the state-action space, during which behaviors can be unsafe. This significantly limits its applicability to large robots with complex actuators operating on unstable terrain. Hence, to design a safe goal-reaching control framework for large-scale robots, this paper decomposes the whole system into a set of tightly coupled functional modules. 1) A real-time visual pose estimation approach is employed to provide accurate robot states to 2) an RL motion planner for goal-reaching tasks that explicitly respects robot specifications. The RL module generates real-time smooth motion commands for the actuator system, independent of its underlying dynamic complexity. 3) In the actuation mechanism, a supervised deep learning model is trained to capture the complex dynamics of the robot and provide this model to 4) a model-based robust adaptive controller that guarantees the wheels track the RL motion commands even on slip-prone terrain. 5) Finally, to reduce human intervention, a mathematical safety supervisor monitors the robot, stops it on unsafe faults, and autonomously guides it back to a safe inspection area. The proposed framework guarantees uniform exponential stability of the actuation system and safety of the whole operation. Experiments on a 6,000 kg robot in different scenarios confirm the effectiveness of the proposed framework.

</details>


### [117] [From 2D to 3D terrain-following area coverage path planning](https://arxiv.org/abs/2601.00614)
*Mogens Plessen*

Main category: cs.RO

TL;DR: 该论文提出了一种用于3D地形的覆盖路径规划算法，实现机械设备在三维地形下的高效区域覆盖。


<details>
  <summary>Details</summary>
Motivation: 在农业等实际应用中，机械设备需要适应三维起伏地形进行作业，传统2D路径规划无法满足这一需求，因此需要开发能够处理3D地形特征的覆盖路径规划算法。

Method: 算法生成多条相邻路径，使其彼此间隔等于机械工作宽度，并距离地面投影等于特定工作高度。为处理3D地形数据，采用了基于反距离加权（IDW）的均匀高程数据生成和局部搜索方法。

Result: 在农业场景下，用真实的3D地形数据测试了该算法，实现了有效的区域覆盖，并展示了算法相较于二维情况在数据处理和算法复杂性上的不同。

Conclusion: 该算法能够适应复杂的三维地形，实现机械设备的高效区域覆盖，对比二维算法提升了实际适用性。

Abstract: An algorithm for 3D terrain-following area coverage path planning is presented. Multiple adjacent paths are generated that are (i) locally apart from each other by a distance equal to the working width of a machinery, while (ii) simultaneously floating at a projection distance equal to a specific working height above the terrain. The complexities of the algorithm in comparison to its 2D equivalent are highlighted. These include uniformly spaced elevation data generation using an Inverse Distance Weighting-approach and a local search. Area coverage path planning results for real-world 3D data within an agricultural context are presented to validate the algorithm.

</details>


### [118] [RoboReward: General-Purpose Vision-Language Reward Models for Robotics](https://arxiv.org/abs/2601.00675)
*Tony Lee,Andrew Wagenmaker,Karl Pertsch,Percy Liang,Sergey Levine,Chelsea Finn*

Main category: cs.RO

TL;DR: 本文提出了RoboReward数据集及基准，通过大规模真实机器人数据集训练和评估视觉-语言奖励模型，并开发了数据增强方法提升失败样本多样性。结果表明，现有VLM在所有任务上表现不均衡，作者训练的中等规模模型在短期机器人任务奖励分配上优于更大模型，并能显著提升强化学习策略效果。


<details>
  <summary>Details</summary>
Motivation: 实际机器人任务中高质量奖励函数难以获得，传统方法依赖人工标注或脆弱的手工目标。视觉-语言模型在自动奖励估计上初见成效，但在真实机器人场景中的实际能力未得到系统评估。

Method: 1）构建基于大型机器人语料的RoboReward数据集和基准；2）提出通过反事实标签和时序裁剪生成失败和接近成功的负例进行数据增强；3）训练和评估4B/8B参数的奖励VLM模型；4）系统评估当下主流开源和闭源VLM，以及作者自研模型在奖励分配和实际机器人RL上的表现。

Result: 发现现有主流VLM均未在所有任务上取得最好表现，作者训练的4B/8B模型在分配短期机器人奖励任务时超过规模更大的知名VLM，并在实际强化学习中，8B模型显著超过当前最优的Gemini Robotics-ER 1.5，并显著接近由人工奖励驱动的RL效果。

Conclusion: 数据集和方法提升了VLM在机器人奖励建模中的能力。中等规模奖励VLM在适配实际机器人强化学习时优势明显，现有VLM尚有很大提升空间，新方法能有效缩小与人工奖励RL之间的性能差距。

Abstract: A well-designed reward is critical for effective reinforcement learning-based policy improvement. In real-world robotic domains, obtaining such rewards typically requires either labor-intensive human labeling or brittle, handcrafted objectives. Vision-language models (VLMs) have shown promise as automatic reward models, yet their effectiveness on real robot tasks is poorly understood. In this work, we aim to close this gap by introducing (1) \textbf{RoboReward}, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment (OXE) and RoboArena, and (2) vision-language reward models trained on this dataset (RoboReward 4B/8B). Because OXE is success-heavy and lacks failure examples, we propose a \emph{negative examples data augmentation} pipeline that generates calibrated \emph{negatives} and \emph{near-misses} via counterfactual relabeling of successful episodes and temporal clipping to create partial-progress outcomes from the same videos. Using this framework, we produce an extensive training and evaluation dataset that spans diverse tasks and embodiments and enables systematic evaluation of whether state-of-the-art VLMs can reliably provide rewards for robotics. Our evaluation of leading open-weight and proprietary VLMs reveals that no model excels across all tasks, underscoring substantial room for improvement. We then train general-purpose 4B- and 8B-parameter models that outperform much larger VLMs in assigning rewards for short-horizon robotic tasks. Finally, we deploy the 8B-parameter reward VLM in real-robot reinforcement learning and find that it improves policy learning over Gemini Robotics-ER 1.5, a frontier physical reasoning VLM trained on robotics data, by a large margin, while substantially narrowing the gap to RL training with human-provided rewards.

</details>


### [119] [DefVINS: Visual-Inertial Odometry for Deformable Scenes](https://arxiv.org/abs/2601.00702)
*Samuel Cerezo,Javier Civera*

Main category: cs.RO

TL;DR: 提出了一种名为DefVINS的新视觉-惯性里程计(VIO)系统，能有效应对场景非刚性变形对定位造成的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统VIO假设场景为刚性，但现实中场景常常发生非刚性变形（如软体、动态物体），导致VIO结果漂移甚至失效，因此需要能区分、处理刚性和非刚性运动的VIO方法。

Method: DefVINS将系统状态分为刚性部分（通过IMU锚定）和非刚性部分（用嵌入式变形图表示）。首先用经典VIO初始化，确定重力、速度和IMU偏置，之后再逐步激活非刚性自由度。系统引入可观性分析，揭示IMU测量约束如何帮助分辨刚性与非刚性运动，并指导何时适合激活非刚性估计，以避免问题不适定。

Result: 消融实验表明，将惯性约束和可观性引导的非刚性激活策略结合后，系统在非刚性环境下具有更强的鲁棒性。

Conclusion: DefVINS能有效区分和处理刚性与非刚性运动，提升了VIO在存在非刚性变形环境下的精度与稳定性。

Abstract: Deformable scenes violate the rigidity assumptions underpinning classical visual-inertial odometry (VIO), often leading to over-fitting to local non-rigid motion or severe drift when deformation dominates visual parallax. We introduce DefVINS, a visual-inertial odometry framework that explicitly separates a rigid, IMU-anchored state from a non--rigid warp represented by an embedded deformation graph. The system is initialized using a standard VIO procedure that fixes gravity, velocity, and IMU biases, after which non-rigid degrees of freedom are activated progressively as the estimation becomes well conditioned. An observability analysis is included to characterize how inertial measurements constrain the rigid motion and render otherwise unobservable modes identifiable in the presence of deformation. This analysis motivates the use of IMU anchoring and informs a conditioning-based activation strategy that prevents ill-posed updates under poor excitation. Ablation studies demonstrate the benefits of combining inertial constraints with observability-aware deformation activation, resulting in improved robustness under non-rigid environments.

</details>


### [120] [Calling for Backup: How Children Navigate Successive Robot Communication Failures](https://arxiv.org/abs/2601.00754)
*Maria Teresa Parreira,Isabel Neto,Filipa Rocha,Wendy Ju*

Main category: cs.RO

TL;DR: 本研究分析了8-10岁儿童在与机器人互动时，面对机器人连续三次对话错误的反应，发现儿童与成人表现出了一些相似与不同的行为特点，对未来儿童友好型人机交互系统的设计具有启示意义。


<details>
  <summary>Details</summary>
Motivation: 目前关于人类对机器人错误反应的研究多集中于成年人，缺乏对儿童这一用户群体的深入了解。本研究希望弥补这个空白，探索儿童在面对机器人社交和性能（对话）错误时的行为反应特征。

Method: 复现了前人研究中的机器人连续失败实验，但参与者换为8-10岁的儿童（N=59），记录他们与连续三次未能理解其指令的机器人互动过程，并对儿童的行为反应进行视频分析。

Result: 儿童在机器人持续错误下，像成年人一样会调整用词、改变语气、表现出更多的情绪性非语言行为。但与成年人不同，儿童更容易表现为暂时无视机器人或主动寻求成年人的帮助。同时，实验未发现错误影响儿童对机器人的整体感知。

Conclusion: 儿童在与机器人互动时对错误的容忍度和行为表现与成年人不同，对话期望也更具弹性。本研究为儿童人机交互系统的设计提供了新的建议，应注重考虑儿童用户的独特需求和反应模式。

Abstract: How do children respond to repeated robot errors? While prior research has examined adult reactions to successive robot errors, children's responses remain largely unexplored. In this study, we explore children's reactions to robot social errors and performance errors. For the latter, this study reproduces the successive robot failure paradigm of Liu et al. with child participants (N=59, ages 8-10) to examine how young users respond to repeated robot conversational errors. Participants interacted with a robot that failed to understand their prompts three times in succession, with their behavioral responses video-recorded and analyzed. We found both similarities and differences compared to adult responses from the original study. Like adults, children adjusted their prompts, modified their verbal tone, and exhibited increasingly emotional non-verbal responses throughout successive errors. However, children demonstrated more disengagement behaviors, including temporarily ignoring the robot or actively seeking an adult. Errors did not affect participants' perception of the robot, suggesting more flexible conversational expectations in children. These findings inform the design of more effective and developmentally appropriate human-robot interaction systems for young users.

</details>
