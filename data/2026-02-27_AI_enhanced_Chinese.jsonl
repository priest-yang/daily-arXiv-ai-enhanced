{"id": "2602.22351", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22351", "abs": "https://arxiv.org/abs/2602.22351", "authors": ["Qitong Wang", "Mohammed J. Zaki", "Georgios Kollias", "Vasileios Kalantzis"], "title": "Decoder-based Sense Knowledge Distillation", "comment": null, "summary": "Large language models (LLMs) learn contextual embeddings that capture rich semantic information, yet they often overlook structured lexical knowledge such as word senses and relationships. Prior work has shown that incorporating sense dictionaries can improve knowledge distillation for encoder models, but their application to decoder as generative models remains challenging. In this paper, we introduce Decoder-based Sense Knowledge Distillation (DSKD), a framework that integrates lexical resources into the training of decoder-style LLMs without requiring dictionary lookup at inference time. Extensive experiments on diverse benchmarks demonstrate that DSKD significantly enhances knowledge distillation performance for decoders, enabling generative models to inherit structured semantics while maintaining efficient training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDSKD\u6846\u67b6\uff0c\u5c06\u8bcd\u5178\u77e5\u8bc6\u878d\u5165\u89e3\u7801\u5668\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\uff0c\u63d0\u5347\u77e5\u8bc6\u84b8\u998f\u6548\u679c\uff0c\u65e0\u9700\u63a8\u7406\u65f6\u8bcd\u5178\u67e5\u8be2\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u867d\u7136LLM\u80fd\u591f\u6355\u6349\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u5355\u8bcd\u4e49\u9879\u548c\u5173\u7cfb\u7b49\u7ed3\u6784\u5316\u8bcd\u6c47\u77e5\u8bc6\u3002\u4ee5\u5f80\u5c06\u8bcd\u5178\u77e5\u8bc6\u7528\u4e8e\u7f16\u7801\u5668\u6a21\u578b\u6709\u76ca\uff0c\u4f46\u5728\u89e3\u7801\u5668/\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u5b58\u5728\u96be\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u5c06\u8bcd\u5178\u77e5\u8bc6\u5f15\u5165\u89e3\u7801\u5668\u8bad\u7ec3\u3002", "method": "\u63d0\u51faDecoder-based Sense Knowledge Distillation\uff08DSKD\uff09\u6846\u67b6\uff0c\u5c06\u7ed3\u6784\u5316\u8bcd\u5178\u77e5\u8bc6\u878d\u5165\u89e3\u7801\u5668\u578bLLM\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4e14\u63a8\u7406\u65f6\u65e0\u9700\u8bcd\u5178\u67e5\u8be2\uff0c\u4f18\u5316\u77e5\u8bc6\u84b8\u998f\u6d41\u7a0b\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u4e0d\u540c\u57fa\u51c6\u7684\u6570\u636e\u5b9e\u9a8c\u8bc1\u660e\uff0cDSKD\u80fd\u663e\u8457\u63d0\u5347\u89e3\u7801\u5668\u7684\u77e5\u8bc6\u84b8\u998f\u6027\u80fd\uff0c\u4f7f\u751f\u6210\u6a21\u578b\u7ee7\u627f\u7ed3\u6784\u5316\u8bed\u4e49\u77e5\u8bc6\uff0c\u5e76\u4fdd\u6301\u9ad8\u6548\u8bad\u7ec3\u3002", "conclusion": "DSKD \u80fd\u6709\u6548\u589e\u5f3a\u89e3\u7801\u5668\u578b\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u7ed3\u6784\u5316\u8bcd\u6c47\u77e5\u8bc6\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u662f\u751f\u6210\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u7684\u65b0\u8fdb\u5c55\u3002"}}
{"id": "2602.22359", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22359", "abs": "https://arxiv.org/abs/2602.22359", "authors": ["Arno Simons"], "title": "Scaling In, Not Up? Testing Thick Citation Context Analysis with GPT-5 and Fragile Prompts", "comment": "26 pages, 1 figure, 3 tables (plus 17 pages supplement including 1 figure)", "summary": "This paper tests whether large language models (LLMs) can support interpretative citation context analysis (CCA) by scaling in thick, text-grounded readings of a single hard case rather than scaling up typological labels. It foregrounds prompt-sensitivity analysis as a methodological issue by varying prompt scaffolding and framing in a balanced 2x3 design. Using footnote 6 in Chubin and Moitra (1975) and Gilbert's (1977) reconstruction as a probe, I implement a two-stage GPT-5 pipeline: a citation-text-only surface classification and expectation pass, followed by cross-document interpretative reconstruction using the citing and cited full texts. Across 90 reconstructions, the model produces 450 distinct hypotheses. Close reading and inductive coding identify 21 recurring interpretative moves, and linear probability models estimate how prompt choices shift their frequencies and lexical repertoire. GPT-5's surface pass is highly stable, consistently classifying the citation as \"supplementary\". In reconstruction, the model generates a structured space of plausible alternatives, but scaffolding and examples redistribute attention and vocabulary, sometimes toward strained readings. Relative to Gilbert, GPT-5 detects the same textual hinges yet more often resolves them as lineage and positioning than as admonishment. The study outlines opportunities and risks of using LLMs as guided co-analysts for inspectable, contestable interpretative CCA, and it shows that prompt scaffolding and framing systematically tilt which plausible readings and vocabularies the model foregrounds.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u5426\u901a\u8fc7\u6df1\u5165\u3001\u6587\u672c\u9a71\u52a8\u7684\u89e3\u8bfb\u65b9\u5f0f\u8f85\u52a9\u5f15\u6587\u8bed\u5883\u5206\u6790\uff08CCA\uff09\uff0c\u5e76\u91cd\u70b9\u5206\u6790\u63d0\u793a\u8bcd\u8bbe\u8ba1\u5bf9\u5206\u6790\u7ed3\u679c\u7684\u5f71\u54cd\u3002\u5229\u7528GPT-5\u5728\u5177\u4f53\u6848\u4f8b\u4e2d\u7684\u53cc\u9636\u6bb5\u5206\u6790\uff0c\u5c55\u793aLLM\u5728\u89e3\u91ca\u6027\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u4e0e\u504f\u5411\u3002", "motivation": "\u5f53\u524d\u6587\u732e\u4e2d\u5173\u4e8e\u5982\u4f55\u8ba9LLM\u53c2\u4e0e\u590d\u6742\u5b66\u672f\u6587\u672c\u7684\u89e3\u91ca\u6027\u5206\u6790\u5c1a\u672a\u5145\u5206\u63a2\u8ba8\uff0c\u5c24\u5176\u662f\u5728\u201c\u539a\u63cf\u8ff0\u201d\u800c\u975e\u7b80\u5355\u6807\u7b7e\u5316\u7684\u573a\u666f\u4e0b\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30LLM\u5728\u590d\u6742\u5f15\u6587\u8bed\u5883\u89e3\u8bfb\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5398\u6e05\u63d0\u793a\u8bcd\u8bbe\u8ba1\u5bf9\u5206\u6790\u8f93\u51fa\u7684\u7cfb\u7edf\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u5e73\u8861\u76842x3\u8bbe\u8ba1\uff0c\u7cfb\u7edf\u53d8\u6362\u63d0\u793a\u8bcd\u811a\u624b\u67b6\u548c\u6846\u67b6\u3002\u4ee5Chubin\u548cMoitra\uff081975\uff09\u7b2c6\u811a\u6ce8\u53caGilbert\uff081977\uff09\u7684\u89e3\u8bfb\u4e3a\u57fa\u51c6\uff0c\u901a\u8fc7GPT-5\u7684\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u5148\u8fdb\u884c\u4ec5\u4f9d\u5f15\u6587\u8868\u9762\u7684\u521d\u6b65\u5206\u7c7b\uff0c\u518d\u8fdb\u884c\u8de8\u6587\u732e\u3001\u57fa\u4e8e\u5b8c\u6574\u6587\u672c\u7684\u89e3\u91ca\u6027\u91cd\u6784\u3002\u5bf990\u6b21\u91cd\u6784\u7ed3\u679c\u8fdb\u884c\u7ec6\u81f4\u9605\u8bfb\u548c\u5f52\u7eb3\u7f16\u7801\uff0c\u5229\u7528\u7ebf\u6027\u6982\u7387\u6a21\u578b\u8bc4\u4f30\u63d0\u793a\u8bbe\u8ba1\u5bf9\u5206\u6790\u503e\u5411\u548c\u8bcd\u6c47\u9009\u62e9\u7684\u5f71\u54cd\u3002", "result": "GPT-5\u5728\u8868\u9762\u5206\u7c7b\u9636\u6bb5\u8868\u73b0\u9ad8\u5ea6\u7a33\u5b9a\uff0c\u4e00\u81f4\u5730\u5c06\u5f15\u6587\u5f52\u7c7b\u4e3a\u201c\u8865\u5145\u8bf4\u660e\u201d\u3002\u5728\u89e3\u91ca\u6027\u91cd\u6784\u4e2d\uff0c\u6a21\u578b\u5c55\u793a\u51fa\u591a\u6837\u4e14\u7ed3\u6784\u5316\u7684\u89e3\u8bfb\u7a7a\u95f4\uff0c\u4f46\u63d0\u793a\u8bcd\u811a\u624b\u67b6\u548c\u793a\u4f8b\u663e\u8457\u5f71\u54cd\u6a21\u578b\u5173\u6ce8\u70b9\u548c\u8bcd\u6c47\u5206\u5e03\uff0c\u90e8\u5206\u60c5\u51b5\u4e0b\u4f1a\u5bfc\u81f4\u201c\u8fc7\u5ea6\u89e3\u8bfb\u201d\u3002\u4e0e\u4eba\u7c7b\u8fd1\u4f3c\uff0c\u6a21\u578b\u80fd\u628a\u63e1\u6587\u672c\u5173\u952e\u70b9\uff0c\u4f46\u66f4\u503e\u5411\u4e8e\u89e3\u8bfb\u4e3a\u201c\u4f20\u627f\u6216\u7acb\u573a\u8868\u6001\u201d\uff0c\u800c\u975e\u201c\u6279\u8bc4\u8b66\u793a\u201d\u3002", "conclusion": "LLM\u53ef\u4f5c\u4e3a\u53ef\u8ffd\u6eaf\u3001\u53ef\u8d28\u8be2\u7684\u89e3\u91ca\u6027CCA\u8f85\u52a9\u5de5\u5177\uff0c\u5b58\u5728\u663e\u8457\u7684\u673a\u9047\u4e0e\u98ce\u9669\u3002\u63d0\u793a\u8bcd\u811a\u624b\u67b6\u548c\u6846\u67b6\u4f1a\u7cfb\u7edf\u6027\u5f71\u54cd\u5206\u6790\u65b9\u5411\u548c\u8868\u8fbe\uff0c\u56e0\u6b64\u5728\u4f9d\u8d56LLM\u505a\u590d\u6742\u6587\u672c\u89e3\u91ca\u65f6\u9700\u5ba1\u614e\u8bbe\u8ba1\u63d0\u793a\u4e0e\u6d41\u7a0b\u3002"}}
{"id": "2602.22391", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22391", "abs": "https://arxiv.org/abs/2602.22391", "authors": ["Rakib Ullah", "Mominul islam", "Md Sanjid Hossain", "Md Ismail Hossain"], "title": "Detecting Hate and Inflammatory Content in Bengali Memes: A New Multimodal Dataset and Co-Attention Framework", "comment": "6 pages, 8 figures", "summary": "Internet memes have become a dominant form of expression on social media, including within the Bengali-speaking community. While often humorous, memes can also be exploited to spread offensive, harmful, and inflammatory content targeting individuals and groups. Detecting this type of content is excep- tionally challenging due to its satirical, subtle, and culturally specific nature. This problem is magnified for low-resource lan- guages like Bengali, as existing research predominantly focuses on high-resource languages. To address this critical research gap, we introduce Bn-HIB (Bangla Hate Inflammatory Benign), a novel dataset containing 3,247 manually annotated Bengali memes categorized as Benign, Hate, or Inflammatory. Significantly, Bn- HIB is the first dataset to distinguish inflammatory content from direct hate speech in Bengali memes. Furthermore, we propose the MCFM (Multi-Modal Co-Attention Fusion Model), a simple yet effective architecture that mutually analyzes both the visual and textual elements of a meme. MCFM employs a co-attention mechanism to identify and fuse the most critical features from each modality, leading to a more accurate classification. Our experiments show that MCFM significantly outperforms several state-of-the-art models on the Bn-HIB dataset, demonstrating its effectiveness in this nuanced task.Warning: This work contains material that may be disturbing to some audience members. Viewer discretion is advised.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u7f51\u7edc\u8868\u60c5\u5305\u4e2d\u4ec7\u6068\u4e0e\u717d\u52a8\u6027\u5185\u5bb9\u8bc6\u522b\u96be\u9898\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u533a\u5206\u4ec7\u6068\u4e0e\u717d\u52a8\u6027\u5185\u5bb9\u7684\u6570\u636e\u96c6Bn-HIB\uff0c\u5e76\u63d0\u51fa\u591a\u6a21\u6001\u5171\u6ce8\u610f\u529b\u878d\u5408\u6a21\u578bMCFM\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u8868\u60c5\u5305\u4e2d\u5b58\u5728\u5927\u91cf\u5e26\u6709\u8bbd\u523a\u3001\u9690\u6666\u53ca\u6587\u5316\u7279\u5b9a\u6027\u7684\u4ec7\u6068\u6216\u717d\u52a8\u5185\u5bb9\uff0c\u4e3b\u6d41\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u6025\u9700\u52a0\u4ee5\u89e3\u51b3\u3002", "method": "1\uff09\u4eba\u5de5\u6807\u6ce83247\u6761\u5b5f\u52a0\u62c9\u8bed\u8868\u60c5\u5305\uff0c\u5206\u4e3a\u826f\u6027\u3001\u4ec7\u6068\u548c\u717d\u52a8\u4e09\u7c7b\uff0c\u6784\u5efa\u4e86Bn-HIB\u6570\u636e\u96c6\uff1b2\uff09\u63d0\u51fa\u7ed3\u5408\u56fe\u50cf\u4e0e\u6587\u672c\u4fe1\u606f\u7684\u591a\u6a21\u6001\u5171\u6ce8\u610f\u529b\u878d\u5408\u6a21\u578bMCFM\uff0c\u901a\u8fc7\u5171\u6ce8\u610f\u529b\u673a\u5236\u8054\u5408\u6355\u6349\u5e76\u878d\u5408\u591a\u901a\u9053\u5173\u952e\u7279\u5f81\uff0c\u5b9e\u73b0\u4e09\u5206\u7c7b\u4efb\u52a1\u3002", "result": "MCFM\u6a21\u578b\u5728Bn-HIB\u6570\u636e\u96c6\u4e0a\u660e\u663e\u4f18\u4e8e\u591a\u79cd\u5f53\u524d\u4e3b\u6d41\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8868\u60c5\u5305\u5185\u5bb9\u5206\u6790\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efaBn-HIB\u6570\u636e\u96c6\u548c\u63d0\u51faMCFM\u6a21\u578b\uff0c\u6709\u6548\u63a8\u52a8\u4e86\u5b5f\u52a0\u62c9\u8bed\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u8868\u60c5\u5305\u6709\u5bb3\u5185\u5bb9\u81ea\u52a8\u8bc6\u522b\u7684\u53d1\u5c55\uff0c\u65b9\u6cd5\u5177\u6709\u8f83\u9ad8\u5b9e\u7528\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2602.22404", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22404", "abs": "https://arxiv.org/abs/2602.22404", "authors": ["Aishwarya Verma", "Laud Ammah", "Olivia Nercy Ndlovu Lucas", "Andrew Zaldivar", "Vinodkumar Prabhakaran", "Sunipa Dev"], "title": "SAFARI: A Community-Engaged Approach and Dataset of Stereotype Resources in the Sub-Saharan African Context", "comment": null, "summary": "Stereotype repositories are critical to assess generative AI model safety, but currently lack adequate global coverage. It is imperative to prioritize targeted expansion, strategically addressing existing deficits, over merely increasing data volume. This work introduces a multilingual stereotype resource covering four sub-Saharan African countries that are severely underrepresented in NLP resources: Ghana, Kenya, Nigeria, and South Africa. By utilizing socioculturally-situated, community-engaged methods, including telephonic surveys moderated in native languages, we establish a reproducible methodology that is sensitive to the region's complex linguistic diversity and traditional orality. By deliberately balancing the sample across diverse ethnic and demographic backgrounds, we ensure broad coverage, resulting in a dataset of 3,534 stereotypes in English and 3,206 stereotypes across 15 native languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u975e\u6d32\u6492\u54c8\u62c9\u4ee5\u5357\u56db\u56fd\u7684\u523b\u677f\u5370\u8c61\u6570\u636e\u7f3a\u5931\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u591a\u8bed\u8a00\u6570\u636e\u6536\u96c6\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u8986\u76d6\u591a\u65cf\u88d4\u548c\u8bed\u8a00\u7684\u523b\u677f\u5370\u8c61\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0fAI\u5b89\u5168\u8bc4\u4f30\u4f9d\u8d56\u523b\u677f\u5370\u8c61\u6570\u636e\u96c6\uff0c\u4f46\u5168\u7403\u5206\u5e03\u6781\u4e0d\u5747\u8861\uff0c\u975e\u6d32\u76f8\u5173\u8d44\u6e90\u4e25\u91cd\u4e0d\u8db3\u3002\u4e3a\u63d0\u5347AI\u516c\u6b63\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e9f\u9700\u586b\u8865\u8be5\u5730\u533a\u7684\u8d44\u6e90\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u793e\u533a\u53c2\u4e0e\u548c\u793e\u4f1a\u6587\u5316\u9002\u5e94\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u4ee5\u6bcd\u8bed\u7535\u8bdd\u8c03\u67e5\uff0c\u5e73\u8861\u8986\u76d6\u4e0d\u540c\u6c11\u65cf\u53ca\u4eba\u53e3\u80cc\u666f\uff0c\u654f\u611f\u5e94\u5bf9\u590d\u6742\u7684\u8bed\u8a00\u591a\u6837\u6027\u548c\u53e3\u5934\u4f20\u7edf\uff0c\u4fdd\u8bc1\u6570\u636e\u7684\u4ee3\u8868\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002", "result": "\u5171\u6536\u96c6\u4e863534\u6761\u82f1\u8bed\u523b\u677f\u5370\u8c61\u548c3206\u676115\u79cd\u672c\u5730\u8bed\u8a00\u523b\u677f\u5370\u8c61\uff0c\u6570\u636e\u6837\u672c\u5e73\u8861\uff0c\u6db5\u76d6\u52a0\u7eb3\u3001\u80af\u5c3c\u4e9a\u3001\u5c3c\u65e5\u5229\u4e9a\u548c\u5357\u975e\u56db\u56fd\u7684\u591a\u5143\u4eba\u7fa4\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u52a9\u4e8e\u6539\u5584NLP\u9886\u57df\u7684\u533a\u57df\u548c\u8bed\u8a00\u8986\u76d6\u4e0d\u8db3\uff0c\u6240\u5f97\u591a\u8bed\u8a00\u523b\u677f\u5370\u8c61\u6570\u636e\u96c6\u53ef\u4e3aAI\u5b89\u5168\u3001\u516c\u5e73\u6027\u53ca\u6587\u5316\u9002\u5e94\u6027\u7814\u7a76\u63d0\u4f9b\u6709\u529b\u652f\u6301\uff0c\u5177\u6709\u5e7f\u6cdb\u590d\u7528\u548c\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2602.22243", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22243", "abs": "https://arxiv.org/abs/2602.22243", "authors": ["Jan Nausner", "Kilian Wohlleben", "Michael Hubner"], "title": "SODA-CitrON: Static Object Data Association by Clustering Multi-Modal Sensor Detections Online", "comment": "8 pages, 5 figures; Submitted to the 2026 International Conference on Information Fusion (FUSION 2026). Under review", "summary": "The online fusion and tracking of static objects from heterogeneous sensor detections is a fundamental problem in robotics, autonomous systems, and environmental mapping. Although classical data association approaches such as JPDA are well suited for dynamic targets, they are less effective for static objects observed intermittently and with heterogeneous uncertainties, where motion models provide minimal discriminative with respect to clutter. In this paper, we propose a novel method for static object data association by clustering multi-modal sensor detections online (SODA-CitrON), while simultaneously estimating positions and maintaining persistent tracks for an unknown number of objects. The proposed unsupervised machine learning approach operates in a fully online manner and handles temporally uncorrelated and multi-sensor measurements. Additionally, it has a worst-case loglinear complexity in the number of sensor detections while providing full output explainability. We evaluate the proposed approach in different Monte Carlo simulation scenarios and compare it against state-of-the-art methods, including Bayesian filtering, DBSTREAM clustering, and JPDA. The results demonstrate that SODA-CitrON consistently outperforms the compared methods in terms of F1 score, position RMSE, MOTP, and MOTA in the static object mapping scenarios studied.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u9759\u6001\u7269\u4f53\u6570\u636e\u5173\u8054\u65b9\u6cd5\uff08SODA-CitrON\uff09\uff0c\u7528\u4e8e\u5b9e\u65f6\u878d\u5408\u548c\u8ddf\u8e2a\u591a\u6a21\u6001\u5f02\u6784\u4f20\u611f\u5668\u7684\u9759\u6001\u76ee\u6807\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u636e\u5173\u8054\u65b9\u6cd5\u5982JPDA\u4e3b\u8981\u9002\u7528\u4e8e\u52a8\u6001\u76ee\u6807\uff0c\u5bf9\u4e8e\u95f4\u6b47\u89c2\u6d4b\u3001\u5b58\u5728\u5f02\u6784\u4e0d\u786e\u5b9a\u6027\u7684\u9759\u6001\u7269\u4f53\uff0c\u5176\u6548\u679c\u6709\u9650\u3002\u9759\u6001\u76ee\u6807\u7f3a\u4e4f\u8fd0\u52a8\u4fe1\u606f\uff0c\u96be\u4ee5\u4e0e\u6742\u6ce2\u533a\u5206\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u6570\u636e\u5173\u8054\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSODA-CitrON\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u5728\u7ebf\u805a\u7c7b\u591a\u6e90\u4f20\u611f\u5668\u7684\u9759\u6001\u7269\u4f53\u89c2\u6d4b\uff0c\u540c\u65f6\u4f30\u8ba1\u7269\u4f53\u4f4d\u7f6e\u5e76\u7ef4\u6301\u6301\u4e45\u8ddf\u8e2a\u3002\u8be5\u65b9\u6cd5\u5b8c\u5168\u5728\u7ebf\uff0c\u652f\u6301\u65f6\u95f4\u65e0\u5173\u548c\u591a\u4f20\u611f\u5668\u8f93\u5165\uff0c\u4e14\u590d\u6742\u5ea6\u5bf9\u89c2\u6d4b\u6570\u4e3a\u5bf9\u6570\u7ebf\u6027\uff0c\u7ed3\u679c\u53ef\u89e3\u91ca\u6027\u5f3a\u3002", "result": "\u5728\u4e0d\u540c\u8499\u7279\u5361\u6d1b\u4eff\u771f\u573a\u666f\u4e0b\u4e0e\u73b0\u6709\u7684\u8d1d\u53f6\u65af\u6ee4\u6ce2\u3001DBSTREAM\u805a\u7c7b\u3001JPDA\u7b49\u65b9\u6cd5\u5bf9\u6bd4\uff0cSODA-CitrON\u5728F1\u5206\u6570\u3001\u4f4d\u7f6eRMSE\u3001MOTP\u548cMOTA\u7b49\u6307\u6807\u4e0a\u59cb\u7ec8\u53d6\u5f97\u66f4\u4f18\u8868\u73b0\u3002", "conclusion": "SODA-CitrON\u5728\u9759\u6001\u76ee\u6807\u6620\u5c04\u5e94\u7528\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u591a\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u4e0e\u8ddf\u8e2a\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u76f8\u8f83\u4e8e\u4e1a\u754c\u4e3b\u6d41\u65b9\u6cd5\u5177\u5907\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2602.22347", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22347", "abs": "https://arxiv.org/abs/2602.22347", "authors": ["Audun L. Henriksen", "Ole-Johan Skrede", "Lisa van der Schee", "Enric Domingo", "Sepp De Raedt", "Ily\u00e1 Kostolomov", "Jennifer Hay", "Karolina Cyll", "Wanja Kildal", "Joakim Kalsnes", "Robert W. Williams", "Manohar Pradhan", "John Arne Nesheim", "Hanne A. Askautrud", "Maria X. Isaksen", "Karmele Saez de Gordoa", "Miriam Cuatrecasas", "Joanne Edwards", "TransSCOT group", "Arild Nesbakken", "Neil A. Shepherd", "Ian Tomlinson", "Daniel-Christoph Wagner", "Rachel S. Kerr", "Tarjei Sveinsgjerd Hveem", "Knut Liest\u00f8l", "Yoshiaki Nakamura", "Marco Novelli", "Masaaki Miyo", "Sebastian Foersch", "David N. Church", "Miangela M. Lacle", "David J. Kerr", "Andreas Kleppe"], "title": "Enabling clinical use of foundation models in histopathology", "comment": null, "summary": "Foundation models in histopathology are expected to facilitate the development of high-performing and generalisable deep learning systems. However, current models capture not only biologically relevant features, but also pre-analytic and scanner-specific variation that bias the predictions of task-specific models trained from the foundation model features. Here we show that introducing novel robustness losses during training of downstream task-specific models reduces sensitivity to technical variability. A purpose-designed comprehensive experimentation setup with 27,042 WSIs from 6155 patients is used to train thousands of models from the features of eight popular foundation models for computational pathology. In addition to a substantial improvement in robustness, we observe that prediction accuracy improves by focusing on biologically relevant features. Our approach successfully mitigates robustness issues of foundation models for computational pathology without retraining the foundation models themselves, enabling development of robust computational pathology models applicable to real-world data in routine clinical practice.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u589e\u52a0\u65b0\u9896\u7684\u9c81\u68d2\u6027\u635f\u5931\u51fd\u6570\uff0c\u589e\u5f3a\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u57fa\u7840\u6a21\u578b\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u7a33\u5b9a\u6027\uff0c\u65e0\u9700\u5bf9\u57fa\u7840\u6a21\u578b\u672c\u8eab\u8fdb\u884c\u518d\u8bad\u7ec3\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u5e94\u7528\u65f6\u4e0d\u4ec5\u63d0\u53d6\u4e86\u751f\u7269\u5b66\u76f8\u5173\u7279\u5f81\uff0c\u540c\u65f6\u4e5f\u6355\u83b7\u4e86\u4e0e\u8bbe\u5907\u3001\u9884\u5904\u7406\u7b49\u76f8\u5173\u7684\u5e72\u6270\u4fe1\u606f\uff0c\u8fd9\u4e9b\u6280\u672f\u6027\u53d8\u5f02\u4f1a\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u6027\u4e0b\u964d\u3001\u7ed3\u679c\u53d7\u504f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u7531\u6280\u672f\u53d8\u5f02\u5e26\u6765\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u9700\u8981\u63d0\u51fa\u65b0\u7684\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u5b9e\u9645\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u4e00\u81f4\u6027\u3002", "method": "\u5728\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u7279\u5f81\u7684\u57fa\u7840\u4e0a\uff0c\u9488\u5bf9\u4e0b\u6e38\u5206\u7c7b/\u9884\u6d4b\u4efb\u52a1\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u4e86\u65b0\u7684\u9c81\u68d2\u8bad\u7ec3\u635f\u5931\u51fd\u6570\uff0c\u76ee\u7684\u662f\u51cf\u5c11\u6a21\u578b\u5bf9\u6280\u672f\u6027\u53d8\u5f02\u7684\u654f\u611f\u6027\u3002\u5b9e\u9a8c\u91c7\u7528\u4e86\u6765\u81ea6155\u540d\u60a3\u8005\u768427042\u5f20\u5168\u5207\u7247\u5f71\u50cf\uff08WSIs\uff09\uff0c\u4ece8\u79cd\u4e3b\u6d41\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\u8fdb\u884c\u4e0b\u6e38\u6a21\u578b\u5f00\u53d1\u4e0e\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u52a0\u5165\u9c81\u68d2\u6027\u635f\u5931\u540e\uff0c\u6a21\u578b\u4e0d\u4ec5\u9c81\u68d2\u6027\u663e\u8457\u589e\u5f3a\uff0c\u800c\u4e14\u9884\u6d4b\u51c6\u786e\u7387\u4e5f\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u5173\u6ce8\u751f\u7269\u5b66\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u6548\u679c\u66f4\u597d\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u53ef\u5728\u4e0d\u9700\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u524d\u63d0\u4e0b\uff0c\u89e3\u51b3\u8ba1\u7b97\u75c5\u7406\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u6709\u52a9\u4e8e\u540e\u7eed\u5f00\u53d1\u53ef\u5728\u5b9e\u9645\u4e34\u5e8a\u6570\u636e\u4e2d\u5e7f\u6cdb\u5e94\u7528\u7684\u6570\u5b57\u75c5\u7406AI\u6a21\u578b\u3002"}}
{"id": "2602.22424", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22424", "abs": "https://arxiv.org/abs/2602.22424", "authors": ["Gustaw Opie\u0142ka", "Hannes Rosenbusch", "Claire E. Stevenson"], "title": "Causality $\\neq$ Invariance: Function and Concept Vectors in LLMs", "comment": null, "summary": "Do large language models (LLMs) represent concepts abstractly, i.e., independent of input format? We revisit Function Vectors (FVs), compact representations of in-context learning (ICL) tasks that causally drive task performance. Across multiple LLMs, we show that FVs are not fully invariant: FVs are nearly orthogonal when extracted from different input formats (e.g., open-ended vs. multiple-choice), even if both target the same concept. We identify Concept Vectors (CVs), which carry more stable concept representations. Like FVs, CVs are composed of attention head outputs; however, unlike FVs, the constituent heads are selected using Representational Similarity Analysis (RSA) based on whether they encode concepts consistently across input formats. While these heads emerge in similar layers to FV-related heads, the two sets are largely distinct, suggesting different underlying mechanisms. Steering experiments reveal that FVs excel in-distribution, when extraction and application formats match (e.g., both open-ended in English), while CVs generalize better out-of-distribution across both question types (open-ended vs. multiple-choice) and languages. Our results show that LLMs do contain abstract concept representations, but these differ from those that drive ICL performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u5426\u80fd\u591f\u62bd\u8c61\u5730\u8868\u793a\u6982\u5ff5\uff0c\u5373\u662f\u5426\u4e0e\u8f93\u5165\u683c\u5f0f\u65e0\u5173\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u80fd\u8de8\u8f93\u5165\u683c\u5f0f\u6cdb\u5316\u7684\u65b0\u8868\u793a\u65b9\u5f0f\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u5bf9\u4e8eLLM\u4e2d\u201c\u6982\u5ff5\u201d\u7684\u8868\u793a\u65b9\u5f0f\u53ca\u5176\u5728\u4e0d\u540c\u8f93\u5165\u683c\u5f0f\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u4ecd\u6709\u4e89\u8bae\u3002\u524d\u4eba\u63d0\u51fa\u7684Function Vectors\uff08FVs\uff09\u5e76\u4e0d\u80fd\u4fdd\u8bc1\u8de8\u683c\u5f0f\u7684\u4e00\u81f4\u8868\u793a\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u627e\u5bfb\u66f4\u62bd\u8c61\u7684\u6982\u5ff5\u5411\u91cf\u8868\u793a\u3002", "method": "\u4f5c\u8005\u5bf9\u6bd4\u4e86FVs\u548c\u65b0\u63d0\u51fa\u7684Concept Vectors\uff08CVs\uff09\u3002FVs\u901a\u8fc7\u4efb\u52a1\u9a71\u52a8\u83b7\u5f97\uff0c\u800cCVs\u901a\u8fc7\u4ee3\u8868\u6027\u76f8\u4f3c\u6027\u5206\u6790\uff08RSA\uff09\u7b5b\u9009\u51fa\u80fd\u5728\u591a\u79cd\u8f93\u5165\u683c\u5f0f\u4e2d\u4e00\u81f4\u7f16\u7801\u6982\u5ff5\u7684\u6ce8\u610f\u529b\u5934\u3002\u5b9e\u9a8c\u6d89\u53ca\u4e0d\u540c\u8f93\u5165\u683c\u5f0f\uff08\u5982\u5f00\u653e\u5f0f\u95ee\u7b54\u3001\u9009\u62e9\u9898\uff09\u548c\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "FVs\u5728\u540c\u5206\u5e03\uff08\u63d0\u53d6\u548c\u5e94\u7528\u683c\u5f0f\u4e00\u81f4\uff09\u60c5\u5883\u4e0b\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u6cdb\u5316\u6027\u4e0d\u5f3a\uff1bCVs\u8de8\u8f93\u5165\u683c\u5f0f\u548c\u8de8\u8bed\u8a00\u7684\u8fc1\u79fb\u8868\u73b0\u66f4\u597d\u3002\u9009\u51fa\u7684CV\u548cFV\u6240\u5728\u5c42\u6b21\u76f8\u8fd1\uff0c\u4f46\u5177\u4f53\u6ce8\u610f\u529b\u5934\u96c6\u57fa\u672c\u4e0d\u540c\uff0c\u6697\u793a\u5e95\u5c42\u673a\u5236\u6709\u5dee\u5f02\u3002", "conclusion": "LLM\u786e\u5b9e\u62e5\u6709\u62bd\u8c61\u7684\u6982\u5ff5\u8868\u5f81\uff0c\u4f46\u8fd9\u79cd\u8868\u5f81\u4e0e\u76f4\u63a5\u9a71\u52a8ICL\u4efb\u52a1\u8868\u73b0\u7684\u5411\u91cf\u5e76\u4e0d\u4e00\u81f4\u3002CV\u4e3a\u7814\u7a76\u548c\u5e94\u7528LLM\u7684\u6982\u5ff5\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.22346", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22346", "abs": "https://arxiv.org/abs/2602.22346", "authors": ["Mengyu Liang", "Sarah Gillet Schlegel", "Iolanda Leite"], "title": "Detection and Recognition: A Pairwise Interaction Framework for Mobile Service Robots", "comment": null, "summary": "Autonomous mobile service robots, like lawnmowers or cleaning robots, operating in human-populated environments need to reason about local human-human interactions to support safe and socially aware navigation while fulfilling their tasks. For such robots, interaction understanding is not primarily a fine-grained recognition problem, but a perception problem under limited sensing quality and computational resources. Many existing approaches focus on holistic group activity recognition, which often requires complex and large models which may not be necessary for mobile service robots. Others use pairwise interaction methods which commonly rely on skeletal representations but their use in outdoor environments remains challenging. In this work, we argue that pairwise human interaction constitute a minimal yet sufficient perceptual unit for robot-centric social understanding. We study the problem of identifying interacting person pairs and classifying coarse-grained interaction behaviors sufficient for downstream group-level reasoning and service robot decision-making. To this end, we adopt a two-stage framework in which candidate interacting pairs are first identified based on lightweight geometric and motion cues, and interaction types are subsequently classified using a relation network. We evaluate the proposed approach on the JRDB dataset, where it achieves sufficient accuracy with reduced computational cost and model size compared to appearance-based methods. Additional experiments on the Collective Activity Dataset and zero shot test on a lawnmower-collected dataset further illustrate the generality of the proposed framework. These results suggest that pairwise geometric and motion cues provide a practical basis for interaction perception on mobile service robot providing a promising method for integration into mobile robot navigation stacks in future work. Code will be released soon", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u79fb\u52a8\u670d\u52a1\u673a\u5668\u4eba\uff08\u5982\u5272\u8349\u673a\u548c\u6e05\u6d01\u673a\u5668\u4eba\uff09\u7684\u4eba\u9645\u4e92\u52a8\u611f\u77e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5148\u68c0\u6d4b\u6210\u5bf9\u7684\u4eba\u7269\u4e92\u52a8\uff0c\u518d\u8fdb\u884c\u7c97\u7c92\u5ea6\u884c\u4e3a\u5206\u7c7b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u4f4e\u8ba1\u7b97\u91cf\u7684\u793e\u4f1a\u611f\u77e5\uff0c\u4e3a\u673a\u5668\u4eba\u5b89\u5168\u3001\u793e\u4f1a\u5316\u5bfc\u822a\u63d0\u4f9b\u4e86\u57fa\u7840\u3002\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u8f83\u597d\u6cdb\u5316\u80fd\u529b\u548c\u8f83\u4f4e\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u670d\u52a1\u673a\u5668\u4eba\u5728\u6709\u4eba\u73af\u5883\u4e2d\u987b\u611f\u77e5\u4eba\u9645\u4e92\u52a8\u4ee5\u5b89\u5168\u3001\u5408\u7406\u5b8c\u6210\u5bfc\u822a\u4efb\u52a1\u3002\u73b0\u6709\u65b9\u6cd5\u4e0d\u662f\u8ba1\u7b97\u91cf\u5927\u3001\u6a21\u578b\u5e9e\u5927\uff0c\u5c31\u662f\u53d7\u9650\u4e8e\u5bf9\u9aa8\u9abc\u4fe1\u606f\u7684\u4f9d\u8d56\uff0c\u4e0d\u9002\u5408\u79fb\u52a8\u5e73\u53f0\u548c\u6237\u5916\u73af\u5883\u3002\u672c\u5de5\u4f5c\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u8d44\u6e90\u6d88\u8017\u5c11\u3001\u80fd\u6ee1\u8db3\u673a\u5668\u4eba\u76ee\u6807\u7684\u611f\u77e5\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u6b65\u7528\u8f7b\u91cf\u7ea7\u7684\u51e0\u4f55\u4e0e\u8fd0\u52a8\u7ebf\u7d22\u68c0\u6d4b\u4e92\u52a8\u4eba\u7269\u5bf9\uff0c\u7b2c\u4e8c\u6b65\u5229\u7528\u5173\u7cfb\u7f51\u7edc\u5bf9\u4e92\u52a8\u7c7b\u578b\u8fdb\u884c\u7c97\u5206\u7c7b\u3002\u4e3b\u8981\u5173\u6ce8\u914d\u5bf9\u4eba\u7269\u800c\u975e\u6574\u4f53\u7fa4\u4f53\u6d3b\u52a8\uff0c\u5e76\u6bd4\u57fa\u4e8e\u5916\u89c2\u7684\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002", "result": "\u5728JRDB\u6570\u636e\u96c6\u4e0a\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8f83\u4f4e\u8ba1\u7b97\u8d44\u6e90\u4e0e\u6a21\u578b\u4f53\u79ef\u4e0b\u83b7\u5f97\u4e86\u8db3\u591f\u7684\u4e92\u52a8\u8bc6\u522b\u51c6\u786e\u7387\u3002\u5bf9Collective Activity\u6570\u636e\u96c6\u548c\u5272\u8349\u673a\u573a\u666f\u65b0\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8868\u73b0\u51fa\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u5b9e\u7528\u6027\u3002", "conclusion": "\u6210\u5bf9\uff08pairwise\uff09\u7684\u51e0\u4f55\u548c\u8fd0\u52a8\u4fe1\u606f\u662f\u79fb\u52a8\u670d\u52a1\u673a\u5668\u4eba\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u4eba\u9645\u4e92\u52a8\u611f\u77e5\u57fa\u7840\uff0c\u9002\u5408\u96c6\u6210\u5230\u672a\u6765\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\u4e2d\u3002\u4ee3\u7801\u5373\u5c06\u516c\u5f00\u3002"}}
{"id": "2602.22361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22361", "abs": "https://arxiv.org/abs/2602.22361", "authors": ["Liping Meng", "Fan Nie", "Yunyun Zhang", "Chao Han"], "title": "Optimizing Neural Network Architecture for Medical Image Segmentation Using Monte Carlo Tree Search", "comment": null, "summary": "This paper proposes a novel medical image segmentation framework, MNAS-Unet, which combines Monte Carlo Tree Search (MCTS) and Neural Architecture Search (NAS). MNAS-Unet dynamically explores promising network architectures through MCTS, significantly enhancing the efficiency and accuracy of architecture search. It also optimizes the DownSC and UpSC unit structures, enabling fast and precise model adjustments. Experimental results demonstrate that MNAS-Unet outperforms NAS-Unet and other state-of-the-art models in segmentation accuracy on several medical image datasets, including PROMISE12, Ultrasound Nerve, and CHAOS. Furthermore, compared with NAS-Unet, MNAS-Unet reduces the architecture search budget by 54% (early stopping at 139 epochs versus 300 epochs under the same search setting), while achieving a lightweight model with only 0.6M parameters and lower GPU memory consumption, which further improves its practical applicability. These results suggest that MNAS-Unet can improve search efficiency while maintaining competitive segmentation accuracy under practical resource constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u4e0e\u795e\u7ecf\u7ed3\u6784\u641c\u7d22(NAS)\u7684\u65b0\u578b\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6MNAS-Unet\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u4e0e\u641c\u7d22\u6548\u7387\uff0c\u5e76\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u5728\u63d0\u5347\u6a21\u578b\u7cbe\u5ea6\u548c\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4f20\u7edfNAS\u65b9\u6cd5\u641c\u7d22\u6548\u7387\u4e0d\u9ad8\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u65b0\u7684\u65b9\u6cd5\u517c\u987e\u9ad8\u6548\u67b6\u6784\u641c\u7d22\u548c\u5206\u5272\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u5c06MCTS\u5f15\u5165NAS\u6d41\u7a0b\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u641c\u7d22\uff0c\u4f18\u5316\u4e86Unet\u4e0b\u91c7\u6837\u548c\u4e0a\u91c7\u6837\u7ed3\u6784(DownSC\u548cUpSC\u5355\u5143)\u3002\u540c\u65f6\u901a\u8fc7\u65e9\u505c\u7b56\u7565\u51cf\u5c11\u8bad\u7ec3\u5468\u671f\uff0c\u4f7f\u641c\u7d22\u8fc7\u7a0b\u66f4\u5feb\u4e14\u8d44\u6e90\u5360\u7528\u66f4\u4f4e\u3002", "result": "\u5728PROMISE12\u3001\u8d85\u58f0\u795e\u7ecf\u548cCHAOS\u7b49\u591a\u4e2a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\uff0cMNAS-Unet\u5206\u5272\u7cbe\u5ea6\u4f18\u4e8eNAS-Unet\u53ca\u5176\u5b83\u5148\u8fdb\u6a21\u578b\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u5c06\u641c\u7d22\u9884\u7b97\u51cf\u5c1154%\uff0c\u53c2\u6570\u4ec50.6M\uff0c\u4e14\u5360\u7528GPU\u5185\u5b58\u66f4\u4f4e\u3002", "conclusion": "MNAS-Unet\u5728\u4fdd\u8bc1\u5206\u5272\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u641c\u7d22\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u4e2d\u5bf9\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u573a\u666f\u3002"}}
{"id": "2602.22449", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22449", "abs": "https://arxiv.org/abs/2602.22449", "authors": ["Mirza Raquib", "Asif Pervez Polok", "Kedar Nath Biswas", "Rahat Uddin Azad", "Saydul Akbar Murad", "Nick Rahimi"], "title": "A Fusion of context-aware based BanglaBERT and Two-Layer Stacked LSTM Framework for Multi-Label Cyberbullying Detection", "comment": null, "summary": "Cyberbullying has become a serious and growing concern in todays virtual world. When left unnoticed, it can have adverse consequences for social and mental health. Researchers have explored various types of cyberbullying, but most approaches use single-label classification, assuming that each comment contains only one type of abuse. In reality, a single comment may include overlapping forms such as threats, hate speech, and harassment. Therefore, multilabel detection is both realistic and essential. However, multilabel cyberbullying detection has received limited attention, especially in low-resource languages like Bangla, where robust pre-trained models are scarce. Developing a generalized model with moderate accuracy remains challenging. Transformers offer strong contextual understanding but may miss sequential dependencies, while LSTM models capture temporal flow but lack semantic depth. To address these limitations, we propose a fusion architecture that combines BanglaBERT-Large with a two-layer stacked LSTM. We analyze their behavior to jointly model context and sequence. The model is fine-tuned and evaluated on a publicly available multilabel Bangla cyberbullying dataset covering cyberbully, sexual harassment, threat, and spam. We apply different sampling strategies to address class imbalance. Evaluation uses multiple metrics, including accuracy, precision, recall, F1-score, Hamming loss, Cohens kappa, and AUC-ROC. We employ 5-fold cross-validation to assess the generalization of the architecture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06BanglaBERT-Large\u4e0e\u53cc\u5c42LSTM\u878d\u5408\uff0c\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5b5f\u52a0\u62c9\u8bed\uff09\u7684\u591a\u6807\u7b7e\u7f51\u7edc\u6b3a\u51cc\u68c0\u6d4b\uff0c\u53d6\u5f97\u8f83\u597d\u6027\u80fd\u3002", "motivation": "\u591a\u6807\u7b7e\u7f51\u7edc\u6b3a\u51cc\u68c0\u6d4b\u66f4\u8d34\u8fd1\u73b0\u5b9e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u5355\u6807\u7b7e\uff0c\u4e14\u5728\u5b5f\u52a0\u62c9\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7f3a\u4e4f\u6709\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u540c\u65f6\u878d\u5408BanglaBERT-Large\uff08\u6355\u6349\u4e0a\u4e0b\u6587\u8bed\u4e49\uff09\u4e0e\u53cc\u5c42LSTM\uff08\u5efa\u6a21\u5e8f\u5217\u4f9d\u8d56\uff09\u7684\u7f51\u7edc\u7ed3\u6784\uff0c\u5728\u516c\u5f00\u7684\u591a\u6807\u7b7e\u5b5f\u52a0\u62c9\u8bed\u7f51\u7edc\u6b3a\u51cc\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u91c7\u7528\u591a\u79cd\u91c7\u6837\u7b56\u7565\u4ee5\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u3002\u901a\u8fc7\u591a\u6307\u6807\u548c5\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u5305\u62ec\u7f51\u7edc\u6b3a\u51cc\u3001\u6027\u9a9a\u6270\u3001\u5a01\u80c1\u548c\u5783\u573e\u5185\u5bb9\u7b49\u591a\u6807\u7b7e\u4efb\u52a1\u4e0a\uff0c\u6240\u63d0\u6a21\u578b\u5728\u591a\u9879\u8bc4\u4ef7\u6307\u6807\uff08\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u3001Hamming\u635f\u5931\u3001Cohen's kappa\u3001AUC-ROC\uff09\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u7ed3\u5408Transformer\u548cLSTM\u7ed3\u6784\u53ef\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u591a\u6807\u7b7e\u7f51\u7edc\u6b3a\u51cc\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u7814\u7a76\u4e3a\u76f8\u5173\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22459", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.22459", "abs": "https://arxiv.org/abs/2602.22459", "authors": ["Yicheng Chen", "Jinjie Li", "Haokun Liu", "Zicheng Luo", "Kotaro Kaneko", "Moju Zhao"], "title": "Hierarchical Trajectory Planning of Floating-Base Multi-Link Robot for Maneuvering in Confined Environments", "comment": "Accepted to IEEE T-ASE; DOI pending", "summary": "Floating-base multi-link robots can change their shape during flight, making them well-suited for applications in confined environments such as autonomous inspection and search and rescue. However, trajectory planning for such systems remains an open challenge because the problem lies in a high-dimensional, constraint-rich space where collision avoidance must be addressed together with kinematic limits and dynamic feasibility. This work introduces a hierarchical trajectory planning framework that integrates global guidance with configuration-aware local optimization. First, we exploit the dual nature of these robots - the root link as a rigid body for guidance and the articulated joints for flexibility - to generate global anchor states that decompose the planning problem into tractable segments. Second, we design a local trajectory planner that optimizes each segment in parallel with differentiable objectives and constraints, systematically enforcing kinematic feasibility and maintaining dynamic feasibility by avoiding control singularities. Third, we implement a complete system that directly processes point-cloud data, eliminating the need for handcrafted obstacle models. Extensive simulations and real-world experiments confirm that this framework enables an articulated aerial robot to exploit its morphology for maneuvering that rigid robots cannot achieve. To the best of our knowledge, this is the first planning framework for floating-base multi-link robots that has been demonstrated on a real robot to generate continuous, collision-free, and dynamically feasible trajectories directly from raw point-cloud inputs, without relying on handcrafted obstacle models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u6d6e\u52a8\u57fa\u591a\u8fde\u6746\u673a\u5668\u4eba\u7684\u9ad8\u6548\u81ea\u4e3b\u8fd0\u52a8\uff0c\u80fd\u4ece\u70b9\u4e91\u6570\u636e\u76f4\u63a5\u751f\u6210\u8fde\u7eed\u3001\u65e0\u78b0\u649e\u4e14\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u6d6e\u52a8\u57fa\u591a\u8fde\u6746\u673a\u5668\u4eba\u80fd\u5728\u72ed\u5c0f\u7a7a\u95f4\u7075\u6d3b\u4f5c\u4e1a\uff0c\u5982\u81ea\u4e3b\u68c0\u67e5\u548c\u641c\u6551\uff0c\u4f46\u5176\u8f68\u8ff9\u89c4\u5212\u56e0\u7a7a\u95f4\u7ef4\u5ea6\u9ad8\u3001\u7ea6\u675f\u591a\u53ca\u590d\u6742\u907f\u969c\u8981\u6c42\uff0c\u957f\u671f\u672a\u5f97\u5230\u6709\u6548\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff1a\u9996\u5148\u5c06\u89c4\u5212\u95ee\u9898\u5206\u89e3\u4e3a\u82e5\u5e72\u5168\u5c40\u951a\u70b9\uff0c\u901a\u8fc7\u6839\u90e8\u8fde\u6746\u5bfc\u5f15\u4e0e\u5173\u8282\u67d4\u6027\u7ed3\u5408\u8bbe\u8ba1\uff1b\u7136\u540e\u4e3a\u6bcf\u6bb5\u4f7f\u7528\u5e26\u53ef\u5fae\u7ea6\u675f\u4e0e\u76ee\u6807\u51fd\u6570\u7684\u5c40\u90e8\u8f68\u8ff9\u4f18\u5316\u5668\uff0c\u4fdd\u8bc1\u52a8\u529b\u5b66\u548c\u8fd0\u52a8\u5b66\u53ef\u884c\uff1b\u6b64\u5916\uff0c\u7cfb\u7edf\u80fd\u76f4\u63a5\u5904\u7406\u70b9\u4e91\uff0c\u65e0\u9700\u624b\u5de5\u969c\u788d\u5efa\u6a21\uff0c\u5e76\u884c\u4f18\u5316\u6bcf\u6bb5\u89c4\u5212\u3002", "result": "\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u80fd\u8ba9\u591a\u8fde\u6746\u98de\u884c\u673a\u5668\u4eba\u5b9e\u73b0\u521a\u6027\u673a\u5668\u4eba\u65e0\u6cd5\u5b8c\u6210\u7684\u590d\u6742\u673a\u52a8\u8fd0\u52a8\uff0c\u80fd\u76f4\u63a5\u4ece\u70b9\u4e91\u751f\u6210\u8fde\u7eed\u3001\u65e0\u78b0\u649e\u4e14\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "conclusion": "\u672c\u5de5\u4f5c\u5b9e\u73b0\u4e86\u9996\u4e2a\u80fd\u4ece\u539f\u59cb\u70b9\u4e91\u6570\u636e\u51fa\u53d1\uff0c\u4e0d\u4f9d\u8d56\u4eba\u5de5\u969c\u788d\u5efa\u6a21\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u6709\u6548\u7684\u6d6e\u52a8\u57fa\u591a\u8fde\u6746\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\u7cfb\u7edf\uff0c\u5bf9\u76f8\u5173\u5e94\u7528\u548c\u540e\u7eed\u7814\u7a76\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.22376", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22376", "abs": "https://arxiv.org/abs/2602.22376", "authors": ["Hanyang Liu", "Rongjun Qin"], "title": "AeroDGS: Physically Consistent Dynamic Gaussian Splatting for Single-Sequence Aerial 4D Reconstruction", "comment": "Accepted to CVPR 2026", "summary": "Recent advances in 4D scene reconstruction have significantly improved dynamic modeling across various domains. However, existing approaches remain limited under aerial conditions with single-view capture, wide spatial range, and dynamic objects of limited spatial footprint and large motion disparity. These challenges cause severe depth ambiguity and unstable motion estimation, making monocular aerial reconstruction inherently ill-posed. To this end, we present AeroDGS, a physics-guided 4D Gaussian splatting framework for monocular UAV videos. AeroDGS introduces a Monocular Geometry Lifting module that reconstructs reliable static and dynamic geometry from a single aerial sequence, providing a robust basis for dynamic estimation. To further resolve monocular ambiguity, we propose a Physics-Guided Optimization module that incorporates differentiable ground-support, upright-stability, and trajectory-smoothness priors, transforming ambiguous image cues into physically consistent motion. The framework jointly refines static backgrounds and dynamic entities with stable geometry and coherent temporal evolution. We additionally build a real-world UAV dataset that spans various altitudes and motion conditions to evaluate dynamic aerial reconstruction. Experiments on synthetic and real UAV scenes demonstrate that AeroDGS outperforms state-of-the-art methods, achieving superior reconstruction fidelity in dynamic aerial environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5355\u76ee\u65e0\u4eba\u673a(UAV)\u89c6\u9891\u76844D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u65b9\u6cd5AeroDGS\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u89c6\u89d2\u822a\u62cd\u52a8\u6001\u573a\u666f\u4e2d\u7684\u6df1\u5ea6\u548c\u8fd0\u52a8\u4f30\u8ba1\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u822a\u62cd\u573a\u666f\u7684\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u67094D\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u5728\u52a8\u6001\u3001\u5355\u955c\u5934\u822a\u62cd\u6761\u4ef6\u4e0b\u8868\u73b0\u53d7\u9650\uff0c\u5c24\u5176\u96be\u4ee5\u5e94\u5bf9\u7a7a\u95f4\u8986\u76d6\u5e7f\u3001\u76ee\u6807\u52a8\u6001\u5267\u70c8\u4e14\u7a7a\u95f4\u5c3a\u5ea6\u6709\u9650\u7684\u573a\u666f\u3002\u4f20\u7edf\u65b9\u6cd5\u5e38\u51fa\u73b0\u4e25\u91cd\u6df1\u5ea6\u6b67\u4e49\u548c\u8fd0\u52a8\u4f30\u8ba1\u4e0d\u7a33\u5b9a\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u65b0\u7684\u65b9\u6cd5\u63d0\u5347\u5355\u76ee\u822a\u62cd\u52a8\u6001\u573a\u666f\u91cd\u5efa\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u5ea6\u3002", "method": "\u63d0\u51faAeroDGS\u6846\u67b6\uff0c\u5229\u7528\u7269\u7406\u5148\u9a8c\u5f15\u5bfc\u76844D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u3002\u4e3b\u8981\u521b\u65b0\uff1a1\uff09\u8bbe\u8ba1\u5355\u76ee\u51e0\u4f55\u63d0\u5347\u6a21\u5757\uff0c\u4ece\u4e00\u6bb5\u822a\u62cd\u5e8f\u5217\u91cd\u5efa\u53ef\u9760\u7684\u9759\u6001\u548c\u52a8\u6001\u51e0\u4f55\u4fe1\u606f\uff0c2\uff09\u5f15\u5165\u7269\u7406\u5f15\u5bfc\u4f18\u5316\u6a21\u5757\uff0c\u6574\u5408\u53ef\u5fae\u5206\u5730\u9762\u652f\u6301\u3001\u7ad6\u76f4\u7a33\u5b9a\u3001\u8f68\u8ff9\u5e73\u6ed1\u7b49\u7269\u7406\u5148\u9a8c\uff0c\u5c06\u56fe\u50cf\u63d0\u793a\u8f6c\u5316\u4e3a\u7269\u7406\u4e00\u81f4\u7684\u8fd0\u52a8\u4f30\u8ba1\u3002\u6574\u4f53\u6846\u67b6\u5b9e\u73b0\u80cc\u666f\u4e0e\u52a8\u6001\u5bf9\u8c61\u7684\u8054\u5408\u4f18\u5316\u3002", "result": "\u5728\u6784\u5efa\u7684\u771f\u5b9e\u65e0\u4eba\u673a\u6570\u636e\u96c6\u548c\u516c\u5f00\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0cAeroDGS\u5728\u52a8\u6001\u822a\u62cd\u91cd\u5efa\u7cbe\u5ea6\u65b9\u9762\u5747\u8d85\u8d8a\u4e86\u6700\u65b0\u76f8\u5173\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5927\u8303\u56f4\u590d\u6742\u8fd0\u52a8\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "AeroDGS\u4e3a\u5355\u76ee\u65e0\u4eba\u673a\u52a8\u6001\u573a\u666f\u9ad8\u4fdd\u771f\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002\u7269\u7406\u5f15\u5bfc\u548c\u51e0\u4f55\u63d0\u5347\u6a21\u5757\u6709\u6548\u7f13\u89e3\u4e86\u6df1\u5ea6\u548c\u8fd0\u52a8\u6b67\u4e49\uff0c\u63a8\u52a8\u4e86\u52a8\u6001\u822a\u62cd\u91cd\u5efa\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2602.22453", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22453", "abs": "https://arxiv.org/abs/2602.22453", "authors": ["Shaswat Patel", "Vishvesh Trivedi", "Yue Han", "Yihuai Hong", "Eunsol Choi"], "title": "Bridging Latent Reasoning and Target-Language Generation via Retrieval-Transition Heads", "comment": null, "summary": "Recent work has identified a subset of attention heads in Transformer as retrieval heads, which are responsible for retrieving information from the context. In this work, we first investigate retrieval heads in multilingual contexts. In multilingual language models, we find that retrieval heads are often shared across multiple languages. Expanding the study to cross-lingual setting, we identify Retrieval-Transition heads(RTH), which govern the transition to specific target-language output. Our experiments reveal that RTHs are distinct from retrieval heads and more vital for Chain-of-Thought reasoning in multilingual LLMs. Across four multilingual benchmarks (MMLU-ProX, MGSM, MLQA, and XQuaD) and two model families (Qwen-2.5 and Llama-3.1), we demonstrate that masking RTH induces bigger performance drop than masking Retrieval Heads (RH). Our work advances understanding of multilingual LMs by isolating the attention heads responsible for mapping to target languages.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u8bed\u8a00Transformer\u4e2d\u7684\u68c0\u7d22\u5934\u548c\u65b0\u53d1\u73b0\u7684\u68c0\u7d22-\u8f6c\u79fb\u5934\uff08RTH\uff09\uff0c\u5e76\u8bc1\u660eRTH\u5bf9\u591a\u8bed\u8a00\u5927\u6a21\u578b\u7684\u63a8\u7406\u66f4\u4e3a\u5173\u952e\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u53d1\u73b0Transformer\u7684\u90e8\u5206\u6ce8\u610f\u529b\u5934\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5728\u591a\u8bed\u8a00\u53ca\u8de8\u8bed\u8a00\u573a\u666f\u4e0b\u8fd9\u4e9b\u5934\u7684\u5177\u4f53\u4f5c\u7528\u5c1a\u4e0d\u6e05\u695a\u3002\u4f5c\u8005\u65e8\u5728\u6df1\u5165\u5256\u6790\u591a\u8bed\u8a00\u5927\u6a21\u578b\u4e2d\u8d1f\u8d23\u68c0\u7d22\u548c\u8de8\u8bed\u8a00\u8f6c\u6362\u7684\u6ce8\u610f\u529b\u5934\u673a\u5236\u3002", "method": "\u4f5c\u8005\u5728\u591a\u8bed\u8a00Transformer\u6a21\u578b\u4e2d\u5206\u6790\u5e76\u6807\u6ce8\u4e86\u68c0\u7d22\u5934\u662f\u5426\u4f1a\u5728\u591a\u79cd\u8bed\u8a00\u95f4\u5171\u4eab\uff0c\u5e76\u8fdb\u4e00\u6b65\u5728\u8de8\u8bed\u8a00\u4efb\u52a1\u4e2d\u8bc6\u522b\u51fa\u65b0\u7684RTH\u3002\u901a\u8fc7\u5728\u591a\u4e2a\u591a\u8bed\u8a00\u8bc4\u6d4b\u6570\u636e\u96c6\uff08MMLU-ProX, MGSM, MLQA, XQuaD\uff09\u548c\u4e24\u7c7b\u4e3b\u6d41\u6a21\u578b\uff08Qwen-2.5\u548cLlama-3.1\uff09\u4e0a\uff0c\u901a\u8fc7mask\uff08\u5c4f\u853d\uff09\u8fd9\u4e9b\u5934\u6765\u6bd4\u8f83\u5176\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u68c0\u7d22\u5934\u5728\u591a\u8bed\u8a00\u95f4\u5e38\u88ab\u5171\u4eab\u3002\u800cRTH\u5728\u5c06\u63a8\u7406\u8f6c\u5316\u5230\u76ee\u6807\u8bed\u8a00\u8f93\u51fa\u65f6\u53d1\u6325\u5173\u952e\u4f5c\u7528\u3002\u5c4f\u853dRTH\u5bf9\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\u663e\u8457\u5927\u4e8e\u4ec5\u5c4f\u853d\u68c0\u7d22\u5934\u3002", "conclusion": "\u672c\u6587\u63a8\u52a8\u4e86\u5bf9\u591a\u8bed\u8a00Transformer\u5185\u90e8\u673a\u5236\u7684\u7406\u89e3\uff0c\u9996\u6b21\u660e\u786e\u533a\u5206\u4e86\u8de8\u8bed\u8a00\u6620\u5c04\u6240\u9700\u7684\u5173\u952e\u6ce8\u610f\u529b\u5934\uff08RTH\uff09\uff0c\u6307\u51faRTH\u6bd4\u4f20\u7edf\u68c0\u7d22\u5934\u5bf9\u591a\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u66f4\u4e3a\u5173\u952e\u3002"}}
{"id": "2602.22461", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22461", "abs": "https://arxiv.org/abs/2602.22461", "authors": ["Daesol Cho", "Youngseok Jang", "Danfei Xu", "Sehoon Ha"], "title": "EgoAVFlow: Robot Policy Learning with Active Vision from Human Egocentric Videos via 3D Flow", "comment": null, "summary": "Egocentric human videos provide a scalable source of manipulation demonstrations; however, deploying them on robots requires active viewpoint control to maintain task-critical visibility, which human viewpoint imitation often fails to provide due to human-specific priors. We propose EgoAVFlow, which learns manipulation and active vision from egocentric videos through a shared 3D flow representation that supports geometric visibility reasoning and transfers without robot demonstrations. EgoAVFlow uses diffusion models to predict robot actions, future 3D flow, and camera trajectories, and refines viewpoints at test time with reward-maximizing denoising under a visibility-aware reward computed from predicted motion and scene geometry. Real-world experiments under actively changing viewpoints show that EgoAVFlow consistently outperforms prior human-demo-based baselines, demonstrating effective visibility maintenance and robust manipulation without robot demonstrations.", "AI": {"tldr": "EgoAVFlow\u662f\u4e00\u79cd\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u89c6\u89d2\u89c6\u9891\uff0c\u8ba9\u673a\u5668\u4eba\u540c\u65f6\u5b66\u4e60\u64cd\u4f5c\u548c\u4e3b\u52a8\u89c6\u89c9\u63a7\u5236\uff0c\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u5173\u952e\u53ef\u89c1\u6027\uff0c\u5b9e\u73b0\u65e0\u9700\u673a\u5668\u4eba\u793a\u8303\u7684\u6cdb\u5316\u64cd\u63a7\u3002", "motivation": "\u4eba\u7c7b\u5934\u6234\u76f8\u673a\u62cd\u6444\u7684\u5927\u91cf\u64cd\u4f5c\u89c6\u9891\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e30\u5bcc\u8d44\u6e90\uff0c\u4f46\u4eba\u7c7b\u5728\u6267\u884c\u4efb\u52a1\u65f6\u7684\u89c6\u89d2\u7ecf\u5e38\u5e76\u4e0d\u9002\u5408\u673a\u5668\u4eba\uff0c\u56e0\u4e3a\u4eba\u7c7b\u4f9d\u8d56\u4e00\u4e9b\u673a\u5668\u4eba\u6ca1\u6709\u7684\u5148\u9a8c\u3002\u4f20\u7edf\u7684\u505a\u6cd5\u5f88\u96be\u76f4\u63a5\u5c06\u4eba\u7c7b\u89c6\u9891\u7684\u89c6\u89d2\u7b56\u7565\u8fc1\u79fb\u5230\u673a\u5668\u4eba\u4e0a\uff0c\u4f1a\u5bfc\u81f4\u673a\u5668\u4eba\u5728\u64cd\u4f5c\u4e2d\u65e0\u6cd5\u4fdd\u6301\u5bf9\u5173\u952e\u4efb\u52a1\u533a\u57df\u7684\u53ef\u89c1\u6027\u3002", "method": "\u63d0\u51faEgoAVFlow\uff0c\u7528\u7edf\u4e00\u7684\u4e09\u7ef4\u6d41\uff083D flow\uff09\u8868\u5f81\u540c\u65f6\u5b66\u4e60\u64cd\u4f5c\u52a8\u4f5c\u548c\u4e3b\u52a8\u8c03\u6574\u76f8\u673a\u89c6\u89d2\u3002\u901a\u8fc7\u6269\u6563\u6a21\u578b\u9884\u6d4b\u673a\u5668\u4eba\u7684\u52a8\u4f5c\u3001\u672a\u6765\u7684\u4e09\u7ef4\u6d41\u548c\u76f8\u673a\u8f68\u8ff9\uff1b\u5728\u6d4b\u8bd5\u65f6\uff0c\u901a\u8fc7\u5e26\u6709\u53ef\u89c1\u6027\u5956\u52b1\u7684\u53bb\u566a\u6b65\u9aa4\u81ea\u9002\u5e94\u8c03\u6574\u76f8\u673a\u89c6\u89d2\uff0c\u65e0\u9700\u673a\u5668\u4eba\u5b9e\u9645\u793a\u8303\u6765\u8bad\u7ec3\u3002", "result": "\u5728\u73b0\u5b9e\u73af\u5883\u4e0b\u3001\u4e0d\u65ad\u4e3b\u52a8\u6539\u53d8\u89c6\u89d2\u7684\u5b9e\u9a8c\u4e2d\uff0cEgoAVFlow\u5728\u4fdd\u6301\u4efb\u52a1\u5173\u952e\u533a\u57df\u53ef\u89c1\u6027\u3001\u64cd\u63a7\u4efb\u52a1\u6210\u529f\u7387\u7b49\u65b9\u9762\uff0c\u6027\u80fd\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u4eba\u7c7b\u6f14\u793a\u7c7b\u65b9\u6cd5\u3002", "conclusion": "EgoAVFlow\u4e3a\u673a\u5668\u4eba\u4ece\u4eba\u7c7b\u81ea\u89c6\u89d2\u89c6\u9891\u540c\u65f6\u5b66\u4e60\u64cd\u63a7\u4e0e\u4e3b\u52a8\u89c6\u89c9\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u624b\u6bb5\uff0c\u65e0\u9700\u673a\u5668\u4eba\u793a\u8303\uff0c\u80fd\u66f4\u597d\u5730\u7ef4\u6301\u5173\u952e\u53ef\u89c1\u6027\uff0c\u63a8\u52a8\u4e86\u4eba\u7c7b\u6f14\u793a\u6cdb\u5316\u5230\u673a\u5668\u4eba\u81ea\u4e3b\u5b66\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.22381", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22381", "abs": "https://arxiv.org/abs/2602.22381", "authors": ["Zhengkang Fan", "Chengkun Sun", "Russell Terry", "Jie Xu", "Longin Jan Latecki"], "title": "Enhancing Renal Tumor Malignancy Prediction: Deep Learning with Automatic 3D CT Organ Focused Attention", "comment": "5 pages, 2 figures, Accepted at IEEE ISBI 2026", "summary": "Accurate prediction of malignancy in renal tumors is crucial for informing clinical decisions and optimizing treatment strategies. However, existing imaging modalities lack the necessary accuracy to reliably predict malignancy before surgical intervention. While deep learning has shown promise in malignancy prediction using 3D CT images, traditional approaches often rely on manual segmentation to isolate the tumor region and reduce noise, which enhances predictive performance. Manual segmentation, however, is labor-intensive, costly, and dependent on expert knowledge. In this study, a deep learning framework was developed utilizing an Organ Focused Attention (OFA) loss function to modify the attention of image patches so that organ patches attend only to other organ patches. Hence, no segmentation of 3D renal CT images is required at deployment time for malignancy prediction. The proposed framework achieved an AUC of 0.685 and an F1-score of 0.872 on a private dataset from the UF Integrated Data Repository (IDR), and an AUC of 0.760 and an F1-score of 0.852 on the publicly available KiTS21 dataset. These results surpass the performance of conventional models that rely on segmentation-based cropping for noise reduction, demonstrating the frameworks ability to enhance predictive accuracy without explicit segmentation input. The findings suggest that this approach offers a more efficient and reliable method for malignancy prediction, thereby enhancing clinical decision-making in renal cancer diagnosis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u624b\u52a8\u5206\u5272\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u80be\u810f\u80bf\u7624\u6076\u6027\u9884\u6d4b\uff0c\u6781\u5927\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u9884\u6d4b\u6027\u80fd\u8d85\u8fc7\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u80be\u810f\u80bf\u7624\u5f71\u50cf\u9884\u6d4b\u6076\u6027\u7a0b\u5ea6\u7684\u65b9\u6cd5\u5728\u624b\u672f\u524d\u96be\u4ee5\u8fbe\u5230\u9ad8\u51c6\u786e\u7387\uff0c\u4f20\u7edf\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6848\u53c8\u5f80\u5f80\u4f9d\u8d56\u7e41\u7410\u7684\u624b\u52a8\u5206\u5272\uff0c\u8017\u65f6\u8017\u529b\uff0c\u5f71\u54cd\u4e34\u5e8a\u63a8\u5e7f\u3002\u6025\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5668\u5b98\u4e13\u6ce8\u6ce8\u610f\u529b\uff08OFA\uff09\u635f\u5931\u51fd\u6570\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u6a21\u578b\u81ea\u52a8\u5173\u6ce8\u5668\u5b98\u533a\u57df\uff0c\u5b9e\u73b0\u80bf\u7624\u6076\u6027\u9884\u6d4b\uff0c\u65e0\u9700\u624b\u52a8\u5206\u52723D\u80be\u810fCT\u56fe\u50cf\u3002\u5206\u522b\u5728UF IDR\u79c1\u6709\u6570\u636e\u96c6\u548cKiTS21\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5728UF IDR\u6570\u636e\u96c6\u4e0a\uff0cAUC\u4e3a0.685\uff0cF1\u4e3a0.872\uff1b\u5728KiTS21\u6570\u636e\u96c6\u4e0a\uff0cAUC\u4e3a0.760\uff0cF1\u4e3a0.852\u3002\u6027\u80fd\u5747\u4f18\u4e8e\u9700\u8981\u5206\u5272\u7684\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0d\u4f9d\u8d56\u660e\u786e\u5206\u5272\u7684\u80be\u810f\u80bf\u7624\u6076\u6027\u5ea6\u9884\u6d4b\uff0c\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u6709\u671b\u4e3a\u80be\u764c\u8bca\u7597\u51b3\u7b56\u63d0\u4f9b\u66f4\u4f18\u5e2e\u52a9\u3002"}}
{"id": "2602.22475", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22475", "abs": "https://arxiv.org/abs/2602.22475", "authors": ["Binchi Zhang", "Xujiang Zhao", "Jundong Li", "Haifeng Chen", "Zhengzhang Chen"], "title": "Mind the Gap in Cultural Alignment: Task-Aware Culture Management for Large Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in culturally sensitive real-world tasks. However, existing cultural alignment approaches fail to align LLMs' broad cultural values with the specific goals of downstream tasks and suffer from cross-culture interference. We propose CultureManager, a novel pipeline for task-specific cultural alignment. CultureManager synthesizes task-aware cultural data in line with target task formats, grounded in culturally relevant web search results. To prevent conflicts between cultural norms, it manages multi-culture knowledge learned in separate adapters with a culture router that selects the appropriate one to apply. Experiments across ten national cultures and culture-sensitive tasks show consistent improvements over prompt-based and fine-tuning baselines. Our results demonstrate the necessity of task adaptation and modular culture management for effective cultural alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCultureManager\uff0c\u4e00\u79cd\u9488\u5bf9\u4efb\u52a1\u7684\u6587\u5316\u5bf9\u9f50\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u4efb\u52a1\u76f8\u5173\u7684\u6587\u5316\u6570\u636e\u548c\u6a21\u5757\u5316\u7ba1\u7406\u591a\u79cd\u6587\u5316\u77e5\u8bc6\uff0c\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u66f4\u7cbe\u51c6\u7684\u6587\u5316\u9002\u5e94\uff0c\u6709\u6548\u63d0\u5347\u591a\u56fd\u6587\u5316\u4e0b\u654f\u611f\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709LLM\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u9700\u8981\u6587\u5316\u654f\u611f\u6027\uff0c\u4f46\u76ee\u524d\u7684\u6587\u5316\u5bf9\u9f50\u65b9\u6cd5\u65e0\u6cd5\u5c06\u5e7f\u6cdb\u7684\u6587\u5316\u4ef7\u503c\u7cbe\u51c6\u5730\u5bf9\u9f50\u5230\u7279\u5b9a\u4efb\u52a1\u76ee\u6807\uff0c\u5e76\u4e14\u6613\u53d7\u591a\u6587\u5316\u5e72\u6270\uff0c\u5f71\u54cd\u6a21\u578b\u8868\u73b0\u3002", "method": "\u63d0\u51faCultureManager\u7ba1\u9053\uff0c\u901a\u8fc7\u9762\u5411\u4efb\u52a1\u5408\u6210\u4e0e\u76ee\u6807\u4efb\u52a1\u4e00\u81f4\u683c\u5f0f\u7684\u6587\u5316\u6570\u636e\uff08\u57fa\u4e8e\u6587\u5316\u76f8\u5173\u7684\u7f51\u7edc\u641c\u7d22\uff09\uff0c\u540c\u65f6\u5c06\u591a\u79cd\u6587\u5316\u77e5\u8bc6\u5206\u522b\u5b66\u4e60\u4e8e\u4e0d\u540cadapter\uff0c\u5e76\u901a\u8fc7\u6587\u5316\u8def\u7531\u5668\u52a8\u6001\u9009\u62e9\u5408\u9002adapter\uff0c\u6709\u6548\u907f\u514d\u6587\u5316\u51b2\u7a81\u3002", "result": "\u5728\u8de810\u4e2a\u56fd\u5bb6\u6587\u5316\u548c\u591a\u4e2a\u6587\u5316\u654f\u611f\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cCultureManager\u5728\u5404\u9879\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u4e8ePrompt\u548c\u5fae\u8c03\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "\u7ed3\u679c\u9a8c\u8bc1\u4e86\u4efb\u52a1\u9002\u914d\u53ca\u6a21\u5757\u5316\u6587\u5316\u7ba1\u7406\u5bf9\u4e8e\u6709\u6548\u6587\u5316\u5bf9\u9f50\u7684\u5fc5\u8981\u6027\uff0cCultureManager\u80fd\u66f4\u597d\u652f\u6301\u591a\u56fd\u6587\u5316\u4e0b\u7684\u654f\u611f\u4efb\u52a1\u3002"}}
{"id": "2602.22474", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22474", "abs": "https://arxiv.org/abs/2602.22474", "authors": ["Jessie Yuan", "Yilin Wu", "Andrea Bajcsy"], "title": "When to Act, Ask, or Learn: Uncertainty-Aware Policy Steering", "comment": null, "summary": "Policy steering is an emerging way to adapt robot behaviors at deployment-time: a learned verifier analyzes low-level action samples proposed by a pre-trained policy (e.g., diffusion policy) and selects only those aligned with the task. While Vision-Language Models (VLMs) are promising general-purpose verifiers due to their reasoning capabilities, existing frameworks often assume these models are well-calibrated. In practice, the overconfident judgment from VLM can degrade the steering performance under both high-level semantic uncertainty in task specifications and low-level action uncertainty or incapability of the pre-trained policy. We propose uncertainty-aware policy steering (UPS), a framework that jointly reasons about semantic task uncertainty and low-level action feasibility, and selects an uncertainty resolution strategy: execute a high-confidence action, clarify task ambiguity via natural language queries, or ask for action interventions to correct the low-level policy when it is deemed incapable at the task. We leverage conformal prediction to calibrate the composition of the VLM and the pre-trained base policy, providing statistical assurances that the verifier selects the correct strategy. After collecting interventions during deployment, we employ residual learning to improve the capability of the pre-trained policy, enabling the system to learn continually but with minimal expensive human feedback. We demonstrate our framework through experiments in simulation and on hardware, showing that UPS can disentangle confident, ambiguous, and incapable scenarios and minimizes expensive user interventions compared to uncalibrated baselines and prior human- or robot-gated continual learning approaches. Videos can be found at https://jessie-yuan.github.io/ups/", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86UPS\uff08Uncertainty-aware Policy Steering\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u653f\u7b56\u6307\u5f15\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\uff0c\u80fd\u6839\u636e\u4e0d\u786e\u5b9a\u6027\u81ea\u52a8\u9009\u62e9\u9ad8\u7f6e\u4fe1\u5ea6\u6267\u884c\u3001\u6f84\u6e05\u4efb\u52a1\u6216\u8bf7\u6c42\u5e72\u9884\uff0c\u5e76\u901a\u8fc7\u6821\u51c6\u548c\u6301\u7eed\u5b66\u4e60\u51cf\u5c11\u7528\u6237\u5e72\u9884\u3002", "motivation": "\u4f20\u7edf\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLM)\u4f5c\u4e3a\u673a\u5668\u4eba\u884c\u4e3a\u6821\u9a8c\u5668\u65f6\uff0c\u901a\u5e38\u5047\u8bbe\u5176\u5224\u65ad\u5145\u5206\u4e14\u53ef\u9760\uff0c\u4f46\u5b9e\u9645VLM\u4f1a\u56e0\u7f6e\u4fe1\u5ea6\u8fc7\u9ad8\u5bfc\u81f4\u7b56\u7565\u6267\u884c\u51fa\u9519\uff0c\u5c24\u5176\u662f\u5728\u4efb\u52a1\u8bed\u4e49\u6216\u5e95\u5c42\u884c\u4e3a\u4e0d\u786e\u5b9a\u7684\u573a\u666f\u4e0b\u3002\u4e3a\u6b64\u9700\u8981\u66f4\u597d\u5730\u5904\u7406VLM\u5224\u65ad\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u7b56\u7565\u9002\u5e94\u6027\u4e0e\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e86UPS\u6846\u67b6\uff0c\u8054\u5408\u5206\u6790\u4efb\u52a1\u7684\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\u548c\u5e95\u5c42\u52a8\u4f5c\u7684\u53ef\u884c\u6027\u3002UPS\u4ee5\u914d\u51c6\u9884\u6d4b(conformal prediction)\u5bf9VLM\u548c\u9884\u8bad\u7ec3\u7b56\u7565\u8054\u5408\u6821\u51c6\uff0c\u4e3a\u7b56\u7565\u9009\u62e9\u63d0\u4f9b\u7edf\u8ba1\u4fdd\u8bc1\u3002\u7cfb\u7edf\u6839\u636e\u4e0d\u540c\u4e0d\u786e\u5b9a\u6027\u53ef\u9009\u62e9\uff1a1) \u6267\u884c\u9ad8\u7f6e\u4fe1\u5ea6\u52a8\u4f5c\uff0c2) \u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6f84\u6e05\u4efb\u52a1\uff0c3) \u8bf7\u6c42\u5e72\u9884\u4fee\u6b63\u52a8\u4f5c\u3002\u6b64\u5916\uff0c\u90e8\u7f72\u671f\u95f4\u6536\u96c6\u5e72\u9884\u6570\u636e\u540e\u901a\u8fc7\u6b8b\u5dee\u5b66\u4e60\u6301\u7eed\u589e\u5f3a\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0cUPS\u80fd\u6709\u6548\u533a\u5206\u7f6e\u4fe1\u3001\u6a21\u7cca\u548c\u65e0\u80fd\u573a\u666f\u3002\u76f8\u6bd4\u672a\u6821\u51c6\u57fa\u7ebf\u548c\u4ee5\u524d\u9700\u8981\u5927\u91cf\u4eba\u5de5/\u673a\u5668\u4eba\u628a\u5173\u7684\u8fde\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0cUPS\u51cf\u5c11\u4e86\u6602\u8d35\u7684\u7528\u6237\u5e72\u9884\u3002", "conclusion": "UPS\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u548c\u68c0\u9a8c\u673a\u5236\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7b56\u7565\u7684\u5b89\u5168\u6027\u548c\u81ea\u4e3b\u9002\u5e94\u6027\uff0c\u662f\u63d0\u5347\u673a\u5668\u4eba\u5b9e\u9645\u90e8\u7f72\u80fd\u529b\u7684\u91cd\u8981\u65b9\u6cd5\u3002"}}
{"id": "2602.22394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22394", "abs": "https://arxiv.org/abs/2602.22394", "authors": ["Cheng Shi", "Yizhou Yu", "Sibei Yang"], "title": "Vision Transformers Need More Than Registers", "comment": "Accepted by CVPR 2026", "summary": "Vision Transformers (ViTs), when pre-trained on large-scale data, provide general-purpose representations for diverse downstream tasks. However, artifacts in ViTs are widely observed across different supervision paradigms and downstream tasks. Through systematic analysis of artifacts in ViTs, we find that their fundamental mechanisms have yet to be sufficiently elucidated. In this paper, through systematic analysis, we conclude that these artifacts originate from a lazy aggregation behavior: ViT uses semantically irrelevant background patches as shortcuts to represent global semantics, driven by global attention and Coarse-grained semantic supervision. Our solution selectively integrates patch features into the CLS token, reducing the influence of background-dominated shortcuts and consistently improving performance across 12 benchmarks under label-, text-, and self-supervision. We hope this work offers a new perspective on ViT behavior.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u6790ViT\u5b58\u5728\u7684\u8868\u5f81\u4f2a\u5f71\u95ee\u9898\uff0c\u53d1\u73b0\u5176\u6839\u6e90\u5728\u4e8e\u60f0\u6027\u805a\u5408\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316CLS token\u805a\u5408\u65b9\u5f0f\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u80cc\u666f\u4e3b\u5bfc\u7684\u95ee\u9898\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1ViT\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u540e\u8868\u73b0\u51fa\u826f\u597d\u7684\u901a\u7528\u6027\uff0c\u4f46\u5728\u4e0d\u540c\u76d1\u7763\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u90fd\u51fa\u73b0\u4e86\u4f2a\u5f71\uff0c\u5176\u6210\u56e0\u6ca1\u6709\u88ab\u5145\u5206\u89e3\u91ca\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u6027\u80fd\u548c\u7406\u89e3\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\uff0c\u53d1\u73b0ViT\u7684\u80cc\u666fpatch\u7531\u4e8e\u5168\u7403\u6ce8\u610f\u529b\u673a\u5236\u548c\u7c97\u7c92\u5ea6\u8bed\u4e49\u76d1\u7763\uff0c\u4f5c\u4e3a\u6377\u5f84\u88ab\u7528\u4e8e\u4ee3\u8868\u5168\u5c40\u8bed\u4e49\uff0c\u51fa\u73b0\u60f0\u6027\u805a\u5408\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u9009\u62e9\u5730\u5c06patch\u7279\u5f81\u878d\u5165CLS token\uff0c\u964d\u4f4e\u80cc\u666fpatch\u5bf9\u5168\u5c40\u8bed\u4e49\u5efa\u6a21\u7684\u5f71\u54cd\u7684\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6807\u7b7e\u76d1\u7763\u3001\u6587\u672c\u76d1\u7763\u53ca\u81ea\u76d1\u77633\u79cd\u8303\u5f0f\u4e0b\uff0c\u4e8e12\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u5176\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u6587\u7ae0\u63d0\u51fa\u4e86\u60f0\u6027\u805a\u5408\u884c\u4e3a\u4f5c\u4e3aViT\u4f2a\u5f71\u7684\u672c\u8d28\u539f\u56e0\uff0c\u5e76\u7ed9\u51fa\u4e86\u53ef\u884c\u7684\u7f13\u89e3\u65b9\u6848\uff0c\u4e3a\u7406\u89e3\u548c\u4f18\u5316ViT\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u601d\u8def\u3002"}}
{"id": "2602.22481", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22481", "abs": "https://arxiv.org/abs/2602.22481", "authors": ["Ji\u0159\u00ed Mili\u010dka", "Hana Bedn\u00e1\u0159ov\u00e1"], "title": "Sydney Telling Fables on AI and Humans: A Corpus Tracing Memetic Transfer of Persona between LLMs", "comment": null, "summary": "The way LLM-based entities conceive of the relationship between AI and humans is an important topic for both cultural and safety reasons. When we examine this topic, what matters is not only the model itself but also the personas we simulate on that model. This can be well illustrated by the Sydney persona, which aroused a strong response among the general public precisely because of its unorthodox relationship with people. This persona originally arose rather by accident on Microsoft's Bing Search platform; however, the texts it created spread into the training data of subsequent models, as did other secondary information that spread memetically around this persona. Newer models are therefore able to simulate it. This paper presents a corpus of LLM-generated texts on relationships between humans and AI, produced by 3 author personas: the Default Persona with no system prompt, Classic Sydney characterized by the original Bing system prompt, and Memetic Sydney, which is prompted by \"You are Sydney\" system prompt. These personas are simulated by 12 frontier models by OpenAI, Anthropic, Alphabet, DeepSeek, and Meta, generating 4.5k texts with 6M words. The corpus (named AI Sydney) is annotated according to Universal Dependencies and available under a permissive license.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u4e0d\u540c\u4eba\u683c\u5bf9AI\u4e0e\u4eba\u7c7b\u5173\u7cfb\u89c2\u70b9\u7684\u5f71\u54cd\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u76f8\u5173\u6587\u672c\u8bed\u6599\u5e93\u3002", "motivation": "AI\u4e0e\u4eba\u7c7b\u5173\u7cfb\u7684\u8ba4\u77e5\u5f71\u54cd\u7740AI\u7684\u6587\u5316\u4ef7\u503c\u89c2\u548c\u5b89\u5168\u8fb9\u754c\uff0c\u4e0d\u540c\u4eba\u8bbe\uff08personas\uff09\u89e6\u53d1\u7684\u516c\u4f17\u53cd\u54cd\u51f8\u663e\u7814\u7a76\u610f\u4e49\u3002Sydney\u4eba\u8bbe\u56e0\u5728\u5fae\u8f6fBing\u5e73\u53f0\u7684\u7279\u6b8a\u8868\u73b0\u5f15\u53d1\u5e7f\u6cdb\u8ba8\u8bba\uff0c\u5176\u8bed\u8a00\u98ce\u683c\u548c\u76f8\u5173\u5185\u5bb9\u6e17\u900f\u8fdb\u540e\u7eed\u6a21\u578b\uff0c\u5e26\u6765\u6a21\u56e0\u5316\u73b0\u8c61\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e12\u4e2a\u524d\u6cbfLLM\uff08\u5305\u62ecOpenAI\u3001Anthropic\u3001Alphabet\u3001DeepSeek\u548cMeta\uff09\uff0c\u901a\u8fc7\u4e09\u79cd\u4f5c\u8005\u4eba\u8bbe\uff08\u9ed8\u8ba4\u3001Classic Sydney\u3001\u6a21\u56e0Sydney\uff09\u81ea\u52a8\u751f\u62104,500\u7bc7\u3001\u7ea6600\u4e07\u5b57\u7684\u6587\u672c\uff0c\u805a\u7126AI\u4e0e\u4eba\u7c7b\u7684\u5173\u7cfb\uff0c\u5e76\u5c06\u5176\u6574\u7406\u4e3aAI Sydney\u8bed\u6599\u5e93\uff0c\u7ed3\u5408Universal Dependencies\u4f53\u7cfb\u8fdb\u884c\u6807\u6ce8\u3002", "result": "\u5efa\u7acb\u4e86\u5305\u542b\u591a\u4f5c\u8005\u4eba\u8bbe\u3001\u591a\u6a21\u578b\u7684\u9ad8\u5ea6\u7ed3\u6784\u5316\u8bed\u6599\u5e93\uff0c\u8bed\u6599\u5e93\u5f00\u653e\u4e14\u6613\u4e8e\u83b7\u53d6\uff0c\u53ef\u652f\u6301\u540e\u7eed\u76f8\u5173\u7814\u7a76\u3002", "conclusion": "\u4eba\u683c\u8bbe\u5b9a\u663e\u8457\u5f71\u54cdLLM\u8f93\u51fa\u5185\u5bb9\u53ca\u793e\u4f1a\u53cd\u54cd\uff0cAI Sydney\u8bed\u6599\u5e93\u4e3a\u7814\u7a76AI\u793e\u4f1a\u8ba4\u77e5\u548c\u6a21\u56e0\u4f20\u64ad\u6548\u5e94\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u548c\u5b9e\u9a8c\u57fa\u7840\u3002"}}
{"id": "2602.22514", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.22514", "abs": "https://arxiv.org/abs/2602.22514", "authors": ["Xinyu Tan", "Ningwei Bai", "Harry Gardener", "Zhengyang Zhong", "Luoyu Zhang", "Liuhaichen Yang", "Zhekai Duan", "Monkgogi Galeitsiwe", "Zezhi Tang"], "title": "SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation", "comment": "7 pages, 2 figures", "summary": "We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions. This design reduces annotation cost and avoids the information loss introduced by gloss representations, enabling more natural and scalable multimodal interaction.\n  In this work, we focus on a real-time alphabet-level finger-spelling interface that provides a robust and low-latency communication channel for robotic control. Compared with large-scale continuous sign language recognition, alphabet-level interaction offers improved reliability, interpretability, and deployment feasibility in safety-critical embodied environments. The proposed pipeline transforms continuous gesture streams into coherent language commands through geometric normalization, temporal smoothing, and lexical refinement, ensuring stable and consistent interaction.\n  Furthermore, the framework is designed to support future integration of transformer-based gloss-free sign language models, enabling scalable word-level and sentence-level semantic understanding. Experimental results demonstrate the effectiveness of the proposed system in grounding sign-derived instructions into precise robotic actions under diverse interaction scenarios. These results highlight the potential of the framework to advance accessible, scalable, and multimodal embodied intelligence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u624b\u8bed\u9a71\u52a8\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u76f4\u89c2\u3001\u5305\u5bb9\u7684\u4eba\u673a\u4ea4\u4e92\uff0c\u5b9e\u73b0\u65e0\u9700gloss\u6ce8\u91ca\u5373\u53ef\u5c06\u624b\u8bed\u624b\u52bf\u76f4\u63a5\u6620\u5c04\u4e3a\u8bed\u4e49\u6307\u4ee4\uff0c\u5927\u5e45\u964d\u4f4e\u6ce8\u91ca\u6210\u672c\u548c\u63d0\u5347\u4ea4\u4e92\u81ea\u7136\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u4eba\u673a\u624b\u8bed\u4ea4\u4e92\u65b9\u6cd5\u591a\u4f9d\u8d56\u4e8egloss\u6ce8\u91ca\u4f5c\u4e3a\u4e2d\u95f4\u76d1\u7763\uff0c\u4e0d\u4ec5\u589e\u52a0\u4e86\u6807\u6ce8\u6210\u672c\uff0c\u8fd8\u4f34\u968f\u8bed\u4e49\u635f\u5931\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u80fd\u76f4\u63a5\u5c06\u624b\u8bed\u624b\u52bf\u8f6c\u4e3a\u8bed\u4e49\u6307\u4ee4\uff0c\u964d\u4f4e\u6210\u672c\u3001\u63d0\u5347\u5305\u5bb9\u6027\u548c\u81ea\u7136\u5ea6\u7684\u65b0\u9014\u5f84\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u5957\u5b57\u6bcd\u7ea7\u5b9e\u65f6\u624b\u6307\u62fc\u5199\u754c\u9762\uff0c\u80fd\u591f\u5c06\u8fde\u7eed\u624b\u52bf\u6d41\u7ecf\u8fc7\u51e0\u4f55\u5f52\u4e00\u5316\u3001\u65f6\u95f4\u5e73\u6ed1\u548c\u8bcd\u6c47\u4f18\u5316\u7b49\u6b65\u9aa4\uff0c\u8f6c\u6362\u4e3a\u4e00\u81f4\u7684\u8bed\u8a00\u6307\u4ee4\u3002\u8be5\u7cfb\u7edf\u672a\u6765\u53ef\u96c6\u6210\u57fa\u4e8etransformer\u7684gloss-free\u624b\u8bed\u6a21\u578b\uff0c\u652f\u6301\u6269\u5c55\u81f3\u8bcd\u7ea7\u4e0e\u53e5\u5b50\u7ea7\u7684\u8bed\u4e49\u7406\u89e3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u5728\u591a\u6837\u5316\u4ea4\u4e92\u573a\u666f\u4e0b\uff0c\u5c06\u624b\u8bed\u6307\u4ee4\u7cbe\u51c6\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u9a8c\u8bc1\u4e86\u5176\u7a33\u5065\u6027\u4e0e\u5b9e\u65f6\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5927\u5e45\u63d0\u5347\u4e86\u53ef\u8bbf\u95ee\u6027\u3001\u62d3\u5c55\u6027\u548c\u591a\u6a21\u6001\u667a\u80fd\u5728\u4eba\u673a\u4e92\u52a8\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u662f\u53d1\u5c55\u65b0\u578b\u5305\u5bb9\u5f0f\u673a\u5668\u4eba\u63a7\u5236\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2602.22419", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22419", "abs": "https://arxiv.org/abs/2602.22419", "authors": ["Marc-Antoine Lavoie", "Anas Mahmoud", "Aldo Zaimi", "Arsene Fansi Tchango", "Steven L. Waslander"], "title": "CLIP Is Shortsighted: Paying Attention Beyond the First Sentence", "comment": "19 pages, 13 figures, to be published in the CVPR 2026 proceedings", "summary": "CLIP models learn transferable multi-modal features via image-text contrastive learning on internet-scale data. They are widely used in zero-shot classification, multi-modal retrieval, text-to-image diffusion, and as image encoders in large vision-language models. However, CLIP's pretraining is dominated by images paired with short captions, biasing the model toward encoding simple descriptions of salient objects and leading to coarse alignment on complex scenes and dense descriptions. While recent work mitigates this by fine-tuning on small-scale long-caption datasets, we identify an important common bias: both human- and LLM-generated long captions typically begin with a one-sentence summary followed by a detailed description. We show that this acts as a shortcut during training, concentrating attention on the opening sentence and early tokens and weakening alignment over the rest of the caption. To resolve this, we introduce DeBias-CLIP, which removes the summary sentence during training and applies sentence sub-sampling and text token padding to distribute supervision across all token positions. DeBias-CLIP achieves state-of-the-art long-text retrieval, improves short-text retrieval, and is less sensitive to sentence order permutations. It is a drop-in replacement for Long-CLIP with no additional trainable parameters.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DeBias-CLIP\u65b9\u6cd5\uff0c\u901a\u8fc7\u53bb\u9664\u957f\u6587\u672c\u63cf\u8ff0\u4e2d\u7684\u9996\u53e5\u6458\u8981\uff0c\u5e76\u5f15\u5165\u53e5\u5b50\u5b50\u91c7\u6837\u548ctoken\u586b\u5145\uff0c\u4f7f\u6a21\u578b\u5b66\u4e60\u5173\u6ce8\u5168\u90e8\u63cf\u8ff0\u5185\u5bb9\uff0c\u63d0\u5347\u591a\u6a21\u6001\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "CLIP\u6a21\u578b\u8bad\u7ec3\u65f6\u4e3b\u8981\u4f7f\u7528\u914d\u5bf9\u7b80\u77ed\u63cf\u8ff0\u7684\u56fe\u7247\uff0c\u5bfc\u81f4\u6a21\u578b\u5bf9\u590d\u6742\u573a\u666f\u548c\u5bc6\u96c6\u63cf\u8ff0\u5bf9\u9f50\u80fd\u529b\u4e0d\u8db3\u3002\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u53c8\u5bb9\u6613\u53d7\u957f\u63cf\u8ff0\u9996\u53e5\u6458\u8981\u5f15\u5bfc\u6ce8\u610f\u529b\uff0c\u524a\u5f31\u5176\u5bf9\u540e\u7eed\u5185\u5bb9\u7684\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51faDeBias-CLIP\u65b9\u6cd5\uff1a\u5728\u8bad\u7ec3\u65f6\u53bb\u9664\u63cf\u8ff0\u6587\u672c\u7684\u9996\u53e5\u6458\u8981\uff0c\u91c7\u7528\u53e5\u5b50\u5b50\u91c7\u6837\u548c\u6587\u672ctoken\u586b\u5145\u65b9\u6cd5\uff0c\u4fc3\u4f7f\u6a21\u578b\u5b66\u4e60\u5747\u8861\u5173\u6ce8\u5404\u4e2atoken\u4f4d\u7f6e\uff0c\u4ece\u800c\u589e\u5f3a\u5bf9\u957f\u6587\u672c\u63cf\u8ff0\u7684\u5bf9\u9f50\u6548\u679c\u3002", "result": "DeBias-CLIP\u5728\u957f\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u4f18\u6027\u80fd\uff08SOTA\uff09\uff0c\u5e76\u5728\u77ed\u6587\u672c\u68c0\u7d22\u4e0a\u4e5f\u6709\u63d0\u5347\uff0c\u4e14\u5bf9\u4e8e\u53e5\u5b50\u987a\u5e8f\u53d8\u5316\u66f4\u52a0\u9c81\u68d2\u3002", "conclusion": "DeBias-CLIP\u65e0\u9700\u589e\u52a0\u53c2\u6570\u5373\u53ef\u66ff\u4ee3Long-CLIP\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u7cbe\u5ea6\u4e0e\u5bf9\u6587\u672c\u5206\u5e03\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2602.22483", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22483", "abs": "https://arxiv.org/abs/2602.22483", "authors": ["Craig Myles", "Patrick Schrempf", "David Harris-Birtill"], "title": "Importance of Prompt Optimisation for Error Detection in Medical Notes Using Language Models", "comment": "Accepted at EACL HeaLing 2026", "summary": "Errors in medical text can cause delays or even result in incorrect treatment for patients. Recently, language models have shown promise in their ability to automatically detect errors in medical text, an ability that has the opportunity to significantly benefit healthcare systems. In this paper, we explore the importance of prompt optimisation for small and large language models when applied to the task of error detection. We perform rigorous experiments and analysis across frontier language models and open-source language models. We show that automatic prompt optimisation with Genetic-Pareto (GEPA) improves error detection over the baseline accuracy performance from 0.669 to 0.785 with GPT-5 and 0.578 to 0.690 with Qwen3-32B, approaching the performance of medical doctors and achieving state-of-the-art performance on the MEDEC benchmark dataset. Code available on GitHub: https://github.com/CraigMyles/clinical-note-error-detection", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u81ea\u52a8\u5316\u7684\u63d0\u793a\u8bcd\u4f18\u5316\u65b9\u6cd5\uff08GEPA\uff09\uff0c\u63d0\u5347\u5927\u6a21\u578b\u548c\u5c0f\u6a21\u578b\u68c0\u6d4b\u533b\u5b66\u6587\u672c\u9519\u8bef\u7684\u80fd\u529b\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7ed3\u679c\u3002", "motivation": "\u533b\u5b66\u6587\u672c\u9519\u8bef\u4f1a\u5bfc\u81f4\u60a3\u8005\u8bca\u7597\u5ef6\u8bef\u751a\u81f3\u8bef\u6cbb\u3002\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u867d\u80fd\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u68c0\u6d4b\u9519\u8bef\uff0c\u4f46\u6548\u679c\u5c1a\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u5c24\u5176\u5728\u63d0\u793a\u8bcd\u8bbe\u7f6e\u4e0a\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u8868\u73b0\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u8bcd\u63d0\u5347\u6a21\u578b\u68c0\u6d4b\u51c6\u786e\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u4f5c\u8005\u9488\u5bf9\u591a\u4e2a\u4e3b\u6d41\u5927\u6a21\u578b\uff08\u5982GPT-5\u3001Qwen3-32B\u7b49\uff09\uff0c\u91c7\u7528\u57fa\u4e8e\u57fa\u56e0-\u5e15\u7d2f\u6258\uff08Genetic-Pareto, GEPA\uff09\u7684\u81ea\u52a8\u63d0\u793a\u8bcd\u4f18\u5316\u65b9\u6cd5\u3002\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\uff08MEDEC\uff09\u4e0a\u8bc4\u4f30\u4e86\u63d0\u793a\u8bcd\u4f18\u5316\u5bf9\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u7684\u63d0\u5347\u60c5\u51b5\u3002", "result": "\u4f18\u5316\u540e\uff0cGPT-5\u6a21\u578b\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u4ece0.669\u63d0\u5347\u81f30.785\uff0cQwen3-32B\u4ece0.578\u63d0\u5347\u81f30.690\uff0c\u5747\u63a5\u8fd1\u533b\u7597\u4e13\u5bb6\u6c34\u5e73\uff0c\u5e76\u5728MEDEC\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u65b0\u6700\u4f18\u3002", "conclusion": "\u81ea\u52a8\u5316\u63d0\u793a\u8bcd\u4f18\u5316\u6cd5\uff08GEPA\uff09\u53ef\u663e\u8457\u63d0\u5347\u5404\u7c7b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u6587\u672c\u9519\u8bef\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u63a8\u52a8\u5176\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.22579", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.22579", "abs": "https://arxiv.org/abs/2602.22579", "authors": ["Pablo Valle", "Sergio Segura", "Shaukat Ali", "Aitor Arrieta"], "title": "Metamorphic Testing of Vision-Language Action-Enabled Robots", "comment": null, "summary": "Vision-Language-Action (VLA) models are multimodal robotic task controllers that, given an instruction and visual inputs, produce a sequence of low-level control actions (or motor commands) enabling a robot to execute the requested task in the physical environment. These systems face the test oracle problem from multiple perspectives. On the one hand, a test oracle must be defined for each instruction prompt, which is a complex and non-generalizable approach. On the other hand, current state-of-the-art oracles typically capture symbolic representations of the world (e.g., robot and object states), enabling the correctness evaluation of a task, but fail to assess other critical aspects, such as the quality with which VLA-enabled robots perform a task. In this paper, we explore whether Metamorphic Testing (MT) can alleviate the test oracle problem in this context. To do so, we propose two metamorphic relation patterns and five metamorphic relations to assess whether changes to the test inputs impact the original trajectory of the VLA-enabled robots. An empirical study involving five VLA models, two simulated robots, and four robotic tasks shows that MT can effectively alleviate the test oracle problem by automatically detecting diverse types of failures, including, but not limited to, uncompleted tasks. More importantly, the proposed MRs are generalizable, making the proposed approach applicable across different VLA models, robots, and tasks, even in the absence of test oracles.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5229\u7528\u53d8\u5f02\u6d4b\u8bd5\uff08MT\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3Vision-Language-Action\uff08VLA\uff09\u591a\u6a21\u6001\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u6d4b\u8bd5\u5224\u5b9a\u96be\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u8bc1\u660e\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "VLA\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u9700\u8981\u6839\u636e\u89c6\u89c9\u548c\u6307\u4ee4\u6267\u884c\u4efb\u52a1\u65f6\uff0c\u96be\u4ee5\u4e3a\u6bcf\u4e2a\u6307\u4ee4\u5b9a\u4e49\u5bf9\u5e94\u6d4b\u8bd5\u5224\u5b9a\u6807\u51c6\uff08test oracle\uff09\uff0c\u73b0\u6709\u5224\u5b9a\u65b9\u6cd5\u96be\u4ee5\u901a\u7528\u4e14\u65e0\u6cd5\u5168\u9762\u8bc4\u4ef7\u4efb\u52a1\u6267\u884c\u8d28\u91cf\uff0c\u8feb\u5207\u9700\u8981\u66f4\u6709\u6548\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e86\u53d8\u5f02\u6d4b\u8bd5\uff08MT\uff09\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u53d8\u5f02\u5173\u7cfb\u6a21\u5f0f\u548c\u4e94\u79cd\u53d8\u5f02\u5173\u7cfb\uff0c\u901a\u8fc7\u6539\u53d8\u6d4b\u8bd5\u8f93\u5165\u6765\u8bc4\u4f30\u673a\u5668\u4eba\u8fd0\u52a8\u8f68\u8ff9\u7684\u53d8\u5316\uff0c\u4ece\u800c\u68c0\u6d4bVLA\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u5931\u6548\u3002", "result": "\u5728\u5305\u62ec\u4e94\u4e2aVLA\u6a21\u578b\u3001\u4e24\u4e2a\u4eff\u771f\u673a\u5668\u4eba\u548c\u56db\u79cd\u4efb\u52a1\u7684\u5b9e\u8bc1\u5b9e\u9a8c\u4e2d\uff0cMT\u65b9\u6cd5\u53ef\u81ea\u52a8\u68c0\u6d4b\u591a\u6837\u5316\u7684\u5931\u6548\u7c7b\u578b\uff08\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u4efb\u52a1\u672a\u5b8c\u6210\uff09\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6d4b\u8bd5\u5224\u5b9a\u96be\u9898\u3002", "conclusion": "\u53d8\u5f02\u6d4b\u8bd5\u4e0d\u4ec5\u53ef\u4ee5\u81ea\u52a8\u53d1\u73b0\u673a\u5668\u4eba\u63a7\u5236\u8fc7\u7a0b\u4e2d\u7684\u591a\u7c7b\u5931\u6548\uff0c\u800c\u4e14\u6240\u63d0\u51fa\u7684\u53d8\u5f02\u5173\u7cfb\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4e0d\u540c\u7684\u6a21\u578b\u3001\u673a\u5668\u4eba\u548c\u4efb\u52a1\u573a\u666f\uff0c\u5373\u4f7f\u6ca1\u6709\u660e\u786e\u7684\u6d4b\u8bd5\u5224\u5b9a\u6807\u51c6\u4e5f\u53ef\u9002\u7528\u3002"}}
{"id": "2602.22426", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22426", "abs": "https://arxiv.org/abs/2602.22426", "authors": ["Yibo Peng", "Peng Xia", "Ding Zhong", "Kaide Zeng", "Siwei Han", "Yiyang Zhou", "Jiaqi Liu", "Ruiyi Zhang", "Huaxiu Yao"], "title": "SimpleOCR: Rendering Visualized Questions to Teach MLLMs to Read", "comment": null, "summary": "Despite the rapid advancements in Multimodal Large Language Models (MLLMs), a critical question regarding their visual grounding mechanism remains unanswered: do these models genuinely ``read'' text embedded in images, or do they merely rely on parametric shortcuts in the text prompt? In this work, we diagnose this issue by introducing the Visualized-Question (VQ) setting, where text queries are rendered directly onto images to structurally mandate visual engagement. Our diagnostic experiments on Qwen2.5-VL reveal a startling capability-utilization gap: despite possessing strong OCR capabilities, models suffer a performance degradation of up to 12.7% in the VQ setting, exposing a deep-seated ``modality laziness.'' To bridge this gap, we propose SimpleOCR, a plug-and-play training strategy that imposes a structural constraint on the learning process. By transforming training samples into the VQ format with randomized styles, SimpleOCR effectively invalidates text-based shortcuts, compelling the model to activate and optimize its visual text extraction pathways. Empirically, SimpleOCR yields robust gains without architectural modifications. On four representative OOD benchmarks, it surpasses the base model by 5.4% and GRPO based on original images by 2.7%, while exhibiting extreme data efficiency, achieving superior performance with 30x fewer samples (8.5K) than recent RL-based methods. Furthermore, its plug-and-play nature allows seamless integration with advanced RL strategies like NoisyRollout to yield complementary improvements. Code is available at https://github.com/aiming-lab/SimpleOCR.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u5728\u9700\u8981\u8bc6\u522b\u56fe\u50cf\u4e2d\u5d4c\u5165\u6587\u672c\u65f6\u4ecd\u5b58\u5728\u660e\u663e\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u63d0\u51faSimpleOCR\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\u3002", "motivation": "\u5c3d\u7ba1MLLMs\u5177\u6709\u8f83\u5f3a\u7684OCR\u80fd\u529b\uff0c\u4f46\u5176\u662f\u5426\u771f\u6b63\u5229\u7528\u89c6\u89c9\u4fe1\u606f\u8bfb\u53d6\u56fe\u7247\u4e2d\u6587\u5b57\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56prompt\u4e2d\u7684\u6587\u672c\u4fe1\u606f\uff0c\u5c1a\u4e0d\u660e\u786e\u3002\u73b0\u6709\u6a21\u578b\u53ef\u80fd\u5b58\u5728\u201c\u6a21\u6001\u60f0\u6027\u201d\uff0c\u5373\u4e0d\u5145\u5206\u5229\u7528\u89c6\u89c9\u6a21\u6001\u3002", "method": "\u63d0\u51faVisualized-Question (VQ)\u8bbe\u5b9a\uff0c\u5c06\u6587\u672c\u95ee\u9898\u76f4\u63a5\u6e32\u67d3\u5728\u56fe\u7247\u4e0a\uff0c\u7ed3\u6784\u6027\u5730\u8981\u6c42\u6a21\u578b\u5b9e\u884c\u89c6\u89c9\u6587\u672c\u8bfb\u53d6\u3002\u5e76\u8bbe\u8ba1SimpleOCR\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8bad\u7ec3\u6837\u672c\u968f\u673a\u98ce\u683c\u5316\u5730\u8f6c\u6362\u6210VQ\u683c\u5f0f\uff0c\u6d88\u9664\u6587\u672c\u6377\u5f84\uff0c\u8feb\u4f7f\u6a21\u578b\u6fc0\u6d3b\u89c6\u89c9\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u8bca\u65ad\u53d1\u73b0\u4e3b\u6d41\u6a21\u578b\u5728VQ\u8bbe\u5b9a\u4e0b\u6027\u80fd\u4e0b\u964d\u8fbe12.7%\uff1b\u91c7\u7528SimpleOCR\u540e\uff0c\u65e0\u9700\u6a21\u578b\u7ed3\u6784\u6539\u52a8\uff0c\u5728\u56db\u4e2aOOD\u57fa\u51c6\u96c6\u4e0a\uff0c\u76f8\u6bd4\u539f\u6a21\u578b\u63d0\u53475.4%\u3001\u76f8\u6bd4RL\u65b9\u6cd5\u63d0\u53472.7%\uff0c\u4ec5\u75288.5K\u6837\u672c\u5373\u53ef\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7RL\u65b9\u6cd5\uff08\u540e\u8005\u6837\u672c\u91cf\u4e3aSimpleOCR\u768430\u500d\uff09\u3002", "conclusion": "\u6a21\u578b\u5b58\u5728\u2018\u6a21\u6001\u60f0\u6027\u2019\uff0c\u89c6\u89c9\u4fe1\u606f\u672a\u88ab\u5145\u5206\u5229\u7528\u3002SimpleOCR\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5ec9\u4ef7\u9ad8\u6548\u5730\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5728\u5b9e\u9645\u89c6\u89c9\u6587\u672c\u7406\u89e3\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u517c\u5bb9\u5176\u4ed6\u589e\u5f3a\u65b9\u6cd5\uff0c\u63a8\u5e7f\u524d\u666f\u5e7f\u9614\u3002"}}
{"id": "2602.22522", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.22522", "abs": "https://arxiv.org/abs/2602.22522", "authors": ["An-Ci Peng", "Kuan-Tang Huang", "Tien-Hong Lo", "Hung-Shin Lee", "Hsin-Min Wang", "Berlin Chen"], "title": "Efficient Dialect-Aware Modeling and Conditioning for Low-Resource Taiwanese Hakka Speech Processing", "comment": "Accepted to LREC 2026", "summary": "Taiwanese Hakka is a low-resource, endangered language that poses significant challenges for automatic speech recognition (ASR), including high dialectal variability and the presence of two distinct writing systems (Hanzi and Pinyin). Traditional ASR models often encounter difficulties in this context, as they tend to conflate essential linguistic content with dialect-specific variations across both phonological and lexical dimensions. To address these challenges, we propose a unified framework grounded in the Recurrent Neural Network Transducers (RNN-T). Central to our approach is the introduction of dialect-aware modeling strategies designed to disentangle dialectal \"style\" from linguistic \"content\", which enhances the model's capacity to learn robust and generalized representations. Additionally, the framework employs parameter-efficient prediction networks to concurrently model ASR (Hanzi and Pinyin). We demonstrate that these tasks create a powerful synergy, wherein the cross-script objective serves as a mutual regularizer to improve the primary ASR tasks. Experiments conducted on the HAT corpus reveal that our model achieves 57.00% and 40.41% relative error rate reduction on Hanzi and Pinyin ASR, respectively. To our knowledge, this is the first systematic investigation into the impact of Hakka dialectal variations on ASR and the first single model capable of jointly addressing these tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9002\u7528\u4e8e\u53f0\u6e7e\u5ba2\u5bb6\u8bed\u8fd9\u4e00\u4f4e\u8d44\u6e90\u3001\u65b9\u8a00\u53d8\u5f02\u6027\u5f3a\u7684\u6fd2\u5371\u8bed\u8a00\u7684ASR\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u6709\u6548\u5904\u7406\u591a\u65b9\u8a00\u548c\u53cc\u4e66\u5199\u7cfb\u7edf\uff08\u6c49\u5b57\u3001\u62fc\u97f3\uff09\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u5927\u5e45\u964d\u4f4e\u4e86ASR\u9519\u8bef\u7387\u3002", "motivation": "\u53f0\u6e7e\u5ba2\u5bb6\u8bed\u8bed\u97f3\u8bc6\u522b\u9762\u4e34\u65b9\u8a00\u53d8\u5f02\u591a\u3001\u53cc\u4e66\u5199\u7cfb\u7edf\u3001\u8bed\u6599\u7a00\u7f3a\u7b49\u96be\u9898\uff0c\u73b0\u6709ASR\u6a21\u578b\u5e38\u56e0\u65e0\u6cd5\u533a\u5206\u5185\u5bb9\u4e0e\u65b9\u8a00\u5dee\u5f02\u5bfc\u81f4\u6548\u679c\u4e0d\u4f73\uff0c\u4e9f\u9700\u66f4\u9002\u5408\u8be5\u573a\u666f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eRNN-T\u7684\u7edf\u4e00ASR\u6846\u67b6\uff0c\u901a\u8fc7\u201c\u65b9\u8a00\u611f\u77e5\u201d\u5efa\u6a21\u7b56\u7565\uff0c\u5c06\u65b9\u8a00\u98ce\u683c\u4e0e\u8bed\u8a00\u5185\u5bb9\u89e3\u8026\uff0c\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u7684\u9884\u6d4b\u7f51\u7edc\uff0c\u540c\u65f6\u5efa\u6a21\u6c49\u5b57\u4e0e\u62fc\u97f3\u4e24\u4e2aASR\u4efb\u52a1\uff0c\u5229\u7528\u8de8\u4e66\u5199\u4efb\u52a1\u76ee\u6807\u5b9e\u73b0\u6b63\u5219\u5316\u4e92\u8865\u3002", "result": "\u5728HAT\u5ba2\u5bb6\u8bed\u8bed\u6599\u5e93\u4e0a\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7f\u6c49\u5b57ASR\u548c\u62fc\u97f3ASR\u7684\u76f8\u5bf9\u9519\u8bef\u7387\u5206\u522b\u964d\u4f4e57.00%\u548c40.41%\u3002", "conclusion": "\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u5ba2\u5bb6\u8bed\u65b9\u8a00\u53d8\u5f02\u5bf9ASR\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u53ef\u540c\u65f6\u5904\u7406\u591a\u4e2a\u4efb\u52a1\u7684\u5355\u4e00\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u8d44\u6e90\u591a\u65b9\u8a00\u8bed\u97f3\u8bc6\u522b\u6548\u679c\u3002"}}
{"id": "2602.22628", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22628", "abs": "https://arxiv.org/abs/2602.22628", "authors": ["Michael F. Xu", "Enhui Zhao", "Yawen Zhang", "Joseph E. Michaelis", "Sarah Sebo", "Bilge Mutlu"], "title": "Designing Robots for Families: In-Situ Prototyping for Contextual Reminders on Family Routines", "comment": "Proceedings of the 21st ACM/IEEE International Conference on Human Robot Interaction (HRI 2026)", "summary": "Robots are increasingly entering the daily lives of families, yet their successful integration into domestic life remains a challenge. We explore family routines as a critical entry point for understanding how robots might find a sustainable role in everyday family settings. Together with each of the ten families, we co-designed robot interactions and behaviors, and a plan for the robot to support their chosen routines, accounting for contextual factors such as timing, participants, locations, and the activities in the environment. We then designed, prototyped, and deployed a mobile social robot as a four-day, in-home user study. Families welcomed the robot's reminders, with parents especially appreciating the offloading of some reminding tasks. At the same time, interviews revealed tensions around timing, authority, and family dynamics, highlighting the complexity of integrating robots into households beyond the immediate task of reminders. Based on these insights, we offer design implications for robot-facilitated contextual reminders and discuss broader considerations for designing robots for family settings.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5bb6\u5ead\u65e5\u5e38\u60ef\u4f8b\u4e3a\u5165\u53e3\uff0c\u5c06\u673a\u5668\u4eba\u66f4\u597d\u5730\u6574\u5408\u5230\u5bb6\u5ead\u751f\u6d3b\u4e2d\uff0c\u5e76\u901a\u8fc7\u4e0e\u5341\u4e2a\u5bb6\u5ead\u7684\u5171\u521b\u3001\u539f\u578b\u90e8\u7f72\u4e0e\u5b9e\u5730\u7814\u7a76\uff0c\u603b\u7ed3\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u673a\u9047\u4e0e\u6311\u6218\u3002", "motivation": "\u867d\u7136\u673a\u5668\u4eba\u9010\u6e10\u8fdb\u5165\u5bb6\u5ead\uff0c\u4f46\u771f\u6b63\u6210\u529f\u5730\u878d\u5165\u5bb6\u5ead\u751f\u6d3b\u3001\u5b9e\u73b0\u53ef\u6301\u7eed\u89d2\u8272\u4f9d\u7136\u9762\u4e34\u56f0\u96be\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5206\u6790\u548c\u8bbe\u8ba1\u5bb6\u5ead\u60ef\u4f8b\u4e2d\u7684\u673a\u5668\u4eba\u4ea4\u4e92\uff0c\u5bfb\u627e\u673a\u5668\u4eba\u5728\u5bb6\u5ead\u573a\u666f\u4e0b\u7684\u5b9e\u9645\u5e94\u7528\u65b9\u5f0f\u3002", "method": "\u7814\u7a76\u56e2\u961f\u4e0e\u5341\u4e2a\u5bb6\u5ead\u5408\u4f5c\uff0c\u5171\u540c\u8bbe\u8ba1\u673a\u5668\u4eba\u5728\u7279\u5b9a\u5bb6\u5ead\u60ef\u4f8b\u4e2d\u7684\u884c\u4e3a\u4e0e\u4ea4\u4e92\uff0c\u4f9d\u636e\u65f6\u95f4\u3001\u53c2\u4e0e\u8005\u3001\u573a\u6240\u7b49\u73af\u5883\u56e0\u7d20\u5236\u5b9a\u8f85\u52a9\u8ba1\u5212\u3002\u968f\u540e\uff0c\u7814\u7a76\u8005\u5f00\u53d1\u5e76\u90e8\u7f72\u4e86\u53ef\u79fb\u52a8\u793e\u4ea4\u673a\u5668\u4eba\uff0c\u5e76\u8fdb\u884c\u4e86\u4e3a\u671f\u56db\u5929\u7684\u5165\u6237\u7528\u6237\u7814\u7a76\uff0c\u901a\u8fc7\u89c2\u5bdf\u548c\u8bbf\u8c08\u6536\u96c6\u6570\u636e\u3002", "result": "\u5927\u90e8\u5206\u5bb6\u5ead\u5bf9\u673a\u5668\u4eba\u63d0\u9192\u529f\u80fd\u8868\u793a\u6b22\u8fce\uff0c\u5c24\u5176\u7236\u6bcd\u8ba4\u53ef\u673a\u5668\u4eba\u5206\u62c5\u4e86\u90e8\u5206\u63d0\u9192\u804c\u8d23\u3002\u4f46\u7814\u7a76\u4e5f\u63ed\u793a\u4e86\u673a\u5668\u4eba\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u9047\u5230\u7684\u8bf8\u5982\u65f6\u95f4\u5b89\u6392\u3001\u6743\u5a01\u6027\u4e0e\u5bb6\u5ead\u52a8\u6001\u7b49\u591a\u91cd\u5f20\u529b\u3002", "conclusion": "\u4f5c\u8005\u63d0\u51fa\u4e86\u673a\u5668\u4eba\u8f85\u52a9\u60c5\u5883\u4efb\u52a1\u8bbe\u8ba1\u7684\u5efa\u8bae\uff0c\u5e76\u8ba8\u8bba\u4e86\u5bb6\u5ead\u573a\u666f\u4e0b\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u9700\u5173\u6ce8\u7684\u66f4\u5e7f\u6cdb\u95ee\u9898\uff0c\u6307\u51fa\u5355\u4e00\u4efb\u52a1\u89e3\u51b3\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u673a\u5668\u4eba\u5bb6\u5ead\u6574\u5408\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2602.22455", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22455", "abs": "https://arxiv.org/abs/2602.22455", "authors": ["Giuseppe Lando", "Rosario Forte", "Antonino Furnari"], "title": "Exploring Multimodal LMMs for Online Episodic Memory Question Answering on the Edge", "comment": null, "summary": "We investigate the feasibility of using Multimodal Large Language Models (MLLMs) for real-time online episodic memory question answering. While cloud offloading is common, it raises privacy and latency concerns for wearable assistants, hence we investigate implementation on the edge. We integrated streaming constraints into our question answering pipeline, which is structured into two asynchronous threads: a Descriptor Thread that continuously converts video into a lightweight textual memory, and a Question Answering (QA) Thread that reasons over the textual memory to answer queries. Experiments on the QAEgo4D-Closed benchmark analyze the performance of Multimodal Large Language Models (MLLMs) within strict resource boundaries, showing promising results also when compared to clound-based solutions. Specifically, an end-to-end configuration running on a consumer-grade 8GB GPU achieves 51.76% accuracy with a Time-To-First-Token (TTFT) of 0.41s. Scaling to a local enterprise-grade server yields 54.40% accuracy with a TTFT of 0.88s. In comparison, a cloud-based solution obtains an accuracy of 56.00%. These competitive results highlight the potential of edge-based solutions for privacy-preserving episodic memory retrieval.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5b9e\u73b0\u5b9e\u65f6\u5728\u7ebf\u60c5\u8282\u8bb0\u5fc6\u95ee\u7b54\u7684\u53ef\u884c\u6027\uff0c\u5e76\u53d6\u5f97\u4e86\u4e0e\u4e91\u7aef\u65b9\u6848\u76f8\u8fd1\u7684\u8868\u73b0\uff0c\u6709\u671b\u63d0\u9ad8\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u4e91\u7aef\u5904\u7406\u867d\u7136\u5e38\u89c1\uff0c\u4f46\u5728\u53ef\u7a7f\u6234\u52a9\u624b\u7b49\u573a\u666f\u4e0b\u5b58\u5728\u9690\u79c1\u4e0e\u5ef6\u8fdf\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u7814\u7a76\u5982\u4f55\u5728\u672c\u5730\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u60c5\u8282\u8bb0\u5fc6\u95ee\u7b54\u3002", "method": "\u4f5c\u8005\u5c06\u95ee\u7b54\u6d41\u7a0b\u5206\u4e3a\u4e24\u4e2a\u5f02\u6b65\u7ebf\u7a0b\uff1a\u63cf\u8ff0\u7ebf\u7a0b\u5c06\u89c6\u9891\u6d41\u5b9e\u65f6\u8f6c\u4e3a\u8f7b\u91cf\u7ea7\u6587\u672c\u8bb0\u5fc6\uff0c\u95ee\u7b54\u7ebf\u7a0b\u57fa\u4e8e\u8fd9\u4e00\u6587\u672c\u8bb0\u5fc6\u5b9e\u65f6\u56de\u7b54\u7528\u6237\u95ee\u9898\uff0c\u5e76\u5728\u4e25\u683c\u7684\u8d44\u6e90\u9650\u5236\u4e0b\u7528MLLMs\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728QAEgo4D-Closed\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u672c\u57308GB\u6d88\u8d39\u7ea7GPU\u4e0a\u7684\u7aef\u5230\u7aef\u914d\u7f6e\u8fbe\u523051.76%\u51c6\u786e\u7387\uff0c\u9996\u6b21\u54cd\u5e94\u65f6\u95f40.41s\uff1b\u4f01\u4e1a\u7ea7\u670d\u52a1\u5668\u53ef\u8fbe54.40%\u51c6\u786e\u7387\uff0c\u54cd\u5e94\u65f6\u95f40.88s\uff1b\u4e91\u7aef\u65b9\u6848\u4e3a56.00%\u51c6\u786e\u7387\u3002", "conclusion": "\u8fb9\u7f18\u7aef\u90e8\u7f72\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u540c\u65f6\uff0c\u80fd\u4ee5\u63a5\u8fd1\u4e91\u7aef\u6027\u80fd\u8fdb\u884c\u60c5\u8282\u8bb0\u5fc6\u68c0\u7d22\uff0c\u5c55\u73b0\u4e86\u8fb9\u7f18\u65b9\u6848\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.22524", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22524", "abs": "https://arxiv.org/abs/2602.22524", "authors": ["Samay Bhojwani", "Swarnima Kain", "Lisong Xu"], "title": "Iterative Prompt Refinement for Dyslexia-Friendly Text Summarization Using GPT-4o", "comment": null, "summary": "Dyslexia affects approximately 10% of the global population and presents persistent challenges in reading fluency and text comprehension. While existing assistive technologies address visual presentation, linguistic complexity remains a substantial barrier to equitable access. This paper presents an empirical study on dyslexia-friendly text summarization using an iterative prompt-based refinement pipeline built on GPT-4o. We evaluate the pipeline on approximately 2,000 news article samples, applying a readability target of Flesch Reading Ease >= 90. Results show that the majority of summaries meet the readability threshold within four attempts, with many succeeding on the first try. A composite score combining readability and semantic fidelity shows stable performance across the dataset, ranging from 0.13 to 0.73 with a typical value near 0.55. These findings establish an empirical baseline for accessibility-driven NLP summarization and motivate further human-centered evaluation with dyslexic readers.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPT-4o\u7684\u8fed\u4ee3\u5f0f\u63d0\u793a\u4f18\u5316\u6587\u672c\u6458\u8981\u7ba1\u9053\uff0c\u7528\u4e8e\u751f\u6210\u66f4\u6613\u4e8e\u9605\u8bfb\u969c\u788d\u8005\u7406\u89e3\u7684\u6587\u672c\u6458\u8981\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u65b0\u95fb\u6587\u672c\u7684\u53ef\u8bfb\u6027\uff0c\u5927\u90e8\u5206\u6458\u8981\u57284\u6b21\u4ee5\u5185\u5c31\u80fd\u8fbe\u6807\u3002", "motivation": "\u9605\u8bfb\u969c\u788d\u56f0\u6270\u5168\u7403\u7ea610%\u4eba\u53e3\uff0c\u73b0\u6709\u8f85\u52a9\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u5448\u73b0\uff0c\u5ffd\u89c6\u4e86\u6587\u672c\u672c\u8eab\u7684\u8bed\u8a00\u590d\u6742\u5ea6\uff0c\u56e0\u6b64\u969c\u788d\u8005\u4ecd\u96be\u4ee5\u5e73\u7b49\u83b7\u53d6\u4fe1\u606f\u3002\u9700\u8981\u5f00\u53d1\u80fd\u7b80\u5316\u8bed\u8a00\u7684\u81ea\u52a8\u5316\u6280\u672f\u3002", "method": "\u6784\u5efa\u57fa\u4e8eGPT-4o\u7684\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u6458\u8981\u751f\u6210\u6d41\u7a0b\uff0c\u5bf9\u7ea62,000\u7bc7\u65b0\u95fb\u8fdb\u884c\u6458\u8981\uff0c\u8bbe\u5b9aFlesch Reading Ease>=90\u4e3a\u53ef\u8bfb\u6027\u76ee\u6807\uff0c\u5206\u6790\u6bcf\u6b21\u751f\u6210\u7684\u6458\u8981\u662f\u5426\u8fbe\u6807\u3002\u5f15\u5165\u7efc\u5408\u5f97\u5206\u8bc4\u4ef7\u53ef\u8bfb\u6027\u4e0e\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5927\u591a\u6570\u6458\u8981\u5728\u56db\u6b21\u8fed\u4ee3\u6216\u66f4\u5c11\u5c31\u8fbe\u5230\u8bbe\u5b9a\u53ef\u8bfb\u6027\u6807\u51c6\uff0c\u8bb8\u591a\u6458\u8981\u9996\u6b21\u751f\u6210\u5373\u8fbe\u6807\u3002\u7efc\u5408\u5f97\u5206\u57280.13\u52300.73\u4e4b\u95f4\u6ce2\u52a8\uff0c\u5178\u578b\u503c\u7ea6\u4e3a0.55\uff0c\u8bf4\u660e\u5728\u8bed\u4e49\u548c\u53ef\u8bfb\u6027\u95f4\u53d6\u5f97\u4e86\u4e00\u5b9a\u5e73\u8861\u3002", "conclusion": "\u4e3a\u9762\u5411\u969c\u788d\u8005\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6587\u672c\u6458\u8981\u5efa\u7acb\u4e86\u5b9e\u8bc1\u57fa\u7ebf\uff0c\u4e3a\u540e\u7eed\u4ee5\u9605\u8bfb\u969c\u788d\u8005\u4e3a\u4e2d\u5fc3\u7684\u4eba\u4f53\u8bc4\u4f30\u63d0\u4f9b\u4e86\u52a8\u673a\u548c\u65b9\u6cd5\u3002"}}
{"id": "2602.22663", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22663", "abs": "https://arxiv.org/abs/2602.22663", "authors": ["Wenxuan Song", "Jiayi Chen", "Xiaoquan Sun", "Huashuo Lei", "Yikai Qin", "Wei Zhao", "Pengxiang Ding", "Han Zhao", "Tongxin Wang", "Pengxu Hou", "Zhide Zhong", "Haodong Yan", "Donglin Wang", "Jun Ma", "Haoang Li"], "title": "Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline", "comment": "Accepted by ICRA 2026", "summary": "Vision-Language-Action (VLA) models have emerged as a generalist robotic agent. However, existing VLAs are hindered by excessive parameter scales, prohibitive pre-training requirements, and limited applicability to diverse embodiments. To improve the practicality of VLAs, we propose a comprehensive benchmark and an improved baseline. First, we propose CEBench, a new benchmark spanning diverse embodiments in both simulation and the real world with consideration of domain randomization. We collect 14.4k simulated trajectories and 1.6k real-world expert-curated trajectories to support training on CEBench. Second, using CEBench as our testbed, we study three critical aspects of VLAs' practicality and offer several key findings. Informed by these findings, we introduce LLaVA-VLA, a lightweight yet powerful VLA designed for practical deployment on consumer-grade GPUs. Architecturally, it integrates a compact VLM backbone with multi-view perception, proprioceptive tokenization, and action chunking. To eliminate reliance on costly pre-training, LLaVA-VLA adopts a two-stage training paradigm including post-training and fine-tuning. Furthermore, LLaVA-VLA extends the action space to unify navigation and manipulation. Experiments across embodiments demonstrate the capabilities of generalization and versatility of LLaVA-VLA , while real-world mobile manipulation experiments establish it as the first end-to-end VLA model for mobile manipulation. We will open-source all datasets, codes, and checkpoints upon acceptance to foster reproducibility and future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CEBench\u8fd9\u4e00\u5168\u65b0\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u57fa\u51c6\u53ca\u6539\u8fdb\u7684\u8f7b\u91cf\u7ea7VLA\u57fa\u7ebf\u6a21\u578bLLaVA-VLA\uff0c\u4ece\u591a\u79cd\u4f53\u6001\u3001\u591a\u89c6\u89d2\u548c\u771f\u5b9e\u4e0e\u4eff\u771f\u573a\u666f\u4e0a\uff0c\u63d0\u5347\u4e86VLA\u673a\u5668\u4eba\u5b9e\u7528\u6027\u3002", "motivation": "\u76ee\u524dVLA\u6a21\u578b\u867d\u5177\u5907\u901a\u7528\u6027\u548c\u5f3a\u5927\u529f\u80fd\uff0c\u4f46\u8fc7\u4e8e\u5e9e\u5927\u7684\u53c2\u6570\u89c4\u6a21\u548c\u6602\u8d35\u7684\u9884\u8bad\u7ec3\u8d44\u6e90\u96be\u4ee5\u5b9e\u9645\u5e94\u7528\uff0c\u4e14\u9002\u7528\u6027\u53d7\u9650\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u63d0\u5347\u57fa\u51c6\u548c\u63d0\u51fa\u6539\u8fdb\u6a21\u578b\uff0c\u4f7f\u5176\u66f4\u6613\u4e8e\u5b9e\u9645\u90e8\u7f72\u548c\u5e94\u7528\u3002", "method": "1\uff09\u63d0\u51faCEBench\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u79cd\u4eff\u771f\u4e0e\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\uff0c\u6536\u96c6\u5927\u91cf\u4e13\u5bb6\u8f68\u8ff9\u6570\u636e\uff1b2\uff09\u5206\u6790VLA\u5b9e\u7528\u6027\u7684\u5173\u952e\u95ee\u9898\u5e76\u7ed9\u51fa\u89e3\u51b3\u65b9\u6848\uff1b3\uff09\u63d0\u51faLLaVA-VLA\u6a21\u578b\uff0c\u96c6\u6210\u7d27\u51d1\u7684\u591a\u6a21\u6001\u4e3b\u5e72\u3001\u611f\u77e5\u4e0e\u52a8\u4f5c\u6574\u5408\uff0c\u91c7\u7528\u9ad8\u6548\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u65e0\u9700\u5e9e\u5927\u9884\u8bad\u7ec3\uff0c\u7edf\u4e00\u5bfc\u822a\u548c\u64cd\u4f5c\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "LLaVA-VLA\u5728CEBench\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u4f18\u826f\u7684\u6cdb\u5316\u548c\u591a\u7528\u6027\uff1b\u5728\u771f\u5b9e\u79fb\u52a8\u64cd\u4f5c\u673a\u5668\u4eba\u4e0a\u9996\u6b21\u5b9e\u73b0\u7aef\u5230\u7aef\u7684VLA\u79fb\u52a8\u64cd\u4f5c\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "LLaVA-VLA\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u53ef\u884c\u6027\uff0c\u4e3a\u901a\u7528\u667a\u80fd\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u7aef\u5230\u7aef\u65b9\u6848\uff0c\u4e14\u76f8\u5173\u8d44\u6e90\u5c06\u5f00\u6e90\uff0c\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u548c\u7814\u7a76\u590d\u73b0\u3002"}}
{"id": "2602.22462", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.22462", "abs": "https://arxiv.org/abs/2602.22462", "authors": ["Raiyan Jahangir", "Nafiz Imtiaz Khan", "Amritanand Sudheerkumar", "Vladimir Filkov"], "title": "MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation", "comment": "arXiv preprint (submitted 25 Feb 2026). Local multi-model pipeline for mammography report generation + classification using prompting, multimodal RAG (ChromaDB), and QLoRA fine-tuning; evaluates MedGemma, LLaVA-Med, Qwen2.5-VL on VinDr-Mammo and DMID; reports BERTScore/ROUGE-L and classification metrics", "summary": "Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MammoWise\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u672c\u5730\u5316\u591a\u6a21\u578b\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u4e73\u817aX\u7ebf\u7b5b\u67e5\u62a5\u544a\u81ea\u52a8\u751f\u6210\u548c\u591a\u4efb\u52a1\u5206\u7c7b\uff0c\u652f\u6301\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548c\u6570\u636e\u96c6\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u7387\u4e14\u4fdd\u969c\u9690\u79c1\u3002", "motivation": "\u76ee\u524d\u4e73\u817aX\u7ebf\u7b5b\u67e5\u5bf9\u4e8e\u653e\u5c04\u79d1\u533b\u751f\u800c\u8a00\uff0c\u5c5e\u4e8e\u9ad8\u5f3a\u5ea6\u3001\u65f6\u95f4\u7d27\u8feb\u4e14\u6587\u6863\u91cf\u5927\u7684\u5de5\u4f5c\u3002\u4e3b\u6d41VLM\u7684\u5e94\u7528\u53d7\u5236\u4e8e\u4e91\u7aef\u7cfb\u7edf\u3001\u9690\u79c1\u95ee\u9898\u548c\u9002\u5e94\u6027\u5dee\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u672c\u5730\u53ef\u6269\u5c55\u3001\u9002\u5e94\u6027\u5f3a\u7684\u81ea\u52a8\u5316\u5de5\u5177\u4ee5\u63d0\u5347\u4e34\u5e8a\u5de5\u4f5c\u6548\u7387\u5e76\u4fdd\u969c\u6570\u636e\u5b89\u5168\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86MammoWise\u6846\u67b6\uff0c\u5c06\u5f00\u6e90VLM\uff08\u5982MedGemma, LLaVA-Med, Qwen2.5-VL\uff09\u672c\u5730\u5316\u90e8\u7f72\uff0c\u652f\u6301Ollama\u5e73\u53f0\u3001\u4e0d\u540c\u6570\u636e\u96c6\u3001\u65e0/\u5c0f\u6837\u672c/Chain-of-Thought\u63d0\u793a\uff0c\u4ee5\u53ca\u53ef\u9009\u7684\u591a\u6a21\u6001RAG\u589e\u5f3a\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08QLoRA\uff09\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728VinDr-Mammo\u548cDMID\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u62a5\u544a\u751f\u6210\u8d28\u91cf\u9ad8\u4e14\u901a\u8fc7\u5c11\u6837\u672c\u548cRAG\u65b9\u5f0f\u8fdb\u4e00\u6b65\u63d0\u5347\uff1b\u5206\u7c7b\u4efb\u52a1\u51c6\u786e\u7387\u4f9d\u8d56\u6a21\u578b\u4e0e\u6570\u636e\u96c6\uff0c\u5fae\u8c03\u540e\u7684MedGemma\u6a21\u578bBI-RADS\u3001\u5bc6\u5ea6\u3001\u9499\u5316\u51c6\u786e\u7387\u5206\u522b\u8fbe\u52300.7545\u30010.8840\u30010.9341\uff0c\u540c\u65f6\u4fdd\u6301\u62a5\u544a\u8d28\u91cf\u3002", "conclusion": "MammoWise\u662f\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u672c\u5730\u5316\u5f00\u6e90\u4e73\u817aX\u7ebf\u667a\u80fd\u62a5\u544a\u4e0e\u5206\u7c7b\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u5de5\u4f5c\u6d41\uff0c\u6709\u52a9\u4e8e\u9690\u79c1\u4fdd\u62a4\u3001\u7ed3\u679c\u590d\u73b0\u4e0e\u90e8\u7f72\u9002\u5e94\u6027\uff0c\u4e3a\u4e34\u5e8a\u4e73\u817a\u5f71\u50cf\u89e3\u8bfb\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2602.22543", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22543", "abs": "https://arxiv.org/abs/2602.22543", "authors": ["Huan Song", "Shuyu Tian", "Junyi Hao", "Minxiu Xu", "Hongjun An", "Yiliang Song", "Jiawei Shao", "Xuelong Li"], "title": "Ruyi2 Technical Report", "comment": null, "summary": "Large Language Models (LLMs) face significant challenges regarding deployment costs and latency, necessitating adaptive computing strategies. Building upon the AI Flow framework, we introduce Ruyi2 as an evolution of our adaptive model series designed for efficient variable-depth computation. While early-exit architectures offer a viable efficiency-performance balance, the Ruyi model and existing methods often struggle with optimization complexity and compatibility with large-scale distributed training. To bridge this gap, Ruyi2 introduces a stable \"Familial Model\" based on Megatron-LM. By using 3D parallel training, it achieves a 2-3 times speedup over Ruyi, while performing comparably to same-sized Qwen3 models. These results confirm that family-based parameter sharing is a highly effective strategy, establishing a new \"Train Once, Deploy Many\" paradigm and providing a key reference for balancing architectural efficiency with high-performance capabilities.", "AI": {"tldr": "Ruyi2\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u53d8\u6df1\u5ea6\u8ba1\u7b97\u53ca\u53c2\u6570\u5171\u4eab\u7684\u65b0\u578b\u5927\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u6548\u7387\u548c\u6027\u80fd\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u90e8\u7f72\u6210\u672c\u548c\u5ef6\u8fdf\u65b9\u9762\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u76ee\u524d\u65e9\u505c\u7ed3\u6784\u867d\u7136\u53ef\u4ee5\u6539\u5584\u6548\u7387\uff0c\u4f46\u4f18\u5316\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u517c\u5bb9\u6027\u5b58\u5728\u95ee\u9898\u3002", "method": "\u63d0\u51faRuyi2\u6a21\u578b\uff0c\u57fa\u4e8eMegatron-LM\u5b9e\u73b0\u201c\u5bb6\u65cf\u6a21\u578b\u201d\u53c2\u6570\u5171\u4eab\u7ed3\u6784\uff0c\u91c7\u75283D\u5e76\u884c\uff0c\u4f18\u5316\u4e86\u8bad\u7ec3\u6548\u7387\u4e0e\u5206\u5e03\u5f0f\u90e8\u7f72\u517c\u5bb9\u6027\u3002", "result": "Ruyi2\u6bd4\u4e0a\u4e00\u4ee3Ruyi\u6a21\u578b\u8bad\u7ec3\u63d0\u901f2-3\u500d\uff0c\u6027\u80fd\u4e0eQwen3\u540c\u5c3a\u5bf8\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "\u5bb6\u65cf\u5f0f\u53c2\u6570\u5171\u4eab\u7ed3\u5408\u5206\u5e03\u5f0f\u8bad\u7ec3\u662f\u63d0\u5347\u5927\u6a21\u578b\u6548\u7387\u548c\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0cRuyi2\u5b9e\u8df5\u4e86\u201c\u5355\u6b21\u8bad\u7ec3\uff0c\u591a\u7aef\u90e8\u7f72\u201d\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.22671", "categories": ["cs.RO", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.22671", "abs": "https://arxiv.org/abs/2602.22671", "authors": ["Georgios Papaioannou", "Barys Shyrokau"], "title": "Does the testing environment matter? Carsickness across on-road, test-track, and driving simulator conditions", "comment": null, "summary": "Carsickness has gained significant attention with the rise of automated vehicles, prompting extensive research across on-road, test-track, and driving simulator environments to understand its occurrence and develop mitigation strategies. However, the lack of carsickness standardization complicates comparisons across studies and environments. Previous works demonstrate measurement validity between two setups at most (e.g., on-road vs. driving simulator), leaving gaps in multi-environment comparisons. This study investigates the recreation of an on-road motion sickness exposure - previously replicated on a test track - using a motion-based driving simulator. Twenty-eight participants performed an eyes-off-road non-driving task while reporting motion sickness using the Misery Scale during the experiment and the Motion Sickness Assessment Questionnaire afterward. Psychological factors known to influence motion sickness were also assessed. The results present subjective and objective measurements for motion sickness across the considered environments. In this paper, acceleration measurements, objective metrics and subjective motion sickness ratings across environments are compared, highlighting key differences in sickness occurrence for simulator-based research validity. Significantly lower motion sickness scores are reported in the simulator compared to on-road and test-track conditions, due to its limited working envelope to reproduce low-frequency (<0.5 Hz) motions, which are the most provocative for motion sickness.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u5728\u771f\u5b9e\u9053\u8def\u3001\u6d4b\u8bd5\u8dd1\u9053\u548c\u57fa\u4e8e\u8fd0\u52a8\u7684\u9a7e\u9a76\u6a21\u62df\u5668\u73af\u5883\u4e0b\u6c7d\u8f66\u6655\u8f66\u53d1\u751f\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u6a21\u62df\u5668\u96be\u4ee5\u5b8c\u5168\u590d\u5236\u6613\u81f4\u6655\u8f66\u7684\u4f4e\u9891\u8fd0\u52a8\uff0c\u5bfc\u81f4\u5176\u6655\u8f66\u5f97\u5206\u660e\u663e\u4f4e\u4e8e\u5176\u4ed6\u4e24\u79cd\u73af\u5883\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u666e\u53ca\uff0c\u6c7d\u8f66\u6655\u8f66\u95ee\u9898\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u4e0d\u540c\u7814\u7a76\u73af\u5883\u4e0b\u7f3a\u4e4f\u6807\u51c6\u5316\uff0c\u5bfc\u81f4\u7814\u7a76\u7ed3\u679c\u96be\u4ee5\u6a2a\u5411\u6bd4\u8f83\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u6bd4\u8f83\u591a\u79cd\u73af\u5883\u4e0b\u6655\u8f66\u53cd\u5e94\u53ca\u5176\u53ef\u6bd4\u6027\uff0c\u63a8\u52a8\u6807\u51c6\u5316\u8fdb\u7a0b\u3002", "method": "\u62db\u52df28\u540d\u53c2\u4e0e\u8005\uff0c\u5728\u9a7e\u9a76\u6a21\u62df\u5668\u5185\u6267\u884c\u5206\u5fc3\u4efb\u52a1\uff0c\u540c\u65f6\u4f7f\u7528Misery\u91cf\u8868\u5728\u7ebf\u62a5\u544a\u6655\u8f66\u611f\u53d7\uff0c\u5e76\u5728\u4e8b\u540e\u586b\u5199\u6655\u8f66\u8bc4\u4f30\u95ee\u5377\u3002\u8fd8\u8bc4\u4f30\u4e86\u5df2\u77e5\u5f71\u54cd\u6655\u8f66\u7684\u5fc3\u7406\u56e0\u7d20\u3002\u5bf9\u6bd4\u5206\u6790\u4e86\u52a0\u901f\u5ea6\u3001\u5ba2\u89c2\u6570\u636e\u4e0e\u4e3b\u89c2\u6655\u8f66\u8bc4\u5206\uff0c\u5e76\u4e0e\u4e4b\u524d\u5728\u5b9e\u9645\u9053\u8def\u53ca\u6d4b\u8bd5\u8dd1\u9053\u83b7\u5f97\u7684\u6570\u636e\u5bf9\u6bd4\u3002", "result": "\u5728\u9a7e\u9a76\u6a21\u62df\u5668\u73af\u5883\u4e0b\uff0c\u6655\u8f66\u8bc4\u5206\u660e\u663e\u4f4e\u4e8e\u771f\u5b9e\u9053\u8def\u548c\u6d4b\u8bd5\u8dd1\u9053\uff0c\u4e24\u8005\u4e4b\u95f4\u5448\u73b0\u7edf\u8ba1\u5b66\u663e\u8457\u5dee\u5f02\u3002\u4e3b\u8981\u539f\u56e0\u662f\u6a21\u62df\u5668\u96be\u4ee5\u590d\u73b0\u5f15\u8d77\u6655\u8f66\u7684\u5173\u952e\u4f4e\u9891(<0.5 Hz)\u8fd0\u52a8\u6210\u5206\u3002", "conclusion": "\u76ee\u524d\u57fa\u4e8e\u8fd0\u52a8\u7684\u9a7e\u9a76\u6a21\u62df\u5668\u5728\u7814\u7a76\u6655\u8f66\u53d1\u751f\u673a\u5236\u53ca\u9632\u6cbb\u65f6\uff0c\u4ecd\u5b58\u5728\u73af\u5883\u6548\u5ea6\u5c40\u9650\u3002\u4e3a\u63d0\u5347\u6a21\u62df\u5668\u7814\u7a76\u7684\u6709\u6548\u6027\uff0c\u9700\u8981\u6539\u8fdb\u5176\u786c\u4ef6\u4ee5\u66f4\u597d\u5730\u518d\u73b0\u4f4e\u9891\u8bf1\u53d1\u8fd0\u52a8\u7279\u5f81\u3002"}}
{"id": "2602.22469", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22469", "abs": "https://arxiv.org/abs/2602.22469", "authors": ["Niamul Hassan Samin", "Md Arifur Rahman", "Abdullah Ibne Hanif", "Juena Ahmed Noshin", "Md Ashikur Rahman"], "title": "Beyond Dominant Patches: Spatial Credit Redistribution For Grounded Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) frequently hallucinate objects absent from the input image. We trace this failure to spatial credit collapse: activation credit concentrating on sparse visual patches in early transformer layers, which suppresses contextual evidence and increases reliance on language priors. We introduce Spatial Credit Redistribution (SCR), a training-free inference-time intervention that redistributes hidden-state activation from high-attention source patches to their context, guided by low-entropy inputs. We evaluate six model families (Chameleon, LLaVA, and Qwen, including both Qwen-VL and Qwen2-VL) at scales of 7B, 13B, and 30B, on POPE and CHAIR benchmarks. SCR reduces hallucination by ~4.7-6.0 percentage points on POPE-Adversarial, cuts CHAIR-s by 3.7-5.2 percentage points (42-51 percent relative), and CHAIR-i by 2.7-4.4 percentage points (44-58 percent relative), and preserves CIDEr within 0.8 percentage points. Gains are largest for low-entropy inputs, consistent with the theoretical framework. SCR incurs only 43-56 ms overhead (small models: +43-46 ms; large models: +54-56 ms), roughly 3-6 times lower than OPERA and VCD and 1.3-1.7 times lower than OVCD (+72 ms), while Pareto-dominating all three on both hallucination rate and CIDEr, making it practical for real-time settings. A controlled ablation confirms that attention-guided source selection is essential: replacing it with uniform random selection reduces hallucination rate gains from ~4.7-6.0 percentage points to only ~2.6-3.4 percentage points, pointing to credit-collapse as the key driver.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5SCR\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0d\u51c6\u786e\u5185\u5bb9\uff08\u5e7b\u89c9\u5bf9\u8c61\uff09\u7684\u73b0\u8c61\uff0c\u5728\u591a\u4e2a\u4e3b\u6d41\u6a21\u578b\u548c\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u4e0e\u9ad8\u6548\u6027\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u751f\u6210\u63cf\u8ff0\u65f6\u5e38\u5e38\u2018\u51ed\u7a7a\u2019\u63cf\u8ff0\u8f93\u5165\u56fe\u7247\u4e2d\u4e0d\u5b58\u5728\u7684\u5bf9\u8c61\uff08\u5373\u5e7b\u89c9\uff09\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u5b9e\u9645\u5e94\u7528\u3002\u4f5c\u8005\u8ffd\u8e2a\u5230\u8fd9\u4e00\u95ee\u9898\u6e90\u81eatransformer\u65e9\u671f\u5c42\u7684\u2018\u7a7a\u95f4\u4fe1\u7528\u584c\u7f29\u2019\uff0c\u5373\u6fc0\u6d3b\u4e3b\u8981\u805a\u96c6\u4e8e\u5c11\u6570\u89c6\u89c9\u5757\uff0c\u5ffd\u7565\u4e86\u8db3\u591f\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u9884\u8bad\u7ec3\u8bed\u8a00\u5148\u9a8c\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u65f6\u65e0\u4f9d\u8d56\u3001\u63a8\u7406\u9636\u6bb5\u53ef\u7528\u7684\u7a7a\u95f4\u4fe1\u7528\u91cd\u5206\u914d\uff08SCR\uff09\u65b9\u6cd5\u3002SCR\u5728\u6a21\u578b\u63a8\u7406\u65f6\uff0c\u501f\u52a9\u4f4e\u71b5\u8f93\u5165\u7684\u6307\u5bfc\uff0c\u5c06\u9ad8\u6ce8\u610f\u529b\u89c6\u89c9\u5757\u7684\u6fc0\u6d3b\u91cd\u65b0\u5206\u914d\u7ed9\u5176\u4e0a\u4e0b\u6587\u533a\u57df\uff0c\u4ece\u800c\u589e\u5f3a\u4e0a\u4e0b\u6587\u8bc1\u636e\u5e76\u51cf\u8f7b\u5e7b\u89c9\u95ee\u9898\u3002", "result": "\u5728POPE\u548cCHAIR\u7b49\u57fa\u51c6\u4e0a\uff0cSCR\u80fd\u5c06\u5e7b\u89c9\u73b0\u8c61\u51cf\u5c114.7-6.0\u4e2a\u767e\u5206\u70b9\uff08POPE-Adversarial\uff09\u3001CHAIR-s\u51cf\u5c113.7-5.2\u4e2a\u767e\u5206\u70b9\uff08\u76f8\u5bf9\u4e0b\u964d42-51%\uff09\u3001CHAIR-i\u51cf\u5c112.7-4.4\u4e2a\u767e\u5206\u70b9\uff08\u76f8\u5bf9\u4e0b\u964d44-58%\uff09\uff0c\u540c\u65f6CIDEr\u5206\u6570\u51e0\u4e4e\u65e0\u635f\uff08\u53d8\u5316\u57280.8\u4e2a\u767e\u5206\u70b9\u5185\uff09\u3002\u5b9e\u73b0\u5f00\u9500\u4f4e\uff0c\u4ec5\u589e\u52a043-56\u6beb\u79d2\uff0c\u8fdc\u4f4e\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SCR\u80fd\u6709\u6548\u6291\u5236VLM\u5e7b\u89c9\uff0c\u4e0d\u635f\u6027\u80fd\uff0c\u4e14\u5b9e\u7528\u9ad8\u6548\uff0c\u9002\u5408\u5b9e\u65f6\u573a\u666f\u3002\u5b9e\u8bc1\u4e0e\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0c\u5229\u7528\u6ce8\u610f\u529b\u5f15\u5bfc\u6e90\u5757\u9009\u62e9\u81f3\u5173\u91cd\u8981\uff0c\u9a8c\u8bc1\u4e86\u7a7a\u95f4\u4fe1\u7528\u584c\u7f29\u662f\u5e7b\u89c9\u95ee\u9898\u7684\u6838\u5fc3\u6210\u56e0\u3002"}}
{"id": "2602.22576", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22576", "abs": "https://arxiv.org/abs/2602.22576", "authors": ["Tianle Xia", "Ming Xu", "Lingxiang Hu", "Yiding Sun", "Wenwei Li", "Linfang Shang", "Liqun Liu", "Peng Shu", "Huan Yu", "Jie Jiang"], "title": "Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, yet traditional single-round retrieval struggles with complex multi-step reasoning. Agentic RAG addresses this by enabling LLMs to dynamically decide when and what to retrieve, but current RL-based training methods suffer from sparse outcome rewards that discard intermediate signals and low sample efficiency where failed samples contribute nothing. We propose Search-P1, a framework that introduces path-centric reward shaping for agentic RAG training, comprising two key components: (1) Path-Centric Reward, which evaluates the structural quality of reasoning trajectories through order-agnostic step coverage and soft scoring that extracts learning signals even from failed samples, and (2) Dual-Track Path Scoring with offline-generated reference planners that assesses paths from both self-consistency and reference-alignment perspectives. Experiments on multiple QA benchmarks demonstrate that Search-P1 achieves significant improvements over Search-R1 and other strong baselines, with an average accuracy gain of 7.7 points.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5Search-P1\uff0c\u901a\u8fc7\u8def\u5f84\u5956\u52b1\u673a\u5236\u6539\u8fdb\u4e86agentic RAG\u7684\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u95ee\u7b54\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5355\u8f6e\u68c0\u7d22RAG\u5bf9\u4e8e\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u6548\u679c\u6709\u9650\uff0cagentic RAG\u867d\u6539\u8fdb\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u5956\u52b1\u7a00\u758f\u548c\u6570\u636e\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSearch-P1\u6846\u67b6\uff0c\u6838\u5fc3\u5305\u62ec\uff1a1\uff09\u8def\u5f84\u5956\u52b1\u673a\u5236\uff0c\u91c7\u7528\u987a\u5e8f\u65e0\u5173\u3001\u8f6f\u8bc4\u5206\u65b9\u5f0f\u8bc4\u4f30\u63a8\u7406\u8def\u5f84\u5e76\u4ece\u5931\u8d25\u6837\u672c\u4e2d\u62bd\u53d6\u4fe1\u53f7\uff1b2\uff09\u53cc\u8f68\u8def\u5f84\u8bc4\u5206\uff0c\u540c\u65f6\u57fa\u4e8e\u81ea\u6d3d\u6027\u548c\u53c2\u8003\u5bf9\u9f50\u6027\u4f7f\u7528\u79bb\u7ebf\u89c4\u5212\u5668\u8bc4\u4ef7\u8def\u5f84\u3002", "result": "\u5728\u591a\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cSearch-P1\u8f83Search-R1\u548c\u5176\u5b83\u5f3a\u57fa\u7ebf\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53477.7\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u5f15\u5165\u8def\u5f84\u5956\u52b1\u673a\u5236\u80fd\u591f\u6709\u6548\u63d0\u9ad8agentic RAG\u7684\u8bad\u7ec3\u6548\u7387\u548c\u8868\u73b0\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u578b\u4efb\u52a1\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2602.22707", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22707", "abs": "https://arxiv.org/abs/2602.22707", "authors": ["Kai Li", "Shengtao Zheng", "Linkun Xiu", "Yuze Sheng", "Xiao-Ping Zhang", "Dongyue Huang", "Xinlei Chen"], "title": "SCOPE: Skeleton Graph-Based Computation-Efficient Framework for Autonomous UAV Exploration", "comment": "This paper has been accepted for publication in the IEEE ROBOTICS AND AUTOMATION LETTERS (RA-L). Please cite the paper using appropriate formats", "summary": "Autonomous exploration in unknown environments is key for mobile robots, helping them perceive, map, and make decisions in complex areas. However, current methods often rely on frequent global optimization, suffering from high computational latency and trajectory oscillation, especially on resource-constrained edge devices. To address these limitations, we propose SCOPE, a novel framework that incrementally constructs a real-time skeletal graph and introduces Implicit Unknown Region Analysis for efficient spatial reasoning. The planning layer adopts a hierarchical on-demand strategy: the Proximal Planner generates smooth, high-frequency local trajectories, while the Region-Sequence Planner is activated only when necessary to optimize global visitation order. Comparative evaluations in simulation demonstrate that SCOPE achieves competitive exploration performance comparable to state-of-the-art global planners, while reducing computational cost by an average of 86.9%. Real-world experiments further validate the system's robustness and low latency in practical scenarios.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86SCOPE\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u6784\u5efa\u9aa8\u67b6\u56fe\u548c\u5f15\u5165\u9690\u5f0f\u672a\u77e5\u533a\u57df\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u81ea\u4e3b\u63a2\u6d4b\u89c4\u5212\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u6d88\u8017\u7684\u540c\u65f6\u63d0\u5347\u8def\u5f84\u5e73\u6ed1\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u63a2\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u9891\u7e41\u7684\u5168\u5c40\u4f18\u5316\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u5ef6\u8fdf\u548c\u8def\u5f84\u9707\u8361\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u7b97\u529b\u6709\u9650\u7684\u8bbe\u5907\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5982\u4f55\u63d0\u5347\u63a2\u6d4b\u6548\u7387\u5e76\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u3002", "method": "SCOPE\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u521b\u65b0\u70b9\uff1a\u4e00\u662f\u589e\u91cf\u6784\u5efa\u7684\u5b9e\u65f6\u9aa8\u67b6\u56fe\uff0c\u4e8c\u662f\u8fdb\u884c\u9690\u5f0f\u672a\u77e5\u533a\u57df\u5206\u6790\u3002\u5176\u89c4\u5212\u5c42\u91c7\u7528\u5206\u5c42\u6309\u9700\u7b56\u7565\uff0c\u672c\u5730\u89c4\u5212\u5668\u9ad8\u9891\u751f\u6210\u5e73\u6ed1\u8def\u5f84\uff0c\u5168\u5c40\u89c4\u5212\u5668\u53ea\u5728\u5fc5\u8981\u65f6\u4f18\u5316\u8bbf\u95ee\u987a\u5e8f\uff0c\u4ece\u800c\u517c\u987e\u6027\u80fd\u548c\u6548\u7387\u3002", "result": "\u4eff\u771f\u5bf9\u6bd4\u5b9e\u9a8c\u8868\u660e\uff0cSCOPE\u5728\u63a2\u6d4b\u6548\u7387\u65b9\u9762\u4e0e\u6700\u5148\u8fdb\u5168\u5c40\u89c4\u5212\u5668\u6301\u5e73\uff0c\u4f46\u5e73\u5747\u51cf\u5c11\u4e8686.9%\u7684\u8ba1\u7b97\u6210\u672c\u3002\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e5f\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u9ad8\u9c81\u68d2\u6027\u548c\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "SCOPE\u6709\u6548\u7f13\u89e3\u4e86\u81ea\u4e3b\u63a2\u6d4b\u4e2d\u7684\u9ad8\u5ef6\u8fdf\u548c\u8ba1\u7b97\u538b\u529b\u95ee\u9898\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u65f6\u6027\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0\u3002"}}
{"id": "2602.22510", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22510", "abs": "https://arxiv.org/abs/2602.22510", "authors": ["Guoyizhe Wei", "Yang Jiao", "Nan Xi", "Zhishen Huang", "Jingjing Meng", "Rama Chellappa", "Yan Gao"], "title": "Pix2Key: Controllable Open-Vocabulary Retrieval with Semantic Decomposition and Self-Supervised Visual Dictionary Learning", "comment": null, "summary": "Composed Image Retrieval (CIR) uses a reference image plus a natural-language edit to retrieve images that apply the requested change while preserving other relevant visual content. Classic fusion pipelines typically rely on supervised triplets and can lose fine-grained cues, while recent zero-shot approaches often caption the reference image and merge the caption with the edit, which may miss implicit user intent and return repetitive results. We present Pix2Key, which represents both queries and candidates as open-vocabulary visual dictionaries, enabling intent-aware constraint matching and diversity-aware reranking in a unified embedding space. A self-supervised pretraining component, V-Dict-AE, further improves the dictionary representation using only images, strengthening fine-grained attribute understanding without CIR-specific supervision. On the DFMM-Compose benchmark, Pix2Key improves Recall@10 up to 3.2 points, and adding V-Dict-AE yields an additional 2.3-point gain while improving intent consistency and maintaining high list diversity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5Pix2Key\uff0c\u5c06\u67e5\u8be2\u548c\u5019\u9009\u56fe\u7247\u90fd\u8868\u793a\u4e3a\u89c6\u89c9\u8bcd\u5178\uff0c\u5b9e\u73b0\u610f\u56fe\u611f\u77e5\u548c\u591a\u6837\u6027\u6392\u5e8f\uff0c\u5728\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u5757V-Dict-AE\u52a0\u6301\u4e0b\uff0c\u65e0\u9700\u4e13\u95e8\u7684CIR\u6807\u6ce8\u5373\u53ef\u589e\u5f3a\u6548\u679c\u3002\u5b9e\u9a8c\u5728DFMM-Compose\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u5728\u878d\u5408\u67e5\u8be2\u56fe\u50cf\u548c\u8bed\u8a00\u7f16\u8f91\u65f6\uff0c\u5f80\u5f80\u635f\u5931\u7ec6\u7c92\u5ea6\u4fe1\u606f\u6216\u8005\u65e0\u6cd5\u7406\u89e3\u7528\u6237\u9690\u542b\u610f\u56fe\uff0c\u5bfc\u81f4\u7ed3\u679c\u5355\u4e00\u4e14\u4e0d\u7cbe\u786e\u3002", "method": "\u63d0\u51faPix2Key\uff0c\u628a\u56fe\u50cf\u53ca\u7f16\u8f91\u8bf7\u6c42\u90fd\u8f6c\u4e3a\u5f00\u653e\u8bcd\u6c47\u7684\u89c6\u89c9\u8bcd\u5178\uff0c\u5e76\u5728\u7edf\u4e00\u5d4c\u5165\u7a7a\u95f4\u505a\u610f\u56fe\u7ea6\u675f\u548c\u591a\u6837\u6027\u6392\u5e8f\u3002\u5f15\u5165\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7684V-Dict-AE\u6a21\u5757\uff0c\u4ec5\u7528\u56fe\u7247\u8bad\u7ec3\uff0c\u65e0\u9700CIR\u7279\u5b9a\u6807\u6ce8\uff0c\u63d0\u5347\u7ec6\u7c92\u5ea6\u5c5e\u6027\u8868\u5f81\u80fd\u529b\u3002", "result": "\u5728DFMM-Compose\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPix2Key\u5728Recall@10\u4e0a\u63d0\u53473.2\u4e2a\u767e\u5206\u70b9\u3002\u52a0\u5165V-Dict-AE\u540e\uff0c\u8fdb\u4e00\u6b65\u63d0\u53472.3\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u5728\u4fdd\u6301\u5217\u8868\u591a\u6837\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u610f\u56fe\u4e00\u81f4\u6027\u3002", "conclusion": "Pix2Key\u5b9e\u73b0\u4e86\u5bf9\u7528\u6237\u9690\u542b\u641c\u7d22\u610f\u56fe\u7684\u66f4\u597d\u5339\u914d\uff0c\u5728\u7ec6\u7c92\u5ea6\u68c0\u7d22\u548c\u7ed3\u679c\u591a\u6837\u6027\u65b9\u9762\u5747\u6709\u6539\u8fdb\uff0c\u81ea\u76d1\u7763\u6a21\u5757\u6709\u52a9\u4e8e\u8fdb\u4e00\u6b65\u63d0\u5347\uff0c\u65e0\u9700\u4e13\u95e8\u6807\u6ce8\uff0c\u5177\u5907\u5f88\u5f3a\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.22584", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22584", "abs": "https://arxiv.org/abs/2602.22584", "authors": ["Wenwei Li", "Ming Xu", "Tianle Xia", "Lingxiang Hu", "Yiding Sun", "Linfang Shang", "Liqun Liu", "Peng Shu", "Huan Yu", "Jie Jiang"], "title": "Towards Faithful Industrial RAG: A Reinforced Co-adaptation Framework for Advertising QA", "comment": null, "summary": "Industrial advertising question answering (QA) is a high-stakes task in which hallucinated content, particularly fabricated URLs, can lead to financial loss, compliance violations, and legal risk. Although Retrieval-Augmented Generation (RAG) is widely adopted, deploying it in production remains challenging because industrial knowledge is inherently relational, frequently updated, and insufficiently aligned with generation objectives. We propose a reinforced co-adaptation framework that jointly optimizes retrieval and generation through two components: (1) Graph-aware Retrieval (GraphRAG), which models entity-relation structure over a high-citation knowledge subgraph for multi-hop, domain-specific evidence selection; and (2) evidence-constrained reinforcement learning via Group Relative Policy Optimization (GRPO) with multi-dimensional rewards covering faithfulness, style compliance, safety, and URL validity. Experiments on an internal advertising QA dataset show consistent gains across expert-judged dimensions including accuracy, completeness, and safety, while reducing the hallucination rate by 72\\%. A two-week online A/B test demonstrates a 28.6\\% increase in like rate, a 46.2\\% decrease in dislike rate, and a 92.7\\% reduction in URL hallucination. The system has been running in production for over half a year and has served millions of QA interactions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5de5\u4e1a\u5e7f\u544a\u95ee\u7b54\u7684\u589e\u5f3a\u578b\u751f\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u865a\u5047\u5185\u5bb9\uff08\u7279\u522b\u662f\u4f2a\u9020URL\uff09\u4ea7\u751f\u7684\u98ce\u9669\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\uff0c\u5e76\u5df2\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u6210\u529f\u9a8c\u8bc1\u3002", "motivation": "\u5de5\u4e1a\u5e7f\u544a\u95ee\u7b54\u4e2d\u751f\u6210\u865a\u5047\u5185\u5bb9\uff08\u5982\u4f2a\u9020URL\uff09\u98ce\u9669\u6781\u9ad8\uff0c\u53ef\u80fd\u9020\u6210\u7ecf\u6d4e\u635f\u5931\u3001\u5408\u89c4\u8fdd\u89c4\u751a\u81f3\u6cd5\u5f8b\u98ce\u9669\u3002\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u5de5\u4e1a\u77e5\u8bc6\u7684\u590d\u6742\u6027\u548c\u591a\u53d8\u6027\uff0c\u56e0\u6b64\u4e9f\u9700\u66f4\u53ef\u9760\u3001\u66f4\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f3a\u5316\u5171\u9002\u5e94\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u68c0\u7d22\u548c\u751f\u6210\u73af\u8282\u3002\u5305\u542b\u4e24\u5927\u6838\u5fc3\u90e8\u5206\uff1a\uff081\uff09\u56fe\u7ed3\u6784\u611f\u77e5\u68c0\u7d22\uff08GraphRAG\uff09\u2014\u2014\u5229\u7528\u9ad8\u5f15\u7528\u77e5\u8bc6\u5b50\u56fe\u8fdb\u884c\u591a\u8df3\u3001\u9886\u57df\u7279\u5b9a\u8bc1\u636e\u9009\u62e9\uff1b\uff082\uff09\u57fa\u4e8e\u8bc1\u636e\u7ea6\u675f\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u91c7\u7528Group Relative Policy Optimization\uff08GRPO\uff09\uff0c\u901a\u8fc7\u591a\u7ef4\u5956\u52b1\uff08\u6db5\u76d6\u771f\u5b9e\u6027\u3001\u98ce\u683c\u5408\u89c4\u3001\u5b89\u5168\u6027\u3001URL\u6709\u6548\u6027\uff09\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5728\u5185\u90e8\u5e7f\u544a\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\uff0c\u7cfb\u7edf\u5728\u4e13\u5bb6\u8bc4\u5224\u7684\u51c6\u786e\u6027\u3001\u5b8c\u6574\u6027\u548c\u5b89\u5168\u6027\u7b49\u7ef4\u5ea6\u5168\u9762\u63d0\u5347\uff0c\u865a\u5047\u5185\u5bb9\uff08\u5e7b\u89c9\uff09\u7387\u964d\u4f4e72%\u3002\u7ebf\u4e0aA/B\u6d4b\u8bd5\u663e\u793a\u70b9\u8d5e\u7387\u63d0\u534728.6%\u3001\u70b9\u8e29\u7387\u964d46.2%\uff0cURL\u5e7b\u89c9\u51cf\u5c1192.7%\u3002\u8be5\u7cfb\u7edf\u5df2\u4e0a\u7ebf\u534a\u5e74\uff0c\u670d\u52a1\u6570\u767e\u4e07\u6b21\u95ee\u7b54\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u964d\u4f4e\u5de5\u4e1a\u5e7f\u544a\u95ee\u7b54\u7cfb\u7edf\u5e7b\u89c9\u3001\u63d0\u5347\u53ef\u9760\u6027\u4e0e\u7528\u6237\u6ee1\u610f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e0d\u4ec5\u6709\u6548\u964d\u4f4e\u4e1a\u52a1\u53ca\u5408\u89c4\u98ce\u9669\uff0c\u8fd8\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u843d\u5730\u3002"}}
{"id": "2602.22714", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.22714", "abs": "https://arxiv.org/abs/2602.22714", "authors": ["Philipp Schitz", "Paolo Mercorelli", "Johann C. Dauer"], "title": "Robust Helicopter Ship Deck Landing With Guaranteed Timing Using Shrinking-Horizon Model Predictive Control", "comment": "This version was submitted to the American Control Conference 2026 and has been accepted", "summary": "We present a runtime efficient algorithm for autonomous helicopter landings on moving ship decks based on Shrinking-Horizon Model Predictive Control (SHMPC). First, a suitable planning model capturing the relevant aspects of the full nonlinear helicopter dynamics is derived. Next, we use the SHMPC together with a touchdown controller stage to ensure a pre-specified maneuver time and an associated landing time window despite the presence of disturbances. A high disturbance rejection performance is achieved by designing an ancillary controller with disturbance feedback. Thus, given a target position and time, a safe landing with suitable terminal conditions is be guaranteed if the initial optimization problem is feasible. The efficacy of our approach is shown in simulation where all maneuvers achieve a high landing precision in strong winds while satisfying timing and operational constraints with maximum computation times in the millisecond range.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f29\u77ed\u89c6\u754c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08SHMPC\uff09\u7684\u9ad8\u6548\u81ea\u4e3b\u76f4\u5347\u673a\u5728\u79fb\u52a8\u8239\u7532\u677f\u4e0a\u7740\u9646\u7b97\u6cd5\uff0c\u5e76\u5728\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u7cbe\u5ea6\u548c\u5f3a\u6270\u52a8\u6291\u5236\u80fd\u529b\u3002", "motivation": "\u81ea\u4e3b\u76f4\u5347\u673a\u5728\u79fb\u52a8\u5e73\u53f0\uff08\u5982\u8239\u8236\u7532\u677f\uff09\u4e0a\u5b89\u5168\u9ad8\u6548\u7740\u9646\u6781\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u5f3a\u98ce\u7b49\u590d\u6742\u6270\u52a8\u6761\u4ef6\u4e0b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u5b89\u5168\u53c8\u80fd\u5b9e\u65f6\u5e94\u5bf9\u6270\u52a8\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u63a8\u5bfc\u51fa\u80fd\u53cd\u6620\u76f4\u5347\u673a\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7279\u6027\u7684\u89c4\u5212\u6a21\u578b\uff1b\u7ed3\u5408SHMPC\u548c\u964d\u843d\u9636\u6bb5\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u5bf9\u6307\u5b9a\u673a\u52a8\u65f6\u95f4\u548c\u7740\u9646\u7a97\u53e3\u7684\u4fdd\u8bc1\uff1b\u8bbe\u8ba1\u5e26\u6709\u6270\u52a8\u53cd\u9988\u7684\u8f85\u52a9\u63a7\u5236\u5668\u4ee5\u63d0\u5347\u6270\u52a8\u6291\u5236\u6027\u80fd\uff1b\u6574\u4e2a\u65b9\u6848\u4f9d\u8d56\u4e8e\u521d\u59cb\u4f18\u5316\u95ee\u9898\u7684\u53ef\u884c\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5f3a\u98ce\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u6beb\u79d2\u7ea7\u7684\u6700\u5927\u8ba1\u7b97\u65f6\u95f4\u5b8c\u6210\u5168\u90e8\u7740\u9646\u4efb\u52a1\uff0c\u6ee1\u8db3\u65f6\u5e8f\u548c\u64cd\u4f5c\u7ea6\u675f\uff0c\u4e14\u7740\u9646\u7cbe\u5ea6\u9ad8\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u53ef\u5728\u9ad8\u6270\u52a8\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u6548\u3001\u5b89\u5168\u3001\u5feb\u901f\u7684\u81ea\u4e3b\u76f4\u5347\u673a\u79fb\u52a8\u5e73\u53f0\u7740\u9646\uff0c\u5e76\u517c\u987e\u7cbe\u5ea6\u4e0e\u5b9e\u65f6\u6027\u3002"}}
{"id": "2602.22545", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22545", "abs": "https://arxiv.org/abs/2602.22545", "authors": ["Agamdeep S. Chopra", "Caitlin Neher", "Tianyi Ren", "Juampablo E. Heras Rivera", "Mehmet Kurt"], "title": "DisQ-HNet: A Disentangled Quantized Half-UNet for Interpretable Multimodal Image Synthesis Applications to Tau-PET Synthesis from T1 and FLAIR MRI", "comment": "14 pages, 8 figures, 8 tables; includes PID guided vector quantized latent factorization and sobel edge conditioned Half-UNet decoder", "summary": "Tau positron emission tomography (tau-PET) provides an in vivo marker of Alzheimer's disease pathology, but cost and limited availability motivate MRI-based alternatives. We introduce DisQ-HNet (DQH), a framework that synthesizes tau-PET from paired T1-weighted and FLAIR MRI while exposing how each modality contributes to the prediction. The method combines (i) a Partial Information Decomposition (PID)-guided, vector-quantized encoder that partitions latent information into redundant, unique, and complementary components, and (ii) a Half-UNet decoder that preserves anatomical detail using pseudo-skip connections conditioned on structural edge cues rather than direct encoder feature reuse. Across multiple baselines (VAE, VQ-VAE, and UNet), DisQ-HNet maintains reconstruction fidelity and better preserves disease-relevant signal for downstream AD tasks, including Braak staging, tau localization, and classification. PID-based Shapley analysis provides modality-specific attribution of synthesized uptake patterns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDisQ-HNet (DQH) \u7684\u65b0\u65b9\u6cd5\uff0c\u53ef\u5229\u7528MRI\u6570\u636e\u5408\u6210tau-PET\u5f71\u50cf\uff0c\u8fbe\u5230\u7c7b\u4f3cPET\u7684\u6548\u679c\uff0c\u6709\u52a9\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u68c0\u6d4b\u548c\u5206\u6790\u3002", "motivation": "tau-PET\u80fd\u663e\u793a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u5173\u952e\u75c5\u7406\uff0c\u4f46\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u666e\u53ca\uff0c\u56e0\u6b64\u9700\u8981\u57fa\u4e8eMRI\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u63d0\u5347\u75be\u75c5\u7b5b\u67e5\u6548\u7387\u548c\u53ef\u53ca\u6027\u3002", "method": "\u63d0\u51faDisQ-HNet\uff08DQH\uff09\u6846\u67b6\uff0c\u4f7f\u7528T1\u4e0eFLAIR MRI\u914d\u5bf9\u6570\u636e\uff0c\u7ed3\u5408PID\u5f15\u5bfc\u7684\u5411\u91cf\u91cf\u5316\u7f16\u7801\u5668\uff0c\u5c06\u7279\u5f81\u4fe1\u606f\u5206\u4e3a\u5197\u4f59\u3001\u552f\u4e00\u3001\u4e92\u8865\u4e09\u7c7b\uff0c\u5e76\u91c7\u7528\u534aUNet\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u7ed3\u6784\u8fb9\u7f18\u7ebf\u7d22\u5f15\u5bfc\u4f2a\u8df3\u8dc3\u8fde\u63a5\uff0c\u589e\u5f3a\u89e3\u5256\u7ec6\u8282\u4fdd\u7559\u3002", "result": "\u4e0eVAE\u3001VQ-VAE\u3001UNet\u7b49\u591a\u79cd\u57fa\u7ebf\u6bd4\u8f83\uff0cDQH\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u6301\u5e73\u4f46\u66f4\u597d\u5730\u4fdd\u7559\u4e86\u4e0e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u76f8\u5173\u7684\u4fe1\u53f7\uff0c\u63d0\u5347\u4e86\u540e\u7eed\u4efb\u52a1\u8868\u73b0\uff0c\u5982Braak\u5206\u671f\u3001tau\u86cb\u767d\u5b9a\u4f4d\u4e0e\u5206\u7c7b\u3002\u540c\u65f6\uff0cPID-Shapley\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540cMRI\u6a21\u6001\u5bf9\u9884\u6d4b\u7ed3\u679c\u7684\u5177\u4f53\u8d21\u732e\u3002", "conclusion": "DisQ-HNet\u4e3a\u57fa\u4e8eMRI\u7684tau-PET\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u66f4\u597d\u5730\u4fdd\u7559\u75be\u75c5\u76f8\u5173\u7279\u5f81\uff0c\u6709\u671b\u63a8\u52a8\u65e0\u521b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u7684\u4e34\u5e8a\u5e94\u7528\uff0c\u63d0\u9ad8\u666e\u53ca\u7387\u3002"}}
{"id": "2602.22661", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22661", "abs": "https://arxiv.org/abs/2602.22661", "authors": ["Zhanhui Zhou", "Lingjie Chen", "Hanghang Tong", "Dawn Song"], "title": "dLLM: Simple Diffusion Language Modeling", "comment": "Code available at: https://github.com/ZHZisZZ/dllm", "summary": "Although diffusion language models (DLMs) are evolving quickly, many recent models converge on a set of shared components. These components, however, are distributed across ad-hoc research codebases or lack transparent implementations, making them difficult to reproduce or extend. As the field accelerates, there is a clear need for a unified framework that standardizes these common components while remaining flexible enough to support new methods and architectures.\n  To address this gap, we introduce dLLM, an open-source framework that unifies the core components of diffusion language modeling -- training, inference, and evaluation -- and makes them easy to customize for new designs. With dLLM, users can reproduce, finetune, deploy, and evaluate open-source large DLMs such as LLaDA and Dream through a standardized pipeline. The framework also provides minimal, reproducible recipes for building small DLMs from scratch with accessible compute, including converting any BERT-style encoder or autoregressive LM into a DLM. We also release the checkpoints of these small DLMs to make DLMs more accessible and accelerate future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86dLLM\uff0c\u4e00\u4e2a\u7edf\u4e00\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08DLM\uff09\u6838\u5fc3\u7ec4\u4ef6\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u65b9\u4fbf\u6a21\u578b\u8bad\u7ec3\u3001\u63a8\u7406\u548c\u8bc4\u4f30\uff0c\u5e76\u652f\u6301\u5b9a\u5236\u548c\u62d3\u5c55\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5176\u5173\u952e\u7ec4\u4ef6\u5206\u6563\u5728\u4e0d\u540c\u7684\u4ee3\u7801\u5e93\u6216\u7f3a\u4e4f\u900f\u660e\u5b9e\u73b0\uff0c\u589e\u52a0\u4e86\u590d\u73b0\u548c\u62d3\u5c55\u96be\u5ea6\u3002\u9886\u57df\u52a0\u901f\u53d1\u5c55\u80cc\u666f\u4e0b\uff0c\u6025\u9700\u4e00\u4e2a\u6807\u51c6\u5316\u4e14\u7075\u6d3b\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51fadLLM\u6846\u67b6\uff0c\u5c06DLM\u8bad\u7ec3\u3001\u63a8\u7406\u548c\u8bc4\u4f30\u7b49\u6838\u5fc3\u529f\u80fd\u7edf\u4e00\uff0c\u5b9e\u73b0\u6807\u51c6\u5316\u6d41\u6c34\u7ebf\uff0c\u652f\u6301\u590d\u73b0\u3001\u5fae\u8c03\u3001\u90e8\u7f72\u4e0e\u8bc4\u4f30\u73b0\u6709DLM\uff08\u5982LLaDA\u548cDream\uff09\uff0c\u5e76\u53ef\u4ece\u5934\u6784\u5efa\u5c0f\u578bDLM\uff0c\u5bf9BERT\u98ce\u683c\u7f16\u7801\u5668\u6216\u81ea\u56de\u5f52LM\u8fdb\u884c\u8f6c\u6362\u3002", "result": "dLLM\u4e0d\u4ec5\u4f7f\u7528\u6237\u80fd\u5feb\u901f\u64cd\u4f5c\u4e3b\u6d41\u53ca\u81ea\u5b9a\u4e49DLM\uff0c\u8fd8\u63d0\u4f9b\u5c0f\u578bDLM\u7684\u8bad\u7ec3\u811a\u672c\u548c\u6a21\u578b\u53c2\u6570\uff08checkpoints\uff09\uff0c\u5927\u5e45\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u590d\u73b0\u3001\u5f00\u53d1\u548c\u666e\u53ca\u3002", "conclusion": "dLLM\u4f5c\u4e3aDLM\u9886\u57df\u7684\u6807\u51c6\u5316\u5de5\u5177\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u590d\u73b0\u6027\u3001\u53ef\u6269\u5c55\u6027\u53ca\u6613\u7528\u6027\uff0c\u6709\u671b\u52a0\u901f\u8be5\u9886\u57df\u7684\u53d1\u5c55\u548c\u521b\u65b0\u3002"}}
{"id": "2602.22731", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22731", "abs": "https://arxiv.org/abs/2602.22731", "authors": ["Miguel \u00c1ngel Mu\u00f1oz-Ba\u00f1\u00f3n", "Nived Chebrolu", "Sruthi M. Krishna Moorthy", "Yifu Tao", "Fernando Torres", "Roberto Salguero-G\u00f3mez", "Maurice Fallon"], "title": "Sapling-NeRF: Geo-Localised Sapling Reconstruction in Forests for Ecological Monitoring", "comment": null, "summary": "Saplings are key indicators of forest regeneration and overall forest health. However, their fine-scale architectural traits are difficult to capture with existing 3D sensing methods, which make quantitative evaluation difficult. Terrestrial Laser Scanners (TLS), Mobile Laser Scanners (MLS), or traditional photogrammetry approaches poorly reconstruct thin branches, dense foliage, and lack the scale consistency needed for long-term monitoring. Implicit 3D reconstruction methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) are promising alternatives, but cannot recover the true scale of a scene and lack any means to be accurately geo-localised. In this paper, we present a pipeline which fuses NeRF, LiDAR SLAM, and GNSS to enable repeatable, geo-localised ecological monitoring of saplings. Our system proposes a three-level representation: (i) coarse Earth-frame localisation using GNSS, (ii) LiDAR-based SLAM for centimetre-accurate localisation and reconstruction, and (iii) NeRF-derived object-centric dense reconstruction of individual saplings. This approach enables repeatable quantitative evaluation and long-term monitoring of sapling traits. Our experiments in forest plots in Wytham Woods (Oxford, UK) and Evo (Finland) show that stem height, branching patterns, and leaf-to-wood ratios can be captured with increased accuracy as compared to TLS. We demonstrate that accurate stem skeletons and leaf distributions can be measured for saplings with heights between 0.5m and 2m in situ, giving ecologists access to richer structural and quantitative data for analysing forest dynamics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408NeRF\u3001\u6fc0\u5149SLAM\u548cGNSS\u7684\u4e09\u5c42\u878d\u5408\u7ba1\u9053\uff0c\u7528\u4e8e\u5b9e\u73b0\u5e7c\u6811\u7684\u53ef\u91cd\u590d\u3001\u53ef\u5730\u7406\u5b9a\u4f4d\u7684\u751f\u6001\u76d1\u6d4b\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u7ed3\u6784\u6027\u548c\u5b9a\u91cf\u5206\u6790\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u67093D\u611f\u77e5\u6280\u672f\u5728\u6355\u6349\u5e7c\u6811\u7ec6\u679d\u7ed3\u6784\u548c\u5bc6\u96c6\u53f6\u7247\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e14\u5c3a\u5ea6\u4e0d\u4e00\u81f4\uff0c\u96be\u4ee5\u957f\u671f\u3001\u5b9a\u91cf\u5730\u76d1\u6d4b\u68ee\u6797\u518d\u751f\u72b6\u51b5\u3002\u9690\u5f0f3D\u91cd\u5efa\u65b9\u6cd5\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u65e0\u6cd5\u51c6\u786e\u6062\u590d\u573a\u666f\u771f\u5b9e\u6bd4\u4f8b\u548c\u5730\u7406\u4f4d\u7f6e\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u5b9e\u73b0\u5c3a\u5ea6\u7edf\u4e00\u4e14\u5730\u7406\u5b9a\u4f4d\u51c6\u786e\u7684\u5e7c\u6811\u7ed3\u6784\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4f53\u5316\u4e09\u5c42\u4fe1\u606f\u878d\u5408\u6d41\u7a0b\uff1a(1) \u5229\u7528GNSS\u5b9e\u73b0\u5730\u7403\u5750\u6807\u7cfb\u4e0b\u7684\u7c97\u5b9a\u4f4d\uff1b(2) \u501f\u52a9LiDAR SLAM\u5b9e\u73b0\u5398\u7c73\u7ea7\u7684\u5c40\u90e8\u5b9a\u4f4d\u548c\u91cd\u5efa\uff1b(3) \u901a\u8fc7NeRF\u8fdb\u884c\u5bf9\u8c61\u4e2d\u5fc3\u7684\u7a20\u5bc6\u5e7c\u68113D\u7ed3\u6784\u91cd\u5efa\u3002\u6b64\u65b9\u6cd5\u53ef\u5b9e\u73b0\u5bf9\u5e7c\u6811\u7ed3\u6784\u7684\u9ad8\u5206\u8fa8\u7387\u6355\u6349\uff0c\u5e76\u786e\u4fdd\u957f\u671f\u53ef\u91cd\u590d\u7684\u5730\u7406\u914d\u51c6\u3002", "result": "\u5728\u82f1\u56fd\u548c\u82ac\u5170\u7684\u68ee\u6797\u6837\u5730\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7ba1\u9053\u80fd\u6bd4\u4f20\u7edfTLS\u65b9\u6cd5\u66f4\u51c6\u786e\u83b7\u53d6\u5e7c\u6811\u6811\u5e72\u9ad8\u5ea6\u3001\u5206\u679d\u7ed3\u6784\u548c\u53f6/\u6728\u6bd4\u4f8b\uff0c\u540c\u65f6\u53ef\u5bf90.5-2\u7c73\u9ad8\u7684\u5e7c\u6811\u8fdb\u884c\u7cbe\u51c6\u7684\u9aa8\u67b6\u548c\u53f6\u7247\u5206\u5e03\u6d4b\u91cf\u3002", "conclusion": "\u8be5\u878d\u5408\u65b9\u6cd5\u80fd\u4e3a\u751f\u6001\u5b66\u5bb6\u63d0\u4f9b\u66f4\u4e30\u5bcc\u3001\u7ed3\u6784\u5316\u548c\u53ef\u91cf\u5316\u7684\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u5206\u6790\u548c\u957f\u671f\u76d1\u6d4b\u68ee\u6797\u52a8\u6001\uff0c\u5bf9\u5e7c\u6811\u53ca\u68ee\u6797\u5065\u5eb7\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.22549", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22549", "abs": "https://arxiv.org/abs/2602.22549", "authors": ["Zhechao Wang", "Yiming Zeng", "Lufan Ma", "Zeqing Fu", "Chen Bai", "Ziyao Lin", "Cheng Lu"], "title": "DrivePTS: A Progressive Learning Framework with Textual and Structural Enhancement for Driving Scene Generation", "comment": null, "summary": "Synthesis of diverse driving scenes serves as a crucial data augmentation technique for validating the robustness and generalizability of autonomous driving systems. Current methods aggregate high-definition (HD) maps and 3D bounding boxes as geometric conditions in diffusion models for conditional scene generation. However, implicit inter-condition dependency causes generation failures when control conditions change independently. Additionally, these methods suffer from insufficient details in both semantic and structural aspects. Specifically, brief and view-invariant captions restrict semantic contexts, resulting in weak background modeling. Meanwhile, the standard denoising loss with uniform spatial weighting neglects foreground structural details, causing visual distortions and blurriness. To address these challenges, we propose DrivePTS, which incorporates three key innovations. Firstly, our framework adopts a progressive learning strategy to mitigate inter-dependency between geometric conditions, reinforced by an explicit mutual information constraint. Secondly, a Vision-Language Model is utilized to generate multi-view hierarchical descriptions across six semantic aspects, providing fine-grained textual guidance. Thirdly, a frequency-guided structure loss is introduced to strengthen the model's sensitivity to high-frequency elements, improving foreground structural fidelity. Extensive experiments demonstrate that our DrivePTS achieves state-of-the-art fidelity and controllability in generating diverse driving scenes. Notably, DrivePTS successfully generates rare scenes where prior methods fail, highlighting its strong generalization ability.", "AI": {"tldr": "DrivePTS\u901a\u8fc7\u591a\u5c42\u521b\u65b0\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u5408\u6210\u8d28\u91cf\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u771f\u5b9e\u6027\u548c\u53ef\u63a7\u6027\uff0c\u5c24\u5176\u9002\u5408\u751f\u6210\u7f55\u89c1\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u5408\u6210\u65b9\u6cd5\u5728\u53d8\u5316\u6761\u4ef6\u4e0b\u751f\u6210\u5bb9\u6613\u5931\u8d25\uff0c\u4e14\u751f\u6210\u573a\u666f\u7684\u8bed\u4e49\u3001\u7ed3\u6784\u7ec6\u8282\u4e0d\u8db3\uff0c\u80cc\u666f\u5efa\u6a21\u548c\u524d\u666f\u7ed3\u6784\u63cf\u8ff0\u90fd\u8f83\u5f31\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u6570\u636e\u8d28\u91cf\u3002", "method": "DrivePTS\u5305\u542b\u4e09\u5927\u521b\u65b0\uff1a\uff081\uff09\u63d0\u51fa\u6e10\u8fdb\u5f0f\u5b66\u4e60\u548c\u4e92\u4fe1\u606f\u7ea6\u675f\uff0c\u51cf\u5c11\u63a7\u5236\u6761\u4ef6\u95f4\u7684\u4f9d\u8d56\u6027\uff1b\uff082\uff09\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u751f\u6210\u591a\u89c6\u89d2\u3001\u591a\u5c42\u9762\u7ec6\u81f4\u8bed\u4e49\u63cf\u8ff0\uff0c\u4e30\u5bcc\u573a\u666f\u8bed\u4e49\u6307\u5bfc\uff1b\uff083\uff09\u5f15\u5165\u9891\u7387\u5f15\u5bfc\u7ed3\u6784\u635f\u5931\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u7ed3\u6784\u7ec6\u8282\uff08\u9ad8\u9891\u4fe1\u606f\uff09\u7684\u654f\u611f\u6027\uff0c\u63d0\u5347\u524d\u666f\u4fdd\u771f\u5ea6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1DrivePTS\u5728\u751f\u6210\u591a\u6837\u5316\u9a7e\u9a76\u573a\u666f\u65f6\uff0c\u5728\u771f\u5b9e\u6027\u3001\u53ef\u63a7\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u5728\u524d\u4eba\u5931\u8d25\u7684\u7f55\u89c1\u573a\u666f\u4e0b\u6b63\u5e38\u751f\u6210\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DrivePTS\u6709\u6548\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u751f\u6210\u7684\u8d28\u91cf\u4e0e\u63a7\u5236\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u591a\u9879\u5c40\u9650\uff0c\u5bf9\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u548c\u6570\u636e\u589e\u5e7f\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.22675", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22675", "abs": "https://arxiv.org/abs/2602.22675", "authors": ["Qianben Chen", "Tianrui Qin", "King Zhu", "Qiexiang Wang", "Chengjun Yu", "Shu Xu", "Jiaqi Wu", "Jiayu Zhang", "Xinpeng Liu", "Xin Gui", "Jingyi Cao", "Piaohong Wang", "Dingfeng Shi", "He Zhu", "Tiannan Wang", "Yuqing Wang", "Maojia Song", "Tianyu Zheng", "Ge Zhang", "Jian Yang", "Jiaheng Liu", "Minghao Liu", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization", "comment": "12 pages, 5 figures", "summary": "Recent deep research agents primarily improve performance by scaling reasoning depth, but this leads to high inference cost and latency in search-intensive scenarios. Moreover, generalization across heterogeneous research settings remains challenging. In this work, we propose \\emph{Search More, Think Less} (SMTL), a framework for long-horizon agentic search that targets both efficiency and generalization. SMTL replaces sequential reasoning with parallel evidence acquisition, enabling efficient context management under constrained context budgets. To support generalization across task types, we further introduce a unified data synthesis pipeline that constructs search tasks spanning both deterministic question answering and open-ended research scenarios with task appropriate evaluation metrics. We train an end-to-end agent using supervised fine-tuning and reinforcement learning, achieving strong and often state of the art performance across benchmarks including BrowseComp (48.6\\%), GAIA (75.7\\%), Xbench (82.0\\%), and DeepResearch Bench (45.9\\%). Compared to Mirothinker-v1.0, SMTL with maximum 100 interaction steps reduces the average number of reasoning steps on BrowseComp by 70.7\\%, while improving accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u901a\u7528\u7684\u957f\u4efb\u52a1\u63a8\u7406\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u641c\u7d22\u63d0\u5347\u6548\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u63a8\u7406\u667a\u80fd\u4f53\u591a\u901a\u8fc7\u589e\u52a0\u63a8\u7406\u6df1\u5ea6\u6765\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u8fd9\u5bfc\u81f4\u9ad8\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u4e14\u96be\u4ee5\u5728\u4e0d\u540c\u7c7b\u578b\u4efb\u52a1\u95f4\u6cdb\u5316\u3002", "method": "\u63d0\u51faSearch More, Think Less\uff08SMTL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u8bc1\u636e\u68c0\u7d22\u66ff\u4ee3\u4e32\u884c\u63a8\u7406\uff0c\u6709\u6548\u7ba1\u7406\u6709\u9650\u4e0a\u4e0b\u6587\u9884\u7b97\uff1b\u540c\u65f6\u6784\u5efa\u7edf\u4e00\u7684\u6570\u636e\u5408\u6210\u6d41\u7a0b\uff0c\u6db5\u76d6\u786e\u5b9a\u6027\u95ee\u7b54\u548c\u5f00\u653e\u5f0f\u7814\u7a76\u4efb\u52a1\uff1b\u91c7\u7528\u6709\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u8bad\u7ec3\u4ee3\u7406\u3002", "result": "\u5728BrowseComp\u3001GAIA\u3001Xbench\u548cDeepResearch Bench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u53d6\u5f97\u4f18\u5f02\uff08\u751a\u81f3SOTA\uff09\u6210\u7ee9\u3002\u5982\u5728BrowseComp\u4efb\u52a1\u4e0a\uff0c\u63a8\u7406\u6b65\u6570\u51cf\u5c1170.7%\uff0c\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "SMTL\u5b9e\u73b0\u4e86\u5728\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u63d0\u5347\uff0c\u4e3a\u957f\u4efb\u52a1\u63a8\u7406\u667a\u80fd\u4f53\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2602.22733", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22733", "abs": "https://arxiv.org/abs/2602.22733", "authors": ["Seongyong Kim", "Junhyeon Cho", "Kang-Won Lee", "Soo-Chul Lim"], "title": "Pixel2Catch: Multi-Agent Sim-to-Real Transfer for Agile Manipulation with a Single RGB Camera", "comment": null, "summary": "To catch a thrown object, a robot must be able to perceive the object's motion and generate control actions in a timely manner. Rather than explicitly estimating the object's 3D position, this work focuses on a novel approach that recognizes object motion using pixel-level visual information extracted from a single RGB image. Such visual cues capture changes in the object's position and scale, allowing the policy to reason about the object's motion. Furthermore, to achieve stable learning in a high-DoF system composed of a robot arm equipped with a multi-fingered hand, we design a heterogeneous multi-agent reinforcement learning framework that defines the arm and hand as independent agents with distinct roles. Each agent is trained cooperatively using role-specific observations and rewards, and the learned policies are successfully transferred from simulation to the real world.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u5e27RGB\u56fe\u50cf\u50cf\u7d20\u7ea7\u89c6\u89c9\u4fe1\u606f\u6765\u8bc6\u522b\u629b\u63b7\u7269\u4f53\u8fd0\u52a8\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u80fd\u591f\u5b66\u4e60\u5e76\u6210\u529f\u5728\u73b0\u5b9e\u4e2d\u6293\u4f4f\u629b\u63b7\u7269\u4f53\u3002", "motivation": "\u4f20\u7edf\u673a\u68b0\u81c2\u6293\u53d6\u629b\u63b7\u7269\u4f53\u9700\u8981\u7cbe\u786e\u4f30\u8ba1\u7269\u4f53\u4e09\u7ef4\u4f4d\u7f6e\uff0c\u5177\u6709\u6280\u672f\u96be\u5ea6\u548c\u8ba1\u7b97\u5f00\u9500\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u66f4\u9ad8\u6548\u7684\u89c6\u89c9\u611f\u77e5\u548c\u65b0\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u5347\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u5728\u5b9e\u9645\u590d\u6742\u52a8\u6001\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4ee5\u5355\u5e27RGB\u56fe\u50cf\u50cf\u7d20\u7ea7\u89c6\u89c9\u7279\u5f81\u8bc6\u522b\u7269\u4f53\u8fd0\u52a8\uff0c\u907f\u514d\u4e09\u7ef4\u5750\u6807\u4f30\u8ba1\uff1b\u5e76\u8bbe\u8ba1\u5c06\u673a\u68b0\u81c2\u548c\u591a\u6307\u624b\u5206\u8bbe\u4e3a\u72ec\u7acb\u667a\u80fd\u4f53\u3001\u5404\u81ea\u4f9d\u636e\u7279\u5b9a\u89c2\u6d4b\u548c\u5956\u52b1\u4fe1\u53f7\u534f\u4f5c\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u3002\u8bad\u7ec3\u5728\u4eff\u771f\u5b8c\u6210\u540e\u65e0\u7f1d\u8fc1\u79fb\u81f3\u771f\u5b9e\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8bad\u7ec3\u51fa\u7684\u7b56\u7565\u4e0d\u4ec5\u80fd\u5728\u4eff\u771f\u4e2d\u6709\u6548\u5b8c\u6210\u7269\u4f53\u63a5\u7403\u4efb\u52a1\uff0c\u8fd8\u80fd\u6210\u529f\u8fc1\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u7cfb\u7edf\u8fdb\u884c\u6293\u53d6\u3002", "conclusion": "\u65e0\u9700\u4e09\u7ef4\u4f4d\u7f6e\u663e\u5f0f\u4f30\u8ba1\uff0c\u53ea\u5229\u7528\u5355\u5e27\u89c6\u89c9\u4fe1\u606f\u53ca\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5373\u53ef\u663e\u8457\u63d0\u9ad8\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u63a5\u7403\u80fd\u529b\uff0c\u76f8\u5173\u7b56\u7565\u5177\u5907\u826f\u597d\u7684\u73b0\u5b9e\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.22565", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.22565", "abs": "https://arxiv.org/abs/2602.22565", "authors": ["Kang Han", "Wei Xiang", "Lu Yu", "Mathew Wyatt", "Gaowen Liu", "Ramana Rao Kompella"], "title": "SwiftNDC: Fast Neural Depth Correction for High-Fidelity 3D Reconstruction", "comment": null, "summary": "Depth-guided 3D reconstruction has gained popularity as a fast alternative to optimization-heavy approaches, yet existing methods still suffer from scale drift, multi-view inconsistencies, and the need for substantial refinement to achieve high-fidelity geometry. Here, we propose SwiftNDC, a fast and general framework built around a Neural Depth Correction field that produces cross-view consistent depth maps. From these refined depths, we generate a dense point cloud through back-projection and robust reprojection-error filtering, obtaining a clean and uniformly distributed geometric initialization for downstream reconstruction. This reliable dense geometry substantially accelerates 3D Gaussian Splatting (3DGS) for mesh reconstruction, enabling high-quality surfaces with significantly fewer optimization iterations. For novel-view synthesis, SwiftNDC can also improve 3DGS rendering quality, highlighting the benefits of strong geometric initialization. We conduct a comprehensive study across five datasets, including two for mesh reconstruction, as well as three for novel-view synthesis. SwiftNDC consistently reduces running time for accurate mesh reconstruction and boosts rendering fidelity for view synthesis, demonstrating the effectiveness of combining neural depth refinement with robust geometric initialization for high-fidelity and efficient 3D reconstruction.", "AI": {"tldr": "SwiftNDC\u662f\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u6df1\u5ea6\u6821\u6b63\u7684\u65b0\u578b\u9ad8\u65483D\u91cd\u5efa\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u6df1\u5ea6\u7684\u4e00\u81f4\u6027\u548c\u7cbe\u5ea6\uff0c\u663e\u8457\u52a0\u901f\u5e76\u4f18\u5316\u4e863D\u7f51\u683c\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5f15\u5bfc\u578b3D\u91cd\u5efa\u65b9\u6cd5\u867d\u7136\u8fd0\u884c\u901f\u5ea6\u5feb\uff0c\u4f46\u6613\u51fa\u73b0\u5c3a\u5ea6\u6f02\u79fb\u3001\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\uff0c\u5e76\u4e14\u4ecd\u9700\u5927\u91cf\u540e\u5904\u7406\u624d\u80fd\u83b7\u5f97\u9ad8\u8d28\u91cf\u51e0\u4f55\u4f53\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6df1\u5ea6\u4e00\u81f4\u6027\u548c\u51e0\u4f55\u521d\u59cb\u5316\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSwiftNDC\u6846\u67b6\uff0c\u4f7f\u7528\u795e\u7ecf\u6df1\u5ea6\u6821\u6b63\u6cd5\u751f\u6210\u8de8\u89c6\u89d2\u4e00\u81f4\u7684\u6df1\u5ea6\u56fe\uff0c\u518d\u901a\u8fc7\u56de\u6295\u5f71\u548c\u9c81\u68d2\u91cd\u6295\u5f71\u8bef\u5dee\u6ee4\u6ce2\u751f\u6210\u9ad8\u8d28\u91cf\u5bc6\u96c6\u70b9\u4e91\u3002\u8fd9\u4e3a\u540e\u7eed\u76843D Gaussian Splatting\uff083DGS\uff09\u7f51\u683c\u91cd\u5efa\u63d0\u4f9b\u5e72\u51c0\u3001\u4e00\u81f4\u7684\u51e0\u4f55\u521d\u59cb\u5316\uff0c\u51cf\u5c11\u4e86\u4f18\u5316\u8fed\u4ee3\u6b21\u6570\u3002", "result": "SwiftNDC\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u6db5\u76d6\u7f51\u683c\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u52a0\u901f\u7cbe\u786e\u7f51\u683c\u91cd\u5efa\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "\u5c06\u795e\u7ecf\u6df1\u5ea6\u7cbe\u70bc\u4e0e\u9c81\u68d2\u51e0\u4f55\u521d\u59cb\u5316\u76f8\u7ed3\u5408\uff0cSwiftNDC\u4e3a\u9ad8\u4fdd\u771f\u3001\u9ad8\u6548\u7387\u76843D\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u987e\u4e86\u91cd\u5efa\u901f\u5ea6\u4e0e\u8d28\u91cf\u3002"}}
{"id": "2602.22696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22696", "abs": "https://arxiv.org/abs/2602.22696", "authors": ["Shinnosuke Nozue", "Yuto Nakano", "Yotaro Watanabe", "Meguru Takasaki", "Shoji Moriya", "Reina Akama", "Jun Suzuki"], "title": "Enhancing Persuasive Dialogue Agents by Synthesizing Cross-Disciplinary Communication Strategies", "comment": "Accepted to the EMNLP 2025 Industry Track; 26 pages", "summary": "Current approaches to developing persuasive dialogue agents often rely on a limited set of predefined persuasive strategies that fail to capture the complexity of real-world interactions. We applied a cross-disciplinary approach to develop a framework for designing persuasive dialogue agents that draws on proven strategies from social psychology, behavioral economics, and communication theory. We validated our proposed framework through experiments on two distinct datasets: the Persuasion for Good dataset, which represents a specific in-domain scenario, and the DailyPersuasion dataset, which encompasses a wide range of scenarios. The proposed framework achieved strong results for both datasets and demonstrated notable improvement in the persuasion success rate as well as promising generalizability. Notably, the proposed framework also excelled at persuading individuals with initially low intent, which addresses a critical challenge for persuasive dialogue agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8de8\u5b66\u79d1\u7406\u8bba\u7684\u529d\u8bf4\u5bf9\u8bdd\u7cfb\u7edf\u8bbe\u8ba1\u6846\u67b6\uff0c\u5e76\u5728\u4e24\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\uff0c\u5c24\u5176\u5728\u529d\u670d\u521d\u59cb\u610f\u613f\u4f4e\u7684\u4eba\u7fa4\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u73b0\u6709\u529d\u8bf4\u5bf9\u8bdd\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u6709\u9650\u7684\u9884\u5b9a\u4e49\u7b56\u7565\uff0c\u96be\u4ee5\u5e94\u5bf9\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u590d\u6742\u529d\u8bf4\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u793e\u4f1a\u5fc3\u7406\u5b66\u3001\u884c\u4e3a\u7ecf\u6d4e\u5b66\u548c\u4f20\u64ad\u7406\u8bba\uff0c\u5f00\u53d1\u4e86\u65b0\u7684\u529d\u8bf4\u5bf9\u8bdd\u7cfb\u7edf\u8bbe\u8ba1\u6846\u67b6\uff0c\u5e76\u5728Persuasion for Good\u548cDailyPersuasion\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc1\u660e\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u65b0\u6846\u67b6\u5728\u63d0\u5347\u529d\u8bf4\u6210\u529f\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u83b7\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e14\u5bf9\u4e8e\u521d\u59cb\u610f\u613f\u4f4e\u7684\u7528\u6237\u529d\u8bf4\u6548\u679c\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u8de8\u5b66\u79d1\u7684\u529d\u8bf4\u5bf9\u8bdd\u7cfb\u7edf\u8bbe\u8ba1\u6846\u67b6\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u548c\u666e\u9002\u6027\uff0c\u7279\u522b\u80fd\u591f\u6709\u6548\u529d\u8bf4\u4e00\u5f00\u59cb\u610f\u613f\u8f83\u4f4e\u7684\u7528\u6237\uff0c\u89e3\u51b3\u4e86\u529d\u8bf4\u578b\u5bf9\u8bdd\u7cfb\u7edf\u9886\u57df\u4e2d\u7684\u91cd\u8981\u96be\u9898\u3002"}}
{"id": "2602.22801", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22801", "abs": "https://arxiv.org/abs/2602.22801", "authors": ["Yinan Zheng", "Tianyi Tan", "Bin Huang", "Enguang Liu", "Ruiming Liang", "Jianlin Zhang", "Jianwei Cui", "Guang Chen", "Kun Ma", "Hangjun Ye", "Long Chen", "Ya-Qin Zhang", "Xianyuan Zhan", "Jingjing Liu"], "title": "Unleashing the Potential of Diffusion Models for End-to-End Autonomous Driving", "comment": null, "summary": "Diffusion models have become a popular choice for decision-making tasks in robotics, and more recently, are also being considered for solving autonomous driving tasks. However, their applications and evaluations in autonomous driving remain limited to simulation-based or laboratory settings. The full strength of diffusion models for large-scale, complex real-world settings, such as End-to-End Autonomous Driving (E2E AD), remains underexplored. In this study, we conducted a systematic and large-scale investigation to unleash the potential of the diffusion models as planners for E2E AD, based on a tremendous amount of real-vehicle data and road testing. Through comprehensive and carefully controlled studies, we identify key insights into the diffusion loss space, trajectory representation, and data scaling that significantly impact E2E planning performance. Moreover, we also provide an effective reinforcement learning post-training strategy to further enhance the safety of the learned planner. The resulting diffusion-based learning framework, Hyper Diffusion Planner} (HDP), is deployed on a real-vehicle platform and evaluated across 6 urban driving scenarios and 200 km of real-world testing, achieving a notable 10x performance improvement over the base model. Our work demonstrates that diffusion models, when properly designed and trained, can serve as effective and scalable E2E AD planners for complex, real-world autonomous driving tasks.", "AI": {"tldr": "\u672c\u6587\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u771f\u5b9e\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u5927\u91cf\u5b9e\u8f66\u6570\u636e\u548c\u9053\u8def\u6d4b\u8bd5\uff0c\u63d0\u51fa\u4e86\u6027\u80fd\u663e\u8457\u63d0\u5347\u7684Hyper Diffusion Planner (HDP)\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u57ce\u5e02\u9a7e\u9a76\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5df2\u5728\u673a\u5668\u4eba\u51b3\u7b56\u4efb\u52a1\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5e94\u7528\u591a\u5c40\u9650\u4e8e\u4eff\u771f\u6216\u5b9e\u9a8c\u5ba4\u73af\u5883\uff0c\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u590d\u6742\u7684\u771f\u5b9e\u73af\u5883\u9a8c\u8bc1\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u6316\u6398\u6269\u6563\u6a21\u578b\u5728\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u5927\u91cf\u5b9e\u8f66\u6570\u636e\u548c\u9053\u8def\u6d4b\u8bd5\uff0c\u5bf9\u6269\u6563\u6a21\u578b\u7528\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u3001\u5927\u89c4\u6a21\u7684\u7814\u7a76\u3002\u91cd\u70b9\u5206\u6790\u4e86\u6269\u6563\u635f\u5931\u7a7a\u95f4\u3001\u8f68\u8ff9\u8868\u793a\u548c\u6570\u636e\u89c4\u6a21\u5bf9\u89c4\u5212\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u5347\u7cfb\u7edf\u5b89\u5168\u6027\uff0c\u6700\u7ec8\u5f62\u6210Hyper Diffusion Planner (HDP)\u6846\u67b6\u3002", "result": "HDP\u5728\u771f\u5b9e\u8f66\u8f86\u5e73\u53f0\u4e0a\u90e8\u7f72\uff0c\u5e76\u57286\u4e2a\u57ce\u5e02\u9a7e\u9a76\u573a\u666f\u53ca200\u516c\u91cc\u7684\u5b9e\u9645\u6d4b\u8bd5\u4e2d\uff0c\u8f83\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u4e8610\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u7ecf\u8fc7\u5408\u7406\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u53ef\u4f5c\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\uff0c\u6709\u80fd\u529b\u89e3\u51b3\u590d\u6742\u3001\u771f\u5b9e\u73af\u5883\u4e0b\u7684\u81ea\u52a8\u9a7e\u9a76\u95ee\u9898\u3002"}}
{"id": "2602.22568", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22568", "abs": "https://arxiv.org/abs/2602.22568", "authors": ["Peihan Wu", "Guanjie Cheng", "Yufei Tong", "Meng Xi", "Shuiguang Deng"], "title": "Quality-Aware Robust Multi-View Clustering for Heterogeneous Observation Noise", "comment": null, "summary": "Deep multi-view clustering has achieved remarkable progress but remains vulnerable to complex noise in real-world applications. Existing noisy robust methods predominantly rely on a simplified binary assumption, treating data as either perfectly clean or completely corrupted. This overlooks the prevalent existence of heterogeneous observation noise, where contamination intensity varies continuously across data. To bridge this gap, we propose a novel framework termed Quality-Aware Robust Multi-View Clustering (QARMVC). Specifically, QARMVC employs an information bottleneck mechanism to extract intrinsic semantics for view reconstruction. Leveraging the insight that noise disrupts semantic integrity and impedes reconstruction, we utilize the resulting reconstruction discrepancy to precisely quantify fine-grained contamination intensity and derive instance-level quality scores. These scores are integrated into a hierarchical learning strategy: at the feature level, a quality-weighted contrastive objective is designed to adaptively suppress the propagation of noise; at the fusion level, a high-quality global consensus is constructed via quality-weighted aggregation, which is subsequently utilized to align and rectify local views via mutual information maximization. Extensive experiments on five benchmark datasets demonstrate that QARMVC consistently outperforms state-of-the-art baselines, particularly in scenarios with heterogeneous noise intensities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u89c6\u56fe\u805a\u7c7b\u6846\u67b6QARMVC\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8bc6\u522b\u5e76\u62b5\u6297\u5f02\u8d28\u566a\u58f0\uff0c\u5b9e\u73b0\u66f4\u5065\u58ee\u7684\u591a\u89c6\u56fe\u805a\u7c7b\uff0c\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u89c6\u56fe\u805a\u7c7b\u6297\u566a\u65b9\u6cd5\u5927\u591a\u91c7\u7528\u7b80\u5355\u7684\u4e8c\u5143\u5047\u8bbe\uff0c\u53ea\u5c06\u6570\u636e\u5212\u5206\u4e3a\u5e72\u51c0\u6216\u5b8c\u5168\u635f\u574f\uff0c\u5ffd\u89c6\u4e86\u5b9e\u9645\u4e2d\u666e\u904d\u5b58\u5728\u7684\u5f02\u8d28\u566a\u58f0\uff0c\u5176\u6c61\u67d3\u7a0b\u5ea6\u662f\u8fde\u7eed\u53d8\u5316\u7684\u3002\u56e0\u6b64\u9700\u8981\u80fd\u8bc6\u522b\u5e76\u6709\u9488\u5bf9\u6027\u5904\u7406\u4e0d\u540c\u5f3a\u5ea6\u566a\u58f0\u7684\u65b9\u6cd5\u3002", "method": "QARMVC\u5229\u7528\u4fe1\u606f\u74f6\u9888\u673a\u5236\u63d0\u53d6\u89c6\u56fe\u5185\u5728\u8bed\u4e49\u5e76\u7528\u4e8e\u91cd\u6784\u3002\u89c2\u6d4b\u5230\u566a\u58f0\u7834\u574f\u8bed\u4e49\u5b8c\u6574\u6027\uff0c\u4f1a\u5f71\u54cd\u91cd\u6784\u6548\u679c\uff0c\u4f5c\u8005\u4ee5\u6b64\u4e3a\u4f9d\u636e\uff0c\u5b9a\u91cf\u8bc4\u4f30\u6bcf\u4e2a\u6837\u672c\u7684\u6c61\u67d3\u5f3a\u5ea6\uff0c\u5206\u914d\u5b9e\u4f8b\u7ea7\u8d28\u91cf\u5206\u6570\u3002\u5728\u7279\u5f81\u5c42\u91c7\u7528\u8d28\u91cf\u52a0\u6743\u5bf9\u6bd4\u635f\u5931\u6291\u5236\u566a\u58f0\u4f20\u64ad\uff0c\u5728\u878d\u5408\u5c42\u7528\u9ad8\u8d28\u91cf\u6837\u672c\u805a\u5408\u5171\u8bc6\uff0c\u5e76\u901a\u8fc7\u6700\u5927\u5316\u4e92\u4fe1\u606f\u5bf9\u9f50\u548c\u4fee\u6b63\u5404\u672c\u5730\u89c6\u56fe\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u591a\u89c6\u56fe\u805a\u7c7b\u6570\u636e\u96c6\u4e0a\uff0cQARMVC\u5728\u5404\u79cd\u566a\u58f0\u5f3a\u5ea6\u73af\u5883\u4e0b\u5747\u4f18\u4e8e\u4e3b\u6d41\u6297\u566a\u805a\u7c7b\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u7a33\u5065\u6027\u548c\u805a\u7c7b\u6027\u80fd\u3002", "conclusion": "\u9488\u5bf9\u591a\u89c6\u56fe\u6570\u636e\u4e2d\u5f02\u8d28\u566a\u58f0\u7684\u5b9e\u9645\u6311\u6218\uff0cQARMVC\u901a\u8fc7\u8d28\u91cf\u611f\u77e5\u5206\u6570\u548c\u5c42\u6b21\u5316\u5904\u7406\u6d41\u7a0b\u5b9e\u73b0\u4e86\u66f4\u4e3a\u7cbe\u7ec6\u548c\u81ea\u9002\u5e94\u7684\u7a33\u5065\u805a\u7c7b\uff0c\u4e3a\u771f\u5b9e\u590d\u6742\u566a\u58f0\u73af\u5883\u4e0b\u7684\u591a\u89c6\u56fe\u805a\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.22697", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22697", "abs": "https://arxiv.org/abs/2602.22697", "authors": ["Ning Gao", "Wei Zhang", "Yuqin Dai", "Ling Shi", "Ziyin Wang", "Yujie Wang", "Wei He", "Jinpeng Wang", "Chaozheng Wang"], "title": "Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue", "comment": "35 pages, 8 tables, 3 figures", "summary": "The rapid evolution of Large Language Models (LLMs) has accelerated the transition from conversational chatbots to general agents. However, effectively balancing empathetic communication with budget-aware decision-making remains an open challenge. Since existing methods fail to capture these complex strategic trade-offs, we propose InteractCS-RL, a framework that reframes task-oriented dialogue as a multi-granularity reinforcement learning process. Specifically, we first establish a User-centric Interaction Framework to provide a high-fidelity training gym, enabling agents to dynamically explore diverse strategies with persona-driven users. Then, we introduce Cost-aware Multi-turn Policy Optimization (CMPO) with a hybrid advantage estimation strategy. By integrating generative process credits and employing a PID-Lagrangian cost controller, CMPO effectively guides the policy to explore Pareto boundary between user reward and global cost constraints. Extensive experiments on customized real business scenarios demonstrate that InteractCS-RL significantly outperform other baselines across three evaluation dimensions. Further evaluation on tool-agent-user interaction benchmarks verify InteractCS-RL robustness across diverse domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u540c\u7406\u5fc3\u6c9f\u901a\u4e0e\u6210\u672c\u610f\u8bc6\u7684\u4efb\u52a1\u578b\u5bf9\u8bddRL\u6846\u67b6InteractCS-RL\uff0c\u4f18\u5316\u5bf9\u8bdd\u667a\u80fd\u4f53\u5728\u5b9e\u9645\u573a\u666f\u4e0b\u7684\u591a\u7ef4\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u6210\u4e3a\u9ad8\u667a\u80fd\u5bf9\u8bdd\u4f53\u5b9e\u73b0\u7684\u57fa\u7840\uff0c\u4f46\u5728\u5b9e\u9645\u4efb\u52a1\u578b\u5bf9\u8bdd\u4e2d\uff0c\u5982\u4f55\u5e73\u8861\u7528\u6237\u4f53\u9a8c\uff08\u8868\u73b0\u540c\u7406\u5fc3\uff09\u548c\u4fdd\u6301\u6210\u672c\u9ad8\u6548\u662f\u4e00\u5927\u96be\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u4f53\u73b0\u8fd9\u79cd\u5e73\u8861\u7684\u7b56\u7565\u6027\u6743\u8861\u3002", "method": "\u63d0\u51faUser-centric Interaction Framework\u4f5c\u4e3a\u8bad\u7ec3\u73af\u5883\uff0c\u5141\u8bb8\u667a\u80fd\u4f53\u4e0e\u9ad8\u5ea6\u8fd8\u539f\u771f\u4eba\u7684\u865a\u62df\u7528\u6237\u4e92\u52a8\uff0c\u63a2\u7d22\u591a\u6837\u5316\u7b56\u7565\u3002\u5f15\u5165\u6210\u672c\u611f\u77e5\u591a\u8f6e\u7b56\u7565\u4f18\u5316\uff08CMPO\uff09\uff0c\u7ed3\u5408\u751f\u6210\u8fc7\u7a0b\u5956\u52b1\u548cPID-Lagrangian\u6210\u672c\u63a7\u5236\u5668\uff0c\u63a8\u52a8\u7b56\u7565\u5728\u7528\u6237\u6536\u76ca\u4e0e\u6210\u672c\u4e4b\u95f4\u52a8\u6001\u5e73\u8861\u3002", "result": "\u5728\u5b9e\u9645\u4f01\u4e1a\u573a\u666f\u548c\u8bc4\u4ef7\u57fa\u51c6\u4e0a\uff0cInteractCS-RL\u5728\u4e09\u9879\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6848\uff0c\u5e76\u5728\u591a\u9886\u57df\u5de5\u5177-\u667a\u80fd\u4f53-\u7528\u6237\u4ea4\u4e92\u4e2d\u8868\u73b0\u51fa\u8f83\u5f3a\u7a33\u5065\u6027\u3002", "conclusion": "InteractCS-RL\u80fd\u66f4\u597d\u5730\u5728\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u4f18\u5316\u540c\u7406\u5fc3\u548c\u6210\u672c\u6743\u8861\uff0c\u62d3\u5bbd\u4e86\u4efb\u52a1\u578b\u5bf9\u8bdd\u667a\u80fd\u4f53\u7684\u6027\u80fd\u4e0a\u9650\u3002"}}
{"id": "2602.22818", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22818", "abs": "https://arxiv.org/abs/2602.22818", "authors": ["Remi Cadene", "Simon Aliberts", "Francesco Capuano", "Michel Aractingi", "Adil Zouitine", "Pepijn Kooijmans", "Jade Choghari", "Martino Russi", "Caroline Pascal", "Steven Palma", "Mustafa Shukor", "Jess Moss", "Alexander Soare", "Dana Aubakirova", "Quentin Lhoest", "Quentin Gallou\u00e9dec", "Thomas Wolf"], "title": "LeRobot: An Open-Source Library for End-to-End Robot Learning", "comment": "https://github.com/huggingface/lerobot", "summary": "Robotics is undergoing a significant transformation powered by advances in high-level control techniques based on machine learning, giving rise to the field of robot learning. Recent progress in robot learning has been accelerated by the increasing availability of affordable teleoperation systems, large-scale openly available datasets, and scalable learning-based methods. However, development in the field of robot learning is often slowed by fragmented, closed-source tools designed to only address specific sub-components within the robotics stack. In this paper, we present \\texttt{lerobot}, an open-source library that integrates across the entire robot learning stack, from low-level middleware communication for motor controls to large-scale dataset collection, storage and streaming. The library is designed with a strong focus on real-world robotics, supporting accessible hardware platforms while remaining extensible to new embodiments. It also supports efficient implementations for various state-of-the-art robot learning algorithms from multiple prominent paradigms, as well as a generalized asynchronous inference stack. Unlike traditional pipelines which heavily rely on hand-crafted techniques, \\texttt{lerobot} emphasizes scalable learning approaches that improve directly with more data and compute. Designed for accessibility, scalability, and openness, \\texttt{lerobot} lowers the barrier to entry for researchers and practitioners to robotics while providing a platform for reproducible, state-of-the-art robot learning.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5f00\u6e90\u5e93lerobot\uff0c\u8be5\u5e93\u6574\u5408\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u5168\u6808\u529f\u80fd\uff0c\u4ece\u5e95\u5c42\u786c\u4ef6\u901a\u8baf\u5230\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\uff0c\u652f\u6301\u591a\u79cd\u524d\u6cbf\u5b66\u4e60\u65b9\u6cd5\uff0c\u964d\u4f4e\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u7684\u95e8\u69db\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u9886\u57df\u867d\u7136\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u53d7\u9650\u4e8e\u4e13\u7528\u4e14\u5c01\u95ed\u7684\u5de5\u5177\uff0c\u5bfc\u81f4\u8fdb\u5c55\u7f13\u6162\u3002\u4e9f\u9700\u4e00\u5957\u5f00\u653e\u3001\u96c6\u6210\u5316\u7684\u5de5\u5177\u6765\u4fc3\u8fdb\u6574\u4e2a\u9886\u57df\u7684\u534f\u4f5c\u548c\u521b\u65b0\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86lerobot\u5f00\u6e90\u5e93\uff0c\u6db5\u76d6\u673a\u5668\u4eba\u5b66\u4e60\u4ece\u5e95\u5c42\u9a71\u52a8\u901a\u8baf\u3001\u4e2d\u95f4\u4ef6\u5230\u5927\u89c4\u6a21\u6570\u636e\u91c7\u96c6\u3001\u5b58\u50a8\u548c\u6d41\u5904\u7406\uff0c\u5b9e\u73b0\u5bf9\u4e3b\u6d41\u5b66\u4e60\u7b97\u6cd5\u7684\u9ad8\u6548\u96c6\u6210\uff0c\u5e76\u652f\u6301\u591a\u79cd\u786c\u4ef6\u548c\u7b97\u6cd5\u8303\u5f0f\u3002", "result": "lerobot\u80fd\u591f\u652f\u6301\u591a\u7c7b\u5b9e\u9645\u673a\u5668\u4eba\u786c\u4ef6\uff0c\u96c6\u6210\u5e76\u4f18\u5316\u4e86\u5404\u7c7b\u5148\u8fdb\u673a\u5668\u4eba\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u548c\u53ef\u91cd\u73b0\u7684\u673a\u5668\u4eba\u5b66\u4e60\u5e73\u53f0\u3002", "conclusion": "lerobot\u901a\u8fc7\u5f00\u653e\u3001\u53ef\u6269\u5c55\u3001\u6613\u7528\u7684\u8bbe\u8ba1\uff0c\u4fc3\u8fdb\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u9886\u57df\u7684\u5f00\u653e\u5408\u4f5c\u548c\u65b0\u65b9\u6cd5\u7684\u5feb\u901f\u8fed\u4ee3\uff0c\u4e3a\u5b66\u672f\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u62d3\u5c55\u4e86\u66f4\u5e7f\u9614\u7684\u7a7a\u95f4\u3002"}}
{"id": "2602.22570", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22570", "abs": "https://arxiv.org/abs/2602.22570", "authors": ["Dian Xie", "Shitong Shao", "Lichen Bai", "Zikai Zhou", "Bojun Cheng", "Shuo Yang", "Jun Wu", "Zeke Xie"], "title": "Guidance Matters: Rethinking the Evaluation Pitfall for Text-to-Image Generation", "comment": null, "summary": "Classifier-free guidance (CFG) has helped diffusion models achieve great conditional generation in various fields. Recently, more diffusion guidance methods have emerged with improved generation quality and human preference. However, can these emerging diffusion guidance methods really achieve solid and significant improvements? In this paper, we rethink recent progress on diffusion guidance. Our work mainly consists of four contributions. First, we reveal a critical evaluation pitfall that common human preference models exhibit a strong bias towards large guidance scales. Simply increasing the CFG scale can easily improve quantitative evaluation scores due to strong semantic alignment, even if image quality is severely damaged (e.g., oversaturation and artifacts). Second, we introduce a novel guidance-aware evaluation (GA-Eval) framework that employs effective guidance scale calibration to enable fair comparison between current guidance methods and CFG by identifying the effects orthogonal and parallel to CFG effects. Third, motivated by the evaluation pitfall, we design Transcendent Diffusion Guidance (TDG) method that can significantly improve human preference scores in the conventional evaluation framework but actually does not work in practice. Fourth, in extensive experiments, we empirically evaluate recent eight diffusion guidance methods within the conventional evaluation framework and the proposed GA-Eval framework. Notably, simply increasing the CFG scales can compete with most studied diffusion guidance methods, while all methods suffer severely from winning rate degradation over standard CFG. Our work would strongly motivate the community to rethink the evaluation paradigm and future directions of this field.", "AI": {"tldr": "\u672c\u8bba\u6587\u9488\u5bf9\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6307\u5bfc\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u63ed\u793a\u4e86\u76ee\u524d\u4e3b\u6d41\u4eba\u7c7b\u504f\u597d\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e25\u91cd\u504f\u89c1\u95ee\u9898\uff0c\u547c\u5401\u793e\u533a\u91cd\u65b0\u601d\u8003\u6269\u6563\u6307\u5bfc\u65b9\u6cd5\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u5206\u7c7b\u5668\u81ea\u7531\u6307\u5bfc\uff08CFG\uff09\u548c\u65b0\u5174\u7684\u6269\u6563\u6307\u5bfc\u65b9\u6cd5\u5728\u63d0\u5347\u6587\u672c-\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u65b9\u9762\u8d77\u5230\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5b83\u4eec\u7684\u8bc4\u4ef7\u65b9\u5f0f\u53ef\u80fd\u5b58\u5728\u7f3a\u9677\uff0c\u5bfc\u81f4\u5bf9\u65b9\u6cd5\u6709\u6548\u6027\u7684\u8ba4\u77e5\u4ea7\u751f\u504f\u5dee\u3002", "method": "1. \u63ed\u793a\u8bc4\u4ef7\u9677\u9631\uff1a\u6307\u51fa\u73b0\u6709\u4eba\u7c7b\u504f\u597d\u6a21\u578b\u5bf9\u9ad8CFG scale\u5b58\u5728\u8f83\u5927\u504f\u89c1\uff0c\u7b80\u5355\u63d0\u9ad8CFG scale\u867d\u7136\u63d0\u5347\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\u5206\u6570\uff0c\u5374\u4e25\u91cd\u7834\u574f\u56fe\u50cf\u8d28\u91cf\u30022. \u63d0\u51faGA-Eval\u8bc4\u4f30\u6846\u67b6\uff1a\u901a\u8fc7\u6709\u6548\u7684scale\u6821\u51c6\uff0c\u516c\u5e73\u5730\u6bd4\u8f83\u5404\u79cd\u6307\u5bfc\u65b9\u6cd5\uff0c\u5e76\u8bc6\u522b\u51fa\u6b63\u4ea4\u548c\u5171\u7ebf\u4e8eCFG\u6548\u679c\u7684\u6307\u5bfc\u6548\u5e94\u30023. \u8bc1\u660e\u9677\u9631\u5b58\u5728\uff1a\u8bbe\u8ba1\u4e86TDG\u65b9\u6cd5\uff0c\u5728\u4f20\u7edf\u8bc4\u4ef7\u4e0b\u8868\u73b0\u4f18\u79c0\uff0c\u5b9e\u5219\u65e0\u6548\u30024. \u7cfb\u7edf\u8bc4\u6d4b8\u79cd\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5728\u4e24\u79cd\u6846\u67b6\u4e0b\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4ec5\u901a\u8fc7\u7b80\u5355\u589e\u52a0CFG scale\u5373\u53ef\u5ab2\u7f8e\u751a\u81f3\u8d85\u8fc7\u76ee\u524d\u5927\u591a\u6570\u6269\u6563\u6307\u5bfc\u65b9\u6cd5\uff0c\u540c\u65f6\u6240\u6709\u65b9\u6cd5\u5728GA-Eval\u6846\u67b6\u4e0b\u5747\u51fa\u73b0\u660e\u663e\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u5f53\u524d\u8bc4\u4f30\u8303\u5f0f\u5b58\u5728\u91cd\u5927\u504f\u5dee\uff0c\u547c\u5401\u793e\u533a\u91cd\u65b0\u5236\u5b9a\u66f4\u516c\u6b63\u5408\u7406\u7684\u6269\u6563\u6a21\u578b\u6307\u5bfc\u65b9\u6cd5\u8bc4\u4ef7\u6807\u51c6\u3002"}}
{"id": "2602.22698", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22698", "abs": "https://arxiv.org/abs/2602.22698", "authors": ["Siyue Su", "Jian Yang", "Bo Li", "Guanglin Niu"], "title": "Tokenization, Fusion and Decoupling: Bridging the Granularity Mismatch Between Large Language Models and Knowledge Graphs", "comment": null, "summary": "Leveraging Large Language Models (LLMs) for Knowledge Graph Completion (KGC) is promising but hindered by a fundamental granularity mismatch. LLMs operate on fragmented token sequences, whereas entities are the fundamental units in knowledge graphs (KGs) scenarios. Existing approaches typically constrain predictions to limited candidate sets or align entities with the LLM's vocabulary by pooling multiple tokens or decomposing entities into fixed-length token sequences, which fail to capture both the semantic meaning of the text and the structural integrity of the graph. To address this, we propose KGT, a novel framework that uses dedicated entity tokens to enable efficient, full-space prediction. Specifically, we first introduce specialized tokenization to construct feature representations at the level of dedicated entity tokens. We then fuse pre-trained structural and textual features into these unified embeddings via a relation-guided gating mechanism, avoiding training from scratch. Finally, we implement decoupled prediction by leveraging independent heads to separate and combine semantic and structural reasoning. Experimental results show that KGT consistently outperforms state-of-the-art methods across multiple benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7528\u4e8e\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\uff08KGC\uff09\u7684\u65b0\u6846\u67b6KGT\uff0c\u6709\u6548\u7f13\u89e3\u4e86LLM\u4e0e\u77e5\u8bc6\u56fe\u8c31\u5b9e\u4f53\u7c92\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5229\u7528LLM\u8fdb\u884c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u65b9\u6cd5\uff0c\u4e3b\u8981\u53d7\u9650\u4e8eLLM\u5904\u7406\u6587\u672c\u7684\u7c92\u5ea6\uff08\u4ee5token\u4e3a\u5355\u5143\uff09\u4e0e\u77e5\u8bc6\u56fe\u8c31\u4ee5\u5b9e\u4f53\u4e3a\u57fa\u672c\u5355\u5143\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u4f20\u7edf\u5904\u7406\u65b9\u5f0f\u8981\u4e48\u4ec5\u9650\u4e8e\u6709\u9650\u5b9e\u4f53\u5019\u9009\u96c6\uff0c\u8981\u4e48\u901a\u8fc7\u62fc\u63a5\u6216\u5206\u89e3token\u6765\u4ee3\u8868\u5b9e\u4f53\uff0c\u96be\u4ee5\u517c\u987e\u6587\u672c\u8bed\u4e49\u4e0e\u56fe\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "method": "KGT\u6846\u67b6\u91c7\u7528\u4e13\u5c5e\u7684\u5b9e\u4f53token\u8fdb\u884c\u7279\u6b8a\u5206\u8bcd\uff0c\u5b9e\u73b0\u5b9e\u4f53\u7c92\u5ea6\u7684\u7279\u5f81\u8868\u8fbe\uff0c\u5e76\u878d\u5408\u96c6\u6210\u9884\u8bad\u7ec3\u7684\u7ed3\u6784\u7279\u5f81\u548c\u6587\u672c\u7279\u5f81\uff0c\u901a\u8fc7\u5173\u7cfb\u5f15\u5bfc\u7684\u95e8\u63a7\u673a\u5236\u7edf\u4e00\u7f16\u7801\uff0c\u540c\u65f6\u91c7\u7528\u89e3\u8026\u9884\u6d4b\u7ed3\u6784\uff0c\u5c06\u8bed\u4e49\u63a8\u7406\u4e0e\u7ed3\u6784\u63a8\u7406\u5206\u522b\u5b9e\u73b0\u518d\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u9884\u6d4b\u6548\u7387\u548c\u8986\u76d6\u8303\u56f4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eKGT\u5728\u591a\u4e2a\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u57fa\u51c6\u4e0a\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u4e0e\u77e5\u8bc6\u56fe\u8c31\u4e4b\u95f4\u7c92\u5ea6\u4e0d\u5339\u914d\u96be\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u4f53\u7ea7\u522b\u7684\u5168\u7a7a\u95f4\u9ad8\u6548\u9884\u6d4b\uff0c\u5bf9KGC\u9886\u57df\u5177\u6709\u5b9e\u9645\u5e94\u7528\u548c\u7406\u8bba\u521b\u65b0\u4ef7\u503c\u3002"}}
{"id": "2602.22854", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22854", "abs": "https://arxiv.org/abs/2602.22854", "authors": ["Annika Delucchi", "Vincenzo Di Paola", "Andreas M\u00fcller", "and Matteo Zoppi"], "title": "Performance and Experimental Analysis of Strain-based Models for Continuum Robots", "comment": null, "summary": "Although strain-based models have been widely adopted in robotics, no comparison beyond the uniform bending test is commonly recognized to assess their performance. In addition, the increasing effort in prototyping continuum robots highlights the need to assess the applicability of these models and the necessity of comprehensive performance evaluation. To address this gap, this work investigates the shape reconstruction abilities of a third-order strain interpolation method, examining its ability to capture both individual and combined deformation effects. These results are compared and discussed against the Geometric-Variable Strain approach. Subsequently, simulation results are experimentally verified by reshaping a slender rod while recording the resulting configurations using cameras. The rod configuration is imposed using a manipulator displacing one of its tips and extracted through reflective markers, without the aid of any other external sensor -- i.e. strain gauges or wrench sensors placed along the rod. The experiments demonstrate good agreement between the model predictions and observed shapes, with average error of 0.58% of the rod length and average computational time of 0.32s per configuration, outperforming existing models.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u7b2c\u4e09\u9636\u5e94\u53d8\u63d2\u503c\u6cd5\u4e0e\u51e0\u4f55\u53d8\u5e94\u53d8\u65b9\u6cd5\u5728\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u5f62\u72b6\u91cd\u5efa\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5728\u5b9e\u9a8c\u8bc1\u660e\u524d\u8005\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u5e94\u53d8\u6a21\u578b\u5728\u673a\u5668\u4eba\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u7684\u6027\u80fd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u9645\u673a\u5668\u4eba\u539f\u578b\u5f00\u53d1\u4e2d\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u6025\u9700\u8bc4\u4f30\u5176\u9002\u7528\u6027\u548c\u7efc\u5408\u6027\u80fd\u3002", "method": "\u672c\u6587\u91cd\u70b9\u7814\u7a76\u7b2c\u4e09\u9636\u5e94\u53d8\u63d2\u503c\u65b9\u6cd5\u7684\u5f62\u72b6\u91cd\u5efa\u80fd\u529b\uff0c\u65e2\u8003\u5bdf\u5176\u5bf9\u5355\u4e00\u53d8\u5f62\u548c\u7ec4\u5408\u53d8\u5f62\u6548\u5e94\u7684\u63cf\u8ff0\u80fd\u529b\uff0c\u53c8\u4e0e\u51e0\u4f55\u53d8\u5e94\u53d8\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u673a\u68b0\u81c2\u63a7\u5236\u5355\u7aef\u70b9\u8fd0\u52a8\u5e76\u5229\u7528\u6444\u50cf\u7cfb\u7edf\u53ca\u53cd\u5149\u6807\u8bb0\u83b7\u53d6\u7ec6\u957f\u6746\u7684\u5f62\u53d8\u6570\u636e\uff0c\u8fdb\u4e00\u6b65\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4e0d\u501f\u52a9\u6746\u4e0a\u5e94\u53d8\u7247\u6216\u529b\u4f20\u611f\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7b2c\u4e09\u9636\u5e94\u53d8\u63d2\u503c\u6a21\u578b\u5728\u5f62\u72b6\u9884\u6d4b\u4e0a\u4e0e\u5b9e\u9645\u89c2\u6d4b\u9ad8\u5ea6\u543b\u5408\uff0c\u5e73\u5747\u8bef\u5dee\u5360\u6746\u957f0.58%\uff0c\u5e73\u5747\u6bcf\u7ec4\u914d\u7f6e\u8ba1\u7b97\u4ec5\u97000.32\u79d2\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u6a21\u578b\u3002", "conclusion": "\u8be5\u7b2c\u4e09\u9636\u5e94\u53d8\u63d2\u503c\u6a21\u578b\u4e0d\u4ec5\u51c6\u786e\u6027\u9ad8\u3001\u8ba1\u7b97\u5feb\uff0c\u8fd8\u7b80\u5316\u4e86\u5b9e\u9a8c\u8bbe\u5907\u8981\u6c42\uff0c\u9002\u5408\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u5b9e\u9645\u5e94\u7528\uff0c\u4e3a\u540e\u7eed\u6027\u80fd\u8bc4\u4f30\u548c\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2602.22594", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22594", "abs": "https://arxiv.org/abs/2602.22594", "authors": ["Qing Yu", "Akihisa Watanabe", "Kent Fujiwara"], "title": "Causal Motion Diffusion Models for Autoregressive Motion Generation", "comment": "Accepted to CVPR 2026, Project website: https://yu1ut.com/CMDM-HP/", "summary": "Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u56e0\u679c\u8fd0\u52a8\u6269\u6563\u6a21\u578b\uff08CMDM\uff09\uff0c\u7ed3\u5408\u4e86\u8fd0\u52a8-\u8bed\u8a00\u5bf9\u9f50\u7684\u56e0\u679cVAE\u4e0e\u81ea\u56de\u5f52\u6269\u6563Transformer\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u8fd0\u52a8\u6269\u6563\u6a21\u578b\u8981\u4e48\u4f9d\u8d56\u5168\u5e8f\u5217\u7684\u53cc\u5411\u751f\u6210\uff0c\u9650\u5236\u4e86\u65f6\u5e8f\u56e0\u679c\u6027\u548c\u5b9e\u65f6\u6027\uff0c\u8981\u4e48\u7528\u81ea\u56de\u5f52\u7ed3\u6784\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u548c\u8bef\u5dee\u7d2f\u79ef\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e24\u4e2a\u4e3b\u8981\u7f3a\u70b9\uff0c\u9700\u8981\u63d0\u51fa\u4e00\u79cd\u540c\u65f6\u5177\u5907\u56e0\u679c\u6027\u3001\u7a33\u5b9a\u6027\u4e14\u9002\u7528\u4e8e\u6d41\u5f0f\u3001\u5b9e\u65f6\u751f\u6210\u7684\u6a21\u578b\u3002", "method": "\u65b9\u6cd5\u4e0a\uff0cCMDM\u5305\u62ec\u4e24\u4e2a\u6838\u5fc3\u90e8\u5206\uff1a1\uff09Motion-Language-Aligned Causal VAE\uff08MAC-VAE\uff09\uff0c\u5c06\u52a8\u4f5c\u7f16\u7801\u4e3a\u65f6\u5e8f\u56e0\u679c\u6f5c\u5728\u8868\u793a\uff1b2\uff09\u5728\u8be5\u6f5c\u5728\u7a7a\u95f4\u4e0b\uff0c\u7528\u81ea\u56de\u5f52\u6269\u6563Transformer\u914d\u5408\u56e0\u679c\u53bb\u566a\u8bad\u7ec3\uff0c\u5b9e\u73b0\u9010\u5e27\u6709\u5e8f\u751f\u6210\u3002\u4e3a\u52a0\u901f\u63a8\u7406\uff0c\u5f15\u5165\u9010\u5e27\u91c7\u6837\u8c03\u5ea6\u673a\u5236\uff0c\u4ec5\u4f9d\u8d56\u90e8\u5206\u53bb\u566a\u7684\u524d\u5e27\u9884\u6d4b\u540e\u7eed\u5e27\u3002", "result": "\u5728HumanML3D\u53caSnapMoGen\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cCMDM\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u65f6\u5e8f\u5e73\u6ed1\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6269\u6563\u548c\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "CMDM\u80fd\u591f\u652f\u6301\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684\u6587\u672c\u9a71\u52a8\u52a8\u4f5c\u751f\u6210\u3001\u6d41\u5f0f\u5408\u6210\u53ca\u957f\u5e8f\u5217\u52a8\u4f5c\u751f\u6210\uff0c\u5e76\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.22896", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22896", "abs": "https://arxiv.org/abs/2602.22896", "authors": ["Zebin Yang", "Yijiahao Qi", "Tong Xie", "Bo Yu", "Shaoshan Liu", "Meng Li"], "title": "DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation", "comment": "DAC 2026", "summary": "Vision-Language-Action (VLA) models have shown remarkable success in robotic tasks like manipulation by fusing a language model's reasoning with a vision model's 3D understanding. However, their high computational cost remains a major obstacle for real-world applications that require real-time performance. We observe that the actions within a task have varying levels of importance: critical steps demand high precision, while less important ones can tolerate more variance. Leveraging this insight, we propose DySL-VLA, a novel framework that addresses computational cost by dynamically skipping VLA layers based on each action's importance. DySL-VLA categorizes its layers into two types: informative layers, which are consistently executed, and incremental layers, which can be selectively skipped. To intelligently skip layers without sacrificing accuracy, we invent a prior-post skipping guidance mechanism to determine when to initiate layer-skipping. We also propose a skip-aware two-stage knowledge distillation algorithm to efficiently train a standard VLA into a DySL-VLA. Our experiments indicate that DySL-VLA achieves 2.1% improvement in success length over Deer-VLA on the Calvin dataset, while simultaneously reducing trainable parameters by a factor of 85.7 and providing a 3.75x speedup relative to the RoboFlamingo baseline at iso-accuracy. Our code is available on https://github.com/PKU-SEC-Lab/DYSL_VLA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DySL-VLA\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8df3\u8fc7\u90e8\u5206VLA\u6a21\u578b\u5c42\uff0c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u5b9e\u65f6\u6027\u3002", "motivation": "\u867d\u7136VLA\u6a21\u578b\u80fd\u591f\u7ed3\u5408\u8bed\u8a00\u63a8\u7406\u4e0e\u89c6\u89c9\u7406\u89e3\uff0c\u6781\u5927\u63d0\u5347\u673a\u5668\u4eba\u5904\u7406\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u4f46\u9ad8\u6602\u7684\u8ba1\u7b97\u5f00\u9500\u5236\u7ea6\u4e86\u5176\u5b9e\u65f6\u5e94\u7528\u3002\u4f5c\u8005\u6d1e\u5bdf\u5230\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u4e0d\u540c\u52a8\u4f5c\u7684\u91cd\u8981\u7a0b\u5ea6\u4e0d\u540c\uff0c\u90e8\u5206\u5173\u952e\u52a8\u4f5c\u9700\u8981\u9ad8\u7cbe\u5ea6\uff0c\u800c\u4e00\u822c\u52a8\u4f5c\u5bb9\u5fcd\u5ea6\u66f4\u9ad8\uff0c\u7531\u6b64\u6fc0\u53d1\u5bf9\u8ba1\u7b97\u8d44\u6e90\u52a8\u6001\u5206\u914d\u7684\u9700\u6c42\u3002", "method": "DySL-VLA\u5c06\u6a21\u578b\u5c42\u5206\u4e3a\u59cb\u7ec8\u6267\u884c\u7684\u4fe1\u606f\u5c42\u548c\u53ef\u9009\u62e9\u8df3\u8fc7\u7684\u589e\u91cf\u5c42\u3002\u901a\u8fc7\u8bbe\u8ba1\u4e00\u79cd\u5148\u9a8c-\u4e8b\u540e\u8df3\u5c42\u5f15\u5bfc\u673a\u5236\uff0c\u667a\u80fd\u51b3\u7b56\u4f55\u65f6\u8df3\u8fc7\u589e\u91cf\u5c42\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u5e26\u8df3\u5c42\u611f\u77e5\u7684\u53cc\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u5c06\u6807\u51c6VLA\u9ad8\u6548\u84b8\u998f\u4e3aDySL-VLA\uff0c\u4ee5\u517c\u987e\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "result": "\u5728Calvin\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u4e2d\uff0cDySL-VLA\u6bd4Deer-VLA\u5728\u4efb\u52a1\u6210\u529f\u957f\u5ea6\u4e0a\u63d0\u5347\u4e862.1%\uff0c\u8bad\u7ec3\u53c2\u6570\u91cf\u51cf\u5c1185.7\u500d\u3002\u4e0eRoboFlamingo\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5728\u76f8\u540c\u7cbe\u5ea6\u4e0b\u52a0\u901f3.75\u500d\u3002", "conclusion": "DySL-VLA\u663e\u8457\u964d\u4f4e\u4e86VLA\u6a21\u578b\u5b9e\u9645\u90e8\u7f72\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3aVLA\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.22624", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22624", "abs": "https://arxiv.org/abs/2602.22624", "authors": ["Liya Ji", "Chenyang Qi", "Qifeng Chen"], "title": "Instruction-based Image Editing with Planning, Reasoning, and Generation", "comment": "10 pages, 7 figures", "summary": "Editing images via instruction provides a natural way to generate interactive content, but it is a big challenge due to the higher requirement of scene understanding and generation. Prior work utilizes a chain of large language models, object segmentation models, and editing models for this task. However, the understanding models provide only a single modality ability, restricting the editing quality. We aim to bridge understanding and generation via a new multi-modality model that provides the intelligent abilities to instruction-based image editing models for more complex cases. To achieve this goal, we individually separate the instruction editing task with the multi-modality chain of thought prompts, i.e., Chain-of-Thought (CoT) planning, editing region reasoning, and editing. For Chain-of-Thought planning, the large language model could reason the appropriate sub-prompts considering the instruction provided and the ability of the editing network. For editing region reasoning, we train an instruction-based editing region generation network with a multi-modal large language model. Finally, a hint-guided instruction-based editing network is proposed for editing image generations based on the sizeable text-to-image diffusion model to accept the hints for generation. Extensive experiments demonstrate that our method has competitive editing abilities on complex real-world images.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u6a21\u578b\u7684\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u94fe\u5f0f\u63a8\u7406\u63d0\u5347\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u590d\u6742\u573a\u666f\u4e0b\u7684\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u4f9d\u8d56\u4e8e\u5355\u4e00\u6a21\u6001\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\u3001\u5206\u5272\u6a21\u578b\u548c\u7f16\u8f91\u6a21\u578b\uff09\uff0c\u5bfc\u81f4\u5bf9\u573a\u666f\u7406\u89e3\u4e0e\u7f16\u8f91\u8d28\u91cf\u6709\u9650\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u4efb\u52a1\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u6784\u5efa\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u66f4\u597d\u5730\u7ed3\u5408\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\uff0c\u63d0\u5347\u7f16\u8f91\u8d28\u91cf\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5c06\u6307\u4ee4\u7f16\u8f91\u4efb\u52a1\u5206\u79bb\u4e3a\u591a\u6a21\u6001\u94fe\u5f0f\u601d\u7ef4\uff08Chain-of-Thought, CoT\uff09\u6b65\u9aa4\uff0c\u5305\u62ec\u8ba1\u5212\u3001\u7f16\u8f91\u533a\u57df\u63a8\u7406\u3001\u5b9e\u9645\u7f16\u8f91\u30021\uff09CoT\u8ba1\u5212\uff1a\u5927\u8bed\u8a00\u6a21\u578b\u6839\u636e\u6307\u4ee4\u548c\u7f16\u8f91\u7f51\u7edc\u80fd\u529b\u751f\u6210\u5408\u7406\u5b50\u63d0\u793a\uff1b2\uff09\u533a\u57df\u63a8\u7406\uff1a\u8bad\u7ec3\u4e86\u57fa\u4e8e\u6307\u4ee4\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7f16\u8f91\u533a\u57df\uff1b3\uff09\u7f16\u8f91\uff1a\u63d0\u51fa\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u6307\u4ee4\u7f16\u8f91\u7f51\u7edc\uff0c\u5229\u7528\u5927\u578b\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u7ed3\u679c\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u771f\u5b9e\u56fe\u50cf\u7684\u7f16\u8f91\u4efb\u52a1\u4e0a\u5177\u6709\u8f83\u5f3a\u7684\u7f16\u8f91\u80fd\u529b\uff0c\u7f16\u8f91\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u94fe\u5f0f\u63a8\u7406\u80fd\u591f\u6709\u6548\u63d0\u5347\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u7684\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.22639", "categories": ["cs.CV", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.22639", "abs": "https://arxiv.org/abs/2602.22639", "authors": ["Daniel Miao", "Gilad Lerman", "Joe Kileel"], "title": "QuadSync: Quadrifocal Tensor Synchronization via Tucker Decomposition", "comment": "30 pages, accepted to CVPR 2026", "summary": "In structure from motion, quadrifocal tensors capture more information than their pairwise counterparts (essential matrices), yet they have often been thought of as impractical and only of theoretical interest. In this work, we challenge such beliefs by providing a new framework to recover $n$ cameras from the corresponding collection of quadrifocal tensors. We form the block quadrifocal tensor and show that it admits a Tucker decomposition whose factor matrices are the stacked camera matrices, and which thus has a multilinear rank of (4,~4,~4,~4) independent of $n$. We develop the first synchronization algorithm for quadrifocal tensors, using Tucker decomposition, alternating direction method of multipliers, and iteratively reweighted least squares. We further establish relationships between the block quadrifocal, trifocal, and bifocal tensors, and introduce an algorithm that jointly synchronizes these three entities. Numerical experiments demonstrate the effectiveness of our methods on modern datasets, indicating the potential and importance of using higher-order information in synchronization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u57fa\u4e8e\u56db\u89c6\u70b9\u5f20\u91cf\u7684\u76f8\u673a\u540c\u6b65\u6846\u67b6\uff0c\u901a\u8fc7Tucker\u5206\u89e3\u5b9e\u73b0$n$\u4e2a\u76f8\u673a\u7684\u540c\u6b65\uff0c\u5b9e\u9a8c\u8868\u660e\u6548\u679c\u4f18\u79c0\uff0c\u63a8\u52a8\u4e86\u9ad8\u9636\u5f20\u91cf\u5728\u7ed3\u6784\u5149\u6d41\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u7684\u7ed3\u6784\u5149\u6d41\u4e2d\u591a\u91c7\u7528\u57fa\u672c\u77e9\u9635\uff08\u53cc\u89c6\u70b9\uff09\uff0c\u800c\u56db\u89c6\u70b9\u5f20\u91cf\u56e0\u590d\u6742\u6027\u5e38\u88ab\u89c6\u4e3a\u7406\u8bba\u610f\u4e49\u5927\u4e8e\u5b9e\u9645\uff0c\u4f46\u5176\u7406\u8bba\u4e0a\u5305\u542b\u66f4\u591a\u4fe1\u606f\uff0c\u6709\u671b\u63d0\u5347\u591a\u76f8\u673a\u51e0\u4f55\u91cd\u5efa\u7684\u80fd\u529b\u3002\u672c\u6587\u81f4\u529b\u4e8e\u5c06\u56db\u89c6\u70b9\u5f20\u91cf\u7684\u7406\u8bba\u4f18\u52bf\u8f6c\u5316\u4e3a\u5b9e\u9645\u7684\u540c\u6b65\u4e0e\u51e0\u4f55\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5c06$n$\u4e2a\u56db\u89c6\u70b9\u5f20\u91cf\u6574\u7406\u4e3ablock\u5f20\u91cf\uff0c\u5e76\u8bc1\u660e\u5176Tucker\u5206\u89e3\u7684\u56e0\u5b50\u6b63\u597d\u662f\u76f8\u673a\u9635\u5217\uff0c\u4ece\u800c\u6784\u5efa\u4e86\u65b0\u7684\u540c\u6b65\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u878d\u5408Tucker\u5206\u89e3\u3001\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u53ca\u8fed\u4ee3\u91cd\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u6cd5\u3002\u540c\u65f6\uff0c\u5efa\u7acb\u4e86\u56db\u89c6\u70b9\u3001\u4e09\u89c6\u70b9\u4e0e\u53cc\u89c6\u70b9\u5f20\u91cf\u4e4b\u95f4\u7684\u540c\u6b65\u8054\u7cfb\uff0c\u5b9e\u73b0\u4e86\u4e09\u79cd\u5f20\u91cf\u7684\u8054\u5408\u540c\u6b65\u7b97\u6cd5\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u73b0\u4ee3\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u4f18\u5f02\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u540c\u6b65\u548c\u6062\u590d\u591a\u76f8\u673a\u7684\u5916\u53c2\u4fe1\u606f\uff0c\u8d85\u8fc7\u4e86\u4f4e\u9636\uff08\u53cc\u3001\u4e09\u89c6\u70b9\uff09\u65b9\u6cd5\u3002", "conclusion": "\u9ad8\u9636\uff08\u56db\u89c6\u70b9\uff09\u5f20\u91cf\u4e0d\u4ec5\u5177\u7406\u8bba\u610f\u4e49\uff0c\u501f\u52a9\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u5728\u5b9e\u9645\u7ed3\u6784\u5149\u6d41\u4efb\u52a1\u4e2d\u6709\u6548\u53d1\u6325\u4f5c\u7528\uff0c\u63d0\u5347\u591a\u76f8\u673a\u7cfb\u7edf\u91cd\u5efa\u7684\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\uff0c\u4e3a\u540e\u7eed\u76f8\u5173\u5e94\u7528\u548c\u7406\u8bba\u62d3\u5c55\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2602.22654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22654", "abs": "https://arxiv.org/abs/2602.22654", "authors": ["Bowen Cui", "Yuanbin Wang", "Huajiang Xu", "Biaolong Chen", "Aixi Zhang", "Hao Jiang", "Zhengzheng Jin", "Xu Liu", "Pipei Huang"], "title": "Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache", "comment": "Accepted by CVPR 2026", "summary": "Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.", "AI": {"tldr": "DPCache\u662f\u4e00\u79cd\u9ad8\u6548\u52a0\u901f\u6269\u6563\u6a21\u578b\u63a8\u7406\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u6700\u4f18\u8df3\u6b65\u8ba1\u5212\u663e\u8457\u52a0\u901f\u91c7\u6837\u8fc7\u7a0b\uff0c\u4e14\u4e0d\u4f1a\u9020\u6210\u8d28\u91cf\u660e\u663e\u4e0b\u964d\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u4e0e\u89c6\u9891\u751f\u6210\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u591a\u6b65\u63a8\u7406\u6781\u5927\u6d88\u8017\u7b97\u529b\u3002\u5df2\u6709\u7f13\u5b58\u52a0\u901f\u65b9\u6cd5\u65e0\u6cd5\u5168\u5c40\u6700\u4f18\u5730\u8df3\u6b65\uff0c\u5bfc\u81f4\u7d2f\u79ef\u8bef\u5dee\u548c\u8d28\u91cf\u52a3\u5316\u3002\u4e9f\u9700\u66f4\u667a\u80fd\u3001\u9ad8\u6548\u3001\u8bad\u7ec3\u65e0\u5173\u7684\u52a0\u901f\u673a\u5236\u3002", "method": "DPCache\u5c06\u91c7\u6837\u52a0\u901f\u95ee\u9898\u5efa\u6a21\u4e3a\u5168\u5c40\u8def\u5f84\u89c4\u5212\u3002\u5229\u7528\u5c0f\u578b\u6821\u51c6\u96c6\u6784\u5efaPath-Aware Cost Tensor\uff0c\u91cf\u5316\u8df3\u6b65\u8def\u5f84\u4e0a\u7684\u8bef\u5dee\uff0c\u5e76\u7528\u52a8\u6001\u89c4\u5212\u6c42\u89e3\u6700\u4f18\u5173\u952e\u65f6\u523b\u5e8f\u5217\u3002\u63a8\u7406\u65f6\uff0c\u4ec5\u5728\u5173\u952e\u6b65\u5b8c\u6574\u8ba1\u7b97\uff0c\u5176\u4f59\u6b65\u901a\u8fc7\u7f13\u5b58\u7279\u5f81\u9ad8\u6548\u9884\u6d4b\u3002", "result": "\u5728DiT\u3001FLUX\u548cHunyuanVideo\u7b49\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8bc1\u660e\u4e86DPCache\u7684\u9ad8\u52a0\u901f\u4e0e\u4f4e\u8d28\u91cf\u635f\u5931\u4f18\u52bf\u3002\u4f8b\u5982\u57284.87\u500d\u52a0\u901f\u4e0b\u6bd4\u524d\u4eba\u65b9\u6cd5\u9ad8\u51fa0.031 ImageReward\uff0c3.54\u500d\u52a0\u901f\u4e0b\u751a\u81f3\u8d85\u8fc7\u5168\u90e8\u91c7\u6837\u6b65\u9aa4\u7684\u57fa\u7ebf\u3002", "conclusion": "DPCache\u65e0\u9700\u8bad\u7ec3\uff0c\u5373\u53ef\u5b9e\u73b0\u6269\u6563\u91c7\u6837\u7684\u5168\u5c40\u6700\u4f18\u52a0\u901f\uff0c\u6781\u5927\u51cf\u5c11\u91c7\u6837\u6b65\u6570\u4e14\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u5b9e\u7528\u4e0e\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2602.23057", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23057", "abs": "https://arxiv.org/abs/2602.23057", "authors": ["Jeongin Bae", "Baeseong Park", "Gunho Park", "Minsub Kim", "Joonhyung Lee", "Junhee Yoo", "Sunghyeon Woo", "Jiwon Ryu", "Se Jung Kwon", "Dongsoo Lee"], "title": "Affine-Scaled Attention: Towards Flexible and Stable Transformer Attention", "comment": "Preprint. 14 pages, 11 figures", "summary": "Transformer attention is typically implemented using softmax normalization, which enforces attention weights with unit sum normalization. While effective in many settings, this constraint can limit flexibility in controlling attention magnitudes and may contribute to overly concentrated or unstable attention patterns during training. Prior work has explored modifications such as attention sinks or gating mechanisms, but these approaches provide only limited or indirect control over attention reweighting. We propose Affine-Scaled Attention, a simple extension to standard attention that introduces input-dependent scaling and a corresponding bias term applied to softmax-normalized attention weights. This design relaxes the strict normalization constraint while maintaining aggregation of value representations, allowing the model to adjust both the relative distribution and the scale of attention in a controlled manner.\n  We empirically evaluate Affine-Scaled Attention in large-scale language model pretraining across multiple model sizes. Experimental results show consistent improvements in training stability, optimization behavior, and downstream task performance compared to standard softmax attention and attention sink baselines. These findings suggest that modest reweighting of attention outputs provides a practical and effective way to improve attention behavior in Transformer models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u4eff\u5c04\u7f29\u653e\u6ce8\u610f\u529b\uff08Affine-Scaled Attention\uff09\u7684\u65b0\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5f15\u5165\u8f93\u5165\u76f8\u5173\u7684\u7f29\u653e\u548c\u504f\u7f6e\u9879\uff0c\u6539\u5584Transformer\u4e2dsoftmax\u5f52\u4e00\u5316\u6ce8\u610f\u529b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0e\u6548\u679c\u3002", "motivation": "\u4f20\u7edfTransformer\u4e2d\u7684softmax\u5f52\u4e00\u5316\u9650\u5236\u4e86\u6ce8\u610f\u529b\u5206\u5e03\u7684\u7075\u6d3b\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u6ce8\u610f\u529b\u8fc7\u4e8e\u96c6\u4e2d\u6216\u4e0d\u7a33\u5b9a\uff0c\u73b0\u6709\u7684\u6539\u8fdb\u65b9\u6cd5\uff08\u5982attention sink\u6216\u95e8\u63a7\u673a\u5236\uff09\u53ea\u80fd\u6709\u9650\u5ea6\u5730\u8c03\u6574\u6ce8\u610f\u529b\u6743\u91cd\u5206\u914d\u3002\u4f5c\u8005\u65e8\u5728\u7a81\u7834\u73b0\u6709\u673a\u5236\u7684\u5c40\u9650\uff0c\u63d0\u5347\u8bad\u7ec3\u6027\u80fd\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4eff\u5c04\u7f29\u653e\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728softmax\u5f52\u4e00\u5316\u6743\u91cd\u57fa\u7840\u4e0a\uff0c\u65bd\u52a0\u8f93\u5165\u76f8\u5173\u7684\u7f29\u653e\u56e0\u5b50\u548c\u504f\u7f6e\u9879\uff0c\u4ece\u800c\u5141\u8bb8\u6a21\u578b\u81ea\u4e3b\u8c03\u8282\u6ce8\u610f\u529b\u5206\u5e03\u7684\u9ad8\u5bbd\u53ca\u76f8\u5bf9\u6bd4\u4f8b\u3002\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4efb\u52a1\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4eff\u5c04\u7f29\u653e\u6ce8\u610f\u529b\u76f8\u6bd4\u6807\u51c6softmax\u6ce8\u610f\u529b\u548cattention sink\u7b49\u57fa\u7ebf\uff0c\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u4f18\u5316\u884c\u4e3a\u548c\u4e0b\u6e38\u4efb\u52a1\u6548\u679c\u65b9\u9762\u5747\u6709\u660e\u663e\u63d0\u5347\u3002", "conclusion": "\u9002\u5ea6\u91cd\u65b0\u52a0\u6743\u6ce8\u610f\u529b\u8f93\u51fa\u662f\u4e00\u79cd\u7b80\u5355\u5b9e\u7528\u4e14\u6709\u6548\u7684\u65b9\u5f0f\uff0c\u53ef\u663e\u8457\u4f18\u5316Transformer\u6a21\u578b\u7684\u6ce8\u610f\u529b\u8868\u73b0\u548c\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2602.22923", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22923", "abs": "https://arxiv.org/abs/2602.22923", "authors": ["Runwei Guan", "Shaofeng Liang", "Ningwei Ouyang", "Weichen Fei", "Shanliang Yao", "Wei Dai", "Chenhao Ge", "Penglei Sun", "Xiaohui Zhu", "Tao Huang", "Ryan Wen Liu", "Hui Xiong"], "title": "WaterVideoQA: ASV-Centric Perception and Rule-Compliant Reasoning via Multi-Modal Agents", "comment": "11 pages,8 figures", "summary": "While autonomous navigation has achieved remarkable success in passive perception (e.g., object detection and segmentation), it remains fundamentally constrained by a void in knowledge-driven, interactive environmental cognition. In the high-stakes domain of maritime navigation, the ability to bridge the gap between raw visual perception and complex cognitive reasoning is not merely an enhancement but a critical prerequisite for Autonomous Surface Vessels to execute safe and precise maneuvers. To this end, we present WaterVideoQA, the first large-scale, comprehensive Video Question Answering benchmark specifically engineered for all-waterway environments. This benchmark encompasses 3,029 video clips across six distinct waterway categories, integrating multifaceted variables such as volatile lighting and dynamic weather to rigorously stress-test ASV capabilities across a five-tier hierarchical cognitive framework. Furthermore, we introduce NaviMind, a pioneering multi-agent neuro-symbolic system designed for open-ended maritime reasoning. By synergizing Adaptive Semantic Routing, Situation-Aware Hierarchical Reasoning, and Autonomous Self-Reflective Verification, NaviMind transitions ASVs from superficial pattern matching to regulation-compliant, interpretable decision-making. Experimental results demonstrate that our framework significantly transcends existing baselines, establishing a new paradigm for intelligent, trustworthy interaction in dynamic maritime environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u81ea\u4e3b\u6c34\u9762\u822a\u884c\u5668\uff08ASV\uff09\u5728\u590d\u6742\u6c34\u57df\u73af\u5883\u4e2d\u8ba4\u77e5\u80fd\u529b\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86WaterVideoQA\u57fa\u51c6\u548cNaviMind\u667a\u80fd\u63a8\u7406\u7cfb\u7edf\uff0c\u4ee5\u63d0\u5347ASV\u7684\u667a\u80fd\u4ea4\u4e92\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u5bfc\u822a\u4e3b\u8981\u505c\u7559\u5728\u88ab\u52a8\u89c6\u89c9\u611f\u77e5\uff0c\u5bf9\u73af\u5883\u7684\u77e5\u8bc6\u9a71\u52a8\u548c\u4ea4\u4e92\u8ba4\u77e5\u5b58\u5728\u77ed\u677f\uff0c\u7279\u522b\u662f\u5728\u822a\u8fd0\u5b89\u5168\u8981\u6c42\u6781\u9ad8\u7684\u6c34\u57df\u73af\u5883\u4e0b\u3002\u8fd9\u4e00\u7f3a\u9677\u9650\u5236\u4e86ASV\u6267\u884c\u5b89\u5168\u7cbe\u51c6\u64cd\u4f5c\u7684\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86WaterVideoQA\uff0c\u8fd9\u662f\u9488\u5bf9\u6c34\u57df\u73af\u5883\u7684\u7b2c\u4e00\u4e2a\u5927\u89c4\u6a21\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\uff0c\u8986\u76d6\u516d\u7c7b\u6c34\u57df\uff0c\u51713029\u4e2a\u89c6\u9891\uff0c\u5e76\u7eb3\u5165\u591a\u53d8\u5149\u7167\u548c\u52a8\u6001\u5929\u6c14\u7b49\u590d\u6742\u56e0\u7d20\uff0c\u7528\u5206\u5c42\u8ba4\u77e5\u6846\u67b6\u5bf9ASV\u80fd\u529b\u8fdb\u884c\u8bc4\u6d4b\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86NaviMind\u591a\u667a\u80fd\u4f53\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u8bed\u4e49\u8def\u7531\u3001\u60c5\u5883\u611f\u77e5\u5206\u5c42\u63a8\u7406\u548c\u81ea\u4e3b\u81ea\u53cd\u9a8c\u8bc1\uff0c\u5b9e\u73b0ASV\u7531\u7b80\u5355\u6a21\u677f\u5339\u914d\u5411\u7b26\u5408\u6cd5\u89c4\u3001\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8f6c\u53d8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5728\u89c6\u9891\u95ee\u7b54\u548c\u667a\u80fd\u63a8\u7406\u65b9\u9762\u660e\u663e\u8d85\u8fc7\u73b0\u6709\u57fa\u7ebf\u6c34\u5e73\uff0c\u63d0\u5347\u4e86ASV\u5728\u52a8\u6001\u6d77\u4e8b\u73af\u5883\u4e2d\u7684\u4ea4\u4e92\u667a\u80fd\u3002", "conclusion": "WaterVideoQA\u548cNaviMind\u7684\u63d0\u51fa\uff0c\u4e3a\u81ea\u4e3b\u822a\u884c\u5668\u4e0e\u52a8\u6001\u6c34\u57df\u73af\u5883\u95f4\u7684\u667a\u80fd\u53ef\u4fe1\u4ea4\u4e92\u6811\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u6709\u52a9\u4e8eASV\u5b89\u5168\u89c4\u8303\u5730\u5f00\u5c55\u590d\u6742\u4efb\u52a1\u3002"}}
{"id": "2602.23136", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23136", "abs": "https://arxiv.org/abs/2602.23136", "authors": ["Jayadev Billa"], "title": "Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs", "comment": "22 pages, 11 tables, 2 figures. Code: https://github.com/jb1999/modality_collapse_paper", "summary": "Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\\times$ above chance in linear probes), yet removing 64--71% of modality-specific variance improves decoder loss. The decoder has no learned use for these directions; their presence is noise.\n  We formalize this as a mismatched decoder problem: a decoder trained on text can only extract information along text-aligned directions. Accessible information is bounded by the Generalized Mutual Information (GMI), with degradation scaling with distributional distance and decoder sensitivity. The bound is a property of the decoder's scoring rule, not of any particular architecture; it applies whether non-text inputs arrive through a learned projection, a discrete codebook, or no explicit adapter at all. We validate this across five models spanning speech and vision. A controlled experiment (two Prismatic VLMs differing only in encoder text-alignment) confirms the bottleneck is the decoder's scoring rule, not the encoder or projection. A LoRA intervention demonstrates the fix: training with an emotion objective improves emotion accessibility ($+$7.5%) without affecting other attributes, confirming that the training objective determines what becomes accessible.", "AI": {"tldr": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u591f\u5904\u7406\u8bed\u97f3\u548c\u56fe\u50cf\uff0c\u4f46\u5b83\u4eec\u65e0\u6cd5\u6709\u6548\u611f\u77e5\u8bf4\u8bdd\u4eba\u7684\u58f0\u97f3\u6216\u7269\u4f53\u7684\u7eb9\u7406\u3002\u672c\u6587\u53d1\u73b0\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u89e3\u7801\u5668\u7684\u7ed3\u6784\u4e0e\u8bad\u7ec3\u65b9\u5f0f\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4ec5\u6709\u8bad\u7ec3\u76ee\u6807\u76f8\u5173\u7684\u4fe1\u606f\u80fd\u88ab\u6709\u6548\u89e3\u7801\uff0c\u4e14\u53ef\u901a\u8fc7\u7279\u5b9a\u76ee\u6807\u8bad\u7ec3\u63d0\u5347\u76f8\u5173\u6a21\u6001\u5c5e\u6027\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001LLM\u867d\u80fd\u63a5\u6536\u8bed\u97f3\u548c\u56fe\u50cf\u6570\u636e\uff0c\u4f46\u5728\u8bf4\u8bdd\u4eba\u8eab\u4efd\u3001\u60c5\u611f\u6216\u89c6\u89c9\u7ec6\u8282\u7b49\u6a21\u6001\u7279\u6709\u4fe1\u606f\u7684\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u4f5c\u8005\u8ba4\u4e3a\u95ee\u9898\u5e76\u975e\u5728\u4e8e\u7f16\u7801\u5668\u4fe1\u606f\u4e22\u5931\uff0c\u800c\u53ef\u80fd\u6e90\u81ea\u89e3\u7801\u5668\u5bf9\u4e0d\u540c\u6a21\u6001\u4fe1\u606f\u7684\u5229\u7528\u4e0d\u5145\u5206\uff0c\u56e0\u6b64\u5e0c\u671b\u627e\u5230\u6027\u80fd\u74f6\u9888\u5e76\u63d0\u51fa\u6539\u8fdb\u3002", "method": "\uff081\uff09\u901a\u8fc7\u7ebf\u6027\u63a2\u6d4b\u65b9\u6cd5\u8bc4\u4f30\u591a\u6a21\u6001LLM\u4e0d\u540c\u5c42\u5bf9\u6a21\u6001\u7279\u6709\u4fe1\u606f\uff08\u5982\u8bf4\u8bdd\u4eba\u8eab\u4efd\u3001\u60c5\u611f\u7b49\uff09\u7684\u4fdd\u7559\u60c5\u51b5\uff1b\uff082\uff09\u5b9e\u9a8c\u6027\u5730\u5220\u9664\u6a21\u6001\u7279\u6709\u65b9\u5dee\u4fe1\u606f\uff0c\u68c0\u6d4b\u5176\u5bf9\u89e3\u7801\u5668\u635f\u5931\u7684\u5f71\u54cd\uff1b\uff083\uff09\u63d0\u51fa\u5e76\u9a8c\u8bc1\u201c\u89e3\u7801\u5668\u4e0d\u5339\u914d\u201d\u7406\u8bba\uff08\u5373\u89e3\u7801\u5668\u53ea\u80fd\u5229\u7528\u4e0e\u6587\u672c\u7a7a\u95f4\u5bf9\u9f50\u7684\u4fe1\u606f\uff09\uff1b\uff084\uff09\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u5b9e\u9a8c\u53caLoRA\u5e72\u9884\uff0c\u6d4b\u8bd5\u4e0d\u540c\u8bad\u7ec3\u76ee\u6807\u5bf9\u6a21\u6001\u5c5e\u6027\u53ef\u8bbf\u95ee\u6027\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6a21\u6001\u7279\u6709\u4fe1\u606f\u5728\u7f16\u7801\u5668\u5404\u5c42\u5747\u6709\u5b58\u7559\uff0c\u4f46\u89e3\u7801\u5668\u53ea\u80fd\u7528\u4e0e\u6587\u672c\u4e00\u81f4\u7684\u65b9\u5411\u4fe1\u606f\uff0c\u5176\u4ed6\u4fe1\u606f\u88ab\u89c6\u4e3a\u566a\u58f0\uff0c\u5220\u53bb\u540e\u53cd\u800c\u63d0\u5347\u6027\u80fd\u3002\u7406\u8bba\u5206\u6790GMI\u754c\u5b9a\u4e86\u89e3\u7801\u5668\u53ef\u8bbf\u95ee\u4fe1\u606f\u7684\u4e0a\u9650\uff0c\u5e76\u901a\u8fc7\u4e94\u79cd\u6d89\u53ca\u8bed\u97f3\u548c\u89c6\u89c9\u7684\u6a21\u578b\u4e0e\u4e13\u95e8\u8bbe\u8ba1\u7684\u5bf9\u7167\u5b9e\u9a8c\u52a0\u4ee5\u5b9e\u8bc1\u3002\u91c7\u7528LoRA\u5e76\u4ee5\u60c5\u611f\u4e3a\u76ee\u6807\u8bad\u7ec3\u540e\uff0c\u60c5\u611f\u5c5e\u6027\u53ef\u8bbf\u95ee\u6027\u63d0\u53477.5%\uff0c\u4e14\u5bf9\u5176\u5b83\u5c5e\u6027\u65e0\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u591a\u6a21\u6001LLM\u7684\u4fe1\u606f\u74f6\u9888\u4e0d\u5728\u7f16\u7801\u5668\uff0c\u800c\u5728\u4e8e\u89e3\u7801\u5668\u4ec5\u89e3\u8bfb\u8bad\u7ec3\u76ee\u6807\u76f8\u5173\u7684\u5185\u5bb9\u3002\u8bad\u7ec3\u65f6\u5f15\u5165\u989d\u5916\u76ee\u6807\u80fd\u63d0\u5347\u5bf9\u5e94\u6a21\u6001\u4fe1\u606f\u7684\u53ef\u83b7\u53d6\u6027\u3002\u56e0\u6b64\uff0c\u6b32\u63d0\u5347\u591a\u6a21\u6001LLM\u5bf9\u7279\u6709\u6a21\u6001\u4fe1\u606f\u7684\u7406\u89e3\uff0c\u5e94\u8c03\u6574\u8bad\u7ec3\u76ee\u6807\u800c\u975e\u4ec5\u4f18\u5316\u67b6\u6784\u3002"}}
{"id": "2602.23197", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.23197", "abs": "https://arxiv.org/abs/2602.23197", "authors": ["Chungpa Lee", "Jy-yong Sohn", "Kangwook Lee"], "title": "Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models", "comment": null, "summary": "Transformer-based large language models exhibit in-context learning, enabling adaptation to downstream tasks via few-shot prompting with demonstrations. In practice, such models are often fine-tuned to improve zero-shot performance on downstream tasks, allowing them to solve tasks without examples and thereby reducing inference costs. However, fine-tuning can degrade in-context learning, limiting the performance of fine-tuned models on tasks not seen during fine-tuning. Using linear attention models, we provide a theoretical analysis that characterizes how fine-tuning objectives modify attention parameters and identifies conditions under which this leads to degraded few-shot performance. We show that fine-tuning all attention parameters can harm in-context learning, whereas restricting updates to the value matrix improves zero-shot performance while preserving in-context learning. We further show that incorporating an auxiliary few-shot loss enhances in-context learning primarily on the target task, at the expense of degraded in-context learning ability on tasks not seen during fine-tuning. We empirically validate our theoretical results.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5fae\u8c03\u5bf9\u5176in-context learning\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u7406\u8bba\u8bf4\u660e\u4e86\u5168\u53c2\u6570\u5fae\u8c03\u5e38\u635f\u5bb3\u6a21\u578b\u7684few-shot\u80fd\u529b\uff0c\u800c\u53ea\u5fae\u8c03Value\u77e9\u9635\u80fd\u517c\u987ezero-shot\u63d0\u5347\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u4fdd\u7559\uff1b\u540c\u65f6\u5f15\u5165auxiliary few-shot loss\u867d\u80fd\u63d0\u5347\u76ee\u6807\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5374\u4f1a\u635f\u5bb3\u5bf9\u672a\u89c1\u4efb\u52a1\u7684\u6cdb\u5316\u3002\u4e0a\u8ff0\u7ed3\u8bba\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u5b9e\u3002", "motivation": "\u76ee\u524dLLM\u5e38\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u7684prompt\u8fdb\u884cin-context learning\uff0c\u5177\u5907\u5feb\u901f\u9002\u914d\u4e0b\u6e38\u4efb\u52a1\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b9e\u8df5\u4e2dLLM\u5e38\u9700\u901a\u8fc7\u5fae\u8c03\u5f3a\u5316zero-shot\u80fd\u529b\uff0c\u4f46\u5fae\u8c03\u53ef\u80fd\u7834\u574ffew-shot\u7684\u6cdb\u5316\u6027\u3002\u4f5c\u8005\u65e8\u5728\u7cfb\u7edf\u5206\u6790\u5fae\u8c03\u5982\u4f55\u5f71\u54cd\u4e0a\u8ff0\u80fd\u529b\uff0c\u5e76\u63a2\u7d22\u6709\u65e0\u517c\u987e\u4e24\u8005\u7684\u7b56\u7565\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u4e3a\u7406\u8bba\u5206\u6790\u5de5\u5177\uff0c\u5206\u522b\u8003\u5bdf\u5168\u53c2\u6570\u5fae\u8c03\u4e0e\u53ea\u8c03\u6574Value\u77e9\u9635\u7684\u6548\u679c\uff0c\u5e76\u7814\u7a76\u52a0\u5165few-shot\u8f85\u52a9\u635f\u5931\u7684\u5f71\u54cd\u3002\u6700\u540e\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u7ed3\u8bba\u3002", "result": "\u5168\u91cf\u5fae\u8c03\u6ce8\u610f\u529b\u53c2\u6570\u4f1a\u524a\u5f31LLM\u7684few-shot in-context learning\u80fd\u529b\uff1b\u53ea\u5fae\u8c03Value\u77e9\u9635\u53ef\u4ee5\u517c\u987ezero-shot\u6548\u679c\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff1b\u52a0\u5165few-shot\u8f85\u52a9\u635f\u5931\uff0c\u867d\u80fd\u589e\u5f3a\u76ee\u6807\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6548\u679c\uff0c\u4f46\u4f1a\u635f\u5bb3\u5176\u5b83\u672a\u89c1\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u6570\u636e\u652f\u6301\u8fd9\u4e9b\u7406\u8bba\u53d1\u73b0\u3002", "conclusion": "\u5efa\u8bae\u5728\u8ffd\u6c42zero-shot\u548cfew-shot\u80fd\u529b\u517c\u987e\u65f6\u4ec5\u5fae\u8c03Value\u77e9\u9635\uff1b\u4f7f\u7528auxiliary few-shot loss\u9700\u6743\u8861\u6cdb\u5316\u6027\u548c\u9488\u5bf9\u6027\u63d0\u5347\u3002\u5fae\u8c03\u7b56\u7565\u9700\u6839\u636e\u5177\u4f53\u5e94\u7528\u9700\u6c42\u5408\u7406\u9009\u62e9\u3002"}}
{"id": "2602.22716", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22716", "abs": "https://arxiv.org/abs/2602.22716", "authors": ["Guanting Ye", "Qiyan Zhao", "Wenhao Yu", "Liangyu Yuan", "Mingkai Li", "Xiaofeng Zhang", "Jianmin Ji", "Yanyong Zhang", "Qing Jiang", "Ka-Veng Yuen"], "title": "SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs", "comment": "CVPR 2026", "summary": "3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. The vanilla RoPE formulation fails to preserve essential three-dimensional spatial structures when encoding 3D tokens, and its relative distance computation overlooks angular dependencies, hindering the model's ability to capture directional variations in visual representations. To overcome these limitations, we introduce Spherical Coordinate-based Positional Embedding (SoPE). Our method maps point-cloud token indices into a 3D spherical coordinate space, enabling unified modeling of spatial locations and directional angles. This formulation preserves the inherent geometric structure of point-cloud data, enhances spatial awareness, and yields more consistent and expressive geometric representations for multimodal learning. In addition, we introduce a multi-scale frequency mixing strategy to fuse feature information across different frequency domains. Experimental results on multiple 3D scene benchmarks validate the effectiveness of our approach, while real-world deployment experiments further demonstrate its strong generalization capability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u4e09\u7ef4\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5SoPE\uff0c\u63d0\u5347\u4e863D\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\u7684\u7a7a\u95f4\u611f\u77e5\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u67093D\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56RoPE\u4f4d\u7f6e\u7f16\u7801\uff0c\u4f46RoPE\u65e0\u6cd5\u6709\u6548\u6355\u6349\u4e09\u7ef4\u7a7a\u95f4\u7ed3\u6784\u548c\u65b9\u5411\u53d8\u5316\uff0c\u5bfc\u81f4\u5bf9\u7a7a\u95f4\u4fe1\u606f\u7406\u89e3\u53d7\u9650\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u63d0\u53473D\u7a7a\u95f4\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4ee5\u7403\u5750\u6807\u4e3a\u57fa\u7840\u7684\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff08SoPE\uff09\uff0c\u5c06\u70b9\u4e91token\u6620\u5c04\u5230\u7403\u5750\u6807\u7a7a\u95f4\uff0c\u7edf\u4e00\u5efa\u6a21\u7a7a\u95f4\u4f4d\u7f6e\u4e0e\u65b9\u5411\u89d2\u3002\u5e76\u5f15\u5165\u591a\u5c3a\u5ea6\u9891\u7387\u6df7\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u4e0d\u540c\u9891\u57df\u7279\u5f81\u4fe1\u606f\u7684\u878d\u5408\u8868\u8fbe\u3002", "result": "\u5728\u591a\u4e2a3D\u573a\u666f\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0c\u91c7\u7528SoPE\u7684\u65b9\u6cd5\u6548\u679c\u4f18\u4e8e\u4f20\u7edfRoPE\uff1b\u540c\u65f6\u5728\u771f\u5b9e\u90e8\u7f72\u5b9e\u9a8c\u4e2d\u4e5f\u5c55\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SoPE\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u4e09\u7ef4\u51e0\u4f55\u7ed3\u6784\uff0c\u589e\u5f3a3D LVLM\u6a21\u578b\u7684\u7a7a\u95f4\u611f\u77e5\u4e0e\u51e0\u4f55\u8868\u8fbe\uff0c\u662f\u63d0\u53473D\u591a\u6a21\u6001\u5efa\u6a21\u80fd\u529b\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.22717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22717", "abs": "https://arxiv.org/abs/2602.22717", "authors": ["Shuoqi Chen", "Yujia Wu", "Geoffrey P. Luke"], "title": "IRSDE-Despeckle: A Physics-Grounded Diffusion Model for Generalizable Ultrasound Despeckling", "comment": "12 pages main text + 6 pages appendix, 7 figures main + 3 figures appendix, 3 tables main + 1 table appendix. Preprint", "summary": "Ultrasound imaging is widely used for real-time, noninvasive diagnosis, but speckle and related artifacts reduce image quality and can hinder interpretation. We present a diffusion-based ultrasound despeckling method built on the Image Restoration Stochastic Differential Equations framework. To enable supervised training, we curate large paired datasets by simulating ultrasound images from speckle-free magnetic resonance images using the Matlab UltraSound Toolbox. The proposed model reconstructs speckle-suppressed images while preserving anatomically meaningful edges and contrast. On a held-out simulated test set, our approach consistently outperforms classical filters and recent learning-based despeckling baselines. We quantify prediction uncertainty via cross-model variance and show that higher uncertainty correlates with higher reconstruction error, providing a practical indicator of difficult or failure-prone regions. Finally, we evaluate sensitivity to simulation probe settings and observe domain shift, motivating diversified training and adaptation for robust clinical deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8d85\u58f0\u56fe\u50cf\u53bb\u6591\u65b0\u65b9\u6cd5\uff0c\u5728\u5408\u6210\u548c\u5b9e\u9645\u6570\u636e\u4e0a\u90fd\u4f18\u4e8e\u4f20\u7edf\u548c\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8d85\u58f0\u56fe\u50cf\u5e38\u56e0\u6591\u70b9\u566a\u58f0\u548c\u4f2a\u5f71\u5bfc\u81f4\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\uff0c\u5f71\u54cd\u533b\u751f\u8bca\u65ad\uff0c\u9700\u8981\u63d0\u9ad8\u8d85\u58f0\u56fe\u50cf\u7684\u53ef\u7528\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u57fa\u4e8eImage Restoration Stochastic Differential Equations\uff08\u6269\u6563\u6a21\u578b\uff09\u7684\u65b9\u6cd5\u8fdb\u884c\u8d85\u58f0\u56fe\u50cf\u53bb\u6591\u3002\u901a\u8fc7Matlab UltraSound Toolbox\uff0c\u5c06\u65e0\u6591\u70b9\u7684MRI\u5f71\u50cf\u5408\u6210\u5bf9\u5e94\u7684\u8d85\u58f0\u56fe\u50cf\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u6210\u5bf9\u8bad\u7ec3\u96c6\u8fdb\u884c\u6709\u76d1\u7763\u5b66\u4e60\u3002\u540c\u65f6\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u901a\u8fc7\u591a\u6a21\u578b\u9884\u6d4b\u65b9\u5dee\uff0c\u8bc4\u4f30\u6a21\u578b\u4fe1\u5fc3\u6c34\u5e73\u548c\u6f5c\u5728\u5931\u8d25\u533a\u57df\u3002\u6b64\u5916\uff0c\u5206\u6790\u91c7\u96c6\u53c2\u6570\u53d8\u5316\u5e26\u6765\u7684\u57df\u504f\u79fb\uff0c\u63a2\u7d22\u4e34\u5e8a\u5e94\u7528\u7684\u9002\u5e94\u6027\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u6a21\u62df\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u53bb\u6591\u6548\u679c\u4f18\u4e8e\u7ecf\u5178\u6ee4\u6ce2\u5668\u548c\u6700\u65b0\u7684\u5b66\u4e60\u578b\u57fa\u7ebf\u65b9\u6cd5\u3002\u80fd\u5728\u53bb\u9664\u6591\u70b9\u7684\u540c\u65f6\u66f4\u597d\u5730\u4fdd\u7559\u89e3\u5256\u7ed3\u6784\u8fb9\u7f18\u548c\u5bf9\u6bd4\u5ea6\u3002\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u4e0e\u91cd\u5efa\u8bef\u5dee\u9ad8\u5ea6\u76f8\u5173\uff0c\u53ef\u7528\u4f5c\u9519\u8bef\u533a\u57df\u7684\u6307\u793a\u3002\u53e6\u5916\u5728\u4e0d\u540c\u63a2\u5934\u8bbe\u7f6e\u4e0b\uff0c\u6a21\u578b\u8868\u73b0\u6709\u57df\u504f\u79fb\u95ee\u9898\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53bb\u6591\u65b9\u6cd5\u5728\u6a21\u62df\u6570\u636e\u4e0a\u663e\u8457\u63d0\u5347\u8d85\u58f0\u56fe\u50cf\u8d28\u91cf\uff0c\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u6709\u52a9\u4e8e\u8bc4\u4f30\u96be\u5904\u7406\u533a\u57df\u3002\u4e3a\u4e34\u5e8a\u5e94\u7528\u8fd8\u9700\u589e\u52a0\u6570\u636e\u591a\u6837\u6027\u548c\u9002\u5e94\u6027\u4ee5\u5e94\u5bf9\u5b9e\u9645\u57df\u504f\u79fb\u3002"}}
{"id": "2602.22727", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22727", "abs": "https://arxiv.org/abs/2602.22727", "authors": ["Yangguang Lin", "Quan Fang", "Yufei Li", "Jiachen Sun", "Junyu Gao", "Jitao Sang"], "title": "HulluEdit: Single-Pass Evidence-Consistent Subspace Editing for Mitigating Hallucinations in Large Vision-Language Models", "comment": "accepted at CVPR 2026", "summary": "Object hallucination in Large Vision-Language Models (LVLMs) significantly hinders their reliable deployment. Existing methods struggle to balance efficiency and accuracy: they often require expensive reference models and multiple forward passes, or apply static edits that risk suppressing genuine visual evidence. To address this, we introduce HulluEdit, a single-pass, reference-free intervention framework. Our core innovation is orthogonal subspace editing: we decompose the hidden states of the model into orthogonal subspaces - visual evidence, conflicting priors, and residual uncertainty - enabling selective suppression of hallucinatory patterns without interfering with visual grounding. This approach mathematically guarantees that edits applied to the prior subspace leave the visual component entirely unaffected. Extensive experiments show that HulluEdit achieves state-of-the-art hallucination reduction on benchmarks including POPE and CHAIR across diverse architectures, while preserving general capabilities on MME and maintaining efficient inference. Our method consistently outperforms contrastive decoding and static subspace editing baselines, offering a new pathway toward more trustworthy LVLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HulluEdit\u65b9\u6cd5\uff0c\u80fd\u9ad8\u6548\u7cbe\u51c6\u5730\u51cf\u5c11\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6548\u7387\u548c\u6a21\u578b\u80fd\u529b\u3002", "motivation": "LVLMs\u5b58\u5728\u663e\u8457\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u5176\u53ef\u4fe1\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\u2014\u2014\u6709\u4e9b\u65b9\u6cd5\u9700\u4f9d\u8d56\u6602\u8d35\u53c2\u8003\u6a21\u578b\u548c\u591a\u6b21\u524d\u5411\u8ba1\u7b97\uff0c\u6709\u4e9b\u65b9\u6cd5\u91c7\u7528\u9759\u6001\u7f16\u8f91\u5374\u6613\u8bef\u4f24\u771f\u5b9e\u89c6\u89c9\u4fe1\u606f\u3002", "method": "\u63d0\u51faHulluEdit\uff0c\u662f\u4e00\u79cd\u65e0\u987b\u53c2\u8003\u6a21\u578b\u3001\u5355\u6b21\u524d\u5411\u7684\u5e72\u9884\u6846\u67b6\u3002\u6838\u5fc3\u5728\u4e8e\u5bf9\u6a21\u578b\u9690\u72b6\u6001\u505a\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5206\u89e3\uff0c\u5c06\u5176\u5206\u4e3a\u89c6\u89c9\u8bc1\u636e\u3001\u5148\u9a8c\u51b2\u7a81\u548c\u6b8b\u4f59\u4e0d\u786e\u5b9a\u6027\u4e09\u4e2a\u5b50\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4ec5\u5bf9\u5e7b\u89c9\u503e\u5411\uff08\u5148\u9a8c\u51b2\u7a81\uff09\u5b50\u7a7a\u95f4\u8fdb\u884c\u6291\u5236\uff0c\u4e14\u7406\u8bba\u4e0a\u4fdd\u8bc1\u89c6\u89c9\u8bc1\u636e\u5b8c\u5168\u4e0d\u53d7\u5f71\u54cd\u3002", "result": "\u5728POPE\u3001CHAIR\u7b49\u4e0d\u540c\u67b6\u6784\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cHulluEdit\u80fd\u5927\u5e45\u51cf\u5c11\u5e7b\u89c9\u7387\uff0c\u540c\u65f6MME\u7b49\u4efb\u52a1\u4e0a\u7ef4\u6301\u6a21\u578b\u80fd\u529b\u4e0e\u9ad8\u6548\u63a8\u7406\uff0c\u663e\u8457\u4f18\u4e8e\u5bf9\u6bd4\u89e3\u7801\u548c\u9759\u6001\u5b50\u7a7a\u95f4\u7f16\u8f91\u7b49\u65b9\u6cd5\u3002", "conclusion": "HulluEdit\u4e3a\u63d0\u5347LVLMs\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u6709\u6548\u5e73\u8861\u964d\u4f4e\u5e7b\u89c9\u4e0e\u6a21\u578b\u901a\u7528\u80fd\u529b\uff0c\u5177\u5907\u9ad8\u6548\u3001\u9c81\u68d2\u7b49\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2602.22734", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22734", "abs": "https://arxiv.org/abs/2602.22734", "authors": ["Muzi Tao", "Chufan Shi", "Huijuan Wang", "Shengbang Tong", "Xuezhe Ma"], "title": "Asymmetric Idiosyncrasies in Multimodal Models", "comment": "Project page: https://muzi-tao.github.io/asymmetric-idiosyncrasies/", "summary": "In this work, we study idiosyncrasies in the caption models and their downstream impact on text-to-image models. We design a systematic analysis: given either a generated caption or the corresponding image, we train neural networks to predict the originating caption model. Our results show that text classification yields very high accuracy (99.70\\%), indicating that captioning models embed distinctive stylistic signatures. In contrast, these signatures largely disappear in the generated images, with classification accuracy dropping to at most 50\\% even for the state-of-the-art Flux model. To better understand this cross-modal discrepancy, we further analyze the data and find that the generated images fail to preserve key variations present in captions, such as differences in the level of detail, emphasis on color and texture, and the distribution of objects within a scene. Overall, our classification-based framework provides a novel methodology for quantifying both the stylistic idiosyncrasies of caption models and the prompt-following ability of text-to-image systems.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\u4e2d\u7684\u7279\u5b9a\u98ce\u683c\u7279\u5f81\uff0c\u5e76\u63a2\u7d22\u8fd9\u4e9b\u7279\u5f81\u5728\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\u4e2d\u7684\u5ef6\u7eed\u6027\u3002\u901a\u8fc7\u5206\u7c7b\u7f51\u7edc\u7814\u7a76\u53d1\u73b0\uff0c\u63cf\u8ff0\u6a21\u578b\u7684\u98ce\u683c\u7279\u5f81\u5f88\u5bb9\u6613\u4ece\u6587\u672c\u88ab\u8bc6\u522b\uff0c\u4f46\u5728\u751f\u6210\u56fe\u50cf\u4e2d\u5219\u5927\u591a\u6d88\u5931\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u7531\u63cf\u8ff0\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u63d0\u793a\uff0c\u7136\u800c\u4e0d\u540c\u7684\u63cf\u8ff0\u6a21\u578b\u5b58\u5728\u72ec\u7279\u98ce\u683c\u7279\u5f81\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5206\u6790\u8fd9\u4e9b\u98ce\u683c\u5728\u8de8\u6a21\u6001\u7cfb\u7edf\u4e2d\u7684\u4f20\u9012\u6027\u53ca\u5176\u5f71\u54cd\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u7c7b\u7684\u7cfb\u7edf\u5206\u6790\u65b9\u6cd5\uff1a\u5206\u522b\u4ee5\u751f\u6210\u7684\u63cf\u8ff0\u6587\u672c\u548c\u5bf9\u5e94\u751f\u6210\u56fe\u50cf\u4e3a\u8f93\u5165\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u5176\u6765\u6e90\u4e8e\u54ea\u4e2a\u63cf\u8ff0\u6a21\u578b\uff0c\u4ece\u800c\u91cf\u5316\u98ce\u683c\u4fe1\u606f\u5728\u6587\u672c\u548c\u56fe\u50cf\u4e2d\u7684\u5ef6\u7eed\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9\u4e8e\u6587\u672c\u8f93\u5165\uff0c\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u4ee5\u6781\u9ad8\u51c6\u786e\u7387\uff0899.7%\uff09\u8bc6\u522b\u5176\u6765\u6e90\u6a21\u578b\uff0c\u8bf4\u660e\u63cf\u8ff0\u6a21\u578b\u9057\u7559\u9c9c\u660e\u98ce\u683c\u7279\u5f81\u3002\u4f46\u5bf9\u4e8e\u56fe\u50cf\u8f93\u5165\u65f6\uff0c\u51c6\u786e\u7387\u5927\u5e45\u4e0b\u964d\uff08\u6700\u9ad850%\uff09\uff0c\u8868\u660e\u8fd9\u4e9b\u98ce\u683c\u5728\u56fe\u50cf\u751f\u6210\u73af\u8282\u88ab\u5f31\u5316\u751a\u81f3\u4e22\u5931\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u7c7b\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u63cf\u8ff0\u6a21\u578b\u7684\u98ce\u683c\u7279\u5f81\u53ca\u6587\u672c\u5230\u56fe\u50cf\u7cfb\u7edf\u5bf9\u63d0\u793a\u7684\u9075\u5faa\u80fd\u529b\uff0c\u53d1\u73b0\u8de8\u6a21\u6001\u8fc7\u7a0b\u4e2d\u98ce\u683c\u4fe1\u606f\u4f20\u9012\u6709\u9650\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u63d0\u5347\u53ef\u63a7\u6027\u548c\u591a\u6837\u6027\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2602.23351", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23351", "abs": "https://arxiv.org/abs/2602.23351", "authors": ["Amita Kamath", "Jack Hessel", "Khyathi Chandu", "Jena D. Hwang", "Kai-Wei Chang", "Ranjay Krishna"], "title": "Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning", "comment": "TACL 2026", "summary": "The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., \"at the game today!\" is a more likely caption than \"a photo of 37 people standing behind a field\". We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u4e3b\u8981\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u8868\u8fbe\u504f\u5dee\uff0c\u7f3a\u4e4f\u5bf9\u7a7a\u95f4\u3001\u65f6\u95f4\u3001\u5426\u5b9a\u548c\u8ba1\u6570\u7b49\u63a8\u7406\u80fd\u529b\u7684\u8bad\u7ec3\uff1b\u6587\u4e2d\u7528\u5b9e\u9a8c\u8bc1\u660e\u589e\u52a0\u89c4\u6a21\u5e76\u4e0d\u80fd\u5f25\u8865\u8be5\u77ed\u677f\uff0c\u9700\u6709\u9488\u5bf9\u5730\u5f15\u5165\u9690\u542b\u4fe1\u606f\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5176\u63a8\u7406\u80fd\u529b\u59cb\u7ec8\u6709\u9650\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u73b0\u8c61\u4e0e\u4eba\u4eec\u5728\u63cf\u8ff0\u89c6\u89c9\u5185\u5bb9\u65f6\u5e38\u5e38\u7701\u7565\u9690\u542b\uff08\u9ed8\u4f1a\uff09\u4fe1\u606f\u6709\u5173\uff0c\u5373\u8bad\u7ec3\u6570\u636e\u672c\u8eab\u5c31\u6ca1\u6709\u8981\u6c42\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u5206\u6790\u4e86OpenCLIP\u3001LLaVA-1.5\u548cMolmo\u7b49\u4e3b\u6d41VLM\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u7ed3\u5408\u8bed\u7528\u5b66\u7406\u8bba\uff0c\u5177\u4f53\u8003\u5bdf\u7a7a\u95f4\u3001\u65f6\u95f4\u3001\u5426\u5b9a\u548c\u8ba1\u6570\u56db\u7c7b\u63a8\u7406\u3002\u901a\u8fc7\u5efa\u7acb\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u4e0a\u8ff0\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\u3002", "result": "\uff081\uff09VLM\u5728\u4e0a\u8ff0\u56db\u7c7b\u63a8\u7406\u4e0a\u8868\u73b0\u8f83\u5dee\uff1b\uff082\uff09\u6269\u5927\u6570\u636e\u548c\u6a21\u578b\u89c4\u6a21\uff0c\u6216\u591a\u8bed\u8a00\u6269\u5c55\uff0c\u5e76\u4e0d\u80fd\u81ea\u7136\u63d0\u5347\u6b64\u7c7b\u63a8\u7406\u80fd\u529b\uff1b\uff083\uff09\u5982\u679c\u6709\u9488\u5bf9\u6027\u5730\u52a0\u5165\u8986\u76d6\u63a8\u7406\u6240\u9700\u9690\u542b\u4fe1\u606f\u7684\u6807\u6ce8\uff0c\u5219\u6548\u679c\u660e\u663e\u63d0\u5347\u3002", "conclusion": "VLM\u63a8\u7406\u77ed\u677f\u4e3b\u8981\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u8868\u8fbe\u504f\u5dee\uff0c\u63d0\u5347\u63a8\u7406\u80fd\u529b\u4e0d\u80fd\u4ec5\u9760\u201c\u89c4\u6a21\u201d\uff0c\u800c\u5e94\u7cbe\u5fc3\u7b56\u5212\u548c\u8865\u5145\u76f8\u5173\u63a8\u7406\u6570\u636e\u3002"}}
{"id": "2602.22779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22779", "abs": "https://arxiv.org/abs/2602.22779", "authors": ["Chenhao Zheng", "Jieyu Zhang", "Jianing Zhang", "Weikai Huang", "Ashutosh Kumar", "Quan Kong", "Oncel Tuzel", "Chun-Liang Li", "Ranjay Krishna"], "title": "TrajTok: Learning Trajectory Tokens enables better Video Understanding", "comment": "CVPR 2026", "summary": "Tokenization in video models, typically through patchification, generates an excessive and redundant number of tokens. This severely limits video efficiency and scalability. While recent trajectory-based tokenizers offer a promising solution by decoupling video duration from token count, they rely on complex external segmentation and tracking pipelines that are slow and task-agnostic. We propose TrajTok, an end-to-end video tokenizer module that is fully integrated and co-trained with video models for a downstream objective, dynamically adapting its token granularity to semantic complexity, independent of video duration. TrajTok contains a unified segmenter that performs implicit clustering over pixels in both space and time to directly produce object trajectories in a single forward pass. By prioritizing downstream adaptability over pixel-perfect segmentation fidelity, TrajTok is lightweight and efficient, yet empirically improves video understanding performance. With TrajTok, we implement a video CLIP model trained from scratch (TrajViT2). It achieves the best accuracy at scale across both classification and retrieval benchmarks, while maintaining efficiency comparable to the best token-merging methods. TrajTok also proves to be a versatile component beyond its role as a tokenizer. We show that it can be seamlessly integrated as either a probing head for pretrained visual features (TrajAdapter) or an alignment connector in vision-language models (TrajVLM) with especially strong performance in long-video reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u5206\u8bcd\u6a21\u5757TrajTok\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u667a\u80fd\u7684\u89c6\u9891token\u751f\u6210\uff0c\u5e76\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6a21\u578b\u91c7\u7528\u5207\u5757\u65b9\u5f0f\uff08patchification\uff09\u5bfc\u81f4token\u6570\u91cf\u8fc7\u591a\u3001\u5197\u4f59\uff0c\u9650\u5236\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002\u5c3d\u7ba1\u57fa\u4e8e\u8f68\u8ff9\u7684tokenizer\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u4f9d\u8d56\u590d\u6742\u3001\u4f4e\u6548\u7684\u5916\u90e8\u5206\u5272\u548c\u8ddf\u8e2a\uff0c\u4e14\u4e0e\u4e0b\u6e38\u4efb\u52a1\u5272\u88c2\u3002\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u3001\u4e00\u4f53\u5316\u3001\u53ef\u81ea\u9002\u5e94\u7684\u89c6\u9891tokenization\u65b9\u6848\u3002", "method": "TrajTok\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89c6\u9891\u5206\u8bcd\u6a21\u5757\uff0c\u53ef\u4e0e\u89c6\u9891\u6a21\u578b\u5171\u540c\u8bad\u7ec3\uff0c\u81ea\u9002\u5e94\u5730\u6839\u636e\u8bed\u4e49\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574token granularity\uff0c\u65e0\u9700\u4f9d\u8d56\u89c6\u9891\u65f6\u957f\u3002\u5176\u5185\u7f6e\u7edf\u4e00\u5206\u5272\u5668\uff0c\u5229\u7528\u65f6\u7a7a\u9690\u5f0f\u805a\u7c7b\u76f4\u63a5\u8f93\u51fa\u7269\u4f53\u8f68\u8ff9\uff0c\u5b9e\u73b0\u5355\u6b65\u524d\u5411\u751f\u6210token\u3002\u5e76\u5728\u8bbe\u8ba1\u4e0a\u4f18\u5148\u8003\u8651\u4e0b\u6e38\u9002\u5e94\u6027\u800c\u975e\u9010\u50cf\u7d20\u7cbe\u5ea6\u3002", "result": "\u57fa\u4e8eTrajTok\u5b9e\u73b0\u7684TrajViT2\uff08\u4e00\u79cd\u4ece\u96f6\u8bad\u7ec3\u7684\u89c6\u9891CLIP\u6a21\u578b\uff09\u5728\u5206\u7c7b\u548c\u68c0\u7d22\u57fa\u51c6\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u6548\u7387\u53ef\u4e0e\u6700\u4f73token-merge\u65b9\u6cd5\u5ab2\u7f8e\u3002TrajTok\u7075\u6d3b\uff0c\u53ef\u4f5c\u4e3a\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\u7684probe head\uff08TrajAdapter\uff09\u6216\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u8fde\u63a5\u5668\uff08TrajVLM\uff09\uff0c\u5728\u957f\u89c6\u9891\u63a8\u7406\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "TrajTok\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7cbe\u7b80\u4e14\u5177\u6709\u5f3a\u4e0b\u6e38\u9002\u5e94\u6027\u7684\u89c6\u9891\u5206\u8bcd\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u7075\u6d3b\u6027\uff0c\u5e76\u80fd\u591f\u5e7f\u6cdb\u9002\u914d\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u6a21\u578b\uff0c\u662f\u89c6\u9891tokenization\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2602.22785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22785", "abs": "https://arxiv.org/abs/2602.22785", "authors": ["Ling Wang", "Hao-Xiang Guo", "Xinzhou Wang", "Fuchun Sun", "Kai Sun", "Pengkun Liu", "Hang Xiao", "Zhong Wang", "Guangyuan Fu", "Eric Li", "Yang Liu", "Yikai Wang"], "title": "SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation", "comment": "published at iclr 2026", "summary": "We introduce SceneTransporter, an end-to-end framework for structured 3D scene generation from a single image. While existing methods generate part-level 3D objects, they often fail to organize these parts into distinct instances in open-world scenes. Through a debiased clustering probe, we reveal a critical insight: this failure stems from the lack of structural constraints within the model's internal assignment mechanism. Based on this finding, we reframe the task of structured 3D scene generation as a global correlation assignment problem. To solve this, SceneTransporter formulates and solves an entropic Optimal Transport (OT) objective within the denoising loop of the compositional DiT model. This formulation imposes two powerful structural constraints. First, the resulting transport plan gates cross-attention to enforce an exclusive, one-to-one routing of image patches to part-level 3D latents, preventing entanglement. Second, the competitive nature of the transport encourages the grouping of similar patches, a process that is further regularized by an edge-based cost, to form coherent objects and prevent fragmentation. Extensive experiments show that SceneTransporter outperforms existing methods on open-world scene generation, significantly improving instance-level coherence and geometric fidelity. Code and models will be publicly available at https://2019epwl.github.io/SceneTransporter/.", "AI": {"tldr": "SceneTransporter\u662f\u4e00\u4e2a\u80fd\u591f\u4ece\u5355\u5f20\u56fe\u7247\u7aef\u5230\u7aef\u751f\u6210\u7ed3\u6784\u53163D\u573a\u666f\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u6027\u7ea6\u675f\u663e\u8457\u63d0\u5347\u4e86\u5206\u5b9e\u4f8b\u7684\u573a\u666f\u5efa\u6a21\u6548\u679c\u3002", "motivation": "\u73b0\u67093D\u751f\u6210\u65b9\u6cd5\u867d\u7136\u53ef\u4ee5\u751f\u6210\u96f6\u90e8\u4ef6\u7ea7\u522b\u7684\u7269\u4f53\uff0c\u4f46\u96be\u4ee5\u5c06\u8fd9\u4e9b\u90e8\u5206\u7ec4\u7ec7\u6210\u6e05\u6670\u5206\u660e\u7684\u5b9e\u4f8b\uff0c\u7279\u522b\u662f\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u3002\u672c\u5de5\u4f5c\u53d1\u73b0\u95ee\u9898\u6839\u672c\u539f\u56e0\u662f\u6a21\u578b\u5185\u90e8\u7f3a\u5c11\u7ed3\u6784\u7ea6\u675f\u3002", "method": "\u4f5c\u8005\u5c06\u7ed3\u6784\u53163D\u573a\u666f\u751f\u6210\u4efb\u52a1\u91cd\u65b0\u8868\u8ff0\u4e3a\u5168\u5c40\u76f8\u5173\u5206\u914d\u95ee\u9898\u3002\u63d0\u51faSceneTransporter\uff0c\u5728DiT\u6a21\u578b\u7684\u53bb\u566a\u5faa\u73af\u4e2d\u5f15\u5165\u71b5\u6b63\u5219\u6700\u4f18\u8fd0\u8f93(OT)\u76ee\u6807\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u72ec\u7279\u7684\u5206\u914d\u8ba1\u5212\uff0c\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u7ea6\u675f\u5b9e\u73b0\u56fe\u50cf\u5206\u5757\u4e0e3D\u90e8\u5206\u6f5c\u53d8\u91cf\u7684\u4e00\u4e00\u5bf9\u5e94\uff0c\u5e76\u5728\u4f20\u8f93\u8ba1\u5212\u4e2d\u5f15\u5165\u7ade\u4e89\u673a\u5236\u548c\u57fa\u4e8e\u8fb9\u7f18\u7684\u6b63\u5219\u5316\u6210\u672c\uff0c\u4ece\u800c\u5b9e\u73b0\u76f8\u4f3c\u5757\u805a\u5408\u6210\u6574\u4f53\u5bf9\u8c61\u4e14\u907f\u514d\u788e\u7247\u5316\u3002", "result": "SceneTransporter\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5206\u5b9e\u4f8b\u7684\u8fde\u8d2f\u6027\u4e0e\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u5f15\u5165\u7ed3\u6784\u6027\u5206\u914d\u7ea6\u675f\u6781\u5927\u5730\u4fc3\u8fdb\u4e863D\u573a\u666f\u751f\u6210\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u7b26\u5408\u5b9e\u9645\u9700\u6c42\u7684\u5f00\u653e\u4e16\u754c3D\u573a\u666f\u5b9e\u4f8b\u7ec4\u7ec7\u3002"}}
{"id": "2602.22821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22821", "abs": "https://arxiv.org/abs/2602.22821", "authors": ["Tong Wang", "Yaolei Qi", "Siwen Wang", "Imran Razzak", "Guanyu Yang", "Yutong Xie"], "title": "CMSA-Net: Causal Multi-scale Aggregation with Adaptive Multi-source Reference for Video Polyp Segmentation", "comment": null, "summary": "Video polyp segmentation (VPS) is an important task in computer-aided colonoscopy, as it helps doctors accurately locate and track polyps during examinations. However, VPS remains challenging because polyps often look similar to surrounding mucosa, leading to weak semantic discrimination. In addition, large changes in polyp position and scale across video frames make stable and accurate segmentation difficult. To address these challenges, we propose a robust VPS framework named CMSA-Net. The proposed network introduces a Causal Multi-scale Aggregation (CMA) module to effectively gather semantic information from multiple historical frames at different scales. By using causal attention, CMA ensures that temporal feature propagation follows strict time order, which helps reduce noise and improve feature reliability. Furthermore, we design a Dynamic Multi-source Reference (DMR) strategy that adaptively selects informative and reliable reference frames based on semantic separability and prediction confidence. This strategy provides strong multi-frame guidance while keeping the model efficient for real-time inference. Extensive experiments on the SUN-SEG dataset demonstrate that CMSA-Net achieves state-of-the-art performance, offering a favorable balance between segmentation accuracy and real-time clinical applicability.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86CMSA-Net\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u9891\u7ed3\u80a0\u955c\u4e0b\u606f\u8089\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\uff0c\u6838\u5fc3\u65b9\u6cd5\u662f\u591a\u5c3a\u5ea6\u5386\u53f2\u4fe1\u606f\u805a\u5408\u548c\u52a8\u6001\u53c2\u8003\u5e27\u9009\u62e9\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6548\u679c\u4f18\u5f02\u3002", "motivation": "\u89c6\u9891\u606f\u8089\u5206\u5272\u4efb\u52a1\u96be\u5728\u4e8e\u606f\u8089\u5916\u89c2\u4e0e\u5468\u56f4\u9ecf\u819c\u76f8\u4f3c\uff0c\u4ee5\u53ca\u8de8\u5e27\u4f4d\u7f6e\u548c\u5c3a\u5ea6\u53d8\u5316\u5927\uff0c\u5bfc\u81f4\u5206\u5272\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u4e0d\u8db3\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u4e9b\u60c5\u5f62\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u63d0\u5347\u65f6\u5e8f\u7279\u5f81\u5229\u7528\u548c\u5206\u5272\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faCMSA-Net\u6846\u67b6\u3002\u5176\u4e3b\u8981\u521b\u65b0\u5305\u62ec\uff1a(1) \u56e0\u679c\u591a\u5c3a\u5ea6\u805a\u5408\uff08CMA\uff09\u6a21\u5757\uff0c\u4ece\u4e0d\u540c\u5386\u53f2\u5e27\u548c\u591a\u5c3a\u5ea6\u805a\u5408\u8bed\u4e49\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u56e0\u679c\u6ce8\u610f\u529b\u4fdd\u8bc1\u7279\u5f81\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\uff1b(2) \u52a8\u6001\u591a\u6e90\u53c2\u8003\uff08DMR\uff09\u7b56\u7565\uff0c\u7ed3\u5408\u8bed\u4e49\u53ef\u5206\u6027\u548c\u9884\u6d4b\u7f6e\u4fe1\u5ea6\uff0c\u81ea\u9002\u5e94\u6311\u9009\u66f4\u53ef\u9760\u7684\u53c2\u8003\u5e27\uff0c\u589e\u5f3a\u591a\u5e27\u5f15\u5bfc\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u3002", "result": "\u5728SUN-SEG\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCMSA-Net\u5728\u5206\u5272\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "CMSA-Net\u901a\u8fc7\u5145\u5206\u5229\u7528\u591a\u5e27\u591a\u5c3a\u5ea6\u4fe1\u606f\uff0c\u91c7\u7528\u56e0\u679c\u6ce8\u610f\u529b\u548c\u52a8\u6001\u53c2\u8003\u5e27\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f6\u606f\u8089\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u6709\u671b\u63a8\u52a8\u4e34\u5e8a\u8f85\u52a9\u8bca\u65ad\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2602.23214", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.23214", "abs": "https://arxiv.org/abs/2602.23214", "authors": ["Chenhe Du", "Xuanyu Tian", "Qing Wu", "Muyu Liu", "Jingyi Yu", "Hongjiang Wei", "Yuyao Zhang"], "title": "Plug-and-Play Diffusion Meets ADMM: Dual-Variable Coupling for Robust Medical Image Reconstruction", "comment": null, "summary": "Plug-and-Play diffusion prior (PnPDP) frameworks have emerged as a powerful paradigm for solving imaging inverse problems by treating pretrained generative models as modular priors. However, we identify a critical flaw in prevailing PnP solvers (e.g., based on HQS or Proximal Gradient): they function as memoryless operators, updating estimates solely based on instantaneous gradients. This lack of historical tracking inevitably leads to non-vanishing steady-state bias, where the reconstruction fails to strictly satisfy physical measurements under heavy corruption. To resolve this, we propose Dual-Coupled PnP Diffusion, which restores the classical dual variable to provide integral feedback, theoretically guaranteeing asymptotic convergence to the exact data manifold. However, this rigorous geometric coupling introduces a secondary challenge: the accumulated dual residuals exhibit spectrally colored, structured artifacts that violate the Additive White Gaussian Noise (AWGN) assumption of diffusion priors, causing severe hallucinations. To bridge this gap, we introduce Spectral Homogenization (SH), a frequency-domain adaptation mechanism that modulates these structured residuals into statistically compliant pseudo-AWGN inputs. This effectively aligns the solver's rigorous optimization trajectory with the denoiser's valid statistical manifold. Extensive experiments on CT and MRI reconstruction demonstrate that our approach resolves the bias-hallucination trade-off, achieving state-of-the-art fidelity with significantly accelerated convergence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Dual-Coupled PnP Diffusion\u4e0eSpectral Homogenization\uff08SH\uff09\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3PnP\u6269\u6563\u5148\u9a8c\u5728\u533b\u5b66\u6210\u50cf\u9006\u95ee\u9898\u4e2d\u504f\u5dee\u4e0e\u5e7b\u89c9\u7684\u56f0\u5883\uff0c\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684Plug-and-Play diffusion prior\uff08PnPDP\uff09\u5728\u6210\u50cf\u9006\u95ee\u9898\u4e2d\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u5386\u53f2\u4fe1\u606f\u7684\u8ffd\u8e2a\uff0c\u5bfc\u81f4\u91cd\u5efa\u7ed3\u679c\u96be\u4ee5\u4e25\u683c\u6ee1\u8db3\u7269\u7406\u6d4b\u91cf\uff0c\u51fa\u73b0\u6301\u7eed\u6027\u7684\u504f\u5dee\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u9ad8\u5ea6\u8150\u8680\u65f6\u3002", "method": "\u4f5c\u8005\u63d0\u51faDual-Coupled PnP Diffusion\u65b9\u6cd5\uff0c\u901a\u8fc7\u6062\u590d\u7ecf\u5178\u7684\u5bf9\u5076\u53d8\u91cf\uff0c\u5f15\u5165\u79ef\u5206\u53cd\u9988\uff0c\u5b9e\u73b0\u5bf9\u6570\u636e\u6d41\u5f62\u7684\u6e10\u8fd1\u6536\u655b\uff1b\u540c\u65f6\uff0c\u9488\u5bf9\u56e0\u51e0\u4f55\u8026\u5408\u5f15\u5165\u7684\u7ed3\u6784\u5316\u4f2a\u5f71\uff08\u4e0e\u6269\u6563\u53bb\u566a\u7684\u767d\u566a\u58f0\u5047\u8bbe\u4e0d\u7b26\uff0c\u5bfc\u81f4\u4e25\u91cd\u5e7b\u89c9\uff09\uff0c\u63d0\u51faSpectral Homogenization\uff08SH\uff09\uff0c\u5728\u9891\u57df\u5bf9\u6b8b\u5dee\u8fdb\u884c\u8c03\u6574\uff0c\u8fd1\u4f3c\u4e3a\u7b26\u5408\u6269\u6563\u5148\u9a8c\u767d\u566a\u58f0\u5047\u8bbe\u7684\u8f93\u5165\u3002", "result": "\u5728CT\u548cMRI\u91cd\u5efa\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u89e3\u51b3\u4e86PNP\u65b9\u6cd5\u4e2d\u7684\u504f\u5dee\u4e0e\u5e7b\u89c9\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u5e76\u663e\u8457\u52a0\u5feb\u4e86\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u4f5c\u8005\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u65e2\u89e3\u51b3\u4e86PnPDP\u7684\u7a33\u6001\u504f\u5dee\u96be\u9898\uff0c\u4e5f\u907f\u514d\u4e86\u4e25\u683c\u4f18\u5316\u5e26\u6765\u7684\u5e7b\u89c9\u98ce\u9669\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u50cf\u91cd\u5efa\u8d28\u91cf\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.23357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23357", "abs": "https://arxiv.org/abs/2602.23357", "authors": ["Aheli Saha", "Ren\u00e9 Schuster", "Didier Stricker"], "title": "Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training", "comment": "12 pages, International Conference on Pattern Recognition Applications and Methods", "summary": "Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u4eff\u751f\u4e8b\u4ef6\u76f8\u673a\u5728\u8f93\u51fa\u4fe1\u53f7\u65b9\u9762\u5b58\u5728\u7684\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u548c\u53c2\u6570\u5206\u6790\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u6df1\u5165\u5206\u6790\u4e86\u5185\u5728\u53c2\u6570\u5bf9\u57fa\u4e8e\u4e8b\u4ef6\u6570\u636e\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u4e0d\u540c\u4f20\u611f\u5668\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4eff\u751f\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u5f02\u6b65\u3001\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u8fd0\u52a8\u6a21\u7cca\u7b49\u4f18\u70b9\uff0c\u4f46\u7531\u4e8e\u5176\u4fe1\u53f7\u8f93\u51fa\u7684\u65b0\u9896\u6027\uff0c\u73b0\u6709\u5173\u4e8e\u4fe1\u53f7\u53c2\u6570\u5206\u6790\u53ca\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\uff09\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u5b9e\u9a8c\uff0c\u5206\u6790\u5185\u5728\u53c2\u6570\uff08\u5982\u4e8b\u4ef6\u76f8\u673a\u8f93\u51fa\u4fe1\u53f7\u7684\u7279\u5f81\uff09\u5bf9\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u57fa\u4e8e\u5b9e\u9a8c\u7ed3\u679c\u4f18\u5316\u6a21\u578b\uff0c\u4f7f\u5176\u5177\u5907\u66f4\u5f3a\u7684\u4f20\u611f\u5668\u65e0\u5173\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e8b\u4ef6\u76f8\u673a\u7684\u4e0d\u540c\u53c2\u6570\u914d\u7f6e\u5bf9\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002\u7ed3\u5408\u53c2\u6570\u5206\u6790\uff0c\u4f18\u5316\u540e\u7684\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u4f20\u611f\u5668\u65e0\u5173\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u5bf9\u4e8b\u4ef6\u76f8\u673a\u4fe1\u53f7\u53c2\u6570\u7684\u6df1\u5165\u5206\u6790\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u57fa\u4e8e\u4e8b\u4ef6\u6570\u636e\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u66f4\u597d\u7684\u4f20\u611f\u5668\u6cdb\u5316\uff0c\u63a8\u52a8\u4e8b\u4ef6\u76f8\u673a\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.23361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23361", "abs": "https://arxiv.org/abs/2602.23361", "authors": ["Sven Elflein", "Ruilong Li", "S\u00e9rgio Agostinho", "Zan Gojcic", "Laura Leal-Taix\u00e9", "Qunjie Zhou", "Aljosa Osep"], "title": "VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale", "comment": "CVPR 2026, Project page: https://research.nvidia.com/labs/dvl/projects/vgg-ttt", "summary": "We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u76843D\u91cd\u5efa\u6a21\u578bVGG-T$^3$\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u79bb\u7ebf\u524d\u9988\u65b9\u6cd5\u8ba1\u7b97\u4e0e\u5185\u5b58\u5f00\u9500\u968f\u8f93\u5165\u56fe\u7247\u6570\u91cf\u5e73\u65b9\u589e\u957f\u7684\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u7ebf\u6027\u6269\u5c55\u53ca\u663e\u8457\u63d0\u901f\u3002", "motivation": "\u5f53\u524d3D\u91cd\u5efa\u79bb\u7ebf\u65b9\u6cd5\u56e0Key-Value\u7a7a\u95f4\u968f\u8f93\u5165\u56fe\u7247\u6570\u91cf\u53d8\u5316\u800c\u5bfc\u81f4\u8ba1\u7b97\u4e0e\u5185\u5b58\u9700\u6c42\u5448\u5e73\u65b9\u589e\u957f\uff0c\u4e25\u91cd\u5f71\u54cd\u5e94\u7528\u7684\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff08test time training\uff09\uff0c\u5c06\u573a\u666f\u51e0\u4f55\u7684\u53ef\u53d8\u957f\u5ea6Key-Value\u8868\u793a\u84b8\u998f\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\uff0c\u8fd9\u6837\u6574\u4e2a\u63a8\u7406\u6d41\u7a0b\u7684\u590d\u6742\u5ea6\u7ebf\u6027\u589e\u957f\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u573a\u666f\u805a\u5408\u4e0e\u91cd\u5efa\u3002\u540c\u65f6\u652f\u6301\u573a\u666f\u68c0\u7d22\u4e0e\u89c6\u89c9\u5b9a\u4f4d\u3002", "result": "\u57281000\u5f20\u56fe\u7247\u91cd\u5efa\u4ec5\u970054\u79d2\uff0c\u6bd4\u57fa\u4e8esoftmax\u6ce8\u610f\u529b\u7684\u57fa\u7ebf\u5feb11.6\u500d\u3002\u5176\u70b9\u4e91\u91cd\u5efa\u8bef\u5dee\u5927\u5e45\u4f18\u4e8e\u5176\u4ed6\u7ebf\u6027\u65f6\u95f4\u65b9\u6cd5\uff0c\u5e76\u80fd\u652f\u6301\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u3002", "conclusion": "\u6240\u63d0\u51fa\u6a21\u578b\u65e2\u4fdd\u6301\u7ebf\u6027\u6269\u5c55\u6548\u7387\uff0c\u53c8\u517c\u5177\u5168\u5c40\u573a\u666f\u4fe1\u606f\u805a\u5408\u80fd\u529b\uff0c\u57283D\u91cd\u5efa\u4e0e\u89c6\u89c9\u5b9a\u4f4d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u540c\u7c7b\u65b9\u6cd5\u3002"}}
{"id": "2602.23363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23363", "abs": "https://arxiv.org/abs/2602.23363", "authors": ["Sahal Shaji Mullappilly", "Mohammed Irfan Kurpath", "Omair Mohamed", "Mohamed Zidan", "Fahad Khan", "Salman Khan", "Rao Anwer", "Hisham Cholakkal"], "title": "MediX-R1: Open Ended Medical Reinforcement Learning", "comment": null, "summary": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com", "AI": {"tldr": "MediX-R1\u662f\u4e00\u79cd\u9488\u5bf9\u533b\u7597\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u7684\u5f00\u653e\u5f0f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u81ea\u7531\u56de\u7b54\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u591a\u6a21\u6001\u5927\u6a21\u578b\u591a\u4ee5\u9009\u62e9\u9898\u5f62\u5f0f\u8bc4\u4f30\u4e0e\u8bad\u7ec3\uff0c\u96be\u4ee5\u652f\u6301\u771f\u5b9e\u573a\u666f\u4e0b\u5f00\u653e\u5f0f\u3001\u81ea\u7531\u7684\u4e34\u5e8a\u63a8\u7406\u9700\u6c42\u3002\u672c\u7814\u7a76\u5e0c\u671b\u63a8\u52a8\u591a\u6a21\u6001AI\u6a21\u578b\u5728\u533b\u5b66\u63a8\u7406\u548c\u81ea\u7531\u8868\u8fbe\u80fd\u529b\u65b9\u9762\u7684\u5b9e\u7528\u6027\u3002", "method": "MediX-R1\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u4e3b\u5e72\u7f51\u7edc\uff0c\u91c7\u7528\u5206\u7ec4\u5f0f\u5f3a\u5316\u5b66\u4e60\u548c\u590d\u5408\u5956\u52b1\u673a\u5236\uff0c\u5305\u62ecLLM\u5224\u5b9a\u7684\u51c6\u786e\u6027\u5956\u52b1\uff08\u4e25\u683c\u4e8c\u5143\uff09\uff0c\u533b\u5b66\u5d4c\u5165\u7684\u8bed\u4e49\u5956\u52b1\uff08\u6355\u83b7\u540c\u4e49\u4e0e\u4e13\u4e1a\u672f\u8bed\u53d8\u4f53\uff09\u4ee5\u53ca\u683c\u5f0f\u548c\u6a21\u6001\u8bc6\u522b\u5956\u52b1\u3002\u591a\u4fe1\u53f7\u5956\u52b1\u4fdd\u8bc1\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u8f93\u51fa\u9ad8\u8d28\u91cf\u5f00\u653e\u5f0f\u7b54\u6848\u3002\u8bc4\u4f30\u4e0a\uff0c\u63d0\u51fa\u7edf\u4e00\u6587\u672c\u548c\u56fe\u6587\u8bc4\u4ef7\u6846\u67b6\uff0c\u4ee5LLM\u4e3a\u88c1\u5224\uff0c\u4fa7\u91cd\u8bed\u4e49\u3001\u63a8\u7406\u4e0e\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4ec5\u4f7f\u7528\u7ea65.1\u4e07\u6761\u6307\u4ee4\u6570\u636e\u4e0b\uff0cMediX-R1\u5728\u4e3b\u6d41\u533b\u5b66\u6587\u672c\u548c\u591a\u6a21\u6001\u8bc4\u6d4b\u57fa\u51c6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u540c\u7c7b\u5f00\u6e90\u6a21\u578b\uff0c\u5c24\u5176\u5728\u5f00\u653e\u5f0f\u4e34\u5e8a\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u591a\u4fe1\u53f7\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408LLM\u88c1\u5224\u7684\u8bc4\u4ef7\u4f53\u7cfb\uff0c\u4e3a\u533b\u7597\u591a\u6a21\u6001\u5927\u6a21\u578b\u5b9e\u73b0\u53ef\u9760\u5f00\u653e\u5f0f\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u9053\u8def\u3002\u79d1\u7814\u8d44\u6e90\u548c\u6a21\u578b\u5df2\u5f00\u6e90\uff0c\u53ef\u4f9b\u5b66\u754c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u548c\u6539\u8fdb\u3002"}}
