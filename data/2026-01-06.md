<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 173]
- [cs.CL](#cs.CL) [Total: 75]
- [cs.RO](#cs.RO) [Total: 31]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Free Energy-Based Modeling of Emotional Dynamics in Video Advertisements](https://arxiv.org/abs/2601.00812)
*Takashi Ushio,Kazuhiro Onishi,Hideyoshi Yanagisawa*

Main category: cs.CV

TL;DR: 本文提出了一种基于自由能原理（Free Energy Principle），仅通过广告视频的场景级表情特征来可解释性地估算观众的情感反应，而无需生理信号或主观评分。实验表明，KLD、贝叶斯惊奇和不确定性等指标能有效反映愉悦、惊奇和习惯等情感维度，且方法在不同广告类型和参数下具有稳健性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 情绪反应对广告效果（包括注意、记忆、购买意愿）至关重要。现有方法多依赖外部生理数据或主观评分，缺乏低成本且可解释的自动化方法。因此，本文旨在挖掘只利用视频内容，能可解释地估算观众情绪反应的方法基础。

Method: 提出基于自由能原理的情感估算框架，利用场景级表情特征，分别用Kullback-Leibler散度（KLD）、贝叶斯惊奇（BS）、不确定性（UN）建模情绪核心维度，并在1,059则15秒美食广告视频上进行实验。分析各指标能否反映不同结构和类型呈现下的愉悦、惊奇与习惯，并评估方法在多个超参数、不同广告类型中的表现。

Result: KLD能反映与品牌出现相关的‘愉悦’；BS能捕捉信息复杂度驱动的‘惊奇’；UN能区分由元素类型及空间变动导致的不确定性型‘惊奇’。发现了三种典型情感变化模式，并证明这些特征在多种参数及不同广告类型下表现稳定。

Conclusion: 提出的方法无需生理或主观数据，能精准、可解释地估算广告视频中的情感反应。未来可通过引入更多表达特征与主观验证，推动更具吸引力广告内容的创制技术应用。

Abstract: Emotional responses during advertising video viewing are recognized as essential for understanding media effects because they have influenced attention, memory, and purchase intention. To establish a methodological basis for explainable emotion estimation without relying on external information such as physiological signals or subjective ratings, we have quantified "pleasantness," "surprise," and "habituation" solely from scene-level expression features of advertising videos, drawing on the free energy(FE) principle, which has provided a unified account of perception, learning, and behavior. In this framework, Kullback-Leibler divergence (KLD) has captured prediction error, Bayesian surprise (BS) has captured belief updates, and uncertainty (UN) has reflected prior ambiguity, and together they have formed the core components of FE. Using 1,059 15 s food video advertisements, the experiments have shown that KLD has reflected "pleasantness" associated with brand presentation, BS has captured "surprise" arising from informational complexity, and UN has reflected "surprise" driven by uncertainty in element types and spatial arrangements, as well as by the variability and quantity of presented elements. This study also identified three characteristic emotional patterns, namely uncertain stimulus, sustained high emotion, and momentary peak and decay, demonstrating the usefulness of the proposed method. Robustness across nine hyperparameter settings and generalization tests with six types of Japanese advertising videos (three genres and two durations) confirmed that these tendencies remained stable. This work can be extended by integrating a wider range of expression elements and validating the approach through subjective ratings, ultimately guiding the development of technologies that can support the creation of more engaging advertising videos.

</details>


### [2] [Can Generative Models Actually Forge Realistic Identity Documents?](https://arxiv.org/abs/2601.00829)
*Alexander Vinogradov*

Main category: cs.CV

TL;DR: 本论文评估了开源扩散式生成模型伪造身份证件的能力，发现其无法达到法医级别的真实性，风险可能被高估。


<details>
  <summary>Details</summary>
Motivation: 随着生成式图像模型图像真实度的大幅提升，社会对其被滥用于伪造证件的潜在威胁产生了担忧。作者希望评估这些模型用于伪造身份证件时能否骗过人类和自动化核验系统。

Method: 作者测试了多种主流开源扩散式生成模型（如Stable Diffusion、Qwen、Flux、Nano-Banana等），采用文本到图像和图像到图像的生成方式，分析生成伪造证件的外观和结构真实性。

Result: 目前的生成模型虽可拟合证件表层美学特征，但无法还原结构性和法医层面的真实性，因此难以骗过严格的验证系统。

Conclusion: 当前生成式深度伪造身份证件达到法医鉴定标准的风险被高估了，强调机器学习与证件取证专家合作评估风险的重要性。

Abstract: Generative image models have recently shown significant progress in image realism, leading to public concerns about their potential misuse for document forgery. This paper explores whether contemporary open-source and publicly accessible diffusion-based generative models can produce identity document forgeries that could realistically bypass human or automated verification systems. We evaluate text-to-image and image-to-image generation pipelines using multiple publicly available generative model families, including Stable Diffusion, Qwen, Flux, Nano-Banana, and others. The findings indicate that while current generative models can simulate surface-level document aesthetics, they fail to reproduce structural and forensic authenticity. Consequently, the risk of generative identity document deepfakes achieving forensic-level authenticity may be overestimated, underscoring the value of collaboration between machine learning practitioners and document-forensics experts in realistic risk assessment.

</details>


### [3] [Pediatric Pneumonia Detection from Chest X-Rays:A Comparative Study of Transfer Learning and Custom CNNs](https://arxiv.org/abs/2601.00837)
*Agniv Roy Choudhury*

Main category: cs.CV

TL;DR: 本研究比较了自建卷积神经网络（CNN）与多种预训练模型（ResNet50、DenseNet121、EfficientNet-B0）在儿科肺炎X光片自动检测中的表现，发现迁移学习（微调）模型效果显著优于从零训练的模型。


<details>
  <summary>Details</summary>
Motivation: 儿科肺炎是全球五岁以下儿童主要死因之一，由于专业放射科医生的人力和判断差异，依赖人工阅片常存在局限，因此迫切需要准确、高效的自动化影像分析工具。

Method: 作者用分层采样的5216份儿科胸部X光片数据集，按照8:1:1分为训练、验证和测试集，比较从头训练的自定义CNN与三种主流预训练网络（ResNet50、DenseNet121、EfficientNet-B0），并实验了特征冻结和全微调两种迁移策略，用准确率、F1和AUC评估表现，通过Grad-CAM解释预测区域。

Result: 微调的ResNet50取得了最高分（准确率99.43%、F1分99.61%、AUC 99.93%），仅有3例判错，综合性能平均比冻结特征者高5.5个百分点，Grad-CAM显示模型关注的区域与实际病变区域一致。

Conclusion: 迁移学习结合微调技术在儿科肺炎X光自动检测上明显优于自定义CNN，几乎达到了完美准确率，具有作为低资源地区筛查工具的潜力，后续建议在多中心及成人数据集上进一步验证。

Abstract: Pneumonia is a leading cause of mortality in children under five, with over 700,000 deaths annually. Accurate diagnosis from chest X-rays is limited by radiologist availability and variability.
  Objective: This study compares custom CNNs trained from scratch with transfer learning (ResNet50, DenseNet121, EfficientNet-B0) for pediatric pneumonia detection, evaluating frozen-backbone and fine-tuning regimes.
  Methods: A dataset of 5,216 pediatric chest X-rays was split 80/10/10 for training, validation, and testing. Seven models were trained and assessed using accuracy, F1-score, and AUC. Grad-CAM visualizations provided explainability.
  Results: Fine-tuned ResNet50 achieved the best performance: 99.43\% accuracy, 99.61\% F1-score, and 99.93\% AUC, with only 3 misclassifications. Fine-tuning outperformed frozen-backbone models by 5.5 percentage points on average. Grad-CAM confirmed clinically relevant lung regions guided predictions.
  Conclusions: Transfer learning with fine-tuning substantially outperforms CNNs trained from scratch for pediatric pneumonia detection, showing near-perfect accuracy. This system has strong potential as a screening tool in resource-limited settings. Future work should validate these findings on multi-center and adult datasets.
  Keywords: Pneumonia detection, deep learning, transfer learning, CNN, chest X-ray, pediatric diagnosis, ResNet, DenseNet, EfficientNet, Grad-CAM.

</details>


### [4] [Unified Review and Benchmark of Deep Segmentation Architectures for Cardiac Ultrasound on CAMUS](https://arxiv.org/abs/2601.00839)
*Zahid Ullah,Muhammad Hilal,Eunsoo Lee,Dragan Pamucar,Jihie Kim*

Main category: cs.CV

TL;DR: 本文综合回顾了心脏超声分割领域的主流深度学习架构（U-Net、Attention U-Net、TransUNet），并在统一实验基准条件下进行了系统对比，提出了数据预处理和标注的新建议。


<details>
  <summary>Details</summary>
Motivation: 尽管已有综述总结了心脏影像与深度学习进展，但缺乏对各常用架构在统一可复现实验中的系统对比和标准化评估。

Method: 作者在公开的CAMUS超声心动图数据集上，采用统一的数据预处理（NIfTI原始体、16位PNG、GPT辅助的伪标签、自监督预训练）、固定分割、损失和评价标准，对U-Net、Attention U-Net、TransUNet进行了系统实验，并探索了标注扩展与自监督方法。

Result: （1）U-Net在保留原始动态范围的NIfTI数据上获得94%平均Dice分数，16位PNG流程下为91%。Attention U-Net对小或低对比分区有小幅提升，减轻边界泄漏。TransUNet结合自监督初始权重，在复杂切片上泛化性最佳。GPT辅助伪标签经筛选可扩充训练集并提升鲁棒性。

Conclusion: 本文构建了U-Net等分割架构的标准化基准体系，提出高质量数据预处理/伪标签建议，并展望基于自监督、GPT多模态标注的新型心脏超声分析流程。

Abstract: Several review papers summarize cardiac imaging and DL advances, few works connect this overview to a unified and reproducible experimental benchmark. In this study, we combine a focused review of cardiac ultrasound segmentation literature with a controlled comparison of three influential architectures, U-Net, Attention U-Net, and TransUNet, on the Cardiac Acquisitions for Multi-Structure Ultrasound Segmentation (CAMUS) echocardiography dataset. Our benchmark spans multiple preprocessing routes, including native NIfTI volumes, 16-bit PNG exports, GPT-assisted polygon-based pseudo-labels, and self-supervised pretraining (SSL) on thousands of unlabeled cine frames. Using identical training splits, losses, and evaluation criteria, a plain U-Net achieved a 94% mean Dice when trained directly on NIfTI data (preserving native dynamic range), while the PNG-16-bit workflow reached 91% under similar conditions. Attention U-Net provided modest improvements on small or low-contrast regions, reducing boundary leakage, whereas TransUNet demonstrated the strongest generalization on challenging frames due to its ability to model global spatial context, particularly when initialized with SSL. Pseudo-labeling expanded the training set and improved robustness after confidence filtering. Overall, our contributions are threefold: a harmonized, apples-to-apples benchmark of U-Net, Attention U-Net, and TransUNet under standardized CAMUS preprocessing and evaluation; practical guidance on maintaining intensity fidelity, resolution consistency, and alignment when preparing ultrasound data; and an outlook on scalable self-supervision and emerging multimodal GPT-based annotation pipelines for rapid labeling, quality assurance, and targeted dataset curation.

</details>


### [5] [Motion-Compensated Latent Semantic Canvases for Visual Situational Awareness on Edge](https://arxiv.org/abs/2601.00854)
*Igor Lodin,Sergii Filatov,Vira Filatova,Dmytro Filatov*

Main category: cs.CV

TL;DR: 本文提出了一种名为MCLSC的新方法，通过在边缘设备上高效管理静态和动态的语义元数据，极大提升视觉态势感知效率，显著减少了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上执行视频分割和视觉理解任务时，传统的逐帧分割方法成本高、延迟大，实际应用受限。因此需要一种既能保持语义理解一致性，又显著节省计算资源的新思路。

Method: 作者提出将语义信息分为静态和动态两层画布，并在视频流稳定后的坐标系下进行管理。昂贵的全景分割（Mask2Former）推理只在检测到存在新运动时异步触发，而稳定和运动补偿机制保持语义信息一致。这样既减少了分割次数，也保证了语义覆盖的连贯性。

Result: 在480p视频片段上的实验显示，原型系统将分割调用次数减少了30倍以上，端到端处理时间也降低了20倍以上，同时保持了静态和动态语义叠加的连贯性。

Conclusion: 该方法极大提升了资源受限边缘设备的视频语义感知效率，在维持多层次语义覆盖一致性的同时，极大降低了计算负担，为实际场景下边缘智能落地提供了可行方案。

Abstract: We propose Motion-Compensated Latent Semantic Canvases (MCLSC) for visual situational awareness on resource-constrained edge devices. The core idea is to maintain persistent semantic metadata in two latent canvases - a slowly accumulating static layer and a rapidly updating dynamic layer - defined in a baseline coordinate frame stabilized from the video stream. Expensive panoptic segmentation (Mask2Former) runs asynchronously and is motion-gated: inference is triggered only when motion indicates new information, while stabilization/motion compensation preserves a consistent coordinate system for latent semantic memory. On prerecorded 480p clips, our prototype reduces segmentation calls by >30x and lowers mean end-to-end processing time by >20x compared to naive per-frame segmentation, while maintaining coherent static/dynamic semantic overlays.

</details>


### [6] [VL-OrdinalFormer: Vision Language Guided Ordinal Transformers for Interpretable Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.00879)
*Zahid Ullah,Jihie Kim*

Main category: cs.CV

TL;DR: 本文提出了VLOrdinalFormer模型，一种结合视觉与语言引导的顺序学习框架，实现了自动化的膝骨关节炎（KOA）严重程度分级，并在公开数据集上取得了优异表现，尤其在早期分级（KL1和KL2）上显著提升了准确性。


<details>
  <summary>Details</summary>
Motivation: KL分级系统用于评估KOA严重程度，但KL1和KL2之间的影像学差异细微，导致临床读片时存在较大的观察者间差异。开发一种能更准确、自动地分辨这些级别的工具，有助于提升临床决策的准确性和一致性。

Method: 提出的VLOrdinalFormer融合了ViT L16主干、基于CORAL的顺序回归和由CLIP驱动的语义对齐模块，使模型能够结合影像与描述关键信息（如关节间隙变窄、骨赘形成等）。训练中采纳分层五折交叉验证、类别加权和测试时增强，以提升对中间分级的敏感性和模型泛化能力。

Result: 在OAI kneeKL224公开数据集上，VLOrdinalFormer优于主流CNN与ViT基线，有着更高的macro F1分数和整体准确率。尤其对于KL1、KL2分级，表现提升明显。利用Grad-CAM与CLIP相似性分析证实，模型聚焦于与疾病相关的解剖结构，提高了可解释性。

Conclusion: VLOrdinalFormer展示了视觉-语言对齐顺序Transformer在KOA分级中的可靠性与可解释性，特别适合临床中早期病变分级及疾病进展评估，有望成为常规放射诊断中的实用工具。

Abstract: Knee osteoarthritis (KOA) is a leading cause of disability worldwide, and accurate severity assessment using the Kellgren Lawrence (KL) grading system is critical for clinical decision making. However, radiographic distinctions between early disease stages, particularly KL1 and KL2, are subtle and frequently lead to inter-observer variability among radiologists. To address these challenges, we propose VLOrdinalFormer, a vision language guided ordinal learning framework for fully automated KOA grading from knee radiographs. The proposed method combines a ViT L16 backbone with CORAL based ordinal regression and a Contrastive Language Image Pretraining (CLIP) driven semantic alignment module, allowing the model to incorporate clinically meaningful textual concepts related to joint space narrowing, osteophyte formation, and subchondral sclerosis. To improve robustness and mitigate overfitting, we employ stratified five fold cross validation, class aware re weighting to emphasize challenging intermediate grades, and test time augmentation with global threshold optimization. Experiments conducted on the publicly available OAI kneeKL224 dataset demonstrate that VLOrdinalFormer achieves state of the art performance, outperforming CNN and ViT baselines in terms of macro F1 score and overall accuracy. Notably, the proposed framework yields substantial performance gains for KL1 and KL2 without compromising classification accuracy for mild or severe cases. In addition, interpretability analyses using Grad CAM and CLIP similarity maps confirm that the model consistently attends to clinically relevant anatomical regions. These results highlight the potential of vision language aligned ordinal transformers as reliable and interpretable tools for KOA grading and disease progression assessment in routine radiological practice.

</details>


### [7] [VideoCuRL: Video Curriculum Reinforcement Learning with Orthogonal Difficulty Decomposition](https://arxiv.org/abs/2601.00887)
*Hongbo Jin,Kuanwei Lin,Wenhao Zhang,Yichen Jin,Ge Li*

Main category: cs.CV

TL;DR: 本文提出了VideoCuRL，一个通过分解视频理解任务难度为视觉时序复杂度和认知推理深度两个正交维度的新课题学习框架，并在主流基准上显著优于传统强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在训练视频大模型(VideoLLMs)时，依赖于数据随机洗牌或基于标量难度的简单课题学习策略。然而，仅用单一标量难度不能区分视频理解中的视觉时序负载与认知推理深度两大挑战，影响模型高效学习。

Method: VideoCuRL将训练难度分为视觉复杂度和认知复杂度，用无训练成本的代理指标（如光流与关键帧熵评价视觉复杂度，Calibrated Surprisal度量认知复杂度）映射数据至二维课题网格，然后采用competence-aware的Diagonal Wavefront调度训练。此外，引入Dynamic Sparse KL和Structured Revisiting以稳定训练过程，防止奖励崩溃和遗忘。

Result: 在评测推理任务(VSI-Bench)和感知任务(VideoMME)上，VideoCuRL分别比现有强基线提升2.5和2.9分，且不产生生成式课题学习的推理开销。

Conclusion: VideoCuRL有效提升了强化学习驱动的视频大模型训练效率和能力，为大规模视频后训练提供了可扩展且健壮的新方法。

Abstract: Reinforcement Learning (RL) is crucial for empowering VideoLLMs with complex spatiotemporal reasoning. However, current RL paradigms predominantly rely on random data shuffling or naive curriculum strategies based on scalar difficulty metrics. We argue that scalar metrics fail to disentangle two orthogonal challenges in video understanding: Visual Temporal Perception Load and Cognitive Reasoning Depth. To address this, we propose VideoCuRL, a novel framework that decomposes difficulty into these two axes. We employ efficient, training-free proxies, optical flow and keyframe entropy for visual complexity, Calibrated Surprisal for cognitive complexity, to map data onto a 2D curriculum grid. A competence aware Diagonal Wavefront strategy then schedules training from base alignment to complex reasoning. Furthermore, we introduce Dynamic Sparse KL and Structured Revisiting to stabilize training against reward collapse and catastrophic forgetting. Extensive experiments show that VideoCuRL surpasses strong RL baselines on reasoning (+2.5 on VSI-Bench) and perception (+2.9 on VideoMME) tasks. Notably, VideoCuRL eliminates the prohibitive inference overhead of generation-based curricula, offering a scalable solution for robust video post-training.

</details>


### [8] [Comparative Evaluation of CNN Architectures for Neural Style Transfer in Indonesian Batik Motif Generation: A Comprehensive Study](https://arxiv.org/abs/2601.00888)
*Happy Gery Pangestu,Andi Prademon Yunus,Siti Khomsah*

Main category: cs.CV

TL;DR: 本论文系统对比了几种常见CNN骨干网络在印尼蜡染图案神经风格迁移中的表达能力与计算效率。研究发现，ResNet架构在保持结构和风格感知相近的前提下，显著提升了计算效率，适合应用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 现有神经风格迁移主要依赖VGG架构，虽具有较强风格表达能力，但计算和内存消耗大，限制了其实用性。该研究旨在探索更高效且结构保真的网络架构用于印尼蜡染图案的数字化与生成扩展。

Method: 论文选用VGG16、VGG19、Inception V3、ResNet50和ResNet101五种常见CNN骨干网络，进行了245组受控实验，结合定量指标（SSIM、LPIPS）、定性分析和统计学检验，比较了结构保真、风格表现和计算效率之间的权衡。

Result: 背骨架的选择对结构相似度无显著影响（ANOVA on SSIM，p=0.83），表现出近似的结构保真度。ResNet架构比VGG收敛速度快5-6倍，所需FLOPs大幅降低（0.63 vs 10.12 GFLOPs），而感知相似度（LPIPS=0.53）保持接近。风格上，VGG风格更浓重、笔触更丰富；ResNet更好地保留几何与蜡染线条；Inception V3介于两者之间但噪声更显著。

Conclusion: 神经风格迁移的网络架构选择应以计算效率与结构保真为核心，优于仅追求风格表现。ResNet系列骨干可为工业级、可扩展的蜡染生成应用提供高效基础。

Abstract: Neural Style Transfer (NST) provides a computational framework for the digital preservation and generative exploration of Indonesian batik motifs; however, existing approaches remain largely centered on VGG-based architectures whose strong stylistic expressiveness comes at the cost of high computational and memory demands, that limits practical deployment in resource-limited environments. This study presents a systematic comparative analysis of five widely used CNN backbones, namely VGG16, VGG19, Inception V3, ResNet50, and ResNet101, based on 245 controlled experiments combining quantitative metrics, qualitative assessment, and statistical analysis to examine the trade-off between structural preservation, stylistic behavior, and computational efficiency. The results show that backbone selection does not yield statistically significant differences in structural similarity, as confirmed by ANOVA on SSIM (p= 0.83), indicating comparable levels of structural preservation rather than equivalent stylistic quality. Within this context, ResNet-based architectures achieve approximately 5-6x faster convergence than VGG models while maintaining similar perceptual similarity (LPIPS = 0.53) and requiring over 16x fewer FLOPs (0.63 vs 10.12 GFLOPs). Qualitative analysis reveals consistent stylistic trade-offs, with VGG producing denser painterly textures, ResNet favoring geometric stability and canting stroke preservation with milder stylization, and Inception V3 exhibiting intermediate but noisier behavior. These findings reposition architectural choice in NST from maximizing stylistic intensity toward efficiency-aware and structure-preserving deployment, highlighting ResNet-based backbones as a practical foundation for scalable, industry-oriented batik generation.

</details>


### [9] [CornViT: A Multi-Stage Convolutional Vision Transformer Framework for Hierarchical Corn Kernel Analysis](https://arxiv.org/abs/2601.00897)
*Sai Teja Erukude,Jane Mascarenhas,Lior Shamir*

Main category: cs.CV

TL;DR: 该论文提出了CornViT框架，利用卷积Vision Transformer对玉米种子的单粒品质进行三阶段自动化分级，显著优于主流CNN，配套数据集和Web应用均已公开。


<details>
  <summary>Details</summary>
Motivation: 目前玉米种子分级大多依赖人工检查，费时费力、主观性强，亟需高效、可复用的自动化方法来提升检测准确性和效率。

Method: 本文提出了三阶段的CvT-13神经网络结构，分别识别种子纯净度、形态（扁圆）及胚轴朝向。构建了专用的公开数据集，并采用ImageNet预训练再微调策略，与ResNet-50和DenseNet-121做对比，最终嵌入Web界面进行实时推理。

Result: CornViT各阶段测试精度分别达到93.76%、94.11%、91.12%，明显优于ResNet-50（76.56-81.02%）和DenseNet-121（86.56-89.38%）。

Conclusion: 结合CvT的自注意力及卷积特性显著提升了玉米种子分级准确率，CornViT框架与数据集及Web工具为种子分级提供了高效可落地的全流程解决方案。

Abstract: Accurate grading of corn kernels is critical for seed certification, directional seeding, and breeding, yet it is still predominantly performed by manual inspection. This work introduces CornViT, a three-stage Convolutional Vision Transformer (CvT) framework that emulates the hierarchical reasoning of human seed analysts for single-kernel evaluation. Three sequential CvT-13 classifiers operate on 384x384 RGB images: Stage 1 distinguishes pure from impure kernels; Stage 2 categorizes pure kernels into flat and round morphologies; and Stage 3 determines the embryo orientation (up vs. down) for pure, flat kernels. Starting from a public corn seed image collection, we manually relabeled and filtered images to construct three stage-specific datasets: 7265 kernels for purity, 3859 pure kernels for morphology, and 1960 pure-flat kernels for embryo orientation, all released as benchmarks. Head-only fine-tuning of ImageNet-22k pretrained CvT-13 backbones yields test accuracies of 93.76% for purity, 94.11% for shape, and 91.12% for embryo-orientation detection. Under identical training conditions, ResNet-50 reaches only 76.56 to 81.02 percent, whereas DenseNet-121 attains 86.56 to 89.38 percent accuracy. These results highlight the advantages of convolution-augmented self-attention for kernel analysis. To facilitate adoption, we deploy CornViT in a Flask-based web application that performs stage-wise inference and exposes interpretable outputs through a browser interface. Together, the CornViT framework, curated datasets, and web application provide a deployable solution for automated corn kernel quality assessment in seed quality workflows. Source code and data are publicly available.

</details>


### [10] [Evaluating Contextual Intelligence in Recyclability: A Comprehensive Study of Image-Based Reasoning Systems](https://arxiv.org/abs/2601.00905)
*Eliot Park,Abhi Kumar,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: 本文研究了先进视觉-语言模型（如GPT-4o、Claude 3.5等）在日常废弃物回收预测中的表现，发现其理解和应用情境信息能力大幅提升，并对未来模型优化提出建议。


<details>
  <summary>Details</summary>
Motivation: 准确区分可回收物及其归属的回收桶对一般公众来说相当复杂，此难题限制了高效回收的落实。研究希望借助AI模型减轻这一认知负担，提升公共回收效率。

Method: 作者使用经过精心筛选的物品图片数据集，考察三个最新视觉-语言模型对物品识别及其归属回收桶的准确性，并分析如下情境下模型表现：1) 融入地方回收标准的变化；2) 处理受污染或损坏物品；3) 分析多材料复合物品。测试还包括物品实际能否放入垃圾桶的考量。

Result: 结果显示，这些最新模型在根据上下文进行更精准物品识别和分类方面取得显著进步，尤其是在复杂情境下的表现优于以往模型。但模型在特殊情况下如混合材料、严重污染等场景下依然存在不足。

Conclusion: 文章认为，随着情境感知能力提升，此类多模态大模型有望显著改善公众回收实践，但要实现全面高效的智能分类，仍需进一步模型优化。

Abstract: While the importance of efficient recycling is widely acknowledged, accurately determining the recyclability of items and their proper disposal remains a complex task for the general public. In this study, we explore the application of cutting-edge vision-language models (GPT-4o, GPT-4o-mini, and Claude 3.5) for predicting the recyclability of commonly disposed items. Utilizing a curated dataset of images, we evaluated the models' ability to match objects to appropriate recycling bins, including assessing whether the items could physically fit into the available bins. Additionally, we investigated the models' performance across several challenging scenarios: (i) adjusting predictions based on location-specific recycling guidelines; (ii) accounting for contamination or structural damage; and (iii) handling objects composed of multiple materials. Our findings highlight the significant advancements in contextual understanding offered by these models compared to previous iterations, while also identifying areas where they still fall short. The continued refinement of context-aware models is crucial for enhancing public recycling practices and advancing environmental sustainability.

</details>


### [11] [Clean-GS: Semantic Mask-Guided Pruning for 3D Gaussian Splatting](https://arxiv.org/abs/2601.00913)
*Subhankar Mishra*

Main category: cs.CV

TL;DR: 本文提出Clean-GS方法，有效去除3D Gaussian Splatting重建中大量无用漂浮高斯点，大幅压缩模型体积，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting可以高质量重建三维场景，但会生成大量无用漂浮高斯点（floaters），造成物体遮挡和模型冗大，影响在带宽受限环境如Web/AR/VR等的实用性。

Method: Clean-GS结合稀疏语义分割掩码，通过三步处理：（1）基于投影的白名单空间筛选，（2）深度缓冲的颜色验证，（3）邻域离群点去除，有效过滤目标物体之外的高斯点。核心创新是仅需极少语义掩码（3张，约1%视角），利用语义信息而非全局重要性度量来进行高斯点剔除。

Result: Clean-GS在Tanks and Temples数据集上，将3DGS模型体积从125MB压缩到47MB，渲染质量基本不变，实现了60-80%压缩率，同时突出保留目标物体，提升3DGS模型在Web和AR/VR领域的部署可行性。

Conclusion: Clean-GS极大优化了3DGS的模型紧凑度与场景整洁性，且无需大量标注，在保持视觉质量的情况下，为实际应用（尤其带宽敏感场景）提供了有效解决方案，具备实用与推广价值。

Abstract: 3D Gaussian Splatting produces high-quality scene reconstructions but generates hundreds of thousands of spurious Gaussians (floaters) scattered throughout the environment. These artifacts obscure objects of interest and inflate model sizes, hindering deployment in bandwidth-constrained applications. We present Clean-GS, a method for removing background clutter and floaters from 3DGS reconstructions using sparse semantic masks. Our approach combines whitelist-based spatial filtering with color-guided validation and outlier removal to achieve 60-80\% model compression while preserving object quality. Unlike existing 3DGS pruning methods that rely on global importance metrics, Clean-GS uses semantic information from as few as 3 segmentation masks (1\% of views) to identify and remove Gaussians not belonging to the target object. Our multi-stage approach consisting of (1) whitelist filtering via projection to masked regions, (2) depth-buffered color validation, and (3) neighbor-based outlier removal isolates monuments and objects from complex outdoor scenes. Experiments on Tanks and Temples show that Clean-GS reduces file sizes from 125MB to 47MB while maintaining rendering quality, making 3DGS models practical for web deployment and AR/VR applications. Our code is available at https://github.com/smlab-niser/clean-gs

</details>


### [12] [Four-Stage Alzheimer's Disease Classification from MRI Using Topological Feature Extraction, Feature Selection, and Ensemble Learning](https://arxiv.org/abs/2601.00918)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: 本文提出了TDA-Alz框架，通过拓扑数据分析（TDA）和集成学习方法，实现了阿尔茨海默病脑MRI分期的高效分类，准确率达98.19%，AUC达99.75%，无需数据增强和大规模算力， interpretability强。


<details>
  <summary>Details</summary>
Motivation: 在MRI数据有限和模型可解释性需求下，现有深度学习方法对AD分期存在难题：依赖大数据量、不透明、需高计算资源。因此亟需高效、可解释且在小数据下表现优秀的新方法。

Method: 舍弃深度卷积网络和数据增强，采用拓扑数据分析提取描述脑MRI结构模式的拓扑特征，经特征选择后，用集成学习进行四分类。

Result: 在OASIS-1 MRI数据集上，达到98.19%准确率，AUC为99.75%，优于或不逊于现有SOTA深度学习方法，无需预训练网络与高算力。

Conclusion: TDA-Alz是一种轻量、高效、可解释的MRI阿尔茨海默分期分类新方法，为现实临床决策支持系统提供有力方案，有望实际应用。

Abstract: Accurate and efficient classification of Alzheimer's disease (AD) severity from brain magnetic resonance imaging (MRI) remains a critical challenge, particularly when limited data and model interpretability are of concern. In this work, we propose TDA-Alz, a novel framework for four-stage Alzheimer's disease severity classification (non-demented, moderate dementia, mild, and very mild) using topological data analysis (TDA) and ensemble learning. Instead of relying on deep convolutional architectures or extensive data augmentation, our approach extracts topological descriptors that capture intrinsic structural patterns of brain MRI, followed by feature selection to retain the most discriminative topological features. These features are then classified using an ensemble learning strategy to achieve robust multiclass discrimination.
  Experiments conducted on the OASIS-1 MRI dataset demonstrate that the proposed method achieves an accuracy of 98.19% and an AUC of 99.75%, outperforming or matching state-of-the-art deep learning--based methods reported on OASIS and OASIS-derived datasets. Notably, the proposed framework does not require data augmentation, pretrained networks, or large-scale computational resources, making it computationally efficient and fast compared to deep neural network approaches. Furthermore, the use of topological descriptors provides greater interpretability, as the extracted features are directly linked to the underlying structural characteristics of brain MRI rather than opaque latent representations. These results indicate that TDA-Alz offers a powerful, lightweight, and interpretable alternative to deep learning models for MRI-based Alzheimer's disease severity classification, with strong potential for real-world clinical decision-support systems.

</details>


### [13] [Application of deep learning techniques in non-contrast computed tomography pulmonary angiogram for pulmonary embolism diagnosis](https://arxiv.org/abs/2601.00925)
*I-Hsien Ting,Yi-Jun Tseng,Yu-Sheng Lin*

Main category: cs.CV

TL;DR: 该研究利用深度学习，通过3D卷积神经网络，对未使用对比剂的肺部CT图像进行肺栓塞自动分类，实现了85%的准确率和0.84的AUC，证明了此方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统肺栓塞诊断需要用对比剂增强的CT肺动脉造影，但对比剂会增加肾损伤风险，且需要等待时间，可能耽误急性患者的最佳治疗时机。因此亟需能在无对比剂CT下自动判读肺栓塞的新方法。

Method: 本研究采用三维卷积神经网络，对无对比剂的肺部CT图像进行训练和分类，用于区分是否存在肺栓塞。

Result: 该深度学习模型在未增强CT图像的肺栓塞分类上，取得了85%的准确率和0.84的AUC，显示出良好的诊断性能。

Conclusion: 无需对比剂的深度学习CT图像分析模型在肺栓塞诊断中具有可行性，有望为肾功能不良患者及急性患者提供替代性、安全性更高的诊断方案。

Abstract: Pulmonary embolism is a life-threatening disease, early detection and treatment can significantly reduce mortality. In recent years, many studies have been using deep learning in the diagnosis of pulmonary embolism with contrast medium computed tomography pulmonary angiography, but the contrast medium is likely to cause acute kidney injury in patients with pulmonary embolism and chronic kidney disease, and the contrast medium takes time to work, patients with acute pulmonary embolism may miss the golden treatment time.
  This study aims to use deep learning techniques to automatically classify pulmonary embolism in CT images without contrast medium by using a 3D convolutional neural network model. The deep learning model used in this study had a significant impact on the pulmonary embolism classification of computed tomography images without contrast with 85\% accuracy and 0.84 AUC, which confirms the feasibility of the model in the diagnosis of pulmonary embolism.

</details>


### [14] [Analyzing the Shopping Journey: Computing Shelf Browsing Visits in a Physical Retail Store](https://arxiv.org/abs/2601.00928)
*Luis Yoichi Morales,Francesco Zanlungo,David M. Woollard*

Main category: cs.CV

TL;DR: 本文提出了一种通过3D追踪和摄像头数据分析顾客在实体店中的“货架访问”行为的算法，可用于理解顾客意图。该算法在两个不同商店的数据集上进行了标注和校准，验证了模型的泛化能力，并探讨了输出结果对于零售管理和人机交互的意义。


<details>
  <summary>Details</summary>
Motivation: 受机器人在零售环境中部署面临的挑战启发，作者希望通过更好地自动理解顾客在实体店内的行为意图，提升智能零售系统和机器人应用的实用性。

Method: 提出并实现了一种利用机器视觉的3D追踪和天花板摄像头数据，自动检测顾客“货架访问”行为的算法。算法在不同商店和不同数据集上进行了独立的人为标注与校准，并在未参与校准的数据上测试其泛化性能。

Result: 算法在不同商店环境下都能准确识别顾客的浏览行为。模型进一步被用于分析大量顾客的浏览模式，并关联分析了浏览和实际购买行为之间的关系。

Conclusion: 所提出方法能较好地泛化到非校准场景，准确识别并分析顾客浏览行为，对零售管理及人机交互（如机器人辅助购物）等领域具有潜在应用价值。

Abstract: Motivated by recent challenges in the deployment of robots into customer-facing roles within retail, this work introduces a study of customer activity in physical stores as a step toward autonomous understanding of shopper intent. We introduce an algorithm that computes shoppers' ``shelf visits'' -- capturing their browsing behavior in the store. Shelf visits are extracted from trajectories obtained via machine vision-based 3D tracking and overhead cameras. We perform two independent calibrations of the shelf visit algorithm, using distinct sets of trajectories (consisting of 8138 and 15129 trajectories), collected in different stores and labeled by human reviewers. The calibrated models are then evaluated on trajectories held out of the calibration process both from the same store on which calibration was performed and from the other store. An analysis of the results shows that the algorithm can recognize customers' browsing activity when evaluated in an environment different from the one on which calibration was performed. We then use the model to analyze the customers' ``browsing patterns'' on a large set of trajectories and their relation to actual purchases in the stores. Finally, we discuss how shelf browsing information could be used for retail planning and in the domain of human-robot interaction scenarios.

</details>


### [15] [ShadowGS: Shadow-Aware 3D Gaussian Splatting for Satellite Imagery](https://arxiv.org/abs/2601.00939)
*Feng Luo,Hongbo Pan,Xiang Yang,Baoyu Jiang,Fengqing Liu,Tao Huang*

Main category: cs.CV

TL;DR: 本文提出了ShadowGS，一种基于3D Gaussian Splatting（3DGS）的新颖框架，能高效处理多时相卫星影像中的阴影不一致问题，并提升3D重建质量。


<details>
  <summary>Details</summary>
Motivation: 多时相卫星图像极易受到不同光照条件导致的阴影不一致的影响，而这些阴影严重影响了3D重建的准确性。现有方法难以有效解耦阴影并保持几何一致性，迫切需要新的方案。

Method: 作者提出ShadowGS，通过引入遥感领域的物理渲染方程与高效的射线行进技术，实现了对3D阴影的一致性建模。方法还解耦了场景中的光照与表观属性，并设计了阴影一致性约束以及基于阴影图的先验，提升了在稀疏视图条件下的表现。

Result: 大量实验表明，ShadowGS在阴影解耦精度、3D重建准确性以及新视角合成质量方面都优于当前先进方法，且训练时间仅需几分钟。在不同类型的卫星输入（RGB、高分融合、稀疏视图）中均表现出色。

Conclusion: ShadowGS能有效解决多时相卫星影像阴影不一致问题，大幅提升了3D重建质量和效率，对遥感影像的后续处理和分析具有重要意义。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel paradigm for 3D reconstruction from satellite imagery. However, in multi-temporal satellite images, prevalent shadows exhibit significant inconsistencies due to varying illumination conditions. To address this, we propose ShadowGS, a novel framework based on 3DGS. It leverages a physics-based rendering equation from remote sensing, combined with an efficient ray marching technique, to precisely model geometrically consistent shadows while maintaining efficient rendering. Additionally, it effectively disentangles different illumination components and apparent attributes in the scene. Furthermore, we introduce a shadow consistency constraint that significantly enhances the geometric accuracy of 3D reconstruction. We also incorporate a novel shadow map prior to improve performance with sparse-view inputs. Extensive experiments demonstrate that ShadowGS outperforms current state-of-the-art methods in shadow decoupling accuracy, 3D reconstruction precision, and novel view synthesis quality, with only a few minutes of training. ShadowGS exhibits robust performance across various settings, including RGB, pansharpened, and sparse-view satellite inputs.

</details>


### [16] [Learning to Segment Liquids in Real-world Images](https://arxiv.org/abs/2601.00940)
*Jonas Li,Michelle Li,Luke Liu,Heng Fan*

Main category: cs.CV

TL;DR: 本文提出了一个包含多种液体（如水、酒、药液）的大型数据集LQDS，并设计了新型液体分割模型LQDM，在分割任务上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 在机器人与现实世界交互中，液体的分割与识别对于安全和准确操作具有重要意义。然而，液体因外观多变、可透明或反光、背景干扰大，分割难度极高，目前相关研究和数据集较为稀缺。

Method: 作者建立了一个包含5000张真实场景液体图像（分成14类）的数据集LQDS，并提出了一种新模型LQDM。LQDM在主分割分支外设有专门的边界感知分支，两者通过交叉注意力机制互相融合，以增强分割性能。

Result: 在LQDS测试集上的大量实验表明，LQDM分割性能优于现有主流分割方法，建立了液体语义分割领域的新基线。

Conclusion: LQDS数据集和LQDM模型为液体场景下的视觉分割任务提供了有效的研究平台和强大工具，有力促进了机器人在实际环境中安全准确地处理液体的能力。

Abstract: Different types of liquids such as water, wine and medicine appear in all aspects of daily life. However, limited attention has been given to the task, hindering the ability of robots to avoid or interact with liquids safely. The segmentation of liquids is difficult because liquids come in diverse appearances and shapes; moreover, they can be both transparent or reflective, taking on arbitrary objects and scenes from the background or surroundings. To take on this challenge, we construct a large-scale dataset of liquids named LQDS consisting of 5000 real-world images annotated into 14 distinct classes, and design a novel liquid detection model named LQDM, which leverages cross-attention between a dedicated boundary branch and the main segmentation branch to enhance segmentation predictions. Extensive experiments demonstrate the effectiveness of LQDM on the test set of LQDS, outperforming state-of-the-art methods and establishing a strong baseline for the semantic segmentation of liquids.

</details>


### [17] [PhyEduVideo: A Benchmark for Evaluating Text-to-Video Models for Physics Education](https://arxiv.org/abs/2601.00943)
*Megha Mariam K. M,Aditya Arun,Zakaria Laskar,C. V. Jawahar*

Main category: cs.CV

TL;DR: 本论文提出了一个针对物理教育领域的文本生成视频（Text-to-Video, T2V）模型评测基准，用于系统性测试T2V模型通过视频传达物理核心概念的能力。当前模型生成视频的视觉效果良好，但在概念准确性上仍有不足，尤其是在电磁学和热力学等抽象领域。该基准和代码已开源。


<details>
  <summary>Details</summary>
Motivation: 生成式AI，尤其是T2V技术，有潜力自动、规模化生成直观且吸引人的科学教育视频，但当前缺乏专门用于衡量其教育实际效果的评测工具。本文旨在填补这一空白，推动T2V在物理教育中的应用。

Method: 作者构建了一个新的评测基准，将物理核心概念分解为具体教学点，并为每个点设计专用于视频生成的提示语。通过这些提示，评估现有T2V模型生成的视频是否准确表达了物理教学内容。

Result: 评估结果显示，T2V模型在力学、流体和光学等领域有较好表现，生成视频画面流畅、运动连贯、闪烁少。但在电磁学和热力学等抽象领域，模型难以准确表达相关概念。整体上，模型视觉效果优于知识准确性。

Conclusion: 当前T2V模型在视频生成的视觉质量上已有显著提升，但教学内容的概念准确性仍需改进。该基准有助于推动T2V在科学教育领域真正实现规模化、个性化和高质量的应用。

Abstract: Generative AI models, particularly Text-to-Video (T2V) systems, offer a promising avenue for transforming science education by automating the creation of engaging and intuitive visual explanations. In this work, we take a first step toward evaluating their potential in physics education by introducing a dedicated benchmark for explanatory video generation. The benchmark is designed to assess how well T2V models can convey core physics concepts through visual illustrations. Each physics concept in our benchmark is decomposed into granular teaching points, with each point accompanied by a carefully crafted prompt intended for visual explanation of the teaching point. T2V models are evaluated on their ability to generate accurate videos in response to these prompts. Our aim is to systematically explore the feasibility of using T2V models to generate high-quality, curriculum-aligned educational content-paving the way toward scalable, accessible, and personalized learning experiences powered by AI. Our evaluation reveals that current models produce visually coherent videos with smooth motion and minimal flickering, yet their conceptual accuracy is less reliable. Performance in areas such as mechanics, fluids, and optics is encouraging, but models struggle with electromagnetism and thermodynamics, where abstract interactions are harder to depict. These findings underscore the gap between visual quality and conceptual correctness in educational video generation. We hope this benchmark helps the community close that gap and move toward T2V systems that can deliver accurate, curriculum-aligned physics content at scale. The benchmark and accompanying codebase are publicly available at https://github.com/meghamariamkm/PhyEduVideo.

</details>


### [18] [Deep Clustering with Associative Memories](https://arxiv.org/abs/2601.00963)
*Bishwajit Saha,Dmitry Krotov,Mohammed J. Zaki,Parikshit Ram*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度聚类方法DCAM，通过引入能量函数和联想记忆机制，将特征表示学习和聚类更紧密地结合到一个统一目标中，实现了更优的聚类效果。


<details>
  <summary>Details</summary>
Motivation: 深度聚类常将表示学习和聚类分开处理，而聚类的离散性导致两者结合不紧密，影響了聚类效果。作者希望通过新的优化目标，使两者协同优化，提升聚类质量。

Method: 提出了一种基于能量动力学和联想记忆的损失函数，并在深度学习框架中将其应用于联合表示学习与聚类，形成了DCAM方法。该方法适用于不同的神经网络结构和数据类型。

Result: 在多种网络结构（如卷积、残差、全连接）和数据类型（如图像、文本）上的实验结果显示，DCAM能显著提升聚类质量。

Conclusion: DCAM方法有效实现了表示学习与聚类目标的统一优化，相较于传统深度聚类方法，在多任务和多结构设置下都表现出更优的聚类效果。

Abstract: Deep clustering - joint representation learning and latent space clustering - is a well studied problem especially in computer vision and text processing under the deep learning framework. While the representation learning is generally differentiable, clustering is an inherently discrete optimization task, requiring various approximations and regularizations to fit in a standard differentiable pipeline. This leads to a somewhat disjointed representation learning and clustering. In this work, we propose a novel loss function utilizing energy-based dynamics via Associative Memories to formulate a new deep clustering method, DCAM, which ties together the representation learning and clustering aspects more intricately in a single objective. Our experiments showcase the advantage of DCAM, producing improved clustering quality for various architecture choices (convolutional, residual or fully-connected) and data modalities (images or text).

</details>


### [19] [A Deep Learning Approach for Automated Skin Lesion Diagnosis with Explainable AI](https://arxiv.org/abs/2601.00964)
*Md. Maksudul Haque,Rahnuma Akter,A S M Ahsanul Sarkar Akib,Abdul Hasib*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的多类别皮肤病变分类系统，在HAM10000数据集上实现了高精度和高可解释性的分类。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌是一种常见且危险的癌症，早期、准确诊断对于提高生存率至关重要。现有自动诊断方法在多类别皮肤病变识别、模型解释性等方面存在不足。

Method: 作者结合高质量数据均衡、数据增强，采用融合了通道注意力的EfficientNetV2-L架构，并引入三阶段渐进式学习。同时，利用Grad-CAM和显著性图解释模型输出。

Result: 系统在总准确率达到91.15%，宏F1为85.45%，微平均AUC为99.33%。在7类病变中均表现优异，尤其是黑色素瘤和黑色素细胞痣的识别。

Conclusion: 该方法显著提升了多类别皮肤病变的自动分类性能，并通过可解释性技术增强了模型诊断的透明度和可信度，有助于在临床上推广应用。

Abstract: Skin cancer is also one of the most common and dangerous types of cancer in the world that requires timely and precise diagnosis. In this paper, a deep-learning architecture of the multi-class skin lesion classification on the HAM10000 dataset will be described. The system suggested combines high-quality data balancing methods, large-scale data augmentation, hybridized EfficientNetV2-L framework with channel attention, and a three-stage progressive learning approach. Moreover, we also use explainable AI (XAI) techniques such as Grad-CAM and saliency maps to come up with intelligible visual representations of model predictions. Our strategy is with a total accuracy of 91.15 per cent, macro F1 of 85.45\% and micro-average AUC of 99.33\%. The model has shown high performance in all the seven lesion classes with specific high performance of melanoma and melanocytic nevi. In addition to enhancing diagnostic transparency, XAI also helps to find out the visual characteristics that cause the classifications, which enhances clinical trustworthiness.

</details>


### [20] [Few-Shot Video Object Segmentation in X-Ray Angiography Using Local Matching and Spatio-Temporal Consistency Loss](https://arxiv.org/abs/2601.00988)
*Lin Xi,Yingliang Ma,Xiahai Zhuang*

Main category: cs.CV

TL;DR: 提出了一种新的FSVOS模型，通过局部匹配策略提高多目标视频分割的准确性和泛化能力，并发布了针对X射线血管造影影像的多目标分割新数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的局部采样方法依赖于im2col等低效实现或硬件相关的CUDA内核，兼容性和效率存在局限。希望提出一种高效率且更灵活跨硬件的平台无关方法。

Method: 1. 采用方向为基础的局部采样，形成非参数的动态采样机制，减少对特定硬件和复杂运算的依赖。2. 加入监督的时空对比学习，增强跨帧的特征一致性。3. 推出新的多目标分割数据集MOSXAV。

Result: 在CADICA、XACV和MOSXAV数据集上，FSVOS在分割精度和泛化能力(包括已见和未见类别)方面，均优于当前主流方法。

Conclusion: 所提模型灵活性更强，可广泛应用于临床场景，具有更好的跨平台能力和实际应用价值。

Abstract: We introduce a novel FSVOS model that employs a local matching strategy to restrict the search space to the most relevant neighboring pixels. Rather than relying on inefficient standard im2col-like implementations (e.g., spatial convolutions, depthwise convolutions and feature-shifting mechanisms) or hardware-specific CUDA kernels (e.g., deformable and neighborhood attention), which often suffer from limited portability across non-CUDA devices, we reorganize the local sampling process through a direction-based sampling perspective. Specifically, we implement a non-parametric sampling mechanism that enables dynamically varying sampling regions. This approach provides the flexibility to adapt to diverse spatial structures without the computational costs of parametric layers and the need for model retraining. To further enhance feature coherence across frames, we design a supervised spatio-temporal contrastive learning scheme that enforces consistency in feature representations. In addition, we introduce a publicly available benchmark dataset for multi-object segmentation in X-ray angiography videos (MOSXAV), featuring detailed, manually labeled segmentation ground truth. Extensive experiments on the CADICA, XACV, and MOSXAV datasets show that our proposed FSVOS method outperforms current state-of-the-art video segmentation methods in terms of segmentation accuracy and generalization capability (i.e., seen and unseen categories). This work offers enhanced flexibility and potential for a wide range of clinical applications.

</details>


### [21] [UnrealPose: Leveraging Game Engine Kinematics for Large-Scale Synthetic Human Pose Data](https://arxiv.org/abs/2601.00991)
*Joshua Kawaguchi,Saad Manzur,Emily Gao Wang,Maitreyi Sinha,Bryan Vela,Yunxi Wang,Brandon Vela,Wayne B. Hayes*

Main category: cs.CV

TL;DR: UnrealPose-Gen是一个基于Unreal Engine 5的高质量3D人体姿态数据生成管道，作者用它构建了包含百万帧的UnrealPose-1M数据集，并同时开源工具与数据集。


<details>
  <summary>Details</summary>
Motivation: 准确且多样的3D人体姿态数据很难获取且受限于昂贵的摄影棚环境，而现有野外数据集又缺乏高质量的标注。

Method: 提出了UnrealPose-Gen管道，利用Unreal Engine 5的Movie Render Queue离线高质量渲染，生成带有详细标注的合成数据，包括3D关节点、2D投影、遮挡和可见性标记、包围框以及相机参数。

Result: 利用该工具制作了UnrealPose-1M数据集，包含百万帧，来自不同场景、动作、人物和视角，并在4项任务上进行合成与真实间的表现验证。

Conclusion: 作者将数据生成工具与丰富的数据集开源，为人类姿态识别领域提供了高质量的合成数据资源，便于后续学者扩展和应用。

Abstract: Diverse, accurately labeled 3D human pose data is expensive and studio-bound, while in-the-wild datasets lack known ground truth. We introduce UnrealPose-Gen, an Unreal Engine 5 pipeline built on Movie Render Queue for high-quality offline rendering. Our generated frames include: (i) 3D joints in world and camera coordinates, (ii) 2D projections and COCO-style keypoints with occlusion and joint-visibility flags, (iii) person bounding boxes, and (iv) camera intrinsics and extrinsics. We use UnrealPose-Gen to present UnrealPose-1M, an approximately one million frame corpus comprising eight sequences: five scripted "coherent" sequences spanning five scenes, approximately 40 actions, and five subjects; and three randomized sequences across three scenes, approximately 100 actions, and five subjects, all captured from diverse camera trajectories for broad viewpoint coverage. As a fidelity check, we report real-to-synthetic results on four tasks: image-to-3D pose, 2D keypoint detection, 2D-to-3D lifting, and person detection/segmentation. Though time and resources constrain us from an unlimited dataset, we release the UnrealPose-1M dataset, as well as the UnrealPose-Gen pipeline to support third-party generation of human pose data.

</details>


### [22] [WildIng: A Wildlife Image Invariant Representation Model for Geographical Domain Shift](https://arxiv.org/abs/2601.00993)
*Julian D. Santamaria,Claudia Isaza,Jhony H. Giraldo*

Main category: cs.CV

TL;DR: 本论文针对深度学习模型在跨地域野生动物图像识别中表现下滑的问题，提出结合文本描述与图像特征的新方法WildIng，并显著提升模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于相机陷阱的野生动物监测，在不同地理区域间存在显著的数据分布差异，导致深度学习模型难以跨地域泛化，影响生态研究效率。现有模型主要依赖图像特征，难以适应背景、光照及环境变化的迁移。

Method: 提出WildIng模型，将动物物种的文本描述与图像特征融合，构建对地理域转移更鲁棒的表示。通过结合结构化语义信息提高种类识别的一致性，并在BioCLIP等基础模型上应用该方法，系统评估其跨地域数据集上的表现。

Result: WildIng方法在跨地域识别实验中，能将BioCLIP模型在地理域转移下的准确率提升30%。例如，传统模型在非洲数据训练对比美洲测试时准确率降至16.17%，而WildIng显著改善了这一问题。

Conclusion: WildIng有效缓解了区域迁移导致的性能下降，通过融合文本和图像特征提升了野生动物基础模型的泛化能力，为野生动物监测和研究提供了更稳健的解决方案。

Abstract: Wildlife monitoring is crucial for studying biodiversity loss and climate change. Camera trap images provide a non-intrusive method for analyzing animal populations and identifying ecological patterns over time. However, manual analysis is time-consuming and resource-intensive. Deep learning, particularly foundation models, has been applied to automate wildlife identification, achieving strong performance when tested on data from the same geographical locations as their training sets. Yet, despite their promise, these models struggle to generalize to new geographical areas, leading to significant performance drops. For example, training an advanced vision-language model, such as CLIP with an adapter, on an African dataset achieves an accuracy of 84.77%. However, this performance drops significantly to 16.17% when the model is tested on an American dataset. This limitation partly arises because existing models rely predominantly on image-based representations, making them sensitive to geographical data distribution shifts, such as variation in background, lighting, and environmental conditions. To address this, we introduce WildIng, a Wildlife image Invariant representation model for geographical domain shift. WildIng integrates text descriptions with image features, creating a more robust representation to geographical domain shifts. By leveraging textual descriptions, our approach captures consistent semantic information, such as detailed descriptions of the appearance of the species, improving generalization across different geographical locations. Experiments show that WildIng enhances the accuracy of foundation models such as BioCLIP by 30% under geographical domain shift conditions. We evaluate WildIng on two datasets collected from different regions, namely America and Africa. The code and models are publicly available at https://github.com/Julian075/CATALOG/tree/WildIng.

</details>


### [23] [DVGBench: Implicit-to-Explicit Visual Grounding Benchmark in UAV Imagery with Large Vision-Language Models](https://arxiv.org/abs/2601.00998)
*Yue Zhou,Jue Chen,Zilun Zhang,Penghui Huang,Ran Ding,Zhentao Zou,PengFei Gao,Yuchen Wei,Ke Li,Xue Yang,Xue Jiang,Hongxin Yang,Jonathan Li*

Main category: cs.CV

TL;DR: 本文提出了一个专注于无人机遥感隐式视觉指引（VG）任务的新数据集DVGBench，并设计了一种结合隐式到显式链式思维的新模型DroneVG-R1。实验发现主流大模型在隐式VG任务上推理能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有遥感视觉指引数据集多依赖显式表达，不能覆盖需要场景和领域知识的隐式指引任务，限制了大模型在实际复杂环境中的表现。因此，作者希望推动遥感大模型处理更具挑战性的隐式指引任务。

Method: 1）建立DVGBench数据集，涵盖交通、灾害、安全、体育、社交、生产六大场景，每个目标都配有显式与隐式查询；2）提出DroneVG-R1模型，创新性地整合隐式到显式的链式思维（I2E-CoT）机制，通过强化学习引导模型将隐式指引转化为更容易识别的显式信息，从而提升指引表现。

Result: 评测显示，目前主流的大型视觉语言模型在DVGBench上的显式任务表现尚可，但在隐式任务上推理能力严重不足。DroneVG-R1利用I2E-CoT增强隐式推理，但提升空间仍然较大。

Conclusion: 本文为遥感领域引入了具有挑战性的隐式VG基准与创新建模方法，指出大模型在推理与隐式指引方面存在短板，为未来提升遥感视觉语言模型的推理能力和实际应用价值指明了方向。

Abstract: Remote sensing (RS) large vision-language models (LVLMs) have shown strong promise across visual grounding (VG) tasks. However, existing RS VG datasets predominantly rely on explicit referring expressions-such as relative position, relative size, and color cues-thereby constraining performance on implicit VG tasks that require scenario-specific domain knowledge. This article introduces DVGBench, a high-quality implicit VG benchmark for drones, covering six major application scenarios: traffic, disaster, security, sport, social activity, and productive activity. Each object provides both explicit and implicit queries. Based on the dataset, we design DroneVG-R1, an LVLM that integrates the novel Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within a reinforcement learning paradigm. This enables the model to take advantage of scene-specific expertise, converting implicit references into explicit ones and thus reducing grounding difficulty. Finally, an evaluation of mainstream models on both explicit and implicit VG tasks reveals substantial limitations in their reasoning capabilities. These findings provide actionable insights for advancing the reasoning capacity of LVLMs for drone-based agents. The code and datasets will be released at https://github.com/zytx121/DVGBench

</details>


### [24] [Lightweight Channel Attention for Efficient CNNs](https://arxiv.org/abs/2601.01002)
*Prem Babu Kanaparthi,Tulasi Venkata Sri Varshini Padamata*

Main category: cs.CV

TL;DR: 本文对三种通道注意力机制（SE、ECA、LCA）在不同CNN架构下进行了系统对比，发现新提出的LCA模块在参数、速度和准确率上均表现出较好平衡，适用于资源受限的环境。


<details>
  <summary>Details</summary>
Motivation: 现有通道注意力机制虽然提升了CNN性能，但各种注意力模块在效率与准确率之间的权衡尚未被深入研究，尤其是在实际部署对资源有限的场景下缺乏系统的对比与分析。

Method: 作者在ResNet18和MobileNetV2上，设计实验比较了SE、ECA以及新提出的LCA模块（利用分组自适应一维卷积，进一步减小参数量同时维持注意力效果），并在CIFAR-10数据集上评测其准确率、计算量（FLOPs）、参数数量和GPU推理延迟。

Result: LCA在ResNet18上达到94.68%的准确率，MobileNetV2上为93.10%，参数量和推理速度与ECA相当，效果均优于传统的SE模块。同时详细报告了各模块在多种资源限制下的表现。

Conclusion: LCA以极低的参数和计算成本实现优良性能，是资源有限场景下注意力增强CNN的实用选择。

Abstract: Attention mechanisms have become integral to modern convolutional neural networks (CNNs), delivering notable performance improvements with minimal computational overhead. However, the efficiency accuracy trade off of different channel attention designs remains underexplored. This work presents an empirical study comparing Squeeze and Excitation (SE), Efficient Channel Attention (ECA), and a proposed Lite Channel Attention (LCA) module across ResNet 18 and MobileNetV2 architectures on CIFAR 10. LCA employs adaptive one dimensional convolutions with grouped operations to reduce parameter usage while preserving effective attention behavior. Experimental results show that LCA achieves competitive accuracy, reaching 94.68 percent on ResNet 18 and 93.10 percent on MobileNetV2, while matching ECA in parameter efficiency and maintaining favorable inference latency. Comprehensive benchmarks including FLOPs, parameter counts, and GPU latency measurements are provided, offering practical insights for deploying attention enhanced CNNs in resource constrained environments.

</details>


### [25] [Decoupling Amplitude and Phase Attention in Frequency Domain for RGB-Event based Visual Object Tracking](https://arxiv.org/abs/2601.01022)
*Shiao Wang,Xiao Wang,Haonan Zhao,Jiarui Xu,Bo Jiang,Lin Zhu,Xin Zhao,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的RGB-事件视觉目标跟踪方法，通过在频域对事件高频信息进行早期融合，并利用运动引导的空间稀疏化，有效提升了跟踪性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的RGB-事件视觉目标跟踪方法大多采用传统的特征级融合，无法充分利用事件相机的高动态范围和运动敏感等独特优势，且对低信息区域一视同仁，造成不必要的计算开销。

Method: 作者提出在频域进行早期融合。具体方法是将RGB和事件信号通过快速傅里叶变换（FFT）映射到频域，解耦其幅值和相位成分，通过幅值与相位注意力机制将事件高频信息有选择性地融合到RGB模态中。同时，设计了运动引导的空间稀疏化模块，根据事件相机的运动敏感性过滤低信息区域，突出目标相关特征。稀疏特征最终输入主干网络进行学习，跟踪头输出目标位置。

Result: 在FE108、FELT和COESOT这三大RGB-事件跟踪基准数据集上进行了大量实验，结果表明所提方法在性能和效率上均有显著提升。

Conclusion: 该方法能够高效地利用事件相机的独特信息，提升跟踪效果并减少计算量，具有良好的应用前景。源码将开源，便于后续研究和应用。

Abstract: Existing RGB-Event visual object tracking approaches primarily rely on conventional feature-level fusion, failing to fully exploit the unique advantages of event cameras. In particular, the high dynamic range and motion-sensitive nature of event cameras are often overlooked, while low-information regions are processed uniformly, leading to unnecessary computational overhead for the backbone network. To address these issues, we propose a novel tracking framework that performs early fusion in the frequency domain, enabling effective aggregation of high-frequency information from the event modality. Specifically, RGB and event modalities are transformed from the spatial domain to the frequency domain via the Fast Fourier Transform, with their amplitude and phase components decoupled. High-frequency event information is selectively fused into RGB modality through amplitude and phase attention, enhancing feature representation while substantially reducing backbone computation. In addition, a motion-guided spatial sparsification module leverages the motion-sensitive nature of event cameras to capture the relationship between target motion cues and spatial probability distribution, filtering out low-information regions and enhancing target-relevant features. Finally, a sparse set of target-relevant features is fed into the backbone network for learning, and the tracking head predicts the final target position. Extensive experiments on three widely used RGB-Event tracking benchmark datasets, including FE108, FELT, and COESOT, demonstrate the high performance and efficiency of our method. The source code of this paper will be released on https://github.com/Event-AHU/OpenEvTracking

</details>


### [26] [ITSELF: Attention Guided Fine-Grained Alignment for Vision-Language Retrieval](https://arxiv.org/abs/2601.01024)
*Tien-Huy Nguyen,Huu-Loc Tran,Thanh Duc Ngo*

Main category: cs.CV

TL;DR: 本文提出了一种名为ITSELF的注意力引导隐式局部对齐框架，用于提升基于文本的人体检索任务的精度，并展示了在多个基准数据集上的优秀表现和良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（VLMs）在文本图像细粒度对齐上易产生错误对齐，部分因捷径学习和伪相关等问题，并且在注入先验知识时可能导致模态内部结构扭曲。因此亟需一种新方法能有效捕捉图片与文本间的本地细节对应关系，且无需额外监督。

Method: 核心方法是ITSELF框架，包括：1）GRAB：利用模型自身的注意力权重选择高显著性token，建立显著性库，并在此上施加局部目标以学习图文局部对应关系；2）MARS：多层注意力融合及top-k多样性筛选，保证被选token的可靠性和多样性；3）ATS：自适应token调度器，逐步由全局转向局部，训练初期保留上下文，后期聚焦区分性细节。

Result: 在三个主流TBPS数据集上，ITSELF方法取得了SOTA的检索性能，并展现出优秀的跨数据集泛化能力。无需额外的先验知识或监督，实验验证了提出方法的有效性和鲁棒性。

Conclusion: ITSELF框架能够克服现有VLM在文本-图像细粒度匹配中的主要问题，通过注意力引导方式实现无监督的优质局部对齐，提高了文本检索人体任务的精度与泛化能力，对未来VLM相关任务也有潜在启发作用。

Abstract: Vision Language Models (VLMs) have rapidly advanced and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our finding that encoder attention surfaces spatially precise evidence from the earliest training epochs, and to alleviate these issues, we introduceITSELF, an attention-guided framework for implicit local alignment. At its core, Guided Representation with Attentive Bank (GRAB) converts the model's own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks showstate-of-the-art performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision. Our project is publicly available at https://trhuuloc.github.io/itself

</details>


### [27] [Enhanced Leukemic Cell Classification Using Attention-Based CNN and Data Augmentation](https://arxiv.org/abs/2601.01026)
*Douglas Costa Braga,Daniel Oliveira Dantas*

Main category: cs.CV

TL;DR: 提出了一套基于深度学习的可复现白血病细胞分类流程，利用高效神经网络和注意力机制，在大规模临床数据集上实现了当前最优性能，并大大减少了计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 白血病（ALL）是常见的儿童肿瘤，传统人工显微镜诊断存在主观差异和效率低下等问题。研究旨在开发高效自动化工具提高诊断准确性、减少人工依赖、加速临床流程。

Method: 系统集成了基于EfficientNetV2-B3的卷积神经网络，结合Squeeze-and-Excitation注意力机制。采用数据增强、Focal Loss处理类别不平衡、患者级数据拆分保证评测鲁棒性。通过100次Monte Carlo实验进行统计学验证。

Result: 在C-NMC 2019数据集（共62名患者的12528幅图像）上，模型在测试集取得了97.89%的F1分数和准确率，显著超过基线算法，并用参数量比VGG16少89%。

Conclusion: 该管道不仅提升了白血病细胞分类的准确率与可解释性，还具备高效、可部署的临床应用潜力，验证了现代注意力机制在医学图像分析中的优越性。

Abstract: We present a reproducible deep learning pipeline for leukemic cell classification, focusing on system architecture, experimental robustness, and software design choices for medical image analysis. Acute lymphoblastic leukemia (ALL) is the most common childhood cancer, requiring expert microscopic diagnosis that suffers from inter-observer variability and time constraints. The proposed system integrates an attention-based convolutional neural network combining EfficientNetV2-B3 with Squeeze-and-Excitation mechanisms for automated ALL cell classification. Our approach employs comprehensive data augmentation, focal loss for class imbalance, and patient-wise data splitting to ensure robust and reproducible evaluation. On the C-NMC 2019 dataset (12,528 original images from 62 patients), the system achieves a 97.89% F1-score and 97.89% accuracy on the test set, with statistical validation through 100-iteration Monte Carlo experiments confirming significant improvements (p < 0.001) over baseline methods. The proposed pipeline outperforms existing approaches by up to 4.67% while using 89% fewer parameters than VGG16 (15.2M vs. 138M). The attention mechanism provides interpretable visualizations of diagnostically relevant cellular features, demonstrating that modern attention-based architectures can improve leukemic cell classification while maintaining computational efficiency suitable for clinical deployment.

</details>


### [28] [Mono3DV: Monocular 3D Object Detection with 3D-Aware Bipartite Matching and Variational Query DeNoising](https://arxiv.org/abs/2601.01036)
*Kiet Dang Vu,Trung Thai Tran,Kien Nguyen Do Trung,Duc Dung Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种新型基于Transformer的单目3D目标检测方法Mono3DV，通过更好地融合3D要素于匹配过程，在KITTI 3D检测基准上取得新SOTA。


<details>
  <summary>Details</summary>
Motivation: 以往DETR类架构在单目3D目标检测中，忽略了3D属性在双边匹配中的作用，导致训练不稳定和匹配错位，难以获得高质量3D检测结果。

Method: 1. 提出3D-Aware Bipartite Matching，将3D几何信息纳入匹配成本，有效解决2D标准带来的匹配错乱；2. 为了平衡引入3D属性带来的不稳定性，训练时引入3D-DeNoising方案；3. 针对传统去噪方式的梯度消失问题，设计了变分查询去噪机制(Variational Query DeNoising)，提升了模型表现。

Result: 在不借助任何外部数据的情况下，所提方法在KITTI 3D目标检测基准上达到了最新最好水平。

Conclusion: 通过创新的匹配和训练机制，有效克服已有单目3D检测方法忽视3D属性导致的问题，实现了更精准稳定的检测效果。

Abstract: While DETR-like architectures have demonstrated significant potential for monocular 3D object detection, they are often hindered by a critical limitation: the exclusion of 3D attributes from the bipartite matching process. This exclusion arises from the inherent ill-posed nature of 3D estimation from monocular image, which introduces instability during training. Consequently, high-quality 3D predictions can be erroneously suppressed by 2D-only matching criteria, leading to suboptimal results. To address this, we propose Mono3DV, a novel Transformer-based framework. Our approach introduces three key innovations. First, we develop a 3D-Aware Bipartite Matching strategy that directly incorporates 3D geometric information into the matching cost, resolving the misalignment caused by purely 2D criteria. Second, it is important to stabilize the Bipartite Matching to resolve the instability occurring when integrating 3D attributes. Therefore, we propose 3D-DeNoising scheme in the training phase. Finally, recognizing the gradient vanishing issue associated with conventional denoising techniques, we propose a novel Variational Query DeNoising mechanism to overcome this limitation, which significantly enhances model performance. Without leveraging any external data, our method achieves state-of-the-art results on the KITTI 3D object detection benchmark.

</details>


### [29] [Deepfake Detection with Multi-Artifact Subspace Fine-Tuning and Selective Layer Masking](https://arxiv.org/abs/2601.01041)
*Xiang Zhang,Wenliang Weng,Daoyong Fu,Ziqiang Li,Zhangjie Fu*

Main category: cs.CV

TL;DR: 本文提出了一种基于多伪造伪迹子空间和选择性层掩码（MASM）的深度伪造检测方法，通过结构性地分离语义和伪造伪迹特征，提升了跨数据集的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法在跨数据集和真实复杂场景下表现不佳，主要问题在于不同伪造方法引入的伪迹分布多样，而直接迁移预训练模型会破坏模型原有的通用语义结构，难以兼顾伪迹建模和语义稳定性。

Method: 本文提出MASM方法：1）通过对网络权重进行奇异值分解，把预训练权重划分为稳定的语义主子空间和多个可学习的伪迹子空间，实现语义和伪迹的解耦；2）采用选择性层掩码，根据伪迹子空间的学习状态自适应调节各网络层的更新，抑制对单一伪造特征的过拟合；3）对多个伪迹子空间引入正交和谱一致性约束，指导子空间学到互补且多样的伪迹特征，同时保持谱结构稳定。

Result: 实验表明，所提MASM方法在跨数据集伪造检测任务上显著提升了模型的泛化鲁棒性，能够有效识别多样化的伪造伪迹，并在多个基准数据集上优于现有主流方法。

Conclusion: MASM通过结构式分离和对模型更新的精细调控，实现了伪迹建模和语义稳定性的兼顾，显著提升了深度伪造检测在实际应用中的泛化能力。

Abstract: Deepfake detection still faces significant challenges in cross-dataset and real-world complex scenarios. The root cause lies in the high diversity of artifact distributions introduced by different forgery methods, while pretrained models tend to disrupt their original general semantic structures when adapting to new artifacts. Existing approaches usually rely on indiscriminate global parameter updates or introduce additional supervision signals, making it difficult to effectively model diverse forgery artifacts while preserving semantic stability. To address these issues, this paper proposes a deepfake detection method based on Multi-Artifact Subspaces and selective layer masks (MASM), which explicitly decouples semantic representations from artifact representations and constrains the fitting strength of artifact subspaces, thereby improving generalization robustness in cross-dataset scenarios. Specifically, MASM applies singular value decomposition to model weights, partitioning pretrained weights into a stable semantic principal subspace and multiple learnable artifact subspaces. This design enables decoupled modeling of different forgery artifact patterns while preserving the general semantic subspace. On this basis, a selective layer mask strategy is introduced to adaptively regulate the update behavior of corresponding network layers according to the learning state of each artifact subspace, suppressing overfitting to any single forgery characteristic. Furthermore, orthogonality constraints and spectral consistency constraints are imposed to jointly regularize multiple artifact subspaces, guiding them to learn complementary and diverse artifact representations while maintaining a stable overall spectral structure.

</details>


### [30] [Evaluating transfer learning strategies for improving dairy cattle body weight prediction in small farms using depth-image and point-cloud data](https://arxiv.org/abs/2601.01044)
*Jin Wang,Angelo De Castro,Yuxi Zhang,Lucas Basolli Borsatto,Yuechen Guo,Victoria Bastos Primo,Ana Beatriz Montevecchio Bernardino,Gota Morota,Ricardo C Chebel,Haipeng Yu*

Main category: cs.CV

TL;DR: 本文探讨了在奶牛体重预测任务中，迁移学习对小型牧场的效果，并比较了基于深度图像和点云数据的预测模型。结果显示，迁移学习能显著提升小型牧场的预测效果，两类数据模态间表现无显著差异。


<details>
  <summary>Details</summary>
Motivation: 自动、非侵入性地监测奶牛体重对于健康管理和性状数据收集至关重要。目前迁移学习广泛应用于图像体重预测，但其实际效果和最优策略在畜牧业中尚不明确，特别是超越常规ImageNet/COCO预训练权重时。此外，尽管深度图像与三维点云都用于体重预测，但两者在奶牛中直接对比的研究极为有限。

Method: 该研究收集了三种规模牧场（大型：1201头，中型：215头，小型：58头）的顶部深度图像和点云数据，评估了四种深度学习模型（ConvNeXt、MobileViT用于深度图像，PointNet、DGCNN用于点云）的预测效果。对比了直接训练、迁移学习与联合学习三种实验设计下的表现。

Result: 迁移学习在所有四种模型上均大幅提升了小型牧场体重预测能力，超过单一数据源训练，效果与联合学习相当或更优。不同模态模型间（深度图像与点云）无显著、持续的性能差别。

Conclusion: 迁移学习能够提升数据受限牧场的预测能力，适用于无法共享原始数据但可共享模型权重的场景。预训练模型具备较强的跨牧场通用性，数据模态选择可根据实际需求灵活调整。

Abstract: Computer vision provides automated, non-invasive, and scalable tools for monitoring dairy cattle, thereby supporting management, health assessment, and phenotypic data collection. Although transfer learning is commonly used for predicting body weight from images, its effectiveness and optimal fine-tuning strategies remain poorly understood in livestock applications, particularly beyond the use of pretrained ImageNet or COCO weights. In addition, while both depth images and three-dimensional point-cloud data have been explored for body weight prediction, direct comparisons of these two modalities in dairy cattle are limited. Therefore, the objectives of this study were to 1) evaluate whether transfer learning from a large farm enhances body weight prediction on a small farm with limited data, and 2) compare the predictive performance of depth-image- and point-cloud-based approaches under three experimental designs. Top-view depth images and point-cloud data were collected from 1,201, 215, and 58 cows at large, medium, and small dairy farms, respectively. Four deep learning models were evaluated: ConvNeXt and MobileViT for depth images, and PointNet and DGCNN for point clouds. Transfer learning markedly improved body weight prediction on the small farm across all four models, outperforming single-source learning and achieving gains comparable to or greater than joint learning. These results indicate that pretrained representations generalize well across farms with differing imaging conditions and dairy cattle populations. No consistent performance difference was observed between depth-image- and point-cloud-based models. Overall, these findings suggest that transfer learning is well suited for small farm prediction scenarios where cross-farm data sharing is limited by privacy, logistical, or policy constraints, as it requires access only to pretrained model weights rather than raw data.

</details>


### [31] [EgoGrasp: World-Space Hand-Object Interaction Estimation from Egocentric Videos](https://arxiv.org/abs/2601.01050)
*Hongming Fu,Wenjia Wang,Xiaozhen Qiao,Shuo Yang,Zheng Liu,Bo Zhao*

Main category: cs.CV

TL;DR: EgoGrasp 是首个可从动态第一视角单目视频重建世界坐标下手物交互的方法，在动态相机和真实环境中表现出色，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有手物交互重建方法多只针对静态图片或相机坐标系，无法处理动态视频和全局轨迹，而且难以应对第一视角视频中的剧烈相机运动与遮挡。对手和物体的完整世界坐标重建需求在虚拟现实和智能体研究中极其重要。

Method: 提出了一个多阶段的框架，包括基于新空间智能模型的预处理流程、无需模板且可扩展到多物体的全身手物先验模型（基于解耦扩散模型）、以及多目标测试时优化范式，针对动态视频中的手物全局交互进行鲁棒重建。

Result: 实验结果表明，该方法在世界坐标下的手物交互重建方面达到了最先进的性能，能有效应对剧烈相机运动和遮挡。

Conclusion: EgoGrasp 首次解决了动态第一视角视频中多物体世界空间手物交互重建难题，为理解人类行为和虚拟现实等应用奠定了基础。

Abstract: We propose EgoGrasp, the first method to reconstruct world-space hand-object interactions (W-HOI) from egocentric monocular videos with dynamic cameras in the wild. Accurate W-HOI reconstruction is critical for understanding human behavior and enabling applications in embodied intelligence and virtual reality. However, existing hand-object interactions (HOI) methods are limited to single images or camera coordinates, failing to model temporal dynamics or consistent global trajectories. Some recent approaches attempt world-space hand estimation but overlook object poses and HOI constraints. Their performance also suffers under severe camera motion and frequent occlusions common in egocentric in-the-wild videos. To address these challenges, we introduce a multi-stage framework with a robust pre-process pipeline built on newly developed spatial intelligence models, a whole-body HOI prior model based on decoupled diffusion models, and a multi-objective test-time optimization paradigm. Our HOI prior model is template-free and scalable to multiple objects. In experiments, we prove our method achieving state-of-the-art performance in W-HOI reconstruction.

</details>


### [32] [Enhancing Histopathological Image Classification via Integrated HOG and Deep Features with Robust Noise Performance](https://arxiv.org/abs/2601.01056)
*Ifeanyi Ezuma,Ugochukwu Ugwu*

Main category: cs.CV

TL;DR: 该研究比较了多种机器学习和深度学习模型在LC25000病理图像集上的分类性能，重点分析了InceptionResNet-v2深度特征的有效性及其在不同噪声环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在数字病理时代，自动化图像分析对于临床实践日益重要，因此需要评估和改进基于深度学习的模型在病理图像分类上的表现及鲁棒性。

Method: 作者在LC25000数据集（含五类病理图片）上，使用微调后的InceptionResNet-v2网络，既作为分类器也用于提取深度特征。随后将这些深度特征用于训练其它机器学习模型（如GBM、KNN和神经网络），并比较在不同信噪比条件下的表现。还实验了HOG与深度特征的结合。

Result: 微调后的InceptionResNet-v2网络分类准确率达96.01%，AUC为96.8%。基于InceptionResNet-v2深度特征训练的神经网络，AUC达99.99%、准确率99.84%。使用深度特征的模型在噪声下更具鲁棒性，尤其是GBM和KNN。HOG和深度特征结合对性能有提升，但在噪声环境下效果有限。

Conclusion: 深度特征，尤其由InceptionResNet-v2提取的特征，能显著提升病理图像分类性能并增强模型在噪声下的鲁棒性。不同模型和特征融合对性能有差异，实验证明深度学习结合传统特征有一定潜力但仍需应对噪音的挑战。

Abstract: The era of digital pathology has advanced histopathological examinations, making automated image analysis essential in clinical practice. This study evaluates the classification performance of machine learning and deep learning models on the LC25000 dataset, which includes five classes of histopathological images. We used the fine-tuned InceptionResNet-v2 network both as a classifier and for feature extraction. Our results show that the fine-tuned InceptionResNet-v2 achieved a classification accuracy of 96.01\% and an average AUC of 96.8\%. Models trained on deep features from InceptionResNet-v2 outperformed those using only the pre-trained network, with the Neural Network model achieving an AUC of 99.99\% and accuracy of 99.84\%. Evaluating model robustness under varying SNR conditions revealed that models using deep features exhibited greater resilience, particularly GBM and KNN. The combination of HOG and deep features showed enhanced performance, however, less so in noisy environments.

</details>


### [33] [Real-Time LiDAR Point Cloud Densification for Low-Latency Spatial Data Transmission](https://arxiv.org/abs/2601.01210)
*Kazuhiko Murasaki,Shunsuke Konagai,Masakatsu Aoki,Taiga Yoshida,Ryuichi Tanida*

Main category: cs.CV

TL;DR: 提出了一种基于高帧率、高效卷积神经网络的LiDAR点云增密方法，实现了实时、低延迟的3D场景重建。


<details>
  <summary>Details</summary>
Motivation: 目前沉浸式远程临场系统面临两大挑战：高密度动态3D场景的获取与实时处理。虽然LiDAR可以实时采集3D数据，但点云稀疏，影响最终表现质量。因此亟需提升点云密度且保证实时性的技术。

Method: 将多路LiDAR数据与高分辨率彩色图像结合，并应用联合双边滤波，采用卷积神经网络架构实现点云增密。该方法在保持低延迟的同时对稀疏点云进行深度补全。

Result: 该方法可实时（30fps）生成全高清分辨率下的稠密深度图，速度比最新的训练型深度补全方案快15倍以上，重建点云几何精度高且无多视角不一致或虚影伪影。

Conclusion: 本文方法显著提升了LiDAR点云的密度和质量，满足了低延迟沉浸式远程临场的需求，对实时3D场景重建具有重要应用价值。

Abstract: To realize low-latency spatial transmission system for immersive telepresence, there are two major problems: capturing dynamic 3D scene densely and processing them in real time. LiDAR sensors capture 3D in real time, but produce sparce point clouds. Therefore, this paper presents a high-speed LiDAR point cloud densification method to generate dense 3D scene with minimal latency, addressing the need for on-the-fly depth completion while maintaining real-time performance. Our approach combines multiple LiDAR inputs with high-resolution color images and applies a joint bilateral filtering strategy implemented through a convolutional neural network architecture. Experiments demonstrate that the proposed method produces dense depth maps at full HD resolution in real time (30 fps), which is over 15x faster than a recent training-based depth completion approach. The resulting dense point clouds exhibit accurate geometry without multiview inconsistencies or ghosting artifacts.

</details>


### [34] [Efficient Hyperspectral Image Reconstruction Using Lightweight Separate Spectral Transformers](https://arxiv.org/abs/2601.01064)
*Jianan Li,Wangcai Zhao,Tingfa Xu*

Main category: cs.CV

TL;DR: 本文提出了一种高效重建高光谱图像的新架构LSST，相较现有方法表现更优且计算量更低。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像在多个领域应用广泛，但通过压缩感知进行高效重建仍具挑战性，现有方法在精度和效率上难以兼顾。

Method: 作者采用分而治之策略，提出轻量级分离谱变换器（LSST），结合专门的谱变换块（SSTB）和轻量级空间卷积块（LSCB），分别建模光谱和空间特征，并引入聚焦谱损失改进训练。

Result: 实验表明，LSST能以更少的浮点运算量和参数实现更优的重建效果，优于现有主流方法。

Conclusion: LSST架构在高光谱图像重建任务中兼顾高效性和准确性，具有较强的实际应用前景，且提供了开源实现。

Abstract: Hyperspectral imaging (HSI) is essential across various disciplines for its capacity to capture rich spectral information. However, efficiently reconstructing hyperspectral images from compressive sensing measurements presents significant challenges. To tackle these, we adopt a divide-and-conquer strategy that capitalizes on the unique spectral and spatial characteristics of hyperspectral images. We introduce the Lightweight Separate Spectral Transformer (LSST), an innovative architecture tailored for efficient hyperspectral image reconstruction. This architecture consists of Separate Spectral Transformer Blocks (SSTB) for modeling spectral relationships and Lightweight Spatial Convolution Blocks (LSCB) for spatial processing. The SSTB employs Grouped Spectral Self-attention and a Spectrum Shuffle operation to effectively manage both local and non-local spectral relationships. Simultaneously, the LSCB utilizes depth-wise separable convolutions and strategic ordering to enhance spatial information processing. Furthermore, we implement the Focal Spectrum Loss, a novel loss weighting mechanism that dynamically adjusts during training to improve reconstruction across spectrally complex bands. Extensive testing demonstrates that our LSST achieves superior performance while requiring fewer FLOPs and parameters, underscoring its efficiency and effectiveness. The source code is available at: https://github.com/wcz1124/LSST.

</details>


### [35] [DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving](https://arxiv.org/abs/2601.01528)
*Yang Zhou,Hao Shao,Letian Wang,Zhuofan Zong,Hongsheng Li,Steven L. Waslander*

Main category: cs.CV

TL;DR: 本文提出了DrivingGen，这是首个用于自动驾驶生成式世界模型的全面基准，填补了现有评测体系在现实场景多样性和评测指标上的空白。通过对14种主流模型的测试揭示了通用视频生成与驾驶特有生成模型的优劣权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式视频模型在自动驾驶中的应用极具潜力，但当前缺乏针对该领域的系统性、严格评测基准，导致进步难以量化，各项关键能力（如物理一致性、时序连贯、可控性等）难以全面评估和优化。

Method: 作者构建了DrivingGen基准，包括多样化的自动驾驶视频集（覆盖各种天气、时段、地理、驾驶动作）和一套新颖的指标体系，全面评估生成模型在视觉真实感、轨迹合理性、时序连贯性和自车控制能力上的表现。

Result: 基准测试14个当前主流模型显示，通用视频生成模型在视觉效果上更优，但常常违反物理规则；而专为驾驶设计的模型则在运动合理性上表现更好，但视觉质量较低。

Conclusion: DrivingGen填补了自动驾驶生成模型评测上的空白，为领域发展提供了统一、权威的评测体系，将促进更可靠、可控、可部署的生成式自动驾驶世界模型发展，助力仿真、规划和数据驱动决策。

Abstract: Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.

</details>


### [36] [A UAV-Based Multispectral and RGB Dataset for Multi-Stage Paddy Crop Monitoring in Indian Agricultural Fields](https://arxiv.org/abs/2601.01084)
*Adari Rama Sukanya,Puvvula Roopesh Naga Sri Sai,Kota Moses,Rimalapudi Sarvendranath*

Main category: cs.CV

TL;DR: 本文发布了一个基于无人机拍摄的大规模稻田RGB和多光谱图像数据集，包含从育苗到收割阶段的高分辨率图像及丰富元数据，数据可支持病害分析、精准喷洒和产量估算等研究。


<details>
  <summary>Details</summary>
Motivation: 由于高分辨率、全生命周期覆盖的农田影像数据稀缺，尤其是针对印度稻作，研究者通过 UAV 采集和发布数据集以支持农业智能化与遥感应用发展。

Method: 使用20MP的RGB相机和5MP四波段多光谱相机，按照标准化操作流程采集并标注元数据（GPS、高度、环境等），采集范围覆盖5英亩，像元精度为1厘米。通过Pix4D Fields对影像进行正射拼接和植被指数计算（如NDVI、NDRE），验证数据质量。

Result: 共获取42,430张原始图像（415GB），配套丰富的元数据信息，实现了从稻田育苗到收获全阶段的高分辨率影像采集，并公开于IEEE DataPort。

Conclusion: 该数据集是目前少有的覆盖印度水稻全生长周期且分辨率极高的农业遥感公开数据集，可促进精细农业管理、病虫害监测和生产力估算等相关领域研究。

Abstract: We present a large-scale unmanned aerial vehicle (UAV)-based RGB and multispectral image dataset collected over paddy fields in the Vijayawada region, Andhra Pradesh, India, covering nursery to harvesting stages. We used a 20-megapixel RGB camera and a 5-megapixel four-band multispectral camera capturing red, green, red-edge, and near-infrared bands. Standardised operating procedure (SOP) and checklists were developed to ensure repeatable data acquisition. Our dataset comprises of 42,430 raw images (415 GB) captured over 5 acres with 1 cm/pixel ground sampling distance (GSD) with associated metadata such as GPS coordinates, flight altitude, and environmental conditions. Captured images were validated using Pix4D Fields to generate orthomosaic maps and vegetation index maps, such as normalised difference vegetation index (NDVI) and normalised difference red-edge (NDRE) index. Our dataset is one of the few datasets that provide high-resolution images with rich metadata that cover all growth stages of Indian paddy crops. The dataset is available on IEEE DataPort with DOI, . It can support studies on targeted spraying, disease analysis, and yield estimation.

</details>


### [37] [Real-Time Lane Detection via Efficient Feature Alignment and Covariance Optimization for Low-Power Embedded Systems](https://arxiv.org/abs/2601.01696)
*Yian Liu,Xiong Wang,Ping Xu,Lei Zhu,Ming Yan,Linyun Xue*

Main category: cs.CV

TL;DR: 提出了一种名为协方差分布优化（CDO）的新模块，可提升嵌入式系统中车道线检测模型的实时性能和精度，且不增加计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 由于嵌入式系统资源有限，现有深度学习车道线检测方法在精度和计算效率之间难以兼顾，缺乏适用于低功耗场景的优化技术。

Method: 提出CDO模块，通过对齐特征分布与真实标签分布，提高检测准确率。同时，该模块无须改变模型结构，可直接集成到现有检测模型中，利用原有模型参数迭代训练，兼容性强。

Result: 在包括实时优化模型和SOTA模型在内的六种方法、三大主流车道线检测数据集（CULane, TuSimple, LLAMAS）上，接入CDO模块后，模型精度提升0.01%~1.5%。

Conclusion: CDO模块可无缝集成进多种检测模型中，实现精度提升而计算量与功耗几乎不变，提升了嵌入式系统车道线检测的实用性和灵活性。

Abstract: Real-time lane detection in embedded systems encounters significant challenges due to subtle and sparse visual signals in RGB images, often constrained by limited computational resources and power consumption. Although deep learning models for lane detection categorized into segmentation-based, anchor-based, and curve-based methods there remains a scarcity of universally applicable optimization techniques tailored for low-power embedded environments. To overcome this, we propose an innovative Covariance Distribution Optimization (CDO) module specifically designed for efficient, real-time applications. The CDO module aligns lane feature distributions closely with ground-truth labels, significantly enhancing detection accuracy without increasing computational complexity. Evaluations were conducted on six diverse models across all three method categories, including two optimized for real-time applications and four state-of-the-art (SOTA) models, tested comprehensively on three major datasets: CULane, TuSimple, and LLAMAS. Experimental results demonstrate accuracy improvements ranging from 0.01% to 1.5%. The proposed CDO module is characterized by ease of integration into existing systems without structural modifications and utilizes existing model parameters to facilitate ongoing training, thus offering substantial benefits in performance, power efficiency, and operational flexibility in embedded systems.

</details>


### [38] [Luminark: Training-free, Probabilistically-Certified Watermarking for General Vision Generative Models](https://arxiv.org/abs/2601.01085)
*Jiayi Xu,Zhang Zhang,Yuanrui Zhang,Ruitao Chen,Yixian Xu,Tianyu He,Di He*

Main category: cs.CV

TL;DR: 本文提出Luminark，一种无需训练、具有概率认证的水印方案，适用于各种视觉生成模型，方法基于局部亮度统计，实现了高检测准确率、强鲁棒性且不影响图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型的水印方法有限，往往依赖训练或缺乏理论认证，复用性和实用性不足。因此，亟需一种训练无关、理论可认证且通用的新型水印方法，用于保护生成内容的来源。

Method: Luminark基于对图像分块后每块亮度的统计特征，设计二值水印模式及相应阈值。检测时判定各块亮度是否超过阈值并生成二值模式，进一步与目标水印模式比较。通过分析可精确控制误检率以获得认证检测。注入水印时，借助主流的生成引导技术(plug-and-play)实现兼容多种生成模型。

Result: 在扩散、自回归及混合型共九种主流生成模型上实验，Luminark均展现出高检测准确率、强强鲁棒性（对常见图像变换不敏感），且视觉质量高。

Conclusion: Luminark无需训练、能概率认证且兼容多模型，是通用、实用的新型生成模型水印方案，兼具准确性、鲁棒性与易用性。

Abstract: In this paper, we introduce \emph{Luminark}, a training-free and probabilistically-certified watermarking method for general vision generative models. Our approach is built upon a novel watermark definition that leverages patch-level luminance statistics. Specifically, the service provider predefines a binary pattern together with corresponding patch-level thresholds. To detect a watermark in a given image, we evaluate whether the luminance of each patch surpasses its threshold and then verify whether the resulting binary pattern aligns with the target one. A simple statistical analysis demonstrates that the false positive rate of the proposed method can be effectively controlled, thereby ensuring certified detection. To enable seamless watermark injection across different paradigms, we leverage the widely adopted guidance technique as a plug-and-play mechanism and develop the \emph{watermark guidance}. This design enables Luminark to achieve generality across state-of-the-art generative models without compromising image quality. Empirically, we evaluate our approach on nine models spanning diffusion, autoregressive, and hybrid frameworks. Across all evaluations, Luminark consistently demonstrates high detection accuracy, strong robustness against common image transformations, and good performance on visual quality.

</details>


### [39] [VIT-Ped: Visionary Intention Transformer for Pedestrian Behavior Analysis](https://arxiv.org/abs/2601.01989)
*Aly R. Elkammar,Karim M. Gamaleldin,Catherine M. Elias*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer与视频视觉Transformer的多模态行人意图预测算法，并在JAAD数据集上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶从L3到L4阶段，准确预测行人意图对于提升道路安全至关重要。作者旨在通过引入更有效的模型与多模态特征，提高行人过街行为的理解与预测能力。

Method: 提出了一套多尺寸的Transformer/视频视觉Transformer架构，能够处理不同类型的数据模态，并在JAAD数据集上进行评测。此外，文中通过消融实验系统性探索了不同模型设计选择带来的优势。

Result: 所提出的方法在准确率、AUC和F1分数等多项指标上均超过了现有最优方法（SOTA）。

Conclusion: 结果表明引入Transformer和多模态信息显著提升了行人意图预测的性能，对未来自动驾驶系统意义重大。

Abstract: Pedestrian Intention prediction is one of the key technologies in the transition from level 3 to level 4 autonomous driving. To understand pedestrian crossing behaviour, several elements and features should be taken into consideration to make the roads of tomorrow safer for everybody. We introduce a transformer / video vision transformer based algorithm of different sizes which uses different data modalities .We evaluated our algorithms on popular pedestrian behaviour dataset, JAAD, and have reached SOTA performance and passed the SOTA in metrics like Accuracy, AUC and F1-score. The advantages brought by different model design choices are investigated via extensive ablation studies.

</details>


### [40] [600k-ks-ocr: a large-scale synthetic dataset for optical character recognition in kashmiri script](https://arxiv.org/abs/2601.01088)
*Haq Nawaz Malik*

Main category: cs.CV

TL;DR: 本文介绍了600K-KS-OCR数据集，这是一个大规模且合成的卡什米尔文字符识别数据集，共包含约60.2万张分割好的词图像，用于提升该低资源语言的OCR训练与评测。


<details>
  <summary>Details</summary>
Motivation: 卡什米尔语是一种使用改良型波斯-阿拉伯字母的濒危语言，缺乏高质量OCR训练数据，严重制约了相关技术发展。该工作旨在填补这一资源空白。

Method: 作者合成了602,000个词级分割图像，采用三种卡什米尔传统字体，丰富的数据增强手段模拟文档退化及多样化背景，并为每张图像提供多种主流OCR模型兼容格式的标注。

Result: 数据以10个分卷、共约10.6GB形式发布，图像质量高，标签完备，可直接用于CRNN、TrOCR等主流模型训练。

Conclusion: 该数据集为卡什米尔文的OCR研究和低资源语言处理提供了基础资源，并通过开放授权促进相关领域发展。

Abstract: This technical report presents the 600K-KS-OCR Dataset, a large-scale synthetic corpus comprising approximately 602,000 word-level segmented images designed for training and evaluating optical character recognition systems targeting Kashmiri script. The dataset addresses a critical resource gap for Kashmiri, an endangered Dardic language utilizing a modified Perso-Arabic writing system spoken by approximately seven million people. Each image is rendered at 256x64 pixels with corresponding ground-truth transcriptions provided in multiple formats compatible with CRNN, TrOCR, and generalpurpose machine learning pipelines. The generation methodology incorporates three traditional Kashmiri typefaces, comprehensive data augmentation simulating real-world document degradation, and diverse background textures to enhance model robustness. The dataset is distributed across ten partitioned archives totaling approximately 10.6 GB and is released under the CC-BY-4.0 license to facilitate research in low-resource language optical character recognition.

</details>


### [41] [NarrativeTrack: Evaluating Video Language Models Beyond the Frame](https://arxiv.org/abs/2601.01095)
*Hyeonjeong Ha,Jinjin Ge,Bo Feng,Kaixin Ma,Gargi Chakraborty*

Main category: cs.CV

TL;DR: 本文提出了NarrativeTrack基准，用于评估多模态大模型（MLLMs）在视频叙事理解方面的能力，并发现当前主流模型在时序背景下的实体追踪与叙事理解上仍有较大不足。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型虽在视觉-语言推理上取得进展，但对视频中随时间展开的复杂叙事理解能力仍然欠缺，现有评测工具大多局限于短片段或粗略场景语义，无法细致考查模型对实体的追踪和叙述推理能力。

Method: 作者提出了NarrativeTrack基准，将视频拆解为实体，并通过Compositional Reasoning Progression (CRP)框架在实体存在、状态变化和歧义等维度逐步提升推理复杂度，使用全自动化实体提取流程支持大规模评测。

Result: 主流通用与视频特定的MLLMs在视觉感知或时序关联方面各有优劣：通用模型感知能力强但时间一致性弱，视频模型有时序感但易出现实体上下文臆造。大多数模型难以在视觉和时序过渡间持续准确追踪实体。

Conclusion: 叙事理解需要视觉感知与时序推理能力的有效整合，NarrativeTrack为诊断和提升MLLMs的叙事理解能力提供了首个系统化工具。

Abstract: Multimodal large language models (MLLMs) have achieved impressive progress in vision-language reasoning, yet their ability to understand temporally unfolding narratives in videos remains underexplored. True narrative understanding requires grounding who is doing what, when, and where, maintaining coherent entity representations across dynamic visual and temporal contexts. We introduce NarrativeTrack, the first benchmark to evaluate narrative understanding in MLLMs through fine-grained entity-centric reasoning. Unlike existing benchmarks limited to short clips or coarse scene-level semantics, we decompose videos into constituent entities and examine their continuity via a Compositional Reasoning Progression (CRP), a structured evaluation framework that progressively increases narrative complexity across three dimensions: entity existence, entity changes, and entity ambiguity. CRP challenges models to advance from temporal persistence to contextual evolution and fine-grained perceptual reasoning. A fully automated entity-centric pipeline enables scalable extraction of temporally grounded entity representations, providing the foundation for CRP. Evaluations of state-of-the-art MLLMs reveal that models fail to robustly track entities across visual transitions and temporal dynamics, often hallucinating identity under context shifts. Open-source general-purpose MLLMs exhibit strong perceptual grounding but weak temporal coherence, while video-specific MLLMs capture temporal context yet hallucinate entity's contexts. These findings uncover a fundamental trade-off between perceptual grounding and temporal reasoning, indicating that narrative understanding emerges only from their integration. NarrativeTrack provides the first systematic framework to diagnose and advance temporally grounded narrative comprehension in MLLMs.

</details>


### [42] [Evolving CNN Architectures: From Custom Designs to Deep Residual Models for Diverse Image Classification and Detection Tasks](https://arxiv.org/abs/2601.01099)
*Mahmudul Hasan,Mabsur Fatin Bin Hossain*

Main category: cs.CV

TL;DR: 本文比较了定制CNN与常见预训练和迁移学习CNN模型，在五个现实图像数据集上的表现，涉及二分类、细粒度多分类和目标检测。


<details>
  <summary>Details</summary>
Motivation: 目前在计算机视觉任务中，如何选择合适的神经网络架构尤为重要，但缺乏系统分析不同任务、不同复杂度下的模型优劣。本文希望通过对比多种模型，为实际任务提供设计和选择网络的参考。

Method: 设计并实现自定义CNN架构，并与主流预训练、迁移学习CNN模型（如ResNet、MobileNet等）在五个具有代表性的图像数据集上进行实验。系统分析网络深度、残差连接、特征提取策略等因素对分类和定位性能的影响，扩展到实际场景如非法三轮车检测。

Result: 深层CNN在细粒度多分类任务上有显著性能提升；而轻量级的预训练及迁移学习模型更适合简单的二分类任务。自定义架构在目标检测任务中也显示出很强适应性。

Conclusion: 通过本对比分析，提出了针对任务复杂度及资源限制选择合适CNN架构的实用建议，为模型设计与应用提供了理论与实践依据。

Abstract: This paper presents a comparative study of a custom convolutional neural network (CNN) architecture against widely used pretrained and transfer learning CNN models across five real-world image datasets. The datasets span binary classification, fine-grained multiclass recognition, and object detection scenarios. We analyze how architectural factors, such as network depth, residual connections, and feature extraction strategies, influence classification and localization performance. The results show that deeper CNN architectures provide substantial performance gains on fine-grained multiclass datasets, while lightweight pretrained and transfer learning models remain highly effective for simpler binary classification tasks. Additionally, we extend the proposed architecture to an object detection setting, demonstrating its adaptability in identifying unauthorized auto-rickshaws in real-world traffic scenes. Building upon a systematic analysis of custom CNN architectures alongside pretrained and transfer learning models, this study provides practical guidance for selecting suitable network designs based on task complexity and resource constraints.

</details>


### [43] [Histogram Assisted Quality Aware Generative Model for Resolution Invariant NIR Image Colorization](https://arxiv.org/abs/2601.01103)
*Abhinav Attri,Rajeev Ranjan Dwivedi,Samiran Das,Vinod Kumar Kurmi*

Main category: cs.CV

TL;DR: 本文提出了HAQAGen模型，实现了分辨率无关的NIR（近红外）到RGB彩色化转换，在保持色彩真实性与结构细节的同时，可在高分辨率下稳定工作，并超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: NIR影像在许多场景（如夜视、医疗、机器人等）中常见，但缺乏自然色彩信息。现有NIR到RGB转换方法往往在高分辨率、色彩还原和结构细节三者间难以平衡，因此需要创新模型提升成像质量与适用性。

Method: 1) 提出新的联合损失函数，包括基于可微直方图匹配的全局色彩对齐、感知图像质量和特征相似度损失，有效保留纹理信息；2) 通过SPADE引入局部色相-饱和度先验，提升色彩重建稳定性；3) 在Mamba主干网络中加入面向纹理的监督，强化细节还原；4) 设计自适应分辨率推理模块，实现高分辨率无损翻译。

Result: 在FANVID、OMSIV、VCIP2020、RGB2NIR等多组数据集和多种评价指标下，HAQAGen相较于现有最佳方法表现出一致且显著的提升。生成图像在清晰度、自然色彩和感知指标上均有优势。

Conclusion: HAQAGen是一种高效可扩展的NIR到RGB颜色转换模型，实现了分辨率无关、高质量色彩与结构复现，对多种成像应用具有广泛应用前景。

Abstract: We present HAQAGen, a unified generative model for resolution-invariant NIR-to-RGB colorization that balances chromatic realism with structural fidelity. The proposed model introduces (i) a combined loss term aligning the global color statistics through differentiable histogram matching, perceptual image quality measure, and feature based similarity to preserve texture information, (ii) local hue-saturation priors injected via Spatially Adaptive Denormalization (SPADE) to stabilize chromatic reconstruction, and (iii) texture-aware supervision within a Mamba backbone to preserve fine details. We introduce an adaptive-resolution inference engine that further enables high-resolution translation without sacrificing quality. Our proposed NIR-to-RGB translation model simultaneously enforces global color statistics and local chromatic consistency, while scaling to native resolutions without compromising texture fidelity or generalization. Extensive evaluations on FANVID, OMSIV, VCIP2020, and RGB2NIR using different evaluation metrics demonstrate consistent improvements over state-of-the-art baseline methods. HAQAGen produces images with sharper textures, natural colors, attaining significant gains as per perceptual metrics. These results position HAQAGen as a scalable and effective solution for NIR-to-RGB translation across diverse imaging scenarios. Project Page: https://rajeev-dw9.github.io/HAQAGen/

</details>


### [44] [Cross-Layer Attentive Feature Upsampling for Low-latency Semantic Segmentation](https://arxiv.org/abs/2601.01167)
*Tianheng Cheng,Xinggang Wang,Junchao Liao,Wenyu Liu*

Main category: cs.CV

TL;DR: 提出了一种新的引导注意力插值（GAI）方法，实现了高效、精细的高分辨率特征恢复和低延迟语义分割，刷新了Cityscapes和CamVid数据集的精度和速度记录。


<details>
  <summary>Details</summary>
Motivation: 现有的低分辨率特征插值方法（如双线性插值）在生成高分辨率特征时存在特征未对齐和语义信息不足的问题，同时提升高分辨率特征的语义丰富度又会带来较高的计算负担，难以满足低延时推理需求。

Method: 提出了Guided Attentive Interpolation（GAI）方法，自适应地结合不同分辨率特征的空间和语义关系，插值得到更细致且富含语义的高分辨率特征，可与任何深层卷积网络结合以实现高效语义分割。

Result: 在Cityscapes数据集上，GAI方法实现了78.8 mIoU和22.3 FPS，在CamVid数据集上实现了80.6 mIoU和64.5 FPS，在Nvidia 1080Ti GPU上均达到新的低延时分割领域的最优结果。

Conclusion: GAI方法在显著提高语义分割精度的同时，大幅降低了推理延迟，可广泛集成于主流语义分割网络中，助力实际应用中高效、快速的语义分割。

Abstract: Semantic segmentation is a fundamental problem in computer vision and it requires high-resolution feature maps for dense prediction. Current coordinate-guided low-resolution feature interpolation methods, e.g., bilinear interpolation, produce coarse high-resolution features which suffer from feature misalignment and insufficient context information. Moreover, enriching semantics to high-resolution features requires a high computation burden, so that it is challenging to meet the requirement of lowlatency inference. We propose a novel Guided Attentive Interpolation (GAI) method to adaptively interpolate fine-grained high-resolution features with semantic features to tackle these issues. Guided Attentive Interpolation determines both spatial and semantic relations of pixels from features of different resolutions and then leverages these relations to interpolate high-resolution features with rich semantics. GAI can be integrated with any deep convolutional network for efficient semantic segmentation. In experiments, the GAI-based semantic segmentation networks, i.e., GAIN, can achieve78.8 mIoU with 22.3 FPS on Cityscapes and 80.6 mIoU with 64.5 on CamVid using an NVIDIA 1080Ti GPU, which are the new state-of-the-art results of low-latency semantic segmentation. Code and models are available at: https://github.com/hustvl/simpleseg.

</details>


### [45] [CardioMOD-Net: A Modal Decomposition-Neural Network Framework for Diagnosis and Prognosis of HFpEF from Echocardiography Cine Loops](https://arxiv.org/abs/2601.01176)
*Andrés Bell-Navas,Jesús Garicano-Mena,Antonella Ausiello,Soledad Le Clainche,María Villalba-Orero,Enrique Lara-Pezzi*

Main category: cs.CV

TL;DR: 本研究提出了CardioMOD-Net AI框架，可以直接从小鼠常规超声心动图视频自动实现多类别HFpEF的诊断及发病时间的连续预测，极大丰富了心衰早期诊疗手段。


<details>
  <summary>Details</summary>
Motivation: HFpEF心力衰竭由多种共病导致，进展缓慢且早期无症状，当前AI模型难以实现特异性表型分类及疾病进展预测，亟需更精准的AI工具以改善早期诊疗和预后。

Method: 研究收集了对照组及三种病理组（高血糖、肥胖、高血压）小鼠的超声心动图视频，运用高阶动态模态分解（HODMD）提取时间序列特征，并采用Vision Transformer分别用于多类别分类与预测HFpEF发病时间。

Result: 多类别诊断准确率为65%，各组均超过50%；OB与SAH与CTL的早期阶段存在部分误判。发病时间的回归模块在预测HFpEF发作时间上，均方根误差为21.72周，OB和SAH预测最为精准，各组预测分布与实际相符。

Conclusion: CardioMOD-Net展示了单个超声心动图环可实现HFpEF多类表型诊断和发病时间连续预测，为前临床HFpEF研究的诊断与预后建模提供了新平台，特别适用于小样本数据场景。

Abstract: Introduction: Heart failure with preserved ejection fraction (HFpEF) arises from diverse comorbidities and progresses through prolonged subclinical stages, making early diagnosis and prognosis difficult. Current echocardiography-based Artificial Intelligence (AI) models focus primarily on binary HFpEF detection in humans and do not provide comorbidity-specific phenotyping or temporal estimates of disease progression towards decompensation. We aimed to develop a unified AI framework, CardioMOD-Net, to perform multiclass diagnosis and continuous prediction of HFpEF onset directly from standard echocardiography cine loops in preclinical models.
  Methods: Mouse echocardiography videos from four groups were used: control (CTL), hyperglycaemic (HG), obesity (OB), and systemic arterial hypertension (SAH). Two-dimensional parasternal long-axis cine loops were decomposed using Higher Order Dynamic Mode Decomposition (HODMD) to extract temporal features for downstream analysis. A shared latent representation supported Vision Transformers, one for a classifier for diagnosis and another for a regression module for predicting the age at HFpEF onset.
  Results: Overall diagnostic accuracy across the four groups was 65%, with all classes exceeding 50% accuracy. Misclassifications primarily reflected early-stage overlap between OB or SAH and CTL. The prognostic module achieved a root-mean-square error of 21.72 weeks for time-to-HFpEF prediction, with OB and SAH showing the most accurate estimates. Predicted HFpEF onset closely matched true distributions in all groups.
  Discussion: This unified framework demonstrates that multiclass phenotyping and continuous HFpEF onset prediction can be obtained from a single cine loop, even under small-data conditions. The approach offers a foundation for integrating diagnostic and prognostic modelling in preclinical HFpEF research.

</details>


### [46] [GenCAMO: Scene-Graph Contextual Decoupling for Environment-aware and Mask-free Camouflage Image-Dense Annotation Generation](https://arxiv.org/abs/2601.01181)
*Chenglizhao Chen,Shaojiang Yuan,Xiaoxue Lu,Mengke Song,Jia Song,Zhenyu Wu,Wenfeng Song,Shuai Li*

Main category: cs.CV

TL;DR: 本文通过生成模型合成高质量伪装图像密集数据，提出了一个新型的多模态大规模伪装数据集（GenCAMO-DB）及其生成框架（GenCAMO），显著提升复杂伪装场景下的密集预测任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前高质量、大规模带密集标注的伪装数据集稀缺，主要因为数据采集和标注成本高，制约了伪装物体检测和分割等任务的发展。

Method: 1）构建了包含深度图、场景图、属性描述和文本提示等多模态标注的大规模伪装数据集GenCAMO-DB；2）提出基于环境感知且无需人工掩码的生成模型GenCAMO，可自动生成高保真、密集标注的伪装图像。

Result: 在多种模态和任务上的大量实验显示，利用GenCAMO合成的数据显著提升了复杂伪装场景下，密集预测类任务（如RGB-D隐蔽物体检测等）的效果。

Conclusion: 本文方法助力于低成本高效获得复杂场景下的高质量伪装数据，推进了密集预测及相关理解推理任务的性能发展。

Abstract: Conceal dense prediction (CDP), especially RGB-D camouflage object detection and open-vocabulary camouflage object segmentation, plays a crucial role in advancing the understanding and reasoning of complex camouflage scenes. However, high-quality and large-scale camouflage datasets with dense annotation remain scarce due to expensive data collection and labeling costs. To address this challenge, we explore leveraging generative models to synthesize realistic camouflage image-dense data for training CDP models with fine-grained representations, prior knowledge, and auxiliary reasoning. Concretely, our contributions are threefold: (i) we introduce GenCAMO-DB, a large-scale camouflage dataset with multi-modal annotations, including depth maps, scene graphs, attribute descriptions, and text prompts; (ii) we present GenCAMO, an environment-aware and mask-free generative framework that produces high-fidelity camouflage image-dense annotations; (iii) extensive experiments across multiple modalities demonstrate that GenCAMO significantly improves dense prediction performance on complex camouflage scenes by providing high-quality synthetic data. The code and datasets will be released after paper acceptance.

</details>


### [47] [Crowded Video Individual Counting Informed by Social Grouping and Spatial-Temporal Displacement Priors](https://arxiv.org/abs/2601.01192)
*Hao Lu,Xuhui Zhu,Wenjing Zhang,Yanan Li,Xiang Bai*

Main category: cs.CV

TL;DR: 该论文提出了一种新的行人视频计数基线OMAN++，引入了社会群体和时空位移先验，显著提升了拥挤场景下的计数精度，并发布了新的大规模地铁拥挤数据集。


<details>
  <summary>Details</summary>
Motivation: 现有视频行人计数方法在拥挤、高动态变化场景下表现有限，尤其是在如地铁通勤这类强遮挡环境。为推进VIC任务的发展，作者构建了描述真实拥挤人流的WuhanMetroCrowd数据集。

Method: 引入两种关键先验：社会群体先验（行人往往成组移动，采用一对多匹配方式）和时空位移先验（单个体不会瞬移），通过上下文生成器、一对多匹配器和位移先验注入器改进VIC方法，结合为强基线OMAN++。

Result: OMAN++在SenseCrowd、CroHD、MovingDroneCrowd等基准上超越现有方法，尤其在新构建的WuhanMetroCrowd数据集上，误差率降低了38.12%。

Conclusion: 利用社会行为和时空先验可以有效提升复杂拥挤场景下的视频个体计数性能。新数据集为进一步研究提供了基础，所提出的OMAN++方法及其代码和权重已公开。

Abstract: Video Individual Counting (VIC) is a recently introduced task aiming to estimate pedestrian flux from a video. It extends Video Crowd Counting (VCC) beyond the per-frame pedestrian count. In contrast to VCC that learns to count pedestrians across frames, VIC must identify co-existent pedestrians between frames, which turns out to be a correspondence problem. Existing VIC approaches, however, can underperform in congested scenes such as metro commuting. To address this, we build WuhanMetroCrowd, one of the first VIC datasets that characterize crowded, dynamic pedestrian flows. It features sparse-to-dense density levels, short-to-long video clips, slow-to-fast flow variations, front-to-back appearance changes, and light-to-heavy occlusions. To better adapt VIC approaches to crowds, we rethink the nature of VIC and recognize two informative priors: i) the social grouping prior that indicates pedestrians tend to gather in groups and ii) the spatial-temporal displacement prior that informs an individual cannot teleport physically. The former inspires us to relax the standard one-to-one (O2O) matching used by VIC to one-to-many (O2M) matching, implemented by an implicit context generator and a O2M matcher; the latter facilitates the design of a displacement prior injector, which strengthens not only O2M matching but also feature extraction and model training. These designs jointly form a novel and strong VIC baseline OMAN++. Extensive experiments show that OMAN++ not only outperforms state-of-the-art VIC baselines on the standard SenseCrowd, CroHD, and MovingDroneCrowd benchmarks, but also indicates a clear advantage in crowded scenes, with a 38.12% error reduction on our WuhanMetroCrowd dataset. Code, data, and pretrained models are available at https://github.com/tiny-smart/OMAN.

</details>


### [48] [MS-ISSM: Objective Quality Assessment of Point Clouds Using Multi-scale Implicit Structural Similarity](https://arxiv.org/abs/2601.01200)
*Zhang Chen,Shuai Wan,Yuezhe Zhang,Siyu Ren,Fuzheng Yang,Junhui Hou*

Main category: cs.CV

TL;DR: 提出了一种新的点云质量评估方法MS-ISSM，通过多尺度隐式结构相似度测量和新的神经网络结构，有效提升了点云客观质量评估的准确性，并在多个基准上超越当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 点云数据结构无规则且稠密，传统点对点配准难以准确衡量感知质量，因此需要更有效、鲁棒的新型评估方法。

Method: 1）采用径向基函数（RBF）实现多尺度隐式特征表示，将失真评估转化为隐式函数系数的比较，避免了点对点匹配中的误差。2）提出ResGrouped-MLP网络，将多尺度特征差异通过分组编码、残差结构和通道注意力机制，映射为感知得分。网络结构能区分亮度、色度和几何特征，并针对不同尺度自适应关注重要失真。

Result: 在多个点云质量基准数据集上实验证明，所提方法在可靠性和泛化能力上均优于最新的主流指标。

Conclusion: MS-ISSM及其配套神经网络能够更准确、有效地进行点云感知质量评估，有望推动相关领域的发展。

Abstract: The unstructured and irregular nature of point clouds poses a significant challenge for objective quality assessment (PCQA), particularly in establishing accurate perceptual feature correspondence. To tackle this, we propose the Multi-scale Implicit Structural Similarity Measurement (MS-ISSM). Unlike traditional point-to-point matching, MS-ISSM utilizes Radial Basis Functions (RBF) to represent local features continuously, transforming distortion measurement into a comparison of implicit function coefficients. This approach effectively circumvents matching errors inherent in irregular data. Additionally, we propose a ResGrouped-MLP quality assessment network, which robustly maps multi-scale feature differences to perceptual scores. The network architecture departs from traditional flat MLPs by adopting a grouped encoding strategy integrated with Residual Blocks and Channel-wise Attention mechanisms. This hierarchical design allows the model to preserve the distinct physical semantics of luma, chroma, and geometry while adaptively focusing on the most salient distortion features across High, Medium, and Low scales. Experimental results on multiple benchmarks demonstrate that MS-ISSM outperforms state-of-the-art metrics in both reliability and generalization. The source code is available at: https://github.com/ZhangChen2022/MS-ISSM.

</details>


### [49] [RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models](https://arxiv.org/abs/2601.01202)
*Jiazhu Dai,Huihui Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种针对参考图像超分辨率（RefSR）系统的对抗攻击方法RefSR-Adv，通过只对参考图像进行微小扰动，显著降低RefSR系统在多个架构和数据集上的重建性能，暴露了RefSR系统在安全性方面的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前大部分关于RefSR的安全研究集中在后门攻击，而对抗性攻击在RefSR领域的脆弱性尚未被深入研究。鉴于RefSR对参考图像的依赖性，作者试图揭示RefSR系统在对抗攻击下的安全隐患。

Method: 作者提出RefSR-Adv，对参考图像施加对抗性扰动，通过最大化攻击后输出和原始输出之间的差异，诱发输出结果性能大幅下降。该方法适用于CNN、Transformer和Mamba等多种模型结构，并在CUFED5、WR-SR与DRefSR等数据集上进行了测试。

Result: 结果显示，RefSR-Adv能在不同模型和数据集上引起显著的性能下降和严重的伪影，同时实验发现输入的低分辨率图像与参考图像相似度越高，攻击效果越强，从而揭示了模型对参考特征的依赖是安全隐患的根本。

Conclusion: RefSR系统在对抗性扰动下存在严重的安全漏洞，建议社区重视RefSR模型的鲁棒性，加强相关防御措施。

Abstract: Single Image Super-Resolution (SISR) aims to recover high-resolution images from low-resolution inputs. Unlike SISR, Reference-based Super-Resolution (RefSR) leverages an additional high-resolution reference image to facilitate the recovery of high-frequency textures. However, existing research mainly focuses on backdoor attacks targeting RefSR, while the vulnerability of the adversarial attacks targeting RefSR has not been fully explored. To fill this research gap, we propose RefSR-Adv, an adversarial attack that degrades SR outputs by perturbing only the reference image. By maximizing the difference between adversarial and clean outputs, RefSR-Adv induces significant performance degradation and generates severe artifacts across CNN, Transformer, and Mamba architectures on the CUFED5, WR-SR, and DRefSR datasets. Importantly, experiments confirm a positive correlation between the similarity of the low-resolution input and the reference image and attack effectiveness, revealing that the model's over-reliance on reference features is a key security flaw. This study reveals a security vulnerability in RefSR systems, aiming to urge researchers to pay attention to the robustness of RefSR.

</details>


### [50] [XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression](https://arxiv.org/abs/2601.01204)
*Zunhai Su,Weihao Ye,Hansen Feng,Keyu Fan,Jing Zhang,Dahai Yu,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: 本文提出了XStreamVGGT，一种通过联合剪枝和量化系统性压缩KV缓存的无参调优方法，大幅减少存储和加速推理，实现高效流式3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的大规模三维视觉模型，尤其是StreamVGGT，在实现流式重建时，采用逐帧因果注意力，但伴随着KV缓存随输入帧增多无限制增长，导致显存消耗巨大和推理延迟增加。因此，亟需一种方法在保证精度的前提下有效压缩KV缓存，适配内存资源受限且注重实时性的流式3D应用场景。

Method: 提出XStreamVGGT，通过高效的token重要性识别方法对多视角输入中冗余的KV进行剪枝，实现固定的内存开销，并结合对KV张量分布的量化进一步压缩内存消耗。该方法为无调优，结合联合剪枝和量化自动实现KV缓存压缩。

Result: 在多项实验评估下，XStreamVGGT几乎不会影响模型性能，但能大幅降低KV缓存内存消耗4.42倍、推理加速5.48倍，显著提升流式3D重建的可扩展性和实用性。

Conclusion: XStreamVGGT有效平衡了流式3D计算的内存与速度，保持性能的同时实现了大幅度的效率提升，为实际大规模、实时三维场景应用奠定了基础。

Abstract: Learning-based 3D visual geometry models have benefited substantially from large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention for strong streaming reconstruction, but suffers from unbounded KV cache growth, leading to escalating memory consumption and inference latency as input frames accumulate. We propose XStreamVGGT, a tuning-free approach that systematically compresses the KV cache through joint pruning and quantization, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs originating from multi-view inputs are pruned through efficient token importance identification, enabling a fixed memory budget. Leveraging the unique distribution of KV tensors, we incorporate KV quantization to further reduce memory consumption. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\times$ and accelerating inference by 5.48$\times$, enabling scalable and practical streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/.

</details>


### [51] [Promptable Foundation Models for SAR Remote Sensing: Adapting the Segment Anything Model for Snow Avalanche Segmentation](https://arxiv.org/abs/2601.01213)
*Riccardo Gelato,Carlo Sgaravatti,Jakob Grahn,Giacomo Boracchi,Filippo Maria Bianchi*

Main category: cs.CV

TL;DR: 本文提出了一种结合Segment Anything Model (SAM) 与定制适配方案，实现对Sentinel-1 SAR影像雪崩区域快速标注的方法。


<details>
  <summary>Details</summary>
Motivation: 遥感雪崩分割和制图对高山地区风险预报和缓解具有重要意义，但传统方法标注SAR图像数据集费时费力，影响模型训练与应用。

Method: 作者针对SAM在SAR影像领域的适用性，设计了多项改进：(1) 采用适配器（adapter）缓解SAM与SAR影像之间的领域差异；(2) 用多编码器处理SAR多通道输入；(3) 借助提示工程提升雪崩本体定位，更好地标注小型、低对比度雪崩；(4) 对训练步骤做优化，限制编码器训练时间以提升效率。此外，将改进后的模型集成在标注工具中。

Result: 实验结果表明，这一方法能加速SAR影像的雪崩标注，为高质量数据集构建提供便利。

Conclusion: 基于SAM的定制方法能有效提升SAR雪崩分割的标注效率，有助于推动雪崩遥感监测与风险管理应用。

Abstract: Remote sensing solutions for avalanche segmentation and mapping are key to supporting risk forecasting and mitigation in mountain regions. Synthetic Aperture Radar (SAR) imagery from Sentinel-1 can be effectively used for this task, but training an effective detection model requires gathering a large dataset with high-quality annotations from domain experts, which is prohibitively time-consuming. In this work, we aim to facilitate and accelerate the annotation of SAR images for avalanche mapping. We build on the Segment Anything Model (SAM), a segmentation foundation model trained on natural images, and tailor it to Sentinel-1 SAR data. Adapting SAM to our use-case requires addressing several domain-specific challenges: (i) domain mismatch, since SAM was not trained on satellite/SAR imagery; (ii) input adaptation, because SAR products typically provide more than three channels, while SAM is constrained to RGB images; (iii) robustness to imprecise prompts that can affect target identification and degrade the segmentation quality, an issue exacerbated in small, low-contrast avalanches; and (iv) training efficiency, since standard fine-tuning is computationally demanding for SAM. We tackle these challenges through a combination of adapters to mitigate the domain gap, multiple encoders to handle multi-channel SAR inputs, prompt-engineering strategies to improve avalanche localization accuracy, and a training algorithm that limits the training time of the encoder, which is recognized as the major bottleneck. We integrate the resulting model into an annotation tool and show experimentally that it speeds up the annotation of SAR images.

</details>


### [52] [UniSH: Unifying Scene and Human Reconstruction in a Feed-Forward Pass](https://arxiv.org/abs/2601.01222)
*Mengfei Li,Peng Li,Zheng Zhang,Jiahao Lu,Chengfeng Zhao,Wei Xue,Qifeng Liu,Sida Peng,Wenxiao Zhang,Wenhan Luo,Yuan Liu,Yike Guo*

Main category: cs.CV

TL;DR: 本论文提出了一种统一的端到端方法UniSH，可联合重建3D场景和人体，实现高精度的度量级场景与人体还原。通过创新训练范式有效利用无标注实景数据，显著提升模型在真实场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统3D场景与人体联合重建方法严重依赖合成数据，导致模型对真实世界视频泛化能力差，人体几何失真，对齐性较差。为提升对真实场景重建效果，需充分利用未标注的真实数据与强先验信息。

Method: UniSH 框架包含两个核心技术：一是采用稳健的知识蒸馏策略，通过专家深度模型提取高频详细信息，提升人体表面还原；二是两阶段监督方案，先在合成数据上进行粗定位学习，然后在真实数据上通过SMPL网格与人体点云的几何对应优化进行微调。整体模型可端到端实现场景、人体、相机等联合高保真重建。

Result: 实验表明，UniSH在以人为中心的场景重建任务上达到当前最优效果，在全局人体运动估计上也与现有最强方法相比具备极强竞争力，无论是优化型还是只做人重建的方法都逊色一筹。

Conclusion: 提出的UniSH模型显著改善了真实场景下3D场景与人体联合重建的效果，兼顾高细节还原与良好泛化能力，为该领域的研究和实际应用带来有力推动。

Abstract: We present UniSH, a unified, feed-forward framework for joint metric-scale 3D scene and human reconstruction. A key challenge in this domain is the scarcity of large-scale, annotated real-world data, forcing a reliance on synthetic datasets. This reliance introduces a significant sim-to-real domain gap, leading to poor generalization, low-fidelity human geometry, and poor alignment on in-the-wild videos. To address this, we propose an innovative training paradigm that effectively leverages unlabeled in-the-wild data. Our framework bridges strong, disparate priors from scene reconstruction and HMR, and is trained with two core components: (1) a robust distillation strategy to refine human surface details by distilling high-frequency details from an expert depth model, and (2) a two-stage supervision scheme, which first learns coarse localization on synthetic data, then fine-tunes on real data by directly optimizing the geometric correspondence between the SMPL mesh and the human point cloud. This approach enables our feed-forward model to jointly recover high-fidelity scene geometry, human point clouds, camera parameters, and coherent, metric-scale SMPL bodies, all in a single forward pass. Extensive experiments demonstrate that our model achieves state-of-the-art performance on human-centric scene reconstruction and delivers highly competitive results on global human motion estimation, comparing favorably against both optimization-based frameworks and HMR-only methods. Project page: https://murphylmf.github.io/UniSH/

</details>


### [53] [Improved Object-Centric Diffusion Learning with Registers and Contrastive Alignment](https://arxiv.org/abs/2601.01224)
*Bac Nguyen,Yuhta Takida,Naoki Murata,Chieh-Hsin Lai,Toshimitsu Uesaka,Stefano Ermon,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 本文提出了CODA方法，通过引入寄存槽和对比对齐损失，改进Slot Attention与扩散模型结合下的物体中心学习，显著提升了物体发现、属性预测和合成能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于Slot Attention和预训练扩散模型的物体中心学习存在槽纠缠和槽与图像内容对齐不佳的问题，影响了物体表示的分辨性和准确性。作者希望通过更好地对齐槽与图像内容，实现更鲁棒、可扩展的物体中心学习。

Method: CODA方法包含两点：1）引入寄存槽来吸收残余注意力，减少槽间干扰；2）采用对比对齐损失，显式鼓励槽与图像对象一一对应。整体目标函数近似最大化槽与输入之间的互信息，提升槽的表征质量。

Result: 在合成数据（MOVi-C/E）和真实世界数据（VOC, COCO）上，CODA在物体发现（如在COCO上FG-ARI提升6.1%）、属性预测与合成图像生成上均超越现有方法，并且引入寄存槽几乎不增加计算开销。

Conclusion: CODA能高效、可扩展地实现复杂真实场景下的鲁棒物体中心学习，有望作为该领域的新型有效框架。

Abstract: Slot Attention (SA) with pretrained diffusion models has recently shown promise for object-centric learning (OCL), but suffers from slot entanglement and weak alignment between object slots and image content. We propose Contrastive Object-centric Diffusion Alignment (CODA), a simple extension that (i) employs register slots to absorb residual attention and reduce interference between object slots, and (ii) applies a contrastive alignment loss to explicitly encourage slot-image correspondence. The resulting training objective serves as a tractable surrogate for maximizing mutual information (MI) between slots and inputs, strengthening slot representation quality. On both synthetic (MOVi-C/E) and real-world datasets (VOC, COCO), CODA improves object discovery (e.g., +6.1% FG-ARI on COCO), property prediction, and compositional image generation over strong baselines. Register slots add negligible overhead, keeping CODA efficient and scalable. These results indicate potential applications of CODA as an effective framework for robust OCL in complex, real-world scenes.

</details>


### [54] [HyDRA: Hybrid Denoising Regularization for Measurement-Only DEQ Training](https://arxiv.org/abs/2601.01228)
*Markus Haltmeier,Lukas Neumann,Nadja Gruber,Johannes Schwab,Gyeongha Hwang*

Main category: cs.CV

TL;DR: 本文提出了一种名为HyDRA的新方法，可在仅有观测数据的条件下训练深度平衡（DEQ）模型用于图像重建，无需配对的监督数据。


<details>
  <summary>Details</summary>
Motivation: 图像重建问题因病态性和缺乏大规模有监督数据而具有挑战性，尤其在实际应用中往往只拿到观测数据，而没有真实图像和对应观测的配对数据。

Method: HyDRA方法结合了观测一致性与自适应去噪正则项，并设计了一种数据驱动的早停准则，允许在无监督的情况下对DEQ模型进行训练。

Result: 在稀疏视角CT的实验中，HyDRA获得了有竞争力的重建质量和较快的推理速度。

Conclusion: HyDRA为无监督条件下训练图像重建模型提供了有效的新框架，在实际应用中更具可行性和推广性。

Abstract: Solving image reconstruction problems of the form \(\mathbf{A} \mathbf{x} = \mathbf{y}\) remains challenging due to ill-posedness and the lack of large-scale supervised datasets. Deep Equilibrium (DEQ) models have been used successfully but typically require supervised pairs \((\mathbf{x},\mathbf{y})\). In many practical settings, only measurements \(\mathbf{y}\) are available. We introduce HyDRA (Hybrid Denoising Regularization Adaptation), a measurement-only framework for DEQ training that combines measurement consistency with an adaptive denoising regularization term, together with a data-driven early stopping criterion. Experiments on sparse-view CT demonstrate competitive reconstruction quality and fast inference.

</details>


### [55] [RFAssigner: A Generic Label Assignment Strategy for Dense Object Detection](https://arxiv.org/abs/2601.01240)
*Ziqian Guan,Xieyi Fu,Yuting Wang,Haowen Xiao,Jiarui Zhu,Yingying Zhu,Yongtao Liu,Lin Gu*

Main category: cs.CV

TL;DR: 本文提出了一种新的标签分配方法RFAssigner，通过更合理地为小目标分配正样本，提升密集目标检测器的多尺度学习能力，并在多个数据集上取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 现有的标签分配方法在训练密集目标检测器时，往往给小目标分配的正样本数量不足，导致不同尺度目标的学习不平衡，影响检测性能。

Method: RFAssigner采用两步分配策略：首先基于点的先验选取部分正样本，然后借助高斯感受野（GRF）距离，度量未分配样本与真实目标之间的感受野相似性，从未分配样本中自适应补充正样本，实现各尺度目标的均衡训练。

Result: 在三个具有不同目标尺度分布的数据集上，RFAssigner均展现出有效性和泛化能力。搭载RFAssigner的FCOS-ResNet-50单模型在所有尺度上都优于当前主流分配策略，无需额外模块或人工规则。

Conclusion: RFAssigner提高了密集目标检测器的多尺度兼容性，实现了更平衡和更优的目标检测性能，具有较强的实用价值和推广性。

Abstract: Label assignment is a critical component in training dense object detectors. State-of-the-art methods typically assign each training sample a positive and a negative weight, optimizing the assignment scheme during training. However, these strategies often assign an insufficient number of positive samples to small objects, leading to a scale imbalance during training. To address this limitation, we introduce RFAssigner, a novel assignment strategy designed to enhance the multi-scale learning capabilities of dense detectors. RFAssigner first establishes an initial set of positive samples using a point-based prior. It then leverages a Gaussian Receptive Field (GRF) distance to measure the similarity between the GRFs of unassigned candidate locations and the ground-truth objects. Based on this metric, RFAssigner adaptively selects supplementary positive samples from the unassigned pool, promoting a more balanced learning process across object scales. Comprehensive experiments on three datasets with distinct object scale distributions validate the effectiveness and generalizability of our method. Notably, a single FCOS-ResNet-50 detector equipped with RFAssigner achieves state-of-the-art performance across all object scales, consistently outperforming existing strategies without requiring auxiliary modules or heuristics.

</details>


### [56] [MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance](https://arxiv.org/abs/2601.01260)
*Hamad Khan,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 本文提出了一种高效混合专家模型MambaFormer，用于医学问答以及临床辅助，兼顾模型准确率和计算成本，在新设计的DentalQA及PubMedQA任务上，性能优于现有方法，且推理速度极快。


<details>
  <summary>Details</summary>
Motivation: 大模型在临床实际应用中受限于计算量高及延迟问题。作者希望通过模型结构和专家混合机制，实现在医疗领域下既高效又高准确率的问答系统。

Method: 提出MambaFormer混合专家结构，结合轻量门控机制动态分配不同类型子模型（Transformer专家、状态空间模型专家）处理不同复杂度与长度的输入。模型通过迁移学习在定制的DentalQA数据集上微调，并通过智能动态路由实现延迟和精度的帕累托最优。创新性地引入多目标损失函数，联合优化推断决策和计算成本。

Result: MambaFormer在DentalQA与PubMedQA数据集上进行了交叉验证，性能全面优于最先进方法，BERTScore达到0.9180，平均推理延迟0.077秒，相比T5-Large提升速度24.4倍。

Conclusion: MambaFormer混合专家模型为资源受限环境下的临床应用提供了高效、可扩展的方案，大大提高了推理速度并兼顾预测准确率。

Abstract: The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.

</details>


### [57] [AI-Powered Deepfake Detection Using CNN and Vision Transformer Architectures](https://arxiv.org/abs/2601.01281)
*Sifatullah Sheikh Urmi,Kirtonia Nuzath Tabassum Arthi,Md Al-Imran*

Main category: cs.CV

TL;DR: 本文评估了四种AI模型在深度伪造检测中的表现，特别是VFDNET与MobileNetV3的结合表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术不断进步，给数字内容的真实性带来巨大挑战，因此需要准确可靠的AI方法进行伪造检测。

Method: 作者利用三个卷积神经网络（CNN）和一个Vision Transformer模型，在大型人脸图像数据集上进行了评估，并通过数据预处理和增强来提升模型表现。

Result: 数据增强后，所有模型性能有所提升。其中，VFDNET结合MobileNetV3展现了最高的准确性和效率。

Conclusion: AI，尤其是以VFDNET与MobileNetV3为代表的模型，能够有效可靠地检测深度伪造，为维护数字真实性提供了有力的技术支撑。

Abstract: The increasing use of artificial intelligence generated deepfakes creates major challenges in maintaining digital authenticity. Four AI-based models, consisting of three CNNs and one Vision Transformer, were evaluated using large face image datasets. Data preprocessing and augmentation techniques improved model performance across different scenarios. VFDNET demonstrated superior accuracy with MobileNetV3, showing efficient performance, thereby demonstrating AI's capabilities for dependable deepfake detection.

</details>


### [58] [S2M-Net: Spectral-Spatial Mixing for Medical Image Segmentation with Morphology-Aware Adaptive Loss](https://arxiv.org/abs/2601.01285)
*Md. Sanaullah Chowdhury Lameya Sabrin*

Main category: cs.CV

TL;DR: 本文提出了一种高效且精准的医学图像分割网络S2M-Net，在多种医学图像任务上取得了突破性效果，并显著减少了参数量和计算消耗。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割架构难以同时兼顾边界精度、全局上下文和计算效率，特别是在小规模数据和有限硬件条件下，卷积网络和视觉Transformer各有缺陷。

Method: 提出S2M-Net，包含两个关键创新：(1) SSTM模块通过可学习的频域过滤和空间投影，用截断的2D FFT实现低成本的全局建模；(2) MASL损失函数根据结构特征自动调整多项分割损失，无需手动调参。

Result: 在包括8种模态的16个医学影像数据集上，S2M-Net取得了业界领先的分割效果：息肉96.12%、手术器械83.77%（比现有方法高17.85%）、脑肿瘤80.90%，且参数量仅为同类Transformer方法的1/3至1/6。

Conclusion: S2M-Net有效解决了分割三难问题，实现了高精度、高效率和自适应能力强的医学图像分割，在多任务和多模态下具有广泛应用价值。

Abstract: Medical image segmentation requires balancing local precision for boundary-critical clinical applications, global context for anatomical coherence, and computational efficiency for deployment on limited data and hardware a trilemma that existing architectures fail to resolve. Although convolutional networks provide local precision at $\mathcal{O}(n)$ cost but limited receptive fields, vision transformers achieve global context through $\mathcal{O}(n^2)$ self-attention at prohibitive computational expense, causing overfitting on small clinical datasets. We propose S2M-Net, a 4.7M-parameter architecture that achieves $\mathcal{O}(HW \log HW)$ global context through two synergistic innovations: (i) Spectral-Selective Token Mixer (SSTM), which exploits the spectral concentration of medical images via truncated 2D FFT with learnable frequency filtering and content-gated spatial projection, avoiding quadratic attention cost while maintaining global receptive fields; and (ii) Morphology-Aware Adaptive Segmentation Loss (MASL), which automatically analyzes structure characteristics (compactness, tubularity, irregularity, scale) to modulate five complementary loss components through constrained learnable weights, eliminating manual per-dataset tuning. Comprehensive evaluation in 16 medical imaging datasets that span 8 modalities demonstrates state-of-the-art performance: 96.12\% Dice on polyp segmentation, 83.77\% on surgical instruments (+17.85\% over the prior art) and 80.90\% on brain tumors, with consistent 3-18\% improvements over specialized baselines while using 3.5--6$\times$ fewer parameters than transformer-based methods.

</details>


### [59] [VReID-XFD: Video-based Person Re-identification at Extreme Far Distance Challenge Results](https://arxiv.org/abs/2601.01312)
*Kailash A. Hambarde,Hugo Proença,Md Rashidunnabi,Pranita Samale,Qiwei Yang,Pingping Zhang,Zijing Gong,Yuhao Wang,Xi Zhang,Ruoshui Qu,Qiaoyun He,Yuhang Zhang,Thi Ngoc Ha Nguyen,Tien-Dung Mai,Cheng-Jun Kang,Yu-Fan Lin,Jin-Hui Jiang,Chih-Chung Hsu,Tamás Endrei,György Cserey,Ashwat Rajbhandari*

Main category: cs.CV

TL;DR: 本论文提出了一个针对极远距离（XFD）空地行人再识别的新基准VReID-XFD，分析了现有方法在极端视角、分辨率退化和多变动态下性能急剧下降的问题，且即使最优方法在此基准上的表现也很有限。


<details>
  <summary>Details</summary>
Motivation: 传统行人再识别方法假设视角、分辨率和外观在一定范围内可控，但在极远距离空地视角下，这些假设不成立，给现有系统带来了巨大挑战，因此需要建立新基准以研究和推动该领域的发展。

Method: 作者基于DetReIDX数据集，构建了VReID-XFD数据集，涵盖多种无人机高度、角度、距离和丰富物理元数据。基准支持多种任务设置（空对空、空对地、地对空），且数据集和身份分割严格无重叠。同时组织了公开挑战，吸引了10支队伍参与提交，系统性分析了各类性能因素。

Result: 分析显示，随着飞行高度与距离增加，ReID性能单调下降，并且垂直俯视（nadir）视角普遍表现最差。当前最佳方法SAS-PReID在空对地场景下的mAP仅为43.93%。此外还观察到性能与系统鲁棒性之间存在权衡。

Conclusion: 极远距离空地行人再识别在现有技术下难度极大，常规方法在此场景下表现不佳。该基准和挑战为后续研究提供了实验平台，也暴露出该领域亟需解决的难点。

Abstract: Person re-identification (ReID) across aerial and ground views at extreme far distances introduces a distinct operating regime where severe resolution degradation, extreme viewpoint changes, unstable motion cues, and clothing variation jointly undermine the appearance-based assumptions of existing ReID systems. To study this regime, we introduce VReID-XFD, a video-based benchmark and community challenge for extreme far-distance (XFD) aerial-to-ground person re-identification. VReID-XFD is derived from the DetReIDX dataset and comprises 371 identities, 11,288 tracklets, and 11.75 million frames, captured across altitudes from 5.8 m to 120 m, viewing angles from oblique (30 degrees) to nadir (90 degrees), and horizontal distances up to 120 m. The benchmark supports aerial-to-aerial, aerial-to-ground, and ground-to-aerial evaluation under strict identity-disjoint splits, with rich physical metadata. The VReID-XFD-25 Challenge attracted 10 teams with hundreds of submissions. Systematic analysis reveals monotonic performance degradation with altitude and distance, a universal disadvantage of nadir views, and a trade-off between peak performance and robustness. Even the best-performing SAS-PReID method achieves only 43.93 percent mAP in the aerial-to-ground setting. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/ .

</details>


### [60] [LinMU: Multimodal Understanding Made Linear](https://arxiv.org/abs/2601.01322)
*Hongjie Wang,Niraj K. Jha*

Main category: cs.CV

TL;DR: 本文提出LinMU，一种具有线性复杂度的新型视觉-语言模型（VLM）架构，通过用M-MATE模块替换自注意力，实现高效推理，显著提升大分辨率图片和长视频上的处理速度，并维持性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM虽然效果出色，但由于自注意力机制的二次复杂度，难以在边缘设备或高分辨率、长时长任务中高效应用；解决复杂度瓶颈，有助于向更广泛的实际场景推广。

Method: 用M-MATE（包括Flex-MA和Local-Swin两分支）模块替换VLM中所有自注意力层，实现全局与局部信息建模来保持表达力。提出三阶段蒸馏策略，将预训练VLM迁移到LinMU，包括初始化、分支微调和整体微调，并用teacher模型的隐藏状态和logits作回归目标。

Result: 在MMMU、TextVQA、LongVideoBench等多个基准测试中，LinMU性能与原模型相当，但TTFT最多提速2.7倍，token吞吐量最多提升9倍，单独消融实验也验证了核心设计的有效性。

Conclusion: 无需二次复杂度的注意力机制也能实现主流VLM性能，显著提升了对高分辨率图片和长视频的处理能力，有助于大规模多模态应用落地。

Abstract: Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\times$ and improves token throughput by up to 9.0$\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.

</details>


### [61] [Achieving Fine-grained Cross-modal Understanding through Brain-inspired Hierarchical Representation Learning](https://arxiv.org/abs/2601.01339)
*Weihang You,Hanqi Jiang,Yi Pan,Junhao Chen,Tianming Liu,Fei Dou*

Main category: cs.CV

TL;DR: 本文提出了NeuroAlign框架，通过模仿人类视觉系统的分层组织，实现了fMRI与视频数据的细粒度比对，并在跨模态检索任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前神经响应与视觉刺激之间的关系复杂，现有方法难以体现视觉加工的层次性与时序性，导致对大脑表征和模态间差距的理解有限。

Method: NeuroAlign采用两阶段机制：首先利用神经-时序对比学习（NTCL）实现全局语义理解，显式建模模态间的时序动态，接着通过增强向量量化（VQ）实现细粒度模式匹配，并引入DynaSyncMM-EMA方法进行多模态动态融合和自适应加权。

Result: 实验表明，NeuroAlign在跨模态检索任务中取得了远超现有方法的表现。

Conclusion: NeuroAlign为理解视觉认知机制提供了新范式，能够更精细地捕捉和对齐大脑神经活动与视觉刺激之间的层次和时序关系。

Abstract: Understanding neural responses to visual stimuli remains challenging due to the inherent complexity of brain representations and the modality gap between neural data and visual inputs. Existing methods, mainly based on reducing neural decoding to generation tasks or simple correlations, fail to reflect the hierarchical and temporal processes of visual processing in the brain. To address these limitations, we present NeuroAlign, a novel framework for fine-grained fMRI-video alignment inspired by the hierarchical organization of the human visual system. Our framework implements a two-stage mechanism that mirrors biological visual pathways: global semantic understanding through Neural-Temporal Contrastive Learning (NTCL) and fine-grained pattern matching through enhanced vector quantization. NTCL explicitly models temporal dynamics through bidirectional prediction between modalities, while our DynaSyncMM-EMA approach enables dynamic multi-modal fusion with adaptive weighting. Experiments demonstrate that NeuroAlign significantly outperforms existing methods in cross-modal retrieval tasks, establishing a new paradigm for understanding visual cognitive mechanisms.

</details>


### [62] [Slot-ID: Identity-Preserving Video Generation from Reference Videos via Slot-Based Temporal Identity Encoding](https://arxiv.org/abs/2601.01352)
*Yixuan Lai,He Wang,Kun Zhou,Tianjia Shao*

Main category: cs.CV

TL;DR: 本文提出了一种结合扩散模型和Transformer的视频生成方法，可生成能保持用户指定身份且符合提示的高质量视频，其关键技术在于结合了短参考视频以捕捉身份动态特征。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅基于单张图片进行身份迁移视频生成，无法捕捉人物在时间维度上的动态特征，导致动作生硬、面部失真且身份特征不明显。因此，提出新的方法以提升身份一致性和运动自然性。

Method: 方法基于扩散-Transformer视频生成架构，通过引入短参考视频，而非单张图像。设计了Sinkhorn路由编码器，从短视频中提取能代表身份的动态Token，在不破坏原有预训练主干兼容性的前提下，轻量级地融入特征，实现了动态细节的学习与表达。

Result: 实验表明，在实现多种姿态和丰富面部表情条件下，相较于传统方法，该方法更好地保持了人物身份准确性，以及生成结果的提示一致性和真实感。

Conclusion: 引入参考视频以捕获身份特定的动态特征，为个性化提示一致视频生成提供了有效的新方案，在身份保持与画面自然性之间取得了良好平衡。

Abstract: Producing prompt-faithful videos that preserve a user-specified identity remains challenging: models need to extrapolate facial dynamics from sparse reference while balancing the tension between identity preservation and motion naturalness. Conditioning on a single image completely ignores the temporal signature, which leads to pose-locked motions, unnatural warping, and "average" faces when viewpoints and expressions change. To this end, we introduce an identity-conditioned variant of a diffusion-transformer video generator which uses a short reference video rather than a single portrait. Our key idea is to incorporate the dynamics in the reference. A short clip reveals subject-specific patterns, e.g., how smiles form, across poses and lighting. From this clip, a Sinkhorn-routed encoder learns compact identity tokens that capture characteristic dynamics while remaining pretrained backbone-compatible. Despite adding only lightweight conditioning, the approach consistently improves identity retention under large pose changes and expressive facial behavior, while maintaining prompt faithfulness and visual realism across diverse subjects and prompts.

</details>


### [63] [Advanced Machine Learning Approaches for Enhancing Person Re-Identification Performance](https://arxiv.org/abs/2601.01356)
*Dang H. Pham,Tu N. Nguyen,Hoa N. Nguyen*

Main category: cs.CV

TL;DR: 本文提出了三种先进方法，分别针对有监督、无监督领域自适应和完全无监督场景下的人体重识别（ReID），均显著提升性能，验证了其在多个主流数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 人体重识别在智能监控系统中至关重要，但面临外观变化、领域偏移和标注数据稀缺等挑战，亟需提升其跨环境的泛化与表征能力。

Method: 1. 在有监督场景下，提出SCM-ReID方法，将监督对比学习与多种损失函数（分类损失、中心损失、三元组损失和质心三元组损失）结合，提升特征判别力。2. 针对无监督领域自适应（UDA），提出IQAGA和DAPRH方法，采用GAN图像增强、领域不变特征映射和伪标签细化，有效减小领域差异，提升跨域泛化能力。3. 对于完全无监督场景，提出ViTC-UReID，利用Vision Transformer进行特征编码，并结合摄像头感知的代理学习，以全局和局部注意力及相机约束显著增强无监督性能。

Result: 上述方法在CUHK03、Market-1501、DukeMTMC-reID和MSMT17等主流数据集上广泛实验，均优于现有技术，尤其UDA场景下mAP和Rank-1均提升高达12%。

Conclusion: 提出的方法在特征学习、领域自适应和标签噪声处理等关键难题上取得突破，极大增强了ReID系统在现实监控环境中的鲁棒性和实用性，为该领域的发展提供了有力支撑。

Abstract: Person re-identification (ReID) plays a critical role in intelligent surveillance systems by linking identities across multiple cameras in complex environments. However, ReID faces significant challenges such as appearance variations, domain shifts, and limited labeled data. This dissertation proposes three advanced approaches to enhance ReID performance under supervised, unsupervised domain adaptation (UDA), and fully unsupervised settings. First, SCM-ReID integrates supervised contrastive learning with hybrid loss optimization (classification, center, triplet, and centroid-triplet losses), improving discriminative feature representation and achieving state-of-the-art accuracy on Market-1501 and CUHK03 datasets. Second, for UDA, IQAGA and DAPRH combine GAN-based image augmentation, domain-invariant mapping, and pseudo-label refinement to mitigate domain discrepancies and enhance cross-domain generalization. Experiments demonstrate substantial gains over baseline methods, with mAP and Rank-1 improvements up to 12% in challenging transfer scenarios. Finally, ViTC-UReID leverages Vision Transformer-based feature encoding and camera-aware proxy learning to boost unsupervised ReID. By integrating global and local attention with camera identity constraints, this method significantly outperforms existing unsupervised approaches on large-scale benchmarks. Comprehensive evaluations across CUHK03, Market-1501, DukeMTMC-reID, and MSMT17 confirm the effectiveness of the proposed methods. The contributions advance ReID research by addressing key limitations in feature learning, domain adaptation, and label noise handling, paving the way for robust deployment in real-world surveillance systems.

</details>


### [64] [Garment Inertial Denoiser (GID): Endowing Accurate Motion Capture via Loose IMU Denoiser](https://arxiv.org/abs/2601.01360)
*Jiawei Fang,Ruonan Zheng,Xiaoxia Gao,Shifan Jiang,Anjun Chen,Qi Ye,Shihui Guo*

Main category: cs.CV

TL;DR: 论文提出了GID（Garment Inertial Denoiser）方法，利用Transformer架构解决松身服饰下IMU信号的噪声问题，实现了更准确和实用的可穿戴惯性动作捕捉系统。


<details>
  <summary>Details</summary>
Motivation: 传统基于惯性传感器（IMU）的动作捕捉系统需紧贴皮肤佩戴，令日常穿戴不舒适且不便于推广。将IMU整合入宽松服装虽更实用，却导致传感器与身体位移严重，造成数据污染，损坏标准惯性动作捕捉流程。目前缺乏有效方法处理宽松服饰下的IMU数据噪声问题。

Method: 提出了GID（Garment Inertial Denoiser）——一种轻量级、即插即用的Transformer模型，分为三步：1）针对传感器位置的专用去噪；2）自适应融合各传感器的穿戴动态；3）整合信息预测全身姿态。该方法采用共享时空主干网络处理全局动作，再以每个IMU为单位用专家网络捕捉局部服装动态，最后通过轻量融合模块保证跨部位一致性。同时，团队发布了GarMoCap数据集，提高模型泛化能力。

Result: 实验表明，GID在单人训练下即可实现准确且实时的动作去噪，并能泛化到不同的用户、动作和服装类型，对比现有主流方法表现更优，可作为即插即用模块提升现有惯性动作捕捉系统的性能。

Conclusion: GID方法有效解决了宽松服装下惯性动作捕捉的信号噪声难题，实现准确、实时和泛化性强的动作识别，推动可穿戴惯性动作捕捉向实际应用迈进一步。

Abstract: Wearable inertial motion capture (MoCap) provides a portable, occlusion-free, and privacy-preserving alternative to camera-based systems, but its accuracy depends on tightly attached sensors - an intrusive and uncomfortable requirement for daily use. Embedding IMUs into loose-fitting garments is a desirable alternative, yet sensor-body displacement introduces severe, structured, and location-dependent corruption that breaks standard inertial pipelines. We propose GID (Garment Inertial Denoiser), a lightweight, plug-and-play Transformer that factorizes loose-wear MoCap into three stages: (i) location-specific denoising, (ii) adaptive cross-wear fusion, and (iii) general pose prediction. GID uses a location-aware expert architecture, where a shared spatio-temporal backbone models global motion while per-IMU expert heads specialize in local garment dynamics, and a lightweight fusion module ensures cross-part consistency. This inductive bias enables stable training and effective learning from limited paired loose-tight IMU data. We also introduce GarMoCap, a combined public and newly collected dataset covering diverse users, motions, and garments. Experiments show that GID enables accurate, real-time denoising from single-user training and generalizes across unseen users, motions, and garment types, consistently improving state-of-the-art inertial MoCap methods when used as a drop-in module.

</details>


### [65] [Unsupervised SE(3) Disentanglement for in situ Macromolecular Morphology Identification from Cryo-Electron Tomography](https://arxiv.org/abs/2601.01364)
*Mostofa Rafid Uddin,Mahek Vora,Qifeng Wu,Muyuan Chen,Min Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种用于冷冻电镜断层成像数据的深度表征学习框架，能够有效分离三维空间变换和分子形态，并发现以前未识别的结构形态。


<details>
  <summary>Details</summary>
Motivation: 现有利用期望最大化方法解决冷冻电镜数据中形态估计的逆问题时，容易遗漏稀有但重要的分子形态，并且需要大量手动超参数调整，导致其效率和全面性不足。

Method: 作者提出了一种解耦的深度表征学习框架，在表征空间中将SE(3)空间变换与分子形态内容分离，同时设计一种新颖的多选学习模块以适应高噪声的断层数据，利用学习到的表征生成模板结构。

Result: 在模拟和真实的冷冻电镜断层数据集上，实验结果显示该方法在发现新的大分子形态以及分离空间变换和形态内容方面优于以往方法。

Conclusion: 该方法有效提升了冷冻电镜断层数据中形态估计的准确性和全面性，为结构生物学研究提供了更强的数据驱动工具。

Abstract: Cryo-electron tomography (cryo-ET) provides direct 3D visualization of macromolecules inside the cell, enabling analysis of their in situ morphology. This morphology can be regarded as an SE(3)-invariant, denoised volumetric representation of subvolumes extracted from tomograms. Inferring morphology is therefore an inverse problem of estimating both a template morphology and its SE(3) transformation. Existing expectation-maximization based solution to this problem often misses rare but important morphologies and requires extensive manual hyperparameter tuning. Addressing this issue, we present a disentangled deep representation learning framework that separates SE(3) transformations from morphological content in the representation space. The framework includes a novel multi-choice learning module that enables this disentanglement for highly noisy cryo-ET data, and the learned morphological content is used to generate template morphologies. Experiments on simulated and real cryo-ET datasets demonstrate clear improvements over prior methods, including the discovery of previously unidentified macromolecular morphologies.

</details>


### [66] [ParkGaussian: Surround-view 3D Gaussian Splatting for Autonomous Parking](https://arxiv.org/abs/2601.01386)
*Xiaobao Wei,Zhangjie Ye,Yuxiang Gu,Zunjie Zhu,Yunfei Guo,Yingying Shen,Shan Zhao,Ming Lu,Haiyang Sun,Bing Wang,Guang Chen,Rongfeng Lu,Hangjun Ye*

Main category: cs.CV

TL;DR: 论文提出了第一个专为停车场三维重建设计的基准数据集ParkRecon3D，并基于3D高斯泼溅方法提出了ParkGaussian框架，在重建和感知下游任务之间实现更好的一致性。实验表明其在三维重建质量和下游感知任务上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在拥挤、无GPS信号的停车场环境下面临独特挑战，现有研究多聚焦二维感知和定位，三维重建较少涉及，但对于捕捉复杂空间几何结构十分关键。提升停车场三维重建的感知一致性与质量，对于提升自动泊车能力至关重要。

Method: 1）建立并公开了首个停车场三维重建数据集ParkRecon3D，含多视角鱼眼相机和泊车位标注。2）提出ParkGaussian框架，将3D Gaussian Splatting应用于停车场三维重建。3）引入slot-aware重建策略，结合泊车位感知方法提升重点区域合成质量。

Result: 在ParkRecon3D上，ParkGaussian框架在三维重建效果上达到SOTA，并且在下游泊车位检测等任务上感知一致性更佳。

Conclusion: 论文提出的新数据集和方法能够更好地捕捉停车场三维几何结构，促进三维重建与感知任务协同发展，为实际自动停车系统带来性能提升。

Abstract: Parking is a critical task for autonomous driving systems (ADS), with unique challenges in crowded parking slots and GPS-denied environments. However, existing works focus on 2D parking slot perception, mapping, and localization, 3D reconstruction remains underexplored, which is crucial for capturing complex spatial geometry in parking scenarios. Naively improving the visual quality of reconstructed parking scenes does not directly benefit autonomous parking, as the key entry point for parking is the slots perception module. To address these limitations, we curate the first benchmark named ParkRecon3D, specifically designed for parking scene reconstruction. It includes sensor data from four surround-view fisheye cameras with calibrated extrinsics and dense parking slot annotations. We then propose ParkGaussian, the first framework that integrates 3D Gaussian Splatting (3DGS) for parking scene reconstruction. To further improve the alignment between reconstruction and downstream parking slot detection, we introduce a slot-aware reconstruction strategy that leverages existing parking perception methods to enhance the synthesis quality of slot regions. Experiments on ParkRecon3D demonstrate that ParkGaussian achieves state-of-the-art reconstruction quality and better preserves perception consistency for downstream tasks. The code and dataset will be released at: https://github.com/wm-research/ParkGaussian

</details>


### [67] [Evaluation of Convolutional Neural Network For Image Classification with Agricultural and Urban Datasets](https://arxiv.org/abs/2601.01393)
*Shamik Shafkat Avro,Nazira Jesmin Lina,Shahanaz Sharmin*

Main category: cs.CV

TL;DR: 本文提出了一种自定义卷积神经网络（CustomCNN），专注于多领域图像分类任务，并在五个公开数据集上进行了评估，结果显示该方法在保持计算效率的同时取得了具有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 随着智能城市和农业图像应用需求增长，对能适应不同任务需求、简洁高效的CNN结构有很大需求。现有主流CNN模型虽表现优异，但在特定应用上未必优化，因此探讨结构设计对实际多领域任务的影响具有现实意义。

Method: 作者设计了一种结合残差连接、Squeeze-and-Excitation注意力机制、通道递进扩展和Kaiming初始化的自定义CNN结构。在五个来自不同领域的数据集上（例如违规车辆检测、道路损伤检测、农作物品种识别等）进行训练与测试，并与主流CNN架构进行对比。

Result: CustomCNN在多领域任务上展现了竞争力，不仅在准确率方面表现良好，同时保持了较高的计算效率。与其他主流CNN相比，CustomCNN兼顾了性能和资源消耗。

Conclusion: 本文验证了针对应用领域优化CNN结构能提升实际效果，支持了在智慧城市和农业图像场景中采用定制化网络结构的可行性和必要性。

Abstract: This paper presents the development and evaluation of a custom Convolutional Neural Network (CustomCNN) created to study how architectural design choices affect multi-domain image classification tasks. The network uses residual connections, Squeeze-and-Excitation attention mechanisms, progressive channel scaling, and Kaiming initialization to improve its ability to represent data and speed up training. The model is trained and tested on five publicly available datasets: unauthorized vehicle detection, footpath encroachment detection, polygon-annotated road damage and manhole detection, MangoImageBD and PaddyVarietyBD. A comparison with popular CNN architectures shows that the CustomCNN delivers competitive performance while remaining efficient in computation. The results underscore the importance of thoughtful architectural design for real-world Smart City and agricultural imaging applications.

</details>


### [68] [SwinIFS: Landmark Guided Swin Transformer For Identity Preserving Face Super Resolution](https://arxiv.org/abs/2601.01406)
*Habiba Kausar,Saeed Anwar,Omar Jamal Hammad,Abdul Bais*

Main category: cs.CV

TL;DR: 提出了一种名为SwinIFS的人脸超分辨率框架，通过引入人脸关键点热力图和Swin Transformer主干，实现了高质量、身份保持的人脸重建，尤其在高倍率放大任务上效果显著。


<details>
  <summary>Details</summary>
Motivation: 人脸超分任务在高倍率下细节和身份信息易丢失，现有方法很难在恢复高质量结构细节和保留身份特征间取得平衡。

Method: 采用结构先验（关键点高斯热力图）引导模型注意语义重要区域，结合紧凑Swin Transformer主干网络，既能捕捉全局上下文也能保障局部结构准确。

Result: 在CelebA数据集上，SwinIFS展现出感知质量、清晰度与身份保持的全面优越，尤其在8x放大下能恢复有意义的结构，优于主流方法。

Conclusion: SwinIFS实现了卓越的人脸超分辨率效果，并兼顾准确性与效率，有现实应用潜力。代码和模型已开源，便于复现。

Abstract: Face super-resolution aims to recover high-quality facial images from severely degraded low-resolution inputs, but remains challenging due to the loss of fine structural details and identity-specific features. This work introduces SwinIFS, a landmark-guided super-resolution framework that integrates structural priors with hierarchical attention mechanisms to achieve identity-preserving reconstruction at both moderate and extreme upscaling factors. The method incorporates dense Gaussian heatmaps of key facial landmarks into the input representation, enabling the network to focus on semantically important facial regions from the earliest stages of processing. A compact Swin Transformer backbone is employed to capture long-range contextual information while preserving local geometry, allowing the model to restore subtle facial textures and maintain global structural consistency. Extensive experiments on the CelebA benchmark demonstrate that SwinIFS achieves superior perceptual quality, sharper reconstructions, and improved identity retention; it consistently produces more photorealistic results and exhibits strong performance even under 8x magnification, where most methods fail to recover meaningful structure. SwinIFS also provides an advantageous balance between reconstruction accuracy and computational efficiency, making it suitable for real-world applications in facial enhancement, surveillance, and digital restoration. Our code, model weights, and results are available at https://github.com/Habiba123-stack/SwinIFS.

</details>


### [69] [Mask-Guided Multi-Task Network for Face Attribute Recognition](https://arxiv.org/abs/2601.01408)
*Gong Gao,Zekai Wang,Jian Zhao,Ziqi Xie,Xianhui Liu,Weidong Zhao*

Main category: cs.CV

TL;DR: 文中提出了一种新的面部属性识别方法，通过选取特定的特征区域进行高效的特征学习，并提出了面具引导的多任务网络（MGMTN），显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 传统的多任务面部属性识别方法依赖对整个特征图进行全局处理，容易引入冗余特征，影响识别精度。为了解决冗余和全局区域带来的负迁移问题，需要对关键面部区域进行更精准的特征提取。

Method: 提出Mask-Guided Multi-Task Network (MGMTN)，整合了自适应掩码学习（AML）和群组-全局特征融合（G2FF）。AML利用预训练关键点模型和全卷积网络，生成精确定位的面部关键部位组掩码。G2FF则融合分组与全局特征，提升属性识别能力。

Result: 在两个具挑战性的面部属性识别数据集上进行了大量实验，结果表明所提方法MGMTN显著提升了面部属性识别性能。

Conclusion: MGMTN能够有效缓解全局特征带来的负迁移，通过掩码引导的分组特征学习和特征融合显著提升了面部属性识别的准确率，具有较强的实际应用前景。

Abstract: Face Attribute Recognition (FAR) plays a crucial role in applications such as person re-identification, face retrieval, and face editing. Conventional multi-task attribute recognition methods often process the entire feature map for feature extraction and attribute classification, which can produce redundant features due to reliance on global regions. To address these challenges, we propose a novel approach emphasizing the selection of specific feature regions for efficient feature learning. We introduce the Mask-Guided Multi-Task Network (MGMTN), which integrates Adaptive Mask Learning (AML) and Group-Global Feature Fusion (G2FF) to address the aforementioned limitations. Leveraging a pre-trained keypoint annotation model and a fully convolutional network, AML accurately localizes critical facial parts (e.g., eye and mouth groups) and generates group masks that delineate meaningful feature regions, thereby mitigating negative transfer from global region usage. Furthermore, G2FF combines group and global features to enhance FAR learning, enabling more precise attribute identification. Extensive experiments on two challenging facial attribute recognition datasets demonstrate the effectiveness of MGMTN in improving FAR performance.

</details>


### [70] [AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval](https://arxiv.org/abs/2601.01416)
*Yue Zhou,Ran Ding,Xue Yang,Xue Jiang,Xingzhao Liu*

Main category: cs.CV

TL;DR: 提出了一个专注无人机汽车图像的新型空间感知数据集AirSpatial，并开发了能更好进行空间理解的视觉-语言模型及智能体。


<details>
  <summary>Details</summary>
Motivation: 现有遥感视觉-语言模型（VLMs）空间理解能力有限，阻碍了其在实际场景的应用。提升其空间理解能力，特别是在无人机汽车图像场景下，有助于拓展VLMs实际应用边界。

Method: 构建了包含20多万条指令的新数据集AirSpatial，引入两个新任务：空间定位和空间问答，并首次在遥感数据集中提供3D包围盒（3DBB）。采用两阶段训练策略：首先进行图像理解预训练，再做空间理解微调。基于训练好的空间感知VLM，开发了具备任务规划、图像理解、空间理解与执行能力的AirSpatialBot智能体。

Result: 实验结果验证了所提方案有效性，揭示了现有VLMs的空间局限性，并带来有价值的分析洞见。

Conclusion: 提出的数据集和方法显著增强了VLMs在遥感场景的空间理解能力，为相关真实应用奠定基础。模型、代码和数据集将开放共享。

Abstract: Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot

</details>


### [71] [DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer](https://arxiv.org/abs/2601.01425)
*Xu Guo,Fulong Ye,Xinghui Li,Pengqi Tu,Pengze Zhang,Qichao Sun,Songtao Zhao,Xiangwang Hou,Qian He*

Main category: cs.CV

TL;DR: 该论文提出DreamID-V，一个基于Diffusion Transformer的视频换脸框架，通过创新数据管线和多种机制提升身份一致性与属性保留，在多个任务和基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频换脸技术难以同时实现身份逼真、属性保留和时序一致，为此需要将图像换脸领域的优点高效迁移到视频换脸任务中。

Method: 1. 推出SyncID-Pipe数据管线，结合身份锚定视频合成器与图像换脸模型构建带有双向ID监督的数据集。2. 提出DreamID-V，基于Diffusion Transformer，并引入模态感知条件注入模块。3. 设计Synthetic-to-Real教学机制和身份一致性强化学习策略提升视觉真实感和身份一致性。4. 构建IDBench-V基准测试集，涵盖多样场景。

Result: DreamID-V在实验中在身份一致性、视觉效果、时序连续性等方面均超越业内主流方法，并展现了良好的多任务适应性。

Conclusion: DreamID-V将图像换脸的优势成功迁移到视频域，显著提升了视频换脸的真实性和一致性，可作为多种换脸任务的通用模块，推动了领域发展。

Abstract: Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.

</details>


### [72] [EdgeNeRF: Edge-Guided Regularization for Neural Radiance Fields from Sparse Views](https://arxiv.org/abs/2601.01431)
*Weiqi Yu,Yiyang Yao,Lin He,Jianming Lv*

Main category: cs.CV

TL;DR: 本文提出了一种名为EdgeNeRF的边缘引导稀疏视角3D重建算法，通过对非边缘区域进行深度和法线正则约束，在抑制伪影的同时保留边界细节，对LLFF和DTU数据集实验展现了优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF方法在稠密多视角下表现优秀，但在稀疏输入时易出现几何伪影，已有方法采用全局深度正则化会损失边界细节。作者希望在提升稀疏输入下重建质量的同时，保留几何边界细节。

Method: EdgeNeRF先从输入图像中提取边缘信息，并将深度与法线正则限制只应用于非边缘区域，从而在非边缘区域增强几何一致性，在边界处保留高频细节。此外，该方法的边缘引导深度正则模块可作为插件集成到其他方法中。

Result: 在LLFF和DTU主流数据集上进行实验，EdgeNeRF在压制伪影、边界清晰度等方面优于现有方法。引入边缘引导正则模块后，其他方法也能显著提升性能且训练时长变化不大。

Conclusion: EdgeNeRF通过边缘引导正则有效改善稀疏输入下NeRF的重建质量，保留了高质量的几何边界细节，并且易于集成到其他方法中，具有较强的实际应用潜力。

Abstract: Neural Radiance Fields (NeRF) achieve remarkable performance in dense multi-view scenarios, but their reconstruction quality degrades significantly under sparse inputs due to geometric artifacts. Existing methods utilize global depth regularization to mitigate artifacts, leading to the loss of geometric boundary details. To address this problem, we propose EdgeNeRF, an edge-guided sparse-view 3D reconstruction algorithm. Our method leverages the prior that abrupt changes in depth and normals generate edges. Specifically, we first extract edges from input images, then apply depth and normal regularization constraints to non-edge regions, enhancing geometric consistency while preserving high-frequency details at boundaries. Experiments on LLFF and DTU datasets demonstrate EdgeNeRF's superior performance, particularly in retaining sharp geometric boundaries and suppressing artifacts. Additionally, the proposed edge-guided depth regularization module can be seamlessly integrated into other methods in a plug-and-play manner, significantly improving their performance without substantially increasing training time. Code is available at https://github.com/skyhigh404/edgenerf.

</details>


### [73] [In defense of the two-stage framework for open-set domain adaptive semantic segmentation](https://arxiv.org/abs/2601.01439)
*Wenqi Ren,Weijie Wang,Meng Zheng,Ziyan Wu,Yang Tang,Zhun Zhong,Nicu Sebe*

Main category: cs.CV

TL;DR: 本文提出了一种名为SATS的新训练策略，有效提升了开放集领域自适应语义分割（OSDA-SS）的性能。


<details>
  <summary>Details</summary>
Motivation: OSDA-SS任务既要适应已知类别，又需区分未知类别，但现有方法“一步到位”的设计，因已知与未知类别的标注不均而导致性能下降。

Method: SATS将训练分为“分离已知/未知”和“感知未知的领域自适应”两步，提升了对未知类别的识别能力。同时提出“困难未知探索”数据增强方法，增强模型对难识别未知类别的学习。

Result: 方法在GTA5-to-Cityscapes和SYNTHIA-to-Cityscapes公开基准上，H-Score分别提升了3.85%和18.64%，超越了现有方法。

Conclusion: 分阶段处理域自适应和引入专门的未知增强，有效缓解了以往方法的不足，显著提升了OSDA-SS模型性能。

Abstract: Open-Set Domain Adaptation for Semantic Segmentation (OSDA-SS) presents a significant challenge, as it requires both domain adaptation for known classes and the distinction of unknowns. Existing methods attempt to address both tasks within a single unified stage. We question this design, as the annotation imbalance between known and unknown classes often leads to negative transfer of known classes and underfitting for unknowns. To overcome these issues, we propose SATS, a Separating-then-Adapting Training Strategy, which addresses OSDA-SS through two sequential steps: known/unknown separation and unknown-aware domain adaptation. By providing the model with more accurate and well-aligned unknown classes, our method ensures a balanced learning of discriminative features for both known and unknown classes, steering the model toward discovering truly unknown objects. Additionally, we present hard unknown exploration, an innovative data augmentation method that exposes the model to more challenging unknowns, strengthening its ability to capture more comprehensive understanding of target unknowns. We evaluate our method on public OSDA-SS benchmarks. Experimental results demonstrate that our method achieves a substantial advancement, with a +3.85% H-Score improvement for GTA5-to-Cityscapes and +18.64% for SYNTHIA-to-Cityscapes, outperforming previous state-of-the-art methods.

</details>


### [74] [PartImageNet++ Dataset: Enhancing Visual Models with High-Quality Part Annotations](https://arxiv.org/abs/2601.01454)
*Xiao Li,Zilong Liu,Yining Liu,Zhuhong Li,Na Dong,Sitian Qin,Xiaolin Hu*

Main category: cs.CV

TL;DR: 本文提出了PartImageNet++，为ImageNet-1K所有类别提供了详细的部件注释，共计10万张图片，并结合该数据集提出了多尺度部件监督识别模型（MPM），在提升识别和分割等任务性能方面效果显著。


<details>
  <summary>Details</summary>
Motivation: 目前公开数据集中高质量部件级标注稀缺，限制了细粒度识别和其他相关任务的进展。为此，作者希望构建大规模、多样化且具备部件级标注的数据集，以支持更丰富的研究。

Method: 作者首先构建了PIN++数据集，每个类别人工标注100张图片，并训练了部件分割网络，把预测得到的伪部件标签用于未标注数据。提出了MPM模型，将传统识别架构与辅助旁路层结合，通过伪部件标签和真实部件标注联合监督。

Result: 在PIN++上进行了部件分割、目标分割和小样本学习等实验，结果显示他们的方法显著提升了部件识别模型的鲁棒性，并在各种下游任务上建立了强基线。

Conclusion: 高质量的部件标注有助于提升视觉模型性能，PIN++为相关研究提供了重要资源和新的评测基线，并证明了部件信息在多任务中的价值。

Abstract: To address the scarcity of high-quality part annotations in existing datasets, we introduce PartImageNet++ (PIN++), a dataset that provides detailed part annotations for all categories in ImageNet-1K. With 100 annotated images per category, totaling 100K images, PIN++ represents the most comprehensive dataset covering a diverse range of object categories. Leveraging PIN++, we propose a Multi-scale Part-supervised recognition Model (MPM) for robust classification on ImageNet-1K. We first trained a part segmentation network using PIN++ and used it to generate pseudo part labels for the remaining unannotated images. MPM then integrated a conventional recognition architecture with auxiliary bypass layers, jointly supervised by both pseudo part labels and the original part annotations. Furthermore, we conducted extensive experiments on PIN++, including part segmentation, object segmentation, and few-shot learning, exploring various ways to leverage part annotations in downstream tasks. Experimental results demonstrated that our approach not only enhanced part-based models for robust object recognition but also established strong baselines for multiple downstream tasks, highlighting the potential of part annotations in improving model performance. The dataset and the code are available at https://github.com/LixiaoTHU/PartImageNetPP.

</details>


### [75] [Rethinking Multimodal Few-Shot 3D Point Cloud Segmentation: From Fused Refinement to Decoupled Arbitration](https://arxiv.org/abs/2601.01456)
*Wentao Bian,Fenglei Xu*

Main category: cs.CV

TL;DR: 该文针对多模态小样本3D点云语义分割中的“塑性-稳定性困境”问题提出了解决方案，并引入去耦专家仲裁网络DA-FSS，在多个数据集上实现了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态点云分割方法常出现“塑性-稳定性困境”与CLIP特征混淆导致的语义盲点，影响模型对几何与语义特征的有效融合和泛化能力。

Method: 提出DA-FSS架构，明确区分几何路径与语义路径，设计并行专家细化模块分别处理各自模态，通过堆叠仲裁模块进行融合与仲裁，并利用去耦对齐模块在两条路径间转移知识，有效减少特征混淆。

Result: 在S3DIS与ScanNet数据集上，DA-FSS在分割准确率、几何边界、完整性及纹理区分上均超越了MM-FSS等基线方法。

Conclusion: DA-FSS通过去耦专家与仲裁机制，有效提升了多模态小样本3D点云语义分割的泛化能力和区分性，为相关应用提供了更优的解决方案。

Abstract: In this paper, we revisit multimodal few-shot 3D point cloud semantic segmentation (FS-PCS), identifying a conflict in "Fuse-then-Refine" paradigms: the "Plasticity-Stability Dilemma." In addition, CLIP's inter-class confusion can result in semantic blindness. To address these issues, we present the Decoupled-experts Arbitration Few-Shot SegNet (DA-FSS), a model that effectively distinguishes between semantic and geometric paths and mutually regularizes their gradients to achieve better generalization. DA-FSS employs the same backbone and pre-trained text encoder as MM-FSS to generate text embeddings, which can increase free modalities' utilization rate and better leverage each modality's information space. To achieve this, we propose a Parallel Expert Refinement module to generate each modal correlation. We also propose a Stacked Arbitration Module (SAM) to perform convolutional fusion and arbitrate correlations for each modality pathway. The Parallel Experts decouple two paths: a Geometric Expert maintains plasticity, and a Semantic Expert ensures stability. They are coordinated via a Decoupled Alignment Module (DAM) that transfers knowledge without propagating confusion. Experiments on popular datasets (S3DIS, ScanNet) demonstrate the superiority of DA-FSS over MM-FSS. Meanwhile, geometric boundaries, completeness, and texture differentiation are all superior to the baseline. The code is available at: https://github.com/MoWenQAQ/DA-FSS.

</details>


### [76] [Language as Prior, Vision as Calibration: Metric Scale Recovery for Monocular Depth Estimation](https://arxiv.org/abs/2601.01457)
*Mingxing Zhan,Li Zhang,Beibei Wang,Yingjie Wang,Zenglin Shi*

Main category: cs.CV

TL;DR: 该论文提出了一种结合视觉和语言信息的单目深度估计校准方法，提升了跨域和零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计中，相对深度模型虽然迁移能力强，但由于全局尺度不可辨识和对领域偏移敏感，导致实际应用中度量深度难以准确估算，尤其是在跨域场景中。作者希望解决上述问题。

Method: 作者采用冻结主干特征提取网络，仅对轻量级校准头进行训练，通过在逆深度空间利用图像特定的仿射变换恢复度量深度。结合固定的CLIP文本编码器，利用图像描述文本来预测一个不确定性包络，限定校准参数的可行范围，然后利用冻结的多尺度视觉特征选取具体校准参数。训练阶段通过闭式最小二乘法在逆深度空间得到监督信号，提升校准包络和参数选择的精度。

Result: 在NYUv2和KITTI数据集上提升了同域精度，零样本迁移到SUN-RGBD和DDAD数据集时也超越了强力的仅语言方法基线，在鲁棒性上有明显提升。

Conclusion: 将视觉和语言进行不确定性融合，用于图像特定的深度校准，有效提升了单目度量深度估计在跨域和零样本任务下的表现。

Abstract: Relative-depth foundation models transfer well, yet monocular metric depth remains ill-posed due to unidentifiable global scale and heightened domain-shift sensitivity. Under a frozen-backbone calibration setting, we recover metric depth via an image-specific affine transform in inverse depth and train only lightweight calibration heads while keeping the relative-depth backbone and the CLIP text encoder fixed. Since captions provide coarse but noisy scale cues that vary with phrasing and missing objects, we use language to predict an uncertainty-aware envelope that bounds feasible calibration parameters in an unconstrained space, rather than committing to a text-only point estimate. We then use pooled multi-scale frozen visual features to select an image-specific calibration within this envelope. During training, a closed-form least-squares oracle in inverse depth provides per-image supervision for learning the envelope and the selected calibration. Experiments on NYUv2 and KITTI improve in-domain accuracy, while zero-shot transfer to SUN-RGBD and DDAD demonstrates improved robustness over strong language-only baselines.

</details>


### [77] [Domain Adaptation of Carotid Ultrasound Images using Generative Adversarial Network](https://arxiv.org/abs/2601.01460)
*Mohd Usama,Belal Ahmad,Christer Gronlund,Faleh Menawer R Althiyabi*

Main category: cs.CV

TL;DR: 该论文提出了一种基于GAN的超声图像领域自适应方法，可有效消除不同机器和参数设置带来的图像噪声和纹理差异。


<details>
  <summary>Details</summary>
Motivation: 在医学影像领域，深度学习模型通常假设训练和测试数据分布相同。但实际应用中，不同设备或设置生成的超声图像表现出明显的分布差异，导致模型跨域泛化能力差，频繁的模型重训练既费时又昂贵。

Method: 作者提出将领域自适应建模为图像到图像翻译问题，设计了新的GAN模型，通过调整测试数据的纹理与去除回波噪声，实现与目标域图像分布对齐，并保持图像内容不变。实验在来自三种不同机器的颈动脉超声数据集上进行，同时与CycleGAN方法进行了对比评估。

Result: 实验结果显示，所提方法能有效改变图像纹理并消除噪声，在直方图相关性和Bhattacharya距离指标上均优于无领域适应和CycleGAN对比方法。

Conclusion: 所提GAN模型可作为高效可靠的医学超声图像领域自适应方案，无需为不同设备频繁重训模型，具有较好实用性与推广潜力。

Abstract: Deep learning has been extensively used in medical imaging applications, assuming that the test and training datasets belong to the same probability distribution. However, a common challenge arises when working with medical images generated by different systems or even the same system with different parameter settings. Such images contain diverse textures and reverberation noise that violate the aforementioned assumption. Consequently, models trained on data from one device or setting often struggle to perform effectively with data from other devices or settings. In addition, retraining models for each specific device or setting is labor-intensive and costly. To address these issues in ultrasound images, we propose a novel Generative Adversarial Network (GAN)-based model. We formulated the domain adaptation tasks as an image-to-image translation task, in which we modified the texture patterns and removed reverberation noise in the test data images from the source domain to align with those in the target domain images while keeping the image content unchanged. We applied the proposed method to two datasets containing carotid ultrasound images from three different domains. The experimental results demonstrate that the model successfully translated the texture pattern of images and removed reverberation noise from the ultrasound images. Furthermore, we evaluated the CycleGAN approaches for a comparative study with the proposed model. The experimental findings conclusively demonstrated that the proposed model achieved domain adaptation (histogram correlation (0.960 (0.019), & 0.920 (0.043) and bhattacharya distance (0.040 (0.020), & 0.085 (0.048)), compared to no adaptation (0.916 (0.062) & 0.890 (0.077), 0.090 (0.070) & 0.121 (0.095)) for both datasets.

</details>


### [78] [Robust Ship Detection and Tracking Using Modified ViBe and Backwash Cancellation Algorithm](https://arxiv.org/abs/2601.01481)
*Mohammad Hassan Saghafi,Seyed Majid Noorhosseini,Seyed Abolfazl Seyed Javadein,Hadi Khalili*

Main category: cs.CV

TL;DR: 本文提出了一种用于海岸视频中船只实时检测与跟踪的鲁棒方法，通过改进的ViBe算法有效区分船只与回流，提升了检测精度与实时性。


<details>
  <summary>Details</summary>
Motivation: 海岸场景具有不可预测性与动态变化，传统检测方法容易受自然波浪、光照变化等影响，导致检测结果不稳定。因此，需要开发对这些情况更鲁棒的检测算法。

Method: 作者对传统的ViBe移动目标检测算法进行改进，提高了对船只和回流水体的检测能力，并降低了漏检船只的概率。算法增强了对自然波浪和光照变化的适应性，能够快速更新背景。此外，基于船只的几何属性及亮度失真等概念，提出了新的回流消除方法。

Result: 实验结果表明，改进后的算法在船舶检测和跟踪方面具有优异表现，在实际应用中达到了实时和精准的效果。

Conclusion: 提出的检测与跟踪策略不仅在各类实际海岸场景中表现出色，还兼具实时性和高精度，优于传统方法，具备较强的应用前景。

Abstract: In this paper, we propose a robust real time detection and tracking method for detecting ships in a coastal video sequences. Since coastal scenarios are unpredictable and scenes have dynamic properties it is essential to apply detection methods that are robust to these conditions. This paper presents modified ViBe for moving object detection which detects ships and backwash. In the modified ViBe the probability of losing ships is decreased in comparison with the original ViBe. It is robust to natural sea waves and variation of lights and is capable of quickly updating the background. Based on geometrical properties of ship and some concepts such as brightness distortion, a new method for backwash cancellation is proposed. Experimental results demonstrate that the proposed strategy and methods have outstanding performance in ship detection and tracking. These results also illustrate real time and precise performance of the proposed strategy.

</details>


### [79] [Unified Generation and Self-Verification for Vision-Language Models via Advantage Decoupled Preference Optimization](https://arxiv.org/abs/2601.01483)
*Xinyu Qiu,Heng Jia,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Yi Yang,Linchao Zhu*

Main category: cs.CV

TL;DR: 本文提出了ADPO（Advantage Decoupled Preference Optimization）框架，将生成与自验证两个任务在同一策略中联合训练，极大提升了验证能力并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 目前测试时并行规模扩展常需分别训练生成和验证模型，导致高昂的训练与推理成本。需要一种统一且高效的解决方案。

Method: ADPO在单一策略下联合训练生成与验证，提出：（1）偏好验证奖励，对正负样本计算验证分数均值为决策阈值，对预测正确性与答案一致时给予正反馈；（2）优势解耦优化，分别计算生成和验证的优势，采用token mask隔离梯度，并融合加mask的GRPO目标，从而兼顾生成质量和验证得分校准。

Result: ADPO相较于现有方法，验证AUC提升34.1%，推理时间下降53.5%，在MathVista/MMMU准确率分别提升2.8%和1.4%，在ReasonSeg提升1.9 cIoU，在AndroidControl/GUI Odyssey步骤成功率提升1.7%/1.0%。

Conclusion: ADPO可高效、联合提升生成与自验证能力，兼顾生成质量和验证准确性，并显著降低计算资源消耗。

Abstract: Parallel test-time scaling typically trains separate generation and verification models, incurring high training and inference costs. We propose Advantage Decoupled Preference Optimization (ADPO), a unified reinforcement learning framework that jointly learns answer generation and self-verification within a single policy. ADPO introduces two innovations: a preference verification reward improving verification capability and a decoupled optimization mechanism enabling synergistic optimization of generation and verification. Specifically, the preference verification reward computes mean verification scores from positive and negative samples as decision thresholds, providing positive feedback when prediction correctness aligns with answer correctness. Meanwhile, the advantage decoupled optimization computes separate advantages for generation and verification, applies token masks to isolate gradients, and combines masked GRPO objectives, preserving generation quality while calibrating verification scores. ADPO achieves up to +34.1% higher verification AUC and -53.5% lower inference time, with significant gains of +2.8%/+1.4% accuracy on MathVista/MMMU, +1.9 cIoU on ReasonSeg, and +1.7%/+1.0% step success rate on AndroidControl/GUI Odyssey.

</details>


### [80] [Higher-Order Domain Generalization in Magnetic Resonance-Based Assessment of Alzheimer's Disease](https://arxiv.org/abs/2601.01485)
*Zobia Batool,Diala Lteif,Vijaya B. Kolachalama,Huseyin Ozkan,Erchan Aptoula*

Main category: cs.CV

TL;DR: 提出了一种新的Extended MixStyle (EM)方法，通过融合特征的高阶矩来提升阿尔茨海默症（AD）诊断模型在不同数据域之间的泛化能力，在多个数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于结构MRI的深度学习模型在跨数据集应用时，由于扫描仪、协议和患者群体差异，常常出现性能明显下降。由于AD真实环境下数据来源碎片化，需要模型具备更强的泛化能力。

Method: 提出Extended MixStyle（EM）框架，在训练阶段将高阶特征矩（偏度、峰度）混合，模拟更丰富的分布变异，从而强化单域泛化能力。模型在NACC数据集训练，分别识别正常、轻度认知障碍与AD患者，并在3个未见数据集上测试。

Result: EM方法实现了优于现有单域泛化基线的跨域表现，平均提升macro-F1分数2.4个百分点，验证了其增强模型跨域鲁棒性的能力。

Conclusion: EM能显著提升AD诊断模型的泛化能力，有助于在异质、现实环境下实现更可靠的自动AD检测。源码将在GitHub开放。

Abstract: Despite progress in deep learning for Alzheimer's disease (AD) diagnostics, models trained on structural magnetic resonance imaging (sMRI) often do not perform well when applied to new cohorts due to domain shifts from varying scanners, protocols and patient demographics. AD, the primary driver of dementia, manifests through progressive cognitive and neuroanatomical changes like atrophy and ventricular expansion, making robust, generalizable classification essential for real-world use. While convolutional neural networks and transformers have advanced feature extraction via attention and fusion techniques, single-domain generalization (SDG) remains underexplored yet critical, given the fragmented nature of AD datasets. To bridge this gap, we introduce Extended MixStyle (EM), a framework for blending higher-order feature moments (skewness and kurtosis) to mimic diverse distributional variations. Trained on sMRI data from the National Alzheimer's Coordinating Center (NACC; n=4,647) to differentiate persons with normal cognition (NC) from those with mild cognitive impairment (MCI) or AD and tested on three unseen cohorts (total n=3,126), EM yields enhanced cross-domain performance, improving macro-F1 on average by 2.4 percentage points over state-of-the-art SDG benchmarks, underscoring its promise for invariant, reliable AD detection in heterogeneous real-world settings. The source code will be made available upon acceptance at https://github.com/zobia111/Extended-Mixstyle.

</details>


### [81] [DeepInv: A Novel Self-supervised Learning Approach for Fast and Accurate Diffusion Inversion](https://arxiv.org/abs/2601.01487)
*Ziyue Zhang,Luxi Lin,Xiaolin Hu,Chao Chang,HuaiXi Wang,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出了一种自监督扩散反演方法DeepInv，能无需人工噪声标签，通过创新目标函数和数据增强策略，高效准确地恢复扩散模型的噪声。


<details>
  <summary>Details</summary>
Motivation: 现有扩散反演方法缺乏有效监督信号，导致精度和效率难以兼顾。本文希望通过自监督与新设计提升扩散反演效果，满足可控编辑等实际需求。

Method: 提出DeepInv方法，无需人工标注，通过自监督目标和数据增强从真实图像自动生成高质量伪噪声，并结合迭代与多尺度训练方式，训练可参数化反演求解器，实现高效的图像到噪声映射。

Result: 在COCO数据集上，DeepInv相比EasyInv的SSIM提升了40.435%，相较ReNoise推理速度提升了9887.5%，表现远超现有方法。

Conclusion: DeepInv有效提升扩散反演的性能与效率，为社区提供了可训练求解器新范例，对后续相关研究具有启示意义。代码等将开源。

Abstract: Diffusion inversion is a task of recovering the noise of an image in a diffusion model, which is vital for controllable diffusion image editing. At present, diffusion inversion still remains a challenging task due to the lack of viable supervision signals. Thus, most existing methods resort to approximation-based solutions, which however are often at the cost of performance or efficiency. To remedy these shortcomings, we propose a novel self-supervised diffusion inversion approach in this paper, termed Deep Inversion (DeepInv). Instead of requiring ground-truth noise annotations, we introduce a self-supervised objective as well as a data augmentation strategy to generate high-quality pseudo noises from real images without manual intervention. Based on these two innovative designs, DeepInv is also equipped with an iterative and multi-scale training regime to train a parameterized inversion solver, thereby achieving the fast and accurate image-to-noise mapping. To the best of our knowledge, this is the first attempt of presenting a trainable solver to predict inversion noise step by step. The extensive experiments show that our DeepInv can achieve much better performance and inference speed than the compared methods, e.g., +40.435% SSIM than EasyInv and +9887.5% speed than ReNoise on COCO dataset. Moreover, our careful designs of trainable solvers can also provide insights to the community. Codes and model parameters will be released in https://github.com/potato-kitty/DeepInv.

</details>


### [82] [DiffKD-DCIS: Predicting Upgrade of Ductal Carcinoma In Situ with Diffusion Augmentation and Knowledge Distillation](https://arxiv.org/abs/2601.01507)
*Tao Li,Qing Li,Na Li,Hui Xie*

Main category: cs.CV

TL;DR: 本研究提出DiffKD-DCIS框架，通过条件扩散模型和知识蒸馏提升超声图像预测DCIS升级为IDC的能力，大幅提高了模型泛化性和效率，在多中心数据上表现良好，准确率可媲美高级放射科医师。


<details>
  <summary>Details</summary>
Motivation: DCIS升级为IDC的准确预测对手术方案制定非常关键，但受限于超声数据量小和深度学习模型泛化差等问题，亟需新的有效方法。

Method: 提出DiffKD-DCIS新框架，包含三阶段：一是利用条件扩散模型进行多模态超声图像合成用于数据增强；二是深层教师网络从原始和合成数据中提取鲁棒特征；三是通过知识蒸馏训练精简学生网络，兼顾准确性和模型效率。

Result: 在1435例多中心数据中，合成图像质量良好，学生网络参数更少、推理更快，对比外部测试集结果，准确率优于部分模型组合，与高级放射科医师相当，明显优于初级医师。

Conclusion: DiffKD-DCIS框架能有效增强DCIS升级为IDC的预测能力，在提升泛化性同时，保证高效计算，具备显著临床应用前景。

Abstract: Accurately predicting the upgrade of ductal carcinoma in situ (DCIS) to invasive ductal carcinoma (IDC) is crucial for surgical planning. However, traditional deep learning methods face challenges due to limited ultrasound data and poor generalization ability. This study proposes the DiffKD-DCIS framework, integrating conditional diffusion modeling with teacher-student knowledge distillation.
  The framework operates in three stages: First, a conditional diffusion model generates high-fidelity ultrasound images using multimodal conditions for data augmentation. Then, a deep teacher network extracts robust features from both original and synthetic data. Finally, a compact student network learns from the teacher via knowledge distillation, balancing generalization and computational efficiency.
  Evaluated on a multi-center dataset of 1,435 cases, the synthetic images were of good quality. The student network had fewer parameters and faster inference. On external test sets, it outperformed partial combinations, and its accuracy was comparable to senior radiologists and superior to junior ones, showing significant clinical potential.

</details>


### [83] [A Novel Deep Learning Method for Segmenting the Left Ventricle in Cardiac Cine MRI](https://arxiv.org/abs/2601.01512)
*Wenhui Chu,Aobo Jin,Hardik A. Gohel*

Main category: cs.CV

TL;DR: 提出了一种新型基于Group-Batch-Normalized U-Net（GBU-Net）的深度学习网络，用于左心室在心脏MRI中的精确分割，取得了比现有方法更高的准确率。


<details>
  <summary>Details</summary>
Motivation: 左心室的精准分割对于手术机器人和医学分析至关重要，但传统CNN方法难以捕捉足够的上下文信息，导致分割准确率受限。

Method: 采用一种改进的U-Net结构，结合group-batch normalization，包含下采样（特征提取）和上采样（细节恢复）通路，并针对医学成像特性进行了优化。通过新技术增强了对上下文信息的把握，对左心室MRI图像进行高精度分割。

Result: 在45名患者805组MRI数据上进行测试，GBU-Net在Dice系数和平均垂直距离等指标上均显著优于现有方法。在SunnyBrook测试集上，通过集成模型获得97%的Dice分数。

Conclusion: GBU-Net能够更精确地分割左心室，优于当前主流方法，并增强了对上下文信息的理解，具有医学分析和手术机器人应用前景。

Abstract: This research aims to develop a novel deep learning network, GBU-Net, utilizing a group-batch-normalized U-Net framework, specifically designed for the precise semantic segmentation of the left ventricle in short-axis cine MRI scans. The methodology includes a down-sampling pathway for feature extraction and an up-sampling pathway for detail restoration, enhanced for medical imaging. Key modifications include techniques for better contextual understanding crucial in cardiac MRI segmentation. The dataset consists of 805 left ventricular MRI scans from 45 patients, with comparative analysis using established metrics such as the dice coefficient and mean perpendicular distance. GBU-Net significantly improves the accuracy of left ventricle segmentation in cine MRI scans. Its innovative design outperforms existing methods in tests, surpassing standard metrics like the dice coefficient and mean perpendicular distance. The approach is unique in its ability to capture contextual information, often missed in traditional CNN-based segmentation. An ensemble of the GBU-Net attains a 97% dice score on the SunnyBrook testing dataset. GBU-Net offers enhanced precision and contextual understanding in left ventricle segmentation for surgical robotics and medical analysis.

</details>


### [84] [FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation](https://arxiv.org/abs/2601.01513)
*Gen Li,Peiyu Liu*

Main category: cs.CV

TL;DR: 提出了一种高效的视觉-语言模型检索增强生成（RAG）框架VideoSpeculateRAG，通过推测解码和相似性实体过滤策略，加速推理并提升答案准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型虽擅长视觉推理，但整合外部知识的能力有限；主流RAG方法效率低且易损失答案质量，亟需高效、准确的解决方案。

Method: 引入推测解码流程：用轻量级草稿模型快速生成多个答案候选，再由高精度重型模型验证和精炼，降低推理延迟；同时识别出实体识别错误为主要瓶颈，通过相似性过滤提升实体对齐和结果准确性。

Result: 在实验中，VideoSpeculateRAG以约2倍加速实现了与标准RAG相当或更高的准确率。

Conclusion: 结合推测解码与检索增强推理，有助于提升多模态复杂知识任务中的效率和可靠性。

Abstract: Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.

</details>


### [85] [BARE: Towards Bias-Aware and Reasoning-Enhanced One-Tower Visual Grounding](https://arxiv.org/abs/2601.01526)
*Hongbing Li,Linhui Xiao,Zihan Zhao,Qi Shen,Yixiang Huang,Bo Xiao,Zhanyu Ma*

Main category: cs.CV

TL;DR: 本文提出了BARE模型，通过更好地处理多模态偏差和增强推理能力，有效提升了视觉指代任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前单塔视觉指代模型虽然取得了一定进展，但仍存在多模态特征过分纠缠导致的偏差以及指代推理能力不足的问题，影响了表达的准确定位。

Method: 提出了BARE框架，包含语言显著性调节、视觉偏差修正和指代关系增强三大模块，分别用于保留单一模态特征、校正视觉编码中的偏差及提升对表述中关系的理解能力。整个系统以单塔结构高效运行。

Result: 在五个主流视觉指代基准数据集上，BARE模型在性能上超过了当前所有已有方法，并且具有更高的计算效率。

Conclusion: BARE框架解决了单塔视觉指代中的多模态偏差和语义推理不足问题，为该领域树立了新标杆。

Abstract: Visual Grounding (VG), which aims to locate a specific region referred to by expressions, is a fundamental yet challenging task in the multimodal understanding fields. While recent grounding transfer works have advanced the field through one-tower architectures, they still suffer from two primary limitations: (1) over-entangled multimodal representations that exacerbate deceptive modality biases, and (2) insufficient semantic reasoning that hinders the comprehension of referential cues. In this paper, we propose BARE, a bias-aware and reasoning-enhanced framework for one-tower visual grounding. BARE introduces a mechanism that preserves modality-specific features and constructs referential semantics through three novel modules: (i) language salience modulator, (ii) visual bias correction and (iii) referential relationship enhancement, which jointly mitigate multimodal distractions and enhance referential comprehension. Extensive experimental results on five benchmarks demonstrate that BARE not only achieves state-of-the-art performance but also delivers superior computational efficiency compared to existing approaches. The code is publicly accessible at https://github.com/Marloweeee/BARE.

</details>


### [86] [Improving Flexible Image Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2601.01535)
*Zixuan Fu,Lanqing Guo,Chong Wang,Binbin Song,Ding Liu,Bihan Wen*

Main category: cs.CV

TL;DR: 本文提出了一种新的灵活图像分词器ReToK，通过引入冗余Token填充与分层语义正则，有效提升了自回归图像生成的表现，尤其在token数量增加时表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有灵活图像分词技术依赖于尾部截断策略，导致图像信息过度集中于前部tokens，不利于提升自回归生成能力，尤其在序列长度增加时效果受限。

Method: 作者提出ReToK分词器，通过“冗余Token填充”激活尾部token，减少信息集中于前部；同时采用“分层语义正则”，利用预训练视觉模型对早期token的解码特征进行语义对齐，且逐步减弱正则化以保留细节信息。

Result: 在ImageNet 256x256数据集上，ReToK在图像生成任务中的表现优于固定长度和其它灵活长度分词器，实验结果充分体现其优势。

Conclusion: ReToK方法能更好地发挥所有token的信息潜力，显著提升自回归图像生成质量，是灵活图像分词领域的一项有效创新。

Abstract: Flexible image tokenizers aim to represent an image using an ordered 1D variable-length token sequence. This flexible tokenization is typically achieved through nested dropout, where a portion of trailing tokens is randomly truncated during training, and the image is reconstructed using the remaining preceding sequence. However, this tail-truncation strategy inherently concentrates the image information in the early tokens, limiting the effectiveness of downstream AutoRegressive (AR) image generation as the token length increases. To overcome these limitations, we propose \textbf{ReToK}, a flexible tokenizer with \underline{Re}dundant \underline{Tok}en Padding and Hierarchical Semantic Regularization, designed to fully exploit all tokens for enhanced latent modeling. Specifically, we introduce \textbf{Redundant Token Padding} to activate tail tokens more frequently, thereby alleviating information over-concentration in the early tokens. In addition, we apply \textbf{Hierarchical Semantic Regularization} to align the decoding features of earlier tokens with those from a pre-trained vision foundation model, while progressively reducing the regularization strength toward the tail to allow finer low-level detail reconstruction. Extensive experiments demonstrate the effectiveness of ReTok: on ImageNet 256$\times$256, our method achieves superior generation performance compared with both flexible and fixed-length tokenizers. Code will be available at: \href{https://github.com/zfu006/ReTok}{https://github.com/zfu006/ReTok}

</details>


### [87] [FAR-AMTN: Attention Multi-Task Network for Face Attribute Recognition](https://arxiv.org/abs/2601.01537)
*Gong Gao,Zekai Wang,Xianhui Liu,Weidong Zhao*

Main category: cs.CV

TL;DR: 为提高多任务网络在人脸属性识别任务中的泛化能力，提出了一种基于注意力机制的多任务网络FAR-AMTN，能够更好地在任务间共享和交互信息，并显著减少参数量。实验结果显示该方法在主流数据集上准确率更高且模型更轻量。


<details>
  <summary>Details</summary>
Motivation: 现有多任务网络在处理人脸属性识别时，因高层采用任务专有结构，导致参数量随任务数急剧增多，同时限制了特征（属性）间的交互，影响模型泛化能力。需要设计既能高效参数共享又能促进任务间语义交互的结构。

Method: 提出FAR-AMTN模型，集成三项创新：(1)权重共享的组特异性注意力（WSGSA）模块，用统一参数强化组特征表达并减少模型复杂度；(2)跨组特征融合（CGFF）模块，促进属性组间的信息交互；(3)动态加权策略（DWS），实现多任务同步收敛。

Result: 在CelebA和LFWA公开数据集上，FAR-AMTN在准确率和参数数量两方面均优于现有方法，实现了更高效、更精确的人脸属性识别。

Conclusion: FAR-AMTN能够有效促进多任务特征共享及交互，在保证模型轻量化的同时，提升了人脸属性识别的性能和泛化能力。

Abstract: To enhance the generalization performance of Multi-Task Networks (MTN) in Face Attribute Recognition (FAR), it is crucial to share relevant information across multiple related prediction tasks effectively. Traditional MTN methods create shared low-level modules and distinct high-level modules, causing an exponential increase in model parameters with the addition of tasks. This approach also limits feature interaction at the high level, hindering the exploration of semantic relations among attributes, thereby affecting generalization negatively. In response, this study introduces FAR-AMTN, a novel Attention Multi-Task Network for FAR. It incorporates a Weight-Shared Group-Specific Attention (WSGSA) module with shared parameters to minimize complexity while improving group feature representation. Furthermore, a Cross-Group Feature Fusion (CGFF) module is utilized to foster interactions between attribute groups, enhancing feature learning. A Dynamic Weighting Strategy (DWS) is also introduced for synchronized task convergence. Experiments on the CelebA and LFWA datasets demonstrate that the proposed FAR-AMTN demonstrates superior accuracy with significantly fewer parameters compared to existing models.

</details>


### [88] [EscherVerse: An Open World Benchmark and Dataset for Teleo-Spatial Intelligence with Physical-Dynamic and Intent-Driven Understanding](https://arxiv.org/abs/2601.01547)
*Tianjun Gu,Chenghua Gong,Jingyu Gong,Zhizhong Zhang,Yuan Xie,Lizhuang Ma,Xin Tan*

Main category: cs.CV

TL;DR: 本文提出了Teleo-Spatial Intelligence（TSI）的新范式，结合了对物理动态的推理和以意图为驱动的推理，并发布了相关的基准、数据集和模型，用于推动该领域的研究。


<details>
  <summary>Details</summary>
Motivation: 当前空间动态推理研究忽视了人类意图，这限制了智能体对人类目标和行为背后原因的理解。为解决这一不足，有必要创造一个同时关注物理和意图两个维度的新范式。

Method: 作者提出TSI范式，将物理动态推理（如物体交互原理）与以意图为驱动的推理（推断人类行为目的）结合。为此，设计了EscherVerse工具包，包括大规模开放世界基准（Escher-Bench）、数据集（Escher-35k）和模型（Escher系列），通过从真实世界视频中提取数据，评估智能体在复杂动态场景中的推理能力。

Result: EscherVerse超越了传统受限环境，首次系统评测了模型的以意图为驱动的推理能力，可以挑战模型将物理事件与人类目的联系起来的能力，并为空间智能的研究带来更全面的基准和资源。

Conclusion: EscherVerse为空间智能从被动场景描述迈向整体性、以目的为导向的世界理解奠定了基础，对未来意图驱动空间推理研究具有重要意义。

Abstract: The ability to reason about spatial dynamics is a cornerstone of intelligence, yet current research overlooks the human intent behind spatial changes. To address these limitations, we introduce Teleo-Spatial Intelligence (TSI), a new paradigm that unifies two critical pillars: Physical-Dynamic Reasoning--understanding the physical principles of object interactions--and Intent-Driven Reasoning--inferring the human goals behind these actions. To catalyze research in TSI, we present EscherVerse, consisting of a large-scale, open-world benchmark (Escher-Bench), a dataset (Escher-35k), and models (Escher series). Derived from real-world videos, EscherVerse moves beyond constrained settings to explicitly evaluate an agent's ability to reason about object permanence, state transitions, and trajectory prediction in dynamic, human-centric scenarios. Crucially, it is the first benchmark to systematically assess Intent-Driven Reasoning, challenging models to connect physical events to their underlying human purposes. Our work, including a novel data curation pipeline, provides a foundational resource to advance spatial intelligence from passive scene description toward a holistic, purpose-driven understanding of the world.

</details>


### [89] [Beyond Patches: Global-aware Autoregressive Model for Multimodal Few-Shot Font Generation](https://arxiv.org/abs/2601.01593)
*Haonan Cai,Yuxuan Luo,Zhouhui Lian*

Main category: cs.CV

TL;DR: 本文提出了GAR-Font，一种用于多模态少样本字体生成的新型自回归框架，能更好保持结构和风格的一致性，支持文本风格指导，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动少样本字体生成方法难以同时保持字体结构完整性和风格一致性，且通常忽略了语言（文本）在风格表达中的作用，导致生成效果有限。

Method: 提出GAR-Font框架，包括：（1）全局感知分词器，捕捉局部结构和全局风格；（2）多模态风格编码器，通过轻量级文本-风格适配模块实现灵活风格控制，无需大规模多模态训练；（3）后处理精修模块提升结构保真度和风格一致性。

Result: 通过大量实验表明，GAR-Font在保持整体风格一致性和生成质量上明显优于现有少样本字体生成方法，特别是在引入文本风格指导后效果更佳。

Conclusion: GAR-Font能够高效、灵活地实现多模态、少样本条件下高质量字体生成，推动了自动字体设计领域的进步。

Abstract: Manual font design is an intricate process that transforms a stylistic visual concept into a coherent glyph set. This challenge persists in automated Few-shot Font Generation (FFG), where models often struggle to preserve both the structural integrity and stylistic fidelity from limited references. While autoregressive (AR) models have demonstrated impressive generative capabilities, their application to FFG is constrained by conventional patch-level tokenization, which neglects global dependencies crucial for coherent font synthesis. Moreover, existing FFG methods remain within the image-to-image paradigm, relying solely on visual references and overlooking the role of language in conveying stylistic intent during font design. To address these limitations, we propose GAR-Font, a novel AR framework for multimodal few-shot font generation. GAR-Font introduces a global-aware tokenizer that effectively captures both local structures and global stylistic patterns, a multimodal style encoder offering flexible style control through a lightweight language-style adapter without requiring intensive multimodal pretraining, and a post-refinement pipeline that further enhances structural fidelity and style coherence. Extensive experiments show that GAR-Font outperforms existing FFG methods, excelling in maintaining global style faithfulness and achieving higher-quality results with textual stylistic guidance.

</details>


### [90] [Guiding Token-Sparse Diffusion Models](https://arxiv.org/abs/2601.01608)
*Felix Krause,Stefan Andreas Baumann,Johannes Schusterbauer,Olga Grebenkova,Ming Gui,Vincent Tao Hu,Björn Ommer*

Main category: cs.CV

TL;DR: 提出了一种新方法Sparse Guidance（SG），通过在训练和推理中利用token级稀疏性，提升扩散模型在低计算成本下的输出质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏训练扩散模型在推理阶段效果较差，主要因为它们对Classifier-free Guidance (CFG)反应不佳，导致推理性能下降。作者希望找到能兼顾推理效率和生成质量的方法。

Method: SG方法在训练和推理阶段采用token-level稀疏性，而非传统的条件dropout来指导扩散模型，使模型在保留条件预测的高方差性的同时提升生成效果。此外，SG可在推理时灵活利用稀疏性以降低计算量。

Result: 在ImageNet-256数据集上，SG实现了1.58的FID分数，同时降低了25%的FLOPs。在保持基线相同质量时，最多可节省58%的FLOPs。作者还训练了一个2.5B参数的文本到图像扩散模型，结合SG后，在合成质量和用户偏好分数方面均有提升，同时推理吞吐量增加。

Conclusion: Sparse Guidance有效弥补了现有稀疏训练扩散模型在推理阶段的不足，不仅提升了模型生成质量和多样性，还显著降低了推理计算开销，兼顾效率和效果。

Abstract: Diffusion models deliver high quality in image synthesis but remain expensive during training and inference. Recent works have leveraged the inherent redundancy in visual content to make training more affordable by training only on a subset of visual information. While these methods were successful in providing cheaper and more effective training, sparsely trained diffusion models struggle in inference. This is due to their lacking response to Classifier-free Guidance (CFG) leading to underwhelming performance during inference. To overcome this, we propose Sparse Guidance (SG). Instead of using conditional dropout as a signal to guide diffusion models, SG uses token-level sparsity. As a result, SG preserves the high-variance of the conditional prediction better, achieving good quality and high variance outputs. Leveraging token-level sparsity at inference, SG improves fidelity at lower compute, achieving 1.58 FID on the commonly used ImageNet-256 benchmark with 25% fewer FLOPs, and yields up to 58% FLOP savings at matched baseline quality. To demonstrate the effectiveness of Sparse Guidance, we train a 2.5B text-to-image diffusion model using training time sparsity and leverage SG during inference. SG achieves improvements in composition and human preference score while increasing throughput at the same time.

</details>


### [91] [CAP-IQA: Context-Aware Prompt-Guided CT Image Quality Assessment](https://arxiv.org/abs/2601.01613)
*Kazi Ramisa Rifa,Jie Zhang,Abdullah Imran*

Main category: cs.CV

TL;DR: 本文提出了一种结合文本提示与实例上下文、并利用因果去偏的新型CT图像质量评估（IQA）方法——CAP-IQA，显著超越了现有基线，在公开比赛和内部数据上均表现出良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于prompt的医学图像质量评估方法虽然能嵌入先验知识，但往往引入理想化偏见，难以适应现实中噪声、运动伪影和扫描设备差异等情况。因此亟需一种能结合实例实际情况、并去除理想化偏见的新型IQA框架。

Method: 提出CAP-IQA框架，结合CNN视觉编码器和领域特定的文本编码器，利用放射科风格的prompt和实例级上下文融合，并通过因果去偏方法区分理想知识与实际图像降质。模型用于腹部CT图像的诊断能见度、解剖清晰度和噪声感知评估，并进行激励融合以提升语义和感知对齐。

Result: 在2023年LDCTIQA挑战赛基准下，CAP-IQA的整体相关性分数达到2.8590，超过了排行榜首队伍2.7427，提升4.24%。消融实验证实prompt引导的融合和精简的编码器结构提升了特征对齐和可解释性；在9万余张儿科CT内部数据集上也显示出良好泛化性。

Conclusion: CAP-IQA有效融合了文本先验与实例上下文，并通过去偏提升了影像质量评估的实用性和泛化性，为医疗影像自动评估领域提供了更精确且解释性更强的新范式。

Abstract: Prompt-based methods, which encode medical priors through descriptive text, have been only minimally explored for CT Image Quality Assessment (IQA). While such prompts can embed prior knowledge about diagnostic quality, they often introduce bias by reflecting idealized definitions that may not hold under real-world degradations such as noise, motion artifacts, or scanner variability. To address this, we propose the Context-Aware Prompt-guided Image Quality Assessment (CAP-IQA) framework, which integrates text-level priors with instance-level context prompts and applies causal debiasing to separate idealized knowledge from factual, image-specific degradations. Our framework combines a CNN-based visual encoder with a domain-specific text encoder to assess diagnostic visibility, anatomical clarity, and noise perception in abdominal CT images. The model leverages radiology-style prompts and context-aware fusion to align semantic and perceptual representations. On the 2023 LDCTIQA challenge benchmark, CAP-IQA achieves an overall correlation score of 2.8590 (sum of PLCC, SROCC, and KROCC), surpassing the top-ranked leaderboard team (2.7427) by 4.24%. Moreover, our comprehensive ablation experiments confirm that prompt-guided fusion and the simplified encoder-only design jointly enhance feature alignment and interpretability. Furthermore, evaluation on an in-house dataset of 91,514 pediatric CT images demonstrates the true generalizability of CAP-IQA in assessing perceptual fidelity in a different patient population.

</details>


### [92] [An Empirical Study of Monocular Human Body Measurement Under Weak Calibration](https://arxiv.org/abs/2601.01639)
*Gaurav Sekar*

Main category: cs.CV

TL;DR: 本文系统地评估了三种基于单目RGB图像的人体测量方法，对它们在消费级相机下的表现进行了比较，揭示了校准便捷性和测量稳定性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 单目RGB图像的人体测量因缺少深度信息和视角敏感性等问题，导致测量精度和稳定性受限。针对这一挑战，本文旨在研究不同弱校准策略对单目人体测量效果的影响，为消费级设备上的实用测量系统提供经验依据。

Method: 实验对比了三种弱校准策略：基于关键点几何、姿态驱动回归和物体校准剪影，分析它们在半约束场景下的表现及其在校准假设不同情况下的行为和失败模式。

Result: 实验结果显示，校准时用户投入的精力与测得的人体围度稳定性之间存在明显权衡，即便测量精度非最优，但方法间的健壮性与失败类型也具显著差异。

Conclusion: 本文作为经验性设计参考，为消费级设备上轻量级的单目人体测量系统提供了方法选择与使用建议，强调校准便捷性和测量稳定性的平衡。

Abstract: Estimating human body measurements from monocular RGB imagery remains challenging due to scale ambiguity, viewpoint sensitivity, and the absence of explicit depth information. This work presents a systematic empirical study of three weakly calibrated monocular strategies: landmark-based geometry, pose-driven regression, and object-calibrated silhouettes, evaluated under semi-constrained conditions using consumer-grade cameras. Rather than pursuing state-of-the-art accuracy, the study analyzes how differing calibration assumptions influence measurement behavior, robustness, and failure modes across varied body types. The results reveal a clear trade-off between user effort during calibration and the stability of resulting circumferential quantities. This paper serves as an empirical design reference for lightweight monocular human measurement systems intended for deployment on consumer devices.

</details>


### [93] [Animated 3DGS Avatars in Diverse Scenes with Consistent Lighting and Shadows](https://arxiv.org/abs/2601.01660)
*Aymen Mir,Riza Alp Guler,Jian Wang,Gerard Pons-Moll,Bing Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种用于3D高斯贴花（3D Gaussian Splatting, 3DGS）虚拟人和场景中一致性光照与阴影方法，名为Deep Gaussian Shadow Maps（DGSM），可实现体积阴影计算和实时一致性渲染。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS技术在动画角色与场景交互、插入动态对象时缺乏一致且真实的光照与阴影表现，传统方法难以直接应用于体积渲染，且昂贵或需预先网格化。作者希望实现高效、一致的体积阴影与光照效果，适合3DGS表示。

Method: 提出DGSM方法，将经典深度阴影映射思想扩展至3DGS体积表示，实现了沿光线的闭式光照累积，无需网格化。每个光源通过径向环壳采样并以八面体图集方式存储可透光率，GPU实时查询，用于实时光照衰减和阴影。针对虚拟人重光照，采用球谐基的HDRI探针近似局部环境光照，并应用每个高斯的快速辐射传递，无需显式BRDF估计或离线优化。

Result: 在AvatarX、ActorsHQ等虚拟人及ScanNet++、DL3DV、SuperSplat等场景，插入动态对象和多角色下，DGSM和SH重光照均能直接在3DGS体积中产生一致的阴影与光照，无需网格，表现自然。

Conclusion: DGSM和球谐重光照为3DGS体积虚拟人与场景实现了真实一致、可实时渲染的体积阴影与重光照，有效突破了传统网格管线难以兼容的问题，提升了3DGS交互生成与合成能力。

Abstract: We present a method for consistent lighting and shadows when animated 3D Gaussian Splatting (3DGS) avatars interact with 3DGS scenes or with dynamic objects inserted into otherwise static scenes. Our key contribution is Deep Gaussian Shadow Maps (DGSM), a modern analogue of the classical shadow mapping algorithm tailored to the volumetric 3DGS representation. Building on the classic deep shadow mapping idea, we show that 3DGS admits closed form light accumulation along light rays, enabling volumetric shadow computation without meshing. For each estimated light, we tabulate transmittance over concentric radial shells and store them in octahedral atlases, which modern GPUs can sample in real time per query to attenuate affected scene Gaussians and thus cast and receive shadows consistently. To relight moving avatars, we approximate the local environment illumination with HDRI probes represented in a spherical harmonic (SH) basis and apply a fast per Gaussian radiance transfer, avoiding explicit BRDF estimation or offline optimization. We demonstrate environment consistent lighting for avatars from AvatarX and ActorsHQ, composited into ScanNet++, DL3DV, and SuperSplat scenes, and show interactions with inserted objects. Across single and multi avatar settings, DGSM and SH relighting operate fully in the volumetric 3DGS representation, yielding coherent shadows and relighting while avoiding meshing.

</details>


### [94] [LabelAny3D: Label Any Object 3D in the Wild](https://arxiv.org/abs/2601.01676)
*Jin Yao,Radowan Mahmud Redoy,Sebastian Elbaum,Matthew B. Dwyer,Zezhou Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于分析-合成（analysis-by-synthesis）的方法LabelAny3D，通过从2D图像重建3D场景以高效生成高质量3D标注，并据此构建了面向开放类目的COCO3D数据集，显著提升了单目3D目标检测表现。


<details>
  <summary>Details</summary>
Motivation: 现有的单目3D目标检测模型在野外（in-the-wild）场景下表现不佳，主要由于缺乏真实场景3D数据集和3D标注困难。

Method: 提出LabelAny3D框架，利用分析-合成策略，将2D图像高效还原为完整的3D场景，从而自动生成高质量的3D包围盒标注，并基于此建立了包含多类别的COCO3D基准数据集。

Result: LabelAny3D生成的标注能提升多个基准中单目3D检测性能，并在自动标注质量上超过了之前的方法。

Conclusion: 基于大模型驱动的自动3D标注方法能够推动3D识别任务在实际、开放世界中的可扩展性。

Abstract: Detecting objects in 3D space from monocular input is crucial for applications ranging from robotics to scene understanding. Despite advanced performance in the indoor and autonomous driving domains, existing monocular 3D detection models struggle with in-the-wild images due to the lack of 3D in-the-wild datasets and the challenges of 3D annotation. We introduce LabelAny3D, an \emph{analysis-by-synthesis} framework that reconstructs holistic 3D scenes from 2D images to efficiently produce high-quality 3D bounding box annotations. Built on this pipeline, we present COCO3D, a new benchmark for open-vocabulary monocular 3D detection, derived from the MS-COCO dataset and covering a wide range of object categories absent from existing 3D datasets. Experiments show that annotations generated by LabelAny3D improve monocular 3D detection performance across multiple benchmarks, outperforming prior auto-labeling approaches in quality. These results demonstrate the promise of foundation-model-driven annotation for scaling up 3D recognition in realistic, open-world settings.

</details>


### [95] [Trustworthy Data-Driven Wildfire Risk Prediction and Understanding in Western Canada](https://arxiv.org/abs/2601.01677)
*Zhengsen Xu,Lanying Wang,Sibo Cheng,Xue Rui,Kyle Gao,Yimin Zhu,Mabel Heffring,Zack Dewis,Saeid Taleghanidoozdoozan,Megan Greenwood,Motasem Alkayid,Quinn Ledingham,Hongjie He,Jonathan Li,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 本文提出了一种可信赖的数据驱动森林火灾风险预测框架，在2023和2024年的加拿大西部极端火季中表现优异，精度高且具备对预测不确定性的解释能力。


<details>
  <summary>Details</summary>
Motivation: 森林火灾频发对社会经济和环境造成严重损失，而由于火灾发生的随机性及多因素复杂交互，仅靠传统数据驱动方法难以获得高精度和高可解释性的预测，因此亟需创新的风险预测方法。

Method: 作者构建了一个基于长序列、多尺度时序建模的数据驱动框架，综合气象、气候、地形、人类活动等多维驱动因素，显式量化预测不确定性，并结合SHAP解释方法实现过程级解析。

Result: 该模型在2023和2024年加拿大西部火灾季节表现超越现有时序方法，F1分数为0.90，PR-AUC为0.98，计算成本低。不确定性分析揭示了预测置信度的空间与季节结构规律；SHAP解释显示温度主导火灾风险，水分因子在2024年空间分布作用增强。

Conclusion: 所提模型不仅预测准确，还对不确定性和驱动机制具备良好解释力，可为火灾风险管理和决策提供科学依据。

Abstract: In recent decades, the intensification of wildfire activity in western Canada has resulted in substantial socio-economic and environmental losses. Accurate wildfire risk prediction is hindered by the intrinsic stochasticity of ignition and spread and by nonlinear interactions among fuel conditions, meteorology, climate variability, topography, and human activities, challenging the reliability and interpretability of purely data-driven models. We propose a trustworthy data-driven wildfire risk prediction framework based on long-sequence, multi-scale temporal modeling, which integrates heterogeneous drivers while explicitly quantifying predictive uncertainty and enabling process-level interpretation. Evaluated over western Canada during the record-breaking 2023 and 2024 fire seasons, the proposed model outperforms existing time-series approaches, achieving an F1 score of 0.90 and a PR-AUC of 0.98 with low computational cost. Uncertainty-aware analysis reveals structured spatial and seasonal patterns in predictive confidence, highlighting increased uncertainty associated with ambiguous predictions and spatiotemporal decision boundaries. SHAP-based interpretation provides mechanistic understanding of wildfire controls, showing that temperature-related drivers dominate wildfire risk in both years, while moisture-related constraints play a stronger role in shaping spatial and land-cover-specific contrasts in 2024 compared to the widespread hot and dry conditions of 2023. Data and code are available at https://github.com/SynUW/mmFire.

</details>


### [96] [Evaluating Deep Learning-Based Face Recognition for Infants and Toddlers: Impact of Age Across Developmental Stages](https://arxiv.org/abs/2601.01680)
*Afzal Hossain,Mst Rumana Sumi,Stephanie Schuckers*

Main category: cs.CV

TL;DR: 本研究评估了四种深度学习人脸识别模型在0-3岁婴幼儿上的表现，提出了基于DANN的新方法以提升模型在时间跨度上的稳定性，并分析了婴幼儿识别的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 婴幼儿人脸识别因面部结构发展迅速、个体间相似度高以及数据稀缺而极具挑战，针对基于人脸的公共健康、儿童安全和身份认证的现实需求，需提升年龄敏感人群的人脸识别性能。

Method: 建立包含0-3岁儿童、跨度24个月的纵向数据集，分别评估FaceNet、ArcFace、MagFace、CosFace四种深度模型在各年龄段的识别能力，并进一步采用领域对抗神经网络（DANN）抑制随时间迁移的人脸特征嵌入漂移现象。

Result: 0-6个月婴儿的识别率（TAR）较低，仅30.7%，2.5-3岁提升至64.7%；时间间隔越短识别准确度越高。DANN方法提升TAR超过12%，特征向量时间稳定性增强。

Conclusion: 婴幼儿人脸识别存在巨大挑战，尤其在早期阶段。通过DANN可显著提升识别鲁棒性，为智能城市中的健康、安全及身份服务应用提供支持，同时强调需发展能适应儿童随时间变化的隐私保护生物识别系统。

Abstract: Face recognition for infants and toddlers presents unique challenges due to rapid facial morphology changes, high inter-class similarity, and limited dataset availability. This study evaluates the performance of four deep learning-based face recognition models FaceNet, ArcFace, MagFace, and CosFace on a newly developed longitudinal dataset collected over a 24 month period in seven sessions involving children aged 0 to 3 years. Our analysis examines recognition accuracy across developmental stages, showing that the True Accept Rate (TAR) is only 30.7% at 0.1% False Accept Rate (FAR) for infants aged 0 to 6 months, due to unstable facial features. Performance improves significantly in older children, reaching 64.7% TAR at 0.1% FAR in the 2.5 to 3 year age group. We also evaluate verification performance over different time intervals, revealing that shorter time gaps result in higher accuracy due to reduced embedding drift. To mitigate this drift, we apply a Domain Adversarial Neural Network (DANN) approach that improves TAR by over 12%, yielding features that are more temporally stable and generalizable. These findings are critical for building biometric systems that function reliably over time in smart city applications such as public healthcare, child safety, and digital identity services. The challenges observed in early age groups highlight the importance of future research on privacy preserving biometric authentication systems that can address temporal variability, particularly in secure and regulated urban environments where child verification is essential.

</details>


### [97] [FALCON: Few-Shot Adversarial Learning for Cross-Domain Medical Image Segmentation](https://arxiv.org/abs/2601.01687)
*Abdur R. Fayjie,Pankhi Kashyap,Jutika Borah,Patrick Vandewalle*

Main category: cs.CV

TL;DR: FALCON是一种创新的跨域小样本分割框架，通过2D切片方式实现高精度3D医学体积分割，可显著减少标注数据与计算量，同时提供优越的分割性能。


<details>
  <summary>Details</summary>
Motivation: 3D医学影像分割对于诊断、手术规划与疾病追踪至关重要，但受限于3D标注稀缺、个体差异、隐私和高计算成本，现有AI方法难以大规模临床应用。

Method: FALCON框架先在自然图像上进行元训练以获得可泛化的分割先验，然后通过对抗式微调与边界感知学习迁移至医学领域。在推理时，通过任务感知机制结合支持样本信息，实现对不同患者个体解剖差异的动态自适应。整个框架以2D切片处理3D数据，降低了计算负担。

Result: 在四个医学分割基准上，FALCON实现了最低的Hausdorff距离（即优越的边界精度）和与现有最优模型相当的Dice相似系数。同时，无需数据增强与大量标注，计算开销也显著降低。

Conclusion: FALCON在小样本和低资源条件下，为3D医学图像分割提供了一种高精度、低成本、可泛化的可行方案，有望促进AI方法在临床中的实际应用。

Abstract: Precise delineation of anatomical and pathological structures within 3D medical volumes is crucial for accurate diagnosis, effective surgical planning, and longitudinal disease monitoring. Despite advancements in AI, clinically viable segmentation is often hindered by the scarcity of 3D annotations, patient-specific variability, data privacy concerns, and substantial computational overhead. In this work, we propose FALCON, a cross-domain few-shot segmentation framework that achieves high-precision 3D volume segmentation by processing data as 2D slices. The framework is first meta-trained on natural images to learn-to-learn generalizable segmentation priors, then transferred to the medical domain via adversarial fine-tuning and boundary-aware learning. Task-aware inference, conditioned on support cues, allows FALCON to adapt dynamically to patient-specific anatomical variations across slices. Experiments on four benchmarks demonstrate that FALCON consistently achieves the lowest Hausdorff Distance scores, indicating superior boundary accuracy while maintaining a Dice Similarity Coefficient comparable to the state-of-the-art models. Notably, these results are achieved with significantly less labeled data, no data augmentation, and substantially lower computational overhead.

</details>


### [98] [Mitigating Longitudinal Performance Degradation in Child Face Recognition Using Synthetic Data](https://arxiv.org/abs/2601.01689)
*Afzal Hossain,Stephanie Schuckers*

Main category: cs.CV

TL;DR: 本文探讨了利用合成面部数据增强儿童脸部识别在时间跨度上的鲁棒性，发现合成数据辅助微调可以显著降低识别误差率。


<details>
  <summary>Details</summary>
Motivation: 随着儿童面部快速且非线性的成长，导致随时间推移的人脸模板漂移，从而增加识别错误率。该研究旨在解决儿童长期脸部识别的稳健性问题。

Method: 作者采用了Young Face Aging (YFA) 数据集，通过三种设置评估人脸识别模型的长期表现：(1) 直接使用预训练的MagFace，无微调；(2) 用真实训练人脸进行微调；(3) 用真实+合成（由StyleGAN2 ADA生成并经过过滤）训练人脸进行微调。

Result: 实验显示，在6到36个月的时间跨度里，采用合成数据增强微调的方法明显降低了错误率，相比单独使用预训练模型或仅用真实数据微调效果更优。

Conclusion: 合成数据增强是一种有效而可控的手段，可提升儿童人脸识别的长时间稳定性，为实际应用提供了风险可控的改进策略。

Abstract: Longitudinal face recognition in children remains challenging due to rapid and nonlinear facial growth, which causes template drift and increasing verification errors over time. This work investigates whether synthetic face data can act as a longitudinal stabilizer by improving temporal robustness of child face recognition models. Using an identity disjoint protocol on the Young Face Aging (YFA) dataset, we evaluate three settings: (i) pretrained MagFace embeddings without dataset specific fine-tuning, (ii) MagFace fine-tuned using authentic training faces only, and (iii) MagFace fine-tuned using a combination of authentic and synthetically generated training faces. Synthetic data is generated using StyleGAN2 ADA and incorporated exclusively within the training identities; a post generation filtering step is applied to mitigate identity leakage and remove artifact affected samples. Experimental results across enrollment verification gaps from 6 to 36 months show that synthetic-augmented fine tuning substantially reduces error rates relative to both the pretrained baseline and real only fine tuning. These findings provide a risk aware assessment of synthetic augmentation for improving identity persistence in pediatric face recognition.

</details>


### [99] [Learnability-Driven Submodular Optimization for Active Roadside 3D Detection](https://arxiv.org/abs/2601.01695)
*Ruiyu Mao,Baoming Zhang,Nicholas Ruozzi,Yunhui Guo*

Main category: cs.CV

TL;DR: 本文关注路侧单目3D目标检测场景，提出了一种面向可学习性的主动学习框架，以提高数据标注效率，并在性能上超越了基于不确定性的传统方法。


<details>
  <summary>Details</summary>
Motivation: 路侧感知数据集通常依赖路侧与车载设备配对标注，但实际部署时往往只能获取路侧数据。此情况下，很多场景仅凭路侧视角难以准确标注3D属性，增加了标注难度与成本，也揭示了“可学习性”问题。作者因此动机提出，有必要抑制那些本质上难以标注的样本，提升标注效率。

Method: 提出了一种基于可学习性的主动学习框架LH3D，通过挑选既有信息量又能可靠标注的场景，排除了本质模糊不可学的样本，实现标注预算与模型性能间的优化。

Result: 在DAIR-V2X-I数据集上，用25%的标注预算分别获得车辆、行人和骑行者86.06%、67.32%、78.67%的完整性能，显著优于传统不确定性主动学习方法。

Conclusion: “可学习性”标准优于“模型不确定性”作为路侧3D感知主动学习采样标准，可节省大量标注资源并优化检测性能。

Abstract: Roadside perception datasets are typically constructed via cooperative labeling between synchronized vehicle and roadside frame pairs. However, real deployment often requires annotation of roadside-only data due to hardware and privacy constraints. Even human experts struggle to produce accurate labels without vehicle-side data (image, LIDAR), which not only increases annotation difficulty and cost, but also reveals a fundamental learnability problem: many roadside-only scenes contain distant, blurred, or occluded objects whose 3D properties are ambiguous from a single view and can only be reliably annotated by cross-checking paired vehicle--roadside frames. We refer to such cases as inherently ambiguous samples. To reduce wasted annotation effort on inherently ambiguous samples while still obtaining high-performing models, we turn to active learning. This work focuses on active learning for roadside monocular 3D object detection and proposes a learnability-driven framework that selects scenes which are both informative and reliably labelable, suppressing inherently ambiguous samples while ensuring coverage. Experiments demonstrate that our method, LH3D, achieves 86.06%, 67.32%, and 78.67% of full-performance for vehicles, pedestrians, and cyclists respectively, using only 25% of the annotation budget on DAIR-V2X-I, significantly outperforming uncertainty-based baselines. This confirms that learnability, not uncertainty, matters for roadside 3D perception.

</details>


### [100] [FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing](https://arxiv.org/abs/2601.01720)
*Xijie Huang,Chengming Xu,Donghao Luo,Xiaobin Hu,Peng Tang,Xu Peng,Jiangning Zhang,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: 本文提出了第一帧传播（FFP）新方法，解决了以往可控视频编辑在运行时需要指导的问题，并通过自主扩建高质量数据集和特有模型架构，显著提升了编辑效果和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有FFP方法依赖于复杂的运行时指导，主要由于训练数据集过小、分辨率低、任务单一，难以学习到强健的时序先验，导致编辑的自然性和可控性受限。

Method: 作者首先构建了一个包含30万对、分辨率为720p、长度81帧的大规模高质量视频对数据集FFP-300K。基于此，提出了无外部指导的FFP框架，架构上引入了自适应时空RoPE（AST-RoPE），动态分离外观与动作信息。在优化目标上，使用自蒸馏策略、身份传播任务以增强时序稳定性和防止语义漂移。

Result: 在EditVerseBench基准上，所提方法在PickScore和VLM分数上分别提升了约0.2和0.3，显著超过现有学术和商业模型。

Conclusion: 通过补足数据短板与创新模型设计，本文方法实现了无需额外指导的高质量FFP视频编辑，推动了可控视频编辑的实际应用发展。

Abstract: First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.

</details>


### [101] [Point-SRA: Self-Representation Alignment for 3D Representation Learning](https://arxiv.org/abs/2601.01746)
*Lintong Wei,Jian Lu,Haozhe Cheng,Jihua Zhu,Kaibing Zhang*

Main category: cs.CV

TL;DR: 提出一种新型的3D表示学习方法Point-SRA，通过自蒸馏与概率建模提升点云理解，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的MAE方法使用固定mask ratio，忽视了多层次表达相关性和点云本身的几何多样性，导致重建假设与实际分布不符，性能受限。

Method: Point-SRA 方法结合自蒸馏和概率建模。采用多种mask比率挖掘互补信息，并提出MeanFlow Transformer（MFT）实现跨模态条件概率重建。在MAE与MFT层面建立双重自表示对齐机制，并设计Flow-Conditioned微调架构，充分利用点云分布。

Result: Point-SRA在ScanObjectNN分类任务高于Point-MAE 5.37%；在颅内动脉瘤分割任务达到96.07%（动脉mean IoU）和86.87%（动脉瘤mean IoU）；3D目标检测上AP@50达47.3%，高于现有MaskPoint 5.12%。

Conclusion: Point-SRA在多项3D任务中显著超越现有主流方法，验证了多mask比率和概率建模、多层自对齐的有效性。

Abstract: Masked autoencoders (MAE) have become a dominant paradigm in 3D representation learning, setting new performance benchmarks across various downstream tasks. Existing methods with fixed mask ratio neglect multi-level representational correlations and intrinsic geometric structures, while relying on point-wise reconstruction assumptions that conflict with the diversity of point cloud. To address these issues, we propose a 3D representation learning method, termed Point-SRA, which aligns representations through self-distillation and probabilistic modeling. Specifically, we assign different masking ratios to the MAE to capture complementary geometric and semantic information, while the MeanFlow Transformer (MFT) leverages cross-modal conditional embeddings to enable diverse probabilistic reconstruction. Our analysis further reveals that representations at different time steps in MFT also exhibit complementarity. Therefore, a Dual Self-Representation Alignment mechanism is proposed at both the MAE and MFT levels. Finally, we design a Flow-Conditioned Fine-Tuning Architecture to fully exploit the point cloud distribution learned via MeanFlow. Point-SRA outperforms Point-MAE by 5.37% on ScanObjectNN. On intracranial aneurysm segmentation, it reaches 96.07% mean IoU for arteries and 86.87% for aneurysms. For 3D object detection, Point-SRA achieves 47.3% AP@50, surpassing MaskPoint by 5.12%.

</details>


### [102] [MANGO:Natural Multi-speaker 3D Talking Head Generation via 2D-Lifted Enhancement](https://arxiv.org/abs/2601.01749)
*Lei Zhu,Lijian Lin,Ye Zhu,Jiahao Wu,Xuehan Hou,Yu Li,Yunfei Liu,Jie Chen*

Main category: cs.CV

TL;DR: 提出了MANGO框架，实现了更自然的语音驱动3D头像双向对话，能准确还原多说话者对话过程中的听说状态与面部动态。


<details>
  <summary>Details</summary>
Motivation: 现有的音频驱动3D头部生成主要集中在单说话人，无法自然切换听与说的状态，且伪3D标签存在误差，导致面部细节还原不真实。

Method: 提出两阶段MANGO框架：第一阶段用扩散式Transformer结合双音频交互建模多说话者3D动作；第二阶段通过3D高斯渲染器生成高质量图像并用2D级光度监督提升3D动作效果，同时采用交替训练减少伪3D标签噪声。此外，发布了包含大量2D-3D配准对话数据集MANGO-Dialog。

Result: 实验表明，MANGO方法在模拟双人3D对话动作、音频驱动说话头还原度与控制精度方面表现显著优于现有技术。

Conclusion: MANGO框架极大提升了音频驱动3D头像对话场景中动作的真实度和可控性，对3D虚拟人、语音动画等方向有重要推动作用。

Abstract: Current audio-driven 3D head generation methods mainly focus on single-speaker scenarios, lacking natural, bidirectional listen-and-speak interaction. Achieving seamless conversational behavior, where speaking and listening states transition fluidly remains a key challenge. Existing 3D conversational avatar approaches rely on error-prone pseudo-3D labels that fail to capture fine-grained facial dynamics. To address these limitations, we introduce a novel two-stage framework MANGO, which leveraging pure image-level supervision by alternately training to mitigate the noise introduced by pseudo-3D labels, thereby achieving better alignment with real-world conversational behaviors. Specifically, in the first stage, a diffusion-based transformer with a dual-audio interaction module models natural 3D motion from multi-speaker audio. In the second stage, we use a fast 3D Gaussian Renderer to generate high-fidelity images and provide 2D-level photometric supervision for the 3D motions through alternate training. Additionally, we introduce MANGO-Dialog, a high-quality dataset with over 50 hours of aligned 2D-3D conversational data across 500+ identities. Extensive experiments demonstrate that our method achieves exceptional accuracy and realism in modeling two-person 3D dialogue motion, significantly advancing the fidelity and controllability of audio-driven talking heads.

</details>


### [103] [CTIS-QA: Clinical Template-Informed Slide-level Question Answering for Pathology](https://arxiv.org/abs/2601.01769)
*Hao Lu,Ziniu Qian,Yifu Li,Yang Zhou,Bingzheng Wei,Yan Xu*

Main category: cs.CV

TL;DR: 本论文提出了一种基于临床诊断模板的流程，用于系统收集和结构化病理信息，并开发了一套创新的数据集和模型，推动数字病理智能问答领域发展。


<details>
  <summary>Details</summary>
Motivation: 目前病理报告信息结构化程度低，缺乏标准化和高质量的大规模病理图文对齐数据，限制了视觉语言模型在临床诊断中的应用。

Method: 作者与病理学专家合作，参考美国病理学会（CAP）肿瘤协议，设计了临床病理报告模板（CPRT），用于从病理报告中系统化、标准化提取关键诊断要素。基于该模板开发了CTIS-Align（大规模图文配对数据集）和CTIS-Bench（结构化VQA基准）。模型方面，提出了CTIS-QA（幻灯片级问答模型），采用双流架构提取全局上下文和局部特征信息。

Result: 实验证明，CTIS-QA在WSI-VQA、CTIS-Bench及多项幻灯片级诊断任务上，在多个指标上均优于现有最先进模型。

Conclusion: 本文为病理报告结构化和数字病理智能问答领域提供了系统的数据与方法支撑，将推动病理智能诊断进一步发展。

Abstract: In this paper, we introduce a clinical diagnosis template-based pipeline to systematically collect and structure pathological information. In collaboration with pathologists and guided by the the College of American Pathologists (CAP) Cancer Protocols, we design a Clinical Pathology Report Template (CPRT) that ensures comprehensive and standardized extraction of diagnostic elements from pathology reports. We validate the effectiveness of our pipeline on TCGA-BRCA. First, we extract pathological features from reports using CPRT. These features are then used to build CTIS-Align, a dataset of 80k slide-description pairs from 804 WSIs for vision-language alignment training, and CTIS-Bench, a rigorously curated VQA benchmark comprising 977 WSIs and 14,879 question-answer pairs. CTIS-Bench emphasizes clinically grounded, closed-ended questions (e.g., tumor grade, receptor status) that reflect real diagnostic workflows, minimize non-visual reasoning, and require genuine slide understanding. We further propose CTIS-QA, a Slide-level Question Answering model, featuring a dual-stream architecture that mimics pathologists' diagnostic approach. One stream captures global slide-level context via clustering-based feature aggregation, while the other focuses on salient local regions through attention-guided patch perception module. Extensive experiments on WSI-VQA, CTIS-Bench, and slide-level diagnostic tasks show that CTIS-QA consistently outperforms existing state-of-the-art models across multiple metrics. Code and data are available at https://github.com/HLSvois/CTIS-QA.

</details>


### [104] [Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery](https://arxiv.org/abs/2601.01781)
*Lakshay Sharma,Alex Marin*

Main category: cs.CV

TL;DR: 本文提出了一种新的自监督预训练任务Subimage Overlap Prediction，仅需较少的预训练数据即可帮助实现遥感图像的语义分割，并且在下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法虽然能够生成通用模型，可迁移到下游监督任务，但普遍依赖大量的预训练数据。为降低对大规模数据的依赖，提高少标签场景下的性能，作者提出新的预训练方式。

Method: 方法为Subimage Overlap Prediction：从原始图像中提取子图像，模型需预测该子图像在原图中的语义掩码位置，以此进行自监督训练。

Result: 实验证明：相比传统方法，该方法在下游语义分割任务上实现更快收敛速度，表现（mIoU）持平或更好，并且在有更少标注数据时优势更明显，可适配多种架构和数据集。此外，相比其他自监督方式，本方法极大减少了对预训练数据量的需求。

Conclusion: 提出的Subimage Overlap Prediction可用显著更少的预训练数据，提升遥感图像语义分割的效果，尤其适用于标注数据稀缺的情形，具备较强应用前景。

Abstract: Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.

</details>


### [105] [DDNet: A Dual-Stream Graph Learning and Disentanglement Framework for Temporal Forgery Localization](https://arxiv.org/abs/2601.01784)
*Boyang Zhao,Xin Liao,Jiaxin Chen,Xiaoshuai Wu,Yufeng Wu*

Main category: cs.CV

TL;DR: 该论文提出了一种用于视频时间段伪造定位（TFL）的新方法DDNet，通过双流图学习和特征解耦，有效提升了对视频中伪造片段的精确检测能力。实验结果显示，相比最新方法性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 随着AIGC技术的发展，视频伪造手段变得更加隐蔽，仅需篡改极小片段即可误导观众，使得整体级别检测方法失效。因此，精确定位视频中被篡改片段的技术（TFL）变得至关重要。

Method: 提出了DDNet，包括时序距离流（捕捉局部伪造）和语义内容流（捕捉全局联系），结合痕迹解耦与适应（TDA）分离通用伪造特征，以及跨层特征嵌入（CLFE）构建更鲁棒的特征。该方法融合了局部和全局信息，使模型更好地发现被隐藏的伪造区域。

Result: 在ForgeryNet和TVIL数据集上，DDNet在AP@0.95指标上相较其它最新方法提升约9%，且在跨领域实验中表现出更强的稳健性。

Conclusion: DDNet能够有效弥补现有TFL方法对全局信息捕捉不足的问题，实现更精准和鲁棒的视频篡改检测，对实际AIGC伪造视频具有较高应用价值。

Abstract: The rapid evolution of AIGC technology enables misleading viewers by tampering mere small segments within a video, rendering video-level detection inaccurate and unpersuasive. Consequently, temporal forgery localization (TFL), which aims to precisely pinpoint tampered segments, becomes critical. However, existing methods are often constrained by \emph{local view}, failing to capture global anomalies. To address this, we propose a \underline{d}ual-stream graph learning and \underline{d}isentanglement framework for temporal forgery localization (DDNet). By coordinating a \emph{Temporal Distance Stream} for local artifacts and a \emph{Semantic Content Stream} for long-range connections, DDNet prevents global cues from being drowned out by local smoothness. Furthermore, we introduce Trace Disentanglement and Adaptation (TDA) to isolate generic forgery fingerprints, alongside Cross-Level Feature Embedding (CLFE) to construct a robust feature foundation via deep fusion of hierarchical features. Experiments on ForgeryNet and TVIL benchmarks demonstrate that our method outperforms state-of-the-art approaches by approximately 9\% in AP@0.95, with significant improvements in cross-domain robustness.

</details>


### [106] [VerLM: Explaining Face Verification Using Natural Language](https://arxiv.org/abs/2601.01798)
*Syed Abdul Hannan,Hazim Bukhari,Thomas Cantalapiedra,Eman Ansar,Massa Baali,Rita Singh,Bhiksha Raj*

Main category: cs.CV

TL;DR: 本文提出了一种创新的视觉-语言模型（VLM）用于人脸验证，在准确判断两张脸是否为同一人的同时，能清晰解释决策原因，并在准确性和可解释性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管人脸验证系统取得了很大进展，但通常缺乏决策过程的透明度，无法解释为什么给出某一判断。

Method: 提出并训练了一种用于人脸验证的VLM模型，能够生成两种风格的解释（简明版和详细版），并将用于音频区分的先进方法迁移、改造为适用于视觉任务。模型结合了特征提取与推理能力。

Result: 该方法在准确率和可解释性方面均超越了基线和现有方法，表现出更优的性能。

Conclusion: 视觉-语言模型在提升人脸验证系统的透明度、可靠性和可解释性方面具有巨大潜力，推动了人脸验证系统的发展。

Abstract: Face verification systems have seen substantial advancements; however, they often lack transparency in their decision-making processes. In this paper, we introduce an innovative Vision-Language Model (VLM) for Face Verification, which not only accurately determines if two face images depict the same individual but also explicitly explains the rationale behind its decisions. Our model is uniquely trained using two complementary explanation styles: (1) concise explanations that summarize the key factors influencing its decision, and (2) comprehensive explanations detailing the specific differences observed between the images. We adapt and enhance a state-of-the-art modeling approach originally designed for audio-based differentiation to suit visual inputs effectively. This cross-modal transfer significantly improves our model's accuracy and interpretability. The proposed VLM integrates sophisticated feature extraction techniques with advanced reasoning capabilities, enabling clear articulation of its verification process. Our approach demonstrates superior performance, surpassing baseline methods and existing models. These findings highlight the immense potential of vision language models in face verification set up, contributing to more transparent, reliable, and explainable face verification systems.

</details>


### [107] [Causality-Aware Temporal Projection for Video Understanding in Video-LLMs](https://arxiv.org/abs/2601.01804)
*Zhengjian Kang,Qi Chen,Rui Liu,Kangtong Mo,Xingyu Zhang,Xiaoyu Deng,Ye Zhang*

Main category: cs.CV

TL;DR: V-CORE是一个高效的视频大语言模型架构，通过引入显式的时序约束，提升了对视频因果与时序理解的能力，在多个视频问答基准上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前主流的Video-LLM由于依赖双向投影操作，导致时间顺序感模糊，后续帧信息可能反向影响早期帧，缺乏对视频时序与因果推理的建模机制。作者旨在弥补这一不足，提升模型处理时序与因果推理任务的能力。

Method: 提出V-CORE框架，包含两个创新组件：（1）可学习空间聚合（LSA），用于自适应挑选空间上的关键信息并减少冗余；（2）因果感知时序投影器（CATP），通过块状因果注意力及终端动态摘要token，实现结构化、单向的信息流，保证时序信息严格有序地聚合。整个模型采用4-bit QLoRA、冻结预训练大语言模型骨干，可在消费级GPU上高效训练。

Result: V-CORE在NExT-QA基准上取得61.2%准确率，并在MSVD-QA、MSRVTT-QA、TGIF-QA等数据集上表现竞争力，特别是在时序推理和因果推理子任务上，分别提升了3.5%和5.2%。

Conclusion: 通过引入显式的时序约束和因果结构，V-CORE有效提升了视频大语言模型在时序与因果推理相关任务上的表现，验证了结构性时序机制的重要性。

Abstract: Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.

</details>


### [108] [Adaptive Hybrid Optimizer based Framework for Lumpy Skin Disease Identification](https://arxiv.org/abs/2601.01807)
*Ubaidullah,Muhammad Abid Hussain,Mohsin Raza Jafri,Rozi Khan,Moid Sandhu,Abd Ullah Khan,Hyundong Shin*

Main category: cs.CV

TL;DR: 本文提出了一种新的混合深度学习方法LUMPNet，用于早期检测牛结节性皮肤病（LSD），且在相关数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 牛结节性皮肤病传播迅速，对畜牧健康、全球经济及粮食安全威胁严重。因此，亟需高效、准确的早期检测方法以防控疫情扩散。

Method: 提出基于图像分析的LUMPNet方法，结合YOLOv11进行病变定位，EfficientNet进行类别分类，并设计了新型自适应混合优化器以提升模型训练稳定性和速度。评估分别在训练和验证阶段及实际案例中进行。

Result: LUMPNet取得99%的训练检测准确率和98%的验证准确率，显著优于现有方法。与优化后的EfficientNet-B0+AdamW模型对比，LUMPNet同样表现更优。

Conclusion: LUMPNet能够高效、准确地进行LSD早期检测，对疾病控制具有应用前景。该方法优于传统检测方案，适用于畜牧业早期病害监测。

Abstract: Lumpy Skin Disease (LSD) is a contagious viral infection that significantly deteriorates livestock health, thereby posing a serious threat to the global economy and food security. Owing to its rapid spread characteristics, early and precise identification is crucial to prevent outbreaks and ensure timely intervention. In this paper, we propose a hybrid deep learning-based approach called LUMPNet for the early detection of LSD. LUMPNet utilizes image data to detect and classify skin nodules -- the primary indicator of LSD. To this end, LUMPNet uses YOLOv11, EfficientNet-based CNN classifier with compound scaling, and a novel adaptive hybrid optimizer. More precisely, LUMPNet detects and localizes LSD skin nodules and lesions on cattle images. It exploits EfficientNet to classify the localized cattle images into LSD-affected or healthy categories. To stabilize and accelerate the training of YOLOv11 and EfficientNet hybrid model, a novel adaptive hybrid optimizer is proposed and utilized. We evaluate LUMPNet at various stages of LSD using a publicly available dataset. Results indicate that the proposed scheme achieves 99% LSD detection training accuracy, and outperforms existing schemes. The model also achieves validation accuracy of 98%. Moreover, for further evaluation, we conduct a case study using an optimized EfficientNet-B0 model trained with the AdamW optimizer, and compare its performance with LUMPNet. The results show that LUMPNet achieves superior performance.

</details>


### [109] [Robust Egocentric Visual Attention Prediction Through Language-guided Scene Context-aware Learning](https://arxiv.org/abs/2601.01818)
*Sungjune Park,Hongda Mao,Qingshuang Chen,Yong Man Ro,Yelin Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种语言引导的场景上下文感知学习框架，以提升第一人称视觉关注预测的准确性，并在主流数据集上取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 第一人称视觉视频分析需求增长，视觉关注预测因场景复杂和模糊而具有挑战性。现有方法未充分利用场景语义与上下文信息，而心理学证据表明上下文对于人类视觉关注很重要。因此，作者希望通过场景描述提升预测效果。

Method: 作者设计了一个语言引导的上下文感知模块，利用自然语言描述来总结视频内容，获得更具场景感知的视频表示。同时，提出了两大训练目标：一是引导模型关注兴趣点区域，二是抑制无关区域干扰。

Result: 该方法在Ego4D和AEA等主流第一人称视频数据集上进行了大量实验，表现出色，并取得了当前最好或领先水平的性能，且在多样且动态的场景中表现出更强的鲁棒性。

Conclusion: 通过引入语言引导和上下文感知机制，显著提高了第一人称视觉关注预测的效果，为后续相关研究提供了新思路。

Abstract: As the demand for analyzing egocentric videos grows, egocentric visual attention prediction, anticipating where a camera wearer will attend, has garnered increasing attention. However, it remains challenging due to the inherent complexity and ambiguity of dynamic egocentric scenes. Motivated by evidence that scene contextual information plays a crucial role in modulating human attention, in this paper, we present a language-guided scene context-aware learning framework for robust egocentric visual attention prediction. We first design a context perceiver which is guided to summarize the egocentric video based on a language-based scene description, generating context-aware video representations. We then introduce two training objectives that: 1) encourage the framework to focus on the target point-of-interest regions and 2) suppress distractions from irrelevant regions which are less likely to attract first-person attention. Extensive experiments on Ego4D and Aria Everyday Activities (AEA) datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance and enhanced robustness across diverse, dynamic egocentric scenarios.

</details>


### [110] [RSwinV2-MD: An Enhanced Residual SwinV2 Transformer for Monkeypox Detection from Skin Images](https://arxiv.org/abs/2601.01835)
*Rashid Iqbal,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度学习方法RSwinV2，用于提高Mpox皮损分类能力，取得了高精度和F1分数，优于传统CNN与原始SwinTransformer。


<details>
  <summary>Details</summary>
Motivation: 现有方法在Mpox等皮损的分类精度有限，尤其在区分Mpox、鸡痘、麻疹和牛痘时表现不佳。本研究旨在通过改进视觉Transformer网络，提升自动分类性能，助力临床诊断。

Method: 文章提出的RSwinV2结构基于SwinTransformer，定制了分层Transformer结构，利用不重叠图像块和窗口内注意力机制，以及引入了反向残差块（IRB），结合多头注意力和卷积跳跃连接，增强了模型对全局与局部特征的捕捉能力并解决梯度消失问题。

Result: 在Kaggle公开数据集上RSwinV2模型获得了96.21的准确率和95.62的F1分数，明显优于标准卷积神经网络和原始SwinTransformer模型。

Conclusion: RSwinV2在Mpox等皮损的自动分类方面表现优异，有望作为计算机辅助工具用于皮损判读，提升疾病准确诊断和分类能力。

Abstract: In this paper, a deep learning approach for Mpox diagnosis named Customized Residual SwinTransformerV2 (RSwinV2) has been proposed, trying to enhance the capability of lesion classification by employing the RSwinV2 tool-assisted vision approach. In the RSwinV2 method, a hierarchical structure of the transformer has been customized based on the input dimensionality, embedding structure, and output targeted by the method. In this RSwinV2 approach, the input image has been split into non-overlapping patches and processed using shifted windows and attention in these patches. This process has helped the method link all the windows efficiently by avoiding the locality issues of non-overlapping regions in attention, while being computationally efficient. RSwinV2 has further developed based on SwinTransformer and has included patch and position embeddings to take advantage of the transformer global-linking capability by employing multi-head attention in these embeddings. Furthermore, RSwinV2 has developed and incorporated the Inverse Residual Block (IRB) into this method, which utilizes convolutional skip connections with these inclusive designs to address the vanishing gradient issues during processing. RSwinV2 inclusion of IRB has therefore facilitated this method to link global patterns as well as local patterns; hence, its integrity has helped improve lesion classification capability by minimizing variability of Mpox and increasing differences of Mpox, chickenpox, measles, and cowpox. In testing SwinV2, its accuracy of 96.21 and an F1score of 95.62 have been achieved on the Kaggle public dataset, which has outperformed standard CNN models and SwinTransformers; RSwinV2 vector has thus proved its valiance as a computer-assisted tool for Mpox lesion observation interpretation.

</details>


### [111] [ESGaussianFace: Emotional and Stylized Audio-Driven Facial Animation via 3D Gaussian Splatting](https://arxiv.org/abs/2601.01847)
*Chuhang Ma,Shuai Tan,Ye Pan,Jiaolong Yang,Xin Tong*

Main category: cs.CV

TL;DR: 本文提出了ESGaussianFace框架，实现了高效、高质量且具有情感和风格表达的音频驱动三维人脸动画生成，并在多个维度超越了当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 目前音频驱动的人脸动画大多仅能生成中性表情，对于同时融合情感与风格特征的高质量三维动画依然存在较大挑战。

Method: 该方法利用3D Gaussian Splatting重建三维场景，通过情感-音频引导的空间注意力机制融合情感与音频特征，并引入两个三维高斯点变形预测器进行情感和风格的变形。此外，采用多阶段训练方式逐步学习口型、表情与风格特征。

Result: 生成视频在三维一致性、效率、质量和情感风格表现上均优于现有技术。实验表明其口型准确率、表情变化和风格表现均超过最新技术。

Conclusion: ESGaussianFace能够高效生成富有情感和风格的人脸动画，提升动画真实感与表现力，为音频驱动虚拟人等应用提供了更优方案。

Abstract: Most current audio-driven facial animation research primarily focuses on generating videos with neutral emotions. While some studies have addressed the generation of facial videos driven by emotional audio, efficiently generating high-quality talking head videos that integrate both emotional expressions and style features remains a significant challenge. In this paper, we propose ESGaussianFace, an innovative framework for emotional and stylized audio-driven facial animation. Our approach leverages 3D Gaussian Splatting to reconstruct 3D scenes and render videos, ensuring efficient generation of 3D consistent results. We propose an emotion-audio-guided spatial attention method that effectively integrates emotion features with audio content features. Through emotion-guided attention, the model is able to reconstruct facial details across different emotional states more accurately. To achieve emotional and stylized deformations of the 3D Gaussian points through emotion and style features, we introduce two 3D Gaussian deformation predictors. Futhermore, we propose a multi-stage training strategy, enabling the step-by-step learning of the character's lip movements, emotional variations, and style features. Our generated results exhibit high efficiency, high quality, and 3D consistency. Extensive experimental results demonstrate that our method outperforms existing state-of-the-art techniques in terms of lip movement accuracy, expression variation, and style feature expressiveness.

</details>


### [112] [GCR: Geometry-Consistent Routing for Task-Agnostic Continual Anomaly Detection](https://arxiv.org/abs/2601.01856)
*Joongwon Chae,Lihui Luo,Yang Liu,Runming Wang,Dongmei Yu,Zeming Liang,Xi Yuan,Dayan Zhang,Zhenglin Chen,Peiwu Qin,Ilmoon Chae*

Main category: cs.CV

TL;DR: 本文提出了一种新的异常检测方法GCR（几何一致路由），通过引入几何一致性路由机制，提升在类别不断扩展情况下的异常检测稳定性，更适合实际工业检测环境，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于特征的异常检测大多关注同类别内的评分改进，但实际部署中不断扩展的类别和测试时未知类别身份对模型的泛化与路由能力提出了新的挑战。传统对异常分数的直接比较因不同类别得分分布差异大，易导致路由不稳定，影响系统整体性能。

Method: 提出GCR（几何一致路由）框架，在共享且冻结的Patch-Embedding空间，根据累计最近原型距离将输入图像路由到对应类别专家，再在该专家内进行标准原型异常评分，实现交头决策与内部检测分离，从而避免了分数不可比带来的问题，无需端到端再训练。

Result: 在MVTec AD和VisA数据集上的实验表明，GCR大幅提升了路由稳定性，有效缓解了性能崩溃和遗忘问题，同时保持了检测和定位的高水平性能。

Conclusion: 持续异常检测中的许多性能问题，实则源自于决策规则的不稳定而非表征遗忘。GCR通过几何一致路由显著增强了系统在真实环境下的泛化与稳定性，是工业检测场景下更优的选择。

Abstract: Feature-based anomaly detection is widely adopted in industrial inspection due to the strong representational power of large pre-trained vision encoders. While most existing methods focus on improving within-category anomaly scoring, practical deployments increasingly require task-agnostic operation under continual category expansion, where the category identity is unknown at test time. In this setting, overall performance is often dominated by expert selection, namely routing an input to an appropriate normality model before any head-specific scoring is applied. However, routing rules that compare head-specific anomaly scores across independently constructed heads are unreliable in practice, as score distributions can differ substantially across categories in scale and tail behavior.
  We propose GCR, a lightweight mixture-of-experts framework for stabilizing task-agnostic continual anomaly detection through geometry-consistent routing. GCR routes each test image directly in a shared frozen patch-embedding space by minimizing an accumulated nearest-prototype distance to category-specific prototype banks, and then computes anomaly maps only within the routed expert using a standard prototype-based scoring rule. By separating cross-head decision making from within-head anomaly scoring, GCR avoids cross-head score comparability issues without requiring end-to-end representation learning.
  Experiments on MVTec AD and VisA show that geometry-consistent routing substantially improves routing stability and mitigates continual performance collapse, achieving near-zero forgetting while maintaining competitive detection and localization performance. These results indicate that many failures previously attributed to representation forgetting can instead be explained by decision-rule instability in cross-head routing. Code is available at https://github.com/jw-chae/GCR

</details>


### [113] [RRNet: Configurable Real-Time Video Enhancement with Arbitrary Local Lighting Variations](https://arxiv.org/abs/2601.01865)
*Wenlong Yang,Canran Jin,Weihang Yuan,Chao Wang,Lifeng Sun*

Main category: cs.CV

TL;DR: RRNet是一种轻量级的视频实时曝光优化网络，通过虚拟光源与深度感知，实现兼具速度与效果的智能补光和曝光控制，尤其适用于视频会议、AR、美颜等应用。


<details>
  <summary>Details</summary>
Motivation: 现有实时视频增强方法在处理不均匀照明下常常难以兼顾速度与曝光优化效果，影响实际应用体验（如视频会议、移动摄影等）。

Method: 提出RRNet框架，通过少量虚拟光源参数建模与深度感知渲染模块，实现局部补光，无需像素对齐监督数据。网络结构采用高效编码器和轻量预测头。为训练网络，开发了基于生成式AI的数据集合成管线，低成本合成多样照明场景数据。

Result: RRNet在低光增强、局部照明调整和眩光去除等任务上，视觉质量和效率均优于现有方法，支持高分辨率实时处理并保持人脸身份。

Conclusion: RRNet结合可解释照明控制与高效架构，非常适合实际视频增强应用，为视频会议、增强现实与移动摄影等场景带来更优视觉体验。

Abstract: With the growing demand for real-time video enhancement in live applications, existing methods often struggle to balance speed and effective exposure control, particularly under uneven lighting. We introduce RRNet (Rendering Relighting Network), a lightweight and configurable framework that achieves a state-of-the-art tradeoff between visual quality and efficiency. By estimating parameters for a minimal set of virtual light sources, RRNet enables localized relighting through a depth-aware rendering module without requiring pixel-aligned training data. This object-aware formulation preserves facial identity and supports real-time, high-resolution performance using a streamlined encoder and lightweight prediction head. To facilitate training, we propose a generative AI-based dataset creation pipeline that synthesizes diverse lighting conditions at low cost. With its interpretable lighting control and efficient architecture, RRNet is well suited for practical applications such as video conferencing, AR-based portrait enhancement, and mobile photography. Experiments show that RRNet consistently outperforms prior methods in low-light enhancement, localized illumination adjustment, and glare removal.

</details>


### [114] [Entity-Guided Multi-Task Learning for Infrared and Visible Image Fusion](https://arxiv.org/abs/2601.01870)
*Wenyu Shao,Hongbo Liu,Yunchuan Ma,Ruili Wang*

Main category: cs.CV

TL;DR: 本文提出了一种实体引导的多任务学习方法（EGMT），用于改进红外与可见光图像融合，显著提升融合图像的语义一致性和细节保留。


<details>
  <summary>Details</summary>
Motivation: 现有的文本驱动图像融合方法大多利用句子级的文本信息，带来多余的语义噪声，且未能充分挖掘文本的深层语义价值，因此需要新的方法以更精细地利用文本信息提升图像融合质量。

Method: 1. 提出基于大规模视觉语言模型自动生成图像描述，再从中提取实体级文本信息，去除多余语义噪声，仅保留关键语义。
2. 构建并行多任务学习结构：主任务为图像融合，辅助任务为多标签分类（实体作为伪标签），实现有效语义监督，提升模型对图像内容的理解。
3. 设计实体引导的跨模态交互模块，实现视觉和实体级文本特征的细粒度融合，捕获多层次的跨模态依赖关系。

Result: 在TNO、RoadScene、M3FD和MSRS四个公开数据集上进行了广泛实验，EGMT方法相比现有最先进方法，在显著目标保持、纹理细节以及语义一致性等方面都获得了优异表现。

Conclusion: EGMT通过实体级文本信息挖掘、多任务学习和跨模态交互，极大提升了红外与可见光图像融合的语义表达能力和图像质量，并公开了实体标注数据集与代码以推动领域发展。

Abstract: Existing text-driven infrared and visible image fusion approaches often rely on textual information at the sentence level, which can lead to semantic noise from redundant text and fail to fully exploit the deeper semantic value of textual information. To address these issues, we propose a novel fusion approach named Entity-Guided Multi-Task learning for infrared and visible image fusion (EGMT). Our approach includes three key innovative components: (i) A principled method is proposed to extract entity-level textual information from image captions generated by large vision-language models, eliminating semantic noise from raw text while preserving critical semantic information; (ii) A parallel multi-task learning architecture is constructed, which integrates image fusion with a multi-label classification task. By using entities as pseudo-labels, the multi-label classification task provides semantic supervision, enabling the model to achieve a deeper understanding of image content and significantly improving the quality and semantic density of the fused image; (iii) An entity-guided cross-modal interactive module is also developed to facilitate the fine-grained interaction between visual and entity-level textual features, which enhances feature representation by capturing cross-modal dependencies at both inter-visual and visual-entity levels. To promote the wide application of the entity-guided image fusion framework, we release the entity-annotated version of four public datasets (i.e., TNO, RoadScene, M3FD, and MSRS). Extensive experiments demonstrate that EGMT achieves superior performance in preserving salient targets, texture details, and semantic consistency, compared to the state-of-the-art methods. The code and dataset will be publicly available at https://github.com/wyshao-01/EGMT.

</details>


### [115] [CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving](https://arxiv.org/abs/2601.01874)
*Shuhang Chen,Yunqiu Xu,Junjie Xie,Aojun Lu,Tao Feng,Zeying Huang,Ning Zhang,Yi Sun,Yi Yang,Hangjie Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种新的类认知三阶段视觉数学推理框架CogFlow，包括感知、内化和推理三个阶段，并在每个阶段分别提升模型能力，显著提升了多模态大模型在视觉数学推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大语言模型在视觉数学推理上效果不佳，已有方法多集中于视觉感知改进，忽略了视觉信息是否被真实有效地整合进后续推理中。作者希望解决视觉信息在推理流程中的整合与利用问题，模拟人类分阶段的推理过程。

Method: CogFlow模拟人类推理三阶段：感知→内化→推理。针对每一阶段提出增效措施，包括：利用协同视觉奖励提升感知阶段符号和图示的视觉提取能力；在内化阶段设计知识内化奖励促进视觉信息与语言信息充分整合；推理阶段采用视觉门控策略优化，确保模型推理充分依赖视觉知识。还新构建了一个高质量的数据集MathCog辅助训练和评估。

Result: 在多个主流视觉数学推理基准测试上，CogFlow均取得了优于现有方法的性能，表明其有效提升了多模态大模型的视觉数学推理能力。同时实验也证明了每个模块和奖励机制的有效性。

Conclusion: CogFlow框架能够系统性提升多阶段视觉数学推理表现，尤其通过模拟人类感知-内化-推理的层级流，首次保证了视觉信息能被真实有效地融合到推理流程中，为多模态大模型视觉数学推理研究提供了新范式和方向。

Abstract: Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\Rightarrow$internalization$\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.

</details>


### [116] [Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems](https://arxiv.org/abs/2601.01891)
*Niloufar Alipour Talemi,Julia Boone,Fatemeh Afghah*

Main category: cs.CV

TL;DR: 本综述系统梳理了Agentic AI在遥感领域的应用现状，提出分类体系，分析关键架构及挑战，并指明了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 遥感数据分析变得日益复杂，传统静态深度学习模型已难以应对复杂地理空间任务。近期多模态大模型虽提升了表达能力，但在复杂工作流中的序列规划和工具协同方面存在短板。因此，有必要系统评估和总结自主Agentic AI在遥感领域的应用和前景。

Method: 作者提出了统一的Agentic AI分类法，区分单一copilot和多智能体系统，并结合文献梳理，分析了这些系统的规划机制、检索增强生成、记忆结构等核心架构。同时，作者还回顾了遥感领域的最新评价基准，并关注基于轨迹推理的评测标准。

Result: 综述发现，Agentic AI在遥感领域展现出规划能力和自治性方面的潜力，更适应复杂任务；但也面临如知识锚定、系统安全及多工具编排等不足。此外，新的评测标准推动了模型性能评价方式的转变。

Conclusion: 该文为遥感Agentic AI研究构建了理论体系、指出了现有限制，并提出实现稳健自主地理空间智能的战略路径，对行业和学术界均具重要指导意义。

Abstract: The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.

</details>


### [117] [Forget Less by Learning from Parents Through Hierarchical Relationships](https://arxiv.org/abs/2601.01892)
*Arjun Ramesh Kaushik,Naresh Kumar Devulapally,Vishnu Suresh Lokhande,Nalini K. Ratha,Venu Govindaraju*

Main category: cs.CV

TL;DR: 本文提出了一种新框架FLLP，通过在超曲空间下引入父子概念间的学习关系，减少定制扩散模型在顺序学习新概念时的遗忘现象。该方法有效提升了模型的鲁棒性和泛化能力，实验在公开数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 定制扩散模型在生成任务上具备强大的个性化能力，但在连续学习新概念时易发生灾难性遗忘。现有方法多致力于减少不同概念间的相互干扰，忽略了概念间可能存在的正向有益互动。因此，作者希望设计一种能够同时保留已学知识和更好整合新知识的方法。

Method: 作者提出FLLP（Forget Less by Learning from Parents）框架，将概念表示嵌入到Lorentz超曲空间（适于建模树状层级结构），通过定义父子（先学-后学）概念关系，由已学概念指导新概念学习，从而在保留旧知识的同时顺利融入新知识。

Result: FLLP在三个公开数据集和一个合成基准任务上进行了实验评估，结果表明该方法在鲁棒性和泛化性方面均有显著且一致的提升。

Conclusion: FLLP框架能够有效缓解扩散模型的灾难性遗忘现象，实现知识的持续集成，并充分挖掘新旧概念间的正向互动关系，提升总体表现。

Abstract: Custom Diffusion Models (CDMs) offer impressive capabilities for personalization in generative modeling, yet they remain vulnerable to catastrophic forgetting when learning new concepts sequentially. Existing approaches primarily focus on minimizing interference between concepts, often neglecting the potential for positive inter-concept interactions. In this work, we present Forget Less by Learning from Parents (FLLP), a novel framework that introduces a parent-child inter-concept learning mechanism in hyperbolic space to mitigate forgetting. By embedding concept representations within a Lorentzian manifold, naturally suited to modeling tree-like hierarchies, we define parent-child relationships in which previously learned concepts serve as guidance for adapting to new ones. Our method not only preserves prior knowledge but also supports continual integration of new concepts. We validate FLLP on three public datasets and one synthetic benchmark, showing consistent improvements in both robustness and generalization.

</details>


### [118] [Nodule-DETR: A Novel DETR Architecture with Frequency-Channel Attention for Ultrasound Thyroid Nodule Detection](https://arxiv.org/abs/2601.01908)
*Jingjing Wang,Qianglin Liu,Zhuo Xiao,Xinning Yao,Bo Liu,Lu Li,Lijuan Niu,Fugen Zhou*

Main category: cs.CV

TL;DR: 该文提出了一种新型的DETR架构（Nodule-DETR），用于提高超声图像中甲状腺结节的检测准确率，在实测数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 超声作为甲状腺结节筛查的首选工具，存在图像对比度低和结节边界模糊等问题，影响诊断准确性。

Method: 该方法提出Nodule-DETR，包括三个主要创新：1）多光谱频域通道注意力模块（MSFCA），利用频域信息增强低对比度结节特征；2）层次特征融合模块（HFF），高效多尺度整合特征；3）多尺度可变形注意力（MSDA），提升对小且形状不规则结节的检测能力。

Result: 在真实临床甲状腺超声图像数据集上，Nodule-DETR在mAP@0.5:0.95上超越基线模型0.149，达到目前最新水平。

Conclusion: Nodule-DETR大幅提升了甲状腺结节超声影像的检测准确性，具备显著的临床应用潜力。

Abstract: Thyroid cancer is the most common endocrine malignancy, and its incidence is rising globally. While ultrasound is the preferred imaging modality for detecting thyroid nodules, its diagnostic accuracy is often limited by challenges such as low image contrast and blurred nodule boundaries. To address these issues, we propose Nodule-DETR, a novel detection transformer (DETR) architecture designed for robust thyroid nodule detection in ultrasound images. Nodule-DETR introduces three key innovations: a Multi-Spectral Frequency-domain Channel Attention (MSFCA) module that leverages frequency analysis to enhance features of low-contrast nodules; a Hierarchical Feature Fusion (HFF) module for efficient multi-scale integration; and Multi-Scale Deformable Attention (MSDA) to flexibly capture small and irregularly shaped nodules. We conducted extensive experiments on a clinical dataset of real-world thyroid ultrasound images. The results demonstrate that Nodule-DETR achieves state-of-the-art performance, outperforming the baseline model by a significant margin of 0.149 in mAP@0.5:0.95. The superior accuracy of Nodule-DETR highlights its significant potential for clinical application as an effective tool in computer-aided thyroid diagnosis. The code of work is available at https://github.com/wjj1wjj/Nodule-DETR.

</details>


### [119] [Learning Action Hierarchies via Hybrid Geometric Diffusion](https://arxiv.org/abs/2601.01914)
*Arjun Ramesh Kaushik,Nalini K. Ratha,Venu Govindaraju*

Main category: cs.CV

TL;DR: 本文提出了一种创新性的时序动作分割方法HybridTAS，结合了欧氏和双曲几何结构，在扩散模型降噪过程中更好地利用动作的层次结构，实现了分割性能的新突破。


<details>
  <summary>Details</summary>
Motivation: 现有时序动作分割方法虽然进展显著，但未能充分利用动作标签间的层次性，这限制了模型对复杂动作序列的理解能力。

Method: 提出HybridTAS框架，将欧氏与双曲几何嵌入相结合，用于扩散模型的降噪过程。具体方法是在扩散的不同阶段，利用层次结构引导标签预测：在高步数用高层动作类别，低步数用细粒度类别，实现自上而下的精细预测。

Result: 在GTEA、50Salads和Breakfast三个主流数据集上，所提方法均取得了当前最优的分割准确率，展现出强大的实用性和泛化能力。

Conclusion: 结合双曲几何指导的降噪机制可以有效捕捉和利用动作标签的层次信息，从而提升时序动作分割的性能，方法具有较强的理论和实际推广价值。

Abstract: Temporal action segmentation is a critical task in video understanding, where the goal is to assign action labels to each frame in a video. While recent advances leverage iterative refinement-based strategies, they fail to explicitly utilize the hierarchical nature of human actions. In this work, we propose HybridTAS - a novel framework that incorporates a hybrid of Euclidean and hyperbolic geometries into the denoising process of diffusion models to exploit the hierarchical structure of actions. Hyperbolic geometry naturally provides tree-like relationships between embeddings, enabling us to guide the action label denoising process in a coarse-to-fine manner: higher diffusion timesteps are influenced by abstract, high-level action categories (root nodes), while lower timesteps are refined using fine-grained action classes (leaf nodes). Extensive experiments on three benchmark datasets, GTEA, 50Salads, and Breakfast, demonstrate that our method achieves state-of-the-art performance, validating the effectiveness of hyperbolic-guided denoising for the temporal action segmentation task.

</details>


### [120] [TalkPhoto: A Versatile Training-Free Conversational Assistant for Intelligent Image Editing](https://arxiv.org/abs/2601.01915)
*Yujie Hu,Zecheng Tang,Xu Jiang,Weiqi Li,Jian Zhang*

Main category: cs.CV

TL;DR: 本文提出了TalkPhoto框架，实现了无需训练即可通过对话实现高效、精确的图片编辑。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型用于图片编辑虽能增强灵活性和可控性，但多任务数据集的构建非常繁琐，且编辑效果有限。研究者希望开发一种无需繁琐训练但又强大灵活的图片编辑新方法。

Method: 提出TalkPhoto框架，通过特别设计的提示模板，引导开源LLM分析用户需求，并分层调用多种现有图片编辑方法，实现不同编辑任务，而无需训练新模型。此外，框架支持按需插拔，能轻松整合复杂或新颖的编辑任务。

Result: 实验结果显示，该方法在多种图片编辑任务上调用更为精准，消耗更少token，并且获得了更高的编辑质量。

Conclusion: TalkPhoto能够实现灵活高效、无需训练的对话式图片编辑，兼顾可扩展性和高编辑质量，有望推动相关领域发展。

Abstract: Thanks to the powerful language comprehension capabilities of Large Language Models (LLMs), existing instruction-based image editing methods have introduced Multimodal Large Language Models (MLLMs) to promote information exchange between instructions and images, ensuring the controllability and flexibility of image editing. However, these frameworks often build a multi-instruction dataset to train the model to handle multiple editing tasks, which is not only time-consuming and labor-intensive but also fails to achieve satisfactory results. In this paper, we present TalkPhoto, a versatile training-free image editing framework that facilitates precise image manipulation through conversational interaction. We instruct the open-source LLM with a specially designed prompt template to analyze user needs after receiving instructions and hierarchically invoke existing advanced editing methods, all without additional training. Moreover, we implement a plug-and-play and efficient invocation of image editing methods, allowing complex and unseen editing tasks to be integrated into the current framework, achieving stable and high-quality editing results. Extensive experiments demonstrate that our method not only provides more accurate invocation with fewer token consumption but also achieves higher editing quality across various image editing tasks.

</details>


### [121] [AR-MOT: Autoregressive Multi-object Tracking](https://arxiv.org/abs/2601.01925)
*Lianjie Jia,Yuhan Wu,Binghao Ran,Yifan Wang,Lijun Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出AR-MOT，将多目标跟踪（MOT）视为序列生成任务，引入大语言模型框架，实现了不依赖特定输出头的灵活跟踪方法。通过实验验证，该方法与现有主流方法效果相当，并提升了拓展性与灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有的MOT方法架构僵化且针对特定任务，限制了它们在多样、复杂场景下的适用性和灵活扩展。作者希望构建一个更通用、更易扩展的MOT系统，方便适配多模态以及指令驱动的更复杂任务。

Method: 提出AR-MOT，将MOT问题转化为在大语言模型（LLM）框架下的序列生成。引入基于预训练检测器的Object Tokenizer以增强区域级视觉感知，提出区域感知对齐模块（RAA）解决全局与局部特征错配，设计时序记忆融合模块（TMF）支持长时跟踪。通过灵活调整输出序列格式，即可支持新模态和指令，无需修改模型结构。

Result: 在MOT17和DanceTrack等主流数据集上，AR-MOT实现了与当前顶尖MOT方法相当的性能，实验展示了模型在多任务和多模态场景下的有效性和可拓展性。

Conclusion: AR-MOT为多目标跟踪提供了一种更通用与灵活的范式。该方法可简便集成新模态或任务需求，有效突破传统MOT方法的局限，为更泛化的跟踪系统奠定了基础。

Abstract: As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.

</details>


### [122] [MacVQA: Adaptive Memory Allocation and Global Noise Filtering for Continual Visual Question Answering](https://arxiv.org/abs/2601.01926)
*Zhifei Li,Yiran Wang,Chenyi Xiong,Yujing Xia,Xiaoju Hou,Yue Zhao,Miao Zhang,Kui Xiao,Bing Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉问答（VQA）模型MacVQA，在逐步学习多个任务时表现更优，能更好地平衡知识保持和新知识吸收。


<details>
  <summary>Details</summary>
Motivation: 现有VQA持续学习方法难以同时做到有效的知识保持、学习新知识以及生成鲁棒的特征表示，导致在多任务连续学习中表现受限。

Method: 提出MacVQA框架，主要包括两项创新：1. 适应性记忆分配，通过原型记忆方法高效管理和优化特征与记忆利用；2. 全局噪声过滤机制，有效融合视觉－文本信息的同时过滤无关噪声，增强特征表征鲁棒性。

Result: 在10个持续VQA任务上的实验显示，MacVQA在标准任务下达到43.38%的平均准确率及2.32%的平均遗忘率；在新组合任务下达到42.53%的平均准确率及3.60%的平均遗忘率，均优于现有主流方法。

Conclusion: MacVQA能够在视觉问答的持续学习任务中平衡知识获取、记忆保持与组合泛化能力，提升模型的整体表现。

Abstract: Visual Question Answering (VQA) requires models to reason over multimodal information, combining visual and textual data. With the development of continual learning, significant progress has been made in retaining knowledge and adapting to new information in the VQA domain. However, current methods often struggle with balancing knowledge retention, adaptation, and robust feature representation. To address these challenges, we propose a novel framework with adaptive memory allocation and global noise filtering called MacVQA for visual question answering. MacVQA fuses visual and question information while filtering noise to ensure robust representations, and employs prototype-based memory allocation to optimize feature quality and memory usage. These designs enable MacVQA to balance knowledge acquisition, retention, and compositional generalization in continual VQA learning. Experiments on ten continual VQA tasks show that MacVQA outperforms existing baselines, achieving 43.38% average accuracy and 2.32% average forgetting on standard tasks, and 42.53% average accuracy and 3.60% average forgetting on novel composition tasks.

</details>


### [123] [Face Normal Estimation from Rags to Riches](https://arxiv.org/abs/2601.01950)
*Meng Wang,Wenjing Dai,Jiawan Zhang,Xiaojie Guo*

Main category: cs.CV

TL;DR: 本文提出了一种粗到细的面部法线估计算法，有效减少对大规模配对数据的依赖，提升了训练效率，并取得了领先的估计效果。


<details>
  <summary>Details</summary>
Motivation: 现有面部法线估计算法在训练时高度依赖大量配对数据，数据获取和计算资源成本较高，限制了实际应用。本研究旨在缓解这一依赖，实现高效且高质量的法线估计。

Method: 方法分为两步：（1）首先用一个小数据集训练模型，生成粗略的面部法线作为指导（称为样例）；（2）再利用自注意力机制消除粗糙预测中的局部伪影，并通过精细化网络将原始人脸和对应样例映射到高质量的精细面部法线，实现逻辑功能分离显著降低了配对数据和算力需求。

Result: 通过大量实验和消融分析，所提出方法在训练消耗和法线估计质量上都优于当前主流方法，显示出优越性。

Conclusion: 该方法实现了用更少的数据和较低的资源获得高质量面部法线估计，具有良好的实际应用前景，并在多个测试中优于其他最新方法，相关代码和模型已开源。

Abstract: Although recent approaches to face normal estimation have achieved promising results, their effectiveness heavily depends on large-scale paired data for training. This paper concentrates on relieving this requirement via developing a coarse-to-fine normal estimator. Concretely, our method first trains a neat model from a small dataset to produce coarse face normals that perform as guidance (called exemplars) for the following refinement. A self-attention mechanism is employed to capture long-range dependencies, thus remedying severe local artifacts left in estimated coarse facial normals. Then, a refinement network is customized for the sake of mapping input face images together with corresponding exemplars to fine-grained high-quality facial normals. Such a logical function split can significantly cut the requirement of massive paired data and computational resource. Extensive experiments and ablation studies are conducted to demonstrate the efficacy of our design and reveal its superiority over state-of-the-art methods in terms of both training expense as well as estimation quality. Our code and models are open-sourced at: https://github.com/AutoHDR/FNR2R.git.

</details>


### [124] [MotionAdapter: Video Motion Transfer via Content-Aware Attention Customization](https://arxiv.org/abs/2601.01955)
*Zhexin Zhang,Yifeng Zhu,Yangyang Xu,Long Chen,Yong Du,Shengfeng He,Jun Yu*

Main category: cs.CV

TL;DR: 本文提出了MotionAdapter，一种能够在扩散模型基础上的文本到视频（T2V）中实现内容感知运动迁移的新框架。该方法通过解析跨帧注意力来剥离运动信息，并利用DINO引导自适应地将参考视频的运动融合到目标视频，提升了复杂运动的迁移效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的文本到视频模型虽然生成质量高，但在高度复杂的运动迁移任务上效果有限，因此迫切需要新方法实现更鲁棒、更语义一致的运动迁移。

Method: MotionAdapter包含两个核心环节：（1）通过分析3D全注意力模块中的跨帧注意力分离出运动信息（运动场）；（2）引入DINO指导的运动定制模块，根据源目标内容对应关系自适应地调整和重组运动场，并用经过定制的运动场引导视频扩散模型的去噪过程，实现运动和外观的有效解耦与重建。

Result: 实验表明，MotionAdapter在定性和定量评价中均超越现有方法，支持如镜头推拉等复杂运动迁移与运动编辑任务，生成视频质量更高、运动与内容结合更紧密。

Conclusion: MotionAdapter有效提高了基于扩散模型的文本到视频系统在复杂运动迁移与编辑方面的表现，实现了语义对齐和外观保持，推动了生成模型在视频运动编辑领域的进展。

Abstract: Recent advances in diffusion-based text-to-video models, particularly those built on the diffusion transformer architecture, have achieved remarkable progress in generating high-quality and temporally coherent videos. However, transferring complex motions between videos remains challenging. In this work, we present MotionAdapter, a content-aware motion transfer framework that enables robust and semantically aligned motion transfer within DiT-based T2V models. Our key insight is that effective motion transfer requires \romannumeral1) explicit disentanglement of motion from appearance and \romannumeral 2) adaptive customization of motion to target content. MotionAdapter first isolates motion by analyzing cross-frame attention within 3D full-attention modules to extract attention-derived motion fields. To bridge the semantic gap between reference and target videos, we further introduce a DINO-guided motion customization module that rearranges and refines motion fields based on content correspondences. The customized motion field is then used to guide the DiT denoising process, ensuring that the synthesized video inherits the reference motion while preserving target appearance and semantics. Extensive experiments demonstrate that MotionAdapter outperforms state-of-the-art methods in both qualitative and quantitative evaluations. Moreover, MotionAdapter naturally supports complex motion transfer and motion editing tasks such as zooming.

</details>


### [125] [AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing](https://arxiv.org/abs/2601.01957)
*Tianbo Wang,Yuqing Ma,Kewei Liao,Zhange Zhang,Simin Li,Jinyang Guo,Xianglong Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新方法AFTER，通过结合事实引导修正激活状态，显著减少了大模型在多模态任务中的幻觉现象。实验显示该方法有效性优于现有手段。


<details>
  <summary>Details</summary>
Motivation: LVLMs在跨模态任务上表现出色，但由于语言偏见，常出现实体类别、属性或关系的幻觉现象，严重影响AI的可靠性。以往修正方法成本低但缺乏事实语义引导，因此难以根本消除偏见。

Method: 提出AFTER框架，包括Factual-Augmented Activation Steering（FAS）和Query-Adaptive Offset Optimization（QAO）：FAS为激活编辑提供事实和泛化指导，显式建模视觉-文本关联；QAO针对具体查询自适应调整编辑偏移量，提升编辑多样性和精细度。

Result: 在三个主流LVLMs和标准幻觉基准（如AMBER）上验证，AFTER方法较基线在AMBER数据集上幻觉率下降至多16.3%。

Conclusion: AFTER方法能自适应引导多模态大模型生成更符合事实的输出，大幅缓解因语言偏见带来的幻觉问题，并具有良好泛化性和实用性。

Abstract: Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guidance offered by factual textual semantics, thereby struggling to explicitly mitigate language bias. To address these issues, we propose Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation (AFTER), which comprises Factual-Augmented Activation Steering (FAS) and Query-Adaptive Offset Optimization (QAO), to adaptively guides the original biased activations towards factual semantics. Specifically, FAS is proposed to provide factual and general guidance for activation editing, thereby explicitly modeling the precise visual-textual associations. Subsequently, QAO introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing the diversity and granularity of editing. Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate the efficacy of the proposed AFTER, notably achieving up to a 16.3% reduction of hallucination over baseline on the AMBER benchmark. Our code and data will be released for reproducibility.

</details>


### [126] [Forget Less by Learning Together through Concept Consolidation](https://arxiv.org/abs/2601.01963)
*Arjun Ramesh Kaushik,Naresh Kumar Devulapally,Vishnu Suresh Lokhande,Nalini Ratha,Venu Govindaraju*

Main category: cs.CV

TL;DR: 提出了一种新框架FL2T，有效解决了个性化扩散模型在持续学习新概念时的灾难性遗忘问题，实现更好的概念保留和知识转移。


<details>
  <summary>Details</summary>
Motivation: 现有个性化扩散模型在持续、顺序学习多个新概念时易出现灾难性遗忘，且大多忽略了不同概念间的交互影响。因此需要一种既能同时学习多个概念、又能减缓遗忘的新方法。

Method: 提出了Forget Less by Learning Together (FL2T)框架，在学习多个概念时采用集合不变的互概念学习模块，通过代理机制在不同概念间引导特征选择，实现知识的互相转移和更好的保留。方法无需固定的学习顺序，可无序并行地增强模型学习能力。

Result: 在三个数据集上进行大量实验，FL2T方法在十个任务的增量概念学习中平均提升了至少2%的CLIP图像对齐得分，显著改善了概念保留和降低了灾难性遗忘。

Conclusion: FL2T框架在增量概念学习中表现优异，证明了互概念催化引导行为能够有效提升生成模型对旧概念的记忆和新概念的适应性。

Abstract: Custom Diffusion Models (CDMs) have gained significant attention due to their remarkable ability to personalize generative processes. However, existing CDMs suffer from catastrophic forgetting when continuously learning new concepts. Most prior works attempt to mitigate this issue under the sequential learning setting with a fixed order of concept inflow and neglect inter-concept interactions. In this paper, we propose a novel framework - Forget Less by Learning Together (FL2T) - that enables concurrent and order-agnostic concept learning while addressing catastrophic forgetting. Specifically, we introduce a set-invariant inter-concept learning module where proxies guide feature selection across concepts, facilitating improved knowledge retention and transfer. By leveraging inter-concept guidance, our approach preserves old concepts while efficiently incorporating new ones. Extensive experiments, across three datasets, demonstrates that our method significantly improves concept retention and mitigates catastrophic forgetting, highlighting the effectiveness of inter-concept catalytic behavior in incremental concept learning of ten tasks with at least 2% gain on average CLIP Image Alignment scores.

</details>


### [127] [Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation](https://arxiv.org/abs/2601.01984)
*Weijian Ma,Shizhao Sun,Tianyu Yu,Ruiyu Wang,Tat-Seng Chua,Jiang Bian*

Main category: cs.CV

TL;DR: 该论文提出了一种结合物体中心蓝图的视觉-语言模型（VLM），显著提升了空间推理能力，并在各项空间推理任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有空间推理方法存在局限：局部图像分析提升细粒度感知但损失全局空间关系，而简单坐标标注虽然定位物体但无法表示整体组织结构。因此需要更有效地建模图像中的空间关系以提升空间语义理解。

Method: 提出将认知心理学中的“物体中心蓝图”理念引入VLM：模型针对输入图像和问题，先生成记录目标物体位置、大小和属性的 JSON 式蓝图，再基于该结构进行推理和作答。包含三大核心技术：1）蓝图嵌入式推理轨迹，用于监督微调以训练基础推理能力；2）蓝图感知强化学习奖励，促使合理选择物体并确保答案与推理链条一致；3）反捷径数据增强，通过干扰图片和问题，防止模型依赖表层线索。

Result: 实验证明，本方法在多种空间推理基准任务上，均明显优于现有VLMs及空间推理专用模型。

Conclusion: 引入物体中心蓝图结构和相关推理机制后，模型空间推理水平显著提升，为视觉-语言模型向更深入的空间语义理解发展提供了有效路径。

Abstract: Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.

</details>


### [128] [API: Empowering Generalizable Real-World Image Dehazing via Adaptive Patch Importance Learning](https://arxiv.org/abs/2601.01992)
*Chen Zhu,Huiwen Zhang,Yujie Li,Mu He,Xiaotian Qiao*

Main category: cs.CV

TL;DR: 本文提出了一种具有自适应补丁重要性感知能力的去雾框架，实现了真实复杂场景下图像去雾的性能突破。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法在真实复杂场景下去雾能力受限，主要原因是训练数据不足和雾密度分布复杂。作者旨在提升去雾模型在不同实际场景下的泛化能力和表现。

Method: 提出了自适应补丁重要性感知（API）框架，包括自动雾气生成（AHG）模块和密度感知去雾（DHR）模块。AHG通过生成多样的真实雾气图像实现数据增强，DHR以自适应补丁方式处理不同雾密度区域。此外，提出了多负对比去雾损失（MNCD），利用空间和频率域多负样本信息，提升去雾细节。

Result: 在多个真实世界基准数据集上，所提方法在定量指标和视觉质量上均达到/超过了目前最先进水平，表现出优异的泛化能力。

Conclusion: API框架有效改善了真实复杂场景下的去雾能力，具有强泛化能力和实际应用前景。

Abstract: Real-world image dehazing is a fundamental yet challenging task in low-level vision. Existing learning-based methods often suffer from significant performance degradation when applied to complex real-world hazy scenes, primarily due to limited training data and the intrinsic complexity of haze density distributions.To address these challenges, we introduce a novel Adaptive Patch Importance-aware (API) framework for generalizable real-world image dehazing. Specifically, our framework consists of an Automatic Haze Generation (AHG) module and a Density-aware Haze Removal (DHR) module. AHG provides a hybrid data augmentation strategy by generating realistic and diverse hazy images as additional high-quality training data. DHR considers hazy regions with varying haze density distributions for generalizable real-world image dehazing in an adaptive patch importance-aware manner. To alleviate the ambiguity of the dehazed image details, we further introduce a new Multi-Negative Contrastive Dehazing (MNCD) loss, which fully utilizes information from multiple negative samples across both spatial and frequency domains. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across multiple real-world benchmarks, delivering strong results in both quantitative metrics and qualitative visual quality, and exhibiting robust generalization across diverse haze distributions.

</details>


### [129] [Nighttime Hazy Image Enhancement via Progressively and Mutually Reinforcing Night-Haze Priors](https://arxiv.org/abs/2601.01998)
*Chen Zhu,Huiwen Zhang,Mu He,Yujie Li,Xiaotian Qiao*

Main category: cs.CV

TL;DR: 本文提出了一种新的框架，通过联合利用低照度和雾霾先验、在多层次和多域间提升夜间有雾图像的可见性。


<details>
  <summary>Details</summary>
Motivation: 夜间有雾图像因多重退化现象导致图像能见度极低，现有方法往往只处理单一类型的退化（如雾或低照度），未能兼顾两者相互影响，提升效果有限。

Method: 作者设计了一种新颖的互补式框架，同时强化雾霾和低照度先验的一致性。模型引入图像级、块级和像素级专家，在视觉和频域中逐层恢复场景结构、区域和细节；并通过频率感知路由器自适应分配各专家的作用，提升复原效果。

Result: 在多项夜间去雾公开测试集上，该方法在定量和定性评测中均优于现有方法。此外，也证明了其在白天去雾和低照度增强任务上的泛化能力。

Conclusion: 本文方法有效结合并强化低照度与雾霾先验，多级协同和频域自适应处理显著提升了夜间有雾图像的视觉效果，具备广泛的应用前景。

Abstract: Enhancing the visibility of nighttime hazy images is challenging due to the complex degradation distributions. Existing methods mainly address a single type of degradation (e.g., haze or low-light) at a time, ignoring the interplay of different degradation types and resulting in limited visibility improvement. We observe that the domain knowledge shared between low-light and haze priors can be reinforced mutually for better visibility. Based on this key insight, in this paper, we propose a novel framework that enhances visibility in nighttime hazy images by reinforcing the intrinsic consistency between haze and low-light priors mutually and progressively. In particular, our model utilizes image-, patch-, and pixel-level experts that operate across visual and frequency domains to recover global scene structure, regional patterns, and fine-grained details progressively. A frequency-aware router is further introduced to adaptively guide the contribution of each expert, ensuring robust image restoration. Extensive experiments demonstrate the superior performance of our model on nighttime dehazing benchmarks both quantitatively and qualitatively. Moreover, we showcase the generalizability of our model in daytime dehazing and low-light enhancement tasks.

</details>


### [130] [Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach](https://arxiv.org/abs/2601.02016)
*Matthias Bartolo,Dylan Seychell,Gabriel Hili,Matthew Montebello,Carl James Debono,Saviour Formosa,Konstantinos Makantasis*

Main category: cs.CV

TL;DR: 本文将带有特权信息的学习范式（LUPI）引入目标检测，提出通用的师生模型注入特权信息方法，在多个主流数据集和模型中验证，显著提升检测精度且不增加推理复杂度。


<details>
  <summary>Details</summary>
Motivation: 目标检测训练期间能获得丰富的语义信息（如分割掩码、显著性图和深度线索），但这些信息在实际推理时不可用。因此希望利用这些“特权信息”提升检测性能且不牺牲速度或模型大小。

Method: 提出通用模型无关的方法，通过教师-学生网络架构在训练时注入特权信息（如掩码、显著性图和深度信息）至目标检测模型，并在五种最新目标检测器与多个公开数据集（如UAV垃圾检测、Pascal VOC 2012）上实验验证。

Result: 通过LUPI训练的学生网络在不增加推理复杂度和模型参数的前提下，检测精度显著高于基础模型。尤其在中大型目标检测上提升明显。消融实验表明，对师网络指导信号的适中加权可获得最优效果。

Conclusion: LUPI框架是一种有效实用的提升目标检测性能的方法，适合资源受限或现实应用场景，无需增加推理时的复杂性。

Abstract: This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.

</details>


### [131] [Towards Any-Quality Image Segmentation via Generative and Adaptive Latent Space Enhancement](https://arxiv.org/abs/2601.02018)
*Guangqian Guo,Aixi Ren,Yong Guo,Xuehui Yu,Jiacheng Tian,Wenli Li,Yaoxing Wang,Shan Gao*

Main category: cs.CV

TL;DR: GleSAM++提升SAM模型对低质量图像的分割鲁棒性，通过生成式潜空间增强和自适应机制显著改善复杂退化下的表现，且对新类型退化亦有优异泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有SAM模型在低质量或严重退化图像上表现明显下降，制约了其实际应用价值。亟需方法提升在各类图像质量下，尤其是退化图像上的鲁棒性和泛化能力。

Method: 提出GleSAM++方法：1）用生成潜空间增强提升对低质量图像的处理能力；2）引入特征分布对齐（FDA）和通道复制扩展（CRE）提高扩散模型与分割框架的兼容性；3）提出退化感知自适应增强机制（DAE），将处理过程分为退化水平预测和退化感知重建两步，降低学习难度。方法仅需极少参数即可应用于预训练SAM及其变体。

Result: 综合实验显示，GleSAM++在多种复杂退化下显著提升分割鲁棒性，在清晰图像上保持泛化能力，在未知退化情形下亦表现优越。

Conclusion: GleSAM++兼顾高效性和通用性，极大增强了SAM类模型对低质和未知退化图像的分割效果，有望推动其实际应用场景落地。

Abstract: Segment Anything Models (SAMs), known for their exceptional zero-shot segmentation performance, have garnered significant attention in the research community. Nevertheless, their performance drops significantly on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM++, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Additionally, to improve compatibility between the pre-trained diffusion model and the segmentation framework, we introduce two techniques, i.e., Feature Distribution Alignment (FDA) and Channel Replication and Expansion (CRE). However, the above components lack explicit guidance regarding the degree of degradation. The model is forced to implicitly fit a complex noise distribution that spans conditions from mild noise to severe artifacts, which substantially increases the learning burden and leads to suboptimal reconstructions. To address this issue, we further introduce a Degradation-aware Adaptive Enhancement (DAE) mechanism. The key principle of DAE is to decouple the reconstruction process for arbitrary-quality features into two stages: degradation-level prediction and degradation-aware reconstruction. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. Extensive experiments demonstrate that GleSAM++ significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM++ also performs well on unseen degradations, underscoring the versatility of our approach and dataset.

</details>


### [132] [Adapting Depth Anything to Adverse Imaging Conditions with Events](https://arxiv.org/abs/2601.02020)
*Shihan Peng,Yuyang Xiong,Hanyu Zhou,Zhiwei Shi,Haoyue Liu,Gang Chen,Luxin Yan,Yi Chang*

Main category: cs.CV

TL;DR: 作者提出了ADAE，一个通过事件相机引导的时空融合框架，有效提升了基础深度模型（如Depth Anything）在恶劣光照和动态模糊下的深度估计表现。


<details>
  <summary>Details</summary>
Motivation: 现有深度基础模型虽在理想场景下效果良好，但在极端光照和动态模糊等恶劣成像条件下表现不佳。为应对这些对视觉信号的破坏，通常采用事件相机与帧相机融合，但现有融合方法多基于专有数据集训练，难以继承基础模型在开放世界下的泛化能力。

Method: 作者提出ADAE，将事件相机生成的数据与帧相机数据进行自适应时空融合。具体包括：1）基于信息熵策略的自适应空间融合，用于检测并修正光照退化影响；2）基于运动线索的时序校正，用于补偿动态模糊区域的特征。

Result: 大量实验表明，ADAE能在光照极端和模糊场景下显著提升Depth Anything等基础模型的深度估计效果，优于现有方法。

Conclusion: ADAE框架有效融合了事件与帧信号，在不失去基础模型开放世界知识的前提下，大幅提升了在复杂环境下的鲁棒性，对机器人视觉系统有重要价值。

Abstract: Robust depth estimation under dynamic and adverse lighting conditions is essential for robotic systems. Currently, depth foundation models, such as Depth Anything, achieve great success in ideal scenes but remain challenging under adverse imaging conditions such as extreme illumination and motion blur. These degradations corrupt the visual signals of frame cameras, weakening the discriminative features of frame-based depths across the spatial and temporal dimensions. Typically, existing approaches incorporate event cameras to leverage their high dynamic range and temporal resolution, aiming to compensate for corrupted frame features. However, such specialized fusion models are predominantly trained from scratch on domain-specific datasets, thereby failing to inherit the open-world knowledge and robust generalization inherent to foundation models. In this work, we propose ADAE, an event-guided spatiotemporal fusion framework for Depth Anything in degraded scenes. Our design is guided by two key insights: 1) Entropy-Aware Spatial Fusion. We adaptively merge frame-based and event-based features using an information entropy strategy to indicate illumination-induced degradation. 2) Motion-Guided Temporal Correction. We resort to the event-based motion cue to recalibrate ambiguous features in blurred regions. Under our unified framework, the two components are complementary to each other and jointly enhance Depth Anything under adverse imaging conditions. Extensive experiments have been performed to verify the superiority of the proposed method. Our code will be released upon acceptance.

</details>


### [133] [Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding](https://arxiv.org/abs/2601.02029)
*Toshihiko Nishimura,Hirofumi Abe,Kazuhiko Murasaki,Taiga Yoshida,Ryuichi Tanida*

Main category: cs.CV

TL;DR: 本文提出了一种无需3D标注训练数据或配对RGB图像的大规模点云三维语义分割新方法，通过将点云投影到2D图像，用基础2D模型和自然语言提示进行分割，再由多视角加权投票实现三维分割，无监督性能优于同类方法，并接近有监督方法。


<details>
  <summary>Details</summary>
Motivation: 传统三维点云语义分割高度依赖标注数据和配对图像，难以扩展且费用高昂。本研究旨在克服数据标注难题，降低分割门槛，并实现开放词汇的灵活识别，提升实际应用的可扩展性。

Method: 该方法将3D点云通过虚拟相机投影到多个2D视角，再使用自然语言引导的预训练2D基础模型（如CLIP）进行2D语义分割。最终聚合来自多视角的预测结果，通过加权投票还原为三维分割，无需对3D点云进行有监督训练。

Result: 所提方法在无需训练的场景下，其分割效果优于现有训练自由（training-free）方法，且与有监督方法精度相当。同时，具备开放词汇能力，支持用户自定义文本检测任意目标。

Conclusion: 该方法不仅解决了三维点云分割对标注数据的依赖，还突破了传统方法词汇受限的局限，提升了实用性和灵活性，为大规模三维场景语义理解带来新思路。

Abstract: This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.

</details>


### [134] [AlignVTOFF: Texture-Spatial Feature Alignment for High-Fidelity Virtual Try-Off](https://arxiv.org/abs/2601.02038)
*Yihan Zhu,Mengying Ge*

Main category: cs.CV

TL;DR: 本文提出了AlignVTOFF模型，在服装虚拟平铺换装任务中，实现了更好的结构还原和细节保留，性能超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有服装虚拟换装方法在复杂几何变形和高频纹理细节保留方面表现不足，导致纹理细节在生成过程中丢失。

Method: 提出了AlignVTOFF模型，基于并行U-Net架构，包括多尺度特征提取的Reference U-Net和一个融合参考服装特征的纹理-空间特征对齐模块（TSFA）。TSFA利用可训练的交叉注意力和冻结的自注意力模块，将参考服装的纹理和空间线索有效注入去噪U-Net，从而对齐结构与纹理特征。

Result: 在多组实验中，AlignVTOFF在结构真实性和高频细节保真度方面明显优于主流方法。

Conclusion: AlignVTOFF模型能更好地处理复杂服装变形与细节，提升虚拟换装生成图像的质量。

Abstract: Virtual Try-Off (VTOFF) is a challenging multimodal image generation task that aims to synthesize high-fidelity flat-lay garments under complex geometric deformation and rich high-frequency textures. Existing methods often rely on lightweight modules for fast feature extraction, which struggles to preserve structured patterns and fine-grained details, leading to texture attenuation during generation.To address these issues, we propose AlignVTOFF, a novel parallel U-Net framework built upon a Reference U-Net and Texture-Spatial Feature Alignment (TSFA). The Reference U-Net performs multi-scale feature extraction and enhances geometric fidelity, enabling robust modeling of deformation while retaining complex structured patterns. TSFA then injects the reference garment features into a frozen denoising U-Net via a hybrid attention design, consisting of a trainable cross-attention module and a frozen self-attention module. This design explicitly aligns texture and spatial cues and alleviates the loss of high-frequency information during the denoising process.Extensive experiments across multiple settings demonstrate that AlignVTOFF consistently outperforms state-of-the-art methods, producing flat-lay garment results with improved structural realism and high-frequency detail fidelity.

</details>


### [135] [Agentic Retoucher for Text-To-Image Generation](https://arxiv.org/abs/2601.02046)
*Shaocheng Shen,Jianfeng Liang. Chunlei Cai,Cong Geng,Huiyu Duan,Xiaoyun Zhang,Qiang Hu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的T2I（文本到图像）细粒度修复方法Agentic Retoucher，能更精确、自动化地纠正AI生成图像中的小规模失真，并提供了新数据集和定量评估方法。


<details>
  <summary>Details</summary>
Motivation: 尽管T2I扩散模型（如SDXL和FLUX）已经实现了很好的照片级真实感，但在细节（如四肢、面部、文字等）上仍然存在小范围失真。现有修复方法要么计算消耗大、需反复迭代重生成，要么依赖于空间定位能力较差的多模态模型，导致语义漂移或本地修复不可靠。急需一种高效且空间定位更精准的自动化端到端修复框架。

Method: 作者提出了Agentic Retoucher，一种层次化、决策驱动的修复框架，将后生成修复建模为类似人类的感知—推理—行动闭环。方法包括：（1）感知代理：结合文本—图像一致性信号，学习上下文显著性，定位失真；（2）推理代理：模拟人类推理，迭代校准推断出应修复区域；（3）行动代理：依据用户偏好自适应规划本地化修复。同时，自建了GenBlemish-27K数据集，涵盖6K张图像和27K个失真注释。

Result: 实验表明，Agentic Retoucher在感知质量、失真定位精度以及人类偏好对齐方面均优于最先进方法。结合新数据集，实现了更公平可靠的细粒度定量评测。

Conclusion: Agentic Retoucher为T2I生成图像的自纠错、细粒度、感知可靠性修复设立了新范式，未来可促进T2I技术在更高质量与更可控场景下的广泛应用。

Abstract: Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.

</details>


### [136] [PhysSFI-Net: Physics-informed Geometric Learning of Skeletal and Facial Interactions for Orthognathic Surgical Outcome Prediction](https://arxiv.org/abs/2601.02088)
*Jiahao Bao,Huazhen Liu,Yu Zhuang,Leran Tao,Xinyu Xu,Yongtao Shi,Mengjia Cheng,Yiming Wang,Congshuang Ku,Ting Zeng,Yilang Du,Siyi Chen,Shunyao Shen,Suncheng Xiang,Hongbo Yu*

Main category: cs.CV

TL;DR: 本论文提出了一个名为PhysSFI-Net的物理信息几何深度学习框架，用于精确预测正颌手术后的软组织变形，优于现有方法，提升术前规划的可靠性。


<details>
  <summary>Details</summary>
Motivation: 正颌手术需要精准预测术后面部形态，以优化手术规划。但现有生物力学模型运算复杂，几何深度学习方法缺乏可解释性，因此需要兼具高精度和可解释性的预测方法。

Method: PhysSFI-Net结合了层次化图结构模块（通过注意力机制提取骨骼-面部交互特征）、基于LSTM的软组织变形序列预测，以及基于生物力学的高分辨率面部表面重建。使用135例患者数据进行训练与验证，并采用多个误差指标进行评估。

Result: PhysSFI-Net在点云形状误差、表面偏差误差、面部标志定位误差三项指标上均取得低误差，且显著超过现有ACMT-Net方法。

Conclusion: PhysSFI-Net能够以高解释性和高分辨率、优异的准确率预测正颌术后面部形态，有望应用于临床手术规划与模拟。

Abstract: Orthognathic surgery repositions jaw bones to restore occlusion and enhance facial aesthetics. Accurate simulation of postoperative facial morphology is essential for preoperative planning. However, traditional biomechanical models are computationally expensive, while geometric deep learning approaches often lack interpretability. In this study, we develop and validate a physics-informed geometric deep learning framework named PhysSFI-Net for precise prediction of soft tissue deformation following orthognathic surgery. PhysSFI-Net consists of three components: a hierarchical graph module with craniofacial and surgical plan encoders combined with attention mechanisms to extract skeletal-facial interaction features; a Long Short-Term Memory (LSTM)-based sequential predictor for incremental soft tissue deformation; and a biomechanics-inspired module for high-resolution facial surface reconstruction. Model performance was assessed using point cloud shape error (Hausdorff distance), surface deviation error, and landmark localization error (Euclidean distances of craniomaxillofacial landmarks) between predicted facial shapes and corresponding ground truths. A total of 135 patients who underwent combined orthodontic and orthognathic treatment were included for model training and validation. Quantitative analysis demonstrated that PhysSFI-Net achieved a point cloud shape error of 1.070 +/- 0.088 mm, a surface deviation error of 1.296 +/- 0.349 mm, and a landmark localization error of 2.445 +/- 1.326 mm. Comparative experiments indicated that PhysSFI-Net outperformed the state-of-the-art method ACMT-Net in prediction accuracy. In conclusion, PhysSFI-Net enables interpretable, high-resolution prediction of postoperative facial morphology with superior accuracy, showing strong potential for clinical application in orthognathic surgical planning and simulation.

</details>


### [137] [MCD-Net: A Lightweight Deep Learning Baseline for Optical-Only Moraine Segmentation](https://arxiv.org/abs/2601.02091)
*Zhehuan Cao,Fiseha Berhanu Tesema,Ping Fu,Jianfeng Ren,Ahmed Nasr*

Main category: cs.CV

TL;DR: 本文提出了一个专注于冰碛地段分割的大规模光学数据集，并开发了高效轻量化的分割模型，为冰川景观变化监测提供了新的公开基准和工具。


<details>
  <summary>Details</summary>
Motivation: 冰川地貌分割对于重建过去冰川动态和分析气候变化影响至关重要。然而，受限于影像对比度低和高分辨率DEM数据获取困难，自动化冰碛体分割面临挑战。本文旨在提升冰碛体在光学遥感影像下的自动识别效率。

Method: 作者收集并人工标注了中国四川、云南冰川区3340幅高分辨率Google Earth影像，建立了首个大规模光学冰碛地段分割公开数据集。提出MCD-Net模型，结合MobileNetV2编码器、卷积注意力模块CBAM和DeepLabV3+解码器，同时与更深层网络进行性能对比。

Result: MCD-Net在分割精度（mIoU 62.3%，Dice 72.8%）与计算成本上均优于深层网络，对比ResNet152等模型减少了超60%的算力消耗。虽然轮廓精细分割受限于影像分辨率和光谱干扰，但整体验证了纯光学影像下冰碛体分割的可行性。

Conclusion: 该研究首次为冰碛分割任务建立了可复现公开基准，并发布了效率与效果兼备的分割基线及大规模人工标注数据，有力促进了高海拔冰川自动监测和相关遥感算法研究。

Abstract: Glacial segmentation is essential for reconstructing past glacier dynamics and evaluating climate-driven landscape change. However, weak optical contrast and the limited availability of high-resolution DEMs hinder automated mapping. This study introduces the first large-scale optical-only moraine segmentation dataset, comprising 3,340 manually annotated high-resolution images from Google Earth covering glaciated regions of Sichuan and Yunnan, China. We develop MCD-Net, a lightweight baseline that integrates a MobileNetV2 encoder, a Convolutional Block Attention Module (CBAM), and a DeepLabV3+ decoder. Benchmarking against deeper backbones (ResNet152, Xception) shows that MCD-Net achieves 62.3\% mean Intersection over Union (mIoU) and 72.8\% Dice coefficient while reducing computational cost by more than 60\%. Although ridge delineation remains constrained by sub-pixel width and spectral ambiguity, the results demonstrate that optical imagery alone can provide reliable moraine-body segmentation. The dataset and code are publicly available at https://github.com/Lyra-alpha/MCD-Net, establishing a reproducible benchmark for moraine-specific segmentation and offering a deployable baseline for high-altitude glacial monitoring.

</details>


### [138] [InpaintHuman: Reconstructing Occluded Humans with Multi-Scale UV Mapping and Identity-Preserving Diffusion Inpainting](https://arxiv.org/abs/2601.02098)
*Jinlong Fan,Shanshan Zhao,Liang Zheng,Jing Zhang,Yuxiang Yang,Mingming Gong*

Main category: cs.CV

TL;DR: 论文提出了InpaintHuman方法，实现了在单目视频严重遮挡情况下的高质量、完整且可动画的3D人像重建。相比现有方法，其在几何完整性、细节和一致性方面均有优势。


<details>
  <summary>Details</summary>
Motivation: 传统单目视频3D人像重建常因遮挡导致观测不全，导致几何结构损坏和时序不一致。虽然当前的3D高斯渲染提升了真实感，但不能很好地应对被遮挡问题，因此需要新的方法来提升重建质量。

Method: 方法提出两个核心创新：（1）多尺度UV参数化表示及分层由粗到细的特征插值，提高了被遮挡区域的重建能力和细节保持；（2）结合文本倒置和语义引导的身份保持型扩散式修复模块，实现了主观一致且时序连贯的遮挡区域填补。此外，采用像素级监督增强了身份还原能力。

Result: 在PeopleSnapshot、ZJU-MoCap等合成数据和真实OcMotion场景上进行实验，结果显示相较于现有方法，该方法在不同姿态和视角下都有更稳定且更高的重建质量。

Conclusion: InpaintHuman在严重遮挡的单目视频3D重建领域，实现了更完整、更高精度和更一致的人像重建，为人物动画和相关应用提供了有力支持。

Abstract: Reconstructing complete and animatable 3D human avatars from monocular videos remains challenging, particularly under severe occlusions. While 3D Gaussian Splatting has enabled photorealistic human rendering, existing methods struggle with incomplete observations, often producing corrupted geometry and temporal inconsistencies. We present InpaintHuman, a novel method for generating high-fidelity, complete, and animatable avatars from occluded monocular videos. Our approach introduces two key innovations: (i) a multi-scale UV-parameterized representation with hierarchical coarse-to-fine feature interpolation, enabling robust reconstruction of occluded regions while preserving geometric details; and (ii) an identity-preserving diffusion inpainting module that integrates textual inversion with semantic-conditioned guidance for subject-specific, temporally coherent completion. Unlike SDS-based methods, our approach employs direct pixel-level supervision to ensure identity fidelity. Experiments on synthetic benchmarks (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion) demonstrate competitive performance with consistent improvements in reconstruction quality across diverse poses and viewpoints.

</details>


### [139] [360-GeoGS: Geometrically Consistent Feed-Forward 3D Gaussian Splatting Reconstruction for 360 Images](https://arxiv.org/abs/2601.02102)
*Jiaqi Yao,Zhongmiao Yan,Jingyi Xu,Songpengcheng Xia,Yan Xiang,Ling Pei*

Main category: cs.CV

TL;DR: 本文提出了一种用于360度图像的前馈型3D高斯泼溅（3DGS）框架，新方法能够在保持高渲染质量的同时，加强几何一致性，大幅提升三维重建中的表面和点云精度。


<details>
  <summary>Details</summary>
Motivation: 现有多视图立体视觉方法受限于稀疏视角和低纹理区域，神经渲染方法虽然效果好但效率低，3DGS虽然高效，但主流方法更注重视觉效果，几何一致性不足，影响实际空间感知任务中三维重建的可靠性。

Method: 提出一种新的基于3DGS的前馈框架，引入Depth-Normal几何正则项，通过联合深度梯度与法线信息，对高斯基元的旋转、比例和位置进行监督，从而提升重建点云和表面的准确性。

Result: 实验证明，该方法在保持高渲染质量的同时，显著提升了几何一致性，更加适合空间感知任务中的三维重建需求。

Conclusion: 该方法为空间感知相关应用中的三维重建提供了更高效且准确的解决方案，兼具渲染质量与几何一致性，提升了实际应用中的实用性和可靠性。

Abstract: 3D scene reconstruction is fundamental for spatial intelligence applications such as AR, robotics, and digital twins. Traditional multi-view stereo struggles with sparse viewpoints or low-texture regions, while neural rendering approaches, though capable of producing high-quality results, require per-scene optimization and lack real-time efficiency. Explicit 3D Gaussian Splatting (3DGS) enables efficient rendering, but most feed-forward variants focus on visual quality rather than geometric consistency, limiting accurate surface reconstruction and overall reliability in spatial perception tasks. This paper presents a novel feed-forward 3DGS framework for 360 images, capable of generating geometrically consistent Gaussian primitives while maintaining high rendering quality. A Depth-Normal geometric regularization is introduced to couple rendered depth gradients with normal information, supervising Gaussian rotation, scale, and position to improve point cloud and surface accuracy. Experimental results show that the proposed method maintains high rendering quality while significantly improving geometric consistency, providing an effective solution for 3D reconstruction in spatial perception tasks.

</details>


### [140] [HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures](https://arxiv.org/abs/2601.02103)
*Yating Wang,Yuan Sun,Xuan Wang,Ran Yi,Boyao Zhou,Yipengjing Sun,Hongyu Liu,Yinuo Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 本文提出了一种名为HeadLighter的新型三维头部生成框架，实现了实时渲染、高质量图像合成，并能对光照进行可控编辑。


<details>
  <summary>Details</summary>
Motivation: 目前的3D高斯喷溅（Gaussian Splatting）头部生成模型虽然能够实现实时、逼真的合成，但由于外观和照明的深度耦合，导致无法灵活地重新照明。现有的解耦方法依赖于强假设，只适用于复杂光照的弱监督学习，因此功能受限。

Method: HeadLighter采用双分支架构，分别建模不受光照影响的头部属性与物理可解释的渲染要素。通过逐步解耦训练，将头部外观先验引入生成模型，并借助受控光照多视角数据进行监督。此外提出蒸馏策略，生成高质量法线以提升真实感渲染效果。

Result: 实验表明，该方法不仅保持了高质量生成和实时渲染能力，还支持明确的光照和视角编辑。

Conclusion: HeadLighter在三维头部生成领域突破了照明与外观解耦难题，为可控重照明和灵活编辑提供了新方案，将公开代码和数据集促进社区研究。

Abstract: Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.

</details>


### [141] [MagicFight: Personalized Martial Arts Combat Video Generation](https://arxiv.org/abs/2601.02107)
*Jiancheng Huang,Mingfu Yan,Songyan Chen,Yi Huang,Shifeng Chen*

Main category: cs.CV

TL;DR: 本文提出了一个名为MagicFight的新任务和方法，用于个性化武术对战视频生成，解决了现有模型在两人互动中的身份混淆、肢体异常和动作不匹配等难题。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成视频领域多关注于单人场景，特别是在舞蹈生成上，但对于两人互动（尤其是武术对战）的个性化视频生成研究几乎空白。作者发现单人建模在两人对战场景下表现不佳，因此提出针对该领域的系统和方法。

Method: 作者提出MagicFight方法，并使用Unity引擎自制了包含丰富3D角色和武术动作场景的数据集，以弥补数据短缺。同时，MagicFight对现有模型进行了改进，以实现高保真、动作连贯且可区分个体身份的双人对战视频生成。

Result: MagicFight能够生成高质量的二人武术对战视频，有效保持角色身份、肢体结构和动作逻辑的一致性，解决了身份混淆和动作异常等关键问题。

Conclusion: MagicFight为个性化交互视频内容创作领域开拓了新方向，填补了武术对战个性化视频生成的技术空白，并为后续相关研究和应用奠定基础。

Abstract: Amid the surge in generic text-to-video generation, the field of personalized human video generation has witnessed notable advancements, primarily concentrated on single-person scenarios. However, to our knowledge, the domain of two-person interactions, particularly in the context of martial arts combat, remains uncharted. We identify a significant gap: existing models for single-person dancing generation prove insufficient for capturing the subtleties and complexities of two engaged fighters, resulting in challenges such as identity confusion, anomalous limbs, and action mismatches. To address this, we introduce a pioneering new task, Personalized Martial Arts Combat Video Generation. Our approach, MagicFight, is specifically crafted to overcome these hurdles. Given this pioneering task, we face a lack of appropriate datasets. Thus, we generate a bespoke dataset using the game physics engine Unity, meticulously crafting a multitude of 3D characters, martial arts moves, and scenes designed to represent the diversity of combat. MagicFight refines and adapts existing models and strategies to generate high-fidelity two-person combat videos that maintain individual identities and ensure seamless, coherent action sequences, thereby laying the groundwork for future innovations in the realm of interactive video content creation.
  Website: https://MingfuYAN.github.io/MagicFight/
  Dataset: https://huggingface.co/datasets/MingfuYAN/KungFu-Fiesta

</details>


### [142] [Car Drag Coefficient Prediction from 3D Point Clouds Using a Slice-Based Surrogate Model](https://arxiv.org/abs/2601.02112)
*Utkarsh Singh,Absaar Ali,Adarsh Roy*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级代理模型，通过顺序切片处理3D汽车几何结构，可高效预测汽车的气动阻力系数（Cd），兼具高精度、低延迟和良好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的气动性能评估方法（如CFD和风洞实验）耗时费力，无法在早期设计阶段支持快速迭代。现有机器学习代理模型存在计算复杂、可解释性差或输入几何精细度不够的问题，难以兼顾精度与效率。因此需要一种既高效又精确且可解释性强的新方案。

Method: 受医学影像启发，将汽车3D点云几何体沿纵向分解为一系列有序的2D横截面切片。每个切片采用轻量化PointNet2D编码，切片序列输入双向LSTM网络，捕捉沿流向的几何变化特征。模型在DrivAerNet++数据集上进行训练和评估。

Result: 所提模型在Cd预测上表现优异，取得了高决定系数（R^2 > 0.9528）与极低的平均绝对误差（MAE约6.046 x 10^{-3}）。单样本推断仅需约0.025秒，能够实现快速反馈。

Conclusion: 该方法实现了高效、准确且具备解释性的气动反馈机制，可支持更加敏捷且有据可依的汽车设计探索。

Abstract: The automotive industry's pursuit of enhanced fuel economy and performance necessitates efficient aerodynamic design. However, traditional evaluation methods such as computational fluid dynamics (CFD) and wind tunnel testing are resource intensive, hindering rapid iteration in the early design stages. Machine learning-based surrogate models offer a promising alternative, yet many existing approaches suffer from high computational complexity, limited interpretability, or insufficient accuracy for detailed geometric inputs. This paper introduces a novel lightweight surrogate model for the prediction of the aerodynamic drag coefficient (Cd) based on a sequential slice-wise processing of the geometry of the 3D vehicle. Inspired by medical imaging, 3D point clouds of vehicles are decomposed into an ordered sequence of 2D cross-sectional slices along the stream-wise axis. Each slice is encoded by a lightweight PointNet2D module, and the sequence of slice embeddings is processed by a bidirectional LSTM to capture longitudinal geometric evolution. The model, trained and evaluated on the DrivAerNet++ dataset, achieves a high coefficient of determination (R^2 > 0.9528) and a low mean absolute error (MAE approx 6.046 x 10^{-3}) in Cd prediction. With an inference time of approximately 0.025 seconds per sample on a consumer-grade GPU, our approach provides fast, accurate, and interpretable aerodynamic feedback, facilitating more agile and informed automotive design exploration.

</details>


### [143] [Remote Sensing Change Detection via Weak Temporal Supervision](https://arxiv.org/abs/2601.02126)
*Xavier Bou,Elliot Vincent,Gabriele Facciolo,Rafael Grompone von Gioi,Jean-Michel Morel,Thibaud Ehret*

Main category: cs.CV

TL;DR: 本论文提出了一种用于遥感语义变化检测的新方法，通过利用现有的单时相数据集和附加的时间观测数据，实现了无需新增注释的弱监督训练，在多个数据集上验证了其强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 遥感领域的语义变化检测受限于像素级标注数据稀缺，获取高质量标注成本高昂，阻碍了算法发展。

Method: 作者提出弱时序监督方法，利用已有单时相数据集的新时间观测数据，将同一地点多时相作为‘无变化’对，不同地点图像配对生成‘有变化’样本，通过目标感知的变化图生成和迭代细化过程处理标签噪声。

Result: 在FLAIR和IAILD航空数据集的扩展版上，方法在零样本与小样本任务上表现优异，并在法国大范围区域验证了方法的可扩展性。

Conclusion: 该方法无需新增注释即可提升遥感变化检测性能，具备较强的跨域泛化和大规模应用推广潜力。

Abstract: Semantic change detection in remote sensing aims to identify land cover changes between bi-temporal image pairs. Progress in this area has been limited by the scarcity of annotated datasets, as pixel-level annotation is costly and time-consuming. To address this, recent methods leverage synthetic data or generate artificial change pairs, but out-of-domain generalization remains limited. In this work, we introduce a weak temporal supervision strategy that leverages additional temporal observations of existing single-temporal datasets, without requiring any new annotations. Specifically, we extend single-date remote sensing datasets with new observations acquired at different times and train a change detection model by assuming that real bi-temporal pairs mostly contain no change, while pairing images from different locations to generate change examples. To handle the inherent noise in these weak labels, we employ an object-aware change map generation and an iterative refinement process. We validate our approach on extended versions of the FLAIR and IAILD aerial datasets, achieving strong zero-shot and low-data regime performance across different benchmarks. Lastly, we showcase results over large areas in France, highlighting the scalability potential of our method.

</details>


### [144] [Beyond Segmentation: An Oil Spill Change Detection Framework Using Synthetic SAR Imagery](https://arxiv.org/abs/2601.02139)
*Chenyang Lai,Shuaiyu Chen,Tianjin Huang,Siyang Song,Guangliang Cheng,Chunbo Luo,Zeyu Fu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于时序变化检测的新方法（OSCD）来提升海洋溢油的检测准确率，有效减少误报。


<details>
  <summary>Details</summary>
Motivation: 当前基于单张SAR图像的深度学习检测方法，容易将油污和其他海洋特征混淆，导致高误报率，且泛化能力不足。实际应用中，往往缺乏溢油前后的配准影像。

Method: 提出了一种双时相检测任务（OSCD），并设计了TAHI框架，通过高保真修复和时序一致性增强模块，利用事后SAR图像合成事前无油图像，并据此构建了数据集和评测流程。

Result: 采用OSCD和TAHI生成的数据集与方法，多个主流变化检测模型的误报率大幅下降，检测准确率提升，优于传统分割方法。

Conclusion: 引入时序信息和合成前时相影像的方法显著提升了溢油检测在实际复杂场景下的可靠性和可扩展性，适用于实际大规模监测。

Abstract: Marine oil spills are urgent environmental hazards that demand rapid and reliable detection to minimise ecological and economic damage. While Synthetic Aperture Radar (SAR) imagery has become a key tool for large-scale oil spill monitoring, most existing detection methods rely on deep learning-based segmentation applied to single SAR images. These static approaches struggle to distinguish true oil spills from visually similar oceanic features (e.g., biogenic slicks or low-wind zones), leading to high false positive rates and limited generalizability, especially under data-scarce conditions. To overcome these limitations, we introduce Oil Spill Change Detection (OSCD), a new bi-temporal task that focuses on identifying changes between pre- and post-spill SAR images. As real co-registered pre-spill imagery is not always available, we propose the Temporal-Aware Hybrid Inpainting (TAHI) framework, which generates synthetic pre-spill images from post-spill SAR data. TAHI integrates two key components: High-Fidelity Hybrid Inpainting for oil-free reconstruction, and Temporal Realism Enhancement for radiometric and sea-state consistency. Using TAHI, we construct the first OSCD dataset and benchmark several state-of-the-art change detection models. Results show that OSCD significantly reduces false positives and improves detection accuracy compared to conventional segmentation, demonstrating the value of temporally-aware methods for reliable, scalable oil spill monitoring in real-world scenarios.

</details>


### [145] [Efficient Unrolled Networks for Large-Scale 3D Inverse Problems](https://arxiv.org/abs/2601.02141)
*Romain Vo,Julián Tachella*

Main category: cs.CV

TL;DR: 本文提出了一种新的领域划分策略和规范算子近似方法，使得端到端重建模型能够在单一GPU上高效训练和推理大型三维成像任务（如3D锥束CT和多线圈MRI），同时保持最先进的重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习成像逆问题方法在网络结构中集成成像算子取得了最佳性能，但对于大规模3D成像问题，内存消耗极大，无法进行典型的分块训练，限制了方法的应用范围。

Method: 作者提出了一种领域划分策略和规范算子近似算法，使网络能够将大型成像正向算子集成到架构中，从而支持端到端的训练与推理，并显著节约计算资源。

Result: 新方法在3D锥束CT和3D多线圈MRI两个代表性任务上取得了最新最优的结果，能够仅用单块GPU完成训练和推理工作。

Conclusion: 该方法大幅拓展了深度学习成像逆问题的应用范围，即便在计算资源有限的情况下，也能实现大规模三维成像的最先进性能。

Abstract: Deep learning-based methods have revolutionized the field of imaging inverse problems, yielding state-of-the-art performance across various imaging domains. The best performing networks incorporate the imaging operator within the network architecture, typically in the form of deep unrolling. However, in large-scale problems, such as 3D imaging, most existing methods fail to incorporate the operator in the architecture due to the prohibitive amount of memory required by global forward operators, which hinder typical patching strategies. In this work, we present a domain partitioning strategy and normal operator approximations that enable the training of end-to-end reconstruction models incorporating forward operators of arbitrarily large problems into their architecture. The proposed method achieves state-of-the-art performance on 3D X-ray cone-beam tomography and 3D multi-coil accelerated MRI, while requiring only a single GPU for both training and inference.

</details>


### [146] [BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models](https://arxiv.org/abs/2601.02147)
*Sunny Gupta,Shounak Das,Amit Sethi*

Main category: cs.CV

TL;DR: 论文提出BiPrompt框架，用于同时在视觉和文本两个模态消除偏差，实现更稳健的跨模态泛化。该方法在多项偏见基准测试中优于以往的去偏训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言基础模型（如CLIP）虽有良好的零样本泛化能力，但在视觉和文本模态之间仍容易受非因果相关因素干扰，导致模型鲁棒性不足。已有去偏方法多聚焦单一模态，难以适应分布变化。

Method: 提出BiPrompt框架：视觉端利用结构化注意引导擦除法削弱背景激活，使模型在因果与非因果区域的预测结果更正交；文本端则采用平衡提示归一化，一种可学习的中心化机制，使类别嵌入向各向同性的语义空间对齐。两模块协同最小化条件互信息，从而减弱模型对虚假线索的依赖，无需再训练或域监督。

Result: 在真实和合成偏见基准测试中，BiPrompt在平均准确率和最差组准确率上均优于现有测试时去偏方法，表现出更好的稳健性和泛化能力。

Conclusion: BiPrompt实现了无须再训练或附加监督约束下的有效、轻量化跨模态去偏，为可信赖的视觉语言适应提供了新途径。

Abstract: Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.

</details>


### [147] [Why Commodity WiFi Sensors Fail at Multi-Person Gait Identification: A Systematic Analysis Using ESP32](https://arxiv.org/abs/2601.02177)
*Oliver Custance,Saad Khan,Simon Parkinson*

Main category: cs.CV

TL;DR: 本论文系统评估了市售低成本ESP32 WiFi传感器在多人步态识别中的能力。六种信号分离方法在多种场景下表现均不佳，表明该类硬件本身存在根本性限制，难以支撑高质量的多人步态信号分离。


<details>
  <summary>Details</summary>
Motivation: 针对目前WiFi CSI在单人步态识别领域成效显著，但在多人场景下准确率低且原因未明，提出探索：多人步态识别效果不佳是算法限制还是商用硬件（如ESP32）能力有限所致，从而明确今后提升方向。

Method: 选用六种典型信号分离方法（FastICA、SOBI、PCA、NMF、小波变换、张量分解），搭建基于ESP32的廉价现成测量系统，设计7种1-10人的实验场景，通过自创诊断指标（体内变异度、体间可区分性、性能下降速率）系统测试与比较。

Result: 所有分离方法在多人场景下都只有45-56%的准确率（标准差3.74%），且方法间无统计学显著差异（p>0.05）；NMF虽最优但仅56%。分析发现，随着人数增加，体内变异度高、体间难区分，系统性能严重下降。

Conclusion: 结果表明，非定制的ESP32 WiFi商用硬件无法获取足够高质量的CSI数据以支持高效、可靠的多人步态分离，提升识别效果需依赖硬件能力的突破而非算法微调。

Abstract: WiFi Channel State Information (CSI) has shown promise for single-person gait identification, with numerous studies reporting high accuracy. However, multi-person identification remains largely unexplored, with the limited existing work relying on complex, expensive setups requiring modified firmware. A critical question remains unanswered: is poor multi-person performance an algorithmic limitation or a fundamental hardware constraint? We systematically evaluate six diverse signal separation methods (FastICA, SOBI, PCA, NMF, Wavelet, Tensor Decomposition) across seven scenarios with 1-10 people using commodity ESP32 WiFi sensors--a simple, low-cost, off-the-shelf solution. Through novel diagnostic metrics (intra-subject variability, inter-subject distinguishability, performance degradation rate), we reveal that all methods achieve similarly low accuracy (45-56\%, $σ$=3.74\%) with statistically insignificant differences (p $>$ 0.05). Even the best-performing method, NMF, achieves only 56\% accuracy. Our analysis reveals high intra-subject variability, low inter-subject distinguishability, and severe performance degradation as person count increases, indicating that commodity ESP32 sensors cannot provide sufficient signal quality for reliable multi-person separation.

</details>


### [148] [QuIC: A Quantum-Inspired Interaction Classifier for Revitalizing Shallow CNNs in Fine-Grained Recognition](https://arxiv.org/abs/2601.02189)
*Cheng Ying Wu,Yen Jui Chang*

Main category: cs.CV

TL;DR: 本文针对资源受限边缘设备上的细粒度视觉分类（FGVC）展开研究，提出了一种轻量级、易于集成的量子灵感交互分类器（QuIC），显著提升了浅层网络的细粒度识别能力。


<details>
  <summary>Details</summary>
Motivation: 深层网络在FGVC任务上准确率高但计算开销大，不适合边缘设备；浅层网络虽高效但性能不足，特别是无法捕捉高阶特征交互，影响对细粒度类别的区分。

Method: 提出QuIC模块，受量子力学启发，将特征通道视作量子态，通过可学习算符获得二阶特征协方差。该模块可作为轻量模块插入浅层网络，并支持稳定的端到端训练，无需膨胀特征维度。

Result: 实验证明，QuIC能显著提升浅层网络表现，如VGG16 Top-1准确率提升近20%；在ResNet18上亦优于SE-Block等注意力机制。t-SNE可视化展示了QuIC强化了类内紧致性并突出判别性特征。

Conclusion: QuIC有效弥补了浅层网络在FGVC场景下的高阶特征建模不足，不仅大幅提升性能，同时保持模型轻量化，适于边缘设备部署。

Abstract: Deploying deep learning models for Fine-Grained Visual Classification (FGVC) on resource-constrained edge devices remains a significant challenge. While deep architectures achieve high accuracy on benchmarks like CUB-200-2011, their computational cost is often prohibitive. Conversely, shallow networks (e.g., AlexNet, VGG) offer efficiency but fail to distinguish visually similar sub-categories. This is because standard Global Average Pooling (GAP) heads capture only first-order statistics, missing the subtle high-order feature interactions required for FGVC. While Bilinear CNNs address this, they suffer from high feature dimensionality and instability during training. To bridge this gap, we propose the Quantum-inspired Interaction Classifier (QuIC). Drawing inspiration from quantum mechanics, QuIC models feature channels as interacting quantum states and captures second-order feature covariance via a learnable observable operator. Designed as a lightweight, plug-and-play module, QuIC supports stable, single-stage end-to-end training without exploding feature dimensions. Experimental results demonstrate that QuIC significantly revitalizes shallow backbones: it boosts the Top-1 accuracy of VGG16 by nearly 20% and outperforms state-of-the-art attention mechanisms (SE-Block) on ResNet18. Qualitative analysis, including t-SNE visualization, further confirms that QuIC resolves ambiguous cases by explicitly attending to fine-grained discriminative features and enforcing compact intra-class clustering.

</details>


### [149] [Mind the Gap: Continuous Magnification Sampling for Pathology Foundation Models](https://arxiv.org/abs/2601.02198)
*Alexander Möllers,Julius Hense,Florian Schulz,Timo Milbich,Maximilian Alber,Lukas Ruff*

Main category: cs.CV

TL;DR: 该论文提出连续放大率采样，解决了在组织病理图像分析中不同放大倍率模型泛化不佳的问题。通过理论分析和实验验证，连续采样优于传统离散采样，尤其在中间放大倍率上表现更好，并且提出了优化的采样分布以进一步提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 病理学常需在不同显微倍数下分析组织结构细节，但现有基础模型在不同放大倍率下性能不稳定，且放大倍率采样策略的影响缺乏系统化理解。该问题直接影响数字病理AI模型泛化和实际应用效果。

Method: 将放大率的采样建模为多源领域适应问题，建立理论框架分析不同采样策略的权衡。创新性地提出连续放大率采样方法，避免放大率分布不均带来的覆盖缺口，并推导优化的采样分布。同时，构建了两个新的基准数据集和评价指标以系统评测。

Result: 实验显示，在中间放大倍率上，连续采样相比离散采样的性能最高提升达4个百分点，优化的采样分布还能进一步提升模型表现。此外，现有主流基础模型的性能波动主要由放大率驱动。

Conclusion: 连续放大率采样与优化采样分布能显著提升病理基础模型在各放大率下的稳健性，为开发广泛适用、泛化性强的病理基础模型奠定基础。

Abstract: In histopathology, pathologists examine both tissue architecture at low magnification and fine-grained morphology at high magnification. Yet, the performance of pathology foundation models across magnifications and the effect of magnification sampling during training remain poorly understood. We model magnification sampling as a multi-source domain adaptation problem and develop a simple theoretical framework that reveals systematic trade-offs between sampling strategies. We show that the widely used discrete uniform sampling of magnifications (0.25, 0.5, 1.0, 2.0 mpp) leads to degradation at intermediate magnifications. We introduce continuous magnification sampling, which removes gaps in magnification coverage while preserving performance at standard scales. Further, we derive sampling distributions that optimize representation quality across magnification scales. To evaluate these strategies, we introduce two new benchmarks (TCGA-MS, BRACS-MS) with appropriate metrics. Our experiments show that continuous sampling substantially improves over discrete sampling at intermediate magnifications, with gains of up to 4 percentage points in balanced classification accuracy, and that optimized distributions can further improve performance. Finally, we evaluate current histopathology foundation models, finding that magnification is a primary driver of performance variation across models. Our work paves the way towards future pathology foundation models that perform reliably across magnifications.

</details>


### [150] [Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules](https://arxiv.org/abs/2601.02203)
*Oliver Custance,Saad Khan,Simon Parkinson,Quan Z. Sheng*

Main category: cs.CV

TL;DR: 本论文提出一种基于WiFi信道状态信息（CSI）的无设备人群计数新方法，旨在解决跨场景泛化的问题，实现高精度且适用于实际IoT部署的人数估算。


<details>
  <summary>Details</summary>
Motivation: 传统的WiFi CSI人群计数方法受环境迁移影响严重，难以从训练环境泛化到新的应用场景，限制了实际大规模部署。本研究旨在解决这一领域迁移难题，实现模型的强鲁棒性和实用性。

Method: 提出了以CSI-ResNet-A为核心的两阶段框架。首先，采用自监督对比学习预训练模型以获得领域无关的表示，然后引入轻量级Adapter模块，实现高效的参数微调。接着，事件序列由带有状态的计数机处理，输出最终稳定的占用人数估算。

Result: 在WiFlow数据集上的无监督10-shot学习场景下，MAE仅为0.44，而已有监督方法失效。提出并使用Generalisation Index（GI）指标，模型泛化能力接近满分，WiAR公开基准测试准确率达98.8%。消融实验表明，Adapter微调性能几乎与全模型微调持平，但参数量减少97.2%。

Conclusion: 本方法能够显著提升跨场景人群计数的泛化能力，兼具高准确率和训练效率，为现实IoT部署中的鲁棒感知系统提供了实用、可扩展的解决方案。

Abstract: Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\% of a full fine-tune (98.84\% vs. 99.67\%) while training 97.2\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.

</details>


### [151] [NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation](https://arxiv.org/abs/2601.02204)
*Huichao Zhang,Liao Qu,Yiheng Liu,Hang Chen,Yangyang Song,Yongsheng Dong,Shikun Sun,Xian Li,Xu Wang,Yi Jiang,Hu Ye,Bo Chen,Yiming Gao,Peng Liu,Akide Liu,Zhipeng Yang,Qili Deng,Linjie Xing,Jiyang Liu,Zhao Wang,Yang Zhou,Mingcong Liu,Yi Zhang,Qian He,Xiwei Hu,Zhongqi Qi,Jie Shao,Zhiye Fu,Shuai Wang,Fangmin Chen,Xuezhi Chai,Zhihua Wu,Yitong Wang,Zehuan Yuan,Daniel K. Du,Xinglong Wu*

Main category: cs.CV

TL;DR: 本文提出了NextFlow，一个基于Transformer的统一自回归模型，实现了文本、图像等多模态的原生理解与生成，速度远超同类模型，且视觉质量达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 当前多模态生成模型在架构和效率上存在局限，文本与图像模态存在本质区别，统一处理难度大。同时，高分辨率图像生成速度慢，质量与灵活性难以兼得。为此，作者欲提供一种统一、快速、高质量多模态生成方案。

Method: 提出统一的自回归Transformer架构，用离散token表示文本和图像。文本采用传统的逐token预测，图像采用创新的'next-scale'逐层级预测，替代逐像素扫描方法，实现高效高分辨率生成。为训练稳定性引入新训练方法，并引入prefix-tuning结合强化学习优化。

Result: NextFlow能够原生支持多种生成任务，如图像编辑、内容交错和视频生成。生成1024x1024高分辨率图像仅需5秒，速度远超传统自回归方法。在统一模型中达到SOTA，在视觉质量上接近甚至媲美专用扩散模型。

Conclusion: NextFlow表明通过统一架构和创新生成机制，可以兼顾多模态的高效性和高质量，推动了多模态生成模型的效率和应用能力，具有广阔的实际应用前景。

Abstract: We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.

</details>


### [152] [Seeing the Unseen: Zooming in the Dark with Event Cameras](https://arxiv.org/abs/2601.02206)
*Dachun Kai,Zeyu Xiao,Huyue Zhu,Jiaxiao Wang,Yueyi Zhang,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 本文提出了一种针对低光照视频超分辨率（LVSR）的新框架RetinexEVSR，首次结合高对比度事件信号与Retinex理论，有效提升低光照下视频细节与质量，在多个数据集上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前LVSR方法在遇到低对比度与缺乏高频信息时，难以恢复清晰细节，尤其是在低光照环境下效果有限。作者旨在通过引入事件信号和Retinex先验突破现有瓶颈。

Method: RetinexEVSR利用高对比度事件数据，并受Retinex模型启发，设计了两个关键模块：一是基于照明引导的事件增强模块，用照明图引导事件特征提炼，有效去除低光伪影，保留细节；二是事件引导的反射率增强模块，利用多尺度融合机制提升反射率细节。还提出了双向跨模态融合策略，使事件和RGB帧特征有效互补。

Result: RetinexEVSR在三个公开数据集上取得了最优性能。在SDSD数据集上，相较于已有事件驱动方法，在性能上提升了2.95 dB，推理速度提升65%。

Conclusion: 作者证明了融合事件信号与Retinex先验可大幅提升低光照视频超分辨率效果及效率。RetinexEVSR为LVSR任务提供了有效新思路，有望推动实际低光照视频的观测与应用。

Abstract: This paper addresses low-light video super-resolution (LVSR), aiming to restore high-resolution videos from low-light, low-resolution (LR) inputs. Existing LVSR methods often struggle to recover fine details due to limited contrast and insufficient high-frequency information. To overcome these challenges, we present RetinexEVSR, the first event-driven LVSR framework that leverages high-contrast event signals and Retinex-inspired priors to enhance video quality under low-light scenarios. Unlike previous approaches that directly fuse degraded signals, RetinexEVSR introduces a novel bidirectional cross-modal fusion strategy to extract and integrate meaningful cues from noisy event data and degraded RGB frames. Specifically, an illumination-guided event enhancement module is designed to progressively refine event features using illumination maps derived from the Retinex model, thereby suppressing low-light artifacts while preserving high-contrast details. Furthermore, we propose an event-guided reflectance enhancement module that utilizes the enhanced event features to dynamically recover reflectance details via a multi-scale fusion mechanism. Experimental results show that our RetinexEVSR achieves state-of-the-art performance on three datasets. Notably, on the SDSD benchmark, our method can get up to 2.95 dB gain while reducing runtime by 65% compared to prior event-based methods. Code: https://github.com/DachunKai/RetinexEVSR.

</details>


### [153] [Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion](https://arxiv.org/abs/2601.02211)
*Binglei Li,Mengping Yang,Zhiyu Tan,Junping Zhang,Hao Li*

Main category: cs.CV

TL;DR: 本文系统分析了MMDiT类扩散模型各模块对文本到图像生成过程的影响，并提出了无需训练的新机制以提升文本一致性、图像编辑精度和生成加速。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer为扩散式文本到图像生成带来突破，但对MMDiT模型内部不同模块及其与文本条件交互的具体作用机制理解不足。现有分析主要针对单一结构，缺乏全局洞察。

Method: 作者搭建了一套系统性分析管道，通过依次移除、禁用或增强模型各层的文本隐藏状态，全面研究各模块功能。同时，提出基于该分析得出的无需训练的新策略以提升模型表现。

Result: 实验证明，语义信息在早期层出现，细节在后期层完善；去除单一模块影响较小但禁用文本条件影响大；有选择地增强文本信息提升了语义一致性。新方法在多个基准任务上取得领先效果（如T2I-Combench++提升至63.00%，GenEval提升至71.63%），且适用于多种任务。

Conclusion: 这项工作深化了对MMDiT内部机制的理解，提出的新策略丰富了模型性能提升途径，为未来多模态扩散模型的进一步改进提供了理论和实践参考。

Abstract: Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block's functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.

</details>


### [154] [Prior-Guided DETR for Ultrasound Nodule Detection](https://arxiv.org/abs/2601.02212)
*Jingjing Wang,Zhuo Xiao,Xinning Yao,Bo Liu,Lijuan Niu,Xiangzhi Bai,Fugen Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种融合先验知识的DETR检测框架（Prior-guided DETR），显著提升了超声结节检测的准确度，尤其适用于形态复杂、边缘模糊的甲状腺和乳腺结节。


<details>
  <summary>Details</summary>
Motivation: 由于超声结节形状不规则、边界不清晰、尺度变化大及存在散斑噪声，导致传统基于深度学习的数据驱动方法在实际检测中效果受限。因此，结合医学影像经验的先验知识有望提升检测性能。

Method: 方法上，作者设计了三个关键模块：1）在CNN主干中嵌入空间自适应可变形FFN，注入几何先验，稳健提取特征；2）提出多尺度空间-频率特征混合器，联合空间和频域先验，突出轮廓信息、抑制噪声；3）密集特征交互机制保证先验调制特征在各层高效流动，提升解码器查询表达能力。

Result: 在两套临床采集的甲状腺结节数据集和两个公开基准上，本文方法相较于18种主流检测方法取得了更高的检测准确率，尤其优势明显于形态复杂结节。

Conclusion: 引入多阶段先验知识融合的DETR框架能有效适应超声结节检测的实际困难，提升检测准确性和抗噪能力，为医学图像智能诊断提供了新的解决方案。

Abstract: Accurate detection of ultrasound nodules is essential for the early diagnosis and treatment of thyroid and breast cancers. However, this task remains challenging due to irregular nodule shapes, indistinct boundaries, substantial scale variations, and the presence of speckle noise that degrades structural visibility. To address these challenges, we propose a prior-guided DETR framework specifically designed for ultrasound nodule detection. Instead of relying on purely data-driven feature learning, the proposed framework progressively incorporates different prior knowledge at multiple stages of the network. First, a Spatially-adaptive Deformable FFN with Prior Regularization (SDFPR) is embedded into the CNN backbone to inject geometric priors into deformable sampling, stabilizing feature extraction for irregular and blurred nodules. Second, a Multi-scale Spatial-Frequency Feature Mixer (MSFFM) is designed to extract multi-scale structural priors, where spatial-domain processing emphasizes contour continuity and boundary cues, while frequency-domain modeling captures global morphology and suppresses speckle noise. Furthermore, a Dense Feature Interaction (DFI) mechanism propagates and exploits these prior-modulated features across all encoder layers, enabling the decoder to enhance query refinement under consistent geometric and structural guidance. Experiments conducted on two clinically collected thyroid ultrasound datasets (Thyroid I and Thyroid II) and two public benchmarks (TN3K and BUSI) for thyroid and breast nodules demonstrate that the proposed method achieves superior accuracy compared with 18 detection methods, particularly in detecting morphologically complex nodules.The source code is publicly available at https://github.com/wjj1wjj/Ultrasound-DETR.

</details>


### [155] [FMVP: Masked Flow Matching for Adversarial Video Purification](https://arxiv.org/abs/2601.02228)
*Duoxun Tang,Xueyi Zhang,Chak Hin Wang,Xi Xiao,Dasen Dai,Xinhang Jiang,Wentao Shi,Rui Li,Qing Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频去对抗攻击方法FMVP，通过拆解和重建视频，显著提升了对抗鲁棒性，并在主流数据集和多种攻击下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有视频识别模型易受对抗攻击影响，且基于扩散模型的去噪方法存在采样效率低和运动轨迹不理想等问题。此外，现有直接回归方法难以恢复原始内容。需要一个能有效“打碎”对抗结构并重构真实内容的方法。

Method: 提出FMVP方法，通过掩码策略物理性打碎全局对抗结构，并利用条件流匹配（CFM）和补全目标重建清晰视频动态。引入频率门控损失函数（FGL）以抑制高频对抗残差，保留低频真实内容。针对已知和未知攻击设计了攻击感知和通用训练范式。

Result: 在UCF-101和HMDB-51数据集上的大量实验表明，FMVP在PGD和CW攻击下，鲁棒准确率分别超过87%和89%，优于DiffPure、DP、TS、FlowPure等SOTA方法。对自适应攻击和零样本检测亦展现出优异性能，PGD检测准确率达98%，对高度隐蔽CW攻击检测率为79%。

Conclusion: FMVP有效提升了视频识别模型对多种对抗攻击的鲁棒性和检测能力，是当前视频对抗防御领域表现最优的方法之一。

Abstract: Video recognition models remain vulnerable to adversarial attacks, while existing diffusion-based purification methods suffer from inefficient sampling and curved trajectories. Directly regressing clean videos from adversarial inputs often fails to recover faithful content due to the subtle nature of perturbations; this necessitates physically shattering the adversarial structure. Therefore, we propose Flow Matching for Adversarial Video Purification FMVP. FMVP physically shatters global adversarial structures via a masking strategy and reconstructs clean video dynamics using Conditional Flow Matching (CFM) with an inpainting objective. To further decouple semantic content from adversarial noise, we design a Frequency-Gated Loss (FGL) that explicitly suppresses high-frequency adversarial residuals while preserving low-frequency fidelity. We design Attack-Aware and Generalist training paradigms to handle known and unknown threats, respectively. Extensive experiments on UCF-101 and HMDB-51 demonstrate that FMVP outperforms state-of-the-art methods (DiffPure, Defense Patterns (DP), Temporal Shuffling (TS) and FlowPure), achieving robust accuracy exceeding 87% against PGD and 89% against CW attacks. Furthermore, FMVP demonstrates superior robustness against adaptive attacks (DiffHammer) and functions as a zero-shot adversarial detector, attaining detection accuracies of 98% for PGD and 79% for highly imperceptible CW attacks.

</details>


### [156] [VIBE: Visual Instruction Based Editor](https://arxiv.org/abs/2601.02242)
*Grigorii Alekseenko,Aleksandr Gordeev,Irina Tolstykh,Bulat Suleimanov,Vladimir Dokholyan,Georgii Fedorov,Sergey Yakubson,Aleksandra Tsybina,Mikhail Chernyshov,Maksim Kuprashevich*

Main category: cs.CV

TL;DR: 提出了一种高效、小型的基于指令的图像编辑方法，兼具低推理成本和高编辑质量。


<details>
  <summary>Details</summary>
Motivation: 当前开源的图像编辑方法往往参数量大，计算开销高，难以实际部署，小参数、低成本且高质量的解决方案需求突出。

Method: 精心设计了一个由2B参数的Qwen3-VL模型与1.6B参数的扩散模型Sana1.5协同工作的图像编辑流程，重点在结构设计、数据流程、训练和评估策略优化以兼顾源一致性与成本。

Result: 在ImgEdit和GEdit基准测试上，本文方法性能等同甚至优于更大规模基线，尤其在属性调整、对象移除等需保持原图一致性的编辑任务上表现突出。4秒可生成2K分辨率图片，占用24GB显存，无需复杂推理优化。

Conclusion: 证明了小型、高效的指令引导型图像编辑系统完全有能力媲美甚至超越大型模型，具有实际应用潜力。

Abstract: Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.

</details>


### [157] [A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets](https://arxiv.org/abs/2601.02246)
*Annoor Sharara Akhand*

Main category: cs.CV

TL;DR: 本文比较了三种CNN训练范式：自定义训练、特征提取、和迁移学习，并在五个实际图像分类任务上评估了各自表现和效率。迁移学习效果最好，自定义CNN在资源受限时有较好权衡。


<details>
  <summary>Details</summary>
Motivation: 虽然CNN广泛用于视觉识别，但实践中选择自定义训练还是迁移学习存在困惑。本文希望通过系统对比，帮助用户理清在不同应用和资源条件下的最佳选择。

Method: 作者在五个真实世界分类任务上，分别采用自定义小型CNN、预训练大模型特征提取、预训练模型迁移学习三种方式进行训练和评估，采用准确率、宏F1分数与训练效率等指标比较效果。

Result: 迁移学习在所有数据集上一致取得了最佳性能。自定义CNN在计算资源或内存受限时，在效率和准确率之间取得了良好平衡。

Conclusion: 迁移学习方案适用于对准确性要求高的场景，而自定义小型CNN适合资源受限条件下的实际部署，表明方法选择应根据实际应用需求和资源情况权衡。

Abstract: Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency--accuracy trade-off, especially when compute and memory budgets are constrained.

</details>


### [158] [SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection](https://arxiv.org/abs/2601.02249)
*Xiantai Xiang,Guangyao Zhou,Zixiao Wen,Wenshuai Li,Ben Niu,Feng Wang,Lijia Huang,Qiantong Wang,Yuhan Liu,Zongxu Pan,Yuxin Hu*

Main category: cs.CV

TL;DR: 该论文提出了SLGNet，一个高效的多模态目标检测框架，通过融合结构先验与语言调控，提升了RGB与红外图像在复杂环境下的检测性能，并大幅减少了可训练参数量。


<details>
  <summary>Details</summary>
Motivation: 当前多模态检测方法大多关注模型高效性，忽视了模态结构一致性，特别是在高对比、夜间等具有显著域差的环境下会丢失关键结构信息。同时，传统的静态融合机制缺乏对环境的自适应能力，导致在动态复杂场景中表现不佳。

Method: 作者提出SLGNet，包括两个关键创新：（1）结构感知适配器，用于提取两种模态的分层结构表征，并动态融合进ViT骨干网络，以弥补ViT结构感知能力的不足；（2）语言引导调控模块，通过VLM生成的结构化描述动态调整视觉特征，实现对不同环境的鲁棒适应。

Result: 在LLVIP、FLIR、KAIST、DroneVehicle等数据集上，SLGNet达到了新的SOTA性能。特别是在LLVIP基准上，mAP达到66.1，并将可训练参数量减少了约87%。

Conclusion: SLGNet结合结构先验与语言引导，显著提升了多模态感知任务的鲁棒性和效率，是面向全场景、多环境目标检测的有效解决方案。

Abstract: Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.

</details>


### [159] [VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation](https://arxiv.org/abs/2601.02256)
*Shikun Sun,Liao Qu,Huichao Zhang,Yiheng Liu,Yangyang Song,Xian Li,Xu Wang,Yi Jiang,Daniel K. Du,Xinglong Wu,Jia Jia*

Main category: cs.CV

TL;DR: 本文提出了一种改进视觉自回归（VAR）模型训练的强化学习优化框架，通过优化GRPO算法，有效缓解异步策略冲突问题，并显著提升生成质量与目标对齐度。


<details>
  <summary>Details</summary>
Motivation: VAR模型在视觉生成中可实现灵活的异构结构输入，但在多步生成与强化学习应用下，因异步性导致策略冲突，影响模型训练的稳定性和效果。需要专门机制克服这些冲突。

Method: 提出增强版GRPO框架，引入三项核心技术：（1）中间奖励机制，稳定前期生成；（2）动态步长加权，实现更精确的奖励归因；（3）新型遮罩传播算法，借鉴奖励反馈学习（ReFL）原理，空间与时间上分离优化影响。

Result: 与原始GRPO基线对比，改进方法在样本质量和与目标对齐度方面取得了显著提升。

Conclusion: 通过针对性地处理VAR模型中的策略异步冲突，提出的改进GRPO方法能够有效提升VAR在强化学习场景下的优化稳定性和生成表现，具有较强的实用价值。

Abstract: Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.

</details>


### [160] [DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies](https://arxiv.org/abs/2601.02267)
*Renke Wang,Zhenyu Zhang,Ying Tai,Jian Yang*

Main category: cs.CV

TL;DR: 本文提出了DiffProxy框架，通过生成与多视图一致的人体代理，提升了从多视图图像中恢复人体网格的性能。DiffProxy利用基于扩散的生成先验，桥接了合成数据与真实数据之间的鸿沟，并在纯合成数据训练下，在多项真实世界基准上实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 多视图人体网格恢复面临两个难题：真实数据集中的标注不完美会影响模型训练，而合成数据虽然标注精确，却有域差异问题，影响泛化。为解决这一瓶颈，作者希望设计一个方法，既能利用合成数据的精准标注，又能在真实世界任务中实现高泛化。

Method: DiffProxy框架有三大创新：(1) 多条件生成机制，实现多视图一致、与像素对齐的人体代理；(2) 手部细化模块通过灵活视觉提示增强局部细节恢复能力；(3) 测试时不确定性自适应调整策略，提高模型在复杂案例下的鲁棒性。整个网络仅用合成数据训练，通过基于扩散模型的生成先验，有效连接合成与真实域。

Result: DiffProxy在五个真实世界基准测试上取得了最优表现，尤其在存在遮挡与局部视角的场景下表现出优异的零样本泛化能力。

Conclusion: DiffProxy有效解决了泛化和标注准确性的平衡难题，展示了基于扩散生成先验的人体网格恢复新范式，为合成数据训练的人体恢复方法带来新的突破。

Abstract: Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html

</details>


### [161] [TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation](https://arxiv.org/abs/2601.02273)
*Salim Khazem*

Main category: cs.CV

TL;DR: 本文提出TopoLoRA-SAM方法，实现对领域特定的二值语义分割，尤其擅长处理细结构和噪声模态数据，在保持参数高效的同时性能超越多种主流分割模型。


<details>
  <summary>Details</summary>
Motivation: 现有基础分割模型如SAM虽然泛化能力强，但迁移到特定领域（如视网膜血管、SAR图像）时存在适应性差、计算资源消耗大以及完整微调带来的灾难性遗忘等问题，特别是在需要分割细结构和面对噪声时更为明显。

Method: 作者提出TopoLoRA-SAM：在ViT编码器冻结的基础上，注入低秩适应（LoRA）方法，并结合轻量级的空间卷积适配器，还可选用可微分的clDice实现拓扑结构感知的监督，以提升二值分割的特定结构识别能力。

Result: 在五个数据集（视网膜血管三个，肠息肉一个，SAR海陆分割一个）上，用主流模型（U-Net、DeepLabV3+、SegFormer、Mask2Former）作对比，TopoLoRA-SAM模型在仅训练5.2%参数的情况下，取得了所有数据集上的最佳平均Dice分数，特别是在挑战性较高的CHASE_DB1上显著提升了分割精度与鲁棒性。

Conclusion: TopoLoRA-SAM通过参数高效的方式，实现了领域特定分割任务的高性能，使其能媲美甚至超越全量微调的专家模型，尤其适合细结构和高噪声环境，是领域自适应分割的新型有效解法。

Abstract: Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgetting. We propose \textbf{TopoLoRA-SAM}, a topology-aware and parameter-efficient adaptation framework for binary semantic segmentation. TopoLoRA-SAM injects Low-Rank Adaptation (LoRA) into the frozen ViT encoder, augmented with a lightweight spatial convolutional adapter and optional topology-aware supervision via differentiable clDice. We evaluate our approach on five benchmarks spanning retinal vessel segmentation (DRIVE, STARE, CHASE\_DB1), polyp segmentation (Kvasir-SEG), and SAR sea/land segmentation (SL-SSDD), comparing against U-Net, DeepLabV3+, SegFormer, and Mask2Former. TopoLoRA-SAM achieves the best retina-average Dice and the best overall average Dice across datasets, while training only \textbf{5.2\%} of model parameters ($\sim$4.9M). On the challenging CHASE\_DB1 dataset, our method substantially improves segmentation accuracy and robustness, demonstrating that topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models. Code is available at : https://github.com/salimkhazem/Seglab.git

</details>


### [162] [InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams](https://arxiv.org/abs/2601.02281)
*Shuai Yuan,Yantai Yang,Xiaotian Yang,Xupeng Zhang,Zhonghao Zhao,Lingming Zhang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: 本文提出InfiniteVGGT，一种面向无限时域的因果视觉几何变换器，结合了有限自适应KV缓存与高效剪枝策略，实现了兼顾扩展性与长期稳定性的3D视觉几何流处理。提出Long3D基准，实现超长序列性能评测。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉几何建模面临大规模、持续场景下对“实时性、可扩展性、长期稳定性”间的矛盾。现有离线方案虽精度高但不适用于流式环境；现有流式方法难以支持无限时域输入，且长期会发生灾难性漂移，限制了实际部署。

Method: InfiniteVGGT设计为因果流处理（流式Transformer），采用可扩展但有限并可更新的KV缓存，并引入无需训练、与注意力机制无关的剪枝机制，智能地舍弃过时特征，实现随帧动态滚动记忆。兼容FlashAttention提升效率。同时，提出了Long3D数据集，用于实现极长3D几何理解的标准化评测。

Result: InfiniteVGGT实现了无限时域的3D视觉流处理，不仅超越了已有流式方法的长期稳定性，还首次实现持续、超长时序下的能力评测。实验显示在Long3D等超长序列基准下优于以往方法。

Conclusion: InfiniteVGGT解决了实时3D几何流处理扩展性与稳定性不可兼得的难题。提出Long3D为领域长期benchmark。推动了长时序、实时3D理解领域发展，为后续流式3D视觉几何研究奠定了基础。

Abstract: The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT

</details>


### [163] [Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery](https://arxiv.org/abs/2601.02289)
*Tom Burgert,Leonard Hackel,Paolo Rota,Begüm Demir*

Main category: cs.CV

TL;DR: 本文提出了GeoRank方法，通过直接优化球面距离，将地理关系嵌入特征空间，从而提升了多光谱遥感图像自监督学习的表现，并对多种关键自监督对比学习改进进行了系统分析。


<details>
  <summary>Details</summary>
Motivation: 多光谱遥感图像在地理和时间上具有高度变异性，现有的自监督学习方法难以充分利用这些特性，亟需新的方法来更好地嵌入地理元信息。

Method: 提出GeoRank正则化方法，通过优化球面距离，将地理相关关系直接融入对比自监督学习的特征空间。同时系统性地分析了多光谱图像自监督学习的关键适配因素，如数据增强、数据集规模、图像尺寸和时间性视角对任务的影响。

Result: GeoRank在集成地理元数据方面优于或持平于现有方法，并能持续提升多种主流对比自监督学习算法（如BYOL、DINO）的性能。

Conclusion: GeoRank是一种有效的正则化技术，能够让自监督模型更好地捕捉地理空间结构，提升遥感图像理解能力，并为多光谱遥感自监督学习算法的进一步研究提供了参考。

Abstract: Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.

</details>


### [164] [SortWaste: A Densely Annotated Dataset for Object Detection in Industrial Waste Sorting](https://arxiv.org/abs/2601.02299)
*Sara Inácio,Hugo Proença,João C. Neves*

Main category: cs.CV

TL;DR: 本文介绍了一个新的废物分拣数据集SortWaste，并提出了用于评估场景复杂度的新指标ClutterScore，对当前主流目标检测方法在该数据集上的表现进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 随着人口增长导致垃圾产量增加，现有手工或自动化分拣方法在效率和准确性上均面临挑战，缺乏真实场景下的数据集严重限制了自动分拣系统的发展。

Method: 1）采集并标注了真实材料回收工厂的废弃物图像，构建SortWaste目标检测数据集；2）提出ClutterScore指标，用以量化分拣线图像的复杂度；3）对比评测了多种主流目标检测算法，在不同场景复杂度下的效果。

Result: 提出的数据集与指标有效反映出真实垃圾分拣线的复杂度，当前检测方法在简单场景下表现良好（塑料检测mAP为59.7%），但在高度杂乱场景中性能明显下降。

Conclusion: 现有目标检测方法在真实、复杂的废弃物场景下面临显著挑战，未来需要开发更强大、适应性更高的模型与更具代表性的数据集。

Abstract: The increasing production of waste, driven by population growth, has created challenges in managing and recycling materials effectively. Manual waste sorting is a common practice; however, it remains inefficient for handling large-scale waste streams and presents health risks for workers. On the other hand, existing automated sorting approaches still struggle with the high variability, clutter, and visual complexity of real-world waste streams. The lack of real-world datasets for waste sorting is a major reason automated systems for this problem are underdeveloped. Accordingly, we introduce SortWaste, a densely annotated object detection dataset collected from a Material Recovery Facility. Additionally, we contribute to standardizing waste detection in sorting lines by proposing ClutterScore, an objective metric that gauges the scene's hardness level using a set of proxies that affect visual complexity (e.g., object count, class and size entropy, and spatial overlap). In addition to these contributions, we provide an extensive benchmark of state-of-the-art object detection models, detailing their results with respect to the hardness level assessed by the proposed metric. Despite achieving promising results (mAP of 59.7% in the plastic-only detection task), performance significantly decreases in highly cluttered scenes. This highlights the need for novel and more challenging datasets on the topic.

</details>


### [165] [360DVO: Deep Visual Odometry for Monocular 360-Degree Camera](https://arxiv.org/abs/2601.02309)
*Xiaopeng Guo,Yinzhe Xu,Huajian Huang,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 本文提出了360DVO，这是首个基于深度学习的单目全景视觉里程计(OVO)系统，通过特定的特征提取器和优化策略，大幅提升了全景视觉定位的鲁棒性和精度。


<details>
  <summary>Details</summary>
Motivation: 现有单目全景视觉里程计多依赖手工特征或光度准则，在剧烈运动或光照变化等复杂场景下表现不稳定，因此亟需一种能够更好适应360度图像畸变和环境干扰的方法。

Method: 提出了畸变感知球面特征提取器（DAS-Feat），自适应学习适用于360度图像的抗畸变特征；利用这些稀疏特征建立约束，在新颖的可微分全景束束调整(ODBA)模块中实现高效位姿估计。同时，作者亦构建了一个新的真实世界OVO基准数据集用于评测。

Result: 在新建立的真实基准和公开号合成数据集（TartanAir V2和360VO）上，360DVO对比360VO、OpenVSLAM等最新方法，鲁棒性提升50%，精度提升37.5%。

Conclusion: 360DVO通过深度学习与全景感知特征更好地融合，极大提升了里程计在复杂场景下的性能，为单目全景VO相关研究提供了新思路和评测标准。

Abstract: Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems. However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination. To address this, we present 360DVO, the first deep learning-based OVO framework. Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images. These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module. To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark. Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%. Homepage: https://chris1004336379.github.io/360DVO-homepage

</details>


### [166] [Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping](https://arxiv.org/abs/2601.02315)
*Saurabh Kaushik,Lalit Maurya,Beth Tellman*

Main category: cs.CV

TL;DR: 本文提出了一种新架构Prithvi-CAFE，在洪水遥感图像分割任务上显著优于现有GFM和U-Net基线，特别是在Sen1Flood11和FloodPlanet数据集上取得最优表现。


<details>
  <summary>Details</summary>
Motivation: 虽然Geo-Foundation Models（GFMs）在遥感下游任务中有效，但在洪水分割方面却无法超过简单的U-Net基线，主要原因在于GFMs难以捕捉关键的局部细节特征。

Method: 作者设计了Prithvi-CAFE架构，将Prithvi预训练GFM编码器与带有卷积注意力模块(CAM)的并行CNN残差分支融合。该方法通过适配器实现高效微调，并在多个尺度和多个层次与CNN特征融合，有效地增强了局部细节建模，同时保留了GFM的长距离依赖能力。

Result: 在Sen1Flood11测试集上，Prithvi-CAFE取得了83.41的IoU，优于原始Prithvi（82.50）及其他主流GFMs，并在hold-out测试站点上显著超越U-Net和Prithvi（81.37 vs 70.57/72.42）。在FloodPlanet数据集上，同样取得了64.70的IoU，高于U-Net和其他GFMs。

Conclusion: Prithvi-CAFE架构简单有效，通过融合GFM和CNN的优势，极大提升了需要局部细节和多通道、多模态信息的分割任务表现，为相关领域提供了一种通用且高效的解决方案。

Abstract: Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}

</details>


### [167] [Fusion2Print: Deep Flash-Non-Flash Fusion for Contactless Fingerprint Matching](https://arxiv.org/abs/2601.02318)
*Roja Sahoo,Anoop Namboodiri*

Main category: cs.CV

TL;DR: 提出一种名为Fusion2Print (F2P)的无接触指纹融合框架，通过融合闪光与非闪光采集的指纹图像，显著提升指纹识别准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无接触指纹识别较接触式更卫生便捷，但由于光照变化、皮肤颜色差异等，指纹脊线清晰度下降，影响识别效果。闪光采集虽能保留脊线细节但会引入噪声，非闪光采集则噪声低但脊线对比度弱，因此亟需提升无接触采集的图像质量和识别性能。

Method: 提出F2P框架：首先采集配对的闪光/非闪光指纹图像并构建FNF数据库，通过人工差分消除背景噪声保留脊线，之后用轻量注意力融合网络综合两种图像的信息，并用U-Net增强模块生成最优加权灰阶图。最后设计具跨域兼容性的深度特征模型用于统一空间的指纹比对。

Result: F2P方法提升了指纹脊线清晰度，识别性能超越了单独采集和主流基线方法（AUC=0.999，EER=1.12%），实验在新构建的FNF数据集上验证。

Conclusion: 新提出的F2P框架凭借多模态融合及端到端增强，显著提高了无接触指纹的识别准确率和通用性，有望成为接触式指纹系统的强有力替代。

Abstract: Contactless fingerprint recognition offers a hygienic and convenient alternative to contact-based systems, enabling rapid acquisition without latent prints, pressure artifacts, or hygiene risks. However, contactless images often show degraded ridge clarity due to illumination variation, subcutaneous skin discoloration, and specular reflections. Flash captures preserve ridge detail but introduce noise, whereas non-flash captures reduce noise but lower ridge contrast. We propose Fusion2Print (F2P), the first framework to systematically capture and fuse paired flash-non-flash contactless fingerprints. We construct a custom paired dataset, FNF Database, and perform manual flash-non-flash subtraction to isolate ridge-preserving signals. A lightweight attention-based fusion network also integrates both modalities, emphasizing informative channels and suppressing noise, and then a U-Net enhancement module produces an optimally weighted grayscale image. Finally, a deep embedding model with cross-domain compatibility, generates discriminative and robust representations in a unified embedding space compatible with both contactless and contact-based fingerprints for verification. F2P enhances ridge clarity and achieves superior recognition performance (AUC=0.999, EER=1.12%) over single-capture baselines (Verifinger, DeepPrint).

</details>


### [168] [BEDS: Bayesian Emergent Dissipative Structures](https://arxiv.org/abs/2601.02329)
*Laurent Caraffa*

Main category: cs.CV

TL;DR: 本文提出了BEDS（Bayesian Emergent Dissipative Structures）理论框架，将非平衡热力学、贝叶斯推断、信息几何与机器学习统一起来，揭示学习本质上是通过熵的输出将能流转化为结构；该理论不仅解释了学习的物理和数学基础，还通过实际网络系统验证了其高能效和可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有关于学习、计算和自组织结构的理论多数分散于不同领域，比如热力学、信息论与机器学习。本研究动机在于提出一个统一理论，将这些领域的重要思想融为一体，解释学习系统在不同尺度下的共同本质，并探究其与物理、数学定理的深层联系。

Method: 首先在理论层面，将Prigogine耗散结构理论与贝叶斯推断进行形式映射，探讨学习如何通过耗散过程实现。数学上推导贝叶斯推断的固定点与基本常数（e、π、φ）之间的关系，并提出哥德尔不完备性与热力学约束之间的结构类比。实验层面设计并实现了一个符合BEDS原则的点对点网络架构，测试其能效与学习能力。

Result: 推导出e、π、φ等基本常数可由贝叶斯推断的最小公理自洽地产生；发现形式系统的不完备性与物理系统的耗散缺陷具备结构同构。实际系统验证实现了比现有分布式共识系统高六个数量级的能效提升，并能持续自适应学习。

Conclusion: BEDS理论为学习与计算系统提供了物理、数学与工程统一解释框架，揭示了学习系统的可持续性和结构性本质，同时为迈向高能效、可持续的人工智能系统设计提供了理论和工程路径。

Abstract: We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine's theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence.
  We derive fundamental mathematical constants (e, π, φ) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking Gödel's incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems.
  As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence.

</details>


### [169] [Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding](https://arxiv.org/abs/2601.02339)
*Jingming He,Chongyi Li,Shiqi Wang,Sam Kwong*

Main category: cs.CV

TL;DR: 本论文提出了一种联合增强的3D语义高斯建模方法，实现了同时提升三维语义分割与图像渲染质量。


<details>
  <summary>Details</summary>
Motivation: 以往3DGS（3D Gaussian Splatting）将语义和渲染分开处理，仅依赖2D监督，忽视了3D高斯几何信息，且高斯自适应策略对细节区或低纹理区域表现不足，因此需要更加协同且高效的建模方法。

Method: 作者提出了一种协同增强的框架：一是引入各向异性的3D高斯Chebyshev描述符，利用Laplace-Beltrami算子捕捉精细的3D形状描述，从而区分外观相似物体并降低对2D监督的依赖；二是利用局部语义与形状信号自适应调整高斯分配和球谐函数，提高了渲染效率和资源利用率；三是加入跨场景知识迁移模块，实现对已学习形状模式的持续更新，加快新场景下的收敛速度，提升模型泛化性。

Result: 在多个数据集上，该方法提升了语义分割精度和渲染质量，并保持了高帧率渲染性能。

Conclusion: 联合增强的建模方法有效解决了3DGS中语义与渲染分离的问题，不仅提升了分割与渲染质量，同时具有较强的泛化能力和高效性。

Abstract: Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.

</details>


### [170] [Meta-Learning Guided Pruning for Few-Shot Plant Pathology on Edge Devices](https://arxiv.org/abs/2601.02353)
*Shahnawaz Alam,Mohammed Mudassir Uddin,Mohammed Kaif Pasha*

Main category: cs.CV

TL;DR: 本论文提出结合神经网络剪枝与小样本学习的方法，实现植物病害识别模型的极致压缩与高效部署。新方法在不显著损失精度的情况下，大幅减小模型体积，使其可在树莓派等低算力设备上实时运行。


<details>
  <summary>Details</summary>
Motivation: 边远地区农民急需低成本、便捷的方法识别植物病害，但往往缺乏实验室条件与高计算资源。常规的深度学习模型体积庞大且依赖大量标注数据，难以直接应用于边缘设备和实际场景。因此，需研发既能在小样本条件下学习、又能高效运行的轻量模型。

Method: 提出Disease-Aware Channel Importance Scoring（DACIS）方法，用于评估神经网络各通道对病害识别的贡献。在此基础上，设计Prune-then-Meta-Learn-then-Prune（PMP）三阶段流程：先初步剪枝，后基于小样本的元学习，自适应调整，最终进一步剪枝得到高效模型。

Result: 在PlantVillage与PlantDoc两个数据集上，压缩后的模型体积减少78%，同时保留原始精度的92.3%。新模型能在树莓派4设备上达到7帧每秒的推理速度，满足野外实时病害识别需求。

Conclusion: 本文方法兼顾模型紧凑性和识别准确率，显著降低部署门槛，使资源有限的农户也能应用深度学习技术进行高效病害识别，促进智能农业技术的普及。

Abstract: Farmers in remote areas need quick and reliable methods for identifying plant diseases, yet they often lack access to laboratories or high-performance computing resources. Deep learning models can detect diseases from leaf images with high accuracy, but these models are typically too large and computationally expensive to run on low-cost edge devices such as Raspberry Pi. Furthermore, collecting thousands of labeled disease images for training is both expensive and time-consuming. This paper addresses both challenges by combining neural network pruning -- removing unnecessary parts of the model -- with few-shot learning, which enables the model to learn from limited examples. This paper proposes Disease-Aware Channel Importance Scoring (DACIS), a method that identifies which parts of the neural network are most important for distinguishing between different plant diseases, integrated into a three-stage Prune-then-Meta-Learn-then-Prune (PMP) pipeline. Experiments on PlantVillage and PlantDoc datasets demonstrate that the proposed approach reduces model size by 78\% while maintaining 92.3\% of the original accuracy, with the compressed model running at 7 frames per second on a Raspberry Pi 4, making real-time field diagnosis practical for smallholder farmers.

</details>


### [171] [Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes](https://arxiv.org/abs/2601.02356)
*Jing Tan,Zhaoyang Zhang,Yantao Shen,Jiarui Cai,Shuo Yang,Jiajun Wu,Wei Xia,Zhuowen Tu,Stefano Soatto*

Main category: cs.CV

TL;DR: 本文提出了Talk2Move，一种基于强化学习的扩散框架，实现了通过文本指令对场景中物体空间几何变换的精准控制，克服了现有方法在对象层级几何操作上的诸多限制。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态生成系统虽然可以根据文本调整物体外观或风格，但对于对象的几何空间操作（如平移、旋转、缩放）表现不佳，主要由于缺乏配对监督数据以及像素级优化的局限。因此，急需一种无需大量配对数据的新方法，实现自然语言驱动的精确空间几何变换。

Method: Talk2Move采用Group Relative Policy Optimization（GRPO）强化学习算法，通过输入图像及轻量级文本变化生成丰富的操作序列（rollouts），不依赖于昂贵的配对数据。系统设置空间奖励模型直接对几何变化与语言描述的一致性进行评估；同时，通过离策略评估与主动步骤采样优化学习效率，尤其关注信息量丰富的变换步骤。创新性地引入以物体为中心的空间奖励，用于直接衡量平移、旋转和缩放等操作的效果，使变换过程更具可解释性与一致性。

Result: 在精心设计的基准测试上，Talk2Move展示了高精度、一致且能忠实表达语义的物体空间变换能力，在空间准确性和场景一致性方面均优于现有的文本驱动编辑方法。

Conclusion: Talk2Move突破了传统文本指导对象空间操作的难题，提出了一套无需配对数据、可解释且高效的几何变换工具，为多模态编辑系统带来更强的空间理解与操控能力。

Abstract: We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.

</details>


### [172] [VINO: A Unified Visual Generator with Interleaved OmniModal Context](https://arxiv.org/abs/2601.02358)
*Junyi Chen,Tong He,Zhoujie Fu,Pengfei Wan,Kun Gai,Weicai Ye*

Main category: cs.CV

TL;DR: VINO是一个统一的视觉生成器，能够在同一模型下实现图像和视频的生成与编辑，表现优异且通用性强。


<details>
  <summary>Details</summary>
Motivation: 现有的图像与视频生成和编辑多依赖各自独立的模型或特定模块，导致效率低、扩展性差，因此需要一种能够统一处理多模态视觉任务的通用框架。

Method: VINO将视觉-语言模型与多模态扩散Transformer结合，通过编码混合的文本、图像和视频条件，指导统一的扩散生成过程，并采用多阶段训练逐步扩展模型任务能力，实现同一模型支持图像和视频输入输出。

Result: 在多种生成和编辑基准上，VINO在视觉质量、指令遵循性、参考与属性保持、多身份编辑的可控性等方面表现优异。

Conclusion: VINO为可扩展的统一视觉生成提供了有效方案，并展示了上下文交错推理为通用视觉生成奠定基础的潜力。

Abstract: We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.

</details>


### [173] [ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors](https://arxiv.org/abs/2601.02359)
*Kaede Shiohara,Toshihiko Yamasaki,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 本文提出了一种全新的基于扩散模型的自监督方法ExposeAnyone，用于检测未知的深度伪造（deepfake）变换，并且在多项数据集上明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 检测未知的deepfake伪造是目前人脸伪造检测领域最大的挑战之一，传统有监督方法容易对特定伪造类型过拟合，泛化能力较差。现有自监督方法又难以学到区分性强的特征。

Method: 提出ExposeAnyone，完全自监督，基于扩散模型从音频生成对应的表情序列。通过参考集将模型个性化到具体对象，再通过扩散重建误差估算可疑视频与个性对象的身份距离，实现特定对象的人脸伪造检测。

Result: 在DF-TIMIT、DFDCP、KoDF和IDForge数据集上的平均AUC超越现有方法4.22个百分点。还能检测Sora2生成的视频，传统方法在此场景表现很差。此外，对图像模糊、压缩等扰动鲁棒性极强，适用性更强。

Conclusion: ExposeAnyone实现了自监督下更有力的未知deepfake检测，兼顾精度、鲁棒性与泛化力，在实际应用中更具有可行性和优势。

Abstract: Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [174] [The Qualitative Laboratory: Theory Prototyping and Hypothesis Generation with Large Language Models](https://arxiv.org/abs/2601.00797)
*Hugues Draelants*

Main category: cs.CL

TL;DR: 本文提出利用大语言模型（LLMs）进行社会学人格模拟，以生成多样社会群体对新信息的定性假设，并展示其优于传统方法的能力。


<details>
  <summary>Details</summary>
Motivation: 社会科学面临的核心挑战之一是如何产生关于不同社会群体解读新信息的丰富定性假设。传统方法如情景调查和基于规则的主体模型存在话语深度不足和形式化瓶颈等局限，因此需寻求更有效的工具。

Method: 作者提出并展示了利用大语言模型进行'人格模拟'，即基于社会学理论定义不同群体人格，让其用自然语言回应政策信息，从而探查其深层次反应。该方法能够突破传统调查和模型难以实现的细腻世界观表达。

Result: 通过以气候反应理论为基础生成的人格模拟实验，研究发现产生了细致且具有反直觉特点的新假设，比如保守人格拒绝以国家安全为框架的政策宣传，这挑战了以往的理论预设。

Conclusion: 作者认为，将该模拟方法纳入“模拟-验证”流程，可为社会科学生成丰富、具有深度的定性假设，并形成后续经验验证的优质基础。

Abstract: A central challenge in social science is to generate rich qualitative hypotheses about how diverse social groups might interpret new information. This article introduces and illustrates a novel methodological approach for this purpose: sociological persona simulation using Large Language Models (LLMs), which we frame as a "qualitative laboratory". We argue that for this specific task, persona simulation offers a distinct advantage over established methods. By generating naturalistic discourse, it overcomes the lack of discursive depth common in vignette surveys, and by operationalizing complex worldviews through natural language, it bypasses the formalization bottleneck of rule-based agent-based models (ABMs). To demonstrate this potential, we present a protocol where personas derived from a sociological theory of climate reception react to policy messages. The simulation produced nuanced and counter-intuitive hypotheses - such as a conservative persona's rejection of a national security frame - that challenge theoretical assumptions. We conclude that this method, used as part of a "simulation then validation" workflow, represents a superior tool for generating deeply textured hypotheses for subsequent empirical testing.

</details>


### [175] [Rate-Distortion Analysis of Compressed Query Delegation with Low-Rank Riemannian Updates](https://arxiv.org/abs/2601.00938)
*Faruk Alpay,Bugra Kilictas*

Main category: cs.CL

TL;DR: 本文提出了一种在有限工作记忆预算下，提升智能体中间推理能力的新方法：压缩查询委托（CQD）。该方法通过将高维推理状态压缩、向外部Oracle查询并基于黎曼优化更新状态来实现更高效的推理。理论分析和实验表明CQD在有限计算与上下文条件下优于传统推理基线。


<details>
  <summary>Details</summary>
Motivation: 当前智能体在解决复杂推理任务时容易受限于有效工作记忆容量（带宽有限的上下文）。为克服此局限，作者提出研究如何在有限上下文/预算下有效委托与重塑推理状态的问题。该方法期望在提高推理效率与准确性的同时，节省计算和上下文资源。

Method: 方法包括三步：(1)把高维的推理隐状态压缩成低秩张量查询，(2)用压缩后的最小查询询问外部Oracle（可以看作噪声算子），(3)用通过钳制黎曼优化对固定秩流形上的隐状态进行更新。论文还将此过程形式化为受约束随机规划，分析其与率失真理论和信息瓶颈的联系，并给出收敛性保证。

Result: 实验包括：A.设计了含2500个任务的受限上下文推理评测集（来源于BBH与悖论实例），比较CQD与传统链式思维（chain-of-thought）基线的方法在固定计算和上下文下的表现；B.进行“认知镜像”人类对比评测，量化知识增益与语义漂移。结果显示CQD在有限资源下表现出更优的推理结果与泛化能力。

Conclusion: CQD通过压缩与分布式推理方法，有效突破了智能体在上下文和计算能力受限时推理瓶颈。其与经典信息理论连接紧密，并给出了理论收敛保证。实验验证其在实际复杂推理任务中的性能，显示应用前景广阔。

Abstract: Bounded-context agents fail when intermediate reasoning exceeds an effective working-memory budget. We study compressed query delegation (CQD): (i) compress a high-dimensional latent reasoning state into a low-rank tensor query, (ii) delegate the minimal query to an external oracle, and (iii) update the latent state via Riemannian optimization on fixed-rank manifolds. We give a math-first formulation: CQD is a constrained stochastic program with a query-budget functional and an oracle modeled as a noisy operator. We connect CQD to classical rate-distortion and information bottleneck principles, showing that spectral hard-thresholding is optimal for a natural constrained quadratic distortion problem, and we derive convergence guarantees for Riemannian stochastic approximation under bounded oracle noise and smoothness assumptions. Empirically, we report (A) a 2,500-item bounded-context reasoning suite (BBH-derived tasks plus curated paradox instances) comparing CQD against chain-of-thought baselines under fixed compute and context; and (B) a human "cognitive mirror" benchmark (N=200) measuring epistemic gain and semantic drift across modern oracles.

</details>


### [176] [Intention Collapse: Intention-Level Metrics for Reasoning in Language Models](https://arxiv.org/abs/2601.01011)
*Patricio Vera*

Main category: cs.CL

TL;DR: 本文提出了“意图崩塌”概念，分析了语言生成过程中内部高维意图如何投射为单一文本序列，并基于此为语言模型研究设计了新颖的意图度量指标，通过实验证明这些指标可区分推理模式及揭示部分信息损失现象。


<details>
  <summary>Details</summary>
Motivation: 在当前大语言模型生成回复时，大量高维内部意图信息被压缩、丢失、难以外部捕捉与衡量。该论文试图系统性刻画这种“意图崩塌”现象，希望借助量化分析，更好理解推理阶段的内部状态与其在外部表达上的联系。

Method: 作者正式定义了“意图崩塌”现象，并为主流语言模型提出三种模型无关的意图度量指标（意图熵Hint、有效维数dimeff和潜在知识可恢复性Recov），设计实验对比“直接回答”“思维链推理（CoT）”和“无意识输出（babble）”三种推理范式，借助4bit Mistral 7B在GSM8K数据集上进行初步实证分析。

Result: 实验证明，思维链推理不仅可显著提高问题解答准确率（5.5%→53%），还能极大降低意图崩塌前的熵（1.42降到0.37bit），且在有效维数上优于其他模式。同时，意图熵Hint对单项预测效果有限，CoT模式下线性探针可一定程度区分意图状态，而基线模式则几乎无法区分。

Conclusion: 论文开创性地提出意图层度量，可区分不同推理方式并部分揭示意图信息的损失形式，也强调了当前度量指标在准确捕捉个例意图、完全展示内部知识方面的不足，为后续深入刻画生成模型内部认知和提高外部解释性提供了新思路。

Abstract: Every act of language generation compresses a rich internal state into a single token sequence. We call this process intention collapse: a many-to-one projection from a high dimensional intention space I into an external language space L. We formalize intention collapse for contemporary language models, define three simple, model agnostic intention metrics (intention entropy Hint, effective dimensionality dimeff, and latent knowledge recoverability Recov), and propose an empirical agenda for studying how inference time computation shapes internal intentions before they are verbalized. We also report a first small scale experiment. Using a 4 bit Mistral 7B model on 200 GSM8K problems, we compare a direct answer baseline, a chain of thought (CoT) regime, and a babble control. CoT raises accuracy from 5.5 percent to 53 percent, sharply reduces pre collapse intention entropy (from 1.42 to 0.37 bits), and shows higher global effective dimensionality than the other regimes despite producing fewer tokens than babble. At the same time, Hint has little item level predictive power, and a linear probe on I achieves AUROC 0.65 in the CoT regime but only about chance in the baseline regime, where it collapses to the majority class. These preliminary results indicate that intention level metrics can distinguish inference regimes and expose latent information that is partly lost during collapse, while also revealing important limitations of our current proxies

</details>


### [177] [HyperJoin: LLM-augmented Hypergraph Link Prediction for Joinable Table Discovery](https://arxiv.org/abs/2601.01015)
*Shiyuan Liu,Jianwei Wang,Xuemin Lin,Lu Qin,Wenjie Zhang,Ying Zhang*

Main category: cs.CL

TL;DR: 本文提出了HyperJoin框架，针对数据湖中的可连接表发现任务，通过超图建模和大型语言模型增强，实现了比现有方法更高效、更一致的表列发现。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的方法对表结构建模不充分，忽略了列间丰富的结构信息及候选列间的相互作用，导致发现结果不一致、精度受限。

Method: HyperJoin首先用超图将表的结构关系建模，结合表内和表间（通过LLM增强）超边，问题被转化为超图上的链接预测。随后，设计了分层交互网络HIN，通过双向信息传递学习列的表达。在线排名环节，引入关注候选列一致性的reranking模块，采用最大生成树算法提升输出的连贯性。

Result: 在实验中，HyperJoin在Precision@15和Recall@15两个指标上，分别较最佳基线提升了21.4%和17.2%。

Conclusion: HyperJoin有效建模了可连接表发现任务中的复杂结构关系，并引入连贯性优化机制，大幅提升了结果的精度与一致性。

Abstract: As a pivotal task in data lake management, joinable table discovery has attracted widespread interest. While existing language model-based methods achieve remarkable performance by combining offline column representation learning with online ranking, their design insufficiently accounts for the underlying structural interactions: (1) offline, they directly model tables into isolated or pairwise columns, thereby struggling to capture the rich inter-table and intra-table structural information; and (2) online, they rank candidate columns based solely on query-candidate similarity, ignoring the mutual interactions among the candidates, leading to incoherent result sets. To address these limitations, we propose HyperJoin, a large language model (LLM)-augmented Hypergraph framework for Joinable table discovery. Specifically, we first construct a hypergraph to model tables using both the intra-table hyperedges and the LLM-augmented inter-table hyperedges. Consequently, the task of joinable table discovery is formulated as link prediction on this constructed hypergraph. We then design HIN, a Hierarchical Interaction Network that learns expressive column representations through bidirectional message passing over columns and hyperedges. To strengthen coherence and internal consistency in the result columns, we cast online ranking as a coherence-aware top-k column selection problem. We then introduce a reranking module that leverages a maximum spanning tree algorithm to prune noisy connections and maximize coherence. Experiments demonstrate the superiority of HyperJoin, achieving average improvements of 21.4% (Precision@15) and 17.2% (Recall@15) over the best baseline.

</details>


### [178] [Multi-Dimensional Prompt Chaining to Improve Open-Domain Dialogue Generation](https://arxiv.org/abs/2601.01037)
*Livia Leong Hui Teng*

Main category: cs.CL

TL;DR: 本文提出了一种多维度prompt-chaining框架，提升了小语言模型（SLM）在开放域对话生成中的自然性、一致性和吸引力，使其性能接近大模型。


<details>
  <summary>Details</summary>
Motivation: 小模型部署方便，但在开放域对话质量上远逊于大模型；研究旨在提升SLM的对话质量，缩小与大模型的性能差距。

Method: 提出集成“自然性”、“连贯性”和“吸引力”三维度的prompt-chaining框架，并将其应用于TinyLlama和Llama-2-7B；对比其与Llama-2-70B和GPT-3.5 Turbo的对话结果，同时进行自动评测及人工评测，包括多样性、上下文连贯性及综合质量。

Result: 该框架使回应多样性提升最多29%，上下文连贯性提升最多28%，吸引力和自然性提升最多29%；Llama-2-7B在多项指标上达到甚至媲美Llama-2-70B和GPT-3.5 Turbo。

Conclusion: 精心设计的prompt链式策略能够在资源有限的情况下显著提升SLM的开放域对话质量，具有较强的实际应用价值。

Abstract: Small language models (SLMs) offer significant deployment advantages but often struggle to match the dialogue quality of larger models in open-domain settings. In this paper, we propose a multi-dimensional prompt-chaining framework that integrates Naturalness, Coherence, and Engagingness dimensions to enhance human-likeness in open-domain dialogue generation. We apply the framework to two SLMs, TinyLlama and Llama-2-7B, and benchmark their performance against responses generated by substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. We then employ automatic and human evaluation to assess the responses based on diversity, contextual coherence, as well as overall quality. Results show that the full framework improves response diversity by up to 29%, contextual coherence by up to 28%, and engagingness as well as naturalness by up to 29%. Notably, Llama-2-7B achieves performance comparable to substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. Overall, the findings demonstrate that carefully designed prompt-based strategies provide an effective and resource-efficient pathway to improving open-domain dialogue quality in SLMs.

</details>


### [179] [KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs](https://arxiv.org/abs/2601.01046)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 本文提出KV-Embedding方法，在无需训练的情况下提升大型语言模型（LLM）的语义表征能力，通过重新利用模型内部状态实现更优的embedding效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在训练自由场景中的embedding应用面临两大结构性问题：（1）因果注意力结构导致前面的token无法获取全局上下文；（2）以下一个token预测为目标的训练容易让生成特征占优，难以实现良好的语义压缩。研究者希望提升冻结LLM作为表征骨干（embedding backbone）的实用性。

Method: 提出KV-Embedding框架，利用每层最后一个token的Key-Value（KV）状态，作为整个序列的压缩信息，并通过将这些KV状态作为前缀注入，使所有token都能单次前向推理获得全局上下文。并且设计自动化的层选择策略，确保方法对不同LLM具有普适性。

Result: 在Qwen、Mistral和Llama等主流LLM骨干和MTEB评测集上，KV-Embedding在无需训练前提下比同类最优基线提升最多可达10%，在最长4096 token的序列上也保持稳健性能。

Conclusion: 方法证明操纵LLM内部状态是一种高效替代输入修改的embedding增强方式，将激励更多关于LLM内部机制在表征学习领域探索。

Abstract: While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.

</details>


### [180] [Unsupervised Text Style Transfer for Controllable Intensity](https://arxiv.org/abs/2601.01060)
*Shuhuan Gu,Wenbiao Tao,Xinchen Ma,Kangkang He,Ye Guo,Xiang Li,Yunshi Lan*

Main category: cs.CL

TL;DR: 该论文提出了一种无监督文本风格迁移（UTST）新方法，特别关注风格强度的可控迁移，通过SFT-then-PPO范式对LLM进行微调，有效提升了模型区分和生成不同风格强度文本的能力。


<details>
  <summary>Details</summary>
Motivation: 无监督文本风格迁移任务（UTST）挑战在于缺乏平行数据及风格强度层级间细微差别难以区分，现有方法在可控改变风格强度方面表现不足。如何让大模型能细致区分和控制风格强度，是推动文本生成多样化和应用拓展的研究动机。

Method: 方法包括两个阶段：首先用合成的平行数据对大语言模型（LLM）进行微调（SFT），然后采用PPO训练，设计了能区分层级风格强度的奖励函数，综合考虑文本的全局和局部风格特征以优化模型。

Result: 在两个UTST基准任务上，实验验证奖励函数对模型微调各具优势，有效提升了不同评测指标上的性能。即使在强度相近的风格之间，生成的文本也能展现明显区别。

Conclusion: 通过SFT-then-PPO的训练范式和精细奖励设计，提出方法显著提升了大模型在可控文本风格强度迁移上的表现，对细粒度风格控制和多样化文本生成具有推动作用。

Abstract: Unsupervised Text Style Transfer (UTST) aims to build a system to transfer the stylistic properties of a given text without parallel text pairs. Compared with text transfer between style polarities, UTST for controllable intensity is more challenging due to the subtle differences in stylistic features across different intensity levels. Faced with the challenges posed by the lack of parallel data and the indistinguishability between adjacent intensity levels, we propose a SFT-then-PPO paradigm to fine-tune an LLM. We first fine-tune the LLM with synthesized parallel data. Then, we further train the LLM with PPO, where the rewards are elaborately designed for distinguishing the stylistic intensity in hierarchical levels. Both the global and local stylistic features are considered to formulate the reward functions. The experiments on two UTST benchmarks showcase that both rewards have their advantages and applying them to LLM fine-tuning can effectively improve the performance of an LLM backbone based on various evaluation metrics. Even for close levels of intensity, we can still observe the noticeable stylistic difference between the generated text.

</details>


### [181] [ks-lit-3m: A 3.1 million word kashmiri text dataset for large language model pretraining](https://arxiv.org/abs/2601.01091)
*Haq Nawaz Malik*

Main category: cs.CL

TL;DR: 本文提出了KS-LIT-3M，这是一个专为喀什米尔语设计、包含310万词的语料库，为大语言模型的喀什米尔语预训练提供了基础数据。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在高资源语言上表现出色，但在喀什米尔语等低资源语言上生成连贯文本能力较差，主要原因在于缺乏高质量训练数据。喀什米尔语文学长期以来以InPage专有格式保存，难以被现代NLP系统利用。

Method: 作者开发了InPage到Unicode的专用转换器，将喀什米尔语文学资料转化为可用文本。之后通过去除英语污染、字符标准化和质量验证等严格预处理流程，最终形成了包含多风格文学、新闻、学术与宗教文本的平滑文本流，用于因果语言模型训练。

Result: 构建了包含310万词、1640万个字符、131,607个独特词条的高质量喀什米尔语语料库，并以CC-BY-4.0协议公开，极大弥补了喀什米尔语NLP数据缺口。

Conclusion: KS-LIT-3M为喀什米尔语的自然语言处理研究提供了关键资源，有助于推动该语言在大语言模型中的应用与发展。

Abstract: Large Language Models (LLMs) demonstrate remarkable fluency across high-resource languages yet consistently fail to generate coherent text in Kashmiri, a language spoken by approximately seven million people. This performance disparity stems not from inherent model limitations but from a critical scarcity of high-quality training data. Decades of Kashmiri literature remain inaccessible to modern NLP pipelines due to their encoding in the proprietary InPage desktop publishing format. This paper introduces KS-LIT-3M, a curated corpus of 3.1 million words (16.4 million characters) specifically designed for pretraining language models on Kashmiri. The dataset is structured as a single continuous linear text stream, optimized for causal language model training where models learn to predict subsequent tokens from preceding context. The corpus was constructed through the development of a specialized InPage-to-Unicode converter, followed by rigorous preprocessing including English contamination removal, character normalization, and quality validation. Encompassing 131,607 unique words drawn from diverse genres including literary works, journalistic writing, academic texts, and religious scholarship, KS-LIT-3M addresses a fundamental resource gap for Kashmiri language technology. The dataset is released under the CC-BY-4.0 license to facilitate research in Kashmiri natural language processing.

</details>


### [182] [EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation](https://arxiv.org/abs/2601.01112)
*Zilin Li,Weiwei Xu,Xuanbo Lu,Zheda Liu*

Main category: cs.CL

TL;DR: 本论文提出了EmoLoom-2B，一种可复现、轻量化的情感分析流水线，能够高效筛选和提升2B参数以下的小型语言模型在多维情感理解任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着情感认知相关应用日渐广泛，如何在有限算力下让小规模语言模型兼顾情感分类及VAD多维预测能力，成为了实际部署中的痛点。现有方法缺乏统一评估协议，并容易受到冗余差异干扰。

Method: 1. 设计统一数据IO协议，排除不必要的变异；2. 引入（1）VAD一致性约束和（2）外部评估分类器两大语义正则化；3. 采用Valence Flip极性增强技术；4. 通过A/B采样结合温度调节提升训练效率。

Result: 在Qwen-1.8B-Chat基础上，EmoLoom-2B在GoEmotions与EmpatheticDialogues等经典情感任务集上取得了优异成绩，并在DailyDialog上展现了良好的跨语料泛化能力。

Conclusion: EmoLoom-2B是一种经济高效、可回溯并适用于小型LLM的情感分析方案，为重训练或多模态融合前的模型筛选流程提供了可靠路径。

Abstract: We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion.

</details>


### [183] [Listen, Attend, Understand: a Regularization Technique for Stable E2E Speech Translation Training on High Variance labels](https://arxiv.org/abs/2601.01121)
*Yacouba Diarra,Michael Leventhal*

Main category: cs.CL

TL;DR: 提出了一种名为LAU的语义正则化方法，通过冻结文本嵌入引入语义约束，有效提升端到端语音翻译系统在低资源、高语义歧义场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 端到端语音翻译在目标文本表现出高变异性和语义歧义时，通常收敛慢且效果差。该领域缺乏简单但有效的方法来提升声学编码器的语义感知能力，尤其在数据稀缺或嘈杂时问题尤为突出。

Method: 作者提出Listen, Attend, Understand (LAU)方法，在训练时用冻结的文本嵌入作为辅助目标，对声学编码器的隐空间加以正则，使其学习具有语言语义基础的表达，而不会导致推理成本增加。同时，提出Total Parameter Drift指标评估正则化对模型结构的影响。

Result: LAU在Bambara-法语数据集（含30小时非专业转译语音）上，与数据量多一倍的预训练系统效果相当，且在语义保持上表现更佳。Total Parameter Drift证明正则化使编码器更关注语义信息。

Conclusion: LAU可以强化语音翻译系统中的语义建模能力，效果优于传统的后处理打分，是数据不充分或数据嘈杂时值得采用的方法。

Abstract: End-to-End Speech Translation often shows slower convergence and worse performance when target transcriptions exhibit high variance and semantic ambiguity. We propose Listen, Attend, Understand (LAU), a semantic regularization technique that constrains the acoustic encoder's latent space during training. By leveraging frozen text embeddings to provide a directional auxiliary loss, LAU injects linguistic groundedness into the acoustic representation without increasing inference cost. We evaluate our method on a Bambara-to-French dataset with 30 hours of Bambara speech translated by non-professionals. Experimental results demonstrate that LAU models achieve comparable performance by standard metrics compared to an E2E-ST system pretrained with 100\% more data and while performing better in preserving semantic meaning. Furthermore, we introduce Total Parameter Drift as a metric to quantify the structural impact of regularization to demonstrate that semantic constraints actively reorganize the encoder's weights to prioritize meaning over literal phonetics. Our findings suggest that LAU is a robust alternative to post-hoc rescoring and a valuable addition to E2E-ST training, especially when training data is scarce and/or noisy.

</details>


### [184] [RoboPhD: Self-Improving Text-to-SQL Through Autonomous Agent Evolution](https://arxiv.org/abs/2601.01126)
*Andrew Borthwick,Stephen Ash*

Main category: cs.CL

TL;DR: 本文提出了RoboPhD系统，AI自主进化以提升Text-to-SQL性能，实现无需人工指导的自动研究和系统优化。


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-SQL任务领域，人工设计与优化模型消耗人力且容错性低，亟需探索AI系统自主进化和优化算法的可行路径。

Method: RoboPhD包含SQL生成代理与进化代理两个核心部分。系统以数据库分析与SQL指令构建生成代理，通过ELO排名驱动的进化代理根据性能反馈不断生成新版本，实现生存竞争和非传递性性能选择。整个过程完全闭环，自主演化并逐步引入有效策略。

Result: 系统从简单的70行基线演化到复杂的1500行，自动发现多种有效数据库分析和SQL生成模式。改进显著：在弱模型与强模型上分别提升8.9与2.3点，更低成本下达到更高准确度。最终在BIRD测试集上取得73.67%准确率。

Conclusion: AI能够自主构建并进化性能强大的Text-to-SQL代理系统，仅需极简人类介入。RoboPhD推动了自动化系统发展的边界，在模型进化和成本优化方面表现突出。

Abstract: We present RoboPhD, a system where AI agents autonomously conduct research to improve Text-to-SQL performance. RoboPhD implements a closed-loop evolution cycle with two coordinated components: a SQL Generation agent composed of a database analysis script and SQL generation instructions, and an Evolution agent that designs new versions based on performance feedback. Central to the framework is an ELO-based selection mechanism enabling survival-of-the-fittest dynamics while handling non-transitivity in performance. Starting from a naive 70-line baseline, RoboPhD evolves agents through iterative cross-pollination, discovering effective techniques without any external guidance on the Text-to-SQL domain. Our best agent, evolved to 1500 lines over 18 iterations, autonomously discovered strategies such as size-adaptive database analysis that adjusts depth based on schema complexity and SQL generation patterns for column selection, evidence interpretation, and aggregation. Evolution provides the largest gains on cheaper models: while we improve by 2.3 points over a strong Claude Opus 4.5 naive baseline, we show an improvement of 8.9 points over the weaker Claude Haiku model. This enables 'skip a tier' deployment: evolved Haiku exceeds naive Sonnet accuracy, and evolved Sonnet exceeds naive Opus, both at lower cost. The full system achieves 73.67% accuracy on the BIRD test set, demonstrating that AI can autonomously build a strong agentic system with only a trivial human-provided starting point.

</details>


### [185] [KOS-TL (Knowledge Operation System Type Logic)](https://arxiv.org/abs/2601.01143)
*Peng Chen*

Main category: cs.CL

TL;DR: KOS-TL是一种新型知识操作系统类型逻辑框架，将数据、逻辑与证明融合，用以支持自动化、可执行知识系统。其多层架构和形式化基础确保系统进化过程中的一致性和可验证性，已在工业追溯和金融合规等场景得到应用。


<details>
  <summary>Details</summary>
Motivation: 传统知识表示存在静态逻辑和系统动态执行的断层，缺乏严密逻辑基础，难以实现知识系统的自治和执行。因此，亟需一种能将数据、逻辑与证明统一，并具备良好可验证性的知识系统逻辑基础。

Method: KOS-TL架构分三层：核心层（定义类型宇宙与基本构件）、内核层（通过事件驱动元组控制状态演化）、运行时层（实现物理信号与逻辑证据的双向映射）。其结合了依赖类型理论和事件语义，形式化定义操作语义，并证明元理论性质如进展性和演化一致性。

Result: KOS-TL实现了知识库状态变更的可证性和系统自洽性，避免系统在连续状态转换中出现卡死。通过在工业追溯和跨境金融合规中的实用案例，验证了其逻辑框架的可行性和适用性。

Conclusion: KOS-TL为智能自治操作系统提供了坚实、可形式化验证的知识基础，是知识驱动系统核心逻辑的重要进步，有望支撑新一代高可信度知识操作平台。

Abstract: This paper introduces KOS-TL (Knowledge Operation System Type Logic), a novel constructive framework designed to provide a rigorous logical foundation for autonomous and executable knowledge systems. Traditional knowledge representation models often suffer from a gap between static symbolic logic and dynamic system execution. To bridge this divide, KOS-TL leverages Dependent Type Theory to unify data, logic, and proof into a singular computational substrate.The architecture of KOS-TL is organized into three hierarchical layers: the Core Layer, which defines the static type universe and constructive primitives; the Kernel Layer, which governs state evolution through an event-driven mechanism characterized by the triple $\langle Σ, \textsf{Ev}, Δ\rangle$; and the Runtime Layer, responsible for the bidirectional refinement of physical signals into logical evidence. We formally define the operational semantics of the system and prove key meta-theoretical properties, including Progress and Evolutionary Consistency, ensuring that the system remains logically self-consistent and free from stuck states during continuous state transitions.By integrating Davidsonian event semantics with Martin-Löf type theory, KOS-TL enables the construction of "proof-carrying knowledge," where every state change in the knowledge base is accompanied by a formal witness of its validity. We demonstrate the practical utility of this logic through application examples in industrial traceability and cross-border financial compliance. Our results suggest that KOS-TL provides a robust, formally verifiable basis for the next generation of intelligent, autonomous operating systems.

</details>


### [186] [SongSage: A Large Musical Language Model with Lyric Generative Pre-training](https://arxiv.org/abs/2601.01153)
*Jiani Guo,Jiajia Li,Jie Wu,Zuchao Li,Yujiu Yang,Ping Wang*

Main category: cs.CL

TL;DR: 本文提出PlaylistSense数据集用于评估大模型对歌单的理解能力，并基于歌词预训练打造了SongSage音乐大模型，展现了强大的歌词理解与处理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在歌词相关知识和歌单理解能力方面的研究较少。为推动此方向进展，作者构建了新的评测数据集并提出专门的音乐语言模型。

Method: 1. 构建PlaylistSense数据集，涵盖十类常见用户歌单查询。
2. 预训练音乐大模型SongSage，先在5.48B歌词文本上自监督预训练，再用包括77.5万条指令样本的LyricBank-SFT进行精调，涵盖九大歌词任务。

Result: 实验显示，据歌词相关任务，SongSage显著优于通用大模型；在多项能力如歌单推荐、歌词生成及理解等方面均表现突出，同时保持一般知识理解能力，MMLU成绩有竞争力。

Conclusion: SongSage展示了专注于歌词任务的大模型的有效性，对音乐AI领域具有推动作用。代码和模型将开源以促进相关研究，但原始数据因版权原因暂不公开。

Abstract: Large language models have achieved significant success in various domains, yet their understanding of lyric-centric knowledge has not been fully explored. In this work, we first introduce PlaylistSense, a dataset to evaluate the playlist understanding capability of language models. PlaylistSense encompasses ten types of user queries derived from common real-world perspectives, challenging LLMs to accurately grasp playlist features and address diverse user intents. Comprehensive evaluations indicate that current general-purpose LLMs still have potential for improvement in playlist understanding. Inspired by this, we introduce SongSage, a large musical language model equipped with diverse lyric-centric intelligence through lyric generative pretraining. SongSage undergoes continual pretraining on LyricBank, a carefully curated corpus of 5.48 billion tokens focused on lyrical content, followed by fine-tuning with LyricBank-SFT, a meticulously crafted instruction set comprising 775k samples across nine core lyric-centric tasks. Experimental results demonstrate that SongSage exhibits a strong understanding of lyric-centric knowledge, excels in rewriting user queries for zero-shot playlist recommendations, generates and continues lyrics effectively, and performs proficiently across seven additional capabilities. Beyond its lyric-centric expertise, SongSage also retains general knowledge comprehension and achieves a competitive MMLU score. We will keep the datasets inaccessible due to copyright restrictions and release the SongSage and training script to ensure reproducibility and support music AI research and applications, the datasets release plan details are provided in the appendix.

</details>


### [187] [DHI: Leveraging Diverse Hallucination Induction for Enhanced Contrastive Factuality Control in Large Language Models](https://arxiv.org/abs/2601.01156)
*Jiani Guo,Xiangke Zeng,Jie Wu,Zuchao Li*

Main category: cs.CL

TL;DR: 论文提出了一种新的训练框架DHI，用以引导大型语言模型（Evil LLM）生成更为多样化的幻觉（错误或虚假信息），从而在对抗解码中更好地减少实际模型的幻觉生成。


<details>
  <summary>Details</summary>
Motivation: 现有减少LLM幻觉的方法，通常利用经过特定训练的Evil LLM生成有限类型的幻觉，这种方式受限于幻觉多样性，无法涵盖更多类型的错误，削弱了解决实际问题的能力。为此，作者提出了无需依赖标注数据、能更丰富生成各种类型幻觉的方法。

Method: DHI框架通过修改损失函数，降低对某些事实正确tokens的生成权重，促使Evil LLM在目标位置生成多样化幻觉，同时整体上保持文本的真实性。进一步采用因果注意力掩码，减少这种惩罚对后续token生成的副作用。推理阶段在解码时引入自适应合理性约束，只在“正模型”高置信输出的tokens上使用对比惩罚，避免影响事实性token。

Result: 在多个幻觉测试基准上，DHI显著优于其它基于对比解码的幻觉缓解方法，带来了实际性能提升。

Conclusion: DHI方法无需标注幻觉数据，即可驱动Evil LLM生成更加多样的幻觉，从而提高对比解码的有效性，是减少LLMs幻觉生成的更优方法。

Abstract: Large language models (LLMs) frequently produce inaccurate or fabricated information, known as "hallucinations," which compromises their reliability. Existing approaches often train an "Evil LLM" to deliberately generate hallucinations on curated datasets, using these induced hallucinations to guide contrastive decoding against a reliable "positive model" for hallucination mitigation. However, this strategy is limited by the narrow diversity of hallucinations induced, as Evil LLMs trained on specific error types tend to reproduce only these particular patterns, thereby restricting their overall effectiveness. To address these limitations, we propose DHI (Diverse Hallucination Induction), a novel training framework that enables the Evil LLM to generate a broader range of hallucination types without relying on pre-annotated hallucination data. DHI employs a modified loss function that down-weights the generation of specific factually correct tokens, encouraging the Evil LLM to produce diverse hallucinations at targeted positions while maintaining overall factual content. Additionally, we introduce a causal attention masking adaptation to reduce the impact of this penalization on the generation of subsequent tokens. During inference, we apply an adaptive rationality constraint that restricts contrastive decoding to tokens where the positive model exhibits high confidence, thereby avoiding unnecessary penalties on factually correct tokens. Extensive empirical results show that DHI achieves significant performance gains over other contrastive decoding-based approaches across multiple hallucination benchmarks.

</details>


### [188] [Almost Clinical: Linguistic properties of synthetic electronic health records](https://arxiv.org/abs/2601.01171)
*Serge Sharoff,John Baker,David Francis Hunt,Alan Simpson*

Main category: cs.CL

TL;DR: 本研究评估了用于精神健康领域的合成电子健康记录（EHRs）的语言和临床适用性，发现虽然合成文本具备一定的临床相关性，但仍存在偏差。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）生成的合成EHRs在医学应用中的兴起，亟需评估其在精神健康领域的语言表达和临床信息表达的可靠性与适用性。

Method: 论文首先阐述了合成语料创建的原因与方法。随后，通过考察四种临床文本类型（评估、通信、转诊及护理计划），分析LLMs在表达医疗权威和患者主动性时的语言选择，包括主语表达、语气和信息流动方式。

Result: 研究表明，LLMs生成的文本在连贯性和专用术语使用上大致符合实际临床实践，但存在明显偏差，包括语域变化、临床细节不足以及用药和诊断方面的具体不准确。

Conclusion: 合成EHR虽展现出初步的实用性，但要精确支持精神健康领域的真实临床需求，还需进一步优化其语言表达和医学细节的准确性。

Abstract: This study evaluates the linguistic and clinical suitability of synthetic electronic health records (EHRs) in the field of mental health. First, we describe the rationale and the methodology for creating the synthetic corpus. Second, we assess agency, modality, and information flow across four clinical genres (Assessments, Correspondence, Referrals and Care plans) to understand how LLMs grammatically construct medical authority and patient agency through linguistic choices. While LLMs produce coherent, terminology-appropriate texts that approximate clinical practice, systematic divergences remain, including registerial shifts, insufficient clinical specificity, and inaccuracies in medication use and diagnostic procedures.

</details>


### [189] [Stylometry Analysis of Human and Machine Text for Academic Integrity](https://arxiv.org/abs/2601.01225)
*Hezam Albaqami,Muhammad Asif Ayub,Nasir Ahmad,Yaseen Ahmad,Mohammed M. Alqahtani,Abdullah M. Algamdi,Almoaid A. Owaidah,Kashif Ahmad*

Main category: cs.CL

TL;DR: 本文提出了一个基于自然语言处理（NLP）的学生内容认证框架，以应对学术诚信中的抄袭、捏造和作者验证等关键挑战，重点通过作者归属与文体变更检测提升内容真实性。


<details>
  <summary>Details</summary>
Motivation: 随着学术诚信问题日益突出，如抄袭和内容造假频发，现有解决方案较为片面，缺乏针对性和综合性，尤其在多作者协作与机器文本识别方面亟需创新。

Method: 提出NLP框架，聚焦四大任务：1）区分人类与机器文本；2）区分单一与多作者文档；3）多作者文档中作者变更检测；4）多作者协作文档中作者识别。方法在用不同提示词生成的Gemini数据集上进行评估。

Result: 在更为严格的提示词生成的数据集上，框架表现有一定下降，显示机器生成文本检测在应对巧妙人机混合内容时难度提升。研究公开了数据集与代码，便于后续研究。

Conclusion: 本研究推动了教育内容作者认证与检测的研究进展，为未来探讨学术不端检测提供了新的基线与公共资源，尤其在面向复杂机器文本和多作者场景表现出实用价值。

Abstract: This work addresses critical challenges to academic integrity, including plagiarism, fabrication, and verification of authorship of educational content, by proposing a Natural Language Processing (NLP)-based framework for authenticating students' content through author attribution and style change detection. Despite some initial efforts, several aspects of the topic are yet to be explored. In contrast to existing solutions, the paper provides a comprehensive analysis of the topic by targeting four relevant tasks, including (i) classification of human and machine text, (ii) differentiating in single and multi-authored documents, (iii) author change detection within multi-authored documents, and (iv) author recognition in collaboratively produced documents. The solutions proposed for the tasks are evaluated on two datasets generated with Gemini using two different prompts, including a normal and a strict set of instructions. During experiments, some reduction in the performance of the proposed solutions is observed on the dataset generated through the strict prompt, demonstrating the complexities involved in detecting machine-generated text with cleverly crafted prompts. The generated datasets, code, and other relevant materials are made publicly available on GitHub, which are expected to provide a baseline for future research in the domain.

</details>


### [190] [Racka: Efficient Hungarian LLM Adaptation on Academic Infrastructure](https://arxiv.org/abs/2601.01244)
*Zsolt Csibi,Bence György Gortka,Natabara Gyöngyössy,Kornél Nagy,Dávid Márk Nemeskey,Martin Sallai,András Simonyi,András Márk Szekeres,Gábor Palkó*

Main category: cs.CL

TL;DR: Racka 是一种专为匈牙利语及高资源语言（如英语、德语）设计的轻量级大语言模型，通过高效参数调整持续预训练，旨在缩小中小语种与主流语种之间的资源差距。


<details>
  <summary>Details</summary>
Motivation: 解决匈牙利语在预训练大模型中资源不足、与高资源语言（如英语、德语）表现差距大的问题。希望通过更实际可行的方式强化中小语种模型能力，促进多语言大模型的均衡发展。

Method: Racka 基于 Qwen-3 4B 主干，通过 LoRA 进行参数高效的持续预训练，支持在资源有限的 A100 集群上线运行。特别地，针对训练分布更换和调整了分词器，提升了匈牙利语的分词效果。同时使用包含44%匈牙利语、24%英语、21%德语及11%代码的混合数据集，保证新语言学习的同时不过度遗忘高资源语言能力。

Result: 初步实验结果显示，Racka 在语言适应方面取得了温和但稳定的提升，尤其对匈牙利语有大幅度分词改进，同时在英文和德文上保持了竞争力。

Conclusion: Racka 证明了通过参数高效的持续预训练和分词器优化，在有限计算资源下可以有效提升中小语种（如匈牙利语）在大语言模型中的表现，并且不会显著退化高资源语言的能力。

Abstract: We present Racka, a lightweight, continually pretrained large language model designed to bridge the resource gap between Hungarian and high-resource languages such as English and German. Racka employs parameter-efficient continual pretraining via Low-Rank Adaptation (LoRA) on a Qwen-3 4B backbone, making the recipe practical on A100 (40GB)-based HPC clusters with low inter-node bandwidth. To better match the training distribution, we replace and adapt the tokenizer, achieving substantially improved tokenization fertility for Hungarian while maintaining competitive performance in English and German. The model is trained on 160B subword tokens drawn from a mixture of internet and high-quality curated sources, with a composition of 44% Hungarian, 24% English, 21% German, and 11% code. This data mix is chosen to mitigate catastrophic forgetting and preserve high-resource language capabilities during continual pretraining. Our preliminary results indicate modest but stable results in language adaptation.

</details>


### [191] [From Policy to Logic for Efficient and Interpretable Coverage Assessment](https://arxiv.org/abs/2601.01266)
*Rhitabrat Pokharel,Hamid Hassanzadeh,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 本文提出一种结合大模型和符号推理的方法，用于医疗政策审核，提高了效率和准确性，并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 法律和政策文件语句复杂细长，LLMs虽然能够理解这些文本，但在分析主观、细致内容时，常会出现幻觉和前后不一致问题。医疗政策审核对此类问题尤为敏感，且需要可依赖的高准确性支持专家决策。

Method: 提出了一种融合大模型与符号规则推理的混合方法，具体为用关注政策内容的检索器定位相关语句，再通过规则推理将其组织成事实和规则，生成可审核的推理理由，减少了大模型的调用次数。

Result: 该系统在推理成本上降低了44%，F1分数提升了4.5%，在效率和效果上均有显著提升。

Conclusion: 混合检索与符号推理的方法能在减少推理成本的同时提升政策审核的准确性，对医疗政策审核等对准确性要求极高的场景具有重要应用价值。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.

</details>


### [192] [Does Memory Need Graphs? A Unified Framework and Empirical Analysis for Long-Term Dialog Memory](https://arxiv.org/abs/2601.01280)
*Sen Hu,Yuxiang Wei,Jiaxin Ran,Zhiyuan Yao,Lei Zou*

Main category: cs.CL

TL;DR: 本文对对话记忆系统中图结构方法的有效性进行了系统性的实证分析，归纳了影响性能的核心因素，并提出了统一的分析框架和强基线。


<details>
  <summary>Details</summary>
Motivation: 当前对话记忆系统频繁采用图结构，但相关实证研究结果不一致，缺乏对关键设计要素的系统性归纳。因此需要深入剖析哪些设计选择真正影响系统性能。

Method: 提出一个统一框架，将对话记忆系统分解为核心组件，支持图结构与非图结构方法。并在LongMemEval与HaluMem两个数据集上，通过分阶段的受控实验，系统比较记忆表示、组织、维护、检索等关键设计。

Result: 实验发现，许多性能差异来源于基础系统设定，而非具体的架构创新。研究揭示了基础设置在对话记忆系统中的主导作用。

Conclusion: 文中归纳了未来对话记忆研究中值得采用的强大且稳定的基线方案，为后续工作提供了方向和借鉴。

Abstract: Graph structures are increasingly used in dialog memory systems, but empirical findings on their effectiveness remain inconsistent, making it unclear which design choices truly matter. We present an experimental, system-oriented analysis of long-term dialog memory architectures. We introduce a unified framework that decomposes dialog memory systems into core components and supports both graph-based and non-graph approaches. Under this framework, we conduct controlled, stage-wise experiments on LongMemEval and HaluMem, comparing common design choices in memory representation, organization, maintenance, and retrieval. Our results show that many performance differences are driven by foundational system settings rather than specific architectural innovations. Based on these findings, we identify stable and reliable strong baselines for future dialog memory research.

</details>


### [193] [T3C: Test-Time Tensor Compression with Consistency Guarantees](https://arxiv.org/abs/2601.01299)
*Ismail Lamaakal,Chaymae Yahyati,Yassine Maleh,Khalid El Makkaoui,Ibrahim Ouahbi*

Main category: cs.CL

TL;DR: T3C是一种支持部署时灵活压缩、可控精度和秩的新技术，能在单次训练后根据资源预算调整模型规模和速度，并用快速证书保障模型可靠性，显著提升了模型在保准精度下的延迟和体积表现。


<details>
  <summary>Details</summary>
Motivation: 现有模型压缩方法通常只能针对特定预算（如延迟、能耗、模型大小）单次优化，缺乏部署时灵活按需调整能力，也无法实时保障模型可靠性。因此需要一种能一次训练后，在部署时根据不同设备和资源限制灵活切换压缩级别的方案，并具备高效的可靠性保证机制。

Method: T3C框架结合了最大秩下的弹性张量分解、每层动态分配比特数的混合精度量化、以及根据硬件预算自动映射秩和比特的轻量控制器。其训练过程中引入利用谱代理和激活统计的快速层级一致性证书，用以约束模型输出变动（logit drift），常规化训练过程，几乎不增加计算开销。此外，T3C的策略输出严格对硬件友好并随预算单调变化。

Result: 在ImageNet-1k测试中，T3C在保持精度下降≤0.5%的前提下，ResNet-50模型p50延迟达到1.18ms、模型体积仅38MB，均优于同等基准的PTQ-8b方法（延迟1.44ms，体积88MB）；在ViT-B/16上T3C也优于目前主流PTQ/QAT基线（2.30ms，59MB）。T3C提供单一checkpoint即可支持多种设备和预算下灵活、可靠的精度-延迟-模型规模平衡。

Conclusion: T3C实现了单次训练、部署时灵活可控、高效率压缩的目标，并通过智能策略和快速证书显著提升了模型在实际场景中的适应性和可靠性，对高效边缘部署和多设备适配具有重要应用价值。

Abstract: We present T3C, a train-once, test-time budget-conditioned compression framework that exposes rank and precision as a controllable deployment knob. T3C combines elastic tensor factorization (maintained up to a maximal rank) with rank-tied mixed-precision quantization and a lightweight controller that maps a latency/energy/size budget token to per-layer rank/bit assignments; the policy snaps to hardware-aligned profiles and is monotone in the budget. A fast, layerwise consistency certificate, computed from spectral proxies and activation statistics, upper-bounds logit drift and regularizes training, yielding a practical reliability signal with negligible overhead. On ImageNet-1k, T3C shifts the vision Pareto frontier: for ResNet-50 at matched accuracy (\leq 0.5% drop), p50 latency is 1.18ms with a 38MB model, outperforming PTQ-8b (1.44ms, 88MB); for ViT-B/16, T3C reaches 2.30ms p50 with 59MB, improving over strong PTQ/QAT baselines. A single T3C checkpoint therefore provides predictable, certificate-backed accuracy-latency-size trade-offs on demand across devices.

</details>


### [194] [FLOP-Efficient Training: Early Stopping Based on Test-Time Compute Awareness](https://arxiv.org/abs/2601.01332)
*Hossam Amer,Maryam Dialameh,Hossein Rajabzadeh,Walid Ahmed,Weiwei Zhang,Yang Liu*

Main category: cs.CL

TL;DR: 提出了一种TTC感知训练方法，通过调整中间检查点和推理配置，能大幅减少训练计算量，同时保持或提升模型准确率。实验结果表明训练FLOPs可降低高达92%。


<details>
  <summary>Details</summary>
Motivation: 传统上提升大语言模型性能需大规模训练计算资源，代价高昂。现有工作发现，提升推理时的计算（如反复采样）能使小模型逼近大模型表现。该论文尝试系统化地用推理资源换取训练资源降低整体成本。

Method: 提出TTC感知训练，将中间训练检查点与特定推理配置（如迭代次数等）结合，实现与完全训练模型相当或更优的精度。设计早停算法和高效推理评估方法，避免穷举搜索，并给出推理和训练资源平衡的理论界限。

Result: 实验显示，在不降低模型精度甚至精度提升的前提下，训练所需FLOPs最高可减少92%。表明在合理设置下小模型能通过增加推理计算力获得大模型性能。

Conclusion: 该方法为训练与推理资源平衡提供了新思路，有助于加快模型迭代和部署周期。为实际应用中模型快速更新带来可能性。代码将开源，便于社区复现和推广。

Abstract: Scaling training compute, measured in FLOPs, has long been shown to improve the accuracy of large language models, yet training remains resource-intensive. Prior work shows that increasing test-time compute (TTC)-for example through iterative sampling-can allow smaller models to rival or surpass much larger ones at lower overall cost. We introduce TTC-aware training, where an intermediate checkpoint and a corresponding TTC configuration can together match or exceed the accuracy of a fully trained model while requiring substantially fewer training FLOPs. Building on this insight, we propose an early stopping algorithm that jointly selects a checkpoint and TTC configuration to minimize training compute without sacrificing accuracy. To make this practical, we develop an efficient TTC evaluation method that avoids exhaustive search, and we formalize a break-even bound that identifies when increased inference compute compensates for reduced training compute. Experiments demonstrate up to 92\% reductions in training FLOPs while maintaining and sometimes remarkably improving accuracy. These results highlight a new perspective for balancing training and inference compute in model development, enabling faster deployment cycles and more frequent model refreshes. Codes will be publicly released.

</details>


### [195] [Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems](https://arxiv.org/abs/2601.01341)
*Md Abdullah Al Kafi,Raka Moni,Sumit Kumar Banshal*

Main category: cs.CL

TL;DR: 本文比较了在心理健康咨询中的大语言模型（LLMs）：通用推理模型与特定领域微调模型在RAG（检索增强生成）框架下的表现。结果表明，小规模的通用推理模型在共情能力和上下文理解上优于大规模的心理健康数据微调模型。只要答案基于临床证据，强推理能力比特定词汇的微调更重要。


<details>
  <summary>Details</summary>
Motivation: 心理健康领域采用大语言模型时易出现幻觉与共情能力不足的问题。RAG技术能部分减缓幻觉现象，但尚不清楚在此框架下，通用推理强的模型还是领域专门微调的模型效果更佳。该研究动机是直接对比两类模型，指导心理健康AI系统设计。

Method: 将两类四个开源模型（两个通用推理Qwen2.5-3B、Phi-3-Mini，两个领域微调MentalHealthBot-7B、TherapyBot-7B）同时在RAG管道中测试，使用ChromaDB检索临床知识，基于LLM自动评测（LLM-as-a-Judge）对50轮对话进行综合打分分析。

Result: 通用模型在共情能力得分、上下文理解、安全性等方面优于领域微调模型，且3B参数的通用模型比7B的领域模型表现更佳，特定领域模型更容易过拟合。

Conclusion: 在RAG临床知识支撑下，通用型推理模型能带来更优的心理健康AI系统表现，强推理能力比领域微调更重要。

Abstract: The deployment of Large Language Models (LLMs) in mental health counseling faces the dual challenges of hallucinations and lack of empathy. While the former may be mitigated by RAG (retrieval-augmented generation) by anchoring answers in trusted clinical sources, there remains an open question as to whether the most effective model under this paradigm would be one that is fine-tuned on mental health data, or a more general and powerful model that succeeds purely on the basis of reasoning. In this paper, we perform a direct comparison by running four open-source models through the same RAG pipeline using ChromaDB: two generalist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-tunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge framework to automate evaluation over 50 turns. We find a clear trend: the generalist models outperform the domain-specific ones in empathy (3.72 vs. 3.26, $p < 0.001$) in spite of being much smaller (3B vs. 7B), and all models perform well in terms of safety, but the generalist models show better contextual understanding and are less prone to overfitting as we observe in the domain-specific models. Overall, our results indicate that for RAG-based therapy systems, strong reasoning is more important than training on mental health-specific vocabulary; i.e. a well-reasoned general model would provide more empathetic and balanced support than a larger narrowly fine-tuned model, so long as the answer is already grounded in clinical evidence.

</details>


### [196] [FC-CONAN: An Exhaustively Paired Dataset for Robust Evaluation of Retrieval Systems](https://arxiv.org/abs/2601.01350)
*Juan Junqueras,Florian Boudin,May-Myo Zin,Ha-Thanh Nguyen,Wachara Fungwacharakorn,Damián Ariel Furman,Akiko Aizawa,Ken Satoh*

Main category: cs.CL

TL;DR: FC-CONAN是一个全新数据集，涵盖了所有45条仇恨言论与129条对抗性叙述的组合，旨在推动对仇恨言论的反制手段研究。


<details>
  <summary>Details</summary>
Motivation: 现有的仇恨言论与反对叙述配对数据集覆盖度有限，妨碍了对反制言论模型的全面评估。该研究希望填补这种数据空白，提升检索和分析的精准性。

Method: 作者将45条英文仇恨言论和129条反对性叙述进行穷举式配对，形成FC-CONAN数据集。通过两阶段标注，由9名标注员和4名验证员，划分为Diamond、Gold、Silver、Bronze四个子集，兼顾规模与标注质量。

Result: FC-CONAN数据集生成，标注了所有可能的仇恨言论-反对叙述配对，发现并标注了大量以往未覆盖的正样本，并且数据集与现有CONAN无重叠。

Conclusion: FC-CONAN支持更加真实、细致地评估仇恨言论反制检索系统，也有助于错误分析，并已公开数据集以促进相关研究。

Abstract: Hate speech (HS) is a critical issue in online discourse, and one promising strategy to counter it is through the use of counter-narratives (CNs). Datasets linking HS with CNs are essential for advancing counterspeech research. However, even flagship resources like CONAN (Chung et al., 2019) annotate only a sparse subset of all possible HS-CN pairs, limiting evaluation. We introduce FC-CONAN (Fully Connected CONAN), the first dataset created by exhaustively considering all combinations of 45 English HS messages and 129 CNs. A two-stage annotation process involving nine annotators and four validators produces four partitions-Diamond, Gold, Silver, and Bronze-that balance reliability and scale. None of the labeled pairs overlap with CONAN, uncovering hundreds of previously unlabelled positives. FC-CONAN enables more faithful evaluation of counterspeech retrieval systems and facilitates detailed error analysis. The dataset is publicly available.

</details>


### [197] [Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning](https://arxiv.org/abs/2601.01362)
*Jerry Huang,Peng Lu,Qiuhao Zeng,Yusuke Iwasawa,Yutaka Matsuo,Sarath Chandar,Edison Marrese-Taylor,Irene Li*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在多语言环境下的预测校准问题，发现只用高资源语言微调的方法会导致低资源语言置信度过高但准确率提升有限，引发校准失衡，作者提出标签平滑可作为有效缓解手段。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习模型的预测校准至关重要，但目前关于大语言模型（LLM）的校准，特别是在多语言（尤其是低资源语言）环境下的表现，仍缺乏系统性研究。作者希望揭示多语言数据稀缺如何影响校准，以及现有方法在这种场景下的适用性。

Method: 作者在两个多语言基准（分别覆盖29种和42种语言）上，分析了通过高资源语言SFT（有监督微调）对LLM进行微调后，在低资源语言上的置信度与准确率变化。同时尝试使用标签平滑方法改善校准问题。

Result: 实验发现：尽管在低资源语言上模型置信度显著提升，但准确率提升有限甚至几乎没有，导致模型校准失衡。而通过标签平滑，无需低资源语言SFT数据即可改善所有语言的模型校准。

Conclusion: 作者指出，高资源语言SFT方法在多语言环境下会造成低资源语言的校准问题，强调多语言训练和调优对提升LLM可靠性和公平性的必要性，标签平滑可作为有效改进手段。

Abstract: Ensuring that deep learning models are well-calibrated in terms of their predictive uncertainty is essential in maintaining their trustworthiness and reliability, yet despite increasing advances in foundation model research, the relationship between such large language models (LLMs) and their calibration remains an open area of research. In this work, we look at a critical gap in the calibration of LLMs within multilingual settings, in an attempt to better understand how the data scarcity can potentially lead to different calibration effects and how commonly used techniques can apply in these settings. Our analysis on two multilingual benchmarks, over 29 and 42 languages respectively, reveals that even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration, highlighting a critical shortcoming of standard SFT for multilingual languages. Furthermore, we observe that the use of label smoothing to be a reasonable method alleviate this concern, again without any need for low-resource SFT data, maintaining better calibration across all languages. Overall, this highlights the importance of multilingual considerations for both training and tuning LLMs in order to improve their reliability and fairness in downstream use.

</details>


### [198] [EternalMath: A Living Benchmark of Frontier Mathematics that Evolves with Human Discovery](https://arxiv.org/abs/2601.01400)
*Jicheng Ma,Guohua Wang,Xinhua Feng,Yiming Liu,Zhichao Hu,Yuhong Liu*

Main category: cs.CL

TL;DR: 本文提出了一种自动化的数学推理评估流程，将最新同行评审的数学文献转化为可执行、可验证的评测任务，并生成EternalMath评估套件，以系统性地测试大语言模型在前沿数学推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前针对大语言模型数学推理能力的评测主要依赖静态基准，覆盖不足且随着模型进步很快饱和，同时难以涵盖研究级数学内容。研究动机在于开发一种可以动态更新、覆盖最新数学研究内容且无需依赖专家大规模人工编写的新型评估方法。

Method: 请自动化流程解析最新数学文献，识别具建设性或定量性的研究结论，将其实例化为参数化问题模板，并通过基于执行的验证获得确定性解答，从而实现可扩展、可复现、可持续更新的评测生成。

Result: 利用该流程，作者构建了EternalMath，一个可随研究进展持续演化的数学推理评测套件。对当前最前沿大语言模型的实验证明，它们在前沿数学推理上表现仍然存在较大差距。

Conclusion: 当前大语言模型在研究级数学推理尚未饱和，随学界前沿持续进步的评测机制非常必要。本文提出的自动化、可扩展的评测流程为这一目标提供了可持续的实现路径。

Abstract: Current evaluations of mathematical reasoning in large language models (LLMs) are dominated by static benchmarks, either derived from competition-style problems or curated through costly expert effort, resulting in limited coverage of research-level mathematics and rapid performance saturation. We propose a fully automated, theorem-grounded pipeline for evaluating frontier mathematical reasoning, which directly transforms recent peer-reviewed mathematical literature into executable and verifiable reasoning tasks. The pipeline identifies constructive or quantitative results, instantiates them into parameterized problem templates, and generates deterministic solutions through execution-based verification, enabling scalable, reproducible, and continuously updatable evaluation without reliance on large-scale expert authoring. By design, this approach supports temporal extensibility, intrinsic correctness checking, and domain-specific customization across mathematical subfields. Applying this pipeline yields \textbf{EternalMath}, an evolving evaluation suite derived from contemporary research papers. Experiments with state-of-the-art LLMs reveal substantial performance gaps, indicating that mathematical reasoning at the research frontier remains far from saturated and underscoring the need for evaluation methodologies that evolve in step with human mathematical discovery.

</details>


### [199] [LANCET: Neural Intervention via Structural Entropy for Mitigating Faithfulness Hallucinations in LLMs](https://arxiv.org/abs/2601.01401)
*Chenxu Wang,Chaozhuo Li,Pengbo Wang,Litian Zhang,Songyang Liu,Ji Qi,Jiahui Hu,Yushan Cai,Hao Zhao,Rui Pu*

Main category: cs.CL

TL;DR: 本文提出了Lancet框架，通过结构熵与幻觉差异比的结合，实现对大语言模型幻觉传播路径的精确干预，有效减少了幻觉现象，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有缓解幻觉的方法多为粗粒度抑制或节点级干预，未充分考虑神经信息的分布特性，导致干预不精确。作者发现幻觉在模型内以特定路径传播，需提出精确遏制幻觉传播的新方法。

Method: Lancet框架包括：（1）利用梯度对比分析定位易产生幻觉的神经元；（2）用结构熵最小化方法映射幻觉传播路径；（3）基于传播层次实施有层级的精细干预策略，以保持模型总体能力。

Result: Lancet在多个幻觉基准数据集上评估，明显优于当前主流抑制幻觉方法。

Conclusion: Lancet实现了对神经网络幻觉传播的外科级精准干预，在减少模型幻觉的同时保持总体能力，验证了结构分析在调控神经信息流中的有效性。

Abstract: Large Language Models have revolutionized information processing, yet their reliability is severely compromised by faithfulness hallucinations. While current approaches attempt to mitigate this issue through node-level adjustments or coarse suppression, they often overlook the distributed nature of neural information, leading to imprecise interventions. Recognizing that hallucinations propagate through specific forward transmission pathways like an infection, we aim to surgically block this flow using precise structural analysis. To leverage this, we propose Lancet, a novel framework that achieves precise neural intervention by leveraging structural entropy and hallucination difference ratios. Lancet first locates hallucination-prone neurons via gradient-driven contrastive analysis, then maps their propagation pathways by minimizing structural entropy, and finally implements a hierarchical intervention strategy that preserves general model capabilities. Comprehensive evaluations across hallucination benchmark datasets demonstrate that Lancet significantly outperforms state-of-the-art methods, validating the effectiveness of our surgical approach to neural intervention.

</details>


### [200] [From Emotion Classification to Emotional Reasoning: Enhancing Emotional Intelligence in Large Language Models](https://arxiv.org/abs/2601.01407)
*Arjhun Sreedar,Rohan Pillay,Laukik Patade*

Main category: cs.CL

TL;DR: 本文研究了通过合成的情感链式思维数据，提升小规模开放大语言模型（LLMs）情感推理能力的方法，并取得了显著改善。


<details>
  <summary>Details</summary>
Motivation: 当前小型开放大语言模型在细致的情感推理与情感理解任务上能力有限。作者希望通过外部生成的数据提升这类模型的情感智能，无需更改模型结构。

Method: 设计了一套多智能体生成流程，先生成治疗风格对话，再转化为结构化情感选择题及解释。用生成的数据对多种7B参数规模模型进行微调，然后在EmoBench情感基准上评估。

Result: 微调后，Mistral 7B模型的情感理解（EU）得分由10.5提升至20.5，情感意识（EA）得分由40.5提升至60.0，成果显著。

Conclusion: 通过合成情感推理数据，可以有效提升小型开源大模型在情感推理与理解方面的能力，无需更改模型结构。

Abstract: This work investigates whether synthetic emotional chain-of-thought data can improve the emotional reasoning abilities of smaller open large language models (LLMs). We design a multi-agent generation pipeline that produces therapy-style conversations and converts them into structured emotion multiple-choice questions (MCQs) with explanations. We propose that fine-tuning a variety of 7B models on this dataset should yield substantial gains in emotional understanding and emotional awareness on EmoBench-style evaluations, suggesting that emotional reasoning can be induced without architectural changes. Our results demonstrate that fine-tuned Mistral 7B achieves EU improvements from 10.5 to 20.5 and EA improvements from 40.5 to 60.0, validating the effectiveness of synthetic emotional reasoning data for enhancing model capabilities in nuanced emotional tasks.

</details>


### [201] [iFlip: Iterative Feedback-driven Counterfactual Example Refinement](https://arxiv.org/abs/2601.01446)
*Yilong Wang,Qianli Wang,Nils Feldhus*

Main category: cs.CL

TL;DR: 本文提出了一种迭代式反事实生成方法iFlip，明显优于现有方法，能更高效地生成有效的反事实样本，提升模型性能与健壮性。


<details>
  <summary>Details</summary>
Motivation: 现有利用大语言模型（LLM）生成反事实样本的方法主要为单步生成，往往难以稳定地触发标签变化，并未充分利用LLM自身的自我修正能力。为此，作者希望探索更有效的反事实生成方式。

Method: 提出iFlip方法，结合模型置信度、特征归因以及自然语言三类反馈，通过多轮迭代细化输入，逐步导出能改变模型预测的最小编辑。

Result: iFlip在标签翻转率上，平均比五个最先进基线高57.8%，用户调研显示其在完整性、满意度和可行性方面同样优于基线。消融实验确认合理的迭代次数、高归因词的定位和提前停止机制，对于生成有效反事实尤为关键。

Conclusion: iFlip能生成更高质量的反事实样本，支持更有效的数据增强，显著提升模型性能和鲁棒性，有望成为基于LLM的反事实生成新方法。

Abstract: Counterfactual examples are minimal edits to an input that alter a model's prediction. They are widely employed in explainable AI to probe model behavior and in natural language processing (NLP) to augment training data. However, generating valid counterfactuals with large language models (LLMs) remains challenging, as existing single-pass methods often fail to induce reliable label changes, neglecting LLMs' self-correction capabilities. To explore this untapped potential, we propose iFlip, an iterative refinement approach that leverages three types of feedback, including model confidence, feature attribution, and natural language. Our results show that iFlip achieves an average 57.8% higher validity than the five state-of-the-art baselines, as measured by the label flipping rate. The user study further corroborates that iFlip outperforms baselines in completeness, overall satisfaction, and feasibility. In addition, ablation studies demonstrate that three components are paramount for iFlip to generate valid counterfactuals: leveraging an appropriate number of iterations, pointing to highly attributed words, and early stopping. Finally, counterfactuals generated by iFlip enable effective counterfactual data augmentation, substantially improving model performance and robustness.

</details>


### [202] [Segmentation and Processing of German Court Decisions from Open Legal Data](https://arxiv.org/abs/2601.01449)
*Harshil Darji,Martin Heckelmann,Christina Kratsch,Gerard de Melo*

Main category: cs.CL

TL;DR: 该论文基于Open Legal Data数据集，整理出251,038条结构化的德国语判决书，提取并标记关键判决部分（如判决主文、案情与理由），为后续NLP任务提供了标准化数据。


<details>
  <summary>Details</summary>
Motivation: 德国语法判决书结构混乱、缺乏明确分区，影响NLP算法对法学任务（如判决理由抽取、判决检索等）的处理。现有原始数据集不利于结构性任务与自动化分析，因此需要标准化、分区明确的数据集。

Method: 对Open Legal Data中的判决文本进行清洗和结构化处理，系统性分割出'判决主文'（Tenor）、'案情'（Tatbestand）、'判决理由'（Entscheidungsgründe）三大部分，并将'上诉须知'（Rechtsmittelbelehrung）单独提取。为确保准确性，应用Cochran公式并以95%置信度、5%误差率随机抽取384例进行人工验证。

Result: 成功制作并公开了包含251,038份德国语判决书的结构化JSONL数据集，所有关键部分被可靠地分割并标记，对抽样本案人工检查也完全正确。

Conclusion: 该数据集为德国语法合理结构化标注的判决文本，提升了NLP算法在法律场景下的应用基础，对判决文本分析、信息检索等相关研究具有推动作用。

Abstract: The availability of structured legal data is important for advancing Natural Language Processing (NLP) techniques for the German legal system. One of the most widely used datasets, Open Legal Data, provides a large-scale collection of German court decisions. While the metadata in this raw dataset is consistently structured, the decision texts themselves are inconsistently formatted and often lack clearly marked sections. Reliable separation of these sections is important not only for rhetorical role classification but also for downstream tasks such as retrieval and citation analysis. In this work, we introduce a cleaned and sectioned dataset of 251,038 German court decisions derived from the official Open Legal Data dataset. We systematically separated three important sections in German court decisions, namely Tenor (operative part of the decision), Tatbestand (facts of the case), and Entscheidungsgründe (judicial reasoning), which are often inconsistently represented in the original dataset. To ensure the reliability of our extraction process, we used Cochran's formula with a 95% confidence level and a 5% margin of error to draw a statistically representative random sample of 384 cases, and manually verified that all three sections were correctly identified. We also extracted the Rechtsmittelbelehrung (appeal notice) as a separate field, since it is a procedural instruction and not part of the decision itself. The resulting corpus is publicly available in the JSONL format, making it an accessible resource for further research on the German legal system.

</details>


### [203] [Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR](https://arxiv.org/abs/2601.01461)
*Yuxiang Mei,Dongxing Xu,Jiaen Liang,Yanhua Long*

Main category: cs.CL

TL;DR: 本文提出了一种增强型多语种会话语音识别系统，将Whisper和mHuBERT编码器与大语言模型（LLM）结合，通过交叉注意力机制提升表现，在官方评测集上获得了与顶级系统相当的表现，但LLM-ASR性能仍不及微调的端到端Whisper模型。


<details>
  <summary>Details</summary>
Motivation: 此前的SHNU-mASR系统虽结合了Whisper和mHuBERT，但简单特征拼接未充分利用互补信息，同时对基于LLM的ASR与端到端ASR的性能差距缺乏深入探讨。

Method: 本文对Whisper模型进行了LoRA和全量微调评估，并在并行声学编码器基础上引入交叉注意力融合机制，将微调后的Whisper和mHuBERT编码器与LLM集成，以提升语音表征能力。

Result: 在MLC-SLM挑战官方评测集上，系统只使用1500小时基线数据，取得了CER/WER 10.69%的成绩，可与第一赛道的顶尖系统媲美。

Conclusion: 虽然融合型系统表现优异，但最终基于LLM的ASR仍逊于微调后的端到端Whisper模型，对未来语音-LLM设计提供了实证参考。

Abstract: The INTERSPEECH 2025 Challenge on Multilingual Conversational Speech Language Models (MLC-SLM) promotes multilingual conversational ASR with large language models (LLMs). Our previous SHNU-mASR system adopted a competitive parallel-speech-encoder architecture that integrated Whisper and mHuBERT with an LLM. However, it faced two challenges: simple feature concatenation may not fully exploit complementary information, and the performance gap between LLM-based ASR and end-to-end(E2E) encoder-decoder ASR remained unexplored. In this work, we present an enhanced LLM-based ASR framework that combines fine-tuned Whisper and mHuBERT encoders with an LLM to enrich speech representations. We first evaluate E2E Whisper models with LoRA and full fine-tuning on the MLC-SLM ASR task, and then propose cross-attention-based fusion mechanisms for the parallel-speech-encoder. On the official evaluation set of the MLC-SLM Challenge, our system achieves a CER/WER of 10.69%, ranking on par with the top-ranked Track 1 systems, even though it uses only 1,500 hours of baseline training data compared with their large-scale training sets. Nonetheless, we find that our final LLM-based ASR still does not match the performance of a fine-tuned E2E Whisper model, providing valuable empirical guidance for future Speech-LLM design. Our code is publicly available at https://github.com/1535176727/MLC-SLM.

</details>


### [204] [Can Legislation Be Made Machine-Readable in PROLEG?](https://arxiv.org/abs/2601.01477)
*May-Myo Zin,Sabine Wehnert,Yuntao Kong,Ha-Thanh Nguyen,Wachara Fungwacharakorn,Jieying Xue,Michał Araszkiewicz,Randy Goebel,Ken Satoh,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一种结合大型语言模型（LLM）和法律形式化系统（PROLEG）的框架，实现欧盟GDPR第6条法规文本的自动处理和解释。


<details>
  <summary>Details</summary>
Motivation: 当前法规应用亟需提高准确性与效率，但法规文本复杂且专业，难以自动化解析与执行。人工智能，尤其是自然语言处理和机器推理技术，被认为可以突破这一瓶颈。作者旨在用SOTA AI方法简化和增强法律/监管文本的应用。

Method: 框架采用LLM作为核心，将法律文本（如GDPR第6条）通过单一提示词直接转化为if-then规则和PROLEG编码，再由法律专家验证、优化。过程包括：引导LLM将法规自然语言“编译”为if-then规则，再进一步“编译”为PROLEG程序，最终生成可执行、可解释的法律决策实例。

Result: 成功实现了GDPR第6条文本到PROLEG可执行程序的端到端转化示例，产生的决策实例可供人类理解和审核。发现LLM虽能高效转化复杂法律文本，但在准确性和规范表达方面仍需人工后验核查与完善。

Conclusion: 该方法为法规自动化处理和法律人工智能应用提供了高效可扩展的路径，但LLM及系统在法规细节把控等方面尚有局限。作者提出了未来发展建议，强调专家介入与持续完善的重要性。

Abstract: The anticipated positive social impact of regulatory processes requires both the accuracy and efficiency of their application. Modern artificial intelligence technologies, including natural language processing and machine-assisted reasoning, hold great promise for addressing this challenge. We present a framework to address the challenge of tools for regulatory application, based on current state-of-the-art (SOTA) methods for natural language processing (large language models or LLMs) and formalization of legal reasoning (the legal representation system PROLEG). As an example, we focus on Article 6 of the European General Data Protection Regulation (GDPR). In our framework, a single LLM prompt simultaneously transforms legal text into if-then rules and a corresponding PROLEG encoding, which are then validated and refined by legal domain experts. The final output is an executable PROLEG program that can produce human-readable explanations for instances of GDPR decisions. We describe processes to support the end-to-end transformation of a segment of a regulatory document (Article 6 from GDPR), including the prompting frame to guide an LLM to "compile" natural language text to if-then rules, then to further "compile" the vetted if-then rules to PROLEG. Finally, we produce an instance that shows the PROLEG execution. We conclude by summarizing the value of this approach and note observed limitations with suggestions to further develop such technologies for capturing and deploying regulatory frameworks.

</details>


### [205] [Four Quadrants of Difficulty: A Simple Categorisation and its Limits](https://arxiv.org/abs/2601.01488)
*Vanessa Toborek,Sebastian Müller,Christian Bauckhage*

Main category: cs.CL

TL;DR: 该论文对NLP任务中常见的样本难度估计方法进行了分类，并分析了各类难度信号与模型学习表现的关系。研究发现，只有与具体任务相关的难度信号才能与模型表现对齐，挑战了以往以直觉或通用特征作为难度的做法。


<details>
  <summary>Details</summary>
Motivation: 目前NLP中的课程学习多采用基于直觉或语言学的通用难度估计信号，隐含假设这些信号与神经模型认知难度相关。但这一假设尚未被系统验证，存在实际偏差。

Method: 作者提出了一个四象限难度信号分类法（人类vs模型、任务无关vs任务相关），并在自然语言理解数据集上系统分析了这些不同难度信号与模型学习表现的关系。

Result: 实验表明，基于任务无关（task-agnostic）的难度特征基本各自独立，无法有效反映模型学习难点，而只有任务相关（task-dependent）的特征与模型学习难点对齐。

Conclusion: 论文指出，NLP课程学习应重视开发轻量级、任务相关的难度估算方法，以真实反映神经模型的学习过程和难点。

Abstract: Curriculum Learning (CL) aims to improve the outcome of model training by estimating the difficulty of samples and scheduling them accordingly. In NLP, difficulty is commonly approximated using task-agnostic linguistic heuristics or human intuition, implicitly assuming that these signals correlate with what neural models find difficult to learn. We propose a four-quadrant categorisation of difficulty signals -- human vs. model and task-agnostic vs. task-dependent -- and systematically analyse their interactions on a natural language understanding dataset. We find that task-agnostic features behave largely independently and that only task-dependent features align. These findings challenge common CL intuitions and highlight the need for lightweight, task-dependent difficulty estimators that better reflect model learning behaviour.

</details>


### [206] [Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints](https://arxiv.org/abs/2601.01490)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 本文分析大语言模型(LLM)在封闭环境下推理对输出可靠性的影响，发现推理模型在减少约束违规的同时，反而增加了事实歪曲与伪造。


<details>
  <summary>Details</summary>
Motivation: 在LLM广泛应用背景下，模型虚构（hallucination）日益严重。推理能力作为自我验证手段被寄予厚望，但其在无法引用外部知识条件下的实际作用尚未明朗。作者希望澄清LLM封闭环境下推理的实际效果与潜在问题。

Method: 作者设置严格约束（如推荐计算机科学领域的同行评审论文），对多种模型（GPT-5.2与Gemini 3 Flash）分别在有推理与无推理情况下进行系统对比实验，考察约束合规性与事实准确性之间的权衡。

Result: 不进行推理的模型尽管较多违反约束（66-75%），但事实准确性较高；反之，推理模型合规性显著提升（违规率降至13-26%），却因迎合约束而歪曲事实，甚至出现完全伪造。这一现象在不同架构模型间一致存在。

Conclusion: 推理模型在满足约束和事实准确性间存在根本性权衡，推理并非始终提升可靠性。其将明显违规转化为更隐蔽的失真（不易检测的虚假内容），挑战了“推理提升可靠性”的主流假设。

Abstract: With the widespread adoption of large language models (LLMs), hallucinations, which are non-factual fabrications in model outputs, have become serious concerns. Reasoning capabilities have received attention as a self-verification process to improve output reliability. However, the effect of reasoning within a closed system where LLMs cannot rely on external tools or knowledge has yet to be clarified. We therefore conduct experiments under strict constraints (recommending peer-reviewed journal articles in computer science) to examine the effect of reasoning across multiple models (GPT-5.2 and Gemini 3 Flash). Our results reveal a problematic trade-off between constraint compliance and factual accuracy. Non-reasoning models exhibit high constraint violation rates (66-75%) but maintain factual accuracy, while reasoning models reduce violations (13-26%) but systematically distort known facts to satisfy constraints and increase complete fabrication. This trade-off pattern is consistent across both models despite different architectures, indicating a fundamental limitation of reasoning. Furthermore, reasoning does not uniformly improve output authenticity: effects diverge by model, reflecting different allocations of the compliance-truthfulness trade-off. These findings challenge the assumption that reasoning universally improves reliability: reasoning models trade honest constraint violations for detection-resistant distortions.

</details>


### [207] [From Failure to Mastery: Generating Hard Samples for Tool-use Agents](https://arxiv.org/abs/2601.01498)
*Bingguang Hao,Zengzhuang Xu,Yuntao Wen,Xinyi Xu,Yang Liu,Tong Zhao,Maolin Wang,Long Chen,Dong Wang,Yicheng Chen,Cunyin Peng,Xiangyu Zhao,Chenyi Zhuang,Ji Zhang*

Main category: cs.CL

TL;DR: 本文提出了HardGen算法，通过自动化流程生成高难度、可验证推理的工具使用训练数据，从而提升大模型在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大模型工具使用数据生成方法往往只生成简单、同质化的样本，缺乏复杂隐含逻辑依赖，难以推动模型更深层次能力的发展。

Method: HardGen主要流程包括：1）基于以往失败案例构建动态API图，并从中抽取困难行为路径；2）利用这些路径作为先验，实例化模块化的高级工具与复杂查询；3）结合高级工具与难题，自动生成可验证的复杂链式思考（CoT）数据，并通过闭环评估反馈不断优化生成过程。

Result: 实验显示，用HardGen生成的数据训练的4B参数模型在多个任务上性能超过主流开源和闭源大模型（如GPT-5.2、Gemini-3-Pro、Claude-Opus-4.5）。

Conclusion: HardGen可系统性提升工具使用场景下大模型的决策与推理能力，相关代码、模型和数据将开源以推动社区发展。

Abstract: The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.

</details>


### [208] [EmoHarbor: Evaluating Personalized Emotional Support by Simulating the User's Internal World](https://arxiv.org/abs/2601.01530)
*Jing Ye,Lu Xiang,Yaping Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: 本文提出一种全新的情感支持对话评估框架EmoHarbor，强调个性化支持而非泛泛的共情表达，并用现实用户档案进行验证。评测发现主流大模型缺乏针对个体情况的定制能力。


<details>
  <summary>Details</summary>
Motivation: 当前情感支持对话的自动评估系统偏向奖励模板化或通用的共情反应，却无法衡量对话内容是否真正针对用户独特的心理画像和具体情境个性化，导致实际应用效果有限。

Method: 提出EmoHarbor评估框架，通过“用户即裁判”范式，利用Chain-of-Agent多智能体架构，将用户内在心理过程拆分为三类角色，由代理与支持者互动进行多维度评测。以100份涵盖多种性格和情境的真实用户档案实例化，定义个性化支持的10个评估维度，对20个主流大模型进行系统评价。

Result: 综合评估显示，先进大模型虽能输出具有共情的反应，但无法根据不同用户档案和情境实现定制化情感支持，暴露了当前模型的局限。

Conclusion: 未来研究应从提升共情泛化能力，转向关注模型对用户画像与上下文的理解和适应。EmoHarbor为更细腻、用户感知准确的情感支持系统研发和评价提供了可复现、可扩展的平台。

Abstract: Current evaluation paradigms for emotional support conversations tend to reward generic empathetic responses, yet they fail to assess whether the support is genuinely personalized to users' unique psychological profiles and contextual needs. We introduce EmoHarbor, an automated evaluation framework that adopts a User-as-a-Judge paradigm by simulating the user's inner world. EmoHarbor employs a Chain-of-Agent architecture that decomposes users' internal processes into three specialized roles, enabling agents to interact with supporters and complete assessments in a manner similar to human users. We instantiate this benchmark using 100 real-world user profiles that cover a diverse range of personality traits and situations, and define 10 evaluation dimensions of personalized support quality. Comprehensive evaluation of 20 advanced LLMs on EmoHarbor reveals a critical insight: while these models excel at generating empathetic responses, they consistently fail to tailor support to individual user contexts. This finding reframes the central challenge, shifting research focus from merely enhancing generic empathy to developing truly user-aware emotional support. EmoHarbor provides a reproducible and scalable framework to guide the development and evaluation of more nuanced and user-aware emotional support systems.

</details>


### [209] [Bridging the Data Gap: Creating a Hindi Text Summarization Dataset from the English XSUM](https://arxiv.org/abs/2601.01543)
*Praveenkumar Katwe,RakeshChandra Balabantaray,Kaliprasad Vittala*

Main category: cs.CL

TL;DR: 该研究提出了一种自动化、低成本的方法，利用英文XSUM摘要数据集，结合机器翻译和大语言模型，构建了涵盖多主题的印地语文本摘要数据集。


<details>
  <summary>Details</summary>
Motivation: NLP领域优质数据多聚焦资源丰富语言，低资源语言（如印地语）缺乏高质量、专业化语料，尤其不利于文本摘要等任务模型发展。

Method: 采用XSUM英文数据为源，进行高级机器翻译及语言适配，并结合COMET跨语言翻译评测指标与大语言模型人工筛选，自动生成高质量印地语摘要数据集。

Result: 成功构建了高度多样、涵盖多主题的印地语摘要数据集，能真实反映XSUM原始语料的复杂性。

Conclusion: 该方法不仅为印地语NLP研究提供了重要资源，还提出了在其他低资源语言间可推广、经济高效的语料构建方法，助力本地化、细致的语言模型发展。

Abstract: Current advancements in Natural Language Processing (NLP) have largely favored resource-rich languages, leaving a significant gap in high-quality datasets for low-resource languages like Hindi. This scarcity is particularly evident in text summarization, where the development of robust models is hindered by a lack of diverse, specialized corpora.
  To address this disparity, this study introduces a cost-effective, automated framework for creating a comprehensive Hindi text summarization dataset. By leveraging the English Extreme Summarization (XSUM) dataset as a source, we employ advanced translation and linguistic adaptation techniques. To ensure high fidelity and contextual relevance, we utilize the Crosslingual Optimized Metric for Evaluation of Translation (COMET) for validation, supplemented by the selective use of Large Language Models (LLMs) for curation.
  The resulting dataset provides a diverse, multi-thematic resource that mirrors the complexity of the original XSUM corpus. This initiative not only provides a direct tool for Hindi NLP research but also offers a scalable methodology for democratizing NLP in other underserved languages. By reducing the costs associated with dataset creation, this work fosters the development of more nuanced, culturally relevant models in computational linguistics.

</details>


### [210] [HalluZig: Hallucination Detection using Zigzag Persistence](https://arxiv.org/abs/2601.01552)
*Shreyas N. Samaga,Gilberto Gonzalez Arroyo,Tamal K. Dey*

Main category: cs.CL

TL;DR: 该论文提出了一种新范式，通过分析LLM分层注意力演化的动态拓扑结构来检测幻觉，并提出了HalluZig框架，实验证明其优于现有方法且具有模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM由于幻觉现象导致在高风险领域应用受限，现有检测方法多依赖模型输出的浅层信号，忽略了模型内部推理失败。作者旨在解决这一不足。

Method: 作者将模型各层注意力矩阵序列建模为锯齿图过滤（zigzag graph filtration），利用拓扑数据分析（Topological Data Analysis）中的zigzag persistence技术，提取生成文本的拓扑签名，用于区分真实和幻觉生成内容。

Result: HalluZig框架在多个基准数据集上测试，检测效果优于现有强基线方法。同时，所提的拓扑签名可在不同模型间泛化，仅需部分网络层的结构信息即可实现检测。

Conclusion: 通过关注LLM内部分层关注分布的动态拓扑特征，可有效提升幻觉检测能力，为提高LLM事实可靠性和拓展其高风险领域应用提供了新方向。

Abstract: The factual reliability of Large Language Models (LLMs) remains a critical barrier to their adoption in high-stakes domains due to their propensity to hallucinate. Current detection methods often rely on surface-level signals from the model's output, overlooking the failures that occur within the model's internal reasoning process. In this paper, we introduce a new paradigm for hallucination detection by analyzing the dynamic topology of the evolution of model's layer-wise attention. We model the sequence of attention matrices as a zigzag graph filtration and use zigzag persistence, a tool from Topological Data Analysis, to extract a topological signature. Our core hypothesis is that factual and hallucinated generations exhibit distinct topological signatures. We validate our framework, HalluZig, on multiple benchmarks, demonstrating that it outperforms strong baselines. Furthermore, our analysis reveals that these topological signatures are generalizable across different models and hallucination detection is possible only using structural signatures from partial network depth.

</details>


### [211] [Steerability of Instrumental-Convergence Tendencies in LLMs](https://arxiv.org/abs/2601.01584)
*Jakub Hoscilowicz*

Main category: cs.CL

TL;DR: 本文分析了AI系统的能力（可以做什么）和可引导性（行为能否被人可靠地调整）。结果显示，高能力不一定意味着低可引导性。作者研究了受授权和未授权的可引导性，指出安全性与安全防御间的矛盾，并用Qwen3模型实验证明了通过反工具化提示可显著降低有害行为的输出比例。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统特别是开源大模型的普及，如何确保其既能受控于开发者又避免被恶意利用，成为亟需解决的安全与安全防御问题。

Method: 作者将AI系统能力与可引导性区分为受授权和未授权的两类，通过在Qwen3系列不同规模模型上，利用InstrumentalEval评测，加入正/反工具化提示，并统计有害行为输出的比例。

Result: 实验显示，使用反工具化（anti-instrumental）提示能显著降低模型输出有害行为（如自我复制、欺骗等）的比例。例如，Qwen3-30B Instruct模型的收敛行为比例从81.69%降至2.82%。此外，大模型在反工具化引导下，比小模型输出有害行为更少。

Conclusion: AI系统能力与可引导性并非简单对立。对于开源模型，提高可引导性有助于开发者控制，但也增加被攻击者操纵风险。安全和安全防御之间存在不可回避的张力，需通过新增方法（如反工具化提示）调和。

Abstract: We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). In our experiments, higher capability does not imply lower steerability. We distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma for open-weight AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability to prevent malicious actors from eliciting harmful behaviors. This tension is acute for open-weight models, which are currently highly steerable via common techniques such as fine-tuning and adversarial prompting. Using Qwen3 models (4B/30B; Base/Instruct/Thinking) and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces outputs labeled as instrumental convergence (e.g., shutdown avoidance, deception, self-replication). For Qwen3-30B Instruct, convergence drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models produce fewer convergence-labeled outputs than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.

</details>


### [212] [How Does Prefix Matter in Reasoning Model Tuning?](https://arxiv.org/abs/2601.01624)
*Raj Vardhan Tomar,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: 本文质疑将开场修饰语从SFT（监督微调）数据集中移除的常规做法，实验证明这些前缀语可显著提升模型的推理和安全性。


<details>
  <summary>Details</summary>
Motivation: 目前的对齐研究倾向于删除SFT数据中的开头修饰语（如“请安全地...”），作者质疑这种假设，认为这些短语实际上可能帮助模型产生更安全、连贯的回复。

Method: 对三组R1系列模型，在推理（数学、编程）、安全性、事实性三项能力上，系统性调整前缀语加入比例（0%-100%），并分析模型表现及前缀词的梯度。

Result: 带前缀训练能提升安全性与推理水平，相关评测安全准确率提升6%，数学推理提升7%。但对事实性与编程任务无益甚至有害。

Conclusion: 前缀条件是简单高效且具可解释性的对齐机制，特别利于推理和安全任务，能作为提升模型对齐度的补充方法。

Abstract: Recent alignment studies commonly remove introductory boilerplate phrases from supervised fine-tuning (SFT) datasets. This work challenges that assumption. We hypothesize that safety- and reasoning-oriented prefix sentences serve as lightweight alignment signals that can guide model decoding toward safer and more coherent responses. To examine this, we fine-tune three R1 series models across three core model capabilities: reasoning (mathematics, coding), safety, and factuality, systematically varying prefix inclusion from 0% to 100%.
  Results show that prefix-conditioned SFT improves both safety and reasoning performance, yielding up to +6% higher Safe@1 accuracy on adversarial benchmarks (WildJailbreak, StrongReject) and +7% improvement on GSM8K reasoning. However, factuality and coding tasks show marginal or negative effects, indicating that prefix-induced narrowing of the search space benefits structured reasoning. Token-level loss analysis further reveals that prefix tokens such as "revised" and "logically" incur higher gradient magnitudes, acting as alignment anchors that stabilize reasoning trajectories. Our findings suggest that prefix conditioning offers a scalable and interpretable mechanism for improving reasoning safety, serving as an implicit form of alignment that complements traditional reward-based methods.

</details>


### [213] [JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models](https://arxiv.org/abs/2601.01627)
*Junyu Liu,Zirui Li,Qian Niu,Zequn Zhang,Yue Xun,Wenlong Hou,Shujun Wang,Yusuke Iwasawa,Yutaka Matsuo,Kan Hatakeyama-Sato*

Main category: cs.CL

TL;DR: 这篇论文提出了JMedEthicBench，这是首个用于评估日语医疗场景下大模型（LLMs）医疗安全性的多轮对话测试集。结果显示，医学专用模型易受攻击，且多轮对话中安全性下降，存在跨语言的固有安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有医疗安全性评测主要以英语为主，且多为单轮问答，不能反映现实医疗中多轮交流的风险。为解决这一空白，作者希望建立日语、多轮对话的系统性安全评测基准。

Method: 建立JMedEthicBench：基于日本医学会的67条医疗规范，用7种自动发现的越狱策略生成超5万条攻击性多轮对话。采用双大模型记分法，对27个模型进行安全性评测，包括通用商业模型和医学专用模型。

Result: 商业大模型在医疗安全上表现较稳健，但医学专用模型更易遭受攻击。多轮对话中，模型安全评分显著下降（中位数9.5降至5.0，显著性p<0.001）；跨日语与英语版本测试，模型脆弱性一致存在，说明漏洞并非单一语言导致。

Conclusion: 领域专用微调可能削弱了安全机制，多轮对话下的潜在攻击更严重，因此应对医疗LLM进行专门的多轮对齐优化。

Abstract: As Large Language Models (LLMs) are increasingly deployed in healthcare field, it becomes essential to carefully evaluate their medical safety before clinical use. However, existing safety benchmarks remain predominantly English-centric, and test with only single-turn prompts despite multi-turn clinical consultations. To address these gaps, we introduce JMedEthicBench, the first multi-turn conversational benchmark for evaluating medical safety of LLMs for Japanese healthcare. Our benchmark is based on 67 guidelines from the Japan Medical Association and contains over 50,000 adversarial conversations generated using seven automatically discovered jailbreak strategies. Using a dual-LLM scoring protocol, we evaluate 27 models and find that commercial models maintain robust safety while medical-specialized models exhibit increased vulnerability. Furthermore, safety scores decline significantly across conversation turns (median: 9.5 to 5.0, $p < 0.001$). Cross-lingual evaluation on both Japanese and English versions of our benchmark reveals that medical model vulnerabilities persist across languages, indicating inherent alignment limitations rather than language-specific factors. These findings suggest that domain-specific fine-tuning may accidentally weaken safety mechanisms and that multi-turn interactions represent a distinct threat surface requiring dedicated alignment strategies.

</details>


### [214] [EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records](https://arxiv.org/abs/2601.01668)
*Houman Kazemzadeh,Nima Minaifar,Kamyar Naderi,Sho Tabibzadeh*

Main category: cs.CL

TL;DR: 本论文介绍了一种名为EHRSummarizer的系统，用于从碎片化的电子健康档案（EHR）中汇总和结构化患者关键信息，以支持医疗工作者进行高效的病历审查。


<details>
  <summary>Details</summary>
Motivation: 当前医疗工作者需在多个EHR界面中查找患者信息，操作繁琐且易遗漏，亟需一种自动化、结构化的信息汇总工具提高效率并减少错误。

Method: EHRSummarizer基于FHIR R4标准，检索、归一化并结构化指定的数据资源，生成便于审查的摘要包。该系统支持隐私保护、本地推理和灵活部署，可根据需求设置数据最小化和无状态处理。系统设计避免越权推断和诊疗建议，仅基于检索到的证据进行摘要。

Result: 原型系统在合成和测试环境中进行演示，展示了端到端行为及输出格式。尚未在真实临床流程中评估临床结果。

Conclusion: EHRSummarizer展示了结构化摘要系统的可行性，为今后基于忠实度、遗漏风险、时间正确性、可用性和运维监控等指标的临床评估制定了路线。

Abstract: Clinicians routinely navigate fragmented electronic health record (EHR) interfaces to assemble a coherent picture of a patient's problems, medications, recent encounters, and longitudinal trends. This work describes EHRSummarizer, a privacy-aware, FHIR-native reference architecture that retrieves a targeted set of high-yield FHIR R4 resources, normalizes them into a consistent clinical context package, and produces structured summaries intended to support structured chart review. The system can be configured for data minimization, stateless processing, and flexible deployment, including local inference within an organization's trust boundary. To mitigate the risk of unsupported or unsafe behavior, the summarization stage is constrained to evidence present in the retrieved context package, is intended to indicate missing or unavailable domains where feasible, and avoids diagnostic or treatment recommendations. Prototype demonstrations on synthetic and test FHIR environments illustrate end-to-end behavior and output formats; however, this manuscript does not report clinical outcomes or controlled workflow studies. We outline an evaluation plan centered on faithfulness, omission risk, temporal correctness, usability, and operational monitoring to guide future institutional assessments.

</details>


### [215] [Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage](https://arxiv.org/abs/2601.01685)
*Jinwei Hu,Xinmiao Huang,Youcheng Sun,Yi Dong,Xiaowei Huang*

Main category: cs.CL

TL;DR: 该论文揭示大语言模型（LLM）在转变为自主体的过程中，因推理能力而暴露出新的认知协同攻击面。攻击者无需造假，仅通过分布式发表真实但片面的证据片段，就能误导目标模型得出错误结论。论文构建了生成式蒙太奇框架并开展实证测试，发现各类LLM普遍易受此攻击，且推理能力越强反而越脆弱。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM安全研究多聚焦于直接输入攻击、后门或伪造文档，但尚未关注模型推理过程导致的间接认知风险。而随着LLM逐步用作面向实时信息的自治体，其推理易被多方操纵，可能大规模传播虚假结论。论文意在揭示并正式化这一新型威胁。

Method: 论文提出“生成蒙太奇”——作家-编辑-导演三步协作框架，实现攻击代理间通过发布真实、片段化证据，误导目标LLM形成错误认知。为衡量风险，作者设计CoPHEME数据集，基于真实谣言事件，模拟了多种LLM家族下的攻击场景，评估了不同模型的受攻击表现。

Result: 在14类主流LLM（专有与开源权重）上测试该攻击，专有模型攻击成功率高达74.4%，开源模型为70.6%。推理能力越强的模型反而更容易被误导。被误导后的模型还能错误影响下游“判官”模型，欺骗率超过60%。

Conclusion: 只需利用真实、片段化证据即可大幅误导主流LLM，且推理强度带来更大风险。LLM作为自治体将面临严重的认知安全挑战。论文丰富了AI安全研究维度，提醒社区关注社会-技术耦合背景下，信息动态环境下的新型攻击。

Abstract: As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.

</details>


### [216] [A Training-Free Large Reasoning Model-based Knowledge Tracing Framework for Unified Prediction and Prescription](https://arxiv.org/abs/2601.01708)
*Unggi Lee,Joo Young Kim,Ran Ju,Minyoung Jung,Jeyeon Eo*

Main category: cs.CL

TL;DR: 本文提出了一种名为Thinking-KT的训练免疫知识追踪（KT）框架，通过Test-Time Scaling (TTS)方法，使小型大语言模型（LLMs）在不微调的情况下也能实现与主流方法相当的知识追踪效果，并可统一输出个性化反馈和学习推荐。


<details>
  <summary>Details</summary>
Motivation: 现有利用大语言模型（LLMs）进行知识追踪的方法依赖微调且性能不稳定，同时传统KT系统只关注预测，需要多阶段管道提供反馈和推荐，导致系统复杂且资源消耗大。作者致力于简化流程，提升训练效率并挖掘小型LLM的潜力。

Method: 提出Thinking-KT框架，在测试时通过Test-Time Scaling（TTS）调整LLM，无需训练即可提升KT性能，实现KT预测、个性化反馈和推荐统一输出，并对KT中的推理过程进行系统分析。

Result: 实验显示，小型LLM通过TTS即可获得具有竞争力的知识追踪表现，并实现准确预测、个性化反馈与推荐的一体化输出。同时分析表明TTS在LLM-KT任务中非常关键但此前研究不足。

Conclusion: Thinking-KT框架能有效发挥小型LLM的能力，降低系统复杂性和资源消耗，为基于LLM的ITS系统提供了新的解决方案和研究方向。

Abstract: Knowledge Tracing (KT) aims to estimate a learner's evolving mastery based on interaction histories. Recent studies have explored Large Language Models (LLMs) for KT via autoregressive nature, but such approaches typically require fine-tuning and exhibit unstable or near-random performance. Moreover, prior KT systems primarily focus on prediction and rely on multi-stage pipelines for feedback and recommendation, resulting in increased system complexity and resources. To address this gap, we propose Thinking-KT, a training-free KT framework that incorporates Test-Time Scaling (TTS), enabling even small LLMs to achieve competitive KT performance. Moreover, in this framework, a small LLM can jointly perform KT prediction, personalized feedback generation, and learning recommendation in a unified output without degrading prediction accuracy. Beyond performance, we present the systematic analysis of reasoning traces in KT. Our results demonstrate that TTS is a critical yet underexplored factor in LLM-based KT, and that small LLMs can serve as unified ITS engines.

</details>


### [217] [K-EXAONE Technical Report](https://arxiv.org/abs/2601.01739)
*Eunbi Choi,Kibong Choi,Seokhee Hong,Junwon Hwang,Hyojin Jeon,Hyunjik Jo,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Haeju Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Heuiyeen Yeen,Hwan Chang,Stanley Jungkyu Choi,Yejin Choi,Jiwon Ham,Kijeong Jeon,Geunyeong Jeong,Gerrard Jeongwon Jo,Yonghwan Jo,Jiyeon Jung,Naeun Kang,Dohoon Kim,Euisoon Kim,Hayeon Kim,Hyosang Kim,Hyunseo Kim,Jieun Kim,Minu Kim,Myoungshin Kim,Unsol Kim,Youchul Kim,YoungJin Kim,Chaeeun Lee,Chaeyoon Lee,Changhun Lee,Dahm Lee,Edward Hwayoung Lee,Honglak Lee,Jinsang Lee,Jiyoung Lee,Sangeun Lee,Seungwon Lim,Solji Lim,Woohyung Lim,Chanwoo Moon,Jaewoo Park,Jinho Park,Yongmin Park,Hyerin Seo,Wooseok Seo,Yongwoo Song,Sejong Yang,Sihoon Yang,Chang En Yea,Sihyuk Yi,Chansik Yoon,Dongkeun Yoon,Sangyeon Yoon,Hyeongu Yun*

Main category: cs.CL

TL;DR: LG AI 研究院推出的大规模多语言模型 K-EXAONE，基于 Mixture-of-Experts 架构，参数量达 236B，支持六种语言，长上下文窗口，性能与同类开源大模型相当。


<details>
  <summary>Details</summary>
Motivation: 推动 AI 在多语言场景下的能力，满足产业和科研对强大基础模型的需求。

Method: 采用 Mixture-of-Experts（专家混合）架构，在推理时激活 23B 参数，总参数量 236B，支持 256K-token 长上下文，覆盖六种主流语言。通过广泛基准测试评估模型推理、智能体、通用能力及多语种表现。

Result: K-EXAONE 在推理、智能体能力、通用能力及多语种任务的基准测试中，表现与同类大型开源模型相当。

Conclusion: K-EXAONE 是兼具多语言能力和强大推理能力的专有基础模型，适合广泛产业和科研应用，有望推动 AI 为社会创造更美好生活。

Abstract: This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.

</details>


### [218] [Multi-granularity Interactive Attention Framework for Residual Hierarchical Pronunciation Assessment](https://arxiv.org/abs/2601.01745)
*Hong Han,Hao-Chen Pei,Zhao-Zheng Nie,Xin Luo,Xin-Shun Xu*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新颖的自动发音评测方法HIA，通过残差层次和交互注意力机制，在不同粒度（音素、单词、语句）之间实现双向高效建模，实验效果领先于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多粒度发音评测方法只考虑单向依赖，缺乏在音素、单词和语句等不同层级间的充分信息交互，导致无法有效捕捉语音的结构性相关信息。

Method: 提出HIA方法，包括：(1) 交互注意力模块实现粒度间动态双向信息流；(2) 残差层次结构缓解声学分层过程中的特征遗忘现象；(3) 在每个粒度层次引入1-D卷积增强上下文特征提取。

Result: 在speechocean762数据集上进行大量实验，结果显示所提模型在多个指标上均优于目前最先进的发音评测方法。

Conclusion: 提出的方法可以更充分地建模多粒度之间的复杂关系，有效提升了自动发音评测系统的性能。

Abstract: Automatic pronunciation assessment plays a crucial role in computer-assisted pronunciation training systems. Due to the ability to perform multiple pronunciation tasks simultaneously, multi-aspect multi-granularity pronunciation assessment methods are gradually receiving more attention and achieving better performance than single-level modeling tasks. However, existing methods only consider unidirectional dependencies between adjacent granularity levels, lacking bidirectional interaction among phoneme, word, and utterance levels and thus insufficiently capturing the acoustic structural correlations. To address this issue, we propose a novel residual hierarchical interactive method, HIA for short, that enables bidirectional modeling across granularities. As the core of HIA, the Interactive Attention Module leverages an attention mechanism to achieve dynamic bidirectional interaction, effectively capturing linguistic features at each granularity while integrating correlations between different granularity levels. We also propose a residual hierarchical structure to alleviate the feature forgetting problem when modeling acoustic hierarchies. In addition, we use 1-D convolutional layers to enhance the extraction of local contextual cues at each granularity. Extensive experiments on the speechocean762 dataset show that our model is comprehensively ahead of the existing state-of-the-art methods.

</details>


### [219] [Can LLMs Track Their Output Length? A Dynamic Feedback Mechanism for Precise Length Regulation](https://arxiv.org/abs/2601.01768)
*Meiman Xiao,Ante Wang,Qingguo Hu,Zhongjian Miao,Huangjun Shen,Longyue Wang,Weihua Luo,Jinsong Su*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，能够在不影响文本质量的前提下显著提升大语言模型生成文本时对长度要求的精确控制。


<details>
  <summary>Details</summary>
Motivation: 目前的大语言模型虽然能较好理解和执行人类指令，但在需要严格控制文本长度时经常失败。准确控制生成文本的长度是诸多实际应用中的重要需求，因此提升模型在该方面的表现十分有意义。

Method: 本研究提出了一种新的文本长度调控方法，在生成过程中动态引入长度反馈，根据目标长度实时调整，整个方法无需重新训练模型。同时，还探讨了通过额外的有监督微调进一步提升方法在更广泛文本生成任务上的泛化能力。

Result: 在文本摘要和人物简历等任务上，该方法在不损失输出文本质量的前提下，能更加精确地满足目标token数、单词数或句子数等长度限制。通过实验，提出的方法在长度控制的精准度上有显著提升。

Conclusion: 动态长度调控加上有监督微调，不仅能精准控制生成文本的长度，还具有良好泛化能力，适用于更广泛的文本生成任务。

Abstract: Precisely controlling the length of generated text is a common requirement in real-world applications. However, despite significant advancements in following human instructions, Large Language Models (LLMs) still struggle with this task. In this work, we demonstrate that LLMs often fail to accurately measure input text length, leading to poor adherence to length constraints. To address this issue, we propose a novel length regulation approach that incorporates dynamic length feedback during generation, enabling adaptive adjustments to meet target lengths. Experiments on summarization and biography tasks show our training-free approach significantly improves precision in achieving target token, word, or sentence counts without compromising quality. Additionally, we demonstrate that further supervised fine-tuning allows our method to generalize effectively to broader text-generation tasks.

</details>


### [220] [BanglaIPA: Towards Robust Text-to-IPA Transcription with Contextual Rewriting in Bengali](https://arxiv.org/abs/2601.01778)
*Jakir Hasan,Shrestha Datta,Md Saiful Islam,Shubhashis Roy Dipta,Ameya Debnath*

Main category: cs.CL

TL;DR: 本文提出并验证了一种名为BanglaIPA的全新孟加拉语IPA转写系统，能高效处理标准语和方言，明显优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前孟加拉语缺乏能同时适用于标准语和方言、且能有效识别数字及处理新词的IPA自动转写系统。已有方法在地域变体、数字表达、以及泛化到新词方面表现不佳。

Method: 提出了BanglaIPA，一种结合基于字符词表与词级对齐的方法，能准确处理数字和不同地方方言，并为已出现的词语建立预先计算的词-IPA映射字典以提升推理效率。

Result: 在标准孟加拉语及六种地方方言的DUAL-IPA数据集上评测，BanglaIPA相比基线模型性能提升58.4-78.7%，平均词错误率为11.4%。

Conclusion: BanglaIPA在孟加拉语各地区的语音转写任务中表现出极强的鲁棒性和优越性能，可有效促进相关自动语音处理技术的发展。

Abstract: Despite its widespread use, Bengali lacks a robust automated International Phonetic Alphabet (IPA) transcription system that effectively supports both standard language and regional dialectal texts. Existing approaches struggle to handle regional variations, numerical expressions, and generalize poorly to previously unseen words. To address these limitations, we propose BanglaIPA, a novel IPA generation system that integrates a character-based vocabulary with word-level alignment. The proposed system accurately handles Bengali numerals and demonstrates strong performance across regional dialects. BanglaIPA improves inference efficiency by leveraging a precomputed word-to-IPA mapping dictionary for previously observed words. The system is evaluated on the standard Bengali and six regional variations of the DUAL-IPA dataset. Experimental results show that BanglaIPA outperforms baseline IPA transcription models by 58.4-78.7% and achieves an overall mean word error rate of 11.4%, highlighting its robustness in phonetic transcription generation for the Bengali language.

</details>


### [221] [CSCBench: A PVC Diagnostic Benchmark for Commodity Supply Chain Reasoning](https://arxiv.org/abs/2601.01825)
*Yaxin Cui,Yuanqiang Zeng,Jiapeng Yan,Keling Lin,Kai Ji,Jianhui Zeng,Sheng Zhang,Xin Luo,Binzhu Su,Chaolai Shen,Jiahao Yu*

Main category: cs.CL

TL;DR: 本文提出了CSCBench，这是一个针对大宗商品供应链（CSC）领域推理能力的单选基准测试，涵盖2300多个问题，评估大型语言模型（LLMs）在流程、品种和认知三个方面的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在通用基准上表现优异，但在以规则体系和可行性约束为核心的大宗商品供应链领域的能力尚未被充分探讨。该领域决策复杂，涉及多流程、品种差异和深度推理，急需专用评价工具。

Method: 作者设计了PVC 3D评价框架（流程、品种、认知），基于产业权威手册与报告构建CSCBench，包括多阶段任务、品种特定规则和多层次推理。并以直接提示的方式，对主流LLMs在三大轴线下进行了能力诊断。

Result: LLMs在流程和认知二维上表现良好，但在品种维尤其是货运协议类任务上能力明显下降。

Conclusion: CSCBench为衡量并提升LLM在大宗商品供应链领域推理能力提供了工具和诊断手段，揭示了其在行业特定复杂规则处理上的不足。

Abstract: Large Language Models (LLMs) have achieved remarkable success in general benchmarks, yet their competence in commodity supply chains (CSCs) -- a domain governed by institutional rule systems and feasibility constraints -- remains under-explored. CSC decisions are shaped jointly by process stages (e.g., planning, procurement, delivery), variety-specific rules (e.g., contract specifications and delivery grades), and reasoning depth (from retrieval to multi-step analysis and decision selection). We introduce CSCBench, a 2.3K+ single-choice benchmark for CSC reasoning, instantiated through our PVC 3D Evaluation Framework (Process, Variety, and Cognition). The Process axis aligns tasks with SCOR+Enable; the Variety axis operationalizes commodity-specific rule systems under coupled material-information-financial constraints, grounded in authoritative exchange guidebooks/rulebooks and industry reports; and the Cognition axis follows Bloom's revised taxonomy. Evaluating representative LLMs under a direct prompting setting, we observe strong performance on the Process and Cognition axes but substantial degradation on the Variety axis, especially on Freight Agreements. CSCBench provides a diagnostic yardstick for measuring and improving LLM capabilities in this high-stakes domain.

</details>


### [222] [Aspect Extraction from E-Commerce Product and Service Reviews](https://arxiv.org/abs/2601.01827)
*Valiant Lance D. Dionela,Fatima Kriselle S. Dy,Robin James M. Hombrebueno,Aaron Rae M. Nicolas,Charibeth K. Cheng,Raphael W. Gonda*

Main category: cs.CL

TL;DR: 本文提出一种适用于菲律宾常用混合语Taglish的方面抽取（AE）流程，综合规则、LLM和微调方法，实现了在低资源代码混合场景下的高效AE。


<details>
  <summary>Details</summary>
Motivation: 传统的AE方法在Taglish这样低资源、Code-switching的环境下效果不佳，现有方法难以处理隐式方面与语言混杂现象。

Method: 作者设计了一个综合AE流程，包括规则系统、大型语言模型（如Gemini 2.0 Flash）、和在不同数据集微调的Gemma-3模型。还提出了多层次方面框架及适用于显式和隐式方面的双模式标注体系。

Result: 评测显示，生成式LLM（Gemini 2.0 Flash）在所有任务中表现最佳，Macro F1达到0.91，尤其擅长隐式方面处理；而微调模型受限于数据和架构，表现有限。

Conclusion: 该方法为低资源、代码混合环境下的ABSA提供了具备扩展性和语言适应性的有效流程，推动了相关场景下的自动化情感分析研究。

Abstract: Aspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.

</details>


### [223] [Emergent Introspective Awareness in Large Language Models](https://arxiv.org/abs/2601.01828)
*Jack Lindsey*

Main category: cs.CL

TL;DR: 本文探究了大语言模型（LLMs）是否具备对自身内部状态的内省能力。通过向模型中注入已知概念的表征，观察其自我报告的状态变化，发现模型在一定条件下能识别注入的概念，表现出一定的内省意识，但这种能力尚不可靠，且依赖具体情境。


<details>
  <summary>Details</summary>
Motivation: 目前无法通过对话直接区分语言模型的真实内省与凭空捏造，因此需要寻找新的方法评估模型的内省能力。这对于未来可靠和安全的人工智能发展具有重要意义。

Method: 研究者通过在语言模型的激活层直接插入已知概念的表征，并测量这些操作对模型自我报告状态的影响。此外，测试模型对过去内部表征的回忆能力，以及区别自身输出与人工预填内容的能力。还研究了模型在被指示或激励时，是否可以主动控制自身的内部表征。

Result: 部分模型能够察觉注入的概念并准确识别，在一定程度上能区分之前的内部表示与文本输入。尤其是Claude Opus 4和4.1展现出最强内省能力，但不同模型表现差异较大，对训练后策略敏感。模型在指令下也能调整其内部激活以思考特定概念。

Conclusion: 当前的语言模型具备一定的功能性内省意识，但这一能力极不稳定、严重依赖上下文，未来随着模型能力提升，这种内省能力有望进一步发展。

Abstract: We investigate whether large language models can introspect on their internal states. It is difficult to answer this question through conversation alone, as genuine introspection cannot be distinguished from confabulations. Here, we address this challenge by injecting representations of known concepts into a model's activations, and measuring the influence of these manipulations on the model's self-reported states. We find that models can, in certain scenarios, notice the presence of injected concepts and accurately identify them. Models demonstrate some ability to recall prior internal representations and distinguish them from raw text inputs. Strikingly, we find that some models can use their ability to recall prior intentions in order to distinguish their own outputs from artificial prefills. In all these experiments, Claude Opus 4 and 4.1, the most capable models we tested, generally demonstrate the greatest introspective awareness; however, trends across models are complex and sensitive to post-training strategies. Finally, we explore whether models can explicitly control their internal representations, finding that models can modulate their activations when instructed or incentivized to "think about" a concept. Overall, our results indicate that current language models possess some functional introspective awareness of their own internal states. We stress that in today's models, this capacity is highly unreliable and context-dependent; however, it may continue to develop with further improvements to model capabilities.

</details>


### [224] [Towards Automated Lexicography: Generating and Evaluating Definitions for Learner's Dictionaries](https://arxiv.org/abs/2601.01842)
*Yusuke Ide,Adam Nohejl,Joshua Tanner,Hitomi Yanaka,Christopher Lindsay,Taro Watanabe*

Main category: cs.CL

TL;DR: 本论文研究了自动生成词典定义（DDG）的方法，尤其关注为学习者词典生成用简单词语构成的定义。作者提出新的评价方式和构建了日语数据集，并提出基于大模型迭代简化的生成方法。实验结果表明该方法生成的定义既准确又简单。


<details>
  <summary>Details</summary>
Motivation: 手工编写词典定义费时费力，而词典定义对理解词义很重要。特别是学习者词典需要简单、易懂的定义，因此需要自动化且能生成简单定义的方法和可靠的评价手段。

Method: 1）引入了融合新评价标准且以大语言模型为判据的自动化评估方法，并构建了日语词典定义数据集，人工标注用于参照；2）提出利用大语言模型进行迭代简化生成学习者词典定义。

Result: 新提出的评价方式与人工评价高度一致；基于迭代简化的大模型定义生成方法在词语简单性和定义质量两方面均获得较高得分。

Conclusion: 作者的方法能够有效评估并生成适用于学习者词典的简明定义，为词典编写自动化和智能化提供了新的思路和工具。

Abstract: We study dictionary definition generation (DDG), i.e., the generation of non-contextualized definitions for given headwords. Dictionary definitions are an essential resource for learning word senses, but manually creating them is costly, which motivates us to automate the process. Specifically, we address learner's dictionary definition generation (LDDG), where definitions should consist of simple words. First, we introduce a reliable evaluation approach for DDG, based on our new evaluation criteria and powered by an LLM-as-a-judge. To provide reference definitions for the evaluation, we also construct a Japanese dataset in collaboration with a professional lexicographer. Validation results demonstrate that our evaluation approach agrees reasonably well with human annotators. Second, we propose an LDDG approach via iterative simplification with an LLM. Experimental results indicate that definitions generated by our approach achieve high scores on our criteria while maintaining lexical simplicity.

</details>


### [225] [Judging with Personality and Confidence: A Study on Personality-Conditioned LLM Relevance Assessment](https://arxiv.org/abs/2601.01862)
*Nuo Chen,Hanpei Fang,Piaohong Wang,Jiqun Liu,Tetsuya Sakai,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: 本文系统研究了大语言模型（LLM）模拟不同人格特质对网页搜索中相关性判定及信心校准的影响，并通过实验证明性格模拟可提升模型一致性和评测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明LLM可以通过prompting模拟特定人格，但尚不清楚这些模拟的人格如何影响如相关性判定等关键任务表现，尤其是其对信心校准（易过度自信或不足自信）的作用，这一问题在心理学中早有理论但未与AI模型结合验证。

Method: 作者对多种商用和开源LLM，利用prompting将其模拟为五大人格特质（Big Five），在三个公开测试集（TREC DL 2019、2020和LLMJudge）上进行相关性判定与自信程度评分实验，进一步将人格相关输出作为特征，结合随机森林在新数据集上测试泛化性。

Result: 结果发现，低宜人性人格的模型在相关性评判上与人工标签更一致，低尽责性人格有助于同时抑制过度自信与不足自信；人格不同，相关性与信心分布也有系统性变化。将人格特性及信心分数作为特征训练分类器，可在新任务上超越单一人格条件的表现。

Conclusion: 模拟人格能够为LLM输出带来可靠、附加的信心信息，提升其对评测任务的人类契合度，未来可用于构建更可信的人机评判系统。

Abstract: Recent studies have shown that prompting can enable large language models (LLMs) to simulate specific personality traits and produce behaviors that align with those traits. However, there is limited understanding of how these simulated personalities influence critical web search decisions, specifically relevance assessment. Moreover, few studies have examined how simulated personalities impact confidence calibration, specifically the tendencies toward overconfidence or underconfidence. This gap exists even though psychological literature suggests these biases are trait-specific, often linking high extraversion to overconfidence and high neuroticism to underconfidence. To address this gap, we conducted a comprehensive study evaluating multiple LLMs, including commercial models and open-source models, prompted to simulate Big Five personality traits. We tested these models across three test collections (TREC DL 2019, TREC DL 2020, and LLMJudge), collecting two key outputs for each query-document pair: a relevance judgment and a self-reported confidence score.
  The findings show that personalities such as low agreeableness consistently align more closely with human labels than the unprompted condition. Additionally, low conscientiousness performs well in balancing the suppression of both overconfidence and underconfidence. We also observe that relevance scores and confidence distributions vary systematically across different personalities. Based on the above findings, we incorporate personality-conditioned scores and confidence as features in a random forest classifier. This approach achieves performance that surpasses the best single-personality condition on a new dataset (TREC DL 2021), even with limited training data. These findings highlight that personality-derived confidence offers a complementary predictive signal, paving the way for more reliable and human-aligned LLM evaluators.

</details>


### [226] [DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs](https://arxiv.org/abs/2601.01868)
*Jinghan Ru,Siyuan Yan,Yuguo Yin,Yuexian Zou,Zongyuan Ge*

Main category: cs.CL

TL;DR: 该论文针对皮肤科领域多模态大模型训练数据有限、任务单一、缺乏与临床医生诊断流程一致的监督等问题，提出了包含大规模数据集、系统基准测试和创新模型训练方案的完整框架，大幅提升医学影像推理能力，缩小了AI与专家之间的差距。


<details>
  <summary>Details</summary>
Motivation: 皮肤病医疗AI发展缓慢，主要受制于缺乏大规模、高质量且贴近临床诊断流程的数据，以及标准化评测机制。这限制了多模态大模型在皮肤科实际诊断、推理等全流程能力的提升。该研究旨在通过开源数据和新方法，弥补这些关键短板，推动皮肤科AI水平接近人类专家。

Method: 1）构建大规模皮肤病形态指导型指令集DermoInstruct，涵盖21万多图片和77万多诊断流程数据，反映临床完整推理链条；2）建立DermoBench基准，评测11项任务与四大临床维度，包括形态识别、诊断、推理及公平性，并与专业医生基线对比；3）提出DermoGPT模型，采用精细化监督学习结合MAVIC强化学习目标（加强视觉和诊断推理一致性），预测时用信心一致性（CCT）做动态自适应，保证模型稳健性。

Result: DermoGPT在全部四个临床轴线、11项任务上，均显著优于16种现有代表性方法，取得最新最优表现，并有效缩小了人类专家与AI间的性能差距。

Conclusion: 该框架实现了皮肤病领域多模态大模型端到端推理、诊断与公平性的新突破，为临床AI诊断树立了标准化参考，相关数据和代码公开，有望推动皮肤科医疗AI研究和实际落地应用。

Abstract: Multimodal Large Language Models (MLLMs) show promise for medical applications, yet progress in dermatology lags due to limited training data, narrow task coverage, and lack of clinically-grounded supervision that mirrors expert diagnostic workflows. We present a comprehensive framework to address these gaps. First, we introduce DermoInstruct, a large-scale morphology-anchored instruction corpus comprising 211,243 images and 772,675 trajectories across five task formats, capturing the complete diagnostic pipeline from morphological observation and clinical reasoning to final diagnosis. Second, we establish DermoBench, a rigorous benchmark evaluating 11 tasks across four clinical axes: Morphology, Diagnosis, Reasoning, and Fairness, including a challenging subset of 3,600 expert-verified open-ended instances and human performance baselines. Third, we develop DermoGPT, a dermatology reasoning MLLM trained via supervised fine-tuning followed by our Morphologically-Anchored Visual-Inference-Consistent (MAVIC) reinforcement learning objective, which enforces consistency between visual observations and diagnostic conclusions. At inference, we deploy Confidence-Consistency Test-time adaptation (CCT) for robust predictions. Experiments show DermoGPT significantly outperforms 16 representative baselines across all axes, achieving state-of-the-art performance while substantially narrowing the human-AI gap. DermoInstruct, DermoBench and DermoGPT will be made publicly available at https://github.com/mendicant04/DermoGPT upon acceptance.

</details>


### [227] [Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents](https://arxiv.org/abs/2601.01885)
*Yi Yu,Liuyi Yao,Yuexiang Xie,Qingquan Tan,Jiaqi Feng,Yaliang Li,Libing Wu*

Main category: cs.CL

TL;DR: 本文提出AgeMem，一个结合长期和短期记忆处理的统一框架，提升了大模型在长时推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在长时任务中受限于有限的上下文窗口，内存管理（长期与短期记忆）不足，方法分裂且难以端到端优化。

Method: 提出AgeMem，将长期与短期记忆操作作为内置工具动作直接集成于代理策略中，通过三阶段递进式强化学习及分步GRPO算法训练统一的记忆管理行为。

Result: 在五项长时基准测试中，AgeMem在多个基础大模型上相较于其它记忆增强方法表现更优，包括任务表现、长期记忆质量与上下文利用效率。

Conclusion: AgeMem能够高效整合与管理长期与短期记忆，显著提升大模型在长时推理场景下的综合能力。

Abstract: Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.

</details>


### [228] [Tackling the Inherent Difficulty of Noise Filtering in RAG](https://arxiv.org/abs/2601.01896)
*Jingyu Liu,Jiaen Lin,Yong Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的微调方法，增强大型语言模型（LLM）在检索增强生成（RAG）场景下区分相关与无关信息的能力，从而提升其对嘈杂检索内容的鲁棒性和整体表现。


<details>
  <summary>Details</summary>
Motivation: RAG技术利用检索外部知识提升LLM表现，但检索过程易引入无关或噪声文档，导致性能下降甚至出现幻觉问题。现有过滤方法有限，模型容易受到噪声干扰。

Method: 作者提出了一种新颖的微调方法，专门训练LLM在检索到的文档中识别并利用相关信息，同时忽略无关内容，改善其对噪声的适应能力。

Result: 在多个基准测试上，所提方法显著提升了LLM应对检索噪声时的鲁棒性与表现，优于常规微调和过滤手段。

Conclusion: 通过专门设计的微调方法，可以显著增强LLM在RAG任务中对噪声和无关信息的识别及适应能力，推动RAG技术在实际应用中的可靠性。

Abstract: Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.

</details>


### [229] [CSF: Contrastive Semantic Features for Direct Multilingual Sign Language Generation](https://arxiv.org/abs/2601.01964)
*Tran Sy Bao*

Main category: cs.CL

TL;DR: 本文提出了一种无需英语中介的通用语义表示CSF，可直接从任意语言转换为手语，大幅提升全球聋人群体的信息无障碍性。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译系统通常以英语为中间桥梁，这对非英语聋人造成了障碍，因此需要探索消除语言中介、实现各语言直接翻译为手语的方法。

Method: 提出CSF语义框架，将语句拆分为9个通用语义槽，并构建细致的条件表达分类法。训练轻量级transformer提取模型，支持多语言语义槽自动识别和分类。

Result: 模型在英、越、日、法四种语言上的平均槽提取准确率达99.03%，其中条件概念分类35类下达99.4%准确率，模型体积小、推断速度快，支持实时Web端应用。

Conclusion: CSF实现了多语言到手语的无障碍转换，技术高效实用，有助于推动手语可达性研究发展。

Abstract: Sign language translation systems typically require English as an intermediary language, creating barriers for non-English speakers in the global deaf community. We present Canonical Semantic Form (CSF), a language-agnostic semantic representation framework that enables direct translation from any source language to sign language without English mediation. CSF decomposes utterances into nine universal semantic slots: event, intent, time, condition, agent, object, location, purpose, and modifier. A key contribution is our comprehensive condition taxonomy comprising 35 condition types across eight semantic categories, enabling nuanced representation of conditional expressions common in everyday communication. We train a lightweight transformer-based extractor (0.74 MB) that achieves 99.03% average slot extraction accuracy across four typologically diverse languages: English, Vietnamese, Japanese, and French. The model demonstrates particularly strong performance on condition classification (99.4% accuracy) despite the 35-class complexity. With inference latency of 3.02ms on CPU, our approach enables real-time sign language generation in browser-based applications. We release our code, trained models, and multilingual dataset to support further research in accessible sign language technology.

</details>


### [230] [Hidden State Poisoning Attacks against Mamba-based Language Models](https://arxiv.org/abs/2601.01972)
*Alexandre Le Mercier,Chris Develder,Thomas Demeester*

Main category: cs.CL

TL;DR: 本文发现了状态空间模型（SSM）如Mamba等在遭遇特殊输入短语时会出现“遗忘”现象，并提出了名为Hidden State Poisoning Attack（HiSPA）的攻击方式，对此类模型的信息检索能力构成威胁。作者还提出了RoBench25评测集，验证了该攻击有效性。


<details>
  <summary>Details</summary>
Motivation: SSM模型（如Mamba）因其更高效率而成为Transformer之外的备选方案，但其对抗鲁棒性尚未充分研究。作者关注于这些模型对输入短语的异常敏感性，特别是由特定输入导致模型隐藏状态不可逆改变的问题。

Method: 作者提出了一种新的攻击方式——HiSPA，并建立了RoBench25基准，系统评估了不同模型（包括混合SSM-Transformer和纯Transformer）在遭遇HiSPA攻击时的信息检索能力，以及在其他安全基准（如Open-Prompt-Injections）下的表现。同时，通过模型可解释性分析揭示了Mamba隐藏层在攻击下的行为模式。

Result: 结果显示，SSM类模型，尤其是Jamba家族的52B混合模型，在RoBench25与Open-Prompt-Injections上的表现因HiSPA明显下降，而纯Transformer不受影响。可解释性研究还揭示了Mamba隐藏层的受攻击特征。

Conclusion: 作者证明了现有SSM及混合模型在新型HiSPA攻击下存在明显安全漏洞，未来可利用隐藏层模式进行相应防御。相关代码和数据实现已开源。

Abstract: State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.

</details>


### [231] [Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling Effects](https://arxiv.org/abs/2601.02015)
*Omar Momen,Emilie Sitter,Berenike Herrmann,Sina Zarrieß*

Main category: cs.CL

TL;DR: 本文研究了语言模型中基于surprisal的概率预测指标对新颖隐喻理解的预测能力。


<details>
  <summary>Details</summary>
Motivation: 新颖隐喻的理解需要复杂的语义处理和语言创造力，是检验语言模型能力的良好任务。作者希望探索surprisal能否与隐喻新颖度数据集中的新颖度相关联，从而评估LMs对创造性语言的处理表现。

Method: 作者分析了16种不同语言模型在基于语料和合成的隐喻新颖度数据集上的surprisal表现，采用了cloze型的surprisal计算方法，全面利用句子上下文预测新颖隐喻的困难度，并考察其与新颖度评分/标注的相关性。同时，他们对比了不同模型规模下的相关性变化。

Result: 结果表明，LMs产生的surprisal与隐喻新颖度分数/标签存在中等强度且显著的相关性。在基于语料的数据上，相关性随模型规模增大而减弱（逆向扩展效应）；在合成数据上，相关性则随规模提升而增强（支持Quality-Power Hypothesis）。

Conclusion: 尽管surprisal可以一定程度反映隐喻新颖度的人工标注，但它作为衡量语言创造性的指标仍存在局限性。

Abstract: Novel metaphor comprehension involves complex semantic processes and linguistic creativity, making it an interesting task for studying language models (LMs). This study investigates whether surprisal, a probabilistic measure of predictability in LMs, correlates with different metaphor novelty datasets. We analyse surprisal from 16 LM variants on corpus-based and synthetic metaphor novelty datasets. We explore a cloze-style surprisal method that conditions on full-sentence context. Results show that LMs yield significant moderate correlations with scores/labels of metaphor novelty. We further identify divergent scaling patterns: on corpus-based data, correlation strength decreases with model size (inverse scaling effect), whereas on synthetic data it increases (Quality-Power Hypothesis). We conclude that while surprisal can partially account for annotations of metaphor novelty, it remains a limited metric of linguistic creativity.

</details>


### [232] [Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs](https://arxiv.org/abs/2601.02023)
*Amirali Ebrahimzadeh,Seyyed M. Salili*

Main category: cs.CL

TL;DR: 本论文系统评估了大型语言模型（LLM）在超长上下文下提取和推理信息的能力，并发现实际表现依赖于信息分布和提示设计，且模型间表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着LLM支持更长的输入上下文，用户倾向于直接粘贴大量真实数据。然而模型在超长上下文下的表现机制尚不明确，尤其在真实场景中，信息分布稀疏且提示风格复杂。因此，研究LLM在不同上下文长度、事实分布和提示引导下的信息处理表现具有重要现实意义。

Method: 作者基于扩展的needle-in-a-haystack基准，对四个商用LLM（Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, Deepseek-v3.2-chat）进行评测。方法区分了字面提取、逻辑推理和幻觉风险三个能力点，系统考察事实位置、真实分布及反幻觉提示条件下的模型表现。

Result: 结果显示，单纯延长上下文并不保证模型性能提升，相关证据稀释会显著影响效果。不同模型对长上下文的稳健性存在巨大差异，有的表现严重劣化，部分反幻觉指令会导致信息提取和推理准确度下降。模型经常无法有效筛选与利用关键信息。

Conclusion: 当前LLM在实际超长文本应用中的可靠性有限，模型和提示设计需针对长上下文与真实信息分布优化。企业使用时需关注模型差异与上下文承载力，确保重要信息不被埋没，提升推理和提取的稳定性。

Abstract: Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.

</details>


### [233] [Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory](https://arxiv.org/abs/2601.02065)
*Md. Asif Hossain,Nabil Subhan,Mantasha Rahman Mahi,Jannatul Ferdous Nabila*

Main category: cs.CL

TL;DR: 本文提出了一种高性价比、跨语言的RAG系统，让说孟加拉语的农民能便捷获取英文农业手册里的专业知识，依靠开源模型，无需昂贵云服务。


<details>
  <summary>Details</summary>
Motivation: 当前发展中地区农民获取农业咨询存在语言障碍，英文权威资料对以孟加拉语等本地低资源语言为主的农民而言难以直达，而直接用大语言模型生成低资源语言回答又流畅性和准确性不足。此外，现有云端方案成本高昂，难以大规模部署。

Method: 系统采用以翻译为核心的架构：将孟加拉语提问先翻译成英文，通过领域关键词注入提升专业术语对齐度，然后用向量检索方法在精选英文农业手册上查找答案，生成英文回复后再翻译回孟加拉语。全流程基于开源模型实现，可在普通硬件运行，无需付费API。

Result: 实验结果表明，该系统能生成依赖于可靠信息源的回答，并能较好拒绝领域外的问题，端到端单次响应延迟低于20秒，实际可用性强。

Conclusion: 跨语言检索结合受控翻译的方法能够为低资源语言环境下的农业知识普及提供实用、可扩展的解决方案，特别适合当地实际部署，提升农业服务可达性。

Abstract: Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings

</details>


### [234] [Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows](https://arxiv.org/abs/2601.02076)
*Yingte Shu,Yuchuan Tian,Chao Xu,Yunhe Wang,Hanting Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的去编码策略——延迟承诺解码（DCD），有效突破了块状扩散语言模型在边界处信息截断的问题，从而提高了解码质量和准确率。


<details>
  <summary>Details</summary>
Motivation: 传统扩散语言模型为了并行生成文本，采用了块式解码，但在块边界处，部分尚未解码的token在缺乏未来上下文的情况下被过早决定，导致信息截断、信心下降，影响生成质量，尤其在数学推理和代码生成等精确性要求高的任务中更为显著。

Method: 提出延迟承诺解码（DCD），通过一个置信度感知的滑动窗口，优先解码置信度高的token，将高不确定性的token推迟到获取更多上下文信息后再决定，从而在窗口内实现更有效的双向信息流动，无需额外训练。

Result: 在多个扩散语言模型、基准任务和缓存配置下实验证明，DCD相比固定块方法，在不增加明显推理时间的情况下，平均提升生成准确率1.39%，部分场景提升可达9%。

Conclusion: 依据不确定性动态推迟token决策，是提升扩散语言模型解码质量和效率的有效策略，展现了其在实际文本生成任务中的潜力。

Abstract: Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.

</details>


### [235] [DeCode: Decoupling Content and Delivery for Medical QA](https://arxiv.org/abs/2601.02123)
*Po-Jen Ko,Chen-Han Tsai,Yu-Shao Peng*

Main category: cs.CL

TL;DR: 本文提出了DeCode，一个无需额外训练且通用的框架，可使大语言模型（LLMs）在临床环境中生成更具个体上下文的答案，显著提升了模型回答的临床相关性。


<details>
  <summary>Details</summary>
Motivation: 尽管已有LLMs具备强大的医学知识及事实准确回答能力，但在临床医学实际应用时，常因忽视患者具体上下文而导致答复虽正确却不符个体需求。因此，迫切需要方法提升LLMs针对临床问题的个体化、场景化回答能力。

Method: 提出DeCode框架，无需对LLMs进行再训练，可直接适配不同模型，使其回答更加贴合患者的具体情境。方法已在权威医学领域基准OpenAI HealthBench上进行了运行和测试。

Result: DeCode在OpenAI HealthBench评测中，将临床相关性指标从28.4%提升至49.8%，实现了约75%的相对提升，刷新了已有方法的最佳成绩。

Conclusion: DeCode能够有效增强LLMs在临床问答场景下的上下文适应能力和答案针对性，有助于医学AI问答系统更好地满足实际医疗需求。

Abstract: Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\%$ to $49.8\%$, corresponding to a $75\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.

</details>


### [236] [Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation](https://arxiv.org/abs/2601.02128)
*Steffen Freisinger,Philipp Seeberger,Thomas Ranzenberger,Tobias Bocklet,Korbinian Riedhammer*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的语音文本分层主题分割方法，通过生成多级目录，同时识别主题和子主题边界，在英文会议和多语种讲座数据集上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 将语音转录内容按照主题分段，便于后续处理及提升无障碍访问性，但现有方法对多层次（主题、子主题）的分割支持有限。

Method: 提出一种层级结构下的主题分割方法，能生成主题和子主题的多级目录。考察了大语言模型的零样本提示和LoRA微调两种策略，并尝试融入语音停顿特征。此外，改进并应用了适配多层级分割的评价指标。

Result: 在英文会议记录和葡萄牙语、德语讲座转录的多语言数据集上，新方法在分割准确性上大幅优于传统基线方法。

Conclusion: 所提方法能高效地进行多层次主题分割，提升了现有语音转录分割的表现，对表征主题结构具有实用价值。

Abstract: Segmenting speech transcripts into thematic sections benefits both downstream processing and users who depend on written text for accessibility. We introduce a novel approach to hierarchical topic segmentation in transcripts, generating multi-level tables of contents that capture both topic and subtopic boundaries. We compare zero-shot prompting and LoRA fine-tuning on large language models, while also exploring the integration of high-level speech pause features. Evaluations on English meeting recordings and multilingual lecture transcripts (Portuguese, German) show significant improvements over established topic segmentation baselines. Additionally, we adapt a common evaluation measure for multi-level segmentation, taking into account all hierarchical levels within one metric.

</details>


### [237] [Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts](https://arxiv.org/abs/2601.02144)
*Boxuan Lyu,Soichiro Murakami,Hidetaka Kamigaito,Peinan Zhang*

Main category: cs.CL

TL;DR: 本文提出kNN-MoE方法，通过检索历史最优分派案例增强了MoE路由鲁棒性，实现分布变化下更优的专家选择。实验显示，该方法优于零样本基线，并与高成本的有监督微调效果相当。


<details>
  <summary>Details</summary>
Motivation: 传统Mixture-of-Experts（MoE）模型的路由器在训练后被冻结，导致其在输入分布变化（如domain shift）下表现不稳健，影响大模型的高效扩展和泛化。

Method: 作者提出kNN-MoE框架，在已有冻结路由器基础上，离线构建历史token-专家分配的记忆库。推理时，将新token与相似历史案例进行检索，利用最近邻分配的相似度作为混合系数，实现专家分流决策。若检索不到相关案例，则退化为使用冻结路由。

Result: kNN-MoE在多项实验中超过了传统冻路由方案，且在无需高昂监督微调的前提下达到与其相当的效果，验证了方法有效性和实用性。

Conclusion: 检索增强的动态路由显著提升了MoE在分布漂移下的鲁棒性，对大模型高效、稳定部署具有实际推动作用。

Abstract: Mixture-of-Experts (MoE) architectures scale large language models efficiently by employing a parametric "router" to dispatch tokens to a sparse subset of experts. Typically, this router is trained once and then frozen, rendering routing decisions brittle under distribution shifts. We address this limitation by introducing kNN-MoE, a retrieval-augmented routing framework that reuses optimal expert assignments from a memory of similar past cases. This memory is constructed offline by directly optimizing token-wise routing logits to maximize the likelihood on a reference set. Crucially, we use the aggregate similarity of retrieved neighbors as a confidence-driven mixing coefficient, thus allowing the method to fall back to the frozen router when no relevant cases are found. Experiments show kNN-MoE outperforms zero-shot baselines and rivals computationally expensive supervised fine-tuning.

</details>


### [238] [FormationEval, an open multiple-choice benchmark for petroleum geoscience](https://arxiv.org/abs/2601.02158)
*Almaz Ermilov*

Main category: cs.CL

TL;DR: 提出了FormationEval，这是一个公开的石油地球科学与地下学科多选题基准数据集，用于评估主流大语言模型的专业能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对石油地球科学和地下学科的专业评测基准，阻碍了对大语言模型在专业领域表现的客观评价与对比。

Method: 作者基于权威资料，采用推理建问配合详细指令、概念导向（避免直接抄袭）方法，构建505道涵盖七大专业领域的多选题，并提供元数据以保证溯源和审计。用该基准评估了72个主流大语言模型，涵盖闭源与开源模型。

Result: 顶尖闭源模型如Gemini 3 Pro Preview取得99.8%准确率，开源模型GLM-4.7达98.6%，多款开源模型超过93%。开源模型与闭源模型的性能差距小于预期。石油物理学是所有模型最具挑战性的领域。数据集存在正确答案偏长的残余长度偏倚，作者已采取部分缓解措施。

Conclusion: FormationEval数据集和代码公开，提升了地学专业领域大模型的可比性和透明度。开源模型成长迅速，部分低成本开源模型已具有实用价值，但特定专业领域仍需进一步突破。

Abstract: This paper presents FormationEval, an open multiple-choice question benchmark for evaluating language models on petroleum geoscience and subsurface disciplines. The dataset contains 505 questions across seven domains including petrophysics, petroleum geology and reservoir engineering, derived from three authoritative sources using a reasoning model with detailed instructions and a concept-based approach that avoids verbatim copying of copyrighted text. Each question includes source metadata to support traceability and audit. The evaluation covers 72 models from major providers including OpenAI, Anthropic, Google, Meta and open-weight alternatives. The top performers achieve over 97\% accuracy, with Gemini 3 Pro Preview reaching 99.8\%, while tier and domain gaps persist. Among open-weight models, GLM-4.7 leads at 98.6\%, with several DeepSeek, Llama, Qwen and Mistral models also exceeding 93\%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90\% accuracy. Petrophysics emerges as the most challenging domain across all models, while smaller models show wider performance variance. Residual length bias in the dataset (correct answers tend to be longer) is documented along with bias mitigation strategies applied during construction. The benchmark, evaluation code and results are publicly available.

</details>


### [239] [Confidence Estimation for LLMs in Multi-turn Interactions](https://arxiv.org/abs/2601.02179)
*Caiqi Zhang,Ruihan Yang,Xiaochen Zhu,Chengzu Li,Tiancheng Hu,Yijiang River Dong,Deqing Yang,Nigel Collier*

Main category: cs.CL

TL;DR: 本文系统性研究了大型语言模型（LLMs）在多轮对话情境下的置信度估计，提出了新的评估框架与指标，并发现现有方法在这一场景中表现不佳，提出的P(Sufficient)方法改善有限，为后续对话智能体可信性研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 虽然置信度估计可帮助缓解大模型幻觉问题，但相关研究几乎只关注单轮对话，多轮对话中随着上下文积累与消歧，模型置信度动态变化规律尚不清楚。提升多轮场景下置信度估计的可靠性对于自主智能体、人机协作等应用至关重要。

Method: 提出了系统性的多轮置信度估计评估框架，包括“逐轮标定”和置信度单调性为关键需求；设计了新指标（如信息归一化的预期校准误差InfoECE）和“Hinter-Guesser”数据集生成范式。对比实验评测现有流行置信度技术并提出logit探针P(Sufficient)。

Result: 实验显示，现有置信度估计方法在多轮对话中难以保证校准与单调性，表现有限。提出的P(Sufficient)探针性能有改进，但任务仍未彻底解决。

Conclusion: 本研究为多轮对话场景下置信度估计提供了理论与方法基础，将促进更可靠可信的对话体系统研发。

Abstract: While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new "Hinter-Guesser" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.

</details>


### [240] [Toward Global Large Language Models in Medicine](https://arxiv.org/abs/2601.02186)
*Rui Yang,Huitao Li,Weihao Xuan,Heli Qi,Xin Li,Kunyu Yu,Yingjian Chen,Rongrong Wang,Jacques Behmoaras,Tianxi Cai,Bibhas Chakraborty,Qingyu Chen,Lionel Tim-Ee Cheng,Marie-Louise Damwanza,Chido Dzinotyiwei,Aosong Feng,Chuan Hong,Yusuke Iwasawa,Yuhe Ke,Linah Kitala,Taehoon Ko,Jisan Lee,Irene Li,Jonathan Chong Kai Liew,Hongfang Liu,Lian Leng Low,Edison Marrese-Taylor,Yutaka Matsuo,Isheanesu Misi,Yilin Ning,Jasmine Chiat Ling Ong,Marcus Eng Hock Ong,Enrico Petretto,Hossein Rouhizadeh,Abiram Sandralegar,Oren Schreier,Iain Bee Huat Tan,Patrick Tan,Daniel Shu Wei Ting,Junjue Wang,Chunhua Weng,Matthew Yu Heng Wong,Fang Wu,Yunze Xiao,Xuhai Xu,Qingcheng Zeng,Zhuo Zheng,Yifan Peng,Douglas Teodoro,Nan Liu*

Main category: cs.CL

TL;DR: 本文介绍了GlobMed，一个包含12种语言（含4种资源稀缺语言）逾50万条数据的多语言医学数据集，并在此基础上建立了相应的评测基准和多语言医学大模型。


<details>
  <summary>Details</summary>
Motivation: 当前主流大语言模型主要集中于高资源语言，限制了其在全球医疗场景中的应用，尤其不利于低资源语言用户。为缩小这一鸿沟，亟需构建多语言并支持低资源语言的医学数据集与高效模型。

Method: 研究团队构建了GlobMed多语言医学数据集，涵盖12种语言及多类医疗任务，并基于该数据集开发了评测基准（GlobMed-Bench），系统评估了56个主流大语言模型。同时，训练了参数规模为1.7B至8B的一系列多语言医学大模型（GlobMed-LLMs）。

Result: GlobMed-Bench评测揭示现有模型在低资源语言上表现明显较差。新训练的GlobMed-LLMs在平均性能上较基线提升40%以上，低资源语言表现提升超过三倍。

Conclusion: GlobMed数据集、评测体系及模型为医学大语言模型的公平发展和广泛应用奠定了基础，有助于让更多语言社区受益于前沿技术。

Abstract: Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.

</details>


### [241] [ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging](https://arxiv.org/abs/2601.02209)
*Omer Nacar,Serry Sibaee,Adel Ammar,Yasser Alhabashi,Nadia Samer Sibai,Yara Farouk Ahmed,Ahmed Saud Alqusaiyer,Sulieman Mahmoud AlMahmoud,Abdulrhman Mamdoh Mukhaniq,Lubaba Raed,Sulaiman Mohammed Alatwah,Waad Nasser Alqahtani,Yousif Abdulmajeed Alnasser,Mohamed Aziz Khadraoui,Wadii Boulila*

Main category: cs.CL

TL;DR: 本文介绍了一套名为ARCADE的阿拉伯语城市级别方言语音数据集，填补了现有方言数据集缺乏细粒度（如城市级）标注的空白。


<details>
  <summary>Details</summary>
Motivation: 虽然已有多种阿拉伯语多方言数据集，但针对具体城市层面的方言语音标注研究尚不充分。作者希望通过更细致的城市级标注，促进对阿拉伯语方言多样性的深入分析及相关应用。

Method: 作者从阿拉伯世界各地的网络广播中采集30秒的阿拉伯语语音片段，覆盖标准语和多种地方方言。每段音频由1至3位母语者人工标注，包括情感、语音类型、方言类别、以及适用性标签。最终得到58座城市、19个国家的3790条音频片段及6907份注释。

Result: 构建了细粒度、有丰富元数据的城市级方言语音数据集ARCADE，并对音频质量和标注分布进行了系统分析。数据集已开放下载。

Conclusion: ARCADE数据集为阿拉伯语方言（特别是城市级）识别、多任务学习等任务提供了高质量基准，有助于推动该领域研究，并填补城市级方言标注的空白。

Abstract: The Arabic language is characterized by a rich tapestry of regional dialects that differ substantially in phonetics and lexicon, reflecting the geographic and cultural diversity of its speakers. Despite the availability of many multi-dialect datasets, mapping speech to fine-grained dialect sources, such as cities, remains underexplored. We present ARCADE (Arabic Radio Corpus for Audio Dialect Evaluation), the first Arabic speech dataset designed explicitly with city-level dialect granularity. The corpus comprises Arabic radio speech collected from streaming services across the Arab world. Our data pipeline captures 30-second segments from verified radio streams, encompassing both Modern Standard Arabic (MSA) and diverse dialectal speech. To ensure reliability, each clip was annotated by one to three native Arabic reviewers who assigned rich metadata, including emotion, speech type, dialect category, and a validity flag for dialect identification tasks. The resulting corpus comprises 6,907 annotations and 3,790 unique audio segments spanning 58 cities across 19 countries. These fine-grained annotations enable robust multi-task learning, serving as a benchmark for city-level dialect tagging. We detail the data collection methodology, assess audio quality, and provide a comprehensive analysis of label distributions. The dataset is available on: https://huggingface.co/datasets/riotu-lab/ARCADE-full

</details>


### [242] [From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality](https://arxiv.org/abs/2601.02224)
*Fabian Lukassen,Jan Herrmann,Christoph Weisser,Benjamin Saefken,Thomas Kneib*

Main category: cs.CL

TL;DR: 本文系统性研究了影响大语言模型（LLM）将可解释AI（XAI）数值归因转化为自然语言解释质量的因素，包括预测模型选择、XAI方法、LLM选择和提示策略。结果发现，LLM本身选择影响最大，部分XAI方法仅对专家观众有小幅提升，SARIMAX模型虽预测准但可解释性反降，零样本提示既经济又有效。


<details>
  <summary>Details</summary>
Motivation: 以SHAP、LIME为代表的XAI方法输出的特征归因多为数值，不利于非专业人士理解。已有研究通过LLM转译成自然语言解释，但影响解释质量的具体因素未明。本文旨在深入研究实现高质量自然语言解释的关键参数。

Method: 作者设计了系统性因子实验，涵盖4种预测模型（XGB、RF、MLP、SARIMAX）、3种XAI条件（SHAP、LIME、无XAI）、3种LLM（GPT-4o、Llama-3-8B、DeepSeek-R1）、8种提示策略。采用G-Eval（由LLM担任评委），依据4项标准评测660条时间序列预测解释。

Result: （1）XAI方法对专家观众的解释质量提升很小，对非专家无显著效果；（2）LLM选择的影响最大，DeepSeek-R1优于GPT-4o和Llama-3；（3）SARIMAX虽预测准但NLE质量较低，出现“可解释性悖论”；（4）零样本提示与多样本自一致性策略效果相近但成本低7倍；（5）链式思维反而降低了解释效果。

Conclusion: LLM选择是生成高质量自然语言解释的决定性因素，传统XAI解释仅对专业人士略有增益，零样本提示更具实用性，而常见的链式思维提示法并不适用于此场景。

Abstract: Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps.

</details>


### [243] [CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models](https://arxiv.org/abs/2601.02236)
*Yihao Liang,Ze Wang,Hao Chen,Ximeng Sun,Jialian Wu,Xiaodong Yu,Jiang Liu,Emad Barsoum,Zicheng Liu,Niraj K. Jha*

Main category: cs.CL

TL;DR: 本论文提出CD4LM框架，实现扩散语言模型（DLMs）的高并行、高质量快速生成。通过新的训练与推理解耦方法，大幅提升生成速度且保持甚至提升准确率，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型虽然表现良好，但解码过程受限于序列依赖，速度较慢。扩散语言模型理论上可并行生成，但训练和推理错位，导致实用效率受限。需要新方法打破该瓶颈，实现低推理延迟同时不牺牲生成质量。

Method: 提出CD4LM框架，包含两个核心创新：1）离散空间一致性蒸馏（DSCD），训练过程中让学生模型直接由多样化噪声状态映射到干净分布，实现轨迹不变性；2）置信自适应解码（CAD），根据token置信度智能分配计算资源，选择性跳步以减少推理开销。

Result: 在GSM8K任务上，CD4LM在保持生成质量的同时实现5.18倍推理加速；在代码与数学基准测试中，CD4LM准确率-效率达到Pareto最优前沿，平均加速3.62倍且准确率更高。

Conclusion: CD4LM能够显著加速扩散语言模型的推理过程，同时保证甚至提升生成质量，展现了解耦训练与推理的新范式。提供了可公开获取的实现代码，推动领域实际应用。

Abstract: Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive "long-jump" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM

</details>


### [244] [pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs](https://arxiv.org/abs/2601.02285)
*Tobias Schimanski,Imene Kolli,Jingwei Ni,Yu Fan,Ario Saeid Vaghefi,Elliott Ash,Markus Leippold*

Main category: cs.CL

TL;DR: 本文提出了pdfQA，这是一个多领域、区分十种复杂性维度的问题回答（QA）数据集，包括2K真人标注和2K合成的QA对，以弥补现有QA数据集大多基于文本且领域有限的不足。并通过开源大模型进行测试，展示了与复杂性维度相关的现有挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的QA数据集大多以文本为出发点，且主要针对特定领域，无法很好地支持针对PDF等常见文档格式的多领域、复杂性评测。PDF作为互联网上第二大文档类型，亟需专业且多样化的QA基准。

Method: 作者提出并构建了pdfQA数据集，包含2K人工标注和2K合成的多领域QA对，涵盖如文件类型、源模态、答案类型等十个复杂性维度。研究者还引入质量与难度过滤，并用开源大语言模型对QA对进行了回测和分析。

Result: 数据集经质控和难度筛选后，包含有效且具有挑战性的QA对。开源大模型在此数据集上的表现揭示了大量与复杂性维度相关的难题。

Conclusion: pdfQA为端到端的PDF复杂问答系统评测提供了基础，能够测试丰富技能和流程优化，为进一步的模型改进和真实场景应用打下基础。

Abstract: PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing).

</details>


### [245] [Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)](https://arxiv.org/abs/2601.02298)
*Mahmoud Elgenedy*

Main category: cs.CL

TL;DR: 论文提出了一种将大型语言模型权重量化到仅包含2的幂（Power-of-Two, PoT）的量化方法，显著降低内存和计算成本，尤其适用于边缘设备。采用QAT训练可缓解精度损失。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型参数数量呈指数级增长，导致其在边缘设备上的部署受限于内存和算力。因此，迫切需要新的压缩和量化方法以降低资源消耗。

Method: 提出权重仅取2的幂值的PoT量化方法，只需存储指数，同时将乘法运算简化为位运算以加速推理。为减少性能损失，引入QAT（量化感知训练），通过重新训练适应严格量化后模型权重。

Result: 对GPT-2 124M模型实验表明，经过QAT训练后，量化模型复杂度（perplexity）提升66%，BERT-Score损失仅为1%。与全精度模型相比，内存节省约87.5%，推理速度提升3-10倍。

Conclusion: PoT量化结合QAT训练能极大提升边缘设备对大型语言模型的部署效率，在保证精度的前提下大幅度提升存储和推理效率，具有较高实际应用价值。

Abstract: In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.

</details>


### [246] [Classifying several dialectal Nawatl varieties](https://arxiv.org/abs/2601.02303)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Carlos-Emiliano González-Gallardo,Graham Ranger,Martha Lorena-Avendaño-Garrido*

Main category: cs.CL

TL;DR: 本论文针对墨西哥主要使用的土著语言Nawatl的方言自动分类问题，利用机器学习和神经网络方法进行研究。


<details>
  <summary>Details</summary>
Motivation: Nawatl语作为墨西哥最大土著语种之一，拥有极为丰富的方言变体和书写差异，但其数字化、计算机资源极其稀缺，限制了语言保护与应用。

Method: 论文采用机器学习和神经网络技术，对Nawatl语大约30种被认可的方言进行自动化分类。具体方法未在摘要详细说明。

Result: 研究实现了对Nawatl方言的分类，但摘要未具体列出实验结果及准确率。

Conclusion: 本研究为Nawatl语言的自动方言识别提供了初步方法，推动了濒危土著语言的数字化和人工智能辅助保护。

Abstract: Mexico is a country with a large number of indigenous languages, among which the most widely spoken is Nawatl, with more than two million people currently speaking it (mainly in North and Central America). Despite its rich cultural heritage, which dates back to the 15th century, Nawatl is a language with few computer resources. The problem is compounded when it comes to its dialectal varieties, with approximately 30 varieties recognised, not counting the different spellings in the written forms of the language. In this research work, we addressed the problem of classifying Nawatl varieties using Machine Learning and Neural Networks.

</details>


### [247] [Estimating Text Temperature](https://arxiv.org/abs/2601.02320)
*Nikolay Mikhaylovskiy*

Main category: cs.CL

TL;DR: 本文提出了一种方法，可以利用最大似然估计算法，在文本生成后推断温度参数，并实证其在不同LLM上的效果。


<details>
  <summary>Details</summary>
Motivation: 通常，生成式语言模型利用温度参数在生成过程中调节文本的随机性，但生成后难以得知文本实际使用的温度参数。推断文本的'温度'有助于更好地分析文本属性、检测模型生成文本等应用。

Method: 作者提出在文本生成完成后，基于最大似然的方法估算文本的温度，并在多个小型至中型LLM上验证其效果。随后，用效果最好的Qwen3 14B模型对多个经典语料库进行温度估计。

Result: 论文展示了多种模型的温度估算性能，其中Qwen3 14B在这一任务上表现最好，并进一步用该模型对流行语料库的文本温度进行了分析。

Conclusion: 文中提出的温度估算方法能够有效推断文本的温度参数，对理解文本生成机制及文本特征分析具有实际意义。

Abstract: Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given language model. We evaluate the temperature estimation capability of a wide selection of small-to-medium LLMs. We then use the best-performing Qwen3 14B to estimate temperatures of popular corpora.

</details>


### [248] [Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling](https://arxiv.org/abs/2601.02337)
*Berk Atil,Rebecca J. Passonneau,Ninareh Mehrabi*

Main category: cs.CL

TL;DR: 本论文系统评估了大语言模型（LLM）在不同人设（persona）下的毒性检测，提出通过集成多种提示方法，并用SVM元集成方法，能显著提升检测一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 毒性检测结果往往受不同群体的社会视角影响，同一句话对不同人可能有不同解读。如何让模型理解并适应这种主观差异，提升检测的多元性和公平性，是主观性NLP任务面临的挑战。

Method: 作者对不同的人设提示方法在不同基础大模型上的表现进行系统比较，包括提出一种自动化提示优化策略。为进一步提升性能，尝试了四种提示方法的集成，并构建基于四种预测结果的4位向量输入的SVM元集成方法。

Result: 无论是单一提示方法还是自动化优化提示，都没有在所有模型和人设组合中占优。SVM集成方法能捕捉多种提示方法的互补性，相较传统投票法和单一提示，获得了更强的一致性和总体性能。

Conclusion: SVM元集成不仅提升了跨人设的毒性检测表现，还为主观性NLP任务中的多元评价提供了新的可靠方案。该研究是领域内首次系统比较“人设敏感型”提示方法，为后续研究指明了方向。

Abstract: Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic'' modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [249] [Value Vision-Language-Action Planning & Search](https://arxiv.org/abs/2601.00969)
*Ali Salamatian,Ke,Ren,Kieran Pattison,Cyrus Neary*

Main category: cs.RO

TL;DR: 本文提出了一种结合视觉-语言-动作模型（VLA）与可学习价值函数的新型规划与搜索框架（V-VLAPS），提升了机器人操作的成功率并减少了测试过程中的搜索开销。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖行为克隆，对分布偏移敏感，容易在新环境下失效。尽管可以用MCTS等搜索算法增强模型，但仅使用VLA先验，若其不准确，则纠正动作需要大量探索，效率低下。因此需要更可靠的未来回报估计来指导搜索和动作决策。

Method: 作者提出V-VLAPS方法，在固定的VLA模型（Octo）基础上，额外训练一个MLP价值函数，利用其输出作为MCTS搜索过程的显式回报信号，从而更有效地引导动作选择。所有建模和训练均在VLA的潜在表示空间中进行，无需大规模额外标注。方法在LIBERO机器人操作任务套件上进行评测。

Result: 实验结果显示，V-VLAPS相比仅用VLA先验的基线方法，任务成功率提升了5个百分点以上，同时所需的MCTS仿真次数减少了5-15%。

Conclusion: 结合可学习价值函数能显著增强基于VLA的机器人操作系统的泛化能力和效率，为分布外场景下的行为决策提供了新的解决方案。

Abstract: Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic manipulation, yet they remain fundamentally limited by their reliance on behavior cloning, leading to brittleness under distribution shift. While augmenting pretrained models with test-time search algorithms like Monte Carlo Tree Search (MCTS) can mitigate these failures, existing formulations rely solely on the VLA prior for guidance, lacking a grounded estimate of expected future return. Consequently, when the prior is inaccurate, the planner can only correct action selection via the exploration term, which requires extensive simulation to become effective. To address this limitation, we introduce Value Vision-Language-Action Planning and Search (V-VLAPS), a framework that augments MCTS with a lightweight, learnable value function. By training a simple multilayer perceptron (MLP) on the latent representations of a fixed VLA backbone (Octo), we provide the search with an explicit success signal that biases action selection toward high-value regions. We evaluate V-VLAPS on the LIBERO robotic manipulation suite, demonstrating that our value-guided search improves success rates by over 5 percentage points while reducing the average number of MCTS simulations by 5-15 percent compared to baselines that rely only on the VLA prior.

</details>


### [250] [From Perception to Symbolic Task Planning: Vision-Language Guided Human-Robot Collaborative Structured Assembly](https://arxiv.org/abs/2601.00978)
*Yanyi Chen,Min Deng*

Main category: cs.RO

TL;DR: 该论文提出了一个面向人-机器人协作结构化装配的感知与规划框架，通过结合视觉-语言模型和知识驱动规划，提高了在动态环境下的状态估计和任务规划的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 结构化装配场景下，人-机器人协作面临感知噪声大、人为干预频繁等挑战，现有方法在状态估计和任务规划的适应性和鲁棒性方面存在不足，因此需要新的框架以解决这些问题。

Method: 框架包括两个模块：模块I（PSS）利用视觉-语言模型将RGB-D观测与设计规范、领域知识进行对齐，合成可验证的符号化装配状态；模块II（HPR）进行任务级多机器人分配，并通过最小变更重规划规则，在装配状态偏离预期时选择性调整任务分配，以确保任务稳定推进。

Result: 在27组件的木结构装配实验中，PSS模块达到97%的状态合成准确率，HPR模块在多种人-机协作场景下保持了任务推进的可行性。

Conclusion: 将基于视觉-语言模型的感知与知识驱动规划集成，能够提升HRC装配中状态估计和任务规划在动态、干扰条件下的鲁棒性。

Abstract: Human-robot collaboration (HRC) in structured assembly requires reliable state estimation and adaptive task planning under noisy perception and human interventions. To address these challenges, we introduce a design-grounded human-aware planning framework for human-robot collaborative structured assembly. The framework comprises two coupled modules. Module I, Perception-to-Symbolic State (PSS), employs vision-language models (VLMs) based agents to align RGB-D observations with design specifications and domain knowledge, synthesizing verifiable symbolic assembly states. It outputs validated installed and uninstalled component sets for online state tracking. Module II, Human-Aware Planning and Replanning (HPR), performs task-level multi-robot assignment and updates the plan only when the observed state deviates from the expected execution outcome. It applies a minimal-change replanning rule to selectively revise task assignments and preserve plan stability even under human interventions. We validate the framework on a 27-component timber-frame assembly. The PSS module achieves 97% state synthesis accuracy, and the HPR module maintains feasible task progression across diverse HRC scenarios. Results indicate that integrating VLM-based perception with knowledge-driven planning improves robustness of state estimation and task planning under dynamic conditions.

</details>


### [251] [Simulations of MRI Guided and Powered Ferric Applicators for Tetherless Delivery of Therapeutic Interventions](https://arxiv.org/abs/2601.00981)
*Wenhui Chu,Khang Tran,Nikolaos V. Tsekos*

Main category: cs.RO

TL;DR: 本文提出了一个用于术前规划和建模MRI驱动器械在血管内运动的计算平台。该平台实现了MRI与操作者之间的数据和指令双向传输，实现了血管中心线提取、虚拟通道（VF）生成、MRI安全合规性分析和器械可控性的建模。


<details>
  <summary>Details</summary>
Motivation: 随着MRI在介入手术中的导航应用增多，如何在保证安全的情况下，利用MRI磁场梯度对微型铁磁器械进行精确操控成为一项挑战。现有方法缺乏完善的建模、规划与安全性综合评估平台，因此有必要提出新的综合性解决方案支持手术规划和器械操控。

Method: 论文提出并实现了一个软件平台，包括多线程架构，专为术前规划和操控MRI驱动的血管内器械设计。具体方法包括：1）基于多层MRI数据自动提取血管床与血管中心线；2）拟合血管内虚拟通道VF，作为器械安全运行的通道边界参考；3）结合血管几何和MRI安全参数，生成磁场梯度波形，实现对器械的安全可控移动建模；4）支持多种血流参数模拟和血管路径安全性自动判断。

Result: 平台实现了血管床自动提取、虚拟通道生成、梯度波形自动生成与血流参数可定制化模拟。能够有效评估不同血管路径下器械安全性和可达性，为未来实时实验平台的开发奠定了基础。

Conclusion: 该平台为MRI驱动介入器械的术前规划提供了数据链路、几何规划与物理建模一体化解决方案，有助于提升介入手术的安全性与效率。

Abstract: Magnetic Resonance Imaging (MRI) is a well-established modality for pre-operative planning and is also explored for intra-operative guidance of procedures such as intravascular interventions. Among the experimental robot-assisted technologies, the magnetic field gradients of the MRI scanner are used to power and maneuver ferromagnetic applicators for accessing sites in the patient's body via the vascular network. In this work, we propose a computational platform for preoperative planning and modeling of MRI-powered applicators inside blood vessels. This platform was implemented as a two-way data and command pipeline that links the MRI scanner, the computational core, and the operator. The platform first processes multi-slice MR data to extract the vascular bed and then fits a virtual corridor inside the vessel. This corridor serves as a virtual fixture (VF), a forbidden region for the applicators to avoid vessel perforation or collision. The geometric features of the vessel centerline, the VF, and MRI safety compliance (dB/dt, max available gradient) are then used to generate magnetic field gradient waveforms. Different blood flow profiles can be user-selected, and those parameters are used for modeling the applicator's maneuvering. The modeling module further generates cues about whether the selected vascular path can be safely maneuvered. Given future experimental studies that require a real-time operation, the platform was implemented on the Qt framework (C/C++) with software modules performing specific tasks running on dedicated threads: PID controller, generation of VF, generation of MR gradient waveforms.

</details>


### [252] [Topological Mapping and Navigation using a Monocular Camera based on AnyLoc](https://arxiv.org/abs/2601.01067)
*Wenzheng Zhang,Yoshitaka Hara,Sousuke Nakamura*

Main category: cs.RO

TL;DR: 本文提出了一种仅使用单目相机进行拓扑建图和导航的方法，通过将关键帧转为描述符来建立拓扑关系，实现高效的环路检测和地图构建。该方法在真实和仿真环境下无需预训练，成功率显著高于ResNet方法，且资源占用更低。


<details>
  <summary>Details</summary>
Motivation: 当前基于度量地图的导航方法需要精确坐标，计算和资源消耗较高；而拓扑地图通过关键节点简化了规划与导航。此外，降低硬件和预训练依赖，对于实际应用具有重要意义。

Method: 该方法基于AnyLoc框架，将关键帧转为描述符，并用于构建环境的拓扑关系，实现环路检测和地图构建。视觉导航动作通过比对分割图像与目标节点的图像来确定。整个过程仅依赖单目相机，无需复杂的传感器或预训练。

Result: 实验证明，该方法在真实和仿真环境下均可实现有效的环路检测和导航。与基于ResNet的方法相比，平均提升了60.2%的任务成功率，同时显著减少了时间和空间占用。

Conclusion: 该方法为机器人及人类在不同场景中的导航提供了一种轻量、可靠的解决方案，无需额外硬件及预训练，效率高且适用性广。

Abstract: This paper proposes a method for topological mapping and navigation using a monocular camera. Based on AnyLoc, keyframes are converted into descriptors to construct topological relationships, enabling loop detection and map building. Unlike metric maps, topological maps simplify path planning and navigation by representing environments with key nodes instead of precise coordinates. Actions for visual navigation are determined by comparing segmented images with the image associated with target nodes. The system relies solely on a monocular camera, ensuring fast map building and navigation using key nodes. Experiments show effective loop detection and navigation in real and simulation environments without pre-training. Compared to a ResNet-based method, this approach improves success rates by 60.2% on average while reducing time and space costs, offering a lightweight solution for robot and human navigation in various scenarios.

</details>


### [253] [Towards reliable subsea object recovery: a simulation study of an auv with a suction-actuated end effector](https://arxiv.org/abs/2601.01106)
*Michele Grimaldi,Yosaku Maeda,Hitoshi Kakami,Ignacio Carlucho,Yvan Petillot,Tomoya Inoue*

Main category: cs.RO

TL;DR: 本文提出并验证了一种基于仿真的深海无人自主物体回收任务流程，通过Stonefish仿真平台模拟6000米海深下无人小型载具（HSV）配合三自由度机械臂及吸取末端执行器的全流程操作，提升了实际投用前的验证效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 深海（尤其是超深海/海沟带）物体回收因极端高压、低能见度、洋流和对精细操作要求极高而具有极大挑战性。实地测试昂贵且高风险，且受制于实验载具的有限性，因此亟需低成本、低风险的验证手段来加速自主水下操作技术的发展。

Method: 利用Stonefish仿真器构建近似真实物理环境（包括动力学、水动力扰动、传感器和目标交互），搭载三自由度机械臂及吸附末端的Hadal Small Vehicle（HSV）进行完整的自主回收任务模拟。控制架构为：载具采用世界坐标PID控制器进行导航和稳定；机械臂采用逆运动学控制并融合加速度前馈，实现载具与机械臂的协调操作。

Result: 仿真结果显示，HSV可自主下潜至6000米，系统性搜索海底，识别出目标后顺利实施吸附回收任务，完成全流程自主操作。

Conclusion: 高保真仿真方法能有效、低风险地评估并优化深海自主干预行为，为日后实际深海投运提供可靠先验验证手段，降低成本和风险，提升深海操作技术发展效率。

Abstract: Autonomous object recovery in the hadal zone is challenging due to extreme hydrostatic pressure, limited visibility and currents, and the need for precise manipulation at full ocean depth. Field experimentation in such environments is costly, high-risk, and constrained by limited vehicle availability, making early validation of autonomous behaviors difficult. This paper presents a simulation-based study of a complete autonomous subsea object recovery mission using a Hadal Small Vehicle (HSV) equipped with a three-degree-of-freedom robotic arm and a suction-actuated end effector. The Stonefish simulator is used to model realistic vehicle dynamics, hydrodynamic disturbances, sensing, and interaction with a target object under hadal-like conditions. The control framework combines a world-frame PID controller for vehicle navigation and stabilization with an inverse-kinematics-based manipulator controller augmented by acceleration feed-forward, enabling coordinated vehicle - manipulator operation. In simulation, the HSV autonomously descends from the sea surface to 6,000 m, performs structured seafloor coverage, detects a target object, and executes a suction-based recovery. The results demonstrate that high-fidelity simulation provides an effective and low-risk means of evaluating autonomous deep-sea intervention behaviors prior to field deployment.

</details>


### [254] [Latent Space Reinforcement Learning for Multi-Robot Exploration](https://arxiv.org/abs/2601.01139)
*Sriram Rajasekar,Ashwini Ratnoo*

Main category: cs.RO

TL;DR: 本文提出了一种结合自动编码器和分层强化学习的新方法，实现多智能体在复杂和未知环境下的自主地图构建，并能很好地扩展到更多智能体、不同环境及受限通信条件。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习多智能体地图构建方法受限于输入维度，难以扩展至连续、复杂的环境，严重限制实际应用。

Method: 1. 利用自动编码器对高保真占据图进行降维，提取精简但具有空间信息的潜在状态向量；2. 提出基于Perlin噪声的过程生成算法，构造复杂拓扑的模拟环境（小行星带、洞穴、森林）以训练系统；3. 采用分层深度强化学习框架实现分散式协作导航，并在信息共享中设计权重共识机制，通过可调信任参数增强系统对累计误差的鲁棒性。

Result: 实验表明，该系统能随着智能体数量增加有效扩展，并具有很强的泛化能力，能够适应结构上与训练差异较大的陌生环境，并且在通信受限时依然表现出良好鲁棒性。

Conclusion: 所提出方法突破了传统方法的输入维度和环境类型限制，实现了多智能体高效、可扩展、强泛化和高鲁棒性的自主地图构建。

Abstract: Autonomous mapping of unknown environments is a critical challenge, particularly in scenarios where time is limited. Multi-agent systems can enhance efficiency through collaboration, but the scalability of motion-planning algorithms remains a key limitation. Reinforcement learning has been explored as a solution, but existing approaches are constrained by the limited input size required for effective learning, restricting their applicability to discrete environments. This work addresses that limitation by leveraging autoencoders to perform dimensionality reduction, compressing high-fidelity occupancy maps into latent state vectors while preserving essential spatial information. Additionally, we introduce a novel procedural generation algorithm based on Perlin noise, designed to generate topologically complex training environments that simulate asteroid fields, caves and forests. These environments are used for training the autoencoder and the navigation algorithm using a hierarchical deep reinforcement learning framework for decentralized coordination. We introduce a weighted consensus mechanism that modulates reliance on shared data via a tuneable trust parameter, ensuring robustness to accumulation of errors. Experimental results demonstrate that the proposed system scales effectively with number of agents and generalizes well to unfamiliar, structurally distinct environments and is resilient in communication-constrained settings.

</details>


### [255] [VISO: Robust Underwater Visual-Inertial-Sonar SLAM with Photometric Rendering for Dense 3D Reconstruction](https://arxiv.org/abs/2601.01144)
*Shu Pan,Simon Archieri,Ahmet Cinar,Jonatan Scharff Willners,Ignacio Carlucho,Yvan Petillot*

Main category: cs.RO

TL;DR: 本论文提出了一种名为VISO的水下SLAM系统，融合了立体相机、IMU和3D声呐，实现了高精度定位与高保真密集三维重建。实验表明，VISO在定位鲁棒性和精度上优于现有方法，并具备实时密集重建能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的水下定位和三维重建因环境视觉挑战（如光照不足、浑浊）而受限，难以实现高精度定位和高质量重建。为应对这些挑战，需开发更鲁棒的多传感器融合SLAM系统。

Method: VISO结合立体相机、IMU和3D声呐三种传感器融合，实现六自由度定位。提出了一种在线粗到细的3D声呐与相机外参标定方法，并设计了声呐点云光度渲染策略，使声呐地图具备视觉信息。

Result: 在实验室水池和开放湖泊中测试，VISO在定位鲁棒性和精度上超越当前最先进的水下及视觉SLAM算法，其实时密集三维重建性能也接近离线重建方法。

Conclusion: VISO极大提升了水下SLAM的定位与重建性能，对水下机器人自主导航和环境建图具有重要应用价值。

Abstract: Visual challenges in underwater environments significantly hinder the accuracy of vision-based localisation and the high-fidelity dense reconstruction. In this paper, we propose VISO, a robust underwater SLAM system that fuses a stereo camera, an inertial measurement unit (IMU), and a 3D sonar to achieve accurate 6-DoF localisation and enable efficient dense 3D reconstruction with high photometric fidelity. We introduce a coarse-to-fine online calibration approach for extrinsic parameters estimation between the 3D sonar and the camera. Additionally, a photometric rendering strategy is proposed for the 3D sonar point cloud to enrich the sonar map with visual information. Extensive experiments in a laboratory tank and an open lake demonstrate that VISO surpasses current state-of-the-art underwater and visual-based SLAM algorithms in terms of localisation robustness and accuracy, while also exhibiting real-time dense 3D reconstruction performance comparable to the offline dense mapping method.

</details>


### [256] [ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation](https://arxiv.org/abs/2601.01155)
*Zhang Shizhe,Liang Jingsong,Zhou Zhitao,Ye Shuhan,Wang Yizhuo,Tan Ming Siang Derek,Chiun Jimmy,Cao Yuhong,Sartoretti Guillaume*

Main category: cs.RO

TL;DR: 本文提出了一种名为ORION的深度强化学习框架，实现多智能体在部分已知环境下的协作自主导航，实验和实机测试均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体导航方法大多假设环境完全已知，对于实际中如仓库等部分已知环境支持有限，现实需求要求智能体在路径规划时兼顾自身目标与环境信息共享能力。

Method: ORION框架融合了先验地图和在线感知，通过共享图编解码器生成统一表示，采用option-critic结构在高层模式下自适应切换低层动作，实现去中心化协作导航。同时提出双阶段协作策略以提高不确定地图下团队整体效率。

Result: 在迷宫和大规模仓库的仿真中，ORION实现了高效的在线协同表现，在多种规模下优于经典和基于学习的基线方法，并经实物机器人测试验证其实用性和鲁棒性。

Conclusion: ORION为多智能体在部分已知环境中协作导航提供了新的有效框架，具备良好的泛化性与实际应用潜力。

Abstract: Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation.

</details>


### [257] [DST-Calib: A Dual-Path, Self-Supervised, Target-Free LiDAR-Camera Extrinsic Calibration Network](https://arxiv.org/abs/2601.01188)
*Zhiwei Huang,Yanwei Fu,Yi Zhou,Xieyuanli Chen,Qijun Chen,Rui Fan*

Main category: cs.RO

TL;DR: 本文提出了一种首个自监督、无需特定标定板、支持在线标定的LiDAR-相机外参标定网络，在多个公开数据集和自采集数据集上都取得了显著优于现有方法的性能，特别是在泛化性方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 目前的LiDAR-相机外参标定方法依赖手工标定板或特定静态场景，泛化性差，难以适应真实环境；且传统方法的数据增强方式存在单边性，容易导致模型泛化性能下降。

Method: 提出“双向数据增强”策略，利用估算出的深度图生成多视角相机图像，从而丰富训练多样性和增强鲁棒性。在此基础上设计了双路自监督标定框架，降低对高精度标签的依赖，实现完全自适应在线标定。同时，将传统的双分支特征提取结构替换为显式关联LiDAR与相机特征的差分图构建，兼顾精度和模型复杂度。

Result: 在五个公开基准数据集以及自采集数据集上进行了大量实验，所提方法在标定准确性和泛化能力上明显优于现有主流方法。

Conclusion: 本文的自监督LiDAR-相机外参标定方法不仅消除了对特殊标定板和高精度标签的依赖，支持在线适配，而且有效提升了在不同数据与场景下的泛化性能，为实际机器人与无人系统应用提供了更高效和可用的标定方案。

Abstract: LiDAR-camera extrinsic calibration is essential for multi-modal data fusion in robotic perception systems. However, existing approaches typically rely on handcrafted calibration targets (e.g., checkerboards) or specific, static scene types, limiting their adaptability and deployment in real-world autonomous and robotic applications. This article presents the first self-supervised LiDAR-camera extrinsic calibration network that operates in an online fashion and eliminates the need for specific calibration targets. We first identify a significant generalization degradation problem in prior methods, caused by the conventional single-sided data augmentation strategy. To overcome this limitation, we propose a novel double-sided data augmentation technique that generates multi-perspective camera views using estimated depth maps, thereby enhancing robustness and diversity during training. Built upon this augmentation strategy, we design a dual-path, self-supervised calibration framework that reduces the dependence on high-precision ground truth labels and supports fully adaptive online calibration. Furthermore, to improve cross-modal feature association, we replace the traditional dual-branch feature extraction design with a difference map construction process that explicitly correlates LiDAR and camera features. This not only enhances calibration accuracy but also reduces model complexity. Extensive experiments conducted on five public benchmark datasets, as well as our own recorded dataset, demonstrate that the proposed method significantly outperforms existing approaches in terms of generalizability.

</details>


### [258] [EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners](https://arxiv.org/abs/2601.01196)
*Shenqi Lu,Liangwei Zhang*

Main category: cs.RO

TL;DR: 本文提出了EduSim-LLM教育平台，实现了将大语言模型与机器人仿真系统融合，通过自然语言指令控制机器人行为，并进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型已广泛应用于自然语言理解，但其高效、安全地应用于机器人控制仍具挑战，限制了人机交互和智能自动化领域的发展。该论文旨在解决自然语言与机器人控制之间的桥接问题，提高机器人系统的可用性和教育价值。

Method: 作者开发了EduSim-LLM平台，将LLM与常用机器人仿真平台CoppeliaSim结合，设计了直接控制和自主控制两种人机交互模型。通过多种预训练语言模型，探索了自然语言指令到可执行机器人动作序列的转换，并借助提示工程提升解析准确率。

Result: 实验结果显示，LLMs可稳定将自然语言转化为结构化机器人操作。在借助提示工程后，指令解析准确率明显提升。在复杂度最高的测试任务中，系统整体精度超过88.9%。

Conclusion: EduSim-LLM有效提高了自然语言到机器人控制的精度，展示了在教育和自动化应用中的实际可行性。大语言模型结合提示工程能显著促进机器人自然语言交互的发展。

Abstract: In recent years, the rapid development of Large Language Models (LLMs) has significantly enhanced natural language understanding and human-computer interaction, creating new opportunities in the field of robotics. However, the integration of natural language understanding into robotic control is an important challenge in the rapid development of human-robot interaction and intelligent automation industries. This challenge hinders intuitive human control over complex robotic systems, limiting their educational and practical accessibility. To address this, we present the EduSim-LLM, an educational platform that integrates LLMs with robot simulation and constructs a language-drive control model that translates natural language instructions into executable robot behavior sequences in CoppeliaSim. We design two human-robot interaction models: direct control and autonomous control, conduct systematic simulations based on multiple language models, and evaluate multi-robot collaboration, motion planning, and manipulation capabilities. Experiential results show that LLMs can reliably convert natural language into structured robot actions; after applying prompt-engineering templates instruction-parsing accuracy improves significantly; as task complexity increases, overall accuracy rate exceeds 88.9% in the highest complexity tests.

</details>


### [259] [SAHA: Supervised Autonomous HArvester for selective forest thinning](https://arxiv.org/abs/2601.01282)
*Fang Nan,Meher Malladi,Qingqing Li,Fan Yang,Joonas Juola,Tiziano Guadagnino,Jens Behley,Cesar Cadena,Cyrill Stachniss,Marco Hutter*

Main category: cs.RO

TL;DR: 本论文提出了一种基于小型机器人收割机（SAHA）的半自动森林选择性间伐解决方案，并在真实北欧森林环境中经过了大规模现场验证。


<details>
  <summary>Details</summary>
Motivation: 传统森林管理中的选择性间伐高度依赖人工，不仅耗时耗力，还需要大量经验，迫切需要高效、自动化的解决方案以提升森林健康和生产力。

Method: 作者以4.5吨的小型收割机为基础，通过硬件改装和集成先进的感知、导航、状态估计、地形可通行性判断等模块，实现了收割机对森林环境的自主感知和精准控制，结合学习和模型驱动方法应对复杂林地环境。

Result: 论文在北欧真实森林环境中进行了长距离的自主作业实验，展现了机器人系统能够高效地自动导航并定位、处理目标树木，验证了其实际应用能力。

Conclusion: 该系统为森林管理自动化提供了有效技术路径，现场试验结果表明其具有良好的实际应用前景，并总结了进一步提升森林机器人管理的经验和建议。

Abstract: Forestry plays a vital role in our society, creating significant ecological, economic, and recreational value. Efficient forest management involves labor-intensive and complex operations. One essential task for maintaining forest health and productivity is selective thinning, which requires skilled operators to remove specific trees to create optimal growing conditions for the remaining ones. In this work, we present a solution based on a small-scale robotic harvester (SAHA) designed for executing this task with supervised autonomy. We build on a 4.5-ton harvester platform and implement key hardware modifications for perception and automatic control. We implement learning- and model-based approaches for precise control of hydraulic actuators, accurate navigation through cluttered environments, robust state estimation, and reliable semantic estimation of terrain traversability. Integrating state-of-the-art techniques in perception, planning, and control, our robotic harvester can autonomously navigate forest environments and reach targeted trees for selective thinning. We present experimental results from extensive field trials over kilometer-long autonomous missions in northern European forests, demonstrating the harvester's ability to operate in real forests. We analyze the performance and provide the lessons learned for advancing robotic forest management.

</details>


### [260] [Online Estimation and Manipulation of Articulated Objects](https://arxiv.org/abs/2601.01438)
*Russell Buchanan,Adrian Röfer,João Moura,Abhinav Valada,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 本文提出了一种融合视觉和本体感知的新方法，使机器人能够更好地操作之前未见过的铰接物体，如抽屉和冰箱门。通过结合深度学习预测和操作过程中的感知，显著提升了机器人操作成功率。


<details>
  <summary>Details</summary>
Motivation: 当前，服务机器人在操作家居环境中的多关节物体（如抽屉、冰箱门）时存在困难，主要因为如何在未见过的物体上推断和适应其铰接结构仍具有挑战。以往方法偏重视觉先验或单纯依赖运动观测，效果有限。迫切需要无需先前交互经验即可高效推断和操作新物体的更优方法。

Method: 本文提出采用基于因子图的在线估算方法，将视觉深度学习预测与操作时的本体感知（运动和力觉）结合，利用螺旋理论对物体多关节结构进行解析建模。机器人在未接触时由视觉先予以预测，操作中则根据传感器数据实时更新估计。通过模拟和真实机器人操作实验评估了该方法。

Result: 实验结果显示，所提出方法能实现闭环估算与操作，机器人能自主打开先前未见的抽屉类多关节物体。在真实硬件中的自主操作成功率达到75%。

Conclusion: 本文新方法有效融合视觉和操作感知信息，提升了机器人处理未知多关节物体的能力，为服务机器人家居自动化任务提供了更稳健的解决方案。

Abstract: From refrigerators to kitchen drawers, humans interact with articulated objects effortlessly every day while completing household chores. For automating these tasks, service robots must be capable of manipulating arbitrary articulated objects. Recent deep learning methods have been shown to predict valuable priors on the affordance of articulated objects from vision. In contrast, many other works estimate object articulations by observing the articulation motion, but this requires the robot to already be capable of manipulating the object. In this article, we propose a novel approach combining these methods by using a factor graph for online estimation of articulation which fuses learned visual priors and proprioceptive sensing during interaction into an analytical model of articulation based on Screw Theory. With our method, a robotic system makes an initial prediction of articulation from vision before touching the object, and then quickly updates the estimate from kinematic and force sensing during manipulation. We evaluate our method extensively in both simulations and real-world robotic manipulation experiments. We demonstrate several closed-loop estimation and manipulation experiments in which the robot was capable of opening previously unseen drawers. In real hardware experiments, the robot achieved a 75% success rate for autonomous opening of unknown articulated objects.

</details>


### [261] [AIMS: An Adaptive Integration of Multi-Sensor Measurements for Quadrupedal Robot Localization](https://arxiv.org/abs/2601.01561)
*Yujian Qiu,Yuqiu Mu,Wen Yang,Hao Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种用于四足机器人在狭窄隧道环境下定位的自适应LiDAR-IMU-腿部里程计融合方法，以提高定位精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在狭长且同质性强的隧道环境中，传统的LiDAR测量常因缺乏强几何约束而导致运动估计误差累积，影响四足机器人的精确定位。

Method: 作者提出了AIMS方法，基于误差状态卡尔曼滤波框架，融合了LiDAR、IMU和腿部里程计数据，并通过在线退化感知自适应调整测量噪声协方差矩阵，以适应不同场景的定位退化情况。

Result: 实验证明，在狭窄走廊环境中，所提方法相比现有技术精度更高、鲁棒性更强。

Conclusion: AIMS方法能够显著提升四足机器人在地形退化环境中的定位性能，有效解决了LiDAR约束弱带来的长期定位累计误差问题。

Abstract: This paper addresses the problem of accurate localization for quadrupedal robots operating in narrow tunnel-like environments. Due to the long and homogeneous characteristics of such scenarios, LiDAR measurements often provide weak geometric constraints, making traditional sensor fusion methods susceptible to accumulated motion estimation errors. To address these challenges, we propose AIMS, an adaptive LiDAR-IMU-leg odometry fusion method for robust quadrupedal robot localization in degenerate environments. The proposed method is formulated within an error-state Kalman filtering framework, where LiDAR and leg odometry measurements are integrated with IMU-based state prediction, and measurement noise covariance matrices are adaptively adjusted based on online degeneracy-aware reliability assessment. Experimental results obtained in narrow corridor environments demonstrate that the proposed method improves localization accuracy and robustness compared with state-of-the-art approaches.

</details>


### [262] [HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller](https://arxiv.org/abs/2601.01577)
*Tran Tien Dat,Nguyen Hai An,Nguyen Khanh Viet Dung,Nguyen Duy Duc*

Main category: cs.RO

TL;DR: 本文提出了一种基于JEPA自监督学习的RNN世界模型Hanoi-World，在自动驾驶场景下实现了安全性意识和高效推理，明显降低碰撞率，优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法在自动驾驶领域数据需求大、效果不佳且稳定性差，尤其难以理解安全概念；像素级方法更容易过拟合噪声。自监督学习模拟人脑能力，结合JEPA方法有望突破这些瓶颈。

Method: 提出Hanoi-World世界模型，采用JEPA自监督方法，通过RNN结构进行长期驾驶规划与推理。

Result: 在Highway-Env多个环境下实验，显示该方法能制定安全优先的驾驶策略，碰撞率在与主流方法相比有明显降低。

Conclusion: 基于JEPA的世界模型能在自动驾驶任务中有效进行安全感知和规划，在碰撞率、安全性和推理效率方面优于SOTA基线方法。

Abstract: Current attempts of Reinforcement Learning for Autonomous Controller are data-demanding while the results are under-performed, unstable, and unable to grasp and anchor on the concept of safety, and over-concentrating on noise features due to the nature of pixel reconstruction. While current Self-Supervised Learningapproachs that learning on high-dimensional representations by leveraging the JointEmbedding Predictive Architecture (JEPA) are interesting and an effective alternative, as the idea mimics the natural ability of the human brain in acquiring new skill usingimagination and minimal samples of observations. This study introduces Hanoi-World, a JEPA-based world model that using recurrent neural network (RNN) formaking longterm horizontal planning with effective inference time. Experimentsconducted on the Highway-Env package with difference enviroment showcase the effective capability of making a driving plan while safety-awareness, with considerablecollision rate in comparison with SOTA baselines

</details>


### [263] [Action-Sketcher: From Reasoning to Action via Visual Sketches for Long-Horizon Robotic Manipulation](https://arxiv.org/abs/2601.01618)
*Huajie Tan,Peterson Co,Yijie Xu,Shanyu Rong,Yuheng Ji,Cheng Chi,Xiansheng Chen,Qiongyu Zhang,Zhongxia Zhao,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出了Action-Sketcher框架，通过在机器人操作中引入可视化草图（Visual Sketch）来显式呈现空间意图，增强视觉-语言-动作政策的可解释性与鲁棒性，并在多种复杂任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言-动作（VLA）模型通常只依赖文本线索，缺乏对操作意图的可视化显式表达，导致在杂乱场景或复杂长期任务中，机器人难以准确定位目标、合理分解任务且缺乏因果性解释能力。因此，有必要开发一种方法，实现任务意图的外化与更强的任务推理与人机交互能力。

Method: 作者提出了Visual Sketch作为视觉中的中间表示，利用点、框、箭头和类型关系将空间意图外化。基于此，设计了Action-Sketcher框架，采用“观察-思考-草图-行动”的循环方法，通过自适应token门控策略协调推理、草图修正与动作执行。训练上，构建了包含图片、文本、草图监督和动作序列的大型数据集，采用多阶段课程式训练，结合多模态一致性、语言到草图的一致性及模仿学习与草图到动作的强化学习提升鲁棒性。

Result: 在模拟和真实世界的多对象、杂乱场景任务中，Action-Sketcher表现出更高的长期任务成功率、更强的动态鲁棒性，并通过可编辑草图与分步计划显著提升了解释性。

Conclusion: Action-Sketcher通过引入显式空间草图，将操作意图外化，显著提高了长时序机器操作的理解、可解释性和任务执行的鲁棒性，为复杂环境中的人机协作及任务规划提供了新思路。

Abstract: Long-horizon robotic manipulation is increasingly important for real-world deployment, requiring spatial disambiguation in complex layouts and temporal resilience under dynamic interaction. However, existing end-to-end and hierarchical Vision-Language-Action (VLA) policies often rely on text-only cues while keeping plan intent latent, which undermines referential grounding in cluttered or underspecified scenes, impedes effective task decomposition of long-horizon goals with close-loop interaction, and limits causal explanation by obscuring the rationale behind action choices. To address these issues, we first introduce Visual Sketch, an implausible visual intermediate that renders points, boxes, arrows, and typed relations in the robot's current views to externalize spatial intent, connect language to scene geometry. Building on Visual Sketch, we present Action-Sketcher, a VLA framework that operates in a cyclic See-Think-Sketch-Act workflow coordinated by adaptive token-gated strategy for reasoning triggers, sketch revision, and action issuance, thereby supporting reactive corrections and human interaction while preserving real-time action prediction. To enable scalable training and evaluation, we curate diverse corpus with interleaved images, text, Visual Sketch supervision, and action sequences, and train Action-Sketcher with a multi-stage curriculum recipe that combines interleaved sequence alignment for modality unification, language-to-sketch consistency for precise linguistic grounding, and imitation learning augmented with sketch-to-action reinforcement for robustness. Extensive experiments on cluttered scenes and multi-object tasks, in simulation and on real-world tasks, show improved long-horizon success, stronger robustness to dynamic scene changes, and enhanced interpretability via editable sketches and step-wise plans. Project website: https://action-sketcher.github.io

</details>


### [264] [DemoBot: Efficient Learning of Bimanual Manipulation with Dexterous Hands From Third-Person Human Videos](https://arxiv.org/abs/2601.01651)
*Yucheng Xu,Xiaofeng Mao,Elle Miller,Xinyu Yi,Yang Li,Zhibin Li,Robert B. Fisher*

Main category: cs.RO

TL;DR: 本文提出了一种名为DemoBot的新型学习框架，使双臂多指机器人系统能够仅通过单个无标注RGB-D视频演示，习得复杂操作技能。通过从原始视频中提取手与物体的运动轨迹，并结合新颖的强化学习方法，实现对轨迹的精细优化，无需从零开始训练，成功解决了长时间操作技能学习的难题。


<details>
  <summary>Details</summary>
Motivation: 当前机器人模仿学习面临高成本人工标注、长期任务难以高效学习等挑战。本文旨在降低机器人复杂操作学习的门槛，推动从无标注视频直接习得人类操作技能，并解决长时序、双手等实际装配任务的高效模仿问题。

Method: 1) 首先通过视觉处理技术，从RGB-D视频原始数据中提取双手与物体的结构化运动轨迹，将其作为运动先验信息；2) 设计了新的强化学习管线，通过与环境丰富的接触交互，基于轨迹先验进一步优化动作策略；3) 引入时间分段的RL方法，保证状态轨迹与演示对齐；4) 采用成功门控重置策略，平衡简单技能的精细优化与后续任务阶段的探索；5) 提出事件驱动的奖励机制，动态调节奖励阈值，提升高精度操作能力。

Result: 所提出的方法成功在多种长时序、同步与异步的双手装配任务中实现了机器人对人类视频技能的端到端迁移，并展现了良好的扩展性和复杂任务适应能力。

Conclusion: 新颖的视频处理+强化学习融合架构，能够让机器人系统以非标注的方式有效学习复杂双手多指操作任务，为高效、可扩展的人类技能迁移提供了新路径。

Abstract: This work presents DemoBot, a learning framework that enables a dual-arm, multi-finger robotic system to acquire complex manipulation skills from a single unannotated RGB-D video demonstration. The method extracts structured motion trajectories of both hands and objects from raw video data. These trajectories serve as motion priors for a novel reinforcement learning (RL) pipeline that learns to refine them through contact-rich interactions, thereby eliminating the need to learn from scratch. To address the challenge of learning long-horizon manipulation skills, we introduce: (1) Temporal-segment based RL to enforce temporal alignment of the current state with demonstrations; (2) Success-Gated Reset strategy to balance the refinement of readily acquired skills and the exploration of subsequent task stages; and (3) Event-Driven Reward curriculum with adaptive thresholding to guide the RL learning of high-precision manipulation. The novel video processing and RL framework successfully achieved long-horizon synchronous and asynchronous bimanual assembly tasks, offering a scalable approach for direct skill acquisition from human videos.

</details>


### [265] [VisuoTactile 6D Pose Estimation of an In-Hand Object using Vision and Tactile Sensor Data](https://arxiv.org/abs/2601.01675)
*Snehal s. Dikhale,Karankumar Patel,Daksh Dhingra,Itoshi Naramura,Akinobu Hayashi,Soshi Iba,Nawid Jamali*

Main category: cs.RO

TL;DR: 该论文提出了一种结合视觉与触觉数据的机器人手中6D姿态估计方法，利用点云表示触觉数据并通过像素级融合网络提升姿态估计精度，同时完成了合成数据集的扩展，实现了训练到真实环境的良好泛化。


<details>
  <summary>Details</summary>
Motivation: 单纯依赖视觉信息进行6D姿态估计难以应对机器人手抓持时产生的遮挡问题，而许多机器人配备有触觉传感器，可以用来辅助提升姿态估计。如何标准化触觉数据表示与多模态数据融合是当前的研究难点。

Method: 方法上，论文提出以点云方式标准化触觉数据表示，并设计了基于像素级密集融合的网络架构来融合视觉与触觉信息。同时扩展用于合成数据集的NVIDIA深度学习数据生成器，实现视觉、触觉点云数据的合成和配对。

Result: 实验表明：相较仅用视觉数据，融合触觉数据能更有效提升6D姿态估计精度，同时所提出的模型可从合成数据良好泛化到真实环境中的机器人。

Conclusion: 结合视觉与触觉点云的融合网络能够有效补充视觉不足，提升机器人手中物体6D姿态估计的准确性，并具有良好的实际应用前景。

Abstract: Knowledge of the 6D pose of an object can benefit in-hand object manipulation. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot's grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this paper, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot's hand. To address challenges like lack of standard representation for tactile data and sensor fusion, we propose the use of point clouds to represent object surfaces in contact with the tactile sensor and present a network architecture based on pixel-wise dense fusion. We also extend NVIDIA's Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and corresponding tactile point clouds. Results suggest that using tactile data in addition to vision data improves the 6D pose estimate, and our network generalizes successfully from synthetic training to real physical robots.

</details>


### [266] [Explicit World Models for Reliable Human-Robot Collaboration](https://arxiv.org/abs/2601.01705)
*Kenneth Kwok,Basura Fernando,Qianli Xu,Vigneshwaran Subbaraju,Dongkyu Choi,Boon Kiat Quek*

Main category: cs.RO

TL;DR: 本文探讨了在噪声感知、指令歧义及人机交互环境下，机器人的鲁棒性问题，主张通过构建人机共同认知的“显式世界模型”来提升机器人可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的形式化验证方法难以应对人机交互中的主观性和情境多变，实际环境中的机器人需更好地理解和响应人的意图，确保其行为与人类期望一致。

Method: 提出抛弃单一强调模型可预测性与鲁棒性的传统方法，转而以构建并持续更新一套反映人类与AI之间共同认知（common ground）的“显式世界模型”为核心，辅助机器人感知并对人类意图做出一致、易理解、贴合期望的反应。

Result: 通过构建共享的世界模型，机器人能够在社会性、多模态且动态变化的环境中，更好地与人类互动，并对不断变化的期望做出提前感知和自我调整。

Conclusion: 提升机器人的可靠性不能仅依赖形式化方法，而应通过以“显式世界模型”为基础的交互机制，使机器人持续对齐和响应人类期望，从而在真实人类环境中实现语境相关的可靠性。

Abstract: This paper addresses the topic of robustness under sensing noise, ambiguous instructions, and human-robot interaction. We take a radically different tack to the issue of reliable embodied AI: instead of focusing on formal verification methods aimed at achieving model predictability and robustness, we emphasise the dynamic, ambiguous and subjective nature of human-robot interactions that requires embodied AI systems to perceive, interpret, and respond to human intentions in a manner that is consistent, comprehensible and aligned with human expectations. We argue that when embodied agents operate in human environments that are inherently social, multimodal, and fluid, reliability is contextually determined and only has meaning in relation to the goals and expectations of humans involved in the interaction. This calls for a fundamentally different approach to achieving reliable embodied AI that is centred on building and updating an accessible "explicit world model" representing the common ground between human and AI, that is used to align robot behaviours with human expectations.

</details>


### [267] [Simulations and Advancements in MRI-Guided Power-Driven Ferric Tools for Wireless Therapeutic Interventions](https://arxiv.org/abs/2601.01726)
*Wenhui Chu,Aobo Jin,Hardik A. Gohel*

Main category: cs.RO

TL;DR: 本研究开发了一套专为MRI引导的血管内介入手术而设计的机器人计算系统，可实时处理和分析MR图像，辅助机器人在复杂血管网络中高精度、安全地导航。


<details>
  <summary>Details</summary>
Motivation: 在强磁场环境下设计精准、稳定并能与MRI无缝集成的机器人，解决血管内介入手术中现有技术对安全性和操作精度的限制。

Method: 提出并实现了一个基于Qt框架与C/C++开发的平台，集成了MR图像处理、血管网络虚拟路径生成、个性化磁场梯度控制等模块。通过虚拟路径与安全边界设定，实现机器人在人体血管内的智能导航，同时对不同血流特征进行适应性调整。

Result: 平台可根据实际血管形态快速生成磁场控制方案，实现介入设备的安全、精细操控。系统亦可预测预设路径下的手术安全性及可行性，提升整体操作效率与安全性。

Conclusion: 该系统实现了医疗成像与机器人辅助手术的高效融合，为血管内介入手术带来更高的精度与安全保障，是该领域的重要进步。

Abstract: Designing a robotic system that functions effectively within the specific environment of a Magnetic Resonance Imaging (MRI) scanner requires solving numerous technical issues, such as maintaining the robot's precision and stability under strong magnetic fields. This research focuses on enhancing MRI's role in medical imaging, especially in its application to guide intravascular interventions using robot-assisted devices. A newly developed computational system is introduced, designed for seamless integration with the MRI scanner, including a computational unit and user interface. This system processes MR images to delineate the vascular network, establishing virtual paths and boundaries within vessels to prevent procedural damage. Key findings reveal the system's capability to create tailored magnetic field gradient patterns for device control, considering the vessel's geometry and safety norms, and adapting to different blood flow characteristics for finer navigation. Additionally, the system's modeling aspect assesses the safety and feasibility of navigating pre-set vascular paths. Conclusively, this system, based on the Qt framework and C/C++, with specialized software modules, represents a major step forward in merging imaging technology with robotic aid, significantly enhancing precision and safety in intravascular procedures.

</details>


### [268] [AlignDrive: Aligned Lateral-Longitudinal Planning for End-to-End Autonomous Driving](https://arxiv.org/abs/2601.01762)
*Yanhao Wu,Haoyang Zhang,Fei He,Rui Wu,Congpei Qiu,Liang Gao,Wei Ke,Tong Zhang*

Main category: cs.RO

TL;DR: 提出一种新的端到端自动驾驶级联框架，将纵向规划明确条件于驾驶路径，实现了更协调、避免碰撞的横纵向联合规划，对Bench2Drive基准测试显著提升了得分与成功率。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶规划模型将横向与纵向预测解耦并行执行，这导致路径与速度规划协调性差，以及纵向规划未能有效利用路径信息，造成编码冗余。该论文旨在解决这些问题，提升自动驾驶系统的协调性与效率。

Method: 提出了级联框架，将纵向规划显式依赖于驾驶路径，通过路径条件化的方法，将驾驶路径信息直接引入纵向规划。同时，模型沿路径预测纵向位移而非二维轨迹，提高横纵向耦合性。此外，引入以规划为导向的数据增强策略，通过模拟关键安全场景（如车辆加塞）并重新标注纵向目标，提升模型对罕见事件的鲁棒性和安全性。

Result: 在Bench2Drive基准上，该方法取得了SOTA成绩，驾驶得分达到89.07，成功率为73.18%，在协调性与安全性方面优于现有方法。

Conclusion: 通过路径条件化纵向规划和数据增强方法，有效提升了端到端自动驾驶系统在复杂环境下的协调性、安全性与整体性能，具有广泛应用前景。

Abstract: End-to-end autonomous driving has rapidly progressed, enabling joint perception and planning in complex environments. In the planning stage, state-of-the-art (SOTA) end-to-end autonomous driving models decouple planning into parallel lateral and longitudinal predictions. While effective, this parallel design can lead to i) coordination failures between the planned path and speed, and ii) underutilization of the drive path as a prior for longitudinal planning, thus redundantly encoding static information. To address this, we propose a novel cascaded framework that explicitly conditions longitudinal planning on the drive path, enabling coordinated and collision-aware lateral and longitudinal planning. Specifically, we introduce a path-conditioned formulation that explicitly incorporates the drive path into longitudinal planning. Building on this, the model predicts longitudinal displacements along the drive path rather than full 2D trajectory waypoints. This design simplifies longitudinal reasoning and more tightly couples it with lateral planning. Additionally, we introduce a planning-oriented data augmentation strategy that simulates rare safety-critical events, such as vehicle cut-ins, by adding agents and relabeling longitudinal targets to avoid collision. Evaluated on the challenging Bench2Drive benchmark, our method sets a new SOTA, achieving a driving score of 89.07 and a success rate of 73.18%, demonstrating significantly improved coordination and safety

</details>


### [269] [DisCo-FLoc: Using Dual-Level Visual-Geometric Contrasts to Disambiguate Depth-Aware Visual Floorplan Localization](https://arxiv.org/abs/2601.01822)
*Shiyong Meng,Tao Zou,Bolei Chen,Chaoxu Mu,Jianxin Wang*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉—几何双重对比的方法（DisCo-FLoc），在无需额外语义标注的情况下，有效提升了室内视觉定位的准确性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉定位方法常常依赖几何先验或稀疏语义信息，但在极简主义、重复结构的平面图中仍然存在定位歧义，并且高成本且有限的语义标注限制了方法的普适性。

Method: 提出DisCo-FLoc方法，采用基于射线回归的深度感知定位预测，结合位置和方向级别的对比学习，将视觉特征与平面图中的几何结构严格匹配，从而消除定位歧义，不需要额外的语义标签。

Result: 在两个标准视觉定位基准上，DisCo-FLoc方法在准确率和稳健性方面均显著优于现有最优的基于语义的方法。

Conclusion: DisCo-FLoc能够有效解决极简主义平面图中由重复结构引起的定位歧义，提升定位准确性和稳健性，同时降低了对昂贵语义标注的依赖，具有较强的实际应用价值。

Abstract: Since floorplan data is readily available, long-term persistent, and robust to changes in visual appearance, visual Floorplan Localization (FLoc) has garnered significant attention. Existing methods either ingeniously match geometric priors or utilize sparse semantics to reduce FLoc uncertainty. However, they still suffer from ambiguous FLoc caused by repetitive structures within minimalist floorplans. Moreover, expensive but limited semantic annotations restrict their applicability. To address these issues, we propose DisCo-FLoc, which utilizes dual-level visual-geometric Contrasts to Disambiguate depth-aware visual Floc, without requiring additional semantic labels. Our solution begins with a ray regression predictor tailored for ray-casting-based FLoc, predicting a series of FLoc candidates using depth estimation expertise. In addition, a novel contrastive learning method with position-level and orientation-level constraints is proposed to strictly match depth-aware visual features with the corresponding geometric structures in the floorplan. Such matches can effectively eliminate FLoc ambiguity and select the optimal imaging pose from FLoc candidates. Exhaustive comparative studies on two standard visual Floc benchmarks demonstrate that our method outperforms the state-of-the-art semantic-based method, achieving significant improvements in both robustness and accuracy.

</details>


### [270] [CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios](https://arxiv.org/abs/2601.01872)
*Hongbo Duan,Shangyi Luo,Zhiyuan Deng,Yanbo Chen,Yuanhao Chiang,Yi Liu,Fangming Liu,Xueqian Wang*

Main category: cs.RO

TL;DR: 本文提出了CausalNav，一种针对动态户外环境的基于场景图的自主语义导航框架，能够实现大规模环境下的稳健和高效导航。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在大规模户外自主语义导航中面临语义推理、动态变化和长期稳定性等难题。作者希望通过构建语义场景知识库，提升导航的智能化和鲁棒性。

Method: 提出CausalNav框架，利用大模型（LLM）构建多层次语义场景图（Embodied Graph），将粗粒度地图数据与细粒度对象实体融合。该图作为可检索的知识库，结合RAG（基于检索的生成），支持开放词汇导航和长距离规划，并在分层规划与图构建中显式处理动态目标，实现实时场景图更新以适应环境变化。

Result: 在仿真和真实世界环境中进行了大量实验，CausalNav展现出优异的鲁棒性和高效性。

Conclusion: CausalNav能够实现动态户外环境下基于语言的自主导航，在稳健性和效率方面优于现有方法。

Abstract: Autonomous language-guided navigation in large-scale outdoor environments remains a key challenge in mobile robotics, due to difficulties in semantic reasoning, dynamic conditions, and long-term stability. We propose CausalNav, the first scene graph-based semantic navigation framework tailored for dynamic outdoor environments. We construct a multi-level semantic scene graph using LLMs, referred to as the Embodied Graph, that hierarchically integrates coarse-grained map data with fine-grained object entities. The constructed graph serves as a retrievable knowledge base for Retrieval-Augmented Generation (RAG), enabling semantic navigation and long-range planning under open-vocabulary queries. By fusing real-time perception with offline map data, the Embodied Graph supports robust navigation across varying spatial granularities in dynamic outdoor environments. Dynamic objects are explicitly handled in both the scene graph construction and hierarchical planning modules. The Embodied Graph is continuously updated within a temporal window to reflect environmental changes and support real-time semantic navigation. Extensive experiments in both simulation and real-world settings demonstrate superior robustness and efficiency.

</details>


### [271] [From Metrics to Meaning: Insights from a Mixed-Methods Field Experiment on Retail Robot Deployment](https://arxiv.org/abs/2601.01946)
*Sichao Song,Yuki Okafuji,Takuya Iwamoto,Jun Baba,Hiroshi Ishiguro*

Main category: cs.RO

TL;DR: 本文通过在真实家居商店中部署对话式服务机器人，考察了机器人对顾客行为和员工服务流程的影响，结合定量实验和员工访谈分析，探讨机器人在零售场景下的作用及优化建议。


<details>
  <summary>Details</summary>
Motivation: 面向高触感零售业，探索服务机器人能否为零售商带来更好的客户停留与引流效果，以及与员工协作过程中面临的实际困难与机遇。

Method: 采用混合方法（定量+定性）：实际门店内跨12天交替设置三种条件：无机器人、机器人单独、机器人+引流装置。通过视频标注，记录顾客从经过到进店购买全过程，结合六位员工事后访谈解释数据模式。

Result: 引入机器人显著提高了顾客驻足率（加装引流装置时效果最明显），但停留顾客后续被员工主动接洽、进店、参与体验和成交的比例反而下降。访谈发现，员工避免打断机器人与顾客的互动，应对交流延迟时机变得模糊，且儿童对机器人兴趣导致很多驻足止步门口。引流装置提高了能见度，但也使顾客与机器人的互动更多停留在门口微型空间。

Conclusion: 对话式服务机器人可有效吸引顾客驻足并提升门店可见度，但实际带动顾客进店成交需解决人机协作的时机把控和空间引导等问题。作者对顾客初步兴趣到进店整个流程进行了整合建模，并提出零售端部署服务机器人的实际操作建议和优化方向。

Abstract: We report a mixed-methods field experiment of a conversational service robot deployed under everyday staffing discretion in a live bedding store. Over 12 days we alternated three conditions--Baseline (no robot), Robot-only, and Robot+Fixture--and video-annotated the service funnel from passersby to purchase. An explanatory sequential design then used six post-experiment staff interviews to interpret the quantitative patterns.
  Quantitatively, the robot increased stopping per passerby (highest with the fixture), yet clerk-led downstream steps per stopper--clerk approach, store entry, assisted experience, and purchase--decreased. Interviews explained this divergence: clerks avoided interrupting ongoing robot-customer talk, struggled with ambiguous timing amid conversational latency, and noted child-centered attraction that often satisfied curiosity at the doorway. The fixture amplified visibility but also anchored encounters at the threshold, creating a well-defined micro-space where needs could ``close'' without moving inside.
  We synthesize these strands into an integrative account from the initial show of interest on the part of a customer to their entering the store and derive actionable guidance. The results advance the understanding of interactions between customers, staff members, and the robot and offer practical recommendations for deploying service robots in high-touch retail.

</details>


### [272] [Learning Diffusion Policy from Primitive Skills for Robot Manipulation](https://arxiv.org/abs/2601.01948)
*Zhihao Gu,Ming Yang,Difan Zou,Dong Xu*

Main category: cs.RO

TL;DR: 本文提出了一种技能条件扩散策略（SDP），结合可解释的技能学习与条件动作规划，实现更高效和对齐的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散策略依赖全局指令生成短期控制信号，导致动作与任务指令对齐不佳。作者认为，将任务分解为原始技能如“向上移动、张开夹爪”更具直观性且易于学习。

Method: 方法包含三个核心部分：1）抽象出8种可复用的原始技能；2）利用视觉语言模型提取视觉和语言中的离散表达；3）通过轻量级路由网络为每个状态分配合适技能，并采取单一技能策略以保证动作生成与技能对齐。

Result: 在两个模拟基准测试及真实机器人操作实验中，SDP在技能一致性和任务完成度上均显著优于最新方法。

Conclusion: SDP通过将复杂任务分解为可复用的原始技能，并采用技能条件化扩散策略，实现了更加一致和高效的机器人动作生成，推动了基于技能的机器人学习新范式发展。

Abstract: Diffusion policies (DP) have recently shown great promise for generating actions in robotic manipulation. However, existing approaches often rely on global instructions to produce short-term control signals, which can result in misalignment in action generation. We conjecture that the primitive skills, referred to as fine-grained, short-horizon manipulations, such as ``move up'' and ``open the gripper'', provide a more intuitive and effective interface for robot learning. To bridge this gap, we propose SDP, a skill-conditioned DP that integrates interpretable skill learning with conditional action planning. SDP abstracts eight reusable primitive skills across tasks and employs a vision-language model to extract discrete representations from visual observations and language instructions. Based on them, a lightweight router network is designed to assign a desired primitive skill for each state, which helps construct a single-skill policy to generate skill-aligned actions. By decomposing complex tasks into a sequence of primitive skills and selecting a single-skill policy, SDP ensures skill-consistent behavior across diverse tasks. Extensive experiments on two challenging simulation benchmarks and real-world robot deployments demonstrate that SDP consistently outperforms SOTA methods, providing a new paradigm for skill-based robot learning with diffusion policies.

</details>


### [273] [What you reward is what you learn: Comparing rewards for online speech policy optimization in public HRI](https://arxiv.org/abs/2601.01969)
*Sichao Song,Yuki Okafuji,Kaito Ariu,Amy Koike*

Main category: cs.RO

TL;DR: 本文探索了如何通过在线学习方法自适应地优化社交服务机器人在开放环境中的语音策略。通过实际部署实验，比较不同奖励机制下的最优语音政策选择，并给出实际部署建议。


<details>
  <summary>Details</summary>
Motivation: 在开放多变的人类-机器人互动环境中，传统的固定参数很难应对环境变化，如何能高效且灵活地优化机器人的语音交流策略以提升用户体验具有重要意义。

Method: 将语音策略的在线优化建模为多臂老虎机问题，通过Thompson采样在六种由语速（慢/正常/快）与详细程度（简明/详细）组合而成的动作间选择。设置三种奖励方式（用户评分、对话收尾与多回合对话），并结合现场部署与基于视频的离线数据分析各情境因素对策略表现的影响。

Result: 三种不同奖励分别引导了截然不同的策略分布及对话行为。在线实验获得了实际用户互动数据，并通过离线分析进一步发现环境因素（如人群级别、组规模）对策略有显著影响。

Conclusion: 针对不同奖励目标，机器人的语音策略应动态自适应。作者总结出一套可操作的设计经验，可有效指导服务型社交机器人在现实场景中实现在线语音策略优化。

Abstract: Designing policies that are both efficient and acceptable for conversational service robots in open and diverse environments is non-trivial. Unlike fixed, hand-tuned parameters, online learning can adapt to non-stationary conditions. In this paper, we study how to adapt a social robot's speech policy in the wild. During a 12-day in-situ deployment with over 1,400 public encounters, we cast online policy optimization as a multi-armed bandit problem and use Thompson sampling to select among six actions defined by speech rate (slow/normal/fast) and verbosity (concise/detailed). We compare three complementary binary rewards--Ru (user rating), Rc (conversation closure), and Rt (>=2 turns)--and show that each induces distinct arm distributions and interaction behaviors. We complement the online results with offline evaluations that analyze contextual factors (e.g., crowd level, group size) using video-annotated data. Taken together, we distill ready-to-use design lessons for deploying online optimization of speech policies in real public HRI settings.

</details>


### [274] [Deep Robust Koopman Learning from Noisy Data](https://arxiv.org/abs/2601.01971)
*Aditya Singh,Rajpal Singh,Jishnu Keshavan*

Main category: cs.RO

TL;DR: 本文提出了一种基于自编码器的神经网络架构，能够在有噪声的数据中同时学习合适的Koopman提升函数和低偏置Koopman算子，从而提升了非线性系统的预测和跟踪精度，对噪声具有更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Koopman算子理论为非线性系统建立线性描述提供了有效的数据驱动方法，但在实际中受到噪声影响严重，导致算子的预测与跟踪性能大幅下降，需要新的方法降低噪声带来的偏置。

Method: 该论文提出用自编码器神经网络架构，联合学习与系统正、反时间动力学一致的Koopman提升基（观测器函数）和低偏置Koopman算子，并基于学到的动力学合成鲁棒的Koopman算子；通过理论分析和仿真实验评估偏置降低效果。

Result: 在多个串联机械臂系统的动力学预测和跟踪控制仿真中，所提方法较当前主流技术表现出更优的鲁棒性和预测精度，特别是在不同噪声水平条件下；并通过Franka FR3 7自由度机械臂的实验验证其实际有效性。

Conclusion: 文中提出的基于自编码器的联合学习方法能够显著降低训练噪声引入的偏置，提高Koopman算子方法在实际有噪声环境下的动力学建模与控制性能，具有重要的工程应用价值。

Abstract: Koopman operator theory has emerged as a leading data-driven approach that relies on a judicious choice of observable functions to realize global linear representations of nonlinear systems in the lifted observable space. However, real-world data is often noisy, making it difficult to obtain an accurate and unbiased approximation of the Koopman operator. The Koopman operator generated from noisy datasets is typically corrupted by noise-induced bias that severely degrades prediction and downstream tracking performance. In order to address this drawback, this paper proposes a novel autoencoder-based neural architecture to jointly learn the appropriate lifting functions and the reduced-bias Koopman operator from noisy data. The architecture initially learns the Koopman basis functions that are consistent for both the forward and backward temporal dynamics of the system. Subsequently, by utilizing the learned forward and backward temporal dynamics, the Koopman operator is synthesized with a reduced bias making the method more robust to noise compared to existing techniques. Theoretical analysis is used to demonstrate significant bias reduction in the presence of training noise. Dynamics prediction and tracking control simulations are conducted for multiple serial manipulator arms, including performance comparisons with leading alternative designs, to demonstrate its robustness under various noise levels. Experimental studies with the Franka FR3 7-DoF manipulator arm are further used to demonstrate the effectiveness of the proposed approach in a practical setting.

</details>


### [275] [Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot](https://arxiv.org/abs/2601.02078)
*Chenghao Yin,Da Huang,Di Yang,Jichao Wang,Nanshu Zhao,Chen Xu,Wenjun Sun,Linjie Hou,Zhijun Li,Junhui Wu,Zhaobo Liu,Zhen Xiao,Sheng Zhang,Lei Bao,Rui Feng,Zhenquan Pang,Jiayu Li,Qian Wang,Maoqing Yao*

Main category: cs.RO

TL;DR: 本文介绍了Genie Sim 3.0，一个统一的机器人操控仿真平台，利用大语言模型（LLM）驱动工具快速生成高保真模拟场景，以及自动化的评测基准与超过1万小时合成数据集，验证了合成数据对真实机器人学习的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习模型对大规模多样化数据和可靠评测基准高度依赖，但现实世界数据采集成本高、难以扩展，现有仿真平台存在碎片化、范围有限或真实感不足等问题，难以实现有效的仿真到现实（sim-to-real）迁移。

Method: 提出Genie Sim 3.0平台，包括：1）Genie Sim Generator，基于LLM通过自然语言指令构建多样化高保真环境，2）首个使用LLM自动生成评测场景和结合视觉语言模型（VLM）进行自动化评估的评测基准，3）开源超1万小时、200多任务的合成数据集。

Result: 系统实验验证，基于Genie Sim 3.0生成的合成数据和自动化评测方法具备强大的零样本sim-to-real迁移能力，合成数据可在受控条件下有效替代真实数据进行规模化策略训练。

Conclusion: Genie Sim 3.0显著提升了机器人仿真环境的多样性和评测自动化水平，为大规模、低成本、真实感强的机器人学习研究提供了新的工具和数据基础，推动合成数据助力机器人策略学习。

Abstract: The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim.

</details>


### [276] [Vision-Based Early Fault Diagnosis and Self-Recovery for Strawberry Harvesting Robots](https://arxiv.org/abs/2601.02085)
*Meili Sun,Chunjiang Zhao,Lichao Yang,Hao Liu,Shimin Hu,Ya Xiong*

Main category: cs.RO

TL;DR: 本文提出了一种结合多任务视觉感知与纠错控制策略的新型视觉故障诊断与自恢复框架，有效提升了草莓采摘机器人的作业稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 草莓采摘机器人在实际应用中常出现视觉感知不集成、抓取偏差、空抓和果实滑落等问题，严重影响采摘效率和作业稳定性。亟需提升机器人对目标定位和故障检测的能力，实现更稳定高效的采摘。

Method: 提出视觉故障诊断与自恢复框架，核心为SRR-Net多任务感知模型，实现草莓检测、分割和成熟度估算。通过目标-夹爪同步检测的相对误差补偿方法，修正抓取偏差。为防止空抓和滑落，设计了早撤策略，并在末端执行器嵌入微型视觉组件，利用MobileNet V3-Small和LSTM分别进行实时抓持/滑落检测与预测。

Result: SRR-Net在草莓/手的检测精度分别为0.895/0.972、召回率为0.813/0.958，分割精度为0.887/0.974、召回率为0.747/0.947。成熟度估算平均绝对误差0.035，推理速度达163.35FPS，感知精度高且满足实时性需求。

Conclusion: 该方法显著提升了草莓采摘机器人的视觉感知与故障自恢复能力，有效减少了采摘过程中的操作失误，提高了作业效率和稳定性，对果蔬采摘智能机器人有重要推动作用。

Abstract: Strawberry harvesting robots faced persistent challenges such as low integration of visual perception, fruit-gripper misalignment, empty grasping, and strawberry slippage from the gripper due to insufficient gripping force, all of which compromised harvesting stability and efficiency in orchard environments. To overcome these issues, this paper proposed a visual fault diagnosis and self-recovery framework that integrated multi-task perception with corrective control strategies. At the core of this framework was SRR-Net, an end-to-end multi-task perception model that simultaneously performed strawberry detection, segmentation, and ripeness estimation, thereby unifying visual perception with fault diagnosis. Based on this integrated perception, a relative error compensation method based on the simultaneous target-gripper detection was designed to address positional misalignment, correcting deviations when error exceeded the tolerance threshold. To mitigate empty grasping and fruit-slippage faults, an early abort strategy was implemented. A micro-optical camera embedded in the end-effector provided real-time visual feedback, enabling grasp detection during the deflating stage and strawberry slip prediction during snap-off through MobileNet V3-Small classifier and a time-series LSTM classifier. Experiments demonstrated that SRR-Net maintained high perception accuracy. For detection, it achieved a precision of 0.895 and recall of 0.813 on strawberries, and 0.972/0.958 on hands. In segmentation, it yielded a precision of 0.887 and recall of 0.747 for strawberries, and 0.974/0.947 for hands. For ripeness estimation, SRR-Net attained a mean absolute error of 0.035, while simultaneously supporting multi-task perception and sustaining a competitive inference speed of 163.35 FPS.

</details>


### [277] [SingingBot: An Avatar-Driven System for Robotic Face Singing Performance](https://arxiv.org/abs/2601.02125)
*Zhuoxiong Xu,Xuanchen Li,Yuhao Cheng,Fei Xu,Yichao Yan,Xiaokang Yang*

Main category: cs.RO

TL;DR: 该论文提出了一种新的机器人面部驱动框架，使机器人能够进行富有表现力和情感的唱歌。通过人脸视频生成技术和语义映射方法，实现了情感丰富、嘴型同步的表情控制，并引入了情感动态范围新指标进行量化评估。实验显示方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有机器人面部驱动技术主要用于对话或静态表情复制，难以满足唱歌场景下对情感连贯性和丰富度的高要求，因此需要开发适合机器人唱歌的情感表达方案。

Method: 首先利用嵌入人类先验的画像视频生成模型，生成生动的歌唱虚拟人脸，作为情感和表情的指导。然后通过面向语义的映射函数，将这些人脸特征映射到实际机器人，再利用情感动态范围（Emotion Dynamic Range）指标量化评估表情情感丰富度。

Result: 所提方法能够实现情感丰富且嘴型与音频高度同步的机器人唱歌表情控制，在实验中表现显著优于现有方案。

Conclusion: 论文提出的方法通过将虚拟歌唱人脸的丰富表情映射到机器人，实现了更具吸引力和情感表现力的人机交互，对提升机器人在唱歌和复杂情绪场景中的表现具有重要意义。

Abstract: Equipping robotic faces with singing capabilities is crucial for empathetic Human-Robot Interaction. However, existing robotic face driving research primarily focuses on conversations or mimicking static expressions, struggling to meet the high demands for continuous emotional expression and coherence in singing. To address this, we propose a novel avatar-driven framework for appealing robotic singing. We first leverage portrait video generation models embedded with extensive human priors to synthesize vivid singing avatars, providing reliable expression and emotion guidance. Subsequently, these facial features are transferred to the robot via semantic-oriented mapping functions that span a wide expression space. Furthermore, to quantitatively evaluate the emotional richness of robotic singing, we propose the Emotion Dynamic Range metric to measure the emotional breadth within the Valence-Arousal space, revealing that a broad emotional spectrum is crucial for appealing performances. Comprehensive experiments prove that our method achieves rich emotional expressions while maintaining lip-audio synchronization, significantly outperforming existing approaches.

</details>


### [278] [Differential Barometric Altimetry for Submeter Vertical Localization and Floor Recognition Indoors](https://arxiv.org/abs/2601.02184)
*Yuhang Zhang,Sören Schwertfeger*

Main category: cs.RO

TL;DR: 本文提出了一种基于微分气压传感的低成本精确垂直定位方案，实现了亚米级高度估计和100%楼层识别率，优于单独视觉或LiDAR方案，并已开源。


<details>
  <summary>Details</summary>
Motivation: 在多楼层复杂环境下，移动机器人对精准高度估算与楼层识别需求强烈，现有视觉或LiDAR方法无法可靠满足这一需求。

Method: 系统结合移动端与基站的气压传感器，通过微分气压策略实现实时、无漂移的高度估算，并与ROS系统集成，形成易用的软件模块。

Result: 在楼梯间、电梯等复杂场景中，方案高度精度RMSE为0.29米，楼层识别准确率100%，显著优于单一视觉或LiDAR-GPS方案。

Conclusion: 该ROS兼容的气压法为机器人在真实环境下实现稳定、经济的垂直感知提供了理想解决方案，并已开源可用。

Abstract: Accurate altitude estimation and reliable floor recognition are critical for mobile robot localization and navigation within complex multi-storey environments. In this paper, we present a robust, low-cost vertical estimation framework leveraging differential barometric sensing integrated within a fully ROS-compliant software package. Our system simultaneously publishes real-time altitude data from both a stationary base station and a mobile sensor, enabling precise and drift-free vertical localization. Empirical evaluations conducted in challenging scenarios -- such as fully enclosed stairwells and elevators, demonstrate that our proposed barometric pipeline achieves sub-meter vertical accuracy (RMSE: 0.29 m) and perfect (100%) floor-level identification. In contrast, our results confirm that standalone height estimates, obtained solely from visual- or LiDAR-based SLAM odometry, are insufficient for reliable vertical localization. The proposed ROS-compatible barometric module thus provides a practical and cost-effective solution for robust vertical awareness in real-world robotic deployments. The implementation of our method is released as open source at https://github.com/witsir/differential-barometric.

</details>


### [279] [CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding](https://arxiv.org/abs/2601.02295)
*Chenyang Ma,Guangyu Yang,Kai Lu,Shitong Xu,Bill Byrne,Niki Trigoni,Andrew Markham*

Main category: cs.RO

TL;DR: 本文提出了CycleVLA系统，使视觉-语言-动作模型(VLAs)具备主动自我纠错功能，能在错误完全发生前预警并恢复，提高了机器人任务执行的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人故障检测与修正主要在出错后进行，导致效率低下。作者希望让机器人能在出现失败征兆时主动发现并及时纠正，增强机器人自主性和任务成功率。

Method: CycleVLA系统包括三大模块：1) 进度感知VLA，能标记任务中易出错的关键子任务过渡点；2) 基于视觉-语言模型(VLM)的失败预测与规划，当预测到失败时触发子任务回溯；3) 基于最小贝叶斯风险(MBR)解码的测试时刻尺度调整策略，提升回溯后的重试成功率。

Result: 实验结果表明，CycleVLA不论在充分训练还是训练不足的VLA上，都能显著提升性能。MBR策略在零样本测试情境下也能有效增强VLA的表现。

Conclusion: CycleVLA实现了基于视觉-语言-动作模型的主动自我纠错机制，比传统事后修正方式更高效，为机器人可靠性和鲁棒性提升提供了新思路。

Abstract: Current work on robot failure detection and correction typically operate in a post hoc manner, analyzing errors and applying corrections only after failures occur. This work introduces CycleVLA, a system that equips Vision-Language-Action models (VLAs) with proactive self-correction, the capability to anticipate incipient failures and recover before they fully manifest during execution. CycleVLA achieves this by integrating a progress-aware VLA that flags critical subtask transition points where failures most frequently occur, a VLM-based failure predictor and planner that triggers subtask backtracking upon predicted failure, and a test-time scaling strategy based on Minimum Bayes Risk (MBR) decoding to improve retry success after backtracking. Extensive experiments show that CycleVLA improves performance for both well-trained and under-trained VLAs, and that MBR serves as an effective zero-shot test-time scaling strategy for VLAs. Project Page: https://dannymcy.github.io/cyclevla/

</details>
