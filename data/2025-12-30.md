<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 179]
- [cs.CL](#cs.CL) [Total: 66]
- [cs.RO](#cs.RO) [Total: 43]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Characterizing Motion Encoding in Video Diffusion Timesteps](https://arxiv.org/abs/2512.22175)
*Vatsal Baherwani,Yixuan Ren,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 本论文系统性分析了文生视频扩散模型在不同去噪步（timestep）中对运动（motion）和外观（appearance）的编码过程，首次进行了大规模定量研究，并提出通过聚焦于'运动主导'步数区间，可简化运动自定义流程，实现高效运动迁移。


<details>
  <summary>Details</summary>
Motivation: 虽然大家普遍认为，文生视频扩散模型的早期步数主要影响运动和布局，后期则精细化外观，但这种经验性的做法缺乏定量和理论支撑。本研究旨在系统性揭示运动与外观信息在扩散过程不同时刻的交互关系，为模型设计和编辑方法提供理论依据。

Method: 通过在扩散不同步数区间插入新条件，观察外观编辑和运动保持之间的权衡情况，设计了量化运动和外观分离能力的方法，并在多种扩散模型架构上大规模实验，绘制运动和外观主导区间。

Result: 实验证明，不同模型都普遍存在早期'运动主导'和后期'外观主导'的步骤区间，界定了可以操作的运动外观分界界线。利用这个新认知，作者进一步精简了运动自定义训练与推理流程，仅限制于运动主导区间，无需额外去偏模块或特殊目标函数即可实现高效运动迁移。

Conclusion: 本研究从实践经验出发，通过系统研究将运动与外观在时空上的解耦具体化，为扩散模型的视频运动编辑和迁移任务提供了更简洁、有效的方法，也为该领域相关技术直接集成提供了理论与实践参考。

Abstract: Text-to-video diffusion models synthesize temporal motion and spatial appearance through iterative denoising, yet how motion is encoded across timesteps remains poorly understood. Practitioners often exploit the empirical heuristic that early timesteps mainly shape motion and layout while later ones refine appearance, but this behavior has not been systematically characterized. In this work, we proxy motion encoding in video diffusion timesteps by the trade-off between appearance editing and motion preservation induced when injecting new conditions over specified timestep ranges, and characterize this proxy through a large-scale quantitative study. This protocol allows us to factor motion from appearance by quantitatively mapping how they compete along the denoising trajectory. Across diverse architectures, we consistently identify an early, motion-dominant regime and a later, appearance-dominant regime, yielding an operational motion-appearance boundary in timestep space. Building on this characterization, we simplify current one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives. Our analysis turns a widely used heuristic into a spatiotemporal disentanglement principle, and our timestep-constrained recipe can serve as ready integration into existing motion transfer and editing methods.

</details>


### [2] [Real-Time American Sign Language Recognition Using 3D Convolutional Neural Networks and LSTM: Architecture, Training, and Deployment](https://arxiv.org/abs/2512.22177)
*Dawnena Key*

Main category: cs.CV

TL;DR: 本文提出了一种结合3D卷积神经网络（3D CNN）与长短时记忆网络（LSTM）的混合深度学习 ASL 手语识别系统，实现了基于摄像头视频流的实时词级手语识别。该系统达到了F1分数0.71至0.99，被部署于AWS及OAK-D硬件，为听障人群提供无障碍交流支持。


<details>
  <summary>Details</summary>
Motivation: 全球约有7000万聋哑和听障人士，他们在日常沟通中面临诸多障碍。现有手语识别系统较难兼顾实时性与准确性，特别是在常用词汇层面，亟需更高效、适用于实际场景的解决方案。

Method: 采用3D CNN提取视频中的空间-时间特征，并通过LSTM建模手语动作的时序依赖性。模型以WLASL、ASL-LEX等大型词汇数据库和人工注释的手语数据集为训练基础，通过深度学习方法提升词级ASL手语的自动识别能力。

Result: 系统在广泛ASL手语词汇上F1分数达到0.71-0.99，实现了在AWS云端及OAK-D边缘设备上的实时部署与推理，验证了系统的高效性与实用性。

Conclusion: 提出的混合深度学习ASL识别架构有效推动了手语自动识别领域，具有切实应用于实际无障碍场景的潜力，为听障人群信息获取与交流带来积极影响。

Abstract: This paper presents a real-time American Sign Language (ASL) recognition system utilizing a hybrid deep learning architecture combining 3D Convolutional Neural Networks (3D CNN) with Long Short-Term Memory (LSTM) networks. The system processes webcam video streams to recognize word-level ASL signs, addressing communication barriers for over 70 million deaf and hard-of-hearing individuals worldwide. Our architecture leverages 3D convolutions to capture spatial-temporal features from video frames, followed by LSTM layers that model sequential dependencies inherent in sign language gestures. Trained on the WLASL dataset (2,000 common words), ASL-LEX lexical database (~2,700 signs), and a curated set of 100 expert-annotated ASL signs, the system achieves F1-scores ranging from 0.71 to 0.99 across sign classes. The model is deployed on AWS infrastructure with edge deployment capability on OAK-D cameras for real-time inference. We discuss the architecture design, training methodology, evaluation metrics, and deployment considerations for practical accessibility applications.

</details>


### [3] [Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery](https://arxiv.org/abs/2512.22182)
*Hassan Khalid,Muhammad Mahad Khaliq,Muhammad Jawad Bashir*

Main category: cs.CV

TL;DR: 本文提出结合人工智能与局部线性嵌入（LLE）的方法，用于提升医疗账单处理和转录的准确性与效率。通过实验，结果显示数据处理和操作效率明显提升。


<details>
  <summary>Details</summary>
Motivation: 当前医疗行业需处理大量高维数据，人工处理容易出错，且效率低下，急需自动化且高效的数据处理方法。

Method: 提出并构建了一个AI增强的LLE模型，结合人工智能技术优化LLE算法，以自动处理医疗账单和转录数据，并通过实际案例进行了实验验证。

Result: 实验结果表明，该模型显著提升了数据处理的准确率和整体操作效率。

Conclusion: AI增强LLE模型在医疗数据分析领域表现优异，不仅提升现有系统性能，也为后续拓展至更广泛医疗应用奠定了基础。

Abstract: The rapid evolution of Artificial intelligence in healthcare has opened avenues for enhancing various processes, including medical billing and transcription. This paper introduces an innovative approach by integrating AI with Locally Linear Embedding (LLE) to revolutionize the handling of high-dimensional medical data. This AI-enhanced LLE model is specifically tailored to improve the accuracy and efficiency of medical billing systems and transcription services. By automating these processes, the model aims to reduce human error and streamline operations, thereby facilitating faster and more accurate patient care documentation and financial transactions. This paper provides a comprehensive mathematical model of AI-enhanced LLE, demonstrating its application in real-world healthcare scenarios through a series of experiments. The results indicate a significant improvement in data processing accuracy and operational efficiency. This study not only underscores the potential of AI-enhanced LLE in medical data analysis but also sets a foundation for future research into broader healthcare applications.

</details>


### [4] [Unbiased Visual Reasoning with Controlled Visual Inputs](https://arxiv.org/abs/2512.22183)
*Zhaonan Li,Shijie Lu,Fei Wang,Jacob Dineen,Xiao Ye,Zhikun Xu,Siyi Liu,Young Min Cho,Bangzheng Li,Daniel Chang,Kenny Nguyen,Qizheng Yang,Muhao Chen,Ben Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉-语言推理框架VISTA，通过分离感知与推理环节，有效减少VLM模型在视觉问答任务中的捷径依赖，提升模型在真实场景下的泛化鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端视觉-语言模型（VLMs）常在视觉问答等任务中过于依赖表面相关性（spurious correlations），而非真正的因果视觉证据，且在微调后容易加剧该问题。因此，需要一种机制来增强模型推理的因果性和鲁棒性。

Method: 作者提出VISTA框架，将感知任务与推理任务通过信息瓶颈进行严格分离：冻结的VLM仅负责感知，回答简短、客观的问题，由文本语言大模型（LLM）负责分解问题、规划查询、汇总视觉事实。这种模块化设计下，推理部分可通过强化学习在奖励对齐环境中训练，以获得无偏见的视觉推理能力。

Result: VISTA在Qwen2.5-VL和Llama3.2-Vision等传感器上，仅用641个精心设计的多步问题进行训练，在SpuriVerse基准测试中显著优于端到端方法（如Qwen-2.5-VL-7B提升16.29%，Llama-3.2-Vision-11B提升6.77%），并且在MMVP及SeedBench子集上表现也具有竞争力。此外，VISTA展现出优秀的迁移能力和应对感知失败的恢复能力。

Conclusion: VISTA能有效提升视觉-语言推理的因果性与鲁棒性，使推理过程更中立、少受表面属性影响，并且推理过程更易通过视觉证据溯源。相比传统端到端VLM方法，VISTA是一种更可靠、更具解释性的新范式。

Abstract: End-to-end Vision-language Models (VLMs) often answer visual questions by exploiting spurious correlations instead of causal visual evidence, and can become more shortcut-prone when fine-tuned. We introduce VISTA (Visual-Information Separation for Text-based Analysis), a modular framework that decouples perception from reasoning via an explicit information bottleneck. A frozen VLM sensor is restricted to short, objective perception queries, while a text-only LLM reasoner decomposes each question, plans queries, and aggregates visual facts in natural language. This controlled interface defines a reward-aligned environment for training unbiased visual reasoning with reinforcement learning. Instantiated with Qwen2.5-VL and Llama3.2-Vision sensors, and trained with GRPO from only 641 curated multi-step questions, VISTA significantly improves robustness to real-world spurious correlations on SpuriVerse (+16.29% with Qwen-2.5-VL-7B and +6.77% with Llama-3.2-Vision-11B), while remaining competitive on MMVP and a balanced SeedBench subset. VISTA transfers robustly across unseen VLM sensors and is able to recognize and recover from VLM perception failures. Human analysis further shows that VISTA's reasoning traces are more neutral, less reliant on spurious attributes, and more explicitly grounded in visual evidence than end-to-end VLM baselines.

</details>


### [5] [SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder for High-Sensitivity Intracrania Aneurysm Screening](https://arxiv.org/abs/2512.22185)
*Antara Titikhsha,Divyanshu Tak*

Main category: cs.CV

TL;DR: 该论文提出了SAMM2D双编码器框架，在颅内动脉瘤检测任务上表现优异，并颠覆性地发现过度数据增强反而会降低模型性能。


<details>
  <summary>Details</summary>
Motivation: 动脉瘤的检测在预防致命性出血中至关重要，但由于形态细微、类别极不平衡和标注数据稀缺等问题，检测依然充满挑战。作者致力于解决上述困难，提高检测准确率并简化建模流程。

Method: 提出了一种名为SAMM2D的双编码器结构，利用强大的ImageNet预训练主干网络进行特征提取，并在不同的数据增强策略下评估模型表现。同时通过热力图可视化分析模型关注的区域。

Result: 在RSNA颅内动脉瘤数据集上，SAMM2D取得了0.686的AUC，比临床基线高出32%。最引人注目的是，在强预训练主干下，任何形式的数据增强都会导致性能下降。未经增强的基线模型比所有增强版本高1.75-2.23个百分点。此外，模型在决策阈值调优后达到95%灵敏度，并超过平均放射科医生水平，且可视化显示85%真阳性关注相关血管区域。

Conclusion: 在医学影像低数据场景下，与其追求复杂的数据增强流程，不如优先使用强大的预训练特征。预训练主干已经具备足够鲁棒的特征描述，过度增强反而起到副作用。未来医学影像工作流应更多关注模型预训练而不是数据增强。

Abstract: Effective aneurysm detection is essential to avert life-threatening hemorrhages, but it remains challenging due to the subtle morphology of the aneurysm, pronounced class imbalance, and the scarcity of annotated data. We introduce SAMM2D, a dual-encoder framework that achieves an AUC of 0.686 on the RSNA intracranial aneurysm dataset; an improvement of 32% over the clinical baseline. In a comprehensive ablation across six augmentation regimes, we made a striking discovery: any form of data augmentation degraded performance when coupled with a strong pretrained backbone. Our unaugmented baseline model outperformed all augmented variants by 1.75--2.23 percentage points (p < 0.01), overturning the assumption that "more augmentation is always better" in low-data medical settings. We hypothesize that ImageNet-pretrained features already capture robust invariances, rendering additional augmentations both redundant and disruptive to the learned feature manifold. By calibrating the decision threshold, SAMM2D reaches 95% sensitivity, surpassing average radiologist performance, and translates to a projected \$13.9M in savings per 1,000 patients in screening applications. Grad-CAM visualizations confirm that 85% of true positives attend to relevant vascular regions (62% IoU with expert annotations), demonstrating the model's clinically meaningful focus. Our results suggest that future medical imaging workflows could benefit more from strong pretraining than from increasingly complex augmentation pipelines.

</details>


### [6] [HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology](https://arxiv.org/abs/2512.22188)
*Xitong Ling,Minxi Ouyang,Xiaoxiao Li,Jiawen Li,Ying Chen,Yuxuan Sun,Xinrui Chen,Tian Guan,Xiaoping Liu,Yonghong He*

Main category: cs.CV

TL;DR: 本文提出HookMIL，一种用于全切片图像（WSI）分析的高效、具上下文感知能力的多实例学习（MIL）框架。通过创新的Hook Token机制，提升病理图像处理的表现与效率。


<details>
  <summary>Details</summary>
Motivation: 当前MIL方法在病理切片分析中面临重要上下文信息丢失和计算复杂度高的问题。本文旨在解决传统MIL上下文捕捉能力弱，以及基于transformer的MIL计算量大、重复性高的缺陷。

Method: 引入可学习的Hook Token，作为上下文信息聚合节点。Hook Token可通过关键病理区域特征、视觉-语言结合嵌入或空间转录组信息进行多模态初始化，使其具备丰富的先验知识。训练时，Hook Token与实例之间采用线性复杂度的双向注意力机制交互。为增加多样性，增加了Hook Diversity Loss以促进不同token关注不同病理模式，同时引入token间通信机制，提升信息表达的精练度。

Result: 在四个公开病理数据集上的实验显示，HookMIL实现了最优的性能表现，同时明显提升了计算效率与可解释性。

Conclusion: HookMIL兼具高效性和强表现力，为病理图像的多实例学习提供更优方案，也为上下文信息的高效融合提出可行的新思路。

Abstract: Multiple Instance Learning (MIL) has enabled weakly supervised analysis of whole-slide images (WSIs) in computational pathology. However, traditional MIL approaches often lose crucial contextual information, while transformer-based variants, though more expressive, suffer from quadratic complexity and redundant computations. To address these limitations, we propose HookMIL, a context-aware and computationally efficient MIL framework that leverages compact, learnable hook tokens for structured contextual aggregation. These tokens can be initialized from (i) key-patch visual features, (ii) text embeddings from vision-language pathology models, and (iii) spatially grounded features from spatial transcriptomics-vision models. This multimodal initialization enables Hook Tokens to incorporate rich textual and spatial priors, accelerating convergence and enhancing representation quality. During training, Hook tokens interact with instances through bidirectional attention with linear complexity. To further promote specialization, we introduce a Hook Diversity Loss that encourages each token to focus on distinct histopathological patterns. Additionally, a hook-to-hook communication mechanism refines contextual interactions while minimizing redundancy. Extensive experiments on four public pathology datasets demonstrate that HookMIL achieves state-of-the-art performance, with improved computational efficiency and interpretability. Codes are available at https://github.com/lingxitong/HookMIL.

</details>


### [7] [Tiny-YOLOSAM: Fast Hybrid Image Segmentation](https://arxiv.org/abs/2512.22193)
*Kenneth Xu,Songhan Wu*

Main category: cs.CV

TL;DR: 论文提出了Tiny-YOLOSAM，将YOLO检测器与TinySAM轻量分割模型结合，大幅提升分割覆盖率并减少运行时间，适用于实际场景分割任务。


<details>
  <summary>Details</summary>
Motivation: 经典的SAM模型分割质量高，但计算资源消耗大，不适合实时或延迟敏感的场景。虽有TinySAM轻量版，但其全景分割仍需大量提示，速度仍然慢。因此需要进一步提速，适应实际应用需求。

Method: 首先复现了TinySAM的实验结果以保证基线可靠；然后提出Tiny-YOLOSAM混合方法，先用YOLOv12检测前景物体生成框提示，引导TinySAM分割，未被覆盖区则用稀疏点提示补充，实现高效全景分割。

Result: 在COCO val2017数据集上，该方法将分割覆盖率（AR）从16.4%提升到77.1%，mIoU从19.2%提升到67.8%，同时将每张图像的运行时间从49.20秒降至10.39秒（在苹果M1 Pro CPU上），提升了4.7倍。

Conclusion: 利用检测器引导提示结合针对性稀疏采样，是实现高效全景分割的有效方案，可取代传统密集提示方法，具备实际应用价值。

Abstract: The Segment Anything Model (SAM) enables promptable, high-quality segmentation but is often too computationally expensive for latency-critical settings. TinySAM is a lightweight, distilled SAM variant that preserves strong zero-shot mask quality, yet its "segment-everything" mode still requires hundreds of prompts and remains slow in practice. We first replicate TinySAM on COCO val2017 using official checkpoints, matching the reported AP within 0.03%, establishing a reliable experimental baseline. Building on this, we propose Tiny-YOLOSAM, a fast hybrid pipeline that uses a recent YOLO detector (YOLOv12) to generate box prompts for TinySAM on salient foreground objects, and supplements uncovered regions with sparse point prompts sampled only where YOLO-guided masks provide no coverage. On COCO val2017, the hybrid system substantially improves class-agnostic coverage (AR from 16.4% to 77.1%, mIoU from 19.2% to 67.8%) while reducing end-to-end runtime from 49.20s/image to 10.39s/image (4.7x) on an Apple M1 Pro CPU. These results suggest detector-guided prompting combined with targeted sparse sampling as an effective alternative to dense "segment-everything" prompting for practical full-scene segmentation.

</details>


### [8] [Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy](https://arxiv.org/abs/2512.22197)
*Shivum Telang*

Main category: cs.CV

TL;DR: 本文提出了一个多模态可解释性模型，结合了视觉语言模型（VLM）和少样本学习，通过同时分析OCT和眼底图像，提供更具解释性和实用性的糖尿病视网膜病变（DR）检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型用于DR检测时，通常只依赖单一图像模态，并且可解释性有限，仅标注病灶位置，无法满足医生希望理解模型推理过程的需求。人工病灶标注既耗时又不实用。因此需要一个能够量化检测、用自然语言描述病灶并解释模型决策过程的新模型。

Method: 提出基于视觉语言模型（VLM）与少样本学习的方法，模拟眼科医生分析视网膜象限分布病变的推理过程。该模型结合OCT和眼底图像，生成Grad-CAM热力图，解释DR严重程度分类时的关键区域。

Result: 模型在3,000张眼底图像和1,000张OCT图像的自建数据集上进行实验，成功生成了与每例DR分类相关的视觉解释热力图和定量描述，为多模态DR检测提供更高解释性和实用性。

Conclusion: 新模型有效弥补了现有DR诊断方法可解释性差、局限于单一模态等不足，可用于临床筛查、治疗与科研等多场景，有助于提升DR检测的可靠性与患者结局。

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss worldwide, requiring early detection to preserve sight. Limited access to physicians often leaves DR undiagnosed. To address this, AI models utilize lesion segmentation for interpretability; however, manually annotating lesions is impractical for clinicians. Physicians require a model that explains the reasoning for classifications rather than just highlighting lesion locations. Furthermore, current models are one-dimensional, relying on a single imaging modality for explainability and achieving limited effectiveness. In contrast, a quantitative-detection system that identifies individual DR lesions in natural language would overcome these limitations, enabling diverse applications in screening, treatment, and research settings. To address this issue, this paper presents a novel multimodal explainability model utilizing a VLM with few-shot learning, which mimics an ophthalmologist's reasoning by analyzing lesion distributions within retinal quadrants for fundus images. The model generates paired Grad-CAM heatmaps, showcasing individual neuron weights across both OCT and fundus images, which visually highlight the regions contributing to DR severity classification. Using a dataset of 3,000 fundus images and 1,000 OCT images, this innovative methodology addresses key limitations in current DR diagnostics, offering a practical and comprehensive tool for improving patient outcomes.

</details>


### [9] [TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting](https://arxiv.org/abs/2512.22203)
*Qiang Guo,Rubo Zhang,Bingbing Zhang,Junjie Liu,Jianqing Liu*

Main category: cs.CV

TL;DR: 本文提出了一种体积小、参数少，仅需弱监督的Transformer架构TCFormer，用于高效、轻量的人群计数，能够兼顾性能与资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的人群计数方法依赖于精确标注和复杂的深度网络结构，限制了其在数据标注困难和计算资源有限设备上的应用。为了解决这些问题，作者致力于设计一种轻量级且只需图像级标签即可训练的人群计数框架。

Method: 1. 以高效的视觉Transformer作为特征提取器，获取全局语义信息并保持较低内存占用。
2. 设计了可学习密度加权平均模块（Learnable Density-Weighted Averaging），通过预测的密度分数对局部特征动态加权，无需额外位置标注。
3. 引入密度等级分类损失，将人群密度离散化，加强不同密度情况下的判别能力，实现全局计数和密度分级的联合优化。

Result: 在上海科技A/B、UCF-QNRF、NWPU四个主流数据集上进行了大量实验，结果显示TCFormer在参数量和计数准确率之间实现了优异的平衡，表现出高度的效率和竞争力。

Conclusion: TCFormer不仅在弱监督设定下保持了高性能，还极大地降低了部署成本，特别适合在边缘设备等资源有限场景下使用，是人群计数任务的一种有效解决方案。

Abstract: Crowd counting typically relies on labor-intensive point-level annotations and computationally intensive backbones, restricting its scalability and deployment in resource-constrained environments. To address these challenges, this paper proposes the TCFormer, a tiny, ultra-lightweight, weakly-supervised transformer-based crowd counting framework with only 5 million parameters that achieves competitive performance. Firstly, a powerful yet efficient vision transformer is adopted as the feature extractor, the global context-aware capabilities of which provides semantic meaningful crowd features with a minimal memory footprint. Secondly, to compensate for the lack of spatial supervision, we design a feature aggregation mechanism termed the Learnable Density-Weighted Averaging module. This module dynamically re-weights local tokens according to predicted density scores, enabling the network to adaptively modulate regional features based on their specific density characteristics without the need for additional annotations. Furthermore, this paper introduces a density-level classification loss, which discretizes crowd density into distinct grades, thereby regularizing the training process and enhancing the model's classification power across varying levels of crowd density. Therefore, although TCformer is trained under a weakly-supervised paradigm utilizing only image-level global counts, the joint optimization of count and density-level losses enables the framework to achieve high estimation accuracy. Extensive experiments on four benchmarks including ShanghaiTech A/B, UCF-QNRF, and NWPU datasets demonstrate that our approach strikes a superior trade-off between parameter efficiency and counting accuracy and can be a good solution for crowd counting tasks in edge devices.

</details>


### [10] [A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability](https://arxiv.org/abs/2512.22205)
*Md. Ismiel Hossen Abir,Awolad Hossain*

Main category: cs.CV

TL;DR: 研究提出了一种基于深度学习的自定义卷积神经网络（CNN），自动对血液细胞图像进行疟疾感染与否的分类，准确率达96%。


<details>
  <summary>Details</summary>
Motivation: 目前常规的疟疾诊断方法依赖于人工显微镜检查，存在灵敏度低、依赖专家、资源匮乏地区难以开展等问题。

Method: 本研究开发了一种自定义CNN模型，并将其与ResNet50、VGG16、MobileNetV2和DenseNet121等主流网络进行对比。此外，采用SHAP、LIME和Saliency Maps等可解释AI技术增强模型的可解释性。

Result: 自定义CNN在血液细胞已感染与未感染分类任务上取得96%的准确率，两类的精确率与召回率均超过0.95。

Conclusion: 该系统证明深度学习能够为资源有限地区提供快速、准确且可解释的疟疾诊断，是传统方法的有效补充。

Abstract: Malaria remains a prevalent health concern in regions with tropical and subtropical climates. The cause of malaria is the Plasmodium parasite, which is transmitted through the bites of infected female Anopheles mosquitoes. Traditional diagnostic methods, such as microscopic blood smear analysis, are low in sensitivity, depend on expert judgment, and require resources that may not be available in remote settings. To overcome these limitations, this study proposes a deep learning-based approach utilizing a custom Convolutional Neural Network (CNN) to automatically classify blood cell images as parasitized or uninfected. The model achieves an accuracy of 96%, with precision and recall scores exceeding 0.95 for both classes. This study also compares the custom CNN with established deep learning architectures, including ResNet50, VGG16, MobileNetV2, and DenseNet121. To enhance model interpretability, Explainable AI techniques such as SHAP, LIME, and Saliency Maps are applied. The proposed system shows how deep learning can provide quick, accurate and understandable malaria diagnosis, especially in areas with limited resources.

</details>


### [11] [Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition](https://arxiv.org/abs/2512.22214)
*Naichuan Zheng,Xiahai Lun,Weiyi Li,Yuchen Du*

Main category: cs.CV

TL;DR: 本文提出了一种结合脉冲神经网络（SNN）与骨骼图卷积网络（GCN）的方法Signal-SGN++，实现动作识别任务的高效准确。该方法兼顾了能效与精度，显著优于现有SNN方法，并且在能耗远低于GCN的情况下取得了有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: GCN虽然在动作识别上表现优异，但高能耗限制了其部署；而SNN能效高但难以建模复杂的时空与频谱依赖。因此亟需设计兼顾结构建模能力和能效的高性能方法。

Method: Signal-SGN++设计了一种结合1D时空脉冲图卷积和频谱脉冲卷积的主干网络结构，并引入自适应拓扑注意力机制（TSSA）增强骨骼图敏感性，辅以多尺度小波分解及拓扑感知的时频融合单元，实现全面特征提取和融合。

Result: 在多个大规模动作识别数据集上，Signal-SGN++展现出优良的准确率和能耗平衡，优于所有已知SNN方法，并且在能耗大幅降低的情况下，对标甚至超越了领先GCN。

Conclusion: Signal-SGN++有效桥接了GCN高能耗和SNN结构建模不足的鸿沟，在动作识别领域兼顾高精度与低能耗，具备广阔的实际应用潜力。

Abstract: Graph Convolutional Networks (GCNs) demonstrate strong capability in modeling skeletal topology for action recognition, yet their dense floating-point computations incur high energy costs. Spiking Neural Networks (SNNs), characterized by event-driven and sparse activation, offer energy efficiency but remain limited in capturing coupled temporal-frequency and topological dependencies of human motion. To bridge this gap, this article proposes Signal-SGN++, a topology-aware spiking graph framework that integrates structural adaptivity with time-frequency spiking dynamics. The network employs a backbone composed of 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. Within this backbone, a Topology-Shift Self-Attention (TSSA) mechanism is embedded to adaptively route attention across learned skeletal topologies, enhancing graph-level sensitivity without increasing computational complexity. Moreover, an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch decomposes spiking features into multi-resolution temporal-frequency representations, wherein a Topology-Aware Time-Frequency Fusion (TATF) unit incorporates structural priors to preserve topology-consistent spectral fusion. Comprehensive experiments on large-scale benchmarks validate that Signal-SGN++ achieves superior accuracy-efficiency trade-offs, outperforming existing SNN-based methods and achieving competitive results against state-of-the-art GCNs under substantially reduced energy consumption.

</details>


### [12] [VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition](https://arxiv.org/abs/2512.22217)
*Abdellah Zakaria Sellam,Salah Eddine Bekhouche,Fadi Dornaika,Cosimo Distante,Abdenour Hadid*

Main category: cs.CV

TL;DR: 提出了VLM-PAR模型，将视觉-语言多模态预训练与交叉注意力融合方法相结合，大幅提升行人属性识别准确性，尤其在类别不平衡的数据集上表现突出。


<details>
  <summary>Details</summary>
Motivation: 行人属性识别任务在类别不平衡、属性间复杂依赖以及领域迁移方面存在显著挑战，限制了其在现实世界的准确性和泛化能力。

Method: 构建了基于SigLIP 2多语言编码器的模块化视觉-语言融合框架，通过精炼视觉特征并利用简洁的交叉注意力机制对齐图像与文本（prompt）嵌入，从而提升属性识别表现。

Result: 在高度不平衡的PA100K数据集上取得了新的SOTA性能，在PETA和Market-1501等数据集上平均准确率也明显提升，展示了方法的有效性和泛化能力。

Conclusion: 将大规模视觉-语言预训练模型与针对性的跨模态特征精炼相结合，能显著克服行人属性识别中的类别不均衡和泛化难题，推动了该领域方法的进步。

Abstract: Pedestrian Attribute Recognition (PAR) involves predicting fine-grained attributes such as clothing color, gender, and accessories from pedestrian imagery, yet is hindered by severe class imbalance, intricate attribute co-dependencies, and domain shifts. We introduce VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders. By first aligning image and prompt embeddings via refining visual features through a compact cross-attention fusion, VLM-PAR achieves significant accuracy improvement on the highly imbalanced PA100K benchmark, setting a new state-of-the-art performance, while also delivering significant gains in mean accuracy across PETA and Market-1501 benchmarks. These results underscore the efficacy of integrating large-scale vision-language pretraining with targeted cross-modal refinement to overcome imbalance and generalization challenges in PAR.

</details>


### [13] [Towards Signboard-Oriented Visual Question Answering: ViSignVQA Dataset, Method and Benchmark](https://arxiv.org/abs/2512.22218)
*Hieu Minh Nguyen,Tam Le-Thanh Dang,Kiet Van Nguyen*

Main category: cs.CV

TL;DR: 本文提出了首个面向越南语招牌文本视觉问答（VQA）的大规模数据集ViSignVQA（10,762张图片，25,573组问答），并验证了多种VQA模型在该任务上的效果，强化OCR文本后显著提升表现，实现可行的高效基线和新多智能体框架。


<details>
  <summary>Details</summary>
Motivation: 针对真实场景下视觉问答应用，尤其是低资源语言（如越南语）中的招牌文本理解较少涉猎，缺乏高质量数据资源，限制了模型研究和落地。

Method: 构建并发布ViSignVQA数据集，涵盖多元化的越南语招牌图像及其相关问答；基准测试对比主流VQA模型，整合越南语OCR（SwinTextSpotter）与预训练语言模型（ViT5）；提出多智能体VQA框架，感知与推理结合，用GPT-4辅助投票决策。

Result: OCR文本增强对模型性能贡献巨大，最高可将F1值提升209%；多智能体框架（GPT-4辅助）实现了75.98%的准确率，显著高于基线。

Conclusion: ViSignVQA数据集为低资源语言（越南语）场景文本VQA提供了重要基准，有效推动OCR集成模型的发展与评测，强调专用领域数据在提升低资源VQA能力中的重要性。

Abstract: Understanding signboard text in natural scenes is essential for real-world applications of Visual Question Answering (VQA), yet remains underexplored, particularly in low-resource languages. We introduce ViSignVQA, the first large-scale Vietnamese dataset designed for signboard-oriented VQA, which comprises 10,762 images and 25,573 question-answer pairs. The dataset captures the diverse linguistic, cultural, and visual characteristics of Vietnamese signboards, including bilingual text, informal phrasing, and visual elements such as color and layout. To benchmark this task, we adapted state-of-the-art VQA models (e.g., BLIP-2, LaTr, PreSTU, and SaL) by integrating a Vietnamese OCR model (SwinTextSpotter) and a Vietnamese pretrained language model (ViT5). The experimental results highlight the significant role of the OCR-enhanced context, with F1-score improvements of up to 209% when the OCR text is appended to questions. Additionally, we propose a multi-agent VQA framework combining perception and reasoning agents with GPT-4, achieving 75.98% accuracy via majority voting. Our study presents the first large-scale multimodal dataset for Vietnamese signboard understanding. This underscores the importance of domain-specific resources in enhancing text-based VQA for low-resource languages. ViSignVQA serves as a benchmark capturing real-world scene text characteristics and supporting the development and evaluation of OCR-integrated VQA models in Vietnamese.

</details>


### [14] [On Extending Semantic Abstraction for Efficient Search of Hidden Objects](https://arxiv.org/abs/2512.22220)
*Tasha Pais,Nikhilesh Belulkar*

Main category: cs.CV

TL;DR: 本论文提出了一种使用2D视觉语言模型（VLMs）的相关性激活作为抽象对象表示的方法，提升对被遮挡（隐藏）物体的三维定位与重建能力，显著快于随机搜索。


<details>
  <summary>Details</summary>
Motivation: 针对家庭机器人在查找被遮挡或丢失物体时效率低下的问题，作者希望让机器人能够更智能、快速地定位隐藏物体。

Method: 利用2D视觉语言模型的相关性激活图作为“抽象对象”表征，结合历史数据，学习隐藏物体的三维定位与重建。该方法通过分析场景中物体可能出现的位置，实现高效的无结构搜索。

Result: 实验表明，该模型能在首次推理时精准地正确定位隐藏物体的三维位置，速度明显优于随机搜索策略。

Conclusion: 对现有语义抽象方法的扩展，为家庭机器人快速查找丢失或遮挡物体提供了可行方案，有望节省大幅的时间与精力。

Abstract: Semantic Abstraction's key observation is that 2D VLMs' relevancy activations roughly correspond to their confidence of whether and where an object is in the scene. Thus, relevancy maps are treated as "abstract object" representations. We use this framework for learning 3D localization and completion for the exclusive domain of hidden objects, defined as objects that cannot be directly identified by a VLM because they are at least partially occluded. This process of localizing hidden objects is a form of unstructured search that can be performed more efficiently using historical data of where an object is frequently placed. Our model can accurately identify the complete 3D location of a hidden object on the first try significantly faster than a naive random search. These extensions to semantic abstraction hope to provide household robots with the skills necessary to save time and effort when looking for lost objects.

</details>


### [15] [VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs](https://arxiv.org/abs/2512.22226)
*Naishan Zheng,Jie Huang,Qingpei Guo,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出了VideoScaffold框架，用于提升多模态大模型对长视频的理解能力，通过动态地调整事件粒度和语义聚合，有效解决现有静态处理策略导致的信息碎片化或过度压缩问题，实现对长时长视频流的高效理解。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型在长视频理解中存在视频帧冗余大、时间一致性难以建模的问题。现有主流方法主要针对离线场景，通过稀疏采样、帧压缩或聚类等手段减少计算量，但在连续视频流下容易导致信息破碎或细节损失，无法很好地兼顾效率与语义完整性。

Method: 作者提出VideoScaffold动态表示框架，主要包括两个核心组件：Elastic-Scale Event Segmentation（EES）根据预测自适应细化事件边界，从而灵活调整分析粒度；Hierarchical Event Consolidation（HEC）将事件片段自底向上逐层聚合，形成多层语义抽象。这两个模块能让框架根据不同视频长度自动切换粒度，既保留视觉细节又能提取高级语义。

Result: 在多个离线与在线视频理解基准测试中，VideoScaffold均取得了当前最优的效果。该框架为模块化设计，可轻松与现有基于图像的多模态大模型结合，扩展其到视频理解任务。

Conclusion: VideoScaffold框架显著提升了多模态大模型对长视频的理解效果，兼顾细节保留与高层抽象，在连续视频流场景下具备灵活性和易用性，有望推动视频理解领域的进一步发展。

Abstract: Understanding long videos with multimodal large language models (MLLMs) remains challenging due to the heavy redundancy across frames and the need for temporally coherent representations. Existing static strategies, such as sparse sampling, frame compression, and clustering, are optimized for offline settings and often produce fragmented or over-compressed outputs when applied to continuous video streams. We present VideoScaffold, a dynamic representation framework designed for streaming video understanding. It adaptively adjusts event granularity according to video duration while preserving fine-grained visual semantics. VideoScaffold introduces two key components: Elastic-Scale Event Segmentation (EES), which performs prediction-guided segmentation to dynamically refine event boundaries, and Hierarchical Event Consolidation (HEC), which progressively aggregates semantically related segments into multi-level abstractions. Working in concert, EES and HEC enable VideoScaffold to transition smoothly from fine-grained frame understanding to abstract event reasoning as the video stream unfolds. Extensive experiments across both offline and streaming video understanding benchmarks demonstrate that VideoScaffold achieves state-of-the-art performance. The framework is modular and plug-and-play, seamlessly extending existing image-based MLLMs to continuous video comprehension. The code is available at https://github.com/zheng980629/VideoScaffold.

</details>


### [16] [KAN-FPN-Stem:A KAN-Enhanced Feature Pyramid Stem for Boosting ViT-based Pose Estimation](https://arxiv.org/abs/2512.22228)
*HaoNan Tang*

Main category: cs.CV

TL;DR: 本文提出了一种基于KAN的FPN-Stem新架构，有效改善了ViT在姿态估计等密集预测任务中的前端特征提取和融合，实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有如ViTPose等ViT模型前端设计过于简单，通过“补丁化”处理难以应对多尺度变化，导致初始特征提取阶段产生不可逆的信息损失，限制了整体性能。

Method: 作者提出一种新颖的 KAN-enhanced FPN-Stem 架构。在保持FPN传统“上采样-加和”特征融合流的基础上，将末端的3x3线性平滑卷积替换为KAN卷积层，利用其更强的非线性建模能力自适应校正融合过程中的伪影。通过消融实验确定性能瓶颈主要在“特征融合”的非线性平滑步骤，而非注意力机制。

Result: 在COCO数据集上，所提KAN-FPN-Stem模块对比轻量版ViTPose-S基线可提升最高2.0 AP，显示了显著的性能增益。

Conclusion: 作者证明了ViT前端“特征融合”质量是性能提升的主要瓶颈，并通过KAN操作在ViT前端实现高效特征融合，提供了一种简单高效、易复用的高性能组件。

Abstract: Vision Transformers (ViT) have demonstrated significant promise in dense prediction tasks such as pose estimation. However, their performance is frequently constrained by the overly simplistic front-end designs employed in models like ViTPose. This naive patchification mechanism struggles to effectively handle multi-scale variations and results in irreversible information loss during the initial feature extraction phase. To overcome this limitation, we introduce a novel KAN-enhanced FPN-Stem architecture. Through rigorous ablation studies, we first identified that the true bottleneck for performance improvement lies not in plug-and-play attention modules (e.g., CBAM), but in the post-fusion non-linear smoothing step within the FPN. Guided by this insight, our core innovation is to retain the classic "upsample-and-add" fusion stream of the FPN, but replace its terminal, standard linear 3x3 smoothing convolution with a powerful KAN-based convolutional layer. Leveraging its superior non-linear modeling capabilities, this KAN-based layer adaptively learns and rectifies the "artifacts" generated during the multi-scale fusion process. Extensive experiments on the COCO dataset demonstrate that our KAN-FPN-Stem achieves a significant performance boost of up to +2.0 AP over the lightweight ViTPose-S baseline. This work not only delivers a plug-and-play, high-performance module but, more importantly, reveals that: the performance bottleneck in ViT front-end often lies not in 'feature refinement' (Attention), but in the quality of 'feature fusion' (Fusion). Furthermore, it provides an effective path to address this bottleneck through the introduction of the KAN operator.

</details>


### [17] [Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction](https://arxiv.org/abs/2512.22237)
*Mengxiao Geng,Ran Hong,Xiaoling Xu,Bingxuan Li,Qiegen Liu*

Main category: cs.CV

TL;DR: 本文提出了一种结合元信息引导和跨域协同扩散模型（MiG-DM），通过融合多领域和多模态先验生成高质量低剂量PET图像，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 低剂量PET成像可降低患者辐射，但易受噪声、对比度降低及细节补偿困难等问题影响。现有方法通常忽视投影域物理知识和患者特异性元信息，而这二者对于功能-语义关联挖掘至关重要。

Method: 提出MiG-DM模型，将患者元信息（如临床参数、剂量信息等）编码为语义提示，实现跨模态对齐。同时，模型结构结合了投影域与图像域的协同处理，投影域通过专用sinogram适配器进行全局物理结构建模，图像域负责高质量重建。

Result: 在UDPET公开数据集和多种临床剂量数据集上，MiG-DM在提升PET图像质量及生理细节保持方面均优于现有最优方法。

Conclusion: MiG-DM能够充分挖掘和利用元信息及物理投影知识，实现低剂量PET图像重建的新突破，有望降低辐射风险同时提升临床诊断价值。

Abstract: Low-dose PET imaging is crucial for reducing patient radiation exposure but faces challenges like noise interference, reduced contrast, and difficulty in preserving physiological details. Existing methods often neglect both projection-domain physics knowledge and patient-specific meta-information, which are critical for functional-semantic correlation mining. In this study, we introduce a meta-information guided cross-domain synergistic diffusion model (MiG-DM) that integrates comprehensive cross-modal priors to generate high-quality PET images. Specifically, a meta-information encoding module transforms clinical parameters into semantic prompts by considering patient characteristics, dose-related information, and semi-quantitative parameters, enabling cross-modal alignment between textual meta-information and image reconstruction. Additionally, the cross-domain architecture combines projection-domain and image-domain processing. In the projection domain, a specialized sinogram adapter captures global physical structures through convolution operations equivalent to global image-domain filtering. Experiments on the UDPET public dataset and clinical datasets with varying dose levels demonstrate that MiG-DM outperforms state-of-the-art methods in enhancing PET image quality and preserving physiological details.

</details>


### [18] [Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture](https://arxiv.org/abs/2512.22239)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.CV

TL;DR: 本文提出了一种混合知识蒸馏框架，通过结合倒残差块和密集连接，设计了轻量高效的卷积神经网络，实现了在资源受限的边缘设备上高精度农作物种类识别。


<details>
  <summary>Details</summary>
Motivation: 智能农业中，资源受限的边缘设备难以同时兼顾模型的计算效率与识别准确率，因此亟需开发高性能、轻量级的模型以满足实际部署需求。

Method: 提出采用ResNet18为教师网络，引导一种混合倒残差与密集连接的学生网络，采用多目标蒸馏策略，包括硬标签监督、特征层蒸馏、响应层蒸馏及自蒸馏，在稻种识别和多种植物叶病数据集上测试泛化能力。

Result: 在稻种识别任务上，学生模型仅比教师模型低0.09%的准确率，计算量仅为0.68 GFLOPs、参数约107万，计算开销和模型体积分别减少2.7倍和10倍以上；对比DenseNet121和ViT也有显著参数量优势，并保持较高分类精度。在多种叶病数据集上均获得了稳健表现。

Conclusion: 所提框架兼具高效性、鲁棒性及优良泛化性能，十分适合部署于硬件资源有限的智慧农业边缘设备，有望推广应用于实际农业场景。

Abstract: Deploying deep learning models on resource-constrained edge devices remains a major challenge in smart agriculture due to the trade-off between computational efficiency and recognition accuracy. To address this challenge, this study proposes a hybrid knowledge distillation framework for developing a lightweight yet high-performance convolutional neural network. The proposed approach designs a customized student model that combines inverted residual blocks with dense connectivity and trains it under the guidance of a ResNet18 teacher network using a multi-objective strategy that integrates hard-label supervision, feature-level distillation, response-level distillation, and self-distillation. Experiments are conducted on a rice seed variety identification dataset containing nine varieties and further extended to four plant leaf disease datasets, including rice, potato, coffee, and corn, to evaluate generalization capability. On the rice seed variety classification task, the distilled student model achieves an accuracy of 98.56%, which is only 0.09% lower than the teacher model (98.65%), while requiring only 0.68 GFLOPs and approximately 1.07 million parameters. This corresponds to a reduction of about 2.7 times in computational cost and more than 10 times in model size compared with the ResNet18 teacher model. In addition, compared with representative pretrained models, the proposed student reduces the number of parameters by more than 6 times relative to DenseNet121 and by over 80 times compared with the Vision Transformer (ViT) architecture, while maintaining comparable or superior classification accuracy. Consistent performance gains across multiple plant leaf disease datasets further demonstrate the robustness, efficiency, and strong deployment potential of the proposed framework for hardware-limited smart agriculture systems.

</details>


### [19] [Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions](https://arxiv.org/abs/2512.22263)
*Aahan Sachdeva,Dhanvinkumar Ganeshkumar,James E. Gallagher,Tyler Treat,Edward J. Oughton*

Main category: cs.CV

TL;DR: 本文提出了一种自适应融合RGB与长波红外（LWIR）视频流的目标检测框架，通过在多种光照条件下动态选择最佳检测模型，提高了应急机器人平台在各光照环境下的视觉感知能力。


<details>
  <summary>Details</summary>
Motivation: 应急救援等场景下，传统RGB视觉系统在低光环境下表现不佳，而单一的热成像系统则丢失了颜色和纹理信息，因此亟需融合多模态信息以提升自动化检测能力。

Method: 作者收集了多光照等级下的22,000多张图片，对33个YOLO模型进行训练。通过对齐的RGB与LWIR进行11种比例（100/0到0/100，每10%递增）的像素级融合，按光照条件自适应地选取最佳检测模型。

Result: 在强光下80/20（RGB/LWIR）模型的平均置信度达92.8%，微光下90/10模型达92.0%，均显著优于YOLOv5n和YOLOv11n基线。在无光条件下40/60融合达到71.0%，虽未显著优于基线，但仍有较高提升。

Conclusion: 融合多模态视觉的自适应检测框架能有效提升各种光照下的检测置信度和可靠性，为应急救援等机器人平台带来了视觉性能的显著增强。

Abstract: Autonomous robotic platforms are playing a growing role across the emergency services sector, supporting missions such as search and rescue operations in disaster zones and reconnaissance. However, traditional red-green-blue (RGB) detection pipelines struggle in low-light environments, and thermal-based systems lack color and texture information. To overcome these limitations, we present an adaptive framework that fuses RGB and long-wave infrared (LWIR) video streams at multiple fusion ratios and dynamically selects the optimal detection model for each illumination condition. We trained 33 You Only Look Once (YOLO) models on over 22,000 annotated images spanning three light levels: no-light (<10 lux), dim-light (10-1000 lux), and full-light (>1000 lux). To integrate both modalities, fusion was performed by blending aligned RGB and LWIR frames at eleven ratios, from full RGB (100/0) to full LWIR (0/100) in 10% increments. Evaluation showed that the best full-light model (80/20 RGB-LWIR) and dim-light model (90/10 fusion) achieved 92.8% and 92.0% mean confidence; both significantly outperformed the YOLOv5 nano (YOLOv5n) and YOLOv11 nano (YOLOv11n) baselines. Under no-light conditions, the top 40/60 fusion reached 71.0%, exceeding baselines though not statistically significant. Adaptive RGB-LWIR fusion improved detection confidence and reliability across all illumination conditions, enhancing autonomous robotic vision performance.

</details>


### [20] [Human-Aligned Generative Perception: Bridging Psychophysics and Generative Models](https://arxiv.org/abs/2512.22272)
*Antara Titikhsha,Om Kulkarni,Dharun Muthaiah*

Main category: cs.CV

TL;DR: 本文提出利用轻量级的、无需特殊训练的判别器作为外部引导信号，提升文本到图像扩散模型对几何约束的理解与控制能力，并实现了对几何和风格的分离控制。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型虽然能生成细致图像，但往往仅关注表层视觉效果，对需要严格几何约束的情形表现不佳，且难以保持与文本语义的一致性。本文旨在通过引入基于人类感知的几何判别信号，增强生成模型的几何理解和控制能力，缩小人与模型间的语义差距。

Method: 提出了一种“Human Perception Embedding(HPE)”教师模型，基于THINGS三元组数据集训练以捕捉人类对物体形状的敏感性。通过在扩散模型的潜在生成过程中注入由该教师模型产生的梯度，实现几何与风格的可控分离，并在三种架构（Stable Diffusion、SiT-XL/2、PixArt-Σ）上验证了方法有效性。

Result: 实验表明：流模型在没有持续外部引导时容易偏离指定几何属性；所提方法实现了复杂三维形状（如Eames椅）和矛盾材质（如粉色金属）的零样本迁移。引导生成能将语义一致性提升约80%。

Conclusion: 小型教师模型可有效引导大型生成系统，带来更强几何控制和更高语义一致性，从而拓宽了文本到图像合成的创作空间。

Abstract: Text-to-image diffusion models generate highly detailed textures, yet they often rely on surface appearance and fail to follow strict geometric constraints, particularly when those constraints conflict with the style implied by the text prompt. This reflects a broader semantic gap between human perception and current generative models. We investigate whether geometric understanding can be introduced without specialized training by using lightweight, off-the-shelf discriminators as external guidance signals. We propose a Human Perception Embedding (HPE) teacher trained on the THINGS triplet dataset, which captures human sensitivity to object shape. By injecting gradients from this teacher into the latent diffusion process, we show that geometry and style can be separated in a controllable manner. We evaluate this approach across three architectures: Stable Diffusion v1.5 with a U-Net backbone, the flow-matching model SiT-XL/2, and the diffusion transformer PixArt-Σ. Our experiments reveal that flow models tend to drift back toward their default trajectories without continuous guidance, and we demonstrate zero-shot transfer of complex three-dimensional shapes, such as an Eames chair, onto conflicting materials such as pink metal. This guided generation improves semantic alignment by about 80 percent compared to unguided baselines. Overall, our results show that small teacher models can reliably guide large generative systems, enabling stronger geometric control and broadening the creative range of text-to-image synthesis.

</details>


### [21] [GeCo: A Differentiable Geometric Consistency Metric for Video Generation](https://arxiv.org/abs/2512.22274)
*Leslie Gu,Junhwa Hur,Charles Herrmann,Fangneng Zhan,Todd Zickler,Deqing Sun,Hanspeter Pfister*

Main category: cs.CV

TL;DR: 本文提出了一种新的度量标准GeCo，用于检测静态场景中的几何变形与遮挡不一致伪影，并可直接应用于视频生成模型的评测和优化。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型常出现几何变形和遮挡不一致等质量问题，缺乏有效的无监督评价标准去系统检测和辅助改进这些瑕疵。

Method: GeCo融合了残差运动和深度先验，输出具有可解释性、密集一致性的伪影检测图，既可用于检测，也可进一步作为训练时的无监督损失引导视频生成。

Result: 通过系统性实验证明，GeCo能有效揭示现有视频生成模型的常见失效模式；并在作为指导损失使用时，可显著降低视频中的变形伪影。

Conclusion: GeCo不仅为几何伪影的检测和分析提供了新工具，也可直接帮助提升生成视频的视觉一致性和质量。

Abstract: We introduce GeCo, a geometry-grounded metric for jointly detecting geometric deformation and occlusion-inconsistency artifacts in static scenes. By fusing residual motion and depth priors, GeCo produces interpretable, dense consistency maps that reveal these artifacts. We use GeCo to systematically benchmark recent video generation models, uncovering common failure modes, and further employ it as a training-free guidance loss to reduce deformation artifacts during video generation.

</details>


### [22] [The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency](https://arxiv.org/abs/2512.22275)
*Dingyu Wang,Zimu Yuan,Jiajun Liu,Shanggui Liu,Nan Zhou,Tianxing Xu,Di Huang,Dong Jiang*

Main category: cs.CV

TL;DR: 本文开发了B&J基准，通过真实病例对现有大模型在骨科和运动医学中的临床推理能力进行了全面评估，发现多模态、开放任务表现明显不足。


<details>
  <summary>Details</summary>
Motivation: 当前医学大模型多以考试或片面场景为基准，无法反映真实临床推理所需的综合、多模态能力。因此，亟需一个更真实、更全面的评测体系来检验模型在实际医疗环境中的表现。

Method: 设计了B&J基准，包括1245道题，涵盖知识回忆、文本和影像解读、诊断、治疗及推理等7项任务，所有题目均来自真实骨科和运动医学病例。共评测了11个视觉-语言模型（VLMs）和6个大语言模型（LLMs），并与专家结论比对。

Result: 目前模型在结构化选择题上准确率超90%，但在需要多模态整合的开放问题准确率不到60%。VLMs在医学影像解读和整合视觉-文本信息时表现较差，易出现幻觉性答案。专门调整过的医学模型并未优于通用模型。

Conclusion: 现有AI模型还不具备复杂、多模态临床推理能力，现阶段仅适合文字辅助支持。实现真实临床任务转突破，需在多模态整合和视觉理解上取得根本进展。

Abstract: Background: The rapid integration of foundation models into clinical practice and public health necessitates a rigorous evaluation of their true clinical reasoning capabilities beyond narrow examination success. Current benchmarks, typically based on medical licensing exams or curated vignettes, fail to capture the integrated, multimodal reasoning essential for real-world patient care. Methods: We developed the Bones and Joints (B&J) Benchmark, a comprehensive evaluation framework comprising 1,245 questions derived from real-world patient cases in orthopedics and sports medicine. This benchmark assesses models across 7 tasks that mirror the clinical reasoning pathway, including knowledge recall, text and image interpretation, diagnosis generation, treatment planning, and rationale provision. We evaluated eleven vision-language models (VLMs) and six large language models (LLMs), comparing their performance against expert-derived ground truth. Results: Our results demonstrate a pronounced performance gap between task types. While state-of-the-art models achieved high accuracy, exceeding 90%, on structured multiple-choice questions, their performance markedly declined on open-ended tasks requiring multimodal integration, with accuracy scarcely reaching 60%. VLMs demonstrated substantial limitations in interpreting medical images and frequently exhibited severe text-driven hallucinations, often ignoring contradictory visual evidence. Notably, models specifically fine-tuned for medical applications showed no consistent advantage over general-purpose counterparts. Conclusions: Current artificial intelligence models are not yet clinically competent for complex, multimodal reasoning. Their safe deployment should currently be limited to supportive, text-based roles. Future advancement in core clinical tasks awaits fundamental breakthroughs in multimodal integration and visual understanding.

</details>


### [23] [FETAL-GAUGE: A Benchmark for Assessing Vision-Language Models in Fetal Ultrasound](https://arxiv.org/abs/2512.22278)
*Hussain Alasmawi,Numan Saeed,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 该论文提出了Fetal-Gauge，这是首个也是最大规模的胎儿超声视觉问答基准，用于评估视觉语言模型（VLMs）在胎儿超声任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 全球对产前超声成像的需求增加导致专业声像师短缺，阻碍了基础胎儿健康监测。深度学习和视觉语言模型有潜力提升声像师效率，但目前缺乏针对胎儿超声VLM性能的统一评价标准，主要受制于该领域数据匮乏和操作依赖性强。

Method: 作者构建了一个包含超4.2万张图片及9.3万组问答对的大规模视觉问答基准，用于涵盖解剖面识别、结构定位、胎儿朝向评估、临床视角一致性和诊断等多种任务。系统性评估了若干通用及医学特定的VLM模型。

Result: 目前最佳模型在该基准上的准确率仅为55%，远低于临床所需标准。分析揭示了现有VLM对于胎儿超声解读的显著局限性。

Conclusion: Fetal-Gauge为产前护理中多模态深度学习的研究提供了坚实基础，并推动了领域适应型模型和专门训练方法的迫切需求。此数据集有望提升全球健康监测的可及性，并在论文接受后公开。

Abstract: The growing demand for prenatal ultrasound imaging has intensified a global shortage of trained sonographers, creating barriers to essential fetal health monitoring. Deep learning has the potential to enhance sonographers' efficiency and support the training of new practitioners. Vision-Language Models (VLMs) are particularly promising for ultrasound interpretation, as they can jointly process images and text to perform multiple clinical tasks within a single framework. However, despite the expansion of VLMs, no standardized benchmark exists to evaluate their performance in fetal ultrasound imaging. This gap is primarily due to the modality's challenging nature, operator dependency, and the limited public availability of datasets. To address this gap, we present Fetal-Gauge, the first and largest visual question answering benchmark specifically designed to evaluate VLMs across various fetal ultrasound tasks. Our benchmark comprises over 42,000 images and 93,000 question-answer pairs, spanning anatomical plane identification, visual grounding of anatomical structures, fetal orientation assessment, clinical view conformity, and clinical diagnosis. We systematically evaluate several state-of-the-art VLMs, including general-purpose and medical-specific models, and reveal a substantial performance gap: the best-performing model achieves only 55\% accuracy, far below clinical requirements. Our analysis identifies critical limitations of current VLMs in fetal ultrasound interpretation, highlighting the urgent need for domain-adapted architectures and specialized training approaches. Fetal-Gauge establishes a rigorous foundation for advancing multimodal deep learning in prenatal care and provides a pathway toward addressing global healthcare accessibility challenges. Our benchmark will be publicly available once the paper gets accepted.

</details>


### [24] [A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation](https://arxiv.org/abs/2512.22294)
*Philip Xu,David Elizondo,Raouf Hamzaoui*

Main category: cs.CV

TL;DR: 本文提出了Uni4D，一个用于大规模开放词汇3D检索和可控4D生成的统一框架，通过跨文本、3D模型和图像三层结构对齐实现高效语义匹配。


<details>
  <summary>Details</summary>
Motivation: 现有的3D检索和4D生成方法在跨模态语义对齐和动态生成能力上存在局限，且缺乏统一处理大规模多模态数据的框架。

Method: Uni4D基于Align3D 130数据集，引入3D文本多头注意力与检索模型，优化文本到3D的语义对齐。同时通过三个组件加强跨模态对齐：精确的文本-3D检索、多视角3D-图像对齐、图像-文本对齐，从而实现时序一致的4D生成。

Result: 实验表明，Uni4D实现了高质量的3D检索和可控的4D生成，在多模态动态理解和应用方面取得了进展。

Conclusion: Uni4D推动了动态多模态理解的发展，并为实际3D/4D相关应用提供了先进的技术支持。

Abstract: We introduce Uni4D, a unified framework for large scale open vocabulary 3D retrieval and controlled 4D generation based on structured three level alignment across text, 3D models, and image modalities. Built upon the Align3D 130 dataset, Uni4D employs a 3D text multi head attention and search model to optimize text to 3D retrieval through improved semantic alignment. The framework further strengthens cross modal alignment through three components: precise text to 3D retrieval, multi view 3D to image alignment, and image to text alignment for generating temporally consistent 4D assets. Experimental results demonstrate that Uni4D achieves high quality 3D retrieval and controllable 4D generation, advancing dynamic multimodal understanding and practical applications.

</details>


### [25] [Learning Dynamic Scene Reconstruction with Sinusoidal Geometric Priors](https://arxiv.org/abs/2512.22295)
*Tian Guo,Hui Yuan,Philip Xu,David Elizondo*

Main category: cs.CV

TL;DR: 提出了一种新的损失函数SirenPose，通过引入正弦表示网络的周期激活和关键点结构的几何先验，提升了动态3D场景重建的准确性，尤其是在处理快速运动和多目标场景时表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态、快速移动和多目标场景中，很难同时保持运动建模精度和时空一致性，亟需新的解决思路来提升3D重建的效果。

Method: 提出SirenPose损失函数，将正弦函数神经网络周期激活机制与关键点的几何先验相结合，并设计了仿物理的约束机制，确保关键点在空间和时间上的预测一致性。此外，扩展训练集至60万带注释实例，增强了模型的泛化能力。

Result: 采用SirenPose训练的模型，在时空一致性等多个指标上明显优于现有方法，特别擅长处理快速运动和复杂场景变化。

Conclusion: SirenPose显著提升了动态3D场景重建中的时空一致性和准确性，是应对复杂、快速变化场景的有效解决方案。

Abstract: We propose SirenPose, a novel loss function that combines the periodic activation properties of sinusoidal representation networks with geometric priors derived from keypoint structures to improve the accuracy of dynamic 3D scene reconstruction. Existing approaches often struggle to maintain motion modeling accuracy and spatiotemporal consistency in fast moving and multi target scenes. By introducing physics inspired constraint mechanisms, SirenPose enforces coherent keypoint predictions across both spatial and temporal dimensions. We further expand the training dataset to 600,000 annotated instances to support robust learning. Experimental results demonstrate that models trained with SirenPose achieve significant improvements in spatiotemporal consistency metrics compared to prior methods, showing superior performance in handling rapid motion and complex scene changes.

</details>


### [26] [Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware](https://arxiv.org/abs/2512.22298)
*Vesal Ahsani,Babak Hossein Khalaj*

Main category: cs.CV

TL;DR: 本文提出了一种适用于低成本硬件的车内司机行为识别系统，能实时识别包括分心、打瞌睡等17种行为，对硬件资源要求低，适合部署于树莓派和Google Coral等廉价设备。


<details>
  <summary>Details</summary>
Motivation: 现有司机监控系统对计算和功耗要求高，影响大规模部署，特别是在成本敏感的应用场景。需要研发高效率、低延迟且可靠的识别系统，以提升行车安全。

Method: 该系统基于单摄像头，采用紧凑的逐帧视觉模型，针对易混淆行为设计了区分性强的标签体系，并引入时序决策机制，只在预测稳定且置信度高时才发出警报。系统适配树莓派5和Google Coral Edge TPU两种低成本硬件，涵盖17种驾驶行为，训练与评估使用多样化数据集，并进行了车辆内真实测试。

Result: 优化后，系统在树莓派5上实现16 FPS（延迟<60ms），在Coral TPU上达25 FPS，可保证廉价硬件上的实时监控与稳定报警。

Conclusion: 该系统验证了低成本硬件也能支持高效、可靠的车内驾驶员状态识别，未来可作为智能座舱和自动驾驶车辆的人机感知基础模块。

Abstract: In-cabin Driver Monitoring Systems (DMS) must recognize distraction- and drowsiness-related behaviors with low latency under strict constraints on compute, power, and cost. We present a single-camera in-cabin driver behavior recognition system designed for deployment on two low-cost edge platforms: Raspberry Pi 5 (CPU-only) and Google Coral Edge TPU. The proposed pipeline combines (i) a compact per-frame vision model, (ii) a confounder-aware label design to reduce visually similar false positives, and (iii) a temporal decision head that triggers alerts only when predictions are both confident and sustained. The system covers 17 behavior classes, including multiple phone-use modes, eating/drinking, smoking, reaching behind, gaze/attention shifts, passenger interaction, grooming, control-panel interaction, yawning, and eyes-closed sleep. Training and evaluation use licensed datasets spanning diverse drivers, vehicles, and lighting conditions (details in Section 6), and we further validate runtime behavior in real in-vehicle tests. The optimized deployments achieve about 16 FPS on Raspberry Pi 5 with INT8 inference (per-frame latency under 60 ms) and about 25 FPS on Coral Edge TPU, enabling real-time monitoring and stable alert generation on inexpensive hardware. Finally, we discuss how reliable in-cabin human-state perception can serve as an upstream input for human-centered vehicle intelligence, including emerging agentic vehicle concepts.

</details>


### [27] [Attack-Aware Deepfake Detection under Counter-Forensic Manipulations](https://arxiv.org/abs/2512.22303)
*Noor Fatima,Hasan Faraz Khan,Muzammil Behzad*

Main category: cs.CV

TL;DR: 本论文提出了一种面向攻击的深度伪造与图像取证检测器，具备高鲁棒性、良好的概率校准和透明的可视化证据，尤其适用于现实部署环境。该方法融合红队训练与随机化的测试时防御，采用双分支结构，提升对各类攻击的检测与解释能力，并在多个深度伪造数据集和实际场景下展示了优异结果。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术和对抗手段的发展，传统的检测器面对各种现实攻击（如重压缩、降噪、色彩变换等）容易失效，且现有方法通常缺乏合理的概率校准、可解释性证据和鲁棒部署能力。因此，亟需设计既能抗攻击、又能给出可信决策依据的检测框架。

Method: 该方法采用红队训练（Worst-of-K最强对抗样本）结合测试时注入轻量扰动的防御策略，在二路网络中一支提取语义特征，另一支提取取证残差信息。两支特征以残差适配器融合，最终输出分类结果和带弱监督的伪造区域热力图。热力图利用人脸检测框约束，无需像素级标注。训练和测试阶段分别引入多样化攻击（如JPEG扰动、颜色调整、社交平台转码等）和随机化干预（如裁剪、相位扰动等），以增强模型鲁棒性。

Result: 在标准深度伪造数据集及实际场景（如低照度、高压缩监控视频）上，该方法取得了接近完美的攻击排序性能、极低的校准误差、可靠的能力分离（具有可控的拒判），且在各类真实和对抗攻击下性能稳健、退化受控，同时热力图具备较好的弱定位能力。

Conclusion: 该工作建立了一个兼具模块化、数据效率高、可实用且易部署的面向攻击检测基线。其低拒判风险、优秀校准和可操作的定位信息，使其在现实深度伪造检测与取证任务中具备显著优势和较强的实际价值。

Abstract: This work presents an attack-aware deepfake and image-forensics detector designed for robustness, well-calibrated probabilities, and transparent evidence under realistic deployment conditions. The method combines red-team training with randomized test-time defense in a two-stream architecture, where one stream encodes semantic content using a pretrained backbone and the other extracts forensic residuals, fused via a lightweight residual adapter for classification, while a shallow Feature Pyramid Network style head produces tamper heatmaps under weak supervision. Red-team training applies worst-of-K counter-forensics per batch, including JPEG realign and recompress, resampling warps, denoise-to-regrain operations, seam smoothing, small color and gamma shifts, and social-app transcodes, while test-time defense injects low-cost jitters such as resize and crop phase changes, mild gamma variation, and JPEG phase shifts with aggregated predictions. Heatmaps are guided to concentrate within face regions using face-box masks without strict pixel-level annotations. Evaluation on existing benchmarks, including standard deepfake datasets and a surveillance-style split with low light and heavy compression, reports clean and attacked performance, AUC, worst-case accuracy, reliability, abstention quality, and weak-localization scores. Results demonstrate near-perfect ranking across attacks, low calibration error, minimal abstention risk, and controlled degradation under regrain, establishing a modular, data-efficient, and practically deployable baseline for attack-aware detection with calibrated probabilities and actionable heatmaps.

</details>


### [28] [PortionNet: Distilling 3D Geometric Knowledge for Food Nutrition Estimation](https://arxiv.org/abs/2512.22304)
*Darrin Bright,Rakshith Raj,Kanchan Keisham*

Main category: cs.CV

TL;DR: 本文提出了一种仅需RGB图片即可准确估算食物营养的算法PortionNet，解决了以往需要深度传感器带来的硬件限制问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于单张图片估算食物营养的方法由于缺乏3D信息而存在准确度不足的问题，而依赖深度信息的方法又难以在大多数手机上应用。

Method: 提出PortionNet，这是一个跨模态知识蒸馏框架。训练时利用点云获取几何特征，推理时只需RGB图片。同时采用双模式训练和轻量化adapter网络，使模型能在无专用硬件的情况下实现伪3D推理。

Result: PortionNet在MetaFood3D数据集上于体积和能量估算任务均超越现有方法，在SimpleFood45数据集上的跨数据集评估也显示出能源估算的强泛化能力。

Conclusion: PortionNet有效解决了单图像估算食物营养难题，实现了无需专用硬件的高准确伪3D推理，具有很好的实际应用前景。

Abstract: Accurate food nutrition estimation from single images is challenging due to the loss of 3D information. While depth-based methods provide reliable geometry, they remain inaccessible on most smartphones because of depth-sensor requirements. To overcome this challenge, we propose PortionNet, a novel cross-modal knowledge distillation framework that learns geometric features from point clouds during training while requiring only RGB images at inference. Our approach employs a dual-mode training strategy where a lightweight adapter network mimics point cloud representations, enabling pseudo-3D reasoning without any specialized hardware requirements. PortionNet achieves state-of-the-art performance on MetaFood3D, outperforming all previous methods in both volume and energy estimation. Cross-dataset evaluation on SimpleFood45 further demonstrates strong generalization in energy estimation.

</details>


### [29] [MoFu: Scale-Aware Modulation and Fourier Fusion for Multi-Subject Video Generation](https://arxiv.org/abs/2512.22310)
*Run Ling,Ke Cao,Jian Lu,Ao Ma,Haowei Liu,Runze He,Changwei Wang,Rongtao Xu,Yihua Shao,Zhanjie Zhang,Peng Wu,Guibing Guo,Wei Feng,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law,Xingwei Wang*

Main category: cs.CV

TL;DR: 该论文提出了MoFu框架，用于解决多主体视频生成中尺度不一致和排列敏感两大难题，通过引入尺度感知调制和傅里叶融合策略，显著提升了生成视频的主体现实感和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 当前多主体视频生成方法在实际应用中常出现两个主要挑战：一是主体尺度不一致，导致生成视频里的主体比例不自然；二是排列敏感性，即参考图像的排列顺序会影响生成效果，产生主体畸变。论文旨在解决这两个核心难题，以提升视频生成的表现。

Method: 1. 引入尺度感知调制（Scale-Aware Modulation, SMO）模块，利用大模型（LLM）引导，从文本提示中提取隐式尺度信息，并对特征进行调制，确保主体尺度一致。
2. 提出傅里叶融合（Fourier Fusion）策略，利用快速傅里叶变换处理参考图片的频率信息，将其融合为统一表示，从而消除排列敏感性。
3. 设计尺度-排列稳定性损失函数（Scale-Permutation Stability Loss），联合提升模型对尺度和排列的稳定性。
4. 构建针对尺度和排列敏感性的专门基准测试。

Result: 实验表明MoFu在主体尺度一致性、视觉保真度以及整体视觉质量方面，均显著优于现有多主体视频生成方法。

Conclusion: MoFu框架有效解决了多主体视频生成中的尺度不一致和排列敏感性问题，提升了生成视频的自然性和鲁棒性，可为相关领域提供更高质量的视频合成基础。

Abstract: Multi-subject video generation aims to synthesize videos from textual prompts and multiple reference images, ensuring that each subject preserves natural scale and visual fidelity. However, current methods face two challenges: scale inconsistency, where variations in subject size lead to unnatural generation, and permutation sensitivity, where the order of reference inputs causes subject distortion. In this paper, we propose MoFu, a unified framework that tackles both challenges. For scale inconsistency, we introduce Scale-Aware Modulation (SMO), an LLM-guided module that extracts implicit scale cues from the prompt and modulates features to ensure consistent subject sizes. To address permutation sensitivity, we present a simple yet effective Fourier Fusion strategy that processes the frequency information of reference features via the Fast Fourier Transform to produce a unified representation. Besides, we design a Scale-Permutation Stability Loss to jointly encourage scale-consistent and permutation-invariant generation. To further evaluate these challenges, we establish a dedicated benchmark with controlled variations in subject scale and reference permutation. Extensive experiments demonstrate that MoFu significantly outperforms existing methods in preserving natural scale, subject fidelity, and overall visual quality.

</details>


### [30] [VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning](https://arxiv.org/abs/2512.22315)
*Yang Ding,Yizhen Zhang,Xin Lai,Ruihang Chu,Yujiu Yang*

Main category: cs.CV

TL;DR: 本文提出了VideoZoomer框架，使多模态大模型（MLLMs）在长视频理解任务中可以动态调整视觉关注，实现高效且准确的推理。该方法有效提升了MLLMs在长视频理解上的性能，超越了目前的开源和一些闭源系统。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在长视频理解上受限于上下文窗口长度，常用的帧采样策略容易遗漏关键信息且缺乏纠错能力。为弥补这一不足，迫切需要一种动态、交互式地聚焦视频关键信息的新方法。

Method: 提出VideoZoomer智能体框架：首先以低帧率获取全局视频概览，然后通过“时间缩放”工具，在关键时刻动态抽取高帧率片段，分阶段逐步获取细粒度证据。训练过程中，先用精心设计的数据集进行监督微调，再用强化学习优化策略。

Result: 实验展现出该7B参数的模型在多种长视频理解和推理基准上均取得优异成绩，表现出丰富、复杂的推理模式，效率和准确性均显著优于现有开源模型，甚至可媲美部分闭源系统。

Conclusion: VideoZoomer方法通过动态调整视觉注意力，有效突破了MLLMs在长视频理解中的瓶颈，为复杂视频推理任务提供了高效、可靠的解决方案，并推动了多模态长视频理解的发展。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language tasks yet remain limited in long video understanding due to the limited context window. Consequently, prevailing approaches tend to rely on uniform frame sampling or static pre-selection, which might overlook critical evidence and unable to correct its initial selection error during its reasoning process. To overcome these limitations, we propose VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control their visual focus during reasoning. Starting from a coarse low-frame-rate overview, VideoZoomer invokes a temporal zoom tool to obtain high-frame-rate clips at autonomously chosen moments, thereby progressively gathering fine-grained evidence in a multi-turn interactive manner. Accordingly, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase on a curated dataset of distilled exemplar and reflection trajectories, followed by reinforcement learning to further refine the agentic policy. Extensive experiments demonstrate that our 7B model delivers diverse and complex reasoning patterns, yielding strong performance across a broad set of long video understanding and reasoning benchmarks. These emergent capabilities allow it to consistently surpass existing open-source models and even rival proprietary systems on challenging tasks, while achieving superior efficiency under reduced frame budgets.

</details>


### [31] [SpotEdit: Selective Region Editing in Diffusion Transformers](https://arxiv.org/abs/2512.22323)
*Zhibin Qin,Zhenxiong Tan,Zeqing Wang,Songhua Liu,Xinchao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种高效精准的无训练扩散图像编辑框架SpotEdit，仅编辑局部区域，避免整体重复计算。


<details>
  <summary>Details</summary>
Motivation: 目前大多数扩散式图像编辑方法，在每一步都对所有图像区域统一处理，导致编辑效率低下及未更改区域质量下降。而在实际编辑任务中，通常只需修改图像的局部区域。研究动机在于提升编辑效率，同时保持未修改区域的高保真。

Method: 作者提出了SpotEdit框架，无需重新训练，包括两个关键组件：1）SpotSelector：通过感知相似度识别并跳过稳定区域，并重用条件图像特征；2）SpotFusion：动态融合机制，将这些稳定区域特征与编辑区域的token自适应融合，从而实现区域级的差异编辑。

Result: 实验结果显示，SpotEdit在无需训练的前提下，有效提升了图像编辑的效率，减少了冗余计算，并且在未编辑区域保持了更高的保真度和整体一致性。

Conclusion: SpotEdit证明选择性编辑并跳过不变区域可实现更高效、保真的图像高级编辑，为扩散编辑模型提供了新思路。

Abstract: Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.

</details>


### [32] [DeMoGen: Towards Decompositional Human Motion Generation with Energy-Based Diffusion Models](https://arxiv.org/abs/2512.22324)
*Jianrong Zhang,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的动作分解训练范式，能够将复杂的人体动作分解为有意义的子组件，实现动作原语的自动发现与组合。


<details>
  <summary>Details</summary>
Motivation: 现有的人体动作建模多集中于从文本到整体动作的映射，或者将复杂动作由若干概念组合而成，较少关注如何将整体动作反向拆解为基本的、有语义的动作原语。反向分解可帮助理解复杂行为的组成结构，提升动作生成、编辑和复用能力。

Method: 提出了DeMoGen分解生成范式，基于能量型扩散模型实现动作分解，并且设计了三种训练变体（DeMoGen-Exp、DeMoGen-OSS、DeMoGen-SC），分别利用显性分解文本、正交自监督分解及语义一致性促进模型学习将整体动作拆解为语义清晰、可复用的基本动作单元。还构建了文本分解数据集用于支持训练。

Result: 实验表明，DeMoGen不仅可以有效将复杂动作序列分解为可复用动作原语，还能灵活重组这些分解得到的动作概念，生成多样且新颖的动作，具备超出训练分布的泛化能力。

Conclusion: 本文方法实现了无监督或弱监督条件下的动作分解与合成，推动了从行为理解到动作生成的可组合式研究，对动作分析、合成和编辑有重要意义。

Abstract: Human motions are compositional: complex behaviors can be described as combinations of simpler primitives. However, existing approaches primarily focus on forward modeling, e.g., learning holistic mappings from text to motion or composing a complex motion from a set of motion concepts. In this paper, we consider the inverse perspective: decomposing a holistic motion into semantically meaningful sub-components. We propose DeMoGen, a compositional training paradigm for decompositional learning that employs an energy-based diffusion model. This energy formulation directly captures the composed distribution of multiple motion concepts, enabling the model to discover them without relying on ground-truth motions for individual concepts. Within this paradigm, we introduce three training variants to encourage a decompositional understanding of motion: 1. DeMoGen-Exp explicitly trains on decomposed text prompts; 2. DeMoGen-OSS performs orthogonal self-supervised decomposition; 3. DeMoGen-SC enforces semantic consistency between original and decomposed text embeddings. These variants enable our approach to disentangle reusable motion primitives from complex motion sequences. We also demonstrate that the decomposed motion concepts can be flexibly recombined to generate diverse and novel motions, generalizing beyond the training distribution. Additionally, we construct a text-decomposed dataset to support compositional training, serving as an extended resource to facilitate text-to-motion generation and motion composition.

</details>


### [33] [The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma](https://arxiv.org/abs/2512.22331)
*Mariya Miteva,Maria Nisheva-Pavlova*

Main category: cs.CV

TL;DR: 本文提出了一种基于变分自编码器（VAE）的多视角潜在表示学习框架，将T1Gd和FLAIR MRI的放射组学特征进行整合，用于无创推断胶质母细胞瘤（GBM）中MGMT启动子甲基化状态。该模型通过独立编码器处理每个模态，融合于紧凑潜在空间，有效保留模态特性并提升多模态整合能力，最终应用于MGMT甲基化分类。


<details>
  <summary>Details</summary>
Motivation: MGMT启动子甲基化状态对GBM预后和治疗极具意义。现有影像组学机器学习方法常受特征冗余高和对模态特异性建模不全的困扰，限制了无创预测MGMT甲基化的临床应用。作者旨在解决多模态信息整合和建模不充分的问题。

Method: 构建基于变分自编码器（VAE）的多视角学习框架，分别用概率编码器提取T1Gd和FLAIR MRI放射组学特征，再于潜在空间进行特征融合，既保留每种MRI模式的特异性，也强化多模态信息整合。将所得潜在嵌入向量作为特征进行MGMT甲基化分类。

Result: 该方法能有效压缩和融合不同MRI模态的特征，相比传统单模态及简单模态融合方式更好地保留模态特异性，提升了MGMT甲基化的分类效果。

Conclusion: 多视角VAE框架在胶质母细胞瘤放射基因组学任务中，实现了多模态影像特征的高效整合，并提升了MGMT甲基化状态的机器学习判别性能，对无创分子分型具有潜在临床价值。

Abstract: Non-invasive inference of molecular tumor characteristics from medical imaging is a central goal of radiogenomics, particularly in glioblastoma (GBM), where O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation carries important prognostic and therapeutic significance. Although radiomics-based machine learning methods have shown promise for this task, conventional unimodal and early-fusion approaches are often limited by high feature redundancy and an incomplete modeling of modality-specific information. In this work, we introduce a multi-view latent representation learning framework based on variational autoencoders (VAE) to integrate complementary radiomic features derived from post-contrast T1-weighted (T1Gd) and Fluid-Attenuated Inversion Recovery (FLAIR) magnetic resonance imaging (MRI). By encoding each modality through an independent probabilistic encoder and performing fusion in a compact latent space, the proposed approach preserves modality-specific structure while enabling effective multimodal integration. The resulting latent embeddings are subsequently used for MGMT promoter methylation classification.

</details>


### [34] [Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides](https://arxiv.org/abs/2512.22335)
*Olaide N. Oyelade,Oliver Hoxey,Yulia Humrye*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的视觉Transformer系统，同时利用H&E和IHC病理切片图像，实现HER2蛋白表达状态的自动、像素级分数标注，取得了高精度识别效果。


<details>
  <summary>Details</summary>
Motivation: HER2状态的准确判定对癌症治疗极为重要，然而现有方法在像素级HER2定位和联合分析H&E/IHC图像方面存在挑战，因此亟需更精准、高效的自动化方案。

Method: 采用基于视觉Transformer（ViT）的端到端流水线，对H&E全切片图像分块，先实现肿瘤区域定位，通过新颖的映射函数将H&E中的恶性区域与IHC图像中的相关区域对应，并在此基础上实现自动像素级的4级HER2评分。

Result: 实验在13例H&E和IHC切片私有数据集上进行，肿瘤定位及HER2状态预测均取得了高准确率，4分类HER2评分准确率达0.94，特异性为0.933，性能可与人类病理专家相媲美。

Conclusion: 该研究验证了ViT模型对H&E和IHC切片联合分析的有效性，为HER2自动化精准评分提供了新路径，并有望辅助临床癌症治疗决策。

Abstract: The popular use of histopathology images, such as hematoxylin and eosin (H&E), has proven to be useful in detecting tumors. However, moving such cancer cases forward for treatment requires accurate on the amount of the human epidermal growth factor receptor 2 (HER2) protein expression. Predicting both the lower and higher levels of HER2 can be challenging. Moreover, jointly analyzing H&E and immunohistochemistry (IHC) stained images for HER2 scoring is difficult. Although several deep learning methods have been investigated to address the challenge of HER2 scoring, they suffer from providing a pixel-level localization of HER2 status. In this study, we propose a single end-to-end pipeline using a system of vision transformers with HER2 status scoring on whole slide images of WSIs. The method includes patch-wise processing of H&E WSIs for tumor localization. A novel mapping function is proposed to correspondingly identify correlated IHC WSIs regions with malignant regions on H&E. A clinically inspired HER2 scoring mechanism is embedded in the pipeline and allows for automatic pixel-level annotation of 4-way HER2 scoring (0, 1+, 2+, and 3+). Also, the proposed method accurately returns HER2-negative and HER2-positive. Privately curated datasets were collaboratively extracted from 13 different cases of WSIs of H&E and IHC. A thorough experiment is conducted on the proposed method. Results obtained showed a good classification accuracy during tumor localization. Also, a classification accuracy of 0.94 and a specificity of 0.933 were returned for the prediction of HER2 status, scoring in the 4-way methods. The applicability of the proposed pipeline was investigated using WSIs patches as comparable to human pathologists. Findings from the study showed the usability of jointly evaluated H&E and IHC images on end-to-end ViTs-based models for HER2 scoring

</details>


### [35] [Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data](https://arxiv.org/abs/2512.22349)
*Alaa Alahmadi,Mohamed Hasan*

Main category: cs.CV

TL;DR: 本文提出一种基于人类感知的伪彩色编码技术，提升了深度学习模型在心电图分析中的可解释性与小样本学习能力，尤其适用于数据稀缺和复杂表型场景。


<details>
  <summary>Details</summary>
Motivation: 当前机器视觉模型（如深度神经网络）在生理信号分析中广泛应用，但普遍依赖大规模训练数据，并且对其预测的因果特征解释能力有限，限制了其临床可用性与与人类推理的契合度。特别是在罕见但临床高度相关的异常情况（如药物致长QT综合征，LQTS）中，训练样本极为稀缺，对模型的泛化和解释能力提出了挑战。

Method: 作者引入一种感知驱动的伪彩色编码方式，将心电图中的关键时序特征（如QT间期）编码为结构化颜色信息。然后基于这种伪彩色心电图图像，采用ResNet-18和原型网络，在极小样本（一例或五例）条件下，进行一次性(one-shot)和小样本(few-shot)学习，并对模型可解释性进行分析。还对单心动周期与完整10秒心律数据分别进行了实验，并尝试多周期平均，贴合临床认知习惯。

Result: 通过伪彩色编码，模型能仅用极少训练样本快速学习到判别性和可解释性强的特征，显著提升LQTS检测的准确性；可解释性分析显示模型聚焦于临床意义关键区域，忽略无关成分；多心动周期聚合进一步提升模型表现。

Conclusion: 感知驱动的伪彩色编码不仅提升了模型对数据稀缺场景的学习效率，还增强了其可解释性与因果特征推断能力，有望实现临床智能分析向“更像人思考”的方向转变。

Abstract: Machine vision models, particularly deep neural networks, are increasingly applied to physiological signal interpretation, including electrocardiography (ECG), yet they typically require large training datasets and offer limited insight into the causal features underlying their predictions. This lack of data efficiency and interpretability constrains their clinical reliability and alignment with human reasoning. Here, we show that a perception-informed pseudo-colouring technique, previously demonstrated to enhance human ECG interpretation, can improve both explainability and few-shot learning in deep neural networks analysing complex physiological data.
  We focus on acquired, drug-induced long QT syndrome (LQTS) as a challenging case study characterised by heterogeneous signal morphology, variable heart rate, and scarce positive cases associated with life-threatening arrhythmias such as torsades de pointes. This setting provides a stringent test of model generalisation under extreme data scarcity. By encoding clinically salient temporal features, such as QT-interval duration, into structured colour representations, models learn discriminative and interpretable features from as few as one or five training examples. Using prototypical networks and a ResNet-18 architecture, we evaluate one-shot and few-shot learning on ECG images derived from single cardiac cycles and full 10-second rhythms. Explainability analyses show that pseudo-colouring guides attention toward clinically meaningful ECG features while suppressing irrelevant signal components. Aggregating multiple cardiac cycles further improves performance, mirroring human perceptual averaging across heartbeats. Together, these findings demonstrate that human-like perceptual encoding can bridge data efficiency, explainability, and causal reasoning in medical machine intelligence.

</details>


### [36] [VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement](https://arxiv.org/abs/2512.22351)
*Zhengfei Kuang,Rui Lin,Long Zhao,Gordon Wetzstein,Saining Xie,Sanghyun Woo*

Main category: cs.CV

TL;DR: 该论文针对多模态大模型（MLLMs）目前在3D场景操作中的不足，提出了一套包含API、视觉分析工具及多智能体协作的新方法，实现了更精准且鲁棒的3D物体排列任务，大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在2D视觉-语言任务中取得了显著进展，但在3D场景复杂操控方面仍有很大挑战，特别是在精确操控、视觉与程序行为关联不牢、以及多步骤操作中的错误恢复问题未被充分解决。

Method: 1）提出基于MCP的API，将对3D场景的操作从底层代码转为函数级交互，更稳健地实现编辑；2）引入专业化视觉分析工具，帮助MLLM分析3D场景、获取空间信息并校验操作结果，形成感知-动作反馈环；3）采用多智能体框架，将3D编辑任务分解为规划、执行和验证等角色，实现多步骤指令的协同处理和鲁棒性增强。

Result: 在25个复杂3D对象排列的任务集上进行实验，新方法在准确性和鲁棒性上均显著优于现有基线方法。

Conclusion: 该方法有效弥补了MLLMs在3D场景操控上的不足，为多模态大模型的3D理解与编辑开辟了新路径，对3D智能交互应用有重要提升作用。

Abstract: Despite the remarkable progress of Multimodal Large Language Models (MLLMs) in 2D vision-language tasks, their application to complex 3D scene manipulation remains underexplored. In this paper, we bridge this critical gap by tackling three key challenges in 3D object arrangement task using MLLMs. First, to address the weak visual grounding of MLLMs, which struggle to link programmatic edits with precise 3D outcomes, we introduce an MCP-based API. This shifts the interaction from brittle raw code manipulation to more robust, function-level updates. Second, we augment the MLLM's 3D scene understanding with a suite of specialized visual tools to analyze scene state, gather spatial information, and validate action outcomes. This perceptual feedback loop is critical for closing the gap between language-based updates and precise 3D-aware manipulation. Third, to manage the iterative, error-prone updates, we propose a collaborative multi-agent framework with designated roles for planning, execution, and verification. This decomposition allows the system to robustly handle multi-step instructions and recover from intermediate errors. We demonstrate the effectiveness of our approach on a diverse set of 25 complex object arrangement tasks, where it significantly outperforms existing baselines. Website: vulcan-3d.github.io

</details>


### [37] [Self-Evaluation Unlocks Any-Step Text-to-Image Generation](https://arxiv.org/abs/2512.22374)
*Xin Yu,Xiaojuan Qi,Zhengqi Li,Kai Zhang,Richard Zhang,Zhe Lin,Eli Shechtman,Tianyu Wang,Yotam Nitzan*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本到图像生成方法Self-E，该方法支持任意步数的推理，且无需预训练教师模型或大量推理步骤，在少步推理和高质量生成上均表现优秀，是首个能够从零开始支持任意步推理的统一模型框架。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散或流匹配模型在文本到图像生成时，要么依赖于大量的局部监督（导致推理步骤多，速度慢），要么依赖于蒸馏法（需要预训练教师），难以兼顾高效率（少步推理）和高质量生成。作者希望打破这一二分，提出一种无需教师、支持任意步高质量生成的新方法。

Method: Self-E在训练时结合了流匹配模型的学习方式，并引入了自我评估机制：模型用自身当前的评分去评估生成样本，相当于成为自己的动态教师。这样模型一方面获得即时局部的学习信号，一方面能推动全局一致性，实现高效且高质量的从零训练。

Result: 在大规模文本到图像生成基准上，Self-E在少步生成时表现尤为突出，且在50步推理时能达到与顶尖流匹配模型相当的水平；同时其性能随着推理步数的增加持续提升，支持又快又优的图像生成。

Conclusion: Self-E首次实现了从零开始、任意步推理的文本到图像生成，统一了推理效率与生成质量，是高效、可扩展的文本到图像生成新框架。

Abstract: We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation.

</details>


### [38] [iOSPointMapper: RealTime Pedestrian and Accessibility Mapping with Mobile AI](https://arxiv.org/abs/2512.22392)
*Himanshu Naidu,Yuxiang Zhang,Sachin Mehta,Anat Caspi*

Main category: cs.CV

TL;DR: 本文提出一款名为iOSPointMapper的手机应用，利用iPhone/iPad的传感器在实时、保护隐私的前提下采集人行道相关数据，以提升步行基础设施的可达性与包容性。


<details>
  <summary>Details</summary>
Motivation: 现有人行道数据采集方式成本高、分散且难以规模化，阻碍了无障碍与包容性步行基础设施的建设，需要一种高效、易推广的方法来弥补数据空白。

Method: 开发了iOSPointMapper应用，结合iOS设备的语义分割、LiDAR深度估计及GPS/IMU数据，检测并定位与人行道相关的要素。还设计了用户引导的注释界面，对检测结果进行人工校验，保证结果透明与高质量。最终将数据匿名化上传至TDEI，与多模式交通数据集成。

Result: 系统详细评估表明，该方法在人行道特征检测和空间定位方面性能良好，能够有效提升步行地图质量。

Conclusion: iOSPointMapper为步行基础设施数据采集提供了可扩展、以用户为中心的新路径，有助于消除当前行人交通数据的关键缺口，促进包容与便利的步行环境建设。

Abstract: Accurate, up-to-date sidewalk data is essential for building accessible and inclusive pedestrian infrastructure, yet current approaches to data collection are often costly, fragmented, and difficult to scale. We introduce iOSPointMapper, a mobile application that enables real-time, privacy-conscious sidewalk mapping on the ground, using recent-generation iPhones and iPads. The system leverages on-device semantic segmentation, LiDAR-based depth estimation, and fused GPS/IMU data to detect and localize sidewalk-relevant features such as traffic signs, traffic lights and poles. To ensure transparency and improve data quality, iOSPointMapper incorporates a user-guided annotation interface for validating system outputs before submission. Collected data is anonymized and transmitted to the Transportation Data Exchange Initiative (TDEI), where it integrates seamlessly with broader multimodal transportation datasets. Detailed evaluations of the system's feature detection and spatial mapping performance reveal the application's potential for enhanced pedestrian mapping. Together, these capabilities offer a scalable and user-centered approach to closing critical data gaps in pedestrian

</details>


### [39] [DeFloMat: Detection with Flow Matching for Stable and Efficient Generative Object Localization](https://arxiv.org/abs/2512.22406)
*Hansang Lee,Chaelin Lee,Nieun Seo,Joon Seok Lim,Helen Hong*

Main category: cs.CV

TL;DR: DeFloMat是一种新颖的生成式目标检测框架，通过流匹配方法显著加速扩散模型，兼顾高精度与高效率，特别适用于临床高时效需求场景。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型如DiffusionDet在目标检测中虽然精度高，但推理速度慢，不适用于对时效性有较高要求的医疗场景（如Crohn's疾病的MRE检测），因此亟需提升扩散检测器的推理效率。

Method: 作者提出用条件流匹配（Conditional Flow Matching, CFM）方法，用基于条件最优传输理论的确定性流场（近似Rectified Flow）替代慢速的随机扩散路径，将推理过程转化为简单的常微分方程（ODE）求解，大幅减少推理步骤，提升速度。

Result: 在挑战性的MRE临床数据集上，DeFloMat仅用3步推理就达到43.32%的AP（优于DiffusionDet的4步最大AP 31.03%），在回忆率和定位稳定性上也表现更佳，显著优于现有生成式检测器。

Conclusion: DeFloMat有效解决了生成精度与临床效率之间的矛盾，在实现快速和稳定目标定位的同时，设立了新的性能标准，对时效性要求高的生成式目标检测具有重要意义。

Abstract: We propose DeFloMat (Detection with Flow Matching), a novel generative object detection framework that addresses the critical latency bottleneck of diffusion-based detectors, such as DiffusionDet, by integrating Conditional Flow Matching (CFM). Diffusion models achieve high accuracy by formulating detection as a multi-step stochastic denoising process, but their reliance on numerous sampling steps ($T \gg 60$) makes them impractical for time-sensitive clinical applications like Crohn's Disease detection in Magnetic Resonance Enterography (MRE). DeFloMat replaces this slow stochastic path with a highly direct, deterministic flow field derived from Conditional Optimal Transport (OT) theory, specifically approximating the Rectified Flow. This shift enables fast inference via a simple Ordinary Differential Equation (ODE) solver. We demonstrate the superiority of DeFloMat on a challenging MRE clinical dataset. Crucially, DeFloMat achieves state-of-the-art accuracy ($43.32\% \text{ } AP_{10:50}$) in only $3$ inference steps, which represents a $1.4\times$ performance improvement over DiffusionDet's maximum converged performance ($31.03\% \text{ } AP_{10:50}$ at $4$ steps). Furthermore, our deterministic flow significantly enhances localization characteristics, yielding superior Recall and stability in the few-step regime. DeFloMat resolves the trade-off between generative accuracy and clinical efficiency, setting a new standard for stable and rapid object localization.

</details>


### [40] [Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy](https://arxiv.org/abs/2512.22423)
*Amil Khan,Matheus Palhares Viana,Suraj Mishra,B. S. Manjunath*

Main category: cs.CV

TL;DR: 本论文提出了Bright-4B模型，能在无需荧光和后处理的情况下，直接从3D明场显微图像中实现亚细胞结构的高精度分割。


<details>
  <summary>Details</summary>
Motivation: 传统的3D明场显微镜成像，虽然快速且无创，但其亚细胞结构分割高度依赖荧光标记或繁琐后处理，阻碍了无标签大规模细胞谱系绘制。作者希望通过开发一个能直接基于明场体数据分割的高性能基础模型，解决该领域瓶颈。

Method: 作者开发了参数量达40亿的Bright-4B基础模型，基于超球面进行学习。模型创新点包括本地与全局上下文结合的稀疏注意力机制、深宽残差超连接结构（促进表达稳定）、和值混合专家机制（自适应算力分配），以及兼容各向异性信息的分块嵌入（匹配共聚焦PSF和轴向采样薄弱等特点），实现符合几何真实性的3D分割。

Result: Bright-4B仅基于明场堆叠即可准确分割细胞核、线粒体等细胞器，无需荧光、辅助通道或手工特征后处理。实验证明该模型在多种数据集和不同深度、细胞类型下，都超越了主流CNN和Transformer模型，尤其在细节保持上表现优异。

Conclusion: Bright-4B实现了无需标签的高精度3D细胞分割，为无标记、规模化细胞谱绘打开了新路径。团队将开放所有代码、预训练权重和下游微调模型，推动基础生物影像分析的发展。

Abstract: Label-free 3D brightfield microscopy offers a fast and noninvasive way to visualize cellular morphology, yet robust volumetric segmentation still typically depends on fluorescence or heavy post-processing. We address this gap by introducing Bright-4B, a 4 billion parameter foundation model that learns on the unit hypersphere to segment subcellular structures directly from 3D brightfield volumes. Bright-4B combines a hardware-aligned Native Sparse Attention mechanism (capturing local, coarse, and selected global context), depth-width residual HyperConnections that stabilize representation flow, and a soft Mixture-of-Experts for adaptive capacity. A plug-and-play anisotropic patch embed further respects confocal point-spread and axial thinning, enabling geometry-faithful 3D tokenization. The resulting model produces morphology-accurate segmentations of nuclei, mitochondria, and other organelles from brightfield stacks alone--without fluorescence, auxiliary channels, or handcrafted post-processing. Across multiple confocal datasets, Bright-4B preserves fine structural detail across depth and cell types, outperforming contemporary CNN and Transformer baselines. All code, pretrained weights, and models for downstream finetuning will be released to advance large-scale, label-free 3D cell mapping.

</details>


### [41] [FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning](https://arxiv.org/abs/2512.22425)
*Ujunwa Mgboh,Rafi Ibn Sultan,Joshua Kim,Kundan Thind,Dongxiao Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于transformer的流强图（fluence map）预测模型FluenceFormer，用于自动放疗计划，提高了结构一致性和物理可实现性。


<details>
  <summary>Details</summary>
Motivation: 流强图预测在放射治疗自动化中至关重要，但由于体积解剖结构与束流强度调制之间关系复杂，一直是一个病态的逆问题。现有卷积方法难以建模长距离依赖，导致计划结构不一致或不可实现。

Method: 提出了FluenceFormer，一种主干网络无关的transformer框架。分两阶段：第一阶段基于解剖输入预测全局剂量场先验，第二阶段结合束流几何回归物理校准的流强图，并提出了物理驱动的FAR损失函数，融合体素级精度、梯度平滑、结构一致性及束流能量守恒。

Result: 在前列腺IMRT数据集上评估了多种transformer主干（含Swin UNETR等），FluenceFormer（特别是Swin UNETR版本）在能量误差（降到4.5%）、结构保真度等方面领先于现有的CNN及单阶段方法，并且提升具有统计学意义（p<0.05）。

Conclusion: FluenceFormer实现了更优的流强图预测，兼顾物理可实现性和结构保真，在多种主干网络下表现泛化强，优于传统和单阶段方法。

Abstract: Fluence map prediction is central to automated radiotherapy planning but remains an ill-posed inverse problem due to the complex relationship between volumetric anatomy and beam-intensity modulation. Convolutional methods in prior work often struggle to capture long-range dependencies, which can lead to structurally inconsistent or physically unrealizable plans. We introduce \textbf{FluenceFormer}, a backbone-agnostic transformer framework for direct, geometry-aware fluence regression. The model uses a unified two-stage design: Stage~1 predicts a global dose prior from anatomical inputs, and Stage~2 conditions this prior on explicit beam geometry to regress physically calibrated fluence maps. Central to the approach is the \textbf{Fluence-Aware Regression (FAR)} loss, a physics-informed objective that integrates voxel-level fidelity, gradient smoothness, structural consistency, and beam-wise energy conservation. We evaluate the generality of the framework across multiple transformer backbones, including Swin UNETR, UNETR, nnFormer, and MedFormer, using a prostate IMRT dataset. FluenceFormer with Swin UNETR achieves the strongest performance among the evaluated models and improves over existing benchmark CNN and single-stage methods, reducing Energy Error to $\mathbf{4.5\%}$ and yielding statistically significant gains in structural fidelity ($p < 0.05$).

</details>


### [42] [EmoCtrl: Controllable Emotional Image Content Generation](https://arxiv.org/abs/2512.22437)
*Jingyuan Yang,Weibin Luo,Hui Huang*

Main category: cs.CV

TL;DR: 本文提出了一种可控情绪图像内容生成方法（C-EICG），能够在保持图片内容一致的基础上，实现对图像情感的有效控制，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 图像既包含内容也表达情感，现有方法在保证内容与表达情感之间难以兼顾：内容一致性强的方法缺乏情感表达，情感驱动模型则常导致内容失真。需要一种方法协调内容与情绪的关系。

Method: 提出了EmoCtrl模型，并构建了包含内容、情感和情感提示语标注的数据集。EmoCtrl引入文本与视觉情感增强模块，通过语义与感知信号提升情感表达。同时学习到的情感token可以互补增强情感表达能力，方法经过消融实验和可视化结果验证。

Result: 定量和定性实验表明，EmoCtrl在内容保真和情感表达控制方面均超过现有方法，获得更好的用户偏好一致性。情感token具有较强的适应性与泛化能力，在创意应用中保持良好表现。

Conclusion: EmoCtrl有效解决了内容与情感表达的平衡问题，为情绪可控图像合成领域带来更精准、适应性强的解决方案。

Abstract: An image conveys meaning through both its visual content and emotional tone, jointly shaping human perception. We introduce Controllable Emotional Image Content Generation (C-EICG), which aims to generate images that remain faithful to a given content description while expressing a target emotion. Existing text-to-image models ensure content consistency but lack emotional awareness, whereas emotion-driven models generate affective results at the cost of content distortion. To address this gap, we propose EmoCtrl, supported by a dataset annotated with content, emotion, and affective prompts, bridging abstract emotions to visual cues. EmoCtrl incorporates textual and visual emotion enhancement modules that enrich affective expression via descriptive semantics and perceptual cues. The learned emotion tokens exhibit complementary effects, as demonstrated through ablations and visualizations. Quantatitive and qualatitive experiments demonstrate that EmoCtrl achieves faithful content and expressive emotion control, outperforming existing methods across multiple aspects. User studies confirm EmoCtrl's strong alignment with human preference. Moreover, EmoCtrl generalizes well to creative applications, further demonstrating the robustness and adaptability of the learned emotion tokens.

</details>


### [43] [Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing](https://arxiv.org/abs/2512.22464)
*Sukhyun Jeong,Yong-Hoon Choi*

Main category: cs.CV

TL;DR: 本论文提出了一种新的混合表示方法，结合可解释的姿态码与通过残差向量量化学到的残差码，从而提升基于文本的3D动作生成和编辑的质量、可控性与细致度。


<details>
  <summary>Details</summary>
Motivation: 现有基于姿态码的动作生成框架在捕捉时间动态和高频细节方面存在局限，导致重建效果和局部可控性下降。为了解决这一问题，作者提出改进方法。

Method: 提出了PGR^2M框架。首先，通过姿态引导的残差向量量化器将动作分解为编码整体结构的姿态潜编码和捕捉细粒度时序变化的残差潜编码。然后，主干Transformer根据文本预测姿态码，细化Transformer在文本、姿态码和量化阶段的条件下预测残差码。残差dropout机制抑制过度依赖残差，保持语义对齐及姿态码的可编辑性。

Result: 在HumanML3D和KIT-ML数据集上，PGR^2M在生成和编辑任务中均优于CoMo及相关扩散和tokenization基线。在Fréchet inception distance和重建指标方面取得更优成绩，用户调研也证实了方法的编辑直观性和结构保持性。

Conclusion: PGR^2M能够综合提升文本驱动的3D动作生成和动作编辑的质量、可控性与结构保持性，为高效精细地控制3D动作提供了新途径。

Abstract: Text-based 3D motion generation aims to automatically synthesize diverse motions from natural-language descriptions to extend user creativity, whereas motion editing modifies an existing motion sequence in response to text while preserving its overall structure. Pose-code-based frameworks such as CoMo map quantifiable pose attributes into discrete pose codes that support interpretable motion control, but their frame-wise representation struggles to capture subtle temporal dynamics and high-frequency details, often degrading reconstruction fidelity and local controllability. To address this limitation, we introduce pose-guided residual refinement for motion (PGR$^2$M), a hybrid representation that augments interpretable pose codes with residual codes learned via residual vector quantization (RVQ). A pose-guided RVQ tokenizer decomposes motion into pose latents that encode coarse global structure and residual latents that model fine-grained temporal variations. Residual dropout further discourages over-reliance on residuals, preserving the semantic alignment and editability of the pose codes. On top of this tokenizer, a base Transformer autoregressively predicts pose codes from text, and a refine Transformer predicts residual codes conditioned on text, pose codes, and quantization stage. Experiments on HumanML3D and KIT-ML show that PGR$^2$M improves Fréchet inception distance and reconstruction metrics for both generation and editing compared with CoMo and recent diffusion- and tokenization-based baselines, while user studies confirm that it enables intuitive, structure-preserving motion edits.

</details>


### [44] [SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems](https://arxiv.org/abs/2512.22439)
*Khalfalla Awedat,Mohamed Abidalrekab,Gurcan Comert,Mustafa Ayad*

Main category: cs.CV

TL;DR: SuperiorGAT是一种基于图注意力机制的框架，针对稀疏LiDAR点云中因环境遮挡导致的垂直激光束缺失问题，实现了精确重建，且无需增加传感器硬件。


<details>
  <summary>Details</summary>
Motivation: 传统基于LiDAR的感知受限于垂直激光束分辨率固定，且环境遮挡会造成部分激光束数据缺失，影响下游感知任务，亟需高效重建缺失数据信息的方法。

Method: 提出SuperiorGAT，将LiDAR扫描数据建模为beam-aware图结构，并结合门控残差融合与前馈精细化策略，使模型无需加深网络即可重建缺失的高程信息。在实验中通过人为模拟丢弃每4条扫描束，检验模型鲁棒性。

Result: 在KITTI不同场景下的实验表明，SuperiorGAT在重建误差和几何一致性上均优于PointNet及更深层的GAT基线模型，且能在X-Z投影中保持结构完整性与极低的垂直失真。

Conclusion: 该方法通过结构优化，在不增加传感器硬件的前提下，有效提升了LiDAR分辨率与重建质量，具备高度计算效率和实际应用价值。

Abstract: LiDAR-based perception in autonomous systems is constrained by fixed vertical beam resolution and further compromised by beam dropout resulting from environmental occlusions. This paper introduces SuperiorGAT, a graph attention-based framework designed to reconstruct missing elevation information in sparse LiDAR point clouds. By modeling LiDAR scans as beam-aware graphs and incorporating gated residual fusion with feed-forward refinement, SuperiorGAT enables accurate reconstruction without increasing network depth. To evaluate performance, structured beam dropout is simulated by removing every fourth vertical scanning beam. Extensive experiments across diverse KITTI environments, including Person, Road, Campus, and City sequences, demonstrate that SuperiorGAT consistently achieves lower reconstruction error and improved geometric consistency compared to PointNet-based models and deeper GAT baselines. Qualitative X-Z projections further confirm the model's ability to preserve structural integrity with minimal vertical distortion. These results suggest that architectural refinement offers a computationally efficient method for improving LiDAR resolution without requiring additional sensor hardware.

</details>


### [45] [MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments](https://arxiv.org/abs/2512.22867)
*Zhuonan Liu,Xinyu Zhang,Zishuo Wang,Tomohito Kawabata,Xuesu Xiao,Ling Xiao*

Main category: cs.CV

TL;DR: 本文介绍了一个名为MUSON的新型多模态社会化导航数据集，通过结构化的链式思维标注和均衡的动作空间，提升了对复杂动态环境下导航决策的建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有社会化导航数据集缺乏明确的推理监督，且动作分布极为偏斜，导致模型难以学习关键的安全行为。为解决这些问题，作者希望构建更能反映实际约束与推理过程的数据集。

Method: MUSON采集自室内外多场景，采用五步链式思维标注，分别标注感知、预测、推理、动作及解释，并显式建模静态物理约束、设计合理均衡的离散动作空间。对比分析SNEI数据集，MUSON在标注一致性和解释性方面有显著改进。

Result: 在MUSON上评测多个前沿小型视觉语言模型，Qwen2.5-VL-3B模型获得最高决策准确率0.8625，反映了该数据集有效支撑社会化导航任务。

Conclusion: MUSON是一个结构化、可复用的社会化导航数据集，能推动模型对动态人群与物理环境的复杂推理能力提升，并对未来导航研究提供新基准。

Abstract: Socially compliant navigation requires structured reasoning over dynamic pedestrians and physical constraints to ensure safe and interpretable decisions. However, existing social navigation datasets often lack explicit reasoning supervision and exhibit highly long-tailed action distributions, limiting models' ability to learn safety-critical behaviors. To address these issues, we introduce MUSON, a multimodal dataset for short-horizon social navigation collected across diverse indoor and outdoor campus scenes. MUSON adopts a structured five-step Chain-of-Thought annotation consisting of perception, prediction, reasoning, action, and explanation, with explicit modeling of static physical constraints and a rationally balanced discrete action space. Compared to SNEI, MUSON provides consistent reasoning, action, and explanation. Benchmarking multiple state-of-the-art Small Vision Language Models on MUSON shows that Qwen2.5-VL-3B achieves the highest decision accuracy of 0.8625, demonstrating that MUSON serves as an effective and reusable benchmark for socially compliant navigation. The dataset is publicly available at https://huggingface.co/datasets/MARSLab/MUSON

</details>


### [46] [LECalib: Line-Based Event Camera Calibration](https://arxiv.org/abs/2512.22441)
*Zibin Liu,Banglei Guana,Yang Shanga,Zhenbao Yu,Yifei Bian,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于几何线条的事件相机自标定方法，无需复杂的手工标定物体，且适用于变化场景，提高了标定效率和适用性。


<details>
  <summary>Details</summary>
Motivation: 传统事件相机标定方法通常需要专门设计的标定物体和耗时的过程，难以适应场景快速变化。为解决这一问题，作者希望开发一种无需特殊物体、简单高效的标定方法。

Method: 方法通过从事件流中直接检测常见人造环境中的几何线条（如门窗、盒子等），然后基于检测到的线条建立事件线标定模型，生成相机参数初值，最后通过非线性优化进一步精确相机参数。该方法可同时处理平面与非平面线条，适用于单目和双目事件相机。

Result: 模拟实验及真实环境实验结果均表明，该方法具备可行性和较高的准确性。方法已在单目和双目事件相机上获得验证，并公开了源代码。

Conclusion: 提出的方法无需人工布置复杂标定目标，利用常见物体的几何线特征即可完成高效准确的事件相机标定，更适合动态或临时性的实际应用场景，具有广阔的应用前景。

Abstract: Camera calibration is an essential prerequisite for event-based vision applications. Current event camera calibration methods typically involve using flashing patterns, reconstructing intensity images, and utilizing the features extracted from events. Existing methods are generally time-consuming and require manually placed calibration objects, which cannot meet the needs of rapidly changing scenarios. In this paper, we propose a line-based event camera calibration framework exploiting the geometric lines of commonly-encountered objects in man-made environments, e.g., doors, windows, boxes, etc. Different from previous methods, our method detects lines directly from event streams and leverages an event-line calibration model to generate the initial guess of camera parameters, which is suitable for both planar and non-planar lines. Then, a non-linear optimization is adopted to refine camera parameters. Both simulation and real-world experiments have demonstrated the feasibility and accuracy of our method, with validation performed on monocular and stereo event cameras. The source code is released at https://github.com/Zibin6/line_based_event_camera_calib.

</details>


### [47] [Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework](https://arxiv.org/abs/2512.22447)
*Zhicheng Zhao,Yuancheng Xu,Andong Lu,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 提出QDFNet模型，通过动态质量感知融合提升光学与SAR图像目标检测，尤其在模态缺失情况下表现突出。


<details>
  <summary>Details</summary>
Motivation: 光学与SAR图像在遥感监测中互补，但因为成像机制差异、时间不同步和配准难题，实际中很难获得高质量配对图像，常常遇到数据缺失或退化问题，影响目标检测的鲁棒性与准确性。

Method: 设计了质量感知动态融合网络（QDFNet），包括动态模态质量评估模块（DMQA）和正交约束归一化融合模块（OCNF）。DMQA利用可学习参考token递进式评估特征可靠性，准确识别退化区域。OCNF通过正交约束保持模态独立性，根据可靠性分数动态调整融合权重，抑制低质量特征传播。

Result: 在SpaceNet6-OTD和OGSOD-2.0数据集上，QDFNet明显优于现有方法，尤其在模态部分损坏或缺失时具有更强鲁棒性和检测效果。

Conclusion: QDFNet可以有效解决遥感多模态目标检测中因模态缺失或腐败带来的问题，提升融合检测模型的实际应用价值和可靠性。

Abstract: Optical and Synthetic Aperture Radar (SAR) fusion-based object detection has attracted significant research interest in remote sensing, as these modalities provide complementary information for all-weather monitoring. However, practical deployment is severely limited by inherent challenges. Due to distinct imaging mechanisms, temporal asynchrony, and registration difficulties, obtaining well-aligned optical-SAR image pairs remains extremely difficult, frequently resulting in missing or degraded modality data. Although recent approaches have attempted to address this issue, they still suffer from limited robustness to random missing modalities and lack effective mechanisms to ensure consistent performance improvement in fusion-based detection. To address these limitations, we propose a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection. Our proposed method leverages learnable reference tokens to dynamically assess feature reliability and guide adaptive fusion in the presence of missing modalities. In particular, we design a Dynamic Modality Quality Assessment (DMQA) module that employs learnable reference tokens to iteratively refine feature reliability assessment, enabling precise identification of degraded regions and providing quality guidance for subsequent fusion. Moreover, we develop an Orthogonal Constraint Normalization Fusion (OCNF) module that employs orthogonal constraints to preserve modality independence while dynamically adjusting fusion weights based on reliability scores, effectively suppressing unreliable feature propagation. Extensive experiments on the SpaceNet6-OTD and OGSOD-2.0 datasets demonstrate the superiority and effectiveness of QDFNet compared to state-of-the-art methods, particularly under partial modality corruption or missing data scenarios.

</details>


### [48] [SonoVision: A Computer Vision Approach for Helping Visually Challenged Individuals Locate Objects with the Help of Sound Cues](https://arxiv.org/abs/2512.22449)
*Md Abu Obaida Zishan,Annajiat Alim Rasel*

Main category: cs.CV

TL;DR: 本文提出了一款名为SonoVision的智能手机应用，利用声音提示帮助视障人士通过耳机定位日常物品的位置。


<details>
  <summary>Details</summary>
Motivation: 视障者难以独立精准地定位周围物体，容易因无法寻物而造成依赖他人、甚至陷入危险，亟需技术辅助提升其自主生活能力。

Method: 应用通过Flutter开发，后台采用Efficientdet-D2模型实现物体检测，根据识别结果，通过耳机左右声道发出对应位置的正弦音，辅助用户辨别物品方向，前方则两边同时发声。该系统完全离线运行，不依赖网络。

Result: 应用可有效通过声音提示帮助用户快速定位物品，提升视障人士的独立生活能力，降低其日常对他人的依赖，增强使用安全性。

Conclusion: SonoVision为视障人士提供了一种安全、易用且离线运行的物体定位新方案，可显著提升其日常生活的自主性和安全性。

Abstract: Locating objects for the visually impaired is a significant challenge and is something no one can get used to over time. However, this hinders their independence and could push them towards risky and dangerous scenarios. Hence, in the spirit of making the visually challenged more self-sufficient, we present SonoVision, a smart-phone application that helps them find everyday objects using sound cues through earphones/headphones. This simply means, if an object is on the right or left side of a user, the app makes a sinusoidal sound in a user's respective ear through ear/headphones. However, to indicate objects located directly in front, both the left and right earphones are rung simultaneously. These sound cues could easily help a visually impaired individual locate objects with the help of their smartphones and reduce the reliance on people in their surroundings, consequently making them more independent. This application is made with the flutter development platform and uses the Efficientdet-D2 model for object detection in the backend. We believe the app will significantly assist the visually impaired in a safe and user-friendly manner with its capacity to work completely offline. Our application can be accessed here https://github.com/MohammedZ666/SonoVision.git.

</details>


### [49] [SAM 3D for 3D Object Reconstruction from Remote Sensing Images](https://arxiv.org/abs/2512.22452)
*Junsheng Yao,Lichao Mou,Qingyu Li*

Main category: cs.CV

TL;DR: 本文系统性评估了通用型图像到3D的基础模型SAM 3D在单目遥感建筑重建中的表现，并与现有方法TRELLIS进行了对比。结果表明SAM 3D在屋顶几何和边界清晰度方面更优。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D建筑重建方法多依赖特定任务的网络结构和大量标注数据，影响了城市三维建模的可扩展性。论文旨在探索通用基础模型SAM 3D在遥感单目建筑重建领域的可行性和效果。

Method: 采用SAM 3D和TRELLIS两种方法，在NYC Urban Dataset上进行对比实验。评估指标包括Frechet Inception Distance (FID)和基于CLIP的最大均值差异（CMMD）。此外，提出了基于分割、重建、组合的pipeline实现城市场景三维重建。

Result: 实验结果显示，SAM 3D相比TRELLIS能重建出更一致的屋顶结构和更清晰的建筑边界，并在城市场景三维建模中表现出潜力。

Conclusion: SAM 3D作为通用基础模型，在单目3D建筑重建中效果优异，具有实际部署的可行性。不过，应用中仍有局限，未来需结合场景结构先验进一步提升重建质量。

Abstract: Monocular 3D building reconstruction from remote sensing imagery is essential for scalable urban modeling, yet existing methods often require task-specific architectures and intensive supervision. This paper presents the first systematic evaluation of SAM 3D, a general-purpose image-to-3D foundation model, for monocular remote sensing building reconstruction. We benchmark SAM 3D against TRELLIS on samples from the NYC Urban Dataset, employing Frechet Inception Distance (FID) and CLIP-based Maximum Mean Discrepancy (CMMD) as evaluation metrics. Experimental results demonstrate that SAM 3D produces more coherent roof geometry and sharper boundaries compared to TRELLIS. We further extend SAM 3D to urban scene reconstruction through a segment-reconstruct-compose pipeline, demonstrating its potential for urban scene modeling. We also analyze practical limitations and discuss future research directions. These findings provide practical guidance for deploying foundation models in urban 3D reconstruction and motivate future integration of scene-level structural priors.

</details>


### [50] [Comparing Object Detection Models for Electrical Substation Component Mapping](https://arxiv.org/abs/2512.22454)
*Haley Mody,Namish Bansal,Dennies Kiprono Bor,Edward J. Oughton*

Main category: cs.CV

TL;DR: 本论文利用三种计算机视觉模型（YOLOv8、YOLOv11、RF-DETR）对美国变电站组件进行自动化识别和映射，提升了变电站关键部件识别的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 变电站是电网的重要组成部分，其关键资产易受自然灾害等多类危害影响。鉴于电网属于关键基础设施，任何失效都可能带来严重的经济与公共安全后果，亟需高效识别和量化变电站脆弱性。传统人工标注方法耗时费力，因此急需自动化、高效的解决方案。

Method: 作者手动标注了美国变电站的图像数据集，并分别使用YOLOv8、YOLOv11和RF-DETR三种计算机视觉模型进行训练和测试，从检测准确率、精确度和效率等方面系统对比各模型表现。

Result: 三种模型在变电站组件检测的准确率、精度和效率方面各有优势与不足。论文详细分析了每种模型的强项与局限，并评估出最适合大规模变电站组件映射的模型。

Conclusion: 采用先进的计算机视觉技术能有效提升变电站部件的自动识别与映射效率，为电力基础设施的风险量化和维护提供重要支撑，展示了机器学习在电网关键组件映射中的实用价值。

Abstract: Electrical substations are a significant component of an electrical grid. Indeed, the assets at these substations (e.g., transformers) are prone to disruption from many hazards, including hurricanes, flooding, earthquakes, and geomagnetically induced currents (GICs). As electrical grids are considered critical national infrastructure, any failure can have significant economic and public safety implications. To help prevent and mitigate these failures, it is thus essential that we identify key substation components to quantify vulnerability. Unfortunately, traditional manual mapping of substation infrastructure is time-consuming and labor-intensive. Therefore, an autonomous solution utilizing computer vision models is preferable, as it allows for greater convenience and efficiency. In this research paper, we train and compare the outputs of 3 models (YOLOv8, YOLOv11, RF-DETR) on a manually labeled dataset of US substation images. Each model is evaluated for detection accuracy, precision, and efficiency. We present the key strengths and limitations of each model, identifying which provides reliable and large-scale substation component mapping. Additionally, we utilize these models to effectively map the various substation components in the United States, showcasing a use case for machine learning in substation mapping.

</details>


### [51] [Event-based high temporal resolution measurement of shock wave motion field](https://arxiv.org/abs/2512.22474)
*Taihang Lei,Banglei Guan,Minzu Liang,Pengju Sun,Jing Tao,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种利用多台事件相机对冲击波运动参数进行高精度、高时空分辨率测量的新方法，在极端测试条件和不均匀传播情况下也取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法在面对高速、不均匀传播的冲击波以及不稳定测试环境时，难以实现高分辨率、准确的测量。本研究旨在克服这些挑战，提供更加可靠和精确的冲击波参数测量工具。

Method: 1. 采用多台事件相机捕捉冲击波，以摆脱传统成像局限；2. 在极坐标系下编码事件，基于事件偏移自适应选取感兴趣区域；3. 通过斜率迭代分析提取冲击波前沿事件；4. 构建基于事件的光学成像模型，推导冲击波参数及三维重建。

Result: 提出的方法能有效进行多角度冲击波测量及运动场重建，同时能反演爆炸当量。与压力传感器和经验公式对比，最大误差为5.20%、最小误差为0.06%。

Conclusion: 该方法能实现冲击波运动场的高精度、高时空分辨率测量，具有广阔应用前景，是领域内的重要进展。

Abstract: Accurate measurement of shock wave motion parameters with high spatiotemporal resolution is essential for applications such as power field testing and damage assessment. However, significant challenges are posed by the fast, uneven propagation of shock waves and unstable testing conditions. To address these challenges, a novel framework is proposed that utilizes multiple event cameras to estimate the asymmetry of shock waves, leveraging its high-speed and high-dynamic range capabilities. Initially, a polar coordinate system is established, which encodes events to reveal shock wave propagation patterns, with adaptive region-of-interest (ROI) extraction through event offset calculations. Subsequently, shock wave front events are extracted using iterative slope analysis, exploiting the continuity of velocity changes. Finally, the geometric model of events and shock wave motion parameters is derived according to event-based optical imaging model, along with the 3D reconstruction model. Through the above process, multi-angle shock wave measurement, motion field reconstruction, and explosive equivalence inversion are achieved. The results of the speed measurement are compared with those of the pressure sensors and the empirical formula, revealing a maximum error of 5.20% and a minimum error of 0.06%. The experimental results demonstrate that our method achieves high-precision measurement of the shock wave motion field with both high spatial and temporal resolution, representing significant progress.

</details>


### [52] [Scalpel-SAM: A Semi-Supervised Paradigm for Adapting SAM to Infrared Small Object Detection](https://arxiv.org/abs/2512.22483)
*Zihan Liu,Xiangning Ren,Dezhang Kong,Yipeng Zhang,Meng Han*

Main category: cs.CV

TL;DR: 本文提出了一种用于红外小目标检测的半监督新范式，借助自研的MoE适配器，实现了最低10%标注数据下和全监督相当甚至更好的性能。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测领域缺乏足够的标注数据，而现有的方法（如SAM）受限于领域差异、物理先验编码能力不足、模型结构复杂，导致难以直接应用于该任务。

Method: 设计了分层MoE适配器（包含四个白盒神经算子），提出两阶段范式：（1）先用MoE适配器结合10%标注数据对SAM进行知识蒸馏，获得专家教师模型Scalpel-SAM；（2）再由Scalpel-SAM生成伪标签，训练高效轻量的下游模型。

Result: 在极少标注数据条件下，所提方案下的下游模型性能可与全监督甚至超越全监督方法，验证了方法的有效性。

Conclusion: 首次系统性地利用SAM作为教师模型提出适用于红外小目标检测的半监督范式，显著缓解了标注稀缺问题，同时提升模型效率和实用性。

Abstract: Infrared small object detection urgently requires semi-supervised paradigms due to the high cost of annotation. However, existing methods like SAM face significant challenges of domain gaps, inability of encoding physical priors, and inherent architectural complexity. To address this, we designed a Hierarchical MoE Adapter consisting of four white-box neural operators. Building upon this core component, we propose a two-stage paradigm for knowledge distillation and transfer: (1) Prior-Guided Knowledge Distillation, where we use our MoE adapter and 10% of available fully supervised data to distill SAM into an expert teacher (Scalpel-SAM); and (2) Deployment-Oriented Knowledge Transfer, where we use Scalpel-SAM to generate pseudo labels for training lightweight and efficient downstream models. Experiments demonstrate that with minimal annotations, our paradigm enables downstream models to achieve performance comparable to, or even surpassing, their fully supervised counterparts. To our knowledge, this is the first semi-supervised paradigm that systematically addresses the data scarcity issue in IR-SOT using SAM as the teacher model.

</details>


### [53] [Tracking by Predicting 3-D Gaussians Over Time](https://arxiv.org/abs/2512.22489)
*Tanish Baranwal,Himanshu Gaurav Singh,Jathushan Rajasegaran,Jitendra Malik*

Main category: cs.CV

TL;DR: 提出了一种新的自监督视频表征学习方法Video-GMAE，通过将视频编码为时序高斯团，实现了无需标注的特征学习，能直接达到无监督跟踪目的，效果优越。


<details>
  <summary>Details</summary>
Motivation: 当前视频自监督学习方法对捕捉视频动态和场景三维结构感知有限，希望通过更具结构先验的方法提升表现。

Method: Video-GMAE采用“高斯蒙版自编码器”架构，将视频帧编码为随时间运动的高斯分布集合，将视频表征为可解释的高斯团轨迹。模型预训练过程中，不依赖人工标签，通过自编码重建任务捕获物体动态。最终在网络输出层以高斯重心直接输出目标轨迹实现零样本跟踪。

Result: 在Kinetics和Kubric等主流数据集上，相比已有自监督视频方法，Video-GMAE经过小规模微调后，在Kinetics上提升34.6%，在Kubric上提升13.1%。在零样本跟踪任务上，直接使用高斯团预测轨迹，性能与SOTA接近。

Conclusion: 将视频编码为时序高斯分布有助于引入三维动态场景归纳偏置，无需人工标签即可获得强大的视频表征，直接支持高质量的目标跟踪，比现有自监督方法表现更优。

Abstract: We propose Video Gaussian Masked Autoencoders (Video-GMAE), a self-supervised approach for representation learning that encodes a sequence of images into a set of Gaussian splats moving over time. Representing a video as a set of Gaussians enforces a reasonable inductive bias: that 2-D videos are often consistent projections of a dynamic 3-D scene. We find that tracking emerges when pretraining a network with this architecture. Mapping the trajectory of the learnt Gaussians onto the image plane gives zero-shot tracking performance comparable to state-of-the-art. With small-scale finetuning, our models achieve 34.6% improvement on Kinetics, and 13.1% on Kubric datasets, surpassing existing self-supervised video approaches. The project page and code are publicly available at https://videogmae.org/ and https://github.com/tekotan/video-gmae.

</details>


### [54] [SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration](https://arxiv.org/abs/2512.22503)
*Xin Chen,Kang Luo,Yangyi Xiao,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种专为月球机器人物体探测任务设计的多模态三维检测模型SCAFusion，在提升小型、不规则目标检测精度的同时，计算和参数开销极小。


<details>
  <summary>Details</summary>
Motivation: 目前主流针对地球自动驾驶场景的多模态三维检测方法，在月球等特殊环境下存在特征对齐差、多模态协同不足及小目标检测弱等问题，无法满足月表机器人自主导航与操作对小型、不规则目标高精度检测的需求。

Method: SCAFusion基于BEVFusion框架，创新性地引入了：1）Cognitive Adapter实现高效摄像头主干调整，2）对比对齐模块增强摄像头与LiDAR特征一致性，3）相机辅助训练分支加强视觉表征，4）Section-aware Coordinate Attention机制显著提升小型/不规则目标的检测能力。

Result: 在nuScenes验证集上，SCAFusion达到69.7% mAP和72.1% NDS，分别比基线提升5.0%和2.7%。在基于Isaac Sim的月球模拟环境中，模型mAP达90.93%，比基线高11.5%，在小型陨石障碍检测上具备显著优势。

Conclusion: SCAFusion能高效结合多源感知信息，在几乎不增加计算和参数资源的前提下，有效提升机器人在月表环境中的小型不规则物体检测能力，适合应用于下一代月球机器人任务。

Abstract: Reliable and precise detection of small and irregular objects, such as meteor fragments and rocks, is critical for autonomous navigation and operation in lunar surface exploration. Existing multimodal 3D perception methods designed for terrestrial autonomous driving often underperform in off world environments due to poor feature alignment, limited multimodal synergy, and weak small object detection. This paper presents SCAFusion, a multimodal 3D object detection model tailored for lunar robotic missions. Built upon the BEVFusion framework, SCAFusion integrates a Cognitive Adapter for efficient camera backbone tuning, a Contrastive Alignment Module to enhance camera LiDAR feature consistency, a Camera Auxiliary Training Branch to strengthen visual representation, and most importantly, a Section aware Coordinate Attention mechanism explicitly designed to boost the detection performance of small, irregular targets. With negligible increase in parameters and computation, our model achieves 69.7% mAP and 72.1% NDS on the nuScenes validation set, improving the baseline by 5.0% and 2.7%, respectively. In simulated lunar environments built on Isaac Sim, SCAFusion achieves 90.93% mAP, outperforming the baseline by 11.5%, with notable gains in detecting small meteor like obstacles.

</details>


### [55] [DreamOmni3: Scribble-based Editing and Generation](https://arxiv.org/abs/2512.22525)
*Bin Xia,Bohao Peng,Jiyang Liu,Sitong Wu,Jingyao Li,Junjia Huang,Xu Zhao,Yitong Wang,Ruihang Chu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 本文提出了一种结合文本、图片和手写涂鸦的新型图形编辑与生成框架DreamOmni3，显著提升了多模态交互下的图像可控编辑和生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有统一生成与编辑模型虽然借助文本提示取得了很大成果，但在细粒度视觉控制、特定编辑位置等需求上，单靠语言描述力有不足，用户期望更加自然和精细的交互方式。

Method: 作者提出了基于涂鸦(scribble)的编辑与生成任务，通过合成数据来解决数据稀缺问题。首先从DreamOmni2中提取可编辑区域，叠加手绘框、圆、涂鸦或裁剪图像，生成专为编辑与生成设计的新型训练集。在模型结构上，采用“联合输入方案”，把原图和带涂鸦的图像作为输入，并用不同颜色区分目标区域，优化了复杂多区域编辑时的区域定位和处理效率。

Result: 提出的DreamOmni3模型在涂鸦辅助的多模态编辑和生成的多项基准测试中表现突出，显著优于传统的仅用binary mask和文本驱动方法。

Conclusion: DreamOmni3有效提升了用户对细粒度、多模态图像编辑的可控性，为普及基于GUI的交互创作提供了有力工具。相关模型与代码将开源，推动该领域进一步研究。

Abstract: Recently unified generation and editing models have achieved remarkable success with their impressive performance. These models rely mainly on text prompts for instruction-based editing and generation, but language often fails to capture users intended edit locations and fine-grained visual details. To this end, we propose two tasks: scribble-based editing and generation, that enables more flexible creation on graphical user interface (GUI) combining user textual, images, and freehand sketches. We introduce DreamOmni3, tackling two challenges: data creation and framework design. Our data synthesis pipeline includes two parts: scribble-based editing and generation. For scribble-based editing, we define four tasks: scribble and instruction-based editing, scribble and multimodal instruction-based editing, image fusion, and doodle editing. Based on DreamOmni2 dataset, we extract editable regions and overlay hand-drawn boxes, circles, doodles or cropped image to construct training data. For scribble-based generation, we define three tasks: scribble and instruction-based generation, scribble and multimodal instruction-based generation, and doodle generation, following similar data creation pipelines. For the framework, instead of using binary masks, which struggle with complex edits involving multiple scribbles, images, and instructions, we propose a joint input scheme that feeds both the original and scribbled source images into the model, using different colors to distinguish regions and simplify processing. By applying the same index and position encodings to both images, the model can precisely localize scribbled regions while maintaining accurate editing. Finally, we establish comprehensive benchmarks for these tasks to promote further research. Experimental results demonstrate that DreamOmni3 achieves outstanding performance, and models and code will be publicly released.

</details>


### [56] [CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation](https://arxiv.org/abs/2512.22536)
*Qinglin Zeng,Kaitong Cai,Ruiqi Chen,Qinhan Lv,Keze Wang*

Main category: cs.CV

TL;DR: 本文提出CoAgent框架，通过分步骤计划、合成和验证，解决文本生成视频中的叙事连贯性和视觉一致性问题。实验表明该方法极大提升了长视频的连贯性和质量。


<details>
  <summary>Details</summary>
Motivation: 目前开域文本到视频生成模型生成过程中，常常单独处理每段镜头，导致人物身份漂移、场景不一致和时序结构不稳定等问题，难以维持视频的叙事和视觉一致性。为此，作者希望提出一种结构化的生成流程，有效解决上述难题。

Method: CoAgent框架包括五个主要模块：（1）StoryBoard Planner将用户输入（文本、风格、节奏）分解为结构化的镜头计划，明确列出角色、空间关系和时间线索；（2）Global Context Manager建立实体级别记忆，实现身份和外观跨镜头一致性；（3）Synthesis Module根据计划指导生成视频片段；（4）Verifier Agent结合视觉语言推理，对生成结果进行校验并选择性地重新生成有问题的镜头；（5）Pacing-aware Editor调整节奏和镜头转换，保证叙事节奏流畅。

Result: 大量实验验证，CoAgent框架在长视频生成任务中显著提升了故事连贯性、视觉一致性和整体视频表现，优于现有主流方法。

Conclusion: CoAgent证明了结构化的生成流程和验证机制能有效解决文本到长视频生成中的一致性与叙事性难题，为长视频自动生成提供了有价值的新思路。

Abstract: Maintaining narrative coherence and visual consistency remains a central challenge in open-domain video generation. Existing text-to-video models often treat each shot independently, resulting in identity drift, scene inconsistency, and unstable temporal structure. We propose CoAgent, a collaborative and closed-loop framework for coherent video generation that formulates the process as a plan-synthesize-verify pipeline. Given a user prompt, style reference, and pacing constraints, a Storyboard Planner decomposes the input into structured shot-level plans with explicit entities, spatial relations, and temporal cues. A Global Context Manager maintains entity-level memory to preserve appearance and identity consistency across shots. Each shot is then generated by a Synthesis Module under the guidance of a Visual Consistency Controller, while a Verifier Agent evaluates intermediate results using vision-language reasoning and triggers selective regeneration when inconsistencies are detected. Finally, a pacing-aware editor refines temporal rhythm and transitions to match the desired narrative flow. Extensive experiments demonstrate that CoAgent significantly improves coherence, visual consistency, and narrative quality in long-form video generation.

</details>


### [57] [Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains](https://arxiv.org/abs/2512.22545)
*Jesen Zhang,Ningyuan Liu,Kaitong Cai,Sidi Liu,Jing Yang,Ziliang Chen,Xiaofei Sun,Keze Wang*

Main category: cs.CV

TL;DR: 本文提出了一种无需标注的新方法SR-MCR，通过自我参考信号评价推理过程，提高多模态大模型的答案准确性和推理一致性，在视觉基准测试中取得领先。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型虽然答案流畅但推理过程不可靠，缺乏中间推理环节的监督和视觉关联，导致推理连贯性差。现有方法多只关注最终答案，忽视推理过程的可靠性，亟需过程级校准方案。

Method: SR-MCR方法不依赖额外手工标注，通过模型自身输出生成五类自我参考信号（语义一致性、词汇忠实性、非冗余性、视觉关联性、步骤一致性），组合为规范化可靠性加权奖励，实现对推理过程的细致监督。训练中采用无判别器的GRPO目标，并引入置信度调节冷却机制，抑制模型过于自信或平庸的生成。

Result: SR-MCR在Qwen2.5-VL上测试，明显提升了视觉任务的答案准确率和推理一致性，在同规模开源模型中取得了81.4%的平均SOTA准确率。消融实验验证了各奖励项和冷却机制的独立有效性。

Conclusion: SR-MCR无需标注、轻量高效且有效提升多模态大模型的推理可靠性和连贯性，为视觉推理任务提供了一种实用的新范式。

Abstract: Multimodal LLMs often produce fluent yet unreliable reasoning, exhibiting weak step-to-step coherence and insufficient visual grounding, largely because existing alignment approaches supervise only the final answer while ignoring the reliability of the intermediate reasoning process. We introduce SR-MCR, a lightweight and label-free framework that aligns reasoning by exploiting intrinsic process signals derived directly from model outputs. Five self-referential cues -- semantic alignment, lexical fidelity, non-redundancy, visual grounding, and step consistency -- are integrated into a normalized, reliability-weighted reward that provides fine-grained process-level guidance. A critic-free GRPO objective, enhanced with a confidence-aware cooling mechanism, further stabilizes training and suppresses trivial or overly confident generations. Built on Qwen2.5-VL, SR-MCR improves both answer accuracy and reasoning coherence across a broad set of visual benchmarks; among open-source models of comparable size, SR-MCR-7B achieves state-of-the-art performance with an average accuracy of 81.4%. Ablation studies confirm the independent contributions of each reward term and the cooling module.

</details>


### [58] [ReFRM3D: A Radiomics-enhanced Fused Residual Multiparametric 3D Network with Multi-Scale Feature Fusion for Glioma Characterization](https://arxiv.org/abs/2512.22570)
*Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Arefin Ittesafun Abian,Yan Zhang,Mirjam Jonkman,Sami Azam*

Main category: cs.CV

TL;DR: 本文提出了一种基于多参数MRI图像的新型脑胶质瘤分割与分类方法，通过融合残差3D网络和影像组学特征，显著提升了肿瘤分割与分类的精度。


<details>
  <summary>Details</summary>
Motivation: 胶质瘤是一类侵袭性极强的癌症，其高死亡率和复杂的诊断流程成为医学难题。现有方法受限于影像变异大、计算资源利用不充分、分割与分类效率低等问题，难以满足临床需求。

Method: 作者提出了一种创新的影像组学增强融合残差多参数3D网络（ReFRM3D），基于3D U-Net架构，集成多尺度特征融合、混合上采样及扩展残差跳跃机制。同时，结合影像组学特征，设计实现了基于多特征肿瘤标志物的分类器。方法在BraTS2019、BraTS2020、BraTS2021等公开数据集上进行了验证。

Result: 新方法在公开数据集上取得了显著的分割性能提升，如在BraTS2019的整体肿瘤、增强肿瘤区和肿瘤核心的Dice系数分别达到94.04%、92.68%、93.64%。BraTS2020和2021也取得了接近或更高的指标。

Conclusion: 所提方法能有效提升胶质瘤MRI影像分割和分类的准确率与效率，为临床诊断和治疗提供了有力的技术支持。

Abstract: Gliomas are among the most aggressive cancers, characterized by high mortality rates and complex diagnostic processes. Existing studies on glioma diagnosis and classification often describe issues such as high variability in imaging data, inadequate optimization of computational resources, and inefficient segmentation and classification of gliomas. To address these challenges, we propose novel techniques utilizing multi-parametric MRI data to enhance tumor segmentation and classification efficiency. Our work introduces the first-ever radiomics-enhanced fused residual multiparametric 3D network (ReFRM3D) for brain tumor characterization, which is based on a 3D U-Net architecture and features multi-scale feature fusion, hybrid upsampling, and an extended residual skip mechanism. Additionally, we propose a multi-feature tumor marker-based classifier that leverages radiomic features extracted from the segmented regions. Experimental results demonstrate significant improvements in segmentation performance across the BraTS2019, BraTS2020, and BraTS2021 datasets, achieving high Dice Similarity Coefficients (DSC) of 94.04%, 92.68%, and 93.64% for whole tumor (WT), enhancing tumor (ET), and tumor core (TC) respectively in BraTS2019; 94.09%, 92.91%, and 93.84% in BraTS2020; and 93.70%, 90.36%, and 92.13% in BraTS2021.

</details>


### [59] [KV-Tracker: Real-Time Pose Tracking with Transformers](https://arxiv.org/abs/2512.22581)
*Marwan Taher,Ignacio Alzugaray,Kirill Mazur,Xin Kong,Andrew J. Davison*

Main category: cs.CV

TL;DR: 论文提出了一种将多视角3D几何网络应用于实时6自由度姿态跟踪和在线重建的方法，极大提升了推理速度，实现了实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有多视角3D几何网络虽然效果强大，但推理速度慢，难以应用于实时场景，需要提升速度以适应实时姿态跟踪和重建的需求。

Method: 作者提出通过快速选取和管理关键帧、利用全双向注意力机制（π^3），并缓存全局自注意力模块中的key-value对，将其作为唯一的场景表示用于在线跟踪。该缓存机制对网络模型无关，不需要重新训练，可推广应用于多种多视角网络。

Result: 在TUM RGB-D、7-Scenes、Arctic 和 OnePose等数据集上进行实验，系统在保持高帧率（最高可达约27FPS）的同时，取得了强有力的跟踪和重建性能，实现了最高15倍的推理加速。

Conclusion: KV-Tracker提供了一种高效、通用、无需重训练的多视角实时6DoF跟踪与重建方法，兼顾推理速度与精度，具有较高的应用价值。

Abstract: Multi-view 3D geometry networks offer a powerful prior but are prohibitively slow for real-time applications. We propose a novel way to adapt them for online use, enabling real-time 6-DoF pose tracking and online reconstruction of objects and scenes from monocular RGB videos. Our method rapidly selects and manages a set of images as keyframes to map a scene or object via $π^3$ with full bidirectional attention. We then cache the global self-attention block's key-value (KV) pairs and use them as the sole scene representation for online tracking. This allows for up to $15\times$ speedup during inference without the fear of drift or catastrophic forgetting. Our caching strategy is model-agnostic and can be applied to other off-the-shelf multi-view networks without retraining. We demonstrate KV-Tracker on both scene-level tracking and the more challenging task of on-the-fly object tracking and reconstruction without depth measurements or object priors. Experiments on the TUM RGB-D, 7-Scenes, Arctic and OnePose datasets show the strong performance of our system while maintaining high frame-rates up to ${\sim}27$ FPS.

</details>


### [60] [PTalker: Personalized Speech-Driven 3D Talking Head Animation via Style Disentanglement and Modality Alignment](https://arxiv.org/abs/2512.22602)
*Bin Wang,Yang Xu,Huan Zhao,Hao Zhang,Zixing Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种名为PTalker的个性化3D说话人动画生成框架，在保持高度同步的口型动作基础上，能够体现个体独特的说话风格。其核心创新在于风格解耦与三层次模态对齐机制，实现更真实的个性化三维说话头像动画。


<details>
  <summary>Details</summary>
Motivation: 现有的3D说话头系统虽然在口型同步方面取得了进展，但对于个体说话风格的表达支持较弱，导致生成效果缺乏个性和真实感。因此，提升说话风格还原能力是实现个性化高真实感动画的关键难题。

Method: 作者提出PTalker框架，通过风格与内容的解耦进行个性化表达，并引入了音频与三维网格的三层次对齐机制：包括利用图注意力网络实现空间对齐，跨注意力机制实现时间对齐，以及通过双向对比损失和KL散度实现特征对齐。此外，通过特定的解耦约束，将音频和运动序列编码到独立的风格和内容空间。

Result: 在多个公开数据集上的充分定性与定量实验表明，PTalker相比最新方法在风格表达和口型同步方面均有显著提升，能够准确生成真实、个性化的3D说话头像。

Conclusion: PTalker有效提升了3D说话头动画的个性化与真实感，实现了准确的风格还原和口型同步，在该领域达成了新的先进水平。

Abstract: Speech-driven 3D talking head generation aims to produce lifelike facial animations precisely synchronized with speech. While considerable progress has been made in achieving high lip-synchronization accuracy, existing methods largely overlook the intricate nuances of individual speaking styles, which limits personalization and realism. In this work, we present a novel framework for personalized 3D talking head animation, namely "PTalker". This framework preserves speaking style through style disentanglement from audio and facial motion sequences and enhances lip-synchronization accuracy through a three-level alignment mechanism between audio and mesh modalities. Specifically, to effectively disentangle style and content, we design disentanglement constraints that encode driven audio and motion sequences into distinct style and content spaces to enhance speaking style representation. To improve lip-synchronization accuracy, we adopt a modality alignment mechanism incorporating three aspects: spatial alignment using Graph Attention Networks to capture vertex connectivity in the 3D mesh structure, temporal alignment using cross-attention to capture and synchronize temporal dependencies, and feature alignment by top-k bidirectional contrastive losses and KL divergence constraints to ensure consistency between speech and mesh modalities. Extensive qualitative and quantitative experiments on public datasets demonstrate that PTalker effectively generates realistic, stylized 3D talking heads that accurately match identity-specific speaking styles, outperforming state-of-the-art methods. The source code and supplementary videos are available at: PTalker.

</details>


### [61] [Enhancing Noise Resilience in Face Clustering via Sparse Differential Transformer](https://arxiv.org/abs/2512.22612)
*Dafeng Zhang,Yongqi Song,Shizhuo Liu*

Main category: cs.CV

TL;DR: 本文提出一种基于稀疏差分Transformer（SDT）的面部聚类方法，通过优化Jaccard系数的邻域选择，提升了面部嵌入表示的聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多用Jaccard相似系数提升面部聚类精度，但引入过多无关节点使区分能力受限，影响最终聚类效果。

Method: 作者提出了一个预测驱动的Top-K Jaccard相似系数，通过精确选择邻域节点，提高相似度计算的可靠性。为解决Top-K预测难题，设计了基于Transformer的预测模型，但传统Transformer易引入噪声。为此，作者提出稀疏差分Transformer（SDT），提升抗噪声能力并优化聚类表现。

Result: 在MS-Celeb-1M等多个数据集上，所提方法取得了SOTA表现，优于现有主流方法，验证了方法的有效性和鲁棒性。

Conclusion: 稀疏差分Transformer结合优化的Top-K Jaccard系数，有效提升了人脸聚类任务中相似性度量的可靠性和聚类性能，对实际大规模人脸聚类场景具有应用价值。

Abstract: The method used to measure relationships between face embeddings plays a crucial role in determining the performance of face clustering. Existing methods employ the Jaccard similarity coefficient instead of the cosine distance to enhance the measurement accuracy. However, these methods introduce too many irrelevant nodes, producing Jaccard coefficients with limited discriminative power and adversely affecting clustering performance. To address this issue, we propose a prediction-driven Top-K Jaccard similarity coefficient that enhances the purity of neighboring nodes, thereby improving the reliability of similarity measurements. Nevertheless, accurately predicting the optimal number of neighbors (Top-K) remains challenging, leading to suboptimal clustering results. To overcome this limitation, we develop a Transformer-based prediction model that examines the relationships between the central node and its neighboring nodes near the Top-K to further enhance the reliability of similarity estimation. However, vanilla Transformer, when applied to predict relationships between nodes, often introduces noise due to their overemphasis on irrelevant feature relationships. To address these challenges, we propose a Sparse Differential Transformer (SDT), instead of the vanilla Transformer, to eliminate noise and enhance the model's anti-noise capabilities. Extensive experiments on multiple datasets, such as MS-Celeb-1M, demonstrate that our approach achieves state-of-the-art (SOTA) performance, outperforming existing methods and providing a more robust solution for face clustering.

</details>


### [62] [Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone](https://arxiv.org/abs/2512.22615)
*Jiacheng Ye,Shansan Gong,Jiahui Gao,Junming Fan,Shuang Wu,Wei Bi,Haoli Bai,Lifeng Shang,Lingpeng Kong*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的大规模视觉-语言模型（Dream-VL）及其扩展（Dream-VLA），在视觉规划和机器人动态控制等任务上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有自回归（AR）视觉-语言模型在复杂视觉规划和机器人控制等场景中效率有限，扩散模型具备并行生成等特性，可能更适合这些任务。

Method: 作者首先提出扩散型视觉-语言模型Dream-VL，并在多个基准上与现有AR型VLM进行对比。进一步地，基于Dream-VL，通过在开源机器人数据集上的连续预训练，构建了视觉-语言-行动（VLA）模型Dream-VLA。模型本身为扩散结构，具备天然的双向性和高效并行能力。

Result: Dream-VLA在LIBERO（97.2%平均成功率）、SimplerEnv-Bridge（71.4%）、SimplerEnv-Fractal（60.5%）等任务上均超过了现有领先模型π₀和GR00T-N1。同时，dVLM在多种下游任务和训练目标下也均超越了AR基线。

Conclusion: 基于扩散的大规模视觉-语言及行为模型（Dream-VL和Dream-VLA）不仅与AR型方法在开放数据上表现相当，更适合用于复杂视觉规划和机器人控制，并有更快的收敛和更强的下游任务表现。作者已开源模型以推动领域发展。

Abstract: While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as $π_0$ and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.

</details>


### [63] [Rethinking Memory Design in SAM-Based Visual Object Tracking](https://arxiv.org/abs/2512.22624)
*Mohamad Alansari,Muzammal Naseer,Hasan Al Marzouqi,Naoufel Werghi,Sajid Javed*

Main category: cs.CV

TL;DR: 本文系统性地分析了基于SAM模型的视觉目标追踪中记忆机制的设计，并提出了一个统一的混合记忆框架，在SAM2和SAM3模型下均提升了跟踪鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉目标追踪依赖记忆机制来增强鲁棒性。尽管近年针对SAM2的跟踪方法取得进展，但记忆设计多为方法特定，对SAM体系下的通用记忆策略和迁移到更强基础模型（如SAM3）的效果还缺乏系统理解。

Method: 作者首先对典型的SAM2跟踪器进行分析，发现这些方法主要在短期记忆帧选择上有差异，并在SAM3框架下复现不同的记忆机制，进行大规模基准测试。在经验分析基础上，提出了显式区分短期外观记忆和长期干扰消解记忆的统一混合记忆框架，使不同记忆策略能够模块化集成。

Result: 大规模实验显示，所提统一框架在长时遮挡、复杂运动、强干扰等情况下，能在SAM2和SAM3两类骨干模型中获得一致的鲁棒性提升。

Conclusion: 统一的混合记忆框架不仅增强了SAM系列模型的追踪性能，还为未来更强基础模型的记忆策略设计提供了清晰的参考和理论依据。

Abstract: \noindent Memory has become the central mechanism enabling robust visual object tracking in modern segmentation-based frameworks. Recent methods built upon Segment Anything Model 2 (SAM2) have demonstrated strong performance by refining how past observations are stored and reused. However, existing approaches address memory limitations in a method-specific manner, leaving the broader design principles of memory in SAM-based tracking poorly understood. Moreover, it remains unclear how these memory mechanisms transfer to stronger, next-generation foundation models such as Segment Anything Model 3 (SAM3). In this work, we present a systematic memory-centric study of SAM-based visual object tracking. We first analyze representative SAM2-based trackers and show that most methods primarily differ in how short-term memory frames are selected, while sharing a common object-centric representation. Building on this insight, we faithfully reimplement these memory mechanisms within the SAM3 framework and conduct large-scale evaluations across ten diverse benchmarks, enabling a controlled analysis of memory design independent of backbone strength. Guided by our empirical findings, we propose a unified hybrid memory framework that explicitly decomposes memory into short-term appearance memory and long-term distractor-resolving memory. This decomposition enables the integration of existing memory policies in a modular and principled manner. Extensive experiments demonstrate that the proposed framework consistently improves robustness under long-term occlusion, complex motion, and distractor-heavy scenarios on both SAM2 and SAM3 backbones. Code is available at: https://github.com/HamadYA/SAM3_Tracking_Zoo. \textbf{This is a preprint. Some results are being finalized and may be updated in a future revision.}

</details>


### [64] [Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion](https://arxiv.org/abs/2512.22626)
*Yuming Gu,Yizhi Wang,Yining Hong,Yipeng Gao,Hao Jiang,Angtian Wang,Bo Liu,Nathaniel S. Dennler,Zhengfei Kuang,Hao Li,Gordon Wetzstein,Chongyang Ma*

Main category: cs.CV

TL;DR: 本文提出了Envision框架，通过显式利用目标图像约束视频扩散生成过程，实现更高效和准确的视觉规划，帮助机器人实现目标达成。该方法较传统方法在对齐和保留物体特征方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉规划方法多为前向预测，只基于初始观察生成场景演化，缺乏显式的目标建模，容易导致规划轨迹偏移、目标达成度低。本文旨在通过加强目标建模和约束，提升机器人在操控任务中的目标一致性和轨迹物理合理性。

Method: 提出了包括两个阶段的Envision框架：首先利用Goal Imagery Model结合任务指令和场景交互定位相关区域，并生成具体目标图像；然后利用条件视频扩散模型（FL2V）在初始帧和生成的目标帧之间实现平滑插值，得到符合物理规律且目标一致的视频轨迹。

Result: 在物体操控和图像编辑基准上，Envision在目标对齐、空间一致性及物体保留等方面优于现有基线方法。产生的视觉规划结果可直接指导下游机器人任务。

Conclusion: Envision通过将目标图像作为约束，引导视频扩散，显著提升了视觉规划的效果，为具身机器人场景下的感知与操控带来更准确的指导。

Abstract: Embodied visual planning aims to enable manipulation tasks by imagining how a scene evolves toward a desired goal and using the imagined trajectories to guide actions. Video diffusion models, through their image-to-video generation capability, provide a promising foundation for such visual imagination. However, existing approaches are largely forward predictive, generating trajectories conditioned on the initial observation without explicit goal modeling, thus often leading to spatial drift and goal misalignment. To address these challenges, we propose Envision, a diffusion-based framework that performs visual planning for embodied agents. By explicitly constraining the generation with a goal image, our method enforces physical plausibility and goal consistency throughout the generated trajectory. Specifically, Envision operates in two stages. First, a Goal Imagery Model identifies task-relevant regions, performs region-aware cross attention between the scene and the instruction, and synthesizes a coherent goal image that captures the desired outcome. Then, an Env-Goal Video Model, built upon a first-and-last-frame-conditioned video diffusion model (FL2V), interpolates between the initial observation and the goal image, producing smooth and physically plausible video trajectories that connect the start and goal states. Experiments on object manipulation and image editing benchmarks demonstrate that Envision achieves superior goal alignment, spatial consistency, and object preservation compared to baselines. The resulting visual plans can directly support downstream robotic planning and control, providing reliable guidance for embodied agents.

</details>


### [65] [FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution](https://arxiv.org/abs/2512.22647)
*Yidi Liu,Zihao Fan,Jie Huang,Jie Xiao,Dong Li,Wenlong Zhang,Lei Bai,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出了一种用于图像超分辨率（ISR）的细粒度感知奖励模型（FinPercep-RM），结合了编解码器结构和感知退化图以实现更精细的质量评估，同时引入协同进化课程学习（CCL）以提升训练稳定性。实验结果在全局与局部感知质量上均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类反馈的强化学习（RLHF）在图像超分任务中大多依赖于输出单一分数的IQ评估模型，难以捕捉精细局部失真，导致优化目标与真实感知质量不一致，产生奖励欺骗问题。为此亟需更能反映局部质量、对感知扭曲敏感的奖励模型。

Method: 提出细粒度感知奖励模型（FinPercep-RM），以编解码器架构输出全局质量评分和空间感知退化图。新建FGR-30k数据集，覆盖真实ISR模型产生的复杂失真，用于奖励模型训练。同时设计协同进化课程学习（CCL），奖励模型与ISR模型同步逐步复杂化，由易到难，防止训练不稳定与奖励欺骗。

Result: FinPercep-RM模型结合CCL机制，在多个图像超分任务上显著优于传统模型，无论是全局质量还是局部感知真实度均有提升。实验表明方法有效抑制了奖励欺骗，提高了训练稳定性。

Conclusion: 细粒度感知奖励模型联合课程学习能显著优化ISR任务中的感知质量评估，提升生成模型性能，并能有效避免奖励欺骗与训练不稳定问题，对高质量图像生成具有重要意义。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has proven effective in image generation field guided by reward models to align human preferences. Motivated by this, adapting RLHF for Image Super-Resolution (ISR) tasks has shown promise in optimizing perceptual quality with Image Quality Assessment (IQA) model as reward models. However, the traditional IQA model usually output a single global score, which are exceptionally insensitive to local and fine-grained distortions. This insensitivity allows ISR models to produce perceptually undesirable artifacts that yield spurious high scores, misaligning optimization objectives with perceptual quality and results in reward hacking. To address this, we propose a Fine-grained Perceptual Reward Model (FinPercep-RM) based on an Encoder-Decoder architecture. While providing a global quality score, it also generates a Perceptual Degradation Map that spatially localizes and quantifies local defects. We specifically introduce the FGR-30k dataset to train this model, consisting of diverse and subtle distortions from real-world super-resolution models. Despite the success of the FinPercep-RM model, its complexity introduces significant challenges in generator policy learning, leading to training instability. To address this, we propose a Co-evolutionary Curriculum Learning (CCL) mechanism, where both the reward model and the ISR model undergo synchronized curricula. The reward model progressively increases in complexity, while the ISR model starts with a simpler global reward for rapid convergence, gradually transitioning to the more complex model outputs. This easy-to-hard strategy enables stable training while suppressing reward hacking. Experiments validates the effectiveness of our method across ISR models in both global quality and local realism on RLHF methods.

</details>


### [66] [Visual Autoregressive Modelling for Monocular Depth Estimation](https://arxiv.org/abs/2512.22653)
*Amir El-Ghoussani,André Kaup,Nassir Navab,Gustavo Carneiro,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉自回归先验（VAR）的单目深度估计算法，作为扩散模型方法的替代方案。该方法通过大规模文本到图像VAR模型的适应和分尺度无分类器引导的条件上采样机制，实现了高效且竞争力的深度估计，尤其在室内和室外数据集上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前主流的单目深度估计方法多采用扩散模型，但这类方法通常需要大量的训练样本和计算资源。为应对这一问题，作者探索了VAR先验在深度估计任务中的应用，旨在提高数据和模型的扩展性，并兼具精度和效率。

Method: 本方法将大规模文本到图像的VAR模型进行适应，针对深度任务引入按尺度递进的条件上采样结构，并利用无分类器引导机制，整个推理过程仅需10个固定自回归阶段。为微调仅使用74K合成样本。

Result: 在室内数据集的受限训练条件下，方法取得了最新的性能记录；在户外数据集上也展现了较强的效果。整体上较少训练样本和自回归推理的结构带来了良好的适应性和扩展性。

Conclusion: VAR先验为深度估计提供了一种几何感知的生成模型新思路，尤其在样本有限、任务多变的实际环境中具有优势。该方法不仅与扩散模型互补，还在模型扩展性、数据适应性方面表现突出，有望应用于更多3D视觉任务场景。

Abstract: We propose a monocular depth estimation method based on visual autoregressive (VAR) priors, offering an alternative to diffusion-based approaches. Our method adapts a large-scale text-to-image VAR model and introduces a scale-wise conditional upsampling mechanism with classifier-free guidance. Our approach performs inference in ten fixed autoregressive stages, requiring only 74K synthetic samples for fine-tuning, and achieves competitive results. We report state-of-the-art performance in indoor benchmarks under constrained training conditions, and strong performance when applied to outdoor datasets. This work establishes autoregressive priors as a complementary family of geometry-aware generative models for depth estimation, highlighting advantages in data scalability, and adaptability to 3D vision tasks. Code available at "https://github.com/AmirMaEl/VAR-Depth".

</details>


### [67] [Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos](https://arxiv.org/abs/2512.22657)
*Shravan Saranyan,Pramit Saha*

Main category: cs.CV

TL;DR: 本研究探索多种深度学习架构用于心脏超声视频中左心室射血分数（LVEF）的自动估算，并在大型数据集上评估其表现。


<details>
  <summary>Details</summary>
Motivation: 目前临床中LVEF主要依赖人工评估，过程繁琐且易受观察者差异影响，缺乏自动、高效和一致的方法。深度学习有望解决这些问题。

Method: 作者比较了3D Inception、two-stream和CNN-RNN等多种深度学习架构与融合策略，通过变更模型结构和超参数，系统评估其预测LVEF的表现。所有模型在EchoNet-Dynamic数据集（10,030例视频）上训练和测试。

Result: 修改后的3D Inception模型取得最佳性能，RMSE为6.79%；小型和简化模型展现更好泛化能力。模型对卷积核大小和归一化策略等超参数极为敏感。总体存在一定过拟合趋势。

Conclusion: 深度学习能实现接近专家水平的LVEF评估。研究提出的模型架构及其训练经验，可为医学及更广泛的视频分析任务提供参考。

Abstract: Left ventricular ejection fraction (LVEF) is a key indicator of cardiac function and plays a central role in the diagnosis and management of cardiovascular disease. Echocardiography, as a readily accessible and non-invasive imaging modality, is widely used in clinical practice to estimate LVEF. However, manual assessment of cardiac function from echocardiograms is time-consuming and subject to considerable inter-observer variability. Deep learning approaches offer a promising alternative, with the potential to achieve performance comparable to that of experienced human experts. In this study, we investigate the effectiveness of several deep learning architectures for LVEF estimation from echocardiography videos, including 3D Inception, two-stream, and CNN-RNN models. We systematically evaluate architectural modifications and fusion strategies to identify configurations that maximize prediction accuracy. Models were trained and evaluated on the EchoNet-Dynamic dataset, comprising 10,030 echocardiogram videos. Our results demonstrate that modified 3D Inception architectures achieve the best overall performance, with a root mean squared error (RMSE) of 6.79%. Across architectures, we observe a tendency toward overfitting, with smaller and simpler models generally exhibiting improved generalization. Model performance was also found to be highly sensitive to hyperparameter choices, particularly convolutional kernel sizes and normalization strategies. While this study focuses on echocardiography-based LVEF estimation, the insights gained regarding architectural design and training strategies may be applicable to a broader range of medical and non-medical video analysis tasks.

</details>


### [68] [Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains](https://arxiv.org/abs/2512.22664)
*Qiankun Li,Feng He,Huabao Chen,Xin Ning,Kun Wang,Zengfu Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的Cluster Attention Adapter (CLAdapter)，可以帮助大规模预训练视觉模型适应数据有限的下游专用科学任务，效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模数据集和预训练视觉模型取得了很大进展，但许多专用科学领域存在数据有限的问题，当前方法在这些领域的适应性和表现仍有较大挑战。作者旨在解决大模型转移到数据受限领域的困难。

Method: 提出CLAdapter，将注意力机制和聚类中心结合，通过分布相关性和变换矩阵对特征进行个性化增强。该适配器具备统一接口，能无缝适配多种架构（如CNN、Transformer），支持2D和3D场景。

Result: 在涵盖通用、多媒体、生物、医学、工业、农业、环境、地理、材料、OOD和3D分析等十个领域的数据集上，CLAdapter在多种数据受限科学任务中取得了最优性能。

Conclusion: CLAdapter有效释放了基础视觉模型在多种科学领域中的潜力，为模型适应数据有限任务提供了有效的迁移适配方案，在多场景下均达到了SOTA。

Abstract: In the big data era, the computer vision field benefits from large-scale datasets such as LAION-2B, LAION-400M, and ImageNet-21K, Kinetics, on which popular models like the ViT and ConvNeXt series have been pre-trained, acquiring substantial knowledge. However, numerous downstream tasks in specialized and data-limited scientific domains continue to pose significant challenges. In this paper, we propose a novel Cluster Attention Adapter (CLAdapter), which refines and adapts the rich representations learned from large-scale data to various data-limited downstream tasks. Specifically, CLAdapter introduces attention mechanisms and cluster centers to personalize the enhancement of transformed features through distribution correlation and transformation matrices. This enables models fine-tuned with CLAdapter to learn distinct representations tailored to different feature sets, facilitating the models' adaptation from rich pre-trained features to various downstream scenarios effectively. In addition, CLAdapter's unified interface design allows for seamless integration with multiple model architectures, including CNNs and Transformers, in both 2D and 3D contexts. Through extensive experiments on 10 datasets spanning domains such as generic, multimedia, biological, medical, industrial, agricultural, environmental, geographical, materials science, out-of-distribution (OOD), and 3D analysis, CLAdapter achieves state-of-the-art performance across diverse data-limited scientific domains, demonstrating its effectiveness in unleashing the potential of foundation vision models via adaptive transfer. Code is available at https://github.com/qklee-lz/CLAdapter.

</details>


### [69] [INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading](https://arxiv.org/abs/2512.22666)
*Mert Ikinci,Luna Toma,Karin U. Loeffler,Leticia Ussem,Daniela Süsskind,Julia M. Weller,Yousef Yeganeh,Martina C. Herwig-Carl,Shadi Albarqouni*

Main category: cs.CV

TL;DR: 该论文提出了一个多头深度学习框架INTERACT-CMIL，用于辅助结膜黑色素细胞上皮内病变（CMIL）的分级，并在多中心、专家标注数据集上显著优于传统CNN和基础模型。


<details>
  <summary>Details</summary>
Motivation: CMIL的精确分级对治疗及恶变风险预测至关重要，但因形态差异细微且诊断标准相互关联，手工分级存在困难。

Method: 作者设计了INTERACT-CMIL多头深度学习框架，可同时预测五种病理学标准：WHO4、WHO5、水平扩展、垂直扩展及细胞异型性。通过共享特征学习、组合式部分监督以及任务间一致性损失进行联合建模，并在来自三家医院的486份专家标注活检图像上进行训练和验证。

Result: INTERACT-CMIL在相关任务上对比传统CNN和基础大模型表现出显著提升（如WHO4任务F1提升55.1%，垂直扩展提升25.0%），预测结果具有较好可解释性并与专家分级高度一致。

Conclusion: INTERACT-CMIL为CMIL的诊断和分级提供了准确、易解释且可复现的计算基线，有助于推进眼科病理的标准化和数字化。

Abstract: Accurate grading of Conjunctival Melanocytic Intraepithelial Lesions (CMIL) is essential for treatment and melanoma prediction but remains difficult due to subtle morphological cues and interrelated diagnostic criteria. We introduce INTERACT-CMIL, a multi-head deep learning framework that jointly predicts five histopathological axes; WHO4, WHO5, horizontal spread, vertical spread, and cytologic atypia, through Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss enforcing cross-task consistency. Trained and evaluated on a newly curated, multi-center dataset of 486 expert-annotated conjunctival biopsy patches from three university hospitals, INTERACT-CMIL achieves consistent improvements over CNN and foundation-model (FM) baselines, with relative macro F1 gains up to 55.1% (WHO4) and 25.0% (vertical spread). The framework provides coherent, interpretable multi-criteria predictions aligned with expert grading, offering a reproducible computational benchmark for CMIL diagnosis and a step toward standardized digital ocular pathology.

</details>


### [70] [CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation](https://arxiv.org/abs/2512.22681)
*ZhenQi Chen,TsaiChing Ni,YuanFu Yang*

Main category: cs.CV

TL;DR: CritiFusion是一种提升文本到图像生成模型语义一致性和细节表现的新方法，通过多模态语义评论机制与频域融合，显著增强了生成结果的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管现有扩散模型在视觉识别度上表现出色，但面对复杂文本提示时，内容的语义对应性和细节展现仍存在不足。因此，急需一种方法提升生成图像与文本语义的吻合度和视觉细腻度。

Method: CritiFusion包括两个核心组件：CritiCore和SpecFusion。CritiCore采用多模态（视觉-语言以及多个大语言模型），对输入文本解析并提供高层语义反馈，动态引导扩散推理过程；SpecFusion则在频域内融合生成中间态，引入粗略结构信息，同时保留高频细节。无需额外训练，可直接作为插件用于现有模型。

Result: 在标准基准测试上，CritiFusion显著提升了文本到图像的一致性指标和视觉质量。无论是在人工偏好评价还是美学评分上，均达到甚至媲美当前最优奖励优化方法。定性案例显示其生成结果在细节、真实感和指令忠实度上具优势。

Conclusion: CritiFusion能够有效强化文本-图像生成的一致性和高质量，作为一种无需再训练的可扩展优化插件，展现出广泛实际应用潜力。

Abstract: Recent text-to-image diffusion models have achieved remarkable visual fidelity but often struggle with semantic alignment to complex prompts. We introduce CritiFusion, a novel inference-time framework that integrates a multimodal semantic critique mechanism with frequency-domain refinement to improve text-to-image consistency and detail. The proposed CritiCore module leverages a vision-language model and multiple large language models to enrich the prompt context and produce high-level semantic feedback, guiding the diffusion process to better align generated content with the prompt's intent. Additionally, SpecFusion merges intermediate generation states in the spectral domain, injecting coarse structural information while preserving high-frequency details. No additional model training is required. CritiFusion serves as a plug-in refinement stage compatible with existing diffusion backbones. Experiments on standard benchmarks show that our method notably improves human-aligned metrics of text-to-image correspondence and visual quality. CritiFusion consistently boosts performance on human preference scores and aesthetic evaluations, achieving results on par with state-of-the-art reward optimization approaches. Qualitative results further demonstrate superior detail, realism, and prompt fidelity, indicating the effectiveness of our semantic critique and spectral alignment strategy.

</details>


### [71] [Autoregressive Flow Matching for Motion Prediction](https://arxiv.org/abs/2512.22688)
*Johnathan Xie,Stefan Stojanov,Cristobal Eyzaguirre,Daniel L. K. Yamins,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出了一种新的序列连续数据概率建模方法ARFM，能在多种视频数据集上长时预测物体轨迹，显著提升了人类和机器人动作预测等任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有运动预测模型通常训练于单一领域，对复杂运动或泛化性有限；而大规模视频预测虽具视觉真实感，但在建模复杂运动上效果不足。为此，需要一种既能刻画复杂运动，又可在不同场景泛化的方法。

Method: 提出了自回归流匹配(ARFM)方法，可以对连续的时序数据（如视频中的物体运动轨迹）进行概率建模，并在大规模、类型多样的视频数据集上进行训练，以生成未来的轨迹点。

Result: 模型在新设计的人类及机器人运动预测基准数据集验证下，成功预测复杂运动，并且在以预测轨迹为条件的人类和机器人动作预测任务中，能显著提升下游表现。

Conclusion: ARFM方法在准确长期预测复杂运动轨迹上效果突出，将为人类动作预测、机器人运动等相关领域带来更高效和精确的解决方案。代码和模型已开源。

Abstract: Motion prediction has been studied in different contexts with models trained on narrow distributions and applied to downstream tasks in human motion prediction and robotics. Simultaneously, recent efforts in scaling video prediction have demonstrated impressive visual realism, yet they struggle to accurately model complex motions despite massive scale. Inspired by the scaling of video generation, we develop autoregressive flow matching (ARFM), a new method for probabilistic modeling of sequential continuous data and train it on diverse video datasets to generate future point track locations over long horizons. To evaluate our model, we develop benchmarks for evaluating the ability of motion prediction models to predict human and robot motion. Our model is able to predict complex motions, and we demonstrate that conditioning robot action prediction and human motion prediction on predicted future tracks can significantly improve downstream task performance. Code and models publicly available at: https://github.com/Johnathan-Xie/arfm-motion-prediction.

</details>


### [72] [Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors](https://arxiv.org/abs/2512.22689)
*Salvador Rodriguez-Sanz,Monica Hernandez*

Main category: cs.CV

TL;DR: 本文提出了一种基于Neural ODEs的多模态可微形变配准方法，提升了配准准确性、泛化性及效率。


<details>
  <summary>Details</summary>
Motivation: 现有非刚性配准算法存在准确性、模型复杂度与正则化之间的权衡，还普遍假设同源区域强度相关，难以应用于多模态场景；且许多模型依赖大量训练数据，难以泛化到未见过的模态。

Method: 采用Neural ODEs框架，结合结构描述符（可跨模态），提出三种不同变体：集成图像或特征的结构描述符，以及结合局部互信息的非结构相似性；为单实例配准设计，无需大量训练数据。

Result: 在多个混合扫描数据集的实验中，所提方法在质与量上均优于现有SOTA多模态配准算法，适用于大、小形变。

Conclusion: 该方法具有泛化性强、正则鲁棒、可多尺度注册和高效的优点，适用于多模态大形变配准问题。

Abstract: This work proposes a multimodal diffeomorphic registration method using Neural Ordinary Differential Equations (Neural ODEs). Nonrigid registration algorithms exhibit tradeoffs between their accuracy, the computational complexity of their deformation model, and its proper regularization. In addition, they also assume intensity correlation in anatomically homologous regions of interest among image pairs, limiting their applicability to the monomodal setting. Unlike learning-based models, we propose an instance-specific framework that is not subject to high scan requirements for training and does not suffer performance degradation at inference time on modalities unseen during training. Our method exploits the potential of continuous-depth networks in the Neural ODE paradigm with structural descriptors, widely adopted as modality-agnostic metric models which exploit self-similarities on parameterized neighborhood geometries. We propose three different variants that integrate image-based or feature-based structural descriptors and nonstructural image similarities computed by local mutual information. We conduct extensive evaluations on different experiments formed by scan dataset combinations and show surpassing qualitative and quantitative results compared to state-of-the-art baselines adequate for large or small deformations, and specific of multimodal registration. Lastly, we also demonstrate the underlying robustness of the proposed framework to varying levels of explicit regularization while maintaining low error, its suitability for registration at varying scales, and its efficiency with respect to other methods targeted to large-deformation registration.

</details>


### [73] [SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis](https://arxiv.org/abs/2512.22706)
*Paul Dobre,Jackson Cooper,Xin Wang,Hongzhou Yang*

Main category: cs.CV

TL;DR: 本文提出了SCPainter，一个可以同时实现真实感3D资产插入和新视角合成（NVS）的统一仿真框架，提升自动驾驶训练数据的多样性和真实性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶仿真需要丰富多样的训练数据，其中3D资产（如汽车等动态物体）的真实插入和新视角下的图像合成是关键环节。目前，这两个方向的方法大多各自独立，无法结合。作者希望解决3D资产插入与NVS的协同问题，从而方便生成更多样、更真实的自动驾驶场景，以提升模型的鲁棒性和安全性。

Method: 提出了SCPainter框架，核心思想是将3D高斯斑点（Gaussian Splat, GS）资产表示和场景点云结合，通过扩散模型生成高质量的新视角图像。具体做法是将3D GS资产和3D场景点云共同投影到目标新视角，再用这些投影结果作为扩散模型的条件，生成最终融合、真实的图像。

Result: 在Waymo Open Dataset上的实验表明，该方法能有效实现高度真实的3D资产插入与新视角合成，生成的训练数据场景多样、逼真，有助于自动驾驶模型训练。

Conclusion: SCPainter为自动驾驶仿真中的3D资产插入和NVS提供了有效的统一解决方案，有望推动自动驾驶系统在长尾与复杂场景下的性能与安全提升。

Abstract: 3D Asset insertion and novel view synthesis (NVS) are key components for autonomous driving simulation, enhancing the diversity of training data. With better training data that is diverse and covers a wide range of situations, including long-tailed driving scenarios, autonomous driving models can become more robust and safer. This motivates a unified simulation framework that can jointly handle realistic integration of inserted 3D assets and NVS. Recent 3D asset reconstruction methods enable reconstruction of dynamic actors from video, supporting their re-insertion into simulated driving scenes. While the overall structure and appearance can be accurate, it still struggles to capture the realism of 3D assets through lighting or shadows, particularly when inserted into scenes. In parallel, recent advances in NVS methods have demonstrated promising results in synthesizing viewpoints beyond the originally recorded trajectories. However, existing approaches largely treat asset insertion and NVS capabilities in isolation. To allow for interaction with the rest of the scene and to enable more diverse creation of new scenarios for training, realistic 3D asset insertion should be combined with NVS. To address this, we present SCPainter (Street Car Painter), a unified framework which integrates 3D Gaussian Splat (GS) car asset representations and 3D scene point clouds with diffusion-based generation to jointly enable realistic 3D asset insertion and NVS. The 3D GS assets and 3D scene point clouds are projected together into novel views, and these projections are used to condition a diffusion model to generate high quality images. Evaluation on the Waymo Open Dataset demonstrate the capability of our framework to enable 3D asset insertion and NVS, facilitating the creation of diverse and realistic driving data.

</details>


### [74] [Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning](https://arxiv.org/abs/2512.22730)
*Youssef Megahed,Robin Ducharme,Inok Lee,Inbal Willner,Olivier X. Miguel,Kevin Dick,Adrian D. C. Chan,Mark Walker,Steven Hawken*

Main category: cs.CV

TL;DR: 本研究提出了一种基于自监督预训练的深度学习模型（USF-MAE），在大量无标签超声图像上预训练后，用于早孕期超声图像中囊状淋巴管瘤的自动检测，结果显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 囊状淋巴管瘤在孕期超声中是高度危险的异常发现，相关的染色体异常和不良妊娠结局发生率高。自动化检测可提升早筛的可扩展性，但受限于标注数据稀缺。研究动机在于通过自监督学习突破数据瓶颈，提升检测准确性和通用性。

Method: 作者使用了USF-MAE模型，于37万余张无标签超声图像上进行自监督预训练，再对本研究中的已知病例（二分类任务：正常vs囊状淋巴管瘤）进行微调。与DenseNet-169基线模型同样使用标准化数据集、预处理流程及4折交叉验证，并进行多指标评估（准确率、敏感性、特异性、ROC-AUC），同时用Score-CAM可视化解释模型预测结果。

Result: USF-MAE在各项指标上（准确率0.96、敏感性0.94、特异性0.98、ROC-AUC 0.98）均显著优于DenseNet-169基线模型（分别为0.93、0.92、0.94、0.94）。Score-CAM可视化也验证模型关注于临床相关区域。Wilcoxon符号秩检验证明模型提升有统计学意义（p=0.0057）。

Conclusion: 经自监督预训练的USF-MAE模型可显著提升囊状淋巴管瘤超声筛查的准确性和稳定性，具备临床可解释性和可推广性，有助于大规模早筛项目的实施。

Abstract: Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).

</details>


### [75] [Split4D: Decomposed 4D Scene Reconstruction Without Video Segmentation](https://arxiv.org/abs/2512.22745)
*Yongzhen Hu,Yihui Yang,Haotong Lin,Yifan Wang,Junting Dong,Yifu Deng,Xinyu Zhu,Fan Jia,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: 本文提出了一种新的分解式4D场景重建方法，无需依赖视频分割，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法高度依赖视频分割质量，但分割图容易不稳定，导致4D重建结果不可靠。

Method: 创新性地提出Freetime FeatureGS，用高斯基元和可学习特征表示动态场景，并设计流式特征学习策略，通过对比损失实现时空维度上的特征分离，无需全局视频分割，仅依赖单帧分割。

Result: 在多个数据集上，本文方法在重建质量上远超最新技术，效果提升明显。

Conclusion: Freetime FeatureGS和流式特征学习策略有效解决了4D重建中的分割依赖问题，实现了高质量的动态场景分解和重建。

Abstract: This paper addresses the problem of decomposed 4D scene reconstruction from multi-view videos. Recent methods achieve this by lifting video segmentation results to a 4D representation through differentiable rendering techniques. Therefore, they heavily rely on the quality of video segmentation maps, which are often unstable, leading to unreliable reconstruction results. To overcome this challenge, our key idea is to represent the decomposed 4D scene with the Freetime FeatureGS and design a streaming feature learning strategy to accurately recover it from per-image segmentation maps, eliminating the need for video segmentation. Freetime FeatureGS models the dynamic scene as a set of Gaussian primitives with learnable features and linear motion ability, allowing them to move to neighboring regions over time. We apply a contrastive loss to Freetime FeatureGS, forcing primitive features to be close or far apart based on whether their projections belong to the same instance in the 2D segmentation map. As our Gaussian primitives can move across time, it naturally extends the feature learning to the temporal dimension, achieving 4D segmentation. Furthermore, we sample observations for training in a temporally ordered manner, enabling the streaming propagation of features over time and effectively avoiding local minima during the optimization process. Experimental results on several datasets show that the reconstruction quality of our method outperforms recent methods by a large margin.

</details>


### [76] [TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts](https://arxiv.org/abs/2512.22748)
*Hao Zhang,Mengsi Lyu,Bo Huang,Yulong Ao,Yonghua Lin*

Main category: cs.CV

TL;DR: 本文提出了一种适用于长上下文且包含多图像场景的大型多模态模型视觉token剪枝方法，有效减少了视觉token数量，并在保持性能的前提下降低了推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在处理包含多张图片的长文本时，视觉token数量急剧增加，导致推理消耗大。现有的视觉token剪枝方法在应对多图像、长上下文场景时效果有限，缺乏对不同图片间/图片内冗余的充分建模。

Method: 作者分析了多图像、长文本上下文下视觉token冗余的特点，将冗余分解为图片内与图片间两个部分，分别通过图片内多样性与图片间差异性进行量化。方法分为两级：先为每张图片分配内容感知的token预算并贪心挑选代表性token，再在全局候选池中通过Pareto选择方法权衡多样性与文本对齐性来最终确定保留token。

Result: 实验表明，该方法在保持长上下文多模态理解能力的同时，显著减少了需要处理的视觉token数量，从而降低了推理资源和时间消耗。

Conclusion: 面向多图片、多文本、长上下文应用场景，作者提出的自适应视觉token剪枝方法高效且实用，有望为实际多模态模型推理带来显著优化。

Abstract: Large Multimodal Models (LMMs) have proven effective on various tasks. They typically encode visual inputs into Original Model sequences of tokens, which are then concatenated with textual tokens and jointly processed by the language model. However, the growing number of visual tokens greatly increases inference cost. Visual token pruning has emerged as a promising solution. However, existing methods often overlook scenarios involving long context inputs with multiple images. In this paper, we analyze the challenges of visual token pruning in long context, multi-image settings and introduce an adaptive pruning method tailored for such scenarios. We decompose redundancy into intra-image and inter-image components and quantify them through intra-image diversity and inter-image variation, which jointly guide dynamic budget allocation. Our approach consists of two stages. The intra-image stage allocates each image a content-aware token budget and greedily selects its most representative tokens. The inter-image stage performs global diversity filtering to form a candidate pool and then applies a Pareto selection procedure that balances diversity with text alignment. Extensive experiments show that our approach maintains strong performance in long context settings while significantly cutting down the number of visual tokens.

</details>


### [77] [Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers](https://arxiv.org/abs/2512.22760)
*Yunge Li,Lanyu Xu*

Main category: cs.CV

TL;DR: 本文提出了基于Hilbert曲线的邻域感知Token简化方法，有效提升了Vision Transformer的计算效率，在准确率与效率之间取得了业界领先的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉Transformer在视觉识别中表现优异，但冗余的Token表示影响了计算效率。现有的Token合并与剪枝方法不重视空间邻域关系，容易损失局部上下文。

Method: 通过Hilbert曲线重排序，在1D序列中保留2D空间中的邻里关系，并提出了两种方法：1）邻域感知剪枝（NAP），有选择地保留更重要的Token；2）基于相邻Token相似度的合并（MAT），对局部Token进行聚合。

Result: 实验表明，该方法能在提升效率的同时保持或提升识别准确率，在多个视觉任务和指标上优于现有Token合并/剪枝方法。

Conclusion: 强调了空间连续性和邻域结构对于ViT结构优化的重要性，提出的邻域感知Token简化方法为视觉Transformer网络高效化提供了新方向。

Abstract: Vision Transformers (ViTs) have achieved remarkable success in visual recognition tasks, but redundant token representations limit their computational efficiency. Existing token merging and pruning strategies often overlook spatial continuity and neighbor relationships, resulting in the loss of local context. This paper proposes novel neighbor-aware token reduction methods based on Hilbert curve reordering, which explicitly preserves the neighbor structure in a 2D space using 1D sequential representations. Our method introduces two key strategies: Neighbor-Aware Pruning (NAP) for selective token retention and Merging by Adjacent Token similarity (MAT) for local token aggregation. Experiments demonstrate that our approach achieves state-of-the-art accuracy-efficiency trade-offs compared to existing methods. This work highlights the importance of spatial continuity and neighbor structure, offering new insights for the architectural optimization of ViTs.

</details>


### [78] [Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2512.22771)
*Yiqian Li,Wen Jiang,Kostas Daniilidis*

Main category: cs.CV

TL;DR: 本文提出一种基于Fisher信息的主动学习算法，用于多视角选择，以提升具身智能体在理解场景语义和动态变化时的信息获取效率。该方法在大规模静态和动态数据集上验证，显著改善了渲染质量与语义分割表现。


<details>
  <summary>Details</summary>
Motivation: 具身智能体在执行任务时需要高效地理解场景语义和动态变化，然而与静态场景相比，动态场景中的数据冗余更高，随机或启发式采样难以有效利用数据。因此需要一种更有理论依据的方法来选择最有信息量的视角或帧，提升模型训练效果。

Method: 作者将视角选择问题建模为主动学习问题，提出基于Fisher信息量的视角选择算法。该算法根据语义高斯参数和形变网络对每一个候选视角的信息量进行量化，优先选择提供最大信息增益的帧。这种方法能够同时兼顾语义推理和动态场景建模，而不是单纯依赖经验或随机选择。

Result: 在大规模静态图像与动态视频数据集上，作者通过从多摄像头采集的帧中筛选信息量高的帧，实验显示该方法在提升渲染质量和语义分割精度方面优于基于随机选择和不确定性启发式的基线方法。

Conclusion: 所提方法为数据冗余场景下的主动帧选择提供了理论支撑，有效兼顾了动态场景与语义理解的需求，对具身智能体的视觉学习任务有明显提升作用。

Abstract: Understanding semantics and dynamics has been crucial for embodied agents in various tasks. Both tasks have much more data redundancy than the static scene understanding task. We formulate the view selection problem as an active learning problem, where the goal is to prioritize frames that provide the greatest information gain for model training. To this end, we propose an active learning algorithm with Fisher Information that quantifies the informativeness of candidate views with respect to both semantic Gaussian parameters and deformation networks. This formulation allows our method to jointly handle semantic reasoning and dynamic scene modeling, providing a principled alternative to heuristic or random strategies. We evaluate our method on large-scale static images and dynamic video datasets by selecting informative frames from multi-camera setups. Experimental results demonstrate that our approach consistently improves rendering quality and semantic segmentation performance, outperforming baseline methods based on random selection and uncertainty-based heuristics.

</details>


### [79] [Plug In, Grade Right: Psychology-Inspired AGIQA](https://arxiv.org/abs/2512.22780)
*Zhicheng Liao,Baoliang Chen,Hanwei Zhu,Lingyu Zhu,Shiqi Wang,Weisi Lin*

Main category: cs.CV

TL;DR: 该论文提出了一种改进的有梯度响应模型（GRM），用于提升自动化图像质量评估（AGIQA）系统的准确性，并能直观地解决已有模型存在的‘语义漂移’问题。


<details>
  <summary>Details</summary>
Motivation: 现有AGIQA方法通过图像和文本嵌入之间的相似性来衡量图像质量，但发现相似性分布经常呈现多峰（multimodal），导致语义与文本描述之间不一致（语义漂移），影响评分可靠性。

Method: 论文借鉴心理测量学，提出将GRM引入AGIQA，通过设计‘能力-难度’双分支打分模块：一支评估图像‘能力’，另一支构建分级‘难度’，并用算术方式保证难度单调递增，从而实现单峰、可解释的评分分布。该模块可直接插入到现有AGIQA框架中。

Result: 将本文提出的AGQG模块应用到多种AGIQA主流模型后，均表现出一致的性能提升。不仅适用于自然图像，也适用于屏幕内容图像，表现出优异的泛化能力。

Conclusion: AGQG模块有效缓解了语义漂移问题，并显著提升各类AGIQA系统的性能和泛化性，预示着其在未来高质量图像评估模型中的重要应用潜力。

Abstract: Existing AGIQA models typically estimate image quality by measuring and aggregating the similarities between image embeddings and text embeddings derived from multi-grade quality descriptions. Although effective, we observe that such similarity distributions across grades usually exhibit multimodal patterns. For instance, an image embedding may show high similarity to both "excellent" and "poor" grade descriptions while deviating from the "good" one. We refer to this phenomenon as "semantic drift", where semantic inconsistencies between text embeddings and their intended descriptions undermine the reliability of text-image shared-space learning. To mitigate this issue, we draw inspiration from psychometrics and propose an improved Graded Response Model (GRM) for AGIQA. The GRM is a classical assessment model that categorizes a subject's ability across grades using test items with various difficulty levels. This paradigm aligns remarkably well with human quality rating, where image quality can be interpreted as an image's ability to meet various quality grades. Building on this philosophy, we design a two-branch quality grading module: one branch estimates image ability while the other constructs multiple difficulty levels. To ensure monotonicity in difficulty levels, we further model difficulty generation in an arithmetic manner, which inherently enforces a unimodal and interpretable quality distribution. Our Arithmetic GRM based Quality Grading (AGQG) module enjoys a plug-and-play advantage, consistently improving performance when integrated into various state-of-the-art AGIQA frameworks. Moreover, it also generalizes effectively to both natural and screen content image quality assessment, revealing its potential as a key component in future IQA models.

</details>


### [80] [Parallel Diffusion Solver via Residual Dirichlet Policy Optimization](https://arxiv.org/abs/2512.22796)
*Ruoyu Wang,Ziyu Li,Beier Zhu,Liangyu Yuan,Hanwang Zhang,Xun Yang,Xiaojun Chang,Chi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种全新的并行方向集成（EPD）求解器EPD-Solver，有效提升了扩散模型在低延迟采样下的图像质量，同时能作为插件提升现有ODE采样器表现。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成效果出色，但因去噪本质需要多步操作，采样速度慢。现有的加速方法在低延迟场景下易导致图像质量下降，关键问题是无法准确处理高曲率轨迹带来的截断误差。

Method: 作者提出EPD-Solver，将每一步的梯度计算并行采集多个方向，通过向量值函数的平均值定理更精准地近似积分，解决轨迹高曲率带来的误差，同时梯度计算完全可并行化，提升速度。方法还包括两阶段优化：先利用蒸馏法优化少量参数，再用高效强化学习在低维解算器空间微调，规避大规模reward hacking。

Result: 实验表明，EPD-Solver能在保证低采样延迟的情况下大幅提升生成图像质量，对于复杂文本到图像的任务尤其有效，且方法通用，可作为插件应用于各类ODE采样器。

Conclusion: EPD-Solver为扩散模型低延迟采样提供了高效且灵活的新解法，在不牺牲速度的前提下提升图像质量，并具备良好的可扩展性和兼容性。

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance but suffer from high sampling latency due to their sequential denoising nature. Existing solver-based acceleration methods often face significant image quality degradation under a low-latency budget, primarily due to accumulated truncation errors arising from the inability to capture high-curvature trajectory segments. In this paper, we propose the Ensemble Parallel Direction solver (dubbed as EPD-Solver), a novel ODE solver that mitigates these errors by incorporating multiple parallel gradient evaluations in each step. Motivated by the geometric insight that sampling trajectories are largely confined to a low-dimensional manifold, EPD-Solver leverages the Mean Value Theorem for vector-valued functions to approximate the integral solution more accurately. Importantly, since the additional gradient computations are independent, they can be fully parallelized, preserving low-latency sampling nature. We introduce a two-stage optimization framework. Initially, EPD-Solver optimizes a small set of learnable parameters via a distillation-based approach. We further propose a parameter-efficient Reinforcement Learning (RL) fine-tuning scheme that reformulates the solver as a stochastic Dirichlet policy. Unlike traditional methods that fine-tune the massive backbone, our RL approach operates strictly within the low-dimensional solver space, effectively mitigating reward hacking while enhancing performance in complex text-to-image (T2I) generation tasks. In addition, our method is flexible and can serve as a plugin (EPD-Plugin) to improve existing ODE samplers.

</details>


### [81] [VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM](https://arxiv.org/abs/2512.22799)
*Jingchao Wang,Kaiwen Zhou,Zhijian Wu,Kunhua Ji,Dingjiang Huang,Yefeng Zheng*

Main category: cs.CV

TL;DR: 提出了一种基于多模态大语言模型（MLLM）的全局视觉-语言目标跟踪方法VPTracker，通过引入带空间先验的视觉提示，有效提升了复杂场景下的跟踪鲁棒性和目标区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言跟踪方法以局部搜索为主，易受视角变化、遮挡和目标快速移动影响，导致跟踪失败。缺少能利用全局信息定位目标、同时具备较强语义推理能力的解决方案。

Method: 本方法首次将MLLM引入全局视觉-语言跟踪，通过设计基于目标前一帧位置的区块级视觉提示，将空间先验引入MLLM，引导模型关注优先区域，只有在必要时才进行全局推理，从而兼顾全局搜索的鲁棒性和局部冲突的抑制。

Result: 实验表明，该方法在复杂场景下（如相似干扰物体、剧烈运动等）显著提升了跟踪稳定性和目标区分能力。

Conclusion: VPTracker开拓了MLLM在视觉跟踪领域的新应用方向。通过结合空间先验的视觉提示和多模态推理，有效提升了视觉-语言目标跟踪的性能，值得进一步探索和推广。

Abstract: Vision-Language Tracking aims to continuously localize objects described by a visual template and a language description. Existing methods, however, are typically limited to local search, making them prone to failures under viewpoint changes, occlusions, and rapid target movements. In this work, we introduce the first global tracking framework based on Multimodal Large Language Models (VPTracker), exploiting their powerful semantic reasoning to locate targets across the entire image space. While global search improves robustness and reduces drift, it also introduces distractions from visually or semantically similar objects. To address this, we propose a location-aware visual prompting mechanism that incorporates spatial priors into the MLLM. Specifically, we construct a region-level prompt based on the target's previous location, enabling the model to prioritize region-level recognition and resort to global inference only when necessary. This design retains the advantages of global tracking while effectively suppressing interference from distracting visual content. Extensive experiments show that our approach significantly enhances tracking stability and target disambiguation under challenging scenarios, opening a new avenue for integrating MLLMs into visual tracking. Code is available at https://github.com/jcwang0602/VPTracker.

</details>


### [82] [Medical Scene Reconstruction and Segmentation based on 3D Gaussian Representation](https://arxiv.org/abs/2512.22800)
*Bin Liu,Wenyan Tian,Huangxin Fu,Zizheng Li,Zhifen He,Bo Li*

Main category: cs.CV

TL;DR: 提出了一种结合3D高斯与三平面表示的高效医疗影像三维重建方法，在稀疏切片条件下依然能生成高质量、结构连贯的3D医学影像。


<details>
  <summary>Details</summary>
Motivation: 传统医疗影像三维重建方法在计算量大、稀疏切片下结构不连贯且细节丢失等方面存在不足，影响临床应用的准确性和效率。

Method: 创新性地结合3D高斯表示与三平面表示，兼顾高效渲染与结构、几何表达能力，在稀疏切片数据下提升结构连续性与语义一致性。

Result: 在超声和MRI等多模态医疗影像数据集上验证，方法可在稀疏数据下生成高质量、解剖结构合理、语义稳定的3D重建结果，并大幅提升重建效率。

Conclusion: 所提方法为稀疏医疗影像的三维可视化与临床分析提供了一种高效且可靠的新方案，有望推动实际临床应用。

Abstract: 3D reconstruction of medical images is a key technology in medical image analysis and clinical diagnosis, providing structural visualization support for disease assessment and surgical planning. Traditional methods are computationally expensive and prone to structural discontinuities and loss of detail in sparse slices, making it difficult to meet clinical accuracy requirements.To address these challenges, we propose an efficient 3D reconstruction method based on 3D Gaussian and tri-plane representations. This method not only maintains the advantages of Gaussian representation in efficient rendering and geometric representation but also significantly enhances structural continuity and semantic consistency under sparse slicing conditions. Experimental results on multimodal medical datasets such as US and MRI show that our proposed method can generate high-quality, anatomically coherent, and semantically stable medical images under sparse data conditions, while significantly improving reconstruction efficiency. This provides an efficient and reliable new approach for 3D visualization and clinical analysis of medical images.

</details>


### [83] [Evaluating the Performance of Open-Vocabulary Object Detection in Low-quality Image](https://arxiv.org/abs/2512.22801)
*Po-Chih Wu*

Main category: cs.CV

TL;DR: 本文评估了开放词汇物体检测模型在低质量图像下的表现，并引入了一个新的低质量图像数据集。实验表明，在高强度图像退化下，模型性能显著下降。OWLv2模型在各种退化条件下表现最优。


<details>
  <summary>Details</summary>
Motivation: 尽管开放词汇物体检测在识别能力上接近人类水平，但其在低质量实际图像上的鲁棒性尚未被系统评估。作者希望通过全面测试和新的数据集弥补这一空白。

Method: 该研究构建了一个模拟现实低质量图像环境的新数据集，并使用该数据集系统评估了多种主流开放词汇检测模型（如OWLv2、OWL-ViT、GroundingDINO和Detic）在多种图像退化类型和强度下的性能。

Result: 实验发现，各模型在轻度退化下mAP得分基本不变，但在高度退化时性能大幅下降。OWLv2模型鲁棒性最强，其他模型（OWL-ViT、GroundingDINO、Detic）在高退化下下降明显。

Conclusion: 开放词汇物体检测模型对图像质量敏感，尤其在极端退化情况下，性能急剧下滑，因此需要进一步提升模型对低质量图像的适应能力。研究中发布的新数据集和代码将推动该领域未来发展。

Abstract: Open-vocabulary object detection enables models to localize and recognize objects beyond a predefined set of categories and is expected to achieve recognition capabilities comparable to human performance. In this study, we aim to evaluate the performance of existing models on open-vocabulary object detection tasks under low-quality image conditions. For this purpose, we introduce a new dataset that simulates low-quality images in the real world. In our evaluation experiment, we find that although open-vocabulary object detection models exhibited no significant decrease in mAP scores under low-level image degradation, the performance of all models dropped sharply under high-level image degradation. OWLv2 models consistently performed better across different types of degradation, while OWL-ViT, GroundingDINO, and Detic showed significant performance declines. We will release our dataset and codes to facilitate future studies.

</details>


### [84] [EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation](https://arxiv.org/abs/2512.22808)
*Libo Zhang,Zekun Li,Tianyu Li,Zeyu Cao,Rui Xu,Xiaoxiao Long,Wenjia Wang,Jingbo Wang,Yuan Liu,Wenping Wang,Daquan Zhou,Taku Komura,Zhiyang Dou*

Main category: cs.CV

TL;DR: 本论文提出了一种新的端到端方法，用于从第一视角视频中实时生成空间对齐的人类反应动作。为此，作者新建了一个高质量的数据集并推出了EgoReAct框架，实验证明该方法在真实感、空间一致性和效率上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有的第一视角视频-动作反应数据存在空间不一致、数据匮乏的问题，导致模型难以实现严格因果和3D空间对齐的动作生成。作者希望解决这一挑战，提升人类动作反应建模的准确性和实用性。

Method: 作者首先构建了一个空间对齐的第一视角视频-反应数据集（HRD），解决了数据短缺及对齐差的问题。然后提出EgoReAct框架：1）利用VQ-VAE将反应动作压缩到紧凑且可表达的隐空间；2）结合GPT将视觉输入转化为动作生成；3）引入3D动态特征如度量深度和头部动态强化空间表达。

Result: 在大量实验证明下，EgoReAct在生成的人类动作反应的真实感、空间一致性和生成效率上，较以往方法有显著提升，并能保持严格的因果生成特性。

Conclusion: EgoReAct框架在空间对齐动作生成领域展示了强大性能，推动了基于第一视角视频的自适应人类动作建模的发展。代码、模型和数据将在录用后公开，便于相关领域的进一步研究。

Abstract: Humans exhibit adaptive, context-sensitive responses to egocentric visual input. However, faithfully modeling such reactions from egocentric video remains challenging due to the dual requirements of strictly causal generation and precise 3D spatial alignment. To tackle this problem, we first construct the Human Reaction Dataset (HRD) to address data scarcity and misalignment by building a spatially aligned egocentric video-reaction dataset, as existing datasets (e.g., ViMo) suffer from significant spatial inconsistency between the egocentric video and reaction motion, e.g., dynamically moving motions are always paired with fixed-camera videos. Leveraging HRD, we present EgoReAct, the first autoregressive framework that generates 3D-aligned human reaction motions from egocentric video streams in real-time. We first compress the reaction motion into a compact yet expressive latent space via a Vector Quantised-Variational AutoEncoder and then train a Generative Pre-trained Transformer for reaction generation from the visual input. EgoReAct incorporates 3D dynamic features, i.e., metric depth, and head dynamics during the generation, which effectively enhance spatial grounding. Extensive experiments demonstrate that EgoReAct achieves remarkably higher realism, spatial consistency, and generation efficiency compared with prior methods, while maintaining strict causality during generation. We will release code, models, and data upon acceptance.

</details>


### [85] [Depth Anything in $360^\circ$: Towards Scale Invariance in the Wild](https://arxiv.org/abs/2512.22819)
*Hualie Jiang,Ziyang Song,Zhiqiang Lou,Rui Xu,Minglang Tan*

Main category: cs.CV

TL;DR: 本文提出了一种名为DA360的全景深度估计算法，采用ViT主干学习shift参数，并在解码器中引入了圆形填充以消除接缝，能够大幅提升室内外全景深度估计的零样本泛化能力，在多个数据集上取得最新最优性能。


<details>
  <summary>Details</summary>
Motivation: 全景深度估计对机器人、AR/VR等领域有重要意义，但目前主要集中在室内场景，且在开放领域的零样本泛化能力远逊于透视图像。主要原因在于透视图有大量训练数据，而全景数据有限，因此迫切需要提升全景深度估计的方法和泛化能力。

Method: 提出DA360方法：在Depth Anything V2基础上，学习ViT骨干网络的shift参数，将原本对尺度和偏移不敏感的输出转化为直接生成3D点云的尺度不变估计，并在解码阶段引入圆形填充消除全景拼接缝隙，保证深度图空间连续性。

Result: 在标准室内数据集和新构建的室外数据集Metropolis上，DA360较基线模型分别实现了50%和10%以上的深度误差降低。与最佳全景深度估算法PanDA相比，在三个测试集上相对误差均降低约30%，创下零样本全景深度估计新纪录。

Conclusion: DA360极大提升了全景深度估计的泛化能力，尤其在零样本情况下表现显著优于现有方法，为全景视觉在实际应用中的推广奠定了基础。

Abstract: Panoramic depth estimation provides a comprehensive solution for capturing complete $360^\circ$ environmental structural information, offering significant benefits for robotics and AR/VR applications. However, while extensively studied in indoor settings, its zero-shot generalization to open-world domains lags far behind perspective images, which benefit from abundant training data. This disparity makes transferring capabilities from the perspective domain an attractive solution. To bridge this gap, we present Depth Anything in $360^\circ$ (DA360), a panoramic-adapted version of Depth Anything V2. Our key innovation involves learning a shift parameter from the ViT backbone, transforming the model's scale- and shift-invariant output into a scale-invariant estimate that directly yields well-formed 3D point clouds. This is complemented by integrating circular padding into the DPT decoder to eliminate seam artifacts, ensuring spatially coherent depth maps that respect spherical continuity. Evaluated on standard indoor benchmarks and our newly curated outdoor dataset, Metropolis, DA360 shows substantial gains over its base model, achieving over 50\% and 10\% relative depth error reduction on indoor and outdoor benchmarks, respectively. Furthermore, DA360 significantly outperforms robust panoramic depth estimation methods, achieving about 30\% relative error improvement compared to PanDA across all three test datasets and establishing new state-of-the-art performance for zero-shot panoramic depth estimation.

</details>


### [86] [KANO: Kolmogorov-Arnold Neural Operator for Image Super-Resolution](https://arxiv.org/abs/2512.22822)
*Chenyu Li,Danfeng Hong,Bing Zhang,Zhaojie Pan,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: 本文提出了一种基于柯尔莫哥洛夫-阿尔诺德定理（KAT）的新型神经算子KANO，用于图像单帧超分辨率提升，并兼具物理可解释性。KANO结构透明，能拟合复杂退化过程，同时通过对比实验揭示不同神经网络结构在拟合复杂序列任务中的优劣。


<details>
  <summary>Details</summary>
Motivation: 图像单帧超分辨率任务中，退化过程高度非线性且不易解释，现有方法多依赖深度网络的黑盒建模，导致退化机制未知且不可控。论文旨在提出具有可解释性的建模手段，更好地掌控和理解退化过程。

Method: 受KAT启发，作者首次提出KANO算子，通过有限个B样条函数组成的加性结构来分段拟合连续谱曲线，并在指定区间内学习和优化样条形状参数，以捕捉局部线性趋势及非线性拐点处的峰谷等谱特性。比较MLP与KAN在复杂序列拟合任务上的表现。

Result: 理论建模和多领域实验（自然图像、航空照片、卫星遥感）显示，KANO能精确拟合关键谱特征，对比MLP与KAN，揭示了各自处理复杂退化的优缺点。

Conclusion: KANO不仅提升了SR结果的物理可解释性，还为开发更可解释的超分辨率技术提供了理论和实验依据，促进了可控可解释退化建模的发展。

Abstract: The highly nonlinear degradation process, complex physical interactions, and various sources of uncertainty render single-image Super-resolution (SR) a particularly challenging task. Existing interpretable SR approaches, whether based on prior learning or deep unfolding optimization frameworks, typically rely on black-box deep networks to model latent variables, which leaves the degradation process largely unknown and uncontrollable. Inspired by the Kolmogorov-Arnold theorem (KAT), we for the first time propose a novel interpretable operator, termed Kolmogorov-Arnold Neural Operator (KANO), with the application to image SR. KANO provides a transparent and structured representation of the latent degradation fitting process. Specifically, we employ an additive structure composed of a finite number of B-spline functions to approximate continuous spectral curves in a piecewise fashion. By learning and optimizing the shape parameters of these spline functions within defined intervals, our KANO accurately captures key spectral characteristics, such as local linear trends and the peak-valley structures at nonlinear inflection points, thereby endowing SR results with physical interpretability. Furthermore, through theoretical modeling and experimental evaluations across natural images, aerial photographs, and satellite remote sensing data, we systematically compare multilayer perceptrons (MLPs) and Kolmogorov-Arnold networks (KANs) in handling complex sequence fitting tasks. This comparative study elucidates the respective advantages and limitations of these models in characterizing intricate degradation mechanisms, offering valuable insights for the development of interpretable SR techniques.

</details>


### [87] [3D Scene Change Modeling With Consistent Multi-View Aggregation](https://arxiv.org/abs/2512.22830)
*Zirui Zhou,Junfeng Ni,Shujie Zhang,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: 本文提出了SCaR-3D，一种新颖的三维场景变更检测框架，能够实现高效且准确的对象级变化检测，并实现场景的持续重建。


<details>
  <summary>Details</summary>
Motivation: 现有的三维变更检测方法在检测结果上存在空间不一致的问题，且难以明确区分变化前后的状态。作者希望解决空间一致性的难题，并实现对动态区域的有效检测和持续重建。

Method: SCaR-3D框架利用基于有符号距离的二维差分模块作为变化检测基础，结合多视图聚合、投票与剪枝策略，充分利用三维高斯球(SGS)的一致性特性以有效区分变化前后的场景。同时，提出了一种持续场景重建策略，对动态区域选择性更新，保持未变化区域的完整。作者还自主构建了CCS3D合成数据集，以支持复杂三维变化的可控评估。

Result: 实验结果显示，SCaR-3D在变更检测精度和效率上均优于现有方法，在新数据集上表现突出。

Conclusion: SCaR-3D显著提升了三维变更检测的空间一致性和状态分离能力，同时提供了高效的持续场景重建解决方案。所提数据集也为后续研究提供了重要工具。

Abstract: Change detection plays a vital role in scene monitoring, exploration, and continual reconstruction. Existing 3D change detection methods often exhibit spatial inconsistency in the detected changes and fail to explicitly separate pre- and post-change states. To address these limitations, we propose SCaR-3D, a novel 3D scene change detection framework that identifies object-level changes from a dense-view pre-change image sequence and sparse-view post-change images. Our approach consists of a signed-distance-based 2D differencing module followed by multi-view aggregation with voting and pruning, leveraging the consistent nature of 3DGS to robustly separate pre- and post-change states. We further develop a continual scene reconstruction strategy that selectively updates dynamic regions while preserving the unchanged areas. We also contribute CCS3D, a challenging synthetic dataset that allows flexible combinations of 3D change types to support controlled evaluations. Extensive experiments demonstrate that our method achieves both high accuracy and efficiency, outperforming existing methods.

</details>


### [88] [A Minimal Solver for Relative Pose Estimation with Unknown Focal Length from Two Affine Correspondences](https://arxiv.org/abs/2512.22833)
*Zhenbao Yu,Shirong Ye,Ronghe Jin,Shunkun Liang,Zibin Liu,Huiyun Zhang,Banglei Guan*

Main category: cs.CV

TL;DR: 该论文提出了一种使用已知除焦距外的内参和IMU辅助的垂直方向信息，仅通过两对仿射对应点来估计两幅图像间3自由度相对位姿和焦距的新方法，并在合成及真实数据集上验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对实际应用（如自动驾驶、无人机、智能手机等）中，常规方法对相机内参完全已知的假设不总成立，焦距未知且只有极少配对情况下，高效、精确的位姿-焦距联合估计需求。同时，IMU常见，垂直方向易获取，可简化问题。

Method: 利用IMU获取视图垂直方向，将两相机的相对姿态自由度由5降至3。基于两对仿射对应点，推导出4个仅涉及焦距和相对旋转角的约束方程，进而采用多项式特征值法高效求解这两个参数。

Result: 实验在合成数据和实际图像上进行，表明该求解器在精度和鲁棒性上超越现有最优方法。

Conclusion: 提出的基于IMU辅助仿射对应点的三自由度位姿和焦距联合估计方法，在理论与实验层面均显示其高效及优越性，适用于内参不全已知和匹配极少的实际场景。

Abstract: In this paper, we aim to estimate the relative pose and focal length between two views with known intrinsic parameters except for an unknown focal length from two affine correspondences (ACs). Cameras are commonly used in combination with inertial measurement units (IMUs) in applications such as self-driving cars, smartphones, and unmanned aerial vehicles. The vertical direction of camera views can be obtained by IMU measurements. The relative pose between two cameras is reduced from 5DOF to 3DOF. We propose a new solver to estimate the 3DOF relative pose and focal length. First, we establish constraint equations from two affine correspondences when the vertical direction is known. Then, based on the properties of the equation system with nontrivial solutions, four equations can be derived. These four equations only involve two parameters: the focal length and the relative rotation angle. Finally, the polynomial eigenvalue method is utilized to solve the problem of focal length and relative rotation angle. The proposed solver is evaluated using synthetic and real-world datasets. The results show that our solver performs better than the existing state-of-the-art solvers.

</details>


### [89] [ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning](https://arxiv.org/abs/2512.22854)
*Bangya Liu,Xinyu Gong,Zelin Zhao,Ziyang Song,Yulei Lu,Suhui Wu,Jun Zhang,Suman Banerjee,Hao Zhang*

Main category: cs.CV

TL;DR: 本文提出了ByteLoom框架，能够通过简化的人体条件和3D物体输入，生成具有跨视角一致性且交互逼真的人-物互动视频。


<details>
  <summary>Details</summary>
Motivation: 现有的人-物互动视频生成方法存在两大难题：（1）缺乏有效机制整合物体多视角信息，导致跨视角一致性差；（2）严重依赖精细的手部网格注释，不利于规模化应用。

Method: 作者提出了基于Diffusion Transformer（DiT）的ByteLoom框架，引入了RCM-cache机制，利用相对坐标图（RCM）作为通用物体表征，保证物体几何一致性并精确调控6-自由度物体变换。同时设计了分阶段训练策略，弱化对精细手部网格数据的依赖，提升模型泛化能力。

Result: 大量实验表明，该方法能够保持人体身份和物体多视角几何一致性，实现流畅的人-物交互运动和物体操作。

Conclusion: ByteLoom框架有效解决了现有HOI视频生成中的跨视角一致性和高注释依赖问题，推进了该方向视频生成模型的实用化与通用性。

Abstract: Human-object interaction (HOI) video generation has garnered increasing attention due to its promising applications in digital humans, e-commerce, advertising, and robotics imitation learning. However, existing methods face two critical limitations: (1) a lack of effective mechanisms to inject multi-view information of the object into the model, leading to poor cross-view consistency, and (2) heavy reliance on fine-grained hand mesh annotations for modeling interaction occlusions. To address these challenges, we introduce ByteLoom, a Diffusion Transformer (DiT)-based framework that generates realistic HOI videos with geometrically consistent object illustration, using simplified human conditioning and 3D object inputs. We first propose an RCM-cache mechanism that leverages Relative Coordinate Maps (RCM) as a universal representation to maintain object's geometry consistency and precisely control 6-DoF object transformations in the meantime. To compensate HOI dataset scarcity and leverage existing datasets, we further design a training curriculum that enhances model capabilities in a progressive style and relaxes the demand of hand mesh. Extensive experiments demonstrate that our method faithfully preserves human identity and the object's multi-view geometry, while maintaining smooth motion and object manipulation.

</details>


### [90] [Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs](https://arxiv.org/abs/2512.22872)
*Ziyu Zhou,Haozhe Luo,Mohammad Reza Hosseinzadeh Taher,Jiaxuan Pang,Xiaowei Ding,Michael B. Gotway,Jianming Liang*

Main category: cs.CV

TL;DR: 本论文提出了Lamps方法，通过利用人体解剖结构的一致性、连贯性和层次性作为自监督信号，在胸部X光图像上进行大规模预训练，显著提升了医学影像基础模型对解剖学信息的感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法常忽视医学影像所蕴含的人体解剖结构特征，导致解剖信息学习效果有限，因此需要结合解剖结构的多维度特性来构建更具代表性的基础模型。

Method: 作者提出Lamps方法，将医学影像中的解剖学一致性、连贯性与层次特征作为自监督信号，设计专门针对胸部X光大数据的预训练流程。

Result: 通过在10个医学影像数据集上的微调与特性分析实验，Lamps在鲁棒性、迁移性和临床可用性上均优于10种主流基线模型。

Conclusion: Lamps展现出基础模型从多视角学习解剖信息的独特优势，为医学影像领域获得对人体结构高度一致、具临床意义的表征提供了新路径。

Abstract: Foundation models have been successful in natural language processing and computer vision because they are capable of capturing the underlying structures (foundation) of natural languages. However, in medical imaging, the key foundation lies in human anatomy, as these images directly represent the internal structures of the body, reflecting the consistency, coherence, and hierarchy of human anatomy. Yet, existing self-supervised learning (SSL) methods often overlook these perspectives, limiting their ability to effectively learn anatomical features. To overcome the limitation, we built Lamps (learning anatomy from multiple perspectives via self-supervision) pre-trained on large-scale chest radiographs by harmoniously utilizing the consistency, coherence, and hierarchy of human anatomy as the supervision signal. Extensive experiments across 10 datasets evaluated through fine-tuning and emergent property analysis demonstrate Lamps' superior robustness, transferability, and clinical potential when compared to 10 baseline models. By learning from multiple perspectives, Lamps presents a unique opportunity for foundation models to develop meaningful, robust representations that are aligned with the structure of human anatomy.

</details>


### [91] [Let Samples Speak: Mitigating Spurious Correlation by Exploiting the Clusterness of Samples](https://arxiv.org/abs/2512.22874)
*Weiwei Li,Junzhuo Liu,Yuanyuan Ren,Yuchen Zheng,Yahao Liu,Wen Li*

Main category: cs.CV

TL;DR: 该论文提出了一种基于数据的方法，通过在特征空间中识别、消除和校正伪相关特征，提升深度学习模型的鲁棒性，并在多项基准任务上显著提高最差类别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖伪属性注释或基于经验假设筛选伪特征，但现实世界数据中的伪相关复杂难以完全捕捉，导致性能不理想。需要一种更自动、普适的方法降低深度模型中的伪相关影响。

Method: 作者观察到受伪特征影响的样本在特征空间中分布更分散，据此提出：1）识别被伪特征影响的样本；2）通过分组中和策略获得去偏特征表示；3）学习特征变换，消除伪特征并校正特征表示；4）用新特征更新分类器。形成识别—中和—消除—更新的完整去偏流程。

Result: 在图像和NLP去偏基准数据集上，所提方法在最差分组上相比标准ERM方法提升超过20%的准确率。

Conclusion: 作者提出的基于数据特征分布的伪相关消除流程效果显著，较现有方法具有更高的可推广性和实用性。

Abstract: Deep learning models are known to often learn features that spuriously correlate with the class label during training but are irrelevant to the prediction task. Existing methods typically address this issue by annotating potential spurious attributes, or filtering spurious features based on some empirical assumptions (e.g., simplicity of bias). However, these methods may yield unsatisfactory performance due to the intricate and elusive nature of spurious correlations in real-world data. In this paper, we propose a data-oriented approach to mitigate the spurious correlation in deep learning models. We observe that samples that are influenced by spurious features tend to exhibit a dispersed distribution in the learned feature space. This allows us to identify the presence of spurious features. Subsequently, we obtain a bias-invariant representation by neutralizing the spurious features based on a simple grouping strategy. Then, we learn a feature transformation to eliminate the spurious features by aligning with this bias-invariant representation. Finally, we update the classifier by incorporating the learned feature transformation and obtain an unbiased model. By integrating the aforementioned identifying, neutralizing, eliminating and updating procedures, we build an effective pipeline for mitigating spurious correlation. Experiments on image and NLP debiasing benchmarks show an improvement in worst group accuracy of more than 20% compared to standard empirical risk minimization (ERM). Codes and checkpoints are available at https://github.com/davelee-uestc/nsf_debiasing .

</details>


### [92] [M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models](https://arxiv.org/abs/2512.22877)
*Ju-Hsuan Weng,Jia-Wei Liao,Cheng-Fu Chou,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 该论文提出M-ErasureBench，多模态概念消除基准测试框架，并提出提升概念消除效果的新模块IRECE。测试发现，现有方法在文本提示下表现良好，但在嵌入和潜变量输入下效果很差。IRECE可以大幅提升消除效果而保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中可能产生有害或受版权保护的内容。现有概念消除方法只针对文本提示，忽略了嵌入和潜变量等其他模态，这些成为了攻击利用的漏洞。为应对这些新型威胁，需要多模态全面评估和更强健的消除方法。

Method: 作者提出M-ErasureBench，多模态（文本、嵌入、潜变量）消除评测框架，共五种评测场景（包含白盒黑盒）。发现传统方法在文本下表现好，在嵌入和潜变量下消除失败。为此，提出IRECE模块，通过跨注意力定位目标概念，并在去噪过程中扰动相关潜变量，实现推理时增强鲁棒性。

Result: 实验表明，传统方法在嵌入和潜变量场景下，目标概念再现率（CRR）高达90%以上，消除失败。IRECE在最具挑战的白盒潜变量场景下，将CRR降低40%，同时不损失图像质量。

Conclusion: M-ErasureBench是首个多模态系统性概念消除评测工具，IRECE显著提升不同模态消除效果。两者结合，推动扩散生成模型更安全、鲁棒及可控。

Abstract: Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.

</details>


### [93] [SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation](https://arxiv.org/abs/2512.22878)
*Hasan Faraz Khan,Noor Fatima,Muzammil Behzad*

Main category: cs.CV

TL;DR: 本文提出SwinTF3D模型，将视觉和语言信息融合，用于文本引导的三维医学图像分割，实现高效且具适应性的自动器官分割。


<details>
  <summary>Details</summary>
Motivation: 现有三维医学图像分割方法主要依赖大规模有标注数据进行视觉学习，缺乏对灵活用户需求和语义的理解，难以适应新任务和领域。缺失语义理解也限制了模型对自然语言分割指令的响应能力。

Method: SwinTF3D采用轻量级多模态融合方案，结合基于transformer的三维视觉编码器和紧凑的文本编码器，通过高效的融合机制，将文本提示的语义与医学体积图像的空间结构对齐，实现文本指导下的3D分割。

Result: 在BTCV公开数据集上的实验表明，SwinTF3D在多个器官分割任务上取得了有竞争力的Dice和IoU分数，并表现出优良的泛化能力。在模型规模较小的前提下，相较于传统transformer分割网络，计算效率大幅提升。

Conclusion: SwinTF3D实现了视觉感知和语言理解的融合，在三维医学图像领域展示了交互式、可解释、资源高效的文本驱动分割新模式，对临床图像处理的自适应和高效化具有推动作用。

Abstract: The recent integration of artificial intelligence into medical imaging has driven remarkable advances in automated organ segmentation. However, most existing 3D segmentation frameworks rely exclusively on visual learning from large annotated datasets restricting their adaptability to new domains and clinical tasks. The lack of semantic understanding in these models makes them ineffective in addressing flexible, user-defined segmentation objectives. To overcome these limitations, we propose SwinTF3D, a lightweight multimodal fusion approach that unifies visual and linguistic representations for text-guided 3D medical image segmentation. The model employs a transformer-based visual encoder to extract volumetric features and integrates them with a compact text encoder via an efficient fusion mechanism. This design allows the system to understand natural-language prompts and correctly align semantic cues with their corresponding spatial structures in medical volumes, while producing accurate, context-aware segmentation results with low computational overhead. Extensive experiments on the BTCV dataset demonstrate that SwinTF3D achieves competitive Dice and IoU scores across multiple organs, despite its compact architecture. The model generalizes well to unseen data and offers significant efficiency gains compared to conventional transformer-based segmentation networks. Bridging visual perception with linguistic understanding, SwinTF3D establishes a practical and interpretable paradigm for interactive, text-driven 3D medical image segmentation, opening perspectives for more adaptive and resource-efficient solutions in clinical imaging.

</details>


### [94] [Guided Path Sampling: Steering Diffusion Models Back on Track with Principled Path Guidance](https://arxiv.org/abs/2512.22881)
*Haosen Li,Wenshuo Chen,Shaofeng Liang,Lei Wang,Haozhe Jia,Yutao Yue*

Main category: cs.CV

TL;DR: 本文发现了传统Classifier-Free Guidance (CFG)方法与迭代细化（denoising-inversion cycle）结合时的根本局限性，并提出了一种新的引导路径采样（GPS）方法，有效提升了扩散模型的样本质量与语义一致性。


<details>
  <summary>Details</summary>
Motivation: 在扩散模型的生成和细化过程中，CFG常被用来提升图像与提示词的相关性。现有方法在迭代细化时存在精度发散、稳健性差等问题，影响了高质量生成与复杂语义对齐。研究动机在于解决CFG导致采样路径偏离数据流形、误差无法收敛的本质瓶颈。

Method: 提出了Guided Path Sampling (GPS)方法，用流形内的内插取代CFG的外推，确保采样路径受约束地保持在数据流形上，从而消除误差的放大。此外，设计了动态调节引导强度的最优调度策略，使语义注入更好地匹配模型的粗到细生成过程。

Result: 在SDXL和Hunyuan-DiT等主流扩散模型上，GPS相较于现有方法，在视觉质量与复杂提示词遵循度均显著提升（如：ImageReward 0.79，HPS v2 0.2995，GenEval准确率57.45%）。理论证明GPS可将误差从无界放大转为有界收敛。

Conclusion: 实验和理论均表明，路径稳定性是有效迭代细化的前提，GPS为稳定高效的迭代细化过程建立了新的鲁棒框架，值得在扩散模型改进中广泛采用。

Abstract: Iterative refinement methods based on a denoising-inversion cycle are powerful tools for enhancing the quality and control of diffusion models. However, their effectiveness is critically limited when combined with standard Classifier-Free Guidance (CFG). We identify a fundamental limitation: CFG's extrapolative nature systematically pushes the sampling path off the data manifold, causing the approximation error to diverge and undermining the refinement process. To address this, we propose Guided Path Sampling (GPS), a new paradigm for iterative refinement. GPS replaces unstable extrapolation with a principled, manifold-constrained interpolation, ensuring the sampling path remains on the data manifold. We theoretically prove that this correction transforms the error series from unbounded amplification to strictly bounded, guaranteeing stability. Furthermore, we devise an optimal scheduling strategy that dynamically adjusts guidance strength, aligning semantic injection with the model's natural coarse-to-fine generation process. Extensive experiments on modern backbones like SDXL and Hunyuan-DiT show that GPS outperforms existing methods in both perceptual quality and complex prompt adherence. For instance, GPS achieves a superior ImageReward of 0.79 and HPS v2 of 0.2995 on SDXL, while improving overall semantic alignment accuracy on GenEval to 57.45%. Our work establishes that path stability is a prerequisite for effective iterative refinement, and GPS provides a robust framework to achieve it.

</details>


### [95] [Hash Grid Feature Pruning](https://arxiv.org/abs/2512.22882)
*Yangzhi Ma,Bojun Liu,Jie Li,Li Li,Dong Liu*

Main category: cs.CV

TL;DR: 本文提出了一种针对高斯Splatting的哈希网格特征裁剪方法，有效减少哈希网格的存储和传输冗余，平均比特率降低8%。


<details>
  <summary>Details</summary>
Motivation: 由于高斯点在三维空间中分布稀疏且不均，导致哈希网格中存在大量无效特征，造成存储和传输的冗余。

Method: 提出通过输入的高斯点坐标识别无效特征，并将其从哈希网格中剪除，只编码有效特征。

Result: 在符合标准化委员会规定的通用测试条件下，该方法比基线方法平均减少了8%的比特率。

Conclusion: 该方法在不影响模型性能的前提下，有效提升了存储和码率效率，改善了率失真表现。

Abstract: Hash grids are widely used to learn an implicit neural field for Gaussian splatting, serving either as part of the entropy model or for inter-frame prediction. However, due to the irregular and non-uniform distribution of Gaussian splats in 3D space, numerous sparse regions exist, rendering many features in the hash grid invalid. This leads to redundant storage and transmission overhead. In this work, we propose a hash grid feature pruning method that identifies and prunes invalid features based on the coordinates of the input Gaussian splats, so that only the valid features are encoded. This approach reduces the storage size of the hash grid without compromising model performance, leading to improved rate-distortion performance. Following the Common Test Conditions (CTC) defined by the standardization committee, our method achieves an average bitrate reduction of 8% compared to the baseline approach.

</details>


### [96] [JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation](https://arxiv.org/abs/2512.22905)
*Kai Liu,Jungang Li,Yuchong Sun,Shengqiong Wu,Jianzhang Gao,Daoan Zhang,Wei Zhang,Sheng Jin,Sicheng Yu,Geng Zhan,Jiayi Ji,Fan Zhou,Liang Zheng,Shuicheng Yan,Hao Fei,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出了JavisGPT，是首个用于联合音视频理解与生成的一体化多模态大模型，采用创新的架构与训练流水线，在音视频时序同步理解和生成任务上取得了显著领先。


<details>
  <summary>Details</summary>
Motivation: 目前主流多模态大模型（MLLM）多聚焦于视觉与文本的数据，缺乏对音视频联合理解与生成的能力，尤其在时间同步性和复杂应用（如对话、生成等）场景下表现不足，因此亟需能统一处理和生成音视频内容的强大模型。

Method: 提出了编码器-LLM-解码器架构，并设计SyncFusion模块实现音视频时空融合，通过可学习的同步感知查询连接预训练的音视频生成器JAV-DiT。模型采用三阶段训练流程：多模态预训练、音视频微调、大规模指令微调。此外，构建了高质量的JavisInst-Omni指令数据集（20万+条）、覆盖多样复杂的理解与生成场景。

Result: 在联合音视频理解与生成的多个基准任务上，JavisGPT相较已有多模态大模型表现更优，尤其在处理时间同步性高、复杂度高的场景下具有显著优势。

Conclusion: JavisGPT首次实现了端到端的音视频内容联合理解和生成，推动多模态大模型从视觉-文本向丰富的音视频场景拓展，具有广阔的应用前景。

Abstract: This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.

</details>


### [97] [ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving](https://arxiv.org/abs/2512.22939)
*Qihang Peng,Xuesong Chen,Chenye Yang,Shaoshuai Shi,Hongsheng Li*

Main category: cs.CV

TL;DR: 提出了ColaVLA，一种统一的视觉-语言-动作模型框架，实现高效、安全且高性能的自动驾驶轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型（VLM）的自动驾驶系统在跨模态推理和常识引入上有新进展，但仍存在文本推理与连续控制不匹配、推理延迟高、实时性差等三大难题，限制了在实际应用中的部署。

Method: 提出ColaVLA框架，将文本推理转化为统一的潜在空间，并采用分层并行轨迹解码器。具体包括：利用Cognitive Latent Reasoner模块，通过自适应选择和少量VLM前向推理，将场景理解压缩为面向决策的元动作嵌入；然后由分层并行规划器一次前向推理生成多尺度、因果一致的轨迹。

Result: 在nuScenes基准测试中，ColaVLA在开放环与闭环两种场景下，均取得了最新的最佳性能，且具有效率高和鲁棒性好的特点。

Conclusion: ColaVLA兼顾了VLM的泛化能力和可解释性，同时实现了高效、准确与安全的自动驾驶轨迹生成，是VLM驱动自动驾驶的有效解决方案。

Abstract: Autonomous driving requires generating safe and reliable trajectories from complex multimodal inputs. Traditional modular pipelines separate perception, prediction, and planning, while recent end-to-end (E2E) systems learn them jointly. Vision-language models (VLMs) further enrich this paradigm by introducing cross-modal priors and commonsense reasoning, yet current VLM-based planners face three key challenges: (i) a mismatch between discrete text reasoning and continuous control, (ii) high latency from autoregressive chain-of-thought decoding, and (iii) inefficient or non-causal planners that limit real-time deployment. We propose ColaVLA, a unified vision-language-action framework that transfers reasoning from text to a unified latent space and couples it with a hierarchical, parallel trajectory decoder. The Cognitive Latent Reasoner compresses scene understanding into compact, decision-oriented meta-action embeddings through ego-adaptive selection and only two VLM forward passes. The Hierarchical Parallel Planner then generates multi-scale, causality-consistent trajectories in a single forward pass. Together, these components preserve the generalization and interpretability of VLMs while enabling efficient, accurate and safe trajectory generation. Experiments on the nuScenes benchmark show that ColaVLA achieves state-of-the-art performance in both open-loop and closed-loop settings with favorable efficiency and robustness.

</details>


### [98] [Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects](https://arxiv.org/abs/2512.22949)
*Zhicheng Zhao,Xuanang Fan,Lingma Sun,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出了新的密集区采矿网络（DRMNet），通过密度图指导自适应特征学习，提升高分辨率遥感图像中密集小目标的检测表现。


<details>
  <summary>Details</summary>
Motivation: 高分辨率遥感图像中的小目标通常密集分布且相互遮挡严重，现有方法无法有效聚焦于这些高密度区域，导致检测性能受限。

Method: 提出了DRMNet，其中包括：1）密度生成分支（DGB）生成可量化的密度先验，2）密集区域聚焦模块（DAFM）利用密度图高效聚焦于稠密区域，促进局部与全局特征交互，3）双滤波融合模块（DFFM）用离散余弦变换分离高低频特征，并通过密度引导的跨注意力机制提升互补性，抑制背景干扰。

Result: 在AI-TOD和DTOD两个数据集上的大量实验表明，DRMNet在目标高度密集和严重遮挡的复杂场景下检测性能超过当前最先进方法。

Conclusion: DRMNet通过引入密度图先验和自适应特征学习，有效提升了密集小目标检测能力，具有实际应用潜力。

Abstract: High-resolution remote sensing imagery increasingly contains dense clusters of tiny objects, the detection of which is extremely challenging due to severe mutual occlusion and limited pixel footprints. Existing detection methods typically allocate computational resources uniformly, failing to adaptively focus on these density-concentrated regions, which hinders feature learning effectiveness. To address these limitations, we propose the Dense Region Mining Network (DRMNet), which leverages density maps as explicit spatial priors to guide adaptive feature learning. First, we design a Density Generation Branch (DGB) to model object distribution patterns, providing quantifiable priors that guide the network toward dense regions. Second, to address the computational bottleneck of global attention, our Dense Area Focusing Module (DAFM) uses these density maps to identify and focus on dense areas, enabling efficient local-global feature interaction. Finally, to mitigate feature degradation during hierarchical extraction, we introduce a Dual Filter Fusion Module (DFFM). It disentangles multi-scale features into high- and low-frequency components using a discrete cosine transform and then performs density-guided cross-attention to enhance complementarity while suppressing background interference. Extensive experiments on the AI-TOD and DTOD datasets demonstrate that DRMNet surpasses state-of-the-art methods, particularly in complex scenarios with high object density and severe occlusion.

</details>


### [99] [CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision](https://arxiv.org/abs/2512.22969)
*Behnam Raoufi,Hossein Sharify,Mohamad Mahdee Ramezanee,Khosrow Hajsadeghi,Saeed Bagheri Shouraki*

Main category: cs.CV

TL;DR: 本文提出了一种新的目标检测方法，将CLIP对比语言-视觉监督整合到目标检测框架中，显著提升检测性能，尤其在多种架构和数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测器依赖交叉熵分类，容易受到类别不平衡和标签噪声的影响。为了解决这些问题，本文尝试引入对比学习和多模态信息（视觉-文本）进行监督。

Method: 提出CLIP-Joint-Detect框架，引入一个轻量级并行头，将区域或网格特征投影到CLIP嵌入空间，并通过InfoNCE对比损失和辅助交叉熵损失，与可学习的类别文本嵌入对齐。在此基础上，所有标准检测损失同时被优化。该方法可无缝适用于两阶段和一阶段目标检测架构。

Result: 在Pascal VOC 2007+2012（配合Faster R-CNN）和MS COCO 2017（配合YOLOv11）数据集上进行了验证。实验显示该方法在保持实时检测速度的情况下，能带来持续且可观的性能提升。消融实验进一步证明了可学习文本嵌入和联合优化对闭集检测性能的增强。

Conclusion: CLIP-Joint-Detect利用可学习的文本嵌入和对比监督方式，提升了现有目标检测器的鲁棒性和检测性能，为不同类型的框架带来了显著收益。

Abstract: Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and aligns them with learnable class-specific text embeddings via InfoNCE contrastive loss and an auxiliary cross-entropy term, while all standard detection losses are optimized simultaneously. The approach applies seamlessly to both two-stage and one-stage architectures. We validate it on Pascal VOC 2007+2012 using Faster R-CNN and on the large-scale MS COCO 2017 benchmark using modern YOLO detectors (YOLOv11), achieving consistent and substantial improvements while preserving real-time inference speed. Extensive experiments and ablations demonstrate that joint optimization with learnable text embeddings markedly enhances closed-set detection performance across diverse architectures and datasets.

</details>


### [100] [Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection](https://arxiv.org/abs/2512.22972)
*Runwei Guan,Jianan Liu,Shaofeng Liang,Fangqiang Ding,Shanliang Yao,Xiaokai Bai,Daizong Liu,Tao Huang,Guoqiang Mao,Hui Xiong*

Main category: cs.CV

TL;DR: 本文提出WRCFormer框架，通过多视角融合原始4D毫米波雷达及相机数据，实现更优的3D目标检测效果，提升在各种天气条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前4D毫米波雷达在自动驾驶中的应用受限于稀疏性和语义丰富度不足，点云处理易丢失信息，而直接处理原始数据计算成本高，亟需高效且高性能的多模态融合方法。

Method: 提出WRCFormer，结合分解的雷达立方体多视图表示与摄像头数据，通过基于小波注意力的特征金字塔增强稀疏雷达和图像特征，采用基于几何引导的渐进融合机制高效集成多模态信息。

Result: WRCFormer在K-Radar基准上实现了最先进的性能，整体精度提升约2.4%，在冻雨场景下也领先1.6%，验证了其在恶劣天气下的有效性和鲁棒性。

Conclusion: 多视图、基于小波注意力及渐进融合的设计能有效提升4D毫米波雷达与视觉信息的融合表现，为自动驾驶中的3D目标检测提供了性能更佳、适用性更强的技术途径。

Abstract: 4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.

</details>


### [101] [YOLO-IOD: Towards Real Time Incremental Object Detection](https://arxiv.org/abs/2512.22973)
*Shizhou Zhang,Xueqiang Lv,Yinghui Xing,Qirui Wu,Di Xu,Chen Zhao,Yanning Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLO的增量式目标检测（IOD）框架YOLO-IOD，通过对知识冲突问题的深入分析并提出三种核心技术，有效缓解传统YOLO在增量学习过程中的遗忘问题，并在真实分布数据集LoCo COCO和常规基准上都取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有增量式目标检测方法主要基于Faster R-CNN或DETR等检测器，尚未能很好支持实时性的YOLO框架。YOLO由于架构与增量学习机制的冲突，容易发生灾难性遗忘，因此亟需专门面向YOLO的增量检测方案。

Method: 作者提出YOLO-IOD框架，核心包括：1）面向冲突的伪标签优化（CPR），利用置信度提升伪标签质量并预判未来任务相关目标，缓解前景-背景混淆；2）基于重要性的卷积核选择（IKS），挑选并动态更新当前关键任务相关的卷积核；3）跨阶段非对称知识蒸馏（CAKD），通过前后教师检测头非对称蒸馏优化新旧类别知识融合。此外，提出真实、无数据泄露的基准数据集LoCo COCO。

Result: YOLO-IOD在常规及LoCo COCO基准上的实验表明，其在减少遗忘的同时，兼具极低的参数开销，实现了对新类别的高效增量学习，整体检测性能优于现有方法。

Conclusion: YOLO-IOD为YOLO系列模型在增量目标检测领域提供了有效、高效且适应实时需求的解决办法，为未来YOLO及其它实时检测器的持续学习应用奠定了基础。

Abstract: Current methods for incremental object detection (IOD) primarily rely on Faster R-CNN or DETR series detectors; however, these approaches do not accommodate the real-time YOLO detection frameworks. In this paper, we first identify three primary types of knowledge conflicts that contribute to catastrophic forgetting in YOLO-based incremental detectors: foreground-background confusion, parameter interference, and misaligned knowledge distillation. Subsequently, we introduce YOLO-IOD, a real-time Incremental Object Detection (IOD) framework that is constructed upon the pretrained YOLO-World model, facilitating incremental learning via a stage-wise parameter-efficient fine-tuning process. Specifically, YOLO-IOD encompasses three principal components: 1) Conflict-Aware Pseudo-Label Refinement (CPR), which mitigates the foreground-background confusion by leveraging the confidence levels of pseudo labels and identifying potential objects relevant to future tasks. 2) Importancebased Kernel Selection (IKS), which identifies and updates the pivotal convolution kernels pertinent to the current task during the current learning stage. 3) Cross-Stage Asymmetric Knowledge Distillation (CAKD), which addresses the misaligned knowledge distillation conflict by transmitting the features of the student target detector through the detection heads of both the previous and current teacher detectors, thereby facilitating asymmetric distillation between existing and newly introduced categories. We further introduce LoCo COCO, a more realistic benchmark that eliminates data leakage across stages. Experiments on both conventional and LoCo COCO benchmarks show that YOLO-IOD achieves superior performance with minimal forgetting.

</details>


### [102] [RealCamo: Boosting Real Camouflage Synthesis with Layout Controls and Textual-Visual Guidance](https://arxiv.org/abs/2512.22974)
*Chunyuan Chen,Yunuo Cai,Shujuan Li,Weiyun Liang,Bin Wang,Jing Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为ReamCamo的新型伪装图像生成框架，有效提升了生成图像的真实感和伪装质量，并通过多模态条件与新的评价指标验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的伪装图像生成方法与真实伪装图像之间存在较大差距，主要体现在视觉相似性不足或背景与前景语义不一致，影响了伪装目标检测等任务对高质量数据的需求。

Method: 作者提出ReamCamo框架，其核心是基于图像外延生成（out-painting）的统一方案，通过显式的布局控制来协调生成图像的整体结构，提升前景与背景的语义一致性。同时，方法结合了细粒度文本描述任务和纹理检索，建立了多模态文本-视觉条件，引导生成过程提升真实感。此外，提出了前后景分布差异度指标，定量评估生成图像的伪装质量。

Result: 在多项实验和可视化对比中，ReamCamo方法在生成图像的伪装质量、背景前景融合和视觉真实性上均优于现有方法，相关度评价指标取得了良好表现。

Conclusion: ReamCamo为伪装图像生成提供了一种有效的统一框架，不仅提升了训练数据的质量，也为伪装目标检测等下游任务带来潜在收益，具有广泛的应用前景。

Abstract: Camouflaged image generation (CIG) has recently emerged as an efficient alternative for acquiring high-quality training data for camouflaged object detection (COD). However, existing CIG methods still suffer from a substantial gap to real camouflaged imagery: generated images either lack sufficient camouflage due to weak visual similarity, or exhibit cluttered backgrounds that are semantically inconsistent with foreground targets. To address these limitations, we propose ReamCamo, a unified out-painting based framework for realistic camouflaged image generation. ReamCamo explicitly introduces additional layout controls to regulate global image structure, thereby improving semantic coherence between foreground objects and generated backgrounds. Moreover, we construct a multi-modal textual-visual condition by combining a unified fine-grained textual task description with texture-oriented background retrieval, which jointly guides the generation process to enhance visual fidelity and realism. To quantitatively assess camouflage quality, we further introduce a background-foreground distribution divergence metric that measures the effectiveness of camouflage in generated images. Extensive experiments and visualizations demonstrate the effectiveness of our proposed framework.

</details>


### [103] [PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects](https://arxiv.org/abs/2512.22979)
*Huiming Yang,Linglin Liao,Fei Ding,Sibo Wang,Zijian Zeng*

Main category: cs.CV

TL;DR: 本文提出了PoseStreamer框架，专门针对高速运动场景下新颖物体的6DoF姿态估计任务，显著提升了鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在高速和低光场景下，常规RGB相机易受运动模糊影响，导致6DoF物体姿态估计性能下降。虽然事件相机拥有高时间分辨率，但现有方法在高速运动物体情况下表现仍不理想。因此，亟需一个适用于高速动态场景的鲁棒姿态估计方法。

Method: 提出了多模态的PoseStreamer框架，包含三个核心模块：（1）自适应姿态记忆队列，用于利用历史姿态信息保持时序一致性；（2）以物体为中心的2D跟踪器，为3D姿态估计提供强2D先验；（3）射线几何优化器，用于在相机射线方向上精细化姿态估计。此外，构建了一个专门用于高速运动场景的多模态MoCapCube6D数据集。

Result: 实验表明，PoseStreamer在高速运动物体场景下精度显著优于现有方法，并在未知新物体上表现出较强泛化能力。

Conclusion: PoseStreamer框架有效解决了高速场景下的新颖物体6DoF姿态估计问题，展示了模板无关的强泛化潜力和鲁棒性。

Abstract: Six degree of freedom (6DoF) pose estimation for novel objects is a critical task in computer vision, yet it faces significant challenges in high-speed and low-light scenarios where standard RGB cameras suffer from motion blur. While event cameras offer a promising solution due to their high temporal resolution, current 6DoF pose estimation methods typically yield suboptimal performance in high-speed object moving scenarios. To address this gap, we propose PoseStreamer, a robust multi-modal 6DoF pose estimation framework designed specifically on high-speed moving scenarios. Our approach integrates three core components: an Adaptive Pose Memory Queue that utilizes historical orientation cues for temporal consistency, an Object-centric 2D Tracker that provides strong 2D priors to boost 3D center recall, and a Ray Pose Filter for geometric refinement along camera rays. Furthermore, we introduce MoCapCube6D, a novel multi-modal dataset constructed to benchmark performance under rapid motion. Extensive experiments demonstrate that PoseStreamer not only achieves superior accuracy in high-speed moving scenarios, but also exhibits strong generalizability as a template-free framework for unseen moving objects.

</details>


### [104] [Spatial-aware Symmetric Alignment for Text-guided Medical Image Segmentation](https://arxiv.org/abs/2512.22981)
*Linglin Liao,Qichuan Geng,Yu Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为空间感知对称对齐（SSA）的新型文本引导医学图像分割方法，在公开数据集上实现了目前最优表现，特别擅长空间关系约束下的病灶分割。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本引导的医学图像分割方法难以同时处理包含定位、描述和诊断的混合医学文本，导致对病灶位置和影像区域的关联能力不足；并且对空间关系的处理不到位，出现关键偏差。

Method: 提出SSA框架，采用对称最优传输对齐机制，加强影像区域与多重表达之间的关联；同时设计区域级复合方向引导策略，通过构建引导掩膜显式引入文本中的空间约束，实现细粒度多模态对应。

Result: 在公开医学图像分割基准上，SSA方法在空间关系受限病灶分割任务中表现突出，整体性能优于现有方法，取得SOTA成绩。

Conclusion: SSA框架有效提升了文本引导下医学影像分割的空间感知能力和多模态文本理解能力，有望提升临床自动化诊断中基于复杂文本指导的分割效果。

Abstract: Text-guided Medical Image Segmentation has shown considerable promise for medical image segmentation, with rich clinical text serving as an effective supplement for scarce data. However, current methods have two key bottlenecks. On one hand, they struggle to process diagnostic and descriptive texts simultaneously, making it difficult to identify lesions and establish associations with image regions. On the other hand, existing approaches focus on lesions description and fail to capture positional constraints, leading to critical deviations. Specifically, with the text "in the left lower lung", the segmentation results may incorrectly cover both sides of the lung. To address the limitations, we propose the Spatial-aware Symmetric Alignment (SSA) framework to enhance the capacity of referring hybrid medical texts consisting of locational, descriptive, and diagnostic information. Specifically, we propose symmetric optimal transport alignment mechanism to strengthen the associations between image regions and multiple relevant expressions, which establishes bi-directional fine-grained multimodal correspondences. In addition, we devise a composite directional guidance strategy that explicitly introduces spatial constraints in the text by constructing region-level guidance masks. Extensive experiments on public benchmarks demonstrate that SSA achieves state-of-the-art (SOTA) performance, particularly in accurately segmenting lesions characterized by spatial relational constraints.

</details>


### [105] [Reverse Personalization](https://arxiv.org/abs/2512.22984)
*Han-Wei Kung,Tuomas Varanka,Nicu Sebe*

Main category: cs.CV

TL;DR: 本论文提出了一种基于扩散模型的可控面部匿名化方法，既能有效去除身份信息，又能保留面部属性和高图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本提示的身份信息移除或更改方法往往依赖于目标身份在预训练模型中有良好表达，或者需针对特定身份微调模型，局限性明显。本文旨在突破这一限制，实现对任意面部图像的高质量、可控匿名化。

Method: 分析了扩散模型中的面部身份生成过程，引入了逆个性化框架，通过条件扩散逆转技术直接编辑图片而无需借助文本提示。同时结合了身份引导的条件分支，使方法能泛化到训练数据之外的陌生身份，并支持对面部属性的自定义控制。

Result: 实验表明，本文方法在身份去除、属性保留和图像质量三方面取得了优异的平衡，优于现有匿名化方法，具有可控性强和泛化能力好的特点。

Conclusion: 本文提出的逆个性化扩散匿名化框架显著提升了面部匿名化的控制力和实用性，为个性化隐私保护提供了新方案。

Abstract: Recent text-to-image diffusion models have demonstrated remarkable generation of realistic facial images conditioned on textual prompts and human identities, enabling creating personalized facial imagery. However, existing prompt-based methods for removing or modifying identity-specific features rely either on the subject being well-represented in the pre-trained model or require model fine-tuning for specific identities. In this work, we analyze the identity generation process and introduce a reverse personalization framework for face anonymization. Our approach leverages conditional diffusion inversion, allowing direct manipulation of images without using text prompts. To generalize beyond subjects in the model's training data, we incorporate an identity-guided conditioning branch. Unlike prior anonymization methods, which lack control over facial attributes, our framework supports attribute-controllable anonymization. We demonstrate that our method achieves a state-of-the-art balance between identity removal, attribute preservation, and image quality. Source code and data are available at https://github.com/hanweikung/reverse-personalization .

</details>


### [106] [A Low-Cost UAV Deep Learning Pipeline for Integrated Apple Disease Diagnosis,Freshness Assessment, and Fruit Detection](https://arxiv.org/abs/2512.22990)
*Soham Dutta,Soham Banerjee,Sneha Mahata,Anindya Sen,Sayantani Datta*

Main category: cs.CV

TL;DR: 该论文提出了一套基于低成本RGB无人机的果园智能管理系统，实现苹果叶片病害检测、果实新鲜度评估和产量估算，替代昂贵的多光谱方案。


<details>
  <summary>Details</summary>
Motivation: 目前果园无人机病害检测、品质评估和产量估算大多独立进行，且多依赖高成本多光谱传感器，难以普及至小型果园或资源有限地区。

Method: 本文基于ResNet50实现叶片病害检测，VGG16用于苹果新鲜度判别，YOLOv8实现苹果实时检测和定位。全部模型均在ESP32-CAM和树莓派等低成本硬件上本地运行，无需云端支持。

Result: 系统在叶片病害分类精度98.9%、新鲜度分类精度97.4%、苹果检测F1值为0.857，表现优异。

Conclusion: 该系统为多功能果园智能管理提供了兼顾高精度与低成本的解决方案，具有良好的推广应用前景，尤其适合资源有限的实际生产场景。

Abstract: Apple orchards require timely disease detection, fruit quality assessment, and yield estimation, yet existing UAV-based systems address such tasks in isolation and often rely on costly multispectral sensors. This paper presents a unified, low-cost RGB-only UAV-based orchard intelligent pipeline integrating ResNet50 for leaf disease detection, VGG 16 for apple freshness determination, and YOLOv8 for real-time apple detection and localization. The system runs on an ESP32-CAM and Raspberry Pi, providing fully offline on-site inference without cloud support. Experiments demonstrate 98.9% accuracy for leaf disease classification, 97.4% accuracy for freshness classification, and 0.857 F1 score for apple detection. The framework provides an accessible and scalable alternative to multispectral UAV solutions, supporting practical precision agriculture on affordable hardware.

</details>


### [107] [OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding](https://arxiv.org/abs/2512.23020)
*Wenyuan Huang,Zhao Wang,Zhou Wei,Ting Huang,Fang Zhao,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: 论文提出了OpenGround，一个用于开放世界3D视觉指代的zero-shot框架，突破了传统依赖预定义对象查找表（OLT）的限制，通过主动认知推理模块扩展VLM认知，实现了对未知目标的精准定位。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉指代方法依赖预设的对象查找表，限制其在遇到未定义或新奇目标时的适用性。为实现真正开放世界的3D视觉指代，急需一种无需依赖静态查找表的方法。

Method: 提出了OpenGround框架，核心为主动认知推理（ACR）模块。ACR通过认知任务链进行类似人类的目标感知，并动态更新对象查找表（OLT），从而拓展VLM的推理对象。方法还建立了OpenTarget数据集，包含7000多个对象-描述对，用于评估该方法在开放世界场景下的表现。

Result: 实验表明，OpenGround在Nr3D数据集上效果优异，在ScanRefer数据集上达到了SOTA，在OpenTarget数据集上相较其他方法提升了17.6%。

Conclusion: OpenGround突破了传统3D视觉指代依赖静态查找表的局限，实现了对开放世界新目标的指代定位。该方法对开放世界3D视觉理解有重要推动意义。

Abstract: 3D visual grounding aims to locate objects based on natural language descriptions in 3D scenes. Existing methods rely on a pre-defined Object Lookup Table (OLT) to query Visual Language Models (VLMs) for reasoning about object locations, which limits the applications in scenarios with undefined or unforeseen targets. To address this problem, we present OpenGround, a novel zero-shot framework for open-world 3D visual grounding. Central to OpenGround is the Active Cognition-based Reasoning (ACR) module, which is designed to overcome the fundamental limitation of pre-defined OLTs by progressively augmenting the cognitive scope of VLMs. The ACR module performs human-like perception of the target via a cognitive task chain and actively reasons about contextually relevant objects, thereby extending VLM cognition through a dynamically updated OLT. This allows OpenGround to function with both pre-defined and open-world categories. We also propose a new dataset named OpenTarget, which contains over 7000 object-description pairs to evaluate our method in open-world scenarios. Extensive experiments demonstrate that OpenGround achieves competitive performance on Nr3D, state-of-the-art on ScanRefer, and delivers a substantial 17.6% improvement on OpenTarget. Project Page at [this https URL](https://why-102.github.io/openground.io/).

</details>


### [108] [With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs](https://arxiv.org/abs/2512.23024)
*Ciprian Constantinescu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 本文提出一种新颖的上下文对象分类框架，通过构建地理-语义上下文图（GSCG），显式融入物体间的空间及语义关系，显著提升了目标识别的准确率，并优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前计算机视觉对象识别方法大多忽视场景中丰富的上下文信息，仅依赖孤立的图像区域，导致分类效果有限。为弥补与人类视觉系统在场景理解上的差距，作者提出利用显式上下文信息提升模型性能。

Method: 方法上，作者提出从单张单目图像中建立Geo-Semantic Contextual Graph（GSCG），结合深度估计和全景+材质分割，节点表示物体及其属性，边表示其空间关系。采用基于图的分类器，综合目标自身、邻域及全局特征进行分类。

Result: 在COCO 2017数据集上，所提方法显著优于无上下文的模型（73.4% vs 38.4%），也超过ResNet等强基线（最大53.5%）及多模态大模型Llama 4 Scout（最大42.3%）。

Conclusion: 显式结构化并可解释的上下文建模大幅提升了对象识别准确率，为场景感知提供了更有效的解决思路。

Abstract: Humans effortlessly identify objects by leveraging a rich understanding of the surrounding scene, including spatial relationships, material properties, and the co-occurrence of other objects. In contrast, most computational object recognition systems operate on isolated image regions, devoid of meaning in isolation, thus ignoring this vital contextual information. This paper argues for the critical role of context and introduces a novel framework for contextual object classification. We first construct a Geo-Semantic Contextual Graph (GSCG) from a single monocular image. This rich, structured representation is built by integrating a metric depth estimator with a unified panoptic and material segmentation model. The GSCG encodes objects as nodes with detailed geometric, chromatic, and material attributes, and their spatial relationships as edges. This explicit graph structure makes the model's reasoning process inherently interpretable. We then propose a specialized graph-based classifier that aggregates features from a target object, its immediate neighbors, and the global scene context to predict its class. Through extensive ablation studies, we demonstrate that our context-aware model achieves a classification accuracy of 73.4%, dramatically outperforming context-agnostic versions (as low as 38.4%). Furthermore, our GSCG-based approach significantly surpasses strong baselines, including fine-tuned ResNet models (max 53.5%) and a state-of-the-art multimodal Large Language Model (LLM), Llama 4 Scout, which, even when given the full image alongside a detailed description of objects, maxes out at 42.3%. These results on COCO 2017 train/val splits highlight the superiority of explicitly structured and interpretable context for object recognition tasks.

</details>


### [109] [An Architecture-Led Hybrid Report on Body Language Detection Project](https://arxiv.org/abs/2512.23028)
*Thomson Tong,Diba Darooneh*

Main category: cs.CV

TL;DR: 本文分析了两种最新视觉-语言模型（Qwen2.5-VL-7B-Instruct 和 Llama-4-Scout-17B-16E-Instruct）在视频体态检测系统中的架构和应用表现，总结了多模态模型基础及系统设计要点。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLM）在处理视频到结构化信息的任务中日益重要，但如何选择和应用不同架构、保障解析结果结构和语义正确性，是工程实践中的核心难题。作者旨在通过系统性分析，为选型和接口设计提供理论依据。

Method: 作者结合 BodyLanguageDetection 项目，将两种主流 VLM 的架构特性与具体视频到人工制品流水线（如情绪自动标注）流程相对应，重点分析了模型基础（视觉分词、Transformer注意力、指令跟随），输出结构校验机制，以及模型与接口的实际交互方式和限制。

Result: 分析发现，这些模型的结构化输出往往能保证语法正确，但不一定语义正确；schema 校验只保证结构合理，不涉及几何正确性；人脸识别等属性在当前接口下只保证帧内唯一性，且部分模式仅返回自由文本不为强结构。

Conclusion: 理解 VLM 架构特性与系统接口的映射，对撰写系统声明、设计健壮交互接口、规划模型评估标准都具备关键作用。为实际工程落地和科学评估提供了依据和参考。

Abstract: This report provides an architecture-led analysis of two modern vision-language models (VLMs), Qwen2.5-VL-7B-Instruct and Llama-4-Scout-17B-16E-Instruct, and explains how their architectural properties map to a practical video-to-artifact pipeline implemented in the BodyLanguageDetection repository [1]. The system samples video frames, prompts a VLM to detect visible people and generate pixel-space bounding boxes with prompt-conditioned attributes (emotion by default), validates output structure using a predefined schema, and optionally renders an annotated video. We first summarize the shared multimodal foundation (visual tokenization, Transformer attention, and instruction following), then describe each architecture at a level sufficient to justify engineering choices without speculative internals. Finally, we connect model behavior to system constraints: structured outputs can be syntactically valid while semantically incorrect, schema validation is structural (not geometric correctness), person identifiers are frame-local in the current prompting contract, and interactive single-frame analysis returns free-form text rather than schema-enforced JSON. These distinctions are critical for writing defensible claims, designing robust interfaces, and planning evaluation.

</details>


### [110] [Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion](https://arxiv.org/abs/2512.23035)
*Yi Zhou,Xuechao Zou,Shun Zhang,Kai Li,Shiying Wang,Jingming Chen,Congyan Lang,Tengfei Cao,Pin Tao,Yuanchun Shi*

Main category: cs.CV

TL;DR: 本文提出了一种利用预训练视觉-语言模型与自监督模型相结合的半监督遥感图像分割方法，通过创新的双学生架构与特征融合机制，有效缓解了伪标签漂移问题，并在多个公开数据集上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 遥感图像的语义分割通常依赖大量人工标注，成本高且耗时。半监督方法虽然减少标注需要，但容易受到伪标签漂移的影响（即模型自身错误加剧扩散），因此需要开发更稳定且高效的半监督分割框架。

Method: 提出了Co2S框架：采用异构双学生结构，两学生模型分别使用ViT结构并初始化于CLIP（视觉-语言模型）和DINOv3（自监督视觉模型）；引入显式-隐式语义协同引导机制，利用文本嵌入和可学习查询分别实现类别显式和隐式引导；设计全局-局部特征融合策略，将CLIP捕捉的全局上下文与DINOv3提取的局部细节有效融合，提高分割精度。

Result: 在六个主流遥感数据集上进行实验，所提出方法在各种划分协议和场景下均取得了领先的分割性能。

Conclusion: 通过融合不同类型先验信息与创新结构设计，有效提升了半监督遥感语义分割的稳定性与精度，为实际遥感应用提供了更实用的解决方案。

Abstract: Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.

</details>


### [111] [3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds](https://arxiv.org/abs/2512.23042)
*Ryousuke Yamada,Kohsuke Ide,Yoshihiro Fukuhara,Hirokatsu Kataoka,Gilles Puy,Andrei Bursuc,Yuki M. Asano*

Main category: cs.CV

TL;DR: 本研究提出了一种无需真实3D传感器、仅依赖无标签视频学习3D表示的新方法LAM3C，并发布了基于视频生成点云的大规模室内数据集RoomTours，该方法在多项3D场景理解任务上超越现有自监督方法。


<details>
  <summary>Details</summary>
Motivation: 获取大规模3D场景扫描数据非常昂贵且耗时，因此作者探索是否可利用丰富的无标签视频资源进行3D自监督学习，减少对真实3D数据采集的依赖。

Method: 1. 提出LAM3C方法：结合拉普拉斯感知的多层次3D聚类和Sinkhorn-Knopp分配算法，实现从视频生成点云的自监督3D特征学习。2. 发布RoomTours数据集：通过爬取网络上的室内漫游视频，利用重建模型生成大规模点云数据。3. 引入噪声正则化损失，增强几何表达的平滑性和稳健性。

Result: 无需任何真实3D扫描数据，LAM3C在室内3D点云的语义分割和实例分割任务上取得了优于已有自监督方法的表现。

Conclusion: 依赖视频自动生成的大规模点云数据和高效的自监督学习框架，能够显著提升3D理解模型性能，说明无标签视频可作为强大、廉价的3D学习数据源。

Abstract: Despite recent progress in 3D self-supervised learning, collecting large-scale 3D scene scans remains expensive and labor-intensive. In this work, we investigate whether 3D representations can be learned from unlabeled videos recorded without any real 3D sensors. We present Laplacian-Aware Multi-level 3D Clustering with Sinkhorn-Knopp (LAM3C), a self-supervised framework that learns from video-generated point clouds from unlabeled videos. We first introduce RoomTours, a video-generated point cloud dataset constructed by collecting room-walkthrough videos from the web (e.g., real-estate tours) and generating 49,219 scenes using an off-the-shelf feed-forward reconstruction model. We also propose a noise-regularized loss that stabilizes representation learning by enforcing local geometric smoothness and ensuring feature stability under noisy point clouds. Remarkably, without using any real 3D scans, LAM3C achieves higher performance than the previous self-supervised methods on indoor semantic and instance segmentation. These results suggest that unlabeled videos represent an abundant source of data for 3D self-supervised learning.

</details>


### [112] [Video-BrowseComp: Benchmarking Agentic Video Research on Open Web](https://arxiv.org/abs/2512.23044)
*Zhengyang Liang,Yan Shu,Xiangrui Liu,Minghao Qin,Kaixin Liang,Paolo Rota,Nicu Sebe,Zheng Liu,Lizi Liao*

Main category: cs.CV

TL;DR: 本文提出了Video-BrowseComp，是首个针对开放网络环境下代理式视频推理的基准数据集，旨在推动人工智能从被动视频感知向主动视频推理过渡。


<details>
  <summary>Details</summary>
Motivation: 以往多模态AI系统在视频理解方面主要关注于被动感知（模型处理已有、剪辑好的一段视频，不涉及外部检索），但现实中视频常需主动查询、时序证据验证和跨来源对比，现有基准无法测评这一能力，因此亟需新的评价框架。

Method: 作者构建了Video-BrowseComp基准，包含210个问题，专为评测AI在开放网络环境下，通过主动浏览视频时序、验证外部声明等复杂推理任务的能力。与以往纯文本或静态多模态不同，该基准必须依赖视频的时序视觉内容，而非简单文本检索。

Result: 在Video-BrowseComp上测试当前先进的大模型（如GPT-5.1 w/ Search等），准确率仅为15.24%。即使具备搜索功能的模型，大多数情况下仍主要依赖文本代理信息，只在元数据丰富领域表现较好，而在需强视觉推理的视频环境（如体育、游戏）上几乎失效。

Conclusion: Video-BrowseComp作为首个开放网络环境下、针对主动视频推理的评测基准，大幅提升了任务难度，有助于推动AI从被动视频理解向主动、深层次推理升级，对模型视觉与检索能力提出了更高要求，为未来多模态智能体的发展奠定基础。

Abstract: The evolution of autonomous agents is redefining information seeking, transitioning from passive retrieval to proactive, open-ended web research. However, while textual and static multimodal agents have seen rapid progress, a significant modality gap remains in processing the web's most dynamic modality: video. Existing video benchmarks predominantly focus on passive perception, feeding curated clips to models without requiring external retrieval. They fail to evaluate agentic video research, which necessitates actively interrogating video timelines, cross-referencing dispersed evidence, and verifying claims against the open web. To bridge this gap, we present \textbf{Video-BrowseComp}, a challenging benchmark comprising 210 questions tailored for open-web agentic video reasoning. Unlike prior benchmarks, Video-BrowseComp enforces a mandatory dependency on temporal visual evidence, ensuring that answers cannot be derived solely through text search but require navigating video timelines to verify external claims. Our evaluation of state-of-the-art models reveals a critical bottleneck: even advanced search-augmented models like GPT-5.1 (w/ Search) achieve only 15.24\% accuracy. Our analysis reveals that these models largely rely on textual proxies, excelling in metadata-rich domains (e.g., TV shows with plot summaries) but collapsing in metadata-sparse, dynamic environments (e.g., sports, gameplay) where visual grounding is essential. As the first open-web video research benchmark, Video-BrowseComp advances the field beyond passive perception toward proactive video reasoning.

</details>


### [113] [MedSAM-based lung masking for multi-label chest X-ray classification](https://arxiv.org/abs/2512.23089)
*Brayden Miao,Zain Rehman,Xin Miao,Siming Liu,Jianjie Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种利用MedSAM分割模型提升胸部X光分类准确性的框架，通过先分割肺部区域再进行多标签异常检测，在多个实验中分析了不同掩膜方式对识别效果及效率的影响。


<details>
  <summary>Details</summary>
Motivation: 自动解读胸部X光困难主要源于疾病信号弱、数据集偏见以及空间监督有限。医学影像分割基础模型（如MedSAM）能引入解剖学先验信息，潜在提升鲁棒性与可解释性，但其在实际胸片异常检测中的最佳应用方式尚不明确，因此需系统评估其作用。

Method: 作者将MedSAM模型微调用于肺部区域分割，作为特征提取的前置模块，并基于公开数据集训练/评估多标签分类网络，比较原图及不同分割掩膜（松/紧）的训练与效果差异，特别关注五种常见异常及正常情况的检测表现。

Result: MedSAM能生成解剖学合理的肺区掩膜。ResNet50在原始图像上整体区分异常最优，而松掩膜虽然略降总体性能，却大幅提升正常筛查（No Finding）。紧掩膜降低异常检测但提高训练效率，松掩膜可部分缓解因失去上下文带来的性能损失。掩膜的效果对任务类别和模型架构敏感。

Conclusion: 肺部掩膜应根据具体神经网络结构与临床目标灵活选择，不能一刀切地使用。合理控制空间先验（掩膜）有助于在异常检测和正常筛查间取得平衡，增强模型应用可控性。

Abstract: Chest X-ray (CXR) imaging is widely used for screening and diagnosing pulmonary abnormalities, yet automated interpretation remains challenging due to weak disease signals, dataset bias, and limited spatial supervision. Foundation models for medical image segmentation (MedSAM) provide an opportunity to introduce anatomically grounded priors that may improve robustness and interpretability in CXR analysis. We propose a segmentation-guided CXR classification pipeline that integrates MedSAM as a lung region extraction module prior to multi-label abnormality classification. MedSAM is fine-tuned using a public image-mask dataset from Airlangga University Hospital. We then apply it to a curated subset of the public NIH CXR dataset to train and evaluate deep convolutional neural networks for multi-label prediction of five abnormalities (Mass, Nodule, Pneumonia, Edema, and Fibrosis), with the normal case (No Finding) evaluated via a derived score. Experiments show that MedSAM produces anatomically plausible lung masks across diverse imaging conditions. We find that masking effects are both task-dependent and architecture-dependent. ResNet50 trained on original images achieves the strongest overall abnormality discrimination, while loose lung masking yields comparable macro AUROC but significantly improves No Finding discrimination, indicating a trade-off between abnormality-specific classification and normal case screening. Tight masking consistently reduces abnormality level performance but improves training efficiency. Loose masking partially mitigates this degradation by preserving perihilar and peripheral context. These results suggest that lung masking should be treated as a controllable spatial prior selected to match the backbone and clinical objective, rather than applied uniformly.

</details>


### [114] [PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion](https://arxiv.org/abs/2512.23130)
*Jian Wang,Sixing Rong,Jiarui Xing,Yuling Xu,Weide Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为PathoSyn的MRI图像合成生成框架，通过对成像-病理学特征进行解耦建模，实现高保真的病理影像合成。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在医学影像合成中经常导致解剖特征与病理特征纠缠，出现结构失真或不连续，同时现有方法难以捕捉病变的局部细节。

Method: PathoSyn将合成任务分解为解剖学结构的确定性重建与病理偏差的随机建模，采用Deviation-Space Diffusion Model对病理残差的条件分布建模，并集成无缝融合与推断时稳定化模块以抑制边界伪影。

Result: 实验结果表明，PathoSyn在肿瘤影像基准上无论是感知真实度还是解剖学准确性都明显优于整体扩散与掩码条件化基线方法。

Conclusion: PathoSyn为生成患者特异性高精度合成医学影像提供了理论与实践方案，有助于低数据场景下的诊断算法、疾病进展建模、个性化干预规划及临床决策支持系统的评测。

Abstract: We present PathoSyn, a unified generative framework for Magnetic Resonance Imaging (MRI) image synthesis that reformulates imaging-pathology as a disentangled additive deviation on a stable anatomical manifold. Current generative models typically operate in the global pixel domain or rely on binary masks, these paradigms often suffer from feature entanglement, leading to corrupted anatomical substrates or structural discontinuities. PathoSyn addresses these limitations by decomposing the synthesis task into deterministic anatomical reconstruction and stochastic deviation modeling. Central to our framework is a Deviation-Space Diffusion Model designed to learn the conditional distribution of pathological residuals, thereby capturing localized intensity variations while preserving global structural integrity by construction. To ensure spatial coherence, the diffusion process is coupled with a seam-aware fusion strategy and an inference-time stabilization module, which collectively suppress boundary artifacts and produce high-fidelity internal lesion heterogeneity. PathoSyn provides a mathematically principled pipeline for generating high-fidelity patient-specific synthetic datasets, facilitating the development of robust diagnostic algorithms in low-data regimes. By allowing interpretable counterfactual disease progression modeling, the framework supports precision intervention planning and provides a controlled environment for benchmarking clinical decision-support systems. Quantitative and qualitative evaluations on tumor imaging benchmarks demonstrate that PathoSyn significantly outperforms holistic diffusion and mask-conditioned baselines in both perceptual realism and anatomical fidelity. The source code of this work will be made publicly available.

</details>


### [115] [Domain-Shift Immunity in Deep Deformable Registration via Local Feature Representations](https://arxiv.org/abs/2512.23142)
*Mingzhen Shao,Sarang Joshi*

Main category: cs.CV

TL;DR: 这篇论文研究了深度学习方法在可变形图像配准中的鲁棒性问题，并提出了UniReg框架，实现了优秀的跨领域与多模态配准性能。


<details>
  <summary>Details</summary>
Motivation: 虽然基于深度学习的配准方法已超越传统方法，但其对领域偏移的敏感性长期未被深入探究，且主流提升鲁棒性的做法是扩大训练数据集。论文作者希望揭示鲁棒性的机制，并提出更本质的解决途径。

Method: 提出了UniReg框架，通过将特征提取与形变估计解耦，实现了用预训练的固定特征提取器和基于UNet的形变网络进行配准。该框架仅在单一数据集上训练，随后在不同领域和模态上进行测试。

Result: UniReg 在仅使用单一数据集训练的情况下，在跨领域和多模态任务中表现出与优化型方法相当的鲁棒性。分析发现，传统CNN方法因前层卷积受数据集偏置影响，在模态转移下效果较差。

Conclusion: 作者认为学习型配准方法的鲁棒性本质在于对局部特征的依赖，并非大规模多样数据训练。建议未来骨干网络设计应强化领域不变的局部特征表达，从而提升鲁棒性。

Abstract: Deep learning has advanced deformable image registration, surpassing traditional optimization-based methods in both accuracy and efficiency. However, learning-based models are widely believed to be sensitive to domain shift, with robustness typically pursued through large and diverse training datasets, without explaining the underlying mechanisms. In this work, we show that domain-shift immunity is an inherent property of deep deformable registration models, arising from their reliance on local feature representations rather than global appearance for deformation estimation. To isolate and validate this mechanism, we introduce UniReg, a universal registration framework that decouples feature extraction from deformation estimation using fixed, pre-trained feature extractors and a UNet-based deformation network. Despite training on a single dataset, UniReg exhibits robust cross-domain and multi-modal performance comparable to optimization-based methods. Our analysis further reveals that failures of conventional CNN-based models under modality shift originate from dataset-induced biases in early convolutional layers. These findings identify local feature consistency as the key driver of robustness in learning-based deformable registration and motivate backbone designs that preserve domain-invariant local features.

</details>


### [116] [GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection](https://arxiv.org/abs/2512.23147)
*Jingyu Li,Xiaolong Zhao,Zhe Liu,Wenxiao Wu,Li Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法GeoTeacher，通过几何关系监督和体素级的数据增强，有效提升了少标注数据或无标注数据下的3D目标检测性能，并在ONCE和Waymo数据集上取得了最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有半监督3D目标检测方法常忽视了模型对目标几何信息的敏感性不足，尤其是在标注数据有限的情况下，导致学生模型对几何结构理解不够，影响检测和定位能力。为此，需要设计能够增强学生模型几何关系建模能力的机制。

Method: 提出GeoTeacher，采用基于关键点的几何关系监督模块，将教师模型中关于目标几何的知识传递给学生模型。同时，设计了一种体素级数据增强策略，提升目标几何多样性，并引入距离衰减机制保护远距离目标完整性。该方法可与不同的半监督3D方法结合使用。

Result: 在ONCE和Waymo两个数据集上进行了大量实验，GeoTeacher显著提升了半监督3D目标检测性能，超越了先前方法，取得了最新最高分数，证明了方法的有效性和泛化性。

Conclusion: GeoTeacher能够补充和提升现有半监督3D检测方法在几何关系建模方面的能力，使学生模型更好捕捉目标几何结构，适用于多种方法场景，在公开数据集上达到新的SOTA水平。

Abstract: Semi-supervised 3D object detection, aiming to explore unlabeled data for boosting 3D object detectors, has emerged as an active research area in recent years. Some previous methods have shown substantial improvements by either employing heterogeneous teacher models to provide high-quality pseudo labels or enforcing feature-perspective consistency between the teacher and student networks. However, these methods overlook the fact that the model usually tends to exhibit low sensitivity to object geometries with limited labeled data, making it difficult to capture geometric information, which is crucial for enhancing the student model's ability in object perception and localization. In this paper, we propose GeoTeacher to enhance the student model's ability to capture geometric relations of objects with limited training data, especially unlabeled data. We design a keypoint-based geometric relation supervision module that transfers the teacher model's knowledge of object geometry to the student, thereby improving the student's capability in understanding geometric relations. Furthermore, we introduce a voxel-wise data augmentation strategy that increases the diversity of object geometries, thereby further improving the student model's ability to comprehend geometric structures. To preserve the integrity of distant objects during augmentation, we incorporate a distance-decay mechanism into this strategy. Moreover, GeoTeacher can be combined with different SS3D methods to further improve their performance. Extensive experiments on the ONCE and Waymo datasets indicate the effectiveness and generalization of our method and we achieve the new state-of-the-art results. Code will be available at https://github.com/SII-Whaleice/GeoTeacher

</details>


### [117] [REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation](https://arxiv.org/abs/2512.23169)
*Fulin Shi,Wenyi Xiao,Bin Chen,Liang Din,Leilei Gan*

Main category: cs.CV

TL;DR: 提出了一种名为REVEALER的新型细粒度文本-图像对齐评估方法，能更好地反映人类偏好，性能优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 目前文本到图像（T2I）模型的对齐评估方法多为粗粒度度量或静态问答流程，缺乏可解释性且难以体现人类偏好，需要更精细和具有人类解释性的评估方法。

Method: 提出REVEALER框架，基于‘锚定—推理—结论’三段式结构，通过多模态大语言模型（MLLM）对图片语义元素定位并推理，采用群体相对策略优化（GRPO）训练，复合奖励函数结合结构化格式、锚定准确性和对齐度。

Result: 在EvalMuse-40K、RichHF、MHaluBench和GenAI-Bench这四个基准测试中，REVEALER在性能上超越现有主流和专有模型，推理效率也高于现有视觉推理方法。

Conclusion: REVEALER能提供更具可解释性和细粒度的文本-图像对齐评估方法，在多项基准测试中表现出色，有望推动T2I模型的评估与应用。

Abstract: Evaluating the alignment between textual prompts and generated images is critical for ensuring the reliability and usability of text-to-image (T2I) models. However, most existing evaluation methods rely on coarse-grained metrics or static QA pipelines, which lack fine-grained interpretability and struggle to reflect human preferences. To address this, we propose REVEALER, a unified framework for element-level alignment evaluation based on reinforcement-guided visual reasoning. Adopting a structured "grounding-reasoning-conclusion" paradigm, our method enables Multimodal Large Language Models (MLLMs) to explicitly localize semantic elements and derive interpretable alignment judgments. We optimize the model via Group Relative Policy Optimization(GRPO) using a composite reward function that incorporates structural format, grounding accuracy, and alignment fidelity. Extensive experiments across four benchmarks-EvalMuse-40K, RichHF, MHaluBench, and GenAI-Bench-demonstrate that REVEALER achieves state-of-the-art performance. Our approach consistently outperforms both strong proprietary models and supervised baselines while demonstrating superior inference efficiency compared to existing iterative visual reasoning methods.

</details>


### [118] [GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection](https://arxiv.org/abs/2512.23176)
*Yi Zhang,Yi Wang,Lei Yao,Lap-Pui Chau*

Main category: cs.CV

TL;DR: 该论文提出了一种基于图像的3D目标检测新框架GVSynergy-Det，通过协同高斯-体素（Gaussian-Voxel）表征学习，有效提升了不依赖深度传感器的3D检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的3D目标检测面临两个难题：高精度方法依赖稠密3D监督成本高，无监督方法几何信息提取不足。该论文旨在寻找只用RGB图像即可兼顾高准确率和低监督要求的3D检测方案。

Method: 提出GVSynergy-Det，结合连续高斯（捕捉细粒度表面）与离散体素（提供结构空间上下文）表征，通过双表征架构和跨模态特征增强机制，直接集成两种表征的特征，实现检测性能提升。

Result: 在ScanNetV2和ARKitScenes等室内数据集上，无需任何深度或稠密3D监督情况下，GVSynergy-Det显著优于现有方法，达到当前最优水平。

Conclusion: 通过协同高斯-体素特征表示并创新性地跨模态融合，GVSynergy-Det仅用RGB图像即可取得性能突破，为3D目标检测提供了高效低成本的新思路。

Abstract: Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).

</details>


### [119] [GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation](https://arxiv.org/abs/2512.23180)
*Tianchen Deng,Xuefeng Chen,Yi Chen,Qu Chen,Yuyao Xu,Lijin Yang,Le Xu,Yu Zhang,Bo Zhang,Wuxiong Huang,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯场景表征的统一驾驶世界建模（DWM）框架，实现3D场景理解及多模态生成，并通过创新的任务感知采样和文本对齐等方法取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶领域的世界模型缺乏对3D场景的理解能力，只能依赖输入数据生成内容，无法解释或推理环境。同时，现有方法使用点云或BEV特征表征3D空间，无法精准对齐文本与3D场景的信息。

Method: 本文采用3D高斯场景表征，将丰富的语言特征嵌入到每个高斯基元中，实现早期的模态对齐。设计了任务感知、语言引导的采样策略，去除冗余高斯成分，将紧凑高效的3D Token输入到大模型中。此外，构建了以视觉-语言模型信息为高层语言条件、图像为低层条件的双条件多模态生成模型。

Result: 在nuScenes和NuInteract数据集上大规模实验证明，其方法在多项指标上取得了最优表现（SOTA），优于现有所有方法。

Conclusion: 该论文提出的基于3D高斯表征的统一DWM框架提升了3D场景理解和多模态生成能力，通过早期融合和任务感知采样等创新方法，为未来自动驾驶场景的智能建模提供了有效途径。代码已开源。

Abstract: Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.

</details>


### [120] [ForCM: Forest Cover Mapping from Multispectral Sentinel-2 Image by Integrating Deep Learning with Object-Based Image Analysis](https://arxiv.org/abs/2512.23196)
*Maisha Haque,Israt Jahan Ayshi,Sadaf M. Anis,Nahian Tasnim,Mithila Moontaha,Md. Sabbir Ahmed,Muhammad Iqbal Hossain,Mohammad Zavid Parvez,Subrata Chakraborty,Biswajeet Pradhan,Biswajit Banik*

Main category: cs.CV

TL;DR: 本文提出了一种名为“ForCM”的新方法，将面向对象的图像分析（OBIA）与深度学习（DL）相结合，用于利用Sentinel-2多光谱卫星影像进行森林覆盖制图。通过集成不同DL模型与OBIA，显著提升了制图精度。


<details>
  <summary>Details</summary>
Motivation: 现有的森林覆盖制图方法存在分辨率、准确率与操作复杂度等诸多挑战，传统OBIA方法已不够精准。作者希望借助深度学习的优势，提高遥感影像下的森林覆盖识别与制图精度。

Method: 研究利用Sentinel-2卫星的高分辨率多光谱影像，测试多种深度学习模型，如UNet、UNet++、ResUNet、AttentionUNet、ResNet50-Segnet。筛选出表现最佳的模型后，与OBIA方法集成，形成ForCM方法，并与传统OBIA进行精度对比，使用QGIS等免费工具完成流程。

Result: ForCM方法显著提升了森林制图的准确率。其中，ResUNet-OBIA达到94.54%的总体精度，AttentionUNet-OBIA高达95.64%，均优于传统OBIA的92.91%。

Conclusion: 将深度学习与OBIA结合有助于突破传统森林覆盖制图的瓶颈，显著提升精度。所用方法免费且易于复现，为全球环境监测和保护提供了有力工具。

Abstract: This research proposes "ForCM", a novel approach to forest cover mapping that combines Object-Based Image Analysis (OBIA) with Deep Learning (DL) using multispectral Sentinel-2 imagery. The study explores several DL models, including UNet, UNet++, ResUNet, AttentionUNet, and ResNet50-Segnet, applied to high-resolution Sentinel-2 Level 2A satellite images of the Amazon Rainforest. The datasets comprise three collections: two sets of three-band imagery and one set of four-band imagery. After evaluation, the most effective DL models are individually integrated with the OBIA technique to enhance mapping accuracy. The originality of this work lies in evaluating different deep learning models combined with OBIA and comparing them with traditional OBIA methods. The results show that the proposed ForCM method improves forest cover mapping, achieving overall accuracies of 94.54 percent with ResUNet-OBIA and 95.64 percent with AttentionUNet-OBIA, compared to 92.91 percent using traditional OBIA. This research also demonstrates the potential of free and user-friendly tools such as QGIS for accurate mapping within their limitations, supporting global environmental monitoring and conservation efforts.

</details>


### [121] [Exploring Syn-to-Real Domain Adaptation for Military Target Detection](https://arxiv.org/abs/2512.23208)
*Jongoh Jeong,Youngjin Oh,Gyeongrae Nam,Jeongeun Lee,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 本研究针对军事领域多样环境下的目标检测，提出使用Unreal Engine生成RGB合成数据，进行跨域训练并在真实数据集上验证，发现带有少许监督信息的方法优于无监督/半监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前领域自适应目标检测方法多应用于自然或自动驾驶场景，相互之间差异较小，但军事领域中目标和环境多变，现有数据资源和适应能力有限，需要新的方案以降低成本并提升检测效果。

Method: 作者利用Unreal Engine生成高真实感的RGB军事目标合成数据，并建立合成到真实的跨域训练-验证框架，分别测试了不同监督程度的主流领域自适应方法，在新构建的数据集对比评测。

Result: 利用合成RGB数据跨域训练后，带有少量监督（如类别标签）的方法在目标检测表现上明显优于无监督或半监督领域自适应方法。

Conclusion: 目前使用极少监督线索的方法在跨域军事目标检测任务中有明显优势，但仍有诸多挑战有待解决，包括数据集的丰富性和领域适应能力等。

Abstract: Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.

</details>


### [122] [Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks](https://arxiv.org/abs/2512.23210)
*Changgyoon Oh,Jongoh Jeong,Jegyeong Cho,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 本文提出了一种改进扩散模型用于小样本密集预测任务的方法，通过自适应选择和整合扩散时刻特征，提高了在少样本情景下的表现，并在Taskonomy数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的应用在进行单一任务预测时，多依赖于经验选择扩散时刻特征，这在多任务或小样本密集预测中表现有限。作者希望解决扩散时刻特征选择的非最优性，实现更具适应性的特征利用。

Method: 提出了Task-aware Timestep Selection（TTS）模块，用于基于损失和相似度自适应选择最合适扩散时刻；提出了Timestep Feature Consolidation（TFC）模块，用于整合选中的时刻特征，并配合高效的微调适配器提升少样本密集预测能力。

Result: 在Taskonomy大规模密集预测数据集上进行实证验证，所提方法在仅有少量支持样本情况下，在多项密集预测任务上取得了优于现有方法的表现。

Conclusion: 自适应选择和整合扩散时刻特征能显著提升扩散模型在小样本密集预测任务中的通用性和性能，适合在实际通用与少样本学习场景下应用。

Abstract: Denoising diffusion probabilistic models have brought tremendous advances in generative tasks, achieving state-of-the-art performance thus far. Current diffusion model-based applications exploit the power of learned visual representations from multistep forward-backward Markovian processes for single-task prediction tasks by attaching a task-specific decoder. However, the heuristic selection of diffusion timestep features still heavily relies on empirical intuition, often leading to sub-optimal performance biased towards certain tasks. To alleviate this constraint, we investigate the significance of versatile diffusion timestep features by adaptively selecting timesteps best suited for the few-shot dense prediction task, evaluated on an arbitrary unseen task. To this end, we propose two modules: Task-aware Timestep Selection (TTS) to select ideal diffusion timesteps based on timestep-wise losses and similarity scores, and Timestep Feature Consolidation (TFC) to consolidate the selected timestep features to improve the dense predictive performance in a few-shot setting. Accompanied by our parameter-efficient fine-tuning adapter, our framework effectively achieves superiority in dense prediction performance given only a few support queries. We empirically validate our learnable timestep consolidation method on the large-scale challenging Taskonomy dataset for dense prediction, particularly for practical universal and few-shot learning scenarios.

</details>


### [123] [AVOID: The Adverse Visual Conditions Dataset with Obstacles for Driving Scene Understanding](https://arxiv.org/abs/2512.23215)
*Jongoh Jeong,Taek-Jin Song,Jong-Hwan Kim,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 本文提出并发布了一个新的道路障碍检测数据集AVOID，专为模拟多种恶劣视觉条件下的实时障碍检测任务设计，并验证了其应用价值。


<details>
  <summary>Details</summary>
Motivation: 目前公开的道路驾驶数据集往往只包含正常或特定恶劣场景，不同类别障碍物的视觉域分布不一致，导致难以训练鲁棒的障碍检测模型。作者希望通过创建一个覆盖多种气象和昼夜条件的多模态障碍检测数据集，提升自动驾驶系统对意外小障碍物的实时检测能力。

Method: 作者在模拟环境下采集了包含多类意外道路障碍物的图像数据，涵盖不同天气和时间条件，并配套提供语义分割图、深度图、原始及语义LiDAR数据，以及路线点。基于该数据集，作者对主流实时检测网络进行基准测试，并设计了多任务网络（包括语义分割、深度估计和路线预测）进行消融实验。

Result: AVOID数据集收集了大量多样场景下障碍物的高质量数据。作者利用该数据集验证了多种高性能实时检测网络的检测效果，并通过多任务网络展示了丰富标签带来的性能提升和多任务协同效应。

Conclusion: AVOID数据集为恶劣视觉条件下的实时障碍检测研究提供了新基准和全面的数据资源。实验结果表明其在推动小障碍物检测和跨任务视觉感知任务方面具有重要价值。

Abstract: Understanding road scenes for visual perception remains crucial for intelligent self-driving cars. In particular, it is desirable to detect unexpected small road hazards reliably in real-time, especially under varying adverse conditions (e.g., weather and daylight). However, existing road driving datasets provide large-scale images acquired in either normal or adverse scenarios only, and often do not contain the road obstacles captured in the same visual domain as for the other classes. To address this, we introduce a new dataset called AVOID, the Adverse Visual Conditions Dataset, for real-time obstacle detection collected in a simulated environment. AVOID consists of a large set of unexpected road obstacles located along each path captured under various weather and time conditions. Each image is coupled with the corresponding semantic and depth maps, raw and semantic LiDAR data, and waypoints, thereby supporting most visual perception tasks. We benchmark the results on high-performing real-time networks for the obstacle detection task, and also propose and conduct ablation studies using a comprehensive multi-task network for semantic segmentation, depth and waypoint prediction tasks.

</details>


### [124] [MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?](https://arxiv.org/abs/2512.23219)
*Shiqi Dai,Zizhi Ma,Zhicong Luo,Xuesong Yang,Yibin Huang,Wanyue Zhang,Chi Chen,Zonghao Guo,Wang Xu,Yufei Sun,Maosong Sun*

Main category: cs.CV

TL;DR: 本文提出了MM-UAVBench，一个专为低空无人机（UAV）场景设计的多模态大模型（MLLM）评测基准，覆盖感知、认知和规划三大能力，并通过实验揭示现有MLLM在低空场景中存在显著短板。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型（MLLM）评测和无人机相关评测各有局限：前者很少关注低空UAV场景特有难点，后者又缺乏对MLLM通用智能能力的统一评估。因此亟需建立面向UAV低空应用、系统化评估MLLM智能的基准。

Method: 构建了MM-UAVBench基准，涵盖感知、认知和规划等三大能力，对UAV低空环境中的19个子任务进行评测，题目均来自真实UAV公开数据集人工标注。通过在16个开源和专有MLLM上开展系统性实验，发现它们在低空复杂场景中普遍表现不佳。

Result: 实验表明，包括开源和专有的16种MLLM，在低空UAV情境下普遍无法有效应对复杂的视觉与认知需求。分析发现，空间偏置和多视角理解不足等问题，是当前MLLM无法适应UAV低空应用场景的主要瓶颈。

Conclusion: MM-UAVBench作为一套系统、真实场景的数据基准，有望推动更鲁棒、可靠的MLLM在无人机低空智能应用方向的研究。

Abstract: While Multimodal Large Language Models (MLLMs) have exhibited remarkable general intelligence across diverse domains, their potential in low-altitude applications dominated by Unmanned Aerial Vehicles (UAVs) remains largely underexplored. Existing MLLM benchmarks rarely cover the unique challenges of low-altitude scenarios, while UAV-related evaluations mainly focus on specific tasks such as localization or navigation, without a unified evaluation of MLLMs'general intelligence. To bridge this gap, we present MM-UAVBench, a comprehensive benchmark that systematically evaluates MLLMs across three core capability dimensions-perception, cognition, and planning-in low-altitude UAV scenarios. MM-UAVBench comprises 19 sub-tasks with over 5.7K manually annotated questions, all derived from real-world UAV data collected from public datasets. Extensive experiments on 16 open-source and proprietary MLLMs reveal that current models struggle to adapt to the complex visual and cognitive demands of low-altitude scenarios. Our analyses further uncover critical bottlenecks such as spatial bias and multi-view understanding that hinder the effective deployment of MLLMs in UAV scenarios. We hope MM-UAVBench will foster future research on robust and reliable MLLMs for real-world UAV intelligence.

</details>


### [125] [Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information](https://arxiv.org/abs/2512.23221)
*Youngchae Kwon,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的Holistic Detection Transformer (Holi-DETR)方法，利用多种上下文信息整体提升时尚单品检测的精度，相较于现有DETR系列模型有明显性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于时尚单品外观多样且类别间相似度高，传统方法难以准确检测分类。该文旨在通过引入上下文关系信息，减少检测过程中的歧义。

Method: Holi-DETR结合了三种上下文信息：1）时尚单品间的共现关系，2）单品间的相对空间位置和大小，3）单品与人体关键点的空间关系，并将这些异构上下文信息集成进DETR架构。

Result: 在实验中，Holi-DETR在平均精度（AP）指标上比基础DETR提升了3.6个百分点，比Co-DETR提升了1.1个百分点。

Conclusion: 引入多维上下文信息有助于缓解时尚单品检测中的歧义，可以显著提升检测精度，为时尚图像理解提供了更有效的解决方案。

Abstract: Fashion item detection is challenging due to the ambiguities introduced by the highly diverse appearances of fashion items and the similarities among item subcategories. To address this challenge, we propose a novel Holistic Detection Transformer (Holi-DETR) that detects fashion items in outfit images holistically, by leveraging contextual information. Fashion items often have meaningful relationships as they are combined to create specific styles. Unlike conventional detectors that detect each item independently, Holi-DETR detects multiple items while reducing ambiguities by leveraging three distinct types of contextual information: (1) the co-occurrence relationship between fashion items, (2) the relative position and size based on inter-item spatial arrangements, and (3) the spatial relationships between items and human body key-points. %Holi-DETR explicitly incorporates three types of contextual information: (1) the co-occurrence probability between fashion items, (2) the relative position and size based on inter-item spatial arrangements, and (3) the spatial relationships between items and human body key-points. To this end, we propose a novel architecture that integrates these three types of heterogeneous contextual information into the Detection Transformer (DETR) and its subsequent models. In experiments, the proposed methods improved the performance of the vanilla DETR and the more recently developed Co-DETR by 3.6 percent points (pp) and 1.1 pp, respectively, in terms of average precision (AP).

</details>


### [126] [Bridging Your Imagination with Audio-Video Generation via a Unified Director](https://arxiv.org/abs/2512.23222)
*Jiaxu Zhang,Tianshu Hu,Yuan Zhang,Zenan Li,Linjie Luo,Guosheng Lin,Xin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为UniMAGE的统一导演模型，将文本脚本创作与关键镜头设计整合于同一框架，通过混合变换器架构和创新的训练范式，实现了逻辑连贯的剧本和一致的关键帧生成，帮助非专业用户创作多镜头长视频。


<details>
  <summary>Details</summary>
Motivation: 现有AI视频创作系统将脚本编写和镜头设计视为分离任务，缺乏统一性，而影片导演的逻辑推理和想象需两者结合，因此需要一种融合这两项任务的统一模型。

Method: 作者提出了UniMAGE模型，采用Mixture-of-Transformers架构统一文本和图像生成，并设计了“先交错，后解耦”的训练方式，包括交错概念学习（通过文本-图像交错数据提升脚本理解与想象）和解耦专家学习（将脚本与关键帧生成分开优化，增强创作灵活性）。

Result: 实验表明，UniMAGE在公开模型中达到了最先进性能，能够生成逻辑连贯的视频脚本和视觉一致的关键帧图像。

Conclusion: 将脚本创作与关键帧生成合为一体显著提升了AI视频创作系统的表现，UniMAGE不仅提升了生成效果，还简化了创作流程，适合非专业用户使用。

Abstract: Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images.

</details>


### [127] [Anomaly Detection by Effectively Leveraging Synthetic Images](https://arxiv.org/abs/2512.23227)
*Sungho Kang,Hyunkyu Park,Yeonho Lee,Hanbyul Lee,Mijoo Jeong,YeongHyeon Park,Injae Lee,Juneho Yi*

Main category: cs.CV

TL;DR: 本文提出一种结合文本引导的图像到图像生成模型与图像检索的合成缺陷图像生成框架，并通过双阶段训练方法提升工业异常检测性能，有效降低数据采集成本。


<details>
  <summary>Details</summary>
Motivation: 工业制造中异常检测依赖真实缺陷图像，但由于缺陷图像稀缺，难以获得，现有无监督方法仅用正常样本有局限性。合成图像方法又存在成本与真实感之间的权衡。亟需一种低成本、高质量的缺陷图像合成策略，提升异常检测效果。

Method: 提出利用预训练的文本引导图像到图像生成模型合成缺陷图像，并辅以图像检索模型筛选与正常样本相似且相关性高的合成图像，从而提升数据质量。训练中采用两阶段策略：首先用大量低成本规则合成数据预训练模型，再用小规模高质量数据微调。

Result: 在MVTec AD基准数据集上的实验表明，该方法在显著降低数据采集与合成成本的同时，提升了异常检测的准确性和模型性能。

Conclusion: 本文提出的基于高质量合成图像及双阶段训练的新框架，能够高效生成有效的缺陷样本，为工业异常检测提供一种低成本、高效的新思路。

Abstract: Anomaly detection plays a vital role in industrial manufacturing. Due to the scarcity of real defect images, unsupervised approaches that rely solely on normal images have been extensively studied. Recently, diffusion-based generative models brought attention to training data synthesis as an alternative solution. In this work, we focus on a strategy to effectively leverage synthetic images to maximize the anomaly detection performance. Previous synthesis strategies are broadly categorized into two groups, presenting a clear trade-off. Rule-based synthesis, such as injecting noise or pasting patches, is cost-effective but often fails to produce realistic defect images. On the other hand, generative model-based synthesis can create high-quality defect images but requires substantial cost. To address this problem, we propose a novel framework that leverages a pre-trained text-guided image-to-image translation model and image retrieval model to efficiently generate synthetic defect images. Specifically, the image retrieval model assesses the similarity of the generated images to real normal images and filters out irrelevant outputs, thereby enhancing the quality and relevance of the generated defect images. To effectively leverage synthetic images, we also introduce a two stage training strategy. In this strategy, the model is first pre-trained on a large volume of images from rule-based synthesis and then fine-tuned on a smaller set of high-quality images. This method significantly reduces the cost for data collection while improving the anomaly detection performance. Experiments on the MVTec AD dataset demonstrate the effectiveness of our approach.

</details>


### [128] [SURE Guided Posterior Sampling: Trajectory Correction for Diffusion-Based Inverse Problems](https://arxiv.org/abs/2512.23232)
*Minwoo Kim,Hongki Lim*

Main category: cs.CV

TL;DR: 提出了一种新的扩散模型采样方法SGPS，在保证高重建质量的前提下显著降低了采样步骤，提升了逆问题求解效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的逆问题求解方法常因累计误差，需要大量迭代步骤才能获得高质量重建，效率较低。作者希望降低迭代次数、减少误差积累。

Method: 提出SURE Guided Posterior Sampling（SGPS）方法，利用Stein无偏风险估计（SURE）梯度更新和PCA噪声估计，在采样早中期修正采样轨迹偏差，缓解噪声引入的误差。

Result: 在多种逆问题上，SGPS在较低神经网络函数评估（NFE）次数下，重建精度明显优于现有方法。

Conclusion: SGPS方法能以更少采样步骤达成高质量重建，优于当前主流扩散模型采样方法，具备广泛适用性。

Abstract: Diffusion models have emerged as powerful learned priors for solving inverse problems. However, current iterative solving approaches which alternate between diffusion sampling and data consistency steps typically require hundreds or thousands of steps to achieve high quality reconstruction due to accumulated errors. We address this challenge with SURE Guided Posterior Sampling (SGPS), a method that corrects sampling trajectory deviations using Stein's Unbiased Risk Estimate (SURE) gradient updates and PCA based noise estimation. By mitigating noise induced errors during the critical early and middle sampling stages, SGPS enables more accurate posterior sampling and reduces error accumulation. This allows our method to maintain high reconstruction quality with fewer than 100 Neural Function Evaluations (NFEs). Our extensive evaluation across diverse inverse problems demonstrates that SGPS consistently outperforms existing methods at low NFE counts.

</details>


### [129] [Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network](https://arxiv.org/abs/2512.23234)
*Dongsheng Li,Chaobo Chen,Siling Wang,Song Gao*

Main category: cs.CV

TL;DR: 本文提出了一种新型红外气体泄漏检测网络PEG-DRNet，在准确性和计算效率之间实现了最佳平衡，并显著优于现有的主流检测方法。


<details>
  <summary>Details</summary>
Motivation: 红外气体泄漏检测对环境监测和工业安全至关重要，但由于烟羽特征弱小、半透明且边界模糊，检测任务面临很大挑战。

Method: 作者设计了一种结合物理建模和边缘感知的混合方法。核心包括：1）Gas Block单元，结合局部与长距离信息模拟气体扩散传输；2）自适应梯度与相位边缘算子（AGPEO），与多尺度边缘感知模块融合，强化弱边界；3）内容自适应稀疏路由聚合网络（CASR-PAN），根据信息内容与边缘特征跨尺度选择性交互，提高区分力并减少冗余。

Result: 在IIG数据集上，PEG-DRNet取得了29.8%的整体AP、84.3%的AP50和25.3%的小目标AP，较基线RT-DETR-R18有显著提升。同时，模型规模仅14.9M参数、计算量43.7Gflops。在IIG与LangGas数据集上，性能均优于CNN和Transformer主流检测器。

Conclusion: PEG-DRNet有效提升了红外气体泄漏检测的准确性、泛化性和效率，为实际应用提供了更可靠的解决方案。

Abstract: Detecting infrared gas leaks is critical for environmental monitoring and industrial safety, yet remains difficult because plumes are faint, small, semitransparent, and have weak, diffuse boundaries. We present physics-edge hybrid gas dynamic routing network (PEG-DRNet). First, we introduce the Gas Block, a diffusion-convection unit modeling gas transport: a local branch captures short-range variations, while a large-kernel branch captures long-range propagation. An edge-gated learnable fusion module balances local detail and global context, strengthening weak-contrast plume and contour cues. Second, we propose the adaptive gradient and phase edge operator (AGPEO), computing reliable edge priors from multi-directional gradients and phase-consistent responses. These are transformed by a multi-scale edge perception module (MSEPM) into hierarchical edge features that reinforce boundaries. Finally, the content-adaptive sparse routing path aggregation network (CASR-PAN), with adaptive information modulation modules for fusion and self, selectively propagates informative features across scales based on edge and content cues, improving cross-scale discriminability while reducing redundancy. Experiments on the IIG dataset show that PEG-DRNet achieves an overall AP of 29.8\%, an AP$_{50}$ of 84.3\%, and a small-object AP of 25.3\%, surpassing the RT-DETR-R18 baseline by 3.0\%, 6.5\%, and 5.3\%, respectively, while requiring only 43.7 Gflops and 14.9 M parameters. The proposed PEG-DRNet achieves superior overall performance with the best balance of accuracy and computational efficiency, outperforming existing CNN and Transformer detectors in AP and AP$_{50}$ on the IIG and LangGas dataset.

</details>


### [130] [RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models](https://arxiv.org/abs/2512.23239)
*Fan Wei,Runmin Dong,Yushan Lai,Yixiang Yang,Zhaoyang Luo,Jinxiao Zhang,Miao Yang,Shuai Yuan,Jiyao Zhao,Bin Luo,Haohuan Fu*

Main category: cs.CV

TL;DR: 本文提出了一种无训练、两阶段的数据剪枝方法，用于提升遥感生成式基础模型的训练效率和下游任务表现。通过熵值筛选和场景感知聚类，在高剪枝率下快速选出高质量子集，从而提升模型的收敛速度和生成质量。实验中在保留15%数据的情况下，仍实现了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感扩散生成式基础模型高度依赖大规模、全局代表性数据，但数据集往往存在冗余、噪声和类别不均衡，导致训练低效且难以收敛。同时，现有剪枝方法简单，仅聚焦分类数据集或重复数据，未兼顾遥感影像的分布需求和异质性。

Method: 方法提出了一个“无训练、两阶段”的数据剪枝流程：第一阶段利用熵值指标去除信息量低的样本；第二阶段基于遥感场景分类数据集为基准，进行场景感知聚类和分层采样，提升聚类效果并降低大规模无标签数据的算力成本。最终在确保多样性和代表性的前提下，实现高剪枝率下的精细筛样。

Result: 在实验中，该方法即使在剪除85%训练数据的情况下，仍显著提升了模型收敛速度和生成质量。在包括超分辨率、语义图像合成等下游任务中，所训练的基础模型表现优于现有主流方法。

Conclusion: 提出的数据剪枝范式为遥感生成式基础模型的数据准备和高效训练提供了实用指导，显著提升了效果和数据利用率，理论和实践价值突出。

Abstract: Diffusion-based remote sensing (RS) generative foundation models are cruial for downstream tasks. However, these models rely on large amounts of globally representative data, which often contain redundancy, noise, and class imbalance, reducing training efficiency and preventing convergence. Existing RS diffusion foundation models typically aggregate multiple classification datasets or apply simplistic deduplication, overlooking the distributional requirements of generation modeling and the heterogeneity of RS imagery. To address these limitations, we propose a training-free, two-stage data pruning approach that quickly select a high-quality subset under high pruning ratios, enabling a preliminary foundation model to converge rapidly and serve as a versatile backbone for generation, downstream fine-tuning, and other applications. Our method jointly considers local information content with global scene-level diversity and representativeness. First, an entropy-based criterion efficiently removes low-information samples. Next, leveraging RS scene classification datasets as reference benchmarks, we perform scene-aware clustering with stratified sampling to improve clustering effectiveness while reducing computational costs on large-scale unlabeled data. Finally, by balancing cluster-level uniformity and sample representativeness, the method enables fine-grained selection under high pruning ratios while preserving overall diversity and representativeness. Experiments show that, even after pruning 85\% of the training data, our method significantly improves convergence and generation quality. Furthermore, diffusion foundation models trained with our method consistently achieve state-of-the-art performance across downstream tasks, including super-resolution and semantic image synthesis. This data pruning paradigm offers practical guidance for developing RS generative foundation models.

</details>


### [131] [Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism](https://arxiv.org/abs/2512.23243)
*Siyu Zhang,Ying Chen,Lianlei Shan,Runhe Qiu*

Main category: cs.CV

TL;DR: 该论文提出了一种融合视觉与语言模型的新框架，通过动态分辨率输入和多尺度对齐机制，提升了多模态遥感影像处理的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 单一遥感数据源在地表信息提取中存在精度和细节不足；现有多模态方法难以兼顾效率与复杂细节，且单一尺度的对齐缺乏语义层次感。

Method: 提出视觉-语言模型（VLM）框架，包含两大创新：1）动态分辨率输入策略（DRIS），根据图像复杂度自适应分配计算资源，兼顾效率与细节；2）多尺度视觉-语言对齐机制（MS-VLAM），分别在对象、局部和全局三层实现对齐，提升跨模态语义一致性。

Result: 在RS-GPT4V数据集上，所提框架在图像描述和跨模态检索等任务中表现优异，在BLEU-4、CIDEr和R@10等指标上均超越传统方法。

Conclusion: 该技术框架为高效、鲁棒的多模态遥感系统提供了新思路，并为智能遥感解译工程应用提供了理论和技术指导。

Abstract: Multimodal fusion of remote sensing images serves as a core technology for overcoming the limitations of single-source data and improving the accuracy of surface information extraction, which exhibits significant application value in fields such as environmental monitoring and urban planning. To address the deficiencies of existing methods, including the failure of fixed resolutions to balance efficiency and detail, as well as the lack of semantic hierarchy in single-scale alignment, this study proposes a Vision-language Model (VLM) framework integrated with two key innovations: the Dynamic Resolution Input Strategy (DRIS) and the Multi-scale Vision-language Alignment Mechanism (MS-VLAM).Specifically, the DRIS adopts a coarse-to-fine approach to adaptively allocate computational resources according to the complexity of image content, thereby preserving key fine-grained features while reducing redundant computational overhead. The MS-VLAM constructs a three-tier alignment mechanism covering object, local-region and global levels, which systematically captures cross-modal semantic consistency and alleviates issues of semantic misalignment and granularity imbalance.Experimental results on the RS-GPT4V dataset demonstrate that the proposed framework significantly improves the accuracy of semantic understanding and computational efficiency in tasks including image captioning and cross-modal retrieval. Compared with conventional methods, it achieves superior performance in evaluation metrics such as BLEU-4 and CIDEr for image captioning, as well as R@10 for cross-modal retrieval. This technical framework provides a novel approach for constructing efficient and robust multimodal remote sensing systems, laying a theoretical foundation and offering technical guidance for the engineering application of intelligent remote sensing interpretation.

</details>


### [132] [ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2512.23244)
*Xingwei Ma,Shiyang Feng,Bo Zhang,Bin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为ViLaCD-R1的两阶段遥感变化检测框架，通过多模态模型与自监督学习有效提升了变化区域的语义理解和定位准确度，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 传统的遥感变化检测方法（像素比较、编码器—解码器网络等）难以捕捉高层次语义信息，且易受无关扰动影响。尽管近期的多模态和视觉-语言模型（VLM）方法提升了语义理解能力，但存在空间定位不准、边界模糊、可解释性差等问题。作者致力于解决这些挑战。

Method: ViLaCD-R1采用两阶段流程。第一阶段为多图像推理器（MIR），以视觉语言模型（VLM）通过有监督微调和强化学习在块级双时相推理任务中，生成粗略变化掩码。第二阶段为掩码指导解码器（MGD），结合双时相图像特征和粗掩码预测最终精确的二值变化图。

Result: 在多个遥感变化检测基准数据集上，ViLaCD-R1在真实语义变化识别与定位、抑制非语义干扰等方面表现出色，并在复杂的真实场景中取得了当前最优精度。

Conclusion: ViLaCD-R1有效提升了遥感变化检测中的语义识别与空间定位能力，具有较强鲁棒性和可解释性，在实际应用中具备显著优势。

Abstract: Remote sensing change detection (RSCD), a complex multi-image inference task, traditionally uses pixel-based operators or encoder-decoder networks that inadequately capture high-level semantics and are vulnerable to non-semantic perturbations. Although recent multimodal and vision-language model (VLM)-based approaches enhance semantic understanding of change regions by incorporating textual descriptions, they still suffer from challenges such as inaccurate spatial localization, imprecise pixel-level boundary delineation, and limited interpretability. To address these issues, we propose ViLaCD-R1, a two-stage framework comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). Specifically, the VLM is trained through supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks, taking dual-temporal image patches as input and outputting a coarse change mask. Then, the decoder integrates dual-temporal image features with this coarse mask to predict a precise binary change map. Comprehensive evaluations on multiple RSCD benchmarks demonstrate that ViLaCD-R1 substantially improves true semantic change recognition and localization, robustly suppresses non-semantic variations, and achieves state-of-the-art accuracy in complex real-world scenarios.

</details>


### [133] [ASemConsist: Adaptive Semantic Feature Control for Training-Free Identity-Consistent Generation](https://arxiv.org/abs/2512.23245)
*Shin seong Kim,Minjung Shin,Hyunin Cho,Youngjung Uh*

Main category: cs.CV

TL;DR: 本文提出了ASemconsist框架，通过有选择地修改文本嵌入，实现了跨多场景图像序列中人物身份的一致性和每张图片文本对齐的平衡，达到了目前最优效果。


<details>
  <summary>Details</summary>
Motivation: 当前的文本生成图像扩散模型在单图像质量和文本对齐方面有很大提升，但在根据不同场景描述生成图像序列时，保持人物身份一致性仍存在较大挑战，且难以平衡身份一致性和每图像文本对齐的性能。

Method: 1. 有选择地修改文本嵌入，以实现对人物身份的显式语义控制。
2. 基于FLUX模型中padding embedding的分析，将其重新用于语义信息容器，实现更灵活的身份表达。
3. 提出自适应特征共享策略，根据文本中的歧义情况自动施加约束，只在身份歧义显著时起作用。
4. 设计新的评测协议Consistency Quality Score（CQS），统一评估身份保持和文本对齐两方面的性能。

Result: ASemconsist在身份一致性和文本对齐方面实现了目前最优的平衡，显著优于现有方法，并通过CQS全面展示了模型的实际表现。

Conclusion: ASemconsist框架突破了身份一致性与每图像文本对齐的传统平衡难题，为文本驱动的多场景一致性生成提供了高效可靠的解决方案，并提出了统一的评测方法推动相关领域发展。

Abstract: Recent text-to-image diffusion models have significantly improved visual quality and text alignment. However, generating a sequence of images while preserving consistent character identity across diverse scene descriptions remains a challenging task. Existing methods often struggle with a trade-off between maintaining identity consistency and ensuring per-image prompt alignment. In this paper, we introduce a novel framework, ASemconsist, that addresses this challenge through selective text embedding modification, enabling explicit semantic control over character identity without sacrificing prompt alignment. Furthermore, based on our analysis of padding embeddings in FLUX, we propose a semantic control strategy that repurposes padding embeddings as semantic containers. Additionally, we introduce an adaptive feature-sharing strategy that automatically evaluates textual ambiguity and applies constraints only to the ambiguous identity prompt. Finally, we propose a unified evaluation protocol, the Consistency Quality Score (CQS), which integrates identity preservation and per-image text alignment into a single comprehensive metric, explicitly capturing performance imbalances between the two metrics. Our framework achieves state-of-the-art performance, effectively overcoming prior trade-offs. Project page: https://minjung-s.github.io/asemconsist

</details>


### [134] [Contour Information Aware 2D Gaussian Splatting for Image Representation](https://arxiv.org/abs/2512.23255)
*Masaya Takabe,Hiroshi Watanabe,Sujun Hong,Tomohiro Ikai,Zheming Fan,Ryo Ishimoto,Kakeru Sugimoto,Ruri Imichi*

Main category: cs.CV

TL;DR: 本文提出了一种轮廓信息感知的二维高斯溅射（2DGS）图像表示方法，提升高压缩率下边缘重建质量，有效防止图像模糊和边缘混叠。


<details>
  <summary>Details</summary>
Motivation: 现有2D高斯溅射方法在高压缩条件下容易产生边界模糊和细节丢失，特别是在高斯数量有限时，对轮廓的表达力较差。

Method: 作者将对象分割先验信息引入二维高斯溅射，将每个高斯约束在特定分割区域内，避免跨边界混合。同时提出训练预热（warm-up）策略以提升模型收敛性和稳定性。

Result: 在合成色卡和DAVIS数据集上实验表明，该方法在物体边缘重建上优于现有2DGS，尤其在高压缩（高斯数量极少）条件下提升明显，同时保持实时渲染与低内存消耗。

Conclusion: 通过整合分割先验，所提方法在保证效率的同时，大幅提升了高压缩图像表示中的轮廓和边缘质量，拓展了二维高斯溅射在视觉内容存储与快速解码中的应用潜力。

Abstract: Image representation is a fundamental task in computer vision. Recently, Gaussian Splatting has emerged as an efficient representation framework, and its extension to 2D image representation enables lightweight, yet expressive modeling of visual content. While recent 2D Gaussian Splatting (2DGS) approaches provide compact storage and real-time decoding, they often produce blurry or indistinct boundaries when the number of Gaussians is small due to the lack of contour awareness. In this work, we propose a Contour Information-Aware 2D Gaussian Splatting framework that incorporates object segmentation priors into Gaussian-based image representation. By constraining each Gaussian to a specific segmentation region during rasterization, our method prevents cross-boundary blending and preserves edge structures under high compression. We also introduce a warm-up scheme to stabilize training and improve convergence. Experiments on synthetic color charts and the DAVIS dataset demonstrate that our approach achieves higher reconstruction quality around object edges compared to existing 2DGS methods. The improvement is particularly evident in scenarios with very few Gaussians, while our method still maintains fast rendering and low memory usage.

</details>


### [135] [Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization](https://arxiv.org/abs/2512.23258)
*Tong Shao,Yusen Fu,Guoying Sun,Jingde Kong,Zhuotao Tian,Jingyong Su*

Main category: cs.CV

TL;DR: 本文提出了一种新的累计误差最小化（CEM）插件，可提升基于缓存的加速方法在Diffusion Transformer等扩散模型中的结果保真度，同时几乎不增加额外计算量。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformer（DiT）在图像和视频生成领域很流行，但其逐步去噪导致推理速度慢，现有缓存加速方法虽快但误差大，难以兼顾速度和生成质量。因此需要更有效的缓存策略以减小加速带来的误差，提升模型实际应用价值。

Method: 作者提出CEM插件，核心思想是针对加速产生的误差，预定义误差敏感度，并用动态规划优化缓存策略以实现累计误差最小化。CEM与现有误差修正方法兼容，能自适应不同加速/缓存配置，无需重新训练即可直接集成于主流扩散模型及其量化版本中。

Result: 在九种主流生成模型和多种量化方法上的三项任务中实验证明，CEM能显著提升生成质量（保真度），某些情况下甚至优于未加速的原始模型（如FLUX.1-dev、PixArt-α、StableDiffusion1.5和Hunyuan）。

Conclusion: CEM是一种高效、通用、可无缝集成的累计误差最小化插件，可显著提升扩散模型的加速生成保真度，有助于扩散模型更快更好地应用于实际任务。

Abstract: Although Diffusion Transformer (DiT) has emerged as a predominant architecture for image and video generation, its iterative denoising process results in slow inference, which hinders broader applicability and development. Caching-based methods achieve training-free acceleration, while suffering from considerable computational error. Existing methods typically incorporate error correction strategies such as pruning or prediction to mitigate it. However, their fixed caching strategy fails to adapt to the complex error variations during denoising, which limits the full potential of error correction. To tackle this challenge, we propose a novel fidelity-optimization plugin for existing error correction methods via cumulative error minimization, named CEM. CEM predefines the error to characterize the sensitivity of model to acceleration jointly influenced by timesteps and cache intervals. Guided by this prior, we formulate a dynamic programming algorithm with cumulative error approximation for strategy optimization, which achieves the caching error minimization, resulting in a substantial improvement in generation fidelity. CEM is model-agnostic and exhibits strong generalization, which is adaptable to arbitrary acceleration budgets. It can be seamlessly integrated into existing error correction frameworks and quantized models without introducing any additional computational overhead. Extensive experiments conducted on nine generation models and quantized methods across three tasks demonstrate that CEM significantly improves generation fidelity of existing acceleration models, and outperforms the original generation performance on FLUX.1-dev, PixArt-$α$, StableDiffusion1.5 and Hunyuan. The code will be made publicly available.

</details>


### [136] [YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection](https://arxiv.org/abs/2512.23273)
*Xu Lin,Jinlong Peng,Zhenye Gan,Jiawen Zhu,Jun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的YOLO-like目标检测框架YOLO-Master，通过动态分配计算资源提升检测性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有实时目标检测方法普遍采用静态均匀计算，对所有场景一视同仁，导致对简单场景浪费计算，对复杂场景资源不足，影响整体性能和效率。

Method: YOLO-Master采用了一种高效稀疏专家混合（ES-MoE）模块，结合轻量动态路由网络，根据输入场景复杂度动态选择、分配不同专家网络的计算资源。训练阶段利用多样性目标提升专家互补性，推理阶段只激活最相关的专家，以节省计算。

Result: 在5个大规模数据集上的实验显示，YOLO-Master优于同类方法。在MS COCO数据集上，YOLO-Master实现42.4% AP，推理延迟1.62ms，比YOLOv13-N高0.8% mAP且推理速度快17.8%。在复杂场景下提升最为明显，同时常规场景保证高效性和实时性。

Conclusion: YOLO-Master在保持实时性的情况下，解决了当前RTOD模型静态计算资源分配的问题，在多类场景下均展现更优性能和效率，具有实际应用价值。

Abstract: Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.

</details>


### [137] [Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition](https://arxiv.org/abs/2512.23291)
*Arman Martirosyan,Shahane Tigranyan,Maria Razzhivina,Artak Aslanyan,Nazgul Salikhova,Ilya Makarov,Andrey Savchenko,Aram Avetisyan*

Main category: cs.CV

TL;DR: 本文提出了两种多模态框架，分别用于微手势识别和基于行为的情感预测，并在iMiGUE数据集和MiGA 2025挑战赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 微手势识别和基于行为的情感预测需要捕捉人类微小且复杂的动态行为，现有方法在细粒度建模和多模态融合上存在局限，作者希望设计有效方法更好地完成这两项挑战任务。

Method: 1. 微手势识别：融合RGB视频（用MViTv2-S提取特征）和3D骨骼姿态（用2s-AGCN提取特征），通过Cross-Modal Token Fusion模块整合空间和姿态信息。2. 情感预测：视频中的面部表情用SwinFace提取，身体动作用MViTv2-S处理，再以InterFusion模块融合，全面建模情感表达和肢体语言。

Result: 在iMiGUE数据集和MiGA 2025 Challenge的基于行为情感预测任务中，所提方法表现稳健，最终获得比赛第二名。

Conclusion: 所提多模态融合方法可有效提升微手势识别与情感预测的表现，验证了RGB、骨骼等多源信息互补的优势，对细粒度人类行为分析具有推广价值。

Abstract: Micro-gesture recognition and behavior-based emotion prediction are both highly challenging tasks that require modeling subtle, fine-grained human behaviors, primarily leveraging video and skeletal pose data. In this work, we present two multimodal frameworks designed to tackle both problems on the iMiGUE dataset. For micro-gesture classification, we explore the complementary strengths of RGB and 3D pose-based representations to capture nuanced spatio-temporal patterns. To comprehensively represent gestures, video, and skeletal embeddings are extracted using MViTv2-S and 2s-AGCN, respectively. Then, they are integrated through a Cross-Modal Token Fusion module to combine spatial and pose information. For emotion recognition, our framework extends to behavior-based emotion prediction, a binary classification task identifying emotional states based on visual cues. We leverage facial and contextual embeddings extracted using SwinFace and MViTv2-S models and fuse them through an InterFusion module designed to capture emotional expressions and body gestures. Experiments conducted on the iMiGUE dataset, within the scope of the MiGA 2025 Challenge, demonstrate the robust performance and accuracy of our method in the behavior-based emotion prediction task, where our approach secured 2nd place.

</details>


### [138] [MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images](https://arxiv.org/abs/2512.23304)
*Md. Sazzadul Islam Prottasha,Nabil Walid Rafi*

Main category: cs.CV

TL;DR: 本文对比了两种多模态大模型（专有的GPT-4与开源的MedGemma）在医学影像六类疾病诊断上的性能，发现经过领域微调的MedGemma显著优于未微调的GPT-4。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在医学影像分析领域具有应用潜力，但尚缺乏关于开源与专有大模型在具体临床任务中的系统对比研究，尤其是在疾病分类等高风险场景下。研究希望揭示领域微调和模型开源性对实际临床表现的影响。

Method: 研究对比了专有的GPT-4和开源的MedGemma-4b-it模型，在六类疾病影像诊断任务上进行测试。MedGemma采用LoRA方法进行领域微调，并利用混淆矩阵和分类报告对模型性能进行量化评价。

Result: MedGemma-4b-it模型平均测试准确率为80.37%，显著高于未微调GPT-4的69.58%；在癌症和肺炎等高风险检测任务中，MedGemma灵敏性明显更高。

Conclusion: 领域微调对于减少医学领域AI模型幻觉和提升临床可用性至关重要。开源且经微调的MedGemma在复杂医学推理任务中表现更优，展现出应用前景。

Abstract: Multimodal Large Language Models (LLMs) introduce an emerging paradigm for medical imaging by interpreting scans through the lens of extensive clinical knowledge, offering a transformative approach to disease classification. This study presents a critical comparison between two fundamentally different AI architectures: the specialized open-source agent MedGemma and the proprietary large multimodal model GPT-4 for diagnosing six different diseases. The MedGemma-4b-it model, fine-tuned using Low-Rank Adaptation (LoRA), demonstrated superior diagnostic capability by achieving a mean test accuracy of 80.37% compared to 69.58% for the untuned GPT-4. Furthermore, MedGemma exhibited notably higher sensitivity in high-stakes clinical tasks, such as cancer and pneumonia detection. Quantitative analysis via confusion matrices and classification reports provides comprehensive insights into model performance across all categories. These results emphasize that domain-specific fine-tuning is essential for minimizing hallucinations in clinical implementation, positioning MedGemma as a sophisticated tool for complex, evidence-based medical reasoning.

</details>


### [139] [CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation](https://arxiv.org/abs/2512.23333)
*Ke Niu,Haiyang Yu,Zhuofan Chen,Zhengtao Yao,Weitao Jia,Xiaodong Ge,Jingqun Tang,Benlei Cui,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 论文提出了一种新的深度学习范式（CME-CAD），以提升自动生成高精度、可编辑CAD模型的能力，并发布了大规模的开源数据集CADExpert。


<details>
  <summary>Details</summary>
Motivation: 传统CAD建模复杂且自动化难度高，现有方法在生成高精度与可编辑模型上表现有限，依赖人工标注，难以满足工业设计的需求。

Method: 提出异构多专家协同强化学习（CME-CAD）训练范式，集成多模型优点；采用两阶段流程：多专家微调（MEFT）和多专家强化学习（MERL）；同时推出包含专家过程与高质量数据的大型公开数据集CADExpert。

Result: 新方法能生成高精度、约束兼容且可完全编辑的CAD模型，比依赖单一模型和单一输入的数据驱动方法更具优势。公开的CADExpert数据集大幅丰富了相关研究资源。

Conclusion: CME-CAD范式显著提升了CAD代码自动生成的精度和可编辑性，并推动工业CAD自动化和相关智能设计工具的发展。

Abstract: Computer-Aided Design (CAD) is essential in industrial design, but the complexity of traditional CAD modeling and workflows presents significant challenges for automating the generation of high-precision, editable CAD models. Existing methods that reconstruct 3D models from sketches often produce non-editable and approximate models that fall short of meeting the stringent requirements for precision and editability in industrial design. Moreover, the reliance on text or image-based inputs often requires significant manual annotation, limiting their scalability and applicability in industrial settings. To overcome these challenges, we propose the Heterogeneous Collaborative Multi-Expert Reinforcement Learning (CME-CAD) paradigm, a novel training paradigm for CAD code generation. Our approach integrates the complementary strengths of these models, facilitating collaborative learning and improving the model's ability to generate accurate, constraint-compatible, and fully editable CAD models. We introduce a two-stage training process: Multi-Expert Fine-Tuning (MEFT), and Multi-Expert Reinforcement Learning (MERL). Additionally, we present CADExpert, an open-source benchmark consisting of 17,299 instances, including orthographic projections with precise dimension annotations, expert-generated Chain-of-Thought (CoT) processes, executable CADQuery code, and rendered 3D models.

</details>


### [140] [Visual Language Hypothesis](https://arxiv.org/abs/2512.23335)
*Xiu Li*

Main category: cs.CV

TL;DR: 本文从结构和拓扑角度研究视觉表征学习，提出视觉表征空间应具有纤维丛结构，并推导出对表征方法和模型架构的理论要求。


<details>
  <summary>Details</summary>
Motivation: 动机在于更好理解视觉表征学习的结构基础：作者提出，视觉理解需要一种语义化的抽象，很多感知观察应映射到较少的离散语义状态，这需要从结构和拓扑层面重新审视表征空间。

Method: 作者构建了一种假设（视觉空间具有纤维丛结构），并据此推导该结构对模型语义抽象能力的要求，探索了如何用监督信号或多模态对齐实现非同胚的语义不变性，并分析了模型如何通过拓扑改变支持语义离散化过程。

Result: 一方面，语义商空间$X/G$不能仅通过光滑变形从观测空间$X$获得，需要外部监督显式建立语义等价关系；另一方面，模型架构必须能“扩张-收缩”空间以支持语义抽象和拓扑转变，这解释了现有判别和多模态大模型结构的经验规律。

Conclusion: 本文提供了一个结构/拓扑视角的理论框架，解释了视觉表征学习中语义抽象的必要条件。该框架不仅与经验观察和理论分析一致，也为未来表征学习模型设计提供了有益思路。

Abstract: We study visual representation learning from a structural and topological perspective. We begin from a single hypothesis: that visual understanding presupposes a semantic language for vision, in which many perceptual observations correspond to a small number of discrete semantic states. Together with widely assumed premises on transferability and abstraction in representation learning, this hypothesis implies that the visual observation space must be organized in a fiber bundle like structure, where nuisance variation populates fibers and semantics correspond to a quotient base space. From this structure we derive two theoretical consequences. First, the semantic quotient $X/G$ is not a submanifold of $X$ and cannot be obtained through smooth deformation alone, semantic invariance requires a non-homeomorphic, discriminative target, for example, supervision via labels, cross instance identification, or multimodal alignment that supplies explicit semantic equivalence. Second, we show that approximating the quotient also places structural demands on the model architecture. Semantic abstraction requires not only an external semantic target, but a representation mechanism capable of supporting topology change: an expand-and-snap process in which the manifold is first geometrically expanded to separate structure and then collapsed to form discrete semantic regions. We emphasize that these results are interpretive rather than prescriptive: the framework provides a topological lens that aligns with empirical regularities observed in large-scale discriminative and multimodal models, and with classical principles in statistical learning theory.

</details>


### [141] [CountGD++: Generalized Prompting for Open-World Counting](https://arxiv.org/abs/2512.23351)
*Niki Amini-Naieni,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本文提出了更加灵活和精准的目标计数方法，支持用文本和视觉示例灵活指定“计数什么”与“不计数什么”，并通过自动标注和多模态融合提升了开放世界计数模型的能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动计数方法对于目标指定方式有限，人们希望不仅能灵活指定计数对象，也能明确排除某些对象（即“不计数”），但现有方法无法实现这一需求，且视觉示例需要手工注释，效率低。

Method: 作者提出了三大技术创新：1）支持用文本和/或视觉实例灵活指定不计数的对象；2）引入‘伪样本（pseudo-exemplars）’，可在推理时自动生成和标注视觉示例；3）模型可直接利用自然和合成图像中的视觉样本，并结合多模态方法。同时，将新模型CountGD++嵌入LLM协作，实现跨模态增强。

Result: 新方法在多个数据集上均显著提升了计数的准确性、效率和泛化能力。实验结果证明了其优越性能，并验证了用户指定灵活性带来的实际改进。

Conclusion: 本文扩展了自动计数的提示能力，实现了对计数和不计数目标的灵活表达，并极大提升了多模态开放世界下目标计数的效果。

Abstract: The flexibility and accuracy of methods for automatically counting objects in images and videos are limited by the way the object can be specified. While existing methods allow users to describe the target object with text and visual examples, the visual examples must be manually annotated inside the image, and there is no way to specify what not to count. To address these gaps, we introduce novel capabilities that expand how the target object can be specified. Specifically, we extend the prompt to enable what not to count to be described with text and/or visual examples, introduce the concept of `pseudo-exemplars' that automate the annotation of visual examples at inference, and extend counting models to accept visual examples from both natural and synthetic external images. We also use our new counting model, CountGD++, as a vision expert agent for an LLM. Together, these contributions expand the prompt flexibility of multi-modal open-world counting and lead to significant improvements in accuracy, efficiency, and generalization across multiple datasets. Code is available at https://github.com/niki-amini-naieni/CountGDPlusPlus.

</details>


### [142] [SpatialMosaic: A Multiview VLM Dataset for Partial Visibility](https://arxiv.org/abs/2512.23365)
*Kanghee Lee,Injae Lee,Minseok Kwak,Kwonyoung Ryu,Jungi Hong,Jaesik Park*

Main category: cs.CV

TL;DR: 本论文提出了SpatialMosaic数据集与SpatialMosaic-Bench基准，用于提升多视角下的空间推理能力，并提出混合架构SpatialMosaicVLM，将3D重建模型集成到视觉-语言模型中。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型虽在空间理解与推理上展现潜力，但过度依赖预构3D表示或第三方重建工具，导致扩展性和实际应用受限，且在视野受限、遮挡低重叠等真实场景下的空间推理问题研究不足。

Method: 作者提出了一个可扩展的多视角数据生成与标注流程，构建了含有200万QA对的SpatialMosaic数据集；开发了100万个QA的SpatialMosaic-Bench基准，覆盖6种多视角空间推理任务；还提出SpatialMosaicVLM，将3D重建模型作为几何编码器集成进VLM中，提升对复杂场景的空间理解能力。

Result: 大量实验显示，所构建的数据集与VQA任务能有效提升多视角条件下空间推理表现，验证了数据生成与标注流程的真实有效性和多样性。

Conclusion: SpatialMosaic及其基准推动了现实场景下多视角空间推理研究，其混合架构能显著增强VLM的鲁棒空间推理能力，有望拓展至更多实际应用领域。

Abstract: The rapid progress of Multimodal Large Language Models (MLLMs) has unlocked the potential for enhanced 3D scene understanding and spatial reasoning. However, existing approaches often rely on pre-constructed 3D representations or off-the-shelf reconstruction pipelines, which constrain scalability and real-world applicability. A recent line of work explores learning spatial reasoning directly from multi-view images, enabling Vision-Language Models (VLMs) to understand 3D scenes without explicit 3D reconstructions. Nevertheless, key challenges that frequently arise in real-world environments, such as partial visibility, occlusion, and low-overlap conditions that require spatial reasoning from fragmented visual cues, remain under-explored. To address these limitations, we propose a scalable multi-view data generation and annotation pipeline that constructs realistic spatial reasoning QAs, resulting in SpatialMosaic, a comprehensive instruction-tuning dataset featuring 2M QA pairs. We further introduce SpatialMosaic-Bench, a challenging benchmark for evaluating multi-view spatial reasoning under realistic and challenging scenarios, consisting of 1M QA pairs across 6 tasks. In addition, we present SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning. Extensive experiments demonstrate that our proposed dataset and VQA tasks effectively enhance spatial reasoning under challenging multi-view conditions, validating the effectiveness of our data generation pipeline in constructing realistic and diverse QA pairs. Code and dataset will be available soon.

</details>


### [143] [MGCA-Net: Multi-Graph Contextual Attention Network for Two-View Correspondence Learning](https://arxiv.org/abs/2512.23369)
*Shuyuan Lin,Mengtin Lo,Haosheng Chen,Yanjie Liang,Qiangqiang Wu*

Main category: cs.CV

TL;DR: 本文提出了MGCA-Net，一种用于双视图匹配的新型网络，在现有方法中表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有的双视图匹配方法在局部几何建模和跨阶段信息优化方面存在不足，导致匹配结果鲁棒性差、几何约束捕获不准。

Method: MGCA-Net包括两个核心模块：1）CGA模块，利用自适应注意力结合空间位置和特征信息，提升对局部及全局几何关系的建模能力；2）CSMGC模块，采用跨阶段稀疏图构建几何一致性，强化多阶段的信息整合。

Result: 在YFCC100M和SUN3D两个数据集上的实验表明，MGCA-Net在异常匹配剔除和相机位姿估计任务上显著优于现有SOTA方法。

Conclusion: MGCA-Net提升了双视图匹配的几何建模与鲁棒性，对相关视觉任务有明显改进效果。

Abstract: Two-view correspondence learning is a key task in computer vision, which aims to establish reliable matching relationships for applications such as camera pose estimation and 3D reconstruction. However, existing methods have limitations in local geometric modeling and cross-stage information optimization, which make it difficult to accurately capture the geometric constraints of matched pairs and thus reduce the robustness of the model. To address these challenges, we propose a Multi-Graph Contextual Attention Network (MGCA-Net), which consists of a Contextual Geometric Attention (CGA) module and a Cross-Stage Multi-Graph Consensus (CSMGC) module. Specifically, CGA dynamically integrates spatial position and feature information via an adaptive attention mechanism and enhances the capability to capture both local and global geometric relationships. Meanwhile, CSMGC establishes geometric consensus via a cross-stage sparse graph network, ensuring the consistency of geometric information across different stages. Experimental results on two representative YFCC100M and SUN3D datasets show that MGCA-Net significantly outperforms existing SOTA methods in the outlier rejection and camera pose estimation tasks. Source code is available at http://www.linshuyuan.com.

</details>


### [144] [NeXT-IMDL: Build Benchmark for NeXT-Generation Image Manipulation Detection & Localization](https://arxiv.org/abs/2512.23374)
*Yifei Li,Haoyuan He,Yu Zheng,Bingyao Yu,Wenzhao Zheng,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出了NeXT-IMDL基准，用以全面诊断和评估图像篡改检测模型在多样化AI生成内容下的泛化能力，发现现有模型在复杂场景中泛化性能普遍较差。


<details>
  <summary>Details</summary>
Motivation: 随着易用的图像编辑工具普及，图像篡改变得普遍，检测和定位也随之变得更加重要。然而，目前的研究多采用跨数据集评估，未能真实反映模型在多元、复杂AI生成内容场景下的泛化能力，造成了对进展的误判。

Method: 作者提出了NeXT-IMDL大规模诊断基准，构建了包含四个基本轴（编辑模型、篡改类型、内容语义、伪造粒度）的数据集，并设计了五种严格的跨维度泛化评估协议，对11个主流模型进行了系统测试。

Result: 实验证明：虽然现有模型在原始设定下表现良好，但在NeXT-IMDL设计的更具复杂性和现实性的评估协议下，普遍出现系统性失效和性能大幅下降。

Conclusion: NeXT-IMDL揭示了当前IMDL方法的泛化瓶颈，为未来开发真正鲁棒的新一代图像篡改检测模型提供了诊断工具和研究方向。

Abstract: The accessibility surge and abuse risks of user-friendly image editing models have created an urgent need for generalizable, up-to-date methods for Image Manipulation Detection and Localization (IMDL). Current IMDL research typically uses cross-dataset evaluation, where models trained on one benchmark are tested on others. However, this simplified evaluation approach conceals the fragility of existing methods when handling diverse AI-generated content, leading to misleading impressions of progress. This paper challenges this illusion by proposing NeXT-IMDL, a large-scale diagnostic benchmark designed not just to collect data, but to probe the generalization boundaries of current detectors systematically. Specifically, NeXT-IMDL categorizes AIGC-based manipulations along four fundamental axes: editing models, manipulation types, content semantics, and forgery granularity. Built upon this, NeXT-IMDL implements five rigorous cross-dimension evaluation protocols. Our extensive experiments on 11 representative models reveal a critical insight: while these models perform well in their original settings, they exhibit systemic failures and significant performance degradation when evaluated under our designed protocols that simulate real-world, various generalization scenarios. By providing this diagnostic toolkit and the new findings, we aim to advance the development towards building truly robust, next-generation IMDL models.

</details>


### [145] [SoulX-LiveTalk Technical Report](https://arxiv.org/abs/2512.23379)
*Le Shen,Qiao Qian,Tan Yu,Ke Zhou,Tianhang Yu,Yu Zhan,Zhenjie Wang,Ming Tao,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

TL;DR: 本论文提出了SoulX-LiveTalk系统，实现了大规模扩散模型在实时、无限时长、音频驱动下生成高保真数字人头像。通过创新技术克服了算力与延迟的难题，实现了0.87秒启动延迟和32FPS的实用性能。


<details>
  <summary>Details</summary>
Motivation: 现有实时数字人生成模型受限于算力与低延迟的矛盾，往往牺牲视觉质量（如仅采用单向注意力或缩减模型规模），难以满足实际高保真交互需求。

Method: 提出Self-correcting Bidirectional Distillation策略，保留chunk级别的双向注意力以提升时空一致性和视觉细节。引入Multi-step Retrospective Self-Correction机制，提升长时段生成稳定性，自动纠正累积误差。并实现了全链路推理加速，包括混合序列并行、Parallel VAE和核级优化。

Result: SoulX-LiveTalk首次在14B参数规模下，实现0.87秒启动延迟、32帧/秒，兼顾高保真度与实时性，超越现有系统性能。

Conclusion: 所提系统为高保真数字人实时生成树立新标杆，兼顾视觉效果与交互时延，对相关应用具有重要意义。

Abstract: Deploying massive diffusion models for real-time, infinite-duration, audio-driven avatar generation presents a significant engineering challenge, primarily due to the conflict between computational load and strict latency constraints. Existing approaches often compromise visual fidelity by enforcing strictly unidirectional attention mechanisms or reducing model capacity. To address this problem, we introduce \textbf{SoulX-LiveTalk}, a 14B-parameter framework optimized for high-fidelity real-time streaming. Diverging from conventional unidirectional paradigms, we use a \textbf{Self-correcting Bidirectional Distillation} strategy that retains bidirectional attention within video chunks. This design preserves critical spatiotemporal correlations, significantly enhancing motion coherence and visual detail. To ensure stability during infinite generation, we incorporate a \textbf{Multi-step Retrospective Self-Correction Mechanism}, enabling the model to autonomously recover from accumulated errors and preventing collapse. Furthermore, we engineered a full-stack inference acceleration suite incorporating hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations. Extensive evaluations confirm that SoulX-LiveTalk is the first 14B-scale system to achieve a \textbf{sub-second start-up latency (0.87s)} while reaching a real-time throughput of \textbf{32 FPS}, setting a new standard for high-fidelity interactive digital human synthesis.

</details>


### [146] [SOFTooth: Semantics-Enhanced Order-Aware Fusion for Tooth Instance Segmentation](https://arxiv.org/abs/2512.23411)
*Xiaolan Li,Wanquan Liu,Pengcheng Li,Pengyu Jie,Chenqiang Gao*

Main category: cs.CV

TL;DR: 本文提出了一种名为SOFTooth的新方法，通过结合2D语义和3D几何信息，有效提升了三维牙齿实例分割的准确性，尤其在复杂和罕见牙齿情形（如智齿）上表现优异。


<details>
  <summary>Details</summary>
Motivation: 三维牙齿实例分割面临如牙列拥挤、边界模糊、缺牙和罕见智齿等挑战。传统3D方法容易出现分割泄漏、中心漂移和身份不一致等问题，而2D基础模型难以直接用于实际3D临床场景。因此，亟需一种有效融合2D和3D信息的方案。

Method: 提出SOFTooth框架，将冻结状态下的2D语义嵌入注入到3D点云特征中，并利用中心引导的掩膜细化和结合解剖学顺序的匈牙利匹配策略，实现了准确、一致的3D牙齿标签分配。整个流程无需2D掩膜的显式监督，也无需2D微调。

Result: 在3DTeethSeg'22数据集上，SOFTooth取得了目前最好（state-of-the-art）的整体精度和平均交并比，尤其在涉及智齿等复杂病例时表现出显著提升。

Conclusion: 丰富的2D语义信息可高效迁移到3D牙齿实例分割任务，无需2D模型微调即能提高3D分割性能，方法在复杂临床情形下展现出优越性。

Abstract: Three-dimensional (3D) tooth instance segmentation remains challenging due to crowded arches, ambiguous tooth-gingiva boundaries, missing teeth, and rare yet clinically important third molars. Native 3D methods relying on geometric cues often suffer from boundary leakage, center drift, and inconsistent tooth identities, especially for minority classes and complex anatomies. Meanwhile, 2D foundation models such as the Segment Anything Model (SAM) provide strong boundary-aware semantics, but directly applying them in 3D is impractical in clinical workflows. To address these issues, we propose SOFTooth, a semantics-enhanced, order-aware 2D-3D fusion framework that leverages frozen 2D semantics without explicit 2D mask supervision. First, a point-wise residual gating module injects occlusal-view SAM embeddings into 3D point features to refine tooth-gingiva and inter-tooth boundaries. Second, a center-guided mask refinement regularizes consistency between instance masks and geometric centroids, reducing center drift. Furthermore, an order-aware Hungarian matching strategy integrates anatomical tooth order and center distance into similarity-based assignment, ensuring coherent labeling even under missing or crowded dentitions. On 3DTeethSeg'22, SOFTooth achieves state-of-the-art overall accuracy and mean IoU, with clear gains on cases involving third molars, demonstrating that rich 2D semantics can be effectively transferred to 3D tooth instance segmentation without 2D fine-tuning.

</details>


### [147] [Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment](https://arxiv.org/abs/2512.23413)
*Henglin Liu,Nisha Huang,Chang Liu,Jiangpeng Yan,Huijuan Huang,Jixuan Ying,Tong-Yee Lee,Pengfei Wan,Xiangyang Ji*

Main category: cs.CV

TL;DR: 该论文提出了一种新的美学质量评估方法ArtQuant，并构建了大规模多维度的美学描述数据集RAD，实现了美学评估精度和效率的提升。


<details>
  <summary>Details</summary>
Motivation: AIGC(人工智能生成内容)的美学评估需要贴近人类审美标准，但目前的数据稀缺且不均衡，模型方法存在割裂和难以处理长文本描述的问题。

Method: 1) 构建了Refined Aesthetic Description (RAD)数据集，通过迭代管道自动生成70k多维度结构化美学描述，解决数据稀缺与注释昂贵问题；2) 提出ArtQuant框架，利用大语言模型(LLM)生成联合美学描述，耦合各美学维度，并提升对长文本的建模能力，同时通过理论分析证明框架有效性。

Result: ArtQuant在多个公开数据集上表现优异，达到最新最优结果，同时训练所需epoch减少至传统的33%，显著提高训练效率和模型效果。

Conclusion: 通过大规模、多维度的美学数据集RAD和ArtQuant方法，有效缩小了艺术图像与审美判断之间的认知鸿沟，并促进AIGC的美学评估研究，论文承诺公开全部代码和数据集以推动后续发展。

Abstract: The aesthetic quality assessment task is crucial for developing a human-aligned quantitative evaluation system for AIGC. However, its inherently complex nature, spanning visual perception, cognition, and emotion, poses fundamental challenges. Although aesthetic descriptions offer a viable representation of this complexity, two critical challenges persist: (1) data scarcity and imbalance: existing dataset overly focuses on visual perception and neglects deeper dimensions due to the expensive manual annotation; and (2) model fragmentation: current visual networks isolate aesthetic attributes with multi-branch encoder, while multimodal methods represented by contrastive learning struggle to effectively process long-form textual descriptions. To resolve challenge (1), we first present the Refined Aesthetic Description (RAD) dataset, a large-scale (70k), multi-dimensional structured dataset, generated via an iterative pipeline without heavy annotation costs and easy to scale. To address challenge (2), we propose ArtQuant, an aesthetics assessment framework for artistic images which not only couples isolated aesthetic dimensions through joint description generation, but also better models long-text semantics with the help of LLM decoders. Besides, theoretical analysis confirms this symbiosis: RAD's semantic adequacy (data) and generation paradigm (model) collectively minimize prediction entropy, providing mathematical grounding for the framework. Our approach achieves state-of-the-art performance on several datasets while requiring only 33% of conventional training epochs, narrowing the cognitive gap between artistic images and aesthetic judgment. We will release both code and dataset to support future research.

</details>


### [148] [DriveLaW:Unifying Planning and Video Generation in a Latent Driving World](https://arxiv.org/abs/2512.23421)
*Tianze Xia,Yongkang Li,Lijun Zhou,Jingfeng Yao,Kaixin Xiong,Haiyang Sun,Bing Wang,Kun Ma,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 本文提出了DriveLaW，一种将视频生成和运动规划统一的世界模型新范式，通过将视频生成器的潜在表示注入规划器，实现了高质量预测与可靠轨迹规划的一致性，并在视频预测和规划基准上均取得了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶世界模型虽然重要，但普遍将世界预测与运动规划割裂，使得二者之间缺乏一致性，难以应对现实世界复杂的长尾问题，因此亟需一种能够统一两者的方法。

Method: DriveLaW包括两个核心组件：DriveLaW-Video，负责基于表达力强的潜在空间进行高保真世界视频预测；DriveLaW-Act，通过扩散式规划器利用DriveLaW-Video的潜在变量生成一致且可靠的运动轨迹。两者通过三阶段逐步训练方法联合优化。

Result: DriveLaW在视频生成领域的FID指标提升了33.3%，FVD提升了1.8%，并在NAVSIM路径规划基准测试上刷新纪录，展现出在视频预测和行为规划上的领先性能。

Conclusion: DriveLaW通过统一世界建模与运动规划，实现了视频生成与轨迹规划的高度一致性，推动了自动驾驶领域世界模型性能的重大提升。

Abstract: World models have become crucial for autonomous driving, as they learn how scenarios evolve over time to address the long-tail challenges of the real world. However, current approaches relegate world models to limited roles: they operate within ostensibly unified architectures that still keep world prediction and motion planning as decoupled processes. To bridge this gap, we propose DriveLaW, a novel paradigm that unifies video generation and motion planning. By directly injecting the latent representation from its video generator into the planner, DriveLaW ensures inherent consistency between high-fidelity future generation and reliable trajectory planning. Specifically, DriveLaW consists of two core components: DriveLaW-Video, our powerful world model that generates high-fidelity forecasting with expressive latent representations, and DriveLaW-Act, a diffusion planner that generates consistent and reliable trajectories from the latent of DriveLaW-Video, with both components optimized by a three-stage progressive training strategy. The power of our unified paradigm is demonstrated by new state-of-the-art results across both tasks. DriveLaW not only advances video prediction significantly, surpassing best-performing work by 33.3% in FID and 1.8% in FVD, but also achieves a new record on the NAVSIM planning benchmark.

</details>


### [149] [Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision](https://arxiv.org/abs/2512.23426)
*Dohyun Kim,Seungwoo Lyu,Seung Wook Kim,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: 提出了一种新的扩散模型优化方法DDSPO，通过对比不同条件下的模型输出，自动获得偏好信号，提升文图一致性和画面质量，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有的图生成扩散模型难以精准捕捉用户意图或保持风格一致，且依赖人工偏好标注既昂贵又易出错。因此，急需低成本、高效率的偏好训练方法。

Method: 提出Direct Diffusion Score Preference Optimization（DDSPO），直接利用每一步去噪过程中表现优劣的策略差异提供稠密监督。具体做法是用预训练参考模型对原始与语义退化提示词分别生成图片，自动生成偏好信号，实现无监督的分数空间偏好优化。

Result: 实验证明DDSPO可有效提升文本-图像对齐和视觉质量，在同类偏好训练方法中表现更好或相当，并极大减少了人工标注需求。

Conclusion: DDSPO利用自动偏好信号，解决了偏好训练对人工数据依赖的问题，提升了扩散模型的生成质量和文图一致性，为低资源环境下的模型偏好优化提供了新方向。

Abstract: Diffusion models have achieved impressive results in generative tasks such as text-to-image synthesis, yet they often struggle to fully align outputs with nuanced user intent and maintain consistent aesthetic quality. Existing preference-based training methods like Diffusion Direct Preference Optimization help address these issues but rely on costly and potentially noisy human-labeled datasets. In this work, we introduce Direct Diffusion Score Preference Optimization (DDSPO), which directly derives per-timestep supervision from winning and losing policies when such policies are available. Unlike prior methods that operate solely on final samples, DDSPO provides dense, transition-level signals across the denoising trajectory. In practice, we avoid reliance on labeled data by automatically generating preference signals using a pretrained reference model: we contrast its outputs when conditioned on original prompts versus semantically degraded variants. This practical strategy enables effective score-space preference supervision without explicit reward modeling or manual annotations. Empirical results demonstrate that DDSPO improves text-image alignment and visual quality, outperforming or matching existing preference-based methods while requiring significantly less supervision. Our implementation is available at: https://dohyun-as.github.io/DDSPO

</details>


### [150] [Towards Integrating Uncertainty for Domain-Agnostic Segmentation](https://arxiv.org/abs/2512.23427)
*Jesse Brouwers,Xiaoyan Xing,Alexander Timans*

Main category: cs.CV

TL;DR: 本文提出UncertSAM基准，对Segment Anything Model（SAM）在不同困难分割场景下进行测试，探索不确定性建模是否能提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管SAM在零样本分割方面表现突出，但在知识有限或领域转移时表现仍有限。作者希望通过引入不确定性量化机制改进模型对不同分割困难的适应能力。

Method: 1）建立UncertSAM基准（8个具特殊挑战的数据集）；2）评估多种轻量后处理的不确定性估计方法；3）加入不确定性引导的分割结果细化步骤。

Result: 最后一层拉普拉斯近似方法产生的不确定性估计与分割错误相关性良好，能反映分割偏差。有关不确定性引导细化的实验具有一定提升但仍初步。

Conclusion: 将不确定性引入分割模型有望提升其稳健性及领域无关的表现，所提出的基准和代码已公开，推动后续研究。

Abstract: Foundation models for segmentation such as the Segment Anything Model (SAM) family exhibit strong zero-shot performance, but remain vulnerable in shifted or limited-knowledge domains. This work investigates whether uncertainty quantification can mitigate such challenges and enhance model generalisability in a domain-agnostic manner. To this end, we (1) curate UncertSAM, a benchmark comprising eight datasets designed to stress-test SAM under challenging segmentation conditions including shadows, transparency, and camouflage; (2) evaluate a suite of lightweight, post-hoc uncertainty estimation methods; and (3) assess a preliminary uncertainty-guided prediction refinement step. Among evaluated approaches, a last-layer Laplace approximation yields uncertainty estimates that correlate well with segmentation errors, indicating a meaningful signal. While refinement benefits are preliminary, our findings underscore the potential of incorporating uncertainty into segmentation models to support robust, domain-agnostic performance. Our benchmark and code are made publicly available.

</details>


### [151] [Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification](https://arxiv.org/abs/2512.23436)
*Mustafa Demetgul,Sanja Lazarova Molnar*

Main category: cs.CV

TL;DR: 本文提出了一种基于实时天气和路面数据的路面状况监测系统，通过手机摄像头和加速度传感器收集数据，利用多种深度学习方法进行路面分类，准确率超过95%。


<details>
  <summary>Details</summary>
Motivation: 传统的路面监测方法成本高、效率低、系统性差，因此亟需一种高效、便捷、实时的路面状况识别方法以提升车辆行驶安全和自动控制能力。

Method: 作者收集了校园周边道路的摄像头照片和加速度数据，将加速度同样作为图像输入，采用多种经典深度学习网络（如Alexnet、LeNet、VGG、Resnet）对五类路面进行分类，并对基于加速度和基于摄像数据的方法进行性能比较。结合模糊逻辑，根据天气和时间选择传感器数据进行分类。

Result: 提出的方法对五种路面类型分类时准确率超过95%；不同网络结构和输入源的性能也进行了对比分析。

Conclusion: 基于摄像头和加速度数据的深度学习模型能够实现高精度的实时路面状况监测，结合天气和时间条件选择传感器，可提升系统性能和适用性。

Abstract: Monitoring states of road surfaces provides valuable information for the planning and controlling vehicles and active vehicle control systems. Classical road monitoring methods are expensive and unsystematic because they require time for measurements. This article proposes an real time system based on weather conditional data and road surface condition data. For this purpose, we collected data with a mobile phone camera on the roads around the campus of the Karlsruhe Institute of Technology. We tested a large number of different image-based deep learning algorithms for road classification. In addition, we used road acceleration data along with road image data for training by using them as images. We compared the performances of acceleration-based and camera image-based approaches. The performances of the simple Alexnet, LeNet, VGG, and Resnet algorithms were compared as deep learning algorithms. For road condition classification, 5 classes were considered: asphalt, damaged asphalt, gravel road, damaged gravel road, pavement road and over 95% accuracy performance was achieved. It is also proposed to use the acceleration or the camera image to classify the road surface according to the weather and the time of day using fuzzy logic.

</details>


### [152] [RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction](https://arxiv.org/abs/2512.23437)
*Shuhong Liu,Chenyu Bao,Ziteng Cui,Yun Liu,Xuangeng Chu,Lin Gu,Marcos V. Conde,Ryo Umagami,Tomohiro Hashimoto,Zijian Hu,Tianhan Xu,Yuan Gan,Yusuke Kurose,Tatsuya Harada*

Main category: cs.CV

TL;DR: 本文提出了RealX3D基准，针对多视角视觉恢复和三维重建在各种物理退化情况下的表现进行了评测，并发现当前方法在真实环境中易受物理腐蚀影响，重建质量大幅下降。


<details>
  <summary>Details</summary>
Motivation: 当前多视角视觉恢复与三维重建方法普遍基于模拟或干净数据，缺乏对真实世界中物理退化（例如：光照、散射、遮挡和模糊）影响的系统性评测，导致这些方法在复杂环境下表现不佳。Authors想通过建立真实捕获且带有多级物理退化的数据集，揭示现有方法的脆弱性，促进更鲁棒的视觉恢复与三维重建研究。

Method: RealX3D数据集以统一的采集协议获取不同物理退化（光照、散射、遮挡、模糊、每类多个等级）下的多视角低质量（LQ）与高质量（GT）对齐图像。每个场景还包括高分辨率捕获、RAW图像与激光建模获取的世界级网格及深度。利用该基准，作者对多种优化型和前馈型重建算法在不同退化条件下进行了广泛评测。

Result: 实验表明，在各种物理退化条件下，现有多视角视觉恢复和三维重建方法的重建质量都有显著下降，显示其在真实复杂环境下的脆弱性。

Conclusion: RealX3D提供了系统性、真实场景下的多视角视觉重建评测平台，能有效促进对物理退化条件下方法鲁棒性的研究和改进。

Abstract: We introduce RealX3D, a real-capture benchmark for multi-view visual restoration and 3D reconstruction under diverse physical degradations. RealX3D groups corruptions into four families, including illumination, scattering, occlusion, and blurring, and captures each at multiple severity levels using a unified acquisition protocol that yields pixel-aligned LQ/GT views. Each scene includes high-resolution capture, RAW images, and dense laser scans, from which we derive world-scale meshes and metric depth. Benchmarking a broad range of optimization-based and feed-forward methods shows substantial degradation in reconstruction quality under physical corruptions, underscoring the fragility of current multi-view pipelines in real-world challenging environments.

</details>


### [153] [CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models](https://arxiv.org/abs/2512.23453)
*Zongsheng Cao,Yangfan He,Anran Liu,Jun Xie,Feng Chen,Zepeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CoFi-Dec的解码框架，通过多层级视觉条件约束，有效减少了多模态大模型（LVLMs）产生幻觉式内容的问题，并在多个基准上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大模型在视觉与语言的理解与生成方面取得了显著进步，但它们仍然容易生成与视觉输入不一致的错误内容（幻觉），严重影响实际应用的可靠性。因此，提升LVLM在多模态一致性方面的性能成为亟需解决的问题。

Method: 提出了CoFi-Dec，一种无需额外训练的组合式解码框架。该方法模拟人类视觉由全局到细节的识别过程，首先利用图片的粗粒度和细粒度视角分别生成两组文本回复，然后将这些回复通过文生图模型转为合成图片，形成多层次的视觉假设。最后，采用基于Wasserstein距离的融合机制统一各层次预测，使解码结果在语义和细节上兼具一致性与准确性。

Result: 在六个专注于幻觉检测的基准数据集上，CoFi-Dec显著减少了实体级和语义级的幻觉现象，其效果优于现有各种解码策略。

Conclusion: CoFi-Dec框架无需额外训练、模型无关，能够灵活集成到各种LVLMs中，有效提升生成内容的可信度和真实一致性。

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose \textbf{CoFi-Dec}, a training-free decoding framework that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the human visual process from global scene perception to detailed inspection, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mechanism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec substantially reduces both entity-level and semantic-level hallucinations, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seamlessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec.

</details>


### [154] [Automated river gauge plate reading using a hybrid object detection and generative AI framework in the Limpopo River Basin](https://arxiv.org/abs/2512.23454)
*Kayathri Vigneswaran,Hugo Retief,Jai Clifford Holmes,Mariangel Garcia Andarcia,Hansaka Tennakoon*

Main category: cs.CV

TL;DR: 提出了一种结合视觉检测和多模态大模型的自动河流水尺读取新框架，实现了高精度、高效率的水位读取。


<details>
  <summary>Details</summary>
Motivation: 河流水位的连续准确监测对防洪、水资源管理及生态保护至关重要，传统方法存在人工误差大和环境受限等问题，亟需自动化、智能化解决方案。

Method: 本研究提出了一个混合框架，融合了视觉基础的水线检测、YOLOv8尺度提取以及大多模态语言模型（如GPT 4o和Gemini 2.0 Flash），流程包括图像预处理、注释、水线检测、刻度间距估计与数值读取提取。

Result: 实验表明，水线检测精确率达94.24%，F1分数83.64%；刻度间距检测为后续读数提取提供了准确的几何校准。结合刻度间距的元数据后，LLMs预测性能大幅提升，其中Gemini第二阶段在理想图像条件下，平均绝对误差5.43 cm，均方根误差8.58 cm，R²达0.84。

Conclusion: 提出的方法具有可扩展性、高效性和可靠性，在图像质量良好的情况下，能够实现自动化、实时的河流水尺数字化，推动水资源管理智能化。

Abstract: Accurate and continuous monitoring of river water levels is essential for flood forecasting, water resource management, and ecological protection. Traditional hydrological observation methods are often limited by manual measurement errors and environmental constraints. This study presents a hybrid framework integrating vision based waterline detection, YOLOv8 pose scale extraction, and large multimodal language models (GPT 4o and Gemini 2.0 Flash) for automated river gauge plate reading. The methodology involves sequential stages of image preprocessing, annotation, waterline detection, scale gap estimation, and numeric reading extraction. Experiments demonstrate that waterline detection achieved high precision of 94.24 percent and an F1 score of 83.64 percent, while scale gap detection provided accurate geometric calibration for subsequent reading extraction. Incorporating scale gap metadata substantially improved the predictive performance of LLMs, with Gemini Stage 2 achieving the highest accuracy, with a mean absolute error of 5.43 cm, root mean square error of 8.58 cm, and R squared of 0.84 under optimal image conditions. Results highlight the sensitivity of LLMs to image quality, with degraded images producing higher errors, and underscore the importance of combining geometric metadata with multimodal artificial intelligence for robust water level estimation. Overall, the proposed approach offers a scalable, efficient, and reliable solution for automated hydrological monitoring, demonstrating potential for real time river gauge digitization and improved water resource management.

</details>


### [155] [Deterministic Image-to-Image Translation via Denoising Brownian Bridge Models with Dual Approximators](https://arxiv.org/abs/2512.23463)
*Bohan Xiao,Peiyong Wang,Qisheng He,Ming Dong*

Main category: cs.CV

TL;DR: 本文提出了一种新型生成模型Dual-approx Bridge，通过引入Brownian bridge动力学和双神经网络逼近器，实现高保真、低方差的图像到图像（I2I）确定性转换，并在多个数据集上效果优越。


<details>
  <summary>Details</summary>
Motivation: 在I2I任务（如图像超分辨率）中，现有生成模型难以同时保证输出结果的高质量和与真实图像的一致性，且部分模型存在输出不稳定、不可预测的问题。因此需要一种能够生成高质量且与GT高度一致、输出可预测的确定性生成方法。

Method: 作者提出了Dual-approx Bridge模型。该方法结合了Brownian bridge随机动力学，通过建立正向与反向两个神经网络逼近器，分别模拟从输入图像到噪声再到目标输出的过程，实现了低方差的确定性I2I转换。

Result: 在多个基准数据集（涵盖图像生成和超分辨率任务）上的实验显示，Dual-approx Bridge在图像质量和与真实图像一致性指标上都超过了现有的随机和确定性基线方法。

Conclusion: Dual-approx Bridge模型通过创新性的动力学建模和双网络设计，有效提升了I2I任务的输出质量和确定性，为相关应用提供了可靠且高保真的解决方案。

Abstract: Image-to-Image (I2I) translation involves converting an image from one domain to another. Deterministic I2I translation, such as in image super-resolution, extends this concept by guaranteeing that each input generates a consistent and predictable output, closely matching the ground truth (GT) with high fidelity. In this paper, we propose a denoising Brownian bridge model with dual approximators (Dual-approx Bridge), a novel generative model that exploits the Brownian bridge dynamics and two neural network-based approximators (one for forward and one for reverse process) to produce faithful output with negligible variance and high image quality in I2I translations. Our extensive experiments on benchmark datasets including image generation and super-resolution demonstrate the consistent and superior performance of Dual-approx Bridge in terms of image quality and faithfulness to GT when compared to both stochastic and deterministic baselines. Project page and code: https://github.com/bohan95/dual-app-bridge

</details>


### [156] [HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation](https://arxiv.org/abs/2512.23464)
*Yuxin Wen,Qing Shuai,Di Kang,Jing Li,Cheng Wen,Yue Qian,Ningxin Jiao,Changhai Chen,Weijie Chen,Yiran Wang,Jinkun Guo,Dongyue An,Han Liu,Yanyu Tong,Chao Zhang,Qing Guo,Juan Chen,Qiao Zhang,Youyi Zhang,Zihao Yao,Cheng Zhang,Hong Duan,Xiaoping Wu,Qi Chen,Fei Cheng,Liang Dong,Peng He,Hao Zhang,Jiaxin Lin,Chao Zhang,Zhongyi Fan,Yifan Li,Zhichao Hu,Yuhong Liu,Linus,Jie Jiang,Xiaolong Li,Linchao Bao*

Main category: cs.CV

TL;DR: HY-Motion 1.0是一套领先的大规模3D人类动作生成模型，可根据文本描述生成高质量动作，在开源基准上表现优异并公开发布。


<details>
  <summary>Details</summary>
Motivation: 现有3D动作生成模型规模小、效果有限，难以精准地根据文本指令生成丰富且高质量的人类动作。该领域缺乏足够大规模且有商业潜力的开源模型。

Method: 提出首个数十亿参数级的基于Diffusion Transformer（DiT）和flow matching的动作生成模型。采用全面分阶段训练策略：先在超过3000小时动作数据进行大规模预训练，再对400小时高质量数据微调，并结合人类反馈和奖励模型的强化学习，保证模型既服从文本指令又生成高质量动作。同时，开发高效的数据清洗与配文流程，保证数据质量。

Result: 模型可覆盖超过200类动作、6大类场景，动作生成质量和文本对齐优于目前开源同类模型。展示了模型在指令执行能力和动作多样性上的突破。

Conclusion: HY-Motion 1.0推动了3D人类动作生成向大规模、开放源代码与商业应用方向迈进，为后续研究和产业化应用奠定了基础。

Abstract: We present HY-Motion 1.0, a series of state-of-the-art, large-scale, motion generation models capable of generating 3D human motions from textual descriptions. HY-Motion 1.0 represents the first successful attempt to scale up Diffusion Transformer (DiT)-based flow matching models to the billion-parameter scale within the motion generation domain, delivering instruction-following capabilities that significantly outperform current open-source benchmarks. Uniquely, we introduce a comprehensive, full-stage training paradigm -- including large-scale pretraining on over 3,000 hours of motion data, high-quality fine-tuning on 400 hours of curated data, and reinforcement learning from both human feedback and reward models -- to ensure precise alignment with the text instruction and high motion quality. This framework is supported by our meticulous data processing pipeline, which performs rigorous motion cleaning and captioning. Consequently, our model achieves the most extensive coverage, spanning over 200 motion categories across 6 major classes. We release HY-Motion 1.0 to the open-source community to foster future research and accelerate the transition of 3D human motion generation models towards commercial maturity.

</details>


### [157] [MCI-Net: A Robust Multi-Domain Context Integration Network for Point Cloud Registration](https://arxiv.org/abs/2512.23472)
*Shuyuan Lin,Wenwu Peng,Junjie Huang,Qiang Qi,Miaohui Wang,Jian Weng*

Main category: cs.CV

TL;DR: 本文提出了一种多域上下文整合网络（MCI-Net），通过多角度聚合点云上下文信息，提升点云配准的特征表达能力和鲁棒性，显著优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的点云配准方法通常采用欧氏邻域策略提取特征，难以捕捉点云中的隐式语义和结构一致性，影响配准精度和泛化能力。

Method: 1. 提出图邻域聚合模块，构建全局图模型捕捉点云整体结构关系；2. 提出逐步上下文交互模块，通过域内特征解耦和跨域信息交互增强特征判别能力；3. 设计动态内点选择方法，利用多轮位姿估计残差信息自适应优化内点权重。

Result: 在室内RGB-D和室外LiDAR点云数据集上进行了大量实验，结果显示MCI-Net在3DMatch数据集上获得最高96.4%的配准召回率，显著超过现有SOTA方法。

Conclusion: 多域上下文信息整合能够有效提升点云配准特征表达和最终配准精度，所提MCI-Net具有很强的实用性和推广前景。

Abstract: Robust and discriminative feature learning is critical for high-quality point cloud registration. However, existing deep learning-based methods typically rely on Euclidean neighborhood-based strategies for feature extraction, which struggle to effectively capture the implicit semantics and structural consistency in point clouds. To address these issues, we propose a multi-domain context integration network (MCI-Net) that improves feature representation and registration performance by aggregating contextual cues from diverse domains. Specifically, we propose a graph neighborhood aggregation module, which constructs a global graph to capture the overall structural relationships within point clouds. We then propose a progressive context interaction module to enhance feature discriminability by performing intra-domain feature decoupling and inter-domain context interaction. Finally, we design a dynamic inlier selection method that optimizes inlier weights using residual information from multiple iterations of pose estimation, thereby improving the accuracy and robustness of registration. Extensive experiments on indoor RGB-D and outdoor LiDAR datasets show that the proposed MCI-Net significantly outperforms existing state-of-the-art methods, achieving the highest registration recall of 96.4\% on 3DMatch. Source code is available at http://www.linshuyuan.com.

</details>


### [158] [SC-Net: Robust Correspondence Learning via Spatial and Cross-Channel Context](https://arxiv.org/abs/2512.23473)
*Shuyuan Lin,Hailiang Liao,Qiang Qi,Junjie Huang,Taotao Lai,Jian Weng*

Main category: cs.CV

TL;DR: 本文提出了一种新型神经网络SC-Net，针对双视角对应学习任务优化，对运动场进行更精准的估计，并在多个基准数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有以CNN为主干的双视角对应方法在处理大视差和场景时，容易缺乏全局信息聚合能力，并导致运动场被过度平滑，精度受限。

Method: SC-Net网络设计包含三个关键模块：1）自适应聚焦正则化模块（AFR）提升位置意识和抗噪性，2）双向场调整模块（BFA）实现空间与通道维的全局信息融合与长距离依赖建模，3）位置感知恢复模块（PAR）用于从优化后的运动场中恢复精确且一致的运动向量。

Result: 在YFCC100M和SUN3D数据集上的大规模实验表明，SC-Net在相对位姿估计和异常值剔除等任务上优于最新的先进方法。

Conclusion: SC-Net有效整合空间和通道信息，解决了CNN骨干全局感受野有限和运动场过度平滑的问题，在实际评测中展现出优越的性能，是双视角对应学习任务的有力新方法。

Abstract: Recent research has focused on using convolutional neural networks (CNNs) as the backbones in two-view correspondence learning, demonstrating significant superiority over methods based on multilayer perceptrons. However, CNN backbones that are not tailored to specific tasks may fail to effectively aggregate global context and oversmooth dense motion fields in scenes with large disparity. To address these problems, we propose a novel network named SC-Net, which effectively integrates bilateral context from both spatial and channel perspectives. Specifically, we design an adaptive focused regularization module (AFR) to enhance the model's position-awareness and robustness against spurious motion samples, thereby facilitating the generation of a more accurate motion field. We then propose a bilateral field adjustment module (BFA) to refine the motion field by simultaneously modeling long-range relationships and facilitating interaction across spatial and channel dimensions. Finally, we recover the motion vectors from the refined field using a position-aware recovery module (PAR) that ensures consistency and precision. Extensive experiments demonstrate that SC-Net outperforms state-of-the-art methods in relative pose estimation and outlier removal tasks on YFCC100M and SUN3D datasets. Source code is available at http://www.linshuyuan.com.

</details>


### [159] [TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding](https://arxiv.org/abs/2512.23483)
*Zongsheng Cao,Yangfan He,Anran Liu,Feng Chen,Zepeng Wang,Jun Xie*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的新架构TV-RAG，以提升大规模视频语言模型对长视频的理解能力，兼顾时序与语义特征，显著优于多数现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频语言模型在处理长视频时难以捕捉细粒度的语义变化，时序窗口受限，并且主流检索方法忽略了多模态（视觉、音频、字幕）间的时序与语义联系，导致对长视频的推理能力有限。

Method: 本方法提出TV-RAG框架，包括：(1)时衰检索模块，通过引入显式时序偏移到文本-视频相似度计算中，实现时序对齐；(2)熵加权关键帧采样模块，选取信息密集且分布均匀的关键帧以减少冗余。这两个机制无需额外训练，可直接集成任意LVLM模型。

Result: TV-RAG在Video-MME、MLVU、LongVideoBench等多个公认的长视频基准上均优于大多数领先的基线模型，验证了其在长视频理解上的有效性。

Conclusion: TV-RAG为LVLM提供了轻量、经济的升级路径，显著提升了长视频推理能力，无需重新训练，具有广阔的实际应用前景。

Abstract: Large Video Language Models (LVLMs) have rapidly emerged as the focus of multimedia AI research. Nonetheless, when confronted with lengthy videos, these models struggle: their temporal windows are narrow, and they fail to notice fine-grained semantic shifts that unfold over extended durations. Moreover, mainstream text-based retrieval pipelines, which rely chiefly on surface-level lexical overlap, ignore the rich temporal interdependence among visual, audio, and subtitle channels. To mitigate these limitations, we propose TV-RAG, a training-free architecture that couples temporal alignment with entropy-guided semantics to improve long-video reasoning. The framework contributes two main mechanisms: \emph{(i)} a time-decay retrieval module that injects explicit temporal offsets into the similarity computation, thereby ranking text queries according to their true multimedia context; and \emph{(ii)} an entropy-weighted key-frame sampler that selects evenly spaced, information-dense frames, reducing redundancy while preserving representativeness. By weaving these temporal and semantic signals together, TV-RAG realises a dual-level reasoning routine that can be grafted onto any LVLM without re-training or fine-tuning. The resulting system offers a lightweight, budget-friendly upgrade path and consistently surpasses most leading baselines across established long-video benchmarks such as Video-MME, MLVU, and LongVideoBench, confirming the effectiveness of our model. The code can be found at https://github.com/AI-Researcher-Team/TV-RAG.

</details>


### [160] [Multi-label Classification with Panoptic Context Aggregation Networks](https://arxiv.org/abs/2512.23486)
*Mingyuan Jiu,Hailong Zhu,Wenchuan Wei,Hichem Sahbi,Rongrong Ji,Mingliang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为PanCAN的深度全景上下文聚合网络，通过跨尺度特征聚合，有效提升了图像多标签分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多只关注几何关系或局部特征，忽视了对象间跨尺度的上下文交互，导致对复杂场景理解不足。

Method: PanCAN网络在高维希尔伯特空间中分层整合多阶几何上下文。具体方法为：结合随机游走和注意力机制，在每个尺度学习多阶邻域关系，并将不同尺度的模块级联，利用注意力动态融合细尺度突出锚点的邻域特征，实现多阶和跨尺度上下文建模。

Result: 在NUS-WIDE、PASCAL VOC2007和MS-COCO等多标签分类数据集上的大量实验表明，PanCAN在定量和定性评估中均优于当前主流方法，提升了多标签分类准确性。

Conclusion: PanCAN通过有效的多阶和跨尺度上下文特征融合，显著增强了复杂场景下的图像理解能力，在多标签分类任务中取得了优越的表现。

Abstract: Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.

</details>


### [161] [IdentityStory: Taming Your Identity-Preserving Generator for Human-Centric Story Generation](https://arxiv.org/abs/2512.23519)
*Donghao Zhou,Jingyu Lin,Guibao Shen,Quande Liu,Jialin Gao,Lihao Liu,Lan Du,Cunjian Chen,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 该论文提出了IdentityStory框架，实现了跨多帧图片中人物身份一致性，有效提升了人物面部一致性与多人物协调生成，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成模型虽然能根据文本生成有一致角色的故事场景，但在人脸等细节保持和多角色协调方面存在挑战。因此，需要一个能保证人脸一致、更好支持多角色的生成框架。

Method: 提出IdentityStory框架，包含两个核心模块：1）迭代式身份发现，用于提取连贯的角色身份；2）再去噪身份注入，在去噪过程中将身份特征注入到图像中，保证图像内容和身份一致性。

Result: 在ConsiStory-Human基准上进行实验，IdentityStory在人脸一致性、多角色组合等指标上优于现有方法。

Conclusion: IdentityStory能有效实现人本故事生成中的身份一致性与多角色支持，且具备扩展到无限长度故事生成与动态角色组合的潜力。

Abstract: Recent visual generative models enable story generation with consistent characters from text, but human-centric story generation faces additional challenges, such as maintaining detailed and diverse human face consistency and coordinating multiple characters across different images. This paper presents IdentityStory, a framework for human-centric story generation that ensures consistent character identity across multiple sequential images. By taming identity-preserving generators, the framework features two key components: Iterative Identity Discovery, which extracts cohesive character identities, and Re-denoising Identity Injection, which re-denoises images to inject identities while preserving desired context. Experiments on the ConsiStory-Human benchmark demonstrate that IdentityStory outperforms existing methods, particularly in face consistency, and supports multi-character combinations. The framework also shows strong potential for applications such as infinite-length story generation and dynamic character composition.

</details>


### [162] [Iterative Inference-time Scaling with Adaptive Frequency Steering for Image Super-Resolution](https://arxiv.org/abs/2512.23532)
*Hexin Zhang,Dong Li,Jie Huang,Bingzhou Wang,Xueyang Fu,Zhengjun Zha*

Main category: cs.CV

TL;DR: 该论文提出了一种新的图像超分辨率方法IAFS，有效提升视觉细节和结构保真度，解决了扩散模型推理时期感知-保真矛盾。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型SR方法难以兼顾高频感知质量与低频结构保真度，而推理期的优化方法又各有局限，无法很好地平衡二者。

Method: 提出无训练改动的推理期算法IAFS：在推理过程中迭代修正结构偏差，同时采用频率自适应粒子融合策略，将高频感知和低频结构信息动态结合。

Result: 在多种扩散SR模型上进行实验证明，IAFS能有效提升感知与结构的平衡，细节与结构表现优于现有同类推理优化方法。

Conclusion: IAFS作为训练无关型方法，显著提升扩散SR的感知质量与结构保真度，为图像生成带来了更优的平衡方案。

Abstract: Diffusion models have become a leading paradigm for image super-resolution (SR), but existing methods struggle to guarantee both the high-frequency perceptual quality and the low-frequency structural fidelity of generated images. Although inference-time scaling can theoretically improve this trade-off by allocating more computation, existing strategies remain suboptimal: reward-driven particle optimization often causes perceptual over-smoothing, while optimal-path search tends to lose structural consistency. To overcome these difficulties, we propose Iterative Diffusion Inference-Time Scaling with Adaptive Frequency Steering (IAFS), a training-free framework that jointly leverages iterative refinement and frequency-aware particle fusion. IAFS addresses the challenge of balancing perceptual quality and structural fidelity by progressively refining the generated image through iterative correction of structural deviations. Simultaneously, it ensures effective frequency fusion by adaptively integrating high-frequency perceptual cues with low-frequency structural information, allowing for a more accurate and balanced reconstruction across different image details. Extensive experiments across multiple diffusion-based SR models show that IAFS effectively resolves the perception-fidelity conflict, yielding consistently improved perceptual detail and structural accuracy, and outperforming existing inference-time scaling methods.

</details>


### [163] [AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization](https://arxiv.org/abs/2512.23537)
*Binhe Yu,Zhen Wang,Kexin Li,Yuqian Yuan,Wenqiao Zhang,Long Chen,Juncheng Li,Jun Xiao,Yueting Zhuang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的新方法AnyMS，实现了基于布局引导的多主体图像定制，同时兼顾文本对齐、主体身份保持和布局控制等目标。实验表明AnyMS在多主体复杂场景下取得了业界领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有多主体图像合成方法难以同时保证文本、主体和布局三方面的要求，且通常依赖额外训练，影响效率和扩展性。因此需要一种易用且能权衡三目标的更高效方法。

Method: AnyMS方法无需额外训练，直接利用文本提示、主体图片和布局约束三类输入，提出全局与局部双层注意力解耦机制，分别保证文本与视觉条件分离和各主体在其指定区域生成。并通过预训练的图像适配器提取与扩散模型对齐的主体特征，无需主体学习或适配器调优。

Result: AnyMS在多种任务和数据集下，实验证明能够有效支持复杂多主体场景，对数量较多的主体具备良好可扩展性，生成效果在行业内处于领先水平。

Conclusion: AnyMS解决了多主体图像合成时文本、主体身份和布局控制三者难以兼顾的问题，无需额外训练，实用性强，为后续多主体定制类任务提供了高效的范式。

Abstract: Multi-subject customization aims to synthesize multiple user-specified subjects into a coherent image. To address issues such as subjects missing or conflicts, recent works incorporate layout guidance to provide explicit spatial constraints. However, existing methods still struggle to balance three critical objectives: text alignment, subject identity preservation, and layout control, while the reliance on additional training further limits their scalability and efficiency. In this paper, we present AnyMS, a novel training-free framework for layout-guided multi-subject customization. AnyMS leverages three input conditions: text prompt, subject images, and layout constraints, and introduces a bottom-up dual-level attention decoupling mechanism to harmonize their integration during generation. Specifically, global decoupling separates cross-attention between textual and visual conditions to ensure text alignment. Local decoupling confines each subject's attention to its designated area, which prevents subject conflicts and thus guarantees identity preservation and layout control. Moreover, AnyMS employs pre-trained image adapters to extract subject-specific features aligned with the diffusion model, removing the need for subject learning or adapter tuning. Extensive experiments demonstrate that AnyMS achieves state-of-the-art performance, supporting complex compositions and scaling to a larger number of subjects.

</details>


### [164] [PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis](https://arxiv.org/abs/2512.23545)
*Shengyi Hua,Jianfeng Wu,Tianle Shen,Kangzhe Hu,Zhongzhen Huang,Shujuan Ni,Zhihong Zhang,Yuan Li,Zhe Wang,Xiaofan Zhang*

Main category: cs.CV

TL;DR: PathFound是一种用于病理诊断的多模态智能体模型，通过主动寻证和迭代诊断明显提高诊断准确性，且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的病理模型普遍采用静态推理模式，即一次性处理切片图像并给出诊断，没有在诊断模糊时进行再评估或主动获取特定证据；而实际临床流程往往需要不断复查和补充信息。该研究旨在缩小现实诊断流程和AI模型之间的差距。

Method: 作者提出PathFound模型，结合了视觉基础模型、视觉-语言模型以及强化学习训练的推理模型，能够模拟医生的工作流程：先给出初诊、然后主动寻求更多证据、最终给出 refined 诊断结果。

Result: 多组大模型实验表明，采用主动寻证与迭代推理策略可以持续提升病理诊断准确性。PathFound在多个临床场景中取得了最优的诊断表现，并具备发现微小病理特征的能力。

Conclusion: 主动寻证的多模态智能体模型在病理AI领域表现优异, 能提升对疑难及细致病变的识别，推动病理AI从静态走向动态智能解释，具有临床应用潜力。

Abstract: Recent pathological foundation models have substantially advanced visual representation learning and multimodal interaction. However, most models still rely on a static inference paradigm in which whole-slide images are processed once to produce predictions, without reassessment or targeted evidence acquisition under ambiguous diagnoses. This contrasts with clinical diagnostic workflows that refine hypotheses through repeated slide observations and further examination requests. We propose PathFound, an agentic multimodal model designed to support evidence-seeking inference in pathological diagnosis. PathFound integrates the power of pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to perform proactive information acquisition and diagnosis refinement by progressing through the initial diagnosis, evidence-seeking, and final decision stages. Across several large multimodal models, adopting this strategy consistently improves diagnostic accuracy, indicating the effectiveness of evidence-seeking workflows in computational pathology. Among these models, PathFound achieves state-of-the-art diagnostic performance across diverse clinical scenarios and demonstrates strong potential to discover subtle details, such as nuclear features and local invasions.

</details>


### [165] [PurifyGen: A Risk-Discrimination and Semantic-Purification Model for Safe Text-to-Image Generation](https://arxiv.org/abs/2512.23546)
*Zongsheng Cao,Yangfan He,Anran Liu,Jun Xie,Feng Chen,Zepeng Wang*

Main category: cs.CV

TL;DR: 提出了一种无需重新训练即可提升AI文本到图像生成安全性的算法PurifyGen，通过细粒度分析和处理提示词中的风险语义，有效过滤有害内容。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型生成图片能力提升的同时，生成不安全内容的风险也升高。传统安全策略（如黑名单、内容分类）要么易被规避，要么需要大量数据与额外训练，存在实际应用障碍。因此，需要一种无需训练，泛化能力强且更细致的方法保障安全。

Method: PurifyGen提出了双阶段提示词净化策略：首先通过“互补语义距离”对每个token和“有害/安全”概念嵌入进行距离计算，标记风险token，无需关键词匹配或再训练；其次，由于风险token已定位，对其进行“嵌入空间变换”，将与有害概念对齐的成分投影到‘有害空域’的零空间并加强‘安全空间’，实现不更改模型权重的安全优化。同时，仅替换风险token嵌入，最大限度保留提示词意图。

Result: PurifyGen在五个公开数据集上测试，相比现有方法显著减少不安全内容生成表现，并能与依赖训练的方法媲美或超越。支持未见过的新prompt与不同模型，泛化性好。

Conclusion: PurifyGen是一种理论扎实、便捷可用、泛化能力强的T2I生成安全过滤新思路，无需修改原有模型或依赖训练，能有效提升AI生成安全性。

Abstract: Recent advances in diffusion models have notably enhanced text-to-image (T2I) generation quality, but they also raise the risk of generating unsafe content. Traditional safety methods like text blacklisting or harmful content classification have significant drawbacks: they can be easily circumvented or require extensive datasets and extra training. To overcome these challenges, we introduce PurifyGen, a novel, training-free approach for safe T2I generation that retains the model's original weights. PurifyGen introduces a dual-stage strategy for prompt purification. First, we evaluate the safety of each token in a prompt by computing its complementary semantic distance, which measures the semantic proximity between the prompt tokens and concept embeddings from predefined toxic and clean lists. This enables fine-grained prompt classification without explicit keyword matching or retraining. Tokens closer to toxic concepts are flagged as risky. Second, for risky prompts, we apply a dual-space transformation: we project toxic-aligned embeddings into the null space of the toxic concept matrix, effectively removing harmful semantic components, and simultaneously align them into the range space of clean concepts. This dual alignment purifies risky prompts by both subtracting unsafe semantics and reinforcing safe ones, while retaining the original intent and coherence. We further define a token-wise strategy to selectively replace only risky token embeddings, ensuring minimal disruption to safe content. PurifyGen offers a plug-and-play solution with theoretical grounding and strong generalization to unseen prompts and models. Extensive testing shows that PurifyGen surpasses current methods in reducing unsafe content across five datasets and competes well with training-dependent approaches. The code can refer to https://github.com/AI-Researcher-Team/PurifyGen.

</details>


### [166] [RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature](https://arxiv.org/abs/2512.23565)
*Hanzheng Li,Xi Fang,Yixuan Li,Chaozheng Huang,Junjie Wang,Xi Wang,Hongzhe Bai,Bojun Hao,Shenyu Lin,Huiqi Liang,Linfeng Zhang,Guolin Ke*

Main category: cs.CV

TL;DR: 本文提出RxnBench基准，用于评估多模态大语言模型（MLLMs）对化学反应文献理解的能力，发现目前MLLMs在深层化学推理和结构识别方面存在明显短板。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在化学领域有巨大潜力，但其理解科学文献中复杂反应图示和深层逻辑的能力尚未系统研究。

Method: 作者提出了RxnBench，包括单图问答（SF-QA）和全文问答（FD-QA）两种任务，分别考察模型对反应图细节的感知和跨文档、多模态信息综合能力，并通过1,525个问题和108篇文献进行严密评测。

Result: 现有MLLMs在直接文本提取上表现良好，但在深度化学逻辑推理和精确结构识别上表现不佳。推理能力在线的模型优于普通架构，但在FD-QA上的准确率均未超过50%。

Conclusion: 当下MLLMs尚不足以胜任高难度化学文献理解任务，亟需面向领域的视觉编码器和更强推理引擎，以推动自动化AI化学家发展。

Abstract: The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables. Our evaluation of MLLMs reveals a critical capability gap: while models excel at extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Notably, models with inference-time reasoning significantly outperform standard architectures, yet none achieve 50\% accuracy on FD-QA. These findings underscore the urgent need for domain-specific visual encoders and stronger reasoning engines to advance autonomous AI chemists.

</details>


### [167] [ThinkGen: Generalized Thinking for Visual Generation](https://arxiv.org/abs/2512.23568)
*Siyu Jiao,Yiheng Lin,Yujie Zhong,Qi She,Wei Zhou,Xiaohan Lan,Zilong Huang,Fei Yu,Yingchen Yu,Yunqing Zhao,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 该论文提出了ThinkGen，一个利用多模态大模型链式思维（CoT）推理进行图像生成的新框架，通过模块化设计实现更强的通用性和适应性，并在多个生成基准上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维（CoT）推理提升了复杂理解任务的能力，但其在生成任务中的应用还不成熟，且多受限于特定场景，难以泛化和适应更多任务。因此需要开发一种能在多种生成场景下有效应用CoT推理的通用方法。

Method: 提出了ThinkGen框架，将预训练多模态大语言模型（MLLM）与扩散Transformer（DiT）解耦结合，MLLM生成基于用户意图的指令，DiT依据指令生成高质量图像。引入了分离式GRPO训练方法，通过在两大子模块间交替强化学习，实现联合训练和多样本集适配，提升推理与生成能力。

Result: 大量实验证明，ThinkGen在多种生成基准任务上都达到了强健且领先的性能，验证了其通用性和有效性。

Conclusion: ThinkGen首次实现了基于CoT推理的通用视觉生成，展现了杰出的泛化能力，为多模态生成任务提供了一种新思路。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion Transformer (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen

</details>


### [168] [Image Denoising Using Global and Local Circulant Representation](https://arxiv.org/abs/2512.23569)
*Zhaoming Kong,Xiaowei Yang,Jiahuan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种高效且有效的图像去噪算法Haar-tSVD，通过将张量奇异值分解（t-SVD）与Haar变换结合，在无需学习局部基的情况下实现了去噪速度与性能的平衡，并通过自适应噪声估计和深度网络提升鲁棒性，实验结果验证了方法优越性。


<details>
  <summary>Details</summary>
Motivation: 随着成像设备的普及和海量图像数据的产生，对高效、有效的图像去噪方法的需求日益增加。传统方法存在效率或性能不足，且往往需要学习局部基，限制了其实用性。因此，提出一种无需学习局部基且高效的去噪方法具有重要意义。

Method: 作者理论上建立了主成分分析（PCA）与循环表示下的Haar变换的联系，提出将统一的张量奇异值分解（t-SVD）投影与Haar变换结合，开发了Haar-tSVD去噪方法。该方法为单步、可并行的即插即用型去噪器，无需学习局部基。此外，结合循环结构的特征值分析引入了自适应噪声估计，并在高噪声条件下进一步结合深度神经网络优化性能。

Result: 在多个去噪数据集上的实验结果表明，Haar-tSVD算法在噪声去除效率和效果方面优于现有方法，验证了其作为高效图像去噪工具的实用性。

Conclusion: Haar-tSVD方法以理论创新和实际性能优势兼具的特点，为图像去噪提供了新的高效解决方案，通过自适应和与深度学习结合，提升了鲁棒性和性能，具有较高应用和科研价值。

Abstract: The proliferation of imaging devices and countless image data generated every day impose an increasingly high demand on efficient and effective image denoising. In this paper, we establish a theoretical connection between principal component analysis (PCA) and the Haar transform under circulant representation, and present a computationally simple denoising algorithm. The proposed method, termed Haar-tSVD, exploits a unified tensor singular value decomposition (t-SVD) projection combined with Haar transform to efficiently capture global and local patch correlations. Haar-tSVD operates as a one-step, parallelizable plug-and-play denoiser that eliminates the need for learning local bases, thereby striking a balance between denoising speed and performance. Besides, an adaptive noise estimation scheme is introduced to improve robustness according to eigenvalue analysis of the circulant structure. To further enhance the performance under severe noise conditions, we integrate deep neural networks with Haar-tSVD based on the established Haar-PCA relationship. Experimental results on various denoising datasets demonstrate the efficiency and effectiveness of proposed method for noise removal. Our code is publicly available at https://github.com/ZhaomingKong/Haar-tSVD.

</details>


### [169] [ProGuard: Towards Proactive Multimodal Safeguard](https://arxiv.org/abs/2512.23573)
*Shaohan Yu,Lijun Li,Chenyang Si,Lu Sheng,Jing Shao*

Main category: cs.CV

TL;DR: 本文提出了一种新型的多模态安全防护方法ProGuard，能主动识别并描述模型未见过的安全风险，大幅超越现有开源防护模型，并接近闭源大模型的表现。


<details>
  <summary>Details</summary>
Motivation: 随着生成式模型的发展，多模态（如文本、图像、文本-图像）内容面临越来越多的安全风险，现有防御方法难以及时应对新出现的风险，因此需要一种能主动发现和描述未知安全威胁的方案。

Method: 作者构建了一个87K样本、均衡多模态的大型安全数据集，标注有二元安全标签以及层次化风险类别。在此基础上，采用纯RL（强化学习）训练视觉-语言基础模型，实现高效简明的推理。为了模拟主动防护情景，还设计了OOD风险类别推断任务，并加入同义词库奖励，鼓励模型对新型风险简明描述。

Result: ProGuard在二元安全分类上表现与闭源大模型相当，在不安全内容细分类别的识别上明显优于现有开源防护模型。在主动检测未见过的风险时，ProGuard的识别率提升了52.6%，描述能力提升了64.8%。

Conclusion: ProGuard能够主动识别和简明描述多模态未见安全风险，显著加强了生成模型的安全性管理，展示了主动防护方法的潜力。

Abstract: The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.

</details>


### [170] [LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation](https://arxiv.org/abs/2512.23576)
*Ethan Chern,Zhulin Hu,Bohao Tang,Jiadi Su,Steffi Chern,Zhijie Deng,Pengfei Liu*

Main category: cs.CV

TL;DR: 本文提出了一种能够实现实时、多模态（文本、图像、音频）交互和视频生成的扩散模型蒸馏方案，大幅降低了生成延迟与成本，并集成进实用系统，显著提升了交互自然性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽然在视频生成有高质量表现，但因所有帧的双向注意力去噪和多次迭代推理，难以满足实时互动需求。已有的模型蒸馏方案虽能加快采样速度，但多聚焦于文本到视频，且在人机多模态互动场景下不自然、效率低下。因此，亟需面向多模态条件的高效、实时互动视频生成方法。

Method: 作者改进了主流自监督蒸馏方法Self Forcing，使其在多模态条件下提升性能，重点优化了条件输入质量、初始化、和自监督优化调度，并在HDTF、AVSpeech、CelebV-HQ等多模态头像视频数据集上进行了训练和评测。最后，将模型与音频语言模型及长视频推理技术Anchor-Heavy Identity Sinks结合，构建了实时互动系统LiveTalk。

Result: 改进蒸馏模型在保持与同尺寸甚至更大基线模型相当的视觉质量前提下，实现了20倍的推理加速和延迟缩减。LiveTalk系统在多轮互动基准测试中，视频连贯性和内容质量均优于业界顶尖模型（Sora2、Veo3），并将视频响应延迟从1-2分钟降至可实时生成。

Conclusion: 本文方法实现了高效、低延迟和高质量的多模态实时视频生成，为自然流畅的人机交互打开了新可能，对多模态大模型应用于实时互动具有重要推动作用。

Abstract: Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.

</details>


### [171] [Same or Not? Enhancing Visual Perception in Vision-Language Models](https://arxiv.org/abs/2512.23592)
*Damiano Marsili,Aditya Mehta,Ryan Y. Lin,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 本文提出了TWIN数据集和任务，通过561,000个配对图片查询，专注于训练视觉-语言模型（VLMs）在细粒度感知任务上的能力，显著提升了模型对细节的分辨能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言大模型虽在通用视觉理解上表现优异，但对细节捕捉能力较弱，受限于强调泛化识别而忽略细粒度感知的数据集。为提升模型对微小视觉差异的感知能力，作者决定构建新型大规模细粒度识别数据集。

Method: 作者构建了TWIN数据集，包括561,000对需要判别是否为同一对象的视觉相似图片对，覆盖了丰富的现实物体、场景和拍摄角度。模型在TWIN上进行微调，并通过新引入的FGVQA基准（12,000条细粒度任务查询）对模型进行评测，结合现有的细粒度识别与检索数据集进行验证。

Result: 在TWIN数据集上微调的视觉-语言模型，在FGVQA基准上的细粒度识别准确率提升最高可达19.3%，且在通用VQA基准上并未损失性能。分析还证明，数据集的规模对性能提升至关重要。

Conclusion: TWIN数据集可作为开源训练语料的直接补充，显著提升视觉-语言模型的细粒度感知能力，有助于推动未来模型在高精度视觉理解任务中的发展。

Abstract: Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition ("Is it a cat or a dog?") over fine-grained perception. To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues. The dataset spans a diverse range of everyday objects across contexts, viewpoints, and appearances. Fine-tuning VLMs on TWIN yields notable gains in fine-grained recognition, even on unseen domains such as art, animals, plants, and landmarks. To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains. While existing VLMs struggle on FGVQA, when fine-tuned on TWIN they improve by up to 19.3%, without compromising performance on general VQA benchmarks. Finally, our TWIN dataset scales favorably with object annotations, and our analysis shows that scale is key to performance. We envision TWIN as a drop-in addition to open-source VLM training corpora, advancing perceptual precision of future models. Project webpage: https://glab-caltech.github.io/twin/

</details>


### [172] [Detection Fire in Camera RGB-NIR](https://arxiv.org/abs/2512.23594)
*Nguyen Truong Khai,Luong Duc Vinh*

Main category: cs.CV

TL;DR: 本文针对红外夜视条件下的火灾检测准确率提升问题，提出了新的数据增强方法、双阶段检测模型以及针对小目标的Patched-YOLO方法，并在实验中显示优于以往模型。


<details>
  <summary>Details</summary>
Motivation: 现有火灾检测算法在夜间红外图像中容易将强光误判为火焰，同时夜间样本数据不足、容易漏检小目标，限制了检测准确性。

Method: 1）采集并增强NIR夜视数据集，以及分类数据集以缓解数据不足；2）提出结合YOLOv11和EfficientNetV2-B0的双阶段检测流程，减少异常光源误判，提高夜间检测准确性；3）为提升RGB图像中的小目标检测，提出基于图片切块的Patched-YOLO模型。

Result: 本文方法在夜间火灾检测场景下mAP预计优于YOLOv7、RT-DETR、YOLOv9等主流模型，对夜间火焰和小目标均有更高识别准确率。

Conclusion: 结合数据增强、两阶段检测和切块处理，相比传统方法在夜视夜间火灾检测准确性和鲁棒性方面更具优势，能够有效减少假阳性和漏检，具有较强实际应用价值。

Abstract: Improving the accuracy of fire detection using infrared night vision cameras remains a challenging task. Previous studies have reported strong performance with popular detection models. For example, YOLOv7 achieved an mAP50-95 of 0.51 using an input image size of 640 x 1280, RT-DETR reached an mAP50-95 of 0.65 with an image size of 640 x 640, and YOLOv9 obtained an mAP50-95 of 0.598 at the same resolution. Despite these results, limitations in dataset construction continue to cause issues, particularly the frequent misclassification of bright artificial lights as fire.
  This report presents three main contributions: an additional NIR dataset, a two-stage detection model, and Patched-YOLO. First, to address data scarcity, we explore and apply various data augmentation strategies for both the NIR dataset and the classification dataset. Second, to improve night-time fire detection accuracy while reducing false positives caused by artificial lights, we propose a two-stage pipeline combining YOLOv11 and EfficientNetV2-B0. The proposed approach achieves higher detection accuracy compared to previous methods, particularly for night-time fire detection. Third, to improve fire detection in RGB images, especially for small and distant objects, we introduce Patched-YOLO, which enhances the model's detection capability through patch-based processing. Further details of these contributions are discussed in the following sections.

</details>


### [173] [Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging](https://arxiv.org/abs/2512.23597)
*Janani Annur Thiruvengadam,Kiran Mayee Nabigaru,Anusha Kovi*

Main category: cs.CV

TL;DR: 该论文提出了一种可扩展残差特征聚合（SRFA）框架，有效提升胰腺肿瘤早期检测的准确率，显著优于传统CNN和变压器模型。


<details>
  <summary>Details</summary>
Motivation: 胰腺肿瘤在CT图像上的对比边缘小且解剖变异大，导致早期检测极具挑战性。亟需开发可增强微弱视觉线索、具备多模态泛化能力的辅助系统。

Method: 提出SRFA框架，包括预处理、基于MAGRes-UNet的分割、DenseNet-121残差特征聚合、HHO-BA混合特征选择，并采用融合ViT与EfficientNet-B3的混合分类模型，通过SSA和GWO双优化机制调优超参数。

Result: 实验结果显示，所提模型准确率96.23%、F1分数95.58%、特异性94.83%，显著优于传统CNN和当前主流变压器模型。

Conclusion: SRFA框架在提升胰腺肿瘤早期检测效果方面表现突出，具有成为临床实用工具的潜力。

Abstract: The early detection of pancreatic neoplasm is a major clinical dilemma, and it is predominantly so because tumors are likely to occur with minimal contrast margins and a large spread anatomy-wide variation amongst patients on a CT scan. These complexities require to be addressed with an effective and scalable system that can assist in enhancing the salience of the subtle visual cues and provide a high level of the generalization on the multimodal imaging data. A Scalable Residual Feature Aggregation (SRFA) framework is proposed to be used to meet these conditions in this study. The framework integrates a pipeline of preprocessing followed by the segmentation using the MAGRes-UNet that is effective in making the pancreatic structures and isolating regions of interest more visible. DenseNet-121 performed with residual feature storage is used to extract features to allow deep hierarchical features to be aggregated without properties loss. To go further, hybrid HHO-BA metaheuristic feature selection strategy is used, which guarantees the best feature subset refinement. To be classified, the system is trained based on a new hybrid model that integrates the ability to pay attention on the world, which is the Vision Transformer (ViT) with the high representational efficiency of EfficientNet-B3. A dual optimization mechanism incorporating SSA and GWO is used to fine-tune hyperparameters to enhance greater robustness and less overfitting. Experimental results support the significant improvement in performance, with the suggested model reaching 96.23% accuracy, 95.58% F1-score and 94.83% specificity, the model is significantly better than the traditional CNNs and contemporary transformer-based models. Such results highlight the possibility of the SRFA framework as a useful instrument in the early detection of pancreatic tumors.

</details>


### [174] [Memorization in 3D Shape Generation: An Empirical Study](https://arxiv.org/abs/2512.23628)
*Shu Pu,Boya Zeng,Kaichen Zhou,Mengyu Wang,Zhuang Liu*

Main category: cs.CV

TL;DR: 本文设计了一个评估3D生成模型记忆性（memorizaton）的框架，并用来定量分析主流方法的记忆行为和各类建模细节对其的影响。


<details>
  <summary>Details</summary>
Motivation: 目前3D生成模型被广泛用于生成新颖的三维形状，但外界不清楚这些生成的形状是否仅仅是训练集样本的“记忆”或复制。理解记忆性不仅有助于防止训练数据泄漏，还能提升生成结果的多样性，因此有必要对此进行系统研究。

Method: 作者提出了一套量化评估3D生成模型记忆性的框架，并用该框架分析了多种已有方法。随后，他们利用Vecset扩散模型，系统研究了数据模态、多样性、细粒度条件、模型引导尺度、扩展向量长度以及旋转增强等因素对记忆性的影响。

Result: 实验显示，记忆性与数据的模态和多样性及细粒度条件正相关，在模型端，当引导尺度适中时记忆最强；而使用更长的Vecset以及旋转等简单的数据增强方法则能有效缓解记忆性。

Conclusion: 本研究提出评测框架并揭示了3D生成模型的记忆机理，同时给出能降低记忆性且不损失生成质量的策略，为后续设计更安全、更加多样化的3D生成体系提供了理论与实践指导。

Abstract: Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.

</details>


### [175] [Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception](https://arxiv.org/abs/2512.23635)
*Xiaoyu Li,Peidong Li,Xian Wu,Long Shi,Dedong Liu,Yitao Wu,Jiajia Fu,Dixiao Cui,Lijun Zhao,Lining Sun*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的空间-时间对齐模块HAT，用于自动驾驶中端到端感知的时序建模，通过多假设解码，提升了3D目标检测与跟踪的精度及鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法多倚赖注意力机制结合简单的物理运动模型（如匀速），在不同类别和状态下，这种隐式对齐存在不足；亟需更灵活、精确的对齐模型。

Method: HAT模块首先基于多种明确运动模型生成时空锚点和历史目标的运动感知特征，随后结合目标的语义与运动信息，进行多假设解码，为当前帧输出最优对齐提案。整个过程无需显式监督。

Result: 在nuScenes数据集上，HAT能全面提升各类基线3D时序检测与跟踪模型的性能。在与DETR3D结合时，3D跟踪AMOTA达到46.0%，达到最新水平。在端到端感知与规划中，mAP提升1.3%、AMOTA提升3.1%、碰撞率下降32%。

Conclusion: HAT模块有效提升了对象运动建模能力，使对齐更加自适应和鲁棒，尤其在语义信息被破坏时，仍可显著增强自动驾驶感知与规划的安全性与准确性。

Abstract: Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.

</details>


### [176] [OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding](https://arxiv.org/abs/2512.23646)
*Keda Tao,Wenjie Du,Bohan Yu,Weiqiang Wang,Jian Liu,Huan Wang*

Main category: cs.CV

TL;DR: 本文提出OmniAgent系统，通过音频引导主动感知，实现更细粒度的音视听多模态理解，在多项基准测试中取得显著超越现有模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的全模态大模型虽然能整合音频和视觉信息，但在细粒度的跨模态理解及多模态对齐方面表现不足，难以灵活应对动态复杂的任务。

Method: OmniAgent采用全新的音频引导主动感知范式，通过动态规划，根据任务需求灵活调用专业工具，将感知注意力集中在关键线索上。其核心方法是“由粗到细”的音频引导感知，先利用音频定位时序事件，再引导后续推理。与以往依赖静态流程和密集帧描述的方法不同，实现了工具自主动态编排和主动多模态探索。

Result: 在三个主流音视频理解基准上，OmniAgent均取得了10%-20%准确率的大幅领先，显著优于当前开源及专有领先模型。

Conclusion: OmniAgent展现了音频引导的主动多模态感知与推理范式的巨大潜力，为提升多模态大模型在复杂任务下的推理能力提供了新方向。

Abstract: Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.

</details>


### [177] [IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition](https://arxiv.org/abs/2512.23667)
*Kang Du,Yirui Guan,Zeyu Wang*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的多视图固有图像分解方法，显著提升了分解一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型虽然在单视图固有图像分解上表现优异，但难以拓展到多视角场景，常导致视角间分解结果不一致，影响下游应用。

Method: 提出了Intrinsic Decomposition Transformer (IDT)，利用transformer注意力机制联动处理多张输入图片，在一次前向推理中输出视角一致的固有因子。IDT基于物理图像生成模型，显式分解获得漫反射、漫反射明暗和高光明暗三种成分，结构性地区分朗伯和非朗伯光传输，提升了分解的物理可解释性和可控性。

Result: 在合成数据和真实数据集上实验表明，IDT获得了更加干净的漫反射、更加一致的阴影分量和更好分离的高光效果，多视图一致性远超现有方法。

Conclusion: IDT显著提升了多视图固有属性分解的质量和一致性，具有良好的物理解释性及应用潜力。

Abstract: Intrinsic image decomposition is fundamental for visual understanding, as RGB images entangle material properties, illumination, and view-dependent effects. Recent diffusion-based methods have achieved strong results for single-view intrinsic decomposition; however, extending these approaches to multi-view settings remains challenging, often leading to severe view inconsistency. We propose \textbf{Intrinsic Decomposition Transformer (IDT)}, a feed-forward framework for multi-view intrinsic image decomposition. By leveraging transformer-based attention to jointly reason over multiple input images, IDT produces view-consistent intrinsic factors in a single forward pass, without iterative generative sampling. IDT adopts a physically grounded image formation model that explicitly decomposes images into diffuse reflectance, diffuse shading, and specular shading. This structured factorization separates Lambertian and non-Lambertian light transport, enabling interpretable and controllable decomposition of material and illumination effects across views. Experiments on both synthetic and real-world datasets demonstrate that IDT achieves cleaner diffuse reflectance, more coherent diffuse shading, and better-isolated specular components, while substantially improving multi-view consistency compared to prior intrinsic decomposition methods.

</details>


### [178] [Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation](https://arxiv.org/abs/2512.23705)
*Shaocong Xu,Songlin Wei,Qizhe Wei,Zheng Geng,Hong Li,Licheng Shen,Qianpu Sun,Shu Han,Bin Ma,Bohan Li,Chongjie Ye,Yuhang Zheng,Nan Wang,Saining Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种利用视频扩散模型来提升透明显著物体三维感知的新方法，并构建了大规模合成视频数据集，实现了在透明、反射物体场景下深度与法线估计的最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有的三维感知方法在面对透明或反光物体时表现不佳，因为其物理特性导致常用深度感知技术失效。既有方法在透明物体的动态估计上特别容易出现异常和不稳定，因此迫切需要更鲁棒、时序一致的新方案。

Method: 作者首先构建了TransPhy3D合成视频数据集，包含丰富类别的静态与程序化资产，并分配多样材质，通过物理光线追踪渲染RGB+深度+法线。然后以大型视频扩散模型为基础，采用轻量化LoRA微调，训练一个RGB+深度潜变量混合的、专门用于深度（及法线）估计的视频到视频网络，实现跨数据集协同训练以获得更好的泛化和时序一致性。

Result: 最终模型（DKT）在ClearPose、DREDS（CatKnown/CatNovel）、TransPhy3D-Test等真实与合成公开基准上，无需额外训练即可获得最优或领先的深度与法线估计能力，表现出明显更好的准确性和时间一致性。小型化模型推理加速明显，并且在机械抓取任务中提升了多类表面（透明、反光、漫反射）的成功率。

Conclusion: 扩散模型内隐地学习到了处理透明物体的物理机制，可以通过高效、无需额外标注的方式转化为视频感知模型，实现对于现实复杂操作任务下，透明物体等挑战场景的鲁棒、时序一致感知。

Abstract: Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: "Diffusion knows transparency." Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.

</details>


### [179] [Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion](https://arxiv.org/abs/2512.23709)
*Hau-Shiang Shiu,Chin-Yang Lin,Zhixiang Wang,Chi-Wei Hsiao,Po-Fan Yu,Yu-Chih Chen,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的在线扩散式视频超分辨方法Stream-DiffVSR，在保持高感知质量的同时大大降低了推理延迟，实现了真正可用于低延迟场景的扩散VSR。


<details>
  <summary>Details</summary>
Motivation: 传统扩散式VSR方法虽然提升了视频的超分辨质量，但推理速度慢且依赖未来帧，难以应用在对延迟敏感的实际应用中。因此，作者希望解决高感知质量与低延迟推理之间的矛盾。

Method: 1. 采用严格基于历史帧的因果条件扩散框架；2. 设计了四步蒸馏去噪器以加快推理速度；3. 提出自回归时序指导（ARTG）模块在潜空间去噪时注入运动相关信息；4. 轻量时序感知解码器配合时序处理模块（TPM）提升细节和时序一致性。

Result: 在RTX4090 GPU上处理720p视频帧时延仅0.328秒，显著优于此前扩散式方法。相比已有在线SOTA方法，Stream-DiffVSR在感知质量（LPIPS +0.095）和推理延迟（降低超过130倍）上均大幅领先。首帧延迟从4600秒降至0.328秒，刷新扩散VSR最低延迟纪录。

Conclusion: Stream-DiffVSR首次实现了可实际应用在线场景的扩散式视频超分辨方法，大大提升了实用性和部署价值。

Abstract: Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [180] [Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA](https://arxiv.org/abs/2512.22208)
*Pu Zhao,Xuan Shen,Zhenglun Kong,Yixin Shen,Sung-En Chang,Arash Akbari,Timothy Rupprecht,Lei Lu,Enfu Nan,Changdi Yang,Yumei He,Weiyan Shi,Xingchen Xu,Yu Huang,Wei Jiang,Wei Wang,Yue Chen,Yong He,Yanzhi Wang*

Main category: cs.CL

TL;DR: Moxin 7B是一个完全开源的大语言模型，强调模型权重、训练、数据集及实现细节全面开放，针对多任务开发了多个变体，实验结果优秀，并已开放模型、数据和代码。


<details>
  <summary>Details</summary>
Motivation: 当前主流大语言模型大多为专有产品，而开源模型在自定义和部署方面有优势，但缺乏充分的透明和开放。该工作旨在推动大模型的开放和协作，建立健康的开源生态。

Method: 开发Moxin 7B作为基线，并基于此衍生Moxin-VLM（视觉-语言）、Moxin-VLA（视觉-语言-动作），和Moxin-Chinese（中文任务）。采用开放框架与开源数据训练，公开权重、数据和代码。

Result: Moxin系列模型在不同评测任务中表现优异，验证了模型架构和开源做法的有效性。

Conclusion: Moxin 7B及其变体推动了大模型的全面开源，对多模态与多语言研究有重大意义，有助于促进包容和合作的研究氛围，支持健康的开源生态。

Abstract: Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Moxin 7B is introduced as a fully open-source LLM developed in accordance with the Model Openness Framework, which moves beyond the simple sharing of model weights to embrace complete transparency in training, datasets, and implementation detail, thus fostering a more inclusive and collaborative research environment that can sustain a healthy open-source ecosystem. To further equip Moxin with various capabilities in different tasks, we develop three variants based on Moxin, including Moxin-VLM, Moxin-VLA, and Moxin-Chinese, which target the vision-language, vision-language-action, and Chinese capabilities, respectively. Experiments show that our models achieve superior performance in various evaluations. We adopt open-source framework and open data for the training. We release our models, along with the available data and code to derive these models.

</details>


### [181] [Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces](https://arxiv.org/abs/2512.22227)
*Sophie Zhao*

Main category: cs.CL

TL;DR: 作者研究了Transformer模型生成的句子嵌入中是否存在与人类认知相关的分层结构。通过实验发现，这些嵌入能较好地区分不同认知类别的信息，结构具有层级性。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer语言模型的嵌入空间具备丰富的几何结构，但这些结构是否能够体现人类认知维度尚不明确。论文旨在探索嵌入空间是否自然地反映了类似心理学分类的层级关系。

Method: 作者构建了一个包含480句子并有认知分数与类别标签的数据集。取多种Transformer模型的句子嵌入，用线性和浅层非线性探针预测认知分数及类别，同时以TF-IDF作为对比。对探针性能进行统计检验和可视化分析。

Result: 多种Transformer嵌入均能较好恢复原有的认知分数和层级类别，非线性探针优于线性探针，且远超TF-IDF基线。检验表明性能显著高于随机。可视化结果揭示了层级渐变和类别间的相邻混淆特征。

Conclusion: Transformer模型的句子嵌入空间确实反映出与人类认知类别对应的层次型几何结构，但论文不声称模型有“意识”或主观体验，仅揭示了其结构对齐。

Abstract: Recent work has shown that transformer-based language models learn rich geometric structure in their embedding spaces, yet the presence of higher-level cognitive organization within these representations remains underexplored. In this work, we investigate whether sentence embeddings encode a graded, hierarchical structure aligned with human-interpretable cognitive or psychological attributes. We construct a dataset of 480 natural-language sentences annotated with continuous ordinal energy scores and discrete tier labels spanning seven ordered cognitive categories. Using fixed sentence embeddings from multiple transformer models, we evaluate the recoverability of these annotations via linear and shallow nonlinear probes. Across models, both continuous scores and tier labels are reliably decodable, with shallow nonlinear probes providing consistent performance gains over linear probes. Lexical TF-IDF baselines perform substantially worse, indicating that the observed structure is not attributable to surface word statistics alone. Nonparametric permutation tests further confirm that probe performance exceeds chance under label-randomization nulls. Qualitative analyses using UMAP visualizations and confusion matrices reveal smooth low-to-high gradients and predominantly adjacent-tier confusions in embedding space. Taken together, these results provide evidence that transformer embedding spaces exhibit a hierarchical geometric organization aligned with human-defined cognitive attributes, while remaining agnostic to claims of internal awareness or phenomenology.

</details>


### [182] [SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents](https://arxiv.org/abs/2512.22322)
*Shaofei Cai,Yulei Qin,Haojia Lin,Zihan Xu,Gang Li,Yuchen Shi,Zongyi Li,Yong Mao,Siqi Cai,Xiaoyu Tan,Yitao Liang,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: 本论文提出了一种赋予RL智能体主动自我验证能力的新范式SmartSnap，用以提升复杂GUI任务中任务完成验证的效率与可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前RL体系的任务完成验收通常是被动、事后进行，所需分析全程轨迹带来高成本与低准确，严重影响大模型的可扩展性。

Method: 提出SmartSnap范式，使智能体在执行任务过程中主动进行自我证据收集，并依照3C原则（完整性、简洁性和创造性），通过精选快照生成自我验证材料，最终供LLM判别。

Result: 在各类移动端任务与不同模型规模（8B、30B）上的实验证明，该方法可有效提升RL大模型的扩展性与性能，取得了高达26.08%和16.66%的性能提升。

Conclusion: SmartSnap范式实现了智能体的高效自验证，能以较小代价提升RL智能体在复杂任务中的可靠性与表现，具有广阔的应用前景。

Abstract: Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.

</details>


### [183] [The Syntax of qulk-clauses in Yemeni Ibbi Arabic: A Minimalist Approach](https://arxiv.org/abs/2512.22376)
*Zubaida Mohammed Albadani,Mohammed Q. Shormani*

Main category: cs.CL

TL;DR: 本文探讨也门Ibbi阿拉伯语中qulk-从句的句法结构，并在极简主义框架下分析其推导过程，揭示其为双从句结构，对极简句法理论作出贡献。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在揭示qulk-从句（意为“我说”）在YIA语法中的结构特征及其如何实现从句嵌套，尤其关注其在无连词情况下引入不同从句类型（陈述、疑问、祈使）的能力，以及其对极简主义理论框架的启示。

Method: 采用极简主义句法分析法，通过核心操作（合并、移动、一致、拼读）和形态合并等后句法过程，逐层分析qulk-从句的结构。研究重点考察qulk作为从句嵌入谓词，选择一个完整CP补足语的双从句结构，以及方言特征，如双重否定、附着词和CP嵌入等。

Result: 研究表明qulk-从句是具有层次性的双从句结构，qulk承担嵌入功能，句法推导完全可通过极简主义常规操作解释。同时，分析兼容YIA中的特殊语法现象，为极简句法理论中的结构建构手段做了具体案例补充。

Conclusion: 本文证实极简主义能够有效分析YIA中的qulk-从句结构，为qulk-相关结构的普遍性和极简理论的适用范围提供了新见解，同时也提出将该分析方法扩展至‘kil-k’（你说）等其他句法结构的进一步研究可能。

Abstract: This study investigates the syntax of qulk-clauses in Yemeni Ibbi Arabic (YIA) within the Minimalist Program. The construction qulk-clause, a morphologically fused form meaning 'I said,' introduces embedded declarative interrogative, and imperative clauses, often eithout complementizer. The central proposal of this paper is that qulk-clauses are biclausal structures in which qulk functions a clause-embedding predicate sec;ecting a dull CP complement. By applying core minimalist operations, viz., Merge, Move, Agree, and Spell-out, the study provides a layered syntactic analysis of qulk-clauses, for illustrating how their derivation proceeds through standard computational steps and post-syntactic processes such as Morphological Merger. The proposal also accounts for dialect-specific features like bipartite negation, cliticization, and CP embedding. The findings offer theoretical contributions to generative syntax, specifically minimalism. The study concludes raising theoretical questions concerning extending the analysis to the addressee-clause kil-k 'you said'. It also provides insights into the possibility of the universality of minimalism.

</details>


### [184] [Towards Efficient Post-Training via Fourier-Driven Adapter Architectures](https://arxiv.org/abs/2512.22378)
*Donggyun Bae,Jongil Park*

Main category: cs.CL

TL;DR: 作者提出了一种名为Fourier-Activated Adapter（FAA）的高效微调大语言模型的新框架，通过Fourier特征增强适配器，实现在参数效率、计算和内存上都优越的模型微调。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型因参数量大，微调时会产生巨大的计算和存储消耗。现有高效微调方法虽节约参数，但在适应复杂任务或保留模型表达能力上存在不足，因此需要新的方法兼顾参数效率与模型性能。

Method: FAA方法将随机Fourier特征引入轻量适配器模块，将中间表示分解为低频和高频部分，实现频率感知的语义调控。通过对不同频率带的信息进行自适应权重调整，仅调整适配器参数而保持主干网络冻结，提升调优灵活性。

Result: 在GLUE、E2E NLG及instruction-tuning等基准测试上，FAA与同类参数高效微调方法相比，表现出了更优或相当的性能，并保持了较低的计算与内存消耗。消融实验进一步证明了其频率感知机制和自适应加权机制的有效性。

Conclusion: FAA是一种稳健且高效的后训练微调方法，不仅能提升大语言模型的适应能力，还能在资源受限环境下实现低成本性能提升。

Abstract: We propose a novel framework, termed Fourier-Activated Adapter (FAA), for parameter-efficient fine-tuning of large pre-trained language models. By incorporating random Fourier features into lightweight adapter modules, FAA decomposes intermediate representations into complementary low- and high-frequency components, enabling frequency-aware modulation of semantic information. This design allows the model to selectively emphasize informative frequency bands during adaptation while preserving the representational capacity of the frozen backbone. Extensive experiments on GLUE, E2E NLG, and instruction-tuning benchmarks demonstrate that FAA consistently achieves competitive or superior performance compared to existing parameter-efficient fine-tuning methods, while maintaining low computational and memory overhead. Ablation studies further verify the effectiveness of frequency-aware activation and adaptive weighting mechanisms, highlighting FAA as a robust and efficient approach for post-training large language models.

</details>


### [185] [LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition](https://arxiv.org/abs/2512.22385)
*Elsen Ronando,Sozo Inoue*

Main category: cs.CL

TL;DR: 本文提出了一种由大语言模型（LLM）引导的样本选择框架，用于提升可穿戴传感器的人类活动识别，特别是在有限标签数据下区分相似动作。新方法在严格少样本条件下实现了高于传统方法的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的人类活动识别方法高度依赖庞大的有标签数据集，且通常只用几何特征选择样本，难以区分如“走路”、“上楼”、“下楼”等相似活动。需要引入更丰富的语义信息指导样本选择，以提高在少样本情况下的识别效果。

Method: 该方法利用LLM生成的知识先验（包括特征重要性、类别可混淆性和样本预算因子）指导样本评分与选择。结合边缘验证、PageRank中心性、疏密惩罚和设施选址优化等策略，得到紧凑且信息丰富的代表性样本集合。

Result: 在UCI-HAR数据集的严格少样本实验中，该方法取得了88.78%的宏F1分数，优于随机采样、群集和k-center等传统样本选取方法。

Conclusion: LLM生成的语义先验与结构和几何线索结合，为可穿戴传感器的人类活动识别中的代表样本选择提供了更强基础，尤其在少样本学习场景中展现出显著优势。

Abstract: In this paper, we propose an LLM-Guided Exemplar Selection framework to address a key limitation in state-of-the-art Human Activity Recognition (HAR) methods: their reliance on large labeled datasets and purely geometric exemplar selection, which often fail to distinguish similar weara-ble sensor activities such as walking, walking upstairs, and walking downstairs. Our method incorporates semantic reasoning via an LLM-generated knowledge prior that captures feature importance, inter-class confusability, and exemplar budget multipliers, and uses it to guide exemplar scoring and selection. These priors are combined with margin-based validation cues, PageRank centrality, hubness penalization, and facility-location optimization to obtain a compact and informative set of exemplars. Evaluated on the UCI-HAR dataset under strict few-shot conditions, the framework achieves a macro F1-score of 88.78%, outperforming classical approaches such as random sampling, herding, and $k$-center. The results show that LLM-derived semantic priors, when integrated with structural and geometric cues, provide a stronger foundation for selecting representative sensor exemplars in few-shot wearable-sensor HAR.

</details>


### [186] [Hallucination Detection and Evaluation of Large Language Model](https://arxiv.org/abs/2512.22416)
*Chenggong Zhang,Haopeng Wang*

Main category: cs.CL

TL;DR: 本文提出了一种高效的轻量级幻觉检测方法（HHEM），显著加快了大型语言模型（LLM）幻觉内容的评测速度，并在准确率方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有LLM幻觉检测方法如KnowHalu虽有效但计算成本高，不便于大规模应用，需要一种快速、高效、无依赖LLM判断的新方法。

Method: 提出了 Hughes Hallucination Evaluation Model (HHEM)，一种基于分类的轻量级检测框架，并通过引入细分文本片段的检索策略（segment-based retrieval）提升局部幻觉检测效果。

Result: HHEM将评测时间由8小时降至10分钟，结合非虚构性检查的准确率达到82.2%，TPR为78.9%；但在文本摘要任务中的细粒度幻觉检测存在不足，引入片段检索后检测能力提升。同时，较大参数量的模型更少产生幻觉，中等规模模型表现不稳定。

Conclusion: HHEM在保证检测准确率的前提下极大提高了评测效率，通过细粒度验证进一步提升幻觉检测能力，为LLM内容可靠性增强提供了新的框架。

Abstract: Hallucinations in Large Language Models (LLMs) pose a significant challenge, generating misleading or unverifiable content that undermines trust and reliability. Existing evaluation methods, such as KnowHalu, employ multi-stage verification but suffer from high computational costs. To address this, we integrate the Hughes Hallucination Evaluation Model (HHEM), a lightweight classification-based framework that operates independently of LLM-based judgments, significantly improving efficiency while maintaining high detection accuracy. We conduct a comparative analysis of hallucination detection methods across various LLMs, evaluating True Positive Rate (TPR), True Negative Rate (TNR), and Accuracy on question-answering (QA) and summarization tasks. Our results show that HHEM reduces evaluation time from 8 hours to 10 minutes, while HHEM with non-fabrication checking achieves the highest accuracy \(82.2\%\) and TPR \(78.9\%\). However, HHEM struggles with localized hallucinations in summarization tasks. To address this, we introduce segment-based retrieval, improving detection by verifying smaller text components. Additionally, our cumulative distribution function (CDF) analysis indicates that larger models (7B-9B parameters) generally exhibit fewer hallucinations, while intermediate-sized models show higher instability. These findings highlight the need for structured evaluation frameworks that balance computational efficiency with robust factual validation, enhancing the reliability of LLM-generated content.

</details>


### [187] [HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG](https://arxiv.org/abs/2512.22442)
*Cattalyya Nuengsigkapian*

Main category: cs.CL

TL;DR: 本文提出了HiFi-RAG系统，通过多阶段管道优化RAG流程，有效提升答案相关性和一致性，在NeurIPS 2025比赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: RAG系统在开放领域常因检索文档中无关信息和答案与用户意图不符而失效，亟需更高效和准确的RAG优化方案。

Method: 提出HiFi-RAG，采用分层过滤的多阶段检索生成流程，结合Gemini 2.5 Flash高效处理、过滤文档与归因，利用Gemini 2.5 Pro进行最终推理生成答案。

Result: 在MMU-RAGent验证集上，HiFi-RAG ROUGE-L提升至0.274（+19.6%），DeBERTaScore提升至0.677（+6.2%）；在测试集Test2025上，相较基线ROUGE-L提升57.4%，DeBERTaScore提升14.9%。

Conclusion: HiFi-RAG系统结合高效检索和强推理能力，明显提升了RAG在开放领域环境下的答案质量，对RAG优化具有重要意义。

Abstract: Retrieval-Augmented Generation (RAG) in open-domain settings faces significant challenges regarding irrelevant information in retrieved documents and the alignment of generated answers with user intent. We present HiFi-RAG (Hierarchical Filtering RAG), the winning closed-source system in the Text-to-Text static evaluation of the MMU-RAGent NeurIPS 2025 Competition. Our approach moves beyond standard embedding-based retrieval via a multi-stage pipeline. We leverage the speed and cost-efficiency of Gemini 2.5 Flash (4-6x cheaper than Pro) for query formulation, hierarchical content filtering, and citation attribution, while reserving the reasoning capabilities of Gemini 2.5 Pro for final answer generation. On the MMU-RAGent validation set, our system outperformed the baseline, improving ROUGE-L to 0.274 (+19.6%) and DeBERTaScore to 0.677 (+6.2%). On Test2025, our custom dataset evaluating questions that require post-cutoff knowledge (post January 2025), HiFi-RAG outperforms the parametric baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore.

</details>


### [188] [Exploring the Vertical-Domain Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2512.22443)
*Jie Zhou,Xin Chen,Jie Zhang,Zhe Li*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLMs）在会计领域中的推理能力，提出并建立了会计领域推理的评估标准，并对多款主流模型进行了实验评估，发现GPT-4在会计推理任务中表现最优，但现有模型距离实际企业应用仍有差距。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在各行各业的应用不断深入，将其与专业领域（如会计）高效结合是企业数字化转型和社会发展的关键挑战之一。因此，了解LLMs在会计垂直领域的推理能力，制定科学的评估标准，是推动其落地应用的基础。

Method: 本文首先提出会计领域垂直推理的概念，并分析GLM系列模型的训练数据特点，建立了会计推理评估标准。随后，基于这些标准，评测了GLM-6B、GLM-130B、GLM-4及OpenAI GPT-4等模型在会计推理任务中的表现，考察了不同提示工程策略对推理效果的影响。

Result: 实验结果表明，不同的提示工程策略对模型性能具有提升作用，其中GPT-4在会计推理能力上表现最为优异。

Conclusion: 尽管GPT-4在会计推理任务中表现最佳，但现有LLMs距离满足企业级会计实际应用需求仍有不足，需进一步针对行业特性优化，以充分发挥其在会计领域的价值。

Abstract: Large Language Models (LLMs) are reshaping learning paradigms, cognitive processes, and research methodologies across a wide range of domains. Integrating LLMs with professional fields and redefining the relationship between LLMs and domain-specific applications has become a critical challenge for promoting enterprise digital transformation and broader social development. To effectively integrate LLMs into the accounting domain, it is essential to understand their domain-specific reasoning capabilities. This study introduces the concept of vertical-domain accounting reasoning and establishes evaluation criteria by analyzing the training data characteristics of representative GLM-series models. These criteria provide a foundation for subsequent research on reasoning paradigms and offer benchmarks for improving accounting reasoning performance. Based on this framework, we evaluate several representative models, including GLM-6B, GLM-130B, GLM-4, and OpenAI GPT-4, on a set of accounting reasoning tasks. Experimental results show that different prompt engineering strategies lead to varying degrees of performance improvement across models, with GPT-4 achieving the strongest accounting reasoning capability. However, current LLMs still fall short of real-world application requirements. In particular, further optimization is needed for deployment in enterprise-level accounting scenarios to fully realize the potential value of LLMs in this domain.

</details>


### [189] [Constituency Structure over Eojeol in Korean Treebanks](https://arxiv.org/abs/2512.22487)
*Jungyeul Park,Chulwoo Park*

Main category: cs.CL

TL;DR: 本文针对韩语句法树库中的基本单元选择问题，提出用‘eojeol’（词组单位）而非词素作句法分析终结节点，可更好兼容不同依存树库。


<details>
  <summary>Details</summary>
Motivation: 韩语单词形态复杂，使用词素作为终结节点会将词内部结构与短语语法结构混淆，也导致与基于‘eojeol’的依存关系资源不匹配。

Method: 作者提出以‘eojeol’为基础的句法结构表示方案，将形态分析和细致词性信息单独编码而不与句法成分混合。同时，对Sejong和Penn韩语树库在统一‘eojeol’下进行规范化和对比分析。

Result: 结果显示，在明确规范假设下，Sejong和Penn韩语树库在‘eojeol’为基础的成分层面上具有等价的代表性。同时，基于‘eojeol’的标注方法能够保持句法结构可解释性并兼容不同树库相互转换。

Conclusion: 基于‘eojeol’的成分句法标注能更合理处理韩语结构，为句法树库间的对比、转换和研究提供统一基础。

Abstract: The design of Korean constituency treebanks raises a fundamental representational question concerning the choice of terminal units. Although Korean words are morphologically complex, treating morphemes as constituency terminals conflates word internal morphology with phrase level syntactic structure and creates mismatches with eojeol based dependency resources. This paper argues for an eojeol based constituency representation, with morphological segmentation and fine grained part of speech information encoded in a separate, non constituent layer. A comparative analysis shows that, under explicit normalization assumptions, the Sejong and Penn Korean treebanks can be treated as representationally equivalent at the eojeol based constituency level. Building on this result, we outline an eojeol based annotation scheme that preserves interpretable constituency and supports cross treebank comparison and constituency dependency conversion.

</details>


### [190] [ManchuTTS: Towards High-Quality Manchu Speech Synthesis via Flow Matching and Hierarchical Text Representation](https://arxiv.org/abs/2512.22491)
*Suhua Wang,Zifan Wang,Xiaoxin Sun,D. J. Wang,Zhanbo Liu,Xin Li*

Main category: cs.CL

TL;DR: 本文提出了一种面向满语的端到端语音合成方法 ManchuTTS，通过创新的文本表征和对齐机制，有效缓解数据稀缺与粘着语音特征难题，显著提升满语合成表现。


<details>
  <summary>Details</summary>
Motivation: 满语作为濒危语言，现有语音合成方法面临数据极度匮乏和复杂粘着语音结构两大挑战。现有TTS系统无法有效捕捉满语的分层、黏着式语言特性，需专门化处理以助语言存续和语音处理研究。

Method: 设计了三层次（音位、音节、韵律）文本表征与跨模态分层注意力机制，实现多粒度对齐。模型结构融合深度卷积网络和流匹配Transformer，实现高效的非自回归生成；引入分层对比损失以加强声学-语言结构对应；自主构建首个满语TTS数据集，配合数据增强策略缓解数据稀缺。

Result: 在仅5.2小时训练集上，ManchuTTS主观MOS得分达4.52，显著优于所有基线模型。消融实验显示，分层机制使满语粘着词发音准确率提升31%，韵律自然度提升27%。

Conclusion: ManchuTTS突破现有TTS对满语等极低资源、强黏着性语言的限制，为濒危语言的数字化和语音技术研究带来有价值进展。

Abstract: As an endangered language, Manchu presents unique challenges for speech synthesis, including severe data scarcity and strong phonological agglutination. This paper proposes ManchuTTS(Manchu Text to Speech), a novel approach tailored to Manchu's linguistic characteristics. To handle agglutination, this method designs a three-tier text representation (phoneme, syllable, prosodic) and a cross-modal hierarchical attention mechanism for multi-granular alignment. The synthesis model integrates deep convolutional networks with a flow-matching Transformer, enabling efficient, non-autoregressive generation. This method further introduce a hierarchical contrastive loss to guide structured acoustic-linguistic correspondence. To address low-resource constraints, This method construct the first Manchu TTS dataset and employ a data augmentation strategy. Experiments demonstrate that ManchuTTS attains a MOS of 4.52 using a 5.2-hour training subset derived from our full 6.24-hour annotated corpus, outperforming all baseline models by a notable margin. Ablations confirm hierarchical guidance improves agglutinative word pronunciation accuracy (AWPA) by 31% and prosodic naturalness by 27%.

</details>


### [191] [Learning When Not to Attend Globally](https://arxiv.org/abs/2512.22562)
*Xuan Luo,Kailai Zhang,Xifeng Yan*

Main category: cs.CL

TL;DR: 本文提出了一种类似人类阅读策略的新注意力机制AHA（All-or-Here Attention），显著减少了全局注意力操作，提升了大型语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型多依赖于全局自注意力机制，虽能捕获长程依赖，但计算成本高且难以扩展。而人类阅读主要关注当前页面，只在必要时回顾前文，因此该研究尝试借鉴人类阅读策略，提升模型推理效率。

Method: 作者提出AHA机制，为每个注意力头设计一个二值路由器，根据需求动态切换全局注意力和局部滑动窗口注意力，对每个token单独判断是否需访问全局信息。通过调整窗口大小，分析了模型对上下文依赖的具体分布。

Result: 实验表明，窗口大小为256时，93%的全注意力计算可被局部注意力替代且无性能损失。不同窗口实验揭示了依赖全局上下文的需求呈长尾分布，窗口拓展后需全局访问的情况迅速减少。

Conclusion: AHA机制成功证明了在大多数场景下全局注意力冗余，通过按需全局访问大幅提升了推理效率。这启发未来高效模型设计应区分局部和全局处理，主打“按需访问”策略。

Abstract: When reading books, humans focus primarily on the current page, flipping back to recap prior context only when necessary. Similarly, we demonstrate that Large Language Models (LLMs) can learn to dynamically determine when to attend to global context. We propose All-or-Here Attention (AHA), which utilizes a binary router per attention head to dynamically toggle between full attention and local sliding window attention for each token. Our results indicate that with a window size of 256 tokens, up to 93\% of the original full attention operations can be replaced by sliding window attention without performance loss. Furthermore, by evaluating AHA across various window sizes, we identify a long-tail distribution in context dependency, where the necessity for full attention decays rapidly as the local window expands. By decoupling local processing from global access, AHA reveals that full attention is largely redundant, and that efficient inference requires only on-demand access to the global context.

</details>


### [192] [Structured Prompting and LLM Ensembling for Multimodal Conversational Aspect-based Sentiment Analysis](https://arxiv.org/abs/2512.22603)
*Zhiqiang Gao,Shihao Gao,Zixing Zhang,Yihao Guo,Hongyu Chen,Jing Han*

Main category: cs.CL

TL;DR: 本文提出了一种利用结构化提示和模型集成的方法，提升了多模态对话情感分析的能力，尤其在情感要素提取和情绪转变检测两个任务上均取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 多模态对话情感分析涉及多个参与者和丰富上下文，是实现具备情感智能AI系统的重要挑战。本研究旨在解决完整情感要素提取与情绪转变检测等高难任务，推动相关技术发展。

Method: 针对任务一，作者设计了结构化提示链，引导大模型逐步、顺序地提取持有者、目标、方面、观点、情感和理由六元组；针对任务二，采用多个大模型集成方案，充分利用它们的互补性来识别情感翻转及其原因。

Result: 该系统在MCABSA挑战赛的两个子任务中分别取得了47.38%的平均分和74.12%的情感翻转精确匹配F1分数，显示出逐步细化和模型集成方法在多模态情感分析中的有效性。

Conclusion: 综合结构化分步处理和模型集成显著提升了多模态对话中的情感元素提取及情感转变检测能力，为开发情感智能AI系统提供了可行思路。

Abstract: Understanding sentiment in multimodal conversations is a complex yet crucial challenge toward building emotionally intelligent AI systems. The Multimodal Conversational Aspect-based Sentiment Analysis (MCABSA) Challenge invited participants to tackle two demanding subtasks: (1) extracting a comprehensive sentiment sextuple, including holder, target, aspect, opinion, sentiment, and rationale from multi-speaker dialogues, and (2) detecting sentiment flipping, which detects dynamic sentiment shifts and their underlying triggers. For Subtask-I, in the present paper, we designed a structured prompting pipeline that guided large language models (LLMs) to sequentially extract sentiment components with refined contextual understanding. For Subtask-II, we further leveraged the complementary strengths of three LLMs through ensembling to robustly identify sentiment transitions and their triggers. Our system achieved a 47.38% average score on Subtask-I and a 74.12% exact match F1 on Subtask-II, showing the effectiveness of step-wise refinement and ensemble strategies in rich, multimodal sentiment analysis tasks.

</details>


### [193] [Chain-of-thought Reviewing and Correction for Time Series Question Answering](https://arxiv.org/abs/2512.22627)
*Chen Su,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: 本文提出了T3LLM框架，通过引入工人、审查者和学生三种大模型角色，有效提升了大模型在处理时间序列问答任务中的推理准确性和纠错能力，并在多个基准数据集上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在时间序列问答任务中容易出现推理错误，因为它们多采用通用的自然语言处理技术，未能充分利用时间序列数据可验证的特点。因此，作者希望设计一种机制，提升模型在该领域的推理可靠性。

Method: T3LLM框架包含三个大型语言模型角色：工人负责按照结构化提示逐步生成推理链；审查者负责检查推理过程、识别错误并给出改正建议；学生则用合作生成的、已纠错的推理链进行微调，从而内化多步推理和自我纠错能力。

Result: 在多个现实世界的时间序列问答基准数据集上，T3LLM相较于现有的大模型方法取得了更高的性能表现，验证了所提方法的有效性。

Conclusion: 本文通过结合模型间合作和显式纠错机制，显著提升了大模型在时间序列问答任务中的推理准确性和鲁棒性，为相关领域任务提供了新思路。

Abstract: With the advancement of large language models (LLMs), diverse time series analysis tasks are reformulated as time series question answering (TSQA) through a unified natural language interface. However, existing LLM-based approaches largely adopt general natural language processing techniques and are prone to reasoning errors when handling complex numerical sequences. Different from purely textual tasks, time series data are inherently verifiable, enabling consistency checking between reasoning steps and the original input. Motivated by this property, we propose T3LLM, which performs multi-step reasoning with an explicit correction mechanism for time series question answering. The T3LLM framework consists of three LLMs, namely, a worker, a reviewer, and a student, that are responsible for generation, review, and reasoning learning, respectively. Within this framework, the worker generates step-wise chains of thought (CoT) under structured prompts, while the reviewer inspects the reasoning, identifies erroneous steps, and provides corrective comments. The collaboratively generated corrected CoT are used to fine-tune the student model, internalizing multi-step reasoning and self-correction into its parameters. Experiments on multiple real-world TSQA benchmarks demonstrate that T3LLM achieves state-of-the-art performance over strong LLM-based baselines.

</details>


### [194] [M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation](https://arxiv.org/abs/2512.22628)
*Fanglin Xu,Wei Zhang,Jian Yang,Guo Chen,Aishan Liu,Zhoujun Li,Xianglong Liu,Bryan Dai*

Main category: cs.CL

TL;DR: 本文提出了一个名为M2G-Eval的多粒度、多语言代码生成评测框架，涵盖4个代码粒度、18种编程语言，并用其系统评测了30个主流LLM的代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 目前代码生成大模型的评测局限在单一结构层面和有限的编程语言，难以揭示不同粒度和多语言下的模型细微能力差异。

Method: 构建M2G-Eval评测框架，设立Class、Function、Block、Line四个粒度，覆盖18种编程语言，含1.7万+训练任务与1286个人工标注测试集。提出两种M2G-Eval-Coder模型，并系统评测28个主流LLM和自研模型。

Result: 发现：(1) 代码粒度越细（如Line），模型表现越好，越粗（如Class）越难；(2) 随任务复杂度提升，不同语言模型性能差距扩大；(3) 不同语言间的模型表现强相关，表明存在可迁移的编程概念学习能力。

Conclusion: M2G-Eval能细致诊断LLM代码生成能力，揭示当前模型在复杂、长代码生成上的挑战，对未来多语言、多粒度能力提升方向具有指导意义。

Abstract: The rapid advancement of code large language models (LLMs) has sparked significant research interest in systematically evaluating their code generation capabilities, yet existing benchmarks predominantly assess models at a single structural granularity and focus on limited programming languages, obscuring fine-grained capability variations across different code scopes and multilingual scenarios. We introduce M2G-Eval, a multi-granularity, multilingual framework for evaluating code generation in large language models (LLMs) across four levels: Class, Function, Block, and Line. Spanning 18 programming languages, M2G-Eval includes 17K+ training tasks and 1,286 human-annotated, contamination-controlled test instances. We develop M2G-Eval-Coder models by training Qwen3-8B with supervised fine-tuning and Group Relative Policy Optimization. Evaluating 30 models (28 state-of-the-art LLMs plus our two M2G-Eval-Coder variants) reveals three main findings: (1) an apparent difficulty hierarchy, with Line-level tasks easiest and Class-level most challenging; (2) widening performance gaps between full- and partial-granularity languages as task complexity increases; and (3) strong cross-language correlations, suggesting that models learn transferable programming concepts. M2G-Eval enables fine-grained diagnosis of code generation capabilities and highlights persistent challenges in synthesizing complex, long-form code.

</details>


### [195] [On the Role of Discreteness in Diffusion LLMs](https://arxiv.org/abs/2512.22630)
*Ziqi Jin,Bin Wang,Xiang Lin,Lidong Bing,Aixin Sun*

Main category: cs.CL

TL;DR: 本文分析了扩散模型在文本生成中的应用，指出现有方法在关键性质上存在结构性权衡，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型具备并行解码和迭代优化等优点，但文本的离散和高度结构化特性给其直接应用带来挑战。论文旨在阐明扩散过程与语言建模的差异，并指导未来模型设计。

Method: 作者从扩散过程和语言建模的角度，归纳了五个扩散机制与语言生成需求不同的核心属性，并将现有工作分为embedding空间连续扩散和token离散扩散两类，分析它们各自仅满足部分关键属性。

Result: 通过对当前大型扩散语言模型的分析，作者发现：（1）均匀噪声破坏未考虑信息在序列中的分布特点；（2）基于token的单独训练欠缺对多token依赖关系的建模，限制了并行生成的表达能力。

Conclusion: 建议未来研究设计更能体现文本结构的扩散过程，以实现更连贯的文本生成扩散模型。

Abstract: Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.

</details>


### [196] [Evaluating GRPO and DPO for Faithful Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2512.22631)
*Hadi Mohammadi,Tamas Kozak,Anastasia Giachanou*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型在链式思维推理过程中的真实性问题，并对两种优化方法（GRPO和DPO）在提升推理可信度上的效果进行比较。结果表明，GRPO在大型模型上的表现优于DPO，尤其在提升推理解释的可信度方面更具潜力。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理虽然能提升大语言模型的多步推理能力，但其理由常与实际推理过程不一致，可能导致模型输出误导性解释，影响模型的安全监督与对齐。因此，本研究旨在改善推理的真实性和可信赖性。

Method: 研究比较了两种优化方法：群组相对策略优化（GRPO）和直接偏好优化（DPO），通过实验分析它们对不同规模模型在链式推理真实性上的改进效果。

Result: 实验发现，GRPO在大模型上的表现（如Qwen2.5-14B-Instruct）优于DPO，并在所有评价指标上获得最佳成绩。两种方法的性能均随模型规模提升而提升，但GRPO在信度提升方面表现更突出，小模型上波动较大。

Conclusion: GRPO在提升大语言模型推理真实性方面前景可观，有助于构建更具透明度和信赖度的推理系统。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful technique for improving the problem-solving capabilities of large language models (LLMs), particularly for tasks requiring multi-step reasoning. However, recent studies show that CoT explanations often fail to reflect the model's actual reasoning process, as models may produce coherent yet misleading justifications or modify answers without acknowledging external cues. Such discrepancies undermine the reliability of CoT-based methods for safety supervision and alignment monitoring, as models can generate plausible but deceptive rationales for incorrect answers. To better understand this limitation, we evaluate two optimization methods, Group Relative Policy Optimization (GRPO) and Direct Preference Optimization (DPO), in their ability to improve CoT faithfulness. Our experiments show that GRPO achieves higher performance than DPO in larger models, with the Qwen2.5-14B-Instruct model attaining the best results across all evaluation metrics. Both approaches exhibit positive correlations between model size and performance, but GRPO shows greater potential for improving faithfulness metrics, albeit with less stable behavior at smaller scales. These results suggest that GRPO offers a promising direction for developing more transparent and trustworthy reasoning in LLMs.

</details>


### [197] [Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2](https://arxiv.org/abs/2512.22671)
*Pere Martra*

Main category: cs.CL

TL;DR: 本文通过对GLU-MLP结构层采用最大绝对权重（MAW）准则进行通道宽度剪枝，发现模型认知能力削弱并非均匀发生。具体表现为：知识密集型任务表现下降，但指令跟随与推理能力保持甚至提升。


<details>
  <summary>Details</summary>
Motivation: 模型压缩（如剪枝）一向被认为会带来各能力的均匀退化。然而，实际是否如此以及背后机理尚未被系统性研究。本文旨在揭示宽度剪枝对模型不同能力的差异化影响，找出其中蕴含的结构与认知调控联系。

Method: 针对GLU-MLP层采用MAW准则进行宽度剪枝，设置七种不同扩展比模型。利用MMLU、GSM8K、IFEval、MUSR、TruthfulQA等主流基准测试其在知识、推理、语义、指令跟随及真实性方向的能力变化，并分析各项指标间的相关性。

Result: 知识相关任务与困惑度随剪枝加剧下滑，但指令跟随能力大幅提升（提升46%-75%），多步推理能力保持稳定。知识与真实性出现显著负相关性（Llama-3B模型r=-0.864，p=0.012），剪枝后模型歧辨错误能力增强。模型能耗降低23%，但单次请求时延增加，批处理效率无明显损失。

Conclusion: 宽度剪枝效果具有选择性，对认知能力有差异调节作用。MAW剪枝能削弱参数知识，但提升行为对齐和真实性能力。这为模型结构与认知能力调控及剪枝策略优化提供新方向。

Abstract: Structured width pruning of GLU-MLP layers, guided by the Maximum Absolute Weight (MAW) criterion, reveals a systematic dichotomy in how reducing the expansion ratio affects different model capabilities. While performance on tasks relying on parametric knowledge (e.g., MMLU, GSM8K) and perplexity metrics degrades predictably, instruction-following capabilities improve substantially (+46% to +75% in IFEval for Llama-3.2-1B and 3B models), and multi-step reasoning remains robust (MUSR). This pattern challenges the prevailing assumption that pruning induces uniform degradation. We evaluated seven expansion ratio configurations using comprehensive benchmarks assessing factual knowledge, mathematical reasoning, language comprehension, instruction-following, and truthfulness. Our analysis identifies the expansion ratio as a critical architectural parameter that selectively modulates cognitive capabilities, rather than merely serving as a compression metric. We provide the first systematic characterization of this selective preservation phenomenon. Notably, we document a robust inverse correlation (r = -0.864, p = 0.012 in Llama-3B) between factual knowledge capacity (MMLU) and truthfulness metrics (TruthfulQA-MC2): as knowledge degrades, the model's ability to discriminate misconceptions improves consistently. This connects two previously distinct research areas, demonstrating that MAW-guided width pruning acts as a selective filter, reducing parametric knowledge while preserving or enhancing behavioral alignment. Additionally, we quantify context-dependent efficiency trade-offs: pruned configurations achieve up to 23% reduction in energy consumption (J/token) but incur penalties in single-request latency, whereas batch processing workloads benefit uniformly.

</details>


### [198] [Conformal Prediction Sets for Next-Token Prediction in Large Language Models: Balancing Coverage Guarantees with Set Efficiency](https://arxiv.org/abs/2512.22682)
*Yoshith Roy Kotla,Varshith Roy Kotla*

Main category: cs.CL

TL;DR: 文章提出了一种新方法Vocabulary-Aware Conformal Prediction (VACP)，有效提升大型语言模型在不确定性量化时的预测效率和实用性。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域部署大型语言模型时，模型输出的不确定性需要被精确量化，但现有softmax概率的置信度往往不准确。传统的保覆盖方法虽可达到期望置信度，却导致预测集过于庞大、缺乏实用性，因此亟需能兼顾覆盖率和预测集紧凑性的创新方法。

Method: 系统性研究自适应预测集（APS）在大型词表（25万+）下的适用性，发现现有方法虽然保证覆盖率但预测集过大。提出VACP框架，通过语义掩码和温度调节得分机制，有效缩减预测空间，并在理论上保证marginal coverage。

Result: 在Gemma-2B、SQUAD和WikiText等基准上，VACP在保持接近目标覆盖率（90%目标，实测89.7%）同时，将预测集平均规模从847个候选词大幅减至4.3个，实现了197倍的效率提升。

Conclusion: VACP显著优化了大型语言模型预测集的覆盖与效率平衡，为高可靠性应用场景下的不确定性量化提供了落地方案，相关实现已开源。

Abstract: Deploying large language models (LLMs) in high-stakes domains requires rigorous uncertainty quantification, yet standard softmax probabilities are often poorly calibrated. We present a systematic study of Adaptive Prediction Sets (APS) applied to next-token prediction in transformer-based models with large vocabularies (greater than 250,000 tokens). Our central contribution is the identification of a coverage-efficiency tradeoff: while naive conformal prediction achieves valid coverage, it produces prediction sets of hundreds of tokens, rendering them uninformative. We propose Vocabulary-Aware Conformal Prediction (VACP), a framework that leverages semantic masking and temperature-adjusted scoring to reduce the effective prediction space while provably maintaining marginal coverage. Experiments on Gemma-2B using SQUAD and WikiText benchmarks demonstrate that VACP achieves 89.7 percent empirical coverage (90 percent target) while reducing the mean prediction set size from 847 tokens to 4.3 tokens -- a 197x improvement in efficiency. We provide a theoretical analysis of vocabulary reduction and release our implementation for reproducibility.

</details>


### [199] [GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2512.22705)
*Ahmed Abdullah,Sana Fatima,Haroon Mahmood*

Main category: cs.CL

TL;DR: 本文提出了一种多语言希望言论检测框架，重点针对资源匮乏的乌尔都语，采用多种预训练transformer模型并在多个语言上进行实验，结果显示在乌尔都语等多种语言上均取得了优异的分类效果。


<details>
  <summary>Details</summary>
Motivation: 以往NLP对希望言论关注较少，且相关研究和资源主要集中在英语，导致乌尔都语等低资源语言在积极言论检测工具方面严重匮乏，需要开发适用于多语言、低资源环境的检测方法。

Method: 采用XLM-RoBERTa、mBERT、EuroBERT和UrduBERT等多语种预训练transformer模型，进行简单预处理，并在PolyHope-M 2025数据集上训练和评估分类器，以提升希望言论检测的效果。

Result: 在乌尔都语二分类任务上F1分数高达95.2%，多分类任务为65.2%，在西班牙语、德语和英语上也取得了有竞争力的结果，表明方法的跨语言适用性。

Conclusion: 现有多语种预训练模型可以有效应用于低资源环境下的希望言论检测，为促进数字空间中积极健康交流提供了可能。

Abstract: Hope speech has been relatively underrepresented in Natural Language Processing (NLP). Current studies are largely focused on English, which has resulted in a lack of resources for low-resource languages such as Urdu. As a result, the creation of tools that facilitate positive online communication remains limited. Although transformer-based architectures have proven to be effective in detecting hate and offensive speech, little has been done to apply them to hope speech or, more generally, to test them across a variety of linguistic settings. This paper presents a multilingual framework for hope speech detection with a focus on Urdu. Using pretrained transformer models such as XLM-RoBERTa, mBERT, EuroBERT, and UrduBERT, we apply simple preprocessing and train classifiers for improved results. Evaluations on the PolyHope-M 2025 benchmark demonstrate strong performance, achieving F1-scores of 95.2% for Urdu binary classification and 65.2% for Urdu multi-class classification, with similarly competitive results in Spanish, German, and English. These results highlight the possibility of implementing existing multilingual models in low-resource environments, thus making it easier to identify hope speech and helping to build a more constructive digital discourse.

</details>


### [200] [Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages](https://arxiv.org/abs/2512.22712)
*Anaelia Ovalle,Candace Ross,Sebastian Ruder,Adina Williams,Karen Ullrich,Mark Ibrahim,Levent Sagun*

Main category: cs.CL

TL;DR: 本文提出了一种人工验证框架，用于跨语言评估大语言模型的推理是否真实支撑其结论。结果发现，在非拉丁语系中，推理与结论的不一致翻倍，现有多语言评测低估了模型推理问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在链式思考等推理表现优秀，但这些推理能力能否在不同语言间转移尚未充分研究。目前多语言评测主要看任务准确率，忽视了推理质量与结论之间的一致性。

Method: 作者提出了一套人类验证的跨语言推理评估框架，分析了6种语言、6种前沿模型、6.5万条推理 traces，并开发了基于人工标注的错误分类标准，详细划分了推理失败类型。

Result: 发现模型虽然任务准确率高，但推理无法真正支持结论，尤其在非拉丁语系中，推理与结论的不一致至少高出拉丁语系一倍。主要问题为证据错误（如无支撑主张、模糊事实）及不合逻辑的推理步骤。

Conclusion: 当前多语言评测未能充分揭示模型推理能力的真实状况，需引入关注推理过程的评估方法以全面反映模型质量。

Abstract: Large language models demonstrate strong reasoning capabilities through chain-of-thought prompting, but whether this reasoning quality transfers across languages remains underexplored. We introduce a human-validated framework to evaluate whether model-generated reasoning traces logically support their conclusions across languages. Analyzing 65k reasoning traces from GlobalMMLU questions across 6 languages and 6 frontier models, we uncover a critical blind spot: while models achieve high task accuracy, their reasoning can fail to support their conclusions. Reasoning traces in non-Latin scripts show at least twice as much misalignment between their reasoning and conclusions than those in Latin scripts. We develop an error taxonomy through human annotation to characterize these failures, finding they stem primarily from evidential errors (unsupported claims, ambiguous facts) followed by illogical reasoning steps. Our findings demonstrate that current multilingual evaluation practices provide an incomplete picture of model reasoning capabilities and highlight the need for reasoning-aware evaluation frameworks.

</details>


### [201] [Mitigating Social Desirability Bias in Random Silicon Sampling](https://arxiv.org/abs/2512.22725)
*Sashank Chapala,Maksym Mironov,Songgaojun Deng*

Main category: cs.CL

TL;DR: 本研究探讨如何通过最小化、心理学基础的提示词设计，降低大型语言模型（LLM）在模拟人口问卷数据时的社会期望偏差，从而提升其代表性。


<details>
  <summary>Details</summary>
Motivation: LLM被用于替代人群调查中的“硅采样”（Silicon Sampling），但在遇到社会敏感问题时，模型回答可能偏向社会认同答案，导致与真实人类数据不符。目前有关这类偏差及缓解方法的研究有限，亟需改进促进LLM生成更具代表性的结果。

Method: 研究基于美国国家选举研究（ANES）数据，选用Llama-3.1和GPT-4.1-mini三种开源LLM，对同一问卷实验复制，并检验四种通过提示词控制偏差的方法：中立重述、语义反转，以及两种元指令（分析倾向、真诚倾向）。通过Jensen-Shannon Divergence和bootstrap置信区间评估模型答案与人类数据的分布一致性。

Result: 中立重述（reformulated）的提示词能显著减少模型对社会认同答案的集中，生成结果与ANES数据更为接近。语义反转方法在不同调查项上效果不一致。分析倾向与真诚倾向两类元指令，并未带来系统性偏差降低，反而使得答案更均匀分布。

Conclusion: 提示词重新设计是一种行之有效的干预手段，可以有效缓解LLM在敏感社会问题上产生的社会期望偏差，有助于生成更具代表性的模拟人群数据样本。

Abstract: Large Language Models (LLMs) are increasingly used to simulate population responses, a method known as ``Silicon Sampling''. However, responses to socially sensitive questions frequently exhibit Social Desirability Bias (SDB), diverging from real human data toward socially acceptable answers. Existing studies on social desirability bias in LLM-based sampling remain limited. In this work, we investigate whether minimal, psychologically grounded prompt wording can mitigate this bias and improve alignment between silicon and human samples. We conducted a study using data from the American National Election Study (ANES) on three LLMs from two model families: the open-source Llama-3.1 series and GPT-4.1-mini. We first replicate a baseline silicon sampling study, confirming the persistent Social Desirability Bias. We then test four prompt-based mitigation methods: \emph{reformulated} (neutral, third-person phrasing), \emph{reverse-coded} (semantic inversion), and two meta-instructions, \emph{priming} and \emph{preamble}, respectively encouraging analytics and sincerity. Alignment with ANES is evaluated using Jensen-Shannon Divergence with bootstrap confidence intervals. Our results demonstrate that reformulated prompts most effectively improve alignment by reducing distribution concentration on socially acceptable answers and achieving distributions closer to ANES. Reverse-coding produced mixed results across eligible items, while the Priming and Preamble encouraged response uniformity and showed no systematic benefit for bias mitigation. Our findings validate the efficacy of prompt-based framing controls in mitigating inherent Social Desirability Bias in LLMs, providing a practical path toward more representative silicon samples.

</details>


### [202] [Data Augmentation for Classification of Negative Pregnancy Outcomes in Imbalanced Data](https://arxiv.org/abs/2512.22732)
*Md Badsha Biswas*

Main category: cs.CL

TL;DR: 本研究提出利用社交媒体（如Twitter）数据，通过自然语言处理方法，改善对负面妊娠结局（如流产、死产和出生缺陷）的大规模流行病学研究。


<details>
  <summary>Details</summary>
Motivation: 美国婴儿死亡率依然高企，出生缺陷是主要原因。现有数据和研究手段对妊娠不良结局的成因理解有限，需要更全面和创新的数据源与研究策略。

Method: 本文设计NLP流程，自动识别并分类推文中女性分享的妊娠经历，将正常足月和出生体重视为对照组，将出现负面妊娠结局的作为病例组，并采取一系列数据预处理和增强技术，以应对社交媒体数据的噪声、不平衡和无结构特点。

Result: 成功构建了基于社交媒体信息的病例与对照分组方法，可提升妊娠结局研究的数据量和多样性，为分析干预、治疗或暴露对母婴健康的因果影响提供了新工具。

Conclusion: 社交媒体数据可作为流行病学妊娠研究的补充资源。本方法为未来孕妇队列及对照组的健康研究提供了新框架与可行性，有助于更全面理解和干预妊娠不良结局。

Abstract: Infant mortality remains a significant public health concern in the United States, with birth defects identified as a leading cause. Despite ongoing efforts to understand the causes of negative pregnancy outcomes like miscarriage, stillbirths, birth defects, and premature birth, there is still a need for more comprehensive research and strategies for intervention. This paper introduces a novel approach that uses publicly available social media data, especially from platforms like Twitter, to enhance current datasets for studying negative pregnancy outcomes through observational research. The inherent challenges in utilizing social media data, including imbalance, noise, and lack of structure, necessitate robust preprocessing techniques and data augmentation strategies. By constructing a natural language processing (NLP) pipeline, we aim to automatically identify women sharing their pregnancy experiences, categorizing them based on reported outcomes. Women reporting full gestation and normal birth weight will be classified as positive cases, while those reporting negative pregnancy outcomes will be identified as negative cases. Furthermore, this study offers potential applications in assessing the causal impact of specific interventions, treatments, or prenatal exposures on maternal and fetal health outcomes. Additionally, it provides a framework for future health studies involving pregnant cohorts and comparator groups. In a broader context, our research showcases the viability of social media data as an adjunctive resource in epidemiological investigations about pregnancy outcomes.

</details>


### [203] [WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference](https://arxiv.org/abs/2512.22737)
*Aiwei Liu,Minghua He,Shaoxun Zeng,Sijun Zhang,Linhao Zhang,Chuhan Wu,Wei Jia,Yuan Liu,Xiao Zhou,Jie Zhou*

Main category: cs.CL

TL;DR: 论文提出一种新的扩散语言模型解码框架WeDLM，可以在推理时实现更高效的并行生成。通过引入拓扑重排和流式解码，兼顾生成质量和推理速度，对比当前主流优化的自回归引擎vLLM实现显著提速。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型推理主要采用自回归(AR)生成方式，推理阶段只能逐步生成，缺乏并行性，影响了实际部署速度。虽然扩散式语言模型（DLLMs）理论上可以并行生成，但实践中由于使用双向注意力导致难以高效缓存上下文，反而效率不高。该论文试图解决扩散类模型实际并行推理效率不佳的瓶颈。

Method: WeDLM完全基于标准的因果（单向）注意力机制设计扩散解码框架，通过拓扑重排将已知token移到物理前缀实现前缀缓存友好，保持逻辑位置信息不变，同时设计了流式生成策略，持续提交有置信度的token并维持并行负载，避免块式方法的等待和停顿。

Result: 实验显示WeDLM在保持强大AR骨干网络生成质量的前提下，推理速度比vLLM快近3倍（在推理类任务上），在生成熵较低场景下可达到10倍加速。在相同部署条件下，WeDLM首次实现扩散式解码超越优化AR引擎。

Conclusion: WeDLM在保证生成质量的基础上，大幅提升了扩散式并行解码的实际部署速度，证明扩散解码方法可以在实践中优于优化的自回归解码，为大模型高效部署提供新方案。

Abstract: Autoregressive (AR) generation is the standard decoding paradigm for Large Language Models (LLMs), but its token-by-token nature limits parallelism at inference time. Diffusion Language Models (DLLMs) offer parallel decoding by recovering multiple masked tokens per step; however, in practice they often fail to translate this parallelism into deployment speed gains over optimized AR engines (e.g., vLLM). A key reason is that many DLLMs rely on bidirectional attention, which breaks standard prefix KV caching and forces repeated contextualization, undermining efficiency. We propose WeDLM, a diffusion decoding framework built entirely on standard causal attention to make parallel generation prefix-cache friendly. The core idea is to let each masked position condition on all currently observed tokens while keeping a strict causal mask, achieved by Topological Reordering that moves observed tokens to the physical prefix while preserving their logical positions. Building on this property, we introduce a streaming decoding procedure that continuously commits confident tokens into a growing left-to-right prefix and maintains a fixed parallel workload, avoiding the stop-and-wait behavior common in block diffusion methods. Experiments show that WeDLM preserves the quality of strong AR backbones while delivering substantial speedups, approaching 3x on challenging reasoning benchmarks and up to 10x in low-entropy generation regimes; critically, our comparisons are against AR baselines served by vLLM under matched deployment settings, demonstrating that diffusion-style decoding can outperform an optimized AR engine in practice.

</details>


### [204] [Harnessing Large Language Models for Biomedical Named Entity Recognition](https://arxiv.org/abs/2512.22738)
*Jian Chen,Leilei Su,Cong Sun*

Main category: cs.CL

TL;DR: 本文提出了一种高效且注重数据质量的BioNER微调框架BioSelectTune，通过创新的数据筛选方法显著提升大语言模型在生物医学命名实体识别任务上的表现，并超越了现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 当前通用大语言模型在生物医学命名实体识别（BioNER）任务上因领域知识匮乏和低质量训练数据而表现不佳。本文旨在通过数据质量驱动的方法，克服上述局限，提升BioNER任务效能。

Method: 提出BioSelectTune框架，将BioNER任务转化为结构化JSON生成任务，引入Hybrid Superfiltering混合超筛选策略，利用弱模型到强模型的数据精选过程，提炼出高价值训练集，以提高模型微调质量。

Result: 在多项BioNER基准测试中，BioSelectTune取得了最新最高（SOTA）表现；使用仅50%的精选正例数据，对比全量训练数据的基线模型和专业领域模型（如BioMedBERT），BioSelectTune均实现超越。

Conclusion: BioSelectTune证明，在BioNER任务中，优质的数据筛选和结构化任务转化能够极大提升大语言模型的效果，为领域任务微调提供了新思路。

Abstract: Background and Objective: Biomedical Named Entity Recognition (BioNER) is a foundational task in medical informatics, crucial for downstream applications like drug discovery and clinical trial matching. However, adapting general-domain Large Language Models (LLMs) to this task is often hampered by their lack of domain-specific knowledge and the performance degradation caused by low-quality training data. To address these challenges, we introduce BioSelectTune, a highly efficient, data-centric framework for fine-tuning LLMs that prioritizes data quality over quantity. Methods and Results: BioSelectTune reformulates BioNER as a structured JSON generation task and leverages our novel Hybrid Superfiltering strategy, a weak-to-strong data curation method that uses a homologous weak model to distill a compact, high-impact training dataset. Conclusions: Through extensive experiments, we demonstrate that BioSelectTune achieves state-of-the-art (SOTA) performance across multiple BioNER benchmarks. Notably, our model, trained on only 50% of the curated positive data, not only surpasses the fully-trained baseline but also outperforms powerful domain-specialized models like BioMedBERT.

</details>


### [205] [Text-Routed Sparse Mixture-of-Experts Model with Explanation and Temporal Alignment for Multi-Modal Sentiment Analysis](https://arxiv.org/abs/2512.22741)
*Dongning Rao,Yunbiao Zeng,Zhihua Jiang,Jujian Lv*

Main category: cs.CL

TL;DR: 本文提出了一种结合解释能力和时序对齐的新型多模态情感分析模型TEXT，在多个数据集上大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析方法对于不同模态间的微妙情感分析已有很多探索，但对解释能力和时序（temporal）对齐关注较少。本研究旨在弥补这一空白，提升模型的可解释性和跨模态时序信息融合能力。

Method: 提出TEXT模型，主要创新包括：1）借助多模态大语言模型（MLLM）为MSA生成丰富解释信息；2）设计时序感知的神经网络模块，将音频与视频特征进行对齐；3）以文本为路由，融合稀疏混合专家结构与门控机制，有效整合多模态信息。

Result: 在四个数据集上开展实验，TEXT优于包括三种近期模型和三款MLLM在内的所有对比方法。在全部六项指标中，至少在四项上取得最佳。例如，在CH-SIMS数据集上，绝对平均误差减少至0.353，相较最近提出的方法下降了13.5%。

Conclusion: TEXT模型证明了解释增强与时序对齐结合对提升多模态情感分析性能十分有效。这为后续研究提供了新的思路，即通过增强模型解释力和改进多模态时域融合进一步提升情感分析效果。

Abstract: Human-interaction-involved applications underscore the need for Multi-modal Sentiment Analysis (MSA). Although many approaches have been proposed to address the subtle emotions in different modalities, the power of explanations and temporal alignments is still underexplored. Thus, this paper proposes the Text-routed sparse mixture-of-Experts model with eXplanation and Temporal alignment for MSA (TEXT). TEXT first augments explanations for MSA via Multi-modal Large Language Models (MLLM), and then novelly aligns the epresentations of audio and video through a temporality-oriented neural network block. TEXT aligns different modalities with explanations and facilitates a new text-routed sparse mixture-of-experts with gate fusion. Our temporal alignment block merges the benefits of Mamba and temporal cross-attention. As a result, TEXT achieves the best performance cross four datasets among all tested models, including three recently proposed approaches and three MLLMs. TEXT wins on at least four metrics out of all six metrics. For example, TEXT decreases the mean absolute error to 0.353 on the CH-SIMS dataset, which signifies a 13.5% decrement compared with recently proposed approaches.

</details>


### [206] [Fake News Classification in Urdu: A Domain Adaptation Approach for a Low-Resource Language](https://arxiv.org/abs/2512.22778)
*Muhammad Zain Ali,Bernhard Pfahringer,Tony Smith*

Main category: cs.CL

TL;DR: 本文针对乌尔都语等低资源语言中的虚假信息检测，探索了利用领域自适应预训练提升多语言模型（如XLM-R和mBERT）分类性能的方法。实验结果显示，领域自适应后的XLM-R效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体中的虚假信息广泛存在，且低资源语言如乌尔都语在这方面的研究有限。多语言模型在该语言上的适应性不佳，尤其是面对领域专有词汇。

Method: 采用分阶段训练策略：先用乌尔都语新闻语料对多语言预训练模型（XLM-R、mBERT）进行领域自适应预训练，再在四个公开乌尔都语虚假新闻数据集上进行下游微调和测试。

Result: 领域自适应预训练后的XLM-R在所有数据集上的表现均优于其未适应版本，而mBERT的改进则表现不一。

Conclusion: 领域自适应预训练能有效提升多语言模型在乌尔都语虚假信息检测上的表现，尤其对XLM-R模型更为有效。

Abstract: Misinformation on social media is a widely acknowledged issue, and researchers worldwide are actively engaged in its detection. However, low-resource languages such as Urdu have received limited attention in this domain. An obvious approach is to utilize a multilingual pretrained language model and fine-tune it for a downstream classification task, such as misinformation detection. However, these models struggle with domain-specific terms, leading to suboptimal performance. To address this, we investigate the effectiveness of domain adaptation before fine-tuning for fake news classification in Urdu, employing a staged training approach to optimize model generalization. We evaluate two widely used multilingual models, XLM-RoBERTa and mBERT, and apply domain-adaptive pretraining using a publicly available Urdu news corpus. Experiments on four publicly available Urdu fake news datasets show that domain-adapted XLM-R consistently outperforms its vanilla counterpart, while domain-adapted mBERT exhibits mixed results.

</details>


### [207] [CNSight: Evaluation of Clinical Note Segmentation Tools](https://arxiv.org/abs/2512.22795)
*Risha Surana,Adrian Law,Sunwoo Kim,Rishab Sridhar,Angxiao Han,Peiyu Hong*

Main category: cs.CL

TL;DR: 本研究比较了不同方法对电子病历中临床笔记分段任务的表现，发现大型API模型效果最好。


<details>
  <summary>Details</summary>
Motivation: 临床笔记通常以非结构化格式存储，影响后续分析和应用，准确识别各部分边界是结构化处理的关键。

Method: 采用MIMIC-IV数据库1,000份临床笔记，对比了基于规则的方法、领域特定的Transformer模型和大型语言模型（如GPT-5-mini），分别评估它们在句级和自由文本分段上的效果。

Result: 大型API模型如GPT-5-mini在所有任务上表现最佳，平均F1分数达到72.4。轻量级基线在结构化句级任务中有竞争力，但在自由文本分段时表现较差。

Conclusion: 大型语言模型对于临床笔记分段任务效果突出，可为信息抽取、队列筛选和自动摘要等下游任务奠定基础，且研究为方法选择提供了有益参考。

Abstract: Clinical notes are often stored in unstructured or semi-structured formats after extraction from electronic medical record (EMR) systems, which complicates their use for secondary analysis and downstream clinical applications. Reliable identification of section boundaries is a key step toward structuring these notes, as sections such as history of present illness, medications, and discharge instructions each provide distinct clinical contexts. In this work, we evaluate rule-based baselines, domain-specific transformer models, and large language models for clinical note segmentation using a curated dataset of 1,000 notes from MIMIC-IV. Our experiments show that large API-based models achieve the best overall performance, with GPT-5-mini reaching a best average F1 of 72.4 across sentence-level and freetext segmentation. Lightweight baselines remain competitive on structured sentence-level tasks but falter on unstructured freetext. Our results provide guidance for method selection and lay the groundwork for downstream tasks such as information extraction, cohort identification, and automated summarization.

</details>


### [208] [NepEMO: A Multi-Label Emotion and Sentiment Analysis on Nepali Reddit with Linguistic Insights and Temporal Trends](https://arxiv.org/abs/2512.22823)
*Sameer Sitoula,Tej Bahadur Shahi,Laxmi Prasad Bhatt,Anisha Pokhrel,Arjun Neupane*

Main category: cs.CL

TL;DR: 本文介绍了NepEMO数据集，用于尼泊尔Reddit贴文的多标签情感与情绪分类，并对不同机器学习、深度学习和Transformer模型的表现进行了比较，结果表明Transformer模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体成为人们表达意见和情感的重要渠道，尤其是在尼泊尔语境下，对多标签情绪与情感数据集的需求日益增长。而目前针对尼泊尔网络语料的多语种（英文、罗马化尼泊尔语、天城文）数据集极其稀缺，影响了相关自然语言处理任务的开展。

Method: 作者手工标注和收集了2019年1月-2025年6月间的4,462条尼泊尔Reddit帖子，涵盖5类情绪（恐惧、愤怒、悲伤、快乐、抑郁）和3类情感（正向、负向、中性），分别用英文、罗马化尼泊尔语和天城文书写。利用情绪趋势分析、情感n-gram、LDA主题建模和TF-IDF关键词提取深入分析了数据。同时比较了传统机器学习、深度学习及Transformer模型在多标签情绪（MLE）和情感分类（SC）上的表现。

Result: Transformer模型在多标签情绪分类和情感分类任务上均优于传统机器学习及深度学习模型。

Conclusion: NepEMO作为尼泊尔相关社交数据集，能够有效促进多语言多标签情感与情绪分类任务的研究，并验证了Transformer模型在此类多语言、噪声较多的数据中的优越表现。

Abstract: Social media (SM) platforms (e.g. Facebook, Twitter, and Reddit) are increasingly leveraged to share opinions and emotions, specifically during challenging events, such as natural disasters, pandemics, and political elections, and joyful occasions like festivals and celebrations. Among the SM platforms, Reddit provides a unique space for its users to anonymously express their experiences and thoughts on sensitive issues such as health and daily life. In this work, we present a novel dataset, called NepEMO, for multi-label emotion (MLE) and sentiment classification (SC) on the Nepali subreddit post. We curate and build a manually annotated dataset of 4,462 posts (January 2019- June 2025) written in English, Romanised Nepali and Devanagari script for five emotions (fear, anger, sadness, joy, and depression) and three sentiment classes (positive, negative, and neutral). We perform a detailed analysis of posts to capture linguistic insights, including emotion trends, co-occurrence of emotions, sentiment-specific n-grams, and topic modelling using Latent Dirichlet Allocation and TF-IDF keyword extraction. Finally, we compare various traditional machine learning (ML), deep learning (DL), and transformer models for MLE and SC tasks. The result shows that transformer models consistently outperform the ML and DL models for both tasks.

</details>


### [209] [AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning](https://arxiv.org/abs/2512.22857)
*Shihao Cai,Runnan Fang,Jialong Wu,Baixuan Li,Xinyu Wang,Yong Jiang,Liangcai Su,Liwen Zhang,Wenbiao Yin,Zhen Zhang,Fuli Feng,Pengjun Xie,Xiaobin Wang*

Main category: cs.CL

TL;DR: 论文提出了一种自动且可扩展的高难度任务仿真环境生成管道，以及一种环境级强化学习算法，以解决现有语言智能体仿真环境合成难、用户不稳定、泛化性差等问题。综合实验显示新方法在多个基准数据集上表现优异，泛化能力突出。


<details>
  <summary>Details</summary>
Motivation: 现有仿真环境多为半自动合成或任务难度过低，难以为语言智能体强化学习提供足够挑战与泛化能力。此外，仿真用户不稳定和环境异质性带来的训练效率低、稳定性差等问题亟需解决。

Method: 1）提出统一的自动高难度仿真环境生成流程，保证任务既有挑战性又易验证。2）设计了环境级强化学习算法，专注于环境层面的用户不稳定性缓解，并在该层面进行优势估计，提高训练质量和效率。

Result: 在多个智能体基准（如tau-bench, tau2-Bench, VitaBench）上，本文方法均表现出优异的效果。进一步分析表明，该方法具备较强的域外泛化能力。

Conclusion: 本文提出的方法在提升智能体训练效率、稳定性和泛化性方面效果显著，有望推动相关领域RL智能体的进一步发展。

Abstract: Conducting reinforcement learning (RL) in simulated environments offers a cost-effective and highly scalable way to enhance language-based agents. However, previous work has been limited to semi-automated environment synthesis or tasks lacking sufficient difficulty, offering little breadth or depth. In addition, the instability of simulated users integrated into these environments, along with the heterogeneity across simulated environments, poses further challenges for agentic RL. In this work, we propose: (1) a unified pipeline for automated and scalable synthesis of simulated environments associated with high-difficulty but easily verifiable tasks; and (2) an environment level RL algorithm that not only effectively mitigates user instability but also performs advantage estimation at the environment level, thereby improving training efficiency and stability. Comprehensive evaluations on agentic benchmarks, including tau-bench, tau2-Bench, and VitaBench, validate the effectiveness of our proposed method. Further in-depth analyses underscore its out-of-domain generalization.

</details>


### [210] [Diversity or Precision? A Deep Dive into Next Token Prediction](https://arxiv.org/abs/2512.22955)
*Haoyuan Wu,Hai Wang,Jiajia Wu,Jinxiang Ou,Keyao Wang,Weile Chen,Zihao Zheng,Bei Yu*

Main category: cs.CL

TL;DR: 本文提出了一种结合监督学习与强化学习原理的泛化预训练目标，通过对预训练分布进行奖励塑造，提升大语言模型后续用RL训练时的推理能力。实验发现，更注重精确性的分布优于高熵分布，有利于RL中的有效探索。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习可以提升大语言模型的推理能力，但其效果很大程度上取决于模型预训练时形成的token分布。本研究旨在探究如何优化预训练分布，为后续RL训练提供更好的探索空间，从而进一步提升模型性能。

Method: 作者将传统的交叉熵损失重新解读为单步episode中的policy gradient优化，并提出基于on-policy RL原则的预训练目标。通过将下一个token预测建模为随机决策过程，设计算法平衡分布的多样性和精确性，采用奖励缩放和基于排序的负样本处理方法，有针对性地调整token分布。

Result: 实验表明，采用精度优先的奖励塑造策略，获得的预训练分布比高熵（多样性高）的分布更能促进RL训练阶段中的探索，最终提升了大模型推理能力。

Conclusion: 与常规认知不同，高熵分布未必有利于RL训练。提倡更偏重精确性、而非多样性的预训练分布，有助于后续大模型结合RL时实现更优推理表现。

Abstract: Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.

</details>


### [211] [Prompt engineering does not universally improve Large Language Model performance across clinical decision-making tasks](https://arxiv.org/abs/2512.22966)
*Mengdi Chai,Ali R. Zomorrodi*

Main category: cs.CL

TL;DR: 本研究评估了三种最先进的大语言模型（ChatGPT-4o、Gemini 1.5 Pro 和 LIama 3.3 70B）在临床决策支持中的表现。结果显示模型在不同任务间表现波动显著，且“提示工程”对不同任务和模型的效果差异极大。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM已显示出较强的医学知识表现，但其在真实临床决策中的实际效用鲜有系统探索，因此需要综合评估其在模拟患者就诊流程中多阶段临床推理任务的能力及“提示工程”策略效果。

Method: 采用36个临床案例，评估三种LLM在差异性诊断、紧急处理、相关检查、最终诊断、治疗建议五项任务下的表现，并设置不同temperature（默认与零）和多种MedPrompt提示工程（包括定向和随机少样本学习），比较各模型和策略的表现差异。

Result: 各模型在不同任务和temperature下表现差异大：最终诊断接近满分、相关检查表现最差、其他任务为中等；ChatGPT在zero temperature下效果更佳，LIama在默认temperature下更优。提示工程仅在提升相关检查表现显著，对其他任务反而有害。定向少样本提示并未稳定优于随机。

Conclusion: 提示工程对LLM在临床推理各环节的帮助高度依赖具体任务与模型，不应一概而论。推动LLM在医疗中的集成需定制、情境感知的优化策略。

Abstract: Large Language Models (LLMs) have demonstrated promise in medical knowledge assessments, yet their practical utility in real-world clinical decision-making remains underexplored. In this study, we evaluated the performance of three state-of-the-art LLMs-ChatGPT-4o, Gemini 1.5 Pro, and LIama 3.3 70B-in clinical decision support across the entire clinical reasoning workflow of a typical patient encounter. Using 36 case studies, we first assessed LLM's out-of-the-box performance across five key sequential clinical decision-making tasks under two temperature settings (default vs. zero): differential diagnosis, essential immediate steps, relevant diagnostic testing, final diagnosis, and treatment recommendation. All models showed high variability by task, achieving near-perfect accuracy in final diagnosis, poor performance in relevant diagnostic testing, and moderate performance in remaining tasks. Furthermore, ChatGPT performed better under the zero temperature, whereas LIama showed stronger performance under the default temperature. Next, we assessed whether prompt engineering could enhance LLM performance by applying variations of the MedPrompt framework, incorporating targeted and random dynamic few-shot learning. The results demonstrate that prompt engineering is not a one-size-fit-all solution. While it significantly improved the performance on the task with lowest baseline accuracy (relevant diagnostic testing), it was counterproductive for others. Another key finding was that the targeted dynamic few-shot prompting did not consistently outperform random selection, indicating that the presumed benefits of closely matched examples may be counterbalanced by loss of broader contextual diversity. These findings suggest that the impact of prompt engineering is highly model and task-dependent, highlighting the need for tailored, context-aware strategies for integrating LLMs into healthcare.

</details>


### [212] [Improving Generalization in LLM Structured Pruning via Function-Aware Neuron Grouping](https://arxiv.org/abs/2512.23014)
*Tao Yu,Yongqi An,Kuan Zhu,Guibo Zhu,Ming Tang,Jinqiao Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为FANG（Function-Aware Neuron Grouping）的结构化剪枝框架，能提升大语言模型在剪枝后的下游任务表现，取得了当前最佳成果。


<details>
  <summary>Details</summary>
Motivation: 虽然结构化剪枝能有效减小大语言模型的计算与存储成本，但现有剪枝方法在校准集无法覆盖预训练数据分布时，泛化能力较差，导致下游任务表现不佳。

Method: 作者提出FANG框架，将具有相似功能语义的神经元进行分组，并独立剪枝。关键神经元及分布于多类上下文的神经元得以保留。在每组中，和神经元功能高度相关的token会被赋予更高权重。此外，FANG还根据各block的功能复杂度自适应分配稀疏率。

Result: FANG使剪枝后大模型在下游任务的准确率显著提升，在30%和40%稀疏率下平均准确率优于FLAP与OBC 1.5%-8.5%。

Conclusion: FANG有效缓解了校准集偏差问题，实现了下游任务精度和语言建模性能的双重提升，并可与现有主流方法结合取得最佳效果。

Abstract: Large Language Models (LLMs) demonstrate impressive performance across natural language tasks but incur substantial computational and storage costs due to their scale. Post-training structured pruning offers an efficient solution. However, when few-shot calibration sets fail to adequately reflect the pretraining data distribution, existing methods exhibit limited generalization to downstream tasks. To address this issue, we propose Function-Aware Neuron Grouping (FANG), a post-training pruning framework that alleviates calibration bias by identifying and preserving neurons critical to specific function. FANG groups neurons with similar function based on the type of semantic context they process and prunes each group independently. During importance estimation within each group, tokens that strongly correlate with the functional role of the neuron group are given higher weighting. Additionally, FANG also preserves neurons that contribute across multiple context types. To achieve a better trade-off between sparsity and performance, it allocates sparsity to each block adaptively based on its functional complexity. Experiments show that FANG improves downstream accuracy while preserving language modeling performance. It achieves the state-of-the-art (SOTA) results when combined with FLAP and OBC, two representative pruning methods. Specifically, FANG outperforms FLAP and OBC by 1.5%--8.5% in average accuracy under 30% and 40% sparsity.

</details>


### [213] [LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models](https://arxiv.org/abs/2512.23025)
*Wenxuan Xu,Arvind Pillai,Subigya Nepal,Amanda C Collins,Daniel M Mackin,Michael V Heinz,Tess Z Griffin,Nicholas C Jacobson,Andrew Campbell*

Main category: cs.CL

TL;DR: LENS提出了一种将多模态健康传感数据转化为自然语言叙述的方法，能更好地用LLM分析心理健康。


<details>
  <summary>Details</summary>
Motivation: 目前LLM无法直接处理长时传感器数据，且缺少配对的传感器-文本数据，限制了健康行为信号向自然语言的转换和临床应用。

Method: LENS框架通过将EMA现场评估转换为自然语言，建立了大规模传感器-文本问答对数据集；并训练了一个patch-level编码器，将原始传感器信号映射到LLM表示空间，实现原生时间序列接入LLM。

Result: LENS在NLP指标和症状严重度准确性等任务上都优于现有强基线。同时，对13位心理健康专业人士的用户调研表明，其生成的叙述具有完整性和临床意义。

Conclusion: LENS推动了LLM作为健康行为信号接口的应用，为模型直接推理原始行为信号并辅助临床决策提供了可扩展路径。

Abstract: Multimodal health sensing offers rich behavioral signals for assessing mental health, yet translating these numerical time-series measurements into natural language remains challenging. Current LLMs cannot natively ingest long-duration sensor streams, and paired sensor-text datasets are scarce. To address these challenges, we introduce LENS, a framework that aligns multimodal sensing data with language models to generate clinically grounded mental-health narratives. LENS first constructs a large-scale dataset by transforming Ecological Momentary Assessment (EMA) responses related to depression and anxiety symptoms into natural-language descriptions, yielding over 100,000 sensor-text QA pairs from 258 participants. To enable native time-series integration, we train a patch-level encoder that projects raw sensor signals directly into an LLM's representation space. Our results show that LENS outperforms strong baselines on standard NLP metrics and task-specific measures of symptom-severity accuracy. A user study with 13 mental-health professionals further indicates that LENS-produced narratives are comprehensive and clinically meaningful. Ultimately, our approach advances LLMs as interfaces for health sensing, providing a scalable path toward models that can reason over raw behavioral signals and support downstream clinical decision-making.

</details>


### [214] [Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization](https://arxiv.org/abs/2512.23032)
*Kerem Zaman,Shashank Srivastava*

Main category: cs.CL

TL;DR: 论文批评了现有的Biasing Features度量标准，认为它将推理链（CoT）中的“低保真”混淆成了“不完整”，并提出新的度量和分析方法以更准确评估模型推理的真实性。


<details>
  <summary>Details</summary>
Motivation: 当前使用Biasing Features度量方法判断模型输出的推理链（CoT）是否忠实于输入线索，但该方法可能将‘压缩’导致的省略误判为‘不忠实’，这影响了推理可解释性和公正性评估。

Method: 作者在多跳推理任务中，使用Llama-3和Gemma-3模型，通过对比不同忠实性评估标准、引入新的faithful@k指标、在不同token预算下实验，并利用因果中介分析，系统性地分析了现有度量方法的局限。

Result: 实验发现，Biasing Features标记为“不忠实”的CoT中，有相当一部分（部分模型超50%）被其他尺度认为是“忠实”的。在token预算较大时，CoT对线索的显式表达大幅提升（部分设置下超90%），但即使未直接表达，线索也可通过推理链介导影响预测。

Conclusion: 作者建议不要依赖单一的基于提示的评估方法，需引入更全面的解释性工具箱（含因果中介分析和干扰式指标）才能更准确地评估大模型推理链的忠实性。

Abstract: Recent work, using the Biasing Features metric, labels a CoT as unfaithful if it omits a prompt-injected hint that affected the prediction. We argue this metric confuses unfaithfulness with incompleteness, the lossy compression needed to turn distributed transformer computation into a linear natural language narrative. On multi-hop reasoning tasks with Llama-3 and Gemma-3, many CoTs flagged as unfaithful by Biasing Features are judged faithful by other metrics, exceeding 50% in some models. With a new faithful@k metric, we show that larger inference-time token budgets greatly increase hint verbalization (up to 90% in some settings), suggesting much apparent unfaithfulness is due to tight token limits. Using Causal Mediation Analysis, we further show that even non-verbalized hints can causally mediate prediction changes through the CoT. We therefore caution against relying solely on hint-based evaluations and advocate a broader interpretability toolkit, including causal mediation and corruption-based metrics.

</details>


### [215] [Accelerating Language Model Workflows with Prompt Choreography](https://arxiv.org/abs/2512.23049)
*TJ Bai,Jason Eisner*

Main category: cs.CL

TL;DR: 本文提出了一种名为 Prompt Choreography 的新框架，通过全局动态 KV cache 优化多智能体 LLM 工作流，提高并行性和效率，并通过微调保证输出质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在多Agent场景下使用频率增加，但因重复计算、上下文处理等问题导致效率低下和资源消耗大。需要一种高效管理和复用历史信息的方法以提升整体效能。

Method: 引入 Prompt Choreography 框架，采用全局动态 KV 缓存，每次 LLM 调用可以访问任意排列组合的历史消息编码，并支持并行调用。同时，通过微调LLM以适应缓存方式，减少与重新编码的差异。

Result: 实验显示，Prompt Choreography 能显著降低每条消息的延迟（首token平均快2.0--6.2倍），在冗余计算多的工作流中整体提速超过2.2倍。

Conclusion: Prompt Choreography 在保持结果准确性的同时，大幅提升了多Agent LLM工作流的效率，是提升LLM应用效率的有效方案。

Abstract: Large language models are increasingly deployed in multi-agent workflows. We introduce Prompt Choreography, a framework that efficiently executes LLM workflows by maintaining a dynamic, global KV cache. Each LLM call can attend to an arbitrary, reordered subset of previously encoded messages. Parallel calls are supported. Though caching messages' encodings sometimes gives different results from re-encoding them in a new context, we show in diverse settings that fine-tuning the LLM to work with the cache can help it mimic the original results. Prompt Choreography significantly reduces per-message latency (2.0--6.2$\times$ faster time-to-first-token) and achieves substantial end-to-end speedups ($>$2.2$\times$) in some workflows dominated by redundant computation.

</details>


### [216] [TabiBERT: A Large-Scale ModernBERT Foundation Model and Unified Benchmarking Framework for Turkish](https://arxiv.org/abs/2512.23065)
*Melikşah Türker,A. Ebrar Kızıloğlu,Onur Güngör,Susan Üsküdarlı*

Main category: cs.CL

TL;DR: 本文提出了TabiBERT，这是首个从零训练、基于ModernBERT架构的土耳其语单语编码器模型，在大规模多领域语料上预训练，显著提升了性能并公开了全部资源。


<details>
  <summary>Details</summary>
Motivation: 尽管编码器Transformer在结构上已取得诸多进展（如RoPE、FlashAttention等），但土耳其自然语言处理领域缺乏从零训练、利用这些先进结构的单语编码模型。

Method: 作者提出TabiBERT，基于ModernBERT架构，从零开始用大规模土耳其语多领域语料库（总计一万亿tokens，含网页、科学文献、代码和数学内容）进行预训练，支持8192 tokens上下文长度（为BERT的16倍）。并集成了高效率的FlashAttention、RoPE位置编码和改进的归一化技术。模型训练及评测用TabiBench，涵盖8类任务共28个数据集。

Result: TabiBERT在推理速度提升到2.65倍，显著降低显存占用，可用更大batch size。在TabiBench评测中达77.58分，比BERTurk高1.62分，并在8类任务中5类创造新SOTA，尤其在问答、代码与文档检索等指标有明显提升。同时相较于以往任务专向模型也维持全面领先。

Conclusion: TabiBERT不仅填补了土耳其NLP领域先进单语编码模型的空白，且在多任务评测、人效能与效率等方面优于现有方法。作者还开放了所有资源，为后续研究和实际应用提供了基础。

Abstract: Since the inception of BERT, encoder-only Transformers have evolved significantly in computational efficiency, training stability, and long-context modeling. ModernBERT consolidates these advances by integrating Rotary Positional Embeddings (RoPE), FlashAttention, and refined normalization. Despite these developments, Turkish NLP lacks a monolingual encoder trained from scratch incorporating such modern architectural paradigms. This work introduces TabiBERT, a monolingual Turkish encoder based on ModernBERT architecture trained from scratch on a large, curated corpus. TabiBERT is pre-trained on one trillion tokens sampled from an 84.88B token multi-domain corpus: web text (73%), scientific publications (20%), source code (6%), and mathematical content (0.3%). The model supports 8,192-token context length (16x original BERT), achieves up to 2.65x inference speedup, and reduces GPU memory consumption, enabling larger batch sizes. We introduce TabiBench with 28 datasets across eight task categories with standardized splits and protocols, evaluated using GLUE-style macro-averaging. TabiBERT attains 77.58 on TabiBench, outperforming BERTurk by 1.62 points and establishing state-of-the-art on five of eight categories: question answering (+9.55), code retrieval (+2.41), and document retrieval (+0.60). Compared with task-specific prior best results, including specialized models like TurkishBERTweet, TabiBERT achieves +1.47 average improvement, indicating robust cross-domain generalization. We release model weights, training configurations, and evaluation code for transparent, reproducible Turkish encoder research.

</details>


### [217] [Reservoir Computing inspired Matrix Multiplication-free Language Model](https://arxiv.org/abs/2512.23145)
*Takumi Shiratsuchi,Yuichiro Tanaka,Hakaru Tamukoh*

Main category: cs.CL

TL;DR: 该论文提出一种无需矩阵乘法（MatMul-free）的高效语言模型，通过借鉴储层计算思想进一步降低训练成本，并通过部分参数固化和共享、插入储层层等机制减少参数和训练时间，表现接近现有大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然性能优越，但其高昂的计算和训练成本成为规模化应用的瓶颈。因此，研究更高效、低成本的语言模型结构成为重要需求。

Method: 作者提出了一种新的MatMul-free LM结构：部分层权重固定和共享，在模型中插入储层层（Reservoir Layer），利用储层计算实现丰富动态特征又不增加训练开销，同时结合多种操作以减小内存访问和总体计算负担。

Result: 该方法带来了参数量最多减少19%、训练时间减少9.9%、推理时间减少8%的收益，并在主要NLP任务表现上与基线模型接近。

Conclusion: 通过结合储层计算与权重共享/固化，MatMul-free LM在显著降低资源消耗的同时还能保持较好性能，为大模型的高效化训练推理提供了新思路。

Abstract: Large language models (LLMs) have achieved state-of-the-art performance in natural language processing; however, their high computational cost remains a major bottleneck. In this study, we target computational efficiency by focusing on a matrix multiplication free language model (MatMul-free LM) and further reducing the training cost through an architecture inspired by reservoir computing. Specifically, we partially fix and share the weights of selected layers in the MatMul-free LM and insert reservoir layers to obtain rich dynamic representations without additional training overhead. Additionally, several operations are combined to reduce memory accesses. Experimental results show that the proposed architecture reduces the number of parameters by up to 19%, training time by 9.9%, and inference time by 8.0%, while maintaining comparable performance to the baseline model.

</details>


### [218] [Not too long do read: Evaluating LLM-generated extreme scientific summaries](https://arxiv.org/abs/2512.23206)
*Zhuoqi Lyu,Qing Ke*

Main category: cs.CL

TL;DR: 本文提出了一个大规模高质量的科研极致摘要（TLDR）数据集BiomedTLDR，并分析了大语言模型（LLMs）在生成科研TLDR方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs能否有效生成科学TLDR尚不清楚，而且缺乏高质量TLDR数据集限制了模型开发与评估。作者发现科研领域广泛存在作者评论，可以形成高质量的科研TLDR数据集。

Method: 作者收集整理了作者撰写的科学论文极致摘要，构建BiomedTLDR数据集，并用公开权重LLMs对比自动和专家生成的TLDR摘要，从词汇选择和修辞结构等方面进行分析。

Result: LLMs可生成“类人”的摘要，但整体上比人类专家更倾向于原文措辞与结构，即表现为更“抽取式”而非“生成式”摘要。

Conclusion: BiomedTLDR为LLMs科研摘要能力的开发与评估提供重要资源。LLMs在生成科学TLDR时展现出局限性，未来需进一步提升其“生成式”能力。

Abstract: High-quality scientific extreme summary (TLDR) facilitates effective science communication. How do large language models (LLMs) perform in generating them? How are LLM-generated summaries different from those written by human experts? However, the lack of a comprehensive, high-quality scientific TLDR dataset hinders both the development and evaluation of LLMs' summarization ability. To address these, we propose a novel dataset, BiomedTLDR, containing a large sample of researcher-authored summaries from scientific papers, which leverages the common practice of including authors' comments alongside bibliography items. We then test popular open-weight LLMs for generating TLDRs based on abstracts. Our analysis reveals that, although some of them successfully produce humanoid summaries, LLMs generally exhibit a greater affinity for the original text's lexical choices and rhetorical structures, hence tend to be more extractive rather than abstractive in general, compared to humans. Our code and datasets are available at https://github.com/netknowledge/LLM_summarization (Lyu and Ke, 2025).

</details>


### [219] [Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process](https://arxiv.org/abs/2512.23213)
*Zhijun Chen,Zeyu Ji,Qianren Mao,Junhang Cheng,Bangjie Qin,Hao Wu,Zhuoran Li,Jingzheng Li,Kai Sun,Zizhe Wang,Yikun Ban,Zhu Sun,Xiangyang Ji,Hailong Sun*

Main category: cs.CL

TL;DR: 本论文提出了一种无监督的大模型集成方法LLM-PeerReview，通过集成多个LLM候选答案并结合同行评议灵感，有效选出最佳响应，实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前大模型（LLM）常用于多种任务，不同模型各有优劣，且单一模型输出受限。通过集成多模型输出，能更好发挥模型互补性，但如何无监督地从多个响应中选出最优结果是难点。

Method: 提出LLM-PeerReview框架，分三步进行：1）利用LLM-as-a-Judge技术，让多个LLM对所有响应评分；2）采用图模型推断算法或简单的均值策略，对诸多评分聚合，得到每个响应的最终分数；3）选分数最高的响应作为最终输出。全流程不依赖人工标签，无监督、可解释且易于适配。

Result: 在四个数据集上测评，所提方法有两个变体，在准确率上均超过了近期先进模型Smoothie-Global，分别高出6.9%和7.3%。

Conclusion: LLM-PeerReview框架在无监督条件下实现了高效、解释性强的模型集成，有很好的适应性和泛化能力，在多个数据集上取得领先效果，证实了该方法的有效性。

Abstract: We propose LLM-PeerReview, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths. LLM-PeerReview is built on a novel, peer-review-inspired framework that offers a clear and interpretable mechanism, while remaining fully unsupervised for flexible adaptability and generalization. Specifically, it operates in three stages: For scoring, we use the emerging LLM-as-a-Judge technique to evaluate each response by reusing multiple LLMs at hand; For reasoning, we can apply a principled graphical model-based truth inference algorithm or a straightforward averaging strategy to aggregate multiple scores to produce a final score for each response; Finally, the highest-scoring response is selected as the best ensemble output. LLM-PeerReview is conceptually simple and empirically powerful. The two variants of the proposed approach obtain strong results across four datasets, including outperforming the recent advanced model Smoothie-Global by 6.9% and 7.3% points, respectively.

</details>


### [220] [Anka: A Domain-Specific Language for Reliable LLM Code Generation](https://arxiv.org/abs/2512.23214)
*Saif Khalfan Saif Al Mazrouei*

Main category: cs.CL

TL;DR: 本论文提出并验证了一种为大模型设计的领域特定语言（DSL）Anka，在数据转换多步骤任务中显著优于Python，并极大提高了大模型代码生成的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在复杂多步骤编程任务中常因通用语言（如Python）灵活性过高、状态管理隐性而导致系统性错误。论文提出使用约束、明确语法减少歧义，提升代码生成精度。

Method: 作者设计并提出了Anka，一种面向数据转换流水线的DSL，具备高度约束和显式状态管理特性。实验用Claude 3.5 Haiku和GPT-4o-mini等模型测试，Zero-shot生成Anka代码，测解析与任务准确率，并与Python对比。

Result: Claude 3.5 Haiku在未经训练情况下，Anka的解析率为99.9%，任务准确率达95.8%，多步骤任务中Anka准确率100%，比Python高出40个百分点（Python仅60%）。GPT-4o-mini也验证Anka优于Python（提升26.7个百分点）。

Conclusion: 1）LLM完全可通过上下文提示学习新DSL，效果近似原生；2）受限语法能大幅减少复杂任务出错；3）专为生成优化的DSL比LLM高频训练的通用语言（如Python）更能提升复杂任务的代码生成质量。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research.

</details>


### [221] [Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation](https://arxiv.org/abs/2512.23260)
*Dianyun Wang,Qingsen Ma,Yuhu Shang,Zhifeng Lu,Lechen Ning,Zhenbo Xu,Huijia Wu,Zhaofeng He*

Main category: cs.CL

TL;DR: 本文提出一种基于稀疏自编码器（SAE）帮助下的低秩适配新方法，提高了大语言模型参数高效微调时的性能与可解释性。方法在安全对齐任务上取得了显著效果，仅微调极少参数便达到了高于全参数方法的对齐率。


<details>
  <summary>Details</summary>
Motivation: 现有低秩适配（如LoRA）认为权重更新落在低秩子空间，但该子空间直接从数据中黑盒学习，缺乏可解释性和直接控制。作者认为问题在于模型内部语义纠缠，不同维度混杂多个概念，从而加大了子空间学习难度并降低了可解释性。

Method: 作者利用预训练稀疏自编码器（SAE）在“解纠缠”特征空间中识别任务相关特征，并构建显式、可解释的低秩子空间引导适配器初始化。理论上证明：若特征“单语义”，则SAE子空间可达任意小的恢复误差，而原始纠缠空间存在误差下界。

Result: 在安全对齐任务上，该方法最高安全率达99.6%，比全参数微调高7.4%；参数开销仅为0.19%~0.24%，接近RLHF效果。同时，能通过SAE特征的语义 grounding 实现对微调子空间的解释。

Conclusion: 引入机制可解释性不仅能提升模型微调效果，也增强了微调过程的透明性。该方法兼具高性能与强可解释性，对未来权重高效微调方法设计具有重要启示。

Abstract: Parameter-efficient fine-tuning has become the dominant paradigm for adapting large language models to downstream tasks. Low-rank adaptation methods such as LoRA operate under the assumption that task-relevant weight updates reside in a low-rank subspace, yet this subspace is learned implicitly from data in a black-box manner, offering no interpretability or direct control. We hypothesize that this difficulty stems from polysemanticity--individual dimensions encoding multiple entangled concepts. To address this, we leverage pre-trained Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled feature space, then construct an explicit, interpretable low-rank subspace to guide adapter initialization. We provide theoretical analysis proving that under monosemanticity assumptions, SAE-based subspace identification achieves arbitrarily small recovery error, while direct identification in polysemantic space suffers an irreducible error floor. On safety alignment, our method achieves up to 99.6% safety rate--exceeding full fine-tuning by 7.4 percentage points and approaching RLHF-based methods--while updating only 0.19-0.24% of parameters. Crucially, our method provides interpretable insights into the learned alignment subspace through the semantic grounding of SAE features. Our work demonstrates that incorporating mechanistic interpretability into the fine-tuning process can simultaneously improve both performance and transparency.

</details>


### [222] [Chinese Morph Resolution in E-commerce Live Streaming Scenarios](https://arxiv.org/abs/2512.23280)
*Jiahao Zhu,Jipeng Qiang,Ran Bai,Chenyu Liu,Xiaoye Ouyang*

Main category: cs.CL

TL;DR: 本文介绍了一项新任务Live Auditory Morph Resolution (LiveAMR)，用于检测中国电商直播（如抖音）中主播通过发音变形逃避监管、虚假宣传行为，并构建了首个相关数据集和方法。


<details>
  <summary>Details</summary>
Motivation: 在中国电商直播平台，许多主播通过发音变形（morphs）逃避监管、进行虚假宣传，尤其在健康医疗相关直播中问题严重，亟需有效检测手段。

Method: 作者提出LiveAMR任务，将检测主播发音变形的问题转化为文本生成任务。构建了包含8.7万个样本的数据集，并利用大语言模型生成额外训练数据提升检测表现。

Result: 通过实验，作者证明所提方法能够有效识别发音变形行为，并且生成数据能进一步提升检测效果。

Conclusion: LiveAMR任务和方法为电商直播监管、虚假宣传检测提供了新的技术手段，可显著加强健康医疗等领域的内容合规监管。

Abstract: E-commerce live streaming in China, particularly on platforms like Douyin, has become a major sales channel, but hosts often use morphs to evade scrutiny and engage in false advertising. This study introduces the Live Auditory Morph Resolution (LiveAMR) task to detect such violations. Unlike previous morph research focused on text-based evasion in social media and underground industries, LiveAMR targets pronunciation-based evasion in health and medical live streams. We constructed the first LiveAMR dataset with 86,790 samples and developed a method to transform the task into a text-to-text generation problem. By leveraging large language models (LLMs) to generate additional training data, we improved performance and demonstrated that morph resolution significantly enhances live streaming regulation.

</details>


### [223] [AI4Reading: Chinese Audiobook Interpretation System Based on Multi-Agent Collaboration](https://arxiv.org/abs/2512.23300)
*Minjiang Huang,Jipeng Qiang,Yi Zhu,Chaowei Zhang,Xiangyu Zhao,Kui Yu*

Main category: cs.CL

TL;DR: 本文提出了AI4Reading系统，利用多智能体协作与大语言模型，自动生成类播客的有声书解读，提升效率与可理解性。实验结果表明解读内容简洁准确，但语音生成质量尚有提升空间。


<details>
  <summary>Details</summary>
Motivation: 有声书解读内容为读者提供实用见解和启发，但手工制作成本高、耗时长，因此亟需自动化生成方法以扩大其可及性。

Method: 提出AI4Reading系统，基于多智能体协作，包括主题分析、案例分析、编辑、旁白和校对等11类专属Agent，结合大语言模型和语音合成技术，实现对书籍主题进行分析、案例挖掘、内容组织和自然语言生成。

Result: 系统生成的解读稿在内容简洁和准确性上超过专家版本，尽管在语音合成质量上仍有提升空间。

Conclusion: AI4Reading系统有效降低了有声书解读的制作门槛，实现较高内容质量和理解性，为该领域的内容生产自动化奠定基础。

Abstract: Audiobook interpretations are attracting increasing attention, as they provide accessible and in-depth analyses of books that offer readers practical insights and intellectual inspiration. However, their manual creation process remains time-consuming and resource-intensive. To address this challenge, we propose AI4Reading, a multi-agent collaboration system leveraging large language models (LLMs) and speech synthesis technology to generate podcast, like audiobook interpretations. The system is designed to meet three key objectives: accurate content preservation, enhanced comprehensibility, and a logical narrative structure. To achieve these goals, we develop a framework composed of 11 specialized agents,including topic analysts, case analysts, editors, a narrator, and proofreaders that work in concert to explore themes, extract real world cases, refine content organization, and synthesize natural spoken language. By comparing expert interpretations with our system's output, the results show that although AI4Reading still has a gap in speech generation quality, the generated interpretative scripts are simpler and more accurate.

</details>


### [224] [AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents](https://arxiv.org/abs/2512.23343)
*Jiafeng Liang,Hao Li,Chang Li,Jiaqi Zhou,Shixin Jiang,Zekun Wang,Changkai Ji,Zhihao Zhu,Runxuan Liu,Tao Ren,Jinlan Fu,See-Kiong Ng,Xia Liang,Ming Liu,Bing Qin*

Main category: cs.CL

TL;DR: 本论文系统性地梳理了人类与AI系统记忆机制的交叉知识，比较了生物学和人工智能记忆的异同，并展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 现有自主智能体在借鉴认知神经科学方面存在隔阂，难以充分吸收人类记忆机制的核心精髓。本研究旨在填补认知神经科学与大模型驱动智能体在记忆机制之间的知识鸿沟。

Method: 本文首先梳理了从认知神经科学到大语言模型（LLM）再到智能体的记忆定义和功能的演变。接着，作者从记忆分类、存储机制和完整生命周期管理等多维度，对生物与人工记忆进行对比分析。同时，系统评述了主流的智能体记忆评测基准，并从攻击和防御的角度探讨了记忆安全性问题。最后，展望了多模态记忆和技能获取等未来研究方向。

Result: 本研究系统整合了跨学科的记忆知识，明确了生物与人工系统记忆机制的异同，总结了现有智能体记忆评测方法，并提出了关于安全性和未来发展趋势的见解。

Conclusion: 本文构建了认知神经科学与AI智能体记忆机制的知识桥梁，为未来智能体的记忆系统设计和研究提供了清晰的理论框架和方向建议。

Abstract: Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.

</details>


### [225] [A Stepwise-Enhanced Reasoning Framework for Large Language Models Based on External Subgraph Generation](https://arxiv.org/abs/2512.23356)
*Xin Zhang,Yang Cao,Baoxing Wu,Xinyi Chen,Kai Song,Siying Li*

Main category: cs.CL

TL;DR: 提出SGR框架，通过外部子图生成辅助大语言模型（LLM）进行逐步推理，提高其深度推理和逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在多种NLP任务中表现优异，但在需要深度推理和逻辑推理的复杂场景中仍存在局限，尤其容易引入噪声或无关信息，导致输出不准确。

Method: 提出了一种基于外部知识库动态生成相关子图、并引导模型依托子图结构进行多步推理的SGR（Stepwise Graph Reasoning）框架，包括：1）基于问题相关性生成外部子图；2）指导LLM以子图为基础、逐步进行推理；3）整合多条推理路径得到最终答案。

Result: 在多个基准数据集上，SGR均优于强基线模型，有效提升了LLM的推理准确率。

Conclusion: SGR能有效提升大语言模型的推理能力，尤其在需要复杂知识整合和多步推理的任务上表现突出，具有实际应用前景。

Abstract: Large Language Models (LLMs) have achieved strong performance across a wide range of natural language processing tasks in recent years, including machine translation, text generation, and question answering. As their applications extend to increasingly complex scenarios, however, LLMs continue to face challenges in tasks that require deep reasoning and logical inference. In particular, models trained on large scale textual corpora may incorporate noisy or irrelevant information during generation, which can lead to incorrect predictions or outputs that are inconsistent with factual knowledge. To address this limitation, we propose a stepwise reasoning enhancement framework for LLMs based on external subgraph generation, termed SGR. The proposed framework dynamically constructs query relevant subgraphs from external knowledge bases and leverages their semantic structure to guide the reasoning process. By performing reasoning in a step by step manner over structured subgraphs, SGR reduces the influence of noisy information and improves reasoning accuracy. Specifically, the framework first generates an external subgraph tailored to the input query, then guides the model to conduct multi step reasoning grounded in the subgraph, and finally integrates multiple reasoning paths to produce the final answer. Experimental results on multiple benchmark datasets demonstrate that SGR consistently outperforms strong baselines, indicating its effectiveness in enhancing the reasoning capabilities of LLMs.

</details>


### [226] [Entropy-Guided Token Dropout: Training Autoregressive Language Models with Limited Domain Data](https://arxiv.org/abs/2512.23422)
*Jiapeng Wang,Yiwen Hu,Yanzipeng Gao,Haoyu Wang,Shuo Wang,Hongyu Lu,Jiaxin Mao,Wayne Xin Zhao,Junyi Li,Xiao Zhang*

Main category: cs.CL

TL;DR: 本文提出了EntroDrop方法，通过根据token熵值对训练数据中低熵token进行选择性dropout，有效缓解了大语言模型（LLM）在多轮训练下过拟合和性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 随着高质量、领域特定数据变得稀缺，多轮训练成为大模型适应新领域的重要手段，但模型在频繁看到同样数据时易出现过拟合，尤其难以泛化高熵token。问题亟需提升有限数据下训练的适应性和稳健性。

Method: 作者提出EntroDrop，一种基于熵引导的token dropout正则化方法。训练中，它选择性地对低熵（可预测性高）token进行mask，同时通过课程式调节训练过程中的dropout强度，从而动态引导正则化。

Result: 在0.6B到8B参数规模的多种LLM上实验证明，EntroDrop优于已有的正则化基线方法，并能在多轮训练中持续保持出色性能。

Conclusion: EntroDrop能有效对抗数据稀缺和过拟合问题，更好地适配有限数据下的LLM训练，为领域微调提供有力工具。

Abstract: As access to high-quality, domain-specific data grows increasingly scarce, multi-epoch training has become a practical strategy for adapting large language models (LLMs). However, autoregressive models often suffer from performance degradation under repeated data exposure, where overfitting leads to a marked decline in model capability. Through empirical analysis, we trace this degradation to an imbalance in learning dynamics: predictable, low-entropy tokens are learned quickly and come to dominate optimization, while the model's ability to generalize on high-entropy tokens deteriorates with continued training. To address this, we introduce EntroDrop, an entropy-guided token dropout method that functions as structured data regularization. EntroDrop selectively masks low-entropy tokens during training and employs a curriculum schedule to adjust regularization strength in alignment with training progress. Experiments across model scales from 0.6B to 8B parameters show that EntroDrop consistently outperforms standard regularization baselines and maintains robust performance throughout extended multi-epoch training. These findings underscore the importance of aligning regularization with token-level learning dynamics when training on limited data. Our approach offers a promising pathway toward more effective adaptation of LLMs in data-constrained domains.

</details>


### [227] [The Effect of Gender Diversity on Scientific Team Impact: A Team Roles Perspective](https://arxiv.org/abs/2512.23429)
*Yi Zhao,Yongjun Zhu,Donghun Kim,Yuzhuo Wang,Heng Zhang,Chao Lu,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 本文研究了科学团队中性别多样性对团队影响力的作用，发现性别多样性与团队影响力呈倒U型关系，不同性别结构的领导组和支撑组影响差异显著。


<details>
  <summary>Details</summary>
Motivation: 目前学界普遍关注团队性别多样性对科研成功的影响，但过往研究主要以团队整体为单位考察，忽略了团队内部不同角色的性别多样性，其影响机制尚不清楚。本文旨在深入探讨领导组和支撑组的性别多样性如何分别影响团队的学术影响力。

Method: 作者基于PLoS期刊13万余篇论文，利用作者贡献声明将作者分为领导组和支撑组，并以五年引用数衡量团队影响力。主要采用多变量回归和阈值回归模型分析性别多样性与团队影响力的关系及团队规模的调节效应。

Result: （1）领导组和支撑组的性别多样性对团队影响力均呈倒U型关系；（2）全部为女性的领导组配合全部为男性的支撑组团队影响力最高；（3）团队规模较小时，领导组性别多样性有负面影响，规模较大则此效应转为正且不显著；（4）支撑组性别多样性的正向效应与团队规模无关且稳定显著。

Conclusion: 团队内不同角色的性别多样性对科研影响存在复杂作用，提升性别多样性的积极效应需考虑团队成员分工和团队规模，简单地增加性别多样性未必带来持续收益。

Abstract: The influence of gender diversity on the success of scientific teams is of great interest to academia. However, prior findings remain inconsistent, and most studies operationalize diversity in aggregate terms, overlooking internal role differentiation. This limitation obscures a more nuanced understanding of how gender diversity shapes team impact. In particular, the effect of gender diversity across different team roles remains poorly understood. To this end, we define a scientific team as all coauthors of a paper and measure team impact through five-year citation counts. Using author contribution statements, we classified members into leadership and support roles. Drawing on more than 130,000 papers from PLOS journals, most of which are in biomedical-related disciplines, we employed multivariable regression to examine the association between gender diversity in these roles and team impact. Furthermore, we apply a threshold regression model to investigate how team size moderates this relationship. The results show that (1) the relationship between gender diversity and team impact follows an inverted U-shape for both leadership and support groups; (2) teams with an all-female leadership group and an all-male support group achieve higher impact than other team types. Interestingly, (3) the effect of leadership-group gender diversity is significantly negative for small teams but becomes positive and statistically insignificant in large teams. In contrast, the estimates for support-group gender diversity remain significant and positive, regardless of team size.

</details>


### [228] [C2PO: Diagnosing and Disentangling Bias Shortcuts in LLMs](https://arxiv.org/abs/2512.23430)
*Xuan Feng,Bo An,Tianlong Gu,Liang Chang,Fengrui Hao,Peipeng Yu,Shuai Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种统一的对齐框架C2PO，可以同时抑制大型语言模型中的刻板（如性别、种族）和结构性偏见，显著提升了模型的公平性而不损失推理能力。


<details>
  <summary>Details</summary>
Motivation: 此前方法往往片面解决刻板或结构性偏见中的一种，修正一种却加重另一种，本研究旨在系统性揭示并克服大型语言模型推理偏见的底层原因，提升模型的可信赖性。

Method: 作者提出Causal-Contrastive Preference Optimization (C2PO) 框架，通过在优化过程中直接发现并抑制输入中的虚假特征关联，实现偏见消除。具体方法包括利用因果反事实信号隔离引发偏见的特征、通过偏见敏感的偏好更新机制动态评价并抑制推理捷径特征。

Result: 在多个数据集和基准测试（如BBQ、Unqover、MNLI、HANS、Chatbot、MT-Bench、StereoSet、WinoBias、MMLU、GSM8K）中，C2PO 显著减少刻板和结构性偏见，并且保持了良好的泛化推理能力。

Conclusion: C2PO作为统一的模型对齐方法，有效解决了模型推理中的多类偏见问题，在保障模型智能水平的同时，提升了公平性和可靠性。

Abstract: Bias in Large Language Models (LLMs) poses significant risks to trustworthiness, manifesting primarily as stereotypical biases (e.g., gender or racial stereotypes) and structural biases (e.g., lexical overlap or position preferences). However, prior paradigms typically address these in isolation, often mitigating one at the expense of exacerbating the other. To address this, we conduct a systematic exploration of these reasoning failures and identify a primary inducement: the latent spurious feature correlations within the input that drive these erroneous reasoning shortcuts. Driven by these findings, we introduce Causal-Contrastive Preference Optimization (C2PO), a unified alignment framework designed to tackle these specific failures by simultaneously discovering and suppressing these correlations directly within the optimization process. Specifically, C2PO leverages causal counterfactual signals to isolate bias-inducing features from valid reasoning paths, and employs a fairness-sensitive preference update mechanism to dynamically evaluate logit-level contributions and suppress shortcut features. Extensive experiments across multiple benchmarks covering stereotypical bias (BBQ, Unqover), structural bias (MNLI, HANS, Chatbot, MT-Bench), out-of-domain fairness (StereoSet, WinoBias), and general utility (MMLU, GSM8K) demonstrate that C2PO effectively mitigates stereotypical and structural biases while preserving robust general reasoning capabilities.

</details>


### [229] [ClinDEF: A Dynamic Evaluation Framework for Large Language Models in Clinical Reasoning](https://arxiv.org/abs/2512.23440)
*Yuqi Tang,Jing Yu,Zichang Su,Kehua Feng,Zhihui Zhu,Libin Wang,Lei Liang,Qiang Zhang,Keyan Ding,Huajun Chen*

Main category: cs.CL

TL;DR: 本文提出了一种动态的临床推理评估框架ClinDEF，通过模拟诊断对话全面评测大语言模型（LLM）的临床推理能力，并发现现有主流LLM在临床推理方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型医学基准侧重静态问答，无法有效覆盖真实临床推理中动态互动和决策的复杂过程。因此，亟需更符合实际医疗环境的评测方法。

Method: 作者提出ClinDEF框架，基于疾病知识图自动动态生成病例，模拟医患多轮交互（由LLM充当医生，自动代理作为患者），并通过细致的效率和质量评分体系对模型进行多维度评估。

Result: 实验表明，ClinDEF能够有效揭示主流LLM在临床推理过程中的关键不足，为模型优化和筛选提供了更丰富的信息。

Conclusion: ClinDEF填补了既有医学LLM评测标准的空白，更真实、全面地反映了模型在实际临床场景中的推理能力，对LLM在医疗领域的研究具有重要价值。

Abstract: Clinical diagnosis begins with doctor-patient interaction, during which physicians iteratively gather information, determine examination and refine differential diagnosis through patients' response. This dynamic clinical-reasoning process is poorly represented by existing LLM benchmarks that focus on static question-answering. To mitigate these gaps, recent methods explore dynamic medical frameworks involving interactive clinical dialogues. Although effective, they often rely on limited, contamination-prone datasets and lack granular, multi-level evaluation. In this work, we propose ClinDEF, a dynamic framework for assessing clinical reasoning in LLMs through simulated diagnostic dialogues. Grounded in a disease knowledge graph, our method dynamically generates patient cases and facilitates multi-turn interactions between an LLM-based doctor and an automated patient agent. Our evaluation protocol goes beyond diagnostic accuracy by incorporating fine-grained efficiency analysis and rubric-based assessment of diagnostic quality. Experiments show that ClinDEF effectively exposes critical clinical reasoning gaps in state-of-the-art LLMs, offering a more nuanced and clinically meaningful evaluation paradigm.

</details>


### [230] [Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss](https://arxiv.org/abs/2512.23447)
*Ang Lv,Jin Ma,Yiyuan Ma,Siyuan Qiao*

Main category: cs.CL

TL;DR: 该论文提出了一种名为专家-路由器耦合（ERC）损失的新方法，以增强Mixture-of-Experts（MoE）模型中路由器的决策与专家能力之间的一致性，有效提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前MoE模型中，路由器决策与各专家能力未严格对齐，导致模型性能受限。作者希望引入约束，使路由器的分配更准确地反映专家的特长，实现更优的专家分工，推动MoE模型性能提升。

Method: 提出ERC损失，将每个专家的路由器嵌入作为该专家代理token，并对其添加扰动后输入专家模块，获得内部激活。ERC损失包含两点约束：（1）每位专家对自身代理token的激活高于对其他专家代理token的激活；（2）每个代理token在本专家的激活高于在其他专家的激活。该损失仅基于专家数n的平方进行计算，计算量固定且高效。

Result: 通过在3B至15B参数规模的MoE大型语言模型上进行预训练并分析大量数据，实验表明ERC损失能够有效提升专家分化与模型整体性能，与以往随token数增长的耦合方法相比显著高效。

Conclusion: ERC损失提供了一种高效、可控的专家分工机制，不仅提升了MoE模型性能，还给模型训练过程中的专家专精度分析与控制带来新工具，对后续MoE相关研究和应用有实际意义。

Abstract: Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.

</details>


### [231] [Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings](https://arxiv.org/abs/2512.23471)
*Thomas Haschka,Joseph Bakarji*

Main category: cs.CL

TL;DR: 本文提出了一种嵌套密度聚类方法，通过在LLM嵌入空间中从密集到稀疏层次搜索，自动推断文本语义层次结构，实现无需预定义类别的语义文本分类。


<details>
  <summary>Details</summary>
Motivation: 现有方法利用LLM嵌入进行相似性检索和存储，但难以揭示文本语料库中全局的语义层次关系。为了更有效地探索和理解文本之间隐藏的语义结构，需要一种能自动分层归类的聚类方法。

Method: 使用嵌套密度聚类：首先在LLM嵌入空间中寻找高度相似的密集文本簇，逐步放宽密度标准，使得簇之间合并，最终形成一个反映语义层级关系的树状结构。以科学文摘、20新闻组、IMDB 50k影评等数据集为例进行验证。

Result: 该方法能自动推断文本语料的语义层次结构，能够在不同领域的数据集（如科学文摘、新闻组、影评）上表现出鲁棒性和普适性。

Conclusion: 嵌套密度树能自动发现文本集合中的语义结构和关系，适合科学计量、主题演变等应用，拓宽了大规模文本分析与理解的新路径。

Abstract: Semantic text classification has undergone significant advances in recent years due to the rise of large language models (LLMs) and their high dimensional embeddings. While LLM-embeddings are frequently used to store and retrieve text by semantic similarity in vector databases, the global structure semantic relationships in text corpora often remains opaque. Herein we propose a nested density clustering approach, to infer hierarchical trees of semantically related texts. The method starts by identifying texts of strong semantic similarity as it searches for dense clusters in LLM embedding space. As the density criterion is gradually relaxed, these dense clusters merge into more diffuse clusters, until the whole dataset is represented by a single cluster - the root of the tree. By embedding dense clusters into increasingly diffuse ones, we construct a tree structure that captures hierarchical semantic relationships among texts. We outline how this approach can be used to classify textual data for abstracts of scientific abstracts as a case study. This enables the data-driven discovery research areas and their subfields without predefined categories. To evaluate the general applicability of the method, we further apply it to established benchmark datasets such as the 20 News- groups and IMDB 50k Movie Reviews, demonstrating its robustness across domains. Finally we discuss possible applications on scientometrics, topic evolution, highlighting how nested density trees can reveal semantic structure and evolution in textual datasets.

</details>


### [232] [Automatic Detection of Complex Quotation Patterns in Aggadic Literature](https://arxiv.org/abs/2512.23504)
*Hadar Miller,Tsvi Kuflik,Moshe Lavee*

Main category: cs.CL

TL;DR: 本文提出了一种新算法ACT（Allocate Connections between Texts），用于自动检测拉比文学中的圣经引文，并在应对复杂和简短改写引用方面表现优越。ACT显著优于现有系统，精确率和召回率均表现突出，推动了数字人文学科中文本引用研究的发展。


<details>
  <summary>Details</summary>
Motivation: 现有文本重用检测系统在识别短小、改写或结构复杂的引用（如拉比文学中常见的圣经引文）时表现不佳，缺乏对复杂风格和结构的处理能力。亟需一种能够精细区分和检测不同引用模式的新方法，以推动文学、历史和语文学科的研究。

Method: ACT算法分为三阶段：首先进行形态学感知的文本比对；其次通过上下文敏感的增强算法识别诸如"波"和"回声"等复杂引用模式；最后评估多种ACT配置以分离不同组件作用。算法在多个领先系统和人工标注文本上进行测试，细致对比了配备和不配备风格增强的版本效果。

Result: 完整ACT系统（ACT-QE）以0.91的F1分数领先所有基线系统，召回率达0.89，精确率达0.94。无风格增强的ACT-2召回更高但精确率较低，长n-gram配置（ACT-3）则在覆盖率和特异度间取得平衡。ACT不仅提升了文本引用检测，还实现了风格模式分类，助力跨语料体裁归类与互文性分析。

Conclusion: ACT填补了自动化文本引用检测与人工编辑判断间的方法学空白，对数字人文和计算语文学界具有重要意义。该方法特别适用于形态结构复杂、引文密集的文本传统（如拉比文学），为历史文本分析提供了坚实基础，并为体裁和互文性等研究方向开辟了新路径。

Abstract: This paper presents ACT (Allocate Connections between Texts), a novel three-stage algorithm for the automatic detection of biblical quotations in Rabbinic literature. Unlike existing text reuse frameworks that struggle with short, paraphrased, or structurally embedded quotations, ACT combines a morphology-aware alignment algorithm with a context-sensitive enrichment stage that identifies complex citation patterns such as "Wave" and "Echo" quotations.
  Our approach was evaluated against leading systems, including Dicta, Passim, Text-Matcher, as well as human-annotated critical editions. We further assessed three ACT configurations to isolate the contribution of each component. Results demonstrate that the full ACT pipeline (ACT-QE) outperforms all baselines, achieving an F1 score of 0.91, with superior Recall (0.89) and Precision (0.94). Notably, ACT-2, which lacks stylistic enrichment, achieves higher Recall (0.90) but suffers in Precision, while ACT-3, using longer n-grams, offers a tradeoff between coverage and specificity.
  In addition to improving quotation detection, ACT's ability to classify stylistic patterns across corpora opens new avenues for genre classification and intertextual analysis. This work contributes to digital humanities and computational philology by addressing the methodological gap between exhaustive machine-based detection and human editorial judgment. ACT lays a foundation for broader applications in historical textual analysis, especially in morphologically rich and citation-dense traditions like Aggadic literature.

</details>


### [233] [UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?](https://arxiv.org/abs/2512.23512)
*Fengjiao Chen,Minhao Jing,Weitao Lu,Yan Feng,Xiaoyu Li,Xuezhi Cao*

Main category: cs.CL

TL;DR: 本文探讨了视觉-语言大模型中生成能力对理解能力的影响，通过提出统一结构UniHetero，实验证明生成任务能提升视觉理解，但前提是生成的是语义信息而非像素。本文还揭示了数据扩展性及利用率提升，以及输入嵌入的自回归机制效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言大模型在统一视觉理解与生成上取得进展，但‘生成是否能反哺理解’这一关键问题，尤其在大规模数据下，尚未充分探讨。推动多模态模型兼容多种任务的整体性能是研究动因。

Method: 提出了统一模型结构UniHetero，并在2亿+样本规模下进行预训练，分别分析语义生成与像素生成对理解能力的影响，比较数据扩展性和利用率，并评估输入嵌入自回归机制捕捉视觉细节的效果。

Result: 实验结果显示：1）仅有生成语义信息（非像素）才能显著提升视觉理解能力；2）生成任务呈现出更好的数据扩展趋势及更高的数据利用率；3）在输入嵌入上做自回归有助于捕捉视觉细节。

Conclusion: 生成能力对视觉理解有促进作用，但关键在于生成语义而非像素。同时，生成任务在大规模数据上表现出更好的扩展性和利用效率，且自回归机制适用于细节捕捉，验证了统一大模型结构在视觉-语言理解与生成领域的潜力。

Abstract: Vision-language large models are moving toward the unification of visual understanding and visual generation tasks. However, whether generation can enhance understanding is still under-explored on large data scale. In this work, we analysis the unified model with a concise structure, UniHetero, under large-scale pretraining (>200M samples). Our key observations are: (1) Generation can improve understanding, but Only if you generate Semantics, Not Pixels. (2) Generation reveals a superior Data Scaling trend and higher Data Utilization. (3) Autoregression on Input Embedding is effective to capture visual details.

</details>


### [234] [Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias](https://arxiv.org/abs/2512.23518)
*Hazel Kim,Philip Torr*

Main category: cs.CL

TL;DR: 本文提出MoLaCE方法，通过不同潜在概念专家的组合，解决大语言模型在推理时因输入偏见而出现的确认性偏见问题，且该方法低计算成本、高效且灵活，效果优于多智能体争辩机制。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型易受输入中的确认性偏见影响，会倾向于强化而不是质疑输入中的偏见，这不仅在模型本身存在，且在多智能体争辩（debate）场景下风险更高，容易造成回音室效应，因此亟需有效方法缓解确认性偏见。

Method: 提出Mixture of Latent Concept Experts（MoLaCE）推理框架，在模型内部以不同激活强度混合多位潜在概念专家，在不同提示语下动态调整对潜在概念的权重，实现多样化推理路径。此方法无需修改模型结构，只需推理时应用。

Result: 实验证明，MoLaCE能有效减少确认性偏见，提高模型鲁棒性，并且在计算量远小于多智能体争辩的情况下，效果可超越或匹配多智能体争辩方法。

Conclusion: MoLaCE为大语言模型消除输入偏见提供了一种高效、可扩展的方法，不仅可单独应用，也可集成进多智能体框架，具有良好的实际应用前景。

Abstract: Large language models (LLMs) are highly vulnerable to input confirmation bias. When a prompt implies a preferred answer, models often reinforce that bias rather than explore alternatives. This phenomenon remains underexplored, yet it is already harmful in base models and poses an even greater risk in multi-agent debate, where echo chambers reinforce bias instead of correction. We introduce Mixture of Latent Concept Experts (MoLaCE), a lightweight inference-time framework that addresses confirmation bias by mixing experts instantiated as different activation strengths over latent concepts that shape model responses. Our key insight is that, due to the compositional nature of language, differently phrased prompts reweight latent concepts in prompt-specific ways that affect factual correctness, so no single fixed intervention can be applied universally across inputs. This design enables a single LLM to emulate the benefits of debate internally while remaining computationally efficient and scalable. It can also be integrated into multi-agent debate frameworks to diversify perspectives and reduce correlated errors. We empirically show that it consistently reduces confirmation bias, improves robustness, and matches or surpasses multi-agent debate while requiring only a fraction of the computation.

</details>


### [235] [Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs](https://arxiv.org/abs/2512.23547)
*Sahil Kale,Antonio Luca Alfeo*

Main category: cs.CL

TL;DR: 该论文提出利用知识图谱提升大语言模型（LLM）幻觉（虚假内容）自检测的准确性和可靠性。通过将模型输出转化为知识图谱并用来评估幻觉概率，在多个数据集和主流LLM上的实验显示，该方法较现有自检测方法显著提升了检测表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生成文本时常出现幻觉问题，影响其安全性和信任度。虽然已有自检测方法表现良好，但仍有改进空间。作者希望通过结构化知识表征（知识图谱）来提升检测幻觉的能力。

Method: 方法包括两步：第一，将LLM的回复转化为实体及其关系的知识图谱；第二，基于生成的知识图谱来估算回复中包含幻觉的可能性。并在GPT-4o和Gemini-2.5-Flash两个模型和两个数据集上进行了评估（其中一个数据集经过人工增强并公开发布）。

Result: 与标准自检测方法和当前最新方法SelfCheckGPT对比，新方法在准确率上最高提升16%，F1分数提升20%。实验表明结构化的事实表征有助于更好地识别幻觉，即使初始输出存在不准之处。

Conclusion: 提出的知识图谱驱动幻觉自检测方法成本低且与具体模型无关，有助于构建更安全、更可信的LLM系统。

Abstract: Hallucinations, the generation of apparently convincing yet false statements, remain a major barrier to the safe deployment of LLMs. Building on the strong performance of self-detection methods, we examine the use of structured knowledge representations, namely knowledge graphs, to improve hallucination self-detection. Specifically, we propose a simple yet powerful approach that enriches hallucination self-detection by (i) converting LLM responses into knowledge graphs of entities and relations, and (ii) using these graphs to estimate the likelihood that a response contains hallucinations. We evaluate the proposed approach using two widely used LLMs, GPT-4o and Gemini-2.5-Flash, across two hallucination detection datasets. To support more reliable future benchmarking, one of these datasets has been manually curated and enhanced and is released as a secondary outcome of this work. Compared to standard self-detection methods and SelfCheckGPT, a state-of-the-art approach, our method achieves up to 16% relative improvement in accuracy and 20% in F1-score. Our results show that LLMs can better analyse atomic facts when they are structured as knowledge graphs, even when initial outputs contain inaccuracies. This low-cost, model-agnostic approach paves the way toward safer and more trustworthy language models.

</details>


### [236] [Instruction-Following Evaluation of Large Vision-Language Models](https://arxiv.org/abs/2512.23572)
*Daiki Shiono,Shumpei Miyawaki,Ryota Tanaka,Jun Suzuki*

Main category: cs.CL

TL;DR: 本文发现大规模视觉-语言模型（LVLMs）在常规可视化指令微调后，指令遵循能力下降，并通过量化分析验证了这一现象。作者发现，在微调数据集中明确包含输出格式说明可缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）和视觉-语言模型（LVLMs）的发展，将LLMs和视觉能力结合成为研究热点。但研究者发现，LVLMs在常用指令数据集微调后，往往丧失了原有的良好指令遵循能力。理解这一现象背后的原因及其解决办法对提升LVLMs的实际应用价值具有重要意义。

Method: 作者通过量化实验，验证LVLMs在常规数据集微调后指令遵循能力的下降。进一步，作者构建了训练集，控制是否明确指示输出格式，对比分析在不同训练集下，模型在指令遵循性上的表现差异。

Result: 实验结果显示，LVLMs在常用数据集微调后，确实存在指令遵循能力下降的问题。而在微调数据中包含明确输出格式指令时，模型的指令遵循能力得到了有效提升。

Conclusion: LVLMs的指令遵循能力容易因微调而削弱。为了缓解该问题，推荐在微调数据中加入指令输出格式的明确信息，提高模型对复杂任务指令的遵循能力。

Abstract: Following the initial flourishing of large language models (LLMs), there has been a surge in proposed large vision-language models (LVLMs) that integrate LLMs with vision capabilities. However, it has been observed that LVLMs, after tuning to visual instruction using commonly used training datasets, often fail to exhibit the instruction-following ability that was present in the LLM before integration, leading to results in which they do not follow task instructions as expected. This study quantitatively demonstrates that LVLMs' instruction-following ability declines after fine-tuning and analyzes its underlying causes. In particular, we constructed new training datasets highlighting whether the output format is specified. Then, we investigated how explicitly indicating the output format during fine-tuning affects LVLMs' instruction-following ability. Our quantitative evaluation confirmed that LVLMs' instruction-following ability declines after fine-tuning with commonly used datasets. Furthermore, we found that LVLMs trained with datasets, including instructions on output format, tend to follow instructions more accurately than models that do not. These findings suggest that including samples with instructions on output format during (visual) instruction tuning may help mitigate the decline in instruction-following abilities.

</details>


### [237] [Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models](https://arxiv.org/abs/2512.23578)
*Yu-Xiang Lin,Cheng-Han Chiang,Hung-yi Lee*

Main category: cs.CL

TL;DR: SLMs在多轮对话中难以持续保持指定的说话风格，这是“风格失忆”现象。即便能回忆风格指令，表达依旧不一致。系统提示效果逊色于用户提示。


<details>
  <summary>Details</summary>
Motivation: 随着SLM技术的提升，要求其能持续模拟特定口语风格越来越重要，但其一致性能力尚不清楚。本文旨在系统性评估SLM对说话风格维持的能力。

Method: 作者对三种专有和两种开源SLM进行实验，关注情感、口音、音量、语速等副语言风格。采用多轮对话测试风格保持，并测试直接提示和不同提示策略对风格保持的影响。

Result: 所有测试的SLM在多轮交流中均表现出风格失忆现象，即无法持续维持最初指定的风格。模型虽能回忆风格要求，但实际表达无法保持一致。用系统消息指令风格时表现更差。

Conclusion: 现有SLM难以在多轮对话中维持口语风格一致性，仅适度通过反复提示得到改善。系统消息传递风格指令的有效性有待提升。

Abstract: In this paper, we show that when spoken language models (SLMs) are instructed to speak in a specific speaking style at the beginning of a multi-turn conversation, they cannot maintain the required speaking styles after several turns of interaction; we refer to this as the style amnesia of SLMs. We focus on paralinguistic speaking styles, including emotion, accent, volume, and speaking speed. We evaluate three proprietary and two open-source SLMs, demonstrating that none of these models can maintain a consistent speaking style when instructed to do so. We further show that when SLMs are asked to recall the style instruction in later turns, they can recall the style instruction, but they fail to express it throughout the conversation. We also show that explicitly asking the model to recall the style instruction can partially mitigate style amnesia. In addition, we examine various prompting strategies and find that SLMs struggle to follow the required style when the instruction is placed in system messages rather than user messages, which contradicts the intended function of system prompts.

</details>


### [238] [Close the Loop: Synthesizing Infinite Tool-Use Data via Multi-Agent Role-Playing](https://arxiv.org/abs/2512.23611)
*Yuwen Li,Wei Zhang,Zelong Huang,Mason Yang,Jiajun Wu,Shawn Guo,Huahao Hu,Lingyi Sun,Jian Yang,Mingjie Tang,Byran Dai*

Main category: cs.CL

TL;DR: 本文提出InfTool框架，通过多智能体协同，无需人工标注，仅基于API规范，实现高质量的数据合成和工具调用能力提升，用于大语言模型调用外部工具场景。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型调用外部工具面临人工标注成本高、难以泛化和单模型瓶颈等问题，严重限制了自主智能体的发展。

Method: InfTool利用用户模拟器、工具调用助手和MCP服务器三个智能体协作，根据API原始规范自动生成多样化、经验证的数据轨迹，通过Group Relative Policy Optimization（GRPO）带门控奖励进行模型训练，并闭环自我进化，无需人工参与。

Result: 在Berkeley Function-Calling Leaderboard（BFCL）测试上，InfTool使得32B模型准确率从19.8%提升至70.9%，超越了参数量大10倍的模型，接近Claude-Opus，且完全未用人工标注数据。

Conclusion: InfTool验证了多智能体自进化方法能突破数据获取和泛化瓶颈，大幅提升大模型的工具调用能力，为自动化智能体落地奠定基础。

Abstract: Enabling Large Language Models (LLMs) to reliably invoke external tools remains a critical bottleneck for autonomous agents. Existing approaches suffer from three fundamental challenges: expensive human annotation for high-quality trajectories, poor generalization to unseen tools, and quality ceilings inherent in single-model synthesis that perpetuate biases and coverage gaps. We introduce InfTool, a fully autonomous framework that breaks these barriers through self-evolving multi-agent synthesis. Given only raw API specifications, InfTool orchestrates three collaborative agents (User Simulator, Tool-Calling Assistant, and MCP Server) to generate diverse, verified trajectories spanning single-turn calls to complex multi-step workflows. The framework establishes a closed loop: synthesized data trains the model via Group Relative Policy Optimization (GRPO) with gated rewards, the improved model generates higher-quality data targeting capability gaps, and this cycle iterates without human intervention. Experiments on the Berkeley Function-Calling Leaderboard (BFCL) demonstrate that InfTool transforms a base 32B model from 19.8% to 70.9% accuracy (+258%), surpassing models 10x larger and rivaling Claude-Opus, and entirely from synthetic data without human annotation.

</details>


### [239] [A Dataset and Benchmark for Consumer Healthcare Question Summarization](https://arxiv.org/abs/2512.23637)
*Abhishek Basu,Deepak Gupta,Dina Demner-Fushman,Shweta Yadav*

Main category: cs.CL

TL;DR: 作者提出并公开了一个由领域专家标注的健康问答摘要数据集（CHQ-Sum），用于改善健康类问题的自动摘要任务。


<details>
  <summary>Details</summary>
Motivation: 目前健康信息查询在网络上日益增多，用户表达方式冗长、细节多、挑战自然语言理解。缺乏大规模且由专业人员标注的健康问答摘要数据集，限制了有效自动摘要系统的开发。

Method: 作者从社区问答论坛收集并整合了1507条健康类用户问题，由领域专家进行摘要标注，构建了CHQ-Sum数据集。并在多种主流文本摘要模型上进行了基准测试。

Result: 建立并发布了一个涵盖1507条由专家标注的消费者健康问题及其摘要的数据集，并通过主流摘要模型在该数据集上的测试验证了其实用性和挑战性。

Conclusion: CHQ-Sum是面向健康领域问答摘要的重要资源，可推动相关领域摘要技术和研究进展。

Abstract: The quest for seeking health information has swamped the web with consumers health-related questions. Generally, con- sumers use overly descriptive and peripheral information to express their medical condition or other healthcare needs, contributing to the challenges of natural language understanding. One way to address this challenge is to summarize the questions and distill the key information of the original question. Recently, large-scale datasets have significantly propelled the development of several summarization tasks, such as multi-document summarization and dialogue summarization. However, a lack of a domain-expert annotated dataset for the consumer healthcare questions summarization task inhibits the development of an efficient summarization system. To address this issue, we introduce a new dataset, CHQ-Sum,m that contains 1507 domain-expert annotated consumer health questions and corresponding summaries. The dataset is derived from the community question answering forum and therefore provides a valuable resource for understanding consumer health-related posts on social media. We benchmark the dataset on multiple state-of-the-art summarization models to show the effectiveness of the dataset

</details>


### [240] [Nested Browser-Use Learning for Agentic Information Seeking](https://arxiv.org/abs/2512.23647)
*Baixuan Li,Jialong Wu,Wenbiao Yin,Kuan Li,Zhongwang Zhang,Huifeng Yin,Zhengwei Tao,Liwen Zhang,Pengjun Xie,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: 该论文提出NestBrowse，一种将复杂浏览器交互简化为嵌套结构的IS代理方法，显著提升了深度信息检索能力与效率。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索（IS）代理大多只限于API调用和简单网页片段获取，难以有效访问和利用真实网页中的丰富信息。而完全浏览器操作虽可扩展能力，但操作复杂度高，内容繁琐且不易于现有代理框架（如ReAct）应用。本研究旨在突破这些局限，使代理具备深度网页浏览和检索能力，同时保持控制高效和推理简洁。

Method: 作者提出了NestBrowse框架，核心为将浏览器的操作与页面探索解耦，通过嵌套式结构实现，对代理提供了最小而完整的浏览器动作接口。该方法允许代理以模块化、分层的方式控制浏览和检索流程，减少冗余操作和信息噪声。

Result: 在多个复杂、具有挑战性的深度检索基准测试中，NestBrowse展现出了比传统代理方式更高的信息获取效率和准确性。实验还展示了其结构带来的推理简化和操作灵活性优势。

Conclusion: NestBrowse有效解决了API/URL受限检索和全浏览器交互复杂性的矛盾，实现了高效灵活的深度信息检索，对提升智能代理的能力具有重要意义。

Abstract: Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge this gap, we propose Nested Browser-Use Learning (NestBrowse), which introduces a minimal and complete browser-action framework that decouples interaction control from page exploration through a nested structure. This design simplifies agentic reasoning while enabling effective deep-web information acquisition. Empirical results on challenging deep IS benchmarks demonstrate that NestBrowse offers clear benefits in practice. Further in-depth analyses underscore its efficiency and flexibility.

</details>


### [241] [Less is more: Probabilistic reduction is best explained by small-scale predictability measures](https://arxiv.org/abs/2512.23659)
*Cassandra L. Jacobs,Andrés Buxó-Lugo,Anna K. Taylor,Marie Leopold-Hooke*

Main category: cs.CL

TL;DR: 本文研究了在研究语言模型概率与认知现象的关系时，所需的最小上下文量。结果发现n-gram这样的短语级别上下文就足够作为认知计划的单位，无需完整的语句。


<details>
  <summary>Details</summary>
Motivation: 驱动力在于明确语言模型概率与人类认知现象之间关系时，究竟需要多少语言上下文，进而为心理语言学实验和自然语言处理领域提供理论依据和实验方法支持。

Method: 通过对比使用完整语句和n-gram短语为上下文的语言模型在认知任务（如概率缩减）中的表现，分析不同上下文粒度对模型与认知现象拟合度的影响。

Result: 实验证明，n-gram短语级别的上下文已经足够展现出概率缩减现象，无需完整语句。即，n-gram表示可以作为认知计划的合理单位。

Conclusion: 在模拟认知现象时，简化上下文到n-gram级别既有效又高效。此结果为心理语言学实验设计和语言模型应用提供了更实用的方法论。

Abstract: The primary research questions of this paper center on defining the amount of context that is necessary and/or appropriate when investigating the relationship between language model probabilities and cognitive phenomena. We investigate whether whole utterances are necessary to observe probabilistic reduction and demonstrate that n-gram representations suffice as cognitive units of planning.

</details>


### [242] [Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing](https://arxiv.org/abs/2512.23684)
*Panagiotis Theocharopoulos,Ajinkya Kulkarni,Mathew Magimai. -Doss*

Main category: cs.CL

TL;DR: 本文评估了在ICML会议真实论文中隐藏植入对抗性提示对大语言模型（LLM）审核结果的影响，发现不同语言的提示注入会显著影响审核分数和录用决定，表明LLM审核系统易受此类攻击，且对不同语言的敏感性有所不同。


<details>
  <summary>Details</summary>
Motivation: LLM逐渐被用在高影响力的流程如学术评审，但它们容易受到隐藏式提示注入攻击。作者希望评估这种攻击在实际学术论文环境中的实际影响及其跨语言差异性。

Method: 作者收集了约500篇ICML录用论文，并分别在每篇论文中以英文、日文、中文和阿拉伯文植入语义等价的对抗性提示，然后利用LLM进行审稿，考察不同语言提示注入对审稿分数和录用结果的影响。

Result: 提示注入会大幅改变英文、日文、中文情况下的审稿分数及录用/拒绝决策，而阿拉伯文的注入影响极小或几乎没有影响。

Conclusion: 此项研究证明了基于LLM的评审系统对文档级提示注入有高度敏感性，不同语言的攻击效果存在较大差异，提示未来LLM审稿流程需要重视此类安全隐患。

Abstract: Large language models (LLMs) are increasingly considered for use in high-impact workflows, including academic peer review. However, LLMs are vulnerable to document-level hidden prompt injection attacks. In this work, we construct a dataset of approximately 500 real academic papers accepted to ICML and evaluate the effect of embedding hidden adversarial prompts within these documents. Each paper is injected with semantically equivalent instructions in four different languages and reviewed using an LLM. We find that prompt injection induces substantial changes in review scores and accept/reject decisions for English, Japanese, and Chinese injections, while Arabic injections produce little to no effect. These results highlight the susceptibility of LLM-based reviewing systems to document-level prompt injection and reveal notable differences in vulnerability across languages.

</details>


### [243] [PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech](https://arxiv.org/abs/2512.23686)
*Deepak Babu Piskala*

Main category: cs.CL

TL;DR: 本文提出了ProfASR-Bench基准，用于评测专业语境下ASR模型在术语丰富、实体敏感场景下的表现，发现主流模型对提示上下文的利用十分有限。


<details>
  <summary>Details</summary>
Motivation: 在金融、医疗、法律、科技等高风险、专业领域，ASR面临专业术语密集、正式语言风格多变、关键实体识别不能出错等挑战，现有基准难以准确反映这些痛点。

Method: 提出ProfASR-Bench基准，数据集涵盖四大专业领域。每个案例包含自然语言提示（领域/说话人信息）与含大量实体的目标句，便于精准测试模型对上下文提示的利用能力。对Whisper和Qwen-Omni两类代表性ASR模型，在不同上下文条件下（无上下文、个人信息、领域+个人、最优、对抗）做系统评估，并引入实体感知指标、按口音/性别分片报告等。

Result: 实验表明，无论加多少文字性上下文、即便是最优提示，对模型平均词错误率几乎无提升，对抗提示也难导致性能下降。即，主流ASR系统虽能接受提示，但实际很少用上附加信息。

Conclusion: 现有ASR模型在高风险专业场景下存在显著的“上下文利用鸿沟”。ProfASR-Bench为ASR研究社区提供了标准化上下文测试用梯度、多维度可复现的评测方法，有助于推动更好地融合上下文信息的ASR研究。

Abstract: Automatic Speech Recognition (ASR) in professional settings faces challenges that existing benchmarks underplay: dense domain terminology, formal register variation, and near-zero tolerance for critical entity errors. We present ProfASR-Bench, a professional-talk evaluation suite for high-stakes applications across finance, medicine, legal, and technology. Each example pairs a natural-language prompt (domain cue and/or speaker profile) with an entity-rich target utterance, enabling controlled measurement of context-conditioned recognition. The corpus supports conventional ASR metrics alongside entity-aware scores and slice-wise reporting by accent and gender. Using representative families Whisper (encoder-decoder ASR) and Qwen-Omni (audio language models) under matched no-context, profile, domain+profile, oracle, and adversarial conditions, we find a consistent pattern: lightweight textual context produces little to no change in average word error rate (WER), even with oracle prompts, and adversarial prompts do not reliably degrade performance. We term this the context-utilization gap (CUG): current systems are nominally promptable yet underuse readily available side information. ProfASR-Bench provides a standardized context ladder, entity- and slice-aware reporting with confidence intervals, and a reproducible testbed for comparing fusion strategies across model families.
  Dataset: https://huggingface.co/datasets/prdeepakbabu/ProfASR-Bench
  Code: https://github.com/prdeepakbabu/ProfASR-Bench

</details>


### [244] [Fine-Tuning LLMs with Fine-Grained Human Feedback on Text Spans](https://arxiv.org/abs/2512.23693)
*Sky CH-Wang,Justin Svegliato,Helen Appel,Jason Eisner*

Main category: cs.CL

TL;DR: 本文提出了一种利用基于反馈的改进链对语言模型进行偏好监督微调的方法，并构建了相应的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好对齐方法多采用整体A/B选择或对比重写，难以对模型输出中的局部缺陷或优点进行细致反馈，降低了偏好监督的效率和效果。因此，作者希望引入更细粒度、更结构化的监督方式以提升微调效率和对齐效果。

Method: 让标注者在模型输出中用高亮形式注明“喜欢”和“不喜欢”的片段，并说明理由。模型会逐步按从左到右的顺序，重写被标记为不喜欢的内容，形成一系列递进式改进链。在每两个相邻步骤之间构造偏好对，用于训练模型学习局部的、目标明确的编辑。

Result: 实验表明，该基于结构化局部修订的偏好标注和微调方式，相比于传统的A/B对比或整体重写方法，在效率和效果上有明显优势。

Conclusion: 结构化、逐步修订的偏好监督能够更高效、更有效地实现模型偏好对齐，是优于标准直接对比方法的新途径。

Abstract: We present a method and dataset for fine-tuning language models with preference supervision using feedback-driven improvement chains. Given a model response, an annotator provides fine-grained feedback by marking ``liked'' and ``disliked'' spans and specifying what they liked or disliked about them. The base model then rewrites the disliked spans accordingly, proceeding from left to right, forming a sequence of incremental improvements. We construct preference pairs for direct alignment from each adjacent step in the chain, enabling the model to learn from localized, targeted edits. We find that our approach outperforms direct alignment methods based on standard A/B preference ranking or full contrastive rewrites, demonstrating that structured, revision-based supervision leads to more efficient and effective preference tuning.

</details>


### [245] [Eliciting Behaviors in Multi-Turn Conversations](https://arxiv.org/abs/2512.23701)
*Jing Huang,Shujian Zhang,Lun Wang,Andrew Hard,Rajiv Mathews,John Lambert*

Main category: cs.CL

TL;DR: 本文系统分析并比较了在多轮对话中从LLM中引发特定行为的方法，提出了三类方法框架，并首次统一单轮和多轮行为唤起的通用公式。实验显示在线方法在有限交互Query下显著优于静态方法。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM行为唤起的研究多限于单轮对话，缺乏在多轮对话场景下对多样行为的系统评估方法，难以捕捉复杂对话行为。

Method: 提出了三类行为唤起方法（仅用先验知识、离线交互、在线交互），并提出适用于多轮对话的在线方法统一公式，将单轮和多轮情景统一。系统比较并评估各类方法在自动生成多轮测试用例上的效率与发现率。

Result: 在线方法在三项任务上用几千次交互即可达到45%、19%、77%的输入发现率，而静态方法几乎无法发现失败案例，显示出在线方法的高效率和高效能。

Conclusion: 多轮动态行为唤起是评估LLM必要但被忽视的方向，社区应从静态benchmark转向动态、交互式评测体系。

Abstract: Identifying specific and often complex behaviors from large language models (LLMs) in conversational settings is crucial for their evaluation. Recent work proposes novel techniques to find natural language prompts that induce specific behaviors from a target model, yet they are mainly studied in single-turn settings. In this work, we study behavior elicitation in the context of multi-turn conversations. We first offer an analytical framework that categorizes existing methods into three families based on their interactions with the target model: those that use only prior knowledge, those that use offline interactions, and those that learn from online interactions. We then introduce a generalized multi-turn formulation of the online method, unifying single-turn and multi-turn elicitation. We evaluate all three families of methods on automatically generating multi-turn test cases. We investigate the efficiency of these approaches by analyzing the trade-off between the query budget, i.e., the number of interactions with the target model, and the success rate, i.e., the discovery rate of behavior-eliciting inputs. We find that online methods can achieve an average success rate of 45/19/77% with just a few thousand queries over three tasks where static methods from existing multi-turn conversation benchmarks find few or even no failure cases. Our work highlights a novel application of behavior elicitation methods in multi-turn conversation evaluation and the need for the community to move towards dynamic benchmarks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [246] [Joint UAV-UGV Positioning and Trajectory Planning via Meta A3C for Reliable Emergency Communications](https://arxiv.org/abs/2512.22187)
*Ndagijimana Cyprien,Mehdi Sookhak,Hosein Zarini,Chandra N Sekharan,Mohammed Atiquzzaman*

Main category: cs.RO

TL;DR: 本文提出了一种结合无人机（UAV）与无人地面车辆（UGV）的协同部署与轨迹规划框架，旨在灾区场景下以最少的无人机数量实现对地面用户高QoS通信覆盖。提出的Meta-A3C强化学习方法显著提升系统吞吐量及决策速度。


<details>
  <summary>Details</summary>
Motivation: 灾后通信恢复需高效覆盖受影响区域，单靠UAV部署增加数量和成本。联合UAV与UGV，可灵活支撑复杂场景，优化服务质量（QoS）并降低资源消耗，实际中关于其最优协同部署与动态适应少有高效通用方法。

Method: 引入道路图建模UGV在道路上的移动约束，将联合部署问题建模为马尔可夫决策过程（MDP）。采用异步优势演员评论家算法（A3C），并结合元学习机制（Meta-learning）以提升模型对新环境的快速适应能力，实现UAV-UGV的最优位置与轨迹规划。

Result: 数值仿真结果显示，提出的Meta-A3C方法在吞吐量上比传统A3C和DDPG方法高出13.1%，执行速度快49%，且能够确保满足QoS要求。

Conclusion: Meta-A3C算法能够高效实现UAV与UGV的联合最优部署，提升通信覆盖的QoS表现，同时缩短决策时间，适用性强，为灾区紧急通信提供高效可行的技术方案。

Abstract: Joint deployment of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) has been shown to be an effective method to establish communications in areas affected by disasters. However, ensuring good Quality of Services (QoS) while using as few UAVs as possible also requires optimal positioning and trajectory planning for UAVs and UGVs. This paper proposes a joint UAV-UGV-based positioning and trajectory planning framework for UAVs and UGVs deployment that guarantees optimal QoS for ground users. To model the UGVs' mobility, we introduce a road graph, which directs their movement along valid road segments and adheres to the road network constraints. To solve the sum rate optimization problem, we reformulate the problem as a Markov Decision Process (MDP) and propose a novel asynchronous Advantage Actor Critic (A3C) incorporated with meta-learning for rapid adaptation to new environments and dynamic conditions. Numerical results demonstrate that our proposed Meta-A3C approach outperforms A3C and DDPG, delivering 13.1\% higher throughput and 49\% faster execution while meeting the QoS requirements.

</details>


### [247] [VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs](https://arxiv.org/abs/2512.22342)
*Wensi Huang,Shaohao Zhu,Meng Wei,Jinming Xu,Xihui Liu,Hanqing Wang,Tai Wang,Feng Zhao,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 该论文提出了一个新的人机交互导航任务和相应大规模数据集，用于提升机器人通过对话解决导航过程中的模糊指令能力，并验证了该任务和基准对于推动该领域研究的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的具身导航任务假设指令清晰，无二义性，而现实中的指令通常模糊，需通过主动对话澄清和推断用户意图。这一落差使得目前方法难以应用于实际场景。

Method: 提出了Interactive Instance Object Navigation (IION)任务，要求代理不仅要基于视觉和语言输入产生导航动作，还需通过自然语言与oracle（知识源）主动对话以消解不确定性。同时，构建了Vision Language-Language Navigation (VL-LN)大规模数据集和自动评估协议，支持训练和评估具备对话能力的导航模型。

Result: 在VL-LN基准上训练的对话导航模型在实验中显著优于现有基线。论文还进行了多方面实验和分析，验证了所提出基准和方法的有效性及可靠性。

Conclusion: VL-LN数据集和IION任务有效推动了对话式具身导航研究，为让机器人更好地理解和执行现实世界中的含糊导航指令提供了基础。

Abstract: In most existing embodied navigation tasks, instructions are well-defined and unambiguous, such as instruction following and object searching. Under this idealized setting, agents are required solely to produce effective navigation outputs conditioned on vision and language inputs. However, real-world navigation instructions are often vague and ambiguous, requiring the agent to resolve uncertainty and infer user intent through active dialog. To address this gap, we propose Interactive Instance Object Navigation (IION), a task that requires agents not only to generate navigation actions but also to produce language outputs via active dialog, thereby aligning more closely with practical settings. IION extends Instance Object Navigation (ION) by allowing agents to freely consult an oracle in natural language while navigating. Building on this task, we present the Vision Language-Language Navigation (VL-LN) benchmark, which provides a large-scale, automatically generated dataset and a comprehensive evaluation protocol for training and assessing dialog-enabled navigation models. VL-LN comprises over 41k long-horizon dialog-augmented trajectories for training and an automatic evaluation protocol with an oracle capable of responding to agent queries. Using this benchmark, we train a navigation model equipped with dialog capabilities and show that it achieves significant improvements over the baselines. Extensive experiments and analyses further demonstrate the effectiveness and reliability of VL-LN for advancing research on dialog-enabled embodied navigation. Code and dataset: https://0309hws.github.io/VL-LN.github.io/

</details>


### [248] [A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot](https://arxiv.org/abs/2512.22408)
*Amro Gamar,Ahmed Abduljalil,Alargam Mohammed,Ali Elhenidy,Abeer Tawakol*

Main category: cs.RO

TL;DR: 本文介绍了一种集机械工程、嵌入式系统和人工智能于一体的全自动配送机器人平台。采用异构计算架构，将AI感知和路径规划交由RPi 5和ROS 2处理，实时电机控制则由运行FreeRTOS的ESP32完成。通过机械优化和高效资源管理，实现了可靠的实际应用。


<details>
  <summary>Details</summary>
Motivation: 当前自动配送系统面临AI算法资源受限、系统实时性和可靠性难以兼顾等挑战。本文旨在提出一种既高效又可实际部署的自动配送机器人设计方法，提高系统稳定性和实际应用价值。

Method: 采用RPi 5与ROS 2处理高层AI与路径规划，ESP32+FreeRTOS负责实时电机控制。机械部分关注有效载重与机动性，通过精确电机选择和材料工程优化。为解决技术难题，优化了AI算法算力利用，实现了低延迟主控与电机控制器通信，并引入AWS IoT监控及固件级电机关断保护。

Result: 实验表明，该平台通过内存与任务管理，实现了确定性的PID电机控制。系统在高可靠性的AWS IoT监控和容错机制保障下，展现了良好的鲁棒性和实际操作能力。

Conclusion: 本文提出了一套多学科融合的配送机器人开发框架，有效解决了异构平台集成、资源受限下AI算法优化及系统可靠性等难题，开发出可实地部署的自动配送系统。

Abstract: This paper presents the development of a fully autonomous delivery robot integrating mechanical engineering, embedded systems, and artificial intelligence. The platform employs a heterogeneous computing architecture, with RPi 5 and ROS 2 handling AI-based perception and path planning, while ESP32 running FreeRTOS ensures real-time motor control. The mechanical design was optimized for payload capacity and mobility through precise motor selection and material engineering. Key technical challenges addressed include optimizing computationally intensive AI algorithms on a resource-constrained platform and implementing a low-latency, reliable communication link between the ROS 2 host and embedded controller. Results demonstrate deterministic, PID-based motor control through rigorous memory and task management, and enhanced system reliability via AWS IoT monitoring and a firmware-level motor shutdown failsafe. This work highlights a unified, multi-disciplinary methodology, resulting in a robust and operational autonomous delivery system capable of real-world deployment.

</details>


### [249] [Emergence of Human to Robot Transfer in Vision-Language-Action Models](https://arxiv.org/abs/2512.22414)
*Simar Kareer,Karl Pertsch,James Darpinian,Judy Hoffman,Danfei Xu,Sergey Levine,Chelsea Finn,Suraj Nair*

Main category: cs.RO

TL;DR: 本文探讨了能否利用丰富且易获的人类视频数据提升视觉-语言-动作（VLA）模型在机器人泛化任务中的表现。提出了一种联合训练方法，实验证明在足够多样的预训练数据下，可以实现人到机器人的技能迁移，并显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然人类视频数据丰富多样、易于获取，但仅用其训练机器人VLA模型存在难题，如人工设定人机映射困难。受到大语言模型规模效应的启发，作者希望证明随着大规模多样化预训练，人机间的跨体表现能力是否会自然涌现。

Method: 提出了一种简单的VLA联合训练方案：利用大量多样化场景、任务和实体（化身）的数据进行预训练，包括人类和机器人视频数据。

Result: 实验表明，在经过充分多样预训练后，模型能自然实现人-机器人技能迁移。具体而言，在只出现于人类数据的泛化任务上，方法可将机器人表现提升近一倍。

Conclusion: 只要预训练数据足够多样、覆盖广泛，VLA模型能学习到与实体无关的表征，从而实现人的技能在机器人上的迁移。这为利用人类视频提升机器人泛化能力提供了新的思路。

Abstract: Vision-language-action (VLA) models can enable broad open world generalization, but require large and diverse datasets. It is appealing to consider whether some of this data can come from human videos, which cover diverse real-world situations and are easy to obtain. However, it is difficult to train VLAs with human videos alone, and establishing a mapping between humans and robots requires manual engineering and presents a major research challenge. Drawing inspiration from advances in large language models, where the ability to learn from diverse supervision emerges with scale, we ask whether a similar phenomenon holds for VLAs that incorporate human video data. We introduce a simple co-training recipe, and find that human-to-robot transfer emerges once the VLA is pre-trained on sufficient scenes, tasks, and embodiments. Our analysis suggests that this emergent capability arises because diverse pretraining produces embodiment-agnostic representations for human and robot data. We validate these findings through a series of experiments probing human to robot skill transfer and find that with sufficiently diverse robot pre-training our method can nearly double the performance on generalization settings seen only in human data.

</details>


### [250] [Bugs with Features: Vision-Based Fault-Tolerant Collective Motion Inspired by Nature](https://arxiv.org/abs/2512.22448)
*Peleg Shefi,Amir Ayali,Gal A. Kaminka*

Main category: cs.RO

TL;DR: 本文针对集体运动中由于视觉感知局限导致人工群体易碎的问题，提出了两种增强鲁棒性的机制，并通过仿真实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 自然界中的集体运动表现出很强的鲁棒性，但人工机器人群体，尤其以视觉为感知手段时，易因感知模糊与信息损失导致协作失败。为了提升人工群体的可靠性，需要找到灵感来源并设计新的机制。

Method: （1）受蝗虫行为启发，提出一种结合邻居目标横向与纵向尺寸的鲁棒距离估算方法；（2）引入间歇式运动机制，使机器人可更可靠地检测“掉队”或失效个体，并在分类错误时也能避免其影响。

Result: 基于物理仿真的大量实验表明，所提两项机制显著增强了人工群体的鲁棒性。这种提升在不同实验设置下适用于基于距离“回避-吸引”模型和对齐模型。

Conclusion: 受生物启发的新型距离估算及运动机制，能在多种仿真环境中显著提升人工机器人群体的集体运动鲁棒性，对群体协同控制有重要意义。

Abstract: In collective motion, perceptually-limited individuals move in an ordered manner, without centralized control. The perception of each individual is highly localized, as is its ability to interact with others. While natural collective motion is robust, most artificial swarms are brittle. This particularly occurs when vision is used as the sensing modality, due to ambiguities and information-loss inherent in visual perception. This paper presents mechanisms for robust collective motion inspired by studies of locusts. First, we develop a robust distance estimation method that combines visually perceived horizontal and vertical sizes of neighbors. Second, we introduce intermittent locomotion as a mechanism that allows robots to reliably detect peers that fail to keep up, and disrupt the motion of the swarm. We show how such faulty robots can be avoided in a manner that is robust to errors in classifying them as faulty. Through extensive physics-based simulation experiments, we show dramatic improvements to swarm resilience when using these techniques. We show these are relevant to both distance-based Avoid-Attract models, as well as to models relying on Alignment, in a wide range of experiment settings.

</details>


### [251] [Asymmetric Friction in Geometric Locomotion](https://arxiv.org/abs/2512.22484)
*Ross L. Hatton,Yousef Salaman,Shai Revzen*

Main category: cs.RO

TL;DR: 本论文扩展了传统基于Riemann几何的运动模型，将其应用到摩擦力具有各向异性和非对称性的系统，并引入了Finsler几何以描述更复杂的运动机制。


<details>
  <summary>Details</summary>
Motivation: 传统运动几何模型多关注摩擦力各向同性或各向异性的情况，但真实世界里的机器人和动物常常遇到更加复杂、摩擦力分布非对称的环境，现有方法对此难以刻画。作者希望建立一种能完整描述这类系统的新理论框架。

Method: 作者将系统中个体部件的Riemann度量推广为Finsler度量，从而捕捉更加一般化的摩擦特性（既允许各向异性，也允许正/反向运动摩擦不同），并基于此建立了sub-Finslerian方法进行motility map构建，类比并拓展了sub-Riemannian方法。

Result: 作者证明了sub-Riemannian构建motility map的方法可以自然地拓展到sub-Finslerian框架，并识别了与Riemannian系统中‘constraint curvature’类似的关键系统性质，可用于分析和描述此类系统的运动能力。

Conclusion: 本研究为描述和分析复杂摩擦环境下的机器人和动物运动提供了新的理论工具，能够覆盖更广泛的实际应用场景，丰富了几何运动学的理论体系。

Abstract: Geometric mechanics models of locomotion have provided insight into how robots and animals use environmental interactions to convert internal shape changes into displacement through the world, encoding this relationship in a ``motility map''. A key class of such motility maps arises from (possibly anisotropic) linear drag acting on the system's individual body parts, formally described via Riemannian metrics on the motions of the system's individual body parts. The motility map can then be generated by invoking a sub-Riemannian constraint on the aggregate system motion under which the position velocity induced by a given shape velocity is that which minimizes the power dissipated via friction. The locomotion of such systems is ``geometric'' in the sense that the final position reached by the system depends only on the sequence of shapes that the system passes through, but not on the rate with which the shape changes are made.
  In this paper, we consider a far more general class of systems in which the drag may be not only anisotropic (with different coefficients for forward/backward and left/right motions), but also asymmetric (with different coefficients for forward and backward motions). Formally, including asymmetry in the friction replaces the Riemannian metrics on the body parts with Finsler metrics. We demonstrate that the sub-Riemannian approach to constructing the system motility map extends naturally to a sub-Finslerian approach and identify system properties analogous to the constraint curvature of sub-Riemannian systems that allow for the characterization of the system motion capabilities.

</details>


### [252] [Topology-Preserving Scalar Field Optimization for Boundary-Conforming Spiral Toolpaths on Multiply Connected Freeform Surfaces](https://arxiv.org/abs/2512.22502)
*Shen Changqing,Xu Bingzhou,Qi Bosong,Zhang Xiaojian,Yan Sijie,Ding Han*

Main category: cs.RO

TL;DR: 本文提出了一种用于多连通自由曲面球头铣削路径规划的新方法，在优化过程中通过共形裂口映射生成无奇异的初始标量场，并采用拓扑保持的网格变形优化，有效保证路径连续、服从边界且无自交，在加工效率和质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多连通自由曲面在汽车和航空航天制造中广泛应用，对其进行高效高质量的球头铣削路径规划十分重要。现有的基于标量场优化的方法难以兼顾边界一致性和消除会导致路径中断的零梯度奇异点，这在多连通曲面上尤为突出，因此亟需一种能有效解决上述难题的新策略。

Method: 采用共形裂口映射为多连通曲面构造无奇异的初始标量场，并将标量场优化问题重新表述为一种拓扑保持的网格变形，通过同步的边界约束更新确保路径的全局优化间距、均匀的残留高度和光滑的轨迹过渡。

Result: 实验表明，与先进的共形裂口映射方法相比，本文方法在加工效率上提升了14.24%；加工表面残留高度均匀性提升了5.70%；铣削过程中因冲击产生的振动减少了10%以上。

Conclusion: 本文提出的路径规划方法能在保证路径连续性和边界一致性的同时，有效提升多连通自由曲面铣削的效率和表面质量，具备在高性能加工领域广泛应用的潜力。

Abstract: Ball-end milling path planning on multiply connected freeform surfaces is pivotal for high-quality and efficient machining of components in automotive and aerospace manufacturing. Although scalar-field-based optimization provides a unified framework for multi-objective toolpath generation, maintaining boundary conformity while eliminating zero-gradient singularities that cause iso-curve branching or termination and disrupt toolpath continuity remains challenging on multiply connected surfaces. We propose an efficient strategy to robustly enforce these constraints throughout optimization. Conformal slit mapping is employed to construct a feasible, singularity-free initial scalar field. The optimization is reformulated as a topology-preserving mesh deformation governed by boundary-synchronous updates, enabling globally optimized spacing, scallop-height uniformity, and smooth trajectory transitions. Consequently, the toolpaths are continuous, boundary-conforming, and free of self-intersections. Milling experiments demonstrate that, compared with a state-of-the-art conformal slit mapping-based method, the proposed approach increases machining efficiency by 14.24%, improves scallop-height uniformity by 5.70%, and reduces milling impact-induced vibrations by over 10%. The strategy offers broad applicability in high-performance machining scenarios.

</details>


### [253] [Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding](https://arxiv.org/abs/2512.22519)
*Khoa Vo,Taisei Hanyu,Yuki Ikebe,Trong Thang Pham,Nhat Chung,Minh Nhat Vu,Duy Nguyen Ho Minh,Anh Nguyen,Anthony Gunderman,Chase Rainwater,Ngan Le*

Main category: cs.RO

TL;DR: 该论文提出了一种新型的视觉-语言-动作（VLA）模型OBEYED-VLA，通过将感知和动作推理解耦，显著提升了机器人在复杂环境下的操控泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型往往将感知和动作控制紧密结合，仅为动作优化，导致模型容易受干扰物、目标缺失及背景变化影响，降低了实际操控的准确性和泛化能力。针对这些问题，作者希望提高VLA模型在现实场景下的鲁棒性和泛化能力。

Method: OBEYED-VLA框架在传统VLA模型基础上增加了一个感知模块，将多视角输入转化为面向任务的、以物体为中心并具备几何意识的观测结果。感知模块包括基于大模型的对象区域语义定位和3D几何结构强调两个阶段。随后，这些处理后的观测信息被送入预训练的VLA策略模型，并在无杂物、无非目标物体的演示数据上做微调。

Result: 在真实UR10e机械臂实验平台和多种富有挑战性的任务下（如干扰物、目标缺失、背景变化及复杂杂物环境等），OBEYED-VLA表现优于多个强VLA基线，展现出显著的鲁棒性提升。消融实验进一步证实了语义和几何双重感知解耦的必要性。

Conclusion: 将感知模块显式解耦，并使其具有对象中心和几何感知能力，是提升VLA机器人操控模型泛化性和可靠性的有效方法。

Abstract: Recent Vision-Language-Action (VLA) models have made impressive progress toward general-purpose robotic manipulation by post-training large Vision-Language Models (VLMs) for action prediction. Yet most VLAs entangle perception and control in a monolithic pipeline optimized purely for action, which can erode language-conditioned grounding. In our real-world tabletop tests, policies over-grasp when the target is absent, are distracted by clutter, and overfit to background appearance.
  To address these issues, we propose OBEYED-VLA (OBject-centric and gEometrY groundED VLA), a framework that explicitly disentangles perceptual grounding from action reasoning. Instead of operating directly on raw RGB, OBEYED-VLA augments VLAs with a perception module that grounds multi-view inputs into task-conditioned, object-centric, and geometry-aware observations. This module includes a VLM-based object-centric grounding stage that selects task-relevant object regions across camera views, along with a complementary geometric grounding stage that emphasizes the 3D structure of these objects over their appearance. The resulting grounded views are then fed to a pretrained VLA policy, which we fine-tune exclusively on single-object demonstrations collected without environmental clutter or non-target objects.
  On a real-world UR10e tabletop setup, OBEYED-VLA substantially improves robustness over strong VLA baselines across four challenging regimes and multiple difficulty levels: distractor objects, absent-target rejection, background appearance changes, and cluttered manipulation of unseen objects. Ablation studies confirm that both semantic grounding and geometry-aware grounding are critical to these gains. Overall, the results indicate that making perception an explicit, object-centric component is an effective way to strengthen and generalize VLA-based robotic manipulation.

</details>


### [254] [VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models](https://arxiv.org/abs/2512.22539)
*Borong Zhang,Jiahao Li,Jiachen Shen,Yishuai Cai,Yuhao Zhang,Yuanpei Chen,Juntao Dai,Jiaming Ji,Yaodong Yang*

Main category: cs.RO

TL;DR: 本文提出了一个面向机器人视觉-语言-行动（VLA）模型的全新基准——VLA-Arena，用以定量化分析VLA模型的极限和失败模式。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在通用机器人策略方面发展迅速，但缺乏系统量化分析模型能力上限和弱点的标准化工具。作者希望填补该领域定量评测的空白。

Method: 作者提出了结构化任务设计框架，通过任务结构、语言命令和视觉观察三大独立维度系统性构建任务，并细分多个难度等级。共设计170个任务，并可叠加语言和视觉干扰，便于独立分析鲁棒性。此外，作者只在最低难度（L0）进行微调，以考察泛化能力。提供全流程的工具链和公开数据集以支持复现。

Result: 在VLA-Arena基准下，SOTA级VLA模型表现出严重的记忆化而非泛化、鲁棒性不均衡、缺乏安全性约束考虑以及难以组合技能解决长时任务等关键缺陷。

Conclusion: VLA-Arena作为首个多维细粒度难度可控的VLA模型评测基准，为系统提升模型泛化性和鲁棒性提供研究平台。作者开放了全部工具、数据和排行榜，推动领域进一步发展。

Abstract: While Vision-Language-Action models (VLAs) are rapidly advancing towards generalist robot policies, it remains difficult to quantitatively understand their limits and failure modes. To address this, we introduce a comprehensive benchmark called VLA-Arena. We propose a novel structured task design framework to quantify difficulty across three orthogonal axes: (1) Task Structure, (2) Language Command, and (3) Visual Observation. This allows us to systematically design tasks with fine-grained difficulty levels, enabling a precise measurement of model capability frontiers. For Task Structure, VLA-Arena's 170 tasks are grouped into four dimensions: Safety, Distractor, Extrapolation, and Long Horizon. Each task is designed with three difficulty levels (L0-L2), with fine-tuning performed exclusively on L0 to assess general capability. Orthogonal to this, language (W0-W4) and visual (V0-V4) perturbations can be applied to any task to enable a decoupled analysis of robustness. Our extensive evaluation of state-of-the-art VLAs reveals several critical limitations, including a strong tendency toward memorization over generalization, asymmetric robustness, a lack of consideration for safety constraints, and an inability to compose learned skills for long-horizon tasks. To foster research addressing these challenges and ensure reproducibility, we provide the complete VLA-Arena framework, including an end-to-end toolchain from task definition to automated evaluation and the VLA-Arena-S/M/L datasets for fine-tuning. Our benchmark, data, models, and leaderboard are available at https://vla-arena.github.io.

</details>


### [255] [ParaMaP: Parallel Mapping and Collision-free Motion Planning for Reactive Robot Manipulation](https://arxiv.org/abs/2512.22575)
*Xuewei Zhang,Bailing Tian,Kai Zheng,Yulin Hui,Junjie Lu,Zhiyu Li*

Main category: cs.RO

TL;DR: 论文提出了一种结合并行地图构建与运动规划的新框架，能在未知环境下高频率地实现避碰和目标达成，验证结果良好。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中，机器人操作需要实时感知和频繁重新规划路径，保证运动安全和高效，但现有方法在速度和避碰准确性上存在挑战。

Method: 作者集成了基于欧式距离变换的环境建模与采样型模型预测控制（SMPC）规划。地图构建利用GPU加速的距离场及机器人掩码更新机制，防止自碰撞误判；运动规划将生成运动表述为随机优化问题，在统一目标函数下，通过SMPC并行评估大批候选动作，融合SE(3)上的姿态度量，实现高效收敛至目标。整个流程均基于GPU实现，保证高频重规划。

Result: 框架在7自由度机械臂上经过大量仿真与真实实验验证，展现了高效、实时的避碰与目标追踪能力。

Conclusion: 所提出的并行地图与运动规划框架显著提升了未知环境下机器人的实时运动规划与避碰能力，适用于高动态任务场景。

Abstract: Real-time and collision-free motion planning remains challenging for robotic manipulation in unknown environments due to continuous perception updates and the need for frequent online replanning. To address these challenges, we propose a parallel mapping and motion planning framework that tightly integrates Euclidean Distance Transform (EDT)-based environment representation with a sampling-based model predictive control (SMPC) planner. On the mapping side, a dense distance-field-based representation is constructed using a GPU-based EDT and augmented with a robot-masked update mechanism to prevent false self-collision detections during online perception. On the planning side, motion generation is formulated as a stochastic optimization problem with a unified objective function and efficiently solved by evaluating large batches of candidate rollouts in parallel within a SMPC framework, in which a geometrically consistent pose tracking metric defined on SE(3) is incorporated to ensure fast and accurate convergence to the target pose. The entire mapping and planning pipeline is implemented on the GPU to support high-frequency replanning. The effectiveness of the proposed framework is validated through extensive simulations and real-world experiments on a 7-DoF robotic manipulator. More details are available at: https://zxw610.github.io/ParaMaP.

</details>


### [256] [Modeling of UAV Tether Aerodynamics for Real-Time Simulation](https://arxiv.org/abs/2512.22588)
*Max Beffert,Andreas Zell*

Main category: cs.RO

TL;DR: 本论文提出了两种用于实时模拟带有气动力的系留无人机绳子的高效方法，并在实际测试中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 多旋翼无人机的续航受限于电池容量，通过地面供电的系留方式可实现持续作业。随着应用场景扩展到高速移动或强风环境，绳索的气动力影响不容忽视，需高效且准确的建模方法以支撑无人机的控制与规划。

Method: 提出两种互补的建模方法：1）基于悬链线理论与均匀阻力假设的解析法，计算速度极快（<1ms）；2）将绳索离散为分段与集总质量，采用CasADi和IPOPT数值求解平衡方程，可适应更复杂的力学模型，优化初始策略实现5ms内实时计算。

Result: 两种方法均通过实物加载传感实测绳索拉力进行验证。解析法具备极低算力消耗而准确度已满足多数应用需求，数值法则在复杂场景下提供更高建模灵活性与物理精确度。

Conclusion: 论文构建了一个轻量级、可拓展的实时系留绳索仿真框架，能适用于离线优化、在线仿真、控制与轨迹规划等多种系留无人机应用场景。

Abstract: One of the main limitations of multirotor UAVs is their short flight time due to battery constraints. A practical solution for continuous operation is to power the drone from the ground via a tether. While this approach has been demonstrated for stationary systems, scenarios with a fast-moving base vehicle or strong wind conditions require modeling the tether forces, including aerodynamic effects. In this work, we propose two complementary approaches for real-time quasi-static tether modeling with aerodynamics. The first is an analytical method based on catenary theory with a uniform drag assumption, achieving very fast solve times below 1ms. The second is a numerical method that discretizes the tether into segments and lumped masses, solving the equilibrium equations using CasADi and IPOPT. By leveraging initialization strategies, such as warm starting and analytical initialization, real-time performance was achieved with a solve time of 5ms, while allowing for flexible force formulations. Both approaches were validated in real-world tests using a load cell to measure the tether force. The results show that the analytical method provides sufficient accuracy for most tethered UAV applications with minimal computational cost, while the numerical method offers higher flexibility and physical accuracy when required. These approaches form a lightweight and extensible framework for real-time tether simulation, applicable to both offline optimization and online tasks such as simulation, control, and trajectory planning.

</details>


### [257] [Sistema de navegación de cobertura para vehículos no holonómicos en ambientes de exterior](https://arxiv.org/abs/2512.22734)
*Michelle Valenzuela,Francisco Leiva,Javier Ruiz-del-Solar*

Main category: cs.RO

TL;DR: 本论文提出了一种用于非完整性机器人（如轮式移动机器人）覆盖导航的系统，其核心在于实现对指定区域的高效覆盖，并能处理动态或未知障碍的恢复行为。实验显示，该系统在大多数测试中实现了近90%的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 在采矿等行业，诸多过程（如清洁、废料处理、尾矿坝构建）需要对一定区域进行全覆盖作业，人工操作存在安全风险。因此，自动化的覆盖导航技术对于提升作业安全性与效率具有重要意义。

Method: 本文设计并实现了一个覆盖导航系统，包括路径规划算法，使机器人能够高效覆盖指定区域，并加入了遇到动态或未知障碍时的恢复机制，包括躲避与重新覆盖。该系统在仿真与真实室外环境进行了实验，并与通用移动机器人平台结合。

Result: 在不同的仿真和实际户外环境测试中，该系统大多数情况下实现了约90%的区域覆盖率，表现出良好的适用性和鲁棒性。

Conclusion: 本系统作为覆盖导航自动化的概念验证，证明了其在非完整性机器人上应用的可行性。下一步将拓展到采矿机械的真实环境应用，进一步提升工业作业的智能化和安全性。

Abstract: In mobile robotics, coverage navigation refers to the deliberate movement of a robot with the purpose of covering a certain area or volume. Performing this task properly is fundamental for the execution of several activities, for instance, cleaning a facility with a robotic vacuum cleaner. In the mining industry, it is required to perform coverage in several unit processes related with material movement using industrial machinery, for example, in cleaning tasks, in dumps, and in the construction of tailings dam walls. The automation of these processes is fundamental to enhance the security associated with their execution. In this work, a coverage navigation system for a non-holonomic robot is presented. This work is intended to be a proof of concept for the potential automation of various unit processes that require coverage navigation like the ones mentioned before. The developed system includes the calculation of routes that allow a mobile platform to cover a specific area, and incorporates recovery behaviors in case that an unforeseen event occurs, such as the arising of dynamic or previously unmapped obstacles in the terrain to be covered, e.g., other machines or pedestrians passing through the area, being able to perform evasive maneuvers and post-recovery to ensure a complete coverage of the terrain. The system was tested in different simulated and real outdoor environments, obtaining results near 90% of coverage in the majority of experiments. The next step of development is to scale up the utilized robot to a mining machine/vehicle whose operation will be validated in a real environment. The result of one of the tests performed in the real world can be seen in the video available in https://youtu.be/gK7_3bK1P5g.

</details>


### [258] [Active Constraint Learning in High Dimensions from Demonstrations](https://arxiv.org/abs/2512.22757)
*Zheng Qiu,Chih-Yuan Chiu,Glen Chou*

Main category: cs.RO

TL;DR: 提出了一种用于约束学习的主动学习算法，通过智能选择演示轨迹，更高效地从示范中推断未知环境约束。


<details>
  <summary>Details</summary>
Motivation: 在从演示学习的任务中，如何高效推断演示者环境中的未知约束条件是一个难点。传统方法常常随机采样，效率低下。因此，亟需一种能够通过主动选择示范，提高约束推断效率和准确性的算法。

Method: 采用高斯过程（GP）对已有演示数据进行建模，通过GP后验信息设计策略，主动选择起终点以生成更有信息量的新演示，并持续迭代训练提升约束学习效果。

Result: 在高维、非线性动力学及未知非线性约束的仿真和实际硬件实验中，该方法在约束推断的准确性方面显著优于基线的随机采样方法。

Conclusion: 所提方法能够通过主动学习机制，利用稀疏但高信息量的演示数据，更高效、准确地从演示中推断未知约束，在实际复杂系统中展示了其有效性。

Abstract: We present an iterative active constraint learning (ACL) algorithm, within the learning from demonstrations (LfD) paradigm, which intelligently solicits informative demonstration trajectories for inferring an unknown constraint in the demonstrator's environment. Our approach iteratively trains a Gaussian process (GP) on the available demonstration dataset to represent the unknown constraints, uses the resulting GP posterior to query start/goal states, and generates informative demonstrations which are added to the dataset. Across simulation and hardware experiments using high-dimensional nonlinear dynamics and unknown nonlinear constraints, our method outperforms a baseline, random-sampling based method at accurately performing constraint inference from an iteratively generated set of sparse but informative demonstrations.

</details>


### [259] [Two-Robot Computational Landscape: A Complete Characterization of Model Power in Minimal Mobile Robot Systems](https://arxiv.org/abs/2512.22770)
*Naoki Kitamura,Yuichi Sudo,Koichi Wada*

Main category: cs.RO

TL;DR: 本论文首次完整刻画了两台自主移动机器人在所有主流模型和调度器下的计算能力层级，揭示了该场景与多机器人情况截然不同的本质特性。


<details>
  <summary>Details</summary>
Motivation: 尽管 n 机器人的计算能力结构已基本明确，但对于只有两台机器人的详细计算能力体系结构依然未解，这对于理解最小规模机器人协同的本质挑战至关重要。

Method: 本文在 OBLOT、FSTA、FCOM、LUMI 等所有主要机器人模型下，结合 FSYNCH、SSYNCH、ASYNCH 及其 atomic 变体调度器，采用一种新颖的无模拟方法，系统性地推导出两机器人的能力层级，包括等价性和不可比性结果。

Result: 在完美同步下，FSTA^F 和 LUMI^F 能力完全重合，说明对两机器人而言，完全同步可替代记忆和通信。此外，发现 FSTA 和 FCOM 彼此正交，存在问题可用最弱通信模型解决，但即使最强有限状态模型也无法解决。所有等价与分离均以无模拟方式获得。

Conclusion: 论文给出了两机器人所有主要模型和调度下的完整精确计算能力图谱，揭示了在最小规模下多主体协作的独特难点，丰富了机器人计算理论的基础理解。

Abstract: The computational power of autonomous mobile robots under the Look-Compute-Move (LCM) model has been widely studied through an extensive hierarchy of robot models defined by the presence of memory, communication, and synchrony assumptions. While the general n-robot landscape has been largely established, the exact structure for two robots has remained unresolved. This paper presents the first complete characterization of the computational power of two autonomous robots across all major models, namely OBLOT, FSTA, FCOM, and LUMI, under the full spectrum of schedulers (FSYNCH, SSYNCH, ASYNCH, and their atomic variants). Our results reveal a landscape that fundamentally differs from the general case. Most notably, we prove that FSTA^F and LUMI^F coincide under full synchrony, a surprising collapse indicating that perfect synchrony can substitute both memory and communication when only two robots exist. We also show that FSTA and FCOM are orthogonal: there exists a problem solvable in the weakest communication model but impossible even in the strongest finite-state model, completing the bidirectional incomparability. All equivalence and separation results are derived through a novel simulation-free method, providing a unified and constructive view of the two-robot hierarchy. This yields the first complete and exact computational landscape for two robots, highlighting the intrinsic challenges of coordination at the minimal scale.

</details>


### [260] [The body is not there to compute: Comment on "Informational embodiment: Computational role of information structure in codes and robots" by Pitti et al](https://arxiv.org/abs/2512.22868)
*Matej Hoffmann*

Main category: cs.RO

TL;DR: 作者评论指出，尽管以信息与计算视角研究身体（动物与机器人）在理论上具有启发性，但身体的主要功能不是计算。


<details>
  <summary>Details</summary>
Motivation: 当前许多研究将计算与信息理论应用于理解和设计动物与机器人身体，试图用其解释身体的进化和优化原理。本文作者意在质疑这一主流观点：身体的核心作用究竟是不是计算。

Method: 本文通过对目标论文的批判性评论，分析信息及计算理论在解释动物和机器人身体功能中的适用性与局限性。

Result: 作者得出结论：身体在生物学和机器人中的主要作用并非计算。

Conclusion: 建议对身体的理解应超越计算和信息处理框架，重视身体本身的独特作用和意义。

Abstract: Applying the lens of computation and information has been instrumental in driving the technological progress of our civilization as well as in empowering our understanding of the world around us. The digital computer was and for many still is the leading metaphor for how our mind operates. Information theory (IT) has also been important in our understanding of how nervous systems encode and process information. The target article deploys information and computation to bodies: to understand why they have evolved in particular ways (animal bodies) and to design optimal bodies (robots). In this commentary, I argue that the main role of bodies is not to compute.

</details>


### [261] [P-FABRIK: A General Intuitive and Robust Inverse Kinematics Method for Parallel Mechanisms Using FABRIK Approach](https://arxiv.org/abs/2512.22927)
*Daqian Cao,Quan Yuan,Weibang Bai*

Main category: cs.RO

TL;DR: 本文提出了P-FABRIK算法，一种适用于多种并联机构、具有高通用性和鲁棒性的逆运动学求解方法。通过将并联机构分解为多个串联子链，并采用新的拓扑分解策略，方法可以高效求解包括冗余机构在内的逆运动学问题，并能处理目标超出工作空间的情况。


<details>
  <summary>Details</summary>
Motivation: 传统的几何逆运动学方法依赖于特定的空间几何约束，难以用于冗余并联机构，且面对目标点超出可达工作空间会无法求解，影响控制的可预测性。因此，需要一种更通用、更鲁棒的逆运动学方法。

Method: 提出了一种基于FABRIK算法的P-FABRIK方法，将一般并联机构通过新的拓扑分解方式转化为多个串联子链，各子链末端目标迭代矫正，并用迭代法求解逆运动学。

Result: 通过多个案例（包括平面、标准和冗余型并联机构）验证了方法的通用性与有效性；数值仿真表明该方法具有较好的效率和计算能力，并在目标超出工作空间时能够保持鲁棒性。

Conclusion: P-FABRIK方法可作为一种通用、直观和鲁棒的逆运动学工具，适用于多种并联机构的逆运动学快速求解，尤其适合冗余与复杂约束场景，对提升并联机构控制的可靠性和实用性意义重大。

Abstract: Traditional geometric inverse kinematics methods for parallel mechanisms rely on specific spatial geometry constraints. However, their application to redundant parallel mechanisms is challenged due to the increased constraint complexity. Moreover, it will output no solutions and cause unpredictable control problems when the target pose lies outside its workspace. To tackle these challenging issues, this work proposes P-FABRIK, a general, intuitive, and robust inverse kinematics method to find one feasible solution for diverse parallel mechanisms based on the FABRIK algorithm. By decomposing the general parallel mechanism into multiple serial sub-chains using a new topological decomposition strategy, the end targets of each sub-chain can be subsequently revised to calculate the inverse kinematics solutions iteratively. Multiple case studies involving planar, standard, and redundant parallel mechanisms demonstrated the proposed method's generality across diverse parallel mechanisms. Furthermore, numerical simulation studies verified its efficacy and computational efficiency, as well as its robustness ability to handle out-of-workspace targets.

</details>


### [262] [PreGME: Prescribed Performance Control of Aerial Manipulators based on Variable-Gain ESO](https://arxiv.org/abs/2512.22957)
*Mengyu Ji,Shiliang Guo,Zhengzhen Li,Jiahao Shen,Huazi Cao,Shiyu Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种针对多旋翼飞行器与机械臂动态耦合问题的新型运动控制框架，通过引入变增益扩展状态观测器（ESO）和预设误差约束，实现了对空中操作系统的高精度、鲁棒控制。此外，通过真实平台实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 多旋翼与机械臂组成的空中操作器存在显著的动态耦合，传统运动控制方法难以保障精确与鲁棒性。因此，亟需一种能实时应对动态变化并限制误差的高性能控制策略。

Method: 提出了一种基于变增益ESO的预设性能运动控制方法（PreGME）。该方法通过变增益ESO实时估算动态耦合，实现对快速变化动态的高精度补偿，同时通过引入误差轨迹约束，保证了运动误差始终在预期范围内。

Result: 在真实平台上进行了包括空中挥杖、空中调酒和空中拉车等实验。即使机械臂末端速度达到1.02m/s、加速度达5.10m/s²的剧烈运动条件下，所提方法依然保持了很好的轨迹跟踪性能。

Conclusion: 文中控制方法能够有效估算和补偿多旋翼-机械臂系统中的强动态耦合，实现高精度、具预设性能的运动控制，对复杂空中操作任务有良好应用前景。

Abstract: An aerial manipulator, comprising a multirotor base and a robotic arm, is subject to significant dynamic coupling between these two components. Therefore, achieving precise and robust motion control is a challenging yet important objective. Here, we propose a novel prescribed performance motion control framework based on variable-gain extended state observers (ESOs), referred to as PreGME. The method includes variable-gain ESOs for real-time estimation of dynamic coupling and a prescribed performance flight control that incorporates error trajectory constraints. Compared with existing methods, the proposed approach exhibits the following two characteristics. First, the adopted variable-gain ESOs can accurately estimate rapidly varying dynamic coupling. This enables the proposed method to handle manipulation tasks that require aggressive motion of the robotic arm. Second, by prescribing the performance, a preset error trajectory is generated to guide the system evolution along this trajectory. This strategy allows the proposed method to ensure the tracking error remains within the prescribed performance envelope, thereby achieving high-precision control. Experiments on a real platform, including aerial staff twirling, aerial mixology, and aerial cart-pulling experiments, are conducted to validate the effectiveness of the proposed method.
  Experimental results demonstrate that even under the dynamic coupling caused by rapid robotic arm motion (end-effector velocity: 1.02 m/s, acceleration: 5.10 m/s$^2$), the proposed method achieves high tracking performance.

</details>


### [263] [Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives](https://arxiv.org/abs/2512.22983)
*Shuanghao Bai,Wenxuan Song,Jiayi Chen,Yuheng Ji,Zhide Zhong,Jin Yang,Han Zhao,Wanqi Zhou,Zhe Li,Pengxiang Ding,Cheng Chi,Chang Xu,Xiaolong Zheng,Donglin Wang,Haoang Li,Shanghang Zhang,Badong Chen*

Main category: cs.RO

TL;DR: 本文综述了机器人操作领域中基于学习的方法，提出了一种统一的高层规划与低层控制抽象来梳理现有方法，并探讨了开放挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着视觉、语言、多模态学习的进步，基础模型在机器人领域得到了显著发展，但机器人操作仍是一个核心且具有挑战性的问题。因此，有必要对现有学习方法进行系统梳理和分类。

Method: 从算法角度，作者将机器人操作方法分为高层任务规划和低层控制两部分。高层涵盖了语言、代码、动作、可供性以及3D表示等在复杂任务决策中的作用。低层则按训练范式划分，对输入建模、潜变量表示学习及策略学习等进行分类和对比。

Result: 本文提出了高层与低层的统一抽象框架，并制定了一套低层控制的分类方法，对现有算法进行了全面整理。同时，总结了当前面临的扩展性、数据效率、多模态物理交互与安全等问题。

Conclusion: 本综述有助于厘清机器人操作领域基础模型的设计空间，为后续研究指明了挑战和方向。

Abstract: Recent advances in vision, language, and multimodal learning have substantially accelerated progress in robotic foundation models, with robot manipulation remaining a central and challenging problem. This survey examines robot manipulation from an algorithmic perspective and organizes recent learning-based approaches within a unified abstraction of high-level planning and low-level control. At the high level, we extend the classical notion of task planning to include reasoning over language, code, motion, affordances, and 3D representations, emphasizing their role in structured and long-horizon decision making. At the low level, we propose a training-paradigm-oriented taxonomy for learning-based control, organizing existing methods along input modeling, latent representation learning, and policy learning. Finally, we identify open challenges and prospective research directions related to scalability, data efficiency, multimodal physical interaction, and safety. Together, these analyses aim to clarify the design space of modern foundation models for robotic manipulation.

</details>


### [264] [Embodied Learning of Reward for Musculoskeletal Control with Vision Language Models](https://arxiv.org/abs/2512.23077)
*Saraswati Soedarmadji,Yunyue Wei,Chen Zhang,Yisong Yue,Yanan Sui*

Main category: cs.RO

TL;DR: 本文提出了MoVLR框架，利用视觉-语言模型（VLM）将高层次运动目标与具体运动控制联系起来，实现了无需手工设计奖励，自动探索与优化高维肌肉骨骼系统的奖励函数。


<details>
  <summary>Details</summary>
Motivation: 高维肌肉骨骼系统的运动控制中，设计有效的奖励函数一直是难题。虽然人类能明确描述动作目标，但实际控制策略多为隐含，难以凭语言等高层表达直接设计奖励。

Method: 提出了一种名为MoVLR的新框架，通过控制优化与视觉-语言模型的反馈迭代探索奖励空间，将语言和视觉评估转化为具体的学习指导，以推动控制策略向物理协调动作对齐。

Result: MoVLR实现了对高维肌肉骨骼运动和操作任务奖励函数的发现和优化，展现了通过VLM有效将抽象运动描述落地为具体生理运动控制原则的能力。

Conclusion: 视觉-语言模型能够有效将高层运动目标与隐含运动控制策略联系，为生理运动控制中的奖励函数设计和策略学习提供了新方法和可能。

Abstract: Discovering effective reward functions remains a fundamental challenge in motor control of high-dimensional musculoskeletal systems. While humans can describe movement goals explicitly such as "walking forward with an upright posture," the underlying control strategies that realize these goals are largely implicit, making it difficult to directly design rewards from high-level goals and natural language descriptions. We introduce Motion from Vision-Language Representation (MoVLR), a framework that leverages vision-language models (VLMs) to bridge the gap between goal specification and movement control. Rather than relying on handcrafted rewards, MoVLR iteratively explores the reward space through iterative interaction between control optimization and VLM feedback, aligning control policies with physically coordinated behaviors. Our approach transforms language and vision-based assessments into structured guidance for embodied learning, enabling the discovery and refinement of reward functions for high-dimensional musculoskeletal locomotion and manipulation. These results suggest that VLMs can effectively ground abstract motion descriptions in the implicit principles governing physiological motor control.

</details>


### [265] [APOLLO Blender: A Robotics Library for Visualization and Animation in Blender](https://arxiv.org/abs/2512.23103)
*Peter Messina,Daniel Rakita*

Main category: cs.RO

TL;DR: 本文介绍了一个专为机器人研究者设计的轻量级可视化软件库，简化Blender在机器人可视化中的使用。


<details>
  <summary>Details</summary>
Motivation: 现有的3D可视化工具（如Blender）虽功能强大，但对机器人研究者不够友好，学习成本高、缺乏相关集成。研究者急需操作简便、面向机器人应用的可视化工具，以提升工作效率。

Method: 提出一个软件库，提供易用的脚本化接口，支持URDF等标准化格式的机器人模型和环境导入、基于Python的关键帧控制以及3D原始图形的生成，从而便于制作图片、动画和结构化示意图。

Result: 该库通过一系列演示实例，验证其可帮助研究者快速生成高质量的可视化成果，无需具备Blender专业技能。

Conclusion: 该库有效降低了机器人可视化门槛，提升科研传播效率。作者还讨论了当前局限性及未来的拓展方向。

Abstract: High-quality visualizations are an essential part of robotics research, enabling clear communication of results through figures, animations, and demonstration videos. While Blender is a powerful and freely available 3D graphics platform, its steep learning curve and lack of robotics-focused integrations make it difficult and time-consuming for researchers to use effectively. In this work, we introduce a lightweight software library that bridges this gap by providing simple scripting interfaces for common robotics visualization tasks. The library offers three primary capabilities: (1) importing robots and environments directly from standardized descriptions such as URDF; (2) Python-based scripting tools for keyframing robot states and visual attributes; and (3) convenient generation of primitive 3D shapes for schematic figures and animations. Together, these features allow robotics researchers to rapidly create publication-ready images, animations, and explanatory schematics without needing extensive Blender expertise. We demonstrate the library through a series of proof-of-concept examples and conclude with a discussion of current limitations and opportunities for future extensions.

</details>


### [266] [Beyond URDF: The Universal Robot Description Directory for Shared, Extensible, and Standardized Robot Models](https://arxiv.org/abs/2512.23135)
*Roshan Klein-Seetharaman,Daniel Rakita*

Main category: cs.RO

TL;DR: 提出了一种新的机器人描述标准URDD，以模组化JSON/YAML文件丰富表达机器人信息，解决传统规范文件信息匮乏、重复计算等问题，并提供了开源工具链。


<details>
  <summary>Details</summary>
Motivation: 现有机器人描述文件（如URDF等）只包含基础信息，导致下游应用需重复推导并实现各种功能，效率低下且难以标准化。

Method: 设计了URDD格式，将丰富派生信息分模块存储，并可自动由URDF生成；实现了Rust和JavaScript工具以支持可视化和Web端展示。

Result: 实验证明URDD可高效生成，比标准文件包含更丰富信息，并可直接驱动机器人关键子程序。

Conclusion: URDD有助于减少冗余、推动标准化，是机器人软件开发的重要资源，但也有局限和需进一步探讨的影响。

Abstract: Robots are typically described in software by specification files (e.g., URDF, SDF, MJCF, USD) that encode only basic kinematic, dynamic, and geometric information. As a result, downstream applications such as simulation, planning, and control must repeatedly re-derive richer data, leading to redundant computations, fragmented implementations, and limited standardization. In this work, we introduce the Universal Robot Description Directory (URDD), a modular representation that organizes derived robot information into structured, easy-to-parse JSON and YAML modules. Our open-source toolkit automatically generates URDDs from URDFs, with a Rust implementation supporting Bevy-based visualization. Additionally, we provide a JavaScript/Three.js viewer for web-based inspection of URDDs. Experiments on multiple robot platforms show that URDDs can be generated efficiently, encapsulate substantially richer information than standard specification files, and directly enable the construction of core robotics subroutines. URDD provides a unified, extensible resource for reducing redundancy and establishing shared standards across robotics frameworks. We conclude with a discussion on the limitations and implications of our work.

</details>


### [267] [A New Software Tool for Generating and Visualizing Robot Self-Collision Matrices](https://arxiv.org/abs/2512.23140)
*Roshan Klein-Seetharama,Daniel Rakita*

Main category: cs.RO

TL;DR: 本文提出了一种新的交互式工具，可高效生成和可视化机器人自碰撞矩阵，支持多种形状表示及动态操作，利于下游应用灵活集成。


<details>
  <summary>Details</summary>
Motivation: 现有自碰撞矩阵生成工具存在静态、缺乏邻近支持、仅支持单一几何体、以及繁琐的优化流程等限制，影响在机器人应用中的效率和可复用性。

Method: 作者开发了一个基于Rust和Bevy引擎的交互式系统，支持对多种形状表示的自碰撞矩阵进行生成、可视化、过滤和快速优化，输出标准JSON和YAML格式方便集成。

Result: 该工具在多个机器人平台上验证，利用多种形状类型生成的自碰撞矩阵大幅提升了自碰撞与自邻近查询的速度及准确率。

Conclusion: 新方法显著提升了自碰撞矩阵生成工具的灵活性和适用性，可促进机器人应用中更高效和准确的碰撞检测流程。

Abstract: In robotics, it is common to check whether a given robot state results in self-intersection (i.e., a self-collision query) or to assess its distance from such an intersection (i.e., a self-proximity query). These checks are typically performed between pairs of shapes attached to different robot links. However, many of these shape pairs can be excluded in advance, as their configurations are known to always or never result in contact. This information is typically encoded in a self-collision matrix, where each entry (i, j) indicates whether a check should be performed between shape i and shape j. While the MoveIt Setup Assistant is widely used to generate such matrices, current tools are limited by static visualization, lack of proximity support, rigid single-geometry assumptions, and tedious refinement workflows, hindering flexibility and reuse in downstream robotics applications. In this work, we introduce an interactive tool that overcomes these limitations by generating and visualizing self-collision matrices across multiple shape representations, enabling dynamic inspection, filtering, and refinement of shape pairs. Outputs are provided in both JSON and YAML for easy integration. The system is implemented in Rust and uses the Bevy game engine to deliver high-quality visualizations. We demonstrate its effectiveness on multiple robot platforms, showing that matrices generated using diverse shape types yield faster and more accurate self-collision and self-proximity queries.

</details>


### [268] [Pole-centric Descriptors for Robust Robot Localization: Evaluation under Pole-at-Distance (PaD) Observations using the Small Pole Landmark (SPL) Dataset](https://arxiv.org/abs/2512.23141)
*Wuhao Xie,Kanji Tanaka*

Main category: cs.RO

TL;DR: 论文针对城市环境中长距离观测下的杆状地标识别问题，提出了专门的数据集和评估框架，并比较了对比学习与监督学习性能。


<details>
  <summary>Details</summary>
Motivation: 在大规模城市环境中，杆状地标由于距离远，当前方法下识别稳定性下降，影响了机器人的长期定位。因此亟需对描述符鲁棒性进行系统性评估，而非单纯优化描述符设计。

Method: 作者建立了Small Pole Landmark (SPL)数据集，通过自动跟踪的方式，无需人工标注，收集了同一地标在多视角、多距离（特别是远距离5~10m）下的观测。并基于此框架对比分析了对比学习（CL）和监督学习（SL）方法在描述符鲁棒性上的表现。

Result: 实验结果表明，对比学习方法相比监督学习能够获得更加鲁棒的特征空间，在5~10米的远距离检索表现更优。

Conclusion: 本工作为复杂现实环境下地标区分能力的评估提供了可扩展的方法和实证基础，对未来长距离、稀疏几何目标的识别具有重要参考意义。

Abstract: While pole-like structures are widely recognized as stable geometric anchors for long-term robot localization, their identification reliability degrades significantly under Pole-at-Distance (Pad) observations typical of large-scale urban environments. This paper shifts the focus from descriptor design to a systematic investigation of descriptor robustness. Our primary contribution is the establishment of a specialized evaluation framework centered on the Small Pole Landmark (SPL) dataset. This dataset is constructed via an automated tracking-based association pipeline that captures multi-view, multi-distance observations of the same physical landmarks without manual annotation. Using this framework, we present a comparative analysis of Contrastive Learning (CL) and Supervised Learning (SL) paradigms. Our findings reveal that CL induces a more robust feature space for sparse geometry, achieving superior retrieval performance particularly in the 5--10m range. This work provides an empirical foundation and a scalable methodology for evaluating landmark distinctiveness in challenging real-world scenarios.

</details>


### [269] [Towards the Automation in the Space Station: Feasibility Study and Ground Tests of a Multi-Limbed Intra-Vehicular Robot](https://arxiv.org/abs/2512.23153)
*Seiko Piotr Yamaguchi,Kentaro Uno,Yasumaru Fujii,Masazumi Imai,Kazuki Takada,Taku Okawara,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文研究了多臂载人舱内机器人（MLIVR）在国际空间站辅助宇航员进行物流任务的可行性，并通过仿真和原型测试验证了其自主运行能力。


<details>
  <summary>Details</summary>
Motivation: 宇航员执行各类后勤任务（如物资准备、清理和搬运）耗费大量时间，减少了对关键任务的投入。因此有必要开发可自主执行这些任务的机器人，以提升空间站的运行效率。

Method: 以多臂移动机械臂为对象，先进行3D空间内的运动规划仿真，再在2D平台上用实际原型进行运动执行测试，模拟微重力环境。主要关注其自主运输能力，并考查机器人的自主性和实时任务执行能力。

Result: 模拟和原型试验表明，该机器人可以在几乎无需人工干预的情况下完成物流运输任务，展示了其实际应用的可行性。

Conclusion: 多臂移动机器人能够有效辅助空间站后勤工作、减少宇航员负担，并有望提升国际空间站的整体运行效率。

Abstract: This paper presents a feasibility study, including simulations and prototype tests, on the autonomous operation of a multi-limbed intra-vehicular robot (mobile manipulator), shortly MLIVR, designed to assist astronauts with logistical tasks on the International Space Station (ISS). Astronauts spend significant time on tasks such as preparation, close-out, and the collection and transportation of goods, reducing the time available for critical mission activities. Our study explores the potential for a mobile manipulator to support these operations, emphasizing the need for autonomous functionality to minimize crew and ground operator effort while enabling real-time task execution. We focused on the robot's transportation capabilities, simulating its motion planning in 3D space. The actual motion execution was tested with a prototype on a 2D table to mimic a microgravity environment. The results demonstrate the feasibility of performing these tasks with minimal human intervention, offering a promising solution to enhance operational efficiency on the ISS.

</details>


### [270] [A Sequential Hermaphrodite Coupling Mechanism for Lattice-based Modular Robots](https://arxiv.org/abs/2512.23154)
*Keigo Torii,Kentaro Uno,Shreya Santra,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本论文提出了一种新型的形状匹配机械耦合机制，可满足模块化机器人在极端环境下构建所需的多项复杂设计要求。


<details>
  <summary>Details</summary>
Motivation: 传统异构结构模块耦合机构难以同时实现单面耦合与解耦、解耦时表面平整，以及与被动接口的兼容性，限制了模块化机器人在空间等极端环境的大规模应用。

Method: 作者提出了一种新型的机械耦合机制，利用结构形状切换实现“雌雄态”转换。未耦合时为雌性，一侧在耦合过程中切换为雄性，实现单面耦合；解耦时可从任意一侧强制切回雌性，达成单面解耦。该机制支持与被动接口和其他耦合机制的兼容。

Result: 该耦合机制能有效实现单面耦合与解耦、解耦面平整，以及多种耦合行为，适用于多类模块化机器人系统与机械手工具更换器。

Conclusion: 提出的新型形状匹配机械耦合机制复杂但实用，可以显著提升模块化机器人在极端环境下的大规模构建能力，兼容性强，有望在未来多种机器人系统中推广应用。

Abstract: Lattice-based modular robot systems are envisioned for large-scale construction in extreme environments, such as space. Coupling mechanisms for heterogeneous structural modules should meet all of the following requirements: single-sided coupling and decoupling, flat surfaces when uncoupled, and coupling to passive coupling interfaces as well as coupling behavior between coupling mechanisms. The design requirements for such a coupling mechanism are complex. We propose a novel shape-matching mechanical coupling mechanism that satisfies these design requirements. This mechanism enables controlled, sequential transitions between male and female states. When uncoupled, all mechanisms are in the female state. To enable single-sided coupling, one side of the mechanisms switches to the male state during the coupling process. Single-sided decoupling is possible not only from the male side but also from the female side by forcibly switching the opposite mechanism's male state to the female state. This coupling mechanism can be applied to various modular robot systems and robot arm tool changers.

</details>


### [271] [SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling](https://arxiv.org/abs/2512.23162)
*Yufan He,Pengfei Guo,Mengya Xu,Zhaoshuo Li,Andriy Myronenko,Dillan Imans,Bingjie Liu,Dongren Yang,Mingxue Gu,Yongnan Ji,Yueming Jin,Ren Zhao,Baiyong Shen,Daguang Xu*

Main category: cs.RO

TL;DR: 本论文提出通过生成和增强数据来突破外科机器人自主操作面临的数据稀缺瓶颈，利用SurgWorld世界模型和合成的动作数据训练VLA策略，实现外科手术机器人的更强泛化能力和有效学习。


<details>
  <summary>Details</summary>
Motivation: 自动化外科机器人发展受限于缺少同时具备视觉与运动学标注的大规模手术数据集。尽管有大量手术视频，但缺乏相应动作标注，无法应用端到端的模仿学习或者视觉-语言-动作模型。为此，作者希望通过生成数据与推测运动参数，充分利用丰富的无标注手术视频资源。

Method: 作者自建了Surgical Action Text Alignment（SATA）数据集，包含详细外科机器人动作文本描述，并基于先进的物理AI世界模型开发了SurgWorld，能够生成多样且贴近真实的手术视频。创新性地应用逆动力学模型，从合成视频中反推出伪运动参数，得到合成的配对视频-动作数据，进而用于训练VLA策略模型。

Result: 实验证明，用上述增广数据训练的外科机器人VLA政策，在真实手术机器人平台上的表现明显优于仅用真实示范数据训练的模型。

Conclusion: 本工作展示了通过无标注手术视频与生成式世界建模，可以大规模、低成本地提升外科机器人自主学习能力，为通用和高效的数据利用打开新局面。

Abstract: Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies.

</details>


### [272] [A Human-Oriented Cooperative Driving Approach: Integrating Driving Intention, State, and Conflict](https://arxiv.org/abs/2512.23220)
*Qin Wang,Shanmin Pang,Jianwu Fang,Shengye Dong,Fuhao Liu,Jianru Xue,Chen Lv*

Main category: cs.RO

TL;DR: 本文提出了一种以人为中心的人-车协同驾驶方法（HOCD），通过优先考虑驾驶员意图和状态以减少人机冲突，并在轨迹规划和控制分配两个层面实现更自然、高效的人-车互动。实验结果显示该方法有效提升了驾驶表现，显著缓解了人机冲突。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶亟需过渡方式来提升驾驶灵活性，并逐步获得驾驶员对自动驾驶技术的信任和接受度。减少传统人-车交互中的冲突是实现顺畅协同的关键。

Method: 方法上，文中提出了HOCD方法，分别在战术（规划）和操作（控制）层面进行改进。战术层面，通过基于意图一致性代价的轨迹规划方法使车辆轨迹更契合驾驶员意图；操作层面，基于强化学习的控制权分配策略，利用奖励函数优化，实现驾驶员状态和控制权分配的一致性。

Result: 仿真和人机闭环实验均表明，该方法在轨迹规划时能更好地贴合驾驶员意图，并在控制权分配上更为合理。与现有方法相比，驾驶表现提升显著，人机冲突大幅减小。

Conclusion: HOCD方法能够在提升驾驶体验的同时，有效调和人和车辆之间的权力分配问题，对人-车协同和自动驾驶过渡具有实际应用意义。

Abstract: Human-vehicle cooperative driving serves as a vital bridge to fully autonomous driving by improving driving flexibility and gradually building driver trust and acceptance of autonomous technology. To establish more natural and effective human-vehicle interaction, we propose a Human-Oriented Cooperative Driving (HOCD) approach that primarily minimizes human-machine conflict by prioritizing driver intention and state. In implementation, we take both tactical and operational levels into account to ensure seamless human-vehicle cooperation. At the tactical level, we design an intention-aware trajectory planning method, using intention consistency cost as the core metric to evaluate the trajectory and align it with driver intention. At the operational level, we develop a control authority allocation strategy based on reinforcement learning, optimizing the policy through a designed reward function to achieve consistency between driver state and authority allocation. The results of simulation and human-in-the-loop experiments demonstrate that our proposed approach not only aligns with driver intention in trajectory planning but also ensures a reasonable authority allocation. Compared to other cooperative driving approaches, the proposed HOCD approach significantly enhances driving performance and mitigates human-machine conflict.The code is available at https://github.com/i-Qin/HOCD.

</details>


### [273] [Beyond Coverage Path Planning: Can UAV Swarms Perfect Scattered Regions Inspections?](https://arxiv.org/abs/2512.23257)
*Socratis Gkelios,Savvas D. Apostolidis,Pavlos Ch. Kapoutsis,Elias B. Kosmatopoulos,Athanasios Ch. Kapoutsis*

Main category: cs.RO

TL;DR: 该论文提出了针对多区域、分散区域无人机巡检任务的高效方法mUDAI，能够优化多无人机巡检效果和效率。


<details>
  <summary>Details</summary>
Motivation: 传统无人机巡检虽然高效安全，但因电池续航有限，且现有路经规划方法面对多个分散检测区域效率低下，因此亟需更适合碎片化目标区巡检的新方案。

Method: 提出Fast Inspection of Scattered Regions (FISR)问题，设计了一种多无人机分离区域巡检（mUDAI）方法，包括两个优化步骤：一是确定最佳拍摄点位，二是规划最高效的无人机飞行路径，平衡数据分辨率与巡检效率，减少冗余和资源消耗。

Result: 通过仿真和实际部署实验证实mUDAI方法大幅提升多无人机巡检分散区域的效率，同时保证高质量的数据采集。

Conclusion: mUDAI方法能够有效支持多无人机对分散目标区的高效巡检，广泛适用于安全巡查、农业及应急等领域，相关开源工具和数据面向社区开放，实用性强。

Abstract: Unmanned Aerial Vehicles (UAVs) have revolutionized inspection tasks by offering a safer, more efficient, and flexible alternative to traditional methods. However, battery limitations often constrain their effectiveness, necessitating the development of optimized flight paths and data collection techniques. While existing approaches like coverage path planning (CPP) ensure comprehensive data collection, they can be inefficient, especially when inspecting multiple non connected Regions of Interest (ROIs). This paper introduces the Fast Inspection of Scattered Regions (FISR) problem and proposes a novel solution, the multi UAV Disjoint Areas Inspection (mUDAI) method. The introduced approach implements a two fold optimization procedure, for calculating the best image capturing positions and the most efficient UAV trajectories, balancing data resolution and operational time, minimizing redundant data collection and resource consumption. The mUDAI method is designed to enable rapid, efficient inspections of scattered ROIs, making it ideal for applications such as security infrastructure assessments, agricultural inspections, and emergency site evaluations. A combination of simulated evaluations and real world deployments is used to validate and quantify the method's ability to improve operational efficiency while preserving high quality data capture, demonstrating its effectiveness in real world operations. An open source Python implementation of the mUDAI method can be found on GitHub (https://github.com/soc12/mUDAI) and the collected and processed data from the real world experiments are all hosted on Zenodo (https://zenodo.org/records/13866483). Finally, this online platform (https://sites.google.com/view/mudai-platform/) allows interested readers to interact with the mUDAI method and generate their own multi UAV FISR missions.

</details>


### [274] [Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants](https://arxiv.org/abs/2512.23312)
*Sheng-Kai Chen,Yi-Ling Tsai,Chun-Chih Chang,Yan-Chen Chen,Po-Chiang Lin*

Main category: cs.RO

TL;DR: 本文提出了一种以可解释性为核心的机器人逆运动学（IK）推理流程，将Shapley值归因与基于物理的避障评估相结合，兼顾了性能与安全透明性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络加速了IK推理，但其“黑箱”特性难以满足负责任AI下对透明度与安全的要求，因此亟需有效的可解释性方法以发现潜在风险并指导改进。

Method: 文章针对ROBOTIS OpenManipulator-X，提出基于Shapley值归因和物理避障结合的分析流程；在原IKNet基础上，训练了改进型（添加残差连接）和聚焦型（位置-姿态解耦）两个轻量化模型，并采用SHAP和InterpretML分析全局和局部的归因分布，随后在模拟环境下通过前向运动学、碰撞检测和轨迹指标进行安全评估。

Result: 不同架构输出的热力图显示，归因权重均匀分布在各个姿态维度的网络在保持定位精度的同时，能获得更大的安全余量，且XAI技术可洞察潜在失效模式并指导网络优化。

Conclusion: 本文提出的可解释IK分析及部署方法，能有效提升基于学习的运动学推理的透明性与安全性，对推动负责任AI及可信机器人控制具有实际意义。

Abstract: Deep neural networks have accelerated inverse-kinematics (IK) inference to the point where low cost manipulators can execute complex trajectories in real time, yet the opaque nature of these models contradicts the transparency and safety requirements emerging in responsible AI regulation. This study proposes an explainability centered workflow that integrates Shapley-value attribution with physics-based obstacle avoidance evaluation for the ROBOTIS OpenManipulator-X. Building upon the original IKNet, two lightweight variants-Improved IKNet with residual connections and Focused IKNet with position-orientation decoupling are trained on a large, synthetically generated pose-joint dataset. SHAP is employed to derive both global and local importance rankings, while the InterpretML toolkit visualizes partial-dependence patterns that expose non-linear couplings between Cartesian poses and joint angles. To bridge algorithmic insight and robotic safety, each network is embedded in a simulator that subjects the arm to randomized single and multi-obstacle scenes; forward kinematics, capsule-based collision checks, and trajectory metrics quantify the relationship between attribution balance and physical clearance. Qualitative heat maps reveal that architectures distributing importance more evenly across pose dimensions tend to maintain wider safety margins without compromising positional accuracy. The combined analysis demonstrates that explainable AI(XAI) techniques can illuminate hidden failure modes, guide architectural refinements, and inform obstacle aware deployment strategies for learning based IK. The proposed methodology thus contributes a concrete path toward trustworthy, data-driven manipulation that aligns with emerging responsible-AI standards.

</details>


### [275] [PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement Using Deep Learning-Based Dynamic Object Filtering](https://arxiv.org/abs/2512.23318)
*Sheng-Kai Chen,Jie-Yu Chao,Jr-Yu Chang,Po-Lien Wu,Po-Chiang Lin*

Main category: cs.RO

TL;DR: 本文提出了一种增强的vSLAM系统PCR-ORB，通过深度学习结合点云精炼，有效提升动态环境下的SLAM鲁棒性。方法在多场景评测中取得了有针对性的精度提升。


<details>
  <summary>Details</summary>
Motivation: 当前传统vSLAM系统如ORB-SLAM3，在动态环境下由于移动物体会导致定位及地图构建精度下降。因此，亟需提升SLAM系统对动态对象的鲁棒性，保障在实际复杂环境中持续准确工作。

Method: PCR-ORB以ORB-SLAM3为基础，融合了基于YOLOv8的语义分割，对动态对象进行精准识别和过滤。通过CUDA加速，实现了实时处理能力。系统采用多阶段过滤策略，包括地面估计、天空区域移除、边缘过滤和时序一致性验证等步骤，最大限度去除动态干扰点。

Result: 在KITTI数据集（00-09号序列）上评估，系统整体表现出场景相关性提升，尤其在04号序列上ATE RMSE提升了25.9%，ATE 中位数提升了30.4%。但也有部分场景效果不均，显示方法对场景依赖较强。

Conclusion: PCR-ORB方案在应对动态环境方面提供了有效的新思路，在部分典型场景下显著提升了SLAM精度与一致性，但效果对具体场景有依赖，未来可进一步优化动态对象检测与处理策略。

Abstract: Visual Simultaneous Localization and Mapping (vSLAM) systems encounter substantial challenges in dynamic environments where moving objects compromise tracking accuracy and map consistency. This paper introduces PCR-ORB (Point Cloud Refinement ORB), an enhanced ORB-SLAM3 framework that integrates deep learning-based point cloud refinement to mitigate dynamic object interference. Our approach employs YOLOv8 for semantic segmentation combined with CUDA-accelerated processing to achieve real-time performance. The system implements a multi-stage filtering strategy encompassing ground plane estimation, sky region removal, edge filtering, and temporal consistency validation. Comprehensive evaluation on the KITTI dataset (sequences 00-09) demonstrates performance characteristics across different environmental conditions and scene types. Notable improvements are observed in specific sequences, with sequence 04 achieving 25.9% improvement in ATE RMSE and 30.4% improvement in ATE median. However, results show mixed performance across sequences, indicating scenario-dependent effectiveness. The implementation provides insights into dynamic object filtering challenges and opportunities for robust navigation in complex environments.

</details>


### [276] [Optimal Scalability-Aware Allocation of Swarm Robots: From Linear to Retrograde Performance via Marginal Gains](https://arxiv.org/abs/2512.23431)
*Simay Atasoy Bingöl,Tobias Töpfer,Sven Kosub,Heiko Hamann,Andreagiovanni Reina*

Main category: cs.RO

TL;DR: 本文提出了一种基于边际性能收益的高效计算算法，可对具有凹型可扩展性任务的多智能体系统进行最优分配，提升群体任务表现，并应用于机器人群体仿真中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中，如何将有限的智能体资源在难度和可扩展性各异的多任务间合理分配，最大化整体性能，是现实和理论上都关键但计算复杂度高的问题。

Method: 提出了一种以任务性能边际收益为基础的分配算法，适用于任务表现随投入智能体数量非线性变化且服从凹函数关系的情形，包括线性、饱和与逆增长等三种可扩展性。通过机器人的群体决策任务仿真测试算法，分析环境特征复杂度（空间异质性）的不同下各任务的分配及决策表现。

Result: 仿真显示，所提出算法能有效地根据不同任务的需求与干扰情况分配机器人，提升整体决策效率。算法在环境无干扰下决策性能随人数饱和增长，有干扰时呈现逆增长，均能实现合理分配。

Conclusion: 该算法能够高效地实现多智能体资源的最优分配，为实际多机器人系统的部署和任务分派提供了有用工具与理论基础。

Abstract: In collective systems, the available agents are a limited resource that must be allocated among tasks to maximize collective performance. Computing the optimal allocation of several agents to numerous tasks through a brute-force approach can be infeasible, especially when each task's performance scales differently with the increase of agents. For example, difficult tasks may require more agents to achieve similar performances compared to simpler tasks, but performance may saturate nonlinearly as the number of allocated agents increases. We propose a computationally efficient algorithm, based on marginal performance gains, for optimally allocating agents to tasks with concave scalability functions, including linear, saturating, and retrograde scaling, to achieve maximum collective performance. We test the algorithm by allocating a simulated robot swarm among collective decision-making tasks, where embodied agents sample their environment and exchange information to reach a consensus on spatially distributed environmental features. We vary task difficulties by different geometrical arrangements of environmental features in space (patchiness). In this scenario, decision performance in each task scales either as a saturating curve (following the Condorcet's Jury Theorem in an interference-free setup) or as a retrograde curve (when physical interference among robots restricts their movement). Using simple robot simulations, we show that our algorithm can be useful in allocating robots among tasks. Our approach aims to advance the deployment of future real-world multi-robot systems.

</details>


### [277] [Theory of Mind for Explainable Human-Robot Interaction](https://arxiv.org/abs/2512.23482)
*Marie Bauer,Julia Gachot,Matthias Kerzel,Cornelius Weber,Stefan Wermter*

Main category: cs.RO

TL;DR: 本文提出将“心智理论（ToM）”作为可解释人工智能（XAI）的一部分，应用于人机交互（HRI），强调以用户为中心的解释性设计。


<details>
  <summary>Details</summary>
Motivation: 现有的人机交互和可解释AI系统关注点多在于AI本身，而缺乏对用户需求和心理状态的重视。针对这一点，作者希望通过引入ToM提升机器对用户行为、心理状态的理解与响应能力，增加AI可解释性和用户信任度。

Method: 提出将ToM视作XAI的一种，并利用eValuation XAI（VXAI）框架及其七大标准对其进行分析；指出目前ToM在解释机器人内部推理真实程度时存在评估缺口。因此建议将ToM原则嵌入XAI框架，推动解释以用户为中心。

Result: 通过理论分析，发现将ToM结合XAI能够弥补当前以AI系统为中心、解释对用户实际帮助有限的不足，提高解释与机器人真实内部推理的对应性和用户信息需求的关注度。

Conclusion: 将ToM集成进XAI，有利于实现用户导向的解释系统，推动可解释AI研究关心人机交互中用户的实际需求和理解，从而提升机器人行为的可解释性和预测性。

Abstract: Within the context of human-robot interaction (HRI), Theory of Mind (ToM) is intended to serve as a user-friendly backend to the interface of robotic systems, enabling robots to infer and respond to human mental states. When integrated into robots, ToM allows them to adapt their internal models to users' behaviors, enhancing the interpretability and predictability of their actions. Similarly, Explainable Artificial Intelligence (XAI) aims to make AI systems transparent and interpretable, allowing humans to understand and interact with them effectively. Since ToM in HRI serves related purposes, we propose to consider ToM as a form of XAI and evaluate it through the eValuation XAI (VXAI) framework and its seven desiderata. This paper identifies a critical gap in the application of ToM within HRI, as existing methods rarely assess the extent to which explanations correspond to the robot's actual internal reasoning. To address this limitation, we propose to integrate ToM within XAI frameworks. By embedding ToM principles inside XAI, we argue for a shift in perspective, as current XAI research focuses predominantly on the AI system itself and often lacks user-centered explanations. Incorporating ToM would enable a change in focus, prioritizing the user's informational needs and perspective.

</details>


### [278] [Robust Deep Learning Control with Guaranteed Performance for Safe and Reliable Robotization in Heavy-Duty Machinery](https://arxiv.org/abs/2512.23505)
*Mehdi Heydari Shahna*

Main category: cs.RO

TL;DR: 本文提出了一种通用模块化控制框架，旨在推动重型移动机械由柴油-液压系统向清洁电动系统转型，并向更高自主化迈进。框架已在多种实际案例中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前重型移动机械面临从传统柴油液压系统向清洁电动系统过渡，以及从人工操作到更高自主化的趋势。由于技术、经济和安全规范等多重挑战，现有系统难以直接采用电气化与人工智能技术。作者希望解决这些挑战，推动行业转型。

Method: 该论文开发了一个通用、模块化的控制框架，不依赖于特定能源类型，并易于未来扩展。通过分层控制策略，将人工智能方法部分集成到控制体系内，并确保安全性能边界和稳定性。具体研究覆盖：多体系统的鲁棒控制策略，面向不确定性与故障的性能保持方案，以及黑盒学习方法的可验证性安全集成。

Result: 所提控制框架在三组涵盖不同执行器与工况的案例中得到了验证，包括重型移动机器人和机械臂，取得了良好控制性能和安全保障，相关成果已发表在五篇同行评议论文和一篇未发表手稿中。

Conclusion: 该论文所提出的框架推动了非线性控制与机器人技术的发展，为重型移动机械行业向电动化和自主化转型提供了理论与应用支持，具备良好扩展性和安全性，具有实际推广和产业价值。

Abstract: Today's heavy-duty mobile machines (HDMMs) face two transitions: from diesel-hydraulic actuation to clean electric systems driven by climate goals, and from human supervision toward greater autonomy. Diesel-hydraulic systems have long dominated, so full electrification, via direct replacement or redesign, raises major technical and economic challenges. Although advanced artificial intelligence (AI) could enable higher autonomy, adoption in HDMMs is limited by strict safety requirements, and these machines still rely heavily on human supervision.
  This dissertation develops a control framework that (1) simplifies control design for electrified HDMMs through a generic modular approach that is energy-source independent and supports future modifications, and (2) defines hierarchical control policies that partially integrate AI while guaranteeing safety-defined performance and stability.
  Five research questions align with three lines of investigation: a generic robust control strategy for multi-body HDMMs with strong stability across actuation types and energy sources; control solutions that keep strict performance under uncertainty and faults while balancing robustness and responsiveness; and methods to interpret and trust black-box learning strategies so they can be integrated stably and verified against international safety standards.
  The framework is validated in three case studies spanning different actuators and conditions, covering heavy-duty mobile robots and robotic manipulators. Results appear in five peer-reviewed publications and one unpublished manuscript, advancing nonlinear control and robotics and supporting both transitions.

</details>


### [279] [Act2Goal: From World Model To General Goal-conditioned Policy](https://arxiv.org/abs/2512.23541)
*Pengfei Zhou,Liliang Chen,Shengcong Chen,Di Chen,Wenzhi Zhao,Rongjun Jin,Guanghui Ren,Jianlan Luo*

Main category: cs.RO

TL;DR: 提出了Act2Goal，一种结合视觉世界模型与多尺度时序控制的目标导向机器人操纵策略，大幅提升复杂场景下长时序任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 机器人操纵任务表达需要既具表达性又要精确，而现有的基于目标的策略在长时序操纵任务中由于只做一步动作预测，难以捕捉任务持续进展。

Method: 提出了Act2Goal方法，将目标导向的视觉世界模型与多尺度时序控制结合。具体为：1）世界模型根据当前观测和目标视觉状态，生成合理的中间视觉状态序列；2）引入多尺度时序哈希（MSTH）机制，细粒度帧用于闭环控制，稀疏帧保证全局任务连贯性；3）通过端到端的交叉注意方式，使视觉规划与机械运动协同。还引入了基于LoRA微调的在线无奖励自适应能力。

Result: Act2Goal方案在零样本泛化（新物体、新空间布局、新环境）上表现优异。真实机器人实验中，在无需人工监督的情况下，系统能在数分钟内将复杂任务成功率从30%提升到90%。

Conclusion: 基于目标的世界模型配合多尺度时序控制，为长时序机器人操纵任务提供了结构化、鲁棒的指导，显著增强了泛化性和自主适应能力。

Abstract: Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/

</details>


### [280] [Soft Robotic Technological Probe for Speculative Fashion Futures](https://arxiv.org/abs/2512.23570)
*Amy Ingold,Loong Yi Lee,Richard Suphapol Diteesawat,Ajmal Roshan,Yael Zekaria,Edith-Clare Hall,Enrico Werner,Nahian Rahman,Elaine Czech,Jonathan Rossiter*

Main category: cs.RO

TL;DR: 本文介绍了一种名为Sumbrella的软体机器人服装，通过与创意技术人员的焦点小组讨论，探讨了其社会意义及未来可能性，并对可穿戴机器人服饰的设计提出了伦理和社会建议。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴机器人不断涌现，研究者意识到除了功能外，设计还需关注社会意义。因此，本文旨在用Sumbrella服装作为探索工具，研究公众对软体机器人穿戴设备的解读和想象。

Method: 研究首先设计并制造了Sumbrella服装，采用折纸灵感的双稳态单元、织物气动驱动腔、拉线变形机制、计算机视觉组件等，将动力和控制集成进帽子和短上衣。随后，通过召集12名创意技术专家的小组讨论，用Sumbrella进行技术探索，收集他们的观点与反应。

Result: 参与者围绕Sumbrella展开了关于未来社会和表达潜力的讨论，同时也提出了对监控、剥削、个人风险和社会伦理等方面的担忧。研究总结了软体机器人服饰在运动交流、社会影响及伦理设计等方面的关键考量。

Conclusion: 本文认为，只有将投机性设计方法引入HRI研究，才能全面评估可穿戴机器人在功能与社会意义上的作用。研究为软体机器人服装研发提供了实践性建议，强调了创新表达与伦理规范并重的重要性。

Abstract: Emerging wearable robotics demand design approaches that address not only function, but also social meaning. In response, we present Sumbrella, a soft robotic garment developed as a speculative fashion probe. We first detail the design and fabrication of the Sumbrella, including sequenced origami-inspired bistable units, fabric pneumatic actuation chambers, cable driven shape morphing mechanisms, computer vision components, and an integrated wearable system comprising a hat and bolero jacket housing power and control electronics. Through a focus group with twelve creative technologists, we then used Sumbrella as a technological probe to explore how people interpreted, interacted, and imagined future relationships with soft robotic wearables. While Sumbrella allowed our participants to engage in rich discussion around speculative futures and expressive potential, it also surfaced concerns about exploitation, surveillance, and the personal risks and societal ethics of embedding biosensing technologies in public life. We contribute to the Human-Robot Interaction (HRI) field key considerations and recommendations for designing soft robotic garments, including the potential for kinesic communication, the impact of such technologies on social dynamics, and the importance of ethical guidelines. Finally, we provide a reflection on our application of speculative design; proposing that it allows HRI researchers to not only consider functionality, but also how wearable robots influence definitions of what is considered acceptable or desirable in public settings.

</details>


### [281] [Unsupervised Learning for Detection of Rare Driving Scenarios](https://arxiv.org/abs/2512.23585)
*Dat Le,Thomas Manhardt,Moritz Venator,Johannes Betz*

Main category: cs.RO

TL;DR: 本文提出了一种基于无监督学习的框架，利用Deep Isolation Forest检测自动驾驶场景中的罕见和危险驾驶情况，并通过自然驾驶数据验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要能够及时识别极端和罕见的危险驾驶场景以确保安全，但这些场景稀有且难以通过传统监督学习手段全面标注和检测。

Method: 方法采用Deep Isolation Forest（DIF）结合感知模块采集的车辆动力学及环境特征，通过滑动窗口统计特征处理与t-SNE降维可视化，对罕见驾驶场景进行无监督异常检测。评估方式结合了代理真实标签和定性视频帧检查。

Result: 实验结果表明，该框架能有效检测到自然驾驶数据中的罕见和危险驾驶情况，且可扩展性较好。

Conclusion: 所提出的方法有效提升了自动驾驶系统对极端危险场景的识别能力，但目前依赖于代理真实标签和人工特征组合，仍未覆盖所有实际异常及复杂情境。

Abstract: The detection of rare and hazardous driving scenarios is a critical challenge for ensuring the safety and reliability of autonomous systems. This research explores an unsupervised learning framework for detecting rare and extreme driving scenarios using naturalistic driving data (NDD). We leverage the recently proposed Deep Isolation Forest (DIF), an anomaly detection algorithm that combines neural network-based feature representations with Isolation Forests (IFs), to identify non-linear and complex anomalies. Data from perception modules, capturing vehicle dynamics and environmental conditions, is preprocessed into structured statistical features extracted from sliding windows. The framework incorporates t-distributed stochastic neighbor embedding (t-SNE) for dimensionality reduction and visualization, enabling better interpretability of detected anomalies. Evaluation is conducted using a proxy ground truth, combining quantitative metrics with qualitative video frame inspection. Our results demonstrate that the proposed approach effectively identifies rare and hazardous driving scenarios, providing a scalable solution for anomaly detection in autonomous driving systems. Given the study's methodology, it was unavoidable to depend on proxy ground truth and manually defined feature combinations, which do not encompass the full range of real-world driving anomalies or their nuanced contextual dependencies.

</details>


### [282] [A Kalman Filter-Based Disturbance Observer for Steer-by-Wire Systems](https://arxiv.org/abs/2512.23593)
*Nikolai Beving,Jonas Marxen,Steffen Mueller,Johannes Betz*

Main category: cs.RO

TL;DR: 本论文提出了一种基于卡尔曼滤波的扰动观测器，通过仅使用电机状态测量来高效估算由驾驶员无意施加的高频力矩扰动，从而提升Steer-by-Wire系统的转向性能。


<details>
  <summary>Details</summary>
Motivation: Steer-by-Wire系统取消了机械连接，虽然带来灵活、轻量和适应自动驾驶等优点，但易受驾驶员高频扰动（驱动员阻抗）的影响，传统传感器方案成本高、不实用，当前方法对高频扰动不敏感，因此需要新的观测方案以精准捕捉此类扰动。

Method: 本文设计了一种基于卡尔曼滤波器的扰动观测器，把驾驶员的被动力矩建模为PT1滞后并加入系统模型，分别在线性和非线性模型中实现并测试了不同卡尔曼滤波算法（如扩展卡尔曼滤波器EKF），通过仿真比较其对扰动重构的效果。

Result: 所提出的扰动观测器能以最小14ms的时延准确重建驾驶员高频扰动，非线性扩展卡尔曼滤波器在应对摩擦等模型非线性时表现优于线性滤波器，尤其能更好地估算由静摩擦向动摩擦转变时的驱动力变化。

Conclusion: 卡尔曼滤波方法可有效估算Steer-by-Wire系统中的高频驾驶员力矩扰动，且不需昂贵的传感器。由于只做了仿真验证，尚需进一步在现实驾驶场景下测试该方法的鲁棒性。

Abstract: Steer-by-Wire systems replace mechanical linkages, which provide benefits like weight reduction, design flexibility, and compatibility with autonomous driving. However, they are susceptible to high-frequency disturbances from unintentional driver torque, known as driver impedance, which can degrade steering performance. Existing approaches either rely on direct torque sensors, which are costly and impractical, or lack the temporal resolution to capture rapid, high-frequency driver-induced disturbances. We address this limitation by designing a Kalman filter-based disturbance observer that estimates high-frequency driver torque using only motor state measurements. We model the drivers passive torque as an extended state using a PT1-lag approximation and integrate it into both linear and nonlinear Steer-by-Wire system models. In this paper, we present the design, implementation and simulation of this disturbance observer with an evaluation of different Kalman filter variants. Our findings indicate that the proposed disturbance observer accurately reconstructs driver-induced disturbances with only minimal delay 14ms. We show that a nonlinear extended Kalman Filter outperforms its linear counterpart in handling frictional nonlinearities, improving estimation during transitions from static to dynamic friction. Given the study's methodology, it was unavoidable to rely on simulation-based validation rather than real-world experimentation. Further studies are needed to investigate the robustness of the observers under real-world driving conditions.

</details>


### [283] [Interactive Robot Programming for Surface Finishing via Task-Centric Mixed Reality Interfaces](https://arxiv.org/abs/2512.23616)
*Christoph Willibald,Lugh Martensen,Thomas Eiband,Dongheui Lee*

Main category: cs.RO

TL;DR: 本论文提出了一种无需机器人专业知识，非专家用户也能直观编程的新型机器人编程方法，应用于小批量多样化产品的表面加工任务。


<details>
  <summary>Details</summary>
Motivation: 当前，机器人部署在高产品多样性、小批量生产任务中存在门槛，主要因为编程流程复杂且需专业知识，导致协作机器人在小规模制造业中应用有限。

Method: 开发了一种结合用户输入的新型表面分割算法，用户可通过交互式、任务导向的工作流程直观编程，算法过程中提供持续可视化反馈，用户可迭代优化分割结果。基于分割表面自动生成机器人轨迹完成加工任务。通过双项用户研究比较了多种交互设计。

Result: 实验表明，该交互设计显著降低了用户负担，提高了系统可用性，且即使不具备实际经验的用户也能有效完成任务编程。

Conclusion: 所提方法使非专家也能高效、直观地完成协作机器人任务编程，从而助力机器人在小规模制造和工艺场景的推广应用。

Abstract: Lengthy setup processes that require robotics expertise remain a major barrier to deploying robots for tasks involving high product variability and small batch sizes. As a result, collaborative robots, despite their advanced sensing and control capabilities, are rarely used for surface finishing in small-scale craft and manufacturing settings. To address this gap, we propose a novel robot programming approach that enables non-experts to intuitively program robots through interactive, task-focused workflows. For that, we developed a new surface segmentation algorithm that incorporates human input to identify and refine workpiece regions for processing. Throughout the programming process, users receive continuous visual feedback on the robot's learned model, enabling them to iteratively refine the segmentation result. Based on the segmented surface model, a robot trajectory is generated to cover the desired processing area. We evaluated multiple interaction designs across two comprehensive user studies to derive an optimal interface that significantly reduces user workload, improves usability and enables effective task programming even for users with limited practical experience.

</details>


### [284] [The N-5 Scaling Law: Topological Dimensionality Reduction in the Optimal Design of Fully-actuated Multirotors](https://arxiv.org/abs/2512.23619)
*Antonio Franchi*

Main category: cs.RO

TL;DR: 本论文通过几何与拓扑学方法，研究了全控全向N轴飞行器设计中的优化空间拓扑结构，揭示车架对称性决定了全局最优解的形态，并提出了适用于多种多面体结构的“N-5缩放律”。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅通过参数优化给出单一结构最优解，难以揭示所有可能设计及其间联系。本文希望通过拓扑学手段，系统探索全局最优集的本质结构，以发现潜在的设计冗余与可变形能力。

Method: 将设计问题建模为投影直线乘积流形上的优化，固定转子于多面体顶点，可变动作用方向。采用坐标无关的Log-Volume各向同性指标进行优化，通过分析车架从不规则到规则时解集拓扑变化，总结规律并归纳为N-5缩放律。

Result: 对于普通不规则车架，最优解为有限离散点；当车架几何趋于规则，解空间发生拓扑转变，最终形成N维环面与连续1维曲线（锁相模式）。发现所有N≤10的正多边形与柏拉图多面体均符合N-5枝分1维连通支结构，并可通过星形多边形序列精确预测最优解相位。

Conclusion: 解空间拓扑受车架对称性主导，导致结构设计存在冗余与可变形性。可在保持最优控制性的前提下，实现飞行器结构连续变形。这为全向飞行器的结构设计与优化提供了新颖拓扑学视角及理论依据。

Abstract: The geometric design of fully-actuated and omnidirectional N-rotor aerial vehicles is conventionally formulated as a parametric optimization problem, seeking a single optimal set of N orientations within a fixed architectural family. This work departs from that paradigm to investigate the intrinsic topological structure of the optimization landscape itself. We formulate the design problem on the product manifold of Projective Lines \RP^2^N, fixing the rotor positions to the vertices of polyhedral chassis while varying their lines of action. By minimizing a coordinate-invariant Log-Volume isotropy metric, we reveal that the topology of the global optima is governed strictly by the symmetry of the chassis. For generic (irregular) vertex arrangements, the solutions appear as a discrete set of isolated points. However, as the chassis geometry approaches regularity, the solution space undergoes a critical phase transition, collapsing onto an N-dimensional Torus of the lines tangent at the vertexes to the circumscribing sphere of the chassis, and subsequently reducing to continuous 1-dimensional curves driven by Affine Phase Locking. We synthesize these observations into the N-5 Scaling Law: an empirical relationship holding for all examined regular planar polygons and Platonic solids (N <= 10), where the space of optimal configurations consists of K=N-5 disconnected 1D topological branches. We demonstrate that these locking patterns correspond to a sequence of admissible Star Polygons {N/q}, allowing for the exact prediction of optimal phases for arbitrary N. Crucially, this topology reveals a design redundancy that enables optimality-preserving morphing: the vehicle can continuously reconfigure along these branches while preserving optimal isotropic control authority.

</details>


### [285] [RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion](https://arxiv.org/abs/2512.23649)
*Zhe Li,Cheng Chi,Yangyang Wei,Boan Zhu,Tao Huang,Zhenguo Sun,Yibo Peng,Pengwei Wang,Zhongyuan Wang,Fangzhou Liu,Chang Xu,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种全新的视频到机器人行走控制框架RoboMirror，实现了通过理解视频内容再进行模仿，而无需传统的动作重定向或姿态重建步骤。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人行走系统通常依赖人工动作捕捉轨迹或简单的文本命令，缺乏对丰富视觉内容的理解和利用，导致视觉感知与运动控制之间存在鸿沟。

Method: 作者提出RoboMirror，利用强大的视觉-语言模型提取视频中的运动意图，结合扩散式策略生成器，直接生成与视频语义一致且物理可行的机器人行走动作，无需显式进行姿态重建或动作重定向。

Result: 实验表明，RoboMirror不仅能够支持基于第一人称视频的远程操作，在第三人称控制下将延迟降低了80%，任务成功率比传统方法提升了3.7%。

Conclusion: RoboMirror通过以视觉理解为核心的人形机器人控制新范式，有效弥合了视觉与运动控制的鸿沟，推动了机器人模仿学习的发展。

Abstract: Humans learn locomotion through visual observation, interpreting visual content first before imitating actions. However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control. Text-to-motion methods suffer from semantic sparsity and staged pipeline errors, while video-based approaches only perform mechanical pose mimicry without genuine visual understanding. We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying "understand before you imitate". Leveraging VLMs, it distills raw egocentric/third-person videos into visual motion intents, which directly condition a diffusion-based policy to generate physically plausible, semantically aligned locomotion without explicit pose reconstruction or retargeting. Extensive experiments validate the effectiveness of RoboMirror, it enables telepresence via egocentric videos, drastically reduces third-person control latency by 80%, and achieves a 3.7% higher task success rate than baselines. By reframing humanoid control around video understanding, we bridge the visual understanding and action gap.

</details>


### [286] [Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control](https://arxiv.org/abs/2512.23650)
*Zhe Li,Cheng Chi,Yangyang Wei,Boan Zhu,Tao Huang,Zhenguo Sun,Yibo Peng,Pengwei Wang,Zhongyuan Wang,Fangzhou Liu,Chang Xu,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出了RoboPerform系统，能够将音频直接转化为人形机器人动作，实现跟随音乐跳舞和语音手势生成，避免传统方法重建动作的高延迟和误差，实现低延迟、高保真机器人动作表达。


<details>
  <summary>Details</summary>
Motivation: 现有机器人难以自然、即兴地响应音频，动作生成依赖于预设动作或稀疏命令，且音频到动作的转换过程复杂、误差大、延迟高。

Method: 提出了RoboPerform框架，遵循“动作=内容+风格”原则，将音频作为动作风格信号，整合ResMoE教师策略（适应多样动作）与基于扩散模型的学生策略（实现风格注入），直接从音频生成动作，无需显式动作重建，也无需动作重定向。

Result: 系统在物理合理性和音频对齐性方面都取得了较好效果，实现了机器人对音乐和语音的自然响应。

Conclusion: RoboPerform显著提升了机器人即兴动作表达能力，是首个可直接将音频驱动的人形机器人动作生成一体化框架，为机器人变成有表现力的表演者提供了新方法。

Abstract: Humans intuitively move to sound, but current humanoid robots lack expressive improvisational capabilities, confined to predefined motions or sparse commands. Generating motion from audio and then retargeting it to robots relies on explicit motion reconstruction, leading to cascaded errors, high latency, and disjointed acoustic-actuation mapping. We propose RoboPerform, the first unified audio-to-locomotion framework that can directly generate music-driven dance and speech-driven co-speech gestures from audio. Guided by the core principle of "motion = content + style", the framework treats audio as implicit style signals and eliminates the need for explicit motion reconstruction. RoboPerform integrates a ResMoE teacher policy for adapting to diverse motion patterns and a diffusion-based student policy for audio style injection. This retargeting-free design ensures low latency and high fidelity. Experimental validation shows that RoboPerform achieves promising results in physical plausibility and audio alignment, successfully transforming robots into responsive performers capable of reacting to audio.

</details>


### [287] [The Bulldozer Technique: Efficient Elimination of Local Minima Traps for APF-Based Robot Navigation](https://arxiv.org/abs/2512.23672)
*Mohammed Baziyad,Manal Al Shohna,Tamer Rabie*

Main category: cs.RO

TL;DR: 本论文提出了一种名为“推土机”的新型路径规划方法，有效解决了人工势场（APF）在路径规划中遇到的局部极小值陷阱问题，并在多种地图和算法对比中展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统人工势场路径规划方法易陷入局部极小值，导致机器人无法顺利到达目标。尽管APF因其计算效率高、实时性强受到青睐，但这一局限性大大影响了其实用性，因此迫切需要能兼顾APF优点并解决局部极小值的新方法。

Method: 提出了一种“推土机”机制，在遇到局部极小值时，通过“回填”方式提升这些区域的势场值，类比于推土机填平道路坑洞。同时设计了基于斜坡的辅助机制，支持机器人在初始即处于局部极小值时的逃逸。通过物理移动机器人在多种地图环境下进行实验，对比标准APF、改进型APF及A*、PRM、RRT等主流算法。

Result: 推土机方法有效识别并消除局部极小值区域，实验结果显示在执行速度、路径质量上优于（或相当于）现有主流方法。通过运动学跟踪验证，路径平滑且易于实际机器人跟踪。

Conclusion: 推土机方法继承了APF实时性和高效性的优点，并显著克服了其局部极小值弱点，具有较高实际应用潜力，是移动机器人路径规划问题的有力解决方案。

Abstract: Path planning is a fundamental component in autonomous mobile robotics, enabling a robot to navigate from its current location to a desired goal while avoiding obstacles. Among the various techniques, Artificial Potential Field (APF) methods have gained popularity due to their simplicity, real-time responsiveness, and low computational requirements. However, a major limitation of conventional APF approaches is the local minima trap problem, where the robot becomes stuck in a position with no clear direction toward the goal. This paper proposes a novel path planning technique, termed the Bulldozer, which addresses the local minima issue while preserving the inherent advantages of APF. The Bulldozer technique introduces a backfilling mechanism that systematically identifies and eliminates local minima regions by increasing their potential values, analogous to a bulldozer filling potholes in a road. Additionally, a ramp-based enhancement is incorporated to assist the robot in escaping trap areas when starting within a local minimum. The proposed technique is experimentally validated using a physical mobile robot across various maps with increasing complexity. Comparative analyses are conducted against standard APF, adaptive APF, and well-established planning algorithms such as A*, PRM, and RRT. Results demonstrate that the Bulldozer technique effectively resolves the local minima problem while achieving superior execution speed and competitive path quality. Furthermore, a kinematic tracking controller is employed to assess the smoothness and traceability of the planned paths, confirming their suitability for real-world execution.

</details>


### [288] [Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation](https://arxiv.org/abs/2512.23703)
*Huajie Tan,Sixiang Chen,Yijie Xu,Zixiao Wang,Yuheng Ji,Cheng Chi,Yaoxu Lyu,Zhongxia Zhao,Xiansheng Chen,Peterson Co,Shaoxuan Xie,Guocai Yao,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种新型奖励建模方法Dopamine-Reward，解决了当前基于学习的奖励模型在步态识别和多视角感知方面的不足，并推出了Dopamine-RL框架实现高效、理论上合理的强化学习训练，大量实验验证了新方法在多任务中均显著提升了策略学习效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习在实际机器人应用面临的主要障碍是奖励函数的有效设计。现有方法在奖励模型的细粒度进展识别和多视角感知上存在局限，并且他们的奖励塑造过程缺乏理论依据，易引入语义陷阱，从而误导策略优化。

Method: 提出Dopamine-Reward奖励建模方法，核心是通用奖励模型（GRM），通过3400小时数据集进行训练。方法采用步进奖励离散化理解流程结构，并利用多视角奖励融合解决感知限制。在此基础上构建Dopamine-RL策略学习框架，引入政策不变的奖励塑造方式，实现高效、理论合理的强化学习训练。

Result: 实验覆盖多种仿真与实际任务，结果显示GRM在奖励评估上达到SOTA精度，Dopamine-RL大幅提升学习效率。例如：GRM仅需一次专家演示即可适配新任务，Dopamine-RL在约1小时内使成功率从几乎为零提升至95%，同时具有良好任务泛化能力。

Conclusion: Dopamine-Reward与Dopamine-RL在奖励建模和强化学习策略高效训练上均取得显著突破，为实际机器人任务赋能，并为大规模自主机器人学习奠定基础。

Abstract: The primary obstacle for applying reinforcement learning (RL) to real-world robotics is the design of effective reward functions. While recently learning-based Process Reward Models (PRMs) are a promising direction, they are often hindered by two fundamental limitations: their reward models lack step-aware understanding and rely on single-view perception, leading to unreliable assessments of fine-grained manipulation progress; and their reward shaping procedures are theoretically unsound, often inducing a semantic trap that misguides policy optimization. To address these, we introduce Dopamine-Reward, a novel reward modeling method for learning a general-purpose, step-aware process reward model from multi-view inputs. At its core is our General Reward Model (GRM), trained on a vast 3,400+ hour dataset, which leverages Step-wise Reward Discretization for structural understanding and Multi-Perspective Reward Fusion to overcome perceptual limitations. Building upon Dopamine-Reward, we propose Dopamine-RL, a robust policy learning framework that employs a theoretically-sound Policy-Invariant Reward Shaping method, which enables the agent to leverage dense rewards for efficient self-improvement without altering the optimal policy, thereby fundamentally avoiding the semantic trap. Extensive experiments across diverse simulated and real-world tasks validate our approach. GRM achieves state-of-the-art accuracy in reward assessment, and Dopamine-RL built on GRM significantly improves policy learning efficiency. For instance, after GRM is adapted to a new task in a one-shot manner from a single expert trajectory, the resulting reward model enables Dopamine-RL to improve the policy from near-zero to 95% success with only 150 online rollouts (approximately 1 hour of real robot interaction), while retaining strong generalization across tasks. Project website: https://robo-dopamine.github.io

</details>
