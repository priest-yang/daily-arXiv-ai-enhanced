<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 63]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.RO](#cs.RO) [Total: 21]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices](https://arxiv.org/abs/2511.03765)
*Hyunseok Kwak,Kyeongwon Lee,Jae-Jin Lee,Woojoo Lee*

Main category: cs.CV

TL;DR: LoRA-Edge是一种高效的卷积神经网络（CNN）端侧微调方法，在极低资源消耗下实现域自适应，并保证了较高识别准确率。


<details>
  <summary>Details</summary>
Motivation: 人类活动识别等边缘应用中，模型经常面临域偏移，需要现场微调提升性能。但完整微调对计算、内存、能耗要求过高，不适合边缘设备。因此需要一种高效的参数微调方法。

Method: 提出LoRA-Edge方法：1）用TT-SVD对预训练卷积层做分解；2）仅有选择性地更新输出侧核心，初始置零辅助路径以降低初始开销；3）更新后回融合到稠密卷积核中，从而不增加推理成本。显著减少所需训练参数。

Result: 在多个HAR数据集和不同CNN结构上，LoRA-Edge在仅更新最多1.49%参数的情况下，准确率比全量微调仅低4.7%，大幅优于现有参数高效基线。在Jetson Orin Nano平台上，带来1.4-3.8倍更快的收敛速度。

Conclusion: LoRA-Edge能够在边缘平台上实现结构对齐、参数高效的CNN模型自适应微调，兼顾性能和资源消耗，可实际应用于资源受限场景。

Abstract: On-device fine-tuning of CNNs is essential to withstand domain shift in edge
applications such as Human Activity Recognition (HAR), yet full fine-tuning is
infeasible under strict memory, compute, and energy budgets. We present
LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on
Low-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies
Tensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional
layers, (ii) selectively updates only the output-side core with
zero-initialization to keep the auxiliary path inactive at the start, and (iii)
fuses the update back into dense kernels, leaving inference cost unchanged.
This design preserves convolutional structure and reduces the number of
trainable parameters by up to two orders of magnitude compared to full
fine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves
accuracy within 4.7% of full fine-tuning while updating at most 1.49% of
parameters, consistently outperforming prior parameter-efficient baselines
under similar budgets. On a Jetson Orin Nano, TT-SVD initialization and
selective-core training yield 1.4-3.8x faster convergence to target F1.
LoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN
adaptation practical for edge platforms.

</details>


### [2] [SILVI: Simple Interface for Labeling Video Interactions](https://arxiv.org/abs/2511.03819)
*Ozan Kanbertay,Richard Vogg,Elif Karakoc,Peter M. Kappeler,Claudia Fichtel,Alexander S. Ecker*

Main category: cs.CV

TL;DR: 本文提出了一款名为SILVI的开源软件，用于视频中动物行为与互动的标注，弥补了现有工具不能同时标注行为和互动的不足。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉方法主要关注于单个动作检测，少有能处理个体间互动的检测与标注。同时，当前主流标注工具要么只能标注行为却不能定位个体，要么只能定位个体却无法刻画互动，导致行为生态学和计算机视觉的结合受限。

Method: 提出并实现开源标注软件SILVI，整合了行为和互动的标注功能，允许研究人员直接在视频中对动物行为和互动进行标注，并生成适用于训练和验证计算机视觉模型的结构化数据输出。

Result: SILVI实现了行为与互动标注的整合，并已开放源代码和文档，支持基于视频的细粒度行为分析，为相关社区提供了新的工具。

Conclusion: SILVI有效填补了现有动物行为标注工具的功能空白，还可扩展应用于其它视频中需提取动态场景图的人类互动标注任务，促进行为生态与视觉计算的交叉发展。

Abstract: Computer vision methods are increasingly used for the automated analysis of
large volumes of video data collected through camera traps, drones, or direct
observations of animals in the wild. While recent advances have focused
primarily on detecting individual actions, much less work has addressed the
detection and annotation of interactions -- a crucial aspect for understanding
social and individualized animal behavior. Existing open-source annotation
tools support either behavioral labeling without localization of individuals,
or localization without the capacity to capture interactions. To bridge this
gap, we present SILVI, an open-source labeling software that integrates both
functionalities. SILVI enables researchers to annotate behaviors and
interactions directly within video data, generating structured outputs suitable
for training and validating computer vision models. By linking behavioral
ecology with computer vision, SILVI facilitates the development of automated
approaches for fine-grained behavioral analyses. Although developed primarily
in the context of animal behavior, SILVI could be useful more broadly to
annotate human interactions in other videos that require extracting dynamic
scene graphs. The software, along with documentation and download instructions,
is available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.

</details>


### [3] [Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets](https://arxiv.org/abs/2511.03855)
*Duong Mai,Lawrence Hall*

Main category: cs.CV

TL;DR: 本文提出通过在训练时注入噪声（高斯、斑点、泊松、椒盐噪声），提升深度学习模型在胸部X光COVID-19检测中的泛化能力，有效缩小了模型在分布内与分布外数据表现的差距。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习图像识别模型难以泛化到新设备或新人群的数据，尤其是在胸部X光COVID-19检测任务上。模型容易学习到特定来源的捷径（即伪特征），导致在新分布下性能急剧下降。提升模型在分布外数据上的鲁棒性具有重要实际意义。

Method: 在模型训练阶段引入不同类型的噪声（高斯、斑点、泊松和椒盐噪声）以扰动输入，迫使模型学习更通用、更鲁棒的特征，而非依赖特定数据源的伪特征。

Result: 通过实验，结果显示引入噪声后模型在分布内与分布外数据表现的性能差距大幅缩小（AUC、F1、准确率、召回率、特异性等指标的差距从原来0.10-0.20降至0.01-0.06），且结果在十个不同随机种子下稳定。

Conclusion: 噪声注入是一种简单有效的训练技巧，可以显著提升医学影像模型在不同数据分布下的泛化能力，对临床实际应用具有潜在推动作用。

Abstract: Deep learned (DL) models for image recognition have been shown to fail to
generalize to data from different devices, populations, etc. COVID-19 detection
from Chest X-rays (CXRs), in particular, has been shown to fail to generalize
to out-of-distribution (OOD) data from new clinical sources not covered in the
training set. This occurs because models learn to exploit shortcuts -
source-specific artifacts that do not translate to new distributions - rather
than reasonable biomarkers to maximize performance on in-distribution (ID)
data. Rendering the models more robust to distribution shifts, our study
investigates the use of fundamental noise injection techniques (Gaussian,
Speckle, Poisson, and Salt and Pepper) during training. Our empirical results
demonstrate that this technique can significantly reduce the performance gap
between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results
averaged over ten random seeds across key metrics such as AUC, F1, accuracy,
recall and specificity. Our source code is publicly available at
https://github.com/Duongmai127/Noisy-ood

</details>


### [4] [Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures](https://arxiv.org/abs/2511.03882)
*Florence Klitzner,Blanca Inigo,Benjamin D. Killeen,Lalithkumar Seenivasan,Michelle Song,Axel Krieger,Mathias Unberath*

Main category: cs.CV

TL;DR: 本论文提出一种基于模仿学习的视频视觉控制策略，用于双平面X光引导下的脊柱穿刺手术模拟，并展示了其有效性及面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统基于模仿学习的视频机器人控制在常规视觉场景下取得进展，但X光多视角的复杂性使其在医疗（如脊柱定位）领域应用还未明朗。作者希望探索这一新兴方法在X光引导手术中的可行性和挑战。

Method: 作者开发了一个高度真实的脊柱手术仿真环境，收集并制作了标准手术路径及对应的双平面X光影像数据集，通过此数据集训练模仿学习策略，让模型仅根据双平面X光图像进行针管逐步对准和插入。对形成的策略进行了有效性和泛化能力的系统评估，同时还在真实X光数据上进行了初步测试。

Result: 在仿真环境中，模型首次尝试即成功完成68.5%的穿刺任务，且能保持安全路径，对不同脊柱解剖形态（含骨折）和初始位置都有较好泛化。在真实X光图片上的回放表明，训练于仿真的模型也能生成合理路径。但模型在入点精度方面仍有限制。

Conclusion: 该方法在脊柱手术模拟中展现出可行性，为无CT的轻量级机器人脊柱导航提供了初步成果。不过要实现临床级用例，还需提升反馈频率、精度和结合更多先验知识。

Abstract: Imitation learning-based robot control policies are enjoying renewed interest
in video-based robotics. However, it remains unclear whether this approach
applies to X-ray-guided procedures, such as spine instrumentation. This is
because interpretation of multi-view X-rays is complex. We examine
opportunities and challenges for imitation policy learning in bi-plane-guided
cannula insertion. We develop an in silico sandbox for scalable, automated
simulation of X-ray-guided spine procedures with a high degree of realism. We
curate a dataset of correct trajectories and corresponding bi-planar X-ray
sequences that emulate the stepwise alignment of providers. We then train
imitation learning policies for planning and open-loop control that iteratively
align a cannula solely based on visual information. This precisely controlled
setup offers insights into limitations and capabilities of this method. Our
policy succeeded on the first attempt in 68.5% of cases, maintaining safe
intra-pedicular trajectories across diverse vertebral levels. The policy
generalized to complex anatomy, including fractures, and remained robust to
varied initializations. Rollouts on real bi-planar X-rays further suggest that
the model can produce plausible trajectories, despite training exclusively in
simulation. While these preliminary results are promising, we also identify
limitations, especially in entry point precision. Full closed-look control will
require additional considerations around how to provide sufficiently frequent
feedback. With more robust priors and domain knowledge, such models may provide
a foundation for future efforts toward lightweight and CT-free robotic
intra-operative spinal navigation.

</details>


### [5] [Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model](https://arxiv.org/abs/2511.03888)
*Abdulmumin Sa'ad,Sulaimon Oyeniyi Adebayo,Abdul Jabbar Siddiqui*

Main category: cs.CV

TL;DR: 本文提出了一种适用于沙漠等特殊环境的、基于改进版YOLOv12的实时垃圾检测框架，能在保障精度的同时实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 全球垃圾危机日益严重，预计到2050年固体垃圾产生量将增加70%，尤其是在偏远或极端环境下，传统垃圾收集方法效率低下且危险。当前自动化垃圾检测研究主要集中于城市环境和可回收物，对有机及危险垃圾、沙漠等地形关注不足。

Method: 作者提出了基于YOLOv12剪枝和轻量化处理，结合自对抗训练（SAT）和特定数据增强策略的检测框架。利用DroneTrashNet数据集进行训练，重点提升了模型效率和适用性，以便在资源受限的空中无人机上运行。

Result: 模型在精度（precision）、召回率（recall）和平均精度均值（mAP）上有显著提升，并保持低延迟和小体积。与主流轻量化YOLO变体对比，展现了更优的准确率与效率平衡。

Conclusion: 本研究验证了数据与模型双向优化的有效性，为沙漠等特殊环境下的实时垃圾检测提供了鲁棒、高效的解决方案。

Abstract: The global waste crisis is escalating, with solid waste generation expected
to increase by 70% by 2050. Traditional waste collection methods, particularly
in remote or harsh environments like deserts, are labor-intensive, inefficient,
and often hazardous. Recent advances in computer vision and deep learning have
opened the door to automated waste detection systems, yet most research focuses
on urban environments and recyclable materials, overlooking organic and
hazardous waste and underexplored terrains such as deserts. In this work, we
propose an enhanced real-time object detection framework based on a pruned,
lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)
and specialized data augmentation strategies. Using the DroneTrashNet dataset,
we demonstrate significant improvements in precision, recall, and mean average
precision (mAP), while achieving low latency and compact model size suitable
for deployment on resource-constrained aerial drones. Benchmarking our model
against state-of-the-art lightweight YOLO variants further highlights its
optimal balance of accuracy and efficiency. Our results validate the
effectiveness of combining data-centric and model-centric enhancements for
robust, real-time waste detection in desert environments.

</details>


### [6] [Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition](https://arxiv.org/abs/2511.03891)
*Hlali Azzeddine,Majid Ben Yakhlef,Soulaiman El Hazzat*

Main category: cs.CV

TL;DR: 提出了一种基于类别的图像合成方法，通过将同一类别的多张图片融合为组合输入图像(CoImg)，以增强训练样本的信息量和类内变化，提高小样本、不均衡数据集下深度学习的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 针对小样本、不均衡且图像质量较差的数据集，深度学习模型误判率高，难以区分细微疾病模式。作者希望通过数据输入重构，提升模型诊断能力。

Method: 将同一类别的多张图片融合为一个3x1布局的组合图像，增强类内方差和信息量。以OCTDL视网膜OCT扫描数据集为例，构建了完美平衡类别的新数据集Co-OCTDL，并利用VGG16模型在原始与新数据集上做对比实验。

Result: 应用该方法的新数据集(Co-OCTDL)下，模型的准确率提升至99.6%，F1分数为0.995，AUC达0.9996，显著优于原始不平衡数据集训练模型。同时，误判率大幅降低。

Conclusion: 基于类别的图像合成显著提升了深度学习模型对小样本、不均衡医学影像数据集的诊断表现，能有效减少误判，是提升弱数据集预测质量的有力工具。

Abstract: Small, imbalanced datasets and poor input image quality can lead to high
false predictions rates with deep learning models. This paper introduces
Class-Based Image Composition, an approach that allows us to reformulate
training inputs through a fusion of multiple images of the same class into
combined visual composites, named Composite Input Images (CoImg). That enhances
the intra-class variance and improves the valuable information density per
training sample and increases the ability of the model to distinguish between
subtle disease patterns. Our method was evaluated on the Optical Coherence
Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et
al., 2024), which contains 2,064 high-resolution optical coherence tomography
(OCT) scans of the human retina, representing seven distinct diseases with a
significant class imbalance. We constructed a perfectly class-balanced version
of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout
composite image. To assess the effectiveness of this new representation, we
conducted a comparative analysis between the original dataset and its variant
using a VGG16 model. A fair comparison was ensured by utilizing the identical
model architecture and hyperparameters for all experiments. The proposed
approach markedly improved diagnostic results.The enhanced Dataset achieved
near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared
to a baseline model trained on raw dataset. The false prediction rate was also
significantly lower, this demonstrates that the method can producehigh-quality
predictions even for weak datasets affected by class imbalance or small sample
size.

</details>


### [7] [I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging](https://arxiv.org/abs/2511.03912)
*Nand Kumar Yadav,Rodrigue Rizk,William CW Chen,KC Santosh*

Main category: cs.CV

TL;DR: 本论文提出了一种无监督的新框架，无需异常标签，通过逐步扩展可信正常样本集，实现医学影像中的未知异常检测。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的异常标注稀缺且专家标注成本高，导致未知异常检测极具挑战性。作者希望在极少监督和无异常标签的情况下，提高医学影像中的异常检测能力和效率。

Method: 以少量已验证的正常图像作为种子，方法通过交替进行轻量级适配器更新和基于不确定性的样本接纳来扩展正常样本集。使用冻结的预训练视觉主干网络和微型卷积适配器，实现高效领域适应。嵌入存入紧凑的核心集，用于高效的k-NN异常打分。采用双重概率门控，结合z-score距离门限和基于SWAG的置信度门限，防止漂移和错误接纳，无需生成式重构或回放缓存。

Result: 在COVID-CXR，ROC-AUC从0.9489提升至0.9982（F1由0.8048至0.9746）；Pneumonia CXR的ROC-AUC从0.6834提升至0.8968；Brain MRI ND-5中ROC-AUC从0.6041提升至0.7269，PR-AUC从0.7539至0.8211，均显著优于现有基线。

Conclusion: 所提方法在无需异常标签和极少人工干预的条件下，有效提升了医学影像未知异常检测的准确性和效率，适用于实际标签稀缺的医学场景。

Abstract: Unknown anomaly detection in medical imaging remains a fundamental challenge
due to the scarcity of labeled anomalies and the high cost of expert
supervision. We introduce an unsupervised, oracle-free framework that
incrementally expands a trusted set of normal samples without any anomaly
labels. Starting from a small, verified seed of normal images, our method
alternates between lightweight adapter updates and uncertainty-gated sample
admission. A frozen pretrained vision backbone is augmented with tiny
convolutional adapters, ensuring rapid domain adaptation with negligible
computational overhead. Extracted embeddings are stored in a compact coreset
enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during
incremental expansion is enforced by dual probabilistic gates, a sample is
admitted into the normal memory only if its distance to the existing coreset
lies within a calibrated z-score threshold, and its SWAG-based epistemic
uncertainty remains below a seed-calibrated bound. This mechanism prevents
drift and false inclusions without relying on generative reconstruction or
replay buffers. Empirically, our system steadily refines the notion of
normality as unlabeled data arrive, producing substantial gains over baselines.
On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on
Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,
ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These
results highlight the effectiveness and efficiency of the proposed framework
for real-world, label-scarce medical imaging applications.

</details>


### [8] [Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization](https://arxiv.org/abs/2511.03943)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

TL;DR: 本论文提出两项互补方法提升时序动作定位的边界检测精度和效率：边界距离回归（BDR）与自适应时序细化（ATR），在多个基准数据集达到更准的边界和更高的mAP，用时和计算量均有节省。


<details>
  <summary>Details</summary>
Motivation: 现有时序动作定位方法在处理不同难度的动作边界时计算分配不均，且多依赖分类方法进行边界定位，面临边界模糊和计算资源浪费问题。论文旨在通过改进边界回归和计算分配方式，提升检测精度和计算效率。

Method: 1. 提出边界距离回归（BDR），用有符号距离回归替代传统分类，提升定位精度并可便捷集成入现有方法。2. 提出自适应时序细化（ATR），基于连续深度选择按需分配计算，实现端到端可微优化。ATR支持用蒸馏训练高效学生模型，降低训练成本。

Result: BDR能使边界预测更锐利，边界峰值提升43%，且集成于现有模型可带来1.8%-3.1%的mAP@0.7提升。ATR在THUMOS14能以更低FLOPs获得更高mAP（56.5%@162G vs 53.6%@198G），对异质性边界和短动作提升更为明显。轻量级学生模型在保持基本性能的同时大幅降低计算成本。四大基准与统计检验充分验证了结果的可靠性。

Conclusion: BDR和ATR为时序动作定位任务带来更加精确的边界检测和高效的计算资源利用，可作为现有方法的通用增强模块。两者结合可显著提升效果并节省计算消耗，对实际应用具重要意义。

Abstract: Temporal action localization requires precise boundary detection; however,
current methods apply uniform computation despite significant variations in
difficulty across boundaries. We present two complementary contributions.
First, Boundary Distance Regression (BDR) provides information-theoretically
optimal localization through signed-distance regression rather than
classification, achieving 43\% sharper boundary peaks. BDR retrofits to
existing methods with approximately 50 lines of code, yielding consistent 1.8
to 3.1\% mAP@0.7 improvements across diverse architectures. Second, Adaptive
Temporal Refinement (ATR) allocates computation via continuous depth selection
$\tau \in [0,1]$, enabling end-to-end differentiable optimization without
reinforcement learning. On THUMOS14, ATR achieves 56.5\% mAP@0.7 at 162G FLOPs,
compared to 53.6\% at 198G for uniform processing, providing a 2.9\%
improvement with 18\% less compute. Gains scale with boundary heterogeneity,
showing 4.2\% improvement on short actions. Training cost is mitigated via
knowledge distillation, with lightweight students retaining 99\% performance at
baseline cost. Results are validated across four benchmarks with rigorous
statistical testing.

</details>


### [9] [Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization](https://arxiv.org/abs/2511.03950)
*Zhejia Cai,Puhua Jiang,Shiwei Mao,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

TL;DR: 本文提出了一种同时优化几何和外观的新型3D重建方法，将高质量的网格几何和细致的颜色融合，用于多视图图像重建场景，兼顾真实感与可编辑性。


<details>
  <summary>Details</summary>
Motivation: 当前3D重建方法往往只注重几何精度或真实感渲染，且几何与外观优化常被割裂，导致下游编辑任务（如重光照和形状变形）难以高质量实现。

Method: 作者提出了一个统一框架，基于高斯引导的可微网格渲染，在优化网格顶点位置、网格面以及顶点颜色的同时，结合图像的光度一致性和法线、深度图的几何正则约束，实现对几何和外观的联合优化。

Result: 该方法实现了高质量的3D重建效果，重建出的对象可直接应用于重光照、形变等多种下游3D编辑任务。

Conclusion: 联合优化几何与外观能有效提升3D重建品质，并极大拓展了后续3D编辑的应用空间。作者计划公开代码，便于社区使用和后续研究。

Abstract: Reconstructing real-world objects from multi-view images is essential for
applications in 3D editing, AR/VR, and digital content creation. Existing
methods typically prioritize either geometric accuracy (Multi-View Stereo) or
photorealistic rendering (Novel View Synthesis), often decoupling geometry and
appearance optimization, which hinders downstream editing tasks. This paper
advocates an unified treatment on geometry and appearance optimization for
seamless Gaussian-mesh joint optimization. More specifically, we propose a
novel framework that simultaneously optimizes mesh geometry (vertex positions
and faces) and vertex colors via Gaussian-guided mesh differentiable rendering,
leveraging photometric consistency from input images and geometric
regularization from normal and depth maps. The obtained high-quality 3D
reconstruction can be further exploit in down-stream editing tasks, such as
relighting and shape deformation. The code will be publicly available upon
acceptance.

</details>


### [10] [A Linear Fractional Transformation Model and Calibration Method for Light Field Camera](https://arxiv.org/abs/2511.03962)
*Zhong Chen,Changfeng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于线性分式变换（LFT）的参数α，用于解耦光场相机主镜头和微透镜阵列，实现了更精确的内部参数标定，包含解析最小二乘解及非线性优化，并验证了其在真实与仿真数据上的有效性。


<details>
  <summary>Details</summary>
Motivation: 光场相机的3D重建高度依赖于内部参数的准确标定，而现有方法难以精确解耦主镜头与微透镜阵列的参数，因此亟需新的高效标定方法。

Method: 提出以LFT参数α为核心的标定模型，并通过最小二乘法求解析解，再进行非线性优化。同时还介绍了从原始图像中提取特征的方法。

Result: 在真实和仿真的实验数据上，所提方法都实现了更高的标定精度，并显著提升了光场图像模拟速度。

Conclusion: 该方法为光场相机的高效、精确标定提供了新思路，并为数据驱动的深度学习方法奠定了更快的数据模拟基础，相关代码已公开。

Abstract: Accurate calibration of internal parameters is a crucial yet challenging
prerequisite for 3D reconstruction using light field cameras. In this paper, we
propose a linear fractional transformation(LFT) parameter $\alpha$ to decoupled
the main lens and micro lens array (MLA). The proposed method includes an
analytical solution based on least squares, followed by nonlinear refinement.
The method for detecting features from the raw images is also introduced.
Experimental results on both physical and simulated data have verified the
performance of proposed method. Based on proposed model, the simulation of raw
light field images becomes faster, which is crucial for data-driven deep
learning methods. The corresponding code can be obtained from the author's
website.

</details>


### [11] [Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images](https://arxiv.org/abs/2511.03970)
*Sam Bahrami,Dylan Campbell*

Main category: cs.CV

TL;DR: 该论文提出了一个新的合成数据集Room Envelopes，用于促进场景结构要素（如墙、地板和天花板）重建任务，并能同时预测可见表面和结构布局。


<details>
  <summary>Details</summary>
Motivation: 现有三维重建方法只能重建在一张或多张图像中可见的表面，容易遗漏被遮挡的区域。尽管生成模型在基于部分观测重建物体有很大进展，但对场景中结构性元素如墙壁、地面等的研究较少，因此有必要专项研究并找到成本更低的解决方案。

Method: 作者构建了一个新的Room Envelopes合成数据集，每张RGB图片都配有两个点云图：一个表示可见表面，另一个表示移除家具后的结构布局表面。该数据集可直接用于监督单目几何估算模型，预测可见表面和结构表面。

Result: 实验验证了利用该数据集可以直接训练前馈式单目模型，同时预测场景首个可见面和首个结构布局面，反映出对场景结构与物体分布的理解。

Conclusion: 该工作为场景结构元素的重建任务提供了新的数据资源和直接监督方法，尤其适用于平面、重复、简单的室内结构要素，推动了相关研究的发展。

Abstract: Modern scene reconstruction methods are able to accurately recover 3D
surfaces that are visible in one or more images. However, this leads to
incomplete reconstructions, missing all occluded surfaces. While much progress
has been made on reconstructing entire objects given partial observations using
generative models, the structural elements of a scene, like the walls, floors
and ceilings, have received less attention. We argue that these scene elements
should be relatively easy to predict, since they are typically planar,
repetitive and simple, and so less costly approaches may be suitable. In this
work, we present a synthetic dataset -- Room Envelopes -- that facilitates
progress on this task by providing a set of RGB images and two associated
pointmaps for each image: one capturing the visible surface and one capturing
the first surface once fittings and fixtures are removed, that is, the
structural layout. As we show, this enables direct supervision for feed-forward
monocular geometry estimators that predict both the first visible surface and
the first layout surface. This confers an understanding of the scene's extent,
as well as the shape and location of its objects.

</details>


### [12] [Simple 3D Pose Features Support Human and Machine Social Scene Understanding](https://arxiv.org/abs/2511.03988)
*Wenshuo Qin,Leyla Isik*

Main category: cs.CV

TL;DR: 本文发现人类在社会互动判断中依赖显式的3D姿态信息，而当前AI视觉模型普遍缺乏这类信息，导致其在人类社会互动识别方面表现不佳。通过引入3D关节点和面部朝向等简单特征，可以显著提高AI模型的社会判断能力。


<details>
  <summary>Details</summary>
Motivation: 虽然人类能够快速从视觉输入中理解他人的社会互动，但现有AI视觉模型很难做到这一点。作者怀疑人类依赖于3D姿态等结构化空间信息，这些通常在AI模型中缺失，因此希望探寻3D姿态对社会互动识别的作用。

Method: 作者利用先进的姿态与深度估计算法，提取视频中人物的3D关节坐标，并用这些信息及其简化特征（如面部三维位置与朝向），与人类社会互动判断进行对比，同时与主流AI视觉模型预测能力进行比较。

Result: 3D关节坐标在社会互动判断方面显著优于大多数AI视觉模型。进一步发现，仅用面部的3D位置和朝向等少量特征，就能匹配完整3D关节信息的表现，并能提升AI模型的预测能力。

Conclusion: 人类社会场景理解依赖于明确的3D姿态表示。将结构化的3D空间特征加入AI视觉模型，能显著提升其社会互动理解表现。这些结果为下一代社会化视觉智能系统设计提供了理论依据。

Abstract: Humans can quickly and effortlessly extract a variety of information about
others' social interactions from visual input, ranging from visuospatial cues
like whether two people are facing each other to higher-level information. Yet,
the computations supporting these abilities remain poorly understood, and
social interaction recognition continues to challenge even the most advanced AI
vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose
information to make social interaction judgments, which is absent in most AI
vision models. To test this, we combined state-of-the-art pose and depth
estimation algorithms to extract 3D joint positions of people in short video
clips depicting everyday human actions and compared their ability to predict
human social interaction judgments with current AI vision models. Strikingly,
3D joint positions outperformed most current AI vision models, revealing that
key social information is available in explicit body position but not in the
learned features of most vision models, including even the layer-wise
embeddings of the pose models used to extract joint positions. To uncover the
critical pose features humans use to make social judgments, we derived a
compact set of 3D social pose features describing only the 3D position and
direction of faces in the videos. We found that these minimal descriptors
matched the predictive strength of the full set of 3D joints and significantly
improved the performance of off-the-shelf AI vision models when combined with
their embeddings. Moreover, the degree to which 3D social pose features were
represented in each off-the-shelf AI vision model predicted the model's ability
to match human social judgments. Together, our findings provide strong evidence
that human social scene understanding relies on explicit representations of 3D
pose and can be supported by simple, structured visuospatial primitives.

</details>


### [13] [CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation](https://arxiv.org/abs/2511.03992)
*Yuwen Tao,Kanglei Zhou,Xin Tan,Yuan Xie*

Main category: cs.CV

TL;DR: 本论文提出了Camera Aware Referring Field（CaRF）方法，显著提升了基于语言的3D高斯空间区域定位任务的多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于语言的3D高斯分割方法在多视角一致性上存在不足，主要由于依赖2D渲染的伪监督和视角特定特征学习，导致单视角过拟合和视角之间表现不一致。

Method: 提出了Camera Aware Referring Field（CaRF）框架，引入了Gaussian Field Camera Encoding（GFCE），将相机几何信息融入高斯-文本交互中，显式建模视角依赖性变化。此外，提出了In Training Paired View Supervision（ITPVS），在训练阶段对标定视角下每个高斯的logits进行对齐，减少了单视角过拟合并优化了多视角一致性。

Result: 在Ref LERF、LERF OVS及3D OVS三个基准数据集上，CaRF方法相较于state-of-the-art提升了mIoU分别达16.8%、4.3%、2.0%。

Conclusion: CaRF大幅度提升了3D高斯分割任务的多视角一致性，结果更可靠，对具身智能、AR/VR交互及自动化感知领域具有潜在应用价值。

Abstract: Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret
free-form language expressions and localize the corresponding 3D regions in
Gaussian fields. While recent advances have introduced cross-modal alignment
between language and 3D geometry, existing pipelines still struggle with
cross-view consistency due to their reliance on 2D rendered pseudo supervision
and view specific feature learning. In this work, we present Camera Aware
Referring Field (CaRF), a fully differentiable framework that operates directly
in the 3D Gaussian space and achieves multi view consistency. Specifically,
CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates
camera geometry into Gaussian text interactions to explicitly model view
dependent variations and enhance geometric reasoning. Building on this, In
Training Paired View Supervision (ITPVS) is proposed to align per Gaussian
logits across calibrated views during training, effectively mitigating single
view overfitting and exposing inter view discrepancies for optimization.
Extensive experiments on three representative benchmarks demonstrate that CaRF
achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of
the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively.
Moreover, this work promotes more reliable and view consistent 3D scene
understanding, with potential benefits for embodied AI, AR/VR interaction, and
autonomous perception.

</details>


### [14] [PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection](https://arxiv.org/abs/2511.03997)
*Peiyao Wang,Weining Wang,Qi Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为PhysCorr的统一框架，针对文本生成视频的不合物理规律问题进行建模、评测与优化，显著提升生成视频的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成技术虽然在视觉质量上有较大提升，但生成内容常常出现物理不真实的问题，如物体动态不合理、交互混乱和运动模式不真实，这限制了其在AI、机器人等领域的应用。

Method: 作者提出了PhysCorr框架，其中包括：1) PhysicsRM：首个双维度奖励模型，用于量化单个物体的稳定性及多个物体之间的交互。2) PhyDPO：一种新颖的直接偏好优化流程，通过对比反馈和物理相关的权重调整，引导模型产出更具物理一致性的视频。该方法具备模型无关性，易于集成进各种主流视频生成模型。

Result: 在多个主流基准集上的实验显示，PhysCorr不仅大幅提升了生成视频的物理真实性，同时保持了视觉质量和语义的一致性。

Conclusion: 本文工作为实现基于物理原则可靠的视频生成迈出了关键一步，有望推动该领域进一步发展。

Abstract: Recent advances in text-to-video generation have achieved impressive
perceptual quality, yet generated content often violates fundamental principles
of physical plausibility - manifesting as implausible object dynamics,
incoherent interactions, and unrealistic motion patterns. Such failures hinder
the deployment of video generation models in embodied AI, robotics, and
simulation-intensive domains. To bridge this gap, we propose PhysCorr, a
unified framework for modeling, evaluating, and optimizing physical consistency
in video generation. Specifically, we introduce PhysicsRM, the first
dual-dimensional reward model that quantifies both intra-object stability and
inter-object interactions. On this foundation, we develop PhyDPO, a novel
direct preference optimization pipeline that leverages contrastive feedback and
physics-aware reweighting to guide generation toward physically coherent
outputs. Our approach is model-agnostic and scalable, enabling seamless
integration into a wide range of video diffusion and transformer-based
backbones. Extensive experiments across multiple benchmarks demonstrate that
PhysCorr achieves significant improvements in physical realism while preserving
visual fidelity and semantic alignment. This work takes a critical step toward
physically grounded and trustworthy video generation.

</details>


### [15] [GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization](https://arxiv.org/abs/2511.04008)
*Mahmoud Soliman,Omar Abdelaziz,Ahmed Radwan,Anand,Mohamed Shehata*

Main category: cs.CV

TL;DR: 本文提出了一种结合图神经网络（GNN）路由与专家混合（MoE）机制的参数高效微调方法（GNN-MoE），通过提升Vision Transformer在看不见领域下的泛化能力，实现了强健而轻量的域泛化效果。


<details>
  <summary>Details</summary>
Motivation: 当前Vision Transformer在域泛化（DG）方面，对于未知领域的适应较弱，而常规的微调方法不仅计算和参数开销大，还可能削弱模型的泛化能力。亟需一种既高效又提升泛化性的微调框架。

Method: 作者提出GNN-MoE方法，将混合专家（MoE）机制与高效的Kronecker适配器相结合。创新点在于采用图神经网络（如GCN、GAT、SAGE）作为专家路由器，对输入图像分割成的patch构建图结构，动态分配给不同专家，实现基于patch间语境关系的专家路由。

Result: GNN-MoE在多个权威域泛化基准上达到SOTA或有竞争力的表现，同时显著减少参数量，实现高效模型微调。

Conclusion: 基于图神经网络的patch上下文感知路由能显著提升ViT在域泛化任务中的表现，为实现高效、健壮的跨域图像识别提供了新思路。

Abstract: Domain generalization (DG) seeks robust Vision Transformer (ViT) performance
on unseen domains. Efficiently adapting pretrained ViTs for DG is challenging;
standard fine-tuning is costly and can impair generalization. We propose
GNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a
Mixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead
of token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT,
SAGE) operates on inter-patch graphs to dynamically assign patches to
specialized experts. This context-aware GNN routing leverages inter-patch
relationships for better adaptation to domain shifts. GNN-MoE achieves
state-of-the-art or competitive DG benchmark performance with high parameter
efficiency, highlighting the utility of graph-based contextual routing for
robust, lightweight DG.

</details>


### [16] [MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging](https://arxiv.org/abs/2511.04016)
*Mahmoud Soliman,Islam Osman,Mohamed S. Shehata,Rasika Rajapakshe*

Main category: cs.CV

TL;DR: 本文提出了专为胸部医学影像设计的ViT基础模型MedDChest，通过大规模医学影像数据集自监督训练，配合内容感知型数据增强策略，在多个下游任务上显著优于常用ImageNet预训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉模型多依赖于在自然图像（如ImageNet）上的预训练骨干网络，导致存在领域不匹配问题，影响医学影像任务的表现。因此需要开发针对医学影像领域特定的基础模型。

Method: 作者从零开始，基于超过120万张多模态胸部医学影像（包括X射线和CT）构建ViT模型MedDChest，并提出了Guided Random Resized Crops的内容感知数据增强方法，将采样引导集中于更具解剖学相关性的区域，提高训练效率和数据利用率。

Result: 大量下游医学诊断任务实验证明，MedDChest在性能上远超基于ImageNet的主流预训练视觉模型，展示出较强的迁移能力和鲁棒性。

Conclusion: 在医学影像领域，基于大规模同域数据的预训练结合专属数据增强策略，能获得显著优于传统方法的特征表示。MedDChest模型为胸部影像诊断等任务提供了更优的起点，模型权重也将公开以促进后续研究与应用。

Abstract: The performance of vision models in medical imaging is often hindered by the
prevailing paradigm of fine-tuning backbones pre-trained on out-of-domain
natural images. To address this fundamental domain gap, we propose MedDChest, a
new foundational Vision Transformer (ViT) model optimized specifically for
thoracic imaging. We pre-trained MedDChest from scratch on a massive, curated,
multimodal dataset of over 1.2 million images, encompassing different
modalities including Chest X-ray and Computed Tomography (CT) compiled from 10
public sources. A core technical contribution of our work is Guided Random
Resized Crops, a novel content-aware data augmentation strategy that biases
sampling towards anatomically relevant regions, overcoming the inefficiency of
standard cropping techniques on medical scans. We validate our model's
effectiveness by fine-tuning it on a diverse set of downstream diagnostic
tasks. Comprehensive experiments empirically demonstrate that MedDChest
significantly outperforms strong, publicly available ImageNet-pretrained
models. By establishing the superiority of large-scale, in-domain pre-training
combined with domain-specific data augmentation, MedDChest provides a powerful
and robust feature extractor that serves as a significantly better starting
point for a wide array of thoracic diagnostic tasks. The model weights will be
made publicly available to foster future research and applications.

</details>


### [17] [Near-Lossless 3D Voxel Representation Free from Iso-surface](https://arxiv.org/abs/2511.04029)
*Yihao Luo,Xianglong He,Chuanyu Pan,Yiwen Chen,Jiaqi Wu,Yangguang Li,Wanli Ouyang,Yuanming Hu,Guang Yang,ChoonHwai Yap*

Main category: cs.CV

TL;DR: 本文提出了一种高效且高保真的稀疏体素化3D网格表示方法（Faithful Contouring），相比以往等值面方法无需网格密封或渲染优化，不牺牲几何精度，支持超过2048分辨率。该方法在表示和重建任务上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于等值面的3D体素化方法需要网格密封或渲染优化，导致几何细节损失、效率低下，不适合复杂结构的忠实三维重建或生成。

Method: 提出Faithful Contouring框架，将任意3D网格直接转为稀疏高分辨率体素表示，无需中间场函数转换与等值面提取，保持几何锐度和内部结构。设计了支持其的双模自编码器，实现可扩展的、细节保真的形状重建。

Result: 直接表示的距离误差达到10^-5数量级；网格重建对比主流基线，Chamfer距离减少93%，F-score提升35%。

Conclusion: Faithful Contouring作为一种高保真、灵活的3D网格表示方法，不仅在准确度和效率上优于已有体素化方法，也适用于3D学习相关任务的数据表示和重建。

Abstract: Accurate and efficient voxelized representations of 3D meshes are the
foundation of 3D reconstruction and generation. However, existing
representations based on iso-surface heavily rely on water-tightening or
rendering optimization, which inevitably compromise geometric fidelity. We
propose Faithful Contouring, a sparse voxelized representation that supports
2048+ resolutions for arbitrary meshes, requiring neither converting meshes to
field functions nor extracting the isosurface during remeshing. It achieves
near-lossless fidelity by preserving sharpness and internal structures, even
for challenging cases with complex geometry and topology. The proposed method
also shows flexibility for texturing, manipulation, and editing. Beyond
representation, we design a dual-mode autoencoder for Faithful Contouring,
enabling scalable and detail-preserving shape reconstruction. Extensive
experiments show that Faithful Contouring surpasses existing methods in
accuracy and efficiency for both representation and reconstruction. For direct
representation, it achieves distance errors at the $10^{-5}$ level; for mesh
reconstruction, it yields a 93\% reduction in Chamfer Distance and a 35\%
improvement in F-score over strong baselines, confirming superior fidelity as a
representation for 3D learning tasks.

</details>


### [18] [A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals](https://arxiv.org/abs/2511.04037)
*Arfina Rahman,Mahesh Banavar*

Main category: cs.CV

TL;DR: 本文提出了一种基于低帧率指尖视频PPG信号的轻量级生物特征认证框架，利用混合深度学习模型实现了98%的认证准确率。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备和移动设备的普及，亟需一种非侵入性、低成本、具备活体检测能力的生物识别技术。PPG信号因其天然的活体检测与易于获取等优势，成为理想选择，但其易受运动伪影、照明变化和个体差异干扰，亟需更鲁棒的特征提取和分类方法。

Method: 采用采样率14 Hz的低帧率指尖视频记录PPG信号，经过基线漂移移除、PCA运动伪影抑制、带通滤波、傅里叶重采样和幅值归一化等预处理；将一维PPG段通过连续小波变换（CWT）变为二维时频图；提出CVT-ConvMixer-LSTM混合深度学习模型，结合视觉Transformer、ConvMixer和LSTM，提取时空特征并完成认证。

Result: 在CFIHSR数据集的46个受试者上实验，认证准确率达98%；结果表明该模型对噪声与受试者间差异具有较强鲁棒性。

Conclusion: 该方法高效、易扩展，具备天然活体检测能力，适合移动和嵌入式生物识别安全应用。

Abstract: Photoplethysmography (PPG) signals, which measure changes in blood volume in
the skin using light, have recently gained attention in biometric
authentication because of their non-invasive acquisition, inherent liveness
detection, and suitability for low-cost wearable devices. However, PPG signal
quality is challenged by motion artifacts, illumination changes, and
inter-subject physiological variability, making robust feature extraction and
classification crucial. This study proposes a lightweight and cost-effective
biometric authentication framework based on PPG signals extracted from
low-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings
from 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The
raw PPG signals undergo a standard preprocessing pipeline involving baseline
drift removal, motion artifact suppression using Principal Component Analysis
(PCA), bandpass filtering, Fourier-based resampling, and amplitude
normalization. To generate robust representations, each one-dimensional PPG
segment is converted into a two-dimensional time-frequency scalogram via the
Continuous Wavelet Transform (CWT), effectively capturing transient
cardiovascular dynamics. We developed a hybrid deep learning model, termed
CVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision
Transformer (CVT) and ConvMixer branches with temporal features from a Long
Short-Term Memory network (LSTM). The experimental results on 46 subjects
demonstrate an authentication accuracy of 98%, validating the robustness of the
model to noise and variability between subjects. Due to its efficiency,
scalability, and inherent liveness detection capability, the proposed system is
well-suited for real-world mobile and embedded biometric security applications.

</details>


### [19] [Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment](https://arxiv.org/abs/2511.04078)
*Zehui Feng,Chenqi Zhang,Mingru Wang,Minuo Wei,Shiwei Cheng,Cuntai Guan,Ting Han*

Main category: cs.CV

TL;DR: 本文提出Bratrix框架，通过语言引导视觉-脑信号对齐，有效提升了脑电（EEG）、脑磁（MEG）、功能磁共振（fMRI）等神经信号与视觉语义的联合解析能力。实验结果显示在多项任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于不同个体之间的差异和视觉特征的复杂交缠，直接将脑信号与视觉特征对齐常常难以揭示深层次语义并且稳健性有限。本文旨在提升神经信号与视觉语义映射的可解释性与鲁棒性。

Method: 提出Bratrix框架，将视觉刺激分解为视觉和语言两个层级语义，通过投影到共享潜空间，实现视觉-语言及脑信号-语言的嵌入对齐。此外，通过不确定性感知模块权重处理神经噪声，并采用单模态预训练和多模态微调的两阶段训练策略。

Result: 在EEG、MEG、fMRI等多组公开基准数据集上，Bratrix在检索、重建和描述生成任务中均超过现有方法。在200-way EEG检索任务上提升了14.3%。

Conclusion: Bratrix通过引入语言锚定和不确定性建模，显著改善了神经信号和视觉语义的对齐效果，有助于更深入、稳健地解读大脑视觉语义表征。代码和模型已开源。

Abstract: Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI
remains a fundamental challenge due to subject variability and the entangled
nature of visual features. Existing approaches primarily align neural activity
directly with visual embeddings, but visual-only representations often fail to
capture latent semantic dimensions, limiting interpretability and deep
robustness. To address these limitations, we propose Bratrix, the first
end-to-end framework to achieve multimodal Language-Anchored Vision-Brain
alignment. Bratrix decouples visual stimuli into hierarchical visual and
linguistic semantic components, and projects both visual and brain
representations into a shared latent space, enabling the formation of aligned
visual-language and brain-language embeddings. To emulate human-like perceptual
reliability and handle noisy neural signals, Bratrix incorporates a novel
uncertainty perception module that applies uncertainty-aware weighting during
alignment. By leveraging learnable language-anchored semantic matrices to
enhance cross-modal correlations and employing a two-stage training strategy of
single-modality pretraining followed by multimodal fine-tuning, Bratrix-M
improves alignment precision. Extensive experiments on EEG, MEG, and fMRI
benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and
captioning performance compared to state-of-the-art methods, specifically
surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.

</details>


### [20] [Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score](https://arxiv.org/abs/2511.04083)
*Abu Hanif Muhammad Syarubany*

Main category: cs.CV

TL;DR: 本文比较了两种无配对训练与自监督学习下的CT图像去噪方法：基于CycleGAN的残差转换器和Noise2Score（N2S）分数匹配去噪器。实验表明，优化配置下的CycleGAN方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 在实际CT图像去噪问题中，干净-有噪声配对数据常常难以获得，因此开发无需配对或自监督的高效训练方法显得尤为重要。

Method: 提出并评估了两种数据高效的CT图像去噪技术：一是依据CycleGAN的残差转换器，通过配置选择U-Net骨架和合适的损失权重（lambda_cycle和lambda_iden）；二是基于Noise2Score的自监督分数匹配网络。两者均在统一协议下系统比较。

Result: 经过最佳参数优化和更长训练后，CycleGAN将输入图像的PSNR从34.66 dB提升到38.913 dB，SSIM从0.9234提升到0.971。在未见测试集上也保持了优异表现。Noise2Score虽在绝对指标上略逊，但在极噪场景下有明显增益。

Conclusion: CycleGAN在最终图像质量上优势明显，适合追求高质量去噪应用；而Noise2Score则为无法获得干净配对数据时提供极具竞争力且稳健的备选方案。

Abstract: We study CT image denoising in the unpaired and self-supervised regimes by
evaluating two strong, training-data-efficient paradigms: a CycleGAN-based
residual translator and a Noise2Score (N2S) score-matching denoiser. Under a
common evaluation protocol, a configuration sweep identifies a simple standard
U-Net backbone within CycleGAN (lambda_cycle = 30, lambda_iden = 2, ngf = ndf =
64) as the most reliable setting; we then train it to convergence with a longer
schedule. The selected CycleGAN improves the noisy input from 34.66 dB / 0.9234
SSIM to 38.913 dB / 0.971 SSIM and attains an estimated score of 1.9441 and an
unseen-set (Kaggle leaderboard) score of 1.9343. Noise2Score, while slightly
behind in absolute PSNR / SSIM, achieves large gains over very noisy inputs,
highlighting its utility when clean pairs are unavailable. Overall, CycleGAN
offers the strongest final image quality, whereas Noise2Score provides a robust
pair-free alternative with competitive performance. Source code is available at
https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score.

</details>


### [21] [When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation](https://arxiv.org/abs/2511.04084)
*Nishchal Sapkota,Haoyan Shi,Yejia Zhang,Xianshi Ma,Bofang Zheng,Danny Z. Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的医学图像分割架构UKAST，将Kolmogorov-Arnold网络（KANs）集成到Swin Transformer中，实现了更高效且表现优异的分割能力。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对精准诊断和治疗规划至关重要，但因解剖结构复杂和标注数据有限，现有方法存在局限。CNN擅长本地特征但难以捕捉全局信息，Transformer能建模长程依赖但对数据和计算资源需求高。

Method: 设计了类似U-Net的新架构UKAST，将以有理函数为基础的Kolmogorov-Arnold网络（尤其是GR-KANs）嵌入到Swin Transformer编码器中，提升表达能力和数据效率，同时减少计算量，参数量略有增加。

Result: 在四个不同2D和3D医学图像分割基准上，UKAST均优于CNN和Transformer基线方法，尤其在数据稀缺场景表现突出，克服了标准Vision Transformer的数据依赖性。

Conclusion: KAN增强的Transformer能够实现更高效、数据友好的医学图像分割，有助于推动领域进步。

Abstract: Medical image segmentation is critical for accurate diagnostics and treatment
planning, but remains challenging due to complex anatomical structures and
limited annotated training data. CNN-based segmentation methods excel at local
feature extraction, but struggle with modeling long-range dependencies.
Transformers, on the other hand, capture global context more effectively, but
are inherently data-hungry and computationally expensive. In this work, we
introduce UKAST, a U-Net like architecture that integrates rational-function
based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By
leveraging rational base functions and Group Rational KANs (GR-KANs) from the
Kolmogorov-Arnold Transformer (KAT), our architecture addresses the
inefficiencies of vanilla spline-based KANs, yielding a more expressive and
data-efficient framework with reduced FLOPs and only a very small increase in
parameter count compared to SwinUNETR. UKAST achieves state-of-the-art
performance on four diverse 2D and 3D medical image segmentation benchmarks,
consistently surpassing both CNN- and Transformer-based baselines. Notably, it
attains superior accuracy in data-scarce settings, alleviating the data-hungry
limitations of standard Vision Transformers. These results show the potential
of KAN-enhanced Transformers to advance data-efficient medical image
segmentation. Code is available at: https://github.com/nsapkota417/UKAST

</details>


### [22] [SpatialLock: Precise Spatial Control in Text-to-Image Synthesis](https://arxiv.org/abs/2511.04112)
*Biao Liu,Yuanzhi Liang*

Main category: cs.CV

TL;DR: 该论文提出了SpatialLock框架，有效提升了文本生成图片时的空间位置控制能力，实现了精准的目标定位，实验中在多个数据集上的IOU得分均超过0.9。


<details>
  <summary>Details</summary>
Motivation: 尽管文本生成图像（T2I）技术发展迅速，但在生成图像时对物体位置的精准控制仍然存在挑战，现有方法未能充分利用空间位置信息，导致生成物体的空间布局感知不足。

Method: 提出了SpatialLock新框架，包括Position-Engaged Injection（PoI）与Position-Guided Learning（PoG）两个核心模块。PoI在注意力层内直接注入空间信息，帮助模型学习物体锚定位置；PoG则通过感知信号进行监督，进一步优化物体定位。两个模块结合，以实现空间位置生成的精确控制。

Result: SpatialLock在多个数据集上进行了实验，能够生成目标空间布局准确的图像，目标定位的IOU分数均超过0.9，刷新了该领域的最新水平。

Conclusion: SpatialLock利用空间信息和感知信号，有效提升了文本生成图像中物体位置的控制精度和图像质量，为相关应用提供了更优方案。

Abstract: Text-to-Image (T2I) synthesis has made significant advancements in recent
years, driving applications such as generating datasets automatically. However,
precise control over object localization in generated images remains a
challenge. Existing methods fail to fully utilize positional information,
leading to an inadequate understanding of object spatial layouts. To address
this issue, we propose SpatialLock, a novel framework that leverages perception
signals and grounding information to jointly control the generation of spatial
locations. SpatialLock incorporates two components: Position-Engaged Injection
(PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial
information through an attention layer, encouraging the model to learn the
grounding information effectively. PoG employs perception-based supervision to
further refine object localization. Together, these components enable the model
to generate objects with precise spatial arrangements and improve the visual
quality of the generated images. Experiments show that SpatialLock sets a new
state-of-the-art for precise object positioning, achieving IOU scores above 0.9
across multiple datasets.

</details>


### [23] [BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems](https://arxiv.org/abs/2511.04388)
*Chang Liu,Juan Li,Sheng Zhang,Chang Liu,Jie Li,Xu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新型的轻量级单目深度估计模型BoRe-Depth，在嵌入式系统上实现高效且高质量的深度估计，特别是在物体边界方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的单目深度估计算法虽然成本低，但在嵌入式设备上表现较差，尤其是深度估计精度低、物体边界模糊。因此亟需设计一种兼具高效和高性能的模型。

Method: 1. 设计了增强特征自适应融合模块（EFAF），自适应融合深度特征，提升边界细节表示能力；2. 在编码器中融合语义知识，增强物体识别与边界感知；3. 在NVIDIA Jetson Orin嵌入式设备上进行部署。

Result: BoRe-Depth模型参数仅8.7M，在Jetson Orin上可达50.7帧每秒，边界质量显著优于以往轻量级模型，在多个标准数据集上均取得领先表现，并通过消融实验验证方法有效性。

Conclusion: BoRe-Depth显著提升了嵌入式系统上单目深度估计的精度及边界质量，兼具高效率和优良性能，适用于资源受限的无人系统3D感知场景。

Abstract: Depth estimation is one of the key technologies for realizing 3D perception
in unmanned systems. Monocular depth estimation has been widely researched
because of its low-cost advantage, but the existing methods face the challenges
of poor depth estimation performance and blurred object boundaries on embedded
systems. In this paper, we propose a novel monocular depth estimation model,
BoRe-Depth, which contains only 8.7M parameters. It can accurately estimate
depth maps on embedded systems and significantly improves boundary quality.
Firstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which
adaptively fuses depth features to enhance boundary detail representation.
Secondly, we integrate semantic knowledge into the encoder to improve the
object recognition and boundary perception capabilities. Finally, BoRe-Depth is
deployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We
demonstrate that the proposed model significantly outperforms previous
lightweight models on multiple challenging datasets, and we provide detailed
ablation studies for the proposed methods. The code is available at
https://github.com/liangxiansheng093/BoRe-Depth.

</details>


### [24] [Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration](https://arxiv.org/abs/2511.04117)
*Yunghee Lee,Byeonghyun Pak,Junwha Hong,Hoseong Kim*

Main category: cs.CV

TL;DR: 本文提出了一种名为Tortoise and Hare Guidance（THG）的无训练加速扩散采样策略，可以在保持高生成保真的同时，显著减少计算量，减少高达30%的函数评估次数，且几乎无质量损失。


<details>
  <summary>Details</summary>
Motivation: 传统的扩散模型采样在提升生成图像质量时，计算量大，效率低。作者发现现有方法在噪声估计与引导项的数值误差敏感性方面存在冗余，传统数值求解器未加以利用。因此，通过挖掘多速率ODE系统的计算冗余，有望加速采样过程。

Method: 作者将无分类器引导（CFG）相关ODE重构为多速率系统，分别对噪声估计和引导项采用不同步长积分策略：对噪声估计在细步长网格上积分（Tortoise），对引导项只在粗步长网格上积分（Hare）。此外，提出了错误界自适应步长采样器和引导规模调度器，以自适应步长、稳定推理过程。

Result: THG方法在几乎不损失生成质量的前提下，最大可减少30%的函数评估次数（NFE减少），并在相同计算预算下优于同类无训练CFG加速方法。

Conclusion: 多速率ODE系统的思想极大提升了扩散采样器效率，为无须再训练的高效高质量图像合成方案提供了新方向，并有望推动实时高质量扩散生成的实际应用。

Abstract: In this paper, we propose Tortoise and Hare Guidance (THG), a training-free
strategy that accelerates diffusion sampling while maintaining high-fidelity
generation. We demonstrate that the noise estimate and the additional guidance
term exhibit markedly different sensitivity to numerical error by reformulating
the classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our
error-bound analysis shows that the additional guidance branch is more robust
to approximation, revealing substantial redundancy that conventional solvers
fail to exploit. Building on this insight, THG significantly reduces the
computation of the additional guidance: the noise estimate is integrated with
the tortoise equation on the original, fine-grained timestep grid, while the
additional guidance is integrated with the hare equation only on a coarse grid.
We also introduce (i) an error-bound-aware timestep sampler that adaptively
selects step sizes and (ii) a guidance-scale scheduler that stabilizes large
extrapolation spans. THG reduces the number of function evaluations (NFE) by up
to 30% with virtually no loss in generation fidelity ($\Delta$ImageReward
$\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free
accelerators under identical computation budgets. Our findings highlight the
potential of multirate formulations for diffusion solvers, paving the way for
real-time high-quality image synthesis without any model retraining. The source
code is available at https://github.com/yhlee-add/THG.

</details>


### [25] [Text to Sketch Generation with Multi-Styles](https://arxiv.org/abs/2511.04123)
*Tengjie Li,Shikui Tu,Lei Xu*

Main category: cs.CV

TL;DR: 本工作提出了一种基于扩散模型的无训练框架，能够通过文本提示和参考素描实现精确的风格控制，并有效提升生成素描的质量和风格对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在素描生成上取得进展，但主要关注通用生成，缺乏对素描风格的精确控制。如何根据文本提示和参考素描实现可控、多样化的风格生成是一个尚未解决的问题。

Method: 本文提出了一种基于扩散模型、无需额外训练的框架。通过文本提示与参考素描做风格引导，与传统风格迁移方法不同，作者未直接覆盖自注意力机制中的关键和值矩阵，而是将参考特征通过线性平滑作为辅助信息，并引入风格-内容引导机制，减少参考素描的内容泄漏。同时，通过联合AdaIN模块，首次支持多风格可控生成。

Result: 大量实验表明，该方法在素描质量、风格准确对齐和风格控制灵活性等方面均取得了优异的性能。特别是在参考与目标素描结构差异较大时，依然表现出高质量和高度的风格一致性。

Conclusion: 该方法为素描生成提供了高效、灵活、精确的风格控制能力，为后续相关研究和应用奠定了坚实基础。作者已开源代码，便于社区复现和进一步改进。

Abstract: Recent advances in vision-language models have facilitated progress in sketch
generation. However, existing specialized methods primarily focus on generic
synthesis and lack mechanisms for precise control over sketch styles. In this
work, we propose a training-free framework based on diffusion models that
enables explicit style guidance via textual prompts and referenced style
sketches. Unlike previous style transfer methods that overwrite key and value
matrices in self-attention, we incorporate the reference features as auxiliary
information with linear smoothing and leverage a style-content guidance
mechanism. This design effectively reduces content leakage from reference
sketches and enhances synthesis quality, especially in cases with low
structural similarity between reference and target sketches. Furthermore, we
extend our framework to support controllable multi-style generation by
integrating features from multiple reference sketches, coordinated via a joint
AdaIN module. Extensive experiments demonstrate that our approach achieves
high-quality sketch generation with accurate style alignment and improved
flexibility in style control. The official implementation of M3S is available
at https://github.com/CMACH508/M3S.

</details>


### [26] [Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)](https://arxiv.org/abs/2511.04126)
*Venkata Manikanta Desu,Syed Fawaz Ali*

Main category: cs.CV

TL;DR: 本文提出了一套完整的自动网球比赛分析流程，结合多个深度学习模型实现实时检测和跟踪球员与网球，并识别球场关键点，最终输出带注释的视频和详细的比赛分析指标。


<details>
  <summary>Details</summary>
Motivation: 目前网球比赛分析依赖人工视频标注和传统跟踪方法，效率低且难以获得细致数据。因此，本文希望通过深度学习技术实现自动化、精准的比赛数据采集和分析，以帮助教练、转播方和选手获得更深入的比赛洞察。

Method: 本框架集成了多种深度学习模型，包括：使用YOLOv8实现球员检测，定制YOLOv5完成网球跟踪，基于ResNet50架构实现球场关键点检测。系统可实时输出球员运动轨迹、球速、击球准确性与反应时间等多项关键指标。

Result: 实验表明，该系统在不同场地环境和比赛场景下均展现出稳健的性能。能够准确输出注释视频和详尽比赛数据分析，适用于实际应用。

Conclusion: 自动化的网球比赛分析系统能够为教练、转播方和运动员带来可操作的比赛洞见，有效提升分析效率和全面性，在实际网球运动分析中具备较高实用价值。

Abstract: This study presents a complete pipeline for automated tennis match analysis.
Our framework integrates multiple deep learning models to detect and track
players and the tennis ball in real time, while also identifying court
keypoints for spatial reference. Using YOLOv8 for player detection, a
custom-trained YOLOv5 model for ball tracking, and a ResNet50-based
architecture for court keypoint detection, our system provides detailed
analytics including player movement patterns, ball speed, shot accuracy, and
player reaction times. The experimental results demonstrate robust performance
in varying court conditions and match scenarios. The model outputs an annotated
video along with detailed performance metrics, enabling coaches, broadcasters,
and players to gain actionable insights into the dynamics of the game.

</details>


### [27] [DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms](https://arxiv.org/abs/2511.04128)
*Shengyu Tang,Zeyuan Lu,Jiazhi Dong,Changdong Yu,Xiaoyu Wang,Yaohui Lyu,Weihao Xia*

Main category: cs.CV

TL;DR: 本论文提出了一种高效的并行多目标跟踪框架DMSORT，专为复杂海事环境下的多目标跟踪(MOT)而设计，通过引入动态相机运动估计和多级特征融合，实现了对目标身份和运动的鲁棒追踪，并在新加坡海事数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 海事环境下MOT面对复杂背景、相机抖动和视觉退化等多重挑战，现有方法很难在动态环境中兼顾运行效率、身份一致性与鲁棒性。因此，亟需一种既高效又能实现鲁棒多目标跟踪的方法以提升船只导航和海上监控的安全性与有效性。

Method: 提出DMSORT框架，其包括两大分支：一是结合了RCDN检测网络和轻量级Transformer特征提取器的目标检测与ReID分支，负责通过多级特征与全局上下文信息实现鲁棒检测和身份特征提取；二是针对平台动态的相机运动估计分支，通过投影变换与Kalman滤波对相机运动进行补偿，从而稳定目标轨迹。最后通过聚类优化的特征融合模块，将运动与外观信息充分结合，提升跟踪中的鲁棒性与身份一致性。

Result: 在Singapore Maritime Dataset上进行了大量实验，结果显示DMSORT在各类ReID型MOT框架中运行速度最快，并能在抖动、遮挡等极端条件下保持高身份一致性和极佳的鲁棒性，达到了当前最优性能水平。

Conclusion: DMSORT通过并行跟踪结构、相机运动补偿及高效的特征融合，显著提升了海事多目标跟踪的效率与识别鲁棒性，可为海上船舶导航与智能监控提供有效技术支撑，并公开了代码以推动领域发展。

Abstract: Accurate perception of the marine environment through robust multi-object
tracking (MOT) is essential for ensuring safe vessel navigation and effective
maritime surveillance. However, the complicated maritime environment often
causes camera motion and subsequent visual degradation, posing significant
challenges to MOT. To address this challenge, we propose an efficient
Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the
framework is a parallel tracker with affine compensation, which incorporates an
object detection and re-identification (ReID) branch, along with a dedicated
branch for dynamic camera motion estimation. Specifically, a Reversible
Columnar Detection Network (RCDN) is integrated into the detection module to
leverage multi-level visual features for robust object detection. Furthermore,
a lightweight Transformer-based appearance extractor (Li-TAE) is designed to
capture global contextual information and generate robust appearance features.
Another branch decouples platform-induced and target-intrinsic motion by
constructing a projective transformation, applying platform-motion compensation
within the Kalman filter, and thereby stabilizing true object trajectories.
Finally, a clustering-optimized feature fusion module effectively combines
motion and appearance cues to ensure identity consistency under noise,
occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset
demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT
attains the fastest runtime among existing ReID-based MOT frameworks while
maintaining high identity consistency and robustness to jitter and occlusion.
Code is available at:
https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.

</details>


### [28] [Learning from Online Videos at Inference Time for Computer-Use Agents](https://arxiv.org/abs/2511.04137)
*Yujian Liu,Ze Wang,Hao Chen,Ximeng Sun,Xiaodong Yu,Jialian Wu,Jiang Liu,Emad Barsoum,Zicheng Liu,Shiyu Chang*

Main category: cs.CV

TL;DR: 该论文提出了一种让计算机代理在推理时有效学习在线视频教程的方法，从视频中提取并筛选实际操作轨迹，动态指导任务执行，显著提升代理完成复杂应用任务的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管计算机代理在自动化任务方面取得进步，但在需要领域特定知识和多步骤流程的任务上仍劣于人类。人类用户能通过观看视频教程快速获取所需的操作策略，因此作者希望模仿这一过程，使代理也能借助在线视频获得指导。

Method: 提出了一套框架：首先检索和筛选相关的在线视频教程；利用多模态大模型（VLM）将视频转为结构化的操作演示轨迹，并将视频分割为短动作序列并分配文字目标；在代理推理过程中，采用两阶段选择机制，动态为每一步选择最相关的轨迹作为上下文指导。

Result: 在两个主流基准任务上的实验表明，该方法持续优于只用文本/转录教程和其他变体。分析指出，分段选择、动作筛选和视觉信息对提升代理效果起到了关键作用。

Conclusion: 证明了在线视频资源可被系统化为高效的操作指导，显著提升计算机代理在实际应用中的表现，并指出行动轨迹的精细提取与选择对效果尤为关键。

Abstract: Computer-use agents can operate computers and automate laborious tasks, but
despite recent rapid progress, they still lag behind human users, especially
when tasks require domain-specific procedural knowledge about particular
applications, platforms, and multi-step workflows. Humans can bridge this gap
by watching video tutorials: we search, skim, and selectively imitate short
segments that match our current subgoal. In this paper, we study how to enable
computer-use agents to learn from online videos at inference time effectively.
We propose a framework that retrieves and filters tutorial videos, converts
them into structured demonstration trajectories, and dynamically selects
trajectories as in-context guidance during execution. Particularly, using a
VLM, we infer UI actions, segment videos into short subsequences of actions,
and assign each subsequence a textual objective. At inference time, a two-stage
selection mechanism dynamically chooses a single trajectory to add in context
at each step, focusing the agent on the most helpful local guidance for its
next decision. Experiments on two widely used benchmarks show that our
framework consistently outperforms strong base agents and variants that use
only textual tutorials or transcripts. Analyses highlight the importance of
trajectory segmentation and selection, action filtering, and visual
information, suggesting that abundant online videos can be systematically
distilled into actionable guidance that improves computer-use agents at
inference time. Our code is available at
https://github.com/UCSB-NLP-Chang/video_demo.

</details>


### [29] [Seeing Straight: Document Orientation Detection for Efficient OCR](https://arxiv.org/abs/2511.04161)
*Suranjan Goswami,Abhinav Ravi,Raja Kolla,Ali Faraz,Shaharukh Khan,Akash,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: 本文提出了旋转校正的新基准和方法，对提升OCR的准确性至关重要，特别是在拍摄或扫描文档存在旋转误差时。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，扫描或拍摄文档经常因为用户操作失误导致方向不正确。这种失误会严重影响后续如OCR等任务的表现，因此亟需高效、准确的文档旋转检测与校正方法。

Method: 作者构建了OCR-Rotation-Bench (ORB) 基准，包括ORB-En（英文结构化及自由格式数据集）和ORB-Indic（覆盖11种印度本地中低资源语言）。同时，提出了基于Phi-3.5-Vision视觉编码器并动态裁剪图片的高效4分类旋转检测方法，并对模型进行了专门微调。

Result: 提出的方法在ORB-En和ORB-Indic数据集上的旋转检测准确率分别达到96%和92%。该模块还显著提升了OCR模型准确率：封闭源OCR提升最高可达14%，开源模型提升最高可达4倍。

Conclusion: 高效且轻量级的旋转校正模块不仅能有效识别文档旋转情况，更能极大提升实际OCR任务的表现，尤其对多语种、低资源场景意义重大。

Abstract: Despite significant advances in document understanding, determining the
correct orientation of scanned or photographed documents remains a critical
pre-processing step in the real world settings. Accurate rotation correction is
essential for enhancing the performance of downstream tasks such as Optical
Character Recognition (OCR) where misalignment commonly arises due to user
errors, particularly incorrect base orientations of the camera during capture.
In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for
evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from
rotation-transformed structured and free-form English OCR datasets, and (ii)
ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource
languages. We also present a fast, robust and lightweight rotation
classification pipeline built on the vision encoder of Phi-3.5-Vision model
with dynamic image cropping, fine-tuned specifically for 4-class rotation task
in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy
on identifying the rotations respectively on both the datasets. Beyond
classification, we demonstrate the critical role of our module in boosting OCR
performance: closed-source (up to 14%) and open-weights models (up to 4x) in
the simulated real-world setting.

</details>


### [30] [Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology](https://arxiv.org/abs/2511.04171)
*Fatemehzahra Darzi,Rodrigo Escobar Diaz Guerrero,Thomas Bocklitz*

Main category: cs.CV

TL;DR: 本文评估了不同颜色变换方法对病理图像配准效果的影响，发现CycleGAN变换能显著提升多模态图像配准的精度。


<details>
  <summary>Details</summary>
Motivation: 数字病理分析中，准确地将不同染色或成像模态的图像配准到同一坐标系，对后续生物标记分析和组织重建等应用至关重要。但不同模态之间的图像在颜色空间上差异较大，配准效果易受影响，因此需要探索优化配准的方法。

Method: 研究使用20对组织切片，包括H&E染色图像和非线性多模态图像。对每对图像进行多种预处理（如CycleGAN、Macenko、Reinhard、Vahadane颜色变换，反色、对比度调整、归一化、去噪），然后采用VALIS方法配准（包含刚性和多步非刚性配准）。用rTRE指标和人工选点法评估配准精度，分别测试原始和反色的多模态图像。

Result: 无论用原图还是反色图像，CycleGAN颜色变换均能获得最低的配准误差，而其他方法配准误差较高。

Conclusion: 在数字病理多模态图像配准前应用颜色变换（尤其是CycleGAN）能显著提升配准效果，为更可靠的后续分析提供技术支撑。

Abstract: Image registration refers to the process of spatially aligning two or more
images by mapping them into a common coordinate system, so that corresponding
anatomical or tissue structures are matched across images. In digital
pathology, registration enables direct comparison and integration of
information from different stains or imaging modalities, sup-porting
applications such as biomarker analysis and tissue reconstruction. Accurate
registration of images from different modalities is an essential step in
digital pathology. In this study, we investigated how various color
transformation techniques affect image registration between hematoxylin and
eosin (H&E) stained images and non-linear multimodal images. We used a dataset
of 20 tissue sample pairs, with each pair undergoing several preprocessing
steps, including different color transformation (CycleGAN, Macenko, Reinhard,
Vahadane), inversion, contrast adjustment, intensity normalization, and
denoising. All images were registered using the VALIS registration method,
which first applies rigid registration and then performs non-rigid registration
in two steps on both low and high-resolution images. Registration performance
was evaluated using the relative Target Registration Error (rTRE). We reported
the median of median rTRE values (MMrTRE) and the average of median rTRE values
(AMrTRE) for each method. In addition, we performed a custom point-based
evaluation using ten manually selected key points. Registration was done
separately for two scenarios, using either the original or inverted multimodal
images. In both scenarios, CycleGAN color transformation achieved the lowest
registration errors, while the other methods showed higher errors. These
findings show that applying color transformation before registration improves
alignment between images from different modalities and supports more reliable
analysis in digital pathology.

</details>


### [31] [Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification](https://arxiv.org/abs/2511.04190)
*Josef Mayr,Anna Reithmeir,Maxime Di Folco,Julia A. Schnabel*

Main category: cs.CV

TL;DR: 本文评估了协方差描述符在医学图像分类中的有效性，尤其结合了预训练的通用视觉编码器（如DINOv2、MedSAM）的特征，并用SPDNet进行分类。结果显示，基于这些编码器特征的协方差描述符显著优于手工特征，并推动医学图像分析取得更好性能。


<details>
  <summary>Details</summary>
Motivation: 协方差描述符已在通用计算机视觉任务中表现良好，但在医学图像领域尚未被深入研究。该研究旨在探索协方差描述符在医学图像分类中的潜力，尤其关注与预训练视觉编码器结合带来的提升。

Method: 本文基于两种预训练通用视觉编码器（DINOv2、MedSAM）提取医学图像特征，通过构建协方差描述符，利用SPDNet进行类别判别。并与传统手工特征的协方差描述符进行对比，涵盖MedMNIST基准中的11个二分类与多分类数据集。

Result: 实验显示，基于预训练编码器提取特征所构建的协方差描述符在所有数据集上表现均优于手工特征；其中，结合DINOv2特征与SPDNet的方案优于当前主流方法（state-of-the-art）。

Conclusion: 通过将协方差描述符和强大的视觉预训练编码器结合，可以显著提升医学图像分析的性能。这一方法值得在医学计算机视觉领域进一步探索和推广。

Abstract: Covariance descriptors capture second-order statistics of image features.
They have shown strong performance in general computer vision tasks, but remain
underexplored in medical imaging. We investigate their effectiveness for both
conventional and learning-based medical image classification, with a particular
focus on SPDNet, a classification network specifically designed for symmetric
positive definite (SPD) matrices. We propose constructing covariance
descriptors from features extracted by pre-trained general vision encoders
(GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and
MedSAM - are evaluated across eleven binary and multi-class datasets from the
MedMNSIT benchmark. Our results show that covariance descriptors derived from
GVE features consistently outperform those derived from handcrafted features.
Moreover, SPDNet yields superior performance to state-of-the-art methods when
combined with DINOv2 features. Our findings highlight the potential of
combining covariance descriptors with powerful pretrained vision encoders for
medical image analysis.

</details>


### [32] [AStF: Motion Style Transfer via Adaptive Statistics Fusor](https://arxiv.org/abs/2511.04192)
*Hanmo Chen,Chenghao Xu,Jiexi Yan,Cheng Deng*

Main category: cs.CV

TL;DR: 本文提出了一种新的运动风格迁移方法，通过引入高阶统计量（偏度和峰度）提升风格表现力，在实验中超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有运动风格迁移方法多借鉴图像风格迁移，主要处理均值和方差，但这些低阶统计量不足以捕捉运动数据的复杂动态模式和时空一致性。为此，作者希望通过加入更多统计量提升模型能力。

Method: 提出了Adaptive Statistics Fusor (AStF) 框架，包含风格解耦模块（Style Disentanglement Module）和高阶多统计量注意力机制（High-Order Multi-Statistics Attention, HOS-Attn），结合Motion Consistency Regularization (MCR) 鉴别器进行训练，以更全面地建模动态风格中的时空统计特征。

Result: 实验表明，所提方法较现有技术在运动风格迁移任务上的表现有明显提升，并具有更强风格表现和动态一致性。

Conclusion: 高阶统计量（如偏度和峰度）在运动风格建模中极为重要，AStF方法可有效提升运动风格迁移的表现力，现已开源。

Abstract: Human motion style transfer allows characters to appear less rigidity and
more realism with specific style. Traditional arbitrary image style transfer
typically process mean and variance which is proved effective. Meanwhile,
similar methods have been adapted for motion style transfer. However, due to
the fundamental differences between images and motion, relying on mean and
variance is insufficient to fully capture the complex dynamic patterns and
spatiotemporal coherence properties of motion data. Building upon this, our key
insight is to bring two more coefficient, skewness and kurtosis, into the
analysis of motion style. Specifically, we propose a novel Adaptive Statistics
Fusor (AStF) which consists of Style Disentanglement Module (SDM) and
High-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in
conjunction with a Motion Consistency Regularization (MCR) discriminator.
Experimental results show that, by providing a more comprehensive model of the
spatiotemporal statistical patterns inherent in dynamic styles, our proposed
AStF shows proficiency superiority in motion style transfers over
state-of-the-arts. Our code and model are available at
https://github.com/CHMimilanlan/AStF.

</details>


### [33] [MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection](https://arxiv.org/abs/2511.04255)
*Marawan Elbatel,Anbang Wang,Keyuan Liu,Kaouther Mouheb,Enrique Almar-Munoz,Lizhuo Lin,Yanqi Yang,Karim Lekadir,Xiaomeng Li*

Main category: cs.CV

TL;DR: 该论文将人类中心的基础模型（如Sapiens）迁移用于医学图像的解剖标志点检测，提出了MedSapiens模型，在多个数据集上取得了新的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像中的标志点检测传统上依赖于专用模型，但大规模人类中心基础视觉模型的发展提供了迁移的新机遇。作者想探索这类模型在医学领域的潜力，并认为其空间姿态定位能力有助于解剖标志点检测。

Method: 作者将Sapiens（为姿态估计设计的人类中心基础模型）通过多数据集预训练，迁移到医学图像标志点检测任务，并开发出MedSapiens模型，随后在多个数据集上进行基准测试，与现有的通用和专用模型对比。

Result: MedSapiens在成功检测率（SDR）指标上取得显著提升——相对通用模型提升最高可达5.26%，相对专用模型提升最高达21.81%；在小样本设置下，相较few-shot最佳方法也提升了2.69%。

Conclusion: 人类中心的基础模型具备优秀的空间定位能力，将其迁移至医学解剖标志点检测任务能显著提升性能，这一潜能此前未被充分挖掘。

Abstract: This paper does not introduce a novel architecture; instead, it revisits a
fundamental yet overlooked baseline: adapting human-centric foundation models
for anatomical landmark detection in medical imaging. While landmark detection
has traditionally relied on domain-specific models, the emergence of
large-scale pre-trained vision models presents new opportunities. In this
study, we investigate the adaptation of Sapiens, a human-centric foundation
model designed for pose estimation, to medical imaging through multi-dataset
pretraining, establishing a new state of the art across multiple datasets. Our
proposed model, MedSapiens, demonstrates that human-centric foundation models,
inherently optimized for spatial pose localization, provide strong priors for
anatomical landmark detection, yet this potential has remained largely
untapped. We benchmark MedSapiens against existing state-of-the-art models,
achieving up to 5.26% improvement over generalist models and up to 21.81%
improvement over specialist models in the average success detection rate (SDR).
To further assess MedSapiens adaptability to novel downstream tasks with few
annotations, we evaluate its performance in limited-data settings, achieving
2.69% improvement over the few-shot state of the art in SDR. Code and model
weights are available at https://github.com/xmed-lab/MedSapiens .

</details>


### [34] [Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery](https://arxiv.org/abs/2511.04260)
*Claudio Giusti,Luca Guarnera,Sebastiano Battiato*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Proto-LeakNet的新方法，有效检测和溯源AI合成图像及Deepfake来源，比现有方法更鲁棒且可解释性更好。


<details>
  <summary>Details</summary>
Motivation: 深度学习生成的合成图像（如Deepfake）的检测与溯源日益重要，因为它们对社会和技术构成威胁。目前主要挑战是如何区分不同生成模型，并检测未知的新型生成器。研究发现扩散模型在生成图像时会留下独特但微弱的信号（称为signal leaks），可用于此目的。

Method: 作者提出Proto-LeakNet，通过在扩散模型的潜在空间中，复现部分前向扩散过程，暴露生成器特异的残余信号。系统架构中，利用时序注意力编码器聚合多步潜特征，并通过特征加权原型头组织嵌入空间，实现闭集合分类与密度为基础的开放集合识别。该方法只需在闭集合（已知来源）数据上训练，但用学习到的潜在特征也能高效区分未知生成器。

Result: Proto-LeakNet在仅用闭集合数据训练的情况下，取得了98.13%的Macro AUC，性能超过现有最优方法。其嵌入空间在图像被后处理后依然保持很好的区分度，对已知与未知生成器的分离效果显著。

Conclusion: 在潜在空间中建模生成信号泄漏，可以实现对AI合成及Deepfake图像的可靠、可解释溯源与鉴别。Proto-LeakNet为生成模型取证提供了新途径，并且在泛化能力和透明性上具备优势。

Abstract: The growing sophistication of synthetic image and deepfake generation models
has turned source attribution and authenticity verification into a critical
challenge for modern computer vision systems. Recent studies suggest that
diffusion pipelines unintentionally imprint persistent statistical traces,
known as signal leaks, within their outputs, particularly in latent
representations. Building on this observation, we propose Proto-LeakNet, a
signal-leak-aware and interpretable attribution framework that integrates
closed-set classification with a density-based open-set evaluation on the
learned embeddings, enabling analysis of unseen generators without retraining.
Operating in the latent domain of diffusion models, our method re-simulates
partial forward diffusion to expose residual generator-specific cues. A
temporal attention encoder aggregates multi-step latent features, while a
feature-weighted prototype head structures the embedding space and enables
transparent attribution. Trained solely on closed data and achieving a Macro
AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under
post-processing, surpassing state-of-the-art methods, and achieves strong
separability between known and unseen generators. These results demonstrate
that modeling signal-leak bias in latent space enables reliable and
interpretable AI-image and deepfake forensics. The code for the whole work will
be available upon submission.

</details>


### [35] [DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification](https://arxiv.org/abs/2511.04281)
*Yujie Yang,Shuang Li,Jun Ye,Neng Dong,Fan Li,Huafeng Li*

Main category: cs.CV

TL;DR: 本文提出了一个新的视频可见光-红外行人再识别框架DinoGRL，通过结合步态特征与外观特征，有效提升了跨模态检索的表现，实验证明该方法优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有VVI-ReID主要利用模态无关的视觉特征，但很少引入步态特征，后者不仅模态无关还具备丰富时序信息，缺乏步态建模使得难以获得跨模态时空一致性强的特征。

Method: 提出DinoGRL框架，利用DINOv2视觉先验，增强步态与外观特征互补。设计SASGL模型使用DINOv2提供的语义先验生成、增强轮廓（步态）特征，并与ReID目标联合优化；再通过PBMGE模块实现步态和外观流的多尺度、双向交互，逐步细化特征表达。

Result: 在HITSZ-VCM和BUPT等数据集开展大量实验，结果显示本文方法在精度等指标上明显超越现有最优方案。

Conclusion: 所提DinoGRL框架能有效结合外观与步态信息，提升跨模态行人视频再识别效果，具备较高实用价值和应用前景。

Abstract: Video-based Visible-Infrared person re-identification (VVI-ReID) aims to
retrieve the same pedestrian across visible and infrared modalities from video
sequences. Existing methods tend to exploit modality-invariant visual features
but largely overlook gait features, which are not only modality-invariant but
also rich in temporal dynamics, thus limiting their ability to model the
spatiotemporal consistency essential for cross-modal video matching. To address
these challenges, we propose a DINOv2-Driven Gait Representation Learning
(DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn
gait features complementary to appearance cues, facilitating robust
sequence-level representations for cross-modal retrieval. Specifically, we
introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which
generates and enhances silhouette representations with general-purpose semantic
priors from DINOv2 and jointly optimizes them with the ReID objective to
achieve semantically enriched and task-adaptive gait feature learning.
Furthermore, we develop a Progressive Bidirectional Multi-Granularity
Enhancement (PBMGE) module, which progressively refines feature representations
by enabling bidirectional interactions between gait and appearance streams
across multiple spatial granularities, fully leveraging their complementarity
to enhance global representations with rich local details and produce highly
discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets
demonstrate the superiority of our approach, significantly outperforming
existing state-of-the-art methods.

</details>


### [36] [FastGS: Training 3D Gaussian Splatting in 100 Seconds](https://arxiv.org/abs/2511.04283)
*Shiwei Ren,Tianci Wen,Yongchun Fang,Biao Lu*

Main category: cs.CV

TL;DR: 本文提出了FastGS，一种用于3D高斯点云加速的新框架，通过多视角一致性的机制，有效控制训练过程中高斯数量，实现了训练速度的大幅提升并保证了渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting的加速方法无法在训练过程中有效调控高斯点数量，导致计算资源浪费和训练过程低效，因此需要一种高效调控数量并兼顾质量的方案。

Method: 提出FastGS框架，采用基于多视角一致性的高斯点密集化与修剪策略，无需额外设定预算机制，在保证渲染质量的情况下智能筛选关键高斯点，以提高训练效率。

Result: 在Mip-NeRF 360、Tanks & Temples和Deep Blending等数据集上，FastGS以3.32倍的训练加速（相对DashGaussian）、15.45倍加速（相对原始3DGS），同时保持可比的渲染质量。此外，该方法在包括动态场景重建、稀疏视图重建等多任务中实现了2到7倍的训练加速。

Conclusion: FastGS在训练速度和渲染质量上都显著优于已有方法，具有良好的通用性和扩展性，是提升3D高斯点云相关任务效率的有效方案。

Abstract: The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to
properly regulate the number of Gaussians during training, causing redundant
computational time overhead. In this paper, we propose FastGS, a novel, simple,
and general acceleration framework that fully considers the importance of each
Gaussian based on multi-view consistency, efficiently solving the trade-off
between training time and rendering quality. We innovatively design a
densification and pruning strategy based on multi-view consistency, dispensing
with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks &
Temples, and Deep Blending datasets demonstrate that our method significantly
outperforms the state-of-the-art methods in training speed, achieving a
3.32$\times$ training acceleration and comparable rendering quality compared
with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\times$ acceleration
compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that
FastGS exhibits strong generality, delivering 2-7$\times$ training acceleration
across various tasks, including dynamic scene reconstruction, surface
reconstruction, sparse-view reconstruction, large-scale reconstruction, and
simultaneous localization and mapping. The project page is available at
https://fastgs.github.io/

</details>


### [37] [Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment](https://arxiv.org/abs/2511.04288)
*Leire Benito-Del-Valle,Artzai Picón,Daniel Mugica,Manuel Ramos,Eva Portillo,Javier Romero,Carlos Javier Jimenez,Ramón Navarra-Mestre*

Main category: cs.CV

TL;DR: 本文提出了一种面向除草剂田间试验的领域特定视觉基础模型，通过自监督学习方法优化图像表征，在植物种类识别及药害评估任务上显著提升了准确率，并有效降低了人工标注成本，为田间试验提供了可扩展的自动化解决方案。


<details>
  <summary>Details</summary>
Motivation: 农业场景（尤其是除草剂田间试验）对植物种类和药害类型的区分要求极高，而通用视觉基础模型难以满足此类细粒度识别需求，因此亟需研发适用于这类任务的领域特定方法。

Method: 作者将通用视觉基础模型迁移并自监督训练在大规模高质量农业数据集上，从而学习更适合除草剂试验图像的特征。同时，探讨了标签用量（低标注）下的表现与分割准确性。

Result: 领域特定模型在植物种类识别和损伤分类的F1分数上分别优于通用模型，即使在未见过的新环境和领域迁移（如无人机图像）下也有显著提升。此外，在注释样本较少的条件下，达到更高精度且所需标注量减少80%。

Conclusion: 领域特定视觉基础模型在农业除草剂试验分析方面展现出更好的泛化能力，大幅降低了人工标注需求，有望推动大规模农业图像分析自动化。

Abstract: Herbicide field trials require accurate identification of plant species and
assessment of herbicide-induced damage across diverse environments. While
general-purpose vision foundation models have shown promising results in
complex visual domains, their performance can be limited in agriculture, where
fine-grained distinctions between species and damage types are critical.
  In this work, we adapt a general-purpose vision foundation model to herbicide
trial characterization. Trained using a self-supervised learning approach on a
large, curated agricultural dataset, the model learns rich and transferable
representations optimized for herbicide trials images.
  Our domain-specific model significantly outperforms the best general-purpose
foundation model in both species identification (F1 score improvement from 0.91
to 0.94) and damage classification (from 0.26 to 0.33). Under unseen conditions
(new locations and other time), it achieves even greater gains (species
identification from 0.56 to 0.66; damage classification from 0.17 to 0.27). In
domain-shift scenarios, such as drone imagery, it maintains strong performance
(species classification from 0.49 to 0.60).
  Additionally, we show that domain-specific pretraining enhances segmentation
accuracy, particularly in low-annotation regimes. An annotation-efficiency
analysis reveals that, under unseen conditions, the domain-specific model
achieves 5.4% higher F1 score than the general-purpose model, while using 80%
fewer labeled samples.
  These results demonstrate the generalization capabilities of domain-specific
foundation models and their potential to significantly reduce manual annotation
efforts, offering a scalable and automated solution for herbicide trial
analysis.

</details>


### [38] [Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data](https://arxiv.org/abs/2511.04304)
*Robin Spanier,Thorsten Hoeser,Claudia Kuenzer*

Main category: cs.CV

TL;DR: 作者利用合成与真实卫星影像结合，提升深度学习模型对海上基础设施（如平台）的检测能力，并验证该方法在不同地区的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着海上基础设施（如风电、油气平台等）的迅速扩张，需要有效的监测手段，而现有模型受限于样本不均衡及某些类型样本稀缺。

Method: 使用YOLOv10目标检测模型，结合合成数据和真实Sentinel-1卫星影像，在全球多个海域进行训练，并以区域留出法在未见过的地区评估模型泛化能力。

Result: 在未见过的三个地区共检测出3,529个海上平台，模型F1分数由0.85提升到0.90，合成数据特别提升了模型对不均衡类别的识别能力。

Conclusion: 合成数据的引入显著提升了模型泛化与识别罕见类别的能力，这种方法为全球化、可扩展的海上基础设施监测提供了新的解决路径。

Abstract: The recent and ongoing expansion of marine infrastructure, including offshore
wind farms, oil and gas platforms, artificial islands, and aquaculture
facilities, highlights the need for effective monitoring systems. The
development of robust models for offshore infrastructure detection relies on
comprehensive, balanced datasets, but falls short when samples are scarce,
particularly for underrepresented object classes, shapes, and sizes. By
training deep learning-based YOLOv10 object detection models with a combination
of synthetic and real Sentinel-1 satellite imagery acquired in the fourth
quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of
Guinea, and Coast of Brazil), this study investigates the use of synthetic
training data to enhance model performance. We evaluated this approach by
applying the model to detect offshore platforms in three unseen regions (Gulf
of Mexico, North Sea, Persian Gulf) and thereby assess geographic
transferability. This region-holdout evaluation demonstrated that the model
generalises beyond the training areas. In total, 3,529 offshore platforms were
detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and
1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which
improved to 0.90 upon incorporating synthetic data. We analysed how synthetic
data enhances the representation of unbalanced classes and overall model
performance, taking a first step toward globally transferable detection of
offshore infrastructure. This study underscores the importance of balanced
datasets and highlights synthetic data generation as an effective strategy to
address common challenges in remote sensing, demonstrating the potential of
deep learning for scalable, global offshore infrastructure monitoring.

</details>


### [39] [RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation](https://arxiv.org/abs/2511.04317)
*Xiangjun Zhang,Litong Gong,Yinglin Zheng,Yansong Liu,Wentao Jiang,Mingyi Xu,Biao Wang,Tiezheng Ge,Ming Zeng*

Main category: cs.CV

TL;DR: 本文提出了RISE-T2V框架，通过整合提示词重述和语义特征提取为一步，提升了文本生成视频（T2V）扩散模型对于用户简洁提示的理解和视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有T2V扩散模型依赖已训练文本编码器，但对简短提示的理解有限，难以准确表达用户意图，并且无法动态重述提示词，导致模型可用性和可扩展性受限。

Method: RISE-T2V引入了Rephrasing Adapter模块，将提示词重述与语义特征提取合并为一步。该模块在大语言模型（LLM）预测下一个token时提取文本隐藏状态，作为视频扩散模型的条件，实现提示词的隐式重述，更好地匹配用户意图。该方法兼容多种预训练LLM和视频扩散模型。

Result: 大量实验表明，RISE-T2V可无缝应用于不同架构的视频扩散模型，大幅提升T2V模型对提示词的理解和视频生成的质量，更好地贴合用户意图。

Conclusion: RISE-T2V是一个通用、灵活的文本生成视频新框架，通过提示词重述适配器显著提高了扩散模型的表现力和对用户意图的把握能力，对T2V任务有重要促进作用。

Abstract: Most text-to-video(T2V) diffusion models depend on pre-trained text encoders
for semantic alignment, yet they often fail to maintain video quality when
provided with concise prompts rather than well-designed ones. The primary issue
lies in their limited textual semantics understanding. Moreover, these text
encoders cannot rephrase prompts online to better align with user intentions,
which limits both the scalability and usability of the models, To address these
challenges, we introduce RISE-T2V, which uniquely integrates the processes of
prompt rephrasing and semantic feature extraction into a single and seamless
step instead of two separate steps. RISE-T2V is universal and can be applied to
various pre-trained LLMs and video diffusion models(VDMs), significantly
enhancing their capabilities for T2V tasks. We propose an innovative module
called the Rephrasing Adapter, enabling diffusion models to utilize text hidden
states during the next token prediction of the LLM as a condition for video
generation. By employing a Rephrasing Adapter, the video generation model can
implicitly rephrase basic prompts into more comprehensive representations that
better match the user's intent. Furthermore, we leverage the powerful
capabilities of LLMs to enable video generation models to accomplish a broader
range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a
versatile framework applicable to different video diffusion model
architectures, significantly enhancing the ability of T2V models to generate
high-quality videos that align with user intent. Visual results are available
on the webpage at https://rise-t2v.github.io.

</details>


### [40] [Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography](https://arxiv.org/abs/2511.04334)
*Saúl Alonso-Monsalve,Leigh H. Whitehead,Adam Aurisano,Lorena Escudero Sanchez*

Main category: cs.CV

TL;DR: 本文提出了一种结合体素稀疏化和亚流形稀疏卷积网络的新方法，实现了对高分辨率医学影像（如CT）的高效肿瘤自动分割，并显著降低了计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 在临床中，肿瘤在放射影像（如CT）中的精确分割非常耗时且依赖专业技术，是定量分析普及的主要瓶颈。因此，非常需要高效、自动化的分割方法以推动医学分析自动化发展。

Method: 提出了一种两步法：首先对3D影像进行体素稀疏化处理；其次使用亚流形稀疏卷积网络进行原生3D架构分割，从而在无须大幅降采样和切分patch的情况下处理高分辨率CT数据。

Result: 在KiTS23肾癌影像公开数据集上，该方法对肾脏+肿块、肿瘤+囊肿和肿瘤分别达到了95.8%、85.7%、80.3%的Dice系数，并且显著减少了推理时间（降低60%）和显存占用（降低最高75%），在性能上接近竞赛获奖方法。

Conclusion: 该方法不仅在肿瘤自动分割精度上达到先进水平，更在计算效率上大幅优于传统3D dense架构，为高分辨率医学影像的临床自动化分析提供了切实可行且高效的技术途径。

Abstract: The accurate delineation of tumours in radiological images like Computed
Tomography is a very specialised and time-consuming task, and currently a
bottleneck preventing quantitative analyses to be performed routinely in the
clinical setting. For this reason, developing methods for the automated
segmentation of tumours in medical imaging is of the utmost importance and has
driven significant efforts in recent years. However, challenges regarding the
impracticality of 3D scans, given the large amount of voxels to be analysed,
usually requires the downsampling of such images or using patches thereof when
applying traditional convolutional neural networks. To overcome this problem,
in this paper we propose a new methodology that uses, divided into two stages,
voxel sparsification and submanifold sparse convolutional networks. This method
allows segmentations to be performed with high-resolution inputs and a native
3D model architecture, obtaining state-of-the-art accuracies while
significantly reducing the computational resources needed in terms of GPU
memory and time. We studied the deployment of this methodology in the context
of Computed Tomography images of renal cancer patients from the KiTS23
challenge, and our method achieved results competitive with the challenge
winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%
for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also
offers significant computational improvements, achieving up to a 60% reduction
in inference time and up to a 75\% reduction in VRAM usage compared to an
equivalent dense architecture, across both CPU and various GPU cards tested.

</details>


### [41] [Comparative Study of CNN Architectures for Binary Classification of Horses and Motorcycles in the VOC 2008 Dataset](https://arxiv.org/abs/2511.04344)
*Muhammad Annas Shaikh,Hamza Zaman,Arbaz Asif*

Main category: cs.CV

TL;DR: 本论文评估了9种卷积神经网络架构在VOC 2008数据集上对马和摩托车二分类任务的表现，并通过数据增强方法缓解类别不平衡问题。ConvNeXt-Tiny表现最佳。


<details>
  <summary>Details</summary>
Motivation: 在现实数据集中，正负类分布往往严重不均衡（即类别不平衡），这直接影响了二分类任务的检测性能。因此作者希望量化主流神经网络架构及数据增强技术在不平衡二分类上的效果。

Method: 作者选取了包括ResNet-50、ConvNeXt-Tiny、DenseNet-121和Vision Transformer等9种现代网络结构，在VOC 2008的马和摩托车二分类数据集上进行训练。为缓解数据不平衡问题，增加了少数类的数据增强方法。各模型在多个性能指标下进行对比实验分析。

Result: 不同网络结构性能差异显著，其中ConvNeXt-Tiny在马检测任务上AP达到95.53%，在摩托车检测任务上AP达到89.12%，居于首位。数据增强尤为提升了深层网络对少数类的检测性能。

Conclusion: 不同架构在不平衡二分类任务中的表现有较大差异，数据增强对于提升少数类检测尤为有效。本文为不平衡二分类场景下的模型架构选择和数据增强策略提供了实证依据。

Abstract: This paper presents a comprehensive evaluation of nine convolutional neural
network architectures for binary classification of horses and motorcycles in
the VOC 2008 dataset. We address the significant class imbalance problem by
implementing minority-class augmentation techniques. Our experiments compare
modern architectures including ResNet-50, ConvNeXt-Tiny, DenseNet-121, and
Vision Transformer across multiple performance metrics. Results demonstrate
substantial performance variations, with ConvNeXt-Tiny achieving the highest
Average Precision (AP) of 95.53% for horse detection and 89.12% for motorcycle
detection. We observe that data augmentation significantly improves minority
class detection, particularly benefiting deeper architectures. This study
provides insights into architecture selection for imbalanced binary
classification tasks and quantifies the impact of data augmentation strategies
in mitigating class imbalance issues in object detection.

</details>


### [42] [Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection](https://arxiv.org/abs/2511.04347)
*Sanjay Kumar,Tim Brophy,Eoin Martino Grua,Ganesh Sistu,Valentina Donzella,Ciaran Eising*

Main category: cs.CV

TL;DR: 本文研究了传感器遮挡对3D目标检测性能的影响，发现BEV融合模型更依赖LiDAR，强调需改进遮挡感知及融合方法。


<details>
  <summary>Details</summary>
Motivation: 虽然BEV（鸟瞰图）融合方法在多传感器3D感知中表现突出，但对于传感器因环境遮挡（如雾、障碍物等）带来的性能下降问题研究不足，应更好理解不同传感器在遮挡条件下对检测的影响。

Method: 本文采用BEVFusion架构，在nuScenes数据集上，通过人为设置摄像头和激光雷达的遮挡，分析不同遮挡对3D检测性能（mAP和NDS）的影响，分别评估单一传感器和多传感器融合下的性能变动。

Result: 摄像头单独遮挡时mAP下降41.3%（35.6%至20.9%）；激光雷达仅在严重遮挡下性能大幅下降47.3%（64.7%至34.1%），尤其影响远距离目标检测。多传感器融合情况下，遮挡摄像头mAP仅下降4.1%，但遮挡激光雷达下降26.8%，显示融合模型对LiDAR的高依赖。

Conclusion: 现有BEV融合模型在传感器被遮挡时，尤其依赖LiDAR，暴露出鲁棒性不足。应开展更多遮挡感知评测与提升融合方法的研究，以增强在恶劣环境下的检测稳定性。

Abstract: Accurate 3D object detection is essential for automated vehicles to navigate
safely in complex real-world environments. Bird's Eye View (BEV)
representations, which project multi-sensor data into a top-down spatial
format, have emerged as a powerful approach for robust perception. Although
BEV-based fusion architectures have demonstrated strong performance through
multimodal integration, the effects of sensor occlusions, caused by
environmental conditions such as fog, haze, or physical obstructions, on 3D
detection accuracy remain underexplored. In this work, we investigate the
impact of occlusions on both camera and Light Detection and Ranging (LiDAR)
outputs using the BEVFusion architecture, evaluated on the nuScenes dataset.
Detection performance is measured using mean Average Precision (mAP) and the
nuScenes Detection Score (NDS). Our results show that moderate camera
occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is
based only on the camera. On the other hand, LiDAR sharply drops in performance
only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%),
with a severe impact on long-range detection. In fused settings, the effect
depends on which sensor is occluded: occluding the camera leads to a minor 4.1%
drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8%
drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task
of 3D object detection. Our results highlight the need for future research into
occlusion-aware evaluation methods and improved sensor fusion techniques that
can maintain detection accuracy in the presence of partial sensor failure or
degradation due to adverse environmental conditions.

</details>


### [43] [A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications](https://arxiv.org/abs/2511.04349)
*Puneet Mishra,Martijntje Vollebregt,Yizhou Ma,Maria Font-i-Furnols*

Main category: cs.CV

TL;DR: 该论文介绍了如何利用现有的开源深度学习模型，从成像数据中提取空间信息，并结合其他数据源进行分析，提供了详细的MATLAB代码教程，方便分析化学领域的研究者实际操作。


<details>
  <summary>Details</summary>
Motivation: 传统图像处理与化学计量方法在分析材料空间信息时，存在效率和深度特征提取难题。尽管深度学习技术在成像处理领域进展迅速，但分析化学界对这些方法的实际应用有限，主要由于缺乏详细、易操作的实施指导。

Method: 论文以教程形式，基于开源深度学习模型，详细讲解如何从分析化学常见的成像数据（包括多种成像模态）中，用MATLAB代码实现深度特征的自动提取与与其他数据的集成。重心在于直接利用预训练模型进行“特征提取”，而不是图像处理模型的训练。

Result: 通过提供详细的代码和实操流程，提升了分析化学领域科研人员的成像数据空间信息处理能力，让非人工智能领域的研究者能够直接上手深度学习技术。

Conclusion: 本教程有效降低了深度学习模型在分析化学领域的技术门槛，为科研实践提供了高效的图像空间信息处理工具，有助于推动深度学习在分析化学领域的广泛应用。

Abstract: Background In analytical chemistry, spatial information about materials is
commonly captured through imaging techniques, such as traditional color cameras
or with advanced hyperspectral cameras and microscopes. However, efficiently
extracting and analyzing this spatial information for exploratory and
predictive purposes remains a challenge, especially when using traditional
chemometric methods. Recent advances in deep learning and artificial
intelligence have significantly enhanced image processing capabilities,
enabling the extraction of multiscale deep features that are otherwise
challenging to capture with conventional image processing techniques. Despite
the wide availability of open-source deep learning models, adoption in
analytical chemistry remains limited because of the absence of structured,
step-by-step guidance for implementing these models.
  Results This tutorial aims to bridge this gap by providing a step-by-step
guide for applying deep learning approaches to extract spatial information from
imaging data and integrating it with other data sources, such as spectral
information. Importantly, the focus of this work is not on training deep
learning models for image processing but on using existing open source models
to extract deep features from imaging data.
  Significance The tutorial provides MATLAB code tutorial demonstrations,
showcasing the processing of imaging data from various imaging modalities
commonly encountered in analytical chemistry. Readers must run the tutorial
steps on their own datasets using the codes presented in this tutorial.

</details>


### [44] [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570)
*Jingqi Tong,Yurong Mou,Hangcheng Li,Mingzhe Li,Yongzhuo Yang,Ming Zhang,Qiguang Chen,Tianyi Liang,Xiaomeng Hu,Yining Zheng,Xinchi Chen,Jun Zhao,Xuanjing Huang,Xipeng Qiu*

Main category: cs.CV

TL;DR: 本文提出了“视觉视频思维”新范式，通过引入视频生成模型（如Sora-2），实现语言与视觉推理的统一，在多个任务上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 目前“文字思维”“图像思维”虽然提升了多模态模型的推理能力，但图像仅能表达单一时刻，无法表示动态过程，同时文本与视觉仍是割裂的两种模态，难以实现真正统一。

Method: 作者提出“Thinking with Video”范式，利用视频生成模型Sora-2，通过Video Thinking Benchmark（VideoThinkBench）对包括视觉与文本主导的多类任务进行统一评测和分析，提升时序信息捕捉能力，探索视频模型推理能力的来源，并引入自洽与上下文学习提升表现。

Result: Sora-2在视觉任务上与现有SOTA VLMs相当，甚至在部分任务超越VLM（如Eyeballing Games），文本任务上MATH达92%准确率、MMMU达75.53%。验证了Sora-2视频生成模型在多模态推理的有效性。

Conclusion: 视频生成模型有望成为统一多模态理解与生成平台，“思维视频”将成为新一代多模态推理范式。

Abstract: "Thinking with Text" and "Thinking with Images" paradigm significantly
improve the reasoning ability of large language models (LLMs) and Vision
Language Models (VLMs). However, these paradigms have inherent limitations. (1)
Images capture only single moments and fail to represent dynamic processes or
continuous changes, and (2) The separation of text and vision as distinct
modalities, hindering unified multimodal understanding and generation. To
overcome these limitations, we introduce "Thinking with Video", a new paradigm
that leverages video generation models, such as Sora-2, to bridge visual and
textual reasoning in a unified temporal framework. To support this exploration,
we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench
encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing
Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our
evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,
Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even
surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric
tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.
Furthermore, we systematically analyse the source of these abilities. We also
find that self-consistency and in-context learning can improve Sora-2's
performance. In summary, our findings demonstrate that the video generation
model is the potential unified multimodal understanding and generation model,
positions "thinking with video" as a unified multimodal reasoning paradigm.

</details>


### [45] [Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA](https://arxiv.org/abs/2511.04384)
*Itbaan Safwan,Muhammad Annas Shaikh,Muhammad Haaris,Ramail Khan,Muhammad Atif Tahir*

Main category: cs.CV

TL;DR: 本文提出了一个多任务框架，利用LoRA微调的Florence-2模型，同时实现医学视觉问答、解释生成和视觉定位，在准确性和定位能力上优于单任务模型。


<details>
  <summary>Details</summary>
Motivation: 医学领域中的视觉问答任务不仅要求模型给出准确的答案，还需要提供可解释的理由和精确的视觉定位，但现有方法通常侧重于单一任务，无法兼顾理解、推理和可解释性。

Method: 作者构建了一个多任务模型，基于LoRA微调的Florence-2大模型，同步进行视觉问答、解释生成和视觉定位。该框架整合了三个精心设计的数据集：Kvasir-VQA-x1用于问答训练、合成的结构化医学推理解释数据集，以及文本-区域（mask）对用于视觉归因。通过联合训练，模型学习到视觉定位、推理与解释能力的协同提升。

Result: 在全面评估中，提出的多任务系统在回答准确率和视觉定位精度上均明显优于单任务基线模型。

Conclusion: 研究表明，基于多任务和视觉归因训练的方法能有效提升医学视觉问答系统的可用性和可解释性，对实际医疗应用更具价值。

Abstract: We present a multi-task framework for the MediaEval Medico 2025 challenge,
leveraging a LoRA-tuned Florence-2 model for simultaneous visual question
answering (VQA), explanation generation, and visual grounding. The proposed
system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer
learning, (2) a synthetically enriched explanation dataset offering structured
medical reasoning, and (3) text-to-region pairs linking visual features with
segmentation masks. This multi-task setup enables the model to jointly learn
visual grounding, reasoning, and interpretation, producing responses that are
both accurate and interpretable. Extensive evaluation demonstrates that our
approach substantially improves over single-task baselines in both answer
accuracy and visual localization, highlighting the effectiveness of grounded
multi-task learning for medical VQA applications.

</details>


### [46] [DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale](https://arxiv.org/abs/2511.04394)
*Ke Du,Yimin Peng,Chao Gao,Fan Zhou,Siqiao Xue*

Main category: cs.CV

TL;DR: DORAEMON是一个开源的PyTorch视觉建模与表示学习库，支持分类、检索与度量学习，集成大量预训练模型和训练工具，便于快速实验和实际部署。


<details>
  <summary>Details</summary>
Motivation: 当前视觉识别和表征学习领域存在工具分散、复现性不足、预训练模型支持有限的问题。研究者和开发者若想在各类视觉任务间迁移或复用新成果，常需耗费大量配置和开发时间。

Method: DORAEMON整合了数据集、模型、损失函数、增强与分布式训练模块，基于YAML配置实现统一的分类、检索、度量学习流程。它通过与timm兼容接口暴露超千种预训练骨干网络，并支持一键ONNX及HuggingFace模型导出，提升了模型复现和部署效率。

Result: 作者在ImageNet-1K、MS-Celeb-1M、Stanford Online Products等数据集上，利用DORAEMON复现或优于已有参考结果，证明了其高性能和强复现能力。

Conclusion: DORAEMON为视觉识别和表征学习提供了统一、可扩展的平台，加速实验与技术转化，促进科研成果在实际场景落地。

Abstract: DORAEMON is an open-source PyTorch library that unifies visual object
modeling and representation learning across diverse scales. A single
YAML-driven workflow covers classification, retrieval and metric learning; more
than 1000 pretrained backbones are exposed through a timm-compatible interface,
together with modular losses, augmentations and distributed-training utilities.
Reproducible recipes match or exceed reference results on ImageNet-1K,
MS-Celeb-1M and Stanford online products, while one-command export to ONNX or
HuggingFace bridges research and deployment. By consolidating datasets, models,
and training techniques into one platform, DORAEMON offers a scalable
foundation for rapid experimentation in visual recognition and representation
learning, enabling efficient transfer of research advances to real-world
applications. The repository is available at https://github.com/wuji3/DORAEMON.

</details>


### [47] [HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats](https://arxiv.org/abs/2511.04426)
*Alan de Aguiar,Michaella Pereira Andrade,Charles Morphy D. Santos,João Paulo Gois*

Main category: cs.CV

TL;DR: 本论文提出了HideAndSeg，一个新颖、最小监督的AI工具，用于在自然环境下对章鱼视频进行分割，解决了因环境复杂和缺乏大规模标注数据导致的分析难题。


<details>
  <summary>Details</summary>
Motivation: 章鱼具有伪装和快速变化体表的能力，在水下环境的光照与浑浊变化下，章鱼的形态和颜色变化大，身体非刚性变形且常有遮挡，给其在自然环境下的自动定量分析和行为研究带来了巨大困难。此外，目前缺乏大规模带标注数据集限制了AI方法在该领域的应用。

Method: 本方法提出HideAndSeg，将SAM2（分割模型）和专门训练的YOLOv11目标检测器结合。首先用户手动提供点坐标，用SAM2得到初始分割掩码，作为YOLO的训练数据。训练后，整个流程自动化，通过YOLO检测框输出作为提示输入SAM2，无需进一步人工干预。提出了两项无监督评估指标（时序一致性DICE_t和新组分数量NC_t），在缺乏真实标注条件下用于评估分割质量并指导掩码优化。

Result: HideAndSeg在实际无标注场景下展现了可观的分割表现，有效降低了分割噪声。可在章鱼完全被遮挡后重新识别并分割，而人工提示方法无法做到。

Conclusion: 本研究提出的新工具极大减少了实际研究中对人工干预的需求，为野生头足类动物的高效行为学研究提供了实用工具，对推动生物学及AI自动分割技术发展具有重要意义。

Abstract: Analyzing octopuses in their natural habitats is challenging due to their
camouflage capability, rapid changes in skin texture and color, non-rigid body
deformations, and frequent occlusions, all of which are compounded by variable
underwater lighting and turbidity. Addressing the lack of large-scale annotated
datasets, this paper introduces HideAndSeg, a novel, minimally supervised
AI-based tool for segmenting videos of octopuses. It establishes a quantitative
baseline for this task. HideAndSeg integrates SAM2 with a custom-trained
YOLOv11 object detector. First, the user provides point coordinates to generate
the initial segmentation masks with SAM2. These masks serve as training data
for the YOLO model. After that, our approach fully automates the pipeline by
providing a bounding box prompt to SAM2, eliminating the need for further
manual intervention. We introduce two unsupervised metrics - temporal
consistency $DICE_t$ and new component count $NC_t$ - to quantitatively
evaluate segmentation quality and guide mask refinement in the absence of
ground-truth data, i.e., real-world information that serves to train, validate,
and test AI models. Results show that HideAndSeg achieves satisfactory
performance, reducing segmentation noise compared to the manually prompted
approach. Our method can re-identify and segment the octopus even after periods
of complete occlusion in natural environments, a scenario in which the manually
prompted model fails. By reducing the need for manual analysis in real-world
scenarios, this work provides a practical tool that paves the way for more
efficient behavioral studies of wild cephalopods.

</details>


### [48] [Solving Convex Partition Visual Jigsaw Puzzles](https://arxiv.org/abs/2511.04450)
*Yaniv Ohayon,Ofir Itzhak Shahar,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 本文提出了一种新的Jigsaw拼图自动求解方法，首次针对多边形凸块拼图（Convex Partitions）设计并测试了算法和数据集。


<details>
  <summary>Details</summary>
Motivation: 此前研究大多关注于求解正方形拼图，实用性有限；而现实中许多拼图都是多边形结构，凸块拼图是其中重要的一类，极大限制了自动拼图算法的应用场景。

Method: 作者结合了几何和图像匹配信息，设计了一种贪心算法，并构建了第一个凸块拼图的基准数据集用于评测方法效果。

Result: 该方法能够处理大类凸块多边形拼图问题，并报告了多种性能指标，在新基准数据集上验证了有效性。

Conclusion: 论文显著扩展了自动拼图求解的适用范围，为多边形拼图提供了实用的解决思路，对实际应用有推动作用。

Abstract: Jigsaw puzzle solving requires the rearrangement of unordered pieces into
their original pose in order to reconstruct a coherent whole, often an image,
and is known to be an intractable problem. While the possible impact of
automatic puzzle solvers can be disruptive in various application domains, most
of the literature has focused on developing solvers for square jigsaw puzzles,
severely limiting their practical use. In this work, we significantly expand
the types of puzzles handled computationally, focusing on what is known as
Convex Partitions, a major subset of polygonal puzzles whose pieces are convex.
We utilize both geometrical and pictorial compatibilities, introduce a greedy
solver, and report several performance measures next to the first benchmark
dataset of such puzzles.

</details>


### [49] [V-Thinker: Interactive Thinking with Images](https://arxiv.org/abs/2511.04460)
*Runqi Qiao,Qiuna Tan,Minghan Yang,Guanting Dong,Peiqing Yang,Shiqiang Lang,Enhui Wan,Xiaowan Wang,Yida Xu,Lan Yang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种增强型多模态模型V-Thinker，实现了更深入的图像交互和复杂推理能力，并建立了相关数据生产、训练机制及评测基准，实验证明其在多种推理任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型在融合图像交互和长程推理方面存在瓶颈。已有方法虽有进步，但受限于视觉工具类型单一和特定任务流程，难以实现通用和高效的视觉推理。

Method: 提出V-Thinker，包括：(1) 数据进化飞轮，自动生成、筛选、验证覆盖多样性、质量和难度的推理数据集；(2) 视觉递进训练课程，先以点级监督对齐感知，再通过两阶段强化学习集成互动推理。还引入了新的VTBench评测基准用于定量分析相关任务表现。

Result: 大量实验验证表明，V-Thinker在常规和交互式推理任务上均显著超越了多种强大的LMM基线模型。

Conclusion: V-Thinker为视觉交互推理领域带来了新机制和性能突破，为后续拓展多模态智能体的推理与交互能力提供了重要参考。

Abstract: Empowering Large Multimodal Models (LMMs) to deeply integrate image
interaction with long-horizon reasoning capabilities remains a long-standing
challenge in this field. Recent advances in vision-centric reasoning explore a
promising "Thinking with Images" paradigm for LMMs, marking a shift from
image-assisted reasoning to image-interactive thinking. While this milestone
enables models to focus on fine-grained image regions, progress remains
constrained by limited visual tool spaces and task-specific workflow designs.
To bridge this gap, we present V-Thinker, a general-purpose multimodal
reasoning assistant that enables interactive, vision-centric thinking through
end-to-end reinforcement learning. V-Thinker comprises two key components: (1)
a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies
interactive reasoning datasets across three dimensions-diversity, quality, and
difficulty; and (2) a Visual Progressive Training Curriculum that first aligns
perception via point-level supervision, then integrates interactive reasoning
through a two-stage reinforcement learning framework. Furthermore, we introduce
VTBench, an expert-verified benchmark targeting vision-centric interactive
reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently
outperforms strong LMM-based baselines in both general and interactive
reasoning scenarios, providing valuable insights for advancing
image-interactive reasoning applications.

</details>


### [50] [Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability](https://arxiv.org/abs/2511.04474)
*Wenwen Li,Sizhe Wang,Hyunho Lee,Chenyan Lu,Sujit Roy,Rahul Ramachandran,Chia-Yu Hsu*

Main category: cs.CV

TL;DR: 本文提出了一种三轴分析框架用于提升地理空间基础模型（GeoFMs）在滑坡制图中的适应性，展示了新模型在多种挑战条件下优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 滑坡对生命、基础设施及环境造成巨大破坏，准时准确的制图对防灾减灾极为重要。传统深度学习模型在不同传感器、区域或训练数据有限情况下表现不佳，因此需要新的方法提升模型泛化能力和适应性。

Method: 提出了一套以传感器、标签和领域为三轴的分析框架。以Prithvi-EO-2.0为代表的GeoFMs进行全局预训练、自监督和灵活微调，通过一系列实验与CNN（如U-Net、U-Net++）、视觉Transformer（Segformer、SwinV2-B）及其他GeoFMs（TerraMind、SatMAE）对比。

Result: Prithvi-EO-2.0在多种场景下表现出色，能够应对光谱变化、标签稀缺、跨数据集和地理区域泛化等问题，并在准确性、鲁棒性和可扩展性上超越传统方法。

Conclusion: GeoFMs如Prithvi-EO-2.0为滑坡风险降低和环境监测提供了更具鲁棒性、可扩展性的方案，但仍面临计算成本和高质量AI训练数据不足等挑战。

Abstract: Landslides cause severe damage to lives, infrastructure, and the environment,
making accurate and timely mapping essential for disaster preparedness and
response. However, conventional deep learning models often struggle when
applied across different sensors, regions, or under conditions of limited
training data. To address these challenges, we present a three-axis analytical
framework of sensor, label, and domain for adapting geospatial foundation
models (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a
series of experiments, we show that it consistently outperforms task-specific
CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other
GeoFMs (TerraMind, SatMAE). The model, built on global pretraining,
self-supervision, and adaptable fine-tuning, proved resilient to spectral
variation, maintained accuracy under label scarcity, and generalized more
reliably across diverse datasets and geographic settings. Alongside these
strengths, we also highlight remaining challenges such as computational cost
and the limited availability of reusable AI-ready training data for landslide
research. Overall, our study positions GeoFMs as a step toward more robust and
scalable approaches for landslide risk reduction and environmental monitoring.

</details>


### [51] [THEval. Evaluation Framework for Talking Head Video Generation](https://arxiv.org/abs/2511.04520)
*Nabyl Quignon,Baptiste Chopin,Yaohui Wang,Antitza Dantcheva*

Main category: cs.CV

TL;DR: 本文提出了一个新的口播视频生成评价框架，针对目前评价手段的不足，引入8个多维指标，对生成视频的质量、自然性和同步性进行系统评估。实验覆盖17个主流模型，依托全新数据集，展示了方法在表达力和细节生成上的挑战。代码和数据将公开，有助于该领域的进步。


<details>
  <summary>Details</summary>
Motivation: 目前口播视频生成的评测主要依赖于少量的通用指标和用户调研，无法全面反映生成视频的自然性、细节动态和与真实人类偏好的符合度，因此需要更系统、更高效且更贴近人类主观评价的评价体系。

Method: 作者提出了一个包含8个指标的新评价框架，从质量、自然性和同步性三个维度出发，并聚焦头部、嘴巴、眉毛动态和面部质量，通过高效的工具对生成视频进行精细化分析。同时构建了涵盖85,000个基于自建无偏数据集生成视频的大规模实验，对17种前沿模型进行了系统测评。

Result: 实验结果发现，现有多种主流生成算法在唇同步方面表现优秀，但在生成更具表现力和无伪影的细节方面仍面临很大挑战。

Conclusion: 提出的评测框架能够更全面、客观地评估口播生成方法，有助于衡量和推动该领域技术进步。公开的数据集、代码和榜单将促进社区共享和发展。

Abstract: Video generation has achieved remarkable progress, with generated videos
increasingly resembling real ones. However, the rapid advance in generation has
outpaced the development of adequate evaluation metrics. Currently, the
assessment of talking head generation primarily relies on limited metrics,
evaluating general video quality, lip synchronization, and on conducting user
studies. Motivated by this, we propose a new evaluation framework comprising 8
metrics related to three dimensions (i) quality, (ii) naturalness, and (iii)
synchronization. In selecting the metrics, we place emphasis on efficiency, as
well as alignment with human preferences. Based on this considerations, we
streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as
well as face quality. Our extensive experiments on 85,000 videos generated by
17 state-of-the-art models suggest that while many algorithms excel in lip
synchronization, they face challenges with generating expressiveness and
artifact-free details. These videos were generated based on a novel real
dataset, that we have curated, in order to mitigate bias of training data. Our
proposed benchmark framework is aimed at evaluating the improvement of
generative methods. Original code, dataset and leaderboards will be publicly
released and regularly updated with new methods, in order to reflect progress
in the field.

</details>


### [52] [Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy](https://arxiv.org/abs/2511.04525)
*Dimitrios Anastasiou,Santiago Barbarisi,Lucy Culshaw,Jayna Patel,Evangelos B. Mazomenos,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 本文提出了一种新方法（STC-Net），能够在腹腔镜胆囊切除术（LC）完整手术视频中，通过Parkland分级标准（PGS）自动评估手术复杂度，对术后分析和医生培训具有潜在意义。


<details>
  <summary>Details</summary>
Motivation: 在LC手术中，炎症严重程度直接影响手术时间和术后并发症，PGS为炎症分级提供了标准，但将其自动化应用到完整手术视频还未有很好的解决方案。目前大多方法只针对静态图片或人工剪辑片段，缺乏实际操作性。

Method: 提出STC-Net框架，通过单一时间戳进行复杂度评估，具备弱时间监督能力，可以直接在未经剪辑的完整手术视频上工作。模型包括局部定位、窗口提议和分级模块，并提出了结合硬性与软性定位目标的新损失函数，辅以背景感知的分级监督。

Result: 在1859个LC手术视频的私有数据集上，STC-Net实现了62.11%的准确率和61.42%的F1分数，较非定位基线模型提升超过10%，证明了弱监督方法的有效性。

Conclusion: STC-Net为完整手术视频中的PGS自动复杂度评估提供了可扩展且有效的方法，对术后分析和医生培训具有良好应用前景。

Abstract: Purpose: Accurate assessment of surgical complexity is essential in
Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with
longer operative times and increased risk of postoperative complications. The
Parkland Grading Scale (PGS) provides a clinically validated framework for
stratifying inflammation severity; however, its automation in surgical videos
remains largely unexplored, particularly in realistic scenarios where complete
videos must be analyzed without prior manual curation. Methods: In this work,
we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity
estimation in LC via the PGS, designed to operate under weak temporal
supervision. Unlike prior methods limited to static images or manually trimmed
clips, STC-Net operates directly on full videos. It jointly performs temporal
localization and grading through a localization, window proposal, and grading
module. We introduce a novel loss formulation combining hard and soft
localization objectives and background-aware grading supervision. Results:
Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy
of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by
over 10% in both metrics and highlighting the effectiveness of weak supervision
for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable
and effective approach for automated PGS-based surgical complexity estimation
from full LC videos, making it promising for post-operative analysis and
surgical training.

</details>


### [53] [UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction](https://arxiv.org/abs/2511.04595)
*Chen Shi,Shaoshuai Shi,Xiaoyang Lyu,Chunyang Liu,Kehua Sheng,Bo Zhang,Li Jiang*

Main category: cs.CV

TL;DR: UniSplat提出了一种高效的三维重建方法，通过构建时空统一的隐空间结构，实现了对动态场景的稳健重建，显著提升了自动驾驶场景下新视角合成和超视野渲染的性能。


<details>
  <summary>Details</summary>
Motivation: 目前前馈式3D重建方法在面对稀疏、非重叠的摄像机视角和复杂动态场景时表现不佳。自动驾驶等实际应用场景急需一种能够在动态环境下可靠进行三维重建的通用方法。

Method: UniSplat设计了一个可学习的3D隐空间结构（scaffold），利用预训练基础模型融合几何与语义信息。提出了一种3D空间内的高效信息融合机制，结合空间视角与时间帧，实现时空一致的特征对齐。重建部分采用双分支解码器：基于点的精细化与基于体素的生成结合，动态信息表达为高斯分布，同时针对静态部分引入持久记忆，实现超视野的场景完整性。

Result: 实验证明UniSplat在真实场景数据集上，在新视角合成任务上获得了最新最好的效果。对于摄像机覆盖以外的视点，也能够输出高质量的渲染结果，表现出强鲁棒性和泛化能力。

Conclusion: UniSplat通过时空一致的隐空间结构和高效的信息融合及解码机制，显著提升了动态图像多视角三维重建的性能，为自动驾驶等实际场景下的高质量三维感知提供了有效方案。

Abstract: Feed-forward 3D reconstruction for autonomous driving has advanced rapidly,
yet existing methods struggle with the joint challenges of sparse,
non-overlapping camera views and complex scene dynamics. We present UniSplat, a
general feed-forward framework that learns robust dynamic scene reconstruction
through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent
scaffold, a structured representation that captures geometric and semantic
scene context by leveraging pretrained foundation models. To effectively
integrate information across spatial views and temporal frames, we introduce an
efficient fusion mechanism that operates directly within the 3D scaffold,
enabling consistent spatio-temporal alignment. To ensure complete and detailed
reconstructions, we design a dual-branch decoder that generates dynamic-aware
Gaussians from the fused scaffold by combining point-anchored refinement with
voxel-based generation, and maintain a persistent memory of static Gaussians to
enable streaming scene completion beyond current camera coverage. Extensive
experiments on real-world datasets demonstrate that UniSplat achieves
state-of-the-art performance in novel view synthesis, while providing robust
and high-quality renderings even for viewpoints outside the original camera
coverage.

</details>


### [54] [PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning](https://arxiv.org/abs/2511.04601)
*Yicheng Xiao,Yu Chen,Haoxuan Ma,Jiale Hong,Caorui Li,Lingxiang Wu,Haiyun Guo,Jinqiao Wang*

Main category: cs.CV

TL;DR: CLIP模型虽然在视觉-语言任务中表现出色，但细粒度图文对齐能力有限。本文提出PixCLIP，通过增强视觉与文本的处理粒度，实现了更精细的图像区域与长文本对齐，取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP模型由于文本编码器的token长度限制，难以处理细粒度的长文本描述，从而限制了模型在更精细图文对齐任务中的表现。增强视觉或文本一方的粒度已经有一定成果，但两者协同提升仍存在挑战。

Method: 提出PixCLIP框架，第一步是利用自动化标注流程，生成像素级的图像局部区域和对应长文本描述，构建了包含约150万样本的LongGRIT数据集。第二步，用LLM替代原CLIP文本编码器，并设计三分支像素-文本对齐学习框架，实现任意粒度下图像区域与文本对齐。

Result: PixCLIP在像素级交互能力和处理长文本能力方面突破现有方法，实验显示其在细粒度视觉-语言理解任务中达到了当前最优效果。

Conclusion: PixCLIP同时提升了视觉和文本信息处理粒度，在细粒度视觉-语言对齐方面展现出极大潜力，推动了该领域的发展。

Abstract: While the Contrastive Language-Image Pretraining(CLIP) model has achieved
remarkable success in a variety of downstream vison language understanding
tasks, enhancing its capability for fine-grained image-text alignment remains
an active research focus. To this end, most existing works adopt the strategy
of explicitly increasing the granularity of visual information processing,
e.g., incorporating visual prompts to guide the model focus on specific local
regions within the image. Meanwhile, researches on Multimodal Large Language
Models(MLLMs) have demonstrated that training with long and detailed textual
descriptions can effectively improve the model's fine-grained vision-language
alignment. However, the inherent token length limitation of CLIP's text encoder
fundamentally limits CLIP to process more granular textual information embedded
in long text sequences. To synergistically leverage the advantages of enhancing
both visual and textual content processing granularity, we propose PixCLIP, a
novel framework designed to concurrently accommodate visual prompt inputs and
process lengthy textual descriptions. Specifically, we first establish an
automated annotation pipeline capable of generating pixel-level localized,
long-form textual descriptions for images. Utilizing this pipeline, we
construct LongGRIT, a high-quality dataset comprising nearly 1.5 million
samples. Secondly, we replace CLIP's original text encoder with the LLM and
propose a three-branch pixel-text alignment learning framework, facilitating
fine-grained alignment between image regions and corresponding textual
descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP
showcases breakthroughs in pixel-level interaction and handling long-form
texts, achieving state-of-the-art performance.

</details>


### [55] [Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality](https://arxiv.org/abs/2511.04615)
*Tushar Kataria,Shikha Dubey,Mary Bronner,Jolanta Jedrzkiewicz,Ben J. Brintz,Shireen Y. Elhabian,Beatrice S. Knudsen*

Main category: cs.CV

TL;DR: 该论文提出了一种自动化、以准确性为基础的框架，用于评估虚拟免疫组化（IHC）染色模型的图像质量，强调现有常用指标无法有效反映染色情况，并用更可靠的方法评测模型。


<details>
  <summary>Details</summary>
Motivation: 深度学习可用H&E切片生成虚拟IHC，从而减少实验室染色成本，但如何客观准确地评估虚拟IHC图像（尤其是染色情况）仍无有效手段。现有的图像质量评估方法多基于纹理和分布，未能真正衡量IHC染色的准确性。

Method: 作者构建了一个自动化评测框架，基于色彩解卷积提取棕色（IHC阳性）像素掩膜，然后采用Dice、IoU和Hausdorff距离等指标，直接衡量像素级染色标签的准确性。该方法无需人工注释。实验中，对16种配对和非配对的图像翻译模型进行全面评估。

Result: 研究发现，常用的图像质量指标如FID、PSNR、SSIM与染色情况及病理医师评价相关性较差。配对型模型如PyramidPix2Pix和AdaptiveNCE染色更准确，而非配对的扩散和GAN模型表现较差。此外，在全切片（WSI）级别可见的性能下降，被小块图像评测所掩盖，说明评测尺度的重要性。

Conclusion: 该框架为虚拟IHC染色模型提供了可复用、以准确性为导向的评测标准，有助于推动虚拟IHC模型在实际病理应用中的落地。

Abstract: Deep learning models can generate virtual immunohistochemistry (IHC) stains
from hematoxylin and eosin (H&E) images, offering a scalable and low-cost
alternative to laboratory IHC. However, reliable evaluation of image quality
remains a challenge as current texture- and distribution-based metrics quantify
image fidelity rather than the accuracy of IHC staining. Here, we introduce an
automated and accuracy grounded framework to determine image quality across
sixteen paired or unpaired image translation models. Using color deconvolution,
we generate masks of pixels stained brown (i.e., IHC-positive) as predicted by
each virtual IHC model. We use the segmented masks of real and virtual IHC to
compute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly
quantify correct pixel - level labeling without needing expert manual
annotations. Our results demonstrate that conventional image fidelity metrics,
including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR),
and structural similarity (SSIM), correlate poorly with stain accuracy and
pathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE
achieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based
models are less reliable in providing accurate IHC positive pixel labels.
Moreover, whole-slide images (WSI) reveal performance declines that are
invisible in patch-based evaluations, emphasizing the need for WSI-level
benchmarks. Together, this framework defines a reproducible approach for
assessing the quality of virtual IHC models, a critical step to accelerate
translation towards routine use by pathologists.

</details>


### [56] [NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment](https://arxiv.org/abs/2511.04628)
*Kylie Cancilla,Alexander Moore,Amar Saini,Carmen Carrano*

Main category: cs.CV

TL;DR: 本文提出了一种可扩展、流式的视频质量评估（VQA）模型，无需参考视频和人工主观分数，能更好地适应实际计算机视觉应用。


<details>
  <summary>Details</summary>
Motivation: 当前VQA方法存在局限：全参考（FR）方法需要原始视频，主流无参考（NR）模型则依赖昂贵的主观评分数据。同时，多数无主观感知的NR方法忽视了对视频时序信息的建模，不适用于需要时序连续性的视频任务。

Method: 作者在DAVIS数据集上引入合成降质，训练具备时序感知能力的卷积架构，使其仅依据降质视频预测多种FR指标（如LPIPS、PSNR、SSIM），无需参考原视频和人工主观评分。

Result: 流式、时序感知模型在多样降质情况下相比自身的图像基础模型泛化能力更好，对主流BRISQUE等NR方法的指标表现出更高的相关性。

Conclusion: 时序建模显著提升了无参考、无主观感知的视频质量评估性能，模型具备较强的实际应用前景。

Abstract: Video quality assessment (VQA) is vital for computer vision tasks, but
existing approaches face major limitations: full-reference (FR) metrics require
clean reference videos, and most no-reference (NR) models depend on training on
costly human opinion labels. Moreover, most opinion-unaware NR methods are
image-based, ignoring temporal context critical for video object detection. In
this work, we present a scalable, streaming-based VQA model that is both
no-reference and opinion-unaware. Our model leverages synthetic degradations of
the DAVIS dataset, training a temporal-aware convolutional architecture to
predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without
references at inference. We show that our streaming approach outperforms our
own image-based baseline by generalizing across diverse degradations,
underscoring the value of temporal modeling for scalable VQA in real-world
vision systems. Additionally, we demonstrate that our model achieves higher
correlation with full-reference metrics compared to BRISQUE, a widely-used
opinion-aware image quality assessment baseline, validating the effectiveness
of our temporal, opinion-unaware approach.

</details>


### [57] [Polarization-resolved imaging improves eye tracking](https://arxiv.org/abs/2511.04652)
*Mantas Žurauskas,Tom Bu,Sanaz Alali,Beyza Kalkanli,Derek Shi,Fernando Alamos,Gauresh Pandit,Christopher Mei,Ali Behrooz,Ramin Mirjalili,Dave Stronks,Alexander Fix,Dmitri Model*

Main category: cs.CV

TL;DR: 本文提出了一种结合偏振分辨近红外成像的新型眼动追踪方法，利用光的偏振状态提升眼动追踪的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的眼动追踪方法主要依赖反射光的强度信息，但在实际应用中易受到眼睑遮挡、眼-摄距离变化以及瞳孔大小变化的影响，造成追踪准确性下降。因此需寻找更优的光学对比机制以提升鲁棒性和性能。

Method: 作者设计了一套偏振增强眼动追踪系统（PET），核心配置包括一个带有偏振滤波阵列的相机和线性偏振近红外照明光源。通过机器学习（卷积神经网络）模型对采集到的偏振信息进行训练与推断，充分挖掘角膜和巩膜区域的可追踪特征，用于改善凝视点估计。

Result: 在346名参与者数据上，PET系统下训练的卷积神经网络模型相较于同等规模的仅强度信息系统，95分位绝对凝视误差降低了10-16%，不论在标准条件还是遇到眼睑遮挡、眼-摄距离变动及瞳孔变化时均有提升。

Conclusion: 本研究首次系统性地展示了光-组织偏振效应对眼动追踪的实际增益，表明PET方法简单、鲁棒，适合未来可穿戴设备的人机交互应用。

Abstract: Polarization-resolved near-infrared imaging adds a useful optical contrast
mechanism to eye tracking by measuring the polarization state of light
reflected by ocular tissues in addition to its intensity. In this paper we
demonstrate how this contrast can be used to enable eye tracking. Specifically,
we demonstrate that a polarization-enabled eye tracking (PET) system composed
of a polarization--filter--array camera paired with a linearly polarized
near-infrared illuminator can reveal trackable features across the sclera and
gaze-informative patterns on the cornea, largely absent in intensity-only
images. Across a cohort of 346 participants, convolutional neural network based
machine learning models trained on data from PET reduced the median
95th-percentile absolute gaze error by 10--16\% relative to capacity-matched
intensity baselines under nominal conditions and in the presence of eyelid
occlusions, eye-relief changes, and pupil-size variation. These results link
light--tissue polarization effects to practical gains in human--computer
interaction and position PET as a simple, robust sensing modality for future
wearable devices.

</details>


### [58] [Benchmark Designers Should "Train on the Test Set" to Expose Exploitable Non-Visual Shortcuts](https://arxiv.org/abs/2511.04655)
*Ellis Brown,Jihan Yang,Shusheng Yang,Rob Fergus,Saining Xie*

Main category: cs.CV

TL;DR: 该论文提出了一套检测和消除多模态大模型基准评测中非视觉偏差的方法，以提升基准测试的有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前多数多模态大模型基准测试容易被利用模型的语言先验、表面模式等非视觉能力“作弊”通过，无法真正评估其视觉理解能力。尤其在强调图像理解的基准上，这一问题更加严重，因此亟需更健壮、去偏见的评测方法。

Method: 提出了两步法：(1) 用"Test-set Stress-Test"（TsT）诊断基准中可被利用的非视觉信息，具体方法是用大语言模型仅在测试集文本输入上进行k折交叉验证微调，为每个样本评估偏差分数；并辅以基于随机森林的人工特征分析加快审计与解释。(2) 用“Iterative Bias Pruning（IBP）”流程过滤高偏差样本，对基准集进行多轮去偏优化。

Result: 在VSI-Bench、CV-Bench、MMMU和VideoMME四个基准集合上发现了普遍的非视觉偏见。以VSI-Bench-Debiased为例，经过该框架处理后，模型靠非视觉途径答题的能力显著下降，视觉能力和“盲做”模型之间的性能差距扩大，说明基准更能反映真实的视觉理解能力。

Conclusion: 论文证明了多模态大模型基准中存在系统性非视觉偏差，并提出了有效的诊断和去偏工具。结果显示，经过去偏的基准集能提升评测的可信度和区分度，对未来基准设计具有参考意义。

Abstract: Robust benchmarks are crucial for evaluating Multimodal Large Language Models
(MLLMs). Yet we find that models can ace many multimodal benchmarks without
strong visual understanding, instead exploiting biases, linguistic priors, and
superficial patterns. This is especially problematic for vision-centric
benchmarks that are meant to require visual inputs. We adopt a diagnostic
principle for benchmark design: if a benchmark can be gamed, it will be.
Designers should therefore try to ``game'' their own benchmarks first, using
diagnostic and debiasing procedures to systematically identify and mitigate
non-visual biases. Effective diagnosis requires directly ``training on the test
set'' -- probing the released test set for its intrinsic, exploitable patterns.
  We operationalize this standard with two components. First, we diagnose
benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.
Our primary diagnostic tool involves fine-tuning a powerful Large Language
Model via $k$-fold cross-validation on exclusively the non-visual, textual
inputs of the test set to reveal shortcut performance and assign each sample a
bias score $s(x)$. We complement this with a lightweight Random Forest-based
diagnostic operating on hand-crafted features for fast, interpretable auditing.
Second, we debias benchmarks by filtering high-bias samples using an
``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four
benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive
non-visual biases. As a case study, we apply our full framework to create
VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider
vision-blind performance gap than the original.

</details>


### [59] [SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding](https://arxiv.org/abs/2511.04668)
*Ellis Brown,Arijit Ray,Ranjay Krishna,Ross Girshick,Rob Fergus,Saining Xie*

Main category: cs.CV

TL;DR: 本文提出了SIMS-V，一个利用3D模拟器生成富含空间信息的视频数据框架，用于提升多模态语言模型的空间推理能力。通过系统性消融研究，找出了最有效的三类问题，模型仅用少量模拟数据即可显著提升对真实世界空间任务的泛化与理解能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在视频理解上虽表现优异，但在跨时空空间推理上仍有不足。真实世界数据获取受限且难以精准标注空间信息，成为进一步提升模型空间能力的瓶颈。

Method: 提出SIMS-V框架，使用3D模拟器合成并带有精确空间注释的视频数据，并设计三类核心空间问题进行训练。系统性地分析哪些模拟数据最有助于模型泛化到现实任务中。通过只用25K模拟样本，对7B参数的模型进行微调，并与更大模型及专有模型对比表现。

Result: 找到三类问题（度量测量、视角相关推理、时序追踪）在空间迁移上最为有效。7B参数的SIMS-V LLM在少量合成数据训练下超越了72B参数基线，并在真实空间理解基准任务中达到与专有模型相当的表现。

Conclusion: 合成空间数据和有针对性的空间训练问题能极大提高多模态模型的现实空间推理能力，实现高效泛化。SIMS-V框架具备推动空间智能研究的实用价值。

Abstract: Despite impressive high-level video comprehension, multimodal language models
struggle with spatial reasoning across time and space. While current spatial
training approaches rely on real-world video data, obtaining diverse footage
with precise spatial annotations remains a bottleneck. To alleviate this
bottleneck, we present SIMS-V -- a systematic data-generation framework that
leverages the privileged information of 3D simulators to create spatially-rich
video training data for multimodal language models. Using this framework, we
investigate which properties of simulated data drive effective real-world
transfer through systematic ablations of question types, mixes, and scales. We
identify a minimal set of three question categories (metric measurement,
perspective-dependent reasoning, and temporal tracking) that prove most
effective for developing transferable spatial intelligence, outperforming
comprehensive coverage despite using fewer question types. These insights
enable highly efficient training: our 7B-parameter video LLM fine-tuned on just
25K simulated examples outperforms the larger 72B baseline and achieves
competitive performance with proprietary models on rigorous real-world spatial
reasoning benchmarks. Our approach demonstrates robust generalization,
maintaining performance on general video understanding while showing
substantial improvements on embodied and real-world spatial tasks.

</details>


### [60] [Cambrian-S: Towards Spatial Supersensing in Video](https://arxiv.org/abs/2511.04670)
*Shusheng Yang,Jihan Yang,Pinzhi Huang,Ellis Brown,Zihao Yang,Yue Yu,Shengbang Tong,Zihan Zheng,Yifan Xu,Muhan Wang,Daohan Lu,Rob Fergus,Yann LeCun,Li Fei-Fei,Saining Xie*

Main category: cs.CV

TL;DR: 本文提出“超感知（supersensing）”作为多模态智能的新范式，不只是处理任务或扩展上下文长度，更要求模型具备事件流记忆、隐式三维理解和预测世界建模等能力。文中发布了VSI-SUPER基准，发现仅靠数据量和规模提升难以大幅提升空间感知能力，提出预测式感知方法能显著提升基准表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态系统通常只在早期阶段如语义感知表现良好，对空间认知、世界建模等要求较高的任务却力不从心，限制了真正的多模态智能发展。

Method: 作者提出“空间超感知”四阶段框架，设计了VSI-SUPER基准，包括长时视觉回忆（VSR）和持续视觉计数（VSC）两项任务。并训练了Cambrian-S模型，探究数据扩展极限，同时提出基于预测误差驱动的自监督事件分割方法。

Result: 扩大训练数据可提升传统基准(VSI-Bench)表现30%以上，但对VSI-SUPER提升有限。预测感知策略在VSI-SUPER上表现显著优于主流专有模型。

Conclusion: 仅靠规模增长难以获得空间超感知能力。预测式感知方法能强化模型在理解和组织连续、多变的空间信息方面的能力，是多模态智能发展的重要方向。

Abstract: We argue that progress in true multimodal intelligence calls for a shift from
reactive, task-driven systems and brute-force long context towards a broader
paradigm of supersensing. We frame spatial supersensing as four stages beyond
linguistic-only understanding: semantic perception (naming what is seen),
streaming event cognition (maintaining memory across continuous experiences),
implicit 3D spatial cognition (inferring the world behind pixels), and
predictive world modeling (creating internal models that filter and organize
information). Current benchmarks largely test only the early stages, offering
narrow coverage of spatial cognition and rarely challenging models in ways that
require true world modeling. To drive progress in spatial supersensing, we
present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial
recall) and VSC (continual visual spatial counting). These tasks require
arbitrarily long video inputs yet are resistant to brute-force context
expansion. We then test data scaling limits by curating VSI-590K and training
Cambrian-S, achieving +30% absolute improvement on VSI-Bench without
sacrificing general capabilities. Yet performance on VSI-SUPER remains limited,
indicating that scale alone is insufficient for spatial supersensing. We
propose predictive sensing as a path forward, presenting a proof-of-concept in
which a self-supervised next-latent-frame predictor leverages surprise
(prediction error) to drive memory and event segmentation. On VSI-SUPER, this
approach substantially outperforms leading proprietary baselines, showing that
spatial supersensing requires models that not only see but also anticipate,
select, and organize experience.

</details>


### [61] [InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation](https://arxiv.org/abs/2511.04675)
*Jinlai Liu,Jian Han,Bin Yan,Hui Wu,Fengda Zhu,Xing Wang,Yi Jiang,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: InfinityStar 是一个统一的时空自回归模型，实现了高分辨率图像和视频的生成，不仅在速度和效果上领先于同类已有方法，还支持多种生成任务。


<details>
  <summary>Details</summary>
Motivation: 当前高质量、高分辨率的视频生成面临速度慢或复杂性高的问题，尤其是自回归模型和扩散模型在表现和效率上各有局限，缺乏一个统一且高效的框架。

Method: 提出了一个纯离散的时空自回归模型InfinityStar，通过单一架构联合建模空间与时间依赖；支持文本到图像、文本到视频、图像到视频及长时连续视频生成等多任务。其核心通过时序自回归提升了视频合成质量和效率。

Result: InfinityStar 在VBench基准获得83.74分，远超其它自回归模型，并优于部分扩散模型如HunyuanVideo。在未做额外优化的情况下，生成5秒720p视频速度比主流扩散模型快约10倍。

Conclusion: InfinityStar 首次实现工业级720p高质量离散自回归视频生成，模型和代码已发布，为高效视频生成研究提供了新方向。

Abstract: We introduce InfinityStar, a unified spacetime autoregressive framework for
high-resolution image and dynamic video synthesis. Building on the recent
success of autoregressive modeling in both vision and language, our purely
discrete approach jointly captures spatial and temporal dependencies within a
single architecture. This unified design naturally supports a variety of
generation tasks such as text-to-image, text-to-video, image-to-video, and long
interactive video synthesis via straightforward temporal autoregression.
Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench,
outperforming all autoregressive models by large margins, even surpassing some
diffusion competitors like HunyuanVideo. Without extra optimizations, our model
generates a 5s, 720p video approximately 10x faster than leading
diffusion-based methods. To our knowledge, InfinityStar is the first discrete
autoregressive video generator capable of producing industrial level 720p
videos. We release all code and models to foster further research in efficient,
high-quality video generation.

</details>


### [62] [Tracking and Understanding Object Transformations](https://arxiv.org/abs/2511.04678)
*Yihong Sun,Xinyu Yang,Jennifer J. Sun,Bharath Hariharan*

Main category: cs.CV

TL;DR: 本文提出了一种新任务Track Any State，旨在跟踪经历状态变化的对象，并引入了新数据集VOST-TAS。作者提出的TubeletGraph系统能够在对象发生剧烈变化时恢复失踪目标，实现状态变化检测与描述，并达到最新跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中对象经常经历显著变化，如苹果被切开、蝴蝶破茧等，这导致传统视觉跟踪方法在对象外观大幅变化后无法继续跟踪。现有技术缺乏对对象状态变化的检测与描述能力，限制了其应用。

Method: 作者提出Track Any State任务，并构建VOST-TAS数据集。提出TubeletGraph系统，通过识别遗漏轨迹并根据语义和距离先验整合，形成状态图，描述跟踪对象的状态变化过程。系统能够零样本恢复发生重大外观变化的对象，实现状态转变的检测与时序建模。

Result: TubeletGraph在对象发生显著状态变化时，展现了先进的目标跟踪表现，并能推理对象状态的时序变化。与现有方法相比，系统实现更好的目标恢复、状态理解及语义推理能力。

Conclusion: TubeletGraph有效突破了对象因外观巨大变化造成的跟踪难题，并增强了对复杂状态转化的理解能力。该系统为实际场景下的目标跟踪、状态检测与时序语义推理提供了更强的方法与工具。

Abstract: Real-world objects frequently undergo state transformations. From an apple
being cut into pieces to a butterfly emerging from its cocoon, tracking through
these changes is important for understanding real-world objects and dynamics.
However, existing methods often lose track of the target object after
transformation, due to significant changes in object appearance. To address
this limitation, we introduce the task of Track Any State: tracking objects
through transformations while detecting and describing state changes,
accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we
present TubeletGraph, a zero-shot system that recovers missing objects after
transformation and maps out how object states are evolving over time.
TubeletGraph first identifies potentially overlooked tracks, and determines
whether they should be integrated based on semantic and proximity priors. Then,
it reasons about the added tracks and generates a state graph describing each
observed transformation. TubeletGraph achieves state-of-the-art tracking
performance under transformations, while demonstrating deeper understanding of
object transformations and promising capabilities in temporal grounding and
semantic reasoning for complex object transformations. Code, additional
results, and the benchmark dataset are available at
https://tubelet-graph.github.io.

</details>


### [63] [Carousel: A High-Resolution Dataset for Multi-Target Automatic Image Cropping](https://arxiv.org/abs/2511.04680)
*Rafe Loya,Andrew Hamara,Benjamin Estell,Benjamin Kilpatrick,Andrew C. Freeman*

Main category: cs.CV

TL;DR: 本论文关注如何从一张图片中产生多个、各具美感的裁剪结果，而非仅仅单一裁剪，并建立了一个包含277张图片和人工标签的数据集。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体的流行，用户常需要将一张图片裁剪为多个不同但同样美观的小图用于展示。目前相关研究多聚焦于单一裁剪，无多裁剪结果优化相关方法。

Method: 作者引入了一个带有人工标注的图片数据集，并使用图像分割算法对已有单裁剪模型进行预处理，探讨其在产生多个裁剪上的表现。

Result: 实验证明，将图像分割作为预处理步骤可提升现有单裁剪模型生成多裁剪结果时的美学表现。

Conclusion: 采用图像分割作为预处理，有助于多裁剪目标的实现。作者还提供了数据集，为未来相关研究提供支持。

Abstract: Automatic image cropping is a method for maximizing the human-perceived
quality of cropped regions in photographs. Although several works have proposed
techniques for producing singular crops, little work has addressed the problem
of producing multiple, distinct crops with aesthetic appeal. In this paper, we
motivate the problem with a discussion on modern social media applications,
introduce a dataset of 277 relevant images and human labels, and evaluate the
efficacy of several single-crop models with an image partitioning algorithm as
a pre-processing step. The dataset is available at
https://github.com/RafeLoya/carousel.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [64] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 本文提出一种新方法，通过分析大模型Transformer层的隐状态并结合Big Five人格特质，实现对大模型输出人格表现的精细控制。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成文本时会表现出隐含的人格特质，但如何有效控制这些特质以满足实际需求仍是挑战。现有文献缺乏生成时行为操控的机制，且模型中的心理学人格结构尚未被充分探讨。

Method: 作者提出了一个新流程，首先借助Big Five人格特质提取模型各层的隐状态，然后利用低秩子空间发现方法，筛选出表达不同特质的最优层，最后通过动态选择层并注入特质相关方向，对个性化输出进行灵活干预。

Result: 实验表明，各个人格特质在模型中呈现低秩共享子空间结构，且通过对该结构精细扰动，可在不影响文本流畅性、多样性和通用能力的前提下，有效操控大模型的输出人格特质。

Conclusion: 该方法为人格-感知大模型的研究打开新方向，将心理学理论与实际模型对齐任务相结合，为后续行为可控的AI系统奠定基础。

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [65] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: 本文提出TextualVerifier框架，为TextGrad文本自动微分系统提供自验证机制，提高文本推理的可靠性与准确性。


<details>
  <summary>Details</summary>
Motivation: 虽然TextGrad实现了无需显式数值方程的文本优化，但缺乏对推理合理性的自验证机制，导致在基于文本决策时难以保证结果准确可信。因此，需要一个能验证和提升文本推理有效性的框架。

Method: 提出TextualVerifier验证框架，采用链式思维分解、变体生成、多数投票与共识聚合四阶段流程，通过大语言模型进行推理与投票，以无侵入方式整合进TextGrad的损失函数和结果验证阶段，并在Gemini 1.5 Pro模型上分两阶段实验评估。

Result: 实验结果表明，在独立验证阶段，TextualVerifier能够将推理步骤的有效性提升29%；集成进TextGrad损失函数后，整体准确率从68.2%提升到70.4%，平均增加5.9次LLM调用。在不同数据集上也有显著提升：GPQA提升8.08个百分点，MMLU-ML提升10.71，MMLU-CP提升3.92。

Conclusion: TextualVerifier是第一个基于LLM的TextGrad自验证框架，不依赖数值梯度，能够提升文本优化推理的可靠性，为文本决策与优化的验证开辟了新方向。

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [66] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

TL;DR: 该论文扩展了希腊方言数据集（GRDD+），涵盖10种方言，共计6,374,939词，并利用该数据集对多种大型语言模型进行了微调和比较。


<details>
  <summary>Details</summary>
Motivation: 现有的希腊方言数据集样本量不足且种类受限，难以支持对希腊方言的深入研究和大模型应用。作者希望通过扩充数据集，支撑更高质量的模型训练，并在方言处理任务上推动进展。

Method: 作者在原有GRDD数据集基础上，增加了克里特、塞浦路斯、旁蒂克、北希腊等现有方言样本，并新增了六种希腊方言（如Greco-Corsican、Griko等），共构建10个方言，总词数超630万。随后，选用Llama-3-8B、Llama-3.1-8B、Krikri-8B等自有模型进行了微调，并与前沿大模型Claude-3.7-Sonnet、Gemini-2.5、ChatGPT-5做效果对比。

Result: 扩展后的数据集为首个覆盖10种希腊方言且规模达630万词级别的大型数据集。微调实验对比显示：高质量的方言数据对大型语言模型表现有积极促进作用。

Conclusion: 该论文首次提供了覆盖10种方言的大规模希腊方言数据集，并实验证明了高质量方言数据能显著提升大语言模型在希腊方言任务上的表现，为相关领域研究和应用奠定了坚实基础。

Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [67] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: 该论文介绍了PLLuM（波兰大型语言模型），是目前最大最开源的专为波兰语定制的大模型家族。PLLuM由主要波兰研究机构联合开发，包含新的1400亿token波兰语训练数据、定制指令与偏好数据，并融入了负责任AI的治理框架。它提升了波兰语AI能力并支持开放研究。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型主要聚焦于英语，导致其它语言（如波兰语）支持有限。这阻碍了非英语地区高质量、可控、文化相关AI模型的发展。作者希望打破这种局限，推动波兰本地AI生态自主和繁荣。

Method: 作者构建了专为波兰语服务的PLLuM，包括:
1. 汇集1400亿token波兰语文本进行预训练。
2. 研发7.7万条定制指令数据和10万条偏好优化样本。
3. 引入严格数据治理和混合输出纠正、安全过滤模块作为负责任AI框架。
4. 详细说明了模型架构、训练流程和对齐（Alignment）技巧。
5. 提供底座模型和指令微调（Instruction-tuned）两种版本。

Result: PLLuM系列模型在公共行政等下游实际任务中展示出良好性能和应用价值。

Conclusion: PLLuM作为最大开源波兰语大模型，填补了英美以外AI模型的空白，提升波兰AI技术主权并促进开放研究，对多语言AI生态建设具有重要意义。

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [68] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

TL;DR: 作者提出了一种名为STARS的新算法，用于在解码时高效且精细地提升大语言模型与人类价值观对齐的程度，显著优于现有微调和偏好优化方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型对齐方法如微调计算消耗大，效果还不理想；而推理时方法如Best-of-N采样又需要极高计算资源。为解决这些成本和效率问题，作者希望找到一种既高效又效果好的新方法。

Method: 提出STARS算法，在生成过程中引入段落级别的候选token片段采样、打分与拒绝接受机制，实现每一小段都能即时调整生成路径，提高效率与对齐质量。

Result: STARS在六个不同的LLM上进行实验，相比SFT提升最多14.9%，相比DPO提升最多4.3%的胜率，并且与强力的Best-of-N基线方法相比也具备很强竞争力。

Conclusion: STARS作为一种以奖励为指导的细粒度采样方法，为LLM的对齐提供了一个具备通用性、鲁棒性和高效率的新方法，有望替代传统微调及全序列排序方法。

Abstract: Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [69] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

TL;DR: 提出了一种有效利用大语言模型(LLM)进行多标签文本分类的新方法，通过将分类任务重构为一系列的二元（是/否）决策，并结合前缀缓存机制，大幅提高了短文本推理的效率且保持精度。方法在情感文本分析24个维度上验证有效，并通过蒸馏提升了小模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有多标签文本分类用LLM通常效率低，尤其是在需要一并生成所有标签时，推理慢且算力消耗大。作者旨在提升LLM在多标签分类特别是情感类任务上的推理效率和实用性。

Method: 1. 将多标签分类问题重铸为对每个标签独立的二元(是/否)决策查询；2. 引入前缀缓存机制加速短文本分析；3. 用LLM到小型语言模型(SLM)蒸馏，先让强大的LLM生成多标签注释，再用这些数据微调小模型。

Result: 微调后的小模型（HerBERT-Large、CLARIN-1B等）在训练中见过的情感维度上，相比zero-shot基线有显著提升，同时采用二元查询和缓存机制在推理效率上也有“大幅提升”。

Conclusion: 多标签分类可以高效分解为二元决策，结合蒸馏与缓存推理，能在保证精度的同时大幅提升LLM分类速度与扩展性。该方法虽在情感分析上验证，但具备跨领域通用性。

Abstract: We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [70] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

TL;DR: 论文分析了三种低资源语言（Afan Oromo、阿姆哈拉语、提格里尼亚语）机器翻译数据集中的性别表征和内容偏见，发现数据集中存在严重的男性倾斜和对女性的有害刻板印象，强调数据数量增加并不等于质量提升。


<details>
  <summary>Details</summary>
Motivation: 随着NLP领域对低资源语言的研究增加，大量数据集被收集但质量未必有保障。作者关注，如果忽视质量，仅追求数量，可能导致模型表现不佳并扩大社会偏见，尤其是在少数语言的数据中。

Method: 论文以Afan Oromo, 阿姆哈拉语和提格里尼亚语三种低资源语言为对象，分析其机器翻译数据集的来源领域和性别表征，包括人名、动词的语法性别以及数据集中的刻板印象表现，并搜集和检视可能存在的有害与毒性内容。

Result: 研究发现训练数据以政治和宗教文本为主，测试集集中于新闻、健康和体育领域。数据在性别维度上极度向男性倾斜，包括人名和语法性别，用以描述女性的毒性和有害内容数量明显，且“数据最多”的语言问题最突出。

Conclusion: 论文认为，单纯扩大低资源语言数据集数量无法保证质量，更有可能加剧语言技术中的偏见和社会危害。呼吁NLP社区在数据收集早期关注数据质量，主动发现和缓解潜在有害内容。

Abstract: As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [71] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: 提出了一种新方法GRAD，通过在解码过程中结合语料库中检索到的统计证据，有效降低大语言模型幻觉（hallucination）发生率，无需额外训练或重知识集成，实验显著提升模型准确性和真实性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型规模持续扩大，但幻觉问题依然严重。已有方法大多依赖外部知识源，如知识图谱或数据库，但这些方法存在成本高、调用难、领域敏感等问题。研究动机在于如何在不引入高开销、不重训练的情况下，有效缓解LLM的幻觉倾向。

Method: 提出Graph-Retrieved Adaptive Decoding（GRAD）方法。在解码时，对一小规模检索语料进行前向推理，得到下一个token的概率，并构建稀疏token转移图。解码过程中将图中检索到的logits与当前模型logits自适应融合，并采用最大归一化策略，引导语言模型生成更有证据支持的内容。

Result: 在三个不同模型和多项基准测试（涵盖内在/外在幻觉、事实性等）上，GRAD方法相较贪婪解码等基线，内在准确率最多提升9.7%，幻觉率降低8.6%，正确率提升6.9%。真值与信息性综合评分最高，优于对比解码与知识图谱增强等方法。

Conclusion: GRAD是一种高效、易用、无须返训、大大提升LLM输出真实性和可验证性的解码方法。该方法表明，仅依靠语料统计证据，便可有效引导模型朝向真实可信生成，适合在各种LLM应用中替代昂贵的知识集成流程。

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [72] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

TL;DR: 论文研究了人在多轮迭代指称游戏（Iterated Reference Games）中的表现，并比较了视觉-语言模型在不同语境（信息量、顺序、相关性）下的表现。结果发现，模型在无关语境下表现逊于人类，但在有相关语境时性能大增。对于涉及抽象指代物的少样本任务，模型依然有很大挑战。


<details>
  <summary>Details</summary>
Motivation: 在现实交流中，语境常常会发生变化，人们需要根据语境灵活解释语言。但目前大多数机器学习模型是否能像人类一样灵活适应语境，并进行多轮推理，尚未得到充分验证。作者希望通过构造迭代指称游戏来系统探查视觉-语言模型与人类处理变换语境时的能力差距。

Method: 作者设计了多轮迭代指称游戏实验，让人类与视觉-语言模型参与，并对语境条件（信息量、语序、相关性）进行操控。比较不同条件下模型与人类的表现，尤其关注模型在关键情境中是否能够动态适应语境变化。

Result: 结果显示，在没有相关语境时，视觉-语言模型表现虽高于随机、低于人类。但一旦引入相关语境，模型在多轮实验中性能大幅提升。尽管如此，对于抽象指代的少样本任务，模型表现依然显著落后于人类。

Conclusion: 作者认为，当前视觉-语言模型对语境有一定敏感性，但与人类的灵活语境适应与推理能力相比尚有差距，尤其是在高阶抽象和少样本条件下。针对多轮、多变语境的推理仍然是机器学习模型亟需攻克的难题。

Abstract: Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [73] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

TL;DR: 该论文提出了一种利用社交媒体推文大数据，量化和分析美国不同地区人类繁荣（幸福、健康、目标、道德等多维度）状况的新方法。


<details>
  <summary>Details</summary>
Motivation: 目前对人类繁荣的指标往往局限于经济数据，时间和空间分辨率有限，无法全面反映社会真实状况。作者希望提供一种更细致、更全面的度量方法。

Method: 作者基于约26亿条2013-2023年美国推文，利用大语言模型对涉及48个繁荣指标的表达进行分类，这些指标与哈佛全球繁荣研究框架一致，并额外引入了移民态度与腐败感知。构建出可按月、年、县和州粒度查询的繁荣相关话语数据集，并通过与传统指标的对比验证其有效性。

Result: 生成了分辨率前所未有的美国社会繁荣状况数据库，能够反映美国各地、各时期繁荣相关舆论变化，并证明这些数据与权威繁荣指标之间存在合理相关性。

Conclusion: HFGI为多学科分析社会福利、不平等和社会变迁提供了全新工具，有助于理解人类繁荣动态及其在社交媒体舆论中的反映。

Abstract: Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [74] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 本文提出了一种在多智能体（multi-agent）场景下，避免只用纯文本令牌进行信息交流的新方法，通过向量翻译跨模型传递语义信息，提升交流效率。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体交流通常只依赖于明文token传递，这种方式丢失了大量潜在语义信息，并带来了不必要的计算开销。作者希望探索更高效、更深层次的语义交流机制。

Method: 作者设计了一个向量桥接（latent bridge）方案，使用学习得到的dual-encoder映射，在不同大模型（如Llama-2-7B和Mistral-7B-Instruct）间进行表示的向量翻译，再以30%的混合比例将翻译后的表示注入目标模型进行生成。

Result: 翻译后的向量在两个模型之间实现了0.538的平均余弦相似度，同时在目标模型生成中不会引发不稳定。双向评测还显示普通预训练模型的表示更便于迁移，且存在2.01:1的迁移非对称性。

Conclusion: 保守地注入跨模型向量能在保证稳定性的前提下，有效实现语义层面的交流，为协作型AI系统构建提供了新思路，不再局限于token交换。

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [75] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: 本文提出将溯因推理（abductive inference）融入检索增强大语言模型（RAG）中，以弥补现有RAG在检索证据不完整时的推理缺口，实验表明提升了答案准确性和推理的可信度。


<details>
  <summary>Details</summary>
Motivation: 检索增强大语言模型（RAG）在知识密集型任务中表现良好，但当检索到的证据不全时，RAG模型常无法完善推理链，影响最终答案的准确性和可解释性。因此，作者旨在解决RAG在信息缺失情况下的推理缺陷。

Method: 提出了一种新框架，将溯因推理集成到RAG系统中。其主要流程包括：1）检测证据是否不足；2）自动生成可能缺失的中间前提信息；3）通过一致性和合理性检验，筛选有效的补全前提。

Result: 在溯因推理和多跳问答任务基准测试上，集成溯因推理的RAG系统提升了答案的准确率和推理过程的可信度。

Conclusion: 溯因推理为RAG系统在面对证据信息不足时提供了推理与补全的能力，使其在鲁棒性和可解释性方面都有显著提升。

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [76] [WST: Weakly Supervised Transducer for Automatic Speech Recognition](https://arxiv.org/abs/2511.04035)
*Dongji Gao,Chenda Liao,Changliang Liu,Matthew Wiesner,Leibny Paola Garcia,Daniel Povey,Sanjeev Khudanpur,Jian Wu*

Main category: cs.CL

TL;DR: 本文提出了一种弱监督Transducer（WST）方法，以减少端到端自动语音识别（ASR）对高质量人工标注数据的依赖， 且在转录错误率高达70%的情况下依然能够保持较好效果，优于现有CTC类弱监督方法。


<details>
  <summary>Details</summary>
Motivation: RNN-T等端到端语音识别模型对大量高质量标注语料依赖很重，但这些数据很难获取且成本高昂，因此需要能在标注数据不完美的情况下也能有效训练的技术。

Method: 作者提出Weakly Supervised Transducer（WST），使用一种更灵活的训练图，能够鲁棒地处理转录文本中的错误，无需额外置信度评估模块或辅助的预训练模型。

Result: 在模拟数据和真实工业数据集上的实验表明，WST在转录错误率高达70%时依然能维持较好性能，并且始终优于BTC和OTC等基于CTC的弱监督方案。

Conclusion: WST提升了ASR在弱标注条件下的实用性与鲁棒性，有效缓解了对精确标注数据的依赖，具备实际应用价值，相关实现会开源。

Abstract: The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in
end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily
on large-scale, high-quality annotated data, which are often costly and
difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised
Transducer (WST), which integrates a flexible training graph designed to
robustly handle errors in the transcripts without requiring additional
confidence estimation or auxiliary pre-trained models. Empirical evaluations on
synthetic and industrial datasets reveal that WST effectively maintains
performance even with transcription error rates of up to 70%, consistently
outperforming existing Connectionist Temporal Classification (CTC)-based weakly
supervised approaches, such as Bypass Temporal Classification (BTC) and
Omni-Temporal Classification (OTC). These results demonstrate the practical
utility and robustness of WST in realistic ASR settings. The implementation
will be publicly available.

</details>


### [77] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

TL;DR: 现有LLMs在知识密集型领域应用时，解释能力评估不够贴合专家直觉。本文提出T-FIX基准，跨七大领域，结合专家开发对齐新指标。


<details>
  <summary>Details</summary>
Motivation: 在外科手术、天文学和心理治疗等知识密集型领域，用户需要LLMs不仅能给出答案，还要给出能够体现专家级推理过程的解释。然而，现有评估方法只关注解释的表面合理性或内部一致性，忽视了内容是否真正对齐专家直觉。为此，作者欲提出更科学的评价标准。

Method: 作者提出一种新的评估标准“expert alignment”，以及一个涵盖七个知识密集型领域的基准数据集T-FIX。他们还与领域专家合作，设计新颖的评价指标，用于衡量LLM解释与专家判断的一致程度。

Result: 建立了T-FIX基准，并开发出可量化LLM与领域专家解释对齐度的评估指标，为解释能力评测提供了新的工具。

Conclusion: 针对LLMs在高知识专业领域解释能力评估的不足，T-FIX和相关指标为后续改进LLM解释和对齐能力提供了更科学的测试工具和参考标准。

Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [78] [Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04072)
*Xinying Qian,Ying Zhang,Yu Zhao,Baohang Zhou,Xuhui Sui,Xiaojie Yuan*

Main category: cs.CL

TL;DR: 本文提出了PoK框架，有效提升了大型语言模型（LLMs）在时序知识图谱问答（TKGQA）中的时间推理能力和检索准确率，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的TKGQA方法（如预训练嵌入或图神经网络）虽然引入了时序知识，但在理解复杂时间约束的语义信息上存在不足。尽管LLMs在语义理解和推理泛化上表现优异，但在时序推理、事实一致性和知识完整性方面仍有限，容易出现幻觉等问题。因此需要一种更能结合结构化事实和推理计划的方法。

Method: 论文提出了PoK（Plan of Knowledge）框架，包括两个核心模块：1）知识计划模块将复杂时间性问题分解成一系列子目标，并借助预定义工具作为推理的中间指导；2）构建时序知识存储（TKS），并使用对比检索机制从TKG中挑选出语义和时间上相关的事实。两个模块协同，增强了结构化推理和事实的一致性。

Result: 在四个TKGQA标准数据集上的大量实验证明，PoK提升了大语言模型的检索精度与推理准确率，相较最先进方法，性能最高提升达56.0%。

Conclusion: PoK利用计划分解与时序检索，有效提升了LLMs在TKGQA任务中的解释性和事实一致性，为时间敏感型知识问答提供了更准确且可解释的推理方案。

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.

</details>


### [79] [The truth is no diaper: Human and AI-generated associations to emotional words](https://arxiv.org/abs/2511.04077)
*Špela Vintar,Jan Jona Javoršek*

Main category: cs.CL

TL;DR: 本文比较了人类与大语言模型（LLM）在词语联想上的表现，关注情感词汇，发现LLM的联想与人类有一定重叠但不完全相同，LLM的联想更倾向情感放大且较为可预测，缺乏人类的创造性。


<details>
  <summary>Details</summary>
Motivation: 人类进行词语联想常被用来解析大脑词汇系统，但联想结果受到个体经历、情绪和认知风格影响，具有不可预测性。理解人类与LLM在这一认知活动上的异同，有助于了解大模型的心理表征与创造力局限。

Method: 对比实验，考查人类与大语言模型对带有情感色彩词汇的联想反应，通过分析两者在联想结果中的相似性、情感放大效应、创造力和可预测性等。

Result: 人类与LLM的联想有一定重叠，但重叠度中等。LLM在面对情绪词时，联想结果情绪色彩更强，同时联想内容更可预测且缺乏创新性。

Conclusion: LLM能在一定程度上模拟人类的词语联想，但在情感放大和创造力方面存在差距，表现出较高的可预测性和较低的创造性。

Abstract: Human word associations are a well-known method of gaining insight into the
internal mental lexicon, but the responses spontaneously offered by human
participants to word cues are not always predictable as they may be influenced
by personal experience, emotions or individual cognitive styles. The ability to
form associative links between seemingly unrelated concepts can be the driving
mechanisms of creativity. We perform a comparison of the associative behaviour
of humans compared to large language models. More specifically, we explore
associations to emotionally loaded words and try to determine whether large
language models generate associations in a similar way to humans. We find that
the overlap between humans and LLMs is moderate, but also that the associations
of LLMs tend to amplify the underlying emotional load of the stimulus, and that
they tend to be more predictable and less creative than human ones.

</details>


### [80] [Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods](https://arxiv.org/abs/2511.04079)
*Eva Prakash,Maayane Attias,Pierre Chambon,Justin Xu,Steven Truong,Jean-Benoit Delbrouck,Tessa Cook,Curtis Langlotz*

Main category: cs.CL

TL;DR: 本文提出并验证了一种基于transformer的大规模影像报告去标识化模型，在多机构数据集上比现有学术和商用系统表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的放射科报告去标识模型在跨机构泛化、检测新型PHI类别和与商用系统性能对比方面存在不足。本文希望通过扩展训练数据集和增加识别类别，提高模型性能和应用广度。

Method: 作者利用Stanford大规模注释数据集（主要涵盖胸部X光、CT、脑部MR等）对现有transformer架构微调，并引入新PHI类别（年龄），在Stanford和Penn大学测试集上检测PHI。同时，评估了合成PHI生成的稳定性，并与多个云厂商系统进行对比。

Result: 模型在Penn和Stanford数据集上的F1分别达0.973和0.996，优于以往模型。对于50组独立的合成PHI Penn数据集，模型依然稳定（F1: 0.959），在合成报告上显著超越各主流商用系统（F1: 0.960 vs. 0.632-0.754）。

Conclusion: 大规模多模态训练和合成PHI数据方法极大提升了去标识模型跨机构鲁棒性。本研究设立了放射科文本隐私保护的新标杆，有望广泛应用于临床文本安全处理。

Abstract: Objective: To enhance automated de-identification of radiology reports by
scaling transformer-based models through extensive training datasets and
benchmarking performance against commercial cloud vendor systems for protected
health information (PHI) detection. Materials and Methods: In this
retrospective study, we built upon a state-of-the-art, transformer-based, PHI
de-identification pipeline by fine-tuning on two large annotated radiology
corpora from Stanford University, encompassing chest X-ray, chest CT,
abdomen/pelvis CT, and brain MR reports and introducing an additional PHI
category (AGE) into the architecture. Model performance was evaluated on test
sets from Stanford and the University of Pennsylvania (Penn) for token-level
PHI detection. We further assessed (1) the stability of synthetic PHI
generation using a "hide-in-plain-sight" method and (2) performance against
commercial systems. Precision, recall, and F1 scores were computed across all
PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the
Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining
the previous state-of-the-art model performance. Synthetic PHI evaluation
showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50
independently de-identified Penn datasets. Our model outperformed all vendor
systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).
Discussion: Large-scale, multimodal training improved cross-institutional
generalization and robustness. Synthetic PHI generation preserved data utility
while ensuring privacy. Conclusion: A transformer-based de-identification model
trained on diverse radiology datasets outperforms prior academic and commercial
systems in PHI detection and establishes a new benchmark for secure clinical
text processing.

</details>


### [81] [A Characterization of List Language Identification in the Limit](https://arxiv.org/abs/2511.04103)
*Moses Charikar,Chirag Pabbaraju,Ambuj Tewari*

Main category: cs.CL

TL;DR: 本文探讨了在“极限下的语言识别”问题中，允许学习者每步给出k个猜测后，哪些语言集合是可识别的，并给出了严密刻画和相关速率。


<details>
  <summary>Details</summary>
Motivation: 传统上Gold和Angluin的理论表明，单一猜测下大多数有趣的语言集合无法被极限识别。然而，最近关于语言生成的正面进展激发作者重新研究，如果允许学习者每步输出一个k长列表（即多猜几次），是否可以克服传统的不可识别性限制。

Method: 作者拓展了Angluin的刻画，提出一种递归方法，准确刻画了可被k-列表极限识别的语言集合，并给出了这些集合与“可分解成k个可极限识别子集合”之间的等价关系。此外，作者还在统计设定下分析了k-列表识别的速率。

Result: 作者证明了只要一个集合可被k-列表极限识别，则它能以指数速率被k-列表识别，这一速率是最好可能的；否则无法以任何收敛速率k-列表识别。

Conclusion: 拓展了经典的语言识别理论，给出了多列表(k-list)极限识别的严密理论刻画，并分析了其实用速率，为理解和设计更强识别能力的学习系统奠定理论基础。

Abstract: We study the problem of language identification in the limit, where given a
sequence of examples from a target language, the goal of the learner is to
output a sequence of guesses for the target language such that all the guesses
beyond some finite time are correct. Classical results of Gold showed that
language identification in the limit is impossible for essentially any
interesting collection of languages. Later, Angluin gave a precise
characterization of language collections for which this task is possible.
Motivated by recent positive results for the related problem of language
generation, we revisit the classic language identification problem in the
setting where the learner is given the additional power of producing a list of
$k$ guesses at each time step. The goal is to ensure that beyond some finite
time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be
$k$-list identified in the limit, based on a recursive version of Angluin's
characterization (for language identification with a list of size $1$). This
further leads to a conceptually appealing characterization: A language
collection can be $k$-list identified in the limit if and only if the
collection can be decomposed into $k$ collections of languages, each of which
can be identified in the limit (with a list of size $1$). We also use our
characterization to establish rates for list identification in the statistical
setting where the input is drawn as an i.i.d. stream from a distribution
supported on some language in the collection. Our results show that if a
collection is $k$-list identifiable in the limit, then the collection can be
$k$-list identified at an exponential rate, and this is best possible. On the
other hand, if a collection is not $k$-list identifiable in the limit, then it
cannot be $k$-list identified at any rate that goes to zero.

</details>


### [82] [Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models](https://arxiv.org/abs/2511.04108)
*Wenmo Qiu,Saurabh Srivastava*

Main category: cs.CL

TL;DR: 本论文发现批处理推理不仅能降低大模型推理成本，还能提升推理准确率和效率，减少无关冗余，且具备推理正则化作用。


<details>
  <summary>Details</summary>
Motivation: 传统上批量推理主要被视为减少推理成本和提升吞吐效率的手段，但作者怀疑批处理可能还会带来额外的推理表现提升，尤其是在大型推理模型（如LLM）多步推理场景下。

Method: 作者在13个不同基准数据集上，对比分析了批处理与非批处理情况下大型语言模型在推理任务中的表现，并分析模型行为，包括语言风格、推理路径、答案决策等。

Result: 批处理不仅提升了推理准确率，同时显著减少了推理过程所需的token数（通常减少3-5倍），还抑制了模型“过度思考”、减少了犹豫和修正性语言，使答案更果断。还观察到模型在批内不同样本间出现了模式泛化等集体效应。

Conclusion: 批处理推理不仅能提高效率，还有力地正则化了LLM的推理行为，提升了推理质量和可靠性，远不只是性能优化工具。

Abstract: Recent work has explored batch prompting as a strategy to amortize inference
cost in large language models (LLMs). In this paper, we show that batching
offers an additional, underappreciated benefit: it regularizes model behavior
during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a
comprehensive study across 13 diverse benchmarks and observe that batching
improves accuracy while substantially reducing reasoning token usage, often by
3x-5x. Through detailed behavioral analysis, we find that batching suppresses
overthinking, reduces hedging language (e.g., repetitive self-corrections), and
encourages more decisive answers. Surprisingly, we also observe emergent
collective effects in batched inference: models often generalize patterns from
earlier examples to solve harder ones in the same batch. These findings
position batching not just as a throughput optimization, but as a powerful
inference-time regularizer for more efficient and reliable LLM reasoning.

</details>


### [83] [RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning](https://arxiv.org/abs/2511.04120)
*Xinyuan Li,Murong Xu,Wenbiao Tao,Hanlun Zhu,Yike Zhao,Jipeng Zhang,Yunshi Lan*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于对抗扰动的问题重写框架RIDE，利用项目反应理论（IRT）来生成更具挑战性且良构的数学问题，从而更有效评估大模型的数学推理能力，实验表明大模型在新生成问题上表现大幅下降，揭示其鲁棒性不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的数学推理能力评价结果可能因训练数据泄漏或表层模式匹配被高估，现有规则扰动方法难以系统、精准地提升问题难度及构建基准，因此需要更客观严谨的方法评估模型的真实推理能力。

Method: 作者提出RIDE框架，通过35个大语言模型模拟“学生”，用IRT评价数学题目的难度，并构建难度排序器和作为奖励信号，引导RL中的问题重写模型将已知题目生成不同难度的高质量变体，针对数学竞赛题集测试，多模型联合验证其效果。

Result: 应用RIDE生成的难题在26个主流LLM上的平均表现下降了21.73%，显著削弱了模型原有的推理表现，显示提出方法能更严格测试模型推理能力和鲁棒性。

Conclusion: RIDE为开发和评估数学推理能力更强的LLM提供了有效工具，实验也证实了当下主流模型在微扰难题面前暴露出明显的鲁棒性短板。

Abstract: Large language models (LLMs) achieve high performance on mathematical
reasoning, but these results can be inflated by training data leakage or
superficial pattern matching rather than genuine reasoning. To this end, an
adversarial perturbation-based evaluation is needed to measure true
mathematical reasoning ability. Current rule-based perturbation methods often
generate ill-posed questions and impede the systematic evaluation of question
difficulty and the evolution of benchmarks. To bridge this gap, we propose
RIDE, a novel adversarial question-rewriting framework that leverages Item
Response Theory (IRT) to rigorously measure question difficulty and to generate
intrinsically more challenging, well-posed variations of mathematical problems.
We employ 35 LLMs to simulate students and build a difficulty ranker from their
responses. This ranker provides a reward signal during reinforcement learning
and guides a question-rewriting model to reformulate existing questions across
difficulty levels. Applying RIDE to competition-level mathematical benchmarks
yields perturbed versions that degrade advanced LLM performance, with
experiments showing an average 21.73% drop across 26 models, thereby exposing
limited robustness in mathematical reasoning and confirming the validity of our
evaluation approach.

</details>


### [84] [CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese](https://arxiv.org/abs/2511.04139)
*Dazhong Chen,Yi-Cheng Lin,Yuchen Huang,Ziwei Gong,Di Jiang,Zeying Xie,Yi R.,Fung*

Main category: cs.CL

TL;DR: 本文提出了一种结合ASR与大音频-语言模型（LALM）的协作纠错框架CantoASR，有效提升了粤语语音识别的准确率，并显著优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 粤语作为一种低资源语言，存在标注数据稀缺、六种声调、变调以及口音变化多等复杂性，主流ASR模型在粤语上表现不佳，尤其语音识别错误率高。现有的大音频-语言模型虽然能充分利用上下文推理能力，但对声调和韵律特征的利用不足。因此，有必要开发结合声学特征与语言模型推理优势的新方法以提升低资源声调和方言的ASR性能。

Method: 提出了CantoASR框架：（1）通过强制对齐方式提取声学特征；（2）对Whisper模型进行LoRA微调以增强声调区分能力；（3）采用经指令微调的Qwen-Audio模型进行韵律感知纠错，将声学线索与LALM推理有效协同。

Result: 在自发式粤语语料测试中，CantoASR在字错误率（CER）上相比Whisper-Large-V3获得了显著的提升。

Conclusion: 集成声学特征和大音频语言模型推理是一种可扩展的低资源声调和方言ASR解决方案，能有效提升粤语等复杂语音的识别准确率。

Abstract: Automatic speech recognition (ASR) is critical for language accessibility,
yet low-resource Cantonese remains challenging due to limited annotated data,
six lexical tones, tone sandhi, and accent variation. Existing ASR models, such
as Whisper, often suffer from high word error rates. Large audio-language
models (LALMs), in contrast, can leverage broader contextual reasoning but
still require explicit tonal and prosodic acoustic cues. We introduce CantoASR,
a collaborative ASR-LALM error correction framework that integrates forced
alignment for acoustic feature extraction, a LoRA-finetuned Whisper for
improved tone discrimination, and an instruction-tuned Qwen-Audio for
prosody-aware correction. Evaluations on spontaneous Cantonese data show
substantial CER gains over Whisper-Large-V3. These findings suggest that
integrating acoustic cues with LALM reasoning provides a scalable strategy for
low-resource tonal and dialectal ASR.

</details>


### [85] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 本文提出了三种多智能体LLM流水线方法，系统性地对不同规模的开源模型在Text-to-SQL任务上的性能进行评测，发现多智能体讨论机制能有效提升小模型的表现。


<details>
  <summary>Details</summary>
Motivation: 目前主流Text-to-SQL研究多依赖大型预训练模型和复杂的推理流程，而高效且小规模的模型被忽视。作者希望研究：如何通过多智能体协作机制提升小型大语言模型在复杂SQL生成任务上的性能。

Method: 作者设计了三条多智能体流水线：（1）多智能体讨论，每轮由不同agent批判并精炼SQL，裁判综合输出；（2）“规划员-代码员”分工，由规划员agent生成步骤计划，代码员负责实现；（3）“代码员-聚合员”并行，多代码员独立生成SQL，再由聚合员选优。作者用Bird-Bench Mini-Dev数据集，对小到大型开源LLM逐步测试。

Result: 实验显示，多智能体讨论能将Qwen2.5-7b-Instruct模型执行准确率提升10.6%；而整体各流水线表现，以Reasoner-Coder（规划员-代码员）组合最佳：DeepSeek-R1-32B、QwQ-32B作为规划员使Gemma 3 27B IT准确率由52.4%升至56.4%，为最高分。

Conclusion: 多智能体策略特别能显著提升小型及中型LLM在Text-to-SQL复杂任务中的表现，为高效SQL生成系统提供了新方案。

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [86] [Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains](https://arxiv.org/abs/2511.04184)
*Mohammed Musthafa Rafi,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CL

TL;DR: AI 生成内容导致了信息冗余和压缩循环，LAAC（LLM 作为沟通者）提出让大模型参与真实意图的捕捉和传达以促进真实交流，但其信任度需全面评估。


<details>
  <summary>Details</summary>
Motivation: 当前 AI 生成内容常被用于将简单想法扩展成冗长文本，收件人再用 AI 压缩为摘要，造成交流脱离真实内容。作者希望打破这个循环，实现真实、有意义的信息交换。

Method: 提出 LAAC（LLM as a Communicator）新范式，通过结构化对话提升 LLM 捕捉和传递意图的能力，并在多场景（如论文、邮件等）系统地、实验性地研究其三大信任维度：信息捕获的准确性、知识输出的一致性、答复的真实性。采用多智能体架构对不同沟通场景展开实验评估。

Result: 初步实验揭示，尽管 LAAC 在多个应用场景下有应用前景，但在高信任场合中依然存在可测量的信任缺口，如意图捕获不准确、答复有幻觉/捏造等问题。

Conclusion: LAAC 模式作为突破 AI 内容“膨胀-压缩”怪圈的方案具有潜力，但要在关键沟通领域大规模应用前，还需进一步提升其在信息保真、一致性和可靠性方面的表现。

Abstract: The proliferation of AI-generated content has created an absurd communication
theater where senders use LLMs to inflate simple ideas into verbose content,
recipients use LLMs to compress them back into summaries, and as a consequence
neither party engage with authentic content. LAAC (LLM as a Communicator)
proposes a paradigm shift - positioning LLMs as intelligent communication
intermediaries that capture the sender's intent through structured dialogue and
facilitate genuine knowledge exchange with recipients. Rather than perpetuating
cycles of AI-generated inflation and compression, LAAC enables authentic
communication across diverse contexts including academic papers, proposals,
professional emails, and cross-platform content generation. However, deploying
LLMs as trusted communication intermediaries raises critical questions about
information fidelity, consistency, and reliability. This position paper
systematically evaluates the trustworthiness requirements for LAAC's deployment
across multiple communication domains. We investigate three fundamental
dimensions: (1) Information Capture Fidelity - accuracy of intent extraction
during sender interviews across different communication types, (2)
Reproducibility - consistency of structured knowledge across multiple
interaction instances, and (3) Query Response Integrity - reliability of
recipient-facing responses without hallucination, source conflation, or
fabrication. Through controlled experiments spanning multiple LAAC use cases,
we assess these trust dimensions using LAAC's multi-agent architecture.
Preliminary findings reveal measurable trust gaps that must be addressed before
LAAC can be reliably deployed in high-stakes communication scenarios.

</details>


### [87] [Computational Turing Test Reveals Systematic Differences Between Human and AI Language](https://arxiv.org/abs/2511.04195)
*Nicolò Pagan,Petter Törnberg,Christopher A. Bail,Anikó Hannák,Christopher Barrie*

Main category: cs.CL

TL;DR: 作者提出了一种新的基于计算方法的图灵测试，结合聚合度量和可解释语言学特征，评估大模型生成文本与人类文本的相似度，并系统比较了不同模型和校准方法在社交媒体数据上的表现。结果显示当前LLM在情感和情绪表达等维度仍与真实人类输出有显著差异，且在人类风格和语义忠实度间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 当前社会科学使用大型语言模型（LLM）模拟人类行为，但LLM生成文本的“逼真性”缺乏系统验证。现有方法多依赖人工判断，不够精细且结果不可靠。研究急需更稳健、可扩展的验证框架，以及对模型校准效果的系统评估。

Method: 提出集成BERT可检测性、语义相似性、语言学特征（风格与主题分布）的计算型图灵测试框架，评估LLM与人类文本间的相似度；系统比较九个开源LLM，通过微调、风格化提示、上下文检索等五种校准方法，在X（推特）、Bluesky、Reddit三平台的用户数据上进行效能基准测试。

Result: 所有LLM，即使经过校准，在情感色彩和情绪表达等关键维度，输出依然与人类文本有显著可分辨性。指令微调的模型表现反而差于基础模型，增大模型规模未能提升人类文本相似度。此外，提升人类风格相似性的同时会损失语义忠实度，二者难以兼顾。

Conclusion: 该文提出的验证框架为LLM仿真研究提供了可扩展、可靠的工具，也揭示了当前模型在捕捉人类沟通特性上的显著局限。未来LLM仿真人类文本还需警惕在人类化与保持语义准确性间的难以权衡。

Abstract: Large language models (LLMs) are increasingly used in the social sciences to
simulate human behavior, based on the assumption that they can generate
realistic, human-like text. Yet this assumption remains largely untested.
Existing validation efforts rely heavily on human-judgment-based evaluations --
testing whether humans can distinguish AI from human output -- despite evidence
that such judgments are blunt and unreliable. As a result, the field lacks
robust tools for assessing the realism of LLM-generated text or for calibrating
models to real-world data. This paper makes two contributions. First, we
introduce a computational Turing test: a validation framework that integrates
aggregate metrics (BERT-based detectability and semantic similarity) with
interpretable linguistic features (stylistic markers and topical patterns) to
assess how closely LLMs approximate human language within a given dataset.
Second, we systematically compare nine open-weight LLMs across five calibration
strategies -- including fine-tuning, stylistic prompting, and context retrieval
-- benchmarking their ability to reproduce user interactions on X (formerly
Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the
literature. Even after calibration, LLM outputs remain clearly distinguishable
from human text, particularly in affective tone and emotional expression.
Instruction-tuned models underperform their base counterparts, and scaling up
model size does not enhance human-likeness. Crucially, we identify a trade-off:
optimizing for human-likeness often comes at the cost of semantic fidelity, and
vice versa. These results provide a much-needed scalable framework for
validation and calibration in LLM simulations -- and offer a cautionary note
about their current limitations in capturing human communication.

</details>


### [88] [LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal](https://arxiv.org/abs/2511.04205)
*Michał Karp,Anna Kubaszewska,Magdalena Król,Robert Król,Aleksander Smywiński-Pohl,Mateusz Szymański,Witold Wydmański*

Main category: cs.CL

TL;DR: 本研究评估了当前大型语言模型（LLM）在波兰国家上诉委员会官方考试中的表现，发现它们仍无法通过全部考试，尤其是在写作判决部分。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在法律领域应用逐步增多，研究团队希望了解其现阶段是否能胜任关键法律资格考试（以波兰公采委员会考试为例），并探讨“LLM作为考生”与“LLM自动判案”两种新型应用的可行性。

Method: 研究详细介绍考试结构，包括公采法多项选择知识题和书面判决任务，构建了混合信息检索及抽取流程，将不同LLM（如GPT-4.1、Claude 4 Sonnet、Bielik-11B-v2.6）分别在闭卷和检索增强生成场景下进行考试测试，并采用“LLM互审”方式进行自动评分。

Result: LLM们在知识选择题部分表现良好，但在实践写作判决部分未能达及合格线。同时，LLM自动评分与官方考官判分存在较大分歧。主要原因包括幻觉（错误内容）、错误引用法规、逻辑论证弱等。

Conclusion: 虽然技术进步迅速，但以现阶段而言，LLM无法取代人类法官或独立考官，特别是在波兰公采判决等复杂法律任务中，未来应加强法律专业人士与技术团队的密切合作。

Abstract: This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.

</details>


### [89] [REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs](https://arxiv.org/abs/2511.04228)
*Liran Cohen,Yaniv Nemcovesky,Avi Mendelson*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的模型遗忘评估方法REMIND，可以更灵敏、可解释地检测目标数据是否真正被模型遗忘，提升了机器遗忘算法在隐私和合规方面的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法多依赖对单个输入的遗忘评估，容易忽视模型对语义相似样本的残余影响，导致隐私泄露和信息间接泄漏。因此需要更全面、敏感且符合实际部署需求的遗忘效果检测方法。

Method: 作者提出了REMIND方法，通过分析模型在样本周围小扰动输入上的损失变化，揭示传统单点评估无法捕捉的遗忘残余。具体地，REMIND考察目标数据在模型损失景观上的平坦程度，以区分已遗忘和未遗忘数据，无需访问模型参数，仅需查询黑盒模型。

Result: 实验证明，REMIND能有效区分已遗忘与未遗忘数据，对不同模型、数据集甚至同义改写的输入均表现出强鲁棒性，在同等条件下优于现有主流方法。

Conclusion: REMIND为语言模型的遗忘效果检测提供了新颖的视角与敏感且解释性强的评估工具，有助于提升机器遗忘的可靠性与可部署性，促进隐私与合规技术的发展。

Abstract: Machine unlearning aims to remove the influence of specific training data
from a model without requiring full retraining. This capability is crucial for
ensuring privacy, safety, and regulatory compliance. Therefore, verifying
whether a model has truly forgotten target data is essential for maintaining
reliability and trustworthiness. However, existing evaluation methods often
assess forgetting at the level of individual inputs. This approach may overlook
residual influence present in semantically similar examples. Such influence can
compromise privacy and lead to indirect information leakage. We propose REMIND
(Residual Memorization In Neighborhood Dynamics), a novel evaluation method
aiming to detect the subtle remaining influence of unlearned data and classify
whether the data has been effectively forgotten. REMIND analyzes the model's
loss over small input variations and reveals patterns unnoticed by single-point
evaluations. We show that unlearned data yield flatter, less steep loss
landscapes, while retained or unrelated data exhibit sharper, more volatile
patterns. REMIND requires only query-based access, outperforms existing methods
under similar constraints, and demonstrates robustness across different models,
datasets, and paraphrased inputs, making it practical for real-world
deployment. By providing a more sensitive and interpretable measure of
unlearning effectiveness, REMIND provides a reliable framework to assess
unlearning in language models. As a result, REMIND offers a novel perspective
on memorization and unlearning.

</details>


### [90] [Reusing Pre-Training Data at Test Time is a Compute Multiplier](https://arxiv.org/abs/2511.04234)
*Alex Fang,Thomas Voice,Ruoming Pang,Ludwig Schmidt,Tom Gunter*

Main category: cs.CL

TL;DR: 本文探讨了当前大型语言模型（LLM）在预训练过程中对于数据集信息的提取效率，以及通过检索增强生成（RAG）方法显著提升模型在多项任务中的表现。研究发现，单纯预训练没有充分利用已有数据，结合检索可大幅提升准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然预训练数据集不断改进，但很少有研究关注预训练过程中实际能够从数据中提取多少有用知识。作者希望量化预训练遗漏了多少有用数据价值，并研究这一问题在模型扩展过程中的变化。

Method: 作者使用检索增强生成（RAG）技术，在测试时结合检索数据，提高语言模型的实际推理能力。通过在标准公开数据集上，将预训练与RAG结合，并在不同规模下评估MMLU、Math-500和SimpleQA等任务表现，以测量剩余可提取的知识。

Result: 实验显示，RAG与前人单靠预训练相比，在MMLU、Math-500和SimpleQA任务中准确率显著提升。即便清洗过的数据也有此趋势。例如在MMLU上，检索相当于将算力提升5倍，用于解析检索内容的额外推理还能带来10个百分点的准确率提升。

Conclusion: 当前的预训练方式未能充分利用训练数据中的信息。通过RAG和增加推理阶段的算力，可以进一步启用被忽略的重要数据价值，提示预训练方法仍有很大改进空间。

Abstract: Large language models learn from their vast pre-training corpora, gaining the
ability to solve an ever increasing variety of tasks; yet although researchers
work to improve these datasets, there is little effort to understand how
efficient the pre-training apparatus is at extracting ideas and knowledge from
the data. In this work, we use retrieval augmented generation along with
test-time compute as a way to quantify how much dataset value was left behind
by the process of pre-training, and how this changes across scale. We
demonstrate that pre-training then retrieving from standard and largely
open-sourced datasets results in significant accuracy gains in MMLU, Math-500,
and SimpleQA, which persist through decontamination. For MMLU we observe that
retrieval acts as a ~5x compute multiplier versus pre-training alone. We show
that these results can be further improved by leveraging additional compute at
test time to parse the retrieved context, demonstrating a 10 percentage point
improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results
suggest that today's pre-training methods do not make full use of the
information in existing pre-training datasets, leaving significant room for
progress.

</details>


### [91] [Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models](https://arxiv.org/abs/2511.04248)
*Salma Mekaoui,Hiba Sofyan,Imane Amaaz,Imane Benchrif,Arsalane Zarghili,Ilham Chaker,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 本文提出了一种基于图的方法，为主题建模产生的主题词集分配有意义的标签，同时提升了解释性且计算效率高。实验结果显示该方法在准确性和效率上均优于传统方法，并可媲美 ChatGPT-3.5。


<details>
  <summary>Details</summary>
Motivation: 当前文本主题提取任务处理的大量无结构数据主要依赖高计算量方法，且现有主题建模产生的主题表述常缺乏可解释性。因此需要一种高效且能提升主题标签解释性的解决方案。

Method: 作者提出了一种基于图的主题标注方法，通过将主题词与相关语义词构建成图结构，挖掘词之间关系，自动生成有意义的主题标签，并对比多个基准方法（含 ChatGPT-3.5），在两个数据集上进行实验评测。

Result: 实验表明该方法在 BERTScore 和余弦相似度指标上优于传统基准，与 ChatGPT-3.5 表现相当，但计算资源消耗更低。

Conclusion: 该图方法能高效且准确地为主题建模结果生成可解释标签，具有较强实用性和推广价值。作者还展望了主题标注领域在可解释性与自动化方面的发展方向。

Abstract: Extracting topics from text has become an essential task, especially with the
rapid growth of unstructured textual data. Most existing works rely on highly
computational methods to address this challenge. In this paper, we argue that
probabilistic and statistical approaches, such as topic modeling (TM), can
offer effective alternatives that require fewer computational resources. TM is
a statistical method that automatically discovers topics in large collections
of unlabeled text; however, it produces topics as distributions of
representative words, which often lack clear interpretability. Our objective is
to perform topic labeling by assigning meaningful labels to these sets of
words. To achieve this without relying on computationally expensive models, we
propose a graph-based approach that not only enriches topic words with
semantically related terms but also explores the relationships among them. By
analyzing these connections within the graph, we derive suitable labels that
accurately capture each topic's meaning. We present a comparative study between
our proposed method and several benchmarks, including ChatGPT-3.5, across two
different datasets. Our method achieved consistently better results than
traditional benchmarks in terms of BERTScore and cosine similarity and produced
results comparable to ChatGPT-3.5, while remaining computationally efficient.
Finally, we discuss future directions for topic labeling and highlight
potential research avenues for enhancing interpretability and automation.

</details>


### [92] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: 论文提出了一种基于句子级重要性比的新方法SSPO，用于提升大语言模型在可验证奖励强化学习后训练阶段的性能，解决了现有算法在稳定性和采样数据利用率方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO和GSPO算法分别因为重要性比计算方式，导致训练不稳定或数据利用率低，限制了大语言模型推理能力的提升。需要一种既能稳定训练又能高效利用采样数据的方法。

Method: 作者提出SSPO，在GRPO（token级）和GSPO（响应级）基础上，采用句子级的重要性比计算，实现了稳定性和数据利用率之间的平衡。进一步引入句子熵结合PPO-CLIP，自适应调整剪切界限，鼓励高熵Token探索、抑制低熵Token过度优化。

Result: 在五个数据集的实验中，SSPO平均得分46.57，优于GRPO（43.01）和GSPO（44.42），并在三项任务上刷新最佳成绩，证明了方法卓越的性能。

Conclusion: SSPO兼顾了GRPO与GSPO的优点，显著提升了生成数据利用效率和训练稳定性，是强化学习后训练大语言模型的新进展。

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [93] [Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning](https://arxiv.org/abs/2511.04406)
*Mohammad Amin Ghanizadeh,Mohammad Javad Dousti*

Main category: cs.CL

TL;DR: 本文提出了一种用于微调机器翻译模型的数据选择方法，通过与预训练参考模型配合，利用可学习性评分筛选最有价值的数据，提升微调训练效果。实验表明，该方法显著提升了数据利用率和计算效率，并改善了翻译性能。


<details>
  <summary>Details</summary>
Motivation: 机器翻译模型的性能高度依赖于高质量的数据以及有效的数据筛选。为提升模型的鲁棒性和泛化能力，需要优化微调阶段的数据选择方法，从而减少无用或干扰性样本，提高整体训练效率和翻译成果。

Method: 作者提出一种数据选择方法，结合了学习模型和预训练参考模型的优势，通过可学习性分数（learnability score）评估每个数据样本对训练的价值，优先选取相关性和影响力最大的样本用于微调。同时，采用考虑样本间依赖关系的批次选择策略，进一步优化训练效率和数据相关性。

Result: 在mBART模型和CCMatrix数据集（包括英-波斯语及其它语对）上的实验显示，该方法的数据利用效率是iid基线的5倍。通过缓存嵌入，还可提高24倍的计算效率，并且在需要更少训练数据点的情况下，提升了泛化能力和翻译表现。

Conclusion: 该方法证明了精细化数据选择在提升机器翻译系统性能中的重要作用，可显著降低算力和数据需求，同时获得更优的翻译结果。

Abstract: Data quality and its effective selection are fundamental to improving the
performance of machine translation models, serving as cornerstones for
achieving robust and reliable translation systems. This paper presents a data
selection methodology specifically designed for fine-tuning machine translation
systems, which leverages the synergy between a learner model and a pre-trained
reference model to enhance overall training effectiveness. By defining a
learnability score, our approach systematically evaluates the utility of data
points for training, ensuring that only the most relevant and impactful
examples contribute to the fine-tuning process. Furthermore, our method employs
a batch selection strategy which considers interdependencies among data points,
optimizing the efficiency of the training process while maintaining a focus on
data relevance. Experiments on English to Persian and several other language
pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that
our method can achieve up to a fivefold improvement in data efficiency compared
to an iid baseline. Experimental results indicate that our approach improves
computational efficiency by 24 when utilizing cached embeddings, as it requires
fewer training data points. Additionally, it enhances generalization, resulting
in superior translation performance compared to random selection method.

</details>


### [94] [If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs](https://arxiv.org/abs/2511.04432)
*Lars Bungum,Charles Yijia Huang,Abeer Kashar*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在时间推理任务中的表现，通过让模型以1940年的视角回答1940年出版的挪威问答题，评估其能力，并比较了英语与挪威语提问的效果。


<details>
  <summary>Details</summary>
Motivation: 目前LLM大多基于现代知识训练，对于需要『假装自己在过去』回答历史时期问题的能力还未深入探索。此外，不同语言下的表现差异、模型规模对结果的影响也缺乏系统测试。本研究旨在填补这些认知空白。

Method: 为LLM设计以1940年为背景的问答实验，选用1940年出版的挪威语问答书，要求LLM用1940年视角作答。问题以英语和挪威语两种语言多轮提问，并选用不同规模和类型的LLM（DeepSeek-R1, Gemma3, Qwen3, Llama3.1，以及一款专门针对挪威语的大模型）。模型答案通过LLM评判，并随机由母语者核查。

Result: 实验发现：1）用英语提问始终优于挪威语，出乎预料；2）更大的模型表现更佳；3）不同模型家族和语种下，模型表现有显著差异。

Conclusion: LLM在时间推理任务上存在语种适应性和大小差异。即使是为小语种定制的模型，也不一定优于大型通用模型。未来需重视模型在不同语言背景及历史场景下的表现测试。

Abstract: In this study, we experiment with the ability of LLMs to do temporal
reasoning. Using a Norwegian book from 1940 containing trivia questions, we
prompt the LLMs to answer the questions as if it were 1940. We also pose the
questions in both English and Norwegian. Correct answers are often presented as
sentences, and grading is done by means of LLM-as-judge, with sampled checks by
a native speaker. Prompting in English consistently gave better results than in
Norwegian, an unexpected result. In contrast, using larger LLMs improved
results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,
and also the largest available LLM especially crafted for Norwegian.

</details>


### [95] [Probabilistic Textual Time Series Depression Detection](https://arxiv.org/abs/2511.04476)
*Fabian Schmidt,Seyedehmoniba Ravan,Vladimir Vlassov*

Main category: cs.CL

TL;DR: 提出了PTTSD框架，利用序列建模与不确定性预测实现抑郁程度自解释预测，验证了其先进性和临床可用性。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁严重程度（如PHQ-8评分）预测模型缺乏随时间变化的不确定性建模，且普遍在准确性与解释性方面存在不足，难以满足临床决策支持需求。

Method: 提出Probabilistic Textual Time Series Depression Detection (PTTSD) 框架，采用双向LSTM、自注意力、残差连接，并加以高斯或t分布输出层，通过负对数似然损失进行训练。设计了sequence-to-sequence和sequence-to-one两种预测方式。在E-DAIC和DAIC-WOZ等数据集上进行评估。

Result: PTTSD达到了目前文本系统中的最好表现（如E-DAIC数据集上MAE=3.85），能输出可信的预测区间。消融实验证明了注意力机制和概率建模的有效性，对比MentalBERT显示其普适性。

Conclusion: PTTSD不仅提升了抑郁程度预测的准确性和不确定性表达，还增强了结果解释性，展现出在临床决策支持上的价值和可行性。

Abstract: Accurate and interpretable predictions of depression severity are essential
for clinical decision support, yet existing models often lack uncertainty
estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time
Series Depression Detection framework that predicts PHQ-8 scores from
utterance-level clinical interviews while modeling uncertainty over time. PTTSD
includes sequence-to-sequence and sequence-to-one variants, both combining
bidirectional LSTMs, self-attention, and residual connections with Gaussian or
Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC
and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only
systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated
prediction intervals. Ablations confirm the value of attention and
probabilistic modeling, while comparisons with MentalBERT establish generality.
A three-part calibration analysis and qualitative case studies further
highlight the interpretability and clinical relevance of uncertainty-aware
forecasting.

</details>


### [96] [ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai](https://arxiv.org/abs/2511.04479)
*Surapon Nonesung,Teetouch Jaknamon,Sirinya Chaiophat,Natapong Nitarach,Chanakan Wittayasakpan,Warit Sirichotedumrong,Adisai Na-Thalang,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 该论文提出了ThaiOCRBench，第一个专门用于泰语视觉-语言模型（VLM）评估的多任务基准，重点关注复杂文档结构理解等泰语场景。作者在人类标注的多样数据集上，对多个前沿VLM进行了零样本测试，并发现专有模型显著优于开源模型。本文揭示了Open Source模型在细粒度文本识别和手写内容提取等任务上的显著弱势。该基准为评估和改进低资源、高复杂度文字场景下的VLM提供了标准和方向。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型相关评测基准主要针对高资源语言，泰语及其他低资源语言极为缺失，尤其是在结构化文本和文档理解等复杂场景。为了推动泰语视觉-语言理解的发展，亟需专门的评测数据集和评测框架。

Method: 作者构建了一个包含13类任务、2808个人工注释样本的数据集ThaiOCRBench，并在零样本设置下系统评测多种SOTA VLM（包括专有和开源模型）。同时，通过详细的错误分析，识别模型在泰语文档理解中的主要挑战。

Result: 实验结果表明，专有模型（如Gemini 2.5 Pro）在所有任务中的表现都明显优于开源模型，后者在细粒度文本识别和手写内容提取等任务上表现尤为糟糕。作者还发现模型存在语言偏见、结构错配和虚构内容等主要问题。

Conclusion: ThaiOCRBench填补了低资源、复杂文本场景下VLM评测基准的空白，能够帮助研究者更好地发现和优化泰语文档理解模型的短板，向标准化和系统化评估迈进。

Abstract: We present ThaiOCRBench, the first comprehensive benchmark for evaluating
vision-language models (VLMs) on Thai text-rich visual understanding tasks.
Despite recent progress in multimodal modeling, existing benchmarks
predominantly focus on high-resource languages, leaving Thai underrepresented,
especially in tasks requiring document structure understanding. ThaiOCRBench
addresses this gap by offering a diverse, human-annotated dataset comprising
2,808 samples across 13 task categories. We evaluate a wide range of
state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and
open-source systems. Results show a significant performance gap, with
proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source
counterparts. Notably, fine-grained text recognition and handwritten content
extraction exhibit the steepest performance drops among open-source models.
Through detailed error analysis, we identify key challenges such as language
bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a
standardized framework for assessing VLMs in low-resource, script-complex
settings, and provides actionable insights for improving Thai-language document
understanding.

</details>


### [97] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 本文提出了RUST-BENCH基准，用于评估大模型在真实表格推理任务中的能力，发现现有模型在处理复杂异构表格时存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有表格推理基准主要针对结构简单的小型表格，未能反映现实世界真实、复杂表格的挑战性，导致无法全面评估大型语言模型（LLMs）的表格推理能力。

Method: 作者构建了RUST-BENCH基准，包含来自科学（NSF基金记录）和体育（NBA统计）的2031个真实表格、7966个问题，涵盖规模、异构性、领域专属性和复杂推理等维度。使用多个主流开源/闭源大模型进行系统评测。

Result: 实验结果显示，无论是开源还是商用大模型，在面对异构模式和复杂多跳推理任务时表现不佳，凸显了当前模型和方法的局限性。

Conclusion: RUST-BENCH为表格推理研究领域提供了新的挑战和统一测试平台，揭示了当前LLMs在真实复杂环境下推理能力的不足，有助于推动模型方法的未来改进。

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [98] [OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation](https://arxiv.org/abs/2511.04495)
*Cuong Huynh,Jie Cao*

Main category: cs.CL

TL;DR: 该论文介绍了OUNLP团队为TSAR-2025比赛提交的可控易读性文本简化系统，基于LLM提示生成方法，并提出了多轮简化新方法。


<details>
  <summary>Details</summary>
Motivation: 通过分析提示驱动的文本简化方法，作者发现文本简化效果与源文本和目标文本的CEFR水平差距高度相关。这个发现激发作者设计了新的多轮简化方法以改进结果。

Method: 提出了两种多轮简化方法：基于规则的多轮简化（MRS-Rule）以及结合规则和大语言模型（LLM）的多轮简化（MRS-Joint），并采用GPT-4o生成。

Result: 所提交系统在20支队伍中排名第7。进一步改进后发现，如果以LLM生成的简化结果为起点，可以进一步提升多轮简化的性能。

Conclusion: 多轮简化方法结合LLM和规则能够提升文本简化质量；且基于CEFR层级差距的简化路线值得采纳，有潜力进一步优化系统表现。

Abstract: This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task
(Alva-Manchego et al., 2025), designed for readability-controlled text
simplification using LLM-prompting-based generation. Based on the analysis of
prompt-based text simplification methods, we discovered an interesting finding
that text simplification performance is highly related to the gap between the
source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by
this finding, we propose two multi-round simplification methods and generate
them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based
LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.
Later improvements with MRS-Joint show that taking the LLM simplified
candidates as the starting point could further boost the multi-round
simplification performance.

</details>


### [99] [Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering](https://arxiv.org/abs/2511.04499)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 该论文分析了六种大型语言模型（LLM）的“人格特质”，采用了BFI-2人格量表和不同采样温度参数，找到了各模型在人格维度表达上的差异及其背后可能的架构影响。这为LLM的调优、选择和伦理治理提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在以人为中心的应用中的普及，其“类人格”行为对模型的安全、伦理和负责任的部署变得越来越重要。因此，系统性评估LLM的人格特质对于理解和治理AI系统具有重要意义。

Method: 作者选取了六个主流LLM，使用大五人格量表（BFI-2）测试，在不同采样温度下分析模型输出的人格特征，并通过层次聚类方法对模型之间的特质分布进行对比。

Result: 结果发现，不同模型在五大人格维度中有显著差异，尤其是神经质和外向性两个维度对采样温度较为敏感。层次聚类分析显示一些模型在特质表现上更接近，说明模型架构或训练方式可能导致人格特质的稳定性差异。

Conclusion: LLM确实表现出类人格特质，不同模型和温度设定会影响其人格表达。这为LLM的调参、选择和伦理管理提供了新思路，提醒研发者关注和规避潜在的类人格风险。

Abstract: As Large Language Models (LLMs) become integral to human-centered
applications, understanding their personality-like behaviors is increasingly
important for responsible development and deployment. This paper systematically
evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to
assess trait expressions under varying sampling temperatures. We find
significant differences across four of the five personality dimensions, with
Neuroticism and Extraversion susceptible to temperature adjustments. Further,
hierarchical clustering reveals distinct model clusters, suggesting that
architectural features may predispose certain models toward stable trait
profiles. Taken together, these results offer new insights into the emergence
of personality-like patterns in LLMs and provide a new perspective on model
tuning, selection, and the ethical governance of AI systems. We share the data
and code for this analysis here:
https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

</details>


### [100] [RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG](https://arxiv.org/abs/2511.04502)
*Joshua Gao,Quoc Huy Pham,Subin Varghese,Silwal Saurav,Vedhus Hoskere*

Main category: cs.CL

TL;DR: 本文提出了一种用于专用领域RAG系统评估的新框架RAGalyst，并实证验证了其在人类对齐和自动化评估上的有效性。


<details>
  <summary>Details</summary>
Motivation: RAG技术虽然提升了LLM的事实依据能力，但其在安全关键或专门领域的评估难度很大。现有评估方法要么局限于启发式指标（难以把握领域细节），要么直接用LLM评判但未证实与人类一致。本研究旨在提出一种自动化且与人类判断高度一致的领域RAG系统评估方式。

Method: RAGalyst利用代理机制自动从原始文档生成高质量的合成QA数据，并通过过滤确保数据准确性。针对评判用的LLM-as-a-Judge指标（答案正确性与可回答性）进行优化，提升其与人类注释一致性。然后用该框架在军事、网络安全和桥梁工程三个领域对多种RAG组建与参数进行评估。

Result: 实验表明，不同领域和任务对最佳RAG配置需求差异很大，没有一种嵌入模型、LLM模型或超参数设置能通用最优。分析了RAG答错的常见原因，验证了RAGalyst指标和人类注释高度相关，能够细致反映领域差异。

Conclusion: RAGalyst为专用领域RAG系统提供了系统化、人类对齐的自动评估工具，有助于开发者明确领域权衡、做出更优设计选择。该工具已开源，为领域RAG的可靠性和效能提升提供了有力支撑。

Abstract: Retrieval-Augmented Generation (RAG) is a critical technique for grounding
Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in
specialized, safety-critical domains remains a significant challenge. Existing
evaluation frameworks often rely on heuristic-based metrics that fail to
capture domain-specific nuances and other works utilize LLM-as-a-Judge
approaches that lack validated alignment with human judgment. This paper
introduces RAGalyst, an automated, human-aligned agentic framework designed for
the rigorous evaluation of domain-specific RAG systems. RAGalyst features an
agentic pipeline that generates high-quality, synthetic question-answering (QA)
datasets from source documents, incorporating an agentic filtering step to
ensure data fidelity. The framework refines two key LLM-as-a-Judge
metrics-Answer Correctness and Answerability-using prompt optimization to
achieve a strong correlation with human annotations. Applying this framework to
evaluate various RAG components across three distinct domains (military
operations, cybersecurity, and bridge engineering), we find that performance is
highly context-dependent. No single embedding model, LLM, or hyperparameter
configuration proves universally optimal. Additionally, we provide an analysis
on the most common low Answer Correctness reasons in RAG. These findings
highlight the necessity of a systematic evaluation framework like RAGalyst,
which empowers practitioners to uncover domain-specific trade-offs and make
informed design choices for building reliable and effective RAG systems.
RAGalyst is available on our Github.

</details>


### [101] [Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways](https://arxiv.org/abs/2511.04506)
*Paloma Rabaey,Jong Hak Moon,Jung-Oh Lee,Min Gwan Kim,Hangyul Yoon,Thomas Demeester,Edward Choi*

Main category: cs.CL

TL;DR: 本文提出了一种处理放射科报告中不确定性的系统性方法，通过对显性和隐性不确定性进行量化和建模，提升医学影像报告的可结构化与下游任务效果，并发布了Lunguage++数据集。


<details>
  <summary>Details</summary>
Motivation: 放射科报告对临床决策至关重要，但由于报告中普遍存在的不确定性，影响了自动化处理和解读。过去基于规则的方法无法有效刻画语境相关的模糊性和遗漏，限制了机器学习在医学影像、辅助诊断等领域的准确性与可解释性。

Method: 作者将不确定性分为显性与隐性两类，分别设计方法处理：(1) 显性不确定性通过专家校验和大语言模型对常见模糊表达短语打分量化，并将每项发现映射至概率值；(2) 隐性不确定性通过医疗路径专家规则，系统扩展原始报告以还原关键子发现。两者结合构建了新型结构化报告数据集Lunguage++。

Result: 成功建立了包含不确定性标签的Lunguage++数据集，显著提升了报告自动结构化、影像分类、诊断推理等任务的可靠性和表现。

Conclusion: 本文方法有效量化和扩展了放射科报告中的不确定性，为临床AI模型提供了更真实、更丰富的数据支持，推动了医学影像分析向不确定性感知方向发展。

Abstract: Radiology reports are invaluable for clinical decision-making and hold great
potential for automated analysis when structured into machine-readable formats.
These reports often contain uncertainty, which we categorize into two distinct
types: (i) Explicit uncertainty reflects doubt about the presence or absence of
findings, conveyed through hedging phrases. These vary in meaning depending on
the context, making rule-based systems insufficient to quantify the level of
uncertainty for specific findings; (ii) Implicit uncertainty arises when
radiologists omit parts of their reasoning, recording only key findings or
diagnoses. Here, it is often unclear whether omitted findings are truly absent
or simply unmentioned for brevity. We address these challenges with a two-part
framework. We quantify explicit uncertainty by creating an expert-validated,
LLM-based reference ranking of common hedging phrases, and mapping each finding
to a probability value based on this reference. In addition, we model implicit
uncertainty through an expansion framework that systematically adds
characteristic sub-findings derived from expert-defined diagnostic pathways for
14 common diagnoses. Using these methods, we release Lunguage++, an expanded,
uncertainty-aware version of the Lunguage benchmark of fine-grained structured
radiology reports. This enriched resource enables uncertainty-aware image
classification, faithful diagnostic reasoning, and new investigations into the
clinical impact of diagnostic uncertainty.

</details>


### [102] [Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics](https://arxiv.org/abs/2511.04527)
*Amir Zur,Atticus Geiger,Ekdeep Singh Lubana,Eric Bigelow*

Main category: cs.CL

TL;DR: 本文探究了大型语言模型在生成文本时，隐藏层激活与推理路径不确定性的关系。研究发现，通过控制隐藏激活可以预测和影响模型的链式推理不确定性，揭示了模型在面临多项选择时对可能推理路径的隐式表征能力。


<details>
  <summary>Details</summary>
Motivation: 在生成式语言模型进行推理时，其在选择每个token时可能存在多条推理路径，使得对模型不确定性的量化变得困难。作者关注于模型是否在其隐藏状态中表征了这些可能的替代路径。

Method: 作者通过分析模型在链式推理（chain-of-thought）中的隐藏激活，研究这些激活对模型不确定性的预测和操控能力。通过操控和解读隐藏层激活，实验验证了激活干预能否反映和影响模型在不同token上的不确定性。

Result: 实验显示：模型对下一个token的不确定性与其隐藏激活驱动下的操控难易程度密切相关。此外，模型的隐藏激活可以预测其未来输出的分布，表明模型已经在内部以某种方式表征了所有可能的推理路径。

Conclusion: 隐藏层激活不仅能揭示模型生成过程中的不确定性，还允许对推理路径进行有效的干预。因此，语言模型在决策过程未完全收敛时，内部已经隐式描述了多种备选路径，这为理解和调控大模型推理开辟了新途径。

Abstract: When a language model generates text, the selection of individual tokens
might lead it down very different reasoning paths, making uncertainty difficult
to quantify. In this work, we consider whether reasoning language models
represent the alternate paths that they could take during generation. To test
this hypothesis, we use hidden activations to control and predict a language
model's uncertainty during chain-of-thought reasoning. In our experiments, we
find a clear correlation between how uncertain a model is at different tokens,
and how easily the model can be steered by controlling its activations. This
suggests that activation interventions are most effective when there are
alternate paths available to the model -- in other words, when it has not yet
committed to a particular final answer. We also find that hidden activations
can predict a model's future outcome distribution, demonstrating that models
implicitly represent the space of possible paths.

</details>


### [103] [IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection](https://arxiv.org/abs/2511.04528)
*Kaveh Eskandari Miandoab,Katharine Kowalyshyn,Kabir Pamnani,Anesu Gavhera,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: IntelliProof 是一个利用大语言模型（LLM）分析论证性作文的交互式系统，将作文以图结构呈现，并可视化各部分关系，增强用户理解和体验。


<details>
  <summary>Details</summary>
Motivation: 现有的自动作文评分系统多侧重评分结果，缺乏对作文结构与论证关系的细致呈现与解释，影响用户理解作文质量。作者希望通过结构化和可解释性方法，提升对论证性作文质量的分析深度和交互体验。

Method: 该系统借助LLM将作文解析为论证图：将主张设为节点，证据作为节点属性，论据之间的支持或反驳关系以边连接。LLM 自动为各关系分类及评分，并以可视化方式展现。用户可看到关系的解释，以及作文内在连贯性的定量分析。

Result: IntelliProof 能快速分析作文的论证质量，并给予结构化、可视化的输出。它同时提供自然语言工具，帮助理解论证图的含义，使用户更易把握作文结构和质量，并保持人工监督。线上demo已上线可用。

Conclusion: IntelliProof 有效弥合了作文结构语义与用户理解之间的差距，提升了论证作文分析的效率与交互体验，为自动作文评估与教育领域带来创新工具。

Abstract: We present IntelliProof, an interactive system for analyzing argumentative
essays through LLMs. IntelliProof structures an essay as an argumentation
graph, where claims are represented as nodes, supporting evidence is attached
as node properties, and edges encode supporting or attacking relations. Unlike
existing automated essay scoring systems, IntelliProof emphasizes the user
experience: each relation is initially classified and scored by an LLM, then
visualized for enhanced understanding. The system provides justifications for
classifications and produces quantitative measures for essay coherence. It
enables rapid exploration of argumentative quality while retaining human
oversight. In addition, IntelliProof provides a set of tools for a better
understanding of an argumentative essay and its corresponding graph in natural
language, bridging the gap between the structural semantics of argumentative
essays and the user's understanding of a given text. A live demo and the system
are available here to try: \textbf{https://intelliproof.vercel.app}

</details>


### [104] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: 本文指出，当前大型语言模型(LLMs)作为代码助手在软件开发中越来越重要，但它们生成的漏洞也对网络安全构成威胁。即便最新开源模型仍容易重复出现早期已报告的典型漏洞，显示当前平衡安全性和功能性的方式尚未根除这些问题。为解决此困境，作者提出新指标Prompt Exposure(PE)和Model Exposure(ME)来衡量和遏制此类漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管有不少提升代码生成安全性的努力，但这些改进对主流LLM实际安全性的具体成效尚不清楚。鉴于LLM代码助手日益普及，评估和量化它们生成安全漏洞的风险具有重要意义。

Method: 本文通过验证开源LLM在现实场景中能否防止历史上已知的漏洞注入，揭示其现存的安全隐患。进一步地，提出了Prompt Exposure(PE)指标，结合漏洞严重性、出现频率及触发脉络，定量刻画模型面临的实际风险，并据此推出Model Exposure(ME)综合评估。

Result: 实验发现，现有（即使是最新的）开源代码LLMs会在真实环境下反复生成早已曝光的典型安全漏洞，显示漏洞补丁与模型功能性之间的权衡至今未能实现有效消除漏洞。PE与ME指标可以更好地揭示和定位模型的安全短板。

Conclusion: 当前代码LLMs在漏洞防护上仍有较大不足，需要更有效的安全增强方法。PE和ME为社区提供了更加量化和细致的评估手段，有助于推动生成式代码模型朝着更安全的方向演进。

Abstract: As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>


### [105] [BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering](https://arxiv.org/abs/2511.04560)
*Sadia Sultana,Saiyma Sittul Muna,Mosammat Zannatul Samarukh,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 本文提出了首个大规模孟加拉语生物医学多项选择题数据集BanglaMedQA和评测基准BanglaMMedBench，并评估了多种基于检索增强生成（RAG）的方法，Agentic RAG表现最佳。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语等低资源语言缺乏高质量生物医学问答数据和系统，限制了医学知识的公平获取。研究旨在填补这一空白，提升孟加拉语医疗AI能力。

Method: 构建孟加拉语医学教材OCR语料，提出两个数据集，并用多种RAG策略（传统、零样本回退、Agentic、迭代反馈、汇总RAG）结合教材和网页检索机制，增强生成式推理。

Result: Agentic RAG方案在openai/gpt-oss-120b模型上达到89.54%的最高准确率，显著优于其他方法，并提升了解释质量。

Conclusion: 基于RAG的方法能显著提升孟加拉语生物医学问答的准确性和可访问性，为多语种医学AI研究奠定基础。

Abstract: Developing accurate biomedical Question Answering (QA) systems in
low-resource languages remains a major challenge, limiting equitable access to
reliable medical knowledge. This paper introduces BanglaMedQA and
BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice
Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical
artificial intelligence (AI). The study applies and benchmarks several
Retrieval-Augmented Generation (RAG) strategies, including Traditional,
Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining
textbook-based and web retrieval with generative reasoning to improve factual
accuracy. A key novelty lies in integrating a Bangla medical textbook corpus
through Optical Character Recognition (OCR) and implementing an Agentic RAG
pipeline that dynamically selects between retrieval and reasoning strategies.
Experimental results show that the Agentic RAG achieved the highest accuracy
89.54% with openai/gpt-oss-120b, outperforming other configurations and
demonstrating superior rationale quality. These findings highlight the
potential of RAG-based methods to enhance the reliability and accessibility of
Bangla medical QA, establishing a foundation for future research in
multilingual medical artificial intelligence.

</details>


### [106] [When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection](https://arxiv.org/abs/2511.04643)
*Alamgir Munir Qazi,John P. McCrae,Jamal Abdul Nasir*

Main category: cs.CL

TL;DR: 本文提出了DeReC（Dense Retrieval Classification）框架，用于高效的事实核查，其效果超过了当前主流的大型语言模型（LLM）方法，计算成本大大降低。


<details>
  <summary>Details</summary>
Motivation: 随着虚假信息的扩散，事实核查变得愈发关键。主流使用LLM的方法虽然准确，但计算消耗高，容易产生肖觉，为实际部署带来障碍。

Method: DeReC利用通用文本嵌入代替自回归的LLM，结合密集检索和专门的分类，把事实核查流程高效简化，同时提升准确性。

Result: DeReC在多个数据集上测试时，运行时显著优于现有LLM方法，RAWFC和LIAR-RAW上效率提升超90%。在RAWFC数据集上的F1得分从61.20%（L-Defense）提高到65.58%。

Conclusion: 经过精心设计的检索-分类系统在专用任务中能匹敌甚至超越LLM，而计算资源节省，实际部署更具可行性。

Abstract: The proliferation of misinformation necessitates robust yet computationally
efficient fact verification systems. While current state-of-the-art approaches
leverage Large Language Models (LLMs) for generating explanatory rationales,
these methods face significant computational barriers and hallucination risks
in real-world deployments. We present DeReC (Dense Retrieval Classification), a
lightweight framework that demonstrates how general-purpose text embeddings can
effectively replace autoregressive LLM-based approaches in fact verification
tasks. By combining dense retrieval with specialized classification, our system
achieves better accuracy while being significantly more efficient. DeReC
outperforms explanation-generating LLMs in efficiency, reducing runtime by 95%
on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%
on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),
showcasing its effectiveness across varying dataset sizes. On the RAWFC
dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art
method L-Defense (61.20%). Our results demonstrate that carefully engineered
retrieval-based systems can match or exceed LLM performance in specialized
tasks while being significantly more practical for real-world deployment.

</details>


### [107] [Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.04654)
*Mohammad Atif Quamar,Mohammad Areeb*

Main category: cs.CL

TL;DR: 本论文提出了一种名为LEASH的新算法，用于自适应地停止大语言模型的推理过程，显著减少生成token数量和响应延迟，但在准确率上存在一定下降。


<details>
  <summary>Details</summary>
Motivation: 传统的Chain-of-Thought（CoT）提示方法在大语言模型中实现复杂推理时，通常需要生成完整的、固定长度的推理过程，这导致了大量计算资源的浪费，包括token使用量和生成延迟显著增加。作者希望找到一种无需额外训练的新方法，能智能确定推理何时足够充分，以提高推理效率。

Method: 作者提出了一种新的解码算法LEASH（Logit-Entropy Adaptive Stopping Heuristic），其核心是训练无关，并可动态判断何时结束推理生成。LEASH通过监测模型每步输出的token级熵斜率变化和最大logit的提升幅度，当两者同时趋于平稳时，认为推理过程已达到稳定阶段即可停止生成。该方法对模型无依赖，适用于多种模型。

Result: 在GSM8K和AQuA-RAT等标准推理基准测试上，LEASH减少了30-35%的平均token生成数，降低了27%的生成延迟，但在准确率上比标准CoT方法下降了10个百分点。

Conclusion: LEASH为大语言模型的推理过程提供了一种无需额外训练、效率更高的新型解码策略，虽然准确率有所降低，但在token节省和延迟方面效果显著，适合作为现有CoT推理的高效替代方案。

Abstract: Chain-of-Thought (CoT) prompting is a key technique for enabling complex
reasoning in large language models. However, generating full, fixed-length
rationales is computationally wasteful, inflating both token usage and latency.
We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free
decoding algorithm that adaptively halts rationale generation. LEASH monitors
two intrinsic signals: the slope of token-level entropy and the improvement in
the top-logit margin. It terminates the generation once both signals plateau,
indicating the model has reached a stable reasoning state. Across four
instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces
average token generation by 30--35% and latency by 27%, while incurring a 10
p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no
additional training or supervision, offering a simple and efficient alternative
to CoT decoding.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [108] [Dynamic Shape Control of Soft Robots Enabled by Data-Driven Model Reduction](https://arxiv.org/abs/2511.03931)
*Iman Adibnazari,Harsh Sharma,Myungsun Park,Jacobo Cervera-Torralba,Boris Kramer,Michael T. Tolley*

Main category: cs.RO

TL;DR: 该论文比较了三种数据驱动的模型降阶方法在用于软体机器人动态形状控制中的效果。结论发现采用LOpInf方法的控制策略能获得更低的跟踪误差。


<details>
  <summary>Details</summary>
Motivation: 软体机器人在需要灵活动态控制其整体形状的场合有巨大潜力，但由于动力学维度高、缺乏通用建模工具，动态形状控制困难。作者意在寻找适合用于控制的线性模型降阶方法。

Method: 论文对三种常见的数据驱动模型降阶方法——特征系统实现算法（eigensystem realization algorithm）、带控制的动态模态分解（dynamic mode decomposition with control）以及拉格朗日算子推断方法（LOpInf）——进行了对比研究，并在仿真鳗鱼式软体机器人上测试了它们在模型预测控制（MPC）中的表现，包括三种实验场景下的轨迹跟踪任务。

Result: LOpInf方法用于模型预测控制时，在三类轨迹跟踪任务中的跟踪误差均低于其它两种降阶模型方法。

Conclusion: LOpInf方法在为软体机器人实现动态形状控制的MPC策略时优于其它主流数据驱动建模方法，是更为有效的模型选择。

Abstract: Soft robots have shown immense promise in settings where they can leverage
dynamic control of their entire bodies. However, effective dynamic shape
control requires a controller that accounts for the robot's high-dimensional
dynamics--a challenge exacerbated by a lack of general-purpose tools for
modeling soft robots amenably for control. In this work, we conduct a
comparative study of data-driven model reduction techniques for generating
linear models amendable to dynamic shape control. We focus on three
methods--the eigensystem realization algorithm, dynamic mode decomposition with
control, and the Lagrangian operator inference (LOpInf) method. Using each
class of model, we explored their efficacy in model predictive control policies
for the dynamic shape control of a simulated eel-inspired soft robot in three
experiments: 1) tracking simulated reference trajectories guaranteed to be
feasible, 2) tracking reference trajectories generated from a biological model
of eel kinematics, and 3) tracking reference trajectories generated by a
reduced-scale physical analog. In all experiments, the LOpInf-based policies
generated lower tracking errors than policies based on other models.

</details>


### [109] [Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots](https://arxiv.org/abs/2511.03996)
*Yushi Wang,Changsheng Luo,Penghui Chen,Jianran Liu,Weijian Sun,Tong Guo,Kechang Yang,Biao Hu,Yangang Zhang,Mingguo Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的统一控制器，使人形机器人可以通过直接融合视觉感知和运动控制，实现高效的足球技能学习。该系统在现实环境下表现出更强的反应能力和连贯的行为。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人足球控制系统通常采用分离的感知与动作模块，导致在动态环境中响应延迟与行为不连贯；此外，现实世界感知能力的局限性会加剧这些问题。因此，亟需一种端到端的协调感知与运动的解决方案，以提升行为反应性与鲁棒性。

Method: 方法上，作者提出了以强化学习为基础的控制器，将对抗性运动先验（Adversarial Motion Priors）扩展到真实动态环境下的感知场景。具体做法为设计端到端的编码-解码结构以及虚拟感知系统，模拟现实中视觉噪声与受限场景，使控制策略能从不完美的观测中恢复“特权状态”，并实现主动的感知-动作协同。

Result: 实验结果表明，该控制器具备很强的反应性，能持续表现出连贯且鲁棒的足球行为，并且在RoboCup等实际比赛中表现优异。

Conclusion: 该研究表明，将视觉感知与运动控制统一在强化学习框架下，可以显著提升人形机器人在真实动态环境中的反应性与行为一致性，为具身智能在实际复杂任务中应用提供了可行路径。

Abstract: Humanoid soccer poses a representative challenge for embodied intelligence,
requiring robots to operate within a tightly coupled perception-action loop.
However, existing systems typically rely on decoupled modules, resulting in
delayed responses and incoherent behaviors in dynamic environments, while
real-world perceptual limitations further exacerbate these issues. In this
work, we present a unified reinforcement learning-based controller that enables
humanoid robots to acquire reactive soccer skills through the direct
integration of visual perception and motion control. Our approach extends
Adversarial Motion Priors to perceptual settings in real-world dynamic
environments, bridging motion imitation and visually grounded dynamic control.
We introduce an encoder-decoder architecture combined with a virtual perception
system that models real-world visual characteristics, allowing the policy to
recover privileged states from imperfect observations and establish active
coordination between perception and action. The resulting controller
demonstrates strong reactivity, consistently executing coherent and robust
soccer behaviors across various scenarios, including real RoboCup matches.

</details>


### [110] [Integrating Ergonomics and Manipulability for Upper Limb Postural Optimization in Bimanual Human-Robot Collaboration](https://arxiv.org/abs/2511.04009)
*Chenzui Li,Yiming Chen,Xi Wu,Giacinto Barresi,Fei Chen*

Main category: cs.RO

TL;DR: 本文提出了一种上肢姿态优化方法，用于提高双人协作搬运任务中的人体工学与操作灵活性。通过建模、优化和机器人控制，有效改善了合作中的肌肉负荷状况。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注人类安全或操作效率，二者难以兼顾。针对搬运任务中人类不同抓握姿势和物体形状变化，亟需一种方法系统提升安全与灵活性。

Method: 采用简化人体骨骼模型，通过优化关节角以兼顾安全与操控能力，并提出机器人末端参考姿态变换模块，引导人类朝向最优姿态。还设计了基于模型预测的阻抗控制器，实现人形机器人抓持轨迹的动态调整。

Result: 在人-人和人-机器人协作下，对于多种物体多名被试，实验验证了该方法，肌肉激活对比显示肌肉状况显著改善。

Conclusion: 所提方法兼顾了安全与可操控性，通过优化和实时控制提升了人体工学，实验证明其对肌肉负担有显著优化作用。

Abstract: This paper introduces an upper limb postural optimization method for
enhancing physical ergonomics and force manipulability during bimanual
human-robot co-carrying tasks. Existing research typically emphasizes human
safety or manipulative efficiency, whereas our proposed method uniquely
integrates both aspects to strengthen collaboration across diverse conditions
(e.g., different grasping postures of humans, and different shapes of objects).
Specifically, the joint angles of a simplified human skeleton model are
optimized by minimizing the cost function to prioritize safety and manipulative
capability. To guide humans towards the optimized posture, the reference
end-effector poses of the robot are generated through a transformation module.
A bimanual model predictive impedance controller (MPIC) is proposed for our
human-like robot, CURI, to recalibrate the end effector poses through planned
trajectories. The proposed method has been validated through various subjects
and objects during human-human collaboration (HHC) and human-robot
collaboration (HRC). The experimental results demonstrate significant
improvement in muscle conditions by comparing the activation of target muscles
before and after optimization.

</details>


### [111] [An LLM-based Framework for Human-Swarm Teaming Cognition in Disaster Search and Rescue](https://arxiv.org/abs/2511.04042)
*Kailun Ji,Xiaoyu Hu,Xinyu Zhang,Jun Chen*

Main category: cs.RO

TL;DR: 本文提出了一种基于大语言模型（LLM）的智能人-无人机群协作系统，有效提升了灾害救援场景中无人机群的协调效率，并大幅减轻了操作者的认知负担。实验表明，该系统优于传统指令接口，显著提升了救援效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 大规模灾害救援中的地形复杂和通信受限问题使无人机群成为理想的辅助工具，但实际中无人机群的高效协调给人类操作者带来了极大的认知负担，特别是在高压力环境下，高层意图与低层指令之间的转化易出错。迫切需要新方法降低人机协作瓶颈，提升无人机群的自主性和救援任务的完成效率。

Method: 提出LLM-CRF系统，使用大语言模型捕捉操作员的自然语言或多模态（语音/图形）意图输入，并进行意图理解、任务分解和无人机群任务规划，实现任务的端到端自动化及闭环反馈，减少人工监控和直接控制需求。通过模拟救援场景进行实验验证。

Result: 实验结果显示，相较于传统命令接口，LLM驱动的方法任务完成时间缩短约64.2%，成功率提升7%，NASA-TLX认知负担评分降低42.9%。

Conclusion: LLM-CRF系统显著提升了复杂救援场景下人-无人机群协作效率，降低了认知负担，有望为高风险场景下的智能人机协作提供更直观高效的解决方案。

Abstract: Large-scale disaster Search And Rescue (SAR) operations are persistently
challenged by complex terrain and disrupted communications. While Unmanned
Aerial Vehicle (UAV) swarms offer a promising solution for tasks like wide-area
search and supply delivery, yet their effective coordination places a
significant cognitive burden on human operators. The core human-machine
collaboration bottleneck lies in the ``intention-to-action gap'', which is an
error-prone process of translating a high-level rescue objective into a
low-level swarm command under high intensity and pressure. To bridge this gap,
this study proposes a novel LLM-CRF system that leverages Large Language Models
(LLMs) to model and augment human-swarm teaming cognition. The proposed
framework initially captures the operator's intention through natural and
multi-modal interactions with the device via voice or graphical annotations. It
then employs the LLM as a cognitive engine to perform intention comprehension,
hierarchical task decomposition, and mission planning for the UAV swarm. This
closed-loop framework enables the swarm to act as a proactive partner,
providing active feedback in real-time while reducing the need for manual
monitoring and control, which considerably advances the efficacy of the SAR
task. We evaluate the proposed framework in a simulated SAR scenario.
Experimental results demonstrate that, compared to traditional order and
command-based interfaces, the proposed LLM-driven approach reduced task
completion time by approximately $64.2\%$ and improved task success rate by
$7\%$. It also leads to a considerable reduction in subjective cognitive
workload, with NASA-TLX scores dropping by $42.9\%$. This work establishes the
potential of LLMs to create more intuitive and effective human-swarm
collaborations in high-stakes scenarios.

</details>


### [112] [Enhancing Fault-Tolerant Space Computing: Guidance Navigation and Control (GNC) and Landing Vision System (LVS) Implementations on Next-Gen Multi-Core Processors](https://arxiv.org/abs/2511.04052)
*Kyongsik Yun,David Bayard,Gerik Kubiak,Austin Owens,Andrew Johnson,Ryan Johnson,Dan Scharf,Thomas Lu*

Main category: cs.RO

TL;DR: 该论文针对未来行星探测任务提出了一种高性能、容错的机载计算架构，通过在多核处理器（HPSC、VOXL2、Versal）上部署GNC与LVS算法，实现了成倍加速，并提出ARBITER多核投票机制提升计算可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着深空探测任务日益复杂，对自主导航与控制（GNC）、着陆视觉系统（LVS）等算法的需求不断提高，现有航天专用计算硬件已难以满足低延迟、高性能和容错性的综合要求。本研究旨在探索新一代多核处理器在行星任务中的应用潜力，并解决计算可靠性难题。

Method: 作者在三种新型多核处理器（HPSC、VOXL2、Versal）上，实现了GNC和LVS算法的迁移与优化，对比分析其相较于传统航天硬件的性能提升。同时，提出ARBITER多核投票机制，实现了实时故障检测与校正，还通过故障注入实验定位易受单粒子翻转影响的关键阶段，并提出针对性保护策略。

Result: LVS图像处理速度最高提升15倍，GFOLD轨迹优化速度提升250倍以上。ARBITER机制在静态优化和动态闭环控制中均有效保障计算可靠。故障注入实验发现GFOLD中的梯度计算环节对比特错误最敏感，提出了择优输出策略以增强系统鲁棒性。

Conclusion: 本工作证实了多核通用处理器在未来航天任务中应用的可行性，ARBITER方案显著提升了系统能效与容错能力，为火星样本返回等任务提供了有力的计算基础。

Abstract: Future planetary exploration missions demand high-performance, fault-tolerant
computing to enable autonomous Guidance, Navigation, and Control (GNC) and
Lander Vision System (LVS) operations during Entry, Descent, and Landing (EDL).
This paper evaluates the deployment of GNC and LVS algorithms on
next-generation multi-core processors--HPSC, Snapdragon VOXL2, and AMD Xilinx
Versal--demonstrating up to 15x speedup for LVS image processing and over 250x
speedup for Guidance for Fuel-Optimal Large Divert (GFOLD) trajectory
optimization compared to legacy spaceflight hardware. To ensure computational
reliability, we present ARBITER (Asynchronous Redundant Behavior Inspection for
Trusted Execution and Recovery), a Multi-Core Voting (MV) mechanism that
performs real-time fault detection and correction across redundant cores.
ARBITER is validated in both static optimization tasks (GFOLD) and dynamic
closed-loop control (Attitude Control System). A fault injection study further
identifies the gradient computation stage in GFOLD as the most sensitive to
bit-level errors, motivating selective protection strategies and vector-based
output arbitration. This work establishes a scalable and energy-efficient
architecture for future missions, including Mars Sample Return, Enceladus
Orbilander, and Ceres Sample Return, where onboard autonomy, low latency, and
fault resilience are critical.

</details>


### [113] [CBMC-V3: A CNS-inspired Control Framework Towards Manipulation Agility with SNN](https://arxiv.org/abs/2511.04109)
*Yanbo Pang,Qingkai Li,Mingguo Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种仿生控制框架，利用类脑脉冲神经网络实现机械臂在复杂动态环境中的灵敏操作，效果优于工业级控制算法。


<details>
  <summary>Details</summary>
Motivation: 随着机械臂应用扩展到医疗、服务和日常生活，现有控制算法难以完成在动态轨迹、多变环境和多样物体下的灵敏操作。

Method: 设计了受人类中枢神经系统启发的脉冲神经网络控制框架，内含大脑皮层、小脑、丘脑、脑干、脊髓五个模块，并按照等级和信息流分为多个子系统；各模块分别实现反馈控制、参数自适应、动力学预测及重力补偿，通过强化学习优化性能，在仿真和真实机械臂平台验证。

Result: 实验证明该方法在不同负载和轨迹下实现了比工业级位置控制更高的操作灵活性和敏捷性。

Conclusion: 类中枢神经系统的脉冲神经网络控制框架可有效提升机械臂在复杂环境下的灵活操作能力，对非工业领域机械臂发展具有重要意义。

Abstract: As robotic arm applications extend beyond industrial settings into
healthcare, service, and daily life, existing control algorithms struggle to
achieve the agile manipulation required for complex environments with dynamic
trajectories, unpredictable interactions, and diverse objects. This paper
presents a biomimetic control framework based on Spiking Neural Networks (SNN),
inspired by the human Central Nervous System (CNS), to achieve agile control in
such environments. The proposed framework features five control modules
(cerebral cortex, cerebellum, thalamus, brainstem, spinal cord), three
hierarchical control levels (first-order, second-order, third-order), and two
information pathways (ascending, descending). Each module is fully implemented
using SNN. The spinal cord module uses spike encoding and Leaky
Integrate-and-Fire (LIF) neurons for feedback control. The brainstem module
employs a network of LIF and non-spiking LIF neurons to dynamically adjust
spinal cord parameters via reinforcement learning. The thalamus module
similarly adjusts the cerebellum's torque outputs. The cerebellum module uses a
recurrent SNN to learn the robotic arm's dynamics through regression, providing
feedforward gravity compensation torques. The framework is validated both in
simulation and on real-world robotic arm platform under various loads and
trajectories. Results demonstrate that our method outperforms the
industrial-grade position control in manipulation agility.

</details>


### [114] [BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning](https://arxiv.org/abs/2511.04131)
*Yitang Li,Zhengyi Luo,Tonghe Zhang,Cunxi Dai,Anssi Kanervisto,Andrea Tirinzoni,Haoyang Weng,Kris Kitani,Mateusz Guzek,Ahmed Touati,Alessandro Lazaric,Matteo Pirotta,Guanya Shi*

Main category: cs.RO

TL;DR: 该论文提出了BFM-Zero框架，可以通过单一策略支持多种下游任务，无需重新训练，实现了类人机器人多任务控制的行为基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有类人机器人控制方法要么局限于仿真环境，要么专注于特定任务，难以统一处理多样化的控制需求。作者希望构建一个可扩展、可提示的通用策略，实现现实类人机器人多任务控制。

Method: 提出BFM-Zero，利用无监督强化学习与前向-后向（FB）模型，学习一个将动作、目标和奖励统一嵌入的共享潜在空间，使单个策略能通过不同方式被提示以完成多项下游任务。同时采用奖励塑形、领域随机化和历史相关非对称学习等技术桥接仿真与现实差异，并在仿真中做消融实验验证各种设计。

Result: BFM-Zero在现实中的Unitree G1类人机器人上实现了多样化和强健的全身技能，包括零样本动作追踪、目标到达、奖励优化和少样本自适应等推理能力。核心设计点经消融实验量化评估。

Conclusion: BFM-Zero为可扩展、可提示的类人机器人行为基础模型提供了新方向，实现了现实环境下多任务控制，为未来该领域的发展奠定基础。

Abstract: Building Behavioral Foundation Models (BFMs) for humanoid robots has the
potential to unify diverse control tasks under a single, promptable generalist
policy. However, existing approaches are either exclusively deployed on
simulated humanoid characters, or specialized to specific tasks such as
tracking. We propose BFM-Zero, a framework that learns an effective shared
latent representation that embeds motions, goals, and rewards into a common
space, enabling a single policy to be prompted for multiple downstream tasks
without retraining. This well-structured latent space in BFM-Zero enables
versatile and robust whole-body skills on a Unitree G1 humanoid in the real
world, via diverse inference methods, including zero-shot motion tracking, goal
reaching, and reward optimization, and few-shot optimization-based adaptation.
Unlike prior on-policy reinforcement learning (RL) frameworks, BFM-Zero builds
upon recent advancements in unsupervised RL and Forward-Backward (FB) models,
which offer an objective-centric, explainable, and smooth latent representation
of whole-body motions. We further extend BFM-Zero with critical reward shaping,
domain randomization, and history-dependent asymmetric learning to bridge the
sim-to-real gap. Those key design choices are quantitatively ablated in
simulation. A first-of-its-kind model, BFM-Zero establishes a step toward
scalable, promptable behavioral foundation models for whole-body humanoid
control.

</details>


### [115] [PUL-SLAM: Path-Uncertainty Co-Optimization with Lightweight Stagnation Detection for Efficient Robotic Exploration](https://arxiv.org/abs/2511.04180)
*Yizhen Yin,Dapeng Feng,Hongbo Chen,Yuhua Qi*

Main category: cs.RO

TL;DR: 该论文提出了一种结合路径-不确定性联合优化深度强化学习框架与轻量级停滞检测机制的新型主动SLAM（Active SLAM）方法，显著提升了探索效率和路径质量。


<details>
  <summary>Details</summary>
Motivation: 现有主动SLAM方法存在探索速度慢和路径次优问题，影响了机器人在复杂环境下的自主定位与建图效率。为解决这一问题，论文致力于提升探索速度并获取更优路径。

Method: 该方法将路径-不确定性联合优化深度强化学习框架与轻量级停滞检测机制结合。前者通过双目标奖励函数共同优化路径距离和地图不确定性，平衡探索与利用；后者借助激光雷达的静态异常检测和地图更新停滞检测，降低冗余探索，并在扩展率过低时中止回合。

Result: 在实验中，该方法相较于frontier-based和RRT方法，最多减少探索时间65%，路径距离减少42%，大幅提升探索效率且保证了地图完整度。消融实验显示协同机制可加速训练收敛。

Conclusion: 提出的方法不仅显著提升了主动SLAM的效率和泛化能力，还通过实际机器人实验验证了从仿真到现实环境的良好迁移性和实用性。

Abstract: Existing Active SLAM methodologies face issues such as slow exploration speed
and suboptimal paths. To address these limitations, we propose a hybrid
framework combining a Path-Uncertainty Co-Optimization Deep Reinforcement
Learning framework and a Lightweight Stagnation Detection mechanism. The
Path-Uncertainty Co-Optimization framework jointly optimizes travel distance
and map uncertainty through a dual-objective reward function, balancing
exploration and exploitation. The Lightweight Stagnation Detection reduces
redundant exploration through Lidar Static Anomaly Detection and Map Update
Stagnation Detection, terminating episodes on low expansion rates. Experimental
results show that compared with the frontier-based method and RRT method, our
approach shortens exploration time by up to 65% and reduces path distance by up
to 42%, significantly improving exploration efficiency in complex environments
while maintaining reliable map completeness. Ablation studies confirm that the
collaborative mechanism accelerates training convergence. Empirical validation
on a physical robotic platform demonstrates the algorithm's practical
applicability and its successful transferability from simulation to real-world
environments.

</details>


### [116] [GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments](https://arxiv.org/abs/2511.04199)
*Shenglin Wang,Mingtong Dai,Jingxuan Su,Lingbo Liu,Chunjie Chen,Xinyu Wu,Liang Lin*

Main category: cs.RO

TL;DR: GraspView是一种仅用RGB摄像头的机器人抓取系统，在复杂杂乱环境下，无需深度信息即可实现准确且鲁棒的抓取，尤其在遮挡严重和透明物体场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统机器人抓取依赖RGB-D相机，但其在透明、反光物体上表现差且近距离效果不佳，而实际场景中经常存在遮挡和3D重建不稳定等问题。因此，亟需一种能够在复杂环境、不同物体类型下稳定工作的抓取方法，且不依赖深度传感器。

Method: 提出GraspView框架，包括三大核心模块：（1）全局的场景重建模块，从单视角RGB恢复局部一致的尺度可控的3D几何，并多视角融合为全局三维场景；（2）主动感知模块，通过渲染和评分动态选择观察角度，主动揭示被遮挡区域；（3）在线度量对齐模块，实现视觉预测和机器人运动学的物理尺度一致性。最终与GraspNet结合，实现稳健抓取。

Result: 在不同台面物体实验中，GraspView在遮挡严重、近距离感知和抓取透明物体等场景下，显著优于传统RGB-D方案和单视角RGB方法，显示出更高的抓取成功率。

Conclusion: GraspView是RGB-D方案的有效替代品，拓展了机器人在无结构真实环境中可靠抓取的能力，尤其适用于难以获取可靠深度信息的情况。

Abstract: Robotic grasping is a fundamental capability for autonomous manipulation, yet
remains highly challenging in cluttered environments where occlusion, poor
perception quality, and inconsistent 3D reconstructions often lead to unstable
or failed grasps. Conventional pipelines have widely relied on RGB-D cameras to
provide geometric information, which fail on transparent or glossy objects and
degrade at close range. We present GraspView, an RGB-only robotic grasping
pipeline that achieves accurate manipulation in cluttered environments without
depth sensors. Our framework integrates three key components: (i) global
perception scene reconstruction, which provides locally consistent, up-to-scale
geometry from a single RGB view and fuses multi-view projections into a
coherent global 3D scene; (ii) a render-and-score active perception strategy,
which dynamically selects next-best-views to reveal occluded regions; and (iii)
an online metric alignment module that calibrates VGGT predictions against
robot kinematics to ensure physical scale consistency. Building on these
tailor-designed modules, GraspView performs best-view global grasping, fusing
multi-view reconstructions and leveraging GraspNet for robust execution.
Experiments on diverse tabletop objects demonstrate that GraspView
significantly outperforms both RGB-D and single-view RGB baselines, especially
under heavy occlusion, near-field sensing, and with transparent objects. These
results highlight GraspView as a practical and versatile alternative to RGB-D
pipelines, enabling reliable grasping in unstructured real-world environments.

</details>


### [117] [Can Context Bridge the Reality Gap? Sim-to-Real Transfer of Context-Aware Policies](https://arxiv.org/abs/2511.04249)
*Marco Iannotta,Yuxuan Yang,Johannes A. Stork,Erik Schaffernicht,Todor Stoyanov*

Main category: cs.RO

TL;DR: 本论文针对强化学习在机器人领域中的仿真到现实（sim-to-real）迁移问题，提出通过为策略引入动力学参数上下文估计模块以提升迁移性能，并在仿真和现实任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 机器人强化学习的仿真到现实迁移往往因环境动力学的差异导致仿真中训练出来的策略难以在现实世界应用。虽然领域随机化（Domain Randomization，DR）通过多样化训练环境有助于泛化，但会牺牲性能。作者希望通过结合上下文信息（动力学参数推断），提升策略对环境变化的鲁棒性。

Method: 本文在基于领域随机化的强化学习框架中，加入了一个上下文估计模块，用于推断并向策略提供当前环境的动力学参数信息。系统性地对比了多种主流的监督策略，以探索哪种方式最能提升策略性能，并分别在标准控制基准测试和实际机器人推箱任务（Franka Emika Panda机器人）上进行了测评。

Result: 实验结果显示，显式利用上下文信息（即动力学参数）的策略，在所有设置中均优于不利用上下文的基线方法。不过，不同任务下最优的监督策略有所差异。

Conclusion: 为策略提供动力学参数上下文能显著提升仿真到现实的迁移性能。未来可根据具体任务选择最佳的上下文监督策略，进一步推动机器人强化学习实际应用。

Abstract: Sim-to-real transfer remains a major challenge in reinforcement learning (RL)
for robotics, as policies trained in simulation often fail to generalize to the
real world due to discrepancies in environment dynamics. Domain Randomization
(DR) mitigates this issue by exposing the policy to a wide range of randomized
dynamics during training, yet leading to a reduction in performance. While
standard approaches typically train policies agnostic to these variations, we
investigate whether sim-to-real transfer can be improved by conditioning the
policy on an estimate of the dynamics parameters -- referred to as context. To
this end, we integrate a context estimation module into a DR-based RL framework
and systematically compare SOTA supervision strategies. We evaluate the
resulting context-aware policies in both a canonical control benchmark and a
real-world pushing task using a Franka Emika Panda robot. Results show that
context-aware policies outperform the context-agnostic baseline across all
settings, although the best supervision strategy depends on the task.

</details>


### [118] [Design and Control of a Coaxial Dual-rotor Reconfigurable Tailsitter UAV Based on Swashplateless Mechanism](https://arxiv.org/abs/2511.04251)
*Jinfeng Liang,Haocheng Guo,Ximin Lyu*

Main category: cs.RO

TL;DR: 本文提出了一种新型可重构机翼结构的尾座式垂直起降（VTOL）无人机，有效提升了多旋翼模式下的抗风干扰能力，降低了能耗并简化了结构，实验证明其在全飞行包线内均表现出良好的飞行稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统尾座式VTOL无人机由于去除了倾转的执行器和机构而重量轻，但在多旋翼模式下因机体迎风面积大，易受风干扰影响，导致飞行稳定性差。本文为了解决多旋翼模式下抗风能力弱和能耗高等问题进行研究。

Method: 1. 设计了一种可重构机翼，能在多旋翼模式下收起机翼，固定翼模式下展开。2. 采用共轴异构双旋翼配置，注重提升动力效率，降低总能耗。3. 设计无滚珠盘机构，并通过增加摆振铰链优化结构，以减轻重量与复杂度，同时降低加速减速过程中的振动。4. 进行了包含过渡飞行在内的全飞行包线试验验证性能。

Result: 实验结果证明，该尾座式无人机具备优秀的抗风性能和飞行稳定性，在固定翼和多旋翼模式之间的过渡也十分平稳，有效降低了能耗和结构振动。

Conclusion: 实现了具备创新结构和动力配置的尾座式VTOL无人机，显著提升了飞行的稳定性、抗扰能力和能效，且结构简化，验证了其在实际飞行任务中的应用前景。

Abstract: The tailsitter vertical takeoff and landing (VTOL) UAV is widely used due to
its lower dead weight, which eliminates the actuators and mechanisms for
tilting. However, the tailsitter UAV is susceptible to wind disturbances in
multi-rotor mode, as it exposes a large frontal fuselage area. To address this
issue, our tailsitter UAV features a reconfigurable wing design, allowing wings
to retract in multi-rotor mode and extend in fixed- wing mode. Considering
power efficiency, we design a coaxial heterogeneous dual-rotor configuration,
which significantly re- duces the total power consumption. To reduce structural
weight and simplify structural complexity, we employ a swashplateless mechanism
with an improved design to control pitch and roll in multi-rotor mode. We
optimize the structure of the swashplateless mechanism by adding flapping
hinges, which reduces vibration during cyclic acceleration and deceleration.
Finally, we perform comprehensive transition flight tests to validate stable
flight performance across the entire flight envelope of the tailsitter UAV.

</details>


### [119] [MacroNav: Multi-Task Context Representation Learning Enables Efficient Navigation in Unknown Environments](https://arxiv.org/abs/2511.04320)
*Kuankuan Sima,Longbin Tang,Haozhe Ma,Lin Zhao*

Main category: cs.RO

TL;DR: MacroNav是一种集成轻量级上下文编码器和基于图的强化学习策略的新型自主导航框架，在识别率和效率上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中实现自主导航，需要系统在部分可观测情况下高效又准确地理解空间环境。现有方法往往难以兼顾丰富上下文表达和导航高效性。因此，作者希望提出能更好平衡空间建模和决策效率的方法。

Method: MacroNav有两个核心组件：第一，一个通过多任务自监督学习训练的轻量级上下文编码器，用于捕捉多尺度、以导航为中心的空间表征；第二，一个强化学习策略，将这些空间表征与基于图的推理有效结合，实现高效动作选择。

Result: 大量实验表明，该上下文编码器对环境理解高效且鲁棒。在真实部署中，MacroNav在任务成功率（SR）与路径长度加权成功率（SPL）上相比最新方法有显著提升，并保持了低计算开销。

Conclusion: MacroNav不仅提高了自主导航的准确性和效率，同时也具备较强的实际应用潜力。代码将在论文接收后开放，便于社区验证和发展。

Abstract: Autonomous navigation in unknown environments requires compact yet expressive
spatial understanding under partial observability to support high-level
decision making. Existing approaches struggle to balance rich contextual
representation with navigation efficiency. We present MacroNav, a
learning-based navigation framework featuring two key components: (1) a
lightweight context encoder trained via multi-task self-supervised learning to
capture multi-scale, navigation-centric spatial representations; and (2) a
reinforcement learning policy that seamlessly integrates these representations
with graph-based reasoning for efficient action selection. Extensive
experiments demonstrate the context encoder's efficient and robust
environmental understanding. Real-world deployments further validate MacroNav's
effectiveness, yielding significant gains over state-of-the-art navigation
methods in both Success Rate (SR) and Success weighted by Path Length (SPL),
while maintaining low computational cost. Code will be released upon
acceptance.

</details>


### [120] [GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies](https://arxiv.org/abs/2511.04357)
*Maëlic Neau,Zoe Falomir,Paulo E. Santos,Anne-Gwenn Bosser,Cédric Buche*

Main category: cs.RO

TL;DR: 提出了一种新型神经符号方法GraSP-VLA，将连续场景图表示与视觉-语言-动作模型结合，提高机器人从演示中自动学习新技能的能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉-语言-动作的端到端模仿学习方法缺乏高层次符号规划，难以胜任长时序任务；而符号方法的泛化和可扩展性较差，需要一种结合两者优点的新方法。

Method: 提出了GraSP-VLA框架，通过连续场景图（Continuous Scene Graph）从人类演示中自动生成符号表示，用于任务推理阶段的规划域生成，并调度底层视觉-语言-动作策略，提升连贯复制多个动作的能力。

Result: GraSP-VLA能高效在观测中学习到任务的符号表示，实现自动规划域生成；真实机器人实验验证该方法可有效调度低层VLA策略完成长时序任务。

Conclusion: GraSP-VLA在从观察生成自动规划域和长时序任务策略调度方面表现优越，证明了神经符号表示结合VLA模型的潜力。

Abstract: Deploying autonomous robots that can learn new skills from demonstrations is
an important challenge of modern robotics. Existing solutions often apply
end-to-end imitation learning with Vision-Language Action (VLA) models or
symbolic approaches with Action Model Learning (AML). On the one hand, current
VLA models are limited by the lack of high-level symbolic planning, which
hinders their abilities in long-horizon tasks. On the other hand, symbolic
approaches in AML lack generalization and scalability perspectives. In this
paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that
uses a Continuous Scene Graph representation to generate a symbolic
representation of human demonstrations. This representation is used to generate
new planning domains during inference and serves as an orchestrator for
low-level VLA policies, scaling up the number of actions that can be reproduced
in a row. Our results show that GraSP-VLA is effective for modeling symbolic
representations on the task of automatic planning domain generation from
observations. In addition, results on real-world experiments show the potential
of our Continuous Scene Graph representation to orchestrate low-level VLA
policies in long-horizon tasks.

</details>


### [121] [Studying the Effect of Explicit Interaction Representations on Learning Scene-level Distributions of Human Trajectories](https://arxiv.org/abs/2511.04375)
*Anna Mészáros,Javier Alonso-Mora,Jens Kober*

Main category: cs.RO

TL;DR: 本文探讨了在自动驾驶场景中，高效建模多智能体联合分布（尤其是智能体间交互关系）的方法。结果显示，直接在网络中通过数据学习智能体交互往往适得其反，而显式定义交互关系可以提升预测效果。


<details>
  <summary>Details</summary>
Motivation: 多智能体场景中，准确预测场景演化对于自动驾驶至关重要。但目前关于智能体交互关系的建模方法存在争议：到底应依靠神经网络从数据隐式学习，还是结合空间时间关系进行显式建模？本文旨在系统分析不同交互建模方式对联合分布学习的影响。

Method: 作者在同样的网络结构下，设计并对比了多种描述智能体交互的方法：一类是让网络自发通过数据建立交互关联，另一类是在输入中显式定义交互关系（如谁先通过路口）。实验对比了这些策略对联合分布学习性能的影响。

Result: 实验证明，让神经网络自发通过数据学习交互关系，在大多数情境下会降低性能。恰当地、明确地定义智能体之间的交互关系，则能显著提升联合分布建模的效果。

Conclusion: 对多智能体交互进行清晰、合理的显式建模，优于仅依赖网络隐式学习。这一点对于设计准确、高效的自动驾驶决策系统具有重要意义。

Abstract: Effectively capturing the joint distribution of all agents in a scene is
relevant for predicting the true evolution of the scene and in turn providing
more accurate information to the decision processes of autonomous vehicles.
While new models have been developed for this purpose in recent years, it
remains unclear how to best represent the joint distributions particularly from
the perspective of the interactions between agents. Thus far there is no clear
consensus on how best to represent interactions between agents; whether they
should be learned implicitly from data by neural networks, or explicitly
modeled using the spatial and temporal relations that are more grounded in
human decision-making. This paper aims to study various means of describing
interactions within the same network structure and their effect on the final
learned joint distributions. Our findings show that more often than not, simply
allowing a network to establish interactive connections between agents based on
data has a detrimental effect on performance. Instead, having well defined
interactions (such as which agent of an agent pair passes first at an
intersection) can often bring about a clear boost in performance.

</details>


### [122] [ForeRobo: Unlocking Infinite Simulation Data for 3D Goal-driven Robotic Manipulation](https://arxiv.org/abs/2511.04381)
*Dexin wang,Faliang Chang,Chunsheng Liu*

Main category: cs.RO

TL;DR: 提出了一种名为ForeRobo的生成式机器人智能体，自主通过生成模拟和目标愿景，实现高效泛化的机械手操控技能学习，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 如何高效利用仿真数据，自动化获得复杂、泛化性强的机械臂操控技能，这是机器人研究中的关键难题；现有端到端方法在泛化性和解释性上均有限。

Method: 该方法提出ForeRobo智能体，采用“提出-生成-学习-执行”循环。首先提出待习得技能并搭建对应仿真环境；利用ForeGen模块生成与技能一致的目标状态和海量数据；基于这些数据训练ForeFormer状态生成模型，用于预测场景中物体的3D目标状态点。最后，借助传统控制算法，在现实环境中根据预测目标状态驱动机器人执行任务。

Result: ForeFormer在刚体和关节物体多种操控任务上，较最新方法平均精度提升达56.32%，泛化能力更强。在20余项真实机器人任务中实现zero-shot仿真到真实转移，平均成功率达79.28%。

Conclusion: ForeRobo通过将生成式模拟与经典控制结合，实现了高效、泛化的机械臂操控技能获自学习，优于端到端方案，并具备良好的解释性与执行效率。

Abstract: Efficiently leveraging simulation to acquire advanced manipulation skills is
both challenging and highly significant. We introduce \textit{ForeRobo}, a
generative robotic agent that utilizes generative simulations to autonomously
acquire manipulation skills driven by envisioned goal states. Instead of
directly learning low-level policies, we advocate integrating generative
paradigms with classical control. Our approach equips a robotic agent with a
self-guided \textit{propose-generate-learn-actuate} cycle. The agent first
proposes the skills to be acquired and constructs the corresponding simulation
environments; it then configures objects into appropriate arrangements to
generate skill-consistent goal states (\textit{ForeGen}). Subsequently, the
virtually infinite data produced by ForeGen are used to train the proposed
state generation model (\textit{ForeFormer}), which establishes point-wise
correspondences by predicting the 3D goal position of every point in the
current state, based on the scene state and task instructions. Finally,
classical control algorithms are employed to drive the robot in real-world
environments to execute actions based on the envisioned goal states. Compared
with end-to-end policy learning methods, ForeFormer offers superior
interpretability and execution efficiency. We train and benchmark ForeFormer
across a variety of rigid-body and articulated-object manipulation tasks, and
observe an average improvement of 56.32\% over the state-of-the-art state
generation models, demonstrating strong generality across different
manipulation patterns. Moreover, in real-world evaluations involving more than
20 robotic tasks, ForeRobo achieves zero-shot sim-to-real transfer and exhibits
remarkable generalization capabilities, attaining an average success rate of
79.28\%.

</details>


### [123] [Temporal Action Selection for Action Chunking](https://arxiv.org/abs/2511.04421)
*Yueyang Weng,Xiaopeng Zhang,Yongjin Mu,Yingcong Zhu,Yanjie Li,Qi Liu*

Main category: cs.RO

TL;DR: 本论文提出了一种新算法——时序动作选择器（TAS），用于在学习示范（LfD）中更好地权衡响应性、决策一致性和运动连贯性。TAS通过缓存和动态选择动作块，有效提升了机器人任务的成功率与训练效率。


<details>
  <summary>Details</summary>
Motivation: 在LfD中，动作分块能提升模型对复杂专家策略的表达能力，但由于决策频率降低，导致模型对环境变化和传感器噪声的适应性变差，现有方法难以兼顾响应性和决策一致性。

Method: 提出TAS算法，缓存多个时间步长的动作块预测结果，利用一个轻量级选择网络动态选取当前最优动作，实现三大能力的平衡优化。方法在多种基础策略下进行实验验证。

Result: TAS显著提升了多任务场景下的成功率，最大绝对提升达73.3%。结合残差RL还大幅提升了训练效率和最终性能。方法在仿真和实体机器人中均有效。

Conclusion: TAS有效解决了以往动作分块方法在响应性和一致性上的权衡难题，显著提升了学习示范的表现，并可进一步提升强化学习策略。

Abstract: Action chunking is a widely adopted approach in Learning from Demonstration
(LfD). By modeling multi-step action chunks rather than single-step actions,
action chunking significantly enhances modeling capabilities for human expert
policies. However, the reduced decision frequency restricts the utilization of
recent observations, degrading reactivity - particularly evident in the
inadequate adaptation to sensor noise and dynamic environmental changes.
Existing efforts to address this issue have primarily resorted to trading off
reactivity against decision consistency, without achieving both. To address
this limitation, we propose a novel algorithm, Temporal Action Selector (TAS),
which caches predicted action chunks from multiple timesteps and dynamically
selects the optimal action through a lightweight selector network. TAS achieves
balanced optimization across three critical dimensions: reactivity, decision
consistency, and motion coherence. Experiments across multiple tasks with
diverse base policies show that TAS significantly improves success rates -
yielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a
base policy with residual reinforcement learning (RL) substantially enhances
training efficiency and elevates the performance plateau. Experiments in both
simulation and physical robots confirm the method's efficacy.

</details>


### [124] [Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment](https://arxiv.org/abs/2511.04555)
*Tao Lin,Yilei Zhong,Yuxin Du,Jingjing Zhang,Jiting Liu,Yinxinyu Chen,Encheng Gu,Ziyan Liu,Hongyi Cai,Yanwen Zou,Lixing Zou,Zhaoye Zhou,Gen Li,Bo Zhao*

Main category: cs.RO

TL;DR: Evo-1是一款轻量级视觉-语言-行动（VLA）模型，无需大规模机器人数据预训练，性能强、计算与部署效率高，显著优于以往同类模型。


<details>
  <summary>Details</summary>
Motivation: 当前的VLA模型参数庞大，训练和推理计算成本高，且依赖大量预训练机器人数据，影响模型泛化与实际部署。传统训练方式还常导致视觉-语言感知能力下降，造成过拟合与下游任务效果不佳。

Method: Evo-1模型以原生多模态视觉-语言模型为基础，创新性地提出了跨调制扩散Transformer及优化集成模块。采用两阶段训练策略，逐步对齐动作与感知模块，有效保留视觉-语言表征。

Result: Evo-1仅有7.7亿参数，无需机器人数据预训练，在Meta-World和RoboTwin等套件上，分别超越现有最佳模型12.4%和6.9%；在LIBERO任务上获得94.8%的高分；真实机器人实验成功率达78%，推理速度快且占用内存小，全面优于所有基线方法。

Conclusion: Evo-1证明轻量、高效且无需机器人预训练的VLA模型可达到甚至超越大模型并显著提升实际部署可行性。代码、数据及权重已开源，有助于推动轻量级VLA模型领域研究。

Abstract: Vision-Language-Action (VLA) models have emerged as a powerful framework that
unifies perception, language, and control, enabling robots to perform diverse
tasks through multimodal understanding. However, current VLA models typically
contain massive parameters and rely heavily on large-scale robot data
pretraining, leading to high computational costs during training, as well as
limited deployability for real-time inference. Moreover, most training
paradigms often degrade the perceptual representations of the vision-language
backbone, resulting in overfitting and poor generalization to downstream tasks.
In this work, we present Evo-1, a lightweight VLA model that reduces
computation and improves deployment efficiency, while maintaining strong
performance without pretraining on robot data. Evo-1 builds on a native
multimodal Vision-Language model (VLM), incorporating a novel cross-modulated
diffusion transformer along with an optimized integration module, together
forming an effective architecture. We further introduce a two-stage training
paradigm that progressively aligns action with perception, preserving the
representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1
achieves state-of-the-art results on the Meta-World and RoboTwin suite,
surpassing the previous best models by 12.4% and 6.9%, respectively, and also
attains a competitive result of 94.8% on LIBERO. In real-world evaluations,
Evo-1 attains a 78% success rate with high inference frequency and low memory
overhead, outperforming all baseline methods. We release code, data, and model
weights to facilitate future research on lightweight and efficient VLA models.

</details>


### [125] [SAFe-Copilot: Unified Shared Autonomy Framework](https://arxiv.org/abs/2511.04664)
*Phat Nguyen,Erfan Aasi,Shiva Sreeram,Guy Rosman,Andrew Silva,Sertac Karaman,Daniela Rus*

Main category: cs.RO

TL;DR: 该论文提出了一种基于高层抽象的共享自动驾驶系统框架，利用视觉语言模型将人类意图和自动驾驶规划进行高阶融合，以提升自动驾驶系统在罕见与复杂场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 目前自动驾驶系统在处理罕见、模糊和分布外场景时表现不佳，缺乏如人类驾驶员一样的情境推理能力。传统共享自主方法多集中于低层面的轨迹调解，难以反映和保持人类的真实驾驶意图，因此有必要探索更高级别、更接近人类思维的融合方式。

Method: 作者提出了一个统一的共享自主框架，将视觉语言模型（VLM）应用于提取驾驶员行为和环境上下文等多模态信息，从而推测人类驾驶意图，并在高层语义层面实现人-机之间的策略协调。方法在模拟人类设置下和真实用户调查中分别进行了测试。

Result: 在模拟人类驾驶实验中，方法实现了完美召回率以及较高的准确率和精确度。实测用户调查显示，参与者在92%的案例中认同系统的裁决。基于Bench2Drive基准测试，该系统显著降低了碰撞率，总体表现优于纯自动驾驶。

Conclusion: 基于语义和语言层面的共享自主调解可视为共享自动驾驶系统的重要设计原则，使得系统具备常识推理能力，并更好地实现与人类意图的一致性和连续性。

Abstract: Autonomous driving systems remain brittle in rare, ambiguous, and
out-of-distribution scenarios, where human driver succeed through contextual
reasoning. Shared autonomy has emerged as a promising approach to mitigate such
failures by incorporating human input when autonomy is uncertain. However, most
existing methods restrict arbitration to low-level trajectories, which
represent only geometric paths and therefore fail to preserve the underlying
driving intent. We propose a unified shared autonomy framework that integrates
human input and autonomous planners at a higher level of abstraction. Our
method leverages Vision Language Models (VLMs) to infer driver intent from
multi-modal cues -- such as driver actions and environmental context -- and to
synthesize coherent strategies that mediate between human and autonomous
control. We first study the framework in a mock-human setting, where it
achieves perfect recall alongside high accuracy and precision. A human-subject
survey further shows strong alignment, with participants agreeing with
arbitration outcomes in 92% of cases. Finally, evaluation on the Bench2Drive
benchmark demonstrates a substantial reduction in collision rate and
improvement in overall performance compared to pure autonomy. Arbitration at
the level of semantic, language-based representations emerges as a design
principle for shared autonomy, enabling systems to exercise common-sense
reasoning and maintain continuity with human intent.

</details>


### [126] [Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions](https://arxiv.org/abs/2511.04665)
*Kaifeng Zhang,Shuo Sha,Hanxiao Jiang,Matthew Loper,Hyunjong Song,Guangyan Cai,Zhuo Xu,Xiaochen Hu,Changxi Zheng,Yunzhu Li*

Main category: cs.RO

TL;DR: 提出了一种将现实视频重建为软体物体数字孪生，并用高保真渲染实现机器人物理操作仿真，用于机器人软体物体操作策略的真实可靠的仿真评估。


<details>
  <summary>Details</summary>
Motivation: 机器人操作策略在真实世界中测试非常昂贵及低效，尤其是软体物体任务，现有仿真器难以还原现实中复杂的视觉和物理交互。

Method: 提出了real-to-sim评估框架：从真实视频自动生成软体物体的数字孪生，利用3D Gaussian Splatting进行高质量渲染，并用于机器人、物体及环境的复现和仿真评估。

Result: 实验验证了该方法在毛绒玩具包装、绳索布线、T型块推送等实际软体操作任务上，与真实执行有很强的结果相关性，并揭示了学习策略的重要行为模式。

Conclusion: 结合物理驱动的重建与高质量渲染，可以实现可复现、可扩展且高精度的机器人软体物体操作策略仿真评估。

Abstract: Robotic manipulation policies are advancing rapidly, but their direct
evaluation in the real world remains costly, time-consuming, and difficult to
reproduce, particularly for tasks involving deformable objects. Simulation
provides a scalable and systematic alternative, yet existing simulators often
fail to capture the coupled visual and physical complexity of soft-body
interactions. We present a real-to-sim policy evaluation framework that
constructs soft-body digital twins from real-world videos and renders robots,
objects, and environments with photorealistic fidelity using 3D Gaussian
Splatting. We validate our approach on representative deformable manipulation
tasks, including plush toy packing, rope routing, and T-block pushing,
demonstrating that simulated rollouts correlate strongly with real-world
execution performance and reveal key behavioral patterns of learned policies.
Our results suggest that combining physics-informed reconstruction with
high-quality rendering enables reproducible, scalable, and accurate evaluation
of robotic manipulation policies. Website: https://real2sim-eval.github.io/

</details>


### [127] [X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations](https://arxiv.org/abs/2511.04671)
*Maximus A. Pace,Prithwish Dan,Chuanruo Ning,Atiksh Bhardwaj,Audrey Du,Edward W. Duan,Wei-Chiu Ma,Kushal Kedia*

Main category: cs.RO

TL;DR: X-Diffusion是一种利用扩散模型有效结合人类演示和机器人学习的策略，成功缓解了执行动作差异导致的问题，在多项任务中明显提升了机器人操作表现。


<details>
  <summary>Details</summary>
Motivation: 人类演示数据丰富且易获取，为机器人学习提供了巨大潜力，但由于人体和机器人在动作执行上的本质差异（如动态和运动学约束），直接使用人类动作会导致机器人产生不可行的动作。因此，亟需一种有效融合人类演示和机器人执行能力的方法。

Method: 提出X-Diffusion框架，先训练一个分类器判别人类与机器人噪声动作的区别。当人类动作加入足够噪声后，直到分类器无法区分是人类还是机器人动作，才将该人类动作用于策略训练。这样，在高噪声下，人类演示提供任务指导，低噪声下则用机器人动作微调，避免学习到不可执行的动作。

Result: 实验表明，直接混合人类和机器人数据训练会降低性能，而X-Diffusion方法可持续提升表现。在五个机器人操作任务中，X-Diffusion的平均成功率比最优对比基线高16%。

Conclusion: X-Diffusion方法能最大化利用人类数据并避免机器人学习到不可行的动作，有效提升了实际机器人操作成功率。该方法为跨主体模仿学习提供了新思路。

Abstract: Human videos can be recorded quickly and at scale, making them an appealing
source of training data for robot learning. However, humans and robots differ
fundamentally in embodiment, resulting in mismatched action execution. Direct
kinematic retargeting of human hand motion can therefore produce actions that
are physically infeasible for robots. Despite these low-level differences,
human demonstrations provide valuable motion cues about how to manipulate and
interact with objects. Our key idea is to exploit the forward diffusion
process: as noise is added to actions, low-level execution differences fade
while high-level task guidance is preserved. We present X-Diffusion, a
principled framework for training diffusion policies that maximally leverages
human data without learning dynamically infeasible motions. X-Diffusion first
trains a classifier to predict whether a noisy action is executed by a human or
robot. Then, a human action is incorporated into policy training only after
adding sufficient noise such that the classifier cannot discern its embodiment.
Actions consistent with robot execution supervise fine-grained denoising at low
noise levels, while mismatched human actions provide only coarse guidance at
higher noise levels. Our experiments show that naive co-training under
execution mismatches degrades policy performance, while X-Diffusion
consistently improves it. Across five manipulation tasks, X-Diffusion achieves
a 16% higher average success rate than the best baseline. The project website
is available at https://portal-cornell.github.io/X-Diffusion/.

</details>


### [128] [GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction](https://arxiv.org/abs/2511.04679)
*Qingzhou Lu,Yao Feng,Baiyu Shi,Michael Piseno,Zhenan Bao,C. Karen Liu*

Main category: cs.RO

TL;DR: 该论文提出GentleHumanoid框架，将阻抗控制融入类人机器人全身运动追踪，实现上半身柔顺性，提高人机物理交互的安全性和自然性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数强化学习策略强调刚性跟踪，抑制外部力，导致类人机器人在与人类和环境交互中柔顺性和安全性不足。已有的阻抗增强方法局限于基础或末端执行器，且更侧重于抵抗极端外力，不能有效实现自然顺从的交互。

Method: 提出了一种新的基于弹簧的统一建模方法，将阻抗控制集成进全身运动追踪策略，用以描述涵盖肩膀、肘部和手腕的恢复性接触与引导性接触，并通过可调节的力阈值机制提升交互安全性。方法在仿真和Unitree G1机器人实机上，针对多种柔顺性需求的任务进行测试。

Result: 在包括温柔拥抱、辅助坐立、物体安全操作等多种任务下，GentleHumanoid策略相较于基线方法，在保持任务成功率的前提下，显著降低了接触力峰值，实现了更柔和自然的人机交互。

Conclusion: GentleHumanoid为类人机器人在实际场景下实现安全、自然、高效的人机协作和物体操作提供了可行的新途径，是推动类人机器人落地应用的重要一步。

Abstract: Humanoid robots are expected to operate in human-centered environments where
safe and natural physical interaction is essential. However, most recent
reinforcement learning (RL) policies emphasize rigid tracking and suppress
external forces. Existing impedance-augmented approaches are typically
restricted to base or end-effector control and focus on resisting extreme
forces rather than enabling compliance. We introduce GentleHumanoid, a
framework that integrates impedance control into a whole-body motion tracking
policy to achieve upper-body compliance. At its core is a unified spring-based
formulation that models both resistive contacts (restoring forces when pressing
against surfaces) and guiding contacts (pushes or pulls sampled from human
motion data). This formulation ensures kinematically consistent forces across
the shoulder, elbow, and wrist, while exposing the policy to diverse
interaction scenarios. Safety is further supported through task-adjustable
force thresholds. We evaluate our approach in both simulation and on the
Unitree G1 humanoid across tasks requiring different levels of compliance,
including gentle hugging, sit-to-stand assistance, and safe object
manipulation. Compared to baselines, our policy consistently reduces peak
contact forces while maintaining task success, resulting in smoother and more
natural interactions. These results highlight a step toward humanoid robots
that can safely and effectively collaborate with humans and handle objects in
real-world environments.

</details>
