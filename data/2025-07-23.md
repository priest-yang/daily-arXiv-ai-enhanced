<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 80]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Salience Adjustment for Context-Based Emotion Recognition](https://arxiv.org/abs/2507.15878)
*Bin Han,Jonathan Gratch*

Main category: cs.CV

TL;DR: 本文提出了一种结合贝叶斯线索整合（BCI）与视觉-语言模型（VLMs）的情感识别框架，通过对面部表情与情境信息的动态加权，提高了复杂社交场景下的情感识别性能。


<details>
  <summary>Details</summary>
Motivation: 在动态社交环境中，单独依赖面部表情无法精准识别情感，情境线索的综合作用亟需更有效的方法来整合。

Method: 提出基于贝叶斯线索整合与视觉-语言模型的显著性调整框架，根据面部表情的表达强度动态调整对面部和情境信息的权重。并以囚徒困境情境设计实验，结合人工标注与自动情感识别系统进行评估。

Result: 显著性调整后，情感识别性能得到提升，无论是人工还是自动系统均显示出更优的识别表现。

Conclusion: 本方法有效提升了社交场景下的情感识别，为将来在更广泛的多模态与社会情境中应用提供了新方向。

Abstract: Emotion recognition in dynamic social contexts requires an understanding of
the complex interaction between facial expressions and situational cues. This
paper presents a salience-adjusted framework for context-aware emotion
recognition with Bayesian Cue Integration (BCI) and Visual-Language Models
(VLMs) to dynamically weight facial and contextual information based on the
expressivity of facial cues. We evaluate this approach using human annotations
and automatic emotion recognition systems in prisoner's dilemma scenarios,
which are designed to evoke emotional reactions. Our findings demonstrate that
incorporating salience adjustment enhances emotion recognition performance,
offering promising directions for future research to extend this framework to
broader social contexts and multimodal applications.

</details>


### [2] [Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark](https://arxiv.org/abs/2507.15882)
*Goeric Huybrechts,Srikanth Ronanki,Sai Muralidhar Jayanthi,Jack Fitzgerald,Srinivasan Veeravanallur*

Main category: cs.CV

TL;DR: 本文提出了Document Haystack基准，专门用于评估多模态大模型（VLMs）在处理长且具有复杂视觉结构文档时的表现。该基准包含了大量不同长度、类型的文档，并通过'针'的插入测试VLM的检索和理解能力。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型在多种模态的数据分析上有很大进步，但在处理长文档方面缺乏系统性基准，这导致相关研究不足。

Method: 作者构建了包含5到200页的复杂文档基准，采用“needle-in-a-haystack”策略在文档不同深度插入文本或图文混合的信息，并设计了8,250个自动化评测问题，支持自动客观评测。

Result: 基准中包含400种文档变体和大量问题，通过该基准对主流VLM的表现进行了系统测试和分析。

Conclusion: Document Haystack解决了评估VLM处理长复杂文档能力的痛点，为未来该方向的模型改进和学术研究提供了有力工具，并讨论了相关未来研究方向。

Abstract: The proliferation of multimodal Large Language Models has significantly
advanced the ability to analyze and understand complex data inputs from
different modalities. However, the processing of long documents remains
under-explored, largely due to a lack of suitable benchmarks. To address this,
we introduce Document Haystack, a comprehensive benchmark designed to evaluate
the performance of Vision Language Models (VLMs) on long, visually complex
documents. Document Haystack features documents ranging from 5 to 200 pages and
strategically inserts pure text or multimodal text+image "needles" at various
depths within the documents to challenge VLMs' retrieval capabilities.
Comprising 400 document variants and a total of 8,250 questions, it is
supported by an objective, automated evaluation framework. We detail the
construction and characteristics of the Document Haystack dataset, present
results from prominent VLMs and discuss potential research avenues in this
area.

</details>


### [3] [PAT++: a cautionary tale about generative visual augmentation for Object Re-identification](https://arxiv.org/abs/2507.15888)
*Leonardo Santiago Benitez Pereira,Arathy Jeevan*

Main category: cs.CV

TL;DR: 本文提出了一种将生成式图像增强方法应用于目标重识别任务的pipeline，但实验结果显示生成增强不仅未提升性能，反而由于域间偏移和身份特征丢失带来性能下降。作者指出对于重识别等需保留精细特征的任务，现有生成式增强方法局限明显。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式数据增强在许多视觉任务中表现出色，但其在目标重识别（需保留精细身份特征）的实际效果尚未充分研究。作者希望评估生成式增强在这类应用中的可行性与挑战。

Method: 作者提出了名为PAT++的增强pipeline，将Diffusion Self-Distillation整合进传统Part-Aware Transformer架构。在Urban Elements ReID Challenge数据集上，他们运用生成图像进行训练和查询扩增，并做了系统实验对比。

Result: 实验显示，使用生成式增强后，无论用于训练还是查询，模型性能稳定下降。性能退化主要与生成图像的域偏移和身份关键特征未能完美保留下来有关。

Conclusion: 生成模型在需要细粒度判别的目标重识别任务中存在转移性和身份特征保留方面的局限性，强调了现有生成式增强方法在身份保持应用中的不足。

Abstract: Generative data augmentation has demonstrated gains in several vision tasks,
but its impact on object re-identification - where preserving fine-grained
visual details is essential - remains largely unexplored. In this work, we
assess the effectiveness of identity-preserving image generation for object
re-identification. Our novel pipeline, named PAT++, incorporates Diffusion
Self-Distillation into the well-established Part-Aware Transformer. Using the
Urban Elements ReID Challenge dataset, we conduct extensive experiments with
generated images used for both model training and query expansion. Our results
show consistent performance degradation, driven by domain shifts and failure to
retain identity-defining features. These findings challenge assumptions about
the transferability of generative models to fine-grained recognition tasks and
expose key limitations in current approaches to visual augmentation for
identity-preserving applications.

</details>


### [4] [Local Dense Logit Relations for Enhanced Knowledge Distillation](https://arxiv.org/abs/2507.15911)
*Liuchi Xu,Kang Liu,Jinshuai Liu,Lu Wang,Lisheng Xu,Jun Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种新的logit蒸馏方法LDRLD（Local Dense Relational Logit Distillation），采用递归解耦和重组logit信息，以获得更细粒度和清晰的类间关系，提升学生模型表现。引入ADW（Adaptive Decay Weight）策略，对关键类别对动态加权以进一步提升效果。实验在多数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的logit蒸馏方法虽然通用、简单且高效，但尚未深入探索logit知识中的细粒度类间关系，导致知识迁移不充分，限制了学生模型的进一步提升。

Method: 作者提出LDRLD方法，通过递归地将logit信息进行解耦和重组，捕捉更细致的类间关系。同时引入自适应衰减加权（ADW）策略，包括逆序权重（IRW）和指数衰减（ERD）为关键类别对动态分配权重。在解耦后，剩余的非目标类别知识也被充分蒸馏，以保证知识完整传递。

Result: 在CIFAR-100、ImageNet-1K和Tiny-ImageNet等数据集上进行的广泛实验显示，该方法在多数情况下显著优于现有logit蒸馏方法。

Conclusion: LDRLD可有效提取和迁移细粒度的logit关系知识，结合关键类别的动态加权策略，能够提升学生模型的性能。该方法具备推广性，未来可在更多蒸馏场景中应用。

Abstract: State-of-the-art logit distillation methods exhibit versatility, simplicity,
and efficiency. Despite the advances, existing studies have yet to delve
thoroughly into fine-grained relationships within logit knowledge. In this
paper, we propose Local Dense Relational Logit Distillation (LDRLD), a novel
method that captures inter-class relationships through recursively decoupling
and recombining logit information, thereby providing more detailed and clearer
insights for student learning. To further optimize the performance, we
introduce an Adaptive Decay Weight (ADW) strategy, which can dynamically adjust
the weights for critical category pairs using Inverse Rank Weighting (IRW) and
Exponential Rank Decay (ERD). Specifically, IRW assigns weights inversely
proportional to the rank differences between pairs, while ERD adaptively
controls weight decay based on total ranking scores of category pairs.
Furthermore, after the recursive decoupling, we distill the remaining
non-target knowledge to ensure knowledge completeness and enhance performance.
Ultimately, our method improves the student's performance by transferring
fine-grained knowledge and emphasizing the most critical relationships.
Extensive experiments on datasets such as CIFAR-100, ImageNet-1K, and
Tiny-ImageNet demonstrate that our method compares favorably with
state-of-the-art logit-based distillation approaches. The code will be made
publicly available.

</details>


### [5] [An empirical study for the early detection of Mpox from skin lesion images using pretrained CNN models leveraging XAI technique](https://arxiv.org/abs/2507.15915)
*Mohammad Asifur Rahim,Muhammad Nazmul Arefin,Md. Mizanur Rahman,Md Ali Hossain,Ahmed Moustafa*

Main category: cs.CV

TL;DR: 本文通过评估多种预训练CNN模型并结合可解释性AI技术，探讨了AI在猩痘早期检测中的有效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 猩痘病毒和其他皮肤病症状相似，早期诊断困难。AI在医学图像分析方面表现优异，但针对猩痘的AI方法及其可解释性研究较少，因此需探索预训练模型和解释方法在猩痘检测中的应用价值。

Method: 利用两个公开皮肤图像数据集（MSLD和MSLD v2.0），对VGG16、VGG19、InceptionV3、MobileNetV2等预训练CNN模型进行微调（冻结底层，添加自定义层），分别针对二分类和多分类任务进行训练和验证。通过Grad-CAM对模型决策进行可视化解释，并通过准确率、精确率、召回率、F1值和ROC评估模型性能和过拟合现象。

Result: InceptionV3在二分类中表现最佳，准确率达95%；MobileNetV2在多分类中表现最佳，准确率93%。Grad-CAM能有效突出关键特征区域。但部分模型存在过拟合问题，训练与验证损失不一致。

Conclusion: 预训练CNN模型在猩痘检测中具有显著潜力，XAI方法提升了模型可解释性。未来应扩充数据集、引入多模态信息及更多解释技术，以提升诊断可靠性和模型透明度。

Abstract: Context: Mpox is a zoonotic disease caused by the Mpox virus, which shares
similarities with other skin conditions, making accurate early diagnosis
challenging. Artificial intelligence (AI), especially Deep Learning (DL), has a
strong tool for medical image analysis; however, pre-trained models like CNNs
and XAI techniques for mpox detection is underexplored. Objective: This study
aims to evaluate the effectiveness of pre-trained CNN models (VGG16, VGG19,
InceptionV3, MobileNetV2) for the early detection of monkeypox using binary and
multi-class datasets. It also seeks to enhance model interpretability using
Grad-CAM an XAI technique. Method: Two datasets, MSLD and MSLD v2.0, were used
for training and validation. Transfer learning techniques were applied to
fine-tune pre-trained CNN models by freezing initial layers and adding custom
layers for adapting the final features for mpox detection task and avoid
overfitting. Models performance were evaluated using metrics such as accuracy,
precision, recall, F1-score and ROC. Grad-CAM was utilized for visualizing
critical features. Results: InceptionV3 demonstrated the best performance on
the binary dataset with an accuracy of 95%, while MobileNetV2 outperformed on
the multi-class dataset with an accuracy of 93%. Grad-CAM successfully
highlighted key image regions. Despite high accuracy, some models showed
overfitting tendencies, as videnced by discrepancies between training and
validation losses. Conclusion: This study underscores the potential of
pre-trained CNN models in monkeypox detection and the value of XAI techniques.
Future work should address dataset limitations, incorporate multimodal data,
and explore additional interpretability techniques to improve diagnostic
reliability and model transparency

</details>


### [6] [A Lightweight Face Quality Assessment Framework to Improve Face Verification Performance in Real-Time Screening Applications](https://arxiv.org/abs/2507.15961)
*Ahmed Aman Ibrahim,Hamad Mansour Alawar,Abdulnasser Abbas Zehi,Ahmed Mohammad Alkendi,Bilal Shafi Ashfaq Ahmed Mirza,Shan Ullah,Ismail Lujain Jaleel,Hassan Ugail*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量且有效的面部图像质量评估框架，能够在面部识别前自动筛除低质量图像，大幅提升识别准确性。


<details>
  <summary>Details</summary>
Motivation: 低质量的人脸图像（如模糊、光照差、遮挡、姿态变化等）会极大影响人脸识别系统的准确性和可靠性，特别是在监控、身份认证等实时应用中。因此，急需一种有效且高效的自动人脸图像质量评估方法来提升整个识别流程性能。

Method: 提出利用标准化人脸关键点特征，结合随机森林回归分类器，对输入的人脸图像进行质量评估，并在识别流程前进行预筛查。同时将评估模块集成到ArcFace人脸验证模型中，并在迪拜警方的真实CCTV数据集（600+人）上进行实验验证。

Result: 所提方法图像质量评估准确率达96.67%。集成到识别流程后，假拒率降低了99.7%，且余弦相似度得分明显提升。与现有技术相比，提出的框架在保持计算效率的同时有效缓解了低质量图像的影响。

Conclusion: 该框架能够有效筛除低质量人脸图像，提高识别系统在真实环境下的表现。从实验和实际应用角度看，其性能优于现有方法，尤其能针对分辨率和姿态变化等关键问题，在实际监控等场景中具备实际部署价值。

Abstract: Face image quality plays a critical role in determining the accuracy and
reliability of face verification systems, particularly in real-time screening
applications such as surveillance, identity verification, and access control.
Low-quality face images, often caused by factors such as motion blur, poor
lighting conditions, occlusions, and extreme pose variations, significantly
degrade the performance of face recognition models, leading to higher false
rejection and false acceptance rates. In this work, we propose a lightweight
yet effective framework for automatic face quality assessment, which aims to
pre-filter low-quality face images before they are passed to the verification
pipeline. Our approach utilises normalised facial landmarks in conjunction with
a Random Forest Regression classifier to assess image quality, achieving an
accuracy of 96.67\%. By integrating this quality assessment module into the
face verification process, we observe a substantial improvement in performance,
including a comfortable 99.7\% reduction in the false rejection rate and
enhanced cosine similarity scores when paired with the ArcFace face
verification model. To validate our approach, we have conducted experiments on
a real-world dataset collected comprising over 600 subjects captured from CCTV
footage in unconstrained environments within Dubai Police. Our results
demonstrate that the proposed framework effectively mitigates the impact of
poor-quality face images, outperforming existing face quality assessment
techniques while maintaining computational efficiency. Moreover, the framework
specifically addresses two critical challenges in real-time screening:
variations in face resolution and pose deviations, both of which are prevalent
in practical surveillance scenarios.

</details>


### [7] [FW-VTON: Flattening-and-Warping for Person-to-Person Virtual Try-on](https://arxiv.org/abs/2507.16010)
*Zheng Wang,Xianbing Sun,Shengyi Wu,Jiahui Zhan,Jianlou Si,Chi Zhang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种面向人对人的虚拟试衣新方法，不像传统方法依赖平铺服装图片，而是直接利用穿衣人的图片，将目标服装无缝贴合到目标人物上，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟试衣系统主要解决服装到人物的试穿问题，要求服装有单独、平铺的图片，但实际中仅有穿着服装的照片更为常见。因此，作者希望实现仅通过两张人物穿衣照片来互换服装，提升虚拟试衣的实用性和泛用性。

Method: 本文提出三阶段的FW-VTON方法：（1）从源图片中提取出平铺的服装图像；（2）对服装进行变形以对齐目标人物的姿态；（3）将处理后的服装无缝融合到目标人物身上。此外，作者还新建设了适用于人对人试衣任务的数据集。

Result: 实验证明，FW-VTON在定性（视觉效果）和定量（评估指标）两个方面均达到甚至超过了当下最优方法，同时在服装提取相关子任务中表现突出。

Conclusion: FW-VTON有效地解决了人对人虚拟试衣的难题，并开创性地提供了新方法和数据集，推动了虚拟试衣技术从服装-人物匹配向人物-人物服装迁移的新方向。

Abstract: Traditional virtual try-on methods primarily focus on the garment-to-person
try-on task, which requires flat garment representations. In contrast, this
paper introduces a novel approach to the person-to-person try-on task. Unlike
the garment-to-person try-on task, the person-to-person task only involves two
input images: one depicting the target person and the other showing the garment
worn by a different individual. The goal is to generate a realistic combination
of the target person with the desired garment. To this end, we propose
Flattening-and-Warping Virtual Try-On (\textbf{FW-VTON}), a method that
operates in three stages: (1) extracting the flattened garment image from the
source image; (2) warping the garment to align with the target pose; and (3)
integrating the warped garment seamlessly onto the target person. To overcome
the challenges posed by the lack of high-quality datasets for this task, we
introduce a new dataset specifically designed for person-to-person try-on
scenarios. Experimental evaluations demonstrate that FW-VTON achieves
state-of-the-art performance, with superior results in both qualitative and
quantitative assessments, and also excels in garment extraction subtasks.

</details>


### [8] [Is Tracking really more challenging in First Person Egocentric Vision?](https://arxiv.org/abs/2507.16015)
*Matteo Dunnhofer,Zaira Manigrasso,Christian Micheloni*

Main category: cs.CV

TL;DR: 本文提出了一项新的基准研究，旨在区分视觉对象追踪与分割任务中的第一人称视角挑战与人-物体活动领域的挑战。通过新的评估策略，更准确地分析了困难的根源。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现，第一人称视角的视觉任务（egocentric vision）相较于第三人称存在更大挑战，但这些结论往往基于不同场景的对比，未严格分离“第一人称视角”与“人-物体活动”领域带来的困难。本文希望更清晰地识别出这两类因素各自对性能下降的影响。

Method: 作者构建了新的基准与评估策略，专门设计实验来区分第一人称视角的独特难点与人-物体互动场景带来的通用困难，从而实现两者影响的解耦分析。

Result: 通过新基准和方法，实验能够更清楚地识别出哪些难点源自第一人称视角，哪些属于人-物体活动本身，为研究提供了更细致的认识。

Conclusion: 本文的方法为今后视觉追踪与分割任务中有针对性地解决不同困难提供了依据，促进了对第一人称视觉任务实质挑战的深入理解，也为技术进步指明了方向。

Abstract: Visual object tracking and segmentation are becoming fundamental tasks for
understanding human activities in egocentric vision. Recent research has
benchmarked state-of-the-art methods and concluded that first person egocentric
vision presents challenges compared to previously studied domains. However,
these claims are based on evaluations conducted across significantly different
scenarios. Many of the challenging characteristics attributed to egocentric
vision are also present in third person videos of human-object activities. This
raises a critical question: how much of the observed performance drop stems
from the unique first person viewpoint inherent to egocentric vision versus the
domain of human-object activities? To address this question, we introduce a new
benchmark study designed to disentangle such factors. Our evaluation strategy
enables a more precise separation of challenges related to the first person
perspective from those linked to the broader domain of human-object activity
understanding. By doing so, we provide deeper insights into the true sources of
difficulty in egocentric tracking and segmentation, facilitating more targeted
advancements on this task.

</details>


### [9] [Artifacts and Attention Sinks: Structured Approximations for Efficient Vision Transformers](https://arxiv.org/abs/2507.16018)
*Andrew Lu,Wentinn Liao,Liuhui Wang,Huzheng Yang,Jianbo Shi*

Main category: cs.CV

TL;DR: 本文提出了对视觉Transformer中的高激活token（即massive tokens）及其产生的artifact tokens的新理解，并基于这些现象提出了一种高效的注意力近似方法（FNA）和降噪掩码策略，能在保持性能的同时显著降低计算量。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉Transformer在多种应用中表现出色，但其内部机制如注意力分布和token交互模式仍未被充分理解，尤其是高激活token在网络中的作用。深入分析这些现象有助于提升模型高效性并优化推理过程。

Method: 1. 分析并揭示vision transformer中massive tokens和artifact tokens的交互及其对信息流的影响。2. 据此提出Fast Nyström Attention（FNA）算法，该方法无需训练，通过利用数据中的结构性token模式以线性时间和空间逼近自注意力。3. 设计特定的token masking策略以降噪并提升模型稳健性。

Result: 在多种主流预训练视觉主干（backbone）上进行实验，FNA方法在图像检索、分类、分割和视觉问答等任务上取得了与原自注意力机制相当的表现，并在计算资源消耗上大幅减少。masking策略带来了轻微但几乎无代价的性能提升。

Conclusion: 理解和利用massive及artifact token现象可提升视觉Transformer推理效率。FNA算法为不用训练的自注意力高效逼近方法，适配广泛视觉任务，在保持准确性的同时，大幅降低了计算开销。

Abstract: Vision transformers have emerged as a powerful tool across a wide range of
applications, yet their inner workings remain only partially understood. In
this work, we examine the phenomenon of massive tokens - tokens with
exceptionally high activation norms that act as attention sinks - and artifact
tokens that emerge as a byproduct during inference. Our analysis reveals that
these tokens mutually suppress one another through the attention mechanism,
playing a critical role in regulating information flow within the network.
Leveraging these insights, we introduce Fast Nystr\"om Attention (FNA), a
training-free method that approximates self-attention in linear time and space
by exploiting the structured patterns formed by massive and artifact tokens.
Additionally, we propose a masking strategy to mitigate noise from these
tokens, yielding modest performance gains at virtually no cost. We evaluate our
approach on popular pretrained vision backbones and demonstrate competitive
performance on retrieval, classification, segmentation, and visual question
answering (VQA), all while reducing computational overhead.

</details>


### [10] [Discovering and using Spelke segments](https://arxiv.org/abs/2507.16038)
*Rahul Venkatesh,Klemen Kotar,Lilian Naing Chen,Seungwoo Kim,Luca Thomas Wheeler,Jared Watrous,Ashley Xu,Gia Ancone,Wanhee Lee,Honglin Chen,Daniel Bear,Stefan Stojanov,Daniel Yamins*

Main category: cs.CV

TL;DR: 本文提出了一种基于“Spelke对象”心理学理论的图像分割方法，用于更好地支持物理操控等任务，并证明其在新数据集和任务基准上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉分割方法依赖于类别语义，但这与人类按物理属性感知世界的方式不同。作者希望通过借鉴心理学中的Spelke对象理论，实现更具物理因果性的分割，以更好地支持操控和规划任务。

Method: 1）提出SpelkeBench数据集，包含自然图像中明确定义的Spelke对象标签；2）构建SpelkeNet视觉模型，通过预测未来物体运动分布发现Spelke对象；3）引入运动可行性图与期望位移图，并结合“统计反事实探查”方法，通过模拟‘虚拟戳探’聚类获得分割。

Result: SpelkeNet在SpelkeBench数据集上的分割性能优于如SegmentAnything等有监督主流方法，在物体操控基准3DEditBench中，作为分割模块也提升了多种操控模型的成绩。

Conclusion: 基于Spelke对象的分割方法更符合物理因果关系，对下游物理操控与规划任务有实际优势，展现了心理学认知理论在计算机视觉和机器人领域的应用潜力。

Abstract: Segments in computer vision are often defined by semantic considerations and
are highly dependent on category-specific conventions. In contrast,
developmental psychology suggests that humans perceive the world in terms of
Spelke objects--groupings of physical things that reliably move together when
acted on by physical forces. Spelke objects thus operate on category-agnostic
causal motion relationships which potentially better support tasks like
manipulation and planning. In this paper, we first benchmark the Spelke object
concept, introducing the SpelkeBench dataset that contains a wide variety of
well-defined Spelke segments in natural images. Next, to extract Spelke
segments from images algorithmically, we build SpelkeNet, a class of visual
world models trained to predict distributions over future motions. SpelkeNet
supports estimation of two key concepts for Spelke object discovery: (1) the
motion affordance map, identifying regions likely to move under a poke, and (2)
the expected-displacement map, capturing how the rest of the scene will move.
These concepts are used for "statistical counterfactual probing", where diverse
"virtual pokes" are applied on regions of high motion-affordance, and the
resultant expected displacement maps are used define Spelke segments as
statistical aggregates of correlated motion statistics. We find that SpelkeNet
outperforms supervised baselines like SegmentAnything (SAM) on SpelkeBench.
Finally, we show that the Spelke concept is practically useful for downstream
applications, yielding superior performance on the 3DEditBench benchmark for
physical object manipulation when used in a variety of off-the-shelf object
manipulation models.

</details>


### [11] [Disrupting Semantic and Abstract Features for Better Adversarial Transferability](https://arxiv.org/abs/2507.16052)
*Yuyang Luo,Xiaosen Wang,Zhijin Ge,Yingzhe He*

Main category: cs.CV

TL;DR: 本论文提出了一种新的特征级对抗攻击方法SAFER，通过同时干扰语义和高频（抽象）特征，有效提升了对抗样本的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 目前基于特征级的对抗样本攻击大多依赖语义信息，但研究发现CNN对高频(抽象)特征也很敏感，因此利用高频特征有望提升攻击迁移性。

Method: 作者提出SAFER方法，在特征重要性权重计算中，分别在图像空间进行BLOCKMIX变换，在频率空间进行SELF-MIX变换，从而引导攻击同时干扰语义与抽象特征。

Result: 在ImageNet数据集上的大量实验显示，该方法能显著提升对抗样本的迁移能力，相较于现有方法效果更佳。

Conclusion: 综合语义和抽象特征的重要性能有效增强对抗攻击的迁移性，所提SAFER方法在实际应用中表现优异，能够更好地威胁基于DNN的系统。

Abstract: Adversarial examples pose significant threats to deep neural networks (DNNs),
and their property of transferability in the black-box setting has led to the
emergence of transfer-based attacks, making it feasible to target real-world
applications employing DNNs. Among them, feature-level attacks, where
intermediate features are perturbed based on feature importance weight matrix
computed from transformed images, have gained popularity. In this work, we find
that existing feature-level attacks primarily manipulate the semantic
information to derive the weight matrix. Inspired by several works that find
CNNs tend to focus more on high-frequency components (a.k.a. abstract features,
e.g., texture, edge, etc.), we validate that transforming images in the
high-frequency space also improves transferability. Based on this finding, we
propose a balanced approach called Semantic and Abstract FEatures disRuption
(SAFER). Specifically, SAFER conducts BLOCKMIX on the input image and SELF-MIX
on the frequency spectrum when computing the weight matrix to highlight crucial
features. By using such a weight matrix, we can direct the attacker to disrupt
both semantic and abstract features, leading to improved transferability.
Extensive experiments on the ImageNet dataset also demonstrate the
effectiveness of our method in boosting adversarial transferability.

</details>


### [12] [Improving Personalized Image Generation through Social Context Feedback](https://arxiv.org/abs/2507.16095)
*Parul Gupta,Abhinav Dhall,Thanh-Toan Do*

Main category: cs.CV

TL;DR: 本文提出了一种基于反馈微调的个性化图像生成方法，通过引入姿态、人体-物体交互、人脸识别和视线估计等反馈，显著提升了复杂动作生成、身份保留和视线一致性，在多个数据集上验证了效果。


<details>
  <summary>Details</summary>
Motivation: 现有个性化图像生成方法在处理复杂动作、保持人物身份和生成自然视线时效果不佳，影响实际应用体验。

Method: 利用最先进的姿态检测、人体-物体交互检测、人脸识别和视线估计作为反馈信号，逐步针对不同层级（低层如姿态，高层如视线）的特征对扩散模型进行微调，提升生成效果。

Result: 经本方法微调后的模型，在三大基准数据集上的互动动作生成、面部身份一致性和整体图像质量方面都取得了明显提升。

Conclusion: 通过多维反馈引导微调扩散模型，解决了个性化图像生成中的关键难题，为更加真实一致的定制化生成奠定基础。

Abstract: Personalized image generation, where reference images of one or more subjects
are used to generate their image according to a scene description, has gathered
significant interest in the community. However, such generated images suffer
from three major limitations -- complex activities, such as $<$man, pushing,
motorcycle$>$ are not generated properly with incorrect human poses, reference
human identities are not preserved, and generated human gaze patterns are
unnatural/inconsistent with the scene description. In this work, we propose to
overcome these shortcomings through feedback-based fine-tuning of existing
personalized generation methods, wherein, state-of-art detectors of pose,
human-object-interaction, human facial recognition and human gaze-point
estimation are used to refine the diffusion model. We also propose
timestep-based inculcation of different feedback modules, depending upon
whether the signal is low-level (such as human pose), or high-level (such as
gaze point). The images generated in this manner show an improvement in the
generated interactions, facial identities and image quality over three
benchmark datasets.

</details>


### [13] [Stop-band Energy Constraint for Orthogonal Tunable Wavelet Units in Convolutional Neural Networks for Computer Vision problems](https://arxiv.org/abs/2507.16114)
*An D. Le,Hung Nguyen,Sungbal Seo,You-Suk Bae,Truong Q. Nguyen*

Main category: cs.CV

TL;DR: 本论文提出了一种停带能量约束，用于正交可调小波单元的滤波器，提升卷积神经网络（CNN）在纹理丰富数据集上的图像分类和异常检测性能。将该方法与ResNet-18和ResNet-34结合，在多个数据集上实现了明显的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 当前的CNN在处理包含复杂纹理信息的数据时，传统的卷积和下采样操作存在局限性，难以充分利用纹理特征。因此，作者希望改进滤波结构，从而更好地提升模型在分类和异常检测任务上的表现。

Method: 作者在正交可调小波单元（OTWU）滤波器中引入了停带能量约束，具体借助晶格结构优化滤波效果。该方法嵌入到ResNet-18架构，以增强卷积、池化和下采样操作能力，并在多个数据集（如CIFAR-10和Describable Textures）上进行实验。

Result: 该方法在CIFAR-10上准确率提升2.48%，在Describable Textures数据集上提升13.56%。相似改进同样出现在ResNet-34上。在MVTec hazelnut异常检测任务中，本方法在分割与检测两个指标上均优于现有方案。

Conclusion: 引入停带能量约束的小波滤波器能有效提升CNN模型在纹理类数据集上的分类与异常检测性能，具有广泛的实际应用价值。

Abstract: This work introduces a stop-band energy constraint for filters in orthogonal
tunable wavelet units with a lattice structure, aimed at improving image
classification and anomaly detection in CNNs, especially on texture-rich
datasets. Integrated into ResNet-18, the method enhances convolution, pooling,
and downsampling operations, yielding accuracy gains of 2.48% on CIFAR-10 and
13.56% on the Describable Textures dataset. Similar improvements are observed
in ResNet-34. On the MVTec hazelnut anomaly detection task, the proposed method
achieves competitive results in both segmentation and detection, outperforming
existing approaches.

</details>


### [14] [PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation](https://arxiv.org/abs/2507.16116)
*Yaofang Liu,Yumeng Ren,Aitor Artola,Yuxuan Hu,Xiaodong Cun,Xiaotong Zhao,Alan Zhao,Raymond H. Chan,Suiyun Zhang,Rui Liu,Dandan Tu,Jean-Michel Morel*

Main category: cs.CV

TL;DR: Pusa通过向量化时间步自适应（VTA）革新了视频扩散模型，实现了更高效、灵活的时序建模，同时降低了算力和数据需求，还支持多种任务、零样本泛化能力，并开源了代码。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散生成模型在时间建模方面受到限制，主要归因于传统的标量时间步同步机制。这导致模型在生成时效率低、泛化性差，并且训练成本极高，难以广泛应用。

Method: 作者提出Pusa范式，引入向量化时间步自适应（Vectorized Timestep Adaptation, VTA）技术，从而突破标量时间步的限制，实现对时间序列的细粒度控制。这种方法以非破坏性方式对现有SOTA模型Wan2.1-T2V-14B进行微调，极大保留和增强基础模型能力，同时支持多任务、零样本推理。

Result: Pusa展现出极高的效率：训练成本和数据需求大幅减少（从10万美元、1000万样本减至500美元、4000样本）。在图像转视频任务（I2V）上，VBench-I2V总分达到87.32%，优于同类Wan-I2V-14B模型。Pusa还支持无特定训练的多任务如起止帧生成和视频扩展及文本生成视频。

Conclusion: Pusa为下一代高效、可扩展、多用的视频生成奠定了基础，显著降低了门槛，有望推动视频扩散模型在科研与产业领域的普及和应用。

Abstract: The rapid advancement of video diffusion models has been hindered by
fundamental limitations in temporal modeling, particularly the rigid
synchronization of frame evolution imposed by conventional scalar timestep
variables. While task-specific adaptations and autoregressive models have
sought to address these challenges, they remain constrained by computational
inefficiency, catastrophic forgetting, or narrow applicability. In this work,
we present Pusa, a groundbreaking paradigm that leverages vectorized timestep
adaptation (VTA) to enable fine-grained temporal control within a unified video
diffusion framework. Besides, VTA is a non-destructive adaptation, which means
it fully preserves the capabilities of the base model. By finetuning the SOTA
Wan2.1-T2V-14B model with VTA, we achieve unprecedented efficiency --
surpassing the performance of Wan-I2V-14B with $\leq$ 1/200 of the training
cost (\$500 vs. $\geq$ \$100,000) and $\leq$ 1/2500 of the dataset size (4K vs.
$\geq$ 10M samples). Pusa not only sets a new standard for image-to-video (I2V)
generation, achieving a VBench-I2V total score of 87.32\% (vs. 86.86\% of
Wan-I2V-14B), but also unlocks many zero-shot multi-task capabilities such as
start-end frames and video extension -- all without task-specific training.
Meanwhile, Pusa can still perform text-to-video generation. Mechanistic
analyses reveal that our approach preserves the foundation model's generative
priors while surgically injecting temporal dynamics, avoiding the combinatorial
explosion inherent to vectorized timesteps. This work establishes a scalable,
efficient, and versatile paradigm for next-generation video synthesis,
democratizing high-fidelity video generation for research and industry alike.
Code is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen

</details>


### [15] [Universal Wavelet Units in 3D Retinal Layer Segmentation](https://arxiv.org/abs/2507.16119)
*An D. Le,Hung Nguyen,Melanie Tran,Jesse Most,Dirk-Uwe G. Bartsch,William R Freeman,Shyamanga Borooah,Truong Q. Nguyen,Cheolhong An*

Main category: cs.CV

TL;DR: 本文首次在光学相干断层扫描（OCT）体积图像的3D视网膜分割任务中应用可调小波单元（UwUs），显著提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 传统的最大池化方法在特征保留上存在局限，尤其在空间细节和结构一致性方面难以兼顾。为提升医学影像分割的表现，需要新型方法增强特征提取。

Method: 在MGU-Net的架构基础上，融入了三种基于小波的下采样模块（OrthLattUwU, BiorthLattUwU, LS-BiorthLattUwU），这些模块采用可学习的晶格滤波器来同时保留低频与高频特征。

Result: 在Jacobs Retina Center（JRC）OCT数据集上的实验表明，特别是采用LS-BiorthLattUwU的模型在准确度和Dice系数上取得了显著提升，优于传统方法。

Conclusion: 可调小波滤波器对于三维医学图像分割具有独特优势，能够提升分割精度与空间结构一致性，显示出广阔的临床应用前景。

Abstract: This paper presents the first study to apply tunable wavelet units (UwUs) for
3D retinal layer segmentation from Optical Coherence Tomography (OCT) volumes.
To overcome the limitations of conventional max-pooling, we integrate three
wavelet-based downsampling modules, OrthLattUwU, BiorthLattUwU, and
LS-BiorthLattUwU, into a motion-corrected MGU-Net architecture. These modules
use learnable lattice filter banks to preserve both low- and high-frequency
features, enhancing spatial detail and structural consistency. Evaluated on the
Jacobs Retina Center (JRC) OCT dataset, our framework shows significant
improvement in accuracy and Dice score, particularly with LS-BiorthLattUwU,
highlighting the benefits of tunable wavelet filters in volumetric medical
image segmentation.

</details>


### [16] [LongSplat: Online Generalizable 3D Gaussian Splatting from Long Sequence Images](https://arxiv.org/abs/2507.16144)
*Guichen Huang,Ruoyu Wang,Xiangjun Gao,Che Sun,Yuwei Wu,Shenghua Gao,Yunde Jia*

Main category: cs.CV

TL;DR: 本文提出LongSplat框架，实现了长序列图像输入下的在线实时3D高斯重建，具备高效、内存友好的特性，并在保真度和效率之间取得业界领先。


<details>
  <summary>Details</summary>
Motivation: 当前3D Gaussian Splatting在新视角合成方面表现优异，但难以应对在线、长序列应用：现有方案要么依赖慢速的每场景优化，要么无法高效增量更新，难以持续且高性能运行。

Method: LongSplat提出了流式更新机制，可渐进结合当前视角观测信息，并通过历史高斯的选择性压缩减少冗余，高效控制内存和计算资源消耗。关键创新在于引入了Gaussian-Image Representation（GIR），即将3D高斯参数编码为结构化的类图像2D格式，实现历史与当前观测的高效融合与冗余压缩。此外，集成现有图像压缩技术，进一步提升了3D高斯的紧凑性和质量。

Result: LongSplat支持长时间序列的在线重建，同时在保持实时性能前提下，较现有逐像素高斯生成方法将高斯点数量减少了44%。多项评测均表明，其在效率和质量之间取得了最佳权衡，领先于现有技术。

Conclusion: LongSplat有效突破了3D高斯在线重建在长序列场景下的效率瓶颈，提升了运行效率、内存占用和重建质量，为实时新视角合成提供了更实用的解决方案。

Abstract: 3D Gaussian Splatting achieves high-fidelity novel view synthesis, but its
application to online long-sequence scenarios is still limited. Existing
methods either rely on slow per-scene optimization or fail to provide efficient
incremental updates, hindering continuous performance. In this paper, we
propose LongSplat, an online real-time 3D Gaussian reconstruction framework
designed for long-sequence image input. The core idea is a streaming update
mechanism that incrementally integrates current-view observations while
selectively compressing redundant historical Gaussians. Crucial to this
mechanism is our Gaussian-Image Representation (GIR), a representation that
encodes 3D Gaussian parameters into a structured, image-like 2D format. GIR
simultaneously enables efficient fusion of current-view and historical
Gaussians and identity-aware redundancy compression. These functions enable
online reconstruction and adapt the model to long sequences without
overwhelming memory or computational costs. Furthermore, we leverage an
existing image compression method to guide the generation of more compact and
higher-quality 3D Gaussians. Extensive evaluations demonstrate that LongSplat
achieves state-of-the-art efficiency-quality trade-offs in real-time novel view
synthesis, delivering real-time reconstruction while reducing Gaussian counts
by 44\% compared to existing per-pixel Gaussian prediction methods.

</details>


### [17] [SPACT18: Spiking Human Action Recognition Benchmark Dataset with Complementary RGB and Thermal Modalities](https://arxiv.org/abs/2507.16151)
*Yasser Ashraf,Ahmed Sharshar,Velibor Bojkovic,Bin Gu*

Main category: cs.CV

TL;DR: 本文提出了首个基于类生物受激视觉传感器（spike camera）的动作识别视频数据集，并包含同步RGB和热成像模态，为脉冲神经网络（SNNs）在多模态视频理解和动作识别方面提供了新的基准。


<details>
  <summary>Details</summary>
Motivation: 当前动作识别任务多以RGB或事件摄像机数据为主，难以兼顾能效与时空精度。spike camera具备超高能效和卓越的时空分辨能力，亟需针对其开发专门的数据集以促进神经网络及多模态视频理解的研究。

Method: 作者通过采集spike camera、同步RGB和热成像数据，构建了三套新的多模态数据集，严格保留了spike数据的稀疏性和时间精度。

Result: 构建了可同时支持spiking、RGB与热成像的数据集，为多模态视频动作识别和SNNs性能评估提供了新平台。

Conclusion: 本文贡献的多模态数据集将推动基于spike数据的高能效、超低功耗动作识别研究，加速脉冲神经网络与多模态融合方法的发展。

Abstract: Spike cameras, bio-inspired vision sensors, asynchronously fire spikes by
accumulating light intensities at each pixel, offering ultra-high energy
efficiency and exceptional temporal resolution. Unlike event cameras, which
record changes in light intensity to capture motion, spike cameras provide even
finer spatiotemporal resolution and a more precise representation of continuous
changes. In this paper, we introduce the first video action recognition (VAR)
dataset using spike camera, alongside synchronized RGB and thermal modalities,
to enable comprehensive benchmarking for Spiking Neural Networks (SNNs). By
preserving the inherent sparsity and temporal precision of spiking data, our
three datasets offer a unique platform for exploring multimodal video
understanding and serve as a valuable resource for directly comparing spiking,
thermal, and RGB modalities. This work contributes a novel dataset that will
drive research in energy-efficient, ultra-low-power video understanding,
specifically for action recognition tasks using spike-based data.

</details>


### [18] [LSSGen: Leveraging Latent Space Scaling in Flow and Diffusion for Efficient Text to Image Generation](https://arxiv.org/abs/2507.16154)
*Jyun-Ze Tang,Chih-Fan Hsu,Jeng-Lin Li,Ming-Ching Chang,Wei-Chao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新方法LSSGen，可以直接在潜在空间（latent space）进行分辨率缩放，提升文本生成图像的速度和质量，显著优于传统的像素空间缩放方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型为提升生成速度，常采用低分辨率先去噪、再缩放的方式，但在像素空间的下采样与上采样会引入伪影或失真，使最终图像质量下降。

Method: LSSGen框架提出将分辨率缩放操作转移到潜在空间，通过轻量级的潜在上采样器，在不修改主干Transformer或U-Net结构的前提下，实现高效且灵活的多分辨率生成。

Result: 实验全方位评估了LSSGen在文本-图像匹配度与感知质量上的表现。生成1024x1024图像时，LSSGen以近乎相同推理速度，TOPIQ分数较传统方法提升高达246%。

Conclusion: LSSGen方法兼顾效率与最终图像质量，显著优于像素空间缩放方案，是提升高分辨率文本生成图像系统的重要进展。

Abstract: Flow matching and diffusion models have shown impressive results in
text-to-image generation, producing photorealistic images through an iterative
denoising process. A common strategy to speed up synthesis is to perform early
denoising at lower resolutions. However, traditional methods that downscale and
upscale in pixel space often introduce artifacts and distortions. These issues
arise when the upscaled images are re-encoded into the latent space, leading to
degraded final image quality. To address this, we propose {\bf Latent Space
Scaling Generation (LSSGen)}, a framework that performs resolution scaling
directly in the latent space using a lightweight latent upsampler. Without
altering the Transformer or U-Net architecture, LSSGen improves both efficiency
and visual quality while supporting flexible multi-resolution generation. Our
comprehensive evaluation covering text-image alignment and perceptual quality
shows that LSSGen significantly outperforms conventional scaling approaches.
When generating $1024^2$ images at similar speeds, it achieves up to 246\%
TOPIQ score improvement.

</details>


### [19] [AMMNet: An Asymmetric Multi-Modal Network for Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2507.16158)
*Hui Ye,Haodong Chen,Zeke Zexi Hu,Xiaoming Chen,Yuk Ying Chung*

Main category: cs.CV

TL;DR: 本文提出了一种新的不对称多模态网络（AMMNet），针对遥感图像中RGB与DSM数据联合分割时的效率低和性能下降问题，通过结构设计有效提升性能并减小计算量。


<details>
  <summary>Details</summary>
Motivation: 当前遥感语义分割结合RGB和DSM多模态数据虽能提升效果，但存在模型冗余导致的高算力消耗及多模态对齐不佳带来的分割效果下降，特别是在复杂城市场景下尤为突出。作者旨在解决这两大瓶颈。

Method: AMMNet主要包括三个创新模块：1）不对称双编码器ADE，根据RGB和DSM的特性分配不同深度与复杂度的编码器；2）不对称先验融合模块APF，利用先验矩阵辅助结构感知与多模态特征融合；3）分布对齐模块DA，通过最小化特征分布差异，实现跨模态对齐。

Result: 在ISPRS Vaihingen和Potsdam遥感数据集上，AMMNet展现出优于现有多模态网络的语义分割精度，并同时显著降低了计算和内存开销。

Conclusion: AMMNet在提升多模态分割精度的同时，还兼顾了模型的高效性和健壮性，适用于复杂城市遥感应用。该结构可为多模态遥感任务的实际部署带来更优表现。

Abstract: Semantic segmentation in remote sensing (RS) has advanced significantly with
the incorporation of multi-modal data, particularly the integration of RGB
imagery and the Digital Surface Model (DSM), which provides complementary
contextual and structural information about the ground object. However,
integrating RGB and DSM often faces two major limitations: increased
computational complexity due to architectural redundancy, and degraded
segmentation performance caused by modality misalignment. These issues
undermine the efficiency and robustness of semantic segmentation, particularly
in complex urban environments where precise multi-modal integration is
essential. To overcome these limitations, we propose Asymmetric Multi-Modal
Network (AMMNet), a novel asymmetric architecture that achieves robust and
efficient semantic segmentation through three designs tailored for RGB-DSM
input pairs. To reduce architectural redundancy, the Asymmetric Dual Encoder
(ADE) module assigns representational capacity based on modality-specific
characteristics, employing a deeper encoder for RGB imagery to capture rich
contextual information and a lightweight encoder for DSM to extract sparse
structural features. Besides, to facilitate modality alignment, the Asymmetric
Prior Fuser (APF) integrates a modality-aware prior matrix into the fusion
process, enabling the generation of structure-aware contextual features.
Additionally, the Distribution Alignment (DA) module enhances cross-modal
compatibility by aligning feature distributions through divergence
minimization. Extensive experiments on the ISPRS Vaihingen and Potsdam datasets
demonstrate that AMMNet attains state-of-the-art segmentation accuracy among
multi-modal networks while reducing computational and memory requirements.

</details>


### [20] [AtrousMamaba: An Atrous-Window Scanning Visual State Space Model for Remote Sensing Change Detection](https://arxiv.org/abs/2507.16172)
*Tao Wang,Tiecheng Bai,Chao Xu,Bin Liu,Erlei Zhang,Jiyun Huang,Hongming Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉状态空间模型AtrousMamba，通过引入空洞窗口扫描机制，增强了模型在局部细节提取和全局上下文融合方面的能力。实验证明该方法在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前VSS模型Mamba虽然在处理长序列方面表现优秀，但在密集预测任务中往往忽略了局部信息的作用，并且Mamba能否像CNN一样有效提取局部特征仍是未解问题。

Method: 提出AtrousMamba模型，核心在于引入空洞窗口选择性扫描机制（AWVSS），可调节扫描范围，实现局部与全局信息的平衡融合，并应用于端到端的变化检测任务（AWMambaBCD、AWMambaSCD）。

Result: 在6个基准数据集上的实验表明，AtrousMamba在二元变化检测和语义变化检测任务中均优于CNN、Transformer和原始Mamba等方法。

Conclusion: AtrousMamba能够兼顾长距离依赖捕捉和细粒度局部信息保留，提升了视觉任务的表现，为VSS模型局部特征提取提供了新思路。

Abstract: Recently, a novel visual state space (VSS) model, referred to as Mamba, has
demonstrated significant progress in modeling long sequences with linear
complexity, comparable to Transformer models, thereby enhancing its
adaptability for processing visual data. Although most methods aim to enhance
the global receptive field by directly modifying Mamba's scanning mechanism,
they tend to overlook the critical importance of local information in dense
prediction tasks. Additionally, whether Mamba can effectively extract local
features as convolutional neural networks (CNNs) do remains an open question
that merits further investigation. In this paper, We propose a novel model,
AtrousMamba, which effectively balances the extraction of fine-grained local
details with the integration of global contextual information. Specifically,
our method incorporates an atrous-window selective scan mechanism, enabling a
gradual expansion of the scanning range with adjustable rates. This design
shortens the distance between adjacent tokens, enabling the model to
effectively capture fine-grained local features and global context. By
leveraging the atrous window scan visual state space (AWVSS) module, we design
dedicated end-to-end Mamba-based frameworks for binary change detection (BCD)
and semantic change detection (SCD), referred to as AWMambaBCD and AWMambaSCD,
respectively. Experimental results on six benchmark datasets show that the
proposed framework outperforms existing CNN-based, Transformer-based, and
Mamba-based methods. These findings clearly demonstrate that Mamba not only
captures long-range dependencies in visual data but also effectively preserves
fine-grained local details.

</details>


### [21] [Explicit Context Reasoning with Supervision for Visual Tracking](https://arxiv.org/abs/2507.16191)
*Fansheng Zeng,Bineng Zhong,Haiying Xia,Yufei Tan,Xiantao Hu,Liangtao Shi,Shuxiang Song*

Main category: cs.CV

TL;DR: 论文提出了一种全新的视觉跟踪方法RSTrack，通过引入三种机制显著提升了跨帧建模的时序一致性与鲁棒性，并在多个基准数据集上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 当前主流的视觉跟踪方法在关联帧间上下文时仅仅堆叠历史信息，缺乏对关联过程的显式监督，导致无法有效建模目标的动态变化，影响时序建模效果。因此需要提出一种新的机制来强化时序一致性与上下文推理能力。

Method: RSTrack提出三大核心机制：（1）上下文推理机制，将无约束的上下文关联转化为有监督的时序推理过程，通过历史目标状态预测当前目标表征；（2）前向监督策略，以真实目标特征为锚点，约束推理流程，引导预测输出靠近真实分布，抑制漂移；（3）高效状态建模，利用压缩-重构机制提取目标核心特征，去除冗余信息，避免无效关联。三者协同优化建模效果。

Result: 实验表明，RSTrack在多个视觉跟踪基准数据集上取得了最新的SOTA（state-of-the-art）性能，并且保持实时的推理速度。

Conclusion: 通过显式的上下文推理建模与合理的监督和特征精简策略，RSTrack有效缓解了传统方法时序建模中存在的上下文关联发散等难题，为视觉跟踪的稳健性和时序一致性做出了新的贡献。

Abstract: Contextual reasoning with constraints is crucial for enhancing temporal
consistency in cross-frame modeling for visual tracking. However, mainstream
tracking algorithms typically associate context by merely stacking historical
information without explicitly supervising the association process, making it
difficult to effectively model the target's evolving dynamics. To alleviate
this problem, we propose RSTrack, which explicitly models and supervises
context reasoning via three core mechanisms. \textit{1) Context Reasoning
Mechanism}: Constructs a target state reasoning pipeline, converting
unconstrained contextual associations into a temporal reasoning process that
predicts the current representation based on historical target states, thereby
enhancing temporal consistency. \textit{2) Forward Supervision Strategy}:
Utilizes true target features as anchors to constrain the reasoning pipeline,
guiding the predicted output toward the true target distribution and
suppressing drift in the context reasoning process. \textit{3) Efficient State
Modeling}: Employs a compression-reconstruction mechanism to extract the core
features of the target, removing redundant information across frames and
preventing ineffective contextual associations. These three mechanisms
collaborate to effectively alleviate the issue of contextual association
divergence in traditional temporal modeling. Experimental results show that
RSTrack achieves state-of-the-art performance on multiple benchmark datasets
while maintaining real-time running speeds. Our code is available at
https://github.com/GXNU-ZhongLab/RSTrack.

</details>


### [22] [ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning](https://arxiv.org/abs/2507.16815)
*Chi-Pin Huang,Yueh-Hua Wu,Min-Hung Chen,Yu-Chiang Frank Wang,Fu-En Yang*

Main category: cs.CV

TL;DR: 本文提出了ThinkAct框架，通过强化视觉潜表示，将高层推理与低层动作执行相结合，提升多模态任务中的长时规划与适应能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作推理任务中，端到端方法缺乏显式推理，难以进行多步规划和复杂任务自适应。

Method: 提出ThinkAct双系统框架，采用多模态大模型生成具备目标和轨迹一致性视觉奖励的推理计划，并将这些计划压缩为视觉潜变量，作为后续动作模型的条件输入，以实现稳健的动作执行。

Result: 在具身推理和机器人操作基准上，ThinkAct展现出更强的少样本自适应、长时规划和自我纠错能力。

Conclusion: ThinkAct能有效提升具身AI任务中复杂推理、适应和规划的能力，优于传统端到端方法。

Abstract: Vision-language-action (VLA) reasoning tasks require agents to interpret
multimodal instructions, perform long-horizon planning, and act adaptively in
dynamic environments. Existing approaches typically train VLA models in an
end-to-end fashion, directly mapping inputs to actions without explicit
reasoning, which hinders their ability to plan over multiple steps or adapt to
complex task variations. In this paper, we propose ThinkAct, a dual-system
framework that bridges high-level reasoning with low-level action execution via
reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate
embodied reasoning plans guided by reinforcing action-aligned visual rewards
based on goal completion and trajectory consistency. These reasoning plans are
compressed into a visual plan latent that conditions a downstream action model
for robust action execution on target environments. Extensive experiments on
embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct
enables few-shot adaptation, long-horizon planning, and self-correction
behaviors in complex embodied AI tasks.

</details>


### [23] [LMM4Edit: Benchmarking and Evaluating Multimodal Image Editing with LMMs](https://arxiv.org/abs/2507.16193)
*Zitong Xu,Huiyu Duan,Bingnan Liu,Guangji Ma,Jiarui Wang,Liu Yang,Shiqi Gao,Xiaoyu Wang,Jia Wang,Xiongkuo Min,Guangtao Zhai,Weisi Lin*

Main category: cs.CV

TL;DR: 本文提出了EBench-18K大规模图文编辑基准和基于大模型的评价指标LMM4Edit，有效提升了文本引导图像编辑任务的评估能力，其性能优异且与人类偏好高度一致。


<details>
  <summary>Details</summary>
Motivation: 当前文本引导图像编辑（TIE）模型在图像质量、编辑精度和原图一致性之间难以平衡，且现有评测基准和指标在规模和人类感知对齐上存在不足，极大限制了TIE模型的实际应用价值。

Method: 构建了包含1,080张原图、18,000余张编辑图和细粒度人工偏好标注的EBench-18K大规模基准，并引入LMM（Large Multimodal Models）用于评测编辑图像。同时提出了基于LMM的方法LMM4Edit，从感知质量、编辑对齐性、属性保留和任务问答准确性四个维度全方位评价编辑效果。

Result: LMM4Edit在大规模实验中表现优异，与人工偏好高度一致，并且在零样本情况下对其他数据集具有良好泛化能力。

Conclusion: EBench-18K填补了TIE领域大规模、多维度评测的空白；LMM4Edit提供了一体化高相关性的评测工具，显著推动了TIE模型的评价标准和发展。

Abstract: The rapid advancement of Text-guided Image Editing (TIE) enables image
modifications through text prompts. However, current TIE models still struggle
to balance image quality, editing alignment, and consistency with the original
image, limiting their practical applications. Existing TIE evaluation
benchmarks and metrics have limitations on scale or alignment with human
perception. To this end, we introduce EBench-18K, the first large-scale image
Editing Benchmark including 18K edited images with fine-grained human
preference annotations for evaluating TIE. Specifically, EBench-18K includes
1,080 source images with corresponding editing prompts across 21 tasks, 18K+
edited images produced by 17 state-of-the-art TIE models, 55K+ mean opinion
scores (MOSs) assessed from three evaluation dimensions, and 18K+
question-answering (QA) pairs. Based on EBench-18K, we employ outstanding LMMs
to assess edited images, while the evaluation results, in turn, provide
insights into assessing the alignment between the LMMs' understanding ability
and human preferences. Then, we propose LMM4Edit, a LMM-based metric for
evaluating image Editing models from perceptual quality, editing alignment,
attribute preservation, and task-specific QA accuracy in an all-in-one manner.
Extensive experiments show that LMM4Edit achieves outstanding performance and
aligns well with human preference. Zero-shot validation on the other datasets
also shows the generalization ability of our model. The dataset and code are
available at https://github.com/IntMeGroup/LMM4Edit.

</details>


### [24] [A Single-step Accurate Fingerprint Registration Method Based on Local Feature Matching](https://arxiv.org/abs/2507.16201)
*Yuwei Jia,Zhe Cui,Fei Su*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的单步指纹配准算法，通过直接预测两幅指纹图像间的半稠密匹配点，实现高效且鲁棒的图像对齐，克服了低质量指纹图像下传统方法易失败的难题。实验显示，该方法的配准性能达到了现有最佳水平。


<details>
  <summary>Details</summary>
Motivation: 当前主流指纹配准方法一般分为两个步骤：基于细节点的初始配准和基于点对的稠密配准。然而在指纹图像质量较低时，细节点数量减少，导致初始配准易失败，进而影响整体配准性能。因此，亟需一种能够有效应对低质量指纹图像的鲁棒配准方法。

Method: 本文提出了一种端到端、单步的指纹配准算法，核心做法是直接预测两幅指纹之间的半稠密匹配点。该算法融合了全局-局部注意力机制，实现像素级的精确对齐，并规避了依赖细节点检测的初始配准。

Result: 实验结果表明，所提方法即使在单步配准的情况下，其匹配性能也达到了现有先进水平（state-of-the-art），此外还可以与密集配准算法结合进一步提升性能。

Conclusion: 本文方法有效提升了指纹配准的鲁棒性和精度，尤其适用于低质量图像场景，为实际指纹识别系统提供了更可靠的技术方案。

Abstract: Distortion of the fingerprint images leads to a decline in fingerprint
recognition performance, and fingerprint registration can mitigate this
distortion issue by accurately aligning two fingerprint images. Currently,
fingerprint registration methods often consist of two steps: an initial
registration based on minutiae, and a dense registration based on matching
points. However, when the quality of fingerprint image is low, the number of
detected minutiae is reduced, leading to frequent failures in the initial
registration, which ultimately causes the entire fingerprint registration
process to fail. In this study, we propose an end-to-end single-step
fingerprint registration algorithm that aligns two fingerprints by directly
predicting the semi-dense matching points correspondences between two
fingerprints. Thus, our method minimizes the risk of minutiae registration
failure and also leverages global-local attentions to achieve end-to-end
pixel-level alignment between the two fingerprints. Experiment results prove
that our method can achieve the state-of-the-art matching performance with only
single-step registration, and it can also be used in conjunction with dense
registration algorithms for further performance improvements.

</details>


### [25] [Advancing Visual Large Language Model for Multi-granular Versatile Perception](https://arxiv.org/abs/2507.16213)
*Wentao Xiang,Haoxian Tan,Cong Wei,Yujie Zhong,Dengjie Li,Yujiu Yang*

Main category: cs.CV

TL;DR: 本文提出了MVP-LM，一个集成视觉大语言模型的多粒度、多任务视觉感知框架，实现了多种感知任务的统一处理。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉感知任务的研究大多专注于少数任务组合，缺乏统一模型处理多种任务的普适能力，限制了实用性和场景适应性。

Method: 提出MVP-LM框架，通过创新性的多粒度解码器和受CoT启发的数据集统一策略，将词级、句级感知任务和框、掩膜预测统一到单一架构下。同时引入查询增强策略，充分利用视觉大语言模型的解码与生成能力。

Result: 在多个基准测试上，MVP-LM在词级和句级感知任务中均展现了优异性能，验证了该框架的有效性和多任务处理能力。

Conclusion: MVP-LM实现了不同类别与粒度下视觉感知任务的统一建模，显著提升了多任务适应能力，推动了视觉大语言模型在多任务视觉感知领域的发展。

Abstract: Perception is a fundamental task in the field of computer vision,
encompassing a diverse set of subtasks that can be systematically categorized
into four distinct groups based on two dimensions: prediction type and
instruction type. Notably, existing researches often focus solely on a limited
subset of these potential combinations, which constrains their applicability
and versatility across various contexts. In response to this challenge, we
present MVP-LM, a Multi-granular and Versatile Perception framework
incorporating Visual Large Language Model. Our framework is designed to
integrate both word-based and sentence-based perception tasks alongside box and
mask predictions within a single architecture. MVP-LM features an innovative
multi-granularity decoder in conjunction with a CoT-inspired dataset
unification strategy, enabling seamless supervised fine-tuning across a wide
spectrum of tasks, including but not limited to panoptic segmentation,
detection, grounding, and referring expression segmentation. Furthermore, we
introduce a query enhancement strategy aimed at harnessing the decoding and
generative capabilities inherent in VLLMs. Extensive experiments conducted
across a range of benchmarks in both word-based and sentence-based perception
tasks substantiate the efficacy of our framework. The code will be available at
https://github.com/xiangwentao666/MVP-LM.

</details>


### [26] [LDRFusion: A LiDAR-Dominant multimodal refinement framework for 3D object detection](https://arxiv.org/abs/2507.16224)
*Jijun Wang,Yan Wu,Yujian Mo,Junqiao Zhao,Jun Yan,Yinghao Hu*

Main category: cs.CV

TL;DR: 提出了一种新的基于LiDAR主导的两阶段融合框架LDRFusion，通过只在第二阶段引入伪点云，降低噪声影响并提升3D目标检测效果，在KITTI数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的激光雷达-相机融合方法虽然提升了3D目标检测准确性，但在利用伪点云优化稀疏点云时引入了噪声，容易导致误检，且不同模态的可靠性与作用不同。

Method: 提出LDRFusion框架：第一阶段只依赖激光雷达，获得精准候选框；第二阶段引入通过深度补全生成的伪点云检测难例，并将两阶段的检测结果融合；还设计了分层伪点云残差编码模块，用特征和位置残差联合提升伪点云的结构表达能力。

Result: 在KITTI数据集各类别和难度下均取得了持续、良好的性能表现。

Conclusion: LiDAR主导、分阶段融合与残差编码等设计有效规避了伪点云噪声影响，实现了多传感器高精度3D目标检测。

Abstract: Existing LiDAR-Camera fusion methods have achieved strong results in 3D
object detection. To address the sparsity of point clouds, previous approaches
typically construct spatial pseudo point clouds via depth completion as
auxiliary input and adopts a proposal-refinement framework to generate
detection results. However, introducing pseudo points inevitably brings noise,
potentially resulting in inaccurate predictions. Considering the differing
roles and reliability levels of each modality, we propose LDRFusion, a novel
Lidar-dominant two-stage refinement framework for multi-sensor fusion. The
first stage soley relies on LiDAR to produce accurately localized proposals,
followed by a second stage where pseudo point clouds are incorporated to detect
challenging instances. The instance-level results from both stages are
subsequently merged. To further enhance the representation of local structures
in pseudo point clouds, we present a hierarchical pseudo point residual
encoding module, which encodes neighborhood sets using both feature and
positional residuals. Experiments on the KITTI dataset demonstrate that our
framework consistently achieves strong performance across multiple categories
and difficulty levels.

</details>


### [27] [MONITRS: Multimodal Observations of Natural Incidents Through Remote Sensing](https://arxiv.org/abs/2507.16228)
*Shreelekha Revankar,Utkarsh Mall,Cheng Perng Phoo,Kavita Bala,Bharath Hariharan*

Main category: cs.CV

TL;DR: 该论文介绍了MONITRS，这是一个包含超过1万个FEMA灾害事件的多模态数据集，含有时序卫星影像与自然语言标注，可用于提升机器学习灾害监测能力。


<details>
  <summary>Details</summary>
Motivation: 因自然灾害破坏性极大，传统灾害响应受限于受灾地区难以获取最新信息。目前遥感和深度学习虽有进展，但依然存在类型局限性、依赖人工解读、缺乏时序与文本注释数据的问题。

Method: 作者提出了MONITRS数据集，该数据集包含：1. 时序卫星影像；2. 新闻文章的自然语言注释；3. 事件地理标签；4. 问答对。随后，作者以该数据集微调了多模态大模型（MLLMs），并在灾害监测任务上测试了性能。

Result: 在MONITRS数据集上微调现有MLLM模型后，灾害监测任务表现有显著提升，成为相关方向的新基准。

Conclusion: MONITRS数据集为基于机器学习的灾害响应系统提供了高质量的训练与评测资源，大大推动了相关模型在实际灾害应急中的应用前景。

Abstract: Natural disasters cause devastating damage to communities and infrastructure
every year. Effective disaster response is hampered by the difficulty of
accessing affected areas during and after events. Remote sensing has allowed us
to monitor natural disasters in a remote way. More recently there have been
advances in computer vision and deep learning that help automate satellite
imagery analysis, However, they remain limited by their narrow focus on
specific disaster types, reliance on manual expert interpretation, and lack of
datasets with sufficient temporal granularity or natural language annotations
for tracking disaster progression. We present MONITRS, a novel multimodal
dataset of more than 10,000 FEMA disaster events with temporal satellite
imagery and natural language annotations from news articles, accompanied by
geotagged locations, and question-answer pairs. We demonstrate that fine-tuning
existing MLLMs on our dataset yields significant performance improvements for
disaster monitoring tasks, establishing a new benchmark for machine
learning-assisted disaster response systems. Code can be found at:
https://github.com/ShreelekhaR/MONITRS

</details>


### [28] [Positive Style Accumulation: A Style Screening and Continuous Utilization Framework for Federated DG-ReID](https://arxiv.org/abs/2507.16238)
*Xin Xu,Chaoyue Ren,Wei Liu,Wenke Huang,Bin Yang,Zhixi Yu,Kui Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种面向行人重识别的联合域泛化联邦学习框架——SSCU，通过筛选和持续利用贡献积极的风格样本（positive styles），显著提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习行人重识别泛化方法通常利用风格变换提升样本多样性，但并非所有风格变换对泛化有益，部分风格甚至会降低性能。本文关注于区分并充分利用真正有利于泛化的风格样本。

Method: 设计了“泛化增益引导的动态风格记忆（GGDSM）”机制，在每个客户端筛选并累积positive styles，并提出风格记忆识别损失以利用记忆库中的正向风格；同时提出协同风格训练（CST）策略，结合新生成风格和已记忆positive styles分支训练客户端模型，实现风格快速获取与充分利用。

Result: 在多个数据集和目标域、源域实验中，所提方法均优于现有主流方法，泛化性能显著提升。

Conclusion: 筛选并持续利用对泛化有益的风格样本对行人重识别联邦泛化具有关键作用，所提SSCU方法实现了显著性能提升，具备较高应用价值。

Abstract: The Federated Domain Generalization for Person re-identification (FedDG-ReID)
aims to learn a global server model that can be effectively generalized to
source and target domains through distributed source domain data. Existing
methods mainly improve the diversity of samples through style transformation,
which to some extent enhances the generalization performance of the model.
However, we discover that not all styles contribute to the generalization
performance. Therefore, we define styles that are beneficial or harmful to the
model's generalization performance as positive or negative styles. Based on
this, new issues arise: How to effectively screen and continuously utilize the
positive styles. To solve these problems, we propose a Style Screening and
Continuous Utilization (SSCU) framework. Firstly, we design a Generalization
Gain-guided Dynamic Style Memory (GGDSM) for each client model to screen and
accumulate generated positive styles. Meanwhile, we propose a style memory
recognition loss to fully leverage the positive styles memorized by Memory.
Furthermore, we propose a Collaborative Style Training (CST) strategy to make
full use of positive styles. Unlike traditional learning strategies, our
approach leverages both newly generated styles and the accumulated positive
styles stored in memory to train client models on two distinct branches. This
training strategy is designed to effectively promote the rapid acquisition of
new styles by the client models, and guarantees the continuous and thorough
utilization of positive styles, which is highly beneficial for the model's
generalization performance. Extensive experimental results demonstrate that our
method outperforms existing methods in both the source domain and the target
domain.

</details>


### [29] [Scale Your Instructions: Enhance the Instruction-Following Fidelity of Unified Image Generation Model by Self-Adaptive Attention Scaling](https://arxiv.org/abs/2507.16240)
*Chao Zhou,Tianyi Wei,Nenghai Yu*

Main category: cs.CV

TL;DR: 提出SaaS方法增强统一图像生成模型对多子指令的遵循能力，无需额外训练，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前统一图像生成模型如OmniGen虽支持多模态输入，简化了任务流程，但在遇到含有多子指令的文本输入时，存在指令忽略问题，降低了模型实用性。

Method: 作者先通过输入扰动分析定位模型在多子指令处理上的关键步骤和层级，并观察跨注意力层的关注图，发现输入图像与被忽略子指令之间存在显著冲突。为解决该问题，提出SaaS方法，通过临时帧间跨注意力的一致性，动态调整每个子指令的注意力激活，无需模型再训练或推理时优化。

Result: SaaS在基于指令的图像编辑与视觉条件图像生成任务中，明显提升了模型对多子指令的遵循性，实验结果优于目前其它主流方法。

Conclusion: SaaS方法有效改善了统一图像生成模型在处理复杂多子指令任务时的表现，提升了模型的实用性和可靠性。

Abstract: Recent advancements in unified image generation models, such as OmniGen, have
enabled the handling of diverse image generation and editing tasks within a
single framework, accepting multimodal, interleaved texts and images in free
form. This unified architecture eliminates the need for text encoders, greatly
reducing model complexity and standardizing various image generation and
editing tasks, making it more user-friendly. However, we found that it suffers
from text instruction neglect, especially when the text instruction contains
multiple sub-instructions. To explore this issue, we performed a perturbation
analysis on the input to identify critical steps and layers. By examining the
cross-attention maps of these key steps, we observed significant conflicts
between neglected sub-instructions and the activations of the input image. In
response, we propose Self-Adaptive Attention Scaling (SaaS), a method that
leverages the consistency of cross-attention between adjacent timesteps to
dynamically scale the attention activation for each sub-instruction. Our SaaS
enhances instruction-following fidelity without requiring additional training
or test-time optimization. Experimental results on instruction-based image
editing and visual conditional image generation validate the effectiveness of
our SaaS, showing superior instruction-following fidelity over existing
methods. The code is available https://github.com/zhouchao-ops/SaaS.

</details>


### [30] [HoliTracer: Holistic Vectorization of Geographic Objects from Large-Size Remote Sensing Imagery](https://arxiv.org/abs/2507.16251)
*Yu Wang,Bo Dang,Wanchun Li,Wei Chen,Yansheng Li*

Main category: cs.CV

TL;DR: 本文提出了HoliTracer框架，能够对大尺寸遥感影像进行全局的高精度矢量化地理对象提取，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高分辨率遥感影像体积庞大，而现有方法通常只能处理小块影像，导致上下文信息丢失和输出矢量化碎片化，不能满足高精度地理对象提取需求。

Method: HoliTracer框架包含Context Attention Net（CAN）和一个矢量化管道。CAN采用局部到全局的注意力机制增强大尺寸影像的语义分割。随后利用Mask Contour Reformer（MCR）重构掩码边界多边形，并用Polygon Sequence Tracer（PST）追踪和提取顶点，实现整体矢量化。

Result: 在包括建筑物、水体和道路等多种大型遥感影像数据集上的实验表明，HoliTracer在准确率和齐全性等指标上均优于当前最先进方法。

Conclusion: HoliTracer首次实现了针对大幅面遥感影像的整体矢量化地理对象提取，兼顾上下文信息与精度，技术领先且具有实用价值。

Abstract: With the increasing resolution of remote sensing imagery (RSI), large-size
RSI has emerged as a vital data source for high-precision vector mapping of
geographic objects. Existing methods are typically constrained to processing
small image patches, which often leads to the loss of contextual information
and produces fragmented vector outputs. To address these, this paper introduces
HoliTracer, the first framework designed to holistically extract vectorized
geographic objects from large-size RSI. In HoliTracer, we enhance segmentation
of large-size RSI using the Context Attention Net (CAN), which employs a
local-to-global attention mechanism to capture contextual dependencies.
Furthermore, we achieve holistic vectorization through a robust pipeline that
leverages the Mask Contour Reformer (MCR) to reconstruct polygons and the
Polygon Sequence Tracer (PST) to trace vertices. Extensive experiments on
large-size RSI datasets, including buildings, water bodies, and roads,
demonstrate that HoliTracer outperforms state-of-the-art methods. Our code and
data are available in https://github.com/vvangfaye/HoliTracer.

</details>


### [31] [Edge-case Synthesis for Fisheye Object Detection: A Data-centric Perspective](https://arxiv.org/abs/2507.16254)
*Seunghyeon Kim,Kyeongryeol Go*

Main category: cs.CV

TL;DR: 本文针对鱼眼摄像头图像中的目标检测问题，提出了一种数据驱动的管道，通过合成边缘案例图像提升检测性能，显著改善了模型在特殊场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 鱼眼相机图像因畸变大，对常规目标检测模型提出了更高挑战。现有数据集和方法无法充分覆盖和解决模型盲区，尤其是在特殊边缘情况（如类别混淆、周边畸变和罕见场景）下。本文试图通过深入数据分析与针对性优化提升模型在鱼眼场景下的检测效果。

Method: 作者进行了详细的模型错误分析，发现了关键的边缘案例。然后利用图像生成模型，结合精心设计的提示词，合成模拟实际失败场景的图像，再用高质量检测器生成伪标签，将这些合成数据纳入训练中，以针对性地补足模型短板。

Result: 通过该数据管道，加入合成边缘案例后，目标检测性能获得了持续且显著的提升。证明了数据驱动和针对性修正对提升特殊领域检测性能尤为有效。

Conclusion: 面向鱼眼目标检测领域，深入分析数据盲点、定向修复其弱点，并结合合成数据扩充，可带来显著性能提升。这一方法对其他特殊领域的目标检测也具有借鉴意义。

Abstract: Fisheye cameras introduce significant distortion and pose unique challenges
to object detection models trained on conventional datasets. In this work, we
propose a data-centric pipeline that systematically improves detection
performance by focusing on the key question of identifying the blind spots of
the model. Through detailed error analysis, we identify critical edge-cases
such as confusing class pairs, peripheral distortions, and underrepresented
contexts. Then we directly address them through edge-case synthesis. We
fine-tuned an image generative model and guided it with carefully crafted
prompts to produce images that replicate real-world failure modes. These
synthetic images are pseudo-labeled using a high-quality detector and
integrated into training. Our approach results in consistent performance gains,
highlighting how deeply understanding data and selectively fixing its
weaknesses can be impactful in specialized domains like fisheye object
detection.

</details>


### [32] [Quality Text, Robust Vision: The Role of Language in Enhancing Visual Robustness of Vision-Language Models](https://arxiv.org/abs/2507.16257)
*Futa Waseda,Saku Sugawara,Isao Echizen*

Main category: cs.CV

TL;DR: 本文提出了一种利用高质量文本描述（caption）增强视觉-语言模型（VLMs）、如CLIP，在零样本任务下的对抗鲁棒性的新方法QT-AFT，并在16个数据集上取得了最先进的鲁棒性与准确率。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs广泛应用于零样本任务，但现有对抗训练方法忽视了语言在提升视觉鲁棒性中的作用。监督对抗训练依赖简短文本，易导致过拟合；无监督方法缺乏语义指导，对实际攻击防御能力不足。

Method: 提出QT-AFT，通过高质量的图片描述文本引导生成对抗样本，使视觉编码器在对抗干扰下更好地识别广泛的图像特征，有效兼顾语义丰富性与鲁棒性。

Result: QT-AFT在16个零样本数据集上实现了最优的对抗鲁棒性和干净准确率，并系统分析了语言在提升视觉鲁棒性中的作用（例如，描述物体属性比仅用物体名称更有效）。

Conclusion: 高质量语言监督对于提升视觉表征鲁棒性至关重要，未来需进一步探索以语义指导为核心的鲁棒视觉学习方法。

Abstract: Defending pre-trained vision-language models (VLMs), such as CLIP, against
adversarial attacks is crucial, as these models are widely used in diverse
zero-shot tasks, including image classification. However, existing adversarial
training (AT) methods for robust fine-tuning largely overlook the role of
language in enhancing visual robustness. Specifically, (1) supervised AT
methods rely on short texts (e.g., class labels) to generate adversarial
perturbations, leading to overfitting to object classes in the training data,
and (2) unsupervised AT avoids this overfitting but remains suboptimal against
practical text-guided adversarial attacks due to its lack of semantic guidance.
To address these limitations, we propose Quality Text-guided Adversarial
Fine-Tuning (QT-AFT), which leverages high-quality captions during training to
guide adversarial examples away from diverse semantics present in images. This
enables the visual encoder to robustly recognize a broader range of image
features even under adversarial noise, thereby enhancing robustness across
diverse downstream tasks. QT-AFT overcomes the key weaknesses of prior methods
-- overfitting in supervised AT and lack of semantic awareness in unsupervised
AT -- achieving state-of-the-art zero-shot adversarial robustness and clean
accuracy, evaluated across 16 zero-shot datasets. Furthermore, our
comprehensive study uncovers several key insights into the role of language in
enhancing vision robustness; for example, describing object properties in
addition to object names further enhances zero-shot robustness. Our findings
point to an urgent direction for future work -- centering high-quality
linguistic supervision in robust visual representation learning.

</details>


### [33] [ToFe: Lagged Token Freezing and Reusing for Efficient Vision Transformer Inference](https://arxiv.org/abs/2507.16260)
*Haoyue Zhang,Jie Zhang,Song Guo*

Main category: cs.CV

TL;DR: 本文提出了一种用于视觉Transformer的新框架ToFe，通过冻结和重用非关键信息token，有效减少计算量，在保持性能的同时提高效率。实验显示，其可将LV-ViT模型的计算开销减少一半，准确率损失不到2%。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer（ViT）在视觉任务中表现出色，但其自注意力机制计算量大，不适合资源受限设备。现有token精简方法不可逆地丢弃信息，造成后续模块不能重用早期被丢弃的信息，影响性能。研究动机在于寻求在性能与计算资源之间的高效平衡方法。

Method: 提出Token Freezing and Reusing(ToFe)框架。在每个阶段识别重要token，对于不重要的token，采取暂时冻结而非直接丢弃的策略，允许后续阶段重用。设计了token辨别预测模块及近似恢复模块，并与主干网络联合、感知预算地端到端训练，以动态应对每一层的token使用。

Result: 实验表明，ToFe在LV-ViT模型上将计算量降低50%，Top-1准确率仅下降不到2%，优于当前主流方法在性能-复杂度上的权衡。

Conclusion: ToFe框架有效实现了token的动态冻结与重用，在大幅度降低计算成本的同时几乎无损性能，为资源受限场景下ViT模型的高效部署提供了新思路。

Abstract: Although vision transformers (ViT) have shown remarkable success in various
vision tasks, their computationally expensive self-attention hinder their
deployment on resource-constrained devices. Token reduction, which discards
less important tokens during forward propagation, has been proposed to enhance
the efficiency of transformer models. However, existing methods handle
unimportant tokens irreversibly, preventing their reuse in subsequent blocks.
Considering that transformers focus on different information among blocks,
tokens reduced in early blocks might be useful later. Furthermore, to adapt
transformer models for resource-constrained devices, it is crucial to strike a
balance between model performance and computational overhead. To address these
challenges, in this paper, we introduce a novel Token Freezing and Reusing
(ToFe) framework, where we identify important tokens at each stage and
temporarily freeze the unimportant ones, allowing their lagged reusing at a
later stage. Specifically, we design a prediction module for token
identification and an approximate module for recovery of the frozen tokens. By
jointly optimizing with the backbone through computation budget-aware
end-to-end training, ToFe can adaptively process the necessary tokens at each
block, thereby reducing computational cost while maintaining performance.
Extensive experiments demonstrate that ToFe reduces the computational cost of
LV-ViT model by 50% with less than 2% drop in Top-1 accuracy, achieving a
better trade-off between performance and complexity compared to
state-of-the-art methods.

</details>


### [34] [MAN++: Scaling Momentum Auxiliary Network for Supervised Local Learning in Vision Tasks](https://arxiv.org/abs/2507.16279)
*Junhao Su,Feiyu Zhu,Hengyu Shi,Tianyang Han,Yurui Qiu,Junfeng Luo,Xiaoming Wei,Jialin Gao*

Main category: cs.CV

TL;DR: 该论文提出了MAN++方法，通过改进本地监督学习，有效减少GPU显存消耗，并实现了接近端到端训练的性能。


<details>
  <summary>Details</summary>
Motivation: 端到端反向传播虽常用，但存在更新锁、内存消耗高、生物合理性差等问题。现有本地监督学习虽能缓解，但信息割裂导致性能下降，无法替代端到端训练。

Method: 作者将网络划分为多个块，每块用独立辅助网络训练，提出MAN++方法，用相邻块参数的指数滑动平均（EMA）作为动态通信机制，并引入可学习的缩放偏置以平衡本地特征差异。

Result: 在图像分类、目标检测、图像分割等任务和多种架构上验证MAN++，其性能接近端到端训练，但GPU显存使用大幅减少。

Conclusion: MAN++有效提升本地监督学习效果，以低显存成本实现端到端水平的性能，是替代传统训练方法的有前景方案。

Abstract: Deep learning typically relies on end-to-end backpropagation for training, a
method that inherently suffers from issues such as update locking during
parameter optimization, high GPU memory consumption, and a lack of biological
plausibility. In contrast, supervised local learning seeks to mitigate these
challenges by partitioning the network into multiple local blocks and designing
independent auxiliary networks to update each block separately. However,
because gradients are propagated solely within individual local blocks,
performance degradation occurs, preventing supervised local learning from
supplanting end-to-end backpropagation. To address these limitations and
facilitate inter-block information flow, we propose the Momentum Auxiliary
Network++ (MAN++). MAN++ introduces a dynamic interaction mechanism by
employing the Exponential Moving Average (EMA) of parameters from adjacent
blocks to enhance communication across the network. The auxiliary network,
updated via EMA, effectively bridges the information gap between blocks.
Notably, we observed that directly applying EMA parameters can be suboptimal
due to feature discrepancies between local blocks. To resolve this issue, we
introduce a learnable scaling bias that balances feature differences, thereby
further improving performance. We validate MAN++ through extensive experiments
on tasks that include image classification, object detection, and image
segmentation, utilizing multiple network architectures. The experimental
results demonstrate that MAN++ achieves performance comparable to end-to-end
training while significantly reducing GPU memory usage. Consequently, MAN++
offers a novel perspective for supervised local learning and presents a viable
alternative to conventional training methods.

</details>


### [35] [Beyond Label Semantics: Language-Guided Action Anatomy for Few-shot Action Recognition](https://arxiv.org/abs/2507.16287)
*Zefeng Qian,Xincheng Yao,Yifei Huang,Chongyang Zhang,Jiangyong Ying,Hong Sun*

Main category: cs.CV

TL;DR: 本论文提出了一种用于少样本动作识别（FSAR）的新框架 Language-Guided Action Anatomy（LGA），通过利用大语言模型和视觉分解技术，在极少标注的条件下提升动作识别精度，并在多个基准数据集上取得了最新最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有FSAR方法由于训练数据稀缺，难以充分捕捉动作中细微变化，仅用标签语义易遗漏动作的本质表征，亟需更丰富的信息来源提升模型泛化能力。

Method: LGA方法利用大语言模型（LLM）对动作标签进行解剖，生成原子级别（主体、动作、物体）的文本描述。同时，提出视觉解剖模块将视频动作序列细分为原子阶段。通过细粒度融合，将文本和视觉特征在原子层面集成，最终通过多模态匹配机制（视频-视频和视频-文本）实现健壮的少样本分类。

Result: 在多个FSAR基准上，LGA框架均取得了比以往方法更高的识别准确率，证明了其优越的泛化能力和性能。

Conclusion: 通过充分利用语言模型和视觉分解的信息，LGA能够更细致、全面地表征动作內涵，有效提升FSAR的识别效果，展示了文本-视觉融合在少样本视频分类领域的重要价值。

Abstract: Few-shot action recognition (FSAR) aims to classify human actions in videos
with only a small number of labeled samples per category. The scarcity of
training data has driven recent efforts to incorporate additional modalities,
particularly text. However, the subtle variations in human posture, motion
dynamics, and the object interactions that occur during different phases, are
critical inherent knowledge of actions that cannot be fully exploited by action
labels alone. In this work, we propose Language-Guided Action Anatomy (LGA), a
novel framework that goes beyond label semantics by leveraging Large Language
Models (LLMs) to dissect the essential representational characteristics hidden
beneath action labels. Guided by the prior knowledge encoded in LLM, LGA
effectively captures rich spatiotemporal cues in few-shot scenarios.
Specifically, for text, we prompt an off-the-shelf LLM to anatomize labels into
sequences of atomic action descriptions, focusing on the three core elements of
action (subject, motion, object). For videos, a Visual Anatomy Module segments
actions into atomic video phases to capture the sequential structure of
actions. A fine-grained fusion strategy then integrates textual and visual
features at the atomic level, resulting in more generalizable prototypes.
Finally, we introduce a Multimodal Matching mechanism, comprising both
video-video and video-text matching, to ensure robust few-shot classification.
Experimental results demonstrate that LGA achieves state-of-the-art performance
across multipe FSAR benchmarks.

</details>


### [36] [Dens3R: A Foundation Model for 3D Geometry Prediction](https://arxiv.org/abs/2507.16290)
*Xianze Fang,Jingnan Gao,Zhe Wang,Zhuo Chen,Xingyu Ren,Jiangjing Lyu,Qiaomu Ren,Zhonglei Yang,Xiaokang Yang,Yichao Yan,Chengfei Lyu*

Main category: cs.CV

TL;DR: Dens3R提出了一个统一的3D稠密重建基础模型，通过联合回归多种几何量（如深度、法向量和点云），显著提升了3D几何预测的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 目前3D重建方法大多只能预测单一几何量，忽视了深度、法向量、点云等几何量之间的内在联系，导致预测不一致、精度不足。因此有必要构建能耦合不同几何特征、实现联合预测的统一框架。

Method: 提出Dens3R模型，采用两阶段训练框架，逐步构建泛化且本质不变的点云表示。模型拥有轻量级共享编码器-解码器结构，并引入位置插值的旋转位置编码，增强高分辨输入的表征能力和鲁棒性。整合图像对匹配特征和本质不变性建模，实现了对多种几何量的高精度回归，并设计后处理流程支持多视角的一致推理。

Result: 在多种稠密3D预测任务上，Dens3R展现出优异表现，显著优于现有方法，验证了其预测多种几何量的一致性和准确性。

Conclusion: Dens3R不仅在实验中表现出色，还显示出在更广泛3D应用场景下的潜力，是更具实际应用价值的统一3D基础模型。

Abstract: Recent advances in dense 3D reconstruction have led to significant progress,
yet achieving accurate unified geometric prediction remains a major challenge.
Most existing methods are limited to predicting a single geometry quantity from
input images. However, geometric quantities such as depth, surface normals, and
point maps are inherently correlated, and estimating them in isolation often
fails to ensure consistency, thereby limiting both accuracy and practical
applicability. This motivates us to explore a unified framework that explicitly
models the structural coupling among different geometric properties to enable
joint regression. In this paper, we present Dens3R, a 3D foundation model
designed for joint geometric dense prediction and adaptable to a wide range of
downstream tasks. Dens3R adopts a two-stage training framework to progressively
build a pointmap representation that is both generalizable and intrinsically
invariant. Specifically, we design a lightweight shared encoder-decoder
backbone and introduce position-interpolated rotary positional encoding to
maintain expressive power while enhancing robustness to high-resolution inputs.
By integrating image-pair matching features with intrinsic invariance modeling,
Dens3R accurately regresses multiple geometric quantities such as surface
normals and depth, achieving consistent geometry perception from single-view to
multi-view inputs. Additionally, we propose a post-processing pipeline that
supports geometrically consistent multi-view inference. Extensive experiments
demonstrate the superior performance of Dens3R across various dense 3D
prediction tasks and highlight its potential for broader applications.

</details>


### [37] [MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation](https://arxiv.org/abs/2507.16310)
*Yanchen Liu,Yanan Sun,Zhening Xing,Junyao Gao,Kai Chen,Wenjie Pei*

Main category: cs.CV

TL;DR: MotionShot是一种无需训练的新框架，可以精细地将参考物体的运动迁移到与其外观或结构差异较大的目标物体上，实现高保真、连贯的动作转移。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频方法在遇到参考物体和目标物体结构或外观差异较大时，难以流畅地实现运动传递，因此需要新的方法突破这一瓶颈。

Method: MotionShot框架首先通过语义特征匹配来保证参考物体与目标物体的高层对齐，接着通过形状重定向完成低层形态对齐，并通过引入时序注意力机制对动作进行编码，实现动作的连贯迁移。整个框架无需训练即可应用。

Result: 通过大量实验，证实了MotionShot在外观和结构差异大的物体间，也能实现高保真和连贯的动作迁移效果。

Conclusion: MotionShot有效解决了传统方法因参考和目标物体差异大导致运动迁移不连贯的问题，为复杂动作迁移提供了一种通用且高效的解决思路。

Abstract: Existing text-to-video methods struggle to transfer motion smoothly from a
reference object to a target object with significant differences in appearance
or structure between them. To address this challenge, we introduce MotionShot,
a training-free framework capable of parsing reference-target correspondences
in a fine-grained manner, thereby achieving high-fidelity motion transfer while
preserving coherence in appearance. To be specific, MotionShot first performs
semantic feature matching to ensure high-level alignments between the reference
and target objects. It then further establishes low-level morphological
alignments through reference-to-target shape retargeting. By encoding motion
with temporal attention, our MotionShot can coherently transfer motion across
objects, even in the presence of significant appearance and structure
disparities, demonstrated by extensive experiments. The project page is
available at: https://motionshot.github.io/.

</details>


### [38] [M-SpecGene: Generalized Foundation Model for RGBT Multispectral Vision](https://arxiv.org/abs/2507.16318)
*Kailai Zhou,Fuqiang Yang,Shixian Wang,Bihan Wen,Chongde Zi,Linsen Chen,Qiu Shen,Xun Cao*

Main category: cs.CV

TL;DR: 本文提出了一种通用的RGB-T多光谱基础模型M-SpecGene，旨在通过自监督学习从大规模数据中学习模态无关的表征，并在多个任务和数据集上实现了泛化。


<details>
  <summary>Details</summary>
Motivation: 当前RGB-T任务大多采用针对具体任务的定制化模型，存在人为归纳偏差、模态偏差及数据瓶颈等问题，因此迫切需要一种通用且无需手工设计的多光谱融合模型。

Method: 作者构建了M-SpecGene基础模型，通过自监督方式在大规模、广覆盖的数据上学习跨模态表征。针对RGB-T数据的信息不均衡问题，提出了跨模态结构稀疏性（CMSS）指标，以量化双模态信息密度，并基于高斯混合模型（GMM）制定了逐步掩码（GMM-CMSS）预训练策略，由易到难聚焦于对象中心进行表征学习。

Result: 实验验证了M-SpecGene在RGB-T相关的四个下游任务、十一组数据集上的泛化能力，显示出广泛适应性和有效性。

Conclusion: M-SpecGene为多光谱融合提供了新的统一范式，有效减少了以往手工定制方法的局限性，并在多项任务中展现了良好性能，对RGB-T视觉研究具有重要推动作用。

Abstract: RGB-Thermal (RGBT) multispectral vision is essential for robust perception in
complex environments. Most RGBT tasks follow a case-by-case research paradigm,
relying on manually customized models to learn task-oriented representations.
Nevertheless, this paradigm is inherently constrained by artificial inductive
bias, modality bias, and data bottleneck. To address these limitations, we make
the initial attempt to build a Generalized RGBT MultiSpectral foundation model
(M-SpecGene), which aims to learn modality-invariant representations from
large-scale broad data in a self-supervised manner. M-SpecGene provides new
insights into multispectral fusion and integrates prior case-by-case studies
into a unified paradigm. Considering the unique characteristic of information
imbalance in RGBT data, we introduce the Cross-Modality Structural Sparsity
(CMSS) metric to quantify the information density across two modalities. Then
we develop the GMM-CMSS progressive masking strategy to facilitate a flexible,
easy-to-hard, and object-centric pre-training process. Comprehensive
experiments validate M-SpecGene's generalizability across eleven datasets for
four RGBT downstream tasks. The code will be available at
https://github.com/CalayZhou/M-SpecGene.

</details>


### [39] [Scene Text Detection and Recognition "in light of" Challenging Environmental Conditions using Aria Glasses Egocentric Vision Cameras](https://arxiv.org/abs/2507.16330)
*Joseph De Mathia,Carlos Francisco Moreno-García*

Main category: cs.CV

TL;DR: 该论文利用Meta的Project Aria智能眼镜，评估了现实环境下不同变量对当前场景文本检测与识别（STDR）算法性能的影响，并提出数据集与优化方法。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴技术的发展，STDR在增强现实等应用中的作用日益突出。论文旨在系统分析环境变量（如光照、距离、分辨率）对STDR实际表现的影响，为更加智能和人性化的AR系统奠定基础。

Method: 作者基于Meta Project Aria智能眼镜采集受控环境下的新型数据集，测试了两条OCR流程（EAST+CRNN与EAST+PyTesseract）。实验系统性分析了光照、距离和分辨率对识别精度的影响，并尝试图像升采样、眼动追踪等技巧以优化性能。

Result: 实验表明，分辨率和距离对识别准确率影响显著，光照影响不稳定。通过图像升采样，字符错误率（CER）由0.65下降至0.48。眼动追踪可提升处理效率。

Conclusion: 论文为STDR算法在真实环境下的表现提供基准，并展示了适应性、以用户为中心的AR系统的方向。结果有助于今后开发鲁棒、情境感知的文本识别技术，支持辅助检测、资产检查、营养分析等应用。

Abstract: In an era where wearable technology is reshaping applications, Scene Text
Detection and Recognition (STDR) becomes a straightforward choice through the
lens of egocentric vision. Leveraging Meta's Project Aria smart glasses, this
paper investigates how environmental variables, such as lighting, distance, and
resolution, affect the performance of state-of-the-art STDR algorithms in
real-world scenarios. We introduce a novel, custom-built dataset captured under
controlled conditions and evaluate two OCR pipelines: EAST with CRNN, and EAST
with PyTesseract. Our findings reveal that resolution and distance
significantly influence recognition accuracy, while lighting plays a less
predictable role. Notably, image upscaling emerged as a key pre-processing
technique, reducing Character Error Rate (CER) from 0.65 to 0.48. We further
demonstrate the potential of integrating eye-gaze tracking to optimise
processing efficiency by focusing on user attention zones. This work not only
benchmarks STDR performance under realistic conditions but also lays the
groundwork for adaptive, user-aware AR systems. Our contributions aim to
inspire future research in robust, context-sensitive text recognition for
assistive and research-oriented applications, such as asset inspection and
nutrition analysis. The code is available at
https://github.com/josepDe/Project_Aria_STR.

</details>


### [40] [One Polyp Identifies All: One-Shot Polyp Segmentation with SAM via Cascaded Priors and Iterative Prompt Evolution](https://arxiv.org/abs/2507.16337)
*Xinyu Mao,Xiaohan Xing,Fei Meng,Jianbang Liu,Fan Bai,Qiang Nie,Max Meng*

Main category: cs.CV

TL;DR: 本文提出OP-SAM框架，利用仅需单一注释图像即可在无需大量人工标注的前提下准确分割息肉，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的息肉分割依赖大量标注数据，且容易受到形态变异和领域迁移影响，需要频繁重训，且边界标注耗时耗力。新一代大模型如SAM虽泛化性强但需手动输入prompts，降低了自动化水平。

Method: 提出OP-SAM，一种基于SAM的one-shot息肉分割框架：1）使用CPG进行相关性先验生成，实现语义标签转移；2）引入SPF多尺度融合机制，适应息肉大小变化并过滤噪音；3）采用EPE机制对prompts进行迭代优化，逐步提升分割效果。所有prompts由单张带标签图片自动生成，无需人工手动干预。

Result: 在五个数据集上进行了大量实验证明方法有效。特别是在Kvasir数据集上，OP-SAM的IoU达76.93%，比最佳现有方法提高11.44%。

Conclusion: OP-SAM实现了通用且高精度的息肉分割，有效减少了人工标签需求，推动了医学图像分割方法向高自动化、高泛化方向发展。

Abstract: Polyp segmentation is vital for early colorectal cancer detection, yet
traditional fully supervised methods struggle with morphological variability
and domain shifts, requiring frequent retraining. Additionally, reliance on
large-scale annotations is a major bottleneck due to the time-consuming and
error-prone nature of polyp boundary labeling. Recently, vision foundation
models like Segment Anything Model (SAM) have demonstrated strong
generalizability and fine-grained boundary detection with sparse prompts,
effectively addressing key polyp segmentation challenges. However, SAM's
prompt-dependent nature limits automation in medical applications, since
manually inputting prompts for each image is labor-intensive and
time-consuming. We propose OP-SAM, a One-shot Polyp segmentation framework
based on SAM that automatically generates prompts from a single annotated
image, ensuring accurate and generalizable segmentation without additional
annotation burdens. Our method introduces Correlation-based Prior Generation
(CPG) for semantic label transfer and Scale-cascaded Prior Fusion (SPF) to
adapt to polyp size variations as well as filter out noisy transfers. Instead
of dumping all prompts at once, we devise Euclidean Prompt Evolution (EPE) for
iterative prompt refinement, progressively enhancing segmentation quality.
Extensive evaluations across five datasets validate OP-SAM's effectiveness.
Notably, on Kvasir, it achieves 76.93% IoU, surpassing the state-of-the-art by
11.44%.

</details>


### [41] [Navigating Large-Pose Challenge for High-Fidelity Face Reenactment with Video Diffusion Model](https://arxiv.org/abs/2507.16341)
*Mingtao Guo,Guanyu Xing,Yanci Zhang,Yanli Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的视频人脸重演框架（FRVD），能在大姿态变化下高质量地将源图像与驱动视频的人脸动作对齐，生成自然流畅的讲述人脸视频，且在姿态准确性、身份保持和可视质感上超越以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于显式或隐式关键点的方法，在处理大幅度人脸姿态变化时会出现变形或难以精准对齐，难以实现高质量的人脸重演。作者希望解决大姿态变化下的失真、扭曲和身份迁移困难问题。

Method: 方法主要包括三步：1）首先用运动提取器从源图像和驱动视频隐式提取精细的人脸关键点以对齐动作，并通过扭曲模块完成对齐；2）针对扭曲带来的质量下降，引入Warping Feature Mapper (WFM) 将扭曲后的源图像特征映射到预训练的图像到视频（I2V）模型的运动感知隐空间；3）利用I2V模型学到的大规模人脸动态先验，实现有效的扭曲纠正和时间连贯的视频生成。

Result: 大量实验结果显示，FRVD在人脸姿态准确性、身份信息保持和视频视觉质量等方面，特别是在大幅度姿态变化场景下，均优于现有各种人脸重演方法。

Conclusion: FRVD为实现高保真、大姿态变化下的人脸重演提供了新思路，显著提升了重演的真实度和稳定性，有望推动相关应用的发展。

Abstract: Face reenactment aims to generate realistic talking head videos by
transferring motion from a driving video to a static source image while
preserving the source identity. Although existing methods based on either
implicit or explicit keypoints have shown promise, they struggle with large
pose variations due to warping artifacts or the limitations of coarse facial
landmarks. In this paper, we present the Face Reenactment Video Diffusion model
(FRVD), a novel framework for high-fidelity face reenactment under large pose
changes. Our method first employs a motion extractor to extract implicit facial
keypoints from the source and driving images to represent fine-grained motion
and to perform motion alignment through a warping module. To address the
degradation introduced by warping, we introduce a Warping Feature Mapper (WFM)
that maps the warped source image into the motion-aware latent space of a
pretrained image-to-video (I2V) model. This latent space encodes rich priors of
facial dynamics learned from large-scale video data, enabling effective warping
correction and enhancing temporal coherence. Extensive experiments show that
FRVD achieves superior performance over existing methods in terms of pose
accuracy, identity preservation, and visual quality, especially in challenging
scenarios with extreme pose variations.

</details>


### [42] [Mamba-OTR: a Mamba-based Solution for Online Take and Release Detection from Untrimmed Egocentric Video](https://arxiv.org/abs/2507.16342)
*Alessandro Sebastiano Catinello,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 本文针对在未剪辑的第一视角视频中在线检测物体的拿取与释放（OTR）问题，提出了Mamba-OTR模型，实现了更高效与准确的实时检测。


<details>
  <summary>Details</summary>
Motivation: OTR任务具有标签极度不平衡、正样本稀疏以及需要精确时序预测等挑战，且实际应用要求计算效率高。现有方法难以在保证性能的同时高效运行，因此需要有新模型和新的训练方案来应对上述问题。

Method: 提出了基于Mamba架构的Mamba-OTR模型，利用时序递归来增强推理能力，并在训练阶段采用短视频片段训练，同时引入focal loss和新型正则化方案来缓解标签不均衡问题，使模型预测更好地与评估指标对齐。

Result: 在EPIC-KITCHENS-100数据集上，Mamba-OTR在滑动窗口模式下mp-mAP达到45.48，流式模式下为43.35，均大幅优于baseline模型（如vanilla transformer和vanilla Mamba），特别适用于全长视频或高帧率序列，即便训练时只用短片段。

Conclusion: Mamba-OTR在OTR任务中表现出优越的准确率与计算效率，为相关领域提供了强大基线；代码将公开推动后续研究发展。

Abstract: This work tackles the problem of Online detection of Take and Release (OTR)
of an object in untrimmed egocentric videos. This task is challenging due to
severe label imbalance, with temporally sparse positive annotations, and the
need for precise temporal predictions. Furthermore, methods need to be
computationally efficient in order to be deployed in real-world online
settings. To address these challenges, we propose Mamba-OTR, a model based on
the Mamba architecture. Mamba-OTR is designed to exploit temporal recurrence
during inference while being trained on short video clips. To address label
imbalance, our training pipeline incorporates the focal loss and a novel
regularization scheme that aligns model predictions with the evaluation metric.
Extensive experiments on EPIC-KITCHENS-100, the comparisons with
transformer-based approach, and the evaluation of different training and test
schemes demonstrate the superiority of Mamba-OTR in both accuracy and
efficiency. These finding are particularly evident when evaluating full-length
videos or high frame-rate sequences, even when trained on short video snippets
for computational convenience. The proposed Mamba-OTR achieves a noteworthy
mp-mAP of 45.48 when operating in a sliding-window fashion, and 43.35 in
streaming mode, versus the 20.32 of a vanilla transformer and 25.16 of a
vanilla Mamba, thus providing a strong baseline for OTR. We will publicly
release the source code of Mamba-OTR to support future research.

</details>


### [43] [LPTR-AFLNet: Lightweight Integrated Chinese License Plate Rectification and Recognition Network](https://arxiv.org/abs/2507.16362)
*Guangzhu Xu,Pengcheng Zuo,Zhi Ke,Bangjun Lei*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级统一网络LPTR-AFLNet，可高效矫正及识别中国车牌。该方法在复杂环境和边缘设备上依然表现优越，具备实时部署能力。


<details>
  <summary>Details</summary>
Motivation: 在实际复杂环境中，车牌识别由于拍摄角度造成的透视畸变及单双排车牌矫正带来很大挑战，且边缘设备算力有限，对端到端低复杂度方案需求迫切。

Method: 提出LPTR-AFLNet网络，将透视变换矫正模块（PTR）与优化后的AFLNet识别网络结合。识别结果作为矫正的弱监督信号，通过改进注意力模块减小相似字符混淆，引入Focal Loss改善类别不均衡。

Result: LPTR-AFLNet在透视畸变矫正和单双排车牌识别上表现优异，各类复杂场景下均维持高识别率。在中低端GPU平台上推理时间小于10毫秒。

Conclusion: 本文方法在准确性、运行效率和适应性方面均具备实际应用价值，为车牌识别提供高效、低算力消耗解决方案，适合边缘场景部署。

Abstract: Chinese License Plate Recognition (CLPR) faces numerous challenges in
unconstrained and complex environments, particularly due to perspective
distortions caused by various shooting angles and the correction of single-line
and double-line license plates. Considering the limited computational resources
of edge devices, developing a low-complexity, end-to-end integrated network for
both correction and recognition is essential for achieving real-time and
efficient deployment. In this work, we propose a lightweight, unified network
named LPTR-AFLNet for correcting and recognizing Chinese license plates, which
combines a perspective transformation correction module (PTR) with an optimized
license plate recognition network, AFLNet. The network leverages the
recognition output as a weak supervisory signal to effectively guide the
correction process, ensuring accurate perspective distortion correction. To
enhance recognition accuracy, we introduce several improvements to LPRNet,
including an improved attention module to reduce confusion among similar
characters and the use of Focal Loss to address class imbalance during
training. Experimental results demonstrate the exceptional performance of
LPTR-AFLNet in rectifying perspective distortion and recognizing double-line
license plate images, maintaining high recognition accuracy across various
challenging scenarios. Moreover, on lower-mid-range GPUs platform, the method
runs in less than 10 milliseconds, indicating its practical efficiency and
broad applicability.

</details>


### [44] [STAR: A Benchmark for Astronomical Star Fields Super-Resolution](https://arxiv.org/abs/2507.16385)
*Kuo-Cheng Wu,Guohang Zhuang,Jinyang Huang,Xiang Zhang,Wanli Ouyang,Yan Lu*

Main category: cs.CV

TL;DR: 本文提出了一个新型天文超分辨率数据集（STAR）以及相应的模型和评估工具，用于提升天文图像的超分辨率效果。


<details>
  <summary>Details</summary>
Motivation: 现有的天文超分辨率（ASR）数据集存在光通量不一致、数据裁剪导致场景不完整，以及数据多样性不足等问题，限制了ASR模型在实际天文观测中的应用和发展。

Method: 作者构建了一个大规模且光通量一致的天文超分辨率数据集STAR，包含54738对覆盖广阔天区的图像对。低分辨率图像通过物理一致的流程合成，确保与高分辨率哈勃望远镜图像的光通量一致。作者还提出了新颖的Flux Error评估指标，用于更贴近物理真实性地评价SR方法。此外设计了基于光通量不变性的超分辨率模型FISR，有效提升了重建效果。

Result: 基于STAR数据集和Flux Error指标，提出的FISR模型在光通量一致性的新指标上，相较当前多种SR方法获得了24.84%的提升，表明其在天体物理领域的优越性。

Conclusion: STAR数据集和FISR模型有效推动了天文超分辨率领域发展，提升了重建准确性，并为后续研究提供了高质量基准和评估工具。作者还开源了数据和代码，促进社区进步。

Abstract: Super-resolution (SR) advances astronomical imaging by enabling
cost-effective high-resolution capture, crucial for detecting faraway celestial
objects and precise structural analysis. However, existing datasets for
astronomical SR (ASR) exhibit three critical limitations: flux inconsistency,
object-crop setting, and insufficient data diversity, significantly impeding
ASR development. We propose STAR, a large-scale astronomical SR dataset
containing 54,738 flux-consistent star field image pairs covering wide
celestial regions. These pairs combine Hubble Space Telescope high-resolution
observations with physically faithful low-resolution counterparts generated
through a flux-preserving data generation pipeline, enabling systematic
development of field-level ASR models. To further empower the ASR community,
STAR provides a novel Flux Error (FE) to evaluate SR models in physical view.
Leveraging this benchmark, we propose a Flux-Invariant Super Resolution (FISR)
model that could accurately infer the flux-consistent high-resolution images
from input photometry, suppressing several SR state-of-the-art methods by
24.84% on a novel designed flux consistency metric, showing the priority of our
method for astrophysics. Extensive experiments demonstrate the effectiveness of
our proposed method and the value of our dataset. Code and models are available
at https://github.com/GuoCheng12/STAR.

</details>


### [45] [From Flat to Round: Redefining Brain Decoding with Surface-Based fMRI and Cortex Structure](https://arxiv.org/abs/2507.16389)
*Sijin Yu,Zijiao Chen,Wenxuan Wu,Shengxian Chen,Zhongliang Liu,Jingxin Nie,Xiaofen Xing,Xiangmin Xu,Xin Zhang*

Main category: cs.CV

TL;DR: 本文通过引入球面分词器、结合结构MRI数据及正样本mixup策略，提升了基于fMRI脑活动视觉重建的准确性和可解释性。新方法在重建效果上超越了现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于fMRI的视觉刺激重建方法忽视了大脑结构-功能关系及个体解剖差异，导致空间信息损失和泛化能力不足。论文旨在解决这些核心问题。

Method: 1）提出球面分词器，将fMRI信号映射为皮层表面上的空间连续2D球面数据；2）整合结构MRI，实现个体化结构信息的建模；3）采用正样本mixup，充分利用同一样本的多次fMRI扫描。

Result: 新方法在多个实验中展现了比现有SOTA方法更高的视觉刺激重建精度，并具备更好的个体泛化能力和生物学解释性。

Conclusion: 通过结构与功能数据结合及创新的分词与混合方法，该研究显著提升了大脑视觉信息解码的效果和解释力，为脑-机接口及认知神经科学提供了新工具。

Abstract: Reconstructing visual stimuli from human brain activity (e.g., fMRI) bridges
neuroscience and computer vision by decoding neural representations. However,
existing methods often overlook critical brain structure-function
relationships, flattening spatial information and neglecting individual
anatomical variations. To address these issues, we propose (1) a novel sphere
tokenizer that explicitly models fMRI signals as spatially coherent 2D
spherical data on the cortical surface; (2) integration of structural MRI
(sMRI) data, enabling personalized encoding of individual anatomical
variations; and (3) a positive-sample mixup strategy for efficiently leveraging
multiple fMRI scans associated with the same visual stimulus. Collectively,
these innovations enhance reconstruction accuracy, biological interpretability,
and generalizability across individuals. Experiments demonstrate superior
reconstruction performance compared to SOTA methods, highlighting the
effectiveness and interpretability of our biologically informed approach.

</details>


### [46] [Are Foundation Models All You Need for Zero-shot Face Presentation Attack Detection?](https://arxiv.org/abs/2507.16393)
*Lazaro Janier Gonzalez-Sole,Juan E. Tapia,Christoph Busch*

Main category: cs.CV

TL;DR: 本文提出利用基础模型进行零样本人脸攻击检测（PAD），提升在未知攻击工具和数据库下的泛化能力，显著优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 常规基于深度学习的人脸攻击检测方法在未知攻击类型和数据库上的泛化性能不足，且对大量训练数据有依赖。本研究旨在解决攻击泛化难题，提升模型在实际复杂场景下的安全性。

Method: 作者首先评估了基础模型在现有和具有挑战性的实验场景中的有效性和泛化能力，随后提出了一个简洁且高效的零样本PAD框架，使用未见过的攻击进行测试。

Result: 实验结果显示，所用基础模型无需专门使用攻击样本进行训练即可在复杂场景下获得优异性能，其中表现最佳的模型在SiW-Mv2数据库上的leave-one-out协议测试中，明显超过当前最优方法。

Conclusion: 基础模型在零样本人脸攻击检测中表现出极佳的泛化能力，是应对新型攻击威胁的有效工具，为后续研究和实际系统部署提供了新思路。

Abstract: Although face recognition systems have undergone an impressive evolution in
the last decade, these technologies are vulnerable to attack presentations
(AP). These attacks are mostly easy to create and, by executing them against
the system's capture device, the malicious actor can impersonate an authorised
subject and thus gain access to the latter's information (e.g., financial
transactions). To protect facial recognition schemes against presentation
attacks, state-of-the-art deep learning presentation attack detection (PAD)
approaches require a large amount of data to produce reliable detection
performances and even then, they decrease their performance for unknown
presentation attack instruments (PAI) or database (information not seen during
training), i.e. they lack generalisability. To mitigate the above problems,
this paper focuses on zero-shot PAD. To do so, we first assess the
effectiveness and generalisability of foundation models in established and
challenging experimental scenarios and then propose a simple but effective
framework for zero-shot PAD. Experimental results show that these models are
able to achieve performance in difficult scenarios with minimal effort of the
more advanced PAD mechanisms, whose weights were optimised mainly with training
sets that included APs and bona fide presentations. The top-performing
foundation model outperforms by a margin the best from the state of the art
observed with the leaving-one-out protocol on the SiW-Mv2 database, which
contains challenging unknown 2D and 3D attacks

</details>


### [47] [ADCD-Net: Robust Document Image Forgery Localization via Adaptive DCT Feature and Hierarchical Content Disentanglement](https://arxiv.org/abs/2507.16397)
*Kahim Wong,Jicheng Zhou,Haiwei Wu,Yain-Whar Si,Jiantao Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的文档图像伪造定位模型ADCD-Net，能够更强健地检测和定位文档图像中的伪造区域，尤其在各种失真条件下也具有优越表现。


<details>
  <summary>Details</summary>
Motivation: 当前图像编辑工具的发展使伪造敏感文档变得容易，而现有的自然图像伪造检测方法处理文档时效果有限，主要由于文档背景和结构化文本使伪造区域难以分辨；而现有针对文档的方法又不够鲁棒，无法适应多种文档退化和失真。

Method: 提出ADCD-Net模型，结合了RGB与DCT伪造痕迹，并融合文档图像的关键特征。具体方法包括：1）自适应调整DCT特征的贡献，增强对块错位和各种失真（缩放、裁剪等）的应对能力；2）层次化内容解耦，缓解文本与背景差异，提高定位效果；3）利用清洁背景的原型信息，提升伪造定位的准确性与鲁棒性。

Result: ADCD-Net在五种常见失真条件下平均性能超过现有主流方法20.79%，在定位伪造区域方面表现出了明显的提升和鲁棒性优势。

Conclusion: ADCD-Net极大提升了文档伪造检测和定位的鲁棒性与准确性，为实际敏感文档安全应用提供了更可靠的技术支撑。代码已开源，可复现。

Abstract: The advancement of image editing tools has enabled malicious manipulation of
sensitive document images, underscoring the need for robust document image
forgery detection.Though forgery detectors for natural images have been
extensively studied, they struggle with document images, as the tampered
regions can be seamlessly blended into the uniform document background (BG) and
structured text. On the other hand, existing document-specific methods lack
sufficient robustness against various degradations, which limits their
practical deployment. This paper presents ADCD-Net, a robust document forgery
localization model that adaptively leverages the RGB/DCT forensic traces and
integrates key characteristics of document images. Specifically, to address the
DCT traces' sensitivity to block misalignment, we adaptively modulate the DCT
feature contribution based on a predicted alignment score, resulting in much
improved resilience to various distortions, including resizing and cropping.
Also, a hierarchical content disentanglement approach is proposed to boost the
localization performance via mitigating the text-BG disparities. Furthermore,
noticing the predominantly pristine nature of BG regions, we construct a
pristine prototype capturing traces of untampered regions, and eventually
enhance both the localization accuracy and robustness. Our proposed ADCD-Net
demonstrates superior forgery localization performance, consistently
outperforming state-of-the-art methods by 20.79\% averaged over 5 types of
distortions. The code is available at https://github.com/KAHIMWONG/ACDC-Net.

</details>


### [48] [ReasonVQA: A Multi-hop Reasoning Benchmark with Structural Knowledge for Visual Question Answering](https://arxiv.org/abs/2507.16403)
*Thuy-Duong Tran,Trung-Kien Tran,Manfred Hauswirth,Danh Le Phuoc*

Main category: cs.CV

TL;DR: 本文提出了ReasonVQA数据集，为视觉问答（VQA）任务提供更具挑战性和规模的数据资源。


<details>
  <summary>Details</summary>
Motivation: 现有VQA数据集在多跳推理和结构化知识整合方面有所不足，限制了模型推理能力的提升。

Method: 作者基于自动化的低成本框架，将结构化百科知识整合进问题生成流程，自动生成复杂、多跳的VQA问题，构建ReasonVQA大规模数据集。

Result: 主流VQA模型在ReasonVQA上的表现不佳，显示出该数据集具有更高挑战性，对VQA模型提出更高要求。数据集规模远超以往同类需要外部知识的数据集。

Conclusion: ReasonVQA不仅能促进VQA领域进步，也为模型推理能力的提升提供了新的基准和资源。

Abstract: In this paper, we propose a new dataset, ReasonVQA, for the Visual Question
Answering (VQA) task. Our dataset is automatically integrated with structured
encyclopedic knowledge and constructed using a low-cost framework, which is
capable of generating complex, multi-hop questions. We evaluated
state-of-the-art VQA models on ReasonVQA, and the empirical results demonstrate
that ReasonVQA poses significant challenges to these models, highlighting its
potential for benchmarking and advancing the field of VQA. Additionally, our
dataset can be easily scaled with respect to input images; the current version
surpasses the largest existing datasets requiring external knowledge by more
than an order of magnitude.

</details>


### [49] [Sparse-View 3D Reconstruction: Recent Advances and Open Challenges](https://arxiv.org/abs/2507.16406)
*Tanveer Younis,Zhanglin Cheng*

Main category: cs.CV

TL;DR: 本综述论文系统梳理了稀疏视角3D重建领域的最新进展，特别聚焦神经隐式模型（如NeRF）、显式点云方法（如3D高斯投影）及混合框架，并分析了这些方法在准确性、效率、泛化性之间的权衡及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 在机器人、AR/VR和自动驾驶等现实应用中，密集拍摄图像既不便捷也不实际，导致传统方法（如SfM和MVS）因视角重叠不足而表现不佳。因此，迫切需要高效、泛化性强的稀疏视角3D重建方法。

Method: 论文综述了三大类方法：1）神经隐式模型（如NeRF和其正则化版本）；2）显式点云重建方法（如3D Gaussian Splatting）；3）结合扩散模型和视觉基础模型先验的混合框架。同时分析了几何正则化、显式形状建模及生成式推断等手段对稀疏视角下常见伪影和姿态不确定性的缓解效果。通过标准基准测试对比不同方法的准确性、效率及泛化能力。

Result: 比较分析显示，各方法在重建精度、运算效率和跨领域泛化等方面存在显著权衡，同时识别出几何正则化和结合生成模型先验对于提升稀疏视角下的重建表现有积极作用。

Conclusion: 本综述首次从统一视角综述了基于几何、神经隐式及生成扩散等稀疏视角3D重建方法，指出依赖场景先验和实时、无约束重建仍是开放难题。未来方向包括3D原生生成式先验开发及提升泛化能力和重建效率。

Abstract: Sparse-view 3D reconstruction is essential for applications in which dense
image acquisition is impractical, such as robotics, augmented/virtual reality
(AR/VR), and autonomous systems. In these settings, minimal image overlap
prevents reliable correspondence matching, causing traditional methods, such as
structure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey
reviews the latest advances in neural implicit models (e.g., NeRF and its
regularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian
Splatting), and hybrid frameworks that leverage priors from diffusion and
vision foundation models (VFMs).We analyze how geometric regularization,
explicit shape modeling, and generative inference are used to mitigate
artifacts such as floaters and pose ambiguities in sparse-view settings.
Comparative results on standard benchmarks reveal key trade-offs between the
reconstruction accuracy, efficiency, and generalization. Unlike previous
reviews, our survey provides a unified perspective on geometry-based, neural
implicit, and generative (diffusion-based) methods. We highlight the persistent
challenges in domain generalization and pose-free reconstruction and outline
future directions for developing 3D-native generative priors and achieving
real-time, unconstrained sparse-view reconstruction.

</details>


### [50] [C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving Reasoning](https://arxiv.org/abs/2507.16518)
*Xiuwei Chen,Wentao Hu,Hanhui Li,Jun Zhou,Zisheng Chen,Meng Cao,Yihan Zeng,Kui Zhang,Yu-Jie Yuan,Jianhua Han,Hang Xu,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出C2-Evo自进化框架，实现多模态大模型与数据协同进化，显著提升模型在数学推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型（MLLMs）性能提升受限于高质量视觉-语言数据集的匮乏，而这些数据集的复杂性和规模难以控制并高成本。现有的自改进方法在处理多模态数据时，常出现视觉和文本信息不匹配、数据进化与模型进化割裂的问题，影响最终训练效果。

Method: 作者提出C2-Evo框架，自动联动模型能力与数据难度的进化。流程包括：对初始数据和模型，首先通过跨模态数据进化循环生成更复杂的多模态问题（结构化子问题与动态图形组合）；之后利用数据-模型进化循环，根据模型在生成问题上的表现，自适应挑选合适问题，交替采用有监督微调与强化学习，提升模型能力。

Result: C2-Evo方法可持续提升模型及训练数据的质量，在多个数学推理基准上取得显著性能提升。

Conclusion: C2-Evo实现了模型与数据协同自进化，克服了现有多模态自改进方法中的割裂与数据错配难题，可为MLLMs带来持续、自主性能提升。

Abstract: Recent advances in multimodal large language models (MLLMs) have shown
impressive reasoning capabilities. However, further enhancing existing MLLMs
necessitates high-quality vision-language datasets with carefully curated task
complexities, which are both costly and challenging to scale. Although recent
self-improving models that iteratively refine themselves offer a feasible
solution, they still suffer from two core challenges: (i) most existing methods
augment visual or textual data separately, resulting in discrepancies in data
complexity (e.g., over-simplified diagrams paired with redundant textual
descriptions); and (ii) the evolution of data and models is also separated,
leading to scenarios where models are exposed to tasks with mismatched
difficulty levels. To address these issues, we propose C2-Evo, an automatic,
closed-loop self-improving framework that jointly evolves both training data
and model capabilities. Specifically, given a base dataset and a base model,
C2-Evo enhances them by a cross-modal data evolution loop and a data-model
evolution loop. The former loop expands the base dataset by generating complex
multimodal problems that combine structured textual sub-problems with
iteratively specified geometric diagrams, while the latter loop adaptively
selects the generated problems based on the performance of the base model, to
conduct supervised fine-tuning and reinforcement learning alternately.
Consequently, our method continuously refines its model and training data, and
consistently obtains considerable performance gains across multiple
mathematical reasoning benchmarks. Our code, models, and datasets will be
released.

</details>


### [51] [Towards Railway Domain Adaptation for LiDAR-based 3D Detection: Road-to-Rail and Sim-to-Real via SynDRA-BBox](https://arxiv.org/abs/2507.16413)
*Xavier Diaz,Gianluca D'Amico,Raul Dominguez-Sanchez,Federico Nesti,Max Ronecker,Giorgio Buttazzo*

Main category: cs.CV

TL;DR: 提出了一个名为SynDRA-BBox的合成铁路场景数据集，用于2D和3D目标检测，并通过领域自适应方法验证其在铁路感知中的有效性。


<details>
  <summary>Details</summary>
Motivation: 铁路自动化运行对环境感知有更高需求，但缺乏公开、带注释的真实数据集，制约了视觉算法的发展和验证。

Method: 构建了逼真的合成数据集SynDRA-BBox，专注于铁路场景下的2D与3D目标检测，并采用先进的半监督领域自适应方法（原用于汽车领域），实现合成数据到铁路领域的迁移和应用。

Result: 实验显示，结合合成数据和领域自适应技术后，在铁路场景3D目标检测任务上取得了有竞争力的性能。

Conclusion: 合成数据集与领域自适应方法能有效解决铁路领域数据稀缺问题，有助于推进铁路环境下的感知和自动化技术发展。

Abstract: In recent years, interest in automatic train operations has significantly
increased. To enable advanced functionalities, robust vision-based algorithms
are essential for perceiving and understanding the surrounding environment.
However, the railway sector suffers from a lack of publicly available
real-world annotated datasets, making it challenging to test and validate new
perception solutions in this domain. To address this gap, we introduce
SynDRA-BBox, a synthetic dataset designed to support object detection and other
vision-based tasks in realistic railway scenarios. To the best of our
knowledge, is the first synthetic dataset specifically tailored for 2D and 3D
object detection in the railway domain, the dataset is publicly available at
https://syndra.retis.santannapisa.it. In the presented evaluation, a
state-of-the-art semi-supervised domain adaptation method, originally developed
for automotive perception, is adapted to the railway context, enabling the
transferability of synthetic data to 3D object detection. Experimental results
demonstrate promising performance, highlighting the effectiveness of synthetic
datasets and domain adaptation techniques in advancing perception capabilities
for railway environments.

</details>


### [52] [Combined Image Data Augmentations diminish the benefits of Adaptive Label Smoothing](https://arxiv.org/abs/2507.16427)
*Georg Siedel,Ekagra Gupta,Weijia Shao,Silvia Vock,Andrey Morozov*

Main category: cs.CV

TL;DR: 本文探讨了“软增强”正则化尤其是在应用激进数据增强（如随机裁剪、随机擦除、噪声注入）时，通过减小标签置信度来改进图像分类器的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有的自适应标签平滑大多仅针对随机裁剪增强，作者希望扩展这一方法到其他类型（如随机擦除、噪声注入），并探究其在更复杂增强组合中的表现。

Method: 作者将自适应标签平滑方法从传统的随机裁剪扩展到随机擦除和噪声注入，并评估其在多种数据增强组合方法（如TrivialAugment）中的表现和局限。

Result: 当单独应用更强的随机擦除时，自适应标签平滑带来更强的正则化效果。但在包含多样化图像转换的增强方法中，标签平滑的益处消失。如果标签平滑过度，还会破坏模型对常见扰动的鲁棒性。

Conclusion: 自适应标签平滑只适用于图像转换类型有限、同质的数据增强情境，而在多样化或复杂增强中效果有限甚至有害。

Abstract: Soft augmentation regularizes the supervised learning process of image
classifiers by reducing label confidence of a training sample based on the
magnitude of random-crop augmentation applied to it. This paper extends this
adaptive label smoothing framework to other types of aggressive augmentations
beyond random-crop. Specifically, we demonstrate the effectiveness of the
method for random erasing and noise injection data augmentation. Adaptive label
smoothing permits stronger regularization via higher-intensity Random Erasing.
However, its benefits vanish when applied with a diverse range of image
transformations as in the state-of-the-art TrivialAugment method, and excessive
label smoothing harms robustness to common corruptions. Our findings suggest
that adaptive label smoothing should only be applied when the training data
distribution is dominated by a limited, homogeneous set of image transformation
types.

</details>


### [53] [Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning](https://arxiv.org/abs/2507.16746)
*Ang Li,Charles Wang,Kaiyu Yue,Zikui Cai,Ollie Liu,Deqing Fu,Peng Guo,Wang Bill Zhu,Vatsal Sharan,Robin Jia,Willie Neiswanger,Furong Huang,Tom Goldstein,Micah Goldblum*

Main category: cs.CV

TL;DR: 提出了Zebra-CoT大规模多模态推理数据集，有效提升多模态模型在视觉推理链条任务上的能力，并开源数据和模型。


<details>
  <summary>Details</summary>
Motivation: 人类在解决复杂问题时常用可视化辅助，但现有多模态模型在视觉推理链（Visual CoT）上的能力较差，同时缺乏高质量视觉CoT数据，限制了相关模型的发展和训练效果。

Method: 构建了Zebra-CoT数据集，包括182,384个逻辑连贯、文本和图像交错的推理样本，涵盖几何、物理、算法、2D/3D视觉推理、机器人规划、视觉逻辑与棋类等多种任务。通过在此数据集上微调Anole-7B和Bagel-7B模型，提升模型的多模态推理能力。

Result: 在Zebra-CoT测试集上，Anole-7B模型精度提升12%；在标准多模态模型基准上提升最高达13%。对Bagel-7B微调后，能生成高质量的视觉推理链。证明Zebra-CoT数据集对多模态推理具有显著提升和促进作用。

Conclusion: Zebra-CoT为视觉推理链（Visual CoT）任务提供了优质资源与评测基础，大幅推动多模态推理模型的发展，并通过开源推动社区的相关研究与应用。

Abstract: Humans often use visual aids, for example diagrams or sketches, when solving
complex problems. Training multimodal models to do the same, known as Visual
Chain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf
visual CoT performance, which hinders reinforcement learning, and (2) the lack
of high-quality visual CoT training data. We introduce $\textbf{Zebra-CoT}$, a
diverse large-scale dataset with 182,384 samples, containing logically coherent
interleaved text-image reasoning traces. We focus on four categories of tasks
where sketching or visual reasoning is especially natural, spanning scientific
questions such as geometry, physics, and algorithms; 2D visual reasoning tasks
like visual search and jigsaw puzzles; 3D reasoning tasks including 3D
multi-hop inference, embodied and robot planning; visual logic problems and
strategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT
training corpus results in an improvement of +12% in our test-set accuracy and
yields up to +13% performance gain on standard VLM benchmark evaluations.
Fine-tuning Bagel-7B yields a model that generates high-quality interleaved
visual reasoning chains, underscoring Zebra-CoT's effectiveness for developing
multimodal reasoning abilities. We open-source our dataset and models to
support development and evaluation of visual CoT.

</details>


### [54] [Robust Noisy Pseudo-label Learning for Semi-supervised Medical Image Segmentation Using Diffusion Model](https://arxiv.org/abs/2507.16429)
*Lin Xi,Yingliang Ma,Cheng Wang,Sandra Howell,Aldo Rinaldi,Kawal S. Rhode*

Main category: cs.CV

TL;DR: 本文提出一种基于扩散模型的半监督医学图像分割方法，通过在扩散过程引入原型对比约束，提升对噪声伪标签的鲁棒性，并发布了新的公开X射线造影多目标分割数据集MOSXAV，实验证明方法优于现有主流半监督分割方法。


<details>
  <summary>Details</summary>
Motivation: 医学领域中获取像素级标注很昂贵且耗时，半监督分割希望通过少量标注和大量未标注数据实现高效精确分割，但现有方法受伪标签噪声影响，难以有效建构潜在空间的语义分布。因此，该文尝试通过创新方法提升半监督分割的可靠性和鲁棒性。

Method: 提出一种新型基于扩散模型的半监督医学图像分割框架。在扩散去噪过程中，利用原型对比一致性约束，强化潜在空间的语义标签结构，通过以类别原型为锚定点，提升模型在伪标签噪声下的预测稳定性。同时，发表多对象X射线造影分割数据集（MOSXAV），丰富评测基准。

Result: 在EndoScapes2023和MOSXAV数据集上大量实验，结果表明所提方法在半监督分割任务上超越了当前最先进的分割方法，展现了更优的性能和鲁棒性。

Conclusion: 提出的扩散模型半监督分割框架具备强大的数据效率和鲁棒性，为医学分割任务提供了更大的灵活性和应用潜力，适用于多种临床医学图像分割场景。

Abstract: Obtaining pixel-level annotations in the medical domain is both expensive and
time-consuming, often requiring close collaboration between clinical experts
and developers. Semi-supervised medical image segmentation aims to leverage
limited annotated data alongside abundant unlabeled data to achieve accurate
segmentation. However, existing semi-supervised methods often struggle to
structure semantic distributions in the latent space due to noise introduced by
pseudo-labels. In this paper, we propose a novel diffusion-based framework for
semi-supervised medical image segmentation. Our method introduces a constraint
into the latent structure of semantic labels during the denoising diffusion
process by enforcing prototype-based contrastive consistency. Rather than
explicitly delineating semantic boundaries, the model leverages class
prototypes centralized semantic representations in the latent space as anchors.
This strategy improves the robustness of dense predictions, particularly in the
presence of noisy pseudo-labels. We also introduce a new publicly available
benchmark: Multi-Object Segmentation in X-ray Angiography Videos (MOSXAV),
which provides detailed, manually annotated segmentation ground truth for
multiple anatomical structures in X-ray angiography videos. Extensive
experiments on the EndoScapes2023 and MOSXAV datasets demonstrate that our
method outperforms state-of-the-art medical image segmentation approaches under
the semi-supervised learning setting. This work presents a robust and
data-efficient diffusion model that offers enhanced flexibility and strong
potential for a wide range of clinical applications.

</details>


### [55] [VGGT-Long: Chunk it, Loop it, Align it -- Pushing VGGT's Limits on Kilometer-scale Long RGB Sequences](https://arxiv.org/abs/2507.16443)
*Kai Deng,Zexin Ti,Jiawei Xu,Jian Yang,Jin Xie*

Main category: cs.CV

TL;DR: 本文提出了一种名为VGGT-Long的新系统，实现了对公里级大规模户外场景RGB视频流的单目3D重建，突破了基础模型在内存和可拓展性上的瓶颈，无需相机标定、深度监督或模型重训练，并在多个公开数据集上与传统方法表现相当。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉基础模型虽然能力强大，但在处理超大规模RGB视频流进行3D重建时受限于显存和运算瓶颈，导致很难应用于真实世界长距离、开放场景，如自动驾驶场景。因此，亟需一种高效、可扩展的新方法来实现大规模单目3D重建。

Method: 作者提出了一种基于分块(chunk-based)处理的策略，将长序列分割为更小片段，再通过重叠对齐和轻量级回环优化，实现无需标定、无需深度监督和无需模型重训练的高效推理，突破了现有基础模型的扩展性限制。

Result: VGGT-Long在KITTI、Waymo、Virtual KITTI等数据集上运行良好，对于传统基础模型无法处理的长序列RGB流也能重建出准确、一致的3D几何信息，并实现了与传统标准方法相媲美的轨迹和重建性能。

Conclusion: VGGT-Long展示了基础模型在大规模、真实世界单目3D重建中的巨大潜力，特别适用于自动驾驶等需要处理长时间视频流的场景，实现了可扩展、高精度的3D场景重建。

Abstract: Foundation models for 3D vision have recently demonstrated remarkable
capabilities in 3D perception. However, extending these models to large-scale
RGB stream 3D reconstruction remains challenging due to memory limitations. In
this work, we propose VGGT-Long, a simple yet effective system that pushes the
limits of monocular 3D reconstruction to kilometer-scale, unbounded outdoor
environments. Our approach addresses the scalability bottlenecks of existing
models through a chunk-based processing strategy combined with overlapping
alignment and lightweight loop closure optimization. Without requiring camera
calibration, depth supervision or model retraining, VGGT-Long achieves
trajectory and reconstruction performance comparable to traditional methods. We
evaluate our method on KITTI, Waymo, and Virtual KITTI datasets. VGGT-Long not
only runs successfully on long RGB sequences where foundation models typically
fail, but also produces accurate and consistent geometry across various
conditions. Our results highlight the potential of leveraging foundation models
for scalable monocular 3D scene in real-world settings, especially for
autonomous driving scenarios. Code is available at
https://github.com/DengKaiCQ/VGGT-Long.

</details>


### [56] [DenseSR: Image Shadow Removal as Dense Prediction](https://arxiv.org/abs/2507.16472)
*Yu-Fan Lin,Chia-Ming Lee,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: 论文提出了一种新方法DenseSR，有效提升了单幅图像阴影去除的效果，特别是在复杂光照条件下，克服了以往方法容易导致细节丢失和边界模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 现有单幅图像阴影去除方法在间接光照等复杂情况下难以同时恢复阴影内部细节和保持边界清晰，导致去除结果不一致、图像模糊，影响后续应用与观感。

Method: 提出DenseSR框架，从密集预测的角度出发，结合几何-语义先验引导的深层场景理解和创新的Dense Fusion Block（DFB）解码器。DFB包含自适应内容平滑模块（ACSM）和纹理-边缘恢复模块（TBRM），分别关注内容一致性和细节及边界清晰，通过有效融合优化了特征表达。

Result: 大量实验结果表明，DenseSR在恢复效果、细节保留和边界清晰度等方面均优于目前已有的方法。

Conclusion: DenseSR框架通过深层场景理解和创新密集融合技术，显著提升了单幅图像阴影去除质量，为实际应用带来了更高的一致性与保真度。

Abstract: Shadows are a common factor degrading image quality. Single-image shadow
removal (SR), particularly under challenging indirect illumination, is hampered
by non-uniform content degradation and inherent ambiguity. Consequently,
traditional methods often fail to simultaneously recover intra-shadow details
and maintain sharp boundaries, resulting in inconsistent restoration and
blurring that negatively affect both downstream applications and the overall
viewing experience. To overcome these limitations, we propose the DenseSR,
approaching the problem from a dense prediction perspective to emphasize
restoration quality. This framework uniquely synergizes two key strategies: (1)
deep scene understanding guided by geometric-semantic priors to resolve
ambiguity and implicitly localize shadows, and (2) high-fidelity restoration
via a novel Dense Fusion Block (DFB) in the decoder. The DFB employs adaptive
component processing-using an Adaptive Content Smoothing Module (ACSM) for
consistent appearance and a Texture-Boundary Recuperation Module (TBRM) for
fine textures and sharp boundaries-thereby directly tackling the inconsistent
restoration and blurring issues. These purposefully processed components are
effectively fused, yielding an optimized feature representation preserving both
consistency and fidelity. Extensive experimental results demonstrate the merits
of our approach over existing methods. Our code can be available on
https://github$.$com/VanLinLin/DenseSR

</details>


### [57] [Survival Modeling from Whole Slide Images via Patch-Level Graph Clustering and Mixture Density Experts](https://arxiv.org/abs/2507.16476)
*Ardhendu Sekhar,Vasu Soni,Keshav Aske,Garima Jain,Pranav Jeevan,Amit Sethi*

Main category: cs.CV

TL;DR: 本文提出了一种模块化框架，用于从全视野病理切片图像（WSI）中预测癌症特异性生存期，显著提升了预测准确率，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于病理全切片图像的生存预测方法在准确率和适应多肿瘤类型上存在局限，病理图像体量大、区域异质性强，亟需新的高效特征提取与分析手段提升肿瘤预后预测能力。

Method: 方法包括四部分：1）通过分位阈值动态选择切片内与预后相关的关键区域；2）采用图引导的k-means聚类，结合空间与形态信息洞察组织异质性；3）使用注意力机制刻画簇内/簇间关系，将局部特征与全局空间语境结合；4）引入专家指导的混合密度模型（高斯混合模型）精准建模复杂生存分布。

Result: 在TCGA-KIRC（肾癌）和TCGA-LUAD（肺腺癌）两个数据集上，所提模型分别达到了优于现有技术的C-index与Brier分数（TCGA-KIRC: C-index 0.712, Brier 0.254；TCGA-LUAD: C-index 0.645, Brier 0.281），均显著优于对比方法。

Conclusion: 所提出的多模块集成方法能有效从WSI提取复杂信息并用于生存预测，具备跨肿瘤类型推广潜力，为提升临床预后评估提供新途径。

Abstract: We introduce a modular framework for predicting cancer-specific survival from
whole slide pathology images (WSIs) that significantly improves upon the
state-of-the-art accuracy. Our method integrating four key components. Firstly,
to tackle large size of WSIs, we use dynamic patch selection via quantile-based
thresholding for isolating prognostically informative tissue regions. Secondly,
we use graph-guided k-means clustering to capture phenotype-level heterogeneity
through spatial and morphological coherence. Thirdly, we use attention
mechanisms that model both intra- and inter-cluster relationships to
contextualize local features within global spatial relations between various
types of tissue compartments. Finally, we use an expert-guided mixture density
modeling for estimating complex survival distributions using Gaussian mixture
models. The proposed model achieves a concordance index of $0.712 \pm 0.028$
and Brier score of $0.254 \pm 0.018$ on TCGA-KIRC (renal cancer), and a
concordance index of $0.645 \pm 0.017$ and Brier score of $0.281 \pm 0.031$ on
TCGA-LUAD (lung adenocarcinoma). These results are significantly better than
the state-of-art and demonstrate predictive potential of the proposed method
across diverse cancer types.

</details>


### [58] [PlantSAM: An Object Detection-Driven Segmentation Pipeline for Herbarium Specimens](https://arxiv.org/abs/2507.16506)
*Youcef Sklab,Florian Castanet,Hanane Ariouat,Souhila Arib,Jean-Daniel Zucker,Eric Chenin,Edi Prifti*

Main category: cs.CV

TL;DR: 本论文提出PlantSAM自动分割流程，有效去除植物标本图像背景，提高深度学习分类准确率。


<details>
  <summary>Details</summary>
Motivation: 植物标本图像存在背景异质性，影响深度学习模型分类效果。解决背景干扰问题对于提升自动分类性能至关重要。

Method: PlantSAM结合了YOLOv10用于植物区域检测，和Segment Anything Model（SAM2）用于分割。YOLOv10输出的目标框用作SAM2分割的提示，两个模型均在标本图像数据集上微调，利用IoU和Dice系数评估分割性能。

Result: PlantSAM分割得到IoU为0.94，Dice值为0.97，均达到领域最优水平。分割后的图像用于下游分类，准确率最高提升4.36%，F1分数提高4.15%。

Conclusion: 消除背景显著提高植物标本图像分类准确性，PlantSAM为标本图像分析提供了有效、泛化的分割工具，强调背景去除的重要性。

Abstract: Deep learning-based classification of herbarium images is hampered by
background heterogeneity, which introduces noise and artifacts that can
potentially mislead models and reduce classification accuracy. Addressing these
background-related challenges is critical to improving model performance. We
introduce PlantSAM, an automated segmentation pipeline that integrates YOLOv10
for plant region detection and the Segment Anything Model (SAM2) for
segmentation. YOLOv10 generates bounding box prompts to guide SAM2, enhancing
segmentation accuracy. Both models were fine-tuned on herbarium images and
evaluated using Intersection over Union (IoU) and Dice coefficient metrics.
PlantSAM achieved state-of-the-art segmentation performance, with an IoU of
0.94 and a Dice coefficient of 0.97. Incorporating segmented images into
classification models led to consistent performance improvements across five
tested botanical traits, with accuracy gains of up to 4.36% and F1-score
improvements of 4.15%. Our findings highlight the importance of background
removal in herbarium image analysis, as it significantly enhances
classification accuracy by allowing models to focus more effectively on the
foreground plant structures.

</details>


### [59] [Spatial 3D-LLM: Exploring Spatial Awareness in 3D Vision-Language Models](https://arxiv.org/abs/2507.16524)
*Xiaoyan Wang,Zeju Li,Yifan Xu,Jiaxing Qi,Zhifei Yang,Ruifei Ma,Xiangde Liu,Chao Zhang*

Main category: cs.CV

TL;DR: 提出了一种新型的3D多模态大语言模型Spatial 3D-LLM，通过增强三维场景的空间嵌入，提高模型的空间感知能力，达到当前最佳的3D视觉-语言任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D多模态大语言模型在空间感知能力上存在不足，主要由于对3D场景表达不够丰富，仅依赖压缩场景信息或分割独立物体，难以捕捉三维空间的复杂多样性。作者旨在弥补这一短板。

Method: 提出Spatial 3D-LLM，将大语言模型与逐步空间感知机制融合，在感知视野扩展时逐步捕获更丰富的空间信息，生成带有位置丰富的3D场景嵌入。此外，设计了3D物体距离测量与3D布局编辑两个新任务，并构建了专用3D指令数据集MODEL，用以评估模型空间感知能力。

Result: Spatial 3D-LLM在多项3D视觉-语言任务上达到了当前最优，实验结果显示其逐步空间感知机制可以捕获更深层的空间信息，大幅提升了空间理解能力。

Conclusion: 该方法有效增强了3D多模态LLM的空间感知能力，对提升三维场景理解和多模态智能具有重要意义，并将代码公开促进后续研究。

Abstract: New era has unlocked exciting possibilities for extending Large Language
Models (LLMs) to tackle 3D vision-language tasks. However, most existing 3D
multimodal LLMs (MLLMs) rely on compressing holistic 3D scene information or
segmenting independent objects to perform these tasks, which limits their
spatial awareness due to insufficient representation of the richness inherent
in 3D scenes. To overcome these limitations, we propose Spatial 3D-LLM, a 3D
MLLM specifically designed to enhance spatial awareness for 3D vision-language
tasks by enriching the spatial embeddings of 3D scenes. Spatial 3D-LLM
integrates an LLM backbone with a progressive spatial awareness scheme that
progressively captures spatial information as the perception field expands,
generating location-enriched 3D scene embeddings to serve as visual prompts.
Furthermore, we introduce two novel tasks: 3D object distance measurement and
3D layout editing, and construct a 3D instruction dataset, MODEL, to evaluate
the model's spatial awareness capabilities. Experimental results demonstrate
that Spatial 3D-LLM achieves state-of-the-art performance across a wide range
of 3D vision-language tasks, revealing the improvements stemmed from our
progressive spatial awareness scheme of mining more profound spatial
information. Our code is available at
https://github.com/bjshuyuan/Spatial-3D-LLM.

</details>


### [60] [EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent Diffusion](https://arxiv.org/abs/2507.16535)
*Shang Liu,Chenjie Cao,Chaohui Yu,Wen Qian,Jing Wang,Fan Wang*

Main category: cs.CV

TL;DR: 本文提出了Aerial-Earth3D大规模航拍3D数据集和针对地理范围大规模3D生成的EarthCrafter方法，提升了大面积地表三维数据的生成质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法难以扩展到广阔地理尺度，比如建模数千平方公里的地表，这对地理建模和虚拟现实等应用有巨大需求。

Method: 1. 构建了Aerial-Earth3D数据集，包括50k个场景、4500万多视角图像，涵盖全美各地区，含姿态、深度、法线、语义分割等信息，并进行质量控制确保地形多样性。
2. 提出EarthCrafter框架，采用稀疏解耦潜变量扩散模型实现大规模3D生成。结构与纹理生成相分离：
  - 双路稀疏3D-VAE分别对高分辨率几何体素和2D纹理（2DGS）编码至紧凑潜空间，减少计算消耗。
  - 条件感知流匹配模型对潜在几何与纹理特征独立建模，可用语义、图像等多模态信息训练。

Result: EarthCrafter在大规模3D地表生成任务上表现优异，实现高质量且具有地理合理性的3D重建，并支持语义引导下的城市布局生成、无条件地形合成等多样化应用。

Conclusion: 结合Aerial-Earth3D大数据和EarthCrafter框架，为地理尺度的3D生成带来新突破，能有效支持多种实际场景，未来可应用于地图、游戏、仿真等广阔领域。

Abstract: Despite the remarkable developments achieved by recent 3D generation works,
scaling these methods to geographic extents, such as modeling thousands of
square kilometers of Earth's surface, remains an open challenge. We address
this through a dual innovation in data infrastructure and model architecture.
First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date,
consisting of 50k curated scenes (each measuring 600m x 600m) captured across
the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene
provides pose-annotated multi-view images, depth maps, normals, semantic
segmentation, and camera poses, with explicit quality control to ensure terrain
diversity. Building on this foundation, we propose EarthCrafter, a tailored
framework for large-scale 3D Earth generation via sparse-decoupled latent
diffusion. Our architecture separates structural and textural generation: 1)
Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D
Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the
costly computation suffering from vast geographic scales while preserving
critical information. 2) We propose condition-aware flow matching models
trained on mixed inputs (semantics, images, or neither) to flexibly model
latent geometry and texture features independently. Extensive experiments
demonstrate that EarthCrafter performs substantially better in extremely
large-scale generation. The framework further supports versatile applications,
from semantic-guided urban layout generation to unconditional terrain
synthesis, while maintaining geographic plausibility through our rich data
priors from Aerial-Earth3D.

</details>


### [61] [Optimization of DNN-based HSI Segmentation FPGA-based SoC for ADS: A Practical Approach](https://arxiv.org/abs/2507.16556)
*Jon Gutiérrez-Zaballa,Koldo Basterretxea,Javier Echanobe*

Main category: cs.CV

TL;DR: 本文提出了一套针对ADS（自动驾驶系统）FPGA平台、结合HSI（高光谱成像）与DNN（深度神经网络）进行图像分割处理的软硬件协同优化方案，实现了模型大幅压缩和推理加速，并保持了分割效果。


<details>
  <summary>Details</summary>
Motivation: HSI较传统灰度和RGB成像在目标物理属性刻画（如光谱反射率、同色异谱）上更准确，但其庞大的数据处理需求与深度网络高计算消耗，使得在资源受限的安全关键系统（如自动驾驶）中部署面临巨大挑战。为了满足低延迟、低资源消耗和高安全等严苛要求，需要有效的软硬件协同设计和优化。

Method: 采用算法-硬件协同设计方法，在FPGA SoC平台上对基于DNN的HSI分割处理流程进行系统优化。方法包括：1) 软件/硬件功能模块合理分布；2) 针对硬件预处理设计；3) 深度学习模型压缩（操作数与参数削减）；4) 完整预处理-推理流水线优化。

Result: 所提出的压缩技术将DNN的运算量降至原来的24.34%，参数量降至1.02%，分割精度几乎无损；总体推理速度提升2.86倍。

Conclusion: 软硬件一体协同优化能有效提升HSI-DNN处理在资源受限的自动驾驶FPGA平台中的实用性，保障推理效率和分割精度，为高安全场景下的视觉系统处理提供参考。

Abstract: The use of HSI for autonomous navigation is a promising research field aimed
at improving the accuracy and robustness of detection, tracking, and scene
understanding systems based on vision sensors. Combining advanced computer
algorithms, such as DNNs, with small-size snapshot HSI cameras enhances the
reliability of these systems. HSI overcomes intrinsic limitations of greyscale
and RGB imaging in depicting physical properties of targets, particularly
regarding spectral reflectance and metamerism. Despite promising results in
HSI-based vision developments, safety-critical systems like ADS demand strict
constraints on latency, resource consumption, and security, motivating the
shift of ML workloads to edge platforms. This involves a thorough
software/hardware co-design scheme to distribute and optimize the tasks
efficiently among the limited resources of computing platforms. With respect to
inference, the over-parameterized nature of DNNs poses significant
computational challenges for real-time on-the-edge deployment. In addition, the
intensive data preprocessing required by HSI, which is frequently overlooked,
must be carefully managed in terms of memory arrangement and inter-task
communication to enable an efficient integrated pipeline design on a SoC. This
work presents a set of optimization techniques for the practical co-design of a
DNN-based HSI segmentation processor deployed on a FPGA-based SoC targeted at
ADS, including key optimizations such as functional software/hardware task
distribution, hardware-aware preprocessing, ML model compression, and a
complete pipelined deployment. Applied compression techniques significantly
reduce the complexity of the designed DNN to 24.34% of the original operations
and to 1.02% of the original number of parameters, achieving a 2.86x speed-up
in the inference task without noticeable degradation of the segmentation
accuracy.

</details>


### [62] [Comparative validation of surgical phase recognition, instrument keypoint estimation, and instrument instance segmentation in endoscopy: Results of the PhaKIR 2024 challenge](https://arxiv.org/abs/2507.16559)
*Tobias Rueckert,David Rauber,Raphaela Maerkl,Leonard Klausmann,Suemeyye R. Yildiran,Max Gutbrod,Danilo Weber Nunes,Alvaro Fernandez Moreno,Imanol Luengo,Danail Stoyanov,Nicolas Toussaint,Enki Cho,Hyeon Bae Kim,Oh Sung Choo,Ka Young Kim,Seong Tae Kim,Gonçalo Arantes,Kehan Song,Jianjun Zhu,Junchen Xiong,Tingyi Lin,Shunsuke Kikuchi,Hiroki Matsuzaki,Atsushi Kouno,João Renato Ribeiro Manesco,João Paulo Papa,Tae-Min Choi,Tae Kyeong Jeong,Juyoun Park,Oluwatosin Alabi,Meng Wei,Tom Vercauteren,Runzhi Wu,Mengya Xu,An Wang,Long Bai,Hongliang Ren,Amine Yamlahi,Jakob Hennighausen,Lena Maier-Hein,Satoshi Kondo,Satoshi Kasai,Kousuke Hirasawa,Shu Yang,Yihui Wang,Hao Chen,Santiago Rodríguez,Nicolás Aparicio,Leonardo Manrique,Juan Camilo Lyons,Olivia Hosie,Nicolás Ayobi,Pablo Arbeláez,Yiping Li,Yasmina Al Khalil,Sahar Nasirihaghighi,Stefanie Speidel,Daniel Rueckert,Hubertus Feussner,Dirk Wilhelm,Christoph Palm*

Main category: cs.CV

TL;DR: 该论文为2024年MICCAI的EndoVis挑战赛推出了PhaKIR子任务，利用一个新构建的腹腔镜胆囊切除视频数据集，推动手术器械识别、定位与手术步骤识别的研究。


<details>
  <summary>Details</summary>
Motivation: 在机器人辅助手术等实际应用场景下，手术器械的识别和定位对训练与辅助系统等非常重要。但在现实条件下鲁棒性不足，现有数据集也缺乏融合手术步骤上下文和时序信息的能力。

Method: 主办方构建了一个包含13例手术、来自3家医院、带有统一标注（手术阶段、器械关键点、实例分割）的新视频数据集，组织了相关算法挑战赛，支持任务间和全程时序研究。

Result: 论文按照BIAS准则报告了本次挑战的各类结果，并介绍了参赛方法、表现及相关发现。

Conclusion: PhaKIR子挑战为RAMIS领域提供了一个融合时序和上下文的基准数据集，将推动手术场景理解方向相关算法发展，对后续研究具有很高的资源价值。

Abstract: Reliable recognition and localization of surgical instruments in endoscopic
video recordings are foundational for a wide range of applications in computer-
and robot-assisted minimally invasive surgery (RAMIS), including surgical
training, skill assessment, and autonomous assistance. However, robust
performance under real-world conditions remains a significant challenge.
Incorporating surgical context - such as the current procedural phase - has
emerged as a promising strategy to improve robustness and interpretability.
  To address these challenges, we organized the Surgical Procedure Phase,
Keypoint, and Instrument Recognition (PhaKIR) sub-challenge as part of the
Endoscopic Vision (EndoVis) challenge at MICCAI 2024. We introduced a novel,
multi-center dataset comprising thirteen full-length laparoscopic
cholecystectomy videos collected from three distinct medical institutions, with
unified annotations for three interrelated tasks: surgical phase recognition,
instrument keypoint estimation, and instrument instance segmentation. Unlike
existing datasets, ours enables joint investigation of instrument localization
and procedural context within the same data while supporting the integration of
temporal information across entire procedures.
  We report results and findings in accordance with the BIAS guidelines for
biomedical image analysis challenges. The PhaKIR sub-challenge advances the
field by providing a unique benchmark for developing temporally aware,
context-driven methods in RAMIS and offers a high-quality resource to support
future research in surgical scene understanding.

</details>


### [63] [A Multimodal Deviation Perceiving Framework for Weakly-Supervised Temporal Forgery Localization](https://arxiv.org/abs/2507.16596)
*Wenbo Xu,Junyan Wu,Wei Lu,Xiangyang Luo,Qian Wang*

Main category: cs.CV

TL;DR: 本文提出了一种多模态偏差感知框架（MDP），用于仅基于视频级标注下的深度伪造时间局部检测，兼具高效与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造取证方法通常作为分类问题或时间伪造定位问题处理，存在局限性，耗时且难以扩展至大规模数据。需要一种弱监督、效率高的方法来实现时间上的伪造段检测。

Method: 提出多模态偏差感知框架（MDP），只利用视频级标注，实现伪造片段的时间定位。主要方法包括：一是设计多模态交互机制（MI），通过保留时序特性的跨模态注意力机制，关联视觉和音频模态，识别模态之间的异常并生成视频特征；二是提出可扩展偏差感知损失，增强弱监督学习中对伪造样本的时间段差异感知。

Result: 大量实验表明，所提方法有效，并在多个评测指标上达到了与全监督方法相当的效果。

Conclusion: 多模态偏差感知框架（MDP）作为弱监督方法，在无需详细标注的情况下，能够高效、准确地定位深度伪造视频的伪造时间片段，具备实际应用的潜力。

Abstract: Current researches on Deepfake forensics often treat detection as a
classification task or temporal forgery localization problem, which are usually
restrictive, time-consuming, and challenging to scale for large datasets. To
resolve these issues, we present a multimodal deviation perceiving framework
for weakly-supervised temporal forgery localization (MDP), which aims to
identify temporal partial forged segments using only video-level annotations.
The MDP proposes a novel multimodal interaction mechanism (MI) and an
extensible deviation perceiving loss to perceive multimodal deviation, which
achieves the refined start and end timestamps localization of forged segments.
Specifically, MI introduces a temporal property preserving cross-modal
attention to measure the relevance between the visual and audio modalities in
the probabilistic embedding space. It could identify the inter-modality
deviation and construct comprehensive video features for temporal forgery
localization. To explore further temporal deviation for weakly-supervised
learning, an extensible deviation perceiving loss has been proposed, aiming at
enlarging the deviation of adjacent segments of the forged samples and reducing
that of genuine samples. Extensive experiments demonstrate the effectiveness of
the proposed framework and achieve comparable results to fully-supervised
approaches in several evaluation metrics.

</details>


### [64] [Dyna3DGR: 4D Cardiac Motion Tracking with Dynamic 3D Gaussian Representation](https://arxiv.org/abs/2507.16608)
*Xueming Fu,Pei Wu,Yingtai Li,Xin Luo,Zihang Jiang,Junhao Mei,Jian Lu,Gao-Jun Teng,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新型的自监督4D心脏运动追踪方法Dyna3DGR，通过结合显式3D高斯表示与隐式神经运动场，使心脏结构和运动可同时高效优化，无需大量标注数据，在公开数据集上超越现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有心脏磁共振运动追踪方法在一致性、细节和训练数据需求上存在局限，且心肌组织特征单一难以追踪，需新方法突破。

Method: 提出Dyna3DGR框架，将3D高斯点云与隐式神经场方法结合，实现心脏结构与运动的自监督优化，并通过可微分体渲染将连续运动与图像空间对齐。

Result: 在ACDC数据集上，Dyna3DGR的心脏运动追踪精度超越了当前主流的深度学习形变配准方法。

Conclusion: Dyna3DGR无需大量训练数据，能高效准确地进行心脏4D运动追踪，且更好地保持了解剖拓扑和时间一致性，为心脏运动分析提供了新方案。

Abstract: Accurate analysis of cardiac motion is crucial for evaluating cardiac
function. While dynamic cardiac magnetic resonance imaging (CMR) can capture
detailed tissue motion throughout the cardiac cycle, the fine-grained 4D
cardiac motion tracking remains challenging due to the homogeneous nature of
myocardial tissue and the lack of distinctive features. Existing approaches can
be broadly categorized into image based and representation-based, each with its
limitations. Image-based methods, including both raditional and deep
learning-based registration approaches, either struggle with topological
consistency or rely heavily on extensive training data. Representation-based
methods, while promising, often suffer from loss of image-level details. To
address these limitations, we propose Dynamic 3D Gaussian Representation
(Dyna3DGR), a novel framework that combines explicit 3D Gaussian representation
with implicit neural motion field modeling. Our method simultaneously optimizes
cardiac structure and motion in a self-supervised manner, eliminating the need
for extensive training data or point-to-point correspondences. Through
differentiable volumetric rendering, Dyna3DGR efficiently bridges continuous
motion representation with image-space alignment while preserving both
topological and temporal consistency. Comprehensive evaluations on the ACDC
dataset demonstrate that our approach surpasses state-of-the-art deep
learning-based diffeomorphic registration methods in tracking accuracy. The
code will be available in https://github.com/windrise/Dyna3DGR.

</details>


### [65] [CTSL: Codebook-based Temporal-Spatial Learning for Accurate Non-Contrast Cardiac Risk Prediction Using Cine MRIs](https://arxiv.org/abs/2507.16612)
*Haoyang Su,Shaohao Rui,Jinyi Xiang,Lianming Wu,Xiaosong Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种无对比剂、无标注分割的自监督模型CTSL，实现了心脏重大不良事件（MACE）的精确预测，准确性优于传统依赖对比剂的方法。


<details>
  <summary>Details</summary>
Motivation: 在心脏MACE预测中，以往方法需依赖对比剂和人工分割掩码，但这些在临床实际中不易获取，限制了广泛应用。因此，开发无需对比剂和掩码的自动化预测方法具有重大现实意义。

Method: 提出了Codebook-based Temporal-Spatial Learning (CTSL)自监督框架。该方法通过多视角蒸馏解耦时间与空间特征——教师模型输入多视角Cine MRI，学生模型则从降维后的序列学习，从而提取动态空间-时间表示。同时通过编码本特征和运动线索实现动态病灶自检测。

Result: CTSL无需分割掩码和对比剂即可实现高置信度的MACE风险预测。

Conclusion: 该模型不仅在无对比剂条件下展现出比传统方法更优的准确性，而且为临床提供了一种快速、无创、易推广的心脏风险评估工具，有助于提升心脏病早期诊断的可行性和普及度。

Abstract: Accurate and contrast-free Major Adverse Cardiac Events (MACE) prediction
from Cine MRI sequences remains a critical challenge. Existing methods
typically necessitate supervised learning based on human-refined masks in the
ventricular myocardium, which become impractical without contrast agents. We
introduce a self-supervised framework, namely Codebook-based Temporal-Spatial
Learning (CTSL), that learns dynamic, spatiotemporal representations from raw
Cine data without requiring segmentation masks. CTSL decouples temporal and
spatial features through a multi-view distillation strategy, where the teacher
model processes multiple Cine views, and the student model learns from
reduced-dimensional Cine-SA sequences. By leveraging codebook-based feature
representations and dynamic lesion self-detection through motion cues, CTSL
captures intricate temporal dependencies and motion patterns. High-confidence
MACE risk predictions are achieved through our model, providing a rapid,
non-invasive solution for cardiac risk assessment that outperforms traditional
contrast-dependent methods, thereby enabling timely and accessible heart
disease diagnosis in clinical settings.

</details>


### [66] [Automatic Fine-grained Segmentation-assisted Report Generation](https://arxiv.org/abs/2507.16623)
*Frederic Jonske,Constantin Seibold,Osman Alperen Koras,Fin Bahnsen,Marie Bauer,Amin Dada,Hamza Kalisch,Anton Schily,Jens Kleesiek*

Main category: cs.CV

TL;DR: 该论文提出了ASaRG方法，通过融合中间特征和精细分割图提升临床报告生成质量。


<details>
  <summary>Details</summary>
Motivation: 自动生成临床报告一直是医学机器学习的重要目标，目的是减轻放射科医生工作负担并为临床医生或患者提供辅助诊断，但报告质量需有良好的准确性和可信度。

Method: ASaRG是在LLaVA架构基础上改进的多模态报告生成方法，通过简单拼接将专业分割模型产生的中间特征与精细分割图，输入到LLaVA的多模态投影层，从而增强模型对医学影像的理解能力。

Result: 只融合中间特征可提升0.89% CE F1分数，融合中间特征和精细分割图则提升2.77%，均优于LLaVA基线，且分别比COMG和ORID方法高出6.98%和6.28%。

Conclusion: ASaRG方法提升了报告生成的准确性和可追溯性，可与其他LLaVA改进方法相结合，对提升医学报告生成有重要意义。

Abstract: Reliable end-to-end clinical report generation has been a longstanding goal
of medical ML research. The end goal for this process is to alleviate
radiologists' workloads and provide second opinions to clinicians or patients.
Thus, a necessary prerequisite for report generation models is a strong general
performance and some type of innate grounding capability, to convince
clinicians or patients of the veracity of the generated reports. In this paper,
we present ASaRG (\textbf{A}utomatic \textbf{S}egmentation-\textbf{a}ssisted
\textbf{R}eport \textbf{G}eneration), an extension of the popular LLaVA
architecture that aims to tackle both of these problems. ASaRG proposes to fuse
intermediate features and fine-grained segmentation maps created by specialist
radiological models into LLaVA's multi-modal projection layer via simple
concatenation. With a small number of added parameters, our approach achieves a
+0.89\% performance gain ($p=0.012$) in CE F1 score compared to the LLaVA
baseline when using only intermediate features, and +2.77\% performance gain
($p<0.001$) when adding a combination of intermediate features and fine-grained
segmentation maps. Compared with COMG and ORID, two other report generation
methods that utilize segmentations, the performance gain amounts to 6.98\% and
6.28\% in F1 score, respectively. ASaRG is not mutually exclusive with other
changes made to the LLaVA architecture, potentially allowing our method to be
combined with other advances in the field. Finally, the use of an arbitrary
number of segmentations as part of the input demonstrably allows tracing
elements of the report to the corresponding segmentation maps and verifying the
groundedness of assessments. Our code will be made publicly available at a
later date.

</details>


### [67] [A2Mamba: Attention-augmented State Space Models for Visual Recognition](https://arxiv.org/abs/2507.16624)
*Meng Lou,Yunxiang Fu,Yizhou Yu*

Main category: cs.CV

TL;DR: 本文提出了A2Mamba，一种将Transformer与Mamba深度集成的视觉识别网络架构，创新性地通过多尺度注意力增强状态空间模型（MASS）实现互补特征融合，实验在ImageNet-1K等任务上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer和Mamba的视觉识别方法，多采用简单堆叠方式，未能实现两者特性的深度融合与优势互补，这限制了网络的表现和建模能力。因此，作者希望设计一种有效的融合机制，实现Transformer与Mamba的协同优化。

Method: 作者设计了A2Mamba网络，核心在于提出多尺度注意力增强状态空间模型（MASS）。MASS将多尺度注意力机制整合进增强型状态空间模型（A2SSM），引入跨空间聚合，将SSM隐状态与多尺度注意力图进行交互，强化空间依赖建模，提升动态建模能力。

Result: A2Mamba在多个视觉识别任务中获得领先表现：A2Mamba-L在ImageNet-1K上top-1准确率86.1%；在语义分割任务mIoU提升2.5%；目标检测和实例分割任务上，A2Mamba-S参数量减少40%的情况下，AP提升1.2%/0.9%。

Conclusion: A2Mamba有效融合Transformer与Mamba的优势，提出的MASS实现了空间建模与动态建模能力提升，结果在主流视觉任务上显著优于现有主流架构，证明了深入交互机制的有效性和先进性。

Abstract: Transformers and Mamba, initially invented for natural language processing,
have inspired backbone architectures for visual recognition. Recent studies
integrated Local Attention Transformers with Mamba to capture both local
details and global contexts. Despite competitive performance, these methods are
limited to simple stacking of Transformer and Mamba layers without any
interaction mechanism between them. Thus, deep integration between Transformer
and Mamba layers remains an open problem. We address this problem by proposing
A2Mamba, a powerful Transformer-Mamba hybrid network architecture, featuring a
new token mixer termed Multi-scale Attention-augmented State Space Model
(MASS), where multi-scale attention maps are integrated into an
attention-augmented SSM (A2SSM). A key step of A2SSM performs a variant of
cross-attention by spatially aggregating the SSM's hidden states using the
multi-scale attention maps, which enhances spatial dependencies pertaining to a
two-dimensional space while improving the dynamic modeling capabilities of
SSMs. Our A2Mamba outperforms all previous ConvNet-, Transformer-, and
Mamba-based architectures in visual recognition tasks. For instance, A2Mamba-L
achieves an impressive 86.1% top-1 accuracy on ImageNet-1K. In semantic
segmentation, A2Mamba-B exceeds CAFormer-S36 by 2.5% in mIoU, while exhibiting
higher efficiency. In object detection and instance segmentation with Cascade
Mask R-CNN, A2Mamba-S surpasses MambaVision-B by 1.2%/0.9% in AP^b/AP^m, while
having 40% less parameters. Code is publicly available at
https://github.com/LMMMEng/A2Mamba.

</details>


### [68] [Benchmarking pig detection and tracking under diverse and challenging conditions](https://arxiv.org/abs/2507.16639)
*Jonathan Henrich,Christian Post,Maximilian Zilke,Parth Shiroya,Emma Chanut,Amir Mollazadeh Yamchi,Ramin Yahyapour,Thomas Kneib,Imke Traulsen*

Main category: cs.CV

TL;DR: 该论文针对猪养殖中动物监测的目标检测与多目标跟踪任务，首次系统性地进行基准评测，并公开了相关数据集及代码。


<details>
  <summary>Details</summary>
Motivation: 尽管已有许多针对猪养殖场动物目标检测与跟踪的研究，但缺乏系统性的基准测试和公开数据集，难以公平比较不同算法，也不利于后续研究发展。

Method: 构建了两个数据集：PigDetect（用于目标检测）和PigTrack（用于多目标跟踪），收集了来自真实猪舍、含有遮挡和能见度差等复杂场景的图像与视频。基于这些数据，系统性地评估了主流目标检测和多目标跟踪方法，并分析了各自的优缺点和特征性失败案例。

Result: 1）高难度训练图片能显著提高检测性能；2）最先进的检测模型在性能上优于实时检测方案；3）SORT系跟踪方法在检测表现上优于端到端模型，但端到端方法在目标关联性方面表现更好，有未来潜力；4）模型在新猪舍测试表现良好，具备良好泛化能力。

Conclusion: 高质量、具有挑战性的训练数据对检测与跟踪模型性能至关重要，端到端方法未来有望成为强大替代方案。公开数据集和代码将促进相关研究的复现与发展。

Abstract: To ensure animal welfare and effective management in pig farming, monitoring
individual behavior is a crucial prerequisite. While monitoring tasks have
traditionally been carried out manually, advances in machine learning have made
it possible to collect individualized information in an increasingly automated
way. Central to these methods is the localization of animals across space
(object detection) and time (multi-object tracking). Despite extensive research
of these two tasks in pig farming, a systematic benchmarking study has not yet
been conducted. In this work, we address this gap by curating two datasets:
PigDetect for object detection and PigTrack for multi-object tracking. The
datasets are based on diverse image and video material from realistic barn
conditions, and include challenging scenarios such as occlusions or bad
visibility. For object detection, we show that challenging training images
improve detection performance beyond what is achievable with randomly sampled
images alone. Comparing different approaches, we found that state-of-the-art
models offer substantial improvements in detection quality over real-time
alternatives. For multi-object tracking, we observed that SORT-based methods
achieve superior detection performance compared to end-to-end trainable models.
However, end-to-end models show better association performance, suggesting they
could become strong alternatives in the future. We also investigate
characteristic failure cases of end-to-end models, providing guidance for
future improvements. The detection and tracking models trained on our datasets
perform well in unseen pens, suggesting good generalization capabilities. This
highlights the importance of high-quality training data. The datasets and
research code are made publicly available to facilitate reproducibility, re-use
and further development.

</details>


### [69] [Synthetic Data Matters: Re-training with Geo-typical Synthetic Labels for Building Detection](https://arxiv.org/abs/2507.16657)
*Shuang Song,Yang Tang,Rongjun Qin*

Main category: cs.CV

TL;DR: 提出了一种结合地理特征合成数据与对抗域适应的新方法，实现了遥感建筑分割模型在不同区域的更好泛化，无需大量真实标注数据。实验显示此方法显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在遥感建筑分割任务上虽然进步明显，但在不同地理区域泛化能力弱，主要因城市结构和建筑多样性大，而全球范围内收集足够的标注数据非常困难和耗时。亟需通过低成本、灵活的方式解决标注数据匮乏与模型泛化能力不足的问题。

Method: 该方法利用OpenStreetMap等地理空间数据，通过过程建模和物理渲染生成与目标区域城市结构相符的高分辨率合成遥感图像，并在建筑形状、材质和环境照明等方面做域随机化，生成可无限扩充的训练样本。在此基础上，将这些具有地典特征的合成数据融入到建筑分割的对抗性域适应框架中，用于模型测试时再训练。

Result: 实验证明该方法能显著提升建筑分割模型在新区域数据上的性能，针对不同区域的域差，模型中位数性能提升最高可达12%。

Conclusion: 这是一种可扩展且低成本的泛化提升方案，通过合成数据与局部地理知识结合，有效缓解纯合成数据常见的“模型崩溃”问题，为遥感建筑分割任务提供了无需大量真实标注的新思路。

Abstract: Deep learning has significantly advanced building segmentation in remote
sensing, yet models struggle to generalize on data of diverse geographic
regions due to variations in city layouts and the distribution of building
types, sizes and locations. However, the amount of time-consuming annotated
data for capturing worldwide diversity may never catch up with the demands of
increasingly data-hungry models. Thus, we propose a novel approach: re-training
models at test time using synthetic data tailored to the target region's city
layout. This method generates geo-typical synthetic data that closely
replicates the urban structure of a target area by leveraging geospatial data
such as street network from OpenStreetMap. Using procedural modeling and
physics-based rendering, very high-resolution synthetic images are created,
incorporating domain randomization in building shapes, materials, and
environmental illumination. This enables the generation of virtually unlimited
training samples that maintain the essential characteristics of the target
environment. To overcome synthetic-to-real domain gaps, our approach integrates
geo-typical data into an adversarial domain adaptation framework for building
segmentation. Experiments demonstrate significant performance enhancements,
with median improvements of up to 12%, depending on the domain gap. This
scalable and cost-effective method blends partial geographic knowledge with
synthetic imagery, providing a promising solution to the "model collapse" issue
in purely synthetic datasets. It offers a practical pathway to improving
generalization in remote sensing building segmentation without extensive
real-world annotations.

</details>


### [70] [QRetinex-Net: Quaternion-Valued Retinex Decomposition for Low-Level Computer Vision Applications](https://arxiv.org/abs/2507.16683)
*Sos Agaian,Vladimir Frants*

Main category: cs.CV

TL;DR: 本文提出了一种基于四元数的Retinex理论新方法，有效提升了低光照图像的色彩还原、去噪和反射率稳定性。实验证明其在多种视觉任务中优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 传统Retinex方法存在四大缺陷：RGB通道独立处理、缺少神经科学的配色机制、无法完美重构输入图像、不能充分解释人类颜色恒常性，因此亟需新的理论和计算模型提升低照度图像的质量与色彩一致性。

Method: 作者首次将Retinex分解建模为四元数乘积，结合Hamilton积运算，用以更自然地模拟颜色感知过程。同时提出了反射率一致性指标（Reflectance Consistency Index）评估反射率的不变性。

Result: 在低光照裂纹检测、人脸检测、红外-可见光融合等任务上，相较于主流方法提升了2-11%的表现，并显著改善了色彩保真、降低噪声、提升反射率稳定性。

Conclusion: 四元数Retinex模型更贴合神经科学原理，能更准确地处理低光照情况下的图像色彩、反射率和噪点问题，为后续相关研究和实际应用提供了更优的方法基础。

Abstract: Images taken in low light often show color shift, low contrast, noise, and
other artifacts that hurt computer-vision accuracy. Retinex theory addresses
this by viewing an image S as the pixel-wise product of reflectance R and
illumination I, mirroring the way people perceive stable object colors under
changing light. The decomposition is ill-posed, and classic Retinex models have
four key flaws: (i) they treat the red, green, and blue channels independently;
(ii) they lack a neuroscientific model of color vision; (iii) they cannot
perfectly rebuild the input image; and (iv) they do not explain human color
constancy. We introduce the first Quaternion Retinex formulation, in which the
scene is written as the Hamilton product of quaternion-valued reflectance and
illumination. To gauge how well reflectance stays invariant, we propose the
Reflectance Consistency Index. Tests on low-light crack inspection, face
detection under varied lighting, and infrared-visible fusion show gains of 2-11
percent over leading methods, with better color fidelity, lower noise, and
higher reflectance stability.

</details>


### [71] [Enhancing Remote Sensing Vision-Language Models Through MLLM and LLM-Based High-Quality Image-Text Dataset Generation](https://arxiv.org/abs/2507.16716)
*Yiguo He,Junjie Zhu,Yiying Li,Xiaoyu Zhang,Chunping Qiu,Jun Wang,Qiangjuan Huang,Ke Yang*

Main category: cs.CV

TL;DR: 本文提出了一种多视角文本生成与整合方法，为遥感图像生成高质量文本描述，并构建了大规模高质量数据集。该方法能大幅提升视觉-语言基础模型在遥感任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 遥感领域视觉-语言基础模型的性能高度依赖于大规模高质量的图像-文本配对数据。目前生成的文本描述质量有限，导致模型训练效率低，性能提升有限。研究动机在于提出生成高质量遥感图像文本的方法，提升模型的表现和应用价值。

Method: 本方法分为两阶段：首先利用多模态大语言模型（MLLM）和规则驱动方法，从不同视角生成详细描述；随后用大语言模型对这些多样化描述进行整合，生成综合性更强的高质量文本。以此方式对21万遥感图像生成了130万高质文本，形成HQRS-IT-210K数据集，并据此微调CLIP和CoCa两种视觉-语言模型。

Result: HQRS-CLIP在多项遥感下游任务中，以仅4.2%的训练数据超越了以往最佳CLIP模型；RS-CoCa在图像到文本生成任务上优于现有先进方法，生成的描述甚至超过人工标注。

Conclusion: 提出的MpGI方法有效提升了遥感图像的文本描述质量，并显著增强了视觉-语言模型的性能。相关数据集、模型及代码将公开，有望推动遥感与视觉-语言领域的发展。

Abstract: The application of Vision-language foundation models (VLFMs) to remote
sensing (RS) imagery has garnered significant attention due to their superior
capability in various downstream tasks. A key challenge lies in the scarcity of
high-quality, large-scale, image-text paired training data. Recently, several
works introduced extensive image-text datasets for RS and trained their VLFMs.
However, due to the rudimentary methods used for generating captions, the
quality of datasets is suboptimal, requiring larger volumes of training data,
while only yielding modest performance improvements. In this paper, we propose
a two-stage method named MpGI(Multi-Perspective Generation and Integration) for
generating high-quality text captions for RS images. Firstly, we generate
distinct and detailed descriptions from different perspectives using
Rule-MLLM(Multimodal Large Language Model) Relay Generation and MLLMs
generation methods. Next, we utilize Large Language Models (LLMs) to integrate
these diverse descriptions into comprehensive captions, capturing details from
multiple perspectives. Finally, we have created the HQRS-IT-210K dataset,
including about 210,000 RS images and 1.3 million captions. We fine-tuned two
VLFMs using our dataset: CLIP, a discriminative model, and CoCa, an
image-to-text generative model. This process resulted in our proposed HQRS-CLIP
and RS-CoCa models. Experimental results demonstrate that HQRS-CLIP surpassed
the previous SOTA RS CLIP model in various downstream tasks while using only
4.2\% of the training data. RS-CoCa outperforms other advanced approaches
across benchmark datasets and can generate captions for RS images that rival or
even exceed manual annotations. Dataset, pre-trained models, and codes will be
released at https://github.com/YiguoHe/HQRS-210K-and-HQRS-CLIP.

</details>


### [72] [Temporally-Constrained Video Reasoning Segmentation and Automated Benchmark Construction](https://arxiv.org/abs/2507.16718)
*Yiqing Shen,Chenjia Li,Chenxiao Fan,Mathias Unberath*

Main category: cs.CV

TL;DR: 该论文针对视频分割仅限于预定义类别与无法处理隐性文本查询对象的问题，提出了具有时序约束的视频推理分割任务及数据集和自动基准构建方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频分割方法局限于封闭的对象类别集，无法识别词汇外对象和仅隐式提及的对象，特别是在医疗等流程多样、对象变化大的场景中，导致实际应用受限。

Method: 提出了具有时序约束的视频推理分割新任务——要求模型根据包含时序推理的信息化文本查询，推断目标对象在视频中何时变为相关，此外，开发了自动化基准构建方法，降低数据集构建成本与提升扩展性。

Result: 构建并发布了TCVideoRSBenchmark时序约束视频推理分割数据集，包含52个样本，并基于MVOR公开视频集整理。

Conclusion: 该工作拓展了视频分割任务在开放类别和复杂动态场景下的适用性，为依赖动态上下文推理的视频分析任务提供了新思路和基础数据支持。

Abstract: Conventional approaches to video segmentation are confined to predefined
object categories and cannot identify out-of-vocabulary objects, let alone
objects that are not identified explicitly but only referred to implicitly in
complex text queries. This shortcoming limits the utility for video
segmentation in complex and variable scenarios, where a closed set of object
categories is difficult to define and where users may not know the exact object
category that will appear in the video. Such scenarios can arise in operating
room video analysis, where different health systems may use different workflows
and instrumentation, requiring flexible solutions for video analysis. Reasoning
segmentation (RS) now offers promise towards such a solution, enabling natural
language text queries as interaction for identifying object to segment.
However, existing video RS formulation assume that target objects remain
contextually relevant throughout entire video sequences. This assumption is
inadequate for real-world scenarios in which objects of interest appear,
disappear or change relevance dynamically based on temporal context, such as
surgical instruments that become relevant only during specific procedural
phases or anatomical structures that gain importance at particular moments
during surgery. Our first contribution is the introduction of
temporally-constrained video reasoning segmentation, a novel task formulation
that requires models to implicitly infer when target objects become
contextually relevant based on text queries that incorporate temporal
reasoning. Since manual annotation of temporally-constrained video RS datasets
would be expensive and limit scalability, our second contribution is an
innovative automated benchmark construction method. Finally, we present
TCVideoRSBenchmark, a temporally-constrained video RS dataset containing 52
samples using the videos from the MVOR dataset.

</details>


### [73] [HarmonPaint: Harmonized Training-Free Diffusion Inpainting](https://arxiv.org/abs/2507.16732)
*Ying Li,Xinzhe Li,Yong Du,Yangyang Xu,Junyu Dong,Shengfeng He*

Main category: cs.CV

TL;DR: 提出了一种无需训练的新型图像修复方法 HarmonPaint，可与扩散模型的注意力机制无缝结合，实现结构和风格高度一致的修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有修复方法往往需要大量重新训练或微调，并难以在修复区域与背景之间维持结构和风格的一致性。为解决这些问题，作者提出新方法。

Method: HarmonPaint 利用扩散模型的自注意力机制结合掩码策略，无需任何训练或微调，保证修复区域的结构一致性，并通过扩散模型内在属性实现风格迁移，从而实现风格和结构的和谐融合。

Result: 大量实验表明 HarmonPaint 在不同场景和风格下均能有效地实现高质量修复，验证了方法的通用性和优越性能。

Conclusion: HarmonPaint 为高效、通用且无需训练的图像修复提供了新方案，能够在保证结构和风格一致性的前提下实现和谐的修复效果。

Abstract: Existing inpainting methods often require extensive retraining or fine-tuning
to integrate new content seamlessly, yet they struggle to maintain coherence in
both structure and style between inpainted regions and the surrounding
background. Motivated by these limitations, we introduce HarmonPaint, a
training-free inpainting framework that seamlessly integrates with the
attention mechanisms of diffusion models to achieve high-quality, harmonized
image inpainting without any form of training. By leveraging masking strategies
within self-attention, HarmonPaint ensures structural fidelity without model
retraining or fine-tuning. Additionally, we exploit intrinsic diffusion model
properties to transfer style information from unmasked to masked regions,
achieving a harmonious integration of styles. Extensive experiments demonstrate
the effectiveness of HarmonPaint across diverse scenes and styles, validating
its versatility and performance.

</details>


### [74] [DFR: A Decompose-Fuse-Reconstruct Framework for Multi-Modal Few-Shot Segmentation](https://arxiv.org/abs/2507.16736)
*Shuai Chen,Fanman Meng,Xiwei Zhang,Haoran Wei,Chenhao Wu,Qingbo Wu,Hongliang Li*

Main category: cs.CV

TL;DR: 提出DFR框架，通过分解、融合与重构，有效整合视觉、文本与音频多模态信息，大幅提升小样本分割表现。


<details>
  <summary>Details</summary>
Motivation: 现有小样本分割方法多只利用视觉样本或文本描述，无法充分挖掘现实中丰富的多模态信息，导致语义理解受限。

Method: DFR框架包括三大创新：1）多模态分解：利用SAM提出区域、细化文本描述、处理音频特征；2）多模态对比融合：通过对比学习确保视觉、文本、音频特征一致性并实现前背景动态交互；3）双路径重构：将三模态语义引导与几何位置信息自适应整合以提升分割效果。

Result: 在视觉、文本、音频等模态下的合成与真实测试中，DFR均显著优于现有最优方法。

Conclusion: 多模态引导的小样本分割框架DFR能有效充分利用多领域信息，为复杂场景下的分割任务带来显著提升。

Abstract: This paper presents DFR (Decompose, Fuse and Reconstruct), a novel framework
that addresses the fundamental challenge of effectively utilizing multi-modal
guidance in few-shot segmentation (FSS). While existing approaches primarily
rely on visual support samples or textual descriptions, their single or
dual-modal paradigms limit exploitation of rich perceptual information
available in real-world scenarios. To overcome this limitation, the proposed
approach leverages the Segment Anything Model (SAM) to systematically integrate
visual, textual, and audio modalities for enhanced semantic understanding. The
DFR framework introduces three key innovations: 1) Multi-modal Decompose: a
hierarchical decomposition scheme that extracts visual region proposals via
SAM, expands textual semantics into fine-grained descriptors, and processes
audio features for contextual enrichment; 2) Multi-modal Contrastive Fuse: a
fusion strategy employing contrastive learning to maintain consistency across
visual, textual, and audio modalities while enabling dynamic semantic
interactions between foreground and background features; 3) Dual-path
Reconstruct: an adaptive integration mechanism combining semantic guidance from
tri-modal fused tokens with geometric cues from multi-modal location priors.
Extensive experiments across visual, textual, and audio modalities under both
synthetic and real settings demonstrate DFR's substantial performance
improvements over state-of-the-art methods.

</details>


### [75] [Denoising-While-Completing Network (DWCNet): Robust Point Cloud Completion Under Corruption](https://arxiv.org/abs/2507.16743)
*Keneni W. Tesema,Lyndon Hill,Mark W. Jones,Gary K. L. Tam*

Main category: cs.CV

TL;DR: 本文提出了一种新的点云补全方法DWCNet，并发布了一个新的腐蚀点云补全数据集CPCCD，用于评估模型在各种噪声和缺陷下的健壮性。DWCNet集成了对比学习和自注意力的噪声管理模块，在合成和真实数据集上均取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有点云补全方法大多在合成数据上训练，面对真实场景下的多重噪声和遮挡时表现不佳，因此需要更鲁棒的方法和新的评测基准。

Method: 提出DWCNet（Denoising-While-Completing Network），引入噪声管理模块（NMM），结合对比学习和自注意力机制，以同时抑制噪声和建模结构信息。此外建立了包含多种腐蚀的评测集CPCCD。

Result: DWCNet在合成和真实数据集，以及不同类型的腐蚀影响下，都实现了优于现有方法的补全效果。

Conclusion: DWCNet有效提升了点云补全过程中的鲁棒性，在对抗真实世界中的多重噪声和遮挡方面表现出色。新提出的数据集促使该领域未来更真实、更具挑战性的研究。

Abstract: Point cloud completion is crucial for 3D computer vision tasks in autonomous
driving, augmented reality, and robotics. However, obtaining clean and complete
point clouds from real-world environments is challenging due to noise and
occlusions. Consequently, most existing completion networks -- trained on
synthetic data -- struggle with real-world degradations. In this work, we
tackle the problem of completing and denoising highly corrupted partial point
clouds affected by multiple simultaneous degradations. To benchmark robustness,
we introduce the Corrupted Point Cloud Completion Dataset (CPCCD), which
highlights the limitations of current methods under diverse corruptions.
Building on these insights, we propose DWCNet (Denoising-While-Completing
Network), a completion framework enhanced with a Noise Management Module (NMM)
that leverages contrastive learning and self-attention to suppress noise and
model structural relationships. DWCNet achieves state-of-the-art performance on
both clean and corrupted, synthetic and real-world datasets. The dataset and
code will be publicly available at
https://github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions

</details>


### [76] [CMP: A Composable Meta Prompt for SAM-Based Cross-Domain Few-Shot Segmentation](https://arxiv.org/abs/2507.16753)
*Shuai Chen,Fanman Meng,Chunjin Yang,Haoran Wei,Chenhao Wu,Qingbo Wu,Hongliang Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的框架CMP，通过自动生成Meta-Prompt和新颖的模块设计，提升了基础大模型（如SAM）在跨域小样本分割任务中的表现，极大改善了其泛化和适应能力，取得了最先进的实验结果。


<details>
  <summary>Details</summary>
Motivation: 近年来基础分割大模型（如SAM）展现出了良好的零样本泛化能力，但直接用于跨域小样本分割时，受限于人工提示和跨域泛化能力有限的问题。因此，急需方法来提升大模型在CD-FSS中的实用性和自动化水平。

Method: 作者提出了CMP框架，包括三个关键模块：1）RCT模块实现语义补充和变换，增丰富学习信息；2）CMPG模块实现自动meta提示词生成，摆脱人工依赖；3）FAI模块用于缓解域间差异，提升跨域泛化能力。整体提升了大模型在跨域小样本分割的性能。

Result: 在四个跨域分割数据集上进行了实验，CMP框架在1-shot和5-shot场景下分别取得了71.8%和74.5%的mIoU，优于现有方法。

Conclusion: CMP框架有效解决了基础大模型在CD-FSS任务中的人工提示依赖和跨域泛化能力有限的难题，显著提升了分割效果，具有较强的实际价值与推广潜力。

Abstract: Cross-Domain Few-Shot Segmentation (CD-FSS) remains challenging due to
limited data and domain shifts. Recent foundation models like the Segment
Anything Model (SAM) have shown remarkable zero-shot generalization capability
in general segmentation tasks, making it a promising solution for few-shot
scenarios. However, adapting SAM to CD-FSS faces two critical challenges:
reliance on manual prompt and limited cross-domain ability. Therefore, we
propose the Composable Meta-Prompt (CMP) framework that introduces three key
modules: (i) the Reference Complement and Transformation (RCT) module for
semantic expansion, (ii) the Composable Meta-Prompt Generation (CMPG) module
for automated meta-prompt synthesis, and (iii) the Frequency-Aware Interaction
(FAI) module for domain discrepancy mitigation. Evaluations across four
cross-domain datasets demonstrate CMP's state-of-the-art performance, achieving
71.8\% and 74.5\% mIoU in 1-shot and 5-shot scenarios respectively.

</details>


### [77] [Faithful, Interpretable Chest X-ray Diagnosis with Anti-Aliased B-cos Networks](https://arxiv.org/abs/2507.16761)
*Marcel Kleinmann,Shashank Agnihotri,Margret Keuper*

Main category: cs.CV

TL;DR: 本文提出改进的B-cos网络，通过引入反混叠策略和扩展多标签分类能力，实现了在医学影像（如胸部X光）领域中预测准确且无伪影、可解释性强的模型。


<details>
  <summary>Details</summary>
Motivation: 在医学影像等安全关键领域，深度神经网络的可解释性和可靠性非常重要。B-cos网络虽具备一定可解释性，但原模型存在显著伪影，且无法支持多标签分类，限制了其临床应用。本文旨在解决这些不足。

Method: 1) 引入两种反混叠池化方法（FLCPooling和BlurPool）来提升可解释性解释图的质量；2) 扩展原B-cos网络以支持多标签分类，满足实际临床环境中多种并发异常的分析需求。在胸部X光数据集上进行了相关实验评估。

Result: 改进后的B-cos网络（B-cos_FLC和B-cos_BP）在保持较强的预测性能基础上，生成了无伪影、可靠性高的可解释性解释图，支持多标签任务，证明了方法的有效性与实用性。

Conclusion: 引入反混叠策略并扩展多标签能力后，B-cos网络更加适用于医学影像临床应用，可实现可靠且明晰的解释性预测，为安全关键领域的深度学习模型应用提供价值。

Abstract: Faithfulness and interpretability are essential for deploying deep neural
networks (DNNs) in safety-critical domains such as medical imaging. B-cos
networks offer a promising solution by replacing standard linear layers with a
weight-input alignment mechanism, producing inherently interpretable,
class-specific explanations without post-hoc methods. While maintaining
diagnostic performance competitive with state-of-the-art DNNs, standard B-cos
models suffer from severe aliasing artifacts in their explanation maps, making
them unsuitable for clinical use where clarity is essential. Additionally, the
original B-cos formulation is limited to multi-class settings, whereas chest
X-ray analysis often requires multi-label classification due to co-occurring
abnormalities. In this work, we address both limitations: (1) we introduce
anti-aliasing strategies using FLCPooling (FLC) and BlurPool (BP) to
significantly improve explanation quality, and (2) we extend B-cos networks to
support multi-label classification. Our experiments on chest X-ray datasets
demonstrate that the modified $\text{B-cos}_\text{FLC}$ and
$\text{B-cos}_\text{BP}$ preserve strong predictive performance while providing
faithful and artifact-free explanations suitable for clinical application in
multi-label settings. Code available at:
$\href{https://github.com/mkleinma/B-cos-medical-paper}{GitHub repository}$.

</details>


### [78] [Task-Specific Zero-shot Quantization-Aware Training for Object Detection](https://arxiv.org/abs/2507.16782)
*Changhao Li,Xinrui Chen,Ji Wang,Kang Zhao,Jianfei Chen*

Main category: cs.CV

TL;DR: 本文提出了一种用于目标检测网络的任务特定零样本量化（ZSQ）框架，通过从预训练网络中合成任务特定的校准集，并结合知识蒸馏，显著提升了量化后检测网络的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的网络量化方法依赖于原始训练数据，但由于隐私或安全等原因，获取真实数据往往受限。现有零样本量化方法用于目标检测时，由于仅用无标签的通用合成图像，缺乏检测需求的特定信息，导致性能下降。因此，亟需一种能够结合任务相关信息的量化方法。

Method: 方法包括两大阶段：1）提出一种边框和类别采样策略，从预训练模型合成包含目标位置信息和类别分布的任务特定校准集，无需任何先验知识；2）将任务特定训练集成到知识蒸馏过程中，从而恢复量化后目标检测网络的性能。

Result: 在MS-COCO和Pascal VOC数据集上的大量实验表明，该方法能有效提升检测网络的量化性能，并达到当前最优水平。

Conclusion: 本工作针对目标检测提出了一种更为有效的任务特定零样本量化方法，在无原始数据的条件下实现了高效且高性能的量化，推动了实际应用场景下模型压缩技术的发展。

Abstract: Quantization is a key technique to reduce network size and computational
complexity by representing the network parameters with a lower precision.
Traditional quantization methods rely on access to original training data,
which is often restricted due to privacy concerns or security challenges.
Zero-shot Quantization (ZSQ) addresses this by using synthetic data generated
from pre-trained models, eliminating the need for real training data. Recently,
ZSQ has been extended to object detection. However, existing methods use
unlabeled task-agnostic synthetic images that lack the specific information
required for object detection, leading to suboptimal performance. In this
paper, we propose a novel task-specific ZSQ framework for object detection
networks, which consists of two main stages. First, we introduce a bounding box
and category sampling strategy to synthesize a task-specific calibration set
from the pre-trained network, reconstructing object locations, sizes, and
category distributions without any prior knowledge. Second, we integrate
task-specific training into the knowledge distillation process to restore the
performance of quantized detection networks. Extensive experiments conducted on
the MS-COCO and Pascal VOC datasets demonstrate the efficiency and
state-of-the-art performance of our method. Our code is publicly available at:
https://github.com/DFQ-Dojo/dfq-toolkit .

</details>


### [79] [Enhancing Domain Diversity in Synthetic Data Face Recognition with Dataset Fusion](https://arxiv.org/abs/2507.16790)
*Anjith George,Sebastien Marcel*

Main category: cs.CV

TL;DR: 本论文提出将不同架构的两种合成面部数据集结合训练人脸识别模型，从而缓解单一生成器产生的偏差，提高模型识别准确率。


<details>
  <summary>Details</summary>
Motivation: 由于网络爬取人脸数据集会引发伦理和隐私争议，合成数据逐渐成为训练人脸识别模型的新选择，但其效果普遍不如真实数据。主要问题在于大多采用单一生成器，易导致生成数据集带有特定模型的伪影和偏差。

Method: 作者提出采用两种架构不同的生成器各自生成合成面部数据集，再进行融合训练。这种组合能减少特定模型伪影，提升数据多样性（包括姿态、光照、人口属性等），并通过强调身份相关特征起到隐式正则化作用。

Result: 实验在标准人脸识别基准上评测，融合数据训练得到的模型在多个基准上取得了更优的识别效果。

Conclusion: 多生成器融合法能有效缓解单一生成器带来的弊端，并提升人脸识别模型在合成数据基础上的性能，对提升合成数据实用性有重要意义。

Abstract: While the accuracy of face recognition systems has improved significantly in
recent years, the datasets used to train these models are often collected
through web crawling without the explicit consent of users, raising ethical and
privacy concerns. To address this, many recent approaches have explored the use
of synthetic data for training face recognition models. However, these models
typically underperform compared to those trained on real-world data. A common
limitation is that a single generator model is often used to create the entire
synthetic dataset, leading to model-specific artifacts that may cause
overfitting to the generator's inherent biases and artifacts. In this work, we
propose a solution by combining two state-of-the-art synthetic face datasets
generated using architecturally distinct backbones. This fusion reduces
model-specific artifacts, enhances diversity in pose, lighting, and
demographics, and implicitly regularizes the face recognition model by
emphasizing identity-relevant features. We evaluate the performance of models
trained on this combined dataset using standard face recognition benchmarks and
demonstrate that our approach achieves superior performance across many of
these benchmarks.

</details>


### [80] [HOComp: Interaction-Aware Human-Object Composition](https://arxiv.org/abs/2507.16813)
*Dong Liang,Jinyuan Jia,Yuhao Liu,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: 本文提出了HOComp方法，实现了将前景物体无缝自然地合成到以人为主的背景图片中，重点解决了以往方法在人与物体交互时合成不自然的问题。通过引入区域姿态引导与细节一致性机制，在新构建的数据集上获得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有图像引导合成方法在人—物互动场景下，难以生成自然且交互自洽的合成图像。尤其是对于人与物体的动作和外观一致性把控不足。因此，提升人与物体交互的合成质量是该工作的初衷。

Method: （1）提出MLLMs驱动的区域姿态引导（MRPG）：利用多模态大模型检测交互区域和类型，实现粗到细的姿态约束，并结合人体关键点实现动作细粒度控制；（2）提出细节一致性外观保持（DCAP）：集成了基于形状的关注机制、多视角外观损失和背景一致性损失，以确保前景与背景人物的形状、纹理和外观一致。

Result: 在作者提出的IHOC数据集上，HOComp在生成自然拥有人—物体互动且外观一致的合成图像方面，实现了定性和定量上均优于现有方法的表现。

Conclusion: HOComp解决了人—物体交互合成中的自然性与一致性难题，并以新数据集为研究提供了测试基准，有效推动了交互感知图像合成领域的发展。

Abstract: While existing image-guided composition methods may help insert a foreground
object onto a user-specified region of a background image, achieving natural
blending inside the region with the rest of the image unchanged, we observe
that these existing methods often struggle in synthesizing seamless
interaction-aware compositions when the task involves human-object
interactions. In this paper, we first propose HOComp, a novel approach for
compositing a foreground object onto a human-centric background image, while
ensuring harmonious interactions between the foreground object and the
background person and their consistent appearances. Our approach includes two
key designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes
MLLMs to identify the interaction region as well as the interaction type (e.g.,
holding and lefting) to provide coarse-to-fine constraints to the generated
pose for the interaction while incorporating human pose landmarks to track
action variations and enforcing fine-grained pose constraints; and (2)
Detail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware
attention modulation mechanism, a multi-view appearance loss, and a background
consistency loss to ensure consistent shapes/textures of the foreground and
faithful reproduction of the background human. We then propose the first
dataset, named Interaction-aware Human-Object Composition (IHOC), for the task.
Experimental results on our dataset show that HOComp effectively generates
harmonious human-object interactions with consistent appearances, and
outperforms relevant methods qualitatively and quantitatively.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [81] [eSapiens's DEREK Module: Deep Extraction & Reasoning Engine for Knowledge with LLMs](https://arxiv.org/abs/2507.15863)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.CL

TL;DR: 本文提出了DEREK模块——一种专为企业文档问答设计的安全、可扩展的检索增强生成（RAG）流程，实现了高精度、可溯源和高安全性的问答能力。


<details>
  <summary>Details</summary>
Motivation: 现有文档问答系统在面对多样化文档格式、高安全性和可追溯性要求时存在疑难，尤其是在法律、金融等高风险领域。亟需一种安全、可扩展且能溯源的企业级RAG方案。

Method: DEREK模块支持多种异构内容输入（PDF、Office、网页），将文档切分为1000词重叠片段，并用HNSW+BM25混合索引。查询经GPT-4o优化，混合向量检索后用Cohere再排序。采用LLM和CO-STAR提示工程生成答案，并由LangGraph验证机制确保所有结论可追溯。所有组件运行于加密容器，端到端TLS 1.3和AES-256加密。

Result: 在LegalBench四个子集上，1000词切片提高Recall@50约1个百分点，混合检索+再排序提升Precision@10约7个百分点；验证机制可将TRACe利用率提升至0.50以上，并使无依据陈述低于3%。

Conclusion: DEREK模块在安全性、可追溯性和实际部署方面表现优秀，适合法律、金融等高需求企业场景，为企业文档问答提供了可靠基线。

Abstract: We present the DEREK (Deep Extraction & Reasoning Engine for Knowledge)
Module, a secure and scalable Retrieval-Augmented Generation pipeline designed
specifically for enterprise document question answering. Designed and
implemented by eSapiens, the system ingests heterogeneous content (PDF, Office,
web), splits it into 1,000-token overlapping chunks, and indexes them in a
hybrid HNSW+BM25 store. User queries are refined by GPT-4o, retrieved via
combined vector+BM25 search, reranked with Cohere, and answered by an LLM using
CO-STAR prompt engineering. A LangGraph verifier enforces citation overlap,
regenerating answers until every claim is grounded. On four LegalBench subsets,
1000-token chunks improve Recall@50 by approximately 1 pp and hybrid+rerank
boosts Precision@10 by approximately 7 pp; the verifier raises TRACe
Utilization above 0.50 and limits unsupported statements to less than 3%. All
components run in containers, enforce end-to-end TLS 1.3 and AES-256. These
results demonstrate that the DEREK module delivers accurate, traceable, and
production-ready document QA with minimal operational overhead. The module is
designed to meet enterprise demands for secure, auditable, and context-faithful
retrieval, providing a reliable baseline for high-stakes domains such as legal
and finance.

</details>


### [82] [Adversarial Demonstration Learning for Low-resource NER Using Dual Similarity](https://arxiv.org/abs/2507.15864)
*Guowen Yuan,Tien-Hsuan Wu,Lianghao Xia,Ben Kao*

Main category: cs.CL

TL;DR: 本文提出了一种基于示范学习的新方法，提升了低资源情景下命名实体识别（NER）的性能，通过整合语义和特征相似性选取示范样本，以及引入对抗性训练促使模型更加有效地参考示范案例。


<details>
  <summary>Details</summary>
Motivation: 现有低资源NER中的示范选择主要依赖语义相似性，未充分利用特征相似性，且模型参考示范样本的能力普遍不足，导致性能提升有限。

Method: 1. 借助“双重相似性”（语义+特征）进行示范样本筛选；2. 设计对抗性示范训练方式强制模型参考示范样本以提高标注准确性。

Result: 在低资源NER任务中，通过大量实验，该方法显著优于多种现有方法。

Conclusion: 结合双重相似性选择与对抗性训练的示范方法能有效提升低资源NER性能，验证了其有效性和优越性。

Abstract: We study the problem of named entity recognition (NER) based on demonstration
learning in low-resource scenarios. We identify two issues in demonstration
construction and model training. Firstly, existing methods for selecting
demonstration examples primarily rely on semantic similarity; We show that
feature similarity can provide significant performance improvement. Secondly,
we show that the NER tagger's ability to reference demonstration examples is
generally inadequate. We propose a demonstration and training approach that
effectively addresses these issues. For the first issue, we propose to select
examples by dual similarity, which comprises both semantic similarity and
feature similarity. For the second issue, we propose to train an NER model with
adversarial demonstration such that the model is forced to refer to the
demonstrations when performing the tagging task. We conduct comprehensive
experiments in low-resource NER tasks, and the results demonstrate that our
method outperforms a range of methods.

</details>


### [83] [Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models](https://arxiv.org/abs/2507.15868)
*Altynbek Ismailov,Salia Asanova*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在代码生成时，对不同类型提示扰动的鲁棒性和敏感度，发现现有模型对语义关键性变化缺乏足够反应。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs被应用于代码生成等高风险场景，但模型对提示中的拼写错误或细微语义变化是否敏感尚不清楚。本研究旨在明确模型在有益的鲁棒性与有害的不敏感性之间的界限。

Method: 作者收集了50道LeetCode题目，并对提示进行三类最小扰动：逐步去除10%的词（信息缩减）、替换重要量词（如“max”与“min”）、将常用名词换成冷僻技术术语。测试六个前沿LLMs（包括三个位“推理调优”版本）在这些扰动下的输出代码正确性和适应能力。

Result: 在11853组生成中，LLMs即使丢失90%提示内容，85%的情况下仍能输出正确答案，表现出对信息缩减的“过拟合鲁棒性”；而对于一处量词反向替换，只有54%能正确应对，推理调优模型反应更差。行话替换的适应率为56%。

Conclusion: 现有LLMs常将看似无害的扰动和语义实质变化一并忽略，导致鲁棒性和敏感性界限模糊。建议评测和训练时，对语义变化敏感性引入差异化奖励：对表面噪声保持稳定，但遇语义变更时能做出响应或拒绝。

Abstract: Large language models (LLMs) now write code in settings where misreading a
single word can break safety or cost money, yet we still expect them to
overlook stray typos. To probe where useful robustness ends and harmful
insensitivity begins, we compile 50 LeetCode problems and craft three minimal
prompt perturbations that should vary in importance: (i) progressive
underspecification deleting 10 % of words per step; (ii) lexical flip swapping
a pivotal quantifier ("max" to "min"); and (iii) jargon inflation replacing a
common noun with an obscure technical synonym. Six frontier models, including
three "reasoning-tuned" versions, solve each mutated prompt, and their Python
outputs are checked against the original test suites to reveal whether they
reused the baseline solution or adapted. Among 11 853 generations we observe a
sharp double asymmetry. Models remain correct in 85 % of cases even after 90 %
of the prompt is missing, showing over-robustness to underspecification, yet
only 54 % react to a single quantifier flip that reverses the task, with
reasoning-tuned variants even less sensitive than their bases. Jargon edits lie
in between, passing through 56 %. Current LLMs thus blur the line between
harmless noise and meaning - changing edits, often treating both as ignorable.
Masking salient anchors such as function names can force re - evaluation. We
advocate evaluation and training protocols that reward differential
sensitivity: stay steady under benign noise but adapt - or refuse - when
semantics truly change.

</details>


### [84] [Enhancing Hindi NER in Low Context: A Comparative study of Transformer-based models with vs. without Retrieval Augmentation](https://arxiv.org/abs/2507.16002)
*Sumit Singh,Rohit Mishra,Uma Shanker Tiwary*

Main category: cs.CL

TL;DR: 本论文提出了一种结合印地语特有的预训练编码器与生成式模型，并利用外部知识检索增强（如Wikipedia上下文）提升印地语命名实体识别（NER）性能的方法，实验证明检索增强可以显著提升多种模型的NER效果，尤其在低资源语言和低上下文数据场景下效果明显。


<details>
  <summary>Details</summary>
Motivation: 印地语等低资源语言的NER效果受到训练数据稀缺的限制。当前主流的预训练语言模型在NER任务上尚未充分结合外部知识。论文旨在通过引入检索增强和印地语专用模型提升NER准确率。

Method: 作者利用印地语专用编码器（MuRIL、XLM-R）和生成式大模型（Llama-2-7B/70B、Llama-3-70B、GPT3.5-turbo），并通过检索外部相关上下文数据（如Wikipedia）做数据增强。部分模型进行微调，部分模型少样本直接生成NER结果，比较有无检索增强的效果差异。

Result: 应用检索增强后，MuRIL和XLM-R的Macro F1分数明显提高（如XLM-R从0.495升至0.71）。微调后的Llama2-7B表现提升显著，生成式大模型在应用检索增强后多有提升，但Llama2-70B和Llama3-70B未能充分利用检索数据。

Conclusion: 引入外部知识检索增强可以有效提升印地语NER模型性能，尤其对低上下文和低资源语言场景帮助很大，为多语言NER提供了新的方法和思路。

Abstract: One major challenge in natural language processing is named entity
recognition (NER), which identifies and categorises named entities in textual
input. In order to improve NER, this study investigates a Hindi NER technique
that makes use of Hindi-specific pretrained encoders (MuRIL and XLM-R) and
Generative Models ( Llama-2-7B-chat-hf (Llama2-7B), Llama-2-70B-chat-hf
(Llama2-70B), Llama-3-70B-Instruct (Llama3-70B) and GPT3.5-turbo), and augments
the data with retrieved data from external relevant contexts, notably from
Wikipedia. We have fine-tuned MuRIL, XLM-R and Llama2-7B with and without RA.
However, Llama2-70B, lama3-70B and GPT3.5-turbo are utilised for few-shot NER
generation. Our investigation shows that the mentioned language models (LMs)
with Retrieval Augmentation (RA) outperform baseline methods that don't
incorporate RA in most cases. The macro F1 scores for MuRIL and XLM-R are 0.69
and 0.495, respectively, without RA and increase to 0.70 and 0.71,
respectively, in the presence of RA. Fine-tuned Llama2-7B outperforms Llama2-7B
by a significant margin. On the other hand the generative models which are not
fine-tuned also perform better with augmented data. GPT3.5-turbo adopted RA
well; however, Llama2-70B and llama3-70B did not adopt RA with our retrieval
context. The findings show that RA significantly improves performance,
especially for low-context data. This study adds significant knowledge about
how best to use data augmentation methods and pretrained models to enhance NER
performance, particularly in languages with limited resources.

</details>


### [85] [Learning without training: The implicit dynamics of in-context learning](https://arxiv.org/abs/2507.16003)
*Benoit Dherin,Michael Munn,Hanna Mazzawi,Michael Wunder,Javier Gonzalvo*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）在推理阶段无需额外权重更新即可进行上下文学习的机制，并揭示了transformer模块能够隐式调整MLP层权重的原理。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）表现出强大的上下文学习能力，即推理时通过提示中的新样例快速学习新模式，但背后的工作机制仍不清楚。本文旨在解释LLM为什么能在没有新权重训练的情况下完成上下文学习。

Method: 作者通过理论分析和实验证明，transformer结构中自注意力层与MLP层的组合可使MLP层的权重根据上下文信息发生隐式的低秩调整。

Result: 实验和理论推导表明，在一定简化假设下，transformer block能自动将上下文信息转换为对MLP层权重的低秩更新。

Conclusion: 该机制可能是支持LLM进行推理时上下文学习的关键原因，有助于理解模型在未见过的新模式下也能做出有效推断。

Abstract: One of the most striking features of Large Language Models (LLM) is their
ability to learn in context. Namely at inference time an LLM is able to learn
new patterns without any additional weight update when these patterns are
presented in the form of examples in the prompt, even if these patterns were
not seen during training. The mechanisms through which this can happen are
still largely unknown. In this work, we show that the stacking of a
self-attention layer with an MLP, allows the transformer block to implicitly
modify the weights of the MLP layer according to the context. We argue through
theory and experimentation that this simple mechanism may be the reason why
LLMs can learn in context and not only during training. Specifically, we show
under mild simplifying assumptions how a transformer block implicitly
transforms a context into a low-rank weight-update of the MLP layer.

</details>


### [86] [Help Me Write a Story: Evaluating LLMs' Ability to Generate Writing Feedback](https://arxiv.org/abs/2507.16007)
*Hannah Rashkin,Elizabeth Clark,Fantine Huot,Mirella Lapata*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型(LLM)为创意写作者提供写作反馈的有效性与局限性。作者通过构建新任务、数据集和评估框架，系统评估了模型表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在文本生成领域表现优异，但它们在为写作者提供有意义反馈方面的能力尚未系统评估。本研究旨在填补该领域的空白，为写作辅助工具的发展提供依据。

Method: 作者首先构建了一个包含1,300篇被有意篡改、引入写作问题的故事测试集。随后采用主流LLM，对这些故事进行写作反馈生成，并结合自动化指标与人工评估，全面分析模型表现。

Result: 研究发现，当前LLM能在多个维度上给予具体且较为准确的写作反馈。然而，模型常常无法识别故事中最重要的写作问题，也难以准确判断何时应提供批评性或正面反馈。

Conclusion: 大型语言模型具有一定的写作反馈能力，但在识别关键问题和合理调节反馈类型方面存在明显不足。未来仍需针对反馈精度、可控性等方向开展研究以提升其实用价值。

Abstract: Can LLMs provide support to creative writers by giving meaningful writing
feedback? In this paper, we explore the challenges and limitations of
model-generated writing feedback by defining a new task, dataset, and
evaluation frameworks. To study model performance in a controlled manner, we
present a novel test set of 1,300 stories that we corrupted to intentionally
introduce writing issues. We study the performance of commonly used LLMs in
this task with both automatic and human evaluation metrics. Our analysis shows
that current models have strong out-of-the-box behavior in many respects --
providing specific and mostly accurate writing feedback. However, models often
fail to identify the biggest writing issue in the story and to correctly decide
when to offer critical vs. positive feedback.

</details>


### [87] [mRAKL: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced Languages](https://arxiv.org/abs/2507.16011)
*Hellina Hailu Nigatu,Min Li,Maartje ter Hoeve,Saloni Potdar,Sarah Chasins*

Main category: cs.CL

TL;DR: 本论文提出了一种利用检索增强生成（RAG）的方法，通过将多语言知识图谱构建任务转化为问答任务，实现了在低资源语言（提格利尼亚语和阿姆哈拉语）中知识图谱的自动扩充，并验证了该方法在与无上下文对比时的性能提升。


<details>
  <summary>Details</summary>
Motivation: 多语言知识图谱常因低资源语言数据稀缺而难以完善，传统方法难以充分利用跨语言的数据资源。因此，亟需一种能够利用高资源语言知识迁移，提升低资源语言知识图谱构建能力的新方法。

Method: 作者将mKGC任务转化为QA任务，采用检索增强生成（RAG）系统mRAKL，通过BM25检索器获取相关上下文，以头实体和关系构建问题，由模型生成尾实体作为答案。同时实验设计涵盖了利用高资源语言（英语和阿拉伯语）进行跨语言迁移的场景，并进行了消融实验分析。

Result: RAG方法在低资源语言Tigrinya和Amharic上性能优于无上下文方法。消融实验显示，在理想检索条件下，mRAKL能分别提升4.92和8.79个百分点的准确率。

Conclusion: 将多语言知识图谱构建任务转化为QA并结合RAG方法，可显著提升低资源语言下的知识图谱扩充活动，尤其是在理想检索支持下，性能提升明显；高资源语言的迁移对于低资源语言的改进效果较好。

Abstract: Knowledge Graphs represent real-world entities and the relationships between
them. Multilingual Knowledge Graph Construction (mKGC) refers to the task of
automatically constructing or predicting missing entities and links for
knowledge graphs in a multilingual setting. In this work, we reformulate the
mKGC task as a Question Answering (QA) task and introduce mRAKL: a
Retrieval-Augmented Generation (RAG) based system to perform mKGC. We achieve
this by using the head entity and linking relation in a question, and having
our model predict the tail entity as an answer. Our experiments focus primarily
on two low-resourced languages: Tigrinya and Amharic. We experiment with using
higher-resourced languages Arabic and English for cross-lingual transfer. With
a BM25 retriever, we find that the RAG-based approach improves performance over
a no-context setting. Further, our ablation studies show that with an idealized
retrieval system, mRAKL improves accuracy by 4.92 and 8.79 percentage points
for Tigrinya and Amharic, respectively.

</details>


### [88] [AutoMeet: a proof-of-concept study of genAI to automate meetings in automotive engineering](https://arxiv.org/abs/2507.16054)
*Simon Baeuerle,Max Radyschevski,Ulrike Pado*

Main category: cs.CL

TL;DR: 本研究开发了一个基于生成式人工智能（genAI）的端到端会议记录自动化流程，并在真实工程部门进行了测试。


<details>
  <summary>Details</summary>
Motivation: 大型组织中会议占据大量时间，且会议记录和知识分散，检索困难。作者希望用AI改善会议知识管理，提高效率。

Method: 实现了会议录音、genAI自动生成会议纪要，并通过聊天机器人接口实现可搜索性。在实际工程部门部署并通过问卷收集团队反馈，关注伦理和技术两方面。

Result: 用户反馈指出genAI可以显著减少会议相关工作量，技术问题基本解决，但组织和伦理问题仍关键。

Conclusion: genAI有望显著提升会议知识管理效率，但其伦理与组织层面的应用需重点关注，以确保成功使用。

Abstract: In large organisations, knowledge is mainly shared in meetings, which takes
up significant amounts of work time. Additionally, frequent in-person meetings
produce inconsistent documentation -- official minutes, personal notes,
presentations may or may not exist. Shared information therefore becomes hard
to retrieve outside of the meeting, necessitating lengthy updates and
high-frequency meeting schedules.
  Generative Artificial Intelligence (genAI) models like Large Language Models
(LLMs) exhibit an impressive performance on spoken and written language
processing. This motivates a practical usage of genAI for knowledge management
in engineering departments: using genAI for transcribing meetings and
integrating heterogeneous additional information sources into an easily usable
format for ad-hoc searches.
  We implement an end-to-end pipeline to automate the entire meeting
documentation workflow in a proof-of-concept state: meetings are recorded and
minutes are created by genAI. These are further made easily searchable through
a chatbot interface. The core of our work is to test this genAI-based software
tooling in a real-world engineering department and collect extensive survey
data on both ethical and technical aspects. Direct feedback from this
real-world setup points out both opportunities and risks: a) users agree that
the effort for meetings could be significantly reduced with the help of genAI
models, b) technical aspects are largely solved already, c) organizational
aspects are crucial for a successful ethical usage of such a system.

</details>


### [89] [Deep Researcher with Test-Time Diffusion](https://arxiv.org/abs/2507.16075)
*Rujun Han,Yanfei Chen,Zoey CuiZhu,Lesly Miculicich,Guan Sun,Yuanjun Bi,Weiming Wen,Hui Wan,Chunfeng Wen,Solène Maître,George Lee,Vishy Tirumalashetty,Emily Xue,Zizhao Zhang,Salem Haykal,Burak Gokturk,Tomas Pfister,Chen-Yu Lee*

Main category: cs.CL

TL;DR: 提出了TTD-DR框架，将研究报告生成过程建模为扩散过程，通过迭代修正初稿，实现高质量长文本报告生成，性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的research agent在复杂长文本报告生成中性能瓶颈、失去上下文与信息。受人类研究的启发，希望模仿搜寻—推理—修正迭代过程，提高报告生成的连贯性和准确性。

Method: 提出Test-Time Diffusion Deep Researcher (TTD-DR) 框架：以可更新初稿为生成基础，通过迭代"去噪"过程持续完善，并在每一步集成外部信息，增强过程兼容自进化算法，从而优化agent端到端生成流程。

Result: 实验显示，TTD-DR在多个需复杂搜索与多跳推理的基准数据集上，均显著优于现有深度研究智能体，取得领先的SOTA结果。

Conclusion: 提出的TTD-DR通过草稿为核心的扩散式迭代生成策略，有效提升了研究报告的连贯性、信息完整性与生成质量，是复杂自动研究任务的新突破。

Abstract: Deep research agents, powered by Large Language Models (LLMs), are rapidly
advancing; yet, their performance often plateaus when generating complex,
long-form research reports using generic test-time scaling algorithms. Drawing
inspiration from the iterative nature of human research, which involves cycles
of searching, reasoning, and revision, we propose the Test-Time Diffusion Deep
Researcher (TTD-DR). This novel framework conceptualizes research report
generation as a diffusion process. TTD-DR initiates this process with a
preliminary draft, an updatable skeleton that serves as an evolving foundation
to guide the research direction. The draft is then iteratively refined through
a "denoising" process, which is dynamically informed by a retrieval mechanism
that incorporates external information at each step. The core process is
further enhanced by a self-evolutionary algorithm applied to each component of
the agentic workflow, ensuring the generation of high-quality context for the
diffusion process. This draft-centric design makes the report writing process
more timely and coherent while reducing information loss during the iterative
search process. We demonstrate that our TTD-DR achieves state-of-the-art
results on a wide array of benchmarks that require intensive search and
multi-hop reasoning, significantly outperforming existing deep research agents.

</details>


### [90] [The Prompt Makes the Person(a): A Systematic Evaluation of Sociodemographic Persona Prompting for Large Language Models](https://arxiv.org/abs/2507.16076)
*Marlene Lutz,Indira Sen,Georg Ahnert,Elisa Rogers,Markus Strohmaier*

Main category: cs.CL

TL;DR: 本论文系统研究了大语言模型（LLM）在模拟不同社会人口群体观点时，角色设定提示（persona prompt）的策略对输出结果的影响。发现LLM在模拟边缘化群体时表现较差，但合理的提示设计能改善模拟效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在研究和应用中用于模拟不同社会群体观点，如何科学、真实地设定角色提示成为关键，但提示方式可能引入偏差，影响结果可靠性。作者希望揭示不同提示策略对模拟准确性的影响。

Method: 作者选用五个开源LLM，设计了多种角色扮演提示格式与人口特征引导策略，覆盖15个交叉社会人口群体，在开放及封闭题任务中系统对比模拟表现。

Result: LLM在模拟非二元、拉美裔、中东等边缘化群体时表现不佳；不同人口特征引导和角色扮演格式极大影响模拟效果。通过访谈式格式及基于姓名的引导可以降低刻板印象并提升对齐度。小模型（如OLMo-2-7B）在这方面甚至优于大模型（如Llama-3.3-70B）。

Conclusion: 合理设计角色扮演提示（如选用访谈风格、姓名引导）有助于提高LLM对社会人口群体的模拟真实度，为相关领域提供实践指导。

Abstract: Persona prompting is increasingly used in large language models (LLMs) to
simulate views of various sociodemographic groups. However, how a persona
prompt is formulated can significantly affect outcomes, raising concerns about
the fidelity of such simulations. Using five open-source LLMs, we
systematically examine how different persona prompt strategies, specifically
role adoption formats and demographic priming strategies, influence LLM
simulations across 15 intersectional demographic groups in both open- and
closed-ended tasks. Our findings show that LLMs struggle to simulate
marginalized groups, particularly nonbinary, Hispanic, and Middle Eastern
identities, but that the choice of demographic priming and role adoption
strategy significantly impacts their portrayal. Specifically, we find that
prompting in an interview-style format and name-based priming can help reduce
stereotyping and improve alignment. Surprisingly, smaller models like OLMo-2-7B
outperform larger ones such as Llama-3.3-70B. Our findings offer actionable
guidance for designing sociodemographic persona prompts in LLM-based simulation
studies.

</details>


### [91] [Efficient Compositional Multi-tasking for On-device Large Language Models](https://arxiv.org/abs/2507.16083)
*Ondrej Bohdal,Mete Ozay,Jijoong Moon,Kyeng-Hun Lee,Hyeonmok Ko,Umberto Michieli*

Main category: cs.CL

TL;DR: 本文提出并研究了在资源受限设备（如移动端）上，利用适配器参数使大语言模型（LLMs）能够同时执行多个文本相关任务的方案。针对复合型多任务的挑战，作者设计了相关基准任务，并提出了高效的任务合并与校准方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在多任务合并（如NLP领域）通常仅支持单测单任务，现实应用中经常需要复杂的复合多任务处理（如同时翻译和摘要）。移动端与资源受限场景对此提出了更高的效率与适用性要求。

Method: 作者提出了4个与实际相关的复合多任务基准，并提出了Learnable Calibration方法，能在资源有限情况下有效合并适配器参数，使模型高效地执行多个任务。

Result: 在提出的基准上，所提出的方法在有限计算资源下实现了高效且性能优良的多任务处理。

Conclusion: 该研究为在现实、多任务、资源受限场景下推广和应用LLM打下了基础，提高了模型的多任务能力和实际部署的适用性。

Abstract: Adapter parameters provide a mechanism to modify the behavior of machine
learning models and have gained significant popularity in the context of large
language models (LLMs) and generative AI. These parameters can be merged to
support multiple tasks via a process known as task merging. However, prior work
on merging in LLMs, particularly in natural language processing, has been
limited to scenarios where each test example addresses only a single task. In
this paper, we focus on on-device settings and study the problem of text-based
compositional multi-tasking, where each test example involves the simultaneous
execution of multiple tasks. For instance, generating a translated summary of a
long text requires solving both translation and summarization tasks
concurrently. To facilitate research in this setting, we propose a benchmark
comprising four practically relevant compositional tasks. We also present an
efficient method (Learnable Calibration) tailored for on-device applications,
where computational resources are limited, emphasizing the need for solutions
that are both resource-efficient and high-performing. Our contributions lay the
groundwork for advancing the capabilities of LLMs in real-world multi-tasking
scenarios, expanding their applicability to complex, resource-constrained use
cases.

</details>


### [92] [BIDWESH: A Bangla Regional Based Hate Speech Detection Dataset](https://arxiv.org/abs/2507.16183)
*Azizul Hakim Fayaz,MD. Shorif Uddin,Rayhan Uddin Bhuiyan,Zakia Sultana,Md. Samiul Islam,Bidyarthi Paul,Tashreef Muhammad,Shahriar Manzoor*

Main category: cs.CL

TL;DR: 本文提出了BIDWESH，这是首个涵盖孟加拉主要方言的多方言仇恨言论数据集，旨在改善对非标准孟加拉语仇恨言论的检测。


<details>
  <summary>Details</summary>
Motivation: 现有针对孟加拉语的仇恨言论检测主要集中在标准语，无法处理内容丰富、文化特性明显的地区方言，导致检测能力有限，偏向性审核，以及大量有害内容未被发现。

Method: 通过从BD-SHS语料库中抽取9,183条实例，翻译并注释为Barishal、Noakhali和Chittagong三大方言，每条数据经过人工校验，并标注了仇恨存在性、类型（诽谤、性别、宗教、暴力号召）和目标群体（个人、男性、女性、群体），确保语境和语言准确性。

Result: 生成了一个包含丰富方言特性、标注全面且均衡的多方言仇恨言论数据集。

Conclusion: BIDWESH为开发方言敏感的自然语言处理工具、实现公平且具语境意识的内容审核提供了基础，具有重要的学术与实际应用价值。

Abstract: Hate speech on digital platforms has become a growing concern globally,
especially in linguistically diverse countries like Bangladesh, where regional
dialects play a major role in everyday communication. Despite progress in hate
speech detection for standard Bangla, Existing datasets and systems fail to
address the informal and culturally rich expressions found in dialects such as
Barishal, Noakhali, and Chittagong. This oversight results in limited detection
capability and biased moderation, leaving large sections of harmful content
unaccounted for. To address this gap, this study introduces BIDWESH, the first
multi-dialectal Bangla hate speech dataset, constructed by translating and
annotating 9,183 instances from the BD-SHS corpus into three major regional
dialects. Each entry was manually verified and labeled for hate presence, type
(slander, gender, religion, call to violence), and target group (individual,
male, female, group), ensuring linguistic and contextual accuracy. The
resulting dataset provides a linguistically rich, balanced, and inclusive
resource for advancing hate speech detection in Bangla. BIDWESH lays the
groundwork for the development of dialect-sensitive NLP tools and contributes
significantly to equitable and context-aware content moderation in low-resource
language settings.

</details>


### [93] [Do Large Language Models Have a Planning Theory of Mind? Evidence from MindGames: a Multi-Step Persuasion Task](https://arxiv.org/abs/2507.16196)
*Jared Moore,Ned Cooper,Rasmus Overmark,Beba Cibralic,Nick Haber,Cameron R. Jones*

Main category: cs.CL

TL;DR: 本文介绍了MindGames，一种针对大型语言模型（LLMs）和人类对‘规划理论心智’（Planning Theory of Mind, PToM）能力的新测试任务，发现人类在该任务显著优于LLM，而LLM在不需推测心理状态的规划任务中表现更佳，表明当前LLM的社交推理能力与人类尚有明显差距。


<details>
  <summary>Details</summary>
Motivation: 现有理论心智（ToM）研究多集中于旁观者角度，仅让参与者预测或解释他人行为。但在人类社会智能中，ToM还用于实际行动规划和影响他人观念。该文动机是检验LLM是否能像人类一样，将ToM用于动态规划和说服情景。

Method: 提出新任务MindGames，要求参与者推断对方的信念和欲望，并据此说服对方改变行为。设置两种条件：一种需推测心智状态（PToM任务），一种只需规划但已知对方偏好（对照组）。对比人类和LLM（o1-preview版本）的表现。

Result: 在人类与LLM的PToM任务比较中，人类表现优于LLM（高11%，p=0.006）。而在仅需规划、不需推理心理的对照任务中，LLM胜于人类。分析认为差异来自人类更擅长隐式因果建模和主动询问偏好。

Conclusion: 当前LLM虽然在明确已知条件下善于规划，但在人类式社交推理和复杂心智状态推断方面仍有较大差距，需要进一步发展。

Abstract: Recent evidence suggests Large Language Models (LLMs) display Theory of Mind
(ToM) abilities. Most ToM experiments place participants in a spectatorial
role, wherein they predict and interpret other agents' behavior. However, human
ToM also contributes to dynamically planning action and strategically
intervening on others' mental states. We present MindGames: a novel `planning
theory of mind' (PToM) task which requires agents to infer an interlocutor's
beliefs and desires to persuade them to alter their behavior. Unlike previous
evaluations, we explicitly evaluate use cases of ToM. We find that humans
significantly outperform o1-preview (an LLM) at our PToM task (11% higher;
$p=0.006$). We hypothesize this is because humans have an implicit causal model
of other agents (e.g., they know, as our task requires, to ask about people's
preferences). In contrast, o1-preview outperforms humans in a baseline
condition which requires a similar amount of planning but minimal mental state
inferences (e.g., o1-preview is better than humans at planning when already
given someone's preferences). These results suggest a significant gap between
human-like social reasoning and LLM abilities.

</details>


### [94] [WakenLLM: A Fine-Grained Benchmark for Evaluating LLM Reasoning Potential and Reasoning Process Stability](https://arxiv.org/abs/2507.16199)
*Zipeng Ling,Yuehao Tang,Shuliang Liu,Junqi Yang,Shenghong Fu,Yao Wan,Kejia Huang,Zhichao Hou,Xuming Hu*

Main category: cs.CL

TL;DR: 本文提出了“模糊认知（Vague Perception）”现象，旨在区分大型语言模型（LLMs）输出“Unknown”标签时，是模型本身能力不足还是问题本就无法判定。作者建立了框架，定量分析“Unknown”原因，并尝试通过引导刺激将这些未知转化为“Known”或本质上无法确定的结果，从而更清楚地了解LLMs推理极限和改进空间。


<details>
  <summary>Details</summary>
Motivation: 大量评测只关注LLMs输出“Unknown”是否诚实，而忽视了为何会出现“Unknown”，导致无法区分“无法判断的问题”与“模型能力不足的可解问题”，从而影响对模型本质推理能力的认知。

Method: 提出了一套框架，定量分析LLMs输出“Unknown”标签的成因，并用引导（guided stimulation）测试这些未知响应能否被转化为正确或本质未知的答案。结合理论推理任务准确率，并利用多种方法检验模型是否能达到基线框架下的准确率。

Result: 该框架能够分离LLM输出“Unknown”的不同原因，量化模型推理能力与模糊认知现象，并实证判断LLMs推理困境是源自能力瓶颈还是问题本质。

Conclusion: 区分LLMs“Unknown”输出的真实原因有助于更准确评估和提升其推理能力，为解决模糊认知问题提供了新视角和方法。

Abstract: Large Language Models (LLMs) frequently output the label \emph{Unknown}, yet
current evaluations focus almost exclusively on whether such answers are
\emph{honest} rather than why they arise. This blurs two distinct cases: (i) an
input that is genuinely indeterminate and (ii) a solvable problem that the
model fails to resolve. We call this phenomenon \emph{Vague Perception}. And
thus we introduce a framework that quantifies the proportion of \emph{Unknown}
responses attributable to model incapacity and tests whether guided stimulation
can convert them into either correct (\emph{Known}) or intrinsically
indeterminate outcomes. By separating these sources of uncertainty, our method
provides a clearer picture of LLM reasoning limits and their potential for
improvement. As we get a theoretical accuracy of reasoning task on different
LLMs, we apply different methods to test whether the model can reach the
accuracy given a baseline framework. Our work is meaningful in exploring the
true reasoning ability of LLMs and providing a new perspective on solving the
\emph{Vague Perception} phenomenon.

</details>


### [95] [Towards Compute-Optimal Many-Shot In-Context Learning](https://arxiv.org/abs/2507.16217)
*Shahriar Golchin,Yanfei Chen,Rujun Han,Manan Gandhi,Tianli Yu,Swaroop Mishra,Mihai Surdeanu,Rishabh Agarwal,Chen-Yu Lee,Tomas Pfister*

Main category: cs.CL

TL;DR: 本文提出了两种高效的样例选择策略，有效提升了long-context大语言模型在many-shot in-context learning任务中的性能，并大幅降低了推理成本。


<details>
  <summary>Details</summary>
Motivation: 随着长上下文大语言模型的发展，模型能够处理百万级token输入，这使得many-shot in-context learning成为可能。但由于推理成本、可重复利用计算效率、规模化后与随机选择性能相近等原因，实际中常用随机选择示例，未充分利用更优选择带来的潜在提升。

Method: 提出两种样例选择策略：1. 混合策略，少量和测试样本相似的示例与大量随机（可缓存）示例结合输入；2. 用k-means聚类在测试样本表示上选中心点，将随机示例替换为聚类中心代表的示例，进一步提升相关性和多样性。

Result: 在多个数据集、使用Gemini Pro和Flash模型实验表明，两种策略均优于完全随机选择，并可支持缓存，推理成本最多降低近十倍，且性能达到了或超过现有最优方法。

Conclusion: 通过合理调整示例选择策略，可以在many-shot ICL中平衡性能与推理成本，实现低代价高精度的推理，所提策略具备实际应用价值。

Abstract: Long-context large language models (LLMs) are able to process inputs
containing up to several million tokens. In the scope of in-context learning
(ICL), this translates into using hundreds/thousands of demonstrations in the
input prompt, enabling many-shot ICL. In practice, a fixed set of
demonstrations is often selected at random in many-shot settings due to (1)
high inference costs, (2) the benefits of caching and reusing computations, and
(3) the similar performance offered by this strategy compared to others when
scaled. In this work, we propose two straightforward strategies for
demonstration selection in many-shot ICL that improve performance with minimal
computational overhead. Our first method combines a small number of
demonstrations, selected based on their similarity to each test sample, with a
disproportionately larger set of random demonstrations that are cached. The
second strategy improves the first by replacing random demonstrations with
those selected using centroids derived from test sample representations via
k-means clustering. Our experiments with Gemini Pro and Flash across several
datasets indicate that our strategies consistently outperform random selection
and surpass or match the most performant selection approach while supporting
caching and reducing inference cost by up to an order of magnitude. We also
show that adjusting the proportion of demonstrations selected based on
different criteria can balance performance and inference cost in many-shot ICL.

</details>


### [96] [FinResearchBench: A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents](https://arxiv.org/abs/2507.16248)
*Run Sun,Zuo Bai,Wentao Zhang,Yuxiang Zhang,Li Zhao,Shan Sun,Zhengwen Qiu*

Main category: cs.CL

TL;DR: 本文提出了一个专为金融研究AI代理设计的评测框架FinResearchBench，有效评估研究代理能力，填补了领域内评估工具的空白。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在诸如STEM、软件开发、金融等专业领域应用广泛，尤其是能完成复杂长周期任务的研究代理。但缺少针对性强、自动化的评估框架，特别是在金融研究任务上存在评估空白。

Method: 本文提出FinResearchBench，基于逻辑树的Agent-as-a-Judge体系，对代理的研究结果进行逻辑结构提取，并自动进行全面、稳健的评估。系统覆盖金融领域7类常见任务，囊括70个典型研究问题。

Result: FinResearchBench实现了金融研究代理系统能力的自动化、多维度评测，有助于科研工作者客观比较和提升代理能力。

Conclusion: FinResearchBench是第一个面向金融研究代理且方法创新的评测框架，为该领域AI代理能力的系统研究和拓展提供了有效工具。

Abstract: Recently, AI agents are rapidly evolving in intelligence and widely used in
professional research applications, such as STEM, software development,
finance, etc. Among these AI agents, deep research agent is a key category as
it can perform long-horizon tasks and solve problems of greater complexity.
However, there are few evaluation frameworks and benchmarks that systematically
and automatically investigate the capabilities of these research agents.
Furthermore, financial research problems have distinct complexity and subtlety.
To fill in the gap, we propose FinResearchBench, which is a logic tree based
Agent-as-a-Judge and targets specifically for the financial research agents. It
provides a comprehensive and automatic assessment of the research agents across
7 key types of tasks in the financial research domain. The contributions of
this work are two-folded: (1) the first and innovative Agent-as-a-Judge system
that extracts the logic tree of the research outcome and uses it as the
intermediate information to present a comprehensive, reliable and robust
evaluation; (2) finance oriented that it covers 70 typical financial research
questions, spreading across 7 frequently encountered types of tasks in the
domain.

</details>


### [97] [Efficient RL for optimizing conversation level outcomes with an LLM-based tutor](https://arxiv.org/abs/2507.16252)
*Hyunji Nam,Omer Gottesman,Amy Zhang,Dean Foster,Emma Brunskill,Lyle Ungar*

Main category: cs.CL

TL;DR: 该论文提出了一种增强大语言模型（LLMs）作为数学辅导工具的方法，通过引入学生的低维潜在状态表示和优化长期决策策略，实现更好的多轮对话教学效果，并在模拟任务中显示出较现有方法更优的长期指导能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于RLHF的LLM主要依据单轮对话中的人类喜好进行优化，难以适应如在线数学辅导这类强调多轮对话和长期目标的应用场景，因此需要新的方法提升模型在多轮任务中的长期教学能力。

Method: 将对话历史编码为学生的低维潜在状态，基于该状态优化长期策略，选择高层次行动，以指导学生逐步自主解题。该方法比端到端训练输出下一句的传统方法更加轻量，计算资源消耗更低。

Result: 实验证明，所提出的潜在状态建模与长期策略优化方法在LLM模拟的辅导任务中，实现了比直接Prompt更优的教学引导和长期学习成果。

Conclusion: 通过引入潜在状态和长期决策优化，LLM辅导工具能够更好地引导学生自主解决数学问题，提高了多轮对话中的教学有效性，并兼具较低的计算资源需求。

Abstract: Large language models (LLMs) built on existing reinforcement learning with
human feedback (RLHF) frameworks typically optimize responses based on
immediate turn-level human preferences. However, this approach falls short in
multi-turn dialogue settings, such as online math tutoring. We propose a method
to enhance LLM-based tutors by representing the dialogue history with a
lower-dimensional latent state representation of a student and optimizing a
long-term policy to determine high-level actions based on the latent state. The
goal is to better align the tutor's behavior with the long-term objective of
guiding the student towards solving a target math problem on their own. Our
model is lightweight, requiring less computational resources than prior work of
training the tutor policy end-to-end to directly output the tutor's next
utterance. Our experiment results demonstrate that these modifications lead to
improved long-term outcomes compared to prompting in LLM-simulated tutoring
tasks.

</details>


### [98] [iShumei-Chinchunmei at SemEval-2025 Task 4: A balanced forgetting and retention multi-task framework using effective unlearning loss](https://arxiv.org/abs/2507.16263)
*Yujian Sun,Tian Li*

Main category: cs.CL

TL;DR: 本文针对大语言模型中“遗忘”敏感或不合规数据的问题，提出了一种新的可控遗忘损失方法，并在SemEval 2025 Task 4竞赛中取得了第五名。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，如何在有限计算资源下让模型遗忘预训练过程中记住的敏感或不合规数据，成为当前研究关注的难题。

Method: 提出了一种更可控的遗忘损失（Effective Unlearning Loss），并探索将其与多种技术进行融合，以实现更加高效和可控的遗忘效果。

Result: 在SemEval 2025 Task 4竞赛中，该系统在遗忘效果和能力保持方面取得了良好表现，最终排名第五。

Conclusion: 采用可控遗忘损失能更高效、精准地让大语言模型“遗忘”敏感内容，为后续相关研究提供了新的方法和基准。

Abstract: As the Large Language Model (LLM) gains widespread adoption, increasing
attention has been given to the challenge of making LLM forget non-compliant
data memorized during its pre-training. Machine Unlearning focuses on
efficiently erasing sensitive information from LLM under limited computational
resources. To advance research in this area, SemEval 2025 Task 4: "Unlearning
Sensitive Content from Large Language Models" introduces three unlearning
datasets and establishes a benchmark by evaluating both forgetting
effectiveness and the preservation of standard capabilities. In this work, we
propose a more controllable forgetting loss, Effective Unlearning Loss, and
explore its integration with various techniques to achieve more efficient and
controlled unlearning. Our system ultimately ranked 5th on the competition
leaderboard.

</details>


### [99] [Beyond Isolated Dots: Benchmarking Structured Table Construction as Deep Knowledge Extraction](https://arxiv.org/abs/2507.16271)
*Tianyun Zhong,Guozhao Mo,Yanjiang Liu,Yihan Chen,Lingdi Kong,Xuanang Chen,Yaojie Lu,Hongyu Lin,Ben He,Le Sun*

Main category: cs.CL

TL;DR: 提出了Arranged and Organized Extraction Benchmark（AOE）数据集，用于系统性评测LLMs从碎片化文档中结构化抽取信息到表格中的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然能处理复杂文档，但结果常常杂乱无章，难以追溯。缺乏能系统性评估其信息组织能力的基准数据集，限制了技术进步。

Method: 构建了AOE基准，支持中英双语，包含不同长度文档和11个跨三大领域的任务，要求模型针对具体查询生成上下文相关表格格式。对多种主流开源及商业LLM进行系统评测。

Result: 即便是最先进的LLM，在该基准下的表现也显著受限，难以高效完成信息抽取及结构化重组任务。

Conclusion: 现有LLM在复杂文档结构化抽取能力上仍有很大提升空间。AOE为后续改进提供了统一的评测基准，有助于推动领域发展。

Abstract: With the emergence of large language models (LLMs), there is an expectation
that LLMs can effectively extract explicit information from complex real-world
documents (e.g., papers, reports). However, most LLMs generate paragraph-style
answers that are chaotic, disorganized, and untraceable. To bridge this gap, we
introduce the Arranged and Organized Extraction Benchmark (AOE), a new
bilingual benchmark with data and documents of varying lengths designed to
systematically evaluate the ability of LLMs to comprehend fragmented documents
and reconstruct isolated information into one organized table. Unlike
conventional text-to-table tasks, which rely on fixed schema and narrow task
domains, AOE includes 11 carefully crafted tasks across three diverse domains,
requiring models to generate context-specific schema tailored to varied input
queries. In the experiment, we evaluated both open-source and closed-source
state-of-the-art LLMs. The results show that even the most advanced models
struggled significantly. The benchmark is available at
https://huggingface.co/datasets/tianyumyum/AOE.

</details>


### [100] [Language Detection by Means of the Minkowski Norm: Identification Through Character Bigrams and Frequency Analysis](https://arxiv.org/abs/2507.16284)
*Paul-Andrei Pogăcean,Sanda-Maria Avram*

Main category: cs.CL

TL;DR: 本文提出并测试了一种基于单字和双字频率排序的传统数学算法，用于语言识别，在多种长度和时期的文本中，均表现出高准确率，对短文本准确率超80%，对长文本和老式文本准确率达100%。


<details>
  <summary>Details</summary>
Motivation: 近年来，尽管AI模型使得语言识别取得突破，但传统的非AI识别方法逐渐被忽视。作者旨在验证经典的统计方法在当前多样化文本下仍然有效。

Method: 研究基于既有语言学成果，利用单字（monogram）和双字（bigram）频率排名，设计数学算法用于检测不同长度、时期和体裁（包括短篇小说、童话、诗歌）文本的语言。

Result: 当文本长度小于150字符时，该方法准确率超过80%；而对更长的文本和年代久远的文本，准确率甚至达到100%。

Conclusion: 经典的基于频率的语言识别办法仍然具备高效性和可扩展性，是AI方法之外值得关注的优选方案。

Abstract: The debate surrounding language identification has gained renewed attention
in recent years, especially with the rapid evolution of AI-powered language
models. However, the non-AI-based approaches to language identification have
been overshadowed. This research explores a mathematical implementation of an
algorithm for language determinism by leveraging monograms and bigrams
frequency rankings derived from established linguistic research. The datasets
used comprise texts varying in length, historical period, and genre, including
short stories, fairy tales, and poems. Despite these variations, the method
achieves over 80\% accuracy on texts shorter than 150 characters and reaches
100\% accuracy for longer texts and older writings. These results demonstrate
that classical frequency-based approaches remain effective and scalable
alternatives to AI-driven models for language detection.

</details>


### [101] [SpeLLM: Character-Level Multi-Head Decoding](https://arxiv.org/abs/2507.16323)
*Amit Ben-Artzy,Roy Schwartz*

Main category: cs.CL

TL;DR: 提出SpeLLM，通过拆分输出头，将输出词表按字符级预测，解决大词表下线性输出层瓶颈，实现大词表LLM高效输出，同时减少运行成本。


<details>
  <summary>Details</summary>
Motivation: 当前大模型（LLM）为减少输入序列长度需扩大词表，但输出层参数随词表线性增长，限制了词表扩展能力，导致大规模应用受限。

Method: SpeLLM方法将输入和输出词表解耦，采用多个并行线性输出头，每个头独立预测一个字符，从而以较小的线性头实现更大的输出空间。训练时使用自蒸馏技术，将标准LLM转化为SpeLLM。

Result: 在四个预训练LLM上的实验证明，SpeLLM变体在下游任务表现与原始模型相当，但平均可减少5.1%的运行时间。

Conclusion: SpeLLM为扩展LLM词表、提升对小语种和特定领域的支持、降低大模型推理成本提供了一种实用新途径。

Abstract: Scaling LLM vocabulary is often used to reduce input sequence length and
alleviate attention's quadratic cost. Yet, current LLM architectures impose a
critical bottleneck to this procedure: the output projection layer scales
linearly with vocabulary size, rendering substantial expansion impractical. We
propose SpeLLM, a method that decouples input and output vocabularies by
predicting character-level strings through multiple output heads. In SpeLLM,
each of the $k$ linear heads predicts a single character simultaneously,
enabling the model to represent a much larger output space using smaller,
independent linear heads. We present a self-distillation approach for
converting a standard LLM to a SpeLLM. Our experiments with four pre-trained
LLMs show their SpeLLM variants achieve competitive performance on downstream
tasks while reducing runtime by 5.1% on average across models. Our approach
provides a potential avenue for reducing LLM costs, while increasing support
for underrepresented languages and domains.

</details>


### [102] [Re:Form -- Reducing Human Priors in Scalable Formal Software Verification with RL in LLMs: A Preliminary Study on Dafny](https://arxiv.org/abs/2507.16331)
*Chuanhao Yan,Fengdi Che,Xuhan Huang,Xu Xu,Xin Li,Yizhi Li,Xingwei Qu,Jingzhe Shi,Zhuangzhuang He,Chenghua Lin,Yaodong Yang,Binhang Yuan,Hang Zhao,Yu Qiao,Bowen Zhou,Jie Fu*

Main category: cs.CL

TL;DR: 本文提出了一种以形式化语言（如Dafny）为基础的LLM训练和验证方案，通过自动化数据整理管道和与形式化验证器结合的强化学习，极大减少了对人工先验的依赖，实现了代码生成和验证的可扩展性。使用DafnyComp基准测试，所提出方法在不同规模模型上均超越了商业化模型，表现出更强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前基于自然语言的LLM在利用强化学习进行训练时，验证机制难以可靠和扩展。而现有大型闭源模型程序生成能力有限，难以自动验证。形式化语言（如Dafny）提供了一个可自动验证推理过程的可能，关键在于其能够实现可扩展和可靠的软件形式化验证，但依赖人工注释和先验的方式在复杂任务下极其低效。

Method: 以Dafny为实验环境，提出了（1）自动化、可扩展的数据整理流程；（2）结合形式化验证器反馈的强化学习策略。（3）设计了DafnyComp基准，评估模型的组合能力和规约能力。同时在有监督微调（SFT）阶段，对小模型进行训练，使其生成符合语法且可验证的Dafny代码。

Result: 小规模模型（如0.5B参数量）经SFT后可生成合格且能被验证的代码，性能超过了现有的闭源商业模型。通过配合正则化的强化学习训练，进一步提升了模型性能，实现在DafnyComp基准上的更强泛化与更优表现，优于所有主要基线。

Conclusion: 形式化语言空间中的自动化数据处理和与形式化验证反馈结合的RL，有效减少了人工先验依赖，提高了可扩展性和可靠性。在DafnyComp基准上实验证明，该方法不仅提升了模型代码生成和验证能力，同时具有更强的泛化能力，相比传统模型有明显优势。

Abstract: Existing informal language-based (e.g., human language) Large Language Models
(LLMs) trained with Reinforcement Learning (RL) face a significant challenge:
their verification processes, which provide crucial training signals, are
neither reliable nor scalable. In fact, the prevalent large proprietary models
could hardly generate verifiable programs. A promising yet largely uncharted
alternative is formal language-based reasoning. Grounding LLMs in rigorous
formal systems where generative models operate in formal language spaces (e.g.,
Dafny) enables the automatic and mathematically provable verification of their
reasoning processes and outcomes. This capability is pivotal for achieving
large-scale, reliable formal software verification. It is a common practice to
employ human-annotated chain-of-thought and other human priors to induce the
reasoning and coding capabilities of LLMs. Unfortunately, it becomes
unacceptably all-consuming to provide such priors for supervising complex
programming tasks. In this work, we systematically explore ways to reduce human
priors with the formal language, Dafny, as the main environment for our pilot
study. Our pipeline mainly relies on introducing an automatic and scalable data
curation pipeline, and careful RL designs integrated with feedback from the
formal language verifier. We introduce DafnyComp, a benchmark of compositional
formal programs with auto-formalized specifications for specification
reasoning. Our supervised fine-tuning (SFT) stage enables even small models
(e.g., 0.5B) to generate syntactically valid and verifiable Dafny code,
surpassing proprietary models. RL with regularization further improves
performance, achieving stronger generalization to out-of-domain tasks and
outperforming all strong baselines on the challenging DafnyComp benchmark.

</details>


### [103] [GG-BBQ: German Gender Bias Benchmark for Question Answering](https://arxiv.org/abs/2507.16410)
*Shalaka Satheesh,Katrin Klug,Katharina Beckh,Héctor Allende-Cid,Sebastian Houben,Teena Hassan*

Main category: cs.CL

TL;DR: 本文针对德语大语言模型的性别偏见进行了评估，采用修正翻译后的英语基准测试集，并发现所有模型均表现出性别偏见。


<details>
  <summary>Details</summary>
Motivation: 现有NLP公平性评估多针对英语，且基于机器翻译的基准数据在有语法性别的德语等语种中存在不足，因此亟需高质量德语性别偏见评估数据集。

Method: 将英语性别偏见问答基准集机器翻译为德语，经语言专家人工校正得到高质量德语数据集，并包含两类子集（性别群体词和专有名词）。利用该数据集评测多款德语大语言模型的准确率和偏见分数。

Result: 所有受评德语大语言模型都在该基准集上表现出性别偏见，既有顺应也有反对社会刻板印象的情况。

Conclusion: 仅依赖机器翻译无法构建有效德语偏见评测集，人工校正至关重要。德语大模型普遍存在性别偏见，为后续模型提升公平性指明了改进方向。

Abstract: Within the context of Natural Language Processing (NLP), fairness evaluation
is often associated with the assessment of bias and reduction of associated
harm. In this regard, the evaluation is usually carried out by using a
benchmark dataset, for a task such as Question Answering, created for the
measurement of bias in the model's predictions along various dimensions,
including gender identity. In our work, we evaluate gender bias in German Large
Language Models (LLMs) using the Bias Benchmark for Question Answering by
Parrish et al. (2022) as a reference. Specifically, the templates in the gender
identity subset of this English dataset were machine translated into German.
The errors in the machine translated templates were then manually reviewed and
corrected with the help of a language expert. We find that manual revision of
the translation is crucial when creating datasets for gender bias evaluation
because of the limitations of machine translation from English to a language
such as German with grammatical gender. Our final dataset is comprised of two
subsets: Subset-I, which consists of group terms related to gender identity,
and Subset-II, where group terms are replaced with proper names. We evaluate
several LLMs used for German NLP on this newly created dataset and report the
accuracy and bias scores. The results show that all models exhibit bias, both
along and against existing social stereotypes.

</details>


### [104] [PromptAL: Sample-Aware Dynamic Soft Prompts for Few-Shot Active Learning](https://arxiv.org/abs/2507.16424)
*Hui Xiang,Jinqiao Shi,Ting Zhang,Xiaojie Zhao,Yong Liu,Yong Ma*

Main category: cs.CL

TL;DR: 本文提出了一种混合型主动学习框架PromptAL，利用未标记数据构建动态软提示，从而优化决策边界，在少样本场景下选取更具代表性的高质量样本，大幅提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习方法过度依赖已标记样本的分布来确定决策边界，在少样本条件下与真实分布差异很大，导致决策边界和样本选择不理想，且普遍忽略了未标记数据在弥合分布差距中的作用。

Method: PromptAL框架首先利用未标记数据构建样本感知的动态软提示（soft prompts），用于调整模型的预测分布和决策边界。然后在调整后的决策边界基础上，结合不确定性估计和全局/局部多样性，筛选更能代表目标分布的高质量样本。

Result: 在六个同分布和三个异分布数据集上，PromptAL都优于九个基线方法，展现出更出色的主动学习性能。

Conclusion: PromptAL证明了在少样本主动学习任务中，通过未标记数据改进分布对齐和决策边界，可以有效选取更有价值的样本，提升模型表现，具有良好的实际应用前景。

Abstract: Active learning (AL) aims to optimize model training and reduce annotation
costs by selecting the most informative samples for labeling. Typically, AL
methods rely on the empirical distribution of labeled data to define the
decision boundary and perform uncertainty or diversity estimation, subsequently
identifying potential high-quality samples. In few-shot scenarios, the
empirical distribution often diverges significantly from the target
distribution, causing the decision boundary to shift away from its optimal
position. However, existing methods overlook the role of unlabeled samples in
enhancing the empirical distribution to better align with the target
distribution, resulting in a suboptimal decision boundary and the selection of
samples that inadequately represent the target distribution. To address this,
we propose a hybrid AL framework, termed \textbf{PromptAL} (Sample-Aware
Dynamic Soft \textbf{Prompts} for Few-Shot \textbf{A}ctive \textbf{L}earning).
This framework accounts for the contribution of each unlabeled data point in
aligning the current empirical distribution with the target distribution,
thereby optimizing the decision boundary. Specifically, PromptAL first
leverages unlabeled data to construct sample-aware dynamic soft prompts that
adjust the model's predictive distribution and decision boundary. Subsequently,
based on the adjusted decision boundary, it integrates uncertainty estimation
with both global and local diversity to select high-quality samples that more
accurately represent the target distribution. Experimental results on six
in-domain and three out-of-domain datasets show that PromptAL achieves superior
performance over nine baselines. Our codebase is openly accessible.

</details>


### [105] [Dutch CrowS-Pairs: Adapting a Challenge Dataset for Measuring Social Biases in Language Models for Dutch](https://arxiv.org/abs/2507.16442)
*Elza Strazda,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本文提出了专用于荷兰语的CrowS-Pairs数据集，用于测量荷兰语语言模型的偏见，并比较了不同语言模型间的偏见程度。


<details>
  <summary>Details</summary>
Motivation: 当前对于语言模型偏见的研究大多集中在英语，导致其它语言（如荷兰语）的偏见问题缺乏数据资源和评估手段，因此需要开发新的多语种偏见评测工具和数据集。

Method: 作者构建了覆盖9大偏见类别（如性取向、性别和残疾）的1463对荷兰语句子对抗数据集，并用其系统评估了多种荷兰语和多语种语言模型（如BERTje、RobBERT等）对偏见的表现。同时作者还用CrowS-Pairs的英语和法语版本评测了相应语言的模型。

Result: 荷兰语、英语和法语的主流语言模型在不同类别上均存在明显偏见，其中英语模型偏见最大，荷兰语模型偏见最小。此外，为语言模型分配不同persona会影响其偏见水平。

Conclusion: 偏见水平因语言、文化和模型设计的不同而存在显著差异，强调了跨语种、跨文化评测和治理AI偏见的重要性。

Abstract: Warning: This paper contains explicit statements of offensive stereotypes
which might be upsetting.
  Language models are prone to exhibiting biases, further amplifying unfair and
harmful stereotypes. Given the fast-growing popularity and wide application of
these models, it is necessary to ensure safe and fair language models. As of
recent considerable attention has been paid to measuring bias in language
models, yet the majority of studies have focused only on English language. A
Dutch version of the US-specific CrowS-Pairs dataset for measuring bias in
Dutch language models is introduced. The resulting dataset consists of 1463
sentence pairs that cover bias in 9 categories, such as Sexual orientation,
Gender and Disability. The sentence pairs are composed of contrasting
sentences, where one of the sentences concerns disadvantaged groups and the
other advantaged groups. Using the Dutch CrowS-Pairs dataset, we show that
various language models, BERTje, RobBERT, multilingual BERT, GEITje and
Mistral-7B exhibit substantial bias across the various bias categories. Using
the English and French versions of the CrowS-Pairs dataset, bias was evaluated
in English (BERT and RoBERTa) and French (FlauBERT and CamemBERT) language
models, and it was shown that English models exhibit the most bias, whereas
Dutch models the least amount of bias. Additionally, results also indicate that
assigning a persona to a language model changes the level of bias it exhibits.
These findings highlight the variability of bias across languages and contexts,
suggesting that cultural and linguistic factors play a significant role in
shaping model biases.

</details>


### [106] [Towards Enforcing Company Policy Adherence in Agentic Workflows](https://arxiv.org/abs/2507.16459)
*Naama Zwerdling,David Boaz,Ella Rabinovich,Guy Uziel,David Amid,Ateret Anaby-Tavor*

Main category: cs.CL

TL;DR: 本文提出了一种确定性、透明、模块化的策略遵循框架，用于大语言模型代理的业务流程自动化，能有效保障其行为符合复杂公司政策。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理虽然在业务流程自动化中展现出灵活性和可扩展性，但在实际应用中难以始终可靠地遵循复杂的企业政策，存在合规性风险。

Method: 提出一种两阶段的方法：（1）离线阶段将公司策略文档编译为可验证的守卫代码，关联到工具使用上；（2）运行时每一步代理操作前由守卫机制验证合规性。

Result: 在$$-bench Airlines的挑战性场景中进行实验，初步结果显示该方法在策略强制执行方面具有积极效果。

Conclusion: 所提框架提升了代理合规性，并且具有良好的可解释性和可移植性，但在实际部署时仍需应对更多挑战。

Abstract: Large Language Model (LLM) agents hold promise for a flexible and scalable
alternative to traditional business process automation, but struggle to
reliably follow complex company policies. In this study we introduce a
deterministic, transparent, and modular framework for enforcing business policy
adherence in agentic workflows. Our method operates in two phases: (1) an
offline buildtime stage that compiles policy documents into verifiable guard
code associated with tool use, and (2) a runtime integration where these guards
ensure compliance before each agent action. We demonstrate our approach on the
challenging $\tau$-bench Airlines domain, showing encouraging preliminary
results in policy enforcement, and further outline key challenges for
real-world deployments.

</details>


### [107] [ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination Detection in LLMs](https://arxiv.org/abs/2507.16488)
*Zhenliang Zhang,Xinyu Hu,Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 该论文提出了一种基于隐藏状态动态更新过程的新型幻觉检测方法，显著提升了检测性能并增强了可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）虽然强大，但容易产生幻觉，现有利用隐藏状态的幻觉检测方法只关注静态的单层表示，忽略了跨层动态演变，导致效果有限。

Method: 作者提出将关注点转向隐藏状态的更新过程，设计出ICR分数（Information Contribution to Residual Stream），量化各模块对隐藏状态更新的贡献。基于ICR分数，提出ICR Probe方法，用于捕捉隐藏状态的跨层演化，从而检测幻觉。

Result: 实验证明ICR分数能有效区分幻觉生成。ICR Probe检测方法在参数量显著减少的情况下，取得了优于现有方法的检测性能。消融实验和案例分析进一步揭示了方法的原理，提升了其可解释性。

Conclusion: 基于隐藏状态动态变化的ICR Probe方法有效提高了LLM幻觉检测的性能和解释性，有望推动相关领域的发展。

Abstract: Large language models (LLMs) excel at various natural language processing
tasks, but their tendency to generate hallucinations undermines their
reliability. Existing hallucination detection methods leveraging hidden states
predominantly focus on static and isolated representations, overlooking their
dynamic evolution across layers, which limits efficacy. To address this
limitation, we shift the focus to the hidden state update process and introduce
a novel metric, the ICR Score (Information Contribution to Residual Stream),
which quantifies the contribution of modules to the hidden states' update. We
empirically validate that the ICR Score is effective and reliable in
distinguishing hallucinations. Building on these insights, we propose a
hallucination detection method, the ICR Probe, which captures the cross-layer
evolution of hidden states. Experimental results show that the ICR Probe
achieves superior performance with significantly fewer parameters. Furthermore,
ablation studies and case analyses offer deeper insights into the underlying
mechanism of this method, improving its interpretability.

</details>


### [108] [Combining Language and Topic Models for Hierarchical Text Classification](https://arxiv.org/abs/2507.16490)
*Jaco du Toit,Marcel Dunaiski*

Main category: cs.CL

TL;DR: 本文研究了在层次文本分类任务中，将预训练语言模型（PLM）和主题模型提取的特征结合是否有助于提升分类性能。实验发现，主题模型特征不但没有提升，反而降低了分类表现。


<details>
  <summary>Details</summary>
Motivation: 近年来层次文本分类方法尝试融合类别层级结构信息和语言模型理解能力，提升分类效果。此前研究在多标签分类中发现主题模型与PLM结合有效，作者希望验证是否在层次分类同样适用。

Method: 方法上，作者用PLM和主题模型分别提取文本特征，再通过各自的卷积层处理，最后合并特征并用标签注意力机制获取类别相关的文档表示进行分类。他们在三组基准数据集上实验。

Result: 实验显示，将主题模型特征与PLM特征结合后，整体分类性能下降，不如单用PLM特征。

Conclusion: 与此前工作相反，本文结果表明在层次文本分类中，直接融合主题模型特征未必带来好处，应谨慎使用。

Abstract: Hierarchical text classification (HTC) is a natural language processing task
which has the objective of categorising text documents into a set of classes
from a predefined structured class hierarchy. Recent HTC approaches use various
techniques to incorporate the hierarchical class structure information with the
natural language understanding capabilities of pre-trained language models
(PLMs) to improve classification performance. Furthermore, using topic models
along with PLMs to extract features from text documents has been shown to be an
effective approach for multi-label text classification tasks. The rationale
behind the combination of these feature extractor models is that the PLM
captures the finer-grained contextual and semantic information while the topic
model obtains high-level representations which consider the corpus of documents
as a whole. In this paper, we use a HTC approach which uses a PLM and a topic
model to extract features from text documents which are used to train a
classification model. Our objective is to determine whether the combination of
the features extracted from the two models is beneficial to HTC performance in
general. In our approach, the extracted features are passed through separate
convolutional layers whose outputs are combined and passed to a label-wise
attention mechanisms which obtains label-specific document representations by
weighing the most important features for each class separately. We perform
comprehensive experiments on three HTC benchmark datasets and show that using
the features extracted from the topic model generally decreases classification
performance compared to only using the features obtained by the PLM. In
contrast to previous work, this shows that the incorporation of features
extracted from topic models for text classification tasks should not be assumed
beneficial.

</details>


### [109] [The Ever-Evolving Science Exam](https://arxiv.org/abs/2507.16514)
*Junying Wang,Zicheng Zhang,Yijin Guo,Farong Wen,Ye Shen,Yingji Liang,Yalun Wu,Wenzhe Li,Chunyi Li,Zijian Chen,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: 论文提出了Ever-Evolving Science Exam (EESE)作为新的科学能力评测基准，旨在避免数据泄露和提高评测效率，通过私有题库和定期采样，实现对基础模型科学理解力的动态、可扩展、稳健评估。


<details>
  <summary>Details</summary>
Motivation: 随着大模型能力和应用范围快速扩展，现有科学评测基准存在数据泄漏风险及大规模测试效率低下的问题，影响了评测的有效性和实用性。

Method: EESE分为两部分：（1）非公开的EESE-Pool，涵盖5大学科和500多个子领域，包含超过10万组专业构建的科学问答对；（2）周期性从问答库中采样、验证的500题子集公开用于模型评测，从设计上降低数据泄漏和评测成本。

Result: 在32个开源与闭源大模型上的实验证明，EESE能够有效区分模型在科学领域及相关认知能力维度上的强弱。

Conclusion: EESE为科学领域大模型评测提供了稳健、可扩展、前瞻性的解决方案，更现实地反映了模型理解科学问题的能力。

Abstract: As foundation models grow rapidly in capability and deployment, evaluating
their scientific understanding becomes increasingly critical. Existing science
benchmarks have made progress towards broad **Range**, wide **Reach**, and high
**Rigor**, yet they often face two major challenges: **data leakage risks**
that compromise benchmarking validity, and **evaluation inefficiency** due to
large-scale testing. To address these issues, we introduce the **Ever-Evolving
Science Exam (EESE)**, a dynamic benchmark designed to reliably assess
scientific capabilities in foundation models. Our approach consists of two
components: 1) a non-public **EESE-Pool** with over 100K expertly constructed
science instances (question-answer pairs) across 5 disciplines and 500+
subfields, built through a multi-stage pipeline ensuring **Range**, **Reach**,
and **Rigor**, 2) a periodically updated 500-instance subset **EESE**, sampled
and validated to enable leakage-resilient, low-overhead evaluations.
Experiments on 32 open- and closed-source models demonstrate that EESE
effectively differentiates the strengths and weaknesses of models in scientific
fields and cognitive dimensions. Overall, EESE provides a robust, scalable, and
forward-compatible solution for science benchmark design, offering a realistic
measure of how well foundation models handle science questions. The project
page is at: https://github.com/aiben-ch/EESE.

</details>


### [110] [Introducing Quality Estimation to Machine Translation Post-editing Workflow: An Empirical Study on Its Usefulness](https://arxiv.org/abs/2507.16515)
*Siqi Liu,Guangrong Dai,Dechao Li*

Main category: cs.CL

TL;DR: 本研究探索句子级质量评估（QE）在英汉机器翻译后编辑（MTPE）中的作用，发现QE能显著提升后编辑效率，相关效果在不同MT质量和翻译经验层次中稳定。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译后编辑流程中，如何帮助译者更高效地识别和处理机器翻译问题段落仍是难题。引入质量评估工具有望提升译后编辑效率，但对其具体影响机制与范围缺乏实证研究。

Method: 采用实验设计，让具备不同翻译经验的学生译者对中高质量的英译中MT输出进行后编辑，并比较有无句子级质量评估（QE）辅助的效率，结合定量（时间统计）和定性（访谈）数据分析。

Result: 结果显示，引入QE显著缩短了后编辑时间。QE与MT质量、与翻译专业水平的相互作用均不显著，说明QE带来的效率提升具有稳定性。同时，QE不仅帮助识别问题段，还被用于校验MT质量和复核输出。

Conclusion: QE在提升MTPE效率方面效果明显，且适用于不同质量MT输出和译者水平，但QE不准确时可能反而影响流程。建议合理整合QE，以促进翻译生产率提升。

Abstract: This preliminary study investigates the usefulness of sentence-level Quality
Estimation (QE) in English-Chinese Machine Translation Post-Editing (MTPE),
focusing on its impact on post-editing speed and student translators'
perceptions. It also explores the interaction effects between QE and MT
quality, as well as between QE and translation expertise. The findings reveal
that QE significantly reduces post-editing time. The examined interaction
effects were not significant, suggesting that QE consistently improves MTPE
efficiency across medium- and high-quality MT outputs and among student
translators with varying levels of expertise. In addition to indicating
potentially problematic segments, QE serves multiple functions in MTPE, such as
validating translators' evaluations of MT quality and enabling them to
double-check translation outputs. However, interview data suggest that
inaccurate QE may hinder post-editing processes. This research provides new
insights into the strengths and limitations of QE, facilitating its more
effective integration into MTPE workflows to enhance translators' productivity.

</details>


### [111] [Learning Text Styles: A Study on Transfer, Attribution, and Verification](https://arxiv.org/abs/2507.16530)
*Zhiqiang Hu*

Main category: cs.CL

TL;DR: 本文围绕文本风格的计算理解与操作，系统研究了文本风格迁移、作者身份识别和验证，提出了结合大型语言模型自适应、对比式风格特征解耦与基于指令的可解释方法。


<details>
  <summary>Details</summary>
Motivation: 文本风格在实际应用如情感分析、问答系统和写作辅助等领域中非常重要，但在风格迁移、作者识别与验证中依然存在内容保持、特征解耦及模型可解释性等重要挑战。

Method: 本研究采用三种途径：一是通过参数高效的大型语言模型自适应实现文本风格迁移；二是利用对比学习实现风格特征的有效解耦，从而提升作者识别和验证的准确性；三是基于指令的微调方式，增强作者验证任务的模型可解释性。

Result: 实验表明，所提方法在文本风格迁移任务中更好地保持了内容与风格的分离，同时在作者识别和验证方面取得了更高的准确率，并提升了模型应用时的可解释性。

Conclusion: 本论文推动了文本风格的计算建模与操作，在提高模型效果、应用广泛性和可解释性方面取得了突破，为相关应用场景提供了强大的技术基础。

Abstract: This thesis advances the computational understanding and manipulation of text
styles through three interconnected pillars: (1) Text Style Transfer (TST),
which alters stylistic properties (e.g., sentiment, formality) while preserving
content; (2)Authorship Attribution (AA), identifying the author of a text via
stylistic fingerprints; and (3) Authorship Verification (AV), determining
whether two texts share the same authorship. We address critical challenges in
these areas by leveraging parameter-efficient adaptation of large language
models (LLMs), contrastive disentanglement of stylistic features, and
instruction-based fine-tuning for explainable verification.

</details>


### [112] [Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language](https://arxiv.org/abs/2507.16557)
*Kristin Gnadt,David Thulke,Simone Kopeinik,Ralf Schlüter*

Main category: cs.CL

TL;DR: 本文提出了五个用于评估LLM在德语环境下性别偏见的数据集，并对多个多语种模型进行了测试，揭示了德语在性别偏见评估中的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的性别偏见评估方法多基于英语，而其对其它语言（如德语）的适用性和有效性存疑。因此，作者希望为德语下的性别偏见评估建立专用的数据集和分析方法，推动非英语语言环境中的公平性研究。

Method: 作者根据性别偏见的经典理论和定义，构建了五个德语数据集，通过多种方法对八个多语种大语言模型进行性别偏见评估。

Result: 评估显示，在德语环境下存在诸多独特的性别偏见，比如职业用词性别模糊、看似中性的名词也会影响性别感知等问题。

Conclusion: 跨语言LLM的性别偏见表现和机制具有显著差异，必须为不同语言设计有针对性的评估方法和框架。

Abstract: In recent years, various methods have been proposed to evaluate gender bias
in large language models (LLMs). A key challenge lies in the transferability of
bias measurement methods initially developed for the English language when
applied to other languages. This work aims to contribute to this research
strand by presenting five German datasets for gender bias evaluation in LLMs.
The datasets are grounded in well-established concepts of gender bias and are
accessible through multiple methodologies. Our findings, reported for eight
multilingual LLM models, reveal unique challenges associated with gender bias
in German, including the ambiguous interpretation of male occupational terms
and the influence of seemingly neutral nouns on gender perception. This work
contributes to the understanding of gender bias in LLMs across languages and
underscores the necessity for tailored evaluation frameworks.

</details>


### [113] [Pixels to Principles: Probing Intuitive Physics Understanding in Multimodal Language Models](https://arxiv.org/abs/2507.16572)
*Mohamad Ballout,Serwan Jassim,Elia Bruni*

Main category: cs.CL

TL;DR: 本文系统评估了多模态大型语言模型在直觉物理任务上的表现，发现当前模型在区分物理合理与不合理场景时仍有较大挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大型语言模型（MLLMs）发展迅速，但它们在涉及物理直觉（如区分物理上合理或不合理场景）任务上的能力仍不明确。因此作者旨在系统评估这些模型的直觉物理推理能力，并分析其内部工作机制。

Method: 作者使用GRASP和IntPhys 2数据集，对多个主流开源和私有MLLMs进行基准测试，不仅关注最终表现，还通过探测中间神经元表示，分析视觉信息和语言信息的结合与传递情况。

Result: 实验发现，虽然视觉编码器能够捕获物理合理性的线索，但这些关键信息在传递给语言模型时未被有效利用，尤其在任务难度提高时，视觉与语言模块之间存在明显失配，导致模型推理失败。

Conclusion: 模型主要瓶颈不是视觉部分，而是视觉和语言信息整合的低效。未来提升MLLMs直觉物理能力的关键在于加强视觉-语言对齐。

Abstract: This paper presents a systematic evaluation of state-of-the-art multimodal
large language models (MLLMs) on intuitive physics tasks using the GRASP and
IntPhys 2 datasets. We assess the open-source models InternVL 2.5, Qwen 2.5 VL,
LLaVA-OneVision, and the proprietary Gemini 2.0 Flash Thinking, finding that
even the latest models struggle to reliably distinguish physically plausible
from implausible scenarios. To go beyond performance metrics, we conduct a
probing analysis of model embeddings, extracting intermediate representations
at key processing stages to examine how well task-relevant information is
preserved. Our results show that, depending on task difficulty, a critical
vision-language misalignment can emerge: vision encoders successfully capture
physical plausibility cues, but this information is not effectively utilized by
the language model, leading to failures in reasoning. This misalignment
suggests that the primary limitation of MLLMs in intuitive physics tasks is not
the vision component but the ineffective integration of visual and linguistic
information. Our findings highlight vision-language alignment as a key area for
improvement, offering insights for future MLLMs development.

</details>


### [114] [Step-Audio 2 Technical Report](https://arxiv.org/abs/2507.16632)
*Boyong Wu,Chao Yan,Chen Hu,Cheng Yi,Chengli Feng,Fei Tian,Feiyu Shen,Gang Yu,Haoyang Zhang,Jingbei Li,Mingrui Chen,Peng Liu,Wang You,Xiangyu Tony Zhang,Xingyuan Li,Xuerui Yang,Yayue Deng,Yechang Huang,Yuxin Li,Yuxin Zhang,Zhao You,Brian Li,Changyi Wan,Hanpeng Hu,Jiangjie Zhen,Siyu Chen,Song Yuan,Xuelin Zhang,Yimin Jiang,Yu Zhou,Yuxiang Yang,Bingxin Li,Buyun Ma,Changhe Song,Dongqing Pang,Guoqiang Hu,Haiyang Sun,Kang An,Na Wang,Shuli Gao,Wei Ji,Wen Li,Wen Sun,Xuan Wen,Yong Ren,Yuankai Ma,Yufan Lu,Bin Wang,Bo Li,Changxin Miao,Che Liu,Chen Xu,Dapeng Shi,Dingyuan Hu,Donghang Wu,Enle Liu,Guanzhe Huang,Gulin Yan,Han Zhang,Hao Nie,Haonan Jia,Hongyu Zhou,Jianjian Sun,Jiaoren Wu,Jie Wu,Jie Yang,Jin Yang,Junzhe Lin,Kaixiang Li,Lei Yang,Liying Shi,Li Zhou,Longlong Gu,Ming Li,Mingliang Li,Mingxiao Li,Nan Wu,Qi Han,Qinyuan Tan,Shaoliang Pang,Shengjie Fan,Siqi Liu,Tiancheng Cao,Wanying Lu,Wenqing He,Wuxun Xie,Xu Zhao,Xueqi Li,Yanbo Yu,Yang Yang,Yi Liu,Yifan Lu,Yilei Wang,Yuanhao Ding,Yuanwei Liang,Yuanwei Lu,Yuchu Luo,Yuhe Yin,Yumeng Zhan,Yuxiang Zhang,Zidong Yang,Zixin Zhang,Binxing Jiao,Daxin Jiang,Heung-Yeung Shum,Jiansheng Chen,Jing Li,Xiangyu Zhang,Yibo Zhu*

Main category: cs.CL

TL;DR: Step-Audio 2是一个为音频理解和语音对话设计的多模态大模型，具备强大音频识别和生成能力，并能高效处理语音交流中的多种信息。


<details>
  <summary>Details</summary>
Motivation: 现有音频理解和语音对话系统在自动语音识别、音频理解及真实语音对话能力方面存在局限，尤其在处理如说话风格、情感等副语言信息时效果有限。

Method: 该方法通过集成潜式音频编码器与以推理为核心的强化学习，实现高效的自动语音识别和音频理解。此外，模型将离散音频标记生成纳入语言建模，增强副语言信息表现。引入检索增强生成（RAG）技术，同时具备调用外部工具（如网页检索、音频检索等）能力，以降低幻觉并改变音色。模型在大量真实语音和音频数据上进行训练。

Result: Step-Audio 2在多项音频理解与语音对话基线测试中取得了同类开源及商业系统中的最先进表现，表现出较高的智能性和表达力。

Conclusion: Step-Audio 2有效提升了音频理解、语音识别及对话中的表达能力，是面向真实工业场景的强大音频大模型方案。

Abstract: This paper presents Step-Audio~2, an end-to-end multi-modal large language
model designed for industry-strength audio understanding and speech
conversation. By integrating a latent audio encoder and reasoning-centric
reinforcement learning (RL), Step-Audio 2 achieves promising performance in
automatic speech recognition (ASR) and audio understanding. To facilitate
genuine end-to-end speech conversation, Step-Audio 2 incorporates the
generation of discrete audio tokens into language modeling, significantly
enhancing its responsiveness to paralinguistic information such as speaking
styles and emotions. To effectively leverage the rich textual and acoustic
knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented
generation (RAG) and is able to call external tools such as web search to
mitigate hallucination and audio search to switch timbres. Trained on millions
of hours of speech and audio data, Step-Audio 2 delivers intelligence and
expressiveness across diverse conversational scenarios. Evaluation results
demonstrate that Step-Audio 2 achieves state-of-the-art performance on various
audio understanding and conversational benchmarks compared to other open-source
and commercial solutions. Please visit
https://github.com/stepfun-ai/Step-Audio2 for more information.

</details>


### [115] [Towards Automated Regulatory Compliance Verification in Financial Auditing with Large Language Models](https://arxiv.org/abs/2507.16642)
*Armin Berger,Lars Hillebrand,David Leonhard,Tobias Deußer,Thiago Bell Felix de Oliveira,Tim Dilmaghani,Mohamed Khaled,Bernd Kliem,Rüdiger Loitz,Christian Bauckhage,Rafet Sifa*

Main category: cs.CL

TL;DR: 本论文探讨了大模型（LLMs）在财务文件合规审核中的应用，特别是开源与专有模型在合规检测上的表现对比。


<details>
  <summary>Details</summary>
Motivation: 传统财务文件审核过程繁琐且需要大量人力，现有AI系统虽然可以推荐相关文本，但难以直接验证内容是否真正合规。该论文希望通过评估大模型（尤其是开源与专有模型）在法规合规性审核中的能力，填补AI审核合规性的实际空白。

Method: 作者采用了来自普华永道德国的两个定制数据集，比较了开源大模型（如Llama-2）与专有模型（如GPT）在检测财务报告合法合规方面的表现。通过对不同模型在多种配置下的表现进行实证测试和对比分析。

Result: Llama-2 70B开源模型在检测不合规（真正负向样本）方面表现优异，超过了所有专有模型。而整体场景下尤其在非英语环境中，GPT-4等专有模型的表现仍然最佳。

Conclusion: 开源大模型已具备在特定合规检测任务中超越专有模型的能力，但在多语种、复杂场景下，专有模型依然更胜一筹。两类模型各有优势，可针对实际需求选用。

Abstract: The auditing of financial documents, historically a labor-intensive process,
stands on the precipice of transformation. AI-driven solutions have made
inroads into streamlining this process by recommending pertinent text passages
from financial reports to align with the legal requirements of accounting
standards. However, a glaring limitation remains: these systems commonly fall
short in verifying if the recommended excerpts indeed comply with the specific
legal mandates. Hence, in this paper, we probe the efficiency of publicly
available Large Language Models (LLMs) in the realm of regulatory compliance
across different model configurations. We place particular emphasis on
comparing cutting-edge open-source LLMs, such as Llama-2, with their
proprietary counterparts like OpenAI's GPT models. This comparative analysis
leverages two custom datasets provided by our partner PricewaterhouseCoopers
(PwC) Germany. We find that the open-source Llama-2 70 billion model
demonstrates outstanding performance in detecting non-compliance or true
negative occurrences, beating all their proprietary counterparts. Nevertheless,
proprietary models such as GPT-4 perform the best in a broad variety of
scenarios, particularly in non-English contexts.

</details>


### [116] [P-CoT: A Pedagogically-motivated Participatory Chain-of-Thought Prompting for Phonological Reasoning in LLMs](https://arxiv.org/abs/2507.16656)
*Dongjun Jang,Youngchae Ahn,Hyopil Shin*

Main category: cs.CL

TL;DR: 本文探讨大型语言模型在处理与语音学相关任务（如押韵生成、音素转写、音节计数）时的潜力，并提出一种新颖的链式思考提示（P-CoT）显著提升了这些任务的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在文本任务上表现优异，但其在语音相关推理领域的能力仍未被充分研究。因此，作者希望评估和改善现有模型在这类任务中的表现。

Method: 使用PhonologyBench基准测试集，对12种主流大型语言模型在押韵生成、g2p转换和音节计数等任务上进行评估。通过几次示例学习和引入全新基于教育理论的参与式链式思考（P-CoT）提示，加强模型的推理能力。

Result: P-CoT方法可使模型在语音相关任务上提升最高52%的表现，并在某些任务上超越人类基线。几次示例学习提升有限且不一致，而P-CoT则表现持续显著。

Conclusion: 参与式链式思考提示（P-CoT）能有效激活和提升大语言模型的语音推理能力。未来可针对不同模型微调P-CoT提示，或扩展该方法到其他语言领域。

Abstract: This study explores the potential of phonological reasoning within text-based
large language models (LLMs). Utilizing the PhonologyBench benchmark, we assess
tasks like rhyme word generation, g2p conversion, and syllable counting. Our
evaluations across 12 LLMs reveal that while few-shot learning offers
inconsistent gains, the introduction of a novel Pedagogically-motivated
Participatory Chain-of-Thought (P-CoT) prompt, which is anchored in educational
theories like scaffolding and discovery learning, consistently enhances
performance. This method leverages structured guidance to activate latent
phonological abilities, achieving up to 52% improvement and even surpassing
human baselines in certain tasks. Future work could aim to optimize P-CoT
prompts for specific models or explore their application across different
linguistic domains.

</details>


### [117] [Self-Contradiction as Self-Improvement: Mitigating the Generation-Understanding Gap in MLLMs](https://arxiv.org/abs/2507.16663)
*Yujin Han,Hao Chen,Andi Han,Zhiheng Wang,Xinyu Lin,Yingya Zhang,Shiwei Zhang,Difan Zou*

Main category: cs.CL

TL;DR: 本文发现多模态大模型（MLLMs）在生成与理解任务中存在自我矛盾现象，生成的内容往往与自身理解不一致。作者提出了Nonunified分数来量化这一矛盾，并探索利用模型自身矛盾促进自我改进的方法，提出了基于课程学习的策略以进一步优化模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型试图统一生成和理解能力，但常出现自相矛盾——模型生成的图像与模型自身对输入提示的理解不符。作者希望系统性分析并量化这一问题，同时探索如何利用这种能力不对称性实现模型自我优化。

Method: 1）定义Nonunified分数以量化模型生成与理解间的自我矛盾；2）通过实验证明矛盾主要源于生成端对提示的不对齐而非理解端误判；3）利用模型自身理解强于生成的特点，通过标准后训练方法（如SFT，DPO）用理解能力反向指导生成端优化；4）提出基于模型表现递进难度的课程学习策略进一步提升模型表现。

Result: 实验证明自监督方式能同步提升生成与理解能力，并首次在后训练阶段也发现仅微调生成分支可以带来理解端提升。分析发现这是对先前误判为对齐样本检测能力提升所致。同时揭示若指导信息质量低，模型可能陷入同步劣化。

Conclusion: 模型生成和理解的协同优化有助于领先多模态模型的统一化，但需要关注数据和内在指标的局限性。课程学习策略显著提升了多模态模型的整体表现，未来需尤其重视监督数据质量以防协同劣化。

Abstract: Despite efforts to unify multimodal generation and understanding tasks in a
single model, we show these MLLMs exhibit self-contradiction where generation
produces images deemed misaligned with input prompts based on the model's own
understanding. We define a Nonunified score that quantifies such
self-contradiction. Our empirical results reveal that the self-contradiction
mainly arises from weak generation that fails to align with prompts, rather
than misunderstanding. This capability asymmetry indicates the potential of
leveraging self-contradiction for self-improvement, where the stronger model
understanding guides the weaker generation to mitigate the
generation-understanding gap. Applying standard post-training methods (e.g.,
SFT, DPO) with such internal supervision successfully improves both generation
and unification. We discover a co-improvement effect on both generation and
understanding when only fine-tuning the generation branch, a phenomenon known
in pre-training but underexplored in post-training. Our analysis shows
improvements stem from better detection of false positives that are previously
incorrectly identified as prompt-aligned. Theoretically, we show the aligned
training dynamics between generation and understanding allow reduced
prompt-misaligned generations to also improve mismatch detection in the
understanding branch. Additionally, the framework reveals a potential risk of
co-degradation under poor supervision-an overlooked phenomenon that is
empirically validated in our experiments. Notably, we find intrinsic metrics
like Nonunified score cannot distinguish co-degradation from co-improvement,
which highlights the necessity of data quality check. Finally, we propose a
curriculum-based strategy based on our findings that gradually introduces
harder samples as the model improves, leading to better unification and
improved MLLM generation and understanding.

</details>


### [118] [PICACO: Pluralistic In-Context Value Alignment of LLMs via Total Correlation Optimization](https://arxiv.org/abs/2507.16679)
*Han Jiang,Dongyao Zhu,Zhihua Wei,Xiaoyuan Yi,Ziang Xiao,Xing Xie*

Main category: cs.CL

TL;DR: 本文提出一种新颖的方法PICACO，以提升大语言模型在无需微调下更好地对齐多元化人类价值，解决以往方法难以均衡多重价值的问题，并在多个数据集和模型上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型通常采用提示（prompt）进行对齐，但这些模型在理解输入时常常不能很好应对多重价值观的冲突（如刺激vs.传统），这造成了对齐不充分或有偏。现有的In-Context Alignment方法存在“指令瓶颈”，即单一提示难以涵盖全部预期价值。

Method: 提出了一种名为PICACO的多元In-Context Alignment方法，通过优化一个meta-instruction（元指令），在不需要微调的前提下，引导模型在多种价值观之间进行更好理解和权衡。该方法核心是最大化模型响应与指定价值观之间的总相关性，从而强化模型对多重价值的协同表达，减少噪音影响。

Result: 在五组价值观数据集上进行广泛实验，PICACO方法在黑盒和开源的大语言模型上都能较好应用，且在多达8个不同价值间取得比近期多种强基线更优的均衡对齐效果。

Conclusion: PICACO为多元价值观对齐问题提供了无需成本高昂后训练的新解法，能更有效引导大模型协同表达和权衡多重价值观，具有良好的通用性和效果。

Abstract: In-Context Learning has shown great potential for aligning Large Language
Models (LLMs) with human values, helping reduce harmful outputs and accommodate
diverse preferences without costly post-training, known as In-Context Alignment
(ICA). However, LLMs' comprehension of input prompts remains agnostic, limiting
ICA's ability to address value tensions--human values are inherently
pluralistic, often imposing conflicting demands, e.g., stimulation vs.
tradition. Current ICA methods therefore face the Instruction Bottleneck
challenge, where LLMs struggle to reconcile multiple intended values within a
single prompt, leading to incomplete or biased alignment. To address this, we
propose PICACO, a novel pluralistic ICA method. Without fine-tuning, PICACO
optimizes a meta-instruction that navigates multiple values to better elicit
LLMs' understanding of them and improve their alignment. This is achieved by
maximizing the total correlation between specified values and LLM responses,
theoretically reinforcing value correlation while reducing distractive noise,
resulting in effective value instructions. Extensive experiments on five value
sets show that PICACO works well with both black-box and open-source LLMs,
outperforms several recent strong baselines, and achieves a better balance
across up to 8 distinct values.

</details>


### [119] [Interpretable Topic Extraction and Word Embedding Learning using row-stochastic DEDICOM](https://arxiv.org/abs/2507.16695)
*Lars Hillebrand,David Biesner,Christian Bauckhage,Rafet Sifa*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于DEDICOM的行随机变换方法，用于从文本语料中的互信息矩阵中提取主题和学习词嵌入，并针对方法的训练与效果进行了探讨。


<details>
  <summary>Details</summary>
Motivation: DEDICOM算法在解释对称和非对称矩阵分解方面表现突出，传统方法在处理文本语料时存在一定限制，因此作者尝试将其改进并应用于文本数据，旨在同时提升主题建模和词嵌入的可解释性。

Method: 作者提出了一种新的行随机（row-stochastic）DEDICOM变体，作用于文本语料的点互信息矩阵，通过这种约束的DEDICOM算法高效训练，以识别词汇的潜在主题簇并学习可解释的词嵌入。还对该方法的训练方式进行优化，并提供了定性的性能评估。

Result: 实验表明，该新方法能够有效捕捉词语间的主题关系，并生成较为可解释的词嵌入，同时在主题模型和词嵌入两个方面均表现出良好性能。

Conclusion: 改进后的DEDICOM算法在文本分析领域具备优秀表现，能够同时满足主题建模和词嵌入的可解释性需求，并具备较好的训练效率及模型效果。

Abstract: The DEDICOM algorithm provides a uniquely interpretable matrix factorization
method for symmetric and asymmetric square matrices. We employ a new
row-stochastic variation of DEDICOM on the pointwise mutual information
matrices of text corpora to identify latent topic clusters within the
vocabulary and simultaneously learn interpretable word embeddings. We introduce
a method to efficiently train a constrained DEDICOM algorithm and a qualitative
evaluation of its topic modeling and word embedding performance.

</details>


### [120] [Advancing Risk and Quality Assurance: A RAG Chatbot for Improved Regulatory Compliance](https://arxiv.org/abs/2507.16711)
*Lars Hillebrand,Armin Berger,Daniel Uedelhoven,David Berghaus,Ulrich Warning,Tim Dilmaghani,Bernd Kliem,Thomas Schmid,Rüdiger Loitz,Rafet Sifa*

Main category: cs.CL

TL;DR: 本文提出了一套结合大型语言模型（LLM）、混合检索与相关性提升的RAG系统，有效提升了高监管行业里风险与质量保障（R&Q）中的政策查询处理能力，并在实际业务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 高监管行业对风险与质量保障工作有严格要求，员工日常需频繁解读复杂政策，而传统依赖专家的办法导致效率低、难以扩展，因此亟需更高效且可扩展的解决方案。

Method: 作者构建了一个基于检索增强生成（RAG）的系统，结合大型语言模型、混合检索策略及相关性提升算法，对政策查询进行高效处理，并在真实世界的124个专家标注案例上进行评估与超参数敏感性分析。

Result: 系统不仅在实际环境下被成功部署，还在人类专家标注的真实问题上，相较于传统RAG方法取得了显著提升，并通过超参数实验为从业者提供了实用参考。

Conclusion: 结合LLM、混合检索与相关性提升的RAG系统可切实提高高监管行业R&Q处理效率和准确性，且对未来系统参数优化和实践应用具有指导意义。

Abstract: Risk and Quality (R&Q) assurance in highly regulated industries requires
constant navigation of complex regulatory frameworks, with employees handling
numerous daily queries demanding accurate policy interpretation. Traditional
methods relying on specialized experts create operational bottlenecks and limit
scalability. We present a novel Retrieval Augmented Generation (RAG) system
leveraging Large Language Models (LLMs), hybrid search and relevance boosting
to enhance R&Q query processing. Evaluated on 124 expert-annotated real-world
queries, our actively deployed system demonstrates substantial improvements
over traditional RAG approaches. Additionally, we perform an extensive
hyperparameter analysis to compare and evaluate multiple configuration setups,
delivering valuable insights to practitioners.

</details>


### [121] [RAVine: Reality-Aligned Evaluation for Agentic Search](https://arxiv.org/abs/2507.16725)
*Yilong Xu,Xiang Long,Zhi Zheng,Jinhua Gao*

Main category: cs.CL

TL;DR: 提出了一个新的针对agentic搜索系统的评估框架RAVine，更真实、更细致地反映模型搜索与回答能力。


<details>
  <summary>Details</summary>
Motivation: 传统agentic搜索的评估方法存在以下三大问题：1）使用复杂但不贴合真实用户场景的查询；2）地面真实答案构建过程中添加噪声，影响细粒度评估；3）仅评估终极答案质量，忽略了agentic搜索过程中模型与搜索工具的交互与迭代过程。

Method: 提出了RAVine评估框架。其创新点包括：（1）专门针对多点查询及长文本回答，贴合真实用户意图；（2）采用可归因的地面真实性答案构建法，提高细粒度评估准确性；（3）整体考察模型与外部工具的迭代过程，并考虑效率因素。

Result: 利用RAVine对一系列agentic模型进行了基准测试，揭示了当前模型在真实检索-生成场景下的表现及部分不足。

Conclusion: RAVine作为现实对齐的评测框架，弥补了传统agentic搜索评测不足，为未来智能搜索系统的研发与评估提供了支撑。相关代码和数据已开源。

Abstract: Agentic search, as a more autonomous and adaptive paradigm of retrieval
augmentation, is driving the evolution of intelligent search systems. However,
existing evaluation frameworks fail to align well with the goals of agentic
search. First, the complex queries commonly used in current benchmarks often
deviate from realistic user search scenarios. Second, prior approaches tend to
introduce noise when extracting ground truth for end-to-end evaluations,
leading to distorted assessments at a fine-grained level. Third, most current
frameworks focus solely on the quality of final answers, neglecting the
evaluation of the iterative process inherent to agentic search. To address
these limitations, we propose RAVine -- a Reality-Aligned eValuation framework
for agentic LLMs with search. RAVine targets multi-point queries and long-form
answers that better reflect user intents, and introduces an attributable ground
truth construction strategy to enhance the accuracy of fine-grained evaluation.
Moreover, RAVine examines model's interaction with search tools throughout the
iterative process, and accounts for factors of efficiency. We benchmark a
series of models using RAVine and derive several insights, which we hope will
contribute to advancing the development of agentic search systems. The code and
datasets are available at https://github.com/SwordFaith/RAVine.

</details>


### [122] [Unpacking Ambiguity: The Interaction of Polysemous Discourse Markers and Non-DM Signals](https://arxiv.org/abs/2507.16748)
*Jingni Wu,Amir Zeldes*

Main category: cs.CL

TL;DR: 本文探讨了英语中话语标记（如but, then）与非标记信号的交互及其在不同体裁中的模式。研究发现，多义的话语标记与更多样化的非标记信号共现，但共现信号的数量未必增加，体裁对其互动有重要影响。


<details>
  <summary>Details</summary>
Motivation: 话语标记对语篇连贯性至关重要，但它们常与非标记信号（如时间短语）共同出现或互相替代，且二者均可能产生歧义。如何区分和解释这些信号的互动机制尚不清楚，因此有必要深入分析多义话语标记与非标记信号的关系，并考察体裁的影响。

Method: 作者提出了基于eRST框架的话语标记多义性分级定义，并利用相关与回归分析，探究多义话语标记是否更易与数量更多、类型更丰富的非标记信号共现。同时，分析了体裁因素对信号互动模式的影响。

Result: 研究结果显示，多义话语标记确实与更丰富的非标记信号类型共现，但并不会显著增加共现信号的总量。此外，体裁对话语标记和信号的互动方式有显著影响。

Conclusion: 话语标记的多义性主要在于与更多样化的非标记信号类型共现，而不是数量的增加，体裁则是驱动这种互动差异的关键因素。

Abstract: Discourse markers (DMs) like 'but' or 'then' are crucial for creating
coherence in discourse, yet they are often replaced by or co-occur with non-DMs
('in the morning' can mean the same as 'then'), and both can be ambiguous
('since' can refer to time or cause). The interaction mechanism between such
signals remains unclear but pivotal for their disambiguation. In this paper we
investigate the relationship between DM polysemy and co-occurrence of non-DM
signals in English, as well as the influence of genre on these patterns.
  Using the framework of eRST, we propose a graded definition of DM polysemy,
and conduct correlation and regression analyses to examine whether polysemous
DMs are accompanied by more numerous and diverse non-DM signals. Our findings
reveal that while polysemous DMs do co-occur with more diverse non-DMs, the
total number of co-occurring signals does not necessarily increase. Moreover,
genre plays a significant role in shaping DM-signal interactions.

</details>


### [123] [Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning](https://arxiv.org/abs/2507.16784)
*Hongyin Luo,Nathaniel Morgan,Tina Li,Derek Zhao,Ai Vy Ngo,Philip Schroeder,Lijie Yang,Assaf Ben-Kish,Jack O'Brien,James Glass*

Main category: cs.CL

TL;DR: 本文提出了一种突破大语言模型（LLM）上下文长度限制的新方法，名为Thread Inference Model（TIM）及其运行时系统TIMRUN，实现了接近无限的推理长程和多步工具调用能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在推理准确性和效率方面受限于上下文长度、输出大小和GPU显存等瓶颈，这限制了其在复杂推理任务中的应用效果。

Method: 作者提出了TIM模型，它通过建模自然语言为树状推理结构而非线性序列，结合递归与分解式任务求解，并在TIMRUN推理运行时环境中实现。该环境通过只保留相关内容的KV缓存状态与基于规则的子任务剪枝机制，提升显存与位置嵌入的利用效率，实现超越传统长度限制的推理能力。

Result: 实验证明，该系统在操作GPU显存中高比例KV缓存时仍保持高推理吞吐，并在数学推理以及需要长过程推理和多步工具调用的信息检索任务上实现了高准确率。

Conclusion: TIM与TIMRUN的结合突破了LLM推理中上下文长度、输出限制和显存瓶颈，为复杂、长程、多步骤的推理任务及工具调用提供了强有力的技术基础。

Abstract: To break the context limits of large language models (LLMs) that bottleneck
reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),
a family of LLMs trained for recursive and decompositional problem solving, and
TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond
context limits. Together, TIM hosted on TIMRUN supports virtually unlimited
working memory and multi-hop tool calls within a single language model
inference, overcoming output limits, positional-embedding constraints, and
GPU-memory bottlenecks. Performance is achieved by modeling natural language as
reasoning trees measured by both length and depth instead of linear sequences.
The reasoning trees consist of tasks with thoughts, recursive subtasks, and
conclusions based on the concept we proposed in Schroeder et al, 2025. During
generation, we maintain a working memory that retains only the key-value states
of the most relevant context tokens, selected by a rule-based subtask-pruning
mechanism, enabling reuse of positional embeddings and GPU memory pages
throughout reasoning. Experimental results show that our system sustains high
inference throughput, even when manipulating up to 90% of the KV cache in GPU
memory. It also delivers accurate reasoning on mathematical tasks and handles
information retrieval challenges that require long-horizon reasoning and
multi-hop tool use.

</details>


### [124] [Test-Time-Matching: Decouple Personality, Memory, and Linguistic Style in LLM-based Role-Playing Language Agent](https://arxiv.org/abs/2507.16799)
*Xiaoyu Zhan,Xinyu Fu,Hao Sun,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的新型角色扮演框架（TTM），显著提升了大语言模型在模拟知名虚构或公共人物方面的沉浸感。


<details>
  <summary>Details</summary>
Motivation: 当前仅依赖prompt和上下文难以让语言模型深度融入特定角色，而基于微调的方法又受限于数据获取和训练资源的瓶颈。

Method: 提出Test-Time-Matching（TTM），一种无需训练的框架，通过测试时的特征缩放与上下文工程，让模型自动将角色特征解耦为人格、记忆和语言风格，并在三阶段生成流水线中有序控制角色扮演表现。

Result: 在人类评测下，这一方法在生成富有表现力且风格一致的人物对话方面表现出色，并支持多种语言风格、人格和记忆的组合。

Conclusion: TTM不依赖训练，操作灵活，能够显著提升大语言模型的角色扮演能力，增强对知名角色的模拟效果，对实际应用具有重要意义。

Abstract: The rapid advancement of large language models (LLMs) has enabled
role-playing language agents to demonstrate significant potential in various
applications. However, relying solely on prompts and contextual inputs often
proves insufficient for achieving deep immersion in specific roles,
particularly well-known fictional or public figures. On the other hand,
fine-tuning-based approaches face limitations due to the challenges associated
with data collection and the computational resources required for training,
thereby restricting their broader applicability. To address these issues, we
propose Test-Time-Matching (TTM), a training-free role-playing framework
through test-time scaling and context engineering. TTM uses LLM agents to
automatically decouple a character's features into personality, memory, and
linguistic style. Our framework involves a structured, three-stage generation
pipeline that utilizes these features for controlled role-playing. It achieves
high-fidelity role-playing performance, also enables seamless combinations
across diverse linguistic styles and even variations in personality and memory.
We evaluate our framework through human assessment, and the results demonstrate
that our method achieves the outstanding performance in generating expressive
and stylistically consistent character dialogues.

</details>


### [125] [Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning](https://arxiv.org/abs/2507.16802)
*Yanjun Zheng,Xiyang Du,Longfei Liao,Xiaoke Zhao,Zhaowen Zhou,Bo Zhang,Jiawei Liu,Xiang Qi,Zhe Li,Zhiqiang Zhang,Wang Wei,Peng Zhang*

Main category: cs.CL

TL;DR: 本文提出Agentar-Fin-R1金融大语言模型（8B和32B参数），专为提升金融领域推理能力、可靠性和专用化而设计，并展示其在多项金融和通用推理任务上的领先表现。


<details>
  <summary>Details</summary>
Motivation: 金融领域对大模型的推理能力、可信度和任务适应性有更高要求，现有通用模型难以满足高风险金融应用的严格标准。

Method: 基于Qwen3模型，通过构建金融任务体系、分层可信度框架（包括知识工程、多智能体数据生成、数据治理），并结合标签引导的自动优化、两阶段学习和归因体系提升训练效率，系统优化模型表现。

Result: Agentar-Fin-R1在FinEva、FinEval、FinanceIQ等金融基准测试，以及MATH-500、GPQA等通用推理测试中表现出色。同时通过新提出的Finova基准测试，验证了其在真实世界下的推理和合规能力。

Conclusion: Agentar-Fin-R1在金融任务和推理能力上达到SOTA水平，且具备高度可信、可用于重要金融场景的实际应用价值。

Abstract: Large Language Models (LLMs) demonstrate tremendous potential in the
financial domain, yet existing models often fall short in scenarios demanding
robust reasoning capabilities, stringent trustworthiness requirements, and
efficient adaptation to task-specific needs. We introduce the Agentar-Fin-R1
series of financial large language models (8B and 32B parameters), specifically
engineered based on the Qwen3 foundation model to enhance reasoning
capabilities, reliability, and domain specialization for financial
applications. Our optimization approach integrates a high-quality, systematic
financial task taxonomy with a comprehensive multi-layered trustworthiness
assurance framework. This framework encompasses high-quality trustworthy
knowledge engineering, multi-agent trustworthy data synthesis, and rigorous
data validation governance. Through label-guided automated difficulty-aware
optimization, tow-stage learning processes, and detailed attribution systems,
we achieve substantial improvements in training efficiency. Our models undergo
comprehensive evaluation on mainstream financial benchmarks including FinEva,
FinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500
and GPQA. To thoroughly assess real-world deployment capabilities, we
innovatively propose the Finova evaluation benchmark, which focuses on
agent-level financial reasoning and compliance verification. Experimental
results demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art
performance on financial tasks but also exhibits exceptional general reasoning
capabilities, validating its effectiveness as a trustworthy solution for
high-stakes financial applications.

</details>


### [126] [LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework for Multi-Step and Cross-Cultural Inference with LLMs](https://arxiv.org/abs/2507.16809)
*Da-Chen Lian,Ri-Sheng Huang,Pin-Er Chen,Chunki Lim,You-Kuan Lin,Guan-Yu Tseng,Zi-Cheng Yang,Shu-Kai Hsieh*

Main category: cs.CL

TL;DR: LingBench++是一个专为复杂语言学任务设计的新基准，侧重过程推理、多语言和认知合理性，通过整合多代理系统提升LLM的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评测过于关注最终答案准确性，忽视了过程推理能力，并且在多语言、文化和语言类型多样性等方面不足。作者希望用更精细的指标和广泛的语言覆盖，推动LLM在语言推理与认知层次的进步。

Method: 作者开发了LingBench++，采用源自国际语言学奥林匹克（IOL）的复杂任务，设计了系统化的推理轨迹评估协议和语言类型元数据。它支持90多种低资源及跨文化语言。此外，构建了包含语法知识检索、工具辅助推理和假设测试的多智能体架构，并将新模型与基线模型进行对比实验。

Result: 实验表明：配备外部知识检索和迭代推理机制的多智能体模型，在任务准确率和可解释性方面均显著优于仅做一次推理的传统方法。

Conclusion: LingBench++为大模型的语言推理、文化适应性和认知合理性评测提供了全面、科学的基础框架，有助于推动此领域新进展。

Abstract: We propose LingBench++, a linguistically-informed benchmark and reasoning
framework designed to evaluate large language models (LLMs) on complex
linguistic tasks inspired by the International Linguistics Olympiad (IOL).
Unlike prior benchmarks that focus solely on final answer accuracy, LingBench++
provides structured reasoning traces, stepwise evaluation protocols, and rich
typological metadata across over 90 low-resource and cross-cultural languages.
We further develop a multi-agent architecture integrating grammatical knowledge
retrieval, tool-augmented reasoning, and deliberate hypothesis testing. Through
systematic comparisons of baseline and our proposed agentic models, we
demonstrate that models equipped with external knowledge sources and iterative
reasoning outperform single-pass approaches in both accuracy and
interpretability. LingBench++ offers a comprehensive foundation for advancing
linguistically grounded, culturally informed, and cognitively plausible
reasoning in LLMs.

</details>


### [127] [MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning](https://arxiv.org/abs/2507.16812)
*Run-Ze Fan,Zengzhi Wang,Pengfei Liu*

Main category: cs.CL

TL;DR: 该论文提出了大规模开源科学推理数据集TextbookReasoning和MegaScience，并使用这些数据集提升了基础大模型在科学推理任务上的表现，同时公开了数据处理流程、评测体系和多款训练模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI社区主要关注数学和编程，科学推理领域因缺乏大规模高质量可验证数据集而发展缓慢。为促进科学推理研究，需要填补这一数据资源的空白。

Method: 作者首先从1.2万本大学教材中提取了高质量答案，构建了包含65万问题的TextbookReasoning数据集，涵盖7个科学学科。进一步通过系统消融实验筛选优化，集合多个高质量开源数据集，构建了拥有125万实例的MegaScience数据集。还建立了覆盖15个基准的科学推理评测体系。

Result: 实验显示新数据集比现有开源科学数据集能显著提升模型表现和训练效率，且生成答案更简洁。用MegaScience训练的Llama3.1、Qwen2.5和Qwen3基础模型均大幅超越官方instruct模型，尤其模型越大提升越明显。

Conclusion: MegaScience和相关工具为科学推理AI的研究提供了坚实支持。开放数据处理流程、评测系统和多个模型，有助于推动社区在高质量科学推理AI方向上的发展。

Abstract: Scientific reasoning is critical for developing AI scientists and supporting
human researchers in advancing the frontiers of natural science discovery.
However, the open-source community has primarily focused on mathematics and
coding while neglecting the scientific domain, largely due to the absence of
open, large-scale, high-quality, verifiable scientific reasoning datasets. To
bridge this gap, we first present TextbookReasoning, an open dataset featuring
truthful reference answers extracted from 12k university-level scientific
textbooks, comprising 650k reasoning questions spanning 7 scientific
disciplines. We further introduce MegaScience, a large-scale mixture of
high-quality open-source datasets totaling 1.25 million instances, developed
through systematic ablation studies that evaluate various data selection
methodologies to identify the optimal subset for each publicly available
scientific dataset. Meanwhile, we build a comprehensive evaluation system
covering diverse subjects and question types across 15 benchmarks,
incorporating comprehensive answer extraction strategies to ensure accurate
evaluation metrics. Our experiments demonstrate that our datasets achieve
superior performance and training efficiency with more concise response lengths
compared to existing open-source scientific datasets. Furthermore, we train
Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which
significantly outperform the corresponding official instruct models in average
performance. In addition, MegaScience exhibits greater effectiveness for larger
and stronger models, suggesting a scaling benefit for scientific tuning. We
release our data curation pipeline, evaluation system, datasets, and seven
trained models to the community to advance scientific reasoning research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [128] [Fast Task Planning with Neuro-Symbolic Relaxation](https://arxiv.org/abs/2507.15975)
*Qiwei Du,Bowen Li,Yi Du,Shaoshu Su,Taimeng Fu,Zitong Zhan,Zhipeng Zhao,Chen Wang*

Main category: cs.RO

TL;DR: 本文提出了一种结合神经网络和符号推理的新策略Flax，用于提升复杂环境中任务规划的效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 现实世界任务规划涉及大量实体及其复杂关系，带来了组合爆炸问题。现有方法基于神经网络筛选“重要”实体，但容易遗漏关键元素，导致方案不可行。

Method: Flax方法分三步：1）通过图神经网络预测实体重要性并生成简化任务让符号规划器求解；2）解决放宽限制的规则任务以迅速获得粗略计划，并将涉及的所有实体重新纳入任务中，防止遗漏；3）最后用补充规则修正和收缩任务，确保高可靠性且紧凑。

Result: 在合成和真实世界迷宫导航任务中，Flax相较最新NeSy基线，平均成功率提升20.82%，平均规划时间下降17.65%。

Conclusion: Flax显著提升了复杂环境下长时间规划的速度和可扩展性，为实际应用中高效、可靠的任务规划提供了新路径。

Abstract: Real-world task planning requires long-horizon reasoning over large sets of
entities with complex relationships and attributes, leading to a combinatorial
explosion for classical symbolic planners. To prune the search space, recent
methods prioritize searching on a simplified task only containing a few
"important" entities predicted by a neural network. However, such a simple
neuro-symbolic (NeSy) integration risks omitting critical entities and wasting
resources on unsolvable simplified tasks. To enable Fast and reliable planning,
we introduce a NeSy relaxation strategy (Flax), combining neural importance
prediction with symbolic expansion. Specifically, we first learn a graph neural
network to predict entity importance to create a simplified task and solve it
with a symbolic planner. Then, we solve a rule-relaxed task to obtain a quick
rough plan, and reintegrate all referenced entities into the simplified task to
recover any overlooked but essential elements. Finally, we apply complementary
rules to refine the updated task, keeping it both reliable and compact.
Extensive experiments are conducted on both synthetic and real-world maze
navigation benchmarks where a robot must traverse through a maze and interact
with movable objects. The results show that Flax boosts the average success
rate by 20.82% and cuts mean wall-clock planning time by 17.65% compared with
the state-of-the-art NeSy baseline. We expect that Flax offers a practical path
toward fast, scalable, long-horizon task planning in complex environments.

</details>


### [129] [A Comprehensive Evaluation of LiDAR Odometry Techniques](https://arxiv.org/abs/2507.16000)
*Easton Potokar,Michael Kaess*

Main category: cs.RO

TL;DR: 本论文系统梳理并实证分析了LiDAR里程计（LO）算法管道各组成技术，给出了未来LO管道设计建议。


<details>
  <summary>Details</summary>
Motivation: 虽然已有大量LO管道相关对比研究，但对于单一组件的消融实验和对比极为有限，难以系统优化LO方法。

Method: 作者梳理并归纳LO管道的不同实现技术，然后在多种环境、激光雷达种类和车辆运动条件下的大量公开数据集上，对各个LO模块进行消融实验和实证评估。

Result: 实验对比了不同LO技术组件的效果，系统揭示了其优劣，并据此总结了设计高性能LO算法的方法建议。

Conclusion: 论文为LO系统构建提供了实证依据，并提出了提升LO精度与可靠性的最佳实践，对相关算法开发具有参考价值。

Abstract: Light Detection and Ranging (LiDAR) sensors have become the sensor of choice
for many robotic state estimation tasks. Because of this, in recent years there
has been significant work done to fine the most accurate method to perform
state estimation using these sensors. In each of these prior works, an
explosion of possible technique combinations has occurred, with each work
comparing LiDAR Odometry (LO) "pipelines" to prior "pipelines". Unfortunately,
little work up to this point has performed the significant amount of ablation
studies comparing the various building-blocks of a LO pipeline. In this work,
we summarize the various techniques that go into defining a LO pipeline and
empirically evaluate these LO components on an expansive number of datasets
across environments, LiDAR types, and vehicle motions. Finally, we make
empirically-backed recommendations for the design of future LO pipelines to
provide the most accurate and reliable performance.

</details>


### [130] [Improved Semantic Segmentation from Ultra-Low-Resolution RGB Images Applied to Privacy-Preserving Object-Goal Navigation](https://arxiv.org/abs/2507.16034)
*Xuying Huang,Sicong Pan,Olga Zatsarynna,Juergen Gall,Maren Bennewitz*

Main category: cs.RO

TL;DR: 该论文提出一种新方法，实现移动机器人在超低分辨率视觉下的语义导航，兼顾任务性能与用户隐私。创新在于联合式学习方法，提升了分割效果和导航成功率，对比基线有显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有移动机器人隐私保护方法常以牺牲任务效果为代价，仅偏重一方。实际需求要求同时保障任务性能（如导航准确率）与视觉隐私，因此需要在超低分辨率条件下提升图像的语义理解能力。

Method: 设计了一种新型联合学习体系，包括聚合特征提取器和感知分割判别器，可处理超低分辨率RGB图像的语义分割任务。该系统允许精准恢复语义信息，支持隐私友好型的导航任务。

Result: 所提方法在超低分辨率下语义分割效果优于多种对比方法，同时分割效果的提升显著增加了实际隐私受限场景下语义目标导航的成功率。

Conclusion: 新的联动方法实现了在高隐私保护前提下的高效语义对象导航，为隐私敏感场景下的移动机器人应用提供了切实可行的技术方案，并优于现有主流方法。

Abstract: User privacy in mobile robotics has become a critical concern. Existing
methods typically prioritize either the performance of downstream robotic tasks
or privacy protection, with the latter often constraining the effectiveness of
task execution. To jointly address both objectives, we study semantic-based
robot navigation in an ultra-low-resolution setting to preserve visual privacy.
A key challenge in such scenarios is recovering semantic segmentation from
ultra-low-resolution RGB images. In this work, we introduce a novel fully
joint-learning method that integrates an agglomerative feature extractor and a
segmentation-aware discriminator to solve ultra-low-resolution semantic
segmentation, thereby enabling privacy-preserving, semantic object-goal
navigation. Our method outperforms different baselines on ultra-low-resolution
semantic segmentation and our improved segmentation results increase the
success rate of the semantic object-goal navigation in a real-world
privacy-constrained scenario.

</details>


### [131] [Therapist-Exoskeleton-Patient Interaction: An Immersive Gait Therapy](https://arxiv.org/abs/2507.16059)
*Emek Barış Küçüktabak,Matthew R. Short,Lorenzo Vianello,Daniel Ludvig,Levi Hargrove,Kevin Lynch,Jose Pons*

Main category: cs.RO

TL;DR: 本文提出了一种新型步态康复方法，即物理人-机器人-人交互（pHRHI）训练模式，并通过实验证明其在中风患者康复中优于传统方式。


<details>
  <summary>Details</summary>
Motivation: 中风后患者经常出现由于下肢无力及关节控制能力下降导致的运动与平衡障碍。传统的康复训练通常需要治疗师高强度人工辅助，这不仅消耗体力，也影响多关节协作的训练效果。虽然机器人外骨骼设备能够减轻治疗师压力，并提供多关节支持和客观反馈，但目前的控制方法往往限制了治疗师的主动参与和适应性，因此亟需创新互动型康复模式。

Method: 研究提出物理人-机器人-人交互（pHRHI）步态康复模式。在该模式下，治疗师与中风患者分别穿戴下肢外骨骼，并通过虚拟的弹簧-阻尼连接在髋部和膝部，从而能实现治疗师与患者之间的双向力反馈和同步协作运动。在包含八名慢性中风患者的实验证明了其有效性。

Result: 实验结果显示，pHRHI训练组在关节活动范围、步态参数、肌肉激活和患者康复积极性等方面均优于传统治疗师引导的跑步机训练组。

Conclusion: pHRHI模式能够很好地融合机器人精度与治疗师的直觉引导，有望在中风患者步态康复领域实现更佳的治疗效果。

Abstract: Following a stroke, individuals often experience mobility and balance
impairments due to lower-limb weakness and loss of independent joint control.
Gait recovery is a key goal of rehabilitation, traditionally achieved through
high-intensity therapist-led training. However, manual assistance can be
physically demanding and limits the therapist's ability to interact with
multiple joints simultaneously. Robotic exoskeletons offer multi-joint support,
reduce therapist strain, and provide objective feedback, but current control
strategies often limit therapist involvement and adaptability.
  We present a novel gait rehabilitation paradigm based on physical
Human-Robot-Human Interaction (pHRHI), where both the therapist and the
post-stroke individual wear lower-limb exoskeletons virtually connected at the
hips and knees via spring-damper elements. This enables bidirectional
interaction, allowing the therapist to guide movement and receive haptic
feedback. In a study with eight chronic stroke patients, pHRHI training
outperformed conventional therapist-guided treadmill walking, leading to
increased joint range of motion, step metrics, muscle activation, and
motivation. These results highlight pHRHI's potential to combine robotic
precision with therapist intuition for improved rehabilitation outcomes.

</details>


### [132] [Compositional Coordination for Multi-Robot Teams with Large Language Models](https://arxiv.org/abs/2507.16068)
*Zhehui Huang,Guangyao Shi,Yuwei Wu,Vijay Kumar,Gaurav S. Sukhatme*

Main category: cs.RO

TL;DR: 本论文提出了LAN2CB框架，可将多机器人任务的自然语言描述自动转化为可执行代码，简化并普适化多机器人协作流程。


<details>
  <summary>Details</summary>
Motivation: 传统多机器人协作依赖专家手工将任务自然语言描述转化为代码，过程繁琐、不易扩展且对非专家不友好。作者希望利用大语言模型（LLMs）提升任务转化自动化程度和通用性。

Method: 提出LAN2CB框架，包括任务分解（将任务解析为带有依赖关系的任务图）和代码生成（基于任务图和知识库生成可部署机器人控制代码）两个核心模块。同时构建了自然语言任务规范数据集用于开发与评测。

Result: LAN2CB在仿真和实际环境中均表现出能够根据自然语言任务实现高效灵活的多机器人协作，大幅减少人工工程工作，并支持多任务类型的泛化。

Conclusion: LAN2CB框架有效促进了多机器人系统从自然语言到执行的转化流程，提高了自动化与通用性，降低了对专业知识的依赖，对多机器人领域的实际应用具有重要意义。

Abstract: Multi-robot coordination has traditionally relied on a task-specific and
expert-driven pipeline, where natural language mission descriptions are
manually translated by domain experts into mathematical formulation, algorithm
design, and executable code. This conventional process is labor-intensive,
inaccessible to non-experts, and inflexible to changes in mission requirements.
Here, we propose LAN2CB (Language to Collective Behavior), a novel framework
that leverages large language models (LLMs) to streamline and generalize the
multi-robot coordination pipeline. LAN2CB directly converts natural language
mission descriptions into executable Python code for multi-robot systems
through two key components: (1) Mission Decomposition for Task Representation,
which parses the mission into a task graph with dependencies, and (2) Code
Generation, which uses the task graph and a structured knowledge base to
generate deployable robot control code. We further introduce a dataset of
natural language mission specifications to support development and
benchmarking. Experimental results in both simulation and real-world settings
show that LAN2CB enables effective and flexible multi-robot coordination from
natural language, significantly reducing the need for manual engineering while
supporting generalization across mission types. Website:
https://sites.google.com/view/lan2cb.

</details>


### [133] [FTIN: Frequency-Time Integration Network for Inertial Odometry](https://arxiv.org/abs/2507.16120)
*Shanshan Zhang,Qi Zhang,Siyue Wang,Tianshui Wen,Ziheng Zhou,Lingxiang Zheng,Yu Yang*

Main category: cs.RO

TL;DR: 本文提出了一种结合频域和时域信息的新型惯性测程网络架构，有效提升了定位精度。在多个公开数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的惯性测程方法大多基于时域CNN，难以捕捉IMU数据中的长期依赖性，限制了定位精度的进一步提升。

Method: 提出了将频域和时域信息融合的网络架构。利用频域的全局视角和能量集中特性来建模长期依赖关系，同时引入Scalar LSTM捕捉时序依赖，实现跨域信息融合，提高惯性定位的稳定性和准确性。

Result: 在多个公开数据集（例如RIDI、RoNIN、OxIOD等）上实验，提出的方法显著优于现有方法。在RoNIN数据集上，绝对轨迹误差降低43.0%，相对轨迹误差降低13.1%。

Conclusion: 频域与时域融合的方式显著提升了惯性测程的定位精度，为惯性导航领域带来了新的设计思路和更稳定可靠的解决方案。

Abstract: In recent years, machine learning has achieved significant advancements in
inertial odometry. However, most existing inertial odometry methods primarily
rely on CNNs in the time domain. These methods often struggle to capture
long-term dependency in inertial measurement unit data, thereby constraining
the potential for further improvements in localization accuracy. To address
these issues, we propose a novel network architecture that integrates both
frequency-domain and time-domain information. Specifically, we leverage the
global view and energy compaction properties of frequency-domain learning to
effectively model long-term dependency and reduce redundancy in IMU data.
Additionally, we introduce a Scalar LSTM to capture sequential dependencies in
the time domain, enabling cross-domain information fusion and providing a
stable and reliable reference for localization. Experimental evaluations on
multiple public datasets (e.g., RIDI, RoNIN, OxIOD, RNIN, TLIO, and IMUNet)
demonstrate the effectiveness of the proposed frequency-time domain fusion
strategy. Notably, on the RoNIN dataset, our method achieves a 43.0% reduction
in absolute trajectory error and a 13.1% reduction in relative trajectory error
compared to RoNIN ResNet.

</details>


### [134] [DWSFormer: A Lightweight Inertial Odometry Network for Complex Motion Modeling](https://arxiv.org/abs/2507.16121)
*Shanshan Zhang,Qi Zhang,Siyue Wang,Tianshui Wen,Ziheng Zhou,Lingxiang Zheng,Yu Yang*

Main category: cs.RO

TL;DR: 本文提出了一种轻量级惯性测程（IO）框架，通过投影到高维特征空间、协同注意力机制、多尺度门控卷积单元，显著提升了在复杂运动场景下的位置估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有的惯性测程方法在面对转弯等复杂运动模式时易产生漂移误差，极大限制了其在现实场景下的应用，因此需要更鲁棒的IO方案以提升定位准确性。

Method: 1）利用Star Operation方法将惯性数据投影到高维非线性特征空间，充分提取复杂运动特征；2）引入协同注意力机制，在通道和时序维度联合建模全局运动动态；3）设计多尺度门控卷积单元，捕获整个运动过程中的细粒度动态变化。

Result: 在六个主流惯性数据集上，本方法均优于SOTA基线，RoNIN数据集上ATE降低2.26%-65.78%，刷新领域新基准。

Conclusion: 提出的方法有效缓解了复杂运动下的漂移问题，提升了惯性测程系统在真实应用场景中的实用性和精度。

Abstract: Inertial odometry (IO) directly estimates the position of a carrier from
inertial sensor measurements and serves as a core technology for the widespread
deployment of consumer grade localization systems. While existing IO methods
can accurately reconstruct simple and near linear motion trajectories, they
often fail to account for drift errors caused by complex motion patterns such
as turning. This limitation significantly degrades localization accuracy and
restricts the applicability of IO systems in real world scenarios. To address
these challenges, we propose a lightweight IO framework. Specifically, inertial
data is projected into a high dimensional implicit nonlinear feature space
using the Star Operation method, enabling the extraction of complex motion
features that are typically overlooked. We further introduce a collaborative
attention mechanism that jointly models global motion dynamics across both
channel and temporal dimensions. In addition, we design Multi Scale Gated
Convolution Units to capture fine grained dynamic variations throughout the
motion process, thereby enhancing the model's ability to learn rich and
expressive motion representations. Extensive experiments demonstrate that our
proposed method consistently outperforms SOTA baselines across six widely used
inertial datasets. Compared to baseline models on the RoNIN dataset, it
achieves reductions in ATE ranging from 2.26% to 65.78%, thereby establishing a
new benchmark in the field.

</details>


### [135] [Benchmarking LLM Privacy Recognition for Social Robot Decision Making](https://arxiv.org/abs/2507.16124)
*Dakota Sullivan,Shirley Zhang,Jennica Li,Heather Kirkorian,Bilge Mutlu,Kassem Fawaz*

Main category: cs.RO

TL;DR: 本论文探讨了大型语言模型（LLMs）赋能的社交机器人在家庭环境中对隐私的感知和处理能力，发现LLMs在隐私认知方面与人类用户存在较大差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的快速发展，它们被引入社交机器人以增强人机交互能力。但这种应用需收集和处理家庭环境中的个人敏感数据，引发了隐私风险与实际效用的权衡。因此，评估LLMs在处理敏感数据时的隐私意识变得尤为重要。

Method: 研究首先通过Contextual Integrity（情境完整性）框架设计了一系列与隐私相关的社交机器人场景；对450名用户进行了隐私偏好调查，并分析其隐私取向对机器人行为选择的影响。同时，将同样的场景和问题提供给10个主流LLMs，比较其与人类用户的隐私判断一致性。此外，还测试了四种不同的提示词策略以提升LLMs的隐私控制表现。

Result: 结果显示，现有LLMs与人类用户在隐私偏好和判断上低度一致，不同提示策略对LLMs的隐私控制能力提升有限。

Conclusion: LLMs目前尚不具备与人类相当的隐私意识和判断，需进一步改进，以保障人机交互中用户的隐私和安全。

Abstract: Social robots are embodied agents that interact with people while following
human communication norms. These robots interact using verbal and non-verbal
cues, and share the physical environments of people. While social robots have
previously utilized rule-based systems or probabilistic models for user
interaction, the rapid evolution of large language models (LLMs) presents new
opportunities to develop LLM-empowered social robots for enhanced human-robot
interaction. To fully realize these capabilities, however, robots need to
collect data such as audio, fine-grained images, video, and locations. As a
result, LLMs often process sensitive personal information, particularly within
home environments. Given the tension between utility and privacy risks,
evaluating how current LLMs manage sensitive data is critical. Specifically, we
aim to explore the extent to which out-of-the-box LLMs are privacy-aware in the
context of household social robots. In this study, we present a set of
privacy-relevant scenarios crafted through the lens of Contextual Integrity
(CI). We first survey users' privacy preferences regarding in-home social robot
behaviors and then examine how their privacy orientation affects their choices
of these behaviors (N = 450). We then provide the same set of scenarios and
questions to state-of-the-art LLMs (N = 10) and find that the agreement between
humans and LLMs is low. To further investigate the capabilities of LLMs as a
potential privacy controller, we implement four additional prompting strategies
and compare their results. Finally, we discuss the implications and potential
of AI privacy awareness in human-robot interaction.

</details>


### [136] [Equivariant Goal Conditioned Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.16139)
*Arsh Tangri,Nichols Crawford Taylor,Haojie Huang,Robert Platt*

Main category: cs.RO

TL;DR: 本文提出了一种在无标签交互中学习高效策略的方法，通过引入等变性约束提升空间泛化能力，实验上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的对比强化学习（CRL）可以无需人工奖励学习有效策略，但对于具有空间对称性的机器人任务，现有方法未能充分利用这些内在对称性，导致泛化能力和样本效率有限。

Method: 提出Equivariant CRL（ECRL），在对比强化学习基础上融入等变性约束。具体做法包括：正式定义具有旋转对称性的目标条件马尔可夫决策过程（Goal-Conditioned Group-Invariant MDP），设计旋转不变的评论家和旋转等变的行为者，提升学习表示的结构性和泛化能力。

Result: ECRL在多种仿真任务上（包括状态和图像输入场景）均超过了强基线方法，并在离线强化学习设置下同样表现良好。

Conclusion: ECRL能有效利用任务对称性，提升样本效率和空间泛化能力，为无奖励的强化学习及其推广到离线环境提供了新思路。

Abstract: Contrastive Reinforcement Learning (CRL) provides a promising framework for
extracting useful structured representations from unlabeled interactions. By
pulling together state-action pairs and their corresponding future states,
while pushing apart negative pairs, CRL enables learning nontrivial policies
without manually designed rewards. In this work, we propose Equivariant CRL
(ECRL), which further structures the latent space using equivariant
constraints. By leveraging inherent symmetries in goal-conditioned manipulation
tasks, our method improves both sample efficiency and spatial generalization.
Specifically, we formally define Goal-Conditioned Group-Invariant MDPs to
characterize rotation-symmetric robotic manipulation tasks, and build on this
by introducing a novel rotation-invariant critic representation paired with a
rotation-equivariant actor for Contrastive RL. Our approach consistently
outperforms strong baselines across a range of simulated tasks in both
state-based and image-based settings. Finally, we extend our method to the
offline RL setting, demonstrating its effectiveness across multiple tasks.

</details>


### [137] [Scanning Bot: Efficient Scan Planning using Panoramic Cameras](https://arxiv.org/abs/2507.16175)
*Euijeong Lee,Kyung Min Han,Young J. Kim*

Main category: cs.RO

TL;DR: 本文提出一种全自动扫描规划方法，实现全景RGB-D相机的高效3D建模，显著提升扫描覆盖率和速度。


<details>
  <summary>Details</summary>
Motivation: 全景RGB-D相机虽然能够生成高质量3D场景重建，但操作繁琐、选点和搬运需人工完成，且新手用户操作容易出错。因此迫切需要提升扫描效率和易用性。

Method: 提出一种自主扫描规划算法，能自动生成高效的扫描路径规划，确保扫描过程中视点帧之间有足够重叠并避免碰撞。

Result: 在合成环境和真实世界中进行大量实验，相较于当前最先进的方案，新方法在实际场景中平均扫描覆盖率达99%，扫描总时间最多缩短至1/3。

Conclusion: 提出的方法不仅提升全景RGB-D扫描的自动化程度，并显著优化建模效率和质量，在覆盖率和扫描时间两个维度均优于现有主流方法。

Abstract: Panoramic RGB-D cameras are known for their ability to produce high quality
3D scene reconstructions. However, operating these cameras involves manually
selecting viewpoints and physically transporting the camera, making the
generation of a 3D model time consuming and tedious. Additionally, the process
can be challenging for novice users due to spatial constraints, such as
ensuring sufficient feature overlap between viewpoint frames. To address these
challenges, we propose a fully autonomous scan planning that generates an
efficient tour plan for environment scanning, ensuring collision-free
navigation and adequate overlap between viewpoints within the plan. Extensive
experiments conducted in both synthetic and real-world environments validate
the performance of our planner against state-of-the-art view planners. In
particular, our method achieved an average scan coverage of 99 percent in the
real-world experiment, with our approach being up to 3 times faster than
state-of-the-art planners in total scan time.

</details>


### [138] [Adaptive Relative Pose Estimation Framework with Dual Noise Tuning for Safe Approaching Maneuvers](https://arxiv.org/abs/2507.16214)
*Batu Candan,Simone Servadio*

Main category: cs.RO

TL;DR: 本文提出了一个结合先进计算机视觉和自适应非线性滤波的完整流程，用于准确、鲁棒地估算废弃卫星的相对位姿，提升主动空间碎片清除任务的能力。


<details>
  <summary>Details</summary>
Motivation: 在执行主动空间碎片清除任务（如针对正在旋转的ENVISAT卫星）时，准确和鲁棒的相对位姿估算是任务成功的关键，但现有方案在动态不确定、图像测量不稳定等方面存在短板。

Method: 流程中首先利用预处理后的卷积神经网络（CNN）检测画面中的结构性标记点（如角点），结合相机建模将2D坐标还原为3D测量值。随后将该信息输入无迹卡尔曼滤波器（UKF），不仅动态调整观测噪声协方差应对CNN检测不稳定，同时通过观测残差自适应调整过程噪声协方差，以在线应对动力学未知因素。两者联动增强了整体鲁棒性。

Result: 在高保真ENVISAT模型仿真框架下，全文方法的定位估算效果在各种情况下表现优异，包括测量缺失时也优于基线方法，充分验证了对测量不完美性和动力学不确定性的鲁棒性。

Conclusion: 本文所提出的自适应集成系统为在轨任务提供了更为鲁棒、精确的相对导航解决方案，有效提升了主动碎片清除任务期间安全接近与操作的能力。

Abstract: Accurate and robust relative pose estimation is crucial for enabling
challenging Active Debris Removal (ADR) missions targeting tumbling derelict
satellites such as ESA's ENVISAT. This work presents a complete pipeline
integrating advanced computer vision techniques with adaptive nonlinear
filtering to address this challenge. A Convolutional Neural Network (CNN),
enhanced with image preprocessing, detects structural markers (corners) from
chaser imagery, whose 2D coordinates are converted to 3D measurements using
camera modeling. These measurements are fused within an Unscented Kalman Filter
(UKF) framework, selected for its ability to handle nonlinear relative
dynamics, to estimate the full relative pose. Key contributions include the
integrated system architecture and a dual adaptive strategy within the UKF:
dynamic tuning of the measurement noise covariance compensates for varying CNN
measurement uncertainty, while adaptive tuning of the process noise covariance,
utilizing measurement residual analysis, accounts for unmodeled dynamics or
maneuvers online. This dual adaptation enhances robustness against both
measurement imperfections and dynamic model uncertainties. The performance of
the proposed adaptive integrated system is evaluated through high-fidelity
simulations using a realistic ENVISAT model, comparing estimates against ground
truth under various conditions, including measurement outages. This
comprehensive approach offers an enhanced solution for robust onboard relative
navigation, significantly advancing the capabilities required for safe
proximity operations during ADR missions.

</details>


### [139] [GFM-Planner: Perception-Aware Trajectory Planning with Geometric Feature Metric](https://arxiv.org/abs/2507.16233)
*Yue Lin,Xiaoxuan Zhang,Yang Liu,Dong Wang,Huchuan Lu*

Main category: cs.RO

TL;DR: 该论文提出了一种感知感知的路径规划框架GFM-Planner，通过指导机器人避开特征稀疏区域，提升基于激光雷达的定位精度。


<details>
  <summary>Details</summary>
Motivation: 机器人定位依赖于环境中的丰富特征，特征稀疏区域会导致定位精度下降。作者希望解决机器人在特征稀疏环境下的定位退化问题。

Method: 作者首先从激光雷达定位的基本问题推导出一个几何特征度量（GFM），用于评估环境特征的丰富程度。然后，设计了二维网格的度量编码地图（MEM）用于高效存储环境各位置的GFM值，并提出了常数时间的解码算法以查询任意姿态的GFM值。最后，开发了一种基于感知的路径规划方法，指导机器人优先选择特征丰富区域的路线。

Result: 仿真和实物实验表明，该方法能有效指导机器人选择能够显著提升激光雷达定位精度的移动路径。

Conclusion: GFM-Planner通过环境特征度量与感知感知路径规划，提升了机器人在复杂环境中的定位表现，为自主机器人定位提供了更可靠的方案。

Abstract: Like humans who rely on landmarks for orientation, autonomous robots depend
on feature-rich environments for accurate localization. In this paper, we
propose the GFM-Planner, a perception-aware trajectory planning framework based
on the geometric feature metric, which enhances LiDAR localization accuracy by
guiding the robot to avoid degraded areas. First, we derive the Geometric
Feature Metric (GFM) from the fundamental LiDAR localization problem. Next, we
design a 2D grid-based Metric Encoding Map (MEM) to efficiently store GFM
values across the environment. A constant-time decoding algorithm is further
proposed to retrieve GFM values for arbitrary poses from the MEM. Finally, we
develop a perception-aware trajectory planning algorithm that improves LiDAR
localization capabilities by guiding the robot in selecting trajectories
through feature-rich areas. Both simulation and real-world experiments
demonstrate that our approach enables the robot to actively select trajectories
that significantly enhance LiDAR localization accuracy.

</details>


### [140] [Trajectory Planning of a Curtain Wall Installation Robot Based on Biomimetic Mechanisms](https://arxiv.org/abs/2507.16305)
*Xiao Liu,Weijun Wang,Tianlun Huang,Zhiyong Wang,Wei Feng*

Main category: cs.RO

TL;DR: 本文受到人类手臂举重动作的启发，提出了一种仿生轨迹规划方法，通过引入人体能量转换机制，大幅降低了施工机器人能耗，在幕墙安装任务仿真中能耗减少了48.4%。


<details>
  <summary>Details</summary>
Motivation: 随着机器人市场的发展，能耗问题愈发突出，尤其在施工机器人领域已成为限制其应用的重要因素。当前缺乏高效能耗优化方法，因此亟需创新性解决方案。

Method: 本研究采集人体在哑铃弯举过程中的运动轨迹与肌电信号，分析人体发力与能量消耗模式，构建仿真人体动作的轨迹规划。采用粒子群优化（PSO）算法，以人类动作特征实现机械臂轨迹的动态载荷分配，并将该仿生特性应用于幕墙安装任务。

Result: 仿真实验表明，采用文中提出的仿生轨迹规划方式，机械臂在幕墙安装任务中能耗降低了48.4%，兼具正确性与优越性。

Conclusion: 基于人体能量转换原理的仿生轨迹规划方法能够显著提升施工机器人的能耗表现，为实际搬运场景下幕墙安装机器人能耗优化提供了新思路和理论依据。

Abstract: As the robotics market rapidly evolves, energy consumption has become a
critical issue, particularly restricting the application of construction
robots. To tackle this challenge, our study innovatively draws inspiration from
the mechanics of human upper limb movements during weight lifting, proposing a
bio-inspired trajectory planning framework that incorporates human energy
conversion principles. By collecting motion trajectories and electromyography
(EMG) signals during dumbbell curls, we construct an anthropomorphic trajectory
planning that integrates human force exertion patterns and energy consumption
patterns. Utilizing the Particle Swarm Optimization (PSO) algorithm, we achieve
dynamic load distribution for robotic arm trajectory planning based on
human-like movement features. In practical application, these bio-inspired
movement characteristics are applied to curtain wall installation tasks,
validating the correctness and superiority of our trajectory planning method.
Simulation results demonstrate a 48.4% reduction in energy consumption through
intelligent conversion between kinetic and potential energy. This approach
provides new insights and theoretical support for optimizing energy use in
curtain wall installation robots during actual handling tasks.

</details>


### [141] [Design and Dimensional Optimization of Legged Structures for Construction Robots](https://arxiv.org/abs/2507.16328)
*Xiao Liu,Xianlong Yang,Weijun Wang,Wei Feng*

Main category: cs.RO

TL;DR: 论文受蚂蚁启发，提出针对施工环境的多关节机器腿结构优化方法，通过运动学建模、工作空间与平均操作性能等多维分析，并结合虚拟仿真，给出适合复杂地形的最优机器腿比例设计。


<details>
  <summary>Details</summary>
Motivation: 轮式与履带式机器人在复杂、非结构化施工环境中适应性与灵活性不足，难以自主运行，因此需研发更适合施工场景的腿式机器人。

Method: 分析腿在摆动和支撑阶段的运动性能，基于运动学建模与多维工作空间分析，提出“改进工作空间”概念并进行图解优化，再利用速度雅可比矩阵提出“平均可操作性”并通过数值方法优化腿段比例，最后采用ADAMS虚拟仿真进一步确定最佳比例。

Result: 获得了机器腿在施工环境下，摆动与支撑两阶段动作最优的腿段长度比例，实现了最佳综合运动性能。

Conclusion: 建立了首个针对施工环境的腿式机器人多维运动性能量化评估框架，为复杂地形下腿式机器人的结构设计与自主移动提供了理论依据和设计基础。

Abstract: Faced with complex and unstructured construction environments, wheeled and
tracked robots exhibit significant limitations in terrain adaptability and
flexibility, making it difficult to meet the requirements of autonomous
operation. Inspired by ants in nature, this paper proposes a leg configuration
design and optimization method tailored for construction scenarios, aiming to
enhance the autonomous mobility of construction robots. This paper analyzes the
full operational motion performance of the leg during both swing and stance
phases. First, based on kinematic modeling and multi-dimensional workspace
analysis, the concept of an "improved workspace" is introduced, and graphical
methods are used to optimize the leg dimensions during the swing phase.
Furthermore, a new concept of "average manipulability" is introduced based on
the velocity Jacobian matrix, and numerical solutions are applied to obtain the
leg segment ratio that maximizes manipulability. To overcome the difficulties
associated with traditional analytical methods, virtual prototype simulations
are conducted in ADAMS to explore the relationship between the robot body's
optimal flexibility and leg segment proportions. In summary, the leg segment
proportions with the best comprehensive motion performance are obtained. This
study presents the first multi-dimensional quantitative evaluation framework
for leg motion performance tailored for construction environments, providing a
structural design foundation for legged construction robots to achieve
autonomous mobility in complex terrains.

</details>


### [142] [Topology Optimization of Leg Structures for Construction Robots Based on Variable Density Method](https://arxiv.org/abs/2507.16335)
*Xiao Liu,Xianlong Yang,Weijun Wang,Wei Feng*

Main category: cs.RO

TL;DR: 该研究采用SIMP变密度法进行机器人腿结构拓扑优化，实现了减重、增效目标，最终优化后腿结构依然满足强度与模态性能要求。


<details>
  <summary>Details</summary>
Motivation: 在复杂地形施工环境中，机器人需兼顾高负载与灵活性的要求，而腿部结构作为关键承重部件，其优化对于性能提升尤为重要。当前对此方面的系统性优化研究相对不足。

Method: 采用基于SIMP变密度法的拓扑优化方法，对机器人腿部中重量占比最大的股骨段进行结构迭代优化，同时结合ANSYS进行静力学与模态分析，确保结构合理性。优化后对股骨进行二次重构并再次分析验证。

Result: 优化后机器人腿部股骨质量减少19.45%，整体腿部质量减少7.92%，且结构性能依然满足工程要求。

Conclusion: 提出的腿部拓扑优化与重设计方法有效实现了轻量化，同时保证了性能，为复杂环境下施工机器人的设计与高效应用提供了理论与技术支持。

Abstract: In complex terrain construction environments, there are high demands for
robots to achieve both high payload capacity and mobility flexibility. As the
key load-bearing component, the optimization of robotic leg structures is of
particular importance. Therefore, this study focuses on the optimization of leg
structures for construction robots, proposing a topology optimization strategy
based on the SIMP (Solid Isotropic Microstructures with Penalization) variable
density method along with a structural re-design approach. The design
performance is comprehensively validated through finite element analysis using
ANSYS. First, static and modal analyses are conducted to evaluate the
rationality of the initial design. Then, topology optimization using the
SIMP-based variable density method is applied to the femur section, which
accounts for the largest proportion of the leg's weight. Based on iterative
calculations, the femur undergoes secondary structural reconstruction. After
optimization, the mass of the femur is reduced by 19.45\%, and the overall leg
mass decreases by 7.92\%, achieving the goal of lightweight design. Finally,
static and modal analyses are conducted on the reconstructed leg. The results
demonstrate that the optimized leg still meets structural performance
requirements, validating the feasibility of lightweight design. This research
provides robust theoretical and technical support for lightweight construction
robot design and lays a foundation for their efficient operation in complex
construction environments.

</details>


### [143] [Humanoid Robot Whole-body Geometric Calibration with Embedded Sensors and a Single Plane](https://arxiv.org/abs/2507.16369)
*Thanh D V Nguyen,Vincent Bonnet,Pierre Fernbach,David Daney,Florent Lamiraux*

Main category: cs.RO

TL;DR: 提出了一种通过单一平面、嵌入式力传感器和从动控制器，实现仿人机器人全身运动学标定的新方法，并用IROC算法选取最优最小标定位姿，实验结果大幅提升精度。


<details>
  <summary>Details</summary>
Motivation: 传统的仿人机器人全身几何标定操作复杂且耗时，且常被忽视，但对于精确控制和仿真又非常关键，因此需探索更高效、实用的标定方法。

Method: 利用单一平面、嵌入式力传感器和从动控制器，实现免人工干预的全身运动学标定；提出IROC（最优标定位姿信息排序）算法，从候选姿势中筛选出最优最小姿势集用于标定。

Result: 在TALOS仿人机器人上进行了实验，仅用31个机器人末端三点接触的最优姿势完成了全身运动学链标定。交叉验证实验中，均方根误差相较于原厂模型降低了2.3倍。

Conclusion: 新方法可大幅简化和提升仿人机器人全身运动学标定流程和精度，减少所需校准动作数量，易于实际应用。

Abstract: Whole-body geometric calibration of humanoid robots using classical robot
calibration methods is a timeconsuming and experimentally burdensome task.
However, despite its significance for accurate control and simulation, it is
often overlooked in the humanoid robotics community. To address this issue, we
propose a novel practical method that utilizes a single plane, embedded force
sensors, and an admittance controller to calibrate the whole-body kinematics of
humanoids without requiring manual intervention. Given the complexity of
humanoid robots, it is crucial to generate and determine a minimal set of
optimal calibration postures. To do so, we propose a new algorithm called IROC
(Information Ranking algorithm for selecting Optimal Calibration postures).
IROC requires a pool of feasible candidate postures to build a normalized
weighted information matrix for each posture. Then, contrary to other
algorithms from the literature, IROC will determine the minimal number of
optimal postures that are to be played onto a robot for its calibration. Both
IROC and the single-plane calibration method were experimentally validated on a
TALOS humanoid robot. The total whole-body kinematics chain was calibrated
using solely 31 optimal postures with 3-point contacts on a table by the robot
gripper. In a cross-validation experiment, the average root-mean-square (RMS)
error was reduced by a factor of 2.3 compared to the manufacturer's model.

</details>


### [144] [Application of LLM Guided Reinforcement Learning in Formation Control with Collision Avoidance](https://arxiv.org/abs/2507.16382)
*Chenhao Yao,Zike Yuan,Xiaoxu Liu,Chi Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种基于大语言模型(LLM)的多智能体系统(MAS)奖励函数生成与调整框架，用于提升编队控制与避障性能，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在协作任务中表现突出。现有多智能体强化学习（MARL）在实现编队控制加避障时，难以设计有效奖励函数以加快收敛和性能提升，因此需要新的方法优化奖励设计。

Method: 提出利用大语言模型（LLM）结合任务优先级和代理可观测信息动态生成奖励函数，并通过高级评估指标在线调整奖励，而非仅依赖原始奖励本身，提升学习效率。

Result: 实验证明，所提出的方法在仿真和实际环境中均能显著提升多智能体系统在编队控制与避障任务上的效率和性能，收敛速度更快，表现更优。

Conclusion: 利用LLM动态生成和调整奖励函数是解决MARL中复杂目标任务收敛与效率问题的有效方案，对多智能体编队与避障具有实际应用价值。

Abstract: Multi-Agent Systems (MAS) excel at accomplishing complex objectives through
the collaborative efforts of individual agents. Among the methodologies
employed in MAS, Multi-Agent Reinforcement Learning (MARL) stands out as one of
the most efficacious algorithms. However, when confronted with the complex
objective of Formation Control with Collision Avoidance (FCCA): designing an
effective reward function that facilitates swift convergence of the policy
network to an optimal solution. In this paper, we introduce a novel framework
that aims to overcome this challenge. By giving large language models (LLMs) on
the prioritization of tasks and the observable information available to each
agent, our framework generates reward functions that can be dynamically
adjusted online based on evaluation outcomes by employing more advanced
evaluation metrics rather than the rewards themselves. This mechanism enables
the MAS to simultaneously achieve formation control and obstacle avoidance in
dynamic environments with enhanced efficiency, requiring fewer iterations to
reach superior performance levels. Our empirical studies, conducted in both
simulation and real-world settings, validate the practicality and effectiveness
of our proposed approach.

</details>


### [145] [AI or Human? Understanding Perceptions of Embodied Robots with LLMs](https://arxiv.org/abs/2507.16398)
*Lavinia Hriscu,Alberto Sanfeliu,Anais Garrell*

Main category: cs.RO

TL;DR: 本研究通过在机器人平台上进行图灵测试，发现参与者难以区分由AI或人类操控的机器人，结果为设计未来交互机器人提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 有效衡量人工智能智能水平一直是该领域的挑战，特别是在具身机器人与人类交互时，现有的图灵测试应用还不充分，因此有必要探索其在实际环境下的有效性。

Method: 研究招募了34名参与者，让他们在信息检索和包裹递送的两项交互任务中，判断操控机器人的是AI还是人类，任务涉及静态和动态环境下的机器人感知与导航能力。

Result: 实验结果显示，参与者无法超越随机水平去区分AI与人类操控的机器人。对回答的分析还揭示了影响具身机器人智能感知的关键因素。

Conclusion: 这些发现不仅为未来交互机器人的设计提供了参考，也对AI系统智能评估方法的相关讨论作出了贡献。

Abstract: The pursuit of artificial intelligence has long been associated to the the
challenge of effectively measuring intelligence. Even if the Turing Test was
introduced as a means of assessing a system intelligence, its relevance and
application within the field of human-robot interaction remain largely
underexplored. This study investigates the perception of intelligence in
embodied robots by performing a Turing Test within a robotic platform. A total
of 34 participants were tasked with distinguishing between AI- and
human-operated robots while engaging in two interactive tasks: an information
retrieval and a package handover. These tasks assessed the robot perception and
navigation abilities under both static and dynamic conditions. Results indicate
that participants were unable to reliably differentiate between AI- and
human-controlled robots beyond chance levels. Furthermore, analysis of
participant responses reveals key factors influencing the perception of
artificial versus human intelligence in embodied robotic systems. These
findings provide insights into the design of future interactive robots and
contribute to the ongoing discourse on intelligence assessment in AI-driven
systems.

</details>


### [146] [Distributed Oscillatory Guidance for Formation Flight of Fixed-Wing Drones](https://arxiv.org/abs/2507.16458)
*Yang Xu,Jesús Bautista,José Hinojosa,Héctor García de Marina*

Main category: cs.RO

TL;DR: 该论文提出了一种无需直接控制速度，便可实现固定翼无人机编队飞行的算法。通过在路径引导场上叠加振荡，使无人机间实现速度协调，并提出了一个新的分布式一致性算法，经理论、仿真和实飞验证。


<details>
  <summary>Details</summary>
Motivation: 固定翼无人机受限于只能在标称空速下飞行，直接通过速度制动难以实现编队协同。因此，需研究无需速度控制下的编队方法。

Method: 将无人机限定在特定路径（如平行直线）上飞行，并基于引导矢量场加入可调节振荡，引入分布式闭环调节，让每架无人机与邻居通信自动调整振荡幅度，从而间接控制在路径上的平均速度。该方法采用一种新颖的非负、非对称饱和函数实现一致性算法。

Result: 算法经过严格理论分析、数值仿真和真实无人机编队飞行实验验证，表明能够有效实现路径上的速度协调与编队飞行。

Conclusion: 无需直接控制速度的编队飞行为固定翼无人机提供了新的自主协同方法，对工程应用和多无人机制导具有实际意义。

Abstract: The autonomous formation flight of fixed-wing drones is hard when the
coordination requires the actuation over their speeds since they are critically
bounded and aircraft are mostly designed to fly at a nominal airspeed. This
paper proposes an algorithm to achieve formation flights of fixed-wing drones
without requiring any actuation over their speed. In particular, we guide all
the drones to travel over specific paths, e.g., parallel straight lines, and we
superpose an oscillatory behavior onto the guiding vector field that drives the
drones to the paths. This oscillation enables control over the average velocity
along the path, thereby facilitating inter-drone coordination. Each drone
adjusts its oscillation amplitude distributively in a closed-loop manner by
communicating with neighboring agents in an undirected and connected graph. A
novel consensus algorithm is introduced, leveraging a non-negative, asymmetric
saturation function. This unconventional saturation is justified since negative
amplitudes do not make drones travel backward or have a negative velocity along
the path. Rigorous theoretical analysis of the algorithm is complemented by
validation through numerical simulations and a real-world formation flight.

</details>


### [147] [Designing for Difference: How Human Characteristics Shape Perceptions of Collaborative Robots](https://arxiv.org/abs/2507.16480)
*Sabrina Livanec,Laura Londoño,Michael Gorki,Adrian Röfer,Abhinav Valada,Andrea Kiesel*

Main category: cs.RO

TL;DR: 本研究探讨了人机协作中，参与者如何基于机器人的行为和用户特征，对协作机器人进行评估。通过线上实验比较不同情境下的反馈，发现协作方式及人类特征会显著影响评估倾向。


<details>
  <summary>Details</summary>
Motivation: 协作型辅助机器人越来越多地用于社会互动，但如何确保其在与不同需求群体（如残障人士、高龄者）的交互中，具备责任感和包容性，目前研究较少。主要原因是现实中参与者缺乏与先进家用机器人的实际互动经验。

Method: 研究采用在线实验，邀请112名参与者分别观看并评价7段人机协作视频（共28种变体），实验组在评价前先进行认知-情感映射（CAM）自省练习，用以引导更有深度的反思。

Result: 认知-情感映射虽然对整体评分影响有限，但在特定机器人行为与人类需求组合下，会引发更极化的评价。整体上，反社会的机器人行为得到最低评价，与高龄者协作时，参与者评价更为敏感。涉及物体移交的协作场景评价更高。

Conclusion: 结果显示，人类特性与互动方式共同影响协作机器人可接受性，强调了亲社会、包容性设计的重要性。同时，反思性的方法（如CAM）有助于收集细致反馈，对面向多元人群的用户中心型负责任机器人系统开发有重要推动作用。

Abstract: The development of assistive robots for social collaboration raises critical
questions about responsible and inclusive design, especially when interacting
with individuals from protected groups such as those with disabilities or
advanced age. Currently, research is scarce on how participants assess varying
robot behaviors in combination with diverse human needs, likely since
participants have limited real-world experience with advanced domestic robots.
In the current study, we aim to address this gap while using methods that
enable participants to assess robot behavior, as well as methods that support
meaningful reflection despite limited experience. In an online study, 112
participants (from both experimental and control groups) evaluated 7 videos
from a total of 28 variations of human-robot collaboration types. The
experimental group first completed a cognitive-affective mapping (CAM) exercise
on human-robot collaboration before providing their ratings. Although CAM
reflection did not significantly affect overall ratings, it led to more
pronounced assessments for certain combinations of robot behavior and human
condition. Most importantly, the type of human-robot collaboration influences
the assessment. Antisocial robot behavior was consistently rated as the lowest,
while collaboration with aged individuals elicited more sensitive evaluations.
Scenarios involving object handovers were viewed more positively than those
without them. These findings suggest that both human characteristics and
interaction paradigms influence the perceived acceptability of collaborative
robots, underscoring the importance of prosocial design. They also highlight
the potential of reflective methods, such as CAM, to elicit nuanced feedback,
supporting the development of user-centered and socially responsible robotic
systems tailored to diverse populations.

</details>


### [148] [Guided Reinforcement Learning for Omnidirectional 3D Jumping in Quadruped Robots](https://arxiv.org/abs/2507.16481)
*Riccardo Bussola,Michele Focchi,Giulio Turrisi,Claudio Semini,Luigi Palopoli*

Main category: cs.RO

TL;DR: 本文提出了一种结合物理直觉的引导式强化学习方法，通过将Bézier曲线和匀加速直线运动（UARM）模型结合，实现了更高效、可解释的四足机器人跳跃控制，相比传统方法展现出显著优势。


<details>
  <summary>Details</summary>
Motivation: 当前四足机器人跳跃控制的优化方法效率低、对机器人和环境信息依赖大，强化学习方法则存在训练样本需求高和运动可预测性差的问题。这些不足限制了其在实际应用中鲁棒性和安全性的保证。

Method: 提出了基于物理规律（Bézier曲线与UARM模型）的引导式强化学习方法，融合物理模型指导网络学习跳跃运动，实现高效、可解释的控制策略。

Result: 实验和仿真都表明，所提方法在跳跃效率、解释性和性能鲁棒性等方面优于现有基线方法。

Conclusion: 通过引入物理直觉做为引导，该方法能高效、可解释地控制四足机器人跳跃，提升了实用性与安全性，有望推动机器人高动态动作的研究和应用。

Abstract: Jumping poses a significant challenge for quadruped robots, despite being
crucial for many operational scenarios. While optimisation methods exist for
controlling such motions, they are often time-consuming and demand extensive
knowledge of robot and terrain parameters, making them less robust in
real-world scenarios. Reinforcement learning (RL) is emerging as a viable
alternative, yet conventional end-to-end approaches lack efficiency in terms of
sample complexity, requiring extensive training in simulations, and
predictability of the final motion, which makes it difficult to certify the
safety of the final motion. To overcome these limitations, this paper
introduces a novel guided reinforcement learning approach that leverages
physical intuition for efficient and explainable jumping, by combining B\'ezier
curves with a Uniformly Accelerated Rectilinear Motion (UARM) model. Extensive
simulation and experimental results clearly demonstrate the advantages of our
approach over existing alternatives.

</details>


### [149] [A Target-based Multi-LiDAR Multi-Camera Extrinsic Calibration System](https://arxiv.org/abs/2507.16621)
*Lorenzo Gentilini,Pierpaolo Serio,Valentina Donzella,Lorenzo Pollini*

Main category: cs.RO

TL;DR: 本文提出了一种多激光雷达与多摄像头传感器套件的目标板外参标定系统，并通过实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶依赖多传感器系统，然而不同传感器的数据融合对齐困难，任何外参误差都可能影响车辆的安全。因此，需要更精确且通用的外参标定方法。

Method: 提出基于定制ChArUco棋盘和专门设计的非线性优化方法，实现多LiDAR与多摄像头之间的外参标定，并能在先验信息有限的情况下运行。

Result: 在实际仓库环境收集数据进行标定实验，结果显示该方法能有效实现不同类型传感器之间的高精度对齐。

Conclusion: 所提出的外参标定系统适用于多类型传感器环境，具备实际可行性与良好标定效果，为自动驾驶感知可靠性提供支持。

Abstract: Extrinsic Calibration represents the cornerstone of autonomous driving. Its
accuracy plays a crucial role in the perception pipeline, as any errors can
have implications for the safety of the vehicle. Modern sensor systems collect
different types of data from the environment, making it harder to align the
data. To this end, we propose a target-based extrinsic calibration system
tailored for a multi-LiDAR and multi-camera sensor suite. This system enables
cross-calibration between LiDARs and cameras with limited prior knowledge using
a custom ChArUco board and a tailored nonlinear optimization method. We test
the system with real-world data gathered in a warehouse. Results demonstrated
the effectiveness of the proposed method, highlighting the feasibility of a
unique pipeline tailored for various types of sensors.

</details>


### [150] [Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control](https://arxiv.org/abs/2507.16645)
*Zongzheng Zhang,Jiawen Yang,Ziqiao Peng,Meng Yang,Jianzhu Ma,Lin Cheng,Huazhe Xu,Hang Zhao,Hao Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种结合刚性驱动与柔性绳驱的混合动力仿生面部系统，并配套自动化、深度学习驱动的表情控制算法，实现了紧凑硬件中对多种复杂情感表情的高精度表达。


<details>
  <summary>Details</summary>
Motivation: 传统仿生面部在情感表达上受限于硬件空间与控制精度的矛盾，以及软件侧表情生成的欠缺，难以自然生动地表达情绪。

Method: 硬件上采刚绳混合：精准控制需求高的眼口由刚性结构驱动，面颊鼻部采用柔性绳牵引，实现结构紧凑且多样表情输出；软件上引入自建模网络，通过梯度反传自动建立面部运动与电机动作间多维映射，并用神经网络将语音输入转化为情感表情。

Result: 系统成功实现了在给定语句下，根据情绪（如高兴、恐惧、厌恶、愤怒）生成具有细腻差异的表情动作，显著优于现有方法。硬件和代码已开源。

Conclusion: 所提出的硬件结构与算法大幅提升了仿生面部情感表达的精细度与多样性，有力推进了自动情感表达仿生面部技术的发展。

Abstract: Previous animatronic faces struggle to express emotions effectively due to
hardware and software limitations. On the hardware side, earlier approaches
either use rigid-driven mechanisms, which provide precise control but are
difficult to design within constrained spaces, or tendon-driven mechanisms,
which are more space-efficient but challenging to control. In contrast, we
propose a hybrid actuation approach that combines the best of both worlds. The
eyes and mouth-key areas for emotional expression-are controlled using rigid
mechanisms for precise movement, while the nose and cheek, which convey subtle
facial microexpressions, are driven by strings. This design allows us to build
a compact yet versatile hardware platform capable of expressing a wide range of
emotions. On the algorithmic side, our method introduces a self-modeling
network that maps motor actions to facial landmarks, allowing us to
automatically establish the relationship between blendshape coefficients for
different facial expressions and the corresponding motor control signals
through gradient backpropagation. We then train a neural network to map speech
input to corresponding blendshape controls. With our method, we can generate
distinct emotional expressions such as happiness, fear, disgust, and anger,
from any given sentence, each with nuanced, emotion-specific control signals-a
feature that has not been demonstrated in earlier systems. We release the
hardware design and code at https://github.com/ZZongzheng0918/Morpheus-Hardware
and https://github.com/ZZongzheng0918/Morpheus-Software.

</details>


### [151] [Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory](https://arxiv.org/abs/2507.16713)
*Guowei Lan,Kaixian Qu,René Zurbrügg,Changan Chen,Christopher E. Mower,Haitham Bou-Ammar,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出了一种名为ExpTeach的新框架，使视觉-语言模型（VLMs）能够更有效地适应和控制真实世界中的机器人。通过自主生成并归纳机器人经验，显著提升了机器人在多样任务中的成功率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉-语言模型（VLMs）在机器人自主规划中被广泛应用，但其主要基于互联网数据训练，如何让这类模型有效地适应并理解真实机器人环境仍然是重大挑战。

Method: ExpTeach框架让VLM在真实环境中自主规划动作、验证结果并反思失败，通过这一闭环过程自我生成和总结经验，构建长期记忆，并在新任务中通过检索增强生成（RAG）利用已有知识。同时，系统配有按需图像注解模块提升空间理解能力。

Result: 在四个具有挑战性的机器人任务中，通过引入反思，任务成功率由36%提升到84%，并观察到机器人开始出现智能的物体交互行为如创造性工具使用。在12个真实场景（包括8个未见过的场景）的大量测试中，有长期记忆的支持下，单次实验的成功率由22%提升到80%。

Conclusion: ExpTeach框架有效地将VLM与真实机器人进行结合，通过自生成经验和长期记忆，显著增强了机器人在多样化、复杂任务中的表现与泛化能力，对未来机器人智能控制具有重要意义。

Abstract: Vision-language models (VLMs) have been widely adopted in robotics to enable
autonomous planning. However, grounding VLMs, originally trained on internet
data, to diverse real-world robots remains a challenge. This paper presents
ExpTeach, a framework that grounds VLMs to physical robots by building a
self-generated memory of real-world experiences. In ExpTeach, the VLM
autonomously plans actions, verifies outcomes, reflects on failures, and adapts
robot behaviors in a closed loop. The self-generated experiences during this
process are then summarized into a long-term memory, enabling retrieval of
learned knowledge to guide future tasks via retrieval-augmented generation
(RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with
an on-demand image annotation module. In experiments, we show that reflection
improves success rates from 36% to 84% on four challenging robotic tasks and
observe the emergence of intelligent object interactions, including creative
tool use. Across extensive tests on 12 real-world scenarios (including eight
unseen ones), we find that grounding with long-term memory boosts single-trial
success rates from 22% to 80%, demonstrating the effectiveness and
generalizability of ExpTeach.

</details>
