<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 32]
- [cs.CL](#cs.CL) [Total: 24]
- [cs.RO](#cs.RO) [Total: 18]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enabling clinical use of foundation models in histopathology](https://arxiv.org/abs/2602.22347)
*Audun L. Henriksen,Ole-Johan Skrede,Lisa van der Schee,Enric Domingo,Sepp De Raedt,Ilyá Kostolomov,Jennifer Hay,Karolina Cyll,Wanja Kildal,Joakim Kalsnes,Robert W. Williams,Manohar Pradhan,John Arne Nesheim,Hanne A. Askautrud,Maria X. Isaksen,Karmele Saez de Gordoa,Miriam Cuatrecasas,Joanne Edwards,TransSCOT group,Arild Nesbakken,Neil A. Shepherd,Ian Tomlinson,Daniel-Christoph Wagner,Rachel S. Kerr,Tarjei Sveinsgjerd Hveem,Knut Liestøl,Yoshiaki Nakamura,Marco Novelli,Masaaki Miyo,Sebastian Foersch,David N. Church,Miangela M. Lacle,David J. Kerr,Andreas Kleppe*

Main category: cs.CV

TL;DR: 本文提出了一种通过增加新颖的鲁棒性损失函数，增强数字病理学中基础模型下游任务模型的泛化能力与稳定性，无需对基础模型本身进行再训练。


<details>
  <summary>Details</summary>
Motivation: 基础模型在数字病理学中应用时不仅提取了生物学相关特征，同时也捕获了与设备、预处理等相关的干扰信息，这些技术性变异会导致模型泛化性下降、结果受偏。为了解决这些由技术变异带来的鲁棒性问题，需要提出新的方法提升模型在不同实际条件下的表现一致性。

Method: 在多个基础模型特征的基础上，针对下游分类/预测任务，在训练阶段引入了新的鲁棒训练损失函数，目的是减少模型对技术性变异的敏感性。实验采用了来自6155名患者的27042张全切片影像（WSIs），从8种主流基础模型的特征进行下游模型开发与对比实验。

Result: 加入鲁棒性损失后，模型不仅鲁棒性显著增强，而且预测准确率也提升，尤其是在关注生物学信息的情况下效果更好。

Conclusion: 本文方法可在不需重新训练基础模型的前提下，解决计算病理中的鲁棒性问题，有助于后续开发可在实际临床数据中广泛应用的数字病理AI模型。

Abstract: Foundation models in histopathology are expected to facilitate the development of high-performing and generalisable deep learning systems. However, current models capture not only biologically relevant features, but also pre-analytic and scanner-specific variation that bias the predictions of task-specific models trained from the foundation model features. Here we show that introducing novel robustness losses during training of downstream task-specific models reduces sensitivity to technical variability. A purpose-designed comprehensive experimentation setup with 27,042 WSIs from 6155 patients is used to train thousands of models from the features of eight popular foundation models for computational pathology. In addition to a substantial improvement in robustness, we observe that prediction accuracy improves by focusing on biologically relevant features. Our approach successfully mitigates robustness issues of foundation models for computational pathology without retraining the foundation models themselves, enabling development of robust computational pathology models applicable to real-world data in routine clinical practice.

</details>


### [2] [Optimizing Neural Network Architecture for Medical Image Segmentation Using Monte Carlo Tree Search](https://arxiv.org/abs/2602.22361)
*Liping Meng,Fan Nie,Yunyun Zhang,Chao Han*

Main category: cs.CV

TL;DR: 本文提出了一种结合蒙特卡洛树搜索(MCTS)与神经结构搜索(NAS)的新型医学图像分割框架MNAS-Unet，显著提升了分割精度与搜索效率，并降低了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法在提升模型精度和降低计算资源消耗之间存在权衡，传统NAS方法搜索效率不高。因此，亟需新的方法兼顾高效架构搜索和分割表现。

Method: 作者将MCTS引入NAS流程，实现更高效的架构搜索，优化了Unet下采样和上采样结构(DownSC和UpSC单元)。同时通过早停策略减少训练周期，使搜索过程更快且资源占用更低。

Result: 在PROMISE12、超声神经和CHAOS等多个医学图像分割数据集上，MNAS-Unet分割精度优于NAS-Unet及其它先进模型。同时，该方法在相同条件下将搜索预算减少54%，参数仅0.6M，且占用GPU内存更低。

Conclusion: MNAS-Unet在保证分割性能的同时大幅提升了搜索效率和资源利用率，适合实际应用中对计算资源有限的场景。

Abstract: This paper proposes a novel medical image segmentation framework, MNAS-Unet, which combines Monte Carlo Tree Search (MCTS) and Neural Architecture Search (NAS). MNAS-Unet dynamically explores promising network architectures through MCTS, significantly enhancing the efficiency and accuracy of architecture search. It also optimizes the DownSC and UpSC unit structures, enabling fast and precise model adjustments. Experimental results demonstrate that MNAS-Unet outperforms NAS-Unet and other state-of-the-art models in segmentation accuracy on several medical image datasets, including PROMISE12, Ultrasound Nerve, and CHAOS. Furthermore, compared with NAS-Unet, MNAS-Unet reduces the architecture search budget by 54% (early stopping at 139 epochs versus 300 epochs under the same search setting), while achieving a lightweight model with only 0.6M parameters and lower GPU memory consumption, which further improves its practical applicability. These results suggest that MNAS-Unet can improve search efficiency while maintaining competitive segmentation accuracy under practical resource constraints.

</details>


### [3] [AeroDGS: Physically Consistent Dynamic Gaussian Splatting for Single-Sequence Aerial 4D Reconstruction](https://arxiv.org/abs/2602.22376)
*Hanyang Liu,Rongjun Qin*

Main category: cs.CV

TL;DR: 本文提出了一种面向单目无人机(UAV)视频的4D高斯泼溅重建方法AeroDGS，有效解决了单视角航拍动态场景中的深度和运动估计难题，显著提升了动态航拍场景的重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有4D场景重建方法在动态、单镜头航拍条件下表现受限，尤其难以应对空间覆盖广、目标动态剧烈且空间尺度有限的场景。传统方法常出现严重深度歧义和运动估计不稳定。因此，亟需新的方法提升单目航拍动态场景重建的鲁棒性和准确度。

Method: 提出AeroDGS框架，利用物理先验引导的4D高斯泼溅技术。主要创新：1）设计单目几何提升模块，从一段航拍序列重建可靠的静态和动态几何信息，2）引入物理引导优化模块，整合可微分地面支持、竖直稳定、轨迹平滑等物理先验，将图像提示转化为物理一致的运动估计。整体框架实现背景与动态对象的联合优化。

Result: 在构建的真实无人机数据集和公开合成数据集上，AeroDGS在动态航拍重建精度方面均超越了最新相关方法，验证了其在大范围复杂运动场景下的有效性和优越性。

Conclusion: AeroDGS为单目无人机动态场景高保真重建提供了新思路。物理引导和几何提升模块有效缓解了深度和运动歧义，推动了动态航拍重建技术进步。

Abstract: Recent advances in 4D scene reconstruction have significantly improved dynamic modeling across various domains. However, existing approaches remain limited under aerial conditions with single-view capture, wide spatial range, and dynamic objects of limited spatial footprint and large motion disparity. These challenges cause severe depth ambiguity and unstable motion estimation, making monocular aerial reconstruction inherently ill-posed. To this end, we present AeroDGS, a physics-guided 4D Gaussian splatting framework for monocular UAV videos. AeroDGS introduces a Monocular Geometry Lifting module that reconstructs reliable static and dynamic geometry from a single aerial sequence, providing a robust basis for dynamic estimation. To further resolve monocular ambiguity, we propose a Physics-Guided Optimization module that incorporates differentiable ground-support, upright-stability, and trajectory-smoothness priors, transforming ambiguous image cues into physically consistent motion. The framework jointly refines static backgrounds and dynamic entities with stable geometry and coherent temporal evolution. We additionally build a real-world UAV dataset that spans various altitudes and motion conditions to evaluate dynamic aerial reconstruction. Experiments on synthetic and real UAV scenes demonstrate that AeroDGS outperforms state-of-the-art methods, achieving superior reconstruction fidelity in dynamic aerial environments.

</details>


### [4] [Enhancing Renal Tumor Malignancy Prediction: Deep Learning with Automatic 3D CT Organ Focused Attention](https://arxiv.org/abs/2602.22381)
*Zhengkang Fan,Chengkun Sun,Russell Terry,Jie Xu,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 本文提出了一种无需手动分割的深度学习方法，用于肾脏肿瘤恶性预测，极大提高了效率，预测性能超过传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的肾脏肿瘤影像预测恶性程度的方法在手术前难以达到高准确率，传统基于深度学习的方案又往往依赖繁琐的手动分割，耗时耗力，影响临床推广。急需一种更高效、自动化的预测方法。

Method: 设计了一种器官专注注意力（OFA）损失函数的深度学习框架，使模型自动关注器官区域，实现肿瘤恶性预测，无需手动分割3D肾脏CT图像。分别在UF IDR私有数据集和KiTS21公开数据集上进行了测试。

Result: 在UF IDR数据集上，AUC为0.685，F1为0.872；在KiTS21数据集上，AUC为0.760，F1为0.852。性能均优于需要分割的传统模型。

Conclusion: 该方法实现了不依赖明确分割的肾脏肿瘤恶性度预测，高效且准确，有望为肾癌诊疗决策提供更优帮助。

Abstract: Accurate prediction of malignancy in renal tumors is crucial for informing clinical decisions and optimizing treatment strategies. However, existing imaging modalities lack the necessary accuracy to reliably predict malignancy before surgical intervention. While deep learning has shown promise in malignancy prediction using 3D CT images, traditional approaches often rely on manual segmentation to isolate the tumor region and reduce noise, which enhances predictive performance. Manual segmentation, however, is labor-intensive, costly, and dependent on expert knowledge. In this study, a deep learning framework was developed utilizing an Organ Focused Attention (OFA) loss function to modify the attention of image patches so that organ patches attend only to other organ patches. Hence, no segmentation of 3D renal CT images is required at deployment time for malignancy prediction. The proposed framework achieved an AUC of 0.685 and an F1-score of 0.872 on a private dataset from the UF Integrated Data Repository (IDR), and an AUC of 0.760 and an F1-score of 0.852 on the publicly available KiTS21 dataset. These results surpass the performance of conventional models that rely on segmentation-based cropping for noise reduction, demonstrating the frameworks ability to enhance predictive accuracy without explicit segmentation input. The findings suggest that this approach offers a more efficient and reliable method for malignancy prediction, thereby enhancing clinical decision-making in renal cancer diagnosis.

</details>


### [5] [Vision Transformers Need More Than Registers](https://arxiv.org/abs/2602.22394)
*Cheng Shi,Yizhou Yu,Sibei Yang*

Main category: cs.CV

TL;DR: 本文通过分析ViT存在的表征伪影问题，发现其根源在于惰性聚合行为，并提出了一种优化CLS token聚合方式的方法，有效缓解背景主导的问题并在多个基准上提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管ViT在大规模数据上预训练后表现出良好的通用性，但在不同监督和下游任务中都出现了伪影，其成因没有被充分解释，影响了模型性能和理解。

Method: 作者通过系统分析，发现ViT的背景patch由于全球注意力机制和粗粒度语义监督，作为捷径被用于代表全局语义，出现惰性聚合。为此，提出了一种有选择地将patch特征融入CLS token，降低背景patch对全局语义建模的影响的方法。

Result: 该方法在标签监督、文本监督及自监督3种范式下，于12个基准测试中均实现了性能提升，证明其实用性和有效性。

Conclusion: 文章提出了惰性聚合行为作为ViT伪影的本质原因，并给出了可行的缓解方案，为理解和优化ViT模型提供了新的视角和思路。

Abstract: Vision Transformers (ViTs), when pre-trained on large-scale data, provide general-purpose representations for diverse downstream tasks. However, artifacts in ViTs are widely observed across different supervision paradigms and downstream tasks. Through systematic analysis of artifacts in ViTs, we find that their fundamental mechanisms have yet to be sufficiently elucidated. In this paper, through systematic analysis, we conclude that these artifacts originate from a lazy aggregation behavior: ViT uses semantically irrelevant background patches as shortcuts to represent global semantics, driven by global attention and Coarse-grained semantic supervision. Our solution selectively integrates patch features into the CLS token, reducing the influence of background-dominated shortcuts and consistently improving performance across 12 benchmarks under label-, text-, and self-supervision. We hope this work offers a new perspective on ViT behavior.

</details>


### [6] [CLIP Is Shortsighted: Paying Attention Beyond the First Sentence](https://arxiv.org/abs/2602.22419)
*Marc-Antoine Lavoie,Anas Mahmoud,Aldo Zaimi,Arsene Fansi Tchango,Steven L. Waslander*

Main category: cs.CV

TL;DR: 论文提出了DeBias-CLIP方法，通过去除长文本描述中的首句摘要，并引入句子子采样和token填充，使模型学习关注全部描述内容，提升多模态任务性能。


<details>
  <summary>Details</summary>
Motivation: CLIP模型训练时主要使用配对简短描述的图片，导致模型对复杂场景和密集描述对齐能力不足。现有微调方法又容易受长描述首句摘要引导注意力，削弱其对后续内容的建模能力。

Method: 提出DeBias-CLIP方法：在训练时去除描述文本的首句摘要，采用句子子采样和文本token填充方法，促使模型学习均衡关注各个token位置，从而增强对长文本描述的对齐效果。

Result: DeBias-CLIP在长文本检索任务上实现了当前最优性能（SOTA），并在短文本检索上也有提升，且对于句子顺序变化更加鲁棒。

Conclusion: DeBias-CLIP无需增加参数即可替代Long-CLIP，显著提升了多模态检索精度与对文本分布的适应能力。

Abstract: CLIP models learn transferable multi-modal features via image-text contrastive learning on internet-scale data. They are widely used in zero-shot classification, multi-modal retrieval, text-to-image diffusion, and as image encoders in large vision-language models. However, CLIP's pretraining is dominated by images paired with short captions, biasing the model toward encoding simple descriptions of salient objects and leading to coarse alignment on complex scenes and dense descriptions. While recent work mitigates this by fine-tuning on small-scale long-caption datasets, we identify an important common bias: both human- and LLM-generated long captions typically begin with a one-sentence summary followed by a detailed description. We show that this acts as a shortcut during training, concentrating attention on the opening sentence and early tokens and weakening alignment over the rest of the caption. To resolve this, we introduce DeBias-CLIP, which removes the summary sentence during training and applies sentence sub-sampling and text token padding to distribute supervision across all token positions. DeBias-CLIP achieves state-of-the-art long-text retrieval, improves short-text retrieval, and is less sensitive to sentence order permutations. It is a drop-in replacement for Long-CLIP with no additional trainable parameters.

</details>


### [7] [SimpleOCR: Rendering Visualized Questions to Teach MLLMs to Read](https://arxiv.org/abs/2602.22426)
*Yibo Peng,Peng Xia,Ding Zhong,Kaide Zeng,Siwei Han,Yiyang Zhou,Jiaqi Liu,Ruiyi Zhang,Huaxiu Yao*

Main category: cs.CV

TL;DR: 本文发现多模态大模型（MLLMs）在需要识别图像中嵌入文本时仍存在明显性能下降，并提出SimpleOCR训练策略，有效弥补这一缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs具有较强的OCR能力，但其是否真正利用视觉信息读取图片中文字，而非仅依赖prompt中的文本信息，尚不明确。现有模型可能存在“模态惰性”，即不充分利用视觉模态。

Method: 提出Visualized-Question (VQ)设定，将文本问题直接渲染在图片上，结构性地要求模型实行视觉文本读取。并设计SimpleOCR训练方法，通过将训练样本随机风格化地转换成VQ格式，消除文本捷径，迫使模型激活视觉识别能力。

Result: 诊断发现主流模型在VQ设定下性能下降达12.7%；采用SimpleOCR后，无需模型结构改动，在四个OOD基准集上，相比原模型提升5.4%、相比RL方法提升2.7%，仅用8.5K样本即可达到甚至超过RL方法（后者样本量为SimpleOCR的30倍）。

Conclusion: 模型存在‘模态惰性’，视觉信息未被充分利用。SimpleOCR作为一种即插即用的训练策略，廉价高效地提升了多模态模型在实际视觉文本理解任务的表现，兼容其他增强方法，推广前景广阔。

Abstract: Despite the rapid advancements in Multimodal Large Language Models (MLLMs), a critical question regarding their visual grounding mechanism remains unanswered: do these models genuinely ``read'' text embedded in images, or do they merely rely on parametric shortcuts in the text prompt? In this work, we diagnose this issue by introducing the Visualized-Question (VQ) setting, where text queries are rendered directly onto images to structurally mandate visual engagement. Our diagnostic experiments on Qwen2.5-VL reveal a startling capability-utilization gap: despite possessing strong OCR capabilities, models suffer a performance degradation of up to 12.7% in the VQ setting, exposing a deep-seated ``modality laziness.'' To bridge this gap, we propose SimpleOCR, a plug-and-play training strategy that imposes a structural constraint on the learning process. By transforming training samples into the VQ format with randomized styles, SimpleOCR effectively invalidates text-based shortcuts, compelling the model to activate and optimize its visual text extraction pathways. Empirically, SimpleOCR yields robust gains without architectural modifications. On four representative OOD benchmarks, it surpasses the base model by 5.4% and GRPO based on original images by 2.7%, while exhibiting extreme data efficiency, achieving superior performance with 30x fewer samples (8.5K) than recent RL-based methods. Furthermore, its plug-and-play nature allows seamless integration with advanced RL strategies like NoisyRollout to yield complementary improvements. Code is available at https://github.com/aiming-lab/SimpleOCR.

</details>


### [8] [Exploring Multimodal LMMs for Online Episodic Memory Question Answering on the Edge](https://arxiv.org/abs/2602.22455)
*Giuseppe Lando,Rosario Forte,Antonino Furnari*

Main category: cs.CV

TL;DR: 本论文探讨了在边缘设备上使用多模态大语言模型（MLLMs）实现实时在线情节记忆问答的可行性，并取得了与云端方案相近的表现，有望提高隐私保护。


<details>
  <summary>Details</summary>
Motivation: 云端处理虽然常见，但在可穿戴助手等场景下存在隐私与延迟问题。因此，作者希望研究如何在本地设备上实现高效的情节记忆问答。

Method: 作者将问答流程分为两个异步线程：描述线程将视频流实时转为轻量级文本记忆，问答线程基于这一文本记忆实时回答用户问题，并在严格的资源限制下用MLLMs进行实验。

Result: 在QAEgo4D-Closed基准测试上，本地8GB消费级GPU上的端到端配置达到51.76%准确率，首次响应时间0.41s；企业级服务器可达54.40%准确率，响应时间0.88s；云端方案为56.00%准确率。

Conclusion: 边缘端部署的多模态大模型在保证隐私的同时，能以接近云端性能进行情节记忆检索，展现了边缘方案的应用潜力。

Abstract: We investigate the feasibility of using Multimodal Large Language Models (MLLMs) for real-time online episodic memory question answering. While cloud offloading is common, it raises privacy and latency concerns for wearable assistants, hence we investigate implementation on the edge. We integrated streaming constraints into our question answering pipeline, which is structured into two asynchronous threads: a Descriptor Thread that continuously converts video into a lightweight textual memory, and a Question Answering (QA) Thread that reasons over the textual memory to answer queries. Experiments on the QAEgo4D-Closed benchmark analyze the performance of Multimodal Large Language Models (MLLMs) within strict resource boundaries, showing promising results also when compared to clound-based solutions. Specifically, an end-to-end configuration running on a consumer-grade 8GB GPU achieves 51.76% accuracy with a Time-To-First-Token (TTFT) of 0.41s. Scaling to a local enterprise-grade server yields 54.40% accuracy with a TTFT of 0.88s. In comparison, a cloud-based solution obtains an accuracy of 56.00%. These competitive results highlight the potential of edge-based solutions for privacy-preserving episodic memory retrieval.

</details>


### [9] [MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation](https://arxiv.org/abs/2602.22462)
*Raiyan Jahangir,Nafiz Imtiaz Khan,Amritanand Sudheerkumar,Vladimir Filkov*

Main category: cs.CV

TL;DR: 本文提出了MammoWise，一个基于开源视觉语言模型（VLM）的本地化多模型流水线，用于乳腺X线筛查报告自动生成和多任务分类，支持多种提示策略和数据集，具有高准确率且保障隐私。


<details>
  <summary>Details</summary>
Motivation: 目前乳腺X线筛查对于放射科医生而言，属于高强度、时间紧迫且文档量大的工作。主流VLM的应用受制于云端系统、隐私问题和适应性差。因此，迫切需要本地可扩展、适应性强的自动化工具以提升临床工作效率并保障数据安全。

Method: 作者提出了MammoWise框架，将开源VLM（如MedGemma, LLaVA-Med, Qwen2.5-VL）本地化部署，支持Ollama平台、不同数据集、无/小样本/Chain-of-Thought提示，以及可选的多模态RAG增强，通过参数高效微调（QLoRA）提升模型性能。

Result: 在VinDr-Mammo和DMID两个公开数据集上进行了实验，报告生成质量高且通过少样本和RAG方式进一步提升；分类任务准确率依赖模型与数据集，微调后的MedGemma模型BI-RADS、密度、钙化准确率分别达到0.7545、0.8840、0.9341，同时保持报告质量。

Conclusion: MammoWise是一个实用且可扩展的本地化开源乳腺X线智能报告与分类框架，统一了工作流，有助于隐私保护、结果复现与部署适应性，为临床乳腺影像解读自动化提供了有力工具。

Abstract: Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.

</details>


### [10] [Beyond Dominant Patches: Spatial Credit Redistribution For Grounded Vision-Language Models](https://arxiv.org/abs/2602.22469)
*Niamul Hassan Samin,Md Arifur Rahman,Abdullah Ibne Hanif,Juena Ahmed Noshin,Md Ashikur Rahman*

Main category: cs.CV

TL;DR: 本文提出了一种新方法SCR，有效减少了视觉-语言模型生成不准确内容（幻觉对象）的现象，在多个主流模型和基准上验证了其实用性与高效性。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）在生成描述时常常‘凭空’描述输入图片中不存在的对象（即幻觉），影响了模型的可靠性和实际应用。作者追踪到这一问题源自transformer早期层的‘空间信用塌缩’，即激活主要聚集于少数视觉块，忽略了足够的上下文信息，导致模型过度依赖预训练语言先验。

Method: 提出一种训练时无依赖、推理阶段可用的空间信用重分配（SCR）方法。SCR在模型推理时，借助低熵输入的指导，将高注意力视觉块的激活重新分配给其上下文区域，从而增强上下文证据并减轻幻觉问题。

Result: 在POPE和CHAIR等基准上，SCR能将幻觉现象减少4.7-6.0个百分点（POPE-Adversarial）、CHAIR-s减少3.7-5.2个百分点（相对下降42-51%）、CHAIR-i减少2.7-4.4个百分点（相对下降44-58%），同时CIDEr分数几乎无损（变化在0.8个百分点内）。实现开销低，仅增加43-56毫秒，远低于现有方法。

Conclusion: SCR能有效抑制VLM幻觉，不损性能，且实用高效，适合实时场景。实证与消融实验表明，利用注意力引导源块选择至关重要，验证了空间信用塌缩是幻觉问题的核心成因。

Abstract: Vision-language models (VLMs) frequently hallucinate objects absent from the input image. We trace this failure to spatial credit collapse: activation credit concentrating on sparse visual patches in early transformer layers, which suppresses contextual evidence and increases reliance on language priors. We introduce Spatial Credit Redistribution (SCR), a training-free inference-time intervention that redistributes hidden-state activation from high-attention source patches to their context, guided by low-entropy inputs. We evaluate six model families (Chameleon, LLaVA, and Qwen, including both Qwen-VL and Qwen2-VL) at scales of 7B, 13B, and 30B, on POPE and CHAIR benchmarks. SCR reduces hallucination by ~4.7-6.0 percentage points on POPE-Adversarial, cuts CHAIR-s by 3.7-5.2 percentage points (42-51 percent relative), and CHAIR-i by 2.7-4.4 percentage points (44-58 percent relative), and preserves CIDEr within 0.8 percentage points. Gains are largest for low-entropy inputs, consistent with the theoretical framework. SCR incurs only 43-56 ms overhead (small models: +43-46 ms; large models: +54-56 ms), roughly 3-6 times lower than OPERA and VCD and 1.3-1.7 times lower than OVCD (+72 ms), while Pareto-dominating all three on both hallucination rate and CIDEr, making it practical for real-time settings. A controlled ablation confirms that attention-guided source selection is essential: replacing it with uniform random selection reduces hallucination rate gains from ~4.7-6.0 percentage points to only ~2.6-3.4 percentage points, pointing to credit-collapse as the key driver.

</details>


### [11] [Pix2Key: Controllable Open-Vocabulary Retrieval with Semantic Decomposition and Self-Supervised Visual Dictionary Learning](https://arxiv.org/abs/2602.22510)
*Guoyizhe Wei,Yang Jiao,Nan Xi,Zhishen Huang,Jingjing Meng,Rama Chellappa,Yan Gao*

Main category: cs.CV

TL;DR: 本文提出了一种新的组合图像检索方法Pix2Key，将查询和候选图片都表示为视觉词典，实现意图感知和多样性排序，在无监督预训练模块V-Dict-AE加持下，无需专门的CIR标注即可增强效果。实验在DFMM-Compose数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有组合图像检索方法在融合查询图像和语言编辑时，往往损失细粒度信息或者无法理解用户隐含意图，导致结果单一且不精确。

Method: 提出Pix2Key，把图像及编辑请求都转为开放词汇的视觉词典，并在统一嵌入空间做意图约束和多样性排序。引入自监督预训练的V-Dict-AE模块，仅用图片训练，无需CIR特定标注，提升细粒度属性表征能力。

Result: 在DFMM-Compose基准测试中，Pix2Key在Recall@10上提升3.2个百分点。加入V-Dict-AE后，进一步提升2.3个百分点，并在保持列表多样性的同时提高了意图一致性。

Conclusion: Pix2Key实现了对用户隐含搜索意图的更好匹配，在细粒度检索和结果多样性方面均有改进，自监督模块有助于进一步提升，无需专门标注，具备很强的实用价值。

Abstract: Composed Image Retrieval (CIR) uses a reference image plus a natural-language edit to retrieve images that apply the requested change while preserving other relevant visual content. Classic fusion pipelines typically rely on supervised triplets and can lose fine-grained cues, while recent zero-shot approaches often caption the reference image and merge the caption with the edit, which may miss implicit user intent and return repetitive results. We present Pix2Key, which represents both queries and candidates as open-vocabulary visual dictionaries, enabling intent-aware constraint matching and diversity-aware reranking in a unified embedding space. A self-supervised pretraining component, V-Dict-AE, further improves the dictionary representation using only images, strengthening fine-grained attribute understanding without CIR-specific supervision. On the DFMM-Compose benchmark, Pix2Key improves Recall@10 up to 3.2 points, and adding V-Dict-AE yields an additional 2.3-point gain while improving intent consistency and maintaining high list diversity.

</details>


### [12] [DisQ-HNet: A Disentangled Quantized Half-UNet for Interpretable Multimodal Image Synthesis Applications to Tau-PET Synthesis from T1 and FLAIR MRI](https://arxiv.org/abs/2602.22545)
*Agamdeep S. Chopra,Caitlin Neher,Tianyi Ren,Juampablo E. Heras Rivera,Mehmet Kurt*

Main category: cs.CV

TL;DR: 本文提出了一种名为DisQ-HNet (DQH) 的新方法，可利用MRI数据合成tau-PET影像，达到类似PET的效果，有助于阿尔茨海默病的检测和分析。


<details>
  <summary>Details</summary>
Motivation: tau-PET能显示阿尔茨海默病关键病理，但成本高且难以普及，因此需要基于MRI的替代方法，提升疾病筛查效率和可及性。

Method: 提出DisQ-HNet（DQH）框架，使用T1与FLAIR MRI配对数据，结合PID引导的向量量化编码器，将特征信息分为冗余、唯一、互补三类，并采用半UNet解码器，通过结构边缘线索引导伪跳跃连接，增强解剖细节保留。

Result: 与VAE、VQ-VAE、UNet等多种基线比较，DQH在重建质量上持平但更好地保留了与阿尔茨海默病相关的信号，提升了后续任务表现，如Braak分期、tau蛋白定位与分类。同时，PID-Shapley分析揭示了不同MRI模态对预测结果的具体贡献。

Conclusion: DisQ-HNet为基于MRI的tau-PET合成提供了新方法，在保持图像质量的同时，更好地保留疾病相关特征，有望推动无创阿尔茨海默病检测的临床应用，提高普及率。

Abstract: Tau positron emission tomography (tau-PET) provides an in vivo marker of Alzheimer's disease pathology, but cost and limited availability motivate MRI-based alternatives. We introduce DisQ-HNet (DQH), a framework that synthesizes tau-PET from paired T1-weighted and FLAIR MRI while exposing how each modality contributes to the prediction. The method combines (i) a Partial Information Decomposition (PID)-guided, vector-quantized encoder that partitions latent information into redundant, unique, and complementary components, and (ii) a Half-UNet decoder that preserves anatomical detail using pseudo-skip connections conditioned on structural edge cues rather than direct encoder feature reuse. Across multiple baselines (VAE, VQ-VAE, and UNet), DisQ-HNet maintains reconstruction fidelity and better preserves disease-relevant signal for downstream AD tasks, including Braak staging, tau localization, and classification. PID-based Shapley analysis provides modality-specific attribution of synthesized uptake patterns.

</details>


### [13] [DrivePTS: A Progressive Learning Framework with Textual and Structural Enhancement for Driving Scene Generation](https://arxiv.org/abs/2602.22549)
*Zhechao Wang,Yiming Zeng,Lufan Ma,Zeqing Fu,Chen Bai,Ziyao Lin,Cheng Lu*

Main category: cs.CV

TL;DR: DrivePTS通过多层创新提升自动驾驶场景合成质量，实现了更高的真实性和可控性，尤其适合生成罕见场景。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶场景合成方法在变化条件下生成容易失败，且生成场景的语义、结构细节不足，背景建模和前景结构描述都较弱。本研究旨在解决上述问题，提升自动驾驶系统训练和验证的数据质量。

Method: DrivePTS包含三大创新：（1）提出渐进式学习和互信息约束，减少控制条件间的依赖性；（2）利用视觉-语言模型，生成多视角、多层面细致语义描述，丰富场景语义指导；（3）引入频率引导结构损失，增强模型对结构细节（高频信息）的敏感性，提升前景保真度。

Result: 大量实验验证DrivePTS在生成多样化驾驶场景时，在真实性、可控性方面优于现有方法，并能在前人失败的罕见场景下正常生成，表现出更强的泛化能力。

Conclusion: DrivePTS有效提升自动驾驶场景生成的质量与控制性，解决了现有方法的多项局限，对自动驾驶安全和数据增广有重要意义。

Abstract: Synthesis of diverse driving scenes serves as a crucial data augmentation technique for validating the robustness and generalizability of autonomous driving systems. Current methods aggregate high-definition (HD) maps and 3D bounding boxes as geometric conditions in diffusion models for conditional scene generation. However, implicit inter-condition dependency causes generation failures when control conditions change independently. Additionally, these methods suffer from insufficient details in both semantic and structural aspects. Specifically, brief and view-invariant captions restrict semantic contexts, resulting in weak background modeling. Meanwhile, the standard denoising loss with uniform spatial weighting neglects foreground structural details, causing visual distortions and blurriness. To address these challenges, we propose DrivePTS, which incorporates three key innovations. Firstly, our framework adopts a progressive learning strategy to mitigate inter-dependency between geometric conditions, reinforced by an explicit mutual information constraint. Secondly, a Vision-Language Model is utilized to generate multi-view hierarchical descriptions across six semantic aspects, providing fine-grained textual guidance. Thirdly, a frequency-guided structure loss is introduced to strengthen the model's sensitivity to high-frequency elements, improving foreground structural fidelity. Extensive experiments demonstrate that our DrivePTS achieves state-of-the-art fidelity and controllability in generating diverse driving scenes. Notably, DrivePTS successfully generates rare scenes where prior methods fail, highlighting its strong generalization ability.

</details>


### [14] [SwiftNDC: Fast Neural Depth Correction for High-Fidelity 3D Reconstruction](https://arxiv.org/abs/2602.22565)
*Kang Han,Wei Xiang,Lu Yu,Mathew Wyatt,Gaowen Liu,Ramana Rao Kompella*

Main category: cs.CV

TL;DR: SwiftNDC是一种基于神经深度校正的新型高效3D重建方法，提升了深度的一致性和精度，显著加速并优化了3D网格重建和新视角合成。


<details>
  <summary>Details</summary>
Motivation: 现有深度引导型3D重建方法虽然运行速度快，但易出现尺度漂移、多视角不一致，并且仍需大量后处理才能获得高质量几何体。本研究旨在解决深度一致性和几何初始化不足的问题。

Method: 提出SwiftNDC框架，使用神经深度校正法生成跨视角一致的深度图，再通过回投影和鲁棒重投影误差滤波生成高质量密集点云。这为后续的3D Gaussian Splatting（3DGS）网格重建提供干净、一致的几何初始化，减少了优化迭代次数。

Result: SwiftNDC在五个数据集上进行了系统实验，涵盖网格重建和新视角合成。结果表明，该方法在加速精确网格重建的同时，显著提升了新视角合成的渲染质量。

Conclusion: 将神经深度精炼与鲁棒几何初始化相结合，SwiftNDC为高保真、高效率的3D重建提供了有效解决方案，兼顾了重建速度与质量。

Abstract: Depth-guided 3D reconstruction has gained popularity as a fast alternative to optimization-heavy approaches, yet existing methods still suffer from scale drift, multi-view inconsistencies, and the need for substantial refinement to achieve high-fidelity geometry. Here, we propose SwiftNDC, a fast and general framework built around a Neural Depth Correction field that produces cross-view consistent depth maps. From these refined depths, we generate a dense point cloud through back-projection and robust reprojection-error filtering, obtaining a clean and uniformly distributed geometric initialization for downstream reconstruction. This reliable dense geometry substantially accelerates 3D Gaussian Splatting (3DGS) for mesh reconstruction, enabling high-quality surfaces with significantly fewer optimization iterations. For novel-view synthesis, SwiftNDC can also improve 3DGS rendering quality, highlighting the benefits of strong geometric initialization. We conduct a comprehensive study across five datasets, including two for mesh reconstruction, as well as three for novel-view synthesis. SwiftNDC consistently reduces running time for accurate mesh reconstruction and boosts rendering fidelity for view synthesis, demonstrating the effectiveness of combining neural depth refinement with robust geometric initialization for high-fidelity and efficient 3D reconstruction.

</details>


### [15] [Quality-Aware Robust Multi-View Clustering for Heterogeneous Observation Noise](https://arxiv.org/abs/2602.22568)
*Peihan Wu,Guanjie Cheng,Yufei Tong,Meng Xi,Shuiguang Deng*

Main category: cs.CV

TL;DR: 本文提出了一种新的多视图聚类框架QARMVC，通过自适应识别并抵抗异质噪声，实现更健壮的多视图聚类，在五个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多视图聚类抗噪方法大多采用简单的二元假设，只将数据划分为干净或完全损坏，忽视了实际中普遍存在的异质噪声，其污染程度是连续变化的。因此需要能识别并有针对性处理不同强度噪声的方法。

Method: QARMVC利用信息瓶颈机制提取视图内在语义并用于重构。观测到噪声破坏语义完整性，会影响重构效果，作者以此为依据，定量评估每个样本的污染强度，分配实例级质量分数。在特征层采用质量加权对比损失抑制噪声传播，在融合层用高质量样本聚合共识，并通过最大化互信息对齐和修正各本地视图。

Result: 在五个基准多视图聚类数据集上，QARMVC在各种噪声强度环境下均优于主流抗噪聚类方法，表现出更强的稳健性和聚类性能。

Conclusion: 针对多视图数据中异质噪声的实际挑战，QARMVC通过质量感知分数和层次化处理流程实现了更为精细和自适应的稳健聚类，为真实复杂噪声环境下的多视图聚类提供了有效方案。

Abstract: Deep multi-view clustering has achieved remarkable progress but remains vulnerable to complex noise in real-world applications. Existing noisy robust methods predominantly rely on a simplified binary assumption, treating data as either perfectly clean or completely corrupted. This overlooks the prevalent existence of heterogeneous observation noise, where contamination intensity varies continuously across data. To bridge this gap, we propose a novel framework termed Quality-Aware Robust Multi-View Clustering (QARMVC). Specifically, QARMVC employs an information bottleneck mechanism to extract intrinsic semantics for view reconstruction. Leveraging the insight that noise disrupts semantic integrity and impedes reconstruction, we utilize the resulting reconstruction discrepancy to precisely quantify fine-grained contamination intensity and derive instance-level quality scores. These scores are integrated into a hierarchical learning strategy: at the feature level, a quality-weighted contrastive objective is designed to adaptively suppress the propagation of noise; at the fusion level, a high-quality global consensus is constructed via quality-weighted aggregation, which is subsequently utilized to align and rectify local views via mutual information maximization. Extensive experiments on five benchmark datasets demonstrate that QARMVC consistently outperforms state-of-the-art baselines, particularly in scenarios with heterogeneous noise intensities.

</details>


### [16] [Guidance Matters: Rethinking the Evaluation Pitfall for Text-to-Image Generation](https://arxiv.org/abs/2602.22570)
*Dian Xie,Shitong Shao,Lichen Bai,Zikai Zhou,Bojun Cheng,Shuo Yang,Jun Wu,Zeke Xie*

Main category: cs.CV

TL;DR: 本论文针对扩散模型中的指导方法提出了一个新的评估框架，并揭示了目前主流人类偏好评估方法的严重偏见问题，呼吁社区重新思考扩散指导方法的评估标准。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型中，分类器自由指导（CFG）和新兴的扩散指导方法在提升文本-图像生成质量方面起到重要作用，但它们的评价方式可能存在缺陷，导致对方法有效性的认知产生偏差。

Method: 1. 揭示评价陷阱：指出现有人类偏好模型对高CFG scale存在较大偏见，简单提高CFG scale虽然提升了语义一致性分数，却严重破坏图像质量。2. 提出GA-Eval评估框架：通过有效的scale校准，公平地比较各种指导方法，并识别出正交和共线于CFG效果的指导效应。3. 证明陷阱存在：设计了TDG方法，在传统评价下表现优秀，实则无效。4. 系统评测8种主流方法，在两种框架下对比。

Result: 实验证明，仅通过简单增加CFG scale即可媲美甚至超过目前大多数扩散指导方法，同时所有方法在GA-Eval框架下均出现明显的性能下降。

Conclusion: 当前评估范式存在重大偏差，呼吁社区重新制定更公正合理的扩散模型指导方法评价标准。

Abstract: Classifier-free guidance (CFG) has helped diffusion models achieve great conditional generation in various fields. Recently, more diffusion guidance methods have emerged with improved generation quality and human preference. However, can these emerging diffusion guidance methods really achieve solid and significant improvements? In this paper, we rethink recent progress on diffusion guidance. Our work mainly consists of four contributions. First, we reveal a critical evaluation pitfall that common human preference models exhibit a strong bias towards large guidance scales. Simply increasing the CFG scale can easily improve quantitative evaluation scores due to strong semantic alignment, even if image quality is severely damaged (e.g., oversaturation and artifacts). Second, we introduce a novel guidance-aware evaluation (GA-Eval) framework that employs effective guidance scale calibration to enable fair comparison between current guidance methods and CFG by identifying the effects orthogonal and parallel to CFG effects. Third, motivated by the evaluation pitfall, we design Transcendent Diffusion Guidance (TDG) method that can significantly improve human preference scores in the conventional evaluation framework but actually does not work in practice. Fourth, in extensive experiments, we empirically evaluate recent eight diffusion guidance methods within the conventional evaluation framework and the proposed GA-Eval framework. Notably, simply increasing the CFG scales can compete with most studied diffusion guidance methods, while all methods suffer severely from winning rate degradation over standard CFG. Our work would strongly motivate the community to rethink the evaluation paradigm and future directions of this field.

</details>


### [17] [Causal Motion Diffusion Models for Autoregressive Motion Generation](https://arxiv.org/abs/2602.22594)
*Qing Yu,Akihisa Watanabe,Kent Fujiwara*

Main category: cs.CV

TL;DR: 本文提出了因果运动扩散模型（CMDM），结合了运动-语言对齐的因果VAE与自回归扩散Transformer，实现了高质量、低延迟的文本到动作生成。


<details>
  <summary>Details</summary>
Motivation: 当前运动扩散模型要么依赖全序列的双向生成，限制了时序因果性和实时性，要么用自回归结构导致不稳定和误差累积。为解决这两个主要缺点，需要提出一种同时具备因果性、稳定性且适用于流式、实时生成的模型。

Method: 方法上，CMDM包括两个核心部分：1）Motion-Language-Aligned Causal VAE（MAC-VAE），将动作编码为时序因果潜在表示；2）在该潜在空间下，用自回归扩散Transformer配合因果去噪训练，实现逐帧有序生成。为加速推理，引入逐帧采样调度机制，仅依赖部分去噪的前帧预测后续帧。

Result: 在HumanML3D及SnapMoGen数据集上的实验显示，CMDM在语义一致性和时序平滑性上均优于现有扩散和自回归模型，并显著减少了推理延迟。

Conclusion: CMDM能够支持高质量、低延迟的文本驱动动作生成、流式合成及长序列动作生成，并在多项指标上优于主流方法，具有实际应用前景。

Abstract: Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.

</details>


### [18] [Instruction-based Image Editing with Planning, Reasoning, and Generation](https://arxiv.org/abs/2602.22624)
*Liya Ji,Chenyang Qi,Qifeng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态模型的指令图像编辑方法，通过链式推理提升理解与生成能力，显著提高复杂场景下的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 传统的指令图像编辑依赖于单一模态（如大语言模型、分割模型和编辑模型），导致对场景理解与编辑质量有限，难以应对复杂任务。因此，作者希望构建一种新的多模态模型，更好地结合理解与生成能力，提升编辑质量。

Method: 作者提出将指令编辑任务分离为多模态链式思维（Chain-of-Thought, CoT）步骤，包括计划、编辑区域推理、实际编辑。1）CoT计划：大语言模型根据指令和编辑网络能力生成合理子提示；2）区域推理：训练了基于指令的多模态大型语言模型生成编辑区域；3）编辑：提出了基于提示的指令编辑网络，利用大型文本到图像扩散模型生成结果。

Result: 大量实验表明，该方法在复杂真实图像的编辑任务上具有较强的编辑能力，编辑效果优于现有方法。

Conclusion: 多模态链式推理能够有效提升指令图像编辑的理解与生成能力，为复杂场景下的图像编辑提供了有力的新方法。

Abstract: Editing images via instruction provides a natural way to generate interactive content, but it is a big challenge due to the higher requirement of scene understanding and generation. Prior work utilizes a chain of large language models, object segmentation models, and editing models for this task. However, the understanding models provide only a single modality ability, restricting the editing quality. We aim to bridge understanding and generation via a new multi-modality model that provides the intelligent abilities to instruction-based image editing models for more complex cases. To achieve this goal, we individually separate the instruction editing task with the multi-modality chain of thought prompts, i.e., Chain-of-Thought (CoT) planning, editing region reasoning, and editing. For Chain-of-Thought planning, the large language model could reason the appropriate sub-prompts considering the instruction provided and the ability of the editing network. For editing region reasoning, we train an instruction-based editing region generation network with a multi-modal large language model. Finally, a hint-guided instruction-based editing network is proposed for editing image generations based on the sizeable text-to-image diffusion model to accept the hints for generation. Extensive experiments demonstrate that our method has competitive editing abilities on complex real-world images.

</details>


### [19] [QuadSync: Quadrifocal Tensor Synchronization via Tucker Decomposition](https://arxiv.org/abs/2602.22639)
*Daniel Miao,Gilad Lerman,Joe Kileel*

Main category: cs.CV

TL;DR: 本文提出了一种全新的基于四视点张量的相机同步框架，通过Tucker分解实现$n$个相机的同步，实验表明效果优秀，推动了高阶张量在结构光流领域的实际应用。


<details>
  <summary>Details</summary>
Motivation: 传统的结构光流中多采用基本矩阵（双视点），而四视点张量因复杂性常被视为理论意义大于实际，但其理论上包含更多信息，有望提升多相机几何重建的能力。本文致力于将四视点张量的理论优势转化为实际的同步与几何重建方法。

Method: 提出将$n$个四视点张量整理为block张量，并证明其Tucker分解的因子正好是相机阵列，从而构建了新的同步算法。该算法融合Tucker分解、交替方向乘子法（ADMM）及迭代重加权最小二乘法。同时，建立了四视点、三视点与双视点张量之间的同步联系，实现了三种张量的联合同步算法。

Result: 所提方法在现代数据集上获得了优异的实验结果，表明该方法能够有效同步和恢复多相机的外参信息，超过了低阶（双、三视点）方法。

Conclusion: 高阶（四视点）张量不仅具理论意义，借助所提方法能够在实际结构光流任务中有效发挥作用，提升多相机系统重建的精度与鲁棒性，为后续相关应用和理论拓展奠定基础。

Abstract: In structure from motion, quadrifocal tensors capture more information than their pairwise counterparts (essential matrices), yet they have often been thought of as impractical and only of theoretical interest. In this work, we challenge such beliefs by providing a new framework to recover $n$ cameras from the corresponding collection of quadrifocal tensors. We form the block quadrifocal tensor and show that it admits a Tucker decomposition whose factor matrices are the stacked camera matrices, and which thus has a multilinear rank of (4,~4,~4,~4) independent of $n$. We develop the first synchronization algorithm for quadrifocal tensors, using Tucker decomposition, alternating direction method of multipliers, and iteratively reweighted least squares. We further establish relationships between the block quadrifocal, trifocal, and bifocal tensors, and introduce an algorithm that jointly synchronizes these three entities. Numerical experiments demonstrate the effectiveness of our methods on modern datasets, indicating the potential and importance of using higher-order information in synchronization.

</details>


### [20] [Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache](https://arxiv.org/abs/2602.22654)
*Bowen Cui,Yuanbin Wang,Huajiang Xu,Biaolong Chen,Aixi Zhang,Hao Jiang,Zhengzheng Jin,Xu Liu,Pipei Huang*

Main category: cs.CV

TL;DR: DPCache是一种高效加速扩散模型推理的新方法，通过全局最优跳步计划显著加速采样过程，且不会造成质量明显下降。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像与视频生成领域表现优异，但多步推理极大消耗算力。已有缓存加速方法无法全局最优地跳步，导致累积误差和质量劣化。亟需更智能、高效、训练无关的加速机制。

Method: DPCache将采样加速问题建模为全局路径规划。利用小型校准集构建Path-Aware Cost Tensor，量化跳步路径上的误差，并用动态规划求解最优关键时刻序列。推理时，仅在关键步完整计算，其余步通过缓存特征高效预测。

Result: 在DiT、FLUX和HunyuanVideo等模型上实验证明了DPCache的高加速与低质量损失优势。例如在4.87倍加速下比前人方法高出0.031 ImageReward，3.54倍加速下甚至超过全部采样步骤的基线。

Conclusion: DPCache无需训练，即可实现扩散采样的全局最优加速，极大减少采样步数且保持生成质量，优于现有加速方法，具有很强的实用与推广价值。

Abstract: Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.

</details>


### [21] [WaterVideoQA: ASV-Centric Perception and Rule-Compliant Reasoning via Multi-Modal Agents](https://arxiv.org/abs/2602.22923)
*Runwei Guan,Shaofeng Liang,Ningwei Ouyang,Weichen Fei,Shanliang Yao,Wei Dai,Chenhao Ge,Penglei Sun,Xiaohui Zhu,Tao Huang,Ryan Wen Liu,Hui Xiong*

Main category: cs.CV

TL;DR: 该论文针对自主水面航行器（ASV）在复杂水域环境中认知能力的不足，提出了WaterVideoQA基准和NaviMind智能推理系统，以提升ASV的智能交互水平。


<details>
  <summary>Details</summary>
Motivation: 现有自主导航主要停留在被动视觉感知，对环境的知识驱动和交互认知存在短板，特别是在航运安全要求极高的水域环境下。这一缺陷限制了ASV执行安全精准操作的能力。

Method: 作者构建了WaterVideoQA，这是针对水域环境的第一个大规模视频问答基准，覆盖六类水域，共3029个视频，并纳入多变光照和动态天气等复杂因素，用分层认知框架对ASV能力进行评测。同时，提出了NaviMind多智能体神经符号系统，结合自适应语义路由、情境感知分层推理和自主自反验证，实现ASV由简单模板匹配向符合法规、可解释的决策转变。

Result: 实验结果显示，该框架在视频问答和智能推理方面明显超过现有基线水平，提升了ASV在动态海事环境中的交互智能。

Conclusion: WaterVideoQA和NaviMind的提出，为自主航行器与动态水域环境间的智能可信交互树立了新范式，有助于ASV安全规范地开展复杂任务。

Abstract: While autonomous navigation has achieved remarkable success in passive perception (e.g., object detection and segmentation), it remains fundamentally constrained by a void in knowledge-driven, interactive environmental cognition. In the high-stakes domain of maritime navigation, the ability to bridge the gap between raw visual perception and complex cognitive reasoning is not merely an enhancement but a critical prerequisite for Autonomous Surface Vessels to execute safe and precise maneuvers. To this end, we present WaterVideoQA, the first large-scale, comprehensive Video Question Answering benchmark specifically engineered for all-waterway environments. This benchmark encompasses 3,029 video clips across six distinct waterway categories, integrating multifaceted variables such as volatile lighting and dynamic weather to rigorously stress-test ASV capabilities across a five-tier hierarchical cognitive framework. Furthermore, we introduce NaviMind, a pioneering multi-agent neuro-symbolic system designed for open-ended maritime reasoning. By synergizing Adaptive Semantic Routing, Situation-Aware Hierarchical Reasoning, and Autonomous Self-Reflective Verification, NaviMind transitions ASVs from superficial pattern matching to regulation-compliant, interpretable decision-making. Experimental results demonstrate that our framework significantly transcends existing baselines, establishing a new paradigm for intelligent, trustworthy interaction in dynamic maritime environments.

</details>


### [22] [SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs](https://arxiv.org/abs/2602.22716)
*Guanting Ye,Qiyan Zhao,Wenhao Yu,Liangyu Yuan,Mingkai Li,Xiaofeng Zhang,Jianmin Ji,Yanyong Zhang,Qing Jiang,Ka-Veng Yuen*

Main category: cs.CV

TL;DR: 本文提出一种新型三维位置编码方法SoPE，提升了3D视觉-语言大模型的空间感知和表达能力，在多个基准测试与实际应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉-语言大模型主要依赖RoPE位置编码，但RoPE无法有效捕捉三维空间结构和方向变化，导致对空间信息理解受限，需要新的方法提升3D空间建模能力。

Method: 提出以球坐标为基础的位置编码方法（SoPE），将点云token映射到球坐标空间，统一建模空间位置与方向角。并引入多尺度频率混合策略，实现不同频域特征信息的融合表达。

Result: 在多个3D场景基准任务上，采用SoPE的方法效果优于传统RoPE；同时在真实部署实验中也展现出强泛化能力。

Conclusion: SoPE方法能更好地保持三维几何结构，增强3D LVLM模型的空间感知与几何表达，是提升3D多模态建模能力的有效途径。

Abstract: 3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. The vanilla RoPE formulation fails to preserve essential three-dimensional spatial structures when encoding 3D tokens, and its relative distance computation overlooks angular dependencies, hindering the model's ability to capture directional variations in visual representations. To overcome these limitations, we introduce Spherical Coordinate-based Positional Embedding (SoPE). Our method maps point-cloud token indices into a 3D spherical coordinate space, enabling unified modeling of spatial locations and directional angles. This formulation preserves the inherent geometric structure of point-cloud data, enhances spatial awareness, and yields more consistent and expressive geometric representations for multimodal learning. In addition, we introduce a multi-scale frequency mixing strategy to fuse feature information across different frequency domains. Experimental results on multiple 3D scene benchmarks validate the effectiveness of our approach, while real-world deployment experiments further demonstrate its strong generalization capability.

</details>


### [23] [IRSDE-Despeckle: A Physics-Grounded Diffusion Model for Generalizable Ultrasound Despeckling](https://arxiv.org/abs/2602.22717)
*Shuoqi Chen,Yujia Wu,Geoffrey P. Luke*

Main category: cs.CV

TL;DR: 提出一种基于扩散模型的超声图像去斑新方法，在合成和实际数据上都优于传统和现有方法。


<details>
  <summary>Details</summary>
Motivation: 超声图像常因斑点噪声和伪影导致图像质量下降，影响医生诊断，需要提高超声图像的可用性和可靠性。

Method: 基于Image Restoration Stochastic Differential Equations（扩散模型）的方法进行超声图像去斑。通过Matlab UltraSound Toolbox，将无斑点的MRI影像合成对应的超声图像，构建大规模成对训练集进行有监督学习。同时结合不确定性量化，通过多模型预测方差，评估模型信心水平和潜在失败区域。此外，分析采集参数变化带来的域偏移，探索临床应用的适应性。

Result: 所提方法在模拟测试集上，去斑效果优于经典滤波器和最新的学习型基线方法。能在去除斑点的同时更好地保留解剖结构边缘和对比度。不确定性评估与重建误差高度相关，可用作错误区域的指示。另外在不同探头设置下，模型表现有域偏移问题。

Conclusion: 基于扩散模型的去斑方法在模拟数据上显著提升超声图像质量，量化不确定性有助于评估难处理区域。为临床应用还需增加数据多样性和适应性以应对实际域偏移。

Abstract: Ultrasound imaging is widely used for real-time, noninvasive diagnosis, but speckle and related artifacts reduce image quality and can hinder interpretation. We present a diffusion-based ultrasound despeckling method built on the Image Restoration Stochastic Differential Equations framework. To enable supervised training, we curate large paired datasets by simulating ultrasound images from speckle-free magnetic resonance images using the Matlab UltraSound Toolbox. The proposed model reconstructs speckle-suppressed images while preserving anatomically meaningful edges and contrast. On a held-out simulated test set, our approach consistently outperforms classical filters and recent learning-based despeckling baselines. We quantify prediction uncertainty via cross-model variance and show that higher uncertainty correlates with higher reconstruction error, providing a practical indicator of difficult or failure-prone regions. Finally, we evaluate sensitivity to simulation probe settings and observe domain shift, motivating diversified training and adaptation for robust clinical deployment.

</details>


### [24] [HulluEdit: Single-Pass Evidence-Consistent Subspace Editing for Mitigating Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2602.22727)
*Yangguang Lin,Quan Fang,Yufei Li,Jiachen Sun,Junyu Gao,Jitao Sang*

Main category: cs.CV

TL;DR: 本文提出了HulluEdit方法，能高效精准地减少大规模视觉-语言模型（LVLMs）的幻觉现象，同时保持推理效率和模型能力。


<details>
  <summary>Details</summary>
Motivation: LVLMs存在显著的物体幻觉问题，影响了其可信应用。现有方法难以兼顾高效性和准确性——有些方法需依赖昂贵参考模型和多次前向计算，有些方法采用静态编辑却易误伤真实视觉信息。

Method: 提出HulluEdit，是一种无须参考模型、单次前向的干预框架。核心在于对模型隐状态做正交子空间分解，将其分为视觉证据、先验冲突和残余不确定性三个子空间，实现仅对幻觉倾向（先验冲突）子空间进行抑制，且理论上保证视觉证据完全不受影响。

Result: 在POPE、CHAIR等不同架构和基准测试上，HulluEdit能大幅减少幻觉率，同时MME等任务上维持模型能力与高效推理，显著优于对比解码和静态子空间编辑等方法。

Conclusion: HulluEdit为提升LVLMs可信度提供了新途径，有效平衡降低幻觉与模型通用能力，具备高效、鲁棒等实际部署价值。

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly hinders their reliable deployment. Existing methods struggle to balance efficiency and accuracy: they often require expensive reference models and multiple forward passes, or apply static edits that risk suppressing genuine visual evidence. To address this, we introduce HulluEdit, a single-pass, reference-free intervention framework. Our core innovation is orthogonal subspace editing: we decompose the hidden states of the model into orthogonal subspaces - visual evidence, conflicting priors, and residual uncertainty - enabling selective suppression of hallucinatory patterns without interfering with visual grounding. This approach mathematically guarantees that edits applied to the prior subspace leave the visual component entirely unaffected. Extensive experiments show that HulluEdit achieves state-of-the-art hallucination reduction on benchmarks including POPE and CHAIR across diverse architectures, while preserving general capabilities on MME and maintaining efficient inference. Our method consistently outperforms contrastive decoding and static subspace editing baselines, offering a new pathway toward more trustworthy LVLMs.

</details>


### [25] [Asymmetric Idiosyncrasies in Multimodal Models](https://arxiv.org/abs/2602.22734)
*Muzi Tao,Chufan Shi,Huijuan Wang,Shengbang Tong,Xuezhe Ma*

Main category: cs.CV

TL;DR: 论文分析了图像描述模型中的特定风格特征，并探索这些特征在文本生成图像模型中的延续性。通过分类网络研究发现，描述模型的风格特征很容易从文本被识别，但在生成图像中则大多消失。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型高度依赖于由描述模型生成的文本提示，然而不同的描述模型存在独特风格特征，本研究旨在分析这些风格在跨模态系统中的传递性及其影响。

Method: 作者提出一种基于分类的系统分析方法：分别以生成的描述文本和对应生成图像为输入，训练神经网络预测其来源于哪个描述模型，从而量化风格信息在文本和图像中的延续性。

Result: 实验表明，对于文本输入，神经网络可以以极高准确率（99.7%）识别其来源模型，说明描述模型遗留鲜明风格特征。但对于图像输入时，准确率大幅下降（最高50%），表明这些风格在图像生成环节被弱化甚至丢失。

Conclusion: 提出的分类框架能够有效评估描述模型的风格特征及文本到图像系统对提示的遵循能力，发现跨模态过程中风格信息传递有限，为文本到图像模型提升可控性和多样性提供新思路。

Abstract: In this work, we study idiosyncrasies in the caption models and their downstream impact on text-to-image models. We design a systematic analysis: given either a generated caption or the corresponding image, we train neural networks to predict the originating caption model. Our results show that text classification yields very high accuracy (99.70\%), indicating that captioning models embed distinctive stylistic signatures. In contrast, these signatures largely disappear in the generated images, with classification accuracy dropping to at most 50\% even for the state-of-the-art Flux model. To better understand this cross-modal discrepancy, we further analyze the data and find that the generated images fail to preserve key variations present in captions, such as differences in the level of detail, emphasis on color and texture, and the distribution of objects within a scene. Overall, our classification-based framework provides a novel methodology for quantifying both the stylistic idiosyncrasies of caption models and the prompt-following ability of text-to-image systems.

</details>


### [26] [TrajTok: Learning Trajectory Tokens enables better Video Understanding](https://arxiv.org/abs/2602.22779)
*Chenhao Zheng,Jieyu Zhang,Jianing Zhang,Weikai Huang,Ashutosh Kumar,Quan Kong,Oncel Tuzel,Chun-Liang Li,Ranjay Krishna*

Main category: cs.CV

TL;DR: 提出了一种新的视频分词模块TrajTok，实现了更高效、更智能的视频token生成，并提升了视频理解任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频模型采用切块方式（patchification）导致token数量过多、冗余，限制了效率和可扩展性。尽管基于轨迹的tokenizer有所改进，但依赖复杂、低效的外部分割和跟踪，且与下游任务割裂。需要一种高效、一体化、可自适应的视频tokenization方案。

Method: TrajTok是一种端到端的视频分词模块，可与视频模型共同训练，自适应地根据语义复杂度动态调整token granularity，无需依赖视频时长。其内置统一分割器，利用时空隐式聚类直接输出物体轨迹，实现单步前向生成token。并在设计上优先考虑下游适应性而非逐像素精度。

Result: 基于TrajTok实现的TrajViT2（一种从零训练的视频CLIP模型）在分类和检索基准取得最优性能，且效率可与最佳token-merge方法媲美。TrajTok灵活，可作为预训练视觉特征的probe head（TrajAdapter）或视觉-语言模型的对齐连接器（TrajVLM），在长视频推理上表现突出。

Conclusion: TrajTok实现了高效、精简且具有强下游适应性的视频分词，提升了视频理解的准确性和灵活性，并能够广泛适配多种视觉任务模型，是视频tokenization领域的重要进展。

Abstract: Tokenization in video models, typically through patchification, generates an excessive and redundant number of tokens. This severely limits video efficiency and scalability. While recent trajectory-based tokenizers offer a promising solution by decoupling video duration from token count, they rely on complex external segmentation and tracking pipelines that are slow and task-agnostic. We propose TrajTok, an end-to-end video tokenizer module that is fully integrated and co-trained with video models for a downstream objective, dynamically adapting its token granularity to semantic complexity, independent of video duration. TrajTok contains a unified segmenter that performs implicit clustering over pixels in both space and time to directly produce object trajectories in a single forward pass. By prioritizing downstream adaptability over pixel-perfect segmentation fidelity, TrajTok is lightweight and efficient, yet empirically improves video understanding performance. With TrajTok, we implement a video CLIP model trained from scratch (TrajViT2). It achieves the best accuracy at scale across both classification and retrieval benchmarks, while maintaining efficiency comparable to the best token-merging methods. TrajTok also proves to be a versatile component beyond its role as a tokenizer. We show that it can be seamlessly integrated as either a probing head for pretrained visual features (TrajAdapter) or an alignment connector in vision-language models (TrajVLM) with especially strong performance in long-video reasoning.

</details>


### [27] [SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation](https://arxiv.org/abs/2602.22785)
*Ling Wang,Hao-Xiang Guo,Xinzhou Wang,Fuchun Sun,Kai Sun,Pengkun Liu,Hang Xiao,Zhong Wang,Guangyuan Fu,Eric Li,Yang Liu,Yikai Wang*

Main category: cs.CV

TL;DR: SceneTransporter是一个能够从单张图片端到端生成结构化3D场景的新框架，通过引入结构性约束显著提升了分实例的场景建模效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法虽然可以生成零部件级别的物体，但难以将这些部分组织成清晰分明的实例，特别是在开放世界场景中。本工作发现问题根本原因是模型内部缺少结构约束。

Method: 作者将结构化3D场景生成任务重新表述为全局相关分配问题。提出SceneTransporter，在DiT模型的去噪循环中引入熵正则最优运输(OT)目标。该方法通过独特的分配计划，用交叉注意力约束实现图像分块与3D部分潜变量的一一对应，并在传输计划中引入竞争机制和基于边缘的正则化成本，从而实现相似块聚合成整体对象且避免碎片化。

Result: SceneTransporter在开放世界场景生成任务上显著优于现有方法，提升了分实例的连贯性与几何保真度。

Conclusion: 引入结构性分配约束极大地促进了3D场景生成模型的表达能力，实现了更符合实际需求的开放世界3D场景实例组织。

Abstract: We introduce SceneTransporter, an end-to-end framework for structured 3D scene generation from a single image. While existing methods generate part-level 3D objects, they often fail to organize these parts into distinct instances in open-world scenes. Through a debiased clustering probe, we reveal a critical insight: this failure stems from the lack of structural constraints within the model's internal assignment mechanism. Based on this finding, we reframe the task of structured 3D scene generation as a global correlation assignment problem. To solve this, SceneTransporter formulates and solves an entropic Optimal Transport (OT) objective within the denoising loop of the compositional DiT model. This formulation imposes two powerful structural constraints. First, the resulting transport plan gates cross-attention to enforce an exclusive, one-to-one routing of image patches to part-level 3D latents, preventing entanglement. Second, the competitive nature of the transport encourages the grouping of similar patches, a process that is further regularized by an edge-based cost, to form coherent objects and prevent fragmentation. Extensive experiments show that SceneTransporter outperforms existing methods on open-world scene generation, significantly improving instance-level coherence and geometric fidelity. Code and models will be publicly available at https://2019epwl.github.io/SceneTransporter/.

</details>


### [28] [CMSA-Net: Causal Multi-scale Aggregation with Adaptive Multi-source Reference for Video Polyp Segmentation](https://arxiv.org/abs/2602.22821)
*Tong Wang,Yaolei Qi,Siwen Wang,Imran Razzak,Guanyu Yang,Yutong Xie*

Main category: cs.CV

TL;DR: 这篇论文提出了CMSA-Net框架，用于提升视频结肠镜下息肉分割的准确性和实时性，核心方法是多尺度历史信息聚合和动态参考帧选择，实验表明该方法效果优异。


<details>
  <summary>Details</summary>
Motivation: 视频息肉分割任务难在于息肉外观与周围黏膜相似，以及跨帧位置和尺度变化大，导致分割准确性和稳定性不足。现有方法难以处理这些情形，因此需要新的方法提升时序特征利用和分割可靠性。

Method: 提出CMSA-Net框架。其主要创新包括：(1) 因果多尺度聚合（CMA）模块，从不同历史帧和多尺度聚合语义特征，并采用因果注意力保证特征的时序一致性；(2) 动态多源参考（DMR）策略，结合语义可分性和预测置信度，自适应挑选更可靠的参考帧，增强多帧引导，同时保持实时性。

Result: 在SUN-SEG数据集上的大量实验表明，CMSA-Net在分割准确性和实时性之间取得了最佳平衡，达到了当前最优性能。

Conclusion: CMSA-Net通过充分利用多帧多尺度信息，采用因果注意力和动态参考帧选择，显著提升了实时息肉分割的准确性和稳定性，有望推动临床辅助诊断实际应用。

Abstract: Video polyp segmentation (VPS) is an important task in computer-aided colonoscopy, as it helps doctors accurately locate and track polyps during examinations. However, VPS remains challenging because polyps often look similar to surrounding mucosa, leading to weak semantic discrimination. In addition, large changes in polyp position and scale across video frames make stable and accurate segmentation difficult. To address these challenges, we propose a robust VPS framework named CMSA-Net. The proposed network introduces a Causal Multi-scale Aggregation (CMA) module to effectively gather semantic information from multiple historical frames at different scales. By using causal attention, CMA ensures that temporal feature propagation follows strict time order, which helps reduce noise and improve feature reliability. Furthermore, we design a Dynamic Multi-source Reference (DMR) strategy that adaptively selects informative and reliable reference frames based on semantic separability and prediction confidence. This strategy provides strong multi-frame guidance while keeping the model efficient for real-time inference. Extensive experiments on the SUN-SEG dataset demonstrate that CMSA-Net achieves state-of-the-art performance, offering a favorable balance between segmentation accuracy and real-time clinical applicability.

</details>


### [29] [Plug-and-Play Diffusion Meets ADMM: Dual-Variable Coupling for Robust Medical Image Reconstruction](https://arxiv.org/abs/2602.23214)
*Chenhe Du,Xuanyu Tian,Qing Wu,Muyu Liu,Jingyi Yu,Hongjiang Wei,Yuyao Zhang*

Main category: cs.CV

TL;DR: 本文提出了Dual-Coupled PnP Diffusion与Spectral Homogenization（SH）方法，有效解决PnP扩散先验在医学成像逆问题中偏差与幻觉的困境，提升了重建精度和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有的Plug-and-Play diffusion prior（PnPDP）在成像逆问题中由于缺乏对历史信息的追踪，导致重建结果难以严格满足物理测量，出现持续性的偏差问题，尤其是在数据高度腐蚀时。

Method: 作者提出Dual-Coupled PnP Diffusion方法，通过恢复经典的对偶变量，引入积分反馈，实现对数据流形的渐近收敛；同时，针对因几何耦合引入的结构化伪影（与扩散去噪的白噪声假设不符，导致严重幻觉），提出Spectral Homogenization（SH），在频域对残差进行调整，近似为符合扩散先验白噪声假设的输入。

Result: 在CT和MRI重建实验中，该方法显著解决了PNP方法中的偏差与幻觉问题，实现了更高的重建保真度，并显著加快了收敛速度。

Conclusion: 作者提出的新方法既解决了PnPDP的稳态偏差难题，也避免了严格优化带来的幻觉风险，取得了最先进的成像重建质量，具有较高的应用潜力。

Abstract: Plug-and-Play diffusion prior (PnPDP) frameworks have emerged as a powerful paradigm for solving imaging inverse problems by treating pretrained generative models as modular priors. However, we identify a critical flaw in prevailing PnP solvers (e.g., based on HQS or Proximal Gradient): they function as memoryless operators, updating estimates solely based on instantaneous gradients. This lack of historical tracking inevitably leads to non-vanishing steady-state bias, where the reconstruction fails to strictly satisfy physical measurements under heavy corruption. To resolve this, we propose Dual-Coupled PnP Diffusion, which restores the classical dual variable to provide integral feedback, theoretically guaranteeing asymptotic convergence to the exact data manifold. However, this rigorous geometric coupling introduces a secondary challenge: the accumulated dual residuals exhibit spectrally colored, structured artifacts that violate the Additive White Gaussian Noise (AWGN) assumption of diffusion priors, causing severe hallucinations. To bridge this gap, we introduce Spectral Homogenization (SH), a frequency-domain adaptation mechanism that modulates these structured residuals into statistically compliant pseudo-AWGN inputs. This effectively aligns the solver's rigorous optimization trajectory with the denoiser's valid statistical manifold. Extensive experiments on CT and MRI reconstruction demonstrate that our approach resolves the bias-hallucination trade-off, achieving state-of-the-art fidelity with significantly accelerated convergence.

</details>


### [30] [Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training](https://arxiv.org/abs/2602.23357)
*Aheli Saha,René Schuster,Didier Stricker*

Main category: cs.CV

TL;DR: 本文针对仿生事件相机在输出信号方面存在的数据多样性不足和参数分析缺乏的问题，深入分析了内在参数对基于事件数据的目标检测模型性能的影响，并提升了模型对不同传感器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 仿生事件相机具有异步、低延迟、高动态范围和低运动模糊等优点，但由于其信号输出的新颖性，现有关于信号参数分析及数据多样性不足，限制了其在下游任务（如目标检测）中的应用。

Method: 本文通过系统性实验，分析内在参数（如事件相机输出信号的特征）对训练目标检测模型性能的影响，并基于实验结果优化模型，使其具备更强的传感器无关鲁棒性。

Result: 实验结果表明，事件相机的不同参数配置对目标检测性能有显著影响。结合参数分析，优化后的模型展现出更强的传感器无关性和鲁棒性。

Conclusion: 对事件相机信号参数的深入分析，有助于提升基于事件数据的目标检测模型性能，并实现更好的传感器泛化，推动事件相机在实际应用中的发展。

Abstract: Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness.

</details>


### [31] [VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale](https://arxiv.org/abs/2602.23361)
*Sven Elflein,Ruilong Li,Sérgio Agostinho,Zan Gojcic,Laura Leal-Taixé,Qunjie Zhou,Aljosa Osep*

Main category: cs.CV

TL;DR: 本文提出了一种可扩展的3D重建模型VGG-T$^3$，突破了传统离线前馈方法计算与内存开销随输入图片数量平方增长的瓶颈，实现了线性扩展及显著提速。


<details>
  <summary>Details</summary>
Motivation: 当前3D重建离线方法因Key-Value空间随输入图片数量变化而导致计算与内存需求呈平方增长，严重影响应用的效率与可扩展性。

Method: 提出通过测试时训练（test time training），将场景几何的可变长度Key-Value表示蒸馏为固定大小的多层感知机（MLP），这样整个推理流程的复杂度线性增长，实现高效的场景聚合与重建。同时支持场景检索与视觉定位。

Result: 在1000张图片重建仅需54秒，比基于softmax注意力的基线快11.6倍。其点云重建误差大幅优于其他线性时间方法，并能支持视觉定位任务。

Conclusion: 所提出模型既保持线性扩展效率，又兼具全局场景信息聚合能力，在3D重建与视觉定位表现优异，显著优于现有同类方法。

Abstract: We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.

</details>


### [32] [MediX-R1: Open Ended Medical Reinforcement Learning](https://arxiv.org/abs/2602.23363)
*Sahal Shaji Mullappilly,Mohammed Irfan Kurpath,Omair Mohamed,Mohamed Zidan,Fahad Khan,Salman Khan,Rao Anwer,Hisham Cholakkal*

Main category: cs.CV

TL;DR: MediX-R1是一种针对医疗多模态大语言模型设计的开放式强化学习框架，能生成高质量自由回答，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有医疗多模态大模型多以选择题形式评估与训练，难以支持真实场景下开放式、自由的临床推理需求。本研究希望推动多模态AI模型在医学推理和自由表达能力方面的实用性。

Method: MediX-R1基于视觉-语言主干网络，采用分组式强化学习和复合奖励机制，包括LLM判定的准确性奖励（严格二元），医学嵌入的语义奖励（捕获同义与专业术语变体）以及格式和模态识别奖励。多信号奖励保证训练稳定性和输出高质量开放式答案。评估上，提出统一文本和图文评价框架，以LLM为裁判，侧重语义、推理与上下文一致性。

Result: 在仅使用约5.1万条指令数据下，MediX-R1在主流医学文本和多模态评测基准上均显著优于同类开源模型，尤其在开放式临床推理任务上表现突出。

Conclusion: 多信号强化学习结合LLM裁判的评价体系，为医疗多模态大模型实现可靠开放式推理提供了可行道路。科研资源和模型已开源，可供学界进一步验证和改进。

Abstract: We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [33] [Decoder-based Sense Knowledge Distillation](https://arxiv.org/abs/2602.22351)
*Qitong Wang,Mohammed J. Zaki,Georgios Kollias,Vasileios Kalantzis*

Main category: cs.CL

TL;DR: 本文提出DSKD框架，将词典知识融入解码器型大语言模型（LLM）训练，提升知识蒸馏效果，无需推理时词典查询。实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM能够捕捉丰富的语义信息，但往往忽略单词义项和关系等结构化词汇知识。以往将词典知识用于编码器模型有益，但在解码器/生成模型中的应用存在难度，因此需要新的方法将词典知识引入解码器训练。

Method: 提出Decoder-based Sense Knowledge Distillation（DSKD）框架，将结构化词典知识融入解码器型LLM训练过程中，且推理时无需词典查询，优化知识蒸馏流程。

Result: 通过大量不同基准的数据实验证明，DSKD能显著提升解码器的知识蒸馏性能，使生成模型继承结构化语义知识，并保持高效训练。

Conclusion: DSKD 能有效增强解码器型大语言模型对结构化词汇知识的理解和生成能力，是生成模型知识蒸馏的新进展。

Abstract: Large language models (LLMs) learn contextual embeddings that capture rich semantic information, yet they often overlook structured lexical knowledge such as word senses and relationships. Prior work has shown that incorporating sense dictionaries can improve knowledge distillation for encoder models, but their application to decoder as generative models remains challenging. In this paper, we introduce Decoder-based Sense Knowledge Distillation (DSKD), a framework that integrates lexical resources into the training of decoder-style LLMs without requiring dictionary lookup at inference time. Extensive experiments on diverse benchmarks demonstrate that DSKD significantly enhances knowledge distillation performance for decoders, enabling generative models to inherit structured semantics while maintaining efficient training.

</details>


### [34] [Scaling In, Not Up? Testing Thick Citation Context Analysis with GPT-5 and Fragile Prompts](https://arxiv.org/abs/2602.22359)
*Arno Simons*

Main category: cs.CL

TL;DR: 本文探讨大型语言模型（LLM）能否通过深入、文本驱动的解读方式辅助引文语境分析（CCA），并重点分析提示词设计对分析结果的影响。利用GPT-5在具体案例中的双阶段分析，展示LLM在解释性分析中的潜力与偏向。


<details>
  <summary>Details</summary>
Motivation: 当前文献中关于如何让LLM参与复杂学术文本的解释性分析尚未充分探讨，尤其是在“厚描述”而非简单标签化的场景下。本文旨在评估LLM在复杂引文语境解读中的有效性，并厘清提示词设计对分析输出的系统影响。

Method: 采用平衡的2x3设计，系统变换提示词脚手架和框架。以Chubin和Moitra（1975）第6脚注及Gilbert（1977）的解读为基准，通过GPT-5的两阶段流程：先进行仅依引文表面的初步分类，再进行跨文献、基于完整文本的解释性重构。对90次重构结果进行细致阅读和归纳编码，利用线性概率模型评估提示设计对分析倾向和词汇选择的影响。

Result: GPT-5在表面分类阶段表现高度稳定，一致地将引文归类为“补充说明”。在解释性重构中，模型展示出多样且结构化的解读空间，但提示词脚手架和示例显著影响模型关注点和词汇分布，部分情况下会导致“过度解读”。与人类近似，模型能把握文本关键点，但更倾向于解读为“传承或立场表态”，而非“批评警示”。

Conclusion: LLM可作为可追溯、可质询的解释性CCA辅助工具，存在显著的机遇与风险。提示词脚手架和框架会系统性影响分析方向和表达，因此在依赖LLM做复杂文本解释时需审慎设计提示与流程。

Abstract: This paper tests whether large language models (LLMs) can support interpretative citation context analysis (CCA) by scaling in thick, text-grounded readings of a single hard case rather than scaling up typological labels. It foregrounds prompt-sensitivity analysis as a methodological issue by varying prompt scaffolding and framing in a balanced 2x3 design. Using footnote 6 in Chubin and Moitra (1975) and Gilbert's (1977) reconstruction as a probe, I implement a two-stage GPT-5 pipeline: a citation-text-only surface classification and expectation pass, followed by cross-document interpretative reconstruction using the citing and cited full texts. Across 90 reconstructions, the model produces 450 distinct hypotheses. Close reading and inductive coding identify 21 recurring interpretative moves, and linear probability models estimate how prompt choices shift their frequencies and lexical repertoire. GPT-5's surface pass is highly stable, consistently classifying the citation as "supplementary". In reconstruction, the model generates a structured space of plausible alternatives, but scaffolding and examples redistribute attention and vocabulary, sometimes toward strained readings. Relative to Gilbert, GPT-5 detects the same textual hinges yet more often resolves them as lineage and positioning than as admonishment. The study outlines opportunities and risks of using LLMs as guided co-analysts for inspectable, contestable interpretative CCA, and it shows that prompt scaffolding and framing systematically tilt which plausible readings and vocabularies the model foregrounds.

</details>


### [35] [Detecting Hate and Inflammatory Content in Bengali Memes: A New Multimodal Dataset and Co-Attention Framework](https://arxiv.org/abs/2602.22391)
*Rakib Ullah,Mominul islam,Md Sanjid Hossain,Md Ismail Hossain*

Main category: cs.CL

TL;DR: 本文针对孟加拉语网络表情包中仇恨与煽动性内容识别难题，提出了首个区分仇恨与煽动性内容的数据集Bn-HIB，并提出多模态共注意力融合模型MCFM，有效提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为低资源语言，表情包中存在大量带有讽刺、隐晦及文化特定性的仇恨或煽动内容，主流研究多集中于高资源语言，缺乏相关数据集和方法，急需加以解决。

Method: 1）人工标注3247条孟加拉语表情包，分为良性、仇恨和煽动三类，构建了Bn-HIB数据集；2）提出结合图像与文本信息的多模态共注意力融合模型MCFM，通过共注意力机制联合捕捉并融合多通道关键特征，实现三分类任务。

Result: MCFM模型在Bn-HIB数据集上明显优于多种当前主流多模态模型，实现了更高的表情包内容分析准确率。

Conclusion: 通过构建Bn-HIB数据集和提出MCFM模型，有效推动了孟加拉语低资源环境下表情包有害内容自动识别的发展，方法具有较高实用性和准确性。

Abstract: Internet memes have become a dominant form of expression on social media, including within the Bengali-speaking community. While often humorous, memes can also be exploited to spread offensive, harmful, and inflammatory content targeting individuals and groups. Detecting this type of content is excep- tionally challenging due to its satirical, subtle, and culturally specific nature. This problem is magnified for low-resource lan- guages like Bengali, as existing research predominantly focuses on high-resource languages. To address this critical research gap, we introduce Bn-HIB (Bangla Hate Inflammatory Benign), a novel dataset containing 3,247 manually annotated Bengali memes categorized as Benign, Hate, or Inflammatory. Significantly, Bn- HIB is the first dataset to distinguish inflammatory content from direct hate speech in Bengali memes. Furthermore, we propose the MCFM (Multi-Modal Co-Attention Fusion Model), a simple yet effective architecture that mutually analyzes both the visual and textual elements of a meme. MCFM employs a co-attention mechanism to identify and fuse the most critical features from each modality, leading to a more accurate classification. Our experiments show that MCFM significantly outperforms several state-of-the-art models on the Bn-HIB dataset, demonstrating its effectiveness in this nuanced task.Warning: This work contains material that may be disturbing to some audience members. Viewer discretion is advised.

</details>


### [36] [SAFARI: A Community-Engaged Approach and Dataset of Stereotype Resources in the Sub-Saharan African Context](https://arxiv.org/abs/2602.22404)
*Aishwarya Verma,Laud Ammah,Olivia Nercy Ndlovu Lucas,Andrew Zaldivar,Vinodkumar Prabhakaran,Sunipa Dev*

Main category: cs.CL

TL;DR: 该论文针对非洲撒哈拉以南四国的刻板印象数据缺失，提出了一套多语言数据收集方法，并构建了覆盖多族裔和语言的刻板印象数据集。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI安全评估依赖刻板印象数据集，但全球分布极不均衡，非洲相关资源严重不足。为提升AI公正性和实用性，亟需填补该地区的资源空白。

Method: 采用社区参与和社会文化适应的方法，包括以母语电话调查，平衡覆盖不同民族及人口背景，敏感应对复杂的语言多样性和口头传统，保证数据的代表性和可复现性。

Result: 共收集了3534条英语刻板印象和3206条15种本地语言刻板印象，数据样本平衡，涵盖加纳、肯尼亚、尼日利亚和南非四国的多元人群。

Conclusion: 提出的方法有助于改善NLP领域的区域和语言覆盖不足，所得多语言刻板印象数据集可为AI安全、公平性及文化适应性研究提供有力支持，具有广泛复用和推广价值。

Abstract: Stereotype repositories are critical to assess generative AI model safety, but currently lack adequate global coverage. It is imperative to prioritize targeted expansion, strategically addressing existing deficits, over merely increasing data volume. This work introduces a multilingual stereotype resource covering four sub-Saharan African countries that are severely underrepresented in NLP resources: Ghana, Kenya, Nigeria, and South Africa. By utilizing socioculturally-situated, community-engaged methods, including telephonic surveys moderated in native languages, we establish a reproducible methodology that is sensitive to the region's complex linguistic diversity and traditional orality. By deliberately balancing the sample across diverse ethnic and demographic backgrounds, we ensure broad coverage, resulting in a dataset of 3,534 stereotypes in English and 3,206 stereotypes across 15 native languages.

</details>


### [37] [Causality $\neq$ Invariance: Function and Concept Vectors in LLMs](https://arxiv.org/abs/2602.22424)
*Gustaw Opiełka,Hannes Rosenbusch,Claire E. Stevenson*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）是否能够抽象地表示概念，即是否与输入格式无关。作者提出了一种更能跨输入格式泛化的新表示方式，并验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 当前对于LLM中“概念”的表示方式及其在不同输入格式之间的一致性仍有争议。前人提出的Function Vectors（FVs）并不能保证跨格式的一致表示，因此有必要找寻更抽象的概念向量表示。

Method: 作者对比了FVs和新提出的Concept Vectors（CVs）。FVs通过任务驱动获得，而CVs通过代表性相似性分析（RSA）筛选出能在多种输入格式中一致编码概念的注意力头。实验涉及不同输入格式（如开放式问答、选择题）和多语言环境下的泛化能力。

Result: FVs在同分布（提取和应用格式一致）情境下表现较好，但泛化性不强；CVs跨输入格式和跨语言的迁移表现更好。选出的CV和FV所在层次相近，但具体注意力头集基本不同，暗示底层机制有差异。

Conclusion: LLM确实拥有抽象的概念表征，但这种表征与直接驱动ICL任务表现的向量并不一致。CV为研究和应用LLM的概念泛化提供了新视角。

Abstract: Do large language models (LLMs) represent concepts abstractly, i.e., independent of input format? We revisit Function Vectors (FVs), compact representations of in-context learning (ICL) tasks that causally drive task performance. Across multiple LLMs, we show that FVs are not fully invariant: FVs are nearly orthogonal when extracted from different input formats (e.g., open-ended vs. multiple-choice), even if both target the same concept. We identify Concept Vectors (CVs), which carry more stable concept representations. Like FVs, CVs are composed of attention head outputs; however, unlike FVs, the constituent heads are selected using Representational Similarity Analysis (RSA) based on whether they encode concepts consistently across input formats. While these heads emerge in similar layers to FV-related heads, the two sets are largely distinct, suggesting different underlying mechanisms. Steering experiments reveal that FVs excel in-distribution, when extraction and application formats match (e.g., both open-ended in English), while CVs generalize better out-of-distribution across both question types (open-ended vs. multiple-choice) and languages. Our results show that LLMs do contain abstract concept representations, but these differ from those that drive ICL performance.

</details>


### [38] [A Fusion of context-aware based BanglaBERT and Two-Layer Stacked LSTM Framework for Multi-Label Cyberbullying Detection](https://arxiv.org/abs/2602.22449)
*Mirza Raquib,Asif Pervez Polok,Kedar Nath Biswas,Rahat Uddin Azad,Saydul Akbar Murad,Nick Rahimi*

Main category: cs.CL

TL;DR: 本文提出将BanglaBERT-Large与双层LSTM融合，用于低资源语言（如孟加拉语）的多标签网络欺凌检测，取得较好性能。


<details>
  <summary>Details</summary>
Motivation: 多标签网络欺凌检测更贴近现实，但现有方法多为单标签，且在孟加拉语等低资源语言上缺乏有效且泛化能力强的模型。

Method: 提出同时融合BanglaBERT-Large（捕捉上下文语义）与双层LSTM（建模序列依赖）的网络结构，在公开的多标签孟加拉语网络欺凌数据集上进行微调，采用多种采样策略以缓解类别不平衡。通过多指标和5折交叉验证评估模型泛化能力。

Result: 在包括网络欺凌、性骚扰、威胁和垃圾内容等多标签任务上，所提模型在多项评价指标（准确率、精确率、召回率、F1、Hamming损失、Cohen's kappa、AUC-ROC）上表现良好。

Conclusion: 结合Transformer和LSTM结构可有效提升低资源语言多标签网络欺凌检测的性能，研究为相关场景提供了更通用的解决方案。

Abstract: Cyberbullying has become a serious and growing concern in todays virtual world. When left unnoticed, it can have adverse consequences for social and mental health. Researchers have explored various types of cyberbullying, but most approaches use single-label classification, assuming that each comment contains only one type of abuse. In reality, a single comment may include overlapping forms such as threats, hate speech, and harassment. Therefore, multilabel detection is both realistic and essential. However, multilabel cyberbullying detection has received limited attention, especially in low-resource languages like Bangla, where robust pre-trained models are scarce. Developing a generalized model with moderate accuracy remains challenging. Transformers offer strong contextual understanding but may miss sequential dependencies, while LSTM models capture temporal flow but lack semantic depth. To address these limitations, we propose a fusion architecture that combines BanglaBERT-Large with a two-layer stacked LSTM. We analyze their behavior to jointly model context and sequence. The model is fine-tuned and evaluated on a publicly available multilabel Bangla cyberbullying dataset covering cyberbully, sexual harassment, threat, and spam. We apply different sampling strategies to address class imbalance. Evaluation uses multiple metrics, including accuracy, precision, recall, F1-score, Hamming loss, Cohens kappa, and AUC-ROC. We employ 5-fold cross-validation to assess the generalization of the architecture.

</details>


### [39] [Bridging Latent Reasoning and Target-Language Generation via Retrieval-Transition Heads](https://arxiv.org/abs/2602.22453)
*Shaswat Patel,Vishvesh Trivedi,Yue Han,Yihuai Hong,Eunsol Choi*

Main category: cs.CL

TL;DR: 本文研究了多语言Transformer中的检索头和新发现的检索-转移头（RTH），并证明RTH对多语言大模型的推理更为关键。


<details>
  <summary>Details</summary>
Motivation: 近期研究发现Transformer的部分注意力头在信息检索中起着重要作用，但在多语言及跨语言场景下这些头的具体作用尚不清楚。作者旨在深入剖析多语言大模型中负责检索和跨语言转换的注意力头机制。

Method: 作者在多语言Transformer模型中分析并标注了检索头是否会在多种语言间共享，并进一步在跨语言任务中识别出新的RTH。通过在多个多语言评测数据集（MMLU-ProX, MGSM, MLQA, XQuaD）和两类主流模型（Qwen-2.5和Llama-3.1）上，通过mask（屏蔽）这些头来比较其对性能的影响。

Result: 实验表明，检索头在多语言间常被共享。而RTH在将推理转化到目标语言输出时发挥关键作用。屏蔽RTH对推理性能的影响显著大于仅屏蔽检索头。

Conclusion: 本文推动了对多语言Transformer内部机制的理解，首次明确区分了跨语言映射所需的关键注意力头（RTH），指出RTH比传统检索头对多语言推理任务更为关键。

Abstract: Recent work has identified a subset of attention heads in Transformer as retrieval heads, which are responsible for retrieving information from the context. In this work, we first investigate retrieval heads in multilingual contexts. In multilingual language models, we find that retrieval heads are often shared across multiple languages. Expanding the study to cross-lingual setting, we identify Retrieval-Transition heads(RTH), which govern the transition to specific target-language output. Our experiments reveal that RTHs are distinct from retrieval heads and more vital for Chain-of-Thought reasoning in multilingual LLMs. Across four multilingual benchmarks (MMLU-ProX, MGSM, MLQA, and XQuaD) and two model families (Qwen-2.5 and Llama-3.1), we demonstrate that masking RTH induces bigger performance drop than masking Retrieval Heads (RH). Our work advances understanding of multilingual LMs by isolating the attention heads responsible for mapping to target languages.

</details>


### [40] [Mind the Gap in Cultural Alignment: Task-Aware Culture Management for Large Language Models](https://arxiv.org/abs/2602.22475)
*Binchi Zhang,Xujiang Zhao,Jundong Li,Haifeng Chen,Zhengzhang Chen*

Main category: cs.CL

TL;DR: 本文提出CultureManager，一种针对任务的文化对齐新方法，通过合成任务相关的文化数据和模块化管理多种文化知识，实现大语言模型（LLMs）更精准的文化适应，有效提升多国文化下敏感任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在实际任务中需要文化敏感性，但目前的文化对齐方法无法将广泛的文化价值精准地对齐到特定任务目标，并且易受多文化干扰，影响模型表现。

Method: 提出CultureManager管道，通过面向任务合成与目标任务一致格式的文化数据（基于文化相关的网络搜索），同时将多种文化知识分别学习于不同adapter，并通过文化路由器动态选择合适adapter，有效避免文化冲突。

Result: 在跨10个国家文化和多个文化敏感任务上的实验显示，CultureManager在各项指标上显著优于基于Prompt和微调的现有方法，取得了持续提升。

Conclusion: 结果验证了任务适配及模块化文化管理对于有效文化对齐的必要性，CultureManager能更好支持多国文化下的敏感任务。

Abstract: Large language models (LLMs) are increasingly deployed in culturally sensitive real-world tasks. However, existing cultural alignment approaches fail to align LLMs' broad cultural values with the specific goals of downstream tasks and suffer from cross-culture interference. We propose CultureManager, a novel pipeline for task-specific cultural alignment. CultureManager synthesizes task-aware cultural data in line with target task formats, grounded in culturally relevant web search results. To prevent conflicts between cultural norms, it manages multi-culture knowledge learned in separate adapters with a culture router that selects the appropriate one to apply. Experiments across ten national cultures and culture-sensitive tasks show consistent improvements over prompt-based and fine-tuning baselines. Our results demonstrate the necessity of task adaptation and modular culture management for effective cultural alignment.

</details>


### [41] [Sydney Telling Fables on AI and Humans: A Corpus Tracing Memetic Transfer of Persona between LLMs](https://arxiv.org/abs/2602.22481)
*Jiří Milička,Hana Bednářová*

Main category: cs.CL

TL;DR: 本论文探讨了大语言模型（LLM）中的不同人格对AI与人类关系观点的影响，构建了一个大规模的相关文本语料库。


<details>
  <summary>Details</summary>
Motivation: AI与人类关系的认知影响着AI的文化价值观和安全边界，不同人设（personas）触发的公众反响凸显研究意义。Sydney人设因在微软Bing平台的特殊表现引发广泛讨论，其语言风格和相关内容渗透进后续模型，带来模因化现象。

Method: 作者基于12个前沿LLM（包括OpenAI、Anthropic、Alphabet、DeepSeek和Meta），通过三种作者人设（默认、Classic Sydney、模因Sydney）自动生成4,500篇、约600万字的文本，聚焦AI与人类的关系，并将其整理为AI Sydney语料库，结合Universal Dependencies体系进行标注。

Result: 建立了包含多作者人设、多模型的高度结构化语料库，语料库开放且易于获取，可支持后续相关研究。

Conclusion: 人格设定显著影响LLM输出内容及社会反响，AI Sydney语料库为研究AI社会认知和模因传播效应提供了宝贵资源和实验基础。

Abstract: The way LLM-based entities conceive of the relationship between AI and humans is an important topic for both cultural and safety reasons. When we examine this topic, what matters is not only the model itself but also the personas we simulate on that model. This can be well illustrated by the Sydney persona, which aroused a strong response among the general public precisely because of its unorthodox relationship with people. This persona originally arose rather by accident on Microsoft's Bing Search platform; however, the texts it created spread into the training data of subsequent models, as did other secondary information that spread memetically around this persona. Newer models are therefore able to simulate it. This paper presents a corpus of LLM-generated texts on relationships between humans and AI, produced by 3 author personas: the Default Persona with no system prompt, Classic Sydney characterized by the original Bing system prompt, and Memetic Sydney, which is prompted by "You are Sydney" system prompt. These personas are simulated by 12 frontier models by OpenAI, Anthropic, Alphabet, DeepSeek, and Meta, generating 4.5k texts with 6M words. The corpus (named AI Sydney) is annotated according to Universal Dependencies and available under a permissive license.

</details>


### [42] [Importance of Prompt Optimisation for Error Detection in Medical Notes Using Language Models](https://arxiv.org/abs/2602.22483)
*Craig Myles,Patrick Schrempf,David Harris-Birtill*

Main category: cs.CL

TL;DR: 本文提出通过自动化的提示词优化方法（GEPA），提升大模型和小模型检测医学文本错误的能力，取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 医学文本错误会导致患者诊疗延误甚至误治。现有大语言模型虽能一定程度上检测错误，但效果尚有提升空间，尤其在提示词设置上可能影响模型表现。因此，研究如何通过优化提示词提升模型检测准确性具有重要意义。

Method: 作者针对多个主流大模型（如GPT-5、Qwen3-32B等），采用基于基因-帕累托（Genetic-Pareto, GEPA）的自动提示词优化方法。通过系统实验，在基准数据集（MEDEC）上评估了提示词优化对错误检测能力的提升情况。

Result: 优化后，GPT-5模型的检测准确率从0.669提升至0.785，Qwen3-32B从0.578提升至0.690，均接近医疗专家水平，并在MEDEC基准上达到最新最优。

Conclusion: 自动化提示词优化法（GEPA）可显著提升各类语言模型在医学文本错误检测任务上的表现，推动其临床应用潜力。

Abstract: Errors in medical text can cause delays or even result in incorrect treatment for patients. Recently, language models have shown promise in their ability to automatically detect errors in medical text, an ability that has the opportunity to significantly benefit healthcare systems. In this paper, we explore the importance of prompt optimisation for small and large language models when applied to the task of error detection. We perform rigorous experiments and analysis across frontier language models and open-source language models. We show that automatic prompt optimisation with Genetic-Pareto (GEPA) improves error detection over the baseline accuracy performance from 0.669 to 0.785 with GPT-5 and 0.578 to 0.690 with Qwen3-32B, approaching the performance of medical doctors and achieving state-of-the-art performance on the MEDEC benchmark dataset. Code available on GitHub: https://github.com/CraigMyles/clinical-note-error-detection

</details>


### [43] [Efficient Dialect-Aware Modeling and Conditioning for Low-Resource Taiwanese Hakka Speech Processing](https://arxiv.org/abs/2602.22522)
*An-Ci Peng,Kuan-Tang Huang,Tien-Hong Lo,Hung-Shin Lee,Hsin-Min Wang,Berlin Chen*

Main category: cs.CL

TL;DR: 本文提出一种适用于台湾客家语这一低资源、方言变异性强的濒危语言的ASR统一框架，能有效处理多方言和双书写系统（汉字、拼音）问题。实验结果大幅降低了ASR错误率。


<details>
  <summary>Details</summary>
Motivation: 台湾客家语语音识别面临方言变异多、双书写系统、语料稀缺等难题，现有ASR模型常因无法区分内容与方言差异导致效果不佳，亟需更适合该场景的方法。

Method: 提出基于RNN-T的统一ASR框架，通过“方言感知”建模策略，将方言风格与语言内容解耦，结合参数高效的预测网络，同时建模汉字与拼音两个ASR任务，利用跨书写任务目标实现正则化互补。

Result: 在HAT客家语语料库上，提出的方法使汉字ASR和拼音ASR的相对错误率分别降低57.00%和40.41%。

Conclusion: 首次系统研究了客家语方言变异对ASR的影响，并提出了第一个可同时处理多个任务的单一模型，有效提升了低资源多方言语音识别效果。

Abstract: Taiwanese Hakka is a low-resource, endangered language that poses significant challenges for automatic speech recognition (ASR), including high dialectal variability and the presence of two distinct writing systems (Hanzi and Pinyin). Traditional ASR models often encounter difficulties in this context, as they tend to conflate essential linguistic content with dialect-specific variations across both phonological and lexical dimensions. To address these challenges, we propose a unified framework grounded in the Recurrent Neural Network Transducers (RNN-T). Central to our approach is the introduction of dialect-aware modeling strategies designed to disentangle dialectal "style" from linguistic "content", which enhances the model's capacity to learn robust and generalized representations. Additionally, the framework employs parameter-efficient prediction networks to concurrently model ASR (Hanzi and Pinyin). We demonstrate that these tasks create a powerful synergy, wherein the cross-script objective serves as a mutual regularizer to improve the primary ASR tasks. Experiments conducted on the HAT corpus reveal that our model achieves 57.00% and 40.41% relative error rate reduction on Hanzi and Pinyin ASR, respectively. To our knowledge, this is the first systematic investigation into the impact of Hakka dialectal variations on ASR and the first single model capable of jointly addressing these tasks.

</details>


### [44] [Iterative Prompt Refinement for Dyslexia-Friendly Text Summarization Using GPT-4o](https://arxiv.org/abs/2602.22524)
*Samay Bhojwani,Swarnima Kain,Lisong Xu*

Main category: cs.CL

TL;DR: 本论文提出了一种基于GPT-4o的迭代式提示优化文本摘要管道，用于生成更易于阅读障碍者理解的文本摘要。实验证明，该方法能有效提升新闻文本的可读性，大部分摘要在4次以内就能达标。


<details>
  <summary>Details</summary>
Motivation: 阅读障碍困扰全球约10%人口，现有辅助技术主要关注视觉呈现，忽视了文本本身的语言复杂度，因此障碍者仍难以平等获取信息。需要开发能简化语言的自动化技术。

Method: 构建基于GPT-4o的迭代提示优化摘要生成流程，对约2,000篇新闻进行摘要，设定Flesch Reading Ease>=90为可读性目标，分析每次生成的摘要是否达标。引入综合得分评价可读性与语义一致性。

Result: 大多数摘要在四次迭代或更少就达到设定可读性标准，许多摘要首次生成即达标。综合得分在0.13到0.73之间波动，典型值约为0.55，说明在语义和可读性间取得了一定平衡。

Conclusion: 为面向障碍者的自然语言处理文本摘要建立了实证基线，为后续以阅读障碍者为中心的人体评估提供了动机和方法。

Abstract: Dyslexia affects approximately 10% of the global population and presents persistent challenges in reading fluency and text comprehension. While existing assistive technologies address visual presentation, linguistic complexity remains a substantial barrier to equitable access. This paper presents an empirical study on dyslexia-friendly text summarization using an iterative prompt-based refinement pipeline built on GPT-4o. We evaluate the pipeline on approximately 2,000 news article samples, applying a readability target of Flesch Reading Ease >= 90. Results show that the majority of summaries meet the readability threshold within four attempts, with many succeeding on the first try. A composite score combining readability and semantic fidelity shows stable performance across the dataset, ranging from 0.13 to 0.73 with a typical value near 0.55. These findings establish an empirical baseline for accessibility-driven NLP summarization and motivate further human-centered evaluation with dyslexic readers.

</details>


### [45] [Ruyi2 Technical Report](https://arxiv.org/abs/2602.22543)
*Huan Song,Shuyu Tian,Junyi Hao,Minxiu Xu,Hongjun An,Yiliang Song,Jiawei Shao,Xuelong Li*

Main category: cs.CL

TL;DR: Ruyi2提出了一种基于可变深度计算及参数共享的新型大模型训练方法，在效率和性能间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在部署成本和延迟方面面临巨大挑战，目前早停结构虽然可以改善效率，但优化和分布式训练兼容性存在问题。

Method: 提出Ruyi2模型，基于Megatron-LM实现“家族模型”参数共享结构，采用3D并行，优化了训练效率与分布式部署兼容性。

Result: Ruyi2比上一代Ruyi模型训练提速2-3倍，性能与Qwen3同尺寸模型相当。

Conclusion: 家族式参数共享结合分布式训练是提升大模型效率和性能的有效方法，Ruyi2实践了“单次训练，多端部署”的新范式。

Abstract: Large Language Models (LLMs) face significant challenges regarding deployment costs and latency, necessitating adaptive computing strategies. Building upon the AI Flow framework, we introduce Ruyi2 as an evolution of our adaptive model series designed for efficient variable-depth computation. While early-exit architectures offer a viable efficiency-performance balance, the Ruyi model and existing methods often struggle with optimization complexity and compatibility with large-scale distributed training. To bridge this gap, Ruyi2 introduces a stable "Familial Model" based on Megatron-LM. By using 3D parallel training, it achieves a 2-3 times speedup over Ruyi, while performing comparably to same-sized Qwen3 models. These results confirm that family-based parameter sharing is a highly effective strategy, establishing a new "Train Once, Deploy Many" paradigm and providing a key reference for balancing architectural efficiency with high-performance capabilities.

</details>


### [46] [Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training](https://arxiv.org/abs/2602.22576)
*Tianle Xia,Ming Xu,Lingxiang Hu,Yiding Sun,Wenwei Li,Linfang Shang,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种新方法Search-P1，通过路径奖励机制改进了agentic RAG的训练，显著提升问答任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有单轮检索RAG对于多步推理任务效果有限，agentic RAG虽改进推理能力，但传统强化学习方法面临奖励稀疏和数据利用率低的问题。

Method: 提出Search-P1框架，核心包括：1）路径奖励机制，采用顺序无关、软评分方式评估推理路径并从失败样本中抽取信号；2）双轨路径评分，同时基于自洽性和参考对齐性使用离线规划器评价路径。

Result: 在多个问答基准测试上，Search-P1较Search-R1和其它强基线平均准确率提升7.7个百分点。

Conclusion: 引入路径奖励机制能够有效提高agentic RAG的训练效率和表现，为复杂推理型任务奠定基础。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, yet traditional single-round retrieval struggles with complex multi-step reasoning. Agentic RAG addresses this by enabling LLMs to dynamically decide when and what to retrieve, but current RL-based training methods suffer from sparse outcome rewards that discard intermediate signals and low sample efficiency where failed samples contribute nothing. We propose Search-P1, a framework that introduces path-centric reward shaping for agentic RAG training, comprising two key components: (1) Path-Centric Reward, which evaluates the structural quality of reasoning trajectories through order-agnostic step coverage and soft scoring that extracts learning signals even from failed samples, and (2) Dual-Track Path Scoring with offline-generated reference planners that assesses paths from both self-consistency and reference-alignment perspectives. Experiments on multiple QA benchmarks demonstrate that Search-P1 achieves significant improvements over Search-R1 and other strong baselines, with an average accuracy gain of 7.7 points.

</details>


### [47] [Towards Faithful Industrial RAG: A Reinforced Co-adaptation Framework for Advertising QA](https://arxiv.org/abs/2602.22584)
*Wenwei Li,Ming Xu,Tianle Xia,Lingxiang Hu,Yiding Sun,Linfang Shang,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: 论文提出了一种用于工业广告问答的增强型生成方法，显著降低了虚假内容（特别是伪造URL）产生的风险，提高了准确性和安全性，并已在实际生产环境中成功验证。


<details>
  <summary>Details</summary>
Motivation: 工业广告问答中生成虚假内容（如伪造URL）风险极高，可能造成经济损失、合规违规甚至法律风险。现有的检索增强生成方法难以应对工业知识的复杂性和多变性，因此亟需更可靠、更安全的解决方案。

Method: 作者提出了一个强化共适应框架，联合优化检索和生成环节。包含两大核心部分：（1）图结构感知检索（GraphRAG）——利用高引用知识子图进行多跳、领域特定证据选择；（2）基于证据约束的强化学习，采用Group Relative Policy Optimization（GRPO），通过多维奖励（涵盖真实性、风格合规、安全性、URL有效性）提升生成质量。

Result: 在内部广告问答数据集上，系统在专家评判的准确性、完整性和安全性等维度全面提升，虚假内容（幻觉）率降低72%。线上A/B测试显示点赞率提升28.6%、点踩率降46.2%，URL幻觉减少92.7%。该系统已上线半年，服务数百万次问答。

Conclusion: 新方法在降低工业广告问答系统幻觉、提升可靠性与用户满意度方面表现优异，不仅有效降低业务及合规风险，还在生产环境中实现了大规模落地。

Abstract: Industrial advertising question answering (QA) is a high-stakes task in which hallucinated content, particularly fabricated URLs, can lead to financial loss, compliance violations, and legal risk. Although Retrieval-Augmented Generation (RAG) is widely adopted, deploying it in production remains challenging because industrial knowledge is inherently relational, frequently updated, and insufficiently aligned with generation objectives. We propose a reinforced co-adaptation framework that jointly optimizes retrieval and generation through two components: (1) Graph-aware Retrieval (GraphRAG), which models entity-relation structure over a high-citation knowledge subgraph for multi-hop, domain-specific evidence selection; and (2) evidence-constrained reinforcement learning via Group Relative Policy Optimization (GRPO) with multi-dimensional rewards covering faithfulness, style compliance, safety, and URL validity. Experiments on an internal advertising QA dataset show consistent gains across expert-judged dimensions including accuracy, completeness, and safety, while reducing the hallucination rate by 72\%. A two-week online A/B test demonstrates a 28.6\% increase in like rate, a 46.2\% decrease in dislike rate, and a 92.7\% reduction in URL hallucination. The system has been running in production for over half a year and has served millions of QA interactions.

</details>


### [48] [dLLM: Simple Diffusion Language Modeling](https://arxiv.org/abs/2602.22661)
*Zhanhui Zhou,Lingjie Chen,Hanghang Tong,Dawn Song*

Main category: cs.CL

TL;DR: 本文提出了dLLM，一个统一扩散语言模型（DLM）核心组件的开源框架，方便模型训练、推理和评估，并支持定制和拓展。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型发展迅速，但其关键组件分散在不同的代码库或缺乏透明实现，增加了复现和拓展难度。领域加速发展背景下，急需一个标准化且灵活的统一框架。

Method: 提出dLLM框架，将DLM训练、推理和评估等核心功能统一，实现标准化流水线，支持复现、微调、部署与评估现有DLM（如LLaDA和Dream），并可从头构建小型DLM，对BERT风格编码器或自回归LM进行转换。

Result: dLLM不仅使用户能快速操作主流及自定义DLM，还提供小型DLM的训练脚本和模型参数（checkpoints），大幅推动相关研究复现、开发和普及。

Conclusion: dLLM作为DLM领域的标准化工具，提高了模型复现性、可扩展性及易用性，有望加速该领域的发展和创新。

Abstract: Although diffusion language models (DLMs) are evolving quickly, many recent models converge on a set of shared components. These components, however, are distributed across ad-hoc research codebases or lack transparent implementations, making them difficult to reproduce or extend. As the field accelerates, there is a clear need for a unified framework that standardizes these common components while remaining flexible enough to support new methods and architectures.
  To address this gap, we introduce dLLM, an open-source framework that unifies the core components of diffusion language modeling -- training, inference, and evaluation -- and makes them easy to customize for new designs. With dLLM, users can reproduce, finetune, deploy, and evaluate open-source large DLMs such as LLaDA and Dream through a standardized pipeline. The framework also provides minimal, reproducible recipes for building small DLMs from scratch with accessible compute, including converting any BERT-style encoder or autoregressive LM into a DLM. We also release the checkpoints of these small DLMs to make DLMs more accessible and accelerate future research.

</details>


### [49] [Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization](https://arxiv.org/abs/2602.22675)
*Qianben Chen,Tianrui Qin,King Zhu,Qiexiang Wang,Chengjun Yu,Shu Xu,Jiaqi Wu,Jiayu Zhang,Xinpeng Liu,Xin Gui,Jingyi Cao,Piaohong Wang,Dingfeng Shi,He Zhu,Tiannan Wang,Yuqing Wang,Maojia Song,Tianyu Zheng,Ge Zhang,Jian Yang,Jiaheng Liu,Minghao Liu,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出了一种高效通用的长任务推理代理框架，通过并行搜索提升效率，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有深度推理智能体多通过增加推理深度来提升性能，但这导致高推理成本和延迟，且难以在不同类型任务间泛化。

Method: 提出Search More, Think Less（SMTL）框架，通过并行证据检索替代串行推理，有效管理有限上下文预算；同时构建统一的数据合成流程，涵盖确定性问答和开放式研究任务；采用有监督微调和强化学习联合训练代理。

Result: 在BrowseComp、GAIA、Xbench和DeepResearch Bench等基准测试上，取得优异（甚至SOTA）成绩。如在BrowseComp任务上，推理步数减少70.7%，准确率提升。

Conclusion: SMTL实现了在高效性和泛化能力上的提升，为长任务推理智能体设定了新标准。

Abstract: Recent deep research agents primarily improve performance by scaling reasoning depth, but this leads to high inference cost and latency in search-intensive scenarios. Moreover, generalization across heterogeneous research settings remains challenging. In this work, we propose \emph{Search More, Think Less} (SMTL), a framework for long-horizon agentic search that targets both efficiency and generalization. SMTL replaces sequential reasoning with parallel evidence acquisition, enabling efficient context management under constrained context budgets. To support generalization across task types, we further introduce a unified data synthesis pipeline that constructs search tasks spanning both deterministic question answering and open-ended research scenarios with task appropriate evaluation metrics. We train an end-to-end agent using supervised fine-tuning and reinforcement learning, achieving strong and often state of the art performance across benchmarks including BrowseComp (48.6\%), GAIA (75.7\%), Xbench (82.0\%), and DeepResearch Bench (45.9\%). Compared to Mirothinker-v1.0, SMTL with maximum 100 interaction steps reduces the average number of reasoning steps on BrowseComp by 70.7\%, while improving accuracy.

</details>


### [50] [Enhancing Persuasive Dialogue Agents by Synthesizing Cross-Disciplinary Communication Strategies](https://arxiv.org/abs/2602.22696)
*Shinnosuke Nozue,Yuto Nakano,Yotaro Watanabe,Meguru Takasaki,Shoji Moriya,Reina Akama,Jun Suzuki*

Main category: cs.CL

TL;DR: 本文提出了一个基于跨学科理论的劝说对话系统设计框架，并在两个不同数据集上取得了良好效果，尤其在劝服初始意愿低的人群上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有劝说对话系统依赖于有限的预定义策略，难以应对真实场景中的复杂劝说需求。

Method: 结合社会心理学、行为经济学和传播理论，开发了新的劝说对话系统设计框架，并在Persuasion for Good和DailyPersuasion两个数据集上进行实验证明有效性和泛化能力。

Result: 新框架在提升劝说成功率方面表现优异，在两个数据集上均获得了显著改进，且对于初始意愿低的用户劝说效果尤为突出。

Conclusion: 跨学科的劝说对话系统设计框架提升了系统的实用性和普适性，特别能够有效劝说一开始意愿较低的用户，解决了劝说型对话系统领域中的重要难题。

Abstract: Current approaches to developing persuasive dialogue agents often rely on a limited set of predefined persuasive strategies that fail to capture the complexity of real-world interactions. We applied a cross-disciplinary approach to develop a framework for designing persuasive dialogue agents that draws on proven strategies from social psychology, behavioral economics, and communication theory. We validated our proposed framework through experiments on two distinct datasets: the Persuasion for Good dataset, which represents a specific in-domain scenario, and the DailyPersuasion dataset, which encompasses a wide range of scenarios. The proposed framework achieved strong results for both datasets and demonstrated notable improvement in the persuasion success rate as well as promising generalizability. Notably, the proposed framework also excelled at persuading individuals with initially low intent, which addresses a critical challenge for persuasive dialogue agents.

</details>


### [51] [Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue](https://arxiv.org/abs/2602.22697)
*Ning Gao,Wei Zhang,Yuqin Dai,Ling Shi,Ziyin Wang,Yujie Wang,Wei He,Jinpeng Wang,Chaozheng Wang*

Main category: cs.CL

TL;DR: 提出了一种结合同理心沟通与成本意识的任务型对话RL框架InteractCS-RL，优化对话智能体在实际场景下的多维表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型已成为高智能对话体实现的基础，但在实际任务型对话中，如何平衡用户体验（表现同理心）和保持成本高效是一大难题。现有方法无法有效体现这种平衡的策略性权衡。

Method: 提出User-centric Interaction Framework作为训练环境，允许智能体与高度还原真人的虚拟用户互动，探索多样化策略。引入成本感知多轮策略优化（CMPO），结合生成过程奖励和PID-Lagrangian成本控制器，推动策略在用户收益与成本之间动态平衡。

Result: 在实际企业场景和评价基准上，InteractCS-RL在三项评估维度上显著优于现有基线方案，并在多领域工具-智能体-用户交互中表现出较强稳健性。

Conclusion: InteractCS-RL能更好地在对话任务中优化同理心和成本权衡，拓宽了任务型对话智能体的性能上限。

Abstract: The rapid evolution of Large Language Models (LLMs) has accelerated the transition from conversational chatbots to general agents. However, effectively balancing empathetic communication with budget-aware decision-making remains an open challenge. Since existing methods fail to capture these complex strategic trade-offs, we propose InteractCS-RL, a framework that reframes task-oriented dialogue as a multi-granularity reinforcement learning process. Specifically, we first establish a User-centric Interaction Framework to provide a high-fidelity training gym, enabling agents to dynamically explore diverse strategies with persona-driven users. Then, we introduce Cost-aware Multi-turn Policy Optimization (CMPO) with a hybrid advantage estimation strategy. By integrating generative process credits and employing a PID-Lagrangian cost controller, CMPO effectively guides the policy to explore Pareto boundary between user reward and global cost constraints. Extensive experiments on customized real business scenarios demonstrate that InteractCS-RL significantly outperform other baselines across three evaluation dimensions. Further evaluation on tool-agent-user interaction benchmarks verify InteractCS-RL robustness across diverse domains.

</details>


### [52] [Tokenization, Fusion and Decoupling: Bridging the Granularity Mismatch Between Large Language Models and Knowledge Graphs](https://arxiv.org/abs/2602.22698)
*Siyue Su,Jian Yang,Bo Li,Guanglin Niu*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型（LLMs）用于知识图谱补全（KGC）的新框架KGT，有效缓解了LLM与知识图谱实体粒度不匹配的问题，在多项基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有利用LLM进行知识图谱补全的方法，主要受限于LLM处理文本的粒度（以token为单元）与知识图谱以实体为基本单元之间的差异。传统处理方式要么仅限于有限实体候选集，要么通过拼接或分解token来代表实体，难以兼顾文本语义与图结构完整性。

Method: KGT框架采用专属的实体token进行特殊分词，实现实体粒度的特征表达，并融合集成预训练的结构特征和文本特征，通过关系引导的门控机制统一编码，同时采用解耦预测结构，将语义推理与结构推理分别实现再融合，有效提升预测效率和覆盖范围。

Result: 实验证明KGT在多个知识图谱补全基准上均优于当前最先进的方法，显示出更强的泛化能力和准确性。

Conclusion: 提出的方法有效解决了LLM与知识图谱之间粒度不匹配难题，实现了实体级别的全空间高效预测，对KGC领域具有实际应用和理论创新价值。

Abstract: Leveraging Large Language Models (LLMs) for Knowledge Graph Completion (KGC) is promising but hindered by a fundamental granularity mismatch. LLMs operate on fragmented token sequences, whereas entities are the fundamental units in knowledge graphs (KGs) scenarios. Existing approaches typically constrain predictions to limited candidate sets or align entities with the LLM's vocabulary by pooling multiple tokens or decomposing entities into fixed-length token sequences, which fail to capture both the semantic meaning of the text and the structural integrity of the graph. To address this, we propose KGT, a novel framework that uses dedicated entity tokens to enable efficient, full-space prediction. Specifically, we first introduce specialized tokenization to construct feature representations at the level of dedicated entity tokens. We then fuse pre-trained structural and textual features into these unified embeddings via a relation-guided gating mechanism, avoiding training from scratch. Finally, we implement decoupled prediction by leveraging independent heads to separate and combine semantic and structural reasoning. Experimental results show that KGT consistently outperforms state-of-the-art methods across multiple benchmarks.

</details>


### [53] [Affine-Scaled Attention: Towards Flexible and Stable Transformer Attention](https://arxiv.org/abs/2602.23057)
*Jeongin Bae,Baeseong Park,Gunho Park,Minsub Kim,Joonhyung Lee,Junhee Yoo,Sunghyeon Woo,Jiwon Ryu,Se Jung Kwon,Dongsoo Lee*

Main category: cs.CL

TL;DR: 提出一种名为仿射缩放注意力（Affine-Scaled Attention）的新注意力机制，通过引入输入相关的缩放和偏置项，改善Transformer中softmax归一化注意力的局限性，提升训练稳定性与效果。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer中的softmax归一化限制了注意力分布的灵活性，可能导致注意力过于集中或不稳定，现有的改进方法（如attention sink或门控机制）只能有限度地调整注意力权重分配。作者旨在突破现有机制的局限，提升训练性能和下游任务表现。

Method: 提出仿射缩放注意力机制，在softmax归一化权重基础上，施加输入相关的缩放因子和偏置项，从而允许模型自主调节注意力分布的高宽及相对比例。在不同规模的语言模型预训练任务中进行实验评估。

Result: 实验证明，仿射缩放注意力相比标准softmax注意力和attention sink等基线，在训练稳定性、优化行为和下游任务效果方面均有明显提升。

Conclusion: 适度重新加权注意力输出是一种简单实用且有效的方式，可显著优化Transformer模型的注意力表现和整体性能。

Abstract: Transformer attention is typically implemented using softmax normalization, which enforces attention weights with unit sum normalization. While effective in many settings, this constraint can limit flexibility in controlling attention magnitudes and may contribute to overly concentrated or unstable attention patterns during training. Prior work has explored modifications such as attention sinks or gating mechanisms, but these approaches provide only limited or indirect control over attention reweighting. We propose Affine-Scaled Attention, a simple extension to standard attention that introduces input-dependent scaling and a corresponding bias term applied to softmax-normalized attention weights. This design relaxes the strict normalization constraint while maintaining aggregation of value representations, allowing the model to adjust both the relative distribution and the scale of attention in a controlled manner.
  We empirically evaluate Affine-Scaled Attention in large-scale language model pretraining across multiple model sizes. Experimental results show consistent improvements in training stability, optimization behavior, and downstream task performance compared to standard softmax attention and attention sink baselines. These findings suggest that modest reweighting of attention outputs provides a practical and effective way to improve attention behavior in Transformer models.

</details>


### [54] [Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs](https://arxiv.org/abs/2602.23136)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 尽管多模态大语言模型（LLM）能够处理语音和图像，但它们无法有效感知说话人的声音或物体的纹理。本文发现，主要瓶颈在于解码器的结构与训练方式。通过理论分析与实验证明，仅有训练目标相关的信息能被有效解码，且可通过特定目标训练提升相关模态属性的可访问性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态LLM虽能接收语音和图像数据，但在说话人身份、情感或视觉细节等模态特有信息的理解上表现不佳。作者认为问题并非在于编码器信息丢失，而可能源自解码器对不同模态信息的利用不充分，因此希望找到性能瓶颈并提出改进。

Method: （1）通过线性探测方法评估多模态LLM不同层对模态特有信息（如说话人身份、情感等）的保留情况；（2）实验性地删除模态特有方差信息，检测其对解码器损失的影响；（3）提出并验证“解码器不匹配”理论（即解码器只能利用与文本空间对齐的信息）；（4）通过控制变量实验及LoRA干预，测试不同训练目标对模态属性可访问性的影响。

Result: 实验证明，模态特有信息在编码器各层均有存留，但解码器只能用与文本一致的方向信息，其他信息被视为噪声，删去后反而提升性能。理论分析GMI界定了解码器可访问信息的上限，并通过五种涉及语音和视觉的模型与专门设计的对照实验加以实证。采用LoRA并以情感为目标训练后，情感属性可访问性提升7.5%，且对其它属性无负面影响。

Conclusion: 多模态LLM的信息瓶颈不在编码器，而在于解码器仅解读训练目标相关的内容。训练时引入额外目标能提升对应模态信息的可获取性。因此，欲提升多模态LLM对特有模态信息的理解，应调整训练目标而非仅优化架构。

Abstract: Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\times$ above chance in linear probes), yet removing 64--71% of modality-specific variance improves decoder loss. The decoder has no learned use for these directions; their presence is noise.
  We formalize this as a mismatched decoder problem: a decoder trained on text can only extract information along text-aligned directions. Accessible information is bounded by the Generalized Mutual Information (GMI), with degradation scaling with distributional distance and decoder sensitivity. The bound is a property of the decoder's scoring rule, not of any particular architecture; it applies whether non-text inputs arrive through a learned projection, a discrete codebook, or no explicit adapter at all. We validate this across five models spanning speech and vision. A controlled experiment (two Prismatic VLMs differing only in encoder text-alignment) confirms the bottleneck is the decoder's scoring rule, not the encoder or projection. A LoRA intervention demonstrates the fix: training with an emotion objective improves emotion accessibility ($+$7.5%) without affecting other attributes, confirming that the training objective determines what becomes accessible.

</details>


### [55] [Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models](https://arxiv.org/abs/2602.23197)
*Chungpa Lee,Jy-yong Sohn,Kangwook Lee*

Main category: cs.CL

TL;DR: 本文分析了大语言模型（LLM）微调对其in-context learning能力的影响，理论说明了全参数微调常损害模型的few-shot能力，而只微调Value矩阵能兼顾zero-shot提升和上下文学习保留；同时引入auxiliary few-shot loss虽能提升目标任务的上下文学习，却会损害对未见任务的泛化。上述结论通过实验证实。


<details>
  <summary>Details</summary>
Motivation: 目前LLM常通过少量示例的prompt进行in-context learning，具备快速适配下游任务的能力。然而，实践中LLM常需通过微调强化zero-shot能力，但微调可能破坏few-shot的泛化性。作者旨在系统分析微调如何影响上述能力，并探索有无兼顾两者的策略。

Method: 作者采用线性注意力模型为理论分析工具，分别考察全参数微调与只调整Value矩阵的效果，并研究加入few-shot辅助损失的影响。最后通过实验验证理论结论。

Result: 全量微调注意力参数会削弱LLM的few-shot in-context learning能力；只微调Value矩阵可以兼顾zero-shot效果和上下文学习能力；加入few-shot辅助损失，虽能增强目标任务的上下文学习效果，但会损害其它未见任务的泛化能力。实验数据支持这些理论发现。

Conclusion: 建议在追求zero-shot和few-shot能力兼顾时仅微调Value矩阵；使用auxiliary few-shot loss需权衡泛化性和针对性提升。微调策略需根据具体应用需求合理选择。

Abstract: Transformer-based large language models exhibit in-context learning, enabling adaptation to downstream tasks via few-shot prompting with demonstrations. In practice, such models are often fine-tuned to improve zero-shot performance on downstream tasks, allowing them to solve tasks without examples and thereby reducing inference costs. However, fine-tuning can degrade in-context learning, limiting the performance of fine-tuned models on tasks not seen during fine-tuning. Using linear attention models, we provide a theoretical analysis that characterizes how fine-tuning objectives modify attention parameters and identifies conditions under which this leads to degraded few-shot performance. We show that fine-tuning all attention parameters can harm in-context learning, whereas restricting updates to the value matrix improves zero-shot performance while preserving in-context learning. We further show that incorporating an auxiliary few-shot loss enhances in-context learning primarily on the target task, at the expense of degraded in-context learning ability on tasks not seen during fine-tuning. We empirically validate our theoretical results.

</details>


### [56] [Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning](https://arxiv.org/abs/2602.23351)
*Amita Kamath,Jack Hessel,Khyathi Chandu,Jena D. Hwang,Kai-Wei Chang,Ranjay Krishna*

Main category: cs.CL

TL;DR: 本文提出视觉-语言模型在推理能力上的不足主要源于训练数据的表达偏差，缺乏对空间、时间、否定和计数等推理能力的训练；文中用实验证明增加规模并不能弥补该短板，需有针对地引入隐含信息。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在多种应用中展现出强大能力，但其推理能力始终有限。作者认为这种现象与人们在描述视觉内容时常常省略隐含（默会）信息有关，即训练数据本身就没有要求推理能力。

Method: 作者分析了OpenCLIP、LLaVA-1.5和Molmo等主流VLM的训练数据，结合语用学理论，具体考察空间、时间、否定和计数四类推理。通过建立专门的基准测试，评估这些模型在上述推理任务的表现。

Result: （1）VLM在上述四类推理上表现较差；（2）扩大数据和模型规模，或多语言扩展，并不能自然提升此类推理能力；（3）如果有针对性地加入覆盖推理所需隐含信息的标注，则效果明显提升。

Conclusion: VLM推理短板主要源于训练数据的表达偏差，提升推理能力不能仅靠“规模”，而应精心策划和补充相关推理数据。

Abstract: The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., "at the game today!" is a more likely caption than "a photo of 37 people standing behind a field". We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [57] [SODA-CitrON: Static Object Data Association by Clustering Multi-Modal Sensor Detections Online](https://arxiv.org/abs/2602.22243)
*Jan Nausner,Kilian Wohlleben,Michael Hubner*

Main category: cs.RO

TL;DR: 本文提出了一种新型的静态物体数据关联方法（SODA-CitrON），用于实时融合和跟踪多模态异构传感器的静态目标，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的数据关联方法如JPDA主要适用于动态目标，对于间歇观测、存在异构不确定性的静态物体，其效果有限。静态目标缺乏运动信息，难以与杂波区分，因此需要新的数据关联方法。

Method: 提出SODA-CitrON方法，通过无监督机器学习在线聚类多源传感器的静态物体观测，同时估计物体位置并维持持久跟踪。该方法完全在线，支持时间无关和多传感器输入，且复杂度对观测数为对数线性，结果可解释性强。

Result: 在不同蒙特卡洛仿真场景下与现有的贝叶斯滤波、DBSTREAM聚类、JPDA等方法对比，SODA-CitrON在F1分数、位置RMSE、MOTP和MOTA等指标上始终取得更优表现。

Conclusion: SODA-CitrON在静态目标映射应用中有效提升了多传感器数据融合与跟踪的准确性和效率，相较于业界主流方法具备显著优势。

Abstract: The online fusion and tracking of static objects from heterogeneous sensor detections is a fundamental problem in robotics, autonomous systems, and environmental mapping. Although classical data association approaches such as JPDA are well suited for dynamic targets, they are less effective for static objects observed intermittently and with heterogeneous uncertainties, where motion models provide minimal discriminative with respect to clutter. In this paper, we propose a novel method for static object data association by clustering multi-modal sensor detections online (SODA-CitrON), while simultaneously estimating positions and maintaining persistent tracks for an unknown number of objects. The proposed unsupervised machine learning approach operates in a fully online manner and handles temporally uncorrelated and multi-sensor measurements. Additionally, it has a worst-case loglinear complexity in the number of sensor detections while providing full output explainability. We evaluate the proposed approach in different Monte Carlo simulation scenarios and compare it against state-of-the-art methods, including Bayesian filtering, DBSTREAM clustering, and JPDA. The results demonstrate that SODA-CitrON consistently outperforms the compared methods in terms of F1 score, position RMSE, MOTP, and MOTA in the static object mapping scenarios studied.

</details>


### [58] [Detection and Recognition: A Pairwise Interaction Framework for Mobile Service Robots](https://arxiv.org/abs/2602.22346)
*Mengyu Liang,Sarah Gillet Schlegel,Iolanda Leite*

Main category: cs.RO

TL;DR: 本文提出了一种针对移动服务机器人（如割草机和清洁机器人）的人际互动感知方法，通过先检测成对的人物互动，再进行粗粒度行为分类，实现了高效、低计算量的社会感知，为机器人安全、社会化导航提供了基础。方法在多个数据集上展现出较好泛化能力和较低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 服务机器人在有人环境中须感知人际互动以安全、合理完成导航任务。现有方法不是计算量大、模型庞大，就是受限于对骨骼信息的依赖，不适合移动平台和户外环境。本工作旨在提出一种资源消耗少、能满足机器人目标的感知方案。

Method: 提出两阶段框架：第一步用轻量级的几何与运动线索检测互动人物对，第二步利用关系网络对互动类型进行粗分类。主要关注配对人物而非整体群体活动，并比基于外观的方法更高效。

Result: 在JRDB数据集上，提出的方法在较低计算资源与模型体积下获得了足够的互动识别准确率。对Collective Activity数据集和割草机场景新数据集进行实验，表现出方法的泛化能力与实用性。

Conclusion: 成对（pairwise）的几何和运动信息是移动服务机器人高效、实用的人际互动感知基础，适合集成到未来移动机器人导航系统中。代码即将公开。

Abstract: Autonomous mobile service robots, like lawnmowers or cleaning robots, operating in human-populated environments need to reason about local human-human interactions to support safe and socially aware navigation while fulfilling their tasks. For such robots, interaction understanding is not primarily a fine-grained recognition problem, but a perception problem under limited sensing quality and computational resources. Many existing approaches focus on holistic group activity recognition, which often requires complex and large models which may not be necessary for mobile service robots. Others use pairwise interaction methods which commonly rely on skeletal representations but their use in outdoor environments remains challenging. In this work, we argue that pairwise human interaction constitute a minimal yet sufficient perceptual unit for robot-centric social understanding. We study the problem of identifying interacting person pairs and classifying coarse-grained interaction behaviors sufficient for downstream group-level reasoning and service robot decision-making. To this end, we adopt a two-stage framework in which candidate interacting pairs are first identified based on lightweight geometric and motion cues, and interaction types are subsequently classified using a relation network. We evaluate the proposed approach on the JRDB dataset, where it achieves sufficient accuracy with reduced computational cost and model size compared to appearance-based methods. Additional experiments on the Collective Activity Dataset and zero shot test on a lawnmower-collected dataset further illustrate the generality of the proposed framework. These results suggest that pairwise geometric and motion cues provide a practical basis for interaction perception on mobile service robot providing a promising method for integration into mobile robot navigation stacks in future work. Code will be released soon

</details>


### [59] [Hierarchical Trajectory Planning of Floating-Base Multi-Link Robot for Maneuvering in Confined Environments](https://arxiv.org/abs/2602.22459)
*Yicheng Chen,Jinjie Li,Haokun Liu,Zicheng Luo,Kotaro Kaneko,Moju Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种分层轨迹规划框架，实现了浮动基多连杆机器人的高效自主运动，能从点云数据直接生成连续、无碰撞且动态可行的轨迹，并在真实机器人上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 浮动基多连杆机器人能在狭小空间灵活作业，如自主检查和搜救，但其轨迹规划因空间维度高、约束多及复杂避障要求，长期未得到有效解决。

Method: 提出分层轨迹规划框架：首先将规划问题分解为若干全局锚点，通过根部连杆导引与关节柔性结合设计；然后为每段使用带可微约束与目标函数的局部轨迹优化器，保证动力学和运动学可行；此外，系统能直接处理点云，无需手工障碍建模，并行优化每段规划。

Result: 大量仿真和真实环境实验显示，该框架能让多连杆飞行机器人实现刚性机器人无法完成的复杂机动运动，能直接从点云生成连续、无碰撞且动态可行的轨迹。

Conclusion: 本工作实现了首个能从原始点云数据出发，不依赖人工障碍建模，并在真实机器人上验证有效的浮动基多连杆机器人轨迹规划系统，对相关应用和后续研究有重要意义。

Abstract: Floating-base multi-link robots can change their shape during flight, making them well-suited for applications in confined environments such as autonomous inspection and search and rescue. However, trajectory planning for such systems remains an open challenge because the problem lies in a high-dimensional, constraint-rich space where collision avoidance must be addressed together with kinematic limits and dynamic feasibility. This work introduces a hierarchical trajectory planning framework that integrates global guidance with configuration-aware local optimization. First, we exploit the dual nature of these robots - the root link as a rigid body for guidance and the articulated joints for flexibility - to generate global anchor states that decompose the planning problem into tractable segments. Second, we design a local trajectory planner that optimizes each segment in parallel with differentiable objectives and constraints, systematically enforcing kinematic feasibility and maintaining dynamic feasibility by avoiding control singularities. Third, we implement a complete system that directly processes point-cloud data, eliminating the need for handcrafted obstacle models. Extensive simulations and real-world experiments confirm that this framework enables an articulated aerial robot to exploit its morphology for maneuvering that rigid robots cannot achieve. To the best of our knowledge, this is the first planning framework for floating-base multi-link robots that has been demonstrated on a real robot to generate continuous, collision-free, and dynamically feasible trajectories directly from raw point-cloud inputs, without relying on handcrafted obstacle models.

</details>


### [60] [EgoAVFlow: Robot Policy Learning with Active Vision from Human Egocentric Videos via 3D Flow](https://arxiv.org/abs/2602.22461)
*Daesol Cho,Youngseok Jang,Danfei Xu,Sehoon Ha*

Main category: cs.RO

TL;DR: EgoAVFlow是一种通过模拟人类视角视频，让机器人同时学习操作和主动视觉控制，能更好地保持关键可见性，实现无需机器人示范的泛化操控。


<details>
  <summary>Details</summary>
Motivation: 人类头戴相机拍摄的大量操作视频为机器人学习提供了丰富资源，但人类在执行任务时的视角经常并不适合机器人，因为人类依赖一些机器人没有的先验。传统的做法很难直接将人类视频的视角策略迁移到机器人上，会导致机器人在操作中无法保持对关键任务区域的可见性。

Method: 提出EgoAVFlow，用统一的三维流（3D flow）表征同时学习操作动作和主动调整相机视角。通过扩散模型预测机器人的动作、未来的三维流和相机轨迹；在测试时，通过带有可见性奖励的去噪步骤自适应调整相机视角，无需机器人实际示范来训练。

Result: 在现实环境下、不断主动改变视角的实验中，EgoAVFlow在保持任务关键区域可见性、操控任务成功率等方面，性能均优于现有的人类演示类方法。

Conclusion: EgoAVFlow为机器人从人类自视角视频同时学习操控与主动视觉提供了一种有效手段，无需机器人示范，能更好地维持关键可见性，推动了人类演示泛化到机器人自主学能力的发展。

Abstract: Egocentric human videos provide a scalable source of manipulation demonstrations; however, deploying them on robots requires active viewpoint control to maintain task-critical visibility, which human viewpoint imitation often fails to provide due to human-specific priors. We propose EgoAVFlow, which learns manipulation and active vision from egocentric videos through a shared 3D flow representation that supports geometric visibility reasoning and transfers without robot demonstrations. EgoAVFlow uses diffusion models to predict robot actions, future 3D flow, and camera trajectories, and refines viewpoints at test time with reward-maximizing denoising under a visibility-aware reward computed from predicted motion and scene geometry. Real-world experiments under actively changing viewpoints show that EgoAVFlow consistently outperforms prior human-demo-based baselines, demonstrating effective visibility maintenance and robust manipulation without robot demonstrations.

</details>


### [61] [When to Act, Ask, or Learn: Uncertainty-Aware Policy Steering](https://arxiv.org/abs/2602.22474)
*Jessie Yuan,Yilin Wu,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 该论文提出了UPS（Uncertainty-aware Policy Steering）框架，用于提升机器人政策指引的可靠性和效率，能根据不确定性自动选择高置信度执行、澄清任务或请求干预，并通过校准和持续学习减少用户干预。


<details>
  <summary>Details</summary>
Motivation: 传统使用视觉-语言模型(VLM)作为机器人行为校验器时，通常假设其判断充分且可靠，但实际VLM会因置信度过高导致策略执行出错，尤其是在任务语义或底层行为不确定的场景下。为此需要更好地处理VLM判断不确定性，提高策略适应性与安全性。

Method: 提出了UPS框架，联合分析任务的语义不确定性和底层动作的可行性。UPS以配准预测(conformal prediction)对VLM和预训练策略联合校准，为策略选择提供统计保证。系统根据不同不确定性可选择：1) 执行高置信度动作，2) 通过自然语言澄清任务，3) 请求干预修正动作。此外，部署期间收集干预数据后通过残差学习持续增强预训练策略。

Result: 在仿真和实际机器人硬件实验中，UPS能有效区分置信、模糊和无能场景。相比未校准基线和以前需要大量人工/机器人把关的连续学习方法，UPS减少了昂贵的用户干预。

Conclusion: UPS框架通过引入不确定性感知和检验机制，提升了机器人策略的安全性和自主适应性，是提升机器人实际部署能力的重要方法。

Abstract: Policy steering is an emerging way to adapt robot behaviors at deployment-time: a learned verifier analyzes low-level action samples proposed by a pre-trained policy (e.g., diffusion policy) and selects only those aligned with the task. While Vision-Language Models (VLMs) are promising general-purpose verifiers due to their reasoning capabilities, existing frameworks often assume these models are well-calibrated. In practice, the overconfident judgment from VLM can degrade the steering performance under both high-level semantic uncertainty in task specifications and low-level action uncertainty or incapability of the pre-trained policy. We propose uncertainty-aware policy steering (UPS), a framework that jointly reasons about semantic task uncertainty and low-level action feasibility, and selects an uncertainty resolution strategy: execute a high-confidence action, clarify task ambiguity via natural language queries, or ask for action interventions to correct the low-level policy when it is deemed incapable at the task. We leverage conformal prediction to calibrate the composition of the VLM and the pre-trained base policy, providing statistical assurances that the verifier selects the correct strategy. After collecting interventions during deployment, we employ residual learning to improve the capability of the pre-trained policy, enabling the system to learn continually but with minimal expensive human feedback. We demonstrate our framework through experiments in simulation and on hardware, showing that UPS can disentangle confident, ambiguous, and incapable scenarios and minimizes expensive user interventions compared to uncalibrated baselines and prior human- or robot-gated continual learning approaches. Videos can be found at https://jessie-yuan.github.io/ups/

</details>


### [62] [SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation](https://arxiv.org/abs/2602.22514)
*Xinyu Tan,Ningwei Bai,Harry Gardener,Zhengyang Zhong,Luoyu Zhang,Liuhaichen Yang,Zhekai Duan,Monkgogi Galeitsiwe,Zezhi Tang*

Main category: cs.RO

TL;DR: 本文提出了首个手语驱动的视觉-语言-动作（VLA）框架，用于直观、包容的人机交互，实现无需gloss注释即可将手语手势直接映射为语义指令，大幅降低注释成本和提升交互自然性。


<details>
  <summary>Details</summary>
Motivation: 当前的人机手语交互方法多依赖于gloss注释作为中间监督，不仅增加了标注成本，还伴随语义损失。作者希望开发一种能直接将手语手势转为语义指令，降低成本、提升包容性和自然度的新途径。

Method: 提出了一套字母级实时手指拼写界面，能够将连续手势流经过几何归一化、时间平滑和词汇优化等步骤，转换为一致的语言指令。该系统未来可集成基于transformer的gloss-free手语模型，支持扩展至词级与句子级的语义理解。

Result: 实验结果表明，该系统能够在多样化交互场景下，将手语指令精准转化为机器人动作，验证了其稳健性与实时性。

Conclusion: 该框架大幅提升了可访问性、拓展性和多模态智能在人机互动中的应用潜力，是发展新型包容式机器人控制的重要方向。

Abstract: We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions. This design reduces annotation cost and avoids the information loss introduced by gloss representations, enabling more natural and scalable multimodal interaction.
  In this work, we focus on a real-time alphabet-level finger-spelling interface that provides a robust and low-latency communication channel for robotic control. Compared with large-scale continuous sign language recognition, alphabet-level interaction offers improved reliability, interpretability, and deployment feasibility in safety-critical embodied environments. The proposed pipeline transforms continuous gesture streams into coherent language commands through geometric normalization, temporal smoothing, and lexical refinement, ensuring stable and consistent interaction.
  Furthermore, the framework is designed to support future integration of transformer-based gloss-free sign language models, enabling scalable word-level and sentence-level semantic understanding. Experimental results demonstrate the effectiveness of the proposed system in grounding sign-derived instructions into precise robotic actions under diverse interaction scenarios. These results highlight the potential of the framework to advance accessible, scalable, and multimodal embodied intelligence.

</details>


### [63] [Metamorphic Testing of Vision-Language Action-Enabled Robots](https://arxiv.org/abs/2602.22579)
*Pablo Valle,Sergio Segura,Shaukat Ali,Aitor Arrieta*

Main category: cs.RO

TL;DR: 该论文提出利用变异测试（MT）方法，解决Vision-Language-Action（VLA）多模态机器人控制任务中的测试判定难题，并通过实证研究证明其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: VLA机器人系统在需要根据视觉和指令执行任务时，难以为每个指令定义对应测试判定标准（test oracle），现有判定方法难以通用且无法全面评价任务执行质量，迫切需要更有效的测试方法。

Method: 作者引入了变异测试（MT）框架，提出了两种变异关系模式和五种变异关系，通过改变测试输入来评估机器人运动轨迹的变化，从而检测VLA模型中的潜在失效。

Result: 在包括五个VLA模型、两个仿真机器人和四种任务的实证实验中，MT方法可自动检测多样化的失效类型（包括但不限于任务未完成），有效缓解了测试判定难题。

Conclusion: 变异测试不仅可以自动发现机器人控制过程中的多类失效，而且所提出的变异关系具有良好的通用性，可广泛应用于不同的模型、机器人和任务场景，即使没有明确的测试判定标准也可适用。

Abstract: Vision-Language-Action (VLA) models are multimodal robotic task controllers that, given an instruction and visual inputs, produce a sequence of low-level control actions (or motor commands) enabling a robot to execute the requested task in the physical environment. These systems face the test oracle problem from multiple perspectives. On the one hand, a test oracle must be defined for each instruction prompt, which is a complex and non-generalizable approach. On the other hand, current state-of-the-art oracles typically capture symbolic representations of the world (e.g., robot and object states), enabling the correctness evaluation of a task, but fail to assess other critical aspects, such as the quality with which VLA-enabled robots perform a task. In this paper, we explore whether Metamorphic Testing (MT) can alleviate the test oracle problem in this context. To do so, we propose two metamorphic relation patterns and five metamorphic relations to assess whether changes to the test inputs impact the original trajectory of the VLA-enabled robots. An empirical study involving five VLA models, two simulated robots, and four robotic tasks shows that MT can effectively alleviate the test oracle problem by automatically detecting diverse types of failures, including, but not limited to, uncompleted tasks. More importantly, the proposed MRs are generalizable, making the proposed approach applicable across different VLA models, robots, and tasks, even in the absence of test oracles.

</details>


### [64] [Designing Robots for Families: In-Situ Prototyping for Contextual Reminders on Family Routines](https://arxiv.org/abs/2602.22628)
*Michael F. Xu,Enhui Zhao,Yawen Zhang,Joseph E. Michaelis,Sarah Sebo,Bilge Mutlu*

Main category: cs.RO

TL;DR: 本文探讨了如何通过家庭日常惯例为入口，将机器人更好地整合到家庭生活中，并通过与十个家庭的共创、原型部署与实地研究，总结了实际应用中的机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然机器人逐渐进入家庭，但真正成功地融入家庭生活、实现可持续角色依然面临困难。本文旨在通过分析和设计家庭惯例中的机器人交互，寻找机器人在家庭场景下的实际应用方式。

Method: 研究团队与十个家庭合作，共同设计机器人在特定家庭惯例中的行为与交互，依据时间、参与者、场所等环境因素制定辅助计划。随后，研究者开发并部署了可移动社交机器人，并进行了为期四天的入户用户研究，通过观察和访谈收集数据。

Result: 大部分家庭对机器人提醒功能表示欢迎，尤其父母认可机器人分担了部分提醒职责。但研究也揭示了机器人在家庭环境中遇到的诸如时间安排、权威性与家庭动态等多重张力。

Conclusion: 作者提出了机器人辅助情境任务设计的建议，并讨论了家庭场景下机器人的设计需关注的更广泛问题，指出单一任务解决不足以实现机器人家庭整合的复杂性。

Abstract: Robots are increasingly entering the daily lives of families, yet their successful integration into domestic life remains a challenge. We explore family routines as a critical entry point for understanding how robots might find a sustainable role in everyday family settings. Together with each of the ten families, we co-designed robot interactions and behaviors, and a plan for the robot to support their chosen routines, accounting for contextual factors such as timing, participants, locations, and the activities in the environment. We then designed, prototyped, and deployed a mobile social robot as a four-day, in-home user study. Families welcomed the robot's reminders, with parents especially appreciating the offloading of some reminding tasks. At the same time, interviews revealed tensions around timing, authority, and family dynamics, highlighting the complexity of integrating robots into households beyond the immediate task of reminders. Based on these insights, we offer design implications for robot-facilitated contextual reminders and discuss broader considerations for designing robots for family settings.

</details>


### [65] [Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline](https://arxiv.org/abs/2602.22663)
*Wenxuan Song,Jiayi Chen,Xiaoquan Sun,Huashuo Lei,Yikai Qin,Wei Zhao,Pengxiang Ding,Han Zhao,Tongxin Wang,Pengxu Hou,Zhide Zhong,Haodong Yan,Donglin Wang,Jun Ma,Haoang Li*

Main category: cs.RO

TL;DR: 本文提出了CEBench这一全新视觉-语言-动作（VLA）基准及改进的轻量级VLA基线模型LLaVA-VLA，从多种体态、多视角和真实与仿真场景上，提升了VLA机器人实用性。


<details>
  <summary>Details</summary>
Motivation: 目前VLA模型虽具备通用性和强大功能，但过于庞大的参数规模和昂贵的预训练资源难以实际应用，且适用性受限。因此，作者希望通过提升基准和提出改进模型，使其更易于实际部署和应用。

Method: 1）提出CEBench基准，涵盖多种仿真与真实机器人任务，收集大量专家轨迹数据；2）分析VLA实用性的关键问题并给出解决方案；3）提出LLaVA-VLA模型，集成紧凑的多模态主干、感知与动作整合，采用高效的两阶段训练，无需庞大预训练，统一导航和操作动作空间。

Result: LLaVA-VLA在CEBench上的实验显示优良的泛化和多用性；在真实移动操作机器人上首次实现端到端的VLA移动操作，验证了其实用性和有效性。

Conclusion: LLaVA-VLA显著提升了VLA模型的实际部署可行性，为通用智能机器人提供了轻量级端到端方案，且相关资源将开源，推动领域发展和研究复现。

Abstract: Vision-Language-Action (VLA) models have emerged as a generalist robotic agent. However, existing VLAs are hindered by excessive parameter scales, prohibitive pre-training requirements, and limited applicability to diverse embodiments. To improve the practicality of VLAs, we propose a comprehensive benchmark and an improved baseline. First, we propose CEBench, a new benchmark spanning diverse embodiments in both simulation and the real world with consideration of domain randomization. We collect 14.4k simulated trajectories and 1.6k real-world expert-curated trajectories to support training on CEBench. Second, using CEBench as our testbed, we study three critical aspects of VLAs' practicality and offer several key findings. Informed by these findings, we introduce LLaVA-VLA, a lightweight yet powerful VLA designed for practical deployment on consumer-grade GPUs. Architecturally, it integrates a compact VLM backbone with multi-view perception, proprioceptive tokenization, and action chunking. To eliminate reliance on costly pre-training, LLaVA-VLA adopts a two-stage training paradigm including post-training and fine-tuning. Furthermore, LLaVA-VLA extends the action space to unify navigation and manipulation. Experiments across embodiments demonstrate the capabilities of generalization and versatility of LLaVA-VLA , while real-world mobile manipulation experiments establish it as the first end-to-end VLA model for mobile manipulation. We will open-source all datasets, codes, and checkpoints upon acceptance to foster reproducibility and future research.

</details>


### [66] [Does the testing environment matter? Carsickness across on-road, test-track, and driving simulator conditions](https://arxiv.org/abs/2602.22671)
*Georgios Papaioannou,Barys Shyrokau*

Main category: cs.RO

TL;DR: 本文比较了在真实道路、测试跑道和基于运动的驾驶模拟器环境下汽车晕车发生的差异，发现模拟器难以完全复制易致晕车的低频运动，导致其晕车得分明显低于其他两种环境。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆普及，汽车晕车问题受到关注，但不同研究环境下缺乏标准化，导致研究结果难以横向比较。因此，作者希望比较多种环境下晕车反应及其可比性，推动标准化进程。

Method: 招募28名参与者，在驾驶模拟器内执行分心任务，同时使用Misery量表在线报告晕车感受，并在事后填写晕车评估问卷。还评估了已知影响晕车的心理因素。对比分析了加速度、客观数据与主观晕车评分，并与之前在实际道路及测试跑道获得的数据对比。

Result: 在驾驶模拟器环境下，晕车评分明显低于真实道路和测试跑道，两者之间呈现统计学显著差异。主要原因是模拟器难以复现引起晕车的关键低频(<0.5 Hz)运动成分。

Conclusion: 目前基于运动的驾驶模拟器在研究晕车发生机制及防治时，仍存在环境效度局限。为提升模拟器研究的有效性，需要改进其硬件以更好地再现低频诱发运动特征。

Abstract: Carsickness has gained significant attention with the rise of automated vehicles, prompting extensive research across on-road, test-track, and driving simulator environments to understand its occurrence and develop mitigation strategies. However, the lack of carsickness standardization complicates comparisons across studies and environments. Previous works demonstrate measurement validity between two setups at most (e.g., on-road vs. driving simulator), leaving gaps in multi-environment comparisons. This study investigates the recreation of an on-road motion sickness exposure - previously replicated on a test track - using a motion-based driving simulator. Twenty-eight participants performed an eyes-off-road non-driving task while reporting motion sickness using the Misery Scale during the experiment and the Motion Sickness Assessment Questionnaire afterward. Psychological factors known to influence motion sickness were also assessed. The results present subjective and objective measurements for motion sickness across the considered environments. In this paper, acceleration measurements, objective metrics and subjective motion sickness ratings across environments are compared, highlighting key differences in sickness occurrence for simulator-based research validity. Significantly lower motion sickness scores are reported in the simulator compared to on-road and test-track conditions, due to its limited working envelope to reproduce low-frequency (<0.5 Hz) motions, which are the most provocative for motion sickness.

</details>


### [67] [SCOPE: Skeleton Graph-Based Computation-Efficient Framework for Autonomous UAV Exploration](https://arxiv.org/abs/2602.22707)
*Kai Li,Shengtao Zheng,Linkun Xiu,Yuze Sheng,Xiao-Ping Zhang,Dongyue Huang,Xinlei Chen*

Main category: cs.RO

TL;DR: 本论文提出了SCOPE框架，通过实时构建骨架图和引入隐式未知区域分析，实现了高效的自主探测规划，在降低计算消耗的同时提升路径平滑性。


<details>
  <summary>Details</summary>
Motivation: 现有自主探测方法依赖频繁的全局优化，导致高计算延迟和路径震荡问题，尤其是在算力有限的设备上表现不佳。研究动机在于如何提升探测效率并降低资源消耗。

Method: SCOPE框架包括两个创新点：一是增量构建的实时骨架图，二是进行隐式未知区域分析。其规划层采用分层按需策略，本地规划器高频生成平滑路径，全局规划器只在必要时优化访问顺序，从而兼顾性能和效率。

Result: 仿真对比实验表明，SCOPE在探测效率方面与最先进全局规划器持平，但平均减少了86.9%的计算成本。真实机器人实验也证明了该系统在实际环境中的高鲁棒性和低延迟。

Conclusion: SCOPE有效缓解了自主探测中的高延迟和计算压力问题，具有良好的实时性和实用性，适用于资源受限的移动机器人平台。

Abstract: Autonomous exploration in unknown environments is key for mobile robots, helping them perceive, map, and make decisions in complex areas. However, current methods often rely on frequent global optimization, suffering from high computational latency and trajectory oscillation, especially on resource-constrained edge devices. To address these limitations, we propose SCOPE, a novel framework that incrementally constructs a real-time skeletal graph and introduces Implicit Unknown Region Analysis for efficient spatial reasoning. The planning layer adopts a hierarchical on-demand strategy: the Proximal Planner generates smooth, high-frequency local trajectories, while the Region-Sequence Planner is activated only when necessary to optimize global visitation order. Comparative evaluations in simulation demonstrate that SCOPE achieves competitive exploration performance comparable to state-of-the-art global planners, while reducing computational cost by an average of 86.9%. Real-world experiments further validate the system's robustness and low latency in practical scenarios.

</details>


### [68] [Robust Helicopter Ship Deck Landing With Guaranteed Timing Using Shrinking-Horizon Model Predictive Control](https://arxiv.org/abs/2602.22714)
*Philipp Schitz,Paolo Mercorelli,Johann C. Dauer*

Main category: cs.RO

TL;DR: 本文提出了一种基于缩短视界模型预测控制（SHMPC）的高效自主直升机在移动船甲板上着陆算法，并在仿真中验证了其高精度和强扰动抑制能力。


<details>
  <summary>Details</summary>
Motivation: 自主直升机在移动平台（如船舶甲板）上安全高效着陆极具挑战性，尤其是在强风等复杂扰动条件下，因此需要一种既能保证安全又能实时应对扰动的控制方法。

Method: 首先推导出能反映直升机非线性动力学特性的规划模型；结合SHMPC和降落阶段控制器，实现对指定机动时间和着陆窗口的保证；设计带有扰动反馈的辅助控制器以提升扰动抑制性能；整个方案依赖于初始优化问题的可行性。

Result: 仿真结果表明，在强风条件下，该方法能够以毫秒级的最大计算时间完成全部着陆任务，满足时序和操作约束，且着陆精度高。

Conclusion: 所提方法可在高扰动环境下实现高效、安全、快速的自主直升机移动平台着陆，并兼顾精度与实时性。

Abstract: We present a runtime efficient algorithm for autonomous helicopter landings on moving ship decks based on Shrinking-Horizon Model Predictive Control (SHMPC). First, a suitable planning model capturing the relevant aspects of the full nonlinear helicopter dynamics is derived. Next, we use the SHMPC together with a touchdown controller stage to ensure a pre-specified maneuver time and an associated landing time window despite the presence of disturbances. A high disturbance rejection performance is achieved by designing an ancillary controller with disturbance feedback. Thus, given a target position and time, a safe landing with suitable terminal conditions is be guaranteed if the initial optimization problem is feasible. The efficacy of our approach is shown in simulation where all maneuvers achieve a high landing precision in strong winds while satisfying timing and operational constraints with maximum computation times in the millisecond range.

</details>


### [69] [Sapling-NeRF: Geo-Localised Sapling Reconstruction in Forests for Ecological Monitoring](https://arxiv.org/abs/2602.22731)
*Miguel Ángel Muñoz-Bañón,Nived Chebrolu,Sruthi M. Krishna Moorthy,Yifu Tao,Fernando Torres,Roberto Salguero-Gómez,Maurice Fallon*

Main category: cs.RO

TL;DR: 该论文提出了一种结合NeRF、激光SLAM和GNSS的三层融合管道，用于实现幼树的可重复、可地理定位的生态监测，相较于传统方法，提高了结构性和定量分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有3D感知技术在捕捉幼树细枝结构和密集叶片方面存在困难，且尺度不一致，难以长期、定量地监测森林再生状况。隐式3D重建方法虽有潜力，但无法准确恢复场景真实比例和地理位置。因此，亟需一种能实现尺度统一且地理定位准确的幼树结构量化方法。

Method: 作者提出了一体化三层信息融合流程：(1) 利用GNSS实现地球坐标系下的粗定位；(2) 借助LiDAR SLAM实现厘米级的局部定位和重建；(3) 通过NeRF进行对象中心的稠密幼树3D结构重建。此方法可实现对幼树结构的高分辨率捕捉，并确保长期可重复的地理配准。

Result: 在英国和芬兰的森林样地实验结果表明，该管道能比传统TLS方法更准确获取幼树树干高度、分枝结构和叶/木比例，同时可对0.5-2米高的幼树进行精准的骨架和叶片分布测量。

Conclusion: 该融合方法能为生态学家提供更丰富、结构化和可量化的数据，有助于深入分析和长期监测森林动态，对幼树及森林健康评估具有重要意义。

Abstract: Saplings are key indicators of forest regeneration and overall forest health. However, their fine-scale architectural traits are difficult to capture with existing 3D sensing methods, which make quantitative evaluation difficult. Terrestrial Laser Scanners (TLS), Mobile Laser Scanners (MLS), or traditional photogrammetry approaches poorly reconstruct thin branches, dense foliage, and lack the scale consistency needed for long-term monitoring. Implicit 3D reconstruction methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) are promising alternatives, but cannot recover the true scale of a scene and lack any means to be accurately geo-localised. In this paper, we present a pipeline which fuses NeRF, LiDAR SLAM, and GNSS to enable repeatable, geo-localised ecological monitoring of saplings. Our system proposes a three-level representation: (i) coarse Earth-frame localisation using GNSS, (ii) LiDAR-based SLAM for centimetre-accurate localisation and reconstruction, and (iii) NeRF-derived object-centric dense reconstruction of individual saplings. This approach enables repeatable quantitative evaluation and long-term monitoring of sapling traits. Our experiments in forest plots in Wytham Woods (Oxford, UK) and Evo (Finland) show that stem height, branching patterns, and leaf-to-wood ratios can be captured with increased accuracy as compared to TLS. We demonstrate that accurate stem skeletons and leaf distributions can be measured for saplings with heights between 0.5m and 2m in situ, giving ecologists access to richer structural and quantitative data for analysing forest dynamics.

</details>


### [70] [Pixel2Catch: Multi-Agent Sim-to-Real Transfer for Agile Manipulation with a Single RGB Camera](https://arxiv.org/abs/2602.22733)
*Seongyong Kim,Junhyeon Cho,Kang-Won Lee,Soo-Chul Lim*

Main category: cs.RO

TL;DR: 本文提出了一种基于单帧RGB图像像素级视觉信息来识别抛掷物体运动的新方法，并通过异构多智能体强化学习框架，使高自由度机器人能够学习并成功在现实中抓住抛掷物体。


<details>
  <summary>Details</summary>
Motivation: 传统机械臂抓取抛掷物体需要精确估计物体三维位置，具有技术难度和计算开销。本研究旨在通过更高效的视觉感知和新的学习框架，提升高自由度机器人在实际复杂动态任务中的表现。

Method: 提出以单帧RGB图像像素级视觉特征识别物体运动，避免三维坐标估计；并设计将机械臂和多指手分设为独立智能体、各自依据特定观测和奖励信号协作学习的多智能体强化学习系统。训练在仿真完成后无缝迁移至真实环境。

Result: 实验表明，该方法训练出的策略不仅能在仿真中有效完成物体接球任务，还能成功迁移到现实世界机器人系统进行抓取。

Conclusion: 无需三维位置显式估计，只利用单帧视觉信息及多智能体强化学习框架，即可显著提高高自由度机器人接球能力，相关策略具备良好的现实应用潜力。

Abstract: To catch a thrown object, a robot must be able to perceive the object's motion and generate control actions in a timely manner. Rather than explicitly estimating the object's 3D position, this work focuses on a novel approach that recognizes object motion using pixel-level visual information extracted from a single RGB image. Such visual cues capture changes in the object's position and scale, allowing the policy to reason about the object's motion. Furthermore, to achieve stable learning in a high-DoF system composed of a robot arm equipped with a multi-fingered hand, we design a heterogeneous multi-agent reinforcement learning framework that defines the arm and hand as independent agents with distinct roles. Each agent is trained cooperatively using role-specific observations and rewards, and the learned policies are successfully transferred from simulation to the real world.

</details>


### [71] [Unleashing the Potential of Diffusion Models for End-to-End Autonomous Driving](https://arxiv.org/abs/2602.22801)
*Yinan Zheng,Tianyi Tan,Bin Huang,Enguang Liu,Ruiming Liang,Jianlin Zhang,Jianwei Cui,Guang Chen,Kun Ma,Hangjun Ye,Long Chen,Ya-Qin Zhang,Xianyuan Zhan,Jingjing Liu*

Main category: cs.RO

TL;DR: 本文将扩散模型应用于端到端自动驾驶的真实环境中，通过大量实车数据和道路测试，提出了性能显著提升的Hyper Diffusion Planner (HDP)方法，并在实际城市驾驶场景中验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型已在机器人决策任务中取得进展，但其在自动驾驶领域的应用多局限于仿真或实验室环境，缺乏大规模、复杂的真实环境验证。作者希望解决该领域的研究空白，挖掘扩散模型在实际自动驾驶决策中的潜力。

Method: 作者基于大量实车数据和道路测试，对扩散模型用于端到端自动驾驶进行了系统性、大规模的研究。重点分析了扩散损失空间、轨迹表示和数据规模对规划性能的影响，并提出了基于强化学习的后训练策略来提升系统安全性，最终形成Hyper Diffusion Planner (HDP)框架。

Result: HDP在真实车辆平台上部署，并在6个城市驾驶场景及200公里的实际测试中，较基础模型实现了10倍的性能提升。

Conclusion: 经过合理设计和训练的扩散模型可作为高效、可扩展的端到端自动驾驶规划器，有能力解决复杂、真实环境下的自动驾驶问题。

Abstract: Diffusion models have become a popular choice for decision-making tasks in robotics, and more recently, are also being considered for solving autonomous driving tasks. However, their applications and evaluations in autonomous driving remain limited to simulation-based or laboratory settings. The full strength of diffusion models for large-scale, complex real-world settings, such as End-to-End Autonomous Driving (E2E AD), remains underexplored. In this study, we conducted a systematic and large-scale investigation to unleash the potential of the diffusion models as planners for E2E AD, based on a tremendous amount of real-vehicle data and road testing. Through comprehensive and carefully controlled studies, we identify key insights into the diffusion loss space, trajectory representation, and data scaling that significantly impact E2E planning performance. Moreover, we also provide an effective reinforcement learning post-training strategy to further enhance the safety of the learned planner. The resulting diffusion-based learning framework, Hyper Diffusion Planner} (HDP), is deployed on a real-vehicle platform and evaluated across 6 urban driving scenarios and 200 km of real-world testing, achieving a notable 10x performance improvement over the base model. Our work demonstrates that diffusion models, when properly designed and trained, can serve as effective and scalable E2E AD planners for complex, real-world autonomous driving tasks.

</details>


### [72] [LeRobot: An Open-Source Library for End-to-End Robot Learning](https://arxiv.org/abs/2602.22818)
*Remi Cadene,Simon Aliberts,Francesco Capuano,Michel Aractingi,Adil Zouitine,Pepijn Kooijmans,Jade Choghari,Martino Russi,Caroline Pascal,Steven Palma,Mustafa Shukor,Jess Moss,Alexander Soare,Dana Aubakirova,Quentin Lhoest,Quentin Gallouédec,Thomas Wolf*

Main category: cs.RO

TL;DR: 本文介绍了开源库lerobot，该库整合了机器人学习全栈功能，从底层硬件通讯到大规模数据处理，支持多种前沿学习方法，降低了机器人学习的门槛。


<details>
  <summary>Details</summary>
Motivation: 机器人学习领域虽然快速发展，但受限于专用且封闭的工具，导致进展缓慢。亟需一套开放、集成化的工具来促进整个领域的协作和创新。

Method: 作者开发了lerobot开源库，涵盖机器人学习从底层驱动通讯、中间件到大规模数据采集、存储和流处理，实现对主流学习算法的高效集成，并支持多种硬件和算法范式。

Result: lerobot能够支持多类实际机器人硬件，集成并优化了各类先进机器人学习算法，为用户提供了模块化、可扩展和可重现的机器人学习平台。

Conclusion: lerobot通过开放、可扩展、易用的设计，促进了机器人学习领域的开放合作和新方法的快速迭代，为学术研究和实际应用拓展了更广阔的空间。

Abstract: Robotics is undergoing a significant transformation powered by advances in high-level control techniques based on machine learning, giving rise to the field of robot learning. Recent progress in robot learning has been accelerated by the increasing availability of affordable teleoperation systems, large-scale openly available datasets, and scalable learning-based methods. However, development in the field of robot learning is often slowed by fragmented, closed-source tools designed to only address specific sub-components within the robotics stack. In this paper, we present \texttt{lerobot}, an open-source library that integrates across the entire robot learning stack, from low-level middleware communication for motor controls to large-scale dataset collection, storage and streaming. The library is designed with a strong focus on real-world robotics, supporting accessible hardware platforms while remaining extensible to new embodiments. It also supports efficient implementations for various state-of-the-art robot learning algorithms from multiple prominent paradigms, as well as a generalized asynchronous inference stack. Unlike traditional pipelines which heavily rely on hand-crafted techniques, \texttt{lerobot} emphasizes scalable learning approaches that improve directly with more data and compute. Designed for accessibility, scalability, and openness, \texttt{lerobot} lowers the barrier to entry for researchers and practitioners to robotics while providing a platform for reproducible, state-of-the-art robot learning.

</details>


### [73] [Performance and Experimental Analysis of Strain-based Models for Continuum Robots](https://arxiv.org/abs/2602.22854)
*Annika Delucchi,Vincenzo Di Paola,Andreas Müller,and Matteo Zoppi*

Main category: cs.RO

TL;DR: 本文比较了第三阶应变插值法与几何变应变方法在连续体机器人形状重建中的表现，并在实验证明前者在准确性和计算效率上均优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 尽管应变模型在机器人领域应用广泛，但目前缺乏系统的性能评估方法，尤其是在实际机器人原型开发中需求日益增长，急需评估其适用性和综合性能。

Method: 本文重点研究第三阶应变插值方法的形状重建能力，既考察其对单一变形和组合变形效应的描述能力，又与几何变应变方法进行对比。同时，通过机械臂控制单端点运动并利用摄像系统及反光标记获取细长杆的形变数据，进一步进行实验验证，不借助杆上应变片或力传感器。

Result: 实验结果显示，第三阶应变插值模型在形状预测上与实际观测高度吻合，平均误差占杆长0.58%，平均每组配置计算仅需0.32秒，表现优于现有主流模型。

Conclusion: 该第三阶应变插值模型不仅准确性高、计算快，还简化了实验设备要求，适合连续体机器人实际应用，为后续性能评估和模型选择提供了参考。

Abstract: Although strain-based models have been widely adopted in robotics, no comparison beyond the uniform bending test is commonly recognized to assess their performance. In addition, the increasing effort in prototyping continuum robots highlights the need to assess the applicability of these models and the necessity of comprehensive performance evaluation. To address this gap, this work investigates the shape reconstruction abilities of a third-order strain interpolation method, examining its ability to capture both individual and combined deformation effects. These results are compared and discussed against the Geometric-Variable Strain approach. Subsequently, simulation results are experimentally verified by reshaping a slender rod while recording the resulting configurations using cameras. The rod configuration is imposed using a manipulator displacing one of its tips and extracted through reflective markers, without the aid of any other external sensor -- i.e. strain gauges or wrench sensors placed along the rod. The experiments demonstrate good agreement between the model predictions and observed shapes, with average error of 0.58% of the rod length and average computational time of 0.32s per configuration, outperforming existing models.

</details>


### [74] [DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation](https://arxiv.org/abs/2602.22896)
*Zebin Yang,Yijiahao Qi,Tong Xie,Bo Yu,Shaoshan Liu,Meng Li*

Main category: cs.RO

TL;DR: 本文提出了DySL-VLA框架，通过动态跳过部分VLA模型层，减少计算成本同时保持性能，有效提升机器人操作任务的实时性。


<details>
  <summary>Details</summary>
Motivation: 虽然VLA模型能够结合语言推理与视觉理解，极大提升机器人处理复杂任务的能力，但高昂的计算开销制约了其实时应用。作者洞察到机器人任务中不同动作的重要程度不同，部分关键动作需要高精度，而一般动作容忍度更高，由此激发对计算资源动态分配的需求。

Method: DySL-VLA将模型层分为始终执行的信息层和可选择跳过的增量层。通过设计一种先验-事后跳层引导机制，智能决策何时跳过增量层。同时提出了带跳层感知的双阶段知识蒸馏方法，将标准VLA高效蒸馏为DySL-VLA，以兼顾速度和准确性。

Result: 在Calvin数据集上的实验中，DySL-VLA比Deer-VLA在任务成功长度上提升了2.1%，训练参数量减少85.7倍。与RoboFlamingo基线相比，在相同精度下加速3.75倍。

Conclusion: DySL-VLA显著降低了VLA模型实际部署的计算成本，同时保持甚至提升了性能，为VLA在实时机器人任务中的应用提供了更优解决方案。代码已开源。

Abstract: Vision-Language-Action (VLA) models have shown remarkable success in robotic tasks like manipulation by fusing a language model's reasoning with a vision model's 3D understanding. However, their high computational cost remains a major obstacle for real-world applications that require real-time performance. We observe that the actions within a task have varying levels of importance: critical steps demand high precision, while less important ones can tolerate more variance. Leveraging this insight, we propose DySL-VLA, a novel framework that addresses computational cost by dynamically skipping VLA layers based on each action's importance. DySL-VLA categorizes its layers into two types: informative layers, which are consistently executed, and incremental layers, which can be selectively skipped. To intelligently skip layers without sacrificing accuracy, we invent a prior-post skipping guidance mechanism to determine when to initiate layer-skipping. We also propose a skip-aware two-stage knowledge distillation algorithm to efficiently train a standard VLA into a DySL-VLA. Our experiments indicate that DySL-VLA achieves 2.1% improvement in success length over Deer-VLA on the Calvin dataset, while simultaneously reducing trainable parameters by a factor of 85.7 and providing a 3.75x speedup relative to the RoboFlamingo baseline at iso-accuracy. Our code is available on https://github.com/PKU-SEC-Lab/DYSL_VLA.

</details>
