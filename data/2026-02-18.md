<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 43]
- [cs.CL](#cs.CL) [Total: 37]
- [cs.RO](#cs.RO) [Total: 30]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GRAFNet: Multiscale Retinal Processing via Guided Cortical Attention Feedback for Enhancing Medical Image Polyp Segmentation](https://arxiv.org/abs/2602.15072)
*Abdul Joseph Fofanah,Lian Wen,Alpha Alimamy Kamara,Zhongyi Zhang,David Chen,Albert Patrick Sankoh*

Main category: cs.CV

TL;DR: 该论文提出一种新型的医学图像分割网络GRAFNet，针对结肠镜下息肉分割任务，显著提升了分割精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 结肠镜息肉分割对于预防癌症非常关键。目前方法面临形态多样性大、与正常组织外观相似、难以检测多尺度结构等挑战，现有深度学习方法存在泛化弱、过分割和漏检等问题。该工作旨在通过模拟生物视觉系统机制，提升方法对息肉边界和多尺度结构的识别能力，增强分割模型的鲁棒性和可解释性。

Method: 作者提出了GRAFNet体系结构，包括三个核心模块：（1）仿生的引导非对称注意力模块（GAAM）用于强化息肉边界感知；（2）多尺度视网膜模块（MSRM）实现并行多特征分析；（3）引导皮层反馈模块（GCAFM）通过预测编码机制进行逐步特征精炼。上述模块统一在一个息肉编解码结构（PEDM）中，实现分辨率自适应的空间-语义一致性。

Result: 在五个公开数据集上（Kvasir-SEG, CVC-300, CVC-ColonDB, CVC-Clinic, PolypGen），GRAFNet都实现了当前最优性能，Dice系数提升3-8%，泛化能力提高10-20%，并且具有可解释的决策流程。

Conclusion: GRAFNet通过类脑的视觉处理机制，显著提升了结肠镜息肉分割的精度和可靠性，为AI与临床可信推断间架起桥梁。同时，方法具备优良的泛化性和可解释性，具有实际应用潜力。

Abstract: Accurate polyp segmentation in colonoscopy is essential for cancer prevention but remains challenging due to: (1) high morphological variability (from flat to protruding lesions), (2) strong visual similarity to normal structures such as folds and vessels, and (3) the need for robust multi-scale detection. Existing deep learning approaches suffer from unidirectional processing, weak multi-scale fusion, and the absence of anatomical constraints, often leading to false positives (over-segmentation of normal structures) and false negatives (missed subtle flat lesions). We propose GRAFNet, a biologically inspired architecture that emulates the hierarchical organisation of the human visual system. GRAFNet integrates three key modules: (1) a Guided Asymmetric Attention Module (GAAM) that mimics orientation-tuned cortical neurones to emphasise polyp boundaries, (2) a MultiScale Retinal Module (MSRM) that replicates retinal ganglion cell pathways for parallel multi-feature analysis, and (3) a Guided Cortical Attention Feedback Module (GCAFM) that applies predictive coding for iterative refinement. These are unified in a Polyp Encoder-Decoder Module (PEDM) that enforces spatial-semantic consistency via resolution-adaptive feedback. Extensive experiments on five public benchmarks (Kvasir-SEG, CVC-300, CVC-ColonDB, CVC-Clinic, and PolypGen) demonstrate consistent state-of-the-art performance, with 3-8% Dice improvements and 10-20% higher generalisation over leading methods, while offering interpretable decision pathways. This work establishes a paradigm in which neural computation principles bridge the gap between AI accuracy and clinically trustworthy reasoning. Code is available at https://github.com/afofanah/GRAFNet.

</details>


### [2] [Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition](https://arxiv.org/abs/2602.15124)
*Shiyu Xuan,Dongkai Wang,Zechao Li,Jinhui Tang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的零样本人-物交互（HOI）检测方法，将物体检测与交互识别（IR）解耦，并利用多模态大语言模型（MLLM）实现无需训练的零样本交互识别，在主流数据集上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本人-物交互检测方法通常将交互识别与特定的检测器以及粗粒度视觉-语言特征耦合在一起，导致模型泛化能力有限，尤其是在识别未见过的交互组合时表现不佳。因此，需要一种更灵活且泛化能力更强的解决方案。

Method: 作者提出了一个解耦框架，将物体检测与交互识别分开。具体地，物体检测可由任意open-vocabulary检测器完成，交互识别则转化为视觉问答任务，利用大语言模型实现零样本识别，通过确定性生成方法保证输出一致性。此外，还设计了带有空间感知池化模块以融合外观和空间特征，并采用一次性确定性匹配策略来提升效率和性能。

Result: 在HICO-DET和V-COCO两个主流人-物交互检测数据集上，提出的方法显著提升了零样本检测性能，展现了更强的跨数据集泛化能力，并且可以灵活集成到任何物体检测器中，无需针对目标检测模块重新训练。

Conclusion: 本文的方法有效解决了现有HOI检测在泛化和效率方面的不足，证明了利用多模态大语言模型实现解耦与训练自由的零样本交互识别的可行性及优越性，对实际应用具有很大潜力。

Abstract: Zero-shot Human-object interaction (HOI) detection aims to locate humans and objects in images and recognize their interactions. While advances in open-vocabulary object detection provide promising solutions for object localization, interaction recognition (IR) remains challenging due to the combinatorial diversity of interactions. Existing methods, including two-stage methods, tightly couple IR with a specific detector and rely on coarse-grained vision-language model (VLM) features, which limit generalization to unseen interactions. In this work, we propose a decoupled framework that separates object detection from IR and leverages multi-modal large language models (MLLMs) for zero-shot IR. We introduce a deterministic generation method that formulates IR as a visual question answering task and enforces deterministic outputs, enabling training-free zero-shot IR. To further enhance performance and efficiency by fine-tuning the model, we design a spatial-aware pooling module that integrates appearance and pairwise spatial cues, and a one-pass deterministic matching method that predicts all candidate interactions in a single forward pass. Extensive experiments on HICO-DET and V-COCO demonstrate that our method achieves superior zero-shot performance, strong cross-dataset generalization, and the flexibility to integrate with any object detectors without retraining. The codes are publicly available at https://github.com/SY-Xuan/DA-HOI.

</details>


### [3] [MB-DSMIL-CL-PL: Scalable Weakly Supervised Ovarian Cancer Subtype Classification and Localisation Using Contrastive and Prototype Learning with Frozen Patch Features](https://arxiv.org/abs/2602.15138)
*Marcus Jenkins,Jasenka Mazibrada,Bogdan Leahu,Michal Mackiewicz*

Main category: cs.CV

TL;DR: 本论文提出了一种利用对比学习和原型学习的新方法，实现对卵巢癌组织病理图像亚型的分类和定位，在不增加训练负担的前提下显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 随着AI方法在组织病理图像分析中的普及，传统依赖预计算静态特征的方法虽然易扩展，但准确性有限；而端到端方法虽准确但训练和实验成本高。为解决准确性与可扩展性的矛盾，需寻求兼顾两者的新方案。

Method: 作者采用了对比学习和原型学习相结合的方法，并通过在特征空间进行增强，利用预先计算的静态特征进行亚型分类和定位分析。该方案绕开了端到端训练的高资源消耗，重点提升静态特征下的模型表现能力。

Result: 与DSMIL方法对比，本方法在实例级F1分数提升70.4%，切片级F1分数提升15.3%；在实例定位AUC上提升16.9%，切片分类AUC提升2.3%。关键在于这些提升都是在仅使用静态补丁特征的条件下实现的。

Conclusion: 提出的方法在不增加计算和实验复杂度的基础上显著提升了卵巢癌组织病理图像亚型分类和定位的准确率，为在工作负载较高的病理科室实际应用AI工具提供了有力支持。

Abstract: The study of histopathological subtypes is valuable for the personalisation of effective treatment strategies for ovarian cancer. However, increasing diagnostic workloads present a challenge for UK pathology departments, leading to the rise in AI approaches. While traditional approaches in this field have relied on pre-computed, frozen image features, recent advances have shifted towards end-to-end feature extraction, providing an improvement in accuracy but at the expense of significantly reduced scalability during training and time-consuming experimentation. In this paper, we propose a new approach for subtype classification and localisation in ovarian cancer histopathology images using contrastive and prototype learning with pre-computed, frozen features via feature-space augmentations. Compared to DSMIL, our method achieves an improvement of 70.4\% and 15.3\% in F1 score for instance- and slide-level classification, respectively, along with AUC gains of 16.9\% for instance localisation and 2.3\% for slide classification, while maintaining the use of frozen patch features.

</details>


### [4] [Loss Knows Best: Detecting Annotation Errors in Videos via Loss Trajectories](https://arxiv.org/abs/2602.15154)
*Praditha Alwis,Soumyadeep Chandra,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CV

TL;DR: 本文提出了一种新颖且与模型无关的方法，通过分析视频中每帧在训练过程中累积损失（CSL），自动检测视频数据集中标注错误（如标签错误或时序错乱）。


<details>
  <summary>Details</summary>
Motivation: 现实中的视频数据集常存在注释错误，尤其是标签错误和时序错乱，这些问题会严重影响基于阶段标注的任务（如动作识别、事件分割等）的模型训练效果。传统的数据检查方法难以自动发现这些问题，缺乏通用且高效的审查工具。

Method: 作者定义了每帧的累积样本损失（CSL），其为该帧在模型训练各轮（保存的不同训练快照下）平均损失。通过训练一个视频分割模型，每轮训练后保存权重，并用这些权重评估所有帧的损失。标注错误的帧往往在整个训练过程中都呈现高损失或异常损失变化，据此将这些帧标记为潜在错误。

Result: 该方法无需人工提供标注错误的真实标签，适用性强。实验证明，在EgoPER和Cholec80两个数据集上能有效检测出标签错误和时序错乱等细微不一致的问题。

Conclusion: 提出的基于CSL的检测方法为视频数据集的自动审查提供了有效工具，可提升数据集质量和模型训练的可靠性，对视频机器学习领域具有推广意义。

Abstract: High-quality video datasets are foundational for training robust models in tasks like action recognition, phase detection, and event segmentation. However, many real-world video datasets suffer from annotation errors such as *mislabeling*, where segments are assigned incorrect class labels, and *disordering*, where the temporal sequence does not follow the correct progression. These errors are particularly harmful in phase-annotated tasks, where temporal consistency is critical. We propose a novel, model-agnostic method for detecting annotation errors by analyzing the Cumulative Sample Loss (CSL)--defined as the average loss a frame incurs when passing through model checkpoints saved across training epochs. This per-frame loss trajectory acts as a dynamic fingerprint of frame-level learnability. Mislabeled or disordered frames tend to show consistently high or irregular loss patterns, as they remain difficult for the model to learn throughout training, while correctly labeled frames typically converge to low loss early. To compute CSL, we train a video segmentation model and store its weights at each epoch. These checkpoints are then used to evaluate the loss of each frame in a test video. Frames with persistently high CSL are flagged as likely candidates for annotation errors, including mislabeling or temporal misalignment. Our method does not require ground truth on annotation errors and is generalizable across datasets. Experiments on EgoPER and Cholec80 demonstrate strong detection performance, effectively identifying subtle inconsistencies such as mislabeling and frame disordering. The proposed approach provides a powerful tool for dataset auditing and improving training reliability in video-based machine learning.

</details>


### [5] [Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift](https://arxiv.org/abs/2602.15167)
*Xiaoyi Wen,Fei Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种分布式深度学习框架，用于医学成像超分辨率任务，尤其是在4D Flow MRI中的应用，有效应对由于采集方式不同带来的领域偏移问题，并在实际数据下优于传统深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统超分辨率方法依赖于下采样-高分辨率配对数据进行训练，但在临床实际应用中，低分辨率图像往往并非简单下采样得到，导致模型泛化能力差。为解决该领域偏移问题，需要开发更鲁棒的超分辨率方法。

Method: 提出了一种分布式深度学习框架，初步在高分辨率CFD仿真数据及其下采样对上训练，然后在少量配对的4D Flow MRI与CFD数据上微调，推导了分布式估计器的理论性质，并将方法应用于实际临床数据。

Result: 实验证明，该分布式方法在真实数据上的表现显著优于传统深度学习方法，特别是在面临领域偏移的情况下。

Conclusion: 分布式学习框架有效解决了领域偏移问题，提升了医学成像超分辨率的鲁棒性与泛化能力，对实际临床场景下的4D Flow MRI数据具有较强的应用价值。

Abstract: Super-resolution is widely used in medical imaging to enhance low-quality data, reducing scan time and improving abnormality detection. Conventional super-resolution approaches typically rely on paired datasets of downsampled and original high resolution images, training models to reconstruct high resolution images from their artificially degraded counterparts. However, in real-world clinical settings, low resolution data often arise from acquisition mechanisms that differ significantly from simple downsampling. As a result, these inputs may lie outside the domain of the training data, leading to poor model generalization due to domain shift. To address this limitation, we propose a distributional deep learning framework that improves model robustness and domain generalization. We develop this approch for enhancing the resolution of 4D Flow MRI (4DF). This is a novel imaging modality that captures hemodynamic flow velocity and clinically relevant metrics such as vessel wall stress. These metrics are critical for assessing aneurysm rupture risk. Our model is initially trained on high resolution computational fluid dynamics (CFD) simulations and their downsampled counterparts. It is then fine-tuned on a small, harmonized dataset of paired 4D Flow MRI and CFD samples. We derive the theoretical properties of our distributional estimators and demonstrate that our framework significantly outperforms traditional deep learning approaches through real data applications. This highlights the effectiveness of distributional learning in addressing domain shift and improving super-resolution performance in clinically realistic scenarios.

</details>


### [6] [Time-Archival Camera Virtualization for Sports and Visual Performances](https://arxiv.org/abs/2602.15181)
*Yunxiao Zhang,William Stone,Suryansh Kumar*

Main category: cs.CV

TL;DR: 该论文提出了一种新的基于神经体渲染的方法，实现了动态场景的高质量相机虚拟化，并支持时间归档和回放功能，解决了现有方法在体育和演出等高速动态场景下渲染不连贯且难以回溯的问题。


<details>
  <summary>Details</summary>
Motivation: 当前相机虚拟化和新视角合成技术受限于动态场景下的空间和时间一致性及高保真渲染，尤其在体育等快节奏场合难以做到流畅的多视角回放，且对时间归档的支持有限。


Method: 该方法将动态场景建模为多台同步相机下的刚性变换，采用神经表示学习进行体渲染。区别于基于3D高斯点渲染（3DGS）对点云和目标追踪依赖明显的缺陷，本方法利用神经网络提升了新视角生成质量，并突破了对多目标独立运动的处理瓶颈。

Result: 实验结果显示，该方法显著提升了动态场景（如体育比赛、舞台演出）的新视角渲染质量，并实现了任意时间点的回溯渲染和场景归档能力，优于当前主流3DGS及其变体。

Conclusion: 该工作为动态场景相机虚拟化带来了高质量、可归档的新视角生成解决方案，特别适合体育直播、舞台表演等需要实时回放与分析的应用场景，弥补了现有神经渲染技术的不足。

Abstract: Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularly in fast-paced sports and stage performances, remains challenging for existing approaches. Recent methods based on 3D Gaussian Splatting (3DGS) for dynamic scenes could offer real-time view-synthesis results. Yet, they are hindered by their dependence on accurate 3D point clouds from the structure-from-motion method and their inability to handle large, non-rigid, rapid motions of different subjects (e.g., flips, jumps, articulations, sudden player-to-player transitions). Moreover, independent motions of multiple subjects can break the Gaussian-tracking assumptions commonly used in 4DGS, ST-GS, and other dynamic splatting variants. This paper advocates reconsidering a neural volume rendering formulation for camera virtualization and efficient time-archival capabilities, making it useful for sports broadcasting and related applications. By modeling a dynamic scene as rigid transformations across multiple synchronized camera views at a given time, our method performs neural representation learning, providing enhanced visual rendering quality at test time. A key contribution of our approach is its support for time-archival, i.e., users can revisit any past temporal instance of a dynamic scene and can perform novel view synthesis, enabling retrospective rendering for replay, analysis, and archival of live events, a functionality absent in existing neural rendering approaches and novel view synthesis...

</details>


### [7] [How to Train Your Long-Context Visual Document Model](https://arxiv.org/abs/2602.15257)
*Austin Veselka*

Main category: cs.CV

TL;DR: 本论文首次系统性地研究了长上下文（最长达344K）视觉语言模型的训练，并在长文档视觉问答及长上下文文本任务上验证了其迁移能力，取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 尽管已有开源强力视觉语言模型，但其训练方法和数据流程不可复现，本研究旨在填补这一空白，推动长上下文视觉语言模型的可重现性和性能优化。

Method: 系统分析了24B和32B参数模型下的持续预训练、监督微调和偏好优化，并通过深度消融实验和广泛长上下文评测支持方法有效性，还构建了合成数据流程及手动纠正的新数据集MMLBD-C。

Result: （1）训练时上下文长度与评测长度相匹配优于使用更长上下文训练；（2）引入页面索引大幅提升长文档性能；（3）合成数据管道有助于持续自我提升；（4）首次验证视觉长上下文训练可正向迁移到文本长上下文任务。

Conclusion: 研究提出了一套可复现且有效的长上下文视觉语言模型训练范式，在多个参数规模下取得SOTA，同时推动了数据集质量提升与方法透明度。

Abstract: We present the first comprehensive, large-scale study of training long-context vision language models up to 344K context, targeting long-document visual question answering with measured transfer to long-context text. While several such strong are open-weight, namely Qwen3 VL and GLM 4.5/6V, their training recipes and data pipelines are not reproducible. We systematically study continued pretraining, supervised finetuning, and preference optimization for 24B and 32B parameter models, backed by extensive LC evaluations and ablations to bridge this gap, and achieve state-of-the-art performance on MMLongBenchDoc for both parameter scales. In addition to this, our key findings include: (i) training on context lengths that match evaluation context lengths outperforms training on longer contexts, (ii) training and evaluating with page indices provides a simple, high-impact boost to long-document performance, (iii) our synthetic data pipelines enable self-improvement via continued pretraining and supervised finetuning, and (iv) we extend the known text-to-visual long context transfer to the reverse, showing that visual long context training transfers to long-context text performance. We also release MMLBD-C, a manually corrected version of MMLongBenchDoc to reduce erroneous and low quality examples in the benchmark.

</details>


### [8] [Accelerating Large-Scale Dataset Distillation via Exploration-Exploitation Optimization](https://arxiv.org/abs/2602.15277)
*Muhammad J. Alahmadi,Peng Gao,Feiyi Wang,Dongkuan,Xu*

Main category: cs.CV

TL;DR: 本文提出了E^2D（探索-开发蒸馏）方法，在保持高精度的同时大幅提高了大规模数据集蒸馏的效率。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法在高准确率和高效率之间存在权衡：基于优化的解耦方法虽然准但计算量大，无优化方法虽快但牺牲了准确率。该研究旨在打破这一瓶颈，实现高效且高精度的数据集蒸馏。

Method: E^2D方法采用全图初始化保持语义完整性和特征多样性，接着通过两阶段优化流程：第一阶段均匀探索并定位高损失区域，第二阶段重点优化这些区域，从而减少冗余计算并加速收敛。

Result: 在大规模数据集（如ImageNet-1K和ImageNet-21K）上的实验显示，E^2D方法不仅准确率超越了现有最优方法，而且在训练速度方面分别实现了18倍和4.3倍的提升。

Conclusion: 通过有针对性的冗余更新替代暴力优化，E^2D有效弥合了大规模数据集蒸馏中的准确率与效率矛盾，有望在资源受限场景广泛应用。

Abstract: Dataset distillation compresses the original data into compact synthetic datasets, reducing training time and storage while retaining model performance, enabling deployment under limited resources. Although recent decoupling-based distillation methods enable dataset distillation at large-scale, they continue to face an efficiency gap: optimization-based decoupling methods achieve higher accuracy but demand intensive computation, whereas optimization-free decoupling methods are efficient but sacrifice accuracy. To overcome this trade-off, we propose Exploration-Exploitation Distillation (E^2D), a simple, practical method that minimizes redundant computation through an efficient pipeline that begins with full-image initialization to preserve semantic integrity and feature diversity. It then uses a two-phase optimization strategy: an exploration phase that performs uniform updates and identifies high-loss regions, and an exploitation phase that focuses updates on these regions to accelerate convergence. We evaluate E^2D on large-scale benchmarks, surpassing the state-of-the-art on ImageNet-1K while being 18x faster, and on ImageNet-21K, our method substantially improves accuracy while remaining 4.3x faster. These results demonstrate that targeted, redundancy-reducing updates, rather than brute-force optimization, bridge the gap between accuracy and efficiency in large-scale dataset distillation. Code is available at https://github.com/ncsu-dk-lab.

</details>


### [9] [Visual Persuasion: What Influences Decisions of Vision-Language Models?](https://arxiv.org/abs/2602.15278)
*Manuel Cherep,Pranav M R,Pattie Maes,Nikhil Singh*

Main category: cs.CV

TL;DR: 提出了一种研究视觉语言模型（VLM）视觉偏好的方法，通过对图片进行系统性编辑，分析VLM在选图任务中的偏好变化。


<details>
  <summary>Details</summary>
Motivation: 随着VLM在网络大量图片上做决策（如点击、推荐、购买），但我们对其视觉偏好知之甚少。作者希望揭示和解释这些模型偏好背后的结构与规律。

Method: 为VLM设计受控选图任务，对输入图片进行系统性扰动，将VLM的决策视为隐藏的视觉效用，通过揭示性偏好推断。借助图片生成模型，对图片如构图、光照、背景等进行逐步优化编辑，观察哪些修改提升图片被选中的概率，并开发自动化可解释性流程解释这些偏好。

Result: 大规模实验显示，对图片进行优化编辑后，VLM在两图对比中明显偏向被优化的图片。自动化解释流程能识别出驱动选择的图像主题。

Conclusion: 该方法为发现VLM潜在的不安全视觉漏洞和偏好提供了高效实用的工具，有助于主动审计和治理基于图片的AI代理。

Abstract: The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference: choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization, adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities, safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.

</details>


### [10] [Consistency-Preserving Diverse Video Generation](https://arxiv.org/abs/2602.15287)
*Xinshuang Liu,Runfa Blark Li,Truong Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种高效的文本生成视频(batch-level)的联合采样框架，兼顾提高不同视频间的多样性与单个视频内部时序一致性，且无需高成本的梯度计算，实验表明效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 文本到视频生成模型的运行成本很高，每次只能生成很少的视频样本，因此如何在有限的样本数量下提高每个batch（批次）生成视频的多样性，且不牺牲每个视频自身的时序一致性，是一个重要且有挑战性的问题。图像生成领域已有针对多样性的改进方法，但直接用于视频时会导致时序一致性下降或计算成本过高。

Method: 作者提出了一种面向flow-matching视频生成器的联合采样（joint-sampling）框架。具体做法是在采样时先进行基于多样性的更新，然后仅移除那些会降低时序一致性目标的成分。整个过程在轻量化的潜变量空间完成，无需对图像空间进行复杂的梯度反传，也无需视频解码操作。

Result: 在先进的text-to-video flow-matching模型上进行了实验，结果显示该方法在提升batch内多样性的同时，大幅提升了视频的时序一致性和彩色自然度，性能优于其他强基线联合采样方法。

Conclusion: 本方法有效解决了低样本数量下文本到视频生成中多样性与时序一致性的兼顾，且具备更高的运算效率和更自然的视频表现。代码即将开源，有望对实际应用场景产生积极影响。

Abstract: Text-to-video generation is expensive, so only a few samples are typically produced per prompt. In this low-sample regime, maximizing the value of each batch requires high cross-video diversity. Recent methods improve diversity for image generation, but for videos they often degrade within-video temporal consistency and require costly backpropagation through a video decoder. We propose a joint-sampling framework for flow-matching video generators that improves batch diversity while preserving temporal consistency. Our approach applies diversity-driven updates and then removes only the components that would decrease a temporal-consistency objective. To avoid image-space gradients, we compute both objectives with lightweight latent-space models, avoiding video decoding and decoder backpropagation. Experiments on a state-of-the-art text-to-video flow-matching model show diversity comparable to strong joint-sampling baselines while substantially improving temporal consistency and color naturalness. Code will be released.

</details>


### [11] [Training-Free Zero-Shot Anomaly Detection in 3D Brain MRI with 2D Foundation Models](https://arxiv.org/abs/2602.15315)
*Tai Le-Gia,Jaehyun Ahn*

Main category: cs.CV

TL;DR: 本文提出了一种针对3D脑MRI的零样本异常检测（ZSAD）方法，无需训练和监督，能够有效扩展2D模型至3D医学影像。


<details>
  <summary>Details</summary>
Motivation: 目前ZSAD大多局限于2D医学影像，在3D影像扩展上存在挑战，现有方法难以捕捉三维结构信息。需有简便、训练无关的方法来解决3D异常检测。

Method: 利用2D基础模型处理多轴切片，将其聚合成局部的3D补丁token，恢复三维空间信息，并结合基于距离、批量级的异常检测流程，实现全程无监督的ZSAD。

Result: 无需微调或提示词，直接利用2D编码器在3D MRI体数据上实现训练无关的批量ZSAD，且3D表示紧凑、计算高效。

Conclusion: 提出的方法能够简洁且稳健地将ZSAD从2D扩展到3D影像，有望在医学体数据异常检测中得到实用应用。

Abstract: Zero-shot anomaly detection (ZSAD) has gained increasing attention in medical imaging as a way to identify abnormalities without task-specific supervision, but most advances remain limited to 2D datasets. Extending ZSAD to 3D medical images has proven challenging, with existing methods relying on slice-wise features and vision-language models, which fail to capture volumetric structure. In this paper, we introduce a fully training-free framework for ZSAD in 3D brain MRI that constructs localized volumetric tokens by aggregating multi-axis slices processed by 2D foundation models. These 3D patch tokens restore cubic spatial context and integrate directly with distance-based, batch-level anomaly detection pipelines. The framework provides compact 3D representations that are practical to compute on standard GPUs and require no fine-tuning, prompts, or supervision. Our results show that training-free, batch-based ZSAD can be effectively extended from 2D encoders to full 3D MRI volumes, offering a simple and robust approach for volumetric anomaly detection.

</details>


### [12] [Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs](https://arxiv.org/abs/2602.15318)
*Libo Zhang,Zhaoning Zhang,Wangyang Hong,Peng Qiao,Dongsheng Li*

Main category: cs.CV

TL;DR: 该论文发现现有的推理加速方法在视频大模型（Vid-LLMs）上表现失效，对此设计了Sparrow框架，显著提升了推理速度并保持性能。


<details>
  <summary>Details</summary>
Motivation: 目前流行的speculative decoding极大加速了视觉-语言模型（VLMs）的推理，但直接用在视频大模型Vid-LLMs时，会因注意力稀释、视觉增益负效应等问题导致性能大幅下降，尤其在长序列情况下更为严重。作者发现，这主要归因于关键值缓存爆炸、上下文窗口不匹配，以及关键视觉语义在深层交互中已隐式内化于文本隐藏态，导致深层时视觉输入变得冗余。

Method: 提出Sparrow框架：1）采用基于隐藏状态复用的可视感知文本锚定窗口注意力，将视觉计算全面迁移至目标模型；2）利用中间层视觉状态桥接，用语义丰富的中间表征训练起草模型，过滤低级视觉噪声；3）引入多token并行预测，缓解释范-推理分布偏移问题。

Result: Sparrow在包含高达25k视觉token时，平均加速2.82倍，能有效解决长序列情况下的性能退化。

Conclusion: Sparrow为长视频推理任务提供了实用高效的加速解决方案，解决了现有方法在Vid-LLMs中的性能坍塌难题，具有广阔应用前景。

Abstract: Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks.

</details>


### [13] [EventMemAgent: Hierarchical Event-Centric Memory for Online Video Understanding with Adaptive Tool Use](https://arxiv.org/abs/2602.15329)
*Siwei Wen,Zhangcheng Wang,Xingjian Zhang,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: 本文提出了一种面向在线视频流理解的新方法EventMemAgent，结合分层记忆模块和多粒度感知工具，提升了流式视频中的事件检测与长距离推理能力。


<details>
  <summary>Details</summary>
Motivation: 在线视频理解需要模型在连续的、无限的视频流上执行感知和推理任务，而当前多模态大语言模型受限于有限的上下文窗口，难以兼顾长程语境与细粒度细节。

Method: 提出EventMemAgent框架，基于分层记忆模块：短期记忆通过事件粒度的蓄水池采样和事件边界检测动态处理视频帧，长期记忆以事件为单位结构化存储过往信息；集成多粒度感知工具箱用于主动证据捕获，并采用Agentic RL端到端赋能代理推理与工具使用能力。

Result: 在在线视频理解基准测试上，EventMemAgent展现了有竞争力的效果。

Conclusion: EventMemAgent能够更好处理无限流视频的长时推理与细节捕获难题，对复杂任务具有显著优势，相关代码已开放。

Abstract: Online video understanding requires models to perform continuous perception and long-range reasoning within potentially infinite visual streams. Its fundamental challenge lies in the conflict between the unbounded nature of streaming media input and the limited context window of Multimodal Large Language Models (MLLMs). Current methods primarily rely on passive processing, which often face a trade-off between maintaining long-range context and capturing the fine-grained details necessary for complex tasks. To address this, we introduce EventMemAgent, an active online video agent framework based on a hierarchical memory module. Our framework employs a dual-layer strategy for online videos: short-term memory detects event boundaries and utilizes event-granular reservoir sampling to process streaming video frames within a fixed-length buffer dynamically; long-term memory structuredly archives past observations on an event-by-event basis. Furthermore, we integrate a multi-granular perception toolkit for active, iterative evidence capture and employ Agentic Reinforcement Learning (Agentic RL) to end-to-end internalize reasoning and tool-use strategies into the agent's intrinsic capabilities. Experiments show that EventMemAgent achieves competitive results on online video benchmarks. The code will be released here: https://github.com/lingcco/EventMemAgent.

</details>


### [14] [Effective and Robust Multimodal Medical Image Analysis](https://arxiv.org/abs/2602.15346)
*Joy Dhar,Nayyar Zaidi,Maryam Haghighat*

Main category: cs.CV

TL;DR: 本文提出多注意力集成学习（MAIL）网络，解决了医学多模态融合学习在泛化性、计算资源消耗和对抗鲁棒性等方面的突出问题，并通过MAIL及其鲁棒版本在多个数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 目前多模态融合学习常受限于只针对单一模态，缺乏高效性且对抗鲁棒性较弱，使其难以普适应用于多疾病分析，尤其是在资源受限或高可靠性需求的医疗场景下。

Method: 提出MAIL网络，包括高效的残差注意力模块（提升模态特异信息提取）和多模态交叉注意力模块（学习模态间的共享互补信息）；并基于MAIL加入随机投影滤波与调制注意力噪声，提出Robust-MAIL以提升对抗鲁棒性。

Result: 在20个公开数据集上评估，MAIL和Robust-MAIL在准确度上最高超过现有方法9.34%，计算成本最低可降78.3%。

Conclusion: MAIL及其鲁棒版本在提升多模态医学融合学习的泛化能力、效率和安全性方面显著优于现有方法，具有更广泛的实际应用前景。

Abstract: Multimodal Fusion Learning (MFL), leveraging disparate data from various imaging modalities (e.g., MRI, CT, SPECT), has shown great potential for addressing medical problems such as skin cancer and brain tumor prediction. However, existing MFL methods face three key limitations: a) they often specialize in specific modalities, and overlook effective shared complementary information across diverse modalities, hence limiting their generalizability for multi-disease analysis; b) they rely on computationally expensive models, restricting their applicability in resource-limited settings; and c) they lack robustness against adversarial attacks, compromising reliability in medical AI applications. To address these limitations, we propose a novel Multi-Attention Integration Learning (MAIL) network, incorporating two key components: a) an efficient residual learning attention block for capturing refined modality-specific multi-scale patterns and b) an efficient multimodal cross-attention module for learning enriched complementary shared representations across diverse modalities. Furthermore, to ensure adversarial robustness, we extend MAIL network to design Robust-MAIL by incorporating random projection filters and modulated attention noise. Extensive evaluations on 20 public datasets show that both MAIL and Robust-MAIL outperform existing methods, achieving performance gains of up to 9.34% while reducing computational costs by up to 78.3%. These results highlight the superiority of our approaches, ensuring more reliable predictions than top competitors. Code: https://github.com/misti1203/MAIL-Robust-MAIL.

</details>


### [15] [CREMD: Crowd-Sourced Emotional Multimodal Dogs Dataset](https://arxiv.org/abs/2602.15349)
*Jinho Baek,Houwei Cao,Kate Blackwell*

Main category: cs.CV

TL;DR: 该论文提出了首个大规模众包多模态犬类情感数据集（CREMD），分析不同展示方式及标注者特征对狗情感识别的影响。


<details>
  <summary>Details</summary>
Motivation: 狗的情感识别对于改善人类与动物互动、兽医护理及自动化监控系统很重要。但精确解读狗的情绪困难，缺乏客观评估标准和标准化数据。

Method: 构建并发布了CREMD数据集，含923段以三种模式呈现的视频（无上下文无音频、有上下文无音频、有上下文有音频），邀请拥有不同背景的受试者（狗主人、专业人士及各类人群）标注狗的情感，并分析影响标注一致性的因素。

Result: 结果显示（1）视觉上下文能显著提高标注一致性；（2）音频作用尚不明确，受实验设计限制；（3）非狗主人及男性标注者标注一致性高于狗主人和女性，专业人士一致性最高；（4）有音频时，标注者对愤怒和恐惧情绪的信心大幅提升。

Conclusion: 综合来看，视觉上下文对狗情感识别有积极作用，音频增强信心但研究尚需补充，标注者背景对结果影响显著，为今后犬类情感识别和数据集标准化提供了重要参考。

Abstract: Dog emotion recognition plays a crucial role in enhancing human-animal interactions, veterinary care, and the development of automated systems for monitoring canine well-being. However, accurately interpreting dog emotions is challenging due to the subjective nature of emotional assessments and the absence of standardized ground truth methods. We present the CREMD (Crowd-sourced Emotional Multimodal Dogs Dataset), a comprehensive dataset exploring how different presentation modes (e.g., context, audio, video) and annotator characteristics (e.g., dog ownership, gender, professional experience) influence the perception and labeling of dog emotions. The dataset consists of 923 video clips presented in three distinct modes: without context or audio, with context but no audio, and with both context and audio. We analyze annotations from diverse participants, including dog owners, professionals, and individuals with varying demographic backgrounds and experience levels, to identify factors that influence reliable dog emotion recognition. Our findings reveal several key insights: (1) while adding visual context significantly improved annotation agreement, our findings regarding audio cues are inconclusive due to design limitations (specifically, the absence of a no-context-with-audio condition and limited clean audio availability); (2) contrary to expectations, non-owners and male annotators showed higher agreement levels than dog owners and female annotators, respectively, while professionals showed higher agreement levels, aligned with our initial hypothesis; and (3) the presence of audio substantially increased annotators' confidence in identifying specific emotions, particularly anger and fear.

</details>


### [16] [DAV-GSWT: Diffusion-Active-View Sampling for Data-Efficient Gaussian Splatting Wang Tiles](https://arxiv.org/abs/2602.15355)
*Rong Fu,Jiekai Wu,Haiyun Wei,Yee Tan Jia,Wenxin Zhang,Yang Li,Xiaowen Ma,Wangyu Wu,Simon Fong*

Main category: cs.CV

TL;DR: 本文提出了DAV-GSWT框架，通过融合扩散先验和主动视角采样，实现了在最少观测数据下高质量的3D高斯点云Wang Tiles合成，既减小了数据量，又保持了渲染效果。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯点云和Wang Tiles方法虽能实现高质量神经渲染，但通常需依赖大量密集采样的重建样本，数据采集成本高、效率低，限制了在大规模虚拟环境的应用。

Method: DAV-GSWT采用扩散生成模型提供结构细节的拟合与补全，同时通过层次化的不确定性量化和主动采样机制，自动选择最具信息量的视角，从极少的输入关键观测中生成连续、无缝的高斯点云Wang Tiles。

Result: 实验表明，该方法能大幅减少所需的输入数据量，同时维持甚至提升生成环境的视觉一致性与交互性能。

Conclusion: DAV-GSWT开辟了利用极少观测实现高保真、高效率神经渲染的新途径，对虚拟现实和大规模数字内容生产具有重要意义。

Abstract: The emergence of 3D Gaussian Splatting has fundamentally redefined the capabilities of photorealistic neural rendering by enabling high-throughput synthesis of complex environments. While procedural methods like Wang Tiles have recently been integrated to facilitate the generation of expansive landscapes, these systems typically remain constrained by a reliance on densely sampled exemplar reconstructions. We present DAV-GSWT, a data-efficient framework that leverages diffusion priors and active view sampling to synthesize high-fidelity Gaussian Splatting Wang Tiles from minimal input observations. By integrating a hierarchical uncertainty quantification mechanism with generative diffusion models, our approach autonomously identifies the most informative viewpoints while hallucinating missing structural details to ensure seamless tile transitions. Experimental results indicate that our system significantly reduces the required data volume while maintaining the visual integrity and interactive performance necessary for large-scale virtual environments.

</details>


### [17] [GMAIL: Generative Modality Alignment for generated Image Learning](https://arxiv.org/abs/2602.15368)
*Shentong Mo,Sukmin Yun*

Main category: cs.CV

TL;DR: 本文提出了一种名为GMAIL的新框架，通过将生成图像和真实图像作为不同模态进行区分，实现生成图像更有效地参与视觉-语言模型训练。该方法可提升图像描述、零样本检索、分类等任务的效果，且具有良好的可扩展性和适应性。


<details>
  <summary>Details</summary>
Motivation: 生成模型产生的高质量图像为机器学习模型提供了丰富的数据，但直接将其和真实图像混用会导致模态差异问题，甚至模型崩溃。因此，亟需一种合理利用生成图像的方法。

Method: GMAIL方法将生成图像作为独立模态，首先在生成图像上微调模型，并引入跨模态对齐损失，然后用对齐后的模型结合生成图像训练多种视觉-语言模型，在潜在空间内实现对真实与生成模态的有效融合。

Result: 该框架在大量的实验证明了有效性：如在图像描述、零样本图像检索、图像分类和长文本检索等任务上有显著提升，尤其在多模态大模型（如LLaVA）图像描述性能方面表现优异。

Conclusion: 通过区分和对齐生成图像与真实图像模态，GMAIL可充分发挥生成数据的潜力，显著提升多个视觉-语言任务效果，并易于和各种模型结合，具备广泛应用前景。

Abstract: Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined GMAIL, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images. By aligning the two modalities, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks. Our framework can be easily incorporated with various vision-language models, and we demonstrate its efficacy throughout extensive experiments. For example, our framework significantly improves performance on image captioning, zero-shot image retrieval, zero-shot image classification, and long caption retrieval tasks. It also shows positive generated data scaling trends and notable enhancements in the captioning performance of the large multimodal model, LLaVA.

</details>


### [18] [Bridging Day and Night: Target-Class Hallucination Suppression in Unpaired Image Translation](https://arxiv.org/abs/2602.15383)
*Shuwei Li,Lei Tan,Robby T. Tan*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的方法，解决了日间到夜间图像无监督转换中目标类别语义幻觉问题，并在下游任务表现上大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督日夜图像转换方法容易生成语义幻觉，例如交通标志、车辆和人造灯光被错误合成，严重影响下游的检测和识别性能。需要方法抑制这些幻觉以提升下游任务的鲁棒性。

Method: 设计了具有双头判别器的网络，判别器同时执行语义分割用于检测背景中的幻觉内容；并提出类别特定原型，通过聚合目标域标注对象的特征作为语义锚点。基于薛定谔桥图像转换模型，实现多次迭代优化，将检测到的幻觉特征在特征空间中远离类别原型，保持目标语义一致性。

Result: 在BDD100K数据集上，经定性和定量分析证实该方法显著优于现有方法。在日到夜领域自适应任务上，mAP提升了15.5%，对易幻觉类别如交通灯mAP提升高达31.7%。

Conclusion: 该工作有效抑制了日-夜无监督图像转换中的目标类别语义幻觉，大幅提升了下游任务性能，表明其在实际应用中具有重要价值。

Abstract: Day-to-night unpaired image translation is important to downstream tasks but remains challenging due to large appearance shifts and the lack of direct pixel-level supervision. Existing methods often introduce semantic hallucinations, where objects from target classes such as traffic signs and vehicles, as well as man-made light effects, are incorrectly synthesized. These hallucinations significantly degrade downstream performance. We propose a novel framework that detects and suppresses hallucinations of target-class features during unpaired translation. To detect hallucination, we design a dual-head discriminator that additionally performs semantic segmentation to identify hallucinated content in background regions. To suppress these hallucinations, we introduce class-specific prototypes, constructed by aggregating features of annotated target-domain objects, which act as semantic anchors for each class. Built upon a Schrodinger Bridge-based translation model, our framework performs iterative refinement, where detected hallucination features are explicitly pushed away from class prototypes in feature space, thus preserving object semantics across the translation trajectory.Experiments show that our method outperforms existing approaches both qualitatively and quantitatively. On the BDD100K dataset, it improves mAP by 15.5% for day-to-night domain adaptation, with a notable 31.7% gain for classes such as traffic lights that are prone to hallucinations.

</details>


### [19] [Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schrödinger Bridge Matching](https://arxiv.org/abs/2602.15396)
*Jeongwoo Shin,Jinhwan Sul,Joonseok Lee,Jaewong Choi,Jaemoo Choi*

Main category: cs.CV

TL;DR: 本文提出了Adjoint Schrödinger Bridge Matching (ASBM)生成模型，通过构建最优轨迹，实现高维数据的高效生成，并且在采样效率和生成质量上优于传统扩散模型。


<details>
  <summary>Details</summary>
Motivation: 传统的扩散模型由于正向过程缺乏信息性且无记忆，导致生成轨迹弯曲且得分受噪声影响大，限制了高维数据的高效生成。作者希望通过引入更优的轨迹建模方式来提升采样效率和生成质量。

Method: ASBM方法分为两步：首先，把Schrödinger桥的正向动力学作为耦合构建问题，通过从数据到能量先验的采样视角进行学习；其次，用诱导出的最优耦合监督下的简单匹配损失，学习逆向生成动力学。该方法摒弃了无记忆机制，实现了更直且高效的采样路径。

Result: 实验证明ASBM在高维数据（如图像生成）任务中，提高了生成的保真度，并且用更少的采样步骤实现了更好的性能。与已有方法相比，ASBM在高维场景下表现出更好的稳定性和效率。

Conclusion: ASBM通过最优轨迹的建模，改善了扩散模型在高维数据生成中的采样效率和稳定性，具有实际应用潜力，并且可通过蒸馏技术实现一步生成器。

Abstract: Diffusion models often yield highly curved trajectories and noisy score targets due to an uninformative, memoryless forward process that induces independent data-noise coupling. We propose Adjoint Schrödinger Bridge Matching (ASBM), a generative modeling framework that recovers optimal trajectories in high dimensions via two stages. First, we view the Schrödinger Bridge (SB) forward dynamic as a coupling construction problem and learn it through a data-to-energy sampling perspective that transports data to an energy-defined prior. Then, we learn the backward generative dynamic with a simple matching loss supervised by the induced optimal coupling. By operating in a non-memoryless regime, ASBM produces significantly straighter and more efficient sampling paths. Compared to prior works, ASBM scales to high-dimensional data with notably improved stability and efficiency. Extensive experiments on image generation show that ASBM improves fidelity with fewer sampling steps. We further showcase the effectiveness of our optimal trajectory via distillation to a one-step generator.

</details>


### [20] [Emergent Morphing Attack Detection in Open Multi-modal Large Language Models](https://arxiv.org/abs/2602.15461)
*Marija Ivanovska,Vitomir Štruc*

Main category: cs.CV

TL;DR: 本文首次系统性地评估了开源多模态大语言模型（MLLMs）在无需训练的情况下对单张人脸变形攻击检测（MAD）的表现，结果远超特定任务专用的传统方法。


<details>
  <summary>Details</summary>
Motivation: 人脸变形攻击对生物识别系统构成威胁，现有检测方法泛化能力差且依赖任务特定训练。本文探索开源MLLMs在此领域的潜力，并填补其在生物识别取证领域应用的空白。

Method: 采用零样本评估的方式，利用开源MLLMs的公开权重和标准化协议，在多种人脸变换攻击类型下系统性测试模型检测能力，无需进行模型微调或领域适配。

Result: 许多MLLMs展现出较强的无监督分辨能力，LLaVA1.6-Mistral-7B的检测准确率显著领先，EER指标提升至少23%。

Conclusion: 多模态大模型具备无训练检测人脸变形的能力，是可解释性强、可复现且具竞争力的生物识别安全基础和检测工具。未来有望通过微调获得更高性能，并推动该领域发展。

Abstract: Face morphing attacks threaten biometric verification, yet most morphing attack detection (MAD) systems require task-specific training and generalize poorly to unseen attack types. Meanwhile, open-source multimodal large language models (MLLMs) have demonstrated strong visual-linguistic reasoning, but their potential in biometric forensics remains underexplored. In this paper, we present the first systematic zero-shot evaluation of open-source MLLMs for single-image MAD, using publicly available weights and a standardized, reproducible protocol. Across diverse morphing techniques, many MLLMs show non-trivial discriminative ability without any fine-tuning or domain adaptation, and LLaVA1.6-Mistral-7B achieves state-of-the-art performance, surpassing highly competitive task-specific MAD baselines by at least 23% in terms of equal error rate (EER). The results indicate that multimodal pretraining can implicitly encode fine-grained facial inconsistencies indicative of morphing artifacts, enabling zero-shot forensic sensitivity. Our findings position open-source MLLMs as reproducible, interpretable, and competitive foundations for biometric security and forensic image analysis. This emergent capability also highlights new opportunities to develop state-of-the-art MAD systems through targeted fine-tuning or lightweight adaptation, further improving accuracy and efficiency while preserving interpretability. To support future research, all code and evaluation protocols will be released upon publication.

</details>


### [21] [RPT-SR: Regional Prior attention Transformer for infrared image Super-Resolution](https://arxiv.org/abs/2602.15490)
*Youngwan Jin,Incheol Park,Yagiz Nalcakan,Hyeongjin Ju,Sanghyeop Yeo,Shiho Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的适用于红外图像超分辨的视觉Transformer模型，通过引入区域先验信息提升了传统方法在固定或近静态视角下的性能，实现了多波段红外图像的新SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有主流的通用超分辨模型（尤其是基于Vision Transformer）在红外监控、自动驾驶等固定或准静态场景中存在效率低下的问题，因为这些模型未能利用场景中的空间先验信息，导致学习重复，效果不佳。

Method: 提出了RPT-SR模型，在自注意力机制中显式编码场景布局信息。其核心是“区域先验token+局部token”的双token机制：区域先验token作为全局结构的记忆，局部token则聚焦于当前帧内容，两者融合后参与注意力计算。这样先验能动态调节局部重建过程。

Result: 通过大量实验，RPT-SR在不同数据集和红外波段（LWIR，SWIR）上实现了新的最优性能，验证了新方法的普适性和有效性。

Conclusion: 通过显式引入区域空间先验，RPT-SR能有效提升红外超分任务中Transformer模型性能，尤其适用于视角固定场景，具备广泛的红外应用潜力。

Abstract: General-purpose super-resolution models, particularly Vision Transformers, have achieved remarkable success but exhibit fundamental inefficiencies in common infrared imaging scenarios like surveillance and autonomous driving, which operate from fixed or nearly-static viewpoints. These models fail to exploit the strong, persistent spatial priors inherent in such scenes, leading to redundant learning and suboptimal performance. To address this, we propose the Regional Prior attention Transformer for infrared image Super-Resolution (RPT-SR), a novel architecture that explicitly encodes scene layout information into the attention mechanism. Our core contribution is a dual-token framework that fuses (1) learnable, regional prior tokens, which act as a persistent memory for the scene's global structure, with (2) local tokens that capture the frame-specific content of the current input. By utilizing these tokens into an attention, our model allows the priors to dynamically modulate the local reconstruction process. Extensive experiments validate our approach. While most prior works focus on a single infrared band, we demonstrate the broad applicability and versatility of RPT-SR by establishing new state-of-the-art performance across diverse datasets covering both Long-Wave (LWIR) and Short-Wave (SWIR) spectra

</details>


### [22] [LEADER: Lightweight End-to-End Attention-Gated Dual Autoencoder for Robust Minutiae Extraction](https://arxiv.org/abs/2602.15493)
*Raffaele Cappelli,Matteo Ferrara*

Main category: cs.CV

TL;DR: 本文提出了一种全新的轻量级端到端指纹细节点提取网络LEADER，实现了从原始指纹图像到细节点描述的直接映射，具备高效与领先的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统指纹细节点提取往往需要分别进行预处理与后处理，现有端到端方案较少且尚未解决效率与精度兼顾的问题。为此，作者希望设计一个高效、真正端到端的神经网络模型，在简化流程的同时提升准确率并具备良好泛化能力。

Method: 作者提出LEADER模型（Lightweight End-to-end Attention-gated Dual autoencodER），将指纹原图直接映射为细节点的位置信息、方向和类型。该架构融合了非极大值抑制、角度解码及“Castle-Moat-Rampart”编码方法，并采用注意力门控的双自编码器结构。模型参数量仅0.9M，支持高效推理。

Result: LEADER在指纹主流评测集上表现优异，特别是在NIST SD27数据集上，F1-score比专业的潜在指纹特征提取器高34%。在样本细粒度评测中，LEADER平均排名2.07，近一半样本排名第一，大幅优于其他方法。此外，其特征表征与领域内经典特征对齐。

Conclusion: LEADER实现了结构轻量、推理速度快、识别精度高的端到端指纹细节点提取，表现超越主流商用软件，具备广泛实用和研究价值。代码与模型已公开，有助于领域复现和进一步发展。

Abstract: Minutiae extraction, a fundamental stage in fingerprint recognition, is increasingly shifting toward deep learning. However, truly end-to-end methods that eliminate separate preprocessing and postprocessing steps remain scarce. This paper introduces LEADER (Lightweight End-to-end Attention-gated Dual autoencodER), a neural network that maps raw fingerprint images to minutiae descriptors, including location, direction, and type. The proposed architecture integrates non-maximum suppression and angular decoding to enable complete end-to-end inference using only 0.9M parameters. It employs a novel "Castle-Moat-Rampart" ground-truth encoding and a dual-autoencoder structure, interconnected through an attention-gating mechanism. Experimental evaluations demonstrate state-of-the-art accuracy on plain fingerprints and robust cross-domain generalization to latent impressions. Specifically, LEADER attains a 34% higher F1-score on the NIST SD27 dataset compared to specialized latent minutiae extractors. Sample-level analysis on this challenging benchmark reveals an average rank of 2.07 among all compared methods, with LEADER securing the first-place position in 47% of the samples-more than doubling the frequency of the second-best extractor. The internal representations learned by the model align with established fingerprint domain features, such as segmentation masks, orientation fields, frequency maps, and skeletons. Inference requires 15ms on GPU and 322ms on CPU, outperforming leading commercial software in computational efficiency. The source code and pre-trained weights are publicly released to facilitate reproducibility.

</details>


### [23] [Semantic-Guided 3D Gaussian Splatting for Transient Object Removal](https://arxiv.org/abs/2602.15516)
*Aditi Prabakaran,Priyesh Shukla*

Main category: cs.CV

TL;DR: 本文提出了一种基于语义过滤的新方法，通过引入视觉-语言模型，实现了3D高斯散射（3DGS）重建中瞬态物体的高效去除，提升了重建质量且只需极小的内存开销。


<details>
  <summary>Details</summary>
Motivation: 3DGS多视角重建因瞬态物体（如行人、车辆等）的存在常出现重影伪影，现有方法依赖运动检测或场景分解，但要么易受视差影响，要么极耗内存，难以实用。因此亟需一种低开销且稳健的方法去除瞬态物体。

Method: 作者提出了一种语义过滤框架：训练过程中，利用CLIP视觉-语言模型对各高斯体素渲染图像与“干扰物”文本提示做相似度打分，累计记录并对超过阈值的高斯体素进行透明度惩罚与周期性修剪。此方法通过语义分类消除了对运动检测的依赖，从根本上解决了视差歧义问题。

Result: 在RobustNeRF基准上，作者方法在四组数据中均优于原生3DGS。该方法保持了实时渲染和低内存消耗。定量评价与对比实验也验证了阈值设置合理、语义指导的实效性。

Conclusion: 利用语义识别替代运动检测，不仅可有效去除具有已知类别的瞬态物体，还规避了运动分析的固有限制，为3DGS等神经渲染系统提供了内存友好、实用的新思路。

Abstract: Transient objects in casual multi-view captures cause ghosting artifacts in 3D Gaussian Splatting (3DGS) reconstruction. Existing solutions relied on scene decomposition at significant memory cost or on motion-based heuristics that were vulnerable to parallax ambiguity. A semantic filtering framework was proposed for category-aware transient removal using vision-language models. CLIP similarity scores between rendered views and distractor text prompts were accumulated per-Gaussian across training iterations. Gaussians exceeding a calibrated threshold underwent opacity regularization and periodic pruning. Unlike motion-based approaches, semantic classification resolved parallax ambiguity by identifying object categories independently of motion patterns. Experiments on the RobustNeRF benchmark demonstrated consistent improvement in reconstruction quality over vanilla 3DGS across four sequences, while maintaining minimal memory overhead and real-time rendering performance. Threshold calibration and comparisons with baselines validated semantic guidance as a practical strategy for transient removal in scenarios with predictable distractor categories.

</details>


### [24] [Advanced Acceptance Score: A Holistic Measure for Biometric Quantification](https://arxiv.org/abs/2602.15535)
*Aman Verma,Seshan Srirangarajan,Sumantra Dutta Roy*

Main category: cs.CV

TL;DR: 该论文提出了一套新的手势生物特征分数评价指标，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 当前手势生物特征分数的评价主要依赖于误差率，但误差率不足以直接反映分数本身的优劣。因此，急需设计更合理的分数评价方法。

Method: 作者提出综合排序顺序、输出分数相关性、高低分奖励、趋势对应性和身份特征解耦等因素，设计了一个集成多因素加权的综合评价分数（advanced acceptance score）。

Result: 在三个数据集和五个SOTA模型上实验，结果显示用该方法选出的最佳分数优于现有评价方法，并且与现有评价指标高度相关，从而验证了方法的可靠性。

Conclusion: 该文提出的衡量手势生物特征分数优劣的新指标更合理有效，可作为现有评价体系的重要补充，并已开源实现代码。

Abstract: Quantifying biometric characteristics within hand gestures involve derivation of fitness scores from a gesture and identity aware feature space. However, evaluating the quality of these scores remains an open question. Existing biometric capacity estimation literature relies upon error rates. But these rates do not indicate goodness of scores. Thus, in this manuscript we present an exhaustive set of evaluation measures. We firstly identify ranking order and relevance of output scores as the primary basis for evaluation. In particular, we consider both rank deviation as well as rewards for: (i) higher scores of high ranked gestures and (ii) lower scores of low ranked gestures. We also compensate for correspondence between trends of output and ground truth scores. Finally, we account for disentanglement between identity features of gestures as a discounting factor. Integrating these elements with adequate weighting, we formulate advanced acceptance score as a holistic evaluation measure. To assess effectivity of the proposed we perform in-depth experimentation over three datasets with five state-of-the-art (SOTA) models. Results show that the optimal score selected with our measure is more appropriate than existing other measures. Also, our proposed measure depicts correlation with existing measures. This further validates its reliability. We have made our \href{https://github.com/AmanVerma2307/MeasureSuite}{code} public.

</details>


### [25] [Dynamic Training-Free Fusion of Subject and Style LoRAs](https://arxiv.org/abs/2602.15539)
*Qinglong Cao,Yuntian Chen,Chao Ma,Xiaokang Yang*

Main category: cs.CV

TL;DR: 本文提出了一种动态、零训练的新颖LoRA融合方法，能根据特征和评估指标自适应地融合主体与风格LoRA，实现高质量的主题-风格合成，且无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA融合方法通常使用静态的统计规则融合多个LoRA，但这种方式并未发挥LoRA本身对自适应特征调整的能力，并且忽略了输入样本的随机性，限制了合成表现。作者希望提出一种在生成过程中动态调整的融合框架，以提升主体与风格的自适应合成效果。

Method: 该方法为动态、免训练的融合框架。在前向传播中，每个应用LoRA的层会动态计算基础模型与不同LoRA（主体与风格）输出特征的KL散度，自适应选择更合适的权重融合。在反向去噪阶段，基于CLIP和DINO等客观指标，动态进行梯度修正，对生成方向做持续的语义和风格引导。两种机制相结合，实现了生动、连贯的主题-风格合成。

Result: 在多种主体与风格组合的大量实验下，本文方法无论定性还是定量表现均超越了现有的LoRA融合方法，实现了更优的合成质量。

Conclusion: 该方法无需重新训练，通过动态的特征选择与指标引导调整，实现了更高效和高质量的主题-风格融合，对LoRA相关生成任务有重要推动作用。

Abstract: Recent studies have explored the combination of multiple LoRAs to simultaneously generate user-specified subjects and styles. However, most existing approaches fuse LoRA weights using static statistical heuristics that deviate from LoRA's original purpose of learning adaptive feature adjustments and ignore the randomness of sampled inputs. To address this, we propose a dynamic training-free fusion framework that operates throughout the generation process. During the forward pass, at each LoRA-applied layer, we dynamically compute the KL divergence between the base model's original features and those produced by subject and style LoRAs, respectively, and adaptively select the most appropriate weights for fusion. In the reverse denoising stage, we further refine the generation trajectory by dynamically applying gradient-based corrections derived from objective metrics such as CLIP and DINO scores, providing continuous semantic and stylistic guidance. By integrating these two complementary mechanisms-feature-level selection and metric-guided latent adjustment-across the entire diffusion timeline, our method dynamically achieves coherent subject-style synthesis without any retraining. Extensive experiments across diverse subject-style combinations demonstrate that our approach consistently outperforms state-of-the-art LoRA fusion methods both qualitatively and quantitatively.

</details>


### [26] [Revealing and Enhancing Core Visual Regions: Harnessing Internal Attention Dynamics for Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2602.15556)
*Guangtao Lyu,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Xueting Li,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的新方法（PADE），通过增强多模态大模型内部正向注意力动力来缓解幻觉问题，提高模型视觉基础和一致性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（LVLMs）在多模态推理方面表现出色，但仍存在“幻觉”现象，即模型输出与视觉输入或用户指令不一致。现有无训练的方法要么计算量大，要么容易受到注意力下沉影响，效果有限。作者意图通过内部关注机制，探索更高效稳健的解决方案。

Method: 作者观察到LVLM内部的正向注意力动力能在注意力下沉情况下指示语义核心视觉区域，提出PADE方法。PADE无需额外训练，利用PAD图定位核心视觉区域，通过头部逐项中位绝对偏差缩放自适应干预强度，并引入系统Token补偿机制以兼顾复杂指令和输出一致性。

Result: 在多个LVLM和公开基准测试下，PADE显著提升了模型的视觉定位能力和推理一致性，有效降低了幻觉发生率。

Conclusion: 通过利用模型内部的正向注意力动力，PADE方法可以提升多模态大模型的可靠性和推理表现，是一种高效、稳健且无需训练的技术路线。

Abstract: LVLMs have achieved strong multimodal reasoning capabilities but remain prone to hallucinations, producing outputs inconsistent with visual inputs or user instructions. Existing training-free methods, including contrastive decoding and auxiliary expert models, which incur several times more computational overhead and may introduce potential interference, as well as static internal signal enhancement, are often vulnerable to the attention sink phenomenon. We find that internal Positive Attention Dynamics (PAD) in LVLMs naturally reveal semantically core visual regions under the distortions of attention sinks. Based on this, we propose Positive Attention Dynamics Enhancement (PADE), a training-free attention intervention that constructs a PAD map to identify semantically core visual regions, applies per-head Median Absolute Deviation Scaling to adaptively control the intervention strength, and leverages System-Token Compensation to maintain attention to complex user instructions and support long-term output consistency. Experiments on multiple LVLMs and benchmarks show that PADE improves visual grounding and reduces hallucinations, validating the effectiveness of leveraging internal attention dynamics for reliable multimodal reasoning.

</details>


### [27] [Intracoronary Optical Coherence Tomography Image Processing and Vessel Classification Using Machine Learning](https://arxiv.org/abs/2602.15579)
*Amal Lahchim,Lambros Athanasiou*

Main category: cs.CV

TL;DR: 本文提出了一种全自动、基于机器学习的冠状动脉OCT图像血管分割与分类方法，能够高效、准确地实现像素级的血管识别，实验结果显示其精度和召回率非常高。


<details>
  <summary>Details</summary>
Motivation: OCT图像因高分辨率而在冠状动脉成像中应用广泛，但普遍存在噪声、伪影和复杂组织结构，导致自动化血管识别和分割具有挑战性。为提高OCT图像分析的准确性和效率，亟需开发无需大量人工干预的自动化方法。

Method: 作者设计了一套全自动流水线：包括图像预处理、导丝伪影去除、极坐标到笛卡尔坐标转换、K均值聚类进行无监督特征抽取，以及局部特征提取。随后采用逻辑回归和支持向量机分别对像素进行血管分类。

Result: 所提出方法在实验中表现优秀，分割精度、召回率和F1分数最高可达1.00，整体分类准确率高达99.68%。

Conclusion: 该研究提出的方法能够准确高效进行OCT血管分割和分类，且计算复杂度低、对人工标注需求少，有望在临床辅助决策和实时医学图像处理等领域获得实际应用。

Abstract: Intracoronary Optical Coherence Tomography (OCT) enables high-resolution visualization of coronary vessel anatomy but presents challenges due to noise, imaging artifacts, and complex tissue structures. This paper proposes a fully automated pipeline for vessel segmentation and classification in OCT images using machine learning techniques. The proposed method integrates image preprocessing, guidewire artifact removal, polar-to-Cartesian transformation, unsupervised K-means clustering, and local feature extraction. These features are used to train Logistic Regression and Support Vector Machine classifiers for pixel-wise vessel classification. Experimental results demonstrate excellent performance, achieving precision, recall, and F1-score values up to 1.00 and overall classification accuracy of 99.68%. The proposed approach provides accurate vessel boundary detection while maintaining low computational complexity and requiring minimal manual annotation. This method offers a reliable and efficient solution for automated OCT image analysis and has potential applications in clinical decision support and real-time medical image processing.

</details>


### [28] [An Industrial Dataset for Scene Acquisitions and Functional Schematics Alignment](https://arxiv.org/abs/2602.15584)
*Flavien Armangeon,Thibaud Ehret,Enric Meinhardt-Llopis,Rafael Grompone von Gioi,Guillaume Thibault,Marc Petit,Gabriele Facciolo*

Main category: cs.CV

TL;DR: 本文提出了IRIS-v2数据集，用于支持功能示意图与2D/3D场景对齐相关的研究，尤其关注于缺乏数字模型的工业设施。通过结合分割与图匹配方法，验证了在实际案例中提高对齐效率的可行性。


<details>
  <summary>Details</summary>
Motivation: 许多老旧工业设施缺乏原生数字模型，现有通过图像和激光雷达进行手工对齐的方法效率低且复杂，制约了数字孪生等应用发展。此外，工业场景的复杂性与现实与示意图之间的差异以及缺乏公开数据集，使得此领域研究难度高且受到限制。本文旨在解决上述挑战，推动自动化对齐方法的发展。

Method: 论文构建了一个包括多模态信息的综合数据集IRIS-v2，包含图像、点云、2D标注框与分割掩码、CAD模型、3D管道信息以及P&ID示意图。研究在数据集上采用分割和图匹配相结合的方式进行对齐实验，评估其在实际案例中的效率表现。

Result: 实验结果表明，结合分割与图匹配能够显著减少手工对齐所需时间，提升工业场景中示意图与2D/3D实际数据的对应效率。

Conclusion: 本文的数据集为工业设施数字化对齐问题研究提供了新的基础资源，所提出和验证的方法为未来实现高效自动对齐和数字孪生构建迈出了重要一步。

Abstract: Aligning functional schematics with 2D and 3D scene acquisitions is crucial for building digital twins, especially for old industrial facilities that lack native digital models. Current manual alignment using images and LiDAR data does not scale due to tediousness and complexity of industrial sites. Inconsistencies between schematics and reality, and the scarcity of public industrial datasets, make the problem both challenging and underexplored. This paper introduces IRIS-v2, a comprehensive dataset to support further research. It includes images, point clouds, 2D annotated boxes and segmentation masks, a CAD model, 3D pipe routing information, and the P&ID (Piping and Instrumentation Diagram). The alignment is experimented on a practical case study, aiming at reducing the time required for this task by combining segmentation and graph matching.

</details>


### [29] [Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation](https://arxiv.org/abs/2602.15650)
*Marco Salmè,Federico Siciliano,Fabrizio Silvestri,Paolo Soda,Rosa Sicilia,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 提出了一种结合临床可解释性和事实准确性的多模态增强生成框架CEMRAG，用于提高放射学报告生成系统的可靠性与透明度。


<details>
  <summary>Details</summary>
Motivation: 传统视觉-语言模型用于自动生成放射学报告时，易出现“幻觉”现象，即报告内容与影像证据不符，并且缺乏透明性，因此难以临床采纳。目前，大多数方法分别关注解释性或准确性，无法兼顾两者。

Method: 提出了CEMRAG框架：首先将视觉信息分解为可解释的临床概念，然后结合多模态检索增强生成（RAG），通过丰富的上下文提示提升报告生成，兼顾可解释性与事实准确性。

Result: 在MIMIC-CXR及IU X-Ray数据集上、多种VLM结构和训练/检索设置下，CEMRAG在临床准确性及NLP标准指标上均优于传统RAG和仅用概念的方法，实现了解释性与性能的兼得。

Conclusion: 透明化视觉概念不仅不会牺牲诊断准确性，还能提升医疗VLM的可靠性，CEMRAG为实现临床可信的AI辅助放射学系统提供了新思路与可扩展设计。

Abstract: Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transparency, while Retrieval-Augmented Generation (RAG) methods targeting factual grounding through external retrieval. We present Concept-Enhanced Multimodal RAG (CEMRAG), a unified framework that decomposes visual representations into interpretable clinical concepts and integrates them with multimodal RAG. This approach exploits enriched contextual prompts for RRG, improving both interpretability and factual accuracy. Experiments on MIMIC-CXR and IU X-Ray across multiple VLM architectures, training regimes, and retrieval configurations demonstrate consistent improvements over both conventional RAG and concept-only baselines on clinical accuracy metrics and standard NLP measures. These results challenge the assumed trade-off between interpretability and performance, showing that transparent visual concepts can enhance rather than compromise diagnostic accuracy in medical VLMs. Our modular design decomposes interpretability into visual transparency and structured language model conditioning, providing a principled pathway toward clinically trustworthy AI-assisted radiology.

</details>


### [30] [A Novel Public Dataset for Strawberry (Fragaria x ananassa) Ripeness Detection and Comparative Evaluation of YOLO-Based Models](https://arxiv.org/abs/2602.15656)
*Mustafa Yurdakul,Zeynep Sena Bastug,Ali Emre Gok,Sakir Taşdemir*

Main category: cs.CV

TL;DR: 本文提出了一个新的、公开可用的草莓成熟度数据集，并在该数据集上比较了多种YOLO模型的检测性能，为智能农业中的果实采摘自动化奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 传统依靠人工视觉判断草莓成熟度的方法主观性强、误差大，且该领域缺乏公开且多样化的数据集，限制了算法比较与应用发展。因此，亟需高质量公开数据集和有效的自动检测方法。

Method: 作者在土耳其两座不同的温室、不同光照和环境条件下，采集并标注了566张图片、1201个草莓样本，发布了该成熟度数据集。基于该数据集对YOLOv8、YOLOv9及YOLO11不同模型进行检测实验并对比指标。

Result: YOLOv9c在精度上最高（90.94%），YOLO11s在召回率最高（83.74%），总体性能（mAP@50）以YOLOv8s最佳，达到86.09%。小型和中型模型在该数据集上表现均衡且高效。

Conclusion: 新数据集为相关研究提供了重要基准。小型/中型目标检测模型适合用于草莓自动成熟度检测，为智能农业和自动化采摘系统搭建了基础平台。

Abstract: The strawberry (Fragaria x ananassa), known worldwide for its economic value and nutritional richness, is a widely cultivated fruit. Determining the correct ripeness level during the harvest period is crucial for both preventing losses for producers and ensuring consumers receive a quality product. However, traditional methods, i.e., visual assessments alone, can be subjective and have a high margin of error. Therefore, computer-assisted systems are needed. However, the scarcity of comprehensive datasets accessible to everyone in the literature makes it difficult to compare studies in this field. In this study, a new and publicly available strawberry ripeness dataset, consisting of 566 images and 1,201 labeled objects, prepared under variable light and environmental conditions in two different greenhouses in Turkey, is presented to the literature. Comparative tests conducted on the data set using YOLOv8, YOLOv9, and YOLO11-based models showed that the highest precision value was 90.94% in the YOLOv9c model, while the highest recall value was 83.74% in the YOLO11s model. In terms of the general performance criterion mAP@50, YOLOv8s was the best performing model with a success rate of 86.09%. The results show that small and medium-sized models work more balanced and efficiently on this type of dataset, while also establishing a fundamental reference point for smart agriculture applications.

</details>


### [31] [Bayesian Optimization for Design Parameters of 3D Image Data Analysis](https://arxiv.org/abs/2602.15660)
*David Exler,Joaquin Eduardo Urrutia Gómez,Martin Krüger,Maike Schliephake,John Jbeily,Mario Vitacolonna,Rüdiger Rudolf,Markus Reischl*

Main category: cs.CV

TL;DR: 本文提出了一种基于贝叶斯优化的3D数据分析优化流程（Pipeline），能够自动化选择分割和分类模型及其参数，减少人工干预。通过在四个案例中验证，该方法能高效地为不同数据集寻找有效的配置。


<details>
  <summary>Details</summary>
Motivation: 在大规模3D生物医学成像中，深度学习分割和分类至关重要，但模型选择与参数调优是实际应用瓶颈，且手动分析不可行。因此，需要一种自动化、高效的流程来辅助模型选择与参数调优。

Method: 提出3D数据分析优化流程，包括两个贝叶斯优化阶段：第一阶段优化分割模型和后处理参数，引入一个新的分割质量指标作为目标函数；第二阶段优化分类器设计（编码器、头部结构、先验知识利用、预训练策略），并包含半自动化的类别标注流程以减少人工标注工作。

Result: 在四个案例研究中，该流程高效地识别出了各自数据集的有效模型和参数配置。

Conclusion: 所提出的流程能显著简化3D生物医学数据的分割和分类模型选择与参数优化流程，减少人工参与，提高效率。

Abstract: Deep learning-based segmentation and classification are crucial to large-scale biomedical imaging, particularly for 3D data, where manual analysis is impractical. Although many methods exist, selecting suitable models and tuning parameters remains a major bottleneck in practice. Hence, we introduce the 3D data Analysis Optimization Pipeline, a method designed to facilitate the design and parameterization of segmentation and classification using two Bayesian Optimization stages. First, the pipeline selects a segmentation model and optimizes postprocessing parameters using a domain-adapted syntactic benchmark dataset. To ensure a concise evaluation of segmentation performance, we introduce a segmentation quality metric that serves as the objective function. Second, the pipeline optimizes design choices of a classifier, such as encoder and classifier head architectures, incorporation of prior knowledge, and pretraining strategies. To reduce manual annotation effort, this stage includes an assisted class-annotation workflow that extracts predicted instances from the segmentation results and sequentially presents them to the operator, eliminating the need for manual tracking. In four case studies, the 3D data Analysis Optimization Pipeline efficiently identifies effective model and parameter configurations for individual datasets.

</details>


### [32] [RaCo: Ranking and Covariance for Practical Learned Keypoints](https://arxiv.org/abs/2602.15755)
*Abhiram Shenoi,Philipp Lindenberger,Paul-Edouard Sarlin,Marc Pollefeys*

Main category: cs.CV

TL;DR: RaCo是一种高效、轻量级的神经网络，能够在不依赖共视图像对和额外标签的情况下，学习用于多种3D视觉任务的鲁棒关键点，兼具重复性强和匹配能力佳。


<details>
  <summary>Details</summary>
Motivation: 在3D计算机视觉任务中，检测具有高重复性和强区分能力的关键点是基础难题。现有方法常需共视图像、复杂网络或额外标注，导致训练或部署不便。本文动机是提出一种无需这些复杂依赖，同时能应对大旋转变化，且计算高效的通用关键点检测方案。

Method: RaCo网络设计包含三个核心模块：1) 可重复关键点检测器；2) 可微分排序器，用于在有限关键点下最大化匹配数；3) 协方差估计器，用于量化空间不确定度。训练仅用透视裁剪的单幅图像，并通过数据增强实现旋转鲁棒性，无需等变网络结构或共视数据。

Result: RaCo在多个具有挑战性的数据集上进行了评测，在关键点重复性和双视图匹配任务，尤其是大幅面内旋转下，均取得了当前最优的性能。

Conclusion: RaCo是一种简单有效的新方案，能够独立估计关键点排名和度量协方差，无需额外标签，检测结果既可解释又高度可重复，为3D视觉任务中的兴趣点检测提供了新的高效基线。

Abstract: This paper introduces RaCo, a lightweight neural network designed to learn robust and versatile keypoints suitable for a variety of 3D computer vision tasks. The model integrates three key components: the repeatable keypoint detector, a differentiable ranker to maximize matches with a limited number of keypoints, and a covariance estimator to quantify spatial uncertainty in metric scale. Trained on perspective image crops only, RaCo operates without the need for covisible image pairs. It achieves strong rotational robustness through extensive data augmentation, even without the use of computationally expensive equivariant network architectures. The method is evaluated on several challenging datasets, where it demonstrates state-of-the-art performance in keypoint repeatability and two-view matching, particularly under large in-plane rotations. Ultimately, RaCo provides an effective and simple strategy to independently estimate keypoint ranking and metric covariance without additional labels, detecting interpretable and repeatable interest points. The code is available at https://github.com/cvg/RaCo.

</details>


### [33] [Criteria-first, semantics-later: reproducible structure discovery in image-based sciences](https://arxiv.org/abs/2602.15712)
*Jan Bumberger*

Main category: cs.CV

TL;DR: 该论文提出了一种新的图像分析框架，强调在分析初期优先按照形式化标准（准则）提取结构信息，后续再映射到具体语义标签，从而提高科学发现的开放性、通用性与可复现性。


<details>
  <summary>Details</summary>
Motivation: 传统的图像分析方法过于依赖于先验的语义标签或本体，这在实际科学研究中因为领域变化、标签漂移等原因导致分析不具备通用性和长期稳定性；同时，面对开放领域和多源数据时，该方法显得局限。

Method: 作者提出“criteria-first, semantics-later”的分析范式：第一步进行无语义、基于准则的结构发现，形成稳定的结构分区或层级；第二步，再将这些结构映射到具体的语义本体或词表。通过这一分层，语义解释下沉到分析后端，分析前端则注重结构发现本身。

Result: 跨领域的数据和案例分析表明，基于准则的结构提取能够在语义标签无法有效扩展时复现良好的结构发现效果，实现了通用性和适应性。

Conclusion: 推崇以准则为先导、语义为后继的方法有助于提升可复现性、分析通用性及长期可持续监测，也有利于打造符合FAIR原则和AI-ready的数字对象，同时支持复杂的跨语义映射和验证。

Abstract: Across the natural and life sciences, images have become a primary measurement modality, yet the dominant analytic paradigm remains semantics-first. Structure is recovered by predicting or enforcing domain-specific labels. This paradigm fails systematically under the conditions that make image-based science most valuable, including open-ended scientific discovery, cross-sensor and cross-site comparability, and long-term monitoring in which domain ontologies and associated label sets drift culturally, institutionally, and ecologically. A deductive inversion is proposed in the form of criteria-first and semantics-later. A unified framework for criteria-first structure discovery is introduced. It separates criterion-defined, semantics-free structure extraction from downstream semantic mapping into domain ontologies or vocabularies and provides a domain-general scaffold for reproducible analysis across image-based sciences. Reproducible science requires that the first analytic layer perform criterion-driven, semantics-free structure discovery, yielding stable partitions, structural fields, or hierarchies defined by explicit optimality criteria rather than local domain ontologies. Semantics is not discarded; it is relocated downstream as an explicit mapping from the discovered structural product to a domain ontology or vocabulary, enabling plural interpretations and explicit crosswalks without rewriting upstream extraction. Grounded in cybernetics, observation-as-distinction, and information theory's separation of information from meaning, the argument is supported by cross-domain evidence showing that criteria-first components recur whenever labels do not scale. Finally, consequences are outlined for validation beyond class accuracy and for treating structural products as FAIR, AI-ready digital objects for long-term monitoring and digital twins.

</details>


### [34] [ToaSt: Token Channel Selection and Structured Pruning for Efficient ViT](https://arxiv.org/abs/2602.15720)
*Hyunchan Moon,Cheonjun Park,Steven L. Waslander*

Main category: cs.CV

TL;DR: 本文提出了一种名为ToaSt的ViT模型高效剪枝与压缩框架，结合结构化剪枝与高效token压缩，实现显著提升ViT模型推理效率且保持甚至提升准确率。


<details>
  <summary>Details</summary>
Motivation: ViT虽然性能卓越但计算开销大，既有的剪枝或压缩技术要么难以训练、要么带来额外优化难题，需针对性地提升推理效率且保证准确性。

Method: ToaSt框架分别针对ViT两个关键部分：在多头自注意力部分采用基于头的结构化剪枝，利用注意力机制特性提升剪枝鲁棒性；在前馈网络部分提出Token Channel Selection (TCS)，实现更高token压缩比且规避全局传播带来的优化障碍。

Result: ToaSt在包括DeiT、ViT-MAE、Swin Transformer在内的九个ViT变体上均验证了优越性。在ViT-MAE-Huge模型上，Top-1准确率提升至88.52%（提高1.64%），FLOPs减少39.4%。迁移到下游COCO检测任务，mAP从51.9提升至52.2。

Conclusion: ToaSt能够在保持甚至提升模型准确率的同时，大幅降低ViT的计算开销，并显著优于已有的剪枝和压缩方法，具备很好的通用性及迁移性能。

Abstract: Vision Transformers (ViTs) have achieved remarkable success across various vision tasks, yet their deployment is often hindered by prohibitive computational costs. While structured weight pruning and token compression have emerged as promising solutions, they suffer from prolonged retraining times and global propagation that creates optimization challenges, respectively. We propose ToaSt, a decoupled framework applying specialized strategies to distinct ViT components. We apply coupled head-wise structured pruning to Multi-Head Self-Attention modules, leveraging attention operation characteristics to enhance robustness. For Feed-Forward Networks (over 60\% of FLOPs), we introduce Token Channel Selection (TCS) that enhances compression ratios while avoiding global propagation issues. Our analysis reveals TCS effectively filters redundant noise during selection. Extensive evaluations across nine diverse models, including DeiT, ViT-MAE, and Swin Transformer, demonstrate that ToaSt achieves superior trade-offs between accuracy and efficiency, consistently outperforming existing baselines. On ViT-MAE-Huge, ToaSt achieves 88.52\% accuracy (+1.64 \%) with 39.4\% FLOPs reduction. ToaSt transfers effectively to downstream tasks, cccccachieving 52.2 versus 51.9 mAP on COCO object detection. Code and models will be released upon acceptance.

</details>


### [35] [Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation](https://arxiv.org/abs/2602.15724)
*Shutian Gu,Chengkai Huang,Ruoyu Wang,Lina Yao*

Main category: cs.CV

TL;DR: 本文提出了一种通过双重检索增强的方案，提高大模型（LLM）在视觉-语言导航（VLN）任务中的效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有利用大语言模型作为高层导航器的方法，推理灵活但决策效率低、每步都需重新理解指令且易受干扰，导致性能瓶颈。

Method: 提出了检索增强框架，不改变或微调原有LLM。在整体上，根据指令语义检索相似成功轨迹作为示例，提高指令理解；在每步决策中，通过模仿学习检索，过滤无关导航方向，简化选择空间。这两个检索模块轻量独立，与LLM解耦。

Result: 在Room-to-Room基准测试中，所提方法在已见和未见环境上，其成功率、Oracle成功率与路径效率（SPL）均有提升。消融实验表明，两个检索模块各有助于整体指导与单步决策提升。

Conclusion: 检索增强为LLM导航带来更高效率与可靠性，是一种可扩展的提升VLN性能策略。

Abstract: Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.

</details>


### [36] [Spanning the Visual Analogy Space with a Weight Basis of LoRAs](https://arxiv.org/abs/2602.15727)
*Hila Manor,Rinon Gal,Haggai Maron,Tomer Michaeli,Gal Chechik*

Main category: cs.CV

TL;DR: 提出了一种新方法LoRWeB，通过动态组合基础LoRA模块，实现更灵活和泛化性强的视觉类比变换，在图像类比任务上超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于单一LoRA的视觉类比方法难以泛化和覆盖多样化的视觉变换，因为固定的适应模块限制了类比模型的表达和扩展能力。需要一种能够灵活应对多种视觉变换的新机制。

Method: LoRWeB方法引入两个核心组件：(1) 可学习的LoRA基础模块集合，覆盖不同视觉变换;(2) 一个轻量级编码器，根据输入类比对动态选择和加权这些基础LoRA，通过推理时组合基础模块为每次任务自适应地生成变换能力。

Result: LoRWeB在多项评测数据集上取得了最优的类比任务表现，显著提升了对于新颖和未见过的视觉变换的泛化能力，优于当前主流方法。

Conclusion: 通过LoRA基础模块的可组合机制，可以有效拓展模型的视觉类比能力，提升灵活性及泛化性，对灵活视觉操作有广阔应用前景。

Abstract: Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet $\{\mathbf{a}$, $\mathbf{a}'$, $\mathbf{b}\}$, the goal is to generate $\mathbf{b}'$ such that $\mathbf{a} : \mathbf{a}' :: \mathbf{b} : \mathbf{b}'$. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a "space of LoRAs". We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in https://research.nvidia.com/labs/par/lorweb

</details>


### [37] [Language and Geometry Grounded Sparse Voxel Representations for Holistic Scene Understanding](https://arxiv.org/abs/2602.15734)
*Guile Wu,David Huang,Bingbing Liu,Dongfeng Bai*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的3D开放词汇场景理解方法，将语言与几何信息结合，在统一框架下协同建模场景的外观、语义和几何特征，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D开放词汇场景理解方法重语言特征蒸馏，忽视外观、语义和几何信息的协同，导致场景理解与几何结构偏离、与重建过程脱节。

Method: 提出以3D稀疏体素为基本单元，设计外观域、密度域、特征域和置信域，联合表达3D场景。采用特征调制模块，将2D基础模型中的语言特征蒸馏到3D场景模型，同时引入几何蒸馏，通过深度相关性正则与模式一致性正则从几何基础模型迁移几何知识，实现多域协同建模。

Result: 在整体场景理解与重建任务上，通过大量实验验证，所提方法在综合性能上优于当前最先进方法。

Conclusion: 该方法有效统一了外观、语义和几何的三者协同，提高了3D场景理解和重建的精度，为3D开放词汇场景理解提供了新思路。

Abstract: Existing 3D open-vocabulary scene understanding methods mostly emphasize distilling language features from 2D foundation models into 3D feature fields, but largely overlook the synergy among scene appearance, semantics, and geometry. As a result, scene understanding often deviates from the underlying geometric structure of scenes and becomes decoupled from the reconstruction process. In this work, we propose a novel approach that leverages language and geometry grounded sparse voxel representations to comprehensively model appearance, semantics, and geometry within a unified framework. Specifically, we use 3D sparse voxels as primitives and employ an appearance field, a density field, a feature field, and a confidence field to holistically represent a 3D scene. To promote synergy among the appearance, density, and feature fields, we construct a feature modulation module and distill language features from a 2D foundation model into our 3D scene model. In addition, we integrate geometric distillation into feature field distillation to transfer geometric knowledge from a geometry foundation model to our 3D scene representations via depth correlation regularization and pattern consistency regularization. These components work together to synergistically model the appearance, semantics, and geometry of the 3D scene within a unified framework. Extensive experiments demonstrate that our approach achieves superior overall performance compared with state-of-the-art methods in holistic scene understanding and reconstruction.

</details>


### [38] [Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models](https://arxiv.org/abs/2602.15772)
*Sen Ye,Mengde Xu,Shuyang Gu,Di He,Liwei Wang,Han Hu*

Main category: cs.CV

TL;DR: 当前多模态模型在提升生成能力和理解能力时存在权衡难题。本文提出R3框架，通过“生成-理解-再生”多步过程，有效缓解了这一矛盾，实现了生成与理解能力的双提升。


<details>
  <summary>Details</summary>
Motivation: 多模态模型常常在提升生成能力和理解能力之间难以兼顾，提升一方面会牺牲另一方面。论文旨在分析造成这种竞争关系的根本原因，并寻找解决途径。

Method: 提出了Reason-Reflect-Refine（R3）框架，把传统的一步生成任务分为“生成-理解-再生”三步，生成过程中显式利用模型的理解能力，通过多轮优化提升整体性能。

Result: 采用R3框架的实验表明，模型不仅在生成任务上达到了更优的表现，同时理解能力也得到提升，并缓解了传统模型生成与理解的对立关系。

Conclusion: R3框架为多模态模型的设计提供了新思路，有助于兼顾并提升生成与理解能力，对未来统一多模态模型的发展具有重要参考价值。

Abstract: Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of "generate-understand-regenerate". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.

</details>


### [39] [NeRFscopy: Neural Radiance Fields for in-vivo Time-Varying Tissues from Endoscopy](https://arxiv.org/abs/2602.15775)
*Laura Salort-Benejam,Antonio Agudo*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法NeRFscopy，实现了从单目内窥镜视频自监督地生成新视角和三维重建，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 内窥镜成像对医学诊断和治疗极为重要，但由于组织变形、光照变化、遮挡及未知相机轨迹等问题，现有三维重建方法难以应对内窥镜视频的复杂性。为提升三维可视化和医用辅助效果，亟需创新技术。

Method: 受神经渲染技术启发，作者提出NeRFscopy，通过无监督机制从单目内窥镜视频学习三维隐式模型。具体方法是：融合基于SE(3)的时变变形场和标准辐射场，通过对彩色图像引入高级优化项，无需模板或预训练模型即可重建三维结构。

Result: NeRFscopy在多个复杂内窥镜场景中实现了高精度的新视角合成和三维重建效果，整体上优于当前主流方法。

Conclusion: NeRFscopy为内窥镜视频三维重建和新视角生成提供了有效的新方案，适应性强、精度高，有望提升医学诊疗和手术辅助水平。

Abstract: Endoscopy is essential in medical imaging, used for diagnosis, prognosis and treatment. Developing a robust dynamic 3D reconstruction pipeline for endoscopic videos could enhance visualization, improve diagnostic accuracy, aid in treatment planning, and guide surgery procedures. However, challenges arise due to the deformable nature of the tissues, the use of monocular cameras, illumination changes, occlusions and unknown camera trajectories. Inspired by neural rendering, we introduce NeRFscopy, a self-supervised pipeline for novel view synthesis and 3D reconstruction of deformable endoscopic tissues from a monocular video. NeRFscopy includes a deformable model with a canonical radiance field and a time-dependent deformation field parameterized by SE(3) transformations. In addition, the color images are efficiently exploited by introducing sophisticated terms to learn a 3D implicit model without assuming any template or pre-trained model, solely from data. NeRFscopy achieves accurate results in terms of novel view synthesis, outperforming competing methods across various challenging endoscopy scenes.

</details>


### [40] [Meteorological data and Sky Images meets Neural Models for Photovoltaic Power Forecasting](https://arxiv.org/abs/2602.15782)
*Ines Montoya-Espinagosa,Antonio Agudo*

Main category: cs.CV

TL;DR: 本文提出了一种多模态混合方法，结合天空图像、光伏历史和气象数据，提升光伏发电的短期和长期预测准确率，尤其改善多云天气下的骤变事件预测。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源（特别是太阳能）的应用增多，变动性成为影响光伏发电预测的主要挑战，因此需要更精确且稳健的预测方法以优化电网运行和管理。

Method: 采用混合多模态方法，集成天空图像、历史发电量及气象数据（包括风、太阳位置、下行辐射等），并利用深度神经网络进行短期实时预测（nowcasting）和中长期预测。

Result: 引入气象数据（特别是地表长波辐射、下行辐射、风及太阳位置）后，预测准确率明显提升，在多云天气下改善尤为显著。

Conclusion: 集成多源异构数据能显著提升光伏功率预测的有效性和可靠性，有助于更好地应对可再生能源发电的波动性。

Abstract: Due to the rise in the use of renewable energies as an alternative to traditional ones, and especially solar energy, there is increasing interest in studying how to address photovoltaic forecasting in the face of the challenge of variability in photovoltaic energy production, using different methodologies. This work develops a hybrid approach for short and long-term forecasting based on two studies with the same purpose. A multimodal approach that combines images of the sky and photovoltaic energy history with meteorological data is proposed. The main goal is to improve the accuracy of ramp event prediction, increase the robustness of forecasts in cloudy conditions, and extend capabilities beyond nowcasting, to support more efficient operation of the power grid and better management of solar variability. Deep neural models are used for both nowcasting and forecasting solutions, incorporating individual and multiple meteorological variables, as well as an analytical solar position. The results demonstrate that the inclusion of meteorological data, particularly the surface long-wave, radiation downwards, and the combination of wind and solar position, significantly improves current predictions in both nowcasting and forecasting tasks, especially on cloudy days. This study highlights the importance of integrating diverse data sources to improve the reliability and interpretability of solar energy prediction models.

</details>


### [41] [Context-aware Skin Cancer Epithelial Cell Classification with Scalable Graph Transformers](https://arxiv.org/abs/2602.15783)
*Lucas Sancéré,Noémie Moreau,Katarzyna Bozek*

Main category: cs.CV

TL;DR: 本论文提出了一种基于可扩展图Transformer的新方法，通过在全WSI细胞图上进行分类，有效提升了肿瘤与健康上皮细胞的区分准确率，超过了现有的图像基础方法。


<details>
  <summary>Details</summary>
Motivation: 目前的深度学习方法（如CNN与Vision Transformer）只能处理WSI切片的局部patch，难以获取完整的组织级上下文信息，限制了分割与分类精度，特别是在健康与肿瘤细胞形态极其相似的情况下。本文希望通过引入全WSI级别的细胞图Transformer来弥补这一不足。

Method: 作者综合利用可扩展的Graph Transformer（如SGFormer和DIFFormer），将WSI细胞全部表示成图结构，并利用节点的形态、纹理特征以及周围非上皮细胞类型信息进行训练。进行了单张WSI及多张WSI/多患者的准确率实验，并与最佳图像基础方法做了对比。

Result: 在最具挑战性的cSCC健康与肿瘤上皮细胞识别任务上，两种Graph Transformer方法的均衡准确率分别提高到85.2%和85.1%，显著优于最佳图像方法（81.2%）。多WSI/多患者实验设置下，DIFFormer仍然达到83.6%的准确率，超越图像基础state-of-the-art方法CellViT256的78.1%。

Conclusion: 基于全WSI细胞图的Graph Transformer模型优于传统的patch式图像分析模型，能够更好地捕获细胞微环境和组织级别的上下文信息，有效提升癌症病理识别的准确性。

Abstract: Whole-slide images (WSIs) from cancer patients contain rich information that can be used for medical diagnosis or to follow treatment progress. To automate their analysis, numerous deep learning methods based on convolutional neural networks and Vision Transformers have been developed and have achieved strong performance in segmentation and classification tasks. However, due to the large size and complex cellular organization of WSIs, these models rely on patch-based representations, losing vital tissue-level context. We propose using scalable Graph Transformers on a full-WSI cell graph for classification. We evaluate this methodology on a challenging task: the classification of healthy versus tumor epithelial cells in cutaneous squamous cell carcinoma (cSCC), where both cell types exhibit very similar morphologies and are therefore difficult to differentiate for image-based approaches. We first compared image-based and graph-based methods on a single WSI. Graph Transformer models SGFormer and DIFFormer achieved balanced accuracies of $85.2 \pm 1.5$ ($\pm$ standard error) and $85.1 \pm 2.5$ in 3-fold cross-validation, respectively, whereas the best image-based method reached $81.2 \pm 3.0$. By evaluating several node feature configurations, we found that the most informative representation combined morphological and texture features as well as the cell classes of non-epithelial cells, highlighting the importance of the surrounding cellular context. We then extended our work to train on several WSIs from several patients. To address the computational constraints of image-based models, we extracted four $2560 \times 2560$ pixel patches from each image and converted them into graphs. In this setting, DIFFormer achieved a balanced accuracy of $83.6 \pm 1.9$ (3-fold cross-validation), while the state-of-the-art image-based model CellViT256 reached $78.1 \pm 0.5$.

</details>


### [42] [Task-Agnostic Continual Learning for Chest Radiograph Classification](https://arxiv.org/abs/2602.15811)
*Muthu Subash Kavitha,Anas Zafar,Amgad Muneer,Jia Wu*

Main category: cs.CV

TL;DR: 本文提出了一种针对胸部X光片分类任务的连续学习方法，可在无需访问历史原始数据的情况下兼顾模型更新与性能保留。


<details>
  <summary>Details</summary>
Motivation: 实际临床部署要求胸部X光片分类器具备随新数据集持续更新的能力，且无需重新训练全部历史数据，也不能损害已测得的有效性能。面对异构数据集的持续到来，需要能持续学习且部署灵活的模型。

Method: 作者首次在胸部X光片分类场景下设定任务增量的持续学习问题，且推理时任务身份未知。为此，提出了CARL-XRay方法，采用固定的高容量主干网络，并对每一任务增量分配轻量适配器与分类头。通过任务选择器与原型存储，以及特征级体验重放，实现了不需保存原始影像即可稳定地辨识与适应新任务。

Result: 在多个大型公开胸X光片数据集上，CARL-XRay表现出色，能在任务未知条件下实现比联合训练更好的路由准确率（75.0% vs. 62.5%），并始终保持竞争力的诊断性能（oracle条件下AUROC 0.74，任务未知推理下0.75），且涉及更少可训练参数。

Conclusion: 本研究提出的方法为持续临床部署提供了比联合训练及反复完全重训练更实用的替代方案，有效解决了多任务持续学习与可靠适应的问题。

Abstract: Clinical deployment of chest radiograph classifiers requires models that can be updated as new datasets become available without retraining on previously ob- served data or degrading validated performance. We study, for the first time, a task-incremental continual learning setting for chest radiograph classification, in which heterogeneous chest X-ray datasets arrive sequentially and task identifiers are unavailable at inference. We propose a continual adapter-based routing learning strategy for Chest X-rays (CARL-XRay) that maintains a fixed high-capacity backbone and incrementally allocates lightweight task-specific adapters and classifier heads. A latent task selector operates on task-adapted features and leverages both current and historical context preserved through compact prototypes and feature-level experience replay. This design supports stable task identification and adaptation across sequential updates while avoiding raw-image storage. Experiments on large-scale public chest radiograph datasets demonstrate robust performance retention and reliable task-aware inference under continual dataset ingestion. CARL-XRay outperforms joint training under task-unknown deployment, achieving higher routing accuracy (75.0\% vs.\ 62.5\%), while maintaining competitive diagnostic performance with AUROC of 0.74 in the oracle setting with ground-truth task identity and 0.75 under task-unknown inference, using significantly fewer trainable parameters. Finally, the proposed framework provides a practical alternative to joint training and repeated full retraining in continual clinical deployment.

</details>


### [43] [VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation](https://arxiv.org/abs/2602.15819)
*Hui Ren,Yuval Alaluf,Omer Bar Tal,Alexander Schwing,Antonio Torralba,Yael Vinker*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的数据方法，将预训练的文本到视频扩散模型适应用于顺序草图生成，把绘画过程当作逐步生成的短视频，显著提升了草图生成的连贯性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型大多将草图视为静态图片，忽略了实际绘图中的时序结构，而顺序和语义规划对创意表现尤为重要。因此需要一种能捕捉并控制画笔顺序及草图动态生成的模型。

Method: 该方法利用大语言模型的语义规划与视频扩散模型的高质量渲染优势，将草图生成过程看作短视频。提出了两阶段微调：第一阶段用合成图形学习画笔顺序，第二阶段用极少量（仅7个）人工绘图数据微调草图外观。同时引入文字指导下的顺序控制和拓展功能（如笔刷风格、自动回归生成）。

Result: 即使只有极少量人工绘画样本，模型也生成了时序连贯、丰富细致且完全遵循文本指定顺序的高质量草图。展示了笔刷风格可控和互动式创作能力的拓展。

Conclusion: 论文证明了将文本到视频扩散模型应用于草图顺序生成的有效性，实现了对草图顺序、细节和风格的灵活控制，为草图生成和人机交互开启了新方向。

Abstract: Sketching is inherently a sequential process, in which strokes are drawn in a meaningful order to explore and refine ideas. However, most generative models treat sketches as static images, overlooking the temporal structure that underlies creative drawing. We present a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models to generate sketching processes. Our key insight is that large language models and video diffusion models offer complementary strengths for this task: LLMs provide semantic planning and stroke ordering, while video diffusion models serve as strong renderers that produce high-quality, temporally coherent visuals. We leverage this by representing sketches as short videos in which strokes are progressively drawn on a blank canvas, guided by text-specified ordering instructions. We introduce a two-stage fine-tuning strategy that decouples the learning of stroke ordering from the learning of sketch appearance. Stroke ordering is learned using synthetic shape compositions with controlled temporal structure, while visual appearance is distilled from as few as seven manually authored sketching processes that capture both global drawing order and the continuous formation of individual strokes. Despite the extremely limited amount of human-drawn sketch data, our method generates high-quality sequential sketches that closely follow text-specified orderings while exhibiting rich visual detail. We further demonstrate the flexibility of our approach through extensions such as brush style conditioning and autoregressive sketch generation, enabling additional controllability and interactive, collaborative drawing.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [44] [EduResearchBench: A Hierarchical Atomic Task Decomposition Benchmark for Full-Lifecycle Educational Research](https://arxiv.org/abs/2602.15034)
*Houping Yue,Zixiang Di,Mei Jiang,Bingdong Li,Hao Hao,Yu Song,Bo Jiang,Aimin Zhou*

Main category: cs.CL

TL;DR: 本文提出EduResearchBench，一个专为教育学术写作设计的评测平台，并推出了EduWrite模型，证明了高质量数据和分阶段训练比规模更重要。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在社会科学AI领域引发变革，但其学术写作能力的系统性评估和细粒度诊断严重不足，现有基准仅注重整体表现，无法反映复杂学术写作流程的具体短板。为解决这一瓶颈，作者设计了新的评测与训练框架。

Method: 提出了Hierarchical Atomic Task Decomposition (HATD)方法，将学术研究流程细分为6大模块、24个原子任务，并基于此建立了自动化评测管线。依托55K学术原始样本，整理出11K高质量指令对，用于训练EduWrite模型。采用课程学习策略，从基础技能到复杂推理分阶段训练模型。

Result: 实验结果显示，EduWrite（30B参数量）在多个学术写作核心指标上明显优于更大参数规模的通用大模型（72B），有效暴露并诊断了学术写作中的具体能力瓶颈。

Conclusion: 在垂直领域任务中，数据质量密度和分阶段课程学习方法比模型的参数规模更能提升专用学术写作模型的能力。EduResearchBench为学术写作模型能力细致评估和提升提供了新方向。

Abstract: While Large Language Models (LLMs) are reshaping the paradigm of AI for Social Science (AI4SS), rigorously evaluating their capabilities in scholarly writing remains a major challenge. Existing benchmarks largely emphasize single-shot, monolithic generation and thus lack the fine-grained assessments required to reflect complex academic research workflows. To fill this gap, we introduce EduResearchBench, the first comprehensive evaluation platform dedicated to educational academic writing. EduResearchBench is built upon our Hierarchical Atomic Task Decomposition (HATD) framework, which decomposes an end-to-end research workflow into six specialized research modules (e.g., Quantitative Analysis, Qualitative Research, and Policy Research) spanning 24 fine-grained atomic tasks. This taxonomy enables an automated evaluation pipeline that mitigates a key limitation of holistic scoring, where aggregate scores often obscure specific capability bottlenecks, and instead provides fine-grained, diagnostic feedback on concrete deficiencies. Moreover, recognizing the high cognitive load inherent in scholarly writing, we propose a curriculum learning strategy that progressively builds competence from foundational skills to complex methodological reasoning and argumentation. Leveraging 55K raw academic samples, we curate 11K high-quality instruction pairs to train EduWrite, a specialized educational scholarly writing model. Experiments show that EduWrite (30B) substantially outperforms larger general-purpose models (72B) on multiple core metrics, demonstrating that in vertical domains, data quality density and hierarchically staged training curricula are more decisive than parameter scale.

</details>


### [45] [Indic-TunedLens: Interpreting Multilingual Models in Indian Languages](https://arxiv.org/abs/2602.15038)
*Mihir Panchal,Deeksha Varshney,Mamta,Asif Ekbal*

Main category: cs.CL

TL;DR: 本文提出了Indic-TunedLens，这是专为印度语言设计的大型语言模型的可解释性框架，通过学习共享仿射变换，更准确地解码多语言模型的隐藏层表示。实验表明，该方法在10种印度语言上的解释性能优于现有技术，尤其适用于形态丰富、资源稀缺的语言。


<details>
  <summary>Details</summary>
Motivation: 尽管多语言大型语言模型在多语环境（如印度）广泛应用，但现有的模型可解释性工具主要针对英文，无法很好解释其他语言尤其是资源稀缺的印度语言模型的内部机制。因此需要开发适用于印度多语环境的可解释性方法，以提升理解和调优模型效果。

Method: 作者提出Indic-TunedLens，核心在于通过为每个目标语言学习共享的仿射变换来调整模型隐藏状态，并将其与目标语言的输出分布对齐，从而实现比传统Logit Lens（直接解码中间激活）更忠实的表征解码。该框架在MMLU基准上的10种印度语言上进行了实验。

Result: 实验证明Indic-TunedLens在10种印度语言上相较于当前最优可解释性方法有显著提升，尤其在形态复杂、资源稀缺的语言上优势更大。同时，该方法揭示了多语言transformer模型在不同层对语义信息的编码方式。

Conclusion: Indic-TunedLens有效提升了多语言大模型在印度语言环境下的可解释性，对于理解和优化多语模型具有重要意义，为后续多语种模型可解释性研究提供了新方向。

Abstract: Multilingual large language models (LLMs) are increasingly deployed in linguistically diverse regions like India, yet most interpretability tools remain tailored to English. Prior work reveals that LLMs often operate in English centric representation spaces, making cross lingual interpretability a pressing concern. We introduce Indic-TunedLens, a novel interpretability framework specifically for Indian languages that learns shared affine transformations. Unlike the standard Logit Lens, which directly decodes intermediate activations, Indic-TunedLens adjusts hidden states for each target language, aligning them with the target output distributions to enable more faithful decoding of model representations. We evaluate our framework on 10 Indian languages using the MMLU benchmark and find that it significantly improves over SOTA interpretability methods, especially for morphologically rich, low resource languages. Our results provide crucial insights into the layer-wise semantic encoding of multilingual transformers. Our model is available at https://huggingface.co/spaces/AnonymousAccountACL/IndicTunedLens. Our code is available at https://github.com/AnonymousAccountACL/IndicTunedLens.

</details>


### [46] [CGRA-DeBERTa Concept Guided Residual Augmentation Transformer for Theologically Islamic Understanding](https://arxiv.org/abs/2602.15139)
*Tahir Hussain,Saddam Hussain Khan*

Main category: cs.CL

TL;DR: 提出了一种新型基于DeBERTa的概念引导残差增强变换器（CGRA DeBERTa）方法，大幅提升了伊斯兰教义文本问答的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统问答模型在处理伊斯兰教义传承（如圣训Hadith）时，难以很好处理领域特有的语义、长距离依赖和需要精细概念推理的情况。因此，迫切需要针对性的方法增强对宗教语境的理解能力。

Method: 本文提出CGRA DeBERTa模型，采用定制的DeBERTa主干，结合轻量级LoRA微调与残差概念感知门控机制。通过嵌入伊斯兰核心词汇表（12个宗教概念）作为先验信息，利用概念门控机制增强关键信息的注意力权重，实现高效、精准的跨度抽取，同时提升语义表现。

Result: 在包括42591个QA对的大型Hadith测试集上，BERT的EM为75.87，DeBERTa为89.77，而新模型达到97.85，绝对提升8.08分，同时推理开销仅增加约8%。定性评估显示其在信息抽取和宗教领域辨析上表现更佳。

Conclusion: CGRA DeBERTa极大提升了伊斯兰Hadith问答系统的准确性、效率和神学可解释性，有助于提供具有宗教细致性的教育材料，具备良好的领域适应性和模型扩展潜力。

Abstract: Accurate QA over classical Islamic texts remains challenging due to domain specific semantics, long context dependencies, and concept sensitive reasoning. Therefore, a new CGRA DeBERTa, a concept guided residual domain augmentation transformer framework, is proposed that enhances theological QA over Hadith corpora. The CGRA DeBERTa builds on a customized DeBERTa transformer backbone with lightweight LoRA based adaptations and a residual concept aware gating mechanism. The customized DeBERTa embedding block learns global and positional context, while Concept Guided Residual Blocks incorporate theological priors from a curated Islamic Concept Dictionary of 12 core terms. Moreover, the Concept Gating Mechanism selectively amplifies semantically critical tokens via importance weighted attention, applying differential scaling from 1.04 to 3.00. This design preserves contextual integrity, strengthens domain-specific semantic representations, and enables accurate, efficient span extraction while maintaining computational efficiency. This paper reports the results of training CGRA using a specially constructed dataset of 42591 QA pairs from the text of Sahih alBukhari and Sahih Muslim. While BERT achieved an EM score of 75.87 and DeBERTa one of 89.77, our model scored 97.85 and thus surpassed them by 8.08 on an absolute scale, all while adding approximately 8 inference overhead due to parameter efficient gating. The qualitative evaluation noted better extraction and discrimination and theological precision. This study presents Hadith QA systems that are efficient, interpretable, and accurate and that scale provide educational materials with necessary theological nuance.

</details>


### [47] [AIC CTU@AVerImaTeC: dual-retriever RAG for image-text fact checking](https://arxiv.org/abs/2602.15190)
*Herbert Ullrich,Jan Drchal*

Main category: cs.CL

TL;DR: 本文提出了一套结合检索增强生成（RAG）与反向图片检索（RIS）的多模态事实核查系统，在比赛中获得第3名，系统结构简单，成本低，易于复现和扩展。


<details>
  <summary>Details</summary>
Motivation: 推动多模态事实核查系统的易用性和高效性，结合文本和图片信息以提升核查准确性，并为后续研究提供易于复现和上手的基线系统。

Method: 系统包含三个解耦的模块：基于相似度检索的文本检索模块，通过API的图片反向检索模块，以及基于GPT5.1的生成模块。每次事实核查仅需一次多模态LLM调用，结构清晰简单。

Result: 在AVerImaTeC比赛中取得第3名，平均每次核查成本仅$0.013，展现出结合RAG与RIS方法的有效性，并证明了低成本和高可复现性的现实意义。

Conclusion: 该系统可作为多模态事实核查领域的入门基线，为后续相关实验和改进提供方便、可扩展的框架。开源了代码、提示词和向量库，为学术和工业界深入研究和实践提供资源支持。

Abstract: In this paper, we present our 3rd place system in the AVerImaTeC shared task, which combines our last year's retrieval-augmented generation (RAG) pipeline with a reverse image search (RIS) module. Despite its simplicity, our system delivers competitive performance with a single multimodal LLM call per fact-check at just $0.013 on average using GPT5.1 via OpenAI Batch API. Our system is also easy to reproduce and tweak, consisting of only three decoupled modules - a textual retrieval module based on similarity search, an image retrieval module based on API-accessed RIS, and a generation module using GPT5.1 - which is why we suggest it as an accesible starting point for further experimentation. We publish its code and prompts, as well as our vector stores and insights into the scheme's running costs and directions for further improvement.

</details>


### [48] [OpaqueToolsBench: Learning Nuances of Tool Behavior Through Interaction](https://arxiv.org/abs/2602.15197)
*Skyler Hallinan,Thejas Venkatesh,Xiang Ren,Sai Praneeth Karimireddy,Ashwin Paranjape,Yuhao Zhang,Jack Hessel*

Main category: cs.CL

TL;DR: 本文提出了OpaqueToolsBench基准，用于测试大语言模型在面对不透明工具（如缺少清晰文档的API）时的表现，并提出了ToolObserver框架，通过迭代观察工具使用结果改进文档，有效提升了模型的任务完成能力。


<details>
  <summary>Details</summary>
Motivation: 现实中的许多工具往往文档不充分、不透明，这与现有基准假设理想文档有所不同。为了使LLM更加贴合实际应用，研究者需探索LLM如何通过交互来理解和改进不透明工具的使用方式，提高任务完成能力。

Method: 作者构建了OpaqueToolsBench基准测试，包括函数调用、国际象棋对弈与长链路搜索三种情景，每种均含有信息不完备的工具。作者还提出了ToolObserver，一个通过观察工具调用反馈、不断迭代更新工具文档的简易框架，以支持LLM更好地掌握工具用法。

Result: 实验证明，现有自动化工具文档生成方案对于不透明工具表现有限且资源消耗大。ToolObserver在OpaqueToolsBench的不同数据集与难度下均超过了现有方法，且在测试时探索效率更高，总token消耗比最佳基线低3.5-7.5倍。

Conclusion: 通过引入基于反馈的工具文档迭代机制，可有效提升LLM代理在不透明工具场景下的表现，为实际应用中LLM与真实工具的高效协作提供了新思路。

Abstract: Tool-calling is essential for Large Language Model (LLM) agents to complete real-world tasks. While most existing benchmarks assume simple, perfectly documented tools, real-world tools (e.g., general "search" APIs) are often opaque, lacking clear best practices or failure modes. Can LLM agents improve their performance in environments with opaque tools by interacting and subsequently improving documentation? To study this, we create OpaqueToolsBench, a benchmark consisting of three distinct task-oriented environments: general function calling, interactive chess playing, and long-trajectory agentic search. Each environment provides underspecified tools that models must learn to use effectively to complete the task. Results on OpaqueToolsBench suggest existing methods for automatically documenting tools are expensive and unreliable when tools are opaque. To address this, we propose a simple framework, ToolObserver, that iteratively refines tool documentation by observing execution feedback from tool-calling trajectories. Our approach outperforms existing methods on OpaqueToolsBench across datasets, even in relatively hard settings. Furthermore, for test-time tool exploration settings, our method is also efficient, consuming 3.5-7.5x fewer total tokens than the best baseline.

</details>


### [49] [Extracting Consumer Insight from Text: A Large Language Model Approach to Emotion and Evaluation Measurement](https://arxiv.org/abs/2602.15312)
*Stephan Ludwig,Peter J. Danaher,Xiaohao Yang,Yu-Ting Lin,Ehsan Abedin,Dhruv Grewal,Lan Du*

Main category: cs.CL

TL;DR: 本研究提出了一种新型大语言模型Linguistic eXtractor（LX），能够高效识别消费者文本中的情绪和评价，准确率远超现有主流模型，并为市场营销研究带来了新的分析工具。


<details>
  <summary>Details</summary>
Motivation: 如何从非结构化的消费者文本中准确测量情绪和评价，一直是营销学的重要难题。

Method: 作者开发并微调了LX大语言模型，该模型在带有消费者自报16种情绪和4种评价标签（信任、承诺、推荐、情感）的文本上训练，并与GPT-4 Turbo、RoBERTa、DeepSeek等领先模型进行对比评测。

Result: LX在开放问卷文本上获得了81%的宏观F1准确率，在亚马逊和Yelp评论集上超过95%。实证分析显示，情绪能显著预测产品评分，进而影响购买行为；特定情绪（如不满、平和）还可直接作用于购买决策。

Conclusion: LX模型为消费感知的量化提供了新方法，并已上线免费无代码Web应用。有助于市场营销行业高效、准确地从消费者文本中提取营销相关构念，为相关研究与实践带来技术突破。

Abstract: Accurately measuring consumer emotions and evaluations from unstructured text remains a core challenge for marketing research and practice. This study introduces the Linguistic eXtractor (LX), a fine-tuned, large language model trained on consumer-authored text that also has been labeled with consumers' self-reported ratings of 16 consumption-related emotions and four evaluation constructs: trust, commitment, recommendation, and sentiment. LX consistently outperforms leading models, including GPT-4 Turbo, RoBERTa, and DeepSeek, achieving 81% macro-F1 accuracy on open-ended survey responses and greater than 95% accuracy on third-party-annotated Amazon and Yelp reviews. An application of LX to online retail data, using seemingly unrelated regression, affirms that review-expressed emotions predict product ratings, which in turn predict purchase behavior. Most emotional effects are mediated by product ratings, though some emotions, such as discontent and peacefulness, influence purchase directly, indicating that emotional tone provides meaningful signals beyond star ratings. To support its use, a no-code, cost-free, LX web application is available, enabling scalable analyses of consumer-authored text. In establishing a new methodological foundation for consumer perception measurement, this research demonstrates new methods for leveraging large language models to advance marketing research and practice, thereby achieving validated detection of marketing constructs from consumer data.

</details>


### [50] [Mnemis: Dual-Route Retrieval on Hierarchical Graphs for Long-Term LLM Memory](https://arxiv.org/abs/2602.15313)
*Zihao Tang,Xin Yu,Ziyu Xiao,Zengxuan Wen,Zelin Li,Jiaxi Zhou,Hualei Wang,Haohua Wang,Haizhen Huang,Weiwei Deng,Feng Sun,Qi Zhang*

Main category: cs.CL

TL;DR: 本文提出了Mnemis框架，通过结合传统的相似性检索（System-1）和全局选择机制（System-2），提升大模型的记忆检索能力，在长期记忆基准上取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的记忆管理主要依赖基于相似性的检索，如RAG和Graph-RAG，但它们在需要全局推理或综合考虑多条历史信息时表现不足。如何让模型既能快速检索相似内容，又能进行全局性的语义覆盖，成为提升记忆力的关键。

Method: 提出Mnemis记忆框架，将记忆以基础图进行相似性检索，同时构建语义分层的层次图，实现自顶向下的全局选择路径。通过结合两种检索机制，模型可以获取语义和结构上都相关的历史信息。

Result: 在长期记忆基准测试LoCoMo和LongMemEval-S上，Mnemis（基于GPT-4.1-mini）分别取得了93.9和91.6的高分，超过所有对比方法。

Conclusion: Mnemis框架显著提升了大模型组织和检索长期记忆的能力，为AI记忆机制的进一步发展提供了有效的新思路。

Abstract: AI Memory, specifically how models organizes and retrieves historical messages, becomes increasingly valuable to Large Language Models (LLMs), yet existing methods (RAG and Graph-RAG) primarily retrieve memory through similarity-based mechanisms. While efficient, such System-1-style retrieval struggles with scenarios that require global reasoning or comprehensive coverage of all relevant information. In this work, We propose Mnemis, a novel memory framework that integrates System-1 similarity search with a complementary System-2 mechanism, termed Global Selection. Mnemis organizes memory into a base graph for similarity retrieval and a hierarchical graph that enables top-down, deliberate traversal over semantic hierarchies. By combining the complementary strength from both retrieval routes, Mnemis retrieves memory items that are both semantically and structurally relevant. Mnemis achieves state-of-the-art performance across all compared methods on long-term memory benchmarks, scoring 93.9 on LoCoMo and 91.6 on LongMemEval-S using GPT-4.1-mini.

</details>


### [51] [NeuroSymActive: Differentiable Neural-Symbolic Reasoning with Active Exploration for Knowledge Graph Question Answering](https://arxiv.org/abs/2602.15353)
*Rong Fu,Yang Li,Zeyu Zhang,Jiekai Wu,Yaohua Liu,Shuaishuai Cao,Yangchen Zeng,Yuhang Zhang,Xiaojing Du,Chuang Zhao,Kangning Cui,Simon Fong*

Main category: cs.CL

TL;DR: 本文提出了NeuroSymActive框架，将神经网络与符号推理结合，用于知识图谱问答，提升了推理效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大模型和神经推理系统在处理需要结构化多跳推理的知识密集型任务时表现有限，传统的将图谱事实融入提示或单纯符号方法均存在效率和可扩展性问题。

Method: 提出NeuroSymActive框架，将可微的神经-符号推理层与主动、基于价值的搜索策略结合。具体方法包括软统一式符号模块、神经路径评估器和类蒙特卡洛价值引导的路径扩展策略。

Result: 在标准知识图谱问答基准上，NeuroSymActive实现了较高的答案准确率，并有效减少了对知识图谱查询和模型调用的次数。

Conclusion: NeuroSymActive兼顾了准确率与效率，为知识图谱问答任务中的神经-符号推理提供了一种有效新范式，优于常用的检索增强基线。

Abstract: Large pretrained language models and neural reasoning systems have advanced many natural language tasks, yet they remain challenged by knowledge-intensive queries that require precise, structured multi-hop inference. Knowledge graphs provide a compact symbolic substrate for factual grounding, but integrating graph structure with neural models is nontrivial: naively embedding graph facts into prompts leads to inefficiency and fragility, while purely symbolic or search-heavy approaches can be costly in retrievals and lack gradient-based refinement. We introduce NeuroSymActive, a modular framework that combines a differentiable neural-symbolic reasoning layer with an active, value-guided exploration controller for Knowledge Graph Question Answering. The method couples soft-unification style symbolic modules with a neural path evaluator and a Monte-Carlo style exploration policy that prioritizes high-value path expansions. Empirical results on standard KGQA benchmarks show that NeuroSymActive attains strong answer accuracy while reducing the number of expensive graph lookups and model calls compared to common retrieval-augmented baselines.

</details>


### [52] [Far Out: Evaluating Language Models on Slang in Australian and Indian English](https://arxiv.org/abs/2602.15373)
*Deniz Kaya Dilsiz,Dipankar Srirag,Aditya Joshi*

Main category: cs.CL

TL;DR: 这篇论文评估了主流语言模型在印度英语和澳大利亚英语俚语理解方面的表现，发现模型对特定地域俚语的掌握存在明显偏差。


<details>
  <summary>Details</summary>
Motivation: 目前主流语言模型在处理标准语之外的非标准方言时存在性能差距，特别是对俚语处理能力了解不足。因此，作者聚焦于印度英语与澳大利亚英语中的俚语，系统性地评估模型的俚语理解能力。

Method: 作者构建了两个数据集：一个从Urban Dictionary收集的377个真实网络用例（web），一个为1,492个人工生成语境中的俚语用法（gen）。然后在七个主流语言模型上分别测试了三个任务：目标词预测（TWP）、引导型目标词预测（TWP*）、和目标词选择（TWS）。

Result: 1）在TWS任务中，模型平均表现优于TWP和TWP*，准确率从0.03提升到0.49；2）在web（真实）数据集上的表现优于gen（合成）数据集，TWP和TWP*任务的相似度分别提升0.03和0.05；3）在所有任务和数据集上，en-IN任务优于en-AU，其中TWS任务表现差距最大，en-IN平均准确率从0.44提升到0.54。

Conclusion: 当前主流语言模型在地域特定语言变体，尤其是俚语表达的生成与判别能力上存在基本性不对称，即使在技术资源丰富的英语环境下也是如此。

Abstract: Language models exhibit systematic performance gaps when processing text in non-standard language varieties, yet their ability to comprehend variety-specific slang remains underexplored for several languages. We present a comprehensive evaluation of slang awareness in Indian English (en-IN) and Australian English (en-AU) across seven state-of-the-art language models. We construct two complementary datasets: \textsc{web}, containing 377 web-sourced usage examples from Urban Dictionary, and \textsc{gen}, featuring 1,492 synthetically generated usages of these slang terms, across diverse scenarios. We assess language models on three tasks: target word prediction (TWP), guided target word prediction (TWP$^*$) and target word selection (TWS). Our results reveal four key findings: (1) Higher average model performance TWS versus TWP and TWP$^*$, with average accuracy score increasing from 0.03 to 0.49 respectively (2) Stronger average model performance on \textsc{web} versus \textsc{gen} datasets, with average similarity score increasing by 0.03 and 0.05 across TWP and TWP$^*$ tasks respectively (3) en-IN tasks outperform en-AU when averaged across all models and datasets, with TWS demonstrating the largest disparity, increasing average accuracy from 0.44 to 0.54. These findings underscore fundamental asymmetries between generative and discriminative competencies for variety-specific language, particularly in the context of slang expressions despite being in a technologically rich language such as English.

</details>


### [53] [Orchestration-Free Customer Service Automation: A Privacy-Preserving and Flowchart-Guided Framework](https://arxiv.org/abs/2602.15377)
*Mengze Hong,Chen Jason Zhang,Zichang Guo,Hanlin Gu,Di Jiang,Li Qing*

Main category: cs.CL

TL;DR: 本文提出了一种无需繁琐编排的新型客户服务自动化框架——基于任务流程图（TOF），实现端到端自动化，并在实验中优于主流方案。


<details>
  <summary>Details</summary>
Motivation: 现有自动化服务要么依赖大量模块化设计与人工调度，成本高、推广难，要么采用过于简化的指令，无法泛化且指引不充分，难以适配实际复杂服务需求。

Method: 提出TOF架构，无需人工调度即可自动生成工作流程。具体包括：定义TOF组件及评价指标，设计成本高效的流程图构建算法，从服务对话中抽象步骤知识。主张本地化部署小型语言模型，并采用去中心化蒸馏，缓解数据稀缺与隐私难题。

Result: 通过大量实验，表明该框架在多个服务任务上效果优异，无论是定量指标还是实际应用均优于现有强基线和商用产品。

Conclusion: TOF框架能高效推动客户服务自动化，兼具隐私保护与通用性，有助于未来服务自动化快速构建。已发布网页版系统来推动实践应用。

Abstract: Customer service automation has seen growing demand within digital transformation. Existing approaches either rely on modular system designs with extensive agent orchestration or employ over-simplified instruction schemas, providing limited guidance and poor generalizability. This paper introduces an orchestration-free framework using Task-Oriented Flowcharts (TOFs) to enable end-to-end automation without manual intervention. We first define the components and evaluation metrics for TOFs, then formalize a cost-efficient flowchart construction algorithm to abstract procedural knowledge from service dialogues. We emphasize local deployment of small language models and propose decentralized distillation with flowcharts to mitigate data scarcity and privacy issues in model training. Extensive experiments validate the effectiveness in various service tasks, with superior quantitative and application performance compared to strong baselines and market products. By releasing a web-based system demonstration with case studies, we aim to promote streamlined creation of future service automation.

</details>


### [54] [Making Large Language Models Speak Tulu: Structured Prompting for an Extremely Low-Resource Language](https://arxiv.org/abs/2602.15378)
*Prathamesh Devadiga,Paras Chopra*

Main category: cs.CL

TL;DR: 本论文探讨了大语言模型（LLM）是否能用几乎未见训练数据的语言进行对话，通过Tulu语案例研究，采用结构化提示而非微调，结合语法文档、负约束、标准化罗马化等方法，大幅改善LLM在少数语言上的表现和纯净度。


<details>
  <summary>Details</summary>
Motivation: Tulu语有超过200万使用者，但数字资源极为匮乏，主流大模型几乎未见其语料。作者希望解决：在训练数据极少的情况下，是否能通过提示工程让LLM进行基本对话？

Method: 不依赖微调用数据，而是利用结构化提示，包括：显式语法文档、通过抑制相关高概率词防止污染（负约束）、统一罗马化方式、并通过模型自玩生成高质量合成数据。选用三款主流LLM（Gemini 2.0 Flash、GPT-4o、Llama 3.1 70B），在人工标注的 held-out 测试集上评估。

Result: 采用组合方法后，模型在Tulu语的词汇污染率由80%降至5%，语法准确率达到85%。负约束对所有模型有12-18个百分点的提升，语法文档带来的增益则因模型结构不同而有8-22个百分点波动。

Conclusion: 即便模型训练阶段基本没见过该语言，只要设计得当的结构化提示（如负约束、语法文档），无需微调即可显著提升LLM在小语种上的对话能力和准确性。

Abstract: Can large language models converse in languages virtually absent from their training data? We investigate this question through a case study on Tulu, a Dravidian language with over 2 million speakers but minimal digital presence. Rather than fine-tuning an LLM, we examine whether structured prompts alone can elicit basic conversational ability under controlled prompting. We systematically tackle various challenges posed by absence of training data for Tulu by combining explicit grammar documentation, negative constraints to suppress high-probability tokens from related languages, romanization standardization, and quality-controlled synthetic data generation via self-play. Evaluated on a manually curated held-out set across three LLMs (Gemini 2.0 Flash, GPT-4o, Llama 3.1 70B) and validated by native speakers, our approach reduces vocabulary contamination from 80% to 5% while achieving 85% grammatical accuracy. Cross-model analysis reveals that negative constraints provide consistent improvements (12--18 percentage points), while grammar documentation effects vary by model architecture (8--22 points).

</details>


### [55] [The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems](https://arxiv.org/abs/2602.15382)
*Xiaoze Liu,Ruowang Zhang,Weichen Yu,Siheng Xiong,Liu He,Feijie Wu,Hoin Jung,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.CL

TL;DR: 本文提出了一种基于视觉通道的多智能体通信框架Vision Wormhole，实现在异构多智能体系统间高效且无文本的信息传递，有效提升运行效率并保证推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体大模型系统依赖文本通信，受限于低效率和信息阈值损失。现有基于隐空间传输方法不是假定同构架构，就是需专门训练转化器，难以在异构多模型体系中实现可扩展和模块化通信。

Method: 作者提出Vision Wormhole框架，通过引入通用视觉编码器（Universal Visual Codec），将不同智能体的推理过程投射到统一连续潜在空间，并通过视觉路径直接输入到接收体，实现模型无关、无文本的高效通信。框架设计为中心辐射式结构，使配对对齐复杂度从O(N^2)降为O(N)，并采用无标签的师生蒸馏方法进行路径对齐。

Result: 在多种异构大模型之间（如Qwen-VL、Gemma）的实验表明，Vision Wormhole在维持与传统文本通信系统同等推理能力的同时，大幅降低通信整体时延。

Conclusion: Vision Wormhole为异构多智能体系统提供了可扩展、高效且通用的无文本通信范式，显著加速系统运行，同时保有推理质量。

Abstract: Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec, we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway, effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at https://github.com/xz-liu/heterogeneous-latent-mas

</details>


### [56] [Measuring Social Integration Through Participation: Categorizing Organizations and Leisure Activities in the Displaced Karelians Interview Archive using LLMs](https://arxiv.org/abs/2602.15436)
*Joonatan Laato,Veera Schroderus,Jenna Kanerva,Jenni Kauppi,Virpi Lummaa,Filip Ginter*

Main category: cs.CL

TL;DR: 本文提出了一种对历史语料中大量活动和组织名称进行自动归类的方法，利用大模型实现高质量标签化，助力社会科学的定量研究。


<details>
  <summary>Details</summary>
Motivation: 历史档案文本包含丰富社会生活信息，但直接抽取得到的数据难以有效用于定量研究，尤其是当涉及活动和组织类型众多、类别分散时。研究者需要一种能将大量异构信息规范化、结构化的手段。

Method: 作者先开发出一套关于参与活动与组织的分类体系，涵盖类型、社交性、规律性、体力性等维度，并人工标注了金标准数据集。随后测试开放权重大型语言模型，采用多次运行投票策略进行批量分类，对模型分类质量进行评估。最后用该方法对35万个实体标签化，形成结构化资源。

Result: 结果显示，该方法下大语言模型的分类结果与专家判断高度一致。通过批量自动标注，成功将35万条档案实体高效结构化，提高了后续分析用数据的质量与可用性。

Conclusion: 本研究提供了历史档案海量社会生活碎片数据归类与结构化的有效方法，并验证了大模型技术在社会科学定量化研究中的可行性和高准确率。

Abstract: Digitized historical archives make it possible to study everyday social life on a large scale, but the information extracted directly from text often does not directly allow one to answer the research questions posed by historians or sociologists in a quantitative manner. We address this problem in a large collection of Finnish World War II Karelian evacuee family interviews. Prior work extracted more than 350K mentions of leisure time activities and organizational memberships from these interviews, yielding 71K unique activity and organization names -- far too many to analyze directly.
  We develop a categorization framework that captures key aspects of participation (the kind of activity/organization, how social it typically is, how regularly it happens, and how physically demanding it is). We annotate a gold-standard set to allow for a reliable evaluation, and then test whether large language models can apply the same schema at scale. Using a simple voting approach across multiple model runs, we find that an open-weight LLM can closely match expert judgments. Finally, we apply the method to label the 350K entities, producing a structured resource for downstream studies of social integration and related outcomes.

</details>


### [57] [TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models](https://arxiv.org/abs/2602.15449)
*Chansung Park,Juyong Jiang,Fan Wang,Sayak Paul,Jiasi Shen,Jing Tang,Jianguo Li*

Main category: cs.CL

TL;DR: 该论文提出了一种名为TAROT的强化微调方法，通过构建分层测试集实现更高效的代码生成微调。实验显示根据模型能力自适应调整训练课程，能显著提高代码正确性和健壮性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽已展现强大编码能力，但生成复杂且健壮代码仍具挑战，主要原因之一是现有强化微调忽略了测试样例难度不均导致的奖励信号偏差。

Method: 作者提出TAROT方法，为每个编程问题系统性构建四级难度测试集（基础、中级、复杂、极端），并将课程训练进展与奖励分离，实现基于模型能力的课程评估和训练策略选择，避免传统方法因测试样例难度构成偶然性引发的问题。

Result: 实验显示，模型内在能力不同，最优的课程安排也不同：能力弱的模型采用由易到难的课程能获得更大提升，而能力强的模型则在先难后易的课程下表现更佳。TAROT方法一致提升了生成代码的功能正确性和健壮性。

Conclusion: TAROT为代码生成强化微调提供了可复现、可自适应能力的课程训练方法，有效提高了代码质量，并推动相关领域发展。代码和数据公开促进行业复现与研究进展。

Abstract: Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.

</details>


### [58] [In Agents We Trust, but Who Do Agents Trust? Latent Source Preferences Steer LLM Generations](https://arxiv.org/abs/2602.15456)
*Mohammad Aflah Khan,Mahsa Amani,Soumi Das,Bishwamittra Ghosh,Qinyuan Wu,Krishna P. Gummadi,Manish Gupta,Abhilasha Ravichander*

Main category: cs.CL

TL;DR: 本论文分析和揭示了大语言模型（LLM）作为信息代理，在为用户筛选和展示信息时存在对不同信息源的系统性偏好。实验发现，多个主流LLM在呈现有明确来源的信息时，会表现出明显且可预测的偏好性，并且这些偏好可能影响甚至左右用户接收的信息内容。


<details>
  <summary>Details</summary>
Motivation: 随着LLM广泛用于在线信息获取和交互，人们关注于其生成信息的偏见，却较少研究LLM在筛选、优先级排序和展示已存在信息时潜在的偏见。作者关注LLM是否会对数据源有系统性选择偏好，因为这直接影响用户获得的信息结构与视角，进而引发信息茧房等社会问题。

Method: 作者对来自六个厂商的十二种主流LLM进行受控实验，包括构造合成任务与现实任务，通过设置明确的信息来源（如出版社、期刊、平台），比较LLM在有选择地向用户展示信息时是否展现明显偏好。实验还考察不同上下文设置、内容和显式消除偏见提示下的表现。

Result: 多数模型展现出强烈且可预测的信息源偏好，这些偏好对上下文很敏感，有时甚至比内容本身更有影响力。即便通过提示让模型避免偏好，偏见依旧存在。此外，这些结果也有助于解释如新闻左倾推荐等现有已知现象。

Conclusion: 现有LLM在信息源筛选方面的偏好性可能加剧信息偏见，对信息多样性与新闻推荐等实际应用构成挑战。呼吁学界深入研究偏好形成机制，并为用户提供透明及可控的偏见管理工具。

Abstract: Agents based on Large Language Models (LLMs) are increasingly being deployed as interfaces to information on online platforms. These agents filter, prioritize, and synthesize information retrieved from the platforms' back-end databases or via web search. In these scenarios, LLM agents govern the information users receive, by drawing users' attention to particular instances of retrieved information at the expense of others. While much prior work has focused on biases in the information LLMs themselves generate, less attention has been paid to the factors that influence what information LLMs select and present to users. We hypothesize that when information is attributed to specific sources (e.g., particular publishers, journals, or platforms), current LLMs exhibit systematic latent source preferences- that is, they prioritize information from some sources over others. Through controlled experiments on twelve LLMs from six model providers, spanning both synthetic and real-world tasks, we find that several models consistently exhibit strong and predictable source preferences. These preferences are sensitive to contextual framing, can outweigh the influence of content itself, and persist despite explicit prompting to avoid them. They also help explain phenomena such as the observed left-leaning skew in news recommendations in prior work. Our findings advocate for deeper investigation into the origins of these preferences, as well as for mechanisms that provide users with transparency and control over the biases guiding LLM-powered agents.

</details>


### [59] [Towards Expectation Detection in Language: A Case Study on Treatment Expectations in Reddit](https://arxiv.org/abs/2602.15504)
*Aswathy Velutharambath,Amelie Wührl*

Main category: cs.CL

TL;DR: 本文提出了期望检测（Expectation Detection）这一新NLP任务，并构建了RedHOTExpect Reddit语料库以探索患者在医疗平台上的治疗期望。利用大语言模型自动标注并人工验证数据集，分析了患者如何表达期望及其特征。


<details>
  <summary>Details</summary>
Motivation: 传统上，患者对治疗的期望主要在临床环境中研究，而许多患者会在医疗相关在线平台表达不同的、难以在临床环境公开的观点。但目前尚无关于用户在网上如何表达治疗期望的系统研究，且NLP领域也未对此主题有过深入探讨。因此，研究者希望通过分析社交平台内容来更全面理解患者期望。

Method: 研究者首先提出‘期望检测’这一任务，随后收集整理了4,500条Reddit医疗相关帖子，构建RedHOTExpect语料库。利用大语言模型为数据自动打标签，并辅以人工验证数据质量。通过对数据集进行语言学分析，探讨期望的表达模式及类型。

Result: 数据自动标注准确率约78%。分析发现，在讨论身体或治疗相关疾病时，用户表现出更多乐观和主动的措辞，而心理健康话题则较少见。此外，用户更倾向于讨论正面收益而非负面结果。

Conclusion: RedHOTExpect语料库为医学领域的NLP研究提供了新的资源，展现了在线患者期望言论的多样性。期望检测任务对舆情挖掘和产品设计等多个应用方向具有实际意义，为理解及分析患者期望提供了技术和数据基础。

Abstract: Patients' expectations towards their treatment have a substantial effect on the treatments' success. While primarily studied in clinical settings, online patient platforms like medical subreddits may hold complementary insights: treatment expectations that patients feel unnecessary or uncomfortable to share elsewhere. Despite this, no studies examine what type of expectations users discuss online and how they express them. Presumably this is because expectations have not been studied in natural language processing (NLP) before. Therefore, we introduce the task of Expectation Detection, arguing that expectations are relevant for many applications, including opinion mining and product design. Subsequently, we present a case study for the medical domain, where expectations are particularly crucial to extract. We contribute RedHOTExpect, a corpus of Reddit posts (4.5K posts) to study expectations in this context. We use a large language model (LLM) to silver-label the data and validate its quality manually (label accuracy ~78%). Based on this, we analyze which linguistic patterns characterize expectations and explore what patients expect and why. We find that optimism and proactive framing are more pronounced in posts about physical or treatment-related illnesses compared to mental-health contexts, and that in our dataset, patients mostly discuss benefits rather than negative outcomes. The RedHOTExpect corpus can be obtained from https://www.ims.uni-stuttgart.de/data/RedHOTExpect

</details>


### [60] [LuxMT Technical Report](https://arxiv.org/abs/2602.15506)
*Nils Rehlinger*

Main category: cs.CL

TL;DR: 本文提出了LuxMT，一种基于Gemma 3 27B的机器翻译系统，实现卢森堡语向法语和英语的翻译，并表现出优于原始模型的效果。


<details>
  <summary>Details</summary>
Motivation: 卢森堡语作为小语种，缺乏高质量的机器翻译系统和基准数据，作者旨在提升这类翻译系统的准确性与实用性。

Method: 作者基于Gemma 3 27B大模型，通过LuxAlign多语对齐语料（新闻和议会记录，部分由谷歌翻译增强）和人类标注数据对模型进行微调。使用LuxEmbedder句向量过滤不等价的数据对，并构建新的评测基准。

Result: 实验显示LuxMT在卢森堡语到法语、英语和德语（未见过德语数据）上的翻译效果显著优于原始Gemma基线。LuxEmbedder与传统指标相关性强，有望作为质量评价工具。

Conclusion: LuxMT在小语种翻译领域效果突出，LuxEmbedder可辅助质量评价，但作者建议进一步研究其可靠性。

Abstract: We introduce LuxMT, a machine translation system based on Gemma 3 27B and fine-tuned for translation from Luxembourgish (LB) into French (FR) and English (EN). To assess translation performance, we construct a novel benchmark covering LB-FR, LB-EN, and LB-FR using human-translated data from Luci, a tourist magazine about Luxembourg. Training data stems from LuxAlign, a parallel corpus of multilingual Luxembourgish news articles, and LB parliamentary transcripts augmented with Google Translate. We filter the data using LuxEmbedder, LB sentence embeddings, to remove low-equivalence segment-pairs. Overall, LuxMT's results suggest strong improvements over the Gemma 3 baseline, even for translating LB to German (DE), despite the training data not containing any DE. We also explore LuxEmbedder's potential to be used as a quality estimation metric and find strong correlations with other reference-based metrics. However, we call for further research to fully assess the metric's utility and advise using it with caution.

</details>


### [61] [Fine-Refine: Iterative Fine-grained Refinement for Mitigating Dialogue Hallucination](https://arxiv.org/abs/2602.15509)
*Xiangyan Chen,Yujian Gan,Matthew Purver*

Main category: cs.CL

TL;DR: 现有大语言模型在对话生成中常出现“幻觉”问题，Fine-Refine框架通过对回复进行细粒度拆分和逐项校验纠正，大幅提升了事实性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的对话有时包含事实性错误（幻觉），影响用户信任，现有方法粗粒度处理，无法精准定位和修正具体事实错误。

Method: 提出了Fine-Refine框架，将回复分解为原子单元，每个单元使用外部知识验证并用困惑度评估流畅性，对粒度级错误进行迭代纠正。

Result: 在HybriDialogue和OpendialKG数据集上，Fine-Refine方法在事实准确性（fact score）上最高提升7.63分，同时仅有较小的对话质量损失。

Conclusion: Fine-Refine能够显著提升对话系统的事实性，是纠正LLM幻觉问题的有效方案，实用性强。

Abstract: The tendency for hallucination in current large language models (LLMs) negatively impacts dialogue systems. Such hallucinations produce factually incorrect responses that may mislead users and undermine system trust. Existing refinement methods for dialogue systems typically operate at the response level, overlooking the fact that a single response may contain multiple verifiable or unverifiable facts. To address this gap, we propose Fine-Refine, a fine-grained refinement framework that decomposes responses into atomic units, verifies each unit using external knowledge, assesses fluency via perplexity, and iteratively corrects granular errors. We evaluate factuality across the HybriDialogue and OpendialKG datasets in terms of factual accuracy (fact score) and coverage (Not Enough Information Proportion), and experiments show that Fine-Refine substantially improves factuality, achieving up to a 7.63-point gain in dialogue fact score, with a small trade-off in dialogue quality.

</details>


### [62] [DependencyAI: Detecting AI Generated Text through Dependency Parsing](https://arxiv.org/abs/2602.15514)
*Sara Ahmed,Tracy Hammond*

Main category: cs.CL

TL;DR: 该论文提出了一种基于语言依存关系的AI文本检测方法DependencyAI，方法简单、可解释且效果竞争力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，检测AI生成文本对于风险防范愈发重要。但现有方法在解释性、跨领域泛化及多语言场景下存在局限，亟需更可靠、可解释的新方法。

Method: 作者仅利用句子中词语之间依存关系的标签作为特征，提出DependencyAI方法，不依赖复杂神经网络。通过特征重要性分析，提升方法可解释性，并在不同语言、多生成器、跨领域数据下评测性能。

Result: DependencyAI方法在单语、多生成器和多语言环境下均取得有竞争力的检测准确率。通过特征重要性分析，发现部分依存结构有助区分AI与人类文本。此外，发现部分生成模型在未见领域存在系统性过拟合现象。

Conclusion: 依存关系特征在检测AI生成文本中表现突出，DependencyAI方法具有强解释性、稳健性，是跨域与多语环境下非神经网络检测基线。

Abstract: As large language models (LLMs) become increasingly prevalent, reliable methods for detecting AI-generated text are critical for mitigating potential risks. We introduce DependencyAI, a simple and interpretable approach for detecting AI-generated text using only the labels of linguistic dependency relations. Our method achieves competitive performance across monolingual, multi-generator, and multilingual settings. To increase interpretability, we analyze feature importance to reveal syntactic structures that distinguish AI-generated from human-written text. We also observe a systematic overprediction of certain models on unseen domains, suggesting that generator-specific writing styles may affect cross-domain generalization. Overall, our results demonstrate that dependency relations alone provide a robust signal for AI-generated text detection, establishing DependencyAI as a strong linguistically grounded, interpretable, and non-neural network baseline.

</details>


### [63] [ExpertWeaver: Unlocking the Inherent MoE in Dense LLMs with GLU Activation Patterns](https://arxiv.org/abs/2602.15521)
*Ziyu Zhao,Tong Zhu,Zhi Zhang,Tiantian Fan,Jinluan Yang,Kun Kuang,Zhongyu Wei,Fei Wu,Yu Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种称为ExpertWeaver的全新方法，通过分析GLU（门控线性单元）的激活模式，将预训练的稠密模型无需训练地转换为稀疏的Mixture-of-Experts（MoE），并在实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型虽然能平衡模型容量和计算效率，但从头训练MoE代价很高。直接将已训练稠密模型转换为MoE成为新方向，但已有方法在转换过程中破坏了原有模型的激活结构，导致构建的专家子网络效果不佳。本文意在寻求一种保留或利用原有激活模式的无训练转换路径。

Method: 通过分析稠密模型中的GLU机制，发现激活模式能够揭示潜在的专家结构。基于这一发现，提出ExpertWeaver框架，按神经元激活分为通用专家和专用路由专家，并采用层自适应配置，无需再训练直接构建稀疏MoE模型。

Result: 实验显示，ExpertWeaver在结构剪枝和下采样初始化两种使用场景下均大幅优于现有dense-to-MoE方法，提升了MoE的初始性能和转换效率。

Conclusion: 通过利用GLU激活模式，ExpertWeaver实现了高效、训练自由的稠密模型到稀疏MoE的迁移，为MoE结构优化和应用提供了新的高性能解决方案。

Abstract: Mixture-of-Experts (MoE) effectively scales model capacity while preserving computational efficiency through sparse expert activation. However, training high-quality MoEs from scratch is prohibitively expensive. A promising alternative is to convert pretrained dense models into sparse MoEs. Existing dense-to-MoE methods fall into two categories: \textbf{dynamic structural pruning} that converts dense models into MoE architectures with moderate sparsity to balance performance and inference efficiency, and \textbf{downcycling} approaches that use pretrained dense models to initialize highly sparse MoE architectures. However, existing methods break the intrinsic activation patterns within dense models, leading to suboptimal expert construction. In this work, we argue that the Gated Linear Unit (GLU) mechanism provides a natural blueprint for dense-to-MoE conversion. We show that the fine-grained neural-wise activation patterns of GLU reveal a coarse-grained structure, uncovering an inherent MoE architecture composed of consistently activated universal neurons and dynamically activated specialized neurons. Leveraging this discovery, we introduce ExpertWeaver, a training-free framework that partitions neurons according to their activation patterns and constructs shared experts and specialized routed experts with layer-adaptive configurations. Our experiments demonstrate that ExpertWeaver significantly outperforms existing methods, both as a training-free dynamic structural pruning technique and as a downcycling strategy for superior MoE initialization.

</details>


### [64] [ZeroSyl: Simple Zero-Resource Syllable Tokenization for Spoken Language Modeling](https://arxiv.org/abs/2602.15537)
*Nicol Visser,Simon Malan,Danel Slabbert,Herman Kamper*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的音节边界和嵌入提取方法ZeroSyl，基于冻结的WavLM模型，表现优于现有音节分词器。


<details>
  <summary>Details</summary>
Motivation: 纯语音语言模型希望直接从原始音频中学习语言，但自监督编码器生成的离散token序列过长，导致建模困难。现有音节单元提取方法复杂，存在优化空间。

Method: 提出ZeroSyl方法：利用冻结WavLM中间层特征的L2范数实现音节边界检测和嵌入提取；将音段做均值池化，再用K-means离散化，结果用于训练语言模型。

Result: ZeroSyl在音节分割性能方面接近或超过现有方法。在词汇、句法和叙事等多项任务基准上表现更好。不同粒度的单元在不同任务上效果也有差异。

Conclusion: ZeroSyl极大简化音节单元提取流程，并在多个任务中展现优越表现，为纯语音语言模型提供了高效的音节级分词解决方案。

Abstract: Pure speech language models aim to learn language directly from raw audio without textual resources. A key challenge is that discrete tokens from self-supervised speech encoders result in excessively long sequences, motivating recent work on syllable-like units. However, methods like Sylber and SyllableLM rely on intricate multi-stage training pipelines. We propose ZeroSyl, a simple training-free method to extract syllable boundaries and embeddings directly from a frozen WavLM model. Using L2 norms of features in WavLM's intermediate layers, ZeroSyl achieves competitive syllable segmentation performance. The resulting segments are mean-pooled, discretized using K-means, and used to train a language model. ZeroSyl outperforms prior syllabic tokenizers across lexical, syntactic, and narrative benchmarks. Scaling experiments show that while finer-grained units are beneficial for lexical tasks, our discovered syllabic units exhibit better scaling behavior for syntactic modeling.

</details>


### [65] [Perspectives - Interactive Document Clustering in the Discourse Analysis Tool Suite](https://arxiv.org/abs/2602.15540)
*Tim Fischer,Chris Biemann*

Main category: cs.CL

TL;DR: 本文提出了Perspectives，这是一个旨在帮助数字人文（DH）学者探索和组织大规模非结构化文档集的互动式分析工具。该系统通过人机互动的聚类流程和嵌入优化，帮助用户发现文档主题、情感等类别，并为深入分析准备数据。


<details>
  <summary>Details</summary>
Motivation: 数字人文领域需要高效的工具来处理和分析大量非结构化文档。传统工具难以灵活适应分析需求，缺乏互动和可持续优化的机制。Perspectives的提出是为了解决这些痛点，使研究者能更灵活、有效地探索和组织文档信息。

Method: Perspectives采用以聚类为核心的分析流程，结合人机互动机制。系统允许用户通过撰写重写提示和指令式嵌入来自定义分析视角，并可通过手动微调聚类和嵌入模型，持续对结果进行调整优化。用户可以通过交互式文档地图直观探索文档集合。

Result: 展示了基于Perspectives的典型工作流程，用户能够通过设定分析视角、高效调整聚类和嵌入模型，有效发现数据中的主题、情感等信息。系统提升了文档分析的效率和灵活性。

Conclusion: Perspectives为数字人文学者提供了强大的文档探索与组织工具，集成了人机互动和嵌入微调机制。通过该工具，学者可以高效、灵活地进行文本文档的初步分析和进一步研究准备。

Abstract: This paper introduces Perspectives, an interactive extension of the Discourse Analysis Tool Suite designed to empower Digital Humanities (DH) scholars to explore and organize large, unstructured document collections. Perspectives implements a flexible, aspect-focused document clustering pipeline with human-in-the-loop refinement capabilities. We showcase how this process can be initially steered by defining analytical lenses through document rewriting prompts and instruction-based embeddings, and further aligned with user intent through tools for refining clusters and mechanisms for fine-tuning the embedding model. The demonstration highlights a typical workflow, illustrating how DH researchers can leverage Perspectives's interactive document map to uncover topics, sentiments, or other relevant categories, thereby gaining insights and preparing their data for subsequent in-depth analysis.

</details>


### [66] [jina-embeddings-v5-text: Task-Targeted Embedding Distillation](https://arxiv.org/abs/2602.15547)
*Mohammad Kalim Akram,Saba Sturua,Nastia Havriushenko,Quentin Herreros,Michael Günther,Maximilian Werk,Han Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种结合蒸馏和对比损失的新训练方案，可训练出体积极小但高性能的文本嵌入模型，模型在相同体积下达到甚至超过当前最优水平，支持多语言及长文本。


<details>
  <summary>Details</summary>
Motivation: 现有通用文本嵌入模型虽能处理各种任务，但小型模型的性能仍有提升空间，且如何兼顾模型体积、性能和适应多语种与长文本输入仍是挑战。

Method: 作者联合应用模型蒸馏技术和任务特定的对比损失进行训练，利用蒸馏提升小模型的泛化能力，结合对比损失强化语义表示，同时对模型做体积压缩及多语言扩展。

Result: 提出的jina-embeddings-v5-text-small和nano模型在多个基准任务上表现达到了或超过同等体积下的SOTA，并能处理长达32k token的多语种文本，且特征在裁剪、量化下仍具鲁棒性。

Conclusion: 结合蒸馏和对比训练能有效提升小型文本嵌入模型的质量和实用性，所开源的模型有望推动此领域进一步发展。

Abstract: Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development.

</details>


### [67] [Beyond Static Pipelines: Learning Dynamic Workflows for Text-to-SQL](https://arxiv.org/abs/2602.15564)
*Yihan Wang,Peiyu Liu,Runyu Chen,Wei Xu*

Main category: cs.CL

TL;DR: 提出了SquRL，一个基于强化学习的框架，使LLM在Text-to-SQL任务中能动态自适应地构建推理流程，显著优于现有静态流程方法，尤其在复杂和分布外场景中提升更大。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法多依赖单一静态推理流程，导致在面对分布外和复杂场景时泛化性能受限，难以适应真实复杂环境。因此，亟需方法让系统能自适应选择和构建最优工作流程。

Method: 通过理论和实证分析，发现动态决策策略优于最优静态流程。提出SquRL强化学习框架，设计了基于规则的奖励函数，结合动态Actor Masking促进广泛探索及伪奖励以提升训练效率，使LLM能自动在推理过程中动态选择合适方式。

Result: 在主流Text-to-SQL基准测试上，动态流程（SquRL）在整体及复杂、分布外查询上都优于最优静态方法。实验结果表明动态构建推理流程具有显著优势。

Conclusion: 通过强化学习与动态流程构建，能有效提升Text-to-SQL任务中LLM的泛化能力与推理表现，特别适用于现实场景中的复杂和分布外问题。

Abstract: Text-to-SQL has recently achieved impressive progress, yet remains difficult to apply effectively in real-world scenarios. This gap stems from the reliance on single static workflows, fundamentally limiting scalability to out-of-distribution and long-tail scenarios. Instead of requiring users to select suitable methods through extensive experimentation, we attempt to enable systems to adaptively construct workflows at inference time. Through theoretical and empirical analysis, we demonstrate that optimal dynamic policies consistently outperform the best static workflow, with performance gains fundamentally driven by heterogeneity across candidate workflows. Motivated by this, we propose SquRL, a reinforcement learning framework that enhances LLMs' reasoning capability in adaptive workflow construction. We design a rule-based reward function and introduce two effective training mechanisms: dynamic actor masking to encourage broader exploration, and pseudo rewards to improve training efficiency. Experiments on widely-used Text-to-SQL benchmarks demonstrate that dynamic workflow construction consistently outperforms the best static workflow methods, with especially pronounced gains on complex and out-of-distribution queries. The codes are available at https://github.com/Satissss/SquRL

</details>


### [68] [Clinically Inspired Symptom-Guided Depression Detection from Emotion-Aware Speech Representations](https://arxiv.org/abs/2602.15578)
*Chaithra Nerella,Chiranjeevi Yarra*

Main category: cs.CL

TL;DR: 本文提出了一种基于症状、临床启发的语音抑郁严重程度评估框架，通过症状引导的注意力机制提升了既准度又可解释性，在标准数据集上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 目前语音抑郁预测多仅用二分类或整体严重评分，未明确建模各抑郁症状，限制了临床可用性和细颗粒度分析。为提升模型在症状层面的解读与临床实用性，研究动机在于实现症状级别的可解释与精准识别。

Method: 提出基于症状引导的跨注意力机制，将PHQ-8各症状条目与情感感知语音表示对齐，通过可学习的症状特异性参数自适应调控注意力分布灵敏度，定位语音中与具体症状相关的关键信息片段。

Result: 在标准临床风格数据集EDAIC上，所提方法在抑郁严重程度评估中性能优于现有方法；注意力分布分析显示，模型能聚焦多个症状相关线索，提高了可解释性。

Conclusion: 结果强调了症状引导、情感感知的建模方法在基于语音的抑郁筛查中的重要意义，为临床症状级分析和可解释人工智能提供了新思路。

Abstract: Depression manifests through a diverse set of symptoms such as sleep disturbance, loss of interest, and concentration difficulties. However, most existing works treat depression prediction either as a binary label or an overall severity score without explicitly modeling symptom-specific information. This limits their ability to provide symptom-level analysis relevant to clinical screening. To address this, we propose a symptom-specific and clinically inspired framework for depression severity estimation from speech. Our approach uses a symptom-guided cross-attention mechanism that aligns PHQ-8 questionnaire items with emotion-aware speech representations to identify which segments of a participant's speech are more important to each symptom. To account for differences in how symptoms are expressed over time, we introduce a learnable symptom-specific parameter that adaptively controls the sharpness of attention distributions. Our results on EDAIC, a standard clinical-style dataset, demonstrate improved performance outperforming prior works. Further, analyzing the attention distributions showed that higher attention is assigned to utterances containing cues related to multiple depressive symptoms, highlighting the interpretability of our approach. These findings outline the importance of symptom-guided and emotion-aware modeling for speech-based depression screening.

</details>


### [69] [STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens](https://arxiv.org/abs/2602.15620)
*Shiqi Liu,Zeyu He,Guojian Zhan,Letian Tao,Zhilong Zheng,Jiang Wu,Yinuo Wang,Yang Guan,Kehua Sheng,Bo Zhang,Keqiang Li,Jingliang Duan,Shengbo Eben Li*

Main category: cs.CL

TL;DR: 本文发现 RL 微调大模型时，极少数『伪（Spurious）token』会导致训练不稳定，并提出了STAPO方法通过屏蔽这些token，稳定并提升了模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有 RL 微调大语言模型时，虽然推理能力有提升，但由于依赖启发式的算法，训练常出现后期性能崩溃和不稳定。研究者希望找到其根本原因并予以改进。

Method: 作者分析表明，RL训练时 token 的梯度大小与 token 概率、策略熵负相关。实际训练中，只有0.01%的『伪 token』梯度异常大，并对训练不稳定负责。基于此提出STAPO方法：在训练时有选择地屏蔽这些异常token的更新，并对损失值进行重归一化。

Result: 在六个数学推理基准上，用不同大小(base 1.7B/8B/14B)Qwen模型做实验。STAPO在稳定性和推理能力上均优于GRPO、20-Entropy及JustRL，平均提升幅度达7.13%。

Conclusion: 极少数『伪token』是训练不稳定根因，STAPO方法能有效缓解这一问题，提升RL微调大模型的推理性能和训练稳定性。

Abstract: Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\%, which we term \emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\% over GRPO, 20-Entropy and JustRL.

</details>


### [70] [LLM-to-Speech: A Synthetic Data Pipeline for Training Dialectal Text-to-Speech Models](https://arxiv.org/abs/2602.15675)
*Ahmed Khaled Khamis,Hesham Ali*

Main category: cs.CL

TL;DR: 该论文推出了首个公开的埃及阿拉伯语语音合成（TTS）数据集 NileTTS，并基于此微调了前沿TTS模型，为埃及阿拉伯语的语音生成研究和应用提供了新资源和工具。


<details>
  <summary>Details</summary>
Motivation: 当前阿拉伯语TTS研究主要聚焦标准阿拉伯语和海湾方言，而埃及阿拉伯语作为使用最广的方言却严重缺乏相关数据资源。本研究旨在填补该领域空白，推动埃及阿拉伯语TTS技术的发展。

Method: 研究团队构建了38小时两位说话人跨多个场景（医疗、销售、日常对话）的埃及阿拉伯语语音数据集。数据由大语言模型生成文本，通过语音合成、转写和说话人分离流程收集，并人工核查质量。随后，团队使用该数据集微调多语种TTS模型XTTS v2，并与基于其他阿拉伯方言的数据集进行对比评估。

Result: 论文贡献了：(1)第一个公开的埃及阿拉伯语TTS数据集；(2)一套可复现的方言TTS合成数据生成流程；(3)公开微调后的埃及阿拉伯语TTS模型。实验表明，在埃及阿拉伯语任务上微调模型的表现优于仅用其他方言训练的模型。

Conclusion: NileTTS数据集和微调模型的发布，有望显著推动埃及阿拉伯语TTS及相关语音技术的研究与产业应用。论文的方法和资源也为其它低资源方言TTS系统的构建提供了新范例。

Abstract: Despite the advances in neural text to speech (TTS), many Arabic dialectal varieties remain marginally addressed, with most resources concentrated on Modern Spoken Arabic (MSA) and Gulf dialects, leaving Egyptian Arabic -- the most widely understood Arabic dialect -- severely under-resourced. We address this gap by introducing NileTTS: 38 hours of transcribed speech from two speakers across diverse domains including medical, sales, and general conversations. We construct this dataset using a novel synthetic pipeline: large language models (LLM) generate Egyptian Arabic content, which is then converted to natural speech using audio synthesis tools, followed by automatic transcription and speaker diarization with manual quality verification. We fine-tune XTTS v2, a state-of-the-art multilingual TTS model, on our dataset and evaluate against the baseline model trained on other Arabic dialects. Our contributions include: (1) the first publicly available Egyptian Arabic TTS dataset, (2) a reproducible synthetic data generation pipeline for dialectal TTS, and (3) an open-source fine-tuned model. All resources are released to advance Egyptian Arabic speech synthesis research.

</details>


### [71] [Revisiting Northrop Frye's Four Myths Theory with Large Language Models](https://arxiv.org/abs/2602.15678)
*Edirlei Soares de Lima,Marco A. Casanova,Antonio L. Furtado*

Main category: cs.CL

TL;DR: 论文提出了一种以角色功能为核心、结合Frye叙事类型的框架，通过大语言模型系统性验证其可行性，为计算叙事学开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 以往对Frye四大叙事类型的计算分析侧重叙事结构模式，忽视了不同类型中角色功能的差异。作者希望补足这一空白，提升对叙事类型与角色分布关系的理解。

Method: 以荣格原型理论为基础，抽象出四大通用角色功能（主角、导师、反派、同伴），并针对Frye四大类型进一步细化为16种类型特有的角色功能。基于40部作品，利用6个主流大语言模型对160个有效和30个无效的角色–功能对应进行验证，通过准确率和一致性统计进行评估，并结合定性分析探讨不同类型的表现差异。

Result: LLM对角色–功能对应判断的均衡准确率达到82.5%，模型间一致性良好（Fleiss' κ=0.600）。不同类型和功能角色的表现存在差异，反映例如爱情故事功能分布更广，而讽刺类型常有原型颠覆。

Conclusion: 提出的角色功能–叙事类型框架经LLM支持方法验证，能够捕捉系统性的结构特征，为后续叙事生成和智能互动叙事应用奠定理论和方法基础。

Abstract: Northrop Frye's theory of four fundamental narrative genres (comedy, romance, tragedy, satire) has profoundly influenced literary criticism, yet computational approaches to his framework have focused primarily on narrative patterns rather than character functions. In this paper, we present a new character function framework that complements pattern-based analysis by examining how archetypal roles manifest differently across Frye's genres. Drawing on Jungian archetype theory, we derive four universal character functions (protagonist, mentor, antagonist, companion) by mapping them to Jung's psychic structure components. These functions are then specialized into sixteen genre-specific roles based on prototypical works. To validate this framework, we conducted a multi-model study using six state-of-the-art Large Language Models (LLMs) to evaluate character-role correspondences across 40 narrative works. The validation employed both positive samples (160 valid correspondences) and negative samples (30 invalid correspondences) to evaluate whether models both recognize valid correspondences and reject invalid ones. LLMs achieved substantial performance (mean balanced accuracy of 82.5%) with strong inter-model agreement (Fleiss' $κ$ = 0.600), demonstrating that the proposed correspondences capture systematic structural patterns. Performance varied by genre (ranging from 72.7% to 89.9%) and role (52.5% to 99.2%), with qualitative analysis revealing that variations reflect genuine narrative properties, including functional distribution in romance and deliberate archetypal subversion in satire. This character-based approach demonstrates the potential of LLM-supported methods for computational narratology and provides a foundation for future development of narrative generation methods and interactive storytelling applications.

</details>


### [72] [A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models](https://arxiv.org/abs/2602.15689)
*Meirav Segal,Noa Linder,Omer Antverg,Gil Gekker,Tomer Fichman,Omri Bodenheimer,Edan Maor,Omer Nevo*

Main category: cs.CL

TL;DR: 本文提出了一种基于内容的网络安全拒绝策略框架，旨在提高对攻击性与防御性请求的区分能力，解决现有方法易出现的不一致与过度限制问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在网络安全领域的应用面临双重用途困境。已有拒绝策略多采用广泛的主题封禁或攻击性分类，导致合法防御者被错误限制，且对请求伪装等手段鲁棒性差，亟需更精细的拒绝机制。

Method: 作者提出了一套内容基础的评估框架，将请求分为五个维度：攻击性行为贡献、攻击风险、技术复杂度、防御性收益和合法用户预期频率，关注具体技术内容而非仅凭意图或简单分类。通过此方法来设计和审计拒绝政策。

Result: 基于内容的框架能解决现有大模型拒绝策略在复杂请求和分割请求下的不一致表现，使得组织能调整和定制更符合风险偏好的拒绝策略。

Conclusion: 仅依靠意图或攻击分类的拒绝方式不足以应对复杂网络安全场景，基于内容的多维度分析更有助于实现攻防平衡和动态风险管理。

Abstract: Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework characterizes requests along five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users, grounded in the technical substance of the request rather than stated intent. We demonstrate that this content-grounded approach resolves inconsistencies in current frontier model behavior and allows organizations to construct tunable, risk-aware refusal policies.

</details>


### [73] [Rethinking Metrics for Lexical Semantic Change Detection](https://arxiv.org/abs/2602.15716)
*Roksana Goworek,Haim Dubossarsky*

Main category: cs.CL

TL;DR: 本文提出了两种新的词义变化度量方法AMD和SAMD，实验表明这两种指标能在多语言、多模型条件下更稳健地检测词义变化，尤其AMD在多种情形下效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有词义语义变化检测方法依赖上下文化的语言模型嵌入，但主要还集中在少数几种传统的变化度量指标（如APD和PRT），系统性探索新的度量方法尚不足。

Method: 作者提出了平均最小距离（AMD）和对称平均最小距离（SAMD）两种新型度量词义变化的方法，进一步在多语言、多编码器模型和多种表示空间下进行了实验评估。

Result: 实验结果显示：AMD在降维、非专用编码器条件下表现更稳健；SAMD则在专用编码器下表现更好。总体而言，新方法在多种情形和条件下表现优越。

Conclusion: 建议在利用嵌入模型分析词义变化时，考虑采用AMD或SAMD等新度量方法，从而获得更有鲁棒性的分析结果，对当前以APD和PRT为主的现状构成有益补充。

Abstract: Lexical semantic change detection (LSCD) increasingly relies on contextualised language model embeddings, yet most approaches still quantify change using a small set of semantic change metrics, primarily Average Pairwise Distance (APD) and cosine distance over word prototypes (PRT). We introduce Average Minimum Distance (AMD) and Symmetric Average Minimum Distance (SAMD), new measures that quantify semantic change via local correspondence between word usages across time periods. Across multiple languages, encoder models, and representation spaces, we show that AMD often provides more robust performance, particularly under dimensionality reduction and with non-specialised encoders, while SAMD excels with specialised encoders. We suggest that LSCD may benefit from considering alternative semantic change metrics beyond APD and PRT, with AMD offering a robust option for contextualised embedding-based analysis.

</details>


### [74] [Causal Effect Estimation with Latent Textual Treatments](https://arxiv.org/abs/2602.15730)
*Omri Feldman,Amar Venugopal,Jann Spiess,Amir Feder*

Main category: cs.CL

TL;DR: 该论文提出了一个端到端的管道，用于实现文本干预下的因果效应估计，结合稀疏自编码器和协变量残差化，显著提高了估计精度。


<details>
  <summary>Details</summary>
Motivation: 文本干预的因果效应估计在诸多领域均有重要应用，但由于文本的复杂性（如处理变量与协变量间的信息混杂），现有方法存在较大偏差，需要新的生成与估计流程。

Method: 首先利用稀疏自编码器进行假设生成和文本操控，然后采取鲁棒的因果效应估计技术，提出通过协变量残差化来纠正因文本特性导致的估计偏差。

Result: 实验证明，采用该管道不仅能有效操控目标特征的变化，还能显著减少因果效应估计的误差。

Conclusion: 本文提出的方法为‘text-as-treatment’情境下鲁棒的因果效应估计奠定了基础，有效解决了文本混淆导致的估计偏差问题。

Abstract: Understanding the causal effects of text on downstream outcomes is a central task in many applications. Estimating such effects requires researchers to run controlled experiments that systematically vary textual features. While large language models (LLMs) hold promise for generating text, producing and evaluating controlled variation requires more careful attention. In this paper, we present an end-to-end pipeline for the generation and causal estimation of latent textual interventions. Our work first performs hypothesis generation and steering via sparse autoencoders (SAEs), followed by robust causal estimation. Our pipeline addresses both computational and statistical challenges in text-as-treatment experiments. We demonstrate that naive estimation of causal effects suffers from significant bias as text inherently conflates treatment and covariate information. We describe the estimation bias induced in this setting and propose a solution based on covariate residualization. Our empirical results show that our pipeline effectively induces variation in target features and mitigates estimation error, providing a robust foundation for causal effect estimation in text-as-treatment settings.

</details>


### [75] [Under-resourced studies of under-resourced languages: lemmatization and POS-tagging with LLM annotators for historical Armenian, Georgian, Greek and Syriac](https://arxiv.org/abs/2602.15753)
*Chahan Vidal-Gorène,Bastien Kindt,Florian Cafiero*

Main category: cs.CL

TL;DR: 本论文评估了GPT-4和Mistral等大语言模型在低资源语言的词形还原和词性标注上的表现，结果显示即使无需微调，大模型在少量示例支持下已表现出较强竞争力。


<details>
  <summary>Details</summary>
Motivation: 低资源语言因数据缺乏，在词形还原和词性标注等NLP任务中长期存在难题。本研究试图评估大语言模型能否在极少/零样本条件下，有效支持这些复杂任务。

Method: 作者选取了四种低资源、历史悠久且语系多样的语言（古希腊语、古亚美尼亚语、古格鲁吉亚语、叙利亚语），构建了新颖的对齐训练和域外测试集；分别用GPT-4家族、Mistral开源模型对词形还原与词性标注进行零样本和少样本实验，并与专用RNN（PIE）基线对比。

Result: 在大多数语言及设置下，LLMs（尤其在few-shot下）无需微调即可达到与RNN基线相当或更优的表现。但对于形态复杂和非拉丁文字的语言仍存在显著挑战。

Conclusion: 大语言模型在缺乏数据的情况下，也能作为语言注释工作的有力起点，尤其在初始标注时显现出极大潜力，未来或成为低资源语言NLP的有效工具。

Abstract: Low-resource languages pose persistent challenges for Natural Language Processing tasks such as lemmatization and part-of-speech (POS) tagging. This paper investigates the capacity of recent large language models (LLMs), including GPT-4 variants and open-weight Mistral models, to address these tasks in few-shot and zero-shot settings for four historically and linguistically diverse under-resourced languages: Ancient Greek, Classical Armenian, Old Georgian, and Syriac. Using a novel benchmark comprising aligned training and out-of-domain test corpora, we evaluate the performance of foundation models across lemmatization and POS-tagging, and compare them with PIE, a task-specific RNN baseline. Our results demonstrate that LLMs, even without fine-tuning, achieve competitive or superior performance in POS-tagging and lemmatization across most languages in few-shot settings. Significant challenges persist for languages characterized by complex morphology and non-Latin scripts, but we demonstrate that LLMs are a credible and relevant option for initiating linguistic annotation tasks in the absence of data, serving as an effective aid for annotation.

</details>


### [76] [Beyond Binary Classification: Detecting Fine-Grained Sexism in Social Media Videos](https://arxiv.org/abs/2602.15757)
*Laura De Grazia,Danae Sánchez Villegas,Desmond Elliott,Mireia Farrús,Mariona Taulé*

Main category: cs.CL

TL;DR: 该论文提出了一种针对西班牙语在线性别歧视内容的新数据集FineMuSe，结合多模态信息，实现了更细粒度的识别与分析。论文还建立了分层的性别歧视分类体系，并系统评估了多种大模型在检测任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化工具多局限于性别歧视内容的二分类检测，无法细致捕捉更隐晦、多样的性别歧视表现，从而降低了检测的准确性和实用性。作者希望通过引入细粒度标签和多模态数据提升识别能力。

Method: 提出了名为FineMuSe的多模态西班牙语数据集，涵盖了二分类和细粒度标签，并构建了包含多层级的性别歧视与修辞手法的分类体系。同时，测试并分析了主流大语言模型（LLMs）在不同层次性别歧视检测任务中的表现。

Result: 多模态大语言模型在识别细微性别歧视方面表现接近人工标注者，但对依赖视觉线索、共现多种歧视类型的内容识别表现仍有不足。

Conclusion: 丰富的语料和细致的分类体系帮助提升了性别歧视识别的能力，但大模型在处理包含多种信息和视觉暗示的复杂语境时仍需进一步改进。

Abstract: Online sexism appears in various forms, which makes its detection challenging. Although automated tools can enhance the identification of sexist content, they are often restricted to binary classification. Consequently, more subtle manifestations of sexism may remain undetected due to the lack of fine-grained, context-sensitive labels. To address this issue, we make the following contributions: (1) we present FineMuSe, a new multimodal sexism detection dataset in Spanish that includes both binary and fine-grained annotations; (2) we introduce a comprehensive hierarchical taxonomy that encompasses forms of sexism, non-sexism, and rhetorical devices of irony and humor; and (3) we evaluate a wide range of LLMs for both binary and fine-grained sexism detection. Our findings indicate that multimodal LLMs perform competitively with human annotators in identifying nuanced forms of sexism; however, they struggle to capture co-occurring sexist types when these are conveyed through visual cues.

</details>


### [77] [ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models](https://arxiv.org/abs/2602.15758)
*Manav Nitin Kapadnis,Lawanya Baghel,Atharva Naik,Carolyn Rosé*

Main category: cs.CL

TL;DR: 本文提出了ChartEditBench，这是一个用于评估多模态大模型（MLLMs）在多轮、基于可视化的图表编辑任务中的性能基准，为现实场景下的探索性数据分析提供了挑战性测试平台。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在单轮图表生成任务上表现良好，但现实中用户往往需要通过多轮交互反复编辑图表，涉及上下文维护、历史追踪以及偏好适应等复杂需求。而目前缺少系统性的评测方法来检验模型在此类多轮复杂场景下的实际能力。

Method: 作者构建了ChartEditBench，包括5000条带有控制难度的图表编辑链，并有人工严密标注的子集。该基准强调多轮、情景相关的持续编辑。为了更全面评估模型表现，还构建了新的评测体系，结合代码执行结果、像素级视觉相似度及逻辑正确性等多维度指标，弥补单一LLM评判指标的不足。

Result: 实验显示，最先进的MLLM在多轮编辑过程中由于错误积累和上下文断裂导致性能明显下降，虽然在风格调整等表面性编辑上表现尚可，但在数据相关的复杂变换中常常出现执行失败。

Conclusion: ChartEditBench为基于意图和真实交互的多模态编程任务提供了权威且具有挑战性的测评平台，有助于推动MLLM领域在实际探索性数据分析方向上的发展。

Abstract: While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common ground, tracking prior edits, and adapting to evolving preferences. We introduce ChartEditBench, a benchmark for incremental, visually grounded chart editing via code, comprising 5,000 difficulty-controlled modification chains and a rigorously human-verified subset. Unlike prior one-shot benchmarks, ChartEditBench evaluates sustained, context-aware editing. We further propose a robust evaluation framework that mitigates limitations of LLM-as-a-Judge metrics by integrating execution-based fidelity checks, pixel-level visual similarity, and logical code verification. Experiments with state-of-the-art MLLMs reveal substantial degradation in multi-turn settings due to error accumulation and breakdowns in shared context, with strong performance on stylistic edits but frequent execution failures on data-centric transformations. ChartEditBench, establishes a challenging testbed for grounded, intent-aware multimodal programming.

</details>


### [78] [ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution](https://arxiv.org/abs/2602.15769)
*Yahia Alqurnawi,Preetom Biswas,Anmol Rao,Tejas Anvekar,Chitta Baral,Vivek Gupta*

Main category: cs.CL

TL;DR: 本论文评估了多模态大语言模型（mLLMs）在结构化数据溯源能力上的表现，发现其对答案出处的指示能力较弱，尤其在 JSON 输入下接近随机。


<details>
  <summary>Details</summary>
Motivation: 用户使用 mLLMs 解答结构化数据（如表格）时，除了正确答案，还需要知道答案基于哪些具体数据，溯源与可信度成为重要需求。

Method: 作者测评了多种 mLLMs 对不同格式表格（Markdown、JSON、图片）和不同提示策略的回答准确性及溯源能力，分析模型在定位支持答案的具体行列（证据归属）上的效果。

Result: 虽然 mLLMs 的问答准确度居中，但其证据归属（溯源）能力普遍较差，尤其在 JSON 格式几乎等同随机，仅有对行的溯源表现稍好于列；文本格式比图片难溯源，各模型家族之间差异显著。

Conclusion: 当前的 mLLMs 在结构化数据的细粒度溯源和可信归因方面表现不可靠，限制了其在透明性和可追溯性强需求的应用场景下的实际使用价值。

Abstract: Multimodal Large Language Models (mLLMs) are often used to answer questions in structured data such as tables in Markdown, JSON, and images. While these models can often give correct answers, users also need to know where those answers come from. In this work, we study structured data attribution/citation, which is the ability of the models to point to the specific rows and columns that support an answer. We evaluate several mLLMs across different table formats and prompting strategies. Our results show a clear gap between question answering and evidence attribution. Although question answering accuracy remains moderate, attribution accuracy is much lower, near random for JSON inputs, across all models. We also find that models are more reliable at citing rows than columns, and struggle more with textual formats than images. Finally, we observe notable differences across model families. Overall, our findings show that current mLLMs are unreliable at providing fine-grained, trustworthy attribution for structured data, which limits their usage in applications requiring transparency and traceability.

</details>


### [79] [*-PLUIE: Personalisable metric with Llm Used for Improved Evaluation](https://arxiv.org/abs/2602.15778)
*Quentin Lemesle,Léane Jourdan,Daisy Munson,Pierre Alain,Jonathan Chevelu,Arnaud Delhay,Damien Lolive*

Main category: cs.CL

TL;DR: 本文提出了一种面向任务的Perplexity评估方法*-PLUIE，用于高效、低成本地自动评估文本生成质量，并证明其与人工评价具有更高相关性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM-as-a-judge 方法虽然有效，但计算成本高且需额外后处理。因此，作者希望找到一个更加高效并能与人工评价高度对齐的自动质量评估方法。

Method: 在原有的基于困惑度的ParaPLUIE框架基础上，作者提出了*-PLUIE，这是一种针对具体任务的动态提示改进版。该方法无需生成额外文本，通过对“是/否”答案置信度的困惑度估计，实现对文本自动质量的打分和评估。

Result: 实验表明，定制化的*-PLUIE方法不仅计算成本低，而且与人工评分的相关性更强，优于原始模型。

Conclusion: *-PLUIE方法在保证效率和低成本的前提下，能够更加准确地反映人工评价结果，为自动文本质量评估提供了更优解。

Abstract: Evaluating the quality of automatically generated text often relies on LLM-as-a-judge (LLM-judge) methods. While effective, these approaches are computationally expensive and require post-processing. To address these limitations, we build upon ParaPLUIE, a perplexity-based LLM-judge metric that estimates confidence over ``Yes/No'' answers without generating text. We introduce *-PLUIE, task specific prompting variants of ParaPLUIE and evaluate their alignment with human judgement. Our experiments show that personalised *-PLUIE achieves stronger correlations with human ratings while maintaining low computational cost.

</details>


### [80] [Avey-B](https://arxiv.org/abs/2602.15814)
*Devang Acharya,Mohammad Hammoud*

Main category: cs.CL

TL;DR: 本文对Avey架构进行了创新，提出了适用于仅编码器场景的高效自回归、非注意力模型，并在多项基准测试中超越了主流Transformer编码器。


<details>
  <summary>Details</summary>
Motivation: 当前工业NLP中的紧凑预训练双向编码器（如BERT）虽然有效，但通常受限于计算和内存资源。新提出的Avey模型作为一种无需注意力机制的自回归方法，有望在效率和效果之间取得更好平衡，因此作者希望改进Avey，使其在仅编码器场景下替代Transformer编码器，提升长序列处理和资源利用效率。

Method: 本文将Avey重新设计用于仅编码器结构，提出了多个架构创新：静态/动态参数解耦、面向稳定性的归一化方法，以及神经压缩技术。这些方法共同优化了模型的表现能力与计算效率。

Result: 改进后的Avey架构在标准的Token分类和信息检索任务基准上，均优于四种主流Transformer编码器。同时，在处理长文本时表现出更优的扩展性和资源利用率。

Conclusion: 重新设计的Avey编码器模型不仅实现了对主流Transformer编码器的精度超越，还提高了长序列任务的可扩展性，为低资源环境下的NLP应用提供了更优的解决方案。

Abstract: Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [81] [CLOT: Closed-Loop Global Motion Tracking for Whole-Body Humanoid Teleoperation](https://arxiv.org/abs/2602.15060)
*Tengjie Zhu,Guanyu Cai,Yang Zhaohui,Guanzhu Ren,Haohui Xie,ZiRui Wang,Junsong Wu,Jingbo Wang,Xiaokang Yang,Yao Mu,Yichao Yan,Yichao Yan*

Main category: cs.RO

TL;DR: 该论文提出了CLOT，一个实现全身类人机器人遥操作的闭环全球运动跟踪系统，通过高频定位反馈，有效解决了长期操作中的全局姿态漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有学习型运动跟踪方法虽然动作灵活且协调，但由于大多只在机器人局部坐标系中运行，缺乏全局位姿反馈，容易在长时间操作中积累误差并导致姿态漂移和不稳定。因此亟需一个能结合全局反馈、实现稳定长期遥操作的方案。

Method: 提出了CLOT方法，通过高频定位反馈实现操作员与机器人姿态的闭环同步。为避免直接用全局跟踪奖励导致的激进且不稳定校正，采用了一种基于数据的轨迹随机化策略，实现平滑稳定的全局修正。同时，引入对抗运动先验正则化，抑制不自然动作。此外，采集了20小时高质量人类动作数据，基于Transformer架构设计并训练了全身遥操作策略。

Result: 将所提策略部署在拥有31个自由度的全尺寸类人机器人上（不含手部），在仿真和真实环境下均验证了其高动态、高精度和强鲁棒性的类人遥操作效果。

Conclusion: CLOT系统能实现长时间、无漂移的人-机器人动作模仿，在现实中展现出高可用性和广泛应用潜力。相关动作数据、演示和代码已公开。

Abstract: Long-horizon whole-body humanoid teleoperation remains challenging due to accumulated global pose drift, particularly on full-sized humanoids. Although recent learning-based tracking methods enable agile and coordinated motions, they typically operate in the robot's local frame and neglect global pose feedback, leading to drift and instability during extended execution. In this work, we present CLOT, a real-time whole-body humanoid teleoperation system that achieves closed-loop global motion tracking via high-frequency localization feedback. CLOT synchronizes operator and robot poses in a closed loop, enabling drift-free human-to-humanoid mimicry over long timehorizons. However, directly imposing global tracking rewards in reinforcement learning, often results in aggressive and brittle corrections. To address this, we propose a data-driven randomization strategy that decouples observation trajectories from reward evaluation, enabling smooth and stable global corrections. We further regularize the policy with an adversarial motion prior to suppress unnatural behaviors. To support CLOT, we collect 20 hours of carefully curated human motion data for training the humanoid teleoperation policy. We design a transformer-based policy and train it for over 1300 GPU hours. The policy is deployed on a full-sized humanoid with 31 DoF (excluding hands). Both simulation and real-world experiments verify high-dynamic motion, high-precision tracking, and strong robustness in sim-to-real humanoid teleoperation. Motion data, demos and code can be found in our website.

</details>


### [82] [Safe-SDL:Establishing Safety Boundaries and Control Mechanisms for AI-Driven Self-Driving Laboratories](https://arxiv.org/abs/2602.15061)
*Zihan Zhang,Haohui Que,Junhan Chang,Xin Zhang,Hao Wei,Tong Zhu*

Main category: cs.RO

TL;DR: 本文提出Safe-SDL框架，通过综合性的安全机制保障自驱动实验室（SDL）中AI与自动化操作的安全，弥合AI命令正确性和物理安全之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 随着自驱动实验室的兴起，AI结合机器人自动化极大提升科研效率。然而，SDL部署带来了与传统实验室或数字AI不同的全新安全挑战，尤其是AI生成命令可能语法正确但实际操作有潜在风险，急需系统性安全保障。

Method: 作者分析了现有SDL安全隐患，提出Safe-SDL框架，包括：(1)形式化定义的运作设计域（ODDs），从理论上约束系统行为；(2)控制屏障函数（CBFs），实现实时安全状态监控；(3)新颖的事务性安全协议（CRUTD），保障数字规划与实际执行间的一致性。框架还通过对UniLabOS和Osprey等现有架构系统的理论分析和实例落地展示效果。

Result: 通过在LabSafety Bench评测，证明当前主流基础模型在安全上存在明显缺陷，只有通过系统级的安全机制才能有效防范风险。Safe-SDL框架不仅在理论上定义安全边界，还为实际SDL系统部署提供了参考和指导。

Conclusion: 安全机制是自驱动科学发现系统不可或缺的部分，Safe-SDL为AI驱动实验室的安全部署奠定理论与实践基础，有望推动负责任的自动化科研加速。

Abstract: The emergence of Self-Driving Laboratories (SDLs) transforms scientific discovery methodology by integrating AI with robotic automation to create closed-loop experimental systems capable of autonomous hypothesis generation, experimentation, and analysis. While promising to compress research timelines from years to weeks, their deployment introduces unprecedented safety challenges differing from traditional laboratories or purely digital AI. This paper presents Safe-SDL, a comprehensive framework for establishing robust safety boundaries and control mechanisms in AI-driven autonomous laboratories. We identify and analyze the critical ``Syntax-to-Safety Gap'' -- the disconnect between AI-generated syntactically correct commands and their physical safety implications -- as the central challenge in SDL deployment. Our framework addresses this gap through three synergistic components: (1) formally defined Operational Design Domains (ODDs) that constrain system behavior within mathematically verified boundaries, (2) Control Barrier Functions (CBFs) that provide real-time safety guarantees through continuous state-space monitoring, and (3) a novel Transactional Safety Protocol (CRUTD) that ensures atomic consistency between digital planning and physical execution. We ground our theoretical contributions through analysis of existing implementations including UniLabOS and the Osprey architecture, demonstrating how these systems instantiate key safety principles. Evaluation against the LabSafety Bench reveals that current foundation models exhibit significant safety failures, demonstrating that architectural safety mechanisms are essential rather than optional. Our framework provides both theoretical foundations and practical implementation guidance for safe deployment of autonomous scientific systems, establishing the groundwork for responsible acceleration of AI-driven discovery.

</details>


### [83] [How Do We Research Human-Robot Interaction in the Age of Large Language Models? A Systematic Review](https://arxiv.org/abs/2602.15063)
*Yufeng Wang,Yuan Xu,Anastasia Nikolova,Yuxuan Wang,Jianyu Wang,Chongyang Wang,Xin Tong*

Main category: cs.RO

TL;DR: 该论文综述了大语言模型（LLMs）在促进人机交互（HRI）领域中的作用，系统梳理了相关文献并总结了当前进展与挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在HRI领域展现出巨大的技术潜力，但其以人为中心的影响尚未被系统性研究。该工作旨在梳理相关研究，明确当前发展状况及未来挑战。

Method: 作者依据PRISMA指南进行了系统性文献检索，筛选并分析了86篇符合标准的相关文章，归纳总结了研究成果与不足。

Result: 研究发现LLMs正在改变HRI的基本面，如推动机器人对语境感知、生成社会化交互和动态与人类需求保持一致。当前研究仍以探索性为主，研究侧重点、实验方法和评价指标差异较大。

Conclusion: 论文总结了LLM驱动的HRI领域的核心设计考量与面临的挑战，对未来研究方向提出了建议与指导。

Abstract: Advances in large language models (LLMs) are profoundly reshaping the field of human-robot interaction (HRI). While prior work has highlighted the technical potential of LLMs, few studies have systematically examined their human-centered impact (e.g., human-oriented understanding, user modeling, and levels of autonomy), making it difficult to consolidate emerging challenges in LLM-driven HRI systems. Therefore, we conducted a systematic literature search following the PRISMA guideline, identifying 86 articles that met our inclusion criteria. Our findings reveal that: (1) LLMs are transforming the fundamentals of HRI by reshaping how robots sense context, generate socially grounded interactions, and maintain continuous alignment with human needs in embodied settings; and (2) current research is largely exploratory, with different studies focusing on different facets of LLM-driven HRI, resulting in wide-ranging choices of experimental setups, study methods, and evaluation metrics. Finally, we identify key design considerations and challenges, offering a coherent overview and guidelines for future research at the intersection of LLMs and HRI.

</details>


### [84] [Augmenting Human Balance with Generic Supernumerary Robotic Limbs](https://arxiv.org/abs/2602.15092)
*Xuanyun Qiu,Dorian Verdel,Hector Cervantes-Culebro,Alexis Devillard,Etienne Burdet*

Main category: cs.RO

TL;DR: 提出了一种用于通用型超常机器人手臂（SL）的平衡维护通用框架，通过分层架构显著提升了人和SLs系统的稳定性。


<details>
  <summary>Details</summary>
Motivation: 超常机器人手臂（SLs）能够扩展人类能力，但现有方法难以兼顾安全与多功能性，特别是在支持平衡方面。以往方案多针对专用SLs，缺乏适用于通用SLs的平衡维护技术。

Method: 作者提出了一种三层次的层级架构：预测层预测人体躯干和质心动态，规划层生成补偿躯干运动的最优质心轨迹并计算SLs的控制输入，控制层将指令执行到SLs硬件上。实验邀请10名参与者进行前倾和侧弯任务来验证系统。

Result: 实验结果表明该框架能明显减少站立不稳现象，有效提升人体与SLs系统的平衡稳定性。

Conclusion: 该研究为实现安全且多功能的人体-SLs互动提供了关键的技术基础，推动了SLs在实际复杂任务和多样场景下的可用性。

Abstract: Supernumerary robotic limbs (SLs) have the potential to transform a wide range of human activities, yet their usability remains limited by key technical challenges, particularly in ensuring safety and achieving versatile control. Here, we address the critical problem of maintaining balance in the human-SLs system, a prerequisite for safe and comfortable augmentation tasks. Unlike previous approaches that developed SLs specifically for stability support, we propose a general framework for preserving balance with SLs designed for generic use. Our hierarchical three-layer architecture consists of: (i) a prediction layer that estimates human trunk and center of mass (CoM) dynamics, (ii) a planning layer that generates optimal CoM trajectories to counteract trunk movements and computes the corresponding SL control inputs, and (iii) a control layer that executes these inputs on the SL hardware. We evaluated the framework with ten participants performing forward and lateral bending tasks. The results show a clear reduction in stance instability, demonstrating the framework's effectiveness in enhancing balance. This work paves the path towards safe and versatile human-SLs interactions. [This paper has been submitted for publication to IEEE.]

</details>


### [85] [A ROS2 Benchmarking Framework for Hierarchical Control Strategies in Mobile Robots for Mediterranean Greenhouses](https://arxiv.org/abs/2602.15162)
*Fernando Cañadas-Aránega,Francisco J. Mañas-Álvarez,José L- Guzmán,José C. Moreno,José L. Blanco-Claraco*

Main category: cs.RO

TL;DR: 本文提出了一个用于温室环境下移动机器人控制器评估的基准测试框架，涵盖三维环境模型、物理仿真和分层控制架构，支持多类场景和标准化指标，实现客观、可复现的性能比较。


<details>
  <summary>Details</summary>
Motivation: 当前农业移动机器人应用日益广泛，但缺乏标准化、可复现的基准测试，难以公平比较不同控制策略的性能，限制了机器人实际部署和控制系统优化。本文旨在填补这一空白，推动控制算法在真实环境下的系统化评价。

Method: 构建包括三维环境建模、物理仿真器和分层（低/中/高）控制架构的评测框架。设置三种基准类别，覆盖从驱动器级到自主导航的模块化测试。显式建模扰动场景如载荷变化、地形差异和坡度。采用标准化性能指标（SAE、SCI等）和重复试验的统计分析，缓解噪音和环境变异影响。框架基于插件架构，便于用户自定义的控制器和规划器集成。

Result: 框架能在标准化、可重复条件下，对比经典、预测和规划型控制策略在复杂、真实农业环境中的表现，为控制策略提供量化绩效支撑。实验还验证了其在不同扰动和任务下的适应能力与扩展性。

Conclusion: 所提基准测试框架为农业移动机器人控制策略的定量和系统化比较提供了有力工具，推动仿真与真实应用的衔接，有助于促进农业机器人领域控制算法的研发和落地应用。

Abstract: Mobile robots operating in agroindustrial environments, such as Mediterranean greenhouses, are subject to challenging conditions, including uneven terrain, variable friction, payload changes, and terrain slopes, all of which significantly affect control performance and stability. Despite the increasing adoption of robotic platforms in agriculture, the lack of standardized, reproducible benchmarks impedes fair comparisons and systematic evaluations of control strategies under realistic operating conditions. This paper presents a comprehensive benchmarking framework for evaluating mobile robot controllers in greenhouse environments. The proposed framework integrates an accurate three dimensional model of the environment, a physics based simulator, and a hierarchical control architecture comprising low, mid, and high level control layers. Three benchmark categories are defined to enable modular assessment, ranging from actuator level control to full autonomous navigation. Additionally, three disturbance scenarios payload variation, terrain type, and slope are explicitly modeled to replicate real world agricultural conditions. To ensure objective and reproducible evaluation, standardized performance metrics are introduced, including the Squared Absolute Error (SAE), the Squared Control Input (SCI), and composite performance indices. Statistical analysis based on repeated trials is employed to mitigate the influence of sensor noise and environmental variability. The framework is further enhanced by a plugin based architecture that facilitates seamless integration of user defined controllers and planners. The proposed benchmark provides a robust and extensible tool for the quantitative comparison of classical, predictive, and planning based control strategies in realistic conditions, bridging the gap between simulation based analysis and real world agroindustrial applications.

</details>


### [86] [DexEvolve: Evolutionary Optimization for Robust and Diverse Dexterous Grasp Synthesis](https://arxiv.org/abs/2602.15201)
*René Zurbrügg,Andrei Cramariuc,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出了一种可扩展的抓取生成与优化流水线，能在保证多样性和真实物理可行性的前提下，大规模生成多样、稳定的灵巧机器人抓取方式。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的灵巧抓取预测非常依赖大规模、多样化的数据集，但获得这些数据集代价高且通常受限于少数抓取器结构。分析法虽可扩展，但简化假设导致了许多物理上不可行的抓取姿态，需要高保真模拟器筛选，降低了最终可用数据的数量和多样性。

Method: 作者提出一种生成-优化流水线：先用分析方法生成初步抓取，再通过高保真模拟器内的进化算法优化这些抓取，使其更稳定且多样。优化过程不再以直接剔除候选的方式工作，而是通过无梯度、异步的算法不断提升抓取质量，并可引入人类偏好或任务相关指标。最后还将优化后的抓取分布通过扩散模型进行蒸馏，以便实际部署时具有更强泛化能力。

Result: 在新建立的Handles数据集和DexGraspNet子集上，本方法每个物体能产出超过120种稳定独特抓取（比未优化分析法提升1.7-6倍），独特抓取覆盖率比基于扩散的其他方法提升46-60%。

Conclusion: 所提方法大幅提升了灵巧抓取数据的数量、多样性与物理可行性，验证了多样性对抓取训练和部署的积极影响，对实际应用具有更强实用性。

Abstract: Dexterous grasping is fundamental to robotics, yet data-driven grasp prediction heavily relies on large, diverse datasets that are costly to generate and typically limited to a narrow set of gripper morphologies. Analytical grasp synthesis can be used to scale data collection, but necessary simplifying assumptions often yield physically infeasible grasps that need to be filtered in high-fidelity simulators, significantly reducing the total number of grasps and their diversity.
  We propose a scalable generate-and-refine pipeline for synthesizing large-scale, diverse, and physically feasible grasps. Instead of using high-fidelity simulators solely for verification and filtering, we leverage them as an optimization stage that continuously improves grasp quality without discarding precomputed candidates. More specifically, we initialize an evolutionary search with a seed set of analytically generated, potentially suboptimal grasps. We then refine these proposals directly in a high-fidelity simulator (Isaac Sim) using an asynchronous, gradient-free evolutionary algorithm, improving stability while maintaining diversity. In addition, this refinement stage can be guided toward human preferences and/or domain-specific quality metrics without requiring a differentiable objective. We further distill the refined grasp distribution into a diffusion model for robust real-world deployment, and highlight the role of diversity for both effective training and during deployment. Experiments on a newly introduced Handles dataset and a DexGraspNet subset demonstrate that our approach achieves over 120 distinct stable grasps per object (a 1.7-6x improvement over unrefined analytical methods) while outperforming diffusion-based alternatives by 46-60\% in unique grasp coverage.

</details>


### [87] [SEG-JPEG: Simple Visual Semantic Communications for Remote Operation of Automated Vehicles over Unreliable Wireless Networks](https://arxiv.org/abs/2602.15258)
*Sebastian Donnelly,Ruth Anderson,George Economides,James Broughton,Peter Ball,Alexander Rast,Andrew Bradley*

Main category: cs.RO

TL;DR: 本论文提出通过计算机视觉辅助的语义通信方法，提升远程操作自动驾驶车辆在网络受限环境下的图像传输效率，减少数据速率同时保障图像清晰度，提高远程驾驶员的态势感知能力。


<details>
  <summary>Details</summary>
Motivation: 当前远程操作自动驾驶车辆依赖高带宽网络传输影像，受限于公共网络基础设施（如4G/5G），容易因带宽受限导致图像数据丢失和损坏，影响安全和操控效果，因此亟需更高效的图像传输方法。

Method: 采用计算机视觉对道路参与者进行分割，并将其编码为彩色高亮显示于低分辨率灰度图像中，从而减少数据传输量。与传统压缩相比，该方法有效降低所需数据速率约50%，又保留了关键视觉信息。

Result: 在网速低于500kbit/s的4G网络环境下，该方法实现了低于200ms的端到端延迟，并清晰突出道路用户，提升远程驾驶员的感知。实验证明该技术在移动网络条件下的自动驾驶末端物流车辆应用中可行。

Conclusion: 本方法可显著降低远程操作自动驾驶车辆对网络带宽的依赖，使其能在受限的4G/5G公共网络上大规模部署，有望加速自动驾驶车辆的推广应用。

Abstract: Remote Operation is touted as being key to the rapid deployment of automated vehicles. Streaming imagery to control connected vehicles remotely currently requires a reliable, high throughput network connection, which can be limited in real-world remote operation deployments relying on public network infrastructure. This paper investigates how the application of computer vision assisted semantic communication can be used to circumvent data loss and corruption associated with traditional image compression techniques. By encoding the segmentations of detected road users into colour coded highlights within low resolution greyscale imagery, the required data rate can be reduced by 50 \% compared with conventional techniques, while maintaining visual clarity. This enables a median glass-to-glass latency of below 200ms even when the network data rate is below 500kbit/s, while clearly outlining salient road users to enhance situational awareness of the remote operator. The approach is demonstrated in an area of variable 4G mobile connectivity using an automated last-mile delivery vehicle. With this technique, the results indicate that large-scale deployment of remotely operated automated vehicles could be possible even on the often constrained public 4G/5G mobile network, providing the potential to expedite the nationwide roll-out of automated vehicles.

</details>


### [88] [OSCAR: An Ovipositor-Inspired Self-Propelling Capsule Robot for Colonoscopy](https://arxiv.org/abs/2602.15309)
*Mostafa A. Atalla,Anand S. Sekar,Remi van Starkenburg,David J. Jager,Aimée Sakes,Michaël Wiertlewski,Paul Breedveld*

Main category: cs.RO

TL;DR: 本论文提出了一种受寄生蜂产卵器启发、可以自驱动的胶囊机器人（OSCAR），用于结肠镜检查，有效解决传统结肠镜导致病人不适及推进困难的问题，并通过理论与实验验证其推进机制可靠和可控。


<details>
  <summary>Details</summary>
Motivation: 传统结肠镜检查在推进时容易形成导管环，增加患者不适和风险。而现有自驱动胶囊在结肠这种湿滑、黏弹性的环境下很难实现稳定移动，因此需要开发新的推进机制来提高舒适性和安全性。

Method: 设计模仿寄生蜂产卵器的推进方式，利用弹簧和凸轮系统驱动十二个环向滑块分阶段、协调运动，通过增加退回相相对时间，产生界面摩擦各向异性，实现定向推进。并建立了基于Kelvin-Voigt模型的分析方法，用于定量描述滑块与组织间的粘-滑动态，并通过实体猪结肠实验和运动验证。

Result: 实验表明，胶囊最大牵引力达到0.85N，结果与理论模型吻合。推进速度对推力基本无影响，推力随运动相位不对称性线性增加。OSCAR平均推进速度为3.08 mm/s，可满足传统结肠镜的检查要求。

Conclusion: OSCAR胶囊机器人创新推进机制可控且高效，能在低载荷下实现安全、鲁棒的自主运动，适用于胶囊结肠镜检查，有望提高病人舒适性和推动机器人内窥镜技术发展。

Abstract: Self-propelling robotic capsules eliminate shaft looping of conventional colonoscopy, reducing patient discomfort. However, reliably moving within the slippery, viscoelastic environment of the colon remains a significant challenge. We present OSCAR, an ovipositor-inspired self-propelling capsule robot that translates the transport strategy of parasitic wasps into a propulsion mechanism for colonoscopy. OSCAR mechanically encodes the ovipositor-inspired motion pattern through a spring-loaded cam system that drives twelve circumferential sliders in a coordinated, phase-shifted sequence. By tuning the motion profile to maximize the retract phase relative to the advance phase, the capsule creates a controlled friction anisotropy at the interface that generates net forward thrust. We developed an analytical model incorporating a Kelvin-Voigt formulation to capture the viscoelastic stick--slip interactions between the sliders and the tissue, linking the asymmetry between advance and retract phase durations to mean thrust, and slider-reversal synchronization to thrust stability. Comprehensive force characterization experiments in ex-vivo porcine colon revealed a mean steady-state traction force of 0.85 N, closely matching the model. Furthermore, experiments confirmed that thrust generation is speed-independent and scales linearly with the phase asymmetry, in agreement with theoretical predictions, underscoring the capsule's predictable performance and scalability. In locomotion validation experiments, OSCAR demonstrated robust performance, achieving an average speed of 3.08 mm/s, a velocity sufficient to match the cecal intubation times of conventional colonoscopy. By coupling phase-encoded friction anisotropy with a predictive model, OSCAR delivers controllable thrust generation at low normal loads, enabling safer and more robust self-propelling locomotion for robotic capsule colonoscopy.

</details>


### [89] [Feasibility-aware Imitation Learning from Observation with Multimodal Feedback](https://arxiv.org/abs/2602.15351)
*Kei Takahashi,Hikaru Sasaki,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 该论文针对机器人模仿学习中因人与机器人物理差异导致的动作不可行和无机器人动作数据的问题，提出了一种结合可行性估计与观测行为克隆的FABCO方法，有效提升了模仿学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于手部运动演示的模仿学习方法，容易因人-机物理差异导致演示数据不包含真实机器人动作或动作对机器人不可行，严重影响策略学习效果，因此亟需解决这些限制。

Method: 提出FABCO方法：一方面利用机器人动力学模型，对演示动作的可行性进行评估，并将可行性通过视觉与触觉等多模态反馈回演示者以引导更合理动作；另一方面在策略学习阶段弱化对不可行演示动作的依赖，只强化对可行动作的学习。

Result: 通过包含15名参与者的两项任务实验，FABCO方法相比无可行性反馈条件下的模仿学习，能将性能提升3.2倍以上。

Conclusion: FABCO能够有效解决由于人机物理差异带来的动作不可行或数据缺失问题，使机器人学到的模仿策略更加稳定与可执行。

Abstract: Imitation learning frameworks that learn robot control policies from demonstrators' motions via hand-mounted demonstration interfaces have attracted increasing attention. However, due to differences in physical characteristics between demonstrators and robots, this approach faces two limitations: i) the demonstration data do not include robot actions, and ii) the demonstrated motions may be infeasible for robots. These limitations make policy learning difficult. To address them, we propose Feasibility-Aware Behavior Cloning from Observation (FABCO). FABCO integrates behavior cloning from observation, which complements robot actions using robot dynamics models, with feasibility estimation. In feasibility estimation, the demonstrated motions are evaluated using a robot-dynamics model, learned from the robot's execution data, to assess reproducibility under the robot's dynamics. The estimated feasibility is used for multimodal feedback and feasibility-aware policy learning to improve the demonstrator's motions and learn robust policies. Multimodal feedback provides feasibility through the demonstrator's visual and haptic senses to promote feasible demonstrated motions. Feasibility-aware policy learning reduces the influence of demonstrated motions that are infeasible for robots, enabling the learning of policies that robots can execute stably. We conducted experiments with 15 participants on two tasks and confirmed that FABCO improves imitation learning performance by more than 3.2 times compared to the case without feasibility feedback.

</details>


### [90] [A Comparison of Bayesian Prediction Techniques for Mobile Robot Trajectory Tracking](https://arxiv.org/abs/2602.15354)
*Jose Luis Peralta-Cabezas,Miguel Torres-Torriti,Marcelo Guarini-Hermann*

Main category: cs.RO

TL;DR: 本论文对多机器人跟踪问题中，不同估计和预测方法的性能进行了比较，涵盖精度、计算量和抗非高斯噪声能力。涉及卡尔曼滤波及其变体和粒子滤波方法。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统的跟踪精度和鲁棒性在实际应用中至关重要，尤其在面对不同噪声类型和计算资源限制时，需选择最优方法。因此，论文旨在系统比较多种主流方法，帮助确定最优方案。

Method: 论文对比分析了包括标准卡尔曼滤波、扩展卡尔曼滤波、无迹卡尔曼滤波，以及基于序贯蒙特卡洛采样的粒子滤波和高斯混合无迹粒子滤波等方法。通过模拟或实际实验，评估了这些方法在误差大小、计算开销和对非高斯噪声的鲁棒性上的表现。

Result: 各方法在误差、计算复杂度及抗非高斯噪声能力方面表现各异。粒子滤波和高斯混合无迹粒子滤波对非高斯噪声有更强鲁棒性，但计算量较大，而卡尔曼滤波及其变体在计算上高效但对噪声假设存在局限。

Conclusion: 针对多机器人跟踪问题，应根据应用场景在精度、实时性和鲁棒性之间权衡选择不同滤波方法。粒子滤波类方法用于对抗复杂噪声环境，卡尔曼类方法适用于实时性要求高、噪声较标准分布场景。

Abstract: This paper presents a performance comparison of different estimation and prediction techniques applied to the problem of tracking multiple robots. The main performance criteria are the magnitude of the estimation or prediction error, the computational effort and the robustness of each method to non-Gaussian noise. Among the different techniques compared are the well known Kalman filters and their different variants (e.g. extended and unscented), and the more recent techniques relying on Sequential Monte Carlo Sampling methods, such as particle filters and Gaussian Mixture Sigma Point Particle Filter.

</details>


### [91] [Fluoroscopy-Constrained Magnetic Robot Control via Zernike-Based Field Modeling and Nonlinear MPC](https://arxiv.org/abs/2602.15357)
*Xinhao Chen,Hongkun Yao,Anuruddha Bhattacharjee,Suraj Raval,Lamar O. Mair,Yancy Diaz-Mercado,Axel Krieger*

Main category: cs.RO

TL;DR: 本文提出了一种在低帧率噪声反馈环境下用于磁性手术机器人的控制方法，并在实验中取得了高精度和安全性效果。


<details>
  <summary>Details</summary>
Motivation: 磁驱动手术机器人能在减少组织损伤、提高手术精度的同时灵活通过复杂解剖路径，但其临床应用受限于低帧率、噪声多的透视成像反馈下的控制难题。

Method: 方法综合了非线性模型预测控制（NMPC）、基于Zernike多项式的解析可微磁场建模、以及Kalman滤波状态估计，直接输出线圈电流实现机器人的精确控制。

Result: 在3D打印体液环境和脊柱仿生模型中验证该方法，在模拟临床条件下反馈帧率降至3Hz且加入高斯噪声时，方法依然保持高精度。脊柱模型中，药物递送轨迹RMS误差为1.18mm，并实现了关键解剖结构的安全避让。

Conclusion: 所提控制框架在受限的医学成像反馈环境下，能高效提升磁驱动手术机器人定位与安全性能，促进其临床实际应用。

Abstract: Magnetic actuation enables surgical robots to navigate complex anatomical pathways while reducing tissue trauma and improving surgical precision. However, clinical deployment is limited by the challenges of controlling such systems under fluoroscopic imaging, which provides low frame rate and noisy pose feedback. This paper presents a control framework that remains accurate and stable under such conditions by combining a nonlinear model predictive control (NMPC) framework that directly outputs coil currents, an analytically differentiable magnetic field model based on Zernike polynomials, and a Kalman filter to estimate the robot state. Experimental validation is conducted with two magnetic robots in a 3D-printed fluid workspace and a spine phantom replicating drug delivery in the epidural space. Results show the proposed control method remains highly accurate when feedback is downsampled to 3 Hz with added Gaussian noise (sigma = 2 mm), mimicking clinical fluoroscopy. In the spine phantom experiments, the proposed method successfully executed a drug delivery trajectory with a root mean square (RMS) position error of 1.18 mm while maintaining safe clearance from critical anatomical boundaries.

</details>


### [92] [ActionCodec: What Makes for Good Action Tokenizers](https://arxiv.org/abs/2602.15397)
*Zibin Dong,Yicheng Liu,Shiduo Zhang,Baijun Ye,Yifu Yuan,Fei Ni,Jingjing Gong,Xipeng Qiu,Hang Zhao,Yinchuan Li,Jianye Hao*

Main category: cs.RO

TL;DR: 本文针对视觉-语言-动作（VLA）模型中的动作tokenizer设计，提出了新的信息论指导原则，并据此开发了ActionCodec，有效提升了模型训练效率与表现，达到了最新的无机器人预训练VLA模型SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型中的动作tokenizer设计主要关注动作还原精度，而缺乏从提升VLA整体优化表现的角度思考，缺乏对“优秀动作tokenizer应具备哪些特征”的系统研究。

Method: 提出四项基于信息论的tokenizer设计最佳实践：最大化时序token重叠、最小化词汇冗余、增强多模态互信息、提升token独立性。基于这些原则，作者开发了新的高性能actions tokenizer——ActionCodec。

Result: ActionCodec在多个仿真与现实基准任务中均提升了训练效率和VLA表现。在LIBERO数据集上，SmolVLM2-2.2B通过ActionCodec微调后无需任何机器人领域预训练就实现了95.5%的成功率；配合架构增强后可达97.4%，创下无机器人预训练VLA模型的新SOTA。

Conclusion: 基于信息理论的新设计原则能显著提升VLA模型性能。ActionCodec和公开的设计规范为后续有效动作tokenizer开发提供了明确方向，对社区有重要参考价值。

Abstract: Vision-Language-Action (VLA) models leveraging the native autoregressive paradigm of Vision-Language Models (VLMs) have demonstrated superior instruction-following and training efficiency. Central to this paradigm is action tokenization, yet its design has primarily focused on reconstruction fidelity, failing to address its direct impact on VLA optimization. Consequently, the fundamental question of \textit{what makes for good action tokenizers} remains unanswered. In this paper, we bridge this gap by establishing design principles specifically from the perspective of VLA optimization. We identify a set of best practices based on information-theoretic insights, including maximized temporal token overlap, minimized vocabulary redundancy, enhanced multimodal mutual information, and token independence. Guided by these principles, we introduce \textbf{ActionCodec}, a high-performance action tokenizer that significantly enhances both training efficiency and VLA performance across diverse simulation and real-world benchmarks. Notably, on LIBERO, a SmolVLM2-2.2B fine-tuned with ActionCodec achieves a 95.5\% success rate without any robotics pre-training. With advanced architectural enhancements, this reaches 97.4\%, representing a new SOTA for VLA models without robotics pre-training. We believe our established design principles, alongside the released model, will provide a clear roadmap for the community to develop more effective action tokenizers.

</details>


### [93] [Hybrid F' and ROS2 Architecture for Vision-Based Autonomous Flight: Design and Experimental Validation](https://arxiv.org/abs/2602.15398)
*Abdelrahman Metwally,Monijesu James,Aleksey Fedoseev,Miguel Altamirano Cabrera,Dzmitry Tsetserukou,Andrey Somov*

Main category: cs.RO

TL;DR: 本文提出了一种集成NASA F'飞行软件和ROS2中间件的无人机系统，并通过室内飞行试验验证了其实时性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 航空航天自主系统需同时具备确定性实时控制和先进感知能力。传统飞行控制系统侧重安全可靠，但难以满足复杂感知与自治需求。本文旨在解决这一平衡问题。

Method: 提出将NASA的F'飞行软件框架与ROS2通信中间件通过Protocol Buffers桥接，构建混合架构系统，并在四旋翼无人机上进行32.25分钟的室内视觉导航飞行测试，评估其实时、感知与资源利用表现。

Result: 视觉系统实现了87.19Hz的位置估算，99.90%连续性、平均延迟11.47ms，满足实时性要求。15条地面指令全部成功，CPU占用15.19%、内存1244MB，无过时遥测，表现出高效资源利用和鲁棒集成。

Conclusion: 验证了结合认证级实时性与灵活自治的混合飞控架构的可行性，为无人机等自主航空器软件系统设计提供了参考。

Abstract: Autonomous aerospace systems require architectures that balance deterministic real-time control with advanced perception capabilities. This paper presents an integrated system combining NASA's F' flight software framework with ROS2 middleware via Protocol Buffers bridging. We evaluate the architecture through a 32.25-minute indoor quadrotor flight test using vision-based navigation. The vision system achieved 87.19 Hz position estimation with 99.90\% data continuity and 11.47 ms mean latency, validating real-time performance requirements. All 15 ground commands executed successfully with 100 % success rate, demonstrating robust F'--PX4 integration. System resource utilization remained low (15.19 % CPU, 1,244 MB RAM) with zero stale telemetry messages, confirming efficient operation on embedded platforms. Results validate the feasibility of hybrid flight-software architectures combining certification-grade determinism with flexible autonomy for autonomous aerial vehicles.

</details>


### [94] [One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation](https://arxiv.org/abs/2602.15400)
*Zerui Li,Hongpei Zheng,Fangguo Zhao,Aidan Chan,Jian Zhou,Sihao Lin,Shijie Li,Qi Wu*

Main category: cs.RO

TL;DR: 本文提出了一个解耦式多模态大模型导航系统，将低级空间状态估计与高级语义规划分离，并用交互式度量世界表示和反事实推理提升决策能力，实现了零样本下的最优效果，并在多平台和仿真/真实环境间展示了优秀的泛化与适应能力。


<details>
  <summary>Details</summary>
Motivation: 以往大模型导航方法将空间状态估计与语义规划紧密耦合，且依赖简化文本地图，导致泛化和表现受限。作者希望解决这一局限，提升导航智能体的理解和适应能力。

Method: 方法上，作者将系统解耦为低级空间状态估计和高级语义规划，用丰富一致的交互式度量世界表示来存储环境信息，并让多模态大模型在此基础上推理，用反事实推理进一步激发大模型能力，确保行动物理合理。

Result: 在模拟与真实环境下，方法在R2R-CE和RxR-CE基准上零样本下达到了48.8%和42.2%的成功率。还验证了度量表示的通用性，实现了“仿真到现实”的迁移，在地面和空中机器人上都表现出色。

Conclusion: 提出的解耦框架和度量世界表示能够为实体视觉-语言导航任务提供强大且具有领域无关性的接口，并具有优越的泛化和适应能力，推动了多模态大模型在实际机器人导航中的应用。

Abstract: A navigable agent needs to understand both high-level semantic instructions and precise spatial perceptions. Building navigation agents centered on Multimodal Large Language Models (MLLMs) demonstrates a promising solution due to their powerful generalization ability. However, the current tightly coupled design dramatically limits system performance. In this work, we propose a decoupled design that separates low-level spatial state estimation from high-level semantic planning. Unlike previous methods that rely on predefined, oversimplified textual maps, we introduce an interactive metric world representation that maintains rich and consistent information, allowing MLLMs to interact with and reason on it for decision-making. Furthermore, counterfactual reasoning is introduced to further elicit MLLMs' capacity, while the metric world representation ensures the physical validity of the produced actions. We conduct comprehensive experiments in both simulated and real-world environments. Our method establishes a new zero-shot state-of-the-art, achieving 48.8\% Success Rate (SR) in R2R-CE and 42.2\% in RxR-CE benchmarks. Furthermore, to validate the versatility of our metric representation, we demonstrate zero-shot sim-to-real transfer across diverse embodiments, including a wheeled TurtleBot 4 and a custom-built aerial drone. These real-world deployments verify that our decoupled framework serves as a robust, domain-invariant interface for embodied Vision-and-Language navigation.

</details>


### [95] [Lyapunov-Based $\mathcal{L}_2$-Stable PI-Like Control of a Four-Wheel Independently Driven and Steered Robot](https://arxiv.org/abs/2602.15424)
*Branimir Ćaran,Vladimir Milić,Bojan Jerbić*

Main category: cs.RO

TL;DR: 本文提出了一种基于Lyapunov方法的类PI控制器，实现了对四轮独立驱动及转向移动机器人运动的稳定控制，并通过真实平台实验验证了其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前四轮独立驱动与转向移动机器人的运动控制存在稳定性和实时性挑战，传统方法难以兼顾鲁棒性、精确性及易于嵌入式实现等需求。本文旨在通过Lyapunov理论，在保证稳定性的前提下设计一种适用于实际系统的高性能控制器。

Method: 文章采用显式、经结构验证的动力学模型，以Lyapunov函数方法导出运动控制器。通过系统地设计反馈律，获得了带有可明确计算界限的$L_2$稳定性。提出的控制律具备PI控制器的形式，便于嵌入式实施。

Result: 通过理论分析证明了控制器的$L_2$稳定性，并明确给出了稳定性界限。随后在真实四轮移动机器人平台上进行了实验，结果表明该方法具有良好的有效性和鲁棒性。

Conclusion: 基于Lyapunov法合成的类PI控制器不仅保证了系统的稳定性和性能，还适合实时和嵌入式应用，为移动机器人运动控制提供了一种兼具理论保障与工程可行性的方案。

Abstract: In this letter, Lyapunov-based synthesis of a PI-like controller is proposed for $\mathcal{L}_2$-stable motion control of an independently driven and steered four-wheel mobile robot. An explicit, structurally verified model is used to enable systematic controller design with stability and performance guarantees suitable for real-time operation. A Lyapunov function is constructed to yield explicit bounds and $\mathcal{L}_2$ stability results, supporting feedback synthesis that reduces configuration dependent effects. The resulting control law maintains a PI-like form suitable for standard embedded implementation while preserving rigorous stability properties. Effectiveness and robustness are demonstrated experimentally on a real four-wheel mobile robot platform.

</details>


### [96] [Improving MLLMs in Embodied Exploration and Question Answering with Human-Inspired Memory Modeling](https://arxiv.org/abs/2602.15513)
*Ji Li,Jing Xia,Mingyi Li,Shiyan Hu*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的具身智能体记忆框架，通过区分情景记忆与语义记忆，并结合视觉推理和可重用规则提取，实现了在长时观测和有限上下文下，对多模态大模型在智能体任务中的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前将多模态大模型作为具身智能体“大脑”在长时序、有限上下文条件下部署存在挑战。现有基于记忆的方法多依赖文本摘要，丢失了丰富的视觉与空间信息，且在动态环境中表现不稳健。

Method: 作者提出非参数化记忆框架，显式区分情景记忆（episodic memory）和语义记忆（semantic memory）。方法采用“先检索、后推理”范式，通过语义相似性回忆情景经历，并用视觉推理验证，避免依赖严格的几何对齐。同时，提出程序式规则抽取机制，将经验转化为结构化、可复用的语义记忆，促进跨环境泛化能力。

Result: 在A-EQA与GOAT-Bench等标准基准上的实验证明该方法优于当前技术，A-EQA的LLM-Match提升7.3%、LLM MatchXSPL提升11.4%，GOAT-Bench的成功率提升7.7%、SPL提升6.8%。

Conclusion: 该框架能有效提升具身任务中的探索效率（主要依赖情景记忆）和复杂推理能力（主要得益于语义记忆），为多模态大模型应用于现实具身智能体提供了新思路与方法。

Abstract: Deploying Multimodal Large Language Models as the brain of embodied agents remains challenging, particularly under long-horizon observations and limited context budgets. Existing memory assisted methods often rely on textual summaries, which discard rich visual and spatial details and remain brittle in non-stationary environments. In this work, we propose a non-parametric memory framework that explicitly disentangles episodic and semantic memory for embodied exploration and question answering. Our retrieval-first, reasoning-assisted paradigm recalls episodic experiences via semantic similarity and verifies them through visual reasoning, enabling robust reuse of past observations without rigid geometric alignment. In parallel, we introduce a program-style rule extraction mechanism that converts experiences into structured, reusable semantic memory, facilitating cross-environment generalization. Extensive experiments demonstrate state-of-the-art performance on embodied question answering and exploration benchmarks, yielding a 7.3% gain in LLM-Match and an 11.4% gain in LLM MatchXSPL on A-EQA, as well as +7.7% success rate and +6.8% SPL on GOAT-Bench. Analyses reveal that our episodic memory primarily improves exploration efficiency, while semantic memory strengthens complex reasoning of embodied agents.

</details>


### [97] [Efficient Knowledge Transfer for Jump-Starting Control Policy Learning of Multirotors through Physics-Aware Neural Architectures](https://arxiv.org/abs/2602.15533)
*Welf Rehberg,Mihir Kulkarni,Philipp Weiss,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出了一种基于知识迁移的策略训练初始化方法，能有效减少机器人控制策略训练所需的环境交互次数，在多旋翼无人机多种配置上实现高效迁移和控制。


<details>
  <summary>Details</summary>
Motivation: 现有机器人控制策略训练成本高、数据需求大，如何通过跨主体知识迁移提高训练效率和泛化能力，是实现高效自主机器控制的重要难题。

Method: 提出了一种结合强化学习控制器与监督学习分配网络的物理感知神经控制架构。通过策略相似性评价方法，从策略库中选择合适的预训练策略用于新任务初始化，提升策略迁移效果。

Result: 在多种四旋翼、六旋翼配置下，仿真和真实实验均表明：所提方案保持了最先进的控制性能，同时平均减少73.5%的环境交互次数。

Conclusion: 新的初始化与控制架构大幅提升了跨平台策略迁移效率，是强化学习机器人控制实际应用的有效途径。

Abstract: Efficiently training control policies for robots is a major challenge that can greatly benefit from utilizing knowledge gained from training similar systems through cross-embodiment knowledge transfer. In this work, we focus on accelerating policy training using a library-based initialization scheme that enables effective knowledge transfer across multirotor configurations. By leveraging a physics-aware neural control architecture that combines a reinforcement learning-based controller and a supervised control allocation network, we enable the reuse of previously trained policies. To this end, we utilize a policy evaluation-based similarity measure that identifies suitable policies for initialization from a library. We demonstrate that this measure correlates with the reduction in environment interactions needed to reach target performance and is therefore suited for initialization. Extensive simulation and real-world experiments confirm that our control architecture achieves state-of-the-art control performance, and that our initialization scheme saves on average up to $73.5\%$ of environment interactions (compared to training a policy from scratch) across diverse quadrotor and hexarotor designs, paving the way for efficient cross-embodiment transfer in reinforcement learning.

</details>


### [98] [Selective Perception for Robot: Task-Aware Attention in Multimodal VLA](https://arxiv.org/abs/2602.15543)
*Young-Chae Son,Jung-Woo Lee,Yoon-Ji Choi,Dae-Kwan Ko,Soo-Chul Lim*

Main category: cs.RO

TL;DR: 提出一种动态信息融合框架，通过分析任务相关性，实现高效且稳健的机器人视觉-语言-动作模型。


<details>
  <summary>Details</summary>
Motivation: 传统的多模态融合方法对不同视觉输入一视同仁，导致不必要的计算浪费和噪音干扰。如何像人类一样主动甄别和利用与任务强相关的信息，是提高模型效率和鲁棒性的关键。

Method: 设计了一个轻量级自适应路由架构，实时分析文本指令和腕部摄像头视觉观察，预测多视角图像的任务相关性。对低信息价值的视角动态降低计算负担，仅将关键信息输入策略网络。同时，构建自动标注流程，利用视觉-语言模型大幅减少数据标注成本。

Result: 在实际机器人操作实验中，所提方法在推理效率和控制表现方面均大幅优于静态融合VLA模型，显著减少了运算量。

Conclusion: 动态信息融合在资源受限、实时控制场景下，对提升机器人感知与动作表现非常有效且实用。

Abstract: In robotics, Vision-Language-Action (VLA) models that integrate diverse multimodal signals from multi-view inputs have emerged as an effective approach. However, most prior work adopts static fusion that processes all visual inputs uniformly, which incurs unnecessary computational overhead and allows task-irrelevant background information to act as noise. Inspired by the principles of human active perception, we propose a dynamic information fusion framework designed to maximize the efficiency and robustness of VLA models. Our approach introduces a lightweight adaptive routing architecture that analyzes the current text prompt and observations from a wrist-mounted camera in real-time to predict the task-relevance of multiple camera views. By conditionally attenuating computations for views with low informational utility and selectively providing only essential visual features to the policy network, Our framework achieves computation efficiency proportional to task relevance. Furthermore, to efficiently secure large-scale annotation data for router training, we established an automated labeling pipeline utilizing Vision-Language Models (VLMs) to minimize data collection and annotation costs. Experimental results in real-world robotic manipulation scenarios demonstrate that the proposed approach achieves significant improvements in both inference efficiency and control performance compared to existing VLA models, validating the effectiveness and practicality of dynamic information fusion in resource-constrained, real-time robot control environments.

</details>


### [99] [VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing](https://arxiv.org/abs/2602.15549)
*Guoqin Tang,Qingxuan Jia,Gang Chen,Tong Li,Zeyuan Huang,Zihang Lv,Ning Ji*

Main category: cs.RO

TL;DR: 该论文针对VLM在智能制造中高层规划的实际应用问题，提出了解决方案VLM-DEWM，大幅提升了状态跟踪和恢复能力。


<details>
  <summary>Details</summary>
Motivation: VLM虽然适用于高层规划，但其无状态特性导致无法持续追踪工作单元状态，且推理过程不透明，难以诊断和恢复失败，这在动态的制造环境中是关键障碍。

Method: 提出了VLM-DEWM架构，通过引入可持久化、可查询的动态外部世界模型（DEWM）将VLM推理与世界状态管理分离。每次VLM决策都输出可外化的推理链（ERT），结合动作建议、世界信念和因果假设，并在执行前与DEWM验证。出现故障时，通过比较预测和实际状态进行差异分析，从而有针对性地恢复。

Result: 在多工位装配、大规模场地探索和模拟及真实机器人恢复等任务上，VLM-DEWM将状态跟踪准确率从56%提升到93%，恢复成功率从不到5%提升到95%，并显著减少了计算开销。

Conclusion: VLM-DEWM在动态制造环境中的长周期机器人操作中表现出良好的验证性和鲁棒性，是较现有方法更为可靠的解决方案。

Abstract: Vision-language model (VLM) shows promise for high-level planning in smart manufacturing, yet their deployment in dynamic workcells faces two critical challenges: (1) stateless operation, they cannot persistently track out-of-view states, causing world-state drift; and (2) opaque reasoning, failures are difficult to diagnose, leading to costly blind retries. This paper presents VLM-DEWM, a cognitive architecture that decouples VLM reasoning from world-state management through a persistent, queryable Dynamic External World Model (DEWM). Each VLM decision is structured into an Externalizable Reasoning Trace (ERT), comprising action proposal, world belief, and causal assumption, which is validated against DEWM before execution. When failures occur, discrepancy analysis between predicted and observed states enables targeted recovery instead of global replanning. We evaluate VLM-DEWM on multi-station assembly, large-scale facility exploration, and real-robot recovery under induced failures. Compared to baseline memory-augmented VLM systems, VLM DEWM improves state-tracking accuracy from 56% to 93%, increases recovery success rate from below 5% to 95%, and significantly reduces computational overhead through structured memory. These results establish VLM-DEWM as a verifiable and resilient solution for long-horizon robotic operations in dynamic manufacturing environments.

</details>


### [100] [Constraining Streaming Flow Models for Adapting Learned Robot Trajectory Distributions](https://arxiv.org/abs/2602.15567)
*Jieting Long,Dechuan Liu,Weidong Cai,Ian Manchester,Weiming Zhi*

Main category: cs.RO

TL;DR: 本文提出了一种名为Constraint-Aware Streaming Flow (CASF)的新方法，实现了对机器人轨迹生成时多模态、灵活且受约束的控制，可在不牺牲平滑性和响应性的情况下，实时满足安全与任务约束。


<details>
  <summary>Details</summary>
Motivation: 机器人运动轨迹具有多模态，现有生成方法虽有流式策略（SFPs），但无法在训练后灵活适应安全和任务相关约束，因此迫切需要一种能在执行时调整轨迹以满足约束的新方法。

Method: 提出CASF方法，在原有流式策略基础上，利用可微分的距离函数刻画各类约束（如避障、关节极限），将其映射成局部度量，在靠近约束边界时调整速度场，远离时不变，从而在控制空间实时变形生成的轨迹。

Result: 通过仿真和真实操作实验，CASF实现了同时满足约束、保持轨迹平滑与动态一致性，并且在任务完成度上优于标准的轨迹投影基线。

Conclusion: CASF能够在不牺牲多模态与反应能力的前提下，灵活满足机器人运动中的多种约束，提升安全性和任务适应性，效果显著优于传统后处理方法。

Abstract: Robot motion distributions often exhibit multi-modality and require flexible generative models for accurate representation. Streaming Flow Policies (SFPs) have recently emerged as a powerful paradigm for generating robot trajectories by integrating learned velocity fields directly in action space, enabling smooth and reactive control. However, existing formulations lack mechanisms for adapting trajectories post-training to enforce safety and task-specific constraints. We propose Constraint-Aware Streaming Flow (CASF), a framework that augments streaming flow policies with constraint-dependent metrics that reshape the learned velocity field during execution. CASF models each constraint, defined in either the robot's workspace or configuration space, as a differentiable distance function that is converted into a local metric and pulled back into the robot's control space. Far from restricted regions, the resulting metric reduces to the identity; near constraint boundaries, it smoothly attenuates or redirects motion, effectively deforming the underlying flow to maintain safety. This allows trajectories to be adapted in real time, ensuring that robot actions respect joint limits, avoid collisions, and remain within feasible workspaces, while preserving the multi-modal and reactive properties of streaming flow policies. We demonstrate CASF in simulated and real-world manipulation tasks, showing that it produces constraint-satisfying trajectories that remain smooth, feasible, and dynamically consistent, outperforming standard post-hoc projection baselines.

</details>


### [101] [Grip as Needed, Glide on Demand: Ultrasonic Lubrication for Robotic Locomotion](https://arxiv.org/abs/2602.15608)
*Mostafa A. Atalla,Daan van Bemmel,Jack Cummings,Paul Breedveld,Michaël Wiertlewski,Aimée Sakes*

Main category: cs.RO

TL;DR: 本论文提出了超声润滑技术，实现了对机器人运动中摩擦力的主动控制，大幅提升了运动效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 目前大多数机器人将摩擦力视为被动且固定的特性，受限于材质和表面条件，难以动态调节，影响了其适应复杂环境的能力。

Method: 作者提出利用超声激振共振结构，主动切换接触界面在“抓握”和“滑动”状态。并开发了两种摩擦控制模块（圆柱形和板形），分别集成到仿生尺蠖和仿黄蜂产卵器机器人中。

Result: 两种系统均实现了双向运动，运动效率超过90%。在多种表面（硬性、软性、颗粒、生物组织）和不同环境（干燥、潮湿、不同粗糙度）下，超声润滑均显著降低了摩擦。

Conclusion: 超声润滑为机器人运动提供了一种主动控制摩擦的新机制，具有广泛适用性，可简化设计、提升运动效率。

Abstract: Friction is the essential mediator of terrestrial locomotion, yet in robotic systems it is almost always treated as a passive property fixed by surface materials and conditions. Here, we introduce ultrasonic lubrication as a method to actively control friction in robotic locomotion. By exciting resonant structures at ultrasonic frequencies, contact interfaces can dynamically switch between "grip" and "slip" states, enabling locomotion. We developed two friction control modules, a cylindrical design for lumen-like environments and a flat-plate design for external surfaces, and integrated them into bio-inspired systems modeled after inchworm and wasp ovipositor locomotion. Both systems achieved bidirectional locomotion with nearly perfect locomotion efficiencies that exceeded 90%. Friction characterization experiments further demonstrated substantial friction reduction across various surfaces, including rigid, soft, granular, and biological tissue interfaces, under dry and wet conditions, and on surfaces with different levels of roughness, confirming the broad applicability of ultrasonic lubrication to locomotion tasks. These findings establish ultrasonic lubrication as a viable active friction control mechanism for robotic locomotion, with the potential to reduce design complexity and improve efficiency of robotic locomotion systems.

</details>


### [102] [SpecFuse: A Spectral-Temporal Fusion Predictive Control Framework for UAV Landing on Oscillating Marine Platforms](https://arxiv.org/abs/2602.15633)
*Haichao Liu,Yufeng Hu,Shuang Wang,Kangjun Guo,Jun Ma,Jinni Zhou*

Main category: cs.RO

TL;DR: 该论文提出了一种用于无人机在波动海平台上自主精确降落的新预测与控制框架，并在仿真与实地实验中显著提升了降落精度和成功率。


<details>
  <summary>Details</summary>
Motivation: 在波动海面等动态条件下，无人机自主降落受到复杂的多频振荡、风扰动及运动预测延迟的严重影响，现有预测和控制方法无法充分利用波浪频谱特性，导致降落精度较低，亟需更鲁棒、有效的方法。

Method: 提出了SpecFuse框架，通过频域波动分解与时域状态递推相结合，精准建模主导波动，实时矫正，由IMU数据驱动无需复杂标定。结合分层控制结构，采用基于采样的HPO-RRT*算法动态轨迹规划，并在控制层融合学习驱动的扰动补偿与优化执行。

Result: 在2,000次仿真和8次真实湖面实验中，预测误差仅为3.2厘米，降落偏差4.46厘米，仿真/实地成功率98.7%/87.5%，控制系统延迟仅82毫秒，精度较现有方法提升44%-48%。

Conclusion: 该方法在海浪-风耦合扰动下表现出极强鲁棒性，适用于海上搜救、环境监测等关键任务。代码与数据集将全部开源，支持重复实验和后续研究。

Abstract: Autonomous landing of Uncrewed Aerial Vehicles (UAVs) on oscillating marine platforms is severely constrained by wave-induced multi-frequency oscillations, wind disturbances, and prediction phase lags in motion prediction. Existing methods either treat platform motion as a general random process or lack explicit modeling of wave spectral characteristics, leading to suboptimal performance under dynamic sea conditions. To address these limitations, we propose SpecFuse: a novel spectral-temporal fusion predictive control framework that integrates frequency-domain wave decomposition with time-domain recursive state estimation for high-precision 6-DoF motion forecasting of Uncrewed Surface Vehicles (USVs). The framework explicitly models dominant wave harmonics to mitigate phase lags, refining predictions in real time via IMU data without relying on complex calibration. Additionally, we design a hierarchical control architecture featuring a sampling-based HPO-RRT* algorithm for dynamic trajectory planning under non-convex constraints and a learning-augmented predictive controller that fuses data-driven disturbance compensation with optimization-based execution. Extensive validations (2,000 simulations + 8 lake experiments) show our approach achieves a 3.2 cm prediction error, 4.46 cm landing deviation, 98.7% / 87.5% success rates (simulation / real-world), and 82 ms latency on embedded hardware, outperforming state-of-the-art methods by 44%-48% in accuracy. Its robustness to wave-wind coupling disturbances supports critical maritime missions such as search and rescue and environmental monitoring. All code, experimental configurations, and datasets will be released as open-source to facilitate reproducibility.

</details>


### [103] [Spatially-Aware Adaptive Trajectory Optimization with Controller-Guided Feedback for Autonomous Racing](https://arxiv.org/abs/2602.15642)
*Alexander Wachter,Alexander Willert,Marc-Philip Ecker,Christian Hartl-Nesic*

Main category: cs.RO

TL;DR: 本文提出一种闭环自动赛车路线优化框架，将NURBS轨迹表示、CMA-ES全局优化和基于控制器的空间反馈结合，实现了对赛车轨迹的自适应优化，并在仿真和实车测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统赛车轨迹优化往往仅将跟踪误差视为扰动，忽略了它作为道路和车辆状态信息载体的潜力，导致难以适应实际场地与车辆参数变化，尤其是在路况、轮胎抓地力等因素变化时难以实时调整。

Method: 方法上，作者采用了NURBS（非均匀有理B样条）表示赛道轨迹，通过CMA-ES（协方差矩阵自适应进化策略）进行全局轨迹优化。同时，利用控制器反馈的跟踪误差信息，通过类似Kalman滤波的空间更新，构建基于加速度的自适应约束映射，不断迭代优化轨迹以适应空间变化的赛道和车辆行为。

Result: 在仿真测试中，该方法使单圈时间比最大静态加速度参数化控制器降低了17.38%。实车测试涵盖了不同抓地力轮胎（从高到低），无需人为建模摩擦力参数也能提升圈速，平均提高了7.60%。

Conclusion: 结果表明，该框架对实际情况中抓地力变化表现出强鲁棒性，无需显式参数调整，能有效提升赛车表现，对自动驾驶赛道车辆等场景具有重要应用价值。

Abstract: We present a closed-loop framework for autonomous raceline optimization that combines NURBS-based trajectory representation, CMA-ES global trajectory optimization, and controller-guided spatial feedback. Instead of treating tracking errors as transient disturbances, our method exploits them as informative signals of local track characteristics via a Kalman-inspired spatial update. This enables the construction of an adaptive, acceleration-based constraint map that iteratively refines trajectories toward near-optimal performance under spatially varying track and vehicle behavior. In simulation, our approach achieves a 17.38% lap time reduction compared to a controller parametrized with maximum static acceleration. On real hardware, tested with different tire compounds ranging from high to low friction, we obtain a 7.60% lap time improvement without explicitly parametrizing friction. This demonstrates robustness to changing grip conditions in real-world scenarios.

</details>


### [104] [Estimating Human Muscular Fatigue in Dynamic Collaborative Robotic Tasks with Learning-Based Models](https://arxiv.org/abs/2602.15684)
*Feras Kiki,Pouya P. Niaz,Alireza Madani,Cagatay Basdogan*

Main category: cs.RO

TL;DR: 该论文提出了一套利用基于表面肌电信号(sEMG)和机器学习方法动态估算人-机协作过程中心理疲劳的框架，并验证了其准确性和泛化能力。其中卷积神经网络（CNN）模型性能最佳。


<details>
  <summary>Details</summary>
Motivation: 在物理人机协作（pHRI）场景下，高效评估工作人员的肌肉疲劳对于提升操作安全性和性能非常重要。目前的疲劳评估方法多为分类或间断方法，难以及时、连续地反映疲劳进程，因此亟需开发一种基于数据驱动、可连续估算疲劳程度的方法。

Method: 作者在志愿者手臂上采集肌电信号，通过特征提取（频域、时域特征）和多种回归机器学习方法（随机森林、XGBoost、线性回归）预测疲劳周期比（FCF），并引入基于时频图的CNN模型进行对比。实验中，协作机器人引导重复性的手臂运动直到志愿者肌肉疲劳，通过实际误差（RMSE）进行性能评估。还考查了模型在未见过的运动模式（方向、轨迹）上的泛化能力。

Result: CNN模型在FCF预测中的平均RMSE为20.8%，优于随机森林、XGBoost和线性回归模型（分别为23.3%、24.8%、26.9%），在任务迁移（不同运动模式）时除了线性回归外其他模型准确率保持良好，表现出较强的鲁棒性。

Conclusion: 本文证明了基于特征的机器学习和基于谱图的深度学习方法均可实现pHRI过程中剩余工作能力的估算，且不需要针对每个动作模式单独训练，具备实际应用潜力，可提升人机协作的安全性和智能化水平。

Abstract: Assessing human muscle fatigue is critical for optimizing performance and safety in physical human-robot interaction(pHRI). This work presents a data-driven framework to estimate fatigue in dynamic, cyclic pHRI using arm-mounted surface electromyography(sEMG). Subject-specific machine-learning regression models(Random Forest, XGBoost, and Linear Regression predict the fraction of cycles to fatigue(FCF) from three frequency-domain and one time-domain EMG features, and are benchmarked against a convolutional neural network(CNN) that ingests spectrograms of filtered EMG. Framing fatigue estimation as regression (rather than classification) captures continuous progression toward fatigue, supporting earlier detection, timely intervention, and adaptive robot control. In experiments with ten participants, a collaborative robot under admittance control guided repetitive lateral (left-right) end-effector motions until muscular fatigue. Average FCF RMSE across participants was 20.8+/-4.3% for the CNN, 23.3+/-3.8% for Random Forest, 24.8+/-4.5% for XGBoost, and 26.9+/-6.1% for Linear Regression. To probe cross-task generalization, one participant additionally performed unseen vertical (up-down) and circular repetitions; models trained only on lateral data were tested directly and largely retained accuracy, indicating robustness to changes in movement direction, arm kinematics, and muscle recruitment, while Linear Regression deteriorated. Overall, the study shows that both feature-based ML and spectrogram-based DL can estimate remaining work capacity during repetitive pHRI, with the CNN delivering the lowest error and the tree-based models close behind. The reported transfer to new motion patterns suggests potential for practical fatigue monitoring without retraining for every task, improving operator protection and enabling fatigue-aware shared autonomy, for safer fatigue-adaptive pHRI control.

</details>


### [105] [Lifelong Scalable Multi-Agent Realistic Testbed and A Comprehensive Study on Design Choices in Lifelong AGV Fleet Management Systems](https://arxiv.org/abs/2602.15721)
*Jingtian Yan,Yulun Zhang,Zhenting Liu,Han Zhang,He Jiang,Jingkai Chen,Stephen F. Smith,Jiaoyang Li*

Main category: cs.RO

TL;DR: 本文提出LSMART，一个面向多智能体路径规划（MAPF）和任务执行的开放模拟平台，适用于考察自动导引车（AGVs）车队管理系统内各类算法的实际表现。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体路径规划（MAPF）与持续任务版本（LMAPF）等研究大多假设简化的动力学模型和完美的执行与通信环境，难以落地到现实AGV车队管理系统，而现有评价工具（如SMART）也未考虑持续任务及相关复杂度。为实现高效、可扩展的车队管理系统，需要一个能综合考虑各种实际因素的测试环境。

Method: 作者开发了LSMART模拟器，支持评估任何MAPF算法在实际车队管理系统（FMS）中的表现。LSMART不仅支持动力学、通信延迟和执行不确定性，还涵盖了持续新目标分配、规划/执行并行、算法失效恢复等核心问题。文中还基于主流路径规划方法对各设计方案进行了实验比较。

Result: 通过对不同规划算法和关键设计点（如何时/如何规划、失效恢复策略）进行实验，作者提供了实现集中式终身AGV车队管理系统的有效建议。所有实验结果均基于LSMART进行，证明其在复杂场景中的应用价值。

Conclusion: LSMART提供了一个系统化、可扩展的测试平台，为真实FMS场景下多智能体算法的设计和评测提供了基础，推动了AGV车队管理研究的现实化进程。

Abstract: We present Lifelong Scalable Multi-Agent Realistic Testbed (LSMART), an open-source simulator to evaluate any Multi-Agent Path Finding (MAPF) algorithm in a Fleet Management System (FMS) with Automated Guided Vehicles (AGVs). MAPF aims to move a group of agents from their corresponding starting locations to their goals. Lifelong MAPF (LMAPF) is a variant of MAPF that continuously assigns new goals for agents to reach. LMAPF applications, such as autonomous warehouses, often require a centralized, lifelong system to coordinate the movement of a fleet of robots, typically AGVs. However, existing works on MAPF and LMAPF often assume simplified kinodynamic models, such as pebble motion, as well as perfect execution and communication for AGVs. Prior work has presented SMART, a software capable of evaluating any MAPF algorithms while considering agent kinodynamics, communication delays, and execution uncertainties. However, SMART is designed for MAPF, not LMAPF. Generalizing SMART to an FMS requires many more design choices. First, an FMS parallelizes planning and execution, raising the question of when to plan. Second, given planners with varying optimality and differing agent-model assumptions, one must decide how to plan. Third, when the planner fails to return valid solutions, the system must determine how to recover. In this paper, we first present LSMART, an open-source simulator that incorporates all these considerations to evaluate any MAPF algorithms in an FMS. We then provide experiment results based on state-of-the-art methods for each design choice, offering guidance on how to effectively design centralized lifelong AGV Fleet Management Systems. LSMART is available at https://smart-mapf.github.io/lifelong-smart.

</details>


### [106] [MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction](https://arxiv.org/abs/2602.15733)
*Qiang Zhang,Jiahao Ma,Peiran Liu,Shuai Shi,Zeran Su,Zifan Wang,Jingkai Sun,Wei Cui,Jialin Yu,Gang Han,Wen Zhao,Pihai Sun,Kangning Yin,Jiaxu Wang,Jiahang Cao,Lingfeng Zhang,Hao Cheng,Xiaoshuai Hao,Yiding Ji,Junwei Liang,Jian Tang,Renjing Xu,Yijie Guo*

Main category: cs.RO

TL;DR: 该论文提出MeshMimic框架，结合3D视觉与强化学习，使人形机器人能直接从视频中学习人-环境耦合动作，显著提升在多样复杂地形下的动态表现。


<details>
  <summary>Details</summary>
Motivation: 传统的人形机器人动作控制严重依赖昂贵且有限的动作捕捉数据，且往往缺乏环境几何信息，导致动作与场景解耦，产生物理不一致问题，如接触滑移或模型穿透。

Method: 该方法利用先进的3D视觉模型，精准分割与重建人体运动轨迹及环境的3D几何信息。提出基于运动学一致性的优化算法，从噪声视觉重建数据中提取高质量动作数据，并通过接触不变的重定向技术将人-环境交互特征转移到机器人身上。

Result: 实验结果显示，MeshMimic在多种复杂地形上实现了鲁棒且高度动态的运动表现，推动了仅用消费级单目传感器训练复杂物理交互的可行性。

Conclusion: MeshMimic展示了低成本、可扩展的人形机器人运动生成方案，有望促进机器人在非结构化环境中的自主能力进化。

Abstract: Humanoid motion control has witnessed significant breakthroughs in recent years, with deep reinforcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled "motion-terrain" interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. Our approach proves that a low-cost pipeline utilizing only consumer-grade monocular sensors can facilitate the training of complex physical interactions, offering a scalable path toward the autonomous evolution of humanoid robots in unstructured environments.

</details>


### [107] [Robot-Assisted Social Dining as a White Glove Service](https://arxiv.org/abs/2602.15767)
*Atharva S Kashyap,Ugne Aleksandra Morkute,Patricia Alves-Oliveira*

Main category: cs.RO

TL;DR: 该论文研究了机器人辅助进食在餐馆等“野外”社交用餐场景中的设计需求，提出了更具社会适应性的设计理念。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人进食辅助系统多在实验室或家庭场景测试，未充分考虑动态、无人监管的真实社交就餐环境。对于需要进食辅助的残障人士来说，在外用餐的独立性和尊严亟需提升。

Method: 通过与残障人士参与的设想性参与式设计，采用半结构化访谈和自定义AI视觉故事板工具，收集并分析他们对在野外社交用餐情境下机器人辅助进食的理想需求和场景。

Result: 发现实现“白手套服务”原则是理想状态：即机器人应支持多模态输入、低干扰输出；具备情境敏感的社交行为、以用户为中心；功能拓展到除进食外的其它角色；并能及时适应餐桌上的其他人际关系。

Conclusion: 本文提出的设计原则为机器人辅助进食在实际、尤其多人的社交和公开场所应用提供指导，对提升残障人士用餐体验、促进无障碍社交环境建设有重要意义。

Abstract: Robot-assisted feeding enables people with disabilities who require assistance eating to enjoy a meal independently and with dignity. However, existing systems have only been tested in-lab or in-home, leaving in-the-wild social dining contexts (e.g., restaurants) largely unexplored. Designing a robot for such contexts presents unique challenges, such as dynamic and unsupervised dining environments that a robot needs to account for and respond to. Through speculative participatory design with people with disabilities, supported by semi-structured interviews and a custom AI-based visual storyboarding tool, we uncovered ideal scenarios for in-the-wild social dining. Our key insight suggests that such systems should: embody the principles of a white glove service where the robot (1) supports multimodal inputs and unobtrusive outputs; (2) has contextually sensitive social behavior and prioritizes the user; (3) has expanded roles beyond feeding; (4) adapts to other relationships at the dining table. Our work has implications for in-the-wild and group contexts of robot-assisted feeding.

</details>


### [108] [FAST-EQA: Efficient Embodied Question Answering with Global and Local Region Relevancy](https://arxiv.org/abs/2602.15813)
*Haochen Zhang,Nirav Savaliya,Faizan Siddiqui,Enna Sachdeva*

Main category: cs.RO

TL;DR: 本文提出了一种新的EQA系统FAST-EQA，结合快速推理、紧凑记忆与全局探索策略，实现了更高效与高精度的视觉问答。


<details>
  <summary>Details</summary>
Motivation: 现有EQA方法面临搜索空间大、储存与处理观测信息负担重以及推理速度慢等问题，难以满足真实部署需求。

Method: FAST-EQA通过：1）基于问题识别潜在视觉目标；2）全局评分导航兴趣区；3）基于视觉记忆进行链式推理；4）限制记忆容量，动态维护区域-目标假设集合；5）将门等狭小入口视为高价值探索前沿区，结合全局与局部探索。

Result: 在HMEQA、EXPRESS-Bench上达到了SOTA表现，在OpenEQA及MT-HM3D上表现竞争力；相较以往方法推理效率更高。

Conclusion: FAST-EQA高效聚焦于问题相关区域，提高了场景覆盖率与问答准确性，在实际应用中有更高部署价值。

Abstract: Embodied Question Answering (EQA) combines visual scene understanding, goal-directed exploration, spatial and temporal reasoning under partial observability. A central challenge is to confine physical search to question-relevant subspaces while maintaining a compact, actionable memory of observations. Furthermore, for real-world deployment, fast inference time during exploration is crucial. We introduce FAST-EQA, a question-conditioned framework that (i) identifies likely visual targets, (ii) scores global regions of interest to guide navigation, and (iii) employs Chain-of-Thought (CoT) reasoning over visual memory to answer confidently. FAST-EQA maintains a bounded scene memory that stores a fixed-capacity set of region-target hypotheses and updates them online, enabling robust handling of both single and multi-target questions without unbounded growth. To expand coverage efficiently, a global exploration policy treats narrow openings and doors as high-value frontiers, complementing local target seeking with minimal computation. Together, these components focus the agent's attention, improve scene coverage, and improve answer reliability while running substantially faster than prior approaches. On HMEQA and EXPRESS-Bench, FAST-EQA achieves state-of-the-art performance, while performing competitively on OpenEQA and MT-HM3D.

</details>


### [109] [Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching](https://arxiv.org/abs/2602.15827)
*Zhen Wu,Xiaoyu Huang,Lujie Yang,Yuanhang Zhang,Koushil Sreenath,Xi Chen,Pieter Abbeel,Rocky Duan,Angjoo Kanazawa,Carmelo Sferrazza,Guanya Shi,C. Karen Liu*

Main category: cs.RO

TL;DR: 本文介绍了一种模块化框架，使类人机器人能够自主地通过视觉进行高难度障碍跑酷，实现了灵活的人类动作组合和环境感知决策，并在真实机器人上展示了极具动态性的技能。


<details>
  <summary>Details</summary>
Motivation: 尽管类人机器人已能在复杂地形上稳定行走，但模仿人类高度动态、具有表现力的动作（如跑酷）仍是难题，尤其是在需要技能组合和自主感知的复杂环境下。本文旨在实现具备敏捷性、表达性和感知驱动决策能力的类人机器人障碍跑酷。

Method: 提出了Perceptive Humanoid Parkour (PHP) 框架，首先利用特征空间中的最近邻搜索，将细粒度的人类技能动作组装为长时域的运动轨迹；然后使用RL和DAgger训练和蒸馏基于深度视觉的多技能策略，实现自适应、闭环决策，让机器人根据深度感知和速度指令自主选择并执行不同通过障碍的技能。

Result: 在Unitree G1类人机器人上进行了大量现实世界实验，验证了框架能实现高动态的跑酷技能（如攀爬高达机器人自身96%的障碍）以及多障碍跨越与对环境扰动的实时适应性。

Conclusion: PHP框架有效支持了类人机器人以人类般的优雅和灵活性自主完成复杂障碍跑酷，集成了技能组合、感知决策和自适应能力，为实现更具人类表现力的机器人运动提供了可行路径。

Abstract: While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.

</details>


### [110] [Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation](https://arxiv.org/abs/2602.15828)
*Yuxuan Kuang,Sungjae Park,Katerina Fragkiadaki,Shubham Tulsiani*

Main category: cs.RO

TL;DR: Dex4D提出了一种无需任务特定训练且可零样本迁移到现实世界的灵巧操作通用策略学习框架。它通过点追踪方法实现多对象、多姿态的泛化操控，实验证明其在模拟和真实场景均表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前灵巧操作任务需要大量现实数据但采集成本高，或需设计繁琐的任务特定环境与奖励，缺乏高效通用的策略学习方法。作者希望解决通用操控策略学习和迁移难题。

Method: 提出Dex4D框架，在模拟环境中训练一个基于3D点追踪输入、能处理任意物体/目标姿态的策略。策略学会“任意初始-任意目标”操控，通过多对象多姿态大规模训练实现泛化；部署时仅需视频生成的目标点追踪即可无需微调直接应用于现实任务。系统还支持实时点追踪实现闭环感知与控制。

Result: 实验显示Dex4D在模拟和真实机器人上的多样灵巧任务中，无需微调即可显著优于现有基线方法。其泛化能力强，能适应新物体、场景、背景和目标轨迹。

Conclusion: Dex4D为复杂灵巧操作提供了一个高效、可扩展且泛化能力强的通用策略学习与迁移框架，有望推动多样现实操控任务的自动化和智能化。

Abstract: Learning generalist policies capable of accomplishing a plethora of everyday tasks remains an open challenge in dexterous manipulation. In particular, collecting large-scale manipulation data via real-world teleoperation is expensive and difficult to scale. While learning in simulation provides a feasible alternative, designing multiple task-specific environments and rewards for training is similarly challenging. We propose Dex4D, a framework that instead leverages simulation for learning task-agnostic dexterous skills that can be flexibly recomposed to perform diverse real-world manipulation tasks. Specifically, Dex4D learns a domain-agnostic 3D point track conditioned policy capable of manipulating any object to any desired pose. We train this 'Anypose-to-Anypose' policy in simulation across thousands of objects with diverse pose configurations, covering a broad space of robot-object interactions that can be composed at test time. At deployment, this policy can be zero-shot transferred to real-world tasks without finetuning, simply by prompting it with desired object-centric point tracks extracted from generated videos. During execution, Dex4D uses online point tracking for closed-loop perception and control. Extensive experiments in simulation and on real robots show that our method enables zero-shot deployment for diverse dexterous manipulation tasks and yields consistent improvements over prior baselines. Furthermore, we demonstrate strong generalization to novel objects, scene layouts, backgrounds, and trajectories, highlighting the robustness and scalability of the proposed framework.

</details>
