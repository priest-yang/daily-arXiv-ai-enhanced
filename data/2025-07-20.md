<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]
- [cs.CL](#cs.CL) [Total: 36]
- [cs.RO](#cs.RO) [Total: 26]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering](https://arxiv.org/abs/2507.12490)
*Maximiliano Hormazábal Lagos,Héctor Cerezo-Costas,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: EaGERS提出了一种无需训练、适用于不同模型的流程，用于改进视觉问答中的解释性和准确性。其在DocVQA数据集上的表现优于原始模型，无需微调。


<details>
  <summary>Details</summary>
Motivation: 改进视觉问答(VQA)任务中答案的可解释性和准确性，尤其是在文档VQA(DocVQA)领域，在不增加训练成本的前提下提升透明度和复现性。

Method: 提出EaGERS流程：1）利用视觉-语言模型生成自然语言推理；2）通过多模态嵌入相似性和多数投票，将推理定位到图像空间的具体子区域；3）只从选定的相关区域生成最终回答。整个流程无需模型训练或微调，对模型本身无依赖。

Result: 在DocVQA数据集测试中，EaGERS的最佳配置在准确匹配率和平均归一化Levenshtein相似度等指标上均优于原始基础模型，同时提升了解决方案的透明性和可复现性。

Conclusion: EaGERS能够显著提升文档视觉问答任务的性能和可解释性，而且无需对现有模型进行额外训练，具有较好应用前景。

Abstract: We introduce EaGERS, a fully training-free and model-agnostic pipeline that
(1) generates natural language rationales via a vision language model, (2)
grounds these rationales to spatial sub-regions by computing multimodal
embedding similarities over a configurable grid with majority voting, and (3)
restricts the generation of responses only from the relevant regions selected
in the masked image. Experiments on the DocVQA dataset demonstrate that our
best configuration not only outperforms the base model on exact match accuracy
and Average Normalized Levenshtein Similarity metrics but also enhances
transparency and reproducibility in DocVQA without additional model
fine-tuning.

</details>


### [2] [MindJourney: Test-Time Scaling with World Models for Spatial Reasoning](https://arxiv.org/abs/2507.12508)
*Yuncong Yang,Jiageng Liu,Zheyuan Zhang,Siyuan Zhou,Reuben Tan,Jianwei Yang,Yilun Du,Chuang Gan*

Main category: cs.CV

TL;DR: 本文提出了MindJourney框架，通过结合世界模型赋予主流视觉-语言模型（VLMs）三维空间推理能力，显著提升其在3D空间任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型擅长处理二维图像，但缺乏对场景三维动态的内部建模，导致其在空间推理和面对自我运动时的场景预测等任务上的表现明显不足。因此，迫切需要一种无需复杂训练即可增强VLM三维推理能力的方法。

Method: 作者提出了MindJourney，一种测试时增强框架：通过将VLM与基于视频扩散的可控世界模型结合，使其能够模拟真实的摄像机移动和视角切换。VLM负责制定简明的摄像机轨迹，世界模型则按轨迹合成对应视图，VLM根据多视角信息进行更精准的空间推理，全流程无需对VLM进行微调。

Result: 在SAT等空间推理基准测试上，MindJourney相比原生VLM平均提升超过8%的表现，同时也超过了在测试时采用强化学习训练的VLM推理能力。

Conclusion: 将VLM与世界模型在测试时进行组合是一种简单、可扩展、即插即用的三维推理能力增强方案，有望推动VLM在空间推理等三维认知任务中的广泛应用。

Abstract: Spatial reasoning in 3D space is central to human cognition and indispensable
for embodied tasks such as navigation and manipulation. However,
state-of-the-art vision-language models (VLMs) struggle frequently with tasks
as simple as anticipating how a scene will look after an egocentric motion:
they perceive 2D images but lack an internal model of 3D dynamics. We therefore
propose MindJourney, a test-time scaling framework that grants a VLM with this
missing capability by coupling it to a controllable world model based on video
diffusion. The VLM iteratively sketches a concise camera trajectory, while the
world model synthesizes the corresponding view at each step. The VLM then
reasons over this multi-view evidence gathered during the interactive
exploration. Without any fine-tuning, our MindJourney achieves over an average
8% performance boost on the representative spatial reasoning benchmark SAT,
showing that pairing VLMs with world models for test-time scaling offers a
simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also
improves upon the test-time inference VLMs trained through reinforcement
learning, which demonstrates the potential of our method that utilizes world
models for test-time scaling.

</details>


### [3] [Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models](https://arxiv.org/abs/2507.12566)
*Gen Luo,Wenhan Dou,Wenhao Li,Zhaokai Wang,Xue Yang,Changyao Tian,Hao Li,Weiyun Wang,Wenhai Wang,Xizhou Zhu,Yu Qiao,Jifeng Dai*

Main category: cs.CV

TL;DR: 本文提出了一种新型的单体多模态大模型（MLLM）Mono-InternVL及其升级版本Mono-InternVL-1.5，通过视觉参数空间嵌入和创新的视觉预训练方法，有效提升了模型的视觉理解能力，并显著降低了训练和推理成本，在多个基准测试中取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有单体MLLM在视觉与语言联合训练时容易出现优化不稳定和灾难性遗忘，难以高效稳定地融合视觉知识，因此亟需一种新结构和预训练方法来解决这些问题。

Method: 提出将新视觉参数空间以delta tuning方式嵌入到预训练LLM，并采用多模态MoE（专家混合）架构。设计了递进式视觉预训练流程（EViP）以充分挖掘视觉能力；升级版Mono-InternVL-1.5采用EViP++，引入额外的视觉注意专家并通过高效数据组织和CUDA融合操作提升推理速度与经济性。

Result: Mono-InternVL在15项基准测试中有12项优于同类模型，其中在OCRBench上超越Emu3 114分。升级版Mono-InternVL-1.5在保证同等多模态表现下，将首token延迟降低高达69%，大幅减少训练及推理成本。

Conclusion: 本文提出的Mono-InternVL系列方法，在多模态理解能力和运行效率方面均超越当前主流单体MLLM，并为相关领域模型的训练与部署提供了高效新路径。

Abstract: This paper focuses on monolithic Multimodal Large Language Models (MLLMs),
which integrate visual encoding and language decoding into a single model.
Existing structures and pre-training strategies for monolithic MLLMs often
suffer from unstable optimization and catastrophic forgetting. To address these
challenges, our key idea is to embed a new visual parameter space into a
pre-trained LLM, enabling stable learning of visual knowledge from noisy data
via delta tuning. Based on this principle, we first introduce Mono-InternVL, an
advanced monolithic MLLM that incorporates a set of visual experts through a
multimodal mixture-of-experts architecture. In addition, we design an
innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize
its visual capabilities via progressive learning. Mono-InternVL achieves
competitive performance against existing MLLMs but also leads to relatively
expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper
and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++
introduces additional visual attention experts to Mono-InternVL-1.5 and
re-organizes the pre-training process in an efficient manner. During inference,
it includes a fused CUDA kernel to speed up its MoE operations. With these
designs, Mono-InternVL-1.5 significantly reduces training and inference costs,
while still maintaining competitive performance with Mono-InternVL. To evaluate
our approach, we conduct extensive experiments across 15 benchmarks. Results
demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out
of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared
to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves
similar multimodal performance while reducing first-token latency by up to 69%.
Code and models are released at https://github.com/OpenGVLab/Mono-InternVL.

</details>


### [4] [Best Practices for Large-Scale, Pixel-Wise Crop Mapping and Transfer Learning Workflows](https://arxiv.org/abs/2507.12590)
*Judy Long,Tao Liu,Sean Alexander Woznicki,Miljana Marković,Oskar Marko,Molly Sears*

Main category: cs.CV

TL;DR: 本文对大规模像元级作物类型识别与分类流程进行了全面综述和系统实验，涵盖传统监督学习与迁移学习新方法，最终给出最优工作流建议。


<details>
  <summary>Details</summary>
Motivation: 随着遥感影像数据的丰富与精细化，作物类型精确识别需求提升。但实际应用中面对不同地区及数据分布存在的域差异，如何选择和优化像元级作物图谱绘制工作流尚无定论。

Method: 系统评估6种主流影像预处理方法与11种监督分类模型，控制训练样本量与变量组合，对比传统监督和多种迁移学习技术在5个农作区的表现。主要遥感数据源为Landsat 8，标签来自权威作物数据和实地调查。

Result: 1）精细区间型预处理+Transformer模型在监督及迁移场景下均表现最佳；RF模型在同域和直接迁移时训练快且表现优异。2）迁移学习提升了适应性，域自适应(UDA)适合作物同质类场景，微调则在多种情境下更稳健。3）充足标注样本时，监督学习更优；样本少时，匹配域差的迁移学习为可选方案。

Conclusion: 推荐针对任务和样本情况灵活选择工作流，大样本优先监督训练，样本不足时根据域差选用适当迁移学习方式，可实现高效大规模作物制图。

Abstract: Crop mapping involves identifying and classifying crop types using spatial
data, primarily derived from remote sensing imagery. This study presents the
first comprehensive review of large-scale, pixel-wise crop mapping workflows,
encompassing both conventional supervised methods and emerging transfer
learning approaches. To identify the optimal supervised crop mapping workflows,
we conducted systematic experiments, comparing six widely adopted satellite
image-based preprocessing methods, alongside eleven supervised pixel-wise
classification models. Additionally, we assessed the synergistic impact of
varied training sample sizes and variable combinations. Moreover, we identified
optimal transfer learning techniques for different magnitudes of domain shift.
The evaluation of best methods was conducted across five diverse agricultural
sites. Landsat 8 served as the primary satellite data source. Labels come from
CDL trusted pixels and field surveys.
  Our findings reveal three key insights. First, fine-scale interval
preprocessing paired with Transformer models consistently delivered optimal
performance for both supervised and transferable workflows. RF offered rapid
training and competitive performance in conventional supervised learning and
direct transfer to similar domains. Second, transfer learning techniques
enhanced workflow adaptability, with UDA being effective for homogeneous crop
classes while fine-tuning remains robust across diverse scenarios. Finally,
workflow choice depends heavily on the availability of labeled samples. With a
sufficient sample size, supervised training typically delivers more accurate
and generalizable results. Below a certain threshold, transfer learning that
matches the level of domain shift is a viable alternative to achieve crop
mapping. Repository:
Best-Practices-for-Large-Scale-Pixel-Wise-Crop-Mapping-and-Transfer-Learning-Workflows

</details>


### [5] [CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling](https://arxiv.org/abs/2507.12591)
*Trong-Thang Pham,Akash Awasthi,Saba Khan,Esteban Duran Marti,Tien-Phat Nguyen,Khoa Vo,Minh Tran,Ngoc Son Nguyen,Cuong Tran Van,Yuki Ikebe,Anh Totti Nguyen,Anh Nguyen,Zhigang Deng,Carol C. Wu,Hien Van Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 本论文发布了首个公开的CT影像眼动追踪数据集，并提出了可预测三维视线路径的新模型CT-Searcher，推动了医学图像可解释计算机辅助诊断的发展。


<details>
  <summary>Details</summary>
Motivation: 现有CT阅读的眼动研究受限于缺乏公开三维眼动追踪数据集及CT体积数据的复杂性，影响了辅助诊断系统的可解释性和智能化发展。

Method: 1）发布了首个公开可用的CT影像眼动数据集CT-ScanGaze；2）提出CT-Searcher模型，能输入三维CT体积数据并预测类似放射科医生的三维视线序列；3）设计了将已有2D眼动数据集转化为3D视线数据的预训练流程，用于提升CT-Searcher性能。

Result: 经过定性和定量实验，CT-Searcher能更有效地预测三维CT阅读过程中的放射科医生视线路径；论文还建立了三维扫描路径预测的评估框架。

Conclusion: 本研究丰富了医学影像AI领域的公开资源，并提出创新模型提升3D视线预测效果，为医学影像可解释AI和辅助诊断系统的研究提供了关键支持。

Abstract: Understanding radiologists' eye movement during Computed Tomography (CT)
reading is crucial for developing effective interpretable computer-aided
diagnosis systems. However, CT research in this area has been limited by the
lack of publicly available eye-tracking datasets and the three-dimensional
complexity of CT volumes. To address these challenges, we present the first
publicly available eye gaze dataset on CT, called CT-ScanGaze. Then, we
introduce CT-Searcher, a novel 3D scanpath predictor designed specifically to
process CT volumes and generate radiologist-like 3D fixation sequences,
overcoming the limitations of current scanpath predictors that only handle 2D
inputs. Since deep learning models benefit from a pretraining step, we develop
a pipeline that converts existing 2D gaze datasets into 3D gaze data to
pretrain CT-Searcher. Through both qualitative and quantitative evaluations on
CT-ScanGaze, we demonstrate the effectiveness of our approach and provide a
comprehensive assessment framework for 3D scanpath prediction in medical
imaging.

</details>


### [6] [MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification](https://arxiv.org/abs/2507.12602)
*Said Ohamouddou,Abdellatif El Afia,Hanaa El Afia,Raddouane Chiheb*

Main category: cs.CV

TL;DR: 本论文提出了一种名为MS-DGCNN++的新型层次多尺度融合动态图卷积网络，实现了对陆地激光雷达点云中树种的高效分类，并优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于MS-DGCNN的林木点云分类方法在多尺度上采用并行处理，难以捕捉树结构分层之间的语义联系，影响模型对复杂自然结构的理解和分类精度。

Method: 提出了MS-DGCNN++，通过对点云数据在局部、分枝和冠层尺度分别进行特征工程，并实现不同尺度的语义特征融合，利用语义一致的多尺度处理替代传统的均匀并行处理。

Result: 在STPCTLS数据集上MS-DGCNN++取得94.96%的准确率，优于DGCNN、MS-DGCNN及PPT。在FOR-species20K数据集上准确率为67.25%，提升6.1%。同时在ModelNet40和ModelNet10上也优于DGCNN与MS-DGCNN。该方法参数量和复杂度更低，适用于资源有限场景。

Conclusion: MS-DGCNN++有效利用分层多尺度语义信息，提升了树种分类和3D物体识别的准确性和泛化能力，适用于多种点云应用，且资源消耗低。

Abstract: Tree species classification from terrestrial LiDAR point clouds is
challenging because of the complex multi-scale geometric structures in forest
environments. Existing approaches using multi-scale dynamic graph convolutional
neural networks (MS-DGCNN) employ parallel multi-scale processing, which fails
to capture the semantic relationships between the hierarchical levels of the
tree architecture. We present MS-DGCNN++, a hierarchical multiscale fusion
dynamic graph convolutional network that uses semantically meaningful feature
extraction at local, branch, and canopy scales with cross-scale information
propagation. Our method employs scale-specific feature engineering, including
standard geometric features for the local scale, normalized relative vectors
for the branch scale, and distance information for the canopy scale. This
hierarchical approach replaces uniform parallel processing with semantically
differentiated representations that are aligned with the natural tree
structure. Under the same proposed tree species data augmentation strategy for
all experiments, MS-DGCNN++ achieved an accuracy of 94.96 \% on STPCTLS,
outperforming DGCNN, MS-DGCNN, and the state-of-the-art model PPT. On
FOR-species20K, it achieves 67.25\% accuracy (6.1\% improvement compared to
MS-DGCNN). For standard 3D object recognition, our method outperformed DGCNN
and MS-DGCNN with overall accuracies of 93.15\% on ModelNet40 and 94.05\% on
ModelNet10. With lower parameters and reduced complexity compared to
state-of-the-art transformer approaches, our method is suitable for
resource-constrained applications while maintaining a competitive accuracy.
Beyond tree classification, the method generalizes to standard 3D object
recognition, establishing it as a versatile solution for diverse point cloud
processing applications. The implementation code is publicly available at
https://github.com/said-ohamouddou/MS-DGCNN2.

</details>


### [7] [Predicting Soccer Penalty Kick Direction Using Human Action Recognition](https://arxiv.org/abs/2507.12617)
*David Freire-Obregón,Oliverio J. Santana,Javier Lorenzo-Navarro,Daniel Hernández-Sosa,Modesto Castrillón-Santana*

Main category: cs.CV

TL;DR: 本文发布了一个足球点球动作预测的新数据集，并用深度学习模型进行了基准测试，预测点球方向取得了超越真实守门员的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的动作预测数据集与实际体育场景存在脱节，尤其缺乏精细标注的点球数据，限制了技术在实际中的应用。作者动机是填补这类数据集的空白并推动体育领域的动作预判研究。

Method: 作者手动标注了足球点球的动作数据，构建了面向点球方向预测的数据集。提出了融合动作识别特征和上下文元数据的深度学习分类器，将其应用于7类共22个主干模型（MViTv2、MViTv1、SlowFast等）进行实验对比。

Result: 在点球方向预测任务上，最佳模型预测准确率达到63.9%，超过了实际守门员的判断能力。

Conclusion: 该新数据集对预测性动作识别任务极具价值，提出的深度学习方法可为体育动作预测提供可泛化解决方案。

Abstract: Action anticipation has become a prominent topic in Human Action Recognition
(HAR). However, its application to real-world sports scenarios remains limited
by the availability of suitable annotated datasets. This work presents a novel
dataset of manually annotated soccer penalty kicks to predict shot direction
based on pre-kick player movements. We propose a deep learning classifier to
benchmark this dataset that integrates HAR-based feature embeddings with
contextual metadata. We evaluate twenty-two backbone models across seven
architecture families (MViTv2, MViTv1, SlowFast, Slow, X3D, I3D, C2D),
achieving up to 63.9% accuracy in predicting shot direction (left or right),
outperforming the real goalkeepers' decisions. These results demonstrate the
dataset's value for anticipatory action recognition and validate our model's
potential as a generalizable approach for sports-based predictive tasks.

</details>


### [8] [Funnel-HOI: Top-Down Perception for Zero-Shot HOI Detection](https://arxiv.org/abs/2507.12628)
*Sandipan Sarma,Agney Talwarr,Arijit Sur*

Main category: cs.CV

TL;DR: 本论文提出了Funnel-HOI框架，用于提升人-物交互检测（HOID），特别是在零样本和长尾分布情形下，方法通过引入新的编码器结构和损失函数，在主流数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 人-物交互检测任务面临数据标注稀缺和长尾分布问题，尤其是稀有或未见过的人物-动作-物体组合。现有方法大多仅关注解码器端改进，对于如何在编码器阶段增强HOI识别缺乏研究。

Method: 提出Funnel-HOI自顶向下建模框架，先在图像中检测具体物体，再逐步关联抽象动作，借鉴人类的理解习惯。设计了非对称共注意力机制，高效利用多模态信息，实现更强的交互特征编码；同时提出新型损失函数，更好地考虑物体-动作相关性和分类惩罚，提升交互识别性能，支持零样本学习。

Result: 在HICO-DET和V-COCO数据集的全监督及6种零样本设置下，Funnel-HOI均取得最优表现，尤其在未见和稀有类别上分别提升12.4%和8.4%。

Conclusion: 通过在编码器阶段挖掘和融合HOI相关信息，并结合创新的损失函数，Funnel-HOI有效缓解了长尾和零样本难题，为人-物交互检测提供了更鲁棒且通用的解决方案。

Abstract: Human-object interaction detection (HOID) refers to localizing interactive
human-object pairs in images and identifying the interactions. Since there
could be an exponential number of object-action combinations, labeled data is
limited - leading to a long-tail distribution problem. Recently, zero-shot
learning emerged as a solution, with end-to-end transformer-based object
detectors adapted for HOID becoming successful frameworks. However, their
primary focus is designing improved decoders for learning entangled or
disentangled interpretations of interactions. We advocate that HOI-specific
cues must be anticipated at the encoder stage itself to obtain a stronger scene
interpretation. Consequently, we build a top-down framework named Funnel-HOI
inspired by the human tendency to grasp well-defined concepts first and then
associate them with abstract concepts during scene understanding. We first
probe an image for the presence of objects (well-defined concepts) and then
probe for actions (abstract concepts) associated with them. A novel asymmetric
co-attention mechanism mines these cues utilizing multimodal information
(incorporating zero-shot capabilities) and yields stronger interaction
representations at the encoder level. Furthermore, a novel loss is devised that
considers objectaction relatedness and regulates misclassification penalty
better than existing loss functions for guiding the interaction classifier.
Extensive experiments on the HICO-DET and V-COCO datasets across
fully-supervised and six zero-shot settings reveal our state-of-the-art
performance, with up to 12.4% and 8.4% gains for unseen and rare HOI
categories, respectively.

</details>


### [9] [Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos](https://arxiv.org/abs/2507.12646)
*Kaihua Chen,Tarasha Khurana,Deva Ramanan*

Main category: cs.CV

TL;DR: 本文提出了一种用于动态场景单目视频的新颖视角合成方法CogNVS，结合了3D重建与2D视频扩散模型，有效提升了动态场景新视角的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有关于单目视频动态场景的新颖视角合成方法，往往需要高昂的测试时优化，或者在端到端训练下无法保持场景几何一致性，因此需要探索既高效又能保留几何信息的方法。

Method: 作者提出了三点创新：1）对于新旧视角下可见的像素，先进行动态3D重建，再从新视角渲染；2）对于新视角下不可见区域，采用基于2D视频的扩散模型（CogNVS）进行修复；3）该扩散模型可通过大量无标注视频进行自监督训练，并支持零样本下的测试时微调。

Result: 实验证明，CogNVS在动态场景单目新视角合成任务上，相较以往方法在大多数指标上有更优表现。

Conclusion: CogNVS不仅提升了重建质量，同时兼顾了效率与应用广泛性，为动态场景新视角合成提供了新路线。

Abstract: We explore novel-view synthesis for dynamic scenes from monocular videos.
Prior approaches rely on costly test-time optimization of 4D representations or
do not preserve scene geometry when trained in a feed-forward manner. Our
approach is based on three key insights: (1) covisible pixels (that are visible
in both the input and target views) can be rendered by first reconstructing the
dynamic 3D scene and rendering the reconstruction from the novel-views and (2)
hidden pixels in novel views can be "inpainted" with feed-forward 2D video
diffusion models. Notably, our video inpainting diffusion model (CogNVS) can be
self-supervised from 2D videos, allowing us to train it on a large corpus of
in-the-wild videos. This in turn allows for (3) CogNVS to be applied zero-shot
to novel test videos via test-time finetuning. We empirically verify that
CogNVS outperforms almost all prior art for novel-view synthesis of dynamic
scenes from monocular videos.

</details>


### [10] [Integrated Oculomics and Lipidomics Reveal Microvascular Metabolic Signatures Associated with Cardiovascular Health in a Healthy Cohort](https://arxiv.org/abs/2507.12663)
*Inamullah,Ernesto Elias Vidal Rosas,Imran Razzak,Shoaib Jameel*

Main category: cs.CV

TL;DR: 本研究通过结合视网膜微血管特征和血清脂质组数据，发现了与心血管疾病早期风险相关的无症状生物标志物，提出了新的、多组学整合的无创风险评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有心血管风险分层方法难以检测早期亚临床改变。以往研究很少将视网膜微血管特征和系统性血清脂质组联合分析，因此缺乏敏感且无创的CVD早期预警指标。

Method: 研究采用创新的成像组学框架，利用深度学习对视网膜影像进行自动化分析，量化微血管表型，并结合UHPLC ESI HRMS进行血清脂质组学检测。在健康人群中做大规模、协变量校正和分层的相关性分析。

Result: 发现视网膜微血管结构，如平均动脉宽度、血管密度与甘三酯、二酰基甘油、神经酰胺等脂质亚类之间存在强的、与年龄性别无关的相关性，提示代谢应激下的微血管重构机制。

Conclusion: 该研究首次系统联结了血管结构表型和特定脂质分子，为心血管疾病早期发病机制提供了新认识，推动无创、生物标志物的发现，有助于心血管疾病的早期检测、精准干预和个体化预防。

Abstract: Cardiovascular disease (CVD) remains the leading global cause of mortality,
yet current risk stratification methods often fail to detect early, subclinical
changes. Previous studies have generally not integrated retinal
microvasculature characteristics with comprehensive serum lipidomic profiles as
potential indicators of CVD risk. In this study, an innovative imaging omics
framework was introduced, combining retinal microvascular traits derived
through deep learning based image processing with serum lipidomic data to
highlight asymptomatic biomarkers of cardiovascular risk beyond the
conventional lipid panel. This represents the first large scale, covariate
adjusted and stratified correlation analysis conducted in a healthy population,
which is essential for identifying early indicators of disease. Retinal
phenotypes were quantified using automated image analysis tools, while serum
lipid profiling was performed by Ultra High Performance Liquid Chromatography
Electrospray ionization High resolution mass spectrometry (UHPLC ESI HRMS).
Strong, age- and sex-independent correlations were established, particularly
between average artery width, vessel density, and lipid subclasses such as
triacylglycerols (TAGs), diacylglycerols (DAGs), and ceramides (Cers). These
associations suggest a converging mechanism of microvascular remodeling under
metabolic stress. By linking detailed
  vascular structural phenotypes to specific lipid species, this study fills a
critical gap in the understanding of early CVD pathogenesis. This integration
not only offers a novel perspective on microvascular metabolic associations but
also presents a significant opportunity for the identification of robust,
non-invasive biomarkers. Ultimately, these findings may support improved early
detection, targeted prevention, and personalized approaches in cardiovascular
healthcare.

</details>


### [11] [FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks](https://arxiv.org/abs/2507.12675)
*Christina Thrainer,Md Meftahul Ferdaus,Mahdi Abdelguerfi,Christian Guetl,Steven Sloan,Kendall N. Niles,Ken Pathak*

Main category: cs.CV

TL;DR: 本文提出了一種新型深度學習架構 FORTRESS，專為土木基礎設施中的結構性缺陷自動分割設計，在大幅提高速度和減少運算量的同時，保證高精度。


<details>
  <summary>Details</summary>
Motivation: 在土木基礎設施自動缺陷檢測中，如何同時實現高準確率與高效能，滿足實時應用場景需求是一大挑戰。現有方法如 U-Net 等雖有一定準確率，但參數量大、推理慢，影響部署。

Method: FORTRESS 採用深度可分離卷積結合自適應 Kolmogorov-Arnold 網路整合（TiKAN），具體包括：1）系統性的深度可分離卷積框架顯著減少參數；2）自適應選擇在有益時應用函數組合轉換的 TiKAN；3）多尺度注意力融合空間、通道和 KAN 增強特徵。

Result: 在保持優異分割表現的同時，推理參數量減少 91%（31M→2.9M），計算複雜度減少 91%（13.7→1.17 GFLOPs），推理速度提升 3 倍。在基準數據集上取得 F1 分數 0.771、mIoU 0.677，顯著優於 U-Net、SA-UNet、U-KAN 等現有方法。

Conclusion: FORTRESS 在推理效率和分割表現間取得平衡，是資源受限環境下結構缺陷分割的魯棒、高效解決方案。

Abstract: Automated structural defect segmentation in civil infrastructure faces a
critical challenge: achieving high accuracy while maintaining computational
efficiency for real-time deployment. This paper presents FORTRESS
(Function-composition Optimized Real-Time Resilient Structural Segmentation), a
new architecture that balances accuracy and speed by using a special method
that combines depthwise separable convolutions with adaptive Kolmogorov-Arnold
Network integration. FORTRESS incorporates three key innovations: a systematic
depthwise separable convolution framework achieving a 3.6x parameter reduction
per layer, adaptive TiKAN integration that selectively applies function
composition transformations only when computationally beneficial, and
multi-scale attention fusion combining spatial, channel, and KAN-enhanced
features across decoder levels. The architecture achieves remarkable efficiency
gains with 91% parameter reduction (31M to 2.9M), 91% computational complexity
reduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while
delivering superior segmentation performance. Evaluation on benchmark
infrastructure datasets demonstrates state-of-the-art results with an F1- score
of 0.771 and a mean IoU of 0.677, significantly outperforming existing methods
including U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves
essential for optimal performance, establishing FORTRESS as a robust solution
for practical structural defect segmentation in resource-constrained
environments where both accuracy and computational efficiency are paramount.
Comprehensive architectural specifications are provided in the Supplemental
Material. Source code is available at URL:
https://github.com/faeyelab/fortress-paper-code.

</details>


### [12] [NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement](https://arxiv.org/abs/2507.12714)
*Yang Yang,Dongni Mao,Hiroaki Santo,Yasuyuki Matsushita,Fumio Okura*

Main category: cs.CV

TL;DR: 本文提出了一种用于3D叶片建模与重建的神经参数化模型NeuraLeaf，能有效生成多样化、可变形的叶片模型，并提升3D数据拟合精度。


<details>
  <summary>Details</summary>
Motivation: 现有的神经参数化模型主要针对人类和动物，而植物叶片因其形态多样和灵活变形带来额外建模挑战。农业和计算机图形学等领域迫切需要高效的3D叶片建模与重建方法。

Method: NeuraLeaf将叶片的几何形状拆解为2D基础形状与3D变形两部分，在基础形状学习中充分利用丰富的2D叶片图像库，同时结合纹理与几何对齐进行学习。为建模3D变形，作者提出了一种新颖的无骨架蒙皮模型，并自主采集了DeformLeaf 3D叶片数据集。

Result: NeuraLeaf模型能够生成大量不同形态及变形的叶片，可精准拟合3D观测数据（如深度图、点云），表现优异。

Conclusion: NeuraLeaf为3D叶片建模提供了高效、灵活的新方法，不仅提升了农业与图形学领域的3D重建能力，还为后续相关研究奠定了基础。代码与数据集已开源。

Abstract: We develop a neural parametric model for 3D leaves for plant modeling and
reconstruction that are essential for agriculture and computer graphics. While
neural parametric models are actively studied for humans and animals, plant
leaves present unique challenges due to their diverse shapes and flexible
deformation. To this problem, we introduce a neural parametric model for
leaves, NeuraLeaf. Capitalizing on the fact that flattened leaf shapes can be
approximated as a 2D plane, NeuraLeaf disentangles the leaves' geometry into
their 2D base shapes and 3D deformations. This representation allows learning
from rich sources of 2D leaf image datasets for the base shapes, and also has
the advantage of simultaneously learning textures aligned with the geometry. To
model the 3D deformation, we propose a novel skeleton-free skinning model and
create a newly captured 3D leaf dataset called DeformLeaf. We show that
NeuraLeaf successfully generates a wide range of leaf shapes with deformation,
resulting in accurate model fitting to 3D observations like depth maps and
point clouds. Our implementation and dataset are available at
https://neuraleaf-yang.github.io/.

</details>


### [13] [SOD-YOLO: Enhancing YOLO-Based Detection of Small Objects in UAV Imagery](https://arxiv.org/abs/2507.12727)
*Peijun Wang,Jinhua Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种针对小目标检测的改进YOLOv8模型——SOD-YOLO，通过特征融合增强、增加高分辨率检测层及改进NMS算法，显著提升了小目标检测性能，对无人机图像具有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 当前小目标检测仍然是目标检测领域的难题，尤其是在无人机图像中，由于目标尺寸小、背景复杂，传统检测方法识别准确率不高。

Method: SOD-YOLO改进了YOLOv8基础模型，在neck部分引入了多尺度特征融合的ASF机制，增加了高分辨率的小目标检测层P2，并采用Soft-NMS来优化置信度分数，提升识别真实目标的能力。

Result: 在VisDrone2019-DET数据集上，SOD-YOLO的mAP$_{50:95}$提升了36.1%，mAP$_{50}$提升了20.6%，相较于基线模型表现大幅提升。

Conclusion: SOD-YOLO在小目标检测任务中表现出色，尤其适合无人机图像场景，兼具实用性和高效性。作者已开源代码、超参数和权重，便于应用和进一步研究。

Abstract: Small object detection remains a challenging problem in the field of object
detection. To address this challenge, we propose an enhanced YOLOv8-based
model, SOD-YOLO. This model integrates an ASF mechanism in the neck to enhance
multi-scale feature fusion, adds a Small Object Detection Layer (named P2) to
provide higher-resolution feature maps for better small object detection, and
employs Soft-NMS to refine confidence scores and retain true positives.
Experimental results demonstrate that SOD-YOLO significantly improves detection
performance, achieving a 36.1% increase in mAP$_{50:95}$ and 20.6% increase in
mAP$_{50}$ on the VisDrone2019-DET dataset compared to the baseline model.
These enhancements make SOD-YOLO a practical and efficient solution for small
object detection in UAV imagery. Our source code, hyper-parameters, and model
weights are available at https://github.com/iamwangxiaobai/SOD-YOLO.

</details>


### [14] [A Privacy-Preserving Semantic-Segmentation Method Using Domain-Adaptation Technique](https://arxiv.org/abs/2507.12730)
*Homare Sueyoshi,Kiyoshi Nishikawa,Hitoshi Kiya*

Main category: cs.CV

TL;DR: 提出了一种基于感知加密的隐私保护语义分割方法，可对训练和测试图像同时加密，且基本不损失分割准确率。利用ViT的嵌入结构和领域自适应技术实现。实验验证了该法在Segmentation Transformer模型上效果良好。


<details>
  <summary>Details</summary>
Motivation: 随着AI在图像分析领域的应用，图像数据的隐私保护成为迫切需求，尤其是医疗、安防等敏感领域。公开或共享的训练和测试图像常含有敏感信息，现有加密方法往往影响模型性能。因此作者希望在保护隐私的同时，保持模型的高精度表现。

Method: 该方法采用感知加密对训练和测试图像加密，然后利用ViT嵌入结构，以及领域自适应技术，缓解因加密带来的数据分布变化，从而实现隐私保护与准确率的兼顾。并基于强大的Segmentation Transformer对方法进行了实验验证。

Result: 实验结果表明，该方法在高度加密情况下，语义分割准确率与未加密模型几乎持平，证明了领域自适应和ViT嵌入结构的有效性。

Conclusion: 本文提出的方法可在保护图像隐私的前提下，实现高性能的语义分割，有望在对隐私要求高的实际场景推广应用。

Abstract: We propose a privacy-preserving semantic-segmentation method for applying
perceptual encryption to images used for model training in addition to test
images. This method also provides almost the same accuracy as models without
any encryption. The above performance is achieved using a domain-adaptation
technique on the embedding structure of the Vision Transformer (ViT). The
effectiveness of the proposed method was experimentally confirmed in terms of
the accuracy of semantic segmentation when using a powerful
semantic-segmentation model with ViT called Segmentation Transformer.

</details>


### [15] [Transformer-based Spatial Grounding: A Comprehensive Survey](https://arxiv.org/abs/2507.12739)
*Ijazul Haq,Muhammad Saqib,Yingjie Zhang*

Main category: cs.CV

TL;DR: 本文系统综述了2018至2025年间基于Transformer的空间指代（spatial grounding）研究进展，梳理了主流方法、数据集、评估指标及其行业应用情况。


<details>
  <summary>Details</summary>
Motivation: 近年来Transformer极大促进了多模态表示和跨模态对齐，空间指代任务有显著提升，但缺少对此领域方法、数据、评测和应用的系统性梳理。

Method: 采用系统性文献综述方法，分析和归纳了2018年至2025年间基于Transformer的空间指代最新研究，包括模型架构、常用数据集、评测指标等。

Result: 明确了主流模型架构、常用数据集和评测标准，同时总结了主要的方法学趋势及业界最佳实践。

Conclusion: 本文为研究者和应用开发者提供了空间指代Transformer方法的全景视角和结构性指导，有助于后续可靠、可落地系统的开发。

Abstract: Spatial grounding, the process of associating natural language expressions
with corresponding image regions, has rapidly advanced due to the introduction
of transformer-based models, significantly enhancing multimodal representation
and cross-modal alignment. Despite this progress, the field lacks a
comprehensive synthesis of current methodologies, dataset usage, evaluation
metrics, and industrial applicability. This paper presents a systematic
literature review of transformer-based spatial grounding approaches from 2018
to 2025. Our analysis identifies dominant model architectures, prevalent
datasets, and widely adopted evaluation metrics, alongside highlighting key
methodological trends and best practices. This study provides essential
insights and structured guidance for researchers and practitioners,
facilitating the development of robust, reliable, and industry-ready
transformer-based spatial grounding models.

</details>


### [16] [Domain-Enhanced Dual-Branch Model for Efficient and Interpretable Accident Anticipation](https://arxiv.org/abs/2507.12755)
*Yanchen Guan,Haicheng Liao,Chengyue Wang,Bonan Wang,Jiaxun Zhang,Jia Hu,Zhenning Li*

Main category: cs.CV

TL;DR: 本文提出了一个高效且精确的交通事故预判系统，融合了行车记录仪视频和事故报告文本，通过多模态大模型和特定提示工程，实现了对事故的高效预测和标准化归档。实验验证了方法在多项基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术的发展要求能实现精准且高效的交通事故预判，以便及时干预和防止损失。目前的方法往往在准确性、效率、可解释性等方面存在不足，因此亟需一种能有效融合多种数据源并优化性能的新方法。

Method: 作者设计了一个双分支架构，分别处理摄像头视频（视觉信息）和事故报告（结构化文本），并通过GPT-4o、Long-CLIP等大模型以及针对性的提示工程，实现多模态特征无缝融合。还提出了新的特征聚合方法，提升模型效果。

Result: 提出的方法在DAD、CCD、A3D等基准数据集上进行了全面评估，结果显示其具有更高的预测准确率、更快的响应速度、更低的计算开销，以及更好的可解释性，领先于现有主流方法。

Conclusion: 该多模态交通事故预判系统在精度、响应和解释等多方面创下了新标杆，为自动驾驶安全性提供了有力的技术支持。

Abstract: Developing precise and computationally efficient traffic accident
anticipation system is crucial for contemporary autonomous driving
technologies, enabling timely intervention and loss prevention. In this paper,
we propose an accident anticipation framework employing a dual-branch
architecture that effectively integrates visual information from dashcam videos
with structured textual data derived from accident reports. Furthermore, we
introduce a feature aggregation method that facilitates seamless integration of
multimodal inputs through large models (GPT-4o, Long-CLIP), complemented by
targeted prompt engineering strategies to produce actionable feedback and
standardized accident archives. Comprehensive evaluations conducted on
benchmark datasets (DAD, CCD, and A3D) validate the superior predictive
accuracy, enhanced responsiveness, reduced computational overhead, and improved
interpretability of our approach, thus establishing a new benchmark for
state-of-the-art performance in traffic accident anticipation.

</details>


### [17] [HairShifter: Consistent and High-Fidelity Video Hair Transfer via Anchor-Guided Animation](https://arxiv.org/abs/2507.12758)
*Wangzheng Shi,Yinglin Zheng,Yuxin Lin,Jianmin Bao,Ming Zeng,Dong Chen*

Main category: cs.CV

TL;DR: 本文提出了HairShifter框架，实现了高质量、时序一致的视频发型迁移。该方法结合单帧精确迁移与跨帧平滑动画，达到了领先的视觉质量与稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前在社交媒体、游戏和娱乐等领域，发型迁移的应用需求不断增长。单幅图像的发型迁移已取得进展，但视频发型迁移仍面临时序一致性与动态适应等难题。本文旨在解决视频发型迁移中的空间准确性与时序平滑性问题。

Method: 作者提出“锚帧+动画”框架（HairShifter），核心包括：1）图像发型迁移（IHT）模块，负责每帧的高精度发型变换；2）多尺度门控SPADE解码器，实现空间融合与时序一致。该方法可保持发型在视频各帧中的连贯性，同时保护非发区域。

Result: 通过大量实验，HairShifter在视频发型迁移任务中取得了领先的视觉效果、时序一致性与可扩展性，超越了现有方法。

Conclusion: HairShifter为视频发型迁移提供了新的方法论，能在保证视觉质量的同时，实现风格连贯的动态发型转换。该工作有望成为该领域的基线方法，促进相关应用的发展。

Abstract: Hair transfer is increasingly valuable across domains such as social media,
gaming, advertising, and entertainment. While significant progress has been
made in single-image hair transfer, video-based hair transfer remains
challenging due to the need for temporal consistency, spatial fidelity, and
dynamic adaptability. In this work, we propose HairShifter, a novel "Anchor
Frame + Animation" framework that unifies high-quality image hair transfer with
smooth and coherent video animation. At its core, HairShifter integrates a
Image Hair Transfer (IHT) module for precise per-frame transformation and a
Multi-Scale Gated SPADE Decoder to ensure seamless spatial blending and
temporal coherence. Our method maintains hairstyle fidelity across frames while
preserving non-hair regions. Extensive experiments demonstrate that HairShifter
achieves state-of-the-art performance in video hairstyle transfer, combining
superior visual quality, temporal consistency, and scalability. The code will
be publicly available. We believe this work will open new avenues for
video-based hairstyle transfer and establish a robust baseline in this field.

</details>


### [18] [Unified Medical Image Segmentation with State Space Modeling Snake](https://arxiv.org/abs/2507.12760)
*Ruicheng Zhang,Haowei Guo,Kanghui Tian,Jun Zhou,Mingliang Yan,Zeyu Zhang,Shen Zhao*

Main category: cs.CV

TL;DR: 本文提出Mamba Snake，一种结合状态空间建模的全新深度snake框架，可更有效地实现统一医学图像分割，显著提升分割性能，平均Dice分数提升3%。


<details>
  <summary>Details</summary>
Motivation: 统一医学图像分割(UMIS)对于全方位解剖结构评估极为重要，但因存在多尺度结构异质性，传统像素级方法常因缺乏目标级结构建模和器官间关系刻画，难以应对形态复杂度与特征冲突，导致分割效果有限。

Method: 提出Mamba Snake框架，通过状态空间建模，将多轮廓演化表示为层次化的状态空间图谱，能兼顾器官宏观拓扑关系及细粒度轮廓精修。创新性引入适应snake轮廓演化的视觉状态空间模块(Mamba Evolution Block，MEB)，实现时空信息融合与复杂形态自适应细化。利用能量图形状先验辅助长距离轮廓演化，并通过双分类协同机制，同时优化检测与分割，缓解微结构欠分割问题。

Result: 在五个临床数据集上广泛验证，Mamba Snake在统一医学图像分割任务中取得领先性能，Dice系数较现有最优方法平均提升3%。

Conclusion: Mamba Snake框架系统提升了统一医学图像分割的精度与鲁棒性，为临床多器官、多结构影像解析提供了更强工具与方法基础。

Abstract: Unified Medical Image Segmentation (UMIS) is critical for comprehensive
anatomical assessment but faces challenges due to multi-scale structural
heterogeneity. Conventional pixel-based approaches, lacking object-level
anatomical insight and inter-organ relational modeling, struggle with
morphological complexity and feature conflicts, limiting their efficacy in
UMIS. We propose Mamba Snake, a novel deep snake framework enhanced by state
space modeling for UMIS. Mamba Snake frames multi-contour evolution as a
hierarchical state space atlas, effectively modeling macroscopic inter-organ
topological relationships and microscopic contour refinements. We introduce a
snake-specific vision state space module, the Mamba Evolution Block (MEB),
which leverages effective spatiotemporal information aggregation for adaptive
refinement of complex morphologies. Energy map shape priors further ensure
robust long-range contour evolution in heterogeneous data. Additionally, a
dual-classification synergy mechanism is incorporated to concurrently optimize
detection and segmentation, mitigating under-segmentation of microstructures in
UMIS. Extensive evaluations across five clinical datasets reveal Mamba Snake's
superior performance, with an average Dice improvement of 3\% over
state-of-the-art methods.

</details>


### [19] [Think-Before-Draw: Decomposing Emotion Semantics & Fine-Grained Controllable Expressive Talking Head Generation](https://arxiv.org/abs/2507.12761)
*Hanlei Shi,Leyuan Qu,Yu Liu,Di Gao,Yuhua Zheng,Taihao Li*

Main category: cs.CV

TL;DR: 该论文提出了一种新的“Think-Before-Draw”框架，通过语言链式推理将抽象的情感标签转为具体面部肌肉动作，实现了更自然的情感对话头像生成，并在多个数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的情感对话头像生成方法多基于离散情感标签，无法真实捕捉和表达复杂多变的面部表情动态，导致生成结果生硬、不自然。该论文旨在解决情感语义到具体动作转换及细粒度表情表达两个核心难题，从而提升人机交互的真实感和情感共鸣能力。

Method: 提出了“Think-Before-Draw”框架：（1）通过引入链式思维（CoT），将高层次的情感标签语义解析为生理层面的面部肌肉动作特征，实现语义到动作的映射；（2）借鉴艺术家绘画思路，引入渐进式引导去噪机制，采用“全局情感定位—局部肌肉控制”的方式，对微表情进行细致调整，提升生成视频的自然度和细腻度。

Result: 实验证明，该方法在MEAD和HDTF等业界常用基准数据集上取得了最先进的性能。此外，作者还采集了肖像图片测试模型的零样本生成能力，取得了良好效果。

Conclusion: “Think-Before-Draw”框架能有效提升文本驱动情感头像生成的自然性和表现力，对情感计算与人机交互领域具有重要推动作用。

Abstract: Emotional talking-head generation has emerged as a pivotal research area at
the intersection of computer vision and multimodal artificial intelligence,
with its core value lying in enhancing human-computer interaction through
immersive and empathetic engagement.With the advancement of multimodal large
language models, the driving signals for emotional talking-head generation has
shifted from audio and video to more flexible text. However, current
text-driven methods rely on predefined discrete emotion label texts,
oversimplifying the dynamic complexity of real facial muscle movements and thus
failing to achieve natural emotional expressiveness.This study proposes the
Think-Before-Draw framework to address two key challenges: (1) In-depth
semantic parsing of emotions--by innovatively introducing Chain-of-Thought
(CoT), abstract emotion labels are transformed into physiologically grounded
facial muscle movement descriptions, enabling the mapping from high-level
semantics to actionable motion features; and (2) Fine-grained expressiveness
optimization--inspired by artists' portrait painting process, a progressive
guidance denoising strategy is proposed, employing a "global emotion
localization--local muscle control" mechanism to refine micro-expression
dynamics in generated videos.Our experiments demonstrate that our approach
achieves state-of-the-art performance on widely-used benchmarks, including MEAD
and HDTF. Additionally, we collected a set of portrait images to evaluate our
model's zero-shot generation capability.

</details>


### [20] [World Model-Based End-to-End Scene Generation for Accident Anticipation in Autonomous Driving](https://arxiv.org/abs/2507.12762)
*Yanchen Guan,Haicheng Liao,Chengyue Wang,Xingcheng Liu,Jiaxun Zhang,Zhenning Li*

Main category: cs.CV

TL;DR: 本论文提出了一种结合生成式场景增强与自适应时序推理的框架，通过生成高质量多样化的驾驶场景，并构建动态预测模型，提升自动驾驶系统对交通事故的预判准确率和时效性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶事故预判面临数据稀缺和信息缺失两大难题：一方面缺乏多样高质量的训练场景，另一方面环境与传感器问题导致关键信息时常缺失。

Method: 1）提出基于世界模型的视频生成流水线，通过领域知识驱动生成高分辨率、统计一致的驾驶场景，增加极端和复杂案例；2）设计动态预测模型，利用强化的图卷积与扩张时间算子编码时空关系，缓解数据缺失和视觉噪声影响；3）发布一个更好反映真实驾驶风险的新基准数据集。

Result: 在公开及自建数据集上的大量实验表明，该框架在事故预判的准确率与提前量两方面显著优于现有方法。

Conclusion: 文中提出的框架能有效克服数据和建模现有限制，为安全关键型自动驾驶提供更鲁棒的事故预判方案。

Abstract: Reliable anticipation of traffic accidents is essential for advancing
autonomous driving systems. However, this objective is limited by two
fundamental challenges: the scarcity of diverse, high-quality training data and
the frequent absence of crucial object-level cues due to environmental
disruptions or sensor deficiencies. To tackle these issues, we propose a
comprehensive framework combining generative scene augmentation with adaptive
temporal reasoning. Specifically, we develop a video generation pipeline that
utilizes a world model guided by domain-informed prompts to create
high-resolution, statistically consistent driving scenarios, particularly
enriching the coverage of edge cases and complex interactions. In parallel, we
construct a dynamic prediction model that encodes spatio-temporal relationships
through strengthened graph convolutions and dilated temporal operators,
effectively addressing data incompleteness and transient visual noise.
Furthermore, we release a new benchmark dataset designed to better capture
diverse real-world driving risks. Extensive experiments on public and newly
released datasets confirm that our framework enhances both the accuracy and
lead time of accident anticipation, offering a robust solution to current data
and modeling limitations in safety-critical autonomous driving applications.

</details>


### [21] [Continuous Marine Tracking via Autonomous UAV Handoff](https://arxiv.org/abs/2507.12763)
*Heegyeong Kim,Alice James,Avishkar Seth,Endrowednes Kuantama,Jane Williamson,Yimeng Feng,Richard Han*

Main category: cs.CV

TL;DR: 该论文提出了一套自主无人机视觉系统，能在复杂的海洋环境下持续追踪鲨鱼，并实现多机协同无缝交接，显著提升了监控时长和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 海洋动物（如鲨鱼）监测对生态保护和科学研究极为重要，但由于环境复杂、目标经常遮挡，以及单架无人机续航受限，现有无人机系统难以实现长期、连续、精准的追踪。

Method: 本研究将带有自稳定RGB-D相机的无人机与定制训练的OSTrack视觉追踪算法结合，开发实时识别与跟踪系统。在此基础上，创新性地引入无人机间的追踪目标无缝交接协议，通过高置信度特征匹配，实现多机协同连续追踪。系统在包含5200帧的鲨鱼数据集上评估表现。

Result: 实验结果显示，系统在实时飞控（100Hz）下，鲨鱼追踪成功率达81.9%；目标交接过程中目标覆盖率为82.9%，对遮挡、光照变化、背景干扰表现出较高鲁棒性。

Conclusion: 研究验证了多无人机协同自主追踪在动态海洋环境中可行性，为未来大规模自主水域动物监控提供了技术基础。

Abstract: This paper introduces an autonomous UAV vision system for continuous,
real-time tracking of marine animals, specifically sharks, in dynamic marine
environments. The system integrates an onboard computer with a stabilised RGB-D
camera and a custom-trained OSTrack pipeline, enabling visual identification
under challenging lighting, occlusion, and sea-state conditions. A key
innovation is the inter-UAV handoff protocol, which enables seamless transfer
of tracking responsibilities between drones, extending operational coverage
beyond single-drone battery limitations. Performance is evaluated on a curated
shark dataset of 5,200 frames, achieving a tracking success rate of 81.9\%
during real-time flight control at 100 Hz, and robustness to occlusion,
illumination variation, and background clutter. We present a seamless UAV
handoff framework, where target transfer is attempted via high-confidence
feature matching, achieving 82.9\% target coverage. These results confirm the
viability of coordinated UAV operations for extended marine tracking and lay
the groundwork for scalable, autonomous monitoring.

</details>


### [22] [AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation](https://arxiv.org/abs/2507.12768)
*Hengkai Tan,Yao Feng,Xinyi Mao,Shuhe Huang,Guodong Liu,Zhongkai Hao,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: 本论文提出了基于视觉-语言-动作（VLA）模型的全新泛任务动作范式，并引入ATARA和AnyPos方法，在复杂操作控制任务中，提高了泛化能力与数据采集效率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在复杂操作控制中表现良好，但严重依赖任务特定的人工演示，导致泛化性差且数据采集成本高。为此，作者提出了任务无关的动作采集新范式来提升扩展性和降低成本。

Method: 作者提出了ATARA（自动任务无关随机动作）框架，通过自监督方式，显著加速泛任务数据收集。为解决数据分布不匹配和无关轨迹问题，提出了AnyPos逆动力学模型，结合机械臂解耦估计和方向感知解码器，并集成视频条件下的动作验证模块。

Result: 与人工远程操作相比，ATARA实现了数据收集速率提升30倍。AnyPos-ATARA流程在测试精度提升了51%，在多种下游操作中（如搬起、放置、点击等）成功率提升了30-40%。

Conclusion: ATARA和AnyPos方法不仅大幅提升了数据采集效率，还有效增强了泛化能力与多任务操作表现，为任务无关的数据采集和高效控制策略学习提供了有力支持。

Abstract: Vision-language-action (VLA) models have shown promise on task-conditioned
control in complex settings such as bimanual manipulation. However, the heavy
reliance on task-specific human demonstrations limits their generalization and
incurs high data acquisition costs. In this work, we present a new notion of
task-agnostic action paradigm that decouples action execution from
task-specific conditioning, enhancing scalability, efficiency, and
cost-effectiveness. To address the data collection challenges posed by this
paradigm -- such as low coverage density, behavioral redundancy, and safety
risks -- we introduce ATARA (Automated Task-Agnostic Random Actions), a
scalable self-supervised framework that accelerates collection by over $
30\times $ compared to human teleoperation. To further enable effective
learning from task-agnostic data, which often suffers from distribution
mismatch and irrelevant trajectories, we propose AnyPos, an inverse dynamics
model equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder
(DAD). We additionally integrate a video-conditioned action validation module
to verify the feasibility of learned policies across diverse manipulation
tasks. Extensive experiments show that the AnyPos-ATARA pipeline yields a 51%
improvement in test accuracy and achieves 30-40% higher success rates in
downstream tasks such as lifting, pick-and-place, and clicking, using
replay-based video validation. Project Page:
https://embodiedfoundation.github.io/vidar_anypos

</details>


### [23] [Local Representative Token Guided Merging for Text-to-Image Generation](https://arxiv.org/abs/2507.12771)
*Min-Jeong Lee,Hee-Dong Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新的token合并方法ReToM，用于提升Stable Diffusion等基于注意力机制的图像生成模型的效率，并在保持推理速度的同时，提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: Stable Diffusion等文本到图像生成模型由于注意力操作的二次复杂度，计算效率低。现有的token合并方法往往忽略了生成模型的特殊性，因此提升空间有限。

Method: 提出了Local Representative Token Guided Merging (ReToM)策略：将输入tokens依据局部窗口划分为局部区域，通过计算相似度选取每个窗口代表性的token进行保留与合并。这样在不同上下文信息下，动态调整窗口大小，精准保留局部显著特征，同时降低了注意力的计算量。

Result: 在各项实验中，ReToM与baseline对比，FID提升6.2%，CLIP分数更高，推理时间相当。显示其在保证视觉质量的情况下，有效提升了计算效率。

Conclusion: ReToM方法在视觉质量和计算效率之间实现了更好的平衡，可推广到任意注意力机制的图像生成任务。

Abstract: Stable diffusion is an outstanding image generation model for text-to-image,
but its time-consuming generation process remains a challenge due to the
quadratic complexity of attention operations. Recent token merging methods
improve efficiency by reducing the number of tokens during attention
operations, but often overlook the characteristics of attention-based image
generation models, limiting their effectiveness. In this paper, we propose
local representative token guided merging (ReToM), a novel token merging
strategy applicable to any attention mechanism in image generation. To merge
tokens based on various contextual information, ReToM defines local boundaries
as windows within attention inputs and adjusts window sizes. Furthermore, we
introduce a representative token, which represents the most representative
token per window by computing similarity at a specific timestep and selecting
the token with the highest average similarity. This approach preserves the most
salient local features while minimizing computational overhead. Experimental
results show that ReToM achieves a 6.2% improvement in FID and higher CLIP
scores compared to the baseline, while maintaining comparable inference time.
We empirically demonstrate that ReToM is effective in balancing visual quality
and computational efficiency.

</details>


### [24] [Compact Vision Transformer by Reduction of Kernel Complexity](https://arxiv.org/abs/2507.12780)
*Yancheng Wang,Yingzhen Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为KCR-Transformer的新型Transformer块，通过可微分通道选择技术，在MLP层进行输入/输出通道裁剪，有效降低计算复杂度，并在多个视觉任务上取得更优性能且资源消耗更低。


<details>
  <summary>Details</summary>
Motivation: 虽已有紧凑型视觉Transformer，但如何进一步降低计算量同时维持甚至提升性能仍是挑战。作者关注如何理论指导Transformer网络的通道裁剪，实现高效且泛化性能强的网络架构。

Method: 提出KCR-Transformer模块，在Transformer的MLP层引入可微分通道选择，通过理论推导给出整体模型更严格的泛化界限，使通道剪枝过程具备泛化误差可控的理论基础。该模块可应用于ViT、Swin Transformer等主流结构。

Result: 实验证明在用KCR-Transformer替换视觉Transformer所有块后，网络不仅计算量和参数显著减少，而且在多项视觉任务上性能优于原模型。

Conclusion: KCR-Transformer有效结合理论泛化界和实际通道裁剪，优化模型计算效率的同时改善或保持模型性能，适合多类主流视觉Transformer架构。

Abstract: Self-attention and transformer architectures have become foundational
components in modern deep learning. Recent efforts have integrated transformer
blocks into compact neural architectures for computer vision, giving rise to
various efficient vision transformers. In this work, we introduce Transformer
with Kernel Complexity Reduction, or KCR-Transformer, a compact transformer
block equipped with differentiable channel selection, guided by a novel and
sharp theoretical generalization bound. KCR-Transformer performs input/output
channel selection in the MLP layers of transformer blocks to reduce the
computational cost. Furthermore, we provide a rigorous theoretical analysis
establishing a tight generalization bound for networks equipped with
KCR-Transformer blocks. Leveraging such strong theoretical results, the channel
pruning by KCR-Transformer is conducted in a generalization-aware manner,
ensuring that the resulting network retains a provably small generalization
error. Our KCR-Transformer is compatible with many popular and compact
transformer networks, such as ViT and Swin, and it reduces the FLOPs of the
vision transformers while maintaining or even improving the prediction
accuracy. In the experiments, we replace all the transformer blocks in the
vision transformers with KCR-Transformer blocks, leading to KCR-Transformer
networks with different backbones. The resulting TCR-Transformers achieve
superior performance on various computer vision tasks, achieving even better
performance than the original models with even less FLOPs and parameters.

</details>


### [25] [City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning](https://arxiv.org/abs/2507.12795)
*Penglei Sun,Yaoxian Song,Xiangru Zhu,Xiang Liu,Qiang Wang,Yue Liu,Changqun Xia,Tiefeng Li,Yang Yang,Xiaowen Chu*

Main category: cs.CV

TL;DR: 本文提出了第一个专注于多域感知的户外场景理解数据集SVM-City，并提出了新的多模态视觉语言模型City-VLM，实现了在缺失模态下的多模态融合，在多个户外场景任务上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视觉语言模型主要聚焦于室内家庭环境，无法有效适应户外大规模场景，原因包括户外场景感知模态多，视角复杂及多模态数据缺失等问题。缺少专门针对户外多域、多视角、多模态的大数据集和对应融合方法，限制了LVLM在实际场景中的应用能力。

Method: 作者构建了包含42万张图像、4811万点云和56.7万问答对的SVM-City开放数据集，涵盖多尺度、多视角（车载、无人机、卫星等）和多模态信息。提出了支持不完全模态输入的多模态学习方法，不通过直接拼接，而是通过联合概率分布空间实现特征融合，并基于此设计了City-VLM模型。

Result: City-VLM在三项典型户外场景理解任务中都显著优于现有LVLM，在问答任务上平均性能提升18.14%。在数据不完整、模态丢失情况下依旧具备较强鲁棒性，并展现出良好的泛化能力。

Conclusion: 针对户外复杂场景，提出的大规模多模态数据集与不完全模态学习的融合方法能够显著提升场景理解性能。City-VLM为多模态融合和实际应用提供了新思路，并推动了LVLM在户外场景中的落地。

Abstract: Scene understanding enables intelligent agents to interpret and comprehend
their environment. While existing large vision-language models (LVLMs) for
scene understanding have primarily focused on indoor household tasks, they face
two significant limitations when applied to outdoor large-scale scene
understanding. First, outdoor scenarios typically encompass larger-scale
environments observed through various sensors from multiple viewpoints (e.g.,
bird view and terrestrial view), while existing indoor LVLMs mainly analyze
single visual modalities within building-scale contexts from humanoid
viewpoints. Second, existing LVLMs suffer from missing multidomain perception
outdoor data and struggle to effectively integrate 2D and 3D visual
information. To address the aforementioned limitations, we build the first
multidomain perception outdoor scene understanding dataset, named
\textbf{\underline{SVM-City}}, deriving from multi\textbf{\underline{S}}cale
scenarios with multi\textbf{\underline{V}}iew and
multi\textbf{\underline{M}}odal instruction tuning data. It contains $420$k
images and $4, 811$M point clouds with $567$k question-answering pairs from
vehicles, low-altitude drones, high-altitude aerial planes, and satellite. To
effectively fuse the multimodal data in the absence of one modality, we
introduce incomplete multimodal learning to model outdoor scene understanding
and design the LVLM named \textbf{\underline{City-VLM}}. Multimodal fusion is
realized by constructing a joint probabilistic distribution space rather than
implementing directly explicit fusion operations (e.g., concatenation).
Experimental results on three typical outdoor scene understanding tasks show
City-VLM achieves $18.14 \%$ performance surpassing existing LVLMs in
question-answering tasks averagely. Our method demonstrates pragmatic and
generalization performance across multiple outdoor scenes.

</details>


### [26] [DeQA-Doc: Adapting DeQA-Score to Document Image Quality Assessment](https://arxiv.org/abs/2507.12796)
*Junjie Gao,Runze Liu,Yingzhe Peng,Shujian Yang,Jin Zhang,Kai Yang,Zhiyuan You*

Main category: cs.CV

TL;DR: 本文提出了DeQA-Doc框架，通过多模态大模型提升文档质量评估的准确性和泛化能力，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文档质量评估方法在准确性和鲁棒性上表现有限，难以满足实际应用需要。随着多模态大模型的进步，有必要探索其在文档质量评估领域的应用潜力。

Method: 作者基于最先进的MLLM质量评分器DeQA-Score，做了适配和改进，提出DeQA-Doc，包括利用MLLM的视觉语言能力和软标签策略回归连续文档质量分数、用两种互补方案构建软标签、放宽分辨率约束、引入集成方法，提升对不同文档劣化类型的适应性。

Result: 大量实验表明，DeQA-Doc在文档质量评估任务上明显优于现有基线方法，无论文档劣化类型如何表现都很优异。

Conclusion: DeQA-Doc框架可为文档相关任务如扫描、档案管理等提供高准确性和良好泛化能力的质量评估，是对现有方法的重要提升。

Abstract: Document quality assessment is critical for a wide range of applications
including document digitization, OCR, and archival. However, existing
approaches often struggle to provide accurate and robust quality scores,
limiting their applicability in practical scenarios. With the rapid progress in
Multi-modal Large Language Models (MLLMs), recent MLLM-based methods have
achieved remarkable performance in image quality assessment. In this work, we
extend this success to the document domain by adapting DeQA-Score, a
state-of-the-art MLLM-based image quality scorer, for document quality
assessment. We propose DeQA-Doc, a framework that leverages the visual language
capabilities of MLLMs and a soft label strategy to regress continuous document
quality scores. To adapt DeQA-Score to DeQA-Doc, we adopt two complementary
solutions to construct soft labels without the variance information. Also, we
relax the resolution constrains to support the large resolution of document
images. Finally, we introduce ensemble methods to further enhance the
performance. Extensive experiments demonstrate that DeQA-Doc significantly
outperforms existing baselines, offering accurate and generalizable document
quality assessment across diverse degradation types. Codes and model weights
are available in https://github.com/Junjie-Gao19/DeQA-Doc.

</details>


### [27] [DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model](https://arxiv.org/abs/2507.13145)
*Maulana Bisyir Azhari,David Hyunchul Shim*

Main category: cs.CV

TL;DR: 本文提出了一种基于DINOv2视觉基础模型的单目视觉里程计（VO）系统DINO-VO，通过融合语义与几何特色，实现了更高的鲁棒性与泛化能力，同时保持高效运行。


<details>
  <summary>Details</summary>
Motivation: 学习型单目视觉里程计在鲁棒性、泛化性和效率上都面临挑战，而DINOv2等视觉基础模型虽表现优异，但难以直接应用于VO任务，主要由于其特征粒度较粗。

Method: 作者提出了DINO-VO系统，核心方法包括基于DINOv2的稀疏特征匹配、针对粗粒度特征设计的显著关键点检测器、结合细粒度几何特征以提升定位性、以及基于Transformer的特征匹配与可微姿态估计层，整体提升了匹配与运动估计的效果。

Result: 与SuperPoint等主流检测-描述网络相比，DINO-VO在TartanAir和KITTI数据集上表现出更强的鲁棒性、精度和泛化能力，在EuRoC数据集上也具备竞争力。同时，系统可在单GPU下以72 FPS高效运行，并且在户外驾驶等场景中与SLAM系统表现接近。

Conclusion: DINO-VO充分发挥了视觉基础模型在视觉里程计中的潜力，显著提升了鲁棒性与泛化能力，且具备高效率，为学习型VO开拓了新的模型选择和应用可能。

Abstract: Learning-based monocular visual odometry (VO) poses robustness,
generalization, and efficiency challenges in robotics. Recent advances in
visual foundation models, such as DINOv2, have improved robustness and
generalization in various vision tasks, yet their integration in VO remains
limited due to coarse feature granularity. In this paper, we present DINO-VO, a
feature-based VO system leveraging DINOv2 visual foundation model for its
sparse feature matching. To address the integration challenge, we propose a
salient keypoints detector tailored to DINOv2's coarse features. Furthermore,
we complement DINOv2's robust-semantic features with fine-grained geometric
features, resulting in more localizable representations. Finally, a
transformer-based matcher and differentiable pose estimation layer enable
precise camera motion estimation by learning good matches. Against prior
detector-descriptor networks like SuperPoint, DINO-VO demonstrates greater
robustness in challenging environments. Furthermore, we show superior accuracy
and generalization of the proposed feature descriptors against standalone
DINOv2 coarse features. DINO-VO outperforms prior frame-to-frame VO methods on
the TartanAir and KITTI datasets and is competitive on EuRoC dataset, while
running efficiently at 72 FPS with less than 1GB of memory usage on a single
GPU. Moreover, it performs competitively against Visual SLAM systems on outdoor
driving scenarios, showcasing its generalization capabilities.

</details>


### [28] [ATL-Diff: Audio-Driven Talking Head Generation with Early Landmarks-Guide Noise Diffusion](https://arxiv.org/abs/2507.12804)
*Hoang-Son Vo,Quang-Vinh Nguyen,Seungwon Kim,Hyung-Jeong Yang,Soonja Yeom,Soo-Hyung Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的音频驱动人脸动画生成方法ATL-Diff，能高效精确地同步音频与人脸表情，并显著优于当前主流技术。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动说话人脸生成方法面临表情与音频同步不准确、噪声大、计算成本高等挑战。本文旨在解决同步性差及生成质量不足的问题。

Method: ATL-Diff方法包含三大模块：1）用音频生成关键点的Landmark Generation Module，2）根据关键点分布引导噪声解耦音频特征的Landmarks-Guide Noise策略，3）保证面部身份保持的3D Identity Diffusion网络。

Result: 在MEAD和CREMA-D数据集上，ATL-Diff在各项评价指标上均超越现有最先进方法，支持近实时，高质量且高效的人脸动画生成。

Conclusion: ATL-Diff在确保同步精度、身份保持及计算效率方面取得显著进展，具备虚拟助手、教育、医疗沟通等多种应用潜力。源码已开源。

Abstract: Audio-driven talking head generation requires precise synchronization between
facial animations and audio signals. This paper introduces ATL-Diff, a novel
approach addressing synchronization limitations while reducing noise and
computational costs. Our framework features three key components: a Landmark
Generation Module converting audio to facial landmarks, a Landmarks-Guide Noise
approach that decouples audio by distributing noise according to landmarks, and
a 3D Identity Diffusion network preserving identity characteristics.
Experiments on MEAD and CREMA-D datasets demonstrate that ATL-Diff outperforms
state-of-the-art methods across all metrics. Our approach achieves near
real-time processing with high-quality animations, computational efficiency,
and exceptional preservation of facial nuances. This advancement offers
promising applications for virtual assistants, education, medical
communication, and digital platforms. The source code is available at:
\href{https://github.com/sonvth/ATL-Diff}{https://github.com/sonvth/ATL-Diff}

</details>


### [29] [SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models](https://arxiv.org/abs/2507.13152)
*Xiangyu Dong,Haoran Zhao,Jiang Gao,Haozhou Li,Xiaoguang Ma,Yaoming Zhou,Fuhai Chen,Juan Liu*

Main category: cs.CV

TL;DR: 提出了一种自我进化的视觉-语言导航(SE-VLN)框架，使导航智能体在测试过程中能持续演化，显著提升了在未知环境中的导航成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型(LLMs)的视觉-语言导航虽在理解和推理方面表现出色，但受限于固定的知识库和推理能力，难以高效融入经验性知识，进而缺乏自我进化能力。

Method: 受自然智能体进化能力启发，提出SE-VLN框架。SE-VLN包含三级核心模块：分层记忆模块(将成功和失败案例转化为可复用知识)、检索增强式思考推理模块(调用经验并实现多步决策)、反思模块(实现持续演化)。

Result: 在R2R和REVERSE数据集上的未知环境实验中，SE-VLN导航成功率分别达57%和35.2%，比当前SOTA方法分别提升23.9%和15.0%；且经验库越丰富性能越强。

Conclusion: SE-VLN框架极大提升了视觉-语言导航智能体的自我进化能力，为VLN领域提供了具有强大扩展性和学习能力的方向。

Abstract: Recent advances in vision-language navigation (VLN) were mainly attributed to
emerging large language models (LLMs). These methods exhibited excellent
generalization capabilities in instruction understanding and task reasoning.
However, they were constrained by the fixed knowledge bases and reasoning
abilities of LLMs, preventing fully incorporating experiential knowledge and
thus resulting in a lack of efficient evolutionary capacity. To address this,
we drew inspiration from the evolution capabilities of natural agents, and
proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the
ability to continuously evolve during testing. To the best of our knowledge, it
was the first time that an multimodal LLM-powered self-evolving VLN framework
was proposed. Specifically, SE-VLN comprised three core modules, i.e., a
hierarchical memory module to transfer successful and failure cases into
reusable knowledge, a retrieval-augmented thought-based reasoning module to
retrieve experience and enable multi-step decision-making, and a reflection
module to realize continual evolution. Comprehensive tests illustrated that the
SE-VLN achieved navigation success rates of 57% and 35.2% in unseen
environments, representing absolute performance improvements of 23.9% and 15.0%
over current state-of-the-art methods on R2R and REVERSE datasets,
respectively. Moreover, the SE-VLN showed performance improvement with
increasing experience repository, elucidating its great potential as a
self-evolving agent framework for VLN.

</details>


### [30] [Semantic-guided Fine-tuning of Foundation Model for Long-tailed Visual Recognition](https://arxiv.org/abs/2507.12807)
*Yufei Peng,Yonggang Zhang,Yiu-ming Cheung*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法Sage，通过将文本的语义信息引入视觉模型微调流程，提升了基础模型在长尾视觉识别中的表现，并创新性地提出了分布失配补偿因子，进一步校正预测偏差。实验表明该方法在各类基准数据集上的表现优异。


<details>
  <summary>Details</summary>
Motivation: 长尾分布数据中类别不均导致稀有类别识别性能下降，虽然基础模型在此领域潜力巨大，但现有微调方法常忽视了视觉和文本模态之间的对齐问题，特别是文本编码器的语义信息。

Method: 提出了Sage方法，具体包括：引入SG-Adapter，将类别描述作为语义引导引入视觉编码器微调流程，通过注意力机制聚焦于更相关的内容，增强视觉与文本对齐。同时，针对损失函数未能处理类别条件分布不一致的问题，设计了新的分布失配补偿因子嵌入损失函数，用以校正预测偏差。

Result: 在多个基准数据集上进行的广泛实验验证了Sage方法能有效提升长尾类别和整体的识别性能，优于现有方法。

Conclusion: Sage方法通过语义引导和对分布失配的补偿，优化了基础模型在长尾视觉识别中的表现，并为多模态长尾学习提供了新思路。

Abstract: The variance in class-wise sample sizes within long-tailed scenarios often
results in degraded performance in less frequent classes. Fortunately,
foundation models, pre-trained on vast open-world datasets, demonstrate strong
potential for this task due to their generalizable representation, which
promotes the development of adaptive strategies on pre-trained models in
long-tailed learning. Advanced fine-tuning methods typically adjust visual
encoders while neglecting the semantics derived from the frozen text encoder,
overlooking the visual and textual alignment. To strengthen this alignment, we
propose a novel approach, Semantic-guided fine-tuning of foundation model for
long-tailed visual recognition (Sage), which incorporates semantic guidance
derived from textual modality into the visual fine-tuning process.
Specifically, we introduce an SG-Adapter that integrates class descriptions as
semantic guidance to guide the fine-tuning of the visual encoder. The
introduced guidance is passesed through the attention mechanism and enables the
model to focus more on semantically relevant content, strengthening the
alignment between the visual and textual modalities. Due to the inconsistent
class-conditional distributions neglected by the existing loss function, the
resulting prediction bias causes performance improvements for the tail class
less than for the head class, even when the multi-modal alignment is enhanced.
To address this challenge, we propose a novel distribution mismatch-aware
compensation factor, which is specifically designed to rectify the prediction
bias caused by the ignored inconsistent distribution based on our theoretical
analysis, and is seamlessly integrated into the loss function. Extensive
experiments on benchmark datasets demonstrate the effectiveness of the proposed
Sage in enhancing performance in long-tailed learning.

</details>


### [31] [$S^2M^2$: Scalable Stereo Matching Model for Reliable Depth Estimation](https://arxiv.org/abs/2507.13229)
*Junhong Min,Youngpil Jeon,Jimin Kim,Minyong Choi*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的立体匹配模型S^2M^2，实现了高精度和高效率的全局匹配，突破了分辨率和视差范围泛化难题，并在多个公开基准上取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 现有立体匹配方法在提升泛化能力和全局一致性方面存在显著不足：迭代式局部搜索只能在受限环境下取得好成绩，难以泛化；全局方法虽然理论上更鲁棒，但资源消耗过大，难以实际应用。因此，亟需兼顾精度、效率和泛化能力的新方法。

Method: 作者提出S^2M^2模型，采用多分辨率Transformer结构用于长距离像素匹配，并设计了一种新颖损失函数，提升了匹配概率的集中性。方法无需高耗资源的代价体滤波和深度精细化堆叠，实现了对视差、遮挡和置信度的联合鲁棒估计。

Result: S^2M^2在Middlebury v3和ETH3D等主流基准上刷新了准确率纪录，大幅超越前人方法，同时能重建高质量细节并保证算法运行效率。

Conclusion: S^2M^2有效解决了立体匹配中全局一致性与计算代价之间的矛盾，实现了泛化性、准确性与高效性的统一，对未来立体视觉任务有重要推动作用。

Abstract: The pursuit of a generalizable stereo matching model, capable of performing
across varying resolutions and disparity ranges without dataset-specific
fine-tuning, has revealed a fundamental trade-off. Iterative local search
methods achieve high scores on constrained benchmarks, but their core mechanism
inherently limits the global consistency required for true generalization. On
the other hand, global matching architectures, while theoretically more robust,
have been historically rendered infeasible by prohibitive computational and
memory costs. We resolve this dilemma with $S^2M^2$: a global matching
architecture that achieves both state-of-the-art accuracy and high efficiency
without relying on cost volume filtering or deep refinement stacks. Our design
integrates a multi-resolution transformer for robust long-range correspondence,
trained with a novel loss function that concentrates probability on feasible
matches. This approach enables a more robust joint estimation of disparity,
occlusion, and confidence. $S^2M^2$ establishes a new state of the art on the
Middlebury v3 and ETH3D benchmarks, significantly outperforming prior methods
across most metrics while reconstructing high-quality details with competitive
efficiency.

</details>


### [32] [FIQ: Fundamental Question Generation with the Integration of Question Embeddings for Video Question Answering](https://arxiv.org/abs/2507.12816)
*Ju-Young Oh,Ho-Joong Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 本文提出一种增强视频问答（VQA）模型推理能力的新方法，称为FIQ，通过生成基于视频描述的基础Q&A对，提升了模型对视频的理解和泛化能力，并在SUTD-TrafficQA数据集上取得了最新最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答方法主要依赖事件中心的Q&A标注，缺乏对视频中对象类型、空间布局和属性等基础信息的捕捉，导致场景表征片面，限制了模型的泛化与高级推理能力。

Method: 提出FIQ方法，核心是自动从视频描述中生成包含基础场景信息的Q&A对，丰富训练数据；同时引入VQ-CAlign模块，将问题嵌入与视觉特征结合，保留重要的领域细节，提升后续任务的适应性。

Result: 在SUTD-TrafficQA数据集上的实验表明，FIQ方法在与现有主流方法的对比中取得了最新最优的性能。

Conclusion: 通过增强视频基础理解和引入额外场景上下文，FIQ方法不仅提升了模型的泛化和推理能力，还验证了该方法在实际场景中优于现有主流方案。

Abstract: Video question answering (VQA) is a multimodal task that requires the
interpretation of a video to answer a given question. Existing VQA methods
primarily utilize question and answer (Q&A) pairs to learn the spatio-temporal
characteristics of video content. However, these annotations are typically
event-centric, which is not enough to capture the broader context of each
video. The absence of essential details such as object types, spatial layouts,
and descriptive attributes restricts the model to learning only a fragmented
scene representation. This issue limits the model's capacity for generalization
and higher-level reasoning. In this paper, we propose a fundamental question
generation with the integration of question embeddings for video question
answering (FIQ), a novel approach designed to strengthen the reasoning ability
of the model by enhancing the fundamental understanding of videos. FIQ
generates Q&A pairs based on descriptions extracted from videos, enriching the
training data with fundamental scene information. Generated Q&A pairs enable
the model to understand the primary context, leading to enhanced
generalizability and reasoning ability. Furthermore, we incorporate a VQ-CAlign
module that assists task-specific question embeddings with visual features,
ensuring that essential domain-specific details are preserved to increase the
adaptability of downstream tasks. Experiments on SUTD-TrafficQA demonstrate
that our FIQ achieves state-of-the-art performance compared to existing
baseline methods.

</details>


### [33] [VITA: Vision-to-Action Flow Matching Policy](https://arxiv.org/abs/2507.13231)
*Dechen Gao,Boqi Zhao,Andrew Lee,Ian Chuang,Hanchu Zhou,Hang Wang,Zhe Zhao,Junshan Zhang,Iman Soltani*

Main category: cs.CV

TL;DR: VITA是一种新颖的视觉到动作流匹配方法，通过将视觉潜变量直接映射到动作潜变量，实现高效且简洁的视觉运动控制，尤其适用于复杂的双手操作任务。其MLP实现比现有主流方法更快且表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配和扩散策略通常需要额外的视觉-动作条件机制（如交叉注意力），导致推理开销大且模型复杂。如何消除冗余条件步骤并提升推理效率，而不损失生成能力，是推动仿生机器人控制发展的关键。

Method: VITA直接将视觉潜变量作为流映射的源头，通过设计结构化动作潜空间（利用自动编码器），并用流潜量解码联合监督，将高维视觉表示转换为结构化动作潜变量。流程通过MLP实现，并采用端到端的损失反向传播优化。

Result: VITA在ALOHA平台的五个仿真任务和两个真实任务上测试，MLP实现的VITA超越或达到最新生成策略的表现，且相比需条件机制/复杂结构的方法，推理延迟降低50-130%。

Conclusion: VITA首次实现了仅用MLP进行复杂双手操作任务的流匹配策略，在保证性能的同时极大简化了推理流程和结构设计，为视觉运动控制提供了高效、可扩展的新方法。

Abstract: We present VITA, a Vision-To-Action flow matching policy that evolves latent
visual representations into latent actions for visuomotor control. Traditional
flow matching and diffusion policies sample from standard source distributions
(e.g., Gaussian noise) and require additional conditioning mechanisms like
cross-attention to condition action generation on visual information, creating
time and space overheads. VITA proposes a novel paradigm that treats latent
images as the flow source, learning an inherent mapping from vision to action
while eliminating separate conditioning modules and preserving generative
modeling capabilities. Learning flows between fundamentally different
modalities like vision and action is challenging due to sparse action data
lacking semantic structures and dimensional mismatches between high-dimensional
visual representations and raw actions. We address this by creating a
structured action latent space via an autoencoder as the flow matching target,
up-sampling raw actions to match visual representation shapes. Crucially, we
supervise flow matching with both encoder targets and final action outputs
through flow latent decoding, which backpropagates action reconstruction loss
through sequential flow matching ODE solving steps for effective end-to-end
learning. Implemented as simple MLP layers, VITA is evaluated on challenging
bi-manual manipulation tasks on the ALOHA platform, including 5 simulation and
2 real-world tasks. Despite its simplicity, MLP-only VITA outperforms or
matches state-of-the-art generative policies while reducing inference latency
by 50-130% compared to conventional flow matching policies requiring different
conditioning mechanisms or complex architectures. To our knowledge, VITA is the
first MLP-only flow matching policy capable of solving complex bi-manual
manipulation tasks like those in ALOHA benchmarks.

</details>


### [34] [MCoT-RE: Multi-Faceted Chain-of-Thought and Re-Ranking for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2507.12819)
*Jeong-Woo Park,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 提出了一种无训练、零样本的复合图像检索（CIR）方法MCoT-RE，通过多方面思维链条和重排序策略，在兼顾文本修改与参考图像上下文信息的基础上，实现了当前训练自由方法中的新性能高点。


<details>
  <summary>Details</summary>
Motivation: 现有CIR方法，如基于预训练模型的零样本管道，仍存在独立处理多模态信息导致信息丢失、跨模态交互受限或未能充分利用视觉上下文等不足，因此亟需一种既无须额外训练又能有效融合多模态信息的新方案。

Method: 提出MCoT-RE框架，利用多方面思维链条指导多模态大模型生成两类描述：一类关注文本修改，另一类融合视觉上下文。先以修改关注型描述过滤候选图像，再将两种描述和参考图像结合，进行多层次重排序，实现更精准的检索。全流程无训练，属零样本方法。

Result: 在FashionIQ和CIRR数据集上，MCoT-RE相比同期零样本方法表现突出，Recall@10提升最高6.24%，Recall@1提升最高8.58%。

Conclusion: MCoT-RE显著提升了训练自由、零样本的复合图像检索性能，有效调和了文本修改与视觉上下文，检索准确率达到当前顶尖水平。

Abstract: Composed Image Retrieval (CIR) is the task of retrieving a target image from
a gallery using a composed query consisting of a reference image and a
modification text. Among various CIR approaches, training-free zero-shot
methods based on pre-trained models are cost-effective but still face notable
limitations. For example, sequential VLM-LLM pipelines process each modality
independently, which often results in information loss and limits cross-modal
interaction. In contrast, methods based on multimodal large language models
(MLLMs) often focus exclusively on applying changes indicated by the text,
without fully utilizing the contextual visual information from the reference
image. To address these issues, we propose multi-faceted Chain-of-Thought with
re-ranking (MCoT-RE), a training-free zero-shot CIR framework. MCoT-RE utilizes
multi-faceted Chain-of-Thought to guide the MLLM to balance explicit
modifications and contextual visual cues, generating two distinct captions: one
focused on modification and the other integrating comprehensive visual-textual
context. The first caption is used to filter candidate images. Subsequently, we
combine these two captions and the reference image to perform multi-grained
re-ranking. This two-stage approach facilitates precise retrieval by aligning
with the textual modification instructions while preserving the visual context
of the reference image. Through extensive experiments, MCoT-RE achieves
state-of-the-art results among training-free methods, yielding improvements of
up to 6.24% in Recall@10 on FashionIQ and 8.58% in Recall@1 on CIRR.

</details>


### [35] [FAR-Net: Multi-Stage Fusion Network with Enhanced Semantic Alignment and Adaptive Reconciliation for Composed Image Retrieval](https://arxiv.org/abs/2507.12823)
*Jeong-Woo Park,Young-Eun Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 本文提出了一种面向复合图像检索的新方法FAR-Net，通过多阶段融合实现图像和文本的高效结合，在主流评测集上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 复合图像检索任务要求根据参考图像和修改文本检索目标图像，对视觉与文本的信息高效融合提出更高要求。现有早融合方法忽略视觉上下文，晚融合方法难以对齐细粒度语义，因此亟需新的融合方式提升效果。

Method: 作者提出FAR-Net多阶段融合框架，包含：1）增强语义对齐模块（ESAM），采用基于交叉注意力的晚融合捕捉区域与文本间的细粒度关系；2）自适应融合模块（ARM），用早融合并引入不确定性嵌入提升系统鲁棒性与适应性，两模块互补提升整体性能。

Result: 在CIRR和FashionIQ两个公开图像检索数据集上，FAR-Net在Recall@1及Recall@50评价指标上分别超越最新方法2.4%和1.04%。

Conclusion: FAR-Net能够更好地对齐和融合视觉与文本信息，为复合图像检索任务提供了稳健且具有可扩展性的解决方案。

Abstract: Composed image retrieval (CIR) is a vision language task that retrieves a
target image using a reference image and modification text, enabling intuitive
specification of desired changes. While effectively fusing visual and textual
modalities is crucial, existing methods typically adopt either early or late
fusion. Early fusion tends to excessively focus on explicitly mentioned textual
details and neglect visual context, whereas late fusion struggles to capture
fine-grained semantic alignments between image regions and textual tokens. To
address these issues, we propose FAR-Net, a multi-stage fusion framework
designed with enhanced semantic alignment and adaptive reconciliation,
integrating two complementary modules. The enhanced semantic alignment module
(ESAM) employs late fusion with cross-attention to capture fine-grained
semantic relationships, while the adaptive reconciliation module (ARM) applies
early fusion with uncertainty embeddings to enhance robustness and
adaptability. Experiments on CIRR and FashionIQ show consistent performance
gains, improving Recall@1 by up to 2.4% and Recall@50 by 1.04% over existing
state-of-the-art methods, empirically demonstrating that FAR Net provides a
robust and scalable solution to CIR tasks.

</details>


### [36] [Feature-Enhanced TResNet for Fine-Grained Food Image Classification](https://arxiv.org/abs/2507.12828)
*Lulu Liu,Zhiyong Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种基于TResNet改进的特征增强网络（FE-TResNet）进行细粒度食品图像分类，显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 随着科技发展，食品图像的精确分类需求增大，但现有CNN难以区分细粒度、形状相似且细节微妙的食品图像，因此需要新的方法提升分类性能。

Method: 设计了一种名为FE-TResNet的创新方法，在TResNet模型基础上，融合了风格再校准模块（StyleRM）和深度通道注意力（DCA），以增强特征提取能力，专注于细粒度食品图像分类。

Result: 在ChineseFoodNet和CNFOOD-241两个中国食品图像数据集上，FE-TResNet将分类准确率分别提升至81.37%和80.29%。

Conclusion: FE-TResNet方法在细粒度食品图像分类任务上展现出卓越的效果和优越性，为食品图像处理相关应用提供了有力工具。

Abstract: Food is not only a core component of humans' daily diets, but also an
important carrier of cultural heritage and emotional bonds. With the
development of technology, the need for accurate classification of food images
has grown, which is crucial for a variety of application scenarios. However,
existing Convolutional Neural Networks (CNNs) face significant challenges when
dealing with fine-grained food images that are similar in shape but subtle in
detail. To address this challenge, this study presents an innovative method for
classifying food images, named Feature-Enhanced TResNet (FE-TResNet),
specifically designed to address fine-grained food images and accurately
capture subtle features within them. The FE-TResNet method is based on the
TResNet model and integrates Style-based Recalibration Module (StyleRM) and
Deep Channel-wise Attention (DCA) technologies to enhance feature extraction
capabilities. In experimental validation on Chinese food image datasets
ChineseFoodNet and CNFOOD-241, the FE-TResNet method significantly improved
classification accuracy, achieving rates of 81.37% and 80.29%, respectively,
demonstrating its effectiveness and superiority in fine-grained food image
classification.

</details>


### [37] [MVA 2025 Small Multi-Object Tracking for Spotting Birds Challenge: Dataset, Methods, and Results](https://arxiv.org/abs/2507.12832)
*Yuki Kondo,Norimichi Ukita,Riku Kanayama,Yuki Yoshida,Takayuki Yamaguchi,Xiang Yu,Guang Liang,Xinyao Liu,Guan-Zhang Wang,Wei-Ta Chu,Bing-Cheng Chuang,Jia-Hua Lee,Pin-Tseng Kuo,I-Hsuan Chu,Yi-Shein Hsiao,Cheng-Han Wu,Po-Yi Wu,Jui-Chien Tsou,Hsuan-Chi Liu,Chun-Yi Lee,Yuan-Fu Yang,Kosuke Shigematsu,Asuka Shin,Ba Tran*

Main category: cs.CV

TL;DR: 本文提出了SMOT4SB挑战，通过引入多时序信息、全新数据集和新评测指标，有效提升了小目标多目标跟踪能力，在无人机多场景中取得显著进步。


<details>
  <summary>Details</summary>
Motivation: 对于目标像素极小的多目标跟踪，传统基于单帧检测及外观关联方法效果不佳，限制了实际应用。因此急需考虑时间信息的新方法，并有高质量数据和评测工具推动该领域发展。

Method: 1）构建SMOT4SB数据集，含211段无人机视频、108,192帧多目标真实标注，覆盖复杂运动状态；2）提出SO-HOTA评测指标，将点距离与HOTA融合，缓解小物体IoU评估偏差；3）举办大规模竞赛，吸引78队、308次方法提交。

Result: SMOT4SB竞赛优胜方法在主基线基础上取得5.1倍性能提升，验证了用时序信息和新评测工具后的SMOT效果显著提高。

Conclusion: 本工作为无人机多场景下的小目标多目标跟踪奠定了新的数据与方法基础，有助于鸟击防控、农业、渔业和生态监测等应用。

Abstract: Small Multi-Object Tracking (SMOT) is particularly challenging when targets
occupy only a few dozen pixels, rendering detection and appearance-based
association unreliable. Building on the success of the MVA2023 SOD4SB
challenge, this paper introduces the SMOT4SB challenge, which leverages
temporal information to address limitations of single-frame detection. Our
three main contributions are: (1) the SMOT4SB dataset, consisting of 211 UAV
video sequences with 108,192 annotated frames under diverse real-world
conditions, designed to capture motion entanglement where both camera and
targets move freely in 3D; (2) SO-HOTA, a novel metric combining Dot Distance
with HOTA to mitigate the sensitivity of IoU-based metrics to small
displacements; and (3) a competitive MVA2025 challenge with 78 participants and
308 submissions, where the winning method achieved a 5.1x improvement over the
baseline. This work lays a foundation for advancing SMOT in UAV scenarios with
applications in bird strike avoidance, agriculture, fisheries, and ecological
monitoring.

</details>


### [38] [AnyCap Project: A Unified Framework, Dataset, and Benchmark for Controllable Omni-modal Captioning](https://arxiv.org/abs/2507.12841)
*Yiming Ren,Zhiqiang Lin,Yu Li,Gao Meng,Weiyun Wang,Junjie Wang,Zicheng Lin,Jifeng Dai,Yujiu Yang,Wenhai Wang,Ruihang Chu*

Main category: cs.CV

TL;DR: 该论文提出了AnyCap Project，通过模型、数据集和评测体系三个层面，全面提升多模态可控生成描述（captioning）的能力和评价方式。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态描述生成模型在精细化控制和可靠评测方面存在不足，无法灵活按照用户指令生成高质量caption，也缺乏相应的大规模高质量数据集和精细评测基准。

Method: 论文提出AnyCapModel（ACM），是一种轻量级、可插拔的框架，在无需重新训练基础模型的情况下增强caption生成的可控性和多样性。模型能够结合用户指令和不同模态特征对原始caption进行优化。同时，构建了AnyCapDataset（ACD），涵盖三种模态、28种用户指令类型，总计30万高质量数据，并设计了AnyCapEval评测体系，将内容和风格两部分分开独立评价。

Result: ACM能够对多种基础模型（包括GPT-4o等）在AnyCapEval评测体系上明显提升caption的内容准确性和风格契合度。其中，ACM-8B使GPT-4o内容得分提升45%，风格得分提升12%，在MIA-Bench和VidCapBench等公开基准上也取得了显著提升。

Conclusion: AnyCap Project为实现可控多模态caption提供了全面可用的技术框架、数据资源和可靠评测标准，在多种基准和真实应用中显示出明显优越性，为后续相关研究及应用转化打下重要基础。

Abstract: Controllable captioning is essential for precise multimodal alignment and
instruction following, yet existing models often lack fine-grained control and
reliable evaluation protocols. To address this gap, we present the AnyCap
Project, an integrated solution spanning model, dataset, and evaluation. We
introduce AnyCapModel (ACM), a lightweight plug-and-play framework that
enhances the controllability of existing foundation models for omni-modal
captioning without retraining the base model. ACM reuses the original captions
from base models while incorporating user instructions and modality features to
generate improved captions. To remedy the data scarcity in controllable
multimodal captioning, we build AnyCapDataset (ACD), covering three modalities,
28 user-instruction types, and 300\,k high-quality data entries. We further
propose AnyCapEval, a new benchmark that provides more reliable evaluation
metrics for controllable captioning by decoupling content accuracy and
stylistic fidelity. ACM markedly improves caption quality across a diverse set
of base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\'s content scores
by 45\% and style scores by 12\%, and it also achieves substantial gains on
widely used benchmarks such as MIA-Bench and VidCapBench.

</details>


### [39] [SEMT: Static-Expansion-Mesh Transformer Network Architecture for Remote Sensing Image Captioning](https://arxiv.org/abs/2507.12845)
*Khang Truong,Lam Pham,Hieu Tang,Jasmin Lampert,Martin Boyer,Son Phan,Truong Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的遥感图像描述生成方法，通过整合多种技术，在遥感图像数据集上取得了优于当前主流方法的效果。


<details>
  <summary>Details</summary>
Motivation: 遥感图像蕴含丰富信息，人类难以高效解读。自动生成图像描述有助于环境监测、灾害评估和城市规划等应用，推动遥感智能化发展。

Method: 设计了一种基于Transformer的网络架构，引入了Static Expansion、Memory-Augmented Self-Attention和Mesh Transformer等多项技术。模型在UCM-Caption与NWPU-Caption两个遥感图像描述数据集上进行了评测。

Result: 所提模型在大多数评估指标上均超越了现有主流方法，表明模型在图像自动描述任务中表现出色。

Conclusion: 本文的技术创新有效提升了遥感图像描述生成的性能，具备进一步在实际遥感系统中应用的潜力。

Abstract: Image captioning has emerged as a crucial task in the intersection of
computer vision and natural language processing, enabling automated generation
of descriptive text from visual content. In the context of remote sensing,
image captioning plays a significant role in interpreting vast and complex
satellite imagery, aiding applications such as environmental monitoring,
disaster assessment, and urban planning. This motivates us, in this paper, to
present a transformer based network architecture for remote sensing image
captioning (RSIC) in which multiple techniques of Static Expansion,
Memory-Augmented Self-Attention, Mesh Transformer are evaluated and integrated.
We evaluate our proposed models using two benchmark remote sensing image
datasets of UCM-Caption and NWPU-Caption. Our best model outperforms the
state-of-the-art systems on most of evaluation metrics, which demonstrates
potential to apply for real-life remote sensing image systems.

</details>


### [40] [Simulate, Refocus and Ensemble: An Attention-Refocusing Scheme for Domain Generalization](https://arxiv.org/abs/2507.12851)
*Ziyi Wang,Zhi Gao,Jin Chen,Qingjie Zhao,Xinxiao Wu,Jiebo Luo*

Main category: cs.CV

TL;DR: 本论文提出了一种新的域泛化方法SRE（模拟、重聚焦与集成），旨在通过增强CLIP模型对领域无关区域的关注，提高其在未见目标域上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的CLIP模型虽然具有强大的语义编码能力，但在跨域任务中往往难以准确聚焦于任务相关的领域无关区域，限制了在未见目标域的泛化性能。为了解决这一问题，论文希望通过优化注意力机制来缓解域偏移。

Method: 论文提出了一项三步式策略：1）通过对源域数据进行增强，模拟生成目标域数据，用于模拟真实的域偏移；2）采用注意力重聚焦策略，通过对比源域与模拟目标域的注意力图，指导模型关注领域无关的特征区域；3）利用集成学习方法，融合多个模型或视角，进一步提升模型对领域无关注意力图的捕捉能力。

Result: 在多个公开数据集上的实验结果显示，SRE方法在大多数任务中优于当前主流的域泛化方法，验证了该方法的有效性。

Conclusion: 通过模拟域偏移、注意力重聚焦以及集成策略，SRE能够显著提升CLIP在域泛化任务中的表现，为相关领域提供了新的思路和工具。

Abstract: Domain generalization (DG) aims to learn a model from source domains and
apply it to unseen target domains with out-of-distribution data. Owing to
CLIP's strong ability to encode semantic concepts, it has attracted increasing
interest in domain generalization. However, CLIP often struggles to focus on
task-relevant regions across domains, i.e., domain-invariant regions, resulting
in suboptimal performance on unseen target domains. To address this challenge,
we propose an attention-refocusing scheme, called Simulate, Refocus and
Ensemble (SRE), which learns to reduce the domain shift by aligning the
attention maps in CLIP via attention refocusing. SRE first simulates domain
shifts by performing augmentation on the source data to generate simulated
target domains. SRE then learns to reduce the domain shifts by refocusing the
attention in CLIP between the source and simulated target domains. Finally, SRE
utilizes ensemble learning to enhance the ability to capture domain-invariant
attention maps between the source data and the simulated target data. Extensive
experimental results on several datasets demonstrate that SRE generally
achieves better results than state-of-the-art methods. The code is available
at: https://github.com/bitPrincy/SRE-DG.

</details>


### [41] [SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation](https://arxiv.org/abs/2507.12857)
*Shiqi Huang,Shuting He,Huaiyuan Qin,Bihan Wen*

Main category: cs.CV

TL;DR: 该论文提出了一种用于遥感实例分割的开放词汇（open-vocabulary, OV）学习新方法，提升了模型对新类别和跨数据集的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前主流的遥感实例分割方法通常采用封闭词汇，无法有效识别新类别或适应不同数据集，这限制了其在实际地球观测中的应用。

Method: 作者提出了SCORE框架，通过融合多粒度场景上下文（区域级和全局级），同时增强视觉与文本特征。在方法上引入了区域感知整合（Region-Aware Integration）优化类别嵌入，并通过全局上下文适配（Global Context Adaptation）改善文本嵌入，使得分类器具备更强的表达能力和适应性。

Result: 作者在多个遥感数据集上构建了开放词汇分割新基准，SCORE方法实验结果优于现有主流方法，达到了SOTA水平。

Conclusion: SCORE为大规模、实际地理空间分析任务中的实例分割提供了一种强健、通用的解决方案，推动了遥感领域开放词汇实例分割的发展。

Abstract: Most existing remote sensing instance segmentation approaches are designed
for close-vocabulary prediction, limiting their ability to recognize novel
categories or generalize across datasets. This restricts their applicability in
diverse Earth observation scenarios. To address this, we introduce
open-vocabulary (OV) learning for remote sensing instance segmentation. While
current OV segmentation models perform well on natural image datasets, their
direct application to remote sensing faces challenges such as diverse
landscapes, seasonal variations, and the presence of small or ambiguous objects
in aerial imagery. To overcome these challenges, we propose $\textbf{SCORE}$
($\textbf{S}$cene $\textbf{C}$ontext matters in $\textbf{O}$pen-vocabulary
$\textbf{RE}$mote sensing instance segmentation), a framework that integrates
multi-granularity scene context, i.e., regional context and global context, to
enhance both visual and textual representations. Specifically, we introduce
Region-Aware Integration, which refines class embeddings with regional context
to improve object distinguishability. Additionally, we propose Global Context
Adaptation, which enriches naive text embeddings with remote sensing global
context, creating a more adaptable and expressive linguistic latent space for
the classifier. We establish new benchmarks for OV remote sensing instance
segmentation across diverse datasets. Experimental results demonstrate that,
our proposed method achieves SOTA performance, which provides a robust solution
for large-scale, real-world geospatial analysis. Our code is available at
https://github.com/HuangShiqi128/SCORE.

</details>


### [42] [WhoFi: Deep Person Re-Identification via Wi-Fi Channel Signal Encoding](https://arxiv.org/abs/2507.12869)
*Danilo Avola,Daniele Pannone,Dario Montagnini,Emad Emam*

Main category: cs.CV

TL;DR: 这篇论文提出了WhoFi，一种基于Wi-Fi信号的人体再识别新方法，通过深度神经网络有效获得个体生物特征，实现了在复杂环境下的准确身份识别。


<details>
  <summary>Details</summary>
Motivation: 在监控场景中，传统的人体再识别依赖视觉信息，但遇到光照不佳、遮挡、视角不佳时效果大打折扣。需要寻找不依赖视觉的新手段增强识别鲁棒性。

Method: 作者提出WhoFi方法，利用Wi-Fi信道状态信息（CSI）提取个体生物特征。通过一种模块化深度神经网络框架，包含基于Transformer的编码器，辅以in-batch negative损失函数，学习更鲁棒并具泛化能力的生物特征表达。

Result: 在NTU-Fi数据集上的实验表明，WhoFi取得与现有最优方法相当的识别效果，验证了基于Wi-Fi的人体再识别方案的有效性。

Conclusion: 本文展示了Wi-Fi信号在特定场景下可作为视觉的有力补充，WhoFi框架对提升监控中的人员识别准确性具有实际应用价值。

Abstract: Person Re-Identification is a key and challenging task in video surveillance.
While traditional methods rely on visual data, issues like poor lighting,
occlusion, and suboptimal angles often hinder performance. To address these
challenges, we introduce WhoFi, a novel pipeline that utilizes Wi-Fi signals
for person re-identification. Biometric features are extracted from Channel
State Information (CSI) and processed through a modular Deep Neural Network
(DNN) featuring a Transformer-based encoder. The network is trained using an
in-batch negative loss function to learn robust and generalizable biometric
signatures. Experiments on the NTU-Fi dataset show that our approach achieves
competitive results compared to state-of-the-art methods, confirming its
effectiveness in identifying individuals via Wi-Fi signals.

</details>


### [43] [HRSeg: High-Resolution Visual Perception and Enhancement for Reasoning Segmentation](https://arxiv.org/abs/2507.12883)
*Weihuang Lin,Yiwei Ma,Xiaoshuai Sun,Shuting He,Jiayi Ji,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出了HRSeg模型，通过高分辨率感知和高分辨率增强两个模块，实现更高精度和效率的推理分割任务。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割方法受限于视觉编码器的低分辨率预训练，提升分辨率通常带来较低性能提升和高昂计算成本，因此亟需一种既高效又能提高感知分辨率的新方法。

Method: HRSeg包含两个创新模块：1）高分辨率感知（HRP）：通过裁剪方式处理高分辨率图像，结合局部和全局特征，实现多粒度信息融合；2）高分辨率增强（HRE）：将高分辨率图像中的细粒度信息整合到mask特征中，提升与文本特征的对齐度，从而实现更精细的分割。

Result: 通过大量消融实验和在多个基准数据集上的全面测试，证明了HRSeg模型在推理分割任务中表现优越，有效提升了分割的精度和效率。

Conclusion: HRSeg能够高效地处理高分辨率感知问题，兼具性能提升和计算效率，为推理分割领域带来了新的进展。

Abstract: The reasoning segmentation task involves segmenting objects within an image
by interpreting implicit user instructions, which may encompass subtleties such
as contextual cues and open-world knowledge. Despite significant advancements
made by existing approaches, they remain constrained by low perceptual
resolution, as visual encoders are typically pre-trained at lower resolutions.
Furthermore, simply interpolating the positional embeddings of visual encoders
to enhance perceptual resolution yields only marginal performance improvements
while incurring substantial computational costs. To address this, we propose
HRSeg, an efficient model with high-resolution fine-grained perception. It
features two key innovations: High-Resolution Perception (HRP) and
High-Resolution Enhancement (HRE). The HRP module processes high-resolution
images through cropping, integrating local and global features for
multi-granularity quality. The HRE module enhances mask features by integrating
fine-grained information from high-resolution images, refining their alignment
with text features for precise segmentation. Extensive ablation studies
validate the effectiveness of our modules, while comprehensive experiments on
multiple benchmark datasets demonstrate HRSeg's superior performance.

</details>


### [44] [From Neck to Head: Bio-Impedance Sensing for Head Pose Estimation](https://arxiv.org/abs/2507.12884)
*Mengxi Liu,Lala Shakti Swarup Ray,Sizhen Bian,Ko Watanabe,Ankur Bhatt,Joanna Sorysz,Russel Torah,Bo Zhou,Paul Lukowicz*

Main category: cs.CV

TL;DR: NeckSense是一款基于生物阻抗感测的新型可穿戴头部姿态追踪系统，利用项链式干电极，并采用结合解剖先验的深度学习方法，取得了与视觉方法相当的追踪精度。


<details>
  <summary>Details</summary>
Motivation: 传统视觉或IMU方法的头部姿态追踪受限于视线遮挡、体积大和舒适性不足；本研究旨在探索一种无需视线、轻便、适用于日常佩戴且具有高精度的头部追踪方案。

Method: 系统利用嵌入于轻量项链中的多通道干式生物阻抗电极，获取头部旋转和肌肉活动下颈部组织阻抗变化，并通过注入人体解剖先验（如关节限制、自然旋转范围）的深度学习模型进行姿态估算。

Result: 在7名被试者上，以当前SOTA视觉追踪模型为基准，采用留一法交叉验证，NeckSense在多种头部运动下的平均点误差为25.9mm，性能接近主流视觉方法。

Conclusion: NeckSense证明了通过紧凑、无视线遮挡的生物阻抗可穿戴设备，可实现与视觉方法媲美的高精度头部姿态追踪，具备实际应用潜力。

Abstract: We present NeckSense, a novel wearable system for head pose tracking that
leverages multi-channel bio-impedance sensing with soft, dry electrodes
embedded in a lightweight, necklace-style form factor. NeckSense captures
dynamic changes in tissue impedance around the neck, which are modulated by
head rotations and subtle muscle activations. To robustly estimate head pose,
we propose a deep learning framework that integrates anatomical priors,
including joint constraints and natural head rotation ranges, into the loss
function design. We validate NeckSense on 7 participants using the current SOTA
pose estimation model as ground truth. Our system achieves a mean per-vertex
error of 25.9 mm across various head movements with a leave-one-person-out
cross-validation method, demonstrating that a compact, line-of-sight-free
bio-impedance wearable can deliver head-tracking performance comparable to SOTA
vision-based methods.

</details>


### [45] [VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning](https://arxiv.org/abs/2507.13348)
*Senqiao Yang,Junyi Li,Xin Lai,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉令牌压缩范式VisionThink，能够根据任务难度动态选择图片分辨率，从而在节省计算资源的同时保证模型性能。


<details>
  <summary>Details</summary>
Motivation: 虽然扩大量化视觉令牌数提升了视觉-语言模型性能，但实际很多任务并不需要大量高分辨率令牌。固定的压缩策略可能不适用于所有样本，因此需要一种更加灵活和智能的方法。

Method: VisionThink方法以低分辨率图像为起点，自动判断当前分辨率是否足够处理问题，若不足则输出特殊令牌请求高分辨率图像。采用强化学习和LLM-as-Judge策略，并设计奖励函数和惩罚机制，实现对视觉令牌压缩决策的动态优化。

Result: 实验证明，VisionThink在OCR等细粒度任务上表现优异，同时在简单任务中显著节省视觉令牌，提升效率。方法在各种VQA任务上均展现出优越性、有效性和高效性。

Conclusion: VisionThink提供了一种兼顾性能与效率的视觉令牌处理新范式，可根据任务动态调整分辨率，有效降低计算消耗，为视觉-语言模型的实用化和高效化提供了新的思路和工具。

Abstract: Recent advancements in vision-language models (VLMs) have improved
performance by increasing the number of visual tokens, which are often
significantly longer than text tokens. However, we observe that most real-world
scenarios do not require such an extensive number of visual tokens. While the
performance drops significantly in a small subset of OCR-related tasks, models
still perform accurately in most other general VQA tasks with only 1/4
resolution. Therefore, we propose to dynamically process distinct samples with
different resolutions, and present a new paradigm for visual token compression,
namely, VisionThink. It starts with a downsampled image and smartly decides
whether it is sufficient for problem solving. Otherwise, the model could output
a special token to request the higher-resolution image. Compared to existing
Efficient VLM methods that compress tokens using fixed pruning ratios or
thresholds, VisionThink autonomously decides whether to compress tokens case by
case. As a result, it demonstrates strong fine-grained visual understanding
capability on OCR-related tasks, and meanwhile saves substantial visual tokens
on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge
strategy to successfully apply RL to general VQA tasks. Moreover, we carefully
design a reward function and penalty mechanism to achieve a stable and
reasonable image resize call ratio. Extensive experiments demonstrate the
superiority, efficiency, and effectiveness of our method. Our code is available
at https://github.com/dvlab-research/VisionThink.

</details>


### [46] [Camera-based implicit mind reading by capturing higher-order semantic dynamics of human gaze within environmental context](https://arxiv.org/abs/2507.12889)
*Mengke Song,Yuge Xie,Qi Cui,Luming Li,Xinyu Liu,Guotao Wang,Chenglizhao Chen,Shanchen Pang*

Main category: cs.CV

TL;DR: 本文提出了一种基于摄像头的、无需用户主动配合的情感识别方法，通过结合注视模式、环境语义及时序动态来推断用户隐含的情感状态。该方法不依赖专用传感器，具备较高的通用性与低部署成本。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别方法主要依赖面部表情、语音或姿态等明确信号，但这些容易被掩饰且难以捕捉深层情感。此外，基于生理信号的方法需复杂传感器，限制自然行为，而基于视线的方法又多局限于静态分析，难以体现视线与环境的复杂互动。因此，需要一种既自然无需主动配合，又能捕捉深层次情感的识别手段。

Method: 该方法利用标准高清摄像头，非侵入式采集用户的眼部特征和头部运动。在无需特定硬件和用户配合的前提下，通过估计时间-空间上的注视轨迹，进一步建模注视的空间、语义与时间维度，融合环境信息深入分析用户的视线行为动态，从而识别情感状态。

Result: 所提方法可实时、持续、自动地推断用户情感，实现高通用性和低成本部署。同时，实验证明结合视线和环境动态能更好捕捉人类情感与环境的复杂关系，优于传统静态或信号单一的方法。

Conclusion: 基于摄像头的多维注视-环境动态分析能够显著提升情感识别的深度和准确性，为无需用户配合、在自然环境中实时监测情感状态提供了更优解决方案，对智能人机交互与用户体验有广泛应用前景。

Abstract: Emotion recognition,as a step toward mind reading,seeks to infer internal
states from external cues.Most existing methods rely on explicit signals-such
as facial expressions,speech,or gestures-that reflect only bodily responses and
overlook the influence of environmental context.These cues are often
voluntary,easy to mask,and insufficient for capturing deeper,implicit emotions.
Physiological signal-based approaches offer more direct access to internal
states but require complex sensors that compromise natural behavior and limit
scalability.Gaze-based methods typically rely on static fixation analysis and
fail to capture the rich,dynamic interactions between gaze and the
environment,and thus cannot uncover the deep connection between emotion and
implicit behavior.To address these limitations,we propose a novel
camera-based,user-unaware emotion recognition approach that integrates gaze
fixation patterns with environmental semantics and temporal dynamics.Leveraging
standard HD cameras,our method unobtrusively captures users'eye appearance and
head movements in natural settings-without the need for specialized hardware or
active user participation.From these visual cues,the system estimates gaze
trajectories over time and space, providing the basis for modeling the spatial,
semantic,and temporal dimensions of gaze behavior. This allows us to capture
the dynamic interplay between visual attention and the surrounding
environment,revealing that emotions are not merely physiological responses but
complex outcomes of human-environment interactions.The proposed approach
enables user-unaware,real-time,and continuous emotion recognition,offering high
generalizability and low deployment cost.

</details>


### [47] [LanePerf: a Performance Estimation Framework for Lane Detection](https://arxiv.org/abs/2507.12894)
*Yin Wu,Daniel Slieter,Ahmed Abouelazm,Christian Hubschneider,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 提出了一种新的无需标签的车道检测模型性能估计方法LanePerf，能更好适应不同环境，大幅提升估计准确率。


<details>
  <summary>Details</summary>
Motivation: 现有车道检测模型在新环境下易受域偏移影响，部署可靠性差。而传统评估方法需标注大量数据，成本高昂。因此，开发无需标注、能准确估计模型性能的方法具有实际意义。

Method: 1. 将五种图像分类中的性能估计方法改进并应用到车道检测模型，构建性能基线。
2. 针对现有方法仅依赖softmax或车道特征的局限，提出新框架LanePerf：结合图像特征和车道特征，利用预训练图像编码器和基于DeepSets的结构，能有效处理0车道和大域偏移场景。

Result: 在OpenLane数据集多种场景、天气、时间域偏移设定下，LanePerf在MAE（0.117）和Spearman相关系数（0.727）上均优于全部基线方法。

Conclusion: 提出的LanePerf框架实现了车道检测性能的无标签高精度估计，为ADAS和自动驾驶系统的安全高效测试提供理论与方法支持，具有推广价值。

Abstract: Lane detection is a critical component of Advanced Driver-Assistance Systems
(ADAS) and Automated Driving System (ADS), providing essential spatial
information for lateral control. However, domain shifts often undermine model
reliability when deployed in new environments. Ensuring the robustness and
safety of lane detection models typically requires collecting and annotating
target domain data, which is resource-intensive. Estimating model performance
without ground-truth labels offers a promising alternative for efficient
robustness assessment, yet remains underexplored in lane detection. While
previous work has addressed performance estimation in image classification,
these methods are not directly applicable to lane detection tasks. This paper
first adapts five well-performing performance estimation methods from image
classification to lane detection, building a baseline. Addressing the
limitations of prior approaches that solely rely on softmax scores or lane
features, we further propose a new Lane Performance Estimation Framework
(LanePerf), which integrates image and lane features using a pretrained image
encoder and a DeepSets-based architecture, effectively handling zero-lane
detection scenarios and large domain-shift cases. Extensive experiments on the
OpenLane dataset, covering diverse domain shifts (scenes, weather, hours),
demonstrate that our LanePerf outperforms all baselines, achieving a lower MAE
of 0.117 and a higher Spearman's rank correlation coefficient of 0.727. These
findings pave the way for robust, label-free performance estimation in ADAS,
supporting more efficient testing and improved safety in challenging driving
scenarios.

</details>


### [48] [Federated Learning for Commercial Image Sources](https://arxiv.org/abs/2507.12903)
*Shreyansh Jain,Koteswar Rao Jerripothula*

Main category: cs.CV

TL;DR: 本文提出了一个专为联邦学习设计的大型图像分类数据集，并创新性地提出了两种新的联邦学习算法（Fed-Cyclic 和 Fed-Star），在新数据集上均优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 虽然联邦学习在图像分类等领域有广泛应用，但缺乏专门设计用于这一任务的公开数据集，且现有算法在面对多源异构数据时性能有限，因此需要既有新数据集又有新的算法提升表现。

Method: 1. 构建了包含23,326张图片、覆盖31个类别、源自8个商用渠道的专用数据集。2. 提出Fed-Cyclic算法，让每个客户端顺次接收并更新前一个客户端的模型权重，形成循环结构；提出Fed-Star算法，每个客户端在聚合预处理后进行本地训练并和所有其他客户端权重交换，形成星状结构。3. 在新数据集上和其他已有方法进行性能对比实验。

Result: 实验表明，Fed-Cyclic和Fed-Star算法在提出的新数据集上均优于主流联邦学习基线算法，尤其在应对统计异质性方面表现突出。

Conclusion: 本文不仅填补了联邦学习用于图像分类领域缺少公开评测数据集的空白，还通过创新的算法设计显著提升了模型在多客户端、异构数据场景下的表现。

Abstract: Federated Learning is a collaborative machine learning paradigm that enables
multiple clients to learn a global model without exposing their data to each
other. Consequently, it provides a secure learning platform with
privacy-preserving capabilities. This paper introduces a new dataset containing
23,326 images collected from eight different commercial sources and classified
into 31 categories, similar to the Office-31 dataset. To the best of our
knowledge, this is the first image classification dataset specifically designed
for Federated Learning. We also propose two new Federated Learning algorithms,
namely Fed-Cyclic and Fed-Star. In Fed-Cyclic, a client receives weights from
its previous client, updates them through local training, and passes them to
the next client, thus forming a cyclic topology. In Fed-Star, a client receives
weights from all other clients, updates its local weights through
pre-aggregation (to address statistical heterogeneity) and local training, and
sends its updated local weights to all other clients, thus forming a star-like
topology. Our experiments reveal that both algorithms perform better than
existing baselines on our newly introduced dataset.

</details>


### [49] [AthleticsPose: Authentic Sports Motion Dataset on Athletic Field and Evaluation of Monocular 3D Pose Estimation Ability](https://arxiv.org/abs/2507.12905)
*Tomohiro Suzuki,Ryota Tanaka,Calvin Yeung,Keisuke Fujii*

Main category: cs.CV

TL;DR: 本文提出了AthleticsPose数据集，并展示了其在提升单目3D姿态估计算法在体育运动分析中的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 单目3D姿态估计作为运动分析的廉价替代方案具有潜力，但缺乏真实体育数据集和模型实际可靠性不明确限制了其应用。

Method: 作者采集了23名运动员在真实田径场地上进行各类运动项目时的动作数据，构建了AthleticsPose数据集，并以此训练3D姿态估计算法模型，与传统仿真运动数据集训练的基线模型进行对比分析。

Result: 在AthleticsPose数据集上训练的模型比用仿真数据集训练的基线模型具有大幅提升，MPJPE降低约75%；同时发现模型对拍摄视角和被试比例敏感。在运动学指标个案分析中，模型能较好捕捉个体化的膝关节角度差异，但在如膝驱速度等高速指标上存在预测偏差。

Conclusion: AthleticsPose数据集的引入显著提高了3D姿态估计在体育领域的准确性，并说明真实运动数据的重要性；同时指出单目3D姿态估计在交互细节和高速度动作上的局限性，为相关领域研究者提供了宝贵资源和明确的研究方向。

Abstract: Monocular 3D pose estimation is a promising, flexible alternative to costly
motion capture systems for sports analysis. However, its practical application
is hindered by two factors: a lack of realistic sports datasets and unclear
reliability for sports tasks. To address these challenges, we introduce the
AthleticsPose dataset, a new public dataset featuring ``real'' motions captured
from 23 athletes performing various athletics events on an athletic field.
Using this dataset, we trained a representative 3D pose estimation model and
performed a comprehensive evaluation. Our results show that the model trained
on AthleticsPose significantly outperforms a baseline model trained on an
imitated sports motion dataset, reducing MPJPE by approximately 75 %. These
results show the importance of training on authentic sports motion data, as
models based on imitated motions do not effectively transfer to real-world
motions. Further analysis reveals that estimation accuracy is sensitive to
camera view and subject scale. In case studies of kinematic indicators, the
model demonstrated the potential to capture individual differences in knee
angles but struggled with higher-speed metrics, such as knee-drive velocity,
due to prediction biases. This work provides the research community with a
valuable dataset and clarifies the potential and practical limitations of using
monocular 3D pose estimation for sports motion analysis. Our dataset, code, and
checkpoints are available at https://github.com/SZucchini/AthleticsPose.

</details>


### [50] [Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding With Large Language Models](https://arxiv.org/abs/2507.12916)
*Yifan Xu,Chao Zhang,Hanqi Jiang,Xiaoyan Wang,Ruifei Ma,Yiwei Li,Zihao Wu,Zeju Li,Xiangde Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Argus的3D多模态大模型框架，通过多视角图像和现有3D点云结合，更好地解决室内场景理解任务，提升了下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前的3D场景理解方法过度依赖3D点云数据，但在点云重建时容易出现信息丢失、细节变形等问题。特别是在场景中存在无纹理平面、重复图案、复杂结构物体时，这些问题尤为突出，多视角2D图像可以很好地补充这些不足，因此亟需一种能融合利用多模态信息的新框架。

Method: 提出了Argus框架，将文本指令、2D多视角图像和3D点云多模态信息作为输入，通过融合和集成多视角图像及相机位姿的信息，生成具有细致综合表征的3D场景特征嵌入，大大增强了大语言模型的三维理解能力。

Result: 大量实验表明，Argus方法在多项下游3D场景理解任务上均优于现有的3D大模型方法。

Conclusion: 将2D多视角图像融入3D多模态大模型能有效弥补纯点云方法的信息缺失，显著提升3D场景理解的效果，为大模型把握三维世界提供了有效路径。

Abstract: Advancements in foundation models have made it possible to conduct
applications in various downstream tasks. Especially, the new era has witnessed
a remarkable capability to extend Large Language Models (LLMs) for tackling
tasks of 3D scene understanding. Current methods rely heavily on 3D point
clouds, but the 3D point cloud reconstruction of an indoor scene often results
in information loss. Some textureless planes or repetitive patterns are prone
to omission and manifest as voids within the reconstructed 3D point clouds.
Besides, objects with complex structures tend to introduce distortion of
details caused by misalignments between the captured images and the dense
reconstructed point clouds. 2D multi-view images present visual consistency
with 3D point clouds and provide more detailed representations of scene
components, which can naturally compensate for these deficiencies. Based on
these insights, we propose Argus, a novel 3D multimodal framework that
leverages multi-view images for enhanced 3D scene understanding with LLMs. In
general, Argus can be treated as a 3D Large Multimodal Foundation Model
(3D-LMM) since it takes various modalities as input(text instructions, 2D
multi-view images, and 3D point clouds) and expands the capability of LLMs to
tackle 3D tasks. Argus involves fusing and integrating multi-view images and
camera poses into view-as-scene features, which interact with the 3D features
to create comprehensive and detailed 3D-aware scene embeddings. Our approach
compensates for the information loss while reconstructing 3D point clouds and
helps LLMs better understand the 3D world. Extensive experiments demonstrate
that our method outperforms existing 3D-LMMs in various downstream tasks.

</details>


### [51] [DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization](https://arxiv.org/abs/2507.12933)
*Dongyeun Lee,Jiwan Hur,Hyounguk Shon,Jae Young Lee,Junmo Kim*

Main category: cs.CV

TL;DR: 本文提出一种用于扩散模型的高效量化方法，显著降低计算成本，同时保有较高的图像生成质量，尤其在低比特宽（如W4A6、W4A8）下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面表现突出，但其高昂的计算开销限制了在资源受限环境中的部署。已有的量化方法未能充分处理扩散模型中的数据异常点，导致低比特宽下性能下降，亟需新的解决方案。

Method: 提出DMQ方法，结合了通道级等效缩放（LES）与通道级二次幂缩放（PTS）。LES通过优化缩放因子重分配权重与激活的量化难度，降低整体误差。PTS用于激活，针对高通道方差的层。还引入了自适应时间步加权，以重点优化初期去噪步骤，并采用投票算法提升PTS因子的选取稳定性。

Result: 在多项实验中，所提方法在W4A6、W4A8等低比特宽设定下，图像生成质量及模型稳定性明显优于前人方法，且保持计算效率。

Conclusion: DMQ是一种兼顾计算高效性和精度的新型扩散模型量化方法，适合实际部署，对提升模型在低资源环境下的可用性有重大意义。

Abstract: Diffusion models have achieved remarkable success in image generation but
come with significant computational costs, posing challenges for deployment in
resource-constrained environments. Recent post-training quantization (PTQ)
methods have attempted to mitigate this issue by focusing on the iterative
nature of diffusion models. However, these approaches often overlook outliers,
leading to degraded performance at low bit-widths. In this paper, we propose a
DMQ which combines Learned Equivalent Scaling (LES) and channel-wise
Power-of-Two Scaling (PTS) to effectively address these challenges. Learned
Equivalent Scaling optimizes channel-wise scaling factors to redistribute
quantization difficulty between weights and activations, reducing overall
quantization error. Recognizing that early denoising steps, despite having
small quantization errors, crucially impact the final output due to error
accumulation, we incorporate an adaptive timestep weighting scheme to
prioritize these critical steps during learning. Furthermore, identifying that
layers such as skip connections exhibit high inter-channel variance, we
introduce channel-wise Power-of-Two Scaling for activations. To ensure robust
selection of PTS factors even with small calibration set, we introduce a voting
algorithm that enhances reliability. Extensive experiments demonstrate that our
method significantly outperforms existing works, especially at low bit-widths
such as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image
generation quality and model stability. The code is available at
https://github.com/LeeDongYeun/dmq.

</details>


### [52] [A Deep-Learning Framework for Land-Sliding Classification from Remote Sensing Image](https://arxiv.org/abs/2507.12939)
*Hieu Tang,Truong Vo,Dong Pham,Toan Nguyen,Lam Pham,Truong Nguyen*

Main category: cs.CV

TL;DR: 本论文提出了一种结合卫星遥感影像和深度学习的新型滑坡检测框架，表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前自动化滑坡检测普遍采用深度学习技术，但在模型选择、性能最优与过拟合防控方面仍存在挑战。尤其是如何处理遥感数据中的类别不平衡对准确检测滑坡非常关键。

Method: 方法包括：1）结合在线与离线数据增强以平衡训练数据分布；2）采用EfficientNet_Large作为特征提取主干网络，提升特征鲁棒性；3）利用SVM分类器进行后处理，增强分类效果与类别平衡。

Result: 该模型在Zindi公开测试集上获得了0.8938的F1分数，显示出其优越的滑坡检测能力。

Conclusion: 有效的数据增强策略、强大的主干网络和合理的后处理方法的结合能够提升深度学习框架在遥感滑坡检测任务中的表现。

Abstract: The use of satellite imagery combined with deep learning to support automatic
landslide detection is becoming increasingly widespread. However, selecting an
appropriate deep learning architecture to optimize performance while avoiding
overfitting remains a critical challenge. To address these issues, we propose a
deep-learning based framework for landslide detection from remote sensing image
in this paper. The proposed framework presents an effective combination of the
online an offline data augmentation to tackle the imbalanced data, a backbone
EfficientNet\_Large deep learning model for extracting robust embedding
features, and a post-processing SVM classifier to balance and enhance the
classification performance. The proposed model achieved an F1-score of 0.8938
on the public test set of the Zindi challenge.

</details>


### [53] [Weakly Supervised Visible-Infrared Person Re-Identification via Heterogeneous Expert Collaborative Consistency Learning](https://arxiv.org/abs/2507.12942)
*Yafei Zhang,Lingqi Kong,Huafeng Li,Jie Wen*

Main category: cs.CV

TL;DR: 本文提出了一种仅依赖单模态身份标签的弱监督可见光-红外（VI）跨模态行人重识别方法，通过异构专家协同一致性学习框架，实现无交叉模态身份标签情况下的鲁棒跨模态识别。


<details>
  <summary>Details</summary>
Motivation: 现有VI跨模态ReID方法通常依赖于人工标注的跨模态身份标签，但实际中很难获取这类高质量标签，因此亟需只用单一模态标签就能进行跨模态识别的方法。

Method: 提出了异构专家协同一致性学习框架：先分别用各模态的带标签数据训练专属分类专家，这些专家作为异构预测器（对另一模态做身份预测）来建立跨模态身份对应关系，并通过跨模态关系融合机制精确集成不同专家的预测，实现协同和一致性优化。

Result: 在两个有挑战性的VI ReID数据集上实验，结果显示该方法在缺乏跨模态标签时依然能有效提升跨模态识别表现，优于现有弱监督方法。

Conclusion: 所提方法能够在仅有单模态标签条件下，大幅提升VI跨模态行人重识别的准确率，为实际难以获得跨模态标签的场景提供了有效解决方案。

Abstract: To reduce the reliance of visible-infrared person re-identification (ReID)
models on labeled cross-modal samples, this paper explores a weakly supervised
cross-modal person ReID method that uses only single-modal sample identity
labels, addressing scenarios where cross-modal identity labels are unavailable.
To mitigate the impact of missing cross-modal labels on model performance, we
propose a heterogeneous expert collaborative consistency learning framework,
designed to establish robust cross-modal identity correspondences in a weakly
supervised manner. This framework leverages labeled data from each modality to
independently train dedicated classification experts. To associate cross-modal
samples, these classification experts act as heterogeneous predictors,
predicting the identities of samples from the other modality. To improve
prediction accuracy, we design a cross-modal relationship fusion mechanism that
effectively integrates predictions from different experts. Under the implicit
supervision provided by cross-modal identity correspondences, collaborative and
consistent learning among the experts is encouraged, significantly enhancing
the model's ability to extract modality-invariant features and improve
cross-modal identity recognition. Experimental results on two challenging
datasets validate the effectiveness of the proposed method.

</details>


### [54] [Analysis of Image-and-Text Uncertainty Propagation in Multimodal Large Language Models with Cardiac MR-Based Applications](https://arxiv.org/abs/2507.12945)
*Yucheng Tang,Yunguan Fu,Weixi Yi,Yipei Wang,Daniel C. Alexander,Rhodri Davies,Yipeng Hu*

Main category: cs.CV

TL;DR: 本文提出了一种多模态不确定性传播模型（MUPM），有效地捕捉并刻画多模态大语言模型（MLLM）输入中由图像、文本及其联合带来的不确定性，并在临床中实现具体应用。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然能处理和集成多种类型的信息（如文本和图像），但输入模态间的不确定性关系、单一模态数据带来的不确定性及其在临床中的应用潜力尚未得到充分理解。因此，需要一种新方法来分解与量化这些不确定性，并推进其实际应用。

Method: 作者提出了一种基于不确定性传播的多模态不确定性传播模型（MUPM），能够刻画图像、文本以及图像-文本联合输入带来的不确定性关系。该方法在实际的临床数据（心脏核磁共振图像与数字健康档案）上进行训练和优化，通过少量样本即可实现鲁棒优化。

Result: 实验表明，训练好的MUPM不仅能在不同的数据分布下泛化，还能迁移到全新的下游任务。这种可迁移性归因于预训练的共享性、MLLMs的轻量调优以及MUPM的低维性质。此外，MUPM在估计不确定性所需的多模态数据较少，有效识别冗余因素，具有实用性和临床价值。

Conclusion: MUPM能够高效量化和分析多模态输入中的不同来源不确定性，实现跨任务的泛化与转移，并直接服务于临床疾病预测等任务中的数据不确定度估计与分析，并具有极高的实际应用价值。

Abstract: Multimodal large language models (MLLMs) can process and integrate
information from multimodality sources, such as text and images. However,
interrelationship among input modalities, uncertainties due to individual
uni-modal data and potential clinical applications following such an
uncertainty decomposition are yet fully understood in the context of
large-scale MLLMs. In this work, we propose a multimodal uncertainty
propagation model (MUPM) based on uncertainty propagation, to characterise the
relationship among the uncertainties arising from image-only, text-only, and
joint image-text variations in MLLM inputs. Using real clinical data consisting
of cardiac MR scans and digital health records, we describe that MUPMs can be
optimised robustly with a few samples. We then show that the fitted MUPMs are
generalisable across different input data distributions and, perhaps
surprisingly, across different downstream tasks. Such a transferability may be
explained by the shared pretraining, comparatively light MLLM fine-tuning,
along with the low-dimensional nature of the MUPMs. More importantly, this
learned transferability, quantifying the relationship between these
uncertainties, led to direct clinical applications in which uncertainties may
be estimated and thus analysed robustly for varying data or even a novel set of
cardiac disease prediction tasks. In addition, we show experimentally the
efficiency in multimodal data required for estimating the overall uncertainty
and its ability to identify redundant factors, both of which are considered
practical yet clinically useful applications with the proposed MUPMs. Codes are
available at https://github.com/yucheng722/MUPM.

</details>


### [55] [LoViC: Efficient Long Video Generation with Context Compression](https://arxiv.org/abs/2507.12952)
*Jiaxiu Jiang,Wenbo Li,Jingjing Ren,Yuping Qiu,Yong Guo,Xiaogang Xu,Han Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 本文提出了一种名为LoViC的新型DiT架构，通过灵活的自编码器和分段生成策略，有效提升了长时持续视频的生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前主流DiT在生成长时持续视频时会因自注意力机制的二次复杂度受限，现有妥协方案（如稀疏注意力、时序自回归方法）又会牺牲时序连贯性或扩展性。因此亟需探索一种既高效，又能保持长时连贯的视频生成方法。

Method: 该方法基于DiT架构，引入了名为FlexFormer的自编码器，能将视频与文本联合压缩为统一的隐变量表示，支持变长输入和线性可调压缩率。LoViC采用分段生成策略，每段视频生成时都能灵活结合上下文信息，通过position-aware机制增强时序建模能力，实现预测、反向预测与插值等多种生成功能。

Result: 在百万级开域视频数据上训练并广泛测试，LoViC在多项任务中的表现显示出其生成的长视频具备更强连贯性和多样性，且模型扩展性优异。

Conclusion: LoViC突破了以往DiT生成长视频的瓶颈，既兼顾了生成效率，又显著提升了视频的时序连贯性和灵活性，在多种视频生成任务中均表现突出。

Abstract: Despite recent advances in diffusion transformers (DiTs) for text-to-video
generation, scaling to long-duration content remains challenging due to the
quadratic complexity of self-attention. While prior efforts -- such as sparse
attention and temporally autoregressive models -- offer partial relief, they
often compromise temporal coherence or scalability. We introduce LoViC, a
DiT-based framework trained on million-scale open-domain videos, designed to
produce long, coherent videos through a segment-wise generation process. At the
core of our approach is FlexFormer, an expressive autoencoder that jointly
compresses video and text into unified latent representations. It supports
variable-length inputs with linearly adjustable compression rates, enabled by a
single query token design based on the Q-Former architecture. Additionally, by
encoding temporal context through position-aware mechanisms, our model
seamlessly supports prediction, retradiction, interpolation, and multi-shot
generation within a unified paradigm. Extensive experiments across diverse
tasks validate the effectiveness and versatility of our approach.

</details>


### [56] [cIDIR: Conditioned Implicit Neural Representation for Regularized Deformable Image Registration](https://arxiv.org/abs/2507.12953)
*Sidaty El Hadramy,Oumeymah Cherkaoui,Philippe C. Cattin*

Main category: cs.CV

TL;DR: 本文提出了一种名为cIDIR的变形图像配准新框架，减少了对正则化超参数的重复调优，大幅提升准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 在变形图像配准中，正则化参数的设定影响配准的平滑性和物理及解剖合理性，但现有学习框架需要多次昂贵的训练来调节这些参数，效率低下。

Method: 作者提出cIDIR框架，基于隐式神经表示（INR），能根据正则化超参数进行条件化配准。与传统方法需每次参数更改都重新训练不同，cIDIR在超参数先验分布上统一训练，并通过分割掩码进行观测优化。此外，cIDIR能建模连续可微的DVF，便于引入高阶正则化。

Result: 在DIR-LAB数据集上的实验结果表明，cIDIR具有较高的配准准确性与鲁棒性。

Conclusion: cIDIR框架有效解决了正则化超参数调节效率低及重复训练带来的计算开销问题，在保持优异配准精度的同时提升了算法灵活性和实用价值。

Abstract: Regularization is essential in deformable image registration (DIR) to ensure
that the estimated Deformation Vector Field (DVF) remains smooth, physically
plausible, and anatomically consistent. However, fine-tuning regularization
parameters in learning-based DIR frameworks is computationally expensive, often
requiring multiple training iterations. To address this, we propose cIDI, a
novel DIR framework based on Implicit Neural Representations (INRs) that
conditions the registration process on regularization hyperparameters. Unlike
conventional methods that require retraining for each regularization
hyperparameter setting, cIDIR is trained over a prior distribution of these
hyperparameters, then optimized over the regularization hyperparameters by
using the segmentations masks as an observation. Additionally, cIDIR models a
continuous and differentiable DVF, enabling seamless integration of advanced
regularization techniques via automatic differentiation. Evaluated on the
DIR-LAB dataset, $\operatorname{cIDIR}$ achieves high accuracy and robustness
across the dataset.

</details>


### [57] [FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers](https://arxiv.org/abs/2507.12956)
*Qiang Wang,Mengchao Wang,Fan Jiang,Yaqi Fan,Yonggang Qi,Mu Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为FantasyPortrait的新型方法，能够从静态图像生成高保真、具备丰富情感的单人或多人面部动画，并在各类实验中显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 当前利用几何先验（如人脸标记点或3DMM）的方法在跨人物表情迁移时易产生伪影，难以表现细腻情感；多人物动画中，不同角色的特征易相互干扰，导致效果不佳。本文旨在解决单人和多人表情动画的高质量生成难题。

Method: 提出了基于扩散Transformer的FantasyPortrait框架，引入情感增强学习策略，使用隐式表征捕捉不依赖身份的面部动态，同时设计了掩码交叉注意力机制，实现多人表情独立又协调的生成。此外，构建了Multi-Expr和ExprBench数据集及基准用于训练和评估。

Result: 在大量实验中，FantasyPortrait在定量指标和定性评价上均优于当前主流方法，尤其在跨人物重演和多人动画场景中表现突出。

Conclusion: FantasyPortrait有效克服了以往方法的伪影与多人物干扰难题，大幅提升了面部动画的保真度与情感表现力，并推动了相关数据集和评测体系的发展。

Abstract: Producing expressive facial animations from static images is a challenging
task. Prior methods relying on explicit geometric priors (e.g., facial
landmarks or 3DMM) often suffer from artifacts in cross reenactment and
struggle to capture subtle emotions. Furthermore, existing approaches lack
support for multi-character animation, as driving features from different
individuals frequently interfere with one another, complicating the task. To
address these challenges, we propose FantasyPortrait, a diffusion transformer
based framework capable of generating high-fidelity and emotion-rich animations
for both single- and multi-character scenarios. Our method introduces an
expression-augmented learning strategy that utilizes implicit representations
to capture identity-agnostic facial dynamics, enhancing the model's ability to
render fine-grained emotions. For multi-character control, we design a masked
cross-attention mechanism that ensures independent yet coordinated expression
generation, effectively preventing feature interference. To advance research in
this area, we propose the Multi-Expr dataset and ExprBench, which are
specifically designed datasets and benchmarks for training and evaluating
multi-character portrait animations. Extensive experiments demonstrate that
FantasyPortrait significantly outperforms state-of-the-art methods in both
quantitative metrics and qualitative evaluations, excelling particularly in
challenging cross reenactment and multi-character contexts. Our project page is
https://fantasy-amap.github.io/fantasy-portrait/.

</details>


### [58] [Demographic-aware fine-grained classification of pediatric wrist fractures](https://arxiv.org/abs/2507.12964)
*Ammar Ahmed,Ali Shariq Imran,Zenun Kastrati,Sher Muhammad Daudpota*

Main category: cs.CV

TL;DR: 本文提出了一种结合细粒度识别与元数据融合的方法，在极小数据集上提升手腕病理识别的准确率。


<details>
  <summary>Details</summary>
Motivation: 手腕疾病常见于儿童，诊断依赖专家且耗时。医学影像数据稀缺，单一模态（仅用图像）难以满足需求，因此需探索多模态和数据融合的新方法以提升诊断效率和准确性。

Method: 1. 将问题视为细粒度识别，捕捉常规CNN难以发现的影像细节；2. 将患者元数据（如性别、年龄等）与X光图像融合输入网络；3. 放弃基于ImageNet等粗粒度数据集的预训练权重，转而采用细粒度数据集上训练的权重。

Result: 结合细粒度识别与元数据融合方法，在极小数据集下诊断准确率提升2%；在较大以骨折为主的数据集上提升超10%。

Conclusion: 细粒度识别与元数据融合对手腕病理自动诊断有显著提升作用，尤其在受限数据环境下成效突出。

Abstract: Wrist pathologies are frequently observed, particularly among children who
constitute the majority of fracture cases. However, diagnosing these conditions
is time-consuming and requires specialized expertise. Computer vision presents
a promising avenue, contingent upon the availability of extensive datasets, a
notable challenge in medical imaging. Therefore, reliance solely on one
modality, such as images, proves inadequate, especially in an era of diverse
and plentiful data types. In this study, we employ a multifaceted approach to
address the challenge of recognizing wrist pathologies using an extremely
limited dataset. Initially, we approach the problem as a fine-grained
recognition task, aiming to identify subtle X-ray pathologies that conventional
CNNs overlook. Secondly, we enhance network performance by fusing patient
metadata with X-ray images. Thirdly, rather than pre-training on a
coarse-grained dataset like ImageNet, we utilize weights trained on a
fine-grained dataset. While metadata integration has been used in other medical
domains, this is a novel application for wrist pathologies. Our results show
that a fine-grained strategy and metadata integration improve diagnostic
accuracy by 2% with a limited dataset and by over 10% with a larger
fracture-focused dataset.

</details>


### [59] [RGB Pre-Training Enhanced Unobservable Feature Latent Diffusion Model for Spectral Reconstruction](https://arxiv.org/abs/2507.12967)
*Keli Deng,Jie Nie,Yuntao Qian*

Main category: cs.CV

TL;DR: 本论文提出了一种新的方法，通过利用RGB预训练扩散模型，重建比RGB图像信息更丰富的高光谱图像，实现光谱重建的重大突破。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像（HSI）包含比RGB图像更多的光谱信息，但实际观测时仅获得RGB，重建出不可观测的光谱特征困难。因此，如何有效从RGB重复构建与补充这些关键信息是迫切需要解决的问题。

Method: 方法将RGB预训练的潜空间扩散模型（RGB-LDM）扩展成不可观测特征LDM（ULDM），采用两阶段流程：第一阶段用谱不可观测特征自编码器（SpeUAE）学习并压缩不可观测特征至3D流形；第二阶段依次用SpeUAE与空间自编码器（SpaAE）编码光谱及空间结构，并通过ULDM结合RGB引导重建分布。

Result: 实验结果显示，该方法在光谱重建与后续重光任务中性能均达到当前最优（state-of-the-art）。

Conclusion: 利用RGB大量预训练知识和对不可观测特征的紧凑表达，可有效提升光谱重建质量，为相关应用提供新的高效解决方案。

Abstract: Spectral reconstruction (SR) is a crucial problem in image processing that
requires reconstructing hyperspectral images (HSIs) from the corresponding RGB
images. A key difficulty in SR is estimating the unobservable feature, which
encapsulates significant spectral information not captured by RGB imaging
sensors. The solution lies in effectively constructing the spectral-spatial
joint distribution conditioned on the RGB image to complement the unobservable
feature. Since HSIs share a similar spatial structure with the corresponding
RGB images, it is rational to capitalize on the rich spatial knowledge in RGB
pre-trained models for spectral-spatial joint distribution learning. To this
end, we extend the RGB pre-trained latent diffusion model (RGB-LDM) to an
unobservable feature LDM (ULDM) for SR. As the RGB-LDM and its corresponding
spatial autoencoder (SpaAE) already excel in spatial knowledge, the ULDM can
focus on modeling spectral structure. Moreover, separating the unobservable
feature from the HSI reduces the redundant spectral information and empowers
the ULDM to learn the joint distribution in a compact latent space.
Specifically, we propose a two-stage pipeline consisting of spectral structure
representation learning and spectral-spatial joint distribution learning to
transform the RGB-LDM into the ULDM. In the first stage, a spectral
unobservable feature autoencoder (SpeUAE) is trained to extract and compress
the unobservable feature into a 3D manifold aligned with RGB space. In the
second stage, the spectral and spatial structures are sequentially encoded by
the SpeUAE and the SpaAE, respectively. The ULDM is then acquired to model the
distribution of the coded unobservable feature with guidance from the
corresponding RGB images. Experimental results on SR and downstream relighting
tasks demonstrate that our proposed method achieves state-of-the-art
performance.

</details>


### [60] [Variance-Based Pruning for Accelerating and Compressing Trained Networks](https://arxiv.org/abs/2507.12988)
*Uranik Berisha,Jens Mehnert,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: 本文提出了一种新的结构化剪枝方法，能够在极小的微调下有效压缩大模型，并且最大限度保持原始性能。


<details>
  <summary>Details</summary>
Motivation: 现有大模型（如Vision Transformers）训练成本高，部署在资源受限硬件上存在延迟、算力和内存瓶颈。主流剪枝虽可减少资源需求，但为了恢复性能往往需要极其昂贵的再训练，降低了实用性。如何高效压缩且维持性能，成为部署的关键难题。

Method: 提出基于方差的结构化一次性剪枝方法。核心流程为：收集网络激活统计数据，据此判断剪除哪些神经元，同时将均值激活重新集成入模型以最大化性能保留。整个流程仅需极少微调。

Result: 在ImageNet-1k上，对DeiT-Base剪枝后直接保留70%以上原本性能，仅需10个epoch微调即可恢复至99%准确率。在此基础上，算力需求降低35%、模型体积减小36%，推理提速1.44倍。

Conclusion: Variance-Based Pruning方法显著提升了结构化剪枝的效率与实际部署价值，不需大规模再训练即可实现模型高效压缩及性能恢复，适合大模型在更广泛硬件条件下应用。

Abstract: Increasingly expensive training of ever larger models such as Vision
Transfomers motivate reusing the vast library of already trained
state-of-the-art networks. However, their latency, high computational costs and
memory demands pose significant challenges for deployment, especially on
resource-constrained hardware. While structured pruning methods can reduce
these factors, they often require costly retraining, sometimes for up to
hundreds of epochs, or even training from scratch to recover the lost accuracy
resulting from the structural modifications. Maintaining the provided
performance of trained models after structured pruning and thereby avoiding
extensive retraining remains a challenge. To solve this, we introduce
Variance-Based Pruning, a simple and structured one-shot pruning technique for
efficiently compressing networks, with minimal finetuning. Our approach first
gathers activation statistics, which are used to select neurons for pruning.
Simultaneously the mean activations are integrated back into the model to
preserve a high degree of performance. On ImageNet-1k recognition tasks, we
demonstrate that directly after pruning DeiT-Base retains over 70% of its
original performance and requires only 10 epochs of fine-tuning to regain 99%
of the original accuracy while simultaneously reducing MACs by 35% and model
size by 36%, thus speeding up the model by 1.44x.

</details>


### [61] [Differential-informed Sample Selection Accelerates Multimodal Contrastive Learning](https://arxiv.org/abs/2507.12998)
*Zihua Zhao,Feng Hong,Mengxi Chen,Pengyi Chen,Benyuan Liu,Jiangchao Yao,Ya Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的差分信息样本选择方法（DISSect），通过考察当前模型预测与历史模型预测之间的差异，自动筛选高质量样本，从而提升多模态对比学习的训练效率、减少噪声影响。实验表明DISSect优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态对比学习模型依赖大规模数据和高计算资源，现有高效样本选择方法要么离线依赖先验模型（冷启动难），要么仅在线依据当前模型（未有效剔除噪声对应）。因此需要新方法在提升效率的同时，准确区分噪声样本。

Method: 作者提出DISSect方法：比较当前模型和历史模型对同一样本的相关性预测差异，将这种差分信号作为样本质量判据，并据此动态选择可靠样本进行训练，减少噪声干扰。并进行了理论分析和系统实验验证。

Result: 在三个基准数据集和多个下游任务上，DISSect在训练加速和最终性能均优于最新的样本选择方法，显示了方法的有效性和鲁棒性。

Conclusion: DISSect无需强依赖先验模型，能高效识别并过滤噪声样本，显著提升多模态对比学习训练效率和泛化性能，具有较好理论与应用前景。

Abstract: The remarkable success of contrastive-learning-based multimodal models has
been greatly driven by training on ever-larger datasets with expensive compute
consumption. Sample selection as an alternative efficient paradigm plays an
important direction to accelerate the training process. However, recent
advances on sample selection either mostly rely on an oracle model to offline
select a high-quality coreset, which is limited in the cold-start scenarios, or
focus on online selection based on real-time model predictions, which has not
sufficiently or efficiently considered the noisy correspondence. To address
this dilemma, we propose a novel Differential-Informed Sample Selection
(DISSect) method, which accurately and efficiently discriminates the noisy
correspondence for training acceleration. Specifically, we rethink the impact
of noisy correspondence on contrastive learning and propose that the
differential between the predicted correlation of the current model and that of
a historical model is more informative to characterize sample quality. Based on
this, we construct a robust differential-based sample selection and analyze its
theoretical insights. Extensive experiments on three benchmark datasets and
various downstream tasks demonstrate the consistent superiority of DISSect over
current state-of-the-art methods. Source code is available at:
https://github.com/MediaBrain-SJTU/DISSect.

</details>


### [62] [Beyond Fully Supervised Pixel Annotations: Scribble-Driven Weakly-Supervised Framework for Image Manipulation Localization](https://arxiv.org/abs/2507.13018)
*Songlin Li,Guofeng Yu,Zhiqing Guo,Yunfeng Diao,Dan Ma,Gaobo Yang,Liejun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于涂鸦标注的新型弱监督图像篡改定位方法，并构建了首个涂鸦标注IML数据集，显著提升了检测性能，超越了现有全监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习图像篡改定位方法依赖于大规模像素级标注数据集，获取高质量标注成本高。虽然弱监督方法尝试用图像级标签减少标注成本，但由于监督信号有限，性能受限。作者希望引入更高效且能提升检测性能的弱监督标注方式。

Method: 作者重新以涂鸦（scribble）标注重标注主流IML数据集并构建了首个此类数据集，提出了基于涂鸦的弱监督IML新框架。方法包括引入结构一致性损失进行自监督学习，加强多尺度输入下一致性预测；提出先验感知特征调制模块（PFMM）引入篡改与真实区域先验，提升特征区分度；采用门控自适应融合模块（GAFM）引导模型关注篡改区域；以及信心感知的熵最小化损失动态抑制不可靠预测。

Result: 实验证明，该方法在分布内与分布外测试下平均性能均超过现有完全监督方法。

Conclusion: 引入涂鸦标注作为更高效的弱监督信号，结合新型架构与损失，有效提升了复杂场景下的图像篡改定位精度，为低成本高质量数据标注提供了新解决方案。

Abstract: Deep learning-based image manipulation localization (IML) methods have
achieved remarkable performance in recent years, but typically rely on
large-scale pixel-level annotated datasets. To address the challenge of
acquiring high-quality annotations, some recent weakly supervised methods
utilize image-level labels to segment manipulated regions. However, the
performance is still limited due to insufficient supervision signals. In this
study, we explore a form of weak supervision that improves the annotation
efficiency and detection performance, namely scribble annotation supervision.
We re-annotated mainstream IML datasets with scribble labels and propose the
first scribble-based IML (Sc-IML) dataset. Additionally, we propose the first
scribble-based weakly supervised IML framework. Specifically, we employ
self-supervised training with a structural consistency loss to encourage the
model to produce consistent predictions under multi-scale and augmented inputs.
In addition, we propose a prior-aware feature modulation module (PFMM) that
adaptively integrates prior information from both manipulated and authentic
regions for dynamic feature adjustment, further enhancing feature
discriminability and prediction consistency in complex scenes. We also propose
a gated adaptive fusion module (GAFM) that utilizes gating mechanisms to
regulate information flow during feature fusion, guiding the model toward
emphasizing potential tampered regions. Finally, we propose a confidence-aware
entropy minimization loss (${\mathcal{L}}_{ {CEM }}$). This loss dynamically
regularizes predictions in weakly annotated or unlabeled regions based on model
uncertainty, effectively suppressing unreliable predictions. Experimental
results show that our method outperforms existing fully supervised approaches
in terms of average performance both in-distribution and out-of-distribution.

</details>


### [63] [Resurrect Mask AutoRegressive Modeling for Efficient and Scalable Image Generation](https://arxiv.org/abs/2507.13032)
*Yi Xin,Le Zhuo,Qi Qin,Siqi Luo,Yuewen Cao,Bin Fu,Yangfan He,Hongsheng Li,Guangtao Zhai,Xiaohong Liu,Peng Gao*

Main category: cs.CV

TL;DR: 本文提出了一种改进的Masked AutoRegressive (MAR) 图像生成模型MaskGIL，实现了高质量图像生成且推理效率大幅提升。


<details>
  <summary>Details</summary>
Motivation: MAR模型推理高效但一直性能不如传统AR模型，作者希望提升MAR模型的生成质量，弥补其与AR模型的差距。

Method: 首先评估多种图像tokenizer以选择最佳方案。随后提出改进的Bidirectional LLaMA架构，将因果注意力替换为双向注意力，并引入2D RoPE，形成MaskGIL模型。

Result: 在ImageNet 256x256基准上，MaskGIL（111M-1.4B参数）以仅8步推理达到3.71的FID分数，与主流AR模型相当，而AR模型需256步。还发展出支持各种分辨率的文本驱动版MaskGIL和语音到图像的实时转换。

Conclusion: MaskGIL成功兼顾了生成质量与推理速度，对图像生成和多模态任务有广泛应用前景。

Abstract: AutoRegressive (AR) models have made notable progress in image generation,
with Masked AutoRegressive (MAR) models gaining attention for their efficient
parallel decoding. However, MAR models have traditionally underperformed when
compared to standard AR models. This study refines the MAR architecture to
improve image generation quality. We begin by evaluating various image
tokenizers to identify the most effective one. Subsequently, we introduce an
improved Bidirectional LLaMA architecture by replacing causal attention with
bidirectional attention and incorporating 2D RoPE, which together form our
advanced model, MaskGIL. Scaled from 111M to 1.4B parameters, MaskGIL achieves
a FID score of 3.71, matching state-of-the-art AR models in the ImageNet
256x256 benchmark, while requiring only 8 inference steps compared to the 256
steps of AR models. Furthermore, we develop a text-driven MaskGIL model with
775M parameters for generating images from text at various resolutions. Beyond
image generation, MaskGIL extends to accelerate AR-based generation and enable
real-time speech-to-image conversion. Our codes and models are available at
https://github.com/synbol/MaskGIL.

</details>


### [64] [Advancing Complex Wide-Area Scene Understanding with Hierarchical Coresets Selection](https://arxiv.org/abs/2507.13061)
*Jingyao Wang,Yiming Chen,Lingyu Si,Changwen Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种分层核心集选择（HCS）机制，用于提升视觉-语言模型（VLM）对复杂大范围场景的适应能力，通过挑选关键区域，实现无需额外微调的高效场景理解，且适用于各种VLM模型，在多项任务中取得了更优的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型在场景理解领域取得进展，但在面对未见过的复杂广域场景时，现有方法在适应性上依然存在不足。如何让VLMs能够快速、准确地适应并理解这些大规模新场景，是亟需解决的问题。

Method: 作者提出了一种层次化核心集选择（HCS）机制，通过引入一个理论上有保证的重要性函数，综合考虑区域的效用、代表性、鲁棒性和协同性，逐步筛选出最具价值的图像区域。在不需要模型微调的前提下，HCS以极少且可解释的区域辅助VLM快速理解不同规模的新场景，并可直接集成至各种VLM。

Result: 实验结果表明，HCS机制能够显著提升各种任务中VLM的表现，且具有良好的通用性和适用性，无论在小规模还是大规模场景下均优于现有方法。

Conclusion: HCS是一种高效、通用、易用的插件式增强机制，可以有效提升VLM在复杂新场景下的理解能力，为实际场景的视觉语言理解提供了有力方法。

Abstract: Scene understanding is one of the core tasks in computer vision, aiming to
extract semantic information from images to identify objects, scene categories,
and their interrelationships. Although advancements in Vision-Language Models
(VLMs) have driven progress in this field, existing VLMs still face challenges
in adaptation to unseen complex wide-area scenes. To address the challenges,
this paper proposes a Hierarchical Coresets Selection (HCS) mechanism to
advance the adaptation of VLMs in complex wide-area scene understanding. It
progressively refines the selected regions based on the proposed theoretically
guaranteed importance function, which considers utility, representativeness,
robustness, and synergy. Without requiring additional fine-tuning, HCS enables
VLMs to achieve rapid understandings of unseen scenes at any scale using
minimal interpretable regions while mitigating insufficient feature density.
HCS is a plug-and-play method that is compatible with any VLM. Experiments
demonstrate that HCS achieves superior performance and universality in various
tasks.

</details>


### [65] [Label-Consistent Dataset Distillation with Detector-Guided Refinement](https://arxiv.org/abs/2507.13074)
*Yawen Zou,Guang Li,Zi Wang,Chunzhi Gu,Chao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于检测器指导的扩散模型数据集蒸馏方法，通过识别并改进合成样本中的标签不一致和细节不足问题，显著提升生成数据的质量和多样性，并在验证集上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的数据集蒸馏方法生成的代理数据常存在标签不一致和图像细节缺失的问题，导致下游任务性能不足，需要提升生成样本的有效性和质量。

Method: 利用在原始数据集上预训练的检测器筛查出扩散模型生成的异常合成样本（如标签不一致或分类置信度低），为每个异常样本基于原型和标签生成多个候选样本，通过检测器置信度与与已合格样本的差异性联合选择最优候选，用于确保标签准确和类内多样性。

Result: 实验表明，本方法合成的图像代表性强、细节丰富、标签准确，多样性好，在验证集上取得了目前最优的性能。

Conclusion: 引入检测器指导和候选选择机制后，数据集蒸馏生成样本的质量和多样性得到显著提升，为后续高效数据集生成和利用提供了有效途径。

Abstract: Dataset distillation (DD) aims to generate a compact yet informative dataset
that achieves performance comparable to the original dataset, thereby reducing
demands on storage and computational resources. Although diffusion models have
made significant progress in dataset distillation, the generated surrogate
datasets often contain samples with label inconsistencies or insufficient
structural detail, leading to suboptimal downstream performance. To address
these issues, we propose a detector-guided dataset distillation framework that
explicitly leverages a pre-trained detector to identify and refine anomalous
synthetic samples, thereby ensuring label consistency and improving image
quality. Specifically, a detector model trained on the original dataset is
employed to identify anomalous images exhibiting label mismatches or low
classification confidence. For each defective image, multiple candidates are
generated using a pre-trained diffusion model conditioned on the corresponding
image prototype and label. The optimal candidate is then selected by jointly
considering the detector's confidence score and dissimilarity to existing
qualified synthetic samples, thereby ensuring both label accuracy and
intra-class diversity. Experimental results demonstrate that our method can
synthesize high-quality representative images with richer details, achieving
state-of-the-art performance on the validation set.

</details>


### [66] [Channel-wise Motion Features for Efficient Motion Segmentation](https://arxiv.org/abs/2507.13082)
*Riku Inoue,Masamitsu Tsuchiya,Yuji Yasui*

Main category: cs.CV

TL;DR: 本文提出了一种全新的运动特征表示方法，显著提升了运动分割效率，减少了模型参数量，并保持了与最新技术相当的精度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶等安全关键机器人应用需实时准确检测所有必要物体。现有运动分割方法虽然有效，但通常依赖多个子网络联合处理，计算代价高，难以满足实时性能需求。如何在保证精度的前提下降低计算开销，实现更高效的运动分割，成为亟需解决的问题。

Method: 作者提出Channel-wise Motion Features，这是一种基于cost-volume的运动特征表示方法。该方法通过提取特征图中每个实例的深度特征并捕捉场景三维运动信息，只需使用Pose Network即可实现，无需其他子网络（如深度、光流、场景流等），从而减少计算负担。

Result: 在KITTI数据集和Cityscapes的VCAS-Motion数据集上，提出方法的FPS约为最新方法的4倍，参数数量减少到约25%，同时保持了与现有最优模型相当的准确率。

Conclusion: 提出的方法实现了高效、低参数且高准确率的运动分割，适用于对实时性与资源消耗要求较高的安全关键机器人应用，推动了相关领域的技术进步。

Abstract: For safety-critical robotics applications such as autonomous driving, it is
important to detect all required objects accurately in real-time. Motion
segmentation offers a solution by identifying dynamic objects from the scene in
a class-agnostic manner. Recently, various motion segmentation models have been
proposed, most of which jointly use subnetworks to estimate Depth, Pose,
Optical Flow, and Scene Flow. As a result, the overall computational cost of
the model increases, hindering real-time performance.
  In this paper, we propose a novel cost-volume-based motion feature
representation, Channel-wise Motion Features. By extracting depth features of
each instance in the feature map and capturing the scene's 3D motion
information, it offers enhanced efficiency. The only subnetwork used to build
Channel-wise Motion Features is the Pose Network, and no others are required.
Our method not only achieves about 4 times the FPS of state-of-the-art models
in the KITTI Dataset and Cityscapes of the VCAS-Motion Dataset, but also
demonstrates equivalent accuracy while reducing the parameters to about 25$\%$.

</details>


### [67] [Decoupled PROB: Decoupled Query Initialization Tasks and Objectness-Class Learning for Open World Object Detection](https://arxiv.org/abs/2507.13085)
*Riku Inoue,Masamitsu Tsuchiya,Yuji Yasui*

Main category: cs.CV

TL;DR: 论文提出了一种新型的开放世界目标检测模型Decoupled PROB，在多个基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开放世界目标检测需要能发现未知目标并增量学习新类别，难点在于缺乏未知目标标注，现有方法存在伪标签依赖或目标性与分类预测冲突。

Method: 1. 提出Decoupled PROB模型。2. 引入早期终止目标性预测（ETOP）机制，在适当层终止目标性预测，缓解目标性与分类预测的冲突。3. 提出任务解耦查询初始化（TDQI），高效提取已知与未知目标的特征，并可无缝集成到DETR式开放世界检测模型。

Result: 在OWOD标准基准上进行了大量实验，Decoupled PROB在多项指标上均超越现有所有方法，显著提升性能。

Conclusion: Decoupled PROB通过结构创新有效解决了目标性与分类预测冲突，提升了开放世界目标检测能力，为相关领域提供了更优解决方案。

Abstract: Open World Object Detection (OWOD) is a challenging computer vision task that
extends standard object detection by (1) detecting and classifying unknown
objects without supervision, and (2) incrementally learning new object classes
without forgetting previously learned ones. The absence of ground truths for
unknown objects makes OWOD tasks particularly challenging. Many methods have
addressed this by using pseudo-labels for unknown objects. The recently
proposed Probabilistic Objectness transformer-based open-world detector (PROB)
is a state-of-the-art model that does not require pseudo-labels for unknown
objects, as it predicts probabilistic objectness. However, this method faces
issues with learning conflicts between objectness and class predictions.
  To address this issue and further enhance performance, we propose a novel
model, Decoupled PROB. Decoupled PROB introduces Early Termination of
Objectness Prediction (ETOP) to stop objectness predictions at appropriate
layers in the decoder, resolving the learning conflicts between class and
objectness predictions in PROB. Additionally, we introduce Task-Decoupled Query
Initialization (TDQI), which efficiently extracts features of known and unknown
objects, thereby improving performance. TDQI is a query initialization method
that combines query selection and learnable queries, and it is a module that
can be easily integrated into existing DETR-based OWOD models. Extensive
experiments on OWOD benchmarks demonstrate that Decoupled PROB surpasses all
existing methods across several metrics, significantly improving performance.

</details>


### [68] [DiffOSeg: Omni Medical Image Segmentation via Multi-Expert Collaboration Diffusion Model](https://arxiv.org/abs/2507.13087)
*Han Zhang,Xiangde Luo,Yong Chen,Kang Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于扩散模型的医学图像分割方法DiffOSeg，可以同时实现多专家共识驱动和个体偏好驱动的分割，在公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中普遍存在专家标注不一致的问题，现有方法要么只追求概率共识，要么仅保存专家个体风格，无法全面表达数据的多样性和专家意见的复杂性。因此，需要一种能够同时兼顾共识和个体偏好的分割框架。

Method: 提出了DiffOSeg，两阶段的扩散框架：第一阶段通过概率共识策略生成融合各专家意见的共识分割，第二阶段结合自适应提示词，实现专家个体偏好分割。

Result: 在LIDC-IDRI和NPC-170两个公开医学图像分割数据集上，DiffOSeg在所有评测指标上都超越了最新的先进方法。

Conclusion: DiffOSeg能够同时实现医学图像分割的专家共识和个体偏好表达，有效提升分割性能，具有应用和研究价值。

Abstract: Annotation variability remains a substantial challenge in medical image
segmentation, stemming from ambiguous imaging boundaries and diverse clinical
expertise. Traditional deep learning methods producing single deterministic
segmentation predictions often fail to capture these annotator biases. Although
recent studies have explored multi-rater segmentation, existing methods
typically focus on a single perspective -- either generating a probabilistic
``gold standard'' consensus or preserving expert-specific preferences -- thus
struggling to provide a more omni view. In this study, we propose DiffOSeg, a
two-stage diffusion-based framework, which aims to simultaneously achieve both
consensus-driven (combining all experts' opinions) and preference-driven
(reflecting experts' individual assessments) segmentation. Stage I establishes
population consensus through a probabilistic consensus strategy, while Stage II
captures expert-specific preference via adaptive prompts. Demonstrated on two
public datasets (LIDC-IDRI and NPC-170), our model outperforms existing
state-of-the-art methods across all evaluated metrics. Source code is available
at https://github.com/string-ellipses/DiffOSeg .

</details>


### [69] [GLAD: Generalizable Tuning for Vision-Language Models](https://arxiv.org/abs/2507.13089)
*Yuqi Peng,Pengfei Wang,Jianzhuang Liu,Shifeng Chen*

Main category: cs.CV

TL;DR: 提出了一种更通用、简单的视觉-语言模型下游任务微调方法GLAD，具有更强泛化能力和更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的prompt tuning方法在小样本场景中易过拟合且依赖复杂架构和超参数，限制了其泛用性。需要一种更简单、泛用、鲁棒的微调方法。

Method: 提出GLAD框架，用LoRA（低秩适应）方法进行微调，并引入基于梯度的正则化技术，有效引导优化过程，增加模型对数据分布变化的鲁棒性。

Result: 在15个基准数据集上，GLAD在新旧类别泛化、图像域泛化、跨数据集泛化等方面，优于已有的微调方法。

Conclusion: GLAD方法更通用、简洁且泛化能力更强，为视觉-语言模型少样本微调提供了有效解决方案。

Abstract: Pre-trained vision-language models, such as CLIP, show impressive zero-shot
recognition ability and can be easily transferred to specific downstream tasks
via prompt tuning, even with limited training data. However, existing prompt
tuning methods face two main challenges: (1) In few-shot scenarios, data
scarcity often leads to overfitting, making the model sensitive to changes in
the input domain. (2) To mitigate overfitting, these methods typically rely on
complex task-specific model architectures and sensitive hyperparameter tuning,
severely restricting their general applicability. To address these issues, we
propose a simpler and more general framework called GLAD (Generalizable LoRA
tuning with RegulArized GraDient). We show that merely applying LoRA achieves
performance in downstream tasks comparable to current state-of-the-art
prompt-based methods. While LoRA is effective and easy to use, it remains
susceptible to overfitting in few-shot learning scenarios. To mitigate this
risk, we introduce a gradient-based regularization technique. This technique
effectively steers the optimization trajectory, encouraging the model to find a
more stable parameter region that is robust to variations in data distribution.
Through extensive experiments conducted on 15 benchmark datasets, we
demonstrate that GLAD outperforms previous tuning approaches in terms of
base-to-novel class generalization, image domain generalization, and
cross-dataset generalization. The code will be publicly available.

</details>


### [70] [Deep Learning-Based Fetal Lung Segmentation from Diffusion-weighted MRI Images and Lung Maturity Evaluation for Fetal Growth Restriction](https://arxiv.org/abs/2507.13106)
*Zhennan Xiao,Katharine Brudkiewicz,Zhen Yuan,Rosalind Aughwane,Magdalena Sokolska,Joanna Chappell,Trevor Gaunt,Anna L. David,Andrew P. King,Andrew Melbourne*

Main category: cs.CV

TL;DR: 本研究提出了一种用于评估胎儿肺成熟度的全自动影像分析流程，结合了深度学习分割模型和组织特征参数提取，其自动化程度高且临床应用前景良好。


<details>
  <summary>Details</summary>
Motivation: 胎儿肺成熟度是预测新生儿结局和制定干预措施的重要指标，特别是在胎儿生长受限的妊娠中。当前基于IVIM（Intra-voxel incoherent motion）分析对胎儿肺发展的无创评估仍依赖人工分割，费时且限制了临床应用。

Method: 本研究开发了一个全自动流程，包括基于3D nnU-Net的胎肺分割模型，以及后续基于分割结果进行IVIM参数拟合以量化肺组织结构和灌注。使用手动分割作为金标准训练分割模型，并将自动分割和手工分割的参数拟合结果进行对比。

Result: 3D nnU-Net分割模型的平均Dice系数为82.14%，自动分割与手工分割在IVIM参数量化结果上无显著差异，说明自动方法与人工方法具有等同的准确度。

Conclusion: 实现了胎肺MRI成熟度评估的全自动化流程，提升了效率，有望为临床决策与新生儿管理提供无创、客观的辅助工具。

Abstract: Fetal lung maturity is a critical indicator for predicting neonatal outcomes
and the need for post-natal intervention, especially for pregnancies affected
by fetal growth restriction. Intra-voxel incoherent motion analysis has shown
promising results for non-invasive assessment of fetal lung development, but
its reliance on manual segmentation is time-consuming, thus limiting its
clinical applicability. In this work, we present an automated lung maturity
evaluation pipeline for diffusion-weighted magnetic resonance images that
consists of a deep learning-based fetal lung segmentation model and a
model-fitting lung maturity assessment. A 3D nnU-Net model was trained on
manually segmented images selected from the baseline frames of 4D
diffusion-weighted MRI scans. The segmentation model demonstrated robust
performance, yielding a mean Dice coefficient of 82.14%. Next, voxel-wise model
fitting was performed based on both the nnU-Net-predicted and manual lung
segmentations to quantify IVIM parameters reflecting tissue microstructure and
perfusion. The results suggested no differences between the two. Our work shows
that a fully automated pipeline is possible for supporting fetal lung maturity
assessment and clinical decision-making.

</details>


### [71] [R^2MoE: Redundancy-Removal Mixture of Experts for Lifelong Concept Learning](https://arxiv.org/abs/2507.13107)
*Xiaohan Guo,Yusong Cai,Zejia Liu,Zhengning Wang,Lili Pan,Hongliang Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为R^2MoE的高效参数混合专家框架，有效解决了生成式大模型在持续学习新视觉概念过程中的遗忘和参数膨胀问题，在多个实验中实现了更高的概念保真度和极大参数压缩。


<details>
  <summary>Details</summary>
Motivation: 当前生成式大模型在个人化和持续学习新视觉概念时，面临着灾难性遗忘（学习新知识会丢失旧知识）和模型参数快速膨胀两大难题。作者旨在研发一种既能持续学习又高效节省参数的方法，满足用户定制化需求。

Method: 提出了Redundancy-Removal Mixture of Experts（R^2MoE）框架：（1）设计了带路由蒸馏的混合专家结构，使专家具有概念特异性且保留门控路由能力，缓解遗忘问题；（2）提出逐层冗余专家移除策略，充分复用已有专家，压缩参数数量；（3）采用层次化局部注意力推理，减少新旧概念干扰。

Result: 在CustomConcept 101等数据集上，所提方法相较SOTA方法，图片概念保真度更高，遗忘率降低87.8%，参数减少63.3%。

Conclusion: R^2MoE在视觉概念持续学习任务上，兼具高效的参数使用和极低的遗忘，显著优于现有方法，适合未来个性化生成模型的发展。

Abstract: Enabling large-scale generative models to continuously learn new visual
concepts is essential for personalizing pre-trained models to meet individual
user preferences. Existing approaches for continual visual concept learning are
constrained by two fundamental challenges: catastrophic forgetting and
parameter expansion. In this paper, we propose Redundancy-Removal Mixture of
Experts (R^2MoE), a parameter-efficient framework for lifelong visual concept
learning that effectively learns new concepts while incurring minimal parameter
overhead. Our framework includes three key innovative contributions: First, we
propose a mixture-of-experts framework with a routing distillation mechanism
that enables experts to acquire concept-specific knowledge while preserving the
gating network's routing capability, thereby effectively mitigating
catastrophic forgetting. Second, we propose a strategy for eliminating
redundant layer-wise experts that reduces the number of expert parameters by
fully utilizing previously learned experts. Third, we employ a hierarchical
local attention-guided inference approach to mitigate interference between
generated visual concepts. Extensive experiments have demonstrated that our
method generates images with superior conceptual fidelity compared to the
state-of-the-art (SOTA) method, achieving an impressive 87.8\% reduction in
forgetting rates and 63.3\% fewer parameters on the CustomConcept 101 dataset.
Our code is available at {https://github.com/learninginvision/R2MoE}

</details>


### [72] [3DKeyAD: High-Resolution 3D Point Cloud Anomaly Detection via Keypoint-Guided Point Clustering](https://arxiv.org/abs/2507.13110)
*Zi Wang,Katsuya Hotta,Koichiro Kamide,Yawen Zou,Chao Zhang,Jun Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于配准的3D点云异常检测方法，结合多原型对齐与分簇差异分析，实现了精确的异常定位，并在真实数据集上取得了最新最优性能。


<details>
  <summary>Details</summary>
Motivation: 高分辨率3D点云在工业检测中能有效发现微小结构异常，但其高密度和不规则性导致计算量大、对空间错位敏感且难以捕获局部差异，因此亟需高效、精准的异常检测方法。

Method: 方法首先将每个测试样本与多个正常原型进行配准，实现直接结构比较。随后对点云分簇，在每簇内部将测试样本与原型的特征进行相似度计算。与传统随机选取簇中心不同，本文采用基于关键点的策略，以几何信息丰富的点为中心，提升聚类效果和距离比较的稳定性。

Result: 在Real3D-AD基准上进行的实验显示，该方法无论在物体级还是点级的异常检测都取得了当前最优结果，即使只用原始特征也能表现出色。

Conclusion: 所提方法在3D点云异常检测任务中表现优异，能够高效、精确地实现异常定位，对工业检测等领域具有实际应用价值。

Abstract: High-resolution 3D point clouds are highly effective for detecting subtle
structural anomalies in industrial inspection. However, their dense and
irregular nature imposes significant challenges, including high computational
cost, sensitivity to spatial misalignment, and difficulty in capturing
localized structural differences. This paper introduces a registration-based
anomaly detection framework that combines multi-prototype alignment with
cluster-wise discrepancy analysis to enable precise 3D anomaly localization.
Specifically, each test sample is first registered to multiple normal
prototypes to enable direct structural comparison. To evaluate anomalies at a
local level, clustering is performed over the point cloud, and similarity is
computed between features from the test sample and the prototypes within each
cluster. Rather than selecting cluster centroids randomly, a keypoint-guided
strategy is employed, where geometrically informative points are chosen as
centroids. This ensures that clusters are centered on feature-rich regions,
enabling more meaningful and stable distance-based comparisons. Extensive
experiments on the Real3D-AD benchmark demonstrate that the proposed method
achieves state-of-the-art performance in both object-level and point-level
anomaly detection, even using only raw features.

</details>


### [73] [Leveraging Language Prior for Infrared Small Target Detection](https://arxiv.org/abs/2507.13113)
*Pranav Singh,Pravendra Singh*

Main category: cs.CV

TL;DR: 本文提出了一种将语言先验引入红外小目标检测（IRSTD）的多模态新方法，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统的IRSTD方法和数据集仅依赖图像模态，受限于小目标尺寸小、分布稀疏、背景模糊等特点，检测难度大，且缺少多模态（图像+文本）数据限制了进一步进步。

Method: 本文提出了一种融合语言先验的新颖多模态IRSTD框架，通过GPT-4视觉模型生成目标位置信息的文本描述，引入语言引导的注意力权重以增强小目标检测能力。此外，扩展并整理了包含图像和文本的多模态红外小目标检测数据集（基于IRSTD-1K和NUDT-SIRST），并在多个数据子集上进行实验和消融分析。

Result: 所提方法在NUAA-SIRST与IRSTD-1k（LangIR数据集）子集上表现优异，IoU、nIoU、Pd、Fa等指标相较现有最优方法提升显著，最大提升达113.43%。

Conclusion: 多模态融合、特别是引入语言先验可以有效提升红外小目标检测能力，本文方法为该领域带来新的研究方向和显著性能提升。

Abstract: IRSTD (InfraRed Small Target Detection) detects small targets in infrared
blurry backgrounds and is essential for various applications. The detection
task is challenging due to the small size of the targets and their sparse
distribution in infrared small target datasets. Although existing IRSTD methods
and datasets have led to significant advancements, they are limited by their
reliance solely on the image modality. Recent advances in deep learning and
large vision-language models have shown remarkable performance in various
visual recognition tasks. In this work, we propose a novel multimodal IRSTD
framework that incorporates language priors to guide small target detection. We
leverage language-guided attention weights derived from the language prior to
enhance the model's ability for IRSTD, presenting a novel approach that
combines textual information with image data to improve IRSTD capabilities.
Utilizing the state-of-the-art GPT-4 vision model, we generate text
descriptions that provide the locations of small targets in infrared images,
employing careful prompt engineering to ensure improved accuracy. Due to the
absence of multimodal IR datasets, existing IRSTD methods rely solely on image
data. To address this shortcoming, we have curated a multimodal infrared
dataset that includes both image and text modalities for small target
detection, expanding upon the popular IRSTD-1k and NUDT-SIRST datasets. We
validate the effectiveness of our approach through extensive experiments and
comprehensive ablation studies. The results demonstrate significant
improvements over the state-of-the-art method, with relative percentage
differences of 9.74%, 13.02%, 1.25%, and 67.87% in IoU, nIoU, Pd, and Fa on the
NUAA-SIRST subset, and 4.41%, 2.04%, 2.01%, and 113.43% on the IRSTD-1k subset
of the LangIR dataset, respectively.

</details>


### [74] [RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects in Remote Sensing Images](https://arxiv.org/abs/2507.13120)
*Xiaozheng Jiang,Wei Zhang,Xuerui Mao*

Main category: cs.CV

TL;DR: 本文提出了一种新模型RS-TinyNet，显著提升了遥感图像中微小目标检测的精度。通过引入多阶段特征融合、显著性建模和特征完整性重建，该方法在多个公开数据集上均优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 由于微小目标在遥感图像中空间信息极为有限、特征表达弱且目标分布密集，使得主流目标检测器表现不佳，因此亟需新的方法来提升检测精度。

Method: RS-TinyNet包括三个主要创新：（1）多维协同注意力（MDCA）模块提升微小目标的显著性；（2）辅助可逆分支（ARB）保持信息流完整；（3）逐步融合检测头（PFDH）模块融合多层特征以弥合语义鸿沟和细节损失。整体为多阶段特征融合与增强结构。

Result: 在公开遥感数据集AI-TOD上，RS-TinyNet比现有最优检测器提升了4.0% AP和6.5% AP75，在DIOR基准数据集上的评估也验证了其在复杂遥感场景下的卓越表现。

Conclusion: 多阶段特征融合和增强策略在提升复杂遥感环境下微小目标检测任务的效果方面非常有效，RS-TinyNet为该领域提供了高效且实用的解决方案。

Abstract: Detecting tiny objects in remote sensing (RS) imagery has been a
long-standing challenge due to their extremely limited spatial information,
weak feature representations, and dense distributions across complex
backgrounds. Despite numerous efforts devoted, mainstream detectors still
underperform in such scenarios. To bridge this gap, we introduce RS-TinyNet, a
multi-stage feature fusion and enhancement model explicitly tailored for RS
tiny object detection in various RS scenarios. RS-TinyNet comes with two novel
designs: tiny object saliency modeling and feature integrity reconstruction.
Guided by these principles, we design three step-wise feature enhancement
modules. Among them, the multi-dimensional collaborative attention (MDCA)
module employs multi-dimensional attention to enhance the saliency of tiny
objects. Additionally, the auxiliary reversible branch (ARB) and a progressive
fusion detection head (PFDH) module are introduced to preserve information flow
and fuse multi-level features to bridge semantic gaps and retain structural
detail. Comprehensive experiments on public RS dataset AI-TOD show that our
RS-TinyNet surpasses existing state-of-the-art (SOTA) detectors by 4.0% AP and
6.5% AP75. Evaluations on DIOR benchmark dataset further validate its superior
detection performance in diverse RS scenarios. These results demonstrate that
the proposed multi-stage feature fusion strategy offers an effective and
practical solution for tiny object detection in complex RS environments.

</details>


### [75] [Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models](https://arxiv.org/abs/2507.13162)
*Arian Mousakhan,Sudhanshu Mittal,Silvio Galesso,Karim Farid,Thomas Brox*

Main category: cs.CV

TL;DR: 本文针对现有自动驾驶世界模型在长时序生成和复杂场景泛化上的不足，提出了一种更高效、无需附加传感器即可取得领先性能的新模型。主要研究了连续模型与离散token模型的对比，结果支持连续自回归模型。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的世界模型在面对长时间预测及复杂路况（如转弯、城市交通）时表现不佳，现有方法通常依赖大量参数或辅助传感器（如地图、深度、多摄像头），导致实现复杂且泛化有限，因此需要更简洁高效、泛化性强的模型。

Method: 作者提出了一种仅使用基础设计选择、无需附加监督或传感器的新型世界模型，该模型包含469M参数，并在280小时视频数据上训练。研究中还设计了可兼容两种方式的混合tokenizer，对离散token模型与基于流匹配的连续模型进行了直接对比。

Result: 新模型在包括转弯和城市交通等复杂场景下表现突出，取得了SOTA（最先进）性能。通过混合tokenizer实验，证明连续自回归模型比基于离散token的模型更鲁棒、灵活和强大。

Conclusion: 提出的连续自回归世界模型，不仅实现了高性能和简化设计，还优于离散token模型，在复杂自动驾驶场景下更具优势。代码与模型等已公开，有利于业界应用和后续研究。

Abstract: Existing world models for autonomous driving struggle with long-horizon
generation and generalization to challenging scenarios. In this work, we
develop a model using simple design choices, and without additional supervision
or sensors, such as maps, depth, or multiple cameras. We show that our model
yields state-of-the-art performance, despite having only 469M parameters and
being trained on 280h of video data. It particularly stands out in difficult
scenarios like turning maneuvers and urban traffic. We test whether discrete
token models possibly have advantages over continuous models based on flow
matching. To this end, we set up a hybrid tokenizer that is compatible with
both approaches and allows for a side-by-side comparison. Our study concludes
in favor of the continuous autoregressive model, which is less brittle on
individual design choices and more powerful than the model built on discrete
tokens. Code, models and qualitative results are publicly available at
https://lmb-freiburg.github.io/orbis.github.io/.

</details>


### [76] [Synthesizing Reality: Leveraging the Generative AI-Powered Platform Midjourney for Construction Worker Detection](https://arxiv.org/abs/2507.13221)
*Hongyang Zhao,Tianyu Liang,Sina Davari,Daeho Kim*

Main category: cs.CV

TL;DR: 本文提出了一种利用生成式AI（Midjourney）合成建筑工人图像数据以增强深度神经网络（DNN）检测能力的新方法。通过生成和标注大量高多样性合成图像，有效应对了现实中数据稀缺的问题。实验显示在真实数据集上取得较好表现，在合成数据集上则表现极佳，揭示了生成式AI在数据增强中的潜力和不足。


<details>
  <summary>Details</summary>
Motivation: 建筑领域用于深度学习的视觉数据存在数量与多样性不足，影响模型精度和泛化能力。本研究旨在解决训练数据稀缺这一现实难题。

Method: 采用Midjourney生成式AI，通过设计3000种不同的提示词生成12000张高真实度和多样性的合成图像，并进行人工标注，作为DNN训练集。随后在真实的建筑工地图像数据集上进行评估。

Result: 在真实建筑工地图片测试集上，模型在IoU阈值为0.5和0.5-0.95时分别取得0.937和0.642的平均精度（AP）；在合成数据上则更高，分别达到0.994和0.919。

Conclusion: 生成式AI合成图像作为训练数据增强手段具有显著潜力，能大幅缓解数据不足导致的模型泛化问题。然而，在实际应用中仍面临合成与真实数据的性能差距，需进一步研究其泛化能力及不足之处。

Abstract: While recent advancements in deep neural networks (DNNs) have substantially
enhanced visual AI's capabilities, the challenge of inadequate data diversity
and volume remains, particularly in construction domain. This study presents a
novel image synthesis methodology tailored for construction worker detection,
leveraging the generative-AI platform Midjourney. The approach entails
generating a collection of 12,000 synthetic images by formulating 3000
different prompts, with an emphasis on image realism and diversity. These
images, after manual labeling, serve as a dataset for DNN training. Evaluation
on a real construction image dataset yielded promising results, with the model
attaining average precisions (APs) of 0.937 and 0.642 at
intersection-over-union (IoU) thresholds of 0.5 and 0.5 to 0.95, respectively.
Notably, the model demonstrated near-perfect performance on the synthetic
dataset, achieving APs of 0.994 and 0.919 at the two mentioned thresholds.
These findings reveal both the potential and weakness of generative AI in
addressing DNN training data scarcity.

</details>


### [77] [Leveraging Pre-Trained Visual Models for AI-Generated Video Detection](https://arxiv.org/abs/2507.13224)
*Keerthi Veeramachaneni,Praveen Tirupattur,Amrit Singh Bedi,Mubarak Shah*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，用于检测AI生成的视频内容，主要利用预训练视觉模型提取的特征，无需训练全新模型，仅通过一个简单的线性分类层即可实现高效检测，并在公开数据集上取得90%以上的检测精度。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成视觉内容质量的提升，生成内容与真实内容变得难以区分，特别是在视频领域，因此需要更加通用有效的检测方法来对抗虚假信息、保护隐私和安全。现有检测方法多聚焦于DeepFake人脸视频，对于其他生成视频的检测能力有限。

Method: 该方法利用在真实视觉内容上训练的预训练视觉模型提取视频特征，这些特征包含可区分真实和生成视频的信号。通过直接使用这些特征和一个简单的线性分类器，不需要复杂的模型训练即可实现高效检测。

Result: 构建的VID-AID数据集包含大约10,000个AI生成视频和4,000个真实视频，实验显示该方法的平均检测准确率高于90%。

Conclusion: 该方法简单高效，能够广泛应用于检测各类AI生成视频。研究团队计划公开代码、预训练模型及数据集，以支持相关领域的后续研究。

Abstract: Recent advances in Generative AI (GenAI) have led to significant improvements
in the quality of generated visual content. As AI-generated visual content
becomes increasingly indistinguishable from real content, the challenge of
detecting the generated content becomes critical in combating misinformation,
ensuring privacy, and preventing security threats. Although there has been
substantial progress in detecting AI-generated images, current methods for
video detection are largely focused on deepfakes, which primarily involve human
faces. However, the field of video generation has advanced beyond DeepFakes,
creating an urgent need for methods capable of detecting AI-generated videos
with generic content. To address this gap, we propose a novel approach that
leverages pre-trained visual models to distinguish between real and generated
videos. The features extracted from these pre-trained models, which have been
trained on extensive real visual content, contain inherent signals that can
help distinguish real from generated videos. Using these extracted features, we
achieve high detection performance without requiring additional model training,
and we further improve performance by training a simple linear classification
layer on top of the extracted features. We validated our method on a dataset we
compiled (VID-AID), which includes around 10,000 AI-generated videos produced
by 9 different text-to-video models, along with 4,000 real videos, totaling
over 7 hours of video content. Our evaluation shows that our approach achieves
high detection accuracy, above 90% on average, underscoring its effectiveness.
Upon acceptance, we plan to publicly release the code, the pre-trained models,
and our dataset to support ongoing research in this critical area.

</details>


### [78] [Efficient Adaptation of Pre-trained Vision Transformer underpinned by Approximately Orthogonal Fine-Tuning Strategy](https://arxiv.org/abs/2507.13260)
*Yiting Yang,Hao Luo,Yuan Sun,Qingsen Yan,Haokui Zhang,Wei Dong,Guoqing Wang,Peng Wang,Yang Yang,Hengtao Shen*

Main category: cs.CV

TL;DR: 本文提出了一种新的低秩权重矩阵结构，增强了参数高效微调ViT模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 主流的参数高效微调方法（如LoRA/Adapter）只在下/上投影矩阵学习新参数，但这些矩阵的向量缺乏预训练主干参数中的近似正交性，而正交性有助于提升模型的泛化能力。作者想解决：如果下/上投影矩阵也具备正交性，能否提升ViT的泛化？

Method: 提出了约正交微调（AOFT）策略，通过一个可学习向量生成近似正交的下/上投影矩阵，使其具备与主干相似的正交性。

Result: 在多个图像分类下游任务上，提出的AOFT表现优异，性能与主流方法持平或更好。

Conclusion: 将近似正交性引入低秩微调矩阵后，ViT微调模型的泛化能力和下游表现进一步提升，验证了该结构的有效性。

Abstract: A prevalent approach in Parameter-Efficient Fine-Tuning (PEFT) of pre-trained
Vision Transformers (ViT) involves freezing the majority of the backbone
parameters and solely learning low-rank adaptation weight matrices to
accommodate downstream tasks. These low-rank matrices are commonly derived
through the multiplication structure of down-projection and up-projection
matrices, exemplified by methods such as LoRA and Adapter. In this work, we
observe an approximate orthogonality among any two row or column vectors within
any weight matrix of the backbone parameters; however, this property is absent
in the vectors of the down/up-projection matrices. Approximate orthogonality
implies a reduction in the upper bound of the model's generalization error,
signifying that the model possesses enhanced generalization capability. If the
fine-tuned down/up-projection matrices were to exhibit this same property as
the pre-trained backbone matrices, could the generalization capability of
fine-tuned ViTs be further augmented? To address this question, we propose an
Approximately Orthogonal Fine-Tuning (AOFT) strategy for representing the
low-rank weight matrices. This strategy employs a single learnable vector to
generate a set of approximately orthogonal vectors, which form the
down/up-projection matrices, thereby aligning the properties of these matrices
with those of the backbone. Extensive experimental results demonstrate that our
method achieves competitive performance across a range of downstream image
classification tasks, confirming the efficacy of the enhanced generalization
capability embedded in the down/up-projection matrices.

</details>


### [79] [DiffClean: Diffusion-based Makeup Removal for Accurate Age Estimation](https://arxiv.org/abs/2507.13292)
*Ekta Balkrishna Gavas,Chinmay Hegde,Nasir Memon,Sudipta Banerjee*

Main category: cs.CV

TL;DR: 本文提出了一种名为DiffClean的防妆攻击方法，通过基于文本引导的扩散模型，去除面部妆容对年龄估计和人脸验证造成的干扰。


<details>
  <summary>Details</summary>
Motivation: 传统年龄估计方法在用户化妆后容易被欺骗，导致未成年人可以绕过年龄验证，带来未成年人保护隐患。现有方法在处理妆容攻击时效果有限，因此需要寻求更有效的防护措施。

Method: 作者设计了一种基于文本引导的扩散模型DiffClean，用于擦除人脸图像中的妆容痕迹，使后续的年龄估计与人脸验证模型能获得更准确的结果。DiffClean可在诸如模拟妆容和真实妆容数据上工作。

Result: DiffClean方法在妆容攻击下提升了年龄估计（未成年/成年人识别准确率提高4.8%）和人脸验证（在FMR=0.01%条件下TMR提升8.9%）性能，超越了现有的对比方法。

Conclusion: DiffClean能够有效抵抗妆容攻击，增强在线平台年龄及身份验证的安全性，有助于保护未成年用户。

Abstract: Accurate age verification can protect underage users from unauthorized access
to online platforms and e-commerce sites that provide age-restricted services.
However, accurate age estimation can be confounded by several factors,
including facial makeup that can induce changes to alter perceived identity and
age to fool both humans and machines. In this work, we propose DiffClean which
erases makeup traces using a text-guided diffusion model to defend against
makeup attacks. DiffClean improves age estimation (minor vs. adult accuracy by
4.8%) and face verification (TMR by 8.9% at FMR=0.01%) over competing baselines
on digitally simulated and real makeup images.

</details>


### [80] [FashionPose: Text to Pose to Relight Image Generation for Personalized Fashion Visualization](https://arxiv.org/abs/2507.13311)
*Chuancheng Shi,Yixiang Chen,Burong Lei,Jichao Chen*

Main category: cs.CV

TL;DR: 本文提出了FashionPose，一个能够根据自然语言描述生成不同姿态和光照条件下服饰虚拟试穿图像的全流程方法，大幅提升了个性化预览的灵活性和逼真度。


<details>
  <summary>Details</summary>
Motivation: 传统的服饰可视化技术主要依赖预设姿态，缺乏语义灵活性及在不同光照下的适应能力，难以满足电商中用户对于个性化和多样化展示的需求。

Method: FashionPose通过三个步骤完成服饰虚拟展示：1）先由自然语言描述预测2D人体姿态；2）利用扩散模型生成高质量人体图像；3）通过轻量级重新光照模块改变整体光照，三个阶段均由文本输入引导，实现了无须显式姿态标注的控制。

Result: 实验表明，该方法可实现细粒度的姿态合成和高效、协调的重光照，极大提升了虚拟服饰展示的表现力和实用性。

Conclusion: FashionPose实现了端到端、基于文本驱动的服饰虚拟展示流程，在姿态与光照控制方面突破了传统方法的局限，具有重要的实际价值和应用前景。

Abstract: Realistic and controllable garment visualization is critical for fashion
e-commerce, where users expect personalized previews under diverse poses and
lighting conditions. Existing methods often rely on predefined poses, limiting
semantic flexibility and illumination adaptability. To address this, we
introduce FashionPose, the first unified text-to-pose-to-relighting generation
framework. Given a natural language description, our method first predicts a 2D
human pose, then employs a diffusion model to generate high-fidelity person
images, and finally applies a lightweight relighting module, all guided by the
same textual input. By replacing explicit pose annotations with text-driven
conditioning, FashionPose enables accurate pose alignment, faithful garment
rendering, and flexible lighting control. Experiments demonstrate fine-grained
pose synthesis and efficient, consistent relighting, providing a practical
solution for personalized virtual fashion display.

</details>


### [81] [Revisiting Reliability in the Reasoning-based Pose Estimation Benchmark](https://arxiv.org/abs/2507.13314)
*Junsu Kim,Naeun Kim,Jaeho Lee,Incheol Park,Dongyoon Han,Seungryul Baek*

Main category: cs.CV

TL;DR: 本文分析了RPE基准用于姿态感知多模态大语言模型评价时的再现性与基准质量问题，并修正了其标注，公开发布以便后续研究更公平地比较。


<details>
  <summary>Details</summary>
Motivation: RPE成为姿态感知多模态大语言模型的主流评测标准，但作者发现其在数据索引、标注、内容冗余等方面存在显著问题，影响定量评价的公平性和再现性。

Method: 作者系统性分析了RPE基准，指出其用的图片索引与原始3DPW数据集不同，需手动匹配获得准确GT标注，并存在冗余、场景失衡、姿态简单和描述模糊等问题。为此，作者通过细致人工视觉比对修正了GT标注，并将改正版开放发布。

Result: 作者完成了更精确的GT标注工作，消除了匹配误差，并发现RPE原始数据存在多种影响评价可靠性的缺陷。修正版资源现已开放共享。

Conclusion: RPE基准存在严重可复现性和评价质量问题。作者的GT标注修正版和开放共享，有助于标准化未来定量评价，推动多模态姿态感知模型研究发展。

Abstract: The reasoning-based pose estimation (RPE) benchmark has emerged as a widely
adopted evaluation standard for pose-aware multimodal large language models
(MLLMs). Despite its significance, we identified critical reproducibility and
benchmark-quality issues that hinder fair and consistent quantitative
evaluations. Most notably, the benchmark utilizes different image indices from
those of the original 3DPW dataset, forcing researchers into tedious and
error-prone manual matching processes to obtain accurate ground-truth (GT)
annotations for quantitative metrics (\eg, MPJPE, PA-MPJPE). Furthermore, our
analysis reveals several inherent benchmark-quality limitations, including
significant image redundancy, scenario imbalance, overly simplistic poses, and
ambiguous textual descriptions, collectively undermining reliable evaluations
across diverse scenarios. To alleviate manual effort and enhance
reproducibility, we carefully refined the GT annotations through meticulous
visual matching and publicly release these refined annotations as an
open-source resource, thereby promoting consistent quantitative evaluations and
facilitating future advancements in human pose-aware multimodal reasoning.

</details>


### [82] [A Real-Time System for Egocentric Hand-Object Interaction Detection in Industrial Domains](https://arxiv.org/abs/2507.13326)
*Antonio Finocchiaro,Alessandro Sebastiano Catinello,Michele Mazzamuto,Rosario Leonardi,Antonino Furnari,Giovanni Maria Farinella*

Main category: cs.CV

TL;DR: 该论文提出了一种用于实时手部与物体交互检测的新方法，通过级联动作识别和目标检测模块，能够高效且准确地检测用户与周围物体的交互。


<details>
  <summary>Details</summary>
Motivation: 在实时应用中，对手-物体交互的准确快速检测对于提供直观的用户体验非常重要。然而，目前该任务仍存在计算效率和识别准确率之间的挑战。

Method: 作者提出了包括两个模块的系统：动作识别模块（基于Mamba模型和EfficientNetV2主干）及目标检测模块（采用微调后的YOLOWorld）。在系统架构上，两模块级联运行，当动作识别模块检测到手与物体接触时，目标检测模块被激活以检测并分类活跃物体。

Result: 在ENIGMA-51基准测试上，动作识别模块的p-AP达38.52%，目标检测模块的AP为85.13%；整体系统能以30帧每秒实时运行。

Conclusion: 所提出的级联手-物体交互检测方法在保持高检测精度的同时，实现了实时性能，可应用于需要自然交互体验的实时场景。

Abstract: Hand-object interaction detection remains an open challenge in real-time
applications, where intuitive user experiences depend on fast and accurate
detection of interactions with surrounding objects. We propose an efficient
approach for detecting hand-objects interactions from streaming egocentric
vision that operates in real time. Our approach consists of an action
recognition module and an object detection module for identifying active
objects upon confirmed interaction. Our Mamba model with EfficientNetV2 as
backbone for action recognition achieves 38.52% p-AP on the ENIGMA-51 benchmark
at 30fps, while our fine-tuned YOLOWorld reaches 85.13% AP for hand and object.
We implement our models in a cascaded architecture where the action recognition
and object detection modules operate sequentially. When the action recognition
predicts a contact state, it activates the object detection module, which in
turn performs inference on the relevant frame to detect and classify the active
object.

</details>


### [83] [Taming Diffusion Transformer for Real-Time Mobile Video Generation](https://arxiv.org/abs/2507.13343)
*Yushu Wu,Yanyu Li,Anil Kag,Ivan Skorokhodov,Willi Menapace,Ke Ma,Arpit Sahni,Ju Hu,Aliaksandr Siarohin,Dhritiman Sagar,Yanzhi Wang,Sergey Tulyakov*

Main category: cs.CV

TL;DR: 本论文提出了一系列优化手段，使基于Diffusion Transformers（DiT）的高质量视频生成首次可在手机等资源受限设备上实现实时（>10FPS）效果。


<details>
  <summary>Details</summary>
Motivation: 尽管DiT在视频生成任务上表现优异，但算力消耗极高，不适合在移动设备上部署，更难以实现实时生成。作者希望通过模型优化，使DiT模型可以在移动端实现高效、实时的视频生成。

Method: 1）采用高压缩变分自编码器（VAE）来减少输入数据维度，降低运算量同时保证视觉质量；2）提出KD指导的三层敏感度剪枝策略，有效缩小模型规模以适配移动设备，同时保证关键性能；3）针对DiT开发了对抗式推理步骤蒸馏技术，将推理步骤压缩到仅4步。

Result: 综合所有优化方法，模型在iPhone 16 Pro Max能实现超过10帧每秒的实时高质量视频生成。

Conclusion: 本论文的方法首次验证了在主流移动设备上也能实现实时高质量视频生成，为视频生成技术在移动端的实际应用奠定了基础。

Abstract: Diffusion Transformers (DiT) have shown strong performance in video
generation tasks, but their high computational cost makes them impractical for
resource-constrained devices like smartphones, and real-time generation is even
more challenging. In this work, we propose a series of novel optimizations to
significantly accelerate video generation and enable real-time performance on
mobile platforms. First, we employ a highly compressed variational autoencoder
(VAE) to reduce the dimensionality of the input data without sacrificing visual
quality. Second, we introduce a KD-guided, sensitivity-aware tri-level pruning
strategy to shrink the model size to suit mobile platform while preserving
critical performance characteristics. Third, we develop an adversarial step
distillation technique tailored for DiT, which allows us to reduce the number
of inference steps to four. Combined, these optimizations enable our model to
achieve over 10 frames per second (FPS) generation on an iPhone 16 Pro Max,
demonstrating the feasibility of real-time, high-quality video generation on
mobile devices.

</details>


### [84] [Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models](https://arxiv.org/abs/2507.13344)
*Yudong Jin,Sida Peng,Xuan Wang,Tao Xie,Zhen Xu,Yifan Yang,Yujun Shen,Hujun Bao,Xiaowei Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，提升用少量视频视角实现高质量人物新视角视频合成的空间和时间一致性，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有4D扩散模型虽然能用稀疏视角生成新视角视频，但生成结果往往在空间和时间上不一致，导致视图合成质量不理想。针对这一问题，作者希望改进4D扩散模型的方法，使其输出的视频在保持高质量的同时具备更好的4D一致性。

Method: 本文提出滑动迭代去噪过程(sliding iterative denoising)。具体方法是在潜变量网格中组织每个视角和时刻的信息（包含图像、相机位姿和人物姿势），逐步沿空间和时间维度用滑动窗口去噪，使信息充分传播到整个网格，再从去噪后的潜变量中解码得到新视角视频。这种方法兼顾了4D一致性和GPU显存开销。

Result: 在DNA-Rendering和ActorsHQ数据集上，作者方法能够生成高质量、4D一致性良好的新视角视频，并且显著优于以往方法。

Conclusion: 作者所提方法有效解决了4D扩散模型在高保真人物新视角合成中的一致性难题，为稀疏视角输入条件下高质量动态视频合成提供了新的解决方案。

Abstract: This paper addresses the challenge of high-fidelity view synthesis of humans
with sparse-view videos as input. Previous methods solve the issue of
insufficient observation by leveraging 4D diffusion models to generate videos
at novel viewpoints. However, the generated videos from these models often lack
spatio-temporal consistency, thus degrading view synthesis quality. In this
paper, we propose a novel sliding iterative denoising process to enhance the
spatio-temporal consistency of the 4D diffusion model. Specifically, we define
a latent grid in which each latent encodes the image, camera pose, and human
pose for a certain viewpoint and timestamp, then alternately denoising the
latent grid along spatial and temporal dimensions with a sliding window, and
finally decode the videos at target viewpoints from the corresponding denoised
latents. Through the iterative sliding, information flows sufficiently across
the latent grid, allowing the diffusion model to obtain a large receptive field
and thus enhance the 4D consistency of the output, while making the GPU memory
consumption affordable. The experiments on the DNA-Rendering and ActorsHQ
datasets demonstrate that our method is able to synthesize high-quality and
consistent novel-view videos and significantly outperforms the existing
approaches. See our project page for interactive demos and video results:
https://diffuman4d.github.io/ .

</details>


### [85] [Imbalance in Balance: Online Concept Balancing in Generation Models](https://arxiv.org/abs/2507.13345)
*Yukai Shi,Jiarong Ou,Rui Chen,Haotian Yang,Jiahao Wang,Xin Tao,Pengfei Wan,Di Zhang,Kun Gai*

Main category: cs.CV

TL;DR: 本文提出了一种新方法用于提升视觉生成任务中复杂概念的响应准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉生成任务对于复杂概念的响应和组合经常不稳定且容易出错，这一问题尚未得到充分研究。

Method: 作者设计了精细的实验来探究复杂概念响应不理想的因果因素，并提出了一种名为IMBA loss的概念均衡损失函数。该方法为在线算法，无需离线处理数据集，且代码改动极小。

Result: 在新提出的Inert-CompBench基准和两个公开测试集上，该方法显著提升了基线模型对复杂概念的响应能力，并取得了极具竞争力的效果。

Conclusion: 新方法不需额外数据处理和复杂代码修改，实现了对复杂视觉生成任务中概念响应的有效加强，有望在实际应用中推广。

Abstract: In visual generation tasks, the responses and combinations of complex
concepts often lack stability and are error-prone, which remains an
under-explored area. In this paper, we attempt to explore the causal factors
for poor concept responses through elaborately designed experiments. We also
design a concept-wise equalization loss function (IMBA loss) to address this
issue. Our proposed method is online, eliminating the need for offline dataset
processing, and requires minimal code changes. In our newly proposed complex
concept benchmark Inert-CompBench and two other public test sets, our method
significantly enhances the concept response capability of baseline models and
yields highly competitive results with only a few codes.

</details>


### [86] [AutoPartGen: Autogressive 3D Part Generation and Discovery](https://arxiv.org/abs/2507.13346)
*Minghao Chen,Jianyuan Wang,Roman Shapovalov,Tom Monnier,Hyunyoung Jung,Dilin Wang,Rakesh Ranjan,Iro Laina,Andrea Vedaldi*

Main category: cs.CV

TL;DR: AutoPartGen是一种自回归生成3D物体部件的模型，可根据图像、2D掩码或3D对象输入生成对应的3D物体重建，具有领先的3D部件生成效果。


<details>
  <summary>Details</summary>
Motivation: 3D物体通常由多个部件组合而成，现有3D生成方法难以灵活、细致地控制部件的生成过程，因此需要一种支持高效、可控且结构合理的3D部件生成方法。

Method: 基于3DShape2VecSet的潜在3D表示，AutoPartGen采用自回归机制，逐步生成物体的各个部件。每次生成新部件时，模型会参考已生成部件以及外部输入（2D图片、掩码或3D对象），并自动判断是否继续生成，实现部件数量和类型的自适应判定，生成的部件可无缝组合为完整物体。

Result: AutoPartGen在整体3D生成能力和部件层级的生成质量方面进行了评估，结果表明其在3D部件生成任务中达到当前最优水平。

Conclusion: AutoPartGen能够实现灵活而高质量的3D部件生成和组装，扩展了3D生成模型在可组合性和控制性方面的能力，推动了3D生成方法的发展。

Abstract: We introduce AutoPartGen, a model that generates objects composed of 3D parts
in an autoregressive manner. This model can take as input an image of an
object, 2D masks of the object's parts, or an existing 3D object, and generate
a corresponding compositional 3D reconstruction. Our approach builds upon
3DShape2VecSet, a recent latent 3D representation with powerful geometric
expressiveness. We observe that this latent space exhibits strong compositional
properties, making it particularly well-suited for part-based generation tasks.
Specifically, AutoPartGen generates object parts autoregressively, predicting
one part at a time while conditioning on previously generated parts and
additional inputs, such as 2D images, masks, or 3D objects. This process
continues until the model decides that all parts have been generated, thus
determining automatically the type and number of parts. The resulting parts can
be seamlessly assembled into coherent objects or scenes without requiring
additional optimization. We evaluate both the overall 3D generation
capabilities and the part-level generation quality of AutoPartGen,
demonstrating that it achieves state-of-the-art performance in 3D part
generation.

</details>


### [87] [$π^3$: Scalable Permutation-Equivariant Visual Geometry Learning](https://arxiv.org/abs/2507.13347)
*Yifan Wang,Jianjun Zhou,Haoyi Zhu,Wenzheng Chang,Yang Zhou,Zizun Li,Junyi Chen,Jiangmiao Pang,Chunhua Shen,Tong He*

Main category: cs.CV

TL;DR: 该论文提出了一种新型前馈神经网络π³，可无偏差地实现视觉几何重建，无需依赖固定参考视角，方法鲁棒且高效。


<details>
  <summary>Details</summary>
Motivation: 现有视觉几何重建方法通常依赖于预设参考视角，这种归纳偏差会导致在参考视角选择不当时结果不稳定甚至失败，因此亟需一种摆脱固定参考视角的方法。

Method: π³是一种完全置换等变的神经网络架构，能够在不依赖任何参考帧的情况下，预测仿射不变的相机位姿和尺度不变的局部点云，无需关心输入顺序，提高了网络的鲁棒性和扩展性。

Result: π³在相机位姿估计、单目/视频深度估计和稠密点云重建等多种任务上达到了最新水平，并且代码和模型已开源。

Conclusion: π³摆脱了传统方法对参考视角的依赖，具备更强的泛化能力与实际适用性，可为视觉几何重建任务提供更稳定、更高效的解决方案。

Abstract: We introduce $\pi^3$, a feed-forward neural network that offers a novel
approach to visual geometry reconstruction, breaking the reliance on a
conventional fixed reference view. Previous methods often anchor their
reconstructions to a designated viewpoint, an inductive bias that can lead to
instability and failures if the reference is suboptimal. In contrast, $\pi^3$
employs a fully permutation-equivariant architecture to predict
affine-invariant camera poses and scale-invariant local point maps without any
reference frames. This design makes our model inherently robust to input
ordering and highly scalable. These advantages enable our simple and bias-free
approach to achieve state-of-the-art performance on a wide range of tasks,
including camera pose estimation, monocular/video depth estimation, and dense
point map reconstruction. Code and models are publicly available.

</details>


### [88] [Hierarchical Rectified Flow Matching with Mini-Batch Couplings](https://arxiv.org/abs/2507.13350)
*Yichi Zhang,Yici Yan,Alex Schwing,Zhizhen Zhao*

Main category: cs.CV

TL;DR: 本文提出在分层流匹配（hierarchical flow matching）模型中引入小批量耦合（mini-batch couplings），以更有效地调整不同层级分布的复杂度，从而提升生成模型对多模态数据的建模能力，并在实际和合成数据上展示了改进效果。


<details>
  <summary>Details</summary>
Motivation: 现有的流匹配生成模型虽然能处理多模态分布，分层流匹配进一步建模了多模态速度场，但层次结构中各层分布复杂度一致，缺乏灵活调整能力。作者希望解决如何在不同层级灵活调控分布复杂度，以更高效、更准确地学习多模态结构。

Method: 作者提出在分层流匹配架构里采用mini-batch couplings机制，在不同层级之间进行小批量耦合，以实现对各层分布复杂度的逐步调整。通过这一机制，模型能在保持层次化同时，更灵活地拟合不同复杂度的分布。

Result: 实验证明，在合成数据和图像生成任务中，引入mini-batch couplings的分层流匹配在建模多模态分布与生成质量方面优于原始方法，取得了更具说服力的结果。

Conclusion: mini-batch couplings机制提升了分层流匹配对多模态复杂分布的适应能力，有效改善了生成数据的质量，对流匹配模型的结构设计和应用有现实意义。

Abstract: Flow matching has emerged as a compelling generative modeling approach that
is widely used across domains. To generate data via a flow matching model, an
ordinary differential equation (ODE) is numerically solved via forward
integration of the modeled velocity field. To better capture the multi-modality
that is inherent in typical velocity fields, hierarchical flow matching was
recently introduced. It uses a hierarchy of ODEs that are numerically
integrated when generating data. This hierarchy of ODEs captures the
multi-modal velocity distribution just like vanilla flow matching is capable of
modeling a multi-modal data distribution. While this hierarchy enables to model
multi-modal velocity distributions, the complexity of the modeled distribution
remains identical across levels of the hierarchy. In this paper, we study how
to gradually adjust the complexity of the distributions across different levels
of the hierarchy via mini-batch couplings. We show the benefits of mini-batch
couplings in hierarchical rectified flow matching via compelling results on
synthetic and imaging data. Code is available at
https://riccizz.github.io/HRF_coupling.

</details>


### [89] [VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding](https://arxiv.org/abs/2507.13353)
*Shihao Wang,Guo Chen,De-an Huang,Zhiqi Li,Minghan Li,Guilin Li,Jose M. Alvarez,Lei Zhang,Zhiding Yu*

Main category: cs.CV

TL;DR: 本论文提出了一种基于用户指令的视频时序定位方法（VideoITG），通过精细化注释与帧采样，有效提升了视频大语言模型（Video-LLMs）的理解能力，并在多项多模态视频理解任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的视频大语言模型虽然已具备一定的视频理解能力，但在长视频的复杂场景下表现不佳，尤其是在帧选择和对用户指令的细致对齐方面存在局限。因此，作者希望开发一种能更好理解长视频，并与用户指令高度匹配的视频帧采样与注释方法。

Method: 作者提出了VideoITG方法，核心包含VidThinker流程：首先根据用户指令生成视频片段级详细描述；随后通过指令引导的推理，检索相关视频片段；最后进行细粒度帧选择，找出最有信息量的画面。基于VidThinker，建立了VideoITG-40K数据集（40K视频、50万条时序定位注释），进一步设计了可插拔的VideoITG模型，强化视觉-语言对齐和推理能力，实现判别式的帧筛选。

Result: VideoITG与主流的视频大语言模型结合后，在多项多模态视频理解基准上持续取得性能提升，证明了其优越性。

Conclusion: 通过结合用户指令和细粒度的自动化标注框架，VideoITG显著增强了视频大语言模型在帧选择及复杂视频理解方面的表现，表现出广阔的应用前景。

Abstract: Recent studies have revealed that selecting informative and relevant video
frames can significantly improve the performance of Video Large Language Models
(Video-LLMs). Current methods, such as reducing inter-frame redundancy,
employing separate models for image-text relevance assessment, or utilizing
temporal video grounding for event localization, substantially adopt
unsupervised learning paradigms, whereas they struggle to address the complex
scenarios in long video understanding. We propose Instructed Temporal Grounding
for Videos (VideoITG), featuring customized frame sampling aligned with user
instructions. The core of VideoITG is the VidThinker pipeline, an automated
annotation framework that explicitly mimics the human annotation process.
First, it generates detailed clip-level captions conditioned on the
instruction; then, it retrieves relevant video segments through
instruction-guided reasoning; finally, it performs fine-grained frame selection
to pinpoint the most informative visual evidence. Leveraging VidThinker, we
construct the VideoITG-40K dataset, containing 40K videos and 500K instructed
temporal grounding annotations. We then design a plug-and-play VideoITG model,
which takes advantage of visual language alignment and reasoning capabilities
of Video-LLMs, for effective frame selection in a discriminative manner.
Coupled with Video-LLMs, VideoITG achieves consistent performance improvements
across multiple multimodal video understanding benchmarks, showing its
superiority and great potentials for video understanding.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [90] [Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)
*Lionel Wong,Katherine M. Collins,Lance Ying,Cedegao E. Zhang,Adrian Weller,Tobias Gersternberg,Timothy O'Donnell,Alexander K. Lew,Jacob D. Andreas,Joshua B. Tenenbaum,Tyler Brooke-Wilson*

Main category: cs.CL

TL;DR: 本文提出了一种融合分布式与符号表示的人类类比推理的模型架构（Model Synthesis Architecture, MSA），并在开放性推理任务中优于仅使用语言模型的方法。


<details>
  <summary>Details</summary>
Motivation: 人类能在新情境下整合大量背景知识并做出推理与预测，而现有 AI 模型难以复制这一能力。本研究试图解释和模拟人类在复杂开放域推理中的方法。

Method: 作者提出了“模型合成架构”（MSA），结合语言模型实现全局相关性检索与模型合成，及概率程序实现个性化、连贯的世界模型。通过新设计的‘Model Olympics’推理数据集对 MSA 进行评估，任务关注模型在全新因果结构、丰富背景知识和新奇观测变量下的推理能力。

Result: MSA 在解释人类判断能力上优于仅用语言模型的基线，无论是直接输出还是链式思考生成，MSA 都表现出更强的人类类比推理能力。

Conclusion: 模型合成架构可以模拟人类结合本地连贯性与全局相关性的推理过程，为理解与复制人类在开放性领域推理提供新路径。

Abstract: When faced with novel situations, people are able to marshal relevant
considerations from a wide range of background knowledge and put these to use
in inferences and predictions. What permits us to draw in globally relevant
information and reason over it coherently? Here, we explore the hypothesis that
people use a combination of distributed and symbolic representations to
construct bespoke mental models tailored to novel situations. We propose a
computational implementation of this idea -- a ``Model Synthesis Architecture''
(MSA) -- using language models to implement global relevance-based retrieval
and model synthesis and probabilistic programs to implement bespoke, coherent
world models. We evaluate our MSA as a model of human judgments on a novel
reasoning dataset. The dataset -- built around a `Model Olympics` domain of
sports vignettes -- tests models' capacity for human-like, open-ended reasoning
by requiring (i) judgments about novel causal structures described in language;
(ii) drawing on large bodies of background knowledge; and (iii) doing both in
light of observations that introduce arbitrary novel variables. Our MSA
approach captures human judgments better than language model-only baselines,
under both direct and chain-of-thought generations from the LM that supports
model synthesis. These results suggest that MSAs can be implemented in a way
that mirrors people's ability to deliver locally coherent reasoning over
globally relevant variables, offering a path to understanding and replicating
human reasoning in open-ended domains.

</details>


### [91] [Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility](https://arxiv.org/abs/2507.12553)
*Michael A. Lepori,Jennifer Hu,Ishita Dasgupta,Roma Patel,Thomas Serre,Ellie Pavlick*

Main category: cs.CL

TL;DR: 本文发现语言模型内部存在可以区分语气范畴（如可能、不可能、无意义等）的线性向量结构，并证明这些结构能有效预测与人类一致的语气判断，且随模型能力提升而有规律地发展。


<details>
  <summary>Details</summary>
Motivation: 语言模型被广泛用于多种任务，但近期研究质疑它们是否能准确判断句子的语气范畴，如区分句子是可能、无法实现或荒谬的。本研究旨在探究语言模型内部是否真具备这种判断能力，并理解其内部实现机制。

Method: 作者在多种主流语言模型中，识别出能够区分不同语气类别的线性表示（称为modal difference vectors）。通过分析这些向量，探索它们的出现顺序与模型能力增长（如训练步数、层数、参数量）的关系。此外，将这些向量与人类对句子语气分类的行为数据进行关联研究。

Result: 发现这些modal difference vectors能在模型内部有效区分语气范畴，其判别能力较此前文献报告更为可靠。随着模型变得更强，这些向量以一致顺序逐渐显现。重要的是，这些向量投影分数能很好地模拟并匹配人类的细致语气判断。

Conclusion: 利用可解释性机制分析法，本文揭示了语言模型内部可预测、可解释的人类式语气区分类似机制。这不仅丰富了我们对大模型内部结构的理解，也为研究人类自身对语气范畴的理解提供了新视角。

Abstract: Language models (LMs) are used for a diverse range of tasks, from question
answering to writing fantastical stories. In order to reliably accomplish these
tasks, LMs must be able to discern the modal category of a sentence (i.e.,
whether it describes something that is possible, impossible, completely
nonsensical, etc.). However, recent studies have called into question the
ability of LMs to categorize sentences according to modality (Michaelov et al.,
2025; Kauf et al., 2023). In this work, we identify linear representations that
discriminate between modal categories within a variety of LMs, or modal
difference vectors. Analysis of modal difference vectors reveals that LMs have
access to more reliable modal categorization judgments than previously
reported. Furthermore, we find that modal difference vectors emerge in a
consistent order as models become more competent (i.e., through training steps,
layers, and parameter count). Notably, we find that modal difference vectors
identified within LM activations can be used to model fine-grained human
categorization behavior. This potentially provides a novel view into how human
participants distinguish between modal categories, which we explore by
correlating projections along modal difference vectors with human participants'
ratings of interpretable features. In summary, we derive new insights into LM
modal categorization using techniques from mechanistic interpretability, with
the potential to inform our understanding of modal categorization in humans.

</details>


### [92] [The first open machine translation system for the Chechen language](https://arxiv.org/abs/2507.12672)
*Abu-Viskhan A. Umishov,Vladislav A. Grigorian*

Main category: cs.CL

TL;DR: 本文首次发布了可用于车臣语与俄语互译的开源模型及其数据集，为弱势语言车臣语的翻译提供了基础工具。


<details>
  <summary>Details</summary>
Motivation: 车臣语是一种弱势语言，缺乏高质量的机器翻译工具。该工作旨在填补车臣语与俄语间数据和翻译模型的空白，支持车臣语的数字语言生存。

Method: 作者收集并发布了车臣语与俄语的平行语料（词、短语、句子），采用 NLLB-200 大型多语种翻译模型进行微调，使其具备车臣语和俄语的双向翻译能力。

Result: 模型实现了从俄语到车臣语 BLEU/ChrF++ 得分为 8.34/34.69，从车臣语到俄语为 20.89/44.55。还发布了适应车臣语的多语种句子编码器。

Conclusion: 该工作推动了弱势语言的机器翻译研究，为车臣语的保存和应用提供了基础资源，并展示了将新语言纳入多语种大模型体系的可行性。

Abstract: We introduce the first open-source model for translation between the
vulnerable Chechen language and Russian, and the dataset collected to train and
evaluate it. We explore fine-tuning capabilities for including a new language
into a large language model system for multilingual translation NLLB-200. The
BLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for
translation from Russian to Chechen and reverse direction, respectively. The
release of the translation models is accompanied by the distribution of
parallel words, phrases and sentences corpora and multilingual sentence encoder
adapted to the Chechen language.

</details>


### [93] [Improving Drug Identification in Overdose Death Surveillance using Large Language Models](https://arxiv.org/abs/2507.12679)
*Arthur J. Funnell,Panayiotis Petousis,Fabrice Harel-Canada,Ruby Romero,Alex A. T. Bui,Adam Koncsol,Hritika Chaturvedi,Chelsea Shover,David Goodman-Meza*

Main category: cs.CL

TL;DR: 本研究利用NLP模型分析验尸报告中的死亡原因文本数据，实现对美国药物过量死亡（尤其是芬太尼相关）的快速精确监测，BioClinicalBERT模型表现最优。


<details>
  <summary>Details</summary>
Motivation: 药物过量死亡人数在美国持续上升，芬太尼是主要驱动因素。传统基于手动ICD-10编码的监测手段有滞后性、信息丢失等问题，因此需要更快速、自动化的方法提升监控效率和准确性。

Method: 构建了包含35,433份2020年美国多地死亡记录的数据集，外部测试集为3,335份2023-2024年数据。采用传统ML、多标签分类、基于BERT与BioClinicalBERT的Encoder-only模型，以及Qwen 3、Llama 3等LLM的Decoder-only模型，对自由文本死亡记录进行药物分类。以F1分数评估模型性能。

Result: 微调后的BioClinicalBERT在内部测试集F1分数接近完美（≥0.998），在外部验证集中表现依然优异（F1=0.966），均优于传统机器学习和其他LLM模型。

Conclusion: 适用于临床文本的深度NLP模型（如BioClinicalBERT）在药物过量死亡监控领域达到了极高的准确性和可扩展性，可大幅加快数据审核与趋势发现流程，克服了人工ICD-10编码的局限，支持近实时的公共卫生干预。

Abstract: The rising rate of drug-related deaths in the United States, largely driven
by fentanyl, requires timely and accurate surveillance. However, critical
overdose data are often buried in free-text coroner reports, leading to delays
and information loss when coded into ICD (International Classification of
Disease)-10 classifications. Natural language processing (NLP) models may
automate and enhance overdose surveillance, but prior applications have been
limited. A dataset of 35,433 death records from multiple U.S. jurisdictions in
2020 was used for model training and internal testing. External validation was
conducted using a novel separate dataset of 3,335 records from 2023-2024.
Multiple NLP approaches were evaluated for classifying specific drug
involvement from unstructured death certificate text. These included
traditional single- and multi-label classifiers, as well as fine-tuned
encoder-only language models such as Bidirectional Encoder Representations from
Transformers (BERT) and BioClinicalBERT, and contemporary decoder-only large
language models such as Qwen 3 and Llama 3. Model performance was assessed
using macro-averaged F1 scores, and 95% confidence intervals were calculated to
quantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect
performance, with macro F1 scores >=0.998 on the internal test set. External
validation confirmed robustness (macro F1=0.966), outperforming conventional
machine learning, general-domain BERT models, and various decoder-only large
language models. NLP models, particularly fine-tuned clinical variants like
BioClinicalBERT, offer a highly accurate and scalable solution for overdose
death classification from free-text reports. These methods can significantly
accelerate surveillance workflows, overcoming the limitations of manual ICD-10
coding and supporting near real-time detection of emerging substance use
trends.

</details>


### [94] [AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.12695)
*S M Rafiuddin,Sadia Kamal,Mohammed Rakib,Arunkumar Bagavathi,Atriya Sen*

Main category: cs.CL

TL;DR: 提出了一种新型的多模态方面情感分析框架AdaptiSent，通过自适应跨模态注意力机制提升文本和图像的情感分类及方面词抽取表现，在多项指标上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态情感分析方法难以灵活结合文字与图像的互补信息，对复杂跨模态情感特征提取能力较弱，因此亟需新的方法提升分析效果。

Method: AdaptiSent框架主要采用动态模态权重分配及上下文自适应注意力机制，将文本和图像的信息通过灵活权重和关注点整合，提升两者互动和特征抽取的有效性。并在标准Twitter多模态数据集上与传统文本模型及其它多模态方法进行了对比。

Result: 在多个指标（准确率、召回率、F1分数）上，AdaptiSent均优于现有主流模型，尤其在识别细致的跨模态关系和抽取方面词、情感方面展现出更强能力。

Conclusion: AdaptiSent能够动态适应不同上下文需求，极大提升多模态情感分析的深度与准确度，显著优于当前方法，在理解复杂跨模态信息方面树立了新的标准。

Abstract: We introduce AdaptiSent, a new framework for Multimodal Aspect-Based
Sentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms
to improve sentiment classification and aspect term extraction from both text
and images. Our model integrates dynamic modality weighting and
context-adaptive attention, enhancing the extraction of sentiment and
aspect-related information by focusing on how textual cues and visual context
interact. We tested our approach against several baselines, including
traditional text-based models and other multimodal methods. Results from
standard Twitter datasets show that AdaptiSent surpasses existing models in
precision, recall, and F1 score, and is particularly effective in identifying
nuanced inter-modal relationships that are crucial for accurate sentiment and
aspect term extraction. This effectiveness comes from the model's ability to
adjust its focus dynamically based on the context's relevance, improving the
depth and accuracy of sentiment analysis across various multimodal data sets.
AdaptiSent sets a new standard for MABSA, significantly outperforming current
methods, especially in understanding complex multimodal information.

</details>


### [95] [AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation](https://arxiv.org/abs/2507.12705)
*Potsawee Manakul,Woody Haosheng Gan,Michael J. Ryan,Ali Sartaz Khan,Warit Sirichotedumrong,Kunat Pipatanakul,William Held,Diyi Yang*

Main category: cs.CL

TL;DR: 提出了基于大型音频模型（Large Audio Model, LAM）的自动语音评测框架AudioJudge，在多类语音特征检测和模拟人类偏好等任务中表现优异，有望解决传统语音评测系统的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有自动语音评测系统存在专用性强、灵活性差和评测结果与人类偏好相关性低两大问题，亟需一种能统一适配多种音频评测需求且结果更贴近人类评判的新框架。

Method: 系统性研究了通过大型音频模型（AudioJudge）进行音频评测的方法，涵盖发音、语速、说话人识别、语音质量、人类偏好模拟等任务。考察了不同Prompt设计，提出音频拼接结合in-context learning策略，并构建多维评测模型，将语音评测分解为词汇内容、语音质量、超语段特征多个子评委。

Result: AudioJudge在单一及多维特征的评测任务中均表现突出，在系统排名基准数据集上获得最高0.91的Spearman相关性。分析发现其在噪声环境下依然表现强劲，但存在冗余输出与位置偏倚问题。

Conclusion: AudioJudge为自动语音评测提供了统一且高性能的解决方案，显著提升了与人类评测结果的一致性，但部分表现偏差仍需进一步改进。

Abstract: Current speech evaluation suffers from two critical limitations: the need and
difficulty of designing specialized systems targeting individual audio
characteristics, and poor correlation between automatic evaluation methods and
human preferences. This work presents a systematic study of Large Audio Model
(LAM) as a Judge, AudioJudge, investigating whether it can provide a unified
evaluation framework that addresses both challenges. We systematically explore
AudioJudge across audio characteristic detection tasks, including
pronunciation, speaking rate, speaker identification and speech quality, and
system-level human preference simulation for automated benchmarking. We
investigate different prompt engineering strategies, finding that audio
concatenation combined with in-context learning significantly improves
performance across both audio characteristic detection and human preference
simulation tasks. We further introduce a multi-aspect ensemble AudioJudge to
enable general-purpose multi-aspect audio evaluation. This method decomposes
speech assessment into specialized judges for lexical content, speech quality,
and paralinguistic features, achieving up to 0.91 Spearman correlation with
human preferences on our system ranking benchmark. Robustness analysis reveals
that while LAMs maintain strong performance under acoustic noise, they exhibit
significant verbosity and positional biases that require careful mitigation.

</details>


### [96] [FLEXITOKENS: Flexible Tokenization for Evolving Language Models](https://arxiv.org/abs/2507.12720)
*Abraham Toluase Owodunni,Orevaoghene Ahia,Sachin Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种名为FLEXITOKENS的可学习分词器，能够让语言模型在自适应新领域时更灵活地进行分词，有效减少分词过碎问题，并显著提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型在适应新数据分布时，分词模块通常保持不变，造成了低效分词，尤其在遇到分布外领域或新的语言/文档时表现不佳。当前的方法难以解决分词器刚性的局限，本工作旨在突破这一瓶颈。

Method: 作者提出字节级语言模型，并引入一个可学习分词子模块，通过预测输入字节序列边界实现对变长片段的编码。不同于现有依赖固定压缩率损失（带来新的刚性）的训练方式，FLEXITOKENS提出了更简化且灵活的训练目标以适应变化。

Result: 在多个多语言基准、不同形态学任务和多样领域中评估，FLEXITOKENS持续减少了token过度碎片化，并在下游任务上相比于传统subword及其它学习型分词器提升至多10%的性能。

Conclusion: FLEXITOKENS作为一种更灵活的分词训练目标，为语言模型适应新领域和任务提供了更优的分词能力，有效提升了模型的适应性与任务表现。相关代码和数据已开源。

Abstract: Language models (LMs) are challenging to adapt to new data distributions by
simple finetuning. This is due to the rigidity of their subword tokenizers,
which typically remain unchanged during adaptation. This inflexibility often
leads to inefficient tokenization, causing overfragmentation of
out-of-distribution domains, unseen languages, or scripts. In this work, we
develop byte-level LMs with learnable tokenizers to make tokenization adaptive.
Our models include a submodule that learns to predict boundaries between the
input byte sequence, encoding it into variable-length segments. Existing
tokenizer-free methods train this boundary predictor using an auxiliary loss
that enforces a fixed compression rate across the training corpus, introducing
a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective
that enables significantly greater flexibility during adaptation. Evaluating
across multiple multilingual benchmarks, morphologically diverse tasks, and
domains, we demonstrate that FLEXITOKENS consistently reduces token
over-fragmentation and achieves up to 10\% improvements on downstream task
performance compared to subword and other gradient-based tokenizers. Code and
data for our experiments will be released at
https://github.com/owos/flexitokens

</details>


### [97] [TransEvalnia: Reasoning-based Evaluation and Ranking of Translations](https://arxiv.org/abs/2507.12724)
*Richard Sproat,Tianyu Zhao,Llion Jones*

Main category: cs.CL

TL;DR: TransEvalnia是一个利用大模型推理能力，通过提示词进行机器翻译质量细致评估与排序的新系统，性能与当前最先进模型媲美，并公开了相关数据与代码。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译自动评估方法与人工评价一致性有限，且大部分系统忽略了评估过程中细粒度、多维度的质量标准。作者希望通过利用大语言模型推理能力，实现更细致且与人类评审高度一致的自动化翻译评估系统。

Method: TransEvalnia以大语言模型如Claude-3.5-Sonnet和Qwen-2.5-72B-Instruct为基础，通过设计提示词促使模型依据Multidimensional Quality Metrics（MQM）中的多个维度评分和解释理由，实现译文的逐维细致打分、排序和优劣评判。此外，分析并提出缓解翻译顺序偏置的方法。

Result: TransEvalnia在作者自建英日数据集及多个WMT任务语对上，评测结果与MT-Ranker持平或更优，其评分和解释被人工评审高度认可。系统生成的分数与人工打分高度相关。指出本系统和MT-Ranker均对译文呈现顺序存在敏感性，并提出了缓解序位偏差的方法。

Conclusion: TransEvalnia实现了基于推理的细粒度多维自动翻译评价，效果达到甚至超过现有最好系统，且评分高度符合人工标准。并公开了所有数据、代码及人评信息，促进后续研究。

Abstract: We present TransEvalnia, a prompting-based translation evaluation and ranking
system that uses reasoning in performing its evaluations and ranking. This
system presents fine-grained evaluations based on a subset of the
Multidimensional Quality Metrics (https://themqm.org/), returns an assessment
of which translation it deems the best, and provides numerical scores for the
various dimensions and for the overall translation. We show that TransEvalnia
performs as well as or better than the state-of-the-art MT-Ranker (Moosa et al.
2024) on our own English-Japanese data as well as several language pairs from
various WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and
Qwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations
returned are deemed highly acceptable to human raters, and that the scores
assigned to the translations by Sonnet, as well as other LLMs, correlate well
with scores assigned by the human raters. We also note the sensitivity of our
system -- as well as MT-Ranker -- to the order in which the translations are
presented, and we propose methods to address this position bias. All data,
including the system's evaluation and reasoning, human assessments, as well as
code is released.

</details>


### [98] [Strategy Adaptation in Large Language Model Werewolf Agents](https://arxiv.org/abs/2507.12732)
*Fuya Nakamori,Yin Jou Huang,Fei Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种通过根据其他玩家态度和对话情境切换预定义策略来提升狼人杀智能体表现的方法，并与基线方法进行了对比验证。


<details>
  <summary>Details</summary>
Motivation: 以往狼人杀智能体大多依赖暗含且固定的策略，难以适应动态变化的游戏情境。为了解决这种适应性不足的问题，作者希望研究一种能根据实际游戏情境灵活调整策略的智能体方法。

Method: 提出根据游戏上下文和对其他玩家角色的估计，显式选择最合适的预定义策略来控制狼人杀智能体。并将该方法与采用隐式或固定策略的基线智能体进行对比实验。

Result: 实验显示，采用策略自适应的狼人杀智能体在性能上优于传统隐式或固定策略的基线智能体。

Conclusion: 作者方法能够有效提升狼人杀智能体的适应能力和整体表现，证明了基于场景显式策略选择的思路优于以往的隐式策略方法。

Abstract: This study proposes a method to improve the performance of Werewolf agents by
switching between predefined strategies based on the attitudes of other players
and the context of conversations. While prior works of Werewolf agents using
prompt engineering have employed methods where effective strategies are
implicitly defined, they cannot adapt to changing situations. In this research,
we propose a method that explicitly selects an appropriate strategy based on
the game context and the estimated roles of other players. We compare the
strategy adaptation Werewolf agents with baseline agents using implicit or
fixed strategies and verify the effectiveness of our proposed method.

</details>


### [99] [Logit Arithmetic Elicits Long Reasoning Capabilities Without Training](https://arxiv.org/abs/2507.12759)
*Yunxiang Zhang,Muhammad Khalifa,Lechen Zhang,Xin Liu,Ayoung Lee,Xinliang Frederick Zhang,Farima Fatahi Bayat,Lu Wang*

Main category: cs.CL

TL;DR: 本文提出了一种推理解码方法 ThinkLogit，无需额外训练即可激发大型语言模型（LLM）的长链式推理能力，并通过更小的指导模型提升表现。进阶方法 ThinkLogit-DPO 进一步利用偏好优化训练指导模型，效果更佳。实验结果显示，在数学推理任务上，使用体积小21倍的引导模型，可使模型准确率显著提升。方法具备高计算效率和低训练成本。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型天生拥有一定的复杂推理能力，但实际激发这种长链推理常常需要额外训练，带来高昂的计算成本。作者希望验证能否不经额外训练，仅依靠推理阶段的技术实现这一能力，以降低成本并提升推理表现。

Method: 作者提出解码阶段方法 ThinkLogit，通过 logits 运算，在推理过程中由小型指导模型实时辅助大型模型推理，从而激发长链推理能力。在进阶方法 ThinkLogit-DPO 中，对指导模型进行偏好优化训练，使其更善于区分正确与错误的推理，有效提升整体表现。

Result: 在使用Qwen2.5-32B模型并由R1-Distill-Qwen-1.5B（体积小21倍）引导下，ThinkLogit和ThinkLogit-DPO分别在四个数学数据集上将pass@1指标提升了26%和29%。另外，通过强化学习获得的长链推理能力可被ThinkLogit迁移，进一步相对提升13%。

Conclusion: 提出的方法无需或极少训练即可激发和提升大型语言模型的长链推理能力，且计算高效，具有较高实际应用潜力。

Abstract: Large reasoning models (LRMs) can do complex reasoning via long
chain-of-thought (CoT) involving cognitive strategies such as backtracking and
self-correction. Recent studies suggest that some models inherently possess
these long reasoning abilities, which may be unlocked via extra training. Our
work first investigates whether we can elicit such behavior without any
training. To this end, we propose a decoding-time approach, ThinkLogit, which
utilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for
long reasoning using a substantially smaller model as guider. We then show that
we can further boost performance by training the guider model with preference
optimization over correct/incorrect reasoning pairs sampled from both the
target and guider model -- a setup we refer to as ThinkLogit-DPO. Our
experiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative
improvement in pass@1 by 26% and 29%, respectively, over four mathematical
datasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model
21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills
acquired through reinforcement learning, improving pass@1 by 13% relative
compared to the Qwen2.5-32B base model. Our work presents a
computationally-efficient method to elicit long reasoning in large models with
minimal or no additional training.

</details>


### [100] [Synergy: End-to-end Concept Model](https://arxiv.org/abs/2507.12769)
*Keli Zheng,Zerong Xie*

Main category: cs.CL

TL;DR: Synergy是一种端到端通过自学习路由机制联通不同抽象层的语言模型，无需传统分词器，性能可与现有模型媲美，具备更灵活的未来潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大部分语言模型需依赖固定的分词器（tokenizer），这限制了模型的灵活性和泛化能力。作者想探索能否构建无需固定分词器、可自动形成不同抽象层次表达的模型架构，提高鲁棒性和适用性。

Method: 提出Synergy模型，通过自学习的路由机制，在字节级输入（byte-level）下学习抽象单元。模型自行学习将原始字节分组为较高层次的语义单元，并与标准的BPE分词器进行对比。在相同规模和数据集下，与Llama3等主流模型性能比较，以考察模型优劣。此外，分析模型去除位置编码后的表现，验证其高层抽象部分是否自然摆脱了位置依赖。

Result: Synergy能自主学习从字节到高层抽象的结构，产生的概念token数量少于传统BBPE分词器，且性能相当。与Llama3对比，在相同规模和数据集下Synergy表现更优。进一步发现在去除位置编码后，高抽象层表现提升，显示出具备位置无关的能力。

Conclusion: Synergy展示了无需固定分词器的架构的可行性，并且能实现更鲁棒、灵活的文本处理管道，对未来语言模型设计提供了新方向。

Abstract: In this paper, we present Synergy, a language model that bridges different
levels of abstraction in an end-to-end fashion through a learned routing
mechanism. Focusing on low-level linguistic abstraction, we trained our model
as a byte-level language model. Our model spontaneously learns to tokenize
bytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE)
tokenizers while keeping comparable performance. By comparing with Llama3, we
observed an advantage of Synergy under the same model scale and training
dataset size. Further studies show that the middle part (the higher abstraction
part) of our model performs better when positional encodings are removed,
suggesting the emergence of position-independent concepts. These findings
demonstrate the feasibility of tokenizer-free architectures, paving the way for
more robust and flexible pipelines.

</details>


### [101] [Learning Robust Negation Text Representations](https://arxiv.org/abs/2507.12782)
*Thinh Hung Truong,Karin Verspoor,Trevor Cohn,Timothy Baldwin*

Main category: cs.CL

TL;DR: 该论文针对小型文本编码器在否定语义理解上的不足，提出利用大模型生成多样化否定与模糊表达的数据，采用对比学习微调BERT类模型，显著提升了否定语理解能力，同时通用性能保持竞争力。该方法也能迁移到大模型上有效提升否定指标。


<details>
  <summary>Details</summary>
Motivation: 虽然大规模自回归语言模型应用广泛，但资源受限场景下小型文本编码器依然重要。目前小模型难以准确捕捉否定语义，影响许多下游依赖文本嵌入的任务，因此需增强其否定鲁棒性。

Method: 作者利用大语言模型产生包含多样否定与模糊句式的数据，借助对比学习对BERT类小模型进行微调，使其更好区分正反语义，同时也尝试将该微调方法应用到大模型。

Result: 经过微调，BERT编码器在否定理解相关任务上表现大幅提升，且在常规基准测试集上仍具竞争力；同样方法迁移至大模型系统（LLMs）也能提升对否定语的处理能力。

Conclusion: 通过数据蒸馏与精细的对比学习微调，可以有效提升文本编码模型的否定语鲁棒性，并不影响其整体表现。所提方法具有通用性，可用于多种模型促进否定语语义理解。

Abstract: Despite rapid adoption of autoregressive large language models, smaller text
encoders still play an important role in text understanding tasks that require
rich contextualized representations. Negation is an important semantic function
that is still not properly captured by such methods, affecting many downstream
applications relying on text embeddings. We propose a strategy to improve
negation robustness of text encoders, by distilling data from large language
models using diverse patterns of negation and hedging. We adopt a standard
contrastive learning strategy to finetune a strong BERT-based model, and
observe large improvement in negation understanding capabilities while
maintaining competitive performance on general benchmarks. In addition, we also
show that our method can be adapted to LLMs, leading to improved performance on
negation benchmarks.

</details>


### [102] [Large Language Models' Internal Perception of Symbolic Music](https://arxiv.org/abs/2507.12808)
*Andrew Shin,Kunitake Kaneko*

Main category: cs.CL

TL;DR: 本文探索了大型语言模型（LLM）在象征性音乐生成中的表现，展示了它们在无需专门音乐训练的情况下能够识别和生成基本的音乐结构及风格，但也揭示了其在缺乏明确音乐语境时的局限性。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型已在自然语言、代码和数学等领域表现出色，但对其在音乐等其他符号领域的建模能力研究较少。本文旨在填补LLM在象征性音乐建模方面的知识空白。

Method: 通过向LLM输入描述不同曲风和风格的文本提示，生成象征性音乐（MIDI 文件），不用专门音乐训练；再用这些生成的数据训练神经网络进行风格和流派分类、旋律补全，并与现有模型进行对比。

Result: LLM可以根据文本推断出初步的音乐结构与时间关系，神经网络在用LLM生成数据训练后能够执行基本的曲风分类和旋律补全任务，其表现具有一定有效性，但也存在因缺乏音乐上下文而带来的局限。

Conclusion: LLM具备从文本推断并编码简单音乐模式的能力，对音乐生成的探索具有启发意义，但要实现高度准确和复杂的音乐生成，还需引入更多专门的音乐知识和数据。

Abstract: Large language models (LLMs) excel at modeling relationships between strings
in natural language and have shown promise in extending to other symbolic
domains like coding or mathematics. However, the extent to which they
implicitly model symbolic music remains underexplored. This paper investigates
how LLMs represent musical concepts by generating symbolic music data from
textual prompts describing combinations of genres and styles, and evaluating
their utility through recognition and generation tasks. We produce a dataset of
LLM-generated MIDI files without relying on explicit musical training. We then
train neural networks entirely on this LLM-generated MIDI dataset and perform
genre and style classification as well as melody completion, benchmarking their
performance against established models. Our results demonstrate that LLMs can
infer rudimentary musical structures and temporal relationships from text,
highlighting both their potential to implicitly encode musical patterns and
their limitations due to a lack of explicit musical context, shedding light on
their generative capabilities for symbolic music.

</details>


### [103] [Are Knowledge and Reference in Multilingual Language Models Cross-Lingually Consistent?](https://arxiv.org/abs/2507.12838)
*Xi Ai,Mahardika Krisna Ihsani,Min-Yen Kan*

Main category: cs.CL

TL;DR: 本文关注于多语言模型在不同语言间传递事实性知识时的一致性，提出了评估和提升跨语言知识一致性的方法，并分析了现有提升多语言性能的策略在一致性上的作用。


<details>
  <summary>Details</summary>
Motivation: 当前多语言模型虽然能够在多种语言间迁移知识，但模型响应在不同语言间往往存在知识不一致的问题，这影响了模型的事实性和各语言间性能公平性。因此，研究如何分析和提升跨语言知识一致性具有重要现实意义。

Method: 作者提出用代码混合（code-mixed）的指代陈述来考察不同语言间知识一致性，并采用可解释性方法分析多语言模型在跨语言环境下的不同行为。同时，系统评估了主流提升多语言表现的训练策略是否同样能提升知识一致性。

Result: 实验发现，现有多语言模型在不同语言族和语言特征下表现出不同程度的一致性，并且在模型的某一特定层存在一致性的瓶颈。常见的多语言训练策略并不总能提升一致性，但采用代码切换训练和跨语言词对齐目标则在提升一致性方面效果最佳。

Conclusion: 跨语言一致性不是自然而然在多语言训练下获得，需通过专门的监督和训练机制如代码切换和对齐来提升，这对于模型在多语言环境下的公平性和知识事实性具有启发意义。

Abstract: Cross-lingual consistency should be considered to assess cross-lingual
transferability, maintain the factuality of the model knowledge across
languages, and preserve the parity of language model performance. We are thus
interested in analyzing, evaluating, and interpreting cross-lingual consistency
for factual knowledge. We examine code-mixed coreferential statements conveyed
identical knowledge across languages to study cross-lingual knowledge
consistency. We use some interpretability approaches to analyze the behavior of
a model in cross-lingual contexts, discovering that multilingual models show
different levels of consistency, subject to language families, linguistic
factors, and a bottleneck in cross-lingual consistency on a particular layer.
In addition, we evaluate common strategies aimed at improving multilingual
performance to observe whether these strategies can improve knowledge
consistency at the same time. While knowledge is not cross-lingual consistency
in many cases, code-switching training and cross-lingual word alignment
objectives show the most promising results, emphasizing the noteworthiness of
cross-lingual alignment supervision and code-switching training for both
multilingual performance and cross-lingual consistency enhancement.

</details>


### [104] [Making Language Model a Hierarchical Classifier and Generator](https://arxiv.org/abs/2507.12930)
*Yihong Wang,Zhonglin Jiang,Ningyuan Xi,Yue Zhao,Qingqing Gu,Xiyuan Chen,Hao Wu,Sheng Xu,Hange Zhou,Yong Chen,Luo Ji*

Main category: cs.CL

TL;DR: 本文提出了一种层次化解码器结构，将解码能力扩展到不同的中间层，并通过实验验证了其在多项任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前主流的解码器（如GPT、LLaMA）仅在最后一层进行解码，低效且不符合人类分层思考的特点，因此作者尝试引入层次化的解码机制。

Method: 作者采用预训练模型，将最后一层的解码头复制到若干中间层，并对不同层进行针对不同任务输入的微调。

Result: 实验显示，被选中的中间层能够生成有意义的文本，且该层次化解码范式在多个文本相关任务上取得了SOTA表现。

Conclusion: 层次化解码结构具有广泛应用潜力，未来可从头预训练实现通用的层次化推理能力。

Abstract: Decoder-only language models, such as GPT and LLaMA, generally decode on the
last layer. Motivated by human's hierarchical thinking capability, we propose
that a hierarchical decoder architecture could be built with different layers
decoding texts simultaneously. Due to limited time and computationally
resources, we choose to adapt a pretrained language model into this form of
hierarchical decoder. Language heads of the last layer are copied to different
selected intermediate layers, and fine-tuned with different task inputs. By
thorough experiments, we validate that these selective intermediate layers
could be adapted to speak meaningful and reasonable contents, and this paradigm
of hierarchical decoder can obtain state-of-the-art performances on multiple
tasks such as hierarchical text classification, classification-guided
generation, and hierarchical text generation. This study suggests the
possibility of a generalized hierarchical reasoner, pretraining from scratch.

</details>


### [105] [MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2507.12981)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Héctor Cerezo-Costas,Pedro Alonso Doval,Jorge Alcalde Vesteiro*

Main category: cs.CL

TL;DR: 本文提出了一种结合生成Python代码的大语言模型（LLM）的方法，用于西班牙语表格问答任务，在IberLEF 2025 PRESTA任务中取得了85%的准确率。


<details>
  <summary>Details</summary>
Motivation: 西班牙语的表格问答是一项具有挑战性的自然语言处理任务，现有方法难以充分理解表格内容并精准生成答案，因此作者希望通过利用代码生成能力提升问答准确性。

Method: 采用多步流程，包括表格内容分析、选择关键字段、生成自然语言指令、将指令转为Python代码、运行代码并处理异常。使用开源LLMs和针对每一步骤优化的prompt。

Result: 所提出方法在PRESTA任务中取得了85%的准确率，表现优异。

Conclusion: 基于LLM的代码生成方法能够有效提升西班牙语表格问答的性能，对于多步骤复杂推理任务具有很大应用潜力。

Abstract: This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas
y Respuestas sobre Tablas en Espa\~nol (Questions and Answers about Tables in
Spanish). Our solution obtains answers to the questions by implementing Python
code generation with LLMs that is used to filter and process the table. This
solution evolves from the MRT implementation for the Semeval 2025 related task.
The process consists of multiple steps: analyzing and understanding the content
of the table, selecting the useful columns, generating instructions in natural
language, translating these instructions to code, running it, and handling
potential errors or exceptions. These steps use open-source LLMs and
fine-grained optimized prompts for each step. With this approach, we achieved
an accuracy score of 85\% in the task.

</details>


### [106] [Formalizing Attack Scenario Description: A Proposed Model](https://arxiv.org/abs/2507.13076)
*Quentin Goux,Nadira Lammari*

Main category: cs.CL

TL;DR: 本文提出了一个新的正式模型，用于形式化攻击场景与上下文的信息，支持自动化攻击分析与脚本生成，助力网络安全自动化。


<details>
  <summary>Details</summary>
Motivation: 自动化网络安全流程日益重要，但实现自动化依赖于对输入数据（如攻击场景）的形式化。论文动机是为此类输入数据建立统一形式化模型，促进攻击分析和仿真等过程的自动化。

Method: 本文设计并提出了一个新的基于UML类模型的正式模型，能够描述攻击的上下文及完整场景。在模型设计完毕后，作者展示了其如何应用于自动化攻击分析和自动生成网络安全训练用攻击脚本的流程中。

Result: 该形式化模型成功地实现了攻击分析流程的上游信息建模和自动生成攻击脚本两大应用案例，展现了其实用性和通用性。

Conclusion: 研究证明，所提出的模型能够为网络安全领域提供标准化、自动化的支持，对攻击分析与安全训练流程带来实际提升，有助于倡导更广泛的网络安全流程自动化实现。

Abstract: Organizations face an ever-changing threat landscape. They must continuously
dedicate significant efforts to protect their assets, making their adoption of
increased cybersecurity automation inevitable. However, process automation
requires formalization of input data. Through this paper, we address this need
for processes that use attack scenarios as input. Among these processes, one
can mention both the generation of scripts for attack simulation and training
purposes, as well as the analysis of attacks. Therefore, the paper's main
research contribution is a novel formal model that encompasses the attack's
context description and its scenario. It is abstracted using UML class model.
Once the description of our model done, we will show how it could serve an
upstream attack analysis process. We will show also its use for an automatic
generation of attack scripts in the context of cybersecurity training. These
two uses cases constitute the second contribution of this present research
work.

</details>


### [107] [SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated Summaries For Scientific Abstracts](https://arxiv.org/abs/2507.13105)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: 本文提出了SemCSE，一种无监督学习科学文本语义表示的方法，通过利用大语言模型（LLM）生成的摘要实施对比学习，以提升学术文本的语义嵌入质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于引文的文本嵌入方法无法准确反映科学文献间的真实语义相似度，亟需新方法更好捕捉科学文本的语义内容。

Method: SemCSE方法利用LLM生成科学摘要，并通过对比学习框架，将语义相近的摘要在嵌入空间中距离拉近，从而训练出能够反映语义内容的嵌入模型。同时提出新基准用于评测模型的语义理解与编码能力。

Result: 在新提出的基准和SciRepEval公开基准集上，SemCSE均展现了优异性能，尤其在同等模型规模下达到了最新最优水平，并实现了较强的语义区分能力。

Conclusion: SemCSE证明了以语义为核心的无监督训练方法在科学文本嵌入上具有显著优势，优于传统方法，为科学文献语义理解提供了更优解决方案。

Abstract: We introduce SemCSE, an unsupervised method for learning semantic embeddings
of scientific texts. Building on recent advances in contrastive learning for
text embeddings, our approach leverages LLM-generated summaries of scientific
abstracts to train a model that positions semantically related summaries closer
together in the embedding space. This resulting objective ensures that the
model captures the true semantic content of a text, in contrast to traditional
citation-based approaches that do not necessarily reflect semantic similarity.
To validate this, we propose a novel benchmark designed to assess a model's
ability to understand and encode the semantic content of scientific texts,
demonstrating that our method enforces a stronger semantic separation within
the embedding space. Additionally, we evaluate SemCSE on the comprehensive
SciRepEval benchmark for scientific text embeddings, where it achieves
state-of-the-art performance among models of its size, thus highlighting the
benefits of a semantically focused training approach.

</details>


### [108] [A Computational Framework to Identify Self-Aspects in Text](https://arxiv.org/abs/2507.13115)
*Jaya Caporusso,Matthew Purver,Senja Pollak*

Main category: cs.CL

TL;DR: 本论文提出开发一个用于识别文本中自我方面的计算框架，涵盖本体构建、数据集标注及多种模型评测，并将模型应用于心理健康等领域。


<details>
  <summary>Details</summary>
Motivation: 自我是一个多维构建体，在语言中有体现，但在自然语言处理领域尚未得到充分探索。心理健康等现象与自我方面密切相关，急需基于NLP的方法进行系统分析。

Method: 首先构建自我方面的本体和金标准标注数据集，然后开发并评估多种模型，包括判别模型、大型生成式语言模型及基于嵌入的检索方法，从可解释性、与真实数据一致性、准确率、计算效率等四项指标进行评估。

Result: 模型将在心理健康和经验现象学两个案例研究中实际应用，验证其效果并展示其潜在价值。

Conclusion: 研究将推动自我相关西文本分析的系统化与自动化，为心理健康等领域带来新的研究工具，同时推动跨学科的自然语言处理应用。

Abstract: This Ph.D. proposal introduces a plan to develop a computational framework to
identify Self-aspects in text. The Self is a multifaceted construct and it is
reflected in language. While it is described across disciplines like cognitive
science and phenomenology, it remains underexplored in natural language
processing (NLP). Many of the aspects of the Self align with psychological and
other well-researched phenomena (e.g., those related to mental health),
highlighting the need for systematic NLP-based analysis. In line with this, we
plan to introduce an ontology of Self-aspects and a gold-standard annotated
dataset. Using this foundation, we will develop and evaluate conventional
discriminative models, generative large language models, and embedding-based
retrieval approaches against four main criteria: interpretability, ground-truth
adherence, accuracy, and computational efficiency. Top-performing models will
be applied in case studies in mental health and empirical phenomenology.

</details>


### [109] [Assessing the Reliability of LLMs Annotations in the Context of Demographic Bias and Model Explanation](https://arxiv.org/abs/2507.13138)
*Hadi Mohammadi,Tina Shahedi,Pablo Mosteiro,Massimo Poesio,Ayoub Bagheri,Anastasia Giachanou*

Main category: cs.CL

TL;DR: 本研究发现，尽管注释者的人口统计特征对性别歧视检测任务中的标注结果有统计意义影响，但其作用远小于文本内容本身，占方差的8%。驱动生成式AI根据人口学特征设定“人格”进行标注，并不能提升AI与真人标注的一致性。真正提升公平性的关键在于内容驱动的解释和加强标注流程。


<details>
  <summary>Details</summary>
Motivation: 随着NLP模型广泛应用于敏感任务，比如性别歧视检测，注释数据的公平性和不偏见性变得尤为重要。该文动因是深入理解注释者人口学特征（如性别、年龄等）对标注一致性的影响，以指导更公平的系统设计。

Method: 作者通过广义线性混合模型(GLMM)分析，量化注释者人口学特征与文本内容对标注方差的贡献。同时，让生成式AI模型根据设定的人格（人口学特征）进行模拟标注，比较不同“人格”配置下AI与人工标注的对齐表现。最后采用可解释性AI方法分析模型的决策依据。

Result: 研究发现，注释者的人口学特征虽然对标注有统计显著影响，但其方差贡献远小于文本内容（仅约8%）。人格设定下的AI标注并未提升，与真人标注的一致性，反而可能降低AI表现。可解释性方法也显示模型主要依赖于与性别歧视内容密切相关的特定词语，而非人口学相关特征。

Conclusion: 作者认为注释者的社会属性影响有限，性别歧视检测的核心还是文本内容本身。与其尝试AI人格模拟，不如着重于文本内容驱动的模型解释和更严谨的标注流程，以提高NLP模型的公平性和可靠性。

Abstract: Understanding the sources of variability in annotations is crucial for
developing fair NLP systems, especially for tasks like sexism detection where
demographic bias is a concern. This study investigates the extent to which
annotator demographic features influence labeling decisions compared to text
content. Using a Generalized Linear Mixed Model, we quantify this inf luence,
finding that while statistically present, demographic factors account for a
minor fraction ( 8%) of the observed variance, with tweet content being the
dominant factor. We then assess the reliability of Generative AI (GenAI) models
as annotators, specifically evaluating if guiding them with demographic
personas improves alignment with human judgments. Our results indicate that
simplistic persona prompting often fails to enhance, and sometimes degrades,
performance compared to baseline models. Furthermore, explainable AI (XAI)
techniques reveal that model predictions rely heavily on content-specific
tokens related to sexism, rather than correlates of demographic
characteristics. We argue that focusing on content-driven explanations and
robust annotation protocols offers a more reliable path towards fairness than
potentially persona simulation.

</details>


### [110] [Feature-based analysis of oral narratives from Afrikaans and isiXhosa children](https://arxiv.org/abs/2507.13164)
*Emma Sharratt,Annelien Smith,Retief Louw,Daleen Klop,Febe de Wet,Herman Kamper*

Main category: cs.CL

TL;DR: 本文利用机器学习分析了南非四五岁儿童的口头叙述技能，发现词汇多样性和平均话语长度可有效区分是否需要干预，而语速类特征作用有限。具体动词和助动词的使用也能辅助区分，有助于多语环境下的早期筛查。


<details>
  <summary>Details</summary>
Motivation: 口头叙述能力预示着后续的识字发展，如何及早、有效地识别需要语言干预的儿童，对于多语言环境尤其重要。现有针对不同语言的评估方法有效性不一，需探讨跨语言可推广的标志性特征。

Method: 研究录制了讲南非荷兰语和科萨语的4-5岁儿童的故事，通过简单的机器学习方法，提取并分析了词汇多样性、话语长度、语速、词性模式等多项特征，对比典型发展儿童与被专家评定需干预儿童间的差异。

Result: 词汇多样性（独特词数）、话语长度（平均话语长度）可区分需干预与典型发展的儿童，语速特征作用较弱。跨语言地，特定动词和助动词的使用与较低的干预需求相关，暴露出两种语言中既有共性预测指标，又存在语言特异性指标。

Conclusion: 口头叙述的词汇与结构特征能作为早期识别语言障碍的有效指标，对多语环境的儿童筛查具有实用价值，推荐将此类分析应用于早期教育与干预决策中。

Abstract: Oral narrative skills are strong predictors of later literacy development.
This study examines the features of oral narratives from children who were
identified by experts as requiring intervention. Using simple machine learning
methods, we analyse recorded stories from four- and five-year-old Afrikaans-
and isiXhosa-speaking children. Consistent with prior research, we identify
lexical diversity (unique words) and length-based features (mean utterance
length) as indicators of typical development, but features like articulation
rate prove less informative. Despite cross-linguistic variation in
part-of-speech patterns, the use of specific verbs and auxiliaries associated
with goal-directed storytelling is correlated with a reduced likelihood of
requiring intervention. Our analysis of two linguistically distinct languages
reveals both language-specific and shared predictors of narrative proficiency,
with implications for early assessment in multilingual contexts.

</details>


### [111] [GEMMAS: Graph-based Evaluation Metrics for Multi Agent Systems](https://arxiv.org/abs/2507.13190)
*Jisoo Lee,Raeyoung Chang,Dongwook Kwon,Harmanpreet Singh,Nikhil Verma*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的基于有向无环图的方法（GEMMAS），用于评估多智能体语言模型系统的内部协作过程，并通过新指标揭示协作效率与传统结果指标间的巨大差距。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统评测大都只关注最终输出的正确性，忽略了内部沟通效率和协作机制；这些被忽视的因素可能导致推理冗余与计算资源浪费。作者希望补全多智能体系统评估的这一短板。

Method: 作者提出GEMMAS框架，将多智能体间的交互编码为有向无环图，并引入两个过程指标：信息多样性分数（IDS）衡量消息间语义多样性，不必要路径比率（UPR）量化冗余推理路径。用这套方法在五个数据集上对多智能体协作系统进行分析。

Result: 在多个基准上评估GEMMAS，尤其是在GSM8K数据集上，发现准确率相差仅2.1%的两个系统在IDS和UPR上分别相差12.8%和80%。显示即便最终表现相近，内部协作效率差异极大。

Conclusion: 仅观察最终结果无法全面评估多智能体系统；过程层级的诊断对于可解释性和资源高效的协作式AI设计至关重要。

Abstract: Multi-agent systems built on language models have shown strong performance on
collaborative reasoning tasks. However, existing evaluations focus only on the
correctness of the final output, overlooking how inefficient communication and
poor coordination contribute to redundant reasoning and higher computational
costs. We introduce GEMMAS, a graph-based evaluation framework that analyzes
the internal collaboration process by modeling agent interactions as a directed
acyclic graph. To capture collaboration quality, we propose two process-level
metrics: Information Diversity Score (IDS) to measure semantic variation in
inter-agent messages, and Unnecessary Path Ratio (UPR) to quantify redundant
reasoning paths. We evaluate GEMMAS across five benchmarks and highlight
results on GSM8K, where systems with only a 2.1% difference in accuracy differ
by 12.8% in IDS and 80% in UPR, revealing substantial variation in internal
collaboration. These findings demonstrate that outcome-only metrics are
insufficient for evaluating multi-agent performance and highlight the
importance of process-level diagnostics in designing more interpretable and
resource-efficient collaborative AI systems.

</details>


### [112] [Automatically assessing oral narratives of Afrikaans and isiXhosa children](https://arxiv.org/abs/2507.13205)
*R. Louw,E. Sharratt,F. de Wet,C. Jacobs,A. Smith,H. Kamper*

Main category: cs.CL

TL;DR: 本文提出了一种自动评估南非学前儿童口语叙述能力的系统，可辅助教师识别需干预的学生。


<details>
  <summary>Details</summary>
Motivation: 学前儿童的叙述与理解能力对后期读写能力至关重要。但由于班级人数较多，教师难以及时准确识别需要干预的学生，因此需要自动化评估工具。

Method: 系统首先使用自动语音识别（ASR）对学前儿童的口头叙述进行转录，然后通过机器学习评分模型对转录内容进行叙述与理解能力的评分。比较了线性模型与大语言模型（LLM）的评分效果。

Result: LLM模型在大多数情况下优于线性模型，且性能接近人类专家。而简单的线性模型也具有较强的评分能力。

Conclusion: 提出的系统为学前班提供了自动化口语评估的技术基础，有助于教师将更多精力投入到个性化支持和干预，有望提升儿童的学习效果。

Abstract: Developing narrative and comprehension skills in early childhood is critical
for later literacy. However, teachers in large preschool classrooms struggle to
accurately identify students who require intervention. We present a system for
automatically assessing oral narratives of preschool children in Afrikaans and
isiXhosa. The system uses automatic speech recognition followed by a machine
learning scoring model to predict narrative and comprehension scores. For
scoring predicted transcripts, we compare a linear model to a large language
model (LLM). The LLM-based system outperforms the linear model in most cases,
but the linear system is competitive despite its simplicity. The LLM-based
system is comparable to a human expert in flagging children who require
intervention. We lay the foundation for automatic oral assessments in
classrooms, giving teachers extra capacity to focus on personalised support for
children's learning.

</details>


### [113] [Enhancing Cross-task Transfer of Large Language Models via Activation Steering](https://arxiv.org/abs/2507.13236)
*Xinyu Tang,Zhihao Lv,Xiaoxue Cheng,Junyi Li,Wayne Xin Zhao,Zujie Wen,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为CAST的跨任务激活引导迁移框架，无需修改模型参数或扩展输入，通过操纵大型语言模型（LLM）的内部激活状态，实现高效的跨任务知识迁移，并在多项实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM通过提示可利用预训练知识，但在面对新任务及数据稀缺情况下表现不佳。现有的跨任务上下文学习在鲁棒性、可扩展性和效率方面仍有不足，促使作者探索无需参数更新的新型跨任务迁移方法。

Method: 作者分析了LLM潜在空间的激活模式，发现上下文示例引发的激活在不同任务间有一致性。据此，提出CAST方法：先从高资源任务选取有代表性样本，利用这些样本生成对比表示增强的激活，对模型的激活状态进行操控，把知识迁移到低资源任务。整个过程中无需修改模型参数或扩展输入。

Result: 在多项跨领域、跨语言的任务迁移实验中，CAST均显著优于主流对比方法，同时具备更好的可扩展性和更低的计算成本。

Conclusion: 通过激活状态操控实现跨任务知识迁移是一种高效、经济且有效的方案，CAST为数据稀缺场景下LLM适应性提供了新的方向。

Abstract: Large language models (LLMs) have shown impressive abilities in leveraging
pretrained knowledge through prompting, but they often struggle with unseen
tasks, particularly in data-scarce scenarios. While cross-task in-context
learning offers a direct solution for transferring knowledge across tasks, it
still faces critical challenges in terms of robustness, scalability, and
efficiency. In this paper, we investigate whether cross-task transfer can be
achieved via latent space steering without parameter updates or input
expansion. Through an analysis of activation patterns in the latent space of
LLMs, we observe that the enhanced activations induced by in-context examples
have consistent patterns across different tasks. Inspired by these findings, we
propose CAST, a novel Cross-task Activation Steering Transfer framework that
enables effective transfer by manipulating the model's internal activation
states. Our approach first selects influential and diverse samples from
high-resource tasks, then utilizes their contrastive representation-enhanced
activations to adapt LLMs to low-resource tasks. Extensive experiments across
both cross-domain and cross-lingual transfer settings show that our method
outperforms competitive baselines and demonstrates superior scalability and
lower computational costs.

</details>


### [114] [HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language Models](https://arxiv.org/abs/2507.13238)
*Ashray Gupta,Rohan Joseph,Sunny Rai*

Main category: cs.CL

TL;DR: 本文提出了首个涵盖印地语类比推理能力的测试集（HATS），并在多语言大模型上进行基准测试，结合链式思考策略显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在英语推理能力评估上较为充分，但对于印地语等印度本地语言的推理能力研究匮乏，缺少对应的评价资源，难以判断模型是否具有跨语言泛化能力。

Method: 作者构建了包含405道政府考试中选取的印地语选择题的类比测试集HATS，并用多种提示策略（包括一种借鉴认知理论的“有依据链式思考”）对主流多语言大模型进行评测。

Result: 实验证明，链式思考方法提升了模型在印地语类比题上的表现，且无论提示策略如何，英文提示下模型表现最佳。

Conclusion: HATS测试集填补了评估印地语推理能力的重要空白，为未来提升多语言大模型的推理泛化能力提供了基准和参考。

Abstract: Analogies test a model's ability to infer implicit relationships between
concepts, making them a key benchmark for evaluating reasoning capabilities.
While large language models (LLMs) are widely evaluated for reasoning in
English, their abilities in Indic languages remain understudied, limiting our
understanding of whether these models generalize across languages. To address
this gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405
multiple-choice questions sourced from Indian government exams. We benchmark
state-of-the-art multilingual LLMs using various prompting strategies and
introduce a grounded Chain of Thought approach that leverages cognitive
theories of analogical reasoning. This approach improves model performance on
Hindi analogy questions. Our experiments show that models perform best with
English prompts, irrespective of the prompting strategy. Our test set addresses
the lack of a critical resource to evaluate LLM reasoning capabilities in
Hindi.

</details>


### [115] [Automating Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2507.13255)
*Lyucheng Wu,Mengru Wang,Ziwen Xu,Tri Cao,Nay Oo,Bryan Hooi,Shumin Deng*

Main category: cs.CL

TL;DR: AutoSteer是一种用于多模态大模型安全防护的推理时自适应干预技术，无需对原模型进行微调，通过多种机制有效降低模型在面对攻击时输出有害内容的概率。


<details>
  <summary>Details</summary>
Motivation: 近年来多模态大语言模型（MLLMs）强大的跨模态推理能力带来了安全隐患，尤其是对抗性输入时易产生有害输出。因此迫切需要提升推理阶段的安全性。

Method: 提出AutoSteer，包括三大模块：1) 安全感知分数（SAS），自动定位模型内部与安全性最强相关的层级；2) 自适应安全探测器，基于中间特征预测有害输出概率；3) 轻量级Refusal Head，在检测到安全风险时，实时干预调控生成。整个方案无需改变或微调原始模型。

Result: 在LLaVA-OV 和 Chameleon 等多模态安全基准测试中，AutoSteer能大幅降低文字、视觉和跨模态威胁的攻击成功率（ASR），同时不损失原有能力。

Conclusion: AutoSteer为多模态大模型的安全部署提供了一种实用、易解释且有效的推理时干预框架。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has unlocked
powerful cross-modal reasoning abilities, but also raised new safety concerns,
particularly when faced with adversarial multimodal inputs. To improve the
safety of MLLMs during inference, we introduce a modular and adaptive
inference-time intervention technology, AutoSteer, without requiring any
fine-tuning of the underlying model. AutoSteer incorporates three core
components: (1) a novel Safety Awareness Score (SAS) that automatically
identifies the most safety-relevant distinctions among the model's internal
layers; (2) an adaptive safety prober trained to estimate the likelihood of
toxic outputs from intermediate representations; and (3) a lightweight Refusal
Head that selectively intervenes to modulate generation when safety risks are
detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical
benchmarks demonstrate that AutoSteer significantly reduces the Attack Success
Rate (ASR) for textual, visual, and cross-modal threats, while maintaining
general abilities. These findings position AutoSteer as a practical,
interpretable, and effective framework for safer deployment of multimodal AI
systems.

</details>


### [116] [QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation](https://arxiv.org/abs/2507.13266)
*Jiazheng Li,Hong Lu,Kaiyue Wen,Zaiwen Yang,Jiaxuan Gao,Hongzhou Lin,Yi Wu,Jingzhao Zhang*

Main category: cs.CL

TL;DR: 本文提出通过问题增强（Question Augmentation）策略，向大语言模型在强化学习训练中引入部分解答，以提升其在多步推理难题上的表现，取得了显著的效果提升。


<details>
  <summary>Details</summary>
Motivation: 近期研究质疑了强化学习对于提升大语言模型多步推理能力的有效性，尤其是在面对难题时表现不佳。

Method: 作者提出一种简单有效的方法——问题增强（Question Augmentation），在训练过程中加入问题的部分解答，从而降低问题难度、提供更丰富的学习信号。将该方法（QuestA）应用到数学推理任务中的RL训练。

Result: QuestA方法提升了通过率（pass@1和pass@k），尤其是在标准RL方法难以提升的难题上。基于1.5B参数模型，在AIME24、AIME25和HMMT25等数学基准任务上取得了新的SOTA表现。

Conclusion: QuestA能提高样本效率，为通过强化学习拓展大语言模型推理能力提供了可行且通用的实践路径，并具有理论支持。

Abstract: Reinforcement learning (RL) has become a key component in training large
language reasoning models (LLMs). However, recent studies questions its
effectiveness in improving multi-step reasoning-particularly on hard problems.
To address this challenge, we propose a simple yet effective strategy via
Question Augmentation: introduce partial solutions during training to reduce
problem difficulty and provide more informative learning signals. Our method,
QuestA, when applied during RL training on math reasoning tasks, not only
improves pass@1 but also pass@k-particularly on problems where standard RL
struggles to make progress. This enables continual improvement over strong
open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing
their reasoning capabilities. We achieve new state-of-the-art results on math
benchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%)
on AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical
explanations that QuestA improves sample efficiency, offering a practical and
generalizable pathway for expanding reasoning capability through RL.

</details>


### [117] [Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management](https://arxiv.org/abs/2507.13275)
*Luis Gasco,Hermenegildo Fabregat,Laura García-Sardiña,Paula Estrella,Daniel Deniz,Alvaro Rodrigo,Rabih Zbib*

Main category: cs.CL

TL;DR: 本文介绍了TalentCLEF 2025，这是人力资本管理领域首个针对技能和职位名称智能的评测活动，旨在推动公共基准数据的建立，促进公平可靠模型的发展。


<details>
  <summary>Details</summary>
Motivation: 尽管自然语言处理与大语言模型在人才招聘和劳动力规划等方面具备巨大潜力，但缺乏公开可用的评测数据和基准，限制了该领域技术的健康发展。

Method: 本文发起了TalentCLEF 2025评测，包括两个任务：A任务为多语言职位名称匹配（涵盖英、西、德、中四种语言）；B任务为基于职位名称的技能预测（英语）。使用去标识、人工标注的真实求职数据，同时关注多语言和性别偏见的评测。共吸引76支队伍、280余次投稿，主流方法为多语言编码器结合对比学习，以及大语言模型用于数据增强或再排序。

Result: 实验表明，训练策略对结果的影响大于模型规模，验证了多样方法的有效性和缺陷，并提供了首次公开领域基准。

Conclusion: TalentCLEF推动了劳动力市场语言系统的公开、可复现、公平性评测，加速了健壮、可迁移的智能系统的发展。

Abstract: Advances in natural language processing and large language models are driving
a major transformation in Human Capital Management, with a growing interest in
building smart systems based on language technologies for talent acquisition,
upskilling strategies, and workforce planning. However, the adoption and
progress of these technologies critically depend on the development of reliable
and fair models, properly evaluated on public data and open benchmarks, which
have so far been unavailable in this domain.
  To address this gap, we present TalentCLEF 2025, the first evaluation
campaign focused on skill and job title intelligence. The lab consists of two
tasks: Task A - Multilingual Job Title Matching, covering English, Spanish,
German, and Chinese; and Task B - Job Title-Based Skill Prediction, in English.
Both corpora were built from real job applications, carefully anonymized, and
manually annotated to reflect the complexity and diversity of real-world labor
market data, including linguistic variability and gender-marked expressions.
  The evaluations included monolingual and cross-lingual scenarios and covered
the evaluation of gender bias.
  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most
systems relied on information retrieval techniques built with multilingual
encoder-based models fine-tuned with contrastive learning, and several of them
incorporated large language models for data augmentation or re-ranking. The
results show that the training strategies have a larger effect than the size of
the model alone. TalentCLEF provides the first public benchmark in this field
and encourages the development of robust, fair, and transferable language
technologies for the labor market.

</details>


### [118] [Multi-Agent Synergy-Driven Iterative Visual Narrative Synthesis](https://arxiv.org/abs/2507.13285)
*Wang Xi,Quan Shi,Tian Yu,Yujie Peng,Jiayi Sun,Mengxing Ren,Zenghui Ding,Ningguang Yao*

Main category: cs.CL

TL;DR: 本论文提出RCPS（反思性连贯演示合成）框架，通过改进内容提取、叙事规划和视觉设计，大幅提升自动化高质量媒体演示文稿的生成效果，并引入PREVAL评价体系验证结果可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有自动生成媒体演示文稿的方法存在逻辑不一致、布局不理想等问题，难以达到专业标准，因此亟需新的方法提升演示文稿的质量和一致性。

Method: 作者提出RCPS框架，包含三大组件：一是深度结构化叙事规划，二是自适应布局生成，三是迭代优化环。同时提出PREVAL评价框架，结合多维模型和增强解释的偏好评估来衡量演示的内容、连贯性与设计等维度。

Result: 实验证明RCPS在所有质量维度上均显著优于主流基线方法，生成的演示文稿质量接近人类专家水平。同时，PREVAL评估结果与真实人工评价高度相关，验证了其作为演示质量自动化评估工具的有效性。

Conclusion: RCPS及PREVAL提供了一套高效的自动化高质量演示文稿生成与评估方案，有助于推动实际自动化演示制作达到专业标准。

Abstract: Automated generation of high-quality media presentations is challenging,
requiring robust content extraction, narrative planning, visual design, and
overall quality optimization. Existing methods often produce presentations with
logical inconsistencies and suboptimal layouts, thereby struggling to meet
professional standards. To address these challenges, we introduce RCPS
(Reflective Coherent Presentation Synthesis), a novel framework integrating
three key components: (1) Deep Structured Narrative Planning; (2) Adaptive
Layout Generation; (3) an Iterative Optimization Loop. Additionally, we propose
PREVAL, a preference-based evaluation framework employing rationale-enhanced
multi-dimensional models to assess presentation quality across Content,
Coherence, and Design. Experimental results demonstrate that RCPS significantly
outperforms baseline methods across all quality dimensions, producing
presentations that closely approximate human expert standards. PREVAL shows
strong correlation with human judgments, validating it as a reliable automated
tool for assessing presentation quality.

</details>


### [119] [AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research](https://arxiv.org/abs/2507.13300)
*Yilun Zhao,Weiyuan Chen,Zhijian Xu,Manasi Patwardhan,Yixin Liu,Chengye Wang,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了AbGen，这是首个专用于评估大语言模型（LLM）在科学研究中设计消融实验能力的基准数据集。通过对主流LLM的测试，发现其在消融实验设计方面与人类专家有显著差距，同时现有自动化评测方法对该任务不可靠。为此，作者还开发了AbGen-Eval，用于评估自动化评测系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在科学任务中的能力评价不充分，尤其在复杂实验设计（如消融实验）方面缺乏专门评测基准。针对消融实验设计的能力评估十分关键，因为它体现了对科学研究细节和严谨性的深刻理解。

Method: 作者创建并发布了AbGen基准集，包含1,500组由专家注释的消融实验设计样例，覆盖807篇NLP论文，任务为根据给定研究背景为特定模块生成消融实验方案。然后对目前领先的LLM如DeepSeek-R1-0528和o4-mini进行了评测，并比较了自动化评测与人工评测之间的一致性。为此还构建了AbGen-Eval数据集，专门测试自动化评测手段的有效性。

Result: 实验显示，当前LLM相较人类专家在消融实验设计的重要性、真实度和合理性方面存在显著落差。同时证明目前通用的自动化评测方法在该任务中的数据与人工评测结果有明显差异。

Conclusion: AbGen为LLM在消融实验设计上的评估提供了标准，实现了更细致的能力刻画。现有技术与评测标准尚有不足，未来需要更优的LLM模型和自动化评测方法，AbGen和AbGen-Eval为相关研究提供了重要基础。

Abstract: We introduce AbGen, the first benchmark designed to evaluate the capabilities
of LLMs in designing ablation studies for scientific research. AbGen consists
of 1,500 expert-annotated examples derived from 807 NLP papers. In this
benchmark, LLMs are tasked with generating detailed ablation study designs for
a specified module or process based on the given research context. Our
evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a
significant performance gap between these models and human experts in terms of
the importance, faithfulness, and soundness of the ablation study designs.
Moreover, we demonstrate that current automated evaluation methods are not
reliable for our task, as they show a significant discrepancy when compared to
human assessment. To better investigate this, we develop AbGen-Eval, a
meta-evaluation benchmark designed to assess the reliability of commonly used
automated evaluation systems in measuring LLM performance on our task. We
investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for
future research on developing more effective and reliable LLM-based evaluation
systems for complex scientific tasks.

</details>


### [120] [HapticCap: A Multimodal Dataset and Task for Understanding User Experience of Vibration Haptic Signals](https://arxiv.org/abs/2507.13318)
*Guimin Hu,Daniel Hershcovich,Hasti Seifi*

Main category: cs.CL

TL;DR: 本文提出了一个多模态数据集和匹配用户描述与振动触觉信号的新任务，创建了第一个全人类标注的触觉描述数据集HapticCap，并系统评估了文本与振动信号匹配的模型。


<details>
  <summary>Details</summary>
Motivation: 触觉信号能有效传递信息，但要让用户产生有意义的共鸣非常难。关键难题在于大规模带文本描述的触觉振动数据集缺乏，以及现有模型难以用文本准确描述振动信号。

Method: 作者创建了名为HapticCap的大型触觉-文本数据集，包含92,070个人工标注的振动-文本描述对，涵盖感官、情感及联想描述。提出了触觉描述检索任务，并基于对比学习框架，结合T5语言模型和AST音频模型，测试文本与振动信号的匹配表现。

Result: T5与AST模型组合在触觉描述检索中表现最佳，尤其是在针对每类描述单独训练时效果更好。

Conclusion: 该研究首次提供了大规模、全人工标注的触觉描述数据集及基准，为触觉信号和自然语言的跨模态研究提供了宝贵资源和基础方法，有望推动触觉人机交互的智能化发展。

Abstract: Haptic signals, from smartphone vibrations to virtual reality touch feedback,
can effectively convey information and enhance realism, but designing signals
that resonate meaningfully with users is challenging. To facilitate this, we
introduce a multimodal dataset and task, of matching user descriptions to
vibration haptic signals, and highlight two primary challenges: (1) lack of
large haptic vibration datasets annotated with textual descriptions as
collecting haptic descriptions is time-consuming, and (2) limited capability of
existing tasks and models to describe vibration signals in text. To advance
this area, we create HapticCap, the first fully human-annotated
haptic-captioned dataset, containing 92,070 haptic-text pairs for user
descriptions of sensory, emotional, and associative attributes of vibrations.
Based on HapticCap, we propose the haptic-caption retrieval task and present
the results of this task from a supervised contrastive learning framework that
brings together text representations within specific categories and vibrations.
Overall, the combination of language model T5 and audio model AST yields the
best performance in the haptic-caption retrieval task, especially when
separately trained for each description category.

</details>


### [121] [Social and Political Framing in Search Engine Results](https://arxiv.org/abs/2507.13325)
*Amrit Poudel,Tim Weninger*

Main category: cs.CL

TL;DR: 本文研究了搜索引擎如何影响搜索结果的偏见，尤其是搜索引擎自身和带有意识形态倾向的用户查询如何共同加剧信息偏见与极化。


<details>
  <summary>Details</summary>
Motivation: 虽然已有大量关于搜索引擎偏见的研究，但关于搜索引擎与用户带有意识形态倾向的查询如何共同作用于搜索结果偏见的问题，仍然较少被系统探讨。

Method: 作者通过收集并分析主流搜索引擎在政治与社会话题上的搜索结果，研究搜索引擎对内容的排序方式以及用户带有意识形态的查询如何影响这些排序。还比较了不同搜索引擎在信息来源优先级上的差别。

Result: 研究发现，搜索引擎不但在内容排序上体现出固有偏见，而且用户的意识形态性强的查询会进一步加剧这种偏见，放大特定叙事。不同搜索引擎在优先展示的信息源上存在显著差异。

Conclusion: 搜索引擎通过内容排序和对特定用户查询的响应，强化了信息偏见和意识形态分化，对塑造公众认知和信息极化起到了关键作用。

Abstract: Search engines play a crucial role in shaping public discourse by influencing
how information is accessed and framed. While prior research has extensively
examined various dimensions of search bias -- such as content prioritization,
indexical bias, political polarization, and sources of bias -- an important
question remains underexplored: how do search engines and
ideologically-motivated user queries contribute to bias in search results. This
study analyzes the outputs of major search engines using a dataset of political
and social topics. The findings reveal that search engines not only prioritize
content in ways that reflect underlying biases but also that
ideologically-driven user queries exacerbate these biases, resulting in the
amplification of specific narratives. Moreover, significant differences were
observed across search engines in terms of the sources they prioritize. These
results suggest that search engines may play a pivotal role in shaping public
perceptions by reinforcing ideological divides, thereby contributing to the
broader issue of information polarization.

</details>


### [122] [Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It](https://arxiv.org/abs/2507.13328)
*Yulu Qin,Dheeraj Varghese,Adam Dahlgren Lindström,Lucia Donatelli,Kanishka Misra,Najoung Kim*

Main category: cs.CL

TL;DR: 该论文探讨了视觉-语言（VL）联合训练是否会实质性地改变语言模型中的语言表征。作者发现在处理涉及分类学知识的文本问答任务时，VL模型（VLMs）通常优于仅有文本训练的模型（LMs），但在分类学知识本身的获得上两者差异不明显。VL训练的改进主要体现在任务执行阶段知识的调动上。


<details>
  <summary>Details</summary>
Motivation: 过去关于VL训练能否改善语言模型中语言知识的相关研究结论并不一致，结果较为有限。本文提出新的假设：VL训练可能会改善词汇-概念知识，尤其是在分类学组织方面，并对比了VL模型与纯文本模型在此领域的表现。

Method: 作者构建了纯文本语言模型和VL训练模型的最小比对组，采用依赖分类学理解能力的文本问答任务进行评测，并通过行为分析和表征分析量化两类模型在分类学知识及其任务调动能力上的异同。

Result: 在文本分类学问答任务中，VL模型往往优于纯文本模型。但在分类学知识本身（知识内容）上，两类模型表现无显著差异；主要区别在于它们在面对含有分类学关系与非分类学关系概念时，问题表征的差异。

Conclusion: 额外的VL训练并不会本质性地提升模型获取的分类学知识，但能显著提升其在特定任务（即使任务仅为纯文本）中调动此类知识的能力。

Abstract: Does vision-and-language (VL) training change the linguistic representations
of language models in meaningful ways? Most results in the literature have
shown inconsistent or marginal differences, both behaviorally and
representationally. In this work, we start from the hypothesis that the domain
in which VL training could have a significant effect is lexical-conceptual
knowledge, in particular its taxonomic organization. Through comparing minimal
pairs of text-only LMs and their VL-trained counterparts, we first show that
the VL models often outperform their text-only counterparts on a text-only
question-answering task that requires taxonomic understanding of concepts
mentioned in the questions. Using an array of targeted behavioral and
representational analyses, we show that the LMs and VLMs do not differ
significantly in terms of their taxonomic knowledge itself, but they differ in
how they represent questions that contain concepts in a taxonomic relation vs.
a non-taxonomic relation. This implies that the taxonomic knowledge itself does
not change substantially through additional VL training, but VL training does
improve the deployment of this knowledge in the context of a specific task,
even when the presentation of the task is purely linguistic.

</details>


### [123] [The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner](https://arxiv.org/abs/2507.13332)
*Zhouqi Hua,Wenwei Zhang,Chengqi Lyu,Yuzhe Gu,Songyang Gao,Kuikun Liu,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出了一种基于图灵机模仿学习（TAIL）的方法，通过生成模仿图灵机执行过程的思维链数据，显著提升大语言模型（LLM）在更长序列上的泛化能力，并在多个算法任务上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 在当前Transformer大语言模型中，处理训练时未出现过的长序列任务的能力（长度泛化）受到限制，现有方法多基于数据驱动且任务相关性强、泛化性弱，亟需寻求更通用的解决方案。

Method: 本文提出TAIL方法，利用计算机程序合成模仿图灵机执行过程的思维链数据，将推理步骤线性展开为原子状态，并引入显式内存获取机制，从而减轻捷径学习和数据访问难度。作者还构建了覆盖8类算法、18个任务的合成数据集，系统验证其有效性。

Result: TAIL方法仅用合成数据，就大幅提升了Qwen2.5-7B等模型在不同任务上的长度泛化和整体性能，表现优于现有任务专用方法和DeepSeek-R1。同时实验发现，图灵机的关键机制而非单纯思维方式，是提升泛化能力的核心。

Conclusion: 本文工作表明借助图灵机原理与合成数据训练，可以有效提升大模型推理和泛化能力，为未来LLM基于合成数据的推理学习开辟了新方向。

Abstract: Length generalization, the ability to solve problems of longer sequences than
those observed during training, poses a core challenge of Transformer-based
large language models (LLM). Although existing studies have predominantly
focused on data-driven approaches for arithmetic operations and symbolic
manipulation tasks, these approaches tend to be task-specific with limited
overall performance. To pursue a more general solution, this paper focuses on a
broader case of reasoning problems that are computable, i.e., problems that
algorithms can solve, thus can be solved by the Turing Machine. From this
perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to
improve the length generalization ability of LLMs. TAIL synthesizes
chain-of-thoughts (CoT) data that imitate the execution process of a Turing
Machine by computer programs, which linearly expands the reasoning steps into
atomic states to alleviate shortcut learning and explicit memory fetch
mechanism to reduce the difficulties of dynamic and long-range data access in
elementary operations. To validate the reliability and universality of TAIL, we
construct a challenging synthetic dataset covering 8 classes of algorithms and
18 tasks. Without bells and whistles, TAIL significantly improves the length
generalization ability as well as the performance of Qwen2.5-7B on various
tasks using only synthetic data, surpassing previous methods and DeepSeek-R1.
The experimental results reveal that the key concepts in the Turing Machine,
instead of the thinking styles, are indispensable for TAIL for length
generalization, through which the model exhibits read-and-write behaviors
consistent with the properties of the Turing Machine in their attention layers.
This work provides a promising direction for future research in the learning of
LLM reasoning from synthetic data.

</details>


### [124] [A Survey of Context Engineering for Large Language Models](https://arxiv.org/abs/2507.13334)
*Lingrui Mei,Jiayu Yao,Yuyao Ge,Yiwei Wang,Baolong Bi,Yujun Cai,Jiazhi Liu,Mingyu Li,Zhong-Zhi Li,Duzhen Zhang,Chenlin Zhou,Jiayi Mao,Tianze Xia,Jiafeng Guo,Shenghua Liu*

Main category: cs.CL

TL;DR: 本文提出并系统梳理了“大语言模型（LLMs）上下文工程”的研究领域，明确了其基础组成和集成实现，为未来研究指明方向。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的能力受到推理时上下文信息质量极大影响，简单的提示词设计难以满足复杂任务需求。因此，亟需将上下文的获取、处理与管理系统性地整合优化，推动模型的实际应用。

Method: 论文通过梳理1300余篇相关文献，将上下文工程细分为四个基础环节：上下文检索与生成、上下文处理、上下文管理，并分析它们在RAG、记忆系统、工具增强推理、多智能体等复杂系统中的集成实现，进而提出统一框架与技术路线图。

Result: 综述发现，当前上下文工程能极大提升模型理解复杂上下文的能力，但在生成高质量、长文本输出方面存在根本性不足，这成为当前研究的主要瓶颈。

Conclusion: 本文为上下文感知AI提供了结构化、系统性综述，指出了研究空白与未来发展优先方向，对推进理论和工程应用具有指导作用。

Abstract: The performance of Large Language Models (LLMs) is fundamentally determined
by the contextual information provided during inference. This survey introduces
Context Engineering, a formal discipline that transcends simple prompt design
to encompass the systematic optimization of information payloads for LLMs. We
present a comprehensive taxonomy decomposing Context Engineering into its
foundational components and the sophisticated implementations that integrate
them into intelligent systems. We first examine the foundational components:
context retrieval and generation, context processing and context management. We
then explore how these components are architecturally integrated to create
sophisticated system implementations: retrieval-augmented generation (RAG),
memory systems and tool-integrated reasoning, and multi-agent systems. Through
this systematic analysis of over 1300 research papers, our survey not only
establishes a technical roadmap for the field but also reveals a critical
research gap: a fundamental asymmetry exists between model capabilities. While
current models, augmented by advanced context engineering, demonstrate
remarkable proficiency in understanding complex contexts, they exhibit
pronounced limitations in generating equally sophisticated, long-form outputs.
Addressing this gap is a defining priority for future research. Ultimately,
this survey provides a unified framework for both researchers and engineers
advancing context-aware AI.

</details>


### [125] [Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour Understanding from Traditional Puns to Topical Jokes](https://arxiv.org/abs/2507.13335)
*Tyler Loakman,William Thorne,Chenghua Lin*

Main category: cs.CL

TL;DR: 本论文研究了大语言模型（LLM）对不同类型幽默的解释能力，并发现现有模型主要在简单笑话上的表现好，面对更复杂的幽默时能力有限。


<details>
  <summary>Details</summary>
Motivation: 尽管幽默形式多样且复杂，过去大多数计算机幽默相关研究仅关注于基于双关的简短笑话，缺乏对其他幽默类型的关注。该研究旨在探究现有LLM在应对不同类型幽默解释任务时的表现差异。

Method: 作者构建了一个包含600个笑话的数据集，覆盖4种笑话类型，包括异形和同形双关、现代网络幽默以及依赖现实世界知识的时事笑话。对所有笑话人工撰写高质量解释，并在此基础上对不同LLM的零样本解释能力进行评估和对比。

Result: 结果显示，无论是基础模型还是带推理增强的模型，都无法对所有类型的笑话生成可靠和充分的解释，尤其是在处理需要世界知识与复杂推理的幽默时表现较差。

Conclusion: 目前主流LLM在理解和解释多样化幽默上的能力有限，现有计算幽默研究过度集中在简单笑话，需要拓展到更复杂、更具现实语境的幽默类型。

Abstract: Humour, as a complex language form, is derived from myriad aspects of life,
whilst existing work on computational humour has focussed almost exclusively on
short pun-based jokes. In this work, we investigate whether the ability of
Large Language Models (LLMs) to explain humour depends on the particular humour
form. We compare models on simple puns and more complex topical humour that
requires knowledge of real-world entities and events. In doing so, we curate a
dataset of 600 jokes split across 4 joke types and manually write high-quality
explanations. These jokes include heterographic and homographic puns,
contemporary internet humour, and topical jokes, where understanding relies on
reasoning beyond "common sense", rooted instead in world knowledge regarding
news events and pop culture. Using this dataset, we compare the zero-shot
abilities of a range of LLMs to accurately and comprehensively explain jokes of
different types, identifying key research gaps in the task of humour
explanation. We find that none of the tested models (inc. reasoning models) are
capable of reliably generating adequate explanations of all joke types, further
highlighting the narrow focus of most works in computational humour on overly
simple joke forms.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [126] [Physically Based Neural LiDAR Resimulation](https://arxiv.org/abs/2507.12489)
*Richard Marcus,Marc Stamminger*

Main category: cs.RO

TL;DR: 本文提出了一种在LiDAR仿真和大规模3D场景重建中的新颖视角合成(NVS)方法，显式建模了传感器特性，提升了仿真精度。


<details>
  <summary>Details</summary>
Motivation: 虽然已有NVS方法提升了渲染速度并能处理动态场景，但针对LiDAR传感器特有的影响(如滚动快门、激光功率变化和强度衰减)研究不足。本文旨在解决这些问题，提高仿真真实性。

Method: 方法上，作者通过对LiDAR传感器的关键特性(滚动快门效应、激光功率变化、激光强度衰减)进行显式建模，提升了仿真的准确性。他们还通过与现有方法的定量和定性对比、消融实验，分析了每个模型组件的重要性。

Result: 通过实验，作者的方法在准确性和细节还原上都优于当前主流技术。在再仿真能力方面，该方法能生成更高分辨率、准确性更高的LiDAR扫描数据，尤其是在摄像头视角下。

Conclusion: 本文提出的方法在LiDAR模拟领域具有更高的准确性和灵活性，对后续高精度仿真和相关数据集构建有重要推动作用。

Abstract: Methods for Novel View Synthesis (NVS) have recently found traction in the
field of LiDAR simulation and large-scale 3D scene reconstruction. While
solutions for faster rendering or handling dynamic scenes have been proposed,
LiDAR specific effects remain insufficiently addressed. By explicitly modeling
sensor characteristics such as rolling shutter, laser power variations, and
intensity falloff, our method achieves more accurate LiDAR simulation compared
to existing techniques. We demonstrate the effectiveness of our approach
through quantitative and qualitative comparisons with state-of-the-art methods,
as well as ablation studies that highlight the importance of each sensor model
component. Beyond that, we show that our approach exhibits advanced
resimulation capabilities, such as generating high resolution LiDAR scans in
the camera perspective.
  Our code and the resulting dataset are available at
https://github.com/richardmarcus/PBNLiDAR.

</details>


### [127] [FOUNDER: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making](https://arxiv.org/abs/2507.12496)
*Yucen Wang,Rui Yu,Shenghua Wan,Le Gan,De-Chuan Zhan*

Main category: cs.RO

TL;DR: 本文提出FOUNDER框架，将基础模型（FMs）和世界模型（WMs）结合，实现无奖励开放式任务求解，并在多任务视觉控制基准上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 基础模型在知识泛化上优秀，世界模型在动态建模上有优势。现实任务常含复杂观测和领域间差异，传统方法难以应对。打造结合两者优势、无需奖励信号的任务求解体系，成为需求。

Method: 1) 学习一个映射函数，将基础模型的表示落地到世界模型的状态空间，从观测中推断物理状态；2) 利用该映射和行为学习期间的想象，实现基于目标的行为策略学习，将映射得到的任务作为目标状态；3) 利用预测到目标状态的时间距离作为奖励信号，指导策略学习。

Result: 在多任务的离线视觉控制任务基准上，FOUNDER表现优于现有方法，尤其擅长对应复杂观测或领域差异较大的场景，对文本或视频指定的任务具有更深层次语义理解能力。此外，其学习的奖励函数与真实奖励高度一致。

Conclusion: FOUNDER 能够融合基础模型知识与世界模型动态建模能力，实现无需奖励信号的开放式任务泛化，为多任务视觉控制提供有效方案，方案表现优异且具有良好的泛化能力。

Abstract: Foundation Models (FMs) and World Models (WMs) offer complementary strengths
in task generalization at different levels. In this work, we propose FOUNDER, a
framework that integrates the generalizable knowledge embedded in FMs with the
dynamic modeling capabilities of WMs to enable open-ended task solving in
embodied environments in a reward-free manner. We learn a mapping function that
grounds FM representations in the WM state space, effectively inferring the
agent's physical states in the world simulator from external observations. This
mapping enables the learning of a goal-conditioned policy through imagination
during behavior learning, with the mapped task serving as the goal state. Our
method leverages the predicted temporal distance to the goal state as an
informative reward signal. FOUNDER demonstrates superior performance on various
multi-task offline visual control benchmarks, excelling in capturing the
deep-level semantics of tasks specified by text or videos, particularly in
scenarios involving complex observations or domain gaps where prior methods
struggle. The consistency of our learned reward function with the ground-truth
reward is also empirically validated. Our project website is
https://sites.google.com/view/founder-rl.

</details>


### [128] [ReAL-AD: Towards Human-Like Reasoning in End-to-End Autonomous Driving](https://arxiv.org/abs/2507.12499)
*Yuhang Lu,Jiadong Tu,Yuexin Ma,Xinge Zhu*

Main category: cs.RO

TL;DR: 提出了一种新的人类认知启发的自动驾驶端到端学习框架（ReAL-AD），通过引入视觉-语言模型和三层人类认知结构提升规划与安全性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法通常依赖固定、稀疏的轨迹监督，难以捕捉人类司机自然而然的分层推理过程，导致适应性和信息表达能力有限。

Method: 提出基于三层人类认知模型（驾驶策略、驾驶决策、驾驶操作），并结合视觉-语言模型增强情境理解与推理能力的框架。设计了（1）战略推理注入器，用VLM解释交通情境制定高层策略；（2）战术推理整合器，将策略意图细化为可解释战术选择（如换道、超车等）；（3）分层轨迹解码器，将战术决策逐步转化为精细控制动作。

Result: 在广泛评估中，该框架将规划准确性和安全性提升了30%以上。

Conclusion: 该方法使得端到端自动驾驶更具可解释性，更贴合人类分层推理过程，推动自动驾驶技术向更安全和高效方向发展。

Abstract: End-to-end autonomous driving has emerged as a promising approach to unify
perception, prediction, and planning within a single framework, reducing
information loss and improving adaptability. However, existing methods often
rely on fixed and sparse trajectory supervision, limiting their ability to
capture the hierarchical reasoning process that human drivers naturally employ.
To bridge this gap, we propose ReAL-AD, a Reasoning-Augmented Learning
framework that structures decision-making in autonomous driving based on the
three-tier human cognitive model: Driving Strategy, Driving Decision, and
Driving Operation, where Vision-Language Models (VLMs) are incorporated to
enhance situational awareness and structured reasoning across these levels.
Specifically, we introduce: (1) the Strategic Reasoning Injector, which
formulates high-level driving strategies by interpreting complex traffic
contexts from VLM-generated insights; (2) the Tactical Reasoning Integrator,
which refines strategic intent into interpretable tactical choices such as lane
changes, overtaking, and speed adjustments; and (3) the Hierarchical Trajectory
Decoder, which progressively translates tactical decisions into precise control
actions for smooth and human-like trajectory execution. Extensive evaluations
show that integrating our framework improves planning accuracy and safety by
over 30%, making end-to-end autonomous driving more interpretable and aligned
with human-like hierarchical reasoning. The project page can be found at:
\href{https://4dvlab.github.io/project_page/realad}{\texttt{4dvlab.github.io/project\_page/realad}}

</details>


### [129] [VLMgineer: Vision Language Models as Robotic Toolsmiths](https://arxiv.org/abs/2507.12644)
*George Jiayuan Gao,Tianyu Li,Junyao Shi,Yihan Li,Zizhe Zhang,Nadia Figueroa,Dinesh Jayaraman*

Main category: cs.RO

TL;DR: 该论文提出VLMgineer框架，结合视觉语言模型和进化搜索，实现自动化工具设计及操作方案生成，并在实际任务基准上优于人类和传统方法。


<details>
  <summary>Details</summary>
Motivation: 工具的设计和使用是衡量生物智能的重要指标。当前机器人智能研究多侧重于优化控制器，但通过智能工具的发明，可以转移解决问题的负担，通过工具创新更高效地应对实际难题。随着大模型的普及，其常识推理和创造力强，作者探索能否用其自动生成创新工具设计及操作方法。

Method: 提出VLMgineer框架，将视觉语言模型的代码生成能力与进化算法相结合，联合迭代设计物理工具和操作计划，以执行指定任务。作者构建了一个包含多种日常任务的新基准评测集，用于测试工具自动设计解决实际操作的能力。

Result: VLMgineer能针对多样化的日常任务，稳定地发现创新且有效的工具及使用策略，显著简化了复杂机器人的解决方案，并优于人类设计和基于VLM的人类规格设计工具。

Conclusion: 自动化工具发明通过VLMgineer极大提升了解决实际操作难题的效率和创新性，具有超越传统人类及自动方法的潜力。作者将开放基准和代码以促进后续研究。

Abstract: Tool design and use reflect the ability to understand and manipulate the
physical world through creativity, planning, and foresight. As such, these
capabilities are often regarded as measurable indicators of intelligence across
biological species. While much of today's research on robotic intelligence
focuses on generating better controllers, inventing smarter tools offers a
complementary form of physical intelligence: shifting the onus of
problem-solving onto the tool's design. Given the vast and impressive
common-sense, reasoning, and creative capabilities of today's foundation
models, we investigate whether these models can provide useful priors to
automatically design and effectively wield such tools? We present VLMgineer, a
framework that harnesses the code generation abilities of vision language
models (VLMs) together with evolutionary search to iteratively co-design
physical tools and the action plans that operate them to perform a task. We
evaluate VLMgineer on a diverse new benchmark of everyday manipulation
scenarios that demand creative tool design and use. Across this suite,
VLMgineer consistently discovers tools and policies that solve tasks more
effectively and innovatively, transforming challenging robotics problems into
straightforward executions. It also outperforms VLM-generated designs from
human specifications and existing human-crafted tools for everyday tasks. To
facilitate future research on automated tool invention, we will release our
benchmark and code.

</details>


### [130] [MoistureMapper: An Autonomous Mobile Robot for High-Resolution Soil Moisture Mapping at Scale](https://arxiv.org/abs/2507.12716)
*Nathaniel Rose,Hannah Chuang,Manuel A Andrade-Rodriguez,Rishi Parashar,Dani Or,Parikshit Maini*

Main category: cs.RO

TL;DR: 该论文提出了一种基于自主移动机器人的土壤水分传感解决方案，通过自适应采样大幅降低部署和运行成本，实验表明能有效提升采样效率和测图精度。


<details>
  <summary>Details</summary>
Motivation: 现有土壤水分检测方法在高分辨率、大范围的应用（如农业灌溉）中，部署成本高且不适用于规模化，因此急需更经济智能的检测方案。

Method: 作者设计并实现了一台自主移动机器人MoistureMapper，配备TDR传感器与土壤直推进钻机制，可自动采集土壤水分数据。提出采用基于高斯过程建模的多种自适应采样策略，实现对土壤水分空间分布的智能映射，对比常规贪婪采样方案，开展了大规模仿真与实地验证。

Result: 自适应采样方法比贪婪采样基线方案能减少高达30%的移动路径距离，同时在重建土壤水分分布图时方差降低5%。

Conclusion: 该移动机器人结合自适应采样技术，不仅降低了土壤水分检测部署与运行成本，还提升了采样效率和重建精度，展现了面向农业与环境监测领域的应用前景。

Abstract: Soil moisture is a quantity of interest in many application areas including
agriculture and climate modeling. Existing methods are not suitable for scale
applications due to large deployment costs in high-resolution sensing
applications such as for variable irrigation. In this work, we design, build
and field deploy an autonomous mobile robot, MoistureMapper, for soil moisture
sensing. The robot is equipped with Time Domain Reflectometry (TDR) sensors and
a direct push drill mechanism for deploying the sensor to measure volumetric
water content in the soil. Additionally, we implement and evaluate multiple
adaptive sampling strategies based on a Gaussian Process based modeling to
build a spatial mapping of moisture distribution in the soil. We present
results from large scale computational simulations and proof-of-concept
deployment on the field. The adaptive sampling approach outperforms a greedy
benchmark approach and results in up to 30\% reduction in travel distance and
5\% reduction in variance in the reconstructed moisture maps. Link to video
showing field experiments: https://youtu.be/S4bJ4tRzObg

</details>


### [131] [Learning to Predict Mobile Robot Stability in Off-Road Environments](https://arxiv.org/abs/2507.12731)
*Nathaniel Rose,Arif Ahmed,Emanuel Gutierrez-Cornejo,Parikshit Maini*

Main category: cs.RO

TL;DR: 论文提出一种结合IMU数据和神经网络的轻量化估算越野地形下移动机器人平台稳定性的方法，避免了对复杂地形建模和力传感器的需求，并通过新颖的视觉跟踪评分作为训练信号，验证了其可泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统的物理稳定性度量方法（如SSM、ZMP）需要难以获取的地形、受力和质心信息，不适合真实复杂环境。为简化本体稳定性实时估算，提出数据驱动替代方案。

Method: 1. 设计轻量神经网络IMUnet，通过IMU和速度等本体数据直接估算稳定性；2. 提出基于ArUco视觉追踪的新C3评分体系，用于量化平台扰动并指导网络训练；3. 在多种地形和速度下采集数据，开展泛化验证。

Result: 在不同地形和速度数据上，训练得到的模型能有效泛化到未见过的工况，通过IMU和速度输入实现平台稳定性的估算。

Conclusion: 利用IMU数据和轻量学习模型可以在无需复杂环境建模和力传感器的前提下估算移动机器人平台稳定性，适用于农业、航天等真实场景中的精准动作和感知任务。

Abstract: Navigating in off-road environments for wheeled mobile robots is challenging
due to dynamic and rugged terrain. Traditional physics-based stability metrics,
such as Static Stability Margin (SSM) or Zero Moment Point (ZMP) require
knowledge of contact forces, terrain geometry, and the robot's precise
center-of-mass that are difficult to measure accurately in real-world field
conditions. In this work, we propose a learning-based approach to estimate
robot platform stability directly from proprioceptive data using a lightweight
neural network, IMUnet. Our method enables data-driven inference of robot
stability without requiring an explicit terrain model or force sensing.
  We also develop a novel vision-based ArUco tracking method to compute a
scalar score to quantify robot platform stability called C3 score. The score
captures image-space perturbations over time as a proxy for physical
instability and is used as a training signal for the neural network based
model. As a pilot study, we evaluate our approach on data collected across
multiple terrain types and speeds and demonstrate generalization to previously
unseen conditions. These initial results highlight the potential of using IMU
and robot velocity as inputs to estimate platform stability. The proposed
method finds application in gating robot tasks such as precision actuation and
sensing, especially for mobile manipulation tasks in agricultural and space
applications. Our learning method also provides a supervision mechanism for
perception based traversability estimation and planning.

</details>


### [132] [ASC-SW: Atrous strip convolution network with sliding windows for visual-assisted map navigation](https://arxiv.org/abs/2507.12744)
*Cheng Liu,Fan Zhu,Yaoyu Zhuang Zhinan Chen Jiefeng Tang*

Main category: cs.RO

TL;DR: 本文提出了一种名为ASC-SW的视觉辅助导航框架，结合轻量级视觉神经网络与深度摄像头辅助移动机器人识别并避让地面线缆等细长障碍，显著提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统高性能视觉模型虽然准确，但计算量大并能耗高，不适合边缘设备。而传统LiDAR难以检测地面线缆等低矮障碍物，亟需兼具高效与高识别性能的视觉解决方案。

Method: 提出了ASC-SW框架，核心为轻量分割模型ASCnet，基于MobileNetV2主干，设计了Atrous Strip Convolution Spatial Pyramid Pooling (ASCSPP)来提取DLO特征，利用滑动窗口后处理进行去噪，整体提升检测准确度且保持低计算成本。

Result: 在自建数据集上，方法获得75.3%的平均交并比（Miou），在Jetson Orin Nano边缘设备上的推理速度达9.3FPS。实际移动机器人平台测试中效果优于主流DLO检测方法。

Conclusion: ASC-SW框架能够高效、准确地检测移动机器人导航中地面细长障碍，兼顾推理速度和分割性能，适合实际边缘部署，有助于提升机器人在复杂环境下的安全性和自主性。

Abstract: With the rapid development of lightweight visual neural network
architectures, traditional high-performance vision models have undergone
significant compression, greatly improving their computational efficiency and
energy consumption ratio. This makes them feasible for deployment on
resource-constrained edge computing devices. We propose a visual-assisted
navigation framework called Atrous Strip Convolution-Sliding Window (ASC-SW),
which leverages a depth camera and a lightweight visual neural network to
assist map-based mobile robot navigation. This framework compensates for the
inability of traditional light detection and range (LiDAR) sensors to detect
ground-level obstacles such as ground-level wires. We introduce a lightweight
and efficient segmentation model, Atrous Strip Convolution Network (ASCnet),
for detecting deformable linear objects (DLOs). MobileNetV2 is used as the
backbone network, and Atrous Strip Convolution Spatial Pyramid Pooling (ASCSPP)
is designed to extract DLO features more effectively. Atrous Strip Convolution
is integrated into ASCSPP to accurately identify the linear structure of DLOs
with low computational cost. Additionally, a Sliding Window (SW)
post-processing module is proposed to denoise the output in complex
environments, improving recognition accuracy. Our method strikes a balance
between inference speed and segmentation performance. It achieves a mean
Intersection over Union (Miou) score of 75.3% on a self-built dataset and
reaches 9.3 FPS inference speed on the Jetson Orin Nano edge device. Overall,
our approach outperforms existing DLO detection models and has been
successfully validated on a physical robotic platform.

</details>


### [133] [Refining Motion for Peak Performance: Identifying Optimal Gait Parameters for Energy-Efficient Quadrupedal Bounding](https://arxiv.org/abs/2507.12751)
*Yasser G. Alqaham,Jing Cheng,Zhenyu Gan*

Main category: cs.RO

TL;DR: 本论文通过调整步态参数（如支撑因子、相位偏移和步幅时长）优化四足机器人运动过程中的能量消耗。


<details>
  <summary>Details</summary>
Motivation: 当前四足机器人研究偏重于机械设计与驱动系统改进，针对步态参数对能耗影响的研究相对较少。鉴于能效对机器人自主性与性能至关重要，论文提出通过调控步态参数优化能耗。

Method: 对Unitree A1四足机器人建模，开发了能独立调节步态参数的运动控制器。在Gazebo仿真平台下，针对低、中、高三种速度，系统性调控步态参数并模拟bounding步态，同时开展实物实验，验证仿真结果。

Result: 仿真与实验结果表明，针对不同速度优化步态参数可显著降低能耗，提高机器人运动效率。

Conclusion: 优化步态参数是提升四足机器人能效的有效手段，为现有商用平台能效控制策略提供了直接参考。

Abstract: Energy efficiency is a critical factor in the performance and autonomy of
quadrupedal robots. While previous research has focused on mechanical design
and actuation improvements, the impact of gait parameters on energetics has
been less explored. In this paper, we hypothesize that gait parameters,
specifically duty factor, phase shift, and stride duration, are key
determinants of energy consumption in quadrupedal locomotion. To test this
hypothesis, we modeled the Unitree A1 quadrupedal robot and developed a
locomotion controller capable of independently adjusting these gait parameters.
Simulations of bounding gaits were conducted in Gazebo across a range of gait
parameters at three different speeds: low, medium, and high. Experimental tests
were also performed to validate the simulation results. The findings
demonstrate that optimizing gait parameters can lead to significant reductions
in energy consumption, enhancing the overall efficiency of quadrupedal
locomotion. This work contributes to the advancement of energy-efficient
control strategies for legged robots, offering insights directly applicable to
commercially available platforms.

</details>


### [134] [osmAG-LLM: Zero-Shot Open-Vocabulary Object Navigation via Semantic Maps and Large Language Models Reasoning](https://arxiv.org/abs/2507.12753)
*Fujing Xie,Sören Schwertfeger,Hermann Blum*

Main category: cs.RO

TL;DR: 该论文提出了一种新颖的机器人地图构建与导航系统，能够有效处理对象可能移动或未被映射的情况，在静态及动态环境下均展现优异性能。


<details>
  <summary>Details</summary>
Motivation: 当前开放词汇量机器人地图构建方法虽然可实现细致的对象映射，但对象频繁移动导致高精度地图很快过时，亟需能适应对象动态变化的导航方法。

Method: 作者设计了一种结合环境语境地图、LLM语义先验并采用主动在线查询的方法，不再仅追求高细节映射，而是借助大语言模型对对象分布的推理来指导导航。

Result: 该方法在仿真与真实场景实验中，相比现有方法，在静态目标导航中可实现更高的检索成功率和更短路径，在对象动态或未被检测情况下具有明显性能优势。

Conclusion: 所提出的系统证明地图的核心价值在于提供全局环境语境，结合语义推理的方法，极大提升了动态或未知目标场景的导航能力，对未来机器人自主导航意义重大。

Abstract: Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features, achieving a high level of detail and
guiding robots to find objects specified by open-vocabulary language queries.
While the issue of scalability for such approaches has received some attention,
another fundamental problem is that high-detail object mapping quickly becomes
outdated, as objects get moved around a lot. In this work, we develop a mapping
and navigation system for object-goal navigation that, from the ground up,
considers the possibilities that a queried object can have moved, or may not be
mapped at all. Instead of striving for high-fidelity mapping detail, we
consider that the main purpose of a map is to provide environment grounding and
context, which we combine with the semantic priors of LLMs to reason about
object locations and deploy an active, online approach to navigate to the
objects. Through simulated and real-world experiments we find that our approach
tends to have higher retrieval success at shorter path lengths for static
objects and by far outperforms prior approaches in cases of dynamic or unmapped
object queries. We provide our code and dataset at:
https://anonymous.4open.science/r/osmAG-LLM.

</details>


### [135] [FFI-VTR: Lightweight and Robust Visual Teach and Repeat Navigation based on Feature Flow Indicator and Probabilistic Motion Planning](https://arxiv.org/abs/2507.12800)
*Jikai Wang,Yunqi Cheng,Zonghai Chen*

Main category: cs.RO

TL;DR: 提出了一种无需精确定位和稠密重建的新颖视觉重复导航方法，实现了轻量、高鲁棒性的移动机器人自主导航。


<details>
  <summary>Details</summary>
Motivation: 现有视觉重复导航方法在效率和鲁棒性之间难以平衡，同时对高精度定位和稠密地图重建有较高依赖，影响系统效率及实用性。为此，作者希望提出不依赖这些模块且具有鲁棒性的解决方案。

Method: 引入特征流的概念，通过配准特征点像素偏移建立特征流与机器人运动的定性映射。建图阶段输出关键帧图，其中边上的特征流编码关键帧间的相对运动。重复导航阶段，将当前帧与关键帧间特征流最小化，并基于特征流—运动映射设计概率运动规划，实现无需精确定位驱动导航。

Result: 在自有移动平台上进行了大量实验，结果显示该方法较基线方法更轻量、鲁棒性更好，且具备优越性能。

Conclusion: 该方法无需高精度定位和稠密重建，整体系统轻量且鲁棒，有助于推动移动机器人导航技术发展，并已开源代码供社区使用。

Abstract: Though visual and repeat navigation is a convenient solution for mobile robot
self-navigation, achieving balance between efficiency and robustness in task
environment still remains challenges. In this paper, we propose a novel visual
and repeat robotic autonomous navigation method that requires no accurate
localization and dense reconstruction modules, which makes our system featured
by lightweight and robustness. Firstly, feature flow is introduced and we
develop a qualitative mapping between feature flow and robot's motion, in which
feature flow is defined as pixel location bias between matched features. Based
on the mapping model, the map outputted by the teaching phase is represented as
a keyframe graph, in which the feature flow on the edge encodes the relative
motion between adjacent keyframes. Secondly, the visual repeating navigation is
essentially modeled as a feature flow minimization problem between current
observation and the map keyframe. To drive the robot to consistently reduce the
feature flow between current frame and map keyframes without accurate
localization, a probabilistic motion planning is developed based on our
qualitative feature flow-motion mapping indicator. Extensive experiments using
our mobile platform demonstrates that our proposed method is lightweight,
robust, and superior to baselines. The source code has been made public at
https://github.com/wangjks/FFI-VTR to benefit the community.

</details>


### [136] [Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering](https://arxiv.org/abs/2507.12846)
*Muhammad Fadhil Ginting,Dong-Ki Kim,Xiangyun Meng,Andrzej Reinke,Bandi Jai Krishna,Navid Kayhani,Oriana Peltzer,David D. Fan,Amirreza Shaban,Sung-Kyun Kim,Mykel J. Kochenderfer,Ali-akbar Agha-mohammadi,Shayegan Omidshafiei*

Main category: cs.RO

TL;DR: 本文提出了长时主动体问答（LA-EQA）任务，要求机器人结合记忆回顾和环境探索，解答复杂、时间相关的问题。针对现有方法的局限性，作者设计了结构化记忆系统和基于信息价值的探索-回忆权衡机制，并在现实和仿真环境中显著提升了问答准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的体问答系统仅关注现有环境或单一历史观测，难以应对长时任务中机器人需要综合过去、当前与可能未来信息来为人类决策服务的需求。因此，推动机器人在更复杂环境下工作，提升其智能问答水平，是本研究的动机。

Method: 作者提出用结构化记忆（受到“记忆宫殿”启发）对机器人经历的事件进行场景图编码，通过这一系统实现记忆获取与推理规划。此外，结合信息价值原理，动态确定机器人探索与回忆的最佳切换时机，从而优化解答的效率与准确性。

Result: 实验在虚拟和真实工业环境下进行，并建立新基准。与现有主体问答方法相比，该方法在答案准确率和探索效率上都获得了大幅度提升。

Conclusion: 结构化记忆系统和探索-回忆自适应机制，使机器人能更好地完成长时、复杂时序问题的解答任务，为机器人长期自主工作和人机协作提供了新思路。

Abstract: As robots become increasingly capable of operating over extended periods --
spanning days, weeks, and even months -- they are expected to accumulate
knowledge of their environments and leverage this experience to assist humans
more effectively. This paper studies the problem of Long-term Active Embodied
Question Answering (LA-EQA), a new task in which a robot must both recall past
experiences and actively explore its environment to answer complex,
temporally-grounded questions. Unlike traditional EQA settings, which typically
focus either on understanding the present environment alone or on recalling a
single past observation, LA-EQA challenges an agent to reason over past,
present, and possible future states, deciding when to explore, when to consult
its memory, and when to stop gathering observations and provide a final answer.
Standard EQA approaches based on large models struggle in this setting due to
limited context windows, absence of persistent memory, and an inability to
combine memory recall with active exploration. To address this, we propose a
structured memory system for robots, inspired by the mind palace method from
cognitive science. Our method encodes episodic experiences as scene-graph-based
world instances, forming a reasoning and planning algorithm that enables
targeted memory retrieval and guided navigation. To balance the
exploration-recall trade-off, we introduce value-of-information-based stopping
criteria that determines when the agent has gathered sufficient information. We
evaluate our method on real-world experiments and introduce a new benchmark
that spans popular simulation environments and actual industrial sites. Our
approach significantly outperforms state-of-the-art baselines, yielding
substantial gains in both answer accuracy and exploration efficiency.

</details>


### [137] [DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task Demonstration Learning](https://arxiv.org/abs/2507.12855)
*Rahel Rickenbach,Bruce Lee,René Zurbrügg,Carmen Amo Alonso,Melanie N. Zeilinger*

Main category: cs.RO

TL;DR: 本文提出了一种新方法，利用任务描述的嵌入表达代替大语言模型在上下文学习中的示例设计，从而提升机器人控制任务中的学习效果，并降低对工程师的专业依赖。


<details>
  <summary>Details</summary>
Motivation: 当前，将大语言模型用于机器人控制时需要工程师设计具有数学表达的任务示例，不仅费力还难以检测模型输出的虚假内容（hallucination）。因此，迫切需要减少人工示例设计和提升任务安全性与泛化能力的方法。

Method: 作者提出了DEMONSTRATE方法，借助逆最优控制（inverse optimal control）用硬件演示的实际数据替代人工示例，并通过多任务学习框架确保目标任务与示例任务高度相关。无需复杂的最优化问题生成，仅用任务描述的嵌入表达表示任务信息，使模型通过少量演示即可高效学习并可在执行前评估虚假行为风险。

Result: 实验在机器人手臂的桌面操作任务中进行，涵盖仿真与真实硬件。结果表明，DEMONSTRATE方法在减少人工依赖的同时，依然能够实现有效任务学习和低虚假风险评估。

Conclusion: DEMONSTRATE显著减少了对高水平工程师专业知识的依赖，使机器人任务学习更易于实施。方法具备较强的泛化与安全性，适合实际机器人控制任务的应用推广。

Abstract: The integration of large language models (LLMs) with control systems has
demonstrated significant potential in various settings, such as task completion
with a robotic manipulator. A main reason for this success is the ability of
LLMs to perform in-context learning, which, however, strongly relies on the
design of task examples, closely related to the target tasks. Consequently,
employing LLMs to formulate optimal control problems often requires task
examples that contain explicit mathematical expressions, designed by trained
engineers. Furthermore, there is often no principled way to evaluate for
hallucination before task execution. To address these challenges, we propose
DEMONSTRATE, a novel methodology that avoids the use of LLMs for complex
optimization problem generations, and instead only relies on the embedding
representations of task descriptions. To do this, we leverage tools from
inverse optimal control to replace in-context prompt examples with task
demonstrations, as well as the concept of multitask learning, which ensures
target and example task similarity by construction. Given the fact that
hardware demonstrations can easily be collected using teleoperation or guidance
of the robot, our approach significantly reduces the reliance on engineering
expertise for designing in-context examples. Furthermore, the enforced
multitask structure enables learning from few demonstrations and assessment of
hallucinations prior to task execution. We demonstrate the effectiveness of our
method through simulation and hardware experiments involving a robotic arm
tasked with tabletop manipulation.

</details>


### [138] [LaViPlan : Language-Guided Visual Path Planning with RLVR](https://arxiv.org/abs/2507.12911)
*Hayeon Oh*

Main category: cs.RO

TL;DR: 本文提出了一种新框架LaViPlan，结合强化学习与可验证奖励，改善了自动驾驶中视觉-语言模型在超出分布（OOD）情景下的决策对齐问题。实验表明，该方法提升了模型在未知场景下的情境感知与决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶模型在遇到OOD情景时存在决策失误风险。视觉-语言模型虽能识别OOD及给出高层次指令，但与实际行动（低层轨迹）存在语义对齐难题。作者希望解决这一从“理解”到“行动”间的失配。

Method: 提出LaViPlan框架，结合强化学习与可验证奖励（RLVR），以规划相关指标优化视觉-语言模型，减少语言推理层与行为层间的错配。

Result: 实验证明LaViPlan在OOD情境下显著提升了模型的情境感知与决策能力，优于仅用监督学习微调的VLMs。

Conclusion: RLVR优化的LaViPlan能够有效缓解视觉-语言-行动不对齐，提升自动驾驶系统在未知场景下的安全性和适应性，为VLMs后训练提供新范式。

Abstract: Out-of-distribution (OOD) scenarios in autonomous driving refer to situations
that deviate from the training domain, often leading to unexpected and
potentially hazardous behavior from planners that lack prior exposure to such
cases. Recently, Vision-Language Models (VLMs) have been introduced into
autonomous driving research for their promising generalization capabilities in
OOD settings. Early studies demonstrated that VLMs could recognize OOD
scenarios and generate user-level decisions such as "go straight" or "turn
right." However, a new challenge has emerged due to the misalignment between
the VLM's high-level decisions or visual reasoning expressed in language, and
the low-level predicted trajectories interpreted as actions. In this paper, we
propose LaViPlan, a framework that leverages Reinforcement Learning with
Verifiable Rewards (RLVR) to optimize VLMs using planning-oriented metrics.
This approach addresses the vision-language-action misalignment observed in
existing VLMs fine-tuned via supervised learning, which can recognize driving
scenarios but often produce context-unaware decisions. Experimental results
demonstrate that our method improves situational awareness and decision-making
under OOD conditions, highlighting its potential to mitigate the misalignment
issue. This work introduces a promising post-training paradigm for VLM agents
in the context of autonomous driving.

</details>


### [139] [MoCap2GT: A High-Precision Ground Truth Estimator for SLAM Benchmarking Based on Motion Capture and IMU Fusion](https://arxiv.org/abs/2507.12920)
*Zichao Shu,Shitao Bei,Jicheng Dai,Lijun Li,Zetao Chen*

Main category: cs.RO

TL;DR: 本文提出了一种新的GT轨迹生成方法MoCap2GT，将MoCap数据与IMU测量联合优化，显著提升了用于SLAM评测的精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于MoCap系统的GT轨迹评测主要存在两个难点：一是MoCap与被测设备（DUT）之间的时空标定误差，二是MoCap本身的抖动噪声。这导致现有评测集中于绝对平移误差，而旋转和帧间误差评测困难，影响SLAM评测的全面性。

Method: 提出MoCap2GT方法：1）利用IMU与MoCap数据联合优化，2）提出稳健的初始状态估计用于全局收敛，3）提出SE(3)流形上的高阶B样条动作建模，并优化时序偏移以更好拟合MoCap因素，4）引入退化感知的测量剔除策略，提高估计精度。

Result: 实验证明，MoCap2GT方法在测量精度和SLAM基准评测能力上均优于现有方法。

Conclusion: MoCap2GT极大提升了基于MoCap的GT轨迹精度，有助于提升SLAM算法的评测效果。

Abstract: Marker-based optical motion capture (MoCap) systems are widely used to
provide ground truth (GT) trajectories for benchmarking SLAM algorithms.
However, the accuracy of MoCap-based GT trajectories is mainly affected by two
factors: spatiotemporal calibration errors between the MoCap system and the
device under test (DUT), and inherent MoCap jitter. Consequently, existing
benchmarks focus primarily on absolute translation error, as accurate
assessment of rotation and inter-frame errors remains challenging, hindering
thorough SLAM evaluation. This paper proposes MoCap2GT, a joint optimization
approach that integrates MoCap data and inertial measurement unit (IMU)
measurements from the DUT for generating high-precision GT trajectories.
MoCap2GT includes a robust state initializer to ensure global convergence,
introduces a higher-order B-spline pose parameterization on the SE(3) manifold
with variable time offset to effectively model MoCap factors, and employs a
degeneracy-aware measurement rejection strategy to enhance estimation accuracy.
Experimental results demonstrate that MoCap2GT outperforms existing methods and
significantly contributes to precise SLAM benchmarking. The source code is
available at https://anonymous.4open.science/r/mocap2gt (temporarily hosted
anonymously for double-blind review).

</details>


### [140] [Non-differentiable Reward Optimization for Diffusion-based Autonomous Motion Planning](https://arxiv.org/abs/2507.12977)
*Giwon Lee,Daehee Park,Jaewoo Jeong,Kuk-Jin Yoon*

Main category: cs.RO

TL;DR: 本文提出了一种结合扩散模型和强化学习的新方法，提高自主机器人运动规划的安全性与有效性，并在众多行人数据集上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型在建模复杂环境下的运动规划方面表现优异，能够预测多峰未来轨迹，但其训练目标通常不能直接优化运动规划最核心的非可微下游目标（如安全性和目标到达）。现有学习算法难以直接处理此类目标，因此有必要探索更直接优化安全性和有效性的新方法。

Method: 作者提出了一种基于强化学习的扩散运动规划模型训练机制，引入奖励加权的动态阈值算法，形成密集的奖励信号，从而有效优化运动规划中的安全和任务达成目标。与传统依赖可微目标的训练方法相比，该方法能更好地适应实际需求。

Result: 在多个行人运动数据集（CrowdNav、ETH-UCY）上与多种基线方法对比，本文方法展现出了更优的安全性和有效性，性能达到当前最优水平。

Conclusion: 通过将强化学习与扩散模型结合，并设计奖励加权动态阈值算法，作者提出的方法能够直接优化运动规划中的非可微目标，在安全性和目标达成能力上显著超越现有方法，具有广泛的应用潜力。

Abstract: Safe and effective motion planning is crucial for autonomous robots.
Diffusion models excel at capturing complex agent interactions, a fundamental
aspect of decision-making in dynamic environments. Recent studies have
successfully applied diffusion models to motion planning, demonstrating their
competence in handling complex scenarios and accurately predicting multi-modal
future trajectories. Despite their effectiveness, diffusion models have
limitations in training objectives, as they approximate data distributions
rather than explicitly capturing the underlying decision-making dynamics.
However, the crux of motion planning lies in non-differentiable downstream
objectives, such as safety (collision avoidance) and effectiveness
(goal-reaching), which conventional learning algorithms cannot directly
optimize. In this paper, we propose a reinforcement learning-based training
scheme for diffusion motion planning models, enabling them to effectively learn
non-differentiable objectives that explicitly measure safety and effectiveness.
Specifically, we introduce a reward-weighted dynamic thresholding algorithm to
shape a dense reward signal, facilitating more effective training and
outperforming models trained with differentiable objectives. State-of-the-art
performance on pedestrian datasets (CrowdNav, ETH-UCY) compared to various
baselines demonstrates the versatility of our approach for safe and effective
motion planning.

</details>


### [141] [Robustness Requirement Coverage using a Situation Coverage Approach for Vision-based AI Systems](https://arxiv.org/abs/2507.12986)
*Sepeedeh Shahbeigi,Nawshin Mannan Proma,Victoria Hodge,Richard Hawkins,Boda Li,Valentina Donzella*

Main category: cs.RO

TL;DR: 本文提出了一种新框架，结合相机噪声因子识别与情境覆盖分析，系统性地提出AI感知系统在摄像头退化场景下的安全鲁棒性需求。


<details>
  <summary>Details</summary>
Motivation: 现有AI机器人和车辆需在复杂环境下安全运行，但摄像头等传感器退化会降低数据质量，影响AI决策。若直接针对所有退化情况制定安全需求，将造成极高复杂性，且难以穷尽所有可能情形，需要更系统的方法来厘清和完善鲁棒性要求。

Method: 作者提出将相机噪声因子的识别与情境覆盖分析结合，构建系统化框架：一方面通过多领域专家和运用Operational Design Domain（ODD）规范来完善退化模式和噪声因子的分析；另一方面，通过情境覆盖分析，找出有代表性的运行上下文场景。以此系统化地提出AI感知系统在摄像头退化下的鲁棒性和安全需求。

Result: 框架可扩展现有退化模型，将与AI性能相关的噪声因子纳入考量，并通过情境覆盖帮助发现关键的操作场景，从而有助于更全面地制定鲁棒性安全需求。

Conclusion: 该研究为将噪声因子分析与情境覆盖结合以支持基于摄像头的AI感知系统鲁棒性安全需求的系统化制定和完整性评估，迈出了重要初步一步。

Abstract: AI-based robots and vehicles are expected to operate safely in complex and
dynamic environments, even in the presence of component degradation. In such
systems, perception relies on sensors such as cameras to capture environmental
data, which is then processed by AI models to support decision-making. However,
degradation in sensor performance directly impacts input data quality and can
impair AI inference. Specifying safety requirements for all possible sensor
degradation scenarios leads to unmanageable complexity and inevitable gaps. In
this position paper, we present a novel framework that integrates camera noise
factor identification with situation coverage analysis to systematically elicit
robustness-related safety requirements for AI-based perception systems. We
focus specifically on camera degradation in the automotive domain. Building on
an existing framework for identifying degradation modes, we propose involving
domain, sensor, and safety experts, and incorporating Operational Design Domain
specifications to extend the degradation model by incorporating noise factors
relevant to AI performance. Situation coverage analysis is then applied to
identify representative operational contexts. This work marks an initial step
toward integrating noise factor analysis and situational coverage to support
principled formulation and completeness assessment of robustness requirements
for camera-based AI perception.

</details>


### [142] [Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities](https://arxiv.org/abs/2507.13019)
*Liuyi Wang,Xinyuan Xia,Hui Zhao,Hanqing Wang,Tai Wang,Yilun Chen,Chengju Liu,Qijun Chen,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 本文提出了一个更接近实际物理环境的视觉-语言导航（VLN）平台，并首次系统评估了不同类型机器人的VLN方法在实际中的表现，发现当前方法在真实物理环境下性能大幅下降，同时该平台可灵活扩展用于未来更全面的评测和提升。


<details>
  <summary>Details</summary>
Motivation: 现有VLN研究大多在理想化的环境和假设下进行，忽略了实际机器人部署中运动、感知与环境变化带来的挑战。缺乏反映实际物理约束的评测平台，阻碍了VLN技术在真实环境中应用的进步。作者希望开发一个能反映真实部署挑战的VLN平台，推进该领域向更实用的方向发展。

Method: 作者开发了VLN-PE平台，支持类人（humanoid）、四足（quadruped）和轮式（wheeled）机器人，并系统对比评估了：1）基于分类的单步离散动作预测模型；2）基于扩散（diffusion model）的密集航点预测模型；3）结合地图规划的大型语言模型（LLM）零训练推理方法。评测内容涵盖视觉感知限制、环境光照变化及物理层面的碰撞、跌倒等挑战。平台还支持环境扩展，能无缝集成超出MP3D的新场景。

Result: 实验发现，受限的观察空间、环境条件变化和物理挑战导致所有VLN方法在实际机器人平台上的性能远低于模拟结果，尤其四足机器人在复杂环境中受限显著。当前方法在不同平台和环境间的泛化能力较弱。

Conclusion: 现有VLN方法在真实物理场景下存在显著性能瓶颈，VLN-PE平台为研究这些问题及跨机器人适应性提供了实验基础，有望推动社区重新审视VLN的局限，开发更健壮和实用的导航模型。

Abstract: Recent Vision-and-Language Navigation (VLN) advancements are promising, but
their idealized assumptions about robot movement and control fail to reflect
physically embodied deployment challenges. To bridge this gap, we introduce
VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and
wheeled robots. For the first time, we systematically evaluate several
ego-centric VLN methods in physical robotic settings across different technical
pipelines, including classification models for single-step discrete action
prediction, a diffusion model for dense waypoint prediction, and a train-free,
map-based large language model (LLM) integrated with path planning. Our results
reveal significant performance degradation due to limited robot observation
space, environmental lighting variations, and physical challenges like
collisions and falls. This also exposes locomotion constraints for legged
robots in complex environments. VLN-PE is highly extensible, allowing seamless
integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN
evaluation. Despite the weak generalization of current models in physical
deployment, VLN-PE provides a new pathway for improving cross-embodiment's
overall adaptability. We hope our findings and tools inspire the community to
rethink VLN limitations and advance robust, practical VLN models. The code is
available at https://crystalsixone.github.io/vln_pe.github.io/.

</details>


### [143] [What Can Robots Teach Us About Trust and Reliance? An interdisciplinary dialogue between Social Sciences and Social Robotics](https://arxiv.org/abs/2507.13041)
*Julien Wacquez,Elisabetta Zibetti,Joffrey Becker,Lorenzo Aloe,Fabio Amadio,Salvatore Anzalone,Lola Cañamero,Serena Ivaldi*

Main category: cs.RO

TL;DR: 本文呼吁采用跨学科视角，将社会科学与社会机器人学的成果相结合，探讨人类与机器人交互中的信任问题，推动建立更加扎实且灵活的信任理解框架。


<details>
  <summary>Details</summary>
Motivation: 随着机器人日益融入社会各领域，人类对机器人的信任问题日益凸显。目前人机交互领域对信任的探讨较为零散，且很少结合社会学长期积累的信任理论，因此亟需跨学科整合。

Method: 文章综合分析了社会科学与社会机器人学中关于信任的理论与研究，通过对比与联系，提出跨学科对话和框架构建的必要性。

Result: 文章通过对已有理论的梳理和比较，展现了仅依靠单一学科难以全面解释人机信任现象，呼吁跨领域整合。

Conclusion: 建立一个结合社会科学和机器人学的跨学科信任理解框架，将有助于更好地把握和促进人机关系中的信任，为未来人机协作的社会应用提供理论支持。

Abstract: As robots find their way into more and more aspects of everyday life,
questions around trust are becoming increasingly important. What does it mean
to trust a robot? And how should we think about trust in relationships that
involve both humans and non-human agents? While the field of Human-Robot
Interaction (HRI) has made trust a central topic, the concept is often
approached in fragmented ways. At the same time, established work in sociology,
where trust has long been a key theme, is rarely brought into conversation with
developments in robotics. This article argues that we need a more
interdisciplinary approach. By drawing on insights from both social sciences
and social robotics, we explore how trust is shaped, tested and made visible.
Our goal is to open up a dialogue between disciplines and help build a more
grounded and adaptable framework for understanding trust in the evolving world
of human-robot interaction.

</details>


### [144] [Efficient Online Learning and Adaptive Planning for Robotic Information Gathering Based on Streaming Data](https://arxiv.org/abs/2507.13053)
*Sanjeev Ramkumar Sudha,Joel Jose,Erlend M. Coates*

Main category: cs.RO

TL;DR: 本文提出了一种结合流式稀疏高斯过程（GP）的高效机器人信息采集与自适应规划方法，实现了在未知或变化环境下对连续标量场的在线实时建图，并在保持准确性的同时显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有机器人信息采集方法多假设环境已知，而实际环境通常未知或变化；现有高斯过程方法在大规模数据和实时性方面性能不足，需要新的高效算法支持自适应、实时的环境建图。

Method: 提出基于流式稀疏GP的自适应信息规划方法，将稀疏高斯过程与在线路径规划结合，通过合成与真实数据集进行实验，对比现有基线方法，验证其实用性和优越性。

Result: 在仿真实验和真实数据集上，该方法与现有基线方法相比，在保持类似建图准确性的前提下，有效降低了计算复杂度，尤其适用于长时间任务。

Conclusion: 本文提出的方法适用于未知或动态环境下的自适应建图任务，可实现高效、实时的信息采集与环境建图，为实际大规模机器人应用提供了更优的技术途径。

Abstract: Robotic information gathering (RIG) techniques refer to methods where mobile
robots are used to acquire data about the physical environment with a suite of
sensors. Informative planning is an important part of RIG where the goal is to
find sequences of actions or paths that maximize efficiency or the quality of
information collected. Many existing solutions solve this problem by assuming
that the environment is known in advance. However, real environments could be
unknown or time-varying, and adaptive informative planning remains an active
area of research. Adaptive planning and incremental online mapping are required
for mapping initially unknown or varying spatial fields. Gaussian process (GP)
regression is a widely used technique in RIG for mapping continuous spatial
fields. However, it falls short in many applications as its real-time
performance does not scale well to large datasets. To address these challenges,
this paper proposes an efficient adaptive informative planning approach for
mapping continuous scalar fields with GPs with streaming sparse GPs. Simulation
experiments are performed with a synthetic dataset and compared against
existing benchmarks. Finally, it is also verified with a real-world dataset to
further validate the efficacy of the proposed method. Results show that our
method achieves similar mapping accuracy to the baselines while reducing
computational complexity for longer missions.

</details>


### [145] [ZipMPC: Compressed Context-Dependent MPC Cost via Imitation Learning](https://arxiv.org/abs/2507.13088)
*Rahel Rickenbach,Alan A. Lahoud,Erik Schaffernicht,Melanie N. Zeilinger,Johannes A. Stork*

Main category: cs.RO

TL;DR: ZipMPC 通过学习压缩且依赖环境的损失函数，使短时域MPC实现类似长时域MPC的行为，在保证计算效率的同时显著提升了长期控制目标优化能力，并能在未见环境下泛化。


<details>
  <summary>Details</summary>
Motivation: 传统MPC在实时系统中的计算负担较重，通常不得不使用较短的预测时域，这限制了控制性能并加剧了损失函数设计的难度。该工作旨在解决短时域MPC在长期目标优化和泛化方面的局限，同时兼顾计算效率。

Method: 提出了ZipMPC方法，通过神经网络学习一个压缩、依赖上下文的损失函数，将其应用于短时域MPC，并利用可微分MPC框架实现 imitation loss 对MPC优化问题的梯度反向传播。方法同时在仿真和真实自动驾驶竞速场景下进行了实验验证。

Result: ZipMPC在多项指标如长期目标优化、计算开销控制、约束满足及泛化能力方面均优于现有方法。实验证明ZipMPC能实现与长时域MPC相近的性能，在短时域基线方法失效时依然能成功完成任务，且在未见轨道上依然保持优势。

Conclusion: ZipMPC能够有效缓解MPC实时控制下的算力瓶颈，使短时域MPC具备长时域优化能力，并实现良好的泛化和控制性能，具有显著的应用潜力。

Abstract: The computational burden of model predictive control (MPC) limits its
application on real-time systems, such as robots, and often requires the use of
short prediction horizons. This not only affects the control performance, but
also increases the difficulty of designing MPC cost functions that reflect the
desired long-term objective. This paper proposes ZipMPC, a method that imitates
a long-horizon MPC behaviour by learning a compressed and context-dependent
cost function for a short-horizon MPC. It improves performance over alternative
methods, such as approximate explicit MPC and automatic cost parameter tuning,
in particular in terms of i) optimizing the long term objective; ii)
maintaining computational costs comparable to a short-horizon MPC; iii)
ensuring constraint satisfaction; and iv) generalizing control behaviour to
environments not observed during training. For this purpose, ZipMPC leverages
the concept of differentiable MPC with neural networks to propagate gradients
of the imitation loss through the MPC optimization. We validate our proposed
method in simulation and real-world experiments on autonomous racing. ZipMPC
consistently completes laps faster than selected baselines, achieving lap times
close to the long-horizon MPC baseline. In challenging scenarios where the
short-horizon MPC baseline fails to complete a lap, ZipMPC is able to do so. In
particular, these performance gains are also observed on tracks unseen during
training.

</details>


### [146] [GraspGen: A Diffusion-based Framework for 6-DOF Grasping with On-Generator Training](https://arxiv.org/abs/2507.13097)
*Adithyavairavan Murali,Balakumar Sundaralingam,Yu-Wei Chao,Wentao Yuan,Jun Yamada,Mark Carlson,Fabio Ramos,Stan Birchfield,Dieter Fox,Clemens Eppner*

Main category: cs.RO

TL;DR: 本文提出了GraspGen框架，通过DiffusionTransformer和判别器提升机器人抓取生成能力，并发布大规模仿真数据集，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有学习型6自由度机器人抓取方法在泛化性与真实场景应用中仍存在挑战，需提升抓取模型适应不同抓手和环境的能力。

Method: 提出基于DiffusionTransformer的抓取生成架构，并配合高效判别器过滤候选抓取点，同时创新性地为判别器设计了生成器端训练方案，且开发了包含5300多万抓取的仿真数据集。

Result: GraspGen在多种仿真抓手与实际FetchBench基准测试中实现了优于先前方法的抓取表现，在真实机器人有视觉噪声条件下同样展现出良好效果。

Conclusion: GraspGen不仅在模拟和真实环境下提升了抓取性能，还具备优良的泛化能力，为机器人抓取任务提供了新方案和资源基础。

Abstract: Grasping is a fundamental robot skill, yet despite significant research
advancements, learning-based 6-DOF grasping approaches are still not turnkey
and struggle to generalize across different embodiments and in-the-wild
settings. We build upon the recent success on modeling the object-centric grasp
generation process as an iterative diffusion process. Our proposed framework,
GraspGen, consists of a DiffusionTransformer architecture that enhances grasp
generation, paired with an efficient discriminator to score and filter sampled
grasps. We introduce a novel and performant on-generator training recipe for
the discriminator. To scale GraspGen to both objects and grippers, we release a
new simulated dataset consisting of over 53 million grasps. We demonstrate that
GraspGen outperforms prior methods in simulations with singulated objects
across different grippers, achieves state-of-the-art performance on the
FetchBench grasping benchmark, and performs well on a real robot with noisy
visual observations.

</details>


### [147] [Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback](https://arxiv.org/abs/2507.13171)
*Suzie Kim,Hye-Bin Shin,Seong-Whan Lee*

Main category: cs.RO

TL;DR: 本论文提出了一种利用脑电（EEG）信号作为隐式人类反馈来辅助强化学习，克服了稀疏奖励环境下的学习困难，验证其在机器人任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在稀疏奖励下难以学习，需要设计复杂奖励函数。虽然引入人类反馈（RLHF）有所帮助，但大部分方法依赖显式反馈（例如按钮、标签），影响交互自然性且增加用户负担。

Method: 作者提出了RLIHF框架，通过非侵入式脑电（EEG）信号，尤其是错误相关电位（ErrPs），作为隐式、连续的反馈信号，不需用户主动参与。利用预训练解码器将原始EEG转为概率性奖励。该方法在MuJoCo仿真环境下，通过Kinova Gen2机械臂进行障碍避让的抓取放置任务来评估。

Result: 通过EEG信号训练的智能体在任务表现上可与用人工密集奖励设计的智能体媲美。

Conclusion: 使用隐式脑电反馈可大幅提升人机交互强化学习的扩展性和对人的对齐性，为交互机器人应用提供新思路。

Abstract: Conventional reinforcement learning (RL) ap proaches often struggle to learn
effective policies under sparse reward conditions, necessitating the manual
design of complex, task-specific reward functions. To address this limitation,
rein forcement learning from human feedback (RLHF) has emerged as a promising
strategy that complements hand-crafted rewards with human-derived evaluation
signals. However, most existing RLHF methods depend on explicit feedback
mechanisms such as button presses or preference labels, which disrupt the
natural interaction process and impose a substantial cognitive load on the
user. We propose a novel reinforcement learning from implicit human feedback
(RLIHF) framework that utilizes non-invasive electroencephalography (EEG)
signals, specifically error-related potentials (ErrPs), to provide continuous,
implicit feedback without requiring explicit user intervention. The proposed
method adopts a pre-trained decoder to transform raw EEG signals into
probabilistic reward components, en abling effective policy learning even in
the presence of sparse external rewards. We evaluate our approach in a
simulation environment built on the MuJoCo physics engine, using a Kinova Gen2
robotic arm to perform a complex pick-and-place task that requires avoiding
obstacles while manipulating target objects. The results show that agents
trained with decoded EEG feedback achieve performance comparable to those
trained with dense, manually designed rewards. These findings validate the
potential of using implicit neural feedback for scalable and human-aligned
reinforcement learning in interactive robotics.

</details>


### [148] [Few-shot transfer of tool-use skills using human demonstrations with proximity and tactile sensing](https://arxiv.org/abs/2507.13200)
*Marina Y. Aoyama,Sethu Vijayakumar,Tetsuya Narita*

Main category: cs.RO

TL;DR: 本论文提出了一种利用多模态传感与少量人类示范，实现机器人器具操作技能迁移的新方法。该方法在仿真中预训练策略，并通过现实世界的人类演示微调，以解决仿真到现实的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管人类擅长工具操作，但机器人学习工具操作困难重重，尤其是机器人-工具与工具-环境的双重接触关系复杂。现有基于触觉和接近传感的学习方法受限于现实数据稀缺和仿真-现实差距，亟需新的迁移学习框架提升机器人实用能力。

Method: 提出了一种基于多模态（触觉和接近）传感的工具操作技能迁移框架，先在仿真中学习常见接触状态的基础策略，再利用现实少量人类示范数据进行策略微调，实现范例稀缺条件下的高效迁移。

Result: 在Franka Emika机械臂上验证了方法有效性，能通过少量演示学习不同物理和几何属性工具的跟随表面等操作任务。多模态感知提升了接触状态和环境几何的识别能力。

Conclusion: 机器人通过迁移已学习的工具-环境接触识别能力，能更快习得新工具操作技能。结合触觉与接近传感器有助于提升操作的泛化能力和精度，为机器人高效学习复杂工具操作提供了新方向。

Abstract: Tools extend the manipulation abilities of robots, much like they do for
humans. Despite human expertise in tool manipulation, teaching robots these
skills faces challenges. The complexity arises from the interplay of two
simultaneous points of contact: one between the robot and the tool, and another
between the tool and the environment. Tactile and proximity sensors play a
crucial role in identifying these complex contacts. However, learning tool
manipulation using these sensors remains challenging due to limited real-world
data and the large sim-to-real gap. To address this, we propose a few-shot
tool-use skill transfer framework using multimodal sensing. The framework
involves pre-training the base policy to capture contact states common in
tool-use skills in simulation and fine-tuning it with human demonstrations
collected in the real-world target domain to bridge the domain gap. We validate
that this framework enables teaching surface-following tasks using tools with
diverse physical and geometric properties with a small number of demonstrations
on the Franka Emika robot arm. Our analysis suggests that the robot acquires
new tool-use skills by transferring the ability to recognise tool-environment
contact relationships from pre-trained to fine-tuned policies. Additionally,
combining proximity and tactile sensors enhances the identification of contact
states and environmental geometry.

</details>


### [149] [Signal Temporal Logic Compliant Co-design of Planning and Control](https://arxiv.org/abs/2507.13225)
*Manas Sashank Juvvi,Tushar Dilip Kurne,Vaishnavi J,Shishir Kolathaya,Pushpak Jagtap*

Main category: cs.RO

TL;DR: 该论文提出了一种新颖的协同设计策略，将轨迹规划与控制相结合，以完成自动机器人中的时序逻辑（STL）任务。方法分为两个阶段：首先利用强化学习学习运动基元控制策略，然后用这些基元构建满足STL任务的运动计划。该方法无需模型，在不同类型机器人上得到验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法在轨迹规划与实际控制分离，难以直接满足时序逻辑任务（如STL规范）。本研究旨在解决如何系统性地将机器人动态限制纳入STL任务规划，使生成的运动计划既物理可行又满足高层规范。

Method: 方法分两阶段：第一阶段通过强化学习，为机器人学习一组满足各种约束的运动基元与控制策略，并建立基元到时空特征的映射；第二阶段基于采样，组合这些可用基元，规划出满足给定STL规范的运动。整个系统为无模型方法，适应不同环境。

Result: 作者在差分驱动机器人和四足机器人等多个平台和多种STL规范下进行了验证。结果表明方法可以有效生成满足复杂规范的运动计划。提供了演示视频。

Conclusion: 本文提出的协同设计策略结合了轨迹规划与控制，通过运动基元和基于采样的任务组合，实现了自动机器人在广泛环境下的STL任务执行，表现出良好鲁棒性和通用性。

Abstract: This work presents a novel co-design strategy that integrates trajectory
planning and control to handle STL-based tasks in autonomous robots. The method
consists of two phases: $(i)$ learning spatio-temporal motion primitives to
encapsulate the inherent robot-specific constraints and $(ii)$ constructing an
STL-compliant motion plan from these primitives. Initially, we employ
reinforcement learning to construct a library of control policies that perform
trajectories described by the motion primitives. Then, we map motion primitives
to spatio-temporal characteristics. Subsequently, we present a sampling-based
STL-compliant motion planning strategy tailored to meet the STL specification.
The proposed model-free approach, which generates feasible STL-compliant motion
plans across various environments, is validated on differential-drive and
quadruped robots across various STL specifications. Demonstration videos are
available at https://tinyurl.com/m6zp7rsm.

</details>


### [150] [Evaluating Reinforcement Learning Algorithms for Navigation in Simulated Robotic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour](https://arxiv.org/abs/2507.13277)
*Emma M. A. Harrison*

Main category: cs.RO

TL;DR: 该论文比较了三种强化学习算法在训练四足机器人自主导航和避障方面的效果，发现PPO算法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现实中引入机器人，尤其是四足机器人，在医疗等多个行业的潜在应用尚未被充分挖掘，特别是在作为导盲犬或医疗辅助宠物方面。

Method: 基于对十三篇相关文献的对比，确定评价标准，分别用三种强化学习算法(PPO, DQN, Q-learning)在自定义模拟环境下训练仿真四足机器人，统一传感器输入和评测指标，重点考察碰撞率、奖励信号及学习进度等。

Result: PPO算法在所有评估指标中（尤其是到达目标点的平均和中值步数）全面优于DQN和Q-learning。

Conclusion: PPO算法非常适合复杂环境下四足机器人导航，为AI驱动的医疗辅助和导盲机器人实现提供了可行性和技术路线。

Abstract: Robots are increasingly integrated across industries, particularly in
healthcare. However, many valuable applications for quadrupedal robots remain
overlooked. This research explores the effectiveness of three reinforcement
learning algorithms in training a simulated quadruped robot for autonomous
navigation and obstacle avoidance. The goal is to develop a robotic guide dog
simulation capable of path following and obstacle avoidance, with long-term
potential for real-world assistance to guide dogs and visually impaired
individuals. It also seeks to expand research into medical 'pets', including
robotic guide and alert dogs.
  A comparative analysis of thirteen related research papers shaped key
evaluation criteria, including collision detection, pathfinding algorithms,
sensor usage, robot type, and simulation platforms. The study focuses on sensor
inputs, collision frequency, reward signals, and learning progression to
determine which algorithm best supports robotic navigation in complex
environments.
  Custom-made environments were used to ensure fair evaluation of all three
algorithms under controlled conditions, allowing consistent data collection.
Results show that Proximal Policy Optimization (PPO) outperformed Deep
Q-Network (DQN) and Q-learning across all metrics, particularly in average and
median steps to goal per episode.
  By analysing these results, this study contributes to robotic navigation, AI
and medical robotics, offering insights into the feasibility of AI-driven
quadruped mobility and its role in assistive robotics.

</details>


### [151] [Latent Policy Steering with Embodiment-Agnostic Pretrained World Models](https://arxiv.org/abs/2507.13340)
*Yiqi Wang,Mrinal Verghese,Jeff Schneider*

Main category: cs.RO

TL;DR: 本论文提出一种结合多种廉价数据源及新颖算法以大幅减少机器人视觉运动策略学习对真实示范数据需求的方法，并在现实世界实验中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人视觉运动策略学习高度依赖真实演示数据，造成数据收集昂贵且效率低下。作者希望通过利用开源机器人数据集及人与物体互动的人类数据，降低对机器人真实数据的依赖，从而减少数据收集成本。

Method: 作者首先用光流作为与硬件无关的动作表征，在多种具身数据集上预训练世界模型（World Model, WM），再以少量目标机器人的数据进行微调。其次，提出潜变量策略引导方法（Latent Policy Steering, LPS），在WM的潜空间中搜索更优的动作序列，以提升从示范行为克隆得到的策略效果。

Result: 在现实机器人实验中，结合预训练WM与LPS方法的策略，在仅有30条示范时表现提升超过50%，仅有50条示范时提升也超过20%。此外，既适用于多机器人数据集（如Open X-embodiment），亦适用于成本低廉的人类游戏数据。

Conclusion: 通过跨多具身数据集预训练WM并结合潜变量空间策略引导，能有效提升仅凭少量真实演示数据训练的机器人视觉运动策略性能，有望大幅降低数据收集成本并加速相关领域发展。

Abstract: Learning visuomotor policies via imitation has proven effective across a wide
range of robotic domains. However, the performance of these policies is heavily
dependent on the number of training demonstrations, which requires expensive
data collection in the real world. In this work, we aim to reduce data
collection efforts when learning visuomotor robot policies by leveraging
existing or cost-effective data from a wide range of embodiments, such as
public robot datasets and the datasets of humans playing with objects (human
data from play). Our approach leverages two key insights. First, we use optic
flow as an embodiment-agnostic action representation to train a World Model
(WM) across multi-embodiment datasets, and finetune it on a small amount of
robot data from the target embodiment. Second, we develop a method, Latent
Policy Steering (LPS), to improve the output of a behavior-cloned policy by
searching in the latent space of the WM for better action sequences. In real
world experiments, we observe significant improvements in the performance of
policies trained with a small amount of data (over 50% relative improvement
with 30 demonstrations and over 20% relative improvement with 50
demonstrations) by combining the policy with a WM pretrained on two thousand
episodes sampled from the existing Open X-embodiment dataset across different
robots or a cost-effective human dataset from play.

</details>
