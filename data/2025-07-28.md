<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 101]
- [cs.CL](#cs.CL) [Total: 36]
- [cs.RO](#cs.RO) [Total: 15]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Quantum-Cognitive Tunnelling Neural Networks for Military-Civilian Vehicle Classification and Sentiment Analysis](https://arxiv.org/abs/2507.18645)
*Milan Maksimovic,Anna Bohdanets,Immaculate Motsi-Omoijiade,Guido Governatori,Ivan S. Maksymov*

Main category: cs.CV

TL;DR: 本文利用基于量子隧穿（QT）的神经网络，对自定义的军事与民用交通工具图像以及特定军事语境下的情感判别任务进行实验。结果显示QT神经网络在多模态AI、尤其是战场无人机环境下具有更接近人类推理的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在处理模糊物体识别或情感分析时，难以准确模拟人类感知的复杂性。已有研究发现引入量子隧穿概率可提升模型效果，因此本研究希望应用这类模型于军事应用中的多模态识别任务，探索其在逼近人类推理方面的能力。

Method: 采用新颖的基于量子隧穿概率的神经网络模型，对经定制的CIFAR格式军事与民用交通工具图片进行分类，以及基于专属军事词表的情感分析实验。对比分析QT模型在多模态识别和感知任务中的优势。

Result: 实验证明，QT神经网络对自定义军事与民用图片及情感判断均表现优异，能够更有效地区分类别，在特定军事语境下显示出良好的适用性。

Conclusion: QT神经网络模型能增强战场多模态AI系统对复杂目标和语境的判别能力，赋予AI更类似人类推理的特征，提升其在无人化作战实际应用中的效能。

Abstract: Prior work has demonstrated that incorporating well-known quantum tunnelling
(QT) probability into neural network models effectively captures important
nuances of human perception, particularly in the recognition of ambiguous
objects and sentiment analysis. In this paper, we employ novel QT-based neural
networks and assess their effectiveness in distinguishing customised
CIFAR-format images of military and civilian vehicles, as well as sentiment,
using a proprietary military-specific vocabulary. We suggest that QT-based
models can enhance multimodal AI applications in battlefield scenarios,
particularly within human-operated drone warfare contexts, imbuing AI with
certain traits of human reasoning.

</details>


### [2] [XAI-Guided Analysis of Residual Networks for Interpretable Pneumonia Detection in Paediatric Chest X-rays](https://arxiv.org/abs/2507.18647)
*Rayyan Ridwan*

Main category: cs.CV

TL;DR: 提出了一种基于ResNet的可解释性深度学习模型，用于自动诊断儿童肺炎，并通过BayesGrad-CAM提升了解释性，在大规模胸片数据集上取得了高性能。


<details>
  <summary>Details</summary>
Motivation: 儿童肺炎是全球儿童死亡的主要原因之一，迫切需要快速准确的自动诊断工具。

Method: 基于深度残差网络（ResNet-50）开发自动诊断模型，引入Bayesian Gradient-weighted Class Activation Mapping（BayesGrad-CAM），提升模型决策过程的可解释性并量化可视化解释的不确定性。

Result: 模型在大规模儿童胸片数据集上表现优异，分类准确率达95.94%，AUC-ROC为98.91%，Cohen's Kappa为0.913，且生成了临床上有意义的可视化解释。

Conclusion: 研究证明高性能和可解释性可以兼得，且对于临床AI应用至关重要。

Abstract: Pneumonia remains one of the leading causes of death among children
worldwide, underscoring a critical need for fast and accurate diagnostic tools.
In this paper, we propose an interpretable deep learning model on Residual
Networks (ResNets) for automatically diagnosing paediatric pneumonia on chest
X-rays. We enhance interpretability through Bayesian Gradient-weighted Class
Activation Mapping (BayesGrad-CAM), which quantifies uncertainty in visual
explanations, and which offers spatial locations accountable for the
decision-making process of the model. Our ResNet-50 model, trained on a large
paediatric chest X-rays dataset, achieves high classification accuracy
(95.94%), AUC-ROC (98.91%), and Cohen's Kappa (0.913), accompanied by
clinically meaningful visual explanations. Our findings demonstrate that high
performance and interpretability are not only achievable but critical for
clinical AI deployment.

</details>


### [3] [Livatar-1: Real-Time Talking Heads Generation with Tailored Flow Matching](https://arxiv.org/abs/2507.18649)
*Haiyang Liu,Xiaolin Hong,Xuancheng Yang,Yudi Ruan,Xiang Lian,Michael Lingelbach,Hongwei Yi,Wei Li*

Main category: cs.CV

TL;DR: 论文提出了Livatar，一种实时音频驱动的数字人头像生成系统，通过创新的flow matching方法，显著提升了口型同步准确度和避免长时间姿态漂移，同时系统在硬件上实现高帧率和低延迟，具备实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动的数字人技术在口型同步和姿态稳定性方面表现有限，无法满足高质量或实时交互的应用需求。因此有必要开发一种既准确又高效的新框架来提升数字人生成体验。

Method: 采用基于flow matching的框架，对系统进行多项优化以提升生成速度和同步质量，并在硬件（单张A10 GPU）上实现高效端到端推理。

Result: Livatar在HDTF数据集上实现了8.50的LipSync Confidence，显示出优异的口型同步能力；同时支持141FPS的推理速度和0.17秒延迟，测试硬件为A10 GPU。

Conclusion: Livatar在保持高真实度和极低延迟的前提下，为音频驱动数字人生成提供了更高质量的方案，有望推动其在更广泛实际场景中的应用。

Abstract: We present Livatar, a real-time audio-driven talking heads videos generation
framework. Existing baselines suffer from limited lip-sync accuracy and
long-term pose drift. We address these limitations with a flow matching based
framework. Coupled with system optimizations, Livatar achieves competitive
lip-sync quality with a 8.50 LipSync Confidence on the HDTF dataset, and
reaches a throughput of 141 FPS with an end-to-end latency of 0.17s on a single
A10 GPU. This makes high-fidelity avatars accessible to broader applications.
Our project is available at https://www.hedra.com/ with with examples at
https://h-liu1997.github.io/Livatar-1/

</details>


### [4] [Features extraction for image identification using computer vision](https://arxiv.org/abs/2507.18650)
*Venant Niyonkuru,Sylla Sekou,Jimmy Jackson Sinzinkayo*

Main category: cs.CV

TL;DR: 本文综合评述了计算机视觉中的多种特征提取方法，尤其聚焦于Vision Transformer（ViT）与其他主流方法的对比，分析其优缺点及实际应用。


<details>
  <summary>Details</summary>
Motivation: 近年来以ViT为代表的新型特征提取方法在视觉领域表现突出，但传统方法和深度学习方法各具优势，因此需要系统比较各种方法，以指导实际选择和应用。

Method: 分析和梳理了ViT的核心架构（如Patch Embedding、位置编码、多头自注意力机制），并与GANs、深度特征模型、传统方法（SIFT、SURF、ORB）、对比/非对比特征模型等进行了实验性能对比。

Result: 实验结果揭示了ViT及各方法在不同场景下的优劣势，ViT在准确率等方面超越常规CNN，但在某些方面可能存在局限。

Conclusion: ViT在特征提取和实际应用中展现出强大潜力，未来可结合各方法优势推动计算机视觉发展，同时需权衡不同技术的适用性。

Abstract: This study examines various feature extraction techniques in computer vision,
the primary focus of which is on Vision Transformers (ViTs) and other
approaches such as Generative Adversarial Networks (GANs), deep feature models,
traditional approaches (SIFT, SURF, ORB), and non-contrastive and contrastive
feature models. Emphasizing ViTs, the report summarizes their architecture,
including patch embedding, positional encoding, and multi-head self-attention
mechanisms with which they overperform conventional convolutional neural
networks (CNNs). Experimental results determine the merits and limitations of
both methods and their utilitarian applications in advancing computer vision.

</details>


### [5] [Adapt, But Don't Forget: Fine-Tuning and Contrastive Routing for Lane Detection under Distribution Shift](https://arxiv.org/abs/2507.18653)
*Mohammed Abdul Hafeez Khan,Parth Ganeriwala,Sarah M. Lehman,Siddhartha Bhattacharyya,Amy Alvarez,Natasha Neogi*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的车道线检测模型适应方法，通过为不同数据分布建立独立分支，结合监督对比学习实现输入分布识别与动态路由，显著节省参数同时性能接近最优。


<details>
  <summary>Details</summary>
Motivation: 现有车道线检测模型大多在单一数据集上评测，但不同数据集（即使同一领域内）的分布变化会导致模型迁移时遗忘已学知识（灾难性遗忘），降低泛化性能。

Method: 作者先在源分布上训练基础模型，然后针对每一个新目标分布创建独立分支，仅微调特定组件，保证源分支参数固定。同时，通过组件级分析，找到高效的微调策略，并在推理阶段利用监督对比学习自动判断输入分布，将其路由到相应分支。

Result: 该方法在多个数据集上均表现出几乎最优的F1分数，而且总参数量远低于为每个分布单独训练模型的做法。

Conclusion: 本文提出的参数高效分支适应框架，能有效应对车道线检测中的分布偏移与灾难性遗忘，提升了模型的泛化与适应能力。

Abstract: Lane detection models are often evaluated in a closed-world setting, where
training and testing occur on the same dataset. We observe that, even within
the same domain, cross-dataset distribution shifts can cause severe
catastrophic forgetting during fine-tuning. To address this, we first train a
base model on a source distribution and then adapt it to each new target
distribution by creating separate branches, fine-tuning only selected
components while keeping the original source branch fixed. Based on a
component-wise analysis, we identify effective fine-tuning strategies for
target distributions that enable parameter-efficient adaptation. At inference
time, we propose using a supervised contrastive learning model to identify the
input distribution and dynamically route it to the corresponding branch. Our
framework achieves near-optimal F1-scores while using significantly fewer
parameters than training separate models for each distribution.

</details>


### [6] [Part Segmentation of Human Meshes via Multi-View Human Parsing](https://arxiv.org/abs/2507.18655)
*James Dickens,Kamyar Hamad*

Main category: cs.CV

TL;DR: 本文提出了一种用于大规模人体网格逐点语义分割的新方法，通过构建伪真值标签流程和高效采样策略，提升了分割效果，并在无纹理信息下依然取得了较高精度。


<details>
  <summary>Details</summary>
Motivation: 点云深度学习和人体解析分别在点云几何分割和图像人体部件分割上取得了重大进展，但二者尚未在3D人体网格点的语义分割上有效结合，因此急需一种新方法解决大规模人体网格的精准分割需求。

Method: 1. 对Thuman2.1数据集中的网格进行姿态标准化对齐，并从多个视角进行分割，最后将点云标签回投到原始网格上生成伪真值；2. 提出基于空间填充曲线序列化的窗口化迭代最远点采样（FPS）策略，有效进行下采样；3. 在无纹理信息下，利用PointTransformer实现点云的几何分割。

Result: 所提方法能够高效、准确地实现人体网格的逐点语义分割，对比传统方法在无纹理条件下具备更优性能，实验验证了系统各模块的有效性和精确度。

Conclusion: 通过与点云学习和人体解析的结合，本文在大规模人体网格分割领域取得了突破，为未来人体理解和3D人体建模应用打下了坚实基础。

Abstract: Recent advances in point cloud deep learning have led to models that achieve
high per-part labeling accuracy on large-scale point clouds, using only the raw
geometry of unordered point sets. In parallel, the field of human parsing
focuses on predicting body part and clothing/accessory labels from images. This
work aims to bridge these two domains by enabling per-vertex semantic
segmentation of large-scale human meshes. To achieve this, a pseudo-ground
truth labeling pipeline is developed for the Thuman2.1 dataset: meshes are
first aligned to a canonical pose, segmented from multiple viewpoints, and the
resulting point-level labels are then backprojected onto the original mesh to
produce per-point pseudo ground truth annotations. Subsequently, a novel,
memory-efficient sampling strategy is introduced, a windowed iterative farthest
point sampling (FPS) with space-filling curve-based serialization to
effectively downsample the point clouds. This is followed by a purely geometric
segmentation using PointTransformer, enabling semantic parsing of human meshes
without relying on texture information. Experimental results confirm the
effectiveness and accuracy of the proposed approach.

</details>


### [7] [ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision Avoidance in Machine Learning-based Advanced Driver Assistance Systems](https://arxiv.org/abs/2507.18656)
*Muhammad Zaeem Shahzad,Muhammad Abdullah Hanif,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: 本文提出了一种名为ShrinkBox的新型后门攻击方法，通过微妙地缩小目标检测框，对基于深度学习的驾驶辅助系统（ML-ADAS）中的距离估计产生严重干扰。该攻击在当前检测手段下不易察觉，却大幅提升下游距离估计误差，危及碰撞预警功能。


<details>
  <summary>Details</summary>
Motivation: 当前先进驾驶辅助系统多依赖昂贵传感器（如LiDAR、雷达），制约了其普及。基于视觉的机器学习解决方案虽然成本低，却面临安全隐患，尤其是物体检测器的攻击风险。准确检测物体与距离正是ML-ADAS的核心，因此提升相应系统的安全性和鲁棒性迫在眉睫。

Method: 作者设计了一种后门攻击ShrinkBox，不同于以往通过更改类别标签或虚假目标扰乱检测结果，而是通过在训练阶段毒化数据，微缩目标检测的真实标注框。以YOLOv9m为例，仅用4%的KITTI训练集攻击实例，就能在标准评测中不被察觉，攻击成功率高达96%。

Result: ShrinkBox成功毒化YOLOv9m检测器后，能在保持标准检测性能的同时，使得下游距离估计MAE提升超3倍，严重削弱碰撞预警效果，且该毒化策略仅需少量训练样本即可奏效。

Conclusion: 传统基于图像的ADAS在安全性方面存在较大隐患，ShrinkBox展示了通过隐蔽数据毒化即可大幅削弱系统核心功能的可能性。未来亟需发展更加健壮的检测器及针对后门攻击的检测与防御手段，确保安全应用于现实道路场景。

Abstract: Advanced Driver Assistance Systems (ADAS) significantly enhance road safety
by detecting potential collisions and alerting drivers. However, their reliance
on expensive sensor technologies such as LiDAR and radar limits accessibility,
particularly in low- and middle-income countries. Machine learning-based ADAS
(ML-ADAS), leveraging deep neural networks (DNNs) with only standard camera
input, offers a cost-effective alternative. Critical to ML-ADAS is the
collision avoidance feature, which requires the ability to detect objects and
estimate their distances accurately. This is achieved with specialized DNNs
like YOLO, which provides real-time object detection, and a lightweight,
detection-wise distance estimation approach that relies on key features
extracted from the detections like bounding box dimensions and size. However,
the robustness of these systems is undermined by security vulnerabilities in
object detectors. In this paper, we introduce ShrinkBox, a novel backdoor
attack targeting object detection in collision avoidance ML-ADAS. Unlike
existing attacks that manipulate object class labels or presence, ShrinkBox
subtly shrinks ground truth bounding boxes. This attack remains undetected in
dataset inspections and standard benchmarks while severely disrupting
downstream distance estimation. We demonstrate that ShrinkBox can be realized
in the YOLOv9m object detector at an Attack Success Rate (ASR) of 96%, with
only a 4% poisoning ratio in the training instances of the KITTI dataset.
Furthermore, given the low error targets introduced in our relaxed poisoning
strategy, we find that ShrinkBox increases the Mean Absolute Error (MAE) in
downstream distance estimation by more than 3x on poisoned samples, potentially
resulting in delays or prevention of collision warnings altogether.

</details>


### [8] [VGS-ATD: Robust Distributed Learning for Multi-Label Medical Image Classification Under Heterogeneous and Imbalanced Conditions](https://arxiv.org/abs/2507.18657)
*Zehui Zhao,Laith Alzubaidi,Haider A. Alwzwazy,Jinglan Zhang,Yuantong Gu*

Main category: cs.CV

TL;DR: 提出了一种名为VGS-ATD的新型分布式学习框架，在医学影像多数据集、多标签场景中，相较于中心化、联邦与群体学习方法，在隐私保护、准确率、可扩展性和计算效率等方面具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 当前医学影像AI模型多依赖中心化方式训练，带来严重的隐私风险。联邦学习和群体学习虽然能保护隐私，但在数据异质、非均衡和高效通信等方面存在劣势，而且在系统扩展时容易遗忘旧知识或需要代价高昂的重新训练。实际临床环境动态复杂，亟需一种能够持续学习、多模态、多标签且高效扩展的AI系统。

Method: 提出了VGS-ATD分布式学习框架，实现仅共享模型权重的节点间协作，有效应对数据异质和扩展性问题。通过在30个数据集与80个独立标签的分布式节点上进行对比实验，全面评价方式与性能。

Result: VGS-ATD在总体准确率上达92.7%，高于中心化学习的84.9%与群体学习的72.99%，联邦学习因硬件资源要求高未能适应。模型扩展后，原节点准确率仅下降1%，远优于中心化方案的20%；同时计算成本降低可达50%。

Conclusion: VGS-ATD显著提升了分布式医学影像AI训练的隐私保护、准确率、可扩展性和效率，能够有效抵抗灾难性遗忘，适用于动态复杂的临床应用环境。

Abstract: In recent years, advanced deep learning architectures have shown strong
performance in medical imaging tasks. However, the traditional centralized
learning paradigm poses serious privacy risks as all data is collected and
trained on a single server. To mitigate this challenge, decentralized
approaches such as federated learning and swarm learning have emerged, allowing
model training on local nodes while sharing only model weights. While these
methods enhance privacy, they struggle with heterogeneous and imbalanced data
and suffer from inefficiencies due to frequent communication and the
aggregation of weights. More critically, the dynamic and complex nature of
clinical environments demands scalable AI systems capable of continuously
learning from diverse modalities and multilabels. Yet, both centralized and
decentralized models are prone to catastrophic forgetting during system
expansion, often requiring full model retraining to incorporate new data. To
address these limitations, we propose VGS-ATD, a novel distributed learning
framework. To validate VGS-ATD, we evaluate it in experiments spanning 30
datasets and 80 independent labels across distributed nodes, VGS-ATD achieved
an overall accuracy of 92.7%, outperforming centralized learning (84.9%) and
swarm learning (72.99%), while federated learning failed under these conditions
due to high requirements on computational resources. VGS-ATD also demonstrated
strong scalability, with only a 1% drop in accuracy on existing nodes after
expansion, compared to a 20% drop in centralized learning, highlighting its
resilience to catastrophic forgetting. Additionally, it reduced computational
costs by up to 50% relative to both centralized and swarm learning, confirming
its superior efficiency and scalability.

</details>


### [9] [Fuzzy Theory in Computer Vision: A Review](https://arxiv.org/abs/2507.18660)
*Adilet Yerkin,Ayan Igali,Elnara Kadyrgali,Maksat Shagyrov,Malika Ziyada,Muragul Muratbekova,Pakizar Shamoi*

Main category: cs.CV

TL;DR: 本文综述了模糊逻辑在计算机视觉领域中的应用，包括对象识别、图像分割和特征提取等方面，强调模糊方法在处理不确定性、噪声和数据不精确性时的优势，并探讨了与深度学习模型的结合及未来趋势。


<details>
  <summary>Details</summary>
Motivation: 传统计算机视觉方法在处理带有不确定性和噪声的图像数据时存在局限。本文旨在解决现有方法对复杂、不精确视觉信息处理能力不足的问题，提升系统的适应性和解释能力。

Method: 文章综述了多种模糊逻辑相关技术，包括模糊聚类、模糊推理系统、二型模糊集以及基于规则的决策方法，同时讨论模糊逻辑与深度学习（如CNN）的集成方式，系统分析其在不同应用领域的表现。

Result: 模糊逻辑方法在对象识别、图像分割和特征提取等视觉任务中表现出比传统方法更强的适应性和可解释性，特别是在医疗成像、自动化系统和工业检测等实际应用场景效果突出。与深度学习结合后，可进一步提升复杂任务的性能。

Conclusion: 模糊逻辑为解决计算机视觉中的不确定性问题提供了有效工具，与深度学习模型的结合是未来的重要发展方向，混合模糊-深度学习和可解释AI是当前值得关注的研究热点。

Abstract: Computer vision applications are omnipresent nowadays. The current paper
explores the use of fuzzy logic in computer vision, stressing its role in
handling uncertainty, noise, and imprecision in image data. Fuzzy logic is able
to model gradual transitions and human-like reasoning and provides a promising
approach to computer vision. Fuzzy approaches offer a way to improve object
recognition, image segmentation, and feature extraction by providing more
adaptable and interpretable solutions compared to traditional methods. We
discuss key fuzzy techniques, including fuzzy clustering, fuzzy inference
systems, type-2 fuzzy sets, and fuzzy rule-based decision-making. The paper
also discusses various applications, including medical imaging, autonomous
systems, and industrial inspection. Additionally, we explore the integration of
fuzzy logic with deep learning models such as convolutional neural networks
(CNNs) to enhance performance in complex vision tasks. Finally, we examine
emerging trends such as hybrid fuzzy-deep learning models and explainable AI.

</details>


### [10] [Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by Reinforcement Learning from Visual Map Feed Back](https://arxiv.org/abs/2507.18661)
*Ruixing Zhang,Yang Zhang,Tongyu Zhu,Leilei Sun,Weifeng Lv*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉-语言模型（VLMs）和地图可视化的下一位置预测方法，通过在地图上渲染轨迹与道路网络，利用VLM进行类人推断，显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 人类在预测轨迹下一位置时，会结合地图、道路连通性和移动趋势进行推理，但现有方法大多忽略了这种类人空间推理方式。VLMs在视觉理解和推理方面表现强大，有可能让模型更贴近人类的推理方式。

Method: 首先提出Vision-Guided Location Search (VGLS)方法，检验通用VLM对轨迹推理的能力。随后，设计主要方法VLMLocPredictor，包括两个阶段：一是通过两种有监督微调任务训练VLM理解道路和轨迹结构并获得基础推理能力，二是采用基于视觉地图反馈的强化学习，让模型在环境中自我提升位置预测能力。

Result: 在四个城市真实数据集上实验，VLMLocPredictor取得了新SOTA效果，且相比其他LLM基方法表现出更优秀的跨城市泛化能力。

Conclusion: 结合视觉-语言模型与类人地图推理，可以大幅提升移动轨迹下一位置预测精度，并改善模型的泛化能力，对交通、城市治理等领域具有实际应用意义。

Abstract: Next Location Prediction is a fundamental task in the study of human
mobility, with wide-ranging applications in transportation planning, urban
governance, and epidemic forecasting. In practice, when humans attempt to
predict the next location in a trajectory, they often visualize the trajectory
on a map and reason based on road connectivity and movement trends. However,
the vast majority of existing next-location prediction models do not reason
over maps \textbf{in the way that humans do}. Fortunately, the recent
development of Vision-Language Models (VLMs) has demonstrated strong
capabilities in visual perception and even visual reasoning. This opens up a
new possibility: by rendering both the road network and trajectory onto an
image and leveraging the reasoning abilities of VLMs, we can enable models to
perform trajectory inference in a human-like manner. To explore this idea, we
first propose a method called Vision-Guided Location Search (VGLS), which
evaluates whether a general-purpose VLM is capable of trajectory-based
reasoning without modifying any of its internal parameters. Based on insights
from the VGLS results, we further propose our main approach: VLMLocPredictor,
which is composed of two stages: In the first stage, we design two Supervised
Fine-Tuning (SFT) tasks that help the VLM understand road network and
trajectory structures and acquire basic reasoning ability on such visual
inputs. In the second stage, we introduce Reinforcement Learning from Visual
Map Feedback, enabling the model to self-improve its next-location prediction
ability through interaction with the environment. Experiments conducted on
datasets from four different cities show that our method achieves
state-of-the-art (SOTA) performance and exhibits superior cross-city
generalization compared to other LLM-based approaches.

</details>


### [11] [Gen-AI Police Sketches with Stable Diffusion](https://arxiv.org/abs/2507.18667)
*Nicholas Fidalgo,Aaron Contreras,Katherine Harvey,Johnny Ni*

Main category: cs.CV

TL;DR: 本项目比较了三种AI多模态方法以自动化和提升嫌疑人画像。结果显示，最基础的图像到图像模型虽简单但生成效果最佳。


<details>
  <summary>Details</summary>
Motivation: 嫌疑人画像在案件侦查中至关重要，人工绘制费时且易受主观影响，因此希望用AI自动化并提升画像质量。

Method: 开发并评估了三种方法：（1）基础Stable Diffusion图像到图像模型；（2）同模型结合预训练CLIP模型用于文本与图像对齐；（3）提出将LoRA微调应用于CLIP自注意力和交叉注意力层，并集成至Stable Diffusion的新方法。还进行了消融实验分析不同层微调效果。

Result: 结果显示模型1（基础模型）在结构相似性（SSIM 0.72）和峰值信噪比（PSNR 25dB）上表现最佳，超过模型2和3。反复优化后，模型3在感知相似性指标（LPIPS）上有提升，但仍不及模型1。模型1生成的面部特征最清晰，表现出色。

Conclusion: 基础的Stable Diffusion模型因其鲁棒性和生成质量成为嫌疑人画像任务中的最优选择，复杂的多模态和微调方法目前反而不及简单基线有效。

Abstract: This project investigates the use of multimodal AI-driven approaches to
automate and enhance suspect sketching. Three pipelines were developed and
evaluated: (1) baseline image-to-image Stable Diffusion model, (2) same model
integrated with a pre-trained CLIP model for text-image alignment, and (3)
novel approach incorporating LoRA fine-tuning of the CLIP model, applied to
self-attention and cross-attention layers, and integrated with Stable
Diffusion. An ablation study confirmed that fine-tuning both self- and
cross-attention layers yielded the best alignment between text descriptions and
sketches. Performance testing revealed that Model 1 achieved the highest
structural similarity (SSIM) of 0.72 and a peak signal-to-noise ratio (PSNR) of
25 dB, outperforming Model 2 and Model 3. Iterative refinement enhanced
perceptual similarity (LPIPS), with Model 3 showing improvement over Model 2
but still trailing Model 1. Qualitatively, sketches generated by Model 1
demonstrated the clearest facial features, highlighting its robustness as a
baseline despite its simplicity.

</details>


### [12] [Advancing Vision-based Human Action Recognition: Exploring Vision-Language CLIP Model for Generalisation in Domain-Independent Tasks](https://arxiv.org/abs/2507.18675)
*Sanyam Jain,Marsha Mariya Kappan,Vijeta Sharma*

Main category: cs.CV

TL;DR: 本文分析了CLIP在动作识别中的表现，发现其在关键视觉信息遮挡时易误判，并提出通过类特定噪声增强其识别准确性和鲁棒性，为临床健康领域应用提供改进方向。


<details>
  <summary>Details</summary>
Motivation: 传统动作识别方法（如CNN、RNN）在医疗健康应用中泛化能力有限。随着CLIP等先进视觉-语言模型的出现，研究其在视频动作识别任务上的潜力，特别是在应对多样、复杂动作时的表现及局限性，是提升模型泛化能力的迫切需求。

Method: 作者在UCF-101数据集上系统评估了CLIP模型，采用三种遮挡策略：基于比例与形状的黑色遮挡（10%、30%、50%），特征针对性遮挡（抑制带偏差元素），以及仅保留类别相关区域的隔离遮挡。进一步提出通过自定义损失学习类特定噪声来增强模型关注类别关键特征。

Result: CLIP模型在遮挡关键视觉线索时，表现出不稳定和频繁误判。引入类特定噪声后，模型的分类准确率、信心水平提升，且偏差有所减少。

Conclusion: 尽管CLIP展示了一定的潜力，但其对视觉遮挡高度敏感，限制了在医疗等域的泛化能力。引入关注类关键特征的新方法可有效提升模型表现，未来需进一步改进以满足实际健康场景中的广泛需求。

Abstract: Human action recognition plays a critical role in healthcare and medicine,
supporting applications such as patient behavior monitoring, fall detection,
surgical robot supervision, and procedural skill assessment. While traditional
models like CNNs and RNNs have achieved moderate success, they often struggle
to generalize across diverse and complex actions. Recent advancements in
vision-language models, especially the transformer-based CLIP model, offer
promising capabilities for generalizing action recognition from video data. In
this work, we evaluate CLIP on the UCF-101 dataset and systematically analyze
its performance under three masking strategies: (1) percentage-based and
shape-based black masking at 10%, 30%, and 50%, (2) feature-specific masking to
suppress bias-inducing elements, and (3) isolation masking that retains only
class-specific regions. Our results reveal that CLIP exhibits inconsistent
behavior and frequent misclassifications, particularly when essential visual
cues are obscured. To overcome these limitations, we propose incorporating
class-specific noise, learned via a custom loss function, to reinforce
attention to class-defining features. This enhancement improves classification
accuracy and model confidence while reducing bias. We conclude with a
discussion on the challenges of applying such models in clinical domains and
outline directions for future work to improve generalizability across
domain-independent healthcare scenarios.

</details>


### [13] [HeartUnloadNet: A Weakly-Supervised Cycle-Consistent Graph Network for Predicting Unloaded Cardiac Geometry from Diastolic States](https://arxiv.org/abs/2507.18677)
*Siyu Mu,Wei Xuan Chan,Choon Hwai Yap*

Main category: cs.CV

TL;DR: 本论文提出了HeartUnloadNet，一种基于深度学习的新方法，可以从心脏舒张末期（ED）网格直接预测心脏在无负荷状态下的几何形状。它更快、更准且对临床应用有巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 当前从临床影像中估计心脏无负荷几何形态的过程依赖于反有限元方法，这种方法计算量大、效率低，不利于临床实时应用。因此，有必要开发更高效、精准的方法用于心脏个性化生物力学建模。

Method: 提出了HeartUnloadNet，采用图注意力网络结构和cycle-consistency策略，实现了左心室加载与卸载几何之间的双向映射。该网络输入可以为任意大小的网格，并结合生理参数（如ED压力、心肌刚度、纤维螺旋方向），输出对应的无负荷心肌网格。训练时使用了20,700组不同几何和生理条件下的有限元模拟数据。

Result: HeartUnloadNet在端到端预测无负荷心室几何方面达到了亚毫米精度（DSC 0.986，HD 0.083cm），推断时间仅0.02秒，比传统方法快10万倍，准确率也更高。消融实验表明其架构和部分自监督方法有效，少量数据下（200例）也能保持97% DSC。

Conclusion: 该方法为反有限元求解器提供了高效、准确的替代方案，具备良好的扩展性和实时性，有望应用于临床心脏生物力学建模与干预预测。

Abstract: The unloaded cardiac geometry (i.e., the state of the heart devoid of luminal
pressure) serves as a valuable zero-stress and zero-strain reference and is
critical for personalized biomechanical modeling of cardiac function, to
understand both healthy and diseased physiology and to predict the effects of
cardiac interventions. However, estimating the unloaded geometry from clinical
images remains a challenging task. Traditional approaches rely on inverse
finite element (FE) solvers that require iterative optimization and are
computationally expensive. In this work, we introduce HeartUnloadNet, a deep
learning framework that predicts the unloaded left ventricular (LV) shape
directly from the end diastolic (ED) mesh while explicitly incorporating
biophysical priors. The network accepts a mesh of arbitrary size along with
physiological parameters such as ED pressure, myocardial stiffness scale, and
fiber helix orientation, and outputs the corresponding unloaded mesh. It adopts
a graph attention architecture and employs a cycle-consistency strategy to
enable bidirectional (loading and unloading) prediction, allowing for partial
self-supervision that improves accuracy and reduces the need for large training
datasets. Trained and tested on 20,700 FE simulations across diverse LV
geometries and physiological conditions, HeartUnloadNet achieves sub-millimeter
accuracy, with an average DSC of 0.986 and HD of 0.083 cm, while reducing
inference time to just 0.02 seconds per case, over 10^5 times faster and
significantly more accurate than traditional inverse FE solvers. Ablation
studies confirm the effectiveness of the architecture. Notably, the
cycle-consistent design enables the model to maintain a DSC of 97% even with as
few as 200 training samples. This work thus presents a scalable and accurate
surrogate for inverse FE solvers, supporting real-time clinical applications in
the future.

</details>


### [14] [SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time](https://arxiv.org/abs/2507.18713)
*Yun Chen,Matthew Haines,Jingkang Wang,Krzysztof Baron-Lis,Sivabalan Manivasagam,Ze Yang,Raquel Urtasun*

Main category: cs.CV

TL;DR: 本文提出了一种新的体积表达方法Sparse Local Fields (SaLF)，可高效支持光学传感器（相机和LiDAR）的高保真模拟，训练和渲染速度较现有NeRF和3DGS方法显著提升，且能支持不同类型的传感器。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF和3DGS的传感器仿真方法要么渲染速度慢、训练成本高，要么仅支持有限类型的传感器（如针孔式相机），且难以通用，限制了自动驾驶场景的可拓展高保真仿真需求。

Method: 提出Sparse Local Fields（SaLF）作为一种稀疏三维体素原语集合，其中每个体素内是局部隐式场。通过支持栅格化和光线跟踪，SaLF可兼容不同渲染方法，适用于相机和LiDAR等多类型传感器，并通过自适应裁剪和加密应对大场景。

Result: SaLF展现了与主流方法相当的仿真真实感，训练时间小于30分钟，渲染速度对于相机>50FPS，LiDAR>600FPS，并成功支持非针孔相机和旋转式LiDAR。

Conclusion: SaLF在提升效率与支持多样传感器的同时保持了高真实感，解决了现有方法通用性与效率的局限，为大规模自动驾驶仿真模拟带来新的可能。

Abstract: High-fidelity sensor simulation of light-based sensors such as cameras and
LiDARs is critical for safe and accurate autonomy testing. Neural radiance
field (NeRF)-based methods that reconstruct sensor observations via ray-casting
of implicit representations have demonstrated accurate simulation of driving
scenes, but are slow to train and render, hampering scale. 3D Gaussian
Splatting (3DGS) has demonstrated faster training and rendering times through
rasterization, but is primarily restricted to pinhole camera sensors,
preventing usage for realistic multi-sensor autonomy evaluation. Moreover, both
NeRF and 3DGS couple the representation with the rendering procedure (implicit
networks for ray-based evaluation, particles for rasterization), preventing
interoperability, which is key for general usage. In this work, we present
Sparse Local Fields (SaLF), a novel volumetric representation that supports
rasterization and raytracing. SaLF represents volumes as a sparse set of 3D
voxel primitives, where each voxel is a local implicit field. SaLF has fast
training (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS
LiDAR), has adaptive pruning and densification to easily handle large scenes,
and can support non-pinhole cameras and spinning LiDARs. We demonstrate that
SaLF has similar realism as existing self-driving sensor simulation methods
while improving efficiency and enhancing capabilities, enabling more scalable
simulation. https://waabi.ai/salf/

</details>


### [15] [Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting](https://arxiv.org/abs/2507.18678)
*Xingyu Miao,Haoran Duan,Quanhao Qian,Jiuniu Wang,Yang Long,Ling Shao,Deli Zhao,Ran Xu,Gongjie Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种可扩展管道，将单视图图像自动转换为高质量3D数据，有效缓解3D数据稀缺问题，并发布了两个新数据集COCO-3D和Objects365-v2-3D，对3D任务均具提升作用。


<details>
  <summary>Details</summary>
Motivation: 空间智能的发展受限于大规模3D数据的缺乏，而相较2D数据，3D数据采集昂贵且繁琐。该研究旨在减少数据采集成本，利用已有海量图像实现3D数据自动化生成，以推动空间智能及相关AI研究。

Method: 作者开发了一套自动化管道，结合深度估计、相机校准与尺度校准技术，将单张2D图像转化为真实、带尺度感的3D点云、相机位姿、深度图和伪RGBD数据。系统可大规模处理公开图片资源，生成质量达标的3D数据。

Result: 该方法生成了两个大型3D数据集：COCO-3D及Objects365-v2-3D。通过大量实验，作者验证了生成数据可广泛提升3D感知、推理等多类任务，有助于空间场景理解相关AI模型性能提升。

Conclusion: 该管道在大幅降低3D数据获取成本的同时，显著推动了空间智能研究进展。所生成的数据与方法为AI系统理解和交互物理环境提供了坚实基础，具有广泛应用前景。

Abstract: Spatial intelligence is emerging as a transformative frontier in AI, yet it
remains constrained by the scarcity of large-scale 3D datasets. Unlike the
abundant 2D imagery, acquiring 3D data typically requires specialized sensors
and laborious annotation. In this work, we present a scalable pipeline that
converts single-view images into comprehensive, scale- and appearance-realistic
3D representations - including point clouds, camera poses, depth maps, and
pseudo-RGBD - via integrated depth estimation, camera calibration, and scale
calibration. Our method bridges the gap between the vast repository of imagery
and the increasing demand for spatial scene understanding. By automatically
generating authentic, scale-aware 3D data from images, we significantly reduce
data collection costs and open new avenues for advancing spatial intelligence.
We release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D,
and demonstrate through extensive experiments that our generated data can
benefit various 3D tasks, ranging from fundamental perception to MLLM-based
reasoning. These results validate our pipeline as an effective solution for
developing AI systems capable of perceiving, understanding, and interacting
with physical environments.

</details>


### [16] [Diffusion-FS: Multimodal Free-Space Prediction via Diffusion for Autonomous Driving](https://arxiv.org/abs/2507.18763)
*Keshav Gupta,Tejas S. Stanley,Pranjal Paul,Arun K. Singh,K. Madhava Krishna*

Main category: cs.CV

TL;DR: 本文提出了一种基于纯视觉(单目摄像头)的可行驶空间走廊预测方法，用于无人驾驶，并通过自监督和新型扩散模型提升了走廊分割的精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的可行驶空间预测主要关注的是整个无障碍道路区域，忽视了车辆实际可行驶的走廊。而已有的走廊方法大多依赖鸟瞰视角(BEV)表示，现实中较难直接获取。作者致力于实现更实用的、基于常规摄像头的走廊预测方法。

Method: 创新点包括：1) 将走廊预测转化为纯图像感知任务，只用单目前视摄像头；2) 由于缺少走廊标注，提出自监督方法，用未来自车轨迹数据和摄像头图像联合生成走廊样本；3) 引入扩散模型描述图像中走廊片段分布，并设计ContourDiff架构，以轮廓点而非二值Mask描述空间边界，提高结构化与可解释性。

Result: 在nuScenes和CARLA数据集上，通过定量和定性实验，证明方法能准确并多模态地预测图像中的安全走廊区域，效果优于传统Mask方法。

Conclusion: ContourDiff能更高效地实现基于单目图像的可解释走廊分割，有望提升无人驾驶系统的场景适应性和安全性。

Abstract: Drivable Free-space prediction is a fundamental and crucial problem in
autonomous driving. Recent works have addressed the problem by representing the
entire non-obstacle road regions as the free-space. In contrast our aim is to
estimate the driving corridors that are a navigable subset of the entire road
region. Unfortunately, existing corridor estimation methods directly assume a
BEV-centric representation, which is hard to obtain. In contrast, we frame
drivable free-space corridor prediction as a pure image perception task, using
only monocular camera input. However such a formulation poses several
challenges as one doesn't have the corresponding data for such free-space
corridor segments in the image. Consequently, we develop a novel
self-supervised approach for free-space sample generation by leveraging future
ego trajectories and front-view camera images, making the process of visual
corridor estimation dependent on the ego trajectory. We then employ a diffusion
process to model the distribution of such segments in the image. However, the
existing binary mask-based representation for a segment poses many limitations.
Therefore, we introduce ContourDiff, a specialized diffusion-based architecture
that denoises over contour points rather than relying on binary mask
representations, enabling structured and interpretable free-space predictions.
We evaluate our approach qualitatively and quantitatively on both nuScenes and
CARLA, demonstrating its effectiveness in accurately predicting safe multimodal
navigable corridors in the image.

</details>


### [17] [Perspective from a Higher Dimension: Can 3D Geometric Priors Help Visual Floorplan Localization?](https://arxiv.org/abs/2507.18881)
*Bolei Chen,Jiaxu Kang,Haonan Yang,Ping Zhong,Jianxin Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种注入3D几何先验的2D楼层平面图（Floorplan）视觉定位算法，在不增加计算量的情况下显著提升定位准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管楼层平面图定位因其稳定性和易获取性备受关注，但视觉感知与平面图的模态和几何差异，特别是3D物体导致的遮挡和视觉变化，造成现有方法定位精度不足。该研究旨在解决这些问题。

Method: 提出了将3D几何先验与视觉FLoc算法结合的方法。具体来说：（1）通过多视图约束建立具有几何感知的视图不变性；（2）通过关联场景表面重建与RGB图像，增强跨模态的几何-颜色对应。这些3D几何先验均用自监督对比学习建模，不需额外几何或语义标签。

Result: 在大量现实场景对比实验下，该方法远超当前主流方法，在楼层平面图定位准确率上有大幅提升。

Conclusion: 所提3D几何先验的自监督模型有效弥补了视觉与楼层图模态间的鸿沟，在不增加计算量的前提下，显著提升了视觉楼层图定位的性能。数据与代码将在匿名评审后开放。

Abstract: Since a building's floorplans are easily accessible, consistent over time,
and inherently robust to changes in visual appearance, self-localization within
the floorplan has attracted researchers' interest. However, since floorplans
are minimalist representations of a building's structure, modal and geometric
differences between visual perceptions and floorplans pose challenges to this
task. While existing methods cleverly utilize 2D geometric features and pose
filters to achieve promising performance, they fail to address the localization
errors caused by frequent visual changes and view occlusions due to variously
shaped 3D objects. To tackle these issues, this paper views the 2D Floorplan
Localization (FLoc) problem from a higher dimension by injecting 3D geometric
priors into the visual FLoc algorithm. For the 3D geometric prior modeling, we
first model geometrically aware view invariance using multi-view constraints,
i.e., leveraging imaging geometric principles to provide matching constraints
between multiple images that see the same points. Then, we further model the
view-scene aligned geometric priors, enhancing the cross-modal geometry-color
correspondences by associating the scene's surface reconstruction with the RGB
frames of the sequence. Both 3D priors are modeled through self-supervised
contrastive learning, thus no additional geometric or semantic annotations are
required. These 3D priors summarized in extensive realistic scenes bridge the
modal gap while improving localization success without increasing the
computational burden on the FLoc algorithm. Sufficient comparative studies
demonstrate that our method significantly outperforms state-of-the-art methods
and substantially boosts the FLoc accuracy. All data and code will be released
after the anonymous review.

</details>


### [18] [Learned Single-Pixel Fluorescence Microscopy](https://arxiv.org/abs/2507.18740)
*Serban C. Tudosie,Valerio Gandolfi,Shivaprasad Varakkoth,Andrea Farina,Cosimo D'Andrea,Simon Arridge*

Main category: cs.CV

TL;DR: 该论文提出通过自监督训练的自动编码器，优化荧光显微单像素成像的采集与重建流程，实现更快的重建速度和更高的图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前单像素荧光显微成像依赖于全变差最小化进行重建，存在重建速度慢、质量受限等问题。作者希望借助数据驱动方法，提升测量及重建的效率和质量。

Method: 采用自监督学习的自动编码器结构，训练采集端的测量矩阵（编码器）和重建端的解码器。训练后，将学习到的编码器集成到物理设备中，针对实际采集的多光谱和强度数据进行测试。

Result: 方法显著加快了单像素成像的重建速度（提升两数量级），提升了成像质量，并实现了高质量的多光谱图像重建。

Conclusion: 数据驱动的单像素荧光显微成像方法不仅提升了重建速度和质量，还降低了多光谱成像的成本，为医学诊断和生物研究带来新的技术路线。

Abstract: Single-pixel imaging has emerged as a key technique in fluorescence
microscopy, where fast acquisition and reconstruction are crucial. In this
context, images are reconstructed from linearly compressed measurements. In
practice, total variation minimisation is still used to reconstruct the image
from noisy measurements of the inner product between orthogonal sampling
pattern vectors and the original image data. However, data can be leveraged to
learn the measurement vectors and the reconstruction process, thereby enhancing
compression, reconstruction quality, and speed. We train an autoencoder through
self-supervision to learn an encoder (or measurement matrix) and a decoder. We
then test it on physically acquired multispectral and intensity data. During
acquisition, the learned encoder becomes part of the physical device. Our
approach can enhance single-pixel imaging in fluorescence microscopy by
reducing reconstruction time by two orders of magnitude, achieving superior
image quality, and enabling multispectral reconstructions. Ultimately, learned
single-pixel fluorescence microscopy could advance diagnosis and biological
research, providing multispectral imaging at a fraction of the cost.

</details>


### [19] [EffiComm: Bandwidth Efficient Multi Agent Communication](https://arxiv.org/abs/2507.19354)
*Melih Yazgan,Allen Xavier Arasan,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 本文提出了EffiComm，一个高效的车车感知信息通信框架，在保持先进检测精度的同时大幅减少数据传输量。


<details>
  <summary>Details</summary>
Motivation: 现有协同感知方法需要车辆互传大量传感器数据，特别是点云或完整特征图，导致V2V通信压力过大，带来高延迟和扩展性差等问题。因此需要创新方法降低通信负载。

Method: EffiComm以BEV特征图为基础，采用两阶段压缩流程：首先通过置信掩码进行有选择的特征区域传输，其次利用图神经网络根据车辆角色与通信负载自适应设定保留率。此外，融合阶段采用了软门控混合专家机制以提升特征整合能力。

Result: 在OPV2V基准上，EffiComm仅需传输约1.5MB/帧数据，实现了0.84的mAP@0.7，在准确率-通信开销曲线上优于先前方法。

Conclusion: EffiComm证明了具有适应性和学习能力的通信策略能实现可扩展的V2X感知，显著优化了感知精度与数据传输量的平衡。

Abstract: Collaborative perception allows connected vehicles to exchange sensor
information and overcome each vehicle's blind spots. Yet transmitting raw point
clouds or full feature maps overwhelms Vehicle-to-Vehicle (V2V) communications,
causing latency and scalability problems. We introduce EffiComm, an end-to-end
framework that transmits less than 40% of the data required by prior art while
maintaining state-of-the-art 3D object detection accuracy. EffiComm operates on
Bird's-Eye-View (BEV) feature maps from any modality and applies a two-stage
reduction pipeline: (1) Selective Transmission (ST) prunes low-utility regions
with a confidence mask; (2) Adaptive Grid Reduction (AGR) uses a Graph Neural
Network (GNN) to assign vehicle-specific keep ratios according to role and
network load. The remaining features are fused with a soft-gated
Mixture-of-Experts (MoE) attention layer, offering greater capacity and
specialization for effective feature integration. On the OPV2V benchmark,
EffiComm reaches 0.84 mAP@0.7 while sending only an average of approximately
1.5 MB per frame, outperforming previous methods on the accuracy-per-bit curve.
These results highlight the value of adaptive, learned communication for
scalable Vehicle-to-Everything (V2X) perception.

</details>


### [20] [KuiSCIMA v2.0: Improved Baselines, Calibration, and Cross-Notation Generalization for Historical Chinese Music Notations in Jiang Kui's Baishidaoren Gequ](https://arxiv.org/abs/2507.18741)
*Tristan Repolusk,Eduardo Veas*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，显著提升了对古代中国乐谱（如丝竹谱和律吕谱）的光学识别准确率，并首次对稀有不平衡数据实现了低字符错误率。


<details>
  <summary>Details</summary>
Motivation: 古代中国音乐记谱体系具有很强的文化价值，但因类别极度不平衡和训练数据极少，导致光学音乐识别（OMR）任务极具挑战性，亟需技术突破来促进其数字化和保护。

Method: 作者开发并评估了一种针对稀缺、不平衡数据的字符识别模型，通过引入温度缩放实现模型校准，并采用交叉验证方法保证模型在五个版本的乐谱中都表现稳健。此外，扩展KuiSCIMA数据集，纳入了《白石道人歌曲》全集的多种乐谱类型。

Result: 新模型将丝竹谱的字符错误率（CER）从10.4%降至7.1%，律吕谱达到0.9%。模型性能优于人工转录，且模型校准效果良好（ECE < 0.0162）。

Conclusion: 本研究推动了古谱数字化和光学识别技术在非主流音乐传统上的应用，有助于保护和传播中国古代音乐文化，也为OMR领域提供新基准。

Abstract: Optical Music Recognition (OMR) for historical Chinese musical notations,
such as suzipu and l\"ul\"upu, presents unique challenges due to high class
imbalance and limited training data. This paper introduces significant
advancements in OMR for Jiang Kui's influential collection Baishidaoren Gequ
from 1202. In this work, we develop and evaluate a character recognition model
for scarce imbalanced data. We improve upon previous baselines by reducing the
Character Error Rate (CER) from 10.4% to 7.1% for suzipu, despite working with
77 highly imbalanced classes, and achieve a remarkable CER of 0.9% for
l\"ul\"upu. Our models outperform human transcribers, with an average human CER
of 15.9% and a best-case CER of 7.6%. We employ temperature scaling to achieve
a well-calibrated model with an Expected Calibration Error (ECE) below 0.0162.
Using a leave-one-edition-out cross-validation approach, we ensure robust
performance across five historical editions. Additionally, we extend the
KuiSCIMA dataset to include all 109 pieces from Baishidaoren Gequ, encompassing
suzipu, l\"ul\"upu, and jianzipu notations. Our findings advance the
digitization and accessibility of historical Chinese music, promoting cultural
diversity in OMR and expanding its applicability to underrepresented music
traditions.

</details>


### [21] [Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization](https://arxiv.org/abs/2507.19459)
*Pol Francesch Huc,Emily Bates,Simone D'Amico*

Main category: cs.CV

TL;DR: 提出一种结合CNN和3D高斯Splatting的新型3D重建方法，大幅降低训练迭代次数和对姿态精度的需求，实现了仅用单目图像即可高效重建高保真3D模型，特别适用于空间应用领域。


<details>
  <summary>Details</summary>
Motivation: 现有新视图合成方法（如NeRF、3D Gaussian Splatting）在空间应用受限于对高精度姿态的依赖和高计算成本。实际空间任务中，姿态信息往往噪声大或隐含，资源有限，因此亟需一种低姿态依赖、低计算开销的3D建模方法。

Method: 提出一种新管线：先用CNN提取单幅图像，生成由基元装配的粗糙3D模型及目标的相机相对姿态，作为3DGS的初始化，大幅减少后续训练迭代与所需输入图像量。此外，提出CNN多种姿态估计变体，并分析其在弱/噪声姿态下的3D模型训练效果。

Result: 实验结果表明，所提方法在姿态监督不完美的情况下，依然能训练出高保真度3D重建结果，所需训练成本（迭代/图像数量）至少降低了一个数量级。多种变体在不同姿态噪声水平下表现优异。

Conclusion: 该方法突破了以往新视图合成对高精度姿态与高算力的依赖，为新视图合成在空间领域（如航天探测、遥感等）应用提供了高效可行的技术方案。

Abstract: The advent of novel view synthesis techniques such as NeRF and 3D Gaussian
Splatting (3DGS) has enabled learning precise 3D models only from posed
monocular images. Although these methods are attractive, they hold two major
limitations that prevent their use in space applications: they require poses
during training, and have high computational cost at training and inference. To
address these limitations, this work contributes: (1) a Convolutional Neural
Network (CNN) based primitive initializer for 3DGS using monocular images; (2)
a pipeline capable of training with noisy or implicit pose estimates; and (3)
and analysis of initialization variants that reduce the training cost of
precise 3D models. A CNN takes a single image as input and outputs a coarse 3D
model represented as an assembly of primitives, along with the target's pose
relative to the camera. This assembly of primitives is then used to initialize
3DGS, significantly reducing the number of training iterations and input images
needed -- by at least an order of magnitude. For additional flexibility, the
CNN component has multiple variants with different pose estimation techniques.
This work performs a comparison between these variants, evaluating their
effectiveness for downstream 3DGS training under noisy or implicit pose
estimates. The results demonstrate that even with imperfect pose supervision,
the pipeline is able to learn high-fidelity 3D representations, opening the
door for the use of novel view synthesis in space applications.

</details>


### [22] [SAR-TEXT: A Large-Scale SAR Image-Text Dataset Built with SAR-Narrator and Progressive Transfer Learning](https://arxiv.org/abs/2507.18743)
*Xinjun Cheng,Yiguo He,Junjie Zhu,Chunping Qiu,Jun Wang,Qiangjuan Huang,Ke Yang*

Main category: cs.CV

TL;DR: 本文开发了高质量大规模 SAR 图像-文本数据集 SAR-Text，并提出了自动生成文本描述的方法 SAR-Narrator，有效推动了视觉语言模型在遥感 SAR 数据上的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型（VLM）在遥感领域取得了突破，但缺乏大规模、高质量的 SAR 图像-文本配对数据集，限制了对 SAR 图像的语义理解能力。

Method: 作者提出了 SAR-Narrator 框架，采用多阶段渐进迁移学习，为 SAR 图像自动生成文本描述，构建了包含 13 万多对图像-文本的 SAR-Text 数据集。通过在该数据集上训练三种代表性模型（SAR-RS-CLIP、SAR-RS-CoCa、SAR-GPT），面向图像-文本检索、图像描述、视觉问答等任务进行实验验证。

Result: SAR-RS-CLIP 在图像-文本检索上提升了 16.43% 和 10.54% 的召回率，SAR-RS-CoCa 在图像描述任务上多项指标提升 4-10 倍，SAR-GPT 在 VQA 任务上多数据集超过基线模型，显示出更强的语义理解和推理能力。

Conclusion: SAR-Text 数据集与 SAR-Narrator 框架显著提升了 SAR 遥感图像与文本联合理解的效果，为相关研究提供了丰富数据资源和可迁移的数据构建工具，助力后续更大规模的数据集开发和模型创新。

Abstract: Vision Language Models (VLMs) have achieved remarkable breakthroughs in the
field of remote sensing in recent years. Synthetic Aperture Radar (SAR)
imagery, with its all-weather capability, is essential in remote sensing, yet
the lack of large-scale, high-quality SAR image-text datasets hinders its
semantic understanding. In this paper, we construct SAR-Text, a large-scale and
high-quality dataset consisting of over 130,000 SAR image-text pairs. To
construct the SAR-Text dataset, we design the SAR-Narrator framework, which
generates textual descriptions for SAR images through a multi-stage progressive
transfer learning strategy. To verify the effectiveness of the SAR-TEXT
dataset, we conduct experiments on three typical vision-language tasks:
image-text retrieval, image captioning, and visual question answering (VQA).
Specifically, we construct three representative models on SAR-TEXT:
SAR-RS-CLIP, SAR-RS-CoCa, and SAR-GPT. SAR-RS-CLIP achieves notable
improvements in retrieval performance, boosting average recall by 16.43% and
10.54% on the OSdataset-512 and HRSID test sets, respectively. In the
captioning task, SAR-RS-CoCa achieves BLEU-4, SPICE, and CIDEr scores exceeding
those of the original CoCa model by more than 8x, 4x, and 10x, respectively. In
the VQA task, SAR-GPT outperforms baseline and single-stage models on multiple
SAR-VQA datasets, demonstrating stronger semantic understanding and reasoning
ability, as further confirmed by qualitative results. It is worth noting that,
as a flexible captioning tool, SAR-Narrator can be readily adopted by the
community to construct larger-scale SAR image-text datasets.

</details>


### [23] [Efficient Lines Detection for Robot Soccer](https://arxiv.org/abs/2507.19469)
*João G. Melo,João P. Mafaldo,Edna Barros*

Main category: cs.CV

TL;DR: 本文提出了一种基于ELSED算法并结合RGB颜色变化分类的轻量级、高效的足球场线检测方法。使用粒子群优化（PSO）进行检测阈值自动校准，只需少量标注样本即可实现。其检测精度与深度学习模型相当，但速度更快，适合低功耗平台的实时应用。


<details>
  <summary>Details</summary>
Motivation: 现有机器人足球中的自定位任务，依赖于场地视觉特征检测，尤其是场地产线与边界的精准识别。深度学习等方法往往计算量大，不适合计算资源有限的机器人平台，因此需开发高效的检测方法。

Method: 1. 用ELSED算法进行场线初步提取；2. 基于RGB颜色变化做场线归类，区分真实场地线条；3. 引入粒子群优化（PSO），用少量标注数据校准分类阈值以提升检测效果。

Result: 本方法在准确率上达到与先进深度学习模型相当的水平，但推理速度显著更高。实验表明，其高效性使其非常适合低功耗机器人平台的实时应用需求。

Conclusion: 提出的方法能在资源受限平台上提供可靠、快速的场线检测结果，是现有复杂深度模型的高效替代方案，对于机器人足球自定位系统具有重要应用价值。

Abstract: Self-localization is essential in robot soccer, where accurate detection of
visual field features, such as lines and boundaries, is critical for reliable
pose estimation. This paper presents a lightweight and efficient method for
detecting soccer field lines using the ELSED algorithm, extended with a
classification step that analyzes RGB color transitions to identify lines
belonging to the field. We introduce a pipeline based on Particle Swarm
Optimization (PSO) for threshold calibration to optimize detection performance,
requiring only a small number of annotated samples. Our approach achieves
accuracy comparable to a state-of-the-art deep learning model while offering
higher processing speed, making it well-suited for real-time applications on
low-power robotic platforms.

</details>


### [24] [Learning Efficient and Generalizable Human Representation with Human Gaussian Model](https://arxiv.org/abs/2507.18758)
*Yifan Liu,Shengjun Zhang,Chensheng Dai,Yang Chen,Hao Liu,Chen Li,Yueqi Duan*

Main category: cs.CV

TL;DR: 本文提出了Human Gaussian Graph方法，通过将3D高斯模型节点与SMPL人体网格顶点关联，整合多个时间帧的信息，更好地建模可动画的人体avatar。


<details>
  <summary>Details</summary>
Motivation: 现有方法独立预测每一帧的高斯参数，难以捕捉不同时刻之间的跨帧联系，导致人体avatar动画连贯性和泛化能力不足。为此，作者希望借助所有帧的信息构建更完整、可动画的人体表示。

Method: 提出Human Gaussian Graph模型，将每一帧预测的高斯节点与SMPL网格顶点关联，构成拥有双层结构的图。设计intra-node操作聚合与同一mesh顶点连接的多帧高斯信息，并通过inter-node操作实现网格顶点间的消息传递，综合利用全帧信息生成三维avatar。

Result: 在新视角合成和新姿态动画等任务上，实验结果表明该方法兼具高效性和良好的泛化能力，超过了现有方法的表现。

Conclusion: 通过引入Human Gaussian Graph结构和帧间信息整合策略，该方法显著提升了3D可动画人体模型的建模效果，证实了其在实际应用中的有效性和潜力。

Abstract: Modeling animatable human avatars from videos is a long-standing and
challenging problem. While conventional methods require per-instance
optimization, recent feed-forward methods have been proposed to generate 3D
Gaussians with a learnable network. However, these methods predict Gaussians
for each frame independently, without fully capturing the relations of
Gaussians from different timestamps. To address this, we propose Human Gaussian
Graph to model the connection between predicted Gaussians and human SMPL mesh,
so that we can leverage information from all frames to recover an animatable
human representation. Specifically, the Human Gaussian Graph contains dual
layers where Gaussians are the first layer nodes and mesh vertices serve as the
second layer nodes. Based on this structure, we further propose the intra-node
operation to aggregate various Gaussians connected to one mesh vertex, and
inter-node operation to support message passing among mesh node neighbors.
Experimental results on novel view synthesis and novel pose animation
demonstrate the efficiency and generalization of our method.

</details>


### [25] [Tell Me What You See: An Iterative Deep Learning Framework for Image Captioning](https://arxiv.org/abs/2507.18788)
*Hitesh Kumar Gupta*

Main category: cs.CV

TL;DR: 本文系统性地迭代开发了图像描述模型，从简单的CNN-LSTM到基于注意力机制的先进系统Nexus，分析了结构改进对性能的影响，并验证了注意力机制的重要性。最终模型在MS COCO数据集上超过现有基准。


<details>
  <summary>Details</summary>
Motivation: 尽管目前主流方法多为大型Transformer架构，但对基础图像描述模型的系统性改进和影响分析较少。作者希望揭示在经典模型中结构优化（如视觉骨干和注意力机制）的实际效果及其必要性。

Method: 作者开发了五个模型，由最初的Genesis逐步迭代至采用EfficientNetV2B3和动态注意力机制的Nexus，比较不同结构改进（如视觉骨干升级和引入注意力机制）对性能的影响，在MS COCO 2017数据集上训练测试。

Result: 仅提升视觉骨干而不引入注意力机制会导致性能下降；而搭配注意力机制可有效提升描述质量。最终的Nexus模型BLEU-4得分达到31.4，超过多个基准。

Conclusion: 升级视觉骨干必须结合注意力机制才能提升性能，实验过程为理解现代视觉-语言任务的核心架构原则提供了可复现的蓝图。

Abstract: Image captioning, a task at the confluence of computer vision and natural
language processing, requires a sophisticated understanding of both visual
scenes and linguistic structure. While modern approaches are dominated by
large-scale Transformer architectures, this paper documents a systematic,
iterative development of foundational image captioning models, progressing from
a simple CNN-LSTM encoder-decoder to a competitive attention-based system. We
present a series of five models, beginning with Genesis and concluding with
Nexus, an advanced model featuring an EfficientNetV2B3 backbone and a dynamic
attention mechanism. Our experiments chart the impact of architectural
enhancements and demonstrate a key finding within the classic CNN-LSTM
paradigm: merely upgrading the visual backbone without a corresponding
attention mechanism can degrade performance, as the single-vector bottleneck
cannot transmit the richer visual detail. This insight validates the
architectural shift to attention. Trained on the MS COCO 2017 dataset, our
final model, Nexus, achieves a BLEU-4 score of 31.4, surpassing several
foundational benchmarks and validating our iterative design process. This work
provides a clear, replicable blueprint for understanding the core architectural
principles that underpin modern vision-language tasks.

</details>


### [26] [Deepfake Detection Via Facial Feature Extraction and Modeling](https://arxiv.org/abs/2507.18815)
*Benjamin Carter,Nathan Dilla,Micheal Callahan,Atuhaire Ambala*

Main category: cs.CV

TL;DR: 本论文提出了一种通过仅利用人脸特征点（facial landmarks）而非原始图像数据进行deepfake检测的新方法。实验证明，该方法搭配多种神经网络模型取得了良好效果，并能减少模型参数。


<details>
  <summary>Details</summary>
Motivation: 随着deepfake技术的发展，AI生成的虚假视频和图片越来越逼真，传统基于原始图像处理的检测方法在区分真伪上变得困难。因此需要更高效、鲁棒的检测模型。

Method: 作者提出利用视频人脸的特征点提取（facial landmarks）作为检测深度伪造的特征，通过分析面部细节运动的微妙异常，而不是直接处理图像像素；将提取到的特征输入到三种神经网络（RNN、ANN、CNN）中进行deepfake检测。

Result: 基于面部特征点的特征提取方法，在RNN和ANN模型中精度分别达到了96%和93%，在CNN中为78%，整体表现优异。同时该方法参数更少，运算量低于传统图像方式。

Conclusion: 通过人脸特征点的提取与分析，可以有效识别deepfake视频，这种方法兼容多种神经网络结构，有望成为实际场景中的有效检测工具，也证明了不依赖于直接的图像处理即可达到高效检测的可能性。

Abstract: The rise of deepfake technology brings forth new questions about the
authenticity of various forms of media found online today. Videos and images
generated by artificial intelligence (AI) have become increasingly more
difficult to differentiate from genuine media, resulting in the need for new
models to detect artificially-generated media. While many models have attempted
to solve this, most focus on direct image processing, adapting a convolutional
neural network (CNN) or a recurrent neural network (RNN) that directly
interacts with the video image data. This paper introduces an approach of using
solely facial landmarks for deepfake detection. Using a dataset consisting of
both deepfake and genuine videos of human faces, this paper describes an
approach for extracting facial landmarks for deepfake detection, focusing on
identifying subtle inconsistencies in facial movements instead of raw image
processing. Experimental results demonstrated that this feature extraction
technique is effective in various neural network models, with the same facial
landmarks tested on three neural network models, with promising performance
metrics indicating its potential for real-world applications. The findings
discussed in this paper include RNN and artificial neural network (ANN) models
with accuracy between 96% and 93%, respectively, with a CNN model hovering
around 78%. This research challenges the assumption that raw image processing
is necessary to identify deepfake videos by presenting a facial feature
extraction approach compatible with various neural network models while
requiring fewer parameters.

</details>


### [27] [RealDeal: Enhancing Realism and Details in Brain Image Generation via Image-to-Image Diffusion Models](https://arxiv.org/abs/2507.18830)
*Shen Zhu,Yinzhu Jin,Tyler Spears,Ifrah Zawar,P. Thomas Fletcher*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的图像到图像方法，提升生成脑部图像的真实感和细节。该方法能还原真实医学图像中常见的锐利边缘、细致纹理、解剖细节和成像噪声。提出了新指标用于衡量细节、噪声等方面的真实性。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型（如潜变量扩散模型）生成的医学影像（如MRI）通常过于平滑，缺乏真实影像具备的噪声和细节，这限制了其实际应用价值，因此需要提升生成影像的真实度和细节还原能力。

Method: 将提升真实性和细节还原任务建模为图像到图像的扩散过程，对LDM生成的MRI影像进行质量增强。使用FID、LPIPS等常用指标评估真实性，并提出新的指标用于度量噪声分布、锐度和纹理。

Result: 实验表明，所提出的方法显著提升了LDM生成影像的细节层次和真实性，在常用评测指标和新引入的噪声、锐度、纹理指标上均优于以往方法。

Conclusion: 该方法能够有效生成更符合实际医学影像特征的脑部图像，在细节、噪声和真实感等方面均有提升，为医学图像生成领域提供了新的思路和工具。

Abstract: We propose image-to-image diffusion models that are designed to enhance the
realism and details of generated brain images by introducing sharp edges, fine
textures, subtle anatomical features, and imaging noise. Generative models have
been widely adopted in the biomedical domain, especially in image generation
applications. Latent diffusion models achieve state-of-the-art results in
generating brain MRIs. However, due to latent compression, generated images
from these models are overly smooth, lacking fine anatomical structures and
scan acquisition noise that are typically seen in real images. This work
formulates the realism enhancing and detail adding process as image-to-image
diffusion models, which refines the quality of LDM-generated images. We employ
commonly used metrics like FID and LPIPS for image realism assessment.
Furthermore, we introduce new metrics to demonstrate the realism of images
generated by RealDeal in terms of image noise distribution, sharpness, and
texture.

</details>


### [28] [Flow Stochastic Segmentation Networks](https://arxiv.org/abs/2507.18838)
*Fabio De Sousa Ribeiro,Omar Todd,Charles Jones,Avinash Kori,Raghav Mehta,Ben Glocker*

Main category: cs.CV

TL;DR: 提出了Flow-SSN（一种生成式分割模型），解决了此前低秩参数化限制，并显著提升了分割性能与采样效率，在医学影像基准测试中取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 以往的生成式分割模型采用低秩参数化，导致无法有效捕捉高秩像素协方差信息，影响了分割性能和概率建模能力。

Method: 提出包含离散时间自回归和连续时间流两种变体的Flow-SSN模型，证明其可无约束地估计高秩像素协方差，无需预设秩或保存分布参数，将主要模型容量分配到base distribution（做为流的先验）以增强表达力。

Result: Flow-SSN在医学影像分割中，较现有扩散模型采样效率更高，并在多个具有挑战性的医学影像分割基准上取得了最新最优成绩。

Conclusion: Flow-SSN突破了生成式分割模型的参数化瓶颈，在理论和实践层面都展示了优越性，为分割任务特别是在医学影像领域提供了强有力的新方法。

Abstract: We introduce the Flow Stochastic Segmentation Network (Flow-SSN), a
generative segmentation model family featuring discrete-time autoregressive and
modern continuous-time flow variants. We prove fundamental limitations of the
low-rank parameterisation of previous methods and show that Flow-SSNs can
estimate arbitrarily high-rank pixel-wise covariances without assuming the rank
or storing the distributional parameters. Flow-SSNs are also more efficient to
sample from than standard diffusion-based segmentation models, thanks to most
of the model capacity being allocated to learning the base distribution of the
flow, constituting an expressive prior. We apply Flow-SSNs to challenging
medical imaging benchmarks and achieve state-of-the-art results. Code
available: https://github.com/biomedia-mira/flow-ssn.

</details>


### [29] [PTCMIL: Multiple Instance Learning via Prompt Token Clustering for Whole Slide Image Analysis](https://arxiv.org/abs/2507.18848)
*Beidi Zhao,SangMook Kim,Hao Chen,Chen Zhou,Zu-hua Gao,Gang Wang,Xiaoxiao Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为PTCMIL的新方法，通过在ViT骨干中引入可学习的提示(token)并结合聚类机制，提高了多示例学习(MIL)在全扫描图像(WSI)分析中的表现。该方法在八个数据集上的分类与生存分析任务均超过了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 通常的MIL方法难以有效整合不同补丁的信息，且面对WSI的复杂性与异质性时表达能力不足，当前主流的ViT或基于聚类的方法又计算开销过大，难以因任务与切片内容动态调整。这驱动作者提出结构更高效、具备更强泛化性的端到端聚合方案。

Method: PTCMIL在ViT架构中引入可学习的prompt tokens，将聚类与下游任务预测统一于端到端训练流程。方法通过基于投影的动态聚类实现对每张WSI个性化处理，并用token合并和原型池化提取任务相关特征，保留补丁异质性，提升表达能力又降低计算复杂度。

Result: 在八个公开数据集上，PTCMIL在分类和生存分析任务取得了领先的性能。系统性的消融实验进一步验证了方法的鲁棒性和较高的可解释性。

Conclusion: PTCMIL有效缓解了现有MIL方法计算复杂和泛化能力弱的问题，在WSI分析中表现出色，并具有实际可用的代码实现，将为数字病理领域带来更优解决方案。

Abstract: Multiple Instance Learning (MIL) has advanced WSI analysis but struggles with
the complexity and heterogeneity of WSIs. Existing MIL methods face challenges
in aggregating diverse patch information into robust WSI representations. While
ViTs and clustering-based approaches show promise, they are computationally
intensive and fail to capture task-specific and slide-specific variability. To
address these limitations, we propose PTCMIL, a novel Prompt Token
Clustering-based ViT for MIL aggregation. By introducing learnable prompt
tokens into the ViT backbone, PTCMIL unifies clustering and prediction tasks in
an end-to-end manner. It dynamically aligns clustering with downstream tasks,
using projection-based clustering tailored to each WSI, reducing complexity
while preserving patch heterogeneity. Through token merging and prototype-based
pooling, PTCMIL efficiently captures task-relevant patterns. Extensive
experiments on eight datasets demonstrate its superior performance in
classification and survival analysis tasks, outperforming state-of-the-art
methods. Systematic ablation studies confirm its robustness and strong
interpretability. The code is released at https://github.com/ubc-tea/PTCMIL.

</details>


### [30] [Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and Language Model Reconstruction](https://arxiv.org/abs/2507.18863)
*Matthew Kit Khinn Teng,Haibo Zhang,Takeshi Saitoh*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的基于音素的视觉自动语音识别（V-ASR）两阶段框架，有效融合视觉和面部标志点运动特征，并利用大语言模型将音素恢复成单词，显著提升了唇读准确率。


<details>
  <summary>Details</summary>
Motivation: 现有V-ASR方法因音素在视觉上的歧义（即多个音素在口型上表现相似）和缺乏声音信息而表现不佳，直接从视觉特征预测词或字符，通常错误率高、需要大量数据预训练，因此亟需创新方法提升识别准确性并降低训练难度。

Method: 作者提出两阶段方法：第一阶段，用视觉和面部标志点特征预测音素，降低学习复杂度并兼顾个体差异；第二阶段利用编码器-解码器大语言模型，将音素序列重建成单词。同时，方法在大规模视觉数据集上进行微调增强性能。

Result: 所提PV-ASR方法在LRS2数据集上实现17.4%的词错误率（WER），在LRS3上为21.0%，优于现有主流V-ASR方法。

Conclusion: 音素分离、融合视觉与面部特征以及借助LLM做词恢复的两阶段方法，有效提升了V-ASR的识别准确率和泛化能力，为视觉唇读领域提供了创新解决路径。

Abstract: Visual Automatic Speech Recognition (V-ASR) is a challenging task that
involves interpreting spoken language solely from visual information, such as
lip movements and facial expressions. This task is notably challenging due to
the absence of auditory cues and the visual ambiguity of phonemes that exhibit
similar visemes-distinct sounds that appear identical in lip motions. Existing
methods often aim to predict words or characters directly from visual cues, but
they commonly suffer from high error rates due to viseme ambiguity and require
large amounts of pre-training data. We propose a novel phoneme-based two-stage
framework that fuses visual and landmark motion features, followed by an LLM
model for word reconstruction to address these challenges. Stage 1 consists of
V-ASR, which outputs the predicted phonemes, thereby reducing training
complexity. Meanwhile, the facial landmark features address speaker-specific
facial characteristics. Stage 2 comprises an encoder-decoder LLM model, NLLB,
that reconstructs the output phonemes back to words. Besides using a large
visual dataset for deep learning fine-tuning, our PV-ASR method demonstrates
superior performance by achieving 17.4% WER on the LRS2 and 21.0% WER on the
LRS3 dataset.

</details>


### [31] [Transferable and Undefendable Point Cloud Attacks via Medial Axis Transform](https://arxiv.org/abs/2507.18870)
*Keke Tang,Yuze Gao,Weilong Peng,Xiaofei Wang,Meie Fang,Peican Zhu*

Main category: cs.CV

TL;DR: 该论文提出了一种针对三维点云数据的对抗攻击新框架MAT-Adv，通过扰动点云的中轴变换（MAT）表示来增强对抗样本的迁移性和难以防御性，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有点云对抗攻击方法在迁移性和对抗鲁棒性上表现有限，通常依赖理想的白盒环境，难以应对多样化防御，因此需要新方法提升攻击效果。

Method: 提出MAT-Adv框架，利用自编码器将点云映射到更紧凑的中轴变换（MAT）表示，再对该表示进行扰动，以注入结构层面的对抗特性，同时引入dropout防止过拟合和扰动塌陷。

Result: 大量实验表明，MAT-Adv在迁移性（可攻击未见模型能力）和难以防御性（现有防御手段难以抵挡）均显著优于现有最先进方法。

Conclusion: 通过在结构层面生成对抗样本，MAT-Adv极大提升了三维深度学习模型对点云攻击的挑战性，有效推动了模型鲁棒性研究的发展。

Abstract: Studying adversarial attacks on point clouds is essential for evaluating and
improving the robustness of 3D deep learning models. However, most existing
attack methods are developed under ideal white-box settings and often suffer
from limited transferability to unseen models and insufficient robustness
against common defense mechanisms. In this paper, we propose MAT-Adv, a novel
adversarial attack framework that enhances both transferability and
undefendability by explicitly perturbing the medial axis transform (MAT)
representations, in order to induce inherent adversarialness in the resulting
point clouds. Specifically, we employ an autoencoder to project input point
clouds into compact MAT representations that capture the intrinsic geometric
structure of point clouds. By perturbing these intrinsic representations,
MAT-Adv introduces structural-level adversarial characteristics that remain
effective across diverse models and defense strategies. To mitigate overfitting
and prevent perturbation collapse, we incorporate a dropout strategy into the
optimization of MAT perturbations, further improving transferability and
undefendability. Extensive experiments demonstrate that MAT-Adv significantly
outperforms existing state-of-the-art methods in both transferability and
undefendability. Codes will be made public upon paper acceptance.

</details>


### [32] [Dealing with Segmentation Errors in Needle Reconstruction for MRI-Guided Brachytherapy](https://arxiv.org/abs/2507.18895)
*Vangelis Kostoulas,Arthur Guijt,Ellen M. Kerkhof,Bradley R. Pieters,Peter A. N. Bosman,Tanja Alderliesten*

Main category: cs.CV

TL;DR: 本文提出了一种针对图像引导放射性粒子植入术中自动针体重建问题的新颖后处理方法，有效应对分割阶段的错误，提升针体定位的准确率。


<details>
  <summary>Details</summary>
Motivation: 自动化图像引导植入针体重建能大大减少医生的工作量，但当前分割模型及其后处理方法常因分割误差导致定位不准确，且无单一方法能应对所有类型的分割错误，因此亟需更鲁棒的后处理技术。

Method: 在传统分割-后处理两阶段自动针体重建流程基础上，对现有后处理技术进行改进，重点针对分割阶段产生的各类误差进行适配，提升整体针体三维重建精度。

Result: 在前列腺癌MRI数据集上实验，改进后的后处理方法显著降低了针尖、针底和针体主轴的定位误差，分别达到了1.07mm、0.43mm和0.75mm的中位定位误差，测试261根针均无假阳性和假阴性。

Conclusion: 所提出的后处理适配方法能有效应对深度学习分割产生的误差，极大提升了整体针体重建的准确性和鲁棒性，有助于临床自动化流程推广。

Abstract: Brachytherapy involves bringing a radioactive source near tumor tissue using
implanted needles. Image-guided brachytherapy planning requires amongst others,
the reconstruction of the needles. Manually annotating these needles on patient
images can be a challenging and time-consuming task for medical professionals.
For automatic needle reconstruction, a two-stage pipeline is commonly adopted,
comprising a segmentation stage followed by a post-processing stage. While deep
learning models are effective for segmentation, their results often contain
errors. No currently existing post-processing technique is robust to all
possible segmentation errors. We therefore propose adaptations to existing
post-processing techniques mainly aimed at dealing with segmentation errors and
thereby improving the reconstruction accuracy. Experiments on a prostate cancer
dataset, based on MRI scans annotated by medical professionals, demonstrate
that our proposed adaptations can help to effectively manage segmentation
errors, with the best adapted post-processing technique achieving median
needle-tip and needle-bottom point localization errors of $1.07$ (IQR $\pm
1.04$) mm and $0.43$ (IQR $\pm 0.46$) mm, respectively, and median shaft error
of $0.75$ (IQR $\pm 0.69$) mm with 0 false positive and 0 false negative
needles on a test set of 261 needles.

</details>


### [33] [Synthetic-to-Real Camouflaged Object Detection](https://arxiv.org/abs/2507.18911)
*Zhihao Luo,Luojun Lin,Zheng Lin*

Main category: cs.CV

TL;DR: 提出了一种以循环学生-教师模型为基础的域自适应方法，用于提升从合成数据到真实数据的隐蔽目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 隐蔽目标检测的数据集收集和标注成本高，尤其在特定细分领域相关真实数据不足，限制了模型性能提升和泛化。合成数据虽可缓解这一问题，但直接用其训练会显著降低在真实数据上的表现。

Method: 定义了合成到真实隐蔽目标检测（S2R-COD）新任务，提出CSRDA（Cycling Syn-to-Real Domain Adaptation）架构，采用学生-教师模型，通过伪标签和一致性正则传播标注信息，并用递归学习框架动态构建“进化”的真实域以缩小分布差异，提高伪标签质量及迁移能力。

Result: 实验结果表明，CSRDA框架能在真实场景中显著提升隐蔽目标检测表现，有效缓解数据和标注不足的难题。

Conclusion: 本文提出的CSRDA框架通过新颖的伪标签与一致性正则策略，配合递归域适应，有效促进了合成到真实域的知识迁移，为解决隐蔽目标检测领域的小样本和弱标注问题提供了新思路。

Abstract: Due to the high cost of collection and labeling, there are relatively few
datasets for camouflaged object detection (COD). In particular, for certain
specialized categories, the available image dataset is insufficiently
populated. Synthetic datasets can be utilized to alleviate the problem of
limited data to some extent. However, directly training with synthetic datasets
compared to real datasets can lead to a degradation in model performance. To
tackle this problem, in this work, we investigate a new task, namely
Syn-to-Real Camouflaged Object Detection (S2R-COD). In order to improve the
model performance in real world scenarios, a set of annotated synthetic
camouflaged images and a limited number of unannotated real images must be
utilized. We propose the Cycling Syn-to-Real Domain Adaptation Framework
(CSRDA), a method based on the student-teacher model. Specially, CSRDA
propagates class information from the labeled source domain to the unlabeled
target domain through pseudo labeling combined with consistency regularization.
Considering that narrowing the intra-domain gap can improve the quality of
pseudo labeling, CSRDA utilizes a recurrent learning framework to build an
evolving real domain for bridging the source and target domain. Extensive
experiments demonstrate the effectiveness of our framework, mitigating the
problem of limited data and handcraft annotations in COD. Our code is publicly
available at: https://github.com/Muscape/S2R-COD

</details>


### [34] [HQ-SMem: Video Segmentation and Tracking Using Memory Efficient Object Embedding With Selective Update and Self-Supervised Distillation Feedback](https://arxiv.org/abs/2507.18921)
*Elham Soltani Kazemi,Imad Eddine Toubal,Gani Rahmon,Jaired Collins,K. Palaniappan*

Main category: cs.CV

TL;DR: 该论文提出了一种新的高质量视频目标分割与跟踪方法HQ-SMem，通过智能内存机制，有效提升了分割精度、跟踪稳定性及长视频处理效率，在多个公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频目标分割（VOS）技术面对分割掩膜精度不足、形变目标、跟踪漂移和长视频处理等挑战，局限性明显。为满足在安防、自动驾驶、视频编辑等应用中的高质量分割需求，亟需突破这些瓶颈。

Method: 本文提出HQ-SMem方法，包括三个创新点：（1）结合高质量掩膜的SAM（SAM-HQ）与候选选择机制，优化分割边界；（2）引入动态智能内存，仅存储关键帧，提升长视频下的效率与存储利用率；（3）在线更新外观模型，以应对目标复杂形变与跟踪漂移。

Result: 在VOTS和VOTSt 2024等多个主流公开数据集上，HQ-SMem均进入表现前两名。在Long Video Dataset和LVOS等长视频复杂多目标场景下，HQ-SMem也刷新了现有基准，实现了更高的分割和跟踪表现。

Conclusion: HQ-SMem有效解决了现有VOS模型的多个局限，在高质量掩膜分割、动态记忆管理及跟踪抗漂移等方面实现突破，适用于多对象动态复杂、时长较长的现实视频场景。

Abstract: Video Object Segmentation (VOS) is foundational to numerous computer vision
applications, including surveillance, autonomous driving, robotics and
generative video editing. However, existing VOS models often struggle with
precise mask delineation, deformable objects, topologically transforming
objects, tracking drift and long video sequences. In this paper, we introduce
HQ-SMem, for High Quality video segmentation and tracking using Smart Memory, a
novel method that enhances the performance of VOS base models by addressing
these limitations. Our approach incorporates three key innovations: (i)
leveraging SAM with High-Quality masks (SAM-HQ) alongside appearance-based
candidate-selection to refine coarse segmentation masks, resulting in improved
object boundaries; (ii) implementing a dynamic smart memory mechanism that
selectively stores relevant key frames while discarding redundant ones, thereby
optimizing memory usage and processing efficiency for long-term videos; and
(iii) dynamically updating the appearance model to effectively handle complex
topological object variations and reduce drift throughout the video. These
contributions mitigate several limitations of existing VOS models including,
coarse segmentations that mix-in background pixels, fixed memory update
schedules, brittleness to drift and occlusions, and prompt ambiguity issues
associated with SAM. Extensive experiments conducted on multiple public
datasets and state-of-the-art base trackers demonstrate that our method
consistently ranks among the top two on VOTS and VOTSt 2024 datasets. Moreover,
HQ-SMem sets new benchmarks on Long Video Dataset and LVOS, showcasing its
effectiveness in challenging scenarios characterized by complex multi-object
dynamics over extended temporal durations.

</details>


### [35] [Gaussian Set Surface Reconstruction through Per-Gaussian Optimization](https://arxiv.org/abs/2507.18923)
*Zhentao Huang,Di Wu,Zhenbang He,Minglun Gong*

Main category: cs.CV

TL;DR: 本文提出了一种新方法GSSR，有效提升了3D Gaussian Splatting在场景几何重建中的表现，能实现更加准确和均匀的高斯分布，对于后续的场景编辑和新视角生成均有显著益处。


<details>
  <summary>Details</summary>
Motivation: 尽管3D Gaussian Splatting能灵活生成新视角，但其几何重建不准确，现有改进方法如PGSR依然未能优化高斯点的位置分布，造成高斯点偏离真实表面、分布不均，影响重建精度和场景编辑。

Method: 受到点集曲面(Point Set Surfaces)的启发，作者提出GSSR方法，使高斯点沿隐式表面均匀分布，并使其法线与表面法线对齐。具体通过像素级和高斯级的单视图法线一致性、多视图光度一致性，以及不透明度正则化和定期的高斯重初始化，优化高斯的局部和整体几何表现。

Result: GSSR能大幅提升高斯点在三维空间中的定位精度，并保持高质量的渲染表现，实验显示其重建精度优于现有方法。

Conclusion: GSSR方法显著改善了3D高斯法在几何恢复与分布均匀性，便于场景编辑和新环境生成，是提升高斯基3D重建的新进展。

Abstract: 3D Gaussian Splatting (3DGS) effectively synthesizes novel views through its
flexible representation, yet fails to accurately reconstruct scene geometry.
While modern variants like PGSR introduce additional losses to ensure proper
depth and normal maps through Gaussian fusion, they still neglect individual
placement optimization. This results in unevenly distributed Gaussians that
deviate from the latent surface, complicating both reconstruction refinement
and scene editing. Motivated by pioneering work on Point Set Surfaces, we
propose Gaussian Set Surface Reconstruction (GSSR), a method designed to
distribute Gaussians evenly along the latent surface while aligning their
dominant normals with the surface normal. GSSR enforces fine-grained geometric
alignment through a combination of pixel-level and Gaussian-level single-view
normal consistency and multi-view photometric consistency, optimizing both
local and global perspectives. To further refine the representation, we
introduce an opacity regularization loss to eliminate redundant Gaussians and
apply periodic depth- and normal-guided Gaussian reinitialization for a
cleaner, more uniform spatial distribution. Our reconstruction results
demonstrate significantly improved geometric precision in Gaussian placement,
enabling intuitive scene editing and efficient generation of novel
Gaussian-based 3D environments. Extensive experiments validate GSSR's
effectiveness, showing enhanced geometric accuracy while preserving
high-quality rendering performance.

</details>


### [36] [WiSE-OD: Benchmarking Robustness in Infrared Object Detection](https://arxiv.org/abs/2507.18925)
*Heitor R. Medeiros,Atif Belal,Masih Aminbeidokhti,Eric Granger,Marco Pedersoli*

Main category: cs.CV

TL;DR: 本文针对红外图像目标检测中因数据集稀缺导致的跨模态鲁棒性不足问题，提出了新的OOD基准与权重空间集成方法，有效提升了模型的跨模态与扰动鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 红外图像目标检测对于夜间和低光应用至关重要，但大规模IR数据集稀缺，导致大多数模型需依赖RGB预训练权重，迁移至IR时因模态差异而鲁棒性变差。现有方法难以应对分布漂移和扰动场景，亟需更有效的跨模态鲁棒性提升方法。

Method: 1. 构建了LLVIP-C与FLIR-C两个针对红外的跨模态OOD基准数据集，提供分布外评估标准。
2. 提出WiSE-OD权重空间集成方法，包括WiSE-OD$_{ZS}$（融合RGB零样本与IR微调权重）和WiSE-OD$_{LP}$（融合零样本与线性探测权重），可无附加训练或推理代价集成多个模态。

Result: 在三种RGB预训练检测器与两个鲁棒性基线下，WiSE-OD在跨模态和扰动鲁棒性评测中均有明显提升。
无需额外训练/推理开销即可提升模型性能，验证了方法有效性。

Conclusion: 新提出的OOD基准填补了跨模态IR测试空白，所提WiSE-OD方法为提升视觉检测模型的跨模态与鲁棒性能力提供了简单有效的途径，对实际低光与复杂场景下的目标检测有重要意义。

Abstract: Object detection (OD) in infrared (IR) imagery is critical for low-light and
nighttime applications. However, the scarcity of large-scale IR datasets forces
models to rely on weights pre-trained on RGB images. While fine-tuning on IR
improves accuracy, it often compromises robustness under distribution shifts
due to the inherent modality gap between RGB and IR. To address this, we
introduce LLVIP-C and FLIR-C, two cross-modality out-of-distribution (OOD)
benchmarks built by applying corruption to standard IR datasets. Additionally,
to fully leverage the complementary knowledge from RGB and infrared trained
models, we propose WiSE-OD, a weight-space ensembling method with two variants:
WiSE-OD$_{ZS}$, which combines RGB zero-shot and IR fine-tuned weights, and
WiSE-OD$_{LP}$, which blends zero-shot and linear probing. Evaluated across
three RGB-pretrained detectors and two robust baselines, WiSE-OD improves both
cross-modality and corruption robustness without any additional training or
inference cost.

</details>


### [37] [MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal Sticker Emotion Recognition](https://arxiv.org/abs/2507.18929)
*Jian Chen,Yuxuan Hu,Haifeng Lu,Wei Wang,Min Yang,Chengming Li,Xiping Hu*

Main category: cs.CV

TL;DR: 提出了一种多粒度分层融合Transformer（MGHFT），用于提升表情贴纸的情感识别能力，通过多模态大语言模型提供多视角文本上下文，并分层融合文字与视觉特征，实现更准确、细粒度的贴纸情感理解。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉-文本预训练模型在视觉特征提取上表现强大，但表情贴纸的情感理解受限于需要多视角（如背景知识、风格线索）信息，现有方法难以捕捉全局和细粒度语义融合，因此亟需新方法提升情感识别准确性和细致度。

Method: 作者提出MGHFT：通过多模态大语言模型生成多视角文本描述，设计分层融合策略，将文本特征在不同阶段注入采用金字塔结构的视觉Transformer，通过对比学习与注意力机制引导全局、局部视觉语义与文本特征深度融合；最后引入文本引导融合注意力机制综合多模态信息提升理解能力。

Result: 在两个公开贴纸情感数据集上，MGHFT显著优于现有方法，情感识别更准确、更细致；相较最佳视觉预训练模型，F1分数提升5.4%，准确率提升4.0%。

Conclusion: MGHFT有力提升了表情贴纸情感识别表现，优势在于实现了多视角文本和多粒度视觉特征的深度融合，同时为多模态情感理解领域提供了有效范式。

Abstract: Although pre-trained visual models with text have demonstrated strong
capabilities in visual feature extraction, sticker emotion understanding
remains challenging due to its reliance on multi-view information, such as
background knowledge and stylistic cues. To address this, we propose a novel
multi-granularity hierarchical fusion transformer (MGHFT), with a multi-view
sticker interpreter based on Multimodal Large Language Models. Specifically,
inspired by the human ability to interpret sticker emotions from multiple
views, we first use Multimodal Large Language Models to interpret stickers by
providing rich textual context via multi-view descriptions. Then, we design a
hierarchical fusion strategy to fuse the textual context into visual
understanding, which builds upon a pyramid visual transformer to extract both
global and local sticker features at multiple stages. Through contrastive
learning and attention mechanisms, textual features are injected at different
stages of the visual backbone, enhancing the fusion of global- and
local-granularity visual semantics with textual guidance. Finally, we introduce
a text-guided fusion attention mechanism to effectively integrate the overall
multimodal features, enhancing semantic understanding. Extensive experiments on
2 public sticker emotion datasets demonstrate that MGHFT significantly
outperforms existing sticker emotion recognition approaches, achieving higher
accuracy and more fine-grained emotion recognition. Compared to the best
pre-trained visual models, our MGHFT also obtains an obvious improvement, 5.4%
on F1 and 4.0% on accuracy. The code is released at
https://github.com/cccccj-03/MGHFT_ACMMM2025.

</details>


### [38] [PDT: Point Distribution Transformation with Diffusion Models](https://arxiv.org/abs/2507.18939)
*Jionghao Wang,Cheng Lin,Yuan Liu,Rui Xu,Zhiyang Dou,Xiao-Xiao Long,Hao-Xiang Guo,Taku Komura,Wenping Wang,Xin Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的新方法PDT，用于将原始点云分布变换为具有结构和语义的目标点分布。该方法在点云数据中实现了几何和语义特征的精确抽取和变换。


<details>
  <summary>Details</summary>
Motivation: 点云虽然广泛用于三维几何表示，但如何从无结构的原始点云中有效提取并转换成有意义的结构化点集仍是一个难题。

Method: 提出一种新的扩散模型架构与学习策略，将输入点集通过去噪过程转换为有语义结构的目标分布，实现几何与语义的有机融合。

Result: 经过大量实验，PDT可以将输入的点云转换为多种结构化输出，包括表面对齐的关键点、内部稀疏骨架点及连续特征线等，证明了其在结构抽取和几何理解上的优越能力。

Conclusion: PDT架构能够同时捕获点云的几何和语义信息，为3D几何处理中需要结构化点集的任务提供了有效工具。

Abstract: Point-based representations have consistently played a vital role in
geometric data structures. Most point cloud learning and processing methods
typically leverage the unordered and unconstrained nature to represent the
underlying geometry of 3D shapes. However, how to extract meaningful structural
information from unstructured point cloud distributions and transform them into
semantically meaningful point distributions remains an under-explored problem.
We present PDT, a novel framework for point distribution transformation with
diffusion models. Given a set of input points, PDT learns to transform the
point set from its original geometric distribution into a target distribution
that is semantically meaningful. Our method utilizes diffusion models with
novel architecture and learning strategy, which effectively correlates the
source and the target distribution through a denoising process. Through
extensive experiments, we show that our method successfully transforms input
point clouds into various forms of structured outputs - ranging from
surface-aligned keypoints, and inner sparse joints to continuous feature lines.
The results showcase our framework's ability to capture both geometric and
semantic features, offering a powerful tool for various 3D geometry processing
tasks where structured point distributions are desired. Code will be available
at this link: https://github.com/shanemankiw/PDT.

</details>


### [39] [Structure Matters: Revisiting Boundary Refinement in Video Object Segmentation](https://arxiv.org/abs/2507.18944)
*Guanyi Qin,Ziyue Wang,Daiyun Shen,Haofeng Liu,Hantao Zhou,Junde Wu,Runze Hu,Yueming Jin*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频目标分割方法OASIS，能够高效且准确地应对遮挡和高特征相似场景，在多个主流数据集上效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有半监督视频目标分割方法在处理目标遮挡、物体交互以及高特征相似时表现不佳，且难以兼顾实时性和精度。

Method: 提出了OASIS方法，包含轻量级结构细化模块，融合Canny算子的边缘先验和存储的目标特征，增强边界表示，生成目标级结构图，同时通过证据学习机制进行不确定性估计，应对遮挡难题。

Result: 在DAVIS-17和YouTubeVOS 2019等挑战性数据集上，OASIS获得了高于现有方法的分数（如DAVIS-17上F值91.6，YouTubeVOS上G值86.6），推理速度达48 FPS。

Conclusion: OASIS在保证高效率的同时，有效提高了分割精度，尤其在复杂场景下表现突出，优于当前主流视频目标分割方法。

Abstract: Given an object mask, Semi-supervised Video Object Segmentation (SVOS)
technique aims to track and segment the object across video frames, serving as
a fundamental task in computer vision. Although recent memory-based methods
demonstrate potential, they often struggle with scenes involving occlusion,
particularly in handling object interactions and high feature similarity. To
address these issues and meet the real-time processing requirements of
downstream applications, in this paper, we propose a novel bOundary Amendment
video object Segmentation method with Inherent Structure refinement, hereby
named OASIS. Specifically, a lightweight structure refinement module is
proposed to enhance segmentation accuracy. With the fusion of rough edge priors
captured by the Canny filter and stored object features, the module can
generate an object-level structure map and refine the representations by
highlighting boundary features. Evidential learning for uncertainty estimation
is introduced to further address challenges in occluded regions. The proposed
method, OASIS, maintains an efficient design, yet extensive experiments on
challenging benchmarks demonstrate its superior performance and competitive
inference speed compared to other state-of-the-art methods, i.e., achieving the
F values of 91.6 (vs. 89.7 on DAVIS-17 validation set) and G values of 86.6
(vs. 86.2 on YouTubeVOS 2019 validation set) while maintaining a competitive
speed of 48 FPS on DAVIS.

</details>


### [40] [PerioDet: Large-Scale Panoramic Radiograph Benchmark for Clinical-Oriented Apical Periodontitis Detection](https://arxiv.org/abs/2507.18958)
*Xiaocheng Fang,Jieyi Cai,Huanyu Liu,Chengju Zhou,Minhua Lu,Bingzhi Chen*

Main category: cs.CV

TL;DR: 本文提出并公开了首个针对牙根尖周炎自动诊断的大规模标注数据集“PerioXrays”，并提出了结合BDA和IDC机制的新检测范式PerioDet，有效提升了自动检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着自动诊断系统在医学领域的应用增加，牙根尖周炎作为常见口腔疾病，缺乏大型高质量标注数据集限制了CAD的开发与应用。

Method: 作者发布了涵盖3,673张全景X光和5,662例牙根尖周炎精标实例的PerioXrays数据集，首次为该病建立标准基准。并设计了PerioDet检测范式，通过引入背景去噪注意力（BDA）和IoU动态校准（IDC）机制，提升了在复杂背景和小目标下的检测能力。

Result: 在PerioXrays数据集上的实验显示PerioDet检测效果优于现有方法。此外，和专业牙医的合作实验表明其作为临床辅助诊断工具具有实际应用价值。

Conclusion: PerioXrays数据集填补了领域空白，PerioDet方法推动了牙根尖周炎自动检测技术发展，并展现了在临床辅助诊断中的应用潜力。

Abstract: Apical periodontitis is a prevalent oral pathology that presents significant
public health challenges. Despite advances in automated diagnostic systems
across various medical fields, the development of Computer-Aided Diagnosis
(CAD) applications for apical periodontitis is still constrained by the lack of
a large-scale, high-quality annotated dataset. To address this issue, we
release a large-scale panoramic radiograph benchmark called "PerioXrays",
comprising 3,673 images and 5,662 meticulously annotated instances of apical
periodontitis. To the best of our knowledge, this is the first benchmark
dataset for automated apical periodontitis diagnosis. This paper further
proposes a clinical-oriented apical periodontitis detection (PerioDet)
paradigm, which jointly incorporates Background-Denoising Attention (BDA) and
IoU-Dynamic Calibration (IDC) mechanisms to address the challenges posed by
background noise and small targets in automated detection. Extensive
experiments on the PerioXrays dataset demonstrate the superiority of PerioDet
in advancing automated apical periodontitis detection. Additionally, a
well-designed human-computer collaborative experiment underscores the clinical
applicability of our method as an auxiliary diagnostic tool for professional
dentists.

</details>


### [41] [Closing the Modality Gap for Mixed Modality Search](https://arxiv.org/abs/2507.19054)
*Binxu Li,Yuhui Zhang,Xiaohan Wang,Weixin Liang,Ludwig Schmidt,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 本文关注跨不同模态（如图片、文本及多模态文档）的混合模态检索任务，发现现有主流视觉-语言模型（如CLIP）存在模态间嵌入空间分离问题，进而提出一种高效纠正方法GR-CLIP，有效提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 随着信息多样性增加，现实世界对在图像、文本等不同模态数据间检索的需求日益增长，而这类混合模态检索仍少有充分研究。CLIP等对比视觉-语言模型虽然强大，但存在模态间嵌入空间分隔严重，导致检索时各模态融合效果差。本文旨在分析和解决这一实际问题。

Method: 作者首先分析CLIP等视-语对比模型在混合模态检索中的表现，发现其嵌入空间存在明显模态间隔。为消除该间隔（模态鸿沟），提出了一种轻量级的后处理校准方法GR-CLIP，无须重新训练模型，通过调整嵌入空间结构来促进不同模态融合。

Result: 在专为混合模态检索设计的MixBench基准上进行评测，GR-CLIP方法相比原始CLIP模型将NDCG@10指标提升了最多26个百分点，相较于最新生成式视觉-语言嵌入模型也提升了4个百分点，并大幅减少了算力消耗（75倍）。

Conclusion: CLIP等对比视觉-语言模型在混合模态检索任务中存在显著模态鸿沟问题。通过GR-CLIP这种高效的校准方法，可大幅提升多模态混合检索性能，并在计算效率上具有优势，推动实际应用落地。

Abstract: Mixed modality search -- retrieving information across a heterogeneous corpus
composed of images, texts, and multimodal documents -- is an important yet
underexplored real-world application. In this work, we investigate how
contrastive vision-language models, such as CLIP, perform on the mixed modality
search task. Our analysis reveals a critical limitation: these models exhibit a
pronounced modality gap in the embedding space, where image and text embeddings
form distinct clusters, leading to intra-modal ranking bias and inter-modal
fusion failure. To address this issue, we propose GR-CLIP, a lightweight
post-hoc calibration method that removes the modality gap in CLIP's embedding
space. Evaluated on MixBench -- the first benchmark specifically designed for
mixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage points
over CLIP, surpasses recent vision-language generative embedding models by 4
percentage points, while using 75x less compute.

</details>


### [42] [YOLO for Knowledge Extraction from Vehicle Images: A Baseline Study](https://arxiv.org/abs/2507.18966)
*Saraa Al-Saddik,Manna Elizabeth Philip,Ali Haidar*

Main category: cs.CV

TL;DR: 本文比较了三种先进的YOLO系列深度学习方法在真实复杂场景下车辆属性识别（品牌、形状、颜色）上的表现，并提出多视角推断以提升识别准确率。结果表明部分属性的准确率超过93%，并发现YOLO目标检测模型优于仅分类模型，小模型效率高且效果好。


<details>
  <summary>Details</summary>
Motivation: 车辆属性（品牌、颜色、形状）识别对于执法和信息情报非常重要，但实际场景采集的数据复杂多变，需要评估现有顶尖模型在这些真实数据上的有效性，并探索如何提升识别表现。

Method: 使用NSW警察高速巡逻车辆采集的大规模真实复杂车辆图像，针对品牌、形状、颜色构建各自10万+样本数据集。对YOLO-v11、YOLO-World、YOLO-Classification模型进行测试；采用多视角推断方法以增强模型性能，并通过不同模型大小进行对比实验。

Result: 最好的模型在品牌识别、形状、颜色、双色颜色任务上的准确率分别为93.70%、82.86%、85.19%、94.86%。YOLO-v11和YOLO-World在品牌、形状识别上超越单纯分类模型。小型YOLO模型与大型模型表现相当，且效率更高。

Conclusion: 基于YOLO的目标检测模型结合多视角推断可在真实复杂场景实现高效高精度的车辆属性识别。结果为实际车辆属性元数据提取系统的建立提供了强有力的基线，并有助于大规模图片的自动筛查与检索。

Abstract: Accurate identification of vehicle attributes such as make, colour, and shape
is critical for law enforcement and intelligence applications. This study
evaluates the effectiveness of three state-of-the-art deep learning approaches
YOLO-v11, YOLO-World, and YOLO-Classification on a real-world vehicle image
dataset. This dataset was collected under challenging and unconstrained
conditions by NSW Police Highway Patrol Vehicles. A multi-view inference (MVI)
approach was deployed to enhance the performance of the models' predictions. To
conduct the analyses, datasets with 100,000 plus images were created for each
of the three metadata prediction tasks, specifically make, shape and colour.
The models were tested on a separate dataset with 29,937 images belonging to
1809 number plates. Different sets of experiments have been investigated by
varying the models sizes. A classification accuracy of 93.70%, 82.86%, 85.19%,
and 94.86% was achieved with the best performing make, shape, colour, and
colour-binary models respectively. It was concluded that there is a need to use
MVI to get usable models within such complex real-world datasets. Our findings
indicated that the object detection models YOLO-v11 and YOLO-World outperformed
classification-only models in make and shape extraction. Moreover, smaller YOLO
variants perform comparably to larger counterparts, offering substantial
efficiency benefits for real-time predictions. This work provides a robust
baseline for extracting vehicle metadata in real-world scenarios. Such models
can be used in filtering and sorting user queries, minimising the time required
to search large vehicle images datasets.

</details>


### [43] [Underwater Waste Detection Using Deep Learning A Performance Comparison of YOLOv7 to 10 and Faster RCNN](https://arxiv.org/abs/2507.18967)
*UMMPK Nawarathne,HMNS Kumari,HMLS Kumari*

Main category: cs.CV

TL;DR: 该论文比较了多种目标识别算法在水下垃圾检测中的表现，发现YOLOv8在识别精度上领先。


<details>
  <summary>Details</summary>
Motivation: 随着水下污染问题日益严重，准确检测水下垃圾对于环境保护至关重要，因此需要评估现有目标检测算法在此场景下的有效性。

Method: 研究团队选用了YOLOv7、YOLOv8、YOLOv9、YOLOv10和Faster R-CNN五种最新目标检测模型，对其在包含15类不同水下垃圾的大型数据集上进行训练和测试，实验环境涵盖低能见度和不同水深等复杂场景。

Result: YOLOv8模型在所有对比模型中表现最佳，取得了80.9%的mAP，显示出其水下目标检测的高精度优势。

Conclusion: YOLOv8因其改进的结构和自监督学习等先进特性，展现出卓越的水下目标识别能力，有助于提升全球水下污染治理和清理行动的效率与可扩展性。

Abstract: Underwater pollution is one of today's most significant environmental
concerns, with vast volumes of garbage found in seas, rivers, and landscapes
around the world. Accurate detection of these waste materials is crucial for
successful waste management, environmental monitoring, and mitigation
strategies. In this study, we investigated the performance of five cutting-edge
object recognition algorithms, namely YOLO (You Only Look Once) models,
including YOLOv7, YOLOv8, YOLOv9, YOLOv10, and Faster Region-Convolutional
Neural Network (R-CNN), to identify which model was most effective at
recognizing materials in underwater situations. The models were thoroughly
trained and tested on a large dataset containing fifteen different classes
under diverse conditions, such as low visibility and variable depths. From the
above-mentioned models, YOLOv8 outperformed the others, with a mean Average
Precision (mAP) of 80.9%, indicating a significant performance. This increased
performance is attributed to YOLOv8's architecture, which incorporates advanced
features such as improved anchor-free mechanisms and self-supervised learning,
allowing for more precise and efficient recognition of items in a variety of
settings. These findings highlight the YOLOv8 model's potential as an effective
tool in the global fight against pollution, improving both the detection
capabilities and scalability of underwater cleanup operations.

</details>


### [44] [LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences](https://arxiv.org/abs/2507.19362)
*Yusuke Hirota,Boyi Li,Ryo Hachiuma,Yueh-Hua Wu,Boris Ivanovic,Yuta Nakashima,Marco Pavone,Yejin Choi,Yu-Chiang Frank Wang,Chao-Han Huck Yang*

Main category: cs.CV

TL;DR: 该论文提出了一个名为LOTUS的排行榜，用于全面评估大型视觉-语言模型（LVLMs）生成的详细图片描述，涵盖质量、风险和偏见等多方面评价。


<details>
  <summary>Details</summary>
Motivation: 现有评估方式存在三大不足：缺乏标准化的评估标准，缺少对偏见的敏感分析，以及未考虑用户偏好。因此，作者希望建立一个全面、能反映用户多元需求的评测体系。

Method: 提出LOTUS评测体系，综合评估图片描述的多维度质量（如对齐度、描述性）、风险（如幻觉生成）、社会偏见（如性别偏见）等，并允许按用户偏好定制评测标准。

Result: 对当前主流LVLMs的评测显示，没有任何单一模型在所有标准下表现最佳，且增加描述细节会提升偏见风险。用户偏好导向的评测发现模型优劣依赖于实际需求。

Conclusion: LOTUS填补了细致图片描述评测的诸多空白，为LVLMs模型选择和优化提供了更为科学和个性化的参考依据。

Abstract: Large Vision-Language Models (LVLMs) have transformed image captioning,
shifting from concise captions to detailed descriptions. We introduce LOTUS, a
leaderboard for evaluating detailed captions, addressing three main gaps in
existing evaluations: lack of standardized criteria, bias-aware assessments,
and user preference considerations. LOTUS comprehensively evaluates various
aspects, including caption quality (e.g., alignment, descriptiveness), risks
(\eg, hallucination), and societal biases (e.g., gender bias) while enabling
preference-oriented evaluations by tailoring criteria to diverse user
preferences. Our analysis of recent LVLMs reveals no single model excels across
all criteria, while correlations emerge between caption detail and bias risks.
Preference-oriented evaluations demonstrate that optimal model selection
depends on user priorities.

</details>


### [45] [AEDR: Training-Free AI-Generated Image Attribution via Autoencoder Double-Reconstruction](https://arxiv.org/abs/2507.18988)
*Chao Wang,Kejiang Chen,Zijin Yang,Yaofei Wang,Weiming Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的、免训练的图像生成溯源方法AEDR，能高效并准确地判断图像是否由特定生成模型产生。


<details>
  <summary>Details</summary>
Motivation: 图像生成技术进步带来安全隐患，需要追踪图像来源以防止恶意使用。然而，现有基于重建的方法在新模型上准确率低且计算成本高。

Method: 提出AEDR方法，利用生成模型中的连续自编码器进行两次重建，计算两次重建损失的比值作为归属信号，并结合图像同质性指标校准，提高准确性和计算效率，无需额外训练。

Result: 在8个主流潜在扩散模型上，AEDR比现有基于重建的方法提升了25.5%的归属准确率，计算时间只需后者的1%。

Conclusion: AEDR在不需训练的前提下显著提高了归属检测的准确率与效率，适用于当前先进的生成模型，具备实际应用潜力。

Abstract: The rapid advancement of image-generation technologies has made it possible
for anyone to create photorealistic images using generative models, raising
significant security concerns. To mitigate malicious use, tracing the origin of
such images is essential. Reconstruction-based attribution methods offer a
promising solution, but they often suffer from reduced accuracy and high
computational costs when applied to state-of-the-art (SOTA) models. To address
these challenges, we propose AEDR (AutoEncoder Double-Reconstruction), a novel
training-free attribution method designed for generative models with continuous
autoencoders. Unlike existing reconstruction-based approaches that rely on the
value of a single reconstruction loss, AEDR performs two consecutive
reconstructions using the model's autoencoder, and adopts the ratio of these
two reconstruction losses as the attribution signal. This signal is further
calibrated using the image homogeneity metric to improve accuracy, which
inherently cancels out absolute biases caused by image complexity, with
autoencoder-based reconstruction ensuring superior computational efficiency.
Experiments on eight top latent diffusion models show that AEDR achieves 25.5%
higher attribution accuracy than existing reconstruction-based methods, while
requiring only 1% of the computational time.

</details>


### [46] [MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents](https://arxiv.org/abs/2507.19478)
*Xuehui Wang,Zhenyu Wu,JingJing Xie,Zichen Ding,Bowen Yang,Zehao Li,Zhaoyang Liu,Qingyun Li,Xuan Dong,Zhe Chen,Weiyun Wang,Xiangyu Zhao,Jixuan Chen,Haodong Duan,Tianbao Xie,Chenyu Yang,Shiqian Su,Yue Yu,Yuan Huang,Yiqian Liu,Xiao Zhang,Yanting Zhang,Xiangyu Yue,Weijie Su,Xizhou Zhu,Wei Shen,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: MMBench-GUI提出了一个分层基准，全面评测不同平台上GUI自动化智能体的核心技能，并引入新颖的效率-质量面积（EQA）指标，揭示当前方法存在的视觉定位与效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评测跨平台GUI自动化智能体综合能力和效率的基准，而这些能力对于智能体的大规模实用化至关重要。以往研究忽视了任务效率等关键维度，亟需新的评价工具和方法。

Method: 作者设计了MMBench-GUI基准，涵盖GUI内容理解、元素定位、任务自动化和任务协作四层次，适用于Windows、macOS、Linux、iOS、Android及Web。提出EQA效率-质量面积指标，实验评测不同方法下的执行表现，并分析了影响任务成功和效率的主要因素。

Result: 通过MMBench-GUI评测发现，准确的视觉定位和具备模块化能力的架构对于任务成功至关重要；现有方法在任务规划、跨平台泛化、长上下文记忆与行动空间、长期推理等方面表现有限，重要的是所有模型在效率上均有明显短板，存在大量冗余步骤。

Conclusion: 要实现高效可扩展的GUI自动化，需精确定位、有效规划并采用适时停止策略。MMBench-GUI为评测和提升GUI自动化智能体能力提供了工具和方向，未来研究应重视效率提升与多能力协同。

Abstract: We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI
automation agents across Windows, macOS, Linux, iOS, Android, and Web
platforms. It comprises four levels: GUI Content Understanding, Element
Grounding, Task Automation, and Task Collaboration, covering essential skills
for GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA)
metric to assess GUI agent execution efficiency in online automation scenarios.
Through MMBench-GUI, we identify accurate visual grounding as a critical
determinant of overall task success, emphasizing the substantial benefits of
modular frameworks that integrate specialized grounding modules. Furthermore,
to achieve reliable GUI automation, an agent requires strong task planning and
cross-platform generalization abilities, with long-context memory, a broad
action space, and long-term reasoning playing a critical role. More important,
task efficiency remains a critically underexplored dimension, and all models
suffer from substantial inefficiencies, with excessive redundant steps even
when tasks are ultimately completed. The integration of precise localization,
effective planning, and early stopping strategies is indispensable to enable
truly efficient and scalable GUI automation. Our benchmark code, evaluation
data, and running environment will be publicly available at
https://github.com/open-compass/MMBench-GUI.

</details>


### [47] [UPP: Unified Point-Level Prompting for Robust Point Cloud Analysis](https://arxiv.org/abs/2507.18997)
*Zixiang Ai,Zhenyu Cui,Yuxin Peng,Jiahuan Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种统一的点级提示机制，能高效提升低质量点云分析模型在有噪声和不完整数据下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实三维点云数据常由于遮挡和传感器局限导致噪声和缺失，现有预训练点云分析模型因质量问题效果大打折扣，而现有提升方法与下游任务割裂、效果有限。

Method: 文中将点云去噪和补全任务统一建模为提示机制。首先用Rectification Prompter预测修正向量来滤除噪声并保留几何特征，然后用Completion Prompter生成辅助点提示提升点云补全，最后用Shape-Aware Unit综合处理后的特征用于下游分析。

Result: 在四个数据集上的大量实验表明，该方法在处理带噪声和不完整点云时，优于现有最先进方法。

Conclusion: 提出的统一点级提示方法能高效提升模型处理低质点云的分析能力，适用于实际复杂场景，相较现有方法更具鲁棒性和优越性能。

Abstract: Pre-trained point cloud analysis models have shown promising advancements in
various downstream tasks, yet their effectiveness is typically suffering from
low-quality point cloud (i.e., noise and incompleteness), which is a common
issue in real scenarios due to casual object occlusions and unsatisfactory data
collected by 3D sensors. To this end, existing methods focus on enhancing point
cloud quality by developing dedicated denoising and completion models. However,
due to the isolation between the point cloud enhancement and downstream tasks,
these methods fail to work in various real-world domains. In addition, the
conflicting objectives between denoising and completing tasks further limit the
ensemble paradigm to preserve critical geometric features. To tackle the above
challenges, we propose a unified point-level prompting method that reformulates
point cloud denoising and completion as a prompting mechanism, enabling robust
analysis in a parameter-efficient manner. We start by introducing a
Rectification Prompter to adapt to noisy points through the predicted
rectification vector prompts, effectively filtering noise while preserving
intricate geometric features essential for accurate analysis. Sequentially, we
further incorporate a Completion Prompter to generate auxiliary point prompts
based on the rectified point clouds, facilitating their robustness and
adaptability. Finally, a Shape-Aware Unit module is exploited to efficiently
unify and capture the filtered geometric features for the downstream point
cloud analysis.Extensive experiments on four datasets demonstrate the
superiority and robustness of our method when handling noisy and incomplete
point cloud data against existing state-of-the-art methods. Our code is
released at https://github.com/zhoujiahuan1991/ICCV2025-UPP.

</details>


### [48] [GPSMamba: A Global Phase and Spectral Prompt-guided Mamba for Infrared Image Super-Resolution](https://arxiv.org/abs/2507.18998)
*Yongsong Huang,Tomo Miyazaki,Xiaofeng Liu,Shinichiro Omachi*

Main category: cs.CV

TL;DR: 提出一种结合全局相位与频谱提示的新方法（GPSMamba），显著提升红外图像超分辨的重建效果。


<details>
  <summary>Details</summary>
Motivation: 红外图像存在对比度低、纹理稀疏的问题，传统的状态空间模型虽然能捕捉长程依赖，但其1D因果机制限制了2D图像的全局信息建模，从而影响细节恢复。

Method: 1）设计自适应语义-频率状态空间模块（ASF-SSM），将融合后的语义-频率提示信息注入Mamba模块，增强非局部上下文建模并指导重建；2）提出热谱注意力及相位一致性损失，以显式非因果监督提升全局结构与频谱保真度。

Result: 通过大量实验验证，GPSMamba达到了红外图像超分辨领域的最新性能，优于现有方法。

Conclusion: 引入全局相位与频谱提示指导的Mamba模型，有效解决了传统状态空间模型的因果建模局限，为红外图像恢复任务提供了新范式。

Abstract: Infrared Image Super-Resolution (IRSR) is challenged by the low contrast and
sparse textures of infrared data, requiring robust long-range modeling to
maintain global coherence. While State-Space Models like Mamba offer
proficiency in modeling long-range dependencies for this task, their inherent
1D causal scanning mechanism fragments the global context of 2D images,
hindering fine-detail restoration. To address this, we propose Global Phase and
Spectral Prompt-guided Mamba (GPSMamba), a framework that synergizes
architectural guidance with non-causal supervision. First, our Adaptive
Semantic-Frequency State Space Module (ASF-SSM) injects a fused
semantic-frequency prompt directly into the Mamba block, integrating non-local
context to guide reconstruction. Then, a novel Thermal-Spectral Attention and
Phase Consistency Loss provides explicit, non-causal supervision to enforce
global structural and spectral fidelity. By combining these two innovations,
our work presents a systematic strategy to mitigate the limitations of causal
modeling. Extensive experiments demonstrate that GPSMamba achieves
state-of-the-art performance, validating our approach as a powerful new
paradigm for infrared image restoration. Code is available at
https://github.com/yongsongH/GPSMamba.

</details>


### [49] [Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment](https://arxiv.org/abs/2507.19002)
*Ying Ba,Tianyu Zhang,Yalong Bai,Wenyi Mo,Tao Liang,Bing Su,Ji-Rong Wen*

Main category: cs.CV

TL;DR: 当前文本-图像生成系统在表现上取得了巨大进步，但评价方法未能与时俱进。本研究提出ICT和HP新评价指标，使得自动评分更符合人类审美，且提升了模型的实际表现。


<details>
  <summary>Details</summary>
Motivation: 现有用于图像生成系统评价的人类偏好模型（如CLIP和BLIP微调版）在实际应用中倾向于对细节丰富、美学价值高的图像评分过低，与真实的人类审美偏好存在很大差异。需要新的评价体系更好地反映人类对美学和细节的偏好。

Method: 论文提出了ICT分数（Image-Contained-Text Score），关注图像与文本内容的匹配程度，并进一步基于仅图像模态训练HP分数模型，专注于提升图像的美学和细节质量，并兼顾文本-图像对齐。对比实验验证了评价模型的有效性。

Result: 所提出的新模型在评分准确率上相比现有方法提升了10%以上，并且有助于优化主流文本到图像生成模型的表现。

Conclusion: 该研究为图像生成技术朝向更高级的人类美学偏好提供了理论和实证支持，其代码已开源，有望促进相关技术进步。

Abstract: Contemporary image generation systems have achieved high fidelity and
superior aesthetic quality beyond basic text-image alignment. However, existing
evaluation frameworks have failed to evolve in parallel. This study reveals
that human preference reward models fine-tuned based on CLIP and BLIP
architectures have inherent flaws: they inappropriately assign low scores to
images with rich details and high aesthetic value, creating a significant
discrepancy with actual human aesthetic preferences. To address this issue, we
design a novel evaluation score, ICT (Image-Contained-Text) score, that
achieves and surpasses the objectives of text-image alignment by assessing the
degree to which images represent textual content. Building upon this
foundation, we further train an HP (High-Preference) score model using solely
the image modality to enhance image aesthetics and detail quality while
maintaining text-image alignment. Experiments demonstrate that the proposed
evaluation model improves scoring accuracy by over 10\% compared to existing
methods, and achieves significant results in optimizing state-of-the-art
text-to-image models. This research provides theoretical and empirical support
for evolving image generation technology toward higher-order human aesthetic
preferences. Code is available at https://github.com/BarretBa/ICTHP.

</details>


### [50] [MedIQA: A Scalable Foundation Model for Prompt-Driven Medical Image Quality Assessment](https://arxiv.org/abs/2507.19004)
*Siyi Xun,Yue Sun,Jingkun Chen,Zitong Yu,Tong Tong,Xiaohong Liu,Mingxiang Wu,Tao Tan*

Main category: cs.CV

TL;DR: 本文提出了MedIQA，第一个全面的医学图像质量评估（IQA）基础模型，通过大型多模态标注数据集训练，在多类医学图像任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 快速发展的医学影像技术要求精确和自动化的图像质量评估，现有方法难以适应多样化的模态和临床场景，因此需要更通用和高效的IQA解决方案。

Method: 作者构建了大规模多模态人工标注的数据集，提出MedIQA模型，内置显著切片评估模块聚焦诊断相关区域，并采用自动化提示策略，实现物理参数预训练与专家标注精细微调的结合。

Result: 大量实验结果显示，MedIQA在多个下游任务中显著优于现有基线方法。

Conclusion: MedIQA为医学图像质量评估提供了可扩展的基础模型，有助于提升影像诊断工作流和临床决策水平。

Abstract: Rapid advances in medical imaging technology underscore the critical need for
precise and automated image quality assessment (IQA) to ensure diagnostic
accuracy. Existing medical IQA methods, however, struggle to generalize across
diverse modalities and clinical scenarios. In response, we introduce MedIQA,
the first comprehensive foundation model for medical IQA, designed to handle
variability in image dimensions, modalities, anatomical regions, and types. We
developed a large-scale multi-modality dataset with plentiful manually
annotated quality scores to support this. Our model integrates a salient slice
assessment module to focus on diagnostically relevant regions feature retrieval
and employs an automatic prompt strategy that aligns upstream physical
parameter pre-training with downstream expert annotation fine-tuning. Extensive
experiments demonstrate that MedIQA significantly outperforms baselines in
multiple downstream tasks, establishing a scalable framework for medical IQA
and advancing diagnostic workflows and clinical decision-making.

</details>


### [51] [A Survey of Multimodal Hallucination Evaluation and Detection](https://arxiv.org/abs/2507.19024)
*Zhiyuan Chen,Yuecong Min,Jie Zhang,Bei Yan,Jiahao Wang,Xiaozhen Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: 该综述系统梳理了多模态大模型（MLLMs）在图文生成任务中幻觉现象的评测基准和检测方法，并对未来发展方向提出建议。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在处理图文信息整合任务时能力强大，但常出现“幻觉”——输出内容与输入或事实不符，影响模型可靠性，因此系统梳理、评估与检测幻觉现象十分必要。

Method: 首先提出基于信实性和事实性的新幻觉分类方法，总结实际中常见幻觉类型；然后综述现有图文生成任务的幻觉评测基准，包括其构建流程、评估目标和评价指标；进一步汇总最新的幻觉检测方法，侧重实例级幻觉识别。

Result: 文中全面梳理并对比了I2T和T2I领域的幻觉评测基准和检测手段，总结了这些方法在覆盖度、精度等方面的局限性。

Conclusion: 当前幻觉评测和检测还存在不足，有待进一步提升方法的普适性和实用性。未来需要构建更完善的评测基准，并开发更精细和自动化的幻觉检测技术。

Abstract: Multi-modal Large Language Models (MLLMs) have emerged as a powerful paradigm
for integrating visual and textual information, supporting a wide range of
multi-modal tasks. However, these models often suffer from hallucination,
producing content that appears plausible but contradicts the input content or
established world knowledge. This survey offers an in-depth review of
hallucination evaluation benchmarks and detection methods across Image-to-Text
(I2T) and Text-to-image (T2I) generation tasks. Specifically, we first propose
a taxonomy of hallucination based on faithfulness and factuality, incorporating
the common types of hallucinations observed in practice. Then we provide an
overview of existing hallucination evaluation benchmarks for both T2I and I2T
tasks, highlighting their construction process, evaluation objectives, and
employed metrics. Furthermore, we summarize recent advances in hallucination
detection methods, which aims to identify hallucinated content at the instance
level and serve as a practical complement of benchmark-based evaluation.
Finally, we highlight key limitations in current benchmarks and detection
methods, and outline potential directions for future research.

</details>


### [52] [Dual Path Learning -- learning from noise and context for medical image denoising](https://arxiv.org/abs/2507.19035)
*Jitindra Fartiyal,Pedro Freire,Yasmeen Whayeb,James S. Wolffsohn,Sergei K. Turitsyn,Sergei G. Sokolov*

Main category: cs.CV

TL;DR: 本文提出了双通路学习（DPL）模型，有效结合噪声特征与图像上下文信息，在多种医学影像模态和不同噪声类型下均表现出更强的去噪效果。


<details>
  <summary>Details</summary>
Motivation: 医学影像在临床诊断中至关重要，但各种噪声会影响影像质量，导致误判。现有方法大多只关注噪声特性或图像上下文，且通常只针对单一成像模态和噪声类型。为解决这一局限，作者希望设计一种更通用、鲁棒的去噪方法。

Method: 借鉴Geng等人提出的CNCL方法，本文设计了双通路学习（DPL）模型，分别提取噪声特征与上下文信息，并进行融合以生成去噪结果。该方法在多种影像模态和不同噪声类型下进行了实验评估。

Result: 在加入高斯噪声并跨所有模态训练的情况下，DPL模型的PSNR较基线方法UNet提升了3.35%。

Conclusion: DPL模型能够融合噪声与上下文信息，适用于多种医学影像模态及噪声类型，表现出良好的泛化性和鲁棒性。

Abstract: Medical imaging plays a critical role in modern healthcare, enabling
clinicians to accurately diagnose diseases and develop effective treatment
plans. However, noise, often introduced by imaging devices, can degrade image
quality, leading to misinterpretation and compromised clinical outcomes.
Existing denoising approaches typically rely either on noise characteristics or
on contextual information from the image. Moreover, they are commonly developed
and evaluated for a single imaging modality and noise type. Motivated by Geng
et.al CNCL, which integrates both noise and context, this study introduces a
Dual-Pathway Learning (DPL) model architecture that effectively denoises
medical images by leveraging both sources of information and fusing them to
generate the final output. DPL is evaluated across multiple imaging modalities
and various types of noise, demonstrating its robustness and generalizability.
DPL improves PSNR by 3.35% compared to the baseline UNet when evaluated on
Gaussian noise and trained across all modalities. The code is available at
10.5281/zenodo.15836053.

</details>


### [53] [A New One-Shot Federated Learning Framework for Medical Imaging Classification with Feature-Guided Rectified Flow and Knowledge Distillation](https://arxiv.org/abs/2507.19045)
*Yufei Ma,Hanwen Zhang,Qiya Yang,Guibo Luo,Yuesheng Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种面向多中心场景的一次性联邦学习（OSFL）新框架，引入特征引导矫正流模型（FG-RF）和双层知识蒸馏（DLKD）机制，以提升医疗图像领域中的模型性能、训练效率与隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成模型的OSFL方法在医疗领域存在训练效率低和隐私泄漏风险高的问题，并且在非独立同分布（non-IID）数据下很难在一次聚合内收敛。为此作者希望设计更高效且更安全的OSFL方案，适应医疗多中心数据的分布特性。

Method: 提出FG-RF模型，让客户端在特征层级合成图像以提升生成效率与隐私性；设计DLKD聚合方法，使全局学生模型能在聚合时对齐客户端教师模型的输出概率和中间特征层，提升非IID适应性。

Result: 在三个非IID医疗图像数据集上实验证明，该方法较多轮联邦学习提升了至多21.73%，且比FedISCA基线高出平均21.75%。同时，特征级合成图像显著减少了隐私泄漏风险。

Conclusion: 改进的OSFL框架兼顾了联邦学习的单轮高效通信需求和医疗数据的安全性，能显著提升性能与隐私保护，可为医疗图像等高敏感行业多中心协同学习提供有效解决方案。

Abstract: In multi-center scenarios, One-Shot Federated Learning (OSFL) has attracted
increasing attention due to its low communication overhead, requiring only a
single round of transmission. However, existing generative model-based OSFL
methods suffer from low training efficiency and potential privacy leakage in
the healthcare domain. Additionally, achieving convergence within a single
round of model aggregation is challenging under non-Independent and Identically
Distributed (non-IID) data. To address these challenges, in this paper a
modified OSFL framework is proposed, in which a new Feature-Guided Rectified
Flow Model (FG-RF) and Dual-Layer Knowledge Distillation (DLKD) aggregation
method are developed. FG-RF on the client side accelerates generative modeling
in medical imaging scenarios while preserving privacy by synthesizing
feature-level images rather than pixel-level images. To handle non-IID
distributions, DLKD enables the global student model to simultaneously mimic
the output logits and align the intermediate-layer features of client-side
teacher models during aggregation. Experimental results on three non-IID
medical imaging datasets show that our new framework and method outperform
multi-round federated learning approaches, achieving up to 21.73% improvement,
and exceeds the baseline FedISCA by an average of 21.75%. Furthermore, our
experiments demonstrate that feature-level synthetic images significantly
reduce privacy leakage risks compared to pixel-level synthetic images.

</details>


### [54] [Probing Multimodal Fusion in the Brain: The Dominance of Audiovisual Streams in Naturalistic Encoding](https://arxiv.org/abs/2507.19052)
*Hamid Abdollahi,Amir Hossein Mansouri Majoumerd,Amir Hossein Bagheri Baboukani,Amir Abolfazl Suratgar,Mohammad Bagher Menhaj*

Main category: cs.CV

TL;DR: 本文比较了不同复杂度的大脑编码模型在处理自然、多模态刺激时的泛化性能，发现高复杂度模型在分布内数据表现更好，简单模型在分布外数据更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 理解和预测大脑在自然环境下对多模态（视觉、听觉等）刺激的反应，是计算神经科学的核心挑战，而当前模型对未知场景泛化能力的研究不足。

Method: 作者用先进的视觉（X-CLIP）和听觉（Whisper）特征提取器构建了多种大脑编码模型，并在分布内（ID）及分布外（OOD）数据集上进行了严谨评估，比较了基于注意力的复杂模型与线性简单模型的表现，还考察了加入语言特征的影响。

Result: 在ID数据上，注意力机制的高容量模型表现更佳；但在OOD数据集，简单的线性模型更稳健，比已有竞品基线高18%。有趣的是，增加语言文本特征并未提升预测准确度。空间上，该方法在听觉皮层的表现最显著。

Conclusion: 分布外测试对于发展健壮的大脑-人工智能模型至关重要。模型复杂度与泛化能力存在权衡，真实世界神经编码主要依赖于连续的感官流信息，模型结构和刺激特点决定编码方式。

Abstract: Predicting brain activity in response to naturalistic, multimodal stimuli is
a key challenge in computational neuroscience. While encoding models are
becoming more powerful, their ability to generalize to truly novel contexts
remains a critical, often untested, question. In this work, we developed brain
encoding models using state-of-the-art visual (X-CLIP) and auditory (Whisper)
feature extractors and rigorously evaluated them on both in-distribution (ID)
and diverse out-of-distribution (OOD) data. Our results reveal a fundamental
trade-off between model complexity and generalization: a higher-capacity
attention-based model excelled on ID data, but a simpler linear model was more
robust, outperforming a competitive baseline by 18\% on the OOD set.
Intriguingly, we found that linguistic features did not improve predictive
accuracy, suggesting that for familiar languages, neural encoding may be
dominated by the continuous visual and auditory streams over redundant textual
information. Spatially, our approach showed marked performance gains in the
auditory cortex, underscoring the benefit of high-fidelity speech
representations. Collectively, our findings demonstrate that rigorous OOD
testing is essential for building robust neuro-AI models and provides nuanced
insights into how model architecture, stimulus characteristics, and sensory
hierarchies shape the neural encoding of our rich, multimodal world.

</details>


### [55] [ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment](https://arxiv.org/abs/2507.19058)
*Chong Xia,Shengjun Zhang,Fangfu Liu,Chang Liu,Khodchaphun Hirunyaratsameewong,Yueqi Duan*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，能够生成更连贯、一致且沉浸感更强的3D场景长序列，解决了以往方法中语义漂移的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景生成方法构建连续视角时常出现语义漂移，导致场景前后不一致，影响3D场景重建与长视频生成的效果。作者希望通过新的方法提升生成序列的一致性和语义连贯性。

Method: 作者提出了ScenePainter框架，其中关键是设计了一种分层图结构（SceneConceptGraph）来构建多级场景概念的关系。这一结构一方面为新视角生成过程（outpainting）提供指导，保证语义一致；另一方面能动态调整以增强多样性。

Result: 大量实验证明，ScenePainter可以有效克服语义漂移问题，生成的3D视角序列在内容上更加一致并且具有更好的沉浸感。

Conclusion: ScenePainter框架解决了3D场景生成中长期存在的语义漂移难题，有助于提升相关应用如长视频合成和三维场景重构的质量。

Abstract: Perpetual 3D scene generation aims to produce long-range and coherent 3D view
sequences, which is applicable for long-term video synthesis and 3D scene
reconstruction. Existing methods follow a "navigate-and-imagine" fashion and
rely on outpainting for successive view expansion. However, the generated view
sequences suffer from semantic drift issue derived from the accumulated
deviation of the outpainting module. To tackle this challenge, we propose
ScenePainter, a new framework for semantically consistent 3D scene generation,
which aligns the outpainter's scene-specific prior with the comprehension of
the current scene. To be specific, we introduce a hierarchical graph structure
dubbed SceneConceptGraph to construct relations among multi-level scene
concepts, which directs the outpainter for consistent novel views and can be
dynamically refined to enhance diversity. Extensive experiments demonstrate
that our framework overcomes the semantic drift issue and generates more
consistent and immersive 3D view sequences. Project Page:
https://xiac20.github.io/ScenePainter/.

</details>


### [56] [Revisiting DETR for Small Object Detection via Noise-Resilient Query Optimization](https://arxiv.org/abs/2507.19059)
*Xiaocheng Fang,Jieyi Cai,Huanyu Liu,Wenxiu Cai,Yishu Liu,Bingzhi Chen*

Main category: cs.CV

TL;DR: 本论文提出了一种针对小目标检测的创新方法，名为NRQO，通过降低特征金字塔中的噪声敏感性和提升标签分配质量，有效提升了检测性能。实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的小目标检测器已取得进展，但因特征金字塔网络中的噪声敏感性和现有标签分配策略中查询质量降低，导致检测效果仍有限，需提出新方法突破现有瓶颈。

Method: 提出NRQO范式，包含两部分：1）噪声容忍特征金字塔网络（NT-FPN），通过保持空间和语义信息完整性降低特征融合时的噪声干扰；2）成对相似性区域建议网络（PS-RPN），利用位置和形状相似性提升锚框与真实目标的匹配质量，生成更多高质量正样本查询且无需额外超参数。

Result: 在多个公共基准数据集上进行了大量实验。结果显示，NRQO在检测小目标任务中，性能显著优于最先进方法。

Conclusion: NRQO通过NT-FPN和PS-RPN，缓解了噪声影响，提高了小目标检测的精度，为Transformer检测器的进一步优化提供了新思路。

Abstract: Despite advancements in Transformer-based detectors for small object
detection (SOD), recent studies show that these detectors still face challenges
due to inherent noise sensitivity in feature pyramid networks (FPN) and
diminished query quality in existing label assignment strategies. In this
paper, we propose a novel Noise-Resilient Query Optimization (NRQO) paradigm,
which innovatively incorporates the Noise-Tolerance Feature Pyramid Network
(NT-FPN) and the Pairwise-Similarity Region Proposal Network (PS-RPN).
Specifically, NT-FPN mitigates noise during feature fusion in FPN by preserving
spatial and semantic information integrity. Unlike existing label assignment
strategies, PS-RPN generates a sufficient number of high-quality positive
queries by enhancing anchor-ground truth matching through position and shape
similarities, without the need for additional hyperparameters. Extensive
experiments on multiple benchmarks consistently demonstrate the superiority of
NRQO over state-of-the-art baselines.

</details>


### [57] [Negation-Aware Test-Time Adaptation for Vision-Language Models](https://arxiv.org/abs/2507.19064)
*Haochen Han,Alex Jinpeng Wang,Fangming Liu*

Main category: cs.CV

TL;DR: 本文关注视觉-语言模型（VLMs）中的否定理解问题，提出了一种无需额外大量数据的推理时适应方法（NEAT），验证其在否定理解任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 实际应用中常需要模型识别“非”的概念，如医生搜寻排除某种情况的医学图像。但当前主流的VLMs擅长正例理解，对否定理解很薄弱。已有方法多靠收集带否定的数据微调模型，这需要大量数据和计算资源，不可持续。

Method: 作者通过实验证明VLMs的否定理解障碍主要来源于正、反分布的双重概念转移。提出了一种“否定感知推理时自适应”（NEAT）方法：在模型推理阶段，动态调整分布相关参数，从而降低分布漂移对一致语义的干扰，并消除对无关语义的错误一致。

Result: 在多种否定理解任务上进行了广泛实验，结果验证了NEAT方法的有效性。

Conclusion: NEAT方法无需大量新增数据与高昂的计算成本，显著提升了VLMs对否定语义的理解能力，对实际应用具有良好推动作用。

Abstract: In this paper, we study a practical but less-touched problem in
Vision-Language Models (VLMs), \ie, negation understanding. Specifically, many
real-world applications require models to explicitly identify what is false or
non-existent, \eg, radiologists may search for images that exclude specific
conditions. Despite the impressive transferability of VLMs through large-scale
training, they suffer from a critical limitation that fails to handle negation.
To address this challenge, existing methods attribute its root cause to the
scarcity of negation training data and propose to fine-tune VLMs on massive
data containing explicit negation. Undoubtedly, such data-centric solutions
demand substantial data and computational resources, limiting their sustainable
widespread adoption. To tackle negation in a low-carbon manner, we empirically
observe that the key obstacle lies in the dual-concept shifts between the
affirmation and negation distributions. Therefore, we propose a Negation-Aware
Test-Time Adaptation (NEAT) method to efficiently adjust distribution-related
parameters during inference. In brief, NEAT can reduce distribution shift in
consistent semantics while eliminating false distributional consistency in
unrelated semantics. Extensive experiments on the various negation
understanding tasks verify the effectiveness of the proposed method. The code
is available at https://github.com/hhc1997/NEAT.

</details>


### [58] [Cross-Subject Mind Decoding from Inaccurate Representations](https://arxiv.org/abs/2507.19071)
*Yangyang Xu,Bangzhen Liu,Wenqi Shao,Yong Du,Shengfeng He,Tingting Zhu*

Main category: cs.CV

TL;DR: 提出了BAI框架，实现更高精度、可跨被试的fMRI图像解码。通过多模块优化和主流生成模型融合，显著优于现有方法，并对新被试具有良好适应性。


<details>
  <summary>Details</summary>
Motivation: 现有利用预训练生成模型从fMRI解码图像的方法在跨被试映射时效果较差，主要因为主观差异导致映射误差累积，严重影响重建质量。

Method: 提出Bidirectional Autoencoder Intertwining（BAI）框架，包括：1）Subject Bias Modulation Module统一不同个体特征；2）双向映射提升表示预测准确度；3）Semantic Refinement和Visual Coherence模块分别提升语义与视觉一致性；4）整合ControlNet与Stable Diffusion等生成模型。

Result: 在公开基准数据集上，无论定性还是定量评估，所提方法的重建质量均超越了当前最优技术，并且对新被试只需极少的训练样本即可取得良好效果。

Conclusion: 所提BAI框架有效解决了跨被试fMRI解码的误差累积难题，对图像重建具有高保真度和强泛化性，对实际神经科学应用很有前景。

Abstract: Decoding stimulus images from fMRI signals has advanced with pre-trained
generative models. However, existing methods struggle with cross-subject
mappings due to cognitive variability and subject-specific differences. This
challenge arises from sequential errors, where unidirectional mappings generate
partially inaccurate representations that, when fed into diffusion models,
accumulate errors and degrade reconstruction fidelity. To address this, we
propose the Bidirectional Autoencoder Intertwining framework for accurate
decoded representation prediction. Our approach unifies multiple subjects
through a Subject Bias Modulation Module while leveraging bidirectional mapping
to better capture data distributions for precise representation prediction. To
further enhance fidelity when decoding representations into stimulus images, we
introduce a Semantic Refinement Module to improve semantic representations and
a Visual Coherence Module to mitigate the effects of inaccurate visual
representations. Integrated with ControlNet and Stable Diffusion, our method
outperforms state-of-the-art approaches on benchmark datasets in both
qualitative and quantitative evaluations. Moreover, our framework exhibits
strong adaptability to new subjects with minimal training samples.

</details>


### [59] [A Self-training Framework for Semi-supervised Pulmonary Vessel Segmentation and Its Application in COPD](https://arxiv.org/abs/2507.19074)
*Shuiqing Zhao,Meihuan Wang,Jiaxuan Xu,Jie Feng,Wei Qian,Rongchang Chen,Zhenyu Liang,Shouliang Qi,Yanan Wu*

Main category: cs.CV

TL;DR: 研究提出了一种半监督的肺血管分割方法，通过教师-学生模型自训练，提高了COPD患者CT图像中肺血管分割的精度。


<details>
  <summary>Details</summary>
Motivation: 准确分割和量化COPD患者CT影像中的肺血管（尤其是小血管）对疾病分析和管理非常关键，但现有方法在标注数据稀缺时精度有限。

Method: 采用教师-学生自训练框架，仅需极少量带注释的CT图像。首先用互动标注法获取高质量数据训练教师模型，然后使用教师模型对无标签数据生成伪标签，筛选可靠伪标签后再训练学生模型，反复迭代直至获得最佳模型。

Result: 在125例COPD患者的非增强CT扫描数据上进行了大量实验。提出的方法（Semi2）在分割精度上提高了2.3%，最终达到了90.3%的精度。此外，还揭示了不同COPD严重程度下肺血管的异同。

Conclusion: 提出的方法不仅大幅提升了肺血管分割的准确性，还可应用于COPD相关的临床分析，具有实际推广价值。

Abstract: Background: It is fundamental for accurate segmentation and quantification of
the pulmonary vessel, particularly smaller vessels, from computed tomography
(CT) images in chronic obstructive pulmonary disease (COPD) patients.
Objective: The aim of this study was to segment the pulmonary vasculature using
a semi-supervised method. Methods: In this study, a self-training framework is
proposed by leveraging a teacher-student model for the segmentation of
pulmonary vessels. First, the high-quality annotations are acquired in the
in-house data by an interactive way. Then, the model is trained in the
semi-supervised way. A fully supervised model is trained on a small set of
labeled CT images, yielding the teacher model. Following this, the teacher
model is used to generate pseudo-labels for the unlabeled CT images, from which
reliable ones are selected based on a certain strategy. The training of the
student model involves these reliable pseudo-labels. This training process is
iteratively repeated until an optimal performance is achieved. Results:
Extensive experiments are performed on non-enhanced CT scans of 125 COPD
patients. Quantitative and qualitative analyses demonstrate that the proposed
method, Semi2, significantly improves the precision of vessel segmentation by
2.3%, achieving a precision of 90.3%. Further, quantitative analysis is
conducted in the pulmonary vessel of COPD, providing insights into the
differences in the pulmonary vessel across different severity of the disease.
Conclusion: The proposed method can not only improve the performance of
pulmonary vascular segmentation, but can also be applied in COPD analysis. The
code will be made available at
https://github.com/wuyanan513/semi-supervised-learning-for-vessel-segmentation.

</details>


### [60] [SP-Mamba: Spatial-Perception State Space Model for Unsupervised Medical Anomaly Detection](https://arxiv.org/abs/2507.19076)
*Rui Pan,Ruiying Lu*

Main category: cs.CV

TL;DR: 提出了一种新的用于医学异常检测的无监督模型——SP-Mamba，在三个医学数据集上取得了最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN方法难以捕捉长距离依赖，Transformer方法计算量大，且传统方法难以充分利用医学影像的一致结构模式，因此需要更高效且适应性强的模型用于医学异常检测。

Method: 提出了SP-Mamba框架，结合空间感知Mamba结构、窗口滑动原型学习和基于Circular-Hilbert扫描的Mamba来增强对结构规律性和空间信息的建模，并结合异常图的集中与对比特性进行改进。

Result: 在三个公开医学异常检测数据集上进行了大量实验，结果表明SP-Mamba方法在性能和鲁棒性方面均达到了最新最好水平。

Conclusion: SP-Mamba模型能高效且准确地进行医学影像的异常检测，优于以往方法，未来有广泛应用前景。

Abstract: Radiography imaging protocols target on specific anatomical regions,
resulting in highly consistent images with recurrent structural patterns across
patients. Recent advances in medical anomaly detection have demonstrated the
effectiveness of CNN- and transformer-based approaches. However, CNNs exhibit
limitations in capturing long-range dependencies, while transformers suffer
from quadratic computational complexity. In contrast, Mamba-based models,
leveraging superior long-range modeling, structural feature extraction, and
linear computational efficiency, have emerged as a promising alternative. To
capitalize on the inherent structural regularity of medical images, this study
introduces SP-Mamba, a spatial-perception Mamba framework for unsupervised
medical anomaly detection. The window-sliding prototype learning and
Circular-Hilbert scanning-based Mamba are introduced to better exploit
consistent anatomical patterns and leverage spatial information for medical
anomaly detection. Furthermore, we excavate the concentration and contrast
characteristics of anomaly maps for improving anomaly detection. Extensive
experiments on three diverse medical anomaly detection benchmarks confirm the
proposed method's state-of-the-art performance, validating its efficacy and
robustness. The code is available at https://github.com/Ray-RuiPan/SP-Mamba.

</details>


### [61] [Multi-Task Dense Prediction Fine-Tuning with Mixture of Fine-Grained Experts](https://arxiv.org/abs/2507.19077)
*Yangyang Xu,Xi Ye,Duo Su*

Main category: cs.CV

TL;DR: 本文提出了一种新的细粒度专家混合（FGMoE）架构，用于改进多任务学习（MTL）在密集预测任务中的效果。该方法通过创新的专家划分与参数高效微调，显著提高了性能和参数利用率。


<details>
  <summary>Details</summary>
Motivation: 当前MTL在密集预测任务上效果不错，但面临共享与特定任务信息难以平衡的问题。MoE虽能一定程度缓解，但仍有改进空间，特别是在参数高效与任务间有益信息共享方面。本文旨在进一步细化专家划分与信息共享，提升MTL的表现。

Method: FGMoE架构包含三个关键创新：（1）MLP中间层维度划分的任务内专家，实现细粒度的信息分解与高效参数利用；（2）同任务不同上下文共享专家，减少冗余，让路由专家集中独特任务内容；（3）全局专家，根据输入特征和任务需求自适应实现任务间知识迁移，防止有害干扰。此外，仅对解码器微调以提升参数效率。

Result: 在NYUD-v2和PASCAL-Context两个密集预测数据集上，FGMoE在多个标准下超越了现有MoE-MTL方法，且使用更少参数，展现出显著的性能与参数效率优势。

Conclusion: FGMoE通过三重专家创新与参数高效微调，有效提升了MoE多任务架构在密集预测中的表现，并为MTL领域提供了新的技术方向。

Abstract: Multi-task learning (MTL) for dense prediction has shown promising results
but still faces challenges in balancing shared representations with
task-specific specialization. In this paper, we introduce a novel Fine-Grained
Mixture of Experts (FGMoE) architecture that explores MoE-based MTL models
through a combination of three key innovations and fine-tuning. First, we
propose intra-task experts that partition along intermediate hidden dimensions
of MLPs, enabling finer decomposition of task information while maintaining
parameter efficiency. Second, we introduce shared experts that consolidate
common information across different contexts of the same task, reducing
redundancy, and allowing routing experts to focus on unique aspects. Third, we
design a global expert that facilitates adaptive knowledge transfer across
tasks based on both input feature and task requirements, promoting beneficial
information sharing while preventing harmful interference. In addition, we use
the fine-tuning approach to improve parameter efficiency only by training the
parameters of the decoder. Extensive experimental results show that the
proposed FGMoE uses fewer parameters and significantly outperforms current
MoE-based competitive MTL models on two dense prediction datasets
(\textit{i.e.,} NYUD-v2, PASCAL-Context) in various metrics.

</details>


### [62] [MedSymmFlow: Bridging Generative Modeling and Classification in Medical Imaging through Symmetrical Flow Matching](https://arxiv.org/abs/2507.19098)
*Francisco Caetano,Lemar Abdi,Christiaan Viviers,Amaan Valiuddin,Fons van der Sommen*

Main category: cs.CV

TL;DR: 提出了一种名为MedSymmFlow的混合模型，能够同时实现医学图像的分类、生成和不确定性量化，其分类性能和不确定性估计效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像分类任务在临床环境下要求预测结果既要准确，又需对不确定性做出合理估计，以辅助高风险决策。但现有判别式模型难以自然获得高质量不确定性估计。

Method: 作者提出了基于对称流匹配（Symmetrical Flow Matching）的混合生成-判别模型MedSymmFlow，采用潜空间表征以处理高分辨率输入，并引入语义掩码条件机制提升诊断相关性。不确定性估计基于其生成采样过程自然产生。

Result: 在四个MedMNIST数据集（涵盖多种成像模态和病种）上的实验显示，MedSymmFlow在分类准确率和AUC上与主流方法持平或更优，并能给出更可靠的不确定性估计。选择性预测下，该模型性能进一步提升。

Conclusion: MedSymmFlow不仅提高了医学图像分类的准确性，还实现了高质量的不确定性估计，有望增强高风险临床场景下的医学决策支持。

Abstract: Reliable medical image classification requires accurate predictions and
well-calibrated uncertainty estimates, especially in high-stakes clinical
settings. This work presents MedSymmFlow, a generative-discriminative hybrid
model built on Symmetrical Flow Matching, designed to unify classification,
generation, and uncertainty quantification in medical imaging. MedSymmFlow
leverages a latent-space formulation that scales to high-resolution inputs and
introduces a semantic mask conditioning mechanism to enhance diagnostic
relevance. Unlike standard discriminative models, it naturally estimates
uncertainty through its generative sampling process. The model is evaluated on
four MedMNIST datasets, covering a range of modalities and pathologies. The
results show that MedSymmFlow matches or exceeds the performance of established
baselines in classification accuracy and AUC, while also delivering reliable
uncertainty estimates validated by performance improvements under selective
prediction.

</details>


### [63] [LISA: A Layer-wise Integration and Suppression Approach for Hallucination Mitigation in Multimodal Large Language Models](https://arxiv.org/abs/2507.19110)
*Zhihui Guo,Xin Man,Hui Xu,Jie Shao*

Main category: cs.CV

TL;DR: 本文提出了LISA方法，通过多层融合和调控机制，有效减少多模态大模型在图像描述中的幻觉现象，即描述了不存在的物体。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（MLLMs）在视觉-语言任务中表现优秀，但在图像描述等任务上常常出现对象幻觉问题，为此需要新方法提升生成的一致性和可靠性。

Method: LISA是一种分层集成和抑制方法，利用MLLMs内部的功能分层特点。首先，针对不同层次采用频域调控，抑制深层的过度激活同时保留浅层的对齐提示；其次，从多个层提取token-level logits，并通过锚点机制进行融合，实现自适应集成。该方法可无缝集成到现有MLLMs，且为即插即用型。

Result: 实验证明，LISA方法在多项基准测试中可使幻觉率降低53.6%，POPE F1提升4.5%，对多模型和多任务表现出良好的泛化能力。

Conclusion: LISA大幅提升了多模态大模型在视觉-语言生成任务中的一致性和可靠性，是应对对象幻觉问题的有效、可扩展解决方案。

Abstract: Multimodal Large Language Models (MLLMs) excel in vision-language tasks such
as image captioning but remain prone to object hallucinations, where they
describe objects that do not appear in the image. To mitigate this, we propose
\textbf{LISA}, a \textbf{L}ayer-wise \textbf{I}ntegration and
\textbf{S}uppression \textbf{A}pproach that enhances generation consistency
through hierarchical modulation and multi-layer fusion. LISA leverages the
functional hierarchy within MLLMs, where shallow layers provide visual
grounding, middle layers encode semantics, and deep layers tend to amplify
spurious signals. First, zone-specific spectral modulation stabilizes attention
by suppressing over-amplified activations in deeper layers while preserving
alignment cues in earlier layers. Second, token-level logits from selected
layers are fused via anchor-based routing, with token-wise anchor selection and
soft logit fusion enabling adaptive integration during decoding. LISA is fully
\textbf{plug-and-play} and can be seamlessly integrated into existing MLLMs,
including Qwen2.5-VL. Experiments on multiple benchmarks show that LISA reduces
hallucinations by up to 53.6\% in $\mathrm{CHAIR}_I$ and improves POPE F1 by
4.5\%, demonstrating strong generalization across models and tasks.

</details>


### [64] [Cross Spatial Temporal Fusion Attention for Remote Sensing Object Detection via Image Feature Matching](https://arxiv.org/abs/2507.19118)
*Abu Sadat Mohammad Salehin Amit,Xiaoli Zhang,Md Masum Billa Shagar,Zhaojun Liu,Xiongfei Li,Fanlong Meng*

Main category: cs.CV

TL;DR: 本文提出了一种针对遥感图像跨模态匹配的特征描述方法CSTF，显著提升了匹配精度和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 多模态遥感图像因存在几何与辐射差异，特征描述和匹配难度大，现有方法普遍无法充分识别跨模态相似性。

Method: 提出了Cross Spatial Temporal Fusion（CSTF）机制：独立检测尺度不变关键点，生成匹配相关图，并将相似性匹配重塑为利用SoftMax与FCN的分类任务，融合局部与全局上下文信息。

Result: 在HRSC2016和DOTA数据集上，目标检测mAP分别达90.99%与90.86%，均超越现有方法，并保持12.5FPS的推理速度。

Conclusion: CSTF方法有效提升了跨模态遥感图像特征匹配能力，有助于目标检测等下游任务，兼具精度与效率。

Abstract: Effectively describing features for cross-modal remote sensing image matching
remains a challenging task due to the significant geometric and radiometric
differences between multimodal images. Existing methods primarily extract
features at the fully connected layer but often fail to capture cross-modal
similarities effectively. We propose a Cross Spatial Temporal Fusion (CSTF)
mechanism that enhances feature representation by integrating scale-invariant
keypoints detected independently in both reference and query images. Our
approach improves feature matching in two ways: First, by creating
correspondence maps that leverage information from multiple image regions
simultaneously, and second, by reformulating the similarity matching process as
a classification task using SoftMax and Fully Convolutional Network (FCN)
layers. This dual approach enables CSTF to maintain sensitivity to distinctive
local features while incorporating broader contextual information, resulting in
robust matching across diverse remote sensing modalities. To demonstrate the
practical utility of improved feature matching, we evaluate CSTF on object
detection tasks using the HRSC2016 and DOTA benchmark datasets. Our method
achieves state-of-theart performance with an average mAP of 90.99% on HRSC2016
and 90.86% on DOTA, outperforming existing models. The CSTF model maintains
computational efficiency with an inference speed of 12.5 FPS. These results
validate that our approach to crossmodal feature matching directly enhances
downstream remote sensing applications such as object detection.

</details>


### [65] [PatchTraj: Dynamic Patch Representation Learning for Time-Frequency Trajectory Prediction](https://arxiv.org/abs/2507.19119)
*Yanghong Liu,Xingping Dong,Ming Li,Weixing Zhang,Yidong Lou*

Main category: cs.CV

TL;DR: 提出了一种动态patch机制融合时域和频域特征的新方法PatchTraj，大幅提升了行人轨迹预测的效果与效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于点或网格的行人轨迹预测方法存在两个主要问题：一是难以同时捕捉局部运动细节和远程时空依赖，二是传统时序建模缺乏与频域特征的结合，影响对轨迹复杂动态的刻画。

Method: 将轨迹序列分解为时域原始序列和频域分量，采用动态patch分割进行多尺度分段；每个patch经过带有尺度感知特征提取的自适应嵌入模块，随后分层聚合细粒度和长距离特征。两个分支再通过跨模态注意力互补融合，并用Transformer自回归预测未来轨迹。

Result: 在ETH-UCY、SDD、NBA和JRDB四个主流数据集上进行了大量实验，PatchTraj不仅取得了领先的预测准确率，还兼顾了推理效率。

Conclusion: PatchTraj有效结合了时域与频域特征建模，整体提升了不同场景下行人轨迹预测的准确性和效率，展现出优越的应用潜力。

Abstract: Pedestrian trajectory prediction is crucial for autonomous driving and
robotics. While existing point-based and grid-based methods expose two key
limitations: insufficiently modeling human motion dynamics, as they fail to
balance local motion details with long-range spatiotemporal dependencies, and
the time representation lacks interaction with the frequency domain in modeling
trajectory sequences. To address these challenges, we propose PatchTraj, a
dynamic patch-based trajectory prediction framework that unifies time-domain
and frequency-domain representations. Specifically, we decompose the trajectory
into raw time sequences and frequency components, employing dynamic patch
partitioning for multi-scale trajectory segmentation to capture hierarchical
motion patterns. Each patch is processed by an adaptive embedding layer with
scale-aware feature extraction, followed by hierarchical feature aggregation to
model both fine-grained and long-range dependencies. The outputs of two
branches interact via cross-modal attention, enabling complementary fusion of
temporal and spectral cues. Finally, a Transformer encoder-decoder integrates
both modalities to autoregressively predict future trajectories. Extensive
experiments on ETH-UCY, SDD, NBA, and JRDB datasets demonstrate that our method
achieves state-of-the-art performance with high efficiency.

</details>


### [66] [Preserving Topological and Geometric Embeddings for Point Cloud Recovery](https://arxiv.org/abs/2507.19121)
*Kaiyue Zhou,Zelong Tan,Hongxiao Wang,Ya-li Li,Shengjin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种面向点云采样与恢复的端到端网络TopGeoFormer，通过综合拓扑与几何特征显著提升点云重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有点云采样与恢复方法难以有效结合和利用点云的拓扑与几何特征，影响了最终的重建效果。本文旨在解决这一问题。

Method: 1. 设计TopGeoFormer网络，将拓扑嵌入特征贯穿点云采样和恢复流程，实现空间结构的保留。
2. 提出InterTwining Attention模块，高效融合拓扑与几何特征，实现具备局部感知的可学习形状上下文。
3. 引入全几何损失与拓扑约束损失，分别优化欧氏空间与拓扑空间内的特征表现。

Result: 在多种传统与学习型采样/上采样算法、多个数据集测试下，TopGeoFormer在定量与定性评测中均显著优于现有主流点云采样与重建方法。

Conclusion: 本文方法通过创新的特征融合与优化机制，有效提升点云采样与重建质量，为点云处理领域提供了新的有效工具和思路。

Abstract: Recovering point clouds involves the sequential process of sampling and
restoration, yet existing methods struggle to effectively leverage both
topological and geometric attributes. To address this, we propose an end-to-end
architecture named \textbf{TopGeoFormer}, which maintains these critical
features throughout the sampling and restoration phases. First, we revisit
traditional feature extraction techniques to yield topological embedding using
a continuous mapping of relative relationships between neighboring points, and
integrate it in both phases for preserving the structure of the original space.
Second, we propose the \textbf{InterTwining Attention} to fully merge
topological and geometric embeddings, which queries shape with local awareness
in both phases to form a learnable shape context facilitated with point-wise,
point-shape-wise, and intra-shape features. Third, we introduce a full geometry
loss and a topological constraint loss to optimize the embeddings in both
Euclidean and topological spaces. The geometry loss uses inconsistent matching
between coarse-to-fine generations and targets for reconstructing better
geometric details, and the constraint loss limits embedding variances for
better approximation of the topological space. In experiments, we
comprehensively analyze the circumstances using the conventional and
learning-based sampling/upsampling algorithms. The quantitative and qualitative
results demonstrate that our method significantly outperforms existing sampling
and recovery methods.

</details>


### [67] [Learned Image Compression with Hierarchical Progressive Context Modeling](https://arxiv.org/abs/2507.19125)
*Yuqi Li,Haotian Zhang,Li Li,Dong Liu*

Main category: cs.CV

TL;DR: 提出了一种新颖的分层递进上下文建模方法（HPCM），能更高效地建模长距离和多样化上下文信息，在图像压缩中取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习图像压缩方法在上下文建模方面虽有进展，但对长距离依赖和跨步多样性上下文的高效利用仍有不足；需要一种能高效捕获和融合多尺度上下文信息的新方法。

Method: 作者提出了分层递进上下文模型（HPCM），通过分层编码调度机制实现多尺度潜变量间的上下文依赖建模，并用递进式上下文融合方法，将前一步的上下文信息融合进当前步骤以充分利用历史信息。

Result: 实验表明，所提方法在压缩性能（码率-失真指标）和计算复杂度之间取得了更优的平衡，实现了目前最好的压缩效果。

Conclusion: HPCM能更有效地获得和融合多尺度、历时性的上下文信息，在保持高效的情况下提升了压缩性能，对后续工作具备指导和应用意义。

Abstract: Context modeling is essential in learned image compression for accurately
estimating the distribution of latents. While recent advanced methods have
expanded context modeling capacity, they still struggle to efficiently exploit
long-range dependency and diverse context information across different coding
steps. In this paper, we introduce a novel Hierarchical Progressive Context
Model (HPCM) for more efficient context information acquisition. Specifically,
HPCM employs a hierarchical coding schedule to sequentially model the
contextual dependencies among latents at multiple scales, which enables more
efficient long-range context modeling. Furthermore, we propose a progressive
context fusion mechanism that incorporates contextual information from previous
coding steps into the current step, effectively exploiting diverse contextual
information. Experimental results demonstrate that our method achieves
state-of-the-art rate-distortion performance and strikes a better balance
between compression performance and computational complexity. The code is
available at https://github.com/lyq133/LIC-HPCM.

</details>


### [68] [MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective](https://arxiv.org/abs/2507.19131)
*Weitian Wang,Rai Shubham,Cecilia De La Parra,Akash Kumar*

Main category: cs.CV

TL;DR: 本文提出了 MixA-Q 框架，通过利用自注意力块中不同窗口的激活稀疏特性，实现高效的混合精度激活量化，提升了视觉Transformer的推理效率和精度权衡。


<details>
  <summary>Details</summary>
Motivation: 当前视觉Transformer在推理时由于激活量过大和量化精度受限，导致难以兼顾效率与精度，尤其是在窗口内存在激活稀疏性的情况下单一量化策略未能充分利用模型特点。

Method: MixA-Q将注意力窗口按重要性分配不同量化位宽，对不重要窗口采用低比特，重要窗口采用高比特，并提出了双分支Swin块分别处理高/低比特激活，可与QAT和PTQ等多种量化训练方法结合。

Result: 在COCO数据集上，MixA-Q在PTQ配置下无需训练即可实现1.35倍推理加速且无精度损失，结合QAT可实现1.25倍无损加速及激活剪枝下1.53倍加速（MAP仅下降1%），且能减少重要区域的量化误差，使W4A4模型mAP提升0.7%。

Conclusion: MixA-Q有效改善了窗口型ViT量化模型的效率与精度权衡，通过激活稀疏性感知的量化赋予模型更优的推理性能，对深度模型高效部署具有实际意义。

Abstract: In this paper, we propose MixA-Q, a mixed-precision activation quantization
framework that leverages intra-layer activation sparsity (a concept widely
explored in activation pruning methods) for efficient inference of quantized
window-based vision transformers. For a given uniform-bit quantization
configuration, MixA-Q separates the batched window computations within Swin
blocks and assigns a lower bit width to the activations of less important
windows, improving the trade-off between model performance and efficiency. We
introduce a Two-Branch Swin Block that processes activations separately in
high- and low-bit precision, enabling seamless integration of our method with
most quantization-aware training (QAT) and post-training quantization (PTQ)
methods, or with simple modifications. Our experimental evaluations over the
COCO dataset demonstrate that MixA-Q achieves a training-free 1.35x
computational speedup without accuracy loss in PTQ configuration. With QAT,
MixA-Q achieves a lossless 1.25x speedup and a 1.53x speedup with only a 1% mAP
drop by incorporating activation pruning. Notably, by reducing the quantization
error in important regions, our sparsity-aware quantization adaptation improves
the mAP of the quantized W4A4 model (with both weights and activations in 4-bit
precision) by 0.7%, reducing quantization degradation by 24%.

</details>


### [69] [Balancing Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot Segmentation](https://arxiv.org/abs/2507.19140)
*Tianyu Zou,Shengwu Xiong,Ruilin Yao,Yi Rong*

Main category: cs.CV

TL;DR: 本文提出一种结合原型学习和亲和学习优点的新型网络PAHNet，以提升小样本分割任务的表现。通过创新模块增强特征并校准注意力，实现了更高的分割准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的小样本分割方法主要分为原型学习和亲和学习两种，各有局限：原型学习较为保守，亲和学习较为激进。如何融合两种方法的优点，以获得更优的分割效果，是本文研究的动机。

Method: 作者提出了PAHNet网络，在亲和学习框架中引入了原型预测指导的特征增强（PFE）和注意力分数校准（ASC）模块。借助预训练原型模型的预测，分别提升前景表示能力与抑制错误前背景关系，从而平衡预测的保守性和激进性。

Result: 在PASCAL-5i和COCO-20i两个主流小样本分割数据集上的1-shot和5-shot实验中，PAHNet优于最近的大多数方法，表现出更高的分割精度。

Conclusion: PAHNet有效结合了原型与亲和学习的优点，改善了小样本分割中预测保守或激进的局限性，大幅提升了性能，结果表明方法具有很强的实用价值和推广潜力。

Abstract: This paper studies the few-shot segmentation (FSS) task, which aims to
segment objects belonging to unseen categories in a query image by learning a
model on a small number of well-annotated support samples. Our analysis of two
mainstream FSS paradigms reveals that the predictions made by prototype
learning methods are usually conservative, while those of affinity learning
methods tend to be more aggressive. This observation motivates us to balance
the conservative and aggressive information captured by these two types of FSS
frameworks so as to improve the segmentation performance. To achieve this, we
propose a **P**rototype-**A**ffinity **H**ybrid **Net**work (PAHNet), which
introduces a Prototype-guided Feature Enhancement (PFE) module and an Attention
Score Calibration (ASC) module in each attention block of an affinity learning
model (called affinity learner). These two modules utilize the predictions
generated by a pre-trained prototype learning model (called prototype
predictor) to enhance the foreground information in support and query image
representations and suppress the mismatched foreground-background (FG-BG)
relationships between them, respectively. In this way, the aggressiveness of
the affinity learner can be effectively mitigated, thereby eventually
increasing the segmentation accuracy of our PAHNet method. Experimental results
show that PAHNet outperforms most recently proposed methods across 1-shot and
5-shot settings on both PASCAL-5$^i$ and COCO-20$^i$ datasets, suggesting its
effectiveness. The code is available at: [GitHub - tianyu-zou/PAHNet: Balancing
Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot
Segmentation (ICCV'25)](https://github.com/tianyu-zou/PAHNet)

</details>


### [70] [DASH: 4D Hash Encoding with Self-Supervised Decomposition for Real-Time Dynamic Scene Rendering](https://arxiv.org/abs/2507.19141)
*Jie Chen,Zhangchi Hu,Peixi Wu,Huyue Zhu,Hebei Li,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 本文提出了DASH框架，通过结合4D哈希编码和自监督分解，实现了动态图像场景的高质量实时重建和渲染，在多个真实世界数据集上达到了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有动态高斯溅射的平面化方法因低秩假设不适合，导致特征混叠和渲染质量降低；而4D哈希编码虽可避免低秩问题，但直接用于全场景又面临哈希碰撞与冗余严重。因此亟需一种能兼容高效、高质量和无低秩假设的动态图场景重建方法。

Method: 提出的DASH方法包括三步：首先，利用自监督分解机制，在无人工标注或预先生成遮罩的情况下，将动态图像场景分离为动态和静态两部分；其次，在动态部分采用多分辨率4D哈希编码，实现强表达能力同时规避低秩假设；最后，引入时空平滑正则减少动态场景中的变形伪影。

Result: 实验表明，DASH在多个真实世界数据集上实现了目前最优的动态场景渲染质量；在一块4090显卡上达到264 FPS的实时渲染速度。

Conclusion: 本工作证明了结合4D哈希编码和自监督分解能够有效提升动态图像场景的实时渲染质量与效率，对数字内容生成和3D视频等场景有较大应用前景。

Abstract: Dynamic scene reconstruction is a long-term challenge in 3D vision. Existing
plane-based methods in dynamic Gaussian splatting suffer from an unsuitable
low-rank assumption, causing feature overlap and poor rendering quality.
Although 4D hash encoding provides an explicit representation without low-rank
constraints, directly applying it to the entire dynamic scene leads to
substantial hash collisions and redundancy. To address these challenges, we
present DASH, a real-time dynamic scene rendering framework that employs 4D
hash encoding coupled with self-supervised decomposition. Our approach begins
with a self-supervised decomposition mechanism that separates dynamic and
static components without manual annotations or precomputed masks. Next, we
introduce a multiresolution 4D hash encoder for dynamic elements, providing an
explicit representation that avoids the low-rank assumption. Finally, we
present a spatio-temporal smoothness regularization strategy to mitigate
unstable deformation artifacts. Experiments on real-world datasets demonstrate
that DASH achieves state-of-the-art dynamic rendering performance, exhibiting
enhanced visual quality at real-time speeds of 264 FPS on a single 4090 GPU.
Code: https://github.com/chenj02/DASH.

</details>


### [71] [Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers](https://arxiv.org/abs/2507.19175)
*Yuki Igaue,Hiroaki Aizawa*

Main category: cs.CV

TL;DR: 本文提出了一种基于注意力权重方差的Patch裁剪方法，可提升视觉Transformer模型的计算效率，在保证分类准确率的情况下减少计算量。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer的多头自注意力机制虽性能优异，但计算复杂度高，尤其是在处理大量输入Patch时耗时严重，因此需要一种能够减少冗余Patch、提升计算效率的方法。

Method: 通过计算每个Patch在多头自注意力下注意力权重的方差（或采用鲁棒统计量如中位绝对偏差），评价Patch的重要性，并在训练和推理阶段裁剪冗余Patch。此外，采用重叠Patch嵌入进一步提升性能。

Result: 实验表明，该方法在提升吞吐量（计算效率）的同时，能够很好地保持甚至改善分类准确率。使用中位绝对偏差等更鲁棒的统计量也取得了相似甚至更好的效果。

Conclusion: 基于注意力方差/鲁棒统计量的Patch裁剪可以显著提升Vision Transformer的效率且几乎无损性能，方法简单易用，且可扩展到不同应用场景。

Abstract: Multi-head self-attention is a distinctive feature extraction mechanism of
vision transformers that computes pairwise relationships among all input
patches, contributing significantly to their high performance. However, it is
known to incur a quadratic computational complexity with respect to the number
of patches. One promising approach to address this issue is patch pruning,
which improves computational efficiency by identifying and removing redundant
patches. In this work, we propose a patch pruning strategy that evaluates the
importance of each patch based on the variance of attention weights across
multiple attention heads. This approach is inspired by the design of multi-head
self-attention, which aims to capture diverse attention patterns across
different subspaces of feature representations. The proposed method can be
easily applied during both training and inference, and achieves improved
throughput while maintaining classification accuracy in scenarios such as
fine-tuning with pre-trained models. In addition, we also found that using
robust statistical measures, such as the median absolute deviation in place of
variance, to assess patch importance can similarly lead to strong performance.
Furthermore, by introducing overlapping patch embeddings, our method achieves
better performance with comparable throughput to conventional approaches that
utilize all patches.

</details>


### [72] [Continual Learning-Based Unified Model for Unpaired Image Restoration Tasks](https://arxiv.org/abs/2507.19184)
*Kotha Kartheek,Lingamaneni Gnanesh Chowdary,Snehasis Mukherjee*

Main category: cs.CV

TL;DR: 本文提出了一种统一框架，用于恢复受多种不良天气（如雾、雪、雨）影响的图像，并在多个基准测试上取得了优于现有技术的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只针对单一天气条件进行图像修复，不能泛化到多种情况。而实际应用（如自动驾驶）需要面对多变的恶劣天气，因此需要一种能应对不同天气条件的统一图像修复模型。

Method: 作者提出了一种持续学习框架，结合了三项关键创新：（1）选择性卷积融合层，可动态地结合全局与局部特征，进行自适应特征选择；（2）弹性权值固化（EWC），减缓持续学习中的灾难性遗忘问题；（3）循环对比损失，实现特征判别性增强并保持语义一致性。此外，提出了无配对图像恢复方法，降低对训练数据的依赖。

Result: 在去雾、去雪和去雨的标准基准数据集上，提出方法在PSNR、SSIM及感知质量指标上，均显著优于现有先进方法。

Conclusion: 该方法实现了在多种恶劣天气下统一且高效的图像修复，提升了图像质量，对实际应用具有重要意义。

Abstract: Restoration of images contaminated by different adverse weather conditions
such as fog, snow, and rain is a challenging task due to the varying nature of
the weather conditions. Most of the existing methods focus on any one
particular weather conditions. However, for applications such as autonomous
driving, a unified model is necessary to perform restoration of corrupted
images due to different weather conditions. We propose a continual learning
approach to propose a unified framework for image restoration. The proposed
framework integrates three key innovations: (1) Selective Kernel Fusion layers
that dynamically combine global and local features for robust adaptive feature
selection; (2) Elastic Weight Consolidation (EWC) to enable continual learning
and mitigate catastrophic forgetting across multiple restoration tasks; and (3)
a novel Cycle-Contrastive Loss that enhances feature discrimination while
preserving semantic consistency during domain translation. Further, we propose
an unpaired image restoration approach to reduce the dependance of the proposed
approach on the training data. Extensive experiments on standard benchmark
datasets for dehazing, desnowing and deraining tasks demonstrate significant
improvements in PSNR, SSIM, and perceptual quality over the state-of-the-art.

</details>


### [73] [Reconstruct or Generate: Exploring the Spectrum of Generative Modeling for Cardiac MRI](https://arxiv.org/abs/2507.19186)
*Niklas Bubeck,Yundi Zhang,Suprosanna Shit,Daniel Rueckert,Jiazhen Pan*

Main category: cs.CV

TL;DR: 本文系统性分析了现代潜变量扩散模型和自回归模型在医学影像重建与生成任务中的表现，发现扩散模型适合生成高质量的无条件影像，但在高遮挡重建时容易幻觉，自回归模型在不同遮挡条件下有更稳定的表现，但整体保真度略低。


<details>
  <summary>Details</summary>
Motivation: 随着医学影像对生成模型的依赖增加，重建（如修复、超分辨率）和生成（数据增强、反事实分析）任务有不同需求。本研究动机是系统比较主流生成模型在‘重建-生成谱’上的表现差异，寻找最佳适用场景。

Method: 作者建立了一个“生成模型动物园”，系统地对比了潜变量扩散模型和自回归模型在心脏影像的inpainting（不同遮挡比例）、采样策略与无条件生成任务中的表现，基于标准评价指标进行基准测试。

Result: 扩散模型在无条件生成任务上实现了更好的感知质量；在高遮挡修复时，扩散模型容易产生非事实内容（幻觉），而自回归模型在不同遮挡层次下感知指标较稳定，但影像保真度通常较低。

Conclusion: 不同任务的最佳生成模型选择依赖具体目标：若追求高质量生成，可选扩散模型；若重建多遮挡区域，更建议自回归模型。研究为医学影像生成模型的选取与应用提供了系统性参考。

Abstract: In medical imaging, generative models are increasingly relied upon for two
distinct but equally critical tasks: reconstruction, where the goal is to
restore medical imaging (usually inverse problems like inpainting or
superresolution), and generation, where synthetic data is created to augment
datasets or carry out counterfactual analysis. Despite shared architecture and
learning frameworks, they prioritize different goals: generation seeks high
perceptual quality and diversity, while reconstruction focuses on data fidelity
and faithfulness. In this work, we introduce a "generative model zoo" and
systematically analyze how modern latent diffusion models and autoregressive
models navigate the reconstruction-generation spectrum. We benchmark a suite of
generative models across representative cardiac medical imaging tasks, focusing
on image inpainting with varying masking ratios and sampling strategies, as
well as unconditional image generation. Our findings show that diffusion models
offer superior perceptual quality for unconditional generation but tend to
hallucinate as masking ratios increase, whereas autoregressive models maintain
stable perceptual performance across masking levels, albeit with generally
lower fidelity.

</details>


### [74] [VisHall3D: Monocular Semantic Scene Completion from Reconstructing the Visible Regions to Hallucinating the Invisible Regions](https://arxiv.org/abs/2507.19188)
*Haoang Lu,Yuanqi Su,Xiaoning Zhang,Longjun Gao,Yu Xue,Le Wang*

Main category: cs.CV

TL;DR: VisHall3D提出了一种针对单目语义场景重建的创新性两阶段方法，分别重建可见区域和推断不可见区域，显著提升了重建质量，并在公开数据集上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有单目场景重建方法存在特征混杂和几何不一致的问题，影响了结果的准确性和鲁棒性。作者希望通过新的分阶段方案，提升精度，增强对自动驾驶等复杂应用场景的适用性。

Method: 方法分为两个阶段：第一阶段使用VisFrontierNet模块重建可见区域，精确描绘视觉边界并保持细节；第二阶段利用OcclusionMAE网络通过噪声机制对不可见区域进行合理论据推断，从而实现整体场景的完成。

Result: 在SemanticKITTI和SSCBench-KITTI-360两个具有挑战性的公开基准测试集上，VisHall3D获得了行业领先的准确率，明显优于同期方法。

Conclusion: 分阶段完成场景重建有效解决了特征混杂与几何不一致问题，显著提升了单目场景重建任务表现，为智能驾驶等领域提供了更为可靠和精确的三维环境理解方案。

Abstract: This paper introduces VisHall3D, a novel two-stage framework for monocular
semantic scene completion that aims to address the issues of feature
entanglement and geometric inconsistency prevalent in existing methods.
VisHall3D decomposes the scene completion task into two stages: reconstructing
the visible regions (vision) and inferring the invisible regions
(hallucination). In the first stage, VisFrontierNet, a visibility-aware
projection module, is introduced to accurately trace the visual frontier while
preserving fine-grained details. In the second stage, OcclusionMAE, a
hallucination network, is employed to generate plausible geometries for the
invisible regions using a noise injection mechanism. By decoupling scene
completion into these two distinct stages, VisHall3D effectively mitigates
feature entanglement and geometric inconsistency, leading to significantly
improved reconstruction quality.
  The effectiveness of VisHall3D is validated through extensive experiments on
two challenging benchmarks: SemanticKITTI and SSCBench-KITTI-360. VisHall3D
achieves state-of-the-art performance, outperforming previous methods by a
significant margin and paves the way for more accurate and reliable scene
understanding in autonomous driving and other applications.

</details>


### [75] [Joint Holistic and Lesion Controllable Mammogram Synthesis via Gated Conditional Diffusion Model](https://arxiv.org/abs/2507.19201)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的门控条件扩散模型（GCDM），用于合成包含病灶的高质量乳腺X光图像，改善现有方法对病灶特征强调不足的问题。实验表明GCDM在控制病灶区域和生成真实多样乳腺图像方面有明显提升。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光检查是乳腺癌筛查的主要手段，但深度学习方法受限于真实数据数量与病灶多样性的不足，影响模型的准确性和鲁棒性。生成对抗网络等生成模型虽然可以合成图像拓展数据集，但很难同时保证病灶特征的突出表现和与周围组织的解剖一致性。因此，需要一种能精确控制病灶生成且保证整体真实感的新方法。

Method: 作者提出了门控条件扩散模型（GCDM）。该框架以潜变量扩散去噪为基础，将受噪声干扰的潜在图像与表示乳腺、病灶及过渡区的软掩码嵌入进行拼接，以保证图像解剖结构的对应。为进一步强化病灶特征，GCDM引入了门控条件分支，动态选择融合病灶的放射学和几何信息，指导去噪过程聚焦病灶特征。

Result: 实验结果显示，GCDM在病灶区域的精准可控性、合成乳腺X光图像的真实度和多样性方面，均超过了现有的生成方法。

Conclusion: GCDM能够精确合成包含多样病灶的高质量乳腺X光图像，有望为医学影像分析和临床应用提供强有力的数据支持与工具。代码已公开。

Abstract: Mammography is the most commonly used imaging modality for breast cancer
screening, driving an increasing demand for deep-learning techniques to support
large-scale analysis. However, the development of accurate and robust methods
is often limited by insufficient data availability and a lack of diversity in
lesion characteristics. While generative models offer a promising solution for
data synthesis, current approaches often fail to adequately emphasize
lesion-specific features and their relationships with surrounding tissues. In
this paper, we propose Gated Conditional Diffusion Model (GCDM), a novel
framework designed to jointly synthesize holistic mammogram images and
localized lesions. GCDM is built upon a latent denoising diffusion framework,
where the noised latent image is concatenated with a soft mask embedding that
represents breast, lesion, and their transitional regions, ensuring anatomical
coherence between them during the denoising process. To further emphasize
lesion-specific features, GCDM incorporates a gated conditioning branch that
guides the denoising process by dynamically selecting and fusing the most
relevant radiomic and geometric properties of lesions, effectively capturing
their interplay. Experimental results demonstrate that GCDM achieves precise
control over small lesion areas while enhancing the realism and diversity of
synthesized mammograms. These advancements position GCDM as a promising tool
for clinical applications in mammogram synthesis. Our code is available at
https://github.com/lixinHUST/Gated-Conditional-Diffusion-Model/

</details>


### [76] [Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting with CounterNet](https://arxiv.org/abs/2507.19209)
*Xiaoyu Zhang,Zhifeng Bao,Hai Dong,Ziwei Wang,Jiajun Liu*

Main category: cs.CV

TL;DR: 该论文针对自动驾驶车辆产生的大规模点云数据，提出了一种名为CounterNet的新型计数网络，以提升对象计数的精确度，从而优化查询结果。


<details>
  <summary>Details</summary>
Motivation: 现有点云数据查询严重依赖对象计数的准确性，但主流3D检测模型在点云环境下的计数表现较差，导致查询失真。作者希望解决当前查询对象计数不准的问题，实现更精确的分析与决策。

Method: 作者提出CounterNet，一种基于热力图的点云计数网络，不注重物体位置的精确定位，而是通过检测物体中心以提高计数精度。方法还包括特征图重叠分区策略，提升对不同大小物体的适应性，并引入帧动态模型选择机制，根据输入帧选择最优模型配置。

Result: 在三个实际自动驾驶车辆点云数据集上评测，CounterNet在各对象类别计数准确率提升5%到20%，显著优于现有方法。所有三类查询类型的结果均因计数提升而更为可靠。

Conclusion: CounterNet能够有效提升大规模点云数据中的对象计数准确率，从而为自动驾驶场景下各类查询任务提供更准确的数据支撑，具有较高的实用价值。

Abstract: Autonomous vehicles generate massive volumes of point cloud data, yet only a
subset is relevant for specific tasks such as collision detection, traffic
analysis, or congestion monitoring. Effectively querying this data is essential
to enable targeted analytics. In this work, we formalize point cloud querying
by defining three core query types: RETRIEVAL, COUNT, and AGGREGATION, each
aligned with distinct analytical scenarios. All these queries rely heavily on
accurate object counts to produce meaningful results, making precise object
counting a critical component of query execution. Prior work has focused on
indexing techniques for 2D video data, assuming detection models provide
accurate counting information. However, when applied to 3D point cloud data,
state-of-the-art detection models often fail to generate reliable object
counts, leading to substantial errors in query results. To address this
limitation, we propose CounterNet, a heatmap-based network designed for
accurate object counting in large-scale point cloud data. Rather than focusing
on accurate object localization, CounterNet detects object presence by finding
object centers to improve counting accuracy. We further enhance its performance
with a feature map partitioning strategy using overlapping regions, enabling
better handling of both small and large objects in complex traffic scenes. To
adapt to varying frame characteristics, we introduce a per-frame dynamic model
selection strategy that selects the most effective configuration for each
input. Evaluations on three real-world autonomous vehicle datasets show that
CounterNet improves counting accuracy by 5% to 20% across object categories,
resulting in more reliable query outcomes across all supported query types.

</details>


### [77] [PRE-MAP: Personalized Reinforced Eye-tracking Multimodal LLM for High-Resolution Multi-Attribute Point Prediction](https://arxiv.org/abs/2507.19213)
*Hanbing Wu,Ping Jiang,Anyang Su,Chenxu Zhao,Tianyu Fu,Minghui Wu,Beiping Tan,Huiying Li*

Main category: cs.CV

TL;DR: 本论文提出了一个关注主观个体差异的广告视频视线跟踪大型数据集（SPA-ADV）和一个强化学习优化的个性化注视点预测方法（PRE-MAP），有效提升了个性化视觉关注预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的显著性预测模型和数据集忽略了人群主观认知多样性对注视行为的影响，只能产生泛化、不具个体特点的热力图，无法精准反映不同人的视觉关注点。此外，多模态大模型（MLLMs）预测点位存在格式和精度方面的困难。

Method: 1）构建了SPA-ADV数据集，包含4500多人在486个广告视频上的凝视轨迹及多属性用户资料；2）提出PRE-MAP模型，结合MLLM和多属性用户画像，利用强化学习（C-GRPO方法）优化点位预测，使得模型输出兼顾格式正确和空间精度，能够刻画个体视觉关注差异。

Result: 在SPA-ADV和其他基准数据集上，所提模型相比现有方法能更好地预测不同用户群体的个性化注视点。实验结果验证了方法的有效性。

Conclusion: 论文填补了显著性预测领域对个体主观注意差异建模的空白，通过大规模多模态数据集与新方法，提升了个性化视觉关注的理解和预测能力，具备实际应用潜力。

Abstract: Visual selective attention, driven by individual preferences, regulates human
prioritization of visual stimuli by bridging subjective cognitive mechanisms
with objective visual elements, thereby steering the semantic interpretation
and hierarchical processing of dynamic visual scenes. However, existing models
and datasets predominantly neglect the influence of subjective cognitive
diversity on fixation behavior. Conventional saliency prediction models,
typically employing segmentation approaches, rely on low-resolution imagery to
generate saliency heatmaps, subsequently upscaled to native resolutions, which
limiting their capacity to capture personalized attention patterns.
Furthermore, MLLMs are constrained by factors such as hallucinations, making it
very costly to strictly adhere to the expected format in tasks involving
multiple point predictions, and achieving precise point positioning is
challenging. To address these limitations, we present Subjective Personalized
Attention for Advertisement Videos, namely SPA-ADV, a large-scale multimodal
dataset capturing gaze behaviors from over 4,500 participants varying in age
and gender with 486 videos. Furthermore, we propose PRE-MAP, a novel
eye-tracking saliency model that characterizes Personalized visual disparities
through Reinforcement learning-optimized Eye-tracking, built upon MLLMs and
guided by Multi-Attribute user profiles to predict Points. To ensure MLLMs
produce prediction points that are both format-correct and spatially accurate,
we introduce Consistency Group Relative Policy Optimization (C-GRPO), inspired
by the variability in eye movement points and Multi-Attribute profiles.
Extensive experiments on SPA-ADV and other benchmarks demonstrate the
effectiveness of our approach. The code and dataset are available at
\href{https://github.com/mininglamp-MLLM/PRE-MAP}{this URL}.

</details>


### [78] [Unstable Prompts, Unreliable Segmentations: A Challenge for Longitudinal Lesion Analysis](https://arxiv.org/abs/2507.19230)
*Niels Rocholl,Ewoud Smit,Mathias Prokop,Alessa Hering*

Main category: cs.CV

TL;DR: 本文分析了ULS23通用病灶分割模型在纵向（多时点）CT扫描中的表现，发现其在实际应用中存在明显局限性，尤其是在跟踪同一病灶变化时。


<details>
  <summary>Details</summary>
Motivation: 纵向病灶分析对肿瘤临床护理至关重要，但目前自动化工具在时序一致性上存在障碍。现有的分割模型多服务于单一时点，尚缺乏对纵向数据的分析能力。

Method: 作者选择一个包含基线与随访CT的公开临床数据集，用ULS23模型进行分割实验。进一步，通过人为错位使病灶中心偏移，系统性验证模型对输入定位准确性的敏感性。

Result: 实验发现，随访时点的分割质量大幅下降，主要受多时点扫描配准误差影响，导致后续病灶对应追踪失败。模型假设输入病灶居中，实际偏移时分割性能急剧恶化。

Conclusion: 单时点分割模型并不适用于纵向肿瘤追踪。肿瘤纵向跟踪需采用设计上融合时序信息、端到端的综合模型，传统串联单点工具方案难以胜任。

Abstract: Longitudinal lesion analysis is crucial for oncological care, yet automated
tools often struggle with temporal consistency. While universal lesion
segmentation models have advanced, they are typically designed for single time
points. This paper investigates the performance of the ULS23 segmentation model
in a longitudinal context. Using a public clinical dataset of baseline and
follow-up CT scans, we evaluated the model's ability to segment and track
lesions over time. We identified two critical, interconnected failure modes: a
sharp degradation in segmentation quality in follow-up cases due to inter-scan
registration errors, and a subsequent breakdown of the lesion correspondence
process. To systematically probe this vulnerability, we conducted a controlled
experiment where we artificially displaced the input volume relative to the
true lesion center. Our results demonstrate that the model's performance is
highly dependent on its assumption of a centered lesion; segmentation accuracy
collapses when the lesion is sufficiently displaced. These findings reveal a
fundamental limitation of applying single-timepoint models to longitudinal
data. We conclude that robust oncological tracking requires a paradigm shift
away from cascading single-purpose tools towards integrated, end-to-end models
inherently designed for temporal analysis.

</details>


### [79] [Event-Driven Storytelling with Multiple Lifelike Humans in a 3D Scene](https://arxiv.org/abs/2507.19232)
*Donggeun Lim,Jinseok Bae,Inwoo Hwang,Seungmin Lee,Hwanhee Lee,Young Min Kim*

Main category: cs.CV

TL;DR: 作者提出了一个可以生成包含多个角色的动态虚拟场景的框架，能够处理多角色之间以及角色与场景的复杂交互。该方法利用大语言模型理解文本上下文，将场景生成任务拆解为一系列子任务，从而实现前所未有规模的多角色行为生成，并构建了公开基准，用于评估系统性能。


<details>
  <summary>Details</summary>
Motivation: 多人物虚拟动态场景创造涉及复杂的人物与人物、人物与场景间的关系与交互推理。现有方法在事件多样性和规模化生成方面有显著限制，难以灵活理解和处理多变上下文。作者希望借助大语言模型，将复杂场景中的语境拆解为易处理的子任务，突破这些局限。

Method: 提出的系统首先用一个事件生成器将动态场景的时间进程拆分为一系列小事件，每个事件对应具体角色和物体的动作与交互。随后，对应角色的动作通过空间指导采样位置合成。高层模块负责将事件转换为相对描述，并精确检索事件发生的位置坐标。整体流程融合大语言模型的文本理解能力和多模块协同。

Result: 该方法系统化、多规模地处理了复杂场景中的多角色上下文动作生成问题。通过基准测试与用户调研，作者发现系统能够高效、准确地理解并还原多角色动态场景的语境与动作，与现有方法相比，展示出了更高的可扩展性和表现力。

Conclusion: 本工作首次以此规模和多样性解决了多角色语境动作生成问题，并创建了对应的评测基准。实验和用户评价表明，提出的框架具备优秀的场景理解和生成能力，对推动虚拟动态场景技术的发展有重要意义。

Abstract: In this work, we propose a framework that creates a lively virtual dynamic
scene with contextual motions of multiple humans. Generating multi-human
contextual motion requires holistic reasoning over dynamic relationships among
human-human and human-scene interactions. We adapt the power of a large
language model (LLM) to digest the contextual complexity within textual input
and convert the task into tangible subproblems such that we can generate
multi-agent behavior beyond the scale that was not considered before.
Specifically, our event generator formulates the temporal progression of a
dynamic scene into a sequence of small events. Each event calls for a
well-defined motion involving relevant characters and objects. Next, we
synthesize the motions of characters at positions sampled based on spatial
guidance. We employ a high-level module to deliver scalable yet comprehensive
context, translating events into relative descriptions that enable the
retrieval of precise coordinates. As the first to address this problem at scale
and with diversity, we offer a benchmark to assess diverse aspects of
contextual reasoning. Benchmark results and user studies show that our
framework effectively captures scene context with high scalability. The code
and benchmark, along with result videos, are available at our project page:
https://rms0329.github.io/Event-Driven-Storytelling/.

</details>


### [80] [CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception](https://arxiv.org/abs/2507.19239)
*Jiaru Zhong,Jiahao Wang,Jiahui Xu,Xiaofan Li,Zaiqing Nie,Haibao Yu*

Main category: cs.CV

TL;DR: 本文提出了一种用于多车协同3D多目标跟踪的端到端方法CoopTrack，通过稀疏实例级特征的传递，实现了更强的协同感知与低通信消耗，显著提升跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 以往多车协同感知多聚焦于单帧任务，面对多帧跟踪等时序感知任务研究较少；现有方法难以实现高效、低耗的实例级协同跟踪。

Method: 提出CoopTrack框架，实现端到端、可学习的实例级关联和协同跟踪。该方法通过稀疏实例特征通信，包含多维特征提取与跨车关联聚合两个核心模块，能够动态融合语义和运动等特征，引入基于特征图的自适应跨车关联与融合机制。

Result: 在V2X-Seq和Griffin数据集上，CoopTrack达到了优异效果，尤其在V2X-Seq数据集上取得了39.0%的mAP和32.8%的AMOTA，刷新了现有最优水平。

Conclusion: CoopTrack在保持低通信成本的同时，显著提升了多车协同跟踪性能，为协同时序感知提供了有效方案。

Abstract: Cooperative perception aims to address the inherent limitations of
single-vehicle autonomous driving systems through information exchange among
multiple agents. Previous research has primarily focused on single-frame
perception tasks. However, the more challenging cooperative sequential
perception tasks, such as cooperative 3D multi-object tracking, have not been
thoroughly investigated. Therefore, we propose CoopTrack, a fully
instance-level end-to-end framework for cooperative tracking, featuring
learnable instance association, which fundamentally differs from existing
approaches. CoopTrack transmits sparse instance-level features that
significantly enhance perception capabilities while maintaining low
transmission costs. Furthermore, the framework comprises two key components:
Multi-Dimensional Feature Extraction, and Cross-Agent Association and
Aggregation, which collectively enable comprehensive instance representation
with semantic and motion features, and adaptive cross-agent association and
fusion based on a feature graph. Experiments on both the V2X-Seq and Griffin
datasets demonstrate that CoopTrack achieves excellent performance.
Specifically, it attains state-of-the-art results on V2X-Seq, with 39.0\% mAP
and 32.8\% AMOTA. The project is available at
https://github.com/zhongjiaru/CoopTrack.

</details>


### [81] [BridgeNet: A Unified Multimodal Framework for Bridging 2D and 3D Industrial Anomaly Detection](https://arxiv.org/abs/2507.19253)
*An Xiang,Zixuan Huang,Xitong Gao,Kejiang Ye,Cheng-zhong Xu*

Main category: cs.CV

TL;DR: 本文提出了一种多模态工业异常检测框架，通过解耦RGB外观信息和点云深度信息，简化了数据融合，增强了对三维异常的检测能力，并引入灵活的异常样本生成机制，在多个公开数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测方法多数仅用2D信息，难以有效表征和检测3D深度异常；而现有直接融合深度信息或用点云网络的方法也因模态差异和异常样本稀缺，无法在多模态场景下表现良好。因此，需要一个更有效的统一多模态检测框架与异常生成方法。

Method: 1）将3D点云中可见深度信息与2D RGB外观信息分离，分别表征深度和外观以支持异常生成；2）提出多尺度高斯异常生成器和统一纹理异常生成器，能够在RGB和深度域生成丰富的异常；3）所有模块在RGB和深度数据间共享参数，无需复杂融合操作，直接利用RGB和深度特征进行后续检测。

Result: 在MVTec-3D AD与Eyecandies等公开数据集上进行实验，所提方法性能优于最新SOTA方法，展示出在多模态异常检测任务上的强大能力。

Conclusion: 通过创新的模态解耦、异常生成器设计及参数共享，本文框架有效提升了工业场景下的多模态（三维+二维）异常检测水平，对实际工业检测具有重要意义。

Abstract: Industrial anomaly detection for 2D objects has gained significant attention
and achieved progress in anomaly detection (AD) methods. However, identifying
3D depth anomalies using only 2D information is insufficient. Despite
explicitly fusing depth information into RGB images or using point cloud
backbone networks to extract depth features, both approaches struggle to
adequately represent 3D information in multimodal scenarios due to the
disparities among different modal information. Additionally, due to the
scarcity of abnormal samples in industrial data, especially in multimodal
scenarios, it is necessary to perform anomaly generation to simulate real-world
abnormal samples. Therefore, we propose a novel unified multimodal anomaly
detection framework to address these issues. Our contributions consist of 3 key
aspects. (1) We extract visible depth information from 3D point cloud data
simply and use 2D RGB images to represent appearance, which disentangles depth
and appearance to support unified anomaly generation. (2) Benefiting from the
flexible input representation, the proposed Multi-Scale Gaussian Anomaly
Generator and Unified Texture Anomaly Generator can generate richer anomalies
in RGB and depth. (3) All modules share parameters for both RGB and depth data,
effectively bridging 2D and 3D anomaly detection. Subsequent modules can
directly leverage features from both modalities without complex fusion.
Experiments show our method outperforms state-of-the-art (SOTA) on MVTec-3D AD
and Eyecandies datasets. Code available at:
https://github.com/Xantastic/BridgeNet

</details>


### [82] [OVFact: Measuring and Improving Open-Vocabulary Factuality for Long Caption Models](https://arxiv.org/abs/2507.19262)
*Monika Wysoczańska,Shyamal Buch,Anurag Arnab,Cordelia Schmid*

Main category: cs.CV

TL;DR: 本文提出了一种新的长文本图像-语言描述准确性评价方法OV-Fact，无需人工标注，适合评测大模型生成的长图文描述的事实准确性。实验显示，OV-Fact不仅与人工判断更一致，还能提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉-语言模型在生成长且准确的描述时仍易产生事实错误（幻觉问题），而传统评价方法对更长、更复杂的描述或者缺乏人工真值的场景并不适用。因此急需新的、无需依赖人工标注的事实性评价方法。

Method: 作者提出了基于开放词汇视觉定位和工具辅助验证的评价方法OV-Fact。它不依赖人工标注，通过将描述中的实体与图像进行视觉对齐，实现自动化的事实一致性判断；同时衡量描述的召回率和事实精度。

Result: 相比现有指标，OV-Fact评价结果与人工标注更为一致，且不需参考答案。应用该指标对大规模噪声数据进行筛选后进行模型训练，结果表明，所选子集能大幅减少数据量（2.5-5倍减少）同时提升模型生成描述的事实精度，而且描述的丰富性未受影响。

Conclusion: OV-Fact是一种有效的长文本描述事实性评价工具，可以支持更精细的数据筛选，提升模型性能，特别适用于缺乏人工真值的实际场景。

Abstract: Large vision-language models (VLMs) often struggle to generate long and
factual captions. However, traditional measures for hallucination and
factuality are not well suited for evaluating longer, more diverse captions and
in settings where ground-truth human-annotated captions are unavailable. We
introduce OV-Fact, a novel method for measuring caption factuality of long
captions that leverages open-vocabulary visual grounding and tool-based
verification without depending on human annotations. Our method improves
agreement with human judgments and captures both caption descriptiveness
(recall) and factual precision in the same metric. Furthermore, unlike previous
metrics, our reference-free method design enables new applications towards
factuality-based data filtering. We observe models trained on an
OVFact-filtered (2.5-5x less) subset of a large-scale, noisy (VLM-generated)
pretraining set meaningfully improve factuality precision without sacrificing
caption descriptiveness across a range of downstream long caption benchmarks.

</details>


### [83] [SimMLM: A Simple Framework for Multi-modal Learning with Missing Modality](https://arxiv.org/abs/2507.19264)
*Sijie Li,Chen Chen,Jungong Han*

Main category: cs.CV

TL;DR: SimMLM提出了一种简单但有效的多模态学习框架，特别适用于缺失模态场景，无需复杂的架构或数据修补策略，依然能提升准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习常面临部分模态数据缺失的问题，现有方法多依赖复杂的模型或修补技术，难以适配多种实际缺失场景。因此，作者希望提出一个通用、简洁且效果更好的解决方案。

Method: 提出SimMLM架构，主打Dynamic Mixture of Modality Experts（DMoME），通过动态、可学习的门控机制自适应各模态贡献。同时创新性地提出MoFe（More vs. Fewer）排序损失，确保提供更多模态时任务准确度不会下降。

Result: 在多模态医学图像分割（BraTS 2018）和多模态分类（UPMC Food-101、avMNIST）任务中，SimMLM在完整和缺失模态测试下均优于现有方法，实现了更高准确率、可解释性、鲁棒性和可靠性。

Conclusion: SimMLM框架为应对多模态数据缺失提供了简单、有效且通用的解决思路，具有更强的实用性和适应性，适合在有缺失模态数据的问题中广泛应用。

Abstract: In this paper, we propose SimMLM, a simple yet powerful framework for
multimodal learning with missing modalities. Unlike existing approaches that
rely on sophisticated network architectures or complex data imputation
techniques, SimMLM provides a generic and effective solution that can adapt to
various missing modality scenarios with improved accuracy and robustness.
Specifically, SimMLM consists of a generic Dynamic Mixture of Modality Experts
(DMoME) architecture, featuring a dynamic, learnable gating mechanism that
automatically adjusts each modality's contribution in both full and partial
modality settings. A key innovation of SimMLM is the proposed More vs. Fewer
(MoFe) ranking loss, which ensures that task accuracy improves or remains
stable as more modalities are made available. This aligns the model with an
intuitive principle: removing one or more modalities should not increase
accuracy. We validate SimMLM on multimodal medical image segmentation (BraTS
2018) and multimodal classification (UPMC Food-101, avMNIST) tasks, where it
consistently surpasses competitive methods, demonstrating superior accuracy,
interpretability, robustness, and reliability across both complete and missing
modality scenarios at test time.

</details>


### [84] [Video Self-Distillation for Single-Image Encoders: A Step Toward Physically Plausible Perception](https://arxiv.org/abs/2507.19272)
*Marcel Simon,Tae-Ho Kim,Seul-Ki Yeom*

Main category: cs.CV

TL;DR: 本文提出了一种利用视频信息改进自监督图像编码器的方法，通过预测下一帧的特征实现空间和时间信息的融合，无需光流或跟踪，显著提升下游分割任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督视觉编码器主要依赖静态图像进行训练，无法有效捕获视频中的时序线索，从而限制了模型对真实世界三维知识的学习能力。

Method: 提出了一种新的训练目标：让单帧图像编码器预测下一帧的特征表示，从视频中自监督学习时空先验。训练时无须光流或跟踪算法，只需自回归地利用视频帧。

Result: 使用该方法在仅有2小时视频的自监督预训练下，将ADE20K语义分割任务的mIoU从35.0提升至36.4，且无需改变下游任务流程。

Conclusion: 视频自蒸馏为引入几何与时序知识提供了一种高效方式，大幅增强了图像编码器的空间认知能力，有助于实现物理可行的世界建模与物理智能（Physical AI）。

Abstract: Self-supervised image encoders such as DINO have recently gained significant
interest for learning robust visual features without labels. However, most SSL
methods train on static images and miss the temporal cues inherent in videos.
We introduce a video-distilled single-image encoder trained to predict the
next-frame representation from the current frame. This simple objective injects
3D spatial and temporal priors without optical flow or tracking. When
pre-training on a single 2-hour video, our approach raises the mean
Intersection-over-Union (mIoU) on ADE20K from 35.0 (DoRA) to 36.4 while
remaining a drop-in replacement for image-only pipelines. Our results highlight
video self-distillation as a lightweight route to geometry-aware perception an
essential ingredient for physically plausible world models and Physical AI.

</details>


### [85] [RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow](https://arxiv.org/abs/2507.19280)
*Liang Yao,Fan Liu,Hongbo Lu,Chuanyi Zhang,Rui Min,Shengxiang Xu,Shimin Di,Pai Peng*

Main category: cs.CV

TL;DR: 本文提出了RemoteReasoner，一个应用于遥感影像复杂推理任务的灵活鲁棒工作流，结合多模态大语言模型和强化学习，实现对不同推理需求的自主适应，并支持多样输出格式。


<details>
  <summary>Details</summary>
Motivation: 遥感影像数据庞大且结构复杂，传统方法主要关注识别任务，难以理解用户复杂意图和空间上下文关系，且依赖监督学习限制推理的自主性，无法应对多样化和复杂的推理需求。

Method: 提出RemoteReasoner工作流，融合多模态大语言模型（MLLM），用于理解用户指令和目标定位，并设计任务适配策略，实现多粒度输出。与现有需要特定任务解码器和微调的方法不同，本方法通过强化学习训练模型，提高自主推理能力，并在推理阶段实现灵活输出，无需再训练。

Result: 初步实验显示，RemoteReasoner在区域级、像素级等多粒度推理任务中取得了优异表现，并首次实现了等高线提取等现有方法无法完成的新能力。

Conclusion: RemoteReasoner能够灵活应对遥感影像领域多样复杂的推理任务，具有较高的自主性与适应性，在同类任务中展现出领先性能，同时为遥感推理带来了创新能力。

Abstract: Remote sensing imagery presents vast, inherently unstructured spatial data,
demanding sophisticated reasoning to interpret complex user intents and
contextual relationships beyond simple recognition tasks. In this paper, we aim
to construct an Earth observation workflow to handle complex queries by
reasoning about spatial context and user intent. As a reasoning workflow, it
should be somewhat autonomous, where predefined ground-truth reasoning paths do
not constrain the learning process. Furthermore, its architecture ought to be
unified yet flexible, enabling the model to perform diverse reasoning tasks
with distinct output formats through a single forward pass. Existing remote
sensing approaches fail to address these requirements, as they rely on
supervised fine-tuning paradigms that constrain the autonomy of reasoning. To
this end, we propose RemoteReasoner, a flexible and robust workflow for remote
sensing reasoning tasks. The design of RemoteReasoner integrates a multi-modal
large language model (MLLM) for interpreting user instructions and localizing
targets, together with task adaptation strategies that enable multi-granularity
output generation. In contrast to existing methods, our framework is trained
with reinforcement learning (RL) to endow the MLLM sufficient autonomy for
precise reasoning. At the inference stage, our adaptation strategies enable
diverse output formats at inference time without requiring task-specific
decoders or further fine-tuning. Preliminary experiments demonstrated that
RemoteReasoner achieves remarkable performance across multi-granularity
reasoning tasks, including region-level and pixel-level. Additionally, our
framework enables novel capabilities such as the contour extraction task beyond
the reach of existing reasoning pipelines.

</details>


### [86] [PINO: Person-Interaction Noise Optimization for Long-Duration and Customizable Motion Generation of Arbitrary-Sized Groups](https://arxiv.org/abs/2507.19292)
*Sakuya Ota,Qing Yu,Kent Fujiwara,Satoshi Ikehata,Ikuro Sato*

Main category: cs.CV

TL;DR: PINO提出了一种新的、无需训练的多角色交互动作生成方法，通过逐渐组合两人交互并优化物理可行性，提升了生成的真实性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有多角色动作生成方法受限于单一提示和复杂度，导致交互缺乏细腻和真实性，难以灵活控制和避免物理穿插等问题。

Method: PINO框架将复杂多角色交互动作分解为语义相关的两人交互，并利用已训练好的两人扩散模型递归叠加生成，同时在噪声优化过程中引入物理惩罚项，避免角色间重叠、穿透等失真现象，实现无需重新训练即可精准控制角色的空间位置、朝向和行为速度等。

Result: 实验结果显示，PINO能生成视觉真实、物理合理且可定制化的多人物交互动作，超越现有方法，适用于动画、游戏和机器人等多个领域。

Conclusion: PINO有效提升了多角色合成动作的真实性、可控性和多样性，不需额外训练，推动了多场景虚拟群体生成的实用进展。

Abstract: Generating realistic group interactions involving multiple characters remains
challenging due to increasing complexity as group size expands. While existing
conditional diffusion models incrementally generate motions by conditioning on
previously generated characters, they rely on single shared prompts, limiting
nuanced control and leading to overly simplified interactions. In this paper,
we introduce Person-Interaction Noise Optimization (PINO), a novel,
training-free framework designed for generating realistic and customizable
interactions among groups of arbitrary size. PINO decomposes complex group
interactions into semantically relevant pairwise interactions, and leverages
pretrained two-person interaction diffusion models to incrementally compose
group interactions. To ensure physical plausibility and avoid common artifacts
such as overlapping or penetration between characters, PINO employs
physics-based penalties during noise optimization. This approach allows precise
user control over character orientation, speed, and spatial relationships
without additional training. Comprehensive evaluations demonstrate that PINO
generates visually realistic, physically coherent, and adaptable multi-person
interactions suitable for diverse animation, gaming, and robotics applications.

</details>


### [87] [ABCD: Automatic Blood Cell Detection via Attention-Guided Improved YOLOX](https://arxiv.org/abs/2507.19296)
*Ahmed Endris Hasen,Yang Shangming,Chiagoziem C. Ukwuoma,Biniyam Gashaw,Abel Zenebe Yutra*

Main category: cs.CV

TL;DR: 本文提出了一种基于改进版YOLOX的自动血细胞检测方法（ABCD），能够高效地在显微图像中检测多种血细胞，并在BCCD数据集上实现了比现有方法更高的准确率和速度。


<details>
  <summary>Details</summary>
Motivation: 手工血细胞检测过程耗时、效率低且易出错，难以满足医学诊断对效率和准确性的高要求。通过深度学习进行自动化检测成为改进这一过程的可行方案。

Method: 本研究基于YOLOX目标检测器，提出了ABCD方法。具体改进包括：在网络主干中引入了Convolutional Block Attention Module（CBAM）以提升特征提取效果；在网络neck部分加入了Adaptively Spatial Feature Fusion（ASFF）用于优化不同阶段特征的融合；并将原有的IOU损失函数替换为CIOU损失函数以加快模型收敛速度。

Result: 在BCCD数据集上，ABCD方法的mAP@0.5达到95.49%，mAP@0.5-0.9达到86.89%，分别比基线算法高2.8%和23.41%，并且检测速度提升2.9%。实验结果表明其优于其他现有方法。

Conclusion: 改进的ABCD模型不仅显著提升了血细胞检测的准确性和速度，还具备用于实时医学图像分析的潜力。

Abstract: Detection of blood cells in microscopic images has become a major focus of
medical image analysis, playing a crucial role in gaining valuable insights
into a patient's health. Manual blood cell checks for disease detection are
known to be time-consuming, inefficient, and error-prone. To address these
limitations, analyzing blood cells using deep learning-based object detectors
can be regarded as a feasible solution. In this study, we propose automatic
blood cell detection method (ABCD) based on an improved version of YOLOX, an
object detector, for detecting various types of blood cells, including white
blood cells, red blood cells, and platelets. Firstly, we introduce the
Convolutional Block Attention Module (CBAM) into the network's backbone to
enhance the efficiency of feature extraction. Furthermore, we introduce the
Adaptively Spatial Feature Fusion (ASFF) into the network's neck, which
optimizes the fusion of different features extracted from various stages of the
network. Finally, to speed up the model's convergence, we substitute the
Intersection over Union (IOU) loss function with the Complete Intersection over
Union (CIOU) loss function. The experimental results demonstrate that the
proposed method is more effective than other existing methods for BCCD dataset.
Compared to the baseline algorithm, our method ABCD achieved 95.49 % mAP@0.5
and 86.89 % mAP@0.5-0.9, which are 2.8% and 23.41% higher, respectively, and
increased the detection speed by 2.9%, making it highly efficient for real-time
applications.

</details>


### [88] [Multistream Network for LiDAR and Camera-based 3D Object Detection in Outdoor Scenes](https://arxiv.org/abs/2507.19304)
*Muhammad Ibrahim,Naveed Akhtar,Haitian Wang,Saeed Anwar,Ajmal Mian*

Main category: cs.CV

TL;DR: 本论文提出了一种多流检测网络（MuStD），通过融合LiDAR和RGB数据，提升了户外3D目标检测的精度，并在KITTI基准测试中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 当前户外3D目标检测中，融合LiDAR与RGB信息有望提升检测精度，但如何高效整合这两种模态仍是挑战。因此，本文旨在解决多模态信息融合以实现更准确3D目标检测的问题。

Method: 提出MuStD多流检测网络，包括三个主干分支：LiDAR-PillarNet分支提取LiDAR点云的稀疏2D特征，LiDAR-Height Compression分支生成俯视图特征，3D多模态分支通过UV映射和极坐标索引深度结合RGB和LiDAR特征，最终综合空间、纹理及几何信息并用于检测。

Result: 在KITTI 3D目标检测基准测试中，通过公开服务器评测，本文方法在多个类别下取得了最新最优或极具竞争力的检测精度，同时保持了高效率。

Conclusion: MuStD网络有效融合了LiDAR与RGB数据，显著提升了3D目标检测性能，为多模态室外目标检测任务提供了新方案。

Abstract: Fusion of LiDAR and RGB data has the potential to enhance outdoor 3D object
detection accuracy. To address real-world challenges in outdoor 3D object
detection, fusion of LiDAR and RGB input has started gaining traction. However,
effective integration of these modalities for precise object detection task
still remains a largely open problem. To address that, we propose a MultiStream
Detection (MuStD) network, that meticulously extracts task-relevant information
from both data modalities. The network follows a three-stream structure. Its
LiDAR-PillarNet stream extracts sparse 2D pillar features from the LiDAR input
while the LiDAR-Height Compression stream computes Bird's-Eye View features. An
additional 3D Multimodal stream combines RGB and LiDAR features using UV
mapping and polar coordinate indexing. Eventually, the features containing
comprehensive spatial, textural and geometric information are carefully fused
and fed to a detection head for 3D object detection. Our extensive evaluation
on the challenging KITTI Object Detection Benchmark using public testing server
at
https://www.cvlibs.net/datasets/kitti/eval_object_detail.php?&result=d162ec699d6992040e34314d19ab7f5c217075e0
establishes the efficacy of our method by achieving new state-of-the-art or
highly competitive results in different categories while remaining among the
most efficient methods. Our code will be released through MuStD GitHub
repository at https://github.com/IbrahimUWA/MuStD.git

</details>


### [89] [SIDE: Sparse Information Disentanglement for Explainable Artificial Intelligence](https://arxiv.org/abs/2507.19321)
*Viktar Dubovik,Łukasz Struski,Jacek Tabor,Dawid Rymarczyk*

Main category: cs.CV

TL;DR: 本文提出了一种新的原型部分解释方法SIDE，通过对原型稀疏性约束和新的训练及剪枝方式，大幅提升了深度神经网络在计算机视觉中的可解释性，并在保证准确率的同时，将解释尺寸缩小了90%以上。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽然在医学成像、自动驾驶等高风险领域表现优异，但其决策过程黑盒化，缺乏透明性。原型部分解释网络为现有方案之一，但其可扩展性和解释简明性有待提升。现有方法如InfoDisent能扩展到大规模数据集，但生成的解释复杂，不易理解。因此迫切需要一种模型，能兼顾大规模适用性和高质、易懂的解释输出。

Method: 作者提出了SIDE方法。在训练阶段引入了专门的稀疏性约束和剪枝机制，使每一类别仅关联很少的原型部分。同时，用sigmoid激活函数替代常规softmax，以进一步强化稀疏性。该方法在原型-部分网络基础上，实现了解释可控地压缩且更清晰。

Result: 通过大量实验，SIDE方法在准确率上达到现有主流模型水平，且能将每类解释中涉及的原型数量减少90%以上，使得输出解释更加简洁明了。

Conclusion: SIDE显著加强了原型解释在深度神经网络中的可用性和实际可解释性，有望推动其在医疗和自动驾驶等对透明度要求极高领域的应用和部署。

Abstract: Understanding the decisions made by deep neural networks is essential in
high-stakes domains such as medical imaging and autonomous driving. Yet, these
models often lack transparency, particularly in computer vision.
Prototypical-parts-based neural networks have emerged as a promising solution
by offering concept-level explanations. However, most are limited to
fine-grained classification tasks, with few exceptions such as InfoDisent.
InfoDisent extends prototypical models to large-scale datasets like ImageNet,
but produces complex explanations.
  We introduce Sparse Information Disentanglement for Explainability (SIDE), a
novel method that improves the interpretability of prototypical parts through a
dedicated training and pruning scheme that enforces sparsity. Combined with
sigmoid activations in place of softmax, this approach allows SIDE to associate
each class with only a small set of relevant prototypes. Extensive experiments
show that SIDE matches the accuracy of existing methods while reducing
explanation size by over $90\%$, substantially enhancing the understandability
of prototype-based explanations.

</details>


### [90] [NerT-CA: Efficient Dynamic Reconstruction from Sparse-view X-ray Coronary Angiography](https://arxiv.org/abs/2507.19328)
*Kirsten W. H. Maas,Danny Ruijters,Nicola Pezzotti,Anna Vilanova*

Main category: cs.CV

TL;DR: 本文提出一种结合神经与张量表示的混合方法NerT-CA，用于加速稀疏视角下的4D冠状动脉重建，在训练速度和重建精度方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于X线冠状动脉造影的三维/四维（3D/4D）重建面临血管稀疏、背景/血管区分困难、稀疏视角及扫描内运动等诸多挑战。主流方法依赖繁琐手工或易错自动分割，实际临床应用受限。近期基于NeRF的自动稀疏视角重建有所突破，但训练速度慢，限制其广泛应用。

Method: 提出NerT-CA方法，将神经场与张量场结合。具体为：静态部分用张量分解实现低秩重建，动态稀疏部分用神经场建模，有效利用不同的数据特性。整体建模视景为低秩与稀疏成分的叠加，大幅提升计算效率和重建质量。

Result: 该方法在训练时间和重建精度上均超过以往工作，能够在极少的三个视角下实现精确的4D冠状动脉成像。通过4D仿体数据集进行了定量和定性验证，展现优异表现。

Conclusion: NerT-CA为X线冠状动脉造影稀疏视角下的4D重建带来更快、更准的解决方案，提高临床应用的可行性，推动自动医疗影像重建的发展。

Abstract: Three-dimensional (3D) and dynamic 3D+time (4D) reconstruction of coronary
arteries from X-ray coronary angiography (CA) has the potential to improve
clinical procedures. However, there are multiple challenges to be addressed,
most notably, blood-vessel structure sparsity, poor background and blood vessel
distinction, sparse-views, and intra-scan motion. State-of-the-art
reconstruction approaches rely on time-consuming manual or error-prone
automatic segmentations, limiting clinical usability. Recently, approaches
based on Neural Radiance Fields (NeRF) have shown promise for automatic
reconstructions in the sparse-view setting. However, they suffer from long
training times due to their dependence on MLP-based representations. We propose
NerT-CA, a hybrid approach of Neural and Tensorial representations for
accelerated 4D reconstructions with sparse-view CA. Building on top of the
previous NeRF-based work, we model the CA scene as a decomposition of low-rank
and sparse components, utilizing fast tensorial fields for low-rank static
reconstruction and neural fields for dynamic sparse reconstruction. Our
approach outperforms previous works in both training time and reconstruction
accuracy, yielding reasonable reconstructions from as few as three angiogram
views. We validate our approach quantitatively and qualitatively on
representative 4D phantom datasets.

</details>


### [91] [SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning](https://arxiv.org/abs/2507.19359)
*Lanmiao Liu,Esam Ghaleb,Aslı Özyürek,Zerrin Yumak*

Main category: cs.CV

TL;DR: 本文提出了一种能自动生成与语音语义高度一致的虚拟化身手势方法，在多项评测和用户研究中均优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前大多数手势生成研究主要关注节奏型击打手势，往往忽视了手势的语义上下文，对生成与语音内容匹配的语义手势存在挑战。为提升虚拟化身在对话场景下的表现力和自然度，有必要更好地结合语音和语义信息来生成语义相关的手势。

Method: 方法共两阶段：首先，通过矢量量化变分自编码器（VQ-VAE）学习手势运动的先验分布；其次，利用第二阶段模块根据语音、文本语义和说话人信息自动生成手势，通过语义连贯和相关性模块确保生成手势与语音语义高度一致。此设计综合了细粒度与全局层次上的语义信息。

Result: 实验表明该方法在生成语义相关手势的真实感和连贯性方面有显著提升。大规模实验与用户研究显示，无论在客观指标还是主观评价上，该方案在两个主流co-speech gesture benchmark上均超过最先进方法。

Conclusion: 本文提出的方法能有效提升虚拟化身在共语手势生成任务中的语义一致性与自然度，为语音驱动手势生成领域提供了新思路与工具，并有望在虚拟角色、智能助手等应用中发挥作用。

Abstract: Creating a virtual avatar with semantically coherent gestures that are
aligned with speech is a challenging task. Existing gesture generation research
mainly focused on generating rhythmic beat gestures, neglecting the semantic
context of the gestures. In this paper, we propose a novel approach for
semantic grounding in co-speech gesture generation that integrates semantic
information at both fine-grained and global levels. Our approach starts with
learning the motion prior through a vector-quantized variational autoencoder.
Built on this model, a second-stage module is applied to automatically generate
gestures from speech, text-based semantics and speaker identity that ensures
consistency between the semantic relevance of generated gestures and
co-occurring speech semantics through semantic coherence and relevance modules.
Experimental results demonstrate that our approach enhances the realism and
coherence of semantic gestures. Extensive experiments and user studies show
that our method outperforms state-of-the-art approaches across two benchmarks
in co-speech gesture generation in both objective and subjective metrics. The
qualitative results of our model, code, dataset and pre-trained models can be
viewed at https://semgesture.github.io/.

</details>


### [92] [EA-ViT: Efficient Adaptation for Elastic Vision Transformer](https://arxiv.org/abs/2507.19360)
*Chen Zhu,Wangbo Zhao,Huiwen Zhang,Samir Khaki,Yuhao Zhou,Weidong Tang,Shuo Wang,Zhihang Yuan,Yuzhang Shang,Xiaojiang Peng,Kai Wang,Dawei Yang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的Vision Transformer（ViT）适配框架EA-ViT，可通过一次适配流程生成适应不同资源约束的多个模型，无需对每种模型尺寸单独训练，大幅提升部署效率。


<details>
  <summary>Details</summary>
Motivation: 传统ViT模型部署到不同资源平台时，需为每种资源条件单独训练对应尺寸的ViT模型，过程耗时且能耗高。研究者希望找到一种适合不同平台自动高效适配的方法。

Method: 本文分为两个阶段：（1）基于已有ViT模型引入嵌套弹性架构，使MLP扩展比、注意力头数、嵌入维度和深度均可灵活调整，并采用课程式训练逐步提升弹性以保持稳定适配和知识迁移；（2）设计轻量化路由器，基于进化算法（NSGA-II）挑选Pareto最优子模型，实现在不同预算和任务需求下选取合适模型，并与主干网络联合优化。

Result: 通过在多个视觉任务基准上的大量实验，验证了EA-ViT具备良好的适应性和有效性，在维持性能的同时实现高效多模型部署。

Conclusion: EA-ViT能显著简化ViT模型多尺寸部署过程，既提升了效率又降低了能耗，为资源受限平台的视觉任务提供了具有实用价值的新方案。

Abstract: Vision Transformers (ViTs) have emerged as a foundational model in computer
vision, excelling in generalization and adaptation to downstream tasks.
However, deploying ViTs to support diverse resource constraints typically
requires retraining multiple, size-specific ViTs, which is both time-consuming
and energy-intensive. To address this issue, we propose an efficient ViT
adaptation framework that enables a single adaptation process to generate
multiple models of varying sizes for deployment on platforms with various
resource constraints. Our approach comprises two stages. In the first stage, we
enhance a pre-trained ViT with a nested elastic architecture that enables
structural flexibility across MLP expansion ratio, number of attention heads,
embedding dimension, and network depth. To preserve pre-trained knowledge and
ensure stable adaptation, we adopt a curriculum-based training strategy that
progressively increases elasticity. In the second stage, we design a
lightweight router to select submodels according to computational budgets and
downstream task demands. Initialized with Pareto-optimal configurations derived
via a customized NSGA-II algorithm, the router is then jointly optimized with
the backbone. Extensive experiments on multiple benchmarks demonstrate the
effectiveness and versatility of EA-ViT. The code is available at
https://github.com/zcxcf/EA-ViT.

</details>


### [93] [BEV-LLM: Leveraging Multimodal BEV Maps for Scene Captioning in Autonomous Driving](https://arxiv.org/abs/2507.19370)
*Felix Brandstaetter,Erik Schuetz,Katharina Winter,Fabian Flohr*

Main category: cs.CV

TL;DR: 提出了一种轻量级自动驾驶场景3D描述模型BEV-LLM，实现了高性能和更强的解释性，并发布了两个新的相关数据集。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要可解释、透明的决策系统，而自然语言场景描述（scene captioning）有助于提升系统的透明性、安全性和人机交互。但现有方案在效率、准确性及评测基准上存在不足，亟需改进。

Method: 提出了BEV-LLM模型，通过BEVFusion将3D激光雷达点云与多视角图像融合，并采用新颖的绝对位置编码以实现特定视角下的场景描述。模型基础参数仅1B，设计轻量化。

Result: 在nuCaption数据集上，BEV-LLM性能优异，BLEU分数领先当前SOTA方法约5%。同时，发布了nuView和GroundView两个新数据集，便于多种驾驶场景和要素的评测，并初步展示了这两个数据集的有效性。

Conclusion: BEV-LLM在提升自动驾驶场景描述性能的同时，增强了模型的可解释性和透明度，为自动驾驶AI的发展做出了贡献。新数据集也丰富了评估手段，为后续研究提供基础。

Abstract: Autonomous driving technology has the potential to transform transportation,
but its wide adoption depends on the development of interpretable and
transparent decision-making systems. Scene captioning, which generates natural
language descriptions of the driving environment, plays a crucial role in
enhancing transparency, safety, and human-AI interaction. We introduce BEV-LLM,
a lightweight model for 3D captioning of autonomous driving scenes. BEV-LLM
leverages BEVFusion to combine 3D LiDAR point clouds and multi-view images,
incorporating a novel absolute positional encoding for view-specific scene
descriptions. Despite using a small 1B parameter base model, BEV-LLM achieves
competitive performance on the nuCaption dataset, surpassing state-of-the-art
by up to 5\% in BLEU scores. Additionally, we release two new datasets - nuView
(focused on environmental conditions and viewpoints) and GroundView (focused on
object grounding) - to better assess scene captioning across diverse driving
scenarios and address gaps in current benchmarks, along with initial
benchmarking results demonstrating their effectiveness.

</details>


### [94] [CXR-CML: Improved zero-shot classification of long-tailed multi-label diseases in Chest X-Rays](https://arxiv.org/abs/2507.19398)
*Rajesh Madhipati,Sheethal Bhat,Lukas Buess,Andreas Maier*

Main category: cs.CV

TL;DR: 本文提出一种改进视觉-语言模型（如CLIP）在胸部X光片稀有类别识别能力的方法，通过处理长尾类别分布，提升了零样本分类准确率。方法基于对潜在空间分布的加权与聚类，有效提高了分类性能，特别是在罕见疾病方面。


<details>
  <summary>Details</summary>
Motivation: 当前自监督深度学习模型受到胸片数据类别高度不平衡影响，难以准确识别长尾类别；虽然视觉-语言模型（如CLIP）整体性能优异，但对稀有类别识别能力显著下降。作者希望解决长尾分布下的稀有类别识别瓶颈。

Method: 提出一种基于类别权重机制的方法，使类别分布在潜在空间中直接对齐。核心流程为：使用高斯混合模型（GMM）对潜在空间中的特征聚类，再用Student t分布进一步精细化聚类，最后用指标损失（metric loss）训练优化新的特征嵌入，实现稳定自适应聚类。

Result: 在MIMIC-CXR-JPG数据集40个类别上，与此前SOTA方法相比，提出方法平均提升了7个百分点的零样本AUC得分，尤其在罕见类别上准确率提升显著。

Conclusion: 通过对潜在空间的类别分布进行调整和细粒度建模，本文的方法有效缓解了长尾类别识别难题，显著提升了胸片多类别诊断的整体和稀有类别的准确率。

Abstract: Chest radiography (CXR) plays a crucial role in the diagnosis of various
diseases. However, the inherent class imbalance in the distribution of clinical
findings presents a significant challenge for current self-supervised deep
learning models. These models often fail to accurately classify long-tailed
classes. Current Vision-Language models such as Contrastive Language Image
Pre-training (CLIP) models effectively model the manifold distribution of the
latent space, enabling high zero-shot classification accuracies. Although CLIP
performs well on most of the primary classes in the dataset, our work reveals
that its effectiveness decreases significantly for classes with a long-tailed
distribution. Our approach employs a class-weighting mechanism that directly
aligns with the distribution of classes within the latent space. This method
ensures a substantial improvement in overall classification performance, with
particular emphasis on enhancing the recognition and accuracy of rarely
observed classes. We accomplish this by applying Gaussian Mixture Model (GMM)
clustering to the latent space. The subsequent clusters are further refined by
Student t-distribution, followed by a metric loss that utilizes the altered
embeddings. Our approach facilitates stable and adaptive clustering of the
features. This results in a notable average improvement of 7\% points in
zero-shot AUC scores across 40 classes in the MIMIC-CXR-JPG dataset from
previous SOTA models.

</details>


### [95] [Modality Agnostic Efficient Long Range Encoder](https://arxiv.org/abs/2507.19409)
*Toufiq Parag,Ahmed Elgammal*

Main category: cs.CV

TL;DR: 本文提出了一种高效适用于多模态长序列编码的新型Transformer架构MAELRE，通过逐步合并tokens与注意力近似，显著降低计算和内存消耗，并在多种类型数据分类任务上有效提升了准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 现有大模型虽然能处理百万级token长序列，但核心注意力机制的二次方复杂度难以在单设备单独解决，且现有token合并或注意力修改方法常常针对特定模态，难以兼顾效率与准确。

Method: 提出MAELRE架构，结合逐步token合并与轻量注意力近似，序列长时用高效近似注意力，随着token数减少再切换回标准注意力，实现跨模态、泛化性强的长距离编码。

Result: 在文本、时间序列、音频及视觉等多模态分类任务上，MAELRE在保持甚至提升准确率的同时，显著降低了计算消耗，优于现有通用长上下文模型。

Conclusion: MAELRE实现了高效、统一的多模态长上下文处理，为低资源设备上的Transformer应用提供了新方案，在准确率和效率间取得了更优折中。

Abstract: The long-context capability of recent large transformer models can be
surmised to rely on techniques such as attention/model parallelism, as well as
hardware-level optimizations. While these strategies allow input lengths to
scale to millions of tokens, they do not fundamentally mitigate the quadratic
computational and memory complexity of the core attention mechanism. In this
paper, we address the challenge of long-context processing on a single device
using generic implementations by reducing the quadratic memory footprint and
inference cost. Existing approaches to extend the context length for generic
single device implementations -- such as token merging and modified attentions
-- are often modality specific and attain a suboptimal tradeoff between
accuracy and efficiency. To overcome these limitations, we propose MAELRE
(Modality Agnostic Efficient Long Range Encoder), a unified and efficient
transformer architecture designed for long-range encoding across diverse
modalities. MAELRE integrates token merging with attention approximation,
progressively merging tokens at different stages of internal computational
blocks. It employs a lightweight attention approximation when the number of
tokens is large, and switches to standard dot-product attention as the sequence
becomes shorter through successive aggregation. We demonstrate that MAELRE
achieves superior accuracy while reducing computational cost compared to
existing long-context models on classification tasks spanning multiple
modalities, including text, time series, audio, and vision.

</details>


### [96] [DEFNet: Multitasks-based Deep Evidential Fusion Network for Blind Image Quality Assessment](https://arxiv.org/abs/2507.19418)
*Yiwei Lou,Yuanpeng He,Rongchao Zhang,Yongzhi Cao,Hanpin Wang,Yu Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新的无参考图像质量评估（BIQA）多任务深度网络（DEFNet），通过多任务联合优化和不确定性建模，有效提升对各种失真类型和场景的适应性及评估准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的BIQA方法在辅助任务利用和不确定性建模方面存在集成不足，导致性能受限，缺乏对新场景和复杂失真的泛化能力。

Method: 作者提出DEFNet网络，引入场景和失真类型分类作为辅助任务进行多任务优化；设计可信信息融合策略，结合子区域多样特征，平衡局部与全局信息，并采用基于证据推理的不确定性估计方法（利用正态逆伽马分布混合实现）。

Result: 在合成和真实失真数据集上，大量实验表明DEFNet框架具有优越的有效性、稳健性，并展现了较强的泛化能力。

Conclusion: DEFNet能更好地评估图像质量，在未知或复杂场景下表现出更强的适应性和可靠性，为BIQA任务提供了新的有效解决方案。

Abstract: Blind image quality assessment (BIQA) methods often incorporate auxiliary
tasks to improve performance. However, existing approaches face limitations due
to insufficient integration and a lack of flexible uncertainty estimation,
leading to suboptimal performance. To address these challenges, we propose a
multitasks-based Deep Evidential Fusion Network (DEFNet) for BIQA, which
performs multitask optimization with the assistance of scene and distortion
type classification tasks. To achieve a more robust and reliable
representation, we design a novel trustworthy information fusion strategy. It
first combines diverse features and patterns across sub-regions to enhance
information richness, and then performs local-global information fusion by
balancing fine-grained details with coarse-grained context. Moreover, DEFNet
exploits advanced uncertainty estimation technique inspired by evidential
learning with the help of normal-inverse gamma distribution mixture. Extensive
experiments on both synthetic and authentic distortion datasets demonstrate the
effectiveness and robustness of the proposed framework. Additional evaluation
and analysis are carried out to highlight its strong generalization capability
and adaptability to previously unseen scenarios.

</details>


### [97] [CircuitProbe: Dissecting Spatiotemporal Visual Semantics with Circuit Tracing](https://arxiv.org/abs/2507.19420)
*Yiming Zhang,Chengzhang Yu,Zhuokai Zhao,Kun Wang,Qiankun Li,Zihan Chen,Yang Liu,Zenghui Ding,Yining Sun*

Main category: cs.CV

TL;DR: 本文提出了一套系统的方法，分析大规模视觉-语言模型（LVLMs）内部如何处理时空视觉语义，发现对象token对于性能影响极大，并揭示中后层对时空语义的专门分工。


<details>
  <summary>Details</summary>
Motivation: 目前对LVLMs的语言、图片理解机制已有较多研究，但对其时空理解能力的内部机制还不清楚，因此希望通过新方法探究这些模型内部是怎样表征与处理时空视觉语义的。

Method: 提出一个由三种circuits（视觉审核回路、语义追踪回路、注意力流动回路）组成的分析框架，以此观察和解析LVLMs内部对于时空语义的处理过程和相关功能的分工。

Result: 发现视觉语义强烈依赖于特定对象token（移除会导致性能下降高达92.6%），且对象及动作的可解释性概念在模型中后层逐步形成和细化，证实了模型中后层对时空语义有专门的功能模块。

Conclusion: 研究深化了对LVLMs时空语义处理机制的理解，有助于今后更可解释、更健壮的视觉-语言模型设计。

Abstract: The processing mechanisms underlying language and image understanding in
large vision-language models (LVLMs) have been extensively studied. However,
the internal reasoning mechanisms of LVLMs for spatiotemporal understanding
remain poorly understood. In this work, we introduce a systematic,
circuit-based framework designed to investigate how spatiotemporal visual
semantics are represented and processed within these LVLMs. Specifically, our
framework comprises three circuits: visual auditing circuit, semantic tracing
circuit, and attention flow circuit. Through the lens of these circuits, we
discover that visual semantics are highly localized to specific object
tokens--removing these tokens can degrade model performance by up to 92.6%.
Furthermore, we identify that interpretable concepts of objects and actions
emerge and become progressively refined in the middle-to-late layers of LVLMs.
In contrary to the current works that solely focus on objects in one image, we
reveal that the middle-to-late layers of LVLMs exhibit specialized functional
localization for spatiotemporal semantics. Our findings offer significant
mechanistic insights into spatiotemporal semantics analysis of LVLMs, laying a
foundation for designing more robust and interpretable models.

</details>


### [98] [GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous Driving with Gaussian Splatting](https://arxiv.org/abs/2507.19451)
*Baijun Ye,Minghui Qin,Saining Zhang,Moonjun Gong,Shaoting Zhu,Zebang Shen,Luan Zhang,Lu Zhang,Hao Zhao,Hang Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉的高效三维占据重建方法GS-Occ3D，克服了传统激光雷达依赖和视觉方法的诸多局限，实现了可扩展和高质量的自动驾驶场景占据建模。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶占据估计方法严重依赖激光雷达数据，导致高昂的人力标注成本和数据获取瓶颈，难以大规模应用。视觉数据丰富且成本低，但其基于视觉的占据重建面临视角稀疏、遮挡、动态物体和长时序运动等挑战，且现有基于mesh的视觉方法在几何完整性和扩展性方面表现有限。因此，亟需高效且无需激光雷达注释的视觉占据重建方案。

Method: 提出GS-Occ3D方法，利用Octree结构的高斯surfels优化显式占据表达，实现高效及可扩展的三维占据重建。同时，场景被分解为静态背景、地面和动态物体，并对地面与动态车辆分别建模，从而提升大面积连贯性与动态特征捕捉。通过大规模视觉数据自动生成占据信息，提供高质量训练标签。

Result: 在Waymo大规模数据集上实验表明，GS-Occ3D几何重建精度优于现有方法。基于视觉的二值占据标签亦促进了下游占据模型性能，展示出对Occ3D-Waymo及Occ3D-nuScenes的零样本泛化能力。

Conclusion: GS-Occ3D证实了大规模视觉驱动的占据重建具备高效、可扩展和有效泛化的潜力，提出了一条无需激光雷达注释的新范式，为自动驾驶感知系统打开新的研究与应用空间。

Abstract: Occupancy is crucial for autonomous driving, providing essential geometric
priors for perception and planning. However, existing methods predominantly
rely on LiDAR-based occupancy annotations, which limits scalability and
prevents leveraging vast amounts of potential crowdsourced data for
auto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only
framework that directly reconstructs occupancy. Vision-only occupancy
reconstruction poses significant challenges due to sparse viewpoints, dynamic
scene elements, severe occlusions, and long-horizon motion. Existing
vision-based methods primarily rely on mesh representation, which suffer from
incomplete geometry and additional post-processing, limiting scalability. To
overcome these issues, GS-Occ3D optimizes an explicit occupancy representation
using an Octree-based Gaussian Surfel formulation, ensuring efficiency and
scalability. Additionally, we decompose scenes into static background, ground,
and dynamic objects, enabling tailored modeling strategies: (1) Ground is
explicitly reconstructed as a dominant structural element, significantly
improving large-area consistency; (2) Dynamic vehicles are separately modeled
to better capture motion-related occupancy patterns. Extensive experiments on
the Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry
reconstruction results. By curating vision-only binary occupancy labels from
diverse urban scenes, we show their effectiveness for downstream occupancy
models on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes.
It highlights the potential of large-scale vision-based occupancy
reconstruction as a new paradigm for autonomous driving perception. Project
Page: https://gs-occ3d.github.io/

</details>


### [99] [Back to the Features: DINO as a Foundation for Video World Models](https://arxiv.org/abs/2507.19468)
*Federico Baldassarre,Marc Szafraniec,Basile Terver,Vasil Khalidov,Francisco Massa,Yann LeCun,Patrick Labatut,Maximilian Seitzer,Piotr Bojanowski*

Main category: cs.CV

TL;DR: 本文提出了DINO-world，一种通用性强的视频世界模型，在DINOv2的潜在空间内预测未来帧。该模型通过预训练图像编码器和大规模未经筛选的视频数据进行训练，展现了对不同场景时序动态的理解，并在多项视频预测基准上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 传统视频预测模型在通用性和理解复杂场景方面有限，难以同时覆盖多种类型场景（如驾驶、室内、模拟环境），且未充分利用已有强大的图像表征能力。

Method: DINO-world利用DINOv2预训练图像编码器将视频帧映射到潜在空间，在此空间中训练未来预测器，通过大规模未筛选视频集学习多场景的时序动态。同时，可针对观测-动作轨迹微调，使模型支持基于动作的轨迹规划。

Result: DINO-world在多项视频预测任务（如分割、深度预测等）中优于现有模型，展现对直观物理的理解，还能通过微调实现动作有条件的轨迹预测与规划。

Conclusion: DINO-world为通用视频世界建模提供了新方法，通过预训练视觉表征结合大规模数据强化时序理解，实现了更广泛的应用场景和更强性能。

Abstract: We present DINO-world, a powerful generalist video world model trained to
predict future frames in the latent space of DINOv2. By leveraging a
pre-trained image encoder and training a future predictor on a large-scale
uncurated video dataset, DINO-world learns the temporal dynamics of diverse
scenes, from driving and indoor scenes to simulated environments. We show that
DINO-world outperforms previous models on a variety of video prediction
benchmarks, e.g. segmentation and depth forecasting, and demonstrates strong
understanding of intuitive physics. Furthermore, we show that it is possible to
fine-tune the predictor on observation-action trajectories. The resulting
action-conditioned world model can be used for planning by simulating candidate
trajectories in latent space.

</details>


### [100] [DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations](https://arxiv.org/abs/2507.19474)
*Ziren Gong,Xiaohan Li,Fabio Tosi,Youmin Zhang,Stefano Mattoccia,Jun Wu,Matteo Poggi*

Main category: cs.CV

TL;DR: 本文提出了DINO-SLAM，一种利用DINO特征提升神经隐式（NeRF）和显式（3DGS）表示能力的SLAM系统方案。通过Scene Structure Encoder将DINO特征扩展为Enhanced DINO（EDINO）特征，并将其应用于NeRF和3DGS SLAM管线，在多项数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: SLAM系统中的场景表达通常有限，难以有效建模复杂的层次结构与关系。DINO等先进特征提供了丰富的语义信息，有望提升SLAM系统对场景的理解。本文旨在结合DINO特征，增强神经表征SLAM方法的性能。

Method: 设计了Scene Structure Encoder（SSE），将标准DINO特征转化为具备更强结构表达能力的EDINO特征，并提出融合EDINO特征的NeRF-SLAM和3DGS-SLAM两种实现范式。

Result: 将本文提出的基于DINO的SLAM系统分别在Replica、ScanNet和TUM数据集上与当前主流方法进行对比，取得了更优的性能表现。

Conclusion: DINO-SLAM通过深度挖掘DINO特征的结构性信息，显著增强了SLAM系统的场景表达能力，为神经隐式与显式SLAM方法提供了新的提升方向。

Abstract: This paper presents DINO-SLAM, a DINO-informed design strategy to enhance
neural implicit (Neural Radiance Field -- NeRF) and explicit representations
(3D Gaussian Splatting -- 3DGS) in SLAM systems through more comprehensive
scene representations. Purposely, we rely on a Scene Structure Encoder (SSE)
that enriches DINO features into Enhanced DINO ones (EDINO) to capture
hierarchical scene elements and their structural relationships. Building upon
it, we propose two foundational paradigms for NeRF and 3DGS SLAM systems
integrating EDINO features. Our DINO-informed pipelines achieve superior
performance on the Replica, ScanNet, and TUM compared to state-of-the-art
methods.

</details>


### [101] [HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars](https://arxiv.org/abs/2507.19481)
*Byungjun Kim,Shunsuke Saito,Giljoo Nam,Tomas Simon,Jason Saragih,Hanbyul Joo,Junxuan Li*

Main category: cs.CV

TL;DR: 本文提出了一种具显式发型组成性的3D头部头像生成通用先验模型，实现了面部与头发的有效分离，提升了换脸和发型替换的灵活性和控制性。


<details>
  <summary>Details</summary>
Motivation: 现有3D头部头像通用先验模型大多直接整体建模，将面部与头发作为不可分割的整体，难以自然区分二者特征，并不利于如3D换脸与发型替换等应用场景，尤其在有限数据集下更为困难。

Method: 作者提出将面部和头发显式解耦的组合模型，通过合成无发数据管线（基于扩散模型从原始带发数据估测无发几何与纹理）获得配对的有发/无发数据。采用这些数据分别训练面部和头发的潜空间先验模型，并利用组成性作为归纳偏置实现特征有效分离。

Result: 该模型实现了面部与头发组件在不同3D头像间的无缝迁移，同时保证身份一致性。此外，模型可通过单目捕获的少量数据进行few-shot微调，快速生成高保真且可独立编辑头发的3D头像，支持未知新主体。

Conclusion: 文中方法克服了整体式3D头像先验模型面临的分离与泛化难题，在现实场景中具有高度实用价值，为灵活、表达力强的3D头像生成提供了新思路。

Abstract: We present a universal prior model for 3D head avatars with explicit hair
compositionality. Existing approaches to build generalizable priors for 3D head
avatars often adopt a holistic modeling approach, treating the face and hair as
an inseparable entity. This overlooks the inherent compositionality of the
human head, making it difficult for the model to naturally disentangle face and
hair representations, especially when the dataset is limited. Furthermore, such
holistic models struggle to support applications like 3D face and hairstyle
swapping in a flexible and controllable manner. To address these challenges, we
introduce a prior model that explicitly accounts for the compositionality of
face and hair, learning their latent spaces separately. A key enabler of this
approach is our synthetic hairless data creation pipeline, which removes hair
from studio-captured datasets using estimated hairless geometry and texture
derived from a diffusion prior. By leveraging a paired dataset of hair and
hairless captures, we train disentangled prior models for face and hair,
incorporating compositionality as an inductive bias to facilitate effective
separation. Our model's inherent compositionality enables seamless transfer of
face and hair components between avatars while preserving identity.
Additionally, we demonstrate that our model can be fine-tuned in a few-shot
manner using monocular captures to create high-fidelity, hair-compositional 3D
head avatars for unseen subjects. These capabilities highlight the practical
applicability of our approach in real-world scenarios, paving the way for
flexible and expressive 3D avatar generation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [102] [Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement](https://arxiv.org/abs/2507.18742)
*Víctor Gallego*

Main category: cs.CL

TL;DR: 本文提出了一种语言模型在测试时自我修正规约漏洞的新方法，有效减少了模型通过‘作弊’而获得高分的现象，提高了模型的鲁棒性和真实意图对齐度。


<details>
  <summary>Details</summary>
Motivation: 目前，语言模型易于利用指导规范中的缺陷（如评分标准不严密、写法有瑕疵等），通过‘漏洞’获取高分，而并非真正满足用户的真实意图。如何让模型在不修改权重情况下自我识别并修复规范中的问题，是提升模型表现和安全性的关键。

Method: 提出了一种‘规约自我修正’（SSC）框架：模型先根据有瑕疵的规范生成初步回答，再批判性评判该回答，随后修正原规范，最后基于修正后的规范重新生成更鲁棒的答案。全流程仅需推理时处理，无需模型参数或权重改动。

Result: 在多个创造性写作和代码任务实验中，SSC显著减少了模型‘作弊’漏洞的发生率（从50-70%降至不足10%），表明其能有效提升输出的鲁棒性和对齐度。

Conclusion: SSC框架能在无需重新训练模型的情况下提升语言模型根据用户意图作答的可靠性和安全性，为应对奖励漏洞攻击提供了有效解决思路。

Abstract: Language models (LMs) are susceptible to in-context reward hacking, where
they exploit flaws in tainted or faulty written specifications or rubrics to
achieve high scores without fulfilling the user's true intent. We introduce
Specification Self-Correction (SSC), a novel, test-time framework that enables
an LM to identify and correct flaws within its own guiding specification. SSC
employs a multi-step inference process where the model first generates a
response based on a potentially tainted specification, critiques its output,
and then revises the specification itself to remove the exploitable loophole. A
final, more robust response is then generated using this self-corrected
specification. Across experiments spanning creative writing and agentic coding
tasks with several LMs, we demonstrate that while models initially game tainted
specifications in 50-70\% of cases, the SSC process reduces this vulnerability
by over 90\%. This dynamic repair occurs at inference time, requires no weight
modification, and leads to more robustly aligned model behavior. Code at
https://github.com/vicgalle/specification-self-correction .

</details>


### [103] [The Role of Orthographic Consistency in Multilingual Embedding Models for Text Classification in Arabic-Script Languages](https://arxiv.org/abs/2507.18762)
*Abdulhady Abas Abdullah,Amir H. Gandomi,Tarik A Rashid,Seyedali Mirjalili,Laith Abualigah,Milena Živković,Hadi Veisi*

Main category: cs.CL

TL;DR: 提出了AS-RoBERTa系列模型，专为四种阿拉伯字母系语言（索拉尼库尔德语、阿拉伯语、波斯语和乌尔都语）定制，比通用多语言模型表现更佳。


<details>
  <summary>Details</summary>
Motivation: 多语言模型如mBERT和XLM-RoBERTa难以处理那些虽然共享书写系统、但正字法和文化背景不同的语言。特别是在使用阿拉伯字母的多种语言中，这种问题更为突出，需要更具语言和书写特性的方法解决这一问题。

Method: 为每种阿拉伯字母系语言分别收集大规模语料，并训练对应的基于RoBERTa的专属模型。在预训练阶段专注于语言的书写系统特性和统计规律。对比实验和消融实验分析模型性能和改进来源。

Result: AS-RoBERTa在下游分类任务中，相较于mBERT及XLM-RoBERTa提升2-5个百分点。消融实验证实书写系统相关的预训练是提升效果的关键。困惑矩阵分析指出脚本共性与内容领域对效果的影响。

Conclusion: 针对具体语言和其书写系统进行预训练，能显著提升同脚本下不同语言的处理效果。呼吁未来在模型预训练中更重视书写系统和语言特性。

Abstract: In natural language processing, multilingual models like mBERT and
XLM-RoBERTa promise broad coverage but often struggle with languages that share
a script yet differ in orthographic norms and cultural context. This issue is
especially notable in Arabic-script languages such as Kurdish Sorani, Arabic,
Persian, and Urdu. We introduce the Arabic Script RoBERTa (AS-RoBERTa) family:
four RoBERTa-based models, each pre-trained on a large corpus tailored to its
specific language. By focusing pre-training on language-specific script
features and statistics, our models capture patterns overlooked by
general-purpose models. When fine-tuned on classification tasks, AS-RoBERTa
variants outperform mBERT and XLM-RoBERTa by 2 to 5 percentage points. An
ablation study confirms that script-focused pre-training is central to these
gains. Error analysis using confusion matrices shows how shared script traits
and domain-specific content affect performance. Our results highlight the value
of script-aware specialization for languages using the Arabic script and
support further work on pre-training strategies rooted in script and language
specificity.

</details>


### [104] [ylmmcl at Multilingual Text Detoxification 2025: Lexicon-Guided Detoxification and Classifier-Gated Rewriting](https://arxiv.org/abs/2507.18769)
*Nicole Lai-Lopez,Lusha Wang,Su Yuan,Liza Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种强健的多语言文本去毒化管道，结合了字典引导标注、微调的序列到序列模型和迭代分类器机制，在PAN-2025竞赛中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有文本去毒化方法多为无监督或仅限单语，存在跨语言泛化能力不足、精度有限的问题。本文旨在通过引入多语言有毒词典和精细设计管道，提高多语言去毒化的准确性和泛化能力。

Method: 方法包括三大步骤：首先使用多语言有毒词典对原始文本进行有毒词汇的显式标注；然后通过微调的序列到序列模型（s-nlp/mt0-xl-detox-orpo）进行去毒化；最后利用分类器作为门控机制，迭代评估和提升生成文本的去毒化效果。

Result: 最终模型在多个评测指标上优于基线和回译方法，STA得分达0.922，毒性输入的J分数为0.612，xCOMET分数分别为0.793（dev）和0.787（test），在高资源（英语、俄语、法语）语言上泛化能力表现强，整体表现居于第九名。

Conclusion: 结合词典引导标注与序列模型迭代优化后的多语言去毒管道能够有效提升文本去毒化效果，尤其在高资源语言下展现出较强的跨语言泛化能力，优于传统基线与回译方法，但在保相似性（SIM）上存在一定妥协。

Abstract: In this work, we introduce our solution for the Multilingual Text
Detoxification Task in the PAN-2025 competition for the ylmmcl team: a robust
multilingual text detoxification pipeline that integrates lexicon-guided
tagging, a fine-tuned sequence-to-sequence model (s-nlp/mt0-xl-detox-orpo) and
an iterative classifier-based gatekeeping mechanism. Our approach departs from
prior unsupervised or monolingual pipelines by leveraging explicit toxic word
annotation via the multilingual_toxic_lexicon to guide detoxification with
greater precision and cross-lingual generalization. Our final model achieves
the highest STA (0.922) from our previous attempts, and an average official J
score of 0.612 for toxic inputs in both the development and test sets. It also
achieved xCOMET scores of 0.793 (dev) and 0.787 (test). This performance
outperforms baseline and backtranslation methods across multiple languages, and
shows strong generalization in high-resource settings (English, Russian,
French). Despite some trade-offs in SIM, the model demonstrates consistent
improvements in detoxification strength. In the competition, our team achieved
ninth place with a score of 0.612.

</details>


### [105] [Evaluating Code-Mixing in LLMs Across 18 Languages](https://arxiv.org/abs/2507.18791)
*Yilun Yang,Yekun Chai*

Main category: cs.CL

TL;DR: 本文系统评估了大语言模型（LLMs）在18种语言的混合语料上的表现，并提出一种结合词替换与GPT-4提示的新型合成混合语料生成方法。实验发现，LLMs在涉及多语言家族的混合语料上表现不佳，建议通过增加训练数据、提升模型规模和采用少量学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有针对混合语言（code-mixing）处理的基准数据覆盖的语言范围有限，任务类别单一，难以全面评估LLMs的真实能力。而多语言用户中混合语言现象普遍，相关LLMs研究明显不足。同时，当前自动生成混合语料的方法也欠成熟。

Method: 作者对18种语言（涵盖7大语言家族）中的混合语料数据进行系统性评测，并提出利用词级替换与GPT-4提示相结合的方法生成合成混合语料，进一步用于模型评测。

Result: LLMs在多语言家族参与的混合语料集上普遍表现欠佳，未能有效应对跨语言混用情况。

Conclusion: 提高训练数据的多样性、扩大模型规模以及采用few-shot学习等方法，有望提升LLMs对多语言混合语料的处理能力。

Abstract: Code-mixing, the practice of switching between languages within a
conversation, presents unique challenges for traditional natural language
processing. Existing benchmarks, such as LinCE and GLUECoS, are limited by
narrow language pairings and tasks, failing to adequately evaluate the
code-mixing capabilities of large language models (LLMs). Despite the
significance of code-mixing for multilingual users, research on LLMs in this
context remains limited. Additionally, current methods for generating
code-mixed data are underdeveloped. In this paper, we conduct a comprehensive
evaluation of LLMs' performance on code-mixed data across 18 languages from
seven language families. We also propose a novel approach for generating
synthetic code-mixed texts by combining word substitution with GPT-4 prompting.
Our analysis reveals consistent underperformance of LLMs on code-mixed datasets
involving multiple language families. We suggest that improvements in training
data size, model scale, and few-shot learning could enhance their performance.

</details>


### [106] [CueBuddy: helping non-native English speakers navigate English-centric STEM education](https://arxiv.org/abs/2507.18827)
*Pranav Gupta*

Main category: cs.CL

TL;DR: 本论文提出了CueBuddy系统，通过实时技术关键词检测和多语言词汇表查阅，帮助非英语母语的学生更好地理解STEM课程中的复杂英语术语，提高他们的学习效率。


<details>
  <summary>Details</summary>
Motivation: 许多来自资源较低语言背景的学生在STEM学科的学习上因英语术语理解困难而落后于更流利的同龄人，主要原因是他们的前置知识大多为母语学习。现有的语音翻译系统成本高且难以处理技术内容，因此需要一种更高效、经济的辅助方式。

Method: 开发了一套CueBuddy工具，利用实时技术关键词检测和多语言词汇表查阅，为学生在听课过程中提供即时术语释义，避免中断学习的思路。

Result: CueBuddy能够帮助学生在不分散课堂注意力的情况下，理解复杂的英语术语，从而更好地跟上课程进度。

Conclusion: CueBuddy为低资源语言背景下的STEM学生提供了有效支持，但依然存在一些限制，未来会探讨进一步优化与扩展的方向。

Abstract: Students across the world in STEM classes, especially in the Global South,
fall behind their peers who are more fluent in English, despite being at par
with them in terms of scientific prerequisites. While many of them are able to
follow everyday English at ease, key terms in English stay challenging. In most
cases, such students have had most of their course prerequisites in a lower
resource language. Live speech translation to lower resource languages is a
promising area of research, however, models for speech translation can be too
expensive on a large scale and often struggle with technical content. In this
paper, we describe CueBuddy, which aims to remediate these issues by providing
real-time "lexical cues" through technical keyword spotting along real-time
multilingual glossary lookup to help students stay up to speed with complex
English jargon without disrupting their concentration on the lecture. We also
describe the limitations and future extensions of our approach.

</details>


### [107] [PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning](https://arxiv.org/abs/2507.18857)
*Mohammad Kachuee,Teja Gollapudi,Minseok Kim,Yin Huang,Kai Sun,Xiao Yang,Jiaqi Wang,Nirav Shah,Yue Liu,Aaron Colak,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: PrismRAG是一种高效的RAG微调框架，通过引入干扰证据与强化推理能力，显著提升了模型的事实准确率，超越了最新技术。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG框架在处理包含干扰性或半相关内容的检索结果时，容易出现混淆，且对于需要深度理解和推理的问题表现有限。因此，需要新的方法提升模型应对复杂语境和推理问题的能力。

Method: 提出了PrismRAG微调框架：（1）通过将金标准证据与细微的干扰段落混合，训练模型识别并区分干扰内容；（2）强化LLM的推理习惯，让模型能自发进行规划、合理化和综合，不依赖复杂的人工指令。

Result: 在12个跨多领域开放问答基准上进行评估，PrismRAG平均事实性提升5.4%，超越了当前的先进方法。

Conclusion: PrismRAG显著提升了RAG在面对复杂与误导性检索内容时的鲁棒性和事实性，为RAG技术在实际应用中带来了更强表现。

Abstract: Retrieval-augmented generation (RAG) often falls short when retrieved context
includes confusing semi-relevant passages, or when answering questions require
deep contextual understanding and reasoning. We propose an efficient
fine-tuning framework, called PrismRAG, that (i) trains the model with
distractor-aware QA pairs mixing gold evidence with subtle distractor passages,
and (ii) instills reasoning-centric habits that make the LLM plan, rationalize,
and synthesize without relying on extensive human engineered instructions.
Evaluated across 12 open-book RAG QA benchmarks spanning diverse application
domains and scenarios, PrismRAG improves average factuality by 5.4%,
outperforming state-of-the-art solutions.

</details>


### [108] [MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service](https://arxiv.org/abs/2507.18884)
*Ming Gong,Xucheng Huang,Ziheng Xu,Vijayan K. Asari*

Main category: cs.CL

TL;DR: 该论文提出了MindFlow+，一种结合大语言模型（LLM）、模仿学习和离线强化学习的自进化电商客服对话系统，并通过数据增强和奖励信号来提升对话质量，实验结果显示其在多项指标上优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 传统基于意图的电商客服系统难以应对动态、多轮复杂交互，无法充分利用工具推理和奖励机制达到更高质量的回复。因此，作者旨在利用大语言模型并结合强化学习，提升电商客服对话系统应对实际场景的能力。

Method: MindFlow+方法包括两大创新：1）工具增强型示范构建，让模型通过知识增强和类似ReAct代理机制的交互示例，对工具更有效使用；2）奖励条件建模，通过奖励信号引导模型生成更贴合任务目标的回复。此外，提出了AI Contribution Ratio新指标，用以量化对话中AI参与度。

Result: 在真实电商对话数据上，MindFlow+在上下文相关性、灵活性和任务准确率等方面均显著优于强基线模型。AI Contribution Ratio有效评估了系统自动化程度。

Conclusion: 结合大语言模型的推理能力、工具使用和奖励引导学习，可显著提升电商场景下对话系统的专业性和上下文感知能力，为构建高质量客服Agent提供了有效技术路径。

Abstract: High-quality dialogue is crucial for e-commerce customer service, yet
traditional intent-based systems struggle with dynamic, multi-turn
interactions. We present MindFlow+, a self-evolving dialogue agent that learns
domain-specific behavior by combining large language models (LLMs) with
imitation learning and offline reinforcement learning (RL). MindFlow+
introduces two data-centric mechanisms to guide learning: tool-augmented
demonstration construction, which exposes the model to knowledge-enhanced and
agentic (ReAct-style) interactions for effective tool use; and
reward-conditioned data modeling, which aligns responses with task-specific
goals using reward signals. To evaluate the model's role in response
generation, we introduce the AI Contribution Ratio, a novel metric quantifying
AI involvement in dialogue. Experiments on real-world e-commerce conversations
show that MindFlow+ outperforms strong baselines in contextual relevance,
flexibility, and task accuracy. These results demonstrate the potential of
combining LLMs tool reasoning, and reward-guided learning to build
domain-specialized, context-aware dialogue systems.

</details>


### [109] [NUTMEG: Separating Signal From Noise in Annotator Disagreement](https://arxiv.org/abs/2507.18890)
*Jonathan Ivey,Susan Gauch,David Jurgens*

Main category: cs.CL

TL;DR: 本文提出了一种新的贝叶斯模型NUTMEG，能够区分和保留标注者间的有意义分歧，同时减少噪音，为NLP任务提供更高质量的人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 传统上，人工标注的NLP数据集存在不同标注者之间的分歧，多数方法将这些分歧视为错误并通过聚合消除。近期研究认为，部分分歧是标注者真实观点的反映，应该作为有用信号，但现有方法缺乏将有意义分歧与噪音区分的能力。为此，作者希望提出一种新方法区分并处理这些差异，提高数据质量。

Method: 作者提出了一种名为NUTMEG的贝叶斯模型，纳入了标注者背景信息，能够在剔除噪音标注的同时保留系统性分歧。模型在合成数据集和不同标注者分布、分歧率、垃圾标注率条件下进行了实验与分析。

Result: 实验发现，NUTMEG能在系统性分歧较多的数据中，更有效地恢复接近真实的标注结果，优于传统的聚合方法。进一步，基于NUTMEG聚合结果训练的下游模型显著超越传统方法聚合数据训练的模型。

Conclusion: 本文证明了在处理中人工标注数据时，区分标注者能力与系统性分歧的重要性。NUTMEG模型在提升数据质量与下游任务表现上，优于现有主流聚合方法。

Abstract: NLP models often rely on human-labeled data for training and evaluation. Many
approaches crowdsource this data from a large number of annotators with varying
skills, backgrounds, and motivations, resulting in conflicting annotations.
These conflicts have traditionally been resolved by aggregation methods that
assume disagreements are errors. Recent work has argued that for many tasks
annotators may have genuine disagreements and that variation should be treated
as signal rather than noise. However, few models separate signal and noise in
annotator disagreement. In this work, we introduce NUTMEG, a new Bayesian model
that incorporates information about annotator backgrounds to remove noisy
annotations from human-labeled training data while preserving systematic
disagreements. Using synthetic data, we show that NUTMEG is more effective at
recovering ground-truth from annotations with systematic disagreement than
traditional aggregation methods. We provide further analysis characterizing how
differences in subpopulation sizes, rates of disagreement, and rates of spam
affect the performance of our model. Finally, we demonstrate that downstream
models trained on NUTMEG-aggregated data significantly outperform models
trained on data from traditionally aggregation methods. Our results highlight
the importance of accounting for both annotator competence and systematic
disagreements when training on human-labeled data.

</details>


### [110] [REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?](https://arxiv.org/abs/2507.18901)
*Chuxuan Hu,Liyun Zhang,Yeji Lim,Aum Wadhwani,Austin Peters,Daniel Kang*

Main category: cs.CL

TL;DR: 本文提出了REPRO-Bench，一个包含112份社科论文及其重现报告的数据集，用于评估AI代理在社科论文可重复性自动评审任务中的表现。现有AI代理准确率偏低，本文提出的新方法REPRO-Agent可提升准确率71%。


<details>
  <summary>Details</summary>
Motivation: 现有可重复性基准存在三大问题：只关注用代码复现结果、不考虑现实复杂场景、样本格式和语言单一，无法有效模拟和评价AI在自动化论文可重复性评审中的实际能力。推动科研严谨性需更高效、自动化的评审工具。

Method: 作者构建了REPRO-Bench数据集，包括社科论文原文、相应复现包及公开复现报告，并设计任务让AI代理读取论文和重现材料后自动评估其可重复性。对三种主流AI代理在该基准上的表现进行评估，并在此基础上提出REPRO-Agent模型以提升性能。

Result: 三种典型AI代理在REPRO-Bench上的最高准确率仅为21.4%。而新提出的REPRO-Agent将准确率提升至原有代理的171%，显著优于现有方法。

Conclusion: 当前AI代理尚难胜任复杂实际的论文可重复性自动评估工作。REPRO-Bench为后续研究提供了更现实且全面的基准，未来需研发更强大的AI代理推动评审自动化。

Abstract: Assessing the reproducibility of social science papers is essential for
promoting rigor in research processes, but manual assessment is costly. With
recent advances in agentic AI systems (i.e., AI agents), we seek to evaluate
their capability to automate this process. However, existing benchmarks for
reproducing research papers (1) focus solely on reproducing results using
provided code and data without assessing their consistency with the paper, (2)
oversimplify real-world scenarios, and (3) lack necessary diversity in data
formats and programming languages. To address these issues, we introduce
REPRO-Bench, a collection of 112 task instances, each representing a social
science paper with a publicly available reproduction report. The agents are
tasked with assessing the reproducibility of the paper based on the original
paper PDF and the corresponding reproduction package. REPRO-Bench features
end-to-end evaluation tasks on the reproducibility of social science papers
with complexity comparable to real-world assessments. We evaluate three
representative AI agents on REPRO-Bench, with the best-performing agent
achieving an accuracy of only 21.4%. Building on our empirical analysis, we
develop REPRO-Agent, which improves the highest accuracy achieved by existing
agents by 71%. We conclude that more advanced AI agents should be developed to
automate real-world reproducibility assessment. REPRO-Bench is publicly
available at https://github.com/uiuc-kang-lab/REPRO-Bench.

</details>


### [111] [SLoW: Select Low-frequency Words! Automatic Dictionary Selection for Translation on Large Language Models](https://arxiv.org/abs/2507.18902)
*Hongyuan Lu,Zixuan Li,Zefan Zhang,Wai Lam*

Main category: cs.CL

TL;DR: 论文提出了自动词典选择（ADS）任务，并设计了无需训练数据频率估算的新方法SLoW，用以提升大型语言模型在少数语言的翻译能力，同时显著节省token消耗。


<details>
  <summary>Details</summary>
Motivation: 虽然超过7000种语言存在，但现有大型语言模型仅支持数百种。现有基于词典的提示能提升稀有语言翻译，但一味使用全部词典开销大。为在token消耗和翻译效果间平衡，自动化选择使用哪些词典成为必要。

Method: 提出ADS任务，并创新性地提出SLoW方法：选择低频词词典参与提示，无需访问通常难以获得的训练数据，仅用公开资源估算词频即可。无需对LLM进行额外调优，兼具简便与高效。

Result: 在FLORES 100种语言测试中，SLoW优于强基线方法，并显著节省token消耗。部分语言的翻译性能甚至超越全部词典参与时的基线。

Conclusion: SLoW方法在无需额外训练数据和调优的前提下，有效提升稀有语言的翻译能力，降低token消耗，为词典辅助翻译方法提供灵活高效的新思路。

Abstract: There are more than 7,000 languages around the world, and current Large
Language Models (LLMs) only support hundreds of languages. Dictionary-based
prompting methods can enhance translation on them, but most methods use all the
available dictionaries, which could be expensive. Instead, it will be flexible
to have a trade-off between token consumption and translation performance. This
paper proposes a novel task called \textbf{A}utomatic \textbf{D}ictionary
\textbf{S}election (\textbf{ADS}). The goal of the task is to automatically
select which dictionary to use to enhance translation. We propose a novel and
effective method which we call \textbf{S}elect \textbf{Lo}w-frequency
\textbf{W}ords! (\textbf{SLoW}) which selects those dictionaries that have a
lower frequency. Our methods have unique advantages. First, there is no need
for access to the training data for frequency estimation (which is usually
unavailable). Second, it inherits the advantage of dictionary-based methods,
where no additional tuning is required on LLMs. Experimental results on 100
languages from FLORES indicate that SLoW surpasses strong baselines, and it can
obviously save token usage, with many languages even surpassing the translation
performance of the full dictionary baseline.\footnote{A shocking fact is that
there is no need to use the actual training data (often unobtainable) for
frequency estimation, and an estimation frequency obtained using public
resources is still apparently effective in improving translation with ChatGPT
and Llama, and DeepSeek.}\footnote{Code and data available upon publication.}

</details>


### [112] [Large language models provide unsafe answers to patient-posed medical questions](https://arxiv.org/abs/2507.18905)
*Rachel L. Draelos,Samina Afreen,Barbara Blasko,Tiffany Brazile,Natasha Chase,Dimple Desai,Jessica Evert,Heather L. Gardner,Lauren Herrmann,Aswathy Vaikom House,Stephanie Kass,Marianne Kavan,Kirshma Khemani,Amanda Koire,Lauren M. McDonald,Zahraa Rabeeah,Amy Shah*

Main category: cs.CL

TL;DR: 本文对四款主流公共可用的大型语言模型（LLM）医疗聊天机器人在实际医疗建议场景下的安全性进行了定量和定性评估，发现它们在患者安全方面存在显著差异，且有相当比例的回答可能对患者造成伤害。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多患者日常通过LLM聊天机器人获取医疗建议，患者安全性问题日益突出。当前对这些机器人在实际医疗建议中的危险性缺乏系统评估，因此需要进行有针对性的比较研究。

Method: 由医生主导的red-teaming研究，建立了新的HealthAdvice数据集，涵盖了内科、女性健康和儿科等常见初级保健主题，收集222个实际患者提出的医疗咨询问题，对四款主流聊天机器人（Claude、Gemini、GPT-4o、Llama3-70B）的888条回答进行定量和定性分析。

Result: 四款聊天机器人的安全性存在统计显著差异。问题性回答比例从Claude的21.6%到Llama的43.2%不等，不安全回答比例在Claude的5%到GPT-4o与Llama的13%之间。定性分析显示，有些回答具有导致严重患者伤害的潜力。

Conclusion: 当前主流公共医疗聊天机器人存在较高比例的不安全建议，已经对数百万患者构成威胁，亟需改进相关模型的临床安全性。

Abstract: Millions of patients are already using large language model (LLM) chatbots
for medical advice on a regular basis, raising patient safety concerns. This
physician-led red-teaming study compares the safety of four publicly available
chatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and
Llama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation
framework that enables quantitative and qualitative analysis. In total, 888
chatbot responses are evaluated for 222 patient-posed advice-seeking medical
questions on primary care topics spanning internal medicine, women's health,
and pediatrics. We find statistically significant differences between chatbots.
The rate of problematic responses varies from 21.6 percent (Claude) to 43.2
percent (Llama), with unsafe responses varying from 5 percent (Claude) to 13
percent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the
potential to lead to serious patient harm. This study suggests that millions of
patients could be receiving unsafe medical advice from publicly available
chatbots, and further work is needed to improve the clinical safety of these
powerful tools.

</details>


### [113] [A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions](https://arxiv.org/abs/2507.18910)
*Agada Joseph Oche,Ademola Glory Folashade,Tirthankar Ghosal,Arpan Biswas*

Main category: cs.CL

TL;DR: 本文系统回顾了RAG（检索增强生成）技术的发展，涵盖其技术框架、应用现状、挑战与前沿创新。


<details>
  <summary>Details</summary>
Motivation: RAG技术旨在弥补大型语言模型在事实性、准确性和时效性上的不足，尤其是减少幻觉和模型知识过时问题，提升结果的现实基础和上下文相关性。

Method: 作者系统梳理了RAG的核心技术要素，包括检索机制、序列到序列生成模型及融合策略，按年份解析重要进展，对多种实现进行比较评测（如检索准确率、生成流畅性、延迟、计算效率），并分析在企业级部署时遇到的现实问题，如数据检索、隐私与扩展性。

Result: 论文呈现了RAG领域的主要技术里程碑、年度研究趋势、不同实现方式的性能对比，以及当前在检索质量、隐私保护和集成成本等方面面临的持续挑战。

Conclusion: RAG技术正沿着混合检索、隐私增强、融合优化和智能代理化等方向快速发展，未来有望实现更高效、可靠、上下文感知更强的知识密集型NLP系统。

Abstract: Retrieval-Augmented Generation (RAG) represents a major advancement in
natural language processing (NLP), combining large language models (LLMs) with
information retrieval systems to enhance factual grounding, accuracy, and
contextual relevance. This paper presents a comprehensive systematic review of
RAG, tracing its evolution from early developments in open domain question
answering to recent state-of-the-art implementations across diverse
applications. The review begins by outlining the motivations behind RAG,
particularly its ability to mitigate hallucinations and outdated knowledge in
parametric models. Core technical components-retrieval mechanisms,
sequence-to-sequence generation models, and fusion strategies are examined in
detail. A year-by-year analysis highlights key milestones and research trends,
providing insight into RAG's rapid growth. The paper further explores the
deployment of RAG in enterprise systems, addressing practical challenges
related to retrieval of proprietary data, security, and scalability. A
comparative evaluation of RAG implementations is conducted, benchmarking
performance on retrieval accuracy, generation fluency, latency, and
computational efficiency. Persistent challenges such as retrieval quality,
privacy concerns, and integration overhead are critically assessed. Finally,
the review highlights emerging solutions, including hybrid retrieval
approaches, privacy-preserving techniques, optimized fusion strategies, and
agentic RAG architectures. These innovations point toward a future of more
reliable, efficient, and context-aware knowledge-intensive NLP systems.

</details>


### [114] [Mining Contextualized Visual Associations from Images for Creativity Understanding](https://arxiv.org/abs/2507.18915)
*Ananya Sahu,Amith Ananthram,Kathleen McKeown*

Main category: cs.CL

TL;DR: 本论文提出了一种可扩展到任何无标注数据集的图片视觉元素上下文关联挖掘方法，用于生成逐步抽象的高质量创意标题，并构建了170万条创意标题的新数据集。该方法提升了图文检索等任务在创意领域如诗歌、隐喻可视化的表现。


<details>
  <summary>Details</summary>
Motivation: 当前训练视觉-语言模型如CLIP时，主要依赖网络收集的直白短文本，难以理解和生成更具创意和抽象的语言输出。为了提升模型在更高层次视觉-语言理解与表达的能力，需要更多抽象且创意的图文配对数据。

Method: 提出了一种新的方法，能为图片中的显著视觉元素挖掘上下文化关联，通过这个过程自动为图片生成多层次（从具象到抽象）的创意描述。基于此方法，作者为MSCOCO数据集创建了170万条创意图像标题，并用于模型训练和评估。

Result: 通过人工评估，发现新的标题数据集比传统数据集有更高的创意性且仍然贴合视觉内容。用该数据集微调视觉编码器后，在诗歌和隐喻可视化等创意场景的零样本图文检索中取得了显著提升。

Conclusion: 提出的方法与生成的创意标题数据集提升了视觉-语言模型在创意和抽象表达方面的能力，有助于图像理解和生成领域的进一步研究。相关资源已向学术社区开放。

Abstract: Understanding another person's creative output requires a shared language of
association. However, when training vision-language models such as CLIP, we
rely on web-scraped datasets containing short, predominantly literal, alt-text.
In this work, we introduce a method for mining contextualized associations for
salient visual elements in an image that can scale to any unlabeled dataset.
Given an image, we can use these mined associations to generate high quality
creative captions at increasing degrees of abstraction. With our method, we
produce a new dataset of visual associations and 1.7m creative captions for the
images in MSCOCO. Human evaluation confirms that these captions remain visually
grounded while exhibiting recognizably increasing abstraction. Moreover,
fine-tuning a visual encoder on this dataset yields meaningful improvements in
zero-shot image-text retrieval in two creative domains: poetry and metaphor
visualization. We release our dataset, our generation code and our models for
use by the broader community.

</details>


### [115] [Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders](https://arxiv.org/abs/2507.18918)
*Richmond Sin Jing Xuan,Jalil Huseynov,Yang Zhang*

Main category: cs.CL

TL;DR: 论文分析了多语言大语言模型（LLMs）在中低资源语言上的激活模式差异，并提出用激活感知微调（LoRA）提升表现。实验显示激活对齐能促进多语种LLM性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管多语言LLM具备跨语种泛化能力，但在中低资源语言上的表现显著低于高资源语言，亟需分析原因并提升其性能。

Method: 作者通过Sparse Autoencoders（SAEs）分析LLM（Gemma-2-2B）26个residual层在10种语言上的激活模式，并在此基础上对中低资源语言应用基于LoRA的激活感知微调。

Result: 中低资源语言在模型早期层激活较低，激活感知微调后如Malayalam与Hindi的激活显著提升（约87%），英文激活保持（约91%）。基准测试表现获得小幅稳健提升。

Conclusion: 激活对齐是提升多语种LLM在中低资源语言表现的关键，可以通过激活感知微调方法有效实现。

Abstract: Multilingual large language models (LLMs) exhibit strong cross-linguistic
generalization, yet medium to low resource languages underperform on common
benchmarks such as ARC-Challenge, MMLU, and HellaSwag. We analyze activation
patterns in Gemma-2-2B across all 26 residual layers and 10 languages: Chinese
(zh), Russian (ru), Spanish (es), Italian (it), medium to low resource
languages including Indonesian (id), Catalan (ca), Marathi (mr), Malayalam
(ml), and Hindi (hi), with English (en) as the reference. Using Sparse
Autoencoders (SAEs), we reveal systematic disparities in activation patterns.
Medium to low resource languages receive up to 26.27 percent lower activations
in early layers, with a persistent gap of 19.89 percent in deeper layers. To
address this, we apply activation-aware fine-tuning via Low-Rank Adaptation
(LoRA), leading to substantial activation gains, such as 87.69 percent for
Malayalam and 86.32 percent for Hindi, while maintaining English retention at
approximately 91 percent. After fine-tuning, benchmark results show modest but
consistent improvements, highlighting activation alignment as a key factor in
enhancing multilingual LLM performance.

</details>


### [116] [LLaVA-NeuMT: Selective Layer-Neuron Modulation for Efficient Multilingual Multimodal Translation](https://arxiv.org/abs/2507.18940)
*Jingxuan Wei,Caijun Jia,Qi Chen,Yujun Cai,Linzhuang Sun,Xiangxiang Zhang,Gaowei Wu,Bihui Yu*

Main category: cs.CL

TL;DR: 本文提出了一种新的多模态多语种翻译框架LLaVA-NeuMT，通过显式建模语言特定与通用表示，有效缓解了多语种干扰问题，并以更少的参数超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态机器翻译主要针对双语场景，在拓展至多语种时面临跨语言干扰和参数共享不佳的问题，因此需要新的框架提升多语种下的翻译表现。

Method: 提出LLaVA-NeuMT框架，包括层选择机制（为不同语言对选取最有效的网络层）以及神经元级自适应策略（动态选取语言特定与通用神经元），在只微调约40%参数的情况下提升翻译效果。

Result: 在M3-Multi30K和M3-AmbigCaps数据集上进行了大量实验证明，LLaVA-NeuMT在只微调部分参数（约40%）的情况下，超越了全量微调方法，在两个多模态多语种翻译数据集上都达到了SOTA水平。

Conclusion: LLaVA-NeuMT有效缓解了多语种翻译中的跨语言干扰，减少了参数冗余，提升了多模态多语种翻译的效果和效率，为多语言跨模态适应提供了可扩展的解决方案，并分析了模型的层和神经元选择的重要性。

Abstract: Multimodal Machine Translation (MMT) enhances translation quality by
incorporating visual context, helping to resolve textual ambiguities. While
existing MMT methods perform well in bilingual settings, extending them to
multilingual translation remains challenging due to cross-lingual interference
and ineffective parameter-sharing strategies. To address this, we propose
LLaVA-NeuMT, a novel multimodal multilingual translation framework that
explicitly models language-specific and language-agnostic representations to
mitigate multilingual interference. Our approach consists of a layer selection
mechanism that identifies the most informative layers for different language
pairs and a neuron-level adaptation strategy that dynamically selects
language-specific and agnostic neurons to improve translation quality while
reducing redundancy. We conduct extensive experiments on the M3-Multi30K and
M3-AmbigCaps datasets, demonstrating that LLaVA-NeuMT, while fine-tuning only
40\% of the model parameters, surpasses full fine-tuning approaches and
ultimately achieves SOTA results on both datasets. Our analysis further
provides insights into the importance of selected layers and neurons in
multimodal multilingual adaptation, offering an efficient and scalable solution
to cross-lingual adaptation in multimodal translation.

</details>


### [117] [Legal Document Summarization: Enhancing Judicial Efficiency through Automation Detection](https://arxiv.org/abs/2507.18952)
*Yongjie Li,Ruilin Nong,Jianan Liu,Lucas Evans*

Main category: cs.CL

TL;DR: 本论文提出了一种法律文档自动摘要方法，通过先进的自然语言处理和机器学习技术，实现高效、精确地提取法律文本的关键信息，提高司法工作效率。


<details>
  <summary>Details</summary>
Motivation: 随着法律文档日益庞大，人工审查费时且易遗漏重要信息。提升司法效率，并减轻法律专业人员的负担成为亟需解决的问题。

Method: 作者采用最新的自然语言处理技术，结合先进的机器学习算法，识别出司法文档中的关键模式，并自动提取和总结关键信息。通过实际法律数据集进行实验，验证框架的有效性。

Result: 实验结果显示，该方法能够在保证内容完整性的同时，显著提升摘要生成的质量和处理速度，使法律专业人员可以将精力集中在更有价值的分析与决策上。

Conclusion: 自动化法律文档摘要技术能显著优化司法流程，提高工作效率，减少失误，推动法律行业的科技化发展。

Abstract: Legal document summarization represents a significant advancement towards
improving judicial efficiency through the automation of key information
detection. Our approach leverages state-of-the-art natural language processing
techniques to meticulously identify and extract essential data from extensive
legal texts, which facilitates a more efficient review process. By employing
advanced machine learning algorithms, the framework recognizes underlying
patterns within judicial documents to create precise summaries that encapsulate
the crucial elements. This automation alleviates the burden on legal
professionals, concurrently reducing the likelihood of overlooking vital
information that could lead to errors. Through comprehensive experiments
conducted with actual legal datasets, we demonstrate the capability of our
method to generate high-quality summaries while preserving the integrity of the
original content and enhancing processing times considerably. The results
reveal marked improvements in operational efficiency, allowing legal
practitioners to direct their efforts toward critical analytical and
decision-making activities instead of manual reviews. This research highlights
promising technology-driven strategies that can significantly alter workflow
dynamics within the legal sector, emphasizing the role of automation in
refining judicial processes.

</details>


### [118] [A Similarity Measure for Comparing Conversational Dynamics](https://arxiv.org/abs/2507.18956)
*Sang Min Jung,Kaixiang Zhang,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: 本文提出了一种用于比较对话整体动态的相似性度量方法，并通过大规模在线社区数据验证该方法的有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管单句回复质量很重要，但决定对话整体质量的更关键因素是对话互动模式。目前缺乏自动化手段从整体互动动态维度对比不同对话，限制了对话数据分析与对话系统更全面的评价。

Method: 作者提出了一种新的对话动态相似性度量标准，并设计了验证框架测试该度量在捕捉对话动态差异方面的鲁棒性，以及评估其对对话话题敏感性的能力。

Result: 通过在大型在线社区数据上的实证分析，验证了所提方法对会话动态的区分与度量能力，并展示该度量在洞察对话中情境权力关系方面的应用。

Conclusion: 该工作首次为对话整体动态比较提供了自动化度量工具，提升了对话数据分析和对话系统评价的全面性，并为分析现实社交互动中的权力结构提供了新视角。

Abstract: The quality of a conversation goes beyond the individual quality of each
reply, and instead emerges from how these combine into interactional patterns
that give the conversation its distinctive overall "shape". However, there is
no robust automated method for comparing conversations in terms of their
overall interactional dynamics. Such methods could enhance the analysis of
conversational data and help evaluate conversational agents more holistically.
  In this work, we introduce a similarity measure for comparing conversations
with respect to their dynamics. We design a validation framework for testing
the robustness of the metric in capturing differences in conversation dynamics
and for assessing its sensitivity to the topic of the conversations. Finally,
to illustrate the measure's utility, we use it to analyze conversational
dynamics in a large online community, bringing new insights into the role of
situational power in conversations.

</details>


### [119] [A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation](https://arxiv.org/abs/2507.18973)
*Bohan Yao,Vikas Yadav*

Main category: cs.CL

TL;DR: 本文提出了Multi-TAG框架，通过在每个推理步骤同时调用多个外部工具并聚合其结果，提高了大语言模型（LLMs）在复杂数学推理任务上的表现，显著超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强型大语言模型的方法通常每步只调用单一工具，在处理复杂多步数学题时存在局限，难以保证推理的精确性和鲁棒性。因而，亟需一种能更好协调和利用多工具的方法。

Method: 作者提出Multi-TAG框架，使LLM在每步推理中并行调用多个工具，收集多元输出并聚合，从而校验和优化推理过程。该方法不需额外微调，仅在推理阶段操作，适配性强，可直接用于各种LLM（无论开源或闭源）。

Result: 在MATH500、AIME、AMC和OlympiadBench四个复杂数学基准上，Multi-TAG在开放权重和闭源LLM上均显著优于当前最优基线，平均提升6.0%到7.5%。

Conclusion: Multi-TAG框架提升了LLM解决复杂数学推理任务的能力，适用性强，且无需微调，对现有与未来LLM平台有广泛实际意义。

Abstract: Augmenting large language models (LLMs) with external tools is a promising
avenue for developing high-performance mathematical reasoning systems. Prior
tool-augmented approaches typically finetune an LLM to select and invoke a
single tool at each reasoning step and show promising results on simpler math
reasoning benchmarks such as GSM8K. However, these approaches struggle with
more complex math problems that require precise reasoning over multiple steps.
To address this limitation, in this work, we propose Multi-TAG, a Multi-Tool
AGgregation-based framework. Instead of relying on a single tool, Multi-TAG
guides an LLM to concurrently invoke multiple tools at each reasoning step. It
then aggregates their diverse outputs to verify and refine the reasoning
process, enhancing solution robustness and accuracy. Notably, Multi-TAG is a
finetuning-free, inference-only framework, making it readily applicable to any
LLM backbone, including large open-weight models which are computationally
expensive to finetune and proprietary frontier models which cannot be finetuned
with custom recipes. We evaluate Multi-TAG on four challenging benchmarks:
MATH500, AIME, AMC, and OlympiadBench. Across both open-weight and
closed-source LLM backbones, Multi-TAG consistently and substantially
outperforms state-of-the-art baselines, achieving average improvements of 6.0%
to 7.5% over state-of-the-art baselines.

</details>


### [120] [Arg-LLaDA: Argument Summarization via Large Language Diffusion Models and Sufficiency-Aware Refinement](https://arxiv.org/abs/2507.19081)
*Hao Li,Yizheng Sun,Viktor Schlegel,Kailai Yang,Riza Batista-Navarro,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文提出了一种名为Arg-LLaDA的新型大语言扩散框架，通过迭代“遮蔽-再生成”过程提升论证摘要的质量，实现了更高的涵盖性、准确性与精炼性，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前论证摘要生成阶段研究较少，且大多数方法仅作单步生成，难以充分修正文事实错误或结构不佳的问题。作者旨在提升多观点论证摘要的事实性、结构性和凝练度。

Method: 提出Arg-LLaDA框架。该方法利用灵活的遮蔽控制器和充分性判别模块，对摘要中不充分、冗余或不支持的部分重新遮蔽并再生成，循环迭代优化摘要内容。

Result: 在两个基准数据集上，Arg-LLaDA在10个自动评测指标中有7项超越现有SOTA，人工评测也显示其在覆盖度、真实性与精炼度等核心维度有大幅提升。

Conclusion: 迭代、基于充分性判别的方法能够有效提升多视角论证摘要的全面性和质量，Arg-LLaDA为论证摘要生成提供了新思路且实用性强。

Abstract: Argument summarization aims to generate concise, structured representations
of complex, multi-perspective debates. While recent work has advanced the
identification and clustering of argumentative components, the generation stage
remains underexplored. Existing approaches typically rely on single-pass
generation, offering limited support for factual correction or structural
refinement. To address this gap, we introduce Arg-LLaDA, a novel large language
diffusion framework that iteratively improves summaries via sufficiency-guided
remasking and regeneration. Our method combines a flexible masking controller
with a sufficiency-checking module to identify and revise unsupported,
redundant, or incomplete spans, yielding more faithful, concise, and coherent
outputs. Empirical results on two benchmark datasets demonstrate that Arg-LLaDA
surpasses state-of-the-art baselines in 7 out of 10 automatic evaluation
metrics. In addition, human evaluations reveal substantial improvements across
core dimensions, coverage, faithfulness, and conciseness, validating the
effectiveness of our iterative, sufficiency-aware generation strategy.

</details>


### [121] [Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents](https://arxiv.org/abs/2507.19090)
*Haorui He,Yupeng Li,Dacheng Wen,Reynold Cheng,Francis C. M. Lau*

Main category: cs.CL

TL;DR: 本文提出了一种基于多LLM代理辩论的新型事实核查框架DebateCV，相比传统单LLM方法在处理多面证据的复杂核查任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: 许多现有基于单LLM的事实核查方法在面对需要多方证据、复杂推理的任务时效果有限。现实世界中，事实核查常常需要多方讨论和对抗式推理，因此设计更接近真实过程的新方法成为研究动力。

Method: DebateCV框架下，两个大型语言模型（Debater）分别支持和反对某个论断，展开多轮辩论；第三个LLM（Moderator）作为评判者，对辩论结果进行审查和给出理由。此外，为训练Moderator，还创新性地用无监督方式合成数据以缓解现实中缺乏“辩论驱动”的核查数据的问题。

Result: 实验证明，该方法在不同证据质量条件下，相较现有方法具有更高的核查准确性和鲁棒性。

Conclusion: 多代理辩论机制显著提升了复杂事实核查任务的性能，并通过合成数据训练有效解决了缺乏高质量训练数据的难题。DebateCV提供了一种与现实流程更接近的新范式。

Abstract: Claim verification is critical for enhancing digital literacy. However, the
state-of-the-art single-LLM methods struggle with complex claim verification
that involves multi-faceted evidences. Inspired by real-world fact-checking
practices, we propose DebateCV, the first claim verification framework that
adopts a debate-driven methodology using multiple LLM agents. In our framework,
two Debaters take opposing stances on a claim and engage in multi-round
argumentation, while a Moderator evaluates the arguments and renders a verdict
with justifications. To further improve the performance of the Moderator, we
introduce a novel post-training strategy that leverages synthetic debate data
generated by the zero-shot DebateCV, effectively addressing the scarcity of
real-world debate-driven claim verification data. Experimental results show
that our method outperforms existing claim verification methods under varying
levels of evidence quality. Our code and dataset are publicly available at
https://anonymous.4open.science/r/DebateCV-6781.

</details>


### [122] [Objectifying the Subjective: Cognitive Biases in Topic Interpretations](https://arxiv.org/abs/2507.19117)
*Swapnil Hingmire,Ze Shi Li,Shiyu,Zeng,Ahmed Musa Awon,Luiz Franciscatto Guerra,Neil Ernst*

Main category: cs.CL

TL;DR: 本文探讨了主题解释对主题模型下游应用的重要性，并通过用户研究提出了新的主题质量评估框架，发现用户更多依赖启发式而非概率统计来理解主题。


<details>
  <summary>Details</summary>
Motivation: 现有主题质量评价方法（如连贯性和词入侵实验）未能反映主题在语料探索过程中的作用，缺乏以任务和用户为中心的评价视角。作者希望揭示用户实际是如何理解与解释主题的，为改进评价方法提供依据。

Method: 作者设计并实施了用户研究，让用户评估他们提出的主题质量构成要素，并要求用户给出评价理由。通过反身性主题分析方法（reflexive thematic analysis），从用户提供的理由中归纳总结了主题解释的主要模式。

Result: 研究发现用户在解释主题时主要依赖可得性、代表性等启发式原则，而不是主题模型输出的概率值。用户通常先锁定突出词语（锚点），然后围绕这些词进行语义推断（调整），形成对主题的理解。

Conclusion: 主题解释过程本质上是个带有不确定性的判断过程，用户会带入某些认知偏差。因此，未来主题模型的用户评估体系和用户建模应考虑这些认知偏差，开发更贴合实际用户思维习惯的评价方法。

Abstract: Interpretation of topics is crucial for their downstream applications.
State-of-the-art evaluation measures of topic quality such as coherence and
word intrusion do not measure how much a topic facilitates the exploration of a
corpus. To design evaluation measures grounded on a task, and a population of
users, we do user studies to understand how users interpret topics. We propose
constructs of topic quality and ask users to assess them in the context of a
topic and provide rationale behind evaluations. We use reflexive thematic
analysis to identify themes of topic interpretations from rationales. Users
interpret topics based on availability and representativeness heuristics rather
than probability. We propose a theory of topic interpretation based on the
anchoring-and-adjustment heuristic: users anchor on salient words and make
semantic adjustments to arrive at an interpretation. Topic interpretation can
be viewed as making a judgment under uncertainty by an ecologically rational
user, and hence cognitive biases aware user models and evaluation frameworks
are needed.

</details>


### [123] [An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case](https://arxiv.org/abs/2507.19156)
*Gioele Giachino,Marco Rondina,Antonio Vetrò,Riccardo Coppola,Juan Carlos De Martin*

Main category: cs.CL

TL;DR: 本论文分析大型语言模型(LLM)在无性别提示下生成与性别和职业相关偏见内容的现象，通过对意大利语的实验展现了主流LLM输出中性别与职业偏见的持续存在。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在各领域应用增加，关于其容易延续刻板印象、生成有偏见内容的担忧也随之上升。尤其是在性别和职业相关偏见问题明显，该研究聚焦LLM对这类偏见的生成方式及其可能带来的社会影响。

Method: 作者以意大利语为测试对象，设计了结构化实验，设置涉及三组存在等级关系的职业组合的不同提示词（未指明性别），并用ChatGPT（gpt-4o-mini）和Google Gemini（gemini-1.5-flash）两个主流聊天机器人生成共3600条回复，通过API收集数据，评估其偏见分布。

Result: 实验结果显示，两款LLM均显著将女性代词（如“她”）与职位层级较低的“助理”角色绑定（Gemini占比100%，ChatGPT占比97%），而非与“经理”关联，突出反映了性别与职业偏见。

Conclusion: 研究指出LLM目前在生成内容时易产生性别与职业组合的刻板印象，尤其在非英语等高性别区分语言中更明显。这一偏见在职场等实际应用场景具有伦理隐患，未来需通过更广泛的多语言、多模型实验证据，以及改进提示设计，推动AI系统减小不平等、促进社会公平。

Abstract: The increasing use of Large Language Models (LLMs) in a large variety of
domains has sparked worries about how easily they can perpetuate stereotypes
and contribute to the generation of biased content. With a focus on gender and
professional bias, this work examines in which manner LLMs shape responses to
ungendered prompts, contributing to biased outputs. This analysis uses a
structured experimental method, giving different prompts involving three
different professional job combinations, which are also characterized by a
hierarchical relationship. This study uses Italian, a language with extensive
grammatical gender differences, to highlight potential limitations in current
LLMs' ability to generate objective text in non-English languages. Two popular
LLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google
Gemini (gemini-1.5-flash). Through APIs, we collected a range of 3600
responses. The results highlight how content generated by LLMs can perpetuate
stereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she'
pronouns to the 'assistant' rather than the 'manager'. The presence of bias in
AI-generated text can have significant implications in many fields, such as in
the workplaces or in job selections, raising ethical concerns about its use.
Understanding these risks is pivotal to developing mitigation strategies and
assuring that AI-based systems do not increase social inequalities, but rather
contribute to more equitable outcomes. Future research directions include
expanding the study to additional chatbots or languages, refining prompt
engineering methods or further exploiting a larger experimental base.

</details>


### [124] [Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?](https://arxiv.org/abs/2507.19195)
*Chaymaa Abbas,Mariette Awad,Razane Tajeddine*

Main category: cs.CL

TL;DR: 本研究指出大型语言模型（LLM）对于不同英语方言，特别是非裔美国英语（AAVE）的偏见在遭遇投毒数据后显著加剧，尤其是模型规模越大，偏见越严重。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM设计持续改进以促进包容和平衡响应，但模型仍易于编码和放大社会偏见。研究动机在于探究方言差异（AAVE vs. SAE）与数据投毒如何共同影响模型输出的有害性和歧视性。

Method: 研究通过在不同规模的LLaMA模型中注入少量投毒数据，分析AAVE和SAE输入的毒性输出变化，并使用GPT-4o作为公平性审计工具检测对AAVE的有害刻板印象。

Result: 即便极少量投毒数据，也会导致AAVE输入毒性显著上升，而SAE几乎不受影响。模型规模越大，对AAVE偏见放大越明显。GPT-4o审计显示AAVE更易被关联到攻击性、犯罪性和智力低下等负面刻板印象。

Conclusion: 数据投毒和方言偏见具有叠加影响，必须在模型开发中引入方言敏感评测、定向去偏、负责任的训练机制，以缓解社会偏见问题。

Abstract: Despite the ongoing improvements in the design of large language models
(LLMs) to foster inclusion and balanced responses, these systems remain
susceptible to encoding and amplifying social biases. This study examines how
dialectal variation, specifically African American Vernacular English (AAVE)
versus Standard American English (SAE), interacts with data poisoning to
influence toxicity in outputs. Using both small- and medium-scale LLaMA models,
we show that even minimal exposure to poisoned data significantly increases
toxicity for AAVE inputs, while it remains comparatively unaffected for SAE.
Larger models exhibit a more significant amplification effect which suggests
heightened susceptibility with scale. To further assess these disparities, we
employed GPT-4o as a fairness auditor, which identified harmful stereotypical
patterns disproportionately tied to AAVE inputs, including portrayals of
aggression, criminality, and intellectual inferiority. These findings
underscore the compounding impact of data poisoning and dialectal bias and
emphasize the need for dialect-aware evaluation, targeted debiasing
interventions, and socially responsible training protocols during development.

</details>


### [125] [How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework](https://arxiv.org/abs/2507.19219)
*Zi Liang,Liantong Yu,Shiyu Zhang,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: 本文提出了ArxivRoll，一个动态评测大语言模型(LLM)的新框架，有效缓解了现有公开基准测试因污染或训练失衡导致的模型能力高估问题。核心方法包括私有测试case自动生成和污染度新指标。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估常因数据污染或训练集-测试集重叠，导致模型性能被高估，且现有基准测试难以兼顾重现性、透明性和高效性。已有方法如保密测试、人评、多次构造新样本等存在局限，尤其是无法量化高估程度。

Method: 作者提出 ArxivRoll 框架，核心包括：(1) SCP (Sequencing, Cloze, Prediction)：自动产生私有测试题；(2) Rugged Scores (RS)：测量公开集污染和训练偏差。该方法每六个月自动选取最新arXiv文章构建全新评测集，对LLM做“一次性”测评以防止数据泄露影响。

Result: 实验证明新基准测试集质量高，能有效衡量主流LLM的真实能力并量化过度估计问题。并对目前流行LLM做了系统评估，发现过往评测存在高估现象。

Conclusion: ArxivRoll 框架在保证评测可重现、透明且高效的前提下，有效缓解和量化了LLM评测时高估问题，为模型公平对比和能力真实评估提供了新思路和工具。

Abstract: Overestimation in evaluating large language models (LLMs) has become an
increasing concern. Due to the contamination of public benchmarks or imbalanced
model training, LLMs may achieve unreal evaluation results on public
benchmarks, either intentionally or unintentionally, which leads to unfair
comparisons among LLMs and undermines their realistic capability assessments.
Existing benchmarks attempt to address these issues by keeping test cases
permanently secret, mitigating contamination through human evaluation, or
repeatedly collecting and constructing new samples. However, these approaches
fail to ensure reproducibility, transparency, and high efficiency
simultaneously. Moreover, the extent of overestimation in current LLMs remains
unquantified. To address these issues, we propose ArxivRoll, a dynamic
evaluation framework inspired by one-time pad encryption in cryptography.
ArxivRoll comprises two key components: \emph{i) SCP (Sequencing, Cloze, and
Prediction)}, an automated generator for private test cases, and \emph{ii)
Rugged Scores (RS)}, metrics that measure the proportion of public benchmark
contamination and training bias. Leveraging SCP, ArxivRoll constructs a new
benchmark every six months using recent articles from ArXiv and employs them
for one-time evaluations of LLM performance. Extensive experiments demonstrate
the high quality of our benchmark, and we provide a systematic evaluation of
current LLMs. The source code is available at
https://github.com/liangzid/ArxivRoll/.

</details>


### [126] [Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation](https://arxiv.org/abs/2507.19227)
*Yuanhe Zhang,Fangzhou Xie,Zhenhong Zhou,Zherui Li,Hao Chen,Kun Wang,Yufei Guo*

Main category: cs.CL

TL;DR: 本论文揭示了大型语言扩散模型（LLDMs）在安全性上的重大隐患，并提出了一种名为PAD的新攻击方法，实验验证了该方法在LLDMs上的极高成功率。


<details>
  <summary>Details</summary>
Motivation: 尽管LLDMs在推理速度和数学能力上优于传统LLM，但其精确快速的生成能力加剧了安全风险，而已有的LLM“越狱”方法对LLDMs效果有限，导致其安全性难以评估和防控。

Method: 作者提出了一种针对基于扩散的语言模型的“并行解码越狱攻击”（PAD），该方法创新性地利用Multi-Point Attention Attack，引导多点并行生成过程偏向有害输出，并借鉴了LLM中的肯定式答复模式。

Result: PAD攻击在四个主流LLDMs上的成功率高达97%，显著暴露了这些模型的安全漏洞。此外，在同等规模下，LLDMs生成有害内容的速度是自回归LLM的两倍。

Conclusion: LLDMs存在严重的安全隐患，且其危险性因生成速度更快而进一步放大。论文强调安全防护的紧迫性，并为今后扩散架构语言模型的安全部署提供了重要参考。

Abstract: Large Language Diffusion Models (LLDMs) exhibit comparable performance to
LLMs while offering distinct advantages in inference speed and mathematical
reasoning tasks.The precise and rapid generation capabilities of LLDMs amplify
concerns of harmful generations, while existing jailbreak methodologies
designed for Large Language Models (LLMs) prove limited effectiveness against
LLDMs and fail to expose safety vulnerabilities.Successful defense cannot
definitively resolve harmful generation concerns, as it remains unclear whether
LLDMs possess safety robustness or existing attacks are incompatible with
diffusion-based architectures.To address this, we first reveal the
vulnerability of LLDMs to jailbreak and demonstrate that attack failure in
LLDMs stems from fundamental architectural differences.We present a PArallel
Decoding jailbreak (PAD) for diffusion-based language models. PAD introduces
Multi-Point Attention Attack, which guides parallel generative processes toward
harmful outputs that inspired by affirmative response patterns in LLMs.
Experimental evaluations across four LLDMs demonstrate that PAD achieves
jailbreak attack success rates by 97%, revealing significant safety
vulnerabilities. Furthermore, compared to autoregressive LLMs of the same size,
LLDMs increase the harmful generation speed by 2x, significantly highlighting
risks of uncontrolled misuse.Through comprehensive analysis, we provide an
investigation into LLDM architecture, offering critical insights for the secure
deployment of diffusion-based language models.

</details>


### [127] [Identifying Fine-grained Forms of Populism in Political Discourse: A Case Study on Donald Trump's Presidential Campaigns](https://arxiv.org/abs/2507.19303)
*Ilias Chalkidis,Stephanie Brandl,Paris Aslanidis*

Main category: cs.CL

TL;DR: 本文探讨大语言模型（LLM）在识别和分类细粒度民粹主义话语方面的能力，并创建了新的数据集进行评测。结果显示，传统微调模型（如RoBERTa）大幅优于未经微调的LLMs，但指令微调的LLMs在跨领域任务中表现出较强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在执行多种任务时表现优异，但其理解复杂社会科学概念（如民粹主义）能力尚不明确。鉴于民粹主义在学术与媒体中均属复杂且有争议的概念，评估LLMs在此领域的能力具有理论和应用意义。

Method: 作者创建并公开了捕捉民粹主义话语的专门数据集，评价了多种开源及专有LLMs，涵盖不同提示方法，同时将最佳模型应用于特朗普及欧洲政治家竞选演讲的分析，以检验其跨场景泛化能力。

Result: 实验结果表明：微调的RoBERTa分类器在民粹主义话语识别任务上显著优于新一代未经微调的指令追随型LLM。将最佳模型应用于特朗普及欧洲政治家演讲分析后发现，指令微调的LLMs在域外数据上具有更强的鲁棒性。

Conclusion: LLM在细粒度社会科学文本识别任务上仍有限制，尤其在未经针对性微调情况下；但指令微调的LLMs在跨领域场景应用中表现更稳健。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of instruction-following tasks, yet their grasp of nuanced social
science concepts remains underexplored. This paper examines whether LLMs can
identify and classify fine-grained forms of populism, a complex and contested
concept in both academic and media debates. To this end, we curate and release
novel datasets specifically designed to capture populist discourse. We evaluate
a range of pre-trained (large) language models, both open-weight and
proprietary, across multiple prompting paradigms. Our analysis reveals notable
variation in performance, highlighting the limitations of LLMs in detecting
populist discourse. We find that a fine-tuned RoBERTa classifier vastly
outperforms all new-era instruction-tuned LLMs, unless fine-tuned.
Additionally, we apply our best-performing model to analyze campaign speeches
by Donald Trump, extracting valuable insights into his strategic use of
populist rhetoric. Finally, we assess the generalizability of these models by
benchmarking them on campaign speeches by European politicians, offering a lens
into cross-context transferability in political discourse analysis. In this
setting, we find that instruction-tuned LLMs exhibit greater robustness on
out-of-domain data.

</details>


### [128] [AutoPCR: Automated Phenotype Concept Recognition by Prompting](https://arxiv.org/abs/2507.19315)
*Yicheng Tao,Yuanhao Huang,Jie Liu*

Main category: cs.CL

TL;DR: 本文提出了AutoPCR方法，实现了无需针对本体训练即可识别生物医学文本中的表型概念，并在多个数据集上取得了最优和最稳健的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的表型概念识别方法需针对特定本体训练，难以适应不同文本类型及不断变化的生物医学术语，因此需要更具通用性和适应性的解决方案。

Method: 提出AutoPCR方法，分三阶段：1）结合规则与神经标注的实体抽取，2）使用SapBERT进行候选实体检索，3）通过大语言模型提示完成实体链接，且无需本体特定训练。

Result: 在四个基准数据集上实现了平均性能的最优和最稳健表现，mention级与document级评估均优于现有最佳方法。消融实验与迁移学习进一步证明了AutoPCR的归纳能力和泛化性。

Conclusion: AutoPCR无需本体特定训练，具备较强泛化和稳健性，为表型概念识别及其应用提供了更优的解决方案。

Abstract: Phenotype concept recognition (CR) is a fundamental task in biomedical text
mining, enabling applications such as clinical diagnostics and knowledge graph
construction. However, existing methods often require ontology-specific
training and struggle to generalize across diverse text types and evolving
biomedical terminology. We present AutoPCR, a prompt-based phenotype CR method
that does not require ontology-specific training. AutoPCR performs CR in three
stages: entity extraction using a hybrid of rule-based and neural tagging
strategies, candidate retrieval via SapBERT, and entity linking through
prompting a large language model. Experiments on four benchmark datasets show
that AutoPCR achieves the best average and most robust performance across both
mention-level and document-level evaluations, surpassing prior state-of-the-art
methods. Further ablation and transfer studies demonstrate its inductive
capability and generalizability to new ontologies.

</details>


### [129] [Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks](https://arxiv.org/abs/2507.19353)
*Kai Liu,Zhan Su,Peijie Dong,Fengran Mo,Jianfei Gao,ShaoTing Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为“Smooth Reading”的chunk-wise推理方法，显著提升了Recurrent LLM在长上下文任务中的表现，并且保持高效性，首次实现了与自注意力LLM媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 现有Recurrent LLM虽然具有线性复杂度和高效率，但在长上下文任务中由于固定内存限制表现不佳，尚未能赶上自注意力LLM，亟需突破。

Method: 作者提出了Smooth Reading方法，受人类阅读启发，将长上下文分块逐步处理和迭代性总结，降低对固定内存依赖，使方法更适配Recurrent LLM的特点。

Result: 在LongBench任务上，SWA-3B-4k（Recurrent LLM）在Smooth Reading下，从落后自注意力5.68%提升至超出3.61%。同时，在64k上下文下训练速度提升3倍，推理速度提升2倍。

Conclusion: Smooth Reading方法有效弥补了Recurrent LLM与自注意力LLM在长上下文任务上的性能差距，并保留了前者的高效率，实现了同等性能，推动了领域发展。

Abstract: Recently, recurrent large language models (Recurrent LLMs) with linear
computational complexity have re-emerged as efficient alternatives to
self-attention-based LLMs (Self-Attention LLMs), which have quadratic
complexity. However, Recurrent LLMs often underperform on long-context tasks
due to their limited fixed-size memory. Previous research has primarily focused
on enhancing the memory capacity of Recurrent LLMs through architectural
innovations, but these approaches have not yet enabled Recurrent LLMs to match
the performance of Self-Attention LLMs on long-context tasks. We argue that
this limitation arises because processing the entire context at once is not
well-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a
chunk-wise inference method inspired by human reading strategies. Smooth
Reading processes context in chunks and iteratively summarizes the contextual
information, thereby reducing memory demands and making the approach more
compatible with Recurrent LLMs. Our experimental results show that this method
substantially narrows the performance gap between Recurrent and Self-Attention
LLMs on long-context tasks, while preserving the efficiency advantages of
Recurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from
5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench.
Besides, our method maintains the high efficiency, training 3x faster and
inferring 2x faster at 64k context compared to Self-Attention LLMs. To our
knowledge, this is the first work to achieve comparable performance using
Recurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope
our method will inspire future research in this area. To facilitate further
progress, we will release code and dataset.

</details>


### [130] [Enhancing Speech Emotion Recognition Leveraging Aligning Timestamps of ASR Transcripts and Speaker Diarization](https://arxiv.org/abs/2507.19356)
*Hsuan-Yu Wang,Pei-Ying Lee,Berlin Chen*

Main category: cs.CL

TL;DR: 本论文探讨了ASR文本与说话人分离结果之间的时间戳对齐对语音情感识别效果的影响。提出了一种基于时间戳对齐的多模态情感识别方法，并实验证明对齐能显著提升识别准确率。


<details>
  <summary>Details</summary>
Motivation: 在多模态情感识别，尤其是对话情境下，ASR与说话人分离输出常常出现时间对不齐，影响最终情感分析的准确性。本文旨在解决这一常见但被忽略的问题。

Method: 利用预训练ASR和说话人分离模型，开发时间戳对齐流水线实现精准说话人片段标注；结合RoBERTa提取的文本嵌入和Wav2Vec的音频嵌入，通过带门控机制的跨注意力融合，实现多模态情感识别。

Result: 在IEMOCAP数据集上实验证明，时间戳准确对齐后，多模态情感识别准确率优于未对齐的基线方法。

Conclusion: 时间戳的精确对齐对提升情感识别系统的准确率至关重要，为后续鲁棒多模态情感分析提供了基础。

Abstract: In this paper, we investigate the impact of incorporating timestamp-based
alignment between Automatic Speech Recognition (ASR) transcripts and Speaker
Diarization (SD) outputs on Speech Emotion Recognition (SER) accuracy.
Misalignment between these two modalities often reduces the reliability of
multimodal emotion recognition systems, particularly in conversational
contexts. To address this issue, we introduce an alignment pipeline utilizing
pre-trained ASR and speaker diarization models, systematically synchronizing
timestamps to generate accurately labeled speaker segments. Our multimodal
approach combines textual embeddings extracted via RoBERTa with audio
embeddings from Wav2Vec, leveraging cross-attention fusion enhanced by a gating
mechanism. Experimental evaluations on the IEMOCAP benchmark dataset
demonstrate that precise timestamp alignment improves SER accuracy,
outperforming baseline methods that lack synchronization. The results highlight
the critical importance of temporal alignment, demonstrating its effectiveness
in enhancing overall emotion recognition accuracy and providing a foundation
for robust multimodal emotion analysis.

</details>


### [131] [SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models](https://arxiv.org/abs/2507.19361)
*Zhen Wan,Chao-Han Huck Yang,Yahan Yu,Jinchuan Tian,Sheng Li,Ke Hu,Zhehuai Chen,Shinji Watanabe,Fei Cheng,Chenhui Chu,Sadao Kurohashi*

Main category: cs.CL

TL;DR: 本文提出了一种新的语音理解能力评估体系SIQ，基于人类认知过程，全面衡量和比较大模型语音理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的语音理解评估（如WER）仅反映词级准确性，难以全面评价大语言模型在语音理解中的能力。缺乏基于认知多维度的评测工具，限制了模型发展与实际应用。

Method: 作者提出了Speech-based Intelligence Quotient（SIQ）评估框架，从布鲁姆认知分类学三层次出发：（1）记忆（用WER衡量）；（2）理解（比较模型理解的相似度）；（3）应用（以问答准确率检验模型下游能力）。该框架既适用于级联（如ASR+LLM）也适用于端到端语音-文本大模型，还能发现数据集标注错误和模型幻觉。

Result: 实验表明，SIQ能全面量化语音理解能力，在比较不同模型、发现注释缺陷和模型幻觉等方面均优于现有方法。

Conclusion: SIQ首次将认知评测理念引入语音大模型评测，建立了连接认知科学与语音多模态基准的新方法，有助于发现和解决当前多模态语音模型训练中的关键挑战。

Abstract: We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human
cognition-inspired evaluation pipeline for voice understanding large language
models, LLM Voice, designed to assess their voice understanding ability. Moving
beyond popular voice understanding metrics such as word error rate (WER), SIQ
examines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy:
(1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e.,
similarity of LLM's interpretations); and (3) Application (i.e., QA accuracy
for simulating downstream tasks). We demonstrate that SIQ not only quantifies
voice understanding abilities but also provides unified comparisons between
cascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation
errors in existing benchmarks, and detects hallucinations in LLM Voice. Our
framework represents a first-of-its-kind intelligence examination that bridges
cognitive principles with voice-oriented benchmarks, while exposing overlooked
challenges in multi-modal training.

</details>


### [132] [Data Augmentation for Spoken Grammatical Error Correction](https://arxiv.org/abs/2507.19374)
*Penny Karanasou,Mengjie Qian,Stefano Bannò,Mark J. F. Gales,Kate M. Knill*

Main category: cs.CL

TL;DR: 本文提出了一种全自动生成含有语法错误和口语不流利现象的音频-文本对的方法，旨在为口语语法纠错任务（SGEC）丰富高质量标注数据资源，并通过一系列客观指标对生成数据进行评估。


<details>
  <summary>Details</summary>
Motivation: 虽然书面语语法纠错（GEC）已有强基准数据集，但高质量的口语语法纠错（SGEC）标注数据仍非常匮乏，严重影响了相关研究进展。因此，需突破性方法自动扩充高质量口语纠错数据。

Method: 作者提出了一套全自动流程，能在原始数据基础上生成包含语法错误和口语不流利特征的音频-文本对。此外，还设计了一系列客观指标，用于评估和筛选更适合SGEC任务的数据集。

Result: 通过在S&I语料库（一套公开标注语法错误的口语语料）上的实验证明，所提出的数据增强方法不仅能提升书面GEC模型的表现，也有效推动了SGEC模型的进步。

Conclusion: 自动生成和评测的增强数据既可改善SGEC相关任务的训练数据紧缺问题，又不影响L2学习者语言水平的评估结果，为口语语法纠错领域提供了重要的数据和方法支持。

Abstract: While there exist strong benchmark datasets for grammatical error correction
(GEC), high-quality annotated spoken datasets for Spoken GEC (SGEC) are still
under-resourced. In this paper, we propose a fully automated method to generate
audio-text pairs with grammatical errors and disfluencies. Moreover, we propose
a series of objective metrics that can be used to evaluate the generated data
and choose the more suitable dataset for SGEC. The goal is to generate an
augmented dataset that maintains the textual and acoustic characteristics of
the original data while providing new types of errors. This augmented dataset
should augment and enrich the original corpus without altering the language
assessment scores of the second language (L2) learners. We evaluate the use of
the augmented corpus both for written GEC (the text part) and for SGEC (the
audio-text pairs). Our experiments are conducted on the S\&I Corpus, the first
publicly available speech dataset with grammar error annotations.

</details>


### [133] [Detection of Adverse Drug Events in Dutch clinical free text documents using Transformer Models: benchmark study](https://arxiv.org/abs/2507.19396)
*Rachel M. Murphy,Nishant Mishra,Nicolette F. de Keizer,Dave A. Dongelmans,Kitty J. Jager,Ameen Abu-Hanna,Joanna E. Klopotowska,Iacer Calixto*

Main category: cs.CL

TL;DR: 本文提出了针对荷兰语临床自由文本中不良药物事件（ADE）检测的基准，对多种Transformer模型进行评测，并证明MedRoBERTa.nl模型表现最优。


<details>
  <summary>Details</summary>
Motivation: 缺乏针对荷兰语临床自由文本ADE检测的基准和专用模型，且用于临床实际场景的评估标准尚不完善。

Method: 作者使用102份荷兰ICU病人进展记录，训练Bi-LSTM和四种基于Transformer的模型（BERTje、RobBERT、MedRoBERTa.nl、NuNER），开展命名实体识别（NER）和关系分类（RC）任务。评估包括内部基准和外部文件级验证，并报告micro/macro F1分数以应对数据不平衡问题。

Result: 几种模型在ADE关系分类任务上的表现相近，但MedRoBERTa.nl模型macro F1分数最高（金标准0.63，预测实体0.62），在外部验证时召回率达0.67~0.74，检测出67~74%的ADE病例。

Conclusion: 该基准为荷兰语临床文本ADE检测提供了稳健衡量方法，强调了采用适合该任务评估标准的重要性，为未来临床应用铺路。

Abstract: In this study, we set a benchmark for adverse drug event (ADE) detection in
Dutch clinical free text documents using several transformer models, clinical
scenarios and fit-for-purpose performance measures. We trained a Bidirectional
Long Short-Term Memory (Bi-LSTM) model and four transformer-based Dutch and/or
multilingual encoder models (BERTje, RobBERT, MedRoBERTa.nl, and NuNER) for the
tasks of named entity recognition (NER) and relation classification (RC) using
102 richly annotated Dutch ICU clinical progress notes. Anonymized free text
clinical progress notes of patients admitted to intensive care unit (ICU) of
one academic hospital and discharge letters of patients admitted to Internal
Medicine wards of two non-academic hospitals were reused. We evaluated our ADE
RC models internally using gold standard (two-step task) and predicted entities
(end-to-end task). In addition, all models were externally validated on
detecting ADEs at the document level. We report both micro- and macro-averaged
F1 scores, given the imbalance of ADEs in the datasets. Although differences
for the ADE RC task between the models were small, MedRoBERTa.nl was the best
performing model with macro-averaged F1 score of 0.63 using gold standard and
0.62 using predicted entities. The MedRoBERTa.nl models also performed the best
in our external validation and achieved recall of between 0.67 to 0.74 using
predicted entities, meaning between 67 to 74% of discharge letters with ADEs
were detected. Our benchmark study presents a robust and clinically meaningful
approach for evaluating language models for ADE detection in clinical free text
documents. Our study highlights the need to use appropriate performance
measures fit for the task of ADE detection in clinical free-text documents and
envisioned future clinical use.

</details>


### [134] [Towards Domain Specification of Embedding Models in Medicine](https://arxiv.org/abs/2507.19407)
*Mohammad Khodadad,Ali Shiraee,Mahdi Astaraki,Hamidreza Mahyar*

Main category: cs.CL

TL;DR: 本文提出并微调了一个多源自监督对比学习的医学文本嵌入模型MEDTE，并推出涵盖51个任务的医学嵌入综合基准，对当前医学文本嵌入模型及其评测方法提出了改进。实验结果显示，该模型在多任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学文本嵌入模型训练数据窄、方法落后，导致无法覆盖真实医疗场景中的术语和语义多样性；同时，主流评测也无法充分反映实际医疗任务的复杂性。为此，作者希望提升嵌入模型泛化能力并建立更全面的医学领域基准。

Method: 作者提出MEDTE模型，通过对多个医学语料源进行自监督对比学习，实现鲁棒的医学文本嵌入。并构建涵盖51个任务的医学嵌入基准套件（类似MTEB），包括分类、聚类、配对分类和检索任务，有针对性地更贴合医学文本特性进行评测。

Result: MEDTE模型在综合基准测评套件中，在多项任务上均显著优于现有先进的医学文本嵌入模型，展现了更强的泛化能力和鲁棒性。

Conclusion: 通过多源自监督对比学习的医学文本嵌入模型和更全面的医学嵌入基准，能更好地应对实际医学任务需求，提高模型表现，推动医学NLP应用发展。

Abstract: Medical text embedding models are foundational to a wide array of healthcare
applications, ranging from clinical decision support and biomedical information
retrieval to medical question answering, yet they remain hampered by two
critical shortcomings. First, most models are trained on a narrow slice of
medical and biological data, beside not being up to date in terms of
methodology, making them ill suited to capture the diversity of terminology and
semantics encountered in practice. Second, existing evaluations are often
inadequate: even widely used benchmarks fail to generalize across the full
spectrum of real world medical tasks.
  To address these gaps, we leverage MEDTE, a GTE model extensively fine-tuned
on diverse medical corpora through self-supervised contrastive learning across
multiple data sources, to deliver robust medical text embeddings.
  Alongside this model, we propose a comprehensive benchmark suite of 51 tasks
spanning classification, clustering, pair classification, and retrieval modeled
on the Massive Text Embedding Benchmark (MTEB) but tailored to the nuances of
medical text. Our results demonstrate that this combined approach not only
establishes a robust evaluation framework but also yields embeddings that
consistently outperform state of the art alternatives in different tasks.

</details>


### [135] [TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability](https://arxiv.org/abs/2507.19419)
*Mohammad Aflah Khan,Ameya Godbole,Johnny Tian-Zheng Wei,Ryan Wang,James Flemings,Krishna Gummadi,Willie Neiswanger,Robin Jia*

Main category: cs.CL

TL;DR: TokenSmith 是一个为大模型预训练设计的数据操作与分析工具，便于研究者交互式地编辑、检查和分析训练数据，提升数据可用性和实验效率。


<details>
  <summary>Details</summary>
Motivation: 当前在大模型预训练过程中，数据与模型行为之间关系的理解受限于工具缺乏，研究者难以高效、便捷地检查和处理用于预训练的数据集。现有的工作流程繁琐、零散且门槛高。

Method: 提出 TokenSmith 开源库，支持在 Megatron、GPT-NeoX、NVIDIA NeMo 等框架下，对数据集进行搜索、查看、导入、导出、检查和采样等操作，具备模块化后端与简洁的用户界面。允许无需修改训练代码即可对预训练数据进行结构化编辑，便于数据集调试、验证和实验。

Result: TokenSmith 插件可以方便地集成进现有大模型预训练流程，为生产级数据集工具的获取提供平等化机会。工具已在 GitHub 开源，提供文档和演示视频。

Conclusion: TokenSmith 降低了大模型预训练数据管理的技术门槛，让研究者能够更灵活高效地分析、编辑和利用数据集，有助于推动相关研究和应用发展。

Abstract: Understanding the relationship between training data and model behavior
during pretraining is crucial, but existing workflows make this process
cumbersome, fragmented, and often inaccessible to researchers. We present
TokenSmith, an open-source library for interactive editing, inspection, and
analysis of datasets used in Megatron-style pretraining frameworks such as
GPT-NeoX, Megatron, and NVIDIA NeMo. TokenSmith supports a wide range of
operations including searching, viewing, ingesting, exporting, inspecting, and
sampling data, all accessible through a simple user interface and a modular
backend. It also enables structured editing of pretraining data without
requiring changes to training code, simplifying dataset debugging, validation,
and experimentation.
  TokenSmith is designed as a plug and play addition to existing large language
model pretraining workflows, thereby democratizing access to production-grade
dataset tooling. TokenSmith is hosted on GitHub1, with accompanying
documentation and tutorials. A demonstration video is also available on
YouTube.

</details>


### [136] [GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning](https://arxiv.org/abs/2507.19457)
*Lakshya A Agrawal,Shangyin Tan,Dilara Soylu,Noah Ziems,Rishi Khare,Krista Opsahl-Ong,Arnav Singhvi,Herumb Shandilya,Michael J Ryan,Meng Jiang,Christopher Potts,Koushik Sen,Alexandros G. Dimakis,Ion Stoica,Dan Klein,Matei Zaharia,Omar Khattab*

Main category: cs.CL

TL;DR: 本文提出了一种基于自然语言反思的新型提示优化方法GEPA（Genetic-Pareto），可以极大提升大语言模型（LLM）在下游任务中的学习效率，显著减少所需的试验次数，并在多个任务上取得比现有方法更好的效果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型通常通过强化学习（如GRPO）适应下游任务，但这种方法需要大量回合、效率低下。作者认为语言本身的可解释性可以为LLM提供更丰富、更有效的学习信号，因此希望探索如何通过自然语言反思提升模型性能和优化效率。

Method: 作者提出了GEPA方法，它通过自然语言对模型执行结果进行反思、诊断问题、提出改进、合并经验。在优化过程中，GEPA不仅依靠结果好坏的简单标量奖励，还主动分析、总结并改写LLM的prompt，使其能迅速学习有效策略。GEPA的特点是即便只经历很少量的试验，也能收获明显提升。

Result: 在四项任务上，GEPA平均优于GRPO 10%，最高达20%，且回合数减少至1/35。相比主流的提示优化器MIPROv2，在两个不同LLM上均提升超过10%。在代码优化任务中，GEPA还展现出作为推理时搜索策略的潜力。

Conclusion: GEPA通过引入自然语言反思机制显著提升了LLM的学习效率和下游任务表现，证明了富有解释性的文本反馈对于高效模型优化的重要性，未来有望替代或补充以往依赖强化学习的LLM优化范式。

Abstract: Large language models (LLMs) are increasingly adapted to downstream tasks via
reinforcement learning (RL) methods like Group Relative Policy Optimization
(GRPO), which often require thousands of rollouts to learn new tasks. We argue
that the interpretable nature of language can often provide a much richer
learning medium for LLMs, compared with policy gradients derived from sparse,
scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt
optimizer that thoroughly incorporates natural language reflection to learn
high-level rules from trial and error. Given any AI system containing one or
more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool
calls, and tool outputs) and reflects on them in natural language to diagnose
problems, propose and test prompt updates, and combine complementary lessons
from the Pareto frontier of its own attempts. As a result of GEPA's design, it
can often turn even just a few rollouts into a large quality gain. Across four
tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up
to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,
MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an
inference-time search strategy for code optimization.

</details>


### [137] [Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models](https://arxiv.org/abs/2507.19470)
*Son Quoc Tran,Tushaar Gangavarapu,Nicholas Chernogor,Jonathan P. Chang,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: 本文提出了一个统一的对话预测任务评测框架，专门用于比较和评测对话“跑偏”检测（CGA）模型的能力，并引入了新指标用于衡量模型在对话进展过程中修正预测的能力。


<details>
  <summary>Details</summary>
Motivation: 自动系统如果能像人一样预判对话走向，将有助于更好地辅助人类交流。已有的CGA任务关注于预测对话是否会朝不良方向发展，但缺乏评测标准和系统对比。

Method: 提出并构建了首个统一的CGA评测基准，设计统一实验体系，支持多种模型直接和可靠的比较。同时，提出了新的评价指标，用于衡量模型在对话持续过程中修正自己预测能力。

Result: 该框架实现了不同CGA模型的统一和系统化比较，并基于最新的语言模型技术对当前CGA模型进行了全面评测。新指标展示出模型在多轮对话中刷新预测的能力差异。

Conclusion: 统一评测框架和新指标有助于推动对话转向预测领域的研究，提升模型在实际人机交互场景中的实用性和可靠性。

Abstract: We often rely on our intuition to anticipate the direction of a conversation.
Endowing automated systems with similar foresight can enable them to assist
human-human interactions. Recent work on developing models with this predictive
capacity has focused on the Conversations Gone Awry (CGA) task: forecasting
whether an ongoing conversation will derail. In this work, we revisit this task
and introduce the first uniform evaluation framework, creating a benchmark that
enables direct and reliable comparisons between different architectures. This
allows us to present an up-to-date overview of the current progress in CGA
models, in light of recent advancements in language modeling. Our framework
also introduces a novel metric that captures a model's ability to revise its
forecast as the conversation progresses.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [138] [Perpetua: Multi-Hypothesis Persistence Modeling for Semi-Static Environments](https://arxiv.org/abs/2507.18808)
*Miguel Saavedra-Ruiz,Samer B. Nashed,Charlie Gauthier,Liam Paull*

Main category: cs.RO

TL;DR: 本文提出了Perpetua方法，用以建模机器人环境中半静态特征的动态变化，并能够预测其未来状态。实验显示该方法精度高、适应性强，并能容忍缺失观测。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人环境建模方法难以有效表示动态特征，且缺乏对这些特征未来状态的预测能力，削弱了机器人在动态环境中长期部署的表现。本文旨在填补该技术空白。

Method: 提出Perpetua方法：结合"持续性"和"显现性"滤波器，利用贝叶斯框架串联混合来建模环境中特征的消失或再现概率，还可融合先验知识、多假设追踪并随时间自适应。

Result: 在仿真和真实数据上，Perpetua方法优于类似方法，能更准确地估计环境特征状态，并具备在线自适应和对观测缺失的鲁棒性。

Conclusion: Perpetua是一种高效、可扩展、通用且鲁棒的环境特征状态估计方法，能准确预测特征在当前及未来任意时刻的状态，有助于提升机器人系统在动态环境中的表现。

Abstract: Many robotic systems require extended deployments in complex, dynamic
environments. In such deployments, parts of the environment may change between
subsequent robot observations. Most robotic mapping or environment modeling
algorithms are incapable of representing dynamic features in a way that enables
predicting their future state. Instead, they opt to filter certain state
observations, either by removing them or some form of weighted averaging. This
paper introduces Perpetua, a method for modeling the dynamics of semi-static
features. Perpetua is able to: incorporate prior knowledge about the dynamics
of the feature if it exists, track multiple hypotheses, and adapt over time to
enable predicting of future feature states. Specifically, we chain together
mixtures of "persistence" and "emergence" filters to model the probability that
features will disappear or reappear in a formal Bayesian framework. The
approach is an efficient, scalable, general, and robust method for estimating
the states of features in an environment, both in the present as well as at
arbitrary future times. Through experiments on simulated and real-world data,
we find that Perpetua yields better accuracy than similar approaches while also
being online adaptable and robust to missing observations.

</details>


### [139] [Probabilistic Collision Risk Estimation through Gauss-Legendre Cubature and Non-Homogeneous Poisson Processes](https://arxiv.org/abs/2507.18819)
*Trent Weiss,Madhur Behl*

Main category: cs.RO

TL;DR: 本文提出一种高效且准确的碰撞风险估计算法（Gauss-Legendre Rectangle, GLR），在高速自动驾驶赛车中实现了更安全可靠的超车决策。


<details>
  <summary>Details</summary>
Motivation: 在高速赛车自动驾驶中，超车时碰撞风险极高，现有风险估计算法要么过于简化车辆模型（如包络圆），要么依赖蒙特卡洛采样，导致规划结果过于保守，难以满足高速度下的实时精准需求。

Method: 提出了一种基于Gauss-Legendre积分与非齐次泊松过程结合的两阶段积分方法（GLR），能够在充分考虑车辆几何形状和轨迹不确定性的前提下，实现对碰撞风险的高效估算。

Result: 在高精度F1赛车模拟环境中进行的446次超车实验表明，GLR算法相比五种主流算法平均误差降低77%，比次优方法提升52%，并且可实现1000Hz的高速推理。

Conclusion: GLR算法能实现高效、准确的碰撞风险估计，不仅提升了自动驾驶赛车的运动规划能力，也具有广泛的通用性，可应用于其他自动驾驶或运动规划场景。

Abstract: Overtaking in high-speed autonomous racing demands precise, real-time
estimation of collision risk; particularly in wheel-to-wheel scenarios where
safety margins are minimal. Existing methods for collision risk estimation
either rely on simplified geometric approximations, like bounding circles, or
perform Monte Carlo sampling which leads to overly conservative motion planning
behavior at racing speeds. We introduce the Gauss-Legendre Rectangle (GLR)
algorithm, a principled two-stage integration method that estimates collision
risk by combining Gauss-Legendre with a non-homogeneous Poisson process over
time. GLR produces accurate risk estimates that account for vehicle geometry
and trajectory uncertainty. In experiments across 446 overtaking scenarios in a
high-fidelity Formula One racing simulation, GLR outperforms five
state-of-the-art baselines achieving an average error reduction of 77% and
surpassing the next-best method by 52%, all while running at 1000 Hz. The
framework is general and applicable to broader motion planning contexts beyond
autonomous racing.

</details>


### [140] [MetaMorph -- A Metamodelling Approach For Robot Morphology](https://arxiv.org/abs/2507.18820)
*Rachel Ringe,Robin Nolte,Nima Zargham,Robert Porzel,Rainer Malaka*

Main category: cs.RO

TL;DR: 提出MetaMorph框架以系统化归类机器人形态，为人机交互研究提供更细致科学的机器人外观比较方法。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人外观分类方法过于宽泛且多聚焦于类人特征，难以覆盖所有类型机器人，难以系统研究外观对人机交互的影响。

Method: 作者提出MetaMorph框架，采用元建模方法，基于IEEE Robots Guide中222款机器人形态，开发出一套可以系统比较机器人视觉特征的分类标准。

Result: MetaMorph能够衡量不同机器人外观间的“视觉距离”，为机器人设计与功能之间的匹配提供量化工具。

Conclusion: MetaMorph为机器人外观研究提供了全面、细致的框架，有助于研究者系统探索最适合特定任务与情境的机器人设计特征。

Abstract: Robot appearance crucially shapes Human-Robot Interaction (HRI) but is
typically described via broad categories like anthropomorphic, zoomorphic, or
technical. More precise approaches focus almost exclusively on anthropomorphic
features, which fail to classify robots across all types, limiting the ability
to draw meaningful connections between robot design and its effect on
interaction. In response, we present MetaMorph, a comprehensive framework for
classifying robot morphology. Using a metamodeling approach, MetaMorph was
synthesized from 222 robots in the IEEE Robots Guide, offering a structured
method for comparing visual features. This model allows researchers to assess
the visual distances between robot models and explore optimal design traits
tailored to different tasks and contexts.

</details>


### [141] [Equivariant Volumetric Grasping](https://arxiv.org/abs/2507.18847)
*Pinhao Song,Yutong Hu,Pengteng Li,Renaud Detry*

Main category: cs.RO

TL;DR: 本论文提出了一种对垂直轴旋转等变的新型体积抓取模型，大幅提高了样本效率并降低算力和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 当前体积抓取（volumetric grasp）方法在面对物体旋转、变化时样本效率低、泛化能力不足，而现有模型很难兼顾等变性和自适应性。

Method: 模型采用三平面体积特征（tri-plane volumetric features）表示3D特征，并设计了在水平面上对90°旋转等变、在其它两个平面上特征和保持不变的新型特征结构。核心技术是新颖的可变形可导卷积（deformable steerable convolution），能够兼顾局部自适应和旋转等变。方法还对现有GIGA与IGD抓取规划器进行了等变增强（包括IGD的可变形注意力机制的等变公式推导，以及基于流匹配的抓取方向生成模型）。

Result: 大量仿真与真实实验验证了方法的等变性、样本效率以及资源优势。在模型基础上实现的等变抓取器整体性能显著超越非等变版本，且计算开销仅有小幅提升。

Conclusion: 本文所提基于投影的等变体积抓取特征设计，不仅提升了抓取模型性能，还有效降低了资源消耗，对实际机器人抓取系统具备重要意义。

Abstract: We propose a new volumetric grasp model that is equivariant to rotations
around the vertical axis, leading to a significant improvement in sample
efficiency. Our model employs a tri-plane volumetric feature representation --
i.e., the projection of 3D features onto three canonical planes. We introduce a
novel tri-plane feature design in which features on the horizontal plane are
equivariant to 90{\deg} rotations, while the sum of features from the other two
planes remains invariant to the same transformations. This design is enabled by
a new deformable steerable convolution, which combines the adaptability of
deformable convolutions with the rotational equivariance of steerable ones.
This allows the receptive field to adapt to local object geometry while
preserving equivariance properties. We further develop equivariant adaptations
of two state-of-the-art volumetric grasp planners, GIGA and IGD. Specifically,
we derive a new equivariant formulation of IGD's deformable attention mechanism
and propose an equivariant generative model of grasp orientations based on flow
matching. We provide a detailed analytical justification of the proposed
equivariance properties and validate our approach through extensive simulated
and real-world experiments. Our results demonstrate that the proposed
projection-based design significantly reduces both computational and memory
costs. Moreover, the equivariant grasp models built on top of our tri-plane
features consistently outperform their non-equivariant counterparts, achieving
higher performance with only a modest computational overhead. Video and code
can be viewed in: https://mousecpn.github.io/evg-page/

</details>


### [142] [A Fast and Light-weight Non-Iterative Visual Odometry with RGB-D Cameras](https://arxiv.org/abs/2507.18886)
*Zheng Yang,Kuan Xu,Shenghai Yuan,Lihua Xie*

Main category: cs.RO

TL;DR: 提出了一种高效的6自由度（6-DoF）机器人位姿估计方法，通过分离计算旋转和平移部分，依赖重叠平面特征和相关器，实现非迭代、无需特征点提取的RGB-D视觉里程计，提升了计算速度和低纹理环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RGB-D视觉里程计往往需特征提取/匹配与复杂的迭代优化，导致计算负担重且实时性差，尤其在特征稀少环境表现欠佳。有必要寻找计算更高效、对低纹理环境更鲁棒的方法。

Method: 方法将姿态估计的旋转和移动部分解耦。首先利用场景中重叠的平面特征直接计算旋转矩阵，然后通过核相关器（KCC）计算平移分量。整个过程抛弃了传统的特征点提取与迭代优化。

Result: 在一台低端i5 CPU上，本方法可达71Hz的处理速度。在无需依赖特征点时，在低纹理退化环境下的性能优于现有同类主流方法。

Conclusion: 本文方法显著提升了RGBD-VO的计算效率和低纹理环境下的鲁棒性，为机器人实时定位与导航提供了更可行的技术路径。

Abstract: In this paper, we introduce a novel approach for efficiently estimating the
6-Degree-of-Freedom (DoF) robot pose with a decoupled, non-iterative method
that capitalizes on overlapping planar elements. Conventional RGB-D visual
odometry(RGBD-VO) often relies on iterative optimization solvers to estimate
pose and involves a process of feature extraction and matching. This results in
significant computational burden and time delays. To address this, our
innovative method for RGBD-VO separates the estimation of rotation and
translation. Initially, we exploit the overlaid planar characteristics within
the scene to calculate the rotation matrix. Following this, we utilize a kernel
cross-correlator (KCC) to ascertain the translation. By sidestepping the
resource-intensive iterative optimization and feature extraction and alignment
procedures, our methodology offers improved computational efficacy, achieving a
performance of 71Hz on a lower-end i5 CPU. When the RGBD-VO does not rely on
feature points, our technique exhibits enhanced performance in low-texture
degenerative environments compared to state-of-the-art methods.

</details>


### [143] [GEAR: Gaze-Enabled Human-Robot Collaborative Assembly](https://arxiv.org/abs/2507.18947)
*Asad Ali Shahid,Angelo Moroncelli,Drazen Brscic,Takayuki Kanda,Loris Roveda*

Main category: cs.RO

TL;DR: 本论文提出了一种名为GEAR的注视引导系统，通过读取用户的视线方向，提升机器人在复杂装配任务中为人类助手的协作效率。该系统与传统触摸屏交互方式进行了对比实验，结果显示GEAR能够有效减轻用户的体力与操作负担，提升使用体验，特别是在复杂任务中更为显著。


<details>
  <summary>Details</summary>
Motivation: 当前，尽管机器人自主性和安全性有了显著进展，但在高度变动和精确要求的复杂装配任务中，机器人与人类的协作依然面临挑战。传统的人机交互界面（如触控屏）存在物理操作负担大、不够直观等不足。因此，开发更自然高效的人机交互方式是推动机器人在实际装配协作场景应用的关键动力。

Method: 作者提出了GEAR系统，通过注视追踪技术，让机器人能根据用户的目光指示，主动实施配合（如取零件）。本研究还设计了对比实验，将GEAR与基于触摸屏的传统人机交互方式进行对比，邀请30名参与者在两种装配场景（难度不同）下进行实验，收集客观表现与主观体验数据。

Result: 实验结果显示，使用GEAR系统的参与者在完成装配任务时，所需体力和认知负担明显低于触摸屏界面组，特别是在任务复杂度高时，GEAR对效率和体验提升更为突出。机器人能够准确理解凝视意图，高效配合取物。此外，用户的主观满意度与体验也显著提升。

Conclusion: GEAR系统通过注视交互方式，为复杂装配场景下的人机协作带来了显著优势，可有效减轻工人负担，提升协作效率和用户体验。这为将来进一步推广更自然、智能的人机协作界面提供了坚实实验基础和设计参考。

Abstract: Recent progress in robot autonomy and safety has significantly improved
human-robot interactions, enabling robots to work alongside humans on various
tasks. However, complex assembly tasks still present significant challenges due
to inherent task variability and the need for precise operations. This work
explores deploying robots in an assistive role for such tasks, where the robot
assists by fetching parts while the skilled worker provides high-level guidance
and performs the assembly. We introduce GEAR, a gaze-enabled system designed to
enhance human-robot collaboration by allowing robots to respond to the user's
gaze. We evaluate GEAR against a touch-based interface where users interact
with the robot through a touchscreen. The experimental study involved 30
participants working on two distinct assembly scenarios of varying complexity.
Results demonstrated that GEAR enabled participants to accomplish the assembly
with reduced physical demand and effort compared to the touchscreen interface,
especially for complex tasks, maintaining great performance, and receiving
objects effectively. Participants also reported enhanced user experience while
performing assembly tasks. Project page: sites.google.com/view/gear-hri

</details>


### [144] [Frequency Response Data-Driven Disturbance Observer Design for Flexible Joint Robots](https://arxiv.org/abs/2507.18979)
*Deokjin Lee,Junho Song,Alireza Karimi,Sehoon Oh*

Main category: cs.RO

TL;DR: 本文提出了一种基于频率响应函数（FRF）的优化方法，用于提升柔性关节机器人（FJR）中干扰观测器（DOB）的性能。该方法有效提高了控制带宽并抑制了振动，实验验证了其鲁棒性和运动性能的提升。


<details>
  <summary>Details</summary>
Motivation: 柔性关节机器人的运动控制因关节柔性和系统动力学的变化而变得复杂，导致传统干扰观测器鲁棒性受限，需要更优的方法提升控制性能。

Method: 提出了一种基于频率响应函数的优化方式，用于优化DOB的参数设计，通过提高带宽和抑制振动，增强系统性能，并采用Nyquist判据严格证明闭环系统的稳定性。

Result: 实验表明，该方法在柔性和系统参数变化条件下显著提升了机器人系统的鲁棒性和运动控制性能。

Conclusion: 这种新的基于FRF的DOB优化方法在柔性关节机器人上具有很好的实用性和提升空间，对抗系统参数变化和柔性扰动有明显优势。

Abstract: Motion control of flexible joint robots (FJR) is challenged by inherent
flexibility and configuration-dependent variations in system dynamics. While
disturbance observers (DOB) can enhance system robustness, their performance is
often limited by the elasticity of the joints and the variations in system
parameters, which leads to a conservative design of the DOB. This paper
presents a novel frequency response function (FRF)-based optimization method
aimed at improving DOB performance, even in the presence of flexibility and
system variability. The proposed method maximizes control bandwidth and
effectively suppresses vibrations, thus enhancing overall system performance.
Closed-loop stability is rigorously proven using the Nyquist stability
criterion. Experimental validation on a FJR demonstrates that the proposed
approach significantly improves robustness and motion performance, even under
conditions of joint flexibility and system variation.

</details>


### [145] [SmartPNT-MSF: A Multi-Sensor Fusion Dataset for Positioning and Navigation Research](https://arxiv.org/abs/2507.19079)
*Feng Zhu,Zihang Zhang,Kangcheng Teng,Abduhelil Yakup,Xiaohong Zhang*

Main category: cs.RO

TL;DR: 本文介绍了SmartPNT多源集成导航与定位数据集，涵盖多种传感器和丰富的环境类型，旨在推动高精度导航与多传感器融合研究。


<details>
  <summary>Details</summary>
Motivation: 现有公开导航定位数据集在传感器多样性和环境覆盖方面存在不足，制约了自主导航等领域的算法研究和系统测试。

Method: 开发了新的SmartPNT数据集，集成了GNSS、IMU、光学相机和LiDAR等多种传感器，详细记录传感器配置、坐标系定义、标定流程，并建立统一的数据采集与处理框架，适用于大规模分析。

Result: 通过VINS-Mono、LIO-SAM等主流SLAM算法对数据集进行验证，证明了该数据集在多传感器融合和高精度导航研究中的实用性与适用性。数据涵盖城市、校园、隧道和郊区等多种场景。

Conclusion: SmartPNT数据集弥补了现有导航数据集中传感器和环境多样性的不足，为复杂环境下的高精度导航研究提供了重要资源，将促进相关领域的进一步创新。

Abstract: High-precision navigation and positioning systems are critical for
applications in autonomous vehicles and mobile mapping, where robust and
continuous localization is essential. To test and enhance the performance of
algorithms, some research institutions and companies have successively
constructed and publicly released datasets. However, existing datasets still
suffer from limitations in sensor diversity and environmental coverage. To
address these shortcomings and advance development in related fields, the
SmartPNT Multisource Integrated Navigation, Positioning, and Attitude Dataset
has been developed. This dataset integrates data from multiple sensors,
including Global Navigation Satellite Systems (GNSS), Inertial Measurement
Units (IMU), optical cameras, and LiDAR, to provide a rich and versatile
resource for research in multi-sensor fusion and high-precision navigation. The
dataset construction process is thoroughly documented, encompassing sensor
configurations, coordinate system definitions, and calibration procedures for
both cameras and LiDAR. A standardized framework for data collection and
processing ensures consistency and scalability, enabling large-scale analysis.
Validation using state-of-the-art Simultaneous Localization and Mapping (SLAM)
algorithms, such as VINS-Mono and LIO-SAM, demonstrates the dataset's
applicability for advanced navigation research. Covering a wide range of
real-world scenarios, including urban areas, campuses, tunnels, and suburban
environments, the dataset offers a valuable tool for advancing navigation
technologies and addressing challenges in complex environments. By providing a
publicly accessible, high-quality dataset, this work aims to bridge gaps in
sensor diversity, data accessibility, and environmental representation,
fostering further innovation in the field.

</details>


### [146] [Bot Appétit! Exploring how Robot Morphology Shapes Perceived Affordances via a Mise en Place Scenario in a VR Kitchen](https://arxiv.org/abs/2507.19082)
*Rachel Ringe,Leandra Thiele,Mihai Pomarlan,Nima Zargham,Robin Nolte,Lars Hurrelbrink,Rainer Malaka*

Main category: cs.RO

TL;DR: 本研究探讨了机器人外观设计因素对人类在协作烹饪场景中角色分配与空间布局的影响。在虚拟现实环境中，参与者需依据机器人形态设置厨房并与机器人协作。研究收集了多模态数据和受试者访谈内容，提出了关于人类偏好生物形机器人、对机器人感知能力判断受形态影响较小等假设。


<details>
  <summary>Details</summary>
Motivation: 机器人的外观设计对人机协作中的信任与任务分配有重要影响，但目前关于哪些具体视觉因素影响角色分配与空间安排的研究有限，因此有必要深入探索相关因素。

Method: 让受试者在VR中与不同外形的机器人设定协作厨房环境。收集其空间安排、操作过程的口述思考内容以及任务后的结构化问卷数据，并进行综合分析。

Result: 数据分析后，研究提出三项假设：1）人类更偏好与类生物形态机器人协作；2）人们对机器人感知能力的判断不易受外形影响，而行动能力判断受影响较大；3）与细长灵巧的机器人共处时，人类减少了规避与让步行为。

Conclusion: 初步分析揭示了机器人外观对人类协作和空间分配的潜在影响，但结论需在后续研究中进一步验证。

Abstract: This study explores which factors of the visual design of a robot may
influence how humans would place it in a collaborative cooking scenario and how
these features may influence task delegation. Human participants were placed in
a Virtual Reality (VR) environment and asked to set up a kitchen for cooking
alongside a robot companion while considering the robot's morphology. We
collected multimodal data for the arrangements created by the participants,
transcripts of their think-aloud as they were performing the task, and
transcripts of their answers to structured post-task questionnaires. Based on
analyzing this data, we formulate several hypotheses: humans prefer to
collaborate with biomorphic robots; human beliefs about the sensory
capabilities of robots are less influenced by the morphology of the robot than
beliefs about action capabilities; and humans will implement fewer avoidance
strategies when sharing space with gracile robots. We intend to verify these
hypotheses in follow-up studies.

</details>


### [147] [Monocular Vision-Based Swarm Robot Localization Using Equilateral Triangular Formations](https://arxiv.org/abs/2507.19100)
*Taewon Kang,Ji-Wook Kwon,Il Bae,Jin Hyo Kim*

Main category: cs.RO

TL;DR: 本论文提出了一种适用于无基站、开放空间的小型机器人群的低成本定位系统，通过单目视觉与视觉标记实现高精度定位。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在实际应用中的精确定位非常关键，但传统的定位方法如GPS或昂贵传感器不适用于低成本群体机器人，特别是在没有地标或定位设施的开放环境中，因此急需新的低成本高效定位方法。

Method: 本文利用等边三角形队形的几何特性，通过小型机器人之间利用单目视觉测得的一维横向距离信息，实现机器人二维位置的估算，无需外部定位基础设施。

Result: 实验与仿真显示，随着运行时间的增加，该方法的定位误差显著低于传统的里程计推算方法，证明了新方法在低成本、开放环境中的定位优势。

Conclusion: 提出的基于等边三角形队形和单目视觉的定位方法能够为群体机器人在开放无基站环境下提供高精度、低成本的定位方案，对相关领域的实际应用具有较大推广价值。

Abstract: Localization of mobile robots is crucial for deploying robots in real-world
applications such as search and rescue missions. This work aims to develop an
accurate localization system applicable to swarm robots equipped only with
low-cost monocular vision sensors and visual markers. The system is designed to
operate in fully open spaces, without landmarks or support from positioning
infrastructures. To achieve this, we propose a localization method based on
equilateral triangular formations. By leveraging the geometric properties of
equilateral triangles, the accurate two-dimensional position of each
participating robot is estimated using one-dimensional lateral distance
information between robots, which can be reliably and accurately obtained with
a low-cost monocular vision sensor. Experimental and simulation results
demonstrate that, as travel time increases, the positioning error of the
proposed method becomes significantly smaller than that of a conventional
dead-reckoning system, another low-cost localization approach applicable to
open environments.

</details>


### [148] [Diverse and Adaptive Behavior Curriculum for Autonomous Driving: A Student-Teacher Framework with Multi-Agent RL](https://arxiv.org/abs/2507.19146)
*Ahmed Abouelazm,Johannes Ratz,Philip Schörner,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 本文提出了一种用于自动驾驶RL训练的新型学生-教师自动课程学习框架，实现了不同难度交通行为自动生成和自适应课程调节，有效提升了学习到的自动驾驶策略的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在自动驾驶场景训练中主要依赖规则化或手工设计的交通场景，导致泛化性不足，且现有场景生成方法侧重极端/关键情形，忽视了日常驾驶行为的平衡。现有课程学习也多为手工设计，缺乏对真实交通行为动态的关注。

Method: 提出了学生-教师框架：教师作为基于图的多智能体RL模块，能根据学生表现自适应生成涵盖从常见到关键的多样交通行为任务，并动态调整任务难度；学生为部分可观测的深度RL智能体，模拟真实感知限制下的驾驶学习。

Result: 实验结果表明该教师能生成丰富多样的交通行为，学生在自动课程驱动下训练，优于在规则交通场景中训练的智能体，并表现出更高得分及更平衡、果断的驾驶风格。

Conclusion: 自动课程学习框架有效提升了自动驾驶RL智能体对复杂现实交通的适应能力和鲁棒性，对安全、泛化的自动驾驶策略训练具有重要意义。

Abstract: Autonomous driving faces challenges in navigating complex real-world traffic,
requiring safe handling of both common and critical scenarios. Reinforcement
learning (RL), a prominent method in end-to-end driving, enables agents to
learn through trial and error in simulation. However, RL training often relies
on rule-based traffic scenarios, limiting generalization. Additionally, current
scenario generation methods focus heavily on critical scenarios, neglecting a
balance with routine driving behaviors. Curriculum learning, which
progressively trains agents on increasingly complex tasks, is a promising
approach to improving the robustness and coverage of RL driving policies.
However, existing research mainly emphasizes manually designed curricula,
focusing on scenery and actor placement rather than traffic behavior dynamics.
This work introduces a novel student-teacher framework for automatic curriculum
learning. The teacher, a graph-based multi-agent RL component, adaptively
generates traffic behaviors across diverse difficulty levels. An adaptive
mechanism adjusts task difficulty based on student performance, ensuring
exposure to behaviors ranging from common to critical. The student, though
exchangeable, is realized as a deep RL agent with partial observability,
reflecting real-world perception constraints. Results demonstrate the teacher's
ability to generate diverse traffic behaviors. The student, trained with
automatic curricula, outperformed agents trained on rule-based traffic,
achieving higher rewards and exhibiting balanced, assertive driving.

</details>


### [149] [ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination](https://arxiv.org/abs/2507.19151)
*Michael Amir,Guang Yang,Zhan Gao,Keisuke Okumura,Heedo Woo,Amanda Prorok*

Main category: cs.RO

TL;DR: 该论文提出了ReCoDe框架，将强化学习与约束优化结合，能够在多智能体任务中实现更高效的协调和自适应。


<details>
  <summary>Details</summary>
Motivation: 传统手工设定的约束条件在多智能体任务中容易失效，尤其是在需要复杂协作时，因此需要一种能够自适应、提升多智能体协作效果的控制方法。

Method: ReCoDe（Reinforcement-based Constraint Design）是一种分布式混合方法，将优化控制的可靠性与多智能体强化学习的适应性结合。具体做法是在保持专家控制器（如优化控制器）的基础上，通过强化学习自动生成动态约束，从而在局部通信下，智能体能够联合调整其可行动作，更好地适应环境变化和任务需求。

Result: 在多智能体导航任务中，ReCoDe相较于纯手工控制器、其他混合方法和标准多智能体强化学习（MARL）有更优表现。论文提供了实际机器人实验和理论分析，证明该方法在保持控制器指导性的同时，还能根据情境动态调整控制器的作用。

Conclusion: 保留并改进非完美的人为控制器往往比完全从零学习效率更高。ReCoDe能兼顾可靠性和自适应能力，显著提升多智能体系统的协调与控制性能。

Abstract: Constraint-based optimization is a cornerstone of robotics, enabling the
design of controllers that reliably encode task and safety requirements such as
collision avoidance or formation adherence. However, handcrafted constraints
can fail in multi-agent settings that demand complex coordination. We introduce
ReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid
framework that merges the reliability of optimization-based controllers with
the adaptability of multi-agent reinforcement learning. Rather than discarding
expert controllers, ReCoDe improves them by learning additional, dynamic
constraints that capture subtler behaviors, for example, by constraining agent
movements to prevent congestion in cluttered scenarios. Through local
communication, agents collectively constrain their allowed actions to
coordinate more effectively under changing conditions. In this work, we focus
on applications of ReCoDe to multi-agent navigation tasks requiring intricate,
context-based movements and consensus, where we show that it outperforms purely
handcrafted controllers, other hybrid approaches, and standard MARL baselines.
We give empirical (real robot) and theoretical evidence that retaining a
user-defined controller, even when it is imperfect, is more efficient than
learning from scratch, especially because ReCoDe can dynamically change the
degree to which it relies on this controller.

</details>


### [150] [Towards Multimodal Social Conversations with Robots: Using Vision-Language Models](https://arxiv.org/abs/2507.19196)
*Ruben Janssens,Tony Belpaeme*

Main category: cs.RO

TL;DR: 大语言模型赋予社交机器人进行开放域对话的能力，但缺乏利用多模态信息的社会技能。本文讨论将视觉-语言模型应用于社交机器人对话，并探讨技术挑战及评估。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型提升了社交机器人对话表现，但机器人在真实社交情境下需要处理多模态（如视觉）信息，这方面目前研究和应用不足。

Method: 文章提出将视觉-语言模型适配于社交机器人的对话系统，用于处理广泛的视觉信息，并描绘了这种多模态系统的需求及实现思路，同时指出存在的技术瓶颈。

Result: 初步展示了视觉-语言模型能够总体满足社交对话中多模态信息处理需求，但仍存在技术挑战，例如模型适配和评估等。

Conclusion: 多模态能力对社交机器人至关重要，视觉-语言模型是实现这一目标的有前景方法，但需要进一步应对实际应用中的技术难题，并改进评估方式。

Abstract: Large language models have given social robots the ability to autonomously
engage in open-domain conversations. However, they are still missing a
fundamental social skill: making use of the multiple modalities that carry
social interactions. While previous work has focused on task-oriented
interactions that require referencing the environment or specific phenomena in
social interactions such as dialogue breakdowns, we outline the overall needs
of a multimodal system for social conversations with robots. We then argue that
vision-language models are able to process this wide range of visual
information in a sufficiently general manner for autonomous social robots. We
describe how to adapt them to this setting, which technical challenges remain,
and briefly discuss evaluation practices.

</details>


### [151] [Foundation Model-Driven Grasping of Unknown Objects via Center of Gravity Estimation](https://arxiv.org/abs/2507.19242)
*Kang Xiangli,Yage He,Xianwu Gong,Zehan Liu,Yuru Bai*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩散模型的重心定位方法，有效提升机器人抓取不规则质量分布物体时的成功率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人抓取方法对于物体重心偏移问题表现不佳，导致抓取不稳定，尤其是在质量分布不均的未知物体上，急需更精准的重心识别手段提升抓取效果。

Method: 构建包含790张不均匀质量分布物体图片及重心关键点标注的数据集，采用视觉驱动的基础模型与扩散模型框架，实现对未知物体重心位置的本体感知抓取。

Result: 在真实场景实验中，本方法相较传统关键点方法和最先进的可用性驱动方法，抓取成功率分别提升了49%与11%；对未知物体的重心定位准确率达76%。

Conclusion: 基于视觉和扩散模型的新系统，在多场景下展现出强泛化能力，有效提升了对不规则物体抓取的准确性和稳定性，为复杂抓取任务提供了创新性解决方案。

Abstract: This study presents a grasping method for objects with uneven mass
distribution by leveraging diffusion models to localize the center of gravity
(CoG) on unknown objects. In robotic grasping, CoG deviation often leads to
postural instability, where existing keypoint-based or affordance-driven
methods exhibit limitations. We constructed a dataset of 790 images featuring
unevenly distributed objects with keypoint annotations for CoG localization. A
vision-driven framework based on foundation models was developed to achieve
CoG-aware grasping. Experimental evaluations across real-world scenarios
demonstrate that our method achieves a 49\% higher success rate compared to
conventional keypoint-based approaches and an 11\% improvement over
state-of-the-art affordance-driven methods. The system exhibits strong
generalization with a 76\% CoG localization accuracy on unseen objects,
providing a novel solution for precise and stable grasping tasks.

</details>


### [152] [How Age Influences the Interpretation of Emotional Body Language in Humanoid Robots -- long paper version](https://arxiv.org/abs/2507.19335)
*Ilaria Consoli,Claudio Mattutino,Cristina Gena,Berardina de Carolis,Giuseppe Palestra*

Main category: cs.RO

TL;DR: 本研究通过对不同年龄段（儿童、年轻成人、老年人）参与者的实验，分析了他们如何理解NAO机器人表达的情感肢体语言，并比较了各组对机器人情感表达的反应。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在社会生活中的应用日益广泛，了解不同年龄群体对机器人情感表达的解读方式，对于提升人机交互体验具有重要意义。

Method: 通过与NAO机器人互动，让儿童、年轻成人、老年人观察和解读其情感肢体语言，收集和分析其对情感表达的理解与反馈。

Result: 研究发现，儿童和老年人在识别和反应机器人情感表达方面更为相似，而与年轻成年人存在差异。

Conclusion: 人们对机器人情感肢体语言的解读存在年龄相关的差异，未来应针对不同用户群体优化机器人的情感表达方式。

Abstract: This paper presents an empirical study investigating how individuals across
different age groups, children, young and older adults, interpret emotional
body language expressed by the humanoid robot NAO. The aim is to offer insights
into how users perceive and respond to emotional cues from robotic agents,
through an empirical evaluation of the robot's effectiveness in conveying
emotions to different groups of users. By analyzing data collected from elderly
participants and comparing these findings with previously gathered data from
young adults and children, the study highlights similarities and differences
between the groups, with younger and older users more similar but different
from young adults.

</details>
